2
2
0
2

t
c
O
0
1

]

G
L
.
s
c
[

3
v
4
4
5
5
1
.
3
0
2
2
:
v
i
X
r
a

Graph Neural Networks are Dynamic Programmers

Andrew Dudzik∗
DeepMind
adudzik@deepmind.com

Petar Veliˇckovi´c∗
DeepMind
petarv@deepmind.com

Abstract

Recent advances in neural algorithmic reasoning with graph neural networks
(GNNs) are propped up by the notion of algorithmic alignment. Broadly, a neural
network will be better at learning to execute a reasoning task (in terms of sam-
ple complexity) if its individual components align well with the target algorithm.
Speciﬁcally, GNNs are claimed to align with dynamic programming (DP), a gen-
eral problem-solving strategy which expresses many polynomial-time algorithms.
However, has this alignment truly been demonstrated and theoretically quantiﬁed?
Here we show, using methods from category theory and abstract algebra, that
there exists an intricate connection between GNNs and DP, going well beyond the
initial observations over individual algorithms such as Bellman-Ford. Exposing
this connection, we easily verify several prior ﬁndings in the literature, produce
better-grounded GNN architectures for edge-centric tasks, and demonstrate empiri-
cal results on the CLRS algorithmic reasoning benchmark. We hope our exposition
will serve as a foundation for building stronger algorithmically aligned GNNs.

1

Introduction

One of the principal pillars of neural algorithmic reasoning [27] is training neural networks that
execute algorithmic computation in a high-dimensional latent space. While this process is in itself
insightful, and can lead to stronger combinatorial optimisation systems [21], it is valuable in terms
of expanding the applicability of classical algorithms. Evidence of this value are emerging, with
pre-trained algorithmic reasoners utilised in implicit planning [11] and self-supervised learning [28].

A fundamental question in this space is: which architecture should be used to learn a particular
algorithm (or collection of algorithms [36])? Naturally, we seek architectures that have low sample
complexity, as they will allow us to create models that generalise better with fewer training examples.

The key theoretical advance towards achieving this aim has been made by [37]. Therein, the authors
formalise the notion of algorithmic alignment, which states that we should favour architectures
that align better to the algorithm, in the sense that we can separate them into modules, which
individually correspond to the computations of the target algorithm’s subroutines. It can be proved
that architectures with higher algorithmic alignment will have lower sample complexity in the NTK
regime [20]. Further, the theory of [37] predicts that graph neural networks (GNNs) algorithmically
align with dynamic programming [3, DP]. The authors demonstrate this by forming an analogy to the
Bellman-Ford algorithm [2].

Since DP is a very general class of problem-solving techniques that can be used to express many
classical algorithms, this ﬁnding has placed GNNs as the central methodology for neural algorithmic
execution [7]. However, it quickly became apparent that it is not enough to just train any GNN—for
many algorithmic tasks, careful attention is required. Several papers illustrated special cases of GNNs
that align with sequential algorithms [31], linearithmic sequence processing [16], physics simulations

∗Equal contribution.

36th Conference on Neural Information Processing Systems (NeurIPS 2022).

 
 
 
 
 
 
[23], iterative algorihtms [26], data structures [29] or auxiliary memory [24]. Some explanations for
this lack of easy generalisation have arisen—we now have both geometric [38] and causal [4] views
into how better generalisation can be achieved.

We believe that the fundamental reason why so many isolated efforts needed to look into learning
speciﬁc classes of algorithms is the fact the GNN-DP connection has not been sufﬁciently explored.
Indeed, the original work of [37] merely mentions in passing that the formulation of DP algorithms
seems to align with GNNs, and demonstrates one example (Bellman-Ford). Our thorough investiga-
tion of the literature yielded no concrete follow-up to this initial claim. But DP algorithms are very
rich and diverse, often requiring a broad spectrum of computations. Hence what we really need is a
framework that could allow us to identify GNNs that could align particularly well with certain classes
of DP, rather than assuming a “one-size-ﬁts-all” GNN architecture will exist.

As a ﬁrst step towards this, in this paper we interpret the operations of both DP and GNNs from the
lens of category theory and abstract algebra. We elucidate the GNN-DP connection by observing
a diagrammatic abstraction of their computations, recasting algorithmic alignment to aligning the
diagrams of (G)NNs to ones of the target algorithm class. In doing so, several previously shown
results will naturally arise as corollaries, and we propose novel GNN variants that empirically align
better to edge-centric algorithms. We hope our work opens up the door to a broader uniﬁcation
between algorithmic reasoning and the geometric deep learning blueprint [5].

2 GNNs, dynamic programming, and the categorical connection

Before diving into the theory behind our connection, we provide a quick recap on the methods being
connected: graph neural networks and dynamic programming. Further, we cite related work to outline
why it is sufﬁcient to interpret DP from the lens of graph algorithms.

We will use the deﬁnition of GNNs based on [5]. Let a graph be a tuple of nodes and edges,
G = (V, E), with one-hop neighbourhoods deﬁned as
. Further, a
v
{
k gives the features of node u as xu; we omit edge- and graph-level
V
node feature matrix X
R|
features for clarity. A (message passing) GNN over this graph is then executed as:

Nu =

(v, u)

E

∈

∈

∈

|×

V

}

|

(cid:80)

where ψ : Rk
Rk is a message function, φ : Rk
a permutation-invariant aggregation function (such as
MLPs, but many special cases exist, giving rise to, e.g., attentional GNNs [30].

Rk

→

→

×

×

hu = φ

xu,

(cid:32)

(cid:77)v
∈N

u

ψ(xu, xv)

(cid:33)
Rk
Rk is a readout function, and
is
or max). Both ψ and φ can be realised as

(1)

(cid:76)

Dynamic programming is deﬁned as a process that solves problems in a divide et impera fashion:
imagine that we want to solve a problem instance x. DP proceeds to identify a set of subproblems,
η(x), such that solving them ﬁrst, and recombining the answers, can directly lead to the solution for
x: f (x) = ρ(
). Eventually, we decompose the problem enough until we arrive at
f (y)
{
an instance for which the solution is trivially given (i.e. f (y) which is known upfront). From these
“base cases”, we can gradually build up the solution for the problem instance we initially care for in a
bottom-up fashion. This rule is often expressed programmatically:

η(x)
}

∈

y

|

dp[x]

←

recombine(score(dp[y], dp[x]) for y in expand(x))

(2)

To initiate our discussion on why DP can be connected with GNNs, it is a worthwhile exercise to
show how Equation 2 induces a graph structure. To see this, we leverage a categorical analysis
of dynamic programming ﬁrst proposed by [10]. Therein, dynamic programming algorithms are
reasoned about as a composition of three components (presented here on a high level):

dp =

ρ

recombine

◦

σ

score

◦

η

expand

(3)

Expansion selects the relevant subproblems; scoring computes the quality of each individual subprob-
lem’s solution w.r.t. the current problem, and recombining combines these solutions into a solution
for the original problem (e.g. by taking the max, or average).

(cid:124)(cid:123)(cid:122)(cid:125)

(cid:124)(cid:123)(cid:122)(cid:125)

(cid:124)(cid:123)(cid:122)(cid:125)

Therefore, we can actually identify every subproblem as a node in a graph. Let V be the space of all
subproblems, and R an appropriate value space (e.g. the real numbers). Then, expansion is deﬁned

2

→ P

as η : V
induces a set of edges between subproblems, E; namely, (x, y)
scored by using a function σ :
P
the recombination function, ρ :
R in each of the subproblems of interest.
computes a function dp : V

(V ), giving the set of all subproblems relevant for a given problem. Note that this also
η(y). Each subproblem is
(R). Finally, the individual scores are recombined using
R. The ﬁnal dynamic programming primitive therefore

→ P
→

E if x

(V )

(R)

P

∈

∈

Therefore, dynamic programming algorithms can be seen as performing computations over a graph
of subproblems, which can usually be precomputed for the task at hand (since the outputs of η are
assumed known upfront for every subproblem). One speciﬁc popular example is the Bellman-Ford
algorithm [2], which computes single-source shortest paths from a given source node, s, in a graph
G = (V, E). In this case, the set of subproblems is exactly the set of nodes, V , and the expansion
η(u) is exactly the set of one-hop neighbours of u in the graph. The algorithm maintains distances of
every node to the source, du. The rule for iteratively recombining these distances is as follows:

→

du ←

min

du, min
v
∈N

u

(cid:18)

dv + wv

u

→

(cid:19)

(4)

u is the distance between nodes v and u. The algorithm’s base cases are ds = 0 for the
where wv
→
source node, du = +
otherwise. Note that more general forms of Bellman-Ford pathﬁnding exist,
for appropriate deﬁnitions of + and min (in general known as a semiring). Several recent research
papers such as NBFNet [39] explicitly call on this alignment in their motivation.

∞

3 The difﬁculty of connecting GNNs and DP

The basic technical obstacle to establishing a rigorous correspondence between neural networks and
DP is the vastly different character of the computations they perform. Neural networks are built from
linear algebra over the familiar real numbers, while DP, which is often a generalisation of path-ﬁnding
, min, +)2, which are usually
problems, typically takes place over “tropical” objects like (N
studied in mathematics as “degenerations” of Euclidean space. The two worlds cannot clearly be
reconciled, directly, with simple equations.

∪ {∞}

However, if we deﬁne an arbitrary “latent space” R and make as few assumptions as possible, we
can observe that many of the behaviors we care about, for both GNNs and DP, arise from looking at
functions S
R, where S is a ﬁnite set. R can be seen as the set of real-valued vectors in the case
of GNNs, and the tropical numbers in the case of DP.

→

So our principal object of study is the category of ﬁnite sets, and “R-valued quantities” on it. By
“category” here we mean a collection of objects (all ﬁnite sets) together with a notion of composable
arrows (functions between ﬁnite sets).

To draw our GNN-DP connection, we need to devise an abstract object which can capture both the
GNN’s message passing/aggregation stages (Equation 1) and the DP’s scoring/recombination stages
(Equation 2). It may seem quite intuitive that these two concepts can and should be relatable, and
category theory is a very attractive tool for “making the obvious even more obvious” [15]. Indeed,
recently concepts from category theory have enabled the construction of powerful GNN architectures
beyond permutation equivariance [9]. Here, we propose integral transforms as such an object.

We will construct the integral transform by composing transformations over our input features in a
way that will depend minimally on the speciﬁc choice of R. In doing so, we will build a computational
diagram that will be applicable for both GNNs and DP (and their own choices of R), and hence
allowing for focusing on making components of those diagrams as aligned as possible.

4 The integral transform

An integral transform can be encoded in a diagram of this form, which we call a polynomial span:

2Here we require the addition of a special “∞” placeholder object to denote the vertices the DP expansion

hasn’t reached so far.

3

X

i

W

p

Y

o

Z

where W, X, Y and Z are ﬁnite sets. The arrows i, p, o stand, respectively, for “input”, “process”,
and “output”. In context, the sets will have the following informal meaning: W represents the set
over which we deﬁne our inputs, Z the set over which we deﬁne outputs. X and Y are, respectively,
carrier sets for the arguments, and the messages3—we will clarify their meaning shortly.

Before proceeding, it is worthy to note the special case of X = Y = E, with p being the identity
map. Such a diagram is commonly known as a span. A span that additionally has W = Z = V is
equivalent to a representation of a directed graph with vertex set Z and edge set Y (V
V );
→
in this case i(e) and o(e) are the functions identifying the source and target nodes of each edge.

←

E

The key question is: given input data f on W , assigning features f (w) to each w
W , how to
transform it, via the polynomial span, into data on Z? If we can do this, we will be able to characterise
both the process of sending messages between nodes in GNNs and scoring subproblems in DP.

∈

For us, data on a carrier set S consists of an element of [S, R] :=
, where R is a “set of
}
possible values”. For now, we will think of R as an arbitrary (usually inﬁnite) set, though we will see
later that it should possess some algebraic structure; it should be a semiring.

f : S
{

→

R

The transform proceeds in three steps, following the edges of the polynomial span:

[X, R]

p⊗

[Y, R]

i∗

o⊕

[W, R]

[Z, R]

(5)

the pullback, the argument pushfoward, and the message pushfor-
We call the three arrows i∗, p
ward. Taken together, they form an integral transform—and we conjecture that this transform can
correspond to the dependent product and
be described as a polynomial functor, where p
dependent sum from type theory (cf. Appendix D for details).

and o

, o

⊕

⊕

⊗

⊗

The pullback i∗ is the easiest to deﬁne. Since we have a function i : X
span) and a function f : W
X

→
R, by composition. We hence deﬁne i∗f = f

W (part of the polynomial
R (our input data), we can produce data on X, that is, a function in

→

i.

→

◦

Unfortunately, the other two arrows of the polynomial span point in the wrong direction for naïve
composition. For the moment, we will focus on how to deﬁne o

and leave p

for later.

⊕

⊗

We start with message data m : Y
to deﬁne a composition with o−
bijective, the preimage o−
technicality: if the composition m
are unable to tell from a subset of R whether multiple messages had the same value.

R. It may be attractive to invert the output arrow o in order
1, as was done in the case of the pullback. However, unless o is
(Y ) takes values in the power set of Y . There is an additional
(R), it will fail to detect multiplicities; we
o−

1 takes values in

→ P
◦

1 : Z

→

P

So instead, our pushforward takes values in bag(R), the set of ﬁnite multisets (or bags) of R, which
we describe in more detail in appendix B. For the moment, it is enough to know that a bag is
equivalent to a formal sum, and we deﬁne an intermediate message pushforward (o
m)(u) :=
Σe
∈

[Z, bag(R)].

t−1(u)m(e)

⊕

∈

3For technical reasons, we ask that for each y ∈ Y , the preimage p−1(y) = {x ∈ X | p(x) = y} should
have a total order. This is to properly support functions with non-commuting arguments, though this detail is
unnecessary for our key examples, where arguments commute.

4

E

E

→

Figure 1: The illustration of how pullback and pushforward combine to form the integral transform,
for two speciﬁc cases. Left: Polynomial span V
V with trivial argument pushforward
←
(identity). Each edge euv is connected to its sender and receiver nodes (u, v) via the span (black
arrows). The pullback then “pulls” the node features f (u) along the span, which the argument
pushforward folds into edge features g(evu) = f (u). Once all sender features are pulled back to
their edges, the message pushforward then “collects” all of the edge features that send to a particular
receiver, by pushing them along the span. Right: Polynomial span V
V ,
a situation more commonly found in GNNs. In this case, the pullback pulls sender and receiver
node features into the argument function, h. The argument pushforward then computes, from these
arguments, the edge messages, g, which are sent to receivers via the message pushforward, as before.
See Appendix A for a visualisation of how these arrows translate into GNN code.

E + E

←

→

→

→

E

All that is missing to complete our deﬁnition of o
R. As we
will see later, specifying a well-behaved aggregator is the same as imposing a commutative monoid
structure on R. With such an aggregator on R, we can deﬁne (o

→
m)(u).

is an aggregator

(cid:76)
m)(u) :=

: bag(R)

(o

⊕

⊕

⊕

We return to p
, which is constructed very similarly. The only difference is that, while we deliberately
regard the collection of messages as unordered, the collection of arguments used to compute a message
has an ordering we wish to respect. So instead of the type bag(R), we use the type list(R) of ﬁnite
lists of elements of R, and our aggregator
: list(R)

R is now akin to a fold operator.

(cid:76)

⊗

We illustrate the use of these two aggregators in a decomposed diagram:

(cid:78)

→

[Y, list(R)]

p⊗

(cid:78)

[X, R]

p⊗

[Y, R]

i∗

[W, R]

o⊕

(cid:76)

o⊕

[Z, R]

[Z, bag(R)]

,

⊕

,
⊗

Note that any semiring (R,

) comes equipped with binary operators

that allow aggregators
to be deﬁned inductively. In fact, the converse—that every set with two such aggregators
is a semiring—is also true, if we assume some reasonable conditions on the aggregators, which
(cid:78)
we can explain in terms of one of the most utilised concepts in category theory and functional
programming—monads [33]. Due to space constraints, we refer the interested reader to Appendices
B and C for a full exposition of how we can use monads over lists and bags to constrain the latent
space R to respect a semiring structure.

(cid:76)

⊕

⊗

,

5

ebaebcebdebeebbbacdef(d)g(ebd)g(eba)g(ebc)g(ebe)g(ebb)pushforwardpullbackebaebcebdebeebbbacdef(d)g(ebd)g(eba)g(ebc)g(ebe)g(ebb)h((ebd,0))h((ebd,1))pullbackmessagepushforwardargumentpushforwardFor now, it’s enough to know that our key examples of the real numbers (with multiplication and
addition, for GNNs) and the tropical natural numbers (with addition and minimum, for DP) both
allow for natural interpretations of

in the integral transform.

and

We are now ready to show how the integral transform can be used to instantiate popular examples
of algorithms and GNNs. We start with the Bellman-Ford algorithm [2] (Equation 4) that was
traditionally used to demonstrate the concept of algorithmic alignment.

(cid:76)

(cid:78)

5 Bellman-Ford

⊕

Let R = (N
and

= +
= min. This is the coefﬁcient semiring over which the Bellman-Ford algorithm takes place.

, +, min) be the “min-plus” semiring of extended natural numbers, with

∪ {∞}

⊗

Let (V, E) be a weighted graph with source and target maps s, t : E
w : E
b : V
necessary for deﬁning the argument pushforward.

V and edge weights
R. For purely technical reasons, we also need to explicitly materialise a bias function
V ) but will prove

R, which is, in practice, a constant-zero function (b(v) = 0 for all v

→

→

→

∈

We interpret Bellman-Ford as the following polynomial span:

(V + E) + (V + E)

p

V + E

i

V + (V + E)

o

V

(6)

Here “+” is the disjoint union of sets, deﬁned as A + B =
that [S + T, R] ∼= [S, R]
data on each component separately.

. Note
(a, 1)
}
[T, R], i.e. specifying data on a disjoint union is equivalent to specifying

(b, 2)

} ∪ {

×

B

A

∈

∈

a

{

b

|

|

Initially, we describe each of the four sets of the polynomial span, making their role clear:

×

×

• Input: W = V + (V + E). Our input to Bellman-Ford includes: the current estimate of
node distances (du; a function in [V, R]), edge weights (w; a function in [E, R]), and the
previously discussed bias b, a function in [V, R]. Hence our overall inputs are members of
[V, R]

[V, R] ∼= [V + (V + E), R], justifying our choice of input space.

[E, R]

• Arguments: X = (V + E) + (V + E). Here we collect the ingredients necessary to
compute Bellman-Ford’s subproblem solutions coming from neighbouring nodes. To do this,
we need to combine data in the nodes with data living on edges—those are the arguments
to the function. And since they meet in the edges, we “lift” our node distances [V, R] to
edges they are sending from, giving us an additional function in [E, R]. Hence our argument
carrier space is now (V + E) + (V + E) (the remaining three inputs remain unchanged).
• Message: Y = V + E. Once the arguments are combined to compute messages, we are left
with signal in each edge (containing the sum of corresponding du and wuv), and each node
(containing just du, for the purposes of access to the previous optimal solution). Hence our
messages are members of [V, R]

[E, R], justifying our choice of message space.

• Output: Z = V . Lastly, the output of one step of Bellman-Ford are updated values d(cid:48)u,

which we can interpret as just (output) data living on V .

×

We now describe how to propagate data along each arrow of the diagram in turn, beginning with
R, a bias b : V
inputs (f, b, w) of node features f : V

R, and edge weights w : E

R:

→

→

→

• Pullback, i∗: First, we can note the input function i : (V + E) + (V + E)

decomposes as the sum of two arrows. i1 : V + E
the source function on E, and i2 : V + E
pullback i∗(f, b, w) = (f, f
• Argument pushforward, p

→

of V + E, and sums their values. So the argument pushforward is p

s, b, w), giving us the arguments to compute messages.
◦
: Next, the process function p simply identiﬁes the two copies
s, b, w) =

(f, f

⊗

V + (V + E)
V is the identity function on V and
V + E is just the identity. So we calculate the

→

→

⊗

◦

6

s)

(f, f
(b, w) = (f + b, (f
b, as a “self-edge” in the graph with weight 0.

⊗

◦

◦

s) + w). This also allows us to interpret the bias function,

• Message pushforward, o

: The output function o : V + E

V and the target function on E. So the message pushforward gives us (o
s)(e) = min(f (u) + b(u), minv
w))(u) = (f (u) + b(u))

t(e)=u(f

→

V is the identity function on
s) +
u).

(f + b, (f
◦
u f (v) + wv

⊕

→

→

⊕

⊕

◦

Letting b(u) = 0, we can see that this is exactly Equation 4. So we have produced the formula for the
Bellman-Ford algorithm directly from the polynomial span in Diagram 6.

(cid:76)

⊕

is aligned with using max aggregation in neural networks—directly explaining several
Note that p
previous proposals, such as [31]. But additionally, p
, as deﬁned, is aligned with concatenating all
message arguments together and passing them through a linear function, which is how such a step is
implemented in GNNs’ message functions. We now direct our polynomial span analysis at GNNs.

⊗

6 GNNs

We study the popular message passing neural network (MPNN) model [19], which can be interpreted
using the following polynomial span diagram:

E + (E + E) + E

p

i

1 + V + E

E

o

V

Here the set 1 refers to a singleton set—sometimes also called (), or unit—which is used as a carrier
for graph-level features. This implies the graph features will be speciﬁed as [1, R] ∼= R, as expected.
Given all these features, how would we compute messages? The natural way is to combine the
features of the sender and receiver node of each edge, features of said edge, and graph-level features—
these will form our arguments, and they need to all “meet” in the edges. This motivates our argument
space as E + (E + E) + E: all of the above four, accordingly broadcast into their respective edge(s).

The input map, i, is then the unique map to the singleton, the sender and receiver functions on the two
middle copies of E, and the identity on the last copy of E, i.e. i(a, b, c, d) =
. The
(), s(b), t(c), d
}
{
process map, p, collapses the four copies of E into just one, to hold the computed message. Lastly,
the output map, o, is the target function, identifying the node to which the message will be delivered.

The actual computation performed by the network (over real values in R, which can support various
semirings of interest) is exactly an integral transform, with an extra MLP processing step on messages:

[E + (E + E) + E, R]

p⊗

[E, R]

M LP

i∗

[1 + V + E, R]

o⊕

[V, R]

It is useful to take a moment to discuss what was just achieved: with a single abstract template (the
polynomial span), we have successfully explained both a dynamic programming algorithm, and a
GNN update rule—merely by choosing the correct support sets and latent space.

7

Improving GNNs with edge updates, with experimental evaluation

From now on, we will set E = V 2, as all our baseline GNNs will use fully connected graphs, and it
will accentuate the polynomial nature of our construction.

7

We now show how our polynomial span view can be used to directly propose better-aligned GNN
architectures for certain algorithmic tasks. Since the MPNN diagram above outputs only node
features, to improve predictive performance on edge-centric algorithms, it is a natural augmentation
to also update edge features, by adding edges to the output carrier (as done by, e.g., [1]):

V 2 + (V 2 + V 2) + V 2

p

i

V 2

o

1 + V + V 2

V + V 2

(7)

But notice that there is a problem with the output arrow. Since we are using each message twice, o is
no longer a function—it’d have to send each edge message to two different objects! To resolve this,
we need to appropriately augment the messages and the arguments. This is equivalent to specifying a
new polynomial span with output V 2, which we can then recombine with Diagram 7:

p

?

i

1 + V + V 2

?

o

V 2

(8)

Most edge-centric algorithms of interest (such as the Floyd-Warshall algorithm for all-pairs shortest
paths [14]), compute edge-level outputs by reducing over a choice of “intermediate” node. Hence, it
would be beneﬁcial to produce messages with shape V 3, which would then reduce to features over
V 2. There are three possible ways to broadcast both node and edge features into V 3, so we propose
the following polynomial span, which materialises each of those arguments:

V 3 + (V 3 + V 3 + V 3) + (V 3 + V 3 + V 3)

p

i

1 + V + V 2

V 3

o

V 2

(9)

Finally, inserting this into Diagram 7 gives us a corrected polynomial span with output V + V 2:

4V 2 + 7V 3

p

V 2 + V 3

i

o

(10)

1 + V + V 2

V + V 2

Here we have collapsed the copies of V 2 and V 3 in the argument position for compactness.

While Diagram 7 doesn’t make sense as a polynomial diagram of sets, we can clearly still implement
it as an architecture [1], since nothing stops us from sending the same tensor to two places. We
want to investigate whether our proposed modiﬁcation of Diagram 10, which materialises order-
3 messages, leads to improved algorithmic alignment on edge-centric algorithms. To support this
evaluation, we initially use a set of six tasks from the recently proposed CLRS Algorithmic Reasoning
Benchmark [32], which evaluates how well various (G)NNs align to classical algorithms, both in-
and out-of-distribution. We reuse exactly the data generation and base model implementations in the
publicly available code for the CLRS benchmark.

8

We implemented each of these options by making our GNN’s message and update functions be
two-layer MLPs with embedding dimension 24, and hidden layers of size 8 and 16. Our test results
(out-of-distribution) are summarised in Table 1. For convenience, we also illustrate the in-distribution
performance of our models via plots given in Appendix E.

Lastly, we scale up our experiments to 27 different tasks in CLRS, 96-dimensional embeddings, and
using the PGN processor [29], which is the current state-of-the-art model on CLRS in terms of task
win count [32]. We summarise the performance improvement obtained by our V 3 variant of PGN in
Table 2, aggregated across edge-centric tasks as well as ones that do not require explicit edge-level
reasoning. For convenience, we provide the per-task test performance in Appendix F (Table 3).
We found that the V 3 architecture was equivalent to, or outperformed, the non-polynomial (V 2) one
in all edge-centric algorithms (up to standard error). Additionally, this architecture appears to also
provide some gains on tasks without explicit edge-level reasoning requirements, albeit smaller on
average and less consistently. Our result directly validates our theory’s predictions, in the context of
presenting a better-aligned GNN for edge-centric algorithmic targets.

Table 1: Test (out-of-distribution) results of our models on all models on the six algorithms studied.
V 2 corresponds to the baseline model offered by Diagram 7, while V 3 corresponds to our proposal
in Diagram 10, which respects the polynomial span.

Algorithm

V 2-large

V 3-large

V 2-small

V 3-small

Dijkstra
Find Maximum Subarray
Floyd-Warshall
Insertion Sort
Matrix Chain Order
Optimal BST

Overall average

2.82
0.50
0.63
1.27
1.23
2.80

59.58%
8.33%
7.46%
15.39%
67.64%
53.03%

±
±
±
±
±
±
35.24%

68.53%
9.06%
9.00%
24.67%
70.79%
54.56%

2.40
0.65
0.81
2.44
1.54
4.34

±
±
±
±
±
±

39.43%

3.25
0.55
0.62
1.32
2.26
3.82

56.10%
8.46%
6.66%
14.69%
68.85%
46.65%

±
±
±
±
±
±
33.57%

2.70
0.64
0.62
2.21
1.21
4.60

60.32%
7.89%
8.23%
20.23%
68.76%
51.94%

±
±
±
±
±
±
36.23%

Table 2: Test (out-of-distribution) results across 27 tasks in CLRS, for the PGN processor network,
averaged across edge-centric and other tasks. See Appendix F for the per-task test performances.

Algorithms

V 2–PGN V 3–PGN Average Improvement

Edge-centric algorithms
Other algorithms

35.03%
35.37%

39.08%
36.33%

Average of the two groups

35.20%

37.70%

1.06
0.11

4.44%
1.01%

±
±
2.73%

8 Conclusions

In this paper, we describe the use of category theory and abstract algebra to explicitly expand on the
GNN-DP connection, which was previously largely handwaved on speciﬁc examples. We derived
a generic diagram of an integral transform (based on standard categorical concepts like pullback,
pushforward and commutative monoids), and argued why it is general enough to support both GNN
and DP computations. With this diagram materialised, we were able to immediately unify large
quantities of prior work as simply manipulating one arrow or element in the integral transform. We
also provided empirical evidence of the utility of polynomial spans for analysing GNN architectures,
especially in terms of algorithmic alignment. It is our hope that our ﬁndings inspire future research
into better-aligned neural algorithmic reasoners, especially focusing on generalising or diving into
several aspects of this diagram.

Lastly, it is not at all unlikely that analyses similar to ours have already been used to describe other
ﬁelds of science—beyond algorithmic reasoners. The principal ideas of span and integral transform
are central to deﬁning Fourier series [35], and appear in the analysis of Yang-Mills equations in
particle physics [13]. Properly understanding the common ground behind all of these deﬁnitions may,
in the very least, lead to interesting connections, and a shared understanding between the various
ﬁelds they span.

9

Acknowledgments and Disclosure of Funding

We would like to thank Charles Blundell, Tai-Danae Bradley, Taco Cohen, Bruno Gavranovi´c, Bogdan
Georgiev, Razvan Pascanu, Karolis Špukas, Grzegorz ´Swirszcz, and Vincent Wang-Ma´scianica for
the very useful discussions and feedback on prior versions of this work.

Special thanks to Tamara von Glehn for key comments helping us to formally connect integral
transforms to polynomial functors.

This research was funded by DeepMind.

References

[1] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius
Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan
Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint
arXiv:1806.01261, 2018.

[2] Richard Bellman. On a routing problem. Quarterly of applied mathematics, 16(1):87–90, 1958.

[3] Richard Bellman. Dynamic programming. Science, 153(3731):34–37, 1966.

[4] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for
graph classiﬁcation extrapolations. In International Conference on Machine Learning, pages
837–851. PMLR, 2021.

[5] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veliˇckovi´c. Geometric deep learning:

Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021.

[6] Francesca Cagliari and Sandra Mantovani. Cartesianness: topological spaces, uniform spaces,

and afﬁne schemes. Topology and its Applications, 41:263–272, 1991.

[7] Quentin Cappart, Didier Chételat, Elias Khalil, Andrea Lodi, Christopher Morris, and Petar
Veliˇckovi´c. Combinatorial optimization and reasoning with graph neural networks. arXiv
preprint arXiv:2102.09544, 2021.

[8] Eugenia Cheng. Iterated distributive laws. In Mathematical Proceedings of the Cambridge

Philosophical Society, volume 150, pages 459–487. Cambridge University Press, 2011.

[9] Pim de Haan, Taco S Cohen, and Max Welling. Natural graph networks. Advances in Neural

Information Processing Systems, 33:3636–3646, 2020.

[10] Oege De Moor. Categories, relations and dynamic programming. Mathematical Structures in

Computer Science, 4(1):33–69, 1994.

[11] Andreea-Ioana Deac, Petar Veliˇckovi´c, Ognjen Milinkovic, Pierre-Luc Bacon, Jian Tang, and
Mladen Nikolic. Neural algorithmic reasoners are implicit planners. Advances in Neural
Information Processing Systems, 34, 2021.

[12] Andrew Dudzik. Quantales and hyperstructures: Monads, mo’problems. arXiv preprint

arXiv:1707.09227, 2017.

[13] Michael G Eastwood, Roger Penrose, and RO Wells. Cohomology and massless ﬁelds. Commun.

Math. Phys, 78(3):305–351, 1981.

[14] Robert W Floyd. Algorithm 97: shortest path. Communications of the ACM, 5(6):345, 1962.

[15] Brendan Fong and David I Spivak. An invitation to applied category theory: seven sketches in

compositionality. Cambridge University Press, 2019.

[16] Karlis Freivalds, Em¯ıls Ozolin, š, and Agris Šostaks. Neural shufﬂe-exchange networks-sequence
processing in o (n log n) time. Advances in Neural Information Processing Systems, 32, 2019.

[17] Nicola Gambino and Joachim Kock. Polynomial functors and polynomial monads. In Math-
ematical proceedings of the cambridge philosophical society, volume 154, pages 153–192.
Cambridge University Press, 2013.

[18] Jeffrey Giansiracusa and Noah Giansiracusa. Equations of tropical varieties. Duke Mathematical

Journal, 165(18):3379–3433, 2016.

10

[19] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International conference on machine learning,
pages 1263–1272. PMLR, 2017.

[20] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. Advances in neural information processing systems, 31,
2018.

[21] Vinod Nair, Sergey Bartunov, Felix Gimeno, Ingrid von Glehn, Pawel Lichocki, Ivan Lobov,
Brendan O’Donoghue, Nicolas Sonnerat, Christian Tjandraatmadja, Pengming Wang, et al.
Solving mixed integer programs using neural networks. arXiv preprint arXiv:2012.13349, 2020.
[22] Susan Nieﬁeld. Cartesianness: topological spaces, uniform spaces, and afﬁne schemes. J. Pure

Appl. Alg., 23:147–163, 1982.

[23] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and
Peter Battaglia. Learning to simulate complex physics with graph networks. In International
Conference on Machine Learning, pages 8459–8468. PMLR, 2020.

[24] Heiko Strathmann, Mohammadamin Barekatain, Charles Blundell, and Petar Veliˇckovi´c. Persis-

tent message passing. arXiv preprint arXiv:2103.01043, 2021.

[25] Daisuke Tambara. On multiplicative transfer. Communications in Algebra, 21(4):1393–1420,

1993.

[26] Hao Tang, Zhiao Huang, Jiayuan Gu, Bao-Liang Lu, and Hao Su. Towards scale-invariant
graph-related problem solving by iterative homogeneous gnns. Advances in Neural Information
Processing Systems, 33:15811–15822, 2020.

[27] Petar Veliˇckovi´c and Charles Blundell. Neural algorithmic reasoning. Patterns, 2(7):100273,

2021.

[28] Petar Veliˇckovi´c, Matko Bošnjak, Thomas Kipf, Alexander Lerchner, Raia Hadsell, Raz-
van Pascanu, and Charles Blundell. Reasoning-modulated representations. arXiv preprint
arXiv:2107.08881, 2021.

[29] Petar Veliˇckovi´c, Lars Buesing, Matthew Overlan, Razvan Pascanu, Oriol Vinyals, and Charles
Blundell. Pointer graph networks. Advances in Neural Information Processing Systems,
33:2232–2244, 2020.

[30] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua

Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.

[31] Petar Veliˇckovi´c, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. Neural

execution of graph algorithms. arXiv preprint arXiv:1910.10593, 2019.

[32] Petar Veliˇckovi´c, Adrià Puigdomènech Badia, David Budden, Razvan Pascanu, Andrea Ban-
ino, Misha Dashevskiy, Raia Hadsell, and Charles Blundell. The clrs algorithmic reasoning
benchmark. arXiv preprint arXiv:2205.15659, 2022.

[33] Philip Wadler. Comprehending monads. In Proceedings of the 1990 ACM Conference on LISP

and Functional Programming, pages 61–78, 1990.

[34] Mark Weber. Polynomials in categories with pullbacks. Theory and Applications of Categories,

30(16):533–598, 2015.

[35] Simon Willerton. Integral transforms and the pull-push perspective, i. The n-Category Café,

2020.

[36] Louis-Pascal Xhonneux, Andreea-Ioana Deac, Petar Veliˇckovi´c, and Jian Tang. How to transfer
algorithmic reasoning knowledge to learn new algorithms? Advances in Neural Information
Processing Systems, 34, 2021.

[37] Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S Du, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. What can neural networks reason about? arXiv preprint arXiv:1905.13211, 2019.
[38] Keyulu Xu, Mozhi Zhang, Jingling Li, Simon S Du, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. How neural networks extrapolate: From feedforward to graph neural networks. arXiv
preprint arXiv:2009.11848, 2020.

[39] Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. Neural bellman-ford
networks: A general graph neural network framework for link prediction. Advances in Neural
Information Processing Systems, 34, 2021.

11

Figure 2: Correspondence between the individual arrows in the polynomial span, and the pseudocode
steps for implementing a plausible graph neural network. The code sections are colour-coded to
correspond to arrows in the polynomial span diagram. The speciﬁc sets X, Y, V, W of the polynomial
= n).
span are initialised to match the design choices in the GNN (for a set of nodes

, such that

V

|V|

A Correspondence between GNN pseudocode and the polynomial span

In Figure 2, we further elaborate on the diagrams given in Figure 1, to explicitly relate the various
steps of how processing data with a GNN might proceed with the individual arrows (i, p, o) of the
polynomial span. To do this, we colour-code parts of a plausible GNN pseudocode, to match the
colours of arrows in a polynomial span diagram.

Additionally, in Figure 3, we follow this construction to explicitly provide the pseudocodes for the
proposed V 2 and V 3-GNN models (as proposed in Diagram 7 and Diagram 10, respectively).

B The bag and list monads

Before we conclude, we turn back to the theory behind our polynomial spans, to more precisely
determine the restrictions on our abstract latent space R. We found this investigation useful to include
in the main paper, as it yields a strong connection to one of the most actively used concepts in
theoretical computer science and functional programming.

Recall that the realisation of our pushforward operations required the existence of two aggregators:
(to reduce bags). Previously, we mentioned only in passing how they can be

(to fold lists) and

(cid:76)

axiomatically.

recovered—now, we proceed to deﬁne
(cid:78)
, the natural-valued functions
Given a set S, we deﬁne bag(S) :=
|
→
of ﬁnite support on S. This has a clear correspondence to multisets over S: p sends each element of
S to the amount of times it appears in the multiset. We can write its elements formally as
S nss,
∈
where all but ﬁnitely many of the ns are nonzero.
Given a function f : S
as follows: bag(f )(

T between sets, we can deﬁne a function bag(f ) : bag(S)
S nsf (s), which we can write as

(cid:80)
bag(T ),
→
T mtt, where mt =

p : S
(cid:76)
{

p(r)
{

= 0
}

∞}

#

<

N

s

s

s

t

→
S nss) :=
∈

∈

∈

(cid:80)

(cid:80)
For each S, we can also deﬁne two special functions. The ﬁrst is unit : S
(cid:80)
S to the multiset
each element to its indicator function (i.e. an element x
join : bag(bag(S))

→
x
{{
bag(S), which interprets a nested sum as a single sum.

(cid:80)

∈

}}

bag(S), sending
). The second is

f (s)=t ns.

These facts tell us that bag is a monad, a special kind of self-transformation of the category of sets.
Monads are very general tools for computation, used heavily in functional programming languages
(e.g. Haskell) to model the semantics of wrapped or enriched types. Monads provide a clean way for
abstracting control ﬂow, as well as gracefully handling functions with side effects [33].

→

It is well-known that the algebras for the monad bag are the commutative monoids, sets equipped
with a commutative and associative binary operation and a unit element.

12

Require:NodefeaturesX∈Rn×k,Messagefunctionψ:Rk×Rk→Rm,Updatefunctionϕ:Rm→RmEnsure:LatentfeaturesH∈Rn×mArgsnd←tile(X,0,n);//Argsnd∈Rn×n×kArgrcv←tile(X,1,n);//Argrcv∈Rn×n×kfor(u,v)∈V×Vdomsguv←ψ(argsndu,argrcvv);//Msg∈Rn×n×mendforforu∈Vdohu←ϕ(Lv∈Vmsgvu);endforVV×V+V×VV×VVipo(cid:54)
Figure 3: Correspondence between the arrows in the polynomial span, and the pseudocode for
implementing the GNNs represented by Diagram 7 (above) and Diagram 10 (below). Edge and
graph features are ignored for simpicity. The code sections are colour-coded to correspond to arrows
in the polynomial span. Note the difference to Figure 2: we now also need to output edge features
(on

2).

V

Concretely, a commutative monoid structure on a set R is equivalent to deﬁning an aggregator
function
R compatible with the unit and monad composition. Here, compatibility
implies it should correctly handle sums of singletons and sums of sums, in the sense that the following
two diagrams commute; that is, they yield the same result regardless of which path is taken:

: bag(R)

(cid:76)

→

R

unit

bag(R)

bag(bag(R))

bag((cid:76))

bag(R)

id

(cid:76)

R

join

bag(R)

(cid:76)

(cid:76)

R

The ﬁrst diagram explains that the outcome of aggregating a singleton multiset (i.e. the one produced
is equivalent to the original value placed in the singleton. The second
by applying unit) with
diagram indicates that the
operator yields the same results over a nested multiset, regardless of
whether we choose to directly apply it twice (once on each level of nesting), or ﬁrst perform the join
function to collapse the nested multiset, then aggregate the collapsed multiset with

(cid:76)

(cid:76)

.

13

(cid:76)

Require:NodefeaturesX∈Rn×k,Messagefunctionψ:Rk×Rk→Rm,Updatefunctionϕ:Rm→RmEnsure:LatentfeaturesH∈Rn×m(nodes),M∈Rn×n×m(edges)Argsnd←tile(X,0,n);//Argsnd∈Rn×n×kArgrcv←tile(X,1,n);//Argrcv∈Rn×n×kfor(u,v)∈V×Vdomsguv←ψ(argsndu,argrcvv);//Msg∈Rn×n×mendforforu∈Vdohu←ϕ(Lv∈Vmsgvu);endforM←Msg//Msgissenttotwoplaces(H,M);outputmorphismoisnotafunction!V2V2V2V+V2ipoRequire:NodefeaturesX∈Rn×k,Edgemessagefunctionψ(e):Rk×Rk→Rm,Tripletmessagefunctionψ(t):Rk×Rk×Rk→Rm,Nodeupdatefunctionϕ(n):Rm→Rm,Edgeupdatefunctionϕ(e):Rm→RmEnsure:LatentfeaturesH∈Rn×m(nodes),M∈Rn×n×m(edges)Argsnd←tile(X,0,n);//Argsnd∈Rn×n×kArgrcv←tile(X,1,n);//Argrcv∈Rn×n×kArgtri1←tile(X,[0,1],n);//Argtri1∈Rn×n×n×kArgtri2←tile(X,[0,2],n);//Argtri2∈Rn×n×n×kArgtri3←tile(X,[1,2],n);//Argtri3∈Rn×n×n×kfor(u,v)∈V×Vdomsgedgeuv←ψ(e)(argsndu,argrcvv);//Msgedge∈Rn×n×mforw∈Vdomsgtriuvw←ψ(t)(argtri1u,argtri2v,argtri3w);//Msgtri∈Rn×n×n×mendforendforforu∈Vdohu←ϕ(n)(Lv∈Vmsgedgevu);forv∈Vdomuv←ϕ(e)(Lw∈Vmsgtriuvw);endforendforV2V2+3V3V2+V3V+V2ipoSo the structure of a commutative monoid on R is exactly what we need to complete our deﬁnition of
. The story for the argument pushforward, p
the message pushforward o

, is remarkably similar.

⊗

S

, the set of all ordered lists of elements of S,
}
0 Sn. We can also extend list to a functor:
n
list(T ) is just the well-known map operation:
(cid:96)

≥

⊕
(s1, . . . , sn)
{

|

n

Deﬁne list(S) :=
N, si ∈
including the empty list. Equivalently, list(S) =
T , list(f ) : list(S)
for a function f : S
→
list(f )(s1, . . . , sn) := (f (s1), . . . , f (sn)).
list is also a monad, with unit : S
join : list(list(S))

∈

→

→

list(S) sending each x
list(S) sending a list of lists to their concatenation.

→

∈

S to the singleton list (x), and

The algebras for the list monads are monoids—not just commutative ones. So R needs a second
monoid structure, possibly noncommutative, to support our deﬁnition of the argument pushforward.
We detail how this can elegantly be done in our speciﬁc case in Appendix C.

C The monad for semirings

We have asked that R be an algebra for two monads: list and bag. But this is an unnatural
condition without some compatibility between the two. It would more useful to ﬁnd a single monad
encapsulating both.

◦

In general, the composition of two monads is not a monad. For example, the composite functor
list

bag does not support a monad structure.

However, the other composite bag
of a distributive law, which is a natural transformation λ : list
axioms, see e.g. [8].4

list is actually a monad in a natural way, due to the existence
list satisfying some

bag

bag

→

◦

◦

◦

is easy to describe λ.
in ain ) =

It
λ(
returns the bag of all ordered selections from the list.

Given any list of bags (
in ain), we have
i1,...,in (ai1 , . . . , ain). In other words, λ takes a list of bags and
(cid:80)

i1 ai1 , . . . ,

i1 ai1 , . . . ,

(cid:80)

(cid:80)

(cid:80)

(cid:80)

This is exactly how multiplication of sums works in a semiring. For example, if I think of a polynomial
as a bag of monomials, and I want to compute a product of polynomials, I interpret this product as a
list of polynomials, i.e. a list of bags. Then I expand it into a bag of lists (a sum of products), and
ﬁnally perform the products to produce the resulting bag of monomials, i.e. polynomial.

So it shouldn’t be a surprise that the algebras for the composite monad bag
semirings, i.e. sets R equipped with a commutative monoid structure

list are exactly
, another monoid structure
, usually written as, e.g. x(a + b)y = xay + xby, and
(cid:76)

◦

, and a “distributive law”

→

extended to arbitrary sums and products by induction.
(cid:78)
Indeed, if R is an algebra for the monad bag
bag(list(R))
→
and we can recover

R. We can recover

: list(R)

: bag(R)

(cid:78) (cid:76)

(cid:76) (cid:78)

⊗

◦

→

⊕

→

list, we have some “double aggregator” ev :
R by packing our list into a singleton bag,

R by packing our bag into a singleton list then applying λ.

D Polynomial functors

Polynomial spans are the starting point for our integral transform, but they are also the starting point
for polynomial functors, which arise in dependent type theory. Let
be a locally cartesian closed
category, and let
, the category of morphisms with target A. A
/A denote, for any object A of
polynomial functor starts with a polynomial span:

C

C

C

X

i

W

p

Y

o

Z

4Cheng [8] also explains the general problem of composing three or more monads and its relation to the

Yang-Baxter equation, which provides further intuition about the unit axioms for semirings.

14

And it produces a composition of three functors:

/X

C

i∗

/W

C

Πp

/Y

C

Σo

/Z

C

(11)

Here Σo and Πp are operations called the dependent sum and dependent product respectively.
Note that there is a direct correspondence between the three arrows in each of the diagrams 5 and 11.
So it is very tempting to ask whether our integral transform is expressible as a polynomial functor.
Can our results be rephrased in those terms?

We don’t have a complete answer, but we can connect the two pictures, at least in the case of
commutative multiplication, via the monoidal category FinPoly, whose objects are ﬁnite sets, whose
morphisms are polynomial diagrams, and whose monoidal product is given by disjoint union +.
A result of Tambara says that FinPoly is the Lawvere theory for commutative semirings [25, 17].
What this means is that the strong monoidal functors F : (FinPoly, +)
) are uniquely
determined by giving a commutative semiring structure on the set F (1).

(Set,

→

×

In other words, once we have decided on a commutative semiring structure on R = [1, R], we
V 1) = [1, R]V = [V, R], and the action of F on morphisms can be
automatially have F (V ) = F (
checked to coincide with our construction of the integral transform.

→

(Cat,

Likewise, we can interpret ﬁnite polynomial functors as the action on the category of categories F :
) with F (1) = FinSet. Note that [V, FinSet] = FinSetV = FinSet/V ,
(FinPoly, +)
as picking one ﬁnite set for each element of V is equivalent to picking a ﬁnite set equipped with a
function to V . So F takes a ﬁnite set V to its slice category FinSet/V , and likewise takes polynomial
diagrams to the associated polynomial functor. In fact, F in this case actually extends to a 2-functor.
Since the 2-categorical structure is important for polynomial functors, it may be useful to explore it
for integral transforms as well.

×

(cid:80)

In any case, we can see that [V, N], where N is the usual natural numbers with addition and multi-
plication, is just a decategoriﬁed version of FinSet/V , obtained by considering only cardinalities.
Indeed, the existence of such a “decategoriﬁcation” for transforms over spans was an early inspiration
for our present work. But what about categorifying other semirings?

R as a classifying morphism for some kind of bundle E

To replace N with an arbitrary semiring R, we would need to ﬁnd a way to interpret a function
W in a suitable category
f : W
of geometric objects over R. For the min-plus semiring R = N∞, one possibility is to deﬁne a
category of R-schemes, which should be certain types of topological spaces equipped with sheaves
of R-modules.

→

→

We don’t know of a place this theory is fully developed, but the spectrum functor from rings to
topological spaces is extended to poset-enriched semirings in [12]. And this construction is certainly
related to tropical schemes, deﬁned in [18]. For R = R, we can also consider the more familiar
category of manifolds, or more generally the category of locally compact Hausdorff spaces.

But do polynomial functors work in categories like this? While polynomial functors were developed
in type theory over locally cartesian closed categories–too strong of a condition for interesting
topology to occur–[34] has shown that polynomial functors can be deﬁned in any category with
Y satisﬁes an abstract condition called
pullbacks, as long as the “processor” morphism p : X
exponentiability. i and o can still be arbitrary morphisms.

→

For some intuition, we quote two results on exponentiability. [6] shows that the exponentiable
morphisms in the category of compact Hausdorff spaces are the local homeomorphisms. And [22]
S of commutative rings gives rise to an exponentiable morphism
shows that a morphism R
of afﬁne schemes exactly when S is dualizable as an R-module. So exponentiability seems to be
strongly linked to covering spaces in classical topology, as well as descent theory in modern algebraic
geometry.

→

15

Expanding on these ideas is far out of scope for the present work, but we hope it gives a glimpse into
the possibilities for future development.

E Plots of in-distribution performance on CLRS

For plots that illustrate in-distribution performance of our proposed V 3 model, against the non-
polynomial (V 2) model, please refer to Figure 4 and Table 4. Our ﬁndings largely mirror the ones
from out-of-distribution—with V 3 either matching the performance of the baseline or signiﬁcantly
outperforming it (e.g. on Insertion Sort and Floyd-Warshall). We do note that sometimes, matched
performance by the non-polynomial V 2 baseline in-distribution can be misleading, as it signiﬁcantly
loses out to V 3 out of distribution (cf. Table 1). This lines up with predicitons of prior art: in-
distribution, many classes of GNNs can properly ﬁt a target function [37], but in order to extrapolate
well, the alignment to the target function needs to be stronger, as otherwise the function learnt by the
model may be highly nonlinear, and therefore less robust out-of-distribution [38].

F Test results for the scaled PGN experiments on CLRS

To supplement the aggregated results provided in Table 2, here we provide the per-task results of our
scaled PGN experiment. Table 3 provides, for each of the 27 CLRS algorithms we investigated here,
the test (out-of-distribution) performance of the PGN model [29], with both the V 2 and V 3 variant.
In all cases, the models compute 96-dimensional embeddings; for memory considerations, the V 2
pipeline computes 128-dimensional latent vectors, the V 3 addition computes 16-dimensional latent
vectors, and these are then all linearly projected to 96 dimensions and combined. We particularly
highlight in Table 3 the edge-centric algorithms within this set, to emphasise our gains on them. An
algorithm is considered edge-centric if it explicitly requires a prediction (either on the algorithm’s
output or its intermediate state) over the given graph’s edges.

16

Table 3: Test (out-of-distribution) results of all PGN variants on all 27 algorithms in our scaled up
experiments, averaged over 8 seeds. Edge-centric algorithms are highlighted in blue. Note that most
of the beneﬁts of our proposed V 3 architecture occur over the edge-centric tasks.

Algorithm

V 2–PGN

V 3–PGN

Activity Selector
Articulation Points
Bellman-Ford
BFS
Binary Search
Bridges
DAG Shortest Paths
DFS
Dijkstra
Find Maximum Subarray
Floyd-Warshall
Graham Scan
Heapsort
Insertion Sort
KMP Matcher
LCS Length
Matrix Chain Order
Minimum
MST-Kruskal
MST-Prim
Naïve String Matcher
Quickselect
Quicksort
Segments Intersect
Strongly Connected Components
Task Scheduling
Topological Sort

Overall average

1.02
4.46
0.87
0.02
2.07
1.68
0.44
0.73
4.13
1.46
1.31
1.77
0.15
0.24
0.11
2.35
1.13
1.82
5.26
3.54
0.59
0.08
0.09
2.15
0.56
1.30
0.56

62.28%
11.91%
80.05%
99.97%
26.20%
26.02%
62.62%
8.70%
34.60%
48.28%
8.01%
37.66%
2.34%
12.14%
2.44%
52.87%
70.94%
58.92%
43.34%
29.05%
2.06%
2.22%
2.45%
61.77%
8.98%
84.36%
12.80%

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
35.30%

1.03
3.69
0.78
0.12
1.95
1.54
0.82
0.95
4.71
1.20
0.92
1.57
0.24
0.98
0.11
4.93
0.92
1.77
6.82
3.78
0.46
0.16
1.01
1.99
2.13
0.63
1.63

63.75%
14.72%
77.69%
99.76%
25.57%
25.48%
62.43%
8.16%
37.51%
52.58%
17.31%
42.08%
4.20%
18.99%
1.59%
67.24%
74.61%
56.54%
38.42%
29.86%
1.80%
2.56%
6.82%
61.24%
11.41%
85.18%
9.91%

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
36.94%

Table 4: Validation (in-distribution) results of all MPNN-based models on all six algorithms studied,
across three random seeds.

Algorithm

V 2–large

V 3–large

V 2–small

V 3–small

Dijkstra
Find Maximum Subarray
Floyd-Warshall
Insertion Sort
Matrix Chain Order
Optimal BST

Overall average

0.46
2.51
0.59
1.96
0.07
0.24

92.03%
81.98%
79.51%
87.48%
97.69%
92.42%

±
±
±
±
±
±
88.52%

92.70%
84.71%
90.02%
87.97%
97.96%
91.61%

0.34
0.93
0.32
1.86
0.06
0.28

±
±
±
±
±
±

90.83%

0.53
1.99
0.67
3.77
0.10
0.46

91.46%
81.91%
78.19%
76.12%
97.59%
91.80%

±
±
±
±
±
±
86.18%

0.49
2.46
0.47
1.68
0.10
0.63

91.54%
76.29%
88.99%
88.84%
97.88%
90.77%

±
±
±
±
±
±
89.05%

17

Figure 4: Validation (in-distribution) curves of all models on all six algorithms studied, across three
random seeds.

18

