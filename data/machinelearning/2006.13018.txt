The classification for High-dimension low-sample size data 

Author: Liran Shen1, Meng Joo Er1, Qingbo Yin2, * 

, 116026, China 

1  College  of  Marine  Electrical  Engineering,  Dalian  Maritime  University,  Dalian, 

116023, China   

2 College of Information Science and Technology, Dalian Maritime University, Dalian, 

116023, China 

*Correspondence and requests for materials should be addressed to Qingbo Yin (Email: 
qingbo@dlmu.edu.cn) 

Copyright  Declaration:  This  work  has  been  submitted  to  the  IEEE  for  possible  publication. 
Copyright may be transferred without notice, after which this version may no longer be accessible. 

1 

 
 
 
 
 
 
 
 
 
 
 
Abstract (no more than 250 words) 

    Huge amount of applications in various fields, such as gene expression analysis or 

computer vision, undergo data sets with high-dimensional low-sample-size (HDLSS), 

which has putted forward great challenges for standard statistical and modern machine 

learning methods. In this paper, we propose a novel classification criterion on HDLSS, 

tolerance similarity, which emphasizes the maximization of within-class  variance on 

the  premise  of  class  separability. According  to  this  criterion,  a  novel  linear  binary 

classifier is designed, denoted by No-separated Data Maximum Dispersion classifier 

(NPDMD). The objective of NPDMD is to find a projecting direction w in which all of 

training  samples  scatter  in  as  large  an  interval  as  possible.  NPDMD  has  several 

characteristics compared to the state-of-the-art classification methods. First, it works 

well  on  HDLSS.  Second,  it  combines  the  sample  statistical  information  and  local 

structural  information  (supporting  vectors)  into  the  objective  function  to  find  the 

solution of projecting direction in the whole feature spaces. Third, it solves the inverse 

of high dimensional matrix in low dimensional space. Fourth, it is relatively simple to 

be  implemented  based  on  Quadratic  Programming.  Fifth,  it  is  robust  to  the  model 

specification for various real applications. The theoretical properties of  NPDMD are 

deduced.  We  conduct  a  series  of  evaluations  on  one  simulated  and  six  real-world 

benchmark data sets, including face classification and mRNA classification. NPDMD 

outperforms those widely used approaches in most cases, or at least obtains comparable 

results. 

2 

 
Keywords:  Binary  linear  classifier,  high-dimensional  low-sample-size,  classification 

criterion 

3 

 
 
 
 
 
1.  Introduction 

With the accumulation of high-dimensional low sample size data (HDLSS) in various 

fields of real-world applications (such as image processing and computer vision, data 

mining,  bioinformatics  and  gene  expression)  (1-4),  classification  on  these  data  is  a 

critically important task and attracts much attention during the last few decades. Here, 

HDLSS indicates that the sample size is less than the feature dimension d (n << d, high-

dimensional  low-sample-size:  HDLSS).  Moreover,  the  performance  of  classical 

statistical  methods  for  classification  on  HDLSS  are  seriously  degraded  (3-6).  The 

HDLSS data has posed significant challenges to standard statistical methods and have 

rendered many existing classification techniques impractical (7). 

    For  HDLSS,  most  study  adopted  the  dimension  reduction  or  regulation  as 

preprocessing before the classification was conducted. These works involve the most 

of  modern  classifiers,  i.e.,  Naïve  Bayes  (NB)  (5),  Logistic  Regression  (LR)(4), 

ensemble methods (6), Linear Discriminant Analysis (LDA) (8), Partial Least Square 

Regression, Mean Difference (MD) (5), neural network (9) and deep learning (10, 11). 

These methods with dimension reduction are time-consuming(12).   

    Meanwhile,  a  few  works  conduct 

the  classification  on  HDLSS  without 

dimensionality  reduction.  Here,  our  work  concentrate  on  the  methods  without 

dimensionality reduction because these methods can be combined with any pre- or post-

processing if necessary. When the sample size n is larger than the feature dimension d, 

linear discriminant analysis(LDA) is a popular and successive method for dimension 

4 

 
reduction and classification, which maximizes the so-called Fisher criterion (Rayleigh 

coefficient) and obtain the projection vector 

, the largest eigenvectors of 

(cid:1)

(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:3)(cid:8)

where 

(cid:3)(cid:8)

  is the between-class scatter matrix and 

  is the within-class scatter matrix. 

(cid:3)(cid:4)

However, LDA is impractical when 

(cid:3)(cid:4)

  is singular due to HDLSS. Li (13) proposed 

the Maximum Margin Criterion (MMC) to improve the formula of linear discriminant 

analysis and avoid solving the inverse of the low rank between-class scatter matrix for 

HDLSS.  Support  Vector  Machine  (SVM)  (14)  is  a  well-known  classifier,  which 

maximize the smallest distance between classes. It can be used directly to any data set, 

no matter what the n is larger or less than d. In the practical applications for HDLSS, a 

phenomenon  of  data  overfitting,  so-called  “data-piling”,  is  often  observed  for  these 

classifiers  (15,  16),  especially  SVM.  Marron  and  Qiao  et  al.  proposed  the  distance-

weighted methods (DWD, wDWD and DWSVM) to improve the SVM in the HDLSS 

setting(16-19). These  distance-weighted  methods  maximize  harmonic  mean between 

classes with heavily computing consumption due to second-order cone programming 

(SOCP), which is more computationally demanding than quadratic programming for 

SVM (20-22). In Ref (23), a new linear binary classifier PGLMC combined the local 

structure  of  the  hyperplane  and  the  global  statistics  information  of  population  with 

lower  computational  complexity  than  the  distance-weighted  methods  owing  to  solve 

the  similar  Quadratic  Programming  formulation  as  SVM.  Although  the  distance-

weighted  methods  and  PGLMC  alleviates  the  data-piling  phenomenon,  it  is  still 

inevitable to suffer from this overfitting issue for HDLSS even. The proofs for this issue 

will be given in the following section.   

5 

 
 
    In  this  paper,  a  novel  linear  binary  classifier  (No-separated  Data  Maximum 

Dispersion  classifier,  NPDMD)  was  proposed  to  work  directly  on  HDLSS  without 

dimensionality  reduction based  on  a  new  discriminant  or  classification  criterion,  the 

tolerance similarity (TS). Geometrically, on the premise of being able to distinguish 

two classes of samples, the farther the samples in each class are in the projection space, 

the better to avoid the data-piling phenomenon. When we look through the formulation 

of LDA, it can be found that data-piling phenomenon is an inevitable result when we 

pursue  to  minimize  intra  class  differences  if  n  is  less  than  d  because  the  projection 

vector 

(cid:1)

  falls in the null space of 

(cid:3)(cid:4)

. However, the principle of tolerance similarity 

avoids data-piling by maximizing intra class differences. In term of tolerance similarity, 

NPDMD  is  designed  to  find  the  hyperplane,  which  can  separate  samples  from  two 

classes and maximize data dispersion in as large an interval as possible. NPDMD holds 

the  same  computational  complexity  as  in  SVM  owing  to  solve  the  similar  Convex 

Quadratic  Programming  formulation.  The  experimental  results  demonstrate  that  our 

method not only addresses the data-piling issue on HDLSS, but also can be applied to 

general data with arbitrary dimensions (or the imbalanced data). Our method is stable; 

no matter the samples are balanced or not across two classes. 

    The rest of this paper is organized as follows. Section 2 briefly introduces the related 

methods in HDLSS, especially SVM and the methods based on Distance Weighting. 

Section  3  elaborates  the  proposed  NPDMD.  Section  4  demonstrates  experimental 

results. Finally, Section 5 concludes the paper. 

6 

 
2. The relative methods and drawbacks 

In this section, we will sketch the reason of data-piling for the related methods without 

dimensional reduction in HDLSS, which involve MMC, SVM, the methods based on 

Distance Weighting and PGLMC. 

    For binary classification problems, a data point in sample space is mapped to a class 

label chosen from 

, 

(cid:9)

(cid:10): (cid:12) → (cid:9)

, where 

(cid:12) ∈ (cid:15)(cid:16)  and 

(cid:9) ∈ (cid:17)+1, −1(cid:22)

. Here, for the sake 

of  simplicity,  all  samples  have  been  centered  with  mean  zero. A  linear  discriminant 

function can be denoted as 

(cid:23)(cid:2)(cid:12)(cid:5) = (cid:2)(cid:1)(cid:25)(cid:12) + (cid:26)(cid:5)

, where the coefficient direction vector 

(cid:1) ∈ (cid:15)(cid:16)   has  unit 

(cid:27)(cid:28)

   norm,  and 

(cid:26) ∈ (cid:15)

   is  the  intercept  term.  Data-piling  mean 

majority of the data lying on two hyper-planes or concentrate on two points after the 

data are projected to a particular direction vector of a classifier [14]. That is to say, for 

the  sample 

 , 

 , 
(cid:1)(cid:25)(cid:12)(cid:29) = 0

(cid:12)(cid:29)

, (cid:31) = 1,2, ⋯ , "

 .  When 

" ≪ $

 ,  there  are 

   nonzero 

$ − "

linear independent solutions for 

(cid:1)

. Therefore, data-piling is a unique phenomenon for 

HDLSS  because  there  is  only  a  zero  solution  for 

   when 

(cid:1)

" ≥ $

   providing  the 

samples are linear independent.   

    MMC is a variant of LDA, and employs the objective function 

)(cid:1)(cid:25)(cid:2)(cid:3)(cid:8) − (cid:3)(cid:4)(cid:5)(cid:1)*

max
(cid:4)

instead of 

(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:3)(cid:8)

  to bypass the singular of 

(cid:3)(cid:4)

  for HDLSS. This method benefits 

from  the  use  of  sample  statistical  information.  But  the  data-piling  is  still  a  plagued 

problem  due  to  original  LDA’  idea,  which  maximize  the  differences  (

 )  between 

(cid:3)(cid:8)

classes and minimize the differences 

(cid:3)(cid:4)

  within classes to find a projection vector 

. 

(cid:1)

This rule is perfect when d << n because there are enough samples to provide similarity 

7 

 
 
and dissimilarity between or within classes. However, when n << d, there are lack of 

samples  to  learn  the  intra-class  differences  if  the  rule  of 

)(cid:1)(cid:25)(cid:3)(cid:4)(cid:1)*

   is  adopted 

min
(cid:4)

because it is almost true that the solution of 

  is in the null space of 

. 

(cid:3)(cid:4)

(cid:1)

    SVM  adopted  another  route,  which  try  to  find  a  hyperplane  with  maximum  gap 

between classes. The hyperplane can be represented by the supporting vectors and the 

projecting direction 

(cid:1)

. In spite of excellent performance in most cases, SVM holds an 

unexpected trend data-piling when 

$ → ∞

  and n is fixed (24). When the samples are 

from independent and identical distribution population 

, and 

.(cid:29)

.(cid:29)(cid:2)/(cid:29), ∑(cid:29)(cid:5)

  is Gaussian, 

we  should  obtain 

123(cid:2)‖(cid:12) − /(cid:29)‖(cid:28)(cid:5) = 2536∑(cid:29)

(cid:28)

 , 
7

(cid:12)

   is  the  sample  in  ith  class.  For 

HDLSS,  all  training  samples  became  supporting  vectors  and  concentrated  on  the 

boundaries for two classes because   

89 =

(cid:28)

:;<

, 
(cid:17)1 + =(cid:2)1(cid:5)(cid:22)

> = 1, ⋯ , "(cid:7)

                                        (1) 

89 =

(cid:28)

:;?

, 
(cid:17)1 + =(cid:2)1(cid:5)(cid:22)

> = "(cid:7) + 1, ⋯ , "

                                  (2) 

Where 

"(cid:29)

  is the number of samples in ith class, 

"(cid:7) + "(cid:28) = "

. 

@ =

AB(cid:2)∑<(cid:5)
;<

+

AB(cid:2)∑?(cid:5)
;?

+

, 

∆

∆= ‖/(cid:7) − /(cid:28)‖(cid:28). Therefore, since SVM merely use the local structural information 

(supporting  vectors)  of  the  training  samples  without  considering  the  statistical 

information of training samples, data-piling is also an inevitable result for HDLSS(12). 

For detailed proofs about this property of SVM on HDLSS, please refer to (24). 

    The methods based on Distance Weighting were developed in recent  years, which 

maximizes the harmonic mean of the distances of all data vectors. While they alleviate 

8 

 
 
 
the data-piling phenomenon or overfitting to a certain extent, there are still a few issues 

(1)  computing  consumption.  The  current  state  of  the  art  implementation  for  these 

methods  is  based  on  second-order  cone  programming  (SOCP),  which  is  more 

computationally  demanding  than  quadratic programming(20).  (2)  Data-piling  cannot 

be completely avoided. For the latter issue, the proof is as follow: 

      For the original objective function of DWD   

argmin
(cid:4),(cid:8)

;
∑ F
(cid:29)K(cid:7)

(cid:7)
BG

+ HI(cid:29)J

                                                        (3) 

subject to 3(cid:29) = (cid:9)(cid:29)(cid:2)(cid:12)(cid:29)

(cid:25)(cid:1) + (cid:26)(cid:5) + I(cid:29)

, 

3(cid:29) ≥ 0, I(cid:29) ≥ 0

,   

‖(cid:1)‖(cid:28) ≤ 1

    For simplicity, we omit the relaxation term 

  when the samples from two classes 

HI(cid:29)

are completely separable. Assuming that 

(cid:27) = ∑ F

;
(cid:29)K(cid:7)

(cid:7)
BG

J ,

(cid:27) ≥ V

. 

, 
∃V ∈ )2, (cid:26)(cid:5)

2

  and 

(cid:26)

are the minimum and maximum of positive real set 

XF

"

3(cid:31)

J ≥ 0, (cid:31) = 1, ⋯ , "Y

. According 

to  the  mean  value  inequality,  it  can  be  true  that 

]

;
V = " ∙ [∏
(cid:29)K(cid:7)

(cid:7)
BG

     and 

V = "

(cid:7)

B^

when 

3_ = 3(cid:7) = 3(cid:28) = ⋯ = 3;

 ,  i.e. 

(cid:27) = "

(cid:7)

   when 

B^

3_ = 3(cid:7) = 3(cid:28) = ⋯ = 3;

 .  Here,  we 

adopted the same limit value 

3_

  for positive and negative classes. However, this does 

not affect the conclusion. Because even if we adopt different limit values 

  and 

3_‘

3_(cid:6)

for positive and negative classes, we will still get similar results of data-piling. 

    The proof is completed. Therefore, DWD is still with the gradual trend of data-piling 

in HDLSS.   

    PGLMC  attempted  to  find  the  projecting  direction 

 ,  on  which  the  distances 

(cid:1)

between the projecting points from two classes are as far as possible and that the gap 

9 

 
 
 
   
 
(minimum distance) between two classes is as large as possible. PGLMC merely uses 

the item 

(cid:2)/(cid:7) − /(cid:28)(cid:5)

  to control the differences between classes, and does not consider 

the intra-class differences. Therefore, as the methods based on Distance Weighting do, 

PGLMC only alleviate the issue of data-piling instead of overcoming it. 

    According to the above description, it can be known that all of the above methods 

only  emphasize  the  difference  between  classes  and  the  similarity  within  a  class,  but 

ignore the difference between samples within a class. In practical applications, when 

the sample size is enough (d << n), these methods have excellent performance. In the 

case of HDLSS, they are biased and not stable because there are plenty of disturbing 

clues  to  meet  the  similarity  criteria 

)(cid:1)(cid:25)(cid:3)(cid:4)(cid:1)*

 ,  which  only  emphasizes  the 

min
(cid:4)

maximization of similarity. 

3. The proposed method 

In the view of consideration of the above-mentioned and other results in the literature 

(25-27),  we  propose  a  new  classification  criterion  on  HDLSS,  tolerance  similarity, 

which involves two rules: (1) Class separability. In theory, there is at least a hyperplane 

to separate clearly the samples for training to classes. (2) The similarity and difference 

of samples within a class must be taken into account. That is to say, the similarity of 

samples within a class has tolerance (difference). It has already known that LDA and 

MMC  leverage 

min
(cid:4)

)(cid:1)(cid:25)(cid:3)(cid:4)(cid:1)*

   to  maximize  the  similarity,  which  discards  the 

difference of intra-class samples and leads to data-piling on HDLSS because this design 

has no tolerance for sample difference within classes. In view of this, on the premise of 
10 

 
 
class separability, 

max
(cid:4)

)(cid:1)(cid:25)(cid:3)(cid:4)(cid:1)*

  is a good choice on HDLSS instead of 

)(cid:1)(cid:25)(cid:3)(cid:4)(cid:1)*

min
(cid:4)

to measure the similarity with tolerance difference. 

    Here, a linear classifier is conceived based on tolerance similarity to maximize the 

dispersion interval of all training samples in the projection space as the gap between 

two classes is maximized. 

  3.1 No-separated Data Maximum Dispersion Linear Classifier 

The  objective  function  of  no-separated  data  maximum  dispersion  linear  classifier 

(NPDMD) is as follow: 

‖(cid:4)‖?
;
(cid:4)abc(cid:4) + Hd ∑ e(cid:29)
(cid:29)K(cid:7) J

min
(cid:4)

F

                                            （4） 

s. t. (cid:9)(cid:29)(cid:2)(cid:1)(cid:25)(cid:12)(cid:29) + (cid:26)(cid:5) ≥ 1 − e(cid:29), (cid:31) = 1,2, ⋯ , "

, 
e(cid:29) = ℓ6(cid:9)(cid:29)(cid:23)(cid:2)(cid:12)(cid:29)(cid:5)7

e(cid:29) ≥ 0

Where   

(cid:3)h = Σ(cid:7) + Σ(cid:28)

                                          （5） 

Σ9 =

(cid:7)
;j

∑

k∈9^lmnn

6(cid:12) − /976(cid:12) − /97

(cid:25)

                              （6） 

According  to  tolerance  similarity,  the  numerator 

‖(cid:1)‖(cid:28)   is  in  charge  of  rule  1  and 

minimized to separate the samples from two classes by maximizing the gap between 

two classes. This is derived from SVM (14). The denominator term 

(cid:1)(cid:25)(cid:3)h(cid:1)

  is for rule 

2 and also maximized to control training samples from two classes as far as possible 

along the projecting direction 

(cid:1)

, and balance the covariance between two classes. The 

11 

 
   
 
 
hinge  loss 

ℓ(cid:2)o(cid:5) = (cid:2)1 − o(cid:5)‘

 ,  where 

(cid:2)o(cid:5)‘ = o

   if 

o ≥ 0

   and  0  others. 

   is  the 

/9

mean of training samples from jth class, 

. 
> = 1,2

To solve (4), the Lagrangian formulation can be written as 

(cid:27)(cid:2)(cid:1), (cid:26), 8(cid:5) =

‖(cid:4)‖?
(cid:4)abc(cid:4) + Hde(cid:25)p + 8(cid:25))p − e − q(cid:2)r(cid:1) + (cid:26)p(cid:5)* − s(cid:25)e

                  (7) 

where 

   is  the 

q

" × "

   diagonal  matrix  with  the  components  of 

   on  its  diagonal; 

(cid:9)(cid:29)

r ∈ (cid:15);×(cid:16), which ith row is sample 

; 

(cid:1) ∈ (cid:15)(cid:16)×(cid:7)  is the projecting vector; 

(cid:12)(cid:29)

p

  is the 

column  vector  1; 

8 = (cid:2)8(cid:7), ⋯ , 8;(cid:5)(cid:25) ∈ (cid:15);×(cid:7) ，

     and 

8(cid:29) > 0

8(cid:29)

 s  are  Lagrangian 

multipliers; 

s = (cid:2)s(cid:7), ⋯ , s;(cid:5)(cid:25) ∈ (cid:15);×(cid:7) ，

s(cid:29) > 0

      and 

s(cid:29)

  s  are  Lagrangian 

multipliers;

 e = (cid:2)e(cid:7), ⋯ , e;(cid:5)(cid:25) ∈ (cid:15);×(cid:7). 

    It  has  been  proven  that 

   and 

(cid:7)
(cid:4)abc(cid:4)

)H − (cid:1)(cid:25)(cid:3)h(cid:1)*

   are  with  same  effect  in  the 

optimization formula(12). The formula (4) can be reformulated to facilitate calculation 

as below: 

(cid:27)(cid:2)(cid:1), (cid:26), 8(cid:5) =

(cid:7)

(cid:28) ‖(cid:1)‖(cid:28) +

(cid:7)

(cid:28) v(cid:2)H − (cid:1)(cid:25)(cid:3)h(cid:1)(cid:5) + 8(cid:25))p − e − q(cid:2)r(cid:1) + (cid:26)p(cid:5)* + Hde(cid:25)p − s(cid:25)e

    (8) 

    By differentiating the Lagrangian formulation with respect to 

, 

(cid:1)

(cid:26)

  and 

w

, we can 

get the following conditions: 

x(cid:27)
x(cid:1)

= (cid:1) − v(cid:3)h(cid:1) − r(cid:25)q(cid:25)8 = 0

(cid:1) = (cid:2)y − v(cid:3)h(cid:5)(cid:6)(cid:7)r(cid:25)q(cid:25)8

                                              (9) 

, 
  z{
zw = Hdp − | − }

Hdp = | + }

                                      (10) 

                                                      (11) 

z{

z(cid:8) = 8(cid:25)qp = 0

12 

 
 
When substituting (9), (10) and (11) into (8), we can obtain the dual form as follow 

(cid:27)(cid:2)8(cid:5) = −

(cid:7)

(cid:28) 8(cid:25)qr(cid:2)y − v(cid:3)h(cid:5)(cid:6)(cid:7)r(cid:25)q(cid:25)8 + 8(cid:25)p +

(cid:7)
(cid:28) vH                  (cid:2)12(cid:5) 

When provided that 

~ = qr(cid:2)y − v(cid:3)h(cid:5)(cid:6)(cid:7)r(cid:25)q(cid:25)                                          (13) 

  is a symmetric positive semidefinite matrix, 

  is a column vector with 1, the 

(cid:127) = p

~

  does not contain variable 

8

  and can be discarded, the above equation (12) 

item  (cid:7)

(cid:28) vH

can be 

(cid:27)(cid:2)8(cid:5) = −

(cid:7)

(cid:28) 8(cid:25)~8 + 8(cid:25)(cid:127)

                                              (14) 

So, the optimization problem (4) can be transformed into the following 

                                                              (15)         

(cid:27)(cid:2)8(cid:5)

argmax
(cid:128)

s. t.  8(cid:25)qp = 0, Hd ≥ 8(cid:29) ≥ 0

    The  above  formula  is  a  classical  quadratic  programming  problem.  From  the 

formulation  (4),  it  can  be  known  that  the  Karush-Kuhn-Tucker  condition  must  be 

satisfied as follow: 

, 
8(cid:29) ≥ 0

s(cid:29) ≥ 0

(cid:9)(cid:29)(cid:2)(cid:1)(cid:25)(cid:12)(cid:29) + (cid:26)(cid:5) − 1 + e(cid:31) ≥ 0

8(cid:29)(cid:129)(cid:9)(cid:29)(cid:2)(cid:1)(cid:25)(cid:12)(cid:29) + (cid:26)(cid:5) − 1 + e(cid:31)(cid:130) = 0

, 

e(cid:29) ≥ 0

s(cid:29)e(cid:31) = 0

    The  formulation  (15)  can  be  regarded  as  a  quadratic  programming  problem  with 
13 

 
 
 
 
 
 
equality  constrains  while  inequality  conditions  are  just  looked  upon  as  to  scale  the 

coefficients 

8(cid:29)

. The intercept term 

(cid:26)

  can be obtained by the Criterion of Minimum 

misclassified samples as follows: 

(cid:131)(cid:2)(cid:26)(cid:5) = ∑ (cid:132)(cid:133)")−1 ∗ (cid:9)(cid:29)(cid:2)(cid:1)(cid:25)(cid:12)(cid:29) + (cid:26)(cid:5)*

(cid:135)
(cid:29)

                                  (16) 

argmin
(cid:8)

(cid:132)(cid:133)"(cid:2)(cid:12)(cid:5) = X

+1,   (cid:12) ≥ 0
−1,   (cid:12) < 0

                                                      (17) 

    Given a new sample 

(cid:12)

, the classifier of NPDMD is defined by 

(cid:9)(cid:137) = (cid:23)(cid:2)(cid:12)(cid:5) = (cid:2)(cid:1)(cid:25)(cid:12) + (cid:26)(cid:5) = )(cid:2)y − v(cid:3)h(cid:5)(cid:6)(cid:7)r(cid:25)q(cid:25)8*(cid:25)(cid:12) + (cid:26)

                      (18) 

    From the above formulation, it can be known that when 

, NPDMD is identical 

v = 0

to SVM. In other words, SVM is a special case of NPDMD. And the concept of support 

vector is still valid, and defined as 

(cid:3) = (cid:17)(cid:12)(cid:29)|(cid:9)(cid:29)(cid:2)(cid:1)(cid:25)(cid:12)(cid:29) + (cid:26)(cid:5) = 1 − e(cid:29) 2"$ 8(cid:29) > 0(cid:22)

. The 

projection vector 

(cid:1)

  is composed of two parts. The first part is about the statistics of 

sample population 

(cid:2)y − v(cid:3)h(cid:5)

, which is in charge of the preferable shape. The second 

part  is  the  sample  matrix 

r(cid:25)q(cid:25)   and  the  weight  vector 

8

 ,  which  represents  the 

selection  of  samples  or  sample  sparsity.  Moreover,  the  term 

(cid:2)y − v(cid:3)h(cid:5)

  ensures  that 

the optimization formula (14) can be solved in the whole feature space.       

The  performance  properties  of  NPDMD  is  deduced  in  three  different  flavors:  (1) 

Fisher  consistency,  (2)  Asymptotic  under  extremely  imbalanced  data,  (3)  High 

Dimension  low  sample  size  asymptotic.  Detailed  information  about  the  theoretical 

properties of NPDMD is discussed in the Supplementary material.                           

3.2 Accelerated Extension of NPDMD 

14 

 
For  formulation  (13),  we  must  calculate  the  inverse  of 

$ × $

   matrix 

(cid:2)y − v(cid:3)h(cid:5)

 , 

which  is  a  time  consuming  problem  for  HDLSS  (

 ).  We  should  simplify  the 

$ ≫ "

involved computation. 

(cid:3)h

  can be decomposed as follow: 

(cid:3)h = Σ(cid:7) + Σ(cid:28) = (cid:140)(cid:25)(cid:141)(cid:140)

                                                            (19) 

(cid:141) = $(cid:31)2(cid:133)(cid:2)(cid:142)

(cid:7)

(cid:7)∗;<

, ⋯ ,

(cid:29)

(cid:29)∗;<

, ⋯ ,

;<
;<∗;<

;<‘(cid:7)
(cid:2);<‘(cid:7)(cid:5)∗;?

,

, ⋯ ,

;<‘;?
(cid:2);<‘;?(cid:5)∗;?

(cid:143)(cid:5)

Where 

   is  a 

(cid:140)

" × $

   matrix,  which  ith  row  is  the  sample 

 .  Then,  it  can  be 

(cid:12)(cid:29) − /9

known 

(cid:2)y − v(cid:3)h(cid:5) = y − v(cid:140)(cid:25)(cid:141)(cid:140)

                                                    (20) 

When we resort to the Sherman-Morrison-Woodbury (SMW) identity (28) 

(cid:2)(cid:144) + (cid:145)H1(cid:5)(cid:6)(cid:7) = (cid:144)(cid:6)(cid:7) − (cid:144)(cid:6)(cid:7)(cid:145)(cid:2)H(cid:6)(cid:7) + 1(cid:144)(cid:6)(cid:7)(cid:145)(cid:5)(cid:6)(cid:7)1(cid:144)(cid:6)(cid:7)                        (21) 

to compute the inverse of 

(cid:2)y − v(cid:3)h(cid:5)

, the following formula is used to transform the 

original matrix to SMW form. 

(cid:2)y − v(cid:3)h(cid:5)(cid:6)(cid:7) = )y + (cid:140)(cid:25)(cid:2)−v(cid:141)(cid:5)(cid:140)*(cid:6)(cid:7)                                      (22) 

Providing 

(cid:144) = y

, 

(cid:145) = (cid:140)(cid:25), 

H = −v(cid:141)

  and 

1 = (cid:140)

(cid:2)y − v(cid:3)h(cid:5)(cid:6)(cid:7) = )y + (cid:140)(cid:25)(cid:2)−v(cid:141)(cid:5)(cid:140)*(cid:6)(cid:7) = y − y(cid:2)(cid:140)(cid:25)(cid:5))(cid:2)−v(cid:141)(cid:5)(cid:6)(cid:7) + (cid:140)(cid:140)(cid:25)*(cid:6)(cid:7)(cid:140)

(cid:2)y − v(cid:3)h(cid:5)(cid:6)(cid:7) = y − (cid:140)(cid:25))(cid:2)−v(cid:141)(cid:5)(cid:6)(cid:7) + (cid:140)(cid:140)(cid:25)*(cid:6)(cid:7)(cid:140)

                            (23) 

    From  Equation  (23),  it  can  be  noticed  that  both  of 

   and 

(cid:141)

(cid:140)(cid:140)(cid:25)   are  the 

" × "

matrix. Furthermore, 

$ × $

  matrix 

(cid:2)y − v(cid:3)h(cid:5)(cid:6)(cid:7)  can be calculated by the inverse of 

   matrix 

" × "

)(cid:2)−v(cid:141)(cid:5)(cid:6)(cid:7) + (cid:140)(cid:140)(cid:25)*(cid:6)(cid:7) .  For  HDLSS  (

15 

$ ≫ "

 ),  the  computation  cost  of 

 
                     
           
 
 
 
(cid:2)y − v(cid:3)h(cid:5)(cid:6)(cid:7)  can be notably reduced.   

3.3 Computation Complexity 

The computation complexity for the objective function of NPDMD involve two parts 

(1) 

$ × $

  matrix 

(cid:2)y − v(cid:3)h(cid:5)(cid:6)(cid:7). (2) Quadratic Programming formulation for Equation 

(15).   

      For 

$ × $

  positive semidefinite matrix, the 

$

  pairs of eigenvalue and eigenvector 

can  be  computed  in 

=(cid:2)$ ∗ $(cid:5)

  time  (29)  (30).  In  the  HDLSS  case,  with 

,  we 

$ ≫ "

adopt the accelerated extension of NPDMD to reduce the computation cost to 

=(cid:2)" ∗

  time.   

"(cid:5)

    Convex quadratic programming (QP) can be solved in polynomial time with either 

the ellipsoid or interior point method. QP’s running time is 

=6"(cid:7) (cid:28)⁄ 7

  iterations, each 

iteration requiring 

=(cid:2)"(cid:147)(cid:5)

  arithmetic operations on integers (31).   

    For SOCP with efficient primal-dual interior point method, it requires is 

=6"(cid:7) (cid:28)⁄ 7

iterations, each requiring 

=(cid:2)"(cid:28)(cid:148)2(cid:12)(cid:17)", $(cid:22)(cid:5)

  operations (16, 32).  In the HDLSS case, 

with 

$ ≫ "

,  the  computation  cost  for  DWD  would be greater  than  in  the  SVM  and 

NPDMD.     

4. Experimental results 

To evaluate the proposed NPDMD algorithm, in this section, we perform a series of 

experiments  systematically  on  both  simulation  data  and  real-world  classification 

problems. First, we present one synthetic data set for clearly comparing NPDMD with 
16 

 
 
 
DWD,  wDWD  SVM  and  PGLMC.  Second,  on  real-world  problems,  six  data  sets 

depicted in Ref (33-39) are used to evaluate the classification accuracies.   

In  the  following  experiments,  to  eliminate  the  dependence  of  the  results  on  the 

particular training data used, some measures are defined and the average (or mean) of 

these measures are reported, which are obtained for different randomly sample splits. 

And the programs were developed in MATLAB and R, and executed in Inter I7-9700 

Processor  3.6G  Hz  system  with  64GB  RAM.  For  the  methods  based  on  Distance 

weighting, we adopt the linear binary implementations in R package ‘kerndwd’(20). 

4.1 Measures of Performance 

In  order  to  evaluate  the  performance  of  NPDMD  and  compare  it  fairly  with  other 

methods,  we  adopted  general performance  measures  (confusion  matrix,  ROC  curve) 

and specific measures for HDLSS (correct classification rate: CCR, mean within-group 

error: MWE, and angle), which were used in reference (23, 25). In general, the Bayer 

rule  classifier  was  as  the  benchmark  in  HDLSS  for  comparison.  Here,  the  Bayes 

direction  (25),  which  serves  as  the  benchmark  to  be  compared  with,  is  only  theory 

direction. In the below simulation setting, we suppose that data are sampled from two 

multivariate  normal  distribution  with  different  mean  vectors  (

  and 

s‘

s(cid:6)

)  and  same 

covariance matrices 

Σ

. We can get the following Bayes rule for simulation dataset 

(cid:132)(cid:31)(cid:133)"6(cid:12)(cid:25)(cid:1)(cid:149)(cid:150)(cid:151)(cid:152)(cid:153) + (cid:26)(cid:149)(cid:150)(cid:151)(cid:152)(cid:153)7

                                          (23) 

(cid:1)(cid:149)(cid:150)(cid:151)(cid:152)(cid:153) = Σ(cid:6)(cid:7)(cid:2)s‘ − s(cid:6)(cid:5)

17 

 
 
     
(cid:26)(cid:149)(cid:150)(cid:151)(cid:152)(cid:153) = −

1
2

(cid:1)(cid:25)(cid:2)s‘ + s(cid:6)(cid:5)

    Since the theory distribution of samples cannot be known in real applications, 

Bayes rule is only adopted in simulation experiment to analyze the experimental results. 

CCR is a measure of classification performance when the balanced data for two classes 

is involved. MWE is a measure while the imbalanced data are involved. When data is 

balanced, 

HH(cid:15) = 1 − (cid:154)(cid:155)(cid:156)

 .  Angle  is  measured  by  the  difference  between  the 

estimated  discrimination  direction 

    and  the  Bayes  rule  direction 

(cid:1)

  : 

(cid:1)(cid:149)(cid:150)(cid:151)(cid:152)(cid:153)

∠6(cid:1), (cid:1)(cid:149)(cid:150)(cid:151)(cid:152)(cid:153)7

. Angle is generally for interpretability performance. The interpretability 

is an uncertain concept and measured by the angle between the discriminant direction 

  of classifier under investigation and of the Bayes classifier (23). Generally speaking, 

(cid:1)

it is reasonable that the closer to the Bayes rule direction, the better the interpretability 

is (25).     

4.2 Simulations Data: Experiment 1 

A  simulated  data  set  is  used  to  compare  the  classification  and  interpretability 

performance among the NPDMD, the original SVM, DWD, wDWD and PGLMC. The 

classification performances are measured for a large test data set with 3000 observations 

(1500 for each class). The process is in accordance with the literature (19, 23, 25). 

The  simulation  settings  are  as below:  constant  mean  difference 

,  identity 

s ≡ (cid:159)p(cid:16)

covariance matrix 

, where 

Σ ≡ y(cid:16)

  is a scaling factor with 

(cid:159) > 0

2(cid:159)‖1(cid:16)‖(cid:28) = 2.7

. This 

setting represents the Mahalanobis distance between the two classes and a reasonable 

18 

 
 
 
difficulty for classification according to the Bayes rule. In this setting, all samples from 

two classes are from multivariate normal distributions 

¡(cid:16)(cid:2)±s, Σ(cid:5)

. 

In training stage, the positive class sample size be 120 and the negative class sample 

size be 90.    The imbalance factor 

  is defined as the sample size ratio between 

(cid:148) ≥ 0

the majority class and minority class. Here, 

(cid:148) = 1.33

. We vary the dimension 

  in 

$

{80, 150, 240, 650, 900, 1500, 2400}, thus last five cases correspond to HDLSS setting. 

    In  Figure  1,  we  report  the  comparison  results  of  five  replications  among  DWD, 

wDWD, SVM，PGLMC and NPDMD. The test data set is with 1500 samples in each 

class.  The  boxplots  in  Figure  1(a)  are  about  the  scatter  intervals  of  the  correct 

classification  rate  for  five  methods.  Figure  1(b)  is  the  mean  curve  of  CCR  for  five 

methods. NPDMD is the best one in all dimensions, and the performance of CCR for 

wDWD, SVM and PGLMC gradually tend to be consistent. While the dimensionality 

increases, the performance superiority of CCR for NPDMD becomes more and more 

obvious. Figure 1(c) and (d) are about the mean within-group errors for five methods 

and their mean curves. It is obvious that according to the measure of MWE, NPDMD 

is also the best one. Figure 1(e) is about the mean curves of angle differences between 

the estimated discrimination direction 

(cid:1)

  and the Bayes rule direction for five methods. 

NPDMD is still optimal, and all methods are gradually becoming consistent while the 

dimensionality increases.   

19 

 
                                      (a)                                                                        (b) 

                                      (c)                                                                      (d) 

                                                                        (e) 
Figure 1. Comparison among five methods for simulation experiment 1 with 5 replications. (a) 

The boxplots of CCRs. (b) The mean curve of CCRs. (c) The boxplot for the mean within-group 

error.  (d) The  mean  curve  of  MWE.  (e) The  mean  curves  of  angle  differences  between  the 

estimated discrimination direction and the Bayes rule direction. 

    In Figure 1, provided that all of these measures are analyzed simultaneously, it can 
20 

 
 
 
 
 
 
be  found  that  NPDMD  gradually  obtains  the  performance  superiority  of  CCR  and 

WME as the dimension increases. The plots of projecting the samples with different 

dimensions  to  five  different  discriminant  directions  (DWD,  wDWD,  SVM,  PGLMC 

and NPDMD) are in Supplementary Material (from Figure S1 to Figure S7). NPDMD 

fits  simulation  data  distribution  well  and  the  data  projection  maintains  the  Gaussian 

pattern, which implies that there is some potential to interpret the data and generalize 

this model to new data in terms of NPDMD direction. 

4.3 Real applications 

In this subsection, we demonstrate the performance of NPDMD which are compared 

with the competing classifiers on six real data sets. These data sets include a face image 

data  set  EYaleB  (http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html)  and 

five gene expression data sets (Alon data set, Shipp data set, Gordon data set, Chowdary 

data set and Borovecki data set) (https://github.com/ramhiser/datamicroarray).   

    The characteristics of the data sets used in the experiments are summarized in Table 

1. The data dimensions range from 1024 to 22283. Here, when the number of samples 

in the data sets are much fewer than the dimensionality of each samples (i.e. n << d), 

the data set is definitely with high dimension. Thus, we report ROC, confusion matrix, 

mean  CCR  and  MWE  as  the  results.  For  each  data  set,  we  report  detailed  data 

preparation, evaluation methods and comparison results. Because there are more classes 

than  2  in  the  EYaleB  data  set,  we  iterate  to  select  one  class  as  positive  and  the  rest 

classes as negative in the experiments for binary classification. 

21 

 
Table 1. Characteristics of the Data Sets Used in the Experiments 

Type 

Data Set 

Dim 

Class  Examples 

Comments 

Image Data 

EYaleB 

High Dim 

Alon   

Shipp 

1024 

2000 

6817 

Gordon 

12533 

Chowdary 

22283 

Borovecki 

22283 

38 

2414  Medium sample size 

2 

2 

2 

2 

2 

62 

Small sample size 

58 

Small sample size 

181  Small sample size 

104  Small sample size 

31 

Small sample size 

4.3.1 Experiment 2: EYaleB data set 

In  the  task  of  face  recognition,  many  classes  and  numerous  of  image  features  (high 

dimension) are always involved. The EYaleB data set (1) consists of 2414 images for 

38  individuals.  For  each  individual,  there  are  about  64  near  frontal  images  under 

different illumination conditions with diverse expressions. All images are cropped and 

resized to 

  pixels as done in (34). The original gray values in the images are 

32 × 32

used as the features in this experiment. Therefore, the feature vectors for a sample in 

each class is with 1024 dimensions.   

    We split all specimens to eleven folds, in which one-fold is for training and the rest 

ten folds for testing. Parameters for each method are tuned via 3-fold cross-validation 

within training data. This process is repeated for 4 times. The CCRs, MWEs and ROC 

curve are exhibited in Fig. 2. 

22 

 
 
 
 
 
(a)                                                                (b) 

(c) 
Figure 2. Comparison between five methods on EYaleB data. (a) The boxplots of CCRs. (b) 

The boxplot for the mean within-group error. (c) ROC curves and AUC. 

As  shown  in  Fig.2, NPDMD  obtains  the best  CCR  and AUC.  wDWD  obtains  the 

lowest  MWE  while  imbalanced  factor 

   for  each  individual.  The  confusion 

(cid:148) = 37

matrix is shown in Table 2. 

Table 2. The confusion matrix on EYaleB dataset 

Method 

Background   

CCR (%) 

True 

False 

Classification 

Total CCR 

1-MWE 

(%) 

(%) 

DWD 

True 

3570459 

94125 

97.43 

False 

53424 

45638 

46.07 

96.08 

71.75 

wDWD 

True 

3553006 

19714 

99.45 

98.49 

81.33 

23 

 
 
 
 
SVM 

PGLMC 

NPDMD 

False 

35528 

61032 

63.21 

True 

3558003 

14717 

99.59 

False 

40047 

56513 

58.53 

True 

3559721  12999 

99.64 

False 

42384 

54176 

56.11 

True 

3569401 

3319 

99.91 

False 

49895 

46665 

48.33 

98.51 

79.06 

98.49 

77.87 

98.55 

74.12 

4.3.2 Experiment 3: Alon data set 

Alon data set (35) contains gene expression levels of 40 tumour and 22 normal colon 

tissues for 6500 human genes, which were obtained with an Affymetrix oligonucleotide 

array. According to Alon setting (35), the 2000 genes with the highest minimal intensity 

across the samples were reserved for classification. 

    There  are  40  and  22  samples  for  two  classes  with  imbalance  factor  m≈1.82.  All 

specimens were splitted to five folds, in which four-folds are used for training and one 

fold  for  testing.  Parameters  for  each  method  are  adjusted  via  4-fold  cross-validation 

within training data. This process is repeated for 20 times. The CCRs, MWEs and ROC 

curve are exhibited in Fig. 3. 

24 

 
 
 
 
                                      (a)                                                                        (b) 

(c) 
Figure 3. Comparison between five methods on Alon data. (a) The boxplots of CCRs. (b) The 

boxplot for the mean within-group error. (c) ROC curves and AUC. 

Table 3. The confusion matrix on Alon dataset 

Method 

Background   

CCR (%) 

True 

False 

Classification 

Total CCR 

1-MWE 

(%) 

(%) 

DWD 

wDWD 

SVM 

PGLMC 

NPDMD 

True 

False 

True 

False 

True 

False 

True 

False 

True 

False 

661 

132 

658 

131 

544 

290 

705 

136 

702 

122 

147 

81.81 

313 

70.34 

150 

81.44 

314 

70.56 

256 

68.00 

150 

34.09 

95 

88.12 

304 

69.09 

98 

87.75 

318 

72.27 

77.73 

76.07 

77.57 

76.00 

55.97 

51.05 

81.37 

78.61 

82.26 

80.01 

As shown in Figure 3, in this data set, NPDMD is the best one than other methods 

(DWD, wDWD, SVM and PGLMC) on both CCR and MWE. The confusion matrix is 

shown in Table 3.   

25 

 
 
4.3.3 Experiment 4: Shipp data set 

In Shipp data set (36), there are 58 diffuse large B-cell lymphomas (DLBCLs) patient 

samples  for  6,817  gene  expression  levels  with  customized  cDNA  ('lymphochip') 

microarrays, which include 32 positive and 26 negative samples with imbalance factor 

m≈3.05. All samples were separated to five folds, in which 4-folds are for training and 

one  fold  for  testing.  Parameters  for  each  method  are  rectified  via  four-fold  cross-

validation within training data. We repeated this process for 10 times. The CCRs, ROC 

and MWEs are exhibited in Fig. 4. 

(a)                                                              (b) 

(c) 
Figure 4. Comparison between five methods on Shipp dataset. (a) The boxplots of CCRs. (b) 

The boxplot for the mean within-group error. (c) ROC curves and AUC. 

26 

 
   
 
 
    As exhibited in Figure 4, NPDMD gets the best CCR and AUC than other methods 

(DWD, wDWD, PGLMC and SVM), especially superior to SVM and PGLMC. DWD 

and wDWD obtains the lowest MWE. The corresponding confusion matrix is presented 

in Table 4. 

Table 4. The confusion matrix on Shipp dataset 

Method 

Background   

CCR (%) 

True 

False 

Classification 

Total CCR 

1-MWE 

(%) 

(%) 

DWD 

wDWD 

SVM 

PGLMC 

NPDMD 

True 

False 

True 

False 

True 

False 

True 

False 

True 

False 

181 

27 

181 

27 

148 

62 

189 

175 

167 

9 

95.26 

553 

95.34 

9 

95.26 

553 

95.34 

42 

77.89 

518 

89.31 

1 

99.47 

405 

69.83 

23 

87.89 

95.32 

95.30 

95.32 

95.30 

86.49 

83.60 

77.14 

84.65 

7 

573 

98.79 

96.10 

93.34 

4.3.4 Experiment 5: Gordon data set 

Gordon data set(37) contains 31 tissue samples with malignant pleural mesothelioma 

(MPM)  and  150  tissue  samples  with  adenocarcinoma  (ADCA)  of  Lung  microarray 

study  for  the  expression  of  12533  genes,  which  were  obtained  assayed  using  U95A 

oligonucleotide probe arrays (Affymetrix, Santa Clara, CA).       

    There are 31 and 150 specimens for two classes with the imbalance factor 

. 
(cid:148) ≈ 4.84

All specimens are splited to five folds, in which 4-folds are for training and one fold 

27 

 
 
for testing. Parameters for each method are adjusted via 4-fold cross-validation within 

training  data. We  repeat  this  process  for  16  times. The  CCRs,  ROC  and  MWEs  are 

exhibited in Fig. 5. 

                                        (a)                                                                    (b) 

(c) 
Figure 5. Comparison between five methods on Gordon dataset. (a) The boxplots of CCRs. (b) 

The boxplot for the mean within-group error. (c) ROC curves and AUC. 

    As shown in Figure 5, in this data set, NPDMD is the best one than other methods, 

especially superior to SVM. NPDMD have almost perfect performances. The confusion 

matrix is in Table 5. 

Table 5. The confusion matrix on Gordon dataset 

Method 

Background   

Classification 

CCR (%) 

1-MWE 

28 

 
 
 
 
True 

False 

Total CCR 

(%) 

DWD 

wDWD 

SVM 

PGLMC 

NPDMD 

True 

False 

True 

False 

True 

False 

True 

False 

True 

False 

463 

33 

93.35 

15 

2385 

99.38 

467 

29 

94.15 

17 

2383 

99.29 

397 

569 

483 

311 

473 

99 

80.04 

1831 

76.29 

13 

97.38 

2089 

87.04 

23 

95.36 

8 

2392 

99.67 

(%) 

98.34 

96.36 

98.41 

96.72 

76.93 

78.17 

88.81 

92.21 

98.93 

97.51 

4.3.5 Experiment 6: Chowdary data set 

In this experiment, there are 62 tissues with breast tumor and 42 tissues with colon. The 

samples were assayed using U133A GeneChips (Affymetrix) and data on the expression 

of 22283 genes (Affymetrix probes) are available (38).   

    The sample size is 62 and 42 for 2 classes with imbalance factor 

(cid:148) ≈ 1.48

. We split 

all specimens to six folds, in which one-fold is for training and the rest five folds for 

testing.  Parameters  for  each  method  are  adjusted  via  3-fold  cross-validation  within 

training data. This process is repeated for 15 times. The CCRs, ROC curve and MWEs 

are exhibited in Fig. 6. 

29 

 
 
(a)                                                                      (b) 

(c) 
Figure 6. Comparison between five methods on Chowdary dataset. (a) The boxplots of CCRs. 

(b) The boxplot for the mean within-group error. (c) ROC curves and AUC. 

    As shown in Figure 6, in this data set, NPDMD is the best one than other methods, 

especially superior to SVM. The confusion matrix is in Table 6. 

Table 6. The confusion matrix on Chowdary dataset 

Method 

Background   

CCR (%) 

Classification 

DWD 

wDWD 

True 

False 

True 

False 

True 

False 

2619 

531 

83.14 

185 

4465 

96.02 

2621 

529 

83.21 

188 

4462 

95.96 

30 

Total CCR 

1-MWE 

(%) 

(%) 

90.82 

89.58 

90.81 

89.58 

 
 
 
 
SVM 

PGLMC 

NPDMD 

True 

False 

True 

False 

True 

False 

2294 

856 

72.83 

1115 

3535 

76.02 

2522 

628 

80.06 

292 

4358 

93.72 

2587 

563 

82.13 

134 

4516 

97.12 

74.73 

74.42 

88.21 

86.89 

91.06 

89.63 

4.3.6 Experiment 7: Borovecki data set 

This data set contains 17 blood samples with Huntington's disease (HD) and 14 blood 

samples  with  matched  controls  for  22283  genes,  which  were  assayed  using  U133A 

GeneChips (Affymetrix) (39).   

    There  are  17  and  14  samples  for  2  classes  with  imbalance  factor 

(cid:148) ≈ 1.21

 . All 

specimens were splited to five folds, in which 4-folds are for training and one fold for 

testing. Parameters for each method are tuned via 4-fold cross-validation within training 

data. We repeated this process for 15 times. The CCRs, ROC curve and MWEs are in 

Fig. 7. 

                                      (a)                                                                    (b) 

31 

 
 
 
 
(c) 

Figure 7. Comparison between five methods on Borovecki dataset. (a) The boxplots of CCRs. 

(b) The boxplot for the mean within-group error. (c) ROC curves and AUC. 

    As shown in Figure 7, in this data set, NPDMD is the best one than other methods, 

especially superior to SVM. The confusion matrix is in Table 7. 

Table 7. The confusion matrix on Borovecki dataset 

Method 

Background   

CCR (%) 

Classification 

DWD 

wDWD 

SVM 

PGLMC 

NPDMD 

True 

False 

True 

False 

True 

False 

True 

False 

True 

False 

True 

False 

240 

15 

94.12 

2 

208 

99.05 

240 

2 

176 

64 

186 

59 

240 

15 

28 

79 

94.12 

99.05 

69.02 

146 

69.52 

69 

72.94 

151 

71.90 

15 

94.12 

0 

210 

100 

32 

Total CCR 

1-MWE 

(%) 

(%) 

96.34 

96.58 

96.34 

96.58 

69.25 

69.27 

72.47 

72.42 

96.77 

97.06 

 
 
4.4 Comprehensive analysis 

In this section, we extend the comparisons on the above six data sets with some classical 

methods such as LR(4, 40) , AdaBoost (41, 42), MD(43) and MMC(13, 44), which were 

ever  studied  for  HDLSS.  In  order  to be  able  to  compare  on  a  fair benchmark,  these 

classical methods also don’t consider dimension reduction as pre-processing. For sake 

of  a  detailed  analysis,  we  summarize  the  experimental  results  in  Table  8.  We  have 

marked the best results in red. It is demonstrated that NPDMD holds the highest CCR 

on all of these datasets and best MWE on most of data sets. wDWD gets the best MWE 

in experiment 2 and 4. Compared with the current methods (DWD, wDWD, SVM and 

PGLMC), the typical classical methods (LR, AdaBoost, MD and MMC) do not perform 

well in most cases. It seems that the CCRs of Adaboost and MMC on EYaleB is close 

or comparable to those of the current methods. However, when we review MWE, it can 

be found that Adaboost and MMC have little discrimination ability because their MWEs 

are equal or approximate to 50%, which means that these methods cannot distinguish 

the  samples  from  two  classes.  NPDMD  shows  the  best  performance  on  HDLSS. 

However, if there are many classes in datasets, the performance of NPDMD is disturbed 

on MWE as experiment 2 except that CCR is still best. It is possible that 

(cid:1)(cid:25)(cid:3)h(cid:1)

  is 

biased because one class is selected as positive and the rest classes as negative in the 

experiments. Anyway, NPDMD is the best one, and holds more obvious advantage with 

the higher dimension of data sets. 

33 

 
 
Table 8. The comprehensive comparison on 6 real datasets 

Data 

CCR of Methods (%) 

1-MWE (%) 

Experiment 

Dim  Classes  LR  AdaBoost  MD  MMC  DWD  wDWD  SVM  PGLMC  NPDMD  LR  AdaBost  MD  MMC  DWD  wDWD  SVM  PGLMC  NPDMD 

Experiment 2 1024 

38 

60.34 

96.7 

67.87  97.37  96.08 

98.49 

98.51 

98.49 

98.55 

56.76  53.05 

57.59  50.00  71.75 

81.33 

79.08 

77.87 

74.12 

Experiment 3 2000 

2 

61.55 

64.62 

69.01  51.43  77.73 

77.57 

55.97 

81.37 

82.26 

60.99  50.00 

71.61  48.81  76.07 

76.00 

51.05 

78.61 

80.01 

Experiment 4 6817 

2 

74.97 

75.33 

79.42  69.88  95.32 

95.32 

86.49 

77.14 

96.10 

74.61  50.00 

80.14  51.25  95.30 

95.30 

83.60 

84.65 

93.34 

Experiment 5 12533 

2 

82.14 

82.88 

93.57  82.57  98.34 

98.41 

76.93 

88.81 

98.93 

76.56  50.00 

85.26  54.22  96.36 

96.72 

78.17 

92.21 

97.51 

Experiment 6 22283 

2 

82.33 

59.61 

71.56  63.88  90.82 

90.81 

74.73 

88.21 

91.06 

81.42  50.00 

65.34  56.10  89.58 

89.58 

74.42 

86.89 

89.63 

Experiment 7 22283 

2 

82.06 

54.76 

69.59  79.87  96.34 

96.34 

69.24 

72.47 

96.77 

81.11  50.00 

70.06  79.11  96.58 

96.58 

69.27 

72.42 

97.06 

5. Conclusion 

After  analyzing  the  causes  of  data-piling  HDLSS,  we  propose  a  novel  classification 

criterion  on  HDLSS,  tolerance  similarity. This  criterion  stands  for  the  maximization 

intra-class  samples  difference  rather  than  the  minimization  of  intra-class  samples 

difference.  In  the  light  of  this  criterion,  we  proposed  a  new  linear  binary  classifier 

NPDMD,  which  maximize  the  dispersion  interval  of  all  training  samples  in  the 

projection  space  when  the  gap  between  two  classes  is  maximized.  Thanks  to  this 

structural design, our method avoids data-piling and exhibits superior performance in 

HDLSS. From the above experimental results, it can be found that compared with other 

methods, NPDMD is the best one and holds more obvious advantage with the higher 

dimension  of  data  sets.  The  theoretical  properties  of  NPDMD  are  deduced,  which 

accounts for the performance in the experiments. 

34 

 
 
The major advantages of this study were four-fold. First, it works well on HDLSS. 

Second, it combines the sample statistical information and local structural information 

(supporting  vectors)  into  the  objective  function  to  find  the  solution  of  projecting 

direction in the whole feature spaces. Third, it solves the inverse of high dimensional 

matrix in low dimensional space. Fourth, it is relatively simple to be implemented based 

on Quadratic Programming. Fifth, it is robust to the model specification for various real 

applications.  The  experiment  results  demonstrated  the  superiority  of  NPDMD. And 

then, NPDMD is with great potential in many applications regardless of HDLSS.   

References 

1. 

Q. Cheng, H. Zhou, J. Cheng, H. Li, A Minimax Framework for Classification with Applications 

to  Images  and  High  Dimensional  Data.  IEEE  Transactions  on  Pattern  Analysis  &  Machine 

2. 

3. 

4. 

5. 

6. 

7. 

8. 

9. 

Intelligence 36, 2117-2130 (2014). 

Qingbo Yin et al., Integrative radiomics expression predicts molecular subtypes of primary clear 

cell renal cell carcinoma Clin Radiol 73, 782-791 (2018). 

L.-F. Chen, H.-Y. M. Liao, M.-T. Ko, J.-C. Lin, G.-J. Yu, A new LDA-based face recognition 

system which can solve the small sample size problem. Pattern Recogn 33, 1713-1726 (2000). 

F.  S.  Kurnaz,  I.  Hoffmann,  P.  Filzmoser,  Robust  and  sparse  estimation  methods  for  high-

dimensional linear and logistic regression. Chemometr Intell Lab 172, 211-222 (2018). 

A.  Bolivar-Cime,  J.  S.  Marron,  Comparison  of  binary  discrimination  methods  for  high 

dimension low sample size data. J Multivariate Anal 115, 108-121 (2013). 

L. Rokach, Ensemble-based classifiers. Artificial Intelligence Review 33, 1-39 (2010). 

J.  IM,  T.  DM,  Statistical  challenges  of  high-dimensional  data.  Philosophical  Transactions: 

Mathematical, Physical and Engineering Sciences 367, 4237-4253 (2009). 

A. Sharma, K. K. Paliwal, Linear discriminant analysis for the small sample size problem: an 

overview. International Journal of Machine Learning and Cybernetics 6, 443-454 (2015). 

S. Abpeikar,  M.  Ghatee,  G.  L.  Foresti,  C.  Micheloni, Adaptive  neural  tree  exploiting  expert 

nodes to classify high-dimensional data. Neural Networks 124, 20-38 (2020). 

10. 

G. E. Hinton, R. R. Salakhutdinov, Reducing the Dimensionality of Data with Neural Networks. 

Science 313, 504-507 (2006). 

11. 

Y. LeCun, Y. Bengio, G. Hinton, Deep learning. Nature 521, 436-444 (2015). 

35 

 
 
 
12. 

L. Shen, Q. Yin, Data maximum dispersion classifier in projection space for high-dimension 

low-sample-size  problems.  Knowl-Based  Syst  https://doi.org/10.1016/j.knosys.2019.105420, 

105420 (2020). 

13. 

H. Li, T. Jiang, K. Zhang, Efficient and robust feature extraction by maximum margin criterion. 

Ieee T Neural Networ 17, 157-165 (2006). 

14. 

15. 

C. Cortes, V. Vapnik, Support-vector networks. Mach Learn 20, 273-297 (1995). 

J. Ahn, J. S. Marron, The maximal data piling direction for discrimination. Biometrika 97, 254-

259 (2010). 

16. 

J. S. Marron, M. J. Todd, J. Ahn, Distance-weighted discrimination. J Am Stat Assoc 102, 1267-

1271 (2007). 

17. 

B. X. Wang, H. Zou, Sparse Distance Weighted Discrimination. J Comput Graph Stat 25, 826-

838 (2016). 

18. 

X. Y.  Qiao,  H.  H.  Zhang, Y.  F.  Liu,  M.  J. Todd,  J.  S.  Marron, Weighted  Distance Weighted 

Discrimination and Its Asymptotic Properties. J Am Stat Assoc 105, 401-414 (2010). 

19. 

X. Y. Qiao, L. S. Zhang, Distance-weighted Support Vector Machine. Stat Interface 8, 331-345 

(2015). 

20. 

B. X. Wang, H. Zou, Another look at distance-weighted discrimination. J Roy Stat Soc B 80, 

177-198 (2018). 

21. 

J. Friedman, T. Hastie, R. Tibshirani, Regularization Paths for Generalized Linear Models via 

Coordinate Descent. J Stat Softw 33, 1-22 (2010). 

22. 

R. H. Tutuncu, K. C. Toh, M. J. Todd, Solving semidefinite-quadratic-linear programs using 

SDPT3. Math Program 95, 189-217 (2003). 

23. 

Q.  Yin,  E.  Adeli,  L.  Shen,  D.  Shen,  Population-guided  large  margin  classifier  for  high-

dimension low-sample-size problems. Pattern Recogn 97, 107030 (2020). 

24. 

Y. Nakayama, K. Yata, M. Aoshima, Support vector machine and its bias correction in high-

dimension, low-sample-size settings. Journal of Statistical Planning & Inference 191 (2017). 

25. 

X.  Y.  Qiao,  L.  S.  Zhang,  Flexible  High-Dimensional  Classification  Machines  and  Their 

Asymptotic Properties. Journal of Machine Learning Research 16, 1547-1572 (2015). 

26. 

P. Hall, Y. Pittelkow, M. Ghosh, Theoretical measures of relative performance of classifiers for 

high dimensional data with small sample sizes. J Roy Stat Soc B 70, 159-173 (2008). 

27. 

P. Hall, J. S. Marron, A. Neeman, Geometric Representation of High Dimension, Low Sample 

Size Data. Journal of the Royal Statistical Society 67, 427-444 (2005). 

28. 

G. H. Golub, C. F. V. Loan, Matrix Computations (Johns Hopkins University Press, ed. 3rd ed, 

1996). 

29. 

I. S. D. a. B. N. Parlett, Orthogonal Eigenvectors and Relative Gaps. SIAM Journal on Matrix 

Analysis and Applications 25, 858-899 (2003). 

30. 

I.  S.  Dhillon,  B.  N.  Parlett,  Multiple  representations  to  compute  orthogonal  eigenvectors  of 

symmetric tridiagonal matrices. Linear Algebra and its Applications 387, 1-28 (2004). 

31. 

S. A. Vavasis, "Complexity theory: quadratic programming" in Encyclopedia of Optimization, 

C. A. Floudas, P. M. Pardalos, Eds. (Springer US, Boston, MA, 2001), 10.1007/0-306-48332-

7_65, pp. 304-307. 

32. 

33. 

F. Alizadeh, D. Goldfarb, Second-order cone programming. Math Program 95, 3-51 (2003). 

Q. Yin  et  al., Associations  between  Tumor  Vascularity,  Vascular  Endothelial  Growth  Factor 

36 

 
Expression and PET/MRI Radiomic Signatures in Primary Clear-Cell–Renal-Cell-Carcinoma: 

Proof-of-Concept Study. Sci Rep-Uk 7, 43356 (2017). 

34. 

A. S. Georghiades, P. N. Belhumeur, D. J. Kriegman, From Few to Many: Illumination Cone 

Models for Face Recognition under Variable Lighting and Pose. IEEE Transactions on Pattern 

Analysis & Machine Intelligence 23, 643-660 (2001). 

35. 

U. Alon, . et al., Broad patterns of gene expression revealed by clustering analysis of tumor and 

normal colon tissues probed by oligonucleotide arrays. Proceedings of the National Academy 

of Sciences of the United States of America 96, 6745-6750 (1999). 

36. 

M. A.  Shipp  et  al.,  Diffuse  large  B-cell  lymphoma  outcome  prediction  by  gene-expression 

profiling and supervised machine learning. Nat Med 8, 68-74 (2002). 

37. 

G. J. Gordon et al., Translation of microarray data into clinically relevant cancer diagnostic tests 

using  gene  expression  ratios  in  lung  cancer  and  mesothelioma.  Cancer  Res  62,  4963-4967 

(2002). 

38. 

D. Chowdary, J. Lathrop, J. Skelton, Prognostic gene expression signatures can be measured in 

tissues collected in RNAlater preservative. Journal of Molecular Diagnostics 8, 31-39 (2006). 

39. 

F. Borovecki, . et al., Genome-wide expression profiling of human blood reveals biomarkers for 

Huntington's disease. Proceedings of the National Academy of Sciences of the United States of 

America 102, 11023-11028 (2005). 

40. 

E.  Makalic,  D.  F.  Schmidt  (2011)  Review  of  Modern  Logistic  Regression  Methods  with 

Application to Small and Medium Sample Size Problems.    (Springer Berlin Heidelberg, Berlin, 

Heidelberg), pp 213-222. 

41. 

R. Blagus, L. Lusa, Boosting for high-dimensional two-class prediction. Bmc Bioinformatics 

16, 300 (2015). 

42. 

H. Seibold, C. Bernau, A.-L. Boulesteix, R. De Bin, On the choice and influence of the number 

of  boosting  steps  for  high-dimensional  linear  Cox-models.  Computation  Stat  33,  1195-1215 

(2018). 

43. 

L.  Zhang, X. Lin, Some considerations of classification for high dimension low-sample  size 

data. Statistical Methods in Medical Research 22, 537 (2013). 

44. 

Q. Wang et al., Adaptive maximum margin analysis for image recognition. Pattern Recogn 61, 

339-347 (2017). 

Figure Legends 

Figure 1. Comparison among five methods for simulation experiment 1 with 5 replications. 

Figure 2. Comparison between five methods on EYaleB data. 

Figure 3. Comparison between five methods on Alon data. 

Figure 4. Comparison between five methods on Shipp dataset. 

37 

 
Figure 5. Comparison between five methods on Gordon dataset. 

Figure 6. Comparison between five methods on Chowdary dataset. 

Figure 7. Comparison between five methods on Borovecki dataset. 

38 

 
 
 
