A Direct ˜O(1/(cid:15)) Iteration Parallel Algorithm for Optimal Transport

Arun Jambulapati∗
Stanford University
jmblpati@stanford.edu

Aaron Sidford†
Stanford University
sidford@stanford.edu

Kevin Tian‡
Stanford University
kjtian@stanford.edu

Abstract

Optimal transportation, or computing the Wasserstein or “earth mover’s” distance between
two n-dimensional distributions, is a fundamental primitive which arises in many learning and
statistical settings. We give an algorithm which solves the problem to additive (cid:15) accuracy with
˜O(1/(cid:15)) parallel depth and ˜O (cid:0)n2/(cid:15)(cid:1) work. [BJKS18, Qua19] obtained this runtime through re-
ductions to positive linear programming and matrix scaling. However, these reduction-based
algorithms use subroutines which may be impractical due to requiring solvers for second-order it-
erations (matrix scaling) or non-parallelizability (positive LP). Our methods match the previous-
best work bounds by [BJKS18, Qua19] while either improving parallelization or removing the
need for linear system solves, and improve upon the previous best ﬁrst-order methods running
in time ˜O(min(n2/(cid:15)2, n2.5/(cid:15))) [DGK18, LHJ19]. We obtain our results by a primal-dual extra-
gradient method, motivated by recent theoretical improvements to maximum ﬂow [She17].

9
1
0
2

n
u
J

3

]
S
D
.
s
c
[

1
v
8
1
6
0
0
.
6
0
9
1
:
v
i
X
r
a

∗This material is based on work supported by NSF Graduate Fellowship DGE-114747.
†This material is based on work supported by NSF CAREER Award CCF-1844855.
‡This material is based on work supported by NSF Graduate Fellowship DGE-1656518.

 
 
 
 
 
 
1 Introduction

Optimal transport is playing an increasingly important role as a subroutine in tasks arising in
machine learning [ACB17], computer vision [BvdPPH11, SdGP+15], robust optimization [EK18,
BK17], and statistics [PZ16]. Given these applications for large scale learning, designing algorithms
for eﬃciently approximately solving the problem has been the subject of extensive recent research
[Cut13, AWR17, GCPB16, CK18, DGK18, LHJ19, BJKS18, Qua19].
Given two vectors r and c in the n-dimensional probability simplex ∆n and a cost matrix C ∈ Rn×n
≥0
the optimal transportation problem is

1,

min
X∈Ur,c

(cid:104)C, X(cid:105), where Ur,c

def=

(cid:110)

X ∈ Rn×n

(cid:111)
≥0 , X1 = r, X (cid:62)1 = c

.

(1)

This problem arises from deﬁning the Wasserstein or Earth mover’s distance between discrete
probability measures r and c, as the cheapest coupling between the distributions, where the cost
of the coupling X ∈ Ur,c is (cid:104)C, X(cid:105). If r and c are viewed as distributions of masses placed on n
points in some space (typically metric), the Wasserstein distance is the cheapest way to move mass
to transform r into c. In (1), X represents the transport plan (Xij is the amount moved from ri to
cj) and C represents the cost of movement (Cij is the cost of moving mass from ri to cj).
Throughout, the value of (1) is denoted OPT. We call ˆX ∈ Ur,c an (cid:15)-approximate transportation
plan if (cid:104)C, ˆX(cid:105) ≤ OPT + (cid:15). Our goal is to design an eﬃcient algorithm to produce such a ˆX.

1.1 Our Contributions

Our main contribution is an algorithm running in ˜O((cid:107)C(cid:107)max/(cid:15)) parallelelizable iterations2 and
˜O(n2(cid:107)C(cid:107)max/(cid:15)) total work producing an (cid:15)-approximate transport plan.
Matching runtimes were given in the recent work of [BJKS18, Qua19]. Their runtimes were obtained
via reductions to matrix scaling and positive linear programming, each well-studied problems in
theoretical computer science. However, the matrix scaling algorithm is a second-order Newton-
type method which makes calls to structured linear system solvers, and the positive LP algorithm
is not parallelizable (i.e. has depth polynomial in dimension). These features potentially limit the
practicality of these algorithms. The key remaining open question this paper addresses is, is there an
eﬃcient ﬁrst-order, parallelizable algorithm for approximating optimal transport? We answer this
aﬃrmatively and give an eﬃcient, parallelizable primal-dual ﬁrst-order method; the only additional
overhead is a scheme for implementing steps, incurring roughly an additional log (cid:15)−1 factor.

Our approach heavily leverages the recent improvement to the maximum ﬂow problem, and more
broadly two-player games on a simplex ((cid:96)1 ball) and a box ((cid:96)∞ ball), due to the breakthrough
work of [She17]. First, we recast (1) as a minimax game between a box and a simplex, proving
correctness via a rounding procedure known in the optimal transport literature. Second, we show
how to adapt the dual extrapolation scheme under the weaker convergence requirements of area-
convexity, following [She17], to obtain an approximate minimizer to our primal-dual objective in
the stated runtime. En route, we slightly simplify analysis in [She17] and relate it more closely to
the existing extragradient literature.

Finally, we give preliminary experimental evidence showing our algorithm can be practical, and
highlight some open directions in bridging the gap between theory and practice of our method, as

1Similarly to earlier works, we focus on square matrices; generalizations to rectangular matrices are straightforward.
2Our iterations consist of vector operations and matrix-vector products, which are easily parallelizable. Through-

out (cid:107)C(cid:107)max is the largest entry of C.

1

well as accelerated gradient schemes [DGK18, LHJ19] and Sinkhorn iteration.

1.2 Previous Work

Optimal Transport. The problem of giving eﬃcient algorithms to ﬁnd (cid:15)-approximate transport
plans ˆX which run in nearly linear time3 has been addressed by a line of recent work, starting with
[Cut13] and improved upon in [GCPB16, AWR17, DGK18, LHJ19, BJKS18, Qua19]. We brieﬂy
discuss their approaches here.

Works by [Cut13, AWR17] studied the Sinkhorn algorithm, an alternating minimization scheme.
Regularizing (1) with an η−1 multiple of entropy and computing the dual, we arrive at the problem

min
x,y∈Rn

1(cid:62)BηC(x, y)1 − r(cid:62)x − c(cid:62)y where BηC(x, y)ij = exi+yj −ηCij .

This problem is equivalent to computing diagonal scalings X and Y for M = exp(−ηC) such that
XM Y has row sums r and column sums c. The Sinkhorn iteration alternates ﬁxing the row sums
and the column sums by left and right scaling by diagonal matrices until an approximation of such
scalings is found, or equivalently until XM Y is close to being in Ur,c.
As shown in [AWR17], we can round the resulting almost-transportation plan to a transportation
plan which lies in Ur,c in linear time, losing at most 2(cid:107)C(cid:107)max((cid:107)X1 − r(cid:107)1 + (cid:13)
(cid:13)1) in the
objective. Further, [AWR17] showed that ˜O((cid:107)C(cid:107)3
max/(cid:15)3) iterations of this scheme suﬃced to obtain
a matrix which (cid:15)/(cid:107)C(cid:107)max-approximately meets the demands in (cid:96)1 with good objective value, by
analyzing it as an instance of mirror descent with an entropic regularizer. The same work pro-
posed an alternative algorithm, Greenkhorn, based on greedy coordinate descent. [DGK18, LHJ19]
max/(cid:15)2(cid:1) work, suﬃce for both
showed that ˜O (cid:0)(cid:107)C(cid:107)2
Sinkhorn and Greenkhorn, the current state-of-the-art for this line of analysis.

max/(cid:15)2(cid:1) iterations, corresponding to ˜O (cid:0)n2(cid:107)C(cid:107)2

(cid:13)X (cid:62)1 − c(cid:13)

An alternative approach based on ﬁrst-order methods was studied by [DGK18, LHJ19]. These works
considered minimizing an entropy-regularized Equation 1; the resulting weighted softmax function
is prevalent in the literature on approximate linear programming [Nes05], and has found similar
applications in near-linear algorithms for maximum ﬂow [She13, KLOS14, ST18] and positive linear
programming [You01, AO15]. An unaccelerated algorithm, viewable as (cid:96)∞ gradient descent, was
analyzed in [DGK18] and ran in ˜O((cid:107)C(cid:107)max/(cid:15)2) iterations. Further, an accelerated algorithm was
discussed, for which the authors claimed an ˜O(n1/4(cid:107)C(cid:107)0.5
[LHJ19] showed
that the algorithm had an additional dependence on a parameter as bad as n1/4, roughly due to
a gap between the (cid:96)2 and (cid:96)∞ norms. Thus, the state of the art runtime in this line is the better
of ˜O (cid:0)n2.5(cid:107)C(cid:107)0.5
max/(cid:15)(cid:1), ˜O (cid:0)n2(cid:107)C(cid:107)max/(cid:15)2(cid:1) operations. The dependence on dimension of the former
of these runtimes matches that of the linear programming solver of [LS14, LS15], which obtain a
polylogarithmic dependence on (cid:15)−1, rather than a polynomial dependence; thus, the question of
obtaining an accelerated (cid:15)−1 dependence without worse dimension dependence remained open.

max/(cid:15)) iteration count.

This was partially settled in [BJKS18, Qua19], which studied the relationship of optimal trans-
port to fundamental algorithmic problems in theoretical computer science, namely positive linear
programming and matrix scaling, for which signiﬁcantly-improved runtimes have been recently ob-
tained [AO15, ZLdOW17, CMTV17]. In particular, they showed that optimal transport could be
reduced to instances of either of these objectives, for which ˜O ((cid:107)C(cid:107)max/(cid:15)) iterations, each of which
required linear O(n2) work, suﬃced. However, both of these reductions are based on black-box
methods for which practical implementations are not known; furthermore, in the case of positive

3We use “nearly linear” to describe complexities which have an n2polylog(n) dependence on the dimension (where

the size of input C is n2), and polynomial dependence on (cid:107)C(cid:107)max , (cid:15)−1.

2

Approach

1st-order Parallel

Year

2015
2017-19
2018
2018-19
2018
2018-19
2019

Author

[LS15]
[AWR17]
[DGK18]
[LHJ19]
[BJKS18]
[BJKS18, Qua19]
This work

Complexity
˜O(n2.5)

˜O(n2(cid:107)C(cid:107)2
max/(cid:15)2)
˜O(n2(cid:107)C(cid:107)max/(cid:15)2)
˜O(n2.5(cid:107)C(cid:107)0.5
max/(cid:15))
˜O(n2(cid:107)C(cid:107)max/(cid:15))
˜O(n2(cid:107)C(cid:107)max/(cid:15))
˜O(n2(cid:107)C(cid:107)max/(cid:15)) Dual extrapolation

Interior point
Sink/Greenkhorn
Gradient descent
Acceleration
Matrix scaling
Positive LP

No
Yes
Yes
Yes
No
Yes
Yes

No
Yes
Yes
Yes
Yes
No
Yes

Table 1: Optimal transport algorithms. Algorithms using second-order information use potentially-
expensive SDD system solvers; the runtime analysis of Sink/Greenkhorn is due to [DGK18, LHJ19].

linear programming a parallel ˜O(1/(cid:15))-iteration algorithm is not known. [BJKS18] also showed any
polynomial improvement to the runtime of our paper in the dependence on either (cid:15) or n would
result in maximum-cardinality bipartite matching in dense graphs faster than ˜O(n2.5) without fast
matrix multiplication [San09], a fundamental open problem unresolved for almost 50 years [HK73].

Specializations of the transportation problem to (cid:96)p metric spaces or arising from geometric settings
have been studied [SA12, AS14, ANOY14]. These specialized approaches seem fundamentally
diﬀerent than those concerning the more general transportation problem.

Finally, we note recent work [ABRW18] showed the promise of using the Nystrm method for low-
rank approximations to achieve speedup in theory and practice for transport problems arising from
speciﬁc metrics. We ﬁnd it interesting to combine our method with these improvements, and believe
that as our method is based on matrix-vector operations, it is amenable to similar speedups.

Remark. During the revision process for this work, an independent result [LMR19] was published
to arXiv, obtaining improved runtimes for optimal transport via a combinatorial algorithm. The
work obtains a runtime of ˜O(n2(cid:107)C(cid:107)max/(cid:15) + n(cid:107)C(cid:107)2
max/(cid:15)2), which is worse than our runtime by a
low-order term. Furthermore, it does not appear to be parallelizable.

Box-simplex objectives. Our main result follows from improved algorithms for bilinear minimax
problems over one simplex domain and one box domain developed in [She17]. This fundamental
minimax problem captures (cid:96)1 and (cid:96)∞ regression over a simplex and box respectively, and inspired
the development of conjugate smoothing [Nes05] as well as mirror prox / dual extrapolation [Nem04,
Nes07]. These latter two approaches are extragradient methods (using two gradient operations per
iteration rather than one) for approximately solving a family of problems, which includes convex
minimization and ﬁnding a saddle point to a convex-concave function. These methods simulate
backwards Euler discretization of the gradient ﬂow, similar to how mirror descent simulates forwards
Euler discretization [DO19]. The role of the extragradient step is a ﬁxed point iteration (of two
steps) which is a good approximation of the backwards Euler step when the operator is Lipschitz.

Nonetheless, the analysis of [Nem04, Nes07] fell short in obtaining a 1/T rate of convergence without
worse dependence on dimension for these domains, where T is the iteration count (which would
correspond to a ˜O (1/(cid:15)) runtime for approximate minimization). The fundamental barrier was that
over a box, any strongly-convex regularizer in the (cid:96)∞ norm has a dimension-dependent domain
size (shown in [ST18]). This barrier can also be viewed as the reason for the worse dimension
dependence in the accelerated scheme of [DGK18, LHJ19].

The primary insight of [She17] was that previous approaches attempted to regularize the schemes of

3

[Nem04, Nes07] with separable regularizers, i.e. the sum of a regularizer which depends only on the
primal block and one which depends only on the dual. If, say, the domain of the primal block was a
box, then such a regularization scheme would run into the (cid:96)∞ barrier and incur a worse dependence
on dimension. However, by more carefully analyzing the requirements of these algorithms, [She17]
constructed a non-separable regularizer with small domain size, satisfying a property termed area-
convexity which suﬃced for provable convergence of dual extrapolation [Nes07]. Interestingly, the
property seems specialized to dual extrapolation and not mirror prox [Nem04].

2 Overview

First, in Section 2.1 we ﬁrst describe a reformulation of (1) as a primal-dual objective, which we
solve approximately in Section 3. Then in Section 2.2 we give additional notation critical for our
analysis. In Section 3 we leverage this to give an overview of our main algorithm.

2.1

(cid:96)1-regression formulation

We adapt the view of [BJKS18, Qua19] of the objective (1) as a positive linear program. Let d
be the (vectorized) cost matrix C associated with the instance and let ∆n2 be the n2 dimensional
simplex4. We recall r, c are speciﬁed row and column sums with 1(cid:62)r = 1(cid:62)c = 1. The optimal
transport problem can be written as, for m = n2, and A ∈ {0, 1}2n×m, b ∈ R2n
≥0, for A the (unsigned)
edge-incidence matrix of the underlying bipartite graph and b the concatenation of r and c.

min
x∈∆n,Ax=b

d(cid:62)x.

(2)

A =











1 1 1 0 0 0 0 0 0
0 0 0 1 1 1 0 0 0
0 0 0 0 0 0 1 1 1
1 0 0 1 0 0 1 0 0
0 1 0 0 1 0 0 1 0
0 0 1 0 0 1 0 0 1











, b =











.











1/3
1/3
1/3
1/3
1/3
1/3

Figure 1: Edge-incidence matrix A of a 3 × 3 bipartite graph and uniform demands.

In particular, A is the 0-1 matrix on V × E such that Ave = 1 iﬀ v is an endpoint of edge e. We
summarize some additional properties of the constraint matrix A and vector b.

Fact 2.1. A, b have the following properties.

1. A ∈ {0, 1}2n×m has 2-sparse columns and n-sparse rows. Thus (cid:107)A(cid:107)1→1 = 2.
2. b(cid:62) = (cid:0)r(cid:62) c(cid:62)(cid:1), so that (cid:107)b(cid:107)1 = 2.
3. A has n2 nonzero entries.

Section 4 recalls the proof of the following theorem, which ﬁrst appeared in [AWR17].

Theorem 2.2 (Rounding guarantee, Lemma 7 in [AWR17]). There is an algorithm which takes ˜x
with (cid:107)A˜x − b(cid:107)1 ≤ δ and produces ˆx in O(n2) time, with

4We use d because C often arises from distances in a metric space, and to avoid overloading c.

Aˆx = b, (cid:107)˜x − ˆx(cid:107)1 ≤ 2δ.

4

We now show how the rounding procedure gives a roadmap for our approach. Consider the following
(cid:96)1 regression objective over the simplex (a similar penalized objective appeared in [She13]):

min
x∈∆m

d(cid:62)x + 2 (cid:107)d(cid:107)∞ (cid:107)Ax − b(cid:107)1 .

(3)

We show that the penalized objective value is still OPT, and furthermore any approximate mini-
mizer yields an approximate transport plan.

Lemma 1 (Penalized (cid:96)1 regression). The value of (3) is OPT. Also, given ˜x, an (cid:15)-approximate
minimizer to (3), we can ﬁnd (cid:15)-approximate transportation plan ˆx in O(n2) time.

Proof. Recall OPT = minAx=b d(cid:62)x. Let ˜x be the minimizing argument in (3). We claim there
is some optimal ˜x with A˜x = b; clearly, the ﬁrst claim is then true. Suppose otherwise, and let
(cid:107)Ax − b(cid:107)1 = δ > 0. Then, let ˆx be the result of the algorithm in Theorem 2.2, applied to ˜x, so that
Aˆx = b, (cid:107)˜x − ˆx(cid:107)1 ≤ 2δ. We then have

d(cid:62) ˆx + 2 (cid:107)d(cid:107)∞ (cid:107)Aˆx − b(cid:107)1 = d(cid:62)(ˆx − ˜x) + d(cid:62) ˜x ≤ d(cid:62) ˜x + (cid:107)d(cid:107)∞ (cid:107)ˆx − ˜x(cid:107)1 ≤ d(cid:62) ˜x + 2 (cid:107)d(cid:107)∞ δ.

The objective value of ˆx is no more than of ˜x, a contradiction. By this discussion, we can take any
approximate minimizer to (3) and round it to a transport plan without increasing the objective.

Section 3 proves Theorem 2.3, which says we can eﬃciently ﬁnd an approximate minimizer to (3).

Theorem 2.3 (Approximate (cid:96)1 regression over the simplex). There is an algorithm (Algorithm 1)
taking input (cid:15), which has O(((cid:107)d(cid:107)∞ log n log γ)/(cid:15)) parallel depth for γ = log n · (cid:107)d(cid:107)∞ /(cid:15), and total
work O(n2((cid:107)d(cid:107)∞ log n log γ)/(cid:15)), and obtains ˜x an (cid:15)-additive approximation to the objective in (3).

We will approach proving Theorem 2.3 through a primal-dual viewpoint, in light of the following
(based on the deﬁnition of the (cid:96)1 norm):

min
x∈∆m

d(cid:62)x + 2 (cid:107)d(cid:107)∞ (cid:107)Ax − b(cid:107)1 = min
x∈∆m

max
y∈[−1,1]2n

d(cid:62)x + 2 (cid:107)d(cid:107)∞

(cid:16)

y(cid:62)Ax − b(cid:62)y

(cid:17)

.

(4)

Further, a low-duality gap pair to (4) yields an approximate minimizer to (3).

Lemma 2 (Duality gap to error). Suppose x, y is feasible (x ∈ ∆m, y ∈ [−1, 1]2n), and for any
feasible u, v,

(cid:16)

d(cid:62)x + 2 (cid:107)d(cid:107)∞

(cid:16)

v(cid:62)Ax − b(cid:62)v

(cid:17)(cid:17)

(cid:16)

−

d(cid:62)u + 2 (cid:107)d(cid:107)∞

(cid:16)

y(cid:62)Au − b(cid:62)y

(cid:17)(cid:17)

≤ δ.

Then, we have d(cid:62)x + 2 (cid:107)d(cid:107)∞ (cid:107)Ax − b(cid:107)1 ≤ δ + OPT.

Proof. The result follows from maximizing over v, and noting that for the minimizing u,

d(cid:62)u + 2 (cid:107)d(cid:107)∞

(cid:16)

y(cid:62)Au − b(cid:62)y

(cid:17)

≤ d(cid:62)u + 2 (cid:107)d(cid:107)∞ (cid:107)Au − b(cid:107)1 = OPT.

Correspondingly, Section 3 gives an algorithm which obtains (x, y) with bounded duality gap within
the runtime of Theorem 2.3.

5

2.2 Notation

R≥0 is the nonnegative reals. 1 is the all-ones vector of appropriate dimension when clear. The
probability simplex is ∆d def= {v | v ∈ Rd
≥0, 1(cid:62)v = 1}. We say matrix X is in the simplex of
appropriate dimensions when its (nonnegative) entries sum to one.
(cid:107)·(cid:107)1 and (cid:107)·(cid:107)∞ are the (cid:96)1 and (cid:96)∞ norms, i.e. (cid:107)v(cid:107)1 = (cid:80)
i |vi| and (cid:107)v(cid:107)∞ = maxi |vi|. When A is
a matrix, we let (cid:107)A(cid:107)p→q be the matrix operator norm, i.e. sup(cid:107)v(cid:107)p=1 (cid:107)Av(cid:107)q, where (cid:107)·(cid:107)p is the (cid:96)p
norm. In particular, (cid:107)A(cid:107)1→1 is the largest (cid:96)1 norm of a column of A.
Throughout log is the natural logarithm. For x ∈ ∆d, h(x) = (cid:80)
where 0 log 0 = 0 by convention. It is well-known that maxx∈∆d h(x) − minx∈∆d h(x) = log d.
We also use the Bregman divergence of a regularizer and the proximal operator of a divergence.

i∈[d] xi log xi is (negative) entropy

Deﬁnition 2.4 (Bregman divergence). For (diﬀerentiable) regularizer r and z, w in its domain,
the Bregman divergence from z to w is

z (w) def= r(w) − r(z) − (cid:104)∇r(z), w − z(cid:105).
V r

When r is convex, the divergence is nonnegative and convex in the argument (w in the deﬁnition).

Deﬁnition 2.5 (Proximal operator). For (diﬀerentiable) regularizer r, z in its domain, and g in
the dual space (when the domain is in Rd, so is the dual space), we deﬁne the proximal operator as

Proxz(g) def= argminw {(cid:104)g, w(cid:105) + V r

z (w)} .

Several variables have specialized meaning throughout. All graphs considered will be on 2n vertices
with m edges, i.e. m = n2. A ∈ R2n×m is the edge-incidence matrix. d is the vectorized cost matrix
C. b is the constraint vector, concatenating row and column constraints r, c. In algorithms for
solving (4), x and y are primal (in a simplex) and dual (in a box) variables respectively. In Section 3,
we adopt the linear programming perspective where the decision variable x ∈ ∆m is a vector. In
Section 4, for convenience we take the perspective where X is an unﬂattened n × n matrix. Ur,c is
the feasible polytope: when the domain is vectors, Ur,c is x | Ax = b, and when it is matrices, Ur,c
is X | X1 = r, X (cid:62)1 = c (by ﬂattening X this is consistent).

3 Main Algorithm

This section describes our algorithm for ﬁnding a primal-dual pair (x, y) with a small duality gap,
with respect to the objective in (4), which we restate here for convenience:

min
x∈X

max
y∈Y

d(cid:62)x + 2 (cid:107)d(cid:107)∞

(cid:16)

y(cid:62)Ax − b(cid:62)y

(cid:17)

, X def= ∆m, Y def= [−1, 1]2n.

(Restatement of (4))

Our algorithm is a specialization of the algorithm in [She17]. One of our technical contributions
in this regard is an analysis of the algorithm which more closely relates it to the analysis of dual
extrapolation [Nes07], an algorithm for ﬁnding approximate saddle points with a more standard
analysis.
In Sec-
In Section 3.1, we give the algorithmic framework and convergence analysis.
tion B.1, we provide analysis of an alternating minimization scheme for implementing steps of the
procedure. The same procedure was used in [She17] which claimed without proof the linear con-
vergence rate of the alternating minimization; we hope the analysis will make the method more
broadly accessible to the optimization community. We defer many proofs to Appendix B.

6

3.1 Dual Extrapolation Framework

For an objective F (x, y) convex in x and concave in y, the standard way to measure the duality gap
is to deﬁne the gradient operator g(x, y) = (∇xF (x, y), −∇yF (x, y)), and show that for z = (x, y)
and any u on the product space, the regret, (cid:104)g(z), z − u(cid:105), is small. Correspondingly, we deﬁne

g(x, y) def=

(cid:16)

d + 2 (cid:107)d(cid:107)∞ A(cid:62)y, 2 (cid:107)d(cid:107)∞ (b − Ax)

(cid:17)

.

The dual extrapolation framework [Nes07] requires a regularizer on the product space. The algo-
rithm is simple to state; it takes two “mirror descent-like” steps each iteration, maintaining a state
st in the dual space5. A typical setup is a Lipschitz gradient operator and a regularizer which is
the sum of canonical strongly-convex regularizers in the norms corresponding to the product space
X , Y. However, recent works have shown that this setup can be greatly relaxed and still obtain
similar rates of convergence. In particular, [She17] introduced the following deﬁnition.

Deﬁnition 3.1 (Area-convexity). Regularizer r is κ-area-convex with respect to operator g if for
any points a, b, c in its domain,

(cid:18)

κ

r(a) + r(b) + r(c) − 3r

(cid:19)(cid:19)

(cid:18) a + b + c
3

≥ (cid:104)g(b) − g(a), b − c(cid:105).

(5)

Area-convexity is so named because (cid:104)g(b)−g(a), b−c(cid:105) can be viewed as measuring the “area” of the
triangle with vertices a, b, c with respect to some Jacobian matrix. In the case of bilinear objectives,
the left hand side in the deﬁnition of area-convexity is invariant to permuting a, b, c, whereas the
sign of the right hand side can be ﬂipped by interchanging a, c, so area-convexity implies convexity.
However, it does not even imply the regularizer r is strongly-convex, a typical assumption for the
convergence of mirror descent methods.

We state the algorithm for time horizon T ; the only diﬀerence from [Nes07] is a factor of 2 in
deﬁning st+1, i.e. adding a 1/2κ multiple rather than 1/κ. We ﬁnd it of interest to explore whether
this change is necessary or speciﬁc to the analysis of [She17].

Algorithm 1 ¯w = Dual-Extrapolation(κ, r, g, T ): Dual extrapolation with area-convex r.

Initialize s0 = 0, let ¯z be the minimizer of r.
for t < T do
zt ← Proxr
¯z(st).
wt ← Proxr
¯z
st+1 ← st + 1
t ← t + 1.

(cid:0)st + 1
2κ g(wt).

κ g(zt)(cid:1).

end for
return ¯w def= 1
T

(cid:80)

t∈[T ] wt.

Lemma 3 (Dual extrapolation convergence). Suppose r is κ-area-convex with respect to g. Further,
suppose for some u, Θ ≥ r(u) − r(¯z). Then, the output ¯w to Algorithm 1 satisﬁes

5In this regard, it is more similar to the “dual averaging” or “lazy” mirror descent setup [Bub15].

(cid:104)g( ¯w), ¯w − u(cid:105) ≤

2κΘ
T

.

7

In fact, by more carefully analyzing the requirements of dual extrapolation we have the following.

Corollary 1. Suppose in Algorithm 1, the proximal steps are implemented with (cid:15)(cid:48) additive error.
Then, the upper bound of the regret in Lemma 3 is 2κΘ/T + (cid:15)(cid:48).

We now state a useful second-order characterization of area-convexity involving a relationship
between the Jacobian of g and the Hessian of r, which was proved in [She17].

Theorem 3.2 (Second-order area-convexity, Theorem 1.6 in [She17]). For bilinear minimax objec-
tives, i.e. whose associated operator g has Jacobian

J =

(cid:18) 0 M (cid:62)
−M 0

(cid:19)

,

and for twice-diﬀerentiable r, if for any z in the domain,

(cid:18)κ∇2r(z)
J

(cid:19)

−J
κ∇2r(z)

(cid:23) 0,

then r is 3κ-area-convex with respect to g.

Finally, we complete the outline of the algorithm by stating the speciﬁc regularizer we use, which
ﬁrst appeared in [She17]. We then prove its 3-area-convexity with respect to g by using Theorem 3.2.





r(x, y) = 2 (cid:107)d(cid:107)∞

10

(cid:88)

j∈[n]

xj log xj + x(cid:62)A(cid:62)(y2)

 ,

(6)

where (y2) is entry-wise.

Lemma 4 (Area-convexity of the Sherman regularizer). For the Jacobian J associated with the
objective in (4) and the regularizer r deﬁned in (6), we have

(cid:18)∇2r(z)
J

(cid:19)

−J
∇2r(z)

(cid:23) 0.

We now give the proof of Theorem 2.3, requiring some claims in Appendix B.1 for the complexity
of Algorithm 1. In particular, Appendix B.1 implies that although the minimizer to the proximal
steps cannot be computed in closed form because of non-separability, a simple alternating scheme
converges to an approximate-minimizer in near-constant time.

Proof of Theorem 2.3. The algorithm is Algorithm 1, using the regularizer r in (6). Clearly, in the
feasible region the range of the regularizer is at most 20 (cid:107)d(cid:107)∞ log n + 4 (cid:107)d(cid:107)∞, where the former
summand comes from the range of entropy and the latter (cid:13)
(cid:13)∞ = 2. Thus, we may choose
Θ = O((cid:107)d(cid:107)∞ log n) in Lemma 3, since (cid:104)∇r(¯z), ¯z − u(cid:105) ≤ 0 ⇒ V r
By Theorem 3.2 and Lemma 4, r is 3-area-convex with respect to g. By Corollary 1, T = 12Θ/(cid:15)
iterations suﬃce, implementing each proximal step to (cid:15)/2-additive accuracy. Finally, using Theo-
rem B.1 to bound this implementation runtime concludes the proof.

(cid:13)A(cid:62)(cid:13)
¯z (u) ≤ r(u) − r(¯z) for all u.

8

4 Rounding to Ur,c

We state the rounding procedure in [AWR17] for completeness here, which takes a transport plan
˜X close to Ur,c and transforms it into a plan which exactly meets the constraints and is close to ˜X
in (cid:96)1, and then prove its correctness in Appendix C. Throughout r(X) def= X1, c(X) def= X (cid:62)1.

Algorithm 2 ˆX = Rounding( ˜X, r, c): Rounding to feasible polytope

X (cid:48) ← diag

(cid:16)

min
(cid:16)

(cid:16) r

r( ˜X)

, 1
(cid:16) c

(cid:17)(cid:17) ˜X.
(cid:17)(cid:17)

min

X (cid:48)(cid:48) ← X (cid:48)diag
c(X (cid:48)) , 1
er ← r − 1(cid:62)r(X (cid:48)(cid:48)), ec ← c − 1(cid:62)c(X (cid:48)(cid:48)), E ← 1(cid:62)er.
ˆX ← X (cid:48)(cid:48) + 1
return ˆX.

E ere(cid:62)
c .

.

5 Experiments

We show experiments illustrating the potential of our algorithm to be useful in practice, by con-
sidering its performance on computing optimal transport distances on the MNIST dataset and
comparing against algorithms in the literature including APDAMD [LHJ19] and Sinkhorn iter-
ation. All comparisons are based on the number of matrix-vector multiplications (rather than
iterations, due to our algorithm’s alternating subroutine), the main computational component of
all algorithms considered.

(a) Comparison with Sinkhorn iteration.

(b) Comparison with APDAMD [LHJ19].

While our unoptimized algorithm performs poorly, slightly optimizing the size of the regularizer
and step sizes used results in an algorithm with competitive performance to APDAMD, the ﬁrst-
order method with the best provable guarantees and observed practical performance. Sinkhorn
iteration outperformed all ﬁrst-order methods experimentally; however, an optimized version of
our algorithm performed better than conservatively-regularized Sinkhorn iteration, and was more
competitive with variants of Sinkhorn found in practice than other ﬁrst-order methods.

As we discuss in our implementation details (Appendix D), we acknowledge that implementations
of our algorithm illustrated are not the same as those with provable guarantees in our paper.
However, we believe that our modiﬁcations are justiﬁable in theory, and consistent with those
made in practice to existing algorithms. Further, we hope that studying the modiﬁcations we made
(step size, using mirror prox [Nem04] for stability considerations), as well as the consideration of

9

other numerical speedups such as greedy updates [AWR17] or kernel approximations [ABRW18],
will become fruitful for understanding the potential of accelerated ﬁrst-order methods in both the
theory and practice of computational optimal transport.

Acknowledgments

We thank Jose Blanchet and Carson Kent for helpful conversations.

References

[ABRW18]

[ACB17]

Jason Altschuler, Francis Bach, Alessandro Rudi, and Jonathan Weed. Approximat-
ing the quadratic transportation metric in near-linear time. CoRR, abs/1810.10046,
2018. 1.2, 5, D

Mart´ın Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative ad-
versarial networks. In Proceedings of the 34th International Conference on Machine
Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 214–223,
2017. 1

[ANOY14] Alexandr Andoni, Aleksandar Nikolov, Krzysztof Onak, and Grigory Yaroslavtsev.
Parallel algorithms for geometric graph problems. In Symposium on Theory of Com-
puting, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 574–583,
2014. 1.2

[AO15]

[AS14]

[AWR17]

[BJKS18]

[BK17]

Zeyuan Allen Zhu and Lorenzo Orecchia. Nearly-linear time positive LP solver with
faster convergence rate. In Proceedings of the Forty-Seventh Annual ACM on Sympo-
sium on Theory of Computing, STOC 2015, Portland, OR, USA, June 14-17, 2015,
pages 229–236, 2015. 1.2

Pankaj K. Agarwal and R. Sharathkumar. Approximation algorithms for bipartite
matching with metric and geometric costs. In Symposium on Theory of Computing,
STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 555–564, 2014. 1.2

Jason Altschuler, Jonathan Weed, and Philippe Rigollet. Near-linear time approxi-
mation algorithms for optimal transport via sinkhorn iteration. In Advances in Neural
Information Processing Systems 30: Annual Conference on Neural Information Pro-
cessing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 1961–1971,
2017. 1, 1.2, ??, 2.1, 2.2, 4, 5, ??, D

Jose Blanchet, Arun Jambulapati, Carson Kent, and Aaron Sidford. Towards optimal
running times for optimal transport. CoRR, abs/1810.07717, 2018. (document), 1,
1.1, 1.2, ??, ??, 2.1

Jose H. Blanchet and Yang Kang. Distributionally robust groupwise regularization
estimator. In Proceedings of The 9th Asian Conference on Machine Learning, ACML
2017, Seoul, Korea, November 15-17, 2017., pages 97–112, 2017. 1

[Bub15]

S´ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations
and Trends in Machine Learning, 8(3-4):231–357, 2015. 5

[BvdPPH11] Nicolas Bonneel, Michiel van de Panne, Sylvain Paris, and Wolfgang Heidrich. Dis-
placement interpolation using lagrangian mass transport. ACM Trans. Graph.,
30(6):158:1–158:12, 2011. 1

[CK18]

Deeparnab Chakrabarty and Sanjeev Khanna. Better and simpler error analysis of
the sinkhorn-knopp algorithm for matrix scaling. In 1st Symposium on Simplicity in

10

Algorithms, SOSA 2018, January 7-10, 2018, New Orleans, LA, USA, pages 4:1–4:11,
2018. 1

[CMTV17] Michael B. Cohen, Aleksander Madry, Dimitris Tsipras, and Adrian Vladu. Matrix
scaling and balancing via box constrained newton’s method and interior point meth-
ods. In 58th IEEE Annual Symposium on Foundations of Computer Science, FOCS
2017, Berkeley, CA, USA, October 15-17, 2017, pages 902–913, 2017. 1.2

[Cut13]

[DGK18]

[DO19]

[EK18]

[GCPB16]

[HK73]

[KLOS14]

[LHJ19]

[LMR19]

[LS14]

[LS15]

Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In
Advances in Neural Information Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Proceedings of a meeting held December
5-8, 2013, Lake Tahoe, Nevada, United States., pages 2292–2300, 2013. 1, 1.2

Pavel Dvurechensky, Alexander Gasnikov, and Alexey Kroshnin. Computational
optimal transport: Complexity by accelerated gradient descent is better than by
sinkhorn’s algorithm.
In Proceedings of the 35th International Conference on Ma-
chine Learning, ICML 2018, Stockholmsm¨assan, Stockholm, Sweden, July 10-15,
2018, pages 1366–1375, 2018. (document), 1, 1.1, 1.2, ??, 1, 1.2

Jelena Diakonikolas and Lorenzo Orecchia. The approximate duality gap technique: A
uniﬁed theory of ﬁrst-order methods. SIAM Journal on Optimization, 29(1):660–689,
2019. 1.2

Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust
optimization using the wasserstein metric: performance guarantees and tractable re-
formulations. Math. Program., 171(1-2):115–166, 2018. 1

Aude Genevay, Marco Cuturi, Gabriel Peyr´e, and Francis R. Bach. Stochastic opti-
mization for large-scale optimal transport. In Advances in Neural Information Pro-
cessing Systems 29: Annual Conference on Neural Information Processing Systems
2016, December 5-10, 2016, Barcelona, Spain, pages 3432–3440, 2016. 1, 1.2
John E. Hopcroft and Richard M. Karp. An n5/2 algorithm for maximum matchings
in bipartite graphs. SIAM J. Comput., 2(4):225–231, 1973. 1.2

Jonathan A. Kelner, Yin Tat Lee, Lorenzo Orecchia, and Aaron Sidford. An almost-
linear-time algorithm for approximate max ﬂow in undirected graphs, and its multi-
commodity generalizations. In Proceedings of the Twenty-Fifth Annual ACM-SIAM
Symposium on Discrete Algorithms, SODA 2014, Portland, Oregon, USA, January
5-7, 2014, pages 217–226, 2014. 1.2

Tianyi Lin, Nhat Ho, and Michael I. Jordan. On eﬃcient optimal transport: An
analysis of greedy and accelerated mirror descent algorithms. CoRR, abs/1901.06482,
2019. (document), 1, 1.1, 1.2, ??, 1, 1.2, 5, 2b, D

Nathaniel Lahn, Deepika Mulchandani, and Sharath Raghvendra. A graph theoretic
additive approximation of optimal transport. CoRR, abs/1905.11830, 2019. 1.2

Yin Tat Lee and Aaron Sidford. Path ﬁnding methods for linear programming: Solv-
ing linear programs in ˜o(vrank) iterations and faster algorithms for maximum ﬂow.
In 55th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2014,
Philadelphia, PA, USA, October 18-21, 2014, pages 424–433, 2014. 1.2

Yin Tat Lee and Aaron Sidford. Eﬃcient inverse maintenance and faster algorithms
for linear programming. In IEEE 56th Annual Symposium on Foundations of Com-
puter Science, FOCS 2015, Berkeley, CA, USA, 17-20 October, 2015, pages 230–249,
2015. 1.2, ??

11

[Nem04]

[Nes05]

[Nes07]

[PZ16]

[Qua19]

[SA12]

[San09]

[SdGP+15]

[She13]

[She17]

[ST18]

[You01]

Arkadi Nemirovski. Prox-method with rate of convergence o(1/t) for variational
inequalities with lipschitz continuous monotone operators and smooth convex-concave
saddle point problems. SIAM Journal on Optimization, 15(1):229–251, 2004. 1.2, 5,
D

Yurii Nesterov. Smooth minimization of non-smooth functions. Math. Program.,
103(1):127–152, 2005. 1.2, 1.2

Yurii Nesterov. Dual extrapolation and its applications to solving variational in-
equalities and related problems. Math. Program., 109(2-3):319–344, 2007. 1.2, 3, 3.1,
3.1

Victor M. Panaretos and Yoav Zemel. Amplitude and phase variation of point pro-
cesses. Annals of Statistics, 44(2):771–812, 2016. 1

Kent Quanrud. Approximating optimal transport with linear programs.
In 2nd
Symposium on Simplicity in Algorithms, SOSA@SODA 2019, January 8-9, 2019 -
San Diego, CA, USA, pages 6:1–6:9, 2019. (document), 1, 1.1, 1.2, ??, 2.1

R. Sharathkumar and Pankaj K. Agarwal. A near-linear time (cid:15)-approximation al-
gorithm for geometric bipartite matching. In Proceedings of the 44th Symposium on
Theory of Computing Conference, STOC 2012, New York, NY, USA, May 19 - 22,
2012, pages 385–394, 2012. 1.2

Piotr Sankowski. Maximum weight bipartite matching in matrix multiplication time.
Theor. Comput. Sci., 410(44):4480–4488, 2009. 1.2

Justin Solomon, Fernando de Goes, Gabriel Peyr´e, Marco Cuturi, Adrian Butscher,
Andy Nguyen, Tao Du, and Leonidas J. Guibas. Convolutional wasserstein dis-
tances: eﬃcient optimal transportation on geometric domains. ACM Trans. Graph.,
34(4):66:1–66:11, 2015. 1

Jonah Sherman. Nearly maximum ﬂows in nearly linear time. In 54th Annual IEEE
Symposium on Foundations of Computer Science, FOCS 2013, 26-29 October, 2013,
Berkeley, CA, USA, pages 263–269, 2013. 1.2, 2.1

Jonah Sherman. Area-convexity, l∞ regularization, and undirected multicommodity
ﬂow.
In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of
Computing, STOC 2017, Montreal, QC, Canada, June 19-23, 2017, pages 452–460,
2017. (document), 1.1, 1.2, 3, 3.1, 3.1, 3.1, 3.2, 3.1

Aaron Sidford and Kevin Tian. Coordinate methods for accelerating (cid:96)∞ regression and
faster approximate maximum ﬂow. In 59th Annual IEEE Symposium on Foundations
of Computer Science, FOCS 2018, 7-9 October, 2018, Paris, France, 2018. 1.2, 1.2

Neal E. Young. Sequential and parallel algorithms for mixed packing and covering.
In 42nd Annual Symposium on Foundations of Computer Science, FOCS 2001, 14-17
October 2001, Las Vegas, Nevada, USA, pages 538–546, 2001. 1.2

[ZLdOW17] Zeyuan Allen Zhu, Yuanzhi Li, Rafael Mendes de Oliveira, and Avi Wigderson. Much
faster algorithms for matrix scaling. In 58th IEEE Annual Symposium on Foundations
of Computer Science, FOCS 2017, Berkeley, CA, USA, October 15-17, 2017, pages
890–901, 2017. 1.2

12

A Algorithm

We give the complete algorithm for approximating optimal transport distance to additive (cid:15) here.
We assume C ∈ Rn×n
≥0 and r, c ∈ ∆n. Finally, we refer to blocks of variable z on a product space as
zx, zy, i.e. z = (zx, zy). Again r(X) def= X1, c(X) def= X (cid:62)1.

Algorithm 3 ˆX = Optimal-Transport(C, (cid:15), r, c): Produces (cid:15)-approximate transportation plan

n2 1, y0 ← 02n.
0 ← 02n.

Vectorize C to produce d.
Let b be r, c concatenated; let A be the incidence matrix of a complete n × n bipartite graph.
t ← 0.
x0 ← 1
0 ← 0n2, sy
sx
Θ ← 20 (cid:107)d(cid:107)∞ log n + 4 (cid:107)d(cid:107)∞.
while d(cid:62)xt+ 1
t ← t + 1.
k ← 0.
x(cid:48)
0 ← xt− 1
for 0 ≤ k <

, y(cid:48)
0 ← yt− 1
(cid:108)
24 log

d + 2 (cid:107)d(cid:107)∞ A(cid:62)yt+ 1

≤ −2 (cid:107)d(cid:107)∞ b(cid:62)yt+ 1

(cid:13)
(cid:13)
(cid:13)Axt+ 1

(cid:16)(cid:16) 88(cid:107)d(cid:107)∞

+ 2 (cid:107)d(cid:107)∞

+ maxj

(cid:13)
(cid:13)
(cid:13)1

− b

do

(cid:105)

(cid:104)

.

j

2

2

2

2

2

2

+ (cid:15) do

(cid:16)

(cid:16)

1
20(cid:107)d(cid:107)∞
1, max

t + 1
sx
(cid:16)

−1,

(cid:17)
(cid:17)(cid:109)
Θ
k−1)2(cid:17)
(cid:17)(cid:17)

(cid:15)2 + 2
(cid:15)
10 A(cid:62)(y(cid:48)
−sy
t
4(cid:107)d(cid:107)∞Ax(cid:48)
k

, x(cid:48)

k ← x(cid:48)

k/ (cid:107)x(cid:48)

k(cid:107)1.

. Operations are element-wise.

x(cid:48)
k ← exp
y(cid:48)
k ← min
end for
xt ← x(cid:48)
sx
t+ 1
2
sy
t+ 1
2
k ← 0.
0 ← xt, y(cid:48)
x(cid:48)
for 0 ≤ k <

← sx
← sy

k, yt ← y(cid:48)
k.
(cid:1).
(cid:0)d + 2 (cid:107)d(cid:107)∞ A(cid:62)yt
t + 1
3
t + 1
3 (2 (cid:107)d(cid:107)∞ (b − Axt)).

0 ← yt.
(cid:108)

24 log

(cid:16)(cid:16) 88(cid:107)d(cid:107)∞

(cid:16)

x(cid:48)
k ← exp

1
20(cid:107)d(cid:107)∞

sx
t+ 1
2
(cid:32)

(cid:32)

y(cid:48)
k ← min

1, max

−1,

(cid:17)(cid:109)
Θ
do
k−1)2(cid:17)
(cid:33)(cid:33)

(cid:17)

(cid:15)2 + 2
+ 1

(cid:15)
10 A(cid:62)(y(cid:48)
−sy

t+ 1
2
4(cid:107)d(cid:107)∞Ax(cid:48)
k

, x(cid:48)

k ← x(cid:48)

k/ (cid:107)x(cid:48)

k(cid:107)1.

. Operations are element-wise.

2

(cid:17)

← x(cid:48)

← y(cid:48)
k.
d + 2 (cid:107)d(cid:107)∞ A(cid:62)yt+ 1
.
2
(cid:17)
)
2 (cid:107)d(cid:107)∞ (b − Axt+ 1

k, yt+ 1
2
(cid:16)
t + 1
6
t + 1
6

end for
xt+ 1
t+1 ← sx
sx
sy
t+1 ← sy
end while
Un-vectorize x to produce ˜X.
(cid:17)(cid:17) ˜X.
X (cid:48) ← diag
min
(cid:17)(cid:17)
(cid:16)

(cid:16) r

r( ˜X)

, 1
(cid:16) c

(cid:16)

(cid:16)

2

.

min

X (cid:48)(cid:48) ← X (cid:48)diag
c(X (cid:48)) , 1
er ← r − 1(cid:62)r(X (cid:48)(cid:48)), ec ← c − 1(cid:62)c(X (cid:48)(cid:48)), E ← 1(cid:62)er.
ˆX ← X (cid:48)(cid:48) + 1
return ˆX.

E ere(cid:62)
c .

.

13

We remark that there are a variety of termination conditions that can be useful in practice for the
alternating minimization procedure. For example, a standard early-stopping condition based on
the observed movement of consecutive iterates was very successful in practice (Appendix D).

B Missing proofs from Section 3

In this section, we state missing proofs from Section 3. We provide the eﬃcient implementation of
the proximal steps required by Algorithm 1 in Appendix B.1.

Lemma 3 (Dual extrapolation convergence). Suppose r is κ-area-convex with respect to g. Further,
suppose for some u, Θ ≥ r(u) − r(¯z). Then, the output ¯w to Algorithm 1 satisﬁes

Proof. Our ﬁrst step is to prove the following inequality:

(cid:104)g( ¯w), ¯w − u(cid:105) ≤

2κΘ
T

.

1
2κ

(cid:104)g(wt), wt − ¯z(cid:105) ≤ (cid:104)st+1, zt+1 − ¯z(cid:105) + V r

¯z (zt+1) − (cid:104)st, zt − ¯z(cid:105) − V r

¯z (zt).

(7)

Let ct = zt+wt+zt+1
with respect to zt+1, and area-convexity (5) with respect to zt, wt, and zt+1. Respectively,

. The proof follows from minimality of zt with respect to ct, minimality of wt

3

(cid:104)st, wt(cid:105) +

1
κ

(cid:104)st, zt(cid:105) + r(zt) ≤ (cid:104)st, ct(cid:105) + r(ct)
1
κ

(cid:104)g(zt), wt(cid:105) + r(wt) ≤ (cid:104)st, zt+1(cid:105) +

(cid:104)g(zt), zt+1(cid:105) + r(zt+1)

(8)

1
κ

(cid:104)g(wt) − g(zt), wt − zt+1(cid:105) ≤ r(zt) + r(wt) + r(zt+1) − 3r (ct) .

Substituting the ﬁrst equation into the third and using the deﬁnition of ct, we have

1
κ

(cid:104)g(wt) − g(zt), wt − zt+1(cid:105) ≤ r(wt) + r(zt+1) − 2r(zt) + (cid:104)st, wt + zt+1 − 2zt(cid:105).

Rearranging the second equation, we have

1
κ

(cid:104)g(zt), wt − zt+1(cid:105) ≤ r(zt+1) − r(wt) + (cid:104)st, zt+1 − wt(cid:105).

Adding these two equations, we have

1
κ

(cid:104)g(wt), wt − zt+1(cid:105) ≤ 2r(zt+1) − 2r(zt) + (cid:104)st, 2zt+1 − 2zt(cid:105).

Dividing by 2 and adding 1
the potential function

2κ (cid:104)g(wt), zt+1 − ¯z(cid:105) to both sides, we obtain the desired (7). Now, deﬁne

Φk =

1
2κ

k−1
(cid:88)

(cid:104)g(wt), wt − ¯z(cid:105) − (cid:104)sk, zk − ¯z(cid:105) − V r

¯z (zk)

t=0

14

Then, by (7), Φk is nonincreasing in k. Therefore for any u, by the deﬁnition of Θ,

1
T

T −1
(cid:88)

(cid:104)g(wt), wt − u(cid:105) ≤

t=0

≤

=

1
T

1
T

T −1
(cid:88)

t=0
T −1
(cid:88)

t=0

(cid:104)g(wt), wt − ¯z(cid:105) +

(cid:104)g(wt), wt − ¯z(cid:105) +

1
T

1
T

2κ
T

ΦT +

2κΘ
T

≤

2κ
T

Φ0 +

t=0
T −1
(cid:88)

t=0
2κΘ
T

=

2κΘ
T

.

T −1
(cid:88)

(cid:104)g(wt), ¯z − u(cid:105) +

(cid:18) 2κΘ
T

−

2κV¯z(u)
T

(cid:19)

(cid:104)g(wt), ¯z − zT (cid:105) +

(cid:18) 2κΘ
T

−

2κV¯z(zT )
T

(cid:19)

The inequality on the second line used the deﬁnition of zT = Proxr
¯z
last inequality is ΦT ≤ Φ0. The conclusion follows from the deﬁnition of g (because it is linear).

, and the

(cid:16) 1
2κ

(cid:80)

(cid:17)
t∈[T −1] g(wt)

Corollary 1. Suppose in Algorithm 1, the proximal steps are implemented with (cid:15)(cid:48) additive error.
Then, the upper bound of the regret in Lemma 3 is 2κΘ/T + (cid:15)(cid:48).

Proof. We see that (7) now holds up to (cid:15)(cid:48) additive error, so that Φk is increasing by at most (cid:15)(cid:48) each
step. Thus, we obtain ΦT ≤ Φ0 + T (cid:15)(cid:48), yielding the conclusion.

Lemma 4 (Area-convexity of the Sherman regularizer). For the Jacobian J associated with the
objective in (4) and the regularizer r deﬁned in (6), we have

(cid:18)∇2r(z)
J

(cid:19)

−J
∇2r(z)

(cid:23) 0.

Proof. We scale both r and J down by 2 (cid:107)d(cid:107)∞, which does not aﬀect positive-semideﬁniteness. By
computation we have (recalling all columns of A have (cid:96)1 norm of 2)

∇2r(x, y) =

(cid:32)

5 (cid:107)A:j(cid:107)1 diag

(cid:17)

(cid:16) 1
xj

2diag (yi) A

(cid:33)

2A(cid:62)diag (yi)
i x(cid:1)
2diag (cid:0)A(cid:62)

.

It suﬃces to show that for any vector (cid:0)a b

c d(cid:1) we have

(cid:0)a b

c d(cid:1)









5 (cid:107)A:j(cid:107)1 diag

(cid:17)

(cid:16) 1
xj

2diag (yi) A
0

−A

2A(cid:62)diag (yi)
2diag (cid:0)A(cid:62)
i x(cid:1)
A(cid:62)
0

0

A

5 (cid:107)A:j(cid:107)1 diag

(cid:17)

(cid:16) 1
xj

2diag (yi) A

Upon simplifying and gathering like terms, it suﬃces to show

−A(cid:62)
0
2A(cid:62)diag (yi)
i x(cid:1)
2diag (cid:0)A(cid:62)















≥ 0.







a
b
c
d

(cid:32) 5a2
j
xj

(cid:88)

i,j

Aij

+ 4ajbiyi + 2b2

i xj − 2ajdi + 2cjbi +

(cid:33)

+ 4cjdiyi + 2d2

i xj

≥ 0.

5c2
j
xj

However, this is true for yi ∈ [−1, 1], since each coeﬃcient groups into clearly nonnegative terms,
(cid:32) 4a2
j
xj

+ 4cjdiyi + d2

+ 4ajbiyi + b2

− 2ajdi + d2

(cid:32) 4c2
j
xj

+ 2cjbi + b2

(cid:32) a2
j
xj

(cid:32) c2
j
xj

i xj

i xj

i xj

i xj

(cid:33)

(cid:33)

(cid:33)

+

+

+

(cid:33)

.

15

B.1 Alternating Minimization Analysis

In this section, we give the convergence analysis of an alternating minimization procedure for
minimizing a function of the form (throughout this section, r(x, y) is as in (6))

f (x, y) def= (cid:104)ξ, x(cid:105) + (cid:104)η, y(cid:105) + r(x, y)

(9)

which is the type of minimization problem arising from steps of the form Proxr
¯z(g). As we will see,
f (x, y) is jointly convex. Throughout this section, let xOPT, yOPT be the minimizer to f . Corollary 1
states that O((cid:15)) additive error to f gives the same asymptotic convergence rate in Algorithm 1.
We will show that a simple alternating minimization scheme enjoys a linear rate of convergence in
our setting; thus, roughly O(log (cid:15)−1) iterations suﬃce. We ﬁrst give a proof of a general condition
which suﬃces for linear convergence.

Lemma 5. Suppose f (x, y) is twice-diﬀerentiable and jointly convex, over the product space X × Y.
Consider the alternating minimization scheme,

1. xk+1

2. yk+1

def= argminx∈X f (x, yk)
def= argminy∈Y f (xk+1, y)

Further, suppose there are convex regions Xk+1 ⊆ X , Yk ⊆ Y which contain xk+1, yk respectively,
such that for any x(cid:48) ∈ Xk+1, y(cid:48), y(cid:48)(cid:48) ∈ Yk, and for some σ ≥ 1,

∇2f (x(cid:48), y(cid:48)) (cid:23)

1
σ

∇2

yyf (xk+1, y(cid:48)(cid:48)),

(10)

where ∇2

yy is the Hessian with all but the yy block zeroed out. Then, for any x∗ ∈ Xk+1, y∗ ∈ Yk,

f (xk+1, yk) − f (xk+1, yk+1) ≥

1
σ

(f (xk+1, yk) − f (x∗, y∗)) .

Proof. Let ˜y = (cid:0)1 − 1
σ

(cid:1) yk + 1

σ y∗. We will prove instead that

f (xk+1, yk) − f (xk+1, ˜y) ≥

1
σ

(f (xk+1, yk) − f (x∗, y∗)) ,

from which the conclusion will follow since f (xk+1, yk+1) ≤ f (xk+1, ˜y). Note by deﬁnition of ˜y, as
well as optimality of xk+1 which implies 0 ≥ (cid:104)∇xf (xk+1, yk), xk+1 − x∗(cid:105),

(cid:104)∇yf (xk+1, yk), yk − ˜y(cid:105) =

1
σ

(cid:104)∇yf (xk+1, yk), yk − y∗(cid:105) ≥

1
σ

(cid:104)∇f (xk+1, yk), zk+ 1

2

− z∗(cid:105)

(11)

where zk+ 1

2

def= (xk+1, yk) and z∗ def= (x∗, y∗). Further, let yα

def= (1 − α)yk + αy∗, ˜yα

def= (1 − α)yk + α˜y,

16

and xα

def= (1 − α)xk+1 + αx∗. Then, by Taylor expansion we have f (xk+1, yk) − f (xk+1, ˜y) equals

(cid:104)∇yf (xk+1, yk), yk − ˜y(cid:105) −

(cid:90) 1

(cid:90) β

(˜y − yk)(cid:62)∇2

yyf (xk+1, ˜yα)(˜y − yk)dαdβ

(cid:104)∇f (xk+1, yk), zk+ 1
(cid:18)

2

(cid:104)∇f (xk+1, yk), zk+ 1

2

0

− z∗(cid:105) −

0
1
σ2

− z∗(cid:105) −

(cid:90) 1

(cid:90) β

0
(cid:90) 1

0
(cid:90) β

0

0

(f (xk+1, yk) − f (x∗, y∗)) .

≥

≥

=

1
σ
1
σ
1
σ

(y∗ − yk)(cid:62)∇2

yyf (xk+1, ˜yα)(y∗ − yk)dαdβ

(z∗ − zk+ 1

2

)(cid:62)∇2f (xα, yα)(z∗ − zk+ 1

2

)dαdβ

(cid:19)

In the ﬁrst inequality, we used (11) and the deﬁnition of ˜y, and in the second we used (10) (since
xα ∈ Xk+1, yα, ˜yα ∈ Yk by convexity).

We now give a helper lemma specialized to the particular f in (9), which will be used in the proof
of convergence.
Lemma 6. For some xk+1, yk, let Xk+1 = (cid:8)x | x ≥ 1
let Yk be the entire domain of y (i.e. Y). Then for any x(cid:48) ∈ Xk+1, y(cid:48), y(cid:48)(cid:48) ∈ Yk,

(cid:9) where the inequality is entrywise, and

2 xk+1

∇2r(x(cid:48), y(cid:48)) (cid:23)

1
12

∇2

yyr(xk+1, y(cid:48)(cid:48)).

Proof. Recall that (since (cid:107)A:j(cid:107)1 = 2)

∇2r(x, y) = 2 (cid:107)d(cid:107)∞

(cid:32)

5 (cid:107)A:j(cid:107)1 diag

(cid:17)

(cid:16) 1
xj

2diag (yi) A

2A(cid:62)diag (yi)
i x(cid:1)
2diag (cid:0)A(cid:62)

(cid:33)

.

Consider the diagonal approximation

D(x) = 2 (cid:107)d(cid:107)∞

(cid:32)

(cid:107)A:j(cid:107)1 diag

(cid:17)

(cid:16) 1
xj

0

(cid:33)

.

0
diag (cid:0)A(cid:62)

i x(cid:1)

We claim for any y,

To see this, consider the quadratic forms with respect to some vector (cid:0)u v(cid:1):

D(x) (cid:22) ∇2r(x, y) (cid:22) 6D(x).

(12)

(cid:0)u v(cid:1) ∇2r(x, y)

(cid:0)u v(cid:1) D(x)

(cid:19)

(cid:18)u
v

(cid:19)

(cid:18)u
v

= 2 (cid:107)d(cid:107)∞

= 2 (cid:107)d(cid:107)∞

(cid:88)

i,j

(cid:88)

i,j

Aij

Aij

(cid:32) 5u2
j
xj
(cid:32) u2
j
xj

(cid:33)

.

+ v2

i xj

(cid:33)

+ 4ujviyi + 2v2

i xj

,

Now (12) follows because for any yi ∈ [−1, 1], it’s easy to verify

u2
j
xj

+ v2

i xj ≤

5u2
j
xj

+ 4ujviyi + 2v2

i xj ≤ 6

(cid:32) u2
j
xj

(cid:33)

+ v2

i xj

.

17

Therefore, to prove the lemma statement we can use

∇2r(x(cid:48), y(cid:48)) (cid:23) D(x(cid:48)) (cid:23)

1
2

D(xk+1) (cid:23)

1
12

∇2

yyr(xk+1, y(cid:48)(cid:48)).

The inequality D(x(cid:48)) (cid:23) 1
followed from D(xk+1) spectrally dominating 1
yy block can only decrease the quadratic form.

2 D(xk+1) followed from the deﬁnition of Xk+1, and the last inequality
6 ∇2r(xk+1, y(cid:48)(cid:48)), and restrictions of D(xk+1) to the

We now give the proof of the linear rate of convergence.

Lemma 7. For f (x, y) deﬁned in (9), the alternating minimization scheme

1. xk+1

2. yk+1

def= argminx∈X f (x, yk).
def= argminy∈Y f (xk+1, y).

decreases the function error f (xk, yk) − f (xOPT, yOPT) by a factor of at least 1/24 in each iteration.

Proof. We can apply Lemma 5 with the sets deﬁned in Lemma 6, with σ = 12. On iteration k,
consider picking the points x∗, y∗ = 1
2 (yk + yOPT). Evidently, x∗ ∈ Xk+1, y∗ ∈ Yk.
Therefore, since f (xk+1, yk+1) ≥ f (xk+2, yk+1),

2 (xk+1 + xOPT), 1

f (xk+1, yk) − f (xk+2, yk+1) ≥ f (xk+1, yk) − f (xk+1, yk+1) ≥

1
12

(f (xk+1, yk) − f (x∗, y∗)).

Furthermore, by convexity, we have

f (xk+1, yk) − f (x∗, y∗) ≥

1
2

(f (xk+1, yk) − f (xOPT, yOPT)).

Finally, combining these two inequalities and rearranging,

23
24

(f (xk+1, yk) − f (xOPT, yOPT)) ≥ f (xk+2, yk+1) − f (xOPT, yOPT).

Thus, by taking a y step and then an x step, we decrease the function error by a 1/24 factor.

Finally, we show that steps of the alternating minimization can be implemented in linear time.

Lemma 8. For f (x, y) deﬁned in (9), we can implement the steps

1. xk+1

2. yk+1

def= argminxf (x, yk).
def= argminyf (xk+1, y).

restricted to the relevant domains, in time O(n2).

Proof. Recall A has n2 nonzero entries, so a matrix-vector multiplication can be performed in this
time. Computing x in linear time is straightforward: it is deﬁned by

argminx (cid:104)γ, x(cid:105) +

(cid:88)

j∈[n]

xj log xj such that x ∈ ∆m, γ def=

1
20 (cid:107)d(cid:107)∞

ξ +

1
10

A(cid:62)(y2).

18

By examining the KKT conditions, it is clear that the minimizing x is proportional to exp(−γ);
computing γ takes O(n2) time, as does the simplex projection. Similarly, computing y in linear
time is simple for ﬁxed x: it is

argminy(cid:104)η, y(cid:105) + (cid:104)2 (cid:107)d(cid:107)∞ Ax, y2(cid:105) such that y ∈ [−1, 1]2n,

which is coordinate-wise decomposable as minimizing a quadratic over an interval.

Theorem B.1 (Complexity of alternating minimization). We can obtain an (cid:15)/2-approximate min-
imizer to the proximal steps required by Algorithm 1 to (cid:15)/2 accuracy, with the regularizer of (6)
and κ = 3, in O(log γ) parallelizable iterations for γ = log n · (cid:107)d(cid:107)∞ · (cid:15)−1, and O(n2 log γ) total work.

Proof. By Lemmas 7 and 8, we can spend O(n2) parallelizable work to decrease the suboptimality
gap by a 1/24 factor, so it remains to argue that the initial error is at most poly(log n, (cid:107)d(cid:107)∞ , (cid:15)−1) to
show that implementing the proximal steps to additive error (cid:15)/2 can be done in O(log γ) iterations.
We show that this is true for implementing the proximal step for zt; a similar argument holds for
wt. To this end, note that by our setting of κ, for any z where we let g(z) = (gx(z), gy(z)),

1
2κ
1
2κ

(cid:107)gx(z)(cid:107)∞ =

(cid:107)gy(z)(cid:107)1 =

1
6
1
6

(cid:13)
(cid:13)d + 2 (cid:107)d(cid:107)∞ A(cid:62)y
(cid:13)

(cid:13)
(cid:13)
(cid:13)∞

(cid:107)2 (cid:107)d(cid:107)∞ (b − Ax)(cid:107)1 ≤

≤

(cid:107)d(cid:107)∞
2
4 (cid:107)d(cid:107)∞
3

,

.

t , sy
Therefore, for st = (sx
steps required where Θ is the range of r, we have

t ), by the triangle inequality, and t ≤ 12Θ/(cid:15) the bound on the number of

(cid:107)sx

t (cid:107)∞ ≤ t ·

(cid:107)sy

t (cid:107)1 ≤ t ·

1
2κ
1
2κ

(cid:107)gx(z)(cid:107)∞ ≤

(cid:107)gy(z)(cid:107)1 ≤

6 (cid:107)d(cid:107)∞ Θ
(cid:15)
16 (cid:107)d(cid:107)∞ Θ
(cid:15)

,

.

A simple calculation yields Θ = 20 (cid:107)d(cid:107)∞ log n + 4 (cid:107)d(cid:107)∞ upper bounds the range of r. Finally, let
t , y∗
x∗

t be the minimizer of the proximal objective,

(cid:104)sx

t , x(cid:105) + (cid:104)sy

t , y(cid:105) + r(x, y).

For any initialization xinit, yinit to the alternating minimization, the suboptimality gap is given by

≤ (cid:107)xinit − x∗

t (cid:107)1 (cid:107)sx

t (cid:107)∞ + (cid:107)yinit − y∗

t (cid:107)1 + Θ ≤

(cid:104)sx

t , xinit − x∗

t (cid:105) + (cid:104)sy

t , yinit − y∗
t (cid:107)∞ (cid:107)sy

t , y∗
t (cid:105) + r(xinit, yinit) − r(x∗
t )
(cid:19)

(cid:18) 44 (cid:107)d(cid:107)∞
(cid:15)

+ 1

Θ.

Therefore, the total number of iterations required is bounded by 24 log

C Missing proofs from Section 4

In this section, we give the proof to Theorem 2.2.

(cid:16)(cid:16) 88(cid:107)d(cid:107)∞

(cid:15)2 + 2

(cid:15)

(cid:17)

(cid:17)

Θ

as desired.

Theorem 2.2 (Rounding guarantee, Lemma 7 in [AWR17]). There is an algorithm which takes ˜x

19

with (cid:107)A˜x − b(cid:107)1 ≤ δ and produces ˆx in O(n2) time, with

Aˆx = b, (cid:107)˜x − ˆx(cid:107)1 ≤ 2δ.

Proof. The algorithm is Algorithm 2. We adopt the alternative view of ˜x as a n × n matrix ˜X in
the simplex, and deﬁne operations r(X) = X1, c(X) = X (cid:62)1, recalling the ﬁrst and last n entries
of b are r, c, i.e. the row and column constraints. Recall we assume we have

(cid:13)
(cid:13)r( ˜X) − r
(cid:13)

(cid:13)
(cid:13)
(cid:13)1

+

(cid:13)
(cid:13)
(cid:13)c( ˜X) − c
(cid:13)
(cid:13)
(cid:13)1

≤ δ.

Clearly all operations in Algorithm 2 take O(n2) time. To explain brieﬂy, X (cid:48) is ﬁxed so that its
row sums are feasible (i.e. X (cid:48)1 ≤ r) and X (cid:48)(cid:48) is ﬁxed so that its column sums are feasible. Further,
entrywise X (cid:48)(cid:48) ≤ X (cid:48) ≤ ˜X, so X (cid:48)(cid:48) is feasible. We ﬁrst bound

d def=

(cid:13)
(cid:13)X (cid:48)(cid:48) − ˜X
(cid:13)

(cid:13)
(cid:13)
(cid:13)1



=









(cid:88)

ri( ˜X) − ri

 +



(cid:88)

cj(X (cid:48)) − cj

 .

i:ri( ˜X)>ri

j:cj (X (cid:48))>cj

Note

(cid:13)
(cid:13)r( ˜X) − r
(cid:13)

(cid:13)
(cid:13)
(cid:13)1

≥ (cid:80)

i:ri( ˜X)>ri

ri( ˜X) − ri. Further, by X (cid:48) ≤ ˜X entrywise,

(cid:88)

cj(X (cid:48)) − cj ≤

j:cj (X (cid:48))>cj

(cid:13)
(cid:13)
(cid:13)c( ˜X) − c
(cid:13)
(cid:13)
(cid:13)1

.

Thus d ≤ δ. ˆX ∈ Ur,c, since er, ec ≥ 0 and 1(cid:62)er = 1(cid:62)ec = e, so ˆX1 = r, ˆX (cid:62)1 = c. Also,

Finally,

(cid:13)
ˆX − ˜X
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)1

≤

(cid:13)
(cid:13)X (cid:48)(cid:48) − ˜X
(cid:13)

(cid:13)
(cid:13)
(cid:13)1

+

ˆX − X (cid:48)(cid:48)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1
(cid:13)

≤ δ + e.

e = 1 − 1(cid:62)X (cid:48)(cid:48)1 = 1 −

(cid:16)

(cid:17)
1(cid:62) ˜X1 − d

= d.

Thus using d ≤ δ proves the claim.

D Experiment details

Here, we give the implementation details for the experimental results discussed in Section 5, and a
brief justiﬁcation of experimental decisions we made.

Dataset. For both ﬁgures in Section 5, we had the following experimental setup. We randomly
sampled a pair of digits from the MNIST dataset corresponding to the digit 1, and added a small
amount of background noise for numerical stability, as is standard in the literature [AWR17]. We
downsampled the 28 × 28 pixel images to size 14 × 14 by skipping every other pixel to speed up
experiments. Similar performances were observed across multiple random instances. Finally, the
cost metric used was by Manhattan distance on the 2-dimensional grid.

Objective value. For simplicity, in all cases we measured objective value by the overestimate
presented in (4). By the proof of Lemma 1, this is an overestimate to the true objective after
performing the rounding procedure in Algorithm 2. In practice, we observed that this overestimate
was negligibly diﬀerent from the objective after rounding.

20

Sinkhorn implementation details. We implemented the standard Sinkhorn algorithm, using
diﬀerent settings of η−1. Sinkhorn iteration converges to an (cid:15)-approximate transportation plan in
theory when η is very large, roughly log n/(cid:15). However, in practice, it is observed that much smaller
values of η suﬃce for rapid convergence. We tracked the convergence of Sinkhorn iteration for
η = 70 and η = 5, which we considered close to a theoretically guaranteed parameter and a much
less conservative practical parameter, respectively. The optimized Sinkhorn algorithm converged at
rates much faster than the predicted (cid:15)−2 rate on all experiments, outperforming all other methods,
which we believe merits further investigation. Signiﬁcantly larger values of η led to numerical
stability issues when computing exp(−ηC).

APDAMD implementation details. We implemented the APDAMD algorithm (Algorithm 4
in [LHJ19]), with the quadratic regularizer (i.e.
2). We observed that the amount of the
quadratic regularizer added did not aﬀect the practical convergence of the algorithm. A simple
reason for this is because the algorithm builds in a more aggressive step-size strategy, because
the pessimistic γ = O(n) is often too conservative to be necessary in practice. The ﬁgure tracks
APDAMD convergence with η = 10−2, (cid:15) = 10−3.

2γ (cid:107)λ(cid:107)2

1

Mirror prox. For numerical stability considerations, we implemented our algorithm as an instance
of mirror prox [Nem04], another extragradient method which takes local iterations rather than
accumulating a dual operator and taking steps with respect to some ¯z (i.e. dual extrapolation).
Although there is not a known proof of mirror prox convergence with an area-convex regularizer, we
ﬁnd this decision reasonable for several reasons. In general, variations of entropic mirror descent are
well-known to be equivalent to their dual averaging versions; it is likely that a similar equivalence
can be drawn between mirror prox and extragradient dual averaging, i.e. dual extrapolation.
Furthermore, the standard proofs of dual extrapolation and mirror prox are quite similar; we
believe it is likely that area-convexity results in convergence for mirror prox, although this merits
further investigation.

Termination. We terminated our alternating minimization procedure when the movement of
iterations in (cid:96)1 was negligible. Typically, we observed that 3-5 alternating steps suﬃced for con-
vergence.
Step sizes. We varied two parameters in our experiments: the step size 1
κ used in our extragradient
algorithm, and the amount of entropy used in our regularizer (in the paper, we used 10 times entropy
compared to the quadratic component x(cid:62)A(cid:62)(y2)). One reason this may be reasonable in practice is
similar to the observed behavior of the Sinkhorn iteration tuning the η−1 parameter, and APDAMD
performing a more-aggressive line search for the observed amount of regularizer necessary. To this
end, we plotted the performance of three settings of our algorithm.

• In the “unoptimized constants”, we set the constants to roughly those with theoretical guar-

antees, i.e. 10 times entropy and step size 1.

• In the “reasonably optimized constants”, we set the amount of entropy to be 4, and the step
size to be (cid:107)d(cid:107)∞ /3, to oﬀset the (cid:107)d(cid:107)∞ multiple of the regularizer used in our iterations. For
smaller values of (cid:15), these settings compared favorably with APDAMD.

• In the “optimized constants”, we set the amount of entropy at 3, and the step size at (cid:107)d(cid:107)∞.
This setting outperformed APDAMD and was more competitive with Sinkhorn iteration.

Discussion. We believe multiple interesting avenues of exploration arise from our experiments.

• Sinkhorn with aggressively chosen η outperformed all other methods we benchmarked against,
and converged at rates faster than suggested by its known analyses. It may prove fruitful to

21

study if further assumptions about practical instances explain this discrepancy.

• Directly accelerated methods such as APDAMD also exhibit (cid:15)−1 convergence rates, at the cost
of a worse dependence on dimension. However, this worst-case dependence can be mitigated
if the instance is favorable in practice, i.e. by choosing γ ≈ O(1). This was observed to be the
case in our experiments for the MNIST dataset. It is interesting to see if a similar adaptive
tuning applies to our method with provable guarantees.

• Our method did not exhibit instability when changing the amount of entropy in the regularizer,
but it did exhibit vastly-improved convergence. It is possible that the amount of regularizer
needed is not quite so large, perhaps through a more careful analysis.

• We did not benchmark against the greedy Sinkhorn method of [AWR17], or consider numerical
speedups such as those in [ABRW18]. It remains open to explore if these practical speedups
are applicable to ﬁrst-order methods such as ours as well.

22

