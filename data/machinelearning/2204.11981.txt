2
2
0
2

r
p
A
5
2

]

G
L
.
s
c
[

1
v
1
8
9
1
1
.
4
0
2
2
:
v
i
X
r
a

END-TO-END MAPPING IN HETEROGENEOUS SYSTEMS USING
GRAPH REPRESENTATION LEARNING

Yao Xiao
USC
Los Angeles, CA, 90089
xiaoyao@usc.edu

Guixiang Ma
Intel Labs
Hillsboro, OR 97124
guixiang.ma@intel.com

Nesreen K. Ahmed
Intel Labs
Santa Clara, CA 95054
nesreen.k.ahmed2@intel.com

Mihai Capot˘a
Intel Labs
Hillsboro, OR 97124
mihai.capota@intel.com

Theodore Willke
Intel Labs
Hillsboro, OR 97124
ted.willke@intel.com

Shahin Nazarian
USC
Los Angeles, CA, 90089
shahin@usc.edu

Paul Bogdan
USC
Los Angeles, CA, 90089
pbogdan@usc.edu

1

Introduction

The recent technological advances have signiﬁcantly contributed to a rapid increase in the algorithmic complexity of
various applications, from digital signal processing to autonomous aerial, ground and underwater systems [1]. In order
to control and manage this increased algorithmic complexity, heterogeneous computing systems require intelligent,
ﬂexible, and highly efﬁcient programming strategies to provide high performance while minimizing energy costs [2, 3].
However, the current monolithic programming models and task mapping to compute engines do not fully exploit the
recent architectural innovations and can exacerbate the load imbalance and communication inefﬁciencies [4].

In order to fully utilize the capabilities of hardware platforms, the compilation of parallel programs requires expert
heuristics to decide how many threads to spawn and how to schedule them onto heterogeneous computing systems [5].
Due to workload imbalance, synchronization overhead, and resource sharing contention, the overall performance may
lead to sub-optimal executions.

To address these issues, we need to predict the optimal processor (e.g., CPU or GPU) using machine learning models to
provide better performance given a software kernel, which is formally deﬁned as the device mapping problem [6]. It is
analyzed and solved [5, 7, 8] by using machine learning approaches to outperform the inefﬁcient heuristics. However,
as applications become more diverse and complex, it is inefﬁcient to map them only onto one type of processors. For
example, autonomous car driving distributes the visualization and recognition tasks, consisting of many for loops, onto
cores in GPUs to provide higher parallelization. At the same time, sequential decisions based on if-else statements
require CPUs to provide a fast execution on a single critical thread. There is a tradeoff between GPUs and CPUs.

Therefore, to combine the beneﬁts of both CPUs and GPUs, as opposed to the traditional device mapping problem, we
formulate a new problem to be considered within the high performance computing and machine learning contexts:

Given a complex software application, the goal is to learn a mapping function that predicts which code segments
would run best on a speciﬁc hardware device in heterogeneous hardware platforms.

Computations in programs can be considered as a graph where each node represents a compute instruction and each
edge represents an information ﬂow from one instruction to another. This graph representation of programs enables
us to model the dynamic dependency structures of software programs and helps analyze program characteristics and
automatically compile programs in heterogeneous platforms. The automation is achieved via graph learning models to

Correspondence to: nesreen.k.ahmed@intel.com, pbogdan@usc.edu, and xiaoyao@usc.edu

 
 
 
 
 
 
Figure 1: Overview of the proposed Programmable Graph Learning framework (PGL). PGL constructs a dynamic
dataﬂow graph for each input software program via LLVM IR. PGL then utilizes a novel feature extraction algorithm
based on random walks and multi-fractal analysis to construct node features that capture the topological dependencies
and structures in dynamic dataﬂow graphs. These features are further used by a graph autoencoder to partition the
graph into clusters (i.e., software kernels) and a graph neural network model to predict the best hardware device for
each kernel.

predict the type of each program from an initial feature matrix. In order to obtain the representative feature matrix from
a graph, we apply multi-fractal analysis [9] to quantitatively measure the topological structures hidden in a graph.

To solve this challenging optimization problem, we propose a uniﬁed, end-to-end, programmable graph representation
learning (PGL) framework capable of mining the complexity of high level programs down to the universal IR, extracting
the speciﬁc computational patterns, and predicting which code segments run best on a speciﬁc core in heterogeneous
hardware platforms. The proposed PGL framework, shown in Figure 1, is ﬂexible and capable of working with various
graph representations of software codes (e.g., regardless of abstract syntax tree, data-control ﬂow graph). We also
propose and evaluate a dynamic data ﬂow graph representation constructed from a partially executed trace of a code,
where nodes represent LLVM intermediate representation (IR) instructions and edges represent control, data, and
memory dependencies, which can better identify the structural information ﬂow and capture memory dependencies.

We evaluate the proposed PGL framework on a heterogeneous platform consisting of 32 CPUs and 32 GPUs. The
GNN is ﬁrst trained with C styled kernels converted from OpenCL from seven benchmark suites to learn the weights of
GNNs. Next, we integrate the trained GNN model with the GAE into the framework and test new incoming applications.
Experimental results demonstrate a maximum speedup of 6.42x when compared to the thread-based execution and
2.02x higher compared to the state-of-the-art technique.

Contributions. Our main contributions are as follows:

• We formulate a new challenging system optimization problem to be considered in the areas of machine learning
and computing systems: Given a software program, the goal is to learn a mapping function that predicts which
code segment should run on which hardware device in a heterogeneous computing system.

• We propose a uniﬁed, end-to-end, programmable graph representation learning framework (PGL) that au-
tomatically maps the computations of complex software applications to the appropriate hardware device in
heterogeneous hardware platforms.

• The proposed PGL framework uses a novel topological feature extraction algorithm based on random walks
and multi-fractal graph analysis to capture the local topological structures of a graph obtained from a program
through advanced static and dynamic compiler analysis techniques.

2 Related Work

We summarize the related work into two areas: (1) deep learning models in compiler optimization, and (2) graph
representation learning for code representation.

Deep Learning in Compiler Optimization. Heuristics used in compilers require expert knowledge to optimize
programs on heterogeneous systems and often lead to sub-optimal performance due to synchronization overhead and

2

resource management. Machine learning techniques, in particular, deep learning methods, are being applied during the
optimization phase to generate efﬁcient machine code [10, 11, 2]. The recent work in [12] proposed an end-to-end deep
reinforcement learning (DRL) method for ML compiler graph optimizations where the learned policies are generalized
to new graphs and transferable to different tasks. [13, 14] proposed an end-to-end framework utilizing DRL for handling
loop vectorization. In addition, machine learning techniques are also used to optimize the execution time of tensor
computation graphs [15] as well as deep neural networks in TASO [16] and SOAP [17].

Graph Representation Learning for Code Representation. While many prior works have employed machine
learning methods from natural language processing to represent programs as sequence of lexical tokens [18, 5], recently
there emerged a number of graph-based machine learning works that aims to capture the structure of programs along
with the syntactic and semantic information in the graph representation [19, 20, 21]. It has been observed that the
graph-based representation learning strategies tend to have superior learning ability on the programs for many code
analysis tasks, such as code similarity learning [22], program classiﬁcation [23], etc. For instance, [21] uses abstract
syntax trees (ASTs) and control-data ﬂow graphs (CDFGs) independently to represent programs and apply GNNs for
learning predictive compiler tasks on these graphs, which outperforms the recurrent neural networks (RNNs) on the
token sequence representation of the programs. [7] models the program’s control, data and call dependencies as a graph,
and applies a GNN to learn representations from the graph for both node-level and graph-level tasks including compiler
analysis, program classiﬁcation and device mapping.

However, compared to previous frameworks, we propose a uniﬁed end-to-end programmable graph representation
learning (PGL) framework to solve the challenging task of heterogeneous device mapping. The framework has the
ofﬂine training and online inference, yet it is fully autonomous, it does not require programmer intervention. Therefore,
for a new platform with untrained machine learning models, we ﬁrst learn the optimal mapping of code segments by
adjusting the network weights. Once models are well trained, a new incoming program has to be compiled and executed
once, next driven into GAE and GNN to predict which code segments are suitable for CPUs or GPUs.

3 Programmable Graph-based Learning Framework (PGL)

In this section, we describe the proposed PGL framework, which consists of four steps as shown in Figure 1. The
descriptions of these steps are detailed in the next sections. Section 3.1 discusses the general approach to transform an
application into a dynamic dataﬂow graph. Then, in Section 3.2, we develop a novel node feature extraction algorithm
based on random walks and multi-fractal graph analysis to quantitatively measure the local fractal structures of a
graph. Sections 3.3.1 and 3.3.2 discuss the GAE graph partitioning and GNN heterogeneous device mapping prediction,
respectively.

3.1

Input Program Modelling

Recently, various graph representations were proposed to represent and capture the latent information ﬂow in a program
(e.g., abstract syntax tree (AST) [19], contextual ﬂow graph (XFG) [20], and control and data ﬂow graph (CDFG)
[21]). These graph representations allow the compiler to analyze the effectiveness and correctness of programs, as well
as enable parallel programming via graph partitioning in high performance computing [4]. However, these statically
compiled graphs have several limitations. First, memory dependencies are difﬁcult to be identiﬁed. If not handled
properly, this can exacerbate the data communication overhead and reduce the application performance. Second, the
number of iterations in for and while loops cannot be statically determined. This plays a signiﬁcant role in predicting
whether the code is running in either CPU or GPU based on the workload. For example, if the number of iterations is
small, it is ideal to run the code in CPU, because of the faster clock frequency. Otherwise, GPU is preferred because
the number of cores on each chip is much denser to provide higher parallelism. Therefore, in order to overcome
these drawbacks, we use information generated from static compiler analysis and dynamic compilation to model the
information ﬂow in high-level programs as a dynamic dataﬂow graph. Next, we propose the following representation.

Deﬁnition 3.1 (DYNAMIC DATAFLOW GRAPH). A dynamic dataﬂow graph is a weighted directed acyclic graph
G = (V, E, W ), where each node v, associated with an attribute va indicating the type of the node (e.g., add, sub, store,
or load), (v, va) ∈ V represents an LLVM IR instruction; each edge e, associated with an attribute ea indicating the
type of dependencies (e.g., control, data, or memory), (e, ea) ∈ E represents a dependency between two instructions; a
weight w ∈ W on each edge e represents the amount of data communication between two instructions and the time to
execute the instruction. It allows us to quantify communication overhead in the memory hierarchy with L1, L2, and L3
caches.

3

To construct these dynamic dataﬂow graphs, we ﬁrst col-
lect the representative dynamic trace generated from ex-
ecuting a program. This trace contains a sequence of
LLVM IR instructions to be executed. Then, for each in-
struction, we check if one of the following dependencies
exists and insert a directed edge to construct the graph:

• Data dependency: Source registers of the cur-
rent instruction depend on the destination regis-
ters of the previous instructions.

• Control dependency: Source registers of the
function calls and branches depend on the des-
tination register of the previous instructions.

• Memory dependency: Memory locations of cur-
rent store-load instruction are the same as the
previous store-load instructions. We perform
this memory alias analysis using "-basicaa -aa-
eval -print-allalias-modref-info" in the LLVM
environment.

Figure 2: An example of a dynamic dataﬂow graph from
a neural network with one hidden layer. Each node repre-
sents an LLVM IR instruction and each edge represents a
dependency between two instructions.

Figure 2 shows the graph representation of forward propagation in a neural network with one hidden layer. Note that a
node is an LLVM IR instruction, not an operand or a high level language (e.g., C/C++, Java) statement. Different from
AST, XFG, and CDFGs, this speciﬁc graph representation in Figure 2 makes explicit some hidden program information
ﬂows from the execution trace generated at run-time and analyzed via data, control, and memory dependencies. One
most recurring pattern is the cone structure due to the LLVM IR "getelementptr" generated from the pointers in
f or-loops to distribute data to different iterations.

3.2 Feature Extraction

Each node in a GNN is associated with numerous features, which are further used for clustering or classiﬁcation to
make decisions at node level or graph level. In the literature, the code2vec [19] and inst2vec [20] are commonly used to
extract features by encoding programs via AST paths. However, the trained representations can put larger weights on
names rather than code structure, which can lead to misclassiﬁcation.

In order to exploit the graph structural information ﬂow of programs, random walks reason about the number of adjacent
nodes and the density of connections around a node [24]. A random walk is deﬁned as a series of nodes, starting from
n0, the jth node is generated by the following distribution with a ﬁxed length l.

P (nj = j|ni = i) =

(cid:40) wij
(cid:80)

j wij

0

if (i, j) ∈ E

otherwise

(1)

where wij is the edge weight between node i and node j. In addition, multifractal analysis mathematically studies
the structural complexity and topological heterogeneity of graphs [9]. The multifractal properties such as generalized
fractal dimensions provides the higher order statistics of a graph, which can be quantiﬁed by a ﬁnite box-covering
method. That is, to study the different fractal structures in a graph, the box-covering method uses the box of the same
size to cover the graph and then studies the relationship of the size of a box (l) and the number of nodes in the ith box
of size l (Ni(l)) as

(cid:88)

Ni(l)q ∼ lτ (q)

(2)

i

where q is the distortion factor to differentiate the topological difference of fractal structures, and τ (q) is the mass
exponent. Next, we can obtain the generalized fractal dimensions D(q) from τ (q), which characterizes the different
fractal structures of a graph.

D(q) =

τ (q)
q − 1

(3)

Therefore, to mine the local and scale dependent topological properties of programs, we propose Algorithm 1 which
exploits random walks and multifractal concepts for encoding topological interdependencies (See the Appendix A
for the full details of Algorithm 1.). Random walks explore the local topological density around a node i in a graph
by ﬁnding random paths starting from node i to node j. Once a random path is identiﬁed, we backtrack to the ﬁnal
destination node j to ﬁnd the subgraph SG starting from i to j. Next, we perform a multifractal analysis on the subgraph

4

SG to estimate its generalized fractal dimension. The time complexity of Algorithm 1 is bounded by the Dijkstra
strategy to ﬁnd the shortest path for each node to every other nodes, which is O(ElogV ), where E and V are the
number of edges and nodes, respectively. Finding all shortest paths in a graph has a time complexity of O(EV logV ).

3.3 Graph Representation Learning

Once we extracted the initial node features from the dynamic dataﬂow graph, we design a deep graph representation
learning module with GNNs [25] for the graph partition and device mapping prediction problem. Speciﬁcally, we
propose to use a graph autoencoder (GAE) for partitioning the graph into kernels and a GNN to predict the correct label.

3.3.1 GAE-based Graph Partitioning

Graph auto-encoders (GAEs) [26] are a category of GNNs that aims at representing nodes into low-dimensional vectors
in an unsupervised training fashion. They are different from other GNNs that are typically used for supervised or
semi-supervised learning tasks. In our framework, the goal of the graph partitioning stage is to obtain a good partition
for each LLVM graph based on a learned representation that captures the intrinsic structural information of the graph,
such that the subgraphs preserve the inherent characteristics of the data, control and memory dependencies in the LLVM
graph. To this end, we propose a graph partitioning strategy based on the GAE [27] and spectral clustering [28] for our
task, as shown in Appendix B.

3.3.2 GNN-based Device Mapping Prediction

Once the graph is partitioned into different clusters/kernels, next for each kernel, we use a GNN to predict the correct
platform to execute the kernel by updating the node vectors iteratively in a similar fashion to the message passing. Note
that our proposed PGL is a general framework that can leverage various GNN models for the device mapping prediction
stage, whereas in this paper, we adopt three different variants of the GNN models: GCN [29], graph attention network
(GAT) [30, 31] and gated graph neural network (GGNN) [32], respectively, for this task discussed brieﬂy in Appendix
B. We also empirically investigate the comparative effectiveness of these GNN strategies in representation learning on
the partitioned LLVM graphs for the graph classiﬁcation task in heterogeneous device mapping.

4 Experiments

Setup. Given a software program, our goal is to identify the subgraphs (i.e., code segments) that are optimal to run on
CPUs or GPUs.1 Our developed end-to-end framework discussed in the previous section consists of two components: a
GAE and a GNN. Unsupervised learning model GAE is used to partition the new complicated program into several
clusters / kernels to be mapped onto heterogeneous systems. Supervised learning model GNN predicts the correct
label for each kernel. In the implementation, we use kernels written in OpenCL [5] as training and testing data with
5-fold cross validation for the GNN model. The ground-truth labels are either CPU or GPU for the kernels. In order
to evaluate the PGL framework, we ﬁrst use the GAE model to partition the graphs, to ﬁnd kernels suitable for either
CPU or GPU. Next, different GNN models are used to predict the correct label to the underlying hardware. Table 3 (in
Appendix C.1) lists the conﬁguration parameters of the heterogeneous system used in this section.

Datasets. We start by using the 256 heterogeneous device mapping OpenCL kernels in [5] for training and validation
of GNNs. These kernels are labelled with CPU vs. GPU; we use the NVIDIA dataset. We then manually convert these
kernels to C code. Furthermore, we use standard application benchmarks in Table 4 (in Appendix C.2) to validate the
overall PGL framework.

Baseline Comparisons. When comparing the accuracy of the prediction results from GNN models, we use the
following: (1) GCN; (2) GAT; and (3) GGNN. We compare our graph representation to the ProGraML [7] graph
representation, NCC [20], and DeepTune [5], state-of-the-art techniques to represent programs as graphs. To quantify the
beneﬁts of graph partitioning, we compare the PGL framework with the following baselines in terms of the application
performance: (1) K-means clustering connected with GCNs (KM+GCN); (2) hierarchical divisive clustering where all
observations start in one cluster, and divisions are performed recursively as one moves down the hierarchy, connected
with GCNs (HDC+GCN); (3) modularity-based community detection where an optimization model is proposed to
measure the structure of graphs [33, 4], connected with GCNs (MOD+GCN); (4) METIS graph partitioning [34]
connected with GCNs (METIS+GCN); (5) feed-forward neural network, connected with GCNs [3] (NN+GCN). In
addition, we compare the PGL framework in terms of the application performance with the following baselines: (1)

1Performance varies by use, conﬁguration and other factors. Learn more at www.Intel.com/PerformanceIndex.

5

Figure 3: Ablation study on the impact of various parameters in PGL on the overall accuracy: the number of random
walkers, the cut-off range, GNN model, the number of neurons in a hidden layer, different graph features, and GNN
architecture.

threads in parallel programming (PAR); (2) modularity based community detection to partition the graph into clusters
and a heuristic mapping [4] (CommDet); (3) sliding window based neural network to locate specialized structures with
a reinforcement learning based mapping (NN+RL) [3]; (4) gem5-aladdin, an end-to-end SoC simulation [35].

4.1 Graph Representation Comparison

In order to validate the effectiveness of PGL, we compare
it with state-of-the-art techniques in terms of the accu-
racy of the prediction results on the same dataset [5]. We
compare against the DeepTune and NCC using the code
released by their authors. We also compare our graph rep-
resentation against the ProGraML graph representation
by extracting ProGraML graphs from the C versions of
the kernels and training a GGNN on the graphs. As we
can see from Table 1, DeepTune and NCC can only give

6

DeepTune
NCC

Table 1: Comparison of graph representations.
Precision Recall
0.71
0.80
0.85
0.90
0.92
0.94

Accuracy
67.31% ± 3.89%
76.57% ± 3.13%
ProGraML-GGNN 80.83% ± 3.37%
86.38% ± 2.78%
89.65% ± 2.24%
91.39% ± 2.59%

PGL-GCN
PGL-GAT
PGL-GGNN

0.72
0.80
0.85
0.90
0.92
0.94

F1
0.71
0.80
0.85
0.90
0.92
0.94

less than 80% accuracy, whereas the ProGraML represen-
tation provides 80.83% accuracy. However, according to neural architecture search, PGL can provide up to 91.39%
accuracy. The reason is that our graph representation of a program differs from the same program with different number
of iterations in for loops. This enables the graph representation with more information.

4.2 Ablation Study

In this section, we measure different accuracy results from different parameters in PGL by running each experiment
5 times for different number of neurons in the hidden layer, as shown in Figure 3 The default parameters include
16 random walkers, cut-off range in multifractal analysis to be the diameter of the graph, GGNN graph model, 64
neurons in a hidden layer, and its architecture to be one input, hidden, and output layer. In the experiments, we vary one
parameter while ﬁxing the rest. In the end, we compare the range and distribution of the accuracy for each parameter.

Random Walkers. We ﬁrst vary the number of random walkers from 2 to 64 to show how much contribution in
random walks. As we can see, increasing the number has a diminishing return beyond 16 walkers as the median accuracy
starts at around 73% at 2 walkers and reaches over 91% at 16 walkers. It is because when the number of walkers
becomes large, some walkers may visit the same nodes, which leads to the same features in multifractal analysis.

Cut-off Range in Multifractal Analysis. Next, we vary the cut-off range in multifractal analysis to illustrate how
important is multi-fractal analysis. Cut-off range is used in the Dijkstra algorithm in multifractal analysis and deﬁned as
the length (sum of edge weights) at which the search is stopped. Therefore, controlling the range allows us to exploit
the local structures around a node. In the experiment, we vary it from 2 to 64. As we can see, when the cut-off range is
only 2, the accuracy is only 55.97%. It is because the multifractal analysis in this case does not provide meaningful
features to the GNN model. However, as we increase the cut-off range to 64, which is the upper bound of network
diameters in the dataset, the accuracy reaches to over 90% because the multifractal analysis has the full visibility of the
graphs and is able to ﬁnd the correct features.

GNN Model. PGL provides common interfaces to connect GNNs to the rest of the pipeline. It is ﬂexible enough to
support different GNN models. Therefore, in this experiment, we choose three commonly used models to be analyzed,
namely, GCN, GAT, and GGNN. As we can see, GGNN provides the highest accuracy with smallest standard deviation
(on average over 92%) whereas GCN and GAT can only provide 81.56% and 86.4% on average, respectively. This is
mainly due to the fact that GGNN uses the gated recurrent unit (GRU) for long-term propagation of information across
a graph structure [32], which enables it to better capture long-range dependencies from the code graphs compared to
GCN and GAT.

Neuron Count. We validate different combinations of parameters with respect to the number of neurons in a hidden
layer on the ﬁnal accuracy, i.e., (GCN, multifractal), (GAT, multifractal), (GGNN, multifractal), and (GGNN, degree). In
general, 64 or 128 neurons provide higher accuracy compared to others. Especially in the case of (GGNN, multifractal),
128 neurons provide smaller standard deviation compared to others. We believe it is because that when using a too
small or too large number of neurons, the models cannot accurately learn the hidden structures of a graph.

Graph Feature Embedding. Next, we vary the graph
features from node degree and weights to multifractal
properties, using the default GGNN architecture. As
we can see from Figure 3, multifractal features can pro-
vide at most 93.98% accuracy and over 90% on average
whereas degree and weight features can only achieve at
most 82.51% and 88.66% accuracy, respectively. This
validates that our proposed graph feature extraction algo-
rithm mentioned in Algorithm 1 can exploit the topologi-
cal structures of a graph and ﬁnd the local information around nodes.

Table 2: Comparison of feature extraction algorithms.
Node Feature
Degree
Weight
inst2vec
PGL

Precision Recall
0.51
0.73
0.77
0.95

Accuracy
48.54% ± 4.33%
68.93% ± 3.32%
75.7% ± 3.51%
91.23% ± 2.75%

F1
0.53
0.73
0.78
0.95

0.51
0.73
0.76
0.95

In addition, we compare the proposed feature extraction algorithm based on random walk and multifractal analysis
concepts rather than simply using the node degree, or edge weight as a feature, and the state-of-the-art inst2vec [20]
on the same GCN architecture to validate the effectiveness of the proposed algorithm. As we can see in Table 2,
simple node features such as degree and edge weight cannot guarantee stable prediction results on the testing graph
data as it only provides up to 72.2% accuracy. Compared with the state-of-the-art learnable representation of code
semantics inst2vec, our feature extraction strategy can provide 14.77% higher accuracy due to the fact that the trained

7

representation of inst2vec puts large weights on semantics rather than the code structure. Therefore, our algorithm can
achieve better results by quantifying local structures of the code.

GNN Architecture. Finally, in order to see the impact
of deep GNNs on the ﬁnal accuracy, we vary the number
of hidden layers from 1 to 3. We observe that the average
accuracy is decreasing (92.43%, 90.19%, and 89.37%)
and the standard deviation is increasing (1.15%, 1.29%,
and 1.44%) when the number of hidden layers increases.
It is mainly due to the over-squashing issue that tends
to occur when increasing the number of layers in GNNs.
This causes the information on graphs is compressed and
fails to learn long-range signals [36].

4.3 Parameter Tuning

As we can see from the ablation study, many hyperpa-
rameters could have a signiﬁcant impact on the overall
accuracy of the model we are training. Therefore, in or-
der to ﬁnd optimal values for the parameters of the GNN
model which is later used in the application-level evaluation in Section 4.4, we rely on grid search to automate parameter
tuning. For each parameter discussed in Section 4.2, we select a range of values to search for and use GridSearchCV to
improve the accuracy of our model. Experimental results suggest that the number of random walkers be 16, cut-off
range be 128, the GNN model be GGNN, the number of neurons be 64, the graph feature be multifractal properties, and
the number of hidden layers be 1.

Figure 4: The impact of training on the performance of
different applications.

4.4 Application-level Evaluation

Training. First, in order to see how the training step has
an impact on the overall accuracy, for each application,
we vary the number of epochs used during training of the
GGNN model and directly use the model to predict labels
for clusters generated from GAE. Finally, we measure
the application performance executed in a heterogeneous
platform in Table 3 to ﬁgure out how well the model is
trained. As we can see from Figure 4, compared to ran-
domly selecting a label for each cluster (slow execution
at 10%-30% training), a fully trained model can provide
up to 3.8x performance improvement.

Figure 5: Comparison of different partitioning algorithms.

Graph Partitioning. Next, in order to validate the ad-
vantages of the GAE used to partition the large input
application into small kernels in the PGL framework, we
ﬁx the graph neural network as GCN with two hidden lay-
ers and 32 neurons per layer, which is used to predict the
correct label for each kernel. We compare the GAE with
different partitioning algorithms such as K-means (KM),
hierarchical divisive clustering (HDC), modularity-based
community detection (MOD), METIS, and feed-forward
neural network (NN) in terms of the total application
execution speedup. As shown in Figure 5, for the par-
titioning models without machine learning such as KM,
HDC, MOD, and METIS, the normalized execution speedup is smaller compared to the learning models such as NN
and GAE. It is mainly because the kernels after graph partitioning are not well recognized by the GCN model. For the
learning models, GAE outperforms NN by up to 32% in a sense that the GAE takes into account the graph structures of
code.

Figure 6: Comparison of different frameworks.

The PGL Framework. The proposed PGL framework is able to predict which code segments run best on a speciﬁc
processor. Therefore, in order to validate the framework including the GAE and GNN models, we use the trained

8

models discussed in Section 3 to predict each application in Table 4. As shown in Figure 6, we use the traditional
thread based parallel programming running on CPUs as our baseline and compare the PGL framework with community
detection, neural network with reinforcement learning, and gem5-aladdin. We observe that the PGL framework can
provide up to 6.42x speedup compared to the baseline and 2.02x speedup higher compared to the state-of-the-art.

5 Conclusion

We proposed an end-to-end learnable PGL framework to predict which code segments run best on a speciﬁc hardware
device. We ﬁrst develop a node feature extraction algorithm based on random walks and multifractal analysis concepts
to quantify the local structures of a program. Next, we build the GAE together with a decoder and spectral clustering to
ﬁnd cluster partition from the distance matrix. Then, we use graph neural networks as the learning model to predict the
type of each cluster. Our evaluation on 32 CPUs and 32 GPUs concludes that the PGL framework can provide up to
6.42x speedup compared to the baseline and 2.02x higher speedup compared to the state-of-the-art technique.

References

[1] Srivatsan Krishnan, Behzad Borojerdian, William Fu, Aleksandra Faust, and Vijay Janapa Reddi. Air learning:
An ai research platform for algorithm-hardware benchmarking of autonomous aerial robots. arXiv preprint
arXiv:1906.00421, 2019.

[2] Ameer Haj-Ali, Nesreen K Ahmed, Ted Willke, Joseph Gonzalez, Krste Asanovic, and Ion Stoica. A view on

deep reinforcement learning in system optimization. arXiv preprint arXiv:1908.01275, 2019.

[3] Yao Xiao, Shahin Nazarian, and Paul Bogdan. Self-optimizing and self-programming computing systems: A
combined compiler, complex networks, and machine learning approach. IEEE Transactions on Very Large Scale
Integration (VLSI) Systems, 27(6):1416–1427, 2019.

[4] Yao Xiao, Yuankun Xue, Shahin Nazarian, and Paul Bogdan. A load balancing inspired optimization framework
for exascale multicore systems: A complex networks approach. In 2017 IEEE/ACM International Conference on
Computer-Aided Design (ICCAD), pages 217–224. IEEE, 2017.

[5] Chris Cummins, Pavlos Petoumenos, Zheng Wang, and Hugh Leather. End-to-end deep learning of optimization
heuristics. In 2017 26th International Conference on Parallel Architectures and Compilation Techniques (PACT),
pages 219–232. IEEE, 2017.

[6] Azalia Mirhoseini, Hieu Pham, Quoc V Le, Benoit Steiner, Rasmus Larsen, Yuefeng Zhou, Naveen Kumar,
Mohammad Norouzi, Samy Bengio, and Jeff Dean. Device placement optimization with reinforcement learning.
In International Conference on Machine Learning, pages 2430–2439. PMLR, 2017.

[7] Chris Cummins, Zacharias Fisches, Tal Ben-Nun, Torsten Hoeﬂer, Michael O’Boyle, and Hugh Leather. Pro-
In
GraML: A Graph-based Program Representation for Data Flow Analysis and Compiler Optimizations.
International Conference on Machine Learning (ICML), 2021.

[8] Dominik Grewe, Zheng Wang, and Michael FP O’Boyle. Portable mapping of data parallel programs to opencl
for heterogeneous systems. In Proceedings of the 2013 IEEE/ACM International Symposium on Code Generation
and Optimization (CGO), pages 1–10. IEEE, 2013.

[9] Yuankun Xue and Paul Bogdan. Reliable multi-fractal characterization of weighted complex networks: algorithms

and implications. Scientiﬁc reports, 7(1):1–22, 2017.

[10] Amir H Ashouri, William Killian, John Cavazos, Gianluca Palermo, and Cristina Silvano. A survey on compiler

autotuning using machine learning. ACM Computing Surveys (CSUR), 51(5):1–42, 2018.

[11] Mingzhen Li, Yi Liu, Xiaoyan Liu, Qingxiao Sun, Xin You, Hailong Yang, Zhongzhi Luan, Lin Gan, Guangwen
Yang, and Depei Qian. The deep learning compiler: A comprehensive survey. IEEE Transactions on Parallel and
Distributed Systems, 32(3):708–727, 2020.

[12] Yanqi Zhou, Sudip Roy, Amirali Abdolrashidi, Daniel Wong, Peter Ma, Qiumin Xu, Hanxiao Liu,
Mangpo Phitchaya Phothilimtha, Shen Wang, Anna Goldie, et al. Transferable graph optimizers for ml compilers.
arXiv preprint arXiv:2010.12438, 2020.

[13] Ameer Haj-Ali, Nesreen K Ahmed, Ted Willke, Yakun Sophia Shao, Krste Asanovic, and Ion Stoica. Neu-
rovectorizer: End-to-end vectorization with deep reinforcement learning. In Proceedings of the 18th ACM/IEEE
International Symposium on Code Generation and Optimization, pages 242–255, 2020.

[14] Ameer Haj-Ali, Nesreen K Ahmed, Ted Willke, Sophia Shao, Krste Asanovic, and Ion Stoica. Learning to
vectorize using deep reinforcement learning. In Neurips workshop on Machine Learning for Systems., 2019.

9

[15] Yuu Jinnai, Arash Mehrjou, Kamil Ciosek, Anna Mitenkova, Alan Lawrence, Tom Ellis, Ryota Tomioka,

Simon Peyton Jones, and Andrew Fitzgibbon. Knossos: Compiling ai with ai. 2019.

[16] Zhihao Jia, Oded Padon, James Thomas, Todd Warszawski, Matei Zaharia, and Alex Aiken. Taso: optimizing
deep learning computation with automatic generation of graph substitutions. In Proceedings of the 27th ACM
Symposium on Operating Systems Principles, pages 47–62, 2019.

[17] Zhihao Jia, Matei Zaharia, and Alex Aiken. Beyond data and model parallelism for deep neural networks. arXiv

preprint arXiv:1807.05358, 2018.

[18] Anh Tuan Nguyen, Trong Duc Nguyen, Hung Dang Phan, and Tien N Nguyen. A deep neural network language
model with contexts for source code. In 2018 IEEE 25th International Conference on Software Analysis, Evolution
and Reengineering (SANER), pages 323–334. IEEE, 2018.

[19] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. code2vec: Learning distributed representations of code.

Proceedings of the ACM on Programming Languages, 3(POPL):1–29, 2019.

[20] Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoeﬂer. Neural code comprehension: A learnable

representation of code semantics. arXiv preprint arXiv:1806.07336, 2018.

[21] Alexander Brauckmann, Andrés Goens, Sebastian Ertel, and Jeronimo Castrillon. Compiler-based graph repre-
sentations for deep learning models of code. In Proceedings of the 29th International Conference on Compiler
Construction, pages 201–211, 2020.

[22] Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching networks for learning
the similarity of graph structured objects. In International Conference on Machine Learning, pages 3835–3845.
PMLR, 2019.

[23] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. Convolutional neural networks over tree structures for
programming language processing. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 30,
2016.

[24] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd
ACM SIGKDD international conference on Knowledge discovery and data mining, pages 855–864, 2016.

[25] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive

survey on graph neural networks. IEEE transactions on neural networks and learning systems, 2020.

[26] Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong

Sun. Graph neural networks: A review of methods and applications. arXiv preprint arXiv:1812.08434, 2018.
[27] Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308, 2016.
[28] Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395–416, 2007.
[29] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv

preprint arXiv:1609.02907, 2016.

[30] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph

attention networks. arXiv preprint arXiv:1710.10903, 2017.

[31] John Boaz Lee, Ryan A Rossi, Sungchul Kim, Nesreen K Ahmed, and Eunyee Koh. Attention models in graphs:

A survey. ACM Transactions on Knowledge Discovery from Data (TKDD), 13(6):1–25, 2019.

[32] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv

preprint arXiv:1511.05493, 2015.

[33] Santo Fortunato. Community detection in graphs. Physics reports, 486(3-5):75–174, 2010.

[34] Dominique LaSalle, Md Mostofa Ali Patwary, Nadathur Satish, Narayanan Sundaram, Pradeep Dubey, and George
Karypis. Improving graph partitioning for modern graphs and architectures. In Proceedings of the 5th Workshop
on Irregular Applications: Architectures and Algorithms, pages 1–4, 2015.

[35] Yakun Sophia Shao, Sam Likun Xi, Vijayalakshmi Srinivasan, Gu-Yeon Wei, and David Brooks. Co-designing
accelerators and soc interfaces using gem5-aladdin. In 2016 49th Annual IEEE/ACM International Symposium on
Microarchitecture (MICRO), pages 1–12. IEEE, 2016.

[36] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. arXiv preprint

arXiv:2006.05205, 2020.

[37] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation.
arXiv preprint arXiv:1406.1078, 2014.

10

[38] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.
[39] Guixiang Ma, Yao Xiao, Theodore Willke, Nesreen Ahmed, Shahin Nazarian, and Paul Bogdan. A distributed
graph-theoretic framework for automatic parallelization in multi-core systems. Proceedings of Machine Learning
and Systems, 3:550–568, 2021.

11

Appendix

A Random Walk based Fractal Analysis

for K times do

/* Perform a random walk */
for D times do

Calculate the probability of the transition to node j
Select the next node and set it as the current node

Algorithm 1 Random Walk based Fractal Analysis
1: INPUTS: An LLVM graph G with N nodes, K random walks, D walk length, Q distortion factors
2: OUTPUT: N by (K × Q) node features F
3: Create a feature matrix F of size N by K × Q
4: for each node indexed by i in the graph G do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19: end for

end for
/* So far we ﬁnd the destination node denoted as j */
/* Find the subgraph SG starting from i to j */
Backtrack the node j to ﬁnd all nodes until the node i
/* Find the generalized fractal dimension from the SG */
Set the distortion factor q to be a vector of -10 to 10
w = GFD(SG, q)
F [i].append(w)

end for

Calculate the shortest path length from i to every node
Calculate the ratio of nodes to be covered with a box size l

20: Function: GFD
21: INPUTS: a graph G, distortion factor q of size Q
22: OUTPUT: generalized fractal dimension of size Q
23: diameter = Diam(G)
24: for each node i in the graph G do
25:
26:
27: end for
28: for each qv in q do
29:
30:
31: end for
32: gfd = tau / (q - 1)

Apply linear regression to ﬁnd the exponent τ in Eq. (2)
tau.append(τ )

B GAE Partitioning

Algorithm 2 GAE Partitioning
1: INPUTS: A graph G and a feature matrix X
2: OUTPUT: A cluster partition
3: repeat
4:
5:
6:
7: until 99% of nodes in the partition are stabilized.

Perform the GAE with two graph convolutional layers to get the embedding Z
Calculate the symmetric distance matrix D by (cid:98)A = ZZ T , D = 1
Obtain the partition via spectral clustering on D

2 (| (cid:98)A| + | (cid:98)A|(cid:62))

Given the graph G = (V, E) with an adjacency matrix A and node features in an N × D matrix X, we apply the graph
auto-encoder (GAE) model introduced in [27] with two graph convolutional layers. We calculate embeddings Z and the
reconstructed matrix (cid:98)A as follows:

(cid:98)A = σ(ZZ(cid:62)), with Z = GCN (X, A)

(4)

After we obtain the node embeddings via GAE, we use spectral clustering [28] on the node embeddings for the graph
partitioning. The overall workﬂow of this stage is shown in Algorithm 2. Speciﬁcally, we ﬁrst perform the GAE with

12

two graph convolutional layers to learn the latent embedding Z. Next, we maintain an inner product decoder (cid:98)A = ZZ(cid:62)
to learn the pairwise distance between nodes. We then perform spectral clustering after calculating the symmetric and
non-negative distance matrix D = 1

2 (| (cid:98)A| + | (cid:98)A|(cid:62)).

B.1 Graph Neural Networks

Graph Convolutional Network. We consider a multi-layer graph convolutional network (GCN) with the layer-wise
propagation rule proposed in [29]. Assume H(l) ∈ Rn×d is the matrix of activations in the l-th layer, according to the
propagation rule, we have

H(l+1) = σ( (cid:101)D− 1

2 (cid:101)A (cid:101)D− 1

2 H(l)Θ(l))

(5)

where (cid:101)A = A + In is the weight matrix of the graph, In is an identity matrix, D ∈ Rn×n is the diagonal degree matrix
of the graph, and σ(·) denotes the activation function. Θ is a layer-speciﬁc trainable weight matrix. H(0) = X is the
input feature matrix of the graph.

Graph Attention Network. Unlike the GCNs discussed above, where the node neighborhoods are aggregated with
equal weights, graph attention network (GAT) introduces an attention mechanism into GCNs to allow variance in the
inﬂuences of neighbors. We use the GAT introduced in [30, 31] and deﬁne the propagation rule in GAT as:

h(l+1)
i

= σ(

(cid:88)

ij h(l)
α(l)

j Θ(l))

j∈ (cid:101)N (i)

where α(l)

ij is node vi’s attention to node vj in the l-th layer:

α(l)

ij =

exp(LeakyReLU(F(h(l)
k∈ (cid:101)N (i) exp(LeakyReLU(F(h(l)

(cid:80)

i Θ(l), h(l)

j Θ(l))))

i Θ(l), h(l)

k Θ(l))))

(6)

(7)

where (cid:101)N (i) is the set of neighboring nodes of node i in the graph, F(·, ·) is a function to be learned. In our experiments,
we use a single-layer feedforward neural network for the attention mechanism parameterized by a weight vector, and
apply the LeakyReLU nonlinearity.

Gated Graph Neural Network. Gated graph neural network (GGNN) incorporates gate mechanism like Gate
Recurrent Units (GRU) [37] or Long Short-term Memory (LSTM) [38] in the propagation stage in the GNN models to
improve the long-term information propagation across the graph. In this work, we use the GGNN with GRU introduced
in [32] and deﬁne the recurrence of the propagation as follows:

i [ht−1
at
i = AT
1
i = σ(Wzat
zt
rt
i = σ(Wrat
i = tanh(Wat
(cid:101)ht
i = (1 − zt
ht

N ]T + b
)

· · · ht−1
i + Uzht−1
i + Urht−1
)
i (cid:12) ht−1
i + U(rt
i
i (cid:12) (cid:101)ht
i

i) (cid:12) ht−1

i + zt

i

i

))

where the node i ﬁrst aggregates message from its neighbors, and Ai is the sub-matrix of the graph adjacency matrix A,
which denotes the connections between node i and its neighbors. z and r are the update and reset gates, respectively.
As such, the GRU update functions incorporate information from the other nodes and from the previous timestamp to
update the hidden state of each node i.

Once the feature embedding is learned from the GNN models, we use two fully connected feed-forward neural network
layers to predict the correct label for each kernel.

13

C Experiment Setup and Benchmarks

C.1 Experiment Setup

l

CPU

GPU

Network

Table 3: Conﬁguration parameters

Cores
Clock frequency
L1 private cache

L2 shared cache
Memory
Core
Clock frequency
Memory capacity
Memory bandwidth
Topology
Routing algorithm
Flow control

32 cores, 16 MSHRs
2.4 GHz
64KB, 4-way associative
32-byte blocks
256KB, distributed
4 GB, 8 GB/s bandwidth
32
575 MHz
768 MB
86.4 GB/s
Mesh
XY routing
Virtual channel ﬂit-based

C.2 Benchmarks

Table 4: Applications and descriptions. We use the following eight benchmarks to validate the beneﬁts of the PGL
framework whereas we use the dataset [5] to train the graph neural network in the framework.

Application Description
dijkstra
fft
kmeans
mandel
md
nn
neuron
cnn

Find the shortest path
Fast Fourier transform
K cluster partitioning
Calculate Mandelbrot set
Molecular dynamics
Neural network
ReLU neurons
Conv. neural network

Input Size
100 nodes
vector of size 4096
256 2D tuples
4092 points
1024 particles
5 hidden FC layers
1024 neurons
conv-pool-FC

Table 5: Application graph statistics.

Application No. Nodes No. Edges Avg Path Length
dijkstra
fft
kmeans
mandel
md
nn
neuron
cnn

588,046
572,053
839,125
260,042
2,361,213
286,714
1174,843
520,596

502,897
456,183
705,184
235,051
1,799,353
227,766
987,184
361,464

17.36
15.54
22.18
11.67
34.29
19.22
52.75
13.34

D Run-time Mapping

At run-time, the tasks generated from each application are mapped onto the system. The mapping exploits and optimizes
the parallelism while considering data communication between tasks and resource utilization [39].

In order to improve performance, the run-time mapping should exploit and optimize the parallelism while considering
data communication between tasks and resource utilization. Therefore, the run-time mapping algorithm takes as inputs
the tasks, their interactions, and data communication and schedules a mapping from tasks to processors with the
objective of improving application performance. If data are transferred between two different tasks, then the greedy
run-time scheduler tried to allocate two available cores that are the closest based on the Manhattan distance.

14

