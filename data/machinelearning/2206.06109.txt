2
2
0
2

n
u
J

3
1

]

C
O
.
h
t
a
m

[

1
v
9
0
1
6
0
.
6
0
2
2
:
v
i
X
r
a

MARKOV DECISION PROCESSES UNDER MODEL UNCERTAINTY

ARIEL NEUFELD1, JULIAN SESTER1, MARIO ˇSIKI ´C2

June 14, 2022

1NTU Singapore, Division of Mathematical Sciences,
21 Nanyang Link, Singapore 637371.
2University of Z¨urich, Department of Banking and Finance,
Plattenstr. 14, 8032 Z¨urich

Abstract. We introduce a general framework for Markov decision problems under model uncer-
tainty in a discrete-time inﬁnite horizon setting. By providing a dynamic programming principle we
obtain a local-to-global paradigm, namely solving a local, i.e., a one time-step robust optimization
problem leads to an optimizer of the global (i.e. inﬁnite time-steps) robust stochastic optimal control
problem, as well as to a corresponding worst-case measure.

Moreover, we apply this framework to portfolio optimization involving data of the S&P 500. We
present two diﬀerent types of ambiguity sets; one is fully data-driven given by a Wasserstein-ball
around the empirical measure, the second one is described by a parametric set of multivariate normal
distributions, where the corresponding uncertainty sets of the parameters are estimated from the
data. It turns out that in scenarios where the market is volatile or bearish, the optimal portfolio
strategies from the corresponding robust optimization problem outperforms the ones without model
uncertainty, showcasing the importance of taking model uncertainty into account.

Keywords: Markov decision problem, Ambiguity, Dynamic programming principle, Portfolio opti-
mization

1. Introduction

Suppose that today and at all future times an agent observes the state of the surrounding world,
and based on the realization of this state she decides to execute an action that may also inﬂuence
future states. All actions are rewarded according to a reward function not immediately but once the
subsequent state is realized. The Markov decision problem consists of ﬁnding at initial time a policy,
i.e., a sequence of state-dependent actions, that optimizes the expected cumulated discounted future
rewards, referred to as the value of the Markov decision problem. The underlying process of states
in a Markov decision problem is a stochastic process (Xt)t
N0 and is called Markov decision process.
∈
This process is usually modelled by a discrete-time time-homogeneous Markov process that follows
a pre-speciﬁed probability which is inﬂuenced by the current state of the process and the agent’s
current action. The Markov decision problem leads to an inﬁnite horizon stochastic optimal control
problem in discrete-time which ﬁnds many applications in ﬁnance and economics, compare, e.g., [7],
[22], or [39] for an overview. It can, among a multitude of other applications, be used to learn the
optimal structure of portfolios and the optimal trading behaviour, see, e.g. [9], [13], [19], [24], [40],
to learn optimal hedging strategies, see, e.g. [3], [4], [12], [16], [17], [21], [29], [34], or even to study
socio-economic systems under the inﬂuence of climate change as in [35].

In most applications the choice of the distribution, or more speciﬁcally the probability kernel, of
the Markov decision process however is a priori unclear and hence ambiguous. For this reason, in
practice, the distributions of the process often need to be estimated, compare e.g. [1], [33], [36]. To
account for distributional ambiguity we are therefore interested to study an optimization problem
N0 by
respecting uncertainty with respect to the choice of the underlying distribution of (Xt)t
∈
identifying a policy that maximizes the expected future cumulated rewards under the worst case
probability measure from an ambiguity set of admissible probability measures. This formulation
allows the agent to act optimally even if adverse scenarios are realized, such as for example during
ﬁnancial crises or extremely volatile market periods in ﬁnancial markets.

1

 
 
 
 
 
 
2

A. NEUFELD, J. SESTER, M. ˇSIKI ´C,

The recent works [5], [14], [37], and [41] also consider inﬁnite horizon robust stochastic optimal
control problems and follow a similar paradigm but use diﬀerent underlying frameworks. More
precisely, [14] and [41] assume a ﬁnite action and state space. The approach from [37] assumes
an atomless probability space and is restricted to so called conditional risk mappings, whereas [5]
assumes the ambiguity set of probability measures to be dominated. To the best of our knowledge,
the generality of the approach presented in this paper has not been established so far in the literature.
Our general formulation enables to specify a wide range of diﬀerent ambiguity sets of probability
measures and associated transition kernels, given some mild technical assumptions are fulﬁlled.
More speciﬁcally, we require the correspondence that maps a state-action pair to the set of transition
probabilities to be non-empty, continuous, compact-valued, and to fulﬁl a linear growth condition;
see Assumption 2.2. As we will show, these requirements are naturally satisﬁed. This is for example
the case if the ambiguity set is modelled by a Wasserstein-ball around a transition kernel or if
parameter uncertainty with respect to multivariate normal distributions is considered.

To solve the robust optimization problem we establish a dynamic programming principle that
involves only a one time-step optimization problem. Via Berge’s maximum theorem (see [8]) we
obtain the existence of both an optimal action and a worst case transition kernel of this local one
time-step problem. It turns out that the optimal action that solves this one time-step optimization
problem determines also the global optimal policy of the inﬁnite time horizon robust stochastic
optimal control problem by repeatedly executing this local solution. Similarly, the global worst
case measure can be determined as a product measure given by the inﬁnite product of the worst
case transition kernel of the local one time-step optimization problem. We refer to Theorem 2.7 for
our main result. This local-to-global principle is in line with similar results for non-robust Markov
decision problems, compare e.g. [7, Theorem 7.1.7], where the optimal global policy can also be
determined locally. Note that the local-to-global paradigm obtained in Theorem 2.7 is noteworthy,
N0 does not need to be a time-homogeneous Markov process under each measure from
since (Xt)t
∈
the ambiguity set, as the corresponding transition kernel might vary with time. However, due to
the particular setting that the set of transition probabilities is constant in time and only depends
on the current state and action, and not on the whole past trajectory, we are able to derive the
analogue local-to-global paradigm for Markov decision processes under model uncertainty as for the
ones without model uncertainty.

Eventually we show how the discussed robust stochastic optimal control framework can be applied
to portfolio optimization with real data, which was already studied extensively in the non-robust
case, for example in [6], [31], [42], and [44]. To that end, we show how, based on a time series
of realized returns of multiple assets of the S&P 500, a data-driven ambiguity set of probability
measures can be derived in two cases. The ﬁrst case is an entirely data-driven approach where
ambiguity is described by a Wasserstein-ball around the empirical measure. In the second case a
multivariate normal distribution of the considered returns is assumed while the set of parameters for
the multivariate normal distribution is estimated from observed data. Hence, this approach can be
considered as semi data-driven approach. We then train neural networks to solve the (semi) data-
driven robust optimization problem based on the local-to-global paradigm obtained in Theorem 2.7
and compare the trading performance of the two approaches with non-robust approaches. It turns
out that under adverse market scenarios both robust approaches outperform comparable non-robust
approaches. These results emphasize the importance of taking into account model uncertainty when
making decisions that rely on ﬁnancial assets.
The remainder of the paper is as follows.

In Section 2 we present the setting and formulate
the underlying distributionally robust stochastic optimal control problem. We also present our
main results that include a dynamic programming principle.
In Section 3 we discuss diﬀerent
possibilities to deﬁne ambiguity sets of probability measures and we show that these speciﬁcations
meet the requirements of our setting. In Section 4 we apply the robust optimization approach to
portfolio optimization using real ﬁnancial data and compare the diﬀerent ambiguity sets introduced
in Section 3 also with non-robust approaches. The proof of the main results is reported in Section 5,
while the proofs of the results from Section 3 and 4 can be found in Section 6 and 7, respectively.
Finally, the appendix contains a description of a numerical routine that can be applied to solve the
optimization problem using neural networks as well as several useful auxiliary known mathematical
results.

MARKOV DECISION PROCESSES UNDER MODEL UNCERTAINTY

3

2. Setting, Problem Formulation, and Main Result

We ﬁrst present the underlying setting for the considered stochastic process and then formulate

an associated distributionally robust optimization problem.

2.1. Setting. We consider a closed subset Ωloc ⊆
we use to deﬁne the inﬁnite Cartesian product

Rd, equipped with its Borel σ-ﬁeld

Floc, which

F

:=

Ω := Ωloc

and the σ-ﬁeld
), by d
(Ω,

Ωloc × · · ·
M1(Ω) the set of probability measures on
N the dimension of the control space.
F
On this space, we consider an inﬁnite horizon time-discrete stochastic process. To this end, we
by the canonical process Xt((ω0, ω1, . . . , ωt, . . . )) := ωt
Rm and deﬁne the set of controls

N the dimension of the state space, and by m

N0. We ﬁx a compact set A

Floc ⊗ Floc ⊗ · · ·

N = Ωloc ×
. We denote by

deﬁne on Ω the stochastic process (Xt)t
∈
for (ω0, ω1, . . . , ωt, . . . )
(also called actions) through

Ω, t

⊆

N0

∈

∈

∈

∈

A

: =

=

{

a = (at)t
N0 |
∈
(at(Xt))t
N0
∈
Rk, and p
N, X
(cid:8)

N0 : Ω
(at)t
∈
at : Ωloc →

(cid:12)
(cid:12)

∈

∈

⊆

A; at is σ(Xt)-measurable for all t

→
A Borel measurable for all t

N0

.

∈

N0}

∈

N0, we deﬁne the set of continuous functions g : X

(cid:9)

For every k

polynomial growth at most of degree p via

R with

→

Cp(X, R) :=

C(X, R)

∈

g
(cid:26)

sup
X
x

∈

g(x)
p <
|
|
x
1 +
k
k

,

∞

(cid:27)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

where C(X, R) denotes the set of continuous functions mapping from X to R and
Euclidean norm on Rk. We deﬁne on Cp(Ωloc, R) the norm
g(x)
|
|
p
1 +
x
k
k
M1(Ωloc) induced by the convergence

kCp := sup
Moreover, recall the Wasserstein p - topology τp on

k · k

g
k

Ωloc

∈

x

denotes the

(2.1)

µn

τp
−→

µ for n

→ ∞ ⇔

lim
n
→∞ Z

gdµn =

gdµ for all g

Cp(Ωloc, R).

∈

Z

Note that for p = 0, the topology τ0 coincides with the topology of weak convergence. To be able to
formulate a robust optimization problem, we make use of the theory of set-valued maps, also called
correspondences, see also [2, Chapter 17] for an extensive introduction to the topic. In the following
we clarify how continuity is deﬁned for correspondences, compare also Lemma B.3 and Lemma B.4,
where characterizations of upper hemicontinuity and lower hemicontinuity are provided.
Deﬁnition 2.1. Let ϕ : X ։ Y be a correspondence between two topological spaces.

(i) ϕ is called upper hemicontinuous, if
(ii) ϕ is called lower hemicontinuous, if
(iii) We say ϕ is continuous, if ϕ is upper and lower hemicontinuous.
Moreover, for a correspondence ϕ : X ։ Y its graph is deﬁned as

A
}
=
∅}

x
{
x
{

|
ϕ(x)

ϕ(x)

∈
X

⊆
A

X

∈

∩

|

is open for all open sets A
is open for all open sets A

Y .
Y .

⊆
⊆

ϕ(x)
}
We impose the following standing assumptions on the process (Xt)t
∈

(x, y)
{

Gr ϕ :=

X

×

∈

∈

Y

y

.

|

measures, which are from now on assumed to be valid for the rest of the paper.

N0

and on the set of admissible

Standing Assumption 2.2 (Assumptions on the set of measures). Fix p

(i) The set-valued map

.
0, 1
}

∈ {

Ωloc ×

A
→
(x, a) ։

M1(Ωloc), τp)
(
(x, a)
P

is assumed to be nonempty, compact-valued, and continuous.
A and P
1 such that for all (x, a)

(ii) There exists CP ≥

(x, a) it holds

∈ P

∈
p) P(dy)
k

≤

Ωloc ×
CP (1 +

x
k

p).
k

(2.2)

(1 +

y
k

ZΩloc

6
4

A. NEUFELD, J. SESTER, M. ˇSIKI ´C,

Under these assumptions we deﬁne for every x

Ωloc, a

∈

∈ A

the set of admissible measures

Px,a :=

δx ⊗
(cid:26)

P0 ⊗

P1 ⊗ · · ·

for all t

N0 : Pt : Ωloc → M1(Ωloc) Borel-measurable,

∈

(cid:12)
(cid:12)
(cid:12)
(cid:12)
P0 ⊗
P1 ⊗ · · · ∈
N0 )
1lB ((ωt)t
∈

· · ·

where the notation P = δx ⊗

P(B) :=

ZΩloc · · ·

ZΩloc · · ·

and Pt(ωt)

(ωt, at(ωt)) for all ωt ∈

∈ P

Ωloc

,

(cid:27)

Px,a abbreviates

Pt

−

1(ωt

−

1; dωt)

· · ·

P0(ω0; dω1)δx(dω0),

B

.

∈ F

Remark 2.3. To ensure that the set Px,a is nonempty, one needs to show that
admits a mea-
(x, a) is closed-valued
surable selector. By Assumption 2.2 the correspondence Ωloc ×
and measurable. Hence, by Kuratovski’s Theorem (compare, e.g., [2, Theorem 18.13]), there exists
(x, a) for all
A
a measurable selector Ωloc ×
∋
A. Since actions are by deﬁnition measurable, we also obtain that for all (at)t
(x, a)
Ωloc ×
N0 ∈ A
∈
∈
P(ωt, at(ωt)) =: Pt(ωt; dωt+1) is measurable, as required.
N0 the map Ωloc ∋
and for all t
∈
Then, the non-emptiness of Px,a follows by the Ionescu–Tulcea theorem.
2.2. Problem Formulation. Let r : Ωloc ×
from now on that it fulﬁls the following assumptions.

∋
∈ M1(Ωloc) such that P(x, a)

R be some reward function. We assume

7→
ωt 7→

Ωloc →

(x, a) ։

P(x, a)

(x, a)

∈ P

×

A

A

P

P

Standing Assumption 2.4 (Assumptions on the reward function and the discount factor). Let
p

be the number ﬁxed in Assumption 2.2.

∈ {

0, 1
}
(i) The map

is continuous

Ωloc ×

A

×

Ωloc ∋

(x0, a, x1)

7→

r(x0, a, x1)

(2.3)

(2.4)

r(x0, a, x1)

Ωloc and a, a′ ∈
(ii) There exists some L > 0 such that for all x0, x′0, x1 ∈
r(x′0, a′, x1)
a
x′0k
x0 −
x1k
L
}
k
k
k
·
1 such that for all x0, x1 ∈
Ωloc we have
p) for all a
x1k
x0k
k
k

(iii) There exists some Cr ≥

r(x0, a, x1)
|

(cid:12)
(cid:12)
Cr(1 +

max

p +

| ≤

A.

+

≤

−

1,

∈

(cid:12)
(cid:12)

{

(cid:0)

p

(iv) We ﬁx an associated discount factor α < 1 which satisﬁes

A we have

a′

−

.

k
(cid:1)

0 < α <

1
Cr(CP + 1)CP

.

(cid:12)
(cid:12)

L

−

Remark 2.5. Note that if r is Lipschitz-continuous, i.e., if there exists some L > 0 such that for
all x0, x′0, x1, x′1 ∈

Ωloc and a, a′ ∈

A we have

a
k

r(x0, a, x1)

r(x′0, a′, x′1)

x0 −
k
then (2.3) follows directly. Therefore the requirement of Assumption 2.4 (i) and (ii) is weaker
than assuming Lipschitz continuity of the reward function. In particular, if m = d holds for the
dimensions, then the function of the form
Ωloc ∋

r(x0, a, x1) := a

x′1k
(cid:1)

Ωloc ×

(x0, a, x1)

x1 −
k

x′0k

fulﬁls the requirement imposed in (2.3) but are not Lipschitz continuous, unless Ωloc is bounded.
Compare also Section 4, where we apply portfolio optimization while taking into account a reward
function of the form (2.5).

(2.5)

x1

7→

+

−

+

≤

×

a′

A

(cid:12)
(cid:12)

k

(cid:0)

,

·

Our main problem consists, for every initial value x

∞t=0 αtr(Xt, at, Xt+1) under the worst case measure from Px,a over all possible actions a

of
More precisely, we introduce the value function

∈ A

∈

Ωloc, in maximizing the expected value
.

P

(2.6)

Ωloc ∋

x

7→

V (x) := sup

a

∈A

inf
Px,a  
∈

P

EP

(cid:20)

∞

t=0
X

αtr(Xt, at, Xt+1)

(cid:21)!

.

Deﬁnition 2.6. We call (Xt)t
∈
space Ωloc ⊆
in (2.6) a Markov decision problem under model uncertainty.

Rd with corresponding set of transition probabilities

a Markov decision process under model uncertainty on state
, and we call the problem deﬁned

N0

P

MARKOV DECISION PROCESSES UNDER MODEL UNCERTAINTY

5

2.3. Main Result: The Dynamic Programming Principle. In this section we provide the
main results of the paper which comprise a dynamic programming principle which in particular
allows to solve the optimization problem (2.6) by solving a related one-step ﬁx point equation.

To this end, we deﬁne the space of one-step actions

aloc : Ωloc →
{
and, we deﬁne for every aloc ∈ Aloc the set of kernels

Aloc :=

A measurable
}

,

Paloc :=

P0 : Ωloc → M1(Ωloc) measurable

(cid:8)

Moreover, we deﬁne on Cp(Ωloc, R) the operator
Ωloc ∋

v(x) : = sup
A

7→ T

x

P

a

T
inf

(x,a)

∈

∈P

P0(x)

∈ P
(cid:12)
which for every v
(cid:12)

∈

(x, aloc(x)) for all x

Ωloc

.

∈
Cp(Ωloc, R)is deﬁned by

(cid:9)

EP [r(x, a, X1) + αv(X1)] .

Our main ﬁndings are collected in the subsequent theorem.

Theorem 2.7. Assume that Assumption 2.2 and Assumption 2.4 hold true. Then the following
holds.

(i) For every v
Ωloc ×

A we have P∗0(x, a)

(x, a) and

∈ P

Cp(Ωloc, R) there exists P∗0 : Ωloc ×

∈

A

→ M1(Ωloc) such that for all (x, a)

∈

(2.7)

(2.8)

(2.9)

EP∗

0(x,a)[r(x, a, X1) + αv(X1)] : =

r(x, a, ω1) + αv(ω1)P∗0(x, a; dω1)

ZΩloc
= inf
P0
∈P

(x,a)

EP0[r(x, a, X1) + αv(X1)].

Moreover, there exists aloc∗ ∈ Aloc such that for every x
∗(x))

EP0[r(x, aloc∗(x), X1) + αv(X1)]

inf
(x,aloc

∈

Ωloc we have

P0
∈P
= sup

aloc∈Aloc

inf
(x,aloc(x))

P0

∈P

EP0[r(x, aloc(x), X1) + αv(X1)].

Furthermore, let P∗loc : Ωloc → M1(Ωloc) be deﬁned by
x

P∗loc(x) := P∗0(x, aloc∗(x)),

Then P∗loc ∈

Paloc

∗ and for every x

Ωloc it holds that

∈

Ωloc.

∈

T

v(x) = sup

aloc∈Aloc

inf
Paloc
P0
∈
EP0(x)

EP0(x)

r(x, aloc(x), X1) + αv(X1)

(cid:2)

r(x, aloc∗(x), X1) + αv(X1)

(cid:3)

= inf
Paloc
P0
∈
= EP∗

loc(x)

∗

(cid:2)

r(x, aloc∗(x), X1) + αv(X1)

.

(cid:3)

(cid:2)
Cp(Ωloc, R)

Cp(Ωloc, R), i.e.,

v

T

∈

⊆

(cid:3)
Cp(Ωloc, R) for all v

Cp(Ωloc, R)

∈

Cp(Ωloc, R) the following inequality holds true

(cid:19)

(ii) We have that

T
and for all v, w

(cid:18)
∈

(2.10)

In particular, there exists a unique v
Cp(Ωloc, R) we have v = limn
v0 ∈
(iii) Let v
Deﬁne a∗ := (aloc∗(X0), aloc∗(X1), . . . )
Px,a∗. Then, for all x

Cp(Ωloc, R) satisfy

∈

T

∈ A
Ωloc we have that

v

kT

− T

w

v
αCP k

w

kCp .

kCp ≤

−
Cp(Ωloc, R) such that
nv0.

∈
→∞ T

T
v = v and let aloc∗ ∈ Aloc, P∗loc ∈

and for all x

∈

v = v. Moreover, for every

Paloc
Ωloc, P∗x := δx ⊗

∗ be deﬁned as in (i).
P∗loc ⊗· · · ∈

P∗loc ⊗

∈

(2.11)

EP∗

x

∞

(cid:20)

t=0
X

αtr(Xt, aloc∗(Xt), Xt+1)
(cid:21)

∞

EP

(cid:20)

t=0
X

αtr(Xt, aloc∗(Xt), Xt+1)
(cid:21)

Px,a∗

= inf
P
∈
= V (x)

= v(x).

6

A. NEUFELD, J. SESTER, M. ˇSIKI ´C,

Remark 2.8. Note that the local-to-global paradigm obtained in Theorem 2.7 is noteworthy, since
N0 does not need to be a (time homogeneous) Markov process under each P
Px,a∗, as the
(Xt)t
∈
corresponding transition kernel might vary with time. However, due to the particular setting that the
set of transition probabilities (x, a)
(x, a) is constant in time and only depends on the current
state and action, and not on the whole past trajectory, we are able to derive the analogue local-
to-global paradigm for Markov decision processes under model uncertainty as for the ones without
N0 is a Markov decision
model uncertainty. Moreover, if (x, a)
(x, a) is single-valued, then (Xt)t
∈
N0 a Markov decision
process in the classical sense, compare, e.g., [7]. This justiﬁes to call (Xt)t
∈
Rd with respect to
. Moreover, note that a
process under model uncertainty on state space Ωloc ⊆
N0 is a time-homogeneous Markov process under the worst-case measure,
posteriori, we see that (Xt)t
∈
and the optimal strategy only depends on the current state of the process, and not on time, as
observed for classical Markov decision problems.

7→ P

7→ P

P

∈

3. Capturing distributional uncertainty

In this section we present diﬀerent approaches that enable to capture uncertainty with respect
to the choice of the underlying probability measure. We show that all of the presented approaches
fulﬁl the requirements of the setting presented in Section 2.

3.1. Uncertainty expressed through the Wasserstein distance. The ﬁrst example involves
the case when distributional uncertainty is captured through the q-Wasserstein-distance Wq(
) for
,
·
·
some q

N. For any P1, P2 ∈ M1(Ωloc) let Wq(P1, P2) be deﬁned as

∈

1/q

Wq(P1, P2) :=

x
Ωloc k
denotes the Euclidean norm on Rd, and where Π(P1, P2) denotes the set of joint distri-

inf
Π(P1,P2)

qdπ(x, y)
k

ZΩloc×

−

(cid:18)

(cid:19)

y

∈

π

,

where
butions of P1 and P2, compare also for example [38, Deﬁnition 6.1.].

k · k

We ﬁx some q

N and specify p := 0 in Assumption 2.2 and 2.4. Further, we assume that there

exists a continuous map

∈

(3.1)

Ωloc ×

A

(x, a)

→

7→

M1(Ωloc), τq)
(
P(x, a)

P(x, a) has ﬁnite q-th moments for all (x, a)

b

Ωloc ×

∈

A. Then, we deﬁne for any ε > 0

(x, a) ։

(x, a) :=

P

(q)
ε

B

P(x, a)

:=

P

∈ M1(Ωloc)

Wq(P,

P(x, a))

≤

ε

,

denotes the q-Wasserstein-ball (or Wasserstein-ball of order q) with ε-radius

b

b

(cid:16)

(cid:17)

n

o

(cid:12)
(cid:12)
(cid:12)

(cid:17)

and center

(cid:16)
P(x, a).
b
Proposition 3.1. Let Ωloc ×
7→
moments. Then, the set-valued map Ωloc ×
requirements of Assumption 2.2 with p = 0.
b

(x, a)

A

∋

b

P(x, a)
A

∋

M1(Ωloc), τq) be continuous with ﬁnite q-th
(
∈
(x, a) ։
(x, a) deﬁned as in (3.2) fulﬁls the

P

3.2. Knightian uncertainty in parametric models. Next, we consider a parametric approach,
taking into account the so called Knightian uncertainty (see [27]). To this end, we consider a
set-valued map of the form
Ωloc ×

(x, a) ։ Θ(x, a)

for some D

RD,

(3.3)

N.

⊆

A

∋

∈

The set Θ(x, a) refers to the set of parameters that are admissible in dependence of (x, a)
The underlying parametric probability distribution is described by

Ωloc ×

A.

∈

(3.4)

(x, a, θ)
{

|

(x, a)

Ωloc ×

∈

A, θ

∈

Θ(x, a)

} →

(x, a, θ)

7→

M1(Ωloc), τp)
(
P(x, a, θ),

which enables us to deﬁne the ambiguity set of probability measures by

(3.5)

Ωloc ×

A

∋

(x, a) ։

P

(x, a) :=

P(x, a, θ)

θ

n

b

(cid:12)
(cid:12)
(cid:12)

∈

b
Θ(x, a)

o

M1(Ωloc), τp).
(

⊆

such that
the set-valued map

b
Ωloc ×

A

∋

(q)
ε

P(x, a)

(3.2)

where

B

MARKOV DECISION PROCESSES UNDER MODEL UNCERTAINTY

7

Proposition 3.2. Let (x, a) ։ Θ(x, a), as deﬁned in (3.3), be nonempty, compact-valued, and
continuous, let (x, a, θ)
(x, a), as
deﬁned in (3.5), is nonempty, compact-valued, and continuous.

P(x, a, θ), as deﬁned in (3.4), be continuous. Then (x, a) ։

7→

P

b

3.3. Uncertainty in autocorrelated time series. Next, we consider the case where the state
N0 is given by an autocorrelated time series. More precisely, we assume that at time
process (Xt)t
∈
may have
m+1, . . . , Yt) of a time series (Yt)t
t
−
∈{−
N a representation
an inﬂuence on the next value of the state process. In this case we have for all t
of the form

N observations (Yt

N0 the past m

···−
∈

1,0,1...

∈

∈

m,

}

Xt := (Yt

−

m+1, . . . , Yt)

∈

Ωloc := T m

RD

·

m, with T

RD closed, for some D

N.

⊆

⊆

∈

To deﬁne the ambiguity set of measures, we ﬁrst consider a set-valued map of the form
(x, a) ։

(x, a)

M1(T ), τp).
(

⊆

π((x1, . . . , xm)) := (x2, . . . , xm)

Ωloc ×
(3.6)
A
We consider the projection Ωloc ∋
projects onto the last m

∋
(x1, . . . , xm)

P

7→
e
1 components, and deﬁne a set-valued map
δπ(x) ⊗

−
(x, a) ։

P

P

P

∈

T m
, in dependence of
P
M1(Ωloc), τp).
(

−

1 that
, by

P

∋

A

(3.7)

(x, a) :=

Ωloc ×
This means, by considering
, we take into account uncertainty with respect to the evolution of the
next value of the time series. However, we do not want to consider uncertainty with respect to the
m
Proposition 3.3. Let (x, a) ։
tinuous. Then (x, a) ։

(x, a), as deﬁned in (3.6), be nonempty, compact-valued, and con-
(x, a), as deﬁned in (3.7) is nonempty, compact-valued, and continuous.

1 preceding values of the time series, as they constitute of the already observed realizations.

(x, a)

⊂

−

(cid:12)
(cid:12)
(cid:12)

P

P

P

∈

n

o

e

e

P
4. Application to Portfolio Optimization

e

In this section we discuss a ﬁnance-related application of the presented robust stochastic opti-
In particular, we compare diﬀerent speciﬁcations to measure

mal control problem of Section 2.
uncertainty with respect to the choice of the underlying probability measure.

Setting. We present a setting that can be applied to the robust optimization of ﬁnancial portfolios.
Compare among many others also [11], [16, Chapter 10], and [18], where alternative approaches to
portfolio optimization relying on the optimal control of Markov decision processes are discussed.

Let D

N denote the number of assets that are taken into account for portfolio optimization.

∈

Then, the underlying asset returns in the time period between t
Si
t −
i=1,...,D :=
Si
t
−
(cid:1)
) denotes the time t-value of asset i
∞

Si
t
−
1 (cid:19)i=1,...,D ∈

where Si

Rt :=

i
t
R

RD,

(cid:0)
(0,

1, . . . , D

t ∈

∈ {

⊆

(cid:18)

T

1

1 and t are given by

−

t

∈ {−

m + 1, . . . , 0, 1, . . . ,

,
}

, m
}

∈

N, and T

RD closed.

⊆

To take into account the autocorrelation of the time series, we want to base our portfolio allocation
decisions not only on the current portfolio allocation and the present state of the ﬁnancial market,
N observed returns. Thus, we consider at every time t
N0 realized
but also on the past m
∈
m. Then, the underlying stochastic process (Xt)t
RD
N0 is modelled as
Rt)
returns (
∈
Ωloc,

·
Xt := (

Rt
−

m+1,

m+1,

(4.1)

N0,

· · ·

∈

∈

t

,

,

Rt
−

Rt)

∈

· · ·

∈

with

Next, we introduce the compact set

A :=

Ωloc := T m

RD

m.

·

⊆

a = (ai)i=1,...,D ∈

[
−

C, C]D

,

for the possible values of the controls, which corresponds to the monetary investment in the D
stocks, where C > 0 relates to a budget constraint when investing. Then, we deﬁne the reward
function by

(cid:8)

(cid:9)

(4.2)

Ωloc ×

A

Ωloc ∋

×

(Xt, at, Xt+1)

7→

r (Xt, at, Xt+1) :=

The reward function in (4.2) expresses the cumulated gain from trading in the period between t
and t + 1.

D

Xi=1

ai
t · R

i
t+1.

8

A. NEUFELD, J. SESTER, M. ˇSIKI ´C,

4.1. Data-driven ambiguity set and Wasserstein-uncertainty. We rely on the setting elab-
orated above.

As exposed in Section 3.1, we may capture distributional uncertainty by considering a Wasserstein-

ball around some kernel

∈ M1(T ),
RD closed. We consider a time series of past realized returns

Ωloc ×

(x, a)

P(x, a)

7→

A

∋

b

(R1, . . . , RN )

T N ,

∈

for some N

N.

∈

for T

(4.3)

⊆

Compare also Figure 1, where we illustrate the relation between this time series and the time series
of future returns.

Past realized return

m returns before t = 0

Future returns

R1

· · ·

RN

−m+1

· · · R

· · ·

0
R
t = 0

1
R

2
R

· · ·

Figure 1. Illustration of the observed and already realized return (Rt)t=1,...,N and
the future random returns (
,0,1,

m+1,

.

Rt)t=

−

···

···

Relying on the time series from (4.3), we aim at constructing an ambiguity set

we deﬁne

P through a sum of Dirac-measures given by1

. To this end,

P

(4.4)

b
Ωloc ∋

Xt = (

Rt
−

m+1,

,

Rt)

7→

· · ·

P (Xt) (dx) :=

N

1

−

s=m
X

πs(Xt)

·

δRs+1 (dx)

∈ M1(T ),

∈

[0, 1], s = m,

N
where πs(Xt)
, N
s=m πs(Xt) = 1. We want to weight the distance
−
Rt+1 and the m returns before Rs+1, while assigning higher
between the past m returns before
P relies its prediction
probabilities to more similar sequences of m returns. This means, the measure
for the next return on the best ﬁtting sequence of m consecutive returns that precede the prediction.
ε > 02
To this end, we set for some (small) constant

· · ·

P

−

1

b
1 with

Ωloc ∋

Xt = (

Rt
−

m+1,

,

Rt)

e
πs(Xt) :=
7→

· · ·

with

P

1

(dists(Xt) +
ε)−
N
ℓ=m (distℓ(Xt) +
e

−

,

1 !

ε)−

dists(Xt) :=

(Rs
k
, N

for all s = m,
−
probability measures on

· · ·

(4.5)

Ωloc ∋

x ։

· · ·

m+1,

, Rs)

m+1,

, Rs)

Xtk

−

=

(Rs
k

· · ·

−

−
1. Then, we deﬁne for any ﬁxed ε > 0 and q
M1(Ωloc) via the set-valued map3
δπ(x) ⊗
n

(x) :=

P(x)

∈ B

(q)
ε

(cid:17)o

⊆

P

P

P

(cid:16)

−

that takes into account a q-Wasserstein-ball around

P for the next future return.

b

M1(Ωloc), τp)
(

(cid:12)
(cid:12)
(cid:12)

,

(
Rt)
Rt
k
−
N the ambiguity set of

· · ·

,

∈

Proposition 4.1. Let T
, deﬁned
in (4.5), satisﬁes Assumption 2.2. Moreover, the reward function r, deﬁned in (4.2), satisﬁes
Assumption 2.4.

RD be compact, and let p = 0. Then, the set-valued map

⊆

P

b

1Note that x 7→
2The constant

P(x) does not depend on a ∈ A.
b
ε > 0 is merely a technical requirement which is considered to avoid division by zero in the case
e
dists(Xt) = 0 for some indices s ∈ {1, . . . , N }, t ∈ N0, i.e., in the case that a sequence of m random returns equals a
sequence of past realized returns. Hence, in practice,

ε can be set to be a negligible small positive real number.
e

3Note that P does not depend on a ∈ A, and recall that Ωloc ∋ (x1, . . . , xm) 7→ π((x1, . . . , xm)) := (x2, . . . , xm) ∈

T m−1 denotes the projection onto the last m − 1 components.

b

1

e
m+1,

 
MARKOV DECISION PROCESSES UNDER MODEL UNCERTAINTY

9

4.2. Parametric Uncertainty. Next, we introduce a parametric approach in which we assume
that the asset returns follow a multivariate normal distribution with unknown parameters.4

To this end, we build on the setting exposed in Section 4, where m > 1, and where we choose
T = RD, and p = 1. Moreover, we consider the following unbiased estimators of mean and covariance

m : (RD)m

x = (x1, . . . , xm)

→

7→

RD

1
m

m

Xi=1

xi,

(4.6)

and

(4.7)

c : (RD)m

RD

×

D

→

x = (x1, . . . , xm)

7→

m

1

−

1

m

Xi=1

(xi −

m(x))

(xi −

·

m(x))T .

Let ε > 0. To deﬁne the set of admissible parameters we consider the following set-valued maps

We deﬁne an ambiguity set related to D-dimensional multivariate normal distributions by

Ωloc ∋
Ωloc ∋
Ωloc ∋

x ։
x ։

µ(x) :=

µ

∈

RD

Σ(x) :=
b
x ։ Θ(x) :=
b

(cid:8)

(cid:8)

RD

Σ

∈
(µ, Σ)

µ
k
D

(cid:12)
×
(cid:12)

n

m(x)

ε

,

k ≤

−
Σ = c(y) for some y

(cid:9)

∈
µ(x), Σ

Ωloc with

y
k

−

x

k ≤

ε

,

Σ(x)

.

(cid:9)

RD
(cid:12)
(cid:12)

∈

×

RD

×

D

µ

∈

∈

(cid:12)
(cid:12)
(cid:12)
(µ, Σ)

b
Θ(x)

o

b
M1(RD), τ1

.

} ⊆

∈
π((x1, . . . , xm)) := (x2, . . . , xm)

(cid:0)

(cid:1)

{ND(µ, Σ)

|

(x1, . . . , xm)

7→

1 components. These deﬁnitions allow us to deﬁne the ambiguity

RD

·

(m

−

1)

∈

x ։

P

(x) :=

Ωloc ∋
As in Section 3.3 we denote by Ωloc ∋
e
the projection onto the last m
M1(Ωloc) by5
set on
A
Ωloc ×

(x, a) ։

(4.8)

−

δπ(x) ⊗
n
This means, we consider as an ambiguity set for the next return a set of multivariate normal
distributions with unknown mean and covariance, where the set of admissible means and covariances
is speciﬁed by the estimators m and c as well as by the degree of ambiguity speciﬁed through ε.

M1(Ωloc), τ1) .
(

(x, a) :=

(x)

⊆

(cid:12)
(cid:12)
(cid:12)

P

P

∋

∈

o

e

P

P

Proposition 4.2. Let T = RD and p = 1. Then, the ambiguity set
, as deﬁned in (4.8), fulﬁls
the requirements from Assumption 2.2. Moreover, the reward function r, deﬁned in (4.2), satisﬁes
Assumption 2.4.

P

4.3. Numerical Experiments. In the sequel we solve the portfolio optimization problem that was
discussed in Section 4 by applying the numerical method based on Theorem 2.7 that is elaborated
in Appendix A to real ﬁnancial data. In particular, we compare the diﬀerent approaches to capture
distributional uncertainty outlined in Section 4.1 and 4.2, respectively, and evaluate how these
approaches perform under diﬀerent market scenarios.

4.3.1. Implementation. To apply the numerical method from Appendix A, we use the following
= 10; Batch size B = 28; Monte-Carlo sample size
hyperparameters: Number of measures N
NMC = 23; Discount factor α = 0.45; Number of epochs E = 50; number of iterations for a:
Itera = 10; number of iterations for v:
Iterv = 10. The neural networks that approximate a
and v constitute of 2 layers with 128 neurons each possessing ReLu activation functions in each
layer, except for the output layer of a which possesses a tanh activation function in order to
constraint the output. The learning rate used to optimize the networks a and v when applying
the Adam optimizer ([26]) is 0.001. Further details of the implementation can be found under
https://github.com/juliansester/Robust-Portfolio-Optimization.

P

4We say that X ∈ RD has a D-dimensional multivariate normal distribution with mean µ ∈ RD and covariance
matrix Σ ∈ RD×D which is symmetric and positive semideﬁnite if the characteristic function of X is of the form
RD ∋ u 7→ ϕX (u) := exp (cid:0)iuT µ − 1

2 uT Σu(cid:1), compare e.g. [20, p. 124]. We write X ∼ ND(µ, Σ).

5Note that P does not depend on a ∈ A, and recall that Ωloc ∋ (x1, . . . , xm) 7→ π((x1, . . . , xm)) := (x2, . . . , xm) ∈

RD(m−1) denotes the projection onto the last m − 1 components.

10

A. NEUFELD, J. SESTER, M. ˇSIKI ´C,

4.3.2. Data. To train and test the performance of the portfolio optimization approach, we consider
the price evolution of d = 5 constituents6 of the S&P 500 between 2010 and 2021. We consider a
lookback period of m = 10 days, i.e., the prediction of the optimal trading execution relies on the
previously realized m = 10 returns. We split the data into a training period ranging from January
2010 until September 2018, and three diﬀerent testing periods thereafter. The normalized evolution
of the asset values is depicted in Figure 2.

Normalized Evolution of the Considered Stocks

Testing Period 1

Testing Period 2

Testing Period 3

Training Period
Testing Period 1
Testing Period 2
Testing Period 3

25

20

15

10

5

0

2010-01-04 00:00:00

2018-09-28 00:00:00

1.6

1.4

1.2

1.0

0.8

0.6

2 0 1 8 - 1 0 - 0 1

2 0 1 9 - 0 1 - 1 0

2 0 1 9 - 0 5 - 0 7

2 0 1 9 - 0 9 - 2 7

2 0 2 0 - 0 5 - 0 5

2 0 2 0 - 0 8 - 2 6

Figure 2. The left panel of the ﬁgure shows the normalized evolution (with initial
value = 1) of the stocks of d = 5 constituents of the S&P 500. Further, we divide
the data into a training phase (blue) and three testing periods thereafter that are
highlighted with diﬀerent colors.
The right panel of the ﬁgure shows the normalized evolution (with initial value = 1
for each of the assets) of the considered stocks in the three testing periods.

The testing periods are illustrated in detail in the right panel of Figure 2, and they comprise three
diﬀerent market scenarios. While the ﬁrst testing period covers an overall declining market phase,
the second testing period is a volatile period without a clear trend. Eventually, the third period is
a bullish market with a strong upward trend.

4.3.3. Results. In Table 1, 2, and 3, respectively, we depict the results of the numerical method
from Appendix A applied to the presented data in the three testing periods with diﬀerent radii ε for
both of the considered approaches to deﬁne ambiguity sets, see Section 4.1 and 4.2. Note also that
we consider diﬀerent ranges of values for ǫ for the two considered approaches. In Figure 3, 4, and 5,
we depict the cumulated trading proﬁts of the trained strategies in the respective testing periods.
Testing Period 1

Cumulated Profit of Trained Strategy in Testing Period 1,
  Wasserstein Approach

Cumulated Profit of Trained Strategy in Testing Period 1,
  Parametric Approach

0.25

0.00

−0.25

−0.50

−0.75

−1.00

−1.25

ε =  0
ε =  0.01
ε =  0.05
ε =  0.1
ε =  0.3

0.0

−0.2

−0.4

−0.6

ε =  0.0
ε =  0.005
ε =  0.025
ε =  0.05
ε =  0.15

0

10

20

30

40

50

60

70

0

10

20

30

40

50

60

70

Figure 3. The ﬁgure shows the cumulated training proﬁt of the trained strategies
in testing period 1. The left panel illustrates the proﬁt when applying a Wasserstein-
ball approach, whereas the right panel illustrates the proﬁt under a parametric ap-
proach.

6The constituents are Apple, Microsoft, Google, Ebay, and Amazon.

MARKOV DECISION PROCESSES UNDER MODEL UNCERTAINTY

11

Overall Proﬁt Average Proﬁt % of proﬁtable trades Sharpe Ratio Sortino Ratio

Wasserstein Approach
ε = 0
ε = 0.01
ε = 0.05
ε = 0.1
ε = 0.3

Parametric Approach
ε = 0
ε = 0.005
ε = 0.025
ε = 0.05
ε = 0.15

-1.035019
0.221663
0.080857
-0.154961
0.005442

-0.378669
-0.165537
-0.143357
-0.006977
0.003136

-0.015221
0.003260
0.001189
-0.002279
0.000080

-0.005569
-0.002434
-0.002108
-0.000103
0.000046

47.06
58.82
55.88
44.12
48.53

50.00
42.65
38.24
50.00
58.82

-0.172211
0.035932
0.015079
-0.028555
0.035702

-0.067022
-0.127499
-0.198093
-0.035993
0.053274

-0.207249
0.052785
0.022100
-0.041425
0.055826

-0.083380
-0.146742
-0.225493
-0.043762
0.066614

Table 1. The table shows the results of the Wasserstein-ball approach (Section 4.1)
and the parametric approach (Section 4.2) in the ﬁrst testing period.

Testing Period 2

Cumulated Profit of Trained Strategy in Testing Period 2,
 Wasserstein Approach

Cumulated Profit of Trained Strategy in Testing Period 2,
 Parametric Approach

0.4

0.2

0.0

−0.2

−0.4

0.4

0.3

0.2

0.1

0.0

−0.1

ε =  0
ε =  0.01
ε =  0.05
ε =  0.1
ε =  0.3

ε =  0.0
ε =  0.005
ε =  0.025
ε =  0.05
ε =  0.15

0

20

40

60

80

100

0

20

40

60

80

100

Figure 4. The ﬁgure shows the cumulated training proﬁt of the trained strategies
in testing period 2. The left panel illustrates the proﬁt when applying a Wasserstein-
ball approach, whereas the right panel illustrates the proﬁt under a parametric ap-
proach.

Overall Proﬁt Average Proﬁt % of proﬁtable trades Sharpe Ratio Sortino Ratio

Wasserstein Approach
ε = 0
ε = 0.01
ε = 0.05
ε = 0.1
ε = 0.3

Parametric Approach
ε = 0
ε = 0.005
ε = 0.025
ε = 0.05
ε = 0.15

-0.319652
-0.086229
-0.128121
-0.108500
0.001415

-0.065187
-0.094396
0.027990
0.022454
-0.001429

-0.003229
-0.000871
-0.001294
-0.001096
0.000014

-0.000658
-0.000953
0.000283
0.000227
-0.000014

51.52
56.57
52.53
50.51
52.53

48.48
50.51
55.56
49.49
49.49

-0.068955
-0.018503
-0.031648
-0.054067
0.027997

-0.014956
-0.063190
0.030187
0.079555
-0.048469

-0.091312
-0.025662
-0.044057
-0.074651
0.040511

-0.020914
-0.079788
0.053250
0.277870
-0.061703

Table 2. The table shows the results of the Wasserstein-ball approach (Section 4.1)
and the parametric approach (Section 4.2) in the second testing period.

12

A. NEUFELD, J. SESTER, M. ˇSIKI ´C,

Testing Period 3

Cumulated Profit of Trained Strategy in Te ting Period 3,
 Wa  er tein Approach

Cumulated Profit of Trained Strategy in Testing Period 3,
 Parametric Approach

ε =  0
ε =  0.01
ε =  0.05
ε =  0.1
ε =  0.3

0.8

0.6

0.4

0.2

0.0

−0.2

−0.4

ε =  0.0
ε =  0.005
ε =  0.025
ε =  0.05
ε =  0.15

1.2

1.0

0.8

0.6

0.4

0.2

0.0

0

10

20

30

40

50

60

70

80

0

10

20

30

40

50

60

70

80

Figure 5. The ﬁgure shows the cumulated training proﬁt of the trained strategies
in testing period 3. The left panel illustrates the proﬁt when applying a Wasserstein-
ball approach, whereas the right panel illustrates the proﬁt under a parametric ap-
proach.

Overall Proﬁt Average Proﬁt % of proﬁtable trades Sharpe Ratio Sortino Ratio

Wasserstein Approach
ε = 0
ε = 0.01
ε = 0.05
ε = 0.1
ε = 0.3

Parametric Approach
ε = 0
ε = 0.005
ε = 0.025
ε = 0.05
ε = 0.15

0.845952
-0.160733
0.358808
0.087670
-0.017214

0.682794
0.176339
0.043324
-0.047627
-0.001740

0.010846
-0.002061
0.004600
0.001124
-0.000221

0.008754
0.002261
0.000555
-0.000611
-0.000022

52.56
48.72
52.56
52.56
43.59

64.10
61.54
64.10
52.56
51.28

0.166630
-0.031032
0.096431
0.037463
-0.199209

0.124028
0.054989
0.014557
-0.064649
-0.039336

0.335301
-0.040478
0.154537
0.053469
-0.231336

0.170558
0.069144
0.017213
-0.075748
-0.049270

Table 3. The table shows the results of the Wasserstein-ball approach (Section 4.1)
and the parametric approach (Section 4.2) in the third testing period.

4.3.4. Discussion of the Results. Note that in contrast to the third bullish testing period, the ﬁrst
and second testing periods comprise scenarios that did not occur in similar form during the training
period. Hence, it cannot be guaranteed that a non-robust trading strategy that was trained in
periods where such scenarios never occurred can trade proﬁtably during testing period 1 and testing
period 2. Indeed, as the numerical results reveal, applying a trained non-robust trading strategy not
respecting any distributional ambiguity (i.e. ε = 0) leads to signiﬁcant losses in the adverse scenarios
considered in testing period 1 and 2, while robust trading strategies, which did encounter also
adverse scenarios during training, clearly outperform the non-robust strategy for both considered
approaches, namely the Wasserstein-ball approach and the parametric approach.

As the third testing period comprises a bullish market period, occurring in similar form in the
training data, the non-robust approach turns out to be the most proﬁtable approach in this period,
since it is the approach that is best adjusted to this scenario. However, when choosing the right
level of ε, the robust approach still can trade proﬁtably in this period.

In general, it turns out to be important to correctly identify the appropriate size of the ambiguity
set (here encoded by the radius ε). Due to the formulation of the robust optimization problem as
a worst-case approach, respecting for more distributional ambiguity means to consider more bad
scenarios, and therefore may eventually result in a more careful, less volatile trading behavior with
smaller returns, as it clearly can be seen in the case ε = 0.3 for the Wasserstein-ball approach and

MARKOV DECISION PROCESSES UNDER MODEL UNCERTAINTY

13

in the case ε = 0.15 for the parametric approach, respectively. In contrast, insuﬃciently accounting
for uncertainty comes at the cost of not being well equipped when adverse scenarios occur, an
observation that was already made in similar empirical studies that use diﬀerent approaches to
solve robust optimization problems, compare, e.g., [30] and [32]. Hence, choosing an intermediate
level of ambiguity seems to be an appropriate choice. Supporting this rationale, our numerical
results show that choosing a level of ε = 0.05 in the Wasserstein-ball approach and of ε = 0.025 in
the parametric approach, respectively, lead to trading strategies that outperform in testing periods
1 and 2 the respective non-robust trading strategies and still lead to proﬁts in testing period 3.

This provides evidence that taking into account distributional uncertainty may be of particular
importance in volatile and crisis-like market scenarios which did not occur previously in a similar
form.

5. Proof of Theorem 2.7

Proof of Theorem 2.7 (i). Let v
(5.1)

F : Gr

=

(x, a0, P0)
{

|

x

∈

P

∈
Ωloc, a0 ∈

Cp(Ωloc, R). We deﬁne the map

A, P0 ∈ P

(x, a0)

} →

R

(x, a0, P0)

7→

ZΩloc

r(x, a0, ω1) + αv(ω1)P0(dω1).

We claim that the map F is continuous.
(x(n), a(n)
Ωloc ×
is compact-valued and continuous (Assumption 2.2 (i)), we have P0 ∈ P

Indeed, to see this, let (x(n), a(n)
. Since Ωloc ×

× M1(Ωloc) for n

0 , P(n)
0 )

(x, a0, P0)

0 , P(n)
0 )
A

⊆
(˜x, ˜a)
∋
(x, a). Moreover,

→ ∞

→

A

∈

Gr

P
7→ P

with
(˜x, ˜a)

+

F (x, a0, P(n)
0 )
|

F (x, a0, P0)
.
|

−

(5.2)

−

F (x(n), a(n)
|
F (x(n), a(n)

0 , P(n)
0 )
0 , P(n)
0 )
−
F (x, a0, P(n)
0 )
|

F (x, a0, P0)
|
F (x, a0, P(n)
0 )
|
F (x, a0, P0)
|

≤|

The second summand
since the integrand ω1 7→
r(x, a0, ω1) + αv(ω1) is an element of Cp(Ωloc, R). For the ﬁrst summand we obtain, by using
Assumption 2.4 (ii), that

vanishes for n

→ ∞

−

lim
n
→∞ (cid:12)
(cid:12)
(cid:12)
≤

F (x(n), a(n)

0 , P(n)
0 )

−
r(x(n), a(n)

F (x, a0, P(n)
0 )
(cid:12)
(cid:12)
(cid:12)

0 , ω1)

−

lim
n
→∞ ZΩloc (cid:12)
(cid:12)
(cid:12)
L
lim
n
→∞ ZΩloc
L

≤

= lim
n
→∞

·

(cid:16)(cid:13)
(cid:13)
(cid:13)

P(n)
0 (dω1)

r(x, a0, ω1)
(cid:12)
(cid:12)
(cid:12)
x

x(n)

max

1,
{

·

x(n)

x

−

p

} ·

ω1k
k
a(n)
0 −
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)

(cid:16)(cid:13)
(cid:13)
(cid:13)
a0

(cid:17)

(cid:13)
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)
(cid:13)
ZΩloc

·

a0

P(n)
0 (dω1)

+

a(n)
0 −
(cid:13)
(cid:13)
(cid:13)
max

1,

{

(cid:13)
(cid:17)
(cid:13)
p
(cid:13)
ω1k
k

}

P0(dω1) = 0,

where we use in the last equality that due to the convergence P(n)
P0 also the p-th moments
converge, see e.g. [38, Deﬁnition 6.8. (i), and Theorem 6.9]. Thus, F is continuous. Therefore, we
may apply Berge’s maximum theorem (Theorem B.2) and obtain that the map

0

τp
−→

(5.3)

G : Ωloc ×

A

→

R

(x, a0)

7→

P0

inf

(x,a0)

∈P

r(x, a0, ω1) + αv(ω1)P0(dω1)

ZΩloc

(x, a0)

is continuous, and the set of minimizers is nonempty for each (x, a0)
A. According to
the measurable maximum theorem ([2, Theorem 18.19]) there exists a measurable selector Ωloc ×
(x, a0), where P0∗(x, a0) minimizes the integral in (5.3) for each
P0∗(x, a0)
A
∋
A. This shows the assertion in (2.7). Next, we use that Ωloc ×
(x, a0)
7→
G(x, a0) is continuous, and apply again Berge’s maximum theorem to the constant compact-valued
correspondence Ωloc ∋

Rm and to G. This yields that the map

7→
Ωloc ×

Ωloc ×

x ։ A

(x, a0)

∈ P

⊂

A

∈

∈

∋

(5.4)

R

H : Ωloc →
x
7→

G(x, a0).

sup
A
a0

∈

14

A. NEUFELD, J. SESTER, M. ˇSIKI ´C,

is continuous and that the set of maximizers of G(x,
Ωloc. Moreover, by
the measurable maximum theorem ([2, Theorem 18.19]) there exists a measurable selector Ωloc ∋
x
Ωloc. This shows (2.8). Next, we
) for each x
aloc∗(x)
·
deﬁne the map P∗loc ∈

) is nonempty for each x
·

A, where aloc∗(x) maximizes G(x,

Paloc

∗ by

7→

∈

∈

∈

P∗loc : Ωloc → M1(Ωloc)
7→

x

P0∗(x, aloc∗(x))

and obtain for each x

Ωloc that

∈

T v(x) = H(x) = sup
A

a0

∈

P0

inf

(x,a0)

∈P

r(x, a0, ω1) + αv(ω1)P0(dω1)

ZΩloc
r(x, aloc∗(x), ω1) + αv(ω1)P0(dω1)

(5.5)

P0

=

=

inf

(x,a0)

∈P

ZΩloc

r(x, aloc∗(x), ω1) + αv(ω1)P∗loc(x; dω1)

ZΩloc
= sup

aloc∈Aloc

P

inf
Paloc ZΩloc
∈

r(x, aloc(x), ω1) + αv(ω1)P(x; dω1).

This shows (2.9) and therefore completes the proof of (i).

(cid:3)

v(x) follows from the continuity of H
Proof of Theorem 2.7 (ii). The continuity of Ωloc ∋
deﬁned in (5.4) and (5.5). By the growth conditions on r and X1 (Assumption 2.4 (iii)), we obtain
for all x

7→ T

x

Ωloc that

EP0

≤

v(x)

Cr(1 +

sup
aloc∈Aloc
x
Cr (
k

inf
Paloc
P0
∈
p + CP (1 +
k
v
Cr + CrCP + α
k
Cp(Ωloc, R). Note that for every nonempty set

x
k
(cid:2)
p)) + α
x
v
k
k
k
(1 +
kCp CP

X1k
k
kCp CP (1 +
p).
x
k
k

≤

≤

(cid:0)

(cid:1)

p +
k

p) + α
v
k
p)
k

x
k

kCp (1 +

p)
X1k
k

(cid:3)

and for all G, H :

R we have

Q →

Q

∈

T

Hence,
that

v

T

∈

(5.6)

Therefore, we obtain for every v, w

Cp(Ωloc, R), x

∈

∈

G(Q)

inf
Q
∈Q

−

inf
Q
∈Q

H(Q)

≤

sup
Q

|

G(Q)

H(Q)
|

.

−

∈Q
Ωloc, by using (2.2), that

v(x)

T

− T

w(x)

≤

≤

≤

α sup
A
a

∈

α sup
A
a

∈

sup

P0

∈P

(x,a) (cid:12)
(cid:12)
(cid:12)
EP0
(cid:12)

sup

P0

∈P

(x,a)

EP0

v(X1)

−

(cid:20)

w(X1)

v(X1)
|

−

(cid:20)

w(X1)
|

α sup
A
a

P0
v
αCP k

∈

sup

EP0

∈P

(x,a)
w

kCp ·

v
k

(cid:20)
(1 +

w

kCp (1 +
p) .
k

−

x
k

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:21)
p)
X1k
k

(cid:21)

≤
By interchanging the roles of v and w we obtain (2.10). Now, let v0 ∈
Assumption 2.4 (iv) we have 0 < αCP < 1,
point theorem (Theorem B.1) implies existence and uniqueness of a ﬁx point v
that v =

Cp(Ωloc, R). Then, since by
is a contraction on Cp(Ωloc, R). Hence Banach’s ﬁx
Cp(Ωloc, R) such
(cid:3)

−

∈

T

v = limn

nv0.

T

→∞ T

Cp(Ωloc, R) be the ﬁx point of

Proof of Theorem 2.7 (iii). We ﬁrst show the inequality v(x)
Let v
(ii), i.e., we have that v =
(aloc∗(X0), aloc∗(X1),
that

≤
whose existence and uniqueness were proved in part
v. Let aloc∗ ∈ Aloc be the minimizer from part (i) and write a∗ :=
2
Ωloc. First, we claim it holds for all n
, and let x
≥

N with n

T
∈ A

V (x).

· · ·

∈

∈

∈

T

)

(5.7)

nv(x)

T

≤

P

inf
Px,a∗
∈

EP

n

1

−

(cid:20)

t=0
X

αtr(Xt, aloc∗(Xt), Xt+1) + αnv(X x
n )
(cid:21)

.

MARKOV DECISION PROCESSES UNDER MODEL UNCERTAINTY

15

We prove the claim (5.7) inductively. To that end, note that by part (i) we can write

v(x) = inf

P0

Paloc
∈

∗

T

EP0(x)

= inf
Paloc
P0
∈

∗

ZΩloc

r(x, aloc∗(x), X1) + αv(X1)
(cid:21)

(cid:20)
r (x, aloc∗(x), ω1) + αv(ω1) P0(x; dω1).

This implies for all t

N0 and ω = (ωt)t
N0 ∈
∈

∈

Ω that

T

(5.8)

v(Xt(ω)) = inf

P0

Paloc
∈

r (Xt(ω), aloc∗(Xt(ω)), ω1) + αv (ω1) P0(Xt(ω); dω1)

∗

ZΩloc

= inf
Paloc
Pt
∈

∗

ZΩloc

r (ωt, aloc∗(ωt), ωt+1) + αv (ωt+1) Pt(ωt; dωt+1),

where we just used the deﬁnition of the canonical process and relabelled the variables P0 and ω1.
For n = 2, as
(5.9)

v = v, we thus have

T

(
T

T

v) (x) =

T

v(x) = inf

P0

Paloc
∈

r (x, aloc∗(x), ω1) + αv(x)P0(x; dω1)

∗

ZΩloc

= inf
Paloc
P0
∈

∗

ZΩloc

r (x, aloc∗(x), ω1) + α
T

v(x)P0(x; dω1)

= inf
Paloc
P0
∈

∗

ZΩloc (cid:20)

r (x, aloc∗(x), ω1)

+ α sup

aloc∈Aloc

P1

inf
Paloc ZΩloc (cid:26)
∈

r (ω1, aloc(ω1), ω2) + αv (ω2)

P1(ω1; dω2)
(cid:21)

(cid:27)

P0(x; dω1).

This and part (i) ensures that

(

T

T

v) (x) = inf

P0

Paloc
∈

r (x, aloc∗(x), ω1)

∗

ZΩloc (cid:20)

+ α

inf
Paloc
∈

∗

P1

1

r (ω1, aloc∗(ω1), ω2) + αv (ω2)

ZΩloc (cid:26)

P1(ω1; dω2)
(cid:21)

(cid:27)

P0(x; dω1)

≤

Pt∈P

inf
aloc
t=0,1

∗ ,

ZΩloc ZΩloc
1

t=0
X

αtr(ωt, aloc∗(ωt), ωt+1) + α2v (ω2) P1(ω1; dω2) P0(x; dω1)

= inf
P
∈

Px,a∗

EP

"
Xt=0

αtr(Xt, aloc∗(Xt), Xt+1) + α2v (X2)
#

,

where we use (5.8) and the structure of the measures Px,a∗. The general case for arbitrary n follows
1, then it follows by the
with analogue arguments. Indeed, let the claim in (5.7) be true for n

−

16

A. NEUFELD, J. SESTER, M. ˇSIKI ´C,

same argument as in (5.9) and by the structure of every P

nv(x) =

T

T

T

n

1v

−

(x)

Pω1,a∗ that

∈

≤

P0

inf
Paloc
∈

∗

(cid:0)
ZΩloc (cid:20)

(cid:1)
r (x, aloc∗(x), ω1)

+ α inf
P

Pω1,a∗
∈

=

P0

inf
Paloc
∈

∗

ZΩloc (cid:20)

ZΩ (cid:26)

t=1
X
r (x, aloc∗(x), ω1)

+ α inf

Pt ∈P

t=1,...,n−1 ZΩloc · · ·

aloc

∗ ,

ZΩloc (cid:26)

t=1
X
+ αn

≤

inf
aloc

Pt∈P

t=0,...,n−1 ZΩloc ZΩloc · · ·

∗ ,

n

1

−

Xt=0

ZΩloc (cid:26)

n

1

−

αt
−

1r(ωt, aloc∗(ωt), ωt+1) + αn

1v (ωn)

−

P(dω)
(cid:21)

(cid:27)

P0(x; dω1)

n

1

−

αt
−

1r(ωt, aloc∗(ωt), ωt+1)

1v (ωn)

−

Pn

(cid:27)

1(ωn

−

−

1; dωn)

· · ·

P1(ω1; dω2)
(cid:21)

P0(x; dω1)

αtr(ωt, aloc∗(ωt), ωt+1)

+ αnv (ωn)

Pn

−

1(ωn

−

1; dωn)

· · ·

(cid:27)

P1(ω1; dω2)P0(x; dω1)

n

1

−

EP

= inf
P
∈

Px,a∗

t=0
X
According to (5.7) we have for all n

(cid:20)

αtr(Xt, aloc∗(Xt), Xt+1) + αnv(Xn)
(cid:21)

.

(5.10)

v(x) =

v(x) =

T

T

Further, we obtain for all P

∈

N that

∈

≤

nv(x)

inf
Px,a∗
∈
Px,a∗ and n

P

∈

EP

(cid:20)

n

1

−

t=0
X

αtr(Xt, aloc∗(Xt), Xt+1) + αnv(Xn)
(cid:21)

.

N by a repeated application of (2.2) that

v(Xn)
|
|
Note that Assumption 2.4 implies 0 < CP ·
for all P

Px,a that

(cid:20)

EP

∈

v
≤ k

(cid:21)
α <

P (1 +

kCp C n
p).
k
Cr(CP +1) < 1. When letting n

x
k

1

, we thus have

→ ∞

(5.11)

0

≤

lim sup

EP

αn

n

→∞

(cid:20)

v(Xn)
|
|

v
≤ k

kCp (1 +

x
k

p)
k

·

lim sup

n

→∞

α)n = 0.

(CP ·

(cid:21)

Moreover, note that by the growth condition on r in Assumption 2.4 (iii) we have for each n
that

N

∈

(5.12)

n

1

−

t=0
X
1
αCr(CP +1) −

αtr(Xt, aloc∗(Xt), Xt+1)

≤

∞

αtCr(1 +

p +

Xtk
k

Xt+1k
k

p).

t=0
X

Let δ :=
using Assumption 2.4 (iv), (2.2), and Beppo Levi’s theorem, we have that

CP which satisﬁes δ > 0 by Assumption 2.4 (iv). Then for all P

Px,a∗, by

∈

EP

∞

αtCr(1 +

(cid:20)

Xt=0

p +

Xtk
k

p)
Xt+1k
k
(cid:21)

EP

≤

(cid:20)

∞

≤

t=0
X

(5.13)

∞

αt (Cr(1 + CP )) (1 +

Xt=0
αt (Cr(1 + CP ))t+1 (1 +

p)
Xtk
k
(cid:21)

x
k

p)
k

t

∞

1
CP + δ

=Cr(1 + CP )
(cid:20)
(CP + δ)(1 +
CP + δ

=Cr(1 + CP )

t=0 (cid:18)
X

·
(cid:21)
p)
k

(1 +

x
k

p)
k

<

.

∞

(cid:19)
x
k
1

−

MARKOV DECISION PROCESSES UNDER MODEL UNCERTAINTY

17

Hence the dominating function in (5.12) is integrable and we obtain, by using the dominated con-
vergence theorem and (5.11), that

v(x)

≤

lim sup

n

→∞

EP

inf
Px,a∗
∈

P

(5.14)

≤

P

inf
Px,a∗
∈

lim sup

EP

n

→∞

n

1

−

(cid:20)

t=0
X
1
n
−

(cid:20)

t=0
X

αtr(Xt, aloc∗(Xt), Xt+1) + αnv(Xn)
(cid:21)

αtr(Xt, aloc∗(Xt), Xt+1)
(cid:21)

+ lim sup
n

→∞

EP

αn

(cid:20)

v(Xn)
|

|

(cid:21)

= inf
P
∈

Px,a∗

∞

EP

(cid:20)

αtr(Xt, aloc∗(Xt), Xt+1)
(cid:21)

≤

V (x).

Xt=0
V (x). To this end, let P∗0 : Ωloc ×
→ M1(Ωloc) be deﬁned
Next, we show the inequality v(x)
Cp(Ωloc, R) of
. Moreover, for every
as in part (i) with respect to the unique ﬁx point v
T
∈
let P∗x,a := δx ⊗
a = (at)t
ωt 7→
, where for t
N0 ∈ A
∈
P∗0(ωt, at(ωt))
(ωt, at(ωt)). Thus, we have, by using the dominated convergence theorem with
∈ P
the same dominating function as in (5.12), that
(5.15)

N we deﬁne P∗at : Ωloc ∋

≥
P∗a0 ⊗

P∗a1 ⊗ · · ·

A

∈

EP

(cid:20)

∞

inf
Px,a
∈

P

EP∗

x,a

(cid:20)

Xt=0

∞

αtr(Xt, at(Xt), Xt+1)
(cid:21)

t=0
X
αtr(Xt, at(Xt), Xt+1)
(cid:21)

EP∗

x,a

αtr(Xt, at(Xt), Xt+1)
(cid:21)

(cid:20)

V (x) = sup

a

∈A

≤

sup
a

∈A

= sup
a

∈A

= sup
a

∈A

= sup
a

∈A

∞

t=0
X
∞

t=0 (cid:18)
X
∞

Xt=0 (cid:18)

αt

αtEP∗
x,a

r(Xt, at(Xt), Xt+1) + αv(Xt+1)
(cid:21)

(cid:20)

−

EP∗

x,a

αt+1v(Xt+1)

(cid:20)

(cid:21)(cid:19)

ZΩloc · · ·

ZΩloc

r(ωt, at(ωt), ωt+1) + αv(ωt+1)P∗0(ωt, at(ωt); dωt+1)

P∗0(x, a0(x); dω1)

· · ·

(cid:20)
Moreover, by using the results from part (i) we have for all ωt ∈
Ωloc
r(ωt, at(ωt), ωt+1) + αv(ωt+1)P∗0(ωt, at(ωt); dωt+1)

EP∗

x,a

αt+1v(Xt+1)

.

−

(cid:21)(cid:19)

ZΩloc
=

P0

inf
(ωt,at(ωt))

∈P

sup
aloc∈Aloc

P0

∈P

ZΩloc
inf

(ωt,aloc(ωt))

≤

=

(5.16)

r(ωt, at(ωt), ωt+1) + αv(ωt+1)P0(ωt, at(ωt); dωt+1)

r(ωt, aloc(ωt), ωt+1) + αv(ωt+1)P0(ωt, aloc(ωt); dωt+1)

ZΩloc

r(ωt, aloc∗(ωt), ωt+1) + αv(ωt+1)P∗0(ωt, aloc∗(ωt); dωt+1) =

v(ωt) = v(ωt).

T

ZΩloc

Hence, we obtain with (5.15) and (5.16) that

V (x)

≤

sup
a

∈A

∞

αt

t=0 (cid:18)
X

ZΩloc · · ·

ZΩloc

v(ωt)P∗0(ωt
−

1, at

−

1(ωt

−

1); dωt)

· · ·

P∗0(x, a0(x); dω1)

EP∗

x,a

αt+1v(Xt+1)

(cid:20)

(cid:21)(cid:19)

−

∞

αtEP∗
x,a

t=0 (cid:18)
X
v(x) = v(x).

v(Xt)
(cid:21)

(cid:20)

−

αt+1EP∗
x,a

v(Xt+1)

(cid:20)

(cid:21)(cid:19)

= sup
a

∈A
= sup
a

∈A

18

A. NEUFELD, J. SESTER, M. ˇSIKI ´C,

This shows V (x) = v(x). Eventually, to see that the ﬁrst line of (2.11) holds, we compute by using
(5.16), the deﬁnition P∗x := δx ⊗
as well as the dominated convergence theorem that

P∗loc ⊗

∞

v(x) =

αtEP∗

x

v(Xt)
(cid:21)

(cid:20)

−

v(Xt+1)

(cid:20)

(cid:21)(cid:19)

P∗loc ⊗ · · ·
αt+1EP∗

x

t=0 (cid:18)
X
∞

Xt=0 (cid:18)
∞

=

=

αtEP∗

x

r(Xt, aloc∗(Xt), Xt+1) + αv(Xt+1)
(cid:21)

(cid:20)

−

αt+1EP∗

x

v(Xt+1)

(cid:20)

(cid:21)(cid:19)

EP∗

x

αtr(Xt, aloc∗(Xt), Xt+1)
(cid:21)

(cid:20)

= EP∗

x

∞

(cid:20)

t=0
X

αtr(Xt, aloc∗(Xt), Xt+1)
(cid:21)

.

t=0
X

Moreover, by (5.14), as we have shown that V = v, we obtain that

(5.17)

V (x) = inf

Px,a∗

P

∈

∞

EP

(cid:20)

t=0
X

αtr(Xt, aloc∗(Xt), Xt+1)
(cid:21)

.

(cid:3)

6.1. Proof of Results in Section 3.1.

6. Proof of Results in Section 3

P(x, a) is contained in

(x, a) by deﬁnition of

P

∈
(x, a) is nonempty since the measure

∈

Ωloc, a

A.

Proof of Proposition 3.1. Let x
We see that
the q-Wasserstein-ball.
The compactness of

P

(q)
ε

with respect to τ0, which is the topology induced by the weak
convergence of measures, follows from, e.g., [43, Theorem 1], where we use the assumption that
P(x, a) has ﬁnite q-th moments.

B

(cid:17)

(cid:16)

b

P(x, a)

b

To show the upper hemicontinuity of
Ωloc ×
∈
for all n

such that (x(n), a(n))
b
that P(n)

(x, a)
→
P(x(n), a(n))

(q)
ε

P

A for n

Let (δn)n

∈ B
N

∈

(cid:16)
⊆

(0, 1) with limm
b

(cid:17)

→∞

assumption, continuous in τq we have limn
P(xnk), a(nk))k

subsequence (

N such that
∈

→∞

(cid:16)

b

∈

→ ∞
N, i.e., we have

x(n), a(n)
δn = 0. Note that, since Ωloc ×
(cid:1)
(cid:0)(cid:0)
P(x(n), a(n))
P(x, a),

Wq

, we apply Lemma B.3. Let (x(n), a(n))n

. Further, consider a sequence (P(n))n
, P(n)

Gr

.

N

∈

⊆

Ωloc ×

A
N such
∈

N ⊆
n
∈
(x, a)
(cid:1)

P
P(x, a) is, by
∋
= 0. Hence, there exists a
b

7→

A

(cid:17)

(6.1)

b

Wq

P(x, a),

(cid:16)
This implies for each P(nk), k

P(x(nk), a(nk))
(cid:17)

N, that
b

b
∈
P(x, a),

≤

Wq

Wq

P(x, a), P(nk )

δk ·
(cid:16)
(cid:17)
(cid:16)
(q)
Hence, P(nk)
b
b
b
P(x, a)) for all k
P(x, a)) in τ0, there exists a
2ε (
∈
N such that P(nkℓ ) τ0
(q)
subsequence (P(nkℓ ))ℓ
P(x, a)) . In particular,
2ε (
−→
∈ B
∈
b
b
P(x, a) possesses ﬁnite q-th moments, P has also ﬁnite q-th moments, see [43,
since by assumption
Lemma 1]. It remains to prove that P

P(x(nk), a(nk)), P(nk)
(cid:16)

P(x(nk), a(nk))
(cid:17)

N. By the compactness of

B
for some P

P as ℓ

+ Wq

→ ∞

(q)
2ε (

ε + ε

∈ B

2ε.

≤

≤

N

(cid:17)

b

b

(q)
ε (

∈ B

P(nk) := (1

δk)

·

−

P(x, a)). To that end, deﬁne for each k
P(nk) + δk ·
b

P(x(nk), a(nk)).

∈

(6.2)

b

Then, for each k

Wq

(6.3)

N we have
e
P(nk)

∈
P(x(nk), a(nk)),
(cid:16)
= Wq
b

δk)

(1

−

= (1

(cid:16)
−

δk)

Wq

·

(cid:16)

b

b

P(x(nk), a(nk)), (1

δk)

P(nk) + δk ·

·

−

(cid:17)

P(x(nk), a(nk)) + δk ·
e
·
P(x(nk), a(nk)), P(nk)
b

(1

δk)

ε.

·

−

≤

b
(cid:17)

P(x(nk), a(nk))
(cid:17)

b

b
ε for all k

< δk ·

N.

∈

MARKOV DECISION PROCESSES UNDER MODEL UNCERTAINTY

19

Therefore, by (6.1) and (6.3) we have for each ℓ

N that

∈

P(nk ℓ)

Wq

(6.4)

P(x, a),

Wq
(cid:16)
ε + (1
δkℓ ·
b
Furthermore, we have by (6.2) that

≤

≤

(cid:16)

(cid:17)

b

e

P(x, a),

−

P(x(nk ℓ), a(nk ℓ))
(cid:17)

ε = ε.

δkℓ)
b

·

+ Wq

P(x(nk ℓ), a(nk ℓ)),
(cid:16)

b

P(nk ℓ)

(cid:17)

e

(6.5)

lim
ℓ
→∞

P(nk ℓ) = lim
→∞

ℓ

P(nk ℓ) = P in τ0.

Since µ
Wq(
and (6.5) that

7→

P(x, a), µ) is lower semicontinuous in τ0, see [15, Corollary 5.3], we obtain from (6.4)

e

b

Wq

P(x, a), P

lim inf
ℓ
→∞
P(x, a)). The assertion that

≤

(cid:16)

(cid:17)

(q)
ε (

and hence P
characterization of upper hemicontinuity provided in Lemma B.3.

∈ B

P

e

b

b

Wq

P(x, a),

P(nk ℓ)

ε,

≤

(cid:16)
is upper hemicontinuous follows now with the

(cid:17)

To show the lower hemicontinuity of
b
: Ωloc ×

(x, a) ։

(q)
ε (

A

P

∋

B

◦

◦

P
P(x, a)) :=

we ﬁrst deﬁne the set-valued map

P

∈ M1(Ωloc)

Wq(P,

P(x, a)) < ε

and conclude the lower hemicontinuity of
b
(x(n), a(n))n
Ωloc ×
N
∈
◦
some P
((x, a)) =
B
B
respect to τq, there exists some 0 < δ < ε such that P
b
measure

A such that (x(n), a(n))
(q)
ε (

P(x, a)). Note that since

→

⊂

P

P

∈

◦

◦

n

(cid:12)
(cid:12)
with Lemma B.4. To this end, we consider a sequence
(cid:12)
(x, a)
, and we consider
A for n
Ωloc ×
∈
→ ∞
◦
(q)
P(x, a)) is deﬁned as an open ball with
ε (
◦
N the

P(x, a)). We deﬁne for n

o

b

(q)
ε

δ(

∈

Then, we claim that P(n)

P(n) :=

P(x(n), a(n)),
P,
b
(x(n), a(n))

(
◦

∈

P

this follows by deﬁnition of P(n), whereas if Wq
by the triangle inequality

(cid:0)

(cid:1)

≤

Wq

P,

P(x, a)

P(x, a),

(cid:16)

P,

Wq

P(x(n), a(n))
(cid:17)
By the continuity of (x, a)
b
there exists some N
weakly for n
that the τ0-closure of

→ ∞

∈

◦

(cid:16)

(cid:17)

(cid:16)

b

7→

P(x, a) in τq we have that

b
N such that we have P(n) = P for all n
≥
, which concludes the lower hemicontinuity of
P(x, a)), denoted by clτ0

b
(q)
ε (

(q)
ε (

b

b

◦

−

∈

B
b
P(x(n), a(n)),
P(x, a)
b

if Wq

else.

(cid:16)

b

(cid:17)

b

for all n

N. Indeed, if Wq

∈
P(x(n), a(n)),

δ

≥

P(x(n), a(n)),

P(x, a)

δ

≥

P(x, a)

< δ, then P(n) = P, and hence

(cid:17)

(cid:16)

b

b

P(x(n), a(n))

< (ε

δ) + δ = ε.

(cid:17)
b
P(x(n), a(n))
−
(cid:17)
P(x, a) as n
N and thus, in particular P(n)

. Thus,
P
with Lemma B.4. Next, we claim
P(x, a)).

, coincides with

τq
−→

→ ∞

(q)(

→

P

b

◦

P(x, a))

(cid:16)

b
+ Wq

Indeed, the inclusion

B
(q)(
Bε

P(x, a))
b

clτ0

⊆

B
(cid:16)
P(x, a))

(q)
ε (

◦

B

(cid:16)

follows, since clτ0

(cid:17)

b

(cid:17)

◦

(q)
ε (

P

in τ0 and hence also in τq. To show the reverse inclusion clτ0
(cid:16)
. Then, there exists a sequence (P(n))n
⊆
∈
Wq(µ,

B
. Hence by using the lower semicontinuity of µ

P(x, a))

(q)
ε (

clτ0

B
N

(cid:17)

b

b

◦

∈
as n
obtain

(cid:16)
→ ∞

b

Bε
P(x, a))

◦

(q)
ε (

is closed
b

B
(cid:16)
P(x, a))
◦

⊆ Bε
b

(cid:17)
P(x, a)) let

(q)(
P(x, a)) with P(n) τ0
b
−→
B
P(x, a)) with respect to τ0 we

(q)
ε (

P

(cid:17)

b

b

Wq

P,

P(x, a)

lim inf
n
→∞

≤

Wq

P(x, a)

ε.

b
≤

◦

Hence, clτ0
: Ωloc ×
Eventually, since p = 0, the growth constraint (2.2) is automatically fulﬁlled.
(cid:17)

is lower hemicontinuous.

(q)
P(x, a))
ε (
(cid:17)
(x, a) ։ clτ0

P(x, a))

(q)(
b

(cid:16)
A

=

P

∋

B

B

(cid:16)

b

b

b

◦

(cid:16)
Bε
(q)
ε (

(cid:17)

(cid:17)
P(x, a)) and [2, Lemma 17.22] implies that the set-valued map

(cid:3)

7→
P(n),
(cid:16)

6.2. Proof of Results in Section 3.2.

Proof of Propositon 3.2. Let (x, a)
The nonemptiness of

A.
(x, a) follows directly since Θ is nonempty.

∈

To show the compactness of

P
P(x, a, θ(n)) for some θ(n)

P(n) =

P(n)

N we have
Θ(x, a). The compactness of Θ(x, a) implies the existence

i.e., for all n

N ⊆ P
∈

(x, a),

∈

n

P

∈

(cid:0)

(cid:1)

b

Ωloc ×
(x, a) let

b

20

A. NEUFELD, J. SESTER, M. ˇSIKI ´C,

→

∈ P

of a subsequence (θ(nk))k
continuous, it follows

N
⊆
P(x, a, θ(nk))

Θ(x, a) such that θ(nk)
P(x, a, θ)

∈

θ

Θ(x, a) for k

→

∈
(x, a) in τp for k

. Hence, since

P is

→ ∞
.

N

⊆

b
→∞

(x(n), a(n))n

We apply Lemma B.3 to show the upper hemicontinuity of

(x(n), a(n)) = (x, a) and a sequence (P(n))n

Ωloc ×
b
∈
∈
∈
Θ(x, a). Hence with the continuity of

A with limn
∈
N. We have a representation P(n) =
∈
N. Then, since Θ is upper hemicontinuous, there exists a subsequence
N such that θ(nk)
Θ(x(nk), a(nk)) for all k
b
for some
→ ∞
P :=
(x, a) in τp for

. To this end, consider a sequence

b
N with P(n)
∈
P(x(n), a(n), θ(n)) for some θ(n)

P it follows P(nk)

∈
(x(n), a(n)) for all n
P
Θ(x(n), a(n)) for all n
(θ(nk))k
N with θ(nk)
∈
θ
k

∈
→ ∞
To show the lower hemicontinuity we let (x(n), a(n))n

P(x, a, θ)

θ for k

→

P

∈

.

→ ∞

P(x, a, θ)

b
(x, a) and P :=
(x, a) for some θ
the existence of a subsequence (x(nk), a(nk))k
N such that θ(k)
for all k
P(k) :=
.

x(nk), a(nk)
P
∋
hemicontinuity of

P(x(nk), a(nk), θ(k))

θ for k

∈ P

→

∈

b

(cid:0)

(cid:1)

P

b

∈

N

⊆

∈ P

A with limn
b

→
Ωloc ×
∈
N and of a sequence (θ(k))k
∈
→ ∞
→

(x(n), a(n)) =
Θ(x, a). Then, the lower hemicontinuity of Θ implies
Θ(x(nk), a(nk))
P that
, implying with Lemma B.4 the lower

. Hence, it follows with the continuity of
P for k

N with θ(k)
∈

→ ∞

→∞

∈

b

(cid:3)

6.3. Proof of Results in Section 3.3. Before reporting the proof of Proposition 3.3, we establish
the following lemma.

Lemma 6.1. Let
:=
Dirac measures on T m

D

−

is continuous.

be the closed subset consisting of all

δx

x

T m

−

1

∈
1. Then, for any p
(cid:8)
(cid:9)

1), τp
M1(T m
−
, the map
0, 1
(cid:1)
}

(cid:12)
(cid:12)
ϕ : (

, τp)

(

×

D

⊆
(cid:0)
∈ {
M1(T ), τp)
(δx, P)

M1(T m), τp)
(
P
δx ⊗

→

7→

Proof. First, we consider the case p = 0. Let (δx(n))n
δx(n)
x
∈ T
have

∈ M1(T ) for n
→

N ⊆ M1(T ) with
is closed, µ = δx for some
(cid:0)
R be Lipschitz continuous with Lipschitz constant L > 0. Then we

µ and P(n)
P
1. Now, let f : T m

N
. Note that, as

τ0
−→
m
−

τ0
−→

→ ∞

, and

⊆ D

D

(cid:1)

∈

∈

n

P(n)

f (y, z)δx(n) (dy)

⊗

P(n)(dz)

f (y, z)δx(dy)

= lim
n

lim
n
ZT m
→∞ (cid:12)
(cid:12)
(cid:12)
(cid:12)
ZT
→∞ (cid:12)
(cid:12)
(cid:12)
lim
(cid:12)
n
→∞ (cid:18)ZT
L
lim
n
→∞

· k

−

ZT
P(n)(dz) +

ZT m
f (x, z)P(dz)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ZT
f (x, z)P(n)(dz)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

(cid:12)
(cid:12)
(cid:12)
ZT

f (x(n), z)P(n)(dz)

f (x(n), z)

f (x, z)

−

(cid:12)
(cid:12)
x(n)
(cid:12)

x

k

−

+ lim
n

→∞ (cid:12)
(cid:12)
(cid:12)
(cid:12)

⊗

P(dz)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f (x, z)P(n)(dz)

−

ZT

−

ZT

f (x, z)P(dz)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f (x, z)P(dz)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 0,

(cid:19)

P. By [25, Theorem 18.7] we conclude

(6.6)

≤

≤

where the second summand in (6.6) vanishes due to P(n) τ0
−→
that ϕ is continuous.

and P(n) τ1
−→

Now, we consider the case p = 1. Let (δx(n))n

τ1
n
−→
∈
. Since convergence in τ1 implies convergence in τ0, we
δx ∈ D
(cid:1)
. It remains
δx ⊗
obtain, by the already considered case p = 0, that δx(n)
to show that the convergence also follows with respect to τ1. To conclude the convergence in τ1 it
suﬃces, by [38, Theorem 6.9], to show that

N ⊆ M1(T ) with δx(n)

∈ M1(T ) for n

(cid:0)
P(n) τ0
−→

P for n

→ ∞

, and

→ ∞

⊆ D

P(n)

⊗

P

N

∈

lim
→∞ ZT m k
n

δx(n) (dy)
(y, z)
k

⊗

P(n)(dz) =

δx(dy)
(y, z)
k

ZT m k

P(dz).

⊗

MARKOV DECISION PROCESSES UNDER MODEL UNCERTAINTY

21

To see this, note that

δx(n) (dy)
(y, z)
k

⊗

P(n)(dz)

δx(dy)
(y, z)
k

= lim
n

lim
ZT m k
n
→∞ (cid:12)
(cid:12)
(cid:12)
(cid:12)
ZT k
→∞ (cid:12)
(cid:12)
(cid:12)
lim
(cid:12)
n
→∞ (cid:18)ZT
lim
n
→∞ (cid:18)

≤

≤

P(n)(dz)
(x(n), z)
k

k − k

(x(n), z)
k
(cid:12)
(cid:12)
x(n)
(cid:12)
k

−

+

x

k

ZT k
(cid:12)
(cid:12)
(cid:12)
(cid:12)
7→ k

(x, y)

−

ZT k

−

ZT m k
P(dz)
(x, z)
k
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ZT k
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ZT k

−

P(n)(dz) +

(x, z)
k
(cid:12)
(cid:12)
P(n)(dz)
(cid:12)
(x, z)
k

where we use that T

y

∋

C1(T, R) and P(n) τ1
−→

k ∈

.

→ ∞

⊗

P(dz)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

ZT k

= 0,

P(dz)
(x, z)
k
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P(n)(dz)
(x, z)
k

P(dz)
(x, z)
k
(cid:12)
(cid:12)
(cid:12)
(cid:12)
P for n

(cid:19)

(cid:19)

(cid:3)

Proof of Proposition 3.3. Let (x, a)
It is immediate that

∈
, since
=
∅
To show the compactness of
P
P(n) for some
N we have P(n) = δπ(x) ⊗

Ωloc ×
P

(x, a)

P

∈

n

e

A.
(x, a)

P(n)

=
(x, a) we consider a sequence

by assumption.

∅

P(n)

there exists a subsequence

P(nk)
e
Cp(Ωloc, R). Then the map T

(cid:16)

g

∈

such that

P

P
∈
P(nk)
e

(cid:1)

(cid:0)
(x, a) in τp as k

N

k
(cid:17)
∋

∈
y

7→

e

→ ∞
P
g(π(x), y) is contained in Cp(T, R), and hence
e

→

∈

e

e

e

N ⊆ P
∈
(x, a). Then, by the compactness of

n

(x, a), where for all

(x, a)

P
. Now, let
e

lim
k
→∞ ZΩloc

g(z)P(nk)(dz) = lim

k

→∞ ZT

g(π(x), y)

P(nk )(dy)

g(π(x), y)

P(dy) =

e

=

ZT

g(z)P(dz)

ZΩloc

(x, a), which proves the compactness of

e

for P := δπ(x) ⊗

P

∈ P

To show the upper hemicontinuity of

, let (x(n), a(n))

P

e

→ ∞

, and let P(n)

for n
δπ(x(n)) ⊗
exists, according to Lemma B.3, a subsequence

(x(n), a(n)) for all n

P(n) with

∈ P
P

(x(n), a(n)) for all n

P(n)

∈

k

e

e

e
. Moreover δπ(x(n)) →

.
(cid:16)
→ ∞
→ ∞
e
P(nk)
We apply Lemma 6.1 and obtain that δπ(x(nk )) ⊗

δπ(x) in τ1 as n

k
(cid:17)

∈

(x, a).
P
Ωloc ×

⊆

A with (x(n), a(n))

(x, a)
N. Then, we have a representation P(n) =
, there

∈
N. By the upper hemicontinuity of
P(nk)

∈
P(nk)

→

P

with

P
(x, a) in τp as
e

→

P

∈

N

e

P

e

e

δπ(x) ⊗
we consider again a sequence (x(n), a(n))

∈ P

hemicontinuity follows with Lemma B.3.
To prove the lower hemicontinuity of
(x, a) for n

(x(n), a(n))
P
∈
(x(nk), a(nk))k
N and
∈
→
P
∈
e
e
N, and we conclude P(nk)
(x(nk), a(nk)) for all k
P(nk) := δπ(x(nk )) ⊗
→
e
e
with Lemma 6.1. Hence, the lower hemicontinuity follows with Lemma B.4.

e
A with
Ωloc ×
P for
(x, a) with a representation P = δπ(x) ⊗
∈ P
there exists, according to Lemma B.4, a subsequence
P in τp. Then, we set
P in τp for k

(x, a). By the lower hemicontinuity of
P(nk)

(x(nk), a(nk)) for all k

P
, and some P

N such that

P(nk)
e

∈
∈ P

→ ∞
(cid:3)

→ ∞

P(nk)

(x, a), and hence the upper

→

→

⊆

P

P

∈

e

e

e

e

e

7.1. Proof of Results in Section 4.1.

7. Proof of Results in Section 4

Proof of Proposition 4.1. Since Assumption 2.2 (ii) is automatically fulﬁlled for p = 0, the fulﬁlment
of Assumption 2.2 follows from Proposition 3.1 and Proposition 3.3, once we have shown that
∈ M1(T ) is continuous in τq and possesses ﬁnite q-th moments.
Ωloc ∋
Ωloc with X (n)

P, we consider a sequence (X (n)

To show the continuity of

P(x)

7→

)n

x

N

⊆
[0, 1] is continuous for all s = m, . . . , N

∈

t →

t

n

→ ∞

. By construction Ωloc ∋

x

7→

πs(x)

∈

b

b

Ωloc for
1, which

Xt ∈
−

6
6
22

A. NEUFELD, J. SESTER, M. ˇSIKI ´C,

implies for all g

Cq(T, R) that

∈

lim
n
→∞ ZT

g(y)

P

X (n)
t

; dy

(cid:16)

b

(cid:17)

N

1

−

πs(X (n)
t

)g(Rs+1)

s=m
X
πs(Xt)g(Rs+1) =

= lim
n
→∞

=

N

1

−

s=m
X

g(y)

P (Xt; dy) .

ZT

b

Moreover, the existence of the q-th moment follows by

P (Xt; dy) =

q
k

y
ZT k

b

N

1

−

s=m
X

πs(Xt)

Rs+1k

q <

.

∞

· k

Now, to verify Assumption 2.4 note that r is continuous and that the compactness of T and of A
Ωloc,
A. Then, by the Cauchy–Schwarz inequality

imply that r is bounded, and thus Assumption 2.4 (i) and (iii) are fulﬁlled. Next, let Xt, X ′t ∈
Xt+1 = (
Rt
−
we see that

Ωloc, and let at, a′t ∈

Rt+1)

m+2,

· · ·

∈

,

r(Xt, at, Xt+1)

−

r(X ′t, a′t, Xt+1)

=

(cid:12)
(cid:12)
and hence Assumption 2.4 (ii) is fulﬁlled.

(cid:12)
(cid:12)

D

(cid:12)
Xi=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(ai

t −

a′t

i)

i
t+1
R

≤ kRt+1k · k

at −

a′tk ≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

x

max
x
∈

T k

at −

k · k

a′tk

(cid:3)

7.2. Proof of Results in Section 4.2.

Proof of Proposition 4.2. To show Assumption 2.2 (ii), let

CP := 1 +

ε2 +

r

1
m

+ 4

ε2 + 1
1
m

−

·

Now we consider some (x, a)
the form P = δπ(x) ⊗
m(x)
µ
k
|| ≤
inequality that

∈
P for some
ε and Σ = c(y) for some y

Ωloc ×
P
P
∈

e

e

−

e

A and some P
(x), where

P

Ωloc with

∈

e

∈ P

(x, a). Then, we have a representation of
D fulﬁlling
RD
ε. Therefore, we have with Jensen’s

∼ ND(µ, Σ) with (µ, Σ)
y
k

k ≤

RD

−

×

∈

x

×

1 +

y
k

P(dy) = 1 +
k

(π(x), z)

P(dz)

k

ZRD k

ZΩloc

(7.1)

1 +

1 +

≤

≤

ZRD k
+
x
k

k

Moreover, for Z = (Z1, . . . , ZD)

∼ ND(µ, Σ) we have

π(x)
k

+

e
z
k
k

P(dz)

sZRD k

e
P(dz).

z

2
k

e

(7.2)

z

2
k

ZRD k

P(dz) = E

e

Z
k

2
k

h

i

D

= E

Z 2
i

#

"
Xi=1
D
E [Zi]2 +

=

=

Xi=1
2 + trace (Σ) .

Xi=1
µ
k
k

D

Var (Zi)

MARKOV DECISION PROCESSES UNDER MODEL UNCERTAINTY

23

In the next step, we write x = (x(j)

i=1,...,m , use the Cauchy–Schwarz inequality, and compute

i )j=1,...,D
m(x)
µ
k
k

−

2

µ
k

k

(

≤

+

)2
m(x)
k
k

1
m

1
m

2

D

v
u
u
t

Xj=1  

D

v
Xj=1  
u
u
t
1
x
√m k

m

Xi=1
m

x(j)
i

!

x(j)
i

Xi=1 (cid:16)

ε2 +

2

2




2

2

!


(cid:17)
1
m

k
(cid:19)

≤

ε +

≤ 



ε +

≤ 



=

ε +

(7.3)

Further, we write y = (y(j)

i

(cid:18)
)j=1,...,D
i=1,...,m , and obtain with the Cauchy–Schwarz inequality that

(cid:19)

(cid:18)

(cid:0)

(cid:1)

1 +

x
k

2
k

.

m(y)
k

−

2 =
m(x)
k

1
m2

(7.4)

D

m

y(j)
i −

2

x(j)
i

!

(cid:17)

Xj=1  

D

m

Xi=1 (cid:16)
y(j)
i −

1
m

≤

Xi=1 (cid:16)
Xj=1
1
x
√m k

k

x(j)
i

2

y
= k

(cid:17)

2
k

x

−
m

ε2
m

.

≤

The above inequality (7.4), and
Schwarz inequality that

m(x)
k

k ≤

(see also (7.3)) imply together with the Cauchy–

1

1

1

trace(Σ) =

m

=

m

≤

m

=

m

≤

m

≤

m

1

−
1

−

2

−
2

−
2

−
2

−
2

(7.5)

m

trace

Xi=1
m

D

Xi=1
m

Xj=1 (cid:16)
D

(yi −

m(y))(yi −

m(y))T

(cid:0)
y(j)
i −

(cid:1)

m(y)(j)

2

(cid:17)

y(j)
i

2

+

m(y)(j)
(cid:16)

2

(cid:21)

(cid:17)

Xi=1
y
1 k

Xj=1 (cid:20)(cid:16)
2 +
k

m

2m

(cid:17)

2
m(y)
k

1 k

y
(
k

1

x

k

−

(ε +

1

x
k

)2 +
k

ε2 + 1

1 +

−
+

x
k

)2 +
k
2m

2m

m

1

(cid:18)

+

m

−
2
k

x
k

≤

m

1
−
ε2 + 1
(cid:0)
1 ·
m
Hence, by combining (7.1), (7.2), (7.3), and (7.5) we have

(cid:1) (cid:0)
x
1 +
k

2
k

= 4

m

−

(cid:0)

(cid:1)

(cid:1)

·

.

−

m(y)
(
k

1

m(x)
k

−

+

)2
m(x)
k
k

+

−
ε
√m
2m

2

x

1
√m k
k
(cid:19)
ε2 + 1

1 ·

m ·

(1 +

x
k

2)
k

1 +

y
k

P(dy)
k

≤

1 +

+

x
k

k

ZΩloc

ε2 +

(1 +

1
m

x
k

2) + 4
k

·

ε2 + 1
m

1 ·

−

(1 +

x
k

2)
k

≤  
= CP ·
as required in Assumption 2.2 (ii).

r
(1 +

1 +

ε2 +

+ 4

·

(1 +

x
k

)
k

1 ! ·

s(cid:18)
1
m

x
k

) ,
k

(cid:19)
ε2 + 1
m

−

Since Assumption 2.2 (ii) is fulﬁlled, the fulﬁlment of Assumption 2.2 follows now with an appli-
cation of Proposition 3.3. Thus, to verify the assumptions of Proposition 3.3, we need to show that
Ωloc ∋
is nonempty, compact-valued, and continuous. This, in turn

M1(RD), τ1

(x) ։

7→

P

x

e

(cid:0)

(cid:1)

24

A. NEUFELD, J. SESTER, M. ˇSIKI ´C,

follows from Proposition 3.2 once we have shown that Ωloc ∋
compact-valued, and continuous and that

x ։ Θ(x)

RD

×

⊆

RD

×

D is nonempty,

(x, µ, Σ)
{

|

x

∈

Ωloc, (µ, Σ)

(7.6)

is continuous.

Θ(x)

∈
(x, µ, Σ)

} →

7→

To that end, let x

Ωloc.
The non-emptiness of Θ(x) follows by deﬁnition.

∈

M1(RD), τ1)
(
P(x, µ, Σ) :=

ND(µ, Σ).

b

∈

To show the compactness of Θ(x), let (µ(n), Σ(n))n
N as well as Σ(n) = c(y(n)) for some y(n)

for all n
k ≤
to the Bolzano–Weierstrass theorem there exists a subsequence (µ(nk), y(nk))k
N
⊆
∈
that y(nk)
m(x)
−
k ≤
Since c is continuous we obtain that (µ(nk), Σ(nk))
Θ(x) for k
x

µ
→
k
(µ, Σ) := (µ, c(y))

→
To show the upper hemicontinuity of Θ, let (x(n))n
N with
∈

N
∈
Θ(x(n)) for all n

Ωloc with (x(n))

ε, and µ(nk)

Ωloc with

RD with

y
k

k ≤

⊆
Ωloc with

Θ(x). Then, we have
y(n)
k

∈
∈

→

⊆

−

−

∈

∈

∈

∈

µ

x

x

y

N

−

k ≤

m(x)

µ(n)
k

ε
ε. Then, according
Ωloc such
.

×
ε for k

RD

→ ∞

.

→ ∞
Ωloc for n

ε and that Σ(n) = c(y(n)) with
(cid:0)
m(x(n))

µ(n)

∈

µ(n), Σ(n)

→ ∞
→
N that
N. We have for all n
∈
∈
ε for some y(n)
Ωloc. Therefore,
k ≤
, the continuity of m ensures for every
2ε. Hence, there exists according to the Bolzano–Weierstrass
(cid:13)
(cid:13)
k ≤
RD. Therefore, since m
N with µ(nk)
(cid:13)
(cid:13)
for some µ
∈

y(n)
||
m(x(n))

−
m(x)

µ for k

(cid:13)
(cid:13)
→ ∞

(cid:1)
+

x(n)

→

−

∈

∈

−

as well as (µ(n), Σ(n))n
m(x(n))
µ(n)
k
µ(n)
since
≤
µ(n)
n large enough that
(cid:13)
(cid:13)
k
theorem a subsequence (µ(nk))k
(cid:13)
(cid:13)
is continuous, we obtain

k ≤
m(x)

(cid:13)
(cid:13)

−

−

−
m(x)

(7.7)

µ
k

−

m(x)
k

= lim
k

µ(nk)

Analogously, we have that
−
implies the existence of a subsequence (y(nk))k
limk
for k

y(nk)
. Thus, the upper hemicontinuity follows with Lemma B.3.

ε. Then, for Σ := c(y) we have (µ, Σ)

→∞ (cid:13)
(cid:13)
x(n)
x
(cid:13)
k
N converging against some y
∈

(cid:17)(cid:13)
(cid:13)
< 2ε for every n large enough. This
(cid:13)
x
=
k
(µ, Σ)

y
k
∈
Θ(x) and (µ(nk), Σ(nk))

Ωloc with

y(n)
k

x(nk)

k ≤ ||

→∞ ||

y(n)

k ≤

−

+

−

−

∈

x

k

→ ∞

To show the lower hemicontinuity of Θ we consider a sequence (x(n))n

−
→
Ωloc with x(n)
∈
ε as well as Σ = c(y)

→

x

Ωloc for n
for some y

RD with

and some (µ, Σ)
y
k

k ≤

−

→ ∞
∈

x

Θ(x). We have by deﬁnition

∈
ε. We deﬁne for every n

N

∈

µ
k

−

N
∈
m(x)

⊆
|| ≤

ε.

≤

−

m

x(nk)
(cid:16)
x(n)
k

µ(n) :=

1

µ +

m

x(n)

.

1
n

−

1
n

(cid:19)

(cid:18)
m(x), there exists a subsequence (x(nk))k
→
1). This implies for all k
< ε/(nk −

∈

(cid:0)

(cid:1)

N that

N such that for
∈

N we have

Then, due to the convergence m(x(n))
m(x)
every k
(7.8)
µ(nk)
k

m(x(nk))
k

m(x(nk))

1
nk

(cid:13)
(cid:13)

−

−

−

=

−

∈

µ

1

(cid:13)
(cid:13)

m(x(nk))
(cid:13)
(cid:13)
+
(cid:13)

m(x)
k

−

(cid:16)
1

(cid:16)

≤

1
nk

−

(cid:17) (cid:13)
(cid:13)
(cid:13)
(cid:17) (cid:18)

N

∈
y(n) :=

µ

k

1

Next, we deﬁne for all n

(7.9)

(cid:18)
We obtain by the convergence x(n)
x(nkl )
< ε/(nkl −
1) for all l
k

−

x

k

→
∈

1
n

−

(cid:19)

y +

1
n

x(n),

x for n

→ ∞
N. This implies

m(x)

−

(cid:13)
(cid:13)
(cid:13)

m(x(nk))
(cid:13)
(cid:13)
(cid:13)

Σ(n) := c

.

y(n)
(cid:16)

(cid:17)

(7.10)

y(nk l)

−

(cid:13)
(cid:13)
(cid:13)

x(nk l)

=

(cid:13)
(cid:13)
(cid:13)

≤

≤

1
(cid:18)
1
(cid:18)
1

(cid:16)

1
nkl (cid:19)
1
nkl (cid:19)

−

−

1
nkl

−

(cid:17) (cid:18)

y

−

·

x(nk l)

(cid:13)
(cid:13)
(cid:13)

·

k
(cid:16)
ε +

(cid:13)
(cid:13)
(cid:13)
+

x

y

x

k

−

ε
nkl −

1

(cid:13)
(cid:13)
(cid:13)
(cid:19)

x(nk l)

−

= ε.

(cid:17)

(cid:13)
(cid:13)
(cid:13)

≤

(cid:19)

1

(cid:16)

1
nk

−

(cid:17) (cid:18)

ε +

ε
nk −

1

(cid:19)

= ε.

the existence of a subsequence x(nkl ) such that

25

N

∈
→

MARKOV DECISION PROCESSES UNDER MODEL UNCERTAINTY

· · ·

D

(cid:12)
Xi=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
D

(cid:1)

∈

Θ

Hence, with (7.8), (7.9), and (7.10), we have shown the existence of a subsequence
N and such that, by the continuity of r,
with
(µ, Σ) for l
(cid:0)

. This implies the lower hemicontinuity of Θ by Lemma B.4.

µ(nk l), Σ(nk l)

for all l

x(nk l)

→ ∞

It remains to show that the map deﬁned in (7.6) is continuous with respect to τ1.
To that end, consider a sequence (x(n))n
Θ(x(n)) for all n

Ωloc as well as a sequence (µ(n), Σ(n))n
Ωloc ×

⊆
x(n), µ(n), Σ(n)
N and such that
∈
∈ M1(RD) for n
ND(µ(n), Σ(n))
. Then we write P(n) :=

(µ(n), Σ(n))
n
M1(RD). The characteristic function of P(n), denoted by

→
N as well as P :=

(x, µ, Σ)

→ ∞

(cid:0)
(cid:0)

(cid:1)
∈

∈

∈

∈

(cid:0)

(cid:1)

(cid:0)

N

∈

l

µ(nk l), Σ(nk l)
µ(nk l), Σ(nk l)
(cid:1)
(cid:1)

ND(µ, Σ)

∈

N with
∈
Θ(x) for

converges for n

→ ∞

RD

u

7→

∋
pointwise against
RD

u

∋

7→

ϕP(n)(u) := exp

iuT µ(n)
(cid:16)

−

1

2 uT Σ(n)u
(cid:17)

ϕP(u) := exp

iuT µ

1

2 uT Σu

,

−

which is the characteristic function of P, and hence by L´evy’s continuity theorem (see, e.g., [25,
Theorem 19.1]) we have P(n)
The convergence of P(n)

.
→
P with respect to τ1 now follows with, e.g., [10, Example 3.8.15], since

P weakly, i.e., in τ0 for n

→ ∞

(cid:1)

(cid:0)

(P(n))n

N, and P are Gaussian.
∈

→

To verify Assumption 2.4 ﬁrst note that r is continuous, and hence Assumption 2.4 (i) is fulﬁlled.
A. Then, the Cauchy–

,

m+2,

Rt+1)

Ωloc, and let at, a′t ∈

∈

Let Xt, X ′t ∈
Schwarz inequality implies

Ωloc, Xt+1 = (

Rt
−

r(Xt, at, Xt+1)

r(X ′t, a′t, Xt+1)

−

=

(ai

t −

a′t

i)

R

i
t+1

≤ kRt+1k · k

at −

a′tk ≤ k

Xt+1k · k

at −

,
a′tk

(cid:12)
(cid:12)
implying Assumption 2.4 (ii). Moreover, we have by using the Cauchy–Schwarz inequality that

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

r(Xt, at, Xt+1)
|
|

=

(cid:12)
(cid:12)
(cid:12)
as required in Assumption 2.4 (iii).
(cid:12)
(cid:12)

Xi=1

atk · kRt+1k ≤

≤ k

i
ai
t+1
tR

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Acknowledgments

a

max
a
∈

A k

,
Xt+1k

k · k

(cid:3)

Financial support by the MOE AcRF Tier 1 Grant RG74/21 and by the Nanyang Assistant Pro-
fessorship Grant (NAP Grant) Machine Learning based Algorithms in Finance and Insurance is
gratefully acknowledged.

We present an explicit numerical algorithm that can be applied to compute the optimal value

Appendix A. Numerics

function V and to determine an optimal policy a∗ ∈ A
A.1. Value Iteration. Theorem 2.7 directly provides an algorithm for the computation of the
optimal value V (x) to which we refer as the value iteration algorithm.
In this algorithm we start with an arbitrary V (0)
V (n+1) :=

Cp(Ωloc, R) and then compute recursively

N0. According to Theorem 2.7 we then have

V (n) for all n

∈

.

T

∈

(A.1)

V (n) = V.

lim
→∞ T
n

Compare also, e.g., [7, Section 7], where this algorithm (in a non-robust setting) is discussed in
detail.

A.2. Numerical Algorithm. With Algorithm 1 we present a pseudocode of the methodology
that can be applied to compute both the optimal value function and the optimal policy nu-
merically. The algorithm relies on the value iteration principle. This means we solve (A.1) by
approximating the value function V through neural networks7 and by repeatedly applying the
V (n). Note that Algorithm 1 approximates an optimal one-step action
recursion V (n+1)

:=

T

7For a general introduction to neural networks we refer the reader to [28], for applications of neural networks in

ﬁnance see [16], for a proof of the universal approximation property of neural networks compare [23].

26

A. NEUFELD, J. SESTER, M. ˇSIKI ´C,

aloc∗ ∈ Aloc. According to Theorem 2.7 an approximation of the optimal policy can then be
obtained as a∗ := (aloc∗(X0), aloc∗(X1), . . . )
.

∈ A

Algorithm 1: Value Iteration

Input : Batch Size B

N; Hyperparameters for the neural networks; Number of epochs E;

∈

Number of iterations Iterv for the improvement of the value function; Number of
iterations Itera for the improvement of the action function; Number of measures
; Number of Monte-Carlo simulations NMC; State space Ωloc; Action space A;
N
Reward function r;

P

Initialize a neural network V 0;
Initialize a neural network aloc;
for epoch = 1, . . . , E do
Set V epoch = V epoch
for iteration = 1, . . . , Itera do
// We maximize infP0

−

1 and freeze the weights of V epoch

1;

−

(x,aloc(x)) EP0[r(x, aloc(x), X1) + αV epoch

1(X1)] with

−

∈P

respect to aloc ∈ Aloc.

Sample a batch of states (xi)i=1,...,B ⊆
for i = 1, . . . , B do

Ωloc;

Pick measures P(i)

1 , . . . , P(i)

(xi, aloc(xi));

NP ∈ P

end
Denote by X (j)
P
Sample X (j)
∈
Deﬁne for i = 1, . . . , B:

1,P for all P

n

∈ M1(Ωloc) for j = 1, . . . , NMC;

P(i)
1 , . . . , P(i)

NP

1,P a random variable that is sampled according to a measure

1, . . . , NMC}

, i

∈ {

1, . . . , B

∈ {

;
}

, j

o

V (xi) :=

T

P

Maximize

d

min
1 ,...,P(i)

NP o

∈nP(i)

1
NMC

NMC

Xj=1

B

r

xi, aloc(xi), X (j)
1,P
(cid:16)

(cid:17)

+ αV epoch

1

−

X (j)
1,P

;

(cid:16)

(cid:17)

with respect to parameters from the neural network aloc (e.g. back-propagation with
a stochastic gradient descent algorithm).

d

V (xi)

T

Xi=1

end
for iteration = 1, . . . , Iterv do

// We minimize the quadratic error between V epoch(x) and the

approximation of
supaloc∈Aloc infP0
V epoch
T
x from a batch of sampled values.

∈P

−

(x,aloc(x)) EP0[r(x, aloc(x), X1) + αV epoch

1(X1)] =

−

1(x), that was computed in the previous step, for all states

Sample Batch of states (xi)i=1,...,B ⊆
Minimize

Ωloc;

B

Xi=1 (cid:16)
with respect to parameters from V epoch.

end

V epoch(xi)

2

V (xi)

(cid:17)

−

T

d

end
Output: Neural network V E approximating the optimal value function;
Neural network aloc approximating the optimal one-step policy;

MARKOV DECISION PROCESSES UNDER MODEL UNCERTAINTY

27

Appendix B. Supplementary Results

The ﬁrst auxiliary result is Banach’s ﬁx point theorem, compare, e.g., [7, Theorem A 3.5.], or

any standard monograph on analysis or functional analysis.

Theorem B.1 (Banach’s Fix Point Theorem). Let M be a complete metric space with metric
(0, 1) such that
d(x, y) and let
d(

M be an operator such that there exists a number β

→
βd(v, w) for all v, w

M . Then, we have that

: M

∈

T

T

w)

v,
T
(i)
T
(ii) limn
(iii) For v

≤
has a unique ﬁx point v∗ in M , i.e.,

∈

nv = v∗ for all v

M we obtain

→∞ T
∈

M .

∈

d(v∗,

v∗ = v∗.

T

βn

nv)

T

d(

v, v).

≤

1

β

T

−
The following result, Berge’s Maximum Theorem, can for example be found in [2, Theorem 17.31].
Theorem B.2 (Berge’s Maximum Theorem). Let ϕ : X ։ Y be a upper and lower hemicontin-
uous correspondence between topological spaces with nonempty compact values, and suppose that
f :

R is continuous. Then the following holds.

ϕ(x)

Y

y

X

(x, y)
|
{
(i) The function

×

∈

∈

} →

m : X

x

→

7→

R

max
ϕ(x)
y
∈

f (x, y)

is continuous.
(ii) The correspondence

c : X ։ Y
y
x

7→ {

has nonempty, compact values.

ϕ(x)

f (x, y) = m(x)
}

|

∈

(iii) If Y is Hausdorﬀ, then r is upper hemicontinuous.

The following two lemmas provide characterizations of upper and lower hemicontinuity, respec-

tively. The results can be found, e.g., in [2, Theorem 17.20], and [2, Theorem 17.21].

Lemma B.3 (Upper Hemicontinuity). Assume that the topological space X is ﬁrst countable and
that Y is metrizable. Then, for a correspondence ϕ : X ։ Y the following statements are equivalent.

∈

(i) The correspondence ϕ is upper hemicontinuous and ϕ(x) is compact for all x
(x(n), y(n))
(ii) For any x

X, if a sequence

∈
x for n

X.

n

N ⊆
∈
→

Gr ϕ satisﬁes x(n)
ϕ(x) for k
y

→
.
→ ∞

, then

→ ∞

k

y(nk)
(cid:0)

there exists a subsequence

N with y(nk)
(cid:1)
∈
Lemma B.4 (Lower Hemicontinuity). For a correspondence ϕ : X ։ Y between ﬁrst countable
topological spaces the following statements are equivalent.
(i) The correspondence ϕ is lower hemicontinuous.
(ii) For any x
x(nk)

ϕ(x) there exists a subsequence

N such that y(k)

, then for each y

x for n
ϕ

for each k

→ ∞
x(nk)

y for k

∈

∈

∈

(cid:0)

(cid:1)

.

X, if x(n)
N and elements y(k)
∈

→

k

∈

→ ∞

→

∈

(cid:0)

(cid:1)

(cid:0)

(cid:1)

References

[1] Victor Aguirregabiria and Pedro Mira. Swapping the nested ﬁxed point algorithm: A class of estimators for

discrete Markov decision models. Econometrica, 70(4):1519–1543, 2002.

[2] Charalambos D. Aliprantis and Kim C. Border. Inﬁnite dimensional analysis. Springer, Berlin, third edition,

2006. A hitchhiker’s guide.

[3] Andrea Angiuli, Nils Detering, Jean-Pierre Fouque, and Jimin Lin. Reinforcement learning algorithm for mixed

mean ﬁeld control games. arXiv preprint arXiv:2205.02330, 2022.

[4] Andrea Angiuli, Jean-Pierre Fouque, and Mathieu Lauriere. Reinforcement learning for mean ﬁeld games, with

applications to economics. arXiv preprint arXiv:2106.13755, 2021.

[5] Nicole B¨auerle and Alexander Glauner. Q-learning for distributionally robust Markov decision processes. In

Modern Trends in Controlled Stochastic Processes:, pages 108–128. Springer, 2021.

[6] Nicole B¨auerle and Ulrich Rieder. MDP algorithms for portfolio optimization problems in pure jump markets.

Finance and Stochastics, 13(4):591–611, 2009.

28

A. NEUFELD, J. SESTER, M. ˇSIKI ´C,

[7] Nicole B¨auerle and Ulrich Rieder. Markov decision processes with applications to ﬁnance. Springer Science &

Business Media, 2011.

[8] Claude Berge. Espaces topologiques: Fonctions multivoques. Collection Universitaire de Math´ematiques, Vol. III.

Dunod, Paris, 1959.

[9] Francesco Bertoluzzo and Marco Corazza. Reinforcement learning for automatic ﬁnancial trading: Introduction
and some applications. University Ca’Foscari of Venice, Dept. of Economics Research Paper Series No, 33, 2012.

[10] Vladimir Igorevich Bogachev. Gaussian measures. Number 62. American Mathematical Soc., 1998.
[11] Stephen Boyd, Enzo Busseti, Steve Diamond, Ronald N Kahn, Kwangmoo Koh, Peter Nystrup, and Jan Speth.
Multi-period trading via convex optimization. Foundations and Trends® in Optimization, 3(1):1–76, 2017.
[12] Jay Cao, Jacky Chen, John Hull, and Zissis Poulos. Deep hedging of derivatives using reinforcement learning.

The Journal of Financial Data Science, 3(1):10–27, 2021.

[13] Ying-Hua Chang and Ming-Sheng Lee. Incorporating Markov decision process on genetic algorithms to formulate

trading strategies for stock markets. Applied Soft Computing, 52:1143–1153, 2017.

[14] Zhi Chen, Pengqian Yu, and William B Haskell. Distributionally robust optimization for sequential decision-

making. Optimization, 68(12):2397–2426, 2019.

[15] Philippe Cl´ement and Wolfgang Desch. Wasserstein metric and subordination. Studia Mathematica, 1(189):35–52,

2008.

[16] Matthew F Dixon, Igor Halperin, and Paul Bilokon. Machine Learning in Finance. Springer, 2020.
[17] Jiayi Du, Muyang Jin, Petter N Kolm, Gordon Ritter, Yixuan Wang, and Bofei Zhang. Deep reinforcement

learning for option replication and hedging. The Journal of Financial Data Science, 2(4):44–57, 2020.

[18] Angelos Filos. Reinforcement learning for portfolio management. PhD thesis, Imperial College London, 2019.
[19] Carl Gold. FX trading via recurrent reinforcement learning. In 2003 IEEE International Conference on Compu-

tational Intelligence for Financial Engineering, 2003. Proceedings., pages 363–370. IEEE, 2003.

[20] Allan Gut. The multivariate normal distribution. In An Intermediate Course in Probability, pages 117–145.

Springer, 2009.

[21] Igor Halperin. QLBS: Q-learner in the Black-Scholes (-Merton) worlds. The Journal of Derivatives, 28(1):99–122,

2020.

[22] Ben Hambly, Renyuan Xu, and Huining Yang. Recent advances in reinforcement learning in ﬁnance. arXiv

preprint arXiv:2112.04553, 2021.

[23] Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):251–257,

1991.

[24] Yuh-Jong Hu and Shang-Jen Lin. Deep reinforcement learning for optimizing ﬁnance portfolio management. In

2019 Amity International Conference on Artiﬁcial Intelligence (AICAI), pages 14–20. IEEE, 2019.

[25] Jean Jacod and Philip Protter. Probability essentials. Universitext. Springer-Verlag, Berlin, second edition, 2003.
[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,

2014.

[27] Frank Hyneman Knight. Risk, uncertainty and proﬁt, volume 31. Houghton Miﬄin, 1921.
[28] Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.
[29] Yuxi Li, Csaba Szepesvari, and Dale Schuurmans. Learning exercise policies for American options. In Artiﬁcial

Intelligence and Statistics, pages 352–359. PMLR, 2009.

[30] Eva L¨utkebohmert, Thorsten Schmidt, and Julian Sester. Robust deep hedging. Quantitative Finance, 2022.
[31] John Moody, Lizhong Wu, Yuansong Liao, and Matthew Saﬀell. Performance functions and reinforcement learning

for trading systems and portfolios. Journal of Forecasting, 17(5-6):441–470, 1998.

[32] Ariel Neufeld, Julian Sester, and Daiying Yin. Detecting data-driven robust statistical arbitrage strategies with

deep neural networks. arXiv preprint arXiv:2203.03179, 2022.

[33] John Rust. Structural estimation of Markov decision processes. Handbook of econometrics, 4:3081–3143, 1994.
[34] Manfred Sch¨al. Markov decision processes in ﬁnance and dynamic options. In Handbook of Markov decision

processes, pages 461–487. Springer, 2002.

[35] Salman Sadiq Shuvo, Yasin Yilmaz, Alan Bush, and Mark Hafen. A Markov decision process model for socio-
economic systems impacted by climate change. In International Conference on Machine Learning, pages 8872–
8883. PMLR, 2020.

[36] Sorawoot Srisuma and Oliver Linton. Semiparametric estimation of Markov decision processes with continuous

state space. Journal of Econometrics, 166(2):320–341, 2012.

[37] Kerem U˘gurlu. Robust optimal control using conditional risk mappings in inﬁnite horizon. Journal of Computa-

tional and Applied Mathematics, 344:275–287, 2018.

[38] C´edric Villani. Optimal transport: old and new, volume 338. Springer, 2009.
[39] Douglas J White. A survey of applications of markov decision processes. Journal of the operational research

society, 44(11):1073–1096, 1993.

[40] Zhuoran Xiong, Xiao-Yang Liu, Shan Zhong, Hongyang Yang, and Anwar Walid. Practical deep reinforcement

learning approach for stock trading. arXiv preprint arXiv:1811.07522, 2018.

[41] Huan Xu and Shie Mannor. Distributionally robust markov decision processes. Mathematics of Operations Re-

search, 37(2):288–300, 2012.

[42] Pengqian Yu, Joon Sern Lee, Ilya Kulyatin, Zekun Shi, and Sakyasingha Dasgupta. Model-based deep reinforce-

ment learning for dynamic portfolio optimization. arXiv preprint arXiv:1901.08740, 2019.

[43] Man-Chung Yue, Daniel Kuhn, and Wolfram Wiesemann. On linear optimization over wasserstein balls. arXiv

preprint arXiv:2004.07162, 2020.

MARKOV DECISION PROCESSES UNDER MODEL UNCERTAINTY

29

[44] Zihao Zhang, Stefan Zohren, and Stephen Roberts. Deep reinforcement learning for trading. The Journal of

Financial Data Science, 2(2):25–40, 2020.

