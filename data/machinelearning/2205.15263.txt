BSNSING: A DECISION TREE INDUCTION METHOD BASED ON
RECURSIVE OPTIMAL BOOLEAN RULE COMPOSITION

2
2
0
2

y
a
M
0
3

]

G
L
.
s
c
[

1
v
3
6
2
5
1
.
5
0
2
2
:
v
i
X
r
a

Yanchao Liu
Department of Industrial & Systems Engineering
Wayne State University
Detroit, Michigan, USA
yanchaoliu@wayne.edu

ABSTRACT

This paper proposes a new mixed-integer programming (MIP) formulation to optimize split rule
selection in the decision tree induction process, and develops an efﬁcient search algorithm that is
able to solve practical instances of the MIP model faster than commercial solvers. The formulation
is novel for it directly maximizes the Gini reduction, an effective split selection criterion which
has never been modeled in a mathematical program for its nonconvexity. The proposed approach
differs from other optimal classiﬁcation tree models in that it does not attempt to optimize the whole
tree, therefore the ﬂexibility of the recursive partitioning scheme is retained and the optimization
model is more amenable. The approach is implemented in an open-source R package named bsnsing.
Benchmarking experiments on 75 open data sets suggest that bsnsing trees are the most capable of
discriminating new cases compared to trees trained by other decision tree codes including the rpart,
C50, party and tree packages in R. Compared to other optimal decision tree packages, including
DL8.5, OSDT, GOSDT and indirectly more, bsnsing stands out in its training speed, ease of use and
broader applicability without losing in prediction accuracy.

Keywords Classiﬁcation Trees, Mixed-Integer Programming, Statistical Computing, R

1

Introduction

Classiﬁcation is the task of assigning objects to one of several predeﬁned categories. A classiﬁcation tree (or a decision
tree classiﬁer) is a predictive model represented in a tree-like structure. Without making excessive assumptions about
the data distribution, a classiﬁcation tree partitions the input space into rectilinear (axis-parallel) regions and ultimately
gives a set of If...Then... rules to classify outputs or make predictions. Starting from the root node, each internal node
is split into two or more child nodes based on the input values. The split stops when some terminal condition is met.
The terminal nodes of the tree are called leaves, which represent the predicted target. Cases move down the tree along
branches according to their input values and all cases reaching a particular leaf are assigned the same predicted value.
The tree-like structure connects naturally to the divide-and-conquer strategy of how people judge, plan and decide.
Therefore, the technique is widely adopted for making decisions that bear substantial consequences for the decision
maker. Example applications include disease diagnosis (Tanner et al. 2008, Ghiasi et al. 2020), loan approval (Mandala
et al. 2012, Alaradi and Hilal 2020) and investment selection (Sorensen et al. 2000).

In this paper, we propose a classiﬁcation tree induction method based on solving a mixed-integer programming (MIP)
model at each split of a node. This new method can generate trees that frequently outperform trees built by other
off-the-shelf tree libraries in R, the popular statistical computing system. To achieve this, the proposed MIP model
explicitly maximizes the reduction in the node impurity and allows a tree node to be split by a multivariate boolean rule.
Such split rules are more ﬂexible and in certain cases more efﬁcient at characterizing nonlinear patterns in data, and in
the meantime, remain highly interpretable. To conquer the computational challenge, we develop an efﬁcient implicit
enumeration algorithm that solves the MIP model faster than the state-of-the-art optimization solvers. Experiments on
an extensive collection of machine learning data sets suggest that the method is accurate in prediction performance and

 
 
 
 
 
 
Y. Liu

scales reasonably well on large training sets. The proposed framework is implemented in an open-source R package
named bsnsing, available for a broad community of data science researchers and practitioners.

While MIP techniques have been used in various ways in the recent literature for building classiﬁers, our method is
unique in several aspects. First, to our knowledge, it is the ﬁrst MIP model that is able to maximize the Gini reduction
of a split, which has been known to be an effective split criterion but in the meantime a nonlinear nonconvex function
of the split decision. Impurity reduction is an oft-used criterion for split selection in leading decision tree heuristics
for its superiority in generating well-balanced child nodes over the accuracy maximization criterion, but it has never
been used in an optimization framework due to its nonconvexity. Second, the MIP model is used for rule selection at
each node split, while other effective elements of the recursive partitioning framework, such as split point generation,
early termination and tree pruning, can be separately implemented with great ﬂexibility. Third, along with the novel
formulation we also develop an efﬁcient exact solution algorithm which runs faster than commercial solver codes,
making the bsnsing package independent of any commercial optimization solvers.

The remainder of the paper is organized as follows. Section 2 reviews the literature on decision tree induction and
particularly the recent literature on optimal classiﬁcation tree (OCT) developments based on mixed-integer optimization.
Section 3 develops the main models and algorithms that underlie the bsnsing package. Section 4 presents computational
experiments to demonstrate the effectiveness of the proposed method and software tool. Section 5 concludes the paper
with pointers for future work.

2 Related Literature

As an extremely ﬂexible non-parametric framework, classiﬁcation trees delegate a great deal of freedom to algorithm
design and implementation (Tan et al. 2005). The entire search space for building the “best” tree can be enormous.
Consider, for instance, splitting a node by a categorical variable consisting of 10 distinct levels. There are 115,974
non-trivial ways of splitting, i.e., B10 − 1, the 10th Bell number minus 1. Moreover, for a set of 10 single-variable
split rules, there are more than 3.6 million (i.e., 10!) differnt ways to order them in a decision list. It is impractical to
evaluate all possible splits and all possible ordering of rules. It is shown in Hyaﬁl and Rivest (1976) that the “optimal
decision tree” problem is NP-complete, and this conclusion has been corroborated in many subsequent attempts at
constructing optimal decision trees using various optimization modeling techniques.

The difﬁculty incurred by the enormity of the search space has been dealt with along three routes in the literature. The
ﬁrst route is via using greedy splitting methods (Breiman et al. 1984, Quinlan 1993) under the recursive partitioning
framework, in which a number of candidate splits are compared and a best one is chosen to split a node. In this general
paradigm, there is a great variety of algorithms addressing issues such as how the split variables are selected, how
the split points are determined and how the split quality is assessed, etc. Many efﬁcient decision tree algorithms,
including C4.5 (Quinlan 1993), CHAID (Kass 1980), CART (Breiman et al. 1984), GUIDE (Loh 2009), and the recent
Bayesian-based approach (Letham et al. 2015, Yang et al. 2017) fall under this paradigm.

The second route is to trim the overall search space down to a reduced model space (as a surrogate) in which global
optimization is used to ﬁnd an optimal model. A popular choice of the surrogate space is the frequent itemsets, e.g.,
results from association rule mining algorithms (Agrawal et al. 1993, Borgelt 2012, Liu et al. 1998). Bertsimas et al.
(2012) devised a two-step approach, in which the ﬁrst step is to generate an efﬁcient frontier of L candidate itemsets
by solving L mixed-integer optimization (MIO) problems, one for each candidate itemset, and then solve another
(larger) MIO problem to rank all the candidates based on their predictive accuracy on all transactions. The top-ranked
candidate is chosen as the ﬁnal classiﬁer. This approach is computationally demanding because of the attempt to build
the whole classiﬁer by solving one large MIO problem. Nijssen and Fromont (2010) noted the link between decision
trees over a binary feature space and the itemset lattice, and built a recursive tree learning algorithm, which did not
invoke a numerical optimization process. Angelino et al. (2017) considered the class of rule lists assembled from
pre-mined frequent itemsets and searched for an optimal rule list that minimizes a regularized risk function, which was
able to solve (and prove optimality for) fairly large classiﬁcation instances. This stream of research is important in
constructing the notion of optimality in tree learning and exploring the use of discrete optimization techniques such as
the branch-and-bound algorithm.

The third route is using exhaustive search for comprehensible rules that do not involve too many clauses. The
1R algorithm (Holte 1993) searched exhaustively the space of single-variable rules and then made classiﬁcation or
prediction based on the best rule found. Considering its simplicity, it was a surprise that it performed well on many data
sets. Nonetheless, the single-variable, single-split strategy will apparently sacriﬁce performance in cases where more
complex and subtle patterns need to be characterized. The EXPLORE algorithm (Rijnbeek and Kors 2010) performs
an exhaustive search in the complete rule spaces consisting of 1 term, 2 terms and so forth, until the increment of
the number of allowed terms stops giving a better performance on the validation set than the previous iteration. By

2

Y. Liu

searching all possible Disjunctive Normal Forms (DNF), the algorithm can ﬁnd the best DNF rule up to a certain
complexity level. An important observation given by this paper is that the phenomenon of oversearching (Quinlan and
Cameron-Jones 1995), i.e., the hypothesis that the more rules are evaluated the greater the chance of ﬁnding a ﬂuke
and poorly-generalizable rule, does not always hold. Even though exhaustive search is hardly viable for large cases,
this gives an encouraging indication that aggressively seeking optimality on the training set does not necessarily incur
overﬁtting, if the sense of optimality is deﬁned on a prudent metric and the tree complexity is properly regulated, an
insight that was also presented in Bertsimas and Dunn (2017). In this paper, we develop a more principled exhaustive
search procedure to solve the split selection problem.

Table 1: Comparison of different tree structures. The proposed multi-variable rectilinear splits are more efﬁcient than
single-variable splits in carving out nonlinear features, and preserves interpretability better than linear combination
splits.

Illustration

Characteristics

Software Codes

• Intuitive decision rules
• E.g., {Is Age ≤ 20?}
• Split only produces (n − 1)-dimensional rectilin-

ear halfspaces

ID3, C5.0, CHAID
SLIQ, rpart, party

• Intuitive decision rules
• E.g., {Is 20 ≤ Age ≤ 25 & 19 ≤ BMI ≤ 24?}
• Split can generates closed and open hypercubes of

any dimension

CORELS, bsnsing

• Obscure decision rules
• E.g., {Is 0.3 ∗ Age − 0.5 ∗ BMI ≥ 3.3?}
• Split can produce halfspaces of any dimension

CART, FACT
QUEST, CRUISE
GUIDE, OCT-H

• If...Then... rule clause
• Less efﬁcient for multi-class problem

CART, QUEST, SLIQ
rpart, bsnsing

• If...Then...ElseIf... rule clause
• Suits multi-class problem better
• Depletes data too quickly

C5.0, CHAID, FACT
CRUISE, QUEST
party

Using mixed-integer optimization to tackle classiﬁcation problems has been frequently investigated in recent years,
following the seminar work of Bertsimas et al. (2012). Malioutov and Varshney (2013) formulated the rule-based
classiﬁcation problem as an integer program and showed that under certain conditions (among which, an excessively

3

Single−variable splitMulti−variable splitLinear combination splitTwo-waysplitMulti-waysplitY. Liu

large pool of boolean questions needed to be generated from the original feature set) the rules could be recovered exactly
by solving the linear programming (LP) relaxation. A probabilistic guarantee of recovery was shown when the required
conditions were satisﬁed weakly. Goh and Rudin (2014) addressed the problem of class imbalance in classiﬁcation
training and proposed an MIP model to ﬁnd the classiﬁcation rule set that optimizes a weighted balance between positive
and negative class accuracies. They developed a “characterize then discriminate” approach to decompose the problem
into manageable subproblems hence alleviate the computational challenge of solving the full MIP. Bertsimas and Dunn
(2017) cast the problem that CART attempted to solve as a global optimization problem, and instantiated the canonical
problem with two MIP models, OCT for building trees using univariate splits and OCT-H for trees of multivariate splits
(separating hyperplanes). The models minimized a weighted sum of the total misclassiﬁcation cost and the number of
splits in the tree, and were constrained by two hyperparameters, the tree depth and the leaf node size. The weighting
factor in the objective function needed to be tuned via a validation set to achieve the best performance. The robust
version of these models were given in Bertsimas et al. (2019). The strengths of OCT and OCT-H complemented each
other and they were able to outperform CART in many cases by signiﬁcant margins. Wang et al. (2017) focused on
searching for a small number of short rules (disjunctive of conjunctives, e.g., “(A and B) or C or ... ” kind of a rule) by
approximately solving a “maximum a posteriori” problem by the simulated annealing algorithm. The authors applied
the method to predict user behavior in a recommender system and reported favorable performance. Verwer and Zhang
(2019b) formulated the optimal classiﬁcation tree (OCT) problem as an integer program in which the number of integer
(binary) decision variables would not depend on the number of training data points (though the number of constraints
would, and big-M constraints were used). The formulation was demonstrated to outperform previous OCT formulations,
including Verwer and Zhang (2017) and Bertsimas and Dunn (2017), on several test data sets.

Hu et al. (2019) proposed an optimal sparse decision trees (OSDT) algorithm that extends the CORELS algorithm
(Angelino et al. 2017) (which creates optimal rule lists) to create optimal trees. The algorithm attempts to minimize the
weighted sum of the misclassiﬁcation error and the number of leaves in the tree. A specialized search procedure within
a branch-and-bound framework is employed for solution. A Python program that implements OSDT is available on
Hu’s Github page. Lin et al. (2020a) provided a general framework for decision tree optimization which was able to
handle a variety of objective functions and optimize over continuous features. The authors observed orders of magnitude
speedup in decision tree construction compared to the state-of-the-art. A C++ based implementation, called GOSDT, as
well as a Python wrapper, is available on Lin’s Github page. Aglin et al. (2020) developed a DL8.5 algorithm which
extends DL8 initially proposed in Nijssen and Fromont (2007). DL8.5 draws upon the association rule mining literature
(Agrawal et al. 1993) and uses branch-and-bound search along with a caching mechanism to achieve a fast training
speed. It has been demonstrated that DL8.5 outruns the BinOCT method by orders of magnitude in training speed.
DL8.5 was implemented in C++ and a Python package is publicly available (i.e., pip install dl8.5). In Section 4.2, we
perform computational comparisons with DL8.5, OSDT and GOSDT on a number of binary classiﬁcation data sets to
demonstrate bsnsing’s advantage in training speed amongst these latest developments in the OCT literature. Zhu et al.
(2020) proposed an MIP model for supervised classiﬁcation by optimally organizing support vector machine (SVM)
type of separating hyperplanes in a tree structure of a given depth, and achieved outstanding performance on a collection
of test sets. An earlier work of this kind can be found in Street (2005), where the separating hyperplane was obtained via
solving a nonlinear conconvex program. Ease of interpretation (or interpretability) of such tree-like models was clearly
not an emphasis in those works. Aghaei et al. (2020) presented a ﬂow-based MIP formulation for the OCT problem.
The formulation did not use big-M constraints and hence boasted a stronger LP relaxation than alternative formulations.
The authors developed a Bender’s decomposition paradigm to further improve solution efﬁciency. Substantial speedup
in comparison to other OCT approaches was reported.

Existing MIP-based OCT investigations invariably attempted to internalize (i.e., globally optimize) the whole process
of building the (tree or rule-based) classiﬁer, and abandoned the recursive partitioning framework. Consequently, any
post-training modiﬁcation, such as pruning, to the optimal tree would nullify optimality in intractable ways, so it would
be difﬁcult to justify pruning or other tactics aimed at improving prediction performance. Furthermore, making all
decisions regarding tree induction in a single MIP model is inevitably challenged by the dilemma that, either a lot of
simpliﬁcations must be imposed to the classiﬁer to limit the size of the search space (hence introducing high bias),
or the decision model ends up being computationally prohibitive. In the author’s opinion, solving an MIP usually
takes too much time to warrant its inclusion in a meaningful hyperparameter search process which requires training a
number of candidate classiﬁers. For instance, a 10-minute run time is not uncommon for solving a moderately sized
MIP model, but it would feel somewhat long for a user of software tools such as R, SAS, Stata and IBM SPSS, which
are able to produce a tree in no more than a couple of seconds (for reasonably sized data sets, such as those oft-used for
benchmarking in the literature). As a result, software codes for most OCT approaches, despite their potential appeal
in high stake applications where training/tuning time is less of a concern than other merits such as interpretability,
accuracy and rule set sparsity, etc., have not been widely picked up by the broader statistical learning or data science
community. The bsnsing approach attempts to alleviate some of these challenges by taking a middle ground - using

4

Y. Liu

MIP to optimize only the split decision while other aspects of tree management, such as when to stop, how to generalize
candidate split points, and pruning, etc., are ofﬂoaded to the recursive partitioning framework.

Table 1 summarizes the prevalent tree structures available in software tools and how the proposed bsnsing package
expands the landscape. Existing algorithms under the recursive partitioning framework either employ one-variable
splits which are too restrictive for expressing nonlinear patterns, or employ linear combination splits which obscures
interpretation. Breiman et al. (1984) brieﬂy entertained the idea of constructing Boolean combinations of single-variable
splits and recognized the associated difﬁculty in split search because the combinations are too many to enumerate. To
our knowledge, the bsnsing package is the ﬁrst decision tree package to implement Boolean combinations of splits in
which the combinatorial difﬁculty is mitigated by an efﬁcient search algorithm.

3 Models and Methods

3.1 Preliminaries and the Framework Overview

Let X denote the input space containing all possible input vectors x. Suppose that objects characterized by x fall into J
classes and let C be the set of classes, i.e., C = {1, . . . , J}. A classiﬁer is a rule that assigns a class membership in
C to every vector in X . In other words, a classiﬁer is a partition of X into J disjoint subsets A1, . . . , Aj, X = ∪jAj,
such that for every x ∈ Aj the predicted class is j. A classiﬁer is constructed or trained by a learning sample L, which
consists of input data on n cases together with their actual class membership, i.e., L = {(x1, j1), . . . , (xn, jn)}, where
xi ∈ X and ji ∈ {1, . . . , J}, for i = 1, . . . , n. At present, we consider the binary classiﬁcation problem where J = 2
and call the two classes as being positive and negative, respectively, i.e., C = {Positive, Negative}.

Figure 1: Process ﬂow of the bsnsing method. First, a binary feature matrix B is created based on the original input
variables. Each feature represents a general question that demands a yes/no answer. Next, a mixed-integer program
(MIP) is solved to select the set of questions to form a boolean OR-clause that would maximize impurity reduction.
Finally, the tree node is split by the selected rule(s). Detailed annotation of the bsnsing tree plot is given in the
appendix.

Given a learning sample L available at a tree node, the algorithm takes two steps to split the node. First, all input
variables are coded into binary features. Each binary feature represents a question that demands a Yes/No answer based
on the value of the original variable. We call this process binarization of the input space. The outcome of this step
includes (1) an n-by-m matrix B consisting of 0/1 entries, where m is the total number of binary features created in the
process, and (2) the original binary response vector y in the sample L.

5

Y. Liu

The second step determines a boolean OR-clause to split the node. Here, a boolean OR-clause refers to a set of general
questions joined by the logical OR operator, e.g., {Is Age > 35 or Age ≤ 28 or BMI ≥ 30?}. If a case answers yes
to any question in the clause, it will be classiﬁed as a positive case; otherwise, it will be classiﬁed as a negative case.
The selection of questions into the clause is the decision to be made here, which will be formulated as a combinatorial
optimization problem via mixed-integer programming. Figure 1 demonstrates how bsnsing handles these steps.

3.2 Feature Binarization

We very brieﬂy outline the default feature binarization approach implemented in bsnsing. It is not the emphasis of
this paper and it is extensible by other developers. For a numeric variable, samples are sorted based on its value. If
the two classes are perfectly separable, i.e., the minimum value of one class is greater than the maximum value of the
other class, the split point is returned and both child nodes are marked as a leaf. Otherwise, the sorted list of samples
is scanned twice, in the sequential and reverse order, to ﬁnd “greater-than” and “less-than” type of split conditions,
respectively. In this manner, the algorithm implements the subsumption principle as described in Rijnbeek and Kors
(2010) to ensure that only potentially valuable split conditions are generated and ones that are not in the efﬁcient frontier
are left out. Other approaches for binarizing numeric features, such as using the empirical quantiles as cut points, see,
e.g., Malioutov and Varshney (2013), are also worth implementing in future work. For a categorical variable of L unique
levels, the binarization process creates L binary dummy variables when L is below a threshold (default 30). When
L is greater than the threshold, value grouping is applied before creating dummy variables. Finally, binary features
generated by other decision tree packages can be imported to bsnsing for optimal selection as well.

3.3 Mixed-Integer Program to Maximize Gini Reduction

The Gini index, developed by Italian statistician Corrado Gini in 1912, is a measure of variability for categorical data.
It can be used to measure the impurity of a decision tree node. In general, the Gini index for a set of objects (e.g.,
cases contained in a tree node) that come from J possible classes is given by 1 − (cid:80)J
j , where pj is the relative
frequency of the target class j, j ∈ {1, . . . , J}, in the node. A split that produces a large reduction in Gini (i.e., ∆Gini)
is preferred. The ∆Gini splitting criterion was ﬁrst proposed in Breiman et al. (1984). It is available in many decision
tree codes such as rpart and tree packages in R and the decision tree method in SAS Enterprise Miner. We will
formulate the ∆Gini criterion as a mixed-integer linear program of the splitting decision. To our knowledge, no prior
work has done so.

j=1 p2

Let P and N denote the numbers of positive and negative cases, respectively, at the current (parent) node. The Gini
index of this node, denoted by G(parent), is

G(parent) = 1 −

(cid:18) P

(cid:19)2

P + N

−

(cid:18) N

(cid:19)2

P + N

=

2P · N
(P + N )2

(1)

Suppose the node is split into two child nodes by a split rule, that is, cases that are ruled to be positive fall in the left
node and cases that are ruled to be negative fall in the right node. Let T P and F P denote the numbers of positive and
negative cases, respectively, that fall in the left node, and T N and F N denote the numbers of negative and positive
cases, respectively, that fall in the right node, then the Gini indexes of the left and right nodes are

G(left) =

2 · T P · F P
(T P + F P )2 and G(right) =

2 · T N · F N
(T N + F N )2

The ∆Gini of the split is deﬁned to be the Gini index of the parent node minus the weighted sum of the Gini indexes of
the child nodes, whereas the weights are the squared proportions of cases that fall in each child node,

∆G = G(parent) −

(cid:18) T P + F P
P + N

(cid:19)2

· G(left) −

(cid:18) T N + F N
P + N

(cid:19)2

· G(right)

(2)

Note that the squared proportions are used here, in stead of the proportions as originally proposed in Breiman et al.
(1984), for the ease of mathematical modeling. Note that the value of ∆G is not affected by the classiﬁcation labels
assigned to the child nodes. For example, if we were to classify the left node as negative and the right node as positive, it
would only cause a swap between T P and F N and a swap between F P and T N in the above equations, which would
not affect the value of ∆G. Equation (2) can be simpliﬁed to ∆G = (2P · N − 2(T P · F P + T N · F N ))/(P + N )2.
Since P and N are known values for the parent node irrespective of the split, maximizing ∆G is equivalent to
minimizing T P · F P + T N · F N , which is in turn equivalent to minimizing

P · F P + N · F N − 2 · F N · F P

(3)

6

Y. Liu

Table 2: Notation deﬁnition for the mathematical program.

Symbol Meaning
I
P, N
K
Bik
yi
wk
zi
θij

= {1, . . . , n}, index set of cases
index sets of Positive and Negative cases, respectively
= {1, . . . , m}, index set of questions
= 1 if case i answers Yes for question k, 0 otherwise
= 1 if case i is positive; 0 otherwise
= 1 if question k is selected into the split rule; 0 otherwise
= 1 if case i is classiﬁed as Positive; 0 Negative
a binary variable for each pair of cases i and j

Here, F P and F N are variables whose values depend on the split rule.

Mathematical symbols used in the MIP formulation is deﬁned in Table 2. Let the binary variable wk indicate whether
question k is selected (=1) or not (=0), then the product Bikwk equals 1 if both question k is selected and case i answers
yes for the question. When the selected questions form an OR-clause classiﬁcation rule, the case i is classiﬁed as
Positive (zi = 1) if there exists a k ∈ K such that Bikwk = 1. Conversely, case i is classiﬁed as Negative (zi = 0) if
Bikwk = 0 for all k ∈ K. This classiﬁcation rule is expressed by the following linear constraints.

Bikwk ≤ zi,

Bikwk ≥ zi,

(cid:88)

k∈K

∀i ∈ I, ∀k ∈ K

∀i ∈ I

(4)

(5)

We only need to enforce 0 ≤ zi ≤ 1, for i ∈ I in the formulation, since the integrality of variable zi is implied by the
constraints (4) and (5), the fact that each wk is binary and the sense (i.e., minimization) of the optimization objective.

Using variable zi, we can express the terms in (3) as follows.

P · F P = P ·

(cid:88)

(1 − yj)zj =

(cid:88)

(cid:88)

zj

j∈I
(cid:88)

N · F N = N ·

i∈P
(cid:88)

j∈N
(cid:88)

yi(1 − zi) =

(1 − zi)

i∈I

(cid:32)

(cid:88)

(cid:33) 


yi(1 − zi)

F N · F P =

j∈N

i∈P


(cid:88)

(1 − yj)zj



j∈I

yi(1 − zi)(1 − yj)zj

i∈I
(cid:88)

=

=

i,j∈I
(cid:88)

(cid:88)

i∈P

j∈N

(1 − zi)zj

Minimizing (3) would reward a greater value of F N · F P , therefore we can replace each term (1 − zi)zj in the last
equation by a free variable θij, deﬁned for i ∈ P and j ∈ N , and impose the following constraints,

θij ≤ 1 − zi,
θij ≤ zj,

∀i ∈ P, ∀j ∈ N
∀i ∈ P, ∀j ∈ N

(6)
(7)

Putting everything together, the problem that optimizes Gini reduction is formulated as a mixed-integer program as
follows. Let us call it OPT-G.

(OPT-G):

Minimize

s. t.

(cid:88)

(cid:88)

(1 − zi + zj − 2θij)

j∈N

i∈P
(4), (5), (6) and (7)
wk ∈ {0, 1},
0 ≤ zi ≤ 1,
θij free

7

(8)

(9)
(10)
(11)

∀k ∈ K
∀i ∈ I
∀i ∈ P, j ∈ N

Y. Liu

⊕⊕⊕
(cid:9)(cid:9)(cid:9)

⊕⊕⊕
(cid:9)(cid:9)(cid:9)

+

-

+

-

⊕

⊕⊕
(cid:9)(cid:9)(cid:9)

⊕⊕(cid:9)

(cid:9)(cid:9)⊕

Figure 2: The two ways of splitting the parent node gives the same number of misclassiﬁcations, hence they are
indistinguishable under OPT-E. In contrast, OPT-G favors the one leading to more balanced child nodes.

For each pair of cases i ∈ P and j ∈ N , θij equals 1 only when both cases are classiﬁed incorrectly. When this happens,
the corresponding objective term (1 − zi + zj − 2θij) will be the same value (zero) as when both cases are classiﬁed
correctly. This corroborates the theory that the ∆G split criterion is agnostic of the polarity of the labels assigned to the
child nodes. Equation (10) can be further reduced to zi ≤ 1 for i ∈ P and zi ≥ 0 for i ∈ N . Most MIP models in the
OCT literature attempted to minimize misclassiﬁcation (plus other terms to suppress overﬁtting). The OPT-G model
can be reduced to minimizing misclassiﬁcation simply by removing the objective terms and constraints that involve θ.
Speciﬁcally, the following model (OPT-E) explicitly minimizes the number of misclassiﬁed cases from the split.

(OPT-E):

Minimize

s. t.

(cid:88)

(cid:88)

(1 − zi + zj)

j∈N

i∈P
(4), (5), (9) and (10)

(12)

The Gini-based splitting criterion is designed to purify the class composition in the child nodes. Compared to error-
based criterion, it is conducive to more balanced child nodes. This is demonstrated in the simple example in Figure 2.
Therefore, in this paper we focus on analyzing the OPT-G model and its solution strategy. Splits based on the OPT-E
model is available in the bsnsing package through the option opt.model = ‘error’. Users can choose Gurobi,
CPLEX and lpSolve to solve the MIP model by setting the opt.solver option in bsnsing, provided that the chosen
solver and its R API package are installed.

In the tree-building process, the instance size of the OPT-G model is largest at the root node, and decreases exponentially
as the process moves down the tree branches. For large training data sets, the root node model can become computation-
ally prohibitive for even the commercial solvers such as CPLEX and Gurobi. In the next section, we propose an implicit
enumeration approach for solving OPT-G that scales better than commercial solver codes in practical settings.

3.4

Implicit enumeration (ENUM) algorithm for solving OPT-G

Each candidate solution to OPT-G can be denoted by its corresponding index set S of the selected questions, i.e.,
S := {k ∈ K | wk = 1}. At a solution S, let us denote the false positive and false negative counts at the solution by
F PS and F NS , respectively, and deﬁne ν(S) to be the objective value at the solution, that is,

ν(S) := P · F PS + N · F NS − 2 · F PS · F NS

(13)

We know that starting at any S, selecting more questions into the solution (i.e., enlarging S) would only encourage
extra cases to fall in the left node (i.e., answer “yes” to the OR-clause), and hence, would cause F P to either stay at the
same value or increase, and would cause F N to stay at the same value or decrease. In other words, for any S+ ⊃ S, we
have F PS+ ≥ F PS and F NS+ ≤ F NS . Leveraging this property, we can derive a lower bound on the objective value
for any possible solution that “branches out” from a given solution S.
Proposition 1. For any superset of S, denoted by S+, the following inequalities hold.

ν(S+) ≥




ν(S),
P · F PS ,
N · (P − F NS ),

0,

if F NS < P/2 and F PS > N/2
if F NS < P/2 and F PS ≤ N/2
if F NS ≥ P/2 and F PS > N/2
if F NS ≥ P/2 and F PS ≤ N/2

8

Y. Liu

Proof. In the case F NS < P/2 and F PS > N/2, we have N − 2 · F PS+ ≤ N − 2 · F PS < 0 and P − 2 · F NS+ ≥
P − 2 · F NS > 0, therefore,

ν(S+) = P · F PS+ + (N − 2 · F PS+) · F NS+
≥ P · F PS+ + (N − 2 · F PS+) · F NS
= (P − 2 · F NS+) · F PS+ + N · F NS
≥ (P − 2 · F NS ) · F PS + N · F NS
= ν(S)

In the case F NS < P/2 and F PS ≤ N/2, we have P − 2 · F NS+ ≥ P − 2 · F NS > 0 and N − 2 · F PS ≥ 0,
therefore,

ν(S+) = N · F NS+ + (P − 2 · F NS+) · F PS+
≥ N · F NS+ + (P − 2 · F NS+) · F PS
= (N − 2 · F PS ) · F NS+ + P · F PS
≥ P · F PS

In the case F NS ≥ P/2 and F PS > N/2, we have N − 2 · F PS+ ≤ N − 2 · F PS < 0 and P − 2 · F NS < 0,
therefore,

ν(S+) = P · F PS+ + (N − 2 · F PS+) · F NS+
≥ P · F PS+ + (N − 2 · F PS+) · F NS
= (P − 2 · F NS ) · F PS+ + N · F NS
≥ (P − 2 · F NS+) · N + N · F NS
= (P − F NS ) · N

In the last case where F NS ≥ P/2 and F PS ≤ N/2, there is insufﬁcient information to derive a nontrivial lower
bound for ν(S+) while ν(S+) ≥ 0 always holds.

Let us denote the lower bound for ν(S+) by τ (S) to emphasize its sole dependence on the current solution S, and
assign its value according to Proposition 1. In the search for the optimal solution, whenever τ (S) is greater than the
best objective value found so far, any solution that would result from enlarging S can be eliminated. The algorithm is
outlined as follows. We start with evaluating all single-variable split rules, i.e., Sk = {k}, for k ∈ K, by calculating
their ν(Sk) and τ (Sk). These are the “root nodes” in the search tree 1. Let νbest be the smallest objective values
encountered so far. We can eliminate those nodes k having τ (Sk) > νbest, and we can terminate a node (i.e., making it
a leaf in the search tree) when τ (Sk) = ν(Sk). For each remaining root node k, we evaluate all two-variable split rules
branched from it in such a way that the added variable’s index is greater than k (to avoid redundant evaluations), and
update the νbest and eliminate unpromising branches on the ﬂy. For instance, if τ ({1, 2}) > νbest, then the candidate
solution {1, 2} ∪ K for each K ⊂ 2{3,...,m} can be eliminated. The search proceeds until all possibilities are examined,
at which point the νbest is the optimal solution to OPT-G. This algorithm is implemented in the bslearn function in the
R source code under the “enum” solver option, and we call it the ENUM algorithm in the rest of this paper.

The ENUM algorithm, if carried out to completion, can guarantee to return a solution having the smallest objective
value. In essence, the sequential evaluation of candidate solutions reduces the search space from K × P × N to 2K.
Since the cost of evaluating a candidate solution ramps up slowly with the size of I (i.e., P ∪ N ) thanks to efﬁcient
vector operations, the ENUM can ﬁnd the optimal solution faster than the typical branch-and-bound method of MIP
solvers, when I is large and K is relatively small.

Compared to using an MIP solver, ENUM boasts the following advantages: (1) it scales betters in terms of memory
cost because no branch-and-bound tree is maintained; (2) the search process can be parallelized (though it is a bit
tricky to implement in R since R is intrinsically single-threaded); (3) the search is breadth ﬁrst, meaning that split rules
having fewer clauses are evaluated before rules having more clauses get evaluated. Therefore, if any time the search is
terminated prematurely (e.g., due to time limit), a simplest possible best found rule can still be returned; in addition, if
the optimal solution is not unique, one having the fewest clauses will be returned.

1The phrase “search tree” is an analogy - in implementation a queue is used for storing yet-to-explore solutions.

9

Y. Liu

1

3

4

2

4

3

4

2

4

3

4

4

4

3

4

Figure 3: Enumeration of all possible solutions in a solution space of four candidate rules indexed by {1, 2, 3, 4}. Each
node, along with the top-down path leading to it, represents a unique subset of the solution space. Candidate solutions
are evaluated in the breadth-ﬁrst order. Proposition 1 enables opportunities to prune the search tree branches.

3.5 Complexity regulation constraints

When OPT-G is used to split the nodes, the classiﬁcation tree can be grown until every node is pure (i.e., containing
observations of the same class). Such a tree, called a maximal tree, does not generalize well on new data. Additional
constraints can be added to OPT-G to regulate the complexity of the tree to curb overﬁtting. In the bsnsing package,
we leverage two types of constraints as follows.

(cid:88)

k∈K
(cid:88)

wk ≤ MaxRules

zi ≥ MinNodeSize

i∈I
(cid:88)

i∈I

(1 − zi) ≥ MinNodeSize

(14)

(15)

(16)

The right-hand sides of these inequalities are control parameters. Constraint (14) limits the number of questions to enter
a split rule. This constraint is quite necessary in practice especially when the tree is to be used for prediction, because
devoid of this constraint each split optimization step would amount to a maximal overﬁtting of the data available in
the present node - ﬁnding a single composite split rule to maximally reduce the Gini. In addition, this constraint also
directly reduces the solution space thus its presence expedites the ENUM search. Speciﬁcally, the number of objective
function evaluations in the worst case (assuming no pruning and early termination opportunities exist in the search
(cid:1) as compared to 2m when the constraint was not present. For instance, for
process) would reduce to (cid:80)MaxRules
a pool of m = 98 candidate questions, constraint (14) with MaxRules = 3 would reduce the worst-case objective
(cid:1) = 156947 from the theoretical worst worst case of 298 = 3.17 × 1029. A
(cid:1) + (cid:0)98
function evaluations to (cid:0)98
demonstration is presented in Section 4.

(cid:1) + (cid:0)98

(cid:0)m
i

i=1

2

3

1

Constraints (15) and (16) require that each child node from a split must contain at least MinNodeSize (a positive
number) observations. They are effective at limiting the tree depth as well as generating well-populated leaf nodes. In
the bsnsing package, the default value for MaxRules is 2, and MinNodeSize is by default set equal to the square root
of n, the number of training examples.

In certain use cases, it is customary to expect the child nodes of a split to bear different majority classes, i.e., to require
T P/(T P + F P ) ≥ 0.5 and F N/(F N + T N ) ≤ 0.5. This requirement can be translated to the following linear
constraint for the OPT-G model.

(cid:88)

i∈P

zi −

(cid:88)

j∈N

zj ≥ max{0, 2 · P − n}

(17)

To include this constraint in the ENUM algorithm, we can simply discard any candidate solution that violates it in
the search process. We comment that this constraint is mainly for the convenience of tree interpretation, and would
lead to an over-regulation (i.e., creating unnecessary bias) to the tree model. Speciﬁcally, its inclusion in the model
tends to produce simple and shallow trees that lack ﬁdelity in discriminating new cases. Therefore, we choose not

10

Y. Liu

to enable it by default in the bsnsing package. To enable the constraint, the user can explicitly set the parameter
no.same.gender.children to True.

Other hyperparameters used in the bsnsing function include (1) the bin size (bin.size), which speciﬁes the minimum
number of observations that must satisfy a candidate binary question for the question to enter the pool; (2) the stop
probability (stop.prob), a node purity threshold in terms of the proportion of the majority class in the node which,
if exceeded, the node will not be further split; (3) the maximum number of segments the range of a numeric variable
(nseg.numeric) is divided into by inequalities of the same direction. These parameters do not directly affect the split
rule optimization, but they affect the overall efﬁciency of the tree building process and the ﬁnal performance of the
tree. Apart from generating candidate binary questions internally, the bsnsing function is also able to import split
questions generated by other decision tree packages (currently including C50, tree, party and rpart packages) into its
own pool for optimal selection. This option is enabled by the parameter import.external. For a complete list of
control parameters, users could consult the help document by typing “?bscontrol” in R.

4 Evaluation

The models and algorithms developed in this paper are implemented in the open-source R package named bsnsing.
The source code is hosted at github.com/profyliu/bsnsing.

In this section, we demonstrate the computational efﬁciency of solving OPT-G using different algorithms, compare
the performance of bsnsing against several other decision tree packages available in R, and showcase the basic usage
of the bsnsing library via some code samples. The experiments were performed in RStudio (R version 3.6.2) on a
MacBook Pro with Intel Core i9 (8 cores) processor and 16GB RAM.

4.1 Computational efﬁciency of solving OPT-G

We perform two sets of experiments to demonstrate the superiority of the ENUM algorithm over the Gurobi solver for
solving OPT-G. In the ﬁrst set of experiments, we generate OPT-G instances of different sizes based on the seismic data
set from the UCI machine learning repository (Dua and Graff 2017) and show how each method scales as data size
increases; in the second set of experiments, we contrast the solutions by the two methods on more classiﬁcation data
sets adopted from the literature, to further solidify our conclusion.

4.1.1 Experiments on the seismic data set.

The seismic data set has 1690 observations and 18 input variables, among which 4 are categorical variables and 14
are numeric variables. The output is a categorical variable with two levels, so it is a binary classiﬁcation problem. We
ﬁrst binarize all inputs to create a binary matrix B (using the binarize function in the bsnsing package), essentially
substituting one or multiple binary features for each of the original input variables. The matrix B has 1690 rows and 100
columns. We experiment two scenario factors that are critical to the computational efﬁciency, the number of training
cases n and the maximum number of rules, i.e., MaxRules, allowed in the solution. We randomly sample n rows from
B and join them with the corresponding response variables to form sets of training data with varying sizes. For each
combination of max.rules in {1, 2, 3, 4} and n in {200, 400, . . . , 1600}, we solve the OPT-G model using both the
Gurobi solver and the ENUM algorithm. For example, suppose the selected rows of matrix B is stored in matrix bx and
the binary target vector is y, the following R code was used to learn the optimal split rule with MaxRules = 3 using the
Gurobi solver:

res <- bslearn(bx, y, bscontrol(opt.solver = ‘gurobi’, solver.timelimit = 7200, max.rules
= 3, node.size = 1))

Note that the node.size option was set to 1 to relax the constraints (15) and (16) that were irrelevant to this experiment.
For the ENUM algorithm, the opt.solver option was set to ‘enum_c’2 .

The results are summarized in Table 3. As expected, in each case both methods were able to ﬁnd the same optimal
objective value, reported in the column Objval. For the same instance, the Objval was non-increasing with the increase
in max.rules, which also matches our expectation.

The next three columns of Table 3 list the computing time in seconds. The Gurobi solver automatically exploited
multiple CPU cores available in the computer (which had 8 physical cores), so we report both the CPU time (actual
computing resource usage) in column Gurobi and the elapsed time (wall time as experienced by the user) in column

2Setting opt.solver=‘enum_c’ invokes the compiled code (written in C) implementing the ENUM algorithm, as opposed to

using the plain R implementation which can be invoked by setting opt.solver = ‘enum’.

11

Y. Liu

Gurobi.E. The ENUM algorithm used only one CPU core, thus only the elapsed time is reported. Clearly, ENUM is the
incontestable winner, running at least two orders of magnitude faster than Gurobi in elapsed time.
(cid:1). However,
For the ENUM algorithm, the number of objective function evaluations in the worst case is (cid:80)max.rules
the actual number of evaluations was signiﬁcantly fewer than the theoretical worst case. The actual numbers of objective
function evaluations, as well as their percentage of the theoretical worst case, are listed in columns Obj.Evals and Pct of
All Feasible, respectively. For example, when n = 200 and max.rule = 1, ENUM evaluated 100 candidate solutions
to ﬁnd the optimal one, and this number is 100% of all candidate solutions in the search space. We can see that the
percentage dropped signiﬁcantly (hence substantial savings in computing accrued) with the increase in max.rules.
The savings are attributed to the bounding and early termination strategy of Proposition 1. Similar experiments on other
data sets and other solvers (i.e., CPLEX and lpSolve) would reveal the same insight, so we forgo repeated experiments
on those scenarios.

(cid:0)100
i

i=1

4.1.2 Experiments on selected DL8.5 datasets.

In validating the DL8.5 algorithm and demonstrating its superiority over the BinOCT algorithm (Verwer and Zhang
2019a), Aglin et al. (2020) employed 24 binary classiﬁcation data sets, all consisting of pure binary features. These
data sets can directly form instances of OPT-G without the need for feature binarization. We selected 13 from the 24
data sets by the criterion n ≤ 1000 and p ≤ 200, and compared Gurobi and ENUM solutions to OPT-G for the root
node split rule identiﬁcation on these data sets. The reason for the above selection criterion is that Gurobi cannot solve
the larger cases in a tolerable amount of time, i.e., two hours. The results are presented in Table 4. The problem size
is noted by n and m (for these cases, p = m). Sharp contrasts in solution time - up to a ratio of 106 - between the
two methods persisted throughout all cases. In addition, for the australian-credit and tic-tac-toe cases, Gurobi failed to
terminate within the 2-hour time limit, ending with a suboptimal solution in the latter case. In comparison, ENUM was
able to ﬁnd the optimal solutions consistently in less than 0.1 second. In cases where Gurobi succeeded, both methods
returned the same optimal rule. These experiments serve to solidify our conclusion that ENUM should be the solver of
choice when the bsnsing package is employed in practice.

4.2 Comparison with the DL8.5, OSDT and GOSDT algorithms

In this section, we compare bsnsing against three recently developed optimal decision tree (ODT) methods, namely,
DL8.5 (Aglin et al. 2020), OSDT (Optimal Sparse Decision Trees) (Hu et al. 2019) and GOSDT (Generalized and
scalable optimal sparse decision trees) (Lin et al. 2020a). The software programs were obtained through the Github links
provided in the respective papers. Given that the model assumptions, formulation and parameters are all different for
the different tools under comparison, we do not aim to provide a comprehensive evaluation, by, for instance, performing
problem-speciﬁc parameter tuning and model interpretation, or making inferences about which method is most suitable
for what kind of data and applications. Instead, we will exhibit our computational experience from a user’s perspective,
and let it convey the unique position of bsnsing among other recent ODT tools.

We experiment on the 24 data sets (i.e., binary classiﬁcation problems with binary features) used in the DL8.5 paper
(Aglin et al. 2020). The experiments were performed as follows. The comparison experiment on each data set was
repeated 20 times. In each repetition, using an arbitrarily sequenced seed for the random number generator (RNG), we
randomly split the data set into two parts, 70% for training and 30% for testing. For each method, we recorded the
classiﬁcation accuracy on the test set, as well as the time taken (in seconds) to train the respective models. For bsnsing,
we adopted the default parameters, speciﬁcally, opt.solver = ‘enum_c’, max.rules = 2, and set the node.size
to n1/4, where n is the training sample size; for DL8.5, we adopted the default parameters as recommended in the
package’s companion paper and in its online sample code, that is, “max_depth=3, min_sup=5”; for OSDT, we adopted
the default values for all optional parameters and set the required parameters to “lamb=0.005, prior_metric=‘curiosity”’,
by consulting the sample code provided in the author’s Github page; and for GOSDT, we set the required parameter
“regulerization” to 0.0013, a value found suitable in several experiments in the extended manuscript of the method, see
the appendices of Lin et al. (2020b). In addition, for OSDT and GOSDT we set the time limit to 5 minutes (via setting
the parameter “timelimit=300” in OSDT and “time_limit=300” in GOSDT), because we observed (which was also
acknowledged by OSDT’s developer) that the memory usage of the OSDT and GOSDT training process increases
linearly and quickly with execution time.

The comparison results are listed in Table 5. The columns n, p and MR characterize the data sets, where MR represents
the rate of the minority class. The best accuracy (averaged over 20 runs) for each data set is highlighted in boldface.
There is not a clear winner (or loser) in terms of prediction accuracy among the four methods - overall, all methods

3The regularization parameter in GOSDT is the multiplier on the number-of-leaves term in the two-term objective function to be

minimized, whereas the other term is the training accuracy (with multiplier 1).

12

Y. Liu

Table 3: Computational costs and scalability of ENUM.

400

200

600

800

Obs. max.rules Objval
512
1
492
2
420
3
378
4
2324
1
2046
2
1974
3
1974
4
5044
1
4710
2
4610
3
4610
4
8494
1
8153
2
7933
3
7802
4
12410
1
11946
2
11652
3
11382
4
17027
1
15840
2
15457
3
15316
4
25354
1
23302
2
22794
3
22578
4
34664
1
32460
2
31887
3
31737
4

1600

1400

1200

1000

Gurobi Gurobi.E
2.8
2.9
3.8
3.7
6.8
14.9
27.6
14.8
19.7
38.6
59.2
50.5
43.9
109.2
191.8
108.2
34.2
188.2
233.7
327
433.8
316.3
409.8
543.3
151.4
442.6
639.5
895.9
293.4
2572.5
2804.5
4212.2

5.6
14.5
20.4
20
14.5
97.7
188.7
93.3
48.5
261.1
421.3
359.4
117.6
796.7
1447.3
787.1
58.5
1359.8
1727.2
2498.2
842.7
2370.9
3094.5
4143.4
625.7
3299.4
4856
6905.6
558.9
19969.1
21834.1
33050.8

ENUM Obj. Evals
100
2959
36998
249951
100
3163
46270
406135
100
3332
49059
445965
100
3308
51389
463565
100
3241
50300
442363
100
3282
54261
535880
100
3373
53873
540219
100
3282
54642
551884

0
0
0.1
0.5
0
0
0.2
2
0
0
0.3
3.8
0
0.1
0.6
5.4
0
0.1
0.7
6.6
0
0.1
0.9
9
0
0.1
1
10.1
0.1
0.1
1.2
11.2

Pct of All Feasible
100%
58.6%
22.2%
6.1%
100%
62.6%
27.7%
9.9%
100%
66%
29.4%
10.9%
100%
65.5%
30.8%
11.3%
100%
64.2%
30.2%
10.8%
100%
65%
32.5%
13.1%
100%
66.8%
32.3%
13.2%
100%
65%
32.8%
13.5%

seem comparably capable. However, the differences in training time is signiﬁcant. First, we can observe that OSDT and
GOSDT are less competitive in training time. Another factor that discounts OSDT algorithm’s actual performance is
that, the OSDT program internally uses scikit-learn’s decision tree classiﬁer (which implements the CART algorithm)
as its baseline predictor, and will return the CART model if the OSDT algorithm cannot do better. In most cases, the
OSDT program indeed returned the CART model for prediction (the CART column in Table 5 notes the proportion
of runs in which the CART model was returned by the OSDT method), indicating that the OSDT algorithm did not
outperform the CART baseline in these cases.

The bsnsing package also compares favorably to DL8.5 in training speed, especially for large instances. Let us look
at the letter data set which consists of 14000 training samples for example: it took bsnsing on average 39.5 seconds
to train a model that worked better than the DL8.5 model, which took 304.4 seconds to train on average. It has been
shown in prior work, i.e., (Aglin et al. 2020), that DL8.5 runs orders of magnitude faster than several other optimal
decision tree methods, including the original DL8 algorithm (Nijssen and Fromont 2007), a constrained programming
(CP)-based method (Verhaeghe et al. 2020), and an MIP-based method BinOCT (Verwer and Zhang 2019a). Hence, we
can infer that bsnsing must also outrun those other methods by a substantial margin. Overall, it is reasonable to claim
that bsnsing, bearing comparable prediction accuracy, stands out in training speed among decision tree methods that
involve solving mathematical optimization problems in the training process.

In most decision tree induction methods, depth (i.e., level distance between the root node and the deepest node in a tree)
is a hyperparameter for adjusting the classiﬁer’s complexity with regard to the bias-variance tradeoff. Heuristic methods
such as CART typically realize depth control via tree pruning, while most ODT methods can explicitly constrain the
maximum depth in the optimization models. However, the bsnsing method does not endogeneously handle a constraint
on the maximum depth. The “Dp” column in Table 5 lists the mode (most frequent value out of the 20 runs) of the
depth of the bsnsing trees. We can see that, compared to trees built by DL8.5 which have a default “max_depth” of 3,
the trees built by bsnsing generally reach deeper, though not all branches extend to the same depth.

13

Y. Liu

Table 4: Comparison between Gurobi and ENUM for root node split on DL85 datasets with max.rules = 2.

anneal

n

812

m

93

audiology

216

148

australian-credit

653

125

breast-wisconsin

683

120

diabetes

768

112

heart-cleveland

hepatitis

lymph

primary-tumor

soybean

tic-tac-toe

vote

zoo-1

296

137

148

336

630

958

435

101

95

68

68

31

50

27

48

36

Method
ENUM
Gurobi
ENUM
Gurobi
ENUM
Gurobi
ENUM
Gurobi
ENUM
Gurobi
ENUM
Gurobi
ENUM
Gurobi
ENUM
Gurobi
ENUM
Gurobi
ENUM
Gurobi
ENUM
Gurobi
ENUM
Gurobi
ENUM
Gurobi

CPU
0.045
9005.889
0.023
63.565
0.057
56229.196
0.051
5276.419
0.063
55471.42
0.023
3190.064
0.006
109.495
0.008
214.038
0.006
530.928
0.016
2304.774
0.012
14160.407
0.011
1027.852
0.002
0.277

Elapsed
0.083
1181.632
0.027
10.994
0.059
7204.716
0.053
691.689
0.063
7205.7
0.025
418.003
0.007
15.128
0.008
28.534
0.006
70.518
0.017
306.424
0.012
7202.697
0.013
346.244
0.002
0.283

Rule

885
885

Objval
47750 V60 | V67
47750 V60 | V67
V2 | V32
V2 | V32
24456 V34 | V76
24456 V38 | V76
9039 V21 | V77
9039 V21 | V77
53312 V20 | V76
53312 V20 | V76
7460 V91 | V96
7460 V91 | V96
876 V37 | V51
876 V37 | V51
1655 V39 | V56
1655 V39 | V56
7728 V21 | V29
7728 V21 | V29
19964 V24 | V36
19964 V24 | V36
95336
105464
3547
3547
0
0

V15
V28
V12
V12
V8
V8

nEval
2204
0
5768
0
4493
0
3427
0
3180
0
2483
0
1183
0
1562
0
284
0
997
0
378
0
631
0
36
0

Table 5: Computational comparison of four ODT packages.

anneal
audiology
australian-credit
breast-wisconsin
diabetes
german-credit
heart-cleveland
hepatitis
hypothyroid
ionosphere
kr-vs-kp
letter
lymph
mushroom
pendigits
primary-tumor
segment
soybean
splice-1
tic-tac-toe
vehicle
vote
yeast
zoo-1

n

812
216
653
683
768
1000
296
137
3247
351
3196
20000
148
8124
7494
336
2310
630
3190
958
846
435
1484
101

p

93
148
125
120
112
112
95
68
88
445
73
224
68
119
216
31
235
50
287
27
252
48
89
36

MR

0.230
0.264
0.453
0.350
0.349
0.300
0.459
0.190
0.085
0.359
0.478
0.041
0.453
0.482
0.104
0.244
0.143
0.146
0.481
0.347
0.258
0.386
0.312
0.406

bsnsing

Accu CPU Dp
7
0.844
3
0.925
6
0.823
4
0.953
7
0.717
7
0.676
5
0.752
4
0.790
6
0.975
4
0.881
0.984
7
0.990
8
0.810
4
0.999
4
0.994
5
6
0.778
0.996
3
6
0.935
0.946
7
7
0.875
5
0.952
4
0.940
8
0.698
1
0.995

2.6
0.4
2.3
0.8
3.8
5.0
1.2
0.4
2.3
4.2
2.0
39.5
0.3
2.7
10.9
0.7
1.6
0.7
18.2
1.0
3.1
0.3
7.1
0.0

14

DL8.5
CPU
1.1
0.2
4.6
3.1
5.4
4.2
1.9
0.5
2.2
235.0
1.3
304.4
0.2
4.4
94.4
0.1
12.8
0.2
51.6
0.1
33.6
0.2
3.1
0.0

Accu
0.847
0.925
0.853
0.955
0.737
0.728
0.766
0.802
0.979
0.869
0.936
0.981
0.801
0.999
0.991
0.843
0.996
0.941
0.926
0.733
0.956
0.943
0.692
0.995

Accu
0.839
0.938
0.858
0.951
0.750
0.723
0.763
0.775
0.979
0.888
0.969
0.959
0.798
0.994
0.989
0.822
0.995
0.938
0.946
0.901
0.946
0.956
0.701
1.000

OSDT

CPU CART
0.8
305.0
0.3
297.9
0.7
304.1
0.3
304.6
0.8
305.7
0.6
305.2
0.9
303.7
1.0
306.0
0.4
304.7
0.7
302.8
1.0
303.9
1.0
3033.1
0.9
305.2
0.0
305.1
0.0
303.4
0.2
304.9
0.9
4.6
0.3
306.2
1.0
303.0
1.0
304.1
0.8
303.5
0.1
304.2
0.3
305.8
1.0
0.2

GOSDT

Accu
0.846
0.934
0.857
0.943
0.749
0.708
0.746
0.829
0.979
0.912
0.853
0.959
0.761
0.943
0.970
0.751
0.996
0.862
0.835
0.757
0.879
0.950
0.701
0.995

CPU
304.4
306.0
311.4
307.8
314.5
319.9
306.9
303.1
307.0
519.6
307.9
631.6
303.3
314.4
381.2
169.6
311.0
304.6
308.4
305.3
354.0
304.7
308.9
0.0

Y. Liu

Table 6: Performance comparison under depth constraints.

anneal
audiology
australian-credit
breast-wisconsin
diabetes
german-credit
heart-cleveland
hepatitis
hypothyroid
ionosphere
kr-vs-kp
letter
lymph
mushroom
pendigits
primary-tumor
segment
soybean
splice-1
tic-tac-toe
vehicle
vote
yeast
zoo-1
Average

bs(2)
0.778
0.929
0.854
0.933
0.716
0.696
0.736
0.798
0.963
0.912
0.772
0.959
0.743
0.951
0.967
0.765
0.991
0.862
0.824
0.665
0.865
0.948
0.687
0.995
0.846

depth = 1

bs(1) DL8.5 OSDT
0.818
0.818
0.778
0.856
0.856
0.856
0.866
0.866
0.866
0.918
0.923
0.919
0.751
0.751
0.725
0.703
0.703
0.699
0.737
0.737
0.737
0.844
0.840
0.802
0.963
0.963
0.963
0.820
0.820
0.758
0.678
0.678
0.684
0.959
0.959
0.959
0.772
0.772
0.752
0.887
0.887
0.887
0.931
0.931
0.895
0.775
0.769
0.765
0.981
0.981
0.926
0.862
0.862
0.862
0.816
0.816
0.816
0.703
0.703
0.669
0.755
0.755
0.736
0.957
0.957
0.957
0.702
0.702
0.687
1.000
0.995
0.995
0.835
0.834
0.822

bs(2)
0.786
0.925
0.854
0.940
0.754
0.709
0.753
0.812
0.973
0.894
0.934
0.959
0.780
0.983
0.988
0.806
0.996
0.854
0.875
0.693
0.913
0.948
0.705
0.995
0.868

depth = 2

bs(1) DL8.5 OSDT
0.826
0.824
0.778
0.934
0.934
0.924
0.866
0.862
0.851
0.960
0.960
0.921
0.762
0.754
0.734
0.717
0.717
0.700
0.729
0.735
0.735
0.831
0.817
0.814
0.979
0.979
0.963
0.904
0.901
0.837
0.868
0.868
0.868
0.969
0.968
0.959
0.766
0.770
0.777
0.969
0.969
0.915
0.978
0.978
0.978
0.806
0.803
0.801
0.990
0.995
0.995
0.913
0.906
0.862
0.831
0.827
0.835
0.678
0.673
0.683
0.904
0.904
0.895
0.957
0.956
0.950
0.696
0.690
0.687
0.995
0.995
0.995
0.867
0.866
0.854

bs(2)
0.806
0.925
0.849
0.956
0.747
0.711
0.774
0.787
0.977
0.875
0.949
0.982
0.811
0.999
0.991
0.803
0.996
0.913
0.943
0.744
0.944
0.944
0.694
0.995
0.880

depth = 3

bs(1) DL8.5 OSDT
0.847
0.836
0.777
0.934
0.925
0.923
0.853
0.849
0.852
0.955
0.955
0.951
0.760
0.755
0.737
0.728
0.725
0.703
0.811
0.798
0.766
0.802
0.800
0.794
0.979
0.979
0.969
0.891
0.869
0.851
0.903
0.936
0.923
0.959
0.981
0.958
0.784
0.801
0.803
0.994
0.999
0.969
0.991
0.987
0.987
0.843
0.836
0.797
0.995
0.996
0.995
0.941
0.920
0.890
0.908
0.926
0.909
0.745
0.735
0.733
0.956
0.907
0.905
0.948
0.943
0.944
0.702
0.692
0.694
0.995
0.995
0.995
0.883
0.879
0.871

Some users might take depth as a proxy for interpretability of a decision tree - shallower trees, or trees with fewer
leaves, are deemed more interpretable than deeper trees. To facilitate performance comparison with depth-constrained
ODT trees, we can naively prune a bsnsing tree so as to keep the number of leaf nodes below that of a binary tree of a
given depth. For instance, a tree of depth 1, 2 and 3 would have at most 2, 4 and 8 leaf nodes, respectively. Using this
method, we repeat the above experiments (over the 24 binary classiﬁcation data sets, 20 runs for each) under different
maximum depth values. The average out-of-sample accuracies are reported in Table 6, with the best value in each depth
group highlighted in boldface. Two max.rules settings are tested for bsnsing: the default setting with max.rules=2,
reported in column bs(2), and the max.rules=1 setting, reported in column bs(1). Since the GOSDT package does not
have a parameter to limit the maximum depth or the number leaves, it is not part of the experimentation. Also, all the
OSDT runs with depth = 2 and 3 have hit the 5-min time limit4.

We can see that the pruned bsnsing trees remain quite competitive, in many cases outperforming the DL8.5 and
OSDT trees of the same depth. The multivariate splits (with max.rules=2) clearly give bsnsing an advantage in
these comparisons. Under max.rules=1, the pruned bsnsing trees become least accurate in most cases. Though
the max.rules=1 setting along with the naive pruning is not recommended for bsnsing’s practical use, comparing
bs(1) with DL8.5 and OSDT does highlight the beneﬁts of holistic optimization in tree induction, as argued in several
ODT papers. Another interesting, yet expected, observation is that the constraint on depth, no matter how it is realized
in different packages, does not affect the new-data prediction accuracy in any deterministic direction (increase or
descrease). An optimal (unconstrained or unpruned) tree may perform worse than a depth-constrained (or naively
pruned, as in the bsnsing case) tree in some cases. This observation enhances the understanding that in machine
learning algorithms, the notion of optimality only applies to the training problem, not to the inference problem. In
other words, there is no single algorithm or parameter setting that is best-performing in all cases. Moreover, comparing
the average (across all data sets) accuracies of the pruned bsnsing trees in column bs(2) and the average accuracy
(0.885) of the original bsnsing trees in Table 5, we can see that the naive pruning strategy generally hurts performance,
upholding the effectiveness of bsnsing’s algorithm design and the default parameter setting.

4.3 Comparison with other decision tree packages in R

In this section, we compare the out-of-the-box performance (i.e., using all default options and no hyperparameter tuning)
of the bsnsing package against several other decision tree packages, namely, C50, party, tree and rpart, that are

4Without the time limit, OSDT would in many cases exhaust the computer memory before terminating.

15

Y. Liu

Name
acute1
acute2
Adult
auto
bank
banknote
birthwt
BreastCancer
circ
climate
compas
connect
credit
diam
dystrophy
Echocard
Fertility
GlaucomaMVF
grid

n
120
120
32561
392
45211
1372
189
699
600
540
7214
208
690
600
209
61
100
170
600

p
6
6
13
7
16
4
9
9
2
18
52
60
15
2
9
11
9
66
2

MR
0.492
0.417
0.241
0.375
0.117
0.445
0.312
0.345
0.500
0.085
0.451
0.466
0.445
0.500
0.359
0.279
0.120
0.500
0.475

Table 7: Binary classiﬁcation data sets.
MR
0.265
0.459
0.478
0.206
0.092
0.286
0.359
0.352
0.486
0.500
0.343
0.480
0.482
0.500
0.500
0.500
0.029
0.063
0.246

Name
haberman
heart
heloc
hepatitis
HTRU2
ILPD
Ionos
magic04
mammo
Monks1
Monks2
Monks3
Mushroom
norm3p10
norm3p5
obli
ozone1
ozone8
parkins

n
306
303
10459
155
17898
583
351
19020
830
556
601
554
8124
600
600
600
2536
2534
195

p
3
13
23
19
8
10
34
10
5
6
6
6
21
10
5
2
72
72
22

Name
pima
Qsar
relax
retention
ring
seismic
sh88
Sonar
spambase
SPECT
spirals
statlog.a
thoraric
tictactoe
titanic
trans
votes
wdbc
wpbc

n
768
1055
182
10000
600
1690
600
208
4601
267
600
690
470
958
2201
748
435
569
198

Name
derm
iris
smiley
xor3
Glass
optdigits
Seeds

n
366
150
500
600
214
5620
210

p
34
4
2
3
9
64
7

J
6
3
4
4
6
10
3

Table 8: Multi-class classiﬁcation data sets.
n
210
132
1473
625
266
47
151

Name
imgsegm
Hayes
contra
balance
soybean.l
soybean.s
tae

J
7
3
3
3
15
4
3

p
19
4
9
4
35
35
5

Name
thyroid
wine
WineQuality
nursery

n
3772
178
4898
12960

p
21
13
11
8

MR
0.349
0.337
0.286
0.338
0.500
0.031
0.500
0.466
0.394
0.206
0.500
0.445
0.149
0.347
0.323
0.238
0.386
0.373
0.237

p
8
41
12
8
2
18
2
60
57
22
2
14
16
9
3
4
16
30
33

J
3
3
7
5

available on the Comprehensive R Archive Network (CRAN). Then, for those cases on which bsnsing performs poorly,
we will demonstrate some simple methods to improve the performance.

Data used in the benchmarking experiments include 57 data sets for binary classiﬁcation and 18 data sets for multi-class
classiﬁcation. Among these 75 data sets, one (iris) is from the datasets package, one (bank) is from a FICO-sponsored
explainable machine learning challenge (FICO 2018), two (compas and heloc) are from the ProPublica and Trusted-AI
GitHub repositories, two (GlaucomaMVF and dystrophy) are from the ipred package, six (BreastCancer, Glass, smiley,
spirals, xor and Sonar) are from / generated by the mlbench package, six (obli, grid, diam, circ, ring and sha88) are
synthetic data sets for 2D pattern recognition (see Figure 4), and the remaining 58 data sets are sourced from the
UCI Machine Learning Repository (Dua and Graff 2017). The names, the number of observations (n), the number
of independent variables (p), (for binary-class) the rate of the minority class (MR), and (for multi-class) the number
of target classes (J) are listed in Tables 7 and 8 for binary and multi-class classiﬁcation data sets, respectively. This
collection covers most of the commonly used data sets for methodology benchmarking in the classiﬁcation tree literature,
hence the data collection itself can be useful for future research. These data sets are accessible by name in the R
environment once the bsnsing library is loaded. The R scripts for conducting the subsequent experiments are not part
of the library but will be published in a different repository.

Figure 4: Synthetic data sets for pattern recognition. Input variables are the x and y coordinate. Some slanted and
nonlinear class boundaries are unamenable to the rectilinear split boundaries produced by tree models.

16

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllObliquelllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllGrid2x2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllDiamondllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllCircularllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllRingllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllShape88Y. Liu

4.3.1 Out-of-the-box performance comparison.

The experiments are conducted as follows. For each data set, we randomly split all observations into two parts, 70% for
training and 30% for testing. The training set is fed into different decision tree functions to build the respective tree
models, then the models are fed into the predict functions of the respective packages to make predictions on the test
set. Accuracy and the area under the ROC curve (AUC) values are calculated for each method based on the prediction
results. To calculate the Accuracy, class label predictions are requested from the predict functions, and to calculate the
AUC, score (or probability) predictions are requested from the predict functions. The whole process (i.e., random 70/30
split, training and testing) is repeated 20 times with documented random number generator (RNG) seeds for each data
set, and the corresponding Mean Accuracy and Mean AUC (for binary classiﬁcation) are reported in Tables 9 and 10.
The average computing time in seconds of the bsnsing method is also reported under the CPU column in the tables.
The computing times of other methods are consistently below 1 second for all test cases, thus they are omitted from the
report.

There are a few points to note: (1) in each run, all ﬁve methods were fed with the same training and test sets, so the
comparison was apple-to-apple; (2) all the original p independent variables contained in each data set were used - no
prior variable selection was done; (3) the tree-building functions from all the ﬁve packages were called in the simplest
form, i.e., only the “regression formula” and the training data set were supplied as arguments in the function calls, to
produce results that represent the out-of-the-box performance.

In Tables 9 and 10, the best Accuracy and AUC values in each data set were highlighted in red. In addition, the
bsnsing results that were above average among the ﬁve methods were printed in boldface. We can see that for binary
classiﬁcation tasks, the bsnsing package is in the leading position under the AUC category - it won 28 cases out of 57,
whereas tree, C50, ctree and rpart won 14, 10, 8 and 3 cases, respectively. The bsnsing package also scored above
average in 48 data sets, i.e., in 84% of all cases. Therefore, as far as the AUC performance is concerned, bsnsing
should be the package of choice for binary classiﬁcation tasks. In terms of the Accuracy metric, C50 was clearly
the leading one, winning 27 cases. For practitioners, we comment that AUC represents a model’s ability to correctly
rank order new data points according to their likelihood of belonging to the target class. The speciﬁc score threshold
for making classiﬁcations is usually application-dependent, e.g., depending on the comparative costs of making a FP
claim versus making a FN claim about a given new case. In contrast, the classiﬁcation accuracy measures the overall
proportion of false claims, i.e., by treating FP and FN claims with equal weight, at a chosen score threshold. Therefore,
we remark that AUC is a more well-rounded performance metric than classiﬁcation Accuracy for binary classiﬁers.

While the OPT-G model is only applicable for binary classiﬁcation, the bsnsing function can handle multi-class
classiﬁcation tasks as well. When more than two unique levels of the target variable are present in the training data set,
a binary classiﬁcation tree is built for each level (as the positive class) versus all the other levels (as the negative class).
In the prediction stage, a score (i.e., probability prediction) is produced from each tree and the target level having the
greatest score will be assigned as the class label for the new case. From Table 10 we can see that bsnsing’s multi-class
performance is second only to C50. A caveat is that the current way bsnsing handles multi-class classiﬁcation tasks is
more of ensemble learning rather than the decision tree learning, and the model’s interpretability is not preserved.

For bsnsing, the median time to train a binary classiﬁcation model is 0.6 seconds, tallied over the 1140 training
instances (i.e., 57 data sets, 20 instances each), and the median time to train a multi-class classiﬁcation model is 1.2
seconds, tallied over the 360 training instances. The computing time is much more tolerable than most (if not all)
MIP-based optimal classiﬁcation tree methods.

4.4 Usage notes of the bsnsing package

Alluding to the No Free Lunch Theorem (Wolpert and Macready 1997), no single machine learning algorithm is
universally the best-performing algorithm for all problems. To be generally useful for classiﬁcation problems, most
decision tree algorithms allow users to control the behavior of the algorithm via choosing values for a number of
hyperparameters. Such ﬂexibility can be a double-edged sword to the usability of an algorithm. Seasoned users, most
likely developers, can have the convenience of experimenting with the algorithm without changing the code, but ordinary
users unconcerned of the internal workings of the underlying algorithm may ﬁnd too many parameters perplexing. The
large, sometimes inﬁnite, value space of hyperparameters also presents practical challenges to automated parameter-
tuning processes. For example, a clear-cut valley point of the generalization error curve in the bias-variance tradeoff
analysis (see Chapter 2 of James et al. (2014)) may be difﬁcult to identify, especially when the available training
samples are relatively few in a high-dimensional feature space.

To ease the usage, we provide some guidance for parameter selection for the bsnsing algorithm from an ordinary
user’s perspective. The most important parameters for bsnsing are max.rules and node.size. For max.rules,
we recommend using the default value of 2 for a good balance between training speed and model performance. A

17

Y. Liu

Table 9: Comparison on binary classiﬁcation cases.

acute1
acute2
Adult
auto
bank
banknote
birthwt
BreastCancer
circ
climate
compas
connect
credit
diam
dystrophy
Echocard
Fertility
GlaucomaMVF
grid
haberman
heart
heloc
hepatitis
HTRU2
ILPD
Ionos
magic04
mammo
Monks1
Monks2
Monks3
Mushroom
norm3p10
norm3p5
obli
ozone1
ozone8
parkins
pima
Qsar
relax
retention
ring
seismic
sh88
Sonar
spambase
SPECT
spirals
statlog.a
thoraric
tictactoe
titanic
trans
votes
wdbc
wpbc
Average

C5.0
1.000
1.000
0.864
0.897
0.902
0.981
0.649
0.942
0.951
0.922
0.890
0.702
0.852
0.923
0.841
0.958
0.882
0.890
0.520
0.736
0.777
0.706
0.795
0.979
0.678
0.892
0.850
0.828
0.898
0.925
0.989
1.000
0.768
0.838
0.945
0.969
0.930
0.854
0.741
0.839
0.726
0.994
0.845
0.970
0.737
0.702
0.924
0.814
0.948
0.847
0.842
0.923
0.777
0.763
0.960
0.939
0.722
0.859

Mean Accuracy
tree
0.994
1.000
0.832
0.879
0.890
0.978
0.626
0.950
0.940
0.921
0.891
0.719
0.840
0.906
0.837
0.966
0.850
0.879
0.610
0.714
0.766
0.697
0.808
0.977
0.673
0.874
0.814
0.823
0.743
0.656
0.986
0.999
0.766
0.836
0.935
0.956
0.923
0.868
0.741
0.818
0.612
0.929
0.881
0.959
0.813
0.719
0.902
0.825
0.942
0.848
0.791
0.880
0.782
0.770
0.954
0.933
0.692
0.844

rpart
0.912
0.958
0.832
0.882
0.901
0.966
0.661
0.940
0.936
0.929
0.891
0.727
0.851
0.890
0.822
0.966
0.865
0.895
0.976
0.726
0.792
0.700
0.789
0.978
0.681
0.870
0.819
0.830
0.840
0.750
0.977
0.994
0.758
0.831
0.914
0.964
0.930
0.861
0.740
0.822
0.594
0.941
0.861
0.970
0.812
0.727
0.896
0.829
0.924
0.855
0.831
0.901
0.781
0.775
0.949
0.927
0.692
0.853

ctree
0.903
0.954
0.847
0.871
0.904
0.967
0.674
0.946
0.580
0.920
0.889
0.694
0.852
0.477
0.821
0.936
0.893
0.836
0.520
0.717
0.752
0.696
0.791
0.979
0.704
0.905
0.844
0.807
0.743
0.650
0.961
0.999
0.749
0.829
0.922
0.972
0.932
0.847
0.746
0.807
0.726
0.971
0.482
0.970
0.477
0.694
0.906
0.792
0.655
0.857
0.850
0.825
0.787
0.761
0.957
0.933
0.726
0.811

bsnsing
0.896
1.000
0.825
0.864
0.901
0.963
0.629
0.945
0.906
0.925
0.890
0.721
0.851
0.906
0.813
0.947
0.860
0.900
0.972
0.729
0.768
0.698
0.798
0.977
0.682
0.859
0.840
0.824
0.871
0.607
0.960
0.987
0.756
0.835
0.902
0.956
0.922
0.858
0.731
0.819
0.617
0.935
0.782
0.970
0.674
0.721
0.911
0.821
0.783
0.851
0.817
0.803
0.782
0.765
0.943
0.944
0.730
0.841

18

C5.0
1.000
1.000
0.887
0.953
0.880
0.985
0.541
0.968
0.961
0.810
0.918
0.755
0.897
0.947
0.846
0.954
0.545
0.933
0.505
0.544
0.811
0.749
0.716
0.948
0.675
0.920
0.885
0.869
0.899
0.973
0.987
1.000
0.823
0.878
0.962
0.614
0.777
0.837
0.772
0.862
0.492
0.999
0.866
0.499
0.753
0.755
0.957
0.786
0.960
0.903
0.505
0.974
0.705
0.680
0.986
0.964
0.562
0.827

ctree
0.957
0.989
0.894
0.901
0.915
0.977
0.486
0.974
0.592
0.809
0.934
0.749
0.909
0.503
0.853
0.953
0.535
0.894
0.505
0.619
0.805
0.757
0.706
0.974
0.665
0.901
0.892
0.855
0.739
0.492
0.983
1.000
0.814
0.877
0.955
0.784
0.805
0.819
0.780
0.850
0.492
0.989
0.498
0.625
0.493
0.749
0.949
0.721
0.650
0.907
0.517
0.915
0.748
0.690
0.978
0.959
0.592
0.787

Mean AUC

rpart
0.932
0.965
0.818
0.938
0.761
0.976
0.581
0.950
0.942
0.809
0.884
0.778
0.902
0.919
0.823
0.963
0.578
0.946
0.990
0.640
0.822
0.706
0.677
0.909
0.657
0.901
0.811
0.868
0.916
0.800
0.979
0.994
0.801
0.865
0.942
0.674
0.750
0.854
0.779
0.841
0.483
0.950
0.896
0.499
0.847
0.778
0.899
0.782
0.948
0.904
0.524
0.961
0.710
0.709
0.962
0.940
0.586
0.825

tree
0.996
1.000
0.852
0.923
0.881
0.984
0.584
0.969
0.966
0.757
0.919
0.770
0.890
0.944
0.857
0.967
0.635
0.948
0.615
0.649
0.808
0.736
0.716
0.969
0.681
0.901
0.842
0.878
0.739
0.540
0.989
0.999
0.815
0.896
0.957
0.632
0.712
0.868
0.787
0.860
0.503
0.966
0.918
0.568
0.851
0.770
0.948
0.785
0.962
0.906
0.560
0.953
0.710
0.688
0.984
0.956
0.586
0.825

bsnsing
0.970
1.000
0.870
0.941
0.900
0.985
0.587
0.978
0.956
0.813
0.934
0.788
0.913
0.953
0.838
0.934
0.664
0.948
0.990
0.672
0.822
0.758
0.764
0.966
0.677
0.894
0.893
0.888
0.956
0.598
0.986
0.999
0.831
0.892
0.961
0.773
0.795
0.842
0.780
0.872
0.509
0.984
0.856
0.570
0.745
0.788
0.960
0.803
0.847
0.912
0.539
0.869
0.725
0.688
0.980
0.967
0.625
0.841

CPU

0.1
0.0
82.0
0.3
71.0
0.5
0.3
0.3
0.2
0.6
4.0
2.0
1.0
0.3
0.2
0.0
0.1
1.0
0.1
0.3
0.4
37.0
0.2
23.0
0.8
1.0
106.0
0.7
0.3
0.7
0.2
4.0
1.0
0.5
0.2
6.0
17.0
0.4
0.9
5.0
0.4
6.0
0.5
1.0
0.6
2.0
27.0
0.3
0.5
0.8
0.7
1.0
0.2
0.6
0.2
1.0
0.9
7.3

Y. Liu

Table 10: Comparison on multi-class classiﬁcation cases.

n

625
1473
366
214
132
210
150
12960
5620
210
500
266
47
151
3772
178
4898
600

p

4
9
34
9
4
19
4
8
64
7
2
35
35
5
21
13
11
3

J

3
3
6
6
3
7
3
5
10
3
4
15
4
3
3
3
7
4

Mean Accuracy

C5.0
0.782
0.514
0.949
0.664
0.825
0.864
0.934
0.992
0.902
0.912
0.990
0.886
0.975
0.507
0.997
0.920
0.574
0.711
0.828

ctree
0.776
0.537
0.934
0.606
0.494
0.784
0.941
0.974
0.844
0.879
0.986
0.699
0.482
0.379
0.993
0.897
0.532
0.217
0.720

rpart
0.771
0.547
0.928
0.661
0.625
0.843
0.930
0.874
0.768
0.900
0.992
0.655
0.579
0.478
0.996
0.889
0.532
0.956
0.773

tree
0.769
0.520
0.923
0.649
0.745
0.848
0.936
0.858
0.774
0.922
0.992
0.779
0.950
0.489
0.997
0.921
0.513
0.709
0.794

bsnsing
0.816
0.525
0.915
0.649
0.698
0.828
0.945
0.903
0.897
0.922
0.985
0.631
0.986
0.455
0.962
0.911
0.534
0.931
0.810

CPU

1.4
5.0
1.6
1.2
0.5
1.1
0.2
21.7
70.8
0.3
0.2
2.3
0.2
0.8
1.6
0.3
39.3
1.1
8.3

balance
contra
derm
Glass
Hayes
imgsegm
iris
nursery
optdigits
Seeds
smiley
soybean.l
soybean.s
tae
thyroid
wine
WineQuality
xor3
Average

Table 11: Improve the bsnsing performance via changing node.size and using ensemble.

Original
AUC
0.598
0.847
0.869

Accu
0.607
0.783
0.803

Accu
0.895
0.927
0.911

Improved

AUC
0.898
0.943
0.934

Parameter
node.size=1
node.size=3
node.size=3

Ensemble

Accu
0.736
0.911
0.920

AUC CPU
8.8
0.921
5.0
0.974
12.0
0.982

Monks2
spirals
tictactoe

higher value would increase the solution time of OPT-G particularly at the root node, as can be observed in Table 3.
In the meantime, a higher value would not necessarily translate to a better classiﬁcation performance because of the
heuristic nature of the recursive partitioning process. For node.size, we recommend using the default value of 0
ﬁrst, meaning to set the minimum node size dynamically. A larger value of node.size would lead to a smaller (thus
more interpretable) tree but might underﬁt the data, while a smaller value would lead to a bigger tree and might leave
some true patterns undistinguished. If it is known that strong, learnable patterns exist in a data case, then manually
setting node.size to a small value, i.e., some positive integer smaller than
n, is likely to improve the classiﬁcation
performance over the default setting. Lastly, if interpretability is unimportant, an ensemble of several bsnsing trees
each trained with different hyperparameter values can effectively boost the performance.

√

Let us look at some concrete examples. We notice from Table 9 that on three data sets, namely, Monks2, spirals
and tictactoe, bsnsing performed especially poorly compared to the best-performing method. This suggests that
discoverable patterns exist in these data sets and that bsnsing could be conﬁgured more ﬂexible at discovering them.
Indeed, if we reduce the node.size value, a signiﬁcant improvement in the out-of-sample performance can be realized
for three cases, as shown under the “Improved” columns in Table 11. A greater improvement has also been achieved by
the ensemble approach, in which we trained a total of nine trees with parameter combinations of max.rules ∈ [1, 2, 3]
and node.size ∈ [0, 1, 10]. The class membership prediction was the result of majority voting, and the score prediction
was the average of the scores predicted by the nine trees in the ensemble. The total time (in seconds) taken to train the
nine trees remains quite manageable, as shown in the column CPU in Table 11. More detailed usage examples with
sample code are provided in the (online) appendix.

5 Conclusion and Future Work

In this paper, we have proposed a new method for classiﬁcation tree induction that combines mathematical optimization
with the recursive partitioning framework. The method optimally selects a boolean combination of multiple variables to
maximize the Gini reduction in each node split. The split optimization model is the ﬁrst one that is able to maximize
the well-justiﬁed but nonlinear Gini reduction metric in a mixed-integer linear program. We have developed an efﬁcient
search algorithm to solve realistically regulated instances faster than commercial optimization solvers, making the
overall solution scheme, as well as the R package bsnsing, more accessible to both the practitioners’ and the developers’

19

Y. Liu

community. Evaluation results have suggested that the bsnsing package can generate the best AUC performance
among other decision tree packages in R, at the cost of a median training time of a few seconds.

A central theme in the design of classiﬁcation tree algorithms is making tradeoffs to strike a balance among competing
objectives, such as speed, accuracy and interpretability. We believe that optimization modeling is no substitute for
the recursive partitioning framework, it, however, can alleviate some structural restrictions via answering key design
questions in a new light. One of the beneﬁts of using mathematical optimization in decision tree induction is that it
makes the process more tractable and justiﬁable. As observed in previous works and in this paper, properly regularized
optimal trees do not lead to overﬁtting. Therefore, decision tree optimization is worthy of further development.

Several aspects of the present work can be extended. First, the OPT-G model exhibits a clear sparsity pattern which may
be exploited to expedite the solution. For instance, it is possible to adapt the bounding technique in the ENUM algorithm
to the branch-and-bound framework via adding user cuts, and to develop multiple branch-and-bound trees for parallel
computing. However, this would require the use of advanced callback functions which are not currently supported in
R APIs (for both Gurobi and CPLEX). Implementation in other languages could exploit these possibilities. Second,
the feature binarization process is unoptimized, and the actual utility of the candidate split rules are unquantiﬁed. It is
possible that a good proportion of the candidate rules are dominated by others hence need not be generated in the ﬁrst
place. Future research could explore the column generation paradigm to generate high-value binary features on the ﬂy
during the optimal selection process.

Appendix: Usage demonstration of the bsnsing package

the

install

To
library(devtools);
install_github(‘profyliu/bsnsing’). The following code snippet demonstrates a stylized use case of
building and evaluating a decision tree model.

an R user

command:

bsnsing

package,

this

can

run

# r a n d o m l y s a m p l e 70% f o r
f o r
# t h e r e m a i n i n g i s

t r a i n i n g
t e s t i n g

f u n c t i o n

t o p r e d i c t C l a s s , u s i n g a l l d e f a u l t o p t i o n s

# S e t

t r a i n s e t )

s e e d f o r RNG i n t h e s a m p l e ( )

l i b r a r y ( b s n s i n g )
s e t . s e e d ( 2 0 2 1 )
n <− nrow ( B r e a s t C a n c e r )
t r a i n s e t <− s a m p l e ( 1 : n , 0 . 7 * n )
t e s t s e t <− s e t d i f f ( 1 : n ,
# B u i l d a t r e e
b s <− b s n s i n g ( C l a s s ~ . , d a t a = B r e a s t C a n c e r [ t r a i n s e t
summary ( b s )
# d i s p l a y t h e
p r e d <− p r e d i c t ( bs , B r e a s t C a n c e r [ t e s t s e t
a c t u a l <− B r e a s t C a n c e r [ t e s t s e t ,
’ C l a s s ’ ]
t a b l e ( p r e d ,
a c t u a l )
# P l o t

1
2
3
4
5
6
7
8
9
10
11
12
t h e ROC c u r v e and d i s p l a y t h e AUC
13 ROC_ f u n c ( d a t a . f r a m e ( p r e d i c t ( bs , B r e a s t C a n c e r [ t e s t s e t
B r e a s t C a n c e r [ t e s t s e t , ’ C l a s s ’ ] ) ,
14
15
2 , 1 , p o s . l a b e l = ’ m a l i g n a n t ’ , p l o t . ROC=T )
16
17

f i l e = ’ . . / b s n s i n g _ t e s t / f i g / B r e a s t C a n c e r . p d f ’ )

# P l o t
p l o t ( bs ,

c o n f u s i o n m a t r i x

# d i s p l a y t h e

and g e n e r a t e

t o PDF f i l e

s t r u c t u r e ,

t r e e

t r e e

t h e

t h e

, ] ,

, ] )

s e e F i g u r e 6

t y p e = ’ c l a s s ’ )

, ] ) ,

l a t e x s o u r c e c o d e

# s e e F i g u r e 7 l e f t

The model summary (generated by line 8) prints out the tree structure as well as node information in plain text, shown
in Figure 5. We can read from the printout, for example, that the root node (Node 0) is classiﬁed as 0 (benign) with
probability 0.6585, and that 100% of all training observations fall in this node, of which 167 observations are class 1
and 322 observations are class 0. The confusion matrix on the training set is given at the end of the summary print.
Detailed information of the bsnsing tree object can be accessed by the R command str(bs).

The bsnsing package implements the S3 method plot for plotting the bsnsing object (see line 17). If a ﬁle name is
provided (as shown in code), the function will save the latex scripts (that utilize the tikz package) to a .tex ﬁle and
attempt to build the .ps and .pdf ﬁles by calling latex, dvips, ps2pdf commands if they are installed. The plot is
shown in Figure 6 left. Each node is represented by a circle with the node number printed inside the circle. The color
of a leaf node indicates its predicted class, green for positive (class 1) and red for negative (class 0). The split rule is
shown on the left of each internal node in blue, and the class 1 probability of the node is shown on the right of the
node. At the bottom of each leaf node, the predicted label (in this case, malignant or benign), along with the number of
training observations that fall in the node, is printed. Of course, these features can be easily customized and extended
by other developers. The right side of Figure 6 plots a smaller tree generated on the same training set with the option
no.same.gender.children = True, to suppress splits that would generate child nodes having the same majority
class. Figure 7 compares the ROC curves of these two trees. In these particular cases, some substantial improvement in
interpretability was only accompanied by a slight drop in AUC, so the smaller tree (in the author’s opinion) is preferred.

20

Y. Liu

Figure 5: Summary display of the bsnsing tree for the BreastCancer data set.

Figure 6: Discriminability versus interpretability. The left tree is built with the default options, and the right tree is built
with option no.same.gender.children = True.

Try-and-compare is a common practice in predictive analytics, and the bsnsing library is generally fast enough and
ﬂexible enough to support such practice.

References

Sina Aghaei, Andres Gomez, and Phebe Vayanos. Learning optimal classiﬁcation trees: Strong max-ﬂow formulations, 2020.
Gaël Aglin, Siegfried Nijssen, and Pierre Schaus. Learning optimal decision trees using caching branch-and-bound search.
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 34(04):3146–3153, Apr. 2020. doi: 10.1609/aaai.v34i04.5711.
URL https://ojs.aaai.org/index.php/AAAI/article/view/5711.

21

Bare.nuclei<1.5orCell.shape<1.500.342Cl.thickness<=4orBl.cromatin<2.510.0353benign2850.0044benign320.312Bare.nuclei>5.520.907Bare.nuclei<=850.9837malignant220.9098malignant991.000Cell.size>4.5orMitoses>2.560.7259malignant280.96410benign230.435Bare.nuclei<1.5orCell.shape<1.500.3421benign3170.0352malignant1720.907Y. Liu

Figure 7: ROC curves constructed on 210 test cases of the BreastCancer data set for two bsnsing trees. The large tree
was built with the default options, and the small tree with option no.same.gender.children = True.

Rakesh Agrawal, Tomasz Imielinski, and Arun Swami. Mining association rules between sets of items in large databases. In
Proceedings of The 1993 ACM SIGMOD International Conference on Management of Data, Washington DC, USA, pages
207–216, 1993.

Mohamed Alaradi and Sawsan Hilal. Tree-based methods for loan approval. In 2020 International Conference on Data Analytics for
Business and Industry: Way Towards a Sustainable Economy (ICDABI), pages 1–6, 2020. doi: 10.1109/ICDABI51230.2020.
9325614.

Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia Rudin. Learning certiﬁably optimal rule
lists. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
KDD ’17, page 35–44, New York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450348874. doi:
10.1145/3097983.3098047. URL https://doi.org/10.1145/3097983.3098047.

Dimitris Bertsimas and Jack Dunn. Optimal classiﬁcation trees. Machine Learning, 106(7):1039–1082, Jul 2017. ISSN 1573-0565.

doi: 10.1007/s10994-017-5633-9. URL https://doi.org/10.1007/s10994-017-5633-9.

Dimitris Bertsimas, Allison Chang, and Cynthia Rudin. An integer optimization approach to associative classiﬁcation. In In

Proceedings of Neural Information Processing Systems, pages 269–277, 2012.

Dimitris Bertsimas, Jack Dunn, Colin Pawlowski, and Ying Daisy Zhuo. Robust classiﬁcation. INFORMS Journal on Optimization,

1(1):2–34, 2019. doi: 10.1287/ijoo.2018.0001. URL https://doi.org/10.1287/ijoo.2018.0001.

Christian Borgelt. Frequent item set mining. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 2(6):437–456,

2012. ISSN 1942-4795. doi: 10.1002/widm.1074. URL http://dx.doi.org/10.1002/widm.1074.

Leo Breiman, Jerome Friedman, Charles J. Stone, and R.A. Olshen. Classiﬁcation and Regression Trees. Taylor & Francis, January

1984.

Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/ml.
FICO. Explainable machine learning challenge, 2018. https://community.ﬁco.com/s/explainable-machine-learning-challenge.
Mohammad M. Ghiasi, Sohrab Zendehboudi, and Ali Asghar Mohsenipour. Decision tree-based diagnosis of coronary artery
disease: Cart model. Computer Methods and Programs in Biomedicine, 192:105400, 2020.
ISSN 0169-2607. doi:
https://doi.org/10.1016/j.cmpb.2020.105400. URL https://www.sciencedirect.com/science/article/pii/S0169260719308971.
Siong Thye Goh and Cynthia Rudin. Box drawings for learning with imbalanced data. In Proceedings of the 20th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, KDD ’14, page 333–342, New York, NY, USA, 2014.
Association for Computing Machinery. ISBN 9781450329569. doi: 10.1145/2623330.2623648. URL https://doi.org/10.1145/
2623330.2623648.

Robert C. Holte. Very simple classiﬁcation rules perform well on most commonly used datasets. Machine Learning, 11(1):63–90,

Apr 1993. ISSN 1573-0565. URL https://doi.org/10.1023/A:1022631118932.

Xiyang Hu, Cynthia Rudin, and Margo I. Seltzer. Optimal sparse decision trees. CoRR, abs/1904.12847, 2019. URL http:

//arxiv.org/abs/1904.12847.

22

Y. Liu

Laurent Hyaﬁl and Ronald L. Rivest. Constructing optimal binary decision trees is np-complete. Information Processing Letters, 5
(1):15 – 17, 1976. ISSN 0020-0190. doi: https://doi.org/10.1016/0020-0190(76)90095-8. URL http://www.sciencedirect.
com/science/article/pii/0020019076900958.

Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statistical Learning: With Applications in R.

Springer Publishing Company, Incorporated, 2014. ISBN 1461471370, 9781461471370.

G. V. Kass. An exploratory technique for investigating large quantities of categorical data. Journal of the Royal Statistical Society.
Series C (Applied Statistics), 29(2):119–127, 1980. ISSN 00359254, 14679876. URL http://www.jstor.org/stable/2986296.
Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, and David Madigan. Interpretable classiﬁers using rules and bayesian
analysis: Building a better stroke prediction model. Ann. Appl. Stat., 9(3):1350–1371, 09 2015. URL https://doi.org/10.1214/
15-AOAS848.

Jimmy Lin, Chudi Zhong, Diane Hu, Cynthia Rudin, and Margo Seltzer. Generalized and scalable optimal sparse decision trees. In

International Conference on Machine Learning, pages 6150–6160, 2020a.

Jimmy Lin, Chudi Zhong, Diane Hu, Cynthia Rudin, and Margo Seltzer. Generalized and scalable optimal sparse decision trees,

2020b.

B. Liu, W. Hsu, and Y. Ma. Integrating classiﬁcation and association rule mining. Knowledge discovery and data mining, page

80–86, 1998.

Wei-Yin Loh. Improving the precision of classiﬁcation trees. Ann. Appl. Stat., 3(4):1710–1737, 12 2009. URL https://doi.org/10.

1214/09-AOAS260.

Dmitry Malioutov and Kush Varshney. Exact rule learning via boolean compressed sensing. In Sanjoy Dasgupta and David
McAllester, editors, Proceedings of the 30th International Conference on Machine Learning, volume 28:3 of Proceedings of
Machine Learning Research, pages 765–773, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR. URL http://proceedings.mlr.
press/v28/malioutov13.html.

I. Gusti Ngurah Narindra Mandala, Catharina Badra Nawangpalupi, and Fransiscus Rian Praktikto. Assessing credit risk: An
application of data mining in a rural bank. Procedia Economics and Finance, 4:406–412, 2012. ISSN 2212-5671. doi: https:
//doi.org/10.1016/S2212-5671(12)00355-3. URL https://www.sciencedirect.com/science/article/pii/S2212567112003553.
International Conference on Small and Medium Enterprises Development with a Theme ?Innovation and Sustainability in
SME Development? (ICSMED 2012).

Siegfried Nijssen and Elisa Fromont. Mining optimal decision trees from itemset lattices.

In Proceedings of the 13th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’07, page 530–539, New York,
NY, USA, 2007. Association for Computing Machinery. ISBN 9781595936097. doi: 10.1145/1281192.1281250. URL
https://doi.org/10.1145/1281192.1281250.

Siegfried Nijssen and Elisa Fromont. Optimal constraint-based decision tree induction from itemset lattices. Data Mining and
Knowledge Discovery, 21(1):9–51, July 2010. ISSN 1384-5810. URL http://dx.doi.org/10.1007/s10618-010-0174-x.
J. R. Quinlan and R. M. Cameron-Jones. Oversearching and layered search in empirical learning. In Proceedings of the 14th
International Joint Conference on Artiﬁcial Intelligence - Volume 2, IJCAI’95, pages 1019–1024, San Francisco, CA, USA,
1995. Morgan Kaufmann Publishers Inc. ISBN 1-55860-363-8. URL http://dl.acm.org/citation.cfm?id=1643031.1643032.

J. Ross Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1993. ISBN

1-55860-238-0.

Peter R. Rijnbeek and Jan A. Kors. Finding a short and accurate decision rule in disjunctive normal form by exhaustive search.

Machine Learning, 80(1):33–62, July 2010. ISSN 0885-6125. URL https://doi.org/10.1007/s10994-010-5168-9.

Eric H. Sorensen, Keith L. Miller, and Chee K. Ooi. The decision tree approach to stock selection. The Journal of Portfolio
Management, 27(1):42–52, 2000. ISSN 0095-4918. doi: 10.3905/jpm.2000.319781. URL https://jpm.pm-research.com/
content/27/1/42.

W. Nick Street. Oblique multicategory decision trees using nonlinear programming. INFORMS Journal on Computing, 17(1):25–31,

2005. doi: 10.1287/ijoc.1030.0047. URL https://doi.org/10.1287/ijoc.1030.0047.

Pang-Ning Tan, Michael Steinbach, and Vipin Kumar. Introduction to Data Mining. Pearson, 1st edition, 2005.
Lukas Tanner, Mark Schreiber, Jenny G. H. Low, Adrian Ong, Thomas Tolfvenstam, Yee Ling Lai, Lee Ching Ng, Yee Sin Leo,
Le Thi Puong, Subhash G. Vasudevan, Cameron P. Simmons, Martin L. Hibberd, and Eng Eong Ooi. Decision tree algorithms
predict the diagnosis and outcome of dengue fever in the early phase of illness. PLOS Neglected Tropical Diseases, 2(3):1–9,
03 2008. doi: 10.1371/journal.pntd.0000196. URL https://doi.org/10.1371/journal.pntd.0000196.

Hélène Verhaeghe, Siegfried Nijssen, Gilles Pesant, Claude-Guy Quimper, and Pierre Schaus. Learning optimal decision trees using

constraint programming. Constraints, 25:1–25, 12 2020. doi: 10.1007/s10601-020-09312-3.

Sicco Verwer and Y. Zhang. Learning optimal classiﬁcation trees using a binary linear program formulation. In Proceedings of
the Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19), pages 1625–1632. AAAI Press, 2019a. ISBN 978-1-
57735-809-1. doi: 10.1609/aaai.v33i01.33011624. URL https://aaai.org/Conferences/AAAI-19/. 33rd AAAI Conference on
Artiﬁcial Intelligence, AAAI-19 ; Conference date: 27-01-2019 Through 01-02-2019.

Sicco Verwer and Yingqian Zhang. Learning decision trees with ﬂexible constraints and objectives using integer optimization. In
Domenico Salvagnin and Michele Lombardi, editors, Integration of AI and OR Techniques in Constraint Programming, pages
94–103, Cham, 2017. Springer International Publishing. ISBN 978-3-319-59776-8.

23

Y. Liu

Sicco Verwer and Yingqian Zhang. Learning optimal classiﬁcation trees using a binary linear program formulation. Proceedings of
the AAAI Conference on Artiﬁcial Intelligence, 33(01):1625–1632, Jul. 2019b. doi: 10.1609/aaai.v33i01.33011624. URL
https://ojs.aaai.org/index.php/AAAI/article/view/3978.

Tong Wang, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampﬂ, and Perry MacNeille. A bayesian framework for
learning rule sets for interpretable classiﬁcation. Journal of Machine Learning Research, 18(70):1–37, 2017. URL http:
//jmlr.org/papers/v18/16-003.html.

D.H. Wolpert and W.G. Macready. No free lunch theorems for optimization. IEEE Transactions on Evolutionary Computation, 1(1):

67–82, 1997. doi: 10.1109/4235.585893.

Hongyu Yang, Cynthia Rudin, and Margo Seltzer. Scalable Bayesian rule lists. In Doina Precup and Yee Whye Teh, editors,
Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning
Research, pages 3921–3930, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR. URL http:
//proceedings.mlr.press/v70/yang17h.html.

Haoran Zhu, Pavankumar Murali, Dzung T. Phan, Lam M. Nguyen, and Jayant Kalagnanam. A scalable mip-based method
for learning optimal multivariate decision trees. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.

24

