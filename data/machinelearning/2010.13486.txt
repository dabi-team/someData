Adaptive Optimal Trajectory Tracking Control
Applied to a Large-Scale Ball-on-Plate System

Florian KÃ¶pf*, Sean Kille*, Jairo Inga, SÃ¶ren Hohmann

1
2
0
2

n
a
J

5
2

]

Y
S
.
s
s
e
e
[

2
v
6
8
4
3
1
.
0
1
0
2
:
v
i
X
r
a

Abstractâ€” While many theoretical works concerning Adap-
tive Dynamic Programming (ADP) have been proposed, appli-
cation results are scarce. Therefore, we design an ADP-based
optimal trajectory tracking controller and apply it to a large-
scale ball-on-plate system. Our proposed method incorporates
an approximated reference trajectory instead of using setpoint
tracking and allows to automatically compensate for constant
offset terms. Due to the off-policy characteristics of the algo-
rithm, the method requires only a small amount of measured
data to train the controller. Our experimental results show that
this tracking mechanism signiï¬cantly reduces the control cost
compared to setpoint controllers. Furthermore, a comparison
with a model-based optimal controller highlights the beneï¬ts of
our model-free data-based ADP tracking controller, where no
system model and manual tuning are required but the controller
is tuned automatically using measured data.

I. INTRODUCTION

Model-free Adaptive Dynamic Programming (ADP) is a
promising approach to control dynamical systems whenever a
system model is unavailable, inaccurate or difï¬cult to achieve
[1]â€“[4]. While many control applications require to track
desired reference trajectories, this is non-trivial to incorporate
into the ADP formalism adequately [5], [6].

Assuming that the reference trajectory is generated directly
by an unknown command system (cf. [7]â€“[9]) limits the ï¬‚ex-
ibility of the reference trajectory that can be commanded1.
Alternative approaches extend the system state by the desired
state [10]â€“[12] or the current and next desired state [13].
Shi et al. [13] take into account the desired position of
an underwater vehicle model at the current and next time
step and train their controller using pseudo-averaged Q-
learning in simulation. Although the learned (projected)
setpoint controller for an autonomous helicopter [10] and the
setpoint controller for a quadrotor [11] have been applied
to real systems, in [10] and [11] the training procedure is
based on simulations, thus requiring a model of the system
to be controlled. Puccetti et al. [12] use model-free ADP for
the speed tracking control of a real car, where a velocity
setpoint is incorporated into the state-action value function.
Nevertheless, these representations of the reference trajectory
have limited [10], [13] or no preview capabilities [11], [12],
which results in a controller that tends to lag behind.

Therefore, in our previous works, we have incorporated the
reference trajectory over a ï¬nite horizon into the Q-function

F. KÃ¶pf, S. Kille, J. Inga, and S. Hohmann are with the Institute of Control

Systems, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany
(e-mail: {ï¬‚orian.koepf, jairo.inga, soeren.hohmann}@kit.edu)

*These authors contributed equally to this work.
1If the reference trajectory does not result from this unknown command

system during training, these methods fail.

Fig. 1. Large-scale ball-on-plate system for ADP-based trajectory tracking
control.

[5] or used an approximated reference trajectory [6]. Instead
of assuming an unknown underlying command system, the
controller approximates an arbitrary reference trajectory in a
way that is compatible with ADP allowing ï¬‚exible reference
trajectories. However, [1]â€“[9], [13] only provide simulation
results and no application to a real systemâ€”an essential step
that is missing in order to validate ADP methods.

In this paper, we propose an ADP tracking controller
which incorporates an approximated reference trajectory and
apply it to a real large-scale ball-on-plate system (depicted in
Fig. 1). The ball-on-plate system is a widely used example
for benchmarking controllers. Existing controllers are either
fully model-based [14]â€“[18] or model-based with additional
fuzzy supervision [19]. Thus, our work is the ï¬rst application
of a model-free ADP-based controller to a ball-on-plate
system. Furthermore, instead of incorporating the reference
trajectory, existing controllers either perform no tracking of
the ball position at all [14], [15], [18] or simply consider the
current deviation from a setpoint causing a trajectory that
lags behind [16], [17], [19].

In contrast to existing controllers, our method does not
require a model of the ball-on-plate system as we train our
optimal tracking controller directly through a policy iteration
(PI) mechanism [20] using measured data from a real system.
This avoids tedious model design followed by manual tuning.
By using an off-policy algorithm, the measured data can be
re-used, reducing the effort to record training data2. Further-
more, instead of the widely-used setpoint tracking, our ADP
controller incorporates information on the course of the ref-
erence trajectory which allows predictive rather than reactive
behavior and avoids lagging behind. Our automatically tuned
controller is also able to learn static offsets to compensate

2In contrast, on-policy learning would require new data to be collected
after each policy improvement step and the estimates would be biased when
(indispensable) exploration noise is used [21].

Â© 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers
or lists, or reuse of any copyrighted component of this work in other works.

 
 
 
 
 
 
for asymmetries. In summary, our main contributions include
an ADP tracking controller which is

âˆ™ data-efï¬cient as it works off-policy and uses a ï¬‚exible
and compact local approximation of arbitrary reference
trajectories that is compatible with ADP

âˆ™ trained on a real system using measured data, requiring

neither system parameters nor manual tuning

âˆ™ compared to a model-based and a setpoint controller.
The remainder of this paper is structured as follows: In
Section II, the system and problem description are given.
The theoretical background to our ADP tracking formalism
is given in Section III. In Section IV, we present our method.
Results are given in Section V, before we conclude the work.

ğ‘– âˆˆ N0, where ğ‘Ÿ(ğ‘ğ‘˜, ğ‘–) is the desired ball position at time ğ‘˜+ğ‘–
(i.e. ğ‘– denotes the time step on the reference from the local
perspective at time ğ‘˜), ğ‘ğ‘˜ âˆˆ Î˜ âŠ† Rğ‘›p a parameter vector
and ğœŒ(ğ‘–) a basis function vector (cf. [6]). The following
problem formalizes that the ball position should follow a
desired reference trajectory while keeping other system states
and the control effort small.

Problem 1. Assume given basis functions ğœŒ(ğ‘–) for ref-
erence trajectory approximation and measurement
tuples
{ğ‘¥ğ‘–, ğ‘¢ğ‘–, ğ‘¥ğ‘–+1}, ğ‘– = ğ‘˜, . . . , ğ‘˜+ğ‘ âˆ’1. Let the system dynamics
ğ‘“ (ğ‘¥ğ‘˜, ğ‘¢ğ‘˜) be unknown. Find the control law ğœ‹*(ğ‘¥ğ‘˜, ğ‘ğ‘˜)
such that âˆ€ğ‘¥ğ‘˜, ğ‘ğ‘˜ the control ğ‘¢*
ğ‘˜ = ğœ‹*(ğ‘¥ğ‘˜, ğ‘ğ‘˜) minimizes
the objective function

II. SYSTEM AND PROBLEM DESCRIPTION
In the following, the ball-on-plate system that is used as
an application example for our ADP tracking method and
the problem formulation are given.

ğ½ğ‘˜ =

âˆ
âˆ‘ï¸

ğ‘–=0

ğ›¾ğ‘–

â›

â¡

âœ
âœ
â

â¢
â¢
â£

ğ‘¥1,ğ‘˜+ğ‘– âˆ’ ğ‘Ÿ(ğ‘ğ‘˜, ğ‘–)
ğ‘¥2,ğ‘˜+ğ‘–
ğ‘¥3,ğ‘˜+ğ‘–
ğ‘¥4,ğ‘˜+ğ‘–
â

(cid:124)
â¤

â¥
â¥
â¦

â¡

â¢
â¢
â£

ğ‘„

ğ‘¥1,ğ‘˜+ğ‘– âˆ’ ğ‘Ÿ(ğ‘ğ‘˜, ğ‘–)
ğ‘¥2,ğ‘˜+ğ‘–
ğ‘¥3,ğ‘˜+ğ‘–
ğ‘¥4,ğ‘˜+ğ‘–

â¤

â¥
â¥
â¦

A. Ball-on-Plate System

The system used in this work is a custom-built large-
scale ball-on-plate system (see Fig. 1). Its centerpiece is a
1 m2 square plate with a mass of 16.3 kg. The plate can
be tilted in two dimensions (denoted by ğ‘‹ and ğ‘Œ ) that are
orthogonal to each other. Each dimension is actuated by its
own designated motor. The plate angles (ğ›¼[ğ‘‹], ğ›¼[ğ‘Œ ]) and
angular velocities (ğœ”[ğ‘‹], ğœ”[ğ‘Œ ]) are measured every 10 ms.
A ball with a mass of 0.042 kg and a radius of 0.02 m is
located on the plate. Its position in plate-ï¬xed coordinates
is tracked via a camera, providing an updated ball position
(ğ‘ [ğ‘‹], ğ‘ [ğ‘Œ ]) and ball velocity (ğ‘£[ğ‘‹], ğ‘£[ğ‘Œ ]) every Î”ğ‘¡ = 40 ms.
For a detailed description of the system architecture and the
hardware, see [18]3. Thus, the resulting system states

[ï¸

]ï¸(cid:124)

ğ‘¥[ğ‘‘]

ğ‘˜ =

ğ‘˜ ğ›¼[ğ‘‘]

ğ‘˜ ğ‘£[ğ‘‘]
ğ‘ [ğ‘‘]
are deï¬ned for both dimensions ğ‘‘ âˆˆ ğ’Ÿ = {ğ‘‹, ğ‘Œ }. The
system input ğ‘¢[ğ‘‘]
is the current for the motor driver
controller.

ğ‘˜ = ğ¼ [ğ‘‘]

ğ‘˜ ğœ”[ğ‘‘]

(1)

ğ‘˜

ğ‘˜

As the two dimensions ğ‘‹ and ğ‘Œ only slightly depend on
each other, they are usually controlled separately (see [14],
[15], [17], [18]). Since the controllers for the two dimensions
are trained in the same way, the index ğ‘‘ is omitted in the
following for the sake of readability.

(cid:124)
ğ‘˜+ğ‘–ğ‘…ğ‘¢ğ‘˜+ğ‘–
+ ğ‘¢

âŸ
âŸ
â 

=:

âˆ
âˆ‘ï¸

ğ‘–=0

ğ›¾ğ‘–ğ‘(ğ‘¥ğ‘˜+ğ‘–, ğ‘¢ğ‘˜+ğ‘–, ğ‘Ÿ(ğ‘ğ‘˜, ğ‘–)),

(4)
where ğ›¾ âˆˆ (0, 1) denotes a discount factor, ğ‘„ is assumed to
be positive semi-deï¬nite and ğ‘… positive deï¬nite.

III. ADP TRACKING THEORY

In this section, we brieï¬‚y summarize the theoretical back-
ground on our ADP tracking formalism related to Problem 1.

Lemma 1. Deï¬ne

(cid:124)

ğ‘(ğ‘–)
ğ‘˜

= ğ‘

(cid:124)
ğ‘˜ğ‘‡ (ğ‘–),

where ğ‘‡ (ğ‘–) is chosen such that

(ï¸

ğ‘Ÿ

ğ‘(ğ‘–)
ğ‘˜ , ğ‘—

)ï¸

= ğ‘Ÿ(ğ‘ğ‘˜, ğ‘– + ğ‘—),

âˆ€ğ‘–, ğ‘— âˆˆ N0

(5)

(6)

holds and
ğ‘„*(ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘ğ‘˜) = ğ‘(ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘Ÿ(ğ‘ğ‘˜, 0))
ğ‘¥ğ‘˜+ğ‘–, ğœ‹*(ï¸

âˆ
âˆ‘ï¸

ğ›¾ğ‘–ğ‘

+

(ï¸

)ï¸

ğ‘¥ğ‘˜+ğ‘–, ğ‘(ğ‘–)
ğ‘˜

)ï¸

, ğ‘Ÿ(ğ‘ğ‘˜, ğ‘–)

ğ‘–=1

= ğ‘(ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘Ÿ(ğ‘ğ‘˜, 0))
ğ‘¥ğ‘˜+1, ğœ‹*(ï¸
+ ğ›¾ğ‘„*(ï¸

ğ‘¥ğ‘˜+1, ğ‘(1)
ğ‘˜

)ï¸

, ğ‘(1)
ğ‘˜

)ï¸

.

B. Problem Formulation

Consider the discrete-time controllable system dynamics

Then,

ğ‘¥ğ‘˜+1 = ğ‘“ (ğ‘¥ğ‘˜, ğ‘¢ğ‘˜)
(2)
where ğ‘˜ âˆˆ N0 describes the discrete time step, ğ‘¥ğ‘˜ âˆˆ ğ’³ âŠ†
Rğ‘› the system state (1), ğ‘¢ğ‘˜ âˆˆ ğ’° âŠ† Rğ‘š the control input
ğ¼ [ğ‘‘]
and ğ‘“ is unknown. From Section II-A, the system order
ğ‘˜
ğ‘› = 4 and number of control inputs ğ‘š = 1 follows for each
dimension in ğ’Ÿ. At each time step ğ‘˜, an approximation of
the desired ball position trajectory is denoted by

ğ‘Ÿ(ğ‘ğ‘˜, ğ‘–) = ğ‘

(cid:124)
ğ‘˜ğœŒ(ğ‘–),

(3)

3Note that we use a heavier plate and a different ball in the present work.

ğ‘¢*
ğ‘˜ = arg min

ğ‘¢ğ‘˜
is a solution to Problem 1.

ğ‘„*(ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘ğ‘˜)

Proof. See [6, Lemma 1].
Note 1. ğ‘„*(ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘ğ‘˜) is the accumulated discounted cost
if the system is in state ğ‘¥ğ‘˜, the control ğ‘¢ğ‘˜ is applied at
time step ğ‘˜ and the optimal control ğœ‹*(Â·) thereafter. Using
the shifted reference trajectory approximation ğ‘(ğ‘–)
(cf. (5))
ğ‘˜
ensures that the Q-function ğ‘„*(Â·) is compatible with ADP
(cf. [6, Note 1]).

(7)

(8)

As the optimal Q-function ğ‘„*(ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘ğ‘˜) is unknown,
linear function approximation (FA) (cf. [1]â€“[9], [12], [20],
[22]) is commonly used4. Thus, suppose ^ğ‘„(ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘ğ‘˜) =
^ğ‘¤(cid:124)ğœ‘(ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘ğ‘˜), where ^ğ‘¤ âˆˆ Rğ‘›w is a weight vector to be
adapted and ğœ‘(Â·) âˆˆ Rğ‘›w a vector of activation functions. A
common approach in order to tune ^ğ‘¤ is given by a PI (see
e.g. [1], [20], [22]). In this iterative procedure, each iteration
ğ‘™ consists of two steps. The policy evaluation step estimates
the Q-function

^ğ‘„^ğœ‹ğ‘™ (ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘ğ‘˜) = ^ğ‘¤

(cid:124)
ğ‘™ ğœ‘(ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘ğ‘˜)

(9)

of the current policy ^ğœ‹ğ‘™, i.e. adapts ^ğ‘¤ğ‘™ in order to solve
^ğ‘„^ğœ‹ğ‘™ (ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘ğ‘˜) = ğ‘(ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘Ÿ(ğ‘ğ‘˜, 0))
(ï¸

.
(10)
The policy improvement step then greedily updates the policy
^ğœ‹ğ‘™+1 based on ^ğ‘„^ğœ‹ğ‘™ :

ğ‘¥ğ‘˜+1, ^ğœ‹ğ‘™

ğ‘¥ğ‘˜+1, ğ‘(1)
ğ‘˜

+ ğ›¾ ^ğ‘„^ğœ‹ğ‘™

, ğ‘(1)
ğ‘˜

)ï¸

)ï¸

(ï¸

^ğœ‹ğ‘™+1(ğ‘¥ğ‘˜, ğ‘ğ‘˜) = arg min

ğ‘¢ğ‘˜

^ğ‘„^ğœ‹ğ‘™ (ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘ğ‘˜).

(11)

Convergence results of a Q-function-based PI are given in
e.g. [20, Theorem 3.1], [8, Theorem 1].

IV. ADP TRACKING ON THE BALL-ON-PLATE SYSTEM

The ADP tracking formalism introduced in Section III is
applied to the ball-on-plate system described in Section II-A.

A. Quadratic Polynomial Reference Approximation

We choose the reference trajectory to be approximated by

means of a quadratic polynomial

ğ‘Ÿ(ğ‘ğ‘˜, ğ‘–) = ğ‘

(cid:124)

ğ‘˜ğœŒ(ğ‘–) = ğ‘ğ‘˜,2(ğ‘–Î”ğ‘¡)2 + ğ‘ğ‘˜,1ğ‘–Î”ğ‘¡ + ğ‘ğ‘˜,0,

(12)

with the basis functions ğœŒ(ğ‘–) = [ï¸€(ğ‘–Î”ğ‘¡)2 ğ‘–Î”ğ‘¡ 1]ï¸€(cid:124)
parameter vector ğ‘ğ‘˜ = [ï¸€ğ‘ğ‘˜,2 ğ‘ğ‘˜,1 ğ‘ğ‘˜,0
the sampling time.

and the
, where Î”ğ‘¡ denotes

]ï¸€(cid:124)

The transformation needed to obtain the propagated ver-

sion ğ‘(ğ‘–)

ğ‘˜ of ğ‘ğ‘˜ according to (3) and (6) is given by

time step, ğ‘ğ‘˜ is determined by a weighted least-squares (LS)
regression. Therefore, we deï¬ne

Â¯ğ‘Ÿğ‘˜:ğ‘˜+â„râˆ’1 = [ï¸€Â¯ğ‘Ÿğ‘˜

Â¯ğ‘Ÿğ‘˜+1
ğ‘Š p = diag(1, ğ›½, . . . , ğ›½â„râˆ’1),

. . .

Â¯ğ‘Ÿğ‘˜+â„râˆ’1

]ï¸€ ,

ğœŒ0:â„râˆ’1 = [ï¸€ğœŒ(0) ğœŒ(1)

. . . ğœŒ(â„r âˆ’ 1)]ï¸€(cid:124)

(14)

(15)

(16)

,

with ğ‘Š p being a weighting matrix with the discount factor
ğ›½ â‰¤ 1, so that future time steps in the horizon are less
important for the ï¬tting process than early time steps. The
parameter for the reference trajectory approximation is then
calculated with the weighted LS regression according to [6]
and given by

(cid:124)
ğ‘˜ = Â¯ğ‘Ÿğ‘˜:ğ‘˜+â„râˆ’1ğ‘Š pğœŒ0,â„râˆ’1
ğ‘
B. Q-Function Approximation

(ï¸

(cid:124)
0,â„râˆ’1ğ‘Š pğœŒ0,â„râˆ’1
ğœŒ

)ï¸âˆ’1

. (17)

â¤

â¡

(cid:124) â¡

^ğ‘„^ğœ‹ğ‘™ (ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘ğ‘˜) =

The approximated Q-function (9) is chosen as
up â„(ğ‘™)
ux â„(ğ‘™)
xp â„(ğ‘™)
xx â„(ğ‘™)
pp â„(ğ‘™)
px â„(ğ‘™)
1p â„(ğ‘™)
1x â„(ğ‘™)
(cid:124)
ğ‘™ ğœ‘(ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘ğ‘˜),

â„(ğ‘™)
uu â„(ğ‘™)
xu â„(ğ‘™)
â„(ğ‘™)
â„(ğ‘™)
pu â„(ğ‘™)
1u â„(ğ‘™)
â„(ğ‘™)
(cid:124)
ğ‘˜ğ» ğ‘™ğ‘§ğ‘˜ = ^ğ‘¤

ğ‘¢ğ‘˜
ğ‘¥ğ‘˜
ğ‘ğ‘˜
1

â¥
â¥
â¥
â¦

â¢
â¢
â¢
â£

= ğ‘§

â¢
â¢
â£

â¥
â¥
â¦

â¤

x1

p1

11

u1

â¡

â¢
â¢
â£

ğ‘¢ğ‘˜
ğ‘¥ğ‘˜
ğ‘ğ‘˜
1

â¤

â¥
â¥
â¦

(18)

(cid:124)
with ğ» ğ‘™ = ğ»
ğ‘™ , i.e. ğœ‘(ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘ğ‘˜) consists of the non-
redundant elements of the Kronecker product ğ‘§ğ‘˜ âŠ— ğ‘§ğ‘˜
and ^ğ‘¤ğ‘™ corresponds to the non-redundant elements of the
5. This quadratic choice is motivated by
unknown matrix ğ» ğ‘™
the successful control of our system using a model-based
linear quadratic (LQ) controller [18] and the fact that the
Q-function of LQ optimal control problems is quadratic [6].
For the policy evaluation step (10) we utilize least-
squares temporal-difference Q-learning (LSTDQ) [20] using
the ï¬xed-point objective [20, Section 5.2]. Consequently, ğ‘
}ï¸
tuples
are used in order to obtain a
least-squares solution of ^ğ‘¤ğ‘™ from (10). Due to its off-policy
characteristic, the measured samples can be re-used in each
iteration of the PI which renders the method data-efï¬cient.
Furthermore, the minimization in (11) requires

{ï¸
ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘¥ğ‘˜+1, ğ‘ğ‘˜, ğ‘(1)

ğ‘˜

(ï¸

ğ‘Ÿ

ğ‘(ğ‘–)
ğ‘˜ , ğ‘—

)ï¸

= ğ‘

(cid:124)
ğ‘˜ğœŒ(ğ‘– + ğ‘—) = ğ‘

(cid:124)
ğ‘˜

â¡

â£

((ğ‘– + ğ‘—)Î”ğ‘¡)2
(ğ‘– + ğ‘—)Î”ğ‘¡
1

â¤

â¦

ğœ• ^ğ‘„^ğœ‹ğ‘™
ğœ•ğ‘¢ğ‘˜

(ï¸

= 2

â„(ğ‘™)
ux ğ‘¥ğ‘˜ + â„(ğ‘™)

up ğ‘ğ‘˜ + â„(ğ‘™)

u1 + â„(ğ‘™)

uu ğ‘¢ğ‘˜

)ï¸ != 0.

(19)

= ğ‘

(cid:124)
ğ‘˜

â¡
1
0
â£
0

âŸ

2ğ‘–Î”ğ‘¡
1
0

â¤

â¦

(ğ‘–Î”ğ‘¡)2
ğ‘–Î”ğ‘¡
1

 â
=:ğ‘‡ (ğ‘–)

ğœŒ(ğ‘—) = ğ‘(ğ‘–)
ğ‘˜

(cid:124)

ğœŒ(ğ‘—),

This leads to the explicit6 policy improvement step (11)

^ğœ‹ğ‘™+1(ğ‘¥ğ‘˜, ğ‘ğ‘˜) = âˆ’

)ï¸âˆ’1 [ï¸

â„(ğ‘™)
uu

(ï¸

âŸ

]ï¸

up â„(ğ‘™)

u1

â„(ğ‘™)
ux â„(ğ‘™)
 â
ğ¿ğ‘™

â¤

â¦ ,

(20)

â¡

â£

ğ‘¥ğ‘˜
ğ‘ğ‘˜
1

(13)
âˆ€ğ‘–, ğ‘— âˆˆ N0. For any desired reference trajectory Â¯ğ‘Ÿğ‘˜, a
parameter vector ğ‘ğ‘˜ is to be found at each time step ğ‘˜,
such that ğ‘Ÿ(ğ‘ğ‘˜, ğ‘–), ğ‘– âˆˆ N0, is an approximation of Â¯ğ‘Ÿğ‘˜+ğ‘–. The
desired reference trajectory is assumed to be known during
runtime over a horizon of â„r âˆˆ N>0 timesteps. In each

ğ‘˜ depending on ğ‘¥ğ‘˜, ğ‘ğ‘˜ and a

which sets a motor current ğ¼ [ğ‘‘]
static offset.
Note 2. The choice of ^ğ‘„(Â·) in (18) extends the approximation
used in [6] by an offset term. This allows the controller to
learn a static offset compensation, i.e. if the weight of the
plate is slightly unbalanced.

4Compared to nonlinear FA, linear FA is easier to handle, usually requires
less training data and allows an analytical relation between the Q-function
and the optimal controller [22].

5Due to the symmetry of ğ» ğ‘™, the weights corresponding to the off-

diagonal elements of ğ» ğ‘™ are multiplied by 2.

6This analytic relation is a result of the quadratic penalty for ğ‘¢ in (4).

 
 
C. Training Procedure

The ofï¬‚ine least-squares policy iteration (LSPI) algorithm
[20] utilized in this work iteratively improves a policy by
using ofï¬‚ine recorded data tuples. These consist
to one
part of system data extracted through interaction with the
system, and to the other part of a generated training reference
trajectory.

1) System Data: System data is collected by human
interaction with the system. Manual control elements allow to
set target plate angles which are controlled with a suboptimal
controller. The system states can then be excited by varying
the plate angle and data tuples {ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘¥ğ‘˜+1} are collected.
2) Training Reference: The Q-function (18) represents the
cost of a chosen control ğ‘¢ğ‘˜ not only referring to the current
state ğ‘¥ğ‘˜, but also to a desired target trajectory Â¯ğ‘Ÿğ‘˜:ğ‘˜+â„râˆ’1
which is approximated by ğ‘ğ‘˜. Therefore, a training reference
trajectory is generated, which consists of a linear combina-
tion of multiple sine functions with varying frequencies. A
weighted LS approximation (17) is used to approximate the
training reference at each time step by means of a quadratic
polynomial (ğ‘›p = 3) with a discount factor of ğ›½ = 0.8 and
â„r = 10, resulting in the parameter vector ğ‘ğ‘˜. This parameter
vector is then propagated according to (13) to ï¬nd ğ‘(ğ‘–)
ğ‘˜ .

{ï¸
ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘¥ğ‘˜+1, ğ‘ğ‘˜, ğ‘(1)

The collected system data is smoothed (moving average of
length 5) and aggregated, together with the training reference
parameters, to the tuples
. We use
ğ‘ = 1200 data tuples for learning, which result with a
sampling time of Î”ğ‘¡ = 40 ms in 48 s of excitation data.
For numerical stability, we introduce a normalizing factor
ğ‘‰N = 10 which is applied to the state vector and parameter
vector (Â¯ğ‘¥ğ‘˜ = ğ‘‰Nğ‘¥ğ‘˜, Â¯ğ‘ğ‘˜ = ğ‘‰Nğ‘ğ‘˜) such that the values of the
system state and control input are in a similar range.

}ï¸

ğ‘˜

Our goal in this work is to track the position of the ball.
Additionally, we want the plate to preferably stay in a hor-
izontal position. Therefore, we set ğ‘„ = diag(800, 0, 400, 0)
to strongly penalize the deviation of the ball position (i.e.
ğ‘¥1) from the parametrized reference as well as a deviation
of the plate angle (i.e. ğ‘¥3) from its horizontal position ğ›¼ = 0
(cf. (4)). We set the discount factor to ğ›¾ = 0.9. For the initial
iteration, we set all weights ^ğ‘¤0 to 1.

Using the LSPI algorithm, where the policy evaluation is
done using a least-squares ï¬xed-point approximation [20,
Section 5.2], we obtain updated weights ^ğ‘¤ğ‘™ in each itera-
tion7 ğ‘™. The algorithm converges towards a ï¬xed-point and
is stopped when the stopping criterion

|| ^ğ‘¤ğ‘™ âˆ’ ^ğ‘¤ğ‘™âˆ’1||2 â‰¤ ğœ– = 1 Ã— 10âˆ’6

(21)

is fulï¬lled. The ï¬nal policy improvement step yields the
control matrix ğ¿ (cf. (20)), i.e. the ï¬nal control policy

ğ‘™

^ğ‘¤

^ğœ‹(ğ‘¥ğ‘˜, ğ‘ğ‘˜) = âˆ’ [ï¸€ğ¿x ğ¿ref ğ¿off

]ï¸€ [ï¸€ğ‘¥
(cid:124)
ğ‘˜ ğ‘

(cid:124)
ğ‘˜

1]ï¸€(cid:124)

(22)

after being re-normalized. All steps are summarized in Fig. 2.

ğœ‘, ^ğ‘¤0

false

(21)

true

system
excitation

ğ‘¢ğ‘˜

ball-on-plate
system

ğ‘¥ğ‘˜+1

ğ‘§âˆ’1

ğ‘¥ğ‘˜

training
reference

ğœŒ(ğ‘–), ğ›½, ğ‘›p, â„r

calculate ğ‘ğ‘˜
(see (17))

ğ‘ğ‘˜

Â¯ğ‘Ÿğ‘˜:ğ‘˜+â„râˆ’1

ğ‘‡ (1)
(see (13))

ğ‘(1)
ğ‘˜

store ğ‘ tuples

{ï¸
ğ‘¥ğ‘˜, ğ‘¢ğ‘˜, ğ‘¥ğ‘˜+1, ğ‘ğ‘˜, ğ‘(1)

ğ‘˜

}ï¸

preprocessing and normalization, set ğ‘™ = 0

training on ğ‘ tuples

^ğœ‹ğ‘™

policy evaluation (10)
^ğ‘¤ğ‘™+1 based on ğ‘ tuples

policy improvement (20)
calculate ^ğœ‹ğ‘™+1, set ğ‘™ = ğ‘™ + 1

^ğ‘¤ğ‘™+1

re-normalization

^ğœ‹(ğ‘¥ğ‘˜, ğ‘ğ‘˜)

Fig. 2.
controller ^ğœ‹(ğ‘¥ğ‘˜, ğ‘ğ‘˜) for the ball-on-plate system.

Training procedure to obtain the approximate optimal tracking

V. RESULTS

To validate the learned ADP controller, we compare
a learned controller ğ¿ADP with a model-based controller
ğ¿model. Since both dimensions are learned using the same
approach, we ï¬rstly focus on a comparison in one dimension.
The controllers are compared using a sine-like step function
as well as a composite validation trajectory. In the second
half of this section, we present the ability of two simultane-
ous controllers to follow a 2-dimensional trajectory.

We train a controller as described in Section IV-C. The
convergence of ^ğ‘¤ğ‘™ is depicted in Fig. 3. The resulting learned
control matrix is
ğ¿[ğ‘Œ ]

ADP = [64.8 32.3 145.3 16.2

].

âˆ’27.9 âˆ’36.9 âˆ’60.7
 â
âŸ
ğ¿ref

âˆ’0.1
âŸ  â  
ğ¿Off

 â
ğ¿x

âŸ

The model-based solution is calculated according to [6,
Theorem 2] which solves the optimization problem described

(23)

Â·104

1

0

âˆ’1

2

4

6

8

10

12

14

iteration ğ‘™

7The complexity of each iteration is dominated by the policy evaluation

step with ğ’ª(ğ‘›3

w + ğ‘›2

wğ‘ ).

Fig. 3. Estimated weights over all iterations of the LSPI algorithm.

 
 
in Problem 1 but uses a system model established speciï¬cally
for our system (cf. [18]). The resulting model-based control
matrix is given by

ğ¿[ğ‘Œ ]

model = [53.4 41.0 167.8 28.0

âŸ

 â
ğ¿x

âˆ’33.7 âˆ’41.0 âˆ’52.9
 â
âŸ
ğ¿ref

].

(24)

A. Setpoint Control: Step

In order to compare the model-free learned controller with
the model-based calculated controller, both controllers are to
follow a sine-like step function Â¯ğ‘Ÿ. Fig. 4a depicts the average
ball position when using a learned (blue) and a model-based
(yellow) setpoint controller with ğ‘›p = 1, over 11 repetitions.
The standard deviation is shown shaded. Both controllers
lag behind as they only have information about the current
setpoint. The learned controller shows a slightly faster step
response, which is reï¬‚ected by lower accumulated one-step
costs âˆ‘ï¸€ğ‘˜

ğœ…=0 ğ‘ (ğ‘¥ğœ…, ğ‘¢ğœ…, ğ‘Ÿ(ğ‘ğœ…, 0)) (see Fig. 4a).

B. Trajectory Control: Step

A comparison between a learned trajectory controller (red)
and a model-based trajectory controller (green), both with
ğ‘›p = 3, is depicted in Fig. 4b. Both trajectory controllers al-
low a signiï¬cantly better tracking of the reference trajectory
compared to the learned setpoint controller (blue), as they
receive information about the future course of the trajectory.
This leads to signiï¬cantly lower accumulated one-step costs,
as seen in Fig. 4b. Similarly to the setpoint controllers, the

learned trajectory controller shows lower accumulated costs
compared to the model-based trajectory controller.

C. Trajectory Control: Validation Trajectory

Fig. 4c compares a learned trajectory controller (ğ‘›p = 3)
with the learned setpoint controller (ğ‘›p = 1) on a validation
reference trajectory, which is composed of overlaid sines,
step functions and ramps. Again, an evidently better tracking
of the trajectory is possible with the trajectory controller
than with the setpoint controller, which leads to signiï¬cantly
lower accumulated one-step costs.

D. 2D Trajectory Control

âŸ

 â
ğ¿x

In order to use the ball-on-plate system to its full extent,
we apply two separately learned controllers, one for each
plate dimension respectively. Learning with the same param-
eters as for the ğ‘Œ -dimension, but with system data tuples for
the ğ‘‹-dimension, we receive the learned control law:
ğ¿[ğ‘‹]

ADP = [65.3 37.0 135.1 18.6

].

âˆ’28.8 âˆ’38.2 âˆ’61.2
âŸ
 â
ğ¿ref

2.2
âŸ â 
ğ¿Off

(25)
ğ¿[ğ‘‹]
off = 2.2 leads to a static offset current of âˆ’2.2 A,
since the plate exhibits a mass-imbalance which needs to be
compensated. For a model-based solution, this current would
have to be determined heuristically, as the mass-imbalance is
not described by the system model. Not using a static offset
current leads to an asymmetric behavior of the ball position,
as depicted in Fig 5. In comparison, a learned controller that
allows the learning of an offset current leads to a symmetric

reference trajectory

ğ‘›p = 1, ADP

ğ‘›p = 1, model

ğ‘›p = 3, ADP

ğ‘›p = 3, model

0.3

0.2

0.1

m
n

i

]

ğ‘Œ

[
ğ‘Ÿ
,
]

ğ‘Œ
ğ‘ 

[

)
Â·
(
ğ‘

0
=
ğ‘˜ ğœ…

âˆ‘ï¸€

0

0

2

1

0

0

200

400

time step ğ‘˜

Â·104

200

400

time step ğ‘˜

(a)

0.3

0.2

0.1

0

0

1

0.5

0

0

200

400

time step ğ‘˜

Â·104

200

400

time step ğ‘˜

(b)

0.2

0

âˆ’0.2

âˆ’0.4

4

2

0

0

200

Â·104

400
600
time step ğ‘˜

800

0

200

400
600
time step ğ‘˜

800

(c)

(a) Setpoint controllers, learned in blue, model-based in yellow, compared on a sine-step function. Top: Average ball position and standard
Fig. 4.
deviation over 11 repetitions. Bottom: Average accumulated one-step cost; (b) Trajectory controllers, learned (red), model-based (green), compared to a
setpoint controller (blue) on a sine-step function. Top: Average ball position and standard deviation over 18, 13 and 11, repetitions. Bottom: Average
accumulated one-step cost; (c) Learned trajectory controller (ğ‘›p = 3, red) and learned setpoint controller (ğ‘›p = 1, blue) on a validation trajectory. Top:
Average ball position and standard deviation over 4 repetitions. Bottom: Average accumulated one-step cost.

 
 
 
 
ğ‘Ÿ[ğ‘‹]

ğ‘ [ğ‘‹]
ğ‘›p=1

ğ‘ [ğ‘‹]
ğ‘›p=1, no offset

m
n
i

]

ğ‘‹
[
ğ‘Ÿ
,
]

ğ‘‹
ğ‘ 

[

0.4

0.2

0

âˆ’0.2

âˆ’0.4

0

100

200

300

400

500

time step ğ‘˜

Fig. 5. Comparison of a learned setpoint controller with base functions
that allow the learning of a static offset current (blue) versus a controller
with base functions that do not allow the learning of a static offset current
(brown).

ğ‘Ÿ

ğ‘ ğ‘›p=3,ADP

m
n

i

]

ğ‘Œ

[
ğ‘Ÿ
,
]

ğ‘Œ
ğ‘ 

[

0.4

0.2

0

âˆ’0.2

âˆ’0.4

âˆ’0.4 âˆ’0.2

0
0.2
ğ‘ [ğ‘‹], ğ‘Ÿ[ğ‘‹] in m

0.4

Fig. 6. Trajectory control of a rectangle. Average ball position and standard
deviation over 4 repetitions.

behavior of the ball position. Fig. 6 displays the tracking of
a 2-dimensional reference trajectory.

VI. CONCLUSION

In this paper, we presented the application of an ADP-
based learned trajectory tracking controller on a large-scale
ball-on-plate system. With less than one minute of measured
real data, our model-free ADP-based method successfully
learned an optimal tracking controller which allows the track-
ing of 2-dimensional reference trajectories and outperforms
its model-based counterpart. In addition, the implemented
reference trajectory approximation led to a faster accelerated
ball, a smaller static error and therefore to overall reduced
accumulated costs compared to setpoint controllers. In sum-
mary, the experimental results show that our ADP method is
suitable for real systems. It includes the autonomous learning
of an offset correction and avoids tedious modeling and
manual tuning. The resulting control law was proved to
be more cost-effective in a real scenario, beneï¬ting from
being trained with real measured data. Finally, due to the
ï¬‚exibility of function approximation, other basis functions
can be studied in the future in order to allow for even more
complex control tasks.

REFERENCES

[1] F. Lewis and D. Vrabie, â€œReinforcement learning and adaptive dy-
namic programming for feedback control,â€ IEEE Circuits and Syst.
Mag., vol. 9, no. 3, pp. 32â€“50, 2009.

[2] D. Wang, D. Liu, C. Mu, and Y. Zhang, â€œNeural network learning and
robust stabilization of nonlinear systems with dynamic uncertainties,â€
IEEE Trans. Neural Netw. Learn. Syst., vol. 29, no. 4, pp. 1342â€“1351,
2018.

[3] Y. Jiang and Z.-P. Jiang, â€œRobust adaptive dynamic programming and
feedback stabilization of nonlinear systems,â€ IEEE Trans. Neural Netw.
Learn. Syst., vol. 25, no. 5, pp. 882â€“893, 2014.

[4] S. Bhasin, R. Kamalapurkar, M. Johnson, K. G. Vamvoudakis, F. L.
Lewis, and W. E. Dixon, â€œA novel actorâ€“criticâ€“identiï¬er architecture
for approximate optimal control of uncertain nonlinear systems,â€
Automatica, vol. 49, no. 1, pp. 82â€“92, 2013.

[5] F. KÃ¶pf, J. Westermann, M. Flad, and S. Hohmann, â€œAdaptive optimal
control for reference tracking independent of exo-system dynamics,â€
Neurocomputing, no. 405, pp. 173â€“185, 2020.

[6] F. KÃ¶pf, S. Ramsteiner, L. Puccetti, M. Flad, and S. Hohmann, â€œAdap-
tive dynamic programming for model-free tracking of trajectories
with time-varying parameters,â€ Int. J. of Adaptive Control and Signal
Processing, vol. 34, no. 7, pp. 839â€“856, 2020.

[7] H. Modares and F. L. Lewis, â€œLinear quadratic tracking control
of partially-unknown continuous-time systems using reinforcement
learning,â€ IEEE Trans. Autom. Control, vol. 59, no. 11, pp. 3051â€“
3056, 2014.

[8] B. Luo, D. Liu, T. Huang, and D. Wang, â€œModel-free optimal tracking
control via critic-only q-learning,â€ IEEE Trans. Neural Netw. Learn.
Syst., vol. 27, no. 10, pp. 2134â€“2144, 2016.

[9] B. Kiumarsi, F. L. Lewis, H. Modares, A. Karimpour, and M.-B.
Naghibi-Sistani, â€œReinforcement-learning for optimal tracking control
of linear discrete-time systems with unknown dynamics,â€ Automatica,
vol. 50, no. 4, pp. 1167â€“1175, 2014.

[10] A. Y. Ng, H. J. Kim, M. I. Jordan, and S. Sastry, â€œAutonomous
helicopter ï¬‚ight via reinforcement learning,â€ NIPS, no. 16, 2004.
[11] J. Hwangbo, I. Sa, R. Siegwart, and M. Hutter, â€œControl of a quadrotor
with reinforcement learning,â€ IEEE Robotics and Autom. Lett., vol. 2,
no. 4, pp. 2096â€“2103, 2017.

[12] L. Puccetti, F. KÃ¶pf, C. Rathgeber, and S. Hohmann, â€œSpeed tracking
control using online reinforcement learning in a real car,â€ in IEEE
Int. Conf. on Control, Automation and Robotics (ICCAR), 2020, pp.
392â€“399.

[13] W. Shi, S. Song, and C. Wu, â€œHigh-level tracking of autonomous
underwater vehicles based on pseudo averaged q-learning,â€ IEEE Int.
Conf. on Syst., Man, and Cybern. (SMC), pp. 4138â€“4143, 2018.
[14] S. Awtar, C. Bernard, N. Boklund, A. Master, D. Ueda, and K. Craig,
â€œMechatronic design of a ball-on-plate balancing system,â€ Mechatron-
ics, vol. 12, no. 2, pp. 217 â€“ 228, 2002.

[15] F. C. Braescu, L. Ferariu, R. Gilca, and V. Bordianu, â€œBall on plate
balancing system for multi-discipline educational purposes,â€ Int. Conf.
on Syst. Theory, Control and Computing (ICSTCC), vol. 16, pp. 1â€“6,
2012.

[16] F. DuÅ¡ek, D. Honc, and K. R. Sharma, â€œModelling of ball and plate
system based on ï¬rst principle model and optimal control,â€ in 21st
Int. Conf. on Process Control (PC), 2017, pp. 216â€“221.

[17] A. Knuplez, A. Chowdhury, and R. Svecko, â€œModeling and control
design for the ball and plate system,â€ in IEEE Int. Conf. on Industrial
Technology, vol. 2, 2003, pp. 1064â€“1067.

[18] A. Kastner, J. Inga, T. Blauth, F. KÃ¶pf, M. Flad, and S. Hohmann,
â€œModel-based control of a large-scale ball-on-plate system with ex-
perimental validation,â€ IEEE Int. Conf. on Mechatronics (ICM), pp.
257â€“262, 2019.

[19] M. Moarref, M. Saadat, and G. Vossoughi, â€œMechatronic design and
position control of a novel ball and plate system,â€ 16th Mediterranean
Conf. on Control and Automation, 2008.

[20] M. G. Lagoudakis and R. Parr, â€œLeast-squares policy iteration,â€ J. of

Mach. Learning Research, no. 4, 2003.

[21] J. Li, T. Chai, F. L. Lewis, Z. Ding, and Y. Jiang, â€œOff-policy
interleaved q-learning: Optimal control for afï¬ne nonlinear discrete-
time systems,â€ IEEE Trans. Neural Netw. Learn. Syst., vol. 30, no. 5,
pp. 1308â€“1320, 2019.

[22] L. Busoniu, R. Babuska, B. de Schutter, and D. Ernst, Reinforcement
learning and dynamic programming using function approximators.
CRC Press, 2010.

