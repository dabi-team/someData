Adaptive Optimal Trajectory Tracking Control
Applied to a Large-Scale Ball-on-Plate System

Florian Köpf*, Sean Kille*, Jairo Inga, Sören Hohmann

1
2
0
2

n
a
J

5
2

]

Y
S
.
s
s
e
e
[

2
v
6
8
4
3
1
.
0
1
0
2
:
v
i
X
r
a

Abstract— While many theoretical works concerning Adap-
tive Dynamic Programming (ADP) have been proposed, appli-
cation results are scarce. Therefore, we design an ADP-based
optimal trajectory tracking controller and apply it to a large-
scale ball-on-plate system. Our proposed method incorporates
an approximated reference trajectory instead of using setpoint
tracking and allows to automatically compensate for constant
offset terms. Due to the off-policy characteristics of the algo-
rithm, the method requires only a small amount of measured
data to train the controller. Our experimental results show that
this tracking mechanism signiﬁcantly reduces the control cost
compared to setpoint controllers. Furthermore, a comparison
with a model-based optimal controller highlights the beneﬁts of
our model-free data-based ADP tracking controller, where no
system model and manual tuning are required but the controller
is tuned automatically using measured data.

I. INTRODUCTION

Model-free Adaptive Dynamic Programming (ADP) is a
promising approach to control dynamical systems whenever a
system model is unavailable, inaccurate or difﬁcult to achieve
[1]–[4]. While many control applications require to track
desired reference trajectories, this is non-trivial to incorporate
into the ADP formalism adequately [5], [6].

Assuming that the reference trajectory is generated directly
by an unknown command system (cf. [7]–[9]) limits the ﬂex-
ibility of the reference trajectory that can be commanded1.
Alternative approaches extend the system state by the desired
state [10]–[12] or the current and next desired state [13].
Shi et al. [13] take into account the desired position of
an underwater vehicle model at the current and next time
step and train their controller using pseudo-averaged Q-
learning in simulation. Although the learned (projected)
setpoint controller for an autonomous helicopter [10] and the
setpoint controller for a quadrotor [11] have been applied
to real systems, in [10] and [11] the training procedure is
based on simulations, thus requiring a model of the system
to be controlled. Puccetti et al. [12] use model-free ADP for
the speed tracking control of a real car, where a velocity
setpoint is incorporated into the state-action value function.
Nevertheless, these representations of the reference trajectory
have limited [10], [13] or no preview capabilities [11], [12],
which results in a controller that tends to lag behind.

Therefore, in our previous works, we have incorporated the
reference trajectory over a ﬁnite horizon into the Q-function

F. Köpf, S. Kille, J. Inga, and S. Hohmann are with the Institute of Control

Systems, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany
(e-mail: {ﬂorian.koepf, jairo.inga, soeren.hohmann}@kit.edu)

*These authors contributed equally to this work.
1If the reference trajectory does not result from this unknown command

system during training, these methods fail.

Fig. 1. Large-scale ball-on-plate system for ADP-based trajectory tracking
control.

[5] or used an approximated reference trajectory [6]. Instead
of assuming an unknown underlying command system, the
controller approximates an arbitrary reference trajectory in a
way that is compatible with ADP allowing ﬂexible reference
trajectories. However, [1]–[9], [13] only provide simulation
results and no application to a real system—an essential step
that is missing in order to validate ADP methods.

In this paper, we propose an ADP tracking controller
which incorporates an approximated reference trajectory and
apply it to a real large-scale ball-on-plate system (depicted in
Fig. 1). The ball-on-plate system is a widely used example
for benchmarking controllers. Existing controllers are either
fully model-based [14]–[18] or model-based with additional
fuzzy supervision [19]. Thus, our work is the ﬁrst application
of a model-free ADP-based controller to a ball-on-plate
system. Furthermore, instead of incorporating the reference
trajectory, existing controllers either perform no tracking of
the ball position at all [14], [15], [18] or simply consider the
current deviation from a setpoint causing a trajectory that
lags behind [16], [17], [19].

In contrast to existing controllers, our method does not
require a model of the ball-on-plate system as we train our
optimal tracking controller directly through a policy iteration
(PI) mechanism [20] using measured data from a real system.
This avoids tedious model design followed by manual tuning.
By using an off-policy algorithm, the measured data can be
re-used, reducing the effort to record training data2. Further-
more, instead of the widely-used setpoint tracking, our ADP
controller incorporates information on the course of the ref-
erence trajectory which allows predictive rather than reactive
behavior and avoids lagging behind. Our automatically tuned
controller is also able to learn static offsets to compensate

2In contrast, on-policy learning would require new data to be collected
after each policy improvement step and the estimates would be biased when
(indispensable) exploration noise is used [21].

© 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers
or lists, or reuse of any copyrighted component of this work in other works.

 
 
 
 
 
 
for asymmetries. In summary, our main contributions include
an ADP tracking controller which is

∙ data-efﬁcient as it works off-policy and uses a ﬂexible
and compact local approximation of arbitrary reference
trajectories that is compatible with ADP

∙ trained on a real system using measured data, requiring

neither system parameters nor manual tuning

∙ compared to a model-based and a setpoint controller.
The remainder of this paper is structured as follows: In
Section II, the system and problem description are given.
The theoretical background to our ADP tracking formalism
is given in Section III. In Section IV, we present our method.
Results are given in Section V, before we conclude the work.

𝑖 ∈ N0, where 𝑟(𝑝𝑘, 𝑖) is the desired ball position at time 𝑘+𝑖
(i.e. 𝑖 denotes the time step on the reference from the local
perspective at time 𝑘), 𝑝𝑘 ∈ Θ ⊆ R𝑛p a parameter vector
and 𝜌(𝑖) a basis function vector (cf. [6]). The following
problem formalizes that the ball position should follow a
desired reference trajectory while keeping other system states
and the control effort small.

Problem 1. Assume given basis functions 𝜌(𝑖) for ref-
erence trajectory approximation and measurement
tuples
{𝑥𝑖, 𝑢𝑖, 𝑥𝑖+1}, 𝑖 = 𝑘, . . . , 𝑘+𝑁 −1. Let the system dynamics
𝑓 (𝑥𝑘, 𝑢𝑘) be unknown. Find the control law 𝜋*(𝑥𝑘, 𝑝𝑘)
such that ∀𝑥𝑘, 𝑝𝑘 the control 𝑢*
𝑘 = 𝜋*(𝑥𝑘, 𝑝𝑘) minimizes
the objective function

II. SYSTEM AND PROBLEM DESCRIPTION
In the following, the ball-on-plate system that is used as
an application example for our ADP tracking method and
the problem formulation are given.

𝐽𝑘 =

∞
∑︁

𝑖=0

𝛾𝑖

⎛

⎡

⎜
⎜
⎝

⎢
⎢
⎣

𝑥1,𝑘+𝑖 − 𝑟(𝑝𝑘, 𝑖)
𝑥2,𝑘+𝑖
𝑥3,𝑘+𝑖
𝑥4,𝑘+𝑖
⎞

(cid:124)
⎤

⎥
⎥
⎦

⎡

⎢
⎢
⎣

𝑄

𝑥1,𝑘+𝑖 − 𝑟(𝑝𝑘, 𝑖)
𝑥2,𝑘+𝑖
𝑥3,𝑘+𝑖
𝑥4,𝑘+𝑖

⎤

⎥
⎥
⎦

A. Ball-on-Plate System

The system used in this work is a custom-built large-
scale ball-on-plate system (see Fig. 1). Its centerpiece is a
1 m2 square plate with a mass of 16.3 kg. The plate can
be tilted in two dimensions (denoted by 𝑋 and 𝑌 ) that are
orthogonal to each other. Each dimension is actuated by its
own designated motor. The plate angles (𝛼[𝑋], 𝛼[𝑌 ]) and
angular velocities (𝜔[𝑋], 𝜔[𝑌 ]) are measured every 10 ms.
A ball with a mass of 0.042 kg and a radius of 0.02 m is
located on the plate. Its position in plate-ﬁxed coordinates
is tracked via a camera, providing an updated ball position
(𝑠[𝑋], 𝑠[𝑌 ]) and ball velocity (𝑣[𝑋], 𝑣[𝑌 ]) every Δ𝑡 = 40 ms.
For a detailed description of the system architecture and the
hardware, see [18]3. Thus, the resulting system states

[︁

]︁(cid:124)

𝑥[𝑑]

𝑘 =

𝑘 𝛼[𝑑]

𝑘 𝑣[𝑑]
𝑠[𝑑]
are deﬁned for both dimensions 𝑑 ∈ 𝒟 = {𝑋, 𝑌 }. The
system input 𝑢[𝑑]
is the current for the motor driver
controller.

𝑘 = 𝐼 [𝑑]

𝑘 𝜔[𝑑]

(1)

𝑘

𝑘

As the two dimensions 𝑋 and 𝑌 only slightly depend on
each other, they are usually controlled separately (see [14],
[15], [17], [18]). Since the controllers for the two dimensions
are trained in the same way, the index 𝑑 is omitted in the
following for the sake of readability.

(cid:124)
𝑘+𝑖𝑅𝑢𝑘+𝑖
+ 𝑢

⎟
⎟
⎠

=:

∞
∑︁

𝑖=0

𝛾𝑖𝑐(𝑥𝑘+𝑖, 𝑢𝑘+𝑖, 𝑟(𝑝𝑘, 𝑖)),

(4)
where 𝛾 ∈ (0, 1) denotes a discount factor, 𝑄 is assumed to
be positive semi-deﬁnite and 𝑅 positive deﬁnite.

III. ADP TRACKING THEORY

In this section, we brieﬂy summarize the theoretical back-
ground on our ADP tracking formalism related to Problem 1.

Lemma 1. Deﬁne

(cid:124)

𝑝(𝑖)
𝑘

= 𝑝

(cid:124)
𝑘𝑇 (𝑖),

where 𝑇 (𝑖) is chosen such that

(︁

𝑟

𝑝(𝑖)
𝑘 , 𝑗

)︁

= 𝑟(𝑝𝑘, 𝑖 + 𝑗),

∀𝑖, 𝑗 ∈ N0

(5)

(6)

holds and
𝑄*(𝑥𝑘, 𝑢𝑘, 𝑝𝑘) = 𝑐(𝑥𝑘, 𝑢𝑘, 𝑟(𝑝𝑘, 0))
𝑥𝑘+𝑖, 𝜋*(︁

∞
∑︁

𝛾𝑖𝑐

+

(︁

)︁

𝑥𝑘+𝑖, 𝑝(𝑖)
𝑘

)︁

, 𝑟(𝑝𝑘, 𝑖)

𝑖=1

= 𝑐(𝑥𝑘, 𝑢𝑘, 𝑟(𝑝𝑘, 0))
𝑥𝑘+1, 𝜋*(︁
+ 𝛾𝑄*(︁

𝑥𝑘+1, 𝑝(1)
𝑘

)︁

, 𝑝(1)
𝑘

)︁

.

B. Problem Formulation

Consider the discrete-time controllable system dynamics

Then,

𝑥𝑘+1 = 𝑓 (𝑥𝑘, 𝑢𝑘)
(2)
where 𝑘 ∈ N0 describes the discrete time step, 𝑥𝑘 ∈ 𝒳 ⊆
R𝑛 the system state (1), 𝑢𝑘 ∈ 𝒰 ⊆ R𝑚 the control input
𝐼 [𝑑]
and 𝑓 is unknown. From Section II-A, the system order
𝑘
𝑛 = 4 and number of control inputs 𝑚 = 1 follows for each
dimension in 𝒟. At each time step 𝑘, an approximation of
the desired ball position trajectory is denoted by

𝑟(𝑝𝑘, 𝑖) = 𝑝

(cid:124)
𝑘𝜌(𝑖),

(3)

3Note that we use a heavier plate and a different ball in the present work.

𝑢*
𝑘 = arg min

𝑢𝑘
is a solution to Problem 1.

𝑄*(𝑥𝑘, 𝑢𝑘, 𝑝𝑘)

Proof. See [6, Lemma 1].
Note 1. 𝑄*(𝑥𝑘, 𝑢𝑘, 𝑝𝑘) is the accumulated discounted cost
if the system is in state 𝑥𝑘, the control 𝑢𝑘 is applied at
time step 𝑘 and the optimal control 𝜋*(·) thereafter. Using
the shifted reference trajectory approximation 𝑝(𝑖)
(cf. (5))
𝑘
ensures that the Q-function 𝑄*(·) is compatible with ADP
(cf. [6, Note 1]).

(7)

(8)

As the optimal Q-function 𝑄*(𝑥𝑘, 𝑢𝑘, 𝑝𝑘) is unknown,
linear function approximation (FA) (cf. [1]–[9], [12], [20],
[22]) is commonly used4. Thus, suppose ^𝑄(𝑥𝑘, 𝑢𝑘, 𝑝𝑘) =
^𝑤(cid:124)𝜑(𝑥𝑘, 𝑢𝑘, 𝑝𝑘), where ^𝑤 ∈ R𝑛w is a weight vector to be
adapted and 𝜑(·) ∈ R𝑛w a vector of activation functions. A
common approach in order to tune ^𝑤 is given by a PI (see
e.g. [1], [20], [22]). In this iterative procedure, each iteration
𝑙 consists of two steps. The policy evaluation step estimates
the Q-function

^𝑄^𝜋𝑙 (𝑥𝑘, 𝑢𝑘, 𝑝𝑘) = ^𝑤

(cid:124)
𝑙 𝜑(𝑥𝑘, 𝑢𝑘, 𝑝𝑘)

(9)

of the current policy ^𝜋𝑙, i.e. adapts ^𝑤𝑙 in order to solve
^𝑄^𝜋𝑙 (𝑥𝑘, 𝑢𝑘, 𝑝𝑘) = 𝑐(𝑥𝑘, 𝑢𝑘, 𝑟(𝑝𝑘, 0))
(︁

.
(10)
The policy improvement step then greedily updates the policy
^𝜋𝑙+1 based on ^𝑄^𝜋𝑙 :

𝑥𝑘+1, ^𝜋𝑙

𝑥𝑘+1, 𝑝(1)
𝑘

+ 𝛾 ^𝑄^𝜋𝑙

, 𝑝(1)
𝑘

)︁

)︁

(︁

^𝜋𝑙+1(𝑥𝑘, 𝑝𝑘) = arg min

𝑢𝑘

^𝑄^𝜋𝑙 (𝑥𝑘, 𝑢𝑘, 𝑝𝑘).

(11)

Convergence results of a Q-function-based PI are given in
e.g. [20, Theorem 3.1], [8, Theorem 1].

IV. ADP TRACKING ON THE BALL-ON-PLATE SYSTEM

The ADP tracking formalism introduced in Section III is
applied to the ball-on-plate system described in Section II-A.

A. Quadratic Polynomial Reference Approximation

We choose the reference trajectory to be approximated by

means of a quadratic polynomial

𝑟(𝑝𝑘, 𝑖) = 𝑝

(cid:124)

𝑘𝜌(𝑖) = 𝑝𝑘,2(𝑖Δ𝑡)2 + 𝑝𝑘,1𝑖Δ𝑡 + 𝑝𝑘,0,

(12)

with the basis functions 𝜌(𝑖) = [︀(𝑖Δ𝑡)2 𝑖Δ𝑡 1]︀(cid:124)
parameter vector 𝑝𝑘 = [︀𝑝𝑘,2 𝑝𝑘,1 𝑝𝑘,0
the sampling time.

and the
, where Δ𝑡 denotes

]︀(cid:124)

The transformation needed to obtain the propagated ver-

sion 𝑝(𝑖)

𝑘 of 𝑝𝑘 according to (3) and (6) is given by

time step, 𝑝𝑘 is determined by a weighted least-squares (LS)
regression. Therefore, we deﬁne

¯𝑟𝑘:𝑘+ℎr−1 = [︀¯𝑟𝑘

¯𝑟𝑘+1
𝑊 p = diag(1, 𝛽, . . . , 𝛽ℎr−1),

. . .

¯𝑟𝑘+ℎr−1

]︀ ,

𝜌0:ℎr−1 = [︀𝜌(0) 𝜌(1)

. . . 𝜌(ℎr − 1)]︀(cid:124)

(14)

(15)

(16)

,

with 𝑊 p being a weighting matrix with the discount factor
𝛽 ≤ 1, so that future time steps in the horizon are less
important for the ﬁtting process than early time steps. The
parameter for the reference trajectory approximation is then
calculated with the weighted LS regression according to [6]
and given by

(cid:124)
𝑘 = ¯𝑟𝑘:𝑘+ℎr−1𝑊 p𝜌0,ℎr−1
𝑝
B. Q-Function Approximation

(︁

(cid:124)
0,ℎr−1𝑊 p𝜌0,ℎr−1
𝜌

)︁−1

. (17)

⎤

⎡

(cid:124) ⎡

^𝑄^𝜋𝑙 (𝑥𝑘, 𝑢𝑘, 𝑝𝑘) =

The approximated Q-function (9) is chosen as
up ℎ(𝑙)
ux ℎ(𝑙)
xp ℎ(𝑙)
xx ℎ(𝑙)
pp ℎ(𝑙)
px ℎ(𝑙)
1p ℎ(𝑙)
1x ℎ(𝑙)
(cid:124)
𝑙 𝜑(𝑥𝑘, 𝑢𝑘, 𝑝𝑘),

ℎ(𝑙)
uu ℎ(𝑙)
xu ℎ(𝑙)
ℎ(𝑙)
ℎ(𝑙)
pu ℎ(𝑙)
1u ℎ(𝑙)
ℎ(𝑙)
(cid:124)
𝑘𝐻 𝑙𝑧𝑘 = ^𝑤

𝑢𝑘
𝑥𝑘
𝑝𝑘
1

⎥
⎥
⎥
⎦

⎢
⎢
⎢
⎣

= 𝑧

⎢
⎢
⎣

⎥
⎥
⎦

⎤

x1

p1

11

u1

⎡

⎢
⎢
⎣

𝑢𝑘
𝑥𝑘
𝑝𝑘
1

⎤

⎥
⎥
⎦

(18)

(cid:124)
with 𝐻 𝑙 = 𝐻
𝑙 , i.e. 𝜑(𝑥𝑘, 𝑢𝑘, 𝑝𝑘) consists of the non-
redundant elements of the Kronecker product 𝑧𝑘 ⊗ 𝑧𝑘
and ^𝑤𝑙 corresponds to the non-redundant elements of the
5. This quadratic choice is motivated by
unknown matrix 𝐻 𝑙
the successful control of our system using a model-based
linear quadratic (LQ) controller [18] and the fact that the
Q-function of LQ optimal control problems is quadratic [6].
For the policy evaluation step (10) we utilize least-
squares temporal-difference Q-learning (LSTDQ) [20] using
the ﬁxed-point objective [20, Section 5.2]. Consequently, 𝑁
}︁
tuples
are used in order to obtain a
least-squares solution of ^𝑤𝑙 from (10). Due to its off-policy
characteristic, the measured samples can be re-used in each
iteration of the PI which renders the method data-efﬁcient.
Furthermore, the minimization in (11) requires

{︁
𝑥𝑘, 𝑢𝑘, 𝑥𝑘+1, 𝑝𝑘, 𝑝(1)

𝑘

(︁

𝑟

𝑝(𝑖)
𝑘 , 𝑗

)︁

= 𝑝

(cid:124)
𝑘𝜌(𝑖 + 𝑗) = 𝑝

(cid:124)
𝑘

⎡

⎣

((𝑖 + 𝑗)Δ𝑡)2
(𝑖 + 𝑗)Δ𝑡
1

⎤

⎦

𝜕 ^𝑄^𝜋𝑙
𝜕𝑢𝑘

(︁

= 2

ℎ(𝑙)
ux 𝑥𝑘 + ℎ(𝑙)

up 𝑝𝑘 + ℎ(𝑙)

u1 + ℎ(𝑙)

uu 𝑢𝑘

)︁ != 0.

(19)

= 𝑝

(cid:124)
𝑘

⎡
1
0
⎣
0

⏟

2𝑖Δ𝑡
1
0

⎤

⎦

(𝑖Δ𝑡)2
𝑖Δ𝑡
1

 ⏞
=:𝑇 (𝑖)

𝜌(𝑗) = 𝑝(𝑖)
𝑘

(cid:124)

𝜌(𝑗),

This leads to the explicit6 policy improvement step (11)

^𝜋𝑙+1(𝑥𝑘, 𝑝𝑘) = −

)︁−1 [︁

ℎ(𝑙)
uu

(︁

⏟

]︁

up ℎ(𝑙)

u1

ℎ(𝑙)
ux ℎ(𝑙)
 ⏞
𝐿𝑙

⎤

⎦ ,

(20)

⎡

⎣

𝑥𝑘
𝑝𝑘
1

(13)
∀𝑖, 𝑗 ∈ N0. For any desired reference trajectory ¯𝑟𝑘, a
parameter vector 𝑝𝑘 is to be found at each time step 𝑘,
such that 𝑟(𝑝𝑘, 𝑖), 𝑖 ∈ N0, is an approximation of ¯𝑟𝑘+𝑖. The
desired reference trajectory is assumed to be known during
runtime over a horizon of ℎr ∈ N>0 timesteps. In each

𝑘 depending on 𝑥𝑘, 𝑝𝑘 and a

which sets a motor current 𝐼 [𝑑]
static offset.
Note 2. The choice of ^𝑄(·) in (18) extends the approximation
used in [6] by an offset term. This allows the controller to
learn a static offset compensation, i.e. if the weight of the
plate is slightly unbalanced.

4Compared to nonlinear FA, linear FA is easier to handle, usually requires
less training data and allows an analytical relation between the Q-function
and the optimal controller [22].

5Due to the symmetry of 𝐻 𝑙, the weights corresponding to the off-

diagonal elements of 𝐻 𝑙 are multiplied by 2.

6This analytic relation is a result of the quadratic penalty for 𝑢 in (4).

 
 
C. Training Procedure

The ofﬂine least-squares policy iteration (LSPI) algorithm
[20] utilized in this work iteratively improves a policy by
using ofﬂine recorded data tuples. These consist
to one
part of system data extracted through interaction with the
system, and to the other part of a generated training reference
trajectory.

1) System Data: System data is collected by human
interaction with the system. Manual control elements allow to
set target plate angles which are controlled with a suboptimal
controller. The system states can then be excited by varying
the plate angle and data tuples {𝑥𝑘, 𝑢𝑘, 𝑥𝑘+1} are collected.
2) Training Reference: The Q-function (18) represents the
cost of a chosen control 𝑢𝑘 not only referring to the current
state 𝑥𝑘, but also to a desired target trajectory ¯𝑟𝑘:𝑘+ℎr−1
which is approximated by 𝑝𝑘. Therefore, a training reference
trajectory is generated, which consists of a linear combina-
tion of multiple sine functions with varying frequencies. A
weighted LS approximation (17) is used to approximate the
training reference at each time step by means of a quadratic
polynomial (𝑛p = 3) with a discount factor of 𝛽 = 0.8 and
ℎr = 10, resulting in the parameter vector 𝑝𝑘. This parameter
vector is then propagated according to (13) to ﬁnd 𝑝(𝑖)
𝑘 .

{︁
𝑥𝑘, 𝑢𝑘, 𝑥𝑘+1, 𝑝𝑘, 𝑝(1)

The collected system data is smoothed (moving average of
length 5) and aggregated, together with the training reference
parameters, to the tuples
. We use
𝑁 = 1200 data tuples for learning, which result with a
sampling time of Δ𝑡 = 40 ms in 48 s of excitation data.
For numerical stability, we introduce a normalizing factor
𝑉N = 10 which is applied to the state vector and parameter
vector (¯𝑥𝑘 = 𝑉N𝑥𝑘, ¯𝑝𝑘 = 𝑉N𝑝𝑘) such that the values of the
system state and control input are in a similar range.

}︁

𝑘

Our goal in this work is to track the position of the ball.
Additionally, we want the plate to preferably stay in a hor-
izontal position. Therefore, we set 𝑄 = diag(800, 0, 400, 0)
to strongly penalize the deviation of the ball position (i.e.
𝑥1) from the parametrized reference as well as a deviation
of the plate angle (i.e. 𝑥3) from its horizontal position 𝛼 = 0
(cf. (4)). We set the discount factor to 𝛾 = 0.9. For the initial
iteration, we set all weights ^𝑤0 to 1.

Using the LSPI algorithm, where the policy evaluation is
done using a least-squares ﬁxed-point approximation [20,
Section 5.2], we obtain updated weights ^𝑤𝑙 in each itera-
tion7 𝑙. The algorithm converges towards a ﬁxed-point and
is stopped when the stopping criterion

|| ^𝑤𝑙 − ^𝑤𝑙−1||2 ≤ 𝜖 = 1 × 10−6

(21)

is fulﬁlled. The ﬁnal policy improvement step yields the
control matrix 𝐿 (cf. (20)), i.e. the ﬁnal control policy

𝑙

^𝑤

^𝜋(𝑥𝑘, 𝑝𝑘) = − [︀𝐿x 𝐿ref 𝐿off

]︀ [︀𝑥
(cid:124)
𝑘 𝑝

(cid:124)
𝑘

1]︀(cid:124)

(22)

after being re-normalized. All steps are summarized in Fig. 2.

𝜑, ^𝑤0

false

(21)

true

system
excitation

𝑢𝑘

ball-on-plate
system

𝑥𝑘+1

𝑧−1

𝑥𝑘

training
reference

𝜌(𝑖), 𝛽, 𝑛p, ℎr

calculate 𝑝𝑘
(see (17))

𝑝𝑘

¯𝑟𝑘:𝑘+ℎr−1

𝑇 (1)
(see (13))

𝑝(1)
𝑘

store 𝑁 tuples

{︁
𝑥𝑘, 𝑢𝑘, 𝑥𝑘+1, 𝑝𝑘, 𝑝(1)

𝑘

}︁

preprocessing and normalization, set 𝑙 = 0

training on 𝑁 tuples

^𝜋𝑙

policy evaluation (10)
^𝑤𝑙+1 based on 𝑁 tuples

policy improvement (20)
calculate ^𝜋𝑙+1, set 𝑙 = 𝑙 + 1

^𝑤𝑙+1

re-normalization

^𝜋(𝑥𝑘, 𝑝𝑘)

Fig. 2.
controller ^𝜋(𝑥𝑘, 𝑝𝑘) for the ball-on-plate system.

Training procedure to obtain the approximate optimal tracking

V. RESULTS

To validate the learned ADP controller, we compare
a learned controller 𝐿ADP with a model-based controller
𝐿model. Since both dimensions are learned using the same
approach, we ﬁrstly focus on a comparison in one dimension.
The controllers are compared using a sine-like step function
as well as a composite validation trajectory. In the second
half of this section, we present the ability of two simultane-
ous controllers to follow a 2-dimensional trajectory.

We train a controller as described in Section IV-C. The
convergence of ^𝑤𝑙 is depicted in Fig. 3. The resulting learned
control matrix is
𝐿[𝑌 ]

ADP = [64.8 32.3 145.3 16.2

].

−27.9 −36.9 −60.7
 ⏞
⏟
𝐿ref

−0.1
⏟  ⏞  
𝐿Off

 ⏞
𝐿x

⏟

The model-based solution is calculated according to [6,
Theorem 2] which solves the optimization problem described

(23)

·104

1

0

−1

2

4

6

8

10

12

14

iteration 𝑙

7The complexity of each iteration is dominated by the policy evaluation

step with 𝒪(𝑛3

w + 𝑛2

w𝑁 ).

Fig. 3. Estimated weights over all iterations of the LSPI algorithm.

 
 
in Problem 1 but uses a system model established speciﬁcally
for our system (cf. [18]). The resulting model-based control
matrix is given by

𝐿[𝑌 ]

model = [53.4 41.0 167.8 28.0

⏟

 ⏞
𝐿x

−33.7 −41.0 −52.9
 ⏞
⏟
𝐿ref

].

(24)

A. Setpoint Control: Step

In order to compare the model-free learned controller with
the model-based calculated controller, both controllers are to
follow a sine-like step function ¯𝑟. Fig. 4a depicts the average
ball position when using a learned (blue) and a model-based
(yellow) setpoint controller with 𝑛p = 1, over 11 repetitions.
The standard deviation is shown shaded. Both controllers
lag behind as they only have information about the current
setpoint. The learned controller shows a slightly faster step
response, which is reﬂected by lower accumulated one-step
costs ∑︀𝑘

𝜅=0 𝑐 (𝑥𝜅, 𝑢𝜅, 𝑟(𝑝𝜅, 0)) (see Fig. 4a).

B. Trajectory Control: Step

A comparison between a learned trajectory controller (red)
and a model-based trajectory controller (green), both with
𝑛p = 3, is depicted in Fig. 4b. Both trajectory controllers al-
low a signiﬁcantly better tracking of the reference trajectory
compared to the learned setpoint controller (blue), as they
receive information about the future course of the trajectory.
This leads to signiﬁcantly lower accumulated one-step costs,
as seen in Fig. 4b. Similarly to the setpoint controllers, the

learned trajectory controller shows lower accumulated costs
compared to the model-based trajectory controller.

C. Trajectory Control: Validation Trajectory

Fig. 4c compares a learned trajectory controller (𝑛p = 3)
with the learned setpoint controller (𝑛p = 1) on a validation
reference trajectory, which is composed of overlaid sines,
step functions and ramps. Again, an evidently better tracking
of the trajectory is possible with the trajectory controller
than with the setpoint controller, which leads to signiﬁcantly
lower accumulated one-step costs.

D. 2D Trajectory Control

⏟

 ⏞
𝐿x

In order to use the ball-on-plate system to its full extent,
we apply two separately learned controllers, one for each
plate dimension respectively. Learning with the same param-
eters as for the 𝑌 -dimension, but with system data tuples for
the 𝑋-dimension, we receive the learned control law:
𝐿[𝑋]

ADP = [65.3 37.0 135.1 18.6

].

−28.8 −38.2 −61.2
⏟
 ⏞
𝐿ref

2.2
⏟ ⏞ 
𝐿Off

(25)
𝐿[𝑋]
off = 2.2 leads to a static offset current of −2.2 A,
since the plate exhibits a mass-imbalance which needs to be
compensated. For a model-based solution, this current would
have to be determined heuristically, as the mass-imbalance is
not described by the system model. Not using a static offset
current leads to an asymmetric behavior of the ball position,
as depicted in Fig 5. In comparison, a learned controller that
allows the learning of an offset current leads to a symmetric

reference trajectory

𝑛p = 1, ADP

𝑛p = 1, model

𝑛p = 3, ADP

𝑛p = 3, model

0.3

0.2

0.1

m
n

i

]

𝑌

[
𝑟
,
]

𝑌
𝑠

[

)
·
(
𝑐

0
=
𝑘 𝜅

∑︀

0

0

2

1

0

0

200

400

time step 𝑘

·104

200

400

time step 𝑘

(a)

0.3

0.2

0.1

0

0

1

0.5

0

0

200

400

time step 𝑘

·104

200

400

time step 𝑘

(b)

0.2

0

−0.2

−0.4

4

2

0

0

200

·104

400
600
time step 𝑘

800

0

200

400
600
time step 𝑘

800

(c)

(a) Setpoint controllers, learned in blue, model-based in yellow, compared on a sine-step function. Top: Average ball position and standard
Fig. 4.
deviation over 11 repetitions. Bottom: Average accumulated one-step cost; (b) Trajectory controllers, learned (red), model-based (green), compared to a
setpoint controller (blue) on a sine-step function. Top: Average ball position and standard deviation over 18, 13 and 11, repetitions. Bottom: Average
accumulated one-step cost; (c) Learned trajectory controller (𝑛p = 3, red) and learned setpoint controller (𝑛p = 1, blue) on a validation trajectory. Top:
Average ball position and standard deviation over 4 repetitions. Bottom: Average accumulated one-step cost.

 
 
 
 
𝑟[𝑋]

𝑠[𝑋]
𝑛p=1

𝑠[𝑋]
𝑛p=1, no offset

m
n
i

]

𝑋
[
𝑟
,
]

𝑋
𝑠

[

0.4

0.2

0

−0.2

−0.4

0

100

200

300

400

500

time step 𝑘

Fig. 5. Comparison of a learned setpoint controller with base functions
that allow the learning of a static offset current (blue) versus a controller
with base functions that do not allow the learning of a static offset current
(brown).

𝑟

𝑠𝑛p=3,ADP

m
n

i

]

𝑌

[
𝑟
,
]

𝑌
𝑠

[

0.4

0.2

0

−0.2

−0.4

−0.4 −0.2

0
0.2
𝑠[𝑋], 𝑟[𝑋] in m

0.4

Fig. 6. Trajectory control of a rectangle. Average ball position and standard
deviation over 4 repetitions.

behavior of the ball position. Fig. 6 displays the tracking of
a 2-dimensional reference trajectory.

VI. CONCLUSION

In this paper, we presented the application of an ADP-
based learned trajectory tracking controller on a large-scale
ball-on-plate system. With less than one minute of measured
real data, our model-free ADP-based method successfully
learned an optimal tracking controller which allows the track-
ing of 2-dimensional reference trajectories and outperforms
its model-based counterpart. In addition, the implemented
reference trajectory approximation led to a faster accelerated
ball, a smaller static error and therefore to overall reduced
accumulated costs compared to setpoint controllers. In sum-
mary, the experimental results show that our ADP method is
suitable for real systems. It includes the autonomous learning
of an offset correction and avoids tedious modeling and
manual tuning. The resulting control law was proved to
be more cost-effective in a real scenario, beneﬁting from
being trained with real measured data. Finally, due to the
ﬂexibility of function approximation, other basis functions
can be studied in the future in order to allow for even more
complex control tasks.

REFERENCES

[1] F. Lewis and D. Vrabie, “Reinforcement learning and adaptive dy-
namic programming for feedback control,” IEEE Circuits and Syst.
Mag., vol. 9, no. 3, pp. 32–50, 2009.

[2] D. Wang, D. Liu, C. Mu, and Y. Zhang, “Neural network learning and
robust stabilization of nonlinear systems with dynamic uncertainties,”
IEEE Trans. Neural Netw. Learn. Syst., vol. 29, no. 4, pp. 1342–1351,
2018.

[3] Y. Jiang and Z.-P. Jiang, “Robust adaptive dynamic programming and
feedback stabilization of nonlinear systems,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 25, no. 5, pp. 882–893, 2014.

[4] S. Bhasin, R. Kamalapurkar, M. Johnson, K. G. Vamvoudakis, F. L.
Lewis, and W. E. Dixon, “A novel actor–critic–identiﬁer architecture
for approximate optimal control of uncertain nonlinear systems,”
Automatica, vol. 49, no. 1, pp. 82–92, 2013.

[5] F. Köpf, J. Westermann, M. Flad, and S. Hohmann, “Adaptive optimal
control for reference tracking independent of exo-system dynamics,”
Neurocomputing, no. 405, pp. 173–185, 2020.

[6] F. Köpf, S. Ramsteiner, L. Puccetti, M. Flad, and S. Hohmann, “Adap-
tive dynamic programming for model-free tracking of trajectories
with time-varying parameters,” Int. J. of Adaptive Control and Signal
Processing, vol. 34, no. 7, pp. 839–856, 2020.

[7] H. Modares and F. L. Lewis, “Linear quadratic tracking control
of partially-unknown continuous-time systems using reinforcement
learning,” IEEE Trans. Autom. Control, vol. 59, no. 11, pp. 3051–
3056, 2014.

[8] B. Luo, D. Liu, T. Huang, and D. Wang, “Model-free optimal tracking
control via critic-only q-learning,” IEEE Trans. Neural Netw. Learn.
Syst., vol. 27, no. 10, pp. 2134–2144, 2016.

[9] B. Kiumarsi, F. L. Lewis, H. Modares, A. Karimpour, and M.-B.
Naghibi-Sistani, “Reinforcement-learning for optimal tracking control
of linear discrete-time systems with unknown dynamics,” Automatica,
vol. 50, no. 4, pp. 1167–1175, 2014.

[10] A. Y. Ng, H. J. Kim, M. I. Jordan, and S. Sastry, “Autonomous
helicopter ﬂight via reinforcement learning,” NIPS, no. 16, 2004.
[11] J. Hwangbo, I. Sa, R. Siegwart, and M. Hutter, “Control of a quadrotor
with reinforcement learning,” IEEE Robotics and Autom. Lett., vol. 2,
no. 4, pp. 2096–2103, 2017.

[12] L. Puccetti, F. Köpf, C. Rathgeber, and S. Hohmann, “Speed tracking
control using online reinforcement learning in a real car,” in IEEE
Int. Conf. on Control, Automation and Robotics (ICCAR), 2020, pp.
392–399.

[13] W. Shi, S. Song, and C. Wu, “High-level tracking of autonomous
underwater vehicles based on pseudo averaged q-learning,” IEEE Int.
Conf. on Syst., Man, and Cybern. (SMC), pp. 4138–4143, 2018.
[14] S. Awtar, C. Bernard, N. Boklund, A. Master, D. Ueda, and K. Craig,
“Mechatronic design of a ball-on-plate balancing system,” Mechatron-
ics, vol. 12, no. 2, pp. 217 – 228, 2002.

[15] F. C. Braescu, L. Ferariu, R. Gilca, and V. Bordianu, “Ball on plate
balancing system for multi-discipline educational purposes,” Int. Conf.
on Syst. Theory, Control and Computing (ICSTCC), vol. 16, pp. 1–6,
2012.

[16] F. Dušek, D. Honc, and K. R. Sharma, “Modelling of ball and plate
system based on ﬁrst principle model and optimal control,” in 21st
Int. Conf. on Process Control (PC), 2017, pp. 216–221.

[17] A. Knuplez, A. Chowdhury, and R. Svecko, “Modeling and control
design for the ball and plate system,” in IEEE Int. Conf. on Industrial
Technology, vol. 2, 2003, pp. 1064–1067.

[18] A. Kastner, J. Inga, T. Blauth, F. Köpf, M. Flad, and S. Hohmann,
“Model-based control of a large-scale ball-on-plate system with ex-
perimental validation,” IEEE Int. Conf. on Mechatronics (ICM), pp.
257–262, 2019.

[19] M. Moarref, M. Saadat, and G. Vossoughi, “Mechatronic design and
position control of a novel ball and plate system,” 16th Mediterranean
Conf. on Control and Automation, 2008.

[20] M. G. Lagoudakis and R. Parr, “Least-squares policy iteration,” J. of

Mach. Learning Research, no. 4, 2003.

[21] J. Li, T. Chai, F. L. Lewis, Z. Ding, and Y. Jiang, “Off-policy
interleaved q-learning: Optimal control for afﬁne nonlinear discrete-
time systems,” IEEE Trans. Neural Netw. Learn. Syst., vol. 30, no. 5,
pp. 1308–1320, 2019.

[22] L. Busoniu, R. Babuska, B. de Schutter, and D. Ernst, Reinforcement
learning and dynamic programming using function approximators.
CRC Press, 2010.

