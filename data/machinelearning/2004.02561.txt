0
2
0
2

r
p
A
4
1

]

G
L
.
s
c
[

2
v
1
6
5
2
0
.
4
0
0
2
:
v
i
X
r
a

A High-Performance Implementation of Bayesian
Matrix Factorization with Limited
Communication

Tom Vander Aa, Roel Wuyts, Wilfried Verachtert
ExaScience Life Lab at imec, Leuven, Belgium

Xiangju Qin, Paul Blomstedt, Samuel Kaski
Helsinki Institute for Information Technology (HIIT),
Department of Computer Science,
Aalto University, Finland

tom.vanderaa@imec.be

April 15, 2020

Abstract

Matrix factorization is a very common machine learning technique in
recommender systems. Bayesian Matrix Factorization (BMF) algorithms
would be attractive because of their ability to quantify uncertainty in
their predictions and avoid over-ﬁtting, combined with high prediction
accuracy. However, they have not been widely used on large-scale data
because of their prohibitive computational cost. In recent work, eﬀorts
have been made to reduce the cost, both by improving the scalability of
the BMF algorithm as well as its implementation, but so far mainly sepa-
rately. In this paper we show that the state-of-the-art of both approaches
to scalability can be combined. We combine the recent highly-scalable
Posterior Propagation algorithm for BMF, which parallelizes computa-
tion of blocks of the matrix, with a distributed BMF implementation that
users asynchronous communication within each block. We show that the
combination of the two methods gives substantial improvements in the
scalability of BMF on web-scale datasets, when the goal is to reduce the
wall-clock time.

1

Introduction

Matrix Factorization (MF) is a core machine learning technique for applications
of collaborative ﬁltering, such as recommender systems or drug discovery, where

1

 
 
 
 
 
 
a data matrix R is factorized into a product of two matrices, such that R ≈
UV(cid:62). The main task in such applications is to predict unobserved elements of
a partially observed data matrix. In recommender systems, the elements of R
are often ratings given by users to items, while in drug discovery they typically
represent bioactivities between chemical compounds and protein targets or cell
lines.

Bayesian Matrix Factorization (BMF) [2, 13], formulates the matrix factor-
ization task as a probabilistic model, with Bayesian inference conducted on the
unknown matrices U and V. Advantages often associated with BMF include
robustness to over-ﬁtting and improved predictive accuracy, as well as ﬂexible
utilization of prior knowledge and side-data. Finally, for application domains
such as drug discovery, the ability of the Bayesian approach to quantify uncer-
tainty in predictions is of crucial importance [9].

Despite the appeal and many advantages of BMF, scaling up the posterior
inference for industry-scale problems has proven diﬃcult. Scaling up to this level
requires both data and computations to be distributed over many workers, and
so far only very few distributed implementations of BMF have been presented
In [16], a high-performance computing implementation of
in the literature.
BMF using Gibbs sampling for distributed systems was proposed. The authors
considered three diﬀerent distributed programming models: Message Passing
Interface (MPI), Global Address Space Programming Interface (GASPI) and
ExaSHARK. In a diﬀerent line of work, [1] proposed to use a distributed version
of the minibatch-based Stochastic Gradient Langevin Dynamics algorithm for
posterior inference in BMF models. While a key factor in devising eﬃcient
distributed solutions is to be able to minimize communication between worker
nodes, both of the above solutions require some degree of communication in
between iterations.

A recent promising proposal, aiming at minimizing communication and thus
reaching a solution in a faster way, is to use a hierarchical embarrassingly parallel
MCMC strategy [12]. This technique, called BMF with Posterior Propagation
(BMF-PP), enhances regular embarrassingly parallel MCMC (e.g.
[10, 17]),
which does not work well for matrix factorization [12] because of identiﬁability
issues. BMF-PP introduces communication at predetermined limited phases in
the algorithm to make the problem identiﬁable, eﬀectively building one model
for all the parallelized data subsets, while in previous works multiple indepen-
dent solutions were found per subset.

The current paper is based on a realization that the approaches of [16] and
[12] are compatible, and in fact synergistic. BMF-PP will be able to parallelize
a massive matrix but will be the more accurate the larger the parallelized blocks
are. Now replacing the earlier serial processing of the blocks by the distributed
BMF in [16] will allow making the blocks larger up to the scalability limit of
the distributed BMF, and hence decrease the wall-clock time by engaging more
processors.

The main contributions of this work are:

• We combine both approaches, allowing for parallelization both at the al-

2

gorithmic and at the implementation level.

• We analyze what is the best way to subdivide the original matrix into
subsets, taking into account both compute performance and model quality.

• We examine several web-scale datasets and show that datasets with diﬀer-
ent properties (like the number of non-zero items per row) require diﬀerent
parallelization strategies.

The rest of this paper is organized as follows. In Section 2 we present the
existing Posterior Propagation algorithm and distributed BMF implementation
and we explain how to combine both. Section 3 is the main section of this paper,
where we document the experimental setup and used dataset, we compare with
related MF methods, and present the results both from a machine learning point
of view, as from a high-performance compute point of view. In Section 4 we
draw conclusions and propose future work.

2 Distributed Bayesian Matrix Factorization with

Posterior Propagation

In this section, we ﬁrst brieﬂy review the BMF model and then describe the
individual aspects of distributed computation and Posterior Propagation and
how to combine them.

2.1 Bayesian Matrix Factorization

In matrix factorization, a (typically very sparsely observed) data matrix R ∈
RN ×D is factorized into a product of two matrices U ∈ RN ×K = (u1, . . . , uN )(cid:62)
and V = (v1, . . . , vD)(cid:62) ∈ RD×K. In the context of recommender systems, R is
a rating matrix, with N the number of users and D the number of rated items.

In Bayesian matrix factorization [2, 13], the data are modelled as

p(R|U, V) =

N
(cid:89)

D
(cid:89)

n=1

d=1

(cid:2)N (cid:0)rnd|u(cid:62)

n vd, τ −1(cid:1)(cid:3)Ind

(1)

where Ind denotes an indicator which equals 1 if the element rnd is observed
and 0 otherwise, and τ denotes the residual noise precision. The two parameter
matrices U and V are assigned Gaussian priors. Our goal is then to compute
the joint posterior density p(U, V|R) ∝ p(U)p(V)p(R|U, V), conditional on
the observed data. Posterior inference is typically done using Gibbs sampling,
see [13] for details.

2.2 Bayesian Matrix Factorization with Posterior Propa-

gation

In the Posterior Propagation (PP) framework [12], we start by partitioning R
with respect to both rows and columns into I × J subsets R(i,j), i = 1, . . . , I,

3

j = 1, . . . , J. The parameter matrices U and V are correspondingly partitioned
into I and J submatrices, respectively. The basic idea of PP is to process
each subset using a hierarchical embarrassingly parallel MCMC scheme in three
phases, where the posteriors from each phase are propagated forwards and used
as priors in the following phase, thus introducing dependencies between the
subsets. The approach proceeds as follows (for an illustration, see Figure 1):

Phase (a): Joint inference for submatrices (U(1), V(1)), conditional on data
subset R(1,1):

(cid:16)

U(1), V(1)|R(1,1)(cid:17)

p

(cid:16)

U(1)(cid:17)

p

(cid:16)

V(1)(cid:17)

p

(cid:16)

R(1,1)|U(1), V(1)(cid:17)

.

∝ p

Phase (b): Joint inference in parallel for submatrices (U(i), V(1)), i = 2, . . . , I,
and (U(1), V(j)), j = 2, . . . , J, conditional on data subsets which share columns
or rows with R(1,1), and using posterior marginals from phase (a) as priors:
R(i,1)|U(i), V(1)(cid:17)
,
(cid:16)
R(1,j)|U(1), V(j)(cid:17)

U(i), V(1)|R(1,1), R(i,1)(cid:17)
U(1), V(j)|R(1,1), R(1,j)(cid:17)

V(1)|R(1,1)(cid:17)
U(1)|R(1,1)(cid:17)

U(i)(cid:17)
V(j)(cid:17)

p
(cid:16)

∝ p

∝ p

(cid:16)

(cid:16)

(cid:16)

(cid:16)

(cid:16)

(cid:16)

p

p

p

p

p

.

Phase (c): Joint inference in parallel for submatrices (U(i), V(j)), i = 2, . . . , I,
j = 2, . . . , J, conditional on the remaining data subsets, and using posterior
marginals propagated from phase (b) as priors:
(cid:16)
U(i), V(j)|R(1,1), R(i,1), R(1,j), R(i,j)(cid:17)
(cid:16)

(cid:16)

(cid:16)

p

V(j)|R(1,1), R(1,j)(cid:17)

p

R(i,j)|U(i), V(j)(cid:17)

.

U(i)|R(1,1), R(i,1)(cid:17)

p

∝ p

Finally, the aggregated posterior is obtained by combining the posteriors
obtained in phases (a)-(c) and dividing away the multiply-counted propagated
posterior marginals; see [12] for details.

2.3 Distributed Bayesian Matrix Factorization

In [16] a distributed parallel implementation of BMF is proposed. In that paper
an implementation the BMF algorithm [13] is proposed that distributes the rows
and columns of R across diﬀerent nodes of a supercomputing system.

Since Gibbs sampling is used, rows of U and rows of V are independent and
can be sampled in parallel, on diﬀerent nodes. However, there is a dependency
between samples of U and V. The communication pattern between U and V
is shown in Figure 2.

The main contribution of this implementation is how to distribute U and V
to make sure the computational load is distributed as equally as possible and
the amount of data communication is minimized. The authors of [16] optimize
the distributions by analysing the sparsity structure of R.

4

V(cid:62)

V(1) V(2) V(3) V(4)

U

U(1)

U(2)

U(3)

(a)

(b)

(b)

(b)

(b)

(c)

(c)

(c)

R

(b)

(c)

(c)

(c)

Figure 1: Illustration of Posterior Propagation (PP) for a data matrix R parti-
tioned into 3 × 4 subsets. Subset inferences for the matrices U and V proceed in
three successive phases, with posteriors obtained in one phase being propagated
as priors to the next one. The letters (a), (b), (c) in the matrix R refer to the
phase in which the particular data subset is processed. Within each phase, the
subsets are processed in parallel with no communication. Figure adapted from
[12].

V T

U

R

Figure 2: Communication pattern in the distributed implementation of BMF

5

Table 1: Statistics about benchmark datasets.

Name
Scale
# Rows
# Columns
# Ratings
Sparsity
Ratings/Row
#Rows/#Cols
K
rows/sec (×1000)
ratings/sec (×106)

Movielens [5] Netﬂix [4] Yahoo [3] Amazon [8]
1-5
21.2M
9.7M
82.5M
2.4M
4
2.2
10
911
3.8

1-5
480.2K
17.8K
100.5M
85
209
27.0
100
15
5.5

0-100
1.0M
625.0K
262.8M
2.4K
263
1.6
100
27
5.2

1-5
138.5K
27.3K
20.0M
189
144
5.1
10
416
70

As presented in the paper, distributed BMF provides a reasonable speed-
up for compute systems up to 128 nodes. After this, speed-up in the strong
scaling case is limited by the increase in communication, while at the same
computations for each node decrease.

2.4 Distributed Bayesian Matrix Factorization with Pos-

terior Propagation

By combining the distributed BMF implementation with PP, we can exploit
the parallelism at both levels. The diﬀerent subsets of R in the parallel phases
of PP are independent and can thus be computed on in parallel on diﬀerent
resources.
Inside each subset we exploit parallelism by using the distributed
BMF implementation with MPI communication.

3 Experiments

In this section we evaluate the distributed BMF implementation with Posterior
Propagation (D-BMF+PP) and competitors for benchmark datasets. We ﬁrst
have a look at the datasets we will use for the experiments. Next, we compare
our proposed D-BMF+PP implementation with other related matrix factoriza-
tion methods in terms of accuracy and runtime, with a special emphasis on
the beneﬁts of Bayesian methods. Finally we look at the strong scaling of D-
BMF+PP and what is a good way to split the matrix in to blocks for Posterior
Propagation.

3.1 Datasets

Table 1 shows the diﬀerent webscale datasets we used for our experiments.
The table shows we have a wide diversity of properties:
from the relatively
small Movielens dataset with 20M ratings, to the 262M ratings Yahoo dataset.
Sparsity is expressed as the ratio of the total number of elements in the matrix

6

Table 2: RMSE of diﬀerent matrix factorization methods on benchmark
datasets.

Dataset
Movielens
Netﬂix
Yahoo
Amazon

BMF+PP NOMAD FPSGD
0.77
0.92
21.78
1.15

0.77
0.91
21.91
1.20

0.76
0.90
21.79
1.13

(#rows × #columns) to the number of ﬁlled-in ratings (#ratings). This metric
also varies: especially the Amazon dataset is very sparsely ﬁlled.

The scale of the ratings (either 1 to 5 or 0 to 100) is only important when

looking at prediction error values, which we will do in the next section.

The ﬁnal three lines of the table are not data properties, but rather properties
of the matrix factorization methods. K is the number of latent dimensions
used, which is diﬀerent per dataset but chosen to be common to all matrix
factorization methods. The last two lines are two compute performance metrics,
which will be discussed in Section 3.4.

3.2 Related Methods

In this section we compare the proposed BMF+PP method to other matrix
factorization (MF) methods, in terms of accuracy (Root-Mean-Square Error or
RMSE on the test set), and in terms of compute performance (wall-clock time).
Stochastic gradient decent (SGD [15]), Coordinate Gradient Descent (CGD
[18]) and Alternating Least Squares (ALS [14]) are the three most-used algo-
rithms for non-Bayesian MF.

CGD- and ALS-based algorithms update along one dimension at a time
while the other dimension of the matrix remains ﬁxed. Many variants of ALS
and CGD exist that improve the convergence [6], or the parallelization degree
[11], or are optimized for non-sparse rating matrices [7]. In this paper we limit
ourselves to comparing to methods, which divide the rating matrix into blocks,
which is necessary to support very large matrices. Of the previous methods,
this includes the SGD-based ones.

FPSGD [15] is a very eﬃcient SGD-based library for matrix factorization on
multi-cores. While FPSGD is a single-machine implementation, which limits its
capability to solve large-scale problems, it has outperformed all other methods
on a machine with up to 16 cores. NOMAD [19] extends the idea of block
partitioning, adding the capability to release a portion of a block to another
thread before its full completion. It performs similarly to FPSGD on a single
machine, and can scale out to a 64-node HPC cluster.

Table 2 compares the RMSE values on the test sets of the four selected
datasets. For all methods, we used the K provided in Table 1. For competitor
methods, we used the default hyperparameters suggested by the authors for the
diﬀerent data sets. As already concluded in [12], BMF with Posterior Propa-

7

Table 3: Wall-clock time (hh:mm) of diﬀerent matrix factorization methods on
benchmark datasets, running on single-node system with 16 cores.

Dataset
Movielens
Netﬂix
Yahoo
Amazon

BMF+PP BMF NOMAD FPSGD
0:09
1:04
2:41
2:28

0:14
4:39
12:22
13:02

0:08
0:08
0:10
0:40

0:07
2:02
2:13
4:15

gation results in equally good RMSE compared to the original BMF. We also
see from the table that on average the Bayesian method produces only slightly
better results, in terms of RMSE. However we do know that Bayesian methods
have signiﬁcant statistical advantages [9] that stimulate their use.

For many applications, this advantage outweighs the much higher compu-
tational cost. Indeed, as can be seem from Table 3, BMF is signiﬁcantly more
expensive, than NOMAD and FPSGD, even when taking in to account the
speed-ups thanks to using PP. NOMAD is the fastest method, thanks to the
aforementioned improvements compared to FPSGD.

3.3 Block Size

The combined method will achieve some of the parallelization with the PP
algorithm, and some with the distributed BMF within each block. We next
compare the performance as the share between the two is varied by varying the
block size. We ﬁnd that blocks should be approximately square, meaning the
number of rows and columns inside the each block should be more or less equal.
This implies the number of blocks across the rows of the R matrix will be less
if the R matrix has fewer rows and vice versa.

Figure 3 shows optimizing the block size is crucial to achieve a good speed up
with BMF+PP and avoid compromising the quality of the model (as measured
with the RMSE). The block size in the ﬁgure, listed as I ×J, means the R matrix
is split in I blocks (with equal amount of rows) in the vertical direction and J
blocks (with equal amount of columns) in the horizontal direction. The ﬁgure
explores diﬀerent block sizes for the Netﬂix dataset, which has signiﬁcantly more
rows than columns (27×) as can be seen from Table 1. This is reﬂected by the
fact that the data point with the smallest bubble area (20 × 3) provides the best
trade-oﬀ between wall-clock time and RMSE.

We stipulate that the reason is the underlying trade-oﬀ between the amount
of information in the block and the amount of compute per block. Both the
amount of information and the amount of compute can optimized (maximized
and minimized respectively) by making the blocks approximately squared, since
both are proportionate to the ratio of the area versus the circumference of the
block.

We will come back to this trade-oﬀ when we look at performance of BMF+PP

when scaling to multiple nodes for the diﬀerent datasets in Section 3.4.

8

Figure 3: Block size exploration: RMSE on test set and wall-clock time (hh:mm)
for diﬀerent block size on Netﬂix. Each bubble is the result for a diﬀerent block
size indicated as the number of blocks across the rows and the columns inside
the bubble. The size of bubbles is an indication for the aspect ratio of the blocks
inside PP. Smaller bubbles indicate the blocks are more square.

9

3x39x916x164x29x320x332x1640x200:001:002:003:004:005:006:007:000.8950.9000.9050.9100.9150.9200.9250.9300.935Wall-Clock Time (h:mm)RMSE3.4 Scaling

In this section we look at how the added parallelization of Posterior Propagation
increases the strong scaling behavior of BMF+PP. Strong scaling means we look
at the speed-up obtained by increasing the amount of compute nodes, while
keeping the dataset constant.

The graphs in this section are displayed with a logarithmic scale on both
axes. This has the eﬀect that linear scaling (i.e. doubling of the amount of re-
sources results in a halving of the runtime) is shown as a straight line . Addition-
ally, we indicate Pareto optimal solutions with a blue dot. For these solutions
one cannot reduce the execution time without increasing the resources.

We performed experiments with diﬀerent block sizes, indicated with diﬀerent
colors in the graphs. For example, the yellow line labeled 16 × 8 in Figure
Figure 4 means we performed PP with 16 blocks for the rows of R and 8 blocks
for the columns of R.

We have explored scaling on a system with up to 128K nodes, and use this

multi-node parallelism in two ways:

1. Parallelisation inside a block using the distributed version of BMF [16].

2. Parallelism across blocks. In phase (b) we can exploit parallelism across
the blocks in the ﬁrst row and ﬁrst column, using up to I + J nodes where
I and J is the number of blocks in the vertical, respectively horizontal
direction of the matrix. In phase (c) we can use up to I × J nodes.

General Trends When looking at the four graphs (Figure 4 and 5), we ob-
serve that for the same amount of nodes using more blocks for posterior propa-
gation, increases the wall-clock time. The main reason is that we do signiﬁcantly
more compute for this solution, because we take the same amount of samples
for each sub-block. This means for a partitioning of 32 × 32 we take 1024× more
samples than 1 × 1.

On the other hand, we do get signiﬁcant speedup, with up to 68× improve-
ment for the Netﬂix dataset. However the amount of resources we need for this
is clearly prohibitively large (16K nodes). To reduce execution time, we plan
to investigate reducing the amount of Gibbs samples and the eﬀect this has on
RMSE.

Netﬂix and Yahoo Runs on the Netﬂix and Yahoo datasets (Figure 4) have
been performed with 100 latent dimensions (K=100). Since computational in-
tensity (the amount of compute per row/column of R), is O(K 3), the amount
of compute versus compute for these experiments is high, leading to good scal-
ability for a single block (1 × 1) for Netﬂix, and almost linear scalability up to
16 or even 64 nodes. The Yahoo dataset is too large to run with a 1 × 1 block
size, but we see a similar trend for 2 × 2.

10

Figure 4: Strong scaling results for the Netﬂix (top) and Yahoo (bottom)
datasets. X-axis indicates the amount of compute resources (#nodes). Y-axis
is wall-clock time. The diﬀerent series correspond to diﬀerent block sizes.

11

12481632641282565121K2K4K8K16KNumber of Nodes1 hour1 day1 weekExecution Timepareto1x12x24x45x316x816x1630x1532x3240x2012481632641282565121K2K4K8K16KNumber of Nodes1 hour1 day1 weekExecution Timepareto2x24x45x316x816x1630x1532x3240x20Figure 5: Strong scaling results for the Movielens (top) and Amazon (bottom)
datasets. X-axis indicates the amount of compute resources (#nodes). Y-axis
is wall-clock time. The diﬀerent series correspond to diﬀerent block sizes.

12

12481632641282565121K2K4K8K16KNumber of Nodes30 min1 hour1 dayExecution Timepareto1x12x24x45x316x816x1630x1532x3240x2012481632641282565121K2K4K8K16KNumber of Nodes1 hour1 day1 weekExecution Timepareto1x12x24x45x316x816x1630x1532x3240x20Movielens and Amazon For Movielens and Amazon (Figure 5), we used
K = 10 for our experiments. This implies a much smaller amount of compute
for the same communication cost inside a block. Hence scaling for 1×1 is mostly
ﬂat. We do get signiﬁcant performance improvements with more but smaller
blocks where the Amazon dataset running on 2048 nodes, with 32 × 32 blocks
is 20× faster than the best block size (1 × 1) on a single node.

When the nodes in the experiment align with the parallelism across of blocks
(either I + J or I × J as explained above), we see a signiﬁcant drop in the run
time. For example for the Amazon dataset on 32 × 32 blocks going from 1024
to 2048 nodes.

4 Conclusions and Further Work

In this paper we presented a scalable, distributed implementation of Bayesian
Probabilistic Matrix Factorization, using asynchronous communication. We
evaluated both the machine learning and the compute performance on several
web-scale datasets.

While we do get signiﬁcant speed-ups, resource requirements for these speed-
ups are extermely large. Hence, in future work, we plan to investigate the eﬀect
of the following measures on execution time, resource usage and RMSE:

• Reduce the number of samples for sub-blocks in phase (b) and phase (c).

• Use the posterior from phase (b) blocks to make predictions for phase (c)

blocks.

• Use the GASPI implementation of [16], instead of the MPI implementa-

tion.

Acknowledgments

The research leading to these results has received funding from the European
Unions Horizon2020 research and innovation programme under the EPEEC
project, grant agreement No 801051. The work was also supported by the
Academy of Finland (Flagship programme: Finnish Center for Artiﬁcial Intel-
ligence, FCAI; grants 319264, 292334). We acknowledge PRACE for awarding
us access to Hazel Hen at GCS@HLRS, Germany.

References

[1] Sungjin Ahn, Anoop Korattikara, Nathan Liu, Suju Rajan, and Max
Large-scale distributed Bayesian matrix factorization using
Welling.
stochastic gradient MCMC.
In Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pages
9–18, 2015.

13

[2] Anirban Bhattacharya and David B Dunson. Sparse Bayesian inﬁnite factor

models. Biometrika, 98:291–306, 2011.

[3] Gideon Dror, Noam Koenigstein, Yehuda Koren, and Markus Weimer. The
Yahoo! music dataset and KDD-Cup11. In Gideon Dror, Yehuda Koren,
and Markus Weimer, editors, Proceedings of KDD Cup 2011, volume 18 of
Proceedings of Machine Learning Research, pages 3–18. PMLR, 2012.

[4] Carlos A. Gomez-Uribe and Neil Hunt. The netﬂix recommender system:
Algorithms, business value, and innovation. ACM Trans. Manage. Inf.
Syst., 6(4):13:1–13:19, December 2015.

[5] F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: His-
tory and context. ACM Trans. Interact. Intell. Syst., 5(4):19:1–19:19, De-
cember 2015.

[6] Cho-Jui Hsieh and Inderjit S. Dhillon. Fast coordinate descent meth-
ods with variable selection for non-negative matrix factorization. In ACM
SIGKDD International Conference on Knowledge Discovery and Data Min-
ing (KDD), 2011.

[7] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization techniques for

recommender systems. Computer, 42(8):30–37, Aug 2009.

[8] Stanford SNAP Lab. Web data: Amazon reviews.

https://snap.

stanford.edu/data/web-Amazon.html. [Online; accessed 18-Aug-2018].

[9] Caroline Labelle, Anne Marinier, and Sbastien Lemieux. Enhancing the
drug discovery process: Bayesian inference for the analysis and comparison
of doseresponse experiments. Bioinformatics, 35(14):i464–i473, 07 2019.

[10] Willie Neiswanger, Chong Wang, and Eric P. Xing. Asymptotically exact,
embarrassingly parallel MCMC. In Proceedings of the Thirtieth Conference
on Uncertainty in Artiﬁcial Intelligence, UAI’14, pages 623–632, Arlington,
Virginia, United States, 2014. AUAI Press.

[11] Istv´an Pil´aszy, D´avid Zibriczky, and Domonkos Tikk. Fast als-based matrix
factorization for explicit and implicit feedback datasets. In Proceedings of
the fourth ACM conference on Recommender systems, pages 71–78. ACM,
2010.

[12] Xiangju Qin, Paul Blomstedt, Eemeli Lepp¨aaho, Pekka Parviainen, and
Samuel Kaski. Distributed bayesian matrix factorization with limited com-
munication. Machine Learning, 108(10):1805–1830, 2019.

[13] Ruslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix
factorization using Markov chain Monte Carlo. In Proceedings of the 25th
International Conference on Machine Learning, pages 880–887. ACM, 2008.

14

[14] Wei Tan, Liangliang Cao, and Liana Fong. Faster and cheaper: Paralleliz-
ing large-scale matrix factorization on gpus.
In Proceedings of the 25th
ACM International Symposium on High-Performance Parallel and Dis-
tributed Computing (HPDC’16), pages 219–230. ACM, 2016.

[15] C. Teﬂioudi, F. Makari, and R. Gemulla. Distributed matrix completion.
In Proceedings of the 2012 IEEE 12th International Conference on Data
Mining (ICDM ’12), pages 655–664, 2012.

[16] Tom Vander Aa, Imen Chakroun, and Tom Haber. Distributed Bayesian
probabilistic matrix factorization. Procedia Computer Science, 108:1030
– 1039, 2017. International Conference on Computational Science, ICCS
2017.

[17] Xiangyu Wang, Fangjian Guo, Katherine A Heller, and David B Dunson.
Parallelizing MCMC with random partition trees.
In Advances in Neu-
ral Information Processing Systems 28, pages 451–459. Curran Associates,
Inc., 2015.

[18] Hsiang-Fu Yu, Cho-Jui Hsieh, Si Si, and Inderjit Dhillon. Scalable coordi-
nate descent approaches to parallel matrix factorization for recommender
systems. In Proceedings of the 2012 IEEE 12th International Conference on
Data Mining (ICDM ’12), pages 765–774. IEEE Computer Society, 2012.

[19] Hyokun Yun, Hsiang-Fu Yu, Cho-Jui Hsieh, S. V. N. Vishwanathan, and
Inderjit Dhillon. Nomad: Non-locking, stochastic multi-machine algorithm
for asynchronous and decentralized matrix completion. Proc. VLDB En-
dow., 7(11):975–986, 2014.

15

