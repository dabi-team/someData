1
2
0
2

t
c
O
1
1

]
L
C
.
s
c
[

1
v
3
2
4
5
0
.
0
1
1
2
:
v
i
X
r
a

USING DOCUMENT SIMILARITY METHODS TO CREATE
PARALLEL DATASETS FOR CODE TRANSLATION

Mayank Agarwal1, Kartik Talamadupula1 ∗, Fernando Martinez2, Stephanie Houde1,
Michael Muller1, John Richards1, Steven I. Ross1, Justin D. Weisz1
1IBM Research AI, USA, 2 IBM Argentina, Argentina

ABSTRACT

Translating source code from one programming language to another is a critical,
time-consuming task in modernizing legacy applications and codebases. Recent
work in this space has drawn inspiration from the software naturalness hypothesis
by applying natural language processing techniques towards automating the code
translation task. However, due to the paucity of parallel data in this domain, super-
vised techniques have only been applied to a limited set of popular programming
languages. To bypass this limitation, unsupervised neural machine translation
techniques have been proposed to learn code translation using only monolingual
corpora. In this work, we propose to use document similarity methods to create
noisy parallel datasets of code, thus enabling supervised techniques to be applied
for automated code translation without having to rely on the availability or expen-
sive curation of parallel code datasets. We explore the noise tolerance of models
trained on such automatically-created datasets and show that these models per-
form comparably to models trained on ground truth for reasonable levels of noise.
Finally, we exhibit the practical utility of the proposed method by creating parallel
datasets for languages beyond the ones explored in prior work, thus expanding the
set of programming languages for automated code translation.

1

INTRODUCTION

As the pace of software development increases and the famous adage “software is eating the
world” (Andreessen, 2011) is borne out, there is a corresponding increase in the amount of source
code and number of software artefacts in active use for which support has lapsed. At the same time,
the number of software professionals and programmers who can support and understand such code
is unable to keep pace with the rate at which it is produced. This problem, while important when
it comes to relatively modern programming languages (such as Java and Python), becomes even
more pressing when it come to legacy languages (like COBOL) that mission-critical applications
and systems are written in (Charette, 2020). In recent years, there have been multiple instances of
organizations struggling to maintain their legacy systems and making considerable investments to
upgrade them. In 2012 the Commonwealth Bank of Australia upgraded its core banking platform
originally written in COBOL: this ultimately took 5 years and more than 1 Billion AUD to com-
plete (Irrera, 2017). During the COVID-19 pandemic, software systems implemented in COBOL
slowed down the release of US unemployment stimulus checks (Kelly, 2020), leaving governments
scrambling to ﬁnd COBOL experts who were already hard to come by. A recent study by the United
States Government Accountability Ofﬁce (Walsh, 2021) has identiﬁed 65 critical federal legacy sys-
tems in need of urgent modernization. Some of these systems are over 50 years old, and cost millions
of dollars annually to operate and maintain.

Parallel to these developments are recent efforts at the intersection of software engineering, machine
learning (ML), and natural language processing (NLP), which have posited the naturalness hypoth-
esis of software (Hindle et al., 2016). The hypothesis states that “...Software is a form of human
communication; software corpora have similar statistical properties to natural language corpora;
and these properties can be exploited to build better software engineering tools” (Allamanis et al.,
2018). This hypothesis has been used to extend breakthroughs and advances from various NLP

∗Correspondance to mayank.agarwal@ibm.com, krtalamad@us.ibm.com

1

 
 
 
 
 
 
sub-ﬁelds to software engineering tasks such as code translation. Prior works in the code trans-
lation domain have proposed the application of statistical, supervised, and unsupervised machine
translation techniques to learn code translation models to varying degrees of success.

A key limitation of a majority of the proposed code translation approaches, however, is the lack of
availability of parallel data for training. Unlike natural language, where a piece of text is verbatim
translated in multiple languages – legal documents, parliamentary proceedings in multilingual soci-
eties – code is rarely implemented as is in multiple languages; thus making it hard to create parallel
datasets. A few limited datasets – such as Java ↔ C# (Nguyen et al., 2013) and AVATAR for Java
↔ Python (Ahmad et al., 2021b) – are currently available. However, these are extremely limited
in the number of programming language they cover, and manually curating a dataset for a speciﬁc
use-case is impractical. To bypass this limitation, unsupervised techniques have been applied to the
code translation task. Unsupervised techniques come with their own limitations however; and often,
supervised techniques can outperform them when the source and target corpora are from different
domains, the source and target languages use different scripts, and on low-resource language pairs,
among other concerns (Kim et al., 2020; Marchisio et al., 2020).

It is for this reason that in this work, we focus on one of the main blockers impeding the application
of supervised techniques to code translation: the availability of parallel corpora and datasets. Specif-
ically, we propose to utilize document similarity methods to create parallel source code datasets that
are noisy by design. In this work, we empirically demonstrate the effectiveness of document sim-
ilarity methods in creating such parallel datasets with high levels of accuracy. Given that datasets
created in this manner are bound to be noisy, we study the performance characteristics of models
for code translation that have been trained on data with varying degrees of noise; and show that
these models have considerable resistance to noise and perform well even with moderate amounts of
noise. Finally, we demonstrate the practical utility of the proposed approach by training models to
translate between 10 pairs of languages – a majority of which have not been looked at in prior work.

2 RELATED WORK

Code translation datasets: Typical methods for creating parallel datasets for code translation
have either relied on the availability of open-sourced projects with implementations in multiple lan-
guages, or on the existence of transpilers. The earliest widely-used large-scale dataset for code
translation was for Java ↔ C# (Nguyen et al., 2013) translation, created by indexing open-sourced
projects implemented in both languages. Aggarwal et al. (2015) used the Python 2to3 1 transpiler
to create a dataset; while Chen et al. (2018) used CoffeeScript’s compiler (which compiles down to
JavaScript) to create a parallel dataset. More recently, Ahmad et al. (2021b) released AVATAR – a
parallel corpus of Java to Python manually curated through submissions on competitive program-
ming websites. Publicly available datasets for code translation are however extremely limited, and
manually curating these datasets for a speciﬁc use-case is expensive and often impractical.

Source-to-Source translation: The earliest code translation models were rule-based systems, op-
erating on handcrafted rules. These systems require a lot of effort to build, are not easily extend-
able to other languages, and are also outperformed by neural techniques. Some of these systems
are: Java2CSharp 2, Java2Python 3, SmallTalk to C (Yasumatsu & Doi, 1995), Cobol to
Java (Mossienko, 2003), and Tangible Software Solutions 4 (VB.NET, C#, Java, C++, and Python).
Moving away from rule-based systems, Nguyen et al. (2013), Karaivanov et al. (2014), and Nguyen
et al. (2014) applied different versions of Phrase-Based Statistical Machine Translation to translate
between Java and C#. Chen et al. (2018) proposed a tree-to-tree neural network to translate the
parsed tree of the source code into the target code parse tree. The aforementioned supervised tech-
niques have all been benchmarked on the Java ↔ C# dataset, and are limited by the availability of
parallel datasets. To bypass this limitation, Roziere et al. (2020) used unsupervised neural machine
translation techniques to translate between languages using only monolingual corpora, and showed
impressive results for translation between Java, C++, and Python. While Roziere et al. (2020) trained

1https://docs.python.org/3/library/2to3.html
2https://sourceforge.net/projects/j2cstranslator/
3https://github.com/natural/java2python
4https://www.tangiblesoftwaresolutions.com/

2

the model speciﬁcally for code translation, large language models – such as GPT-2 (Radford et al.,
2019), GPT-3 (Brown et al., 2020), and Codex (Chen et al., 2021) – have also been shown to have
some competence in generating code (Hendrycks et al., 2021).

Parallel corpus mining: Prior work in natural language research has looked at various ways of
creating parallel corpora from a non-parallel corpus. Munteanu & Marcu (2005) train a maximum
entropy classiﬁer to identify if two given sentences are translations of each other. They extract
parallel data from large-scale Chinese, Arabic, and English non-parallel newspaper corpora, and
show improvement in model performance when trained with a combination of a small parallel corpus
and the extracted dataset. Uszkoreit et al. (2010) describe a system that uses n-gram features to mine
parallel documents from a billion-scale corpus. Smith et al. (2010) focus on aligning Wikipedia
documents by creating features suitable for such documents. Artetxe & Schwenk (2019) utilize
speciﬁc scoring functions based on multilingual sentence embeddings to create parallel corpora,
and Hangya & Fraser (2019) rely on continuous parallel segments rather than word similarities to
ﬁnd parallel sentences. Ban´on et al. (2020) released the largest publicly available parallel corpora
of sentences (223 million parallel sentences) by aligning sentences from data crawled over the web.
There is a substantial precedence of parallel corpus mining in the natural language domain; however,
such studies in the code translation domain are non-existent.

Machine Translation using noisy data: Prior studies have aimed to study the impact of noise
on the performance of machine translation systems. Formiga & Fonollosa (2012) study the impact
of misspelled words on the performance of Statistical Machine Translation and suggest strategies
to deal with them, while Goutte et al. (2012) study the impact of sentence alignment errors on the
performance of SMT. Further, Khayrallah & Koehn (2018) deﬁne 5 categories of artiﬁcial noise in
Neural Machine Translation, and study the impact each of these types has on performance. We moti-
vate our work from these prior efforts in order to study the impact that noise has on the performance
of code translation models.

3 PROPOSED METHOD

In this work, we propose to utilize document similarity methods to create noisy parallel datasets for
code translation. We refer to the datasets created in this manner as “noisy” because unlike manually
curated datasets, there is no guarantee of a parallel implementation of the speciﬁc source ﬁle/code
being available in the corpus: this may result in near-similar code samples being paired as ground
truth examples instead. Algorithm 1 presents the proposed approach as pseudocode.

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

, M, δ)

1, · · · , d

Algorithm 1 Creating parallel code corpus

1: CreateParallelCorpora(D, D
2: initialize P = {}
3: for i = 1 to |D| do
4: Dsim = GetSimilarDocuments(di, D
j) in Dsim do
5:
j) /∈ P and M (di, d

The algorithm expects two non-parallel sets
of documents D = {d1, · · · , dn} and D
=
{d
m} as input. Within the context
of our work, the documents in these two sets
represent code samples from two distinct pro-
gramming languages. Along with the docu-
ments, the algorithm also expects a similarity
measure M (d, d
) as input, to compare two
given documents for similarity. A lower score
from the similarity measure indicates higher
similarity between documents. Finally, the
algorithm expects a similarity threshold δ to
help keep only sufﬁciently similar documents
in the resulting parallel corpus. Thereafter,
the algorithm follows a simple procedure of iterating over all documents; ﬁnding the most similar
documents in the target set; and adding the newly found similar document pairs to the result only
if the target document has not been paired before, and if the similarity is below the threshold value.
Once all the documents are iterated upon, the algorithm produces a list of unique pairs of code
segments (documents) ordered by their similarity, ready to be used for downstream tasks.

7:
8:
9: Dres = sort((d1, d2) ∈ P , key = M (d1, d2))
10: Return: Dres

P = P ∪ (di, d
break

for (di, d
if (·, d

j) ≤ δ then

, M )

j)

6:

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

3

4 EXPERIMENTAL SETUP

To better understand the effectiveness and practical utility of the method proposed in Section 3, we
devise 3 research questions and design experiments to empirically answer them (see Section 5). In
this section, we brieﬂy summarize the different document similarity methods, datasets, pre-trained
models, and evaluation metrics we use in our experiments.

4.1 DOCUMENT SIMILARITY METHODS

TF-IDF:
TF-IDF (Salton & Buckley, 1988) computes the product of the term frequency (TF)
(fraction of times a term appears in a document) with the inverse document frequency (IDF) (log-
arithm of the inverse fraction of documents a particular token occurs in). The cosine similarity
between the document vectors thus created computes the similarity of documents.

Okapi-BM25:
The Okapi-BM25 model (Robertson et al., 1995) uses the following scoring func-
tion (Equation 1) to score the importance of a word w in a document D. Here, IDF (w) represents
the inverse document frequency of the word w, T F (w, D) represents the term frequency of the
word w in the document D, |D| and Davg are the lengths of the current document and the average
document lengths respectively, and k1 and b are free parameters of the model.

BM 25(w, D) = IDF (w) ×

T F (w, D)(k1 + 1)
T F (w, D) + k1(1 − b + b |D|
Davg

)

(1)

Latent Dirichlet Allocation (LDA): LDA (Blei et al., 2003) is a hierarchical generative Bayesian
model that models each document as a ﬁnite mixture over an underlying set of topics. The cosine
similarity of the topic distribution of two documents computes their similarity.

Latent Semantic Indexing (LSI):
LSI (Deerwester et al., 1990) computes the Singular Value
Decomposition of the Bag of Words representation of documents. The cosine similarity of the
decomposed vectors of documents computes their similarity.

Word Movers Distance (WMD): WMD (Kusner et al., 2015) models the document distance
problem as a variant of the Earth Movers Distance (Monge, 1781; Rubner et al., 1998) and solves
the optimization problem deﬁned in Equation 2.

min
T ≥0

n
(cid:88)

i,j=1

Tijc(i, j)

subject to:

n
(cid:88)

j=1
n
(cid:88)

i=1

Tij = di ∀i ∈ {1, · · · , n}

(2)

Tij = dj ∀j ∈ {1, · · · , n}

Here, T ∈ Rn×n is a ﬂow matrix where Tij ≥ 0 and denotes how much of word i in document d
. c(i, j) = (cid:107)xi − xj(cid:107)2 is the cost associated with travelling from
travels to word j in document d
are the nBOW representations of the documents, and X ∈ Rd×n is the
one word to another, d and d
word embedding matrix where xi ∈ Rd represents the d-dimensional embedding of the ith word.

(cid:48)

(cid:48)

4.2 DATASETS

For the experiments whose results are detailed in Section 5, we utilize the following datasets. We
provide representative code samples and statistics from the datasets in Appendix A.

4

Java ↔ C#: The Java ↔ C# dataset is one of the earliest large-scale datasets introduced for the
code translation task (Nguyen et al., 2013; Zhong et al., 2010). It is created by indexing several
open-source projects which have both Java and C# implementations, and pairing methods with the
same ﬁle name and method name. The earlier version of this data was created by indexing the db4o
and Lucene projects. More recently however, Chen et al. (2018) indexed 6 open-sourced projects
to create the dataset. We use the version provided by Chen et al. (2018) in our work.

Java ↔ Python, Java ↔ C++, and Python ↔ C++: Roziere et al. (2020) extracted parallel func-
tions in C++, Python, and Java from the online competitive programming platform GeeksForGeeks5,
and used these code samples as validation and test sets. We, however, concatenate the two datasets
and use the uniﬁed dataset for our experiments. The code samples in this dataset are function-scope
code samples that solve an algorithmic problem.

CodeNet: Project CodeNet (Puri et al., 2021) is a recently released large-scale AI for Code dataset,
created by indexing two online competitive programming websites. The dataset is organized into
about 4000 different problem sets, and contains a little under 14 million total solutions in 55 pro-
gramming languages. Besides providing the code samples, CodeNet also provides input-output pairs
to evaluate solutions to the problem sets.

4.3 MODELS

CodeBERT: CodeBERT (Feng et al., 2020) is a Transformer (Vaswani et al., 2017) based model,
pre-trained on a unimodal data of function-level code samples, and a bimodal data of code and
the associated documentation in natural language. The pre-training data contains code samples in
Go, Java, JavaScript, PHP, Python, and Ruby, and is trained using the Masked Language Modeling
(Devlin et al., 2019) (MLM) and the Replaced Token Detection (Clark et al., 2019) objectives.

GraphCodeBERT: GraphCodeBERT (Guo et al., 2020) is a Transformer based model for code
that also considers the inherent structure in code by integrating the data ﬂow in the pre-training
stage. The model is trained on the CodeSearchNet dataset (Husain et al., 2019) using the MLM,
Edge Prediction, and Node Alignment objectives.

PLBART: PLBART (Ahmad et al., 2021a) is a BART (Lewis et al., 2020) based model pre-trained
on over 700 million Java, Python, and natural language documents collected from open-sourced code
on Github and posts on StackOverﬂow. The model is pre-trained via denoising autoencoding, where
the model learns to reconstruct input corrupted by a noise function. The authors use three noising
strategies: token masking, token deletion, and token inﬁlling to create the corrupted inputs.

4.4 EVALUATION METRICS

BLEU score: BLEU score (Papineni et al., 2002) is a common automatic evaluation metric for
machine-generated text, and exhibits a high correlation with human judgment of quality. BLEU
score is computed as the overlapping fraction of n-grams between the machine-generated text and
the reference text. The metric has however been shown to not be a reliable measure for source code
(Ren et al., 2020; Allamanis et al., 2018; Austin et al., 2021).

CodeBLEU score: Ren et al. (2020) propose the CodeBLEU score to leverage the tree structure
and semantic information in code. It is computed as a combination of the standard BLEU score,
weighted n-gram match, syntactic abstract syntax tree match, and the semantic data ﬂow match.

Exact Match (EM): EM (Nguyen et al., 2013) evaluates if the generated code matches exactly to
the reference code.

Computational Accuracy @ k (CA@k): Recent work in code synthesis has adopted the CA@k
metric (Austin et al., 2021; Roziere et al., 2020) to evaluate code generation models. To compute

5https://practice.geeksforgeeks.org/

5

CA@k, k samples are generated from the model, and the problem is considered solved if any of the
generated k samples pass the unit tests associated with the problem.

5 RESEARCH QUESTIONS AND RESULTS

To validate the central hypothesis of this paper – using document similarity methods to create
datasets for supervised training of code translation models – we deﬁne and seek answers to the
following research questions (RQ):

RQ1: How accurate are document similarity methods in creating parallel datasets for code?
RQ2: Given the created dataset will be noisy, what is the effect of varying degrees of noise on

code translation models?

RQ3: Can the proposed method be used in practice to train models for programming languages

not explored in prior work?

5.1 RQ1: EFFICACY OF DOCUMENT SIMILARITY METHODS

We start our analysis by examining how effective document similarity methods are in creating code
translation datasets. For this experiment, we utilize 4 datasets with known ground-truth mapping
between pairs of programming languages – Java ↔ C#, Java ↔ Python, Java ↔ C++, and Python
↔ C++. For each of these datasets, we create a parallel dataset using 5 different document similarity
methods, and compute the match accuracy as the number of correctly matched code samples.

We summarize the results for this experiment in Table 1, and observe that similarity methods that
operate in a latent space (such as LDA and LSI) perform much worse than methods that operate in
the original space (such as TF-IDF, Okapi-BM25, and WMD). We posit that because code is written
in a more formal language than natural language, and each data sample in the datasets used in this
experiment implements an independent unique function, there is likely no underlying topic or latent
semantic associations that can be captured by LSI and LDA. Therefore these methods perform worse
than methods that directly utilize the tokens in the original space.

Table 1: Match accuracy of parallel datasets created using various document similarity methods.
Match accuracy computes True if the matched code sample is the same as the code sample in the
ground truth dataset.

N (→)
LDA
LSI
TF-IDF
Okapi-BM25
WMD

Java ↔ C#
10,300
47.21%
57.21%
87.36%
87.86%
89.53%

Java ↔ Python
1,418
21.44%
66.08%
86.10%
89.91%
91.04%

Java ↔ C++ Python ↔ C++

1,418
35.83%
87.66%
94.08%
95.77%
95.06%

1,418
16.64%
78.84%
89.35%
89.99%
94.08%

We note that the datasets used for the experiments in Table 1 are not true representatives of code
we expect to ﬁnd while using this method in practice. This is due to the fact that the 4 datasets
used contain code samples for which a true parallel implementation in the other language exists.
When trying to create a parallel dataset from code collected in the wild, we cannot be sure of the
availability of a true parallel implementation, which might affect the performance of the proposed
method. Thus, to account for this phenomenon, we conduct a similar experiment with the CodeNet
dataset. Since code samples in the CodeNet dataset are not parallel implementations, this gives us
a better idea of the effectiveness of document similarity methods in creating parallel datasets from
code in the wild. We select 6 languages and randomly sub-sample 50 problems from the dataset. For
each of the code sample in each of the 50 sampled problem sets, we create a parallel dataset using 3
different document similarity methods. Since we do not have a ground-truth parallel implementation
available for the CodeNet dataset, we cannot compute the match accuracy like we did in the previous
experiment. We therefore compute the pseudo-match accuracy instead. The pseudo-match accuracy
computes True if the matched code sample is a solution of the same problem set, and False otherwise.

6

We report the pseudo-match accuracy results for this experiment in Table 2. We note that while the
TF-IDF and Okapi-BM25 methods performed well in the previous experiment, their performance
varies greatly for this experiment. Datasets created using TF-IDF and Okapi-BM25 methods are
matched to the correct problem set with as little as 30% accuracy and as high as 70% accuracy in
some cases. Datasets created using the WMD method however achieve a high match accuracy for
both experiments (Table 1 and Table 2).

Table 2: Pseudo-match accuracy of datasets created by different document similarity methods on 50
subsampled problems from CodeNet. Pseudo-match accuracy computes True if the matched code
sample is from the same problem set, and False otherwise.

Source language: Go
JavaScript
41.91%
49.07%

Python
Ruby
Java
30.70% 40.35% 27.07%
29.56%
50.93%
50.52% 36.93% 35.37%
71.47% 55.70% 51.35% 60.89% 45.12%

PHP

Source language: JavaScript

Go

Java

Ruby
PHP
50.93% 46.64% 44.89% 41.18% 55.34%
51.74% 58.12% 56.26% 59.74% 61.60%
66.70% 74.71% 67.52% 70.88% 73.55%

Python

Source language: Python

Go

Java

Ruby
JavaScript
69.54% 72.46%
66.67%
67.19% 52.79%
73.52% 57.53%
73.33% 80.28%
74.13%
82.08% 73.75% 77.39% 79.59% 87.31%

PHP

TF-IDF
BM25
WMD

TF-IDF
BM25
WMD

TF-IDF
BM25
WMD

TF-IDF
BM25
WMD

TF-IDF
BM25
WMD

TF-IDF
BM25
WMD

Source language: Java
JavaScript
53.40%
73.46%

Python
Ruby
Go
29.41% 31.32% 23.47%
65.05%
63.94%
34.11% 51.63% 36.09%
79.30% 73.93% 57.51% 66.49% 56.06%

PHP

Source language: PHP

Go

Java

Python
Ruby
JavaScript
26.04% 37.62%
45.66%
64.31% 27.49%
62.54% 50.48%
27.97% 55.63%
48.71%
71.38% 61.25% 73.79% 85.05% 84.08%

Source language: Ruby

Go

Java

JavaScript
61.94% 51.53%
69.85%
67.33% 58.66% 74.37%
68.80% 54.35%

Python
61.42% 67.51%
62.22% 82.45%
78.59% 77.05% 90.39%

PHP

From the experiments designed to answer RQ1, we conclude that document similarity methods are
capable of creating parallel datasets of code with a signiﬁcantly high degree of match accuracy.
Speciﬁcally, the WMD metric seems to be quite adept at delineating datasets for code translation.

5.2 RQ2: NOISE TOLERANCE OF MODELS TRAINED ON CODE

In the previous section, we showed that document similarity methods – and speciﬁcally the Word
Movers Distance (WMD) – are quite adept at creating parallel datasets for code. However, datasets
created in this manner contain errors; therefore in this section, we seek to understand the effect that
varying the degree of such noise has on the performance of code translation models.

We use the CodeBERT and the GraphCodeBERT pre-trained models for this experiments, and ﬁne-
tune these models on different pairings of the Java ↔ C# dataset (Nguyen et al., 2013) created using
the different document similarity methods. We compare the performance of models trained on these
paired datasets with models trained on the ground-truth dataset, and a random baseline with random
pairings of code samples from the two programming languages. Following Ahmad et al. (2021a),
we compute the BLEU score, CodeBLEU score, and the Exact Match score.

The results for this experiment are summarized in Table 3. We additionally refer the reader to Table 1
to see the corresponding match accuracy of the different document similarity methods. Interestingly,
we ﬁnd that models trained on noisy code datasets have a certain degree of resistance to noise; and
while the performance drops with increasing levels of noise, the degradation is not sudden. Even
with high levels of noise, the models perform considerably well. With a high-performing method
such as the Word Movers Distance (WMD) – with about 90% match accuracy – the degradation
in performance is roughly 1 percentage point across the three measures and for both directions of
translation. For methods with a higher level of noise – such as LDA with 47.21% match accuracy –
while the performance goes down signiﬁcantly, it is still signiﬁcantly higher than the performance of
the random baseline. We posit that although noisy datasets create pairs of code with incorrect parallel
implementations, much of the semantics is still retained by the dataset due to the formal nature of
programming languages. For example, even if code samples are incorrectly paired, the syntax for
function and variable deﬁnition, code blocks, and indentation stays the same and is preserved. This
allows the model to learn the translation task to a certain degree.

While the preceding experiment allowed us to understand the performance of models trained on
noisy datasets created using different document similarity methods, we wish to understand the per-
formance characteristics of models trained with varying levels of noise on a more granular level.

7

Table 3: Model performance on the Java ↔ C# dataset matched using various document similarity
methods. Each method introduces a different amount of noise in the resulting dataset (see Table 1)
thereby affecting the performance.

C# −→ Java
BLEU CodeBLEU

CodeBERT

GraphCodeBERT

Random baseline
LDA
LSI
Okapi-BM25
TF-IDF
WMD
Ground-truth
Random baseline
LDA
LSI
TF-IDF
Okapi-BM25
WMD
Ground-truth

Java −→ C#
BLEU CodeBLEU
12.2
57.55
71.8
79.39
78.78
79.59
80.83
6.88
60.34
73.74
79.14
79.65
79.47
80.89

31.71
65.97
77.64
83.54
82.70
83.85
84.86
20.09
67.91
79.30
83.04
83.42
83.72
85.05

4.4

EM
0.0%
33.2% 43.75
47.9% 52.44
59.5% 73.83
58.1% 72.52
58.2% 75.16
60.6% 75.84
0.0%
2.94
37.3% 47.14
49.1% 52.86
57.8% 73.14
59.4% 74.24
59.0 % 75.63
61.1 % 76.76

16.56
55.39
66.26
80.64
77.34
80.93
81.64
14.31
58.79
66.49
77.55
80.56
81.06
82.03

EM
0.0%
23.7%
30.9%
57.6%
57.1%
59.2%
59.9%
0.0%
26.3%
30.5%
57.9%
58.0%
60.2 %
62.3 %

Thus we create datasets for translation between Java and C# by artiﬁcially injecting noise of varying
levels. For explanation, in a dataset with x% noise level, we randomly misalign x% of code sam-
ples in that dataset, while keeping the remaining code samples correctly paired. We then train the
GraphCodeBERT model on these datasets, and compute the BLEU score, CodeBLEU score, and
the Exact Match (EM) score. Figure 1 shows the performance curve with levels of noise varying
from 0% (ground-truth dataset) to 100% (complete random pairings). We observe that the degra-
dation in the performance is gradual for initial levels of noise – compared to performance at 0%
noise, the performance at about 30% noise goes down slowly by about 20 percentage points across
all measures and both ways of translation. Post this 30% noise level, we see a sharper degradation
in performance; and post 70% noise, we observe that the performance is just slightly better than the
performance at 100% noise.

Figure 1: Noise performance curve for GraphCodeBERT model ﬁne-tuned for Java → C# translation
(left) and C# → Java translation (right). While the performance decreases with increasing levels of
noise, the degradation is gradual for initial levels of noise.

Through the experiments in Table 3 and in Figure 1, we get a better idea of the performance charac-
teristics of models for code under varying levels of noise. We conclude that the performance of these
models is not severely affected with moderate amounts of noise; therefore when creating datasets
from code in the wild where we expect a certain amount of noise, we can expect the models to
perform reasonably well.

8

020406080100Noise (%)020406080Evaluation metricCodeBLEUBLEUExact Match020406080100Noise (%)020406080Evaluation metricCodeBLEUBLEUExact Match5.3 RQ3: TRANSLATING BETWEEN A WIDER SET OF PROGRAMMING LANGUAGES

In Section 5.1, we concluded that document similarity methods are adept at creating parallel datasets
for code with acceptable levels of noise; and in Section 5.2, we concluded that models trained on
noisy datasets of code perform reasonably well under moderate levels of noise. In this section, we
take advantage of these two ﬁndings and demonstrate the practical utility of the proposed method by
creating noisy code translation datasets for languages not explored previously in the literature; and
by training models for translating between these languages.

For this experiment, we utilize the CodeNet dataset by creating noisy parallel datasets between the
following 10 languages – C, C#, C++, Go, Java, JavaScript, PHP, Python, Ruby, and Scala. We
choose these languages in order of their frequency in the CodeNet dataset, thereby maximizing the
number of data samples we can potentially create. We also sub-sample about 2500 problem sets from
the original 4000 problem sets from the CodeNet dataset for computational reasons. Thereafter, we
match the solutions in one programming language to another for each of the 2500 sub-sampled
problem sets using the WMD metric. Since the PLBART model can only translate sequences with a
maximum of 512 tokens, we only match code samples with less than 512 tokens. We additionally use
a similarity threshold of 3.0 and ﬁlter samples accordingly. In Appendix B.3, we provide statistics
of the ﬁnal dataset along with some representative code samples and their corresponding similarity
scores. To create the test set for the language pairs, we sub-sample 100 problems that are not seen in
the training and the validation set, and randomly sample 5 different implementations in the source
language from each of the 100 problem sets for a ﬁnal test set size of 500 code samples.

We ﬁne-tune the PLBART model on the matched training data for each language pair, and evaluate
the computational accuracy @ 5 on the test set. We compare the performance of this ﬁne-tuned
model against a model ﬁne-tuned on a dataset created by randomly matching solutions from each
problem set rather than using the WMD metric. To compare these two models fairly, we keep the
dataset sizes, problem sets, and all the hyperparameters constant across the two training procedures.
The CA@5 results for the model trained on the WMD-matched dataset are shown in Figure 2; while
Figure 3 shows the difference in the performance of the model ﬁne-tuned using the WMD-matched
data and the performance of the model ﬁne-tuned using the randomly matched data. We present
some of the model generated code samples in Appendix B.4.

Overall, we observe that models trained using the WMD-matched datasets achieve noteworthy per-
formance across language pairs. More importantly, when compared with models trained on ran-
domly paired data, we see substantial improvements for a majority of the language pairs. While the
common language pairs, such as C → C++, Python → C++, Ruby → C++ see the biggest improve-
ments, more obscure language pairs such as PHP → C++, PHP → Ruby, JavaScript → C++, and
Python → Ruby also demonstrate substantial improvements over their random counterparts. This
leads us to the conclusion that the proposed method is a viable way of creating high-quality datasets
for code translation, thereby alleviating the paucity of training data in the domain.

6 DISCUSSION & CONCLUSION

Modernizing legacy applications into a new programming language is a process that requires a lot of
time, intellect, and monetary investment. Automatic code translation techniques have the potential
to speed up this process, and to reduce the human effort required by either working in tandem with
humans or automatically translating legacy code to a modern language of choice. While multiple
techniques have been proposed to improve the quality of code translation, their practical utility is
hampered due to the limited availability of parallel data required to train these models between
languages of choice. In this work, we proposed a simple technique to utilize document similarity
methods to create noisy datasets for code translation; and demonstrated that models for code have a
certain amount of tolerance for noise and perform well even under signiﬁcant amounts of noise. We
speciﬁcally demonstrated the effectiveness of the Word Movers Distance (WMD) metric in creating
parallel datasets between numerous language pairs that have not been explored in prior literature;
and showed signiﬁcantly improved model performances as compared to models trained on randomly
matched datasets. Future work will explore better metrics in terms of both match accuracy and
computational efﬁciency, thereby further reducing the noise in the dataset; and incorporating the
similarity score in the model to weight samples according to their computed similarity.

9

Figure 2: CA@5 for PLBART model ﬁne-tuned on
code translation dataset created from the CodeNet
dataset using the WMD metric.

Figure 3: Difference between the CA@5 of
PLBART trained using data created using the
proposed method and the CA@5 of PLBART
trained using randomly matched dataset.

7 ETHICS STATEMENT

One of the major ethical points to consider when dealing with the automatic creation and translation
of source code centers around the effects on humans: both humans who create and maintain code
for a living; and humans that are affected by the decisions and outcomes produced by the execution
of such code. For the former concern, our work merely seeks to align pre-existing bits of open-
sourced code so that downstream data-hungry techniques may have more reasonable approximations
of correct and on-purpose code to learn from. Our work does not replace jobs that humans are
trained to do and more adept at; and indeed defers to and takes inspiration from prior studies (Weisz
et al., 2021) that show that human generators of code are very likely to engage in partnerships
with automatically learned models to produce or maintain code better. For the latter concern, we
acknowledge that it is possible to use the output of artefacts from our work in downstream systems
that can produce automatic code with little to no oversight. Similar to work on examining the
effects of large language models for human natural languages (Bender et al., 2021), much attention
is needed where it comes to automatic code generation and translation techniques and models. We
look forward to studying some of these issues in partnership with colleagues in the future.

REFERENCES

Karan Aggarwal, Mohammad Salameh, and Abram Hindle. Using machine translation for convert-

ing python 2 to python 3 code. Technical report, PeerJ PrePrints, 2015.

Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Uniﬁed pre-training for pro-
gram understanding and generation. In Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.
2655–2668, 2021a.

Wasi Uddin Ahmad, Md Golam Rahman Tushar, Saikat Chakraborty, and Kai-Wei Chang. Avatar:
A parallel corpus for java-python program translation. arXiv preprint arXiv:2108.11590, 2021b.

Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of machine

learning for big code and naturalness. ACM Computing Surveys (CSUR), 51(4):1–37, 2018.

Marc Andreessen. Why software is eating the world. Wall Street Journal, 20(2011):C2, 2011.

10

CC#C++GoJavaPHPPythonRubyTarget languageCC#C++GoJavaJavaScriptPHPPythonRubyScalaSource language14.5344.418.1029.8317.5731.1319.8021.7426.7932.5522.4722.3424.7922.669.725.9618.499.2926.3313.4720.0716.6130.3837.7317.8322.2822.4333.8419.3741.0620.9718.3524.8730.4925.5919.0628.3514.5326.5016.7513.9224.9125.6023.2831.5317.4628.3330.5632.9920.8810.4630.6514.3116.699.6430.4711.7411.8424.2315.5316.2428.4319.2113.4729.3517.8321.3420.4427.6128.66816243240CC++JavaPHPPythonRubyTarget languageCC#C++GoJavaJavaScriptPHPPythonRubyScalaSource language33.054.752.711.523.2521.7417.312.525.642.372.055.192.052.412.92-0.5220.0713.498.2917.66-2.211.715.2724.710.501.843.919.6019.865.170.00-7.98-2.03-2.064.181.37-0.866.914.3828.600.50-1.707.94-1.7623.252.590.483.83-1.528.850.00-1.373.380.6008162432Mikel Artetxe and Holger Schwenk. Margin-based parallel corpus mining with multilingual sen-
tence embeddings. In Proceedings of the 57th Annual Meeting of the Association for Computa-
tional Linguistics, pp. 3197–3203, 2019.

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,
Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language
models. arXiv preprint arXiv:2108.07732, 2021.

Marta Ban´on, Pinzhen Chen, Barry Haddow, Kenneth Heaﬁeld, Hieu Hoang, Miquel Espla-Gomis,
Mikel L Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, et al. Paracrawl: Web-scale
acquisition of parallel corpora. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, pp. 4555–4567, 2020.

Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the
dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM
Conference on Fairness, Accountability, and Transparency, pp. 610–623, 2021.

David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation.

the Journal of

machine Learning research, 3:993–1022, 2003.

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.

Robert N Charette. No one notices the creaky software systems that run the world—until they fail.

IEEE Spectrum, 57(9):24–30, 2020.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Ed-
wards, Yura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models
trained on code. arXiv preprint arXiv:2107.03374, 2021.

Xinyun Chen, Chang Liu, and Dawn Song. Tree-to-tree neural networks for program translation.

Advances in Neural Information Processing Systems, 31, 2018.

Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training
text encoders as discriminators rather than generators. In International Conference on Learning
Representations, 2019.

Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman.
Indexing by latent semantic analysis. Journal of the American society for information science, 41
(6):391–407, 1990.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing
Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural
languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing: Findings, pp. 1536–1547, 2020.

Lluis Formiga and Jos´e AR Fonollosa. Dealing with input noise in statistical machine translation.

In Proceedings of COLING 2012: Posters, pp. 319–328, 2012.

Cyril Goutte, Marine Carpuat, and George Foster. The impact of sentence alignment errors on
In Proceedings of the 10th Conference of the
phrase-based machine translation performance.
Association for Machine Translation in the Americas: Research Papers, San Diego, California,
USA, October 28-November 1 2012. Association for Machine Translation in the Americas. URL
https://aclanthology.org/2012.amta-papers.7.

Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, LIU Shujie, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, et al. Graphcodebert: Pre-training code representations with
data ﬂow. In International Conference on Learning Representations, 2020.

11

Viktor Hangya and Alexander Fraser. Unsupervised parallel sentence extraction with parallel seg-
In Proceedings of the 57th Annual Meeting of the

ment detection helps machine translation.
Association for Computational Linguistics, pp. 1224–1234, 2019.

Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin
Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence
with apps. arXiv preprint arXiv:2105.09938, 2021.

Abram Hindle, Earl T Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu. On the natural-

ness of software. Communications of the ACM, 59(5):122–131, 2016.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt.
arXiv preprint

Codesearchnet challenge: Evaluating the state of semantic code search.
arXiv:1909.09436, 2019.

Anna Irrera. Banks scramble to ﬁx old systems as it ’cowboys’ ride into sunset, Apr 2017.
https://www.reuters.com/article/us-usa-banks-cobol/banks-

URL
scramble-to-fix-old-systems-as-it-cowboys-ride-into-sunset-
idUSKBN17C0D8.

Svetoslav Karaivanov, Veselin Raychev, and Martin Vechev. Phrase-based statistical translation of
In Proceedings of the 2014 ACM International Symposium on New

programming languages.
Ideas, New Paradigms, and Reﬂections on Programming & Software, pp. 173–184, 2014.

Makena Kelly.

Unemployment checks are being held up by a coding language al-
URL https://www.theverge.com/2020/4/14/
most nobody knows, Apr 2020.
21219561/coronavirus-pandemic-unemployment-systems-cobol-legacy-
software-infrastructure.

Huda Khayrallah and Philipp Koehn. On the impact of various types of noise on neural machine
translation. In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation,
pp. 74–83, 2018.

Yunsu Kim, Miguel Grac¸a, and Hermann Ney. When and why is unsupervised neural machine
translation useless? In Proceedings of the 22nd Annual Conference of the European Association
for Machine Translation, pp. 35–44, 2020.

Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to document

distances. In International conference on machine learning, pp. 957–966. PMLR, 2015.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-
training for natural language generation, translation, and comprehension. In Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics, pp. 7871–7880, 2020.

Kelly Marchisio, Kevin Duh, and Philipp Koehn. When does unsupervised machine translation

work? In Proceedings of the Fifth Conference on Machine Translation, pp. 571–583, 2020.

Gaspard Monge. M´emoire sur la th´eorie des d´eblais et des remblais. Histoire de l’Acad´emie Royale

des Sciences de Paris, 1781.

Maxim Mossienko. Automated cobol to java recycling. In Seventh European Conference onSoftware

Maintenance and Reengineering, 2003. Proceedings., pp. 40–50. IEEE, 2003.

Dragos Stefan Munteanu and Daniel Marcu.

Improving machine translation performance by ex-

ploiting non-parallel corpora. Computational Linguistics, 31(4):477–504, 2005.

Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N Nguyen. Lexical statistical machine translation
for language migration. In Proceedings of the 2013 9th Joint Meeting on Foundations of Software
Engineering, pp. 651–654, 2013.

Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N Nguyen. Migrating code with statistical ma-
chine translation. In Companion Proceedings of the 36th International Conference on Software
Engineering, pp. 544–547, 2014.

12

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association
for Computational Linguistics, pp. 311–318, 2002.

Ruchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladmir Zolotov,
Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, et al. Project codenet: A large-scale ai
for code dataset for learning a diversity of coding tasks. arXiv preprint arXiv:2105.12655, 2021.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language

models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou,
Ambrosio Blanco, and Shuai Ma. Codebleu: a method for automatic evaluation of code synthesis.
arXiv preprint arXiv:2009.10297, 2020.

Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford,

et al. Okapi at trec-3. Nist Special Publication Sp, 109:109, 1995.

Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. Unsupervised
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Bal-
translation of programming languages.
can, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
20601–20611. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/
paper/2020/file/ed23fbf18c2cd35f8c7f8de44f85c08d-Paper.pdf.

Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. A metric for distributions with applica-
In Sixth International Conference on Computer Vision (IEEE Cat.

tions to image databases.
No. 98CH36271), pp. 59–66. IEEE, 1998.

Gerard Salton and Christopher Buckley. Term-weighting approaches in automatic text retrieval.

Information processing & management, 24(5):513–523, 1988.

Jason Smith, Chris Quirk, and Kristina Toutanova. Extracting parallel sentences from comparable
corpora using document level alignment. In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the Association for Computational Linguistics, pp.
403–411, 2010.

Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Dubiner. Large scale parallel document mining
for machine translation. In Proceedings of the 23rd International Conference on Computational
Linguistics (Coling 2010), pp. 1101–1109, 2010.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017.

Kevin Walsh. Agencies need to develop and implement modernization plans for critical legacy

systems. 2021. URL https://www.gao.gov/assets/gao-21-524t.pdf.

Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross, Fernando Mar-
tinez, Mayank Agarwal, and Kartik Talamadupula. Perfection not required? human-ai partner-
In 26th International Conference on Intelligent User Interfaces, pp.
ships in code translation.
402–412, 2021.

Kazuki Yasumatsu and Norihisa Doi. Spice: a system for translating smalltalk programs into a c

environment. IEEE Transactions on Software Engineering, 21(11):902–912, 1995.

Hao Zhong, Suresh Thummalapenta, Tao Xie, Lu Zhang, and Qing Wang. Mining api mapping for
language migration. In Proceedings of the 32nd ACM/IEEE International Conference on Software
Engineering-Volume 1, pp. 195–204, 2010.

13

A REPRESENTATIVE CODE SAMPLES FROM UTILIZED DATASETS

In this section, we provide representative code samples from the datasets we use in this work. List-
ings 1 and 2 are data samples from the Java ↔ C# dataset. Listings 3 and 4 are data samples from
the Java ↔ Python dataset. Listings 5 and 6 are data samples from the Java ↔ C++ dataset. Listings
7 and 8 are data samples from the C++ ↔ Python dataset. Finally, listings 9, 10, 11, 12, 13, 14, 15,
16 provide code samples from one particular problem set from the CodeNet dataset.

Listing 1: Java ↔ C#: Java code sample
public static Cell getCell(Row row, int columnIndex)
{

Listing 2: Java ↔ C#: C# code sample
public static ICell GetCell(IRow row, int column)
{

Cell cell = row.getCell(columnIndex);
if (cell == null)
{

cell = row.createCell(columnIndex);

ICell cell = row.GetCell(column);
if (cell == null)
{

cell = row.CreateCell(column);

}
return cell;

}

}
return cell;

}

Listing 3: Java ↔ Python: Java code sample

static int binaryToDecimal ( int n )
{

int num = n ;
int dec_value = 0 ;
int base = 1 ;
int temp = num ;
while ( temp > 0 ) {

int last_digit = temp %
temp = temp / 10 ;
dec_value += last_digit * base ;
base = base * 2 ;

}
return dec_value ;

}

Listing 4: Java ↔ Python: Python code sample
def binaryToDecimal ( n ) :

num = n ;
dec_value = 0 ;
base = 1 ;
temp = num ;
while ( temp ) :

last_digit = temp %
temp = int ( temp / 10 ) ;
dec_value += last_digit * base ;
base = base * 2 ;

return dec_value ;

Listing 5: Java ↔ C++: Java code sample

Listing 6: Java ↔ C++: C++ code sample

static int findS ( int s ) {

int sum = 0 ;
for ( int n = 1 ;
sum < s ;
n ++ ) {

sum += n * n ;
if ( sum == s ) return n ;

}
return - 1 ;

}

int findS ( int s ) {
int sum = 0;
for ( int n = 1;
sum < s;
n ++ ) {

sum += n * n;
if ( sum == s ) return n;

}
return - 1;

}

Listing 7: C++ ↔ Python: C++ code sample

void printDistinct ( int arr [ ], int n ) {

Listing 8: C++ ↔ Python: Python code sample
def printDistinct ( arr , n ) :

sort ( arr, arr + n );
for ( int i = 0;
i < n;
i ++ ) {

arr.sort ( ) ;
for i in range ( n ) :

if ( i < n - 1 and arr [ i ] == arr [ i + 1

(cid:44)→ ] ) :

while ( i < n - 1 && arr [ i ] == arr [ i +

while ( i < n - 1 and ( arr [ i ] == arr

(cid:44)→ 1 ] ) i ++;
cout << arr [ i ] << " ";

}

}

(cid:44)→ [ i + 1 ] ) ) :

i += 1 ;

else :

print ( arr [ i ] , end = " " ) ;

Listing 9: CodeNet: C code sample

#include<stdio.h>
int main(){
int i,j;
for(i=1;i<10;i++){

for(j=1;j<10;j++){

printf("%

}

}
return 0;

}

Listing 10: CodeNet: C# code sample

using System;
class test
{

static void Main()
{for (int i = 1; i < 10; i++) for (int j = 1; j

(cid:44)→ < 10; j++) Console.WriteLine(i+"x"+j+"="
(cid:44)→ +i*j);}

}

14

Listing 11: CodeNet: Go code sample

Listing 12: CodeNet: Java code sample

package main

import "fmt"

func main() {

for i := 1; i < 10; i++ {

for j := 1; j < 10; j++ {

fmt.Printf("%

}

}

}

public class Main
{

public static void main(String[] args)
{

for(int a=1;a<=9;a++){

for(int b=1;b<=9;b++){

System.out.println(a+"x"+b+"="+a*b);

}

}

}

}

Listing 13: CodeNet: JavaScript code sample

Listing 14: CodeNet: PHP code sample

for (var i=1; i<10; i++) {

for (var j=1; j<10; j++) {

console.log(i + ’x’ + j + ’=’ + i*j)

}

}

<?php
for($i=1;$i<10;$i++){
for($j=1;$j<10;$j++){
echo $i."x".$j."=".$i*$j."\n";
}
}

Listing 15: CodeNet: Ruby code sample

Listing 16: CodeNet: Scala code sample

# Your code here!

9.times{|i|

i=i+1
9.times{|j|
j=j+1

puts i.to_s+’x’+j.to_s+’=’+(i*j).to_s

}}

object Main{

def main(args: Array[String]){

for(i <- 1 to 9){

for(j <- 1 to 9){

println(i + "x" + j + "=" + i*j)

}

}

}

}

B PARALLEL DATA CREATED FROM CODENET

In this section we look at the various properties of the parallel dataset created from the CodeNet
dataset. In section B.1, we present most similar data samples identiﬁed by the WMD metric across
various language pairs. In section B.2, we present the histograms of similarity scores in the ﬁnal
dataset. Finally, in section B.3, we present the number of data samples in the ﬁnal dataset created
from the CodeNet dataset.

B.1

IDENTIFIED MOST SIMILAR DATA SAMPLES ACROSS LANGUAGE PAIRS

In table 4, we provide the most-similar code samples identiﬁed by the WMD metric across various
language pairs along with the computed similarity between the two samples.

15

Table 4: Most similar code samples across various language pairs as
identiﬁed by the WMD metric

C → Python dataset: Similarity: 0.75

#include <stdio.h>
#include <math.h>

int main()
{

int n;
int i;
double x1, x2, x3, y1, y2, y3, px, py, r;

scanf("%

for(i = 0; i < n; i++){

scanf("%

px = ((y2 - y3)*(x1*x1 + y1*y1) + (y3 - y1)*(

(cid:44)→ x2*x2 + y2*y2) + (y1 - y2)*(x3*x3 +
(cid:44)→ y3*y3))/(2*(x1*(y2 - y3) + x2*(y3 -
(cid:44)→ y1) + x3*(y1 - y2)));

py = ((x2 - x3)*(x1*x1 + y1*y1) + (x3 - x1)*(

(cid:44)→ x2*x2 + y2*y2) + (x1 - x2)*(x3*x3 +
(cid:44)→ y3*y3))/(2*(y1*(x2 - x3) + y2*(x3 -
(cid:44)→ x1) + y3*(x1 - x2)));

r = sqrt(pow((x1 - px), 2) + pow((y1 - py), 2)

import math

n = int(raw_input())

for i in range(n):

x1, y1, x2, y2, x3, y3 = map(float, raw_input().

(cid:44)→ split())

px = ((y2 - y3)*(x1*x1 + y1*y1) + (y3 - y1)*(x2*
(cid:44)→ x2 + y2*y2) + (y1 - y2)*(x3*x3 + y3*y3)
(cid:44)→ )/(2*(x1*(y2 - y3) + x2*(y3 - y1) + x3
(cid:44)→ *(y1 - y2)))

py = ((x2 - x3)*(x1*x1 + y1*y1) + (x3 - x1)*(x2*
(cid:44)→ x2 + y2*y2) + (x1 - x2)*(x3*x3 + y3*y3)
(cid:44)→ )/(2*(y1*(x2 - x3) + y2*(x3 - x1) + y3
(cid:44)→ *(x1 - x2)))

r = math.sqrt(pow((x1 - px), 2) + pow((y1 - py),

(cid:44)→ 2))

print "%

(cid:44)→ );

printf("%

}

return 0;

}

using System;

class Program
{

C# → Java dataset: Similarity: 0.62

static void Main(string[] args)
{

for (int i = 0; i < 1000; i++)
{

Console.WriteLine("Hello World");

}

}

}

class Main
{

public static void main(String[] args)
{

for (int i = 0; i < 1000; i++)
{

System.out.println("Hello World");

}

}

}

Scala → Ruby dataset: Similarity: 0.91

object Main extends App {

val a = Array(1, 1, 1, 2, 1, 2, 1, 5, 2, 2, 1,
(cid:44)→ 5, 1, 2, 1, 14, 1, 5, 1, 5, 2, 2, 1,
(cid:44)→ 15, 2, 2, 5, 4, 1, 4, 1, 51)

val k = scala.io.StdIn.readInt - 1
println(a(k))

}

k = gets.to_i
a = "1, 1, 1, 2, 1, 2, 1, 5, 2, 2, 1, 5, 1, 2, 1,
(cid:44)→ 14, 1, 5, 1, 5, 2, 2, 1, 15, 2, 2, 5, 4,
(cid:44)→ 1, 4, 1, 51".split(",").map(&:to_i)

p a[k-1]

PHP → Python dataset: Similarity: 1.34

16

<?php

while True:

while ($line = trim(fgets(STDIN))) {

$num = explode(" ",$line);
if ($num[0] == 0 && $num[1] == 0) {

exit();

}
sort($num);
$line = implode(" ",$num);
echo $line . PHP_EOL;

}

?>

num = [int(x) for x in input().rstrip().split

(cid:44)→ ()]

if num[0] ==0 and num[1] == 0:

break

num.sort()
ans = " ".join(str(x) for x in num)
print(ans)

# <?php

exit();

# while ($line = trim(fgets(STDIN))) {
$num = explode(" ",$line);
#
#
if ($num[0] == 0 && $num[1] == 0) {
#
#
#
#
#
# }

}
sort($num);
$line = implode(" ",$num);
echo $line . PHP_EOL;

# ?>

B.2 SIMILARITY HISTOGRAMS IN THE CREATED DATASET

We show the histograms of the similarity scores in the datasets matched through the WMD metric
on the CodeNet dataset in ﬁgure 4.

B.3 DATA STATISTICS

In table 5, we provide the number of data samples in the ﬁnal dataset created from the CodeNet
dataset.

Table 5: Number of data samples for each code translation dataset. Each row represents the source
language, while each column represents the target language. Due to the skewed number of submis-
sions in different languages in the CodeNet dataset, we see the same in the created parallel dataset.

C

×

C#

C++

Go

Java

JS

PHP

Python Ruby

Scala

2,295

15,534

709

8,900

1,354

837

10,499

5,564

1,247

2,504

×

3,145

595

2,629

846

635

2,639

2,007

821

C

C#

C++

30,977

3,388

×

1,358

17,809

1,793

1,550

44,707

14,375

2,183

Go

1,732

974

2,496

×

2,077

642

669

1,755

1,315

729

Java

15,909

5,016

23,821

2,858

×

2,262

2,202

12,846

7,746

2,034

JS

2,876

1,907

2,954

998

2,653

×

1,238

2,893

2,307

1,349

PHP

2,802

1,741

2,898

1,180

2,454

1,389

×

2,996

2,310

1,133

Python

27,947

7,778

76,825

7,235

29,007

6,666

7,641

×

48,301

7,126

Ruby

28,423

12,595

48,985

8,838

23,515

8,322

9,252

62,298

×

6,457

Scala

5,506

4,720

6,660

3,343

5,953

3,536

3,249

6,937

5,586

×

17

Figure 4: Histogram of similarity scores for various language pairs matched by WMD

B.4 CODE GENERATED FROM FINE-TUNED PLBART MODEL

In table 6, we show examples of code generated from a PLBART model ﬁne-tuned on a dataset
created using the WMD metric. We show examples from C → Python, C# → Java, C++ → Ruby,
Java → Ruby, JavaScript → C, PHP → Python, Python → C, Ruby → C, and Scala → Python
language pairs. For a given input source code, we show an example of the correctly generated and
incorrectly generated code.

18

0102030405060C#  C dataset01020304050607080C#  C++ dataset0246810121416C#  Go dataset01020304050607080C#  Java dataset0510152025C#  JavaScript dataset0.02.55.07.510.012.515.017.520.0C#  PHP dataset010203040506070C#  Python dataset0102030405060C#  Ruby dataset051015202530C#  Scala dataset0100200300400500600700800C++  C dataset020406080C++  C# dataset010203040C++  Go dataset0100200300400500C++  Java dataset0102030405060C++  JavaScript dataset01020304050C++  PHP dataset0200400600800100012001400C++  Python dataset0100200300400500C++  Ruby dataset010203040506070C++  Scala dataset01020304050C  C# dataset0100200300400C  C++ dataset0.02.55.07.510.012.515.017.520.0C  Go dataset050100150200C  Java dataset010203040C  JavaScript dataset051015202530C  PHP dataset050100150200250300350C  Python dataset020406080100120140160C  Ruby dataset05101520253035C  Scala dataset01020304050Go  C dataset051015202530Go  C# dataset01020304050607080Go  C++ dataset01020304050607080Go  Java dataset0510152025Go  JavaScript dataset0.02.55.07.510.012.515.017.520.0Go  PHP dataset0102030405060Go  Python dataset05101520253035Go  Ruby dataset05101520Go  Scala dataset0100200300400Java  C dataset020406080100120140160Java  C# dataset0100200300400500600700Java  C++ dataset020406080100Java  Go dataset010203040506070Java  JavaScript dataset01020304050607080Java  PHP dataset0100200300400500Java  Python dataset050100150200250Java  Ruby dataset01020304050Java  Scala dataset020406080JavaScript  C dataset0102030405060JavaScript  C# dataset020406080100JavaScript  C++ dataset051015202530JavaScript  Go dataset01020304050607080JavaScript  Java dataset010203040JavaScript  PHP dataset020406080JavaScript  Python dataset01020304050607080JavaScript  Ruby dataset010203040JavaScript  Scala dataset020406080PHP  C dataset01020304050PHP  C# dataset020406080100PHP  C++ dataset05101520253035PHP  Go dataset01020304050607080PHP  Java dataset010203040PHP  JavaScript dataset020406080100PHP  Python dataset020406080100120PHP  Ruby dataset0510152025303540PHP  Scala dataset0200400600800Python  C dataset0255075100125150175200Python  C# dataset050010001500200025003000Python  C++ dataset0255075100125150175200Python  Go dataset020040060080010001200Python  Java dataset050100150200250Python  JavaScript dataset050100150200250300350Python  PHP dataset02004006008001000120014001600Python  Ruby dataset050100150200250Python  Scala dataset020040060080010001200Ruby  C dataset050100150200250300350400Ruby  C# dataset025050075010001250150017502000Ruby  C++ dataset050100150200250Ruby  Go dataset0100200300400500600700800Ruby  Java dataset050100150200250300Ruby  JavaScript dataset0100200300400Ruby  PHP dataset0500100015002000Ruby  Python dataset050100150200Ruby  Scala dataset0.00.51.01.52.02.53.00255075100125150175Scala  C dataset0.00.51.01.52.02.53.00255075100125150175Scala  C# dataset0.00.51.01.52.02.53.0050100150200Scala  C++ dataset0.00.51.01.52.02.53.0020406080100120Scala  Go dataset0.00.51.01.52.02.53.00255075100125150175Scala  Java dataset0.00.51.01.52.02.53.0020406080100120140Scala  JavaScript dataset0.00.51.01.52.02.53.0020406080100120140160Scala  PHP dataset0.00.51.01.52.02.53.0050100150200250300Scala  Python dataset0.00.51.01.52.02.53.00255075100125150175200Scala  Ruby datasetTable 6: Examples of correct and incorrect translations given the source
code to a PLBART model ﬁne-tuned on a dataset matched using the
WMD metric

Source C code

C → Python translation
Correct Python translation

Incorrect Python translation

n = int(input())
a = list(map(int, input().split()

n = int(input())
a = list(map(int, input().split()

(cid:44)→ ))
for i in range(n):
if a[i]%

print("first")
exit()

(cid:44)→ ))
for i in range(n):
if a[i]%

print("first")
exit()

print("second")

print("second")

#include <stdio.h>
int main(void) {
long n;
scanf("%
long a[n];
for (long i = 0; i < n; i++) {
scanf("%
}
for (long i = 0; i < n; i++) {
if (a[i]%
printf("first\n");
return 0;
}
}
printf("second\n");
return 0;
}

Source C# code

Correct Java translation

Incorrect Java translation

C# → Java translation

import java.util.Scanner;
public class Main {
public static void main(String[]

import java.util.Scanner;
public class Main {
public static void main(String[]

(cid:44)→ args) {

(cid:44)→ args) {

Scanner sc = new Scanner(System.

Scanner sc = new Scanner(System.

(cid:44)→ in);

int A = sc.nextInt();
int B = sc.nextInt();
int C = sc.nextInt();
boolean flg = false;
if (A > B) {
if (C > B && A>C) {
flg = true;
}
} else if (A < B) {
if (B>C && C>A) {
flg = true;
}
}
if (flg) {
System.out.println("Yes");
} else {
System.out.println("No");
}
}
}

(cid:44)→ in);

int A = sc.nextInt();
int B = sc.nextInt();
int C = sc.nextInt();
boolean flg = false;
if (A > B) {
if (C > B && A>C) {
flg = true;
}
} else if (A < B) {
if (B>C && A>C) {
flg = true;
}
}
if (flg) {
System.out.println("Yes");
} else {
System.out.println("No");
}
sc.close();
}
}

using System;

namespace Tenka1A
{
class Program
{
static void Main(string[] args)
{
string[] input = Console.

(cid:44)→ ReadLine().Split(’ ’);

int A = int.Parse(input[0]);
int B = int.Parse(input[1]);
int C = int.Parse(input[2]);
bool flg = false;
if (A > B)
{
if (C > B&&A>C)
{
flg = true;
}
}else if (B>A) {
if (B>C&&C>A)
{
flg=true;
}
}
if (flg)
{
Console.WriteLine("Yes");
}
else {
Console.WriteLine("No");
}
}
}
}

Source C++ code

Correct Ruby translation

Incorrect Ruby translation

C++ → Ruby translation

19

N = gets.to_i
as = gets.split.map(&:to_i)
mx = Array.new(N + 1, -Float::

N = gets.to_i
as = gets.split.map(&:to_i)
mx = Array.new(N + 1, -Float::

(cid:44)→ INFINITY)

mx[0] = 0
as.each do |a|
if a == 1
mx[a] = 1
elsif mx[a] < mx[a - 1]
mx[a] = mx[a - 1] + 1
end
end
puts N - mx.max

(cid:44)→ INFINITY)

mx[0] = 0
as.each do |a|
if a == 1
mx[a] = 1
elsif mx[a] < mx[a - 1]
mx[a] = mx[a - 1] + 1
end
end
ans = N - (mx.max + 1)
puts ans

#include <cstdio>

#include <iostream>
#include <algorithm>
#include <cstring>
#include <cmath>
using namespace std;
typedef long long LL;
int n,k,ans = 0,flg = 0;
int a[500005];
int mx[500005] = {0};
int main(){
ios::sync_with_stdio(false);
cin >> n;
memset(mx,-0x3f,sizeof(mx));
mx[0] = 0;
for(int i = 1;i <= n;i ++) cin

(cid:44)→ >> a[i];

for(int i = 1;i <= n;i ++){
if(a[i] == 1) flg = 1;
mx[a[i]] = max(mx[a[i]],mx[a[i]

(cid:44)→ - 1] + 1);
ans = max(ans,mx[a[i]]);
}
if(!flg) cout << -1 << endl;
else cout << n - ans << endl;
return 0;
}

Source Java code

Correct Ruby translation

Incorrect Ruby translation

Java → Ruby translation

n = gets.to_i
a = gets.split.map(&:to_i)
h = Hash.new(0)
a.each do |i|
h[i] += 1
end
puts h.values.all? { |v| v == 1 }

n = gets.to_i
a = gets.split.map(&:to_i)
h = Hash.new(0)
a.each do |i|
h[i] += 1
end
puts h.values.all? { |v| v == 0 }

(cid:44)→ ? "YES" : "NO"

(cid:44)→ ? "YES" : "NO"

import java.util.HashMap;

import java.util.Map;
import java.util.Scanner;
public class Main {
public static void main(String[]
(cid:44)→ args) throws Exception
(cid:44)→ {

// File file = new File("test.

(cid:44)→ txt");

// Scanner sc = new Scanner(file

(cid:44)→ );

Scanner sc = new Scanner(System.

(cid:44)→ in);

int n = sc.nextInt();
Map<Integer, Boolean> map = new

(cid:44)→ HashMap<>();
for(int i = 0; i < n; i++){
int a = sc.nextInt();
if(null != map.get(a)){
System.out.println("NO");
sc.close();
return;
}else{
map.put(a, true);;
}
}
sc.close();
System.out.println("YES");
}
}

Source JavaScript code

Javascript → C translation
Correct C translation

Incorrect C translation

20

’use strict’

const main = input => {
const args = input.split(’\n’).
(cid:44)→ map(arg => arg.split(’
(cid:44)→ ’))

const A = parseInt(args[0][0],

(cid:44)→ 10)

const B = parseInt(args[0][1],

(cid:44)→ 10)

if (A %
else console.log(’Impossible’)
}
main(require(’fs’).readFileSync
(cid:44)→ (’/dev/stdin’, ’utf8’))

Source PHP code

<?php

fscanf(STDIN,"%
if(strpos($s, "A") === false){
}else{
if(strpos($s,"B") === false){
}else{
$flg = true;
}
}

if($flg){
echo "Yes";
}else{
echo "No";
}

#include <stdio.h>
int main(void)
{
int a, b;
scanf("%
if (a %
else printf("Impossible\n");
return 0;
}

#include <stdio.h>
int main(void)
{
int a, b;
scanf("%
if (a %
printf("Possible\n");
else
printf("Impossible\n");
return 0;
}

PHP → Python translation
Correct Python translation

Incorrect Python translation

s = input()
flg = False
if "A" in s or "B" in s:

flg = True

else:

if "A" in s:

flg = True
print("Yes" if flg else "No")

s = input()
a = 0
b = 0
for i in range(len(s)):

if "A" in s[i]:

if "B" in s[i]:
a += 1

else:

b += 1

if a == b:

print("Yes")

else:

print("No")

Source Python code

Correct C translation

Incorrect C translation

Python → C translation

import sys

N = int(input())
a = list(map(int, input().split

(cid:44)→ ()))

ans = 0
count = 1
if 1 not in a :

print(-1)
sys.exit()
for i in range(N) :

if a[i] == count :

count += 1

else :

ans += 1

print(ans)

#include <stdio.h>
int main()
{
int n;
scanf("%
int i;
int a[100005];
for (i = 0; i < n; i++)
scanf("%
int ans = 0;
int count = 1;
for (i = 0; i < n; i++)
{
if (a[i] == count)
count++;
else
ans++;
}
printf("%
return 0;
}

#include <stdio.h>
int main(void){
int N;
scanf("%
int a[N];
for (int i = 0; i < N; i++) {
scanf("%
}
int ans = 0;
int count = 1;
for (int i = 0; i < N; i++) {
if (a[i] == count) {
ans++;
count++;
}
}
printf("%
return 0;
}

Source Ruby code

Correct C translation

Incorrect C translation

Ruby → C translation

21

A,B = gets.split.map(&:to_i)

S = gets.chomp
if /ˆ\d{#{A}}-\d{#{B}}/.match(S)
puts ’Yes’
else
puts ’No’
end

#include<stdio.h>
int main(void)
{
int A,B;
scanf("%
char S[100];
scanf("%
if(S[0]==’A’&&S[1]==’B’&&S[2]==’A

(cid:44)→ ’&&S[3]==’B’&&S[4]==’A’
(cid:44)→ &&S[5]==’B’&&S[6]==’A’){

printf("Yes\n");
}
else{
printf("No\n");
}
return 0;

#include<stdio.h>
int main(void)
{
int A,B;
scanf("%
char S[100];
scanf("%
if(S[0]==’A’&&S[1]==’B’&&S[2]==’A

(cid:44)→ ’){

printf("No");
return 0;
}
if(S[0]==’A’&&S[1]==’B’&&S[2]==’A

(cid:44)→ ’){

printf("No");
return 0;
}
if(S[0]==’A’&&S[1]==’B’&&S[2]==’A

(cid:44)→ ’){

printf("No");
return 0;
}
if(S[0]==’A’&&S[1]==’B’&&S[2]==’A

(cid:44)→ ’){

printf("No");
return 0;
}
if(S[0]==’A’&&S[1]==’B’&&S[2]==’A

(cid:44)→ ’){

printf("No");
return 0;
}
if(S[0]==’A’&&S[1]==’B’&&S[2]==’A

(cid:44)→ ’){

printf("No");
return 0;
}
if(S[0]==’A’&&S[1]==’B’&&S[2]==’A

(cid:44)→ ’){

printf("No");
return 0;
}
if(S[0]==’A’&&S[1]==’B’&&S[2]==’B

(cid:44)→ ’){

printf("No");
return 0;
}
printf("Yes");
return 0;
}

Source Scala code

Scala → Python translation
Correct Python translation

Incorrect Python translation

import scala.io.Source

object Main extends App {
val lines: Iterator[String] =

(cid:44)→ Source.stdin.getLines()
val line = lines.next.split(" ")

(cid:44)→ .map(_.toInt).take(2)

val a = line.head
val b = line(1)
println((if (b >= a) a else a -

(cid:44)→ 1).toString)

}

import sys
lines = sys.stdin.readlines()
for line in lines:

import sys
import os
f = lambda:list(map(int,input().

a, b = map(int, line.split()

(cid:44)→ split()))

(cid:44)→ )
if b >= a:

print(a)

else:

print(b - 1)

if ’local’ in os.environ :

sys.stdin = open(’./input.
(cid:44)→ txt’, ’r’)

def solve():

a = f()[0]
b = f()[0]
print(a if b >= a else a -

(cid:44)→ 1)

solve()

22

