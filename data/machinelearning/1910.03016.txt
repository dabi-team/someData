0
2
0
2

b
e
F
8
2

]

G
L
.
s
c
[

4
v
6
1
0
3
0
.
0
1
9
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2020

IS A GOOD REPRESENTATION SUFFICIENT FOR SAM-
PLE EFFICIENT REINFORCEMENT LEARNING?

Simon S. Du
Institute for Advanced Study
ssdu@ias.edu

Sham M. Kakade
University of Washington, Seattle
sham@cs.washington.edu

Ruosong Wang
Carnegie Mellon University
ruosongw@andrew.cmu.edu

Lin F. Yang
University of California, Los Angles
linyang@ee.ucla.edu

ABSTRACT

Modern deep learning methods provide effective means to learn good represen-
tations. However, is a good representation itself sufﬁcient for sample efﬁcient
reinforcement learning? This question has largely been studied only with respect
to (worst-case) approximation error, in the more classical approximate dynamic
programming literature. With regards to the statistical viewpoint, this question is
largely unexplored, and the extant body of literature mainly focuses on conditions
which permit sample efﬁcient reinforcement learning with little understanding of
what are necessary conditions for efﬁcient reinforcement learning.
This work shows that, from the statistical viewpoint, the situation is far subtler
than suggested by the more traditional approximation viewpoint, where the re-
quirements on the representation that sufﬁce for sample efﬁcient RL are even more
stringent. Our main results provide sharp thresholds for reinforcement learning
methods, showing that there are hard limitations on what constitutes good function
approximation (in terms of the dimensionality of the representation), where we
focus on natural representational conditions relevant to value-based, model-based,
and policy-based learning. These lower bounds highlight that having a good (value-
based, model-based, or policy-based) representation in and of itself is insufﬁcient
for efﬁcient reinforcement learning, unless the quality of this approximation passes
certain hard thresholds. Furthermore, our lower bounds also imply exponential
separations on the sample complexity between 1) value-based learning with perfect
representation and value-based learning with a good-but-not-perfect representation,
2) value-based learning and policy-based learning, 3) policy-based learning and
supervised learning and 4) reinforcement learning and imitation learning.

1

INTRODUCTION

Modern reinforcement learning (RL) problems are often challenging due to the huge state space.
To tackle this challenge, function approximation schemes are often employed to provide a compact
representation, so that reinforcement learning can generalize across states. A common paradigm
is to ﬁrst use a feature extractor to transform the raw input to features (a succinct representation)
and then apply a linear predictor on top of the features. Traditionally, the feature extractor is often
handcrafted (Sutton & Barto, 2018), while more modern methods often train a deep neural network
to extract features. The hope of this paradigm is that, if there exists a good low dimensional (linear)
representation, then efﬁcient reinforcement learning is possible.

Empirically, combining various RL function approximation algorithms with neural networks for
feature extraction has lead to tremendous successes on various tasks (Mnih et al., 2015; Schulman
et al., 2015; 2017). A major problem, however, is that these methods often require a large amount of
samples to learn a good policy. For example, deep Q-network requires millions of samples to solve
certain Atari games (Mnih et al., 2015). Here, one may wonder if there are fundamental statistical

1

 
 
 
 
 
 
Published as a conference paper at ICLR 2020

limitations on such methods, and, if so, under what conditions it would be possible to efﬁciently learn
a good policy?

In the supervised learning context, it is well-known that empirical risk minimization is a statistically
efﬁcient method when using a low-complexity hypothesis space (Shalev-Shwartz & Ben-David,
2014), e.g. a hypothesis space with bounded VC dimension. For example, polynomial number
of samples sufﬁce for learning a near-optimal d-dimensional linear classiﬁer, even in the agnostic
setting1. In contrast, in the more challenging RL setting, we seek to understand if efﬁcient learning
is possible (say from a sample complexity perspective) when we have access to an accurate (and
compact) parametric representation — e.g. our policy class contains a near-optimal policy or our
hypothesis class accurately approximates the optimal value function. In particular, this work focuses
on the following question:

Is a good representation sufﬁcient for sample-efﬁcient reinforcement learning?

This question has largely been studied only with respect to approximation error in the more classical
approximate dynamic programming literature, where it is known that algorithms are stable to certain
worst-case approximation errors. With regards to sample efﬁciency, this question is largely unexplored,
where the extant body of literature mainly focuses on conditions which are sufﬁcient for efﬁcient
reinforcement learning though there is little understanding of what are necessary conditions for
efﬁcient reinforcement learning. In reinforcement learning, there is no direct analogue of empirical
risk minimization as in the supervised learning context, and it is not evident what are the statistical
limits of learning based on properties of our underlying hypothesis class (which may be value-based,
policy-based, or model-based).

Many recent works have provided polynomial upper bounds under various sufﬁcient conditions, and
in what follows we list a few examples. For value-based learning, the work of Wen & Van Roy (2013)
showed that for deterministic systems2, if the optimal Q-function can be perfectly predicted by linear
functions of the given features, then the agent can learn the optimal policy exactly with polynomial
number of samples. Recent work (Jiang et al., 2017) further showed that if certain complexity
measure called Bellman rank is bounded, then the agent can learn a near-optimal policy efﬁciently.
For policy-based learning, Agarwal et al. (2019) gave polynomial upper bounds which depend on a
parameter that measures the difference between the initial distribution and the distribution induced by
the optimal policy.

Our Contributions. This paper gives, perhaps surprisingly, strong negative results to this question.
The main results are exponential lower bounds in terms of planning horizon H for value-based, model-
based, and policy-based algorithms with given good representations3. Notably, the requirements
on the representation that sufﬁce for sample efﬁcient RL are even more stringent than the more
traditional approximation viewpoint. A comprehensive summary of previous upper bounds and our
lower bounds is given in Table 1, and here we brieﬂy summarize our hardness results.

1. For value-based learning, we show even if Q-functions of all policies can be approximated
(cid:17)

by linear functions of the given representation with approximation error δ = Ω
where d is the dimension of the representation and H is the planning horizon, then the agent
still needs to sample exponential number of trajectories to ﬁnd a near-optimal policy.

(cid:16)(cid:113) H
d

2. For model-based learning, we show even if the transition matrix and the reward function
can be approximated by linear functions of the given representation with approximation

sense), the agent still needs to sample exponential number of

(cid:17)

(cid:16)(cid:113) H
d

error δ = Ω
trajectories to ﬁnd a near-optimal policy.

(in (cid:96)

∞

3. We show even if optimal policy can be perfectly predicted by a linear function of the given
representation with a strictly positive margin, the agent still requires exponential number of
trajectories to ﬁnd a near-optimal policy.

1Here we only study the sample complexity and ignore the computational complexity.
2MDPs where both reward and transition are deterministic.
3 Our results can be easily extend to inﬁnite horizon MDPs with discount factors by replacing the planning
1−γ , where γ is the discount factor. We omit the discussion on discount MDPs for simplicity.

horizon H with 1

2

Published as a conference paper at ICLR 2020

These lower bounds hold even in deterministic systems and even if the agent knows the transition
model. Note these negative results apply to the case where the Q-function, the model, or the optimal
policy can be predicted well by a linear function of the given representation. Since the class of linear
functions is a strict subset of many more complicated function classes, including neural networks in
particular, our negative results imply lower bounds for these more complex function classes as well.
Our results highlight the following conceptual insights:

• The requirements on the representation that sufﬁce for sample efﬁcient RL are signiﬁcantly
more stringent than the more traditional approximation viewpoint; our statistical lower
bounds show that there are hard thresholds on the worst-case approximation quality of the
representation which are not necessary from the approximation viewpoint.

• Since our lower bounds apply even when the agent knows the transition model, the hardness
is not due to the difﬁculty of exploration in the standard sense. The unknown reward function
is sufﬁcient to make the problem exponentially difﬁcult.

• Our lower bounds are not due to the agent’s inability to perform efﬁcient supervised learning,
since our assumptions do admit polynomial sample complexity upper bounds if the data
distribution is ﬁxed.

• Our lower bounds are not pathological in nature and suggest that these concerns may arise
in practice. In a precise sense, almost all feature extractors induce a hard MDP instance in
our construction (see Section 4.4).

Instead, one interpretation is that the hardness is due to a distribution mismatch in the following sense:
the agent does not know which distribution to use for minimizing a (supervised) learning error (see
Kakade (2003) for discussion), and even a known transition model is not information-theoretically
sufﬁcient to reduce the sample complexity.

Furthermore, our work implies several interesting exponential separations on the sample complexity
between: 1) value-based learning with perfect representation and value-based learning with a good-
but-not-perfect representation, 2) value-based learning and policy-based learning, 3) policy-based
learning and supervised learning and 4) reinforcement learning and imitation learning. We provide
more details in Section 5.

2 RELATED WORK

A summary of previous upper bounds, together with lower bounds proved in this paper, is provided
in Table 1. Some key assumptions are formally stated in Section 3 and Section 4. Our lower bounds
highlight that classical complexity measures in supervised learning including small approximation
error and margin, and standard assumptions in reinforcement learning including optimality gap
and deterministic systems, are not enough for efﬁcient RL with function approximation. We need
additional assumptions, e.g., ones used in previous upper bounds, for efﬁcient RL.

2.1 PREVIOUS LOWER BOUNDS

Existing exponential lower bounds, to our knowledge, construct unstructured MDPs with an ex-
ponentially large state space and reduce a bandit problem with exponentially many arms to an
MDP (Krishnamurthy et al., 2016; Sun et al., 2017). However, these lower bounds cannot apply to
MDPs whose transition models, value functions, or policies can be approximated with some natural
function classes, e.g., linear functions, neural networks, etc. The current paper gives the ﬁrst set of
lower bounds for RL with linear function approximation (and thus also hold for super classes of
linear functions such as neural networks).

2.2 PREVIOUS UPPER BOUNDS

We divide previous algorithms (with provable guarantees) into three classes: those that utilize
uncertainty-based bonuses (e.g. UCB variants or Thompson sampling variants); approximate dynamic
programming variants (which often make assumptions with respect to concentrability coefﬁcients);
and direct policy search-based methods (such as conserve policy iteration (CPI, see Kakade (2003)) or
policy gradient methods, which make assumptions with respect to distribution mismatch coefﬁcients).

3

Published as a conference paper at ICLR 2020

Query Oracle

RL Generative Model Known Transition

Previous Upper Bounds

Exact linear Q∗ + DetMDP (Wen & Van Roy, 2013)

Exact linear Q∗ + Bellman-Rank (Jiang et al., 2017)

Exact Linear Q∗ + Low Var + Gap (Du et al., 2019a)

Exact Linear Q∗ + Gap (Open Problem / Theorem C.1)
Exact Linear Qπ for all π (Open Problem / Theorem D.1)
Approx. Linear Qπ for all π +
Concentratability (Munos, 2005; Antos et al., 2008)
Approx. Linear Qπ for all π +
Bounded Dist Mismatch Coeff (Kakade & Langford, 2002)

Lower Bounds (this work)

Approx Linear Q∗ (Theorem 4.1)
Approx Linear Qπ for all π (Theorem 4.1)

Approx Linear MDP (Theorem 4.2)

(cid:96)
∞

(cid:88)

(cid:88)

(cid:88)

?

?
(cid:88)×

(cid:88)×

×

×

×

Exact Linear π∗ + Margin + Gap + DetMDP (Theorem 4.3) ×

Exact Linear Q∗ (Open Problem)

?

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

×

×

×

×

?

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

×

×

×

×

?

(cid:17)

(cid:16)(cid:113) H
d

Table 1: Summary of theoretical results on reinforcement learning with linear function approximation.
See Section 2 for discussion on this table. RL, Generative Model, Known Transition are deﬁned
in Section 3.3. Exact linear Q∗: Assumption 4.1 with δ = 0. Approx linear Q∗: Assumption 4.1
. Exact linear π∗: Assumption 4.4. Margin: Assumption 4.5. Exact Linear Qπ
with δ = Ω
for all π: Assumption 4.2 with δ = 0. Approximate Linear Qπ for all π: Assumption 4.2 with
. DetMDP: deterministic system deﬁned in Section 3.1. Bellman-rank: Deﬁnition 5 in
δ = Ω
Jiang et al. (2017). Low Var: Assumption 1 in Du et al. (2019b). Gap: Assumption 3.1. Bounded
Approx Linear
Distribution Mismatch Coefﬁcient: Deﬁnition 3.3 in Agarwal et al. (2019). (cid:96)
. (cid:88): there exists an algorithm with polynomial sample
MDP: Assumption 4.3 with δ = Ω
complexity to ﬁnd a near-optimal policy. (cid:88)×: requires certain condition on the initial distribution. ×:
exponential number of samples is required. ?: open problem.

(cid:16)(cid:113) H
d

(cid:16)(cid:113) H
d

(cid:17)

(cid:17)

∞

4

Published as a conference paper at ICLR 2020

The ﬁrst class of methods include those based on witness rank, Belman rank, and the Eluder dimension,
while the latter two classes of algorithms make assumptions either on concentrability coefﬁcients or
on distribution mismatch coefﬁcients (see Agarwal et al. (2019); Scherrer (2014) for discussions).

Uncertainty bonus-based algorithms. Now we discuss existing theoretical results on value-based
learning with function approximation. The most relevant work is Wen & Van Roy (2013) which
showed in deterministic systems, if the optimal Q-function is within a pre-speciﬁed function class
which has bounded Eluder dimension, for which the class of linear functions is a special case, then the
agent can learn the optimal policy using polynomial number of samples. This result has recently been
generalized by Du et al. (2019a) which can deal with stochastic reward and low variance transition
but requires strictly positive optimality gap. As we listed in Table 1, it is an open problem whether
the condition that the optimal Q-function is linear itself is sufﬁcient for efﬁcient RL.

Li et al. (2011) proposed a Q-learning algorithm which requires the Know-What-It-Knows oracle.
However, it is in general unknown how to implement such oracle in practice. Jiang et al. (2017)
proposed the concept of Bellman Rank to characterize the sample complexity of value-based learning
methods and gave an algorithm that has polynomial sample complexity in terms of the Bellman Rank,
though the proposed algorithm is not computationally efﬁcient. Bellman rank is bounded for a wide
range of problems, including MDP with small number of hidden states, linear MDP, LQR, etc. Later
work gave computationally efﬁcient algorithms for certain special cases (Dann et al., 2018; Du et al.,
2019a; Yang & Wang, 2019b; Jin et al., 2019). Recently, Witness rank, a generalization of Bellman
rank to model-based methods, is studied in Sun et al. (2019).

Approximate dynamic programming-based algorithms. We now discuss approximate dynamic
programming-based results characterized in terms of the concentrability coefﬁcient. While classical
approximate dynamic programming results typically require (cid:96)
-bounded errors, the notion of
concentrability (originally due to (Munos, 2005)) permits sharper bounds in terms of average-case
function approximation error, provided that the concentrability coefﬁcient is bounded (e.g. see
Munos (2005); Szepesv´ari & Munos (2005); Antos et al. (2008); Geist et al. (2019)). Under the
assumption that this problem-dependent parameter is bounded, Munos (2005); Szepesv´ari & Munos
(2005) and Antos et al. (2008) proved sample complexity and error bounds for approximate dynamic
programming methods when there is a data collection policy (under which value-function ﬁtting
occurs) that induces a ﬁnite concentrability coefﬁcient. The assumption that the concentrability
coefﬁcient is ﬁnite is in fact quite limiting. See Chen & Jiang (2019) which provides a more detailed
discussion on this quantity.

∞

Direct policy search-based algorithms.
Stronger guarantees over approximate dynamic
programming-based algrithm can be obtained with direct policy search-based methods, where instead
of having a bounded concentrability coefﬁcient, one only needs to have a bounded distribution
mismatch coefﬁcient. The latter assumption requires the agent to have access to a “good” initial state
distribution (e.g. a measure which has coverage over where an optimal policy tends to visit); note
that this assumption does not make restrictions over the class of MDPs. There are two classes of algo-
rithms that fall into this category. First, there is Conservative Policy Iteration (Kakade & Langford,
2002), along with Policy Search by Dynamic Programming (PSDP) (Bagnell et al., 2004), and other
boosting-style of policy search-based methods Scherrer & Geist (2014); Scherrer (2014), which have
guarantees in terms of bounded distribution mismatch ratio. Second, more recently, Agarwal et al.
(2019) showed that policy gradient styles of algorithms also have comparable guarantees.

Recent extensions. Subsequent to this work, the work by Van Roy & Dong (2019) and Lattimore &
Szepesvari (2019) made notable contributions to the misspeciﬁed linear bandit problem. In particular,
both papers found that Theorem 4.1 in our paper can be extended to the misspeciﬁed linear bandit
problem and gave upper bounds for this problem showing that our lower bound has tight dependency
on δ and d. Lattimore & Szepesvari (2019) further gave an upper bound for the setting where the
Q-functions of all policies can be approximated by linear functions with small approximation errors
and the agent can interact with the environment using a generative model. This upper bound also
demonstrates that our lower bound has tight dependency on δ and d.

3 PRELIMINARIES

Throughout this paper, for a given integer H, we use [H] to denote the set {0, 1, . . . , H − 1}.

5

Published as a conference paper at ICLR 2020

3.1 EPISODIC REINFORCEMENT LEARNING

Let M = (S, A, H, P, R) be an Markov Decision Process (MDP) where S is the state space,
A is the action space whose size is bounded by a constant, H ∈ Z+ is the planning horizon,
P : S × A → (cid:52) (S) is the transition function which takes a state-action pair and returns a distribution
over states and R : S × A → (cid:52) (R) is the reward distribution. Without loss of generality, we assume
4. A policy π : S → (cid:52)(A) prescribes a distribution over actions for each
a ﬁxed initial state s0
state. The policy π induces a (random) trajectory s0, a0, r0, s1, a1, r1, . . . , sH
1 where
a0 ∼ π(s0), r0 ∼ R(s0, a0), s1 ∼ P (s0, a0), a1 ∼ π(s1), etc. To streamline our analysis, for each
h ∈ [H], we use Sh ⊆ S to denote the set of states at level h, and we assume Sh do not intersect
with each other. We also assume (cid:80)H
1
h=0 rh ∈ [0, 1] almost surely. Our goal is to ﬁnd a policy π that
−
(cid:105)
(cid:104)(cid:80)H
maximizes the expected total reward E
. We use π∗ to denote the optimal policy. We
(cid:104)(cid:80)H

(cid:104)(cid:80)H

1, aH

1, rH

(cid:105)

−

−

−

1

−

1
h=0 rh | π
(cid:105)
≥ E

say a policy π is ε-optimal if E

− ε.

1
h=0 rh | π

−

h=0 rh | π∗

−

In this paper we prove lower bounds for deterministic systems, i.e., MDPs with deterministic
transition P , deterministic reward R. In this setting, P and R can be regarded as functions instead
of distributions. Since deterministic systems are special cases of general stochastic MDPs, lower
bounds proved in this paper still hold for more general MDPs.

3.2 Q-FUNCTION AND OPTIMALITY GAP

An important concept in RL is the Q-function. Given a policy π, a level h ∈ [H] and a state-action
(cid:105)
pair (s, a) ∈ Sh × A, the Q-function is deﬁned as Qπ
h(cid:48)=h rh(cid:48) | sh = s, ah = a, π
.
For simplicity, we denote Q∗h(s, a) = Qπ∗
h (s, a). In addition to these deﬁnitions, we list below an
important assumption, the optimality gap assumption, which is widely used in reinforcement learning
and bandit literature. To state the assumption, we ﬁrst deﬁne the function gap : S × A → R as
gap(s, a) = arg maxa(cid:48)
Assumption 3.1 (Optimality Gap). There exists ρ > 0 such that ρ ≤ gap(s, a) for all (s, a) ∈ S × A
with gap(s, a) > 0.

Q∗(s, a(cid:48)) − Q∗(s, a). Now we formally state the assumption.

h(s, a) = E

(cid:104)(cid:80)H

∈A

−

1

Here, ρ is the smallest reward-to-go difference between the best set of actions and the rest. Recently,
Du et al. (2019b) gave a provably efﬁcient Q-learning algorithm based on this assumption and
Simchowitz & Jamieson (2019) showed that with this condition, the agent only incurs logarithmic
regret in the tabular setting.

3.3 QUERY MODELS

Here we discuss three possible query oracles interacting with the MDP.

• RL: The most basic and weakest query oracle for MDP is the standard reinforcement
learning query oracle where the agent can only interact with the MDP by choosing actions
and observe the next state and the reward.

• Generative Model: A stronger query model assumes the agent can transit to any
state (Kearns & Singh, 2002; Kakade, 2003; Sidford et al., 2018). This query model
is available in certain robotic applications where one can control the robot to reach the target
state.

• Known Transition: The strongest query model considered is that the agent can not only
transit to any state, but also knows the whole transition function. In this model, only the
reward is unknown.

In this paper, we will prove lower bounds for the strongest Known Transition query oracle. Therefore,
our lower bounds also apply to RL and Generative Model query oracles.

4Some papers assume the initial state is sampled from a distribution P1. Note this is equivalent to assuming
a ﬁxed initial state s0, by setting P (s0, a) = P1 for all a ∈ A and now our state s1 is equivalent to the initial
state in their assumption.

6

Published as a conference paper at ICLR 2020

4 MAIN RESULTS

In this section we formally present our lower bounds. We also discuss proof ideas in Section 4.4.

4.1 LOWER BOUND FOR VALUE-BASED LEARNING

We ﬁrst present our lower bound for value-based learning. A common assumption is that the Q-
function can be predicted well by a linear function of the given features (representation) (Bertsekas
& Tsitsiklis, 1996). Formally, the agent is given a feature extractor φ : S × A → Rd which can be
hand-crafted or a pre-trained neural network that transforms a state-action pair to a d-dimensional
embedding. The following assumption states that the given feature extractor can be used to predict
the Q-function with approximation error at most δ using a linear function.
Assumption 4.1. There exists δ > 0 and θ0, θ1, . . . , θH
(s, a) ∈ Sh × A, |Q∗h (s, a) − (cid:104)θh, φ (s, a)(cid:105)| ≤ δ.

1 ∈ Rd such that for any h ∈ [H] and any

−

Here δ is the approximation error, which indicates the quality of the representation. If δ = 0, then
Q-function can be perfectly predicted by a linear function of φ (·, ·). In general, δ becomes smaller as
we increase the dimension of φ, since larger dimension usually has more expressive power. When the
feature extractor is strong enough, previous papers (Chen & Jiang, 2019; Farahmand, 2011) assume
that linear functions of φ can approximate the Q-function of any policy.
Assumption 4.2 (Policy Completeness). There exists δ > 0, such that for any h ∈ [H] and any
policy π, there exists θπ

h ∈ Rd such that for any (s, a) ∈ Sh × A, |Qπ

h (s, a) − (cid:104)θh, φ (s, a)(cid:105)| ≤ δ.

In the theoretical reinforcement learning literature, Assumption 4.2 is often called the (approximate)
policy completeness assumption. This assumption is crucial in proving polynomial sample complexity
guarantee for value iteration type of algorithms (Chen & Jiang, 2019; Farahmand, 2011).
(cid:16)(cid:113) H
d

The following theorem shows when δ = Ω
trajectories to ﬁnd a near-optimal policy.
Theorem 4.1 (Exponential Lower Bound for Value-based Learning). There exists a family of MDPs
with |A| = 2 and a feature extractor φ that satisfy Assumption 4.2, such that any algorithm that
returns a 1/2-optimal policy with probability 0.9 needs to sample Ω (cid:0)min{|S|, 2H , exp(dδ2/16)}(cid:1)
trajectories.

, the agent needs to sample exponential number of

(cid:17)

Note this lower bound also applies to MDPs that satisfy Assumption 4.1, since Assumption 4.2
is strictly stronger. We would like to emphasize that since linear functions is a subclass of more
complicated function classes, e.g., neural networks, our lower bound also holds for these function
classes. Moreover, in many scenarios, the feature extractor φ is the last layer of a neural network.
Modern neural networks are often over-parameterized, which makes d large. In this case, d is much
larger than H. Thus, our lower bound holds even if the representation has small approximation
error. Furthermore, the assumption that |A| = 2 is only for simplicity. Our lower bound can be
easily generalized to the case that |A| > 2, in which case the sample complexity lower bound is
Ω (cid:0)min{|S|, |A|H , exp(dδ2/16)}(cid:1).

4.2 LOWER BOUND FOR MODEL-BASED LEARNING

Here we present our lower bound for model-based learning. Recently, Yang & Wang (2019b)
proposed the linear transition assumption which was later studied in Yang & Wang (2019a); Jin et al.
(2019). Again, we assume the agent is given a feature extractor φ : S × A → Rd, and now we state
the assumption formally as follow.
1 ∈ Rd
Assumption 4.3 (Approximate Linear MDP). There exists δ > 0, β0, β1, . . . , βH
and ψ : S → Rd such that for any h ∈ [H − 1], (s, a) ∈ Sh × A and s(cid:48) ∈ Sh+1,
|P (s(cid:48) | s, a) − (cid:104)ψ(s(cid:48)), φ (s, a)(cid:105)| ≤ δ and |E[R(s, a)] − (cid:104)βh, φ(s, a)(cid:105)| ≤ δ.

−

It has been shown in Yang & Wang (2019b;a); Jin et al. (2019) if (cid:107)P (· | s, a) − (cid:104)ψ(·), φ (s, a)(cid:105)(cid:107)1 is
bounded, then the problem admits an algorithm with polynomial sample complexity. Now we show

that when δ = Ω
a near-optimal policy.

(cid:17)

(cid:16)(cid:113) H
d

in Assumption 4.3, the agent needs exponential number of samples to ﬁnd

7

Published as a conference paper at ICLR 2020

Theorem 4.2 (Exponential Lower Bound for Linear Transition Model). There exists a family of
MDPs with |A| = 2 and a feature extractor φ that satisfy Assumption 4.3, such that any algorithm that
returns a 1/2-optimal policy with probability 0.9 needs to sample Ω (cid:0)min{|S|, 2H , exp(dδ2/16)}(cid:1)
trajectories.

Again, our lower bound can be easily generalized to the case that |A| > 2.

We do note that an (cid:96)
approximation for a transition matrix may be a weak condition. Under the
stronger condition that the transition matrix can be approximated well under the total variational
distance, there exists polynomial sample complexity upper bounds that can tolerate approximation
errors (Yang & Wang, 2019b;a; Jin et al., 2019).

∞

4.3 LOWER BOUND FOR POLICY-BASED LEARNING

Next we present our lower bound for policy-based learning. This class of methods use function
approximation on the policy and use optimization techniques, e.g., policy gradient, to ﬁnd the optimal
policy. In this paper, we focus on linear policies on top of a given representation. A linear policy
(cid:104)θh, φ(sh, a)(cid:105) where sh ∈ Sh, φ (·, ·) is a given
π is a policy of the form π(sh) = arg maxa
feature extractor and θh ∈ Rd is the linear coefﬁcient. Note that applying policy gradient on softmax
parameterization of the policy is indeed trying to ﬁnd the optimal policy among linear policies.

∈A

Similar to value-based learning, a natural assumption for policy-based learning is that the optimal
policy is realizable5, i.e., the optimal policy is linear.
Assumption 4.4. For any h ∈ [H], there exists θh ∈ Rd that satisﬁes for any s ∈ Sh, we have
π∗ (s) ∈ arg maxa (cid:104)θh, φ (s, a)(cid:105) .

Here we discuss another assumption. For learning a linear classiﬁer in the supervised learning setting,
one can reduce the sample complexity signiﬁcantly if the optimal linear classiﬁer has a margin.
Assumption 4.5. We assume φ (s, a) ∈ Rd satisﬁes (cid:107)φ(s, a)(cid:107)2 = 1 for any (s, a) ∈ S × A. For any
h ∈ [H], there exists θh ∈ Rd with (cid:107)θh(cid:107)2 = 1 and (cid:52) > 0 such that for any s ∈ Sh, there is a unique
optimal action π∗(s), and for any a (cid:54)= π∗(s), (cid:104)θh, φ (s, π∗(s))(cid:105) − (cid:104)θh, φ (s, a)(cid:105) ≥ (cid:52).

Here we restrict the linear coefﬁcients and features to have unit norm for normalization. Note that
Assumption 4.5 is strictly stronger than Assumption 4.4. Now we present our result for linear policy.
Theorem 4.3 (Exponential Lower Bound for Policy-based Learning). There exists an absolute
constant (cid:52)0, such that for any (cid:52) ≤ (cid:52)0, there exists a family of MDPs with |A| = 2 and a
and Assumption 4.5, such
feature extractor φ that satisfy Assumption 3.1 with ρ =
that any algorithm that returns a 1/4-optimal policy with probability at least 0.9 needs to sample
Ω (cid:0)min{2H , 2d}(cid:1) trajectories.

1
2 min

H,d

{

}

Again, our lower bound can be easily generalized to the case that |A| > 2.

Compared with Theorem 4.1, Theorem 4.3 is even more pessimistic, in the sense that even with
perfect representation with benign properties (gap and margin), the agent still needs to sample
exponential number of samples. It also suggests that policy-based learning could be very different
from supervised learning.

4.4 PROOF IDEAS

The binary tree hard instance. All our lower bound are proved based on reductions from the
following hard instance. In this instance, both the transition P and the reward R are deterministic.
There are H levels of states, which form a full binary tree of depth H. There are 2h states in level h,
and thus 2H − 1 states in total. Among all the 2H
1 states in level H − 1, there is only one state with
−
reward R = 1, and for all other states in the MDP, the corresponding reward value R = 0. Intuitively,
to ﬁnd a 1/2-optimal policy for such MDPs, the agent must enumerate all possible states in level
H − 1 to ﬁnd the state with reward R = 1. Doing so intrinsically induces a sample complexity of
Ω(2H ). This intuition is formalized in Theorem A.1 using Yao’s minimax principle (Yao, 1977).

5 Unlike value-based learning, it is hard to deﬁne completeness on the policy-based learning with function

approximation, since not all policy has the arg max form.

8

Published as a conference paper at ICLR 2020

∞

∞

∞

n, (cid:107)A − B(cid:107)

to denote the entry-wise (cid:96)

Lower bound for value-based and model-based learning We now show how to construct a set of
features so that Assumption 4.1-4.3 hold. Our main idea is to the utilize the following fact regarding
the identity matrix: ε-rank(I2H ) ≤ O(H/ε2). Here for a matrix A ∈ Rn
n, its ε-rank (a.k.a
×
approximate rank) is deﬁned to be min{rank(B) : B ∈ Rn
≤ ε}, where we use
×
norm of a matrix. The upper bound ε-rank(In) ≤ O(log n/ε2)
(cid:107) · (cid:107)
was ﬁrst proved in Alon (2009) using the Johnson-Lindenstrauss Lemma (Johnson & Lindenstrauss,
1984), and we also provide a proof in Lemma A.1. The concept of ε-rank has wide applications in
theoretical computer science (Alon, 2009; Barak et al., 2011; Alon et al., 2013; 2014; Chen & Wang,
2019), but to our knowledge, this is the ﬁrst time that it appears in reinforcement learning.
O(H/ε2) such that (cid:107)I2H −
This fact can be alternatively stated as follow: there exists Φ ∈ R2H
ΦΦ(cid:62)(cid:107)
≤ ε. We interpret each row of Φ as the feature of a state in the binary tree. By construction
of Φ, now features of states in the binary tree have a nice property that (i) each feature vector has
approximately unit norm and (ii) different feature vector are nearly orthogonal. Using this set of
features, we can now show that Assumption 4.1-4.3 hold. Here we prove Assumption 4.1 holds as
an example and prove other assumptions also hold in the appendix. To prove Assumption 4.1, we
note that in the binary tree hard instance, for each level h, only a single state satisﬁes Q∗ = 1, and all
other states satisfy Q∗ = 0. We simply take θh to be the feature of the state with Q∗ = 1. Since all
feature vectors are nearly orthogonal, Assumption 4.1 holds.

∞

×

Since the above fact regarding the ε-rank of the identity matrix can be proved by simply taking each
row of Φ to be a random unit vector, our lower bound reveals another intriguing (yet pessimistic)
aspect of Assumption 4.1-4.3: for the binary tree instance, almost all feature extractors induce a
hard MDP instance. This again suggests that a good representation itself may not necessarily lead to
efﬁcient RL and additional assumptions (e.g. on the reward distribution) could be crucial.

Lower bound for policy-based learning.
It is straightfoward to construct a set of feature vectors
for the binary tree instance so that Assumption 4.4 holds, even if d = 1. We set φ(s, a) to be +1 if
a = a1 and −1 if a = a2. For each level h, for the unique state s in level h with Q∗ = 1, we set θh
to be 1 if π∗(s) = a1 and −1 if π∗(s) = a2. With this construction, Assumption 4.4 holds.

−

To prove that the lower bound under Assumption 4.5, we use a new reward function for states in
level H − 1 in the binary tree instance above so that there exists a unique optimal action for each
state in the MDP. See Figure 2 for an example with H = 3 levels of states. Another nice property
of the new reward function is that for all states s we always have π∗(s) = a1. Now, we deﬁne
2H
1 different new MDPs as follow: for each state in level H − 1, we change its original reward
(deﬁned in Figure 2) to 1. An exponential sample complexity lower bound for these MDPs can be
proved using the same argument as the original binary tree hard instance, and now we show this set
of MDPs satisfy Assumption 4.5. We ﬁrst show in Lemma A.2 that there exists a set N ⊆ Sd
1 with
|N | = (1/(cid:52))Ω(d), so that for each p ∈ N , there exists a hyperplane L that separates p and N \ {p},
and all vectors in N have distance at least (cid:52) to L. Equivalently, for each p ∈ N ,we can always deﬁne
a linear function fp so that fp(p) ≥ (cid:52) and fp(q) ≤ −(cid:52) for all q ∈ N \ {p}. This can be proved
using standard lower bounds on the size of ε-nets. Now we simply use vectors in N as features of
states. By construction of the reward function, for each level h, there could only be two possible
cases for the optimal policy π∗. I.e., either π∗(s) = a1 for all states in level h, or π∗(s) = a2 for a
unique state s and π∗(s(cid:48)) = a1 for all s (cid:54)= s(cid:48). In both cases, we can easily deﬁne a linear function
with margin (cid:52) to implement the optimal policy π∗, and thus Assumption 4.5 holds. Notice that in
this proof, we critically relies on d = Θ(H), so that we can utilize the curse of dimensionality to
construct a large set of vectors as features.

−

5 SEPARATIONS

Perfect representation vs. good-but-not-perfect representation. For value-based learning in
deterministic systems, Wen & Van Roy (2013) showed polynomial sample complexity upper bound
when the representation can perfectly predict the Q-function. In contrast, if the representation is only
able to approximate the Q-function, then the agent requires exponential number of trajectories. This
exponential separation demonstrates a provable exponential beneﬁt of better representation.

Value-based learning vs. policy-based learning. Note that if the optimal Q-function can be
perfectly predicted by the provided representation, then the optimal policy can also be perfectly

9

Published as a conference paper at ICLR 2020

predicted using the same representation. Since Wen & Van Roy (2013) showed polynomial sample
complexity upper bound when the representation can perfectly predict the Q-function, our lower
bound on policy-based learning, which applies to perfect representations, thus demonstrates that the
ability of predicting the Q-function is much stronger than that of predicting the optimal policy.

Supervised learning vs. reinforcement learning. For policy-based learning, if the planning horizon
H = 1, the problem becomes learning a linear classiﬁer, for which there are polynomial sample
complexity upper bounds. For policy-based learning, the agent needs to learn H linear classiﬁers
sequentially. Our lower bound on policy-based learning shows the sample complexity dependency on
H is exponential.

Imitation learning vs. reinforcement learning. In imitation learning (IL), the agent can observe
trajectories induced by the optimal policy (expert). If the optimal policy is linear in the given
representation, it can be shown that the simple behavior cloning algorithm only requires polynomial
number of samples to ﬁnd a near-optimal policy (Ross et al., 2011). Our Theorem 4.3 shows if the
agent cannot observe expert’s behavior, then it requires exponential number of samples. Therefore,
our lower bound shows there is an exponential separation between policy-based RL and IL when
function approximation is used.

6 ACKNOWLEDGMENTS

The authors would like to thank Yuping Luo, Wenlong Mou, Martin Wainwright, Mengdi Wang and
Yifan Wu for insightful discussions. Also, the authors would also like to gratefully acknowledge
Benjamin Van Roy, Shi Dong, Tor Lattimore and Csaba Szepesv´ari for sharing a draft of their
work and their comments. Simon S. Du is supported by NSF grant DMS-1638352 and the Infosys
Membership. Sham M. Kakade acknowledges funding from the Washington Research Foundation
Fund for Innovation in Data-Intensive Discovery; the NSF award CCF 1740551; and the ONR award
N00014-18-1-2247. Ruosong Wang is supported in part by NSF IIS1763562, AFRL CogDeCON
FA875018C0014, and DARPA SAGAMORE HR00111990016. Part of this work was done while
Simon S. Du was visiting Google Brain Princeton and Ruosong Wang was visiting Princeton
University.

REFERENCES

Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation
with policy gradient methods in markov decision processes. arXiv preprint arXiv:1908.00261,
2019.

Noga Alon. Perturbed identity matrices have high rank: Proof and applications. Combinatorics,

Probability and Computing, 18(1-2):3–15, 2009.

Noga Alon, Troy Lee, Adi Shraibman, and Santosh Vempala. The approximate rank of a matrix
and its algorithmic applications: approximate rank. In Proceedings of the forty-ﬁfth annual ACM
symposium on Theory of computing, pp. 675–684. ACM, 2013.

Noga Alon, Troy Lee, and Adi Shraibman. The cover number of a matrix and its algorithmic
applications. Approximation, Randomization, and Combinatorial Optimization. Algorithms and
Techniques, pp. 34, 2014.

Andr´as Antos, Csaba Szepesv´ari, and R´emi Munos. Learning near-optimal policies with bellman-
residual minimization based ﬁtted policy iteration and a single sample path. Machine Learning, 71
(1):89–129, 2008.

Baruch Awerbuch and Robert Kleinberg. Online linear optimization and adaptive routing. Journal of

Computer and System Sciences, 74(1):97–114, 2008.

J. A. Bagnell, Sham M Kakade, Jeff G. Schneider, and Andrew Y. Ng. Policy search by dynamic
programming. In S. Thrun, L. K. Saul, and B. Sch¨olkopf (eds.), Advances in Neural Information
Processing Systems 16, pp. 831–838. MIT Press, 2004.

10

Published as a conference paper at ICLR 2020

Boaz Barak, Zeev Dvir, Amir Yehudayoff, and Avi Wigderson. Rank bounds for design matrices
with applications to combinatorial geometry and locally correctable codes. In Proceedings of the
forty-third annual ACM symposium on Theory of computing, pp. 519–528. ACM, 2011.

Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming, volume 5. Athena Scientiﬁc

Belmont, MA, 1996.

Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.

arXiv preprint arXiv:1905.00360, 2019.

Lijie Chen and Ruosong Wang. Classical algorithms from quantum and arthur-merlin communication

protocols. 10th Innovations in Theoretical Computer Science, 2019.

Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E
Schapire. On polynomial time PAC reinforcement learning with rich observations. arXiv preprint
arXiv:1803.00606, 2018.

Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of johnson and lindenstrauss.

Random Structures & Algorithms, 22(1):60–65, 2003.

Simon S Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dud´ık, and John Lang-
ford. Provably efﬁcient RL with rich observations via latent state decoding. arXiv preprint
arXiv:1901.09018, 2019a.

Simon S Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang. Provably efﬁcient Q-learning with func-
tion approximation via distribution shift error checking oracle. arXiv preprint arXiv:1906.06321,
2019b.

Amir-massoud Farahmand. Regularization in reinforcement learning. 2011.

Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision

processes. arXiv preprint arXiv:1901.11275, 2019.

Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Con-
textual decision processes with low bellman rank are PAC-learnable. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pp. 1704–1713. JMLR. org, 2017.

Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efﬁcient reinforcement

learning with linear function approximation. arXiv preprint arXiv:1907.05388, 2019.

William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space.

Contemporary mathematics, 26(189-206):1, 1984.

Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In

ICML, volume 2, pp. 267–274, 2002.

Sham Machandranath Kakade. On the sample complexity of reinforcement learning. PhD thesis,

University of College London, 2003.

Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Mach.
Learn., 49(2-3):209–232, November 2002. ISSN 0885-6125. doi: 10.1023/A:1017984413808.
URL https://doi.org/10.1023/A:1017984413808.

Akshay Krishnamurthy, Alekh Agarwal, and John Langford. PAC reinforcement learning with rich
observations. In Advances in Neural Information Processing Systems, pp. 1840–1848, 2016.

Tor Lattimore and Csaba Szepesvari. Learning with good feature representations in bandits and in rl

with a generative model. arXiv preprint arXiv:1911.07676, 2019.

Lihong Li, Michael L Littman, Thomas J Walsh, and Alexander L Strehl. Knows what it knows: a

framework for self-aware learning. Machine learning, 82(3):399–443, 2011.

GG Lorentz. Metric entropy and approximation. Bulletin of the American Mathematical Society, 72

(6):903–937, 1966.

11

Published as a conference paper at ICLR 2020

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529, 2015.

R´emi Munos. Error bounds for approximate value iteration. In Proceedings of the National Conference
on Artiﬁcial Intelligence, volume 20, pp. 1006. Menlo Park, CA; Cambridge, MA; London; AAAI
Press; MIT Press; 1999, 2005.

St´ephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured
prediction to no-regret online learning. In Proceedings of the fourteenth international conference
on artiﬁcial intelligence and statistics, pp. 627–635, 2011.

Bruno Scherrer. Approximate policy iteration schemes: A comparison. In Proceedings of the 31st
International Conference on International Conference on Machine Learning - Volume 32, ICML’14.
JMLR.org, 2014.

Bruno Scherrer and Matthieu Geist. Local policy search in a convex space and conservative policy
In Joint European Conference on Machine Learning and

iteration as boosted policy search.
Knowledge Discovery in Databases, pp. 35–50. Springer, 2014.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889–1897, 2015.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy

optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algo-
rithms. Understanding Machine Learning: From Theory to Algorithms. Cambridge University
Press, 2014. ISBN 9781107057135. URL https://books.google.com/books?id=
ttJkAwAAQBAJ.

Aaron Sidford, Mengdi Wang, Xian Wu, Lin F Yang, and Yinyu Ye. Near-optimal time and sample
complexities for solving discounted markov decision process with a generative model. arXiv
preprint arXiv:1806.01492, 2018.

Max Simchowitz and Kevin Jamieson. Non-asymptotic gap-dependent regret bounds for tabular

MDPs. 05 2019.

Wen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron Boots, and J Andrew Bagnell. Deeply
aggrevated: Differentiable imitation learning for sequential prediction. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pp. 3309–3318. JMLR. org, 2017.

Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
rl in contextual decision processes: Pac bounds and exponential improvements over model-free
approaches. In Conference on Learning Theory, pp. 2898–2933, 2019.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.

Csaba Szepesv´ari and R´emi Munos. Finite time bounds for sampling based ﬁtted value iteration. In
Proceedings of the 22nd international conference on Machine learning, pp. 880–887. ACM, 2005.

Benjamin Van Roy and Shi Dong. Comments on the du-kakade-wang-yang lower bounds. arXiv

preprint arXiv:1911.07910, 2019.

Zheng Wen and Benjamin Van Roy. Efﬁcient exploration and value function generalization in
deterministic systems. In Advances in Neural Information Processing Systems, pp. 3021–3029,
2013.

Lin F. Yang and Mengdi Wang. Reinforcement leaning in feature space: Matrix bandit, kernels, and

regret bound. arXiv preprint arXiv:1905.10389, 2019a.

Lin F. Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features.

In International Conference on Machine Learning, pp. 6995–7004, 2019b.

Andrew Chi-Chin Yao. Probabilistic computations: Toward a uniﬁed measure of complexity. In 18th
Annual Symposium on Foundations of Computer Science (sfcs 1977), pp. 222–227. IEEE, 1977.

12

Published as a conference paper at ICLR 2020

A PROOFS OF LOWER BOUNDS

In this section we present our lower bounds. It will also be useful to deﬁne the value function of a
given state s ∈ Sh as V π
h (s).
Throughout the appendix, for the Q-function Qπ
h and V ∗h , we may
omit h from the subscript when it is clear from the context.

(cid:105)
. For simplicity, we denote V ∗h = V π∗

h and Q∗h and the value function V π

h(cid:48)=h rh(cid:48) | sh = s, π

h (s) = E

(cid:104)(cid:80)H

−

1

We ﬁrst introduce the INDEX-QUERY problem, which will be useful in our lower bound arguments.
Deﬁnition A.1 (INDEX-QUERY). In the INDQn problem, there is an underlying integer i∗ ∈ [n].
The algorithm sequentially (and adaptively) outputs guesses i ∈ [n] and queries whether i = i∗. The
goal is to output i∗, using as few queries as possible.
Deﬁnition A.2 (δ-correct algorithms). For a real number δ ∈ (0, 1), we say a randomized algorithm
A is δ-correct for INDQn, if for any underlying integer i∗ ∈ [n], with probability at least 1 − δ, A
outputs i∗.

The following theorem states the query complexity of INDQn for 0.1-correct algorithms, whose proof
is provided in Section B.1.
Theorem A.1. Any 0.1-correct algorithm A for INDQn requires at least 0.9n queries in the worst
case.

A.1 PROOF OF LOWER BOUND FOR VALUE-BASED LEARNING

In this section we prove Theorem 4.1. We need the following existential result, whose proof is
provided in Section B.2.
Lemma A.1. For any n > 2, there exists a set of vectors P = {p0, p1, . . . , pn
d = (cid:100)8 ln n/ε2(cid:101) such that

1} ⊂ Rd with

−

1. (cid:107)pi(cid:107)2 = 1 for all 0 ≤ i ≤ n − 1;

2. |(cid:104)pi, pj(cid:105)| ≤ ε for any 0 ≤ i, j ≤ n − 1 with i (cid:54)= j.

Now we give the construction of the hard MDP instances. We ﬁrst deﬁne the transitions and the
reward functions. In the hard instances, both the rewards and the transitions are deterministic. There
are H levels of states, and level h ∈ [H] contains 2h distinct states. Thus we have |S| = 2H − 1. If
|S| > 2H − 1 we simply add dummy states to the state space S. We use s0, s1, . . . , s2H
2 to name
these states. Here, s0 is the unique state in level h = 0, s1 and s2 are the two states in level h = 1,
s3, s4, s5 and s6 are the four states in level h = 2, etc. There are two different actions, a1 and a2, in
the MDPs. For a state si in level h with h < H − 1, playing action a1 transits state si to state s2i+1
and playing action a2 transits state si to state s2i+2, where s2i+1 and s2i+2 are both states in level
h + 1. See Figure 1 for an example with H = 3.

−

In our hard instances, r(s, a) = 0 for all (s, a) pairs except for a unique state s in level H − 2 and a
unique action a ∈ {a1, a2}. It is convenient to deﬁne r(s(cid:48)) = r(s, a), if playing action a transits s to
s(cid:48). For our hard instances, we have r(s) = 1 for a unique node s in level H − 1 and r(s) = 0 for all
other nodes.

Now we deﬁne the features map φ(·, ·). Here we assume d ≥ 2 · (cid:100)8 ln 2 · H/δ2(cid:101), and otherwise we
can simply decrease the planning horizon so that d ≥ 2 · (cid:100)8 ln 2 · H/δ2(cid:101). We invoke Lemma A.1 to
1} ⊂ Rd/2. For each state si, φ(si, a1) ∈ Rd is deﬁned to be [pi; 0],
get a set P = {p0, p1, . . . , p2H
and φ(si, a2) ∈ Rd is deﬁned to be [0; pi]. This ﬁnishes the deﬁnition of the MDPs. We now show
that no matter which state s in level H − 1 satisﬁes r(s) = 1, the resulting MDP always satisﬁes
Assumption 4.2.

−

Verifying Assumption 4.2. By construction, for each level h ∈ [H], there is a unique state sh
in level h and action ah ∈ {a1, a2}, such that Q∗(sh, ah) = 1. For all other (s, a) pairs such that
s (cid:54)= sh or a (cid:54)= ah, it is satisﬁed that Q∗(s, a) = 0. For a given level h and policy π, we take θπ
h to be
Qπ(sh, ah) · φ(sh, ah). Now we show that |Qπ(s, a) − (cid:104)θπ
h , φ(s, a)(cid:105)| ≤ δ for all states s in level h
and a ∈ {a1, a2}.

13

Published as a conference paper at ICLR 2020

Figure 1: An example with H = 3. For this example, we have r(s5) = 1 and r(s) = 0 for all other
states s. The unique state s5 which satisﬁes r(s) = 1 is marked as dash in the ﬁgure. The induced
Q∗ function is marked on the edges.

Case I: a (cid:54)= ah. In this case, we have Qπ(s, a) = 0 and (cid:104)θπ
not have a common non-zero coordinate.

h , φ(s, a)(cid:105) = 0, since θπ

h and φ(s, a) do

Case II: a = ah and s (cid:54)= sh.

In this case, by the second property of P in Lemma A.1 and the fact

that Qπ(sh, ah) ≤ 1, we have |(cid:104)θπ

Case III: a = ah and s = sh. In this case, we have (cid:104)θπ

h , φ(s, a)(cid:105)| ≤ δ. Meanwhile, we have Qπ(s, a) = 0.
h , φ(s, a)(cid:105) = Qπ(sh, ah).

Finally, we prove any algorithm that solves these MDP instances and succeeds with probability
20 · 2H trajectories. We do so by providing a reduction from
at least 0.9 needs to sample at least 9
INDQ2H−1 to solving MDPs. Suppose we have an algorithm for solving these MDPs, we show that
such an algorithm can be transformed to solve INDQ2H−1 . For a speciﬁc choice of i∗ in INDQ2H−1,
there is a corresponding MDP instance with
(cid:26)1
0

if s = si∗+2H−1
otherwise

r(s) =

−

1

.

Notice that for all MDPs that we are considering, the transition and features are always the same.
Thus, the only thing that the learner needs to learn by interacting with the environment is the reward
value. Since the reward value is non-zero only for states in level H − 1, each time the algorithm for
solving MDP samples a trajectory that ends at state si where si is a state in level H − 1, we query
whether i∗ = i − 2H
1 + 1
and 0 otherwise. If the algorithm is guaranteed to return a 1/2-optimal policy, then it must be able to
ﬁnd i∗.

1 + 1 or not in INDQ2H−1, and return reward value 1 if i∗ = i − 2H

−

−

A.2 PROOF OF LOWER BOUND FOR MODEL-BASED LEARNING

Proof of Theorem 4.2. We use the same construction as in the proof of Theorem 4.1. Note we
just need to verify that the construction satisﬁes Assumption 4.3. By construction, for all h ∈
{1, 2, . . . , H − 1}, for each state s(cid:48) in level h, there exists a unique (s, a) pair such that playing action
a transits s to s(cid:48), and we take ψ(s(cid:48)) = φ(s, a). We also take βh = 0 for h ∈ {0, 1, . . . , H − 4, H − 3}
and βH
2 = φ(s, a) where (s, a) is the unique pair with R(s, a) = 1. Now, according to the design
of φ(·, ·) and Lemma A.1, Assumption 4.3 is satisﬁed.

−

A.3 PROOF OF LOWER BOUND FOR POLICY-BASED LEARNING

In this section, we present our hardness results for linear policy learning. We ﬁrst prove a weaker
lower bound which only satisﬁes Assumption 4.4, and then prove Theoerem 4.3.

14

s0s1s3Q∗(s1,a1)=0s4Q∗(s1,a2)=0Q∗(s0,a1)=0s2s5Q∗(s2,a1)=1s6Q∗(s2,a2)=0Q∗(s0,a2)=1Published as a conference paper at ICLR 2020

Warmup: Lower Bound for Linear Policy Without Margin. To present the hardness results, we
ﬁrst give the construction of the hard instances. The transitions and rewards functions of these MDP
instances are exactly the same as those in Section A.1. The main difference is in the deﬁnition of the
feature map φ(·, ·). For this lower bound, we deﬁne φ(s, a) = 1 ∈ R if a = a1 and φ(s, a) = −1
if a = a2. By construction, these MDPs satisfy Assumption 3.1 with ρ = 1. We now show that
no matter which state s in level H − 1 satisﬁes r(s) = 16, the resulting MDP always satisﬁes
Assumption 4.4.

Verifying Assumption 4.4. Recall that for each level h ∈ [H], there is a unique state sh in level h
and action ah ∈ {a1, a2}, such that Q∗(sh, ah) = 1. For all other (s, a) pairs such that s (cid:54)= sh or
a (cid:54)= ah, it is satisﬁed that Q∗(s, a) = 0. We simply take θh to be 1 if ah = a1, and take θh to be −1
if ah = a2.

Using the same lower bound argument (by reducing INDEX-QUERY to MDPs), we have the
following theorem.
Theorem A.2. There exists a family of MDPs and a feature map φ (·, ·) that satisfy Assumption 4.4
with d = 1 and Assumption 3.1 with ρ = 1, such that any algorithm that returns a 1/2-optimal policy
with probability at least 0.9 needs to sample Ω (cid:0)2H (cid:1) trajectories.

Proof of Theoerem 4.3 Now we prove Theoerem 4.3. In order to prove Theoerem 4.3, we need
the following geometric lemma whose proof is provided in Section B.3.
Lemma A.2. Let d ∈ N+ be a positive integer and (cid:15) ∈ (0, 1) be a real number. Then there exists a
set of points N ⊂ Sd

1 with size |N | = Ω(1/(cid:15)d/2) such that for every point x ∈ N ,

−

inf
conv(

N \{

y

∈

x

)

}

(cid:107)x − y(cid:107)2 ≥ (cid:15)/2.

(1)

Now we are ready to prove Theorem 4.3. In the proof we assume H = d, since otherwise we can
take H and d to be min{H, d} by decreasing the planning horizon H or adding dummy dimensions
to the feature extractor φ.

−

Proof of Theorem 4.3. We deﬁne a set of 2H
1 deterministic MDPs. The transitions of these hard
instances are exactly the same as those in Section A.1. The main difference is in the deﬁnition of the
feature map φ(·, ·) and the reward function. Again in the hard instances, r(s, a) = 0 for all s in the
ﬁrst H − 2 levels. Using the terminology in Section A.1, we have r(s) = 0 for all states in the ﬁrst
H − 1 levels. Now we deﬁne r(s) for states s in level H − 1. We do so by recursively deﬁning the
optimal value function V ∗(·). The initial state s0 in level 0 satisﬁes V ∗(s0) = 1/2. For each state si
in the ﬁrst H − 2 levels, we have V ∗(s2i+1) = V ∗(si) and V ∗(s2i+2) = V ∗(si) − 1/2H. For each
state si in the level h = H − 2, we have r(s2i+1) = V ∗(si) and r(s2i+2) = V ∗(si) − 1/2H. This
implies that ρ = 1/2H. In fact, this implies a stronger property that each state has a unique optimal
action. See Figure 2 for an example with H = 3.
To deﬁne 2H
1 different MDPs, for each state s in level H − 1 of the MDP deﬁned above, we deﬁne a
new MDP by changing r(s) from its original value to 1. This also affects the deﬁnition of the optimal
V function for states in the ﬁrst H − 1 levels. In particular, for each level i ∈ {0, 1, 2, . . . , H − 2},
we have changed the V value of a unique state in level i from its original value (at most 1/2) to 1. By
doing so we have deﬁned 2H

1 different MDPs. See Figure 3 for an example with H = 3.

−

−

Now we deﬁne the feature function φ(·, ·). We invoke Lemma A.2 with (cid:15) = 8(cid:52) and d = H/2 − 1.
Since (cid:52) is sufﬁciently small, we have |N | ≥ 2H . We use P = {p0, p2, . . . , p2H
1 to
denote an arbitrary subset of N with cardinality 2H . By Lemma A.2, for any p ∈ P, the distance
between p and the convex hull of P \ {p} is at least 4(cid:52). Thus, there exists a hyperplane L which
separates p and P \ {p}, and for all points q ∈ P, the distance between q and L is at least 2(cid:52).
Equivalently, for each point p ∈ P, there exists np ∈ RH/2
1 and op ∈ R such that (cid:107)np(cid:107)2 = 1,
−
|op| ≤ 1 and the linear function fp(q) = (cid:104)q, np(cid:105) + op satisﬁes fp(p) ≥ 2(cid:52) and fp(q) ≤ −2(cid:52)
1, we construct a new set
for all q ∈ P \ {p}. Given the set P = {p0, p2, . . . , p2H

1} ⊂ RH/2
−

−

−

1} ⊂ RH/2
−

6Recall that r(s(cid:48)) = r(s, a), if playing action a transits s to s(cid:48). Moreover, for the instances in Section A.1,

we have r(s) = 1 for a unique node s in level H − 1 and r(s) = 0 for all other nodes.

15

Published as a conference paper at ICLR 2020

Figure 2: An example with H = 3.

Figure 3: An example with H = 3. Here we deﬁne a new MDP by changing r(s5) from its original
value 1/3 to 1. This also affects the value of V (s2) and V (s0).

16

V∗(s0)=1/2V∗(s1)=1/2r(s3)=1/2r(s4)=1/3V∗(s2)=1/3r(s5)=1/3r(s6)=1/6V∗(s0)=1V∗(s1)=1/2r(s3)=1/2r(s4)=1/3V∗(s2)=1r(s5)=1r(s6)=1/6Published as a conference paper at ICLR 2020

√

−

−

√

1} ⊂ RH/2, where pi = [pi; 1] ∈ RH/2. Thus (cid:107)pi(cid:107)2 =

P = {p0, p2, . . . , p2H
2 for all pi ∈ P.
Clearly, for each p ∈ P, there exists a vector ωp ∈ RH/2 such that (cid:104)ωp, p(cid:105) ≥ 2(cid:52) and (cid:104)ωp, q(cid:105) ≤ −2(cid:52)
2. We take φ(si, a1) = [0; pi] ∈ RH and
for all q ∈ P \ {p}. It is also clear that (cid:107)ωp(cid:107)2 ≤
φ(si, a2) = [pi; 0] ∈ RH .
We now show that all the 2H
1 MDPs constructed above satisfy the linear policy assumption. Namely,
we show that for any state s in level H − 1, after changing r(s) to be 1, the resulting MDP satisﬁes the
linear policy assumption. As in Section A.1, for each level h ∈ [H], there is a unique state sh in level
h and action ah ∈ {a1, a2}, such that Q∗(sh, ah) = 1. For all other (s, a) pairs such that s (cid:54)= sh or
a (cid:54)= ah, it is satisﬁed that Q∗(s, a) = 0. For each level h, if ah = a1, then we take (θh)H/2 = 1 and
(θh)H = −1, and all other entries in θh are zeros. If ah = a2, we use p to denote the vector formed
by the ﬁrst H/2 coordinates of φ(sh, a2). By construction, we have p ∈ P. We take θh = [ωp; 0] in
this case. In any case, we have (cid:107)θh(cid:107)2 ≤
2. Now for each level h, if ah = a1, then for all states s in
level h, we have π∗(s) = a1. In this case, (cid:104)φ(s, a1), θh(cid:105) = 1 and (cid:104)φ(s, a2), θh(cid:105) = −1 for all states
in level h, and thus Assumption 4.5 is satisﬁed. If ah = a2, then π∗(sh) = a2 and π∗(s) = a1 for
all states s (cid:54)= sh in level h. By construction, we have (cid:104)θh, φ(s, a1)(cid:105) = 0 for all states s in level h,
since θh and φ(s, a1) do not have a common non-zero entry. We also have (cid:104)θh, φ(sh, a2)(cid:105) ≥ 2(cid:52)
and (cid:104)θh, φ(s, a2)(cid:105) ≤ −2(cid:52) for all states s (cid:54)= sh in level h. Finally, we normalize all θh and φ(s, a)
so that they all have unit norm. Since (cid:107)φ(s, a)(cid:107)2 =
2 for all (s, a) pairs before normalization,
Assumption 4.5 is still satisﬁed after normalization.

√

√

Finally, we prove any algorithm that solves these MDP instances and succeeds with probability
at least 0.9 needs to sample at least Ω(2H ) trajectories. We do so by providing a reduction from
INDQ2H−1 to solving MDPs. Suppose we have an algorithm for solving these MDPs, we show that
such an algorithm can be transformed to solve INDQ2H−1 . For a speciﬁc choice of i∗ in INDQ2H−1,
there is a corresponding MDP instance with
(cid:26)1

r(s) =

the original (recursively deﬁned) value

if s = si∗+2H−1
otherwise

1

−

.

Notice that for all MDPs that we are considering, the transition and features are always the same.
Thus, the only thing that the learner needs to learn by interacting with the environment is the reward
value. Since the reward value is non-zero only for states in level H − 1, each time the algorithm for
solving MDP samples a trajectory that ends at state si where si is a state in level H − 1, we query
whether i∗ = i − 2H
1 + 1
and it original reward value otherwise. If the algorithm is guaranteed to return a 1/4-optimal policy,
then it must be able to ﬁnd i∗.

1 + 1 or not in INDQ2H−1, and return reward value 1 if i∗ = i − 2H

−

−

B TECHNICAL PROOFS

B.1 PROOF OF THEOREM A.1

Proof. The proof is a straightforward application of Yao’s minimax principle Yao (1977). We provide
the full proof for completeness.

Consider an input distribution where i∗ is drawn uniformly at random from [n]. Suppose there is
a 0.1-correct algorithm for INDQn with worst-case query complexity T such that T < 0.9n. By
averaging, there is a deterministic algorithm A(cid:48) with worst-case query complexity T , such that

[A(cid:48) correctly outputs i when i∗ = i] ≥ 0.9.

Pr
[n]
∼

i

We may assume that the sequence of queries made by A(cid:48) is ﬁxed. This is because (i) A(cid:48) is deterministic
and (ii) before A(cid:48) correctly guesses i∗, all responses that A(cid:48) receives are the same (i.e., all guesses
are incorrect). We use S = {s1, s2, . . . , sm} to denote the sequence of queries made by A(cid:48). Notice
that m is the worst-case query complexity of A(cid:48). Suppose m < 0.9n, there exist 0.1n distinct i ∈ [n]
such that A(cid:48) will never guess i, and will be incorrect if i∗ equals i, which implies

[A(cid:48) correctly outputs i when i∗ = i] < 0.9.

Pr
[n]
∼

i

17

Published as a conference paper at ICLR 2020

B.2 PROOF OF LEMMA A.1

We need the following tail inequality for random unit vectors, which will be useful for the proof of
Lemma A.1.
Lemma B.1 (Lemma 2.2 in Dasgupta & Gupta (2003)). For a random unit vector u in Rd and β > 1,
we have

Pr (cid:2)u2
In particular, when β ≥ 6,we have

1 ≥ β/d(cid:3) ≤ exp((1 + ln β − β)/2).

Pr (cid:2)u2

1 > β/d(cid:3) ≤ exp(−β/4).

Proof of Lemma A.1. Let Q = {q1, q2, . . . , qn} be a set of n independent random unit vectors in Rd
with d = (cid:100)8 ln n/ε2(cid:101). We will prove that with probability at least 1/2, Q satisﬁes the two desired
properties as stated in Lemma A.1. This implies the existence of such set P.

It is clear that (cid:107)qi(cid:107)2 = 1 for all i ∈ [n], since each qi is drawn from the unit sphere. We now prove
that for any i, j ∈ [n] with i (cid:54)= j, with probability at least 1 − 1
n2 , we have |(cid:104)qi, qj(cid:105)| ≤ ε. Notice that
this is sufﬁcient to prove the lemma, since by a union bound over all the (cid:0)n
(cid:1) = n(n − 1)/2 possible
pairs of (i, j), this implies that Q satisﬁes the two desired properties with probability at least 1/2.
Now, we prove that for two independent random unit vectors u and v in Rd with d = (cid:100)8 ln n/ε2(cid:101),
with probability at least 1 − 1
n2 , |(cid:104)u, v(cid:105)| ≤ ε. By rotational invariance, we assume that v is a standard
basis vector. I.e., we assume v1 = 1 and vi = 0 for all 1 < i ≤ d. Notice that now (cid:104)u, v(cid:105) is the
magnitude of the ﬁrst coordinate of u. We ﬁnish the proof by invoking Lemma B.1 and taking
β = 8 ln n > 6.

2

B.3 PROOF OF LEMMA A.2

√

(cid:15)-packing N with size Ω(1/(cid:15)d/2) on the d-dimensional unit
Proof of Lemma A.2. Consider a
sphere Sd
1 (for the existence of such a packing, see, e.g., Lorentz (1966)). Let o be the origin. For
two points x, x(cid:48) ∈ Rd, we denote |xx(cid:48)| := (cid:107)x − x(cid:48)(cid:107)2 the length of the line segment between x, x(cid:48).
√
Note that every two points x, x(cid:48) ∈ N satisfy |xx(cid:48)| ≥

(cid:15).

−

To prove the lemma, it sufﬁces to show that N satisﬁes the property equation 1. Consider a point
x ∈ N , let A be a hyperplane that is perpendicular to x (notice that x is a also a vector) and separates
x and every other points in N . We let the distance between x and A be the largest possible, i.e., A
contains a point in N \{x}. Since x is on the unit sphere and N is a
(cid:15)-packing, we have that x is at
least
(cid:15) away from every point on the spherical cap not containing x, deﬁned by the cutting plane A.
More formally, let b be the intersection point of the line segment ox and A. Then

√

√

∀y ∈ (cid:8)y(cid:48) ∈ Sd

s : (cid:104)b, y(cid:48)(cid:105) ≤ (cid:107)b(cid:107)2
2

−

(cid:107)x − y(cid:107)2 ≥

√

(cid:15).

(cid:9) :
(cid:9),

Indeed, by symmetry, ∀y ∈ {y(cid:48) ∈ Sd

−

1 : (cid:104)b, y(cid:48)(cid:105) ≤ (cid:107)b(cid:107)2
2

(cid:107)x − y(cid:107)2 ≥ (cid:107)x − z(cid:107)2 ≥

√

(cid:15).

where z ∈ N ∩ A. Notice that the distance between x and the convex hull of N \{x} is lower
bounded by the distance between x and A, which is given by |bx|. Consider the triangles deﬁned by
x, z, o, b. We have bz ⊥ ox (note that bz lies inside A). By Pythagorean theorem, we have

|bz|2 + |bx|2 = |xz|2;

|bx| + |bo| = |xo| = 1;
|bz|2 + |bo|2 = |oz|2 = 1.

Solve the above three equations for |bx|, we have

as desired.

|bx| = |xz|2/2 ≥ (cid:15)/2

18

Published as a conference paper at ICLR 2020

C EXACT LINEAR Q∗ + GAP IN GENERATIVE MODEL

In this section we present and prove the following theorem.
Theorem C.1. Under Assumption 3.1, Assumption 4.2 and Generative Model query model, the
ρ , log (cid:0) 1
agent can ﬁnd the optimal π∗ with poly
queries with probability 1 − δ for a given
failure probability δ > 0,

d, H, 1

(cid:1)(cid:17)

(cid:16)

δ

∈S

∈A

h,a

h).

h, ai

h, a1

h, ad

s,aφ(si

h), . . . φ(sd

h)(cid:9) ⊂ Φh (cid:44) {φ (s, a)}s

s,a ∈ [−1, 1] such that φ(s, a) = (cid:80)d

Proof of Theorem C.1. We ﬁrst describe the algorithm. For each level, the agent ﬁrst construct
a barycentric spanner Λh (cid:44) (cid:8)φ(s1
(Awerbuch
& Kleinberg, 2008). We have the property that any φ(s, a) with sh ∈ Sh, a ∈ A, we have
s,a, . . . , cd
i=1 ci
c1
The algorithm learns the optimal policy from h = H − 1, . . . , 0. At any level h, we assume the agent
has learned the optimal policy π∗h(cid:48) at level h(cid:48) = h + 1, . . . , H − 1.
Now we present a procedure to show how to learn the optimal policy at level h. At level h, the
(cid:1)) times and uses π∗h+1, . . . , π∗H
agent queries every vector φ(si
as the roll-out to get the on-the-go reward. Note by the deﬁnition of π∗ and Q∗, the on-the-go
reward is an unbiased sample of Q∗(si
h) the average of these on-the-go
rewards. By Hoeffding inequality, it is easy to show with probability 1 − δ
H , for all i = 1, . . . , d,
(cid:12)
(cid:12)
(cid:12) ≤ poly (cid:0) 1
d , ρ(cid:1). Now we deﬁne our estimated Q∗ at level h as follow: for
(cid:12)
(cid:12)
(cid:12) (cid:98)Q(si
h, ai
h)
any (s, a) ∈ Sh × A, (cid:98)Q (s, a) = (cid:80)d
i=1 ci
h). By the boundedness property of cs,a, we
know for any (s, a) ∈ Sh × A, (cid:98)Q (s, a) − Q∗ (s, a) < ρ
2 . Note this implies the policy induced by (cid:98)Q
is the same as π∗. Therefore by induction we ﬁnish the proof.

h) in Λh for poly(d, 1

h). We denote (cid:98)Q(si

ρ , log (cid:0) H

h) − Q∗(si

s,a (cid:98)Q(si

h, ai

h, ai

h, ai

h, ai

h, ai

δ

D LINEAR Qπ FOR ALL π IN GENERATIVE MODEL

In this section we present and prove the following theorem.
Theorem D.1. Under Assumption 4.2 with δ = 0, in the Generative Model query model, there is
(cid:1) trajectories with probability 0.99.
an algorithm that ﬁnds an (cid:15)-optimal policy ˆπ using poly (cid:0)d, H, 1

(cid:15)

−

Proof of Theorem D.1. The algorithm is the same as the one in Theorem C.1 We only need to change
1 for
the analysis. Suppose we are learning at level h and we have learned policies πh+1, . . . , πH
level h + 1, h + 2, . . . , H − 1, respectively. Because we use the roll-out policy πh+1 ◦ · · · ◦ πH
1,
by Assumption 4.2 and the property of barycentric spanner, using the same argument in the proof of
(cid:1)
Theorem C.1, we know with probability 1 − 0.01/H, we can learn a policy πh with poly (cid:0)d, H, 1
(cid:15)
samples such that for any s ∈ Sh, we know πh is only sub-optimal by (cid:15)
H from the ˜πh where ˜πh is the
optimal policy at level h such that πh+1 ◦ · · · ◦ πH
Now we can bound the sub-optimality of ˆπ (cid:44) π0 ◦ · · · ◦ πH
π∗
V π0
= V π0
+V ˜π0
◦
+V π∗
0 ◦
H by our estimation bound, The second term is positive by deﬁnition of ˜π0.

πH−1 (s1) − V π∗
0 ◦
πH−1 (s1) − V ˜π0
πH−1 (s1) − V π∗
0 ◦
πH−1(s1) − V π∗
0 ◦

π∗
H−1 (s1)
πH−1 (s1)
πH−1(s1)
π∗
H−1 (s1) .

1 is the ﬁxed roll-out policy.

−
1 ◦···◦

1 ◦···◦

◦···◦

◦···◦

◦···◦

◦···◦

◦···◦

◦···◦

1:

π∗

π1

π1

π1

π1

π1

π1

−

−

◦

◦

◦

The ﬁrst term is at least − (cid:15)
We can just recursively apply this argument to obtain
πH−1 (s1) − V π∗
V π0
0 ◦
◦
πH−1 (s1) − V π∗
≥V π∗
0 ◦
0 ◦

◦···◦

◦···◦

π1

π1

π∗

1 ◦···◦

π∗
H−1 (s1)

π∗

1 ◦···◦

π∗
H−1 (s1) −

≥V π∗
0 ◦

π∗

1 ◦···◦

πH−1 (s1) − V π∗
0 ◦

19

π∗

1 ◦···◦

π∗
H−1 (s1) −

(cid:15)
H
2(cid:15)
H

.

.

Published as a conference paper at ICLR 2020

≥ . . .
≥ − (cid:15).

20

