Eﬃcient Automatic Diﬀerentiation
of Implicit Functions

Charles C. Margossian and Michael Betancourt

Abstract. Derivative-based algorithms are ubiquitous in statistics, ma-
chine learning, and applied mathematics. Automatic diﬀerentiation of-
fers an algorithmic way to eﬃciently evaluate these derivatives from
computer programs that execute relevant functions. Implementing au-
tomatic diﬀerentiation for programs that incorporate implicit functions,
such as the solution to an algebraic or diﬀerential equation, however,
requires particular care. Contemporary applications typically appeal
to either the application of the implicit function theorem or, in certain
circumstances, specialized adjoint methods. In this paper we show that
both of these approaches can be generalized to any implicit function,
although the generalized adjoint method is typically more eﬀective for
automatic diﬀerentiation. To showcase the relative advantages and lim-
itations of the two methods we demonstrate their application on a suite
of common implicit functions.

Charles Margossian is a PhD candidate in the Department of Statistics, Columbia
University. Michael Betancourt is the principal research scientist at Symplectomorphic,
LLC.

2
2
0
2

b
e
F
6
2

]

O
C

.
t
a
t
s
[

2
v
7
1
2
4
1
.
2
1
1
2
:
v
i
X
r
a

1

 
 
 
 
 
 
2

MARGOSSIAN AND BETANCOURT

CONTENTS

1 Automatic Diﬀerentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.1 A Little Derivative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.1.1 Locations and Directions.

. . . . . . . . . . . . . . . . . . . . . . . .

1.1.2 Transforming Locations and Directions.

. . . . . . . . . . . . . . . .

1.1.3 The Total Derivative.

. . . . . . . . . . . . . . . . . . . . . . . . . .

1.1.4 Directional Derivatives.

. . . . . . . . . . . . . . . . . . . . . . . . .

1.1.5 The Chain Rule.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.2 Express Yourself

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.3 Automatic Diﬀerentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Automatic Diﬀerentiation of Finite-Dimensional Implicit Functions . . . . . . . .

2.1 The Finite-Dimensional Implicit Function Theorem . . . . . . . . . . . . . .

2.2 Evaluating Directional Derivatives of Finite-Dimensional Implicit Functions

2.2.1 Trace Method.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2.2 Finite-Dimensional Implicit Function Theorem. . . . . . . . . . . . .

2.2.3 The Finite-Dimensional Adjoint Method.

. . . . . . . . . . . . . . .

2.3 Demonstrations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3.1 Algebraic systems.

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3.2 Diﬀerence Equations.

. . . . . . . . . . . . . . . . . . . . . . . . . .

2.3.3 Optimization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Automatic Diﬀerentiation of Inﬁnite-Dimensional Implicit Functions . . . . . . .

3.1 The Inﬁnite-Dimensional Implicit Function Theorem . . . . . . . . . . . . .

3.1.1 Fr´echet derivatives. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1.2 The Implicit Function Theorem.

. . . . . . . . . . . . . . . . . . . .

5

5

5

7

8

8

9

11

15

16

16

17

18

18

21

23

23

25

29

31

32

32

33

3.2 Evaluating Directional Derivatives of Inﬁnite-Dimensional Implicit Functions 34

3.2.1 Trace Method.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2.2

Inﬁnite-Dimensional Implicit Function Theorem.

. . . . . . . . . . .

3.2.3 The Inﬁnite-Dimensional Adjoint Method. . . . . . . . . . . . . . . .

3.3 Demonstrations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

35

35

35

40

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

3.3.1 Ordinary Diﬀerential Equations.

. . . . . . . . . . . . . . . . . . . .

3.3.2 Diﬀerential Algebraic Equations.

. . . . . . . . . . . . . . . . . . . .

4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Acknowledgment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A Inﬁnite-Dimensional Forward Method . . . . . . . . . . . . . . . . . . . . . . . .

References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3

40

42

44

46

46

48

4

MARGOSSIAN AND BETANCOURT

Automatic diﬀerentiation is a powerful tool for algorithmically evaluating derivatives of
functions implemented as computer programs (Griewank and Walther, 2008; Baydin et al.,
2018; Margossian, 2019). The method is implemented in an increasing diversity of software
packages such as Stan (Carpenter et al., 2015, 2017) and Jax (Bradbury et al., 2018), driv-
ing state of the art computational tools such as the aforementioned Stan and TensorFlow
(Dillon et al., 2017). Each automatic diﬀerentiation package provides a library of diﬀeren-
tiable expressions and routines that propagate derivatives through programs comprised of
those expressions.

Implicit functions are deﬁned not as explicit expressions but rather by a potentially
inﬁnite set of equality constraints that an output must satisfy for a given input. Common
examples include algebraic equations, optima, and diﬀerential equations. Although deﬁned
only implicitly by the constraints they must satisfy, these functions and their derivatives can
be evaluated at a given input which puts them within the scope of automatic diﬀerentiation.

Many approaches to evaluating the derivatives of implicit functions have been devel-
oped, most designed for speciﬁc classes of implicit functions. Here we focus on two ap-
proaches: direct application of the implicit function theorem and adjoint methods. The
former is commonly applied to ﬁnite-dimensional systems such as algebraic equations and
optimization problems (for example Bell and Burke, 2008; Lorraine, Vicol and Duvenaud,
2019; Gaebler, 2021) while the latter is particularly well-suited to inﬁnite-dimensional sys-
tems such as ordinary diﬀerential equations (for example Pontryagin et al., 1963; Errico,
1997), algebraic diﬀerential equations (Cao et al., 2002), and stochastic diﬀerential equa-
tions (Li et al., 2020). Adjoint methods have also been derived for some ﬁnite-dimensional
systems such as diﬀerence equations (Betancourt, Margossian and Leos-Barajas, 2020).
When they can be derived the performance of adjoint methods often scales better than
the performance of implicit function theorem methods; the details of those derivations,
however, can change drastically from one system to another.

In this paper we derive implicit function theorem and adjoint methods that implement
automatic diﬀerentiation for any implicit function regardless of its dimensionality. We
begin by reviewing derivatives of real-valued functions – drawing a careful distinction be-
tween total, partial, and directional derivatives – and then introduce the basics of au-
tomatic diﬀerentiation. Next we derive implicit function theorem and adjoint methods
for any ﬁnite-dimensional implicit function and demonstrate their application to the re-
verse mode automatic diﬀerentiation of general algebraic equations, the special case of
diﬀerence equations, and optimization problems. Finally we generalize these methods to
inﬁnite-dimensional implicit functions with demonstrations on ordinary and algebraic dif-
ferential equations. In each example we examine the particular challenges that arise with
each method.

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

5

1. AUTOMATIC DIFFERENTIATION

Before discussing the automatic diﬀerentiation of implicit functions, in this section we
will review the basics of diﬀerentiating real-valued functions and then how derivatives of
computer programs can be implemented algorithmically as automatic diﬀerentiation.

1.1 A Little Derivative

Diﬀerentiation is a pervasive topic, but terminology and notation can vary strongly from
ﬁeld to ﬁeld. In this section we review the mathematics of derivatives on real spaces and
introduce all of the terminology and notation that we will use throughout the paper. We
ﬁrst discuss the parameterization of real spaces and the vector space interpretation that
emerges before introducing formal deﬁnitions for total and directional derivatives and their
properties.

1.1.1 Locations and Directions. An I-dimensional real space RI models a rigid and
smooth continuum of points. Here we will consider a subset of the real numbers X ⊆ RI
which may or may not be compact.

A parameterization of X decomposes the I-dimensional space into I copies of the one-

dimensional real line,

X ≈ R1 × . . . × Ri × . . . × RI ,
which we refer to as a coordinate system. Within a parameterization each point x ∈ X can
be identiﬁed by I real numbers denoted parameters or coordinates,

x = (x1, . . . , xi, . . . , xI ).

Every real space X admits an inﬁnite number of parameterizations (Figure 1). A one-to-one
map from X into itself can be interpreted as a map from one parameterization to another
and consequently is often denoted a reparameterization or change of coordinate system.

The choice of any given parameterization endows X with a rich geometry. In particular

we can use coordinates to deﬁne how to scale any point x ∈ X by a real number α ∈ R,

α · x = α · (x1, . . . , xi, . . . , xI )

= (α · x1, . . . , α · xi, . . . , α · xI )
= x′ ∈ X,

as well as add two points x, x′ ∈ X together,

x + x′ = (x1, . . . , xi, . . . , xI ) + (x′

= (x1 + x′
= x′′ ∈ X.

1, . . . , xi + x′

i, . . . , x′
1, . . . , x′
I )
i, . . . , xI + x′
I )

6

MARGOSSIAN AND BETANCOURT

x1

x2

x1

x2

x1

x2

Fig 1. Every real space X admits an inﬁnite number of parameterizations, or coordinate systems, each of
which are capable of uniquely identifying every point with an ordered tuple of real numbers.

These properties make the parameterization of X a vector space over the real numbers;
each point x ∈ X identiﬁes a unique vector x and the coordinates deﬁne a distinguished
vector space basis. If we further use the coordinates to deﬁne an inner product,

I

x, x′

≡

(xi − x′

i)2

(cid:10)

(cid:11)

i=1
X

then this vector space becomes a Euclidean vector space, E(X).

This Euclidean vector space structure deﬁnes a notion of direction and orientation in X.
For example any point x ∈ X can be interpreted as a vector x stretching from the origin at
(0, . . . , 0, . . . , 0) ≡ O to x. Similarly any two points x, x′ ∈ X are connected by the vector

∆x = x′ − x
= (x′

1 − x1, . . . , x′

i − xi, . . . , x′

I − xI ).

Equivalently we can think of vectors as a way to translate from one point to another (Figure
2). For example x shifts the origin to x,

x = (x1, . . . , xi, . . . , xI )

= (0 + x1, . . . , 0 + xi, . . . , 0 + xI )
= (0, . . . , 0, . . . , 0) + (x1, . . . , xi, . . . , xI )
= O + x,

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

7

′

x

x′ − x = ∆x

x

x

x′

x1

x

x

x1

O

x2

O

x2

Fig 2. Given a parameterization of the real space X vectors quantify the direction and distance between
points, such as x between point x and the origin O or x′ − x between x and x
. By following a vector we
can also translate from the initial point to the ﬁnal point.

′

while ∆x shifts from x to x′,

x′ = x + (x′ − x) = x + (x′ − x) = x + ∆x.

Consequently once we ﬁx a parameterization each set of coordinates (x1, . . . , xi, . . . , xI )

can be interpreted as either a location x ∈ X or a direction x ∈ E(X).

1.1.2 Transforming Locations and Directions. Given two real spaces X ⊆ RI and Y ⊆
RJ a real-valued function f : X → Y maps points in X to points in Y . Once a parameter-
ization has been ﬁxed for both spaces such a mapping also induces a map from vectors in
E(X) to vectors in E(Y ), F : E(X) → E(Y ).

Induced maps that preserve the additive and multiplicative structure of the Euclidean

vector space,

F (α · x + β · x′) = α · F (x) + β · F (x′),

are said to be linear. Linear maps can be represented by a matrix of real numbers that
maps the components of the input vector to the components of the output vector,

I

yj =

Fji xi,

i=1
X

or in standard linear algebra notation,

y = F (x) = F · x.

We will denote the space of linear maps between E(X) and E(Y ) as L(X, Y ).

8

MARGOSSIAN AND BETANCOURT

1.1.3 The Total Derivative. The total derivative of a function f : X → Y quantiﬁes
the behavior of f in the local neighborhood of an input point x ∈ X. This local behavior
provides a way of mapping vectors that represent inﬁnitesimal translations from x to vectors
that represent inﬁnitesimal translations from f (x).

More formally the total derivative assigns to each point x ∈ X a linear transformation

between E(X) and E(Y ),

d
dx

: X → L(X, Y )

x 7→

df
dx

(x) ≡ J(x),

that quantiﬁes the ﬁrst-order variation of f in the neighborhood of each input x. We can
then say that the total derivative of f at x is the linear transformation J(x) : E(X) →
E(Y ).

In components the total derivative at a point x ∈ X is speciﬁed by a matrix of partial

derivative functions evaluated at x,

Jji(x) =

∂fj
∂xi

(x),

denoted the Jacobian. The action of the total derivative at x on a vector v ∈ E(X) is then
given by matrix multiplication,

J(x)(v) = J(x) · v =

Jji(x) vi.

I

i=1
X

1.1.4 Directional Derivatives. The total derivative of a function f at a point x applied to
a vector v ∈ E(X), or more compactly J(x)(v), quantiﬁes how much the output of f varies
as x is translated inﬁnitesimally in the direction of v. Consequently J(x)(v) = J(x) · v is
called the forward directional derivative.

The forward directional derivative is often used to construct the linear function between
X and Y that best approximates f at x. The best approximation evaluated at x′ is given
by translating f (x) along J(x)(x′ − x) (Figure 3)

˜f (x′) = f (x) + J(x)(x′ − x)

= f (x) + J(x) · (x′ − x),

or in components,

˜fj(x′

1, . . . , x′

I ) = fj(x1, . . . , xI ) +

∂fj
∂xi

I

i=1
X

(x1, . . . , xI ) · (x′

i − xi).

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

9

x′ − x

′

x

x

X

f : X → Y

′

˜f (x

)

Jf · (x′ − x)

f (x)

Y

Fig 3. The forward directional derivative of the function f at x ∈ X, Jf , propagates inﬁnitesimal per-
turbations to the input, x′ − x, to inﬁnitesimal perturbations of the output, Jf · (x′ − x). Translating the
function output f (x) by Jf · (x′ − x) generates the best linear approximation to f at x, ˜f .

The total derivative also deﬁnes an adjoint transformation that maps vectors in E(Y )
to vectors in E(X), J †(x) : E(Y ) → E(X). The matrix components of this adjoint trans-
formation are given by the transpose of the Jacobian matrix,

J †
ji(x) = Jij(x).

The application of this adjoint transformation to a vector α ∈ E(Y ),

β = J †(x)(α) = JT · α,

quantiﬁes how the inputs need to vary around x in order to achieve the given output
variation α. We will refer to this action as a reverse directional derivative.

1.1.5 The Chain Rule. The chain rule provides an explicit construction for the total
derivative of a composite function constructed from many component functions. Consider
for example a sequence of functions fn : Xn → Xn+1 and the composite function

f = fN ◦ . . . ◦ fn ◦ . . . ◦ f1 : X1 → XN +1.

The total derivative of f at any point x1 ∈ X1 is given by composing the total derivatives

of each component function together in the same order,

Jf = JfN (xN ) ◦ . . . ◦ Jfn(xn) ◦ . . . ◦ Jf1(x1),

where xn = fn−1(xn−1). Likewise the components of the composite Jacobian matrix are
given by a sequence of matrix products,

Jf = JfN (xN ) · . . . · Jfn(xn) · . . . · Jf1(x1).

10

MARGOSSIAN AND BETANCOURT

Unfortunately these intermediate matrix products are expensive to evaluate, especially
when the intermediate spaces Xn are high-dimensional. Constructing the full composite
Jacobian matrix is usually computationally burdensome.

On the other hand this composite structure is well-suited to the evaluation of directional
derivatives. For example the forward directional derivative of a composite function is given
by

or

Jf (v) =

JfN (xN ) ◦ . . . ◦ Jfn(xn) ◦ . . . ◦ Jf1(x1)

(v),

(cid:0)
Jf · v =

JfN (xN ) · . . . · Jfn(xn) · . . . · Jf1(x1)

(cid:1)
· v.

Because of the associativity of matrix multiplication we can apply each component deriva-
tive to v in sequence and avoid the intermediate compositions entirely,

(cid:1)

(cid:0)

Jf (v) = JfN (xN )(· · · Jfn(xn)(· · · (Jf1 (x1)(v)) · · · ) · · · ),

or

Jf · v = JfN (xN ) · (· · · Jfn(xn) · (· · · Jf1(x1) · v) · · · ) · · · ).

In other words we can evaluate the total forward directional derivative iteratively,

or in components,

v1 = Jf1(x1)(v)
v2 = Jf2(x2)(v1)
. . .
vn = Jfn(xn)(vn−1)
. . .

vN −1 = JfN−1 (xN −1)(vN −2)

Jf (x)(v) = vN = JfN (xN )(vN −1),

v1 = Jf1(x1) · v
v2 = Jf2(x2) · v1
. . .
vn = Jfn(xn) · vn−1
. . .

vN −1 = JfN−1 (xN −1) · vN −2

Jf (x) · v = vN = JfN (xN ) · vN −1.

The evaluation of each intermediate directional derivative requires only a matrix-vector
product which is substantially less expensive to implement than the matrix-matrix products
needed to evaluate the composite Jacobian.

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

11

The reverse directional derivative can be evaluated sequentially as well,

(xN )(α)

αN = J †
fN
αN −1 = J †
fN−1 (xN −1)(αN )
. . .
αN −n = J †
. . .
α2 = J †
f (x)(α) = α1 = J †
J †

f2(x2)(α3)
f1(x1)(α2),

fN−n

(xN −n)(αN −n+1)

or in components,

αN = JT
αN −1 = JT
. . .
αN −n = JT
. . .
α2 = JT
f (x) · α = α1 = JT
JT

fN (xN ) · α
fN−1(xN −1) · αN

fN−n(xN −n) · αN −n+1

f2(x2) · α3
f1(x1) · α2.

1.2 Express Yourself

Before we can consider how to implement derivatives of real-valued functions in practice
we ﬁrst have to consider how to implement the real-valued functions themselves. Functions
f : X → Y are often implemented as computer programs which take in any input value
x ∈ X and return the corresponding output value f (x) ∈ Y . These computer programs
are themselves implemented as a sequence of expressions, each of which transforms some
descendent of the initial value towards the ﬁnal output value.

If these expressions deﬁned full component functions then we could use the chain rule to
automatically propagate directional derivatives through the program as we discussed in the
previous section. Well-deﬁned component functions, however, would depend on only the
output of the previous component function, while expressions can depend on the output of
multiple previous expressions. Because of this expressions do not immediately deﬁne valid
component functions on their own, and the chain rule does not immediately apply.

That said we can manipulate each expression into a well-deﬁned component function with
a little bit of work. First we’ll need to take advantage of the fact that the expressions that

12

MARGOSSIAN AND BETANCOURT

Pseudo-Code

Expression Graph

real f(vector[3] x1) {

real z1 = x1[1] + x1[2];
real z2 = x1[2] * x1[3];
return z1 / z2;

}

/

+

∗

x1,1

x1,2

x1,3

Fig 4. This pseudo-code implements a function f : R3 → R that maps every input x1 = (x1,1, x1,2, x1,3)
to a real-valued output through three intermediate expressions. The dependencies between these expressions
and the input variables forms an expression graph.

comprise a complete program can be represented as a directed acyclic graph, also known
as an expression graph (Figure 4). In each expression graph the root nodes designate the
input variables, internal and leaf nodes designate the expressions, and edges designate the
dependencies between the expressions.

A topological sort of an expression graph is any ordering of the nodes such that each
expression follows all expressions on which it depends (Figure 5). Such a sorting ensures
that if we process the sorted expressions in order then we will evaluate an expression only
once all of the expressions on which it depends have already been evaluated.

Any topological sort provides an explicit sequence of expressions, but each expression
can still depend on the output of any expression that precedes it in the stack. We can
use the ordering to limit this dependence to only the previous output, however, if we can
buﬀer each expression with any of the previous outputs that are used by future expressions.
For example this buﬀering can be implemented by introducing identify expressions that
propagate any necessary values forward (Figure 6). Together each initial expression and the
added identify expressions deﬁne a layer of expressions that depend on only the expressions
in the previous layer, so that these layers deﬁne a sequence of valid component functions
(Figure 7).

Once we’ve derived these component functions we can ﬁnally apply the chain rule. In
particular we can evaluate forward directional derivatives by propagating an initial vector
through the constructed sequence of component functions. At the same time we can eval-
uate a reverse directional derivative by ﬁrst evaluating all of the component functions in
a forward sweep through the sequence before executing a reverse sweep that propagates a
vector in the output space to the input space.

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

13

Expression Graph

Topological Sort

Stack/Tape

/

/

+

+

∗

∗

x1,1

x1,2

x1,3

x1,1

x1,2

x1,3

/

+

∗

x1,3

x1,2

x1,1

Fig 5. A topological sort of an expression graph is an ordering of the expressions, often called a stack or a
tape, that guarantees that when progressing across the stack in order each expression will not be evaluated
until all of the expressions on which it depends have already been evaluated.

Topological Sort

Expression Layers

/

/

+

+

I

∗

I

I

∗

x1,1

x1,2

x1,3

x1,1

x1,2

x1,3

Fig 6. In order to turn each topologically-sorted expression into a valid component function they must be
complemented with identity maps that carry forward intermediate values needed by future expressions. The
resulting layers of expressions depend on only expressions in the previous layer.

14

MARGOSSIAN AND BETANCOURT

Reconstructed Component Functions

/

f3 : X3 ⊆ R2

(x3,1, x3,2)

→ X4 ⊆ R1
7→ (x4,1 = x3,1/x3,2)

+

I

f2 : X2 ⊆ R3

→ X3 ⊆ R2

(x2,1, x2,2, x2,3) 7→ (x3,1 = x2,1 + x2,2, x3,2 = x2,3)

I

I

∗

f1 : X1 ⊆ R3

→ X2 ⊆ R3

(x1,1, x1,2, x1,3) 7→ (x2,1 = x1,1, x2,2 = x1,2, x2,3 = x1,2 · x1,3)

x1,1

x1,2

x1,3

Fig 7. Complementing each topologically-sorted expression with the appropriate identity maps deﬁnes valid
component functions. When composed together these component functions yield the function implemented
by the computer program, here f = f3 ◦ f2 ◦ f1, and allow for the application of the chain rule to diﬀerentiate
through the program.

Something interesting happens, however, when we evaluate the total derivative of one
of these reconstructed component functions. Let’s denote the action of the component
function as fn : (x, x′) 7→ (g(x), I(x′)), where g is the action implemented by the initial
expression and I is the identify map that propagates any auxiliary outputs. We can also
decompose the input vector v into components that map onto g and I, respectively,

v = (vg, vI )T .

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

15

In this notation the forward directional derivative becomes

Jfn(v) = Jfn((vg, vI )T )
= Jfn · (vg, vI )T

∂g
∂x

∂I
∂x

∂g
∂x′
∂x′ 
∂I


·

vg



vI






vg

=

=

=













·

∂g
∂x

0

0
0

∂g
∂x · vg

0 




vI







The only non-vanishing contribution to the directional derivative comes from the initial
expression; the vector corresponding to the buﬀered outputs, vI , completely decouples from
any derivatives that follow. Consequently the only aspect of the reconstructed component
function that inﬂuences propagation of the forward directional derivative is the forward
directional derivative of the expression itself.

In other words we can evaluate the forward directional derivative of a function imple-
mented by a computer program by propagating only the forward directional derivatives of
the individual expressions; at no point do we actually have to construct explicit component
functions! The action of the adjoint derivative behaves similarly, allowing us to evaluate
the reverse directional derivative using only the reverse directional derivatives local to each
expression. When considering higher-order derivatives, however, the equivalence between
propagating vectors across expressions and full component functions is not always preserved
and explicit component functions may be needed. For more see Betancourt (2018).

1.3 Automatic Diﬀerentiation

Automatic diﬀerentiation exploits the equivalence between propagating vectors across
expressions and full component functions to algorithmically evaluate forward and reverse
directional derivatives.

Forward mode automatic diﬀerentiation implements forward directional derivatives, pass-
ing intermediate values and intermediate forward directional derivatives between expres-
sions as the program is evaluated. These intermediate forward directional derivatives are
denoted tangents or sensitivities.

Any implementation of a function g that supports forward mode automatic diﬀerentia-
tion must provide not only a map from input values to output values but also a map from

16

MARGOSSIAN AND BETANCOURT

input tangents to output tangents,

v′ = Jg · v.

Similarly reverse mode automatic diﬀerentiation implements reverse directional deriva-
tives. Here the program must be evaluated ﬁrst before intermediate reverse directional
derivatives are propagated between expressions in the reverse order that the expressions
are evaluated. These intermediate reverse directional derivatives are denoted cotangents or
adjoints.

Any implementation of a function g that supports reverse mode automatic diﬀerentiation
must provide not only a map from input values to output values but also a map from output
cotangents to input cotangents,

α′ = J T

g · α.

2. AUTOMATIC DIFFERENTIATION OF FINITE-DIMENSIONAL IMPLICIT
FUNCTIONS

A ﬁnite collection of constraint functions that an output has to satisfy for a given input
implicitly deﬁnes a map from inputs to outputs, or an implicit function. In this section we
discuss how implicit functions are formally deﬁned, how to diﬀerentiate these implicitly
deﬁned functions, and then ﬁnally demonstrate their application on several instructive
examples.

2.1 The Finite-Dimensional Implicit Function Theorem

Consider a ﬁnite-dimensional real-valued space of known inputs, X, a ﬁnite-dimensional
real-valued space of unknown outputs, Y , a ﬁnite-dimensional real-valued space of con-
straint values, Z, and the constraint function

c : X × Y → Z

(x, y)

7→ c(x, y).

The implicit function theorem deﬁnes the conditions under which the constraint function
implicitly deﬁnes a map from f : X → Y which satisﬁes c(x, f (x)) = 0 for all inputs in a
local neighborhood x ∈ U ⊂ X.

More formally consider the neighborhoods 0 ∈ W ⊂ Z and V ⊂ Y such that the kernel

of the constraint function falls into product of U and V ,

c−1(0) = U × V,

and assume that a function f : U → V that satisﬁes c(x, f (x)) = 0 exists. If the constraint
function c is diﬀerentiable across U × V then the total derivative of c evaluated at (x, f (x))

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

17

is given by

0 =

=

d
dx
∂c
∂x

c(x, f (x))

(x, f (x)) +

∂c
∂y

(x, f (x)) ◦

df
dx

(x).

When the dimension of Z equals the dimension of Y then the partial derivative ∂c/∂y (x, y)

might deﬁne a bijection from V to W . If it does then we can solve for the total derivative
of the assumed function,

(x)

df
dx
U →V

= −

∂c
∂y

(cid:18)

(x, f (x))

(cid:19)

W →V

−1

◦

∂c
∂x

(x, f (x))

.

U →W

| {z }

When ∂c/∂y (x, y) is bijective this derivative is well-deﬁned, and the existence and unique-
ness of ordinary diﬀerential equation solutions guarantees that an implicit function that sat-
isﬁes c(x, f (x)) = 0 is well-deﬁned in the neighborhood around x. In other words the system
of constraints deﬁnes an implicit function if and only if the partial derivative ∂c/∂y (x, y)
is invertible

{z

{z

|

|

}

}

The implicit function theorem determines when an implicit function is well-deﬁned, but
not how to evaluate it. In practice we typically have to rely on numerical methods that
heuristically search the output space for a value y that satisﬁes c(x, y) = 0 for the given
input x.

2.2 Evaluating Directional Derivatives of Finite-Dimensional Implicit Functions

To incorporate implicit functions into an automatic diﬀerentiation library we need to be
able to evaluate not only the output consistent with a given input but also the directional
derivatives. Here we consider three general approaches: a trace method that works with a
given numerical solver, a method that utilizes intermediate results of the implicit function
theorem, and an adjoint method that evaluates the directional derivative directly.

In all three approaches we will consider not the implicit function alone but rather its
composition with a summary function that maps the outputs into some real space, g : Y →
RK. Often g will be the identify map, but the ﬂexibility oﬀered by this summary function
will facilitate some of the examples that we consider below.

When X = RI and Y = Z = RJ the composition of the summary function with the

implicit function deﬁnes the real-valued function

h = g ◦ f : RI → RJ → RK

and our goal will be to evaluate either the forward directional derivative Jg◦f (x)(v) or the
reverse directional derivative J †

g◦f (x)(α).

18

MARGOSSIAN AND BETANCOURT

2.2.1 Trace Method. Each step that an iterative numerical solver takes while searching
for a consistent output can be interpreted as a map from the output space to itself given
the ﬁxed input x,

˜fn : X × Y → Y

(x, yn−1) 7→ yn.

The trace of the solver’s evaluation then deﬁnes a composite function,

˜f (x, y0) =

˜fN (x) ◦ ˜fN −1(x) ◦ . . . ◦ f1(x)

(y0)

(cid:0)
that maps the input and an initial guess to an approximate solution,

(cid:1)

˜f : X × Y → Y
(x, y0) 7→ ˜y

satisfying c(x, ˜y) ≈ 0.

If each of these intermediate steps are diﬀerentiable and supported by an automatic
diﬀerentiation library then we can evaluate the directional derivatives of ˜f using automatic
diﬀerentiation and use them to approximate the directional derivatives of the exact im-
plicit function f . While straightforward to implement this approach can suﬀer from poor
performance in practice, especially as the number of solver iterations grows and propagat-
ing derivatives through the composite function becomes slow and memory intensive. See
for example (Bell and Burke, 2008) for further discussion of this trace method applied to
optimization problems and (Margossian, 2019) for one on algebraic equations.

2.2.2 Finite-Dimensional Implicit Function Theorem. Conveniently the derivative of the
implicit function f is explicitly constructed in the derivation of the implicit function theo-
rem,

df
dx

(x) = −

∂c
∂y

(cid:18)

(x, f (x))

(cid:19)

−1

◦

∂c
∂x

(x, f (x)).

If we can evaluate these derivatives of the constraint function then we can immediately
evaluate the derivative for f once we have numerically solved for the output of the implicit
function y = f (x),

df
dx

(x) = −

∂c
∂y

(cid:18)

(x, y)

(cid:19)

−1

◦

∂c
∂x

(x, y).

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

19

The Jacobian of the composite function g ◦ f is then given by

Jg◦f (x) =

∂(g ◦ f )
∂x

(x)

=

dg
dy

(f (x)) ◦

df
dx

(x)

= −

dg
dy

(f (x)) ◦

∂c
∂y

(cid:18)

−1

◦

∂c
∂xi

(x, f (x))

(cid:19)

(x, f (x)).

In components this becomes

(Jg◦f )ik(x) =

∂hk
∂xi
J

=

(x)

dgk
dyj

(f (x)) ·

dfj
dxi

(x)

j=1
X
J

J

= −

j=1
X

Xj′=1

dgk
dyj

(f (x)) ◦

∂cj′
∂yj

(cid:18)

(x, f (x))

(cid:19)

−1

◦

∂cj′
∂xi

(x, f (x)),

or in more compact matrix notation,

where

Jg◦f = −Jg · C−1
y

· Cx,

(Cy)ij =

(Cx)ij =

(Jg)ij =

∂ci
∂yj
∂ci
∂xj
∂gj
∂yi

(x, f (x))

(x, f (x))

(f (x)).

In order to incorporate this composite function into forward mode automatic diﬀerenti-
ation we need to evaluate the action of the Jacobian contracted against a tangent vector,

I

(Jg◦f (x) · v)j =

(Jg◦f )ij(x) · vi

or equivalently

i=1
X

I

= −

i=1
X

dgj
dy

(f (x)) ◦

∂c
∂y

(cid:18)

(f (x))

(cid:19)

−1

◦

∂c
∂xi

(x) · vi,

Jg◦f (x) · v = −Jg · C−1
y

· Cx · v.

20

MARGOSSIAN AND BETANCOURT

Similarly to incorporate this composite function into reverse mode automatic diﬀerentiation
we need to evaluate the action of the Jacobian contracted against a cotangent vector,

(JT

g◦f (x) · α)i =

(Jg◦f )ij(x) · αj

J

j=1
X
J

= −

j=1
X

dgj
dy

(f (x)) ◦

∂c
∂y

(cid:18)

(f (x))

(cid:19)

−1

◦

∂c
∂x

(f (x)) · αj,

or

g◦f (x) · α = −CT
JT
x ·
= −CT
x ·

C−1
y
CT
y

(cid:0)

T

−1
(cid:1)

· JT
· JT

g · α

g · α.

With so many terms there are multiple ways to evaluate these directional derivatives.
The forward method, for example, explicitly constructs Jg◦f (x) = −Jg · C−1
· Cx before
y
evaluating the contractions Jg◦f (x) · v or JT
g◦f (x) · α. With careful use of automatic dif-
ferentiation, however, we can evaluate the ﬁnal directional derivatives more eﬃciently. In
particular we don’t need to explicitly construct Jg and Cx at all.

(cid:0)

(cid:1)

For example when evaluating Jg◦f (x) · v we can avoid Cx by using one sweep of forward
mode automatic diﬀerentiation to evaluate u = Cx · v directly. Because of the inversion
we have to construct the entirety of Cy, for example with J sweeps of forward mode or
reverse mode automatic diﬀerentiation, but we can avoid constructing (Cy)−1 by solving
for only the linear system

Cy · t = u.

Finally we can avoid constructing Jg with one sweep of forward mode automatic diﬀeren-
tiation to evaluate Jg · t. These steps are outlined in Algorithm 1.

Algorithm 1 Forward mode automatic diﬀerentiation of ﬁnite-dimensional implicit func-
tion.
1: input: constraint function, c; summary function, g; tangent, v. Note: we assume we have a diﬀerentiable

program for all the input functions.
2: u = Cx · v (one forward mode sweep)
3: Cy = ∂c/∂y (J forward or reverse mode sweep)
4: t = C−1
· u (linear solve)
y
5: Jg◦f · v = −Jg · t (one forward sweep)
6: return: Jg◦f · v

Similar optimizations are also possible when evaluating the reverse directional derivative
JT
g◦f (x) · α. We ﬁrst evaluate β = JT
g · α using one sweep of reverse mode automatic
diﬀerentiation and then construct Cy as above. This allows us to solve the linear system

CT

y · γ = β,

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

21

and then evaluate CT
through the constraint function. These steps are outlined in Algorithm 2.

x · β with one more sweep of reverse mode automatic diﬀerentiation

Algorithm 2 Reverse mode automatic diﬀerentiation of ﬁnite-dimensional implicit func-
tion.
1: input: constraint function, c; summary function, g; cotangent, α. Note: we assume we have a diﬀeren-

tiable program for all the input functions.
2: β = JT
g · α (one reverse mode sweep)
3: Cy = ∂c/∂y (J forward or reverse mode sweep)
4: γ = (cid:0)CT
y (cid:1)
g◦f · α = −CT
5: JT
6: return: JT
g◦f · α

x · γ (one reverse mode sweep)

β (linear solve)

−1

Often the form of a particular constraint function results in sparsity structure in Cx and
Cy that can be exploited to reduce the cost of the irreducible linear algebraic operations.
Identifying this structure and implementing faster operations, however, requires substantial
experience with numerical linear algebra.

2.2.3 The Finite-Dimensional Adjoint Method. The adjoint method provides a way of
directly implementing the contractions that give forward and reverse directional derivatives
without explicitly constructing the Jacobian df /dx(x). Adjoint methods have historically
been constructed for speciﬁc implicit functions but here we present a general construction
for any ﬁnite-dimensional system of constraints.

We begin by constructing a binary Lagrangian function,

L : X × Y → R,

whose derivative is equal to the desired contraction when c(x, y) = 0. More formally we
compose L with the implicit function deﬁned by the constraints to give the unary function

and then require that

L(x) = L(x, f (x)) : x 7→ R,

dL
dxi

(x) = (Jg◦f · v)i(x).

for forward mode automatic diﬀerentiation or

dL
dxi

(x) = (JT

g◦f · α)i(x)

for reverse mode automatic diﬀerentiation. To simplify the presentation from here on we
will focus on only this latter application to reverse mode automatic diﬀerentiation.

22

MARGOSSIAN AND BETANCOURT

Next we introduce any mapping Λ : Z → R that preserves the kernel of the constraint

function, Λ ◦ c(x, y) = 0 whenever c(x, y) = 0. This allows us to deﬁne a second function

Q = Λ ◦ c : X × Y → R

with Q(x, f (x)) = 0 for all x ∈ X.

These two functions together then deﬁne an augmented Lagrangian function,

J = L + Q : X × Y → R.

Substituting y = f (x) gives a unary function,

J (x) = J (x, f (x))

= L(x, f (x)) + Q(x, f (x)).

The second term, however, vanishes by construction so that J (x) reduces to L(x) and

dJ
dxi

(x) =

dL
dxi

(x) = (JT

g◦f · α)i(x).

While the contribution from C(x, f (x)) vanishes, its inclusion into the augmented La-
grangian introduces another way to evaluate the desired directional derivative. The total
derivative of the unary augmented Lagrangian is

dJ
dxi

(x) =

=

=

=

dJ
dxi
dL
dxi

(x, f (x))

(x, f (x)) +

dQ
dxi

(x, f (x))

∂L
∂xi
∂Q
∂xi
∂L
∂xi

(x, f (x)) +

(x, f (x)) +

(x, f (x)) +

(cid:18)

∂L
∂y
∂Q
∂y
(cid:18)
∂Q
∂xi

(x, f (x)) ◦

(x, f (x)) ◦

∂f
∂xi (cid:19)
∂f
∂xi (cid:19)

(x)

(x)

(x, f (x))

∂L
∂y

(cid:18)(cid:18)

(x, f (x)) +

∂Q
∂y

(x, f (x))

◦

(cid:19)

∂f
∂xi (cid:19)

(x).

+

+

Once we’ve identiﬁed the solution y = f (x) the ﬁrst two terms are ordinary partial deriva-
tives that are straightforward to evaluate. The second two terms, however, are tainted by
the total derivative of the implicit function, df /dx, which is expensive to evaluate as we
saw in the previous section.

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

23

At this point, however, we can exploit the freedom in the choice of kernel-preserving

function Λ and hence J itself. If we can engineer a Λ such that

∂L
∂y

(x, f (x)) +

∂Q
∂y

(x, f (x)) = 0

then the contribution from the last two terms, and the explicit dependence on the derivative
of the implicit function vanishes entirely!

The adjoint method attempts to solve this adjoint system

∂L
∂y

(x, f (x)) +

∂Λ
∂c

(c(x, f (x))) ◦

∂c
∂y

(x, f (x)) = 0

for a suitable Λ and then evaluate the desired contraction from the remaining two terms,

dJ
dxi

(x) =

∂L
∂xi

(x, f (x)) +

∂Λ
∂c

(c(x, f (x))) ◦

∂c
∂xi

(x, f (x)).

If a suitable Λ exists, and we can ﬁnd it, then this two-stage method allows us to evaluate the
directional derivative directly without constructing the derivatives of the implicit function.

2.3 Demonstrations

To compare and contrast the two presented methods for evaluating directional derivatives
of an implicit function we examine their application on three common implicit systems:
algebraic equations, diﬀerence equations, and optimization problems.

2.3.1 Algebraic systems. When X = RI and Y = RJ a system of J transverse constraint
functions cj(x, y) deﬁnes an algebraic system and a well-deﬁned implicit function. In Sec-
tion 2.2.2 we saw that the ﬁnite-dimensional implicit function theorem gives the reverse
directional derivative

JT (x) · α = −CT

x ·

T

· JT

g · α.

where

(Jg)ij =

(Cy)ij =

(Cx)ij =

C−1
y

(cid:1)

(f (x))

(x, f (x))

(x, f (x)).

(cid:0)
∂gj
∂yi
∂ci
∂yj
∂ci
∂xj

Here we will take g to be the identify function so that Jg reduces to the identify matrix
and the reverse directional derivative simpliﬁes to

JT (x) · α = −CT

x ·

C−1
y

T

· α.

(cid:0)

(cid:1)

24

MARGOSSIAN AND BETANCOURT

To apply the adjoint method we need to construct a Lagrangian function that satisﬁes

For example we can take

dL
dxi

(x, f (x)) = (JT · α)i(x).

L = f T (x) · α =

fj(x) · αj.

J

j=1
X

Next we need to augment L with the contribution from the constraints Q. Because inner

products vanish whenever either input is zero we can take

Q = cT (x, y) · λ =

cj(x, y) · λj,

J

j=1
X

for any real-valued, non-zero constants λj.

With these choices of L and Q the adjoint system becomes

0 =

∂L
∂yj

+

∂Q
∂yj
J

∂cj′
∂yj

= αj +

Xj′=1

(x, f (x)) · λj′,

or, in matrix notation,

0 = α + CT

y · λ.

Because Cy is non-singular we can directly solve this for λ,

λ = −(C−1

y )T · α.

Substituting this into the remaining terms then gives

(JT · α)i(x) =

=

dJ
dxi
∂L
∂xi

(x, f (x))

(x, f (x)) +

∂Q
∂xi

(x, f (x))

∂cj′
∂xi

(x, f (x))

J

= 0 +

λj′ ·

Xj′=1
x · λ)i

= (CT
= (−CT

x · (C−1

y )T · α)i,

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

25

or

JT · α = −CT

x · (C−1

y )T · JT

g · α.

which is exactly the same as the result from the implicit function theorem.

In this general case there isn’t any structure in the constraint functions to exploit and

consequently both methods yield equivalent calculations.

2.3.2 Diﬀerence Equations. To better contrast the two methods let’s consider a more
structured system. A discrete dynamical system over the state space RN deﬁnes trajectories

y = (y1, . . . , yi, . . . , yI ),

where the individual states yi ∈ RN are implicitly deﬁned by the diﬀerence equations

for some initial condition y0 = u(x). To simplify the notation we will write

yi+1 − yi = ∆(yi, x, i),

∆i = ∆(yi, x, i)

from here on.

Organizing these diﬀerence equations into constraint equations deﬁnes a highly struc-

tured algebraic system,

y1 − y0 − ∆0 = c1(x, y) = 0

· · ·
yi − yi−1 − ∆i−1 = ci(x, y) = 0
· · ·
yI − yI−1 − ∆I−1 = cI (x, y) = 0,

which then sets the stage for the implicit function machinery.

Formally this system of constraints deﬁnes an entire trajectory; the output space is given
by Y ⊂ RN ×I. Often, however, we are interested not in the entire trajectory but only the
ﬁnal state, yI ∈ RN . Conveniently we can readily accommodate this by using the summary
function to project out the ﬁnal state,

g : Y = RN ×I

→ R
(y1, . . . , yi, . . . , yI ) 7→ yI .

Having deﬁned an implicit system and summary function we can now apply the implicit
function theorem and adjoint methods to derive the gradients of the implicitly-deﬁned ﬁnal

26

MARGOSSIAN AND BETANCOURT

state. Although these two methods yield equivalent results, the adjoint method more di-
rectly incorporates the natural structure of the problem without any explicit linear algebra.

Diﬀerentiation with the Implicit Function Theorem. To apply the implicit function theorem
directly we proceed as in 2.3.1 and and compute
x · (C−1

JT · α = −CT

y )T · JT

g · α.

term by term from the right.

Our chosen summary function yields the Jacobian matrix

Jg =

∂g
∂y

= [0N , · · · , 0N , IN ] ,

where 0N is an N × N matrix of zeros and IN is the N × N identity matrix. The ﬁrst
contraction on the right then gives

βT = (JT

g · α)T = [0, 0, · · · , 0

, α1, α2, · · · , αN

].

(I−1)N

N

|
At this point we construct Cy from the derivatives of the constraint functions,

{z

{z

}

}

|

IN 0N · · ·
C1
IN 0N
· · ·
y
0N C2
0N
IN
y
...
. . .
. . .
. . .
· · · CI−1
0N · · ·

y

· · ·
. . .
IN

Cy =










Ci

y =

∂
∂yi

(yi+1 − yi − ∆i) = −1 −

where



,







∂∆i
∂yi

,

and then solve the linear system

CT

y · γ = β.

Because of the structure of the constraints the matrix Cy is triangular and with enough
linear algebra proﬁciency we would know that we can eﬃciently solve for γ with backwards
elimination. To clarify the derivation we ﬁrst split the elements of γ into subvectors of
length N ,

γT = [γ1, ..., γ I ].

The linear system can then be written as

y 0N · · ·

IN C1
0N IN C2
y 0N · · ·
0N 0N IN C3
y 0N
...
. . .
. . .
IN
0
0

. . .
· · ·

. . .
· · ·










γ1
γ2
γ3
...
γI










.

0
0
0
...
α



















=










·










EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

27

Starting from the bottom and going up we can solve this system recursively to obtain the
backward diﬀerence equations

with terminal condition

γi −

1 +

(cid:18)

∂∆i
∂yi (cid:19)

γi+1 = 0

γI = α.

We now evaluate Cx; for i > 1 the derivatives are straightforward,

(Cx)i =

∂ci
∂x

= −

∂∆i−1
∂x

,

but for i = 1 we have to be careful to incorporate the implicit dependence of the initial
state, y0 = u(x), on x,

(Cx)1 =

=

=

∂c1
∂x
∂
∂x
∂
∂x

(y1 − y0 − ∆(y0, x, 0))

(y1 − u(x) − ∆(u(x), x, 0))

= −

∂u
∂x

−

= −

1 +

(cid:18)

∂∆
∂x

(cid:18)
∂∆
∂u

(cid:19)

+

∂∆
∂u

·

∂u
∂x

∂u
∂x

−

∂∆
∂x

.

(cid:19)

Finally we multiply Cx and γ to give

dyI
dx

(cid:18)

T

(cid:19)

· α = −CT

x · γ

=

∂u
∂x

(cid:18)

T

·

1 +

(cid:19)

(cid:18)

∂∆0
∂u

T

(cid:19)

I

· γ1 +

i=1 (cid:18)
X

∂∆i−1
∂x

T

(cid:19)

· γi.

Although the steps are straightforward, implementing them correctly, let alone eﬃciently,

has required careful organization.

Diﬀerentiation with the Adjoint Method. Because this discrete dynamical system is a spe-
cial case of an algebraic system we could appeal to the augmented Lagrangian that we
constructed in Section 2.3.1 and then repeat the same linear algebra needed for the im-
plicit function theorem method. A more astute choice of Lagrangian, however, allows us to
directly exploit the structure of the constraints and the summary function.

28

MARGOSSIAN AND BETANCOURT

Given our choice of summary function we need to construct a Lagrangian function which

satisﬁes

∂L
∂xi
Because ∆i is a telescoping series,

(x, yI ) =

T

dyI
dxi (cid:19)

(cid:18)

· α.

a natural choice is

I

yI = u(x) +

∆i,

i=1
X

L(x, y) = yT

I · α

= uT (x) · α +

∆T
i

· α.

I

i=1
X

For the constraint term C we utilize a similar form as in the general algebraic case,

Q(x, y) =

=

I

i=1
X
I

i=1
X

cT
i

· λi

[yi − yi−1 − ∆i−1]T · λi,

where λi ∈ RN .

The adjoint system deﬁned by L and Q decouples into the equations

0 =

∂L
∂yi

+

∂Q
∂yi
T

∂∆i
∂yi (cid:19)
for i ∈ {1, . . . , I − 1} along with the terminal condition for i = I,

· α + λi+1 − λi −

∂∆i
∂yi (cid:19)

=

(cid:18)

(cid:18)

· λi.

T

0 = λI .

In other words the adjoint system deﬁnes a backward diﬀerence equation that we can solve
recursively from λI to λI−1 all the way to λ1.

Once we have solved for these adjoint states we can substitute them into the remaining

terms to give the desired directional derivative,

T

dyI
dx

(cid:18)

(cid:19)

· α =

∂L
∂x

+

∂Q
∂x
T

=

∂u
∂x

(cid:18)

·

1 +

(cid:19)

(cid:18)

∂∆0
∂u

T

(cid:19)

· (α − λ0) +

I

i=1 (cid:18)
X

∂∆i
∂x

T

(cid:19)

· (α − λi).

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

29

As expected the adjoint method has provided an alternative path to the same result
we obtained with the implicit function theorem. Indeed matching the two expressions for
(dy/dx)T · α, suggests taking

γi = α − λi,
in which case the intermediate diﬀerence equations that arise in both methods are exactly
the same as well! The advantage of the adjoint method is that we did not have to construct
Jg, Cy, or Cx, let alone manage their sparsity to ensure the most eﬃcient computation.

Note also that this procedure yields the same result for diﬀerence equations derived in
(Betancourt, Margossian and Leos-Barajas, 2020) only with fewer steps, and hence fewer
opportunities for mistakes.

2.3.3 Optimization. Another common class of ﬁnite-dimensional, implicit functions are
deﬁned as solutions to optimization problems. Given an objective function F : X × Y →
R we can deﬁne the output of the implicit function as the value which maximizes that
objective function for a given input,

y = argmax

v

F (x, v).

In a neighborhood U × V ⊂ X × Y where F (x, −) is convex for all x ∈ U , this implicit

function is also deﬁned by the diﬀerential constraint

c(x, y) =

∂F
∂y

(x, y) = 0,

which allows us to apply our machinery to evaluate the derivatives of this implicit function.
The convexity constraint is key here; without it the constraint function will identify only
general extrema which can include not only maxima but also minima and saddle points.
When X ⊂ RI and Y ⊂ RJ the diﬀerential constraint function reduces to an algebraic
system and we can directly apply the results of Section 2.3.1. In particular without any
assumptions on the objective function there is no diﬀerence in the implementation of the
implicit function theorem or adjoint methods.

We start by setting the summary function to the identity so that Jg = I and

β = JT

g · α = α.

Next we compute

(Cy)ij =

∂ci
∂yj

(x, y) =

∂2F
∂yi∂yj

(x, y),

for example analytically or with higher-order automatic diﬀerentiation (Griewank and Walther,
2008; Betancourt, 2018). Conveniently when the optimization is implemented with a higher-
order numerical method this Hessian matrix will already be available at the ﬁnal solution.

30

MARGOSSIAN AND BETANCOURT

Once Cy has been constructed we then solve the linear system

Finally we construct

and then evaluate

∂2F
∂yi∂yj

(x, y) · γ = α.

(Cx)ij =

∂ci
∂xj

=

∂2F
∂yi∂xj

,

JT · α = −CT

x · γ.

This ﬁnal contraction deﬁnes a second-order directional derivative of the objective func-
tion. Conveniently it can be evaluated with higher-order automatic diﬀerentiation without
having to explicitly construct Cx.

These optimization problems become more sophisticated with the introduction of an
additional equality constraint so that the output of the implicit function is now deﬁned by
the condition

y = argmax

s

F (x, s) such that k(x, y) = 0,

for the auxiliary constraint function k : RI × RJ → RK.

In order to deﬁne an implicit system that consistently incorporates both of these con-
straints we have to augment the output space. We ﬁrst introduce the Lagrange multipliers
µ ∈ M ⊂ RK and the augmented objective function

Φ : X × Y × M → R

(x, y, µ)

7→ F (x, y) + µ · k(x, y).

A consistent solution to the constrained optimization problem is then given by

(y, µ) = argmax

s,m

Φ(x, s, m).

In other words we can incorporate all of the constraints directly on the augmented output
space ζ = (y, µ) and then project back down to the original output space using the summary
function g : (y, µ) 7→ y.

Within a suﬃciently convex neighborhood we can also deﬁne the constrained optimiza-

tion problem with a constraint function over this augmented output space,

c(x, ζ) =

∂Φ
∂ζ

(x, ζ) = 0,

which allows us to apply our methods for evaluating directional derivatives.

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

31

From the projective summary function we ﬁrst construct

β = JT

g · α.

Next we diﬀerentiate the constraint function on the augmented output space,

and then solve the linear system

Finally we construct

and then evaluate

(Cζ)ij =

∂2Φ
∂ζi∂ζj

(x, ζ),

∂2Φ
∂ζi∂ζj

(x, ζ) · γ = β.

(Cx)ij =

∂2Φ
∂ζi∂xj

,

JT · α = −CT

x · γ.

As before this ﬁnal contraction deﬁnes a second-order directional derivative that can be
directly evaluated with higher-order automatic diﬀerentiation, although this time on the
augmented output space.

Inequality constraints introduce an additional challenge. While the Karush-Kuhn-Tucker
conditions deﬁne an appropriate system of constraints (Karush, 1939; Kuhn and Tucker,
1951), the dimension of the constraint space varies with the input x as diﬀerent inequality
constraints become active. Moreover even when the objective function and the additional
constraint functions are all smooth the implicit function they deﬁne might not be diﬀeren-
tiable at every x. We leave a detailed treatment of this problem to future work.

3. AUTOMATIC DIFFERENTIATION OF INFINITE-DIMENSIONAL IMPLICIT
FUNCTIONS

With care in how derivatives are deﬁned we can immediately generalize the ﬁnite di-
mensional methods for evaluating directional derivatives of implicit functions to inﬁnite
dimensional systems, for example systems that implicitly deﬁne entire trajectories, ﬁelds,
or even probability distributions. In this section we review the basics of diﬀerentiable, in-
ﬁnite dimensional spaces and the generalization of the implicit function theorem before
generalizing the directional derivative evaluation methods and demonstrating them on two
instructive examples.

32

MARGOSSIAN AND BETANCOURT

3.1 The Inﬁnite-Dimensional Implicit Function Theorem

The machinery of diﬀerential calculus over the real numbers generalizes quite naturally
to a Fr´echet calculus over Banach vector spaces. In this section we review the key concepts
and then use them to construct an inﬁnite-dimensional implicit function theorem. For a
more in depth presentation of these topics see for example Kesavan (2020).

3.1.1 Fr´echet derivatives. The Fr´echet derivative generalizes the concept of a derivative
that we introduced for real spaces to the more general Banach vector spaces, or more
compactly Banach spaces. Banach spaces include not only ﬁnite-dimensional Euclidean
vector spaces but also inﬁnite-dimensional function spaces.

Consider two Banach spaces, X and Y , and a function f : X → Y mapping between
them. If f is Fr´echet diﬀerentiable then the Fr´echet derivative assigns to each input point
x ∈ X a bounded, linear map

δf
δx

: X → BL(X, Y ),

where BL(X, Y ) is the space of bounded, linear functions from X to Y . In other words the
Fr´echet derivative of f evaluated at x deﬁnes a bounded, linear map from X to Y ,

δf
δx

(x) : X → Y.

Note that unlike the total derivative we introduced on real spaces the Fr´echet derivative
is deﬁned directly on a vector space and so there is no distinction between locations and
directions.

If Z is a third Banach space then the composition of f with g : Y → Z deﬁnes a map

from X to Z,

g ◦ f : X → Z.

The Fr´echet derivative of this composition,

follows a chain rule,

δ(g ◦ f )
δx

(x) : X → Z,

δ(g ◦ f )
δx

(x) =

δg
δx

(f (x))

◦

Y →Z

.

(x)

δf
δx
X→Y

A binary map h : X × Y → Z also admits a partial Fr´echet derivative. For example the

|

{z

}

| {z }

partial Fr´echet derivative of h with respect to the ﬁrst input deﬁnes a map

δh
δx

: X × Y → BL(X, Z).

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

33

Equivalently the partial Fr´echet derivative of h evaluated at (x, y) is a bounded, linear map
from X to Z,

δh
δx

(x, y) : X → Z.

Similarly we can deﬁne a partial derivative with respect to the second output as

with

δh
δy

: X × Y → BL(Y, Z)

δh
δy

(x, y) : Y → Z.

When a binary map like h is composed with unary maps in each argument then we can
use the chain rule to deﬁne a notion of a total Fr´echet derivative. Let h : X × Y → Z,
f : W → X and g : W → Y . The component-wise composition h(f ( ), g( )) then deﬁnes a
unary map

q = h(f ( ), g( )) : W → Z,

with the corresponding Fr´echet derivative

δq
δw

(w) : W → Z,

which decomposes into contributions from each argument,

δq
δx

(w) =

δh
δx

(f (w), g(w)) ◦

δf
δw

(w) +

δh
δy

(f (w), g(w)) ◦

δg
δw

(w).

3.1.2 The Implicit Function Theorem. The ﬁnite dimensional implicit function theorem
immediately generalizes to Banach spaces with the use of Fr´echet derivatives. Consider a
Banach space of known inputs, X, a Banach space of unknown outputs, Y , a Banach space
of constraint values, Z, and the constraint function

c : X × Y → Z

(x, y)

7→ c(x, y).

Once again, we examine a particular input, x ∈ X, and the neighborhoods x ∈ U ⊂ X,
V ⊂ Y and W ⊂ Z.

If c is Fr´echet diﬀerentiable across U × V then it deﬁnes two partial Fr´echet derivatives

and

δc
δx

δc
δy

(x, y) : U → W

(x, y) : V → W.

34

MARGOSSIAN AND BETANCOURT

When δc/δy(y) deﬁnes a bijection from V to W we can also deﬁne a corresponding

inverse operator,

δc
δy

(cid:18)

−1

(x, y)

: W → V.

(cid:19)

In this case the implicit function theorem guarantees that the kernel of the constraint
function, c−1(0), implicitly deﬁnes a Fr´echet diﬀerentiable function from the input space
to the output space, f : U → V that satisﬁes c(x, f (x)) = 0.

As in the ﬁnite-dimensional case we can calculate the Fr´echet derivative of this implicit

function by diﬀerentiating the constraint function,

0 =

=

δc
δx
δc
δx

(x)

(x, y) +

δc
δy

(x, y) ◦

δy
δx

(x).

Because δc/δy is invertible at (x, f (x)) we can immediately solve for δy/δx to give

δf
δx

(x)

= −

U →V

δc
δy

(cid:18)

(x, f (x))

(cid:19)

W →V

−1

◦

δc
δx

(x, f (x))

.

U →W

|
Moving forward we will denote Jf = δf /δx(x) the Fr´echet Jacobian.

{z

}

|

{z

}

| {z }

3.2 Evaluating Directional Derivatives of Inﬁnite-Dimensional Implicit Functions

While we can handle inﬁnite dimensional spaces mathematically any practical automatic
diﬀerentiation implementation will be restricted to ﬁnite dimensional inputs and ﬁnal out-
puts. To that end we will assume that the input space is real, X = RI , while allowing
the output space Y to be inﬁnite dimensional, for example corresponding to a smooth
trajectory or a latent ﬁeld. We will use the summary function, however, to project that
potentially-inﬁnite dimensional output to a ﬁnite-dimensional real space, g : Y → RJ . For
example we might consider a dynamical system that deﬁnes an entire trajectory but project
out only the ﬁnite-dimensional ﬁnal state.

In order to implement such a system in a reverse mode automatic diﬀerentiation library

we then need to be able to evaluate the reverse directional derivative

J †
g◦f (x)(α) =

J †
f (x) ◦ J †

g (f (x))

(α).

Because g ◦ f is a real-to-real map the total action is given by a matrix-vector product,

(cid:0)

(cid:1)

J †
g◦f (x)(α) = JT

g◦f · α,

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

35

but the component operators J †
real space.

f (x) and J †

g (f (x)) will not be unless Y is a ﬁnite-dimensional

Here we will consider the generalizations of the three methods for evaluating this direc-

tional derivative that we constructed in Section 2.

3.2.1 Trace Method. Although we can’t practically construct a numerical method for an
implicit function f : RI → Y with inﬁnite-dimensional output space, we often can construct
numerical methods for the ﬁnite-dimensional composition g ◦ f : RI → RJ . Numerical
integrators for ordinary and partial diﬀerential equations, for example, discretize Y in
order to approximate the ﬁnite dimensional outputs of g ◦ f without having to confront an
inﬁnite dimensional space directly.

Because the intermediate calculations of the numerical solver will be ﬁnite-dimensional
we immediately apply the trace method discussed in Section 2.2.1, automatically diﬀer-
entiating through each iteration of the solve, to approximate Jg◦f (x)(α). As in the ﬁnite-
dimensional case, however, this direct approach is often too computationally expensive and
memory intensive to be practical.

3.2.2 Inﬁnite-Dimensional Implicit Function Theorem. In theory the reverse directional

derivative is given immediately by the implicit function theorem,

J †
g◦f (x)(α) =

J †
f (x) ◦ J †

g (f (x))

(α)

=

−

(cid:0)





δc
δx

(cid:18)

(cid:1)
(x, f (x))

†

◦

(cid:19)

 (cid:18)

δc
δy

−1

†

(x, f (x))

!

(cid:19)

◦ J †

g (f (x))



(α).



Unfortunately whenever Y is inﬁnite-dimensional each of these Fr´echet derivatives will be
inﬁnite dimensional operators that are diﬃcult, if not impossible, to implement in practice.
For example J †
g (f (x)) maps the ﬁnite covector α to an inﬁnite dimensional space that can’t
be represented in ﬁnite memory.

3.2.3 The Inﬁnite-Dimensional Adjoint Method. With a careful use of Fr´echet deriva-
tives the adjoint method deﬁned in Section 2.2.3 generalizes to the case where Y and
Z are general Banach spaces. As usual we consider a particular input, x ∈ X, and the
neighborhoods x ∈ U ⊂ X, V ⊂ Y , and W ⊂ Z.

We ﬁrst deﬁne a binary Lagrangian functional

L : U × V → R

that gives a unary functional when we substitute the solution of the implicit function,

L(x) = L(x, f (x))

36

MARGOSSIAN AND BETANCOURT

with the Fr´echet derivative

δL
δx

(x) = J †

g◦f (x)(α).

Next we introduce a mapping Λ : W → R that preserves the kernel of the constraint
function, Λ ◦ c(x, y) = 0 whenever c(x, y) = 0. Composing this mapping with the constraint
function gives the constraint Lagrangian functional,

Q = Λ ◦ c : U × V → R,

with Q(x, f (x)) = 0 for all x ∈ U .

Together these two functionals deﬁne an augmented Lagrangian functional,

along with the corresponding unary functional

J = L + Q,

J (x) = J (x, f (x))

= L(x, f (x)) + Q(x, f (x))

= L(x, f (x)),

because the constraint functional vanishes by construction when evaluated at the solution
to the constraint problem.

The total derivative of this augmented unary functional is given by

δJ
δx

(x) =

=

δJ
δx
δJ
∂x

(x, f (x))

(x, f (x)) +

δJ
δy

(x, f (x)) ◦

δf
δx

(x).

If we could engineer a map Λ such that the adjoint system δJ /δy vanishes,

0 =

=

=

δJ
δy
δL
δy
δL
δy

(x, f (x))

(x, f (x)) +

(x, f (x)) +

δQ
δy
δΛ
δc

(x, f (x))

(x, f (x)) ◦

δc
δy

(x, f (x))

then the reverse directional derivative would reduce to

J †
g◦f (x)(α) =

=

=

δJ
δx
δL
δx
δL
δx

(x, f (x))

(x, f (x)) +

(x, f (x)) +

δQ
δx
δΛ
δc

(x, f (x))

(x, f (x)) ◦

δc
δx

(x, f (x)).

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

37

Unfortunately if Y is inﬁnite-dimensional then the adjoint system also becomes inﬁnite-
dimensional, and the general Fr´echet derivatives will typically be too ungainly to implement
in practice.

One important exception is when Y is a Sobolev space. Informally a Sobolev space
of order k is an inﬁnite-dimensional Banach space comprised of integrable, real-valued
functions whose ﬁrst k derivatives are suﬃciently well-deﬁned. What makes Sobolev spaces
so useful is that a large class of functionals over these spaces can be written as integrals.
Consider for example the input space T ⊆ R, the output space S ⊆ RN , and the Sobolev
space Y of k-times diﬀerentiable functions y : T → S, such as those arising from the
solutions to k-th order ordinary diﬀerential equations. Any integral of the form

G(y) =

dt g

≡

ZT

ZT

dt g

dy
dt

t, y(t),

(t), . . . ,

dKy
dtK (t)
(cid:19)
(cid:18)
t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)
(cid:16)

deﬁnes a unary, real-valued functional G : Y → R whose Fr´echet derivative is given by

δG
δy

(y) =

δ
δy

ZT

dt g

(cid:16)

=

dt

ZT

"

t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)

∂g
∂y

K

+

Xk=1

∂g
∂y(k)

t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)
(cid:16)
t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)
(cid:16)

·

δy(k)
.
δy #

Similarly if X ⊆ RI then

G(x, y) =

dt g

ZT

x, t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)

(cid:16)

deﬁnes a binary, real-valued functional G : X × Y → R. Consequently we can construct a
large class of Lagrangian functionals and constraint functionals as integrals,

L(x, y) =

Q(x, y) =

dt l

x, t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)
(cid:16)
x, t, y(t), y(1)(t), . . . , y(K)(t)
dt q
(cid:17)
(cid:16)

ZT

ZT

38

MARGOSSIAN AND BETANCOURT

with the corresponding augmented Lagrangian,

J (x, y) =

dt j

=

ZT

ZT

x, t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)
(cid:16)

dt

l

(cid:20)

(cid:16)
+ q

x, t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)
x, t, y(t), y(1)(t), . . . , y(K)(t)
(cid:16)

.

(cid:17) (cid:21)

The choice of l needs to verify the condition that

L(x, f (x))

δ
δx
δ
δx

J †
g◦f (x)(α) =

=

=

ZT
dt

ZT

(cid:20)

dt l

(cid:16)
∂l
∂x
K

+

Xk=1

x, t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)
x, t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)

(cid:16)

∂l
∂y(k)

t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)
(cid:16)

·

δy(k)
δx

.

(cid:21)

Depending on the problems, choosing l can be straightforward or require a bit more work,
as we will see in the examples. The choice of q needs to satisfy the condition that, for all
x ∈ U ,

0 = Q(x, f (x))

=

dt q

ZT

x, t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)
(cid:16)

.

One straightforward strategy is to deﬁne the integrand q such that it vanishes whenever
evaluated at the solution, (x, f (x)). We can accomplish this for example by taking q to be
the Sobolev inner product of the constraint function, f , with an auxiliary function λ ∈ Z,

q(x, f (x)) = hλ, c(x, f (x))i ,

where h , i denotes the Sobolev inner product. When evaluated at the solution the con-
straint function c vanishes so that the above inner product, and hence the integrand q, also
vanish.

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

39

When using functionals of this form the reverse directional derivative becomes

J †
g◦f (x)(α) =

δJ
δx

(x, f (x))

=

dt

ZT

"

∂j
∂x

∂j
∂y
K

+

+

=

dt

ZT

"

x, t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)
(cid:16)
x, t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)

(cid:16)

·

dy
dx

(t)

∂j
∂y(k)

x, t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)

(cid:16)

·

x, t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)
(cid:16)
x, t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)

(cid:16)

·

dy
dx

(t)

∂j
∂y(k)

x, t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)

(cid:16)

·

Xk=1
∂j
∂x

∂j
∂y
K

+

+

Xk=1

δy(k)
δy

·

dy
dx

(t)

#

dy(k)
dx

(t)

.

#

Because the elements of the Sobalev space are continuous we can simplify the terms in the
last line by repeated application of Stokes’ Theorem,

dt

∂j
∂y(k)

·

dy(k)
dx

=

ZT

k

(−1)k′

dk′
dtk′

"

∂j
∂y(k)

(cid:18)

(cid:19)

dk−k′−1
dtk−k′−1

dy
dx

(cid:18)

(cid:19)#∂T

Xk′=0
+ (−1)k

dt

dk
dtk

∂j
∂y(k)

(cid:18)

(cid:19)

·

dy
dx

,

ZT

where we have dropped the arguments to ease the notational burden. Substituting this into
the reverse directional derivative gives

J †
g◦f (x)(α) =

dt

∂j
∂x

k

(−1)k′

ZT
K

+

Xk=1

Xk′=0

+

dt

ZT

∂j
∂y

"

+

dk′
dtk′

"
K

(cid:18)
(−1)k dk
dtk

Xk=1

∂j
∂y(k)

dk−k′−1
dtk−k′−1

(cid:19)

∂j
∂y(k)

·

(cid:19)#

(cid:18)

(cid:19)#∂T

dy
dx

(cid:18)
dy
dx

.

40

MARGOSSIAN AND BETANCOURT

If we can engineer a q such that j solves the diﬀerential adjoint system

dk′
dtk′

∂j
∂y(k)

(cid:18)

(cid:19)

dk−k′−1
dtk−k′−1

dy
dx

(cid:18)

(cid:19)#∂T

K

k

0 =

0 =

Xk=1
∂j
∂y

(−1)k′

Xk′=0

K

+

"
(−1)k dk
dtk

Xk=1

∂j
∂y(k)

(cid:18)

(cid:19)

then the reverse directional derivative reduces to the manageable form

J †
g◦f (x)(α) =

=

dt

dt

ZT

ZT

∂j
∂x

(cid:20)
+

x, t, y(t), y(1)(t), . . . , y(K)(t)
(cid:16)
(cid:17)
∂l
∂x
∂q
∂x

x, t, y(t), y(1)(t), . . . , y(K)(t)
(cid:17)
(cid:16)
x, t, y(t), y(1)(t), . . . , y(K)(t)
(cid:16)

(cid:17) (cid:21)

.

For diﬀerent choices of Y the form of the functionals, their Fr´echet derivatives, and
the resulting diﬀerential adjoint system will vary, but the basic procedure of the adjoint
method remains the same. By considering more sophisticated Sobelev spaces this general
adjoint method can be applied to partial diﬀerential equations and other more sophisticated
inﬁnite-dimensional implicit systems.

3.3 Demonstrations

The particular diﬀerential adjoint system we have derived in Section 3.2.3 is immediately
applicable to implicit systems deﬁned by ordinary diﬀerential and algebraic diﬀerential
equations. In this section we demonstrate that application on two such systems.

3.3.1 Ordinary Diﬀerential Equations. Consider a time interval T = [0, τ ] ⊂ R and
the N -dimensional trajectories that map each time point to an N -dimensional state, y :
T → RN . The space of trajectories that are at least once-diﬀerentiable forms a ﬁrst-order
Sobalev space, Y .

A linear system of ﬁrst-order, ordinary diﬀerential equations

dy
dt

= r(x, y, t),

along with an initial condition y(0) = u(x), deﬁnes a separate constraint for the trajectory
behavior at each t ∈ T ,

c(x, y)(t) =

dy
dt

(t) − r(x, y)(t)

c(x, y)(0) = y(0) − u(x).

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

41

Collecting all of these constraints together deﬁnes an inﬁnite-dimensional constraint func-
tion c : X × Y → Z where Z is also the space of diﬀerentiable functions that map from T
to RN .

The summary function g : y 7→ y(τ ) projects inﬁnite-dimensional trajectories down to
their N -dimensional ﬁnal states so that the composition g ◦ f maps inputs x ∈ X to a
ﬁnal state at time t = τ . In order to implement this map into a reverse mode automatic
diﬀerentiation library we need to be able to implement the reverse directional derivative
Jg◦f (x)(α).

To implement the adjoint method we need a Lagrangian functional that satisﬁes

dL
dxi

(x, f (x)) =

T

(T )

· α.

dy
dxi

(cid:18)
Integrating the deﬁning diﬀerential equation gives an integral functional that provides a
particularly useful option,

(cid:19)

L(x, y) = uT (x) · α +

dt rT (x, y, t) · α.

τ

0
Z

Next we need to construct a functional Q that vanishes when when evaluated at the

implicit solution. Integrating over the constraint function gives

Q(x, y) =

y(0) − u(x)
(cid:21)

(cid:20)
for any function λ : R → RN and constant µ ∈ RN .

(cid:20)

0
Z

T

· µ +

T

dt

dy
dt

(t) − r(x, y, t)
(cid:21)

T

· λ(t),

Substituting these integral functionals into the diﬀerential adjoint system that we derived

in Section 3.2.3 yields the system

0 =

0 =

dy
dx

∂r
∂y

(cid:18)

(cid:18)

T

(0)

· (µ − λ(0)) +

(cid:19)

T

(cid:18)

dy
dx

(x, y, t)

· (α − λ(t)) −

(cid:19)

(cid:19)

(t).

dλ
dt

T

(τ )

· λ(τ )

The boundary terms vanish if we take µ = λ(0) and λ(τ ) = 0 while the diﬀerential term
vanishes for the λ(t) given by integrating λ(τ ) = 0 backwards from t = τ to t = 0. In
other words the diﬀerential adjoint system deﬁnes a linear, ﬁrst-order ordinary diﬀerential
equation that reverses time relative to our initial ordinary diﬀerential equation.

Once we’ve solved for λ(t) the reverse directional derivative is given by the partial

derivatives of the augmented Lagrangian with respect to x,

J †
g◦f (x)(α) =

∂u
∂x

(cid:18)

T

(x)

· (α − λ(0)) +

(cid:19)

T

dt

∂r
∂x

(cid:18)

0
Z

T

(x, y, t)

· (α − λ(t)) .

(cid:19)

42

MARGOSSIAN AND BETANCOURT

While we do have to solve both the nominal and adjoint diﬀerential equations, these solves
require evaluating only ﬁnite-dimensional derivatives. We have completely avoided any
inﬁnite-dimensional Fr´echet derivatives.

3.3.2 Diﬀerential Algebraic Equations. Introducing an algebraic constraint to the previ-
ous systems deﬁnes a diﬀerentiable algebraic system, or DAE. A DAE might, for example,
impose the constraint that the component states at each time sum to one so that the states
can model how the allocation of a conserved quantity evolves over time.

To simplify the derivation we start by decomposing the trajectories y(t) ∈ RN into a
diﬀerential component, yd ∈ RD, and an algebraic component, ya ∈ RA, with N = D + A.
A diﬀerential algebraic constraint function can similarly be decomposed into a diﬀerential
constraint function,

cd(x, y, ˙y, t) ∈ RD,

where ˙y is shorthand for dy/dt, and an algebraic constraint function,

with c = (cd, ca)T .

ca(x, y, t) ∈ RA.

If the diﬀerential constraint is given by a linear, ﬁrst-order diﬀerential equation then the

diﬀerential constraint function becomes

cd(x, y, ˙y, t) = ˙yd − rd(x, y, t)
cd(x, y, ˙y, 0) = y(0) − u(x).

When the constraints are consistent this diﬀerential algebraic system implicitly deﬁnes
a map from inputs x ∈ X to N -state trajectories, y ∈ T × RN . We can also write this as a
map from inputs and times to states,

such that cd(x, f (x), ˙f (x), t) = 0 and ca(x, f (x), t) = 0.

f : X × T → RN ,

As in the previous example we will consider a summary function that projects the
inﬁnite-dimensional trajectories down to their N -dimensional ﬁnal states, g : y 7→ y(τ ), so
that the composition g ◦ f maps inputs x ∈ X to a ﬁnal state at time t = τ . Our goal is
then to evaluate the ﬁnite-dimensional reverse directional derivative Jg◦f (x)(α).

As with ordinary diﬀerential systems integrating the trajectory provides an appropriate

integral functional,

L(x, y) = uT (x) · α +

dt ˙yT (x, t) · α

τ

0

Z

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

43

that satisﬁes

dL
dx

(x, f (x)) =

T

(x, τ )

· α.

df
dx

(cid:18)
Unlike in the ordinary diﬀerential case, however, the diﬀerential algebraic system does not
immediately provide an analytical expression for ˙y(x).

(cid:19)

The ﬁrst-order linear diﬀerential equation does provide the derivative of the diﬀerential

component,

˙yd = rd.

In order to obtain the derivative of the algebraic component we have to diﬀerentiate the
algebraic constraint,

0 =

=

ca(x, y, t)

d
dt
∂ca
∂yd ˙yd(x, t) +

∂ca
∂ya ˙ya(x, t) +

∂ca
∂t

(x, t),

or

˙ya = −

(cid:18)
where we assume ∂ca/∂ya is square invertible.

(cid:21)

(cid:20)

−1

∂ca
∂ya

∂ca
∂yd ˙yd +

∂ca
∂t

,

(cid:19)

The constraint functional Q is identical to that from the ordinary diﬀerential equation

system,

C(x, y) =

y(0) − u(x)
(cid:21)

(cid:20)

T

· µ +

τ

0
Z

dt c(x, y, t)T · λ(t).

Plugging J = L + Q into the results of Section 3.2.3 gives the system

0 =

0 =

dy
dxi

(cid:20)

∂r
∂y

(cid:18)

(0) −

du
dxi

T

(x)

· µ +

(cid:21)

T

(x, y, t)

· α +

(cid:19)

∂c
∂y

(cid:18)

T

dy
dxi (cid:19)

(cid:18)

(x, y, t)

τ

· λD0(t)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
· λ(t) − ˙λD0(t)
(cid:12)

T

0

(cid:19)

44

where

MARGOSSIAN AND BETANCOURT

T

(cid:19)

(cid:18)

=

· λ(t)

(x, y, t)

λD0(t) =

∂c
∂ ˙y
∂cd
∂ ˙yd
∂ca
∂ ˙yd
ID×D 0D×A
0A×D 0A×A(cid:21)
λ1:D(t)
.
01×A (cid:21)
Decomposing each term in the diﬀerential adjoint system into algebraic and a diﬀerential

∂cd
∂ ˙ya
∂ca
∂ ˙ya #

· λ(t)

· λ(t)

=

=

"

(cid:20)

(cid:20)

components gives

T

∂r
∂yd (x, y, t)
(cid:17)
∂r
(x, y, t)
∂ya

· α +
T



(cid:16)

∂cd
∂yd

(x, y, t)

T

· λd(t) +
T

(cid:16)
· α +

∂cd
∂ya

(cid:17)
(x, y, t)

(cid:16)
· λd(t) +


(cid:16)

which deﬁnes an adjoint DAE

(cid:16)

(cid:17)

(cid:17)

∂ca
∂yd

(x, y, t)

T

· λa(t) − ˙λD0(t)

∂ca
∂ya

(cid:17)
(x, y, t)

(cid:16)

(cid:17)

T

· λa(t)

=

0D
0A(cid:21)

.

(cid:20)






To ensure that the boundary term vanish we need to set µ = λD0(0) and λD0(τ ) = 0N .

The algebraic component of λ at t = τ is also given by

∂r
∂ya

0A =

(cid:18)

λa(τ ) = −

T

(x, y, τ )

· α + 0 +

∂ca
∂ya

(cid:20)(cid:18)

(cid:19)

(x, y, τ )

−1

·

(cid:19)(cid:21)

(cid:18)

(cid:18)
∂r
∂ya

∂ca
∂ya

T

(x, y, τ )

· λa(τ )

(cid:19)

T

(x, y, τ )

· α,

(cid:19)

where we recall our assumption that ∂ca/∂ya is square invertible.

Now we can ensure that the diﬀerential term vanishes by integrating this adjoint DAE
backwards from the terminal condition λ(τ ) = (λd(τ ), λa(τ ))T at t = τ to an initial
condition at t = 0.

Once we have solved for λ(t) the reverse directional derivative reduces to

J †
g◦f (x)(α) = −

T

du
dxi (cid:19)

(cid:18)

· λD0(0) +

τ

dt

0

Z

(cid:18)

∂r
∂xi

(x, y, t) + λT ∂c
∂xi

T

(x, y, t)

· λ(t).

(cid:19)

4. DISCUSSION

The implicit function theorem allows us to construct an expression for the directional
derivatives of an implicit function as a composition of Fr´echet derivatives. The adjoint

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

45

method computes these directional derivatives directly without evaluating any of the in-
termediate terms.

When the output of the implicit function is ﬁnite-dimensional these diﬀerential oper-
ators can be implemented with linear algebra, although care and experience is required
to ensure that the linear algebra operations are as eﬃcient as possible. While the adjoint
method yields the same result, it naturally incorporates any available structure in the im-
plicit system so that optimal performance can be achieved automatically as we saw in our
discussion on diﬀerence equations (Section 2.3.2).

If the output of the implicit function is inﬁnite-dimensional then the component Fr´echet
derivatives that make up the directional derivatives of the implicit function can no longer be
evaluated directly, making the composition intractable. Implementing the adjoint method
also requires Fr´echet derivatives which in general we cannot evaluate. In the important
special case where the output of the implicit function falls into a Sobolev space, however, we
can engineer the augmented Lagrangian so that the Fr´echet derivatives reduce to tractable
functional derivatives. This is notably the strategy we deploy when in the case of ODEs
and DAEs (Section 3.3).

While the adjoint method is more generally applicable it is not as systematic as the
implicit function theorem method. The practicality and performance of the method depends
on the choice of Lagrangian and constraint functionals. Engineering performant functionals,
let alone valid functionals at all, is by no means trivial. Fortunately in many problems the
structure of the implicit system guides the design.

Beyond the implicit function theorem and adjoint methods, we may use the trace method
which automatically diﬀerentiates through the trace of a numerical solver. In most cases
this approach leads to computationally expensive and memory intensive algorithms.

For ﬁnite-dimensional systems we could also construct a “forward method” that com-
putes the Jacobian J = Jg◦f before evaluating its action on an input sensitivity or adjoint
to form the wanted directional derivatives. As we discussed in Section 2.2.1, however, fully
computing J ﬁrst is always less eﬃcient that the iterative evaluation of the directional
derivative; see also Gaebler (2021).

Although not as general, we can also construct a forward method that fully computes
the Jacobian J = Jg◦Jf for certain inﬁnite-dimensional problems. This approach notably
applies to certain classes of ODEs and DAEs (Appendix A). In these cases the com-
putational trade-oﬀs between the forward method and the adjoint method is more nu-
anced; which method is more eﬃcient depends on the speciﬁc of the problem. For example
Rackauckas et al. (2018) compares the forward and adjoint approaches to implementing
automatic diﬀerentiation for ODEs. For small ODE systems the overhead cost associated
with solving the adjoint system can make the method relatively slow, but as the size of the
system and the dimension of x increases the adjoint method beneﬁts from superior scal-

46

MARGOSSIAN AND BETANCOURT

ability. See also Hindmarsh and Serban (2020); Betancourt, Margossian and Leos-Barajas
(2020) for additional scaling discussions.

5. ACKNOWLEDGMENT

We thank David Childers, David Kent, and Dalton AR Sakthivadivel for helpful discus-

sion.

APPENDIX A: INFINITE-DIMENSIONAL FORWARD METHOD

As in Section 3.2 consider ﬁnite-dimensional real input, X = RI , an inﬁnite-dimensional
output space Y , and the summary function g : Y → RJ . The forward method explicitly
computes the matrix representation of the full Jacobian, Jg◦f , before contracting this ma-
trix with a sensitivity or adjoint vector to implement directional derivatives for automatic
diﬀerentiation.

The implicit function theorem prescribes a composite expression for Jg◦f , but in general
we cannot evaluate the intermediate Fr´echet derivatives. In certain special cases, however,
we can bypass the implicit function theorem and evaluate Jg◦f directly.

As with the adjoint method, we focus on the special case where Y is a Sobolev space.
Theoretically we can reduce a key Fr´echet derivative to a functional derivative, which we
can evaluate by solving a forward diﬀerential system. In practice our ability to construct
such a system and solve it depends on the speciﬁcs of the problem.

Let Y be the order K Sobolev space of functions T ⊂ R → RN , and suppose that we

can construct a functional

which satisﬁes

P : X × Y → RJ
x, y

7→ P(x, y),

δP
δx

(x) = Jg◦f (x).

In addition, assume there exists such a functional which takes the form of an integral,

For example if our summary function g is a Sobolev inner product with the implicit func-

P(x, y) =

dt p(x, t, y(t), ...).

ZT

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

47

tion,

then we can take

(g ◦ f )(x) = hγ, f (x)i

dt γ(t) f (x, t)

dt γT (t) · y(x, t)

=

=

Z

Z

p = γT (t) · y(x, t),

and obtain a satisfactory functional P.

Taking the derivative with respect to x gives,

δP
δx

=

=

dt

dt

dp
dx

∂p
∂x

ZT

ZT

x, t, y(t), y(1)(t), · · · , y(K)(t)
(cid:17)
(cid:16)

K

∂p
∂y(k)

dy(k)
dx

,

+

Xk=0

where crucially the Fr´echet derivative reduces to a functional derivative.

In order to evaluate this integral we need to evaluate the derivatives of the implicit func-
tion, dy(k)/dx, at all times t ∈ T . In theory we could achieve this by fully constructing the
ﬁrst-order Fr´echet derivative from the implicit function theorem, repeatedly diﬀerentiating
it, and then evaluating all of those Fr´echet derivatives at each time t.

The need to evaluate Fr´echet derivatives, however, makes this approach infeasible in
practice. A more viable alternative is to evaluate the derivatives only at speciﬁc times,
where they reduce to manageable ﬁnite-dimensional objects.

By deﬁnition our constraint function deﬁnes a map c : X × Y → Z where Y and Z
are both the space of functions which map from T to RN . In this case the constraint
function can equivalently be deﬁned as a collection of maps X × Y → RN for each t ∈ T .
Denoting these maps as c(x, y, t) the implicit function f : X → Y is deﬁned by the system
of constraints,

c(x, f (x), t) = 0, ∀ t ∈ T.

Fixing t and then diﬀerentiating with respect to the input x gives

0 =

=

∂c
∂x

∂c
∂x

(t) +

(t) +

K

Xk=0
K

Xk=0

∂c
∂y(k)

dy(k)
dx

(t)

∂c
∂y(k)

dy
dx

(cid:18)

(cid:19)

(k)

(t).

48

MARGOSSIAN AND BETANCOURT

This forward diﬀerential system implicitly deﬁnes the derivative evaluations at each t as
a diﬀerential equation. Once we’ve solved for y(t) we can, at least in theory, solve this
forward diﬀerential system for each dy(k)/dx (x, f (x), t) and then evaluate

K

dt

∂p
∂y(k)

dy(k)
dx

K

=

Xk=0 ZT
as a sum of one-dimensional numerical quadratures.

Xk=0

ZT

dt

∂p
∂y(k)

dy(k)
dx

,

If c(x, y, t) is a linear a function of dy/dt and does not depend on higher-order derivatives,

c(y, x, t) =

dy
dt

(t) − f (x, y, t) = 0,

then the forward diﬀerential system becomes particularly manageable. In particular the
forward diﬀerential system is also linear in the ﬁrst-order derivative with respect to t,

dc
dx

(y, x, t) =

d
dt

dy
dx

(t) −

∂f
∂x

(x, y, t) −

∂f
∂y

dy
dt

(t) = 0.

In the absence of such a linearity we need to solve for the evaluations of the trajectory y,
the ﬁrst-order derivative dy/dx, and the higher-order derivatives of y at the given x and
every t needed for the numerical quadratures.

For a demonstration of this forward approach on certain ODEs and DAEs see (Hindmarsh and Serban,

2020).

Finally similar to the Sobolev adjoint method (Section 3.2.3) the above derivation gener-
alizes immediately to constrained systems over Sobolev spaces of functions T ⊂ RM → RN ,
with M > 1. Here instead of an ordinary diﬀerential forward system we recover a partial
diﬀerential forward system, and the Jacobian is recovered as a sum of multidimensional
integrals instead of one-dimensional integrals.

REFERENCES

Baydin, A. G., Pearlmutter, B. A., Radul, A. A. and Siskind, J. M. (2018). Automatic diﬀerentiation

in machine learning: a survey. Journal of Machine Learning Research 18 1 – 43.

Bell, B. M. and Burke, J. V. (2008). Algorithmic Diﬀerentiation of Implicit Functions and Optimal
Values. In Advances in Automatic Diﬀerentiation. Lecture Notes in Computational Science and Engi-
neering, (C. H. Bischof, H. M. B¨ucker, P. Hovland, U. Naumann and J. Utke, eds.) 64 Springer, Berlin,
Heidelberg.

Betancourt, M.

(2018). A Geometric Theory

of Higher-Order Automatic Diﬀerentiation.

arXiv:1812.11592.

Betancourt, M., Margossian, C. and Leos-Barajas, V. (2020). The Discrete Adjoint Method: Eﬃcient

Derivatives for Functions of Discrete Sequences. arXiv:2002.00326.

Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G.,
Paszke, A., VanderPlas, J., Wanderman-Milne, S. and Zhang, Q. (2018). JAX: composable trans-
formations of Python+NumPy programs.

EFFICIENT AUTOMATIC DIFFERENTIATION OF IMPLICIT FUNCTIONS

49

Cao, Y., Li, S., Petzold, L. and R, S. (2002). Adjoint Sensitivity Analysis for Diﬀerential-Algebraic
Equations: The Adjoint DAE System and Its Numerical Solution. SIAM Journal on Scientiﬁc Computing
24 1076 – 1089.

Carpenter, B., Hoffman, M. D., Brubaker, M. A., Lee, D., Li, P. and Betancourt, M. J. (2015).

The Stan Math Library: Reverse-Mode Automatic Diﬀerentiation in C++. arXiv 1509.07164.

Carpenter, B., Gelman, A., Hoffman, M., Lee, D., Goodrich, B., Betancourt, M.,
Brubaker, M. A., Guo, J., Li, P. and Riddel, A. (2017). Stan: A Probabilistic Programming Lan-
guage. Journal of Statistical Software 76 1 –32.

Dillon, J. V., Langmore, I., Tran, D., Brevdo, E., Vasudevan, S., Moore, D., Patton, B.,
Alemi, A., Hoffman, M. and Saurous, R. A. (2017). Tensorﬂow distributions. arXiv preprint
arXiv:1711.10604.

Errico, M. (1997). What is an adjoint model? Bulletin of the American Meteorological Society 78 2577 –

2591.

Gaebler, J. D. (2021). Autodiﬀ for Implicit Functions in Stan.
Griewank, A. and Walther, A. (2008). Evaluating derivatives, Second ed. Society for Industrial and

Applied Mathematics (SIAM), Philadelphia, PA.

Hindmarsh, A. and Serban, R. (2020). User Documentation for CVODES v5.1.0. Technical Report.
Karush, W. (1939). Minima of Functions of Several Variables with Inequalities as Side Constraints. (M.Sc.

thesis). Dept. of Mathematics, Univ. of Chicago, Chicago, Illinois.

Kesavan, S. (2020). Nonlinear functional analysis—a ﬁrst course, second ed. Texts and Readings in Math-

ematics 28. Hindustan Book Agency, New Delhi. MR4288179

Kuhn, H. W. and Tucker, A. W. (1951). Nonlinear programming. Proceedings of 2nd Berkeley Symposium

481–492.

Li, X., Wong, L. T.-K., Chen, R. T. Q. and Duvenaud, D. (2020). Scalable Gradients for Stochastic Dif-
ferential Equations. Proceedings of the Twenty Third International Conference on Artiﬁcial Intelligence
and Statistics, PMLR.

Lorraine, J., Vicol, P. and Duvenaud, D. (2019). Optimizing Millions of Hyperparameters by Implicit
Diﬀerentiation. Proceedings of the Twenty Third International Conference on Artiﬁcial Intelligence and
Statistics, PMLR 108 1540 – 1552.

Margossian, C. C. (2019). A Review of automatic diﬀerentiation and its eﬃcient implementation. Wiley

interdisciplinary reviews: data mining and knowledge discovery 9.

Pontryagin, L., Boltyanskii, V., Gamkrelidze, R. and Mishechenko, E. (1963). The Mathematical

Theory of Optimal Processes.

Rackauckas, C., Ma, Y., Dixit, V., Guo, X., Innes, M., Revels, J., Nyberg, J. and Ivaturi, V.
(2018). A Comparison of Automatic Diﬀerentiation and Continuous Sensitivity Analysis for Derivatives
of Diﬀerential Equation Solutions. arXiv:1812.01892.

