1

2
2
0
2

l
u
J

2
1

]

G
L
.
s
c
[

3
v
5
2
6
4
0
.
2
0
1
2
:
v
i
X
r
a

WheaCha: A Method for Explaining the Predictions of
Models of Code

YU WANG, Nanjing University, China
KE WANG, Visa Research, USA
LINZHANG WANG, Nanjing University, China

Attribution methods have emerged as a popular approach to interpreting model predictions based on the
relevance of input features. Although the feature importance ranking can provide insights of how models
arrive at a prediction from a raw input, they do not give a clear-cut definition of the key features models use
for the prediction. In this paper, we present a new method, called WheaCha, for explaining the predictions
of code models. Although WheaCha employs the same mechanism of tracing model predictions back to the
input features, it differs from all existing attribution methods in crucial ways. Specifically, WheaCha divides
an input program into “wheat” (i.e., the defining features that are the reason for which models predict the label
that they predict) and the rest “chaff ” for any prediction of a learned code model. We realize WheaCha in a
tool, HuoYan, and use it to explain four prominent code models: code2vec, seq-GNN, GGNN, and CodeBERT.
Results show (1) HuoYan is efficient — taking on average under twenty seconds to compute the wheat for an
input program in an end-to-end fashion (i.e., including model prediction time); (2) the wheat that all models
use to predict input programs is made of simple syntactic or even lexical properties (i.e., identifier names); (3)
Based on wheat, we present a novel approach to explaining the predictions of code models through the lens of
training data.

1 INTRODUCTION
Riding on the major breakthroughs in deep learning methods along with the ever-increasing public
datasets and computation power, modern machine learning (ML) models, such as neural networks,
have been increasingly applied to solve programming language tasks, and achieved remarkable
success in a variety of problem domains: method name prediction [Alon et al. 2019a,b; Fernandes
et al. 2019], program repair [Chen et al. 2019; Dinella et al. 2019], and program verification [Si
et al. 2018; Yao et al. 2020]. Despite those accomplishments, neural networks mostly operate in a
black-box manner, making it difficult to get insight into their internal mechanism of work. This
lack of transparency has become an impediment to the use of learning-based program analysis
tools, especially in security-critical settings (e.g., malware detection) as the degree to which their
predictions can be trusted is rather unclear. From a scientific standpoint, improving the transparency
of neural models is also essential to the soundness of science. Because their inability to provide
explanations for their decisions not only weakens the validity but also hinders the openness of
scientific discovery.
A Review of Attribution Methods. In the past few years, significant progress has been made in
explaining the predictions of ML models. A prominent class of explainability techniques, called
attribution methods, has sparked a lot of interest in the ML community. The idea is to assign an
attribution score to each input feature w.r.t. a particular output of a network. In general, attribution
methods can be classified into two categories: perturbation-based and backpropagation-based. The
former refers to those that make perturbations to input features and observe the impact on later

Authors’ addresses: Yu Wang, State Key Laboratory for Novel Software Technology Department of Computer Science and
Technology, Nanjing University, Nanjing, Jiangsu, 210023, China, yuwang_cs@smail.nju.edu.cn; Ke Wang, Visa Research,
Palo Alto, CA, USA, kewang@visa.com; Linzhang Wang, State Key Laboratory for Novel Software Technology Department
of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, 210023, China, lzwang@nju.edu.cn.

2018. 2475-1421/2018/1-ART1 $15.00
https://doi.org/

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

 
 
 
 
 
 
1:2

Yu Wang, Ke Wang, and Linzhang Wang

void f( int position ) {

List < Obj > mItems = retQueue ();
if ( position > mItems . size ())

return ;

mItems . add ( position , genItem ());
notifyItemInserted ( position );
log (" Add item ;" );

}

Fig. 1. An example method whose name is correctly predicted by seq-GNN [Fernandes et al. 2019]. The
top-five predictions made by seq-GNN are shown on the right. Throughout the paper, we will use this program
as our running example.

neurons in the network. Zeiler et al. [2011] is a typical example in the image classification domain.
In particular, it occludes different segments of an input image and visualizes the change in the
activation of later layers. The strength of perturbation-based methods lies in their visibility, that is,
one can directly visualize the marginal effect of any input feature via perturbation. Backpropagation-
based methods exceed the perturbation-based in terms of efficiency. In particular, they can compute
the attributions for all input features in a single forward and backward pass through the network.
As an early attempt, Simonyan et al. [2014] proposed using the gradient of the output w.r.t. pixels
of an input image to compute a saliency map of the image. A key drawback of this approach is its
inability to address the saturation problem, namely, gradients can be tiny at certain inputs that may
yield significant activations within the network [Glorot and Bengio 2010; Shrikumar et al. 2017a].
Integrated Gradients [Sundararajan et al. 2017], a well-known explainability technique, offers a
solution. Instead of computing the gradients at only the current value of the input, Sundararajan
et al. [2017] propose to integrate the gradients as the inputs are scaled up from a pre-set starting
value to their current value. We defer a detailed survey on the attribution methods to Section 5.
Insights of WheaCha. While the attribution methods can be readily applied to explain the
predictions of learning-based program analysis tools, such as producing a ranking on the importance
of different parts of the input program, they do not give a clear-cut definition of the critical features
that models use for that prediction. Considering the ideal case in which there might exist a “sweet-
spot” in the ranking that separates the critical features from the rest, then how to determine the
“sweet-spot” is rather unclear. To address this shortcoming, this paper presents a new explanation
method, WheaCha (Wheat and Chaff ), for interpreting the predictions of models of code, a broad
class of models that predict the properties of source code. Figure 1 shows one such task called
method name prediction. We will use the program as our running example throughout the paper.
Unlike the existing attribution methods that assign a relevance score to each input feature, WheaCha
classifies an entire input into two kinds of features: the defining features, wheat, that are the reason
models predict the label that they predict, and the remaining features, chaff . A natural question
arises: how do we define wheat for an input program given a particular model prediction? Our
insight is to observe how models react to a pair of complementary prediction samples derived
from the original input. Technically, we formulate the following two constraints to quantify the
influence of wheat. That is, the very same model must (1) preserve its prediction when the original
input becomes wheat; and (2) change its prediction when the original input becomes chaff .

Below we illustrate these two constraints with our running example. Figure 2a shows that
the expression, mItems.add();, alone preserves the prediction, addItem, seq-GNN makes for the
original input, thus the first constraint is satisfied; Figure 2b shows that removing mItems.add();
from the input program changes seq-GNN’s prediction to retQueue, therefore the second constraint
is also met. Compared to the existing attribution methods, WheaCha’s explanations allow end users
to know precisely and definitively the features that models use for a prediction they make, and

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

set9.7e-2addNewItem2.1e-2genItem2.9e-2retQueue0.101addItem0.743Short Title

1:3

void addItem ( int

position ) {

mItems . add ();

}

void retQueue ( int position ) {

List < Obj > mItems = retQueue ();
if ( position > mItems . size ())

return ;

mItems.add(position ; genItem ());
notifyItemInserted ( position );
log (" Add item ;" );}

(a)

(b)

1
Fig. 2. A statement in the example program that satisfies the two constraints.

in turn evaluate the trustworthiness of the prediction more accurately. Considering the running
example, WheaCha lets end users know exactly mItems.add(); are the features seq-GNN uses
to predict the label addItem, whereas, a ranking of the attribution score for each feature would
have been vague (i.e., unclear how small an attribution score indicates feature irrelevance) and
redundant (i.e., unnecessary to compare the attribution scores of features in the wheat, such as
mItems vs. add, or those in chaff such as retQueue vs. log).

Using WheaCha’s explanations as the ground-truth, we find that the ranking produced by some
of the most prominent attribution methods routinely underestimates the importance of wheat.
Take seq-GNN’s prediction for the example program as an instance, Figure 3 shows that none of
the top-five important tokens computed by Integrated Gradients [Sundararajan et al. 2017] is part
of the wheat, the underlined expression in the program. This finding rebuts the aforementioned
“sweet-spot” hypothesis, and confirms that existing attribution methods are unsuitable for wheat
detection.

void addItem ( int position ) {

List <Obj > mItems = retQueue ();
if ( position > mItems . size ())

return ;

mItems.add(position , genItem ());
notifyItemInserted ( position );
log (" Add item ;" );

}

Fig. 3. Regarding seq-GNN’s prediction for the running example, the left shows the five most important
features (highlighted within the shadow boxes) computed by Integrated Gradients and the wheat (underlined).
The ranking of the attribution scores of the five most important features is shown on the right.

Identifying the wheat from an input program is a challenging task. With a token-based represen-
tation, a program composed of 𝑛 tokens will yield a search space of 2𝑛 candidates (i.e., every token
may or may not be part of the wheat). Even after taking into account the syntactic and semantic
constraints, the search space will not shrink dramatically. As a result, a brute-force search would
be computationally intractable for any non-trivial program.

Our solution is based on the finding from Rabin et al. [2021a] — few simple code edits, albeit
preserving the property of interest for input programs, frequently cause models to alter their pre-
dictions — we hypothesize that models heavily utilize small, local program features for predictions.
To confirm our hypothesis, we conduct a preliminary study in which we test how models respond
to new programs obtained via systematic reduction of the input programs. Quite surprisingly, we
find that almost always an input program can be reduced to very few statements (i.e., ≤ 3) for
which models make the same prediction as they do for the original program. This observation

1The reason that wheat does not compile is that seq-GNN, like most code models, does not require input programs to
compile. In fact, WheaCha is general, capable of explaining models with or without compilation requirements.

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

item5.6e-4addPosition1.4e-3setItem7.1e-3add2.2e-2addItem0.953add8.3e-3addItem1.5e-2addEvent5.3e-2addQueue9.1e-2retQueue0.712size9.2e-3mItems4.2e-2log7.3e-2List0.13position8.1e-21:4

Yu Wang, Ke Wang, and Linzhang Wang

indicates that a small code fragment may already contain the wheat that models need for making
the correct prediction. Therefore, a more efficient approach is to first detect such code fragments,
and then locate the wheat within them. At the technical level, we present “Reduce and Mutate”,
a coarse-to-fine method for identifying the wheat that code models use for prediction. Given a
program 𝑃 for which a model 𝑀 makes a prediction. First, we find a global minimum program
¯𝑃 for 𝑃 that satisfies the first two above-mentioned constraints (coarse-grained search). Second,
we mutate the expressions in ¯𝑃’s statements to pinpoint the program properties ˜𝑃 that led to the
satisfaction of the two constraints (fine-grained search). Although it looks like, at this point, we
have found the wheat ˜𝑃 that 𝑀 uses to predict 𝑃, a core issue must be resolved: are 𝑀’s predictions
for all intermediate programs that WheaCha uses to query 𝑀 (e.g., ¯𝑃, ˜𝑃, etc.) still valid given
the seemingly distribution shift between intermediate programs and original programs on which
𝑀 is trained (e.g., 𝑃). Inspired by ROAR [Hooker et al. 2019], we present a similar approach to
tackle the potential out-of-distribution issue. The key idea is to retrain 𝑀 with additional data
that resembles the intermediate programs. If WheaCha finds the same wheat ˜𝑃 — which are now
clearly in distribution thanks to the retraining of 𝑀 — that retrained 𝑀 uses to predict 𝑃, we have
confirmed that ˜𝑃 is indeed the wheat of 𝑃 that 𝑀 uses for prediction.

We realize our approach in a tool, called HuoYan, and use it to evaluate four pioneering code
models: code2vec [Alon et al. 2019b], seq-GNN [Fernandes et al. 2019], GGNN [Allamanis et al. 2018],
CodeBERT [Feng et al. 2020]. We choose them not only because they are prominent representatives
of models of code, but more importantly, they reflect a wide variety of both model architectures and
downstream tasks so that the effectiveness and generality of HuoYan can be thoroughly tested. Our
evaluation results show (1) first, HuoYan is efficient — taking on average less than twenty seconds
to compute the wheat for each evaluated model; (2) the wheat that all evaluated models use for
prediction are simple as they never exceed fifteen tokens, many times down to the name of a single
variable; (3) After training models on programs coming from a similar distribution to those with
which WheaCha queried the models before, WheaCha almost always finds the same wheat that
original and retrained models use for prediction. This is strong evidence that WheaCha’s approach
is valid in light of the out-of-distribution issues; (4) Integrated Gradients and SHAP [Lundberg
and Lee 2017], two among the most well-known attribution methods, do not precisely identify the
wheat because those they assign higher scores frequently miss out on the wheat.

As an example use of wheat, we present a novel approach to explaining the predictions of code
models. At the high-level, our approach answers a more fundamental question: which programs in
the training set are most responsible for a model prediction? Ultimately, the weights of a model are
derived from the training data. Hence our approach identifies the root cause for a prediction rather
than interpreting the internal states of a model after its weights have been learned. At the technical
level, given an unseen program 𝑃 for which a model 𝑀 predicts a label 𝐿, we rank 𝑀’s training
programs with the label 𝐿 based on their distance to 𝑃, measured using their respective feature
properties, and then present the top-𝑘 closest programs as the explanations for the prediction 𝑀
made for 𝑃.

This paper makes the following contributions:

• A method for explaining predictions of models of code.
• A definition of the defining features (i.e., wheat), the reason for which code models predict

the label that they predict.

• A method for identifying the wheat that code models use for predictions.
• An implementation, HuoYan, which we use to evaluate code2vec, seq-GNN, GGNN, and
CodeBERT. Results show that (1) HuoYan is efficient: taking on average less than twenty
seconds to compute the wheat; (2) all models use simple syntactic or even lexical properties

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Short Title

1:5

for prediction; (3) Integrated Gradients and SHAP, two attribution methods do not precisely
identify the wheat for any evaluated model.

• An approach to explaining predictions of code models through the lens of training data.

The rest of the paper is structured as follows. Section 2 gives the formalization. Section 3 describes
in detail our method including the discussion on the out-of-distribution issues and an example
application of wheat. Next, we present our evaluation of HuoYan (Section 4). Finally, we survey
related work (Section 5) and conclude (Section 6).

2 FORMALIZATION
We consider a machine learning model 𝑀 as a program (with semantics ⟦𝑀⟧) which executes on
an input (e.g., an image, a document, a piece of code, etc.) and returns a prediction as output. Given
a prediction 𝐿 that 𝑀 makes for an input program 𝑃 (i.e., ⟦𝑀⟧(cid:0)𝑃 (cid:1) = 𝐿), the problem WheaCha
aims to solve is to identify the wheat 𝑀 uses for this prediction. Below, we give a formal definition
of wheat, which is at the core of WheaCha.

𝑚)𝑚 ∈N. Formally, (𝑡 ˜𝑃

Definition 2.1. (Wheat) The wheat that 𝑀 uses for predicting the label of 𝑃 is a set of statements
˜𝑃 such that (1) ˜𝑃 is a constituent of 𝑃: ˜𝑃’s token sequence, denoted by (𝑡 ˜𝑃
𝑛 )𝑛 ∈N, is a subsequence
of 𝑃’s denoted by (𝑡 𝑃
𝑚𝑘 )𝑘 ∈N where (𝑚𝑘 )𝑘 ∈N is a strictly increasing
sequence of positive integers. (2) ˜𝑃 is sufficient: 𝑀 makes the same prediction for ˜𝑃 as it does for 𝑃
(i.e., ⟦𝑀⟧(cid:0)𝑃 (cid:1) = ⟦𝑀⟧(cid:0) ˜𝑃 (cid:1) = 𝐿); (3) ˜𝑃 is necessary: 𝑀 makes a different prediction for 𝑃 \ ˜𝑃 than it
does for 𝑃 (i.e., ⟦𝑀⟧(cid:0)𝑃 \ ˜𝑃 (cid:1) ≠ ⟦𝑀⟧(cid:0)𝑃 (cid:1)) where 𝑃 \ ˜𝑃 denotes the operation that subtracts program
˜𝑃 from 𝑃 (Definition 2.2). Finally (4) ˜𝑃 is minimum: there does not exist ˜𝑃 ′ such that ˜𝑃 ′ satisfies the
above three requirements, and | ˜𝑃 ′| < | ˜𝑃 | where |·| denotes the length of a sequence.

𝑛 )𝑛 ∈N = (𝑡 𝑃

Here we discuss the intuition behind Definition 2.1. First, the constituent requirement captures
an obvious intuition, that is, the wheat must be part of an input program. Second, the sufficient
requirement is also quite intuitive — as the wheat, they must single-handedly lead models to
predict the same label as before without the rest of the input. However, the satisfaction of sufficient
requirement alone does not suffice features to be the wheat. Consider the following example in
Figure 4. Even though Figure 4a shows the statement, log("Add item;");, manages to preserve
the prediction, addItem, the model makes for the original input (i.e., the sufficient requirement
is satisfied), it is not the reason for which seq-GNN makes this prediction because removing it
from the input program barely influences seq-GNN. As displayed in Figure 4b, the probabilities
at which seq-GNN predicts the top-five labels remain almost unchanged. Clearly, this prediction
result suggests that seq-GNN does not even need log("Add item;"); let alone use it as the wheat
to predict the example program. This is precisely the reason that Rabin et al. [2021b], which seeks
exclusively the sufficient features as model interpretations, is flawed. In fact, our large-scale study
shows that removing the features discovered by Rabin et al. [2021b] from input programs always
never makes models alter their original predictions.

To address this issue, we design the necessary requirement: the removal of the wheat from the
input program, which we call the subtraction operation (Definition 2.2), must lead models to predict
different labels than they did for the original input.

Definition 2.2. (Subtraction) Given a program 𝑃 and a set of statements ˆ𝑃, 𝑃 \ ˆ𝑃 means for each
statement 𝑆 in ˆ𝑃, first locate the subtree, which is equivalent to the Abstract Syntax Tree (AST)
of 𝑆, from the AST of 𝑃; then remove the located subtree from the AST of 𝑃. Finally, serialize the
resultant AST back to source code.

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:6

Yu Wang, Ke Wang, and Linzhang Wang

void addItem ( int

void addItem ( int position ) {

position ) {

log (" Add item ;" );

List < Obj > mItems = retQueue ();
if ( position > mItems . size ())

return ;

mItems . add ( position , genItem ());
notifyItemInserted ( position );
log("Add item;");

}

}

(a) A statement that satis-
fies the sufficient require-
ment.

(b) The same statement does not satisfy the necessary requirement.
seq-GNN’s prediction probabilities of the top-five labels remain
almost unchanged.

Fig. 4. A statement that satisfies the sufficient but not necessary requirement.

Using only the constituent, sufficient, and necessary requirement, every input program is the
wheat for itself — an uninteresting explanation for any model prediction. To address this issue, we
impose the last constraint to ensure the precision of wheat. Specifically, we argue that ˜𝑃 should
be the globally minimum sequence of tokens since it’s the most precise — covering the pattern
that models have learned with the least amount of features (i.e., having the highest signal-to-noise
ratio).

Given Definition 2.1, it is obvious that the wheat always exists for any input program for which

models make a prediction. We left the proof to the supplemental material.

Theorem 2.1 (Existence of Wheat). Given a prediction 𝐿 that 𝑀 makes for an input program 𝑃,
the wheat ˜𝑃 that models use to predict the label of 𝑃 always exists.

3 METHODOLOGY
In this section, we first briefly describe a crucial weakness of models of code, which motivates a
key idea of finding the wheat. Then, we give a detailed presentation of Reduce and Mutate, a simple
yet efficient method for identifying the defining features. Next, we address the out-of-distribution
issues, a potential validity concern about WheaCha’s approach. Finally, we present an application
of the wheat.

3.1 Background
While deep neural networks have been gaining increasing levels of interest in programming
languages research, Rabin et al. [2021a] cautioned that some eminent code models are surprisingly
unstable with their predictions. Simple, natural, and semantically-preserving transformations
frequently cause models to alter their predictions. Here we use an example to demonstrate their
finding. Figure 5a depicts the original method which is correctly predicted by code2vec to be
factorial; Figure 5b depicts the transformed method, albeit semantically equivalent, is totally
mishandled. None of the top-five predictions even remotely resemble the ground truth considering
that all we changed is the order of the operands in a multiplication expression.
Applicability of Delta Debugging. Their finding suggests that models don’t evenly distribute
their attention across the entire structure of input programs; instead, they focus on a small fragment
of code. At first glance, Delta Debugging (DD) [Zeller and Hildebrandt 2002] seems to be a perfect
approach to finding such code fragment from input programs. In particular, we can apply DD to
remove the irrelevant code in reference to the sufficient and necessary requirement. It is worth
mentioning a property that has to be satisfied in order for DD to be applicable is Monotony
(Definition 6 in Zeller [1999]), meaning, in the software debugging setting, whenever an input
causes a program to fail a test, others that include this input will also cause the program to

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

set9.7e-3addNewItem2.1e-2genItem2.9e-2retQueue0.108addItem0.741Short Title

1:7

int f ( int n) {
if (n == 0)
{

return 1;

}
else
{

int f( int n) {
if (n == 0)
{

return 1;

}
else
{

return n * f (n -1) ;

return f(n -1) * n ;

}

}

}

}

(a) Prediction for the original method.

(b) Prediction for the transformed method.

Fig. 5. A simple, natural, semantically-preserving transformation causes code2vec to change its prediction.
Note that the probability of the top-one prediction is even higher on the transformed method.

fail. This property is crucial because it allows DD to remove partitions of the input that are
unrelated to the cause of the failure. As a result, the search space still shrinks even when the failure-
inducing faults can not be precisely located from the input. Conversely, without the monotony
property, an irrelevant partition may not be removed from the input because its complement,
albeit including the failure-inducing faults, can still have the program pass the test. We can easily
redefine monotony in our settings to be whenever 𝑝 satisfies both the sufficient and necessary
requirement, then every super-sequence of 𝑝 satisfies the two requirements as well, formally,
∀𝑝 ⪯ 𝑃 (⟦𝑀⟧(cid:0)𝑝(cid:1) = 𝐿 ∧ ⟦𝑀⟧(cid:0)𝑃 \ 𝑝(cid:1) ≠ 𝐿 ∧ 𝑝 ⪯ 𝑝 ′) ⇒ ⟦𝑀⟧(cid:0)𝑝 ′(cid:1) = 𝐿 ∧ ⟦𝑀⟧(cid:0)𝑃 \ 𝑝 ′(cid:1) ≠ 𝐿 where
𝑝 ⪯ 𝑃 denotes 𝑝 is a sub-sequence of 𝑃. It is rather clear that machine learning models do not
guarantee to satisfy this property. Below, we use the running example to demonstrate seq-GNN’s
non-satisfaction of the monotony property (Table 1), and in turn the ramifications of applying DD
for wheat detection.

We skip the steps in which DD successfully reduces the example into the code fragment consisting
of size();, return; and mItems.add(position);. Since then DD can not make further reductions
even though the fragment is clearly not the wheat. In fact, DD had an opportunity to remove either
the statement size(); (at Step 6), the statement return; (at Step 7), or the parameter position
(at Step 10) had the model satisfied the monotony property. That is, in any of the three steps,
the reduced program, which includes the wheat, would have satisfied the sufficient and necessary
requirement and allowed DD to proceed. However, since predictions of seq-GNN are not monotonic,
the resultant programs at all three steps turn out to violate the sufficient requirement. As a result,

Table 1. Reducing the running example using DD. Δ𝑖 denotes partitions and ∇𝑖 is the complement of Δ𝑖 .
For simplicity, we use tokens to represent programs that are tested against the sufficient and necessary
requirement at each step. The last column shows the requirement that partitions do not satisfy.

Step Partition

1
2
3
4
5
6
7
8
9
10

Δ1
Δ2
Δ3
Δ4
Δ5
∇1
∇2
∇3
∇4
∇5

size
✓

✓
✓
✓
✓

Tokens
return mItems

add position

Unsatisfied

✓

✓

✓
✓
✓

✓

✓
✓

✓
✓

✓

✓
✓
✓

✓

✓
✓
✓
✓
✓

Both
Both
Both
Both
Both
Sufficient
Sufficient
Sufficient
Sufficient
Sufficient

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

getN4.6e-2spaces7.1e-2fac9.1e-2fact0.229factorial0.447method1.4e-2fOo2.3e-2f7.4e-2n7.5e-2m$0.7831:8

Step 1

void addItem ( int position ) {

List < Obj > mItems = retQueue ();
if ( position > mItems . size ())

return ;

mItems . add ( position , genItem ());
notifyItemInserted ( position );
log ( " Add item ;" );

}

Step 5

Yu Wang, Ke Wang, and Linzhang Wang

List of statements

Stmt. 1
Stmt. 2
Stmt. 3

Stmt. 4

Stmt. 5

Stmt. 6

List<Obj> mItems = retQueue();
if(position > mItems.size());
return;
mItems.add(position,
genItem());
notifyItemInserted(
position);
log("Add item;");

Step 4

Step 2

void retQueue (

int position ) {
List < Obj > mItems =

retQueue ();

}

Step 3

void retQueue ( int position ) {

void addItem ( int position ) {

void onItemClick (

List < Obj > mItems = retQueue ();
if ( position > mItems . size ())

return ;

mItems.add(position, genItem());
notifyItemInserted ( position );
log ( " Add item ;" );}

mItems . add ( position , genItem ());

}

}

int position ) {

if ( position >

mItems . size ());

Fig. 6. High-level steps of Reduce. The label that seq-GNN predicts at each step is highlighted in shadowbox.

DD gets stalled and the final output is not minimal. For interested readers, we left DD’s complete
procedure in reducing the example program to Section C in the supplemental material.

Although refining DD to deal with non-monotony subjects is a pathway forward, it is out of the
scope of this work. Instead, we present a simple, efficient search strategy that first aggressively
prunes the search space of the wheat (i.e., the goal of Reduce), and then precisely locate its constituent
program properties from the remaining code fragment (i.e., the goal of Mutate).

3.2 Reduce
Because fragments that contain the wheat are usually small, the search can be made very efficient
by testing out smaller code fragments first. Figure 6 illustrates the high-level steps of Reduce on the
running example.

First, we flatten the input method into a list of statements (Step 1 ). We then traverse the
fragments (i.e., combination of statements) in the ascending order of their size, starting with
those that contain only one statement. We pick Stmt. 1: List<Obj> mItems=retQueue(); as the
first statement to check against the sufficient and necessary requirement. As depicted in Step 2 ,
seq-GNN makes a different prediction for this statement2, thus, the sufficient requirement is not
met. We move on to the other statements. As shown at Step 3 , Stmt. 2 also does not satisfy the
sufficient requirement. We omit Stmt. 3 for its non-satisfaction of the sufficient requirement either.
Next, we arrive at Stmt. 4: mItems.add(position, genItem());. In particular, we find that Stmt. 4
satisfies the sufficient requirement at Step 4 and necessary requirement at Step 5 , we declare Stmt.
4 to be the fragment that contains the wheat. To avoid missing other candidates, we continue the
traversal until reaching the last statement Stmt. 7 log("Add item;");, which violates the necessary
requirement. Because future fragments to be explored will be non-minimum compared to Stmt. 4,
the Reduce step completes with the lone output of Stmt. 4.

Algorithm 1 gives the details of the Reduce step. The goal of the Flatten function is to reduce
an input program into a list of statements (line 2). The for loop at line 3 gradually increases the
size of the subsets drawn from the Flatten function (line 2) while searching for the minimum
code fragments. The CombineKStmt function at line 4 creates subsets of statements with size k.
Then, for each subset, the algorithm invokes Reconstruct function (line 7) to return two new

2To facilitate readers’ understanding, we highlight the predicted label within the shadowbox at each step in Figure 6 and 7.

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Short Title

1:9

Algorithm 1 Find the minimum fragment of code.

1: procedure FindMinimumFragment(𝑝𝑟𝑜𝑔𝑟𝑎𝑚, 𝑚𝑜𝑑𝑒𝑙)
2:

statements ← Flatten(program)
⊲ traversing all subsets in the ascending of the
cardinality; ‘-1’ excludes the entire set.

3:

4:
5:
6:
7:

8:
9:
10:
11:
12:
13:

for k ← 1 to statements.size - 1 do
⊲ getting combinations of size k
sets ← CombineKStmt (statements, k)
fragments ← ∅
foreach set ∈ sets do

suff_mth, nec_mth ← Reconstruct (set, program)
if VerifyCodeFragment (suff_mth, nec_mth, model) then

fragments ← fragments ∪ set

end if
end foreach
if fragments ≠ ∅ then
return fragments

14:
end if
15:
end for
16: end procedure

programs: one for verifying against the sufficient requirement (i.e., suff_mth) and the other for
verifying against the necessary requirement (i.e., nec_mth). In particular, suff_mth is simply the
current subset being explored, whereas nec_mth is resulted from the subtraction of suff_mth from
program (Definition 2.2). Once the created subset is found out to be valid by VerifyCodeFragment
function (line 8), we add it to the collection of all minimum subsets fragments (line 9). After we have
traversed all combinations of k statements, we return fragments if it’s not an empty set (line 13),
otherwise, we will continue to explore the code fragments with the size of k+1.
Reduce vs. DD Regarding Non-monotony. Since the code fragments that Reduce identifies also
include extra code that is irrelevant to the wheat, Reduce is affected by models’ non-monotony, the
same way DD is affected. However, the degree to which non-monotony impacts Reduce is in general
less than that impacts DD. This is because the extra features included always stem from the same
statement where the wheat is, as a result, models, by and large, still behave monotonically (e.g.,
seq-GNN makes the same prediction for mItems.add(position,genItem()); and mItems.add()).
On the contrary, what DD encounters is that the extra code included along with the wheat is
significantly less constrained (i.e., it can be any statement or expression in the program) and come
in larger quantities, making models far more likely to be non-monotonic.

3.3 Mutate
The features discovered in the Reduce step lie at the level of statements (i.e., coarse-grained), thus
they are likely to contain redundant elements. To pinpoint the fine-grained wheat, we mutate the
program discovered in the Reduce step in an attempt to identify the minimal features that keep the
sufficient and necessary requirement satisfied.

Since the number of tokens in the wheat is not guaranteed to be so small as that of the statements
in the minimum code fragment, the style of search adopted by the Reduce step is likely to be

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:10

Step 1

Yu Wang, Ke Wang, and Linzhang Wang

Step 2

Step 3

void genItem ( int position ) {

void add ( int position ) {

void addItem ( int position ) {

mItems. add ( position , genItem ());

}

Step 6

- mItems . add ( position , genItem ());
+ oov . add ( position , genItem ());
}

}

mItems . add (position,genItem ());

Step 5

Step 4

void retQueue ( int position ) {
List < Obj > mItems = retQueue ();
if ( position > mItems . size ())

return ;

void addItem ( int position ) {

mItems.add(position ; genItem ());
notifyItemInserted ( position );
log ( " Add item ; " );}

}

mItems . add (genItem());

void retQueue ( int position ) {
List < Obj > mItems = retQueue ();
if ( position > mItems . size ())

return ;

mItems.add(position,genItem());
notifyItemInserted ( position );
log (" Add item ;" );}

Fig. 7. High-level steps of Mutate. - expression (resp., + expression ) signals removed (resp., added) lines.

inefficient for the Mutate step. For this reason, we only remove the irrelevant part of the code
fragment that does not cause the violation of either the sufficient or necessary requirement.
Mutation as well as Reduction. It is worth mentioning that a significant difference between
the two steps is Mutate does not adopt solely a program reduction approach to identifying the
fine-grained wheat. As an example, suppose a code fragment is already successfully reduced to a
field access expression, foo.bar. We then attempt to further reduce the expression into a single
identifier foo. If foo turns out to be invalid, we will mutate foo back into a field access expression,
this time with an out-of-vocabulary field name instead of bar — foo.oov3. Our rational is, even if
foo.bar satisfies both the sufficient and necessary requirement, it is still possible that models use
the type of the syntactic structure — field access expression — associated with the object foo rather
than the specific name of its field — bar — as the wheat. To deal with features of this kind that are
not explicitly presented, we mix the deletion and modification operations in the Mutate step to
pinpoint both the explicit and implicit fine-grained features.

Figure 7 illustrates the procedure of the Mutate step. As depicted in Step 1 and 2 , it turns out that
neither removing mItems nor mutating mItems into oov satisfies the sufficient requirement. Similarly,
we find that the identifier add is also integral to seq-GNN’s prediction that can not be changed. Next,
we move our attention to the first parameter position. It turns out that mItems.add(genItem());
satisfies both the sufficient (Step 3 ) and necessary (Step 4 ) requirement, thus, we remove position
from the code fragment. Regarding the subtraction operation (Definition 2.2) invoked for checking
code against the necessary requirement, directly removing a subtree from an entire AST may result
in dangling nodes that do not connect to any other node in the AST (e.g., the identifier position
marked in dashed box in Figure 8c after we remove the AST of mItems.add(genItem()); from that
of the original program). In such cases, we simply connect the dangling nodes to the parent node
of the root node for the deleted AST (e.g., connecting position to the method body as depicted
in Figure 8d). Figure 8 gives a detailed illustration of this procedure. Subsequently, we succeed in
removing the second parameter genItem() because mItems.add(); also satisfies the sufficient (Step
5 ) as well as the necessary requirement (Step 6 ). Note that the comma that separates the two then
parameters position and getItem() becomes a semicolon at Step 6 because both position and
getItem() will be considered as standalone statements in the resultant program of the subtraction

3out-of-vocabulary words are those models have never encountered during training. Thus, replacing a token with an out-of-
vocabulary word erases the influence of the replaced name on models. Throughout the paper, we use oov to represent
out-of-vocabulary words for simplicity and clarity.

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Short Title

1:11

(a)

(c)

(b)

(d)

Fig. 8. A illustration of the four-step process of subtracting mItems.add(genItem); from the original
program. (1) Figure (a) presents the AST of the running example, and the AST of mItems.add(genItem); in
the bottom left corner. For brevity, the ASTs are simplified. (2) Figure (b) highlights the overlapping nodes
between the two ASTs. (3) Figure (c) emphasizes the resultant AST after the overlapping nodes are removed,
as a result, position becomes a dangling node. (4) Figure (d) connects position to the body of the method
as the completion of the subtraction operation.

of mItems.add() (Figure 14 in the supplemental material gives a detailed illustration). Finally, we
conclude that mItems.add() is the wheat that seq-GNN uses for predicting the running example.
Algorithm 2 gives the details about how to pinpoint the wheat within the minimum code fragment
discovered by the Reduce step. Technically, we perform a postorder traversal on the AST (line 4)
of the minimum fragments. For each node operation in the AST, we consider the following two
cases. If a node does not have any child node, then we first try deleting the node (line 22). If the
resultant program no longer satisfies both the sufficient and necessary requirement, we will mutate
the node into one with an out-of-vocabulary value (line 23). If a node has children nodes, it will
be kept intact to preserve the status of its children (the if condition at line 21 will be evaluated
to false). If by any chance a node happens to be irremovable (resp., immutable), we simply skip
the deletion (resp., mutation) operation. This entire process is repeated until the AST of the code
fragment can not be reduced any further (line 18 to 20). Considering some expressions, which can
not be removed or mutated before, may become removable or mutable after others are dealt with
first, thus, we compute a fixed point on the result of the Mutate function to make certain the final
output will be minimal. (lines 12 to 15).4
Minimality Although the result of Algorithm 2 may not satisfy the minimum requirement in
Definition 2.1, it satisfies 1-minimality, or more precisely 1-tree-minimality [Misherghi and Su
4Interested readers may refer to Section E in the supplemental material for the details of DeleteNode and MutateNode
in Algorithm 2.

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Member CallMember ExprmItemsaddParametersgenItemMethod CallMethod BodyMember CallMember ExprmItemsaddnotifyItem Inserted (position)ParametersgenItempositionMethod CallList<Obj>mItems =retQueue()if(position >mItems.size())returnlog("Additem;")Method BodyMember CallMember ExprmItemsaddnotifyItem Inserted (position)ParametersgenItempositionMethod CallList<Obj>mItems =retQueue()if(position >mItems.size())returnlog("Additem;")Method BodyMember CallMember ExprmItemsaddnotifyItem Inserted (position)ParametersgenItempositionMethod CallList<Obj>mItems =retQueue()if(position >mItems.size())returnlog("Additem;")Method BodynotifyItem Inserted (position)positionList<Obj>mItems =retQueue()if(position >mItems.size())returnlog("Additem;")1:12

Yu Wang, Ke Wang, and Linzhang Wang

Algorithm 2 Find the wheat.

1: procedure FindFeatures(𝑓 𝑟𝑎𝑔𝑚𝑒𝑛𝑡𝑠, 𝑝𝑟𝑜𝑔𝑟𝑎𝑚, 𝑚𝑜𝑑𝑒𝑙)
2: min_root ← Parse(program).root

⊲ min_root is defined to track the minimal features

⊲ fragments is a list of the minimum fragments returned from the Reduce step
foreach min_mth ∈ fragments do
root ← Parse(min_mth).root
MutateToFixpoint(root, program, model)
min_root ← Min(root, min_root)

⊲ return the minimum of two code features

3:
4:
5:
6:
7:

end foreach
return min_root

8:
9: end procedure

last_root ← ∅

10: function MutateToFixpoint(𝑟𝑜𝑜𝑡, 𝑝𝑟𝑜𝑔𝑟𝑎𝑚, 𝑚𝑜𝑑𝑒𝑙)
11:
12: while last_root ≠ root do
last_root ← root
13:
Mutate(root, root, program, model)
14:
15:
end while
16: end function

Mutate(child, root, program, model)

foreach child ∈ node.children do

17: function Mutate(𝑛𝑜𝑑𝑒, 𝑟𝑜𝑜𝑡, 𝑝𝑟𝑜𝑔𝑟𝑎𝑚, 𝑚𝑜𝑑𝑒𝑙)
18:
19:
20:
21:
22:

end foreach
if node.children.size == 0 then

⊲ if fails, mutating node
MutateNode(node, root, program, model)

end if

23:
24:
25:
end if
26: end function

if not DeleteNode(node, root, program, model) then

⊲ removing node

2006] that DD no longer satisfies due to the non-monotony of machine learning models. In addition,
we show in Section 4.5 that even without the guarantees in theory, Reduce and Mutate is in fact
remarkably effective in finding the global minimum features in practice.
Compilability We can easily adjust Reduce and Mutate to accommodate code models that do
require input programs to compile. For example, we can filter out uncompilable code with the help
of existing compilers in both steps and only use those that compile to query the model.

3.4 The Validity of WheaCha’s Approach in Light of the Out-of-distribution Issues
Since the programs that WheaCha queries a model with at both Reduce and Mutate step differ
from those that the model has seen in training, it seems that WheaCha violates one of the key
assumptions in machine learning: the training and evaluation data come from the same distribution,
in which case the predictions that the model makes for the query programs will be unsound, and
the validity of WheaCha’s approach will be called into serious question. To address this issue,
we draw from a seminal work in explainable ML, ROAR [Hooker et al. 2019], which proposes a

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Short Title

1:13

retraining approach for handling the distribution shift when evaluating the explainability methods.
Their insight is that a commonly used strategy in estimating the feature importance — removing
the supposedly informative features from the input and observing how the classifier degrades —
comes at a significant drawback because a performance degradation might be caused by a shift in
distribution instead of removal of information. In contrast, Hooker et al. [2019] first retrains the
model on the modified data (e.g., in image classification domain, they replace the fraction of the
pixels estimated to be most important by an attribution method with a fixed uninformative value
for each training image) and then evaluates on test images, which are modified in the same manner,
if a feature estimator can identify the important input pixels whose subsequent removal causes
the sharpest degradation in accuracy. Since retraining makes certain that training and test data
comes from a similar distribution, the removal of important features becomes the only plausible
explanation for accuracy degradation. On the other hand, ROAR is not without limitations. For
one, while the architecture is the same, the model used during evaluation is not the same as the
model on which the feature importance estimates were originally obtained.

Inspired by Hooker et al. [2019]’s approach, we design a similar retraining procedure that not only
tackles the out-of-distribution issue but also addresses the inherent limitation of ROAR. Denoting
the model by 𝑀, its training set by P and test set by Q, we describe the steps of our approach:

(1) for each program 𝑃 ∈ P, we perform Reduce and Mutate to generate new programs P — what

WheaCha would have queried the model with for finding the wheat of 𝑃.

(2) we manually label each program 𝑃 ∈ P before training 𝑀 on P ∪ P.
(3) we deploy WheaCha to find the wheat that retrained 𝑀 uses to predict each program 𝑄 ∈ Q.
(4) we compare the wheat that WheaCha finds for each program 𝑄 ∈ Q before and after the

retraining of 𝑀.

Using the wheat produced by step (3), albeit no longer out-of-distribution, creates the same
problem: WheaCha is essentially explaining the predictions of the retrained model rather than
the original model. Instead, we check the equivalence of the two sets of wheat at step (4), if they
turn out to be identical, we declare that the wheat WheaCha finds for each test program 𝑄 are
indeed the wheat that 𝑀 uses for prediction; furthermore, the consistency between the two sets of
wheat also indicates the retraining procedure does not fundamentally change the model behavior,
therefore, WheaCha can explain predictions of a model in its original form (without retraining).

3.5 An Application of The Wheat
We utilize the revealed features code models use to explain their predictions. The idea is as follows.
Given a Model 𝑀, and a program 𝑃 from 𝑀’s test set for which 𝑀 predicts 𝐿. First, we discover the
wheat 𝑀 uses to predict 𝐿; Second, we do the same thing for all programs in 𝑀’s training set with
the label 𝐿. Note that in a generative setting (e.g., code2vec, seq-GNN) where training set might
not contain a method of label 𝐿, we consider all methods whose name is a subset of the words in
𝐿. Finally, we rank these training programs based on the distance between their and 𝑃’s revealed
features. Our rationale is to find the training programs from which models extracted the wheat at
the first place. Thus, our method explains the root cause of a prediction rather than interprets the
internal state of models.

4 EVALUATION
We realize our method Reduce and Mutate in a tool, called HuoYan, which extracts the wheat
that code models use for prediction. In this evaluation, we first deploy HuoYan to explain several
prominent code models. Then we examine the validity of WheaCha’s approach. Next, we evaluate

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:14

Yu Wang, Ke Wang, and Linzhang Wang

Table 2. Avg. end-to-end time for HuoYan and the baseline. Hereinafter, stronger results are marked in bold.

Methods

Baseline
HuoYan

Java-small
26.45
19.76

code2vec
Java-med
18.24
14.37

Java-large
23.28
17.71

Java-small
7.28
10.57

seq-GNN
Java-med
12.78
12.07

GGNN

CodeBERT

Java-large C# Datasets CodeSearchNet

5.34
9.79

5.21
7.54

16.37
11.23

if attribution methods can find the wheat that HuoYan finds. Finally, we evaluate the effectiveness
of our wheat-based explanation to code models.

4.1 Evaluation Subjects
Models. We choose code2vec [Alon et al. 2019b] because it is among the first to predict method
names in large-scale, cross-project settings. We choose seq-GNN since it’s the state-of-the-art
model in method name prediction. GGNN [Allamanis et al. 2018] is selected because it is the first
graph model applied in the programming domain for variable misuses detection. We also include
CodeBERT [Feng et al. 2020], a bimodal pre-trained model for programming language and natural
language. It achieves state-of-the-art performance on both natural language code search and code
documentation generation, the latter of which is used in this evaluation.
Datasets. For all models except code2vec, we use the datasets on which they are evaluated when
they are first proposed. That is Java-small for seq-GNN; a dataset of 2.9 million lines of real-
world C# code for GGNN; and CodeSearchNet [Husain et al. 2019] for CodeBERT. To keep our
engineering effort manageable, we only use Java programs in CodeSearchNet (496,688 in total),
which should be sufficient for validating HuoYan’s effectiveness. Since Java-med, and Java-large
share similar characteristics with Java-small except for their bigger size, and code2vec and seq-GNN
target the same downstream tasks, we use Java-small, Java-med, and Java-large to evaluate both
models. We have re-trained all models using their implementations open-sourced on GitHub. The
performance of all re-trained models is either comparable or superior to the original (Table 11-13 in
the supplemental material). Note that we only use programs in test sets as the evaluation subjects
in case models over-fit to training sets.
Baseline. We use DD as a baseline method; in particular, we apply DD to remove code that is
irrelevant to a prediction (determined by the sufficient or necessary requirement). In the end, we
declare the program that can not be further removed as the wheat.
Hardware. All experiments are conducted on 5 Red Hat Linux servers each having 64 Intel(R)
Xeon(R) 2.10GHz CPU, 755GB RAM and 4 Tesla V100 GPU (32GB GPU memory).

4.2 Performance of HuoYan
We evaluate the performance of WheaCha by the time HuoYan spends generating explanations
end-to-end (i.e., including the time models spend on prediction). The results show that the two
methods are neck and neck in practice even though DD is more efficient than HuoYan in terms of
the worst-case complexity (i.e., quadratic time vs. exponential time). This is due to the size of the
minimum code fragment HuoYan finds, which never exceeds three statements for over a million
programs used in the evaluation. Furthermore, since the minimal fragment of three statements
rarely occurs, DD would not display a significant upgrade. To conclude, HuoYan is efficient: taking
on average less than twenty seconds to compute the wheat.

4.3 Makeup of the Wheat
Finding the Wheat We follow Algorithm 1 and 2 to identify the minimum code fragments and
pinpoint the fine-grained features within these fragments. Table 3 gives the average number of

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Short Title

1:15

void init (

void sort ( int [] array ) {

CipherParameters
params ) {

boolean swapped = true ;
for ( int i = 0; i < array . length

reset ();

&& swapped ; i ++) {

swapped = false ;
for ( int j = 0; j < array . length

- 1 - i; j ++) {

cipher . init ( true ,

if ( array [j] > array [j +1]) {

params );

int temp = array [j ];
array [j] = array [j +1];
array [j +1]= temp ;
swapped = true ;

}

}}}}

void init () { init ; }

void sort ( int [] array ) { swapped ; }

Fig. 9. Lexical wheat. Top (resp., bottom) figures are the original methods (resp., wheat).

void checkDone () {

boolean done = false ;

while (! done ) {

if ( remaining () <= 0) {

done = true ;

}

}

}

int nextInt () {

while ( st == null ||

! st . hasMoreElements ()) {
try {

st = new StringTokenizer (

br . readLine ());

} catch ( Exception e) {
e. printStackTrace ();

}}

return Integer . parseInt (

st . nextToken ());}

void checkDone () {
while (! done ) {
}}

int nextInt () {

return parseInt (
nextToken ());}

Fig. 10. Syntactic wheat.

tokens the wheat is composed of. Overall, we find that none of the wheat that any evaluated model
uses for prediction exceed fifteen tokens. This result indicates that existing code models use simple
program properties for prediction. Additionally, the wheat generated with those properties, albeit
not guaranteed to be the global minimum, still help end users to know the wheat that models use
for predictions. Table 3 also gives a comprehensive, head-to-head comparison between HuoYan
and DD. Clearly, HuoYan is better across the board. A deeper analysis reveals that for less than 9%
of all test programs, DD manages to find the same wheat as HuoYan, while HuoYan finds smaller
wheat for the remaining over 91%. In other words, DD never finds smaller wheat than HuoYan.
This is a concrete piece of evidence that the degree to which non-monotony impacts HuoYan is
significantly less than that impacts DD.

Table 3. The average number of tokens in the wheat.

Methods

Baseline
HuoYan

Java-small
14.21
8.53

code2vec
Java-med
17.13
9.91

Java-large
16.24
9.58

Java-small
12.30
6.74

seq-GNN
Java-med
14.84
6.79

GGNN

CodeBERT

Java-large C# Datasets CodeSearchNet

10.04
5.66

14.22
6.81

13.63
6.78

Investigating the Substance of the Wheat. We classify the wheat into three categories according
to their constituent program properties: lexical, syntactic, and semantic.
• Lexical: If each statement in the wheat consists of a single identifier. Figure 9 depicts two

examples, in both cases, the wheat consist of only one identifier.

• Syntactic: If there is at least one statement in the wheat composed of a syntactic expression

(Figure 10).

• Semantic: seq-GNN and GGNN are the two models which take in semantic properties as model
inputs. In particular, they use nine kinds of manually designed edges (described in Section G of

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:16

Yu Wang, Ke Wang, and Linzhang Wang

void writeSumToFile ( String filename ,

int compare ( Object obj1 , Object obj2 ) {

int [] x) {

BufferedWriter outputWriter = null ;
int sum = 0;
outputWriter = new BufferedWriter (

int intObj1 = ( int ) obj1 ;
int intObj2 = ( int ) obj2 ;
int difference = intObj1 - intObj2 ;

new FileWriter ( filename ));
for ( int i =0; i <x . length ; i ++) sum += i;
outputWriter . write ( String . valueOf ( sum ));
outputWriter . close ();}

}

if ( difference < 0) return -1;
else if ( difference > 0) return 1;
else return 0;

Fig. 11. Semantic wheat. The bottom figure shows the AST of the wheat, in which dash arrows denote the
semantic edges. For clarity, the AST is simplified.

the supplemental material), out of which seven can be deemed as semantic in nature, to enrich
the original AST of input programs. Therefore, we define the wheat to be semantic if its AST is
augmented with at least one semantic edge. Take the method compare in Figure 11 as an example,
the three semantic edges — one LastWrite edge and two ComputeFrom edges — must be present in
the AST of the wheat, otherwise the sufficient and necessary requirement are no longer satisfied
at the same time. Thus, we classify this example as semantic. To determine whether seq-GNN
and GGNN used semantic properties in their wheat, we first run Reduce and Mutate as usual, then
we remove all semantic edges from the augmented AST of the obtained wheat. If the resulted
tree no longer satisfies both the sufficient and necessary requirement, we categorize the wheat to
be semantic.

Table 4 gives detailed statistics on the classification for each model. Clearly, for all evaluated

models, the wheat primarily consists of lexical properties.

Table 4. Types of the wheat that each model uses for prediction.
GGNN

CodeBERT

Types

Lexical
Syntactic
Semantic

Java-small
59.27%
40.73%
–

code2vec
Java-med
51.34%
48.66%
–

Java-large
50.22%
49.78%
–

Java-small
77.60%
21.73%
0.67%

seq-GNN
Java-med
64.39%
32.96%
2.65%

Java-large C# Datasets CodeSearchNet

71.25%
26.24%
2.51%

53.39%
46.12%
0.49%

49.17%
50.83%
–

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

RootAddition AssignStmtsumMethod Invocation StmtwriteLastWriteoutputWritersumisum += i; outputWriter.write(sum);ComputeFromComputeFromRootIfStmtAssignStmtdifferenceBinaryExprdifference0oov_2difference = oov_1 - oov_2; if (difference < 0);oov_1BinaryExprLastWriteComputeFromShort Title

1:17

Table 5. The number of programs that we add into each dataset for retraining.

#original programs – training
#added programs – training
#original programs – validation
#added programs – validation

Java-small
665,115
336,938
23,505
11,908

Java-med C# Datasets CodeSearchNet
3,004,536
1,106,288
410,699
151,222

130,101
46,862
21,594
7,778

454,451
267,922
15,328
9,036

4.4 Confirm the Validity of WheaCha in Light of the Out-of-distribution Issues
Based on the high-level steps introduced in Section 3.4, we present the technical details from two
key aspects: data generation and labeling.
Data Generation. First, we perform the Reduce operation to generate code fragments for each
program in the training set. Then, for each code fragment, we generate additional programs by
randomly removing or mutating its constituent expressions. Such programs resemble the code
produced out of the Mutate step.
Data Labeling. Given the amount of work, we attempt to recruit all undergraduate, master and
PhD students from Math, Physics, Chemistry, Electronic Engineering, and Computer Science
department at our university for the labeling task. In the end, we find 458 students in total and each
student has at least one year of programming experience. We also hired 84 professional developers
via Amazon Mechanical Turk. Here, we describe the set-up of this experiment in detail. First, for
each generated program, we assign it the label of the program from which the wheat is derived.
Second, we ask participants to confirm the assigned label for each generated program. Specifically,
participants are required to assess based on their intuition if the assigned label correlates to the
generated program. An example we gave to all participants is that the wheat of the running example
correlates well to the label addItem, therefore, the label should be accepted for the wheat as a stand-
alone program. To ensure the quality of labeling, we present every generated program with the
assigned label to two participants, and we will only approve the program if both agree to accept
the label, otherwise, the program is rejected. We also assign all rejected programs a new label that
has not appeared in the original training set.

We randomly select either training or validation set to add each approved program. We keep the
same ratio between the added training programs and validation programs as the original dataset.
To keep the added data balanced, we also randomly sample a rejected program to accompany each
approved program. Table 5 shows the number of programs we add including both the confirmed
and rejected into Java-small, Java-med, C# Datasets, and CodeSearchNet. We disregard Java-large
because it demands excessive human effort to label enough programs given the size of its training
set (more than 15 million programs). In addition, Java-small and Java-med should already contain
enough data points to validate model behavior, in turn, WheaCha’s approach.
Results. First, we validate the accuracy of all evaluated models on the augmented datasets. We find
that all retrained models become more accurate even though the increase of accuracy is negligible
(Table 14-16 in the supplemental material). We deploy WheaCha to explain the retrained models,
and then compare the two sets of wheat before and after the retraining. It is worth mentioning
we also set out to validate Sivand’s approach [Rabin et al. 2021b] using the same original and
retrained models. Considering the manually labeled programs also resemble those with which
Sivand would have queried the model, thus, in the same way we can evaluate if Sivand’s approach
is still valid after the distribution shift between the training and test data is erased. Table 6 presents
the results. Regarding WheaCha, we find that an overwhelming majority of the programs in every
dataset has the same wheat before and after the retraining (the row designated by “Matched”),
confirming that those are indeed the wheat models use for prediction with or without the seemingly

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:18

Yu Wang, Ke Wang, and Linzhang Wang

Table 6. The percentage of programs for which wheat are identical (designated by “Matched”), come from
the same code fragment (designated by “Related”), or entirely different (designated by “Distinct”) before and
after retraining for Sivand and WheaCha.

Approach Comparison

Sivand

WheaCha

Matched
Related
Distinct
Matched
Related
Distinct

code2vec

seq-GNN

GGNN

CodeBERT

Java-small
46.2%
23.9%
29.9%
92.5%
3.6%
3.9%

Java-med
37.8%
30.8%
31.4%
96.8%
2.4%
0.8%

Java-small
44.9%
32.5%
22.6%
94.8%
2.1%
3.1%

Java-med C# Datasets CodeSearchNet

41.2%
29.5%
29.3%
97.3%
1.7%
1.0%

38.4%
26.4%
35.2%
93.9%
4.3%
1.8%

41.9%
36.7%
21.4%
96.5%
2.2%
1.3%

distribution shift. “Related” represents the wheat models use before and after the retraining come
from the same code fragment. They differ at the level of expressions, for example, using retrained
models, WheaCha manages to reduce more features from the previous wheat at the Mutate step or
vice versa. “Distinct” means retrained models use entirely different wheat than the original model.
Apparently, this raises issues despite the tiny number of programs that falls into this category. To
dig deeper, we discover that all such wheat met the sufficient and necessary requirement before,
only to get discarded due to the violation of the minimum requirement. The reason that they
become the wheat for retrained models is simple: the then wheat that original models use gets
bloated after the retraining (i.e., Mutate operation could not reduce so many features as before),
making wheat the minimum. Nevertheless, the “Distinct” (as well as the “Related”) scenario does
not invalidate WheaCha’s approach considering the change of model behavior, to a minor degree,
is to be expected after retraining. Overall, we conclude WheaCha’s approach is valid in light of the
out-of-distribution issues.

On the contrary, Sivand’s explanations change significantly after the retraining. In particular,
only around 40% of the wheat that retrained models use are the same as those that original models
use. There is also a large number of programs on each dataset that falls into the “Distinct” category.
Given that Sivand’s approach does not have a similar notion of minimality, “Distinct” wheat suggests
a drastic change in Sivand’s explanations after retraining, Overall, the results strongly indicate the
distribution shift heavily influences Sivand. This is yet another fundamental flaw of Rabin et al.
[2021b] in addition to ignoring the necessary requirement when producing model explanations.
Discussion. A natural question arises: for the original models (without retraining) how is it possible
for the wheat, which is far from the data that models have seen during training, still within the
learned distribution? We believe it is because of the very intrinsic limitation of models’ that they
only learn local, lexical/syntactic features from training programs, In other words, even with a
whole program to learn, models still extract patterns from simple parts of the program which the
wheat is likely to resemble, as a result, they lie in the distribution that models learned.

4.5 Can WheaCha Find the Global Minimum Features
In this experiment, we set out to confirm the minimality of the wheat found by HuoYan. As
explained earlier, finding the global minimum wheat requires exhausting all subsequences of the
token sequence of the input method. Therefore, to lessen the computational burden, we limit the
evaluation data for this experiment to those methods that have either a small set of tokens or small
wheat found by HuoYan (Table 7). We also diversify the data w.r.t. the size of programs and wheat to
make our results unbiased. To confirm the global minimality of the wheat for the selected methods,
we exhaust all subsequences of the token sequence of the input method up to the size of the wheat,
specifically, we start from the subsequences of size one, and gradually increase to the size of the

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Short Title

1:19

Table 7. Statistics on the selected methods for the minimality experiment.

Models

#statements in methods

#tokens in wheat

Min Max Mean Median Min Max Mean Median
1.0
code2vec
code2seq
1.0
seq-GNN 1.0
1.0
extreme

17.0
16.0
16.0
15.0

9.4
10.1
8.2
7.5

12.0
15.0
12.0
14.0

5.0
6.0
5.0
6.0

8.0
9.0
7.0
7.0

7.4
7.2
5.8
8.1

2.0
2.0
2.0
2.0

#methods in total

21,324
18,105
19,751
20,028

Models

Table 8. Percentage of programs whose wheat are fully covered by explainability methods.
SHAP (Top-N%)
70%
50%
15.24
6.37
38.24
12.02
21.45
11.72
27.30
15.67

Integrated Gradients (Top-N%)
90%
50%
27.93
13.85
25.54
10.17
23.18
9.23
29.50
13.12

Attention-based (Top-N%)
50%
62.50
51.27
52.53
52.02

10%
8.23
code2vec
seq-GNN 1.74
1.95
GGNN
CodeBERT 6.23

70%
64.91
56.92
57.21
58.19

10%
52.50
41.36
38.23
41.97

30%
11.42
6.32
4.56
10.33

70%
21.69
17.90
15.68
20.90

30%
5.02
9.33
7.64
11.54

30%
60.00
45.81
42.32
48.44

90%
69.22
60.14
61.21
63.12

10%
4.38
6.12
4.72
6.20

90%
55.19
60.58
65.23
73.56

wheat. Note that this is also the way in which we verify the wheat for our running example. To cope
with such a heavy computational burden, we exhaustively generate the test candidates for each
program beforehand in a parallel fashion; then we place them into separate batches to fully exploit
the potential of our GPUs (i.e., ≈10K test candidates per second per GPU). The whole experiment
takes almost three months to complete.

We find that for 79,208 wheat that HuoYan identified, the brute-force search never finds a single
instance where the wheat is smaller. Our results speak volume to the precision of our Reduce and
Mutate method in finding the critical features that models use for prediction even if they are not
guaranteed to be the global minimum.

4.6 Can Other Explainability Methods Find Wheat
In this experiment, we evaluate whether or not some of the most prominent attribution methods
can also find the wheat that HuoYan finds. We choose Integrated Gradients [Sundararajan et al.
2017] and SHAP [Lundberg and Lee 2017], both of which have been widely used for explaining the
predictions of image and natural language models. Worth noting that the two methods are typically
contrastive (i.e., they account for deviations from a baseline), we design a baseline in which the
embedding for each token (or node) is set to 0. This is the conventional approach followed by
the explainability literature. Additionally, we choose the attention mechanism [Bahdanau et al.
2015], which makes models pay greater attention to certain factors (e.g., elements in an input)
when processing the input data. Therefore, the features that are heavily attended to can be deemed
as an explanation naturally. In theory, the three explainability methods may find the wheat that
HuoYan does not, therefore, we only use the 79,208 programs whose wheat are already verified in
Section 4.5.

First, we pick input features with top-𝑁 % highest attribution scores computed by the method.
Then, we show the percentage of the programs that are fully covered by those input features
(Table 8). Quite unexpectedly, the attention mechanism turns out to be the top performer in this
experiment. Up to the features that receive the top-70% attribution scores, attention beats the other
two methods by a significant margin across all evaluated models. Another interesting observation
we made about the attention is it can already cover a good amount of programs when only using
the top-10% of the input features and the increase of the coverage slows down when more features
are selected. This indicates the important role the attention plays in helping models to learn the
right features for prediction. As for the other two methods, SHAP’s advantage over Integrated
Gradients only stands out after the 50% mark, otherwise, their numbers are in the same ballpark.

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:20

Yu Wang, Ke Wang, and Linzhang Wang

Overall, it’s evident that none of those explanation methods are good at finding the wheat. This is
an expected outcome because the criteria by which WheaCha evaluates the importance of input
features is fundamentally different than the above explanation methods. Specifically, WheaCha
uses sufficient, necessary and Minimum requirement whereas the other three methods quantify the
influence of features in mathematical terms (e.g., gradients, SHAP values, or attention scores).

4.7 Explaining the Predictions of Code Models
We conduct a user study to evaluate our technique for producing explanations for the prediction
of all four models. As described in Section 3.5, our technique finds training methods similar to a
test method based on the AST distance between their wheat. We also adopt a simple baseline that
searches for training methods based on the AST distance of the entire method.

String [] reverse ( final String []

FastStringBuffer reverse () {

static <T > SList <T > reverse (

array ) {

final String [] newArray =

final int limit = count / 2;

new String [ array . length ];

for ( int i = 0; i < limit ; ++ i) {

SList <T > r) {

SList <T > res = null ;

for ( int index = 0; index <

array . length ; index ++) {

newArray [ array . length -

char c = value [i ];
value [i ] = value [ count - i - 1];
value [ count - i - 1] = c;

for (; r != null ; res = cons (r. car ,

res ), r = r. cdr )

System . out . println ("" );

index - 1] = array [ index ];

}

}
return newArray ;

}

return this ;

}

return res ;

}

(a) A test method.

(b) The closet found by our technique.

(c) The closet found by baseline.

Fig. 12. An example prediction produced by our technique and baseline.

(1) The system finds training examples that have similar structure (or substructure) to the test example.

(2) Given the similarity of the structure (or substructure), explanations that the system produce are accurate.

(3) As a user of the model, I find the system helpful in provide an insight of how models work.

(4) As a user of the model, I feel comfortable using this model along with the system providing the explana-

tions.

(5) As a designer of the model, I find the system helpful in debugging models’ mispredictions and improves

their quality.

Fig. 13. Questionnaire used in the study.

Procedure. Each participant is given two sets of methods. One set contains 4 methods randomly
selected from the test set of the three datasets, and each method is accompanied with 5 methods
found to the closet by our technique on the corresponding training set. As an example, we show a
test method (Figure 12a) accompanied by a training method (Figure 12b) that is the closest found
by our technique. We highlight their wheat in the shadow box. Similarly, the other set presents the
same content except that the 5 methods are found by the baseline approach. An example is shown in
Figure 12c. After reviewing a set of methods, each participant is given a set of questions to answer.
The questionnaire is depicted in Figure 13. For each question, participants are given 5 choices to
make: strong disagree, disagree, neutral, agree, and strongly agree. Each choice is interpreted as a
score starting from 1 for strongly disagree all the way to 5 for strong agree. We also conduct an

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Short Title

1:21

Table 9. Participants’ responses.

Question Our technique Baseline
4.7
4.5
3.9
3.1
3.6

2.1
1.8
1.8
1.2
1.5

1
2
3
4
5

interview after the study during which we encourage them to give any comments/suggestions that
they may have.
Participants. We have recruited 32 data scientists from a high-tech company through an internal
email thread and paid them $50 each for participating the study. Each participant has at least
one year of Java programming experience. All of them have at least two years of experience in
developing machine learning models at work. We have explained to each participant beforehand
that our technique finds the training programs from which models learned their parameters for
predicting the name of a particular test method, and they are asked to rate the quality of the
explanations.
Results. Table 9 shows the results of the questionnaire. In particular, each row contains the average
score to a question for our technique and baseline approach. It is clear that participants have rated
our technique consistently higher than the baseline approach by a notable margin. In terms of the
actual score, participants have given more than 3 to all questions indicating an overall positive
attitude toward our technique. Specifically, for Questions 1 and 2, participants are very positive
about the accuracy of the predictions found by our technique. For Question 3 and 5, participants
also agree that the system would benefit both users and designers of the model by providing the
rationale of model predictions. One participant even raises the possibility of performing re-training
with examples generated based on the wheat to improve the model accuracy. Question 4 is the only
one to which participants reacted a bit more negatively. Therefore, we focus on this question in
our post-study interview.

The main message we received from participants who give particularly low scores to this question
is that even though the system gives accurate explanations, they won’t be very helpful when models
only seem to use almost random textual properties to predict. For example, one participant explained
that once is explanation is given, albeit having a similar substructure of the test method, users still
have to manually examine the rest of the program in order to decide if the explanation can really
match the prediction of the test method. Therefore, having a model that learns the right features is
also necessary.

As standard in user studies, we performed a t-test to evaluate the statistical significance of our
results. The p-value of a two-tailed t-test (assuming potentially unequal variance) comparing our
technique against the baseline approach is 2 × 10−6. This means the probability that our technique
has no influence on classification accuracy is less than 1 in 100,000, indicating our results are
statistically significant.

5 RELATED WORK
The work closest to ours is Rabin et al. [2021b], which discovers that input programs can be reduced
to significantly smaller code snippets for which models still make the correct predictions. Our
work in fact surpasses Rabin et al. [2021b] at many levels. Conceptually, we propose necessary

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:22

Yu Wang, Ke Wang, and Linzhang Wang

requirement that wheat has to satisfy in addition to the sufficient requirement. Technically, we show
DD, which powers Sivand [Rabin et al. 2021b], suffers from models’ non-monotony. In contrast, our
technique Reduce and Mutate is more precise in identifying the wheat evidenced by our extensive
evaluation. Empirically, we demonstrate the validity of WheaCha’s approach in light of out-of-the
distribution issues, whereas, Sivand is fundamentally flawed in dealing with the distribution shift
between the training and query programs. Finally, we also present a novel application of the wheat
while Rabin et al. [2021b] have not demonstrated any utility of their method. For the remainder of
this section, we survey two lines of related work: attribution methods and models of code.

5.1 Attribution Methods
In machine learning field, attribution methods are usually classified into two categories: Perturbation-
based and backpropagation-based. The former generates explanations by iteratively probing a
trained machine learning model with different variations of the inputs. As a few representa-
tives, Zeiler et al. [2011] visualized the neural activations of individual layers of a deep convolu-
tional network by occluding different segments of the input image and generating visualizations
using a deconvolution network (DeConvNet). Zintgraf et al. [2017] use a conditional sampling
based multi-variate approach to generate more targeted explanations on image classification CNNs.
The Interpretability Randomization Test (IRT) and the One-Shot Feature Test (OSFT) introduced
by Burns et al. [2020] focuses on discovering important features by replacing the features with
uninformative counter-factuals. To derive a representation that is understandable by humans,
LIME [Ribeiro et al. 2016] tries to find important contiguous superpixels (a patch of pixels) in a
source image towards the output class. Lundberg and Lee [2017] present a unified framework,
SHAP, which computes individual feature contributions towards that output prediction.

As for the backpropagation-based methods, Saliency maps [Simonyan et al. 2014] construct
attributions by taking the absolute value of the partial derivative of the target output with respect
to the input features. Gradient∗Input [Shrikumar et al. 2016] was then proposed to improve the
sharpness of the attribution maps. The attribution is computed by taking the (signed) partial deriva-
tives of the output with respect to the input and multiplying them with the input itself. Integrated
Gradients [Sundararajan et al. 2017], similarly to Gradient∗Input, computes the partial derivatives
of the output with respect to each input feature. However, while Gradient∗Input computes a single
derivative, evaluated at the provided input 𝑥, Integrated Gradients computes the average gradient
while the input varies along a linear path from a baseline ˆ𝑥 to 𝑥. Layer-wise Relevance Propagation
(LRP) introduced by Bach et al. [2015] is used to find relevance scores for individual features in
the input data by decomposing the output predictions of the DNN. DeepLIFT [Shrikumar et al.
2017b], similar to LRP, assigns each unit 𝑖 an attribution that represents the relative effect of the
unit activated at the original network input 𝑥 compared to some other reference input.

5.2 Models of Code
Machine learning methods have been applied to a variety of programming language tasks such as
method name prediction [Alon et al. 2019a,b; DeFreez et al. 2018a,b; Fernandes et al. 2019; Wang and
Su 2020], bug detection [Allamanis et al. 2018; Wang et al. 2020], program repair [Chen et al. 2019;
Dinella et al. 2019; Sakkas et al. 2020], and type inference [Allamanis et al. 2020; Wei et al. 2019].
Below we survey a few notable representatives. GGNN is the first to learn program embeddings
from graph representations of source code. code2vec and code2seq are among the first in predicting
method names in large-scale, cross-projecting settings. DeFreez et al. [2018b] is the first to use static
program traces to learn function embeddings. Wang et al. [2018] is the first to embed programs
with their executions. Chen et al. [2019] and Dinella et al. [2019] utilize sequence and graph models
for program repair. Yao et al. [2020] and Si et al. [2018] are the noteworthy efforts in inferring

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Short Title

1:23

loop invariant with deep learning models. CodeBERT [Feng et al. 2020] learns general-purpose
representations that support downstream natural language-programming language applications.

6 CONCLUSION
In this paper, we present WheaCha, an explanation method for models of code. Conceptually,
we formalize the defining features that models use for predicting the label of input programs.
Technically, we develop Reduce and Mutate and its implementation HuoYan, which we use to
explain code2vec, seq-GNN, GGNN, and CodeBERT. We found that (1) HuoYan is efficient and
effective in finding wheat; (2) through retraining, we confirm the validity of WheaCha amid the
distribution shift between training and queried programs; (3) all models use simple syntactic or
even lexical properties for prediction; (4) some of the most popular attribution methods routinely
miss out on the wheat; (5) we present an example application of the revealed features: providing
explanations for predictions of code models. Through a user study, we have shown the usefulness
of our wheat-based explanation method.

REFERENCES
Miltiadis Allamanis, Earl T. Barr, Soline Ducousso, and Zheng Gao. 2020. Typilus: Neural Type Hints (PLDI ’20). Association

for Computing Machinery, New York, NY, USA, 91–105.

Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning to Represent Programs with Graphs. In

International Conference on Learning Representations (ICLR ’18).

Uri Alon, Omer Levy, and Eran Yahav. 2019a. code2seq: Generating Sequences from Structured Representations of Code. In

International Conference on Learning Representations (ICLR ’19).

Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019b. code2vec: Learning Distributed Representations of Code.

Proceedings of the ACM on Programming Languages 3, POPL (2019), 1–29.

Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek.
2015. On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation. PLOS ONE
10, 7 (07 2015), 1–46.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and

translate. In International Conference on Learning Representations (ICLR ’15).

Collin Burns, Jesse Thomason, and Wesley Tansey. 2020.

Interpreting Black Box Models via Hypothesis Testing. In
Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference (Virtual Event, USA) (FODS ’20). Association
for Computing Machinery, New York, NY, USA, 47–57.

Zimin Chen, Steve James Kommrusch, Michele Tufano, Louis-Noël Pouchet, Denys Poshyvanyk, and Martin Monperrus. 2019.
Sequencer: Sequence-to-sequence learning for end-to-end program repair. IEEE Transactions on Software Engineering
(2019).

Daniel DeFreez, Aditya Thakur, and Cindy Rubio-González. 2018a. Poster: Path-Based Function Embeddings. In 2018
IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion ’18). IEEE, 430–431.
Daniel DeFreez, Aditya V Thakur, and Cindy Rubio-González. 2018b. Path-based function embedding and its application to
error-handling specification mining. In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE ’18). 423–433.

Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang. 2019. Hoppity: Learning Graph Transforma-

tions to Detect and Fix Bugs in Programs. In International Conference on Learning Representations (ICLR ’19).

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin
Jiang, and Ming Zhou. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. In Findings of the
Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics, Online, 1536–1547.
Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Structured Neural Summarization. In International

Conference on Learning Representations (ICLR ’19).

Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In
Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (Proceedings of Machine
Learning Research), Yee Whye Teh and Mike Titterington (Eds.), Vol. 9. PMLR, Chia Laguna Resort, Sardinia, Italy,
249–256.

Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. 2019. A Benchmark for Interpretability Methods in
Deep Neural Networks. In Proceedings of the 33rd International Conference on Neural Information Processing Systems.

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:24

Yu Wang, Ke Wang, and Linzhang Wang

Curran Associates Inc., Red Hook, NY, USA, Article 873, 12 pages.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. CodeSearchNet challenge:

Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019).

D’Angelo John Philip and Douglas Brent West. 1997. Mathematical thinking: problem-solving and proofs. Prentice-Hall.
Scott M Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Advances in Neural
Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (Eds.). Curran Associates, Inc., 4765–4774.

Ghassan Misherghi and Zhendong Su. 2006. HDD: Hierarchical Delta Debugging. In Proceedings of the 28th International
Conference on Software Engineering (ICSE ’06). Association for Computing Machinery, New York, NY, USA, 142–151.
Md Rafiqul Islam Rabin, Nghi D.Q. Bui, Ke Wang, Yijun Yu, Lingxiao Jiang, and Mohammad Amin Alipour. 2021a. On the
generalizability of Neural Program Models with respect to semantic-preserving program transformations. Information
and Software Technology 135 (2021), 106552. https://doi.org/10.1016/j.infsof.2021.106552

Md Rafiqul Islam Rabin, Vincent J Hellendoorn, and Mohammad Amin Alipour. 2021b. Understanding Neural Code

Intelligence Through Program Simplification. arXiv preprint arXiv:2106.03353 (2021).

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why Should I Trust You?": Explaining the Predictions of
Any Classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
(San Francisco, California, USA) (KDD ’16). Association for Computing Machinery, New York, NY, USA, 1135–1144.
Georgios Sakkas, Madeline Endres, Benjamin Cosman, Westley Weimer, and Ranjit Jhala. 2020. Type Error Feedback via
Analytic Program Repair. In Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and
Implementation (London, UK) (PLDI ’20). Association for Computing Machinery, New York, NY, USA, 16–30.

Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017a. Learning Important Features Through Propagating
Activation Differences. In Proceedings of the 34th International Conference on Machine Learning (Proceedings of Machine
Learning Research), Doina Precup and Yee Whye Teh (Eds.), Vol. 70. PMLR, 3145–3153.

Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017b. Learning Important Features through Propagating
Activation Differences. In Proceedings of the 34th International Conference on Machine Learning - Volume 70 (Sydney,
NSW, Australia) (ICML ’17). JMLR.org, 3145–3153.

Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. 2016. Not just a black box: Learning important

features through propagating activation differences. arXiv preprint arXiv:1605.01713 (2016).

Xujie Si, Hanjun Dai, Mukund Raghothaman, Mayur Naik, and Le Song. 2018. Learning Loop Invariants for Program
Verification. In Proceedings of the 32nd International Conference on Neural Information Processing Systems (Montréal,
Canada) (NIPS ’18). 7762–7773.

Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional networks: Visualising image

classification models and saliency maps. In In Workshop at International Conference on Learning Representations.

Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic Attribution for Deep Networks. In Proceedings of the 34th
International Conference on Machine Learning - Volume 70 (Sydney, NSW, Australia) (ICML ’17). JMLR.org, 3319–3328.
Ke Wang, Rishabh Singh, and Zhendong Su. 2018. Dynamic Neural Program Embedding for Program Repair. In International

Conference on Learning Representations (ICLR ’18).

Ke Wang and Zhendong Su. 2020. Blended, Precise Semantic Program Embeddings. In Proceedings of the 41st ACM SIGPLAN

International Conference on Programming Language Design and Implementation (PLDI ’20).

Yu Wang, Ke Wang, Fengjuan Gao, and Linzhang Wang. 2020. Learning Semantic Program Embeddings with Graph Interval
Neural Network. Proceedings of the ACM on Programming Languages 4, OOPSLA, Article 137 (Nov. 2020), 27 pages.
Jiayi Wei, Maruth Goyal, Greg Durrett, and Isil Dillig. 2019. LambdaNet: Probabilistic Type Inference using Graph Neural

Networks. In International Conference on Learning Representations (ICLR ’19).

Jianan Yao, Gabriel Ryan, Justin Wong, Suman Jana, and Ronghui Gu. 2020. Learning Nonlinear Loop Invariants with Gated
Continuous Logic Networks. In Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and
Implementation (London, UK) (PLDI ’20). Association for Computing Machinery, New York, NY, USA, 106–120.

Matthew D Zeiler, Graham W Taylor, and Rob Fergus. 2011. Adaptive deconvolutional networks for mid and high level

feature learning. In 2011 International Conference on Computer Vision. IEEE, 2018–2025.

Andreas Zeller. 1999. Yesterday, my program worked. Today, it does not. Why? ACM SIGSOFT Software engineering notes 24,

6 (1999), 253–267.

Andreas Zeller and Ralf Hildebrandt. 2002. Simplifying and isolating failure-inducing input. IEEE Transactions on Software

Engineering 28, 2 (2002), 183–200.

Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and Max Welling. 2017. Visualizing Deep Neural Network Decisions:

Prediction Difference Analysis. In International Conference on Learning Representations (ICLR ’17).

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Short Title

1:25

A DEFINITION OF SUBSEQUENCE
A subsequence of <𝑎> is a sequence <𝑏> defined by 𝑏𝑘 = 𝑎𝑛𝑘 , where 𝑛1 < 𝑛2 < ... is an increasing
sequence of indices [John Philip and West 1997].

For example, if 𝑎𝑛 = 2𝑛 − 1 and 𝑛𝑘 = 𝑘 2, then 𝑏𝑘 = 2𝑘 2 − 1 [John Philip and West 1997].

𝑛
𝑎𝑛
𝑘
𝑏𝑘

1
1
1
1

2
3

3
5

4
7
2
7

5
9

6
11

7
13

8
15

9
17
3
17

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:26

Yu Wang, Ke Wang, and Linzhang Wang

B THE EXISTENCE OF WHEAT
Theorem B.1 (Existence of Wheat). Given a prediction 𝐿 that 𝑀 makes for an input program 𝑃,
the wheat ˜𝑃 that models use to predict the label of 𝑃 always exists.

Proof. Assume otherwise, so that the wheat ˜𝑃 does not exist for 𝑃.
Because the body of 𝑃, 𝐵𝑃 , satisfies constituent, sufficient, and necessary requirement in Defi-
nition 2.1. It has to be the minimum requirement that 𝐵𝑃 violates, meaning, there exists a set of
statements/expressions 𝑃 ′ that also satisfies all but the minimum requirement, and |(𝑡 𝑃 ′
𝑛 )𝑛 ∈N| <
|(𝑡 𝐵𝑃
𝑛 )𝑛 ∈N|. Since 𝑃 ′ is not the wheat either, we can infer that 𝑃 ′ is also not minimum.
Because the domain that contains all sets of statements/expressions whose token sequence is a
subsequence of 𝑃’s is finite, and the size of the candidate programs will monotonically decreases
(i.e., |(𝑡 𝑃 ′
𝑛 )𝑛 ∈N|, and so on), there will be a global minimum set
of statements/expressions ˜𝑃 that satisfies all requirements in Definition 2.1, which implies that ˜𝑃 is
the wheat for 𝑃. This contradicts the assumption that ˜𝑃 does not exist for 𝑃.
□

𝑛 )𝑛 ∈N| < |(𝑡 𝐵𝑃

𝑛 )𝑛 ∈N| < |(𝑡 𝑃 ′

𝑛 )𝑛 ∈N|, |(𝑡 𝑃 ′′

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Short Title

1:27

C EXAMPLES OF DELTA DEBUGGING
The motivating example includes nineteen tokens. At the first step, we split the program into two par-
titions: the first partition (Δ1) contains the first nine tokens (List, Object, mItems, =, retQueue,
if, position, >, mItems) and the second one (Δ2) has the last ten tokens (size, return, mItems,
add, position, genItem, notifyItemInserted, position, log, "add item"). We proceed with Δ2
since it satisfies the sufficient and necessary requirement. Then, we split Δ2 into two partitions, and
demonstrate the subsequent steps in Table 10.

Table 10. An example from sequence GNN model without the monotonicity. Δ𝑖 denotes partitions and ∇𝑖
is the complement of Δ𝑖 . For simplicity, we use tokens to represent programs that are tested against the
sufficient and necessary requirement at each step. The last column shows the requirements that partitions do
not satisfy, but pass means the partitions satisfy both requirements.

Step Partition

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

Δ3
Δ4
Δ5 = ∇6
Δ6 = ∇5
Δ7
Δ8
Δ9
Δ10
∇7
∇8
∇9
∇10
Δ11
Δ12
Δ13
Δ14
Δ15
∇11
∇12
∇13
∇14
∇15

size
✓

✓

✓

✓
✓
✓
✓

✓
✓
✓
✓

return mItems

✓

✓

✓

✓

✓
✓

✓

✓

✓
✓
✓

✓

✓

✓

✓
✓

✓

✓

✓
✓

✓
✓

Tokens
add position genItem log
✓

✓

"add item" position

✓

✓

✓

✓

✓

✓

✓
✓
✓
✓

✓
✓
✓
✓
✓

✓
✓
✓
✓

✓

✓
✓
✓

✓

Results

Pass
Necessary
Both
Both
Both
Both
Both
Both
Sufficient
Sufficient
Sufficient
Sufficient
Both
Both
Both
Both
Both
Sufficient
Sufficient
Sufficient
Sufficient
Sufficient

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:28

Yu Wang, Ke Wang, and Linzhang Wang

D AN EXAMPLE OF THE SUBTRACTION OPERATION IN THE MUTATE STEP
Figure 14 gives a detailed illustration for subtracting mItems.add(); from the original program.

(a)

(c)

(b)

(d)

Fig. 14. A illustration of the four-step process of subtracting mItems.add(); from our example program. (1)
Figure (a) presents the AST of the running example, and the AST of mItems.add(); in the bottom left corner.
For brevity, the ASTs are simplified. (2) Figure (b) highlights the overlapping nodes between the two ASTs. (3)
Figure (c) emphasizes the resultant AST after the overlapping nodes are removed, as a result, position and
getItem() become dangling nodes. (4) Figure (d) connects position and getItem() to the body of the
method.

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Member CallMember ExprmItemsaddParametersMethod BodyMember CallMember ExprmItemsaddnotifyItem Inserted (position)ParametersgenItempositionMethod CallList<Obj>mItems =retQueue()if(position >mItems.size())returnlog("Additem;")Method BodyMember CallMember ExprmItemsaddnotifyItem Inserted (position)ParametersgenItempositionMethod CallList<Obj>mItems =retQueue()if(position >mItems.size())returnlog("Additem;")Method BodyMember CallMember ExprmItemsaddnotifyItem Inserted (position)ParametersgenItempositionMethod CallList<Obj>mItems =retQueue()if(position >mItems.size())returnlog("Additem;")Method BodynotifyItem Inserted (position)genItempositionMethod CallList<Obj>mItems =retQueue()if(position >mItems.size())returnlog("Additem;")Short Title

1:29

E THE FUNCTIONALITY OF DELETENODE AND MUTATENODE
Algorithm 3 gives a detailed illustration for DeleteNode and MutateNode functions. In the DeleteNode
function, we first try removing the node, which comes from its first parameter, then check whether
the resultant program satisfies both the sufficient and necessary requirement (Line 3). If it satisfies,
we update the current program to the new one whose node is deleted (Line 4). Similarly, in the
MutateNode function, we will mutate the node into one with an out-of-vocabulary value (Line 8)
and update the program if the resultant program satisfies the two requirements (Lines 9 and 10).
Both DeleteNode and MutateNode functions invoke the VerifyWheat function, which constructs
two programs for checking against the sufficient and necessary requirements, respectively. The
function returns True only when the resultant program satisfy both requirements, otherwise it
returns False.

Algorithm 3 Delete and mutate AST nodes.

1: function DeleteNode(𝑛𝑜𝑑𝑒, 𝑟𝑜𝑜𝑡, 𝑝𝑟𝑜𝑔𝑟𝑎𝑚, 𝑚𝑜𝑑𝑒𝑙)
2:
3:
4:

new_root_del ← root.Delete (node)
if VerifyWheat(new_root_del , root, program, model) then

Update (root, new_root_del)

⊲ remove node

⊲ update to the new program

5:
end if
6: end function

7: function MutateNode(𝑛𝑜𝑑𝑒, 𝑟𝑜𝑜𝑡, 𝑝𝑟𝑜𝑔𝑟𝑎𝑚, 𝑚𝑜𝑑𝑒𝑙)
new_root_mut ← root.Replace (node, oov)
8:
if VerifyWheat(new_root_mut , root, program, model) then

Update (root, new_root_mut)

9:
10:
11:
end if
12: end function

⊲ replace node with oov

program_suff ← Serialize (node)
if model.Predict (program_suff) != model.Predict (program) then

13: function VerifyWheat(𝑛𝑜𝑑𝑒, 𝑟𝑜𝑜𝑡, 𝑝𝑟𝑜𝑔𝑟𝑎𝑚, 𝑚𝑜𝑑𝑒𝑙)
14:
15:
16:
17:
18:
19:

end if
program_necc ← Subtract (root, node)
if model.Predict (program_necc) ==model.Predict (program) then

return False

⊲ checking against the sufficient requirement

⊲ does not satisfy the sufficient requirement

⊲ checking against the necessary requirement

⊲ does not satisfy the necessary requirement

return False

20:
21:
end if
return True
22:
23: end function

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:30

Yu Wang, Ke Wang, and Linzhang Wang

F PERFORMANCE OF RE-IMPLEMENTED MODELS
We have re-implemented code2vec, Sequence GNN, GGNN and CodeBERT. Table 11, 12, and 13 show
the performance of all re-implemented models is either comparable or superior to the originals.

Models

Table 11. Compare reimplementations (bolded) to originals for code2vec and Seq-GNN.
Java-large
Precision Recall
38.40
37.26
—
50.32

Java-small
Precision Recall
18.74
17.72
—
47.35

Java-med
Precision Recall
28.31
28.89
—
45.73

F1
32.49
33.66
—
51.32

F1
18.62
18.44
51.4
48.61

38.12
40.32
—
58.46

18.51
19.23
—
49.94

48.15
48.90
—
61.82

code2vec
code2vec
Seq-GNN
Seq-GNN

F1
42.73
42.29
—
55.48

Table 12. Compare reimplementations (bolded) to
originals for GGNN.

Table 13. Compare reimplementations (bolded) to
originals for CodeBERT.

Models

GGNN
GGNN

C# Datasets
Accuracy
78.2
78.0

Models

CodeBERT
CodeBERT

CodeSearchNet
Smoothed BLEU score
17.65
17.66

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Short Title

1:31

G ADDITION EDGES USED IN FERNANDES ET AL.
Below we give the list of edges Fernandes et al. [2019] incorporate into ASTs.
• NextToken connects each terminal node (syntax token) to its successor.
• LastRead connects a terminal node of a variable to all elements of the set of terminal nodes

at which the variable could have been read last.

• LastWrite connects a terminal node of a variable to all elements of the set of syntax tokens

at which the variable was could have been last written to.

• ComputedFrom connects a terminal node of a variable 𝑣 to all variable tokens occurring in

𝑒𝑥𝑝𝑟 when 𝑒𝑥𝑝𝑟 is assigned to 𝑣.

• LastLexicalUse chains all uses of the same variable.
• ReturnsTo connects return tokens to the method declaration.
• FormalArgName connects arguments in method calls to the formal parameters that they

are matched to.

• GuardedBy connects every token corresponding to a variable (in the true branch of a if

statement) to the enclosing guard expression that uses the variable.

• GuardedByNegation connects every token corresponding to a variable (in the false branch

of a if statement) to the enclosing guard expression that uses the variable.

We exclude NextToken and ReturnsTo since they do not represent any semantic properties.

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:32

Yu Wang, Ke Wang, and Linzhang Wang

H PERFORMANCE OF THE RE-TRAINED MODELS ON THE AUGMENTED

DATASETS.

We have re-trained code2vec, Sequence GNN, GGNN and CodeBERT on the manually labeled
datasets. Table 14, 15, and 16 show the performance of all re-trained models is negligibly higher
than original models.

Table 14. Compare code2vec and Seq-GNN before and after the retraining (bolded numbers denotes the
results of retrained models).

Models

code2vec
code2vec
Seq-GNN
Seq-GNN

Java-small
Precision Recall
17.72
18.01
47.35
48.39

19.23
19.33
49.94
50.12

F1
18.44
18.74
48.61
48.62

Java-med
Precision Recall
28.89
28.97
45.73
46.09

40.32
40.38
58.46
58.51

F1
33.66
33.96
51.32
52.00

Table 15. Compare GGNN before and after the re-
training (bolded numbers denotes the results of the
retrained model).

Table 16. Compare CodeBERT before and after the
retraining (bolded numbers denotes the results of
the retrained model).

Models

GGNN
GGNN

C# Datasets
Accuracy
78.0
78.8

Models

CodeBERT
CodeBERT

CodeSearchNet
Smoothed BLEU score
17.66
17.91

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

