2
2
0
2

y
a
M
0
3

]
L
M

.
t
a
t
s
[

2
v
6
3
1
2
1
.
3
0
2
2
:
v
i
X
r
a

Wasserstein Distributionally Robust Optimization
with Wasserstein Barycenters

Tim Tsz-Kit Lau∗

Han Liu†

May 31, 2022

Abstract

In many applications in statistics and machine learning, the availability of data samples from multiple
possibly heterogeneous sources has become increasingly prevalent. On the other hand, in distributionally
robust optimization, we seek data-driven decisions which perform well under the most adverse distribution
from a nominal distribution constructed from data samples within a certain discrepancy of probability
distributions. However, it remains unclear how to achieve such distributional robustness in model learning
and estimation when data samples from multiple sources are available. In this work, we propose construct-
ing the nominal distribution in optimal transport-based distributionally robust optimization problems
through the notion of Wasserstein barycenter as an aggregation of data samples from multiple sources.
Under speciﬁc choices of the loss function, the proposed formulation admits a tractable reformulation
as a ﬁnite convex program, with powerful ﬁnite-sample and asymptotic guarantees. As an illustrative
example, we demonstrate with the problem of distributionally robust sparse inverse covariance matrix
estimation for zero-mean Gaussian random vectors that our proposed scheme outperforms other widely
used estimators in both the low- and high-dimensional regimes.

1

Introduction

In various statistical and machine learning applications, data samples are collected from multiple sources,
which can be viewed as samples drawn from multiple data distributions. A notable example is federated
learning (Kairouz et al., 2021; McMahan et al., 2017), in which many users collaboratively learn a common
model but the samples collected by the clients might have highly heterogeneous distributions. This distribution
heterogeneity leads to diﬃculty in building a robust model in two aspects: (i) how to aggregate estimations
of the distributions with data samples from these sources; (ii) how to perform robust estimation with this
data aggregation given distributional uncertainty.

In practice, the ﬁrst issue is usually dealt with by simply taking a simple (weighted) average of the
distribution estimates, whereas the second one is tackled by minimizing the weighted aggregate loss with
possibly diﬀerent weights. However, the mixture distribution constructed from the weighted average of
distributions does not take into account the geometric structure of data samples, thus failing to well summarize
the characteristics from all sources. In this work, we consider the notion of barycenter (a.k.a. Fr´echet mean)
in the space of probability distributions endowed with the Wasserstein distance, called Wasserstein barycenter
(Agueh and Carlier, 2011), which is a nonlinear interpolation between distributions.

To perform robust estimation of models against distributional uncertainty, distributionally robust opti-
mization (DRO; Delage and Ye, 2010; Goh and Sim, 2010; Wiesemann et al., 2014) has been shown to be a
powerful modeling framework, which has aroused much attention in the machine learning community lately
attributed to its connections to generalization, regularization and robustness.

∗Department of Statistics and Data Science, Northwestern University, Evanston,

IL 60208, USA; Email:

timlautk@u.northwestern.edu.

†Department of Computer Science and Department of Statistics and Data Science, Northwestern University, Evanston, IL

60208, USA; Email: hanliu@northwestern.edu.

1

 
 
 
 
 
 
Contributions. We thus propose a uniﬁed approach to overcome these two aspects of diﬃculty. We ﬁrst
construct an aggregate distribution of multiple data distributions through Wasserstein barycenter, followed by
using it to deﬁne an ambiguity set in a DRO problem, which is a family of distributions lying within a certain
Wasserstein distance from this Wasserstein barycenter, coined the Wasserstein barycentric ambiguity set. We
hence introduce Wasserstein Barycentric DRO (WBDRO) as a general aggregate data-driven decision making
framework with distributional robustness against uncertainty arising in multiple possibly heterogeneous
unknown true distributions.

We establish ﬁnite-sample guarantees and asymptotic consistency results for WBDRO. We also consider
an approximation of the Wasserstein ambiguity set by characterizing it using only the ﬁrst two moments of
the family of distributions and those of the nominal distribution, called the Gelbrich ambiguity set. We further
extend this construction in the case of multiple nominal distributions using the 2-Wasserstein barycenter. We
also exemplify WBDRO through distributionally robust maximum likelihood estimation for sparse inverse
covariance matrices of zero-mean Gaussian random vectors, which numerically outperforms other widely-used
estimators.

1.1 Related Work

Distributionally Robust Optimization. As a powerful modeling framework, DRO has recently found a
wide range of applications in statistics and machine learning (Bertsimas and Van Parys, 2022; Blanchet
et al., 2019a; Duchi and Namkoong, 2021; Duchi et al., 2021; Li et al., 2021; Nguyen et al., 2021b, 2022;
Shaﬁeezadeh-Abadeh et al., 2015, 2019; Taskesen et al., 2021b), signal processing (Shaﬁeezadeh-Abadeh et al.,
2018), portfolio selection and maximization (Blanchet et al., 2021a; Nguyen et al., 2021a,c; Ob(cid:32)l´oj and Wiesel,
2021), etc. One key component of DRO is the choice of data-driven ambiguity sets, which can be deﬁned
through f -divergence (Ben-Tal et al., 2013; Duchi and Namkoong, 2021; Duchi et al., 2021), Wasserstein
distance (Blanchet et al., 2019b; Gao, 2020; Gao and Kleywegt, 2016; Gao et al., 2017; Pﬂug and Wozabal,
2007), generalized moment constraints (Bertsimas et al., 2018; Delage and Ye, 2010; Goh and Sim, 2010;
Wiesemann et al., 2014), maximum mean discrepancy (MMD; Staib and Jegelka, 2019), etc. Tractable
reformulation as ﬁnite convex programs are available for DRO problems with these diﬀerent ambiguity sets.
We refer to Carmon and Hausler (2022); Haddadpour et al. (2022); Jin et al. (2021); Levy et al. (2020); Li
et al. (2021); Yu et al. (2022) for recent advances in the computational perspectives of DRO, and Zhen et al.
(2021) for a review of the mathematical foundations of DRO.

Notion of Mean Distributions. Fr´echet mean or barycenter in diﬀerent metric spaces, as a notion of
mean distributions, has long been a central object in statistical analysis. Notably, a (weighted) average of
distributions on Rd is a barycenter in Euclidean space. The Wasserstein barycenter (Agueh and Carlier,
2011; Kroshnin, 2018) is a more appropriate notion of mean distributions since the geometric structure of the
distributions can be considered (see e.g., Backhoﬀ-Veraguas et al., 2018). This notion has already appeared
in various applications in statistics and machine learning (Backhoﬀ-Veraguas et al., 2018; Bigot et al., 2019b;
Bishop, 2014; Bishop and Doucet, 2021; Schmitz et al., 2018; Srivastava et al., 2018; Yang and Tabak, 2021).
In particular, the work ´Alvarez-Esteban et al. (2018) shares similar motivation to ours, which is to perform
consensus-based estimation combining several estimations of probability distributions.

Learning with Data from Multiple Sources. Modern machine learning applications involve the use of data
collected from multiple sources. Such examples include federated learning (Kairouz et al., 2021; McMahan
et al., 2017; Wang et al., 2021a), (multiple-source) domain adaptation (Mansour et al., 2021; Zhang et al.,
2021), information fusion and network consensus (Bishop, 2014; Bishop and Doucet, 2021). However, the
consensus problem indeed has a much longer history (see e.g., DeGroot, 1974).

A more detailed discussion on other related prior work can be found in Appendix A.

2 Preliminaries

Notation. We denote by Id ∈ Rd×d the d × d identity matrix and 1d ∈ Rd the d-dimensional all-one vector.
:= {1, . . . , n} for
The subscripts for dimensions are suppressed if they are clear from context. We deﬁne

n

(cid:74)

(cid:75)

2

++ (resp. Sd

+) denote the set of symmetric positive (resp. semi-)deﬁnite matrices. P(X) is the
n ∈ N∗. Let Sd
set of Borel probability measures over the Polish space X, Pk(X) is the set of probability measures over X
with ﬁnite k-order moments, and P ac
k (X) is the set of absolutely continuous probability measures over X
(w.r.t. the Lebesgue measure) with ﬁnite k-order moments. The set (cid:52)d := {p ∈ [0, +∞)d : (cid:104)p, 1d(cid:105) = 1} is
the (d − 1)-dimensional probability simplex, where (cid:104)·, ·(cid:105) is the usual inner product. We denote by N(µ, Σ) a
Gaussian distribution with mean µ ∈ Rd and covariance matrix Σ ∈ Sd
+, and δx a Dirac measure at point
x ∈ X.

Optimal Transport. We introduce several notions from optimal transport (OT) used throughout the whole
paper, which can be found in various monographs on the subject (Ambrosio et al., 2021; Figalli and Glaudo,
2021; Peyr´e and Cuturi, 2019; Santambrogio, 2015; Villani, 2003, 2009). We also refer to Panaretos and Zemel
(2019, 2020); Peyr´e and Cuturi (2019) for comprehensive reviews of recent advances of optimal transport
in machine learning and statistics. Let Ω ⊆ Rm be a closed convex set. For p ∈ [1, +∞), the p-Wasserstein
distance between two probability measures ρ, ν ∈ Pp(Ω) is deﬁned by

Wp(ρ, ν) :=

(cid:18)

inf
π∈Π(ρ,ν)

(cid:90)

Ω×Ω

(cid:107)x − y(cid:107)p dπ(x, y)

(cid:19)1/p

,

(2.1)

where (cid:107)·(cid:107) is the Euclidean norm on Rm, and Π(ρ, ν) denotes the set of joint distributions on Rm × Rm with
ρ and ν as marginals. The p-Wasserstein distance is a distance on the space Pp(Ω) (see e.g., Figalli and
Glaudo, 2021, Theorem 3.1.5). We call the metric space Wp(Ω) := (Pp(Ω), Wp) the p-Wasserstein space
(Ambrosio et al., 2005).

The notion of Wasserstein barycenter (Agueh and Carlier, 2011) can be viewed as the mean of probability
distributions in the Wasserstein space. For p ∈ [1, +∞), the p-Wasserstein barycenter of P ∈ Wp(Pp(Ω)) is
deﬁned by

bp(P) := argmin
ν∈Pp(Rm)

Eρ∼P

(cid:2)Wp

p(ν, ρ)(cid:3),

(2.2)

where ρ ∈ Pp(Ω) is a random measure with distribution P. If we take P = (cid:80)K
λ = (λk)k∈

∈ (cid:52)K, we recover the λ-weighted empirical p-Wasserstein barycenter, deﬁned by

k=1 λkδρk in (2.2), where

K
(cid:74)

(cid:75)

(cid:98)bλ,p(ρ1, . . . , ρK) := argmin
ν∈Pp(Rm)

K
(cid:88)

k=1

λkWp

p(ν, ρk).

Thus, we use the term p-Wasserstein barycenter to refer to both empirical and population p-Wasserstein
barycenters whenever it is clear from context. Note that Wasserstein barycenters do not always exist, and
might not be unique if exist. Technical conditions for their existence and uniqueness are studied in e.g.,
Agueh and Carlier (2011); Le Gouic and Loubes (2017).

Distributionally Robust Optimization.
In DRO, we investigate a learning problem under distributional
uncertainty which is casted as a generic expected loss minimization framework. The loss function (cid:96) : Rm →
R := R ∪ {±∞} is a function of the the uncertainty vector ξ ∈ Rm whose distribution P is supported on
Ξ ⊆ Rm. The risk (or expected loss) of a decision (cid:96) ∈ L is deﬁned as

RP((cid:96)) := Eξ∼P[(cid:96)(ξ)],

where L is the set of all admissible loss functions. The optimal risk is then deﬁned as the inﬁmum of the risk
over L. However, P is often unknown in practice except for some limited statistical and structural information
about it. We thus assume that P is known to lie in an ambiguity set Uε((cid:98)P), which is a ball of radius ε (cid:62) 0 in
P(Ξ) centered at the nominal distribution (cid:98)P in some discrepancy between probability distributions. We can
then deﬁne the worst-case risk of (cid:96) ∈ L by

RUε((cid:98)P)((cid:96)) := sup

P∈Uε((cid:98)P)

RP((cid:96)).

3

(2.3)

Such a nominal distribution (cid:98)P is usually constructed from a set of observed data D := {zi}n
its empirical measure 1
n
achieving the optimal worst-case risk

i=1 ⊂ Rm, e.g.,
i=1 δzi. The distributionally robust optimization (DRO) problem seeks decisions

(cid:80)n

RUε((cid:98)P)(L) := inf
(cid:96)∈L

RUε((cid:98)P)((cid:96)).

(2.4)

Remark 2.1. Note that if the loss function is parameterized by the decision x ∈ X ⊆ Rd, i.e., (cid:96) : X × Rm → R,
then the risk can be deﬁned in terms of x as RP(x) := Eξ∼P[(cid:96)(x, ξ)]. The worst-case risk and the worst-case
optimal risk can be deﬁned similarly.

In this paper, we consider the ambiguity set deﬁned via the p-Wasserstein distance. Then, the p-Wasserstein

ambiguity set is deﬁned by

Wε,p((cid:98)P) := {Q ∈ Pp(Ξ) : Wp(Q, (cid:98)P) (cid:54) ε},
where Ξ ⊆ Rm is a closed set which is known to contain the support of the unknown true distribution P(cid:63) and
ε (cid:62) 0. Such a DRO formulation is called the Wasserstein DRO (WDRO).

3 Learning with Aggregation of Multiple Distributions

Problem Formulation.
In diﬀerent centralized model learning scenarios with data from multiple sources,
such as federated learning, the learning objective can usually be casted as a stochastic composition optimization
problem (see e.g., Wang et al., 2021a; Yuan et al., 2022):

minimize
x∈X

F (x) := Ek∼D[fk(x)], where

fk(x) := Eξ∼Pk [(cid:96)(x, ξ)],

(3.1)

where x ∈ X is the parameter of the global model for some closed convex set X ⊆ Rd, ξ ∈ Ξ is a random vector
representing an input-output pair for some sample space ⊆ Rm, fk : Rd → R is the local objective function
of the kth source, Pk is the distribution associated to the kth source, and D is a distribution supported on
the set of sources K. Assuming there is only a ﬁnite number of K sources, i.e., K =
, then the objective
in (3.1) can be written as Fλ(x) := (cid:80)K
k=1 λkfk(x), where D is taken to be a categorical distribution with
probabilities λ = (λk)k∈

Usually, each data source k has a ﬁnite number of local samples, denoted by Dk = (zk,1, . . . , zk,nk ),
k=1 nk. Using their empirical distributions
[(cid:96)(x, ξ)], we usually solve the following empirical risk minimization

where nk is the sample size of the kth source and N := (cid:80)K
(cid:98)Pk := 1
nk
(ERM) problem in practice:

i=1 δzk,i , with (cid:98)fk(x) := Eξ∼(cid:98)Pk

∈ (cid:52)K.

(cid:80)nk

K
(cid:74)

K

(cid:74)

(cid:75)

(cid:75)

minimize
x∈X

(cid:98)Fλ(x) :=

K
(cid:88)

k=1

λk (cid:98)fk(x) =

K
(cid:88)

k=1

λk
nk

nk(cid:88)

i=1

(cid:96)(x, zk,i),

(3.2)

Note that λ is usually taken as the uniform distribution over the numbers of samples from the sources, i.e.,
λk = nk/N , so that the ERM objective (3.2) is amount to an ERM objective with the union of all the local
data samples.

However, as argued by Mohri et al. (2019); Ro et al. (2021), this choice of the uniform distribution is
questionable since there is often a mismatch between the target distribution (for which the centralized model
is learned) and the mixture distribution (cid:80)K
k=1 nkPk/N . Instead, the target distribution is better expressed
as a λ-mixture of P1, . . . , PK, i.e., Pλ := (cid:80)K
k=1 λkPk for some λ ∈ (cid:52)K. Then, with the λ-mixture of the
empirical distributions (cid:98)Pλ := (cid:80)K
i=1 δzk,i , it is not hard to see that the objective in
[(cid:96)(x, ξ)].
(3.2) is equivalent to Eξ∼(cid:98)Pλ

k=1 λk (cid:98)Pk = (cid:80)K

(cid:80)nk

λk
nk

k=1

Stochastic Barycentric Optimization. Let us recall that Pλ is the λ-weighted Euclidean barycenter of the
distributions P1, . . . , PK. Leveraging this important fact, we consider more generally a λ-weighted barycenter
(cid:98)bλ(P1, . . . , PK) of P1, . . . , PK deﬁned via some discrepancy between distributions such as the Wasserstein
distance, so that (3.1) can be formulated with the objective

F b
λ(x) := Eξ∼(cid:98)bλ(P1,...,PK )[(cid:96)(x, ξ)],

4

which we refer to as a stochastic barycentric optimization (SBO) problem. With the data samples D1, . . . , DK,
we also have its surrogate objective deﬁned with the empirical distributions (cid:98)P1, . . . , (cid:98)PK, given by

(cid:98)F b
λ(x) := Eξ∼(cid:98)bλ((cid:98)P1,...,(cid:98)PK )[(cid:96)(x, ξ)].

Unfortunately, except for the case of the Euclidean barycenter, (cid:98)F b

λ usually cannot be expressed as a ﬁnite
sum. This appears to be unfavorable computationally compared to (3.2). To solve it computationally, one
can resort to ERM by drawing samples from the barycenter of empirical distributions. However, solving such
an ERM problem requires (i) the computation of a barycenter; and (ii) sampling from such a barycenter.
In the case of the Wasserstein barycenter, these tasks could be computationally intensive (Altschuler and
Boix-Adser`a, 2022) or not well addressed until recently (Daaloul et al., 2021). However, the choice of the
Wasserstein barycenter over the Euclidean barycenter in SBO is justiﬁed in the sense that the Euclidean
barycenter usually fails to take into account the underlying geometry of these distributions (see e.g., Backhoﬀ-
Veraguas et al., 2018). Thus, despite the potential computational obstacles, we speciﬁcally consider the
Wasserstein barycenter (and possibly its entropic-regularized variants). We also provide further discussion on
the connections of this formulation to other related machine learning paradigms in Appendix A.

4 Wasserstein Barycentric Distributionally Robust Optimization

Although the use of the Wasserstein barycenter might give a better consensus representation of samples from
diﬀerent sources, discrepancy between the target distribution and the Wasserstein barycenter of the empirical
distributions might still arise, due to e.g., sampling errors and data heterogeneity across the sources. In a
similar spirit to the single-source case, we propose to hedge against the impact of such model misspeciﬁcation
through the lens of WDRO.

Wasserstein Ambiguity Sets with Wasserstein Barycenters. Suppose that we have a ﬁnite number of K
data sources with probability distributions Q1, . . . , QK respectively. Aside from directly considering the notion
of Wasserstein barycenters, one way to construct an ambiguity set out of these K probability distributions
is to consider the intersection of the individual Wasserstein ambiguity sets (cid:84)K
Wε,p(Qk), but if the data
sources are very heterogeneous then this could lead to an overly conservative ambiguity set using the same
large ε. Alternatively, a less conservative ambiguity set based on these K distributions can be deﬁned by

k=1

(cid:102)Wε,p(Q1, . . . , QK; λ) :=

P ∈ Pp(Ξ) :

(cid:40)

λkWp

p(P, Qk) (cid:54) εp

(cid:41)
.

K
(cid:88)

k=1

(4.1)

It is straightforward to observe that (cid:84)K
Wε,p(Qk) ⊆ (cid:102)Wε,p(Q1, . . . , QK; λ) for any λ ∈ (cid:52)K. In the following,
we illustrate that how the ambiguity set (4.1) is related to another ambiguity set deﬁned with the λ-weighted
p-Wasserstein barycenter of Q1, . . . , QK.

k=1

Deﬁnition 4.1 (Wasserstein barycentric ambiguity set). For λ ∈ (cid:52)K, the p-Wasserstein barycentric
ambiguity set with radius ε (cid:62) 0 centered at a λ-weighted p-Wasserstein barycenter of Q1, . . . , QK (if exists),
denoted by Qλ,p := (cid:98)bλ,p(Q1, . . . , QK), is deﬁned by

Wε,p(Q1, . . . , QK; λ) := (cid:8)P ∈ Pp(Ξ) : Wp(P, Qλ,p) (cid:54) ε(cid:9).

(4.2)

Note that Wε,p(Q1, . . . , QK; λ) = Wε,p(Qλ,p). The ambiguity sets (4.1) and (4.2) of diﬀerent radii (diﬀered

by a factor of 2p) can be related by the following inclusion.

Theorem 4.2. For λ ∈ (cid:52)K, suppose that a λ-weighted p-Wasserstein barycenter of Q1, . . . , QK exists. Then,
for any ε (cid:62) 0, the following inclusion of the ambiguity sets (4.1) and (4.2) of diﬀerent radii holds:

(cid:102)Wε,p(Q1, . . . , QK; λ) ⊆ W2p·ε,p(Q1, . . . , QK; λ).

(4.3)

5

All proofs of this paper are deferred to Appendix C. Consequently, it is more feasible to use the Wasserstein
barycentric ambiguity set (4.2) since ε is often tuned in practice. We can therefore apply existing results of
WDRO if the Wasserstein barycenter Qλ exists and is available, without the need of redeveloping tools to
handle the ambiguity set (4.1).

Remark 4.3. The overall rationale of the construction of (4.2) is that the most adverse distribution should
be close to a population p-Wasserstein barycenter b(cid:63) := bp(P(cid:63)) ∈ Pp(Ξ) of the unknown true distribution
(of distributions) P(cid:63) ∈ Wp(Pp(Ξ)) within a radius ε in Wp distance. This population barycenter can be
approximated by its empirical counterpart (see Section 4.1 for details). This is as opposed to the usual WDRO
in which the most adverse distribution is close to the unknown true distribution P(cid:63) ∈ Pp(Ξ) approximated
by a nominal (empirical) distribution (cid:98)P.

We now revisit the WDRO problem (2.4) with K available nominal distributions (cid:98)P = ((cid:98)P1, . . . , (cid:98)PK) and the
Wasserstein barycentric ambiguity set Uε((cid:98)P) = Wε,p((cid:98)P; λ), which we refer to as the Wasserstein barycentric
DRO (WBDRO). Consequently, due to the deﬁnition of the Wasserstein barycentric ambiguity set (4.2),
solving a WBDRO involves two consecutive steps: (i) computing a Wasserstein barycenter (cid:98)bλ,p((cid:98)P1, . . . , (cid:98)PK);
(ii) solving a WDRO problem. Such a problem decomposition enables us to leverage existing theoretical
results and computational tools for Wasserstein barycenters (e.g., ´Alvarez-Esteban et al., 2016; Carlier et al.,
2015; Chewi et al., 2020; Heinemann et al., 2022) and WDRO (e.g., Blanchet et al., 2021d) respectively.
In the original WDRO formulation (2.4), we usually observe some i.i.d. realizations D := {zi}n

i=1 of the
unknown true distribution P(cid:63), from which we can construct a nominal distribution (cid:98)Pn, e.g., the empirical
distribution (cid:98)Pn = 1
i=1 δzi. However, in WBDRO, the way of constructing the K nominal distributions
n
(cid:98)P1, . . . , (cid:98)PK as approximations of their corresponding unknown true distributions P(cid:63)
K becomes more
subtle. For instance, they can be constructed from the same set of observed data Dn via resampling techniques
such as bootstrap (requiring K (cid:54) n), or from K diﬀerent sources where the observed data at the kth source
Dk = {zi,k}n
k (assuming the same sample size n
for simplicity). The former scenario is often used to avoid overﬁtting, while the latter is often encountered
in the setting of federated learning with possibly heterogeneous data sources. Again, for each k ∈
, the
K
(cid:74)
(cid:75)
nominal distributions (cid:98)Pn
k converges to P(cid:63)
k in Wp
distance as n → ∞ (see e.g., Bolley et al., 2007; Fournier and Guillin, 2015).

k can be taken as the empirical distributions. Note that (cid:98)Pn

i=1 are i.i.d. realizations of its unknown true distribution P(cid:63)

1, . . . , P(cid:63)

(cid:80)n

Under this construction, we are interested in statistical properties of WBDRO in the following two case:
(i) n → ∞ with ﬁxed and ﬁnite K; (ii) K → ∞ with ﬁxed and ﬁnite n. We argue that both cases are well
motivated by the two statistical frameworks with applications discussed in Boissard et al. (2015); Le Gouic
and Loubes (2017).

4.1 Two Statistical Paradigms

Asymptotic Sample Size. Under this paradigm, there are K unknown true distributions P(cid:63)
K ∈
Pp(Ξ), where K ∈ N∗ is ﬁnite and ﬁxed. These are approximated by a sequence of distributions constructed
(cid:80)n
k := 1
from data samples, e.g., the empirical measures (cid:98)Pn
. A crucial result in this case is
n
(cid:75)
λ := (cid:80)K
that, for λ ∈ (cid:52)K, a p-Wasserstein barycenter of (cid:98)ρn
(if exists) converges to a p-Wasserstein
barycenter of the limit ρλ := (cid:80)K
3).

in Wp distance as n → ∞, by Le Gouic and Loubes (2017, Theorem

i=1 δzi,k , k ∈
k=1 λkδ

1, . . . , P(cid:63)

k=1 λkδ

(cid:98)Pn
k

(cid:98)P(cid:63)
k

K

(cid:74)

Asymptotic Number of Data Sources. For now, we consider the case where the unknown true distribution
P(cid:63) ∈ Wp(Pp(Ξ)) is approximated by a growing discrete distribution ρK supported on K elements, with
K → ∞. Consider a sequence of K distributions Pk ∈ Pp(Ξ) with weights λK
,
K
k
(cid:74)
(cid:75)
from which we deﬁne the sequence of distributions by ρK := (cid:80)K
k δPk , where K ∈ N∗. Assume that ρK
converges to some distribution P(cid:63) in Wp distance. Then the p-Wasserstein barycenter of ρK converges to the
p-Wasserstein barycenter of P(cid:63) in Wp distance as K → ∞, by Le Gouic and Loubes (2017, Theorem 3).

(cid:62) 0 for each k ∈

k=1 λK

Remark 4.4. The case of n and K both growing to inﬁnity is even more of interest to our case. Indeed, since
k → P(cid:63)
(cid:98)Pn
k for

, using the above argument with Pk replaced by P(cid:63)

k in Wp distance as n → ∞, for each k ∈

K

(cid:74)

(cid:75)

6

each k ∈
(cid:75)
in Wp distance as K → ∞.

K
(cid:74)

, the p-Wasserstein barycenter of (cid:80)K

k=1 λK

k δP(cid:63)

k

converges to the p-Wasserstein barycenter of P(cid:63)

Remark 4.5. Under some more speciﬁc frameworks, e.g., in deformation models (Allassonni`ere et al.,
2007, 2013), the 2-Wasserstein barycenter of (cid:98)P1, . . . , (cid:98)PK is a consistent estimate of P(cid:63), in the sense that
(cid:98)b1/K,2((cid:98)P1, . . . , (cid:98)PK) → P(cid:63) as K → ∞ in W2 distance. In the case with empirical observations available, as
both n → ∞ and K → ∞, (cid:98)ρn,K := 1
→ P(cid:63) in W2 distance. We refer to Boissard et al. (2015,
Theorem 4.2 and Proposition 5.1) and Bigot and Klein (2018); Zemel and Panaretos (2019) for details.

k=1 δ

(cid:80)K

(cid:98)Pn
k

K

4.2 Performance Guarantees

We now study the implications of the two statistical paradigms in Section 4.1 on performance guarantees of
WBDRO. To simplify discussion, we only consider the case of p = 2 and equal weights, i.e., λk = 1/K for each
. To simplify discussion, we also assume that all 2-Wasserstein barycenters exist (subject to some
k ∈
technical regularity conditions). In this subsection, with slight abuse of notation, we write (cid:98)P = ((cid:98)Pk)k∈
,
(cid:98)Pn = ((cid:98)Pn

, and (cid:98)P(cid:63) = ((cid:98)P(cid:63)

K
(cid:74)

K

(cid:75)

(cid:74)

.

(cid:75)

k )k∈

k)k∈

The ﬁnite-sample guarantees of WBDRO are derived by the rates of convergence of Wasserstein barycenters
(Ahidar-Coutrix et al., 2020; Le Gouic et al., 2021; Sch¨otz, 2019), characterized by measure concentration of
the 2-Wasserstein barycenter of the nominal distributions (cid:98)Pn

K
(cid:74)

(cid:75)

K
(cid:74)

(cid:75)

With data samples Dn,k = {zi,k}n

i=1 δzi,k .
K
(cid:74)
For now, we let K ∈ N∗ be ﬁnite and ﬁxed. The following measure concentration result simpliﬁed from Le Gouic
et al. (2021, Theorem 12) states that the 2-Wasserstein barycenter of sub-Gaussian (cid:98)ρ(cid:63)
should
be contained in the 2-Wasserstein barycentric ambiguity set centered at the 2-Wasserstein barycenter of (cid:98)ρn,K
in WBDRO with high probability.

i=1 for each k ∈

where (cid:98)Pn

K := 1
K

k := 1
n

k=1 δ

k=1 δ

(cid:80)K

(cid:98)Pn
k

(cid:98)P(cid:63)
k

K

(cid:75)

1 , . . . , (cid:98)Pn
K.
, let (cid:98)ρn,K := 1

(cid:80)K

(cid:80)n

K ∈ W2(P2(Ξ))
Theorem 4.6 (Concentration inequality). Let K ∈ N∗ be ﬁnite and ﬁxed. Suppose that (cid:98)ρ(cid:63)
is sub-Gaussian with 2-Wasserstein barycenter (cid:98)b(cid:63)
K := b2((cid:98)ρ(cid:63)
K is unique
and there exist constants (c1, c2) ∈ (0, +∞)2 independent of n such that for any β ∈ (0, 1), the concentration
inequality

K) = (cid:98)b1/K,2((cid:98)P(cid:63)) ∈ P2(Ξ). Then (cid:98)b(cid:63)

Pn(cid:110)

(cid:111)
K ∈ Wε,2((cid:98)Pn; 1/K)
(cid:98)b(cid:63)

(cid:62) 1 − β − e−c2n

holds whenever ε exceeds

εn(β) =

(cid:115)

c1
n

log

(cid:19)
.

(cid:18) 2
β

(4.4)

Theorem 4.6 indicates that any 2-Wasserstein barycentric ambiguity set Wε((cid:98)Pn; 1/K) with radius ε (cid:62) εn(β)
K, which is a K-sample approximation of the

represents an approximate (1 − β)-conﬁdence region for (cid:98)b(cid:63)
2-Wasserstein barycenter of the unknown true distribution P(cid:63).

Note that the distributional uncertainty radius εn(β) decays as O(n−1/2). Therefore, there is no curse of
dimensionality in the uncertainty dimension m when choosing the distributional uncertainty radius εn(β), as
opposed to the case of WDRO (see Kuhn et al., 2019, §3 and Shaﬁeezadeh-Abadeh et al., 2019, Remark 37).

From Theorem 4.6, we can immediately derive the following ﬁnite-sample guarantee.

Theorem 4.7 (Finite-sample guarantee). Suppose that all conditions of Theorem 4.6 hold with εn(β) deﬁned
in (4.4). Then for all β ∈ (0, 1) and ε (cid:62) εn(β), we have
Pn(cid:110)

(cid:62) 1 − β − e−c2n.

(∀(cid:96) ∈ L) R

(cid:111)
((cid:96)) (cid:54) RWε,2((cid:98)Pn;1/K)((cid:96))

(cid:98)b(cid:63)
K

Theorem 4.7 asserts that the worst-case risk provides an upper conﬁdence bound on the approximate
true risk under the 2-Wasserstein barycenter of the unknown true distribution P(cid:63) uniformly across all loss
functions (cid:96) ∈ L. In particular, if we take (cid:96) to be an optimizer of RWε,2((cid:98)Pn;1/K), then this result implies that
the optimal value of WBDRO provides an upper conﬁdence bound on the out-of-sample performance of its
optimizers.

7

When K is ﬁxed and ﬁnite, we recall that εn → 0 as n → ∞. We can then derive asymptotic consistency
of WBDRO in the sample size n, which asserts that the solution of WBDRO converges to the worst-case
optimal risk under (cid:98)b(cid:63)
K with a suitably chosen β = βn → 0 decaying to 0 as n → ∞. On the other hand, let
us also recall the 2-Wasserstein barycenter (cid:98)b1/K,2((cid:98)P(cid:63)) converges to a 2-Wasserstein barycenter b2(P(cid:63)) of the
unknown true distribution P(cid:63) ∈ W2(P2(Ξ)) in W2 distance as K → ∞. Using this fact, we can also derive
the asymptotic consistency of WBDRO in the number of data sources K.

Theorem 4.8 (Asymptotic consistency). Suppose that all conditions of Theorem 4.6 hold. If K ∈ N∗ is
ﬁnite and ﬁxed, we can choose βn ∈ (0, 1) and εn = εn(βn) given in (4.4), n ∈ N∗, satisfying (cid:80)∞
n=1 βn < ∞
and limn→∞ εn(βn) = 0. If (cid:96) is upper semicontinuous and there exists C > 0 such that |(cid:96)(ξ)| (cid:54) C(1 + (cid:107)ξ(cid:107)2)
for all (cid:96) ∈ L and ξ ∈ Ξ, then we have for P∞-almost surely, as n → ∞,

RWεn(βn ),2((cid:98)Pn;1/K)(L) → R
(cid:98)b(cid:63)
K

(L).

Furthermore, suppose that P(cid:63) is sub-Gaussian with 2-Wasserstein barycenter b(cid:63) := b2(P(cid:63)). If we choose
γK ∈ (0, 1) and ωK = (cid:112)c3 log(2/γK)/K with c3 > 0 independent of K satisfying (cid:80)∞
K=1 γK < ∞ and
(L) → Rb(cid:63) (L) as K → ∞.
limK→∞ ωK(γK) = 0, then we have for P∞-almost surely, R

(cid:98)b(cid:63)
K

The second part of Theorem 4.8 implies that, if (cid:98)b(cid:63)

K (or (cid:98)P(cid:63)) is available, the solution of WBDRO converges

to the worst-case optimal risk under b(cid:63) with a suitably chosen γK decaying to 0 as K → ∞.

5 Gelbrich Ambiguity Set with 2-Wasserstein Barycenter

In WDRO, more precise structural assumptions about the nominal distribution (cid:98)P can be made, e.g., it belongs
to some family of distributions. In this section, we consider the case of (cid:98)P ∈ P2(Rm) with mean (cid:98)µ ∈ Rm
and covariance matrix (cid:98)Σ ∈ Sm
+ . While computing the Wasserstein distance between any two distributions is
NP-hard in general (Taskesen et al., 2021a), an analytical lower bound of their W2 distance is indeed given
by the Gelbrich distance (Gelbrich, 1990), which only involves their ﬁrst two moments.

Deﬁnition 5.1 (Gelbrich distance). For any P1, P2 ∈ P2(Rm) with means µ1, µ2 ∈ Rm and covariance
matrices Σ1, Σ2 ∈ Sm

++, their Gelbrich distance is deﬁned through

G((µ1, Σ1), (µ2, Σ2)) :=

(cid:113)

(cid:107)µ1 − µ2(cid:107)2

2 + B2(Σ1, Σ2),

(cid:16)
where B2(Σ1, Σ2) := tr(Σ1) + tr(Σ2) − 2 tr

Σ

1/2
1 Σ2Σ

1/2
1

(cid:17)1/2

(5.1)

is the squared Bures–Wasserstein distance (Bhatia et al., 2019).

For any P1, P2 ∈ P2(Rm), the Gelbrich bound (Gelbrich, 1990):

W2(P1, P2) (cid:62) G((µ1, Σ1), (µ2, Σ2))

holds, where equality holds if P1, P2 belongs to the same location-scatter family F(P0) for some P0 ∈ P ac
which is deﬁned as follows.

2 (Rm),

Deﬁnition 5.2 (Location-scatter family). Let X0 ∈ Rm be a random vector with Law(X0) = P0 ∈ P ac
2 (Rm).
The set F(P0) := {Law(AX0 + b) : A ∈ Sm
++, b ∈ Rm} of probability distributions induced by positive deﬁnite
aﬃne transformations from P0 is called a location-scatter family. Notably, location-scatter families encompass
the Gaussian and elliptical distributions (see e.g., Muzellec and Cuturi, 2018).

We then deﬁne the mean-covariance ambiguity set (Kuhn et al., 2019; Nguyen et al., 2021a) as a ball
centered at ((cid:98)µ, (cid:98)Σ) with radius ε (cid:62) 0 in terms of the Gelbrich distance by Vε((cid:98)µ, (cid:98)Σ) := {(µ, Σ) ∈ Rm × Sm
+ :
G((µ, Σ), ((cid:98)µ, (cid:98)Σ)) (cid:54) ε}. Next, we deﬁne the Gelbrich ambiguity set (Kuhn et al., 2019; Nguyen et al., 2021a,b),
which is the preimage of Vε((cid:98)µ, (cid:98)Σ) under the mean-covariance projection, through
(cid:111)
(cid:110)
Q ∈ P2(Ξ) : (EQ[ξ], VarQ(ξ)) ∈ Vε((cid:98)µ, (cid:98)Σ)
.

Gε((cid:98)µ, (cid:98)Σ) :=

(5.2)

8

Now we consider the setting of Section 4 with p = 2, and that all Qk’s belong to the same location-scatter
family. The 2-Wasserstein barycenter Qλ,2 also belongs to the same location-scatter family ( ´Alvarez-Esteban
et al., 2018, see Proposition B.7 for details).

Note that we can recover the Gelbrich ambiguity set centered at Qλ,2 if we impose further distributional

restrictions on the 2-Wasserstein barycentric ambiguity set.
2 (Rm) and Q1, . . . , QK ∈ F(P0) with means µ1, . . . , µK ∈ Rm and co-
Proposition 5.3. Let P0 ∈ P ac
variance matrices Σ1, . . . , ΣK ∈ Sm
++ respectively. For λ ∈ (cid:52)K, the λ-weighted 2-Wasserstein barycen-
ter of Q1, . . . , QK is given by Qλ,2 ∈ F(P0) with mean µλ ∈ Rm and covariance matrix Σλ ∈ Sm
++.
If the family of distributions in the 2-Wasserstein barycentric ambiguity set also belongs to F(P0), i.e,
Wls
ε,2(Q1, . . . , QK; λ) := (cid:8)P ∈ F(P0) : W2(P, Qλ,2) (cid:54) ε(cid:9), then this ambiguity set equals the Gelbrich ambiguity
set (5.2) centered at (µλ, Σλ) restricted to F(P0), i.e.,

Wls

ε,2(Q1, . . . , QK; λ) = Gε(µλ, Σλ) ∩ F(P0).

Similar to WDRO (cf. Corollary B.6), we can derive the following (optimal) worst-case risk upper bounds

for 2-WBDRO with the Gelbrich risk (i.e., risk under the Gelbrich ambiguity set).

2 (Ξ) has mean µk ∈ Rm and covariance matrix Σk ∈ Sm

++ for each

Theorem 5.4. Assume that (cid:98)Pk ∈ P ac
k ∈

. Then, we have
(cid:75)
(∀(cid:96) ∈ L) RWε,2((cid:98)P1,...,(cid:98)PK ;λ)((cid:96)) (cid:54) RGε(µλ,Σλ)((cid:96))

K
(cid:74)

and RWε,2((cid:98)P1,...,(cid:98)PK ;λ)(L) (cid:54) RGε(µλ,Σλ)(L),

where µλ and Σλ are the mean and covariance matrix of (cid:98)bλ,2((cid:98)P1, . . . , (cid:98)PK) respectively.

The above risk bounds indeed reveal a trade-oﬀ between tractability and the use of available information.
While the worst-case Gelbrich risk minimization problem is more tractable, it uses merely information of the
nominal distributions up to their ﬁrst two moments and discards higher-order moment information.

6 Distributionally Robust Inverse Covariance Matrix Estimation

We demonstrate the proposed WBDRO via an example of sparse inverse covariance (precision) matrix
estimation with a Wasserstein barycentric ambiguity set for a Gaussian random vector ξ ∈ Rm with covariance
matrix Σ ∈ Sm
++, where n independent samples are obeserved for each of the K possibly heterogeneous data
sources. Estimation of precision matrices is of more interest than that of covariance matrices since it ﬁnds
various applications to, e.g., mean-variance portfolio optimization and linear discriminant analysis. However,
the sample covariance matrix (cid:98)Σ is usually rank-deﬁcient when m > n even if Σ has full rank, so na¨ıvely
inverting (cid:98)Σ to obtain a meaningful precision matrix estimator is not viable.

For simplicity, we assume that the unknown true distribution P(cid:63) has zero mean. In this case the precision

matrix is usually estimated via maximum likelihood estimation (MLE) by minimizing

f (X) := − log det X +

1
n

n
(cid:88)

(cid:104)zi, Xzi(cid:105)

i=1

++ with n independent samples {zi}n

i=1. However, this MLE problem is unbounded for n (cid:54) m. Nguyen
over Sm
et al. (2022) alleviate this issue by incorporating distributional robustness using a 2-Wasserstein ambiguity
set centered at the nominal distribution N(0, (cid:98)Σ), leading to the Wasserstein Shrinkage Estimator (WSE),
which can be solved in a quasi-closed form.

With observed data from K data sources {zi,k}i∈
(cid:74)

, it is unclear how to construct a common
estimator using aggregate information from them even in the low-dimensional regime using the MLE approach
other than a simple weighted average.
In view of this, we propose the use of WBDRO to construct a
distributionally robust aggregate estimator. We consider the Bures–Wasserstein ambiguity set centered at
1 , . . . , (cid:98)Pn
the λ-weighted 2-Wasserstein barycenter Σλ of (cid:98)Pn
k ) with empirical covariance
(cid:98)Σn
i,k for each k ∈

K, where (cid:98)Pn
. The Bures–Wasserstein ambiguity set is deﬁned by
(cid:75)

k = N(0, (cid:98)Σn

i=1 zi,kz(cid:62)

k = 1
n

,k∈
(cid:74)
(cid:75)

(cid:80)n

K

K

n

(cid:75)

(cid:74)
Bε((cid:98)Σ) := {Q ∼ N(0, Σ) : B(Σ, (cid:98)Σ) (cid:54) ε}.

9

Note that Σλ also coincides the λ-weighted Bures–Wasserstein barycenter (Kroshnin et al., 2021b) of
(cid:98)Σ1, . . . , (cid:98)ΣK (see Appendix B.5). The distributionally robust maximum likelihood estimation (DRMLE)
problem can hence be formulated as

(cid:40)

minimize
X∈Sm
+

− log det X + sup

Eξ∼P[(cid:104)ξ, Xξ(cid:105)]

P∈Bε(Σλ)

(cid:41)
.

(6.1)

Note that the population Wasserstein barycenter of P(cid:63) ∈ W2(P2(Rm)) is also Gaussian since any location-
scatter family (which includes Gaussian) is closed for Wasserstein barycenters ( ´Alvarez-Esteban et al., 2018).
The problem (6.1) is indeed equivalent to a WDRO problem with a Wasserstein ambiguity set centered
at the Wasserstein barycenter N(0, Σλ), which also admits an analytical solution and is referred to as the
Wasserstein Barycentric Shrinkage Estimator (WBSE). Further details are given in Appendix D.

Simulations. We compare WBSE with two other estimators constructed from widely used precision matrix
estimators for single data source, namely linear shrinkage (LS) and L1-regularized maximum likelihood
estimators (L1). We choose m = 20, λ = 1/K, n ∈ {50, 100, 200}, K ∈ {25, 50, 100}, in order to observe the
eﬀects of both the sample size n and the number of data sources K on each estimator. We generate K sparse
matrices in Sm
k . The true covariance matrix
Σ(cid:63) is approximated by the Bures–Wasserstein barycenter of another 1000 samples of Σk(cid:48). Then the true
i=1 from each of N(0, Σk) to construct the
precision matrix is X (cid:63) = (Σ(cid:63))−1. We then generate samples {zi,k}n
empirical covariance matrices (cid:98)Σk, which are used to compute Σλ and construct the three estimators. See
Appendix D for additional details.

++ with sparsity level s = 50% as the true precision matrices Σ−1

We measure performance of estimators using the Stein loss L( (cid:98)X, Σ(cid:63)) := − log det( (cid:98)XΣ(cid:63)) + tr( (cid:98)X (cid:62)Σ(cid:63)) − m,
which vanishes if (cid:98)X = (Σ(cid:63))−1. The losses of the estimators are given in Table 1, averaged over 20 independent
trials. We observe that WBSE outperforms the other two estimators by a large margin. The performance of
WBSE also improves as n and K increase.

Table 1: Stein losses of LS, L1 and WBSE.

n

50

100

200

K

25
50
100

25
50
100

25
50
100

LS

L1

WBSE

6.77 ± 0.58
6.81 ± 0.43
6.91 ± 0.29

6.72 ± 0.49
6.76 ± 0.33
6.90 ± 0.27

6.68 ± 0.56
6.72 ± 0.36
6.87 ± 0.29

7.66 ± 0.63
7.72 ± 0.46
7.84 ± 0.31

7.61 ± 0.53
7.67 ± 0.35
7.83 ± 0.29

7.57 ± 0.60
7.63 ± 0.38
7.79 ± 0.31

1.77 ± 0.30
1.27 ± 0.20
0.99 ± 0.14

1.74 ± 0.27
1.32 ± 0.19
1.12 ± 0.16

1.76 ± 0.28
1.34 ± 0.21
0.62 ± 0.18

Despite being motivated by the high-dimensional setting, the proposed WBS estimator is not feasible when
n < m since all (cid:98)Σk’s are singular and their Bures–Wasserstein barycenter Σλ does not exist, as opposed to
the applicability of WSE. A possible remedy is to consider the entropic-regularized variants of the barycenter
(Bigot et al., 2019c; Carlier et al., 2021; Janati et al., 2020b; Mallasto et al., 2021; Minh, 2022). We use the
Sinkhorn barycenter (see Appendix B) and give additional simulation results under the high-dimensional
setting in Appendix D.

7 Concluding Remarks

In this paper, we propose the use of Wasserstein barycenter in the construction of ambiguity sets in WDRO
to aggregate data samples from multiple sources. In addition to the performance guarantees established in

10

this paper, extending the statistical analysis (Bartl et al., 2021; Blanchet and Kang, 2021; Blanchet et al.,
2021b,c) and generalization bounds (An and Gao, 2021) for WDRO to WBDRO are also important research
directions. Motivated by computational tractability and diﬀerent use cases, alternative barycenters based on
other optimal transport distances can also be considered (Bigot et al., 2019c; Bonneel et al., 2015; Carlier
et al., 2021; Cazelles et al., 2021; Friesecke et al., 2021; Janati et al., 2020a; Kim and Pass, 2018; Li et al.,
2020; Peyr´e et al., 2016). The same applies to the choice of discrepancy between probability distributions in
the ambiguity set in WBDRO (Azizian et al., 2022; Wang et al., 2021b). Finally, it is also interesting to
build more general machine learning applications upon the general framework of WBDRO.

References

Anshul Adve and Alp´ar M´esz´aros. On nonexpansiveness of metric projection operators on Wasserstein spaces.

arXiv preprint arXiv:2009.01370, 2020.

Martial Agueh and Guillaume Carlier. Barycenters in the Wasserstein space. SIAM Journal on Mathematical

Analysis, 43(2):904–924, 2011.

Adil Ahidar-Coutrix, Thibaut Le Gouic, and Quentin Paris. Convergence rates for empirical barycenters in
metric spaces: curvature, convexity and extendable geodesics. Probability Theory and Related Fields, 177
(1):323–368, 2020.

St´ephanie Allassonni`ere, Yali Amit, and Alain Trouv´e. Towards a coherent statistical framework for dense
deformable template estimation. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
69(1):3–29, 2007.

St´ephanie Allassonni`ere, J´er´emie Bigot, Joan Alexis Glaun`es, Florian Maire, and Fr´ed´eric J.P. Richard.
Statistical models for deformable templates in image and shape analysis. Annales math´ematiques Blaise
Pascal, 20(1):1–35, 2013.

Jason M. Altschuler and Enric Boix-Adser`a. Wasserstein barycenters are NP-hard to compute. SIAM Journal

on Mathematics of Data Science, 4(1):179–203, 2022.

Pedro C. ´Alvarez-Esteban, E. Del Barrio, J.A. Cuesta-Albertos, and C. Matr´an. A ﬁxed-point approach to
barycenters in Wasserstein space. Journal of Mathematical Analysis and Applications, 441(2):744–762,
2016.

Pedro C. ´Alvarez-Esteban, Eustasio del Barrio, Juan A. Cuesta-Albertos, and Carlos Matr´an. Wide consensus
aggregation in the Wasserstein space. Application to location-scatter families. Bernoulli, 24(4A):3147–3179,
2018.

Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar´e. Gradient Flows: In Metric Spaces and in the Space of

Probability Measures. Lectures in Mathematics ETH Z¨urich. Birkh¨auser Basel, 2005.

Luigi Ambrosio, Elia Bru´e, and Daniele Semola. Lectures on Optimal Transport, volume 130 of UNITEXT.

Springer, 2021.

Yang An and Rui Gao. Generalization bounds for (Wasserstein) robust optimization. Advances in Neural

Information Processing Systems (NeurIPS), 2021.

Wa¨ıss Azizian, Franck Iutzeler, and J´erˆome Malick. Regularization for Wasserstein distributionally robust

optimization. arXiv preprint arXiv:2205.08826, 2022.

Julio Backhoﬀ-Veraguas, Joaquin Fontbona, Gonzalo Rios, and Felipe Tobar. Bayesian learning with

Wasserstein barycenters. arXiv preprint arXiv:1805.10833, 2018.

Daniel Bartl, Samuel Drapeau, Jan Ob(cid:32)l´oj, and Johannes Wiesel. Sensitivity analysis of Wasserstein dis-
tributionally robust optimization problems. Proceedings of the Royal Society A, 477(2256):20210176,
2021.

11

Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen. Robust
solutions of optimization problems aﬀected by uncertain probabilities. Management Science, 59(2):341–357,
2013.

Espen Bernton, Promit Ghosal, and Marcel Nutz. Entropic optimal transport: Geometry and large deviations.

arXiv preprint arXiv:2102.04397, 2021.

Dimitris Bertsimas and Bart Van Parys. Bootstrap robust prescriptive analytics. Mathematical Programming,

2022.

Dimitris Bertsimas, Vishal Gupta, and Nathan Kallus. Data-driven robust optimization. Mathematical

Programming, 167(2):235–292, 2018.

Rajendra Bhatia, Tanvi Jain, and Yongdo Lim. On the Bures–Wasserstein distance between positive deﬁnite

matrices. Expositiones Mathematicae, 37(2):165–191, 2019.

J´er´emie Bigot and Thierry Klein. Characterization of barycenters in the Wasserstein space by averaging

optimal transport maps. ESAIM: Probability and Statistics, 22:35–57, 2018.

J´er´emie Bigot, Elsa Cazelles, and Nicolas Papadakis. Central limit theorems for entropy-regularized optimal
transport on ﬁnite spaces and statistical applications. Electronic Journal of Statistics, 13(2):5120–5150,
2019a.

J´er´emie Bigot, Elsa Cazelles, and Nicolas Papadakis. Data-driven regularization of Wasserstein barycenters
with an application to multivariate density registration. Information and Inference: A Journal of the IMA,
8(4):719–755, 2019b.

J´er´emie Bigot, Elsa Cazelles, and Nicolas Papadakis. Penalization of barycenters in the Wasserstein space.

SIAM Journal on Mathematical Analysis, 51(3):2261–2285, 2019c.

Adrian N. Bishop. Information fusion via the Wasserstein barycenter in the space of probability measures:
Direct fusion of empirical measures and Gaussian fusion with unknown correlation. In 17th International
Conference on Information Fusion (FUSION). IEEE, 2014.

Adrian N. Bishop and Arnaud Doucet. Network consensus in the Wasserstein metric space of probability

measures. SIAM Journal on Control and Optimization, 59(5):3261–3277, 2021.

Jose Blanchet and Yang Kang. Sample out-of-sample inference based on Wasserstein distance. Operations

Research, 69(3):985–1013, 2021.

Jose Blanchet and Nian Si. Optimal uncertainty size in distributionally robust inverse covariance estimation.

Operations Research Letters, 47(6):618–621, 2019.

Jose Blanchet, Yang Kang, and Karthyek Murthy. Robust Wasserstein proﬁle inference and applications to

machine learning. Journal of Applied Probability, 56(3):830–857, 2019a.

Jose Blanchet, Yang Kang, Karthyek Murthy, and Fan Zhang. Data-driven optimal transport cost selection
for distributionally robust optimization. In Proceedings of the Winter Simulation Conference (WSC). IEEE,
2019b.

Jose Blanchet, Lin Chen, and Xun Yu Zhou. Distributionally robust mean-variance portfolio selection with

Wasserstein distances. Management Science, 2021a.

Jose Blanchet, Karthyek Murthy, and Viet Anh Nguyen. Statistical analysis of Wasserstein distributionally
robust estimators. In Tutorials in Operations Research: Emerging Optimization Methods and Modeling
Techniques with Applications, pages 227–254. INFORMS, 2021b.

Jose Blanchet, Karthyek Murthy, and Nian Si. Conﬁdence regions in Wasserstein distributionally robust

estimation. Biometrika, 2021c.

12

Jose Blanchet, Karthyek Murthy, and Fan Zhang. Optimal transport-based distributionally robust optimiza-

tion: Structural properties and iterative schemes. Mathematics of Operations Research, 2021d.

Emmanuel Boissard, Thibaut Le Gouic, and Jean-Michel Loubes. Distribution’s template estimate with

Wasserstein metrics. Bernoulli, 21(2):740–759, 2015.

Fran¸cois Bolley, Arnaud Guillin, and C´edric Villani. Quantitative concentration inequalities for empirical

measures on non-compact spaces. Probability Theory and Related Fields, 137(3-4):541–593, 2007.

Nicolas Bonneel, Julien Rabin, Gabriel Peyr´e, and Hanspeter Pﬁster. Sliced and Radon Wasserstein

barycenters of measures. Journal of Mathematical Imaging and Vision, 51(1):22–45, 2015.

Guillaume Carlier, Adam Oberman, and Edouard Oudet. Numerical methods for matching for teams and
Wasserstein barycenters. ESAIM: Mathematical Modelling and Numerical Analysis, 49(6):1621–1642, 2015.

Guillaume Carlier, Katharina Eichinger, and Alexey Kroshnin. Entropic-Wasserstein barycenters: PDE
characterization, regularity, and CLT. SIAM Journal on Mathematical Analysis, 53(5):5880–5914, 2021.

Yair Carmon and Danielle Hausler. Distributionally robust optimization via ball oracle acceleration. arXiv

preprint arXiv:2203.13225, 2022.

Elsa Cazelles, Felipe Tobar, and Joaquin Fontbona. A novel notion of barycenter for probability distributions
based on optimal weak mass transport. In Advances in Neural Information Processing Systems (NeurIPS),
2021.

Sinho Chewi, Tyler Maunu, Philippe Rigollet, and Austin J. Stromme. Gradient descent algorithms for
Bures-Wasserstein barycenters. In Proceedings of the Conference on Learning Theory (COLT), 2020.

L´ena¨ıc Chizat, Pierre Roussillon, Flavien L´eger, Fran¸cois-Xavier Vialard, and Gabriel Peyr´e. Faster Wasserstein
distance estimation with the Sinkhorn divergence. In Advances in Neural Information Processing Systems
(NeurIPS), 2020.

Kai Lai Chung. A Course in Probability Theory. Academic Press, 3rd edition, 2001.

Corinna Cortes, Mehryar Mohri, Dmitry Storcheus, and Ananda Theertha Suresh. Boosting with multiple

sources. Advances in Neural Information Processing Systems (NeurIPS), 2021.

Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural

Information Processing Systems (NeurIPS), 2013.

Marco Cuturi and Arnaud Doucet. Fast computation of Wasserstein barycenters. In Proceedings of the

International Conference on Machine Learning (ICML), 2014.

Marco Cuturi and Gabriel Peyr´e. Semidual regularized optimal transport. SIAM Review, 60(4):941–965,

2018.

Chiheb Daaloul, Thibaut Le Gouic, Jacques Liandrat, and Magali Tournus. Sampling from the Wasserstein

barycenter. arXiv preprint arXiv:2105.01706, 2021.

Morris H. DeGroot. Reaching a consensus. Journal of the American Statistical Association, 69(345):118–121,

1974.

Erick Delage and Yinyu Ye. Distributionally robust optimization under moment uncertainty with application

to data-driven problems. Operations Research, 58(3):595–612, 2010.

Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Distributionally robust federated averaging.

In Advances in Neural Information Processing Systems (NeurIPS), 2020.

John C. Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally

robust optimization. The Annals of Statistics, 49(3):1378–1406, 2021.

13

John C. Duchi, Peter W. Glynn, and Hongseok Namkoong. Statistics of robust optimization: A generalized

empirical likelihood approach. Mathematics of Operations Research, 46(3):946–969, 2021.

Pavel Dvurechenskii, Darina Dvinskikh, Alexander Gasnikov, Cesar Uribe, and Angelia Nedich. Decentralize
and randomize: Faster algorithm for Wasserstein barycenters. In Advances in Neural Information Processing
Systems (NeurIPS), 2018.

Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization using the
wasserstein metric: Performance guarantees and tractable reformulations. Mathematical Programming, 171
(1-2):115–166, 2018.

Jean Feydy, Thibault S´ejourn´e, Fran¸cois-Xavier Vialard, Shun-ichi Amari, Alain Trouv´e, and Gabriel Peyr´e.
Interpolating between optimal transport and mmd using sinkhorn divergences. In Proceedings of the
International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2019.

Alessio Figalli and Federico Glaudo. An Invitation to Optimal Transport, Wasserstein Distances, and Gradient

Flows. EMS Textbooks in Mathematics. EMS Press, Z¨urich, 2021.

Nicolas Fournier and Arnaud Guillin. On the rate of convergence in Wasserstein distance of the empirical

measure. Probability Theory and Related Fields, 162(3):707–738, 2015.

Gero Friesecke, Daniel Matthes, and Bernhard Schmitzer. Barycenters for the Hellinger–Kantorovich distance

over Rd. SIAM Journal on Mathematical Analysis, 53(1):62–110, 2021.

Rui Gao. Finite-sample guarantees for Wasserstein distributionally robust optimization: Breaking the curse

of dimensionality. arXiv preprint arXiv:2009.04382, 2020.

Rui Gao and Anton J. Kleywegt. Distributionally robust stochastic optimization with Wasserstein distance.

arXiv preprint arXiv:1604.02199, 2016.

Rui Gao, Xi Chen, and Anton J. Kleywegt. Wasserstein distributionally robust optimization and variation

regularization. arXiv preprint arXiv:1712.06050, 2017.

Matthias Gelbrich. On a formula for the L2 Wasserstein metric between measures on Euclidean and Hilbert

spaces. Mathematische Nachrichten, 147(1):185–203, 1990.

Aude Genevay, Gabriel Peyr´e, and Marco Cuturi. Learning generative models with Sinkhorn divergences. In
Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2018.

Aude Genevay, L´ena¨ıc Chizat, Francis Bach, Marco Cuturi, and Gabriel Peyr´e. Sample complexity of
Sinkhorn divergences. In Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS), 2019.

Promit Ghosal, Marcel Nutz, and Espen Bernton. Stability of entropic optimal transport and Schr¨odinger

bridges. arXiv preprint arXiv:2106.03670, 2021.

Joel Goh and Melvyn Sim. Distributionally robust optimization and its tractable approximations. Operations

Research, 58(4):902–917, 2010.

Ziv Goldfeld, Kengo Kato, Gabriel Rioux, and Ritwik Sadhu. Statistical inference with regularized optimal

transport. arXiv preprint arXiv:2205.04283, 2022.

Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Amin Karbasi. Learning distri-
butionally robust models at scale via composite optimization. In International Conference on Learning
Representations (ICLR), 2022.

Charles R. Harris, K. Jarrod Millman, St´efan J. van der Walt, Ralf Gommers, Pauli Virtanen, David
Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus,
Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern´andez del R´ıo, Mark
Wiebe, Pearu Peterson, Pierre G´erard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer
Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):
357–362, 2020.

14

Florian Heinemann, Axel Munk, and Yoav Zemel. Randomized Wasserstein barycenter computation:
Resampling with statistical guarantees. SIAM Journal on Mathematics of Data Science, 4(1):229–259,
2022.

Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervised learning
give robust classiﬁers? In Proceedings of the International Conference on Machine Learning (ICML), 2018.

Hicham Janati, Marco Cuturi, and Alexandre Gramfort. Debiased Sinkhorn barycenters. In Proceedings of

the International Conference on Machine Learning (ICML), 2020a.

Hicham Janati, Boris Muzellec, Gabriel Peyr´e, and Marco Cuturi. Entropic optimal transport between
unbalanced Gaussian measures has a closed form. In Advances in Neural Information Processing Systems
(NeurIPS), 2020b.

Jikai Jin, Bohang Zhang, Haiyang Wang, and Liwei Wang. Non-convex distributionally robust optimization:

Non-asymptotic analysis. Advances in Neural Information Processing Systems (NeurIPS), 2021.

Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Hubert
Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adri`a Gasc´on, Badih Ghazi,
Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson,
Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konecn´y, Aleksandra Korolova,
Farinaz Koushanfar, Sanmi Koyejo, Tancr`ede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard
Nock, Ayfer ¨Ozg¨ur, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova, Dawn
Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tram`er, Praneeth
Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances
and open problems in federated learning. Foundations and Trends® in Machine Learning, 14(1–2):1–210,
2021.

Young-Heon Kim and Brendan Pass. A canonical barycenter via Wasserstein regularization. SIAM Journal

on Mathematical Analysis, 50(2):1817–1828, 2018.

Martin Knott and Cyril S. Smith. On a generalization of cyclic monotonicity and distances among random

vectors. Linear Algebra and Its Applications, 199:363–371, 1994.

Alexey Kroshnin. Fr´echet barycenters in the Monge-Kantorovich spaces. Journal of Convex Analysis, 25(4):

1371–1395, 2018.

Alexey Kroshnin, Vladimir Spokoiny, and Alexandra Suvorikova. Multiplier bootstrap for Bures-Wasserstein

barycenters. arXiv preprint arXiv:2111.12612, 2021a.

Alexey Kroshnin, Vladimir Spokoiny, and Alexandra Suvorikova. Statistical inference for Bures–Wasserstein

barycenters. The Annals of Applied Probability, 31(3):1264–1298, 2021b.

Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh Shaﬁeezadeh-Abadeh. Wasserstein
distributionally robust optimization: Theory and applications in machine learning. In Operations Research
& Management Science in the Age of Analytics, pages 130–166. INFORMS, 2019.

Long Tan Le, Josh Nguyen, Canh T. Dinh, and Nguyen Hoang Tran. On the generalization of Wasserstein

robust federated learning, 2022. URL https://openreview.net/forum?id=nWprF5r2spe.

Thibaut Le Gouic and Jean-Michel Loubes. Existence and consistency of Wasserstein barycenters. Probability

Theory and Related Fields, 168(3):901–917, 2017.

Thibaut Le Gouic, Quentin Paris, Philippe Rigollet, and Austin J. Stromme. Fast convergence of empirical
barycenters in Alexandrov spaces and the Wasserstein space. arXiv preprint arXiv:1908.00828v4, 2021.

Daniel Levy, Yair Carmon, John C. Duchi, and Aaron Sidford. Large-scale methods for distributionally

robust optimization. In Advances in Neural Information Processing Systems (NeurIPS), 2020.

15

Lingxiao Li, Aude Genevay, Mikhail Yurochkin, and Justin M. Solomon. Continuous regularized Wasserstein

barycenters. In Advances in Neural Information Processing Systems (NeurIPS), 2020.

Mengmeng Li, Tobias Sutter, and Daniel Kuhn. Distributionally robust optimization with Markovian data.

In Proceedings of the International Conference on Machine Learning (ICML), 2021.

Giulia Luise, Saverio Salzo, Massimiliano Pontil, and Carlo Ciliberto. Sinkhorn barycenters with free support

via Frank-Wolfe algorithm. Advances in Neural Information Processing Systems (NeurIPS), 2019.

Anton Mallasto, Augusto Gerolin, and H`a Quang Minh. Entropy-regularized 2-Wasserstein distance between

Gaussian measures. Information Geometry, 2021.

Yishay Mansour, Mehryar Mohri, Jae Ro, Ananda Theertha Suresh, and Ke Wu. A theory of multiple-source
adaptation with limited target labeled data. In Proceedings of the International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), 2021.

H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag¨uera y Arcas.
Communication-eﬃcient learning of deep networks from decentralized data. In Proceedings of the Interna-
tional Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2017.

H`a Quang Minh. Entropic regularization of Wasserstein distance between inﬁnite-dimensional Gaussian

measures and Gaussian processes. Journal of Theoretical Probability, pages 1–96, 2022.

Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Proceedings of

the International Conference on Machine Learning (ICML), 2019.

Boris Muzellec and Marco Cuturi. Generalizing point embeddings using the Wasserstein space of elliptical

distributions. In Advances in Neural Information Processing Systems (NeurIPS), 2018.

Viet Anh Nguyen, Soroosh Shaﬁeezadeh-Abadeh, Damir Filipovi´c, and Daniel Kuhn. Mean-covariance robust

risk measurement. arXiv preprint arXiv:2112.09959, 2021a.

Viet Anh Nguyen, Soroosh Shaﬁeezadeh-Abadeh, Daniel Kuhn, and Peyman Mohajerin Esfahani. Bridging
Bayesian and minimax mean square error estimation via Wasserstein distributionally robust optimization.
Mathematics of Operations Research, 2021b.

Viet Anh Nguyen, Fan Zhang, Jose Blanchet, Erick Delage, and Yinyu Ye. Robustifying conditional portfolio

decisions via optimal transport. arXiv preprint arXiv:2103.16451, 2021c.

Viet Anh Nguyen, Daniel Kuhn, and Peyman Mohajerin Esfahani. Distributionally robust inverse covariance

estimation: The Wasserstein shrinkage estimator. Operations Research, 70(1):490–515, 2022.

Marcel Nutz. Introduction to Entropic Optimal Transport. 2021. URL https://www.math.columbia.edu/

˜mnutz/docs/EOT_lecture_notes.pdf. Lecture Notes, Columbia University.

Marcel Nutz and Johannes Wiesel. Entropic optimal transport: Convergence of potentials. Probability Theory

and Related Fields, pages 1–24, 2021.

Jan Ob(cid:32)l´oj and Johannes Wiesel. Distributionally robust portfolio maximization and marginal utility pricing

in one period ﬁnancial markets. Mathematical Finance, 31(4):1454–1493, 2021.

Victor M. Panaretos and Yoav Zemel. Statistical aspects of Wasserstein distances. Annual Review of Statistics

and Its Application, 6:405–431, 2019.

Victor M. Panaretos and Yoav Zemel. An Invitation to Statistics in Wasserstein Space. SpringerBriefs in

Probability and Mathematical Statistics. Springer Nature, 2020.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.

16

Gabriel Peyr´e and Marco Cuturi. Computational optimal transport. Foundations and Trends® in Machine

Learning, 11(5-6):355–607, 2019.

Gabriel Peyr´e, Marco Cuturi, and Justin Solomon. Gromov-Wasserstein averaging of kernel and distance

matrices. In Proceedings of the International Conference on Machine Learning (ICML), 2016.

Georg Pﬂug and David Wozabal. Ambiguity in portfolio selection. Quantitative Finance, 7(4):435–442, 2007.

Aaditya Ramdas, Nicol´as Garc´ıa Trillos, and Marco Cuturi. On Wasserstein two-sample testing and related

families of nonparametric tests. Entropy, 19(2):47, 2017.

Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated learning:
The case of aﬃne distribution shifts. In Advances in Neural Information Processing Systems (NeurIPS),
2020.

Jae Ro, Mingqing Chen, Rajiv Mathews, Mehryar Mohri, and Ananda Theertha Suresh. Communication-

eﬃcient agnostic federated averaging. arXiv preprint arXiv:2104.02748, 2021.

Ludger R¨uschendorf and Ludger Uckelmann. On the n-coupling problem. Journal of Multivariate Analysis,

81(2):242–258, 2002.

Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural
networks for group shifts: On the importance of regularization for worst-case generalization. In International
Conference on Learning Representations (ICLR), 2019.

Filippo Santambrogio. Optimal Transport for Applied Mathematicians: Calculus of Variations, PDEs, and
Modeling, volume 87 of Progress in Nonlinear Diﬀerential Equations and Their Applications. Birkh¨auser,
2015.

Morgan A. Schmitz, Matthieu Heitz, Nicolas Bonneel, Fred Ngole, David Coeurjolly, Marco Cuturi, Gabriel
Peyr´e, and Jean-Luc Starck. Wasserstein dictionary learning: Optimal transport-based unsupervised
nonlinear dictionary learning. SIAM Journal on Imaging Sciences, 11(1):643–678, 2018.

Christof Sch¨otz. Convergence rates for the generalized Fr´echet mean via the quadruple inequality. Electronic

Journal of Statistics, 13(2):4280–4345, 2019.

Soroosh Shaﬁeezadeh-Abadeh, Peyman Mohajerin Esfahani, and Daniel Kuhn. Distributionally robust logistic

regression. In Advances in Neural Information Processing Systems (NeurIPS), 2015.

Soroosh Shaﬁeezadeh-Abadeh, Viet Anh Nguyen, Daniel Kuhn, and Peyman Mohajerin Esfahani. Wasserstein
distributionally robust Kalman ﬁltering. In Advances in Neural Information Processing Systems (NeurIPS),
2018.

Soroosh Shaﬁeezadeh-Abadeh, Daniel Kuhn, and Peyman Mohajerin Esfahani. Regularization via mass

transportation. Journal of Machine Learning Research, 20(103):1–68, 2019.

Agnieszka S(cid:32)lowik and L´eon Bottou. On distributionally robust optimization and data rebalancing.
Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2022.

In

Sanvesh Srivastava, Cheng Li, and David B. Dunson. Scalable Bayes via barycenter in Wasserstein space.

Journal of Machine Learning Research, 19(1):312–346, 2018.

Matthew Staib and Stefanie Jegelka. Distributionally robust optimization and generalization in kernel

methods. Advances in Neural Information Processing Systems (NeurIPS), 2019.

Bahar Taskesen, Soroosh Shaﬁeezadeh-Abadeh, and Daniel Kuhn. Semi-discrete optimal transport: Hardness,

regularization and numerical solution. arXiv preprint arXiv:2103.06263, 2021a.

Bahar Taskesen, Man-Chung Yue, Jose Blanchet, Daniel Kuhn, and Viet Anh Nguyen. Sequential domain
adaptation by synthesizing distributionally robust experts. In Proceedings of the International Conference
on Machine Learning (ICML), 2021b.

17

C´edric Villani. Topics in Optimal Transportation, volume 58 of Graduate Studies in Mathematics. American

Mathematical Society, 2003.

C´edric Villani. Optimal Transport: Old and New, volume 338. Springer Science & Business Media, 2009.

Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni
Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St´efan J. van der Walt, Matthew Brett,
Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric
Larson, C. J. Carey, ˙Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold,
Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antˆonio H.
Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental
Algorithms for Scientiﬁc Computing in Python. Nature Methods, 17:261–272, 2020.

Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H. Brendan McMahan, Blaise Aguera y Arcas, Maruan
Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, Suhas Diggavi, Hubert
Eichner, Advait Gadhikar, Zachary Garrett, Antonious M. Girgis, Filip Hanzely, Andrew Hard, Chaoyang
He, Samuel Horvath, Zhouyuan Huo, Alex Ingerman, Martin Jaggi, Tara Javidi, Peter Kairouz, Satyen
Kale, Sai Praneeth Karimireddy, Jakub Konecny, Sanmi Koyejo, Tian Li, Luyang Liu, Mehryar Mohri,
Hang Qi, Sashank J. Reddi, Peter Richtarik, Karan Singhal, Virginia Smith, Mahdi Soltanolkotabi, Weikang
Song, Ananda Theertha Suresh, Sebastian U. Stich, Ameet Talwalkar, Hongyi Wang, Blake Woodworth,
Shanshan Wu, Felix X. Yu, Honglin Yuan, Manzil Zaheer, Mi Zhang, Tong Zhang, Chunxiang Zheng, Chen
Zhu, and Wennan Zhu. A ﬁeld guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021a.

Jie Wang, Rui Gao, and Yao Xie.

Sinkhorn distributionally robust optimization.

arXiv preprint

arXiv:2109.11926, 2021b.

Wolfram Wiesemann, Daniel Kuhn, and Melvyn Sim. Distributionally robust convex optimization. Operations

Research, 62(6):1358–1376, 2014.

Hongkang Yang and Esteban G. Tabak. Clustering, factor discovery and optimal transport. Information and

Inference: A Journal of the IMA, 10(4):1353–1387, 2021.

Yaodong Yu, Tianyi Lin, Eric Mazumdar, and Michael I. Jordan. Fast distributionally robust learning
with variance reduced min-max optimization. In Proceedings of the International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), 2022.

Honglin Yuan, Warren Richard Morningstar, Lin Ning, and Karan Singhal. What do we mean by generalization

in federated learning? In International Conference on Learning Representations (ICLR), 2022.

Yoav Zemel and Victor M. Panaretos. Fr´echet means and procrustes analysis in Wasserstein space. Bernoulli,

25(2):932–976, 2019.

Ningshan Zhang, Mehryar Mohri, and Judy Hoﬀman. Multiple-source adaptation theory and algorithms.

Annals of Mathematics and Artiﬁcial Intelligence, 89(3):237–270, 2021.

Jianzhe Zhen, Daniel Kuhn, and Wolfram Wiesemann. Mathematical foundations of robust and distributionally

robust optimization. arXiv preprint arXiv:2105.00760, 2021.

18

Appendix

A Other Related Work

In this section, we provide a more detailed discussion on the connections of the proposed framework to other
existing machine learning paradigms.

We now introduce some additional notation. In the following, we consider a supervised learning setting
i=1, the most notable

with the input space X and the output space Y. With n samples Dn = {(xi, yi)}n
framework for building machine learning models is the ERM formulation, which solves

minimize
θ∈Θ

E(x,y)∼(cid:98)P[(cid:96)(hθ(x), y)] =

1
n

n
(cid:88)

i=1

(cid:96)(hθ(xi), yi),

where hθ : X → Y represents the model with parameter θ ∈ Θ ⊆ Rd, (cid:96)(·, ·) is the loss function, and
(cid:98)P := 1
n

i=1 δ(xi,yi) is the empirical distribution of Dn.

(cid:80)n

Federated Learning. Following the discussion in Section 3 of the main text, federated learning (FL) and
its optimization formulation is a crucial motivation of the problem considered in this paper. The agnostic
federated learning (AFL) framework (Mohri et al., 2019; Ro et al., 2021) considers the worst-case setting
in which the learner seeks a solution that is favorable for any λ ∈ Λ ⊆ (cid:52)K, where Λ is a closed convex set.
Again, if we deﬁne the λ-mixture of distributions Pλ := (cid:80)K

k=1 λkPk, then the agnostic risk is given by

LPΛ (θ) := sup
λ∈Λ

E(x,y)∼Pλ [(cid:96)(hθ(x), y)].

In practice, only the empirical distributions (cid:98)Pk’s are accessible (constructed from ﬁnite samples), so we can
deﬁne the λ-mixture of empirical distributions Pλ := (cid:80)K
k=1 λk (cid:98)Pk. Then, the agnostic empirical risk is given
by

L

PΛ

(θ) := sup
λ∈Λ

E(x,y)∼Pλ

[(cid:96)(hθ(x), y)].

(A.1)

Two notable diﬀerences between AFL and our proposed framework are that in AFL the choice of λ is also
optimized, and the use of mixture distributions instead of Wasserstein barycenters. Following this line of
work, Deng et al. (2020) develop communication-eﬃcient distributed algorithms for minimizing the agnostic
empirical risk (A.1), whereas Reisizadeh et al. (2020) study the notion of robustness against aﬃne distribution
drifts in clients’ data in federated learning.

On the other hand, Wasserstein distributionally robust federated learning (WAFL; Le et al., 2022) shares
a very similar spirit to our work which considers a WDRO formulation under the federated learning setting,
but again with the mixture distribution (Euclidean barycenter) usually considered in FL instead of the notion
of (entropic-regularized) Wasserstein barycenters used in this work.
Remark A.1. In this work, we however do not aim at solving the FL problem, which requires distributed
and decentralized computations. Yet, it is very interesting to make our proposed paradigm amenable to the
full federated learning setting, which might require decentralized distributed computation of Wasserstein
barycenters (Dvurechenskii et al., 2018).

(Multiple-Source) Domain Adaptation.
In the multiple-source domain adaptation problem (see e.g., Man-
sour et al., 2021; Zhang et al., 2021, and references therein), each domain is deﬁned by the corresponding
distribution Pk. The target distribution could be assumed to be close to some convex combination of source
distributions, (cid:80)K
k=1 λkPk. The learner wants to learn a model on the target domain. Similar to the AFL
formulation, the learner can do this by solving

minimize
θ∈Θ

E(x,y)∼Pλ(cid:63) [(cid:96)(hθ(x), y)],

where λ(cid:63) := argminλ∈(cid:52)K E(λ) for some discrepancy measure E between the empirical target distribution
(cid:98)P0 and Pλ, e.g., a Bregman divergence B((cid:98)P0 || Pλ). Another related work Taskesen et al. (2021b) study a
distributionally robust formulation for supervised domain adaptation with scarce labeled target data.

19

Boosting. Also inspired by the agnostic loss in the AFL framework (Mohri et al., 2019), Cortes et al. (2021)
study boosting in the presence of multiple source domains. They put forward the so-called Q-ensembles,
which are convex combinations weighted by a domain classiﬁer Q. The typical assumption that the target
distribution is a mixture of the source distributions is also made. They also provide an algorithmic extension
to the federated learning scenario. Further related work can be found in Cortes et al. (2021).

Group DRO. Machine learning models might rely on spurious correlations—misleading heuristics which
hold for most training examples but are wrongly linked to the target. Thus, these models could suﬀer high
risk on minority groups where these correlations do not hold. The Group DRO framework (Hu et al., 2018;
Sagawa et al., 2019), which aims to obtain high performance across all groups, minimizes the worst-group
risk :

minimize
θ∈Θ

sup
P∈Q(P1,...,PK ;λ)

Eξ∼P[(cid:96)(hθ(x), y)],

(A.2)

where the ambiguity set is deﬁned as Q(P1, . . . , PK; λ) :=

(cid:110)(cid:80)K

k=1 λkPk : λ ∈ (cid:52)K(cid:111)
.

Note that (A.2) is equivalent to

minimize
θ∈Θ

sup
K
k∈
(cid:74)

(cid:75)

Eξ∼Pk [(cid:96)(hθ(x), y)].

Since in practice we only observe the empirical distributions (cid:98)Pk’s, we instead minimize the empirical
worst-group risk :

which can be rewritten as

minimize
θ∈Θ

sup
K
k∈
(cid:74)

(cid:75)

Eξ∼(cid:98)Pk

[(cid:96)(hθ(x), y)],

minimize
θ∈Θ

sup
λ∈(cid:52)K

(cid:40) K
(cid:88)

k=1

λkEξ∼(cid:98)Pk

[(cid:96)(hθ(x), y)] = Eξ∼Pλ

[(cid:96)(hθ(x), y)]

.

(cid:41)

This has an almost identical formulation to the AFL framework (A.1) above (when Λ = (cid:52)K). A recent work
Carmon and Hausler (2022) study an accelerated optimization method for Group DRO.

As a work close to Group DRO, S(cid:32)lowik and Bottou (2022) study the relation between solving a DRO
problem and optimizing the expected error for a single distribution constructed by the mixture distribution
(cid:80)K

k=1 λkPk for some λ ∈ (cid:52)K, particularly with nonconvex loss functions.

B Additional Technical Details and Results

B.1 Additional Notation and Deﬁnitions

(cid:75)

(cid:75)

d1

,j∈

d2
(cid:74)

and B := (bi,j)i∈

in Rd1×d2 , we denote the Frobenius
For any two matrices A := (ai,j)i∈
d1
,j∈
(cid:74)
(cid:75)
(cid:74)
(cid:80)d2
inner product of A and B by ⟪A, B⟫F := tr(A(cid:62)B) = (cid:80)d1
j=1 ai,jbi,j and the Frobenius norm of A by
|||A|||F := (cid:112)⟪A, A⟫F. The elementwise L1-norm of A is denoted by (cid:107)A(cid:107)1 := (cid:80)d1
j=1 |ai,j|. The Lebesgue
measure over X is denoted by LX. For two probability measures µ and ν on B(X), the relative entropy or the
Kullback–Leibler (KL) divergence from µ to ν is DKL(µ || ν) := (cid:82)
X log(dµ/dν) dµ if µ is absolutely continuous
w.r.t. ν (denoted by µ (cid:28) ν) with the Radon–Nikodym derivative dµ/dν and +∞ otherwise. The product
measure µ ⊗ ν ∈ P(Rd × Rd) of µ and ν is characterized by (µ ⊗ ν)(X × Y) = µ(X)ν(Y) for any pair of Borel
sets X, Y ⊂ Rd.

(cid:80)d2

d2
(cid:74)

i=1

i=1

(cid:75)

Deﬁnition B.1 (Sub-Gaussian measure). Let Ξ ⊆ Rm be a closed convex set. A probability measure
P ∈ Wp(Pp(Ξ)) is sub-Gaussian with variance proxy σ2 > 0 if

(cid:20)

Eρ∼P

exp

(cid:26) 1
2σ2 W2

p(bp(P), ρ)

(cid:27)(cid:21)

(cid:54) 2,

where ρ ∈ Pp(Ξ) is a random measure with distribution P.

20

B.2 Entropic Optimal Transport

Based on computational consideration, entropic regularization has been introduced to approximate Wasser-
stein distances. The so-called entropic(-regularized) optimal transport has aroused much theoretical and
computational interests across the ﬁelds of machine learning, statistics, economics, image processing, and
theoretical and applied probability. We refer to Bernton et al. (2021); Bigot et al. (2019a); Ghosal et al. (2021);
Goldfeld et al. (2022); Nutz (2021); Nutz and Wiesel (2021) for recent theoretical advances in probability and
statistics.

In the machine learning community, Cuturi (2013) proposes the so-called Sinkhorn distance (which is
indeed not a metric), which is referred to as the entropic-p-Wasserstein distance in this paper and is the
central object in entropic optimal transport. Let us recall that Ω ⊆ Rm is a closed convex set.

Deﬁnition B.2 (Entropic-Wasserstein distance). For σ > 0, the entropic-p-Wasserstein distance between
ρ, ν ∈ P(Ω) is deﬁned by

OTω1,ω2
p,σ

(ρ, ν) := inf

π∈Π(ρ,ν)

(cid:26)(cid:90)

Ω×Ω

(cid:107)x − y(cid:107)p dπ(x, y) + σDKL(π || ω1 ⊗ ω2)

(cid:27)

,

(B.1)

where ω1 and ω2 are two reference measures such that ρ (cid:28) ω1 and ν (cid:28) ω2. Note that the entropic-p-
Wasserstein distance equals the p-Wasserstein distance when σ → 0.

The choice of the reference measures in (B.1) is known to induce diﬀerent types of entropy bias (Janati
et al., 2020a), since in general OTω1,ω2
(ρ, ρ) (cid:54)= 0 due to the regularization term. For example, the Lebesgue
measure (ω1 = ω2 = LRm) induces a blurring bias, whereas simply taking the product measure with ω1 = ρ
and ω2 = ν induces a shrinking bias. To circumvent the entropy bias, the p-Sinkhorn divergence (Chizat
et al., 2020; Feydy et al., 2019; Genevay et al., 2018; Luise et al., 2019; Ramdas et al., 2017) can be deﬁned
without specifying any reference measures (Feydy et al., 2019):

p,σ

Sp,σ(ρ, ν) := OTp,σ(ρ, ν) −

OTp,σ(ρ, ρ) + OTp,σ(ν, ν)
2

.

(B.2)

The Sinkhorn divergence can be viewed as an interpolation between the (unregularized) Wasserstein distance
(when σ → 0) and maximum mean discrepancy (MMD; when σ → ∞) (Feydy et al., 2019; Ramdas et al.,
2017). Note that there have also been lots of recent results regarding the computational eﬃciency guarantees
of entropic optimal transport, see e.g., Chizat et al. (2020); Genevay et al. (2019).

To avoid confusion, we reserve Sinkhorn to solely refer to notions deﬁned via the Sinkhorn divergence
(B.2), whereas entropic-Wasserstein to solely refer to notions deﬁned via the entropic-Wasserstein distance
(B.1).

The Gaussian Case. Similar to the (unregularized) Wasserstein distance, entropic-Wasserstein distances
and Sinkhorn divergences usually do not admit closed forms, with the notable exception for the one between
two multivariate Gaussians. The following closed form expressions of entropic-2-Wasserstein distance and
Sinkhorn divergence between two multivariate Gaussians are directly stated from Janati et al. (2020b);
Mallasto et al. (2021); Minh (2022) without proofs.
Proposition B.3. The entropic-2-Wasserstein distance between two Gaussians ρk = N(µk, Σk) with µk ∈ Rm
and Σk ∈ Sm

is given by

+ for k ∈

OT⊗

2,γ(ρ1, ρ2) = (cid:107)µ1 − µ2(cid:107)2

2 + tr(cid:0)Σ1 + Σ2 − 2DΣ1,Σ2

γ

(cid:1) +

γ
2

(cid:2)m(1 − log γ) + log det(2DΣ1,Σ2

γ

+ γI/2)(cid:3),

2
(cid:74)

(cid:75)

γ

:= (Σ

where DΣ1,Σ2
ρk = N(µk, Σk) for k ∈

1/2
1/2
1 + γ2I/16)1/2. Then using (B.2), the Sinkhorn divergence between two Gaussians
1 Σ2Σ
2
(cid:75)
2 + tr(cid:0)DΣ1,Σ1
S2,γ(ρ1, ρ2) = (cid:107)µ1 − µ2(cid:107)2

log det(2DΣ1,Σ2

+ DΣ2,Σ2
γ

is given by

+ γI/2)

(cid:1) +

(cid:74)

γ

γ

γ
2

− 2DΣ1,Σ2
γ
γ
4

−

(cid:2)log det(2DΣ1,Σ1

γ

+ γI/2) + log det(2DΣ2,Σ2

γ

+ γI/2)(cid:3).

Note that, unlike the (squared) 2-Wasserstein distance between two Gaussians (i.e., Gelbrich distance),
the entropic-2-Wasserstein distance and the Sinkhorn divergence are both deﬁned for degenerate Gaussians,
i.e., when (any of or both) Σ1 and Σ2 are singular.

21

B.2.1 Entropic-Wasserstein and Sinkhorn Barycenters

Similar to Wasserstein distances, Wasserstein barycenters are also NP-hard to compute in general (Altschuler
and Boix-Adser`a, 2022). A potential remedy is to introduce entropic regularization, which we refer to as the
entropic-Wasserstein barycenters (Bigot et al., 2019b,c; Carlier et al., 2021; Cuturi and Doucet, 2014; Cuturi
and Peyr´e, 2018; Janati et al., 2020a; Kim and Pass, 2018). Recent theoretical results on entropic-Wasserstein
barycenters such as their existence and uniqueness can be found in Carlier et al. (2021). A variant of the
entropic-Wasserstein barycenter is the Sinkhorn barycenter (Janati et al., 2020a; Luise et al., 2019), deﬁned
via the Sinkhorn divergence (B.2), in order to debias the entropic-Wasserstein barycenter due to the entropic
regularization. We give the deﬁnitions of both the empirical entropic-Wasserstein and Sinkhorn barycenters
below.

Deﬁnition B.4 (Empirical entropic-Wasserstein and Sinkhorn barycenters). For p ∈ [1, +∞) and λ =
(λk)k∈

∈ (cid:52)K, the λ-weighted entropic-p-Wasserstein barycenter is

K
(cid:74)

(cid:75)

(cid:98)bOT,σ
λ,p (ρ1, . . . , ρK) := argmin
ν∈P(Rm)

K
(cid:88)

k=1

λkOT⊗

p,σ(ν, ρk),

where OT⊗
p-Sinkhorn barycenter is

p,σ(ρ, ν) ≡ OTρ,ν

p,σ(ρ, ν). Likewise, for p ∈ [1, +∞) and λ = (λk)k∈

∈ (cid:52)K, the λ-weighted

K
(cid:74)

(cid:75)

(cid:98)bS,σ
λ,p(ρ1, . . . , ρK) := argmin
ν∈P(Rm)

K
(cid:88)

k=1

λkSp,σ(ν, ρk).

The Gaussian Case. Unlike the unregularized case, the entropic-2-Wasserstein barycenter and Sinkhorn
barycenter of Gaussians are no longer guaranteed to be Gaussian, so we have to restrict them to the manifold
of Gaussians. Then, under this assumption, similar to the unregularized case, both the entropic-2-Wasserstein
barycenter and the Sinkhorn barycenter can be computed by solving ﬁxed-point equations (Janati et al.,
2020b; Mallasto et al., 2021; Minh, 2022). For completeness, we state (without proof) the most general results
from Minh (2022, Theorems 11–12) below, and refer the readers to Minh (2022) for details.

Proposition B.5. Let ρ1, . . . , ρK be K possibly degenerate Gaussian distributions ρk = N(µk, Σk) with
µk ∈ Rm and Σk ∈ Sm
. Their entropic-2-Wasserstein barycenter restricted to the manifold of
Gaussians is (cid:98)bOT,σ

λ,p (ρ1, . . . , ρK) = N(µλ, Σλ,σ), where µλ = (cid:80)K

k=1 λkµk and Σλ,σ satisﬁes the equation

+ for k ∈

K

(cid:74)

(cid:75)

Σλ,σ =

σ
4

K
(cid:88)

k=1

(cid:32)

(cid:18)

λk

−I +

I +

16
σ2 Σ

1/2
λ,σΣkΣ

1/2
λ,σ

(cid:19)1/2(cid:33)
.

Furthermore, their Sinkhorn barycenter restricted to the manifold of Gaussians is also (cid:98)bS,σ
N(µλ, (cid:101)Σλ,σ) ∈ Sm
+ , where (cid:101)Σλ,σ is the unique solution of the following equation

λ,p(ρ1, . . . , ρK) =

(cid:101)Σλ,σ = ϕσ((cid:101)Σλ,σ)

K
(cid:88)

k=1



(cid:32)

λk

Σ

1/2
k

(cid:18)

I +

I +

16
γ2 Σ

1/2
k (cid:101)Σλ,σΣ

1/2
k

(cid:19)1/2(cid:33)−1



Σ

1/2
k

ϕσ((cid:101)Σλ,σ),

where ϕσ(M ) :=
the equation

(cid:16)

I + (cid:0)I + 16M 2/σ2(cid:1)1/2(cid:17)1/2

. Furthermore, if (cid:101)Σλ,σ ∈ Sm

++, then (cid:101)Σλ,σ is the unique solution of

(cid:101)Σλ,σ =



−I +

σ
4

(cid:34) K
(cid:88)

k=1

(cid:18)

λk

I +

16
σ2 (cid:101)Σ

1/2
λ,σΣk (cid:101)Σ

1/2
λ,σ

1/2

(cid:19)1/2(cid:35)2


.

22

B.3 An Example for Section 3

We now illustrate the formulation of stochastic barycentric optimization (SBO) through an example with
Gaussian distribution which admits closed form for their 2-Wasserstein barycenters. Suppose that the source
distributions are nondegenerate Gaussians, i.e., Pk = N(µk, Σk), where µk ∈ Rm and Σk ∈ Sm
.
(cid:75)
Given the assumption that the source distributions are Gaussian, the empirical distributions are instead
taken as (cid:98)Pk = N((cid:98)µk, (cid:98)Σk) for k ∈
i=1(zk,i − (cid:98)µk)(zk,i − (cid:98)µk)(cid:62) are
nk
their respective empirical means and empirical covariance matrices (which we assume to be full-rank).

i=1 zk,i and (cid:98)Σk = 1
nk

, where (cid:98)µk = 1

+ for k ∈

K
(cid:74)

(cid:80)nk

(cid:80)nk

K

(cid:74)

(cid:75)

Then, by Proposition B.7, the λ-weighted 2-Wasserstein barycenter of (cid:98)P1, . . . , (cid:98)PK is (cid:98)bλ((cid:98)P1, . . . , (cid:98)PK) =

N(µλ, Σλ), where

µλ =

K
(cid:88)

k=1

λkµk

and

Σλ = argmin

Σ∈Sm

++

K
(cid:88)

k=1

λkB2(Σ, (cid:98)Σk).

Instead of directly minimizing the SBO objective (cid:98)F b

λ(x) := Eξ∼(cid:98)bλ((cid:98)P1,...,(cid:98)PK )[(cid:96)(x, ξ)], we ﬁrst draw N

independent samples {(cid:98)ξi}N

i=1 from N(µλ, Σλ) and then minimize the ERM objective

(cid:98)F b
ERM(x) :=

1
N

N
(cid:88)

i=1

(cid:96)(x, (cid:98)ξi).

B.4 Additional Results for Section 4

Following the development of the Gelbrich ambiguity set in Section 5, we deﬁne similar notions in the case of
Wasserstein barycentric ambiguity set.

Let µ1, . . . , µK ∈ Rm and Σ1, . . . , ΣK ∈ Sm

++. The mean-covariance barycentric ambiguity set is deﬁned as

(cid:101)Uε

(cid:0)(µk, Σk)k∈

(cid:74)

; λ(cid:1) :=

K

(cid:75)

(cid:40)

(µ, Σ) ∈ Rm × Sm

+ :

K
(cid:88)

k=1

λkG2((µ, Σ), (µk, Σk)) (cid:54) ε2

(cid:41)
.

Then, the Gelbrich barycentric ambiguity set is deﬁned as

(cid:101)Gε

(cid:0)(µk, Σk)k∈

K
(cid:74)

(cid:75)

; λ(cid:1) := (cid:8)Q ∈ P2(Ξ) : (EQ[ξ], CovQ(ξ)) ∈ Uε

(cid:0)(µk, Σk)k∈

(cid:1)(cid:9).

K
(cid:74)

(cid:75)

Now let P, Q1, . . . , QK ∈ P2(Ξ) with means µ, µ1, . . . , µK ∈ Rm and covariance matrices Σ, Σ1, . . . , ΣK ∈

Sm

++ respectively. Then, by the Gelbrich bound, we have

K
(cid:88)

k=1

λkW2

2(P, Qk) (cid:62)

K
(cid:88)

k=1

λkG2((µ, Σ), (µk, Σk)).

The above inequality becomes an equality if P, Q1, . . . , QK belong to the same location-scatter family.
Furthermore, this inequality implies

(cid:102)Wε,2(Q1, . . . , QK; λ) ⊆ (cid:101)Gε

(cid:0)(µk, Σk)k∈

; λ(cid:1).

K
(cid:74)

(cid:75)

B.5 Additional Results for Section 5

The Gelbrich bound implies the Gelbrich ambiguity set is an outer approximation of the p-Wasserstein
ambiguity set with p (cid:62) 2, i.e., Wε,p((cid:98)P) ⊆ Gε((cid:98)µ, (cid:98)Σ) for every p (cid:62) 2. This result immediately leads to an upper
bound on the (optimal) worst-case risk by the (optimal) Gelbrich risk (i.e., risk under the Gelbrich ambiguity
set; Kuhn et al., 2019; Nguyen et al., 2021a, Corollary 1).

Corollary B.6 (Worst-case risk bounds). If the nominal distribution (cid:98)P ∈ P2(Ξ) has mean (cid:98)µ ∈ Rm and
covariance matrix (cid:98)Σ ∈ Sm

+ , then, for every p (cid:62) 2, we have

(∀(cid:96) ∈ L) RWε,p((cid:98)P)((cid:96)) (cid:54) RGε((cid:98)µ,(cid:98)Σ)((cid:96))

and

RWε,p((cid:98)P)(L) (cid:54) RGε((cid:98)µ,(cid:98)Σ)(L).

23

We now state the following proposition from Peyr´e and Cuturi (2019, Remark 9.5), which is proved in
Agueh and Carlier (2011, Theorem 6.1) and previously known from Knott and Smith (1994); R¨uschendorf
and Uckelmann (2002).

Proposition B.7. Let P0 ∈ P ac
matrices Σ1, . . . , ΣK ∈ Sm
of Q1, . . . , QK is Qλ,2 ∈ F(P0) with mean µλ ∈ Rm and covariance matrix Σλ ∈ Sm
k ∈

2 (Rm) and Q1, . . . , QK ∈ F(P0) with means µ1, . . . , µK ∈ Rm and covariance
++ respectively. Then, for λ ∈ (cid:52)K, the (unique) λ-weighted 2-Wasserstein barycenter
++ given by, for each

,

K
(cid:74)

(cid:75)

µλ =

K
(cid:88)

k=1

λkµk

and

Σλ = argmin

Σ∈Sm

++

K
(cid:88)

k=1

λkB2(Σ, Σk),

(B.3)

where B(·, ·) is the Bures–Wasserstein distance (5.1). The covariance matrix Σλ can be obtained by ﬁnding
the unique positive deﬁnite ﬁxed point of the equation

Σ =

K
(cid:88)

k=1

(cid:16)

Σ1/2ΣkΣ1/2(cid:17)1/2

.

λk

(B.4)

Bures–Wasserstein Barycenter. Another important example is the Bures–Wasserstein barycenter of Σ1, . . . , ΣK ∈
Sm
++, deﬁned via the Bures–Wasserstein distance, which coincides with the Wasserstein barycenter of
Q1, . . . , QK in the same zero-mean location-scatter family with covariances Σ1, . . . , ΣK respectively, i.e., Σλ
in (B.3). The issues of approximation, statistical inference and computational algorithms of the Bures–
Wasserstein barycenter can be found in Chewi et al. (2020); Kroshnin et al. (2021a,b).

B.6 An Example of Tractability Results

We now derive a tractability result from Propositions 5.3 and B.7 for nominal distributions of location-scatter
families and a quadratic loss function, similar to the one for WDRO (Kuhn et al., 2019, Theorem 16). Assume
that Ξ = Rm and consider the quadratic loss function (cid:96)(ξ) = (cid:104)ξ, Qξ(cid:105) + 2(cid:104)q, ξ(cid:105) with Q ∈ Sm and q ∈ Rm. If
(cid:98)µk ∈ Rm and (cid:98)Σk ∈ Sm
, then the Gelbrich risk is equal to the optimal values of the following
(cid:75)
tractable SDP:

+ for each k ∈

K

(cid:74)

(cid:110)

(cid:16)

α

ε2 − (cid:107)µλ(cid:107)2

2 − tr(Σλ)

(cid:17)

+ x + tr(X)

(cid:111)
,

RGε(µλ,Σλ)((cid:96)) = inf

subject to α (cid:62) 0, x (cid:62) 0, X ∈ Sm
+ ,

(cid:18) αI − Q q + αµλ
q(cid:62) + αµ(cid:62)
λ

x

(cid:19)

(cid:23) 0,

(cid:32)

αI − Q αΣ

αΣ

1/2
λ

(cid:33)

(cid:23) 0.

1/2
λ
X

2 (Rm) with mean (cid:98)µk ∈ Rm and covariance matrix (cid:98)Σk ∈ Sm
If for each k ∈
++,
and p = 2, then the optimal value of the above SDP, the worst-case risk (2.3), and the Gelbrich risk all
coincide.

, (cid:98)Pk ∈ F(P0) for some P0 ∈ P ac

K

(cid:74)

(cid:75)

To verify the above claim, we assume that Q(cid:63) ∈ F(P0) with mean (cid:98)µ(cid:63) and covariance (cid:98)Σ(cid:63) is the extremal

distribution, i.e.,

Q(cid:63) = argmax

RQ((cid:96))

Q∈Gε(µλ,Σλ)

and RQ(cid:63) ((cid:96)) = RGε(µλ,Σλ)((cid:96)).

Note that Q(cid:63) ∈ Wε,p((cid:98)P1, . . . , (cid:98)PK; λ). Then the worst-case risk satisﬁes

RQ(cid:63) ((cid:96)) (cid:54) RWε,p((cid:98)P1,...,(cid:98)PK ;λ)((cid:96)) (cid:54) RGε(µλ,Σλ)((cid:96)),

which follows from the Gelbrich bound and Theorem 5.4. Details of how to solve this SDP can be found in
Kuhn et al. (2019).

24

C Proofs

C.1 Preparatory Lemmas

We ﬁrst state some useful preparatory lemmas.

Lemma C.1. For 1 (cid:54) p < q < ∞, we have

Wp(ρ, ν) (cid:54) Wq(ρ, ν)

for any ρ, ν ∈ Pq(Ω) with Ω ⊆ Rm.
Proof of Lemma C.1. Since (cid:107)x − y(cid:107)p is convex in (cid:107)x − y(cid:107) for all p ∈ [1, +∞), by Jensen’s inequality, for
p (cid:54) q we have

(cid:18)(cid:90)

Ω×Ω

(cid:107)x − y(cid:107)p dπ(x, y)

(cid:19)1/p

(cid:18)(cid:90)

(cid:54)

(cid:107)x − y(cid:107)q dπ(x, y)

(cid:19)1/q

,

Ω×Ω

which implies that Wp(ρ, ν) (cid:54) Wq(ρ, ν).

Lemma C.2. For a, b ∈ R+ and p ∈ [1, +∞), we have (a + b)p (cid:54) 2p−1(ap + bp).

Proof of Lemma C.2. by the convexity of R+ (cid:51) s (cid:55)→ sp, we get

(cid:19)p

(cid:18) a + b
2

(cid:54) ap + bp
2

,

which is equivalent to (a + b)p (cid:54) 2p−1(ap + bp).

C.2 Proof of Theorem 4.2

Proof of Theorem 4.2. Assume that Q1, . . . , QK ∈ Pp(Ξ) have a λ-weighted p-Wasserstein barycenter Qλ,p,
where λ ∈ (cid:52)K. Using the argument of Figalli and Glaudo (2021, Remark 3.1.2), by the triangle inequality
and Lemma C.2, for any P ∈ Pp(Ξ) and k ∈
, we have
(cid:75)

K

(cid:74)

Wp

p(P, Qλ,p) (cid:54) (cid:0)Wp(P, Qk) + Wp(Qλ,p, Qk)(cid:1)p (cid:54) 2p−1(cid:0)Wp

p(P, Qk) + Wp

p(Qλ,p, Qk)(cid:1).

This implies that

(∀P ∈ Pp(Ξ)) Wp

p(P, Qλ,p) =

K
(cid:88)

k=1

λkWp

p(P, Qλ,p)

(cid:54) 2p−1

(cid:54) 2p−1

K
(cid:88)

k=1

K
(cid:88)

k=1

λkWp

p(P, Qk) + 2p−1

λkWp

p(P, Qk) + 2p−1

K
(cid:88)

k=1

K
(cid:88)

k=1

λkWp

p(Qλ,p, Qk)

λkWp

p(P, Qk)

= 2p

K
(cid:88)

k=1

λkWp

p(P, Qk),

where the second inequality follows from the deﬁnition of the λ-weighted p-Wasserstein barycenter.

Consequently, for any ε (cid:62) 0,

K
(cid:88)

k=1

which implies the desired result.

λkWp

p(P, Qk) (cid:54) ε =⇒ Wp

p(P, Qλ,p) (cid:54) 2p · ε,

25

C.3 Proof of Theorem 4.6
Proof of Theorem 4.6. First note that the 2-Wasserstein space W2(Rm) = (P2(Rm), W2) is positively curved
in the sense of Alexandrov (Ambrosio et al., 2005, §7.3). Then, by Le Gouic et al. (2021, Theorem 12), we
have, for any β ∈ (0, 1),

(cid:26)

Pn

(cid:16)
(cid:98)b1/K,2((cid:98)Pn), (cid:98)b(cid:63)
K

(cid:17)

W2
2

(cid:54) c1
n

log

(cid:18) 2
β

(cid:19)(cid:27)

(cid:62) 1 − β − e−c2n,

where (cid:98)b1/K,2((cid:98)Pn) = b2((cid:98)ρn,K), and c1, c2 ∈ R++ are independent of n. By the deﬁnition of Wε,2((cid:98)Pn; 1/K),
we also have
= Pn(cid:110)

(cid:16)
(cid:98)b1/K,2((cid:98)Pn), (cid:98)b(cid:63)
K

K ∈ Wε,2((cid:98)Pn; 1/K)
(cid:98)b(cid:63)

Pn(cid:110)

(cid:54) ε

(cid:111)
,

W2

(cid:111)

(cid:17)

hence the desired result.

Remark C.3. Note that Adve and M´esz´aros (2020) provides a heuristic argument for the non-negative
curvature of Wp(Rm) when p ∈ (1, p(m)), where p(m) > 1 is close to 1 (e.g., p(m) = 1 + 1/O(m2 log m)).
Consequently, Theorem 4.6 should also hold for p ∈ (1, p(m)). For p /∈ (1, p(m)) ∪ {2}, it remains unclear
whether Wp(Rm) is positively curved even though it is expected. In this case, the general concentration
inequality in Le Gouic et al. (2021, Theorem 12) involving various abstract constants cannot be easily
simpliﬁed.

C.4 Proof of Theorem 4.7
Proof of Theorem 4.7. By Theorem 4.6, the inequality Pn(cid:110)
implies that

K ∈ Wε,2((cid:98)Pn; 1/K)
(cid:98)b(cid:63)

(cid:111)

(cid:62) 1−β−e−c2n immediately

R

(cid:98)b(cid:63)
K

((cid:96)) (cid:54)

sup
P∈Wε,2((cid:98)Pn;1/K)

RP((cid:96)) =: RWεn ,2((cid:98)Pn;1/K)((cid:96))

with probability at least 1 − β − e−c2n, where c2 > 0 is independent of n.

C.5 Proof of Theorem 4.8

To prove Theorem 4.8, we need the following lemma which is a slight modiﬁcation of Esfahani and Kuhn
(2018, Lemma 3.7).
Lemma C.4. Let K ∈ N∗ be ﬁnite and ﬁxed. Suppose that P ∈ W2(P2(Ξ)) is sub-Gaussian, βn ∈ (0, 1)
and εn = εn(βn) ∈ R++ for n ∈ N∗, satisﬁes (cid:80)∞
n=1 βn < ∞ and limn→∞ εn(βn) = 0, then any sequence
(cid:98)Qn ∈ Wεn(βn),2((cid:98)Pn; 1/K), n ∈ N∗, where (cid:98)Qn may depend on the observations, converges in the W2 distance
to (cid:98)b(cid:63)

K almost surely with respect to P∞, i.e,
P∞(cid:110)

(cid:16)
(cid:98)b(cid:63)
K, (cid:98)Qn

W2

(cid:17)(cid:111)

= 1.

lim
n→∞

Proof of Lemma C.4. For any (cid:98)Qn ∈ Wεn(βn),2((cid:98)Pn; 1/K), the triangle inequality implies
(cid:17)
(cid:16)
(cid:54) W2
K, (cid:98)b1/K,2((cid:98)Pn)
(cid:98)b(cid:63)
In addition, by Theorem 4.7, we have Pn(cid:110)

(cid:16)
(cid:98)b1/K,2((cid:98)Pn), (cid:98)Qn

(cid:16)
(cid:98)b(cid:63)
K, (cid:98)Qn

(cid:17)
(cid:16)
K, (cid:98)b1/K,2((cid:98)Pn)
(cid:98)b(cid:63)

(cid:17)
(cid:16)
K, (cid:98)b1/K,2((cid:98)Pn)
(cid:98)b(cid:63)

(cid:54) εn(βn)

(cid:54) W2

+ W2

W2

W2

(cid:111)

(cid:17)

(cid:17)

+ εn(βn).

(cid:62) 1 − βn − e−c2n, which implies

Pn(cid:110)

W2

(cid:16)
(cid:98)b(cid:63)
K, (cid:98)Qn

(cid:17)

(cid:54) 2εn(βn)

(cid:111)

(cid:62) 1 − βn − e−c2n.

Since (cid:80)∞
2001, Theorem 4.2.4) yields

n=1 βn < ∞ and (cid:80)∞

n=1 e−c2n < ∞ as c2 > 0, invoking the (second) Borel–Cantelli lemma (Chung,

P∞(cid:110)

W2

(cid:16)
(cid:98)b(cid:63)
K, (cid:98)Qn

(cid:17)

(cid:54) εn(βn) for suﬃciently large n

(cid:111)

= 1.

Since εn(β) → 0 as n → ∞, we conclude that limn→∞ W2

(cid:16)
(cid:98)b(cid:63)
K, (cid:98)Qn

(cid:17)

P∞-almost surely.

26

Now we are ready to prove Theorem 4.8.

Proof of Theorem 4.8. Let K ∈ N∗ be ﬁnite and ﬁxed. Let us deﬁne

(cid:96)(cid:63)
n,K := argmin

(cid:96)∈L

RWε,2((cid:98)Pn;1/K)

and R

(cid:98)b(cid:63)
K

(L) := inf
(cid:96)∈L

R

(cid:98)b(cid:63)
K

((cid:96)).

Since (cid:96)(cid:63)

n,K ∈ L, we have R

(cid:98)b(cid:63)
K

((cid:96)(cid:63)

n,K) (cid:54) R

(cid:98)b(cid:63)
K

(L). Theorem 4.6 implies

Pn(cid:110)
R

(cid:98)b(cid:63)
K

((cid:96)(cid:63)

n,K) (cid:54) R

(cid:98)b(cid:63)
K

(L) (cid:54) RWεn(βn ),2((cid:98)Pn;1/K)((cid:96)(cid:63)

n,K)

(cid:111)

(cid:62) Pn(cid:110)
K ∈ Wεn(βn),2((cid:98)Pn; 1/K)
(cid:98)b(cid:63)
(cid:62) 1 − β − e−c2n

(cid:111)

for all n ∈ N∗. Since (cid:80)∞
lemma again yields
P∞(cid:110)
R

((cid:96)(cid:63)

n=1 βn < ∞ and (cid:80)∞

n=1 e−c2n < ∞ as c2 > 0, invoking the (second) Borel–Cantelli

n,K) (cid:54) R

(L) (cid:54) RWεn (βn),2((cid:98)Pn;1/K)((cid:96)(cid:63)

n,K) for suﬃciently large n

(cid:111)

= 1.

(cid:98)b(cid:63)
K

(cid:98)b(cid:63)
K

RWεn(βn ),2((cid:98)Pn;1/K)((cid:96)(cid:63)
n,K) with probability one. This
Thus, it remains to show that lim supn→∞
part is more subtle and involves overwhelming technicalities—we refer to the proof of Esfahani and Kuhn
(2018, Theorem 3.6) as our proof follows exactly the same steps.

n,K) (cid:54) R

((cid:96)(cid:63)

(cid:98)b(cid:63)
K

The second part of Theorem 4.8 follows exactly the same procedures as that of the ﬁrst part. By
considering that n has already been taken to ∞, results similar to Theorems 4.6 and 4.7 also hold, by
replacing n with K, (cid:98)Pn with (cid:98)P(cid:63), (cid:98)b(cid:63)

K with b(cid:63), etc.

C.6 Proof of Proposition 5.3

Proof of Proposition 5.3. Since Q1, . . . , QK belong to the same location-scatter family F(P0) with P0 ∈
P ac
2 (Rm), so is their λ-weighted 2-Wasserstein barycenter Qλ,2, since location-scatter families are closed for
barycenters ( ´Alvarez-Esteban et al., 2018, Theorem 3.8).

Let us recall the deﬁnition of the 2-Wasserstein ambiguity set

Wls

ε,2(Q1, . . . , QK; λ) := (cid:8)P ∈ F(P0) : W2(P, Qλ,2) (cid:54) ε(cid:9).

Recall that the W2 distance between two distributions of the same location-scatter family is the Gelbrich
distance, i.e., W2(P, Qλ,2) = G((µ, Σ), (µλ, Σλ)), where µ and Σ are the mean and the covariance matrix of
P ∈ F(P0) respectively. Therefore, we have

Wls

ε,2(Q1, . . . , QK; λ) = (cid:8)P ∈ F(P0) : G((µ, Σ), (µλ, Σλ)) (cid:54) ε(cid:9)

= (cid:8)P ∈ P2(Ξ) : G((µ, Σ), (µλ, Σλ)) (cid:54) ε(cid:9) ∩ F(P0)
= Gε(µλ, Σλ) ∩ F(P0).

C.7 Proof of Theorem 5.4

Proof of Theorem 5.4. Let P ∈ P2(Ξ) with mean µ ∈ Rm and covariance matrix Σ ∈ Sm
P ac
bound, we have

2 (Ξ) with means µ1, . . . , µK ∈ Rm and covariance matrices Σ1, . . . , ΣK ∈ Sm

+ , and let (cid:98)P1, . . . , (cid:98)PK ∈
++ respectively. By the Gelbrich

W2(P, (cid:98)bλ,2((cid:98)P1, . . . , (cid:98)PK)) (cid:62) G((µ, Σ), (µλ, Σλ)),

where µλ and Σλ are the mean and the covariance matrix of (cid:98)bλ,2((cid:98)P1, . . . , (cid:98)PK). The two worst-case risk
bounds immediately follow from this inequality due to the deﬁnitions of the worst-case risk and the optimal
worst-case risk.

27

C.8 Proof of Corollary B.6

Proof of Corollary B.6. The Gelbrich bound and Lemma C.1 together imply that, for any P, (cid:98)P ∈ P2(Ξ) with
means µ, (cid:98)µ ∈ Rm and covariance matrices Σ, (cid:98)Σ ∈ Sm

++, we have

Consequently, we have the inclusion property

G((µ, Σ), ((cid:98)µ, (cid:98)Σ)) = W2(ρ, ν) (cid:54) Wp(ρ, ν).

Wε,p((cid:98)P) ⊆ Gε((cid:98)µ, (cid:98)Σ)
for every p (cid:62) 2. Thus, according to the deﬁnitions of the worst-case risk (2.3) and the optimal worst-case risk
(2.4), the desired results follow.

C.9 Proof of Proposition B.7

Proof of Proposition B.7. See e.g., Agueh and Carlier (2011, Theorem 6.1) and Knott and Smith (1994);
R¨uschendorf and Uckelmann (2002).

D Experimental Details

In this section, we give further details about Section 6.

D.1 The Wasserstein Barycentric Shrinkage Estimator

The following theorem indicates the way to compute the solution X (cid:63) of the DRMLE problem (6.1).

Theorem D.1. Assume that ε > 0 and (cid:98)Σk ∈ Sm
++,
and that the λ-weighted Bures–Wasserstein barycenter Σλ of (cid:98)Σ1, . . . , (cid:98)ΣK admits the spectral decomposition
Σλ = (cid:80)m
.
m
(cid:75)
Then the unique minimizer of the DRMLE problem (6.1) is given by X (cid:63) = (cid:80)m
j , where, for each
j ∈

j with eigenvalues ζj ∈ R+ and corresponding orthonormal eigenvectors vj ∈ Rm, j ∈

with all least one of the (cid:98)Σk’s in Sm

+ for each k ∈

j=1 ζjvjv(cid:62)

j=1 x(cid:63)

j vjv(cid:62)

K
(cid:74)

(cid:75)

(cid:74)

,

m
(cid:75)

(cid:74)

j = χ(cid:63)
x(cid:63)

(cid:20)

1 −

1
2

(cid:16)(cid:113)

j (χ(cid:63))2 + 4ζ 2
ζ 2

j χ(cid:63) − ζjχ(cid:63)(cid:17)(cid:21)

,

and χ(cid:63) > 0 is the unique positive solution of the equation


ε2 −

1
2

m
(cid:88)

j=1



ζj

χ − m +

1
2

m
(cid:88)

j=1

(cid:113)

ζ 2
j χ2 + 4ζjχ = 0.

(D.1)

Proof of Theorem D.1. According to the formulation of the DRMLE problem (6.1). The above theorem is
simply Nguyen et al. (2022, Theorem 3.1) with (cid:98)Σ replaced by Σλ.

Recall that Σλ can be obtained by ﬁnding the unique positive deﬁnite ﬁxed point of the equation (B.4).

Thus, to approximate the barycenter Σλ, one iterates

(∀t ∈ N∗) St+1 = S−1/2

t

(cid:32) K
(cid:88)

k=1

λk(S

1/2
t ΣkS

1/2
t )1/2

(cid:33)2

S−1/2
t

,

which gives limt→∞ St = Σλ. Details of this iterative scheme can be found in ´Alvarez-Esteban et al. (2016).

28

D.2 The Averaged Linear Shrinkage Estimator

A na¨ıve estimator is constructed by simply replacing the sample covariance matrix in the widely-used linear
shrinkage estimator by the λ-weighted average of the sample covariance matrices (cid:98)Σ1, . . . , (cid:98)ΣK, deﬁned as
follows.

Deﬁnition D.2 (Averaged linear shrinkage estimator). Given K empirical covariance matrices (cid:98)Σ1, . . . , (cid:98)ΣK ∈
++, the λ-weighted linear shrinkage estimator for the precision matrix X ∈ Sm
Sm

++ is deﬁned by

(cid:34)

X (cid:63) =

(1 − α)

K
(cid:88)

k=1

λk (cid:98)Σk + α

(cid:35)−1

λk Diag((cid:98)Σk)

,

K
(cid:88)

k=1

where α ∈ [0, 1], λ ∈ (cid:52)K and Diag(A) is the diagonal matrix with the same diagonal of the square matrix
A ∈ Rm×m.

Let us recall that the λ-weighted average of the sample covariance matrices (cid:98)Σ1, . . . , (cid:98)ΣK is indeed the

Frobenius barycenter of (cid:98)Σ1, . . . , (cid:98)ΣK:

K
(cid:88)

k=1

λk (cid:98)Σk = argmin

Σ∈Sm

++

K
(cid:88)

k=1

λk|||Σ − (cid:98)Σk|||2
F.

D.3 The Averaged L1-Regularized Maximum Likelihood Estimator

An averaged L1-regularized maximum likelihood estimator of the precision matrix can be obtained from
simply minimizing a λ-weighted loss function of the original L1-regularized maximum likelihood estimation
problem.

Deﬁnition D.3 (Averaged L1-regularized maximum likelihood estimator). Let us recall that the original
L1-regularized maximum likelihood estimation problem takes the following objective:

minimize
X∈Sm

++

g(X, (cid:98)Σ) := − log det X + ⟪(cid:98)Σ, X⟫
F

+ τ (cid:107)X(cid:107)1,

where τ (cid:62) 0. Then, given K empirical covariance matrices (cid:98)Σ1, . . . , (cid:98)ΣK ∈ Sm
maximum likelihood estimator for the precision matrix X ∈ Sm

++ is deﬁned by

++, the λ-weighted L1-regularized

X (cid:63) = argmin

K
(cid:88)

X∈Sm

k=1
where τ (cid:62) 0 and λ ∈ (cid:52)K.

++

λkg(X, (cid:98)Σk) = − log det X +

K
(cid:88)

k=1

λk⟪(cid:98)Σk, X⟫
F

+ τ (cid:107)X(cid:107)1 = g

X,

(cid:32)

(cid:33)
,

λk (cid:98)Σk

K
(cid:88)

k=1

D.4 The Sinkhorn Barycentric Shrinkage Estimator

The Sinkhorn Barycentric Shrinkage Estimator (SBSE) is simply an estimator similar to WBSE with the
2-Wasserstein barycenter replaced by the Sinkhorn barycenter, which is used because of the non-existence
of the 2-Wasserstein barycenter under the high-dimensional setting and computational consideration used
in simulations. Theoretical treatment of this estimator is left for future work. We do not use the entropic-
2-Wasserstein barycenter in simulations since it does not make much sense (see Minh, 2022, Remark 4).
Also recall from Proposition B.5 for the Sinkhorn barycenter of Gaussians (restricted to the manifold of
Gaussians).

D.5 Simulation Settings

All experiments were run with a laptop with Intel Core i7-7700HQ CPU (2.80 GHz) and 32GB RAM, using
Python 3.9 with libraries numpy (Harris et al., 2020), scipy (Virtanen et al., 2020) and scikit-learn
(Pedregosa et al., 2011).

The equation (D.1) is solved via the Netwon–Raphson method in scipy. We give the choice of tuning

parameters in Table 2, which are obtained based on grid search.

29

Table 2: Tuning parameters of LS, L1 and WBSE.

n

50

100

200

K

25
50
100

25
50
100

25
50
100

α

0.1
0.1
0.1

0.1
0.1
0.1

0.1
0.1
0.1

τ

0.1
0.1
0.1

0.1
0.1
0.1

0.1
0.1
0.1

ε

0.3
0.3
0.3

0.03
0.03
0.03

0.03
0.03
0.005

Remark D.4. While we treat the choice of ε as a tuning parameter in simulations, in Blanchet and Si (2019),
the optimal distributional uncertainty size ε = εn is studied as a function of the sample size n for the
Wasserstein Shrinkage Estimator (Nguyen et al., 2022). Blanchet and Si (2019) prove that εn should scale
at rate εn = ε(cid:63)n−1(1 + o(1)) = O(n−1), which aligns with the empirical ﬁndings of Nguyen et al. (2022).
This is as opposed to the theoretical rate of O(n−1/2). It is interesting to ﬁnd the optimal scaling of ε in our
proposed Wasserstein Barycentric Shrinkage Estimator in terms of both n and K, and is left for future work.

E Simulations under High-Dimensional Setting

In this section, we study the performance of the proposed Sinkhorn barycentric shrinkage estimator (see
Appendix D.4) under the high-dimensional setting (m > n). Let us recall that under the high-dimensional
setting, the sample covariance matrices (cid:98)Σk’s are all rank-deﬁcient, so the empirical (unregularized) 2-
Wasserstein barycenter of K Gaussians does not exist. We thus resort to the Sinkhorn barycenter with
entropic regularization strength σ > 0. In particular, we choose m = 20, n ∈ {5, 10, 15}, K ∈ {25, 50, 100}
and σ = 0.1. The Stein losses of the estimators are given in Table 3, averaged over 20 independent trials.

Table 3: Stein losses of LS, L1 and SBSE.

n

5

10

15

K

25
50
100

25
50
100

25
50
100

LS

L1

SBSE

7.61 ± 0.73
7.75 ± 0.59
7.70 ± 0.47

7.12 ± 0.69
7.12 ± 0.44
7.20 ± 0.35

6.96 ± 0.55
6.99 ± 0.40
6.97 ± 0.27

8.43 ± 0.79
8.67 ± 0.63
8.67 ± 0.51

7.99 ± 0.74
8.04 ± 0.47
8.15 ± 0.38

7.84 ± 0.60
7.90 ± 0.43
7.90 ± 0.29

2.72 ± 0.23
2.54 ± 0.21
2.43 ± 0.15

1.94 ± 0.29
1.37 ± 0.17
1.07 ± 0.10

1.91 ± 0.27
1.12 ± 0.16
0.71 ± 0.10

Similar to WBSE under the low-dimensional setting, we observe that SBSE also outperforms the other

two estimators by a large margin. The performance of SBSE also improves as n and K increase.

We also give the choice of tuning parameters in Table 4, which are obtained based on grid search.

The Eﬀect of Entropic Regularization Strength σ in SBSE. We also numerically study how diﬀerent
entropic regularization strengths in the Sinkhorn barycenter aﬀect the performance of the proposed Sinkhorn

30

Table 4: Tuning parameters of LS, L1 and SBSE.

n

5

10

15

K

25
50
100

25
50
100

25
50
100

α

0.1
0.1
0.1

0.1
0.1
0.1

0.1
0.1
0.1

τ

0.1
0.1
0.1

0.1
0.1
0.1

0.1
0.1
0.1

ε

1
1
1

0.5
0.5
0.5

0.3
0.3
0.3

barycentric shrinkage estimator. We choose m = 20, n = 5, K = 25, α = 0.1, τ = 0.1, ε = 0.8 and
σ ∈ {0.01, 0.1, 1, 10, 100}.

Table 5: Stein losses of LS, L1 and SBSE with diﬀerent σ’s.

σ

0.01
0.1
1
10
100

LS

L1

SBSE

7.61 ± 0.73

8.43 ± 0.79

4.47 ± 0.35
2.03 ± 0.22
5.58 ± 0.36
9.82 ± 0.71
11.70 ± 0.88

We observe that, given a ﬁxed set of (m, n, K, ε), smaller σ (i.e., closer approximation to the unregularized
2-Wasserstein barycenter) does not necessarily yield lower Stein loss. However, as the strength of entropic
regularization grows, the performance of SBSE decays sharply, which could be even worse than the averaged
linear shrinkage and averaged L1-regularized maximum likelihood estimators.

31

