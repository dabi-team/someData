RMP2: A Structured Composable Policy Class for
Robot Learning

Anqi Li1*, Ching-An Cheng2*, M. Asif Rana3, Man Xie3, Karl Van Wyk4, Nathan Ratliff4, and Byron Boots1,4
1University of Washington, 2Microsoft Research, 3Georgia Institute of Technology, 4NVIDIA
* equal contribution

1
2
0
2

r
a

M
0
1

]

O
R
.
s
c
[

1
v
2
2
9
5
0
.
3
0
1
2
:
v
i
X
r
a

Abstract—We consider the problem of learning motion poli-
cies for acceleration-based robotics systems with a structured
policy class speciﬁed by RMPﬂow. RMPﬂow is a multi-task
control framework that has been successfully applied in many
robotics problems. Using RMPﬂow as a structured policy class
in learning has several beneﬁts, such as sufﬁcient expressiveness,
the ﬂexibility to inject different levels of prior knowledge as
well as the ability to transfer policies between robots. However,
implementing a system for end-to-end learning RMPﬂow policies
faces several computational challenges. In this work, we re-
examine the message passing algorithm of RMPﬂow and propose
a more efﬁcient alternate algorithm, called RMP2, that uses
modern automatic differentiation tools (such as TensorFlow
and PyTorch) to compute RMPﬂow policies. Our new design
retains the strengths of RMPﬂow while bringing in advantages
from automatic differentiation, including 1) easy programming
interfaces to designing complex transformations; 2) support of
general directed acyclic graph (DAG) transformation structures;
3) end-to-end differentiability for policy learning; 4) improved
computational efﬁciency;. Because of these features, RMP2 can
be treated as a structured policy class for efﬁcient robot learning
which is suitable encoding domain knowledge. Our experiments
show that using structured policy class given by RMP2 can
improve policy performance and safety in reinforcement learning
tasks for goal reaching in cluttered space.

I. INTRODUCTION

Designing reactive motion policies is a fundamental problem
in robotics. This problem has been tackled by analytic control
techniques and machine learning tools, but
they lead to
different compromises. Traditionally control techniques have
been used, offering motion policies with analytical forms and
desirable properties such as stability, safety, and performance
guarantees [4, 16, 23, 35]. However, as tasks become more
complex and the environment become less structured, syn-
thesizing an analytical policy with such properties becomes
difﬁcult, and, even if one succeeds, the resulting policy may
be highly sub-optimal [7]. In contrast to hand-designed control
techniques, learning-based approaches (such as reinforcement
learning [33, 34] or imitation learning [31, 32]) make minimal
assumptions and promise to improve policy performance
through interactions with the environment. However, such
approaches often require many interactions to achieve rea-
sonable results, especially when learning policies under sparse
reward signals through reinforcement learning. Furthermore,
most learning algorithms are sensitive to distribution shifts
and have poor out-of-distribution generalization ability. For
example, a policy that is trained to go to the left of a room in

training often does not know how to go to the right, because
such examples are never presented in training.

In practice, we desire motion policy optimization algorithms
that possess both the non-statistical guarantees from the control-
based approaches and the ﬂexibility of the learning-based
approaches. A promising direction is the use of structured
policies [8, 9, 12, 21, 22]. Here the main idea is to apply
learning to optimize only within policy parameterizations that
have provable control-theoretic properties (such as stability and
safety guarantees). In other words, it uses data to optimize
the hyperparameters of a class of controllers with provable
guarantees. From a learning perspective, structured policies
provide a way to inject prior knowledge about a problem
domain into learning. More often than not, before running the
robot in the ﬁeld, we may know (approximate) kinematic and
dynamic models, task constraints, and some notion of desired
behaviors. For example, the kinematics of the robot is provided
by the manufacturer, and we know generally that colliding
with obstacles in the environment and hitting joint limits
are undesirable. By using this information through a control
framework to design structured policies, we can ensure policies
produced a learning algorithm can always satisfy certain task
speciﬁcations (such as safety) regardless how they are learned.
In this paper, we are interested in learning structured motion
policies for acceleration-based robotics systems [35]. We adopt
the RMPﬂow control framework [6] as the foundation of
the structured policy class. RMPﬂow is an multi-task control
algorithm that designs the acceleration-based motion policy by
decomposing a full robot control problem into smaller subtasks,
such as goal reaching, collision avoidance, maintaining balance,
etc. RMPﬂow treats these subtask spaces as manifolds and
provides a message passing algorithm to combine subtasks
policies into a stable motion policy for all the subtasks [6].
Because of its control-theoretic guarantees and computational
efﬁciency, RMPﬂow has been applied to a range of robotic
applications [15, 18, 19, 21, 25, 36, 38].

Learning motion policies that can be expressed by the
RMPﬂow framework, which we call the RMPﬂow policies,
has several advantages: 1) The task decomposition scheme
in RMPﬂow provides an interface to inject different levels
of prior knowledge. As an example, one can hand design
safety-critical subtask policies for, e.g. collision avoidance, and
learn other subtask policies to improve the overall performance;
2) Hand-designed RMPﬂow policies have been applied to solve

 
 
 
 
 
 
Algorithm
RMP2 (Algorithm 3)
Naïve Implementation (Algorithm 4) O(N bd3L) O(N Ld2)
RMPﬂow (Algorithm 1) [6]

Req. Tree Req. Auto.Diff.
(cid:51)
(cid:55)
(cid:51)
(cid:55)
(cid:55)a
O(Ld2 + N d2) (cid:51)

Space
O(Ld2 + N d)

Time
O(N bd3)

O(N bd3)

TABLE I: Comparison between different implementations of the RMPﬂow policy for a graph/tree with N nodes of dimension
in O(d), where L ≤ N nodes are leaf nodes and the branching factor is in O(b). Computing the task map is assumed to have
time complexity of O(N bd2) and memory complexity of O(1). We assume the automatic differentiation library is based on
reverse-mode automatic differentiation, for which calling the Gradient Oracle to compute the derivative of a scalar function with
respect to a variable in O(d) requires time complexity in O(N bd2) and space complexity O(N d). See Section III for details.

many real-world robotics applications that require complex mo-
tions, so RMPﬂow policies represent a sufﬁciently expressive
for motion control problems; 3) The RMPﬂow policy learned
on one robot can be transferred to other robots because of
its differential geometry centered design [6]. This allows us
to easily adopt existing hand-designed subtask policies in the
literature to partially parameterize the RMPﬂow policy to help
warm-start the learning process;

Despite these promises, implementing a system for learning
RMPﬂow policies faces several practical computational chal-
lenges, because the RMPﬂow message passing algorithm was
originally designed for reactive control rather than learning,
One reason is that RMPﬂow uses a rather complicated user
interface involving a tree data structure that is non-trivial for
users to specify. The other reason, which is perhaps more
important, is that it is hard and computationally expensive to
trace the gradient ﬂow in the message passing algorithm of
RMPﬂow, and yet tracing gradient ﬂows is commonly required
for end-to-end learning. Although recent work has looked into
learning with RMPﬂow [21, 22, 27], these methods either
largely simplify the learning problem so that differentiating
through RMPﬂow is not needed [2, 21, 27] or only allows for
a very limited parameterization of the policy [22]. For example,
Meng et al. [21] and Rana et al. [27] learn the subtask policies
independently through imitation and then use RMPﬂow to
combine the learned RMPs with other hand-speciﬁed policies;
and Mukadam et al. [22] learn scalar weight functions for pre-
deﬁned subtask polices. Recently Aljalbout et al. [2] explored
learning collision avoidance RMPs with RMPﬂow through
reinforcement learning, where differentiating through RMPﬂow
is not required as the gradients can be estimated through
samples and value function estimate.

In this work, we propose a simple alternative algorithm,
called RMP2 (RMPﬂow Reactive Motion Policy), to replace
the message passing algorithm in RMPﬂow for computing
RMPﬂow policies, so that end-to-end learning RMPﬂow polices
can be more easily implemented and scaled up in practice. We
emphasize that we do not propose a new structured policy
class, but a more efﬁcient and ﬂexible implementation of
RMPﬂow policies. Policies realized by either the original
message passing algorithm of RMPﬂow or our new RMP2
algorithm are therefore the same, and learning with them would
lead to the same results statistically. For clarity, we will refer
to the RMPﬂow algorithm as the message passing routine
in RMPﬂow (which our algorithm RMP2 replaces) and the
RMPﬂow policy as the effective motion policy that RMPﬂow

represents (which our RMP2 also represents).

RMP2 realizes the RMPﬂow policy by querying the Gradient
Oracle [13] in an automatic differentiation library (such as
TensorFlow [1] and PyTorch [24]) instead of using the tree data
structure and message passing steps of the RMPﬂow algorithm.
In comparison, RMP2 has several advantages over the original
RMPﬂow algorithm: 1) RMP2 allows for a simper user interface.
The user only needs to specify the task map using an automatic
differentiation library, the automatically constructed (directed
acyclic) computational graph can then be used for computing
the RMPﬂow policy. 2) RMP2 relaxes the assumption on using
tree-structured task maps in the RMPﬂow algorithm [6] to work
with any directed acyclic graph (DAG) task maps. 3) RMP2
is much easier to implement in conjunction with learning
algorithms: as RMP2 is implemented using operators supported
by automatic differentiation libraries, it is convenient to take
the gradient, or higher order derivatives, of any function with
respect to the parameters in subtask policies and task maps.
4) RMP2 uses a smaller memory footprint than RMPﬂow, while
having the same time complexity (see Table I).

These computational advantages make RMP2 generally ap-
plicable to many end-to-end learning scenarios and algorithms.
In the rest of the paper, we provide the details of our new
algorithmic design and its complexity analysis. At the end of
the paper, we validate our algorithm RMP2 in applications
of learning acceleration-based motion control policies with
reinforcement learning in simulated reaching tasks with a three-
link robot arm and a Franka robot arm.

A. Acceleration-based Motion Control

II. BACKGROUND

We focus on learning policies for controlling the motion
of acceleration-based robotics systems. Typically this type of
kinematic control problems arises when one wishes to reactively
generate smooth reference trajectories for a low-level tracking
controller, or wants to control a robot that is fully actuated
and feedback linearized (e.g., by an inverse dynamics model).
Suppose the robot’s conﬁguration space C (e.g. the joint
space of a robot with revolute joints) is a d-dimensional smooth
manifold that can be described by generalized coordinates
q ∈ Rd. We can view the acceleration-based motion control
problem as a continuous-time deterministic Markov decision
process (MDP): the state is the position-velocity (q, ˙q), the
action is the acceleration ¨q, the transition is the integration
rule, and the reward is deﬁned to encourage desired behaviors
for a task (such as smooth, collision-free motions). Our goal

is to ﬁnd an acceleration-based policy π such that the system
following ¨q = π(q, ˙q) would exhibit good performance for
the tasks of interest.

In these motion control problems, the desired behavior of
a task (equivalently the reward function) is often not directly
described in the generalized coordinates q ∈ Rd, but in terms
of another set of task coordinates x ∈ Rm that are related to
the generalized coordinates through a nonlinear mapping ψ,
i.e. x = ψ(q). For example, in controlling an anthropomorphic
robot, we wish to control the torso’s motion to maintain
the stability and the reachable region of the hands, whose
performance is more easily described in the coordinates of
the torso and the hand rather than directly in the joint space
(i.e. the conﬁguration space). We call this mapping ψ the task
map and refer to the image manifold of C under ψ as the task
space, which is denoted as T . As shown in the above example,
the task here is often multi-task in nature requiring the robot
to satisfy various performance criteria. Mathematically, this
implies that the overall task space T is a composition of many
subtask spaces, such as T = (cid:81)K
k=1 Tk in a K-task control
problem based on subtask manifolds {Tk}K
k=1. These subtask
coordinates are often not independent but intertwined together
as the image of the common conﬁguration space C under the
task map ψ (in the previous example, moving the torso affects
the position of the robot hand). Therefore, generally, policies
designed for each subtask cannot be trivially combined together
(e.g., with using a convex combination) to generate a good
policy for the full task.

B. RMPﬂow: A Framework for Multi-Task Problems

RMPﬂow [6] is a control-theoretic computational framework
designed to address the multi-task control problems mentioned
above. Inspired by the geometric control theory [4], RMPﬂow
resolves the conﬂicts between different subtasks by describing
each subtask policy as a Riemannian Motion Policy (RMP) [30].
An RMP associates a subtask (e.g. for the kth subtask) not only
with the desired acceleration ad
k(x, ˙x), but also with a positive
semi-deﬁnite matrix function Mk(x, ˙x) that depends on the
state (i.e. the position and the velocity) of the subtask. Given
RMPs for the subtasks, RMPﬂow generates the policy π on the
conﬁguration space by combining these subtask RMPs through
message passing on a tree data structure of manifolds (called
the RMP-tree), where the root and leaf nodes correspond to
the conﬁguration space C and the subtask spaces {Tk}, and an
edge represents a smooth map from a parent node manifold to
its child node manifold.

This message passing scheme of RMPﬂow effectively
realizes a differential-geometric operation, called the pullback,
which propagates differential forms from the subtask manifolds
to the conﬁguration space manifold. As a result, it can be
proved that the ﬁnal policy π output by RMPﬂow is Lyapunov
stable, when Mk is derived appropriately from a Riemannian
metric that describes the motion induced by ad
k for each
policy [6, 18]. These nice control-theoretical properties makes
RMPﬂow a promising candidate of the structured policy class
for acceleration-based motion control.

Algorithm 1 The RMPﬂow Algorithm (Message Passing) [6]
1: Input:
2: Return: motion policy π(q, ˙q)
3: nodes ← T.topologically_sorted_nodes()

root state (q, ˙q), RMP-tree T, RMPs rmp_eval

// forward pass

4: For node in nodes: // from root to leaves
5:
6:

For child in node.children:

child.state ← pushforward(node.state)

// evaluate leaf RMPs
7: For node in T.leaves:
8:

node.rmp ← rmp_eval(node.state)

// backward pass

9: For node in reversed(nodes): // from leaves to root
node.rmp ← pullback (node.child.rmps)
10:

// resolve for the motion policy

11: π(q, ˙q) ← resolve (T.root.rmp)

Let us provide some intuitions as to why RMPﬂow works.
Here we take an optimization viewpoint recently made in [5]
(rather than the common geometric control viewpoint of
RMPﬂow used in the literature) and show the optimization
problem that RMPﬂow effectively solves in the pullback
operation when generating the multi-task control policy. This
insight explains the properties of RMPﬂow, which is more
directly related to our proposed algorithm RMP2, without going
through its complex algorithmic steps. The message passing
procedure of RMPﬂow is listed in Algorithm 1 and a detailed
description can be found in Appendix A.

Consider an RMP-tree with a set of nodes V. Let L :=
{lk}K
k=1 ⊂ V be the set of leaf nodes and r be the root node.
Cheng [5, Chapter 11.7] observed that there is a connection
between the message passing algorithm of RMPﬂow and sparse
linear solvers: the desired acceleration of the RMPﬂow policy
π(q, ˙q) is the solution to the following least squares problem,
and the message passing routine of RMPﬂow effectively uses
the duality of (1) and the sparsity in the task map to efﬁciently
compute its solution.

min
{av:v∈V}

s.t.

K
(cid:88)

1
2

(cid:107)alk − ad

k(cid:107)2

Mk

,

k=1
av = Jv;uau + ˙Jv;u ˙xu,

(1)

(2)

∀ v ∈ V \ r,

u = parent(v)

where, for a leaf node lk, ad
k and Mk together are a leaf-node
RMP, Jv;u denotes the Jacobian of the task map from node u
to node v, and ˙Jv;u is the time-derivative of Jacobian Jv;u. The
objective in (1) is the sum of deviation between the acceleration
alk on a leaf space and the desired one, ad
k, weighted by
the importance matrix Mk. The constraints (2) enforce the
accelerations to be consistent with the maps between spaces.
In other words, the leaf-node RMPs in RMPﬂow deﬁnes a
trade-off between different subtask control schemes and the
policy of RMPﬂow is the optimal solution that can be realized
under the geometric constraints between the subtask spaces
and the conﬁguration space.

Implementing a system for learning RMPﬂow policies,
however, can be computationally challenging, because the user

needs to implement the complex message passing algorithm and
the data structure described in Appendix A (which we omitted
here due to its complexity!). Moreover, the user may need to
trace the gradient ﬂow through this large computational graph.
This difﬁculty has limited the applicability of existing work of
end-to-end learning of RMPﬂow policies [21, 22, 27, 29].
Improving the efﬁciency and simplicity of implementing
RMPﬂow policies is the main objective of our paper.

BASED ON AUTOMATIC DIFFERENTIATION

III. RMP2
the
We propose an alternate algorithm to implement
RMPﬂow policy. Our new algorithm, RMP2, works by properly
calling the basic oracles (such as evaluation and the Gradient
Oracle) of an automatic differentiation library, without using
the message passing algorithm of RMPﬂow in Appendix A. The
result is an easy-to-use and computationally efﬁcient framework
suitable for end-to-end learning RMPﬂow policies.

A. Key Idea

We observe that the constrained optimization problem (1)–
(2) is equivalent to the following unconstrained least-squares
optimization problem if the constraints are aggregated:

min
r∈Rd
a(cid:48)

K
(cid:88)

k=1

1
2

(cid:107)Jlk;ra(cid:48)

r + ˙Jlk;r ˙q − ad

k(cid:107)2

Mk

,

(3)

where Jlk;r is the Jacobian matrix of the subtask map from the
root node r to the kth leaf node lk. Note that the Jacobians
and velocities here are treated as constants in the optimization
as they are only dependent on the state (not the accelerations).
This observation implies that we can compute the RMPﬂow
policy by solving (3), which has the following closed-form
solution:

Algorithm 2 Jacobian-vector product [10] jvp(v,u,w)

1: Input: u, v, w
2: Return: (∂uv) w
3: λ ← 1 // dummy variable for reverse accumulation
4: g ← gradient(λ(cid:62)v,u) // sum of partial derivatives
5: compute Jacobian-vector product

(∂uv) w ← gradient(g(cid:62)v,λ)

Algorithm 3 RMP2
1: Input:
2: Return: motion policy π(q, ˙q)

root state (q, ˙q), task_map, rmp_eval

// forward pass

3: {xk}K
4: { ˙xk}K
5: {ck}K

k=1 ← task_map(q)
k=1 ← jvp({xk}K
k=1 ← jvp({ ˙xk}K

k=1,q, ˙q) // leaf node velocity
k=1,q, ˙q) // curvature terms

// evaluate leaf RMPs
k)}K

6: {(Mk, ad

k=1 ← rmp_eval({(xk, ˙xk)}K

k=1)

// backward pass

7: q(cid:48) = copy(q), q(cid:48)(cid:48) = copy(q) // copies without gradient
8: {x(cid:48)

k=1 ← task_map(q(cid:48)), {x(cid:48)(cid:48)

k=1 ← task_map(q(cid:48)(cid:48)) //

k }K

k}K

mirrored images
k=1(x(cid:48)

9: r ← (cid:80)K

auxiliary variables

k)(cid:62)Mkx(cid:48)(cid:48)
k ,

s ← (cid:80)K

k=1(x(cid:48)

k)(cid:62)Mk(ad

k − ck) //

10: Mr ← jacobian(gradient(r,q),q(cid:48)),
11: fr ← gradient(s,q) // root force
// resolve for the motion policy
r fr

12: π(q, ˙q) ← M†

// root matrix

needed, whereas RMPﬂow requires the user to specify an
RMP-tree and implement the message passing algorithm on it.
RMP2 uses the following common functionalities provided

(cid:33)

by automatic differentiation libraries:

ar =

(cid:32) K
(cid:88)

k=1

(cid:124)

J(cid:62)
lk;rMkJlk;r

(cid:123)(cid:122)
M†
r

(cid:33)†

(cid:32) K
(cid:88)

k=1

(cid:125)

(cid:124)

lk;rMk(ad
J(cid:62)

k − ˙Jlk;r ˙q)

.

(cid:123)(cid:122)
fr

(cid:125)

(4)

This means that if we can compute the solution in (4) efﬁciently
for a large set of sparsely-connected manifolds, then we can
realize the RMPﬂow Policy without the RMPﬂow algorithm
(in Algorithm 1).

B. RMP2 based on Reverse Accumulation

We propose RMP2 (Algorithm 3), an efﬁcient technique
that computes the closed-form solution (4) using automatic
differentiable libraries, which are commonly used in modern
machine learning pipelines. As we will show in Appendix B,
RMP2 has the same time complexity of as RMPﬂow [6] while
enjoying a smaller memory footprint. More importantly, RMP2
provides a more simple and intuitive interface for learning.

In automatic differentiation libraries, a computation graph
is automatically built as transform maps are speciﬁed, and
derivatives are computed through message passing on the
computation graph. RMP2 leverages this feature so that no
additional data structure and message passing routine are

• gradient(s,u): the gradient operator. It computes the
gradient of scalar graph output s ∈ R with respect to
graph input vector u ∈ Rn through back-propagation;
• jacobian(v,u): the Jacobian operator. It computes the
Jacobian matrix ∂uv ∈ Rm×n through back-propagation;
jacobian is equivalent to multiple calls of gradient.
• jvp(v,u,w): the Jacobian-vector product. It computes
(∂uv) w ∈ Rm given graph input vector u ∈ Rn, graph
output vector v ∈ Rm, and an addition vector w ∈ Rn.
The Jacobian-vector product can be efﬁciently realized by
gradient (see Algorithm 2) using a technique, called
reverse accumulation [10] (also known as double back-
ward). The algorithm computes Jacobian-vector product
through 2 passes. An auxiliary all ones vector λ is created
and the gradient with respect to λ is tracked (line 2).
By using the gradient, jacobian, jvp operators,
RMP2 in Algorithm 3 efﬁciently can compute quantities needed
for evaluating (4). In the forward pass of RMP2 (Algorithm 3,
line 3–5), RMP2 evaluate subtask maps and compute their
velocities and curvature terms using Jacobian-vector products.
Speciﬁcally, for the k-th subtask map ψlk;r : q (cid:55)→ xk (which
we may think as the map from the joint space of a robot

manipulator to the workspace), the velocity ˙xk := Jlk;r ˙q and
curvature term ck := ˙Jlk;r ˙q on the subtask space can be
computed through Jacobian-vector products:

˙xk = jvp(xk,q, ˙q),

and ck = jvp( ˙xk,q, ˙q).

(5)

Using {(xk, ˙xk)}, RMP2 then evaluates the values of the leaf
RMPs (line 6), which implicitly deﬁne the objective of the
weighted least squares problem in (3). Next, in the backward
pass (line 7–11), RMP2 computes the pullback force fr and
importance weight matrix Mr in (4) using reverse accumulation
(i.e., the technique used in jvp in Algorithm 2). This is
accomplished by creating auxiliary variables q(cid:48) and q(cid:48)(cid:48) (which
have the same numerical value as q but are different nodes
in the computational graph of automatic differentiation) and
their mirrored task images (line 7 and 8), and then querying
the gradients and Jacobians:

fr = gradient

(cid:16) K
(cid:88)

(x(cid:48)

k)(cid:62)Mk(ad

k − ck),q(cid:48)(cid:17)
,

Mr = jacobian

k=1

(cid:16)
gradient(cid:0)

K
(cid:88)

(x(cid:48)

k)(cid:62)Mkx(cid:48)(cid:48)

k ,q(cid:48)(cid:1),q(cid:48)(cid:48)(cid:17)

(6)

,

k=1

where x(cid:48)

k = ψk(q(cid:48)), x(cid:48)(cid:48)
C. Complexity of RMP2

k = ψk(q(cid:48)(cid:48)), and q = q(cid:48) = q(cid:48)(cid:48).

In Appendix B, we analyze the time and space complexities
of RMP2. We show that RMP2 has a time complexity of
O(N bd3) and a memory complexity of O(N d + Ld2), where
N is the total number of nodes, L is the number of leaf nodes,
b is the maximum branching factor, and d is the maximum
dimension of nodes. In comparison, we prove in Appendix C
that the original RMPﬂow algorithm (Algorithm 1) by [6] has
a time complexity of O(N bd3) and a worse space complexity
of O(N d2 + Ld2). Please see Table I for a summary.

D. Discussion: A Naïve Alternative Algorithm

With the RMPﬂow policy expression in (4), one may attempt
to explicitly compute the matrices and vectors listed in (4) using
automatic differentiation and then combine them to compute
the RMPﬂow policy. This idea leads to a conceptually simpler
algorithm, shown in Algorithm 4, which directly computes the
Jacobians for the task maps through the jacobian operator
provided by the automatic differentiation library, and then
compute the root RMPs Mr and fr based on (4).

However, this naïve approach turns out to be not as efﬁcient
as RMP2 and RMPﬂow. Because the Jacobian matrix Jk :=
Jlk;r is constructed here (line 4 in Algorithm 4), the time
complexity of the naïve algorithm is O(L) times larger than
RMP2, where L is the number of the leaf nodes. For a binary
tree with N nodes, this means the time complexity is in O(N 2),
not the O(N ) of RMP2 and RMPﬂow.

Moreover,

this naïve algorithm also has a large space
complexity of O(N Ld2). This large space usage is created by
the computation of the curvature term: the curvature term here
is computed through differentiating velocities { ˙xk} that are
computed by the explicit Jacobian vector product in line 5; as

Algorithm 4 A Naïve Implementation
1: Input:
2: Return: motion policy π(q, ˙q)

root state (q, ˙q), task_map, rmp_eval

// forward pass

3: {xk}K
4: {Jk}K
5: { ˙xk}K
6: {ck}K

k=1 ← task_map(q)
k=1 ← jacobian({xk}K
k=1 ← {Jk ˙qk}K
k=1 ← jvp({ ˙xk}K

k=1,q)

k=1 // leaf space velocity

k=1,q, ˙q)// curvature terms

// evaluate leaf RMPs

7: compute leaf RMPs
k)}K

{(Mk, ad

k=1 ← rmp_eval({(xk, ˙xk)}K

k=1)

// backward pass
8: compute root RMP
Mr ← (cid:80)K
k=1 J(cid:62)
// resolve for the motion policy
r fr

9: π(q, ˙q) ← M†

k MkJk,

fr ← (cid:80)K

k=1 J(cid:62)

k Mk(ad

k − ck)

a result, the intermediate graph created by the Jacobian needs
to be stored, which is the source of high memory usage. A
simple to ﬁx this memory usage is to compute the curvature
term instead by two calls of Jacobian-vector-product (as in
RMP2). This modiﬁed algorithm has the time complexity of
O(N bd3L) (still O(L) times larger than RMP2 and RMPﬂow)
but an space complexity of O(N d + Ld2) (same as RMP2).
E. Key Beneﬁts of RMP2

Simpler User Interface: The major beneﬁt of RMP2 is
that it is much easier to implement and apply than RMPﬂow,
while producing the same policy and having the same time
complexity. For RMP2, the user no longer needs to construct
the RMP-tree data structure (just like implementing a neural
network architecture from scratch) or implement the message
passing algorithm. Instead, the user only needs to specify
the task maps with automatic differentiation libraries, and the
policy can be computed through standard operators in automatic
differentiation libraries. See Appendix E for a case study.

More General Taskmaps: Another beneﬁt of RMP2 is that
it supports task maps described by arbitrary directed acyclic
graphs (DAGs), whereas RMPﬂow is limited to tree-structured
task maps. While Cheng et al. [6] show that every task map
has a tree representation, not all motion control problems
have an intuitive RMP-tree representation (e.g. multi-robot
control [19]). If they are implemented using a tree structure,
extra high-dimensional nodes would be induced and the user
interface becomes tedious.

Differentiable Policies: Since RMP2 is implemented using
the automatic differentiation libraries, computational graph can
be automatically constructed while calculating the policy. This
allows for convenient gradient calculation of any function with
respect to any parameters in, e.g., parameters used to describe
task maps and RMPs. This fully differentiable structure is
useful to end-to-end learning of these parameters in many
scenarios.

Smaller Memory Footprint: As is analyzed in Appendix B,
RMP2 has a memory footprint of O(N d + Ld2), which is

smaller than RMPﬂow (see Table I).

In summary, RMP2 is an efﬁcient algorithm that is easier
to implement and apply, while providing a more convenient
interface for learning applications.

IV. RMP2

FOR LEARNING

In this section, we discuss various options of using RMP2
to parameterize structured policies for learning. We note that
some examples below have already been explored by existing
work using the RMPﬂow algorithm. However, in most cases,
realizing these ideas with the new RMP2 algorithm instead of
the origin message passing routine of the RMPﬂow algorithm
would largely simplify the setup and implementation, as RMP2
provides a more natural interface for learning applications.
Moreover, RMP2 enables graph-structured policies, whereas the
RMPﬂow algorithm works only with tree-structured policies.

A. Parameterizing RMP2 Policies

RMP2 policies are alternate parameterizations of RMPﬂow
policies. They differ only in the way how (4) is computed
(RMP2 policies use automatic differentiation whereas RMPﬂow
policies use the message passing routine in the RMPﬂow
algorithm). These two paramertizations therefore have the same
representation power, but potentially the RMP2 poclies are more
computationally efﬁcient, because the tree structure used in
RMPﬂow may not be the most natural way to describe the
task relationship.

Learnable leaf RMPs: One way to parameterize RMP2
policies is through parameterizing leaf RMPs in RMP2. There
have been existing work exploring various ways to represent
RMPs with neural networks so that the resulting policy can
have certain theoretical properties, e.g. positive-deﬁnite weight
matrices, Lyapunov-type stability guarantees, etc. [22, 27]

Learnable task maps: Perhaps less obviously, one can also
learn the task maps. For example, existing work has developed
task map learning techniques such that the latent space policies
take in simple forms or are easier to be learned [28, 37]. These
representation learning techniques can be easily realized by
RMP2 for learning RMP2 polices.

B. Learning Setups

Because RMP2 is based on automatic differentiation, it can
be implemented naturally with the typical machine learning
pipelines. Below we discuss common scenarios.

Supervised Learning: When there is an expert policy, one
common scenario for robot learning is behavior cloning [26],
which minimizes the empirical difference between the learner
and expert policies on a dataset collected by the expert policy.
Gradient-based algorithms are often used to minimize the error,
which requires computing the derivative of the acceleration-
based policy with respect to the parameters. Due to the difﬁculty
in differentiating through the RMPﬂow algorithm, most ex-
isting work on learning the RMPs in RMPﬂow policies with
supervised learning either differentiates through an approximate
algorithm (e.g. without the curvature terms) [21], or learn with
a trivial task map [27]. By contrast, using our proposed RMP2

algorithm, we can easily combine gradient-based learning
algorithm with arbitrary task or RMP parameterizations.

Reinforcement Learning:

In reinforcement learning (RL)
applications, one can choose whether to differentiate through
the RMP2 algorithm: one can either choose RMP2 as part of
the policy, or as a component of the environment dynamics.
This choice can be made in consideration of the policy
parameterization. For example, if a large number of leaf RMPs
are parameterized, it could be beneﬁcial to consider RMP2 as
part of the policy and differentiate through it, because otherwise
it will result in a high-dimensional action space for RL. On the
other hand, if RMP2 is considered as part of the environment,
the output of the parameterized RMPs or parameterized task
maps are treated as the action in RL, and there is no need to
differentiate through RMP2. When there is only a single low-
dimensional leaf RMP to be learned, it might be convenient
to consider RMP2 as part of the environment so that policy
update is faster.b Recently Aljalbout et al. [2] explored learning
collision avoidance RMPs with RMPﬂow as a component of
the environment.

C. Learning with Residuals

For many robotics tasks, hand-crafted RMPs [6] can provide
a possibly sub-optimal but informative prior solution to the
task or some subtasks (e.g. avoiding collision with obstacles,
respecting joint limits, etc.). In many cases, making use of
these hand-crafted RMPs within RMP2 policies can beneﬁt
learning, as it could provide a reasonable initialization for the
learner and, for RL, an initial state visitation distribution with
higher rewards.

Residual Acceleration Learning:

Perhaps the most
straightforward solution is to learn the residual policy of the
RMP2 policy using universal functional approximators, e.g.
neural networks [14]. As we show in the experiments, this can
often provide a signiﬁcant improvement to the performance
compared to randomly initialized policies, especially when the
tasks are more challenging.

Residual RMP learning: Another option is to learn a
residual leaf RMP with a universal functional approximator
(i.e. the leaf RMP is initialized as the hand-crafted RMP).
In this way, the structure of the RMP2 policy is preserved.
As is shown in the experiments, residual RMP learning can
perform signiﬁcantly better than randomly initialized neural
network policies, and can sometimes learn faster than the
residual acceleration learning approach.

A. Three-Link Robot Reaching

V. EXPERIMENTS

We ﬁrst consider a three-link robot simulated by the PyBullet
physics engine [11]. The robot has 3 links, each of length 0.25
m. The workspace of the robot is a 2-dimensional disk of
radius 0.75 m. The z-coordinates of all links are different so
that the links cannot collide into one another. The objective
here is for the robot to move the tip of the last link (i.e., the

bThe time for differentiating through RMP2 is a constant factor more than

the time required for computing RMP2, which is still reasonably fast.

Fig. 1: Mean episode reward over training iterations for the three-link robot reaching task. See text for details.

Fig. 2: Percentage of safe episodes over training iterations for the Franka robot reaching task. See text for details.

Fig. 3: (Left) PyBullet simulation for the three-link robot reaching task and the Franka robot reachng task. (Right) Mean
episode reward and safe episode percentage over training iterations for the Franka reaching task.

end-effector of the robot) to a randomly generated goal location
while avoiding randomly generated cylinders, as shown in Fig. 3
(left). Acceleration-based control is realized through a low-level
PD tracking controller, where the acceleration motion policy π
generates the desired reference state for the PD controller. The
robot does not have joint limits but have joint velocity limit
of 1.0 rad/s for all joints.

Environment Setups The robot is initialized at a random
conﬁguration within a small range (±0.1rad) around the zero-
conﬁguration (all links pointing right) and at a random low
joint velocity within [−0.005, 0.005] rad/s.

The (2-dimensional) center of the base of each obstacle is
sampled from an annulus centered at the origin with outer
radius 0.9 m and inner radius 0.4 m, and the radius is sampled
uniformly from [0.05, 0.1] m. The height of the cylinder is ﬁxed
at 0.5 m, which would result in collision if the x, y-coordinates
of the robot intersects with a cylinder. The consideration
behind this obstacle conﬁguration is that their intersection
with the workspace is usually non-zero, and they would not
result in a conﬁguration where the goal is not achievable.
The initial conﬁgurations of the environment also ensures a
minimum of 0.1 m between the goal and any obstacles as
well as that between the robot and any obstacles (otherwise,
the initialization is rejected, and the goal and obstacle(s) are

re-sampled).

We consider three environment setups for the three-link robot

with increasing difﬁculty:

• Env 1 (small goal range; 1 obstacle): the goal is sampled
from the intersection of an octants (sector with central
angle π/2) at the origin and an annulus with outer radius
0.275 m and inner radius 0.475 m. One obstacle is
sampled from the procedure described above;

• Env 2 (small goal range; 3 obstacles): the goal is sampled
from the same region as Env 1. Three obstacles are
sampled through the same procedure;

• Env 3 (large goal range; 3 obstacles): the goal is sampled
from the intersection of the left half-disk and an annulus
with outer radius 0.125 m and inner radius 0.625 m.

than Env 1 as there are more
Env 2 is more difﬁcult
obstacles. Env 3 is the most complicated scenario as it has a
larger range of random goals, which is known to be challenging
for RL algorithms [3]. Moreover, although the goals here are
generally closer than the previous 2 environment setups, this
also increase the frequency of the scenarios where the obstacles
are directly obstructing the way to the goal, requiring a more
sophisticated policy.

Inspired by [17], we deﬁne the reward function as, r =

0200400Iteration0200400MeanRewardSmallGoalRange,1ObstacleNNNN-RESIDUALRMP-RESIDUAL0200400Iteration0100200300MeanRewardSmallGoalRange,3Obstacles0200400Iteration0100200MeanRewardLargeGoalRange,3Obstacles0200400Iteration050100SafePercentageSmallGoalRange,1ObstacleNNNN-RESIDUALRMP-RESIDUAL0200400Iteration050100SafePercentageSmallGoalRange,3Obstacles0200400Iteration050100SafePercentageLargeGoalRange,3Obstacles0200400Iteration0200400MeanRewardNNNN-RESIDUALRMP-RESIDUAL0200400Iteration050100SafePercentage(cid:17)

(cid:16) (cid:107)x−g(cid:107)2
2σ2

2

δ

+ (cid:80)N

i=1 max (cid:0)0, 1 − di
(cid:1) − λ(cid:107)τ (cid:107)2, where x is
exp
the position of the end-effector of the robot, g is the goal
position, di is the distance between the robot and the ith
obstacle, and τ is the torque applied to the robot by the low-
level PD controller. The scalars σ, δ, and λ are the characteristic
length scale of the goal reward, characteristic length scale of
the obstacle cost, and the multiplier for the actuation cost. We
choose σ = 0.1, δ = 0.05, and λ = 1 × 10−5. Further, we clip
the reward if it is smaller than −5 so that the reward is in the
range of [−5, 1] for each step.

The horizon of each episode is 600 steps, giving the episode
reward a range of [−3000, 600]. For each step, the acceleration
command is applied to the low-level PD-controller, and the
simulation proceeds for 0.0125s (simulation time). Therefore,
each episode is 7.5s (simulation time) of policy rollout. The
episode can end early if the robot collides with an obstacle.

Policies We compare the results for learning with the fol-
lowing 3 types of policies, all implemented in TensorFlow [1].
NN: A randomly initialized 3-layer neural network policy
with relu activation function and hidden layers of sizes 256
and 128, respectively. The input to the neural network policy
consists of [sin(q), cos(q), ˙q, g − x, {vi}i, {oi}i], where vi
is the vector pointing from the i-th obstacle and its closest
point on the robot, and oi denotes the center position and
radius of the i-th obstacle. The output of the policy is the
3-dimensional joint acceleration.

NN-RESIDUAL: A residual neural network policy to a
hand-designed RMP2 policy. The input and neural network
architecture here is the same as the randomly-initialized case
above. However, the joint acceleration now is the sum of the
output of the residual policy and the hand-designed RMP2
policy. The hand-designed RMP2 policy consists of a joint
damping RMP, a joint velocity limit RMP, collision avoidance
RMPs, and a goal attractor RMP [6].

RMP-RESIDUAL: A residual RMP policy on the 2-
dimensional end-effector space, which is the residual to a
hand-designed goal attractor RMP (same as the one used for
NN-RESIDUAL). The residual RMP policy is parameterized
as a 3-layer neural network with hidden layer sizes 128 and 64.
We use elu activation function for the neural network. The input
to the residual RMP policy consists of [x, ˙x, g, {oi}i], which
are the end effector position, velocity, and the information
of goal and obstacle(s). The output of the neural network
is (the concatenation of) a matrix Ar ∈ R2×2 and a residual
acceleration vector ar ∈ R2. Let (Ma, aa) be the output for the
hand-designed attractor, the output of the residual RMP policy
is then, M = (Ar + chol(Ma))(Ar + chol(Ma))(cid:62) and
a = ar + aa, where chol(·) is the lower-triangular Cholesky
decomposition of the matrix. This parameterization ensures that
the importance weight M is always positive-semideﬁnite, and
the output is close to the hand-designed RMP when the neural
network is initialized with weights close to zero. The output of
the residual RMP policy, is combined with other hand-designed
RMPs (the same set of RMPs with NN-RESIDUAL) with
RMP2 to produce the joint acceleration. Since we are learning
a low-dimensional RMP, as is discussed in Section IV, it is

more convenient to consider RMP2 as part of the environment
so that the policy update is faster.

Training Details We train the three policies under the
three environment setups by Proximal Policy Optimization
(PPO) [34], implemented by RLlib [20], for 500 training
iterations. For each iteration, we collect a batch of 67312
interactions with the environment (112 episodes if there is
not collision). We use a learning rate of 5 × 10−5, PPO clip
parameter of 0.2. To compute the policy gradient, we use
Generalized Advantage Estimation (GAE) [33] with λ = 0.99.
The value function is parameterized by a neural network with
2 hidden layers (of sizes 256 and 128, respectively) and tanh
activation function.

Results The mean episode reward and percentage of safe
episodes over training iterations for the three environment
setups are shown in Fig. 1 and Fig. 2, respectively. The curve
and the shaded region show the average, and the region within
1 standard deviation from the mean over 4 random seeds.

For env 1 with small goal range and 1 obstacle, all three
policies manage to achieve high reward of around 400 (meaning
that, on average, the robot stays very close to the goal for at
least 400 steps out of the 600 steps) with a similar number
of iterations. The random-initialized neural network (NN)
conducts a large number of unsafe exploration, especially
during the ﬁrst 200 iterations, as shown in Fig. 2 (left). The
other two policies manage to learn a good policy without
many collisions as the red and blue curves stay close to 100
throughout training.

For env 2 with 3 obstacles, the randomly-initialized neural
network policy (NN) learns slightly faster than the other two
polices, though it has a higher variance (shown by the large
gray shaded region in Fig. 1 (middle)). Notably, although
the reward seems high, the resulting policies are not safe,
as shown in Fig. 2 (middle). This reﬂects the difﬁculty of
reward design: as high reward policies do not necessarily have
desirable behaviors. On the contrary, the residual RMP policy
(RMP-RESIDUAL) achieves similar reward but is able to keep
the number of collisions low. The residual neural network
policy (NN-RESIDUAL) improves rather slowly, possibly due
to the lack of knowledge about the internal structure of RMP2.
For env 3 with large goal range and 3 obstacles, the
neural netowrk policy (NN) struggles to achieve reasonable
performance under 500 training iterations, and the collision rate
remains high. The performance improvement for the reisual
neural network policy (NN-RESIDUAL) is also slow, with
a slope similar to the neural network policy, though it starts
with a higher rewards. The residual RMP policy manages to
improve the performance by more than one-fold.

Remark: One may notice that the initial reward for the
residual policies is different for each experiment setup. For
example, the initial rewards for env 3 is signiﬁcantly higher
than env 2. This is because we used the same hand-designed
RMP policy for all three setups, and in env 3, the goals are
generally initialized closer to the robot than the other two steps
(as discussed, this does not make env 3 easier however).

B. Franka Robot Reaching

We additionally train the three aforementioned policies on a
simulated 7-degree-of-freedom Franka manipulator on PyBullet.
The environment setup is similar to the three-link robot reaching
task, although the initial conﬁguration is a centered position,
as shown in 3. We randomly sample 3 ball obstacles, where
the center is sampled a half-torus of major radius 0.5 m, minor
radius 0.3 m, and height 0.5 m; and the radius is uniformly
sampled from [0.05, 0.1] m. The goal is sampled from the
same half-torus with a ball centered at the initial end-effector
position and radius 0.5 m excluded, meaning that the distance
between the goal and initial end-effector position should be
at least 0.5 m. The Franka reaching task is more challenging
than the three-link robot reaching task as the states are of
higher-dimension; however, it is easier in the sense that it has
higher degrees of freedom to avoid collision with obstacles.

Results The mean episode reward and safe percentage of the
three polices for the Franka reaching task is shown in Fig. 3
(right 2). Again, the neural network policy (NN) struggles
to learn a good policy and avoid collision with obstacles.
The residual neural network policy (NN-RESIDUAL) achieves
slightly higher reward than the residual RMP policy (RMP-
RESIDUAL), possibly because it has higher degree-of-freedoms
to control the robot and the learning of the residual RMP
policy (RMP-RESIDUAL) has not fully converged. Notably,
the residual RMP policy (RMP-RESIDUAL) maintain zero
collisions throughout the training process. The visualization
of the learned policies for both experiments can be found at
https://youtu.be/dliQ-jsYhgI.

REFERENCES

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin,
S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard,
Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg,
D. Mané, R. Monga, S. Moore, D. Murray, C. Olah,
M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Tal-
war, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas,
O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu,
and X. Zheng. TensorFlow: Large-scale machine learning
on heterogeneous systems, 2015. URL http://tensorﬂow.
org/. Software available from tensorﬂow.org.

[2] E. Aljalbout, J. Chen, K. Ritt, M. Ulmer, and S. Had-
dadin. Learning vision-based reactive policies for obstacle
avoidance. arXiv preprint arXiv:2010.16298, 2020.
[3] P. Aumjaud, D. McAuliffe, F. J. Rodríguez-Lera, and
P. Cardiff. Reinforcement learning experiments and bench-
mark for solving robotic reaching tasks. In Workshop of
Physical Agents, pages 318–331. Springer, 2020.

[4] F. Bullo and A. D. Lewis. Geometric control of
mechanical systems: modeling, analysis, and design for
simple mechanical control systems. Springer New York,
2005.

[5] C. A. Cheng. Efﬁcient and principled robot learning:
theory and algorithms. PhD thesis, Georgia Institute of
Technology, 2020.

[6] C.-A. Cheng, M. Mukadam, J. Issac, S. Birchﬁeld, D. Fox,
B. Boots, and N. Ratliff. RMPﬂow: A computational
graph for automatic motion policy generation. Proceed-
ings of the 13th Annual Workshop on the Algorithmic
Foundations of Robotics (WAFR), 2018.

[7] C.-A. Cheng, M. Mukadam, J. Issac, S. Birchﬁeld,
D. Fox, B. Boots, and N. Ratliff. RMPﬂow: A geometric
framework for generation of multi-task motion policies.
arXiv preprint arXiv:2007.14256, 2020.

[8] J. Choi, F. Castañeda, C. J. Tomlin, and K. Sreenath. Rein-
forcement learning for safety-critical control under model
uncertainty, using control Lyapunov functions and control
barrier functions. arXiv preprint arXiv:2004.07584, 2020.
[9] Y. Chow, O. Nachum, A. Faust, E. Duenez-Guzman,
and M. Ghavamzadeh. Lyapunov-based safe policy
arXiv preprint
optimization for continuous control.
arXiv:1901.10031, 2019.

[10] B. Christianson. Automatic hessians by reverse accumula-
tion. IMA Journal of Numerical Analysis, 12(2):135–150,
1992.

[11] E. Coumans and Y. Bai. PyBullet, a python module
for physics simulation in robotics, games and machine
learning, 2017.

[12] G. Dalal, K. Dvijotham, M. Vecerik, T. Hester, C. Padu-
raru, and Y. Tassa. Safe exploration in continuous action
spaces. arXiv preprint arXiv:1801.08757, 2018.

[13] A. Griewank and A. Walther. Evaluating derivatives:
principles and techniques of algorithmic differentiation.
SIAM, 2008.

[14] T. Johannink, S. Bahl, A. Nair, J. Luo, A. Kumar,
M. Loskyll, J. A. Ojea, E. Solowjow, and S. Levine.
Residual reinforcement learning for robot control. In 2019
International Conference on Robotics and Automation
(ICRA), pages 6023–6029. IEEE, 2019.

[15] D. Kappler, F. Meier, J. Issac, J. Mainprice, C. Garcia Ci-
fuentes, M. Wüthrich, V. Berenz, S. Schaal, N. Ratliff,
and J. Bohg. Real-time perception meets reactive motion
generation. IEEE Robotics and Automation Letters, 3(3):
1864–1871, 2018. URL https://arxiv.org/abs/1703.03512.
[16] H. K. Khalil and J. W. Grizzle. Nonlinear systems,
volume 3. Prentice hall Upper Saddle River, NJ, 2002.
[17] V. Kumar, D. Hoeller, B. Sundaralingam, J. Tremblay, and
S. Birchﬁeld. Joint space control via deep reinforcement
learning. arXiv preprint arXiv:2011.06332, 2020.
[18] A. Li, C.-A. Cheng, B. Boots, and M. Egerstedt. Stable,
concurrent controller composition for multi-objective
robotic tasks. In 2019 IEEE 58th Conference on Decision
and Control (CDC), pages 1144–1151. IEEE, 2019.
[19] A. Li, M. Mukadam, M. Egerstedt, and B. Boots. Multi-
objective policy generation for multi-robot systems using
Riemannian motion policies. In International Symposium
on Robotics Research, 2019.

[20] E. Liang, R. Liaw, R. Nishihara, P. Moritz, R. Fox,
K. Goldberg, J. Gonzalez, M. Jordan, and I. Stoica. RLlib:
Abstractions for distributed reinforcement learning. In
International Conference on Machine Learning, pages

arXiv preprint arXiv:1707.06347, 2017.

[35] B. Siciliano, L. Sciavicco, L. Villani, and G. Oriolo.
Robotics: modelling, planning and control. Springer
Science & Business Media, 2010.

[36] G. Sutanto, N. Ratliff, B. Sundaralingam, Y. Chebotar,
Z. Su, A. Handa, and D. Fox. Learning latent space
In 2019 International
dynamics for tactile servoing.
Conference on Robotics and Automation (ICRA), 2019.
[37] J. Urain, M. Ginesi, D. Tateo, and J. Peters. Imitationﬂow:
Learning deep stable stochastic dynamic systems by
normalizing ﬂows. arXiv preprint arXiv:2010.13129,
2020.

[38] B. Wingo, C.-A. Cheng, M. Murtaza, M. Zafar, and
S. Hutchinson. Extending Riemmanian motion policies
to a class of underactuated wheeled-inverted-pendulum
In 2020 IEEE International Conference on
robots.
Robotics and Automation (ICRA), pages 3967–3973. IEEE,
2020.

3053–3062. PMLR, 2018.

[21] X. Meng, N. Ratliff, Y. Xiang, and D. Fox. Neural
autonomous navigation with Riemannian motion policy.
In 2019 International Conference on Robotics and Au-
tomation (ICRA), pages 8860–8866. IEEE, 2019.
[22] M. Mukadam, C.-A. Cheng, D. Fox, B. Boots, and
N. Ratliff. Riemannian motion policy fusion through
learnable Lyapunov function reshaping. In Conference
on Robot Learning, pages 204–219, 2019.

[23] J. Nakanishi, R. Cory, M. Mistry, J. Peters, and S. Schaal.
Operational space control: A theoretical and empirical
The International Journal of Robotics
comparison.
Research, 27(6):737–757, 2008.

[24] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang,
Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer.
Automatic differentiation in PyTorch. In NIPS Autodiff
Workshop, 2017.

[25] C. Paxton, N. Ratliff, C. Eppner, and D. Fox. Representing
robot task plans as robust logical-dynamical systems. In
2019 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), 2019.

[26] D. A. Pomerleau. ALVINN: an autonomous land vehicle
In Proceedings of the 1st Inter-
in a neural network.
national Conference on Neural Information Processing
Systems, pages 305–313, 1988.

[27] M. A. Rana, A. Li, H. Ravichandar, M. Mukadam,
S. Chernova, D. Fox, B. Boots, and N. Ratliff. Learning
reactive motion policies in multiple task spaces from
human demonstrations. In Conference on Robot Learning,
2019.

[28] M. A. Rana, A. Li, D. Fox, B. Boots, F. Ramos, and
N. Ratliff. Euclideanizing ﬂows: Diffeomorphic reduction
for learning stable dynamical systems. In Learning for
Dynamics and Control, pages 630–639. PMLR, 2020.

[29] M. A. Rana, A. Li, D. Fox, S. Chernova, B. Boots, and
N. Ratliff. Towards coordinated robot motions: End-to-
end learning of motion policies on transform trees. arXiv
preprint arXiv:2012.13457, 2020.

[30] N. D. Ratliff, J. Issac, D. Kappler, S. Birchﬁeld, and
D. Fox. Riemannian motion policies. arXiv preprint
arXiv:1801.02854, 2018.

[31] S. Ross and J. A. Bagnell. Reinforcement and imitation
learning via interactive no-regret learning. arXiv preprint
arXiv:1406.5979, 2014.

[32] S. Ross, G. Gordon, and D. Bagnell. A reduction of imita-
tion learning and structured prediction to no-regret online
learning. In Proceedings of the fourteenth international
conference on artiﬁcial intelligence and statistics, pages
627–635. JMLR Workshop and Conference Proceedings,
2011.

[33] J. Schulman, P. Moritz, S. Levine, M. Jordan, and
P. Abbeel. High-dimensional continuous control us-
ing generalized advantage estimation. arXiv preprint
arXiv:1506.02438, 2015.

[34] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and
O. Klimov. Proximal policy optimization algorithms.

APPENDIX A
THE RMPFLOW ALGORITHM

APPENDIX B
COMPLEXITY ANALYSIS OF RMP2

Here we describe the message passing steps of RMPﬂow [6].
As is noted in [5], this message passing routine effectively uses
the duality of (1) and the sparsity in the task map to efﬁciently
compute ar.

The RMPﬂow algorithm (Algorithm 1) is based on two
components: 1) the RMP-tree: a directed tree encoding the
structure of the task map; and 2) the RMP-algebra: a set of
operations to propagate information across the RMP-tree.

In the RMP-tree, a node u stores the state (xu, ˙xu) on a
manifold Mu and the associated RMP (au, Mu). We deﬁne
the natural form of an RMP as [fu, Mu], where fu = Muau is
the force. An edge e represents to a smooth map ψe from the
manifold of a parent node manifold to its child node manifold.
The root node of the RMP-tree (denoted as r) and the leaf nodes
correspond to the conﬁguration space C and the subtask spaces
{Tk} on which the subtask RMPs are hosted, respectively.

The RMP-algebra comprises of three operators, which are for
propagating information on the RMP-tree. For illustration, we
consider the node u on manifold M with coordinates x and its
M child nodes (vm with coordinates ym for m = 1, . . . , M )
on the RMP-tree.
(i) pushforward propagates the state of a node (x, ˙x)
in the RMP-tree to update the states of its M child
nodes {(ym, ˙ym)}M
m=1. The state of its mth child node is
computed as (ym, ˙ym) = (ψvm;u(x), Jvm;u(x) ˙x), where
ψm is the smooth map of the edge connecting the two
nodes and Jvm;u = ∂xψvm;u is the Jacobian matrix.
(ii) pullback propagates the RMPs of the child nodes in
m=1, to the parent node

the natural form, {[fvm, Mvm]}M
as [fu, Mu]:

fu =

Mu =

M
(cid:88)

m=1
M
(cid:88)

m=1

J(cid:62)
vm;u(fvm − Mvm

˙Jvm;u ˙x),

(7)

J(cid:62)
vm;uMvmJvm;u.

The natural form of RMPs are used here since they more
efﬁcient to combine.

(iii) resolve maps an RMP from its natural form [fu, Mu]
u fu, where †

to its canonical form (au, Mu) by au = M†
denotes Moore-Penrose inverse.

RMPﬂow in Algorithm 1 computes the policy π(q(t), ˙q(t)) =
ar on the conﬁguration space C through the following proce-
dure. Given the state (q(t), ˙q(t)) of the conﬁguration space
C at time t, the pushforward operator is ﬁrst recursively
applied to the RMP-tree to propagate the states up to the leaf
nodes. Then, the subtask RMPs are evaluated on the leaf nodes
and combined recursively backward along the RMP-tree by
the pullback operator. The resolve operator is ﬁnally
applied on the root node r to compute the desired acceleration
π(q, ˙q) = ar.

In this section, we analyze the time and space complexities
of RMP2. We show that RMP2 has a time complexity of
O(N bd3) and a memory complexity of O(N d + Ld2), where
N is the total number of nodes, L is the number of leaf nodes,
b is the maximum branching factor, and d is the maximum
dimension of nodes In comparison, we prove in Appendix C
that the original RMPﬂow algorithm (Algorithm 1) by [6] has
a time complexity of O(N bd3) and a worse space complexity
of O(N d2 + Ld2). Please see Table I for a summary.

Speciﬁcally, consider a directed-acyclic-graph-structured task
map with N nodes, where each node has dimension in O(d)
and has at most b parents. We suppose that L ≤ N nodes are
leaf nodes, and that the automatic differentiation library is based
on reverse-mode automatic differentiation. We ﬁrst analyze the
complexity of task map evaluation and Jacobian-vector-product
subroutine (Algorithm 2) based on reverse accumulation in
preparation for the complexity analysis for RMP2.

Task map evaluation: For each node in the graph, the
input and output dimensions are bounded by O(bd) and O(d),
respectively. Hence, evaluating each node has a time complexity
in O(bd2). Because each node is evaluated exactly once in in
computing the full task map, the total time complexity of task
map evaluation O(N bd2). If the Gradient Oracle gradient
will be called (as in RMP2), the value of each node needs to
be stored in preparation for the gradient computation. Overall
this would require a space complexity in O(N d) to store the
values in the entire graph.

Jacobian-vector product with L output nodes: Suppose
that the output of the graph in Algorithm 2 is a collection of
L nodes in the graph. During reverse accumulation, the task
map is ﬁrst computed, which, based on the previous analysis,
has time and space complexity of O(N bd2) and O(N d),
respectively. The dummy variable λ is of size O(Ld) and
computing the inner product λ(cid:62)v requires O(Ld) computation
(i.e. it creats a new node of dimension 1 with 2L parent
nodes of dimension in O(d)). By the reverse-mode automatic
differentiation assumption, the ﬁrst backward pass on the graph
(line 4) has time complexity of O(N bd2 + Ld) = O(N bd2)
and space complexity of O(N d + Ld) = O(N d) [13]. The
ﬁnal backward pass (line 5) is on a graph of size O(N ),
as the ﬁrst backward pass creates additional O(N ) nodes.
With a similar analysis,
the second backward pass have
time complexity of O(N bd2 + Ld) = O(N bd2) and space
complexity of O(N d + Ld) = O(N d). Therefore, the time
and space complexity of Algorithm 2 is O(N bd2) and O(N d),
respectively.

Forward pass: The complexity of line 3–5 in Algorithm 3
follow the precursor analyses above. Here the computation
graph is always of size O(N ) (the original task map is in O(N )
and each call of Gradient Oracle gradient in the Jacobian-
vector-product subroutine jvp creates additional O(N ) nodes
in the computation graph). By previous analysis, the time and
space complexity of the forward pass are O(N bd2) and O(N d),

(a) task map structure

(b) Policy structure

(c) Computation time

Fig. 4: (a) task map and (b) policy structure for benchmarking the complexity of the algorithms. (c) The computation time of
RMP2 and the naïve implementation (direct) on chain-structured graphs with varying lengths. The results show that RMP2 has
a linear time complexity whereas the naïve implementation (direct) has a super-linearly time complexity.

respectively.

Leaf evaluation: Assume, for each leaf node, O(d3)
computation is needed for evaluating the weight matrix and
O(d2) for acceleration in an RMP. The leaf evaluation step
then have time complexity of O(Ld3) and space complexity
of O(Ld2 + Ld) = O(Ld2).

Backward pass: By previous analysis, task map evaluation
requires O(N bd2) computation and O(N d) space. The vector-
matrix-vector product for computing auxiliary variables r and
s has time complexity of O(Ld2). To compute the metric at
root, Mr, the ﬁrst backward pass, gradient(r,q) needs
O(N bd2) time and O(N d) space as it operates on a graph of
size O(N ) where the number of parents of each node is in
O(b)c. The jacobian operator in line 10 is done by O(d)
sequential calls of the Gradient Oracle gradient. Hence, it
has a time complexity of O(N bd3) and a space complexity
of O(N d). (Because we are not taking further derivatives, the
values of the new graphs created in calling the jacobian
operator do not need to be stored.) With a similar analysis,
computing fr requires O(N bd3) computation and O(N d) space
complexities.

Resolve: The matrix inversion has time complexity of O(d3)

and space complexity of O(d2).

In summary, the time complexity of RMP2 is O(N bd2 +
Ld3 + N bd3 + d3) = O(N bd3) and the space complexity is
O(N d + Ld2).

APPENDIX C
COMPLEXITY OF RMPFLOW
First we need to convert a graph with O(b) parent nodes into
a tree. This can be done by creating meta nodes that merge
all the parents of a node into a single parent node; inside
the mega node, each component is computed independently.
Therefore, for a fair comparison, in the following analysis,
we shall assume that in the tree version evaluating each node
would need a time complexity in O(bd2). We suppose the space
complexity to store all the nodes is still in O(N d) because the
duplicated information resulting from the creation of the meta

nodes can be handled by sharing the same memory reference
in a proper implementation [6].

Forward pass: During the forward pass, similar to the
reverse accumulation analysis, O(bd2) per node is needed
for pushforward, i.e. computing the pushforward velocity
through reverse accumulation, yielding a time complexity of
O(N bd2). The space complexity is O(N d) for storing the state
at every node.

Leaf evaluation: Same as RMP2.
Backward pass: Computing the metric in (7) requires
O(bd3) computation per node, yielding time complexity of
O(N bd3) and space complexity of O(N d2). The curvature
term ˙Jvm;u ˙x can be computed through reverse accumulation
similar to RMP2, which has time complexity of O(bd2) per
node. The matrix-vector products to compute the force requires
O(bd2) space and computation for each node.

Resolve: Same as RMP2.
Thus, the time complexity of RMPﬂow is O(N bd2 + Ld3 +
N bd3 + N bd2 + d3) = O(N bd3) and the space complexity is
O(N d + Ld2 + N d2 + bd2 + d2) = O(N d2 + Ld2).

APPENDIX D
EXPERIMENTAL VALIDATION OF TIME COMPLEXITY
We validate the time complexity analysis for RMP2 and the
naïve implementation (Algorithms 3 and 4, respectively). In
particular, we are interested in how the two algorithms scale
with respect to the number of nodes in the graph.

We consider a directed chain-like graph of length l: Each
node on the chain is connected to b leaf nodes; every node in the
graph is of dimension d. Overall such a graph has 1 + (b + 1)l
nodes, where bl nodes are leaf nodes. The task map structure
for the time complexity experiment is shown in Fig. 4. In the
experiment, we vary the length of the chain while ﬁxing the
branching factord and the dimension of the nodes (b = d = 3).
The map associated with each edge is implemented as a single-
layer neural network with tanh activation function. Both
algorithms as well as the graph structure are implemented in
TensorFlow [1]. We consider chain length in [4, 8, 12, . . . , 36].

cExcept the ﬁnal node aggregating L outputs. However, it does not change

the complexity as it adds a complexity in O(Ld2) < O(N bd2).

dThe branching factor and the chain length have similar effect to the size

of the graph and hence only the effect of chain length is evaluated.

Reshape(a) The computational graph used by RMP2

(b) An example RMP-tree which can be used by RMPﬂow

Fig. 5: (a) The computation graph automatically built by automatic differentiation libraries when computing control point
positions. RMP2 directly operates on this graph. (b) An example RMP-tree constructed for the same problem under similar
strategy as [6, Appendix D]. It introduces intermediate high-dimensional nodes as well as redundant computation. In both
ﬁgures, qi is the joint angle of the i-th joint, xi is the pose (position and orientation) of the i-th link, and x0 is the base pose.

Fig. 4(c) shows the computation time of the two algorithms.
The computation time of RMP2 increases linearly with respect
to the size of the graph whereas the naïve implementation
(direct) suffers from a super-linear growth. The computation
time reported in Fig. 4(c) is the average over 1000 runs of
the algorithms with random input on a static computational
graph in TensorFlow [1]. The constant time required to compile
the static computational graph is not included in the reported
average computation time.

APPENDIX E

VERSUS RMPFLOW: A CASE STUDY

RMP2

In this appendix, we demonstrate how RMP2 provides a more
convenient interface for the user. Consider the planar three-link
robot from the experiment section (Fig. 3). Assume that the
subtask spaces are with the positions of control points along
the robot arm. In Fig. 5, for example, there are 9 spaces, each
corresponding to the position of one control point on the robot.
This type of subtask space is useful for specifying behaviors
such as collision avoidance, where we need each control point
to avoid collision with obstacles in the environment.

Intuitively, to compute the control point positions along the
robot, we can ﬁrst compute the pose of each link, {xi}3
i=1,
through the kinematic chain of the robot, where xi ∈ SE(2)
denotes the position and orientation of the i-th link. Then, the
control point positions can be obtained through interpolating
the positions of any two adjacent links. Fig. 5(a) shows the
computational graph that is automatically built through the
above computation, where green nodes denote joint angles,
yellow nodes denote link poses, and orange nodes denote

control point positions. As is introduced in Section III, RMP2
directly uses the computational graph as the core data structure
and compute the policy through calling the Gradient Oracles
provided by automatic differentiation libraries.

In contrast, RMPﬂow (see Appendix A) requires the user to
specify a tree data structure called the RMP-tree. In the RMP-
tree structure, it is required that the states in the parent node
contain sufﬁcient information for computing the child node
states (see the pullback operator in Appendix A). Here
we use a RMP-tree structure (Fig. 5(b)) similar to what is
introduced in [6, Appendix D]. The root node includes x0, the
pose of the base, as well as all joint angles {qi}3
i=1 as we
need all these quantities to compute the control point positions.
Then, the RMP-tree branches out to compute the control points
on each link. To compute the control point positions for the
ﬁrst link, we need the posese of the base link and link 1,
(x0, x1) ∈ (SE(2))2, similarly for the other two links. This
gives us a RMP-tree structure shown in Fig. 5(b). Note that
computationally, to compute the poses of link 3, for example,
we need to compute the poses for all previous links, as is
shown in the light yellow nodes on the path for the second
and third links. Therefore, not only does the construction of
RMP-tree cost additional effort, it also introduces nodes with
high dimensions (e.g. SE(2) × R2) and redundant computation
(the forward mapping for the ﬁrst link is computed by all three
branches) under less careful design choices. These, in turn,
impair the computational efﬁciency of RMPﬂow as the time

eOnly the positions are sufﬁcient. However, we use poses here to make the

notation more compact.

complexity is a function of node dimension and node number.
Moreover, RMP2 allows us to specify complicated task maps
more easily. For example, if one would like to additionally
consider self-collision avoidance (even though it is not relevant
to the planar three-link robot). For RMP2, one can directly
compute the distance between any two control points (perhaps
from different links), creating nodes which are child nodes
to pairs of control point position nodes. However, RMPﬂow
requires the user to redesign the RMP-tree structure entirely,
creating even more intermediate nodes due to the limitation of
the tree structure.

