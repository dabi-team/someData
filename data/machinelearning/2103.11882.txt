Published as a conference paper at ICLR 2021

GENERATING ADVERSARIAL COMPUTER PROGRAMS
USING OPTIMIZED OBFUSCATIONS

Shashank Srikant1
Quanfu Fan2

Tamara Mitrovska1
Sijia Liu2,3
Gaoyuan Zhang2 Una-May O’Reilly1

Shiyu Chang2

1CSAIL, MIT
shash@mit.edu, liusiji5@msu.edu, unamay@csail.mit.edu

2MIT-IBM Watson AI Lab

3Michigan State University

1
2
0
2

r
a

M
8
1

]

G
L
.
s
c
[

1
v
2
8
8
1
1
.
3
0
1
2
:
v
i
X
r
a

ABSTRACT

Machine learning (ML) models that learn and predict properties of computer pro-
grams are increasingly being adopted and deployed.
In this work, we investi-
gate principled ways to adversarially perturb a computer program to fool such
learned models, and thus determine their adversarial robustness. We use program
obfuscations, which have conventionally been used to avoid attempts at reverse
engineering programs, as adversarial perturbations. These perturbations modify
programs in ways that do not alter their functionality but can be crafted to deceive
an ML model when making a decision. We provide a general formulation for an
adversarial program that allows applying multiple obfuscation transformations to
a program in any language. We develop ﬁrst-order optimization algorithms to ef-
ﬁciently determine two key aspects – which parts of the program to transform,
and what transformations to use. We show that it is important to optimize both
these aspects to generate the best adversarially perturbed program. Due to the
discrete nature of this problem, we also propose using randomized smoothing to
improve the attack loss landscape to ease optimization. We evaluate our work on
Python and Java programs on the problem of program summarization.1 We show
that our best attack proposal achieves a 52% improvement over a state-of-the-art
attack generation approach for programs trained on a SEQ2SEQ model. We further
show that our formulation is better at training models that are robust to adversarial
attacks.

1

INTRODUCTION

Machine learning (ML) models are increasingly being used for software engineering tasks. Appli-
cations such as refactoring programs, auto-completing them in editors, and synthesizing GUI code
have beneﬁted from ML models trained on large repositories of programs, sourced from popular
websites like GitHub (Allamanis et al., 2018). They have also been adopted to reason about and
assess programs (Srikant & Aggarwal, 2014; Si et al., 2018), ﬁnd and ﬁx bugs (Gupta et al., 2017;
Pradel & Sen, 2018), detect malware and vulnerabilities in them (Li et al., 2018; Zhou et al., 2019)
etc. thus complementing traditional program analysis tools. As these models continue to be adopted
for such applications, it is important to understand how robust they are to adversarial attacks. Such
attacks can have adverse consequences, particularly in settings such as security (Zhou et al., 2019)
and compliance automation (Pedersen, 2010). For example, an attacker could craft changes in ma-
licious programs in a way which forces a model to incorrectly classify them as being benign, or
make changes to pass off code which is licensed as open-source in an organization’s proprietary
code-base.

Adversarially perturbing a program should achieve two goals – a trained model should ﬂip its deci-
sion when provided with the perturbed version of the program, and second, the perturbation should
be imperceivable. Adversarial attacks have mainly been considered in image classiﬁcation (Good-
fellow et al., 2014; Carlini & Wagner, 2017; Madry et al., 2018), where calculated minor changes
made to pixels of an image are enough to satisfy the imperceptibility requirement. Such changes
escape a human’s attention by making the image look the same as before perturbing it, while modify-
ing the underlying representation enough to ﬂip a classiﬁer’s decision. However, programs demand
a stricter imperceptibility requirement – not only should the changes avoid human attention, but the
changed program should also importantly functionally behave the same as the unperturbed program.

1Source code: https://github.com/ALFA-group/adversarial-code-generation

1

 
 
 
 
 
 
Published as a conference paper at ICLR 2021

Program obfuscations provide the agency to implement one such set of imperceivable changes in
programs. Obfuscating computer programs have long been used as a way to avoid attempts at
reverse-engineering them. They transform a program in a way that only hampers humans’ com-
prehension of parts of the program, while retaining its original semantics and functionality. For
example, one common obfuscation operation is to rename variables in an attempt to hide the pro-
gram’s intent from a reader. Renaming a variable sum in the program statement int sum = 0 to
int xyz = 0 neither alters how a compiler analyzes this variable nor changes any computations
or states in the program; it only hampers our understanding of this variable’s role in the program.
Modifying a very small number of such aspects of a program marginally affects how we compre-
hend it, thus providing a way to produce changes imperceivable to both humans and a compiler. In
this work, we view adversarial perturbations to programs as a special case of applying obfuscation
transformations to them.

Having identiﬁed a set of candidate transformations
which produce imperceivable changes, a speciﬁc
subset needs to be chosen in a way which would
make the transformed program adversarial. Recent
attempts (Yefet et al., 2019; Ramakrishnan et al.,
2020; Bielik & Vechev, 2020) which came closest
to addressing this problem did not offer any rigor-
ous formulation. They recommended using a vari-
ety of transformations without presenting any prin-
cipled approach to selecting an optimal subset of
transformations. We present a formulation which
when solved provides the exact location to trans-
form as well as a transformation to apply at the loca-
tion. Figure 1 illustrates this. A randomly selected
local-variable (name) when replaced by the name
virtualname, which is generated by the state-
of-the-art attack generation algorithm for programs
(Ramakrishnan et al., 2020), is unable to fool a program summarizer (which predicts set item)
unless our proposed site optimization is applied. We provide a detailed comparison in Section 2. In
our work, we make the following key contributions –
• We identify two problems central to deﬁning an adversarial program – identifying the sites in a
program to apply perturbations on, and the speciﬁc perturbations to apply on the selected sites.
These perturbations are involve replacing existing tokens or inserting new ones.

Figure 1: The advantage of our formulation
when compared to the state-of-the-art.

• We provide a general mathematical formulation of a perturbed program that models site loca-
tions and the perturbation choice for each location. It is independent of programming languages
and the task on which a model is trained, while seamlessly modeling the application of multiple
transformations to the program.

• We propose a set of ﬁrst-order optimization algorithms to solve our proposed formulation efﬁ-
ciently, resulting in a differentiable generator for adversarial programs. We further propose a
randomized smoothing algorithm to achieve improved optimization performance.

• Our approach demonstrates a 1.5x increase in the attack success rate over the state-of-the-art attack
generation algorithm (Ramakrishnan et al., 2020) on large datasets of Python and Java programs.
• We further show that our formulation provides better robustness against adversarial attacks com-

pared to the state-of-the-art when used in training an ML model.

2 RELATED WORK

Due to a large body of literature on adversarial attacks in general, we focus on related works in
the domain of computer programs. Wang & Christodorescu (2019), Quiring et al. (2019), Rabin
et al. (2020), and Pierazzi et al. (2020) identify obfuscation transformations as potential adversarial
examples. They do not, however, ﬁnd an optimal set of transformations to deceive a downstream
model. Liu et al. (2017) provide a stochastic optimization formulation to obfuscate programs op-
timally by maximizing its impact on an obscurity language model (OLM). However, they do not
address the problem of adversarial robustness of ML models of programs, and their formulation is
only to ﬁnd the right sequence of transformations which increases their OLM’s perplexity. They use
an MCMC-based search to ﬁnd the best sequence.

Yefet et al. (2019) propose perturbing programs by replacing local variables, and inserting print
statements with replaceable string arguments. They ﬁnd optimal replacements using a ﬁrst-order
optimization method, similar to Balog et al. (2016) and HotFlip (Ebrahimi et al., 2017). This is
an improvement over Zhang et al. (2020), who use the Metropolis-Hastings algorithm to ﬁnd an

2

Published as a conference paper at ICLR 2021

Figure 2:
(a) A sample program P containing a function foo (b) P contains ﬁve sites which can be trans-
formed - two replace sites corresponding to local variables b and r , and three insert sites at locations I1,
I2, I3. Ω is a vocabulary of tokens which can be used for the transformations. (c) This is a perturbed program
with the tokens world and set from Ω used to replace tokens b and at location I3. These transformations
do not change the original functionality of P, but cause an incorrect prediction delete (d) Examples of two
site selection vectors zi, zii selecting different components. zi = 1 for a location i signiﬁes that the ith token
in P is selected to be optimally transformed. zi corresponds to the perturbed program in (c).

optimal replacement for variable names. Bielik & Vechev (2020) propose a robust training strategy
which trains a model to abstain from deciding when uncertain if an input program is adversarially
perturbed. The transformation space they consider is small, which they search through greedily.
Moreover, their solution is designed to reason over a limited context of the program (predicting
variable types), and is non-trivial to extend to applications such as program summarization (explored
in this work) which requires reasoning over an entire program.

Ramakrishnan et al. (2020) extend the work by Yefet et al. (2019) and is most relevant to what we
propose in this work. They experiment with a larger set of transformations and propose a standard
min-max formulation to adversarially train robust models. Their inner-maximizer, which generates
adversarial programs, models multiple transformations applied to a program in contrast to Yefet et al.
(2019). However, they do not propose any principled way to solve the problem of choosing between
multiple program transformations. They randomly select transformation operations to apply, and
then randomly select locations in the program to apply those transformations on.

We instead show that optimizing for locations alone improves the attack performance. Further, we
propose a joint optimization problem of ﬁnding the optimal location and optimal transformation,
only the latter of which Ramakrishnan et al. (2020) (and Yefet et al. (2019)) address in a princi-
pled manner. Although formally unpublished at the time of preparing this work, we compare our
experiments to Ramakrishnan et al. (2020), the state-of-the-art in evaluating and defending against
adversarial attacks on models for programs, and contrast the advantages of our formulation.

3 PROGRAM OBFUSCATIONS AS ADVERSARIAL PERTURBATIONS

In this section, we formalize program obfuscation operations, and show how generating adversarial
programs can be cast as a constrained combinatorial optimization problem.

Program obfuscations. We view obfuscation transformations made to programs as adversarial per-
turbations which can affect a downstream ML/DL model like a malware classiﬁer or a program
summarizer. While a variety of such obfuscation transformations exist for programs in general (see
section 2A, Liu et al. (2017)), we consider two broad classes – replace and insert transformations.
In replace transformations, existing program constructs are replaced with variants which decrease
readability. For example, replacing a variable’s name, a function parameter’s name, or an object
ﬁeld’s name does not affect the semantics of the program in any way. These names in any program
exclusively aid human comprehension, and thus serve as three replace transformations. In insert
transformations, we insert new statements to the program which are unrelated to the code it is in-
serted around, thereby obfuscating its original intent. For example, including a print statement with
an arbitrary string argument does not change the semantics of the program in any way.

Our goal hence is to introduce a systematic way to transform a program with insert or replace
transformations such that a trained model misclassiﬁes a program P that it originally classiﬁed
correctly.

3

Published as a conference paper at ICLR 2021

Site-selection and Site-perturbation – Towards deﬁning adversarial programs. Before we for-
mally deﬁne an adversarial program, we highlight the key factors which need to be considered in
our formulation through the example program introduced in Figure 2.

Consider applying the following two obfuscation transformations on the example program P in
Figure 2.a – replacing local variable names (a replace transform), and inserting print statements
(an insert transform). The two local variables b and r in P are potential candidates where the
replace transform can be applied, while a print statement can potentially be inserted at the three
locations I1, I2, I3 (highlighted in Figure 2.b). We notate these choices in a program as sites–
locations in a program where a unique transformation can be applied.

Thus, in order to adversarially perturb P, we identify two important questions that need to be ad-
dressed. First, which sites in a program should be transformed? Of the n sites in a program, if we
are allowed to choose at most k sites, which set of ≤ k sites would have the highest impact on the
downstream model’s performance? We identify this as the site-selection problem, where the con-
straint k is the perturbation strength of an attacker. Second, what tokens should be inserted/replaced
at the k selected sites? Once we pick k sites, we still have to determine the best choice of tokens
to replace/insert at those sites which would have the highest impact on the downstream model. We
refer to this as the site-perturbation problem.

Mathematical formulation. In what follows, we propose a general and rigorous formulation of
adversarial programs. Let P denote a benign program which consists of a series of n tokens {Pi}n
i=1
in the source code domain. For example, the program in Figure 2.a, when read from top to bottom
and left to right, forms a series of n = 12 tokens {def, b, . . . , r, +, 5}. We ignore white spaces
and other delimiters when tokenizing. Each Pi ∈ {0, 1}|Ω| here is considered a one-hot vector of
length |Ω|, where Ω is a vocabulary of tokens. Let P (cid:48) deﬁne a perturbed program (with respect to
P) created by solving the site-selection and site-perturbation problems, which use the vocabulary
Ω to ﬁnd an optimal replacement. Since our formulation is agnostic to the type of transformation,
perturbation in the remainder of this section refers to both replace and insert transforms. In our
work, we use a shared vocabulary Ω to select transforms from both these classes. In practice, we
can also assign a unique vocabulary to each transformation we deﬁne.
To formalize the site-selection problem, we introduce a vector of boolean variables z ∈ {0, 1}n to
indicate whether or not a site is selected for perturbation. If zi = 1 then the ith site (namely, Pi) is
perturbed. If there exist multiple occurrences of a token in the program, then all such sites are marked
1. For example, in Figure 2.d, if the site corresponding to local variable b is selected, then both
indices of its occurrences, z3, z9 are marked as 1 as shown in zi. Moreover, the number of perturbed
sites, namely, 1T z ≤ k provides a means of measuring perturbation strength. For example, k = 1
is the minimum perturbation possible, where only one site is allowed to be perturbed. To deﬁne
site-perturbation, we introduce a one-hot vector ui ∈ {0, 1}|Ω| to encode the selection of a token
from Ω which would serve as the insert/replace token for a chosen transformation at a chosen site. If
the jth entry [ui]j = 1 and zi = 1, then the jth token in Ω is used as the obfuscation transformation
applied at the site i (namely, to perturb Pi). We also have the constraint 1T ui = 1, implying that
only one perturbation is performed at Pi. Let vector u ∈ {0, 1}n×|Ω| denote n different ui vectors,
one for each token i in P.
Using the above formulations for site-selection, site-perturbation and perturbation strength, the
perturbed program P (cid:48) can then be deﬁned as

P (cid:48) = (1 − z) · P + z · u, where 1T z ≤ k, z ∈ {0, 1}n, 1T ui = 1, ui ∈ {0, 1}|Ω|, ∀i,

(1)

where · denotes the element-column wise product.
The adversarial effect of P (cid:48) is then measured by passing it as input to a downstream ML/DL model
θ and seeing if it successfully manages to fool it.

Generating a successful adversarial program is then formulated as the optimization problem,

minimize
z,u
subject to

(cid:96)attack((1 − z) · P + z · u; P, θ)
constraints in (1),

(2)

where (cid:96)attack denotes an attack loss. In this work, we specify (cid:96)attack as the cross-entropy loss
on the predicted output evaluated at P (cid:48) in an untargeted setting (namely, without specifying the
prediction label targeted by an adversary) (Ramakrishnan et al., 2020). One can also consider other
speciﬁcations of (cid:96)attack, e.g., C&W untargeted and targeted attack losses (Carlini & Wagner, 2017).

4

Published as a conference paper at ICLR 2021

4 ADVERSARIAL PROGRAM GENERATION VIA FIRST-ORDER OPTIMIZATION

Solving problem (2) is not trivial because of its combinatorial nature (namely, the presence of
boolean variables), the presence of a bi-linear objective term (namely, z · u), as well as the presence
of multiple constraints. To address this, we present a projected gradient descent (PGD) based joint
optimization solver (JO) and propose alternates which promise better empirical performance.

PGD as a joint optimization (JO) solver. PGD has been shown to be one of the most effective
attack generation methods to fool image classiﬁcation models (Madry et al., 2018). Prior to applying
PGD, we instantiate (2) into a feasible version by relaxing boolean constraints to their convex hulls,

minimize
z,u

(cid:96)attack(z, u)

(3)

subject to 1T z ≤ k, z ∈ [0, 1]n, 1T ui = 1, ui ∈ [0, 1]|Ω|, ∀i,
where for ease of notation, the attack loss in (2) is denoted by (cid:96)attack(z, u). The continuous relax-
ation of binary variables in (3) is a commonly used trick in combinatorial optimization to boost the
stability of learning procedures in practice (Boyd et al., 2004). Once the continuous optimization
problem (3) is solved, a hard thresholding operation or a randomized sampling method (which re-
gards z and u as probability vectors with elements drawn from a Bernoulli distribution) can be called
to map a continuous solution to its discrete domain (Blum & Roli, 2003). We use the randomized
sampling method in our experiments.

The PGD algorithm is then given by

{z(t), u(t)} = {z(t−1), u(t−1)} − α{∇z(cid:96)attack(z(t−1), u(t−1)), ∇u(cid:96)attack(z(t−1), u(t−1))}
{z(t), u(t)} = Proj({z(t), u(t)}),

(5)
where t denotes PGD iterations, z(0) and u(0) are given initial points, α > 0 is a learning rate, ∇z
denotes the ﬁrst-order derivative operation w.r.t. the variable z, and Proj represents the projection
operation w.r.t. the constraints of (3).

(4)

The projection step involves solving for z and ui simultaneously in a complex convex problem. See
Equation 9 in Appendix A for details.
A key insight is that the complex projection problem (9) can equivalently be decomposed into a
sequence of sub-problems owing to the separability of the constraints w.r.t. z and {ui}. The two
sub-problems are –
(cid:107)z − z(t)(cid:107)2
minimize
2
z
subject to 1T z ≤ k, z ∈ [0, 1]n,

(cid:107)ui − u(t)
subject to 1T ui = 1, ui ∈ [0, 1]|Ω|,

minimize
ui

i (cid:107)2

and

∀i.

(6)

2

The above subproblems w.r.t. z and ui can optimally be solved by using a bisection method that
ﬁnds the root of a scalar equation. We provide details of a closed-form solution and its corresponding
proof in Appendix A. We use this decomposition and solutions to design an alternating optimizer,
which we discuss next.

Alternating optimization (AO) for fast attack generation. While JO provides an approach to
solve the uniﬁed formulation in (2), it suffers from the problem of getting trapped at a poor local
optima despite attaining stationarity (Ghadimi et al., 2016). We propose using AO (Bezdek & Hath-
away, 2003) which allows the loss landscape to be explored more aggressively, thus leading to better
empirical convergence and optimality (see Figure 4a).

AO solves problem (2) one variable at a time – ﬁrst, by optimizing the site selection variable z
keeping the site perturbation variable u ﬁxed, and then optimizing u keeping z ﬁxed. That is,

z(t) =

arg min
1T z≤k, z∈[0,1]n

(cid:96)attack(z, u(t−1))

and u(t)

i =

arg min
1T ui=1, ui∈[0,1]|Ω|

(cid:96)attack(z(t), u) ∀i.

(7)

We can use PGD, as described in (6), to similarly solve each of z and u separately in the two
alternating steps. Computationally, AO is expensive than JO by a factor of 2, since we need two iter-
ations to cover all the variables which JO covers in a single iteration. However, in our experiments,
we ﬁnd AO to converge faster. The decoupling in AO also eases implementation, and provides the
ﬂexibility to set a different number of iterations for the u-step and the z-step within one iteration of
AO. We also remark that the AO setup in (7) can be speciﬁed in other forms, e.g. alternating direc-
tion method of multipliers (ADMM) (Boyd et al., 2011). However, such methods use an involved
alternating scheme to solve problem (2). We defer evaluating these options to future work.

5

Published as a conference paper at ICLR 2021

Randomized smoothing (RS) to improve generating adversarial programs.
In our experi-
ments, we noticed that the loss landscape of generating adversarial program is not smooth (Figure 3).
This motivated us to explore surrogate loss functions which could smoothen it out. In our work, we
employ a convolution-based RS technique (Duchi et al., 2012) to circumvent the optimization dif-
ﬁculty induced by the non-smoothness of the attack loss (cid:96)attack. We eventually obtain a smoothing
loss (cid:96)smooth:

(cid:96)smooth(z, u) = Eξ,τ [(cid:96)attack(z + µξ, u + µτ )],
(8)
where ξ and τ are random samples drawn from the uniform distribution within the unit Euclidean
ball, and µ > 0 is a small smoothing parameter (set to 0.01 in our experiments).

The rationale behind RS (8) is that
the convolution of
two functions
(smooth probability density function
and non-smooth attack loss) is at least
as smooth as the smoothest of the
two original functions. The advan-
tage of such a formulation is that it
is independent of the loss function,
downstream model, and the optimiza-
tion solver chosen for a problem. We
evaluate RS on both AO and JO. In
practice, we consider an empirical
Monte Carlo approximation of (8),
(cid:96)smooth(z, u) = (cid:80)m
j=1[(cid:96)attack(z +
µξj, u+µτj)]. We set m = 10 in our
experiments to save on computation
time. We also ﬁnd that smoothing
the site perturbation variable u con-
tributes the most to improving attack
performance. We hence perturb only u to further save computation time.

(a)

(b)

Figure 3: The original loss landscape for a sample program
(3a). Randomized smoothing produces a ﬂatter and smoother loss
landscape (3b). We plot the loss along the space determined by
the vector (α.sgn(∇xf (x)) + β.Rademacher(0.5)) for α, β ∈
[−0.05, 0.05] (Engstrom et al., 2018)

5 EXPERIMENTS & RESULTS

We begin by discussing the following aspects of our experiment setup – the classiﬁcation task, the
dataset and model we evaluate on, and the evaluation metrics we use.

Task, Transformations, Dataset. We evaluate our formulation of generating optimal adversarial
programs on the problem of program summarization, ﬁrst introduced by Allamanis et al. (2016).
Summarizing a function in a program involves predicting its name, which is usually indicative of its
intent. We use this benchmark to test whether our adversarially perturbed program, which retains the
functionality of the original program, can force a trained summarizer to predict an incorrect function
name. We evaluate this on a well maintained dataset of roughly 150K Python programs(Raychev
et al., 2016) and 700K Java programs (Alon et al., 2018). They are pre-processed into functions,
and each function is provided as input to an ML model. The name of the function is omitted from
the input. The ML model predicts a sequence of tokens as the function name. We evaluate our
work on six transformations (4 replace and 2 insert transformations); see Appendix B for details on
these transformations. The results and analysis that follow pertains to the case when any of these
six transformations can be used as a valid perturbation, and the optimization selects which to pick
and apply based on the perturbation strength k. This is the same setting employed in the baseline
(Ramakrishnan et al., 2020).

Model. We evaluate a trained SEQ2SEQ model. It takes program tokens as input, and generates
a sequence of tokens representing its function name. We note that our formulation is independent
of the learning model, and can be evaluated on any model for any task. The SEQ2SEQ model is
trained and validated on 90% of the data while tested on the remaining 10%. It is optimized using
the cross-entropy loss function.

CODE2SEQ (Alon et al., 2018) is another model which has been evaluated on the task of program
Its architecture is similar to that of SEQ2SEQ and contains two encoders - one
summarization.
which encodes tokens, while another which encodes AST paths. The model when trained only on
tokens performs similar to a model trained on both tokens and paths (Table 3, Alon et al. (2018)).
Thus adversarial changes made to tokens, as accommodated by our formulation, should have a
high impact on the model’s output. Owing to the similarity in these architectures, and since our

6

Published as a conference paper at ICLR 2021

Method

No attack
Random replace
BASELINE*
AO
JO
AO + RS
JO + RS

0.00
0.00
19.87
23.16
23.32
30.25
23.95

k = 1 site

k = 5 sites

ASR

F1

ASR

F1

100.00
100.00
78.18
74.78
74.56
69.52
74.24

0.00
0.00
37.50
-3.40 (cid:78) 43.53
-3.62 (cid:78) 41.95
-8.66 (cid:78) 51.68
-3.94 (cid:78) 48.70

+3.29 (cid:78)
+3.45 (cid:78)
+10.38 (cid:78)
+4.08 (cid:78)

100.00
100.00
59.54
53.75
56.06
47.92
51.55

-5.79 (cid:78)
-3.48 (cid:78)
-11.62 (cid:78)
-7.99 (cid:78)

+6.03 (cid:78)
+4.45 (cid:78)
+14.18 (cid:78)
+11.20 (cid:78)

Table 1: Our work solves two key problems to ﬁnd optimal adversarial perturbations – site-selection and
site-perturbation. The BASELINE method refers to (Ramakrishnan et al., 2020). The perturbation strength k
is the maximum number of sites which an attacker can perturb. Higher the the Attack Success Rate (ASR),
better the attack; the converse holds for F1 score. Our formulation (Eq. 2), solved using two methods –
alternate optimization (AO) and joint optimization (JO), along with randomized smoothing (RS), shows a
consistent improvement in generating adversarial programs. Differences in ASR, marked in blue, are relative
to BASELINE. The results on a Java dataset are tabulated in Table 4, Appendix.

computational bench is in Pytorch while the original CODE2SEQ implementation is in TensorFlow,
we defer evaluating the performance of our formulation on CODE2SEQ to future work.

i,j

(cid:80)
(cid:80)

i)(cid:54)=yij )

Evaluation metrics. We report two metrics – Attack Success Rate (ASR) and F1-score. ASR is
deﬁned as the percentage of output tokens misclassiﬁed by the model on the perturbed input but
1(θ(x(cid:48)
correctly predicted on the unperturbed input, i.e. ASR =
1(θ(xi)=yij ) for each token j in the
expected output of sample i. Higher the ASR, better the attack. Unlike (Ramakrishnan et al., 2020),
we evaluate our method on those samples in the test-set which were fully, correctly classiﬁed by
the model. Evaluating on such fully correctly classiﬁed samples provides direct evidence of the
adversarial effect of the input perturbations (also the model’s adversarial robustness) by excluding
test samples that have originally been misclassiﬁed even without any perturbation. We successfully
replicated results from (Ramakrishnan et al., 2020) on the F1-score metric they use, and acknowl-
edge the extensive care they have taken to ensure that their results are reproducible. As reported in
Table 2 of (Ramakrishnan et al., 2020), a trained SEQ2SEQ model has an F1-score of 34.3 evaluated
on the entire dataset. We consider just those samples which were correctly classiﬁed. The F1-score
corresponding to ‘No attack’ in Table 1 is hence 100. In all, we perturb 2800 programs in Python
and 2300 programs in Java which are correctly classiﬁed.

i,j

5.1 EXPERIMENTS

We evaluate our work in three ways – ﬁrst, we evaluate the overall performance of the three ap-
proaches we propose – AO, JO, and their combination with RS, to ﬁnd the best sites and pertur-
bations for a given program. Second, we evaluate the sensitivity of two parameters which control
our optimizers – the number of iterations they are evaluated on, and the perturbation strength (k)
of an attacker. Third, we use our formulation to train an adversarially robust SEQ2SEQ model, and
evaluate its performance against the attacks we propose.

Overall attack results. Table 1 summarizes our overall results. The ﬁrst row corresponds to the
samples not being perturbed at all. The ASR as expected is 0. The ‘Random replace’ in row
2 corresponds to both z and u being selected at random. This produces no change in the ASR,
suggesting that while obfuscation transformations can potentially deceive ML models, any random
transformation will have little effect. It is important to have some principled approach to selecting
and applying these transformations.

Ramakrishnan et al. (2020) (referred to as BASELINE in Table 1) evaluated their work in two settings.
In the ﬁrst setting, they pick 1 site at random and optimally perturb it. They refer to this as Q1
G.
We contrast this by selecting an optimal site through our formulation. We use the same algorithm
as theirs to optimally perturb the chosen site i.e. to solve for u. This allows us to ablate the effect
of incorporating and solving the site-selection problem in our formulation. In the second related
setting, they pick 5 sites at random and optimally perturb them, which they refer to as Q5
G. In our
setup, Q1
G are equivalent to setting k = 1 and k = 5 respectively, and picking random sites
in z instead of optimal ones. We run AO for 3 iterations, and JO for 10 iterations.

G and Q5

We ﬁnd that our formulation consistently outperforms the baseline. For k = 1, where the at-
tacker can perturb at most 1 site, both AO and JO provide a 3 point improvement in ASR, with

7

Published as a conference paper at ICLR 2021

JO marginally performing better than AO. Increasing k improves the ASR across all methods – the
attacker now has multiple sites to transform. For k = 5, where the attacker has at most 5 sites to
transform, we ﬁnd AO to provide a 6 point improvement in ASR over the baseline, outperforming
JO by 1.5 points.

Smoothing the loss function has a marked effect. For k = 1, we ﬁnd smoothing to provide a 10
point increase (∼ 52% improvement) in ASR over the baseline when applied to AO, while JO+RS
provides a 4 point increase. Similarly, for k = 5, we ﬁnd AO+RS to provide a 14 point increase (∼
38% improvement), while JO+RS provides a 11 point increase, suggesting the utility of smoothing
the landscape to aid optimization.

Overall, we ﬁnd that accounting for site location in our formulation combined with having a smooth
loss function to optimize improves the quality of the generated attacks by nearly 1.5 times over the
state-of-the-art attack generation method for programs.

Effect of solver iterations and perturbation strength k. We evaluate the attack performance
(ASR) of our proposed approaches against the number of iterations at k = 5 (Figure 4a). For
comparison, we also present the performance of BASELINE, which is not sensitive to the number
of iterations (consistent with the empirical ﬁnding in (Ramakrishnan et al., 2020)), implying its
least optimality. Without smoothing, JO takes nearly 10 iterations to reach its local optimal value,
whereas AO achieves it using only 3 iterations but with improved optimality (in terms of higher ASR
than JO). This supports our hypothesis that AO allows for a much more aggressive exploration of
the loss landscape, proving to empirically outperform JO. With smoothing, we ﬁnd both AO+RS
and JO+RS perform better than AO and JO respectively across iterations. We thus recommend using
AO+RS with 1-3 iterations as an attack generator to train models that are robust to such adversarial
attacks.

In Figure 4b, we plot the per-
formance of the best perform-
ing methods as we vary the
attacker’s perturbation strength
(k). We make two key obser-
vations – First, allowing a few
sites (< 5) to be perturbed is
enough to achieve 80% of the
best achievable attack rate. For
example, under the AO+RS at-
tack, the ASR is 50 when k = 5
and 60 when k = 20. From
an attacker’s perspective,
this
makes it convenient
to effec-
tively attack models of programs
without being discovered. Second, we observe that the best performing methods we propose con-
sistently outperform BASELINE across different k. The performance with BASELINE begins to
converge only after k = 10 sites, suggesting the effectiveness of our attack.

Figure 4: ASRs of our approaches and BASELINE against the number
of optimization iterations (4a) and perturbation strength of an attacker
(4b).

(b)

(a)

Train

No AT
BASELINE
AO+RS

AO+RS
30.25
19.11
13.75

BASELINE
19.87
17.99
12.73

Attack (ASR)
AO
23.16
18.87
13.01

Improved adversarial training under proposed
attack. Adversarial training (AT) (Madry et al.,
2018) is a min-max optimization based training
method to improve a model’s adversarial robust-
ness. In AT, an attack generator is used as the in-
ner maximization oracle to produce perturbed train-
ing examples that are then used in the outer min-
imization for model training. Using AT, we in-
vestigate if our proposed attack generation method
(AO+RS) yields an improvement in adversarial ro-
bustness (over BASELINE) when it is used to adver-
sarially train the SEQ2SEQ model. We evaluate AT
in three settings – ‘No AT’, corresponding to the regularly trained SEQ2SEQ model (the one used in
all the experiments in Table 1), BASELINE - SEQ2SEQ trained under the attack by (Ramakrishnan
et al., 2020), and AO+RS - SEQ2SEQ trained under our AO+RS attack. We use three attackers on
these models – BASELINE and two of our strongest attacks - AO and AO+RS. The row corresponding
to ‘No AT’ is the same as the entries under k = 1 in Table 1. We ﬁnd AT with BASELINE improves
robustness by ∼11 points under AO+RS, our strongest attack. However, training with AO+RS pro-
vides an improvement of ∼16 points. This suggests AO+RS provides better robustness to models
when used as an inner maximizer in an AT setting.

Table 2: We employ an AT setup to train
SEQ2SEQ with the attack formulation we propose.
Lower the ASR, higher the robustness to adversar-
ial attacks. Training under AO+RS attacks pro-
vides best robustness results.

8

Published as a conference paper at ICLR 2021

6 CONCLUSION

In this paper, we propose a general formulation which mathematically deﬁnes an adversarial attack
on program source code. We model two key aspects in our formalism – location of the transforma-
tion, and the speciﬁc choice of transformation. We show that the best attack is generated when both
these aspects are optimally chosen. Importantly, we identify that the joint optimization problem we
set up which models these two aspects is decomposable via alternating optimization. The nature of
decomposition enables us to easily and quickly generate adversarial programs. Moreover, we show
that a randomized smoothing strategy can further help the optimizer to ﬁnd better solutions. Even-
tually, we conduct extensive experiments from both attack and defense perspectives to demonstrate
the improvement of our proposal over the state-of-the-art attack generation method.

7 ACKNOWLEDGMENT

This work was partially funded by a grant by MIT Quest for Intelligence, and the MIT-IBM AI lab.
We thank David Cox for helpful discussions on this work. We also thank Christopher Laibinis for his
constant support with computational resources. This work was partially done during an internship
by Shashank Srikant at MIT-IBM AI lab.

9

Published as a conference paper at ICLR 2021

REFERENCES

Miltiadis Allamanis, Hao Peng, and Charles Sutton. A convolutional attention network for extreme
summarization of source code. In International conference on machine learning, pp. 2091–2100,
2016.

Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of machine

learning for big code and naturalness. ACM Computing Surveys (CSUR), 51(4):1–37, 2018.

Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. code2seq: Generating sequences from struc-

tured representations of code. arXiv preprint arXiv:1808.01400, 2018.

Matej Balog, Alexander L Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow.

Deepcoder: Learning to write programs. arXiv preprint arXiv:1611.01989, 2016.

James C Bezdek and Richard J Hathaway. Convergence of alternating optimization. Neural, Parallel

& Scientiﬁc Computations, 11(4):351–368, 2003.

Pavol Bielik and Martin Vechev. Adversarial robustness for code. arXiv preprint arXiv:2002.04694,

2020.

Christian Blum and Andrea Roli. Metaheuristics in combinatorial optimization: Overview and

conceptual comparison. ACM computing surveys (CSUR), 35(3):268–308, 2003.

Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-

versity press, 2004.

Stephen Boyd, Neal Parikh, and Eric Chu. Distributed optimization and statistical learning via the

alternating direction method of multipliers. Now Publishers Inc, 2011.

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017

ieee symposium on security and privacy (sp), pp. 39–57. IEEE, 2017.

John C Duchi, Peter L Bartlett, and Martin J Wainwright. Randomized smoothing for stochastic

optimization. SIAM Journal on Optimization, 22(2):674–701, 2012.

Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotﬂip: White-box adversarial examples

for text classiﬁcation. arXiv preprint arXiv:1712.06751, 2017.

Logan Engstrom, Andrew Ilyas, and Anish Athalye. Evaluating and understanding the robustness

of adversarial logit pairing. arXiv preprint arXiv:1807.10272, 2018.

Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation meth-
ods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):
267–305, 2016.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. arXiv preprint arXiv:1412.6572, 2014.

Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. Deepﬁx: Fixing common c lan-
guage errors by deep learning. In Proceedings of the Thirty-First AAAI Conference on Artiﬁcial
Intelligence, pp. 1345–1351, 2017.

Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun Deng, and Yuyi
Zhong. Vuldeepecker: A deep learning-based system for vulnerability detection. arXiv preprint
arXiv:1801.01681, 2018.

Han Liu, Chengnian Sun, Zhendong Su, Yu Jiang, Ming Gu, and Jiaguang Sun. Stochastic opti-
mization of program obfuscation. In 2017 IEEE/ACM 39th International Conference on Software
Engineering (ICSE), pp. 221–231. IEEE, 2017.

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.

Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in optimization, 1(3):

127–239, 2014.

10

Published as a conference paper at ICLR 2021

Renaud Pawlak, Martin Monperrus, Nicolas Petitprez, Carlos Noguera, and Lionel Seinturier.
Spoon: A library for implementing analyses and transformations of java source code. Software:
Practice and Experience, 46(9):1155–1179, 2016.

Palle Martin Pedersen. Methods and systems for identifying an area of interest in protectable content,

September 14 2010. US Patent 7,797,245.

Fabio Pierazzi, Feargus Pendlebury, Jacopo Cortellazzi, and Lorenzo Cavallaro. Intriguing proper-
ties of adversarial ml attacks in the problem space. In 2020 IEEE Symposium on Security and
Privacy (SP), pp. 1332–1349. IEEE, 2020.

Michael Pradel and Koushik Sen. Deepbugs: A learning approach to name-based bug detection.

Proceedings of the ACM on Programming Languages, 2(OOPSLA):1–25, 2018.

Erwin Quiring, Alwin Maier, and Konrad Rieck. Misleading authorship attribution of source code
using adversarial learning. In 28th {USENIX} Security Symposium ({USENIX} Security 19), pp.
479–496, 2019.

Md Rabin, Raﬁqul Islam, and Mohammad Amin Alipour. Evaluation of generalizability of neural
program analyzers under semantic-preserving transformations. arXiv preprint arXiv:2004.07313,
2020.

Goutham Ramakrishnan, Jordan Henkel, Zi Wang, Aws Albarghouthi, Somesh Jha, and Thomas
Reps. Semantic robustness of models of source code. arXiv preprint arXiv:2002.03043, 2020.

Veselin Raychev, Pavol Bielik, and Martin Vechev. Probabilistic model for code with decision trees.

ACM SIGPLAN Notices, 51(10):731–747, 2016.

Xujie Si, Hanjun Dai, Mukund Raghothaman, Mayur Naik, and Le Song. Learning loop invariants

for program veriﬁcation. In Neural Information Processing Systems, 2018.

Shashank Srikant and Varun Aggarwal. A system to grade computer programming skills using ma-
chine learning. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining, pp. 1887–1896, 2014.

Ke Wang and Mihai Christodorescu. Coset: A benchmark for evaluating neural program embed-

dings. arXiv preprint arXiv:1905.11445, 2019.

Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong, and Xue Lin.
Topology attack and defense for graph neural networks: An optimization perspective. arXiv
preprint arXiv:1906.04214, 2019.

Noam Yefet, Uri Alon, and Eran Yahav. Adversarial examples for models of code. arXiv preprint

arXiv:1910.07517, 2019.

Huangzhao Zhang, Zhuo Li, Ge Li, Lei Ma, Yang Liu, and Zhi Jin. Generating adversarial examples
for holding robustness of source code processing models. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, volume 34, pp. 1169–1176, 2020.

Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. Devign: Effective vulnera-
bility identiﬁcation by learning comprehensive program semantics via graph neural networks. In
Advances in Neural Information Processing Systems, pp. 10197–10207, 2019.

11

Published as a conference paper at ICLR 2021

A SOLVING THE PAIR OF PROJECTION SUB-PROBLEMS IN EQ. 6

The projection step (5) can formally be described as ﬁnding the solution of the convex problem

(cid:107)z − z(t)(cid:107)2

minimize
z,u
subject to 1T z ≤ k, z ∈ [0, 1]n, 1T ui = 1, ui ∈ [0, 1]|Ω|, ∀i.

i (cid:107)ui − u(t)

i (cid:107)2
2

2 + (cid:80)

(9)

In Equation 6, we proposed to decompose the projection problem on the combined variables z and
ui into the following sub-problems –

(cid:107)z − z(t)(cid:107)2
minimize
2
z
subject to 1T z ≤ k, z ∈ [0, 1]n,

and

(cid:107)ui − u(t)
minimize
ui
subject to 1T ui = 1, ui ∈ [0, 1]|Ω|,

i (cid:107)2
2

∀i.

(10)

The following proposition is a closed-form solution which solves them.

Proposition 1 Let z(t+1) and {u(t+1)
are given by

i

} denote solutions of problems given in (6). Their expressions

z(t+1) = [z(t) − µ1]+, and µ is the root of 1T [z(t) − µ1]+ = 1;

(cid:40)

u(t+1)
i

=

P[0,1][u(t)
P[0,1][u(t)

]

i
i − τi1]

if 1T P[0,1][u(t)
if ∃τi > 0 s.t. 1T P[0,1][u(t)

] ≤ k,

i

i − τi1] = k,

(11)

(12)

∀i,

where [·]+ = max{0, ·} denotes the (elementwise) non-negative operation, and P[0,1](·) is the (el-
ementwise) box projection operation, namely, for a scalar x P[0,1](x) = x if x ∈ [0, 1], 0 if x < 0,
and 1 if x > 1.

Proof. We ﬁrst reformulate problems in (6) as problems subject to a single inequality or equality
constraint. That is, min1T z≤k (cid:107)z − z(t)(cid:107)2
2 + Iui≥0(ui),
where IC(P) denotes an indicator function over the constraint set C and IC(P) = 0 if P ∈ C and ∞
otherwise. We then derive Karush–Kuhn–Tucker (KKT) conditions of the above problems for their
(cid:3)
optimal solutions; see (Parikh & Boyd, 2014; Xu et al., 2019) for details.

2 + I[0,1]n (z) and min1T ui=1 (cid:107)ui − u(t)

i (cid:107)2

In (11) and (12), we need to call an internal solver to ﬁnd the root of a scalar equation, e.g., 1T [z(t) −
µ1]+ = 1 in (11). This can efﬁciently be accomplished using the bisection method in the logarithmic
rate O(− log2 (cid:15)) for the solution of (cid:15)-error tolerance (Boyd et al., 2004). Eventually, the PGD-based
JO solver is formed by (4), (5), (11) and (12).

12

Published as a conference paper at ICLR 2021

B PROGRAM TRANSFORMATIONS

To compare our work against the state-of-the-art (Ramakrishnan et al., 2020), we adopt the transfor-
mations introduced in their work and apply our formulation to them. This allows to solve the same
setup they try while contrasting the advantages of our formulation.

They implement the following transformations -

• Renaming local variables. This is the most common obfuscation operation, where local variables
in a scope are renamed to less meaningful names. We use this to replace it with a name which has
an adversarial effect on a downstream model.

• Renaming function parameters. Similar to renaming variable names, function parameter names

are also changeable. We replace names of such parameters with

• Renaming object ﬁelds. A class’ referenced ﬁeld is renamed in this case. These ﬁelds are refer-
enced using self and this in Python and Java respectively. We assume that the class deﬁnitions
corresponding to these objects are locally deﬁned and can be changed to reﬂect these new names.

• Replacing boolean literals A boolean literal (True, False) occurring in the program is replaced
with an equivalent expression containing optimizable tokens. This results in a statement of the
form <token> == <token> and <token> != <token> for True and False respectively.

• Inserting print statements. A print with optimizable string arguments are inserted at a location

recommended by the site-selection variable. See Figure 2 for an example.

• Adding dead code. A statement having no consequence to its surrounding code, guarded by an

if-condition with an optimizable argument, is added to the program.

The last two transformations are insert transforms, which introduce new tokens in the original pro-
gram, whereas the others are replace transforms, which modify existing tokens.

They implement two other transformations – inserting a try-catch block with an optimizable
parameter, and unrolling a while loop once. We did not add these to our ﬁnal set of transforms since
they are very similar to adding print statements and dead code, while producing a negligible effect.
Adding every insert transformation increases the number of variables to be optimized. Since these
two transforms did not impact model performance, we omitted them to keep the number of variables
to be optimized bounded.

13

Published as a conference paper at ICLR 2021

C AN ATTACK EXAMPLE

Table 3: We present an additional example which contrasts the advantages of different aspects of
our formulation. In Figure 1, we saw how selecting an optimal site led to the optimal local variable
(qisrc) being found. In this example, we show how randomized smoothing, a key solution we
propose to ease optimization, helps in ﬁnding the best variable. In this case, just ﬁnding the optimal
site is not enough to ﬂip the classiﬁer’s decision (call). Smoothing however enables to ﬁnd a local
variable datetime which ﬂips the classiﬁer’s decision to create.

14

Published as a conference paper at ICLR 2021

D ADDITIONAL RESULTS

D.1

JAVA DATASET

We tabulate results of evaluating our formulation on an additional dataset containing Java programs.
This dataset was released by Alon et al. (2018) in their work on CODE2SEQ. The transformations
were implemented using Spoon (Pawlak et al., 2016). We ﬁnd the results to be consistent with the
results on the Python dataset. AO + RS provides the best attack.

Method

No attack
Random replace
BASELINE*
AO
JO
AO + RS
JO + RS

0.00
0.00
22.93
25.95
23.26
29.08
26.71

k = 1 site

k = 5 sites

ASR

F1

ASR

F1

100.00
100.00
70.75
67.17
70.08
63.90
65.41

0.00
0.00
33.16
-3.58 (cid:78) 33.41
-0.67 (cid:78) 33.65
-6.85 (cid:78) 40.53
-5.34 (cid:78) 38.30

100.00
100.00
59.45
59.03
58.85
51.91
53.14

-0.42 (cid:78)
-0.60 (cid:78)
-7.54 (cid:78)
-6.31 (cid:78)

+0.25 (cid:78)
+0.49 (cid:78)
+7.37 (cid:78)
+5.14 (cid:78)

+3.02 (cid:78)
+0.33 (cid:78)
+6.15 (cid:78)
+3.78 (cid:78)

Table 4: Performance of our formulation on a dataset containing Java programs (Alon et al., 2018).
See Table 1 for results on a Python dataset.

D.2 FALSE POSITIVE RATE (FPR) UNDER DIFFERENT ATTACKS

We evaluated the False Positive Rate (FPR) of the model when provided perturbed programs as
inputs. It is important the perturbations made to the programs cause the model to start predicting
ground truth tokens instead of incorrect tokens. In principle, our optimization objective strictly picks
replacements which degrade the classiﬁer’s performance. As a consequence, we should expect that
the model does not end up perturbing the program in a way which leads it closer to the ground truth.
We empirically ﬁnd the FPR consistent with this explanation. As seen in Table 5, in all our attacks,
the FPR is almost 0, validating that our perturbations do not introduce changes which result in the
model predicting the right output.

Method
No attack
BASELINE
AO
AO+RS

FPR, k=1 FPR, k=5

0.0000
0.0077
0.0073
0.0075

0.0000
0.0130
0.0114
0.0148

Table 5: False positive rates of the model under different attacks.

D.3 EFFECT OF VARIOUS TRANSFORMATIONS

We study the effect of different transformations used in our work on the attack success rate of our
attacks. In our analysis, we found that omitting print statements do not affect the ASR of our
attacks. This is helpful since for the current task we evaluate (generating program summaries), a
print statement likely affects the semantics of the task.

Training

All transformations (All)
(All − print)
(All − vars, function params)

Model’s F1-score
33.74
33.14
30.90

ASR, k=1
AO
23.16
20.22
26.36

BASELINE
19.87
16.39
21.48

AO+RS
30.25
26.46
31.43

ASR, k=5
AO
43.53
42.70
37.53

BASELINE
37.50
35.14
37.21

AO+RS
51.68
51.24
48.54

Table 6: The effect of variable names, function parameter names, and print statements on the robustness of the
learned model. These results correspond to the Python dataset.

We further investigated the effect of variable names and function parameter names on our attacks.
The SEQ2SEQ model and the task of generating program summaries can appear to overly rely on the
presence of variable and parameter names in the program. To empirically ascertain this, we masked

15

Published as a conference paper at ICLR 2021

out all the variable and parameter names from our training set and trained the model with place-
holder tokens. We ﬁnd the model’s performance to remain similar (row All - vars, function
params: 1, Table 6). Likewise, the attack performance also remains similar. We test another con-
dition where we mask all these token names in the test set as well, during inference. We ﬁnd a
decrease in performance (row 4, Table 6) under attack strength k = 1, but the trend observed re-
mains – AO+RS >AO >BASELINE. The ASR is largely unchanged under k = 5.

16

