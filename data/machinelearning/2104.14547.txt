2
2
0
2

n
a
J

3
1

]

G
L
.
s
c
[

4
v
7
4
5
4
1
.
4
0
1
2
:
v
i
X
r
a

NURBS-Diff: A Differentiable Programming Module for NURBS
Anjana Deva Prasad1, Aditya Balu1, Harshil Shah1, Soumik Sarkar1,
Chinmay Hegde2, Adarsh Krishnamurthy1*

1 Iowa State University
2 New York University

Abstract
Boundary representations (B-reps) using Non-Uniform Rational B-splines (NURBS) are the de facto stan-
dard used in CAD, but their utility in deep learning-based approaches is not well researched. We propose
a differentiable NURBS module to integrate NURBS representations of CAD models with deep learning
methods. We mathematically deÔ¨Åne the derivatives of the NURBS curves or surfaces with respect to the
input parameters (control points, weights, and the knot vector). These derivatives are used to deÔ¨Åne an ap-
proximate Jacobian used for performing the ‚Äúbackward‚Äù evaluation to train the deep learning models. We
have implemented our NURBS module using GPU-accelerated algorithms and integrated it with PyTorch,
a popular deep learning framework. We demonstrate the efÔ¨Åcacy of our NURBS module in performing
CAD operations such as curve or surface Ô¨Åtting and surface offsetting. Further, we show its utility in deep
learning for unsupervised point cloud reconstruction and enforce analysis constraints. These examples
show that our module performs better for certain deep learning frameworks and can be directly integrated
with any deep-learning framework requiring NURBS.

Keywords
Differentiable NURBS Layer | NURBS | Geometric Deep Learning | Surface Modeling

Introduction

1
In modern CAD systems, a solid model is represented using boundary representation (B-Rep), where the
solid boundaries are deÔ¨Åned using spline surfaces. Non-Uniform Rational B-splines (NURBS) are the stan-
dard representation used for deÔ¨Åning the spline surfaces 41. NURBS surfaces offer a high level of control
and versatility; they can also compactly represent the surface geometry. NURBS surfaces can represent
more complex shapes than B√®zier or B-splines due to the non-uniformity of the knot vectors and the non-
linear transformation due to the weights assigned to the control points. In addition, the NURBS deÔ¨Åni-
tion allows for local control via the knots and the control points and global control via the weights. On
the other hand, deep learning for 3D Euclidean geometry is emerging as a critical and well-explored re-
search area in engineering. This area includes fundamental computer vision works such as 3D shape re-
constructions from point clouds or multi-view stereo and 3D semantic segmentation for shape understand-
ing 2,10,13,16,20,31,36,38,40. While NURBS are the standard CAD representation in engineering, their utility in
deep learning-based approaches is not well researched. Current 3D deep learning (DL) research focuses on
converting standard CAD representations to geometric representations that are more amenable to machine
learning (such as voxels, triangular meshes, etc.) 5. This conversion from the standard CAD geometries to
other representations is often irreversible and is not trivial to incorporate with DL algorithms.

One of the main challenges in extending NURBS-based representation to deep learning is the differentiable
programming of the NURBS evaluations. The main idea of differentiable programming is to deÔ¨Åne each
operation in a neural network in a differentiable manner. Differentiable programming uses gradient-based
approaches to optimize the neural network parameters, thereby obtaining the desired output through neu-
ral network operations. This differentiable programming paradigm allows an end-to-end programmable
system that can be used to train deep neural networks. This paradigm has found use in a large variety
of applications such as scientiÔ¨Åc computing 22,23,45, image processing 29, physics engines 14, computational
simulations 1, and graphics 9,28.

For differentiable programming of NURBS, we need to compute gradients of the NURBS surface points
with respect to the parameters of the NURBS representation (see Figure 1). However, since the NURBS
surface points are a function of knots, control points, and weights, the partial derivative with respect to
each input parameter needs to be computed, making the backward evaluation challenging. Further, due to
the use of basis functions (which are piecewise-continuous polynomials), gradients might be discontinuous

1

 
 
 
 
 
 
Figure 1: We propose a differentiable NURBS module that can be used for CAD geometric modeling using standard deep learning systems.
The NURBS parameters are input to the module during the forward evaluation, and we evaluate the surface mesh. Once a loss is computed,
and gradients for the surface mesh are obtained, we perform a backward evaluation to enable backpropagation of the losses to modify the input
parameters.

(or even zero) at the knots. The recursive deÔ¨Ånition of the basis functions adds an additional challenge to
deÔ¨Åne derivatives with respect to the knot vectors. To alleviate these issues, we exploit the recent theoreti-
cal advances in the paradigm of differentiable programming. It has been theoretically proven that if a weak
form of the Jacobian for the ‚Äúforward‚Äù evaluation operator can be represented using a block-sparse matrix,
then this approximate Jacobian can be used to perform the ‚Äúbackward‚Äù evaluation of that operation 11,12.
This approach has been used to deÔ¨Åne approximate derivatives for operations such as sorting 7,12, loops
and algorithmic conditions, and even derivatives for piecewise polynomial functions 6. We use a similar
approach by deÔ¨Åning a block-sparse Jacobian for NURBS surface evaluation. Existing differentiable pro-
gramming approaches for splines 33 mainly focus on optimizing the control point locations. In this paper,
we provide a complete module for integrating NURBS with differentiable frameworks that optimize not
just the control points but also the knots for reparameterization.

Formally, using the differentiable programming approach explained above, we formulate a differentiable
NURBS (‚ÄúNURBS-Diff ‚Äù) module, which enables deep learning frameworks to integrate NURBS-based rep-
resentation of B-Rep surfaces and perform CAD operations using them. The forward pass of our NURBS
module uses the standard NURBS evaluation as explained in Section 3.1. The backward pass uses the
derivatives of the NURBS curve or surface with respect to the input parameters of the NURBS (Section 3.2).
The derivatives are used to deÔ¨Åne the Jacobian that is then used for backpropagation of the losses during
training. After deÔ¨Åning NURBS-Diff , we validate our approach by performing traditional CAD operations
such as curve Ô¨Åtting, surface Ô¨Åtting, and surface offsetting. Finally, we show the applicability of NURBS-Diff
in deep learning by using it as an additional decoder for point cloud reconstruction.

In this paper, we have developed a differentiable NURBS module that can be used in machine learning and
CAD applications. The key contributions of this work are:

1. A differentiable programming framework using NURBS representation where the losses can be back-
propagated using the NURBS deÔ¨Ånitions. SpeciÔ¨Åcally, we deÔ¨Åne the Jacobian based on the derivatives
of the NURBS with respect to its input parameters.

2. A GPU-accelerated implementation of our NURBS module in PyTorch for better integration with

existing deep learning programming frameworks.

3. A gradient descent-based optimization framework using NURBS-Diff for performing CAD opera-

tions such as curve/surface Ô¨Åtting and surface offsetting.

4. The applicability of our proposed differentiable programming framework to extend the training pro-

cess of unsupervised point cloud reconstruction of NURBS surfaces.

The rest of the paper is arranged as follows. We outline some close related work in point cloud recon-
struction, machine learning approaches in CAD, and differentiable programming in Section 2. We provide
the mathematical details of our differentiable NURBS module in Section 3. We show the application of
NURBS-Diff to CAD operations in Section 4 and to unsupervised point cloud reconstruction in Section 5.

2

NURBS LayerControl Points PWeights WuKnot Vector UvKnot Vector VForward Evaluationùêí=ùíá(ùöø)Backward EvaluationùíÖùêíùíÖùöøSurface MeshSurface Points2 Related Work
The problem of extracting concise geometry representations from a spectrum of input data formats such
as images, depth maps, and point clouds has been extensively studied over the last few decades. While
methodologies that derive such representations are pervasive in 3D reconstruction literature today, our
NURBS module focuses on Ô¨Ålling the gap between the NURBS-based CAD representation and the other
input formats used in machine learning.
In this context, we broadly categorize the prior related work
under differentiable programming and splines in deep learning.

2.1 DiÔ¨Äerentiable Programming

NURBS surfaces are obtained as a tensor product of two-piecewise polynomial B-spline curves. To concep-
tualize end-to-end trainable deep learning systems that can Ô¨Åt NURBS surfaces to various input geometries,
we require a framework that can backpropagate over such piecewise polynomial functions. Several recent
works have been proposed that take advantage of the differentiable programming paradigm to approxi-
mate gradients for such functions. Cuturi et al. 12 and Blondel et al. 8 propose differentiable operators for
sorting based tasks. Similarly, Vlastelica et al. 49 compute gradients for several optimization problems by
constructing linear approximations to discrete-values functions. We model our module on prior work that
incorporates structured priors as modules in the deep learning framework, similar to Sheriffdeen et al. 47 ,
Joshi et al. 25 , and Djolonga and Krause 17 . Beyond deep learning-based approaches, automatic differentia-
tion for NURBS parametric coordinates for obtaining the surface derivatives for Adjoint-based sensitivity
analysis has been performed by Zhang 50 . Ugolotti et al. 48 performed a gradient-based aerodynamic shape
optimization using a robust Machine Learning model, which is created to integrate the geometry generation
and the mesh generation process using one single polynomial module for the volumetric mesh. Mykhaskiv
et al. 35 and M√ºller et al. 34 deÔ¨Åne a differentiated CAD kernel in OpenCASCADE for applying algorithmic
differentiation to a complete CAD system for shape optimization and imposing constraints. These works
encourage us to pursue a similar research direction in developing the NURBS-Diff module. However, their
approach for obtaining the gradients involves tedious operations such as performing singular value decom-
position (SVD) on the control points to obtain the gradients. We also note that the mathematical deÔ¨Ånition
of the derivatives for NURBS and its application to Ô¨Åtting has been explored previously 21,42. However, due
to discontinuities, a more stable and faster approach for computing the derivatives is needed.

2.2 Splines in Deep Learning
Several deep learning frameworks use splines. Minto et al. 32 use NURBS surfaces Ô¨Åtted over the 3D geom-
etry as an input representation for the object classiÔ¨Åcation task of ModelNet10 and ModelNet40 datasets.
Erwinski et al. 18 presented a neural-network-based contour error prediction method for NURBS paths. Fey
et al. 19 present a new convolution operator based on B-splines for irregular structured and geometric in-
put, e.g., graphs or meshes. Balestriero et al. 3 build a theoretical link between deep networks and spline
functions and build end-to-end deep learning systems using spline-based activation functions. Balu et al. 4
propose a NURBS-aware convolutional neural network that maintains the topological structure similar to
a parametric NURBS surface evaluation grid. Very recently, Sharma et al. 46 performed point cloud recon-
struction to predict a B-spline surface, which is later processed to obtain a complete CAD model with other
primitives ‚Äústitched‚Äù together. In our work, we perform comparison between our approach and Sharma
et al. 46 in Section 5.

3 NURBS-DiÔ¨Ä Module
Modern CAD systems make use of boundary-representation (B-Rep) for representing a solid geometry,
‚Ñ¶, which is embedded in a 2D or 3D Euclidean space ((cid:60)2 or (cid:60)3). A B-Rep consists of a set of surfaces d‚Ñ¶
representing the boundary of the solid. Each surface S ‚äÇ d‚Ñ¶ in a standard CAD system is represented using
a Non-Uniform Rational B-spline (NURBS) surface. The NURBS representation is a compact representation
that uses a set of control points, knot vectors, degrees, and weights to map a parametric space to span the
entire surface S in the Euclidean space. In this work, we propose a differentiable NURBS module which
could evaluate the surface S given the control points, knot vectors, degrees, and weights, usually obtained
as an output from a deep learning system, NN(Œ∏). This deep learning system is trained using a loss function
L(¬∑, ¬∑), computed between a target point cloud P ‚àà {(cid:60)Nx2 or (cid:60)Nx3} (where N is the number of points) and
a set of points S sampled (or evaluated) from the surface S. During the training process, the gradient

3

of the loss function with respect to the parameters of the deep learning model, ‚àÇL/‚àÇŒ∏ is required for back
propagation. It is usually straightforward to compute the derivative of the loss function ‚àÇL/‚àÇS for some of
the standard loss functions used, such as Chamfer distance, L2 distance, etc., since they are differentiable.
However, ‚àÇL/‚àÇŒ∏ requires a mathematically consistent deÔ¨Ånition of ‚àÇS/‚àÇŒ®, where Œ® refers to the complete set
of NURBS parameters (i.e. the set of control points P, its corresponding weights W, and the knot vectors
U and V) that deÔ¨Åne the surface. The gradient ‚àÇS/‚àÇŒ® is necessary because the deep learning system NN(Œ∏)
predicts this set of NURBS parameters Œ® and computing ‚àÇL/‚àÇŒ∏ requires computing ‚àÇS/‚àÇŒ®, ‚àÇL/‚àÇS and Ô¨Ånally,
‚àÇŒ®/‚àÇŒ∏. Formally, this can be explained using the chain rule as:

‚àÇL
‚àÇŒ∏

=

‚àÇL
‚àÇS

‚àÇS
‚àÇŒ®

‚àÇŒ®
‚àÇŒ∏

(1)

The main challenge in this approach is computing ‚àÇS/‚àÇŒ®. To this end, we propose a differentiable NURBS
module implemented as a forward and backward machine learning module. While our module can handle
both curve and surface point computations, we limit the discussions of our forward and backward algo-
rithms to surfaces. As shown in the results section, the approach can be directly used for curves embedded
in both 2D and 3D space by suitably adjusting the dimensions of the NURBS parameters.

3.1 Forward Evaluation for NURBS Surface
The NURBS surface S is sampled over a Ô¨Ånite parametric space (u, v) where (u, v) ‚àà ([0, 1] √ó [0, 1]), and this
set of Ô¨Ånite points S representing the surface is used for performing the loss computation and the backward
gradient computation. This set S is computed as a function of NURBS parameters Œ® = {P, U, V, W}. Given
the NURBS surface points are a function of the NURBS parameters in Equation 2, we compute the forward
evaluation using the NURBS formulation:

S = f (P , U , V , W)

3.1.1 NURBS Formulation
Formally, a point in the NURBS surface parametrized using (u, v) is deÔ¨Åned as follows:

S(u, v) =

‚àën

j=0 N p
‚àëm
j=0 N p
‚àëm

i (u)Nq
i (u)Nq

j (v)wijPij
j (v)wij

i=0
‚àën

i=0

,

(2)

(3)

Here, the basis functions of NURBS, (Ni, Nj) are polynomials that are recursively computed using Cox-de
Boor recursion formula in Equation 4, where u is the parameter value, N p
is the ith basis function of degree
i
p.

N p

i (u) =

u ‚àí ui
ui+p ‚àí ui

N p‚àí1
i

(u) +

ui+p+1 ‚àí u
ui+p+1 ‚àí ui+1

N0

i (u) =

(cid:26) 1

if ui ‚â§ u ‚â§ ui+1

0 otherwise

N p‚àí1

i+1 (u)

(4)

(5)

Here, ui (also known as knots) refers to the elements of the knot vector U (similarly, vi ‚àà V). The knot
vector is a non-decreasing sequence of parametric coordinates, which divides the B-spline into non-uniform
piecewise functions. The basis functions N p
i spans over the parametric domain based on the knot vector
and degree as shown in Equation 4 and Equation 5. Note that the formulation explained in Equation 3 uses
the vector notation, where Pij is embedded in (cid:60)3.
3.1.2 Surface Point Evaluation
The complete algorithm for forward evaluation of S(u, v) as described in Piegl and Tiller 41 can be divided
into three steps:

1. Finding the knot span of u ‚àà [ui, ui+1) and the knot span of v ‚àà [vj, vj+1), where ui, ui+1 ‚àà U and

vj, vj+1 ‚àà V. This is required for the efÔ¨Åcient computation of only the non-zero basis functions.

4

2. Now, we compute the non-zero basis functions N p

j (v) using the knot span. The basis
functions have speciÔ¨Åc mathematical properties that help us evaluate them efÔ¨Åciently. The partition
of unity and the recursion formula ensure that the basis functions are non-zero only over a Ô¨Ånite span
of p + 1 control points. Therefore, we only compute those p + 1 non-zero basis functions instead of
the entire n basis function. Similarly in the v direction we only compute q + 1 basis functions instead
of m.

i (u) and Nq

3. We Ô¨Årst compute the weighted control points Pw

ij for a given control point Pij = {Px, Py, Pz} and
weight wij as {Pxw, Pyw, Pzw} representing the surface after homogeneous transformation for ease of
computation. Once the basis functions are computed we multiply the non-zero basis functions with
ij . This result, S(cid:48) is then used to compute S(u, v) as
the corresponding weighted control points, Pw
{S(cid:48)
w}.

z/S(cid:48)

w, S(cid:48)

y/S(cid:48)

w, S(cid:48)
x/S(cid:48)
Implementation

3.1.3

In a deep learning system, each module is considered an independent unit that performs the computation.
During the forward pass, the module takes a batch of input and transforms them using the parameters of
the module parameters. Further, to reduce the computations needed during the backward pass, we store
extra information for computing the gradients during the forward computation. The NURBS-Diff module
takes as input the control points, weights, and knot vectors for a batch of NURBS surfaces. We deÔ¨Åne a
parameter to control the number of points evaluated from the NURBS surface. We deÔ¨Åne a mesh grid of
a uniformly spaced set of parametric coordinates ugrid √ó vgrid. We perform a parallel evaluation of each
surface point S(u, v) in the ugrid √ó vgrid for all surfaces in the batch and store all the required information
for the backward computation. The complete algorithm is shown in Algorithm 1.

: U, V, P, W, output resolution ngrid, mgrid

Algorithm 1: Forward algorithm for multiple surfaces
Input
Output: S
Initialize a meshgrid of parametric coordinates uniformly from [0, 1] using ngrid √ó mgrid : ugrid √ó vgrid
Initialize: S ‚Üí 0
for k = 1 : sur f aces in parallel do

for j = 1 : mgrid points in parallel do

for i = 1 : ngrid points in parallel do

Compute uspan and vspan for the corresponding ui and vi using knot vectors Uk and Vk
Compute basis functions Ni and Nj basis functions using uspan and vspan and knot vectors
Uk and Vk
Compute surface point S(ui, vj) (in x, y, and z directions).
Store uspan, vspan, N p

j , and S(ui, vj) for backward computation

i , Nq

end

end

end

Our implementation is robust and modular for different applications. For example, if an end-user desires
to use this for a B-spline evaluation, they need to set the knot vectors to be uniform and weights W to be 1.0.
In this case, the forward evaluation can be simpliÔ¨Åed to S(u, v) = f(P). Further, we can also pre-compute
the knot spans and basis functions during the initialization of the NURBS-Diff module. During computa-
tion, we could use tensor comprehension that signiÔ¨Åcantly increases the computational speed. We can also
handle NUBS (Non-Uniform B-splines), where the knot vectors are still non-uniform, but the weights W
are set to 1.0. Note in the case of B-splines Œ® = {P} (the output from the deep learning framework) and in
the case of NUBS Œ® = {P, U, V}.
3.2 Backward Evaluation for NURBS Surface

In a modular machine learning system, each computational module requires the gradient of a loss function
with respect to the output tensor for the backward computation or the backpropagation. For our NURBS-

5

Diff module this corresponds to ‚àÇL/‚àÇS . As an output to the backward pass, we need to provide ‚àÇL/‚àÇŒ®.
While we represent S for the boundary surface, computationally, we only compute S (the set of surface
points evaluated from S). Therefore, we would be using the notation of ‚àÇS instead of ‚àÇS to represent
the gradients with respect to the boundary surface. Here, we assume that with increasing the number of
evaluated points, ‚àÇS will asymptotically converge to ‚àÇS. Now, we explain the computation of ‚àÇS/‚àÇŒ® in order
to compute ‚àÇL/‚àÇŒ® using the chain rule. To explain the implementation of the backward algorithm, we Ô¨Årst
explain the NURBS derivatives for a given surface point with respect to the different NURBS parameters.
3.2.1 NURBS Derivatives

We rewrite the NURBS formulation as follows:

S(u, v) =

NR(u, v)
w(u, v)

(6)

where,

NR(u, v) =

w(u, v) =

n
‚àë
i=0

m
‚àë
j=0

n
‚àë
i=0

m
‚àë
j=0

N p
i (u)Nq

j (v)wijPij

N p
i (u)Nq

j (v)wij

For the forward evaluation of S(u, v) = f (P , U , V , W), we can deÔ¨Åne four derivatives for a given surface
evaluation point: S,u := ‚àÇS(u,v)/‚àÇu, S,v := ‚àÇS(u,v)/‚àÇv, S,P := ‚àÇS(u,v)/‚àÇP, and S,W := ‚àÇS(u,v)/‚àÇW. Note that S,P
and S,W are represented as a vector of gradients {S,Pij

‚àÄPij ‚àà P} and {Swij ‚àÄwij ‚àà W}.

Now, we show the mathematical form of each of these four derivatives. The Ô¨Årst two derivatives are tradi-
tionally known as the parametric surface derivatives, S,u and S,v. Here, N p
i,u(u) refers to the derivative of
basis functions with respect to u and v, respectively. These are the standard parametric derivatives, and we
do not repeat them here; they are provided in the Appendix for completeness. These derivatives are useful
in the sense of differential geometry of NURBS for several CAD applications 27. However, we do not use it
in our module since many deep learning applications such as surface Ô¨Åtting are not dependent on the (u, v)
parametric coordinates. Also, note that S,u and S,v are not the same as S,U and S,V. The formulation for S,U
and S,V is provided later in this section.
Now, let us deÔ¨Åne S,pij (u, v):

S,Pij (u, v) =

‚àën

k=0

i (u)Nq
N p
l=0 N p
‚àëm

j (v)wij
k (u)Nq

l (v)wkl

(7)

where S,Pij (u, v) is the rational basis functions themselves. Computing S,wij (u, v) is more involved with wij
terms in both the numerator and the denominator of the evaluation.

S,wij (u, v) =

NR,wij (u, v)w(u, v) ‚àí NR(u, v)w,wij (u, v)
w(u, v)2

(8)

where,

NR,wij (u, v) = N p
w,wij (u, v) = N p

i (u)Nq
i (u)Nq

j (v)

j (v)Pij

For the forward evaluation of S(u, v) = f (P , U , V , W), we have deÔ¨Åned S,P(u, v) and S,W(u, v) along
with the derivatives S,u(u, v) and S,v(u, v). However, computing the S,U(u, v) and S,V(u, v) is not trivial.
S,U(u, v) and S,V(u, v) refer to the ‚àÇS(u,v)/‚àÇui, s.t.ui ‚àà U and ‚àÇS(u,v)/‚àÇvi, s.t.vi ‚àà V. U and V inÔ¨Çuence
the computation of the basis functions, and these derivatives are helpful for reparameterization of the sur-
faces by changing the knot vectors. However, due to the recursive computation of the basis functions, the
derivatives for U and V are not deÔ¨Åned. Therefore, we need a more rigorous approach for deÔ¨Åning the
differentiable programming of knot vectors.

6

First, lets decompose the derivative ‚àÇS(u,v)/‚àÇui, s.t.ui ‚àà U1 into the derivative of ‚àÇS(u,v)/‚àÇN
derivative of ‚àÇN

p
i (u) and the partial
p
i (u) can be easily computed from chain rule as shown here:

p
i (u)/‚àÇui. The derivative ‚àÇS(u,v)/‚àÇN

‚àÇS(u, v)
‚àÇN p
i (u)
‚àën

‚àí

r=0
(‚àën

r=0

=

j=0 Nq
‚àëm
j=0 N p
‚àën
‚àëm
r=0
j=0 N p
r (u)Nq
‚àëm
r (u)Nq
j=0 N p
‚àëm

j (v)wijPij
r (u)Nq
j (v)wrjPrj
j (v)wrj)2

j (v)wrj
(cid:16) m
‚àë
j=0

Nq

j (v)wij

(cid:17)

(9)

Now, we evaluate the derivative of N p
the recursive nature of the deÔ¨Ånition, we can accordingly compute the derivatives of N p
fashion using chain rule, provided we can evaluate:

i (u) with respect to the knot points {ui}. We observe that due to
i (u) in a recursive

‚àÇN0
i (u)
‚àÇui

=

‚àÇ1([ui, ui+1])
‚àÇui

(10)

(and likewise for ui+1) where 1 denotes the indicator function over an interval. However, this derivative is
not well-deÔ¨Åned since the gradient is zero everywhere and undeÔ¨Åned at the interval edges.

We propose to approximate this derivative using Gaussian smoothing by rewriting the interval as the differ-
ence between step functions convolved with deltas shifted by ui and ui+1 respectively:

1([ui, ui+1))(u) = sign(u) (cid:63) Œ¥(u ‚àí ui) ‚àí sign(u) (cid:63) Œ¥(u ‚àí ui+1)

and approximate the delta function with a Gaussian of sufÔ¨Åciently small (but constant) bandwidth:

1([ui, ui+1])(u) = sign(u) (cid:63) GœÉ(u ‚àí ui) ‚àí sign(u) (cid:63) GœÉ(u ‚àí ui+1)

where

GœÉ(u ‚àí ¬µ) =

‚àö

1
2œÄœÉ2

exp(‚àí

(u ‚àí ¬µ)2
2œÉ2

)

The derivative with respect to ¬µ is therefore given by:

G(cid:48)

œÉ(u = ¬µ) =

(u ‚àí ¬µ)
2œÉ2 GœÉ(u ‚àí ¬µ)

(11)

(12)

(13)

(14)

which means that the approximate gradient introduces a multiplicative (u ‚àí ¬µ) factor with the original
basis function.

p
i (u)/‚àÇui by recursively deÔ¨Åning the derivatives ‚àÇN0

i (u)/‚àÇui, until
Therefore, now we can compute ‚àÇN
p‚àí1
p
p‚àí1
i (u)/‚àÇN
(u)/‚àÇui. The derivative of ‚àÇN
‚àÇN
(u) can easily be obtained from chain rule of Equation 4. Now,
i
i
p
i (u)/‚àÇU and Ô¨Ånally obtain ‚àÇS(u,v)/‚àÇU. Same
we perform the same operations of ‚àÇN
operations can be performed to obtain ‚àÇS(u,v)/‚àÇV. With all these operations for each point parameterized in
the surface by (u, v), we extend this to all the surfaces as explained in the next section.
3.2.2

i (u)/‚àÇui‚àÄui ‚àà U to obtain ‚àÇN

Jacobian for Surface Evaluation

i (u)/‚àÇui, ‚àÇN1

p

We deÔ¨Åne the Jacobian for the NURBS evaluation, which is then directly used for the backward evaluation.
The Jacobian for each surface evaluation point Si,j is represented as the vector:

Bi,j =

Ô£´

Ô£¨
Ô£¨
Ô£≠

{S,pij (u, v)}‚àÄi ‚àà [1, m], ‚àÄj ‚àà [1, n]
{S,wij (u, v)}‚àÄi ‚àà [1, m], ‚àÄj ‚àà [1, n]
{S,ui (u, v)}‚àÄui, ‚àà U
{S,vj (u, v)}‚àÄvj, ‚àà V

Ô£∂

Ô£∑
Ô£∑
Ô£∏

(15)

1We explain the formulation for U; a similar formulation exists for V.

7

Each Jacobian vector represented here is the contribution of the gradient from one evaluation point at the
grid locations (i, j) in the parametric coordinate space. These Jacobian vectors are each of length 4nm.
However, as noted in the previous section on forward evaluation, the basis functions satisfy the partition of
unity and span only p + 1 control points starting from uspan (correspondingly, q + 1 control points starting
from vspan in the other parametric direction). Therefore, the total number of non-zero elements in a 4nm
size Jacobian vector is 4(p + 1)(q + 1), making it sparse. However, note that this Jacobian is for only one
surface point. The complete Jacobian for the backward pass is given as:

J =

Ô£´

Ô£¨
Ô£¨
Ô£¨
Ô£≠

B1,1
B1,2
...
Bmgrid,ngrid

Ô£∂

Ô£∑
Ô£∑
Ô£∑
Ô£∏

.

(16)

The size of this Jacobian is ngridmgrid √ó 4nm. Here, Bi,j is the Jacobian for one surface point evaluation. As
the parametric coordinates keep changing, the position of uspan and vspan keep changing, and the location of
the non-zero elements keeps shifting to form a block diagonal matrix. This Jacobian is ‚àÇS/‚àÇŒ®. For completing
the backward pass, we multiply ‚àÇL/‚àÇS to ‚àÇS/‚àÇŒ®, giving us ‚àÇL/‚àÇŒ®. Since, each module in the deep learning
framework is independent and modular, we just return this output for the NURBS backward evaluation.

3.2.3

Implementation

For the implementation of the backward pass, since the basis functions are block sparse, we make use of
the stored information of uspan and vspan for identifying the index of the control points derivative and we
use the stored basis functions information for computing the Jacobian explained above. This computation
is performed for all the surfaces in the batch. This complete algorithm is explained in detail in Algorithm 2.

Algorithm 2: Backward Algorithm

: S(cid:48)

Input
Output: P(cid:48), W(cid:48)
Initialize: P(cid:48) ‚Üí 0
Initialize: W(cid:48) ‚Üí 0
for k = 1 : sur f aces do
for j = 1 : mgrid do

for i = 1 : ngrid do

Retrieve uspan, vspan, N p
for r = 0 : p + 1 do

i , Nq

j , S(u, v)

for h = 0 : q + 1 do

P(cid:48)
W(cid:48)
U(cid:48)
V(cid:48)

uspan+r,vspan+h = S,pij (ui, vj)
uspan+r,vspan+h = S,wij (ui, vj)
uspan+r = S,uspan+r(ui, vj)
vspan+h = S,vspan+h(ui, vj)

end

end

end

end

end

3.3 GPU Implementation
We implemented the code in Python 3.6 44. The backend for the GPU-accelerated code is written in C++
using the Pybind11 API 24 and CUDA toolkit 37 for GPU acceleration and is integrated with PyTorch 39
using a custom layer deÔ¨Ånition. The forward evaluation can be performed for each surface in the batch
for each tuple (u, v) in the mesh grid of ugrid √ó vgrid in parallel. Further, the three coordinates x, y, z are

8

evaluated simultaneously. This enables an embarrassingly parallel implementation on the GPU for the
forward evaluation of the NURBS-Diff module. Each x, y, and z component is mapped to a separate thread
on the GPU using the 3D block and grid structure in CUDA. The same process is employed in the backward
algorithm with one additional operation. Each surface point gradient needs to be added to several control
points that lie in the evaluated point‚Äôs span during the backward pass. Hence we perform this operation of
the gradient update using a scatter operation by using the indices stored from uspan and vspan.

4 CAD Applications using the NURBS-DiÔ¨Ä Module
The differentiable programming approach explained above is designed mainly for deep learning applica-
tions. However, we can also use the framework for standard CAD operations, such as curve Ô¨Åtting, surface
Ô¨Åtting, and surface offsetting. Note that some of these operations could be performed much faster using
traditional approaches explicitly optimized for each application. However, using NURBS-Diff along with
gradient-descent-based optimization approaches for these CAD applications is not well explored. More-
over, this shows the versatility of the NURBS-Diff module in handling traditional CAD operations as con-
straints in a deep learning system.

To use our differentiable programming approach for CAD applications, we have to deÔ¨Åne two key ele-
ments: a loss function L for computing the gradients and an optimization algorithm. We consider four
loss functions: L1 loss, L2 loss (also called as mean squared error), the Chamfer distance LCD, and the
Hausdorff distance LHD.
The L1 loss can be mathematically deÔ¨Åned as:

L1(P, Q) =

(cid:16)

1
npoints

‚àë
(Pi,Qi)‚àà(P,Q)

||Pi ‚àí Qi||1

(cid:17)

(17)

Here, ||Pi ‚àí Qi||1 refers to the L1 norm of the difference between the two points Pi and Qi. Similarly, we
can deÔ¨Åne L2 loss based on the L2 norm.

L2(P, Q) =

(cid:16)

1
npoints

‚àë
(Pi,Qi)‚àà(P,Q)

||Pi ‚àí Qi||2

(cid:17)

(18)

While both the L1 and L2 loss functions are pairwise distance metrics, the Chamfer distance (LCD) and the
Hausdorff distance (LHD) are global distance metrics between two sets of points as shown below:

LCD = ‚àë
Pi‚ààP

min
Qj‚ààQ

||Pi ‚àí Qj||2 + ‚àë
Qj‚ààQ

min
Pi‚ààP

||Pi ‚àí Qj||2

(cid:32)

(cid:33)

LHD = max
Pi‚ààP

min
Qj‚ààQ

||Pi ‚àí Qj||2

+ max
Qj‚ààQ

(cid:18)

(cid:19)

||Pi ‚àí Qj||2

min
Pi‚ààP

(19)

(20)

For each CAD application, a target point cloud Q is obtained directly from measurements (for the case of
Ô¨Åtting from point clouds) or from analytical computations (for CAD operations such as surface offsetting).
While our formulation works well when we initialize the control points and weights to random values
(Gaussian distributed), we can also initialize it using a prior for faster convergence. After we initialize the
control points P and weights W, we initialize the knot vectors U and V. While the knot vectors are Ô¨Åxed
in most applications we demonstrate, we show one example where we could reparameterize the surface
to obtain a better Ô¨Åt. We evaluate the surface using P, W, U, and V and compute the loss between the
evaluated surface S and Q using the appropriate loss function L. We perform an update using gradient
descent algorithms and their variants. We can write a simple update for the NURBS parameters as:

Œ® = Œ® ‚àí Œ±

‚àÇL
‚àÇŒ®

(21)

While our formulation can use a simple gradient descent algorithm, in our work, we use more sophisticated
algorithms such as stochastic gradient descent (SGD), SGD with momentum, Adam 26, and Adagrad 30.

9

Our experiments illustrated in the Appendix show that SGD with momentum and Adam perform well
in all scenarios and have faster convergence. Now, we discuss the speciÔ¨Åc CAD applications using our
NURBS-Diff module.

4.1 Curve Fitting

We Ô¨Årst demonstrate curve Ô¨Åtting using our NURBS-Diff module. While the problem of Ô¨Åtting splines to
point clouds using data-driven techniques has been extensively studied, we show curve Ô¨Åtting to validate
our approach and provide insights (such as the convergence of the optimization, behavior of the Ô¨Åt with the
variation in control points, evaluation points, etc.). In this validation case, we initialize a uniform knot vec-
tor U (which is kept Ô¨Åxed) and compute the non-zero basis functions N(u) and uspan as a pre-computation
step before evaluating the points on the curve. This pre-computation step reduces the computational time
signiÔ¨Åcantly (due to the removal of repeated basis function evaluations). This approach can be used when
we are trying to Ô¨Åt a B-spline curve/surface where the basis functions do not change or if the user does not
want to change the parameterization of the curve or surface.

For a simple validation, we sample points from an analytically deÔ¨Åned curve y = sin(x) + 2sin(2x) +
sin(4x) and obtain the best-Ô¨Åt curves for different control points and evaluation points as shown in Figure 2.
Similarly, for points sampled from a 3D helical curve, we Ô¨Åt a B-Spline curve as shown in Figure 3. The
behavior of our Ô¨Åtting method with different loss functions and the number of control points is shown in

(a) 16 control points, 64 evaluation points

(b) 32 control points, 64 evaluation points

(c) 64 control points, 64 evaluation points

Figure 2: Curve Ô¨Åtting to points from an analytically generated curve y = sin(x) + 2sin(2x) + sin(4x) with different number of control points.

Figure 3: Curve Ô¨Åtting for points sampled from a 3D helical curve in R3 for different numbers of control points and evaluated points.

Figure 4: The loss performance with the number of iterations for Ô¨Åtting the 3D helical curve shown in Figure 3. The behavior of the loss is shown
for different loss functions and different numbers of control points used for the Ô¨Åtting.

10

Figure 5: Curve Ô¨Åtting on point cloud data obtained from binary images. We convert the image data into a point cloud using the pixel size and
the locations of the pixels. Once we obtain the point cloud, we use a curve-Ô¨Åtting module with Chamfer distance loss function and regularization
to obtain these results.

Table 1: Performance (LCD) of Ô¨Åtting different curves using the NURBS-Diff module with and without curve length regularization.

Curve
Analytical
Helix
Apple
Flower
Bunny

No Regularization Regularization
0.0016
0.0320
1.6863
0.8383
1.4099

0.0025
0.0370
8.5267
23.2484
50.0020

Figure 4. While all the loss functions converge well with the number of iterations, L2 loss and LCD losses
have better convergence characteristics. LCD loss plateaus after some iterations since that is the best error
that could be achieved for a given number of control points and evaluation points. Please note that the LCD
also reduces the oscillations that might occur in Ô¨Åtting a higher degree curve.

We extend the curve Ô¨Åtting framework to a more general Ô¨Åtting of random unordered point cloud data.
For the 2D point cloud data, we use the images from the Pixel dataset of the Skelneton challenge 15 and
sample points from the object boundaries as shown in Figure 5. Since the problem is ill-posed (due to the
unordered aspect of the point cloud), we initialize the control points using randomly sampled points from
the point cloud. To avoid unnecessary loops and self-intersections in our output curve, we use a curve-
length regularization term in addition to the LCD. As shown in Figure 5 our framework Ô¨Åts the target well
for point clouds that are not excessively complex. Some complex point cloud data that had self-intersections
are illustrated in the Appendix.

We also analyze the performance of our Ô¨Åtting method for different curves in Table 1. For a consistent
comparison, we use the LCD between a dense set of points evaluated on the Ô¨Åtted curve (using 16 control
points) and the input point cloud. We set the number of evaluation points to be twice the number of points
in the input. Our NURBS-Diff module achieves better Ô¨Åtting results with the curve length regularization
added. While this difference is less evident for simple analytical curves, the advantage of using a curve
length regularization is more pronounced over curves Ô¨Åtted using the Pixel dataset.

4.2 Surface Fitting

We extend the validation of our NURBS-Diff module to the Ô¨Åtting of NURBS surfaces. In this case, we
consider two scenarios for testing the Ô¨Åtting process. In the Ô¨Årst scenario, we keep the knot vectors U and V
Ô¨Åxed. In this scenario, we can precompute the basis functions N p
j (v) along with their respective
span indices, uspan and vspan). In the second scenario, we allow the knot vectors to be changed, leading to
the reparameterization of the surface to obtain a better Ô¨Åt. We cannot precompute the basis functions in

i (u) and Nq

Table 2: Performance L2 of the different surface CAD applications using the NURBS-Diff module. B-Spline refers to Ô¨Åxed uniform knot vector,
and NURBS refers to non-uniform knot vector obtained through optimization.

Test Case
Analytical
Ducky

B-Spline NURBS
0.005262
0.039528
0.000180
0.000195

11

Figure 6: Surface Ô¨Åtting using NURBS for points sampled from an analytical surface z = sin(x) ‚àó cos(y).

Figure 7: NURBS surface Ô¨Åtting for point cloud representation of Ducky‚Äôs body.

Table 3: L2 error for Ô¨Åtting different control point mesh sizes for the analytical surface with 128√ó128 evaluation points and surface degree 3.

Control Points
6√ó6
9√ó9
12√ó12
24√ó24
48√ó48

L2 Error
2.2719 √ó 10+1
3.9528 √ó 10‚àí2
6.9547 √ó 10‚àí4
3.7953 √ó 10‚àí7
1.0997 √ó 10‚àí7

the second scenario since the knot vector is updated each iteration. We deÔ¨Åne a set of points sampled from
the analytical surface z = xysin(x)cos(y) to compare both scenarios. The control points and weights are
initialized randomly for the Ô¨Åtting, while the knot vectors are initialized as uniformly spaced. Using the
L2 loss for the optimization, we perform the Ô¨Åtting for both the scenarios as shown in Figure 6. While the
Ô¨Årst scenario provides a good Ô¨Åt, we can reduce the minor oscillation errors in the surface Ô¨Åt with the knot
optimization to obtain an order of magnitude lower L2 loss for the second scenario, as shown in Table 2.
Note that both the surface Ô¨Åts look similar and Ô¨Åt the target surface well, but the surface reparameterization
reduces the overall error. The changes in the knot vector are depicted using the gridlines on the heatmap
showing the error.

We extend this surface Ô¨Åtting to a more complicated surface (the Ducky shown in Figure 7). We have the
target control points, weights, and knot vectors for this geometry. Using this information, we sample points
from the geometry uniformly and then use that as a target for performing surface Ô¨Åtting. We only show the

12

ùëß=ùë•ùë¶ùë†ùëñùëõùë•cos(ùë¶)‚àí5<ùë•<5,‚àí5<ùë¶<5xyzSurfacePredicted NURBS (no reparameterization)Predicted NURBS (with reparameterization)-0.0050.005-0.0050.005Pointwise Normalized MSE0.00.20.40.60.81.0u0.00.20.40.60.81.0v0.0010.0000.001Target SurfaceTarget Control PointsPredicted SurfacePredicted Control PointsTable 4: L2 error for Ô¨Åtting a 9√ó9 control mesh to the analytical surface for degree 3 with different numbers of evaluated points.

Evaluation Points L2 Error
0.0402
0.0395
0.0391
0.0389

64√ó64
128√ó128
256√ó256
512√ó512

Table 5: L2 error for different degrees of a 9√ó9 control mesh to the analytical surface using 128√ó128 evaluation points.

Degree L2 Error
0.3615
0.0702
0.0395
0.0585

1
2
3
4

results for the best Ô¨Åt surface with knot vector optimization for brevity. We observe improvement in the Ô¨Åt
by performing knot optimization as shown in Table 2. However, the improvement is not as pronounced as
seen in the analytical geometry in this case.

Finally, we study the performance of our NURBS-Diff module for Ô¨Åtting the analytical surface shown in
Figure 6 using a variety of control point sizes, evaluation point sizes, and degrees. To benchmark the per-
formance of our module, we study the Ô¨Årst 500 iterations of the optimization (both forward and backward
pass) and report the L2 loss. The results of these experiments are highlighted under Table 3, Table 4 and
Table 5. We observe that 12 control points are required to Ô¨Åt the surface properly. As expected, increasing
the number of control points reduces the L2 error. For the number of evaluation points, a similar trend is
observed, where increasing the number of evaluation points reduces the L2 error. Similarly, the L2 error
decreases as the degree of the surface is increased until it stabilizes, after which it shows a slight increase in
the error probably due to overÔ¨Åtting oscillations. Particularly, in Table 5, we notice that the error is higher
with a lower degree due to being an underdetermined system. With increasing the degree, the Ô¨Åt improves,
and then Ô¨Ånally, at degree 4, the system becomes overdetermined and hence overÔ¨Åts.

4.3 Surface OÔ¨Äsetting

Generating an offset surface is one of the fundamental CAD operations. Traditionally an offset surface for
NURBS is generated by Ô¨Årst performing a B√®zier decomposition of the NURBS surface and performing the
offset for each patch. However, this changes the parameterization of the resulting offset surfaces. However,
speciÔ¨Åc applications might require an offset surface with the same parameterization. We can easily use our
NURBS-Diff module to perform such an offset operation. Note that this approach only works for small
offset distances such that the topology of the offset surface does not change.

To perform the offset operation, we Ô¨Årst compute a dense set of points and normals at speciÔ¨Åc (u, v) and
calculate the points on the corresponding offset surface by moving the points along the normal by the offset
distance. We then Ô¨Åt a NURBS surface for the offset point cloud data using the same parameterization of
the base surface using the surface Ô¨Åtting method (see Section 4.2).

Using our offsetting method, we can also generate offsets of objects consisting of multiple NURBS surfaces.
Generating offset surfaces from multiple base surfaces requires applying constraints on the control points
on the common edges of the base surfaces. While computing the surface normals, we identify the surface
points along the shared edge. We then compute the average normals of these common points and then
normalize them. Calculating the average normal allows us to ensure the continuity of the offset point cloud
data. We then apply the constraints on the Ô¨Åtted control points of the common edges to be the same on both
the surfaces that share the common edge. To ensure this continuity for the NURBS surfaces, we create a list
of the shared control points. After the surface Ô¨Åtting iteration, the control points are updated; based on the
shared list, i.e., the control points of the common edge are assigned the same coordinates. We perform this
by computing the average of all the common points and setting them the same on both surfaces.

13

Figure 8: Test cases for NURBS surface offset (a) Double curved surface, (b) C0 continuous multi patch, (c) C1 continuous multi patch, and (d)
C1 continuous patches of the aerofoil proÔ¨Åle of a wind turbine blade.

Table 6: Normalized Chamfer distance between the offset surface and the offset point cloud using our NURBS-Diff module and SGD compared to
a direct offset of the control points (CP Offset). The Chamfer distance is normalized using the minimum size of the bounding box of the base surface
along the offset direction.

Test Case
Double Curve
Multi patch - C0
Multi patch - C1
Aerofoil Surface

Min BB Size Offset Distance LCD NURBS-Diff Offset LCD CP Offset
0.0239
0.0008
0.0390
0.0545

0.0235
0.0006
0.0389
0.0529

11.54
0.75
6.32
0.66

1.50
0.10
2.00
0.25

Figure 8 shows examples of the surface offsets generated using our NURBS-Diff module. Figure 8(a) is
a single surface patch with a double-curved surface. Figure 8(b) is a set of C0 continuous surfaces with a
single shared edge. Figure 8(c) is a set of C1 continuous conic section surfaces. Figure 8(d) is a set of surfaces
from a wind turbine blade model. To compare the accuracy of our NURBS-Diff module approach for offset
surfaces, we computed a simpler offset by offsetting the control points along the average normal direction
of the control mesh. This approach works well for surfaces with low curvature. We then evaluated the
computed offset surface at 25√ó denser points than the number of points used for Ô¨Åtting and computed the
LCD with the input offset points. We Ô¨Ånd that our Ô¨Åtting approach achieves a lower LCD than the control
point offset approach for all cases, as seen in Table 6.

4.4 Timings for NURBS-DiÔ¨Ä

In the previous sections, we demonstrate how the NURBS-Diff module can perform CAD operations such
as curve Ô¨Åtting, surface Ô¨Åtting, and surface offsetting. In this section, we assess the computational per-
formance of our module. For brevity, we restrict our analysis to surface Ô¨Åtting operation and analyze the
timings with variations in the number of control points, evaluation points, and surface degree. We only
study the Ô¨Årst 500 iterations (which include both the forward and backward pass). We perform all our
experiments on a desktop with a 32 core 2.4 GHz Intel Xeon processor, 64 GB RAM, and an NVIDIA Titan
Black GPU with 6 GB RAM.

We analyze the timings against the different number of control points as shown in Table 7. We begin with
the minimum number of control points we would get meaningful surfaces for the given experiment ( i.e.,
6√ó6 control points). We observe that the timings increase with the number of control points. Variations
in iteration times for different evaluation point sizes are shown in Table 8. Similar to Table 7, there is a
steady increase in iteration times with an increase in the number of evaluation points. Note that, while the
iteration time is increasing drastically (especially when going from 256 √ó 256 to 512 √ó 512), the performance

Table 7: Time to Ô¨Åt a surface for different numbers of control points.

Control Points
6 √ó 6
12 √ó 12
24 √ó 24
48 √ó 48

Iteration time (s)
0.098
0.106
0.110
0.110

14

(a)(b)(c)(d)Offset NURBSBase NURBSOffset NURBS CPTable 8: Time to Ô¨Åt a surface for different numbers of evaluation points.

Evaluation Points
64 √ó 64
128 √ó 128
256 √ó 256
512 √ó 512

Iteration time (s)
0.074
0.120
0.170
0.266

Table 9: Computation time to Ô¨Åt a surface of different degrees.

Degree
1
2
3
4

Iteration time (s)
0.074
0.120
0.170
0.266

of the Ô¨Åt does not improve much, as seen in Table 4. Therefore, the end-user must judiciously choose the
number of evaluation points sampled for the NURBS-Diff module to get an accurate Ô¨Åt of the surface while
not increasing the computational time. Finally, increasing the degree also increases the time required to Ô¨Åt
a surface, as shown in Table 9. However, for most cases, it can be seen the NURBS-Diff module can perform
5-10 iterations per second, which makes it tractable for Ô¨Åtting a large number of surfaces.

5 Point Cloud Reconstruction
In this section, we show the utility of the NURBS-Diff module for unsupervised point cloud reconstruction.
We use the experiments performed by Sharma et al. 46 as our baseline, which is a supervised learning frame-
work for surface reconstruction framework. Sharma et al. 46 introduced an end-to-end trainable network
called ParSeNet that Ô¨Åts an assembly of geometric primitives, including B-spline patches, to a segmented
point cloud. In the ParSeNet framework, the authors develop a spline Ô¨Åtting module (called SplineNet)
which takes an input point cloud and reconstructs a spline surface. This surface, however, is obtained us-
ing a supervised learning approach, as we highlight later. Therefore, to improve the framework and obtain
a better Ô¨Åt without the supervised labels, we integrate our NURBS-Diff module with SplineNet to develop
an unsupervised training approach for spline Ô¨Åtting.

The ParSeNet framework is divided into three stages. The Ô¨Årst stage incorporates prior work done in point
cloud segmentation 43 to decompose the input point cloud into segments classiÔ¨Åed under a parametric patch
type. The second stage is the spline Ô¨Åtting SplineNet that generates B-spline patches to the segmented point
cloud data. The Ô¨Ånal stage performs geometric optimizations to seamlessly stitch the collection of predicted
primitives together into a single object. We are interested in replacing the surface evaluation performed in
the SplineNet stage of their network with our NURBS-Diff module for the experiments in this section.
For training and testing our experiments, we use the SplineDataset provided by Sharma et al. 46. The
SplineDataset is a diverse collection of open and closed splines that have been extracted from one million
CAD geometries included in the ABC dataset. A random set of points is sampled from the surfaces as the
input for the point cloud reconstruction task. We run our experiments on open splines split into 24K, 4K,
and 4K for training, testing, and validation.

In our work, we focus only on the SplineNet module of the ParSeNet framework to illustrate the applica-
bility of our proposed NURBS-Diff module. SplineNet is a module for performing supervised learning of
B-spline surfaces using the SplineDataset. The training procedure includes a loss deÔ¨Ånition as follows:

LSplineNet = LCD + Œª1LLapMat + Œª2LCP.

(22)

Here, LCD refers to the Chamfer distance between the input point cloud and the surface points evaluated
on the target surface. LLapMat is the Laplacian matching loss where the difference in the Laplacian (the
second derivative of the control points mesh obtained using a Sobel Ô¨Ålter) of the Ô¨Åtted control points and
the target control points is minimized. LCP is the control point regression loss which is L2 error between

15

Table 10: Comparison between SplineNet 46 and our NURBS-Diff implementation (with different number of control points). We compare the
two-sided Chamfer distance (scaled by 100, 100√óLCD), the two-sided Hausdorff distance (scaled by 100, 100√óLHD) between the input point
cloud and a dense set of points sampled on the Ô¨Åtted surface, and also the Laplacian of the predicted control points LLap (scaled by 100, 100√óLLap).

Loss Function

LCD + LLapMat + LCP
LCD + 0.1LLap + 10LHD
LCD + 0.1LLap + LHD
LCD + 0.1LLap
LCD
LHD

Baseline
(20 √ó 20)
LHD
0.994

LLap
1.340

LCD
1.184

NURBS-Diff
(20 √ó 20)
LHD

NURBS-Diff
(5 √ó 5)
LHD

NURBS-Diff
(4 √ó 4)
LHD

LLap

LCD

LLap

LCD

0.234

0.725
2.914
0.329
0.192

0.202

0.106
0.189
42.288
30.877

0.020

0.032
0.105
0.013
0.018

0.256

0.805
3.511
0.446
0.210

2.462

1.972
1.296
21.236
23.076

0.028

0.047
0.113
0.015
0.026

0.274

1.173
3.602
0.499
0.282

LLap

3.356

3.606
2.637
28.212
32.443

LCD

0.018

0.027
0.098
0.012
0.018

the predicted and the target control points. Œª1 and Œª2 are used for weighting different loss functions. For
brevity, we use the same values for these constants as reported in Sharma et al. 46 .

In the SplineNet approach, the LLapMat and the LCP make the framework supervised since they require a
target Ô¨Åtted control point mesh to compute these losses. In real life, obtaining the target control point mesh
for point clouds is challenging. Therefore, we need an unsupervised learning approach for point cloud
reconstruction of spline surfaces. To make the framework unsupervised and not require the target control
points of the spline surface, we modify the framework to the following:

LNURBS-Diff = LCD + Œª1LLap + Œª2LHD.

(23)

Here, LCD and LHD refers to the Chamfer distance and the Hausdorff distance between the input point
cloud and the surface points evaluated using the NURBS-Diff module. LLap is a modiÔ¨Åed loss of LLapMat
where instead of matching the Laplacian of the predicted control points and actual control points, we min-
imize the Laplacian itself. Further, we scale down the contribution of LLap by an order of magnitude to
reduce any adverse effects from minimizing the Laplacian. In addition, to Ô¨Åt the edges of the surfaces well,
we add a Hausdorff distance loss to the objective function. We use different scale factors Œª1, Œª2 for LLap
and LHD to tune the objective function to obtain the best possible results.

Apart from the loss function deÔ¨Åned above, the baseline supervised approach is restricted to non-rational
B-spline surfaces because of no target weights. Here, in our case, we can perform the Ô¨Åtting for rational
B-Spline surfaces or even B-Spline surfaces with reparameterized knots. Since the dataset available is very
primitive and does not contain many complex structures, we restrict our analysis to just studying rational
B-Splines reconstruction from the point clouds in an unsupervised manner.

Table 10 illustrates all the experiments we have performed for choosing an appropriate loss function. Per-
forming surface reconstruction under no supervision is very challenging, and therefore several metrics
have to be compared together to understand the performance. In Figure 9, we illustrate how LCD alone
provides a bad Ô¨Åt by not covering the edges of the surfaces. Similarly, LHD alone does not perform well.
Therefore, we need a loss function that performs well in combination. We compare the results obtained
from our approach with the baseline results obtained from Sharma et al. 46 , as illustrated in Table 10. We

Figure 9: Comparison of the reconstructed B-spline surfaces obtained using NURBS-Diff module for different loss functions using a 5√ó5 control
mesh.

16

Predicted Control MeshPredicted SurfaceBaseline‚ÑíCD+0.1‚ÑíLap+10‚ÑíHD‚ÑíCD+0.1‚ÑíLap+‚ÑíHD‚ÑíCD+0.1‚ÑíLap‚ÑíCD‚ÑíHDPoint CloudFigure 10: Point clouds and unsupervised reconstruction of B-spline surfaces (20√ó20 and 5√ó5) predicted by SplineNet using the NURBS-Diff
module.

observe that our approach performs an order of magnitude better than the baseline architecture in terms
of LCD. Further, unlike Table 3 where we have a complex analytical surface, the SplineDataset includes a
collection of simpler open and closed surfaces. Therefore, we observe that reducing the number of control
points required for representing the surface does not affect the Ô¨Åtting error as demonstrated by the (5√ó5)
and (4√ó4) columns in Table 10. Also, we note that using just LCD and LHD is not recommended due to high
laplacian loss. Further, the combination of LCD + 0.1LLap + 10LHD gives the best result in terms of com-
bined loss. We can also verify the same visually from Figure 9. We also visualize a few anecdotal predicted
surfaces along with the input point cloud in Figure 10. We see that the surfaces in the SplineNet dataset are
not complex enough to require a large 20√ó20 control point mesh and can be easily Ô¨Åtted with a small 4√ó4
control point mesh.

6 Geometric Constraints using NURBS-DiÔ¨Ä
One of the advantages of our NURBS-Diff module is the ease of enforcing surface constraints in deep-
learning applications. We showcase the utility of our module in enforcing constraints using the example of
valve deformation analysis. Analyzing bioprosthetic heart valves is essential for obtaining diagnostic infor-
mation such as estimating the remaining life, fatigue, and patient-speciÔ¨Åc design. Traditionally, analysis of
deformation behavior is performed using Ô¨Ånite element or isogeometric analysis. However, such analyses
are often computationally intensive. Recently, Balu et al. 4 proposed a deep learning framework for per-
forming Ô¨Ånite element analysis (called DLFEA) using a NURBS-aware convolutional neural network. Each
heart valve is represented using three NURBS surface patches, and isogeometric analysis is performed to
obtain the deformations for each control point under constant pressure applied during the valve closure.
Fixed boundary conditions are applied on the valve edges that will be sutured to the aorta. Their previous
work generated a large dataset of input geometry, pressure, thickness, and corresponding target deforma-
tions of the control points. However, their work does not perform a NURBS surface reconstruction loss
for obtaining the deformations, which are more physically meaningful. Further, they did not explore the
application of the Ô¨Åxed boundary conditions; it was implicitly enforced by modifying the loss function.

In this work, we use the dataset and deep learning framework available from their work to demonstrate
the utility of our NURBS-Diff module in performing an evaluation of the deformations for the leaÔ¨Çets of
the bioprosthetic heart valve. We perform a comparison between DLFEA, DLFEA with additional surface
reconstruction loss (DLFEA-SR), and DLFEA with additional surface reconstruction loss and boundary
condition enforcement loss (DLFEA-BC). We show the visualization of DLFEA-BC along with the input
geometry, the target deformed geometry, and the predicted deformed geometry in Figure 11. In DLFEA-
BC, apart from the surface reconstruction loss, we add a constraint (BC loss) that the Dirichlet boundary
conditions on the edge which is sutured to the aorta must be satisÔ¨Åed (i.e., the edges closest to the blue
region in Figure 11 must have zero deformation).

17

NURBS-Diff20x20NURBS-Diff4x4Predicted Control MeshInput Point CloudPredicted SurfaceFigure 11: Visualization of an anecdotal bioprosthetic heart valve for DLFEA-BC from the test dataset. We show the input geometry, the target
deformed geometry, and the predicted deformed geometry.

Table 11: Performance comparison of DLFEA, DLFEA-SR, and DLFEA-BC. We compare the error on the test dataset for control points recon-
struction, NURBS surface reconstruction, and boundary condition enforcement.

CP loss
Test Case
7.79√ó10‚àí3
DLFEA
12.84√ó10‚àí3
DLFEA-SR
DLFEA-BC 12.83√ó10‚àí3

SR loss BC loss
37.69
46.14
10.90

4.47√ó10‚àí3
4.76√ó10‚àí3
5.69√ó10‚àí3

In Table 11, we show the results obtained on a test dataset (not used during the training). The CP loss is the
L2 loss between the input control points and the target control points, whereas the surface reconstruction
(SR) loss represents the L2 loss between the NURBS surface reconstruction of the target deformed shape
and the actual deformed shape. We observe that DLFEA performs very well for CP loss (naturally because
it was originally trained using that loss) but does not do well on the BC loss. The DLFEA-SR performs
comparably for SR loss but has worse performance for BC loss. At the same time, DLFEA-BC performs the
best for BC loss while performing comparably (although worse) on CP loss and SR loss. While the DLFEA-
BC performs worse, the enforcement of boundary conditions makes it more physically meaningful. Further,
in this ablation study, we show the result for each loss function applied independently. In general, we use
a combined loss with several loss functions in tandem as done in Section 5.

7 Conclusions
We have developed a differentiable NURBS module that can be directly integrated with existing machine
learning frameworks. We have developed a mathematical framework that enables both forward evaluation
and backpropagation of the losses while training. Our module is GPU-accelerated to allow fast evaluation
of NURBS surface points and fast backpropagation of the derivatives. We have demonstrated the utility of
our NURBS module for several CAD applications and deep learning applications. NURBS-Diff performs at
the same level as existing standalone spline solutions used previously in the literature. Future work on the
NURBS module includes developing support for trimmed NURBS surfaces and integrating complex curve
constraints along trim edges for the watertight representation of CAD models. We have released the code
for our NURBS module along with this paper. We believe this NURBS module will be the Ô¨Årst step to better
integrate deep learning with CAD and would lead to more diverse machine learning CAD applications.

Acknowledgement
This work was partly supported by the NSF under grant CMMI-1644441 and the ARPA-E under DIF-
FERENTIATE:DE-AR0001215. This work used the Extreme Science and Engineering Discovery Environ-
ment (XSEDE), which is supported by NSF grant ACI-1548562 and the Bridges system supported by NSF
grant ACI-1445606, at the Pittsburgh Supercomputing Center (PSC).

18

Input GeometryTarget Deformed GeometryPredicted Deformed GeometryReferences
[1] Aln√¶s, M., Blechta, J., Hake, J., Johansson, A., Kehlet, B., Logg, A., Richardson, C., Ring, J., Rognes,

M.E., Wells, G.N., 2015. The FEniCS project version 1.5. Archive of Numerical Software 3.

[2] Atzmon, M., Lipman, Y., 2020. SAL: Sign agnostic learning of shapes from raw data, in: Proceedings

of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2565‚Äì2574.

[3] Balestriero, R., et al., 2018. A spline theory of deep learning, in: International Conference on Machine

Learning, PMLR. pp. 374‚Äì383.

[4] Balu, A., Nallagonda, S., Xu, F., Krishnamurthy, A., Hsu, M.C., Sarkar, S., 2019. A deep learning
framework for design and analysis of surgical bioprosthetic heart valves. ScientiÔ¨Åc Reports 9, 1‚Äì12.

[5] Barill, G., Dickson, N.G., Schmidt, R., Levin, D.I.W., Jacobson, A., 2018. Fast winding numbers for

soups and clouds. ACM Transactions on Graphics 37, 1‚Äì12.

[6] Baydin, A.G., Pearlmutter, B.A., Radul, A.A., Siskind, J.M., 2018. Automatic differentiation in machine

learning: A survey. arXiv arXiv:1502.05767.

[7] Blondel, M., Teboul, O., Berthet, Q., Djolonga, J., 2020a. Fast differentiable sorting and ranking. ArXiv

abs/2002.08871.

[8] Blondel, M., Teboul, O., Berthet, Q., Djolonga, J., 2020b. Fast differentiable sorting and ranking. arXiv

arXiv:2002.08871.

[9] Chen, W., Ling, H., Gao, J., Smith, E., Lehtinen, J., Jacobson, A., Fidler, S., 2019. Learning to predict
3D objects with an interpolation-based differentiable renderer, in: Advances in Neural Information
Processing Systems, pp. 1‚Äì11.

[10] Chen, Z., Tagliasacchi, A., Zhang, H., 2020. BSP-Net: Generating compact meshes via binary space

partitioning, in: Conference on Computer Vision and Pattern Recognition, pp. 45‚Äì54.

[11] Cho, M., Joshi, A., Lee, X.Y., Balu, A., Krishnamurthy, A., Ganapathysubramanian, B., Sarkar, S.,
Hegde, C., 2021. Differentiable programming for piecewise polynomial functions, in: Association
for the Advancement of ArtiÔ¨Åcial Intelligence Conference, pp. 1‚Äì10.

[12] Cuturi, M., Teboul, O., Vert, J.P., 2019. Differentiable ranks and sorting using optimal transport. arXiv

, 1‚Äì10arXiv:1905.11885.

[13] Davies, T., Nowrouzezahrai, D., Jacobson, A., 2020. OverÔ¨Åt neural networks as a compact shape

representation. arXiv arXiv:2009.09808.

[14] Degrave, J., Hermans, M., Dambre, J., Wyffels, F., 2017. A differentiable physics engine for deep

learning in robotics. Frontiers Neurorobotics 13.

[15] Demir, I., Hahn, C., Leonard, K., Morin, G., Rahbani, D., Panotopoulou, A., Fondevilla, A., Balashova,
E., Durix, B., Kortylewski, A., 2019. SkelNetOn 2019: Dataset and challenge on deep learning for
geometric shape understanding. arXiv arXiv:1903.09233.

[16] Deng, B., Genova, K., Yazdani, S., Bouaziz, S., Hinton, G., Tagliasacchi, A., 2020. CvxNet: Learnable
convex decomposition, in: Conference on Computer Vision and Pattern Recognition, pp. 31‚Äì44.

[17] Djolonga, J., Krause, A., 2017. Differentiable learning of submodular models, in: Neural Information

Processing Systems, p. 1014‚Äì1024.

[18] Erwinski, K., Paprocki, M., Wawrzak, A., Grzesiak, L.M., 2016. Neural network contour error predictor
in CNC control systems, in: 2016 21st International Conference on Methods and Models in Automation
and Robotics (MMAR), IEEE. pp. 537‚Äì542.

[19] Fey, M., Lenssen, J.E., Weichert, F., M√ºller, H., 2018. SplineCNN: Fast geometric deep learning with
continuous B-spline kernels, in: Conference on Computer Vision and Pattern Recognition, pp. 869‚Äì877.

19

[20] Groueix, T., Fisher, M., Kim, V.G., Russell, B.C., Aubry, M., 2018. AtlasNet: A Papier-M√¢ch√© approach

to learning 3D surface generation. arXiv arXiv:1802.05384.

[21] Hoschek, J., 1988. Intrinsic parametrization for approximation. Computer Aided Geometric Design 5,

27‚Äì31.

[22] Innes, M., Edelman, A., Fischer, K., Rackauckas, C., Saba, E., Shah, V.B., Tebbutt, W., 2019. A
arXiv

differentiable programming system to bridge machine learning and scientiÔ¨Åc computing.
abs/1907.07587.

[23] Innes, M.J., 2020. Algorithmic differentiation, in: Machine Learning and Systems, pp. 1‚Äì12.

[24] Jakob, W., Rhinelander, J., Moldovan, D., 2017. Pybind11 - Seamless operability between C++ 11 and

Python.

[25] Joshi, A., Cho, M., Shah, V., Pokuri, B., Sarkar, S., Ganapathysubramanian, B., Hegde, C., 2020. In-
vNet: Encoding geometric and statistical invariances in deep generative models, in: Association for
the Advancement of ArtiÔ¨Åcial Intelligence Conference, pp. 1‚Äì8.

[26] Kingma, D.P., Ba, J., 2014. Adam: A method for stochastic optimization. arXiv .

[27] Krishnamurthy, A., Khardekar, R., McMains, S., Haller, K., Elber, G., 2009. Performing efÔ¨Åcient NURBS
modeling operations on the GPU. IEEE Transactions on Visualization and Computer Graphics 15, 530‚Äì
543.

[28] Li, T.M., Aittala, M., Durand, F., Lehtinen, J., 2018a. Differentiable Monte-Carlo ray tracing through

edge sampling. Transactions on Graphics 37, 1‚Äì11.

[29] Li, T.M., Gharbi, M., Adams, A., Durand, F., Ragan-Kelley, J., 2018b. Differentiable programming for

image processing and deep learning in Halide. Transactions on Graphics 37, 1‚Äì13.

[30] Lydia, A., Francis, S., 2019. Adagrad-An optimizer for stochastic gradient descent. International Jour-

nal of Computing and Information Sciences in Engineering 6.

[31] Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A., 2019. Occupancy networks: Learn-
ing 3D reconstruction in function space, in: Conference on Computer Vision and Pattern Recognition,
pp. 4460‚Äì4470.

[32] Minto, L., Zanuttigh, P., Pagnutti, G., 2018. Deep learning for 3D shape classiÔ¨Åcation based on volumet-
ric density and surface approximation clues, in: International Joint Conference on Computer Vision,
Imaging and Computer Graphics Theory and Application, pp. 317‚Äì324.

[33] M√ºller, J.D., Zhang, X., Akbarzadeh, S., Wang, Y., 2019a. Geometric continuity constraints of automat-
ically derived parametrisations in cad-based shape optimisation. International Journal of Computa-
tional Fluid Dynamics 33, 272‚Äì288.

[34] M√ºller, J.D., Zhang, X., Akbarzadeh, S., Wang, Y., 2019b. Geometric continuity constraints of automat-
ically derived parametrisations in CAD-based shape optimisation. International Journal of Computa-
tional Fluid Dynamics 33, 272‚Äì288.

[35] Mykhaskiv, O., Banovi¬¥c, M., Auriemma, S., Mohanamuraly, P., Walther, A., Legrand, H., M√ºller,
J.D., 2018. NURBS-based and parametric-based shape optimization with differentiated CAD kernel.
Computer-Aided Design and Applications 15, 916‚Äì926.

[36] Niemeyer, M., Mescheder, L., Oechsle, M., Geiger, A., 2020. Differentiable volumetric rendering:
Learning implicit 3D representations without 3D supervision, in: Conference on Computer Vision
and Pattern Recognition, pp. 3504‚Äì3515.

[37] NVIDIA, 2011. NVIDIA CUDA C programming guide. NVIDIA Corporation 120, 8.

[38] Park, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove, S., 2019. DeepSDF: Learning continuous
signed distance functions for shape representation, in: Computer Vision and Pattern Recognition, pp.
165‚Äì174.

20

[39] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N.,
Antiga, L., et al., 2019. PyTorch: An imperative style, high-performance deep learning library. arXiv
arXiv:1912.01703.

[40] Peng, S., Niemeyer, M., Mescheder, L., Pollefeys, M., Geiger, A., 2020. Convolutional occupancy net-

works. ArXiv 2.

[41] Piegl, L., Tiller, W., 1997. The NURBS book. Springer-Verlag, Berlin, Heidelberg.

[42] Piegl, L.A., Tiller, W., 1998. Computing the derivative of NURBS with respect to a knot. Computer

Aided Geometric Design 15, 925‚Äì934.

[43] Qi, C.R., Su, H., Mo, K., Guibas, L.J., 2017. Pointnet: Deep learning on point sets for 3D classiÔ¨Åcation

and segmentation. arXiv arXiv:1612.00593.

[44] Rossum, G., 1995. Python reference manual. Technical Report. CWI (Centre for Mathematics and

Computer Science). Amsterdam.

[45] Schafer, F., Kloc, M., Bruder, C., Lorch, N., 2020. A differentiable programming method for quantum

control. ArXiv .

[46] Sharma, G., Liu, D., Maji, S., Kalogerakis, E., Chaudhuri, S., MÀáech, R., 2020. ParSeNet: A parametric

surface Ô¨Åtting network for 3D point clouds. arXiv arXiv:2003.12181.

[47] Sheriffdeen, S., Ragusa, J.C., Morel, J.E., Adams, M.L., Bui-Thanh, T., 2019. Accelerating PDE-
constrained inverse solutions with deep learning and reduced order models. arXiv arXiv:1912.08864.

[48] Ugolotti, M., Vaughan, B., Orkwis, P.D., 2021. Differentiated ml-based modeling of structured grids

for gradient-based optimization, in: AIAA Scitech 2021 Forum, p. 0895.

[49] Vlastelica, M., Paulus, A., Musil, V., Martius, G., Rol√≠nek, M., 2020. Differentiation of blackbox combi-

natorial solvers. arXiv arXiv:1912.02175.

[50] Zhang, X., 2018. CAD-based geometry parametrization for shape optimization using Non-uniform

Rational B-Splines. Ph.D. thesis. Queen Mary University of London.

21

A Parametric Derivatives
Here we provide the parametric surface derivatives with respect to the parameters u and v for complete-
ness. Please refer to Piegl and Tiller 41 for details.

S,u(u, v) =

NR,u(u, v)w(u, v) ‚àí NR(u, v)w,u(u, v)
w(u, v)2

(24)

where,

NR,u(u, v) =

w,u(u, v) =

n
‚àë
i=0

m
‚àë
j=0

n
‚àë
i=0

m
‚àë
j=0

N p
i,u(u)Nq

j (v)wijPij

N p
i,u(u)Nq

j (v)wij

B Test Surface Details
Table 12 shows geometrical details for each surface used for the Ô¨Åtting and surface offset tests. The surface
Ô¨Åtting examples (Analytical and Ducky) are evaluated at 1282 and 5122 points, respectively. The surface
offset test examples are evaluated at 202 points each.

Table 12: Test NURBS surface parameters.

Surface Model

Analytical

Ducky

Double Curve
Multi Patch - C0
Multi Patch - C1
Aerofoil

p

3

3

3

3

3

3

q

3

3

3

3

3

3

n m

12

14

6

4

6

12

13

6

4

6

50

24

C Computing Normals on the Common Edge
For C0 continuous surfaces, the surface normals at the common edge of two surfaces point in different
directions. Hence, if the individual surface normals are used for the offset operation, it might lead to a
gap or self-intersection between the offset surfaces. To deal with this case, we identify the common control
points at the surface edges, and we recompute the resulting normal as the average of both the individual
surface normals as shown in Figure 12. We then use this average normal to move the points on the common
edges to generate the offset points.

Figure 12: Without consolidated normals may lead to discontinuous surfaces, especially for C0 connectivity (shown on the left). Normals consoli-
dation ensures connectivity for the offset surfaces (shown on the right).

D Additional Curve Fitting Results
We tested the curve Ô¨Åtting example of the 3D helix shown in Figure 3 with different optimizers. Figure 13
shows the results for four different optimizers used in this test. We can see that all the optimizers converge

22

Without normalsconsolidationWith normalsconsolidationBase SurfaceOffset surfaceBase surface normalConsolidated normalSurface fromconsolidated normalFigure 13: Results of the curve-Ô¨Åtting on a 3D helix point data with different optimizers.

Figure 14: Curve Ô¨Åtting test performed on various skeleton geometry which results in an invalid curve generation due to self-intersecting geometry.

to the same value for the L2 loss, but the Adagrad optimizer converges to a different value for the L1 and
LCD loss. In addition, SGD with momentum has the fastest convergence rates for all losses.

For some complex shapes, our method cannot generate a curve without self-intersections and loops for the
Pixel dataset. This is because the weightage of the curve length regularization parameter needs to be tuned
for each object based on its complexity. Figure 14 shows the results from 3 curve Ô¨Åtting tests where the
curves generated are self-intersecting. Adding additional constraints to prevent this is a possible future
research direction.

E NURBS-DiÔ¨Ä Implementation Details
Our NURBS-Diff module was implemented using Pytorch library, which allows us to implement cus-
tom deep learning layers that we can use alongside traditional deep learning layers such as convolu-
tion, max-pooling, dense layers, etc. torch.nn.module gives us an interface to create our custom layers;
however, the functions have to be deÔ¨Åned in an automatically differentiable manner. In our application,
we use torch.autograd.Function to deÔ¨Åne our custom forward and backward pass computations for
the NURBS basis functions evaluation and curve/surface evaluation. This function is now used in the
torch.nn.module to create the layer. The actual code for the forward and backward pass (for evaluation)
is written in C++ Language with CUDA integration for GPU acceleration. These modules (written in C++)
are compiled along with PyBind11 package to use in Python (supported by Pytorch). The compilation is
performed using a simple setup from torch.utils.cpp_extension. The main source code of NURBS-Diff
module will be made public.

We create two different layers: (i) B-Spline evaluation (ii) NURBS evaluation. Although the functions used
in both are the same, in B-Spline evaluation, we avoid computing the basis functions every forward pass.
Instead, we precompute the basis functions initially and only perform the forward evaluation of the curve/-
surface. This saves us computational time during the forward and backward pass.

23

