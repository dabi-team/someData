Epistemic Risk-Sensitive Reinforcement Learning

Hannes Eriksson 1 2 Christos Dimitrakakis 1

9
1
0
2

n
u
J

4
1

]

G
L
.
s
c
[

1
v
3
7
2
6
0
.
6
0
9
1
:
v
i
X
r
a

Abstract

We develop a framework for interacting with un-
certain environments in reinforcement learning
(RL) by leveraging preferences in the form of
utility functions. We claim that there is value in
considering different risk measures during learn-
ing. In this framework, the preference for risk can
be tuned by variation of the parameter β and the
resulting behavior can be risk-averse, risk-neutral
or risk-taking depending on the parameter choice.
We evaluate our framework for learning problems
with model uncertainty. We measure and control
for epistemic risk using dynamic programming
(DP) and policy gradient-based algorithms. The
risk-averse behavior is then compared with the
behavior of the optimal risk-neutral policy in en-
vironments with epistemic risk.

1. Introduction

In this work, we consider the problem of reinforcement
learning (RL) for risk-sensitive policies under epistemic un-
certainty, i.e. the uncertainty due to the agent not knowing
how the environment responds to the agent’s actions. This
is in contrast to typical approaches in risk-sensitive decision
making, which have focused on the aleatory uncertainty
due to inherent stochasticity in the environment. Modelling
risk-sensitivity in this way makes more sense for applica-
tions such as autonomous driving, where systems are nearly
deterministic, and most uncertainty is due to the lack of
information about the environment. In this paper, we con-
sider epistemic uncertainty and risk sensitivity both within
a Bayesian utilitarian framework. We develop novel algo-
rithms for policy optimisation in this setting, and compare
their performance quantitatively with policies optimised for
the risk-neutral setting.

Reinforcement learning (RL) is a sequential decision-
making problem under uncertainty. Typically, this is for-

1Department of Computer Science and Engineering, Chalmers
University of Technology, Gothenburg, Sweden 2Zenuity AB,
Gothenburg, Sweden. Correspondence to: Hannes Eriksson <han-
nese@chalmers.se>.

mulated as the maximisation of an expected return, where
the return R is deﬁned as the sum of scalar rewards ob-
tained over time R (cid:44) (cid:80)T
t=1 rt to some potentially inﬁnite
or random horizon T . As is common in RL, we assume that
agents acting in a discrete Markov decision process (MDP).
For any given ﬁnite MDP µ ∈ M, the optimal risk-neutral
Eπ
policy π∗(µ) ∈ arg maxπ
µ[R] be found efﬁciently via
dynamic programming algorithms. Because the learning
problem speciﬁes that µ is unknown, the optimal policy un-
der epistemic uncertainty must take into account expected
information gain. In the Bayesian setting, we maintain a
subjective belief in the form of a probability distribution ξ
over MDPs M and the optimal solution is given by:

max
π

Eπ

ξ [R] = max

π

(cid:90)

M

Eπ

µ[R] dξ(µ).

(1)

This problem is generally intractable, as the optimisation
is performed over adaptive policies, which is exponentially
large in the problem horizon. A risk-neutral agent would
only wish to maximise expected return. However, we are
interested in the case where the agent is risk-sensitive, and
in particular with respect to uncertainty about µ.

Contribution. Typically, risk sensitivity in reinforcement
learning has addressed risk due to the inherent environment
stochasticity (aleatory) and that due to uncertainty (epis-
temic) with different mechanisms. We instead wish to con-
sider both under a coherent utility maximising framework,
where the convexity of the utility with respect to the return
R determines whether our behaviour will be risk-seeking or
risk-averse. By applying this utility function on the actual
return or the expected return given the MDP we can easily
trade off between aleatory and epistemic risk-sensitivity.

1.1. Related work

Deﬁning risk sensitivity with respect to the return R can
be done in a number of ways. In this paper, we focus on
the expected utility formulation, whereby the utility is de-
ﬁned as a function of the return: concave functions lead
to risk aversion and convex to risk seeking. In the con-
text of reinforcement learning, this approach was ﬁrst pro-
posed by Mihatsch & Neuneier (2002), who derived efﬁ-
cient temporal-difference algorithms for exponential utility
functions. However, the authors considered risk only with

 
 
 
 
 
 
respect to the MDP stochasticity.

optimal epistemic risk sensitive policy maximises:

Epistemic Risk-Sensitive Reinforcement Learning

Works on Robust MDPs have traditionally dealt with un-
certainty due to epistemic risk. In Givan et al. (2000) the
authors extend the exact MDP setting by allowing bounded
transition probabilities and rewards. They introduce optimal
pessimistic and optimistic policy updates for this setting.
Mannor et al. (2012) further extend this by allowing the
parameter uncertainties to be coupled which allows for less
conservative solutions. Tamar et al. (2014) use RL and ap-
proximate dynamic programming (ADP) to scale the Robust
MDP framework to problems with larger state spaces using
function approximators.

Another way of dealing with risk is conditional value-
at-risk (CVaR) (Rockafellar et al., 2000; Sivaramakrish-
nan & Stamicar, 2017) measures. Compared to more
traditional risk measures such as Mean-Variance trade-
off (Markowitz, 1952; Tamar et al., 2012) or expected util-
ity framework (Friedman & Savage, 1948), CVaR allows
us to control for tail risk. CVaR has been used for risk-
sensitive MDPs in Chow & Ghavamzadeh (2014); Chow
et al. (2015b;a); Stanko (2018).

A Bayesian setting is also considered by Depeweg et al.
(2018), who focus on a risk decomposition in aleatory and
epistemic components. The authors model the underlying
dynamics with a neural network. Under the risk-sensitive
framework, they aim to trade-off expected performance
with variation in performance. The risk-sensitive criterion
they aim to maximise is E(cid:2) (cid:80)
(cid:1) where
β controls for the level of risk-aversion. Thus, they are
essentially considering risk only with respect to individual
rewards. Our paper instead considers the risk with respect
to the total return, which we believe is a more interesting
setting for long-term planning problems under uncertainty.

(cid:3) + βσ(cid:0) (cid:80)

t rt

t rt

2. Optimal policies for epistemic risk

Under the expected utility hypothesis, risk sensitivity can
be modelled (Friedman & Savage, 1948) through a concave
or convex utility function U : R → R of the return R. Then,
for a given model µ, the optimal U -sensitive policy with
respect to aleatory risk is the solution to maxπ Eπ
µ[U (R)].
In the case where we are uncertain about what is the true
MDP, we can express it through belief ξ over models µ.
Then optimal policy is the solution to

πA(U, ξ) (cid:44) arg max

(cid:90)

π

M

Eπ

µ[U (R)] dξ(µ).

(2)

However, this is only risk-sensitive with respect to the
stochasticity of the underlying MDPs. We believe that epis-
temic risk, i.e.
the risk due to our uncertainty about the
model is more pertinent for reinforcement learning. The

πE(U, ξ) (cid:44) arg max

(cid:90)

π

M

U (cid:0)Eπ

ξ (R)(cid:1) dξ(µ).

(3)

When U is the identity function, both solutions are risk-
neutral. In this paper, we shall consider functions of the
form U (x) = β−1eβx, so that β > 0 is risk-seeking and
β < 0 risk-averse.

We consider two algorithms for this problem. The ﬁrst,
based on an approximate dynamic programming algorithm
for Bayesian reinforcement learning introduced in (Dimi-
trakakis, 2011), is introduced in Section 2.2. The second,
based on the policy gradient framework (Sutton et al., 2000),
allows us to extend the previous algorithm to larger MDPs
and allows for learning of stochastic policies. The priors
used in the experiments are detailed in Section 3.

2.1. Utility functions.

Previous works (Mihatsch & Neuneier, 2002; Howard &
Matheson, 1972) on utility functions for risk-sensitive re-
inforcement learning used an exponential utility function
of the form U (x) = β−1eβx. The β parameter could then
be used to control the shape of the utility function and de-
termine how risk-sensitive we want to be with respect to
x. This is the utility function we will use in this paper.1
Mihatsch & Neuneier (2002) consider not only exponential
utility functions, but a special form of them, that is:

πE(U ) = arg max

π

(cid:16)

log E

1
β

(cid:17)

exp(βR)

(4)

Mihatsch & Neuneier (2002); Coraluppi & Marcus (1999);
Marcus et al. (1997) argue that this utility function has some
interesting properties. In particular, maximising leads to
maximising:

E[R] +

β
2

V ar[R] + O(β2)

(5)

From this, we get that the behavior of our policy πE(U ) as
β → 0 is the same as for the risk-neutral case. For β < 0 we
get risk-averse behavior and for β > 0 risk-taking behavior.

In our work, we are working in a Bayesian setting. Con-
sequently, we introduce a belief ξ over models and deﬁne
the expected utility of a policy π related to the model uncer-
tainty as follows:

β (ξ, π) (cid:44) 1
U E
β

(cid:90)

log

M

exp (cid:0)βEπ

µ[R](cid:1)dξ(µ).

(6)

1Other choices are U (x) = xβ or U (x) = log(x), however
they both come with signiﬁcant drawbacks, such as handling nega-
tive returns R.

Epistemic Risk-Sensitive Reinforcement Learning

Algorithm 1 Epistemic Risk Sensitive Backwards Induction

Input: M (set of MDPs), ξ (current posterior)
repeat

for µ ∈ M s ∈ S, a ∈ A do

Qµ(s, a) = Rµ(s, a) + γ (cid:80)

end for
for s ∈ S do

s(cid:48) T ss(cid:48)

µ Vµ(s(cid:48))

for a ∈ A do

Qξ(s, a) = (cid:80)

µ ξ(µ)U [(Qµ(s, a)]

end for
π(s) = arg maxa Qξ(s, a).
for µ ∈ M do

Vµ(s) = Qµ(s, π(s)).

end for

end for

until convergence
return π

2.2. Epistemic risk sensitive backward induction

Algorithm 1 is an Approximate Dynamic Programming
(ADP) (Powell, 2007) algorithm for optimising policies in
our setting. While the algorithm is given for a belief over a
ﬁnite set of MDPs, it can be easily extended to arbitrary ξ
through simple Monte-Carlo sampling, as in Dimitrakakis
(2011).

The algorithm essentially maintains a separate Qµ-value
function for every MDP µ. At every step, it ﬁnds the best
local policy π with respect to the utility function U . Then
the value function Vµ of each MDP reﬂects the value of π
within that MDP.

This algorithm will be used as a baseline for the experiments
with risk aversion. We know that as the epistemic uncer-
tainty vanishes; if there is only one underlying true model,
then the optimal policy for the epistemic risk-sensitive algo-
rithm will be the same as the optimal policy for the epistemic
risk-neutral algorithm. It is important to point out that this
is only true when our belief is concentrated around the true
MDP. Behavior during learning of the MDP could be very
different. For cases with multiple models, the epistemic un-
certainty will always exist and there are no such guarantees.

2.3. Bayesian policy gradient

A common method for model-free reinforcement learning
is policy gradient (Sutton et al., 2000). It is also very useful
in model-based settings, and speciﬁcally for the Bayesian
reinforcement learning problem, where sampling from the
posterior allows us to construct efﬁcient stochastic gradi-
ent algorithms. Bayesian policy gradient (BPG) methods
have been explored for these contexts before, Ghavamzadeh
& Engel (2006) uses BPG to plan how to maximise infor-
mation gain given our current belief ξt for the risk-neutral

setting. In our speciﬁc setting, we are interested in max-
imising (3), where we use utility function (6) to model risk
sensitivity.

Our choice of policy parametrisation is a softmax pol-
icy with non-linear features. The probability of select-
ing action a in state s, given current parameters θ, is
eφ(s,a,θ)
πθ(a|s) =
a(cid:48) ∈A eφ(s,a(cid:48) ,θ) , where the features φ(s, a, θ)
are calculated by a feedforward neural network with one
hidden layer. Full parameter details of the neural networks
are given in Section 3.

(cid:80)

We choose to introduce a policy parametrisation over (3).
This gives us (7).

πE(U, ξ, θ) =

(cid:90)

M

(cid:16)

U

Eµ
πθ

[R]

(cid:17)

dξ(µ)

(7)

Combining our choice of utility function in (6) and (7) gives
us our new objective function (8).

πE(U, ξ, θ) =

1
β

(cid:90)

log

M

exp

(cid:16)

βEµ
πθ

(cid:17)

[R]

dξ(µ)

(8)

Our goal is now to ﬁnd the set of parameters θ so as to
maximise information gain. Taking the gradient of (8) gives
(9) which leads to (10) after a straightforward derivation.

∇θπE(U, ξ, θ) = ∇θ

1
β

(cid:90)

log

M

exp

(cid:16)

βEµ
πθ

(cid:17)

[R]

dξ(µ)

(cid:82)
M exp
(cid:82)

(cid:16)

βEµ
πθ
(cid:16)

M exp

=

(cid:17)

[R]

∇θEµ
πθ
(cid:17)

βEµ

πθ [R]

dξ(µ)

(9)

(10)

[R]dξ(µ)

The RHS of the numerator is the classical policy gradi-
ent (Sutton et al., 2000). We can replace the integrals in (10)
with a sum by sampling models from our belief ξ. Note
that this has to be done independently for the numerator and
the denominator to avoid bias. To get each of the separate
Eµ
[R] we make use of rollouts. Each expected return term
πθ
is estimated independently with their own set of rollouts.

We now have an estimate for the gradient and can move our
policy parameters accordingly to act optimally given our
belief ξt.

The procedure of calculating the gradient in (10) is given in
Algorithm 2. The algorithm builds upon the classic episodi-
cal REINFORCE algorithm (Williams, 1992). Basing the
algorithm on an episodical update makes sense in this case
since it decreases the number of rollouts we have to do. One

Epistemic Risk-Sensitive Reinforcement Learning

Algorithm 2 Epistemic Risk Sensitive Policy Gradient

following;

Input: Policy parametrisation θt, ξt (current posterior).
repeat

Simulate to get θt+1
for i = 1 to N do

µ(1), µ(2) ∼ (Mt, Rt)
for j = 1 to M do
τ (1)
µ(1), τ (2)
τ (3)
µ(2) ∼ πθ, µ(2)
end for

µ(1) ∼ πθ, µ(1)

end for

θt+1 ← θt −

(cid:80)N

i=0 exp

(cid:16)

(1)

βτµi

(cid:17)

(2)∇θ log πθ(a|s)

τµi
(cid:16)

(cid:17)

(3)

(cid:80)N

i=0 exp

βτµi

Deploy πθt+1
τ ∼ µ, πθt+1
ξt+1 ← ξt, τ
until convergence

drawback is that it restricts updates episode by episode and
so the information gained in the current episode cannot be
used until after the episode ends. Monte-Carlo methods
such as REINFORCE also have a few other drawbacks such
as high variance and low convergence, problems that could
be addressed by extending the current framework to an actor-
critic based one, as in Barto et al. (1983); Ghavamzadeh &
Engel (2007); Ghavamzadeh et al. (2016).

The algorithm consists of two stages. One planning stage
which is used to identify the best policy parameters θt given
our current belief ξt. This is done through model-based
simulation by sampling MDPs from our belief. After the
rollouts have been collected the neural network is optimised
and a new policy πθt+1 is attained. We then commit to this
policy for one episode and move on to the next stage of the
algorithm.

The second stage of the algorithm uses the new pol-
icy to act in the real environment. Trajectories τ =
(s0, a0, r1, s1, ..., sT −1, aT −1, rT , sT ) are collected from
this environment and used to update our belief over transi-
tions and reward functions.

2.4. Bayesian Epistemic CVaR

Another common approach of handling risk-aversion is
to optimize with respect to a CVaR objective, (Chow &
Ghavamzadeh, 2014; Chow et al., 2015b; Tamar et al., 2015;
Stanko, 2018). In this paper, we investigate Bayesian epis-
temic CVaR, ﬁrst studied by Chen et al. (2018) in the context
of investment strategies. In their case, they use it to do pos-
terior inference of a SV-ALD model to efﬁciently estimate
risk. We deﬁne Bayesian epistemic CVaR as follows, deﬁn-
ing the set of MDPs where we get at least z utility as the

Mπ
z

(cid:44)

(cid:110)

µ | Eπ

µ[R] ≥ z

(cid:111)
.

(11)

ξ (α) is value-at-risk in the Bayesian setting for a given

The νπ
quantile α.

νπ
ξ (α) = inf

(cid:110)

z | ξ(Mπ

z ) ≥ α

(cid:111)
.

C π

ξ (α) = Eπ
ξ
(cid:90)

=

(cid:104)

Eπ
µ[R] | Eπ

µ[R] ≤ νπ

(cid:105)
ξ (α)

Eπ

µ[R] dξ(µ).

Mπ
νπ
ξ

(α)

(12)

(13)

(14)

Concisely stated; we want to maximize our performance
for the α least likely MDPs according to our belief ξ. The
intuition behind this is that we want to be risk-averse with
respect to what we are the most uncertain about.

3. Experiments

In this paper, we conduct two kinds of experiments. Firstly,
a classical learning problem on a Gridworld. This is dis-
crete, so we can use Algorithm 1 to ﬁnd a near-optimal
deterministic policy. Similar problems have been studied
before in the ﬁeld of robust MDPs and there are already so-
lutions for ﬁnite state and action-space problems. As this is
a discrete state-action space, the belief maintained over the
MDP transitions is in the form of a Dirichlet-product prior
and Belief over rewards is in the form of a NormalGamma
prior. Further experimental details about this problem can
be found in Section 3.1 and Appendix A.1.

Secondly, we consider a continuous state-space problem in
Section 3.2. We do not see a trivial extension of previous
works in robust MDPs to the case with inﬁnite state-space.
Previous works such as Tamar et al. (2014) scale earlier
works on robust MDP to large state-space problems but not
to continuous. The problem has been studied to a great
extent in the ﬁeld of risk-aversion, see Tamar et al. (2012;
2015); Chow & Ghavamzadeh (2014) and Appendix A.2.
For this problem, we maintain function priors in the form of
Gaussian Processes on the reward functions and transition
kernels.

3.1. Gridworld experiment

Shown in Figure 1 are the results of Algorithm 1 ran for dif-
ferent values of β. We aimed to test both risk-aversion and
risk-taking behavior through the choice of this parameter.
The regret is averaged over ≈ 100 independent experiment

Epistemic Risk-Sensitive Reinforcement Learning

Figure 1. Experiment detailing the results of the run of ADP Al-
gorithm 1 for varying choices of β. (a) The top plot of the grid
is the regret over time with respect to the optimal deterministic
risk-neutral policy. (b) The bottom plot shows the distribution of
falls throughout over all experiments.

Figure 2. Experiment detailing the results of the run of Epistemic
Bayesian Policy gradient EBPG Algorithm 2 for varying choices
of β. For comparison we also include a risk-neutral PG, a risk-
neutral BPG and CVaR BPG. (a) The upper plot is a histogram
over returns for all episodes observed. (b) The bottom plot is a
histogram of the return over the last 10000 episodes, so the most
current policy.

runs. We can see that the regret increases for more risk-
taking policies in the top plot.
In this environment the
risk-neutral and the risk-averse behavior is almost identical,
with only a minor difference in #F alls. The risk-taking
policy is in one sense, over-exploring while the risk-averse
policy in general, would be more inclined to do exploitation.
Note that in this experiment the only epistemic uncertainty
comes from that the agent does not know the true MDP µ.
This uncertainty will go down over time as more transitions
are observed.

3.2. Option pricing experiment

The result of runs in this environment is depicted in Figure 2.
The blue colored line will serve as a reference and has been
veriﬁed to have an almost optimal risk-neutral policy. It also
has the most observed data (millions of episodes) compared
to the other algorithms (hundreds of thousands of episodes).
We do see some notable difference in behavior between
the policies and can identify that the algorithm with CVaR
objective and the algorithm with β = −0.001 are overly
cautious. On the other hand for β = {−0.01, −0.1} and
BPG we see behavior similar to that of regular PG.

4. Conclusions

We have introduced a framework that allows us to control
for how risk-sensitive we want to be with respect to model
uncertainty in the Bayesian RL setting. In Section 3.1 we
detail the performance of Algorithm 1, an ADP algorithm
that has the ability to control for risk-sensitiveness in envi-

ronments with discrete state and action-spaces. The results
point towards risk-averse policies lead to less exploratory
policies; that is, we would only expect to explore addition-
ally if we are convinced there is much more to be gained
by doing so, compared to a risk-neutral policy. Figure 2
shows an extension of this, using Policy gradient algorithms
to more complicated environments with continuous state-
space.

A few hurdles noted is the speed of Algorithm 2. In order
to get a rich representation of the model uncertainty, a lot
of MDPs should be sampled. However, sampling MDPs in
this setup is quite expensive. Other belief models could be
considered instead of GP if we were to scale this up to real
problems.

Future work. We can see an adaptive agent that could
change its risk-sensitive parameter β with time. For prob-
lems where we know uncertainties will resolve later into the
future, this could be one approach.

There could be an avenue to explore using utility functions
to induce more exploratory behavior in complicated envi-
ronments. Current methods use concepts such as curios-
ity (Houthooft et al., 2016; Pathak et al., 2017) for reward
shaping or entropy regularization (Williams & Peng, 1991;
Mnih et al., 2016; O’Donoghue et al., 2016) to enforce ad-
ditional exploration. However, under the expected utility
framework we could perhaps get this for free by changing

Epistemic Risk-Sensitive Reinforcement Learning

the utility function.

Decreasing variance and speeding up Algorithm 2 is of
utmost importance for this framework to be used for
more complex problems. As we touched upon in Sec-
tion 2.3 a straight-forward improvement would be to move
towards the Bayesian actor-critic framework introduced
in Ghavamzadeh & Engel (2007).

Acknowledgement

This work was partially supported by the Wallenberg AI, Au-
tonomous Systems and Software Program (WASP) funded
by the Knut and Alice Wallenberg Foundation.
The simulations were performed on resources provided by
the Swedish National Infrastructure for Computing (SNIC)
at Chalmers Centre for Computational Science and Engi-
neering (C3SE).

References

Barto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike
adaptive elements that can solve difﬁcult learning con-
trol problems. IEEE transactions on systems, man, and
cybernetics, (5):834–846, 1983.

Chen, L., Zerilli, P., and Baum, C. F. Leverage effects and
stochastic volatility in spot oil returns: A bayesian ap-
proach with var and cvar applications. Energy Economics,
2018.

Chow, Y. and Ghavamzadeh, M. Algorithms for cvar op-
timization in mdps. In Advances in neural information
processing systems, pp. 3509–3517, 2014.

Chow, Y., Ghavamzadeh, M., Janson, L., and Pavone, M.
Risk-constrained reinforcement learning with percentile
risk criteria, 2015a.

Chow, Y., Tamar, A., Mannor, S., and Pavone, M. Risk-
sensitive and robust decision-making: a cvar optimization
approach. In Advances in Neural Information Processing
Systems, pp. 1522–1530, 2015b.

Coraluppi, S. P. and Marcus, S. I. Risk-sensitive and mini-
max control of discrete-time, ﬁnite-state markov decision
processes. Automatica, 35(2):301–309, 1999.

Depeweg, S., Hernandez-Lobato, J.-M., Doshi-Velez, F.,
and Udluft, S. Decomposition of uncertainty in bayesian
deep learning for efﬁcient and risk-sensitive learning. In
International Conference on Machine Learning, pp. 1192–
1201, 2018.

Friedman, M. and Savage, L. J. The Utility Analysis of
Choices Involving Risk. The Journal of Political Econ-
omy, 56(4):279, 1948.

Ghavamzadeh, M. and Engel, Y. Bayesian policy gradient

algorithms. In NIPS 2006, 2006.

Ghavamzadeh, M. and Engel, Y. Bayesian actor-critic algo-
rithms. In Proceedings of the 24th international confer-
ence on Machine learning, pp. 297–304. ACM, 2007.

Ghavamzadeh, M., Engel, Y., and Valko, M. Bayesian
policy gradient and actor-critic algorithms. The Journal
of Machine Learning Research, 17(1):2319–2371, 2016.

Givan, R., Leach, S., and Dean, T. Bounded-parameter
markov decision processes. Artiﬁcial Intelligence, 122
(1-2):71–109, 2000.

Houthooft, R., Chen, X., Duan, Y., Schulman, J., De Turck,
F., and Abbeel, P. Vime: Variational information maxi-
mizing exploration. In Advances in Neural Information
Processing Systems, pp. 1109–1117, 2016.

Howard, R. A. and Matheson, J. E. Risk-sensitive markov
decision processes. Management science, 18(7):356–369,
1972.

Leike, J., Martic, M., Krakovna, V., Ortega, P. A., Everitt,
T., Lefrancq, A., Orseau, L., and Legg, S. Ai safety
gridworlds. arXiv preprint arXiv:1711.09883, 2017.

Mannor, S., Mebel, O., and Xu, H. Lightning does not strike
twice: Robust mdps with coupled uncertainty. arXiv
preprint arXiv:1206.4643, 2012.

Marcus, S. I., Fern´andez-Gaucherand, E., Hern´andez-
Hernandez, D., Coraluppi, S., and Fard, P. Risk sensitive
markov decision processes. In Systems and control in the
twenty-ﬁrst century, pp. 263–279. Springer, 1997.

Markowitz, H. Portfolio selection. The journal of ﬁnance, 7

(1):77–91, 1952.

Mihatsch, O. and Neuneier, R. Risk-sensitive reinforcement
learning. Machine learning, 49(2-3):267–290, 2002.

Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-
chronous methods for deep reinforcement learning. In
International conference on machine learning, pp. 1928–
1937, 2016.

Murphy, K. Conjugate bayesian analysis of the gaussian

distribution. Technical report, UBC, 2007.

Dimitrakakis, C. Robust bayesian reinforcement learning
through tight lower bounds. In European Workshop on Re-
inforcement Learning (EWRL 2011), pp. 177–188, 2011.

O’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V.
Combining policy gradient and q-learning. arXiv preprint
arXiv:1611.01626, 2016.

Epistemic Risk-Sensitive Reinforcement Learning

Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T.
Curiosity-driven exploration by self-supervised predic-
tion. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition Workshops, pp. 16–17,
2017.

Powell, W. B. Approximate Dynamic Programming: Solving
the curses of dimensionality, volume 703. John Wiley &
Sons, 2007.

Rockafellar, R. T., Uryasev, S., et al. Optimization of condi-

tional value-at-risk. Journal of risk, 2:21–42, 2000.

Sivaramakrishnan, K. and Stamicar, R. A cvar scenario-
based framework: Minimizing downside risk of multi-
asset class portfolios. The Journal of Portfolio Manage-
ment, 44:114–129, 12 2017. doi: 10.3905/jpm.2018.44.2.
114.

Stanko, S. Risk-averse distributional reinforcement learning,

2018.

Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour,
Y. Policy gradient methods for reinforcement learning
with function approximation. In Advances in neural in-
formation processing systems, pp. 1057–1063, 2000.

Tamar, A., Di Castro, D., and Mannor, S. Policy gradients
In Proceedings of
with variance related risk criteria.
the twenty-ninth international conference on machine
learning, pp. 387–396, 2012.

Tamar, A., Mannor, S., and Xu, H. Scaling up robust mdps
using function approximation. In International Confer-
ence on Machine Learning, pp. 181–189, 2014.

Tamar, A., Glassner, Y., and Mannor, S. Optimizing the
cvar via sampling. In Twenty-Ninth AAAI Conference on
Artiﬁcial Intelligence, 2015.

Williams, R. J. Simple statistical gradient-following algo-
rithms for connectionist reinforcement learning. Machine
learning, 8(3-4):229–256, 1992.

Williams, R. J. and Peng, J. Function optimization using
connectionist reinforcement learning algorithms. Con-
nection Science, 3(3):241–268, 1991.

A. Environment descriptions

A.1. Gridworld

A visual representation of the environment can be seen
in Leike et al. (2017). The experiments are run with
slightly different parameters, which are the following;
Let γ = 0.99 be the discount factor, and T = 100 be
the maximum allowed steps within an episode. The al-
lowed actions correspond to the four cardinal directions

{N orth, East, South, W est}. The environment consists
of a GOAL tile, which terminates the episode and gives a
reward of rg = 0.0. The environment also has WATER tiles
which, upon entering, terminates the episode and gives a
reward of rw = −10.0. Every other action in any other
state gives a reward of r0 = −0.1. To ensure there is always
epistemic uncertainty, we introduce another almost identical
environment but with a goal at another location.

We measure performance in Regret with respect to the op-
timal deterministic risk-neutral policy. The regret of an
algorithm A with respect to the optimal deterministic risk-
neutral policy is given by the following;

Regret(A, π∗) =

(cid:90)

M

(cid:16)

V π∗
µ −

T
(cid:88)

t=0

(cid:17)

dµ.

γtrt

(15)

We let our prior belief over the MDP transitions be
µ (s(cid:48)|s, a) = Dir(α0), α0 = 0.5. We can then, given ob-
T 0
µ (s(cid:48)|s, a) =
served transitions in the environment, update T T

α0+ns(cid:48)
(cid:80)
s(cid:48)(cid:48) ns(cid:48)(cid:48)

s,a

s,a

), that is, we count the number of transitions
Dir(
(s, a) → s(cid:48) and (s, a) → ·. From these counts we can com-
pute αt which is our posterior on the transition (s, a) → s(cid:48)
at time t.

We also hold a belief over the reward matrices Rµ(s, a). A
common prior is to use is a N ormalGamma-prior (Mur-
phy, 2007) over each rµ(s, a). The NormalGamma distri-
bution is the conjugate prior to the N ormal distribution
with unknown parameters µ, λ. The model is given as
p(µ, λ|D) = N G(µ, λ|µn, κn, αn, βn). So our belief over
rewards Rµ(s, a) = N G(µ, λ|µn, κn, αn, βn).

A.2. Options

A common experiment (Chow & Ghavamzadeh, 2014;
Tamar et al., 2012), used to test risk-averse algorithms in a
continuous environment is the Option Pricing experiment.
It can have many forms and classically we aim to minimise
expected cost. In this work, we consider the reward per-
spective instead and the goal is to maximise the expected
return. The environment is as follows; At any time t the
agent represents an investor with the opportunity of buying
an option which gives an immediate reward of rt. The re-
ward of this option varies with time and with probability p,
rt+1 ← γfurt, and with probability (1 − p), rt+1 ← γfdrt.
The goal is then for the agent to chose a suitable time to
stop. Upon choosing to buy the option, the episode ends and
the agent returns to x0. Every time step the agent decides
to wait will also incur a small negative reward ph which
represents the opportunity cost for not using its resources.
If the agent chooses to wait until the horizon T , it is then
forced to take the option for its current value.

Epistemic Risk-Sensitive Reinforcement Learning

We use the following parameters in our experiment; x0 =
[1; 0], ph = −0.1, T = 20, γ = 0.95, fu = 2.0, fd =
0.5, p = [0.45; 0.65; 0.85], K = 5.0.

Algorithm 2 requires us to be able to sample MDPs from
the environment and to be able to do rollouts in them. There
are no obvious candidates of priors over continuous state-
space transitions and reward functions. In this work, we
chose to use Gaussian Processes as priors over functions.
We use multiple Gaussian Processes to maintain our belief
over transition kernels and reward functions. We update our
belief with transitions and reward from the true underlying
models. We use a Dirichlet prior over models and update it
when we receive information on which model we acted in.

Given data from rollouts sampled from our belief, we can
use any standard Policy gradient-like algorithm to update
our policy πθt.
In this paper, we chose to focus on an
episodic Policy gradient algorithm similar to the REIN-
FORCE framework. We parametrise our policy by a one
hidden layer neural network with parameters θ.

