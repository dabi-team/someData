1
2
0
2

p
e
S
9
2

]

G
L
.
s
c
[

2
v
8
2
2
0
1
.
6
0
9
1
:
v
i
X
r
a

arXiv: arXiv:1906.10228

A Theoretical Connection Between
Statistical Physics and Reinforcement Learning

Jad Rahme and Ryan P. Adams

Princeton University
{jrahme,rpa}@princeton.edu

Abstract: Sequential decision making in the presence of uncertainty and stochastic dynamics gives
rise to distributions over state/action trajectories in reinforcement learning (RL) and optimal control
problems. This observation has led to a variety of connections between RL and inference in probabilistic
graphical models (PGMs). Here we explore a diï¬€erent dimension to this relationship, examining
reinforcement learning using the tools and abstractions of statistical physics. The central object in the
statistical physics abstraction is the idea of a partition function Z, and here we construct a partition
function from the ensemble of possible trajectories that an agent might take in a Markov decision
process. Although value functions and ğ‘„-functions can be derived from this partition function and
interpreted via average energies, the Z-function provides an object with its own Bellman equation
that can form the basis of alternative dynamic programming approaches. Moreover, when the MDP
dynamics are deterministic, the Bellman equation for Z is linear, allowing direct solutions that are
unavailable for the nonlinear equations associated with traditional value functions. The policies learned
via these Z-based Bellman updates are tightly linked to Boltzmann-like policy parameterizations. In
addition to sampling actions proportionally to the exponential of the expected cumulative reward as
Boltzmann policies would, these policies take entropy into account favoring states from which many
outcomes are possible.

1. Introduction

One of the central challenges in the pursuit of machine intelligence is robust sequential decision making. In a
stochastic and uncertain environment, an agent must capture information about the distribution over ways
they may act and move through the state space. Indeed, the algorithmic process of planning and learning itself
can lead to a well-deï¬ned distribution over state/action trajectories. This observation has led to a variety
of connections between reinforcement learning (RL) and inference in probabilistic graphical models (PGMs)
(Levine, 2018). In some ways this connection is unsurprising: belief propagation (and its relatives such as
the sum-product algorithm) is understood to be an example of dynamic programming (Koller and Friedman,
2009) and dynamic programming was developed to solve control problems (Bellman, 1966; Bertsekas, 1995).
Nevertheless, the exploration of the connection between control and inference has yielded fruitful insights
into sequential decision making algorithms (Kalman, 1960; Attias, 2003; Ziebart, 2010; Kappen, 2011; Levine,
2018).

In this work, we present another point of view on reinforcement learning as a distribution over trajectories,
one in which we draw upon useful abstractions from statistical physics. This view is in some ways a
natural continuation of the agenda of connecting control to inference, as many insights in probabilistic
graphical models have deep connections to, e.g., spin glass systems (Hopï¬eld, 1982; Yedidia et al., 2001;
ZdeborovÂ´a and Krzakala, 2016). More generally, physics has often been a source of inspiration for ideas in
machine learning (MacKay, 2003; Mezard and Montanari, 2009). Boltzmann machines (Ackley et al., 1985),
Hamiltonian Monte Carlo (Duane et al., 1987; Neal et al., 2011; Betancourt, 2017) and, more recently, tensor
networks (Stoudenmire and Schwab, 2016) are a few examples. In addition to direct inspiration, physics
provides a compelling framework to reason about certain problems. The terms momentum, energy, entropy,
and phase transition are ubiquitous in machine learning. However, abstractions from physics have generally
not been so far helpful for understanding reinforcement learning models and algorithms. That is not to say
there is a lack of interaction; RL is being used in some experimental physics domains, but physics has not
yet as directly informed RL as it has, e.g., graphical models (Carleo et al., 2019).

Nevertheless, we should expect deep connections between reinforcement learning and physics: an RL
agent is trying to ï¬nd a policy that maximizes expected reward and many natural phenomena can be

1

 
 
 
 
 
 
viewed through a minimization principle. For example, in classical mechanics or electrodynamics, a mass
or light will follow a path that minimizes a physical quantity called the action, a property known as the
principle of least action. Similarly, in thermodynamics, a system with many degrees a freedomâ€”such as
a gasâ€”will explore its conï¬guration space in the search for a conï¬guration that minimizes its free energy.
In reinforcement learning, rewards and value functions have a very similar ï¬‚avor to energies, as they are
extensive quantities and the agent is trying to ï¬nd a path that maximizes them. In RL, however, value
functions are often treated as the central object of study. This stands in contrast to statistical physics
formulations of such problems in which the partition function is the primary abstraction, from which all the
relevant thermodynamic quantitiesâ€”average energy, entropy, heat capacityâ€”can be derived. It is natural to
ask, then, is there a theoretical framework for reinforcement learning that is centered on a partition function,
in which value functions can be interpreted via average energies?

In this work, we show how to construct a partition function for a reinforcement learning problem. In
a deterministic environment (Section 2), the construction is elementary and very natural. We explicitly
identify the link between the underlying average energies associated with these partition functions and
value functions of Boltzmann-like stochastic policies. As in the inference-based view on RL, moving from
deterministic to stochastic environments introduces complications. In Section 3.2, we propose a construction
for stochastic environments that results in realistic policies. Finally, in Section 4, we show how the partition
function approach leads to an alternative model-free reinforcement learning algorithm that does not explicitly
represent value functions.

We model the agentâ€™s sequential decision-making task as a Markov decision process (MDP), as is typical.
The agent selects actions in order to maximize its cumulative expected reward until a ï¬nal state is reached.
The MDP is deï¬ned by the objects (S, A, R, P). S and A are the sets of states and actions, respectively.
P(ğ‘ , ğ‘, ğ‘ â€²) = P(ğ‘ â€² | ğ‘ , ğ‘) is the probability of landing in state ğ‘ â€² after taking action ğ‘ from state ğ‘ . R(ğ‘ , ğ‘, ğ‘ â€²)
is the reward resulting from this transition. We also make the following additional assumptions: 1) S is
ï¬nite, 2) all rewards R(ğ‘ , ğ‘, ğ‘ â€²) are bounded from above by Rmax and deterministic, and 3) the number of
available actions is uniformly bounded over all states by ğ‘‘. We also allow for terminal states to have rewards
even though there are no further actions and transitions. We denote these ï¬nal-state rewards by R(ğ‘  ğ‘“ ). By
shifting all rewards by Rmax we can assume without loss of generality that Rmax = 0 making all transition
rewards R(ğ‘ , ğ‘, ğ‘ â€²) non positive. The ï¬nal state rewards R(ğ‘  ğ‘“ ) are still allowed to be positive however.

2. Partition Functions for Deterministic MDPs

Our starting point is to consider deterministic Markov decision processes. Deterministic MDPs are those in
which the transition probability distributions assign all their mass to one state. Deterministic MDPs are a
widely studied special case (Madani, 2002; Wen and Van Roy, 2013; Dekel and Hazan, 2013) and they are
realistic for many practical control problems, such as robotic manipulation and locomotion, drone maneuver
or machine-controlled scientiï¬c experimentation. For the deterministic setting, we will use ğ‘  + ğ‘ to denote
the state that follows the taking of action ğ‘ in state ğ‘ . Similarly, we will denote the reward more concisely
as R(ğ‘ , ğ‘).

2.1. Construction of State-Dependent Partition Functions

To construct a partition function, two ingredients are needed: a statistical ensemble, and an energy function ğ¸
on that ensemble. We will construct our ensembles from trajectories through the MDP; a trajectory ğœ” is a
sequence of tuples ğœ” = (ğ‘ 0, ğ‘0, ğ‘Ÿ0), (ğ‘ 1, ğ‘1, ğ‘Ÿ1), . . . , (ğ‘ ğ‘‡ , ğ‘ğ‘‡ , ğ‘Ÿğ‘‡ ) such that state ğ‘ ğ‘‡ +1 is a terminal state. We use
the notation ğ‘ ğ‘¡ (ğœ”), ğ‘ğ‘¡ (ğœ”), and ğ‘Ÿğ‘¡ (ğœ”) to indicate the state, action, and reward, respectively, of trajectory ğœ”
at step ğ‘¡. Each state-dependent ensemble Î©(ğ‘ ) is then the set of all trajectories that start at ğ‘ , i.e., for
which ğ‘ 0 (ğœ”) = ğ‘ . We will use these ensembles to construct a partition function for each state ğ‘  âˆˆ S. Taking |ğœ”|
to be the length of the trajectory, we write the energy function as

| ğœ” |âˆ’1

ğ¸ (ğœ”) = âˆ’

ğ‘Ÿğ‘¡ (ğœ”) âˆ’ ğ‘…(ğ‘  | ğœ” |) = âˆ’

ğ‘¡=0
Ã•

2

ğ‘Ÿğ‘¡ (ğœ”) .

| ğœ” |

ğ‘¡=0
Ã•

(1)

The form on the right takes a notational shortcut of deï¬ning ğ‘Ÿ | ğœ” | (ğœ”) := ğ‘…(ğ‘ ğ‘‡ +1) for the reward of the terminal
state. Since the agent is trying to maximize their cumulative reward, ğ¸ (ğœ”) is a reasonable measure of the
agentâ€™s preference for a trajectory in the sense that lower energy solutions accumulate higher rewards. Note
in particular that the ground state conï¬gurations are the most rewarding trajectories for the agent. With
the ingredients Î©(ğ‘ ) and ğ¸ (ğœ”) deï¬ned, we get the following partition function

Z(ğ‘ , ğ›½) =

ğ‘’âˆ’ğ›½ ğ¸ ( ğœ”) =

ğ‘’ğ›½

| ğœ”|

ğ‘¡=0 ğ‘Ÿğ‘¡ ( ğœ”) .

Ã•ğœ” âˆˆÎ©(ğ‘ )

Ã•ğœ” âˆˆÎ©(ğ‘ )

Ã

(2)

In this expression, ğ›½ â‰¥ 0 is a hyper-parameter that can be interpreted as the inverse of a temperature.
(This interpretation comes from statistical physics where ğ›½ = 1
ğ¾ğµğ‘‡ , where ğ¾ğµ is the Boltzmann constant.)
This partition function does not distinguish between two trajectories having identical cumulative rewards
but diï¬€erent lengths. However, among equivalently rewarding trajectories, it seems natural to prefer shorter
trajectories. One way to encode this preference is to add an explicit penalty ğœ‡ â‰¤ 0 on the length |ğœ”| of a
trajectory, leading to a partition function

Z(ğ‘ , ğ›½) =

ğ‘’âˆ’ğ›½ ğ¸ ( ğœ”)+ğœ‡ | ğœ” | .

Ã•ğœ” âˆˆÎ©(ğ‘ )

(3)

In statistical physics, ğœ‡ is called a chemical potential and it measures the tendency of a system (such as
a gas) to accept new particles. It is sometimes inconvenient to reason about systems with a ï¬xed number
of particles, adding a chemical potential oï¬€ers a way to relax that constraint, allowing a system to have a
varying number of particles while keeping the average ï¬xed.

Note that since MDPs can allow for both inï¬nitely long trajectories and inï¬nite sets of ï¬nite
trajectories, Î©(ğ‘ ) can be inï¬nite even in relatively simple settings. In Appendix A.1, we ï¬nd that a suï¬ƒcient
condition for Z(ğ‘ , ğ›½) to be well deï¬ned is taking ğœ‡ < âˆ’ log ğ‘‘. As written, the partition function in Eq. 3 is
ambiguous for ï¬nal states. For clarity we deï¬ne Z(ğ‘  ğ‘“ , ğ›½) := ğ‘’ğ›½ ğ‘… (ğ‘  ğ‘“ ) for a terminal state ğ‘  ğ‘“ . We will refer to
these as the boundary conditions.

Mathematically, the parameter ğœ‡ has a similar role as the one played by ğ›¾, the discount rate commonly
used in reinforcement learning problems. They both make inï¬nite series convergent in an inï¬nite horizon
setting, and ensure that the Bellman operators are contractions in their respective frameworks (A.3 ,B.3).
However, when using ğ›¾, the order in which the rewards are observed can have an impact on the learned
policy which does not happen when ğœ‡ is used. This could be a desirable property for some problems as it
uncouples rewards from preferences for shorter paths.

2.2. A Bellman Equation for Z

As we have deï¬ned an ensemble Î©(ğ‘ ) for each state ğ‘  âˆˆ S, there is a partition function Z(ğ‘ , ğ›½) deï¬ned for
each state. These partition functions are all related through a Bellman-like recursion:

Z(ğ‘ , ğ›½) =

ğ‘’ğ›½ R(ğ‘ ,ğ‘)+ğœ‡ Z(ğ‘  + ğ‘, ğ›½) ,

(4)

where, as before, ğ‘  + ğ‘ indicates the state deterministically following from taking action ğ‘ in state ğ‘ . This
Bellman equation can be easily derived by decomposing each trajectory ğœ” âˆˆ Î©(ğ‘ ) into two parts: the ï¬rst
transition resulting from taking initial action ğ‘ and the remainder of the trajectory ğœ”â€² which is a member
of Î©(ğ‘  + ğ‘). The total energy and length can also be decomposed in the same way, so that:

ğ‘
Ã•

Z(ğ‘ , ğ›½) =

ğ‘’âˆ’ğ›½ ğ¸ ( ğœ”)+ğœ‡ | ğœ” | =

Ã•ğœ” âˆˆÎ©(ğ‘ )

ğ‘’ğ›½ R(ğ‘ ,ğ‘)+ğœ‡

Ã•ğœ” âˆˆÎ©(ğ‘ )
ğ‘’ğ›½

| ğœ”|
ğ‘¡=0 ğ‘Ÿğ‘¡ ( ğœ”)+ğœ‡ | ğœ” |

ğ‘’ğ›½

Ã

=

=

Ã•ğ‘ âˆˆA

ğ‘’ğ›½ R(ğ‘ ,ğ‘)+ğœ‡ Z(ğ‘  + ğ‘, ğ›½) .

| ğœ”|
ğ‘¡=1 ğ‘Ÿğ‘¡ ( ğœ”)+ğœ‡ ( | ğœ” |âˆ’1) =

ğ‘’ğ›½ R(ğ‘ ,ğ‘)+ğœ‡

ğ‘’âˆ’ğ›½ ğ¸ ( ğœ”â€²)+ğœ‡ | ğœ”â€² |

Ã•ğœ”â€² âˆˆÎ©(ğ‘ +ğ‘)

Ã

Ã•ğ‘ âˆˆA

Ã•ğœ”â€² âˆˆÎ©(ğ‘ +ğ‘)

Note in particular that this Bellman recursion is linear in Z.

ğ‘
Ã•

3

2.3. The Underlying Value Function and Policy

The partition function can be used to compute an average energy to shed light on the behavior of the system.
This average is computed under the Boltzmann (Gibbs) distribution induced by the energy on the ensemble
of trajectories :

P(ğœ” | ğ›½, ğœ‡, ğ‘ 0(ğœ”) = ğ‘ ) =

1Î©(ğ‘ ) (ğœ”)
Z(ğ‘ , ğ›½)

ğ‘’âˆ’ğ›½ ğ¸ ( ğœ”)+ğœ‡ | ğœ” | .

(5)

In probabilistic machine learning, this is usually how one sees the partition function: as the normalizer for an
energy-based learning model or an undirected graphical model (see, e.g., Murray and Ghahramani (2004)).
Under this probability distribution, high-reward trajectories are the most likely but sub-optimal ones could
still be sampled. This approach is closely related to the soft-optimality approach to RL (Levine, 2018).
This distribution over trajectories allows us to compute an average energy for state ğ‘  either as an explicit
expectation or as the partial derivative of the log partition function with respect to the inverse temperature:

hğ¸i =

Ã•ğœ” âˆˆÎ©(ğ‘ )

1
Z(ğ‘ , ğ›½)

ğ‘’âˆ’ğ›½ ğ¸ ( ğœ”)+ğœ‡ | ğœ” | ğ¸ (ğœ”) = âˆ’

ğœ•
ğœ• ğ›½

log Z(ğ‘ , ğ›½) .

(6)

The negative of the average energy is the value function: ğ‘‰ (ğ‘ , ğ›½) := âˆ’hğ¸i = ğœ•
ğœ•ğ›½ log Z(ğ‘ , ğ›½). This is an
intuitive result: recall that the energy ğ¸ (ğœ”) is low when the trajectory ğœ” accumulates greater rewards,
so lower average energy indicates that the expected cumulative rewardâ€”the valueâ€”is greater. Since the
partition functions {Z(ğ‘ , ğ›½)}ğ‘  âˆˆğ‘† are connected by a Bellman equation, we expect that the underlying value
functions {ğ‘‰ (ğ‘ , ğ›½)}ğ‘  âˆˆğ‘† would be connected in a similar way, and there is indeed a non-linear Bellman recursion:

ğ‘‰ (ğ‘ , ğ›½) = ğœ•
ğœ• ğ›½

log Z(ğ‘ , ğ›½) =

1
Z(ğ‘ , ğ›½)
ğ‘’ğ›½ R(ğ‘ ,ğ‘)+ğœ‡ ğœ•
ğœ• ğ›½

ğœ•
ğœ• ğ›½

Z(ğ‘ , ğ›½) =

1
Z(ğ‘ , ğ›½)

ğœ•
ğœ• ğ›½

Ã•ğ‘ âˆˆA

ğ‘’ğ›½ R(ğ‘ ,ğ‘)+ğœ‡ Z(ğ‘  + ğ‘, ğ›½)

Z(ğ‘  + ğ‘, ğ›½) + R(ğ‘ , ğ‘)ğ‘’ğ›½ R(ğ‘ ,ğ‘)+ğœ‡ Z(ğ‘  + ğ‘, ğ›½) .

=

1
Z(ğ‘ , ğ›½)

Ã•ğ‘ âˆˆA

The derivative rule for natural log gives us ğœ•
ğœ•ğ›½

Z(ğ‘ , ğ›½) = Z(ğ‘ , ğ›½) ğœ•

ğœ•ğ›½ log Z(ğ‘ , ğ›½) = Z(ğ‘ , ğ›½)ğ‘‰ (ğ‘ , ğ›½), so:

ğ‘‰ (ğ‘ , ğ›½) =

1
Z(ğ‘ , ğ›½)

=

1
Z(ğ‘ , ğ›½)

Ã•ğ‘ âˆˆA

Ã•ğ‘ âˆˆA

ğ‘’ğ›½ R(ğ‘ ,ğ‘)+ğœ‡Z(ğ‘  + ğ‘, ğ›½)ğ‘‰ (ğ‘  + ğ‘, ğ›½) + R(ğ‘ , ğ‘)ğ‘’ğ›½ R(ğ‘ ,ğ‘)+ğœ‡ Z(ğ‘  + ğ‘, ğ›½)

ğ‘’ğ›½ R(ğ‘ ,ğ‘)+ğœ‡Z(ğ‘  + ğ‘, ğ›½) [ğ‘‰ (ğ‘  + ğ‘, ğ›½) + R(ğ‘ , ğ‘)] .

(7)

Note that the quantities ğ‘’ğ›½ R(ğ‘ ,ğ‘)+ğœ‡Z(ğ‘  + ğ‘, ğ›½) inside the summation of Eq. 7 are positive and sum to Z(ğ‘ , ğ›½)
due to the Bellman recursion for Z(ğ‘ , ğ›½) from Eq. 4. Thus we can view this Bellman equation for ğ‘‰ (ğ‘ , ğ›½) as
an expectation under a distribution on actions, i.e., a policy:

ğ‘‰ (ğ‘ , ğ›½) =

ğœ‹(ğ‘ | ğ‘ ) [ğ‘‰ (ğ‘  + ğ‘, ğ›½) + R(ğ‘ , ğ‘)]

ğœ‹(ğ‘ | ğ‘ ) =

Ã•ğ‘ âˆˆA

Z(ğ‘  + ğ‘, ğ›½)
Z(ğ‘ , ğ›½)

ğ‘’ğ›½ R(ğ‘ ,ğ‘)+ğœ‡ .

(8)

ğœ‹ğµ

selects

actions

proportionally

The policy ğœ‹ resembles a Boltzmann policy but
is not. A Boltzmann
cumulative
the
policy
reward: ğœ‹B(ğ‘ | ğ‘ ) âˆ exp ( ğ›½ [R(ğ‘ , ğ‘) + ğ‘‰ (ğ‘  + ğ‘)]). In particular, ğœ‹ğµ does not take entropy into account:
if two actions have the same expected optimal value, they will be picked with equal probability regardless
of the possibility that one of them could achieve this optimality in a larger number of ways. In the partition
function view, ğœ‹ does take entropy into account and to clarify this diï¬€erence we will look at the two extreme
cases ğ›½ â†’ {0, âˆ}.

strictly speaking it
exponential

expected

their

to

of

4

When ğ›½ â†’ 0, where the temperature of the system is inï¬nite, rewards become irrelevant and we ï¬nd
ğœ” âˆˆÎ©(ğ‘ +ğ‘) ğ‘’ ğœ‡ | ğœ” | . This means that ğœ‹ is picking action ğ‘ proportionally to the number of
that: ğœ‹(ğ‘ | ğ‘ ) âˆ
trajectories that begin with ğ‘  + ğ‘. Here the counting of trajectories happens in a weighted way: longer
trajectories contribute less than shorter ones. This is diï¬€erent from a Boltzmann policy that would pick
actions uniformly at random.

Ã

When ğ›½ â†’ âˆ, the low-temperature limit, we ï¬nd in Section A.2
ğœ‹(ğ‘ | ğ‘ ) âˆ ğ‘max(ğ‘  + ğ‘) exp ( ğ›½ [R(ğ‘ , ğ‘) + ğ‘‰ (ğ‘  + ğ‘)])
that
where
is a weighted count of the number of optimal
ğ‘max(ğ‘  + ğ‘)
trajectories that begin at the state ğ‘  + ğ‘. Boltzmann policies
completely ignore the ğ‘max entropic factor.

ğ‘†0

ğ‘†1

ğ‘†2

ğ‘†3

To illustrate this diï¬€erence more clearly, we consider the
deterministic decision tree MDP shown in Figure 1 where ğ‘†0
is the initial state and the leafs ğ‘†4, ğ‘†5, ğ‘†6, and ğ‘†7 are the
ï¬nal states. The arrows represent the actions available at each state. There are no rewards and
the boundary conditions are: R(ğ‘†4) = R(ğ‘†5) = R(ğ‘†6) = 1 and R(ğ‘†7) = 0. This gives us the boundary
condition: Z(ğ‘†4, ğ›½) = Z(ğ‘†5, ğ›½) = Z(ğ‘†6, ğ›½) = ğ‘’ğ›½ and Z(ğ‘†7, ğ›½) = 1. Computing the Z-functions at
the
intermediate states ğ‘†1, ğ‘†2 and ğ‘†3 we ï¬nd: Z(ğ‘†1, ğ›½) = 2ğ‘’ğ›½+ğœ‡, Z(ğ‘†2, ğ›½) = ğ‘’ğ›½+ğœ‡ and Z(ğ‘†3, ğ›½) = ğ‘’ ğœ‡. Finally we
have Z(ğ‘†0, ğ›½) = 3ğ‘’ğ›½+2ğœ‡ + ğ‘’2ğœ‡. The underlying policy for picking the ï¬rst action is given by:

Fig 1: Decision Tree MDP

ğ‘†4

ğ‘†5

ğ‘†6

ğ‘†7

ğœ‹ğ›½ (1 | 0) =

2ğ‘’ğ›½+2ğœ‡
3ğ‘’ğ›½+2ğœ‡ + ğ‘’2ğœ‡
When ğ›½ â†’ 0, we get: ğœ‹0(1 | 0) = 1
2 , ğœ‹0(2 | 0) = 1
three actions with equal probability. The policy ğœ‹ is biased towards the heavier subtree.
3 , ğœ‹âˆ(2 | 0) = 1
When ğ›½ â†’ âˆ we get: ğœ‹âˆ(1 | 0) = 2
and 2 with a probability of 1

ğ‘’ğ›½+2ğœ‡
3ğ‘’ğ›½+2ğœ‡ + ğ‘’2ğœ‡
4 , ğœ‹0(3 | 0) = 1

3 , ğœ‹âˆ(3 | 0) = 0. A Boltzmann policy would pick action 1
2 . ğœ‹ prefers states from which many possible optimal trajectories are possible.

ğ‘’2ğœ‡
3ğ‘’ğ›½+2ğœ‡ + ğ‘’2ğœ‡
4 . A Boltzmann policy would pick these

ğœ‹ğ›½ (2 | 0) =

ğœ‹ğ›½ (3 | 0) =

(9)

2.4. A Planning Algorithm

When the dynamics of the environment are known, it is possible to to learn Z(ğ‘ , ğ›½) by exploiting the Bellman
equation (4). We denote by ğ‘  â†’ ğ‘ â€² the property that there exists an action ğ‘ that takes an agent from state ğ‘ 
to state ğ‘ â€². The reward associated with this transition will be denoted R(ğ‘  â†’ ğ‘ â€²). Let Z( ğ›½) = [Z(ğ‘ , ğ›½)] ğ‘  âˆˆS be
the vector of all partition functions and ğ¶ ( ğ›½) âˆˆ R|S |Ã— |S | be the matrix:

ğ¶ ( ğ›½)ğ‘ ,ğ‘ â€² = 1ğ‘ â†’ğ‘ â€²ğ‘’ğ›½R(ğ‘ â†’ğ‘ â€²)+ğœ‡ + 1ğ‘ =ğ‘ â€²=ï¬nal state

(10)

ğ¶ ( ğ›½) is a matrix representation of the Bellman operator in Eq. 4. With these notations, the Bellman equations
in (4) can be compactly written as: Z( ğ›½) = ğ¶ ( ğ›½) Z( ğ›½) highlighting the fact that Z( ğ›½) is a ï¬xed point of the
map: ğœ™ : ğ‘‹ â†’ ğ¶ ( ğ›½) ğ‘‹. In Appendix A.3, we show that ğœ™ is a contraction which makes it possible to learn Z( ğ›½)
by starting with an initial vector Z0 having compatible boundary conditions and successively iterating the
map ğœ™: Zğ‘›+1 = ğ¶ ( ğ›½)Zğ‘›. We could also interpret Z( ğ›½) as an eigenvector of ğ¶ ( ğ›½). In this context, this algorithm
is simply doing a power method.

Interestingly, we can learn Z( ğ›½) by solving the underdetermined linear system [ğ¼ |S | âˆ’ ğ¶ ( ğ›½)] Z( ğ›½) = 0 |S |
with the right boundary conditions. We show in Appendix A.2 that the policies learned are related to
Boltzmann policies which produce non linear Bellman equations at the value function level:

ğ‘‰ (ğ‘ , ğ›½) =

ğ‘
Ã•

ğ‘’ğ›½ (R(ğ‘ ,ğ‘)+ğ›¾ğ‘‰ (ğ‘ +ğ‘,ğ›½))
W(ğ‘ , ğ›½)

[ğ‘Ÿ (ğ‘ ,ğ‘) + ğ›¾ğ‘‰ (ğ‘  + ğ‘, ğ›½)]

(11)

ğ‘ ğ‘’ğ›½ (R(ğ‘ ,ğ‘)+ğ›¾ğ‘‰ (ğ‘ +ğ‘,ğ›½)) is a normalization constant diï¬€erent
where ğ›¾ is the discount factor and W(ğ‘ , ğ›½) =
from Z(ğ‘ , ğ›½). By working with partition functions we transformed a non linear problem into a linear one.
This remarkable result is reminiscent of linearly solvable MDPs (Todorov, 2007).

Ã

Once Z is learned the agentâ€™s policy is given by: P(ğ‘ | ğ‘ ) âˆ ğ‘’ğ›½R(ğ‘ ,ğ‘) Z(ğ‘  + ğ‘, ğ›½).

5

3. Partition functions for Stochastic MDPs

We now move to the more general MDP setting. The dynamics of the environment can now be stochastic.
However, as mentioned at the end of the introduction, we still assume that given an initial state ğ‘ , an action ğ‘,
and a landing state ğ‘ â€², the reward R(ğ‘ , ğ‘, ğ‘ â€²) is deterministic.

3.1. A First Attempt: Averaging the Bellman Equation

A ï¬rst approach to incorporating the stochasticity of the environment is to average the right-hand side of
the Bellman equation (4) and deï¬ne Z(ğ‘ , ğ›½) as the solution of:

Z(ğ‘ , ğ›½) =

Eğ‘ â€² |ğ‘ ,ğ‘

ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ Z(ğ‘ â€², ğ›½)

=

P(ğ‘ â€² | ğ‘ , ğ‘) ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ Z(ğ‘ â€², ğ›½) .

(12)

ğ‘
Ã•

h

i

ğ‘,ğ‘ â€²
Ã•

Interestingly, the solution of this equation can be constructed in the same spirit of Section 2.1 by
summing a functional over the set of trajectories. If we deï¬ne ğ¿(ğœ”) to be the log likelihood of a
trajectory: ğ¿(ğœ”) =

log P(ğ‘ ğ‘¡+1 | ğ‘ ğ‘¡ , ğ‘ğ‘¡ ) then Z(ğ‘ , ğ›½) is deï¬ned by

| ğœ” |âˆ’1
ğ‘¡=0

Ã

Z(ğ‘ , ğ›½) =

ğ‘’âˆ’ğ›½ğ¸ ( ğœ”)+ğœ‡ | ğœ” |+ğ¿ ( ğœ”) ,

(13)

Ã•ğœ” âˆˆÎ©(ğ‘ )

satisï¬es the Bellman equation (12). The proof can be found in Appendix B.1. In Appendix B.2 we derive
the Bellman equation satisï¬ed by the underlying value function ğ‘‰ (ğ‘ , ğ›½) and we ï¬nd:

ğ‘‰ (ğ‘ , ğ›½) =

ğ‘,ğ‘ â€²
Ã•

ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ Z(ğ‘ â€², ğ›½)
Z(ğ‘ , ğ›½)

Ã— P(ğ‘ â€² | ğ‘ , ğ‘) Ã— (R(ğ‘ , ğ‘, ğ‘ â€²) + ğ‘‰ (ğ‘ â€², ğ›½)) .

(14)

This Bellman equation does not correspond to a realistic policy; the policy depends on the landing state ğ‘ â€²
which is a random variable. The agentâ€™s policy and the environmentâ€™s transitions cannot be decoupled. This
is not surprising, from Eq. 13 we see that Z puts rewards and transition probabilities on an equal footing.
As a result an agent believes they can choose any available transition as long as they are willing to pay the
price in log probability. This encourages risky behavior: the agent is encouraged to bet on highly unlikely
but beneï¬cial transitions. These observations were also noted in Levine (2018).

3.2. A Variational Approach

Constructing a partition function for a stochastic MDP is not straightforward because there are two types
of randomness: the ï¬rst comes from the agentâ€™s policy and the second from stochasticity of the environment.
Mixing these two sources of randomness can lead to unrealistic policies as we saw in Section 3.1. A more
principled approach is needed.

We construct a new deterministic MDP (ËœS, ËœA, ËœR, ËœP) from (S, A, R, P). We take ËœS to be the space of
probability distributions over S, similar to belief state representations for partially-observable MDPs (Astrom,
1965; Sondik, 1978; Kaelbling et al., 1998). We make the assumption that the actions A are the same for
all states and take ËœA = A. For ğœŒ âˆˆ ËœS and ğ‘ âˆˆ ËœA we deï¬ne ËœP(ğœŒ, ğ‘) := ğ‘ƒğ‘
ğ‘‡ ğœŒ where ğ‘ƒğ‘ is the transition matrix
corresponding to choosing action ğ‘ in the original MDP. We deï¬ne ËœR(ğœŒ, ğ‘) := Eğ‘ âˆ¼ğœŒ

Eğ‘ â€² |ğ‘ ,ğ‘ [R(ğ‘ , ğ‘, ğ‘ â€²)]

S being ï¬nite, it has a ï¬nite number ğ‘€ of ï¬nal states which we denote { ğ‘“ğ‘–}ğ‘– âˆˆ{1, Â·Â·Â· , ğ‘€ }. The ï¬nal states
(cid:2)
ğ‘€
ğ‘–=1 ğ›¼ğ‘– = 1 and ğ›¿ ğ‘“ğ‘– is a Dirac delta function at
ğ‘€
ğ‘–=1 ğ›¼ğ‘–R( ğ‘“ğ‘–). This leads to the

of ËœS are of the form ğœŒ ğ‘“ =
state ğ‘“ğ‘–. The intrinsic value ğœŒ ğ‘“ of such a ï¬nal state is then given by R(ğœŒ ğ‘“ ) =
boundary conditions:

ğ‘€
ğ‘–=1 ğ›¼ğ‘–ğ›¿ ğ‘“ğ‘– where 0 â‰¤ ğ›¼ğ‘– â‰¤ 1 verify

Ã

Ã

(cid:3)

.

Ã

Z(ğœŒ ğ‘“ ) = exp

ğ›½

ğ‘€

Ã•ğ‘–=1

ğ›¼ğ‘–R(ğ‘  ğ‘“ğ‘– )

6

=

!

ğ‘€

Ã–ğ‘–=1

Z( ğ‘“ğ‘–, ğ›½) ğ›¼ğ‘– .

(15)

 
This new MDP (ËœS, ËœA, ËœR, ËœP) is deterministic, and we can follow the same approach of Section 2 and construct
a partition function Z(ğœŒ, ğ›½) on ËœS. Z(ğ‘ , ğ›½) can be recovered by evaluating Z(ğ›¿ğ‘ , ğ›½). From this construction
we also get that Z(ğœŒ, ğ›½) satisï¬es the following Bellman equation:
ğ‘’ğ›½R(ğœŒ,ğ‘)+ğœ‡ Z(ğ‘ƒğ‘

Z(ğœŒ, ğ›½) =

ğ‘‡ ğœŒ, ğ›½) .

(16)

ğ‘
Ã•

Just as it is the case for deterministic MDPs, the Bellman operator associated with this equation is
a contraction. This is proved in Appendix B.3. However ËœS is now inï¬nite which makes solving Eq. 16
intractable. We adopt a variational approach which consists in ï¬nding the best approximation of Z(ğœŒ, ğ›½)
within a parametric family {Z ğœƒ } ğœƒ âˆˆÎ˜. We measure the ï¬tness of a candidate through the following loss
function: Î”(ğœƒ) = 1
|S |

ğ‘ ğ‘’ğ›½R( ğ›¿ğ‘  ,ğ‘)+ğœ‡ Z ğœƒ (ğ‘ƒğ‘

Z ğœƒ (ğ›¿ğ‘ , ğ›½) âˆ’

ğ‘‡ ğ›¿ğ‘ , ğ›½)

ğ‘  âˆˆS

2

.

(cid:0)

Ã

For illustration purposes, and inspired by the form of the boundary conditions (15), we consider a
ğœŒğ‘– , where ğœƒ âˆˆ R|S |.
simple parametric family given by the partition functions of the form Z ğœƒ (ğœŒ) =
The optimal ğœƒ can be found using usual optimization techniques such as gradient descent. By evaluation
Z(ğ‘†ğ‘–)ğœŒğ‘– .
of Z ğœƒ at ğœŒ = ğ›¿ğ‘†ğ‘– we see that we must have ğœƒğ‘– = Z(ğ›¿ğ‘†ğ‘– ) = Z(ğ‘†ğ‘–) and consequently we have Z ğœƒ (ğœŒ) =
The optimal solution satisï¬es the following Bellman equation:

|S |
ğ‘–=1 ğœƒğ‘–

|S |
ğ‘–=1

Ã

Ã

(cid:1)

Z(ğ‘ , ğ›½) â‰ˆ

ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ Z(ğ‘ â€², ğ›½)

P(ğ‘ â€² |ğ‘ ,ğ‘)

Ã

(17)

h
The underlying value function veriï¬es ğ‘‰ (ğ‘ , ğ›½) â‰ˆ
policy ğœ‹ is given by ğœ‹(ğ‘ | ğ‘ ) âˆ
Ã
as its only dependency is on the current state, not a future one, unlike the policies arising from Eq. 14.

ğ‘,ğ‘ â€² ğœ‹(ğ‘ | ğ‘ ) P(ğ‘ â€² | ğ‘ , ğ‘) (R(ğ‘ , ğ‘, ğ‘ â€²) + ğ‘‰ (ğ‘ â€², ğ›½)) where the
. This approach leads to a realistic policy

ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ Z(ğ‘ â€², ğ›½)

P(ğ‘ â€² |ğ‘ ,ğ‘)

ğ‘ â€² âˆˆS

i

ğ‘
Ã•

Ã–ğ‘ â€² âˆˆS

Ã

(cid:2)

(cid:3)

Ã

4. The Model-Free Case

4.1. Construction of State-Action-Dependent Partition Function

In a model free setting, where the transition dynamics are unknown, state-only value functions such as ğ‘‰ (ğ‘ )
are less useful than state-action value functions such as ğ‘„(ğ‘ , ğ‘). Consequently, we will extend our construction
to state-action partition functions Z(ğ‘ , ğ‘, ğ›½). For a deterministic environment, we extend the construction in
Section 2 and deï¬ne Z(ğ‘ , ğ‘, ğ›½) by

Z(ğ‘ , ğ‘, ğ›½) =

ğ‘’âˆ’ğ›½ğ¸ ( ğœ”)+ğœ‡ | ğœ” | =

ğ‘’ğ›½

| ğœ”|
ğ‘–=0 ğ‘Ÿğ‘– +ğœ‡ | ğœ” |

(18)

Ã•ğœ” âˆˆÎ©(ğ‘ ,ğ‘)
where Î©(ğ‘ , ğ‘) denotes the set of trajectories having (ğ‘ 0, ğ‘0) = (ğ‘ , ğ‘). Since Î©(ğ‘ ) =
Z(ğ‘ , ğ›½) =
equation:

ğ‘ âˆˆA Î©(ğ‘ , ğ‘), we have
ğ‘ Z(ğ‘ , ğ‘, ğ›½). As a consequence of this construction, Z(ğ‘ , ğ‘, ğ›½) satisï¬es the following linear Bellman

Ã•ğœ” âˆˆÎ©(ğ‘ ,ğ‘)

Ã

Ã

Z(ğ‘ , ğ‘, ğ›½) = ğ‘’ğ›½R(ğ‘ ,ğ‘)+ğœ‡

Z(ğ‘  + ğ‘, ğ‘â€², ğ›½) .

(19)

This Bellman equation can be easily derived by decomposing each trajectory ğœ” âˆˆ Î©(ğ‘ , ğ‘) into two parts:
the ï¬rst transition resulting from taking initial action ğ‘ and the remainder of the trajectory ğœ”â€² which is a
member of Î©(ğ‘  + ğ‘, ğ‘â€²) for some action ğ‘â€² âˆˆ A . The total energy and length can also be decomposed in the
same way, so that:

ğ‘â€²
Ã•

Z(ğ‘ , ğ‘, ğ›½) =

ğ‘’âˆ’ğ›½ğ¸ ( ğœ”)+ğœ‡ | ğœ” | =

ğ‘’ğ›½

| ğœ”|
ğ‘–=0 ğ‘Ÿğ‘– +ğœ‡ | ğœ” |

Ã•ğœ” âˆˆÎ©(ğ‘ ,ğ‘)
= ğ‘’ğ›½ R(ğ‘ ,ğ‘)+ğœ‡

= ğ‘’ğ›½ R(ğ‘ ,ğ‘)+ğœ‡

ğ‘’ğ›½

Ã

Ã•ğœ” âˆˆÎ©(ğ‘ ,ğ‘)

Ã

Ã•ğœ” âˆˆÎ©(ğ‘ ,ğ‘)
ğ‘¡=1 ğ‘Ÿğ‘¡ ( ğœ”)+ğœ‡ ( | ğœ” |âˆ’1) = ğ‘’ğ›½ R(ğ‘ ,ğ‘)+ğœ‡

| ğœ”|

ğ‘’âˆ’ğ›½ ğ¸ ( ğœ”â€²)+ğœ‡ | ğœ”â€² |

ğ‘’âˆ’ğ›½ ğ¸ ( ğœ”â€²)+ğœ‡ | ğœ”â€² | = ğ‘’ğ›½ R(ğ‘ ,ğ‘)+ğœ‡

Z(ğ‘  + ğ‘, ğ‘â€², ğ›½).

Ã•ğœ”â€² âˆˆÎ©(ğ‘ +ğ‘)

Ã•ğ‘â€² âˆˆA

Ã•ğœ”â€² âˆˆÎ©(ğ‘ +ğ‘,ğ‘â€²)

7

Ã•ğ‘â€² âˆˆA

In the same spirit of Section 2.3, one can show that
ğ‘„(ğ‘ , ğ‘, ğ›½) = ğœ•

ğœ•ğ›½ log Z(ğ‘ , ğ‘, ğ›½) satisï¬es a Bellman equation:

the average underlying value function

ğ‘„(ğ‘ , ğ‘, ğ›½) = R(ğ‘ , ğ‘) +

ğœ‹(ğ‘â€² | ğ‘  + ğ‘) ğ‘„(ğ‘  + ğ‘, ğ‘â€², ğ›½)

ğœ‹(ğ‘ | ğ‘ ) =

Ã•ğ‘â€²

Z(ğ‘ , ğ‘, ğ›½)
ğ‘â€² Z(ğ‘ , ğ‘â€², ğ›½)

(20)

ğ‘„(ğ‘ , ğ‘, ğ›½) can be then reinterpreted as the ğ‘„-function of the policy ğœ‹. Similarily to the results of Section 2.3
and Appendix A.2, the policy ğœ‹ can be thought of a Boltzmann policy of parameter ğ›½ that takes entropy
into account. This construction can be extend to a stochastic environments by following the same approach
used in Section 3.2.

Ã

In the following we show how learning the state-action partition function Z(ğ‘ , ğ‘, ğ›½) leads to an alternative

approach to model-free reinforcement learning that does not explicitly represent value functions.

4.2. A Learning Algorithm

In ğ‘„-Learning, the update rule typically consists of a linear interpolation between the current value estimate
and the one arising a posteriori:

ğ‘„(ğ‘ ğ‘¡ , ğ‘ğ‘¡ ) â† (1 âˆ’ ğ›¼)ğ‘„(ğ‘ ğ‘¡ , ğ‘ğ‘¡ ) + ğ›¼

ğ‘Ÿğ‘¡ + ğ›¾ max
ğ‘ğ‘¡+1

ğ‘„(ğ‘ ğ‘¡+1, ğ‘ğ‘¡+1)

(cid:18)

(cid:19)

(21)

where ğ›¼ âˆˆ [0, 1] is the learning rate and ğ›¾ is the discount factor. For Z-functions we will replace the linear
interpolation with a geometric one. We take the update rule for Z-functions to be the following:

Z(ğ‘ ğ‘¡ , ğ‘ğ‘¡ , ğ›½) â† Z(ğ‘ ğ‘¡ , ğ‘ğ‘¡ , ğ›½)1âˆ’ğ›¼ Ã—

ğ‘’ğ›½ğ‘Ÿğ‘¡ +ğœ‡

Z(ğ‘ ğ‘¡+1, ğ‘ğ‘¡+1, ğ›½)

ğ‘ğ‘¡+1
Ã•

ğ›¼

.

!

(22)

To understand what this update rule is doing, it is insightful to look at how how the underlying ğ‘„-function,
ğ‘„(ğ‘ , ğ‘) = ğœ•

ğœ•ğ›½ log Z(ğ‘ ğ‘¡ , ğ‘ğ‘¡ , ğ›½) is updated. We ï¬nd:

ğ‘„(ğ‘ ğ‘¡ , ğ‘ğ‘¡ , ğ›½) â† (1 âˆ’ ğ›¼)ğ‘„(ğ‘ ğ‘¡ , ğ‘ğ‘¡ , ğ›½) + ğ›¼

ğ‘Ÿğ‘¡ +

Z(ğ‘ ğ‘¡+1, ğ‘ğ‘¡+1, ğ›½)
ğ‘â€² Z(ğ‘ ğ‘¡+1, ğ‘â€², ğ›½)

ğ‘„(ğ‘ ğ‘¡+1, ğ‘ğ‘¡+1, ğ›½)

.

!

ğ‘ğ‘¡+1
Ã•

(23)

We see that we recover a weighted version of the SARSA update rule. This update rule is referred to as
expected SARSA. Expected SARSA is known to reduce the variance in the updates by exploiting knowledge
about stochasticity in the behavior policy and hence is considered an improvement over vanilla SARSA
(Van Seijen et al., 2009).

Ã

Since the underlying update rule is equivalent to the expected SARSA update rule, we can use any
exploration strategy that works for expected SARSA. One exploration strategy could be ğœ–-greedy which
consists in taking action ğ‘ = argmaxğ‘ âˆˆAZ(ğ‘ , ğ‘, ğ›½) with probability 1 âˆ’ ğœ– and picking an action uniformly
at random with probability ğœ–. Another possibility would be a Boltzmann-like exploration which consists in
taking action ğ‘ with probability P(ğ‘ | ğ‘ ) âˆ Z(ğ‘ , ğ‘, ğ›½).

We would like to emphasize that even though the expected SARSA update is not novel, the learned
policies through this updates rule are proper to the partition-function approach. In particular, the learned
policies ğœ‹(ğ‘ | ğ‘ ) âˆ Z(ğ‘ , ğ‘, ğ›½) are Boltzmann-like policies with some entropic preference properties as described
in Section 2.3 and Appendix A.2.

5. Conclusion

In this article we discussed how planning and reinforcement learning problems can be approached through
the tools and abstractions of statistical physics. We started by constructing partition functions for each
8

 
 
state of a deterministic MDP and then showed how to extend that deï¬nition to the more general stochastic
MDP setting through a variational approach. Interestingly, these partition functions have their own Bellman
equation making it possible to solve planning and model-free RL problems without explicit reference to
value functions. Nevertheless, conventional value functions can be derived from our partition function and
interpreted via average energies. Computing the implied value functions can also shed some light on the
policies arising from these algorithms. We found that the learned policies are closely related to Boltzmann
policies with the additional interesting feature that they take entropy into consideration by favoring states
from which many trajectories are possible. Finally, we observed that working with partition functions is
more natural in some settings. In a deterministic environment for example, near-optimal Bellman equations
become linear which is not the case in a value-function-centric approach.

6. Acknowledgments

We would like to thank Alex Beatson, Weinan E, Karthik Narasimhan and Geoï¬€rey Roeder for helpful
discussions and feedback. This work was funded by a Princeton SEAS Innovation Grant and the Alfred P.
Sloan Foundation.

References

D Ackley, G Hinton, and T Sejnowski. A learning algorithm for Boltzmann machines. Cognitive Science, 9

(1):147â€“169, 1985.

Karl J Astrom. Optimal control of Markov processes with incomplete state information. Journal of

Mathematical Analysis and Applications, 10(1):174â€“205, 1965.

Hagai Attias. Planning by probabilistic inference. In International Conference on Artiï¬cial Intelligence and

Statistics, 2003.

Richard Bellman. Dynamic programming. Science, 153(3731):34â€“37, 1966.
Dimitri P Bertsekas. Dynamic programming and optimal control, volume 1. Athena scientiï¬c Belmont, MA,

1995.

Michael Betancourt.

A conceptual

introduction to Hamiltonian Monte Carlo.

arXiv preprint

arXiv:1701.02434, 2017.

Giuseppe Carleo, Ignacio Cirac, Kyle Cranmer, Laurent Daudet, Maria Schuld, Naftali Tishby, Leslie

Vogt-Maranto, and Lenka ZdeborovÂ´a. Machine learning and the physical sciences, 2019.

Ofer Dekel and Elad Hazan. Better rates for any adversarial deterministic MDP. In International Conference

on Machine Learning, pages 675â€“683, 2013.

Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid Monte Carlo. Physics

Letters B, 195(2):216â€“222, 1987.

John J Hopï¬eld. Neural networks and physical systems with emergent collective computational abilities.

Proceedings of the National Academy of Sciences, 79(8):2554â€“2558, 1982.

Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially

observable stochastic domains. Artiï¬cial intelligence, 101(1-2):99â€“134, 1998.

Rudolph Emil Kalman. A new approach to linear ï¬ltering and prediction problems. Journal of Basic

Engineering, 82(1):35â€“45, 1960.

Hilbert J Kappen. Optimal control theory and the linear Bellman equation. In D Barber, A T Cemgil, and

S Chiappa, editors, Bayesian Time Series Models. Cambridge University Press, 2011.

Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review, 2018.
David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003.
In Proceedings of
Omid Madani.
the Eighteenth conference on Uncertainty in Artiï¬cial Intelligence, pages 311â€“318. Morgan Kaufmann
Publishers Inc., 2002.

Polynomial value iteration algorithms for deterministic MDPs.

Marc Mezard and Andrea Montanari. Information, physics, and computation. Oxford University Press, 2009.

9

Iain Murray and Zoubin Ghahramani. Bayesian learning in undirected graphical models: approximate MCMC
algorithms. In Proceedings of the 20th Conference on Uncertainty in Artiï¬cial Intelligence, pages 392â€“399.
AUAI Press, 2004.

Radford M Neal et al. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2

(11):2, 2011.

Edward J Sondik. The optimal control of partially observable Markov processes over the inï¬nite horizon:

Discounted costs. Operations Research, 26(2):282â€“304, 1978.

Edwin Stoudenmire and David J Schwab. Supervised learning with tensor networks. In Advances in Neural

Information Processing Systems, pages 4799â€“4807, 2016.

Emanuel Todorov. Linearly-solvable Markov decision problems.

In Advances in Neural Information

Processing Systems, pages 1369â€“1376, 2007.

Harm Van Seijen, Hado Van Hasselt, Shimon Whiteson, and Marco Wiering. A theoretical and empirical
In 2009 IEEE Symposium on Adaptive Dynamic Programming and

analysis of expected SARSA.
Reinforcement Learning, pages 177â€“184. IEEE, 2009.

Zheng Wen and Benjamin Van Roy. Eï¬ƒcient exploration and value function generalization in deterministic

systems. In Advances in Neural Information Processing Systems, pages 3021â€“3029, 2013.

Jonathan S Yedidia, William T Freeman, and Yair Weiss. Generalized belief propagation. In Advances in

Neural Information Processing Systems, pages 689â€“695, 2001.

Lenka ZdeborovÂ´a and Florent Krzakala. Statistical physics of inference: Thresholds and algorithms. Advances

in Physics, 65(5):453â€“552, 2016.

Brian D Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy.

PhD thesis, University of Washington, 2010.

10

Appendix A: Deterministic MDPs

A.1. Z(ğ’”, ğœ·) is well deï¬ned

Proposition 1. Z(ğ‘ , ğ›½) =
Proof. The MDP being ï¬nite, S has a ï¬nite number of ï¬nal state we can then ï¬nd a constant ğ¾ such that,
for all ï¬nal states ğ‘  ğ‘“ we have R(ğ‘  ğ‘“ ) â‰¤ ğ¾.

ğ‘–=0 ğ‘Ÿğ‘– +ğœ‡ | ğœ” | is well deï¬ned for ğœ‡ < âˆ’ log ğ‘‘.

ğœ” âˆˆÎ©(ğ‘ ) ğ‘’ğ›½

Ã

Ã

| ğœ”|

Z(ğ‘ , ğ›½) =

Ã•ğœ” âˆˆÎ©(ğ‘ )

=

Ã•ğœ” âˆˆÎ©(ğ‘ )

â‰¤ ğ‘’ğ›½ğ¾

| ğœ”|
ğ‘–=0 ğ‘Ÿğ‘– +ğœ‡ | ğœ” |

| ğœ”|âˆ’1
ğ‘–=0

ğ‘Ÿğ‘– +ğ›½R(ğ‘ | ğœ”| )+ğœ‡ | ğœ” |

ğ‘’ğ›½

ğ‘’ğ›½

Ã

Ã

Ã•ğœ” âˆˆÎ©(ğ‘ )

| ğœ”|âˆ’1
ğ‘–=0

ğ‘Ÿğ‘– +ğœ‡ | ğœ” |

ğ‘’ğ›½

Ã

ğ‘’ ğœ‡ | ğœ” |

Ã•ğœ” âˆˆÎ©(ğ‘ )
ğ‘’ ğœ‡ğ‘›

1

Ã•ğœ” âˆˆÎ©(ğ‘ ), | ğœ” |=ğ‘›

ğ‘’ ğœ‡ğ‘›ğ‘‘ğ‘›

(ğ‘’ ğœ‡+log ğ‘‘)ğ‘›

Ã•ğ‘›âˆˆN

Ã•ğ‘›âˆˆN

Ã•ğ‘›âˆˆN

â‰¤ ğ‘’ğ›½ğ¾

â‰¤ ğ‘’ğ›½ğ¾

â‰¤ ğ‘’ğ›½ğ¾

= ğ‘’ğ›½ğ¾

Where used the fact that all rewards {ğ‘Ÿğ‘– }ğ‘– âˆˆ{0,..., | ğœ” |âˆ’1} are non positive and that the number of available
ğ‘›âˆˆN (ğ‘’ ğœ‡+log ğ‘‘)ğ‘› becomes convergent and
actions at each state is bounded by ğ‘‘. When ğœ‡ < âˆ’ log ğ‘‘, the sum
(cid:3)
Z(ğ‘ , ğ›½) is well deï¬ned.
Remark 1. ğœ‡ < âˆ’ log ğ‘‘ is a suï¬ƒcient condition, but not a necessary one. Z(ğ‘ , ğ›½) could be well deï¬ned for
all values of ğœ‡. This happens for instance when Î©(ğ‘ ) is ï¬nite for all ğ‘ .

Ã

A.2. The underlying policy is Boltzmann-like

ğ‘–=0 ğ‘Ÿğ‘– +ğœ‡ | ğœ” | will become dominated by the contribution of few of its
For high values of ğ›½, the sum
terms. As ğ›½ â†’ +âˆ, the sum will be dominated by the contribution of the paths with the biggest reward. We
have

ğœ” âˆˆÎ©(ğ‘ ) ğ‘’ğ›½

Ã

Ã

| ğœ”|

log Z(ğ‘ , ğ›½) âˆ¼
ğ›½â†’âˆ

We see that ğ‘‰ (ğ‘ , ğ›½) = ğœ•

ğœ•ğ›½ log Z(ğ‘ , ğ›½) â†’

ğ›½â†’âˆ

max

ğ›½ max

| ğœ” |

ğ‘Ÿğ‘– (ğœ”), ğœ” âˆˆ Î©(ğ‘ )

(

Ã•ğ‘–=0
| ğœ” |
ğ‘–=0 ğ‘Ÿğ‘– (ğœ”), ğœ” âˆˆ Î©(ğ‘ )

)

.

o

| ğœ” |

Since the MDP is ï¬nite and deterministic, it has a ï¬nite number of transitions and rewards. Consequently,
takes discrete values, in particular, there is a ï¬nite gap Î” between the maximum
the set
value and the second biggest value of this set. Letâ€™s denote by Î©max(ğ‘ ) the set of trajectories that achieve
this maximum and by ğ‘max(ğ‘ ) =

ğ‘–=0 ğ‘Ÿğ‘– (ğœ”), ğœ” âˆˆ Î©(ğ‘ )

ğ‘’ ğœ‡ | ğœ” | .

nÃ

o

nÃ

ğ‘max(ğ‘ ) counts the number of trajectories Î©max(ğ‘ ) in a weighted way: longer trajectories contribute less
than shorter ones. It is a measure of the size of Î©max(ğ‘ ) that takes into account our preference for shorter
trajectories. Putting everything together we get:

ğœ” âˆˆÎ©max (ğ‘ )
Ã

Z(ğ‘ , ğ›½)
ğ‘’ğ›½ğ‘‰ (ğ‘ ,ğ›½)

(cid:18)

âˆ’ ğ‘max(ğ‘ )

ğ‘’âˆ’ğ›½Î”

â‰¤
ğ›½â†’âˆ

(cid:19)

11

Ã•ğœ” âˆˆÎ©(ğ‘ )

ğ‘’ ğœ‡ | ğœ” | â†’
ğ›½â†’âˆ

0

This shows that Z(ğ‘ , ğ›½) âˆ¼
ğ›½â†’âˆ

ğ‘max(ğ‘ ) ğ‘’ğ›½ğ‘‰ (ğ‘ ,ğ›½) , which results in the following policy for ğ›½ >> 1:

ğœ‹(ğ‘ | ğ‘ ) âˆ

ğ›½â†’âˆ

ğ‘max(ğ‘  + ğ‘)ğ‘’ğ›½ (R(ğ‘ ,ğ‘)+ğ‘‰ (ğ‘ +ğ‘,ğ›½))

ğœ‹ diï¬€ers from a traditional Boltzmann policy in the following way: if we have two actions ğ‘1 and ğ‘2 such
that R(ğ‘ , ğ‘1) + ğ‘‰ (ğ‘  + ğ‘1, ğ›½) = R(ğ‘ , ğ‘2) + ğ‘‰ (ğ‘  + ğ‘2, ğ›½) but there are twice more optimal trajectories spanning
from ğ‘  + ğ‘1 than there are from ğ‘  + ğ‘2 then action ğ‘1 will be chosen twice as often as ğ‘2. This is to contrast
with the usual Boltzmann policy that will pick ğ‘1 and ğ‘2 with equal probability. When ğ‘max(ğ‘ ) is the same
for all ğ‘ , we recover a Boltzmann policy. When ğ›½ â†’ +âˆ the policy converges to a an optimal policy and ğ‘‰
converges to the optimal value function.

A.3. ğ‘¿ â†’ ğ‘ª(ğœ·) ğ‘¿ is a contraction

Proposition 2. Let
ğ¶ ( ğ›½)ğ‘ ,ğ‘ â€² = 1ğ‘ â†’ğ‘ â€²ğ‘’ğ›½R(ğ‘ â†’ğ‘ â€²)+ğœ‡ + 1ğ‘ =ğ‘ â€²=ï¬nal state. The map deï¬ned by

n

X( ğ›½) =

ğ‘ âˆˆ R|S |
+

such for all ï¬nal states ğ‘  ğ‘“ we have ğ‘ğ‘  ğ‘“

= ğ‘’ğ›½R(ğ‘  ğ‘“ )

and let

o

X( ğ›½) â†’ X( ğ›½)
ğ‘‹

â†’ ğ¶ ( ğ›½) ğ‘‹

ğœ“ :

(

is a contraction for the sup-norm:

||ğ‘¥||âˆ = max

ğ‘– âˆˆ{1, Â·Â·Â· , |S | }

|ğ‘¥ğ‘– |.

Proof. X( ğ›½) is the set of all possible partition functions with compatible boundary conditions. The
matrix ğ¶ ( ğ›½) is more explicitly deï¬ned by:

ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´
ï£³

ğ¶ ( ğ›½)ğ‘ ,ğ‘ â€² =

1
0
ğ‘’ğ›½R(ğ‘ â†’ğ‘ â€²)+ğœ‡

if s = sâ€™ and state s is a ï¬nal state.
if there is no one step transition from state s to state sâ€™.
if the transition from state s to state sâ€™ generates reward R(ğ‘  â†’ ğ‘ â€²).

Because ğ¶ ( ğ›½)ğ‘ ,ğ‘  = 1 when ğ‘  is a ï¬nal state, the map ğœ“ is well deï¬ned (i.e. X( ğ›½) â†’ X( ğ›½)). Since the MDP is
ï¬nite, it has a ï¬nite number of ï¬nal state so there exists a constant ğ¾ such that, for all ï¬nal states ğ‘  ğ‘“ we
have R(ğ‘  ğ‘“ ) â‰¤ ğ¾.

Let ğ‘‹1, ğ‘‹2 âˆˆ X( ğ›½) we have:

kğœ“( ğ‘‹1) âˆ’ ğœ“( ğ‘‹2) kâˆ

= max

ğ‘– âˆˆ{1, Â·Â·Â· , |S | }

(ğ¶ ( ğ›½) ğ‘‹1 âˆ’ ğ¶ ( ğ›½) ğ‘‹2)ğ‘–

Without loss of generality we can assume that the MDP has ğ‘š ï¬nal states that are labeled |S| âˆ’ ğ‘š + 1, Â· Â· Â· , |S|.
Under this assumption we have:

(cid:12)
(cid:12)

(cid:12)
(cid:12)

max
ğ‘– âˆˆ{1, Â·Â·Â· , |S | }

|( ğ‘‹1 âˆ’ ğ‘‹2)ğ‘– | =

max
ğ‘– âˆˆ{1, Â·Â·Â· , |S |âˆ’ğ‘š}

|( ğ‘‹1 âˆ’ ğ‘‹2)ğ‘– |

This is because ğ‘‹1 and ğ‘‹2 have the same boundary conditions: âˆ€ ğ‘  ğ‘“ âˆˆ {|S| âˆ’ ğ‘š + 1, Â· Â· Â· , |S|}, ( ğ‘‹1)ğ‘  ğ‘“
Since ğ¶ ( ğ›½)ğ‘  ğ‘“ ,ğ‘  ğ‘“
conditions, we have: âˆ€ğ‘  ğ‘“ âˆˆ {|S| âˆ’ ğ‘š + 1, Â· Â· Â· , |S|}, [ğ¶ ( ğ›½) ğ‘‹1] ğ‘  ğ‘“

= ( ğ‘‹2)ğ‘  ğ‘“ .
is the index a ï¬nal state, ğ¶ ( ğ›½) ğ‘‹1 and ğ¶ ( ğ›½) ğ‘‹2 still have the same boundary

= [ğ¶ ( ğ›½) ğ‘‹2] ğ‘  ğ‘“ . This gives us:

= 1 if ğ‘  ğ‘“

max
ğ‘  âˆˆ{1, Â·Â·Â· , |S | }

(ğ¶ ( ğ›½) ğ‘‹1 âˆ’ ğ¶ ( ğ›½) ğ‘‹2)ğ‘ 

=

max
ğ‘  âˆˆ{1, Â·Â·Â· , |S |âˆ’ğ‘š}

(ğ¶ ( ğ›½) ğ‘‹1 âˆ’ ğ¶ ( ğ›½) ğ‘‹2)ğ‘ 

For ğ‘  âˆˆ {1, Â· Â· Â· , |S|}, we have:

(cid:12)
(cid:12)
(ğ¶ ( ğ›½) ğ‘‹1 âˆ’ ğ¶ ( ğ›½) ğ‘‹2)ğ‘ 

(cid:12)
(cid:12)
=

(cid:12)
(cid:12)
|S |
ğ‘ â€²=1 [ğ¶ ( ğ›½)] ğ‘ ,ğ‘ â€² ( ğ‘‹1 âˆ’ ğ‘‹2)ğ‘ â€²

.

(cid:12)
(cid:12)

(cid:12)
Ã
(cid:12)
Since there are at most ğ‘‘ available actions at each state and the environment is deterministic, at
(cid:12)
12

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

most ğ‘‘ coeï¬ƒcients ğ¶ ( ğ›½)ğ‘ ,ğ‘ â€² in this sum are non zero. Because the rewards are non positive, the non zero
ones can be bounded by ğ‘’ ğœ‡.

Putting all these pieces together we can write:
get:

|S |
ğ‘ â€²=1 [ğ¶ ( ğ›½)] ğ‘ ,ğ‘ â€² ( ğ‘‹1 âˆ’ ğ‘‹2)ğ‘ â€²

â‰¤ ğ‘‘ Ã— ğ‘’ ğœ‡ k ğ‘‹1 âˆ’ ğ‘‹2kâˆ. Finally we

(cid:12)
Ã
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

kğ¶ ( ğ›½) ğ‘‹1 âˆ’ ğ¶ ( ğ›½) ğ‘‹2kâˆ â‰¤

ğ‘‘ Ã— ğ‘’ ğœ‡

k ğ‘‹1 âˆ’ ğ‘‹2kâˆ

This proves that ğœ“ is a contraction.

<1 because ğœ‡<âˆ’ log ğ‘‘

|

{z

}

(cid:3)

Remark 2. We see here another mathematical similarity between the discount factor ğ›¾ < 1 usually used in
RL and the chemical potential ğœ‡ < âˆ’ log ğ‘‘. They both ensure that the Bellman operators are contractions.

Appendix B: Stochastic MDPs

B.1. Averaging the Bellman Equation and adding a likelihood cost are equivalent

Proposition 3. The partition function Z(ğ‘ , ğ›½) deï¬ned by Z(ğ‘ , ğ›½) :=
the following Bellman equation:

Z(ğ‘ , ğ›½) =

Eğ‘ â€² |ğ‘ ,ğ‘

Ã
ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ Z(ğ‘ â€², ğ›½)

ğœ” âˆˆÎ©(ğ‘ ) ğ‘’âˆ’ğ›½ğ¸ ( ğœ”)+ğœ‡ | ğœ” |+ğ¿ ( ğœ”)

satisï¬es

h
Proof. The proof follows the same path as the one in Section 2.2. We decompose each trajectory ğœ” âˆˆ Î© into
two parts: the ï¬rst transition resulting from taking a ï¬rst action ğ‘ and the rest of the trajectory ğœ”â€². The
energy, the length and the likelihood of the trajectory can be decomposed in a similar way as the sum of the
contribution of the ï¬rst transition and the contribution of the rest of the trajectory. We get:

i

ğ‘
Ã•

Z(ğ‘ , ğ›½) =

ğ‘’âˆ’ğ›½ğ¸ ( ğœ”)+ğœ‡ | ğœ” |+ğ¿ ( ğœ”)

=

=

=

=

Ã•ğœ” âˆˆÎ©(ğ‘ )

ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡+log(P(ğ‘ â€² |ğ‘ ,ğ‘)

ğ‘’âˆ’ğ›½ğ¸ ( ğœ”â€²)+ğœ‡ | ğœ”â€² |+ğ¿ ( ğœ”â€²)

ğ‘,ğ‘ â€²
Ã•

Ã•ğ‘,ğ‘ â€²

ğ‘,ğ‘ â€²
Ã•

ğ‘
Ã•

Ã•ğœ”â€² âˆˆÎ©(ğ‘ â€²)
ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡+log(P(ğ‘ â€² |ğ‘ ,ğ‘) Z(ğ‘ â€², ğ›½)

P(ğ‘ â€² | ğ‘ , ğ‘) ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ Z(ğ‘ â€², ğ›½)

Eğ‘ â€² |ğ‘ ,ğ‘

ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ Z(ğ‘ â€², ğ›½)

h

i

This proves the equivalence.

B.2. Deriving the Unrealistic Bellman Equation

Proposition 4. The value function ğ‘‰ (ğ‘ , ğ›½) = ğœ•

ğœ•ğ›½ log Z(ğ‘ , ğ›½) where

satisï¬es the following Bellman equation:

Z(ğ‘ , ğ›½) =

ğ‘’âˆ’ğ›½ğ¸ ( ğœ”)+ğœ‡ | ğœ” |+ğ¿ ( ğœ”)

Ã•ğœ” âˆˆÎ©(ğ‘ )

ğ‘‰ (ğ‘ , ğ›½) =

ğ‘,ğ‘ â€²
Ã•

ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ Z(ğ‘ â€², ğ›½)
Z(ğ‘ , ğ›½)

13

P(ğ‘ â€² | ğ‘ , ğ‘) [R(ğ‘ , ğ‘, ğ‘ â€²) + ğ‘‰ (ğ‘ â€², ğ›½)]

(cid:3)

 
 
Proof. From Appendix
Z(ğ‘ , ğ›½) =

B.1
we
ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ Z(ğ‘ â€², ğ›½)

ğ‘ Eğ‘ â€² |ğ‘ ,ğ‘

known

that

Z(ğ‘ , ğ›½)

satisï¬es

the

Bellman

equation:

.

(cid:3)

log Z(ğ‘ , ğ›½)

Ã

(cid:2)

ğ‘‰ (ğ‘ , ğ›½) = ğœ•
ğœ• ğ›½
= ğœ•
ğœ• ğ›½

log

Eğ‘ â€² |ğ‘ ,ğ‘

ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ Z(ğ‘ â€², ğ›½)

!

i

ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ Z(ğ‘ â€², ğ›½)

!

ğ‘
Ã•
ğœ•
ğœ• ğ›½  

h
Eğ‘ â€² |ğ‘ ,ğ‘

ğ‘
Ã•
Eğ‘ â€² |ğ‘ ,ğ‘

Eğ‘ â€² |ğ‘ ,ğ‘

(cid:20)

(cid:20)

=

=

=

=

=

=

=

1
Z(ğ‘ , ğ›½)

1
Z(ğ‘ , ğ›½)

1
Z(ğ‘ , ğ›½)

1
Z(ğ‘ , ğ›½)

1
Z(ğ‘ , ğ›½)

1
Z(ğ‘ , ğ›½)

Ã•ğ‘,ğ‘ â€²

ğ‘
Ã•

ğ‘
Ã•

ğ‘
Ã•

ğ‘
Ã•

ğ‘
Ã•

h

i
ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ Z(ğ‘ â€², ğ›½)

ğœ•
ğœ• ğ›½
R(ğ‘ , ğ‘, ğ‘ â€²) ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ Z(ğ‘ â€², ğ›½) + ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ ğœ•
ğœ• ğ›½

(cid:17)(cid:21)

(cid:16)

Z(ğ‘ â€², ğ›½)

(cid:21)

Eğ‘ â€² |ğ‘ ,ğ‘

"  

R(ğ‘ , ğ‘, ğ‘ â€²) +

Eğ‘ â€² |ğ‘ ,ğ‘

R(ğ‘ , ğ‘, ğ‘ â€²) +

ğœ•
ğœ• ğ›½

Z(ğ‘ â€², ğ›½)
ğœ•
ğœ•ğ›½
Z(ğ‘ â€², ğ›½)

!

ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ Z(ğ‘ â€², ğ›½)

#

log Z(ğ‘ â€², ğ›½)

ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ Z(ğ‘ â€², ğ›½)

(cid:20) (cid:18)
(R(ğ‘ , ğ‘, ğ‘ â€²) + ğ‘‰ (ğ‘ â€², ğ›½)) ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ Z(ğ‘ â€², ğ›½)

(cid:19)

Eğ‘ â€² |ğ‘ ,ğ‘

h

ğ‘’ğ›½R(ğ‘ ,ğ‘,ğ‘ â€²)+ğœ‡ Z(ğ‘ â€², ğ›½)
Z(ğ‘ , ğ›½)

P(ğ‘ â€² | ğ‘ , ğ‘) [R(ğ‘ , ğ‘, ğ‘ â€²) + ğ‘‰ (ğ‘ â€², ğ›½)]

(cid:21)

i

(cid:3)

B.3. The Bellman operator of Z(ğ†, ğœ·) is a contraction

Proposition
and X( ğ›½) =
The map deï¬ned by

(cid:8)

5.

Let
ğ‘‹ âˆˆ ğ¶0 (D, R) s.t. ğ‘‹ (ğœŒ ğ‘“ ) = exp

ğ›½

ğ‘€
ğ‘–=1 ğ›¼ğ‘–R( ğ‘“ğ‘–)

D = {ğ›¼ âˆˆ R|S | such that âˆ€ğ‘– âˆˆ 1, Â· Â· Â· , |S|, 0 â‰¤ ğ›¼ğ‘– â‰¤ 1 and

for mixtures of ï¬nal states ğœŒ ğ‘“ =
Ã
Ã

, |S |
ğ‘–=1 ğ›¼ğ‘– = 1}
ğ‘€
.
ğ‘–=1 ğ›¼ğ‘–ğ›¿ ğ‘“ğ‘–

(cid:9)

(cid:2)
X( ğ›½) â†’ X( ğ›½)

Ã

(cid:3)

ğœ“ :

ğ‘‹

â†’

D â†’ R
ğœŒ â†’

(

ğ‘ ğ‘’ğ›½R(ğœŒ,ğ‘)+ğœ‡ ğ‘‹ (ğ‘ƒğ‘

ğ‘‡ ğœŒ, ğ›½)

ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´
ï£³

is a contraction for the sup-norm: k ğ‘‹ kâˆ = max
ğœŒâˆˆD

| ğ‘‹ (ğœŒ)|.

Ã

Proof. D is the standard (|S| âˆ’ 1)-simplex in R|S | and X( ğ›½) be the set of continuous functions on D
satisfying the right boundary conditions. The original MDP is ï¬nite, consequently it has a ï¬nite number ğ‘€
of ï¬nal state and it is possible to ï¬nd a constant ğ¾ such that, for all ï¬nal states ğ‘  ğ‘“ we have R(ğ‘  ğ‘“ ) â‰¤ ğ¾.

Let ğ‘‹1, ğ‘‹2 âˆˆ X( ğ›½). ğ‘‹1 and ğ‘‹2 have the same boundary conditions by construction. Not only that,
ğœ“( ğ‘‹1) and ğœ“( ğ‘‹2) have also the same boundary conditions since the map ğœ“ doesnâ€™t alter boundary conditions.
Consequently we have:

kğœ“( ğ‘‹1) âˆ’ ğœ“( ğ‘‹2) kâˆ

= max
ğœŒâˆˆD

|ğœ“( ğ‘‹1) (ğœŒ) âˆ’ ğœ“( ğ‘‹2) (ğœŒ)| =

max
ğœŒâˆˆD, ğœŒ non ï¬nal

|ğœ“( ğ‘‹1) (ğœŒ) âˆ’ ğœ“( ğ‘‹2) (ğœŒ)|

14

 
Finally we can write:

kğœ“( ğ‘‹1) âˆ’ ğœ“( ğ‘‹2) kâˆ

=

=

â‰¤

â‰¤

max
ğœŒâˆˆD, ğœŒ non ï¬nal

|ğœ“( ğ‘‹1) (ğœŒ) âˆ’ ğœ“( ğ‘‹2) (ğœŒ)|

max
ğœŒâˆˆD, ğœŒ non ï¬nal

max
ğœŒâˆˆD, ğœŒ non ï¬nal

ğ‘‘ Ã— ğ‘’ ğœ‡

ğ‘’ğ›½R(ğœŒ,ğ‘)+ğœ‡

ğ‘’ğ›½R(ğœŒ,ğ‘)+ğœ‡

(cid:12)
ğ‘
(cid:12)
Ã•
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ğ‘
(cid:12)
Ã•
(cid:12)
k ğ‘‹1 âˆ’ ğ‘‹2kâˆ
(cid:12)
(cid:12)

ğ‘‹1(ğ‘ƒğ‘

ğ‘‡ ğœŒ, ğ›½) âˆ’ ğ‘‹2(ğ‘ƒğ‘

ğ‘‡ ğœŒ, ğ›½)

(cid:2)
Ã— k ğ‘‹1 âˆ’ ğ‘‹2kâˆ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:3)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

<1 because ğœ‡<âˆ’ log ğ‘‘

{z
Where we use the fact that all rewards are non positive and that the number of available actions is bounded
by ğ‘‘. This concludes the proof that the Bellman operator of Z(ğœŒ, ğ›½) is a contraction.

|

}

This proof is generalization of the proof presented in Appendix A.3 for MDPs with ï¬nite state spaces.

(cid:3)

15

 
 
