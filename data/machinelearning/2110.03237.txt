2
2
0
2

n
a
J

5
2

]

G
L
.
s
c
[

4
v
7
3
2
3
0
.
0
1
1
2
:
v
i
X
r
a

Score-based Generative Neural Networks for
Large-Scale Optimal Transport

Max Daniels
Northeastern University
daniels.g@northeastern.edu

Tyler Maunu ∗
Brandeis University
maunu@brandeis.edu

Paul Hand
Northeastern University
p.hand@northeastern.edu

Abstract

We consider the fundamental problem of sampling the optimal transport coupling
between given source and target distributions. In certain cases, the optimal transport
plan takes the form of a one-to-one mapping from the source support to the
target support, but learning or even approximating such a map is computationally
challenging for large and high-dimensional datasets due to the high cost of linear
programming routines and an intrinsic curse of dimensionality. We study instead
the Sinkhorn problem, a regularized form of optimal transport whose solutions are
couplings between the source and the target distribution. We introduce a novel
framework for learning the Sinkhorn coupling between two distributions in the
form of a score-based generative model. Conditioned on source data, our procedure
iterates Langevin Dynamics to sample target data according to the regularized
optimal coupling. Key to this approach is a neural network parametrization of the
Sinkhorn problem, and we prove convergence of gradient descent with respect to
network parameters in this formulation. We demonstrate its empirical success on a
variety of large scale optimal transport tasks.

1

Introduction

It is often useful to compare two data distributions by computing a distance between them in some
appropriate metric. For instance, statistical distances can be used to ﬁt the parameters of a distribution
to match some given data. Comparison of statistical distances can also enable distribution testing,
quantiﬁcation of distribution shifts, and provide methods to correct for distribution shift through
domain adaptation [12].

Optimal transport theory provides a rich set of tools for comparing distributions in Wasserstein
Distance. Intuitively, an optimal transport plan from a source distribution σ ∈ M+(X ) to a target
distribution τ ∈ M+(Y) is a blueprint for transporting the mass of σ to match that of τ as cheaply as
possible with respect to some ground cost. Here, X and Y are compact metric spaces and M+(X )
denotes the set of positive Radon measures over X , and it is assumed that σ, τ are supported over all
of X , Y respectively. The Wasserstein Distance between two distributions is deﬁned to be the cost of
an optimal transport plan.

Because the ground cost can incorporate underlying geometry of the data space, optimal transport
plans often provide a meaningful correspondence between points in X and Y. A famous example
is given by Brenier’s Theorem, which states that, when X , Y ⊆ Rd and σ, τ have ﬁnite variance,
the optimal transport plan under a squared-l2 ground cost is realized by a map T : X → Y [26,
Theorem 2.12]. However, it is often computationally challenging to exactly compute optimal transport
plans, as one must exactly solve a linear program requiring time which is super-quadratic in the size
of input datasets [5].

∗Work done while an Instructor in Applied Mathematics at MIT.

35th Conference on Neural Information Processing Systems (NeurIPS 2021).

 
 
 
 
 
 
Figure 1: We use SCONES to sample the mean-squared-L2 cost, entropy regularized optimal transport
mapping between 2x downsampled CelebA images (Source) and unmodiﬁed CelebA images (Target)
at λ = 0.005 regularization.

Instead, we opt to study a regularized form of the optimal transport problem whose solution takes the
form of a joint density π(x, y) with marginals πX (x) = σ(x) and πY (y) = τ (y). A correspondence
between points is given by the conditional distribution πY |X=x(y), which relates each input point to
a distribution over output points.

In recent work [22], the authors propose a large-scale stochastic dual approach in which π(x, y) is
parametrized by two continuous dual variables that may be represented by neural networks and trained
at large-scale via stochastic gradient ascent. Then, with access to π(x, y), they approximate an optimal
EπY |X=x [d(y, Y )], where
transport map using a barycentric projection of the form T : x (cid:55)→ arg miny
d : Y × Y → R is a convex cost on Y. Their method is extended by [15] to the problem of learning
regularized Wasserstein barycenters. In both cases, the Barycentric projection is observed to induce
averaging artifacts such as those shown in Figure 2.

Instead, we propose a direct sampling strategy to generate samples from πY |X=x(y) using a score-
based generative model. Score-based generative models are trained to sample a generic probability
density by iterating a stochastic dynamical system knows as Langevin dynamics [24]. In contrast to
projection methods for large-scale optimal transport, we demonstrate that pre-trained score based
generative models can be naturally applied to the problem of large-scale regularized optimal transport.
Our main contributions are as follows:

1. We show that pretrained score based generative models can be easily adapted for the purpose
of sampling high dimensional regularized optimal transport plans. Our method eliminates
the need to estimate a barycentric projection and it results in sharper samples because it
eliminates averaging artifacts incurred by such a projection.

2. Score based generative models have been used for unconditional data generation and for
conditional data generation in settings such as inpainting. We demonstrate how to adapt
pretrained score based generative models for the more challenging conditional sampling
problem of regularized optimal transport.

3. Our method relies on a neural network parametrization of the dual regularized optimal
transport problem. Under assumptions of large network width, we prove that gradient
descent w.r.t. neural network parameters converges to a global maximizer of the dual
problem. We also prove optimization error bounds based on a stability analysis of the dual
problem.

4. We demonstrate the empirical success of our method on a synthetic optimal transport task

and on optimal transport of high dimensional image data.

2 Background and Related Work

We will brieﬂy review some key facts about optimal transport and generative modeling. For a more
expansive background on optimal transport, we recommend the references [26] and [25].

2

Figure 2: Samples generated by SCONES for entropy regularized optimal transport including the
samples shown in Figure 1. At regularization λ = 0.005, optimal transportation with L2 cost has a
visible effect on generated images. This effect diminishes at increased regularization λ = 0.1.

2.1 Regularized Optimal Transport

We begin by reviewing the formulation of the regularized OT problem.

Deﬁnition 2.1 (Regularized OT). Let σ ∈ M+(X ) and τ ∈ M+(Y) be probability measures
supported on compact sets X , Y. Let c : X × Y → R be a convex, lower semi-continuous function
representing cost of transporting a point x ∈ X to y ∈ Y. The regularized optimal transport distance
OTλ(σ, τ ) is given by

OTλ(σ, τ ) = min

π

Eπ[c(x, y)] + λH(π)

subject to πX = σ,

πY = τ

π(x, y) ≥ 0

(1)

where H : M+(X × Y) → R is a convex regularizer and λ ≥ 0 is a regularization parameter.

We are mainly concerned with optimal transport of empirical distributions, where X and Y are ﬁnite
and σ, τ are empirical probability vectors. In most of the following theorems, we will work in the
empirical setting of Deﬁnition 2.1, so that X and Y are ﬁnite subsets of Rd and σ, τ are vectors in
the probability simplices of dimension |X | and |Y|, respectively.
We refer to the objective Kλ(π) = Eπ[c(x, y)] + λH(π) as the primal objective, and we will use
Jλ(ϕ, ψ) to refer to the associated dual objective, with dual variables ϕ, ψ. Two common regularizers
are H(π) = KL(π||σ×τ ) and H(π) = χ2(π||σ×τ ), sometimes called entropy and l2 regularization
respectively:

KL(π||σ × τ ) = Eπ

(cid:20)

log

(cid:18) dπ(x, y)

(cid:19)(cid:21)

dσ(x)dτ (y)

, χ2(π||σ × τ ) = Eσ×τ

(cid:34)(cid:18) dπ(x, y)
dσ(x)dτ (y)

(cid:19)2(cid:35)

dπ(x,y)
dσ(x)dτ (y) is the Radon-Nikodym derivative of π with respect to the product measure σ × τ .

where
These regularizers contribute useful optimization properties to the primal and dual problems.

For example, KL(π||σ × τ ) is exactly the mutual information Iπ(X; Y ) of the coupling (X, Y ) ∼ π,
so intuitively speaking, entropy regularization explicitly prevents πY |X=x from concentrating on a
point by stipulating that the conditional measure retain some bits of uncertainty after conditioning.
The effects of this regularization are described by Propositions 2.2 and 2.3.

First, regularization induces convexity properties which are useful from an optimization perspective.
Proposition 2.2. In the empirical setting of Deﬁnition 2.1, the entropy regularized primal prob-
lem Kλ(π) is λ-strongly convex in l1 norm. The dual problem Jλ(ϕ, ψ) is concave, uncon-
strained, and 1
λ -strongly smooth in l∞ norm. Additionally, these objectives witness strong duality:
inf π∈M+(X ×Y) Kλ(π) = supϕ,ψ∈R2d Jλ(ϕ, ψ), and the extrema of each objective are attained over
their respective domains.

In addition to these optimization properties, regularizing the OT problem induces a speciﬁc form of
the dual objective and resulting optimal solutions.

3

SourceTarget samplesKL λ=0.005KL λ=0.1Proposition 2.3. In the setting of Proposition 2.2, the KL-regularized dual objective takes the form

Jλ(ϕ, ψ) := Eσ[ϕ(x)] + Eτ [ψ(y)]

− λEσ×τ

(cid:20) 1
e

exp

(cid:18) 1
λ

(ϕ(x) + ψ(y) − c(x, y))

.

(cid:19)(cid:21)

The optimal solutions ϕ∗, ψ∗ = arg maxϕ,ψ∈R2d Jλ(ϕ, ψ) and π∗ = arg minπ∈M+(X ×Y) Kλ(π)
satisfy

π∗(x, y) =

1
e

exp

(cid:18) 1
λ

(ϕ∗(x) + ψ∗(y) − c(x, y))

(cid:19)

σ(x)τ (y).

These propositions are specializations of Proposition 2.4 and they are well-known to the literature on
entropy regularized optimal transport [5, 2]. The solution π∗(x, y) of the entropy regularized problem
is often called the Sinkhorn coupling between σ and τ in reference to Sinkhorn’s Algorithm [23], a
popular approach to efﬁciently solving the discrete entropy regularized OT problem. For arbitrary
choices of regularization, we call π∗(x, y) a Sinkhorn coupling.

Propositions 2.2 and 2.3 illustrate the main desiderata when choosing the regularizer: that H(π), and
hence Kλ(π), be strongly convex in l1 norm and that H(π) induce a nice analytic form of π∗ in terms
of ϕ∗, ψ∗. In regards to the former, H(π) is akin to barrier functions used by interior point methods
[5]. Prior work [2] is an example of the latter, in which it is shown that for discrete optimal transport,
χ2 regularization yields an analytic form of π∗ having a thresholding operation that promotes sparsity.
Conveniently, the KL and χ2 regularizers both belong to the class of f -Divergences, which are
statistical divergences of the form

Df (p||q) = Eq

(cid:20)
f

(cid:18) p(x)
q(x)

(cid:19)(cid:21)

.

where f : R → R is convex, f (1) = 0, p, q are probability measures, and p is absolutely continuous
with respect to q. For example, the KL regularizer has f KL(t) = t log(t) and the χ2 regularizer has
fχ2 (t) = t2 − 1. The f -Divergences are good choices for regularizing optimal transport: strong
convexity of f is a sufﬁcient condition for strong convexity of Hf (π) := Df (π||σ × τ ) in l1 norm,
and the form of f is the aspect which determines the form of π∗ in terms of ϕ∗, ψ∗. This relationship
is captured by the following generalization of Propositions 2.2 and 2.3, which we prove in Section B
of the Supplemental Materials.
Proposition 2.4. Consider the empirical setting of Deﬁnition 2.1. Let f (v) : R → R be a differen-
tiable α-strongly convex function with convex conjugate f ∗(v). Set f ∗(cid:48)(v) = ∂vf ∗(v). Deﬁne the
violation function V (x, y; ϕ, ψ) = ϕ(x) + ψ(y) − c(x, y). Then,

1. The Df regularized primal problem Kλ(π) is λα-strongly convex in l1 norm. With respect
to dual variables ϕ ∈ R|X | and ψ ∈ R|Y|, the dual problem Jλ(ϕ, ψ) is concave, uncon-
strained, and 1
λα -strongly smooth in l∞ norm. Strong duality holds: Kλ(π) ≥ Jλ(ϕ, ψ) for
all π, ϕ, ψ, with equality for some triple π∗, ϕ∗, ψ∗.

2. Jλ(ϕ, ψ) takes the form Jλ(ϕ, ψ) = Eσ[ϕ(x)] + Eτ [ψ(y)] − Eσ×τ [H ∗

f (V (x, y; ϕ, ψ))],

where H ∗

f (v) = λf ∗(λ−1v).

3. The optimal solutions (π∗, ϕ∗, ψ∗) satisfy

π∗(x, y) = Mf (V (x, y; ϕ, ψ))σ(x)τ (y)

where Mf (x, y) = f ∗(cid:48)(λ−1v).

For this reason, we focus in this work on f -Divergence-based regularizers. Where it is clear, we will
drop subscripts on regularizer H(π) and the so-called compatibility function M (v) and we will omit
the dual variable arguments of V (x, y). The speciﬁc form of these terms for KL regularization, χ2
regularization, and a variety of other regularizers may be found in Section A of the Appendix.

4

2.2 Langevin Sampling and Score Based Generative Modeling

Given access to optimal dual variables ϕ∗(x), ψ∗(y), it is easy to evaluate the density of the
corresponding optimal coupling according to Proposition 2.4. To generate samples distributed
according to this coupling, we apply Langevin Sampling. The key quantity used in Langevin
sampling of a generic (possibly unnormalized) probability measure p(x) is its score function, given
by ∇x log p(x) for x ∈ X . The algorithm is an iterative Monte Carlo method which generates
approximate samples ˜xt by iterating the map

˜xt = ˜xt−1 + (cid:15)∇x log p(˜xt−1) +

√

2(cid:15)zt

where (cid:15) > 0 is a step size parameter and where zt ∼ N (0, I) independently at each time step t ≥ 0.
In the limit (cid:15) → 0 and T → ∞, the samples ˜xT converge weakly in distribution to p(x). Song and
Ermon [24] introduce a method to estimate the score with a neural network sϑ(x), trained on samples
from p(x), so that it approximates sϑ(x) ≈ ∇x log p(x) for a given x ∈ X . To generate samples,
one may iterate Langevin dynamics with the score estimate in place of the true score.

To scale this method to high dimensional image datasets, Song and Ermon [24] propose an annealing
scheme which samples noised versions of p(x) as the noise is gradually reduced. One ﬁrst samples a
noised distribution p(x) ∗ N (0, τ1), at noise level τ1. The noisy samples, which are presumed to lie
near high density regions of p(x), are used to initialize additional rounds of Langevin dynamics at
diminishing noise levels τ2 > . . . > τN > 0. At the ﬁnal round, Annealed Langevin Sampling outputs
approximate samples according to the noiseless distribution. Song and Ermon [24] demonstrate that
Annealed Langevin Sampling (ALS) with score estimatation can be used to generate sample images
that rival the quality of popular generative modeling tools like GANs or VAEs.

3 Conditional Sampling of Regularized Optimal Transport Plans

Our approach can be split into two main steps. First, we approximate the density of the optimal
Sinkhorn coupling π∗(x, y) which minimizes Kλ(π) over the data. To do so, we apply the large-scale
stochastic dual approach introduced by Seguy et al. [22], which involves instantiating neural networks
ϕθ : X → R and ψθ : Y → R that serve as parametrized dual variables. We then maximize
Jλ(ϕθ, ψθ) with respect to θ via gradient descent and take the resulting parameters θ∗ and the
associated transport plan ˆπ(x, y) = M (V (x, y; ϕθ∗ , ψθ∗ ))σ(x)τ (y). This procedure is shown in
Algorithm 1. Note that when the dual problem is only approximately maximized, ˆπ need not be a
normalized density. We therefore call ˜π the pseudo-coupling which approximates the true Sinkhorn
coupling ˆπ.
After optimizing θ∗, we sample the conditional ˆπY |X=x(y) using Langevin dynamics. The score
estimator for the conditional distribution is,

∇y log ˆπY |X=x(y) = ∇y[log(M (V (x, y; ϕθ∗ , ψθ∗ ))σ(x)τ (y)) − log(σ(x))]

≈ ∇y log(M (V (x, y; ϕθ∗ , ψθ∗ ))) + sϑ(y).

We therefore approximate ∇y ˆπY |X=x(y) by directly differentiating log M (V (x, y)) using standard
automatic differentiation tools and adding the result to an unconditional score estimate sϑ(y). The
full Langevin sampling algorithm for general regularized optimal transport is shown in Algorithm 2.

We note that our method has the effect of biasing the Langevin iterates towards the region where
ˆπY |X=x is localized. This may be beneﬁcial for Langevin sampling, which enjoys exponentially
fast mixing when sampling log-concave distributions. In the supplementary material, we prove a
known result that for the entropy regularized problem given in Proposition 2.3: the compatibility
λ (ϕ∗(x) + ψ∗(y) − c(x, y))(cid:1) is log-concave with respect to y. For
M (V (x, y; ϕ∗, ψ∗)) = 1
λ → 0, this localizes around the optimal transport of x, T (x), and so heuristically should lead to
faster mixing.

e exp (cid:0) 1

4 Theoretical Analysis

In principle, the empirical setting of Deﬁnition 2.1 poses an unconstrained optimization problem
over R|X | × R|Y|, which could be optimized directly by gradient descent on vectors (ϕ, ψ). The

5

Algorithm 1 Density Estimation.

Algorithm 2 SCONES Sampling Procedure

Input: Step size γ, batch size m
Input: Nets ϕθ1 , ψθ2 .
Input: Datasets σ, τ . Time steps T > 0.
Output: Trained ϕθ∗
for t = 1 . . . T . do

1 , ψθ∗
2 .

Sample X1, . . . , Xm ∼ σ,
and Y1, . . . , Ym ∼ τ .
Stochastic gradient update ϕθ1, ψθ2:
∆1 ←

∇θ1 [ϕθ1(Xi) − H ∗(V (Xi, Yj))].

∇θ2 [ψθ2(Yj) − H ∗(V (Xi, Yj))].

m
(cid:80)
i,j=1
m
(cid:80)
i,j=1
θ1 ← θ1 + γ∆1.
θ2 ← θ2 + γ∆2.

∆2 ←

end for
Output parameters {θ1, θ2}.

Input: Noise levels τ1 > . . . > τN .
Input: Dual vars. ˜ϕ(x), ˜ψ(y). Source x ∈ X .
Input: Time steps T > 0. Step size (cid:15) > 0.
Output: Data sample ˜y ∼ πY |X=x(y).
Initialize ˜y1,0 ∼ N (0, I).
for τi, i = 1 . . . N do
for t = 1 . . . T . do

Sample z ∼ N (0, τi).
Compute score update:
∆s ← sϑ(˜yi,t−1).
∆π ← ∇y log M (V (x, ˜yi,t−1; ˜ϕ, ˜ψ)).
√
˜xi,t = ˜xi,t−1 + ((cid:15)/2)(∆s + ∆π) +

(cid:15)z.

end for
Initialize ˜xi+1,0 = ˜xi,T .

end for
Output sample ˜xN,T .

point of a more expensive neural network parametrization ϕθ, ψθ is to learn a continuous distribution
that agrees with the empirical optimal transport plan between discrete σ, τ , and that generalizes to a
continuous space containing X × Y. By training ϕθ, ψθ, we approximate the underlying continuous
data distribution up to optimization error and up to statistical estimation error between the empirical
coupling and the population coupling. In the present section, we justify this approach by proving
convergence of Algorithm 1 to the global maximizer of Jλ(ϕ, ψ), under assumptions of large network
width, along with a quantitative bound on optimization error. In Section B.1 of the Appendix, we
provide a cursory analysis of rates of statistical estimation of entropy regularized Sinkhorn couplings.
We make the following main assumptions on the neural networks ϕθ and ψθ.
Assumption 4.1 (Approximate Linearity). Let fθ(x) be a neural network with parameters θ ∈ Θ,
where Θ is a set of feasible weights, for example those reachable by gradient descent. Fix a dataset
i=1 and let Kθ ∈ RN ×N be the Gram matrix of coordinates [Kθ]ij = (cid:104)∇θfθ(Xi), ∇θfθ(Xj)(cid:105).
{Xi}N
Then fθ(x) must satisfy,

1. There exists R (cid:29) 0 so that Θ ⊆ B(0, R), where B(0, R) is the Euclidean ball of radius R.

2. There exist ρM > ρm > 0 such that for θ ∈ Θ,

ρM ≥ λmax(Kθ) ≥ λmin(Kθ) ≥ ρm > 0.

3. For θ ∈ Θ and for all data points {Xi}N

in spectral norm: (cid:107)D2
regularization λ.

θfθ(Xi)(cid:107) ≤ ρM
Ch

i=1, the Hessian matrix D2

θfθ(Xi) is bounded
, where Ch (cid:29) 0 depends only on R, N , and the

The dependencies of Ch are made clear in the Supplemental Materials, Section B. The quantity Kθ is
called the neural tangent kernel (NTK) associated with the network fθ(x). It has been shown for
a variety of nets that, at sufﬁciently large width, the NTK is well conditioned and nearly constant
on the set of weights reachable by gradient descent on a convex objective function [16, 6, 7]. For
instance, fully-connected networks with smooth and Lipschitz-continuous activations fall into this
class and hence satisfy Assumption 4.1 when the width of all layers is sufﬁciently large [16].

First, we show in Theorem 4.2 that when ϕ, ψ are parametrized by neural networks satisfying
Assumption 4.1, gradient descent converges to global maximizers of the dual objective. This provides
additional justiﬁcation for the large-scale approach of Seguy et al. [22].
Theorem 4.2 (Optimizing Neural Nets). Suppose Jλ(ϕ, ψ) is 1
ψθ be neural networks satisfying Assumption 4.1 for the dataset {(xi, yi)}N
Then gradient descent of Jλ(ϕθ, ψθ) with respect to θ at learning rate η = λ
2ρM

s -strongly smooth in l∞ norm. Let ϕθ,

i=1, N = |X | · |Y|.

converges to an

(cid:15)-approximate global maximizer of Jλ in at most

(cid:17)

(cid:16) 2κR2
s

(cid:15)−1 iterations, where κ = ρM
ρm

.

6

Figure 3: Comparison of Barycentric Projection [22] to SCONES for optimal transport between USPS
and MNIST datasets of handwritten digits. (Left) Transporting MNIST to USPS. (Right) Transporting
USPS to MNIST. Here, we show transportation of the χ2 regularized problem at λ = 0.001.

Given outputs ˆϕ, ˆψ of Algorithm 1, we may assume by Theorem 4.2 that the networks are (cid:15)-
approximate global maximizers of Jλ(ϕ, ψ). Due to λα-strong convexity of the primal objective, the
optimization error (cid:15) bounds the distance of the underlying pseudo-plan ˆπ from the true global extrema.
We make this bound concrete in Theorem 4.3, which guarantees that approximately maximizing
Jλ(ϕ, ψ) is sufﬁcient to produce a close approximation of the true empirical Sinkhorn coupling.
Theorem 4.3 (Stability of the OT Problem). Suppose Kλ(π) is s-strongly convex in l1 norm and
let L(ϕ, ψ, π) be the Lagrangian of the regularized optimal transport problem. For ˆϕ, ˆψ which are
(cid:15)-approximate maximizers of Jλ(ϕ, ψ), the pseudo-plan ˆπ = Mf (V (x, y; ˆϕ, ˆψ))σ(x)τ (y) satisﬁes

|ˆπ − π∗|1 ≤

(cid:114) 2(cid:15)
s

≤

1
s

(cid:12)
(cid:12)
(cid:12)∇ˆπL( ˆϕ, ˆψ, ˆπ)
(cid:12)
(cid:12)
(cid:12)1

.

Theorem 4.3 guarantees that if one can approximately optimize the dual objective using Algorithm
1, then the corresponding coupling ˆπ is close in l1 norm to the true optimal transport coupling.
This approximation guarantee justiﬁes the choice to draw samples ˜πY |X=x as an approximation to
sampling πY |X=x instead. Both Theorems 4.2 and 4.3 are proven in Section B of the Appendix.

5 Experiments

Our main point of comparison is the barycentric projection method proposed by Seguy et al. [22],
which trains a neural network Tθ : X → Y to map source data to target data by optimizing the
EπY |X=x [|Tθ(x) − Y |2]. For transportation experiments between USPS [19]
objective θ := arg minθ
and MNIST [13] datasets, we scale both datasets to 16px and parametrize the dual variables and
barycentric projections by fully connected ReLU networks. We train score estimators for MNIST and
USPS at 16px resolution using the method and architecture of [24]. For transportation experiments
using CelebA [17], dual variables ϕ, ψ are parametrized as ReLU FCNs with 8, 2048-dimensional
hidden layers. Both the barycentric projection and the score estimators use the U-Net based image-
to-image architecture introduced in [24]. Numerical hyperparameters like learning rates, optimizer
parameters, and annealing schedules, along with additional details of our neural network architectures,
are tabulated in Section C of the Appendix.

5.1 Optimal Transportation of Image Data

We show in Figure 3 a qualitative plot of SCONES samples on transportation between MNIST and
USPS digits. We also show in Section 1, Figure 1 a qualitative plot of transportation of CelebA
images. Because barycentric projection averages πY |X=x, output images are blurred and show visible
mixing of multiple digits. By directly sampling the optimal transport plan, SCONES can separate
these modes and generate more realistic images.

At low regularization levels, Algorithm 1 becomes more expensive and can become numerically
unstable. As shown in Figures 3 and 1, SCONES can be used to sample the Sinkhorn coupling in
intermediate regularization regimes, where optimal transport has a nontrivial effect despite πY |X=x
not concentrating on a single image.

7

SourceSCONES SamplesBPSourceSCONES SamplesBPKL regularization,
λ = 0.1

λ = 0.01

λ = 0.005

χ2 regularization,
λ = 0.1

λ = 0.01

λ = 0.001

35.59

35.77

43.80

25.84

25.64

25.59

193.92

230.85

228.78

190.10

216.54

212.72

36.62

34.84

43.99

25.51

25.65

27.88

195.64

217.24

217.67

188.29

219.96

214.90

SCONES,
Super-res.
Bary. Proj.,
Super-res.

SCONES,
Identity
Bary. Proj.,
Identity

Table 1: FID metric of samples generated by barycentric projection and SCONES, computed on
n = 5000 samples from each model. For comparison to unregularized OT methods, we also trained a
Wasserstein-2 GAN (W2 GAN) [14] and a Wasserstein-2 Generative Network (W2 Gen) [11]. W2
GAN achieves FIDs 55.77 on the super-res. task and 32.617 on the identity task. W2 Gen achieves
FIDs 32.80 on the super-res. task and 20.57 on the identity task.

To quantitatively assess the quality of images
generated by SCONES, we compute the FID
scores of generated CelebA images on two op-
timal transport problems: transporting 2x down-
sampled CelebA images to CelebA (the ‘Super-
res.’ task) and transporting CelebA to CelebA
(the ’Identity’ task) for a variety of regulariza-
tion parameters. The FID score is a popular
measurement of sample quality for image gen-
erative models and it is a proxy for agreement
between the distribution of SCONES samples
of the marginal πY (y) and the true distribution
τ (y). In both cases, we partition CelebA into
two datasets of equal size and optimize Algo-
rithm 1 using separated partitions as source and
target data, resizing the source data in the super-
resolution task. As shown in Table 1, SCONES
has a signiﬁcantly lower FID score than samples
generated by barycentric projection. However,
under ideal tuning, the unconditional score net-
work generates CelebA samples with FID score
10.23 [24], so there is some cost in sample qual-
ity incurred when using SCONES.

5.2 Sampling Synthetic Data

To compare SCONES to a ground truth Sinkhorn
coupling in a continuous setting, we con-
sider entropy regularized optimal transport be-
tween Gaussian measures on Rd. Given σ =
N (µ1, Σ1) and τ = N (µ2, Σ2), the Sinkhorn
coupling of σ, τ is itself a Gaussian mea-
sure and it can be written in closed form in
terms of the regularization λ and the means
and covariances of σ, τ [8].
In dimensions
d ∈ {2, 16, 54, 128, 256}, we consider Σ1, Σ2
whose eigenvectors are uniform random (i.e.
drawn from the Haar measure on SO(d)) and
whose eigenvalues are sampled uniform i.i.d.

Figure 4: Entropy regularized, λ = 2, L2 cost
SCONES and BP samples on transportation from a
unit Gaussian source distribution to the Swiss Roll
target distribution. For many samples, the Barycen-
tric average lies off the manifold of high target
density, whereas SCONES can separate multiple
modes of the conditional coupling and correctly
recover the target distribution.

8

11.014.511.014.5SCONES Samples11.014.511.014.5Barycentric Projectiond = 2

d = 16

d = 64

d = 128

d = 256

SCONES

0.025 ± 0.0014

0.52 ± 0.0086

1.2 ± 0.014

1.4 ± 0.066

2.0 ± 0.047

BP

7.1 ± 0.13

35 ± 0.23

42 ± 0.14

41 ± 0.088

41 ± 0.098

Table 2: Comparison of SCONES to BP on KL-regularized optimal transport between random
high-dimensional gaussians. In each cell, we report the average BW-UVP between a sample empirical
covariance and the analytical solution. We report the average over n = 10 independent random
source, target Gaussians and the standard error of the mean.

from [1, 10]. In all cases, we set means µ1, µ2 equal to zero and choose regularization λ = 2d. In the
Gaussian setting, E[(cid:107)x − y(cid:107)2
2] is of order d, so this choice of scaling ensures a fair comparison across
problem dimensions by ﬁxing the relative magnitudes of the cost and regularization terms.

We evaluate performance on this task using the Bures-Wasserstein Unexplained Variance Percentage
[4], BW-UV(ˆπ, πλ), where πλ is the closed form solution given by Janati et al. [8] and where
ˆπ is the joint empirical covariance of k = 10000 samples (x, y) ∼ π generated using either
SCONES or Barycentric Projection. We train SCONES according to Algorithm 1 and generate
samples according to Algorithm 2. In place of a score estimate, we use the ground truth target score
∇y log τ (y) = Σ−1
2 (y − µ2) and omit annealing. We compare SCONES samples to the true solution
in the BW-UVP metric [4] which is measured on a scale from 0 to 100, lower is better. We report
BW-UVP(ˆπ, πλ) where ˆπ is a 2d-by-2d joint empirical covariance of SCONES samples (x, y) ∼ ˆπ
or of BP samples (x, Tθ(x)), x ∼ σ and πλ is the closed-form covariance.

6 Discussion and Future Work

We introduce and analyze the SCONES method for learning and sampling large-scale optimal trans-
port plans. Our method takes the form of a conditional sampling problem for which the conditional
score decomposes naturally into a prior, unconditional score ∇y log τ (y) and a “compatibility term”
∇y log M (V (x, y)). This decomposition illustrates a key beneﬁt of SCONES: one score network
may re-used to cheaply transport many source distributions to the same target. In contrast, learned
forward-model-based transportation maps require an expensive training procedure for each distinct
pair of source and target distribution. This beneﬁt comes in exchange of increased computational
cost of iterative sampling. For example, generating 1000 samples requires roughly 3 hours using
one NVIDIA 2080 Ti GPU. The cost to sample score-based models may fall with future engineering
advances, but iterative sampling intrinsically require multiple forward pass evaluations of the score
estimator as opposed to a single evaluation of a learned transportation mapping.

There is much future work to be done. First, we study only simple fully connected ReLU networks as
parametrizations of the dual variables. Interestingly, we observe that under L2 transportation cost,
parametrization by multi-layer convolutional networks perform equally or worse than their FCN
counterparts when optimizing Algorithm 1. One explanation may be the permutation invariance of L2
cost: applying a permutation of coordinates to the source and target distribution does not change the
optimal objective value and the optimal coupling is simply conjugated by a coordinate permutation.
As a consequence, the optimal coupling may depend non-locally on input data coordinates, violating
the inductive biases of localized convolutional ﬁlters. Understanding which network parametrizations
or inductive biases are best for a particular choice of transportation cost, source distribution, and
target distribution, is one direction for future investigation.

Second, it remains to explore whether there is a potential synergistic effect between Langevin
sampling and optimal transport. Heuristically, as λ → 0 the conditional plan πY |X=x concentrates
around the transport image of x, which should improve the mixing time required by Langevin
dynamics to explore high density regions of space. In Section B of the Appendix, we prove a
known result, that the entropy regularized L2 cost compatibility term M (V (x, y)) = eV (x,y)/λ is a
log-concave function of y for ﬁxed x. It the target distribution is itself log-concave, the conditional
coupling πY |X=x is also log-concanve and hence Langevin sampling enjoys exponentially fast mixing
time. However, more work is required to understand the impacts of non-log-concavity of the target
and of optimization errors when learning the compatibility and score functions in practice. We
look forward to future developments on these and other aspects of large-scale regularized optimal
transport.

9

Acknowledgments and Disclosure of Funding

M.D. acknowledges funding from Northeastern University’s Undergraduate Research & Fellowships
ofﬁce and the Goldwater Award. P.H. was supported in part by NSF awards 2053448, 2022205, and
1848087.

References

[1] Jean-David Benamou and Mélanie Martinet. Capacity constrained entropic optimal transport,

sinkhorn saturated domain out-summation and vanishing temperature. 2020.

[2] Mathieu Blondel, Vivien Seguy, and Antoine Rolet. Smooth and sparse optimal transport. In
Amos Storkey and Fernando Perez-Cruz, editors, Proceedings of the Twenty-First International
Conference on Artiﬁcial Intelligence and Statistics, volume 84 of Proceedings of Machine
Learning Research, page 880–889. PMLR, Apr 2018. URL http://proceedings.mlr.
press/v84/blondel18a.html.

[3] S. Boyd, S.P. Boyd, L. Vandenberghe, and Cambridge University Press. Convex Optimization.
Berichte über verteilte messysteme. Cambridge University Press, 2004. ISBN 978-0-521-83378-
3. URL https://books.google.com/books?id=mYm0bLd3fcoC.

[4] Yongxin Chen, Jiaojiao Fan, and Amirhossein Taghvaei. Scalable computations of wasser-
stein barycenter via input convex neural networks. In Marina Meila and Tong Zhang, editors,
Proceedings of the 38th International Conference on Machine Learning, volume 139 of Pro-
ceedings of Machine Learning Research, pages 1571–1581. PMLR, 18–24 Jul 2021. URL
https://proceedings.mlr.press/v139/chen21e.html.

[5] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport.

In
C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, ed-
itors, Advances in Neural Information Processing Systems, volume 26. Curran As-
sociates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/
af21d0c97db2e27e13572cbf59eb343d-Paper.pdf.

[6] Simon Du and Wei Hu. Width provably matters in optimization for deep linear neural networks.
In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International
Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research,
page 1655–1664. PMLR, Jun 2019. URL http://proceedings.mlr.press/v97/du19a.
html.

[7] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds
global minima of deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov,
editors, Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, page 1675–1685. PMLR, Jun 2019. URL
http://proceedings.mlr.press/v97/du19c.html.

[8] Hicham Janati, Boris Muzellec, Gabriel Peyré, and Marco Cuturi. Entropic optimal transport

between unbalanced gaussian measures has a closed form. In Neurips 2020, 2020.

[9] Alexia Jolicoeur-Martineau, Rémi Piché-Taillefer, Ioannis Mitliagkas, and Remi Tachet des
Combes. Adversarial score matching and improved sampling for image generation. In Inter-
national Conference on Learning Representations, 2021. URL https://openreview.net/
forum?id=eLfqMl3z3lq.

[10] Sham M Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. On the duality of strong convexity

and strong smoothness: Learning applications and matrix regularization. page 10.

[11] Alexander Korotin, Vage Egiazarian, Arip Asadulaev, Alexander Saﬁn, and Evgeny Burnaev.
Wasserstein-2 generative networks. In International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=bEoxzW_EXsa.

[12] Wouter M. Kouw and Marco Loog. An introduction to domain adaptation and transfer learning.
arXiv:1812.11806 [cs, stat], Jan 2019. URL http://arxiv.org/abs/1812.11806. arXiv:
1812.11806.

10

[13] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs

[Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.

[14] Jacob Leygonie, Jennifer She, Amjad Almahairi, Sai Rajeswar, and Aaron Courville. Adversarial

computation of optimal transport maps, 2019.

[15] Lingxiao Li, Aude Genevay, Mikhail Yurochkin, and Justin M Solomon. Continuous regularized
wasserstein barycenters. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin,
editors, Advances in Neural Information Processing Systems, volume 33, page 17755–17765.
Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/cdf1035c34ec380218a8cc9a43d438f9-Paper.pdf.

[16] Chaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linear models: when
and why the tangent kernel is constant. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,
and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, page
15954–15964. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/
paper/2020/file/b7ae8fecf15b8b6c3c69eceae636d203-Paper.pdf.

[17] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the
wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.

[18] Giulia Luise, Saverio Salzo, Massimiliano Pontil, and Carlo Ciliberto. Sinkhorn barycenters with
free support via frank-wolfe algorithm. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-
Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems,
volume 32, page 9322–9333. Curran Associates, Inc., 2019. URL https://proceedings.
neurips.cc/paper/2019/file/9f96f36b7aae3b1ff847c26ac94c604e-Paper.pdf.

[19] O. Matan, R. Kiang, C. E. Stenard, B. E. Boser,

J. Denker,

D. Henderson, R. Howard, W. Hubbard, L.
recognition using neural network architectures.
ten character
/paper/Handwritten-character-recognition-using-neural-Matan-Kiang/
8f2b909fa1aad7e9f13603d721ff953325a4f97d.

and et al.

Jackel,

1990.

J. Denker,
Handwrit-
URL

[20] James Melbourne. Strongly convex divergences. Entropy, 22(1111):1327, Nov 2020. doi:

10.3390/e22111327.

[21] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural
samplers using variational divergence minimization. Advances in Neural Information Pro-
cessing Systems, 29, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/
cedebb6e872f539bef8c3f919874e9d7-Abstract.html.

[22] Vivien Seguy, Bharath Bhushan Damodaran, Remi Flamary, Nicolas Courty, Antoine Rolet,
and Mathieu Blondel. Large scale optimal transport and mapping estimation. In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=B1zlp1bRW.

[23] Richard Sinkhorn. A relationship between arbitrary positive matrices and stochastic matrices.
Canadian Journal of Mathematics, 18:303–306, 1966. ISSN 0008-414X, 1496-4279. doi:
10.4153/CJM-1966-033-9.

[24] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-
Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual,
2020.

[25] Matthew Thorpe. Introduction to optimal transport. page 56.

[26] C. Villani. Topics in Optimal Transportation. Graduate studies in mathematics. American
Mathematical Society, 2003. ISBN 978-0-8218-3312-4. URL https://books.google.com/
books?id=R_nWqjq89oEC.

11

A Regularizing Optimal Transport with f -Divergences

Name

f (v)

Kullback-Leibler

v log(v)

− log(v)
(v − 1)2
√

Reverse KL
Pearson χ2

Squared Hellinger

Jensen-Shannon

GAN

f ∗(v)

f ∗(cid:48)

Dom(f ∗(v))

v ) − 1

exp(v − 1)
log(− 1
v2
4 + v
v
1−v
ex
2−ex

exp(v − 1)
− 1
v
v
2 + 1
(1 − v)−2
ex−2 + x − log(2 − ex)
(e−y − 1)−1

2x

v ∈ R

v < 0
v ∈ R

v < 1

v < log(2)

v < 0

v − 1)2

(
−(v + 1) log( 1+v
v log(v) − (v + 1) log(v + 1) −v − log(e−v − 1)

2 ) + v log v

Table 3: A list of f -Divergences, their Fenchel-Legendre conjugates, and the derivative of their
conjugates. These functions determine the corresponding dual regularizers H ∗
f (v) and compatibility
functions Mf (v). We take deﬁnitions of each divergence from [21]. Note that there are many
equivalent formulations as each f (v) is deﬁned only up to additive c(t − 1), c ∈ R, and the resulting
optimization problems are deﬁned only up to shifting and scaling the objective.

Here are some general properties of f -Divergences which are also used in Section B. We provide
examples of f -Divergences in Table 3. The speciﬁc forms of H ∗
f (v) and Mf (v) are determined
by f (v), f ∗(v), and f ∗(cid:48)(v), which can in turn be used to formulate Algorithms 1 and 2 for each
divergence.
Deﬁnition A.1 (f -Divergences). Let f : R → R be convex with f (1) = 0 and let p, q be probability
measures such that p is absolutely continuous with respect to q. The corresponding f -Divergence is
deﬁned Df (p||q) = Eq[f ( dp(x)
Proposition A.2 (Strong Convexity of Df ). Let X be a countable compact metric space. Fix
q ∈ M+(X ) and let Pq(X ) be the set of probability measures on X that are absolutely continuous
with respect to q and which have bounded density over X . Let f : R → R be α-strongly convex
with corresponding f -Divergence Df (p||q). Then, the function Hf (p) := Df (p||q) deﬁned over
p ∈ Pq(X ) is α-strongly convex in 1-norm: for p0, p1 ∈ Pq(X ),

dq(x) is the Radon-Nikodym derivative of p w.r.t. q.

dq(x) )] where dp(x)

Hf (p1) ≥ Hf (p0) + (cid:104)∇pHf (p0), p1 − p0(cid:105) +

α
2

|p1 − p0|2
1.

(2)

Proof. Deﬁne the measure pt = tp1 + (1 − t)p0. Then Hf satisﬁes the following convexity inequality
(Melbourne [20], Proposition 2).

Hf (pt) ≤ tHf (p1) + (1 − t)Hf (p0) − α (cid:0)t|p1 − pt|2

TV + (1 − t)|p0 − pt|2
TV

(cid:1)

By assumption that X is countable, |p − q|TV = 1

2 |p − q|1. It follows that,

Hf (p1) ≥ Hf (p0) +

≥ Hf (p0) +

Hf (p0 + t(p1 − p0)) − Hf (p0)
t
Hf (p0 + t(p1 − p0)) − Hf (p0)
t

+

+

α
2
α
2

and, taking the limit t → 0, the inequality (2) follows.

(cid:0)|p1 − pt|2

1 + (t−1 − 1)|p0 − pt|2
1

(cid:1)

|p1 − pt|2
1

For the purposes of solving empirical regularized optimal transport, the technical conditions of
Proposition A.2 hold. Additionally, note that α-strong convexity of f is sufﬁcient but not necessary
for strong convexity of Hf . For example, entropy regularization uses fKL(v) = v log(v) which is
not strongly convex over its domain, R+, but which yields a regularizer HKL(p) = KL(p||q) that is
1-strongly convex in l1 norm when q is uniform. This follows from Pinksker’s inequality as shown
in [22]. Also, if f is α-strongly convex over a subinterval [a, b] of its domain, then Proposition A.2
holds under the additional assumption that a ≤ dp(x)

dq(x) (x) ≤ b uniformly over x ∈ X .

12

B Proofs

For convenience, we repeat the main assumptions and statements of theorems alongside their proofs.
First, we prove the following properties about f -divergences.
Proposition, 2.4 – Regularization with f -Divergences. Consider the empirical setting of Deﬁnition
2.1. Let f (v) : R → R be a differentiable α-strongly convex function with convex conjugate f ∗(v).
Set f ∗(cid:48)(v) = ∂vf ∗(v). Deﬁne the violation function V (x, y; ϕ, ψ) = ϕ(x) + ψ(y) − c(x, y). Then,

1. The Df regularized primal problem Kλ(π) is λα-strongly convex in l1 norm. With respect
to dual variables ϕ ∈ R|X | and ψ ∈ R|Y|, the dual problem Jλ(ϕ, ψ) is concave, uncon-
strained, and 1
λα -strongly smooth in l∞ norm. Strong duality holds: Kλ(π) ≥ Jλ(ϕ, ψ) for
all π, ϕ, ψ, with equality for some triple π∗, ϕ∗, ψ∗.

2. Jλ(ϕ, ψ) takes the form

Jλ(ϕ, ψ) = Eσ[ϕ(x)] + Eτ [ψ(y)] − Eσ×τ [H ∗

f (V (x, y; ϕ, ψ))]

where H ∗

f (v) = λf ∗(λ−1v).

3. The optimal solutions (π∗, ϕ∗, ψ∗) satisfy

π∗(x, y) = Mf (V (x, y; ϕ, ψ))σ(x)τ (y)

where Mf (x, y) = f ∗(cid:48)(λ−1v).

Proof. By assumption that f is differentiable, Kλ(π) is continuous and differentiable with respect to
π ∈ M+(X × Y). By Proposition A.2, it is λα-strongly convex in l1 norm. By the Fenchel-Moreau
theorem, Kλ(π) therefore has a unique minimizer π∗ satisfying strong duality, and by [10, Theorem
6], the dual problem is 1

λα -strongly smooth in l∞ norm.

The primal and dual are related by the Lagrangian L(π, ϕ, ψ),

L(ϕ, ψ, π) = Eπ[c(x, y)] + λHf (π) + Eσ[ϕ(x)] − Eπ[ϕ(x)] + Eτ [ϕ(y)] − Eπ[ψ(y)]

(3)

which has Kλ(π) = maxϕ,ψ L(ϕ, ψ, π) and Jλ(ϕ, ψ) = minπ L(ϕ, ψ, π). In the empirical setting,
π, σ, τ may be written as ﬁnite dimensional vectors with coordinates πx,y, σx, τy for x, y ∈ X × Y.
Minimizing the π terms of Jλ,

(cid:26)

min
π∈M(X ×Y)

Eπ[c(x, y) − ϕ(x) − ψ(y)] + λEσ×τ

(cid:20)

f

(cid:18) dπ(x, y)

(cid:19)(cid:21)(cid:27)

dσ(x)dτ (y)

=

=

(cid:88)

x,y∈X ×Y
(cid:88)

x,y∈X ×Y

(cid:26)

− max
πx,y≥0

πx,y · (ϕ(x) + ψ(y) − c(x, y)) − λσxτyf

(cid:19)(cid:27)

(cid:18) πx,y
σxτy

−h∗

x,y(ϕ(x) + ψ(y) − c(x, y))

where h∗
convex f (p), it is true that [λf (p)]∗(v) = λf ∗(λ−1v) [3, Chapter 3]. Applying twice,

x,y is the convex conjugate of (λσxτy) · f (p/(σxτy)) w.r.t. the argument p. For general

[(λσxτy) · f (p/(σxτy))]∗(v) = λ[(σxτy)f (p/(σxτy))]∗(λ−1v) = (λσxτy) · f ∗(v/λ)

so that

min
π∈M+(X ×Y)

Eπ[c(x, y) − ϕ(x) − ψ(y)] + λEσ×τ

(cid:20)

f

(cid:18) dπ(x, y)

(cid:19)(cid:21)

dσ(x)dτ (y)

(cid:88)

=

σxτyλf ∗(λ−1v)

x,y∈X ×Y
= − Eσ×τ [H ∗

f (V (x, y; ϕ, ψ))]

for H ∗

f (v) = λf ∗(λ−1v). The claimed form of Jλ(ϕ, ψ) follows.

13

Additionally, for general convex f (p), it is true that ∂vf ∗(v) = arg maxp {(cid:104)v, p(cid:105) − f (p)}, [3,
Chapter 3]. For ϕ∗, ψ∗ maximizing Jλ(ϕ, ψ), it follows by strong duality that

π∗
x,y = arg min

L(ϕ∗, ψ∗, π)

π∈M+(X ×Y)
= ∇V Eσ×τ [H ∗

f (V (x, y; ϕ∗, ψ∗))] = Mf (V (x, y; ϕ∗, ψ∗))σxτy.

as claimed.

We proceed to proofs of the theorems stated in Section 4.
Assumption, 4.1 – Approximate Linearity. Let fθ(x) be a neural network with parameters θ ∈ Θ,
where Θ is a set of feasible weights, for example those reachable by gradient descent. Fix a dataset
i=1 and let Kθ ∈ RN ×N be the Gram matrix of coordinates [Kθ]ij = (cid:104)∇θfθ(Xi), ∇θfθ(Xj)(cid:105).
{Xi}N
Then fθ(x) must satisfy,

1. There exists R (cid:29) 0 so that Θ ⊆ B(0, R), where B(0, R) is the Euclidean ball of radius R.

2. There exist ρM > ρm > 0 such that for θ ∈ Θ,

ρM ≥ λmax(Kθ) ≥ λmin(Kθ) ≥ ρm > 0.

3. For θ ∈ Θ and for all data points {Xi}N

i=1, the Hessian matrix D2

θfθ(xi) is bounded in

spectral norm:

(cid:107)D2

θfθ(xi)(cid:107) ≤

ρM
Ch

where Ch (cid:29) 0 depends only on R, N , and the regularization λ.

The constant Ch may depend on the dataset size N , the upper bound of ρM for eigenvalues of the
NTK, the regularization parameter λ, and it may also depend indirectly on the bound R.
Theorem, 4.2 – Optimizing Neural Nets. Suppose Jλ(ϕ, ψ) is 1
ϕθ, ψθ be neural networks satisfying Assumption 4.1 for the dataset {(xi, yi)}N
Then gradient descent of Jλ(ϕθ, ψθ) with respect to θ at learning rate η = λ
2ρM

s -strongly smooth in l∞ norm. Let
i=1, N = |X | · |Y|.

converges to an

(cid:15)-approximate global maximizer of Jλ in at most

(cid:17)

(cid:16) 2κR2
s

(cid:15)−1 iterations, where κ = ρM
ρm

.

Proof. For indices i, let Sθi = (ϕθi , ψθi) so that Assumption 4.1 applies with Sθ in place of fθ.
Lemma B.1 (Smoothness). Jλ(Sθ) is 2ρM

s -strongly smooth in l2 norm with respect to θ:

Jλ(Sθ2 ) ≤ Jλ(Sθ1) + (cid:104)∇θJλ(Sθ1), Sθ2 − Sθ1(cid:105) +

ρM
λ

(cid:107)θ2 − θ1(cid:107)2
2.

Proof. It is assumed that Jλ(S) is ( 1
Note that ( 1
( 1
s , lq)-strong smoothness for 2 ≤ q ≤ ∞.

s , l∞)-strongly smooth and that Kλ(π) is (s, l1)-strongly convex.
s , l2)-strong smoothness is weakest in the sense that it is implied via norm equivalence by

Jλ(S2) ≤ Jλ(S1) + (cid:104)∇SJλ(S1), S2 − S1(cid:105) +

=⇒ Jλ(S2) ≤ Jλ(S1) + (cid:104)∇SJλ(S1), S2 − S1(cid:105) +

1
2s
1
2s

| S2 − S1 |2
q

(cid:107)S2 − S1(cid:107)2
2

A symmetric property holds for (s, l2)-strong convexity of Kλ(π) which is implied by (s, lp)-strong
convexity, 1 ≤ p ≤ 2. By Assumption 4.1,

Jλ(Sθ2) − Jλ(Sθ1 ) − (cid:104)∇SJλ(Sθ1), Sθ2 − Sθ1(cid:105) ≤

1
2s

(cid:107)Sθ2 − Sθ1(cid:107)2

2 ≤

ρM
2s

(cid:107)θ2 − θ1(cid:107)2
2.

(4)

To establish smoothness, it remains to bound (cid:104)∇SJλ(Sθ1 ), Sθ2 − Sθ1 (cid:105). Set v = ∇SJλ(Sθ1 ) ∈ RN
and consider the ﬁrst-order Taylor expansion in θ of (cid:104)v, Sθ(cid:105) evaluated at θ = θ2. Applying Lagrange’s

14

form of the remainder, there exists 0 < c < 1 such that

(cid:104)v, Sθ2(cid:105) = (cid:104)v, Sθ1(cid:105) + (cid:104)v, J S
1
2

n
(cid:88)

+

i=1

θ (Sθ2 − Sθ1)(cid:105)

vi(θ2 − θ1)T [D2

θ(Sθ1(xi) + c(Sθ2(xi) − Sθ1(xi)))](θ2 − θ1)

and so by Cauchy-Schwartz,

(cid:104)v, Sθ2 − Sθ1(cid:105) ≤ (cid:104)v, J S

θ (Sθ2 − Sθ1)(cid:105) +

√

N (cid:107)v(cid:107)2(cid:107)θ2 − θ1(cid:107)2

2 ≤

ρM
2s

(cid:107)θ2 − θ1(cid:107)2
2.

(cid:107)D2
θ(cid:107)
2
√

The ﬁnal inequality follows by taking Ch ≥ λ
assumption that Θ ⊆ B(0, R). Plugging in v = ∇SJλ(Sθ1), we have

N supv (cid:107)v(cid:107)2. This supremum is bounded by

(cid:104)∇SJλ(Sθ1), Sθ2 − Sθ1(cid:105) ≤ (cid:104)∇SJλ(Sθ1), J S

θ (Sθ2 − Sθ1)(cid:105) +

(cid:107)θ2 − θ1(cid:107)2
2

= (cid:104)∇θJλ(Sθ1), θ2 − θ1(cid:105) +

ρM
2s

ρM
2s
(cid:107)θ2 − θ1(cid:107)2
2.

Returning to (4), we have

Jλ(Sθ2 ) − Jλ(Sθ1) ≤ (cid:104)∇θJλ(Sθ1), θ2 − θ1(cid:105) +

ρM
s

(cid:107)θ2 − θ1(cid:107)2
2.

from which Lemma B.1 follows.

Lemma B.2 (Gradient Descent). Gradient descent over the parameters θ with learning rate η = s
2ρM
T where κ = ρM
converges in T iterations to parameters θt satisfying Jλ(Sθt) − Jλ(S∗) ≤
ρm
is the condition number.

(cid:16) 2κR2
s

(cid:17) 1

Proof. Fix θ0 and set θt+1 = θt − η∇θJλ(Sθ). The step size η is chosen so that by Lemma B.1,
Jλ(St) − Jλ(St+1) ≥ s
2ρM

(cid:107)∇θJλ(Sθt)(cid:107)2
2.
By convexity, Jλ(S∗) ≥ Jλ(Sθt ) + (cid:104)∇SJλ(Sθt), S∗ − Sθt(cid:105), so that

(cid:107)∇θJλ(Sθt)(cid:107)2

2 ≥ ρm(cid:107)∇SJλ(Sθt)(cid:107)2

2 ≥ (Jλ(Sθt) − Jλ(S∗))2

(cid:18)

ρm
(cid:107)Sθt − S∗(cid:107)2
2

(cid:19)

.

Setting ∆t = Jλ(Sθt) − Jλ(S∗), this implies ∆t ≥ ∆t+1 + ∆2
t
(cid:104)

(cid:17)(cid:105)−1

(cid:16)

. The claim follows from (cid:107)Sθt − S∗(cid:107)2 < R.

T

sρm
2ρM (cid:107)Sθt −S∗(cid:107)2
2

(cid:16)

sρm
2ρM (cid:107)Sθt −S∗(cid:107)2
2

(cid:17)

and thus ∆t ≤

Theorem 4.2 follows immediately from Lemmas B.1 and B.2.

Theorem, 4.3 – Stability of Regularized OT Problem. Suppose Kλ(π) is s-strongly convex in l1
norm and let L(ϕ, ψ, π) be the Lagrangian of the regularized optimal transport problem. For ˆϕ, ˆψ
which are (cid:15)-approximate maximizers of Jλ(ϕ, ψ), the pseudo-plan ˆπ = Mf (V (x, y; ˆϕ, ˆψ))σ(x)τ (y)
satisﬁes

|ˆπ − π∗|1 ≤

(cid:114) 2(cid:15)
s

≤

1
s

(cid:12)
(cid:12)
(cid:12)∇ˆπL( ˆϕ, ˆψ, ˆπ)
(cid:12)
(cid:12)
(cid:12)1

.

Proof. For indices i, denote by Si the tuple (ϕi, ψi, πi). The regularized optimal transport problem
has Lagrangian L(ϕ, ψ, π) given by

L(ϕ, ψ, π) = Eπ[c(x, y)] + λHf (π) + Eσ[ϕ(x)] − Eπ[ϕ(x)] + Eτ [ϕ(y)] − Eπ[ψ(y)]

Because L(ϕ, ψ, π) is a sum of Kλ(π) and linear terms, the Lagrangian inherits s-strong convexity
w.r.t. the argument π:

L(S2) ≥ L(S1) + (cid:104)∇L(S1), S2 − S1(cid:105) +

s
2

|π2 − π1|2
1.

15

Letting S∗ = (ϕ∗, ψ∗, π∗) be the optimal solution and ˆS = ( ˆϕ, ˆψ, ˆπ) be an (cid:15)-approximation, it
follows that

(cid:15) ≥ L( ˆS) − L(S∗) ≥

s
2

|ˆπ − π∗|2

1 =⇒ |ˆπ − π∗| ≤

(cid:114) 2(cid:15)
s

.

Additionally, note that strong convexity implies a Polyak-Łojasiewicz (PL) inequality w.r.t. ˆπ.

(cid:16)

s

L( ˆS) − L(S∗)

(cid:17)

≤

1
2

|∇πL( ˆS)|2
1.

The second inequality follows from (5) and the PL inequality (6).

(5)

(6)

B.1 Statistical Estimation of Sinkhorn Plans

We consider consider estimating an entropy regularized OT plan when Y = X . Let ˆσ, ˆτ be empirical
distributions generated by drawing n ≥ 1 i.i.d. samples from σ, τ respectively. Let πλ
n be the
Sinkhorn plan between ˆσ and ˆτ at regularization λ, and let D := diam(X ). For simplicity, we also
assume that σ and τ are sub-Gaussian. We also assume that n is ﬁxed. Under these assumptions, we
will show that W1(πλ
The following result follows from Proposition E.4 and E.5 of of Luise et al. [18] and will be useful in
deriving the statistical error between πλ
n and πλ. This result characterizes fast statistical convergence
of the Sinkhorn potentials as long as the cost is sufﬁciently smooth.
Proposition B.3. Suppose that c ∈ Cs+1(X × X ). Then, for any σ, τ probability measures supported
on X , with probability at least 1 − τ ,

n, πλ) (cid:46) n−1/2.

(cid:107)v − vn(cid:107)∞, (cid:107)u − un(cid:107)∞ (cid:46) λe3D/λ log 1/τ

√

n

,

where (u, v) are the Sinkhorn potentials for σ, τ and (un, vn) are the Sinkhorn potentials for ˆσ, ˆτ .
Let πλ

n = Mnσnτn and πλ = M στ , We recall that

M (x, y) =

1
e

exp

Mn(x, y) =

1
e

exp

(cid:18) 1
λ
(cid:18) 1
λ

(ϕ(x) + ψ(y) − c(x, y))

,

(cid:19)

(ϕn(x) + ψn(y) − c(x, y))

(cid:19)

,

We note that M and Mn are uniformly bounded by e3D/λ [18] and M inherits smoothness properties
from ϕ, ψ, and c.

We can write (for some optimal, bounded, 1-Lipschitz fn)

W1(πλ

n, πλ) = |

≤ |

(cid:90)

(cid:90)

(cid:90)

fnπλ

n −

fnπλ|

fn(Hn − H)σnτn| + |

(cid:90)

fnH(σnτn − στ )|

≤ |fn|∞|Hn − H|∞ + |

(cid:90)

fnH(σnτn − στ )|.

(7)

If σ and τ are β2 subGaussian, then we can bound the second term with high probability:



P

|

1
n2

(cid:88)

(cid:88)

i

j

fn(Xi, Yj)H(Xi, Yj) − Eσ×τ fn(X, Y )H(X, Y )| > t


 < e−n2 t2
2β2 .

√

Setting t =

2 log(δ)β/n in this expression, we get that w.p. at least 1 − δ,

|

1
n2

(cid:88)

(cid:88)

i

j

fn(Xi, Yj)H(Xi, Yj) − Eσ×τ fn(X, Y )H(X, Y )| <

√

2β log δ
n

.

16

Now to bound the ﬁrst term in (7), we use the fact that fn is 1-Lipschitz and bounded by D. For
the optimal potentials ϕ and ψ in the original Sinkhorn problem for σ and τ , we use the result of
Proposition B.3 to yield

|Hn(x, y) − H(x, y)| =

(ϕn(x) + ψn(y) − c(x, y))

(ϕ(x) + ψ(y) − c(x, y))

(cid:19)

−

1
e

exp

(cid:18) 1
λ

(ϕ(x) + ψ(y) − c(x, y))

(cid:19) (cid:18)

1 − exp

(cid:18) ϕ(x) − ϕn(x)
λ

(cid:19)

exp

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:18) ψ(y) − ψn(y)
λ

(cid:19)(cid:19)(cid:12)
(cid:12)
)
(cid:12)
(cid:12)

=

exp

(cid:12)
1
(cid:12)
(cid:12)
e
(cid:12)
(cid:12)
1
(cid:12)
exp
(cid:12)
e
(cid:12)

(cid:18) 1
λ
(cid:18) 1
λ
(cid:46) e3D/λ|1 − e
(cid:46) e3D/λ
√
n
λ

.

2
√

λ

n |

Thus, putting this all together,

W1(πλ

n, πλ) (cid:46) D
√
n

+

1
n

.

Interestingly, the rate of estimation of the Sinkhorn plan breaks the curse of dimensionality. It must
be noted, however, that the exponential dependence of Proposition B.3 on λ−1 implies we can only
attain these fast rates in appropriately large regularization regimes.

B.2 Log-concavity of Sinkhorn Factor

The optimal entropy regularized Sinkhorn plan is given by

π∗(x, y) =

1
e

exp

(cid:18) 1
λ

(ϕ∗(x) + ψ∗(y) − c(x, y))

(cid:19)

σ(x)τ (y).

This implies that the conditional Sinkhorn density of Y |X is

π∗(y|x) =

1
e

exp

(cid:18) 1
λ

(ϕ∗(x) + ψ∗(y) − c(x, y))

(cid:19)

τ (y).

The optimal potentials satisfy ﬁxed point equations. In particular,

ψ∗(y) = −λ log

(cid:90)

(cid:20)

exp

−

1
λ

(c(x, y) − ϕ∗(x))

(cid:21)

dσ(x).

Using this result, one can prove the following lemma.
Lemma B.4 ([1]). For the cost (cid:107)x − y(cid:107)2, the map

h(y) = exp

(cid:18) 1
λ

(cid:0)ϕ∗(x) + ψ∗(y) − (cid:107)x − y(cid:107)2(cid:1)

(cid:19)

is log-concave.

Proof. The proof comes by differentiating the map. We calculate the gradient,

∇ log h(y) = −2

y − x
λ

+

2
λ

(cid:82) exp (cid:2)− 1

λ

(cid:0)(cid:107)x − y(cid:107)2 − ϕ∗(x)(cid:1)(cid:3) (y − x)dσ(x)
λ ((cid:107)x − y(cid:107)2 − ϕ∗(x))(cid:3) dσ(x)

(cid:82) exp (cid:2)− 1

17

and the Hessian,

∇2 log h(y) = −2
(cid:82) exp (cid:2)− 1

λ

I
λ
(cid:0)(cid:107)x − y(cid:107)2 − ϕ∗(x)(cid:1)(cid:3) (y − x)dσ(x) (cid:82) exp (cid:2)− 1

λ

(cid:82) exp (cid:2)− 1

((cid:82) exp (cid:2)− 1
(cid:0)(cid:107)x − y(cid:107)2 − ϕ∗(x)(cid:1)(cid:3) (y − x)(y − x)(cid:62)dσ(x)
λ ((cid:107)x − y(cid:107)2 − ϕ∗(x))(cid:3) dσ(x)

(cid:82) exp (cid:2)− 1

λ

λ ((cid:107)x − y(cid:107)2 − ϕ∗(x))(cid:3) dσ(x))2

+

4
λ2

−

4
λ2

(cid:0)(cid:107)x − y(cid:107)2 − ϕ∗(x)(cid:1)(cid:3) (y − x)(cid:62)dσ(x)

(cid:0)(cid:107)x − y(cid:107)2 − ϕ∗(x)(cid:1)(cid:3) dσ(x)
λ ((cid:107)x − y(cid:107)2 − ϕ∗(x))(cid:3) dσ(x)

λ

(cid:82) exp (cid:2)− 1
(cid:82) exp (cid:2)− 1
(cid:16)

−

+ 2I/λ

= −

4
λ2

λ

(cid:82) exp (cid:2)− 1

(cid:0)(cid:107)x − y(cid:107)2 − ϕ∗(x)(cid:1)(cid:3) (y − x)dσ(x) (cid:82) exp (cid:2)− 1

(cid:0)(cid:107)x − y(cid:107)2 − ϕ∗(x)(cid:1)(cid:3) (y − x)(cid:62)dσ(x)

((cid:82) exp (cid:2)− 1

λ ((cid:107)x − y(cid:107)2 − ϕ∗(x))(cid:3) dσ(x))2

λ

(cid:82) exp (cid:2)− 1

+

λ

(cid:0)(cid:107)x − y(cid:107)2 − ϕ∗(x)(cid:1)(cid:3) (y − x)(y − x)(cid:62)dσ(x)
λ ((cid:107)x − y(cid:107)2 − ϕ∗(x))(cid:3) dσ(x)

(cid:82) exp (cid:2)− 1

(cid:17)

In the last term, we recognize that

ρ(x) =

exp (cid:2)− 1

λ

(cid:0)(cid:107)x − y(cid:107)2 − ϕ∗(x)(cid:1)(cid:3)
λ ((cid:107)x − y(cid:107)2 − ϕ∗(x))(cid:3) dσ(x)

(cid:82) exp (cid:2)− 1

forms a valid density with respect to σ, and thus

where we take the covariance matrix of X − y with respect to the density ρdσ.

∇2 log h(y) = −

4
λ2 Covρdσ(X − y)

Suppose, for sake of argument, that τ (y) is α strongly log-concave, and the function h(y) is β strongly
log-concave. Then, πY |X=x ∝ h(y)τ (y), α + β strongly log-concave. In particular, standard results
on the mixing time of the Langevin diffusion implies that the diffusion for πY |X=x mixes faster
than the diffusion for the marginal τ alone. Also, as λ → 0, the function h(y) concentrates around
ϕOT (x) + ψOT (y) − (cid:107)x − y(cid:107)2, where ϕOT and ψOT are the optimal transport potentials.
In
particular, if there exists an optimal transport map between σ and τ , then h(y) concentrates around
the unregularized optimal transport image y = T (x).

C Experimental Details

C.1 Network Architectures

Our method integrates separate neural networks playing the roles of unconditional score estimator,
compatibility function, and barycentric projector. In our experiments each of these networks uses
one of two main architectures: a fully connected network with ReLU activations, and an image-to-
image architecture introduced by Song and Ermon [24] that is inspired by architectures for image
segmentation.

For the ﬁrst network type, we write “ReLU FCN, Sigmoid output, w0 → w1 → . . . → wk → wk+1,”
for integers wi ≥ 1, to indicate a k-hidden-layer fully connected network whose internal layers use
ReLU activations and whose output layer uses sigmoid activation. The hidden layers have dimension
w1, w2, . . . , wk and the network has input and output with dimension w0, wk+1 respectively.

For the second network type, we replicate the architectures listed in Song and Ermon [24, Appendix
B.1, Tables 2 and 3] and refer to them by name, for example “NCSN 322 px” or “NCSNv2 322 px.”

Our implementation of these experiments may be found in the supplementary code submission.

C.2

Image Sampling Parameter Sheets

MNIST ↔ USPS: details for qualitative transportation experiments between MNIST and USPS in
Figure 3 are given in Table 4.

18

CelebA, Blur-CelebA → CelebA: we sample 642 px CelebA images. The Blur-CelebA dataset
is composed of CelebA images which are ﬁrst resized to 322 px and then resized back to 642 px,
creating a blurred effect. The FID computations in Table 1 used a shared set of training parameters
given in Table 5. The sampling parameters for each FID computation are given in Table 6.

Synthetic Data: details for the synthetic data experiment shown in Figure 2 are given in Table 7.

Problem Aspect Hyperparameters

Numbers and details

Dataset

USPS [19]

Preprocessing

None

Source

Target

Score Estimator

Dataset

Preprocessing

Architecture

Loss

Optimization

Training

Architecture

Compatibility

Regularization

Optimization

Training

Architecture

Optimization

Training

Barycentric

Projection

Sampling

Annealing Schedule

Step size

Steps per noise level

MNIST [13]
Nearest neighbor resize to 162 px.
NCSN 322 px, applied as-is to 162 px images.
Denoising Score Matching
Adam, lr = 10−4, β1 = 0.9, β2 = 0.999.
No EMA of model parameters.

40000 training iterations,

128 samples per minibatch.

ReLU network with ReLU output activation,

256 → 1024 → 1024 → 1
χ2 Regularization, λ = 0.001.
Adam, lr = 10−6, β1 = 0.9, β2 = 0.999
5000 training iterations,

1000 samples per minibatch.

ReLU network with sigmoid output activation,

256 → 1024 → 1024 → 256.
Input pixels are scaled to [−1, 1] by x (cid:55)→ 2x − 1.
Adam, lr = 10−6, β1 = 0.9, β2 = 0.999
5000 training iterations,

1000 samples per minibatch.

7 noise levels decaying geometrically,
τ0 = 0.2154, . . . , τ6 = 0.01.
(cid:15) = 5 · 10−6
T = 20

Denoising? [9]
χ2 SoftPlus threshold α = 1000

Yes

Table 4: Data and model details for the USPS → MNIST qualitative experiment shown in Figure 3.
For MNIST → USPS, we use the same conﬁguration with source and target datasets swapped.

19

Problem Aspect Hyperparameters Numbers and details

Source

Dataset

Preprocessing

Target

Dataset

Score Estimator

Preprocessing

Architecture

Loss

Optimization

Training

Architecture

Compatibility

Regularization

Optimization

Training

Architecture

Optimization

Training

Barycentric

Projection

CelebA or Blur-CelebA [17]
1402 px center crop.
If Blur-CelebA: nearest neighbor resize to 322 px.
Nearest neighbor resize to 642 px.
Horizontal ﬂip with probability 0.5.

CelebA [17]
1402 px center crop.
Nearest neighbor resize to 642 px.
Horizontal ﬂip with probability 0.5.
NCSNv2 642 px.
Denoising Score Matching
Adam, lr = 10−4, β1 = 0.9, β2 = 0.999.
Parameter EMA at rate 0.999.

210000 training iterations,

128 samples per minibatch.

ReLU network with ReLU output activation,
3 · 642 → 2048 → . . . → 2048 → 1 (8 hidden layers).
Varies in χ2 reg., λ ∈ {0.1, 0.1, 0.001},
and KL reg., λ ∈ {0.1, 0.01, 0.005}.
Adam, lr = 10−6, β1 = 0.9, β2 = 0.999
5000 training iterations,

1000 samples per minibatch.
NCSNv2 642 px applied as-is for image generation.
Adam, lr = 10−7, β1 = 0.9, β2 = 0.999
20000 training iterations,

64 samples per minibatch.

Table 5: Training details for the CelebA, Blur-CelebA → CelebA FID experiment (Figure 2).

Problem
χ2, λ = 0.1
χ2, λ = 0.01
χ2, λ = 0.001
KL, λ = 0.1

KL, λ = 0.01

Noise (τ1, τk)
(9, 0.01)

Step Size
15 · 10−7

Steps

Denoising? [9] χ2 SoftPlus Param.

k = 500 Yes

α = 10

(90, 0.1)

15 · 10−7

k = 500 Yes

–

–

KL, λ = 0.005

(90, 0.1)

1 · 10−7

k = 500 Yes

Table 6: Sampling details for the CelebA, Blur-CelebA → CelebA FID experiment (Figure 2).

20

Problem Aspect Hyperparameters

Source

Target

Dataset

Preprocessing

Dataset

Preprocessing

Numbers and details
Gaussian in R784,
Mean and covariance are that of MNIST

None
Unit gaussian in R784.
None

Score Estimator Architecture

None (score is given by closed form)

Architecture

ReLU network with ReLU output activation,

784 → 2048 → 2048 → 2048 → 2048 → 1

Compatibility

Regularization

Optimization

Training

KL Regularization, λ ∈ {1, 0.5, 0.25}.
Adam, lr = 10−6, β1 = 0.9, β2 = 0.999
5000 training iterations,

1000 samples per minibatch.

Annealing Schedule No annealing.

Step size

Sampling

Mixing steps

(cid:15) = 5 · 10−3
T = 1000

Denoising? [9]

Not applicable.

Table 7: Sampling and model details for the synthetic experiment shown in Figure 2.

21

