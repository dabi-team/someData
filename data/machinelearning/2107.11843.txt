Deep Learning Explicit Diﬀerentiable
Predictive Control Laws for Buildings

J´an Drgoˇna ∗ Aaron Tuor ∗ Elliott Skomski ∗ Soumya Vasisht ∗
Draguna Vrabie ∗

∗ Paciﬁc Northwest National Laboratory, Richland, Washington, USA
(e-mails: {jan.drgona, aaron.tuor, elliott.skomski, soumya.vasisht,
draguna.vrabie}@pnnl.gov).

Abstract: We present a diﬀerentiable predictive control (DPC) methodology for learning
constrained control laws for unknown nonlinear systems. DPC poses an approximate solution
to multiparametric programming problems emerging from explicit nonlinear model predictive
control (MPC). Contrary to approximate MPC, DPC does not require supervision by an expert
controller. Instead, a system dynamics model is learned from the observed system’s dynamics,
and the neural control law is optimized oﬄine by leveraging the diﬀerentiable closed-loop
system model. The combination of a diﬀerentiable closed-loop system and penalty methods
for constraint handling of system outputs and inputs allows us to optimize the control law’s
parameters directly by backpropagating economic MPC loss through the learned system model.
The control performance of the proposed DPC method is demonstrated in simulation using
learned model of multi-zone building thermal dynamics.

Keywords: constrained deep learning; neural state space models; system identiﬁcation;
diﬀerentiable predictive control; explicit nonlinear MPC

1. INTRODUCTION

Model predictive control (MPC) (Mayne, 2014) is a suc-
cessful control method that found its way into build-
ing control applications including thermal energy storage
(Tang and Wang, 2019), thermal comfort (Park and Nagy,
2018) and optimization of energy management systems
(Hilliard et al., 2015). MPC with linear predictive models
and constraints has been extensively studied and success-
fully applied over several decades for set-point tracking
and economic optimization (Tang and Wang, 2019). How-
ever, building models are generally nonlinear and more
complex than expressed by Jacobian linearizations (Liu
et al., 2018). Expanding research on numerical methods
and tools (Andersson et al., 2019) for optimal control
problems paved the way for wider use of nonlinear MPC
(NMPC) in building control applications (Jorissen et al.,
2018). However, despite the advances in computational
tools, the complexity of the online optimization is pre-
venting low-cost deployment and maintenance of MPC in
building control applications.

Explicit MPC (Bemporad et al., 2002; Alessio and Bem-
porad, 2009) aims to overcome the online computational
drawbacks by using oﬀ-line optimization based on mul-
tiparametric programming to obtain an explicit control
law, whose online evaluation is reduced to simple function
evaluations (Parisio et al., 2014). However, despite its ap-
pealing features, current multiparametric solvers (Herceg
et al., 2013) unfortunately do not scale to the complexity
required by building control applications. Addressing the
scalability issues, approximate MPC relies on learning-
based methods to obtain explicit control laws by imitating

MPC from observational data (Drgoˇna et al., 2018; G´ang´o
et al., 2019). Karg and Lucia (2018) show that deep neural
networks with ReLU activations can exactly represent
piecewise aﬃne explicit MPC control laws. While Hertneck
et al. (2018) synthesize a neural network-based approx-
imate robust MPC controller with statistical guarantees
for closed-loop stability and constraint satisfaction.

A promising research direction emerged in recent years,
combining constrained optimization with deep learning in
so-called diﬀerentiable optimization (Agrawal et al., 2019).
Amos et al. (2018) learn the controller by diﬀerentiating
through the KKT conditions of underlying linear MPC
problem, while East et al. (2020) propose an inﬁnite-
horizon diﬀerentiable MPC framework based on discrete-
time algebraic Riccati equation. Chen et al. (2019) intro-
duced Gnu-RL, a method that adopts the diﬀerentiable
MPC (Amos et al., 2018) by combining it with policy gra-
dient algorithm for online learning. However, diﬀerentiable
MPC methods described thus far require the supervision of
a pre-computed expert controller to generate the training
data, similarly as in the case of approximate MPC.

In this paper, we focus on diﬀerentiable predictive con-
trol (DPC) (Drgoˇna et al., 2020a), a recently proposed
learning-based method for synthesizing explicit control
laws for unknown dynamical systems. Even though similar
in spirit to diﬀerentiable MPC approaches, DPC can pro-
vide scalable optimization of explicit control laws directly
without the supervision of an expert controller. Previous
studies of DPC have been performed considering unknown
linear (Drgoˇna et al., 2020b), and nonlinear (Drgoˇna et al.,
2020a) systems in single-input single-output (SISO) set-

1
2
0
2

l
u
J

5
2

]

Y
S
.
s
s
e
e
[

1
v
3
4
8
1
1
.
7
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
tings. In this paper, we expand the capabilities of DPC
with the following contributions:

(1) Use of novel system identiﬁcation method for the un-
known nonlinear dynamical systems, combining prin-
ciples of constrained optimization with deep neural
networks and state-space models.

(2) Diﬀerentiable parametrization of the closed-loop dy-
namics models composed of neural controllers and the
identiﬁed neural state-space models.

(3) Scalable constrained deep learning of explicit control
laws for multi-input multi-output nonlinear systems
with economic objectives and dynamic input and
output constraints enforced during learning using
penalty methods.

(4) Empirical demonstration of the closed-loop control
and scalability of DPC in simulations using a white-
box multi-zone building model as a controlled system.

The presented case study represents an empirical evalu-
ation of the DPC method for learning constrained con-
trol policies for multi-input multi-output systems. Dealing
with the plant-model mismatch and lack of theoretical
stability guarantees represent the main limitations of the
proposed study and will be addressed in future work.

2. DIFFERENTIABLE PREDICTIVE CONTROL

In this section, we introduce Diﬀerentiable Predictive Con-
trol (DPC), a new deep learning-based constrained control
method inspired by MPC. The conceptual methodology
shown in Fig. 1 consists of two main steps. Assuming
time series dataset obtained from the measurements of
the system dynamics, in the ﬁrst step, we perform sys-
tem identiﬁcation using a physics-constrained neural state-
space model (SSM) introduced in section 2.1. In the second
step, we close the loop by combining the neural SSM (1)
with neural control law obtaining a diﬀerentiable closed-
loop dynamics model parametrized by neural networks.
This allows us to leverage standard tools in deep learning
to optimize the neural control law parameters by back-
propagating the MPC-inspired loss function through a the
learned system dynamics model.

2.1 Neural State Space Models

As a inherent part of the DPC methodology, we consider
the approximation of the controlled system dynamics by
means of neural state space models (SSM) (Masti and
Bemporad, 2018; Drgoˇna et al., 2020c). In this paper, we
consider a neural SSM architecture given as follows:
xt = fo([yt−N ; . . . ; yt])

xt+1 = fu(xt) + fu(ut) + fd(dt)
yt+1 = fy(xt+1)

(1a)
(1b)
(1c)
where fx and fy represent the state and output dynamics,
respectively. The dynamics of control actions ut, and
disturbances dt is modeled by nonlinear sub-modules fu,
and fd. All components, fx, fy, fu, and fd can be either
parametrized by deep neural networks or linear maps.
To handle partially observable systems, we use additional
neural network fo, representing state observer estimating
the hidden states xt from the past time series of the output
measurements. Then combining all equations in (1) gives

us nonlinear autoregressive model mapping measurements
of the system outputs over the past N -steps [yt−N ; . . . ; yt]
to future time step prediction yt+1.

2.2 Neural Parametrization of the Closed-Loop Dynamics

Constructing a diﬀerentiable model of the closed-loop
dynamical system represents a core conceptual idea behind
the DPC methodology. In this paper, we use the neural
state space model (1) to represent the open-loop system
dynamics and a fully connected neural network control law
πθ(ξ) : Rm → Rn given as:

πθ(ξ) = WLhL + bL

hl = σ(Wl−1hl−1 + bl−1)
h0 = ξ

(2a)
(2b)
(2c)
where πθ(ξ) parametrized by θ = {W1, . . . WL, b1, . . . bL}
with Wl and bl representing weights and biases for hidden
1 , respectively. Then σ : Rnh → Rnh repre-
layers l ∈ NL
sents the elementwise application of a univariate activation
function σ : R → R to linearly transformed hidden layer
states hl as given in (2b).

law Uf = πθ(ξ) is map-
In this paper, the control
ping the features ξ to future control actions trajectories
Uf = [ut; . . . ; ut+N ] where N deﬁnes the length of the
prediction horizon. The computed control trajectories Uf
are used to rollout the system dynamics model (1) over
N -steps ahead into the future. Then we close the loop
by using past output trajectories of the system dynamics
Yp = [yt−N ; . . . ; yt] as features ξ = Yp for the the neural
control law πθ(ξ). For more expressive control laws, the
features ξ can contain additional vectors such as distur-
bance forecasts Df , or previews of dynamic references and
constraints imposed on states, inputs or outputs. For the
sake of brevity, we compactly represent the closed-loop
dynamics as follows:

(3a)
Uf = πθ(ξ)
(3b)
Yf = fSSM([Yp; Uf ; Df ]),
where πθ gives the control law (2) and fSSM represents
N -step ahead rollout of the system dynamics model (1).
The parametrization of the closed-loop model (3) by deep
neural networks (DNN) is motivated by the expressivity
and scalability of DNNs.
Remark 1. To decrease the dimensionality of the feature
space, we could leverage the latent space xt of the pre-
diction model fSSM as policy features. Similarly, we could
apply the principle of receding horizon control (RHC) to
the DPC policy πθ, computing only the ﬁrst time step of
the control action ut instead of a control trajectory Uf .

2.3 Constraints Satisfaction via Penalty Methods

Constraints satisfaction is a premier feature of MPC facil-
itated by system dynamics equations. We argue that in-
corporating principles of MPC in the parametrized closed-
loop model represents a systematic way of handling con-
straints in learning-based control synthesis. In particular,
having explicit parametrizations of the system dynam-
ics (1) and control law (2) allow us to simulate the in-
ﬂuence of the computed control actions on future state
trajectories. In this work, we enforce the time-varying

Fig. 1. Conceptual overview of Diﬀerentiable Predictive Control (DPC) methodology. Step 1: physics-constrained system
identiﬁcation. Step 2: Learning neural control law by backpropagation of the MPC loss through the system model.

constraints on the control actions and system outputs by
using penalty functions:

t

∼= xt − sx
t ≤ xt
∼= xt ≤ xt + sx

p(xt, xt) = max(0, xt − xt)
p(xt, xt) = max(0, −xt + xt)

(4a)
(4b)
The penalties quantifying the constraint violations are in-
corporated in to the loss function of the learning problem.
In the context of deep learning, they can be straightfor-
wardly implemented using standard ReLU functions. In this
paper, we leverage more recent use of penalties to impose
constraints in the context of deep learning (M´arquez-Neila
et al., 2017). This allows us to enforce the inequality con-
straints during optimization of the closed-loop dynamics
model (3) parametrized by neural networks.
Remark 2. Even though in this paper, the authors use
ReLU functions to model the penalties (4), other commonly
used continuously diﬀerentiable functions such as GELU,
ELU, or Softplus could be used as well.

2.4 Economic MPC Loss Function

The problems in the classical process control applications
are typically formulated as reference tracking subject to
constraints. In building control applications, instead of
tracking setpoint values, we are typically concerned with
economic objectives such as energy use or cost minimiza-
tion while maintaining the desired thermal comfort level
prescribed by temperature constraints. In classical control
literature, we refer to this method as economic MPC (Ellis
et al., 2014; Rawlings et al., 2012). To achieve the desired
performance of the closed-loop dynamics (3), we formulate
the following loss functions inspired by economic MPC:

L =

1
nN

n
(cid:88)

N
(cid:88)

(cid:16)

i=1

k=1

Qumin||ui

t||2

2 + Qdu||ui

t − ui

t−1||2

2+

(5)

)||2

Qy||p(yi

Qu||p(ui

2 + Qy||p(yi
2 + Qu||p(ui

t, yi
t
t)||2
t, ui
Here t is a time index of N -step ahead prediction horizon,
and i is an index of a total number of n batches of the
training time-series data. The ﬁrst term of the loss function
weighted with Qumin represents the energy minimization
term by pushing the control actions towards zero values.

t)||2
2+
(cid:17)
t)||2
2

t, yi
t, ui

The second term weighted with Qdu penalizes the con-
trol rate of change and represents a simple strategy for
smoothening the control trajectories. Third and fourth
terms weighted with Qy are constraints penalties on con-
trolled outputs, while the last two terms weighted with Qu
are penalties on control action constraints. Using economic
MPC objective with soft constraints penalties oﬀers a
principled way of optimizing the parametrized closed-loop
dynamics (3) with complex performance criteria.

2.5 Optimization of Diﬀerentiable Predictive Control Laws

In this section, we present DPC as a two-step algo-
rithm (Algorithm 1) for optimizing explicit control laws
using parametrized diﬀerentiable closed-loop dynamics
model (3).

Algorithm 1 Diﬀerentiable Predictive Control (DPC)

1: Inputs: datasets for modeling, DsysID, and control

Dctrl.

3: Step 2: Learning explicit control

2: Step 1: System identiﬁcation of the model fSSM (1)
using the dataset DsysID of observed system dynamics.
law πθ (2) by
optimizing economic MPC loss function (5) subject
to closed-loop dynamics model (3) using synthetic
dataset Dctrl.

4: Outputs: Learned system dynamics model fSSM (1)

and explicit control law πθ (2).

The ﬁrst step of the DPC algorithm (1) performs system
identiﬁcation of the neural state-space model (1). In the
second step of Algorithm 1, we optimize the parameters θ
of neural control law πθ (2) by minimizing the loss function
L (5) subject to closed-loop dynamics model (3) and
penalty constraints on states and outputs (4), respectively.
As part of the closed-loop model (3), we use the system
dynamics model M (1) with parameters obtained from the
system identiﬁcation in step one.

For decoupling the system identiﬁcation from control opti-
mization, the system model parameters in (1) remain ﬁxed
during the optimization of the control law parameters (2)
using the backpropagation algorithm. In the forward pass,
we sample the features ξ of the control law (2) to generate

Weather  Building physics emulator time series datasetyudDifferentiable Predictive ControlfxfufyNfox0u1, u2, ... , uNy1-Ny2-Ny0...y1y2yN...fdd1, d2, ... , dNAutoregressive neural state space model 1, Physics-constrained system identificationParametrized Closed-loop dynamics2, Control law learning with MPC loss functionfxfufyNfox0u1, u2, ... , uNy1-Ny2-Ny0...y1y2yN...fdd1, d2, ... , dNSystem modelNeural control lawMPC loss functionHeatflowsHVACBuilding envelopeuybackward propagationforward propagationLdLudycandidate control trajectories Uf and rollout the closed-
loop dynamics N -steps ahead into the future to obtain the
output response Yf of the system dynamics model (1).
Then we compute the derivatives of the loss function (5)
and backpropagate them through the closed-loop dynam-
ics model (3) to optimize the parameters of the neural
control law (2) using stochastic gradient descent updates.

From a control-theoretic perspective, the structure of the
unrolled closed-loop system dynamics model (3) in DPC
resembles the structure of the constraints in the dense
MPC formulation, also called the single shooting approach.
Then the optimization of the DPC control policy via au-
tomatic diﬀerentiation of diﬀerentiable closed-loop system
model (3) can be interpreted as an oﬄine model-based
iterative learning approach. As a result, DPC is capable
of synthesizing highly complex constrained control laws
by optimizing the simulated closed-loop behavior without
solving supervisory MPC as in the case of imitation learn-
ing (Hertneck et al., 2018; Zhang et al., 2019).

3. SIMULATION CASE STUDIES

3.1 Emulator of Building Physics

As a ground truth plant model, we use a white-box
building thermal model developed using Modelica’s IDEAS
library (Baetens et al., 2015) and envelope linearization
method (Picard et al., 2015). The building of interest
represents a six-zone residential house located in Belgium.
The heating ventilation and air conditioning (HVAC)
system consists of a central gas-boiler and a single radiator
per zone of the building. From the control perspective,
the system has six outputs representing zone operative
temperatures, seven control actions corresponding to one
supply temperature of the central heating unit, and six
mass ﬂow rates, one per zone. The building envelope is
modeled by 286 unmeasured states, and the dynamics
are additively aﬀected by 41 environmental disturbances
such as solar irradiation and ambient temperature. For
system identiﬁcation and control, we assume to have the
forecast of only the ambient temperature. This model was
previously used in the MPC context in Drgoˇna et al.
(2018). We refer to this reference for further technical
details on the emulator’s building physics.

3.2 Datasets and Optimization

For the system identiﬁcation step of DPC algorithm 1, we
use the building physics emulator described in section 3.1
to generate the time series dataset DsysID sampled with
Ts = 15 min rate, in the form of tuples of control inputs
u, disturbance signals d, and system outputs y given as:
t ), (u(i)
DsysID = {(u(i)
t
(u(i)

, y(i)
, d(i)
t+Ts
, y(i)
, d(i)
t+N Ts
with n batches indexed by i = Nn
1 of time series trajecto-
ries, each N -steps long. We use this dataset that carries the
building operation data to train the parameters of physics-
constrained autoregressive state-space model (1).

, d(i)
t

, y(i)

), . . . ,

t+N Ts

t+N Ts

t+Ts

t+Ts

(6)

)},

In the second step of DPC algorithm 1, we train the control
law πθ(ξ) using following synthetically generated training
dataset:

(7)

(˜y(i)
t

Dctrl = {(˜y(i)
t−N Ts
, y(i)

, y(i)

, y(i)
t
, u(i)

, y(i)
t

, u(i)
t
, u(i)

, u(i)
t

, d(i)
, d(i)

t ), . . . ,

)},

t+N Ts

t+N Ts

t+N Ts

t+N Ts

t+N Ts
where ˜y are sampled past output trajectories, which rep-
resent perturbation of the initial conditions for the closed-
loop system dynamics (3). Similarly we sample y, y, u,
and u representing time-varying lower and upper bounds
on controlled outputs and control actions, respectively.
For measured disturbances d we assume to have access
to their N -step ahead forecast. We select a subset of
trajectories from dataset Dctrl to act as features ξ for
the neural control law (2) with their compact notation
˜Yp = [˜y(i)
], Yf = [y(i)
], Df =
t
[d(i)
]. Both datasets DsysID and Dctrl contain
t
8640 time samples corresponding to 90 days of the building
operation data. We split each to training, development,
and test sets of equal length with 2858 time samples.

; . . . ; ˜y(i)
t

; . . . ; d(i)

; . . . ; y(i)

t−N Ts

t+N Ts

t+N Ts

We implement the DPC algorithm 1 using Pytorch li-
brary (Paszke et al., 2019). In the ﬁrst step, we train the
system model (1) with 25266 parameters. Subsequently,
we construct the closed-loop dynamics model (3) and train
the neural control law (2) over the prediction horizon of
N = 32 steps by optimizing the loss function (5) using
the Adam optimizer (Kingma and Ba, 2014), with 1000
gradient descent updates, randomly initialized weights,
and learning rate of 0.001, The resulting control
law
πθ(ξ) : R416 → R224 with 4-layers with 100 hidden
units and GELU activation functions maps 416 features
ξ = [ ˜Yp; Yf ; Df ] to 224 control action points Uf what
corresponds to 188624 parameters in total. Optimizing this
control law using DPC algorithm 1 and the above-speciﬁed
setup took under 100 seconds on a desktop machine with
64-bit 2.60 GHz Intel(R) i7-8850H CPU and 16 GB RAM.

3.3 Physics-constrained System Identiﬁcation

To improve the accuracy and generalization of the pro-
posed neural state space model (1) we constrain the model
based on known building physics. The white-box building
models typically use thermal resistance-capacitance (RC)
networks and convective heat ﬂow equations as summa-
rized in Drgoˇna et al. (2020c). These type of building mod-
els can be accurately approximated by Hammerstein archi-
tecture, with state fx(xt) = Axt, and output fy(xt) = Cxt
dynamics represented by linear maps. Then the system dy-
namics matrix A approximates the RC network describing
the thermal dynamics of the building envelope structure,
fu is the nonlinear HVAC dynamics approximating the
convective heat ﬂow equation, and fd is the nonlinear eﬀect
generated by the ambient conditions. The system out-
puts yt represent zone operative temperatures, the hidden
states xt represent lumped temperatures of the building
envelope, control actions ut denote the supply temperature
and mass ﬂows for each zone, while disturbance signal dt
represents the ambient temperature. This interpretation
allows us to enforce physically realistic bounds on the dy-
namic behavior of individual components. For instance, we
know that building envelopes are dissipative systems. Thus
we impose constraints on the eigenvalues of A to learn
a strictly stable system (Tuor et al., 2020). Additionally,
we can use the penalty method to constrain the input
dynamics inﬂuence to remain within a realistic range.

3.4 Closed-Loop Control

We demonstrate the control performance of the explicit
control laws obtained by means of DPC algorithm 1. Fig. 2
plots the closed-loop control trajectories evaluated on the
learned state-space model (1), hence considering no plant-
model mismatch. The left column represents controlled
outputs (zone temperatures) that need to stay within
time-varying bounds. The right column shows constrained
control actions (mass ﬂows) for each output computed
by explicit DPC control laws. These trajectories demon-
strate the capability to synthesize control laws with near-
optimal performance in terms of economic objectives and
constraints satisfaction using DPC algorithm 1. However,
when deployed to control the emulator, the DPC policy
learned on the nominal model was not able to compensate
for the plant model mismatch. Thus these initial results
indicate that due to close-to-optimal performance on the
nominal model (Fig. 2), the ﬁeld performance of the DPC
boils down to a problem of accurately identifying the
system dynamics with coupling.

4. LIMITATIONS AND FUTURE WORK

The lack of theoretical stability guarantees and the ability
to deal with the plant model are the main limitations of
the DPC methodology presented in this study. In future
work, we plan to expand the DPC methodology for the
systematic handling of plant-model mismatch. To intro-
duce real-time feedback capabilities into DPC, we aim to
incorporate the principles of robust and oﬀ-set free MPC,
as well as adaptive control updates. The authors are also
working on theoretical closed-loop stability guarantees for
DPC. Our future empirical work will include a systematic
comparison with classical MPC methods on experiments
including a wide range of nonlinear systems.

5. CONCLUSIONS

This paper applied a recently proposed diﬀerentiable pre-
dictive control (DPC) methodology to a multi-zone build-
ing control problem. We have expanded the prior capabil-
ities of DPC in learning complex explicit control laws for
unknown nonlinear dynamical systems. In particular, we
have shown that DPC can systematically handle economic
objectives subject to dynamic constraints imposed on non-
linear system dynamics with multiple inputs and outputs.
The DPC closed-loop control capabilities are empirically
demonstrated in a simulation case study using a multi-zone
building thermal dynamics model. We have shown that it
is possible to learn explicit control laws for constrained
nonlinear optimal control problems with a large number of
states and long prediction horizons. By doing so, we tackle
the complexity limitations of classical explicit MPC based
on multiparametric programming. Simultaneously, DPC
overcomes the main limitation of imitation learning-based
approaches such as approximate MPC by alleviating the
need for supervision from the model predictive controller
(MPC). Based on the presented features, we believe that
the proposed DPC methodology has long-term potential,
not only for research but also for practical applications,
such as building control that requires fast development
and low-cost deployment and maintenance on hardware

with limited computational resources. However, several
practical challenges, such as dealing with the plant model
mismatch, need to be addressed in future work.

ACKNOWLEDGEMENTS

This work was funded by the Physics Informed Machine
Learning (PIML) investment at the Paciﬁc Northwest
National Laboratory (PNNL).

REFERENCES

Agrawal, A., Barratt, S., Boyd, S., Busseti, E., and Moursi,
W.M. (2019). Diﬀerentiating through a cone program.
Alessio, A. and Bemporad, A. (2009). A survey on explicit
model predictive control. In Nonlinear model predictive
control, 345–369. Springer.

Amos, B., Rodriguez, I.D.J., Sacks, J., Boots, B., and
Kolter, J.Z. (2018). Diﬀerentiable MPC for end-to-end
planning and control. CoRR, abs/1810.13400.

Andersson, J.A.E., Gillis, J., Horn, G., Rawlings, J.B.,
and Diehl, M. (2019). Casadi: a software framework
for nonlinear optimization and optimal control. Math-
ematical Programming Computation, 11(1), 1–36. doi:
10.1007/s12532-018-0139-4.

Baetens, R., De Coninck, R., Jorissen, F., Picard, D.,
Helsen, L., and Saelens, D. (2015). OpenIDEAS - An
open framework for integrated district energy simula-
tions. In Proceedings of Building simulation 2015. Hy-
derabad, India.

Bemporad, A., M., M., Dua, V., and Pistikopoulos, E.N.
(2002). The explicit linear quadratic regulator for
constrained systems. Automatica, 38(1), 3 – 20. doi:
10.1016/S0005-1098(01)00174-1.

Chen, B., Cai, Z., and Berg´es, M. (2019). Gnu-rl: A
precocial reinforcement learning solution for building
hvac control using a diﬀerentiable mpc policy.
In
Proceedings of the 6th ACM International Conference
on Systems for Energy-Eﬃcient Buildings, Cities, and
Transportation, BuildSys ’19, 316–325. Association for
Computing Machinery, New York, NY, USA.

Drgoˇna, J., Kis, K., Tuor, A., Vrabie, D., and Klauco, M.
(2020a). Diﬀerentiable predictive control: An mpc alter-
native for unknown nonlinear systems using constrained
deep learning. arXiv preprint arXiv:2011.03699.

Drgoˇna, J., Picard, D., Kvasnica, M., and Helsen, L.
(2018). Approximate model predictive building control
via machine learning. Applied Energy, 218, 199 – 216.
doi:https://doi.org/10.1016/j.apenergy.2018.02.156.
Drgoˇna, J., Tuor, A., and Vrabie, D. (2020b). Constrained
physics-informed deep learning for stable system iden-
tiﬁcation and control of unknown linear systems. URL
http://arxiv.org/abs/2004.11184.

Drgoˇna, J., Tuor, A.R., Chandan, V., and Vrabie, D.L.
(2020c). Physics-constrained deep learning of multi-zone
building thermal dynamics.

East, S., Gallieri, M., Masci, J., Koutnik, J., and Cannon,
M. (2020). Inﬁnite-horizon diﬀerentiable model predic-
tive control.
In International Conference on Learning
Representations.

Ellis, M., Durand, H., and Christoﬁdes, P.D. (2014). A tu-
torial review of economic model predictive control meth-
ods. Journal of Process Control, 24(8), 1156–1178. doi:
https://doi.org/10.1016/j.jprocont.2014.03.010.
Eco-
nomic nonlinear model predictive control.

Fig. 2. Closed-loop control performance of explicit DPC control law evaluated on learned nominal model. Left column

represents controlled zone temperatures. Right column shows manipulated mass ﬂows.

G´ang´o, D., P´eni, T., and T´oth, R. (2019). Learning
based approximate model predictive control for nonlin-
ear systems. IFAC-PapersOnLine, 52(28), 152–157. doi:
https://doi.org/10.1016/j.ifacol.2019.12.363. 3rd IFAC
Workshop on Linear Parameter Varying Systems LPVS
2019.

Herceg, M., Kvasnica, M., Jones, C., and Morari, M.
(2013). Multi-parametric toolbox 3.0. In 2013 European
Control Conference, Zurich, Switzerland, 502–510.

Hertneck, M., K¨ohler, J., Trimpe, S., and Allg¨ower, F.
(2018). Learning an approximate model predictive con-
troller with guarantees. IEEE Control Systems Letters,
2(3), 543–548.

Hilliard, T., Kavgic, M., and Swan, L. (2015). Model
predictive control for commercial buildings: trends and
opportunities. Advances in Building Energy Research.
Jorissen, F., Picard, D., Cupeiro Figueroa, I., Boydens,
W., and Helsen, L. (2018). Towards real MPC imple-
In 5th
mentation in an oﬃce building using TACO.
International High Performance Building ConferenceAt:
Purdue University, West Lafayette, IN, USA.

Karg, B. and Lucia, S. (2018). Eﬃcient representation and
approximation of model predictive control laws via deep
learning.
Kingma, D.P.

and Ba,

J.

(2014).
method for stochastic optimization.
arXiv:1412.6980.

Adam: A
arXiv preprint

Liu, Y., Yu, N., Wang, W., Guan, X., Xu, Z.,
Coordinating
smart
doi:

Dong, B., and Liu, T.
in
the
grids.
Applied Energy, 228, 2510–2525.
https://doi.org/10.1016/j.apenergy.2018.07.089.

operations

buildings

(2018).

smart

of

M´arquez-Neila, P., Salzmann, M., and Fua, P. (2017).
Imposing hard constraints on deep networks: Promises
and limitations.
URL
http://arxiv.org/abs/1706.02025.

CoRR, abs/1706.02025.

Masti, D. and Bemporad, A. (2018). Learning nonlin-
ear state-space models using deep autoencoders.
In
2018 IEEE Conference on Decision and Control (CDC),
3862–3867.

Mayne, D.Q. (2014). Model predictive control: Recent
developments and future promise. Automatica, 50(12),
2967 – 2986. doi:10.1016/j.automatica.2014.10.128.
Parisio, A., Fabietti, L., Molinari, M., Varagnolo, D., and
Johansson, K. (2014). Control of HVAC systems via
scenario-based explicit MPC. In Decision and Control
(CDC), 2014 IEEE 53rd Annual Conference on, 5201–
5207. doi:10.1109/CDC.2014.7040202.

Park, J.Y. and Nagy, Z. (2018). Comprehensive analysis of
the relationship between thermal comfort and building
control research-a data-driven literature review. Renew-
able and Sustainable Energy Reviews, 82, 2664–2679.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury,
J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N.,
Antiga, L., et al. (2019). Pytorch: An imperative style,
high-performance deep learning library. In Advances in
Neural Information Processing Systems, 8024–8035.
Picard, D., Jorissen, F., and Helsen, L. (2015). Method-
ology for obtaining linear state space building energy
simulation models. In Proceedings of the 11th Interna-
tional Modelica Conference, 51–58. Paris, France.

Rawlings, J.B., Angeli, D., and Bates, C.N. (2012). Funda-
mentals of economic model predictive control. In 2012
IEEE 51st IEEE Conference on Decision and Control
(CDC), 3851–3861. doi:10.1109/CDC.2012.6425822.
Tang, R. and Wang, S. (2019). Model predictive control
for thermal energy storage and thermal comfort opti-
mization of building demand response in smart grids.
Applied Energy, 242, 873–882.

Tuor, A., Drgoˇna, J., and Vrabie, D. (2020). Con-
strained neural ordinary diﬀerential equations with sta-
bility guarantees. arXiv preprint arXiv:2004.10883.
Zhang, X., Bujarbaruah, M., and Borrelli, F. (2019). Safe
and near-optimal policy learning for model predictive
control using primal-dual neural networks. 2019 Amer-
ican Control Conference (ACC), 354–359.

