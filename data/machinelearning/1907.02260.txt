0
2
0
2

n
a
J

0
1

]
E
N
.
s
c
[

3
v
0
6
2
2
0
.
7
0
9
1
:
v
i
X
r
a

On Explaining Machine Learning Models
by Evolving Crucial and Compact Features

Life Sciences and Health Group, Centrum Wiskunde & Informatica, Amsterdam 1098 XG, the Netherlands

Marco Virgolin

Tanja Alderliesten

Department of Radiation Oncology, Amsterdam UMC, Amsterdam 1105 AZ, the Netherlands

Peter A.N. Bosman

Life Sciences and Health Group, Centrum Wiskunde & Informatica, Amsterdam 1098XG, the Netherlands

Algorithmics Group, Delft University of Technology, Delft 2628 XE, the Netherlands

Abstract
Feature construction can substantially improve the accuracy of Machine Learning (ML) algorithms. Genetic
Programming (GP) has been proven to be eﬀective at this task by evolving non-linear combinations of input features.
GP additionally has the potential to improve ML explainability since explicit expressions are evolved. Yet, in most
GP works the complexity of evolved features is not explicitly bound or minimized though this is arguably key for
explainability.
In this article, we assess to what extent GP still performs favorably at feature construction when
constructing features that are (1) Of small-enough number, to enable visualization of the behavior of the ML model;
(2) Of small-enough size, to enable interpretability of the features themselves; (3) Of suﬃcient informative power,
to retain or even improve the performance of the ML algorithm. We consider a simple feature construction scheme
using three diﬀerent GP algorithms, as well as random search, to evolve features for ﬁve ML algorithms, including
support vector machines and random forest. Our results on 21 datasets pertaining to classiﬁcation and regression
problems show that constructing only two compact features can be suﬃcient to rival the use of the entire original
feature set. We further ﬁnd that a modern GP algorithm, GP-GOMEA, performs best overall. These results, combined
with examples that we provide of readable constructed features and of 2D visualizations of ML behavior, lead us
to positively conclude that GP-based feature construction still works well when explicitly searching for compact
features, making it extremely helpful to explain ML models.

This preprint is associated to a manuscript accepted for publication on Swarm and Evolutionary Computation, doi.org/10.1016/j.swevo.2019.100640.
This work is licensed under a Creative Commons “Attribution-NonCommercial-NoDerivs 3.0 Unported” license.

Keywords: feature construction, interpretable machine learning, genetic programming, GOMEA

1. Introduction

of) the original feature space into one that the ML algo-
rithm can better exploit.

Feature selection and feature construction are two im-
portant steps to improve the performance of any Machine
Learning (ML) algorithm [1, 2]. Feature selection is the
task of excluding features that are redundant or mislead-
ing. Feature construction is the task of transforming (parts

A very interesting method to perform feature construc-
tion automatically is Genetic Programming (GP) [3, 4].
GP can synthesize functions without many prior assump-
tions on their form, diﬀerently from, e.g., logistic regres-
sion or regression splines [5, 6]. Moreover, feature con-

Preprint submitted to Swarm and Evolutionary Computation

January 13, 2020

 
 
 
 
 
 
modern ML research, as many practical, sensitive applica-
tions exist, where explaining (part of) the behavior of ML
models is essential to trust their use (e.g., in medical appli-
cations) [7, 8, 9, 10]. Typically, GP for feature construc-
tion searches in a subspace of mathematical expressions.
Adding to the appeal and potential of GP, these expres-
sions can be human-interpretable if simple enough [8, 11].
Figure 1 presents an example of the potential held
by such an approach: a multi-dimensional dataset trans-
formed into a 2D one, where both the behavior of the ML
algorithm and the meaning of the new features is clear,
while the performance of the ML algorithm is not com-
the use of the original feature set (it is
promised w.r.t.
actually improved).

In this article we study whether GP can be useful to
construct a low number of small features, to increase the
chance of obtaining interpretable ML models, without
compromising their accuracy (compared to using the orig-
inal feature set). To this end, we design a simple, iterative
feature construction scheme, and perform a wide set of
experiments: we consider four types of feature construc-
tion methods (three GP algorithms and random search),
ﬁve types of machine learning algorithms. We apply their
combinations on 21 datasets between classiﬁcation and
regression to determine to what extent they are capable
of eﬀectively and eﬃciently ﬁnding crucial and compact
features for speciﬁc ML algorithms.

The main original scientiﬁc contribution of this work is
an investigation of whether GP can be used to construct
features that are:

• Of small-enough number, to enable visualization of

the behavior of the ML model;

• Of small-enough size, to enable interpretability of

the features themselves;

• Of suﬃcient informative power, to retain or even im-
prove the performance of the ML, compared to using
the original feature set;

These aspects are assessed under diﬀerent circumstances:

• We test diﬀerent search algorithms, including mod-

ern model-based GP and random search;

• We test diﬀerent ML algorithms.

Figure 1: Regression surface learned by SVM for the Yacht dataset (in
blue), expressed as a 2D function of the two features (on the bottom
axes) constructed by our approach. Circles are training samples, dia-
monds are test samples. The dataset has six features (x(i)). Our approach
constructs two new features (using GP-GOMEA, see Sec. 4.1), which
are non-linear transformations of the prismatic coeﬃcient (x(2)) and the
Froude number (x(6)). With only two features the SVM prediction sur-
face can be visualized. Moreover, these new features are understandable.
Finally, the modeling quality is actually improved over employing SVM
directly on all six features. The coeﬃcient of determination of SVM in-
creased from 85% using the original features to 98% using the two new
features.

struction not only depends on the data at hand, but also
on the way a speciﬁc ML algorithm can model that data.
Evolutionary methods in general are highly ﬂexible in
their use due to the way they perform search (i.e., deriva-
tive free). This makes it possible, for example, to evaluate
the quality of a feature for a speciﬁc ML algorithm by di-
rectly measuring what its impact is on the performance of
the ML algorithm (i.e., by training and validating the ML
algorithm when using that feature).

Explaining what constructed features mean can shed
light on the behavior of ML-inferred models that use such
features. Reducing the number of features is also im-
portant to improve interpretability.
If the original fea-
ture space is reduced to few constructed features (e.g., up
to two for regression and up to three for classiﬁcation),
the function learned by the ML model can be straightfor-
wardly visualized w.r.t. the new features. In fact, how to
make ML models more understandable is a key topic of

2

The remainder of this article is organized as follows.
Related work is reported in Section 2. The proposed fea-
ture construction scheme is presented in Section 3. The
search algorithms to construct features, as well as the con-
sidered ML algorithms, are presented in Section 4. The
experimental setup is described in Section 5. Results re-
lated to performance are reported in Section 6, while re-
sults concerning interpretability are reported in Section 8.
Section 10 discusses our ﬁndings, and Section 11 con-
cludes this article.

2. Related work

In this article, we consider GP for feature construc-
tion to achieve better explainable ML models. Diﬀer-
ent forms of GP to obtain explainable ML have been
explored in literature, but they do not necessarily lever-
age feature construction. E.g., [12] introduced a form
of GP for the automatic synthesis of interpretable classi-
ﬁers, generated from scratch as self-contained ML mod-
els, made of IF-THEN rules. A very diﬀerent paradigm
for explainable ML by GP is considered in [13], where the
authors explore the use of GP to recover the behavior of
a given unintelligible classiﬁer by evolving interpretable
approximation models. Other GP-based approaches and
paradigms to synthesize interpretable ML models from
scratch, or to approximate the behavior of pre-existing
ML models by interpretable expressions, are reported in
recent surveys on explainable artiﬁcial ingelligence such
as [8, 9].

Since in this article we particularly study what the po-
tential of GP for feature construction is in terms of added
value for explaining complex, not directly explainable
models learned by various popular ML algorithms, the re-
lated work that follows describes GP approaches for fea-
ture construction. For readers interested in feature selec-
tion, we refer to a recent survey [14].

One of the ﬁrst approaches of GP for feature construc-
tion is presented in [15]. There, each GP solution is a set
of K features. The ﬁtness of a set is the cross-validation
performance of a decision tree [16] using that set. The re-
sults on six classiﬁcation datasets show that the approach
is able to synthesize a feature set that is competitive with
the original one, and can also be added to the original set
for further improvements. No attention is however given
to the interpretability of evolved features.

The work in [17] generates one feature with Standard,
tree-based GP (SGP) [3], to be added to the original set.
Feature importance metrics of decision trees such as in-
formation gain, Gini index and Chi2 are used as ﬁtness
measure. An advantage of using such ﬁtness measures
over ML performance is that they can be computed very
quickly. However, they are decision tree-speciﬁc. Results
show that the approach can improve prediction accuracy,
and, for a few problems, it is shown that decision trees
that are simple enough to be reasonably interpretable, can
be found.

Feature construction for high-dimensional datasets is
considered in [18], for eight bio-medical binary classi-
ﬁcation problems, with 2,000 to 24,188 features. This
approach is diﬀerent from the typical ones, as the au-
thors propose to use SGP to evolve classiﬁers rather than
features, and extract features from the components (sub-
trees) of such classiﬁers. These are then used as new fea-
tures for an ML algorithm. Results on K-Nearest Neigh-
bors [19], Naive Bayes classiﬁer [20, 21], and decision
tree show that a so-found feature set can be competitive
or outperform the original one. The authors show an ex-
ample where a single interpretable feature is constructed
that enables linear separation of the classiﬁcation exam-
ples.

Diﬀerent from the aforementioned works, [22] explores
feature construction for regression. A SGP-based ap-
proach is designed to tackle regression problems with a
large number of features, and is tested on six datasets. In-
stead of using the constructed features for a diﬀerent ML
algorithm, SGP dynamically incorporates them within an
ongoing run, to enrich the terminal set. Every α genera-
tions of SGP, the subtrees composing the best solutions
become new features by encapsulation into new termi-
nal nodes. The approach is found to improve the ability
of SGP to ﬁnd accurate solutions. However, the features
found by encapsulating subtrees are not interpretable be-
cause allowing subsequent encapsulations leads to an ex-
ponential growth of solution size.

A recent work that focuses on evolutionary dimen-
sionality reduction and consequent visualization is [23],
where a multi-objective, grammar-based SGP approach is
employed. K feature transformations are evolved in syn-
ergy to enable, at the same time, good classiﬁcation ac-
curacy, and visualization through dimensionality reduc-
tion. The system is thoroughly tested on 42 classiﬁcation

3

tasks, showing that the algorithm performs well compared
to state-of-the-art dimensionality reduction methods, and
it enables visualization of the learned space. However, as
trees are free to grow up to a height of 50, the constructed
features themselves cannot be interpreted.

Bayes classiﬁer, as well as “strong”, state-of-the-art ones,
which are rarely used in literature for feature construc-
tion, such as support vector machine and random forest;
on both classiﬁcation and regression tasks.

The most similar works to ours that we found are [24]
and [25]. In [24], which is our previous work, the pos-
sibility of using a modern model-based GP algorithm
(which we also use in our comparisons) for feature con-
struction is explored on four regression datasets. There,
focus is put on keeping feature size small, to actively at-
tempt to obtain readable features. These features are itera-
tively constructed to be added to the original feature set to
improve the performance of the ML algorithm, and three
ML algorithms are compared (linear regression, support
vector machines [26], random forest [27]). Reducing the
feature space to enable a better understanding of inferred
ML models is not considered.

In [25], diﬀerent feature construction approaches are
compared on gene-expression datasets that have a large
number of features (thousands to tens of thousands) to
study if evolving class-dependent features, i.e., features
that are each targeted at aiding the ML algorithm detect
one speciﬁc class, can be beneﬁcial. Similarly to us, the
authors show visualizations of feature space reduced to
up to three constructed features, and an example of three
features that are encoded as very small, easy-to-interpret
trees. However, such small features are a rare outcome as
the trees used to encode features typically had more than
75 nodes. These trees are therefore arguably extremely
hard to read and interpret.

Our work is diﬀerent from previous research in two ma-
jor aspects. First, none of the previous work principally
addresses the conﬂicting objectives of retaining good per-
formance of an ML algorithm while attempting to explain
both its behavior (by dimensionality reduction to allow vi-
sualization), and the meaning of the features themselves
(by constraining feature complexity). Second, multiple
GP algorithms within a same feature construction scheme,
on multiple ML algorithms, are not compared in previ-
ous work. Most of the times, it is a diﬀerent feature con-
struction scheme that is tested, using arguably small vari-
ations of SGP. Here, we consider random search, two ver-
sions of SGP, as well as another modern GP algorithm.
Furthermore, we adopt both “weak” ML algorithms such
as ordinary least squares linear regression and the naive

3. Iterative evolutionary feature construction

We use a remarkably simple scheme to construct fea-
tures. Our approach constructs K ∈ N+ features by it-
erating K GP runs. The evolution of the k-th feature
(k ∈ {1, . . . , K}) uses the previously constructed k − 1 fea-
tures.

3.1. Feature construction scheme

The dataset D deﬁning the problem at hand is split into
two parts: the training T r and the test T e set. This parti-
tion is kept ﬁxed through the whole procedure. Only T r is
used to construct features, while T e is exclusively used for
ﬁnal evaluation to avoid positive bias in the results [28].
We use the notation x(i)
to refer to the i-th feature value
j
of the j-th example, and y j for the desired outcome (label
for classiﬁcation or target value for regression) of the j-th
example.

The k-th GP run evolves the k-th feature. An exam-
ple is shown in Figure 2. Each solution in the population
competes to become the new feature x(k), that represents
a transformation of the original feature set. In every run,
the population is initialized at random.

We evaluate the ﬁtness of a feature of the k-th run
by measuring the performance of the ML algorithm on
a dataset that contains that feature and the previously
evolved k − 1 features.

We only use original features (and random constants)
as terminals.
In particular, the features constructed by
previous iterations are not used as terminal nodes in the
k-th run. This prevents the generation of nested features,
which could harm interpretability.

At the end of the k-th run, the best feature is stored
are added to T r and T e for the next

and its values x(k)
j
iterations.

3.2. Feature ﬁtness

The ﬁtness of a feature is computed by measuring the
performance (i.e., error) of the ML algorithm when the

4

T r iteration k − 1

x(1)

22.49
12.98
. . .

. . .

. . .
. . .
. . .

x(k−1)

-3.10
-7.41
. . .

y

10.4
7.49
. . .

GP

Best New Feature x(k)

T r iteration k
x(k−1)

x(k)

-3.10
-7.41
. . .

7.12
9.41
. . .

y

10.4
7.49
. . .

T e iteration k
x(k−1)

x(k)

9.87
6.45
. . .

1.11
4.78
. . .

y

5.55
12.01
. . .

. . .

. . .
. . .
. . .

. . .

. . .
. . .
. . .

ML alg.

Trained Model

Test Error k

Figure 2: Construction of the k-th feature and computation of the k-th test error. Evolved features use the features of the original dataset (not shown)
and random constants as terminal nodes. Dashed arrows represent inputs, solid arrows represents outputs.

new feature is added to T r. We consider the C-fold cross-
validation error rather than the training error to promote
generalization and prevent overﬁtting. The pseudo code
of the evaluation function is shown in Algorithm 1.

Speciﬁcally, the C-fold cross-validation error is com-
puted by partitioning T r into C splits. For each c =
1, . . . , C iteration, a diﬀerent split is used for validation
(set V c), and the remaining C − 1 splits are used for train-
ing (set T rc). The mean validation error is the ﬁnal result.
For classiﬁcation tasks, in order to take into account
both multiple and possibly imbalanced class distributions,
the prediction error is computed as 1 minus the macro
F1 score, i.e., 1 minus the mean of the class-speciﬁc F1
scores:

1 − F1 = 1 −

1
#classes

= 1 −

2
#classes

(cid:88)

F1γ

γ∈classes

(cid:88)

γ∈classes

T Pγ
T Pγ+FPγ
T Pγ
T Pγ+FPγ

T Pγ
T Pγ+FNγ
+ T Pγ

T Pγ+FNγ

,

where T Pγ, FNγ, FPγ are the true positive, false nega-
tive, and false positive classiﬁcations for the class γ, re-
spectively. If the computation of F1γ results in 0
0 , we set
F1γ = 0.

For regression, the prediction error is computed with

the Mean Squared Error (MSE).

5

Algorithm 1 Computation of the ﬁtness of a feature s

T r(cid:48) ←AddFeatureToCurrentTrainingSet(s)
error ← 0
for c = 1, . . . , C do

1 function ComputeFeatureFitness(s)
2
3
4
5
6
7
8

T c, V c ←SplitSet(c, C, T r(cid:48))
M ←TrainMLModel(T c)
error ← error+ComputeError(M, V c)

Return

(cid:17)

(cid:16) error
C

3.3. Preventing unnecessary ﬁtness computations

Computing the ﬁtness of a feature is particularly ex-
pensive, as it consists of a C-fold cross-validation of the
ML algorithm. This limits the feasibility of, e.g., adopting
large population sizes and large numbers of evaluations
for the GP algorithms.

We therefore attempt to prevent unnecessary cross-
validation calls, by assessing if features meet four criteria.
Let n be the number of examples in T r. The criteria are
the following:

1. The feature is not a constant. We avoid evaluating
constant features as they are likely to be useless for
many ML algorithms, which internally already com-
pute an intercept.

2. The feature does not contain extreme values that
may cause numerical errors, i.e., with absolute value
above a lower-bound β(cid:96) or above an upper-bound βu.
Here, we set β(cid:96) = 10−10, and βu = 1010 (none of the
datasets considered here have values exceeding these

bounds).

3. The feature is not equivalent to one constructed in
the previous k − 1 iterations. Equivalence is deter-
mined by checking the values available in T r, i.e.,
equivalence holds if:

∀ j ∈ T r, ∃i ∈ {1, . . . , k − 1} : x(k)
j

= x(i)
j .

Note that a constructed feature that is equivalent to
a feature of the original feature set can be valid, as
long as no other previously constructed feature exists
that is already equivalent. Thus, our approach can in
principle perform pure feature selection.

4. The values of

the feature in consideration have
changed since the last time the feature was evalu-
ated. GP variation can change the syntax of a fea-
ture without necessarily aﬀecting its behavior (e.g.,
inserting a multiplication by 1 will not change the ﬁ-
nal values a feature computes). If the values do not
change, then the ﬁtness of the feature will not change
either (see Sec. 3.2). We therefore avoid unnecessary
re-computations of feature ﬁtnesses, by caching the
feature values prior to GP variation, and checking
whether they have changed after variation.

The computational eﬀort for each criterion is O(n) (it
is O((k − 1)n) for criterion 3, however in our experiments
k (cid:28) n). The ﬁtness of a feature failing criterion 1, 2, or
3 is set to the maximum possible error value. If criterion
4 fails, the ﬁtness remains the same (although perform-
ing cross-validation may lead to slightly diﬀerent results
when using stochastic ML algorithms like random forest).

4. Considered search algorithms and machine learn-

ing algorithms

We consider SGP, Random Search (RS), and the GP
instance of the Gene-pool Optimal Mixing Evolutionary
Algorithm (GP-GOMEA) as competing search algorithms
to construct features. SGP is widely used in feature con-
struction (see related work in Sec. 2). RS is not typi-
cally considered, yet we believe it is important to assess
whether evolution does bring any beneﬁt over random
enumeration within the conﬁnes of our study, i.e., when
forcing to ﬁnd small features. GP-GOMEA is a recently

introduced GP algorithm that has proven to be particu-
larly proﬁcient in evolving accurate solutions of limited
size [11, 29, 24].

As ML algorithms, we consider the Naive Bayes classi-
ﬁer (NB), ordinary least-squares Linear Regression (LR),
Support Vector Machines (SVM), Random Forest (RF),
and eXtreme Gradient Boosting (XGB). NB is used only
for classiﬁcation tasks, LR only for regression tasks,
SVM, RF, and XGB for both tasks. We provide more de-
tails in the following sections.

4.1. Details on the search algorithms

All search algorithms use the ﬁtness evaluation func-
tion. A feature s is evaluated by ﬁrst checking whether
the four criteria of Section 3.3 are met, and then, if the
outcome is positive, by running the ML algorithm over
the feature-extended dataset.

For SGP, we use subtree crossover and subtree muta-
tion, picking the depth of subtree roots uniformly ran-
domly as proposed in [30]. The candidate parents for vari-
ation are chosen with tournament selection. Since we are
interested in constructing small features so as to increase
the chances they will be interpretable, we consider two
versions of SGP. The ﬁrst is the classic one where solu-
tions are free to grow to tree heights typically much larger
than the one used for tree initialization. In the following,
the notation SGP refers to this ﬁrst version. The second
one uses trees that are not allowed to grow past the initial
maximum tree height. We call this version bounded SGP,
and use the notation SGPb.

RS is realized by continuously sampling and evaluating
new trees, keeping the best [3]. Like for SGPb, a maxi-
mum tree height is ﬁxed during the whole run. If evolu-
tion is hypothetically no better than RS, then we expect
that SGPb and GP-GOMEA will construct features that
are no better than the ones constructed by RS.

GP-GOMEA is a recently introduced GP algorithm that
has been found to deliver accurate solutions of small size
on benchmark problems [29], and to work well when a
small size is enforced in symbolic regression [11, 24].
GP-GOMEA uses a tree template ﬁxed by a maximum
tree height (which can include intron nodes to allow for
unbalanced tree shapes) and performs homologous varia-
tion, i.e., mixed tree nodes come from the same positions
in the tree. Each generation prior to mixing, a hierarchical
model that captures interdependencies (linkage) between

6

nodes is built (using mutual information). This model,
called Linkage Tree (LT), drives variation by indicating
what nodes should be changed en block during mixing, to
avoid the disruption of patterns with large linkage.

The LT has been shown to enable GP-GOMEA to out-
perform subtree crossover and subtree mutation of SGP,
as well as the use of a randomly-build LT, i.e., the Ran-
dom Tree (RT), on problems of diﬀerent nature [11, 29].
However, the LT requires suﬃciently large population
sizes to be accurate and beneﬁcial (e.g., several thou-
sand solutions in GP for symbolic regression) [11]. Be-
cause in the framework of this article ﬁtness evaluations
use the cross-validation of a ML algorithm, we cannot
aﬀord to use large population sizes. Accordingly, we
found the adoption of the LT to not be superior to the
adoption of the RT under these circumstances in prelimi-
nary experiments. Therefore, for the most part, we adopt
GP-GOMEA with the RT (GP-GOMEART). This means
we eﬀectively compare random hierarchical homologous
variation with subtree-based variation. An example of
adopting the LT and large population sizes for feature con-
struction is provided in Section 10.

4.2. Details on the ML algorithms

We now brieﬂy describe the ML algorithms used in this
work: NB, LR, SVM, RF, and XGB. NB and LR are less
computationally expensive compared to SVM, RF, and
XGB. Details on the computational time complexity of
these algorithms are reported at: https://bit.ly/2PG0xse.

NB is a classiﬁer which assumes independence be-
tween features [20, 21]. NB is often used as a baseline,
as it is simple and fast to train. We use the mlpack imple-
mentation of NB [31] and assume the data to be normally
distributed (default setting).

Similarly to NB, LR is often used as a baseline as it is
simple and fast, for regression tasks. LR assumes that the
target variable can be explained by a linear combination
of the features [20]. We use the mlpack implementation
of LR [31].

SVM is a powerful ML algorithm that can be used for
non-linear classiﬁcation and regression [26, 32]. We use
the libsvm C++ implementation [32]. We consider the Ra-
dial Basis Function (RBF) kernel, which works well in
practice for many problems, with C-SVM for classiﬁca-
tion, and E-SVM for regression.

Table 1: Parameter settings of the GP algorithms.

SGP(b)

GP-GOMEART

Population size
Initialization method
Initialization max tree height
Max tree height
Variation
Selection
Function set
Terminal set

100
Ramped Half and Half
2–6 (2 or 4)
17 (2 or 4)
SX 0.9, SM 0.1
Tournament 7, Elitism 1
{+, ×, −, ÷, ·2,

√

100
Half and Half
2 or 4
2 or 4
parameter-less
parameter-less

·, logp, exp} for all

{x(i), ERC} for all

RF is an ensemble ML algorithm which, like SVM, can
be used for both classiﬁcation and regression and can in-
fer non-linear patterns [27]. RF builds an ensemble of
(typically deep) decision trees, each trained on a sample
of the training set (bagging). At prediction time, the mean
(or maximum agreement) prediction of the decision trees
is returned. We use the ranger C++ implementation [33].

XGB is, like RF, an ensemble ML algorithm, typically
based on decision trees, and capable of learning non-
linear models [34]. XGB works by boosting, i.e., stacking
together multiple weak estimators (small decision tress)
that ﬁt the data in an incremental fashion. We use the
dmlc C++ implementation (https://bit.ly/34fBNeA).

5. Experiments

We perform 30 runs of our Feature Construction
Scheme (FCS), with SGP, SGPb, RS, and GP-GOMEART,
in combination with each ML algorithm (NB only for
classiﬁcation and LR only for regression), on each prob-
lem. Each run of the FCS uses a random train-test split of
80%-20%, and considers up to K = 5 features construc-
tion rounds. We use a population size of 100 for the search
algorithms, and assign a maximum budget of 10, 000
function evaluations to each FCS iteration. This results
in relatively large running times for complex ML algo-
rithms (see Sec. 9). An experiment including larger evo-
lutionary budgets and the use of the LT in GP-GOMEA
is presented in the discussion (Sec. 10). We use a limit
on the total number of evaluations instead of a a limit on
the total number of generations because GP-GOMEART
performs more evaluations than SGP per generation [29].
For GP-GOMEART, SGPb, and RS, we consider two
levels of maximum tree height h: 2 and 4. This choice
yields a maximum solution size of 7 and 31 respectively

7

(using function nodes with a maximum arity r = 2). We
choose these two height levels because we found features
with h = 2 to be arguably easy to read and interpret,
whereas features with h = 4 can already be very hard
to understand. This indication is also reported in [11] for
the evolution of symbolic regression formulas. Note that
using a tree height limit over a solution size limit pre-
vents ﬁnding deep trees containing the nesting of the ar-
guably more complicated to understand non-linear func-
tions ·2,
·, logp, exp. We do not consider bigger tree
heights as resulting features may likely be impossible to
interpret, defying a key focus of this work.

√

Other parameter settings used for the GP algorithms are
shown in Table 1. SGPb uses the same settings as SGP,
except for the maximum tree height (at initialization and
along the whole run), which is set to the same of GP-
GOMEART. In GP-GOMEART we use the Half and Half
(HH) tree initialization method instead of the Ramped
Half and Half (RHH) [3] commonly used for SGP. This
proved to be beneﬁcial since GOM varies nodes instead
of subtrees [11, 24]. For both HH and RHH, syntactical
uniqueness of solutions is enforced for up to 100 tries [3].
In GP-GOMEART we additionally avoid sampling trees
having a terminal node as root by setting the minimum
tree height of the grow method to 1. This is not done for
SGP and SGPb, because diﬀerently from GP-GOMEART
where homologous nodes are varied, subtree root nodes
for subtree crossover (SX) and subtree mutation (SM) are
chosen uniformly randomly. RS samples new trees using
the same initialization method as SGPb, i.e., RHH.

√

The division operator ÷ used in the function set is the
analytic quotient operator (a ÷ b = a/
1 + b2), which
was shown to lead to better generalization performance
than protected division [35]. The logarithm is protected
logp(·) = log(| · |) and logp(0) = 0, and so is the square
root operator. The terminal set contains the original fea-
ture set, and an Ephemeral Random Constant (ERC) [4]
with values uniformly sampled between the minimum and
maximum values of the features in the original training
set, i.e., [min x(i)

j ], ∀i, j ∈ T r.
The hyperparameter settings for the SVM, RF and
XGB are shown in Table 2, and are mostly default [27,
32, 33] (for XGB, we referred to https://bit.ly/2JCM9x4).
NB and LR implementations do not have hyperparame-
ters.

j , max x(i)

We consider 10 classiﬁcation and 10 regression bench-
mark datasets1 that can be considered traditional, i.e, they
have small to moderate dimensionality (number of fea-
tures). We mostly study this type of dataset because we
seek to ﬁnd small constructed features that can be inter-
preted. Hence, they can represent a transformation of
only a limited number of original features. Details on
the datasets are reported in Table 3. Rows with miss-
ing values are omitted. Most datasets are taken from the
UCI Machine Learning repository2, with exception for
Dow Chemical and Tower, which come from GP litera-
ture [36, 37].

We further consider a very high-dimensional dataset
from UCI (https://bit.ly/334KbgW) to assess whether GP
can still be useful to construct features in this type of sce-
nario. The dataset in question concerns the classiﬁcation
of cancer type, given RNA-Seq gene expression levels as
features. Five cancer class types are present, and class
proportions in the data presents some unbalance: the class
frequencies are 0.37, 0.18, 0.18, 0.17, 0.10. A total of
20, 531 features are considered, in 801 examples. Since
large computational resources are needed to handle this
dataset, we consider only NB as ML algorithm for feature
construction upon this data.

6. Results: performance on traditional datasets

The results described in this section aim at assessing
whether it is possible to construct few and small features
that lead to an equal or better performance than the origi-
nal set, and whether some search algorithms can construct
better features than others.

6.1. General performance of feature construction

We begin by observing the dataset-wise aggregated per-
formance of FCS for the diﬀerent GP algorithms and the
diﬀerent ML algorithms, separately for classiﬁcation and
regression.

6.1.1. Classiﬁcation

Figure 3 shows dataset-wise aggregated results ob-
tained for NB, SVM, RF, and XGB, for the 10 tradi-
tional classiﬁcation tasks. Each data point is the mean

1The datasets are available at http://goo.gl/9D2z3b
2http://archive.ics.uci.edu/ml/

8

NB

SVM

RF

XGB

Training

Test

Training

Test

Training

Test

Training

Test

2
=
h

4
=
h

Figure 3: Aggregated results on the classiﬁcation datasets. Horizontal axis: Number of features. Vertical axis: Average of median F1 score obtained
on 30 runs for each dataset.

LR

SVM

RF

XGB

Training

Test

Training

Test

Training

Test

Training

Test

2
=
h

4
=
h

Figure 4: Aggregated results on the regression datasets. Horizontal axis: Number of features. Vertical axis: Average of median R2 score obtained
on 30 runs for each dataset.

9

0.50.60.70.80.91.0123450.50.60.70.80.91.012345123451234512345123451234512345RT0.50.60.70.80.91.0123450.50.60.70.80.91.012345123451234512345123451234512345RTTable 2: Salient hyper-parameter settings of SVM, RF, and XGB.

Kernel
Cost
Epsilon
Tolerance
Gamma
Shrinking

Number of trees
Bagging sampling
Classiﬁcation mtry
Regression mtry
Min node size
Split rule

SVM

RF

RBF
1
0.1
0.001
1
k
Active

√

100
with replacement
#features
min(1, #features
1 classiﬁcation, 5 regression
Gini classiﬁcation, Variance regression

)

3

XGB

Number of trees
Booster
Max depth
Objective
Learning rate

100
gbtree
6
multiclass softmax, MSE regression
0.3

Table 3: Traditional classiﬁcation and regression datasets.

Dataset

# Features

# Examples

# Classes

n
o
i
t
a
c
ﬁ
i
s
s
a
l
C

n
o
i
s
s
e
r
g
e
R

Cylinder Bands
Breast Cancer Wisc.
Ecoli
Ionosphere
Iris
Madelon
Image Segmentation
Sonar
Vowel
Yeast

Airfoil
Boston Housing
Concrete
Dow Chemical
Energy Cooling
Energy Heating
Tower
Wine Red
Wine White
Yacht

39
29
7
34
4
500
19
60
9
8

6
13
9
57
9
9
26
12
12
7

277
569
336
351
150
2600
2310
208
990
1484

1503
506
1030
1066
768
768
4999
1599
4898
308

2
2
8
2
3
2
7
2
11
10

–
–
–
–
–
–
–
–
–
–

among the dataset-speciﬁc medians of macro F1 from the
30 runs.

In general, the use of only one constructed feature does
not perform as good as the use of the original feature set.
Constructing more features improves the performance,
but with diminishing returns.

Speciﬁcally for NB, the use of two constructed features
is already preferable to the use of the original feature set.
This is likely due to the fact that NB assumes complete
independence between the provided features, and this can
be implicitly tackled by FCS. SGP (unbounded) is the best
performing algorithm as it can evolve arbitrarily com-
plex features, however, the magnitude of improvement of
the macro F1 score with respect to GP-GOMEART and
SGPb is limited. For h = 4 and K = 5, GP-GOMEART
reaches the performance of SGP. GP-GOMEART is typi-
cally slightly better than SGPb, and RS has worse perfor-
mance. Training and test F1 scores do not diﬀer much for
any feature construction algorithm, meaning that overﬁt-
ting is not an issue for NB. Rather, compared to the other
ML algorithms, NB underﬁts.

The performance of FCS for SVM has an almost iden-
tical pattern to the one observed for NB, except for the
fact that the performance is found to be consistently bet-
ter. However, for SVM it is preferable to use the original
feature set rather than few constructed features. This is
evident in terms of training performance, but less at test
time. In fact, using only 5 constructed features leads to
similar test performance compared to using the original
set. The GP algorithms compare to each other similarly
to when using NB. Compared to NB, it can be seen that
SVM exhibits larger gaps between training and test re-
sults, suggesting that some overﬁtting takes place, espe-
cially when the original feature set is used.

The way performance improves for RF by constructing
features is similar to the one observed for NB and SVM.
However, for RF the diﬀerences between the search al-
gorithms is particularly small: notice that using RS leads
to close performance to the ones obtained by using the
other GP algorithms, compared to the SVM case. More-
over, virtually no diﬀerence can be seen between GP-
GOMEART and SGPb. This suggests that RF already
works well with less reﬁned features. Now, the features
constructed by SGP are no longer the best performing
at test time. This is likely because SGP evolves larger,
more complex features than the other algorithms (see

10

Sec. 6.1.3), making RF overﬁt. In fact, RF exhibits the
largest diﬀerence between training and test results com-
pared to NB and SVM, for any feature construction algo-
rithm and h limit. Still, the test results of RF are slightly
better than the ones of SVM and markedly better than the
ones of NB, meaning that the latter two are underﬁtting.

The training and test performance obtained when us-
ing XGB is similar to the one obtained when using RF,
but the diﬀerences the between diﬀerent search algorithms
are even less marked than for RF. Some diﬀerences can be
seen for K = 1 on the training set (SGP better than GP-
GOMEART, and GP-GOMEART better than SGPb and
RS), but this diﬀerence is much less marked on the test
set. When more features are constructed, essentially all
search algorithms deliver the same performance. XGB
seems to be able to construct non-linear relationships even
better than RF. As to potential overﬁtting, the trend of dif-
ferences between training and test performance that can
be observed for XGB mirrors the one visible for RF.

As to maximum tree height, allowing the constructed
features to be bigger (h = 4 vs h = 2) moderately im-
Interestingly, GP-GOMEART
proves the performance.
with h = 4 reaches competitive performance with SGP
on all ML algorithms, despite the latter having no strict
limitation on feature size.

6.1.2. Regression

Results on the regression tasks are shown in Figure 4,
dataset-wise aggregated for LR, SVM, RF, and XGB. We
report the results in terms of coeﬃcient of determination,
i.e., R2(y, ¯y) = 1 − MS E(y, ¯y)/var(y). For the four ML
algorithms, results overall follow the same pattern. SGP
is typically better, especially for LR and SVM, although
constructing more features reduces the performance gap
with the other GP algorithms. GP-GOMEART is slightly,
yet consistently, the best performing within the maximum
tree height limitation of 2, while SGPb is visibly prefer-
able only when a single feature is constructed for LR
and SVM, for h = 4. Diﬀerently from the classiﬁcation
case, two features are typically enough to reach the per-
formance of the original feature set for all ML algorithms
except for XGB. Moreover, for LR, SVM, and RF, the per-
formance between training and test is similar, meaning no
considerable overﬁtting is taking place, no matter the fea-
ture construction algorithm used nor the limit of h. This
however is not the case for XGB, where a large perfor-

mance gap is encountered. Still, the test performance ob-
tained when using XGB is ultimately slightly better than
the obtained for RF.

As for classiﬁcation, allowing for larger trees results in
better performance overall, and reduces the gap between
SGP and the other GP algorithms. With XGB, all search
algorithms perform similarly.

6.1.3. Feature size

Figure 5 shows the aggregated feature size for the dif-
ferent GP algorithms and RS. The aggregated solution
size is computed by taking the median solution size per
run, then averaging over datasets, and ﬁnally averaging
over ML algorithms (classiﬁcation and regression are con-
sidered together). The picture shows how, overall, the
known SGP tendency to bloat diﬀers compared to the al-
gorithms working with a strict tree height limitation. SGP
features are so large that it is nearly impossible to interpret
them (see Sec. 8.1).

RS ﬁnds the smallest features for both height limits
h = 2 and h = 4. Considering that GP-GOMEART and
SGPb generate trees within the same height bounds of
RS, we conclude that it is the variation operators that al-
low ﬁnding larger trees with improved ﬁtness within the
height limit. GP-GOMEART seems to construct slightly,
yet consistently, larger trees than SGPb.

For SGP, it can be seen that subsequently constructed
features are smaller (this is barely visible for GP-
GOMEART and SGPb as well). This is interesting be-
cause we do not use any mechanism to promote smaller
trees. This result is likely linked to the diminishing re-
turns in performance observed in Figure 3 and 4: con-
structing new complex and informative features becomes
harder with the number of FCS iterations.

6.2. Statistical signiﬁcance: comparing GP algorithms

The aggregated results of Section 6.1 show moderate
diﬀerences between GP-GOMEART and SGPb. These are
arguably the most interesting algorithms to compare in-
depth, as they are able to construct small features that lead
to good performance (RS typically constructs less infor-
mative features, while SGP constructs very large ones).

We perform statistical signiﬁcance tests to compare
GP-GOMEART and SGPb. We consider their median per-
formance on the test set T e, obtained by the FCS, and

11

e
z
i
s

e
r
u
t
a
e
F

Figure 5: Aggregated feature size for k = 1, . . . , 5. Solid (dotted) lines
represent solution size for maximum tree height h = 2 (h = 4). Shaded
areas represent standard deviation. SGP is free to grow solutions up to
h = 17.

also compare it with the use of the original feature set,
for each ML algorithm and each dataset. In our case, the
treatments of our signiﬁcance tests are the two search al-
gorithms (i.e., GP-GOMEART and SGPb) and the original
feature set, while the subjects are the conﬁgurations given
by pairing ML algorithms and datasets [38].

We ﬁrst perform a Friedman test to assess whether dif-
ferences exists among the use of diﬀerent treatments (GP
algorithms and original feature set) upon multiple sub-
jects (ML algorithm-dataset combinations). As post-hoc
analysis, we use the pairwise Wilcoxon signed rank tests,
paired by subject (ML algorithm-dataset combination), to
see how the treatments compare to each other [38]. We
adopt the Holm correction method to prevent reporting
false positive results that might have happened due to pure
chance [39].

We consider both h = 2 and h = 4, and focus on
K = 2, since consideration of only two constructed fea-
tures makes interpretation easier, and allows human visu-
alization (see Sec. 8.1).

6.2.1. Classiﬁcation

For both h = 2, 4, the Friedman test strongly indicates
diﬀerences between GP-GOMEART, SGPb, and the use of
the original feature set (p-value (cid:28) 0.05).

Figure 6 (top) shows the Holm-corrected p-values ob-
tained by the pairwise Wilcoxon tests for classiﬁcation,

where the alternative hypothesis is that the row allows for
larger macro F1 scores than the column. No signiﬁcant
diﬀerences between GP-GOMEART and SGPb are found
for both h = 2, 4. Both the GP algorithms can deliver con-
structed features that are competitive with the use of the
original feature set. The original feature set is not signif-
icantly better than using feature construction. Moreover,
for GP-GOMEART and h = 4, the hypothesis that feature
construction is not better than the original feature set can
be rejected with a corrected p-value below 0.1. The latter
result appears to be in contrast with the results from Fig. 3
for SVM, RF and XGB, where it can be seen that the con-
struction of only two features does, on average, lead to
slightly worse test results than using the original feature
set. Nonetheless, the opposite is true for NB, and with
rather large magnitude. A more in-depth analysis on this
is provided in Sec. 6.3.

6.2.2. Regression

As for classiﬁcation datasets, the Friedman test indi-
cates that diﬀerences are presents between the treatments.
Figure 6 (bottom) shows the Holm-corrected p-values ob-
tained by the pairwise Wilcoxon tests for regression.

The statistical tests conﬁrm the hypothesis that the al-
gorithms are capable of providing constructed features
that are more informative than the original feature set, as
observed in Fig. 4 for the regression datasets. Now, GP-
GOMEART is signiﬁcantly better than SGPb when h = 2.
For h = 4, instead, GP-GOMEART is not found to be sig-
niﬁcantly better than SGPb.

6.3. Statistical signiﬁcance: two constructed features vs.

the original feature set per ML algorithm

Results presented in Sec. 6.1 indicate that our FCS
brings most beneﬁt if used with the weak ML algorithms.
We now report, for each ML algorithm, on how many
datasets 2 features constructed using GP-GOMEA (with
h = 2 and h = 4) lead to statistically signiﬁcantly (using
Holm-corrected pairwise Wilcoxon test, p-value < 0.05)
better, equal, or worse results compared to using the orig-
inal feature set on the test set. This is shown in Table 4.

These results conﬁrm what seen in Figures 3 and 4.
Using FCS typically outperforms the use of the original
feature set for the weak ML algorithms. For the strong
ML algorithms, in most cases, using the original feature

12

RTn
o
i
t
a
c
ﬁ
i
s
s
a
l
C

n
o
i
s
s
e
r
g
e
R

h = 2

h = 4

Figure 6: Holm-corrected p-values of pairwise Wilcoxon tests on test
performance. Rows are tested to be signiﬁcantly better than columns.
Orig stands for the original feature set.

set is preferable. However, for some datasets reducing
the space to two compact features without compromising
performance is still possible.

The use of the original feature set is generally hardest
to beat when adopting RF or XGB. For RF, in the regres-
sion case with h = 4, FCS brings beneﬁts on the datasets
Airfoil, Energy Cooling, Energy Heating, and Yacht; and
performs on par with the use of the original feature set on
the datasets Boston Housing and Concrete. These datasets
are the ones with the smallest number of original features.
We ﬁnd similar results for SVM and for XGB. In the latter
case, FCS is, in terms of statistical signiﬁcance, equal to
the original feature set only on Energy Cooling, Energy
Heating, and Yacht. It is reasonable to expect that FCS
works well when few features can be combined.

In the classiﬁcation case, ﬁndings are diﬀerent. For RF
and h = 4, the datasets where using two constructed fea-
tures bring similar or better results than using the original
feature set are Breast Cancer Wisconsin and Iris. The lat-
ter does have a small number of original features (4), but
the former has more than several other datasets (29). Fur-
thermore, the datasets where FCS helps are diﬀerent for
SVM: FCS performs equally good to the original feature
set on Iris and Cylinder Bands (39 features), and better on
Madelon (500 features) and Image Segmentation (19 fea-
tures). Regarding XGB, there is no dataset where FCS is

13

Table 4: Number of datasets where using two features constructed with
GP-GOMEA results in signiﬁcantly better/equal/worse test performance
compared to using the original feature set.

h

. 2
4

s
s
a
l
C

h

. 2
r
g
e
4
R

NB
8/1/1
8/1/1

LR
5/3/2
7/1/2

SVM RF
1/1/8
2/2/6
1/1/8
2/2/6

SVM RF
4/0/6
5/2/3
4/2/4
5/2/3

XGB
0/4/6
0/4/6

XGB
0/2/8
0/3/7

superior to the original feature set, but it is also not worse
on almost half of the datasets. For classiﬁcation datasets,
we cannot conclude that a small cardinality of the original
feature set is a good indication feature construction will
work well. Furthermore, feature construction inﬂuences
diﬀerent ML algorithms in diﬀerent ways.

7. Results: performance on a highly-dimensional

dataset

We further consider the RNA-Seq cancer gene expres-
sion dataset, comparing FCS by GP-GOMEART with h =
4 against the use of the original feature set, when using
NB. Fig. 7 shows that NB with the original feature set
overﬁts: the training performance is maximal, while the
test performance reaches an F1 of approximately 0.65.
Even tough NB is typically considered a weak estimator,
the system described by the data is so severely underdeter-
mined (over 20, 000 features vs less than 1, 000 examples)
that actual patterns cannot be retrieved. The use of FCS
forces NB to use only a small number of constructed fea-
tures, which, in turn, can contain only a small number of
the original features. Essentially, FCS provides both the
advantages of feature construction and feature selection.
This leads to large F1 scores already when solely two fea-
tures are constructed.

8. Results: improving interpretability

The results presented in Sec. 6 and 7 showed that the
original feature set can be already outperformed by two
small constructed features in many cases. We now aim
at assessing whether constraining features size can en-
able interpretability of the features themselves, as well

For h = 2, we argue that constructed features are mostly
easy to interpret. For example, the feature shown for
LR on Concrete tells us that aging (x(8)) has a negative
impact on concrete compressive strength, whereas using
more water (x(4)) than cement (x(1)) has a positive eﬀect
(both features are in kg/cm3). The impact of other fea-
tures is less important (within the data variability of the
dataset). For h = 4, some features can be harder to read
and understand, however many are still accessible. This is
mostly because, even though the total solution size reach-
able with h = 4 is 31, constructed features are typically
half the size (see Fig. 5).

The features constructed for

the RNA-Seq gene-
expression dataset by GP-GOMEART in Sec. 7 are also
not excessively complex to be understood. For example,
the ﬁrst two features for the median run are:

(cid:113)

(cid:0)x(18382)(cid:1)2 + x(8014) + x(3885) + x(17316)
x(7296) + x(19333)(cid:17)
x(7491) +

√

×

(cid:16)

1st :

2nd :




x(5524) + x(18053)
x(5579) − x(4417)(cid:17)2

1 + (cid:16)

(cid:114)

+ x(14153) + x(19751) −

x(13744)
1 + (cid:16)

x(16581) (cid:17)2

(cid:114)





Even tough the second feature is somewhat involved, it is
arguably still possible to carefully analyze it and obtain a
picture of how gene expression levels interact.

Overall, we cannot draw a strict conclusion on whether
the features found by our approach are interpretable, as in-
terpretability is a subjective matter and, to date, no clear-
cut metric exists [7, 8] (we discuss this more in Sec. 10).
Yet, it appears evident that enforcing a restriction on their
size is a necessary condition. We generally ﬁnd that fea-
tures using 15 or more nodes start to be hard to interpret
w.r.t. our experimental settings, i.e., using our function
set. Lastly, features constructed without a strict size lim-
itation (by SGP) are in general very large, and thus ex-
tremely hard to understand. As an example, Figure 8
shows the ﬁrst of the two features with median test per-
formance constructed by SGP for LR on Concrete (this
is smaller than the ﬁrst feature found by SGP for NB on
Ecoli).

8.2. Visualizing what the ML algorithm learns

The construction of a small number of interpretable
features can enable a better understanding of the problem

Figure 7: Comparison between the use of the original feature set and
FCS with GP-GOMEART (h = 4) on high-dimensional gene expression
data. The vertical axis reports the median F1 score, the horizontal axis
reports the number of features constructed by FCS. Stars indicate sta-
tistical signiﬁcant superiority (p-value < 0.05) of one method w.r.t. the
other.

as if extra insight can be achieved by plotting and visu-
alizing the behavior of a trained ML model in the new
two-dimensional space.

8.1. Interpretability of small features

Table 5 shows some examples of features constructed
by GP-GOMEART, for h = 2 and h = 4. We report the
ﬁrst feature constructed for the K = 2 case, with median
test performance. We show the ﬁrst feature as it is typ-
ically not smaller than the second (see Fig. 5). Analytic
quotients and protected logarithms are replaced by their
respective deﬁnitions. We remark that we do not check
whether the meaning of the features is sound (e.g., ensur-
ing a certain unit of measure is returned). Constraining
feature meaning is problem-dependent, and outside the
scope of this work.

For classiﬁcation, we choose NB as it is the method
which beneﬁts most from feature construction. The
dataset considered is Ecoli, where NB achieves the largest
median test improvement when K = 2: from F1 = 0.51
with the original set, to F1 = 0.63 for h = 2, and to
F1 = 0.66 for h = 4.

For regression, we consider LR on the Concrete dataset,
for the same aforementioned reasons. The test R2 ob-
tained with the original feature set is 0.59, the one with
two features constructed by GP-GOMEART is 0.76 (0.78)
for h = 2 (h = 4).

14

RTRTTable 5: Examples of features constructed by GP-GOMEART with h ∈
{2, 4}, K = 2, for NB on Ecoli, and for LR on Concrete.

h

2

4

B
N

1st Feature
(cid:113)

x(3) + x(6) + x(1)/

1 + (cid:0)x(6)(cid:1)2

x(6) (cid:16)

x(7)(cid:17)2

x(3) + 0.144/

(cid:113)

1 + (cid:0)exp(x(2))(cid:1)2 − x(1) x(2) x(5)

R 2
L
4

(cid:112)

x(4) − x(1) + 932.204/
19.764 log |x(8)| + x(2) + 2x(1)/

1 + (x(8))2
(cid:112)

1 + (x(4))2

(cid:112)

(cid:18)

logp(((((((x(4) + x(2)) ÷ x(4)) × (x(1) ÷ x(4) ÷ x(4) × (x(1) ÷ x(4)x(8)1065.162×
logp((((x(4)x(1) + x(1) ÷ x(8) ÷ x(8)) × (x(4) − (x(1) + ((x(2) ÷ x(4) + logp(x(1))))2)))+
− (x(5) x(1) + x(2) ÷ x(8) + (((x(8) + x(6) + x(2)) ÷ ((x(1) ÷ x(4)) +
exp((((x(8) + x(2)) ÷ x(8) + (x(4) + x(2)) ÷ x(4)) ÷ x(5) + x(6) ÷ x(4)))))))))) ÷ x(7))×

x(4) ÷ x(2)))+

√

(x(8) − (441.237 + x(2)))) − x(1)))

(cid:19) 1
2

Figure 8: Example of a relatively “small” feature constructed by SGP,
derived from a tree with 96 nodes. Note that the analytic quotient op-
erator (÷) and the protected logarithm (logp) are not expanded to their
respective deﬁnitions to keep the feature contained. This feature is ar-
guably very hard to interpret.

and of the learned ML models. The case where up to two
features are constructed is particularly interesting, since it
allows visualization.

We provide one example of classiﬁcation boundaries
and one of a regressed surface, inferred by SVM on a two
dimensional feature space obtained with our approach us-
ing GP-GOMEART.

The classiﬁcation dataset on which we ﬁnd the best test
improvement for h = 4 is Image Segmentation, where
the F1 score of SVM reaches 0.88, against 0.65 using the
original feature set (median run). Figure 9 shows the clas-
siﬁcation boundaries learned by SVM. The analytic quo-
tient operator ÷ and the protected log logp are replaced by
their deﬁnition for readability. The constructed features
are rather complex here, yet readable. At the same time,
it can be clearly seen how the training and test examples
are distributed in the 2D space, and what classiﬁcation
boundaries SVM learned.

For regression, Figure 1 shows the surface learned by
SVM on Yacht (median run), where GP-GOMEART with
h = 2 constructs two features that lead to an R2 of 0.98,
against 0.85 obtained using the original feature set. The
features are arguably easy to interpret, while it can be seen

Figure 9: Classiﬁcation boundaries learned by SVM with two features
constructed by GP-GOMEART (h = 4) on the Image Segmentation
dataset. The run with median test performance is shown. Circles are
training samples, diamonds are test samples.

that the learned surface accurately models most of the data
points.

9. Running time

Our results are made possible by evaluating the ﬁtness
of constructed features with cross-validation, a procedure
which is particularly expensive. Table 6 shows the (mean
over 30 runs) serial running time to construct ﬁve fea-
tures on the smallest and largest classiﬁcation and regres-
sion datasets, using GP-GOMEART with h = 4 and the
parameter settings of Sec. 5, on the relatively old AMD
OpteronTM Processor 6386 SE3. Running time has a large
variability, from seconds to dozens of hours, depending
on dataset size and ML algorithm. For the traditional
datasets and ML algorithms we considered, it can be ar-
gued that our approach can be used in practice. However,
for very high-dimensional datasets, only fast ML algo-
rithms can be used. The construction of 5 features for the
RNA-Seq gene expression dataset took 25 minutes even
tough NB was used. To use slower ML algorithms would
easily require dozens to hundreds of hours.

3http://cpuboss.com/cpu/AMD-Opteron-6386-SE

15

1.52.02.53.0log|log|x(2)|+x(19)/1+(x(5))2+x(12)+exp(x(19))|024681012(x(19))2+x(2)/1+(x(3))4+4x(12)Table 6: Mean serial running time to construct ﬁve features using GP-
GOMEART (h = 4) on the smallest and largest traditional datasets.
XGB

NB/LR

Dataset

SVM

Size

RF

. Iris
s
a
l
C

Madelon

150 × 4
2600 × 500

. Yacht
r
g
e
Tower
R

308 × 7
4999 × 26

7 s
4 m

8 s
2 m

2 m
14 h

4 m
34 h

25 m 42 m
10 h
8 h

1 h
34 h

1 h
13 h

As to memory occupation, it basically mostly depends
on the way the chosen ML algorithm handles the dataset.
Our runs required at most few hundreds of MBs when
dealing with the larger traditional datasets, for SVM and
RF. Handling the parallel execution of FCS experiments
upon the gene expression dataset required a few GBs.

10. Discussion

We believe this is one of the few works on evolutionary
feature construction where the focus is put on both im-
proving the performance of an ML algorithm, and on hu-
man interpretability at the same time. The interpretability
we aimed for is twofold: understanding the meaning of
the features themselves, as well as reducing their num-
ber. GP algorithms are key, as they can provide con-
structed features as interpretable expressions given basic
functional components, and a complexity limit (e.g., tree
height).

We have run a large set of experiments, totaling more
than 150,000 cpu-hours. Our results strongly support the
hypothesis that the original feature set can be replaced by
few (even solely K = 2) features built with our FCS with-
out compromising performance in many cases. In some
cases, performance even improved. GP-GOMEART and
SGPb achieve this result while keeping the constructed
feature size extremely limited (h = 2, 4). SGP leads
to slightly better performance than GP-GOMEART and
SGPb, but at the cost of constructing ﬁve to ten times
larger features. RS proved to be less eﬀective than the
GP algorithms.

Our FCS is arguably most sensible to use for simpler
ML algorithms, such as NB and LR. Constructed features
change the space upon which the ML algorithm operates.
SVM already includes the kernel trick to change the fea-
ture space. Similarly, the trees of RF and XGB eﬀectively

embody complex non-linear feature combinations to ex-
plain the variance in the data. NB and LR, instead, do
not include such mechanisms. Rather, they have particu-
lar assumptions on how the features should be combined
(NB assumes normality, LR linearity). The features con-
structed by GP can transform the input the ML algorithm
operates upon, to better ﬁt its assumptions.

We found that performance was almost always signif-
icantly better than compared to using the original feature
set for NB and LR. As running times for these ML algo-
rithms can be in the order of seconds or minutes (Sec. 9),
feature construction has the potential to be routinely used
in data analysis and machine learning practice. Further-
more, FCS (or a modiﬁcation where the constructed fea-
tures are added to the original set) can be used as an al-
ternative way to tune simple ML algorithms which have
limited or no hyper-parameters.

We have shown that our approach can also be helpful
when dealing with high-dimensional data (on the RNA-
Seq gene expression dataset), where system underdeter-
mination can cause even simpler ML algorithms to overﬁt.
This is because FCS essentially embodies feature selec-
tion, as we only construct a small number of small-sized
features.

We remark that we did not adopt very popular high-
dimensional datasets concerning image recognition such
as MNIST [40], CIFAR [41], or ImageNet [42]. In these
datasets, features represent pixels, and each pixel has
no particular meaning. Consequently, constructing fea-
tures as readable pixel transformations will likely carry
no unhelpful information to explain the behavior of a ML
model.

Regarding the comparison between the search algo-
rithms, GP-GOMEART was found to be slightly prefer-
able to SGPb (especially for h = 2, K = 2). We believe
that signiﬁcantly better results can be achieved if bigger
population sizes and larger evaluations budgets can be
employed (we kept the population size limited due to the
computational expensiveness of SVM and RF).

Particularly for GP-GOMEA, previous work has shown
that having suﬃciently large population sizes enables
the possibility to exploit linkage estimation and perform
better-than-random mixing [11, 29]. To validate this also
within the framework of our proposed FCS, we scaled the
population size and the budget of ﬁtness evaluations, and
compared the use of the LT with the use of the RT, on

16

proposed. Simulatability represents the capability of hu-
mans to predict the output of a model given an input. De-
composability represents the capacity to intuitively under-
stand the components of a model. Crucially, to measure
this type of metrics, user studies need to be conducted.
For example, experts of a ﬁeld should be asked to provide
feedback, on features constructed for datasets they are
knowledgeable about (e.g., biochemists for data on gene
expression, civil engineers for data on concrete strength).
Nonetheless, we believe that enforcing features (and GP
programs in general) to be small still remains a necessary
condition to allow interpretability, although it is often ig-
nored in GP literature [11].

Considering the visualization examples proposed in
Section 8, it is natural to compare our approach with well-
known dimensionality reduction techniques, such as Prin-
cipal Component Analysis (PCA) [43] or t-Distributed
Stochastic Neighbor Embedding (t-SNE) [44]. We re-
mark that those techniques and our FCS have very dif-
ferent objectives. In general, the sole aim of such tech-
niques is to reduce the data dimensionality. PCA does so
by detecting components that capture maximal variance.
However, it does not attempt to optimize the transforma-
tion of the original feature set to improve an ML algo-
rithm’s performance. Also, PCA does not focus on the
interpretability of the feature transformations. FCS takes
the performance of the ML algorithm and interpretability
of the features into account, while dimensionality reduc-
tion comes from forcing the construction of few features.
We compared using 2 features constructed with RS (the
worst search algorithm) with maximum h = 2, with using
the ﬁrst 2 PCs found by PCA. The use of constructed fea-
tures over PCs resulted in signiﬁcantly superior or equal
test performance for all ML algorithms and for all prob-
lems. We remark, however, that PCA is extremely fast
and independent from the ML algorithm.

Our FCS has several limitations. A ﬁrst limitation re-
gards the performance obtainable by the ML algorithm
using the constructed features. FCS is iterative, and this
can lead to suboptimal performance for a chosen K, com-
pared to attempting to ﬁnd K features at once. This
is because the contributions of multiple features to an
ML algorithm are not necessarily perpendicular to each
other [25]. FCS could be changed to ﬁnd at any given it-
eration, a synergistic set of K features, that is independent
from previous iterations. To this end, larger population

Figure 10: Comparison between the use of the RT and of the LT in GP-
GOMEA. Vertical axis: median F1 score of 30 runs, obtained by NB on
Image Segmentation (left) and on Madelon (right) using the ﬁrst con-
structed feature, with h = 4 (note the diﬀerent scale). Horizontal axis:
population size / ﬁtness evaluations budget. Stars indicate signiﬁcant
superiority (p − value < 0.05) of one method w.r.t. the other.

two traditional classiﬁcation dataset: Image Segmenta-
tion (19 features) and Madelon (500 features), using NB.
The outcome is shown in Figure 10: the employment of
big-enough population sizes (and of suﬃcient numbers of
ﬁtness evaluation) can lead to better performance, if sta-
tistical metrics can be measured reliably. For Image Seg-
mentation, the number of terminals to be considered in the
genotype is relatively small due to the use of 19 features.
This allows the LT to estimate node interdependencies re-
liably, and deliver better-than-random performance. For
Madelon, the large number of terminals (500 features)
makes it hard for the LT to outperform the RT within a
limited computational budget. All in all, we recommend
the use of GP-GOMEA as feature constructor since it was
not worse on classiﬁcation and was statistically better for
regression. Furthermore, we advice to use the LT if the
population size can be of the order of thousands or more
(or even better, if exponential population sizing schemes
are used as in [11, 29]). Otherwise, the RT should be pre-
ferred.

To assess if small constructed features are interpretable
and if it is possible to visualize what the behavior of
learned ML models, we showed some examples, provid-
ing evidence that both requirements can be reasonably sat-
isﬁed. However, we did not perform a thorough study on
interpretability of the constructed features. Several met-
rics have been recently proposed to measure some form of
interpretability for ML models, that could be used to mea-
sure the interpretability of features as well. E.g., in [7]
two metrics called simulatability and decomposability are

17

sizes need to be employed, and the search algorithms need
to be modiﬁed so that they can evolve sets of constructed
features (a similar proposal for SGP was done in [15]).
Yet, it is reasonable to expect that if K features need to be
learned at the same time, larger population sizes may be
needed compared to learning the K features iteratively.

Another limitation of this work is that hyper-parameter
tuning was not considered. To include hyper-parameter
tuning within FCS could bring even higher performance
scores, or help prevent overﬁtting. A possibility could
be, for example, to evolve pairs of features and hyper-
parameter settings, where every time a feature is evalu-
ated, the optimal hyper-parameters are also searched for.
Such a procedure may likely require strong paralleliza-
tion eﬀorts, as C-fold cross-validation should be carried
out for each combination of hyper-parameter values.

Lastly, it would be interesting to extend our approach to
other classiﬁcation and regression settings, e.g., problems
with missing data; or to unsupervised tasks, as simple fea-
tures may lead to better clustering of the examples.

11. Conclusion

With a simple evolutionary feature construction frame-
work we have studied the feasibility of constructing few
crucial and compact features with Genetic Programming
(GP), towards improving the explainability of Machine
Learning (ML) models without losing prediction accu-
racy. Within the proposed framework, we compared stan-
dard GP, random search, and the GP adaptation of the
Gene-pool Optimal Mixing Evolutionary Algorithm (GP-
GOMEA) as feature constructors, and found that GP-
GOMEA is overall preferable when strict limitations on
feature size are enforced. Despite limitations on feature
size, and despite the reduction of problem dimensional-
ity that we imposed by constructing only two features, we
obtained equal or better ML prediction performance com-
pared to using the original feature set for more than half
the combinations of datasets and ML algorithms. In many
cases, humans can understand what the feature means,
and it is possible to visualize how trained ML models will
behave. All in all, we conclude that feature construction
is most useful and sensible for simpler ML algorithms,
where more resources can be used for evolution (e.g.,
larger population sizes), which, in turn, unlock the added

beneﬁts of more advanced evolutionary mechanisms (e.g.,
using linkage learning in GP-GOMEA).

Acknowledgments

The authors acknowledge the Kinderen Kankervrij
foundation for ﬁnancial support (project #187). The ma-
jority of the computations for this work were performed
on the Lisa Compute Cluster with the support of SURF-
sara.

References

[1] H. Liu, H. Motoda, Feature extraction, construction
and selection: A data mining perspective, Vol. 453,
Springer Science & Business Media, 1998.

[2] J. Friedman, T. Hastie, R. Tibshirani, The elements
of statistical learning, Vol. 1, Springer series in
statistics New York, NY, USA:, 2001.

[3] J. R. Koza, Genetic Programming: On the Program-
ming of Computers by Means of Natural Selection,
MIT Press, Cambridge, MA, USA, 1992.

[4] R. Poli, W. B. Langdon, N. F. McPhee, J. R. Koza,
A ﬁeld guide to genetic programming, Lulu. com,
2008.

[5] J. H. Friedman, Multivariate adaptive regression

splines, The annals of statistics (1991) 1–67.

[6] D. W. Hosmer Jr, S. Lemeshow, R. X. Sturdivant,
Applied logistic regression, Vol. 398, John Wiley &
Sons, 2013.

[7] Z. C. Lipton, The mythos of model interpretability,

Queue 16 (3) (2018) 30:31–30:57.

[8] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini,
F. Giannotti, D. Pedreschi, A survey of methods for
explaining black box models, ACM computing sur-
veys (CSUR) 51 (5) (2018) 93.

[9] A. Adadi, M. Berrada, Peeking inside the black-box:
A survey on explainable artiﬁcial intelligence (xai),
IEEE Access 6 (2018) 52138–52160.

18

[10] B. Goodman, S. Flaxman, European union regula-
tions on algorithmic decision-making and a right to
explanation, AI Magazine 38 (3) (2017) 50–57.

[11] M. Virgolin, T. Alderliesten, C. Witteveen, P. A. N.
Bosman, Improving model-based genetic program-
ming for symbolic regression of small expressions,
CoRR abs/1904.02050. arXiv:1904.02050.

[12] A. Cano, A. Zafra, S. Ventura, An interpretable clas-
siﬁcation rule mining algorithm, Information Sci-
ences 240 (2013) 1–20.

[13] B. P. Evans, B. Xue, M. Zhang, What’s inside the
black-box?: a genetic programming method for in-
terpreting complex machine learning models,
in:
Genetic and Evolutionary Computation (GECCO
2019), ACM, 2019, pp. 1012–1020.

[21] K. P. Murphy, Naive Bayes classiﬁers, University of

British Columbia 18.

[22] Q. Chen, M. Zhang, B. Xue, Genetic program-
ming with embedded feature construction for high-
dimensional symbolic regression, in: Intelligent and
Evolutionary Systems, Springer, 2017, pp. 87–102.

[23] A. Cano, S. Ventura, K. J. Cios, Multi-objective ge-
netic programming for feature extraction and data
visualization, Soft Computing 21 (8) (2017) 2069–
2089.

[24] M. Virgolin, T. Alderliesten, A. Bel, C. Witteveen,
P. A. N. Bosman, Symbolic regression and feature
construction with GP-GOMEA applied to radiother-
apy dose reconstruction of childhood cancer sur-
vivors, in: Genetic and Evolutionary Computation
(GECCO 2018), ACM, 2018, pp. 1395–1402.

[14] B. Xue, M. Zhang, W. N. Browne, X. Yao, A survey
on evolutionary computation approaches to feature
selection, IEEE Transactions on Evolutionary Com-
putation 20 (4) (2016) 606–626.

[25] B. Tran, B. Xue, M. Zhang, Genetic program-
ming for multiple-feature construction on high-
dimensional classiﬁcation, Pattern Recognition 93
(2019) 404–417.

[15] K. Krawiec, Genetic programming-based construc-
tion of features for machine learning and knowledge
discovery tasks, Genetic Programming and Evolv-
able Machines 3 (4) (2002) 329–343.

[16] L. Breiman, Classiﬁcation and regression trees,

Routledge, 2017.

[17] M. Muharram, G. D. Smith, Evolutionary construc-
tive induction, IEEE Transactions on Knowledge
and Data Engineering 17 (11) (2005) 1518–1528.

[18] B. Tran, B. Xue, M. Zhang, Genetic programming
for feature construction and selection in classiﬁca-
tion on high-dimensional data, Memetic Computing
8 (1) (2016) 3–15.

[19] N. S. Altman, An introduction to kernel and nearest-
neighbor nonparametric regression, The American
Statistician 46 (3) (1992) 175–185.

[20] S. J. Russell, P. Norvig, Artiﬁcial intelligence: a
modern approach, Malaysia; Pearson Education
Limited,, 2016.

[26] C. Cortes, V. Vapnik, Support-vector networks, Ma-

chine learning 20 (3) (1995) 273–297.

[27] L. Breiman, Random forests, Machine learning

45 (1) (2001) 5–32.

[28] R. Kohavi, G. H. John, Wrappers for feature sub-
set selection, Artiﬁcial intelligence 97 (1-2) (1997)
273–324.

[29] M. Virgolin, T. Alderliesten, C. Witteveen, P. A. N.
Bosman, Scalable genetic programming by gene-
pool optimal mixing and input-space entropy-based
building-block learning, in: Genetic and Evolution-
ary Computation (GECCO 2017), ACM, New York,
NY, USA, 2017, pp. 1041–1048.

[30] T. P. Pawlak, B. Wieloch, K. Krawiec, Semantic
backpropagation for designing search operators in
genetic programming, IEEE Transactions on Evolu-
tionary Computation 19 (3) (2015) 326–340.

[31] R. R. Curtin, J. R. Cline, N. P. Slagle, W. B. March,
P. Ram, N. A. Mehta, A. G. Gray, MLPACK: A scal-
able C++ machine learning library, Journal of Ma-
chine Learning Research 14 (2013) 801–805.

19

[42] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-
Fei, Imagenet: A large-scale hierarchical image
database, in: IEEE Conference on Computer Vision
and Pattern Recognition, IEEE, 2009, pp. 248–255.

[43] S. Wold, K. Esbensen, P. Geladi, Principal compo-
nent analysis, Chemometrics and intelligent labora-
tory systems 2 (1-3) (1987) 37–52.

[44] L. v. d. Maaten, G. Hinton, Visualizing data using t-
sne, Journal of Machine Learning Research 9 (2008)
2579–2605.

[32] C.-C. Chang, C.-J. Lin, LIBSVM: A library for
support vector machines, ACM Transactions on In-
telligent Systems and Technology 2 (2011) 27:1–
27:27, software available at http://www.csie.
ntu.edu.tw/~cjlin/libsvm.

[33] M. N. Wright, A. Ziegler, ranger: A fast implemen-
tation of random forests for high dimensional data
in C++ and R, arXiv preprint arXiv:1508.04409.

[34] T. Chen, C. Guestrin, Xgboost: A scalable tree
boosting system, in: Proceedings of the 22nd acm
sigkdd international conference on knowledge dis-
covery and data mining, ACM, 2016, pp. 785–794.

[35] J. Ni, R. H. Drieberg, P. I. Rockett, The use of
an analytic quotient operator in genetic program-
ming, IEEE Transactions on Evolutionary Compu-
tation 17 (1) (2013) 146–152.

[36] D. R. White, J. Mcdermott, M. Castelli, L. Manzoni,
B. W. Goldman, G. Kronberger, W. Ja´skowski, U.-
M. OReilly, S. Luke, Better GP benchmarks: com-
munity survey results and proposals, Genetic Pro-
gramming and Evolvable Machines 14 (1) (2013) 3–
29.

[37] J. Albinati, G. L. Pappa, F. E. Otero, L. O. V.
Oliveira, The eﬀect of distinct geometric seman-
tic crossover operators in regression problems, in:
European Conference on Genetic Programming,
Springer, 2015, pp. 3–15.

[38] J. Demˇsar, Statistical comparisons of classiﬁers over
multiple data sets, Journal of Machine Learning Re-
search 7 (2006) 1–30.

[39] S. Holm, A simple sequentially rejective multiple
test procedure, Scandinavian Journal of Statistics
(1979) 65–70.

[40] Y. LeCun, The mnist database of handwritten digits,

http://yann. lecun. com/exdb/mnist/.

[41] A. Krizhevsky, G. Hinton, Learning multiple layers
of features from tiny images, Tech. rep., University
of Toronto (2009).

20

