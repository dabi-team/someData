1
2
0
2

n
a
J

6
2

]

V
C
.
s
c
[

1
v
5
8
7
0
1
.
1
0
1
2
:
v
i
X
r
a

Developing emotion recognition for video
conference software to support people with autism

Marc Franzen
Faculty E
Ravensburg-Weingarten University
88250 Weingarten, Deutschland

Michael Stephan Gresser
Faculty E
Ravensburg-Weingarten University
88250 Weingarten, Deutschland

Tobias Müller
Faculty E
Ravensburg-Weingarten University
88250 Weingarten, Deutschland

Prof. Dr. Sebastian Mauser
Faculty E
Ravensburg-Weingarten University
88250 Weingarten, Deutschland
sebastian.mauser@rwu.de

Abstract—We develop an emotion recognition software for the
use with a video conference software for autistic individuals which
are unable to recognize emotions properly. It can get an image
out of the video stream, detect the emotion in it with the help
of a neural network and display the prediction to the user. The
network is trained on facial landmark features. The software is
fully modular to support adaption to different video conference
software, programming languages and implementations.

Index Terms—emotion recognition, communication aids, com-

puter science, artiﬁcial intelligence

I. INTRODUCTION

Autism is a spectrum-condition, where the affected person
can have a wide variety of impacts on various abilities. Some
individuals have repetitive and stereotypical interests which
makes them prefer doing recurring and monotonous work.
Other individuals have impacts on social skills, like the ability
to interact and communicate well with other people. [1][2]

Oftentimes, they lack the ability to detect emotions in the
faces of their conversation partners. [3] This makes it difﬁcult
for them to estimate the course of an interview and therefore
we want to support them with our software.

The software recognizes the emotion of a communication
partner during a video conference and displays the result to the
autistic individual. As a result, the software may compensate
for a vital part of the social skillset of an autistic individual.

II. SOLUTIONS

In terms of emotion recognition of a facial image, there are
already different commercial and non-commercial solutions
with different accuracies available.

A. Commercial solution

In the commercial space, Affectiva is a company that
develops emotion measurement technology and offers it as
a service for other companies to use in their products. Their
core product is the AFFDEX algorithm [4], that is used in
Affectiva’s AFFDEX SDK, which is available for purchase on
their website [5]. It is mainly meant for market research, but

it is also used in other environments, such as the automotive
industry to monitor emotions and reactions of a driver. [6]

AFFDEX has already found its way into a similar context
as this project. With the help of AR glasses, children and
adults can be assisted in learning "crucial social and cognitive
skills" with a special focus on emotion recognition. [7] This
also supports our motivation, since many people with autism
have problems understanding emotions.

Also, AFFDEX has a high accuracy in detecting "key

emotions", more precisely in the "high 90th percentile". [8]

As the AFFDEX SDK is a commercial solution, it is not
easily accessible and has a price point of 25000 USD [5].
Therefore, this SDK is not the ideal solution for this research
project and other solutions must be considered.

B. Existing solutions

As an overview, we consult the paper [9], which is sum-
marized in the following paragraphs. It distinguishes between
three different steps: Image Acquisition, Feature Extraction,
Classiﬁcation.

a) Image Acquisition: This step contains the acquisition
of images from various sources, including "a database, a live
video stream or other sources, in 2D or 3D". [9]

The data source can either be static by using still images or

dynamic by using image sequences.

Afterwards, there might be pre-processing applied to the
data. This could be de-noising, scaling and cropping to opti-
mize the data for the next step.

b) Feature Extraction: The extraction of features from
facial data is an essential step, as they describe the "physical
phenomena" [9] on which the detection of facial expressions
is based on.

The better the selection of features and their representation

of the face, the more robust the recognition afterwards is.

The available methods can be grouped into appearance
features, geometric features, a hybrid approach using both and
a template-based approach.

 
 
 
 
 
 
c) Classiﬁcation: In the ﬁnal classiﬁcation step, the de-
tected facial expression is assigned to a predeﬁned expression.
The available classiﬁers can typically be split into para-
metric and non-parametric machine-learning-methods, where
either a predetermined function is given and parameters for
this function are learned or the mapping function is learned
without predetermining the form of a function.

Non-machine-learning methods might be feasible but are

not discussed in this overview-paper.

C. Examples

a) Linear Directional Patterns: A solution referenced in
[9] uses Linear Directional Patterns (LDP) representing the
changes in the image as a histogram, which results in stable
facial features.

These features are then fed into a Support Vector Machine
(SVM) that maps these features to their corresponding emo-
tions.

They state, that they reach an accuracy between 80% and

99% depending on the used parameters and test-scenarios.

b) Active Appearance Model: Another already existing
project uses an Active Appearance Model (AAM) to gather
features which are then used to detect emotions in an image.
[10] The AAM takes the statistical model of a face, represent-
ing its shape and tries to match this model to the image that
is currently processed. More precisely, the model consists of a
set of connected points (landmark coordinates) which are then
iteratively deformed until they ﬁt onto the current image. This
process can be improved by training a model with images and
their respective landmark coordinates. [11]

In that project,

the extracted features are then used to
calculate a mean parameter vector for each of seven emotions.
These mean parameter vectors are then compared to the
Euclidean distance from the face parameters of the current
image. The accuracy of this approach is at around 90% for
the emotions fear, joy, disgust and neutral, but around 60-80%
for surprise, anger and sadness. [10]

D. Proposed solution

Our hypothesis is that the direct matching from faces to
emotions, as well as the manual feature engineering like in the
above solutions with e.g. the Euclidean distance can lead to a
lesser accurate detection than could be theoretically possible.
For similar tasks, neural networks are often considered,
because of their ability to learn relevant correlations in the data
on their own and it could perform better because of additional
learned features.

Because of this, we propose the use of a neural network
as our classiﬁer to match faces to emotions, because they can
learn features, a human would not consider to be relevant in
the face.

To circumvent the problem to match raw image data to
emotions, we take the common approach to use stable features
similar to AAM in [10] as the input to our neural network
which then matches these to the corresponding emotions.
This enables a more diverse prediction, without the need to
distinguish between e.g. genders, skin-color and age.

III. METHOD

TABLE I
MODULES

Module

Description

main

input

Starts the other modules and waits for
them to exit.

Gathers the image in some way and
sends the image-data as .jpg over the
socket.

model

Receives the image-data and constructs
a standardized OpenCV image object.

controller

Receives the OpenCV image object,
performs the emotion recognition and
sends the conclusion as string.

view

Receives the string and displays it on
the screen.

Our solution is divided into four different kinds of modules

(See Tab. I).

Each module runs in a separate process and uses ZeroMQ
for communication. ZeroMQ is a programming-language in-
dependent library, which enables us to open sockets on the
localhost of a machine and then use TCP connections to
exchange data [12]. We implement this as a "Request-Reply"
pattern, where one module requests data by sending "ready"
over the socket, where the other module then sends the data
in the previously deﬁned format. If a module stops, it sends
"done" and quits. As soon as another module receives "done"
instead of "ready", it itself sends the "done" message to its
adjacent module and quits.

By implementing this pattern, we achieve an exchangeabil-
ity, in which we can replace any module from any program-
ming language, which supports OpenCV and ZeroMQ.

A. Module: input

We decide to use the static approach described in II-B for
better performance. For this, the input takes a screenshot using
the mss library [13], converts it to a .jpg format and sends it
as a byte-stream to the model.

B. Module: model

The model expects any image type as a byte-stream and
creates an OpenCV image object from that. To improve the
performance, we scale this image down and thus reduce the
amount of data for further processing. It is then serialized and
forwarded to the controller.

C. Module: controller

The controller module expects a serialized OpenCV image
from the model. It then de-serializes it and starts processing it
in two steps, as shown in ﬁgure 1 and outlined in the following
paragraphs.

ference is going well. Therefore anger and surprise are less
important.

So, for the scope of this project we decide to leave them
out for now and focus on happiness and neutral as facial
expressions. If this works well, it should be expandable to
other emotions with larger datasets.

This leaves us with 11228 images for training and 382 for

validation.

TABLE II
NUMBER OF IMAGES FOR EACH LABEL

Emotion

Training

Validation

Importance

Anger

Contempt

Disgust

Fear

Happiness

Neutral

Sadness

Surprise

169

9

12

11

4961

6267

116

288

45

-

-

-

191

191

-

49

medium

low

low

low

high

high

low

medium

Fig. 1. Steps for detecting emotions. Photo of face from dataset [14]

facial

1) Facial

landmark detection: For

landmark-
detection we use the technique from [15] that is included in the
dlib library [16] and adapted the implementation from [17],
which is using the pre-trained detector inside the library. We
choose this method from the summary [18] because it incor-
porates the face detection and the facial landmark detection in
a single solution while providing real-time capabilities and an
open source.

2) Emotion detection: From the gathered facial landmark
points, we use artiﬁcial intelligence as a mapping function
between the facial landmark features and the given emotion.
The neural network learns the correlation between certain
points of a face and the corresponding emotion while ignoring
irrelevant points. As an example, humans would rate, that the
position of the tip of the nose does not correspond as strongly
to emotions as the corners of the mouth do.

a) Dataset: We use the Dataset "facial_expressions" [14]
from this challenge [19]. This dataset has 13718 images,
each labeled manually by the submitter with one of 8 facial
expressions: anger, contempt, disgust, fear, happiness, sadness,
surprise and neutral. We choose this dataset because it contains
natural images of celebrities, as these reﬂect real emotions
better than datasets crafted with actors. For better training
we remove the data submitted by "jhamski" and "628" as
these images vary in resolution and color space and are
sometimes distorted. 456 images were not included into the
dataset because the chosen facial landmark recognition could
not detect a face in them. We also move random pictures of
every category from the training dataset to a validation set to
compare this solution against others (see Section IV). These
images are not used to train the network.

The exact composition of the full possible training and

validation sets is shown in table II.

Our ﬁnal dataset consists of 12309 gray-scale images, each
with a resolution of 350x350 pixels. The composition is
unevenly distributed and thus not suitable for training neural
networks. This is veriﬁed through initial tests, where all classes
with small amounts of images are ignored in any predictions.
However, we determine, that the emotions contempt, dis-
gust, fear and sadness are not important in a business video
call. In most cases, happiness and neutral indicate if a con-

b) Training: For training we use two common imple-
mentations of neural networks and some variation in the fed
data which results in three total tested solutions.

We use TensorFlow [20] with Keras [21] as the framework

to create all neural networks.

Fully-Connected Neural Network:
The ﬁrst tested network is a dense neural network (DNN)
to create a multi-layer perceptron (MLP). It has the following
architecture:

• Input Layer with X- & Y-Coordinates for each of the 68

detected facial landmarks (136 inputs)

• Dense hidden Layer with 1024 neurons, followed by a

dropout layer with rate 0.5

• Dense hidden Layer with 512 neurons, followed by a

dropout layer with rate 0.5

• Dense hidden Layer with 256 neurons, followed by a

dropout layer with rate 0.5

• Dense output layer with a node for each of the 2 possible

emotions

As this model hits a plateau between 75% and 80% accuracy
on the training and validation dataset, we suspect this could be
caused by the changing absolute coordinates between images.
So as a second attempt we add additional features and convert
all absolute coordinates to relative ones.

Fully-Connected Neural Network with modiﬁed features:
As the most relevant changes happen relative to the center
of the respective portion of the face, the center point of each
portion is calculated. This was added as an additional feature
and all points of this portion were added relative to that center.
We decide to split the face in 4 parts: mouth, nose, left
eye, right eye. The eyebrows are given from the center of the
corresponding eye, because we think that the movement of

Facial LandmarkRecognitionDNNDNNDNNDNNCNNCNNCalculate Relative Coordinates3 options„happiness“an eyebrow relative to the eye (raising eyebrows) is the most
signiﬁcant change in an emotion.

Additionally, the positions of the landmarks depend on the
shape of a face. As an accommodation, we add the width,
height and the center point of the face outline as features for
the neural network. With these, the network can potentially
predict the emotion independently from the geometry of the
face.

This modiﬁed dataset uses the same DNN architecture with
the only difference being the input layer to accommodate for
the additional features.

This yields slightly better results on both datasets, as later

discussed in IV.

Convolutional Neural Network:
Convolutional Neural Networks (CNN) are usually used for
image recognition, as they can make use of the spatial position
of features. [22] So we think this concept might be applied
here, since the position of each facial landmark can correspond
to certain emotions.

For the dataset to be used with a CNN, all points of each
image were projected into a 350x350 matrix, initially ﬁlled
with zeros, where each added ’1’ corresponds to a facial
landmark position.

The CNN has the following architecture:
• Input layer with a 350x350 matrix with a depth of 1 for
the single channel representing either a facial landmark
at the given point or not

• 2D-Convolution layer with 32 neurons and a kernel size

of 3

• Maximum 2D-Pooling layer with a size of 2x2
• Dropout layer with a rate of 0.25 to combat overﬁtting
• Flattening layer to prepare the data for the fully connected

layer

• Dense output layer with a node for each of the 2 possible

emotions

To combat overﬁtting, additionally to the dropout layer, we
use data augmentation since in this case it is easy to ﬂip each
matrix horizontally to get twice as many data rows as before.

Common parameters:
All neural networks were trained using the optimizer Adam
[23] with a learning rate of 1 ∗ 10−6. The used activation
function was always Rectiﬁed Linear Unit (relu), except for
the output layer, which used softmax. The range of the full
width and height (350px each) of all coordinates for the DNNs
was mapped to values from 0 to 1.
Model architecture creation:
For a basis on which we can improve on, we use the model
architectures provided by the Keras framework as examples
[24].

With the Multilayer Perceptron and Convolutional Neural
Network as starting points for our further development, we
adapt them to our data format and reach the ﬁnal presented
networks by training, testing and altering the models.

The base remains conceptually the same, with minor modi-
ﬁcations, like adding or removing layers and adjusting param-
eters, like the neuron-count or the dropout rate.

c) Use: With the trained model, we run inference on the
solution. The trained model is simply loaded using the Keras
framework and inference is called using the given methods
provided by the Keras Model class.

The result of this inference is then sent as a string for the

emotion to the view module.

D. Module: view

The view accepts any kind of string and displays it in a
window on the screen. In future work, there could be an
implementation of this bachelor’s thesis [1].

IV. RESULTS

To evaluate the software, we deﬁne the following ofﬁce
scenario: The opponent has a webcam with a resolution of
720p30 and the user has a PC with a recent (as of 2019, 9th
Generation) Intel Core i5 processor.

To compare our solution with a representative of the
commercial sector, we selected the AFFDEX algorithm. It
is implemented in the demo-program "AffdexMe" [25]. We
install it on the same machine we use for our own solution.
Instead of the webcam input we use a stream of the desktop to
simulate how AffdexMe would perform with the same input.

A. Performance

As AffdexMe does not provide interfaces for time mea-
surement, we use a camera with a high framerate to count
the milliseconds passed between the image being visible for
AffdexMe and the program detecting the face and emotion.

In our solution, we also measure the time between the image
reaching the controller module and the detected emotion leav-
ing the controller module for the different types of networks.
All tests are executed on the same machine with an Intel

Core i5 5500U and 8 GB of RAM.

The performance of each software varies greatly with the
input resolution. For AffdexMe the input-images have a resolu-
tion of 525 x 525 pixels, while our software takes a screenshot
with a resolution of 1366 x 768 pixels.

This has to be taken into consideration when evaluating the

results in table III.

TABLE III
PERFORMANCE

Average

Standard Deviation

DNN

737,1ms

Modiﬁed DNN

727,4ms

CNN

AffdexMe

767,8ms

367,8ms

35,3ms

25,5ms

40,7ms

116,9ms

B. Precision

We compare all of our 3 optional solutions with each other
and the solution from Affectiva [4]. We separated random
images from the dataset for validation, based on our rating of
importance of each emotion for a business video conference.

Fig. 2. DNN: Accuracy on training and validation data over 5000 epochs

Fig. 3. CNN: Accuracy on training and validation data over 500 epochs

a) Metrics: We use two metrics to evaluate the results.
For one, we collect the certainty for the emotions in AffdexMe
and our solutions that correspond to the labeled emotions of
the dataset and calculate the average per emotion. As Affectiva
does not have a "neutral" emotion, we use the strongest
detected emotion and calculate the reciprocal value. This is
shown in table IV.

TABLE IV
CERTAINTY

Happiness

Neutral

Total

Images tested

191

191

382

DNN

69,48%

75,04% 72,26%

Modiﬁed DNN

72,39%

76,79% 74,59%

CNN

Affectiva

55,27%

52,36%

62,46% 58,87%

26,54% 39,45%

As the other metric we calculate the accuracy of the pre-
dictions by each solution and AffdexMe. This is achieved by
measuring the percentage of all correctly classiﬁed emotions
on our dataset for validation. These results are shown in table
V.

TABLE V
ACCURACY

Happiness

Neutral

Total

Images tested

191

191

382

DNN

74,35%

86,91% 80,63%

Modiﬁed DNN

78,01%

86,39% 82,20%

CNN

Affectiva

58,64%

59,26%

75,39% 67,02%

52,15% 55,70%

Both versions increase steadily on the training dataset, while
on the validation dataset it has more variation. The training
accuracy increases faster with modiﬁed features, but they get
to an equal growth towards the end.

With modiﬁed features it performs better overall, but only
by a small margin. We suspect this is because the relative
features have a slightly more relevant changes in their values
and it might also be caused by the additional width and height
features.

The training begins to plateau at about 3000 epochs. After
that it only increases slowly. They might not be fully ﬁtted yet,
but are close to being fully trained, where it does not increase
anymore or even decreases.

As the modiﬁed features train faster and get better overall
results, this might be the preferred option when continuing to
develop these methods further.

Like the DNN,

the CNN continues improving over the
whole 500 trained epochs and also does not reach a point yet
where any accuracy starts to decrease. From this we can also
argue, that with more training time these results can be further
improved. As training on the CNN takes 72 to 73 seconds per
epoch, it is stopped at 500 epochs, which is equivalent to 10
hours of training time, because of the time constraint of this
project.

We suspect

it would get better quickly on the training
dataset but would stay in the same range on the validation
dataset, as it already plateaus early on and increases only by
a small margin towards the end.

As it is shown in ﬁgure 3 the accuracy of the CNN is after
1/6th of the steps already more accurate on the training dataset.
At the end of our training it is less accurate on the validation
data than the DNN at its end.

To perform similar or better on the validation set than the
DNNs, we think the CNNs architecture would need some
modiﬁcation and ﬁne tuning.

b) History of accuracy while training: The DNNs are
trained over 5000 epochs. The accuracy on both the training
and validation datasets are shown in ﬁgure 2 over time.

c) Discussion: As the DNN is smaller in ﬁle size of the
model and is faster for inference on a single image, this is the
preferred type for our solution for now, based on these results.

50%55%60%65%70%75%80%85%150110011501200125013001350140014501AccuracyEpochDNN training accuracyDNN validation accuracyModified DNN training accuracyModified DNN validation accuracy50%55%60%65%70%75%80%85%90%1101201301401AccuracyEpochCNN training accuracyCNN validation accuracyAs already mentioned, we choose the DNN with relative
feature points, as it is slightly better. The time for calculating
relative coordinates from the absolute ones is negligible as the
few needed array operations lay in the margin of error when
measuring runtime.

Although the results show, that we do not reach accuracies
on the validation-dataset in the higher 90th percentile, like the
shown solutions in II-C, we think that our solution has the
beneﬁt of using natural emotions for training.

We see many of the existing solutions use images of acted
emotions, while our solution is trained on non-acted, everyday
images of famous people.

To support

this claim, we conducted several real-world
tests with our solution in different conditions. They show a
subjectively higher accuracy than our validation dataset.

Additionally, our solution cannot be easily compared with
many other solutions, as we do not use the same datasets for
validation. With different images, our accuracy can improve
signiﬁcantly when compared to the other solutions.

Finally, our solutions are more accurate on our chosen
dataset than "AffdexMe", but also slightly slower. As already
mentioned, this is due to the different resolutions.

Also, based on our experience using the AFFDEX algo-
rithm, we think that it beneﬁts from a stream of moving
images. However, we use static images to test it to be able
to compare the results to our solution.

Furthermore, our solution has the beneﬁt compared to
AFFDEX, that we are just training on the relevant emotions,
while their solution is inﬂuenced by additional emotions which
can result in more false predictions.

V. FUTURE IMPROVEMENTS

a) Multi-modal information: Because our social interac-
tion is not just based on visual feedback alone, we propose
the idea of combining several sources of human reactions as
a result of the current emotional state.

This might improve the overall precision and conﬁdence in

the recognition system.

In case of video conference software, voice data might be
taken into consideration. This was already evaluated in more
detail in [26] and [27].

Additionally, chat messages can reﬂect the current emo-
tional state and might also be used as another source of
information, as discussed in [28].

These sources might be used as a starting point for further

investigation.

b) Adaption to video conference software: Because this
project is built with modularity in mind, it is easy to provide
adaptions for various video conference software and different
User-Interfaces like the one presented in [1].

c) Larger Dataset: Our dataset consists of 12309 images
with labels. As the labels are distributed very unequally,
future work can concentrate on gathering a larger and more
evenly distributed dataset to improve the detection of the less
important emotions as well as helping the classiﬁer to distinct
between similar emotions like anger and disgust.

VI. CONCLUSION

We conclude that this software is a big step forward for the

effective employment of individuals with autism.

Although our solution is limited to two distinct emotions,
we think it can already help such individuals to achieve the
everyday task of video conferences in an ofﬁce environment.
And, as we stated before, with more data and further

learning this can be improved to include more emotions.

It might be a good approach before deploying such a
software, to discuss how this affects the emotional climate in
the ofﬁce, as well as what the effects of a wrong classiﬁcation
could be, if decisions are based on these judgements.

Overall, it is a helpful software that could already be used
in its current state when all other hurdles about privacy,
fairness and the possible impacts of misclassiﬁcations can be
overcome.

REFERENCES
[1] S. S. Bobek, User Experience Design für Anwendungen
mit Künstlicher Intelligenz, Doggenriedstraße, 88250
Weingarten, Germany, 2019.

[2] Auticon. (2019). “Autismus | auticon erklärt wie wir das
Autismus-Spektrum sehen,” [Online]. Available: https:
//auticon.de/autismus/ (visited on Aug. 31, 2019).
[3] M. Uljarevic and A. Hamilton, “Recognition of emo-
tions in autism: A formal meta-analysis,” Journal of
autism and developmental disorders, vol. 43, no. 7,
pp. 1517–1526, 2013.

[4] D. McDuff, A. Mahmoud, M. Mavadati, M. Amr, J.
Turcot, and R. e. Kaliouby, “Affdex sdk: A cross-
platform real-time multi-face expression recognition
toolkit,” in Proceedings of the 2016 CHI Conference
Extended Abstracts on Human Factors in Computing
Systems, ser. CHI EA ’16, San Jose, California, USA:
ACM, 2016, pp. 3723–3726, ISBN: 978-1-4503-4082-
3. DOI: 10.1145/2851581.2890247. [Online]. Available:
http://doi.acm.org/10.1145/2851581.2890247.

[5] Affectiva. (2019). “Pricing – Affectiva Developer Por-
tal,” [Online]. Available: https : / / developer . affectiva .
com/pricing/ (visited on Aug. 31, 2019).

[6] D. McDuff, R. El Kaliouby, J. F. Cohn, and R. W.
Picard, “Predicting ad liking and purchase intent: Large-
scale analysis of facial responses to ads,” IEEE Transac-
tions on Affective Computing, vol. 6, no. 3, pp. 223–235,
2014.

[7] Affectiva. (2019). “Success Stories - Brain Power -
Autism Support,” [Online]. Available: https : / / www .
affectiva . com / success - story / brain - power/ (visited on
Aug. 25, 2019).

[8] Affectiva. (2019). “Determining Accuracy,” [Online].
Available: https://developer.affectiva.com/determining-
accuracy/ (visited on Sep. 23, 2019).

[25] Affectiva. (2019). “AffdexMe - Affectiva Developer
Portal,” [Online]. Available: https://developer.affectiva.
com/affdexme/ (visited on Sep. 18, 2019).

[26] Z.-J. Chuang and C.-H. Wu, “Multi-modal emotion
recognition from speech and text,” in International
Journal of Computational Linguistics & Chinese Lan-
guage Processing, Volume 9, Number 2, August 2004:
Special Issue on New Trends of Speech and Language
Processing, 2004, pp. 45–62.

[27] L. De Silva, T. Miyasato, and R. Nakatsu, “Facial
emotion recognition using multi-modal information,” in
Proceedings of the IEEE Intelligent Conf. Information,
Comm. And Signal Processing, vol. 1, Oct. 1997, 397–
401 vol.1, ISBN: 0-7803-3676-3. DOI: 10.1109/ICICS.
1997.647126.

[28] C.-H. Wu, Z.-J. Chuang, and Y.-C. Lin, “Emotion
recognition from text using semantic labels and sep-
arable mixture models,” ACM transactions on Asian
language information processing (TALIP), vol. 5, no. 2,
pp. 165–183, 2006.

[9] C.-D. C˘aleanu, “Face expression recognition: A brief
overview of the last decade,” in 2013 IEEE 8th Inter-
national Symposium on Applied Computational Intelli-
gence and Informatics (SACI), IEEE, 2013, pp. 157–
161.

[10] M. S. Ratliff and E. Patterson, “Emotion recognition us-
ing facial expressions with active appearance models,”
in Proc. of HRI, Citeseer, 2008.

[11] T. F. Cootes, G. J. Edwards, and C. J. Taylor, “Active
appearance models,” in European conference on com-
puter vision, Springer, 1998, pp. 484–498.

[12] The ZeroMQ authors. (2019). “ZeroMQ | Get started,”
[Online]. Available: https : / / zeromq . org / get - started/
(visited on Sep. 3, 2019).

[13] T2. (2019). “mss · PyPI,” [Online]. Available: https :
//pypi.org/project/mss/ (visited on Oct. 4, 2019).

[14] B. L. Y. Rowe.

“GitHub

- muxs-
(2019).
pace/facial_expressions: A set of images for classifying
facial
:
//github.com/muxspace/facial%5C_expressions (visited
on Sep. 1, 2019).

[Online]. Available:

expressions,”

https

[15] V. Kazemi and J. Sullivan, “One millisecond face
alignment with an ensemble of regression trees,” in
Proceedings of the IEEE conference on computer vision
and pattern recognition, 2014, pp. 1867–1874.
[16] D. E. King. (Sep. 2019). “dlib C++ Library,” [Online].
Available: http://dlib.net/ (visited on Oct. 3, 2019).

[17] A. Rosebrock. (2019). “Facial

landmarks with dlib,
OpenCV, and Python,” [Online]. Available: https : / /
www . pyimagesearch . com / 2017 / 04 / 03 / facial -
landmarks - dlib - opencv - python/ (visited on Aug. 31,
2019).

[18] Y. Wu. (Dec. 2016). “List of Code,” [Online]. Available:
https://www.ecse.rpi.edu/~cvrl/CodeList.pdf (visited on
Aug. 31, 2019).

[19] Kaggle Inc. (2019). “Emotion Detection From Facial
Expressions,” [Online]. Available: https://www.kaggle.
com / c / emotion - detection - from - facial - expressions /
overview (visited on Sep. 1, 2019).

[20] TensorFlow. (2019). “TensorFlow,” [Online]. Available:

https://www.tensorﬂow.org/ (visited on Oct. 3, 2019).

[21] Keras Documentation.

(2019). “Keras: The Python
Deep Learning library,” [Online]. Available: https : / /
keras.io/ (visited on Oct. 3, 2019).

[22] Wikipedia.

(2019). “Convolutional neural network,”
[Online]. Available: https : / / en . wikipedia . org / wiki /
Convolutional _ neural _ network (visited on Sep. 22,
2019).

[23] D. P. Kingma and J. Ba, “Adam: A method for stochas-
tic optimization,” arXiv preprint arXiv:1412.6980,
2014.

[24] Keras Documentation. (2019). “Guide to the Sequential
model - Examples,” [Online]. Available: https://keras.
io / getting - started / sequential - model - guide / #examples
(visited on Sep. 30, 2019).

