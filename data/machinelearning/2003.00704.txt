Stochastically Differentiable Probabilistic Programs

0
2
0
2

r
a

M
5

]

G
L
.
s
c
[

2
v
4
0
7
0
0
.
3
0
0
2
:
v
i
X
r
a

David Tolpin
Ben-Gurion University
of the Negev

Yuan Zhou
University of Oxford

Hongseok Yang
Korea Advanced Institute
of Science and Technology

Abstract

Probabilistic programs with mixed support
(both continuous and discrete latent random
variables) commonly appear in many proba-
bilistic programming systems (PPSs). How-
ever, the existence of the discrete random vari-
ables prohibits many basic gradient-based in-
ference engines, which makes the inference
procedure on such models particularly chal-
Existing PPSs either require the
lenging.
user to manually marginalize out the discrete
variables or to perform a composing infer-
ence by running inference separately on dis-
crete and continuous variables. The former
is infeasible in most cases whereas the lat-
ter has some fundamental shortcomings. We
present a novel approach to run inference ef-
ﬁciently and robustly in such programs us-
ing stochastic gradient Markov Chain Monte
Carlo family of algorithms. We compare our
stochastic gradient-based inference algorithm
against conventional baselines in several im-
portant cases of probabilistic programs with
mixed support, and demonstrate that it out-
performs existing composing inference base-
lines and works almost as well as inference
in marginalized versions of the programs, but
with less programming effort and at a lower
computation cost.

1

Introduction

Probabilistic programming [8, 11, 26, 7] represents sta-
tistical models as programs written in an otherwise gen-
eral programming language that provides syntax for the
deﬁnition and conditioning of random variables.
In-
ference can be performed on probabilistic programs to

obtain the posterior distribution or point estimates of
the variables. Inference algorithms are provided by the
probabilistic programming framework, and each algo-
rithm is usually applicable to a wide class of probabilis-
tic programs in a black-box automated manner. The al-
gorithms include Markov Chain Monte Carlo (MCMC)
variants — Metropolis-Hastings [24, 11, 27], Hamilto-
nian Monte Carlo (HMC) [14, 2], as well as expecta-
tion propagation [12], extensions of Sequential Monte
Carlo [26, 23, 15, 17, 13], variational inference [25, 9],
and gradient-based optimization [2, 1].

A probabilistic program computes the (unnormalized)
probability of its execution [26]. An execution is sum-
marized as an instantiation of the program trace, and
the probability is a function of the trace. Some proba-
bilistic programming frameworks require that the trace
shape be speciﬁed upfront, i.e.
static [2, 21], while
others allow introduction of trace components dynami-
cally [8, 16, 7, 5, 22], in the course of a program execu-
tion.

Efﬁcient inference methods for probabilistic programs,
especially in the high-dimensional scenario, often in-
volve computation of the gradient with respect to the la-
tent variables [19, 2, 5, 21, 1]. This restricts gradient-
based inference methods to differentiable programs with
continuous variables only. Probabilistic programs with
a mixture of continuous and discrete variables, such as
mixture models, state space models, and factor models,
must resort to alternatives which do not rely solely on
differentiability, at the cost of lower performance and
poorer scalability.

Take a standard Gaussian mixture model (GMM) as an
example. In GMM, the continuous latent variables are
usually the mean and variance of the Gaussian distribu-
tion of each mixture component, and the discrete vari-
ables are the assignments of each data point to a mixture
component. Inference in GMM can be performed by ei-
ther manually marginalizing out the discrete latent vari-

 
 
 
 
 
 
ables, or by combining conditional gradient-based infer-
ence on continuous variables, and conditional gradient-
free (such as Gibbs sampling) inference on discrete vari-
ables. Manual marginalization is straightforward in the
case of GMM, as suggested by Stan [19], but can be more
complex in most other cases, and even intractable. Con-
ditional inference on each group of variables is prone to
slow mixing and poor robustness in the face of multi-
modality. A generic robust inference method which both
exploits differentiability and is applicable to probabilistic
programs with mixed support is highly desirable.

In this work, we focus on the probabilistic programs with
a mixture of continuous and discrete variables, differ-
entiable with respect to the continuous variables, and
propose to treat them as stochastically differentiable.
We derive an unbiased stochastic estimate of the gra-
dient of the marginal log likelihood that can be efﬁ-
ciently computed. We then demonstrate how one can ap-
ply stochastic gradient-based inference methods [10], in
particular stochastic gradient Hamiltonian Monte Carlo
(sgHMC) [3], on stochastically differentiable models
with this estimate. To show the potential usage, a ref-
erence implementation of the probabilistic programming
facility that supports stochastically differentiable proba-
bilistic programs is presented in Section 4. We compare
our proposed adaptation of sgHMC on three stochasti-
cally differentiable models against manually marginal-
ized inference and the state-of-the-art automated method
in PPSs, composing inference, and empirically conﬁrm
the substantial improvements of our approach.

Contributions This work brings the following contri-
butions:

• the notion of a stochastically differentiable proba-

bilistic program;

• an unbiased stochastic estimator for the gradient of
the marginal log likelihood of such a program;

• an adaptation of sgHMC with this estimator as the

automated inference engine;

• a reference implementation of probabilistic pro-
gramming with support for stochastically differen-
tiable probabilistic programs in Go programming
language [6].

Notation We denote by p(xxx) the probability or prob-
ability density of random variable xxx, and by p(xxx|yyy) the
conditional probability or probability density of xxx given
yyy, depending on the domain of xxx. We write xxx ∼ p(·)
when xxx is a random variable with probability p(·). We
denote by ˜p(·) a probability known up to a normalization
constant, i.e. the unnormalized probability.

2 Stochastically Differentiable

Probabilistic Program

To deﬁne a stochastically differentiable probabilistic pro-
gram, we begin with deﬁnitions of a deterministic and
a stochastic probabilistic program. Different deﬁnitions
of (deterministic) probabilistic programs are given in the
literature and reﬂect different views and accents. For the
purpose of this work, let us give the following broad def-
inition:
Deﬁnition 1. A deterministic probabilistic program Π
that deﬁnes a distribution over traces p(xxx|yyy) is a com-
puter program that accepts a trace assignment xxx as one
of its arguments, and returns the unnormalized probabil-
ity of xxx conditioned on other arguments yyy:

Π(xxx, yyy) ⇒ ˜p(xxx|yyy).

(1)

The trace assignment xxx may have the form of a vector,
or of a list of address-value pairs, or any other form suit-
able for a particular implementation. Accordingly, let us
deﬁne a stochastic probabilistic program:
Deﬁnition 2. A stochastic probabilistic program Ξ that
deﬁnes a distribution over traces p(xxx|yyy) is a computer
program that accepts a trace assignment xxx as one of its
arguments, and returns the unnormalized probability of
xxx conditioned on other arguments yyy and random variable
zzz conditioned on yyy:

zzz ∼ p(zzz|yyy)
Ξ(xxx, yyy) ⇒ ˜p(xxx|yyy, zzz).

(2)

A rationale for this deﬁnition is that zzz corresponds to the
nuisance parameters or nondeterministic choices inside
the program. Finally, let us deﬁne a stochastically differ-
entiable probabilistic program:

Deﬁnition 3. A stochastically differentiable probabilis-
tic program Ξ is a stochastic probabilistic program (Def-
inition 2) with trace xxx ∈ Rn such that ˜p(xxx|yyy, zzz) is differ-
entiable w.r.t. xxx for any yyy and zzz.

Let us illustrate a stochastic (and stochastically differen-
tiable) probabilistic program on an example:

Example 1. A survey is conducted among a company’s
employees. The survey contains a single question: “Are
you satisﬁed with your compensation?” To preserve em-
ployees’ privacy, the employee ﬂips a coin before an-
swering the question. If the coin shows head, the em-
ployee answers honestly; otherwise, the employee ﬂips
a coin again, and answers ‘yes’ on head, ‘no’ on tail.
Based on survey outcomes, we want to know how many
of the employees are satisﬁed with their compensations.

:= 0;

i++ {

i != len(y );

coin := Bernoulli (0.5). sample()
if coin {

1 func Survey( theta ﬂoat , y [] bool) ﬂoat {
prob := Beta (1, 1). pdf( theta )
2
for i
3
4
5
6
7
8
9
10
11
12

}
return prob

prob ∗= Bernoulli (0.5). cdf(y[ i ])

} else {

prob ∗= Bernoulli ( theta ). cdf(y[ i ])

}

}

(a) A probabilistic program for the compensation survey.
Random variables coin at line 4 are the nuisance variables
which make the program stochastic.

:= 0;

i != len(y );

1 func Survey( theta ﬂoat , y [] bool) ﬂoat {
prob := Beta (1, 1). pdf( theta )
2
for i
3
4
5
6
7
8 }

}
return prob

+0.5∗Bernoulli (0.5). cdf(y[ i ])

prob ∗= 0.5∗ Bernoulli ( theta ). cdf(y[ i ])

i++ {

(b) An equivalent deterministic version of the compen-
sation survey model in Figure 1a. The probability is
marginalized over the coin ﬂip in the code of the program.

Figure 1: Compensation survey model

Stochastic probabilistic program The stochastic
probabilistic program modelling the survey (Figure 1a)
receives two parameters: probability theta that a ran-
domly chosen employee is satisﬁed with the compensa-
tion and vector y of boolean survey outcomes. The pro-
gram trace consists of a single variable theta . There are
nuisance random variables coin representing coin ﬂips
which are sampled inside the program from their prior
distribution, but are not included in the trace — this
makes the probabilistic program stochastic. The unnor-
malized probability of the trace is accumulated in vari-
able prob. First, a Beta prior is imposed on theta (line
2). Then, prob is multiplied by the probability of each
answer given theta and coin (lines 5–9).

Manual marginalization Though not generally the
case, the program in Figure 1a can be rewritten as a de-
terministic probabilistic program (Figure 1b). Instead of
ﬂipping a coin as in line 4 of the stochastic program in
Figure 1a, the probability of observation y[ i ] is com-
puted in lines 5–6 as the sum of probabilities given either
head or tail, weighted by the probabilities of head and
tail (both are 0.5). For case studies (Section 4), we chose
probabilistic programs for which manual marginalization
was possible, and compared inference in stochastic and
marginalized versions.

subsampling — computing the gradient on a small part
of the dataset [3], but stochastic gradient MCMC meth-
ods are agnostic to the way the gradient is estimated,
given an unbiased estimate is available, and can as well
be applied to stochastically differentiable probabilistic
programs. However, a naive estimate of the gradient as
∇xxx log ˜p(xxx|yyy, zzz), where zzz is drawn from p(zzz|yyy), is a bi-
ased estimate of the gradient of ∇xxx log ˜p(xxx|yyy).

In what follows, we derive an unbiased estimate of the
stochastic gradient, and elaborate on the use of the esti-
mate within the framework of stochastic gradient Hamil-
tonian Monte Carlo (sgHMC).

3.1 Unbiased Stochastic Gradient Estimate

The unnormalized probability density ˜p(xxx|yyy) is a
marginalization of ˜p(xxx|yyy, zzz) over zzz:

˜p(xxx|yyy) =

(cid:90)

z

p(zzz|yyy)˜p(xxx|yyy, zzz)dzzz

(3)

Posterior inference and maximum a posteriori estimation
require a stochastic estimate of ∇xxx log ˜p(xxx|yyy):

3

Inference

∇xxx log ˜p(xxx|yyy) =

Efﬁcient posterior inference in probabilistic models for
which a stochastic gradient estimate of the logarithm
of the unnormalized posterior density is available can
be performed using a stochastic gradient Markov Chain
Monte Carlo method [10]. Stochastic gradient poste-
rior inference was originally motivated by the handling
of large data sets, where the gradient was estimated by

=

=

=

z p(zzz|yyy)˜p(xxx|yyy, zzz)∇xxx log ˜p(xxx|yyy, zzz)dzzz
˜p(xxx|yyy)

p(zzz|yyy)

˜p(xxx|yyy, zzz)
˜p(xxx|yyy)
p(xxx|yyy, zzz)p(zzz|yyy)
p(xxx|yyy)

∇xxx log ˜p(xxx|yyy, zzz)dzzz

∇xxx log ˜p(xxx|yyy, zzz)dzzz

p(zzz|xxx, yyy)∇xxx log ˜p(xxx|yyy, zzz)dzzz

(4)

(cid:82)

(cid:90)

z
(cid:90)

z
(cid:90)

z

By Monte Carlo approximation,

zzzi ∼ p(zzz|xxx, yyy)

∇xxx log ˜p(xxx|yyy) ≈

1
N

N
(cid:88)

i=1

∇xxx log ˜p(xxx|yyy, zzzi)

(5)

Draws of zzzi are conditioned on xxx and can be approxi-
mated by a Monte Carlo method, such as Markov chain
Monte Carlo; p(zzz|yyy) need not be known for the approxi-
mation. Note that there are two Monte Carlo approxima-
tions involved:

1. gradient ∇xxx log ˜p(xxx|yyy) is approximated by a ﬁnite

sum of gradients ∇xxx log ˜p(xxx|yyy, zzzi);

2. draws of zzzi from p(zzz|xxx, yyy) are approximated.

An intuition behind estimate (5) is that for marginaliza-
tion, assignments to nuisance parameters which make the
trace assignment more likely contribute more to the gra-
dient estimate.

3.2

Inference with Stochastic Gradient HMC

Stochastic gradient Hamiltonian Monte Carlo [3] does
not involve Metropolis-Hastings correction and requires
only that a routine that computes a stochastic estimate
of the gradient of the unnormalized posterior probability
density should be provided. We propose here to use a
single-sample gradient estimate, where zzz is drawn using
an MCMC method, such as a Gibbs sampler or a variant
of Metropolis-Hastings:

Algorithm 1: Single-sample gradient estimate

1. Draw zzz from p(zzz|xxx, yyy) (MCMC).

2. Return ∇xxx log ˜p(xxx|yyy, zzz).

Note that zzz is freshly drawn for each estimation of the
gradient. Hence, every step through xxx is performed using
a gradient estimate based on a new sample of zzz.

Variance of this stochastic gradient estimate can be re-
duced by computing and averaging multiple estimates,
each for a new draw of zzz. This is similar to the well-
known mini-batch stochastic gradient descent in stochas-
tic optimization. In the case studies (Section 4), we com-
pare statistical efﬁciency and computation time of infer-
ence with the single-sample and multi-sample stochastic
gradient estimates.

Compare inference with sgHMC, using Algorithm 1 as
the gradient estimate, to alternating inference, with (non-
stochastic) HMC on xxx and an MCMC variant on zzz:

Algorithm 2: Baseline: alternating HMC on xxx and
MCMC on zzz

1. Draw zzz from p(zzz|xxx, yyy) (MCMC).

2. Draw xxx from p(xxx|yyy, zzz) (HMC).

Despite apparent similarity between Algorithms 1 and 2,
one HMC iteration of Algorithm 2 involves multiple
computations of the gradient for each draw of xxx, all for
the same sample of zzz, as in the Leapfrog steps. A new
sample of zzz is only drawn between iterations of HMC.
This results in poorer mixing and may lead the sampler
to get stuck in one of the modes of a multimodal poste-
rior distribution.

As an illustration, consider the probabilistic program in
Figure 2a. This program speciﬁes a Gaussian mixture
model, p(x) = 0.5p(x|µ = −1, σ = 0.5) + 0.5p(x|µ =
1, σ = 0.5). Nuisance variable z selects either compo-
nent with equal probability. Figure 2b shows densities
of each of the components and of the mixture. For Al-
gorithm 1,
it directly samples from the entire mixture
density, i.e. it re-draws zzz before each gradient step of xxx,
which enables the posterior samples of xxx to cover both
modes. However, the HMC step of Algorithm 2 will con-
ditioned on a ﬁxed zzz, i.e. sample xxx for L leapfrog steps
conditioned on zzz = 1 or 2, which makes the samples
mainly concentrated around one mode. The estimates
of xxx will only evolve to the other mode once zzz changes
based on samples from the current mode, with potentially
low mixing rate.

More evidence for poorer mixing and lower statistical ef-
ﬁciency is provided in the case studies. Compared to
that, Algorithm 1 estimates the gradient for a new draw
of zzz on each invocation.

4 Case Studies

We implemented inference in stochastically differen-
tiable probabilistic programs using Infergo [21], a prob-
abilistic programming facility for the Go programming
language. Due to reliance of Infergo on pure Go code for
probabilistic programs, no changes were necessary to the
way probabilistic programs are represented. Implemen-
tation of sgHMC for the case studies in this paper will be
made publicly available upon publication and proposed
for inclusion into Infergo.

We evaluated inference in stochastically differentiable
probabilistic programs on several models. In each evalu-
ation, we created two probabilistic programs: the stan-
dard probabilistic program with a mixed support as

1 func TwoNormals(x ﬂoat) ﬂoat {
z := Bernoulli (0.5). sample()
2
if z {
3
4
5
6
7
8 }

} else {

}

return Normal(1, 0.5). pdf(x)

return Normal(−1, 0.5). pdf(x)

(a) Mixture of two Gaussians: nuisance variable z selects
either N (−1, 0.5) or 5N (1, 0.5).

(b) Mixture of two Gaussians: HMC conditioned on either
zzz = false or zzz = true will alternate between exploring
each of the components, while sgHMC will directly draw
samples from the posterior.

Figure 2: An illustration example: sgHMC vs. MCMC + HMC

users may write (akin to Figure 1a) and a correspond-
ing marginalized program (akin to Figure 1b). Addi-
tionally, for comparison, we implemented a marginal-
ized program for each model in Stan [2]; we veriﬁed that
the posterior distributions inferred by each of inference
schemes on both mixed support and marginalized pro-
grams are consistent with the Stan version. We provide
results for three models in this study: compensation sur-
vey (Section 4.1), Gaussian mixture model, and hidden
Markov model. Details of each model are provided later
in this section. The full source code, data, and evaluation
scripts for the case studies will be made publicly avail-
able upon publication.

The purpose of these case studies is to compare statisti-
cal efﬁciency and computation time of sgHMC, combi-
nation of MCMC and HMC on mixed-support models,
and HMC on marginalized models. For each model, we
measured, and averaged over 10 independent runs, the
effective sample size and the computation time. Four in-
ference schemes were applied:

• (ours) sgHMC with a single-sample gradient esti-

mate;

• (ours) sgHMC with 10-sample gradient estimate;

• a combination of MCMC (sitewise Metropolis-
Hastings) on discrete variables and HMC on con-
tinuous variables;

• HMC on the marginalized model.

All inference schemes were run to produce 10 000 sam-
ples (sufﬁcient for convergence on all models in the stud-
ies), with 10 steps (gradient estimates) between samples.
The step resulting in the largest effective sample size for
HMC on the marginalized model was chosen and ﬁxed

for all over schemes on the same model. This choice
of the step size favors HMC in comparison, meaning
that the actual comparative performance of sgHMC on
stochastically differentiable probabilistic programs is at
least as good, or better, as what follows from the evalua-
tion.

The measurements are summarized in Tables 1 and 2, in-
cluding empirical standard deviations over multiple runs.
Table 1 shows effective sample sizes. Larger numbers are
better. Effective sample size of HMC on marginalized
program is 15%–20% larger than sgHMC with a single-
sample gradient estimate and 5–7% larger than sgHMC
with 10-sample estimate. The MH+HMC combination
consistently produces sample sizes which are lower by
30–60%. Table 2 shows computation times. While 10-
sample gradient estimate is computationally expensive
and results in almost linear increase in the total computa-
tion time, cancelling the improvement due to a less noisy
gradient estimate, sgHMC exhibits the shortest compu-
tation time for all models, 2.5–4.5 times shorter than
HMC on the marginalized program. This is because
the marginalized version has both a higher asymptotic
complexity (more details are in descriptions of each of
the case studies) and involves computationally expensive
exponentiation (calls to LogSumExp in Figures 3b, 4b,
and 5b).

Table 3 combines Tables 1 and 2 to show approximate
effective sample size in seconds. Despite a slightly
lower total effective sample size of sgHMC, the effec-
tive sample size of sgHMC with a single-sample gra-
dient estimate is 2–4 times larger than that of HMC on
the marginalized model. To summarize, sgHMC with a
single-sample gradient estimate has the best performance
on the mixed support probabilistic programs used in the
case studies, outperforming HMC on marginalized mod-
els by a number of times even for parameters favoring

 0 0.2 0.4 0.6 0.8 1-3-2-1 0 1 2 3sgHMC: p(x)HMC: p(x|z=false)HMC: p(x|z=true)Table 1: Effective sample size

model
Survey
GMM
HMM

sgHMC, 1 sample
4600 ± 180
5900 ± 240
6200 ± 280

sgHMC, 10 samples MH+HMC HMC, marginalized
2800 ± 300
1900 ± 200
4800 ± 190

5200 ± 200
7200 ± 150
6700 ± 180

5000 ± 160
6800 ± 200
6300 ± 220

Table 2: Computation time, seconds

model
Survey
GMM
HMM

sgHMC, 1 sample
6.5 ± 0.1
35 ± 0.7
10 ± 0.3

sgHMC, 10 samples MH+HMC HMC, marginalized
7.4 ± 0.1
38 ± 0.7
10 ± 0.3

21 ± 0.5
95 ± 2
46 ± 0.8

40 ± 1
340 ± 5
98 ± 3

Table 3: Effective sample size per second

model
Survey
GMM
HMM

sgHMC MH+HMC HMC
240
380
75
50
145
480

650
170
620

HMC in the comparison.

4.3 Hidden Markov Model

4.1 Compensation Survey

The compensation survey follows the description in Sec-
tion 4.1. The mixed support program is shown in Fig-
ure 3a and the marginalized version in Figure 3b. For
evaluation, a data set of 60 observations, corresponding
to 67% satisfaction, was used. The marginalized version
has the same asymptotic complexity as the mixed sup-
port version, but should take at least twice as long to run
due to computation of the log probability for both coin
ﬂip outcomes (lines 5–7). This is consistent with the em-
pirical measurements, 21 vs 6.5 seconds, (Table 2), with
extra time spent in LogSumExp.

4.2 Gaussian Mixture Model

The Gaussian mixture model infers means and standard
deviation of the components, given a set of samples from
the mixture and the number of components. The mixed
support program is shown in Figure 4a and the marginal-
ized version in Figure 4b. For evaluation, a data set
of 100 observations, corresponding to two components
with equal probability of either component, was used.
The computation time of the marginalized version is at
least the computation time of the mixed support program
times the number of components (at least twice longer
for two components) due to the inner loop in lines 8–
15. This is consistent with the empirical measurements,
95 vs. 35 seconds (Table 2), with extra time spent in
LogSumExp.

The Hidden Markov model infers the transition matrix
given the emission matrix and the observations. The
mixed support program is shown in Figure 5a and the
marginalized version in Figure 5b. For evaluation, a data
set of 16 observations, corresponding to three distinct
hidden states, was used. Following [20], the marginal-
ized version implements the costly forward pass of the
forward-backward algorithm. The computation time of
the marginalized version, in a naïve implementation, is
at least the computation time of the mixed support pro-
gram times the squared number of hidden states (at least
nine times longer for three hidden states). This is due to
the doubly-nested loop in lines 11–20. The forward pass
also involves two distinct calls to LogSumExp, one in
line 18, on each iteration of the inner loop, and the other
in line 22. The marginalized version in Figure 5b op-
timizes the inner loop by reusing computations, but the
computation time of the marginalized version is still ﬁve
times longer than that of the mixed support version, 46
vs. 10 seconds (Table 2).

5 Related Work

Stochastically differentiable probabilistic programs as
introduced in Section 2, though may be named differ-
ently, commonly appeared in many different probabilis-
tic programming systems (PPSs). However, the mix-
ture of continuous and discrete random variables sub-
stantially complicates the inference procedures in PPS as
automated engines. Especially, the inference procedure

i != len(m.Y); i++ {

Observe(x [] ﬂoat64 ) ﬂoat64 {
:= 0.

:= 0;
if m.Coin[i] {

ll
theta := mathx.Sigm(x[0])
for i

1 func (m ∗StochasticModel)
2
3
4
5
6
7
8
9
10
11
12
13 }

}
return ll

} else {

}

ll += Flip . Logp(0.5, m.Y[i])

ll += Flip .Logp(theta , m.Y[i])

Observe(x [] ﬂoat64 ) ﬂoat64 {
:= 0.

:= 0;

ll
theta := mathx.Sigm(x[0])
for i

1 func (m ∗DeterministicModel)
2
3
4
5
6
7
8
9
10
11 }

ll += mathx.LogSumExp(

}
return ll

i != len(m.Y); i++ {

Flip .Logp(theta , m.Y[i])+math.Log (0.5),
Flip .Logp(0.5, m.Y[i])+math.Log(0.5))

(a) Compensation survey

(b) Compensation survey, marginalized

Figure 3: Code Example for the Compensation Survey

Observe(x [] ﬂoat64 ) ﬂoat64 {
:= Normal.Logps(0, 10, x ...)

ll
for j

:= range m.Mu {

1 func (m ∗StochasticModel)
2
3
4
5
6
7
8
9
10
11
12
13
14 }

}
return ll

m.Mu[j] = x[2∗j ]
m.Sigma[j] = math.Exp(x[2∗j+1])

:= 0;

}
for i
j
ll += Normal.Logp(m.Mu[j], m.Sigma[j],

i != len(m.Data);

:= m.Assgn[i]

i++ {

m.Data[i ])

(a) Gaussian mixture model

i != len(m.Data);

i++ {

Observe(x [] ﬂoat64 ) ﬂoat64 {
:= Normal.Logps(0, 10, x ...)

m.Mu[j] = x[2∗j ]
m.Sigma[j] = math.Exp(x[2∗j+1])

ll
for j

}
for i

:= range m.Mu {

:= 0;
var l ﬂoat64
for j
lj
if

1 func (m ∗DeterministicModel)
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21 }

}
return ll

}
ll += l

}

j != m.NComp; j++ {

:= 0;
:= Normal.Logp(m.Mu[j], m.Sigma[j], m.Data[i ])
j == 0 {
l = lj
} else {

l = mathx.LogSumExp(l, lj)

(b) Gaussian mixture model, marginalized

Figure 4: Code Example for the GMM

can become extremely challenging if the program trace
is not static, i.e. the support of the program is dynamic.

To perform inference in such models in PPSs such as
Stan [2], which are usually designed around one or two
efﬁcient gradient-based inference methods such as HMC,
the user would need to manually marginalize out the dis-
crete nuisance variables since they violate the prerequi-
site of the inference engines. Unfortunately, this is not

feasible in most cases and also adds unnecessary burden
for the user, which violates the principle of automation
in PPS at the ﬁrst place.

Alternative choices may be PPSs such as PyMC3 [19],
Turing.jl [5], and Gen [4] where one can customize dif-
ferent kernels for different variables as composing infer-
ence. For example, one can use the Metropolis-within-
Gibbs sampler for the nuisance variables and HMC for

D.SoftMax(x[i∗m.NStat:(i+1)∗m.NStat],

m.logT[i ][ j ] = math.Log(m.logT[i][ j ])

}

for j

ll
for i

:= range m.logT {

m.logT[i ])
:= range m.logT[i] {

1 func (m ∗StochasticModel)
2 Observe(x [] ﬂoat64 ) ﬂoat64 {
3
:= Normal.Logps(0, 10, x ...)
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20 }

}
noise := math.Exp(m.Noise)
for i

}
return ll

:= range m.Data {

if

}

ll += Normal.Logp(ﬂoat64(m.States [ i ]), noise ,
m.Data[i ])

i != 0 {
ll += m.logT[m.States[ i−1]][m.States [ i ]]

(a) Hidden Markov model

D.SoftMax(x[i∗m.NStat:(i+1)∗m.NStat], m.logT[i ])
for j

m.logT[i ][ j ] = math.Log(m.logT[i][ j ])

}

for j

:= 0;

ll
for i

:= range m.Data {

:= range m.logT {

:= range m.logT[i] {

}
noise := math.Exp(m.Noise)
for i

1 func (m ∗DeterministicModel)
2 Observe(x [] ﬂoat64 ) ﬂoat64 {
3
:= Normal.Logps(0, 10, x ...)
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25 }

i != 0 {
for j_ := 0;

if

}

}

j != m.NStat; j++ {

m.gamma[i][j] = Normal.Logp(ﬂoat64( j ), noise ,

m.Data[i ])

j_ != m.NStat; j_++ {
m.acc[j_ ] = m.gamma[i−1][j_] + m.logT[j_][ j ]

}
m.gamma[i][j] += D.LogSumExp(m.acc)

}
ll += D.LogSumExp(m.gamma[len(m.gamma)−1])
return ll

(b) Hidden Markov model, marginalized

Figure 5: Code Example for the HMM

the remaining continuous ones. It is probably the state-
of-the-art method for this type of models, especially in
the PPSs oriented for the gradient-based inference en-
gines. However, as we have discussed in Section 3 and
empirically conﬁrmed in Section 4, this method has some
fundamental shortcomings and our approach provides
substantial practical improvements.

There are also some other approaches for improving in-
ference performance for this type of models. For ex-
ample, LF-PPL [28] proposed a novel low-level lan-
guage for PPS to incorporate more sophisticated in-
ference techniques for piecewise differentiable models;
Stochaskell [18] provided a way to perform reversible
jump MCMC in PPS; [29] introduced a new sampling
scheme, called Divide, Conquer and Combine, to per-
form inference in the general purpose PPS by dividing
the program into sub-programs and conquering individ-
ual one before combing into an overall estimate. How-
ever, LF-PPL imposes some restrictions to the language
of the PPS that are not required in our setup, whereas
the latter two do not exploit gradient information at all.
Therefore, they are less relevant to our approach and we

leave the possible connection for future work.

6 Discussion

In this paper, we introduced the notion of a stochasti-
cally differentiable probabilistic program as well as an
inference scheme for such programs. We provided a
reference implementation of probabilistic programming
with stochastically differentiable probabilistic programs.
Stochastically differentiable probabilistic programs fa-
cilitate natural speciﬁcation of probabilistic models and
support efﬁcient
inference in the models, providing
a viable alternative to explicit model marginalization,
whether manual or algorithmic.

Our understanding of stochastically differentiable proba-
bilistic programs, classes of models for which they are
best suited, and inference schemes is still at an early
stage and evolving.
In particular, we are working, to-
wards future publications, on maximum a posteriori esti-
mation in stochastically differentiable probabilistic pro-
grams. Another research direction would be selection
and adaption of stochastic gradient-based inference al-

gorithms to probabilistic programming inference. Last
but not least, we are looking into introducing support for
stochastically differentiable probabilistic programs into
existing probabilistic programming frameworks.

[9] Alp Kucukelbir, Dustin Tran, Rajesh Ranganath,
Andrew Gelman, and David M. Blei.
Auto-
matic differentiation variational inference. J. Mach.
Learn. Res., 18(1):430–474, January 2017.

References

[1] Eli Bingham, Jonathan P. Chen, Martin Jankowiak,
Fritz Obermeyer, Neeraj Pradhan, Theofanis Kar-
aletsos, Rohit Singh, Paul Szerlip, Paul Horsfall,
and Noah D. Goodman. Pyro: deep universal prob-
abilistic programming. Journal of Machine Learn-
ing Research, 20(28):1–6, 2019.

[2] Bob Carpenter, Andrew Gelman, Matthew Hoff-
man, Daniel Lee, Ben Goodrich, Michael Betan-
court, Marcus Brubaker, Jiqiang Guo, Peter Li, and
Allen Riddell. Stan: a probabilistic programming
language. Journal of Statistical Software, Articles,
76(1):1–32, 2017.

[3] Tianqi Chen, Emily B. Fox, and Carlos Guestrin.
Stochastic gradient Hamiltonian Monte Carlo.
In
Proceedings of the 31st International Conference
on International Conference on Machine Learn-
ing, ICML’14, pages II–1683–II–1691. JMLR.org,
2014.

[4] Marco F Cusumano-Towner, Feras A Saad,
Alexander K Lew, and Vikash K Mansinghka. Gen:
a general-purpose probabilistic programming sys-
tem with programmable inference. In Proceedings
of the 40th ACM SIGPLAN Conference on Pro-
gramming Language Design and Implementation,
pages 221–236, 2019.

[5] Hong Ge, Kai Xu, and Zoubin Ghahramani. Tur-
ing: composable inference for probabilistic pro-
gramming. In International Conference on Artiﬁ-
cial Intelligence and Statistics, AISTATS 2018, 9-
11 April 2018, Playa Blanca, Lanzarote, Canary
Islands, Spain, pages 1682–1690, 2018.

[6] The Go team. The Go programming language.

http://golang.org/, 2009.

[7] N. D. Goodman and A. Stuhlmüller. The Design
and Implementation of Probabilistic Programming
Languages. 2014. electronic; retrieved 2019/3/29.

[8] Noah D. Goodman, Vikash K. Mansinghka,
Daniel M. Roy, Keith Bonawitz, and Joshua B.
Tenenbaum. Church: a language for generative
models. In Proc. of Uncertainty in Artiﬁcial Intel-
ligence, 2008.

[10] Yi-An Ma, Tianqi Chen, and Emily Fox. A
complete recipe for stochastic gradient MCMC.
In C. Cortes, N. D. Lawrence, D. D. Lee,
M. Sugiyama, and R. Garnett, editors, Advances in
Neural Information Processing Systems 28, pages
2917–2925. Curran Associates, Inc., 2015.

[11] Vikash K. Mansinghka, Daniel Selsam, and Yura N.
Perov. Venture: a higher-order probabilistic pro-
gramming platform with programmable inference,
2014.

[12] T Minka, J Winn, J Guiver, and D Knowles. Infer
.net 2.4, Microsoft research Cambridge, 2010.

[13] Lawrence M. Murray and Thomas B. Schön. Au-
tomated learning with a probabilistic programming
language: Birch. Annual Reviews in Control, 46:29
– 43, 2018.

[14] Radford M. Neal. MCMC using Hamiltonian dy-
namics. Published as Chapter 5 of the Handbook of
Markov Chain Monte Carlo, 2011, 2012.

[15] B. Paige, F. Wood, A. Doucet, and Y.W. Teh. Asyn-
chronous anytime sequential Monte Carlo. In Ad-
vances in Neural Information Processing Systems,
2014.

[16] Avi Pfeffer. Figaro: An object-oriented probabilis-
tic programming language. In Charles River Ana-
lytics Technical Report (2009), 2009.

[17] Tom Rainforth, Christian A Naesseth, Fredrik
Lindsten, Brooks Paige, Jan-Willem van de Meent,
Arnaud Doucet, and Frank Wood. Interacting par-
In Proceedings
ticle Markov chain Monte Carlo.
of the 33rd International Conference on Machine
Learning, volume 48 of JMLR: W&CP, 2016.

[18] David A Roberts, Marcus Gallagher, and Thomas
Taimre. Reversible jump probabilistic program-
ming. In The 22nd International Conference on Ar-
tiﬁcial Intelligence and Statistics, pages 634–643,
2019.

[19] John Salvatier, Thomas V. Wiecki, and Christopher
Fonnesbeck. Probabilistic programming in Python
using PyMC3. PeerJ Computer Science, 2:e55, apr
2016.

[20] Stan Development Team.

Stan Modeling Lan-
guage User’s Guide and Reference Manual, Ver-
sion 2.18.0. 2018.

[21] David Tolpin. Deployable probabilistic program-
In Proceedings of the 2019 ACM SIG-
ming.
PLAN International Symposium on New Ideas, New
Paradigms, and Reﬂections on Programming and
Software, Onward! 2019, pages 1–16, New York,
NY, USA, 2019. ACM.

[22] David Tolpin, Jan-Willem van de Meent, Hongseok
Yang, and Frank Wood. Design and implementa-
tion of probabilistic programming language Angli-
can. In Proceedings of the 28th Symposium on the
Implementation and Application of Functional Pro-
gramming Languages, IFL 2016, pages 6:1–6:12,
New York, NY, USA, 2016. ACM.

[23] Jan-Willem van de Meent, Hongseok Yang, Vikash
Mansinghka, and Frank Wood. Particle Gibbs with
ancestor sampling for probabilistic programs.
In
Artiﬁcial Intelligence and Statistics, 2015.

[24] David Wingate, Andreas Stuhlmüller, and Noah D.
Goodman. Lightweight implementations of prob-
abilistic programming languages via transforma-
tional compilation. In Proceedings of the 14th Ar-
tiﬁcial Intelligence and Statistics, 2011.

[25] David Wingate and Theophane Weber. Automated
variational inference in probabilistic programming,
2013.

[26] Frank Wood, Jan-Willem van de Meent, and Vikash
Mansinghka. A new approach to probabilistic pro-
gramming inference. In Artiﬁcial Intelligence and
Statistics, 2014.

[27] Lingfeng Yang, Pat Hanrahan, and Noah D Good-
man. Generating efﬁcient MCMC kernels from
probabilistic programs. In Proceedings of the Sev-
enteenth International Conference on Artiﬁcial In-
telligence and Statistics, pages 1068–1076, 2014.

[28] Yuan Zhou, Bradley J Gram-Hansen, Tobias Kohn,
Tom Rainforth, Hongseok Yang, and Frank Wood.
LF-PPL: A Low-Level First Order Probabilis-
tic Programming Language for Non-Differentiable
In The 22nd International Conference
Models.
on Artiﬁcial Intelligence and Statistics, pages 148–
157, 2019.

[29] Yuan Zhou, Hongseok Yang, Yee Whye Teh, and
Tom Rainforth. Divide, conquer, and combine: a
new inference strategy for probabilistic programs
with stochastic support, 2019.

A Stan versions of the programs in the case studies

A.1 Compensation Survey

int<lower=0> N;
int<lower=0,upper=1> y[N];

real<lower=0,upper=1> theta;

for (n in 1:N) {

target += log_mix(0.5,

bernoulli_lpmf(y[n ]| theta ),
bernoulli_lpmf(y[n ]|0.5));

1 data {
2
3
4 }
5 parameters {
6
7 }
8 model {
9
10
11
12
13
14 }

}

A.2 Gaussian Mixture Model

int<lower = 0> N;
vector[N] y;

vector [2] mu;
real<lower=0> sigma[2];

mu ~ normal(0, 10);
sigma ~ lognormal (0, 10);
for (n in 1:N)

target += log_mix(0.5,

1 data {
2
3
4 }
5 parameters {
6
7
8 }
9 model {
10
11
12
13
14
15
16
17 }

}

normal_lpdf(y[n] | mu[1], sigma [1]),
normal_lpdf(y[n] | mu[2], sigma [2]));

A.3 Hidden Markov model

real<lower=0> noise;
int<lower=1> K; // number of states
int<lower=1> N;
real y[N];

simplex[K] theta [K];

1 data {
2
3
4
5
6 }
7 parameters {
8
9 }
10 model {
11 real acc[K];
12 real gamma[N,K];
13 for (k in 1:K)
14
15
16
17
18
19
20
21
22
23
24 }

}

gamma[1,k] = normal_lpdf(y[0]|k−1, noise );
for ( t

in 2:N) {
for (k in 1:K) {

for ( j

in 1:K)

acc[ j ] = gamma[t−1,j] + log( theta [ j ,k])
+ normal_lpdf(y[t ]| k−1, noise );

gamma[t,k] = log_sum_exp(acc);

}
target += log_sum_exp(gamma[N]);

