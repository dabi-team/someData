Zap Q-Learning for Optimal Stopping Time Problems

Shuhang Chen∗

Adithya M. Devraj†

Ana Buˇsi´c‡

Sean Meyn§

9
1
0
2

p
e
S
0
3

]

Y
S
.
s
c
[

3
v
8
3
5
1
1
.
4
0
9
1
:
v
i
X
r
a

Abstract— The objective in this paper is to obtain fast
converging reinforcement learning algorithms to approxi-
mate solutions to the problem of discounted cost optimal
stopping in an irreducible, uniformly ergodic Markov
chain, evolving on a compact subset of Rn. We build on the
dynamic programming approach taken by Tsitsikilis and
Van Roy, wherein they propose a Q-learning algorithm
to estimate the optimal state-action value function, which
then deﬁnes an optimal stopping rule. We provide insights
as to why the convergence rate of this algorithm can be
slow, and propose a fast-converging alternative, the “Zap-
Q-learning” algorithm, designed to achieve optimal rate of
convergence. For the ﬁrst time, we prove the convergence
of the Zap-Q-learning algorithm under the assumption
of linear function approximation setting. We use ODE
analysis for the proof, and the optimal asymptotic variance
property of the algorithm is reﬂected via fast convergence
in a ﬁnance example.

I. INTRODUCTION

Consider a discrete-time Markov chain X = {Xn :
n ≥ 0} evolving on a general state-space X. The goal
in optimal stopping time problems is to minimize over
all stopping times τ , the associated expected cost:

(cid:34) τ −1
(cid:88)

(cid:35)
βnc(Xn) + βτ cs(Xτ )

E

(1)

n=0

where c : X → R denotes the per-stage cost, cs : X → R
the terminal cost, and β ∈ (0, 1) is the discount factor.
Examples of such problems arise mostly in ﬁnancial
applications such as derivatives analysis (see Section V),
timing of a purchase or sale of an asset, and more
generally in problems that involve sequential analysis.
In this work, the optimal decision rule is approximated
using reinforcement learning techniques. We propose
and analyze an optimal variance algorithm to approx-
imate the value function associated with the optimal
stopping rule.

∗S.C. is with the Department of Mathematics at the University of

Florida in Gainesville

†A.D. is with the Department of ECE at the University of Florida
‡A.B. is with Inria and DI ENS, ´Ecole Normale Sup´erieure, CNRS,

PSL Research University, Paris, France

§S.M. is with Department of Electrical and Computer Engineering,

University of Florida, and Inria International Chair, Paris

Acknowledgements:

from ARO grant
W911NF1810334 is gratefully acknowledged. Additional support
from EPCN 1609131 & CPS 1646229, and French National Research
Agency grant ANR-16-CE05-0008.

Financial

support

A. Deﬁnitions & Problem Setup

We assume that the state-space X ⊂ Rm is compact,
and we let B denote the associated Borel σ-algebra.
The time-homogeneous Markov chain X is deﬁned
on a probability space (Ω, F, P), and its dynamics is
determined by an initial distribution µ : X → [0, 1], and
a transition kernel P : for each x ∈ X and A ∈ B,

P (x, A) = Pr(Xn+1 ∈ A | Xn = x)

It is assumed that X is uniformly ergodic: There exisits a
unique invariant probability measure π, a constant D <
∞, and 0 < ρ < 1, such that, for all x ∈ X and A ∈ B,

(cid:107)P n(x, A) − π(A)(cid:107) ≤ Dρn ,

n ≥ 0

(2)

Denote by {Fn : n ≥ 0} the ﬁltration associated
with X. The Markov property asserts that for bounded
measurable functions h : X → R,

E[h(Xn+1) | Fn , Xn = x] =

(cid:90)

P (x, dy)h(y)

In this paper, a stopping time τ : Ω → [0, ∞) is
a random variable taking on values in the non-negative
integers, with the deﬁning property {ω : τ (ω) ≤ n , ω ∈
Ω} ∈ Fn for each n ≥ 0. A stationary policy is deﬁned
to be a measurable function φ : X → {0, 1} that deﬁnes
a stopping time:

τ φ = min{n ≥ 0 : φ(Xn) = 1}

(3)

The optimal value function is deﬁned as the inﬁmum

of (1) over all stopping times: for any x ∈ X,

h∗(x) := inf
τ

E(cid:2)

τ −1
(cid:88)

n=0

βnc(Xn) + βτ cs(Xτ )|X0 = x(cid:3) (4)

Similarly, the associated Q-function is deﬁned as

Q∗(x) := c(x) + βE[h∗(X1) | X0 = x]
It follows that Q∗ solves the associated Bellman equa-
tion [15]: for each x ∈ X,
Q∗(x) = c(x)+βE[min(cs(X1), Q∗(X1))|X0 = x] (5)

and the optimal stopping rule is deﬁned by the corre-
sponding stationary policy,

φ∗(x) = I{cs(x) ≤ Q∗(x)}
(6)
where I{·} denotes the indicator function. Using the
general deﬁnition (3), an optimal stopping time satisﬁes
τ ∗ = τ φ∗

.

 
 
 
 
 
 
The Bellman equation (5) can be expressed as the
functional ﬁxed point equation: Q∗ = F Q∗, where F
denotes the dynamic programming operator: for any
function Q : X → R, and x ∈ X,

F Q(x):=c(x)+βE[min(cs(X1), Q(X1))|X0 = x] (7)

Analysis is framed in the usual Hilbert space L2(π)
of real-valued measurable functions on X with inner
product:

(cid:104)f, g(cid:105)π = E[f (X)g(X)] ,

and norm:

(cid:107)f (cid:107)π = (cid:112)(cid:104)f, f (cid:105)π ,

(8)

(9)

where the expectation in (8) is with respect to the steady
state distribution π. It is assumed throughout that the
cost functions c and cs are in L2(π).

B. Objective

The goal in this work is to approximate Q∗ using
a parameterized family of functions {Qθ}, where θ ∈
Rd denotes the parameter vector. We restrict to linear
parameterization throughout, so that:

Qθ(x) := θTψ(x),

x ∈ X

(10)

: X → R , ψi ∈
where ψ := [ψ1, . . . , ψd]T with ψi
L2(π), 1 ≤ i ≤ d, denotes the basis functions. For any
parameter vector θ ∈ Rd, we denote the Bellman error

E = F Qθ − Qθ.
Bθ

It is assumed that the basis functions are linearly
independent: The d × d covariance matrix Σψ is full
rank, where

Σψ(i, j) = (cid:104)ψi, ψj(cid:105)π,

1 ≤ i, j ≤ d

(11)

In a ﬁnite state-space setting, it is possible to construct
a consistent algorithm that computes the Q-function
exactly [8]. The Q-learning algorithm of Watkins [16],
[17] can be used in this case (see [18] for a discussion).
In a function approximation setting, we need to relax
the goal of solving (5). As in previous works [15], [6],
[18], the goal in this paper is to obtain the solution to a
Galerkin relaxation of (5): Find θ∗ such that,

E[Bθ∗

E (Xn)ψi(Xn)] = 0 ,

1 ≤ i ≤ d,

(12)

or equivalently,

(cid:104)F Qθ∗

− Qθ∗

, ψi(cid:105)π = 0 ,

1 ≤ i ≤ d.

(13)

In [15], the authors show that the solution to the ﬁxed
point equations in (13) satisﬁes (see [15, Theorem 2]):

(cid:107)Qθ∗

− Q∗(cid:107)π ≤

(cid:104)

1
1 − β2

min
θ

(cid:107)Qθ − Q∗(cid:107)π

(cid:105)
.

C. Literature Survey

Obtaining an approximate solution to the original
problem (5) using a modiﬁed objective (13) was ﬁrst
considered in [15]. The authors propose an extension
of the TD(0) algorithm of [12], [14], and obtain con-
vergence results under the assumption of a ﬁnite state-
action space setting.

Though it is not obvious at ﬁrst sight, the algorithm in
[15] is more closely connected to Watkins’ Q-learning
algorithm [16], [17], than the TD(0) algorithm. This is
speciﬁcally due to a minimum term that appears in (13)
(see deﬁnition of F in (7)), similar to what appears in Q-
learning. This is important to note, because Q-learning
algorithms are known to have convergence issues under
function approximation settings, and this is due to the
fact that the dynamic programming operator may not be
a contraction in general [1]. The operator F deﬁned in
(7) is quite special in this sense: it can be shown that it
is a contraction with respect to the π-norm [15]:

(cid:107)F Q − F Q(cid:48)(cid:107)π ≤ β(cid:107)Q − Q(cid:48)(cid:107)π,

for all Q, Q(cid:48) ∈ L2(π)

Since [15], many other algorithms have been proposed
to improve the convergence rate. In [6] the authors
propose a matrix gain variant of the algorithm presented
in [15], improving the rate of convergence in numerical
studies. In [18],
the authors take a “least squares”
approach to solve the problem, and propose the least
squares Q-learning algorithm,
that has close resem-
blance to the least squares policy evaluation algorithm
(LSPE (0) of [10]). The authors recognize the high
computational complexity of the algorithm, and propose
alternative variants. In prior works [6] and [18], though
a function-approximation setting is considered, the state-
space is assumed ﬁnite.

More recently, in [8], [7], the authors propose the
Zap Q-learning algorithm to solve for a solution to a
ﬁxed point equation similar to (but more general than)
(5). The proof of convergence is provided only for the
ﬁnite state-action space setting, and more restrictively,
a tabular basis is assumed (wherein the ψi’s span all
possible functions).

D. Contributions

We make the following contributions in this work:
(i) We extend the convergence analysis of Zap-Q-
learning of [7] to the problem of optimal-stopping
(1) in a linear function approximation setting (the
authors consider only a ‘tabular’ basis in [7])

(ii) The algorithm and analysis presented in this
work is superior to previous works on optimal
stopping [15], [6], [18] in two ways: Firstly, the
analysis in previous works only concern a ﬁ-
nite state-action space setting; more importantly,

the algorithm we propose has optimal asymptotic
variance, implying better convergence rates (see
Section III for a discussion and Section V for
numerical results).
The extension of the work [7] to the current setting
is not
trivial: The tabular case is much simpler to
analyze with lots of special structures, and in general,
the theory for convergence of any Q-learning algorithm
in a function approximation setting does not exist.
Furthermore, the ODE analysis obtained in this paper
(cf. Theorem 3.5) provides great insights into the be-
havior of the Zap-Q algorithm, even in a linear function
approximation setting.

The remainder of the paper is organized as follows:
Section II contains the approximation architecture, and
introduces the Zap-Q-learning algorithm. The assump-
tions and main results are contained in Section III.
Section IV provides a high-level proof of the results,
numerical results are collected together in Section V,
and conclusions in Section VI. Full proofs are available
in the extended version of this paper, available on arXiv
[4].

II. Q-LEARNING FOR OPTIMAL STOPPING

A. Notation

The following notation is useful for the convergence
analysis. For each θ ∈ Rd, we denote φθ : X → {0, 1}
to be the corresponding policy:

φθ(x) := I{cs(x) ≤ Qθ(x)}

(14)

For any function f with domain X, the operators Sθ and
S c

θ are deﬁned as the simple products,

Sθf (x) := I{Qθ(x) < cs(x)}f (x)
θ f (x) := I{cs(x) ≤ Qθ(x)}f (x)
S c

(15a)

(15b)

Observe that for each x ∈ X, Sθf (x) = (1−φθ(x))f (x).

The objective (13) can then be expressed:

algorithm for optimal stopping is given by the following
recursion:

θn+1 = θn + αn+1Gn+1ψ(Xn)dn+1

(20)

with {dn} denoting the “temporal difference” sequence:

dn+1:=c(Xn)+βmin(cs(Xn+1), Qθn (Xn+1))−Qθn (Xn)

The algorithm proposed in [15] is (20), with Gn ≡ I
(the d × d identity matrix). This is similar to the TD(0)
algorithm [14], [12].

The ﬁxed point Kalman ﬁlter algorithm of [6] can
also be written as a special case of (20): We have
n ]†, where M † denotes the pseudo-inverse of
Gn ≡ [(cid:98)Σψ
any matrix M , and (cid:98)Σψ
n is an estimate of the mean Σψ
deﬁned in (11); The estimate can be recursively obtained
using standard Monte-Carlo recursion:

(cid:98)Σψ

n+1 = (cid:98)Σψ

n + αn+1

(cid:2)ψ(Xn)ψT(Xn) − (cid:98)Σψ

n

(cid:3)

(21)

In the Zap-Q algorithm, the matrix gain sequence
{Gn} is designed so that the asymptotic covariance of
the resulting algorithm is minimized (see Section III for
details). It uses a matrix gain Gn = − (cid:98)A†
n+1, with (cid:98)An+1
being an estimate of A(θn), and A(θ) deﬁned in (17).
The term inside the expectation in (17), following the

substitution θ = θn, is denoted

An+1 := ψ(Xn)(cid:2)βSθn ψ(Xn+1) − ψ(Xn)(cid:3)T

(22)

Using (22), the matrix A(θn) is recursively estimated
using stochastic approximation in the Zap-Q algorithm:

Algorithm 1 Zap-Q for Optimal Stopping
Input: Initial θ0 ∈ Rd, (cid:98)A0: d × d, negative deﬁnite;
step-size sequences {αn} and {γn} and n = 0

1: repeat
2:

Obtain the Temporal Difference term:

dn+1 = c(Xn)+β min(cs(Xn+1), Qθn (Xn+1))−Qθn (Xn)

3:

Update the matrix gain estimate (cid:98)An of A(θn),

A(θ∗)θ∗ + βcs(θ∗) + b∗ = 0 ,

(16)

with An+1 deﬁned in (22):

where, for each θ ∈ Rd, A(θ) is a d × d matrix, and b∗
and cs(θ) are d-dimensional vectors:

A(θ) := E[ψ(Xn)βSθψT(Xn+1) − ψ(Xn)ψT(Xn)] (17)
(18)

b∗ := E[ψ(Xn)c(Xn)]

cs(θ) := E[ψ(Xn)S c

θ cs(Xn+1)]

(19)

B. Zap Q-Learning

Before we introduce our main algorithm, it is useful
to ﬁrst consider a more general class of “matrix gain” Q-
learning algorithms. Given a d × d matrix gain sequence
{Gn : n ≥ 0}, and a scalar step-size sequence {αn :
the corresponding matrix gain Q-learning
n ≥ 0},

(cid:98)An+1 = (cid:98)An + γn+1

(cid:2)An+1 − (cid:98)An

(cid:3)

(23)

4:

Update the parameter vector:
θn+1 = θn − αn+1 (cid:98)A†

n+1ψ(Xn)dn+1

(24)

n = n + 1

5:
6: until n ≥ N
Output: θ = θN

C. Discussion

Algorithm 1 belongs to a general class of algorithms
known as two-time-scale stochastic approximation [2]:
the recursion in (24) on the slower time-scale intends to

estimate the parameter vector θ∗, and for each n ≥ 0,
the recursion (23) on the faster time-scale intends to
estimate the mean A(θn). The step-size sequences {αn}
and {γn} have to satisfy the standard requirements for
separation of time-scales [2]: for any (cid:37) ∈ (0.5, 1), we
choose

αn = 1/n ,

γn = 1/n(cid:37)

(25)

For each θ ∈ Rd, consider the following terms:

b(θ) = −A(θ)θ − βcs(θ)
cθ(x) = Qθ(x)

(26b)
− E(cid:2)β min(cs(Xn+1), Qθ(Xn+1)) | Xn = x(cid:3)

(26a)

The vector b(θ) is analogous to b∗ in (16), and (26b)
recalls the Bellman equation (5). The following Prop. 2.1
is direct from these deﬁnitions. It shows that b(θ) is the
“projection” of the cost function cθ, similar to how b∗
is related to c through (18).

Proposition 2.1: For each θ ∈ Rd, we have:

b(θ) = E(cid:2)cθ(Xn)ψ(Xn)(cid:3)

(27)

where the expectation is in steady state. In particular,

b∗ = b(θ∗)

(cid:117)(cid:116)

Lemma 3.2: For each θ ∈ Rd,

the operator F θ

satisﬁes:

(cid:107)F θQ − F θQ(cid:48)(cid:107) ≤ β(cid:107)Q − Q(cid:48)(cid:107),

Q, Q(cid:48) ∈ L2(π)

(cid:117)(cid:116)

The next result is a direct consequence of Lemma 3.2,
and establishes the inveritbility of the matrix A(θ) for
any θ ∈ Rd:

Lemma 3.3: For each θ ∈ Rd,
(i) The d × d matrix A(θ) deﬁned in (17) satisﬁes:

− vTA(θ)v ≥ (1 − β)vTΣψv,

(30)

for each v ∈ Rd, with Σψ deﬁned in (11).

(ii) Eigenvalues of A(θ) are strictly bounded away
from 0, and {A−1(θ) : θ ∈ Rd} is uniformly
bounded.

(cid:117)(cid:116)

Prop. 2.1 implies a Lipschitz bound on the function b

deﬁned in (26a):

Lemma 3.4: The mapping b is Lipschitz: For some

(cid:96)1 > 0, and each θ1, θ2 ∈ Rd,

(cid:107)b(θ1) − b(θ2)(cid:107) ≤ (cid:96)1(cid:107)θ1 − θ2(cid:107)

(cid:117)(cid:116)

III. ASSUMPTIONS AND MAIN RESULTS

B. Assumptions & Main Result

A. Preliminaries

We ﬁrst summarize preliminary results here that will
be used to establish the main results in the following
sections. The proofs of all
the technical results are
contained in the Appendix of [4].

We start with the contraction property of the operator
F deﬁned in (7). The following is a result directly
obtained from [15] (see [15, Lemma 4 on p. 1844]).

Lemma 3.1: The dynamic programming operator F

deﬁned in (7) satisﬁes:

(cid:107)F Q − F Q(cid:48)(cid:107) ≤ β(cid:107)Q − Q(cid:48)(cid:107),

Q, Q(cid:48) ∈ L2(π).

Furthermore, Q∗ is the unique ﬁxed point of F in L2(π).
(cid:117)(cid:116)

Recall that Qθ : X → R is deﬁned in (10). Similar to
the operator F , for each θ ∈ Rd we deﬁne operators H θ
and F θ that operate on functions Q : X → R as follows:

H θQ(x) =

(cid:40)

Q(x),
cs(x),

if Qθ(x) < cs(x)
otherwise

F θQ = c + βP H θQ.

(28)

(29)

The following Lemma is a slight extension of

Lemma 3.1.

The following assumptions are made throughout:
Assumption A1: X is a uniformly ergodic Markov
chain on the compact state space X, with a unique
invariant probability measure, π (cf. (2)).
Assumption A2: There exists a unique solution θ∗ to
the objective (13).
Assumption A3.1: The conditional distribution of
ψ(Xn+1) given Xn = x has a density, fψ|x(z). This
density is also assumed to have uniformly bounded
likelihood ratio (cid:96)(z | x) with respect to the Gaussian
density N (ψ(x), I).
Assumption A3.2: It
function cs − 1 is in the span of {ψi}.
Assumption A4: The parameter sequence {θn : n ≥ 1}
is bounded a.s..

is assumed moreover that

the

Assumption A3 consists of technical conditions re-
quired for the proof of convergence. The density as-
the conditional
sumption is imposed to ensure that
expectation given Xn of functions such as SθψT(Xn+1)
are smooth as a function of θ. Furthermore, it implies
that Σψ is positive deﬁnite.

Assumption A4 is a standard assumption in much
of the recent stochastic approximation literature. We
conjecture that the boundedness can be established via
an extension of the results in [3], [2]. The “ODE

at inﬁnity” posed there is stable as required, but the
extension of the results to the current setting of two time-
scale stochastic approximation with Markovian noise is
the only challenge.

The main result of this paper establishes convergence

of iterates {θn} obtained using Algorithm 1:

Theorem 3.5: Suppose

that Assumptions A1-A4

hold. Then,

(i) The parameter sequence {θn} obtained using the
Zap-Q algorithm converges to θ∗ a.s., where θ∗
satisﬁes (13).

(ii) An ODE approximation holds for the sequences
{θn, b(θn)} by continuous time functions (w, b)
satisfying

d
dt b(t) = −b(t) + b

b(t) = −A(w(t))w(t) − βcs(w(t))

(31)

(cid:117)(cid:116)

The term ODE approximation is standard in the SA

literature: For t ≥ s, let ws(t) denote the solution to:

d

dt ws(t) = ξ(ws(t)), ws(s) = w(s)
for some ξ : Rd → Rd, and w(t) denoting the contin-
uous time process constructed from the sequence {θn :
n ≥ 0} via linear interpolation. We say that the ODE
approximation d
dt w(t) = ξ(w(t)) holds for the sequence
{θn : n ≥ 1}, if the following is true for any T > 0:

(32)

lim
s→∞

sup
t∈[s,s+T ]

(cid:107)w(t) − ws(t)(cid:107) = 0, a.s.

Details are made precise in Section IV-B. The optimality
of the algorithm in terms of the asymptotic variance is
discussed next.

C. Asymptotic Variance

The asymptotic covariance ΣΘ of any algorithm is

deﬁned to be the following limit:

ΣΘ := lim
n→∞

nE(cid:2)(cid:0)θn − θ∗(cid:1)(cid:0)θn − θ∗(cid:1)T(cid:3)

(33)

Consider the matrix gain Q-learning algorithm (20),
and suppose the matrix sequence {Gn : n ≥ 0} is
constant: Gn ≡ G. Also, suppose that all eigenvalues of
GA(θ∗) satisfy λ(cid:0)GA(θ∗)(cid:1) < − 1
2 . Following standard
analysis (see Section 2.2 of [8] and references therein),
it can be shown that, under general assumptions, the
asymptotic covariance of the algorithm (20) can be
obtained as a solution to the Lyapunov equation:

(cid:0)GA(θ∗) + 1

2 I(cid:1)ΣΘ + ΣΘ

(cid:0)GA(θ∗) + 1

2 I(cid:1)T

+ GΣE GT = 0

(34)

where ΣE is the “noise covariance matrix”,
deﬁned as follows.

that

is

A “noise sequence” {En} is deﬁned as
En := ˜An+1θ∗ + ˜bn+1 + ˜An+1
where ˜An+1 := An+1 − A(θ∗), ˜bn+1 := bn+1 − b∗,
˜θn := θn − θ∗, with An+1 deﬁned in (22), A(θ) deﬁned
in (17),

(35)

˜θn

bn+1 := ψ(Xn)(cid:2)c(Xn) + S c
θn

cs(Xn+1)(cid:3)

(36)

and b∗ deﬁned in (18). The noise covariance matrix is
then deﬁned as the limit

ΣE = lim
T →∞

1
T

E[ST S T
T ]

(37)

in which ST = (cid:80)T
steady state.

n=1 En, and the expectation is in

Optimality of the asymptotic covariance: The asymp-
totic covariance ΣΘ can be obtained as a solution to
(34) only when all eigenvalues satisfy λ(GA(θ∗)) <
− 1
2 . If there exists at least one eigenvalue such that
λ(GA(θ∗)) ≥ − 1
2 , then, under general conditions, it can
be shown that the asymptotic covariance is not ﬁnite [8].
This implies that the rate of convergence of θn is slower
than O(1/

n).

√

It is possible to optimize the covariance ΣΘ over all
matrix gains G using (34). Speciﬁcally, it can be shown
that letting G∗ = −A(θ∗) will result in the minimum
asymptotic covariance Σ∗, where

Σ∗ = A(θ∗)−1ΣE

(cid:0)A(θ∗)−1(cid:1)T

(38)

That is, for any other gain Gn ≡ G, denoting ΣG
Θ
to be the asymptotic covariance of the algorithm (20)
obtained as a solution to the Lyapunov equation (34),
Θ−Σ∗ is positive semi-deﬁnite. This is
the difference ΣG
speciﬁcally true for the algorithms proposed in [15] and
[6].

The Zap Q algorithm is speciﬁcally designed to
achieve the optimal asymptotic covariance. A full proof
of optimality will require extra effort. Thm. 3.5 tells us
that we have the required convergence Gn → G∗ for this
algorithm. Provided we can obtain additional tightness
√
n˜θn, we obtain a functional
bounds for the scaled error
Central Limit Theorem with optimal covariance Σ∗ [2].
Minor additional bounds ensure convergence of (33) to
the optimal covariance Σ∗.

The next section is dedicated to the proof of Thm. 3.5.

IV. PROOF OF THEOREM 3.5

A. Overview of the Proof

Unlike martingale difference assumptions in standard
stochastic approximation, the noise in our algorithm is
Markovian. The ﬁrst part of this section establishes that
our noise sequence satisﬁes the so called ODE friendly
property [13]: A vector-valued sequence of random

variables {Ek} will be called ODE-friendly if it admits
the decomposition,

Ek = ∆k + Tk − Tk−1 + εk ,

k ≥ 1

(39)

in which:

(i) {∆k : k ≥ 1} is a martingale-difference sequence
∆ < ∞ a.s. for all k

satisfying E[(cid:107)∆k(cid:107)2 | Fk] ≤ ¯σ2
(ii) {Tk : k ≥ 1} is a bounded sequence
(iii) The ﬁnal sequence {εk} is bounded and satisﬁes:

∞
(cid:88)

k=1

γk(cid:107)εk(cid:107) < ∞ a.s. .

(40)

Intuitively, if an error sequence satisﬁes the above prop-
erties, it can be shown that its asymptotic effect on the
parameter update is zero. This allows us to argue that
the matrix gain estimate (cid:98)An+1 is close to the mean
A(θn) in the fast time-scale recursion (23). We then
consider the slow time-scale recursion (24), and obtain
the ODE approximations for {θn} and the expected
projected cost {b(θn)}. The fact that these ODE’s are
stable (with a unique stationary point) will then establish
the convergence of the algorithm.

B. ODE Analysis

The remainder of this section is dedicated to the proof
of the ODE approximation (31). The construction of an
approximating ODE involves ﬁrst deﬁning a continuous
time process w. Denote

tn =

n
(cid:88)

i=1

αi, n ≥ 1,

t0 = 0 ,

(41)

and deﬁne w(tn) = θn at these time points, with the
deﬁnition extended to R+ via linear interpolation.

Along with the piecewise linear continuous-time pro-
cess {wt : t ≥ 0}, denote by { ¯At : t ≥ 0} the piecewise
linear continuous-time process deﬁned similarly, with
¯Atn = (cid:98)An, n ≥ 1. Furthermore, for each t ≥ 0, denote
¯bt ≡ b(wt) := −A(wt)wt − βcs(wt)

To construct an ODE, it is convenient ﬁrst to obtain
an alternative and suggestive representation for the pair
of equations (23,24).

Lemma 4.1 establishes that the error sequences that
appear in the updates for {θn} and { (cid:98)An} are “ODE
friendly”.

Lemma 4.1: The pair of equations (23, 24) can be

expressed,

θn+1 = θn −αn+1 (cid:98)A†

n+1

(cid:2)A(θn)θn +βcs(θn)+b∗ (42a)

(cid:98)An+1 = (cid:98)An + γn+1

+ E A
n+1θn + E θ
(cid:2)A(θn) − (cid:98)An + E A

n+1

n+1
(cid:3)

(cid:3)

(42b)

in which the sequences {E θ
friendly.

n , E A

n : n ≥ 1} are ODE-
(cid:117)(cid:116)

The following result establishes that (cid:98)An recursively

obtained by (23) approximates the mean A(θn):

Lemma 4.2: Suppose the sequence {E A

n : n ≥ 1} is

ODE-friendly. Then,

(i)

lim
n→∞

(cid:107) (cid:98)An − A(θn)(cid:107) = 0, a.s.

(ii) Consequently, (cid:98)A†
(cid:107) (cid:98)A†

n (cid:54)= (cid:98)A−1
n − A−1(θn)(cid:107) = 0, a.s.

lim
n→∞

n only ﬁnitely often, and

(cid:117)(cid:116)

With the deﬁnition of ODE approximation below (32),

we have:

Lemma 4.3: The ODE approximation for {θn} holds:
with probability one, the piece-wise continuous function
w(t) asymptotically tracks the ODE:

d

dt w(t) = −A−1(w(t))(cid:2)b∗ − b(w(t))(cid:3)

(43)

(cid:117)(cid:116)

For a ﬁxed (but arbitrary) time horizon T > 0, we
deﬁne two families of uniformly bounded and uniformly
Lipschitz continuous functions: {w(s+t), t ∈ [0, T ]}s≥0
and {¯b(s + t), t ∈ [0, T ]}s≥0. Sub-sequential limits of
{w(s + t), t ∈ [0, T ]}s≥0 and {¯b(s + t), t ∈ [0, T ]}s≥0
are denoted wt and bt respectively.

We recast the ODE limit of the projected cost as

follows:

Lemma 4.4: For any sub-sequential limits {wt, bt},

(i) they satisfy bt = b(wt).
(ii) for a.e. t ∈ [0, T ],

d
dt

bt = −A(wt)

d
dt

wt = −bt + b∗

(44)
(cid:117)(cid:116)

Proof of Thm. 3.5: Bounedness of sequences { (cid:98)An :
n ≥ 0} and { (cid:98)A−1
n : n ≥ 0} is established in Lemma 4.2.
Together with boundedness assumption of {θn : n ≥
0}, the ODE approximation is established in Lemma
4.4. Result (i) then follows from those two results using
(cid:4)
standard arguments from [2].

V. NUMERICAL RESULTS

In this section we illustrate the performance of the
Zap Q-learning algorithm in comparison with existing
techniques, on a ﬁnance problem that has been studied in
prior work [6], [15]. We observe that the Zap algorithm
performs very well, despite the fact that some of the
technical assumptions made in Section III do not hold.

Fig. 1: Average rewards obtained using various Q-learning algorithms

A. Finance model

C. Implementation Details

The following ﬁnance example is used in [6], [15] to
evaluate the performance of their algorithms for optimal
stopping. The reader is referred to these references for
complete details of the problem set-up.

The Markovian state process considered is the vector
of ratios: Xn = ((cid:101)pn−99, (cid:101)pn−98, . . . , (cid:101)pn)T/(cid:101)pn−100, n ≥
: t ∈ R} is a geometric Brownian
0, in which {(cid:101)pt
motion (derived from an exogenous price-process). This
uncontrolled Markov chain is positive Harris recurrent
on the state space X ≡ R100, so X is not compact. The
Markov chain is uniformly ergodic.

The “time to exercise” is modeled as a stopping time
τ ∈ Z+. The associated expected reward is deﬁned as
E[βτ r(Xτ )], with r(Xn) := Xn(100) = (cid:101)pn/(cid:101)pn−100 and
β ∈ (0, 1) ﬁxed. The objective of ﬁnding a policy that
maximizes the expected reward is modeled as an optimal
stopping time problem.

The value function is deﬁned to be the inﬁmum (4),
with c ≡ 0 and cs ≡ −r (the objective in Section I is to
minimize the expected cost, while here, the objective is
to maximize the expected reward). The associated Q-
function is deﬁned using (5), and the associated optimal
policy using (6): φ∗(x) = I{r(x) ≥ Q∗(x)}.

When the Q-function is linearly approximated using
(10), for a ﬁxed parameter vector θ, the associated value
function can be expressed:

The experimental setting of [6], [15] is used to deﬁne
the set of basis functions and other parameters. We
choose the dimension of the parameter vector d = 10,
with the basis functions deﬁned in [6]. The objective
here is to compare the performances of the ﬁxed point
Kalman ﬁlter algorithm with the Zap-Q learning algo-
rithm in terms of the resulting average reward (45).

Recall that the step-size for the Zap Q-learning al-
gorithm is given in (25). We set γn = n−0.85 for all
implementations of the Zap algorithm, but similar to
what is done in [6], we experiment with different choices
for αn. Speciﬁcally, in addition to αn = n−1, we let:

αn =

g
b + n

(47)

with b = 1e4 and experiment with g = 2, 5, and
10. In addition, we also implement Zap with (αn =
0.1/n , γn = 1/n0.85). Based on the discussion in Sec-
tion III-C, we expect this choice of step-size sequences
to result in inﬁnite asymptotic variance.

In the implementation of the ﬁxed point Kalman ﬁlter
algorithm, as suggested by the authors, we choose step-
size αn = 1/n for the matrix gain update rule in (21), and
step-size of the form (47) for the parameter update in
(20). Speciﬁcally, we let b = 1e4, and g = 100 and 200.
The number of iterations for each of the algorithm is

ﬁxed to be N = 2e6.

hφθ (x) := E[βτθ r(Xτθ ) | x0 = x] ,

(45)

D. Experimental Results

where,

τθ := min{n : φθ(Xn) = 1}

φθ(x) := I{r(x) ≥ Qθ(x)}

(46)

Given a parameter estimate θ and the initial state
X(0) = x, the corresponding average reward hφθ (x)
was estimated using Monte-Carlo in the numerical ex-
periments that follow.

B. Approximation & Algorithms

Along with Zap Q-learning algorithm we also imple-
ment the ﬁxed point Kalman ﬁlter algorithm of [6] to
estimate θ∗. This algorithm is given by the update equa-
tions (20) and (21). The computational as well as storage
complexities of the least squares Q-learning algorithm
(and its variants) [18] are too high for implementation.

The average reward histogram was obtained by the
following steps: We simulate 500 parallel simulations of
each of the algorithms to obtain as many estimates of θ∗.
Each of these estimates deﬁnes a policy φθN deﬁned in
(46). We then estimate the corresponding average reward
hφθN (x) deﬁned in (45), with X(0) = x = 1.

Along with the average discounted rewards, we also
plot the histograms to visualize the asymptotic variance
(33), for each of the algorithms. The theoretical values
of the covariance matrices Σ∗ and ΣG
Θ were estimated
through the following steps: The matrices A(θ∗) and
Σψ (the limit of the matrix gain used in [6]) were
estimated via Monte-Carlo. Estimation of A(θ∗) requires
an estimate of θ∗; this was taken to be θN obtained
using the Zap-Q algorithm with αn = n−1 and γn =
n−0.85. This estimate of θ∗ was also used to estimate the

11.051.11.151.21.25Zap g = 0.1Zap g = 2Zap g = 5Zap g = 1011.051.11.151.21.25KF 100KF 200SNRZap g = 1 (optimal)Fig. 2: Asymptotic variance for various Q-learning algorithms for optimal stopping in ﬁnance

covariance matrix Σ∆ deﬁned in (37) using the batch
means method. The matrices ΣG
Θ and Σ∗ were then
obtained using (34) and (38), respectively.

Fig. 1 contains the histograms of the average rewards
obtained using the above algorithms. Fig. 2 contains the
N (cid:0)θN (8) − θ∗(8)(cid:1) along with a plot of
histograms of
the theoretical prediction.

√

It was observed that the eigenvalues of the matrix
A(θ∗) have a wide spread: The condition-number is of
the order 104. Despite a badly conditioned matrix gain,
it is observed in Fig. 1, that the average rewards of
the Zap-Q algorithms are better than its competitors.
It is also observed that the algorithm is robust to the
choice of step-sizes. In Fig. 2 we observe that
the
asymptotic behavior of the algorithms is close match
to the theoretical prediction. Speciﬁcally, large variance
of Zap-Q with step-size αn = 0.1/n conﬁrms that the
asymptotic variance is very large (ideally, inﬁnity), if the
eigenvalues of the matrix GA(θ∗) > − 1
2 .

VI. CONCLUSION

In this paper, we extend the theory for the Zap Q-
learning algorithm to a linear function approximation
setting, with application to optimal stopping. We prove
convergence of the algorithm using ODE analysis, and
also observe that it achieves optimal asymptotic vari-
ance. The extension of the previous analysis to the
current setting is not trivial: Analysis of Zap-Q in the
tabular case is much simpler, with lots of special struc-
tures, and in general, the theory for convergence of Q-
learning algorithms in a function approximation setting
does not exist. More importantly, we believe that the
ODE analysis obtained in this paper provides important
insights into the behavior of the Zap-Q algorithm, even
in a function approximation setting. This may be a
starting point for analysis of Q-learning algorithms in
general function-approximation settings, which is an on-
going work.

REFERENCES

[1] D. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming.

Atena Scientiﬁc, Cambridge, Mass, 1996.

[2] V. S. Borkar. Stochastic Approximation: A Dynamical Systems
Viewpoint. Hindustan Book Agency and Cambridge University
Press (jointly), Delhi, India and Cambridge, UK, 2008.

[3] V. S. Borkar and S. P. Meyn. The ODE method for convergence
of stochastic approximation and reinforcement learning. SIAM
J. Control Optim., 38(2):447–469, 2000. (also presented at the
IEEE CDC, December, 1998).

[4] S. Chen, A. M. Devraj, A. Buˇsi´c, and S. P. Meyn. Zap q-learning
for optimal stopping time problems. arXiv preprint arXiv, 2019.
[5] G. Cheng. Note on some upper bounds for the condition number.
JOURNAL OF MATHEMATICAL INEQUALITIES, 8(2):369–
374, 2014.

[6] D. Choi and B. Van Roy. A generalized Kalman ﬁlter for ﬁxed
point approximation and efﬁcient temporal-difference learning.
Discrete Event Dynamic Systems: Theory and Applications,
16(2):207–239, 2006.

[7] A. M. Devraj and S. Meyn. Zap Q-learning.

In Advances in
Neural Information Processing Systems, pages 2235–2244, 2017.
[8] A. M. Devraj and S. P. Meyn. Fastest convergence for Q-learning.

ArXiv e-prints, July 2017.

[9] S. P. Meyn and R. L. Tweedie. Markov chains and stochastic
stability. Cambridge University Press, Cambridge, second edi-
tion, 2009. Published in the Cambridge Mathematical Library.
1993 edition online.

[10] A. Nedic and D. Bertsekas. Least squares policy evaluation
algorithms with linear function approximation. Discrete Event
Dynamic Systems: Theory and Applications, 13(1-2):79–110,
2003.

[11] A. Shwartz and A. Makowski. On the Poisson equation for
Markov chains: existence of solutions and parameter dependence.
Technical Report, Technion—Israel
Institute of Technology,
Haifa 32000, Israel., 1991.

[12] R. S. Sutton. Learning to predict by the methods of temporal

differences. Mach. Learn., 3(1):9–44, 1988.

[13] V. B. Tadic and S. P. Meyn. Asymptotic properties of two time-
scale stochastic approximation algorithms with constant step
sizes. In Proceedings of the 2003 American Control Conference,
2003., volume 5, pages 4426–4431. IEEE, 2003.

[14] J. N. Tsitsiklis and B. Van Roy. An analysis of temporal-
IEEE Trans.

difference learning with function approximation.
Automat. Control, 42(5):674–690, 1997.

[15] J. N. Tsitsiklis and B. Van Roy. Optimal stopping of Markov
processes: Hilbert space theory, approximation algorithms, and
an application to pricing high-dimensional ﬁnancial derivatives.
IEEE Trans. Automat. Control, 44(10):1840–1851, 1999.
[16] C. J. C. H. Watkins. Learning from Delayed Rewards. PhD
thesis, King’s College, Cambridge, Cambridge, UK, 1989.
[17] C. J. C. H. Watkins and P. Dayan. Q-learning. Machine

Learning, 8(3-4):279–292, 1992.

[18] H. Yu and D. P. Bertsekas. Q-learning algorithms for optimal
In 2007 European Control

stopping based on least squares.
Conference (ECC), pages 2368–2375. IEEE, 2007.

-600-400-2000200-100-50050100-1000100-1000100-1000100-200002000-300-200-1000100200KF 100KF 200Zap 1Zap 2Zap 5Zap 10Zap 0.1EmpiricalTheoreticalProof of Lemma 3.2: Based on the deﬁnition (29), we have:

VII. APPENDIX

(cid:107)F θQ − F θQ(cid:48)(cid:107) = β(cid:107)P H θQ − P H θQ(cid:48)(cid:107)

≤ β(cid:107)H θQ − H θQ(cid:48)(cid:107)
≤ β(cid:107)Q − Q(cid:48)(cid:107) ,

where the ﬁrst inequality follows from the fact that (cid:107)P (cid:107) ≤ 1 (with (cid:107)P (cid:107) being the induced operator norm in L2(π)).
The last inequality is true because:

H θQ(x) − H θQ(cid:48)(x) = Sθ

(cid:0)Q − Q(cid:48)(cid:1)(x) ,

x ∈ X

(cid:117)(cid:116)

Proof of Lemma 3.3: To show result (i), we rewrite A(θ) as the difference of two matrices, A(θ) = AN L(θ) −
AL, denoting AN L(θ) to be the part of the matrix that depends on θ and AL to be the one that is independent of
θ:

AN L(θ) := E[ψ(Xn)βSθψT(Xn+1)]
AL := E[ψ(Xn)ψT(Xn)]

Proving (30) is equivalent to proving:

vTAN L(θ)v − vTALv ≤ (β − 1)vTΣψv,

v ∈ Rd.

The proof is easier to follow if we suppose that the vector v is a difference of two parameter vectors, v = θ1 − θ2.
Expanding the left hand side of the above inequality:

vTAN L(θ)v = (θ1 − θ2)TE(cid:2)ψ(Xn)βP H θ(cid:0)Qθ1

(Xn) − Qθ2

Next, using Cauchy-Schwartz and the fact that (cid:107)P (cid:107)π ≤ 1,

= βE(cid:2)(cid:0)Qθ1

(Xn) − Qθ2

(Xn)(cid:1)P H θ(cid:0)Qθ1

(Xn)(cid:1)(cid:3)
(Xn) − Qθ2

(Xn)(cid:1)(cid:3).

− Qθ2
− Qθ2

vTAN L(θ)v ≤ β(cid:107)Qθ1
≤ β(cid:107)Qθ1
= βvTALv.
= (β − 1)vTALv + vTALv.

(cid:107)(cid:107)H θ(Qθ1
(cid:107)2

− Qθ2

)(cid:107)

Rearranging the terms, and noting that Σψ = AL, the statement of the Lemma follows:

vTA(θ)v = vTAN L(θ)v − vTALv
≤ (β − 1)vTΣψv

(48)

Next, for ﬁxed matrix A(θ) with eigenvalue-eigenvector pair λA ∈ C, v = a + bi ∈ Cd, we consider

v∗A(θ)v = (aT − bTi)A(θ)(a + bi) = aTA(θ)a + bTA(θ)b + [aTA(θ)b − bTA(θ)a]i

where v∗ denotes the conjugate transpose of v. With v∗A(θ)v = λAv∗v, it follows that

Re{λA}v∗v = Re{v∗A(θ)v} = aTA(θ)a + bTA(θ)b

Let λψ > 0 be the largest eigenvalue of Σψ, by the inequality (48), the following relation holds

Re{λA}v∗v = aTA(θ)a + bTA(θ)b

≤ (β − 1)λψ[aTa + bTb]
= (β − 1)λψv∗v

Therefore, Re{λA} is negative and bounded above by (β − 1)λψ. For the last part, (cid:107)A(θ)−1(cid:107)F is bounded using
an inequality from [5]
√

(cid:107)A(θ)−1(cid:107)F ≤

d
|detA(θ)|

(cid:16) (cid:107)A(θ)(cid:107)F
d − 1

(cid:17) d−1

2

(49)

where d is the dimension. Provided the bound over eigenvalues of A(θ) and compactness assumption of state space
X, there exists some constant (cid:96)A such that

(cid:107)A(θ)−1(cid:107)F ≤

√

d
[(1 − β)λψ]d

(cid:16) (cid:96)A

(cid:17) d−1

2

d − 1

The claim of uniform boundedness of {A(θ)−1 : θ ∈ Rd} then follows.

Proof of Lemma 3.4: For any two parameter vectors θ1, θ2 ∈ Rd, we have:

(cid:107)b(θ1) − b(θ2)(cid:107) = (cid:107)ψ(−F Qθ1

+ F Qθ2

− Qθ2

+ Qθ1
)(cid:107) + (cid:107)ψ(Qθ1

)(cid:107)
− Qθ2

)(cid:107) + (cid:107)ψ(Qθ1

− Qθ2

)(cid:107)

≤ (cid:107)ψ(F Qθ1
≤ β(cid:107)ψ(Qθ1
≤ (1 + β)(cid:107)ψ(cid:107)2(cid:107)θ1 − θ2(cid:107).

− F Qθ2
− Qθ2

(50)

(cid:4)

(cid:4)

)(cid:107)

Lemma 7.1: Suppose that f is a bounded function on X with zero mean. Then the sequence {f (Xn)} is ODE-
friendly, with {εk} equal to zero, and Tk = ˆf (Xk), with ˆf the solution to Poisson’s equation with forcing function
f ; That is, ˆf satisﬁes

ˆf (x) = f (x) − π(f ) + E[ ˆf (Xn+1)|Xn = x] , x ∈ X ,

where π(f ) := E[f (X)] , X ∼ π.

Proof: Let ˆf : X → Rd solve Poisson’s equation:

(cid:117)(cid:116)

with the forcing function f (Xn) in Lemma 7.1. The following decomposition is obtained:

E[ ˆf (Xn+1) − ˆf (Xn)|Fn] = f (Xn)

f (Xn) = E[ ˆf (Xn+1) − ˆf (Xn)|Fn]

= E[ ˆf (Xn+1)|Fn] − ˆf (Xn+1)
(cid:123)(cid:122)
(cid:125)
Martingale difference

(cid:124)

+ ˆf (Xn+1) − ˆf (Xn)
(cid:123)(cid:122)
(cid:125)
telescoping

(cid:124)

where ˆf (Xn) is bounded since the forcing function f (Xn) is bounded [9].

Lemma 7.2: Denote for n ≥ 0, Mψ,θ(n + 1) = ψ(Xn)SθψT(Xn+1). There exists a constant (cid:96)M < ∞ such that,

with probability one,

(cid:107)E(cid:2)Mψ,θ1(n + 1) − Mψ,θ2(n + 1) | Fn

(cid:3)(cid:107) ≤ (cid:96)M (cid:107)θ1 − θ2(cid:107)

(cid:117)(cid:116)

Proof: For θ1, θ2, by Markov property with Xn = x, we rewrite the term as

E(cid:2)ψ(Xn)Sθ1ψ(Xn+1)T|Fn

(cid:3) − E(cid:2)ψ(Xn)Sθ2 ψ(Xn+1)T|Fn

(cid:3)

= E(cid:2)ψ(x)Sθ1 ψ(Xn+1)T|Xn = x(cid:3) − E(cid:2)ψ(x)Sθ2 ψ(Xn+1)T|Xn = x(cid:3)

By Assumption A3, the terminating cost function cs − 1 is in the span of basis. There exists some constant θcs

such that

cs(y) ≡ ψT(y)θcs + 1,

∀y ∈ X.

Denote Z ∼ N (ψ(x), I) as the random variable with density pZ(·), we have, for some (cid:96)E > 0,

(cid:107)E(cid:2)ψ(x)Sθ1ψ(Xn+1)T|Xn = x(cid:3) − E(cid:2)ψ(x)Sθ2 ψ(Xn+1)T|Xn = x(cid:3)(cid:107)

≤ E(cid:2)(cid:107)ψ(x)ψT(Xn+1)(cid:107) · (cid:12)
= E(cid:2)(cid:107)ψ(x)ψT(Xn+1)(cid:107) · (cid:12)
(cid:107)ψ(x)zT(cid:107) · (cid:96)(z|x) · (cid:12)
(cid:90)

=

(cid:90)

(cid:12)I{ψ(Xn+1)Tθ1 ≤ cs(Xn+1)} − I{ψ(Xn+1)Tθ2 ≤ cs(Xn+1)}(cid:12)
(cid:12)I{ψ(Xn+1)T(θ1 − θcs) ≤ 1} − I{ψ(Xn+1)T(θ2 − θcs) ≤ 1}(cid:12)
(cid:12)I{zT(θ1 − θcs) ≤ 1} − I{zT(θ2 − θcs) ≤ 1}(cid:12)

(cid:12)pZ(z)dz

(cid:12)|Xn = x(cid:3)
(cid:12)|Xn = x(cid:3)

≤ (cid:96)E
= (cid:96)EE[(cid:12)

(cid:12)I{zT(θ1 − θc) ≤ 1} − I{zT(θ2 − θc) ≤ 1}(cid:12)
(cid:12)
(cid:12)I{Z T(θ1 − θc) ≤ 1} − I{Z T(θ2 − θc) ≤ 1}(cid:12)
(cid:12)

(cid:12)pZ(z)dz
(cid:3)

As a result, we only need to show that the expectation in the last line is Lipschitz continuous for a Gaussian random
variable, for which a bounded derivative of E[I{Z Tθ ≤ 1}] w.r.t θ will sufﬁce. For each θ, Z Tθ follows Gaussian
distribution N (θTψ(x), θTθ). Denote E[I{Z Tθ ≤ 1}] as m(θ)

m(θ) := E[I{Z Tθ ≤ 1}(cid:3) =

(cid:90) 1

−∞

√

1
2πθTθ

(cid:16)

−

exp

1

2(cid:107)θ(cid:107)2 (v − θTψ(x))2(cid:17)

dv

Applying the change of variable u =

1√

2(cid:107)θ(cid:107)2

(v − θTψ(x)), we have

(cid:90)

1√

2(cid:107)θ(cid:107)2

(1−θTψ(x))

1
(cid:112)2π(cid:107)θ(cid:107)2

exp (cid:0) − u2(cid:1)du(cid:112)2(cid:107)θ(cid:107)2

1√

2(cid:107)θ(cid:107)2

(1−θTψ(x))

exp (cid:0) − u2(cid:1)du

m(θ) =

=

−∞
(cid:90)

1
√
π

−∞

Its derivative is

We observe that

d
dθ

m(θ) = [

1
√
2π

−ψ(x)(cid:107)θ(cid:107) − (1 − ψ(x)Tθ) · (cid:107)θ(cid:107)−1θ
(cid:107)θ(cid:107)2

] exp (cid:0) −

(1 − θTψ(x))2
2(cid:107)θ(cid:107)2

(cid:1)

1) If (cid:107)θ(cid:107) → ∞, the exponential term is bounded by 1, and the coefﬁcient before exponential term goes to zero

since

ψ(x)
(cid:107)θ(cid:107)

+

(1 − ψ(x)Tθ)
(cid:107)θ(cid:107)2

→ 0

2) m(cid:48)(θ) also vanishes as (cid:107)θ(cid:107) → 0 since

(cid:2) ψ(x)
(cid:107)θ(cid:107)

+

(1 − ψ(x)Tθ)
(cid:107)θ(cid:107)2

(cid:3) exp (cid:0) −

(1 − θTψ(x))2
(cid:112)2(cid:107)θ(cid:107)2

(cid:1) → 0

Since the state space X is compact, there exists a deterministic bound over m(cid:48)(θ) that does not depend on x. As a
result, the objective in Lemma 7.2 is Lipschitz continuous with some deterministic Lipschitz constant (cid:96)M .

Lemma 7.3: There is a constant BZ such that the following holds: For any family of zero mean functions {fθ}

satisfying for some constants BF , (cid:96)F ,

(cid:107)fθ(x)(cid:107) ≤ BF ,

sup
x

(cid:107)fθ1 (x) − fθ2(x)(cid:107) ≤ (cid:96)F (cid:107)θ1 − θ2(cid:107)

sup
x

for all θ, θ1, θ2, then, there are solutions to corresponding Poisson’s equation, { ˆfθ}, with zero mean, and satisfying

(cid:107) ˆfθ1 (x) − ˆfθ2(x)(cid:107) ≤ BZ(cid:96)F (cid:107)θ1 − θ2(cid:107)

sup
x

(cid:117)(cid:116)

Proof: Let P denote transition kernel of the Markov chain, we deﬁne the following fundamental kernel Z

Z := [I − P + 1 ⊗ π]−1 =

∞
(cid:88)

[P − 1 ⊗ π]n = I +

∞
(cid:88)

[P n − 1 ⊗ π]

(51)

n=0

n=1

Note that ˆfθ = Zfθ solves Poisson’s equation:

P ˆfθ = ˆfθ − fθ

Since Z : L∞ → L∞ is a bounded linear operator, we have that for any x ∈ X,

(cid:107)hθ1 (x) − hθ2 (x)(cid:107) ≤ (cid:107)hθ1 − hθ2(cid:107)∞ ≤ (cid:107)Z(cid:107)∞(cid:107)fθ1 − fθ2 (cid:107)∞ ≤ (cid:96)F (cid:107)Z(cid:107)∞(cid:107)θ1 − θ2(cid:107)

where (cid:107)Z(cid:107)∞ denotes the induced operator norm.

Proof of Lemma 4.1: Based on (23, 24), the two error sequences in (42) are

n+1 = c(Xn)ψ(Xn) − b∗ + βS c
E θ
θn
E A
n+1 = An+1 − A(θn)

cs(Xn+1) − βcs(θn)

The argument proceeds by decomposing noise sequences into tractable terms, each of which is shown to be ODE-
friendly by standard arguments based on solutions to Poisson’s equation [11]. We only present the treatment for
n+1 here since same technique can be applied to E A
E A
Denote for n ≥ 0,

n+1θn and E θ

n+1.

Mψ,θ(n + 1) = ψ(Xn)SθψT(Xn+1)

For noise sequence E A

n+1 in (42b), we have

n+1 = An+1 − A(θn) = ψ(Xn)(cid:2)βSθn ψ(Xn+1) − ψ(Xn)(cid:3)T − E(cid:2)ψ(Xn)[βSθn ψ(Xn+1) − ψ(Xn)]T(cid:3)
E A

= ˜A1

n+1 + β ˜A2

n+1 + β ˜A3

n+1

(52)

where

˜A1
˜A2
˜A3

n+1 = −ψ(Xn)ψT(Xn) + E[ψ(Xn)ψT(Xn)]
n+1 = Mψ,θn (n + 1) − E[Mψ,θn (n + 1) | Fn]
n+1 = E[Mψ,θn (n + 1) | Fn] − E[Mψ,θn(n + 1)]

Lemma 7.1 implies that the sequence { ˜A1

n+1} is ODE-friendly since it is bounded over state space X, and has

zero mean. { ˜A2

n+1} is ODE friendly as it is martingale difference sequence with bounded moments.

For ˜A3

n+1, let ˆfθ : Θ × X → Rd×d solve Poisson’s equation:

E[ ˆfθ(Xn+1) − ˆfθ(Xn)|Fn] = fθ(Xn)

with forcing function fθ(Xn) := ˜A3

n+1. The following representation is obtained:

fθn (Xn) =E[ ˆfθn (Xn+1) − ˆfθn (Xn)|Fn]

=E[ ˆfθn (Xn+1)|Fn] − ˆfθn (Xn+1) + ˆfθn(Xn+1) − ˆfθn (Xn)
= E[ ˆfθn (Xn+1)|Fn] − ˆfθn(Xn+1)
(cid:125)
(cid:123)(cid:122)
Martingale difference

+ ˆfθn+1(Xn+1) − ˆfθn (Xn)
(cid:123)(cid:122)
(cid:125)
telescoping

(cid:124)

(cid:124)

+ ˆfθn(Xn+1) − ˆfθn+1 (Xn+1)
(cid:125)
(cid:123)(cid:122)
perturbation

(cid:124)

which admits the form of being ODE-friendly, provided the perturbation term εn := ˆfθn(Xn+1) − ˆfθn+1(Xn+1)
satisﬁes

∞
(cid:88)

k=1

γk(cid:107)εk(cid:107) < ∞,

a.s..

Recall that Lemma 7.2 shows ˜A3
n+1 is Lipschitz continuous with deterministic Lipschitz constant. Combining this
with Lemma 7.3 shows that ˆfθ(Xn+1) is uniformly Lipschitz continuous w.r.t θ with Lipschitz constant BZ(cid:96)F ,
which indicates

∞
(cid:88)

k=1

γk(cid:107)εk(cid:107) =

∞
(cid:88)

k=1

γk(cid:107) ˆfθk (Xk+1) − ˆfθk+1 (Xk+1)(cid:107) ≤

∞
(cid:88)

k=1

γkBZ(cid:96)F (cid:107)θk − θk+1(cid:107)

≤ BZ(cid:96)F

∞
(cid:88)

k=1

γkαk(cid:107) (cid:98)A†

kψ(Xk)(c(Xk) + β min(cs(Xk+1), Qθk (Xk+1)) − Qθk (Xk))(cid:107)

which is bounded by the boundedness assumption over {θn} and the fact that we use projected pseudo-inverse of
(cid:4)
(cid:98)An+1.

Proof of Lemma 4.2: Consider the update rules for θn, (cid:98)An in (23) and (24). When both update rules are viewed

as over the faster time-scale (that is, with step-size sequence {γn}), they can be rewritten as [2]:

θn+1 =θn + γn+1
(cid:98)An+1 = (cid:98)An + γn+1

(cid:2)o(1)(cid:3)
(cid:2)A(θn) − (cid:98)An + E A

n+1

(cid:3)

(53)

where the o(1) term is:

o(1) = −

αn+1
γn+1

nψ(Xn)(cid:2)c(Xn) + β min(cs(Xn+1), Qθn (Xn+1)) − Qθn (Xn)(cid:3)
(cid:98)A†

It goes to zero provided stability assumption of {θn} and boundedness of (cid:98)A†
a.s. to the internally chain transitive invariant set of the following ODE [2]

n. It follows that {θn, (cid:98)An} converges

˙w(t) = 0
˙A(t) = A(wt) − A(t)

Which is {(θ, A(θ)) : θ ∈ Rd}. In other words, (cid:107) ˆAn − A(θn)(cid:107) converges to 0 a.s.. While the invertibility of A(θ)
(cid:4)
has been established in Lemma 3.3.

Proof of Lemma 4.3: Within time interval [s, s+T ], consider the evolution of wtk over slow time scale deﬁned

by {αk}, where tk denote the time of k-th update of θ.

wtk+1 = wtk −αk+1 (cid:98)A†

(cid:104)
k+1ψ(Xk)

c(Xk)

+ β min(cs(Xk+1), wT
tk

ψ(Xk+1)) − wT
tk

(cid:105)
ψ(Xk)

Write it in standard stochastic approximation form and replace (cid:98)A†
4.2

k+1 with −A(θk)−1 + o(1) by Lemma 3.3 and

(cid:104)
wtk+1 = wtk −αk+1A−1(wtk )

A(wtk )wtk + βcs(wtk ) + b∗

+ E A

k+1wtk + E θ

k+1

(cid:105)

+ o(αk+1)

With the noise terms being

E A
k+1wtk = Ak+1wtk − A(wtk )wtk

k+1 = c(Xk)ψ(Xk) − b∗ + βcs(Xk+1)I{Qθk (Xk+1) > cs(Xk+1)} − βcs(θk)
E θ

With them being shown ODE-friendly in Lemma 4.1, the ODE approximation for wt follows under the stability
(cid:4)
assumption of θn [13].

Proof of Lemma 4.4: For result in (i), the ﬁrst relation holds since mapping b(·) is Lipschitz continuous. The
sub-sequential limit wt is the solution of the ODE in (43) since the right hand side of (43) is Lipschitz continuous.
It is differentiable everywhere within [0, T ].

By deﬁnition in (26a), bt = b(wt) can be written as follows:

bt = −A(wt)wt − βcs(wt)

= −E(cid:2)ψ(Xn)(β min (cid:0)cs(Xn+1), Qwt(Xn+1)(cid:1) − Qwt(Xn))(cid:3)
= E(cid:2)ψ(Xn)cwt (Xn)(cid:3)

(54)

where, for any w ∈ Rd, cw is the cost function that solves the ﬁxed point equation (5) with θ∗ replaced by w:

cw(x) := −E(cid:2)β min(cs(Xn+1), Qw(Xn+1)) | Xn = x(cid:3) + Qw(x),

x ∈ X

The cost function cw is Lipschitz in w. Therefore it is Lipschitz continuous over [0, T ] and absolutely continuous
over [0, T ] that has derivative almost everywhere. Let t0 be a point of differentiability for ct = cwt. Both wt and
ct(x) (for each x ∈ X) are approximated by a line at this time point:
wt = wt0 + (t − t0)vw + o(|t − t0|)

ct(x) = ct0 (x) + (t − t0)vc(x) + o(|t − t0|),

t ∼ t0

where vw, vc(x) are the respective derivatives. The assertion

vc(x) = −E[βSwt0

ψ(Xn+1)T − ψ(Xn)T | Xn = x]vw

(55)

will then imply the statement of the Lemma.

Denote Lc

t = ct0 + (t − t0)vc, Lw

t = wt0 + (t − t0)vw. For each t, we have:

t (x) = − E[β min(cs(Xn+1), QLw
Lc

t (Xn+1)) − QLw

t (Xn) | Xn = x]

t

QLw
QLw

= − E[βSLw
≥ − E[βSwt0
=ct0(x) − (t − t0)E[βSwt0

t (Xn+1) + βS c
Lw
t
t (Xn+1) + βS c

cs(Xn+1) − QLw
cs(Xn+1) − QLw
ψ(Xn+1)T − ψ(Xn)T | Xn = x]vw

wt0

t (Xn) | Xn = x]

t (Xn) | Xn = x]

The above inequality is true for both t > t0 and t < t0. For t ∼ t0, it follows that the inequality becomes an
equality, and therefore (55) holds. The ODE (44) for bt holds by replacing wt with bt in (43) using the above
(cid:4)
derivative.

