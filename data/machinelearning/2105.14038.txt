1
2
0
2

y
a
M
8
2

]

G
L
.
s
c
[

1
v
8
3
0
4
1
.
5
0
1
2
:
v
i
X
r
a

Learning to Extend Program Graphs to Work-in-Progress Code

Xuechen Li∗
Stanford University
lxuechen@stanford.edu

Chris J. Maddison
University of Toronto & Vector Institute
cmaddis@cs.toronto.edu

Daniel Tarlow
Google Research
dtarlow@google.com

Abstract

Source code spends most of its time in a broken or incomplete state during software
development. This presents a challenge to machine learning for code, since high-
performing models typically rely on graph structured representations of programs
derived from traditional program analyses. Such analyses may be undeﬁned for
broken or incomplete code. We extend the notion of program graphs to work-
in-progress code by learning to predict edge relations between tokens, training
on well-formed code before transferring to work-in-progress code. We consider
the tasks of code completion and localizing and repairing variable misuse in a
work-in-process scenario. We demonstrate that training relation-aware models with
ﬁne-tuned edges consistently leads to improved performance on both tasks.

1

Introduction

Source code written by people is natural in the sense of being predictable [30, 60]. This warrants
building machine learning models for code [4]. These models range from ones based on n-grams [30,
28] and graphical models [61], to those based on various neural networks [3, 13, 12, 84, 38, 66, 83,
49, 5, 6, 14, 27] and combinations thereof [29, 69, 80].

While promising results can be obtained by building models of the source code token se-
quence alone, domain-speciﬁc task performance can typically be improved by leveraging graph-
structured representations of programs, where edge relations are derived from traditional pro-
gram analyses [3, 10, 18, 17]. Given well-formed code, a variety of program analyses
can be run to determine properties of code semantics and relationships between code ele-
ments [16, 62, 51]. However, for work-in-progress code that may be broken or incomplete,
these analyses are typically undeﬁned under the programming language’s standard speciﬁcation.

One could attempt to build program graph libraries on top
of robust analyzers [1] or by training a machine learning
model to repair code and then analyzing the ﬁxed code.
Yet, both approaches create a new hard subproblem that
may not be necessary to solve perfectly in order to obtain
good performance on the downstream task. Instead, we
propose a simpler option that requires only the ability to
run program analyses on well-formed code, and results in
model components that can be ﬁned-tuned while training a
relation-aware model on a downstream task.

∗Work done during Google AI Residency.

Preprint. Under review.

Figure 1: Our relation model predicts
LastRead edges sensibly for a piece of
code with invalid indentation.

 
 
 
 
 
 
Consider the example in Figure 1 where there is an extra indentation in a piece of Python code. This
code cannot be executed, and thus it does not strictly make sense to analyze properties of its semantics.
However, one could reasonably expect that the data ﬂow relations illustrated by the arrows still hold.
In fact, these arrows are predictions made by our proposed relation prediction model, trained on
well-formed code and then transferred to the broken code snippet in the ﬁgure.

Our primary contribution is to show that graph-structured representations of programs derived from
traditional program analyses can be extended to broken and incomplete code by leveraging inherent
generalization in machine learning models that predict edges. We train these models with direct
supervision on well-formed code before transferring to work-in-progress code. We demonstrate the
effectiveness of this out-of-distribution generalization by showing that relation-aware models [65, 79]
can effectively use the predicted edges on work-in-process code. We evaluate our approach on
two downstream tasks where work-in-progress code is broken in different ways: (a) localizing and
repairing variable misuses and (b) code completion. By additionally ﬁne-tuning the relation prediction
model during task learning, we achieve strong improvements over relation-agnostic baselines. In total,
this work points towards a simple and practical solution for realizing the power of relation-aware
models on the work-in-progress code that commonly arises in the software development process.

2 Background

We review Transformers and relation-aware models for code, on which our work is heavily based.

Transformer. The Transformer model [71] consists of stacked multi-head attention and position-
wise fully-connected layers. A multi-head attention layer enables encodings at a position in subsequent
layers to attend to encodings at all locations in the previous layer via softmax attention whose scores
are computed as α = Softmax(q(cid:62)k/
d). Here, α is the attention score, q and k are respectively
the query and key, and d is the dimension of the encoding space.

√

Relation-aware Models for Source Code. Many machine learning models for code take into
account relational structure. These models include variants of graph neural networks [3] and Trans-
formers using relative positions [65, 29, 73, 84]. Our focus is speciﬁcally on models that modify the
standard attention computation [65, 19] by altering q or k using edge embeddings. We use the terms
relation-aware and edge-aware exclusively for these models in the paper for ease of presentation.

3 Learning to Extend Program Graphs

We propose to learn a relation prediction model for source code tokens so that relation-aware models
can be applied to work-in-progress code which may be broken or incomplete. We cast the learning
problem as a supervised one: Given ground truth edges between tokens, train a model to predict
edges given only the token sequence. This is a multi-label classiﬁcation problem, where multiple
edges of different types might exist between the same ordered token pair.

3.1 Ground Truth Relations

We use the edge relations described by Hellendoorn et al.
[29] as the ground truth. The relations are derived from
ASTs, control ﬂow graphs, and data ﬂow graphs of well-
formed code and are of 10 types. Not all types have
similar frequencies. For instance, among the relations
we consider, four types form less than 0.5% of the total
edges in a task we consider. The edge relations are also
very sparse: Less than 0.3% of the ordered token pairs
are connected (see Figure 2). The sparsity of edges
makes the binary classiﬁcation problem for each edge
type a label-imbalanced one, and we follow Johnson
et al. [34] in optimizing the focal loss [43] as a remedy.

Figure 2: Edge occurrence frequencies on
the large training and validation sets from
the Python corpus [37].

3.2 The Edge Prediction Model
Architecture. Our edge prediction model is based on the Transformer base architecture without its
ﬁnal encoding-to-logit layer (nlayers = 6, dmodel = 512, and dﬀ = 2048). We add a ﬁnal block to

2

CFGNEXTLASTREADLASTWRITECOMPUTEDFROMRETURNSTOFORMALARG NAMEFIELDNEXTSYNTAXLASTLEXICALCALLSEdge Type104103102Percentagetrainvalthis architecture that produces a logit tensor which has size quadratic in the sequence length, each
entry of which is used for computing a sigmoid score of whether a type of relation exists between
an ordered token pair. This ﬁnal block consists of (i) a multi-head attention that produces a tensor
of size (B, L, L, H), followed by (ii) a dense layer that produces a tensor of size (B, L, L, E),
where B is the batch size, L is the sequence length, H is the number of hidden units, and E is the
number of edge types. We use “Pre-LN” described in [77] and adopted in GPT-2 [59], where Layer
Normalization [8] is placed inside residual blocks. The multi-head attention in the ﬁnal block has
improved capacity with H = 32 and dmodel = 1024. This architecture is similar to the dot product
baseline used by Johnson et al. [34], but is different in that we train one model for all relation types.

Causal Masking. For applications with causal ordering (e.g. code completion), the model should
not attend to a future token when producing the edge score for two earlier appearing tokens. This is
because the model does not have access to future information at test time. Due to the edge prediction
model being based on the Transformer, it sufﬁces to apply the usual causal mask (masking out
position i’s attention to any position j > i) to the base Transformer and the last attention block.
This is because the edge logit between any ordered pair of distinct positions (i, j) is a position-wise
non-linear transform of the dot product between the encodings ei and ej, each of which is independent
of future information after applying the causal mask.

3.3 Using Predicted Relations

We apply the learned relation prediction model to code completion and localizing and repairing
variable misuse. For the variable misuse task, we speciﬁcally focus on localizing and repairing
such bugs for work-in-progress code that is broken. This task is motivated by the desiderata that a
machine-learning-powered IDE should be able to perform high quality analysis even on code that is
temporarily broken due to intermittent editing by a developer.

The core architecture we adopt use relative position representations [65, 19, 79]. Speciﬁcally, in all
Transformer architectures, we use the relative positional encoding described by Dai et al. [19]. The
corresponding attention computation has the following form:

αi,j = E(cid:62)
xi

(cid:124)

W(cid:62)

q Wk,EExj
(cid:125)

(cid:123)(cid:122)
(a)

+ E(cid:62)
xi
(cid:124)

W(cid:62)

q Wk,RRi−j
(cid:125)

(cid:123)(cid:122)
(b)

+ u(cid:62)Wk,EExj
(cid:123)(cid:122)
(cid:125)
(c)

(cid:124)

+ v(cid:62)Wk,RRi−j
(cid:123)(cid:122)
(cid:125)
(d)

(cid:124)

.

(1)

Here, αi,j and Ri−j are respectively the attention score and relative positional encoding from the
ith to the jth token in the sequence. Et is the encoding for token t, Wq is used to compute queries,
Wk,E is used to compute keys with the encoding, Wk,R is used to compute keys with the relative
position, and u and v are biases shared across all multi-head attention layers. The choice of relative
positional encoding as opposed to the absolute one is inspired by their improved performance on
code summarization [84].

To leverage the additional edge relations provided by the relation prediction models, we associate
each type of edge with a trainable embedding vector, and augment terms (b) and (d) in equation 1.
Concretely, suppose the ordered token pair (xi, xj) has a relation of type r with the embedding vector
R(r), the updated attention has the following form:

(cid:101)αi,j = αi,j + E(cid:62)

xi

(cid:124)

W(cid:62)

q Wk,R(cid:48)R(r)
(cid:125)

(cid:123)(cid:122)
(b(cid:48))

+ v(cid:62)Wk,R(cid:48)R(r)
(cid:125)
(cid:123)(cid:122)
(d(cid:48))

(cid:124)

.

(2)

When there exist multiple relations between an ordered pair, we aggregate their contributions by
summing the terms (b(cid:48)) and (d(cid:48)) for each edge type. For a multi-head attention layer, we replicate the
update in equation 2 across all heads. With the aid of ground truth relations, (b(cid:48)) has been used to
modify the attention in the GREAT [29] and RAT-SQL [73] models, whereas the combination of (b(cid:48))
and (d(cid:48)) has been used in the Code Transformer model [84]. Since the edges we model are sparse, the
additional term in equation 2 can be computed and backpropagated through with sparse primitives in
standard automatic differentiation libraries. In practice, we observe a minor overhead for training2.

2We observe a 7% wall time training overhead (estimated using torch.autograd.profiler) of the
relation-aware model compared to one without computing the additional term in equation 2. While wall time is
dependent on the hardware and implementation, we expect to see similar numbers across similar settings.

3

3.3.1 Architecture for Code Completion

The architecture for the code completion task is slightly involved due to the need to predict subwords,
multiple of which form a token. This is at odds with the ground truth edge relations described in
Section 3.1, which is between full tokens. While it is possible to extend the same edge relation
between tokens for subwords in a fashion of creating bipartite graphs (connect every subword for
token A to every subword for token B, if a relation exists between tokens A and B), our preliminary
experiments suggest that learning such extended edges between subwords is difﬁcult (e.g. precision
and recall scores for certain edge types are lower than 0.6 even after hyperparameter tuning).

We propose to maintain the edge relation structure,
but adjust the model for code completion to better
adapt to the edge data. Speciﬁcally, to predict each
token, our model predicts an encoding and gener-
ates all subwords for the token in an autoregressive
manner conditional on the encoding. Our archi-
tecture has three components: 1(cid:13) a Transformer
encoder that takes in averaged embeddings of sub-
words for each token with attention scores com-
puted based on equation 2, 2(cid:13) an attention-based
Gated Recurrent Unit (GRU, [15]) decoder which
predicts the next subword for each token given all
previous subwords, and 3(cid:13) a linear projection layer
that maps encodings to logits. See Figure 3 for an
illustration.

Figure 3: Architecture for the code completion
task consists of three blocks: 1(cid:13) a Transformer
encoder with attention based on relative posi-
tions, 2(cid:13) an attention-based GRU, and 3(cid:13) a lin-
ear projection. The individual subtoken embed-
dings passed to 1(cid:13) is reused in 2(cid:13). For ease of
demonstration, we limit each token to have 3
subwords in the ﬁgure. Hashtag marks the end
of a token.

The attention mechanism of the GRU 2(cid:13) is based on
that described by Luong et al. [44]: The subword
embedding and hidden state from previous steps
are given to the GRU cell to produce an output,
which is concatenated with the encoding of the
whole token produced by the Transformer. The
joint encoding is passed through a Multi-Layer Perceptron (MLP) with a single hidden layer to obtain
logits. We use a dimension size 1/2 of that of the Transformer encoding for the hidden layer in the
MLP. The overall model has roughly 2.3% fewer parameters than the base Transformer due to this
bottleneck hidden layer in the MLP. Since code completion has a natural temporal ordering, we apply
causal masks at the token level during training to prevent the model from peaking at the future.

3.3.2 Architecture for Variable Misuse

For the variable misuse task, we adopt the model used by Hellendoorn et al. [29], except that we
compute attention scores using the formulation based on relative positions in equation 2. In summary,
the model takes in a sequences of subwords, averages the subword embeddings for each token, and
feeds the embedding into a Transformer. To produce candidate locations for bug and repair targets,
the model includes a linear projection at the end which outputs two logits for each location.

3.4 Backpropagating Through Predicted Relations

While a ﬁxed relation model brings some performance gains (as we show in Section 4), the paradigm
is not restricted to use only a ﬁxed model. In fact, when training to optimize performance for a
speciﬁc task, one may adapt the relation model by backpropagating through the discrete structure
using any method within the established suite [76, 46, 32, 11, 56].

For simplicity, we explore using a variant of the straight-through estimator [31]. Speciﬁcally, for a
binary relation random variable with logit l, we use the “hard-version” 1[l ≥ 0] during the forward
pass, and during backpropagation treat the variable as if the “soft-version” σ(l/τ ) were used in
the forward pass. Here, σ(·) is the sigmoid function and τ is a temperature hyperparameter. This
node operates differently than the Gumbel-Softmax/Concrete random variable, since no additional
noise variable is sampled. While Bengio et al. [11] reported worse results when using this estimator
compared to straight-through, we found the latter to fail on our task.

4

...Transformer encoder + relative positional encodingencoding 1encoding 2encoding (L-1)def#<PAD><PAD>avg. embedding 1_send_msg#avg. embedding 2options#<PAD>avg. embedding (L-1)def_send_msgoptionsGRUCellGRUCellGRUCell_send__send_msg#GRUCellGRUCellGRUCell(#<PAD>(#<PAD><PAD>...GRUCellGRUCellGRUCell)#<PAD>)#<PAD><PAD>321hhhhhhhhh(a) precision

(b) recall

(c) F score

Figure 4: Relation prediction model produces high quality edges on the validation and test splits of
the Python corpus [37]. The model has an F score above 0.8 for all edge types, and attains an F score
close to 1.0 for the most frequently occurring types.

We will see that even on clean code, ﬁne-tuning the relation prediction model in this manner leads to
improved performance compared to just using the ﬁxed ground truth edges. This improvement is also
observed across different settings for work-in-progress code with learned edges (see Section 4.4).

4 Experiments

We present experiments in four subsections. We ﬁrst verify that we can learn edge prediction models
of reasonable quality across a suite of edge types. We then present results on two applications: (a)
code completion without assuming the existence of any partial AST structure, and (b) localizing and
repairing variable misuse for ill-formed work-in-progress code. To close the section, we present some
analyses for ﬁne-tuning the edge prediction model during task optimization. All reported numbers
are averaged over three seeds. Error bars are based on one standard deviation.

4.1 Learning Relations for Source Code

We show that the edge prediction model described in Section 3.2 is able to learn a wide variety of
relations by training such as models for each task separately on their task speciﬁc data for the two
tasks we consider (code completion and variable misuse). We report the shared settings and overall
results in this subsection.

We trained all edge prediction models with the Adam optimizer [39] using a ﬁxed learning rate of
0.0001 and a batch size of 48. For other hyperparameters, we adopted the default in PyTorch. We
performed early stopping based on validation performance with a patience of 50k iterations. For focal
loss, we set its hyperparameter γ = 2 for all experiments as this was reported to be near optimal
for a wide array of settings [43]. Training the edge prediction model is memory intensive due to
its last layer instantiating several tensors with size quadratic in the sequence length L and linear in
the number of hidden units H or edge types E. To reduce the memory footprint during training, we
apply gradient checkpointing for the multi-head attention layers. With this setup, training converges
to a validation F score of around 99% within 3 days for both tasks using a single V100 GPU.

Results. Since occurrences of edges are extremely sparse, a model that always predicts the non-
existence of edges can attain an accuracy beyond 99% on the test split. We therefore report the
precision, recall, and F score. Despite having a simple architecture, the edge prediction model is able
to produce high quality predictions for the code completion task and attains an F score close to 1.0
for edge types with the highest frequencies (see Figure 4). For low frequency edge types and edge
types related to control ﬂow (e.g. ReturnsTo and CFGNext), the model performs slightly worse, but
still achieves an F score beyond 0.8. Similar but slightly worse results are obtained for the variable
misuse task; see Appendix A for results.

4.2 Code Completion

We adopted the Python source code dataset used by Karampatsis and Sutton [36] that is comprised of
two training splits (one small and one large), a validation, a test, and an encoding split used only for
learning a tokenization. The corpus sanitized by Karampatsis et al. [37] is licensed under CC BY 4.0.

5

CFGNEXTLASTREADLASTWRITECOMPUTEDFROMRETURNSTOFORMALARG NAMEFIELDNEXTSYNTAXLASTLEXICALCALLSEdge type0.00.20.40.60.81.0PrecisionCFGNEXTLASTREADLASTWRITECOMPUTEDFROMRETURNSTOFORMALARG NAMEFIELDNEXTSYNTAXLASTLEXICALCALLSEdge type0.00.20.40.60.81.0RecallCFGNEXTLASTREADLASTWRITECOMPUTEDFROMRETURNSTOFORMALARG NAMEFIELDNEXTSYNTAXLASTLEXICALCALLSEdge type0.00.20.40.60.81.0F scorevaltestTable 1: Results for next token prediction. Small and Large respectively refer to when the small
and large training split is used. Numbers in bold are the best results in each column. We exclude
comparing to Aware-true which cannot be obtained in practice given any unseen code preﬁx.

Model

Perplexity (↓)

Top-1 accuracy in % (↑)

Small

Large

Small

Large

Transformer (stride=100) 12.084 ± 0.215
4.249 ± 0.051
Transformer (stride=5) 12.002 ± 0.221 4.217 ± 0.051

66.152 ± 0.583
66.158 ± 0.584

76.380 ± 0.188
76.444 ± 0.189

Agnostic
Aware-ﬁxed
Aware-tuned

Aware-true

79.776 ± 0.064
14.858 ± 0.170
14.637 ± 0.045
79.686 ± 0.002
12.381 ± 0.189 3.495 ± 0.016 72.350 ± 0.065 80.745 ± 0.059

71.576 ± 0.106
71.857 ± 0.156

3.764 ± 0.019
3.782 ± 0.012

13.458 ± 0.171

3.535 ± 0.003

73.627 ± 0.148

81.347 ± 0.017

Dataset Preprocessing. We followed the preprocessing setups in [28, 36]: replace non-ASCII
character sequences such as Chinese ideograms inside strings with the special token 〈non-en〉, remove
comments, and replace string literals consisting of 15 characters or more by the empty string. After
preprocessing, the small training, large training, validation and test sets contain 20738, 999000,
14090 and 12865 ﬁles, respectively. Since the dataset consists of Python projects mined directly from
Github, certain sequences are very long and contain a variety of long words. We also converted all
ﬁles in Python2 to Python3 using Python’s automated translation tool lib2to3. To reduce the size
of the vocabulary, we followed Karampatsis et al. [37] in splitting tokens into subtokens with the
byte-pair encoding algorithm [64] ran for 10k merges using the Tokenizer package from Hugging
Face. While the model presented in Section 3.3 is in principle able to handle tokens that consist
of an arbitrary number of subwords, we truncate each token to have a maximum of 6 subwords for
computational efﬁciency. This covers 98.9% and 99.2% of the vocabulary in the small training and
validation sets, respectively. We trained the edge model with the usual causal masks for Transformers.

Training and Evaluation. Since many ﬁles are longer than the typical context size of a
Transformer-based model, which parts of a ﬁle should one train with becomes a question. Un-
like Karampatsis et al. [37], we did not take a ﬁxed size preﬁx of each ﬁle. Instead, we sampled
uniformly a new window each time a ﬁle is selected for a gradient update. The window is of the
context size if the ﬁle is longer; otherwise, it is chosen to be the whole ﬁle. For testing, we sampled 3
ﬁxed windows for each test ﬁle. We set the context size to be 256 for computational efﬁciency.

We compared the performance of the relation-aware architecture described in Section 3.3 trained
and tested with a learned edge prediction model against a model with no relational information
(relation-agnostic). We also report results for the relation-aware models trained and tested with
ground truth edges as a guideline for the gain one could expect with the present suite of edge types.

As a baseline, we also report results for the Transformer base architecture with attention scores
computed as in equation 1, modeling sequences directly at the subword level3. During training,
we sampled windows of subwords of context size 256. Since examples in the test set can vary in
length and contain up to 256 tokens, which may be “ﬂattened” into much more than 256 subwords,
we performed the typical sliding window evaluation starting at the beginning of each test window
and report results with various strides. With our implementation, each gradient update for the
baseline is roughly 1.7x as fast as that for the relation-aware architecture proposed in Section 3.3 in
terms of wall time. However, inference for the baseline is much slower due to the sliding window
evaluation (approximately 1/10 and 1/20 the speed of relation-aware model with strides 100 and 5,
respectively).

For all experiments, we used the Adam optimizer with a ﬁxed learning rate of 0.0001 and adopted the
PyTorch default for other hyperparameters. We applied gradient clipping with a max 2-norm of 0.25.
These hyperparameter choices were primarily optimized for training stability. Additionally, we set
the dropout rate to be 0.1 and also randomly dropout subwords to reduce overﬁtting. We used a batch

3The architecture is therefore very similar to Transformer-XL [19] with the minor difference being the
positioning of Layer Normalization and Dropout. Since the present focus is not long sequence modeling, our
models also don’t have their segment-level recurrence.

6

Table 2: Results for the variable misuse task on ill-formed code each with k corruptions.

Task

Model

k = 1

Small

k = 2

k = 5

k = 1

Large

k = 2

k = 5

Localization

Agnostic

44.0 ± 0.8 43.0 ± 1.7 42.3 ± 2.2 75.4 ± 1.1 76.0 ± 0.3 72.6 ± 1.1
Aware-ﬁxed 61.5 ± 0.5 61.6 ± 0.1 60.6 ± 0.5 76.6 ± 0.3 75.6 ± 0.6 73.3 ± 1.7
Aware-tuned 68.6 ± 0.2 68.0 ± 0.4 67.6 ± 0.4 79.2 ± 0.1 79.0 ± 0.1 78.4 ± 0.3

Repair

Agnostic

25.9 ± 1.1 24.0 ± 2.1 23.4 ± 2.7 63.7 ± 6.3 69.6 ± 1.1 65.5 ± 0.8
Aware-ﬁxed 45.0 ± 1.0 44.1 ± 1.2 42.5 ± 0.3 69.3 ± 0.3 66.4 ± 4.6 66.7 ± 1.6
Aware-tuned 55.8 ± 0.2 55.7 ± 0.3 54.4 ± 0.1 73.9 ± 0.5 73.8 ± 0.0 72.8 ± 0.2

size of 32 and data parallel training across 8 V100 GPUs across all settings. We trained all models for
at most 1k epochs and early stopped based on validation performance with a patience of 50k updates.
Training time is between 1-2 weeks for the large dataset and less than 1 day for the small dataset.

Results. We report the perplexity and top-1 accuracy for next token prediction in Table 1. The
table shows that compared to a relation-agnostic model, the relation-aware model given the ground
truth edges is able to perform much better in both perplexity and accuracy. Nevertheless, when using
only learned and ﬁxed edges, the relation-aware model performs similarly as the relation-agnostic
baseline. Further ﬁne-tuning the edge prediction model brings a noticeable performance gain to the
relation-aware setting (0.774% and 0.969% absolute gain in top-1 accuracy for Small and Large).

Notably, both the agnostic and aware model perform better than the baseline transformers which
directly model subword sequences on all metrics except the perplexity score when the small training
split is used. Results on next subword prediction have the same trend. See Appendix D for details.

To diagnose the poor performance of the relation-aware model supplied with learned and ﬁxed
edges, we conducted ablation studies and observe that the edges types most difﬁcult to predict are
the most inﬂuential in achieving the performance gain. This indicates that downstream accuracy
is non-uniformly affected by different edge types and suggests that future work may shift edge
prediction capacity to the harder edge types. Details of this study can be found in Appendix C.

We do not compare directly against results in [37], since our setting is not strictly comparable to
theirs due to subtle differences in data preprocessing (e.g. the speciﬁc procedure of creating subwords
is different). However, we note that our reported perplexities are in the range of that reported for their
static evaluation setting and is lower possibly due to using the Transformer architecture and that we
don’t merely train on ﬁle preﬁxes (e.g. our models trained on the large split attains a per-token cross
entropy around 1.2, whereas they report a cross entropy of 2.09 for their LSTM models).

4.3 Localizing and Fixing Variable Misuse for WIP Code

With a pretrained edge prediction model, we show that relational structure can be generalized to
ill-formed code to improve task performance. We focus on a setting where the code to be analyzed for
variable misuse issues has undergone multiple mild perturbations such that a standard parser would
fail to generate ASTs4. We consider the following set of perturbations:

• Keyword: Randomly corrupt keywords in the language (e.g. for, while, if, def in Python).
• Deletion: Randomly delete a ﬁxed number of tokens.
• Punctuation: Add random punctuation marks at random locations.
• Indentation5: Indent or dedent a randomly selected span of lines.

We based our dataset on that used by Hellendoorn et al. [29] and applied a ﬁxed amount of perturba-
tions to each ﬁle. The perturbations were uniformly sampled to be one of the types described above.
We created three perturbed datasets, varying the number of corruptions applied to each example in the
training, validation, and test splits (number of corruptions k = 1, 2, 5). To show the effect of dataset

4While it is desirable to seek for methods that may handle strong perturbations, learning to generalize to

out-of-distribution examples “in the wild” is still in general an open problem in machine learning [41].

5We avoid (re)indenting only the ﬁrst line in any indentation block, since in this scenario, any standard

tokenizer would be confused about the amount of whitespace characters for each tab/indentation.

7

(a) well-formed

(b) work-in-progress

Figure 5: Fine-tuning the edge prediction model leads to improved repair accuracy on well-formed
and work-progress code. Different curves terminate at different times due to early stopping. Work-in-
progress examples in (b) each contain k = 5 corruptions. “aware-ﬁxed” uses a pretrained relation
model that is not jointly optimized with the task model. “aware-true” uses the ground truth edges and
is only applicable for well-formed code. Results are based on training on the large training splits.

size on the model’s performance, we created a separate small training set based on subsampling 1%
of the original training set. For efﬁciency, we also ignored examples in the training and test sets that
are longer than the context size of 512 tokens. We adopted the same hyperparameters and setup for
experiments in this section as those used previously, except we parallelize training across 4 V100
GPUs for each experiment. Training time is between 1-2 weeks for the large training set and less
than 1 day for the small training set.

Results. Table 2 shows that with a ﬁxed pretrained edge prediction model, a relation-aware model
(Aware-ﬁxed) is able to achieve a much better localization and repair performance compared to
the relation-agnostic baseline (Agnostic) when the training set is small. This advantage, however,
diminishes as training data become abundant. On the other hand, by further ﬁne-tuning the edge
prediction model during downstream learning (Aware-tuned), the relation-aware model achieves
consistent gains compared to both using the learned-but-ﬁxed edges and the relation-agnostic baseline.

4.4 Fine-tuning the Relation Prediction Model: Is Pretraining to Predict Edges Necessary?

We aim to gain a better understanding of why ﬁne-tuning the relation prediction model helps.
Speciﬁcally, we seek to answer the following question: Is ﬁne-tuning the relation model helpful
solely because the joint model has more parameters or is it that the joint model is beneﬁtting from
the structure learned by the relation model? To answer this question, we trained a relation-aware
model for the variable misuse task where edges come from a randomly initialized relation model that
is jointly optimized with the task model (aware-random-init). We compare this model against the
relation-aware model with ﬁne-tuned edges initialized from pretraining (aware-pretrain-init), and the
relation-agnostic model (agnostic) for both well-formed and work-in-progress code.

Results. Figure 5 (a) demonstrates that ﬁne-tuning a pretrained relation model during downstream
learning improves on using the ground truth edges6. Moreover, jointly training relation-aware models
with relation models initialized with random weights has a performance no better than the relation-
agnostic baseline. This suggests that the pretrain-then-ﬁne-tune combination is the key to producing a
relation model useful for downstream learning. The same trend consistently holds for models trained
on the small training split and work-in-progress code with various perturbation levels (see Appendix
B for results and comments).

5 Related Work

The problem of identifying structure from broken and incomplete code is not new. In the programming
languages community, the problem has been studied as part of robust parsing and error handling in

6The careful reader who champions the utility of hand-crafted relations derived from expert knowledge might
ﬁnd this result curious. We note it is in general unsurprising that features extracted from large amounts of data
with neural nets may beat their hand-crafted counterparts in the data-rich regime [20, 42, 58]

8

02000004000006000008000001000000Iterations0.00.10.20.30.40.50.60.70.8Repair Accuracyagnosticaware-random-initaware-trueaware-pretrain-init0200000400000600000800000Iterations0.00.10.20.30.40.50.60.70.8Repair Accuracyagnostic (k=5)aware-random-init (k=5)aware-fixed (k=5)aware-pretrain-init (k=5)compilers [75, 2], where popular approaches are based on parsing with non-standard productions [47,
52, 70, 48]. These approaches rely on language dependent and hand-designed grammars and typically
require multiple stages of execution. Notably, the fuzzy parser in Joern runs in three stages, each
of which follows a separate bespoke island grammar [78]. While being less structured than robust
parsing, our approach of learning and transferring edges is more ﬂexible and direct.

We draw inspiration from a number of recent works on learning edge structure for use in relation-
aware neural networks and graph neural networks [33, 40, 25, 34]. Compared to these works, the main
methodological distinction comes from our assumption about the availability of strong supervision.
Whereas strong supervision for latent variables is often costly to obtain, e.g., meaning representations
in semantic parsing [82], our key observation is that it is readily available for edge structures in well-
formed code. Thus, the main challenge is transferring from the distribution where strong supervision
is available to a distribution where it is not. Perhaps the most similar work is that by Velickovic et al.
[72], which assumes strong supervision for edges is available on small examples, but then the goal is
to systematically generalize to longer examples. A difference is that in our setting, there is not clearly
the same kind of systematicity in the needed generalization.

From the application perspective, there are alternative approaches to dealing with work-in-progress
code. A common approach for source code modeling is to deﬁne relations that can be computed
from a program preﬁx or a partial expansion using standard productions [45, 50, 55, 57, 81, 13,
38, 7]. Relations are then explicitly deﬁned for the work-in-progress case and can be computed
deterministically. However, these approaches usually are only possible in restricted cases and would
not be applicable to all useful edge types, or remain valid for all corruptions encountered in practice.

It is worthwhile to mention that to handle slightly buggy code, a different approach would be to
leverage existing bug ﬁxers [21, 9, 63] to transform buggy instances into well-formed ones and
thereafter perform downstream tasks based on the latter as input. While potentially excelling at
handling code with common errors [9], this approach is limited in handling the incomplete code
preﬁxes studied in this paper. Aside, we note that the approach is not inherently in conﬂict with ours,
as one could imagine leveraging learned relations to build improved neural bug ﬁxers.

While our work is not solely devoted to code completion, we comment on several recent neural-based
attempts in the literature. Svyatkovskoy et al. [68] consider aiding completion candidate providers
(most notably ones based on static analysis) with learned neural representations. Their setting differs
from ours in assuming access to a candidate provider of reasonable quality upfront. On the other
hand, Svyatkovskiy et al. [67] and Wang et al. [74] respectively study multilingual and whole line
completion using Transformers. Our work is, in every respect, orthogonal to the two aforementioned,
as the idea of leveraging ﬁne-tuned relations for better completion is applicable to both settings

Lastly, we comment that ML4Code researchers have borrowed successful ideas in the NLP community
such as pretraining large Transformers on heterogenous datasets for transfer learning and multi-task
learning [24, 26, 23, 35]. The idea of learning to predict edges has already seen modest successes
for pretraining [26]. Exploring whether learning relations could be useful beyond this setting is an
interesting direction for future research.

6 Conclusion and Limitation

We showed that by simply learning a relation prediction model, the notion of program graphs can be
generalized to work-in-progress code. This enables relation-aware models to be used for source code
that may be broken or incomplete. Task performance is signiﬁcantly improved when a pretrained
relation model is simultaneously ﬁne-tuned during task optimization for both work-in-progress and
well-formed code. Future work may investigate whether the ﬁne-tuning approach could improve
the performance on other tasks and with other types of relational structure (such as ones used by
graph neural networks for algorithmic reasoning [72], ones in text data with corresponding parse
trees [53, 54], or even ones in knowledge graphs [22]).

Having improved ﬂexibility and capacity, modern-day neural-based approaches for learning are
typically at the same time less transparent and interpretable. Our relation learning procedure is
no outlier in this regard: Fine-tuned relation models may predict edges that are unexpected and
conform less to their initially prescribed category. Future work may improve the explainability and
interpretability aspects of adapted relations.

9

Acknowledgements

We thank Michael Sloan and Tatsunori Hashimoto for helpful discussions. We thank the ML4Code
team at Google and Michihiro Yasunaga for helpful feedback on an earlier draft.

References

[1] Joern: Open-source code analysis platform for c/c++/java based on code property graphs.

https://github.com/joernio/joern. Accessed: 2021-05-20.

[2] Alfred V Aho, Ravi Sethi, and Jeffrey D Ullman. Compilers, principles, techniques. Addison

wesley, 7(8):9, 1986.

[3] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent

programs with graphs. arXiv preprint arXiv:1711.00740, 2017.

[4] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of
machine learning for big code and naturalness. ACM Computing Surveys (CSUR), 51(4):1–37,
2018.

[5] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. code2seq: Generating sequences from

structured representations of code. arXiv preprint arXiv:1808.01400, 2018.

[6] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. code2vec: Learning distributed
representations of code. Proceedings of the ACM on Programming Languages, 3(POPL):1–29,
2019.

[7] Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. Structural language models of code. In

International Conference on Machine Learning, pages 245–256. PMLR, 2020.

[8] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint

arXiv:1607.06450, 2016.

[9] Johannes Bader, Andrew Scott, Michael Pradel, and Satish Chandra. Getaﬁx: Learning to ﬁx
bugs automatically. Proceedings of the ACM on Programming Languages, 3(OOPSLA):1–27,
2019.

[10] Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoeﬂer. Neural code comprehension: A

learnable representation of code semantics. arXiv preprint arXiv:1806.07336, 2018.

[11] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.

[12] David Bieber, Charles Sutton, Hugo Larochelle, and Daniel Tarlow. Learning to exe-
cute programs with instruction pointer attention graph neural networks. arXiv preprint
arXiv:2010.12621, 2020.

[13] Marc Brockschmidt, Miltiadis Allamanis, Alexander L Gaunt, and Oleksandr Polozov. Genera-

tive code modeling with graphs. arXiv preprint arXiv:1805.08490, 2018.

[14] Xinyun Chen, Chang Liu, and Dawn Song. Tree-to-tree neural networks for program translation.

Advances in neural information processing systems, 31:2547–2557, 2018.

[15] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-
decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.

[16] Patrick Cousot and Radhia Cousot. Abstract interpretation: a uniﬁed lattice model for static
analysis of programs by construction or approximation of ﬁxpoints. In Proceedings of the 4th
ACM SIGACT-SIGPLAN symposium on Principles of programming languages, pages 238–252,
1977.

10

[17] Chris Cummins, Zacharias V Fisches, Tal Ben-Nun, Torsten Hoeﬂer, and Hugh Leather. Pro-
graml: Graph-based deep learning for program optimization and analysis. arXiv preprint
arXiv:2003.10536, 2020.

[18] Milan Cvitkovic, Badal Singh, and Animashree Anandkumar. Open vocabulary learning on
source code with a graph-structured cache. In International Conference on Machine Learning,
pages 1475–1485. PMLR, 2019.

[19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov.
Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv preprint
arXiv:1901.02860, 2019.

[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.

[21] Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang. Hoppity:
Learning graph transformations to detect and ﬁx bugs in programs. In International Conference
on Learning Representations (ICLR), 2020.

[22] Lisa Ehrlinger and Wolfram Wöß. Towards a deﬁnition of knowledge graphs. SEMANTiCS

(Posters, Demos, SuCCESS), 48:1–4, 2016.

[23] Ahmed Elnaggar, Wei Ding, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Silvia
Severini, Florian Matthes, and Burkhard Rost. Codetrans: Towards cracking the language of
silicone’s code through self-supervised deep learning and high performance computing. CoRR,
abs/2104.02443, 2021. URL https://arxiv.org/abs/2104.02443.

[24] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun
Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. Codebert: A pre-trained model for
programming and natural languages. CoRR, abs/2002.08155, 2020. URL https://arxiv.
org/abs/2002.08155.

[25] Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. Learning discrete
structures for graph neural networks. In International conference on machine learning, pages
1972–1982. PMLR, 2019.

[26] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan
Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin B. Clement,
Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou. Graphcodebert: Pre-
training code representations with data ﬂow. CoRR, abs/2009.08366, 2020. URL https:
//arxiv.org/abs/2009.08366.

[27] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. Deepﬁx: Fixing common
c language errors by deep learning. In Proceedings of the Thirty-First AAAI Conference on
Artiﬁcial Intelligence, pages 1345–1351, 2017.

[28] Vincent J Hellendoorn and Premkumar Devanbu. Are deep neural networks the best choice
for modeling source code? In Proceedings of the 2017 11th Joint Meeting on Foundations of
Software Engineering, pages 763–773, 2017.

[29] Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber.
Global relational models of source code. In International Conference on Learning Representa-
tions, 2019.

[30] Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. On the
naturalness of software. In 2012 34th International Conference on Software Engineering (ICSE),
pages 837–847. IEEE, 2012.

[31] Geoffrey Hinton, Nitsh Srivastava, and Kevin Swersky. Neural networks for machine learning.

Coursera, video lectures, 264(1), 2012.

[32] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.

arXiv preprint arXiv:1611.01144, 2016.

11

[33] Daniel D Johnson. Learning graphical state transitions. In International Conference on Learning

Representations, 2017.

[34] Daniel D Johnson, Hugo Larochelle, and Daniel Tarlow. Learning graph structure with a

ﬁnite-state automaton layer. arXiv preprint arXiv:2007.04929, 2020.

[35] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating
contextual embedding of source code. In International Conference on Machine Learning, pages
5110–5121. PMLR, 2020.

[36] Rafael-Michael Karampatsis and Charles Sutton. Maybe deep neural networks are the best

choice for modeling source code. arXiv preprint arXiv:1903.05734, 2019.

[37] Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and Andrea Janes.
arXiv preprint

Big code!= big vocabulary: Open-vocabulary models for source code.
arXiv:2003.07914, 2020.

[38] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. Code prediction by feeding trees

to transformers. arXiv preprint arXiv:2003.13848, 2020.

[39] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

[40] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural

relational inference for interacting systems. arXiv preprint arXiv:1802.04687, 2018.

[41] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay
Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds:
A benchmark of in-the-wild distribution shifts. arXiv preprint arXiv:2012.07421, 2020.

[42] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. Advances in neural information processing systems, 25:1097–
1105, 2012.

[43] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense
object detection. In Proceedings of the IEEE international conference on computer vision,
pages 2980–2988, 2017.

[44] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-

based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.

[45] Chris Maddison and Daniel Tarlow. Structured generative models of natural source code. In

International Conference on Machine Learning, pages 649–657. PMLR, 2014.

[46] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous

relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.

[47] Leon Moonen. Generating robust parsers using island grammars.

In Proceedings Eighth

Working Conference on Reverse Engineering, pages 13–22. IEEE, 2001.

[48] Leon Moonen. Lightweight impact analysis using island grammars. In Proceedings 10th

International Workshop on Program Comprehension, pages 219–228. IEEE, 2002.

[49] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. Convolutional neural networks over tree
structures for programming language processing. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, volume 30, 2016.

[50] Anh Tuan Nguyen and Tien N Nguyen. Graph-based statistical language model for code. In
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering, volume 1,
pages 858–868. IEEE, 2015.

[51] Flemming Nielson, Hanne R Nielson, and Chris Hankin. Principles of program analysis.

Springer Science & Business Media, 2004.

12

[52] Emma Nilsson-Nyman, Torbjörn Ekman, and Görel Hedin. Practical scope recovery using
bridge parsing. In International Conference on Software Language Engineering, pages 95–113.
Springer, 2008.

[53] Joakim Nivre, Marie-Catherine De Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajic, Christo-
pher D Manning, Ryan McDonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, et al. Universal
dependencies v1: A multilingual treebank collection. In Proceedings of the Tenth International
Conference on Language Resources and Evaluation (LREC’16), pages 1659–1666, 2016.

[54] Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Hajiˇc, Christopher D Manning,
Sampo Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. Universal dependencies
v2: An evergrowing multilingual treebank collection. arXiv preprint arXiv:2004.10643, 2020.

[55] Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and
Pushmeet Kohli. Neuro-symbolic program synthesis. arXiv preprint arXiv:1611.01855, 2016.

[56] Max B Paulus, Dami Choi, Daniel Tarlow, Andreas Krause, and Chris J Maddison. Gradient

estimation with stochastic softmax tricks. arXiv preprint arXiv:2006.08063, 2020.

[57] Maxim Rabinovich, Mitchell Stern, and Dan Klein. Abstract syntax networks for code genera-

tion and semantic parsing. arXiv preprint arXiv:1704.07535, 2017.

[58] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language

understanding by generative pre-training. 2018.

[59] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.

Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

[60] Baishakhi Ray, Vincent Hellendoorn, Saheel Godhane, Zhaopeng Tu, Alberto Bacchelli, and
Premkumar Devanbu. On the" naturalness" of buggy code. In 2016 IEEE/ACM 38th Interna-
tional Conference on Software Engineering (ICSE), pages 428–439. IEEE, 2016.

[61] Veselin Raychev, Martin Vechev, and Andreas Krause. Predicting program properties from" big

code". ACM SIGPLAN Notices, 50(1):111–124, 2015.

[62] Thomas Reps. Program analysis via graph reachability. Information and software technology,

40(11-12):701–726, 1998.

[63] Caitlin Sadowski, Jeffrey Van Gogh, Ciera Jaspan, Emma Soderberg, and Collin Winter. Tri-
corder: Building a program analysis ecosystem. In 2015 IEEE/ACM 37th IEEE International
Conference on Software Engineering, volume 1, pages 598–608. IEEE, 2015.

[64] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words

with subword units. arXiv preprint arXiv:1508.07909, 2015.

[65] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position repre-

sentations. arXiv preprint arXiv:1803.02155, 2018.

[66] Vighnesh Shiv and Chris Quirk. Novel positional encodings to enable tree-based transformers.

In Advances in Neural Information Processing Systems, pages 12081–12091, 2019.

[67] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. Intellicode compose:
Code generation using transformer. In Proceedings of the 28th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software Engineering,
pages 1433–1443, 2020.

[68] Alexey Svyatkovskoy, Sebastian Lee, Anna Hadjitoﬁ, Maik Riechert, Juliana Franco, and
Miltiadis Allamanis. Fast and memory-efﬁcient neural code completion. arXiv preprint
arXiv:2004.13651, 2020.

[69] Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen, Pierre-Antoine Manzagol,
Charles Sutton, and Edward Aftandilian. Learning to ﬁx build errors with graph2diff neural
In Proceedings of the IEEE/ACM 42nd International Conference on Software
networks.
Engineering Workshops, pages 19–20, 2020.

13

[70] Arie Van Deursen and Tobias Kuipers. Building documentation generators. In Proceedings IEEE
International Conference on Software Maintenance-1999 (ICSM’99).’Software Maintenance for
Business Change’(Cat. No. 99CB36360), pages 40–49. IEEE, 1999.

[71] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998–6008, 2017.

[72] Petar Velickovic, Lars Buesing, Matthew C Overlan, Razvan Pascanu, Oriol Vinyals, and

Charles Blundell. Pointer graph networks. stat, 1050:11, 2020.

[73] Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson.
Rat-sql: Relation-aware schema encoding and linking for text-to-sql parsers. arXiv preprint
arXiv:1911.04942, 2019.

[74] Wenhan Wang, Sijie Shen, Ge Li, and Zhi Jin. Towards full-line code completion with neural

language models. arXiv preprint arXiv:2009.08603, 2020.

[75] Carl Weir. Efﬁciency, robustness and accuracy in picky chart parsing. In ACL, 1992.

[76] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-

ment learning. Machine learning, 8(3-4):229–256, 1992.

[77] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai
Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer
architecture. arXiv preprint arXiv:2002.04745, 2020.

[78] Fabian Yamaguchi. Pattern-based vulnerability discovery. PhD thesis, Niedersächsische

Staats-und Universitätsbibliothek Göttingen, 2015.

[79] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V
Le. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in
neural information processing systems, pages 5753–5763, 2019.

[80] Michihiro Yasunaga and Percy Liang. Graph-based, self-supervised program repair from

diagnostic feedback. arXiv preprint arXiv:2005.10636, 2020.

[81] Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code

generation. arXiv preprint arXiv:1704.01696, 2017.

[82] Pengcheng Yin, Chunting Zhou, Junxian He, and Graham Neubig. Structvae: Tree-structured
latent variable models for semi-supervised semantic parsing. arXiv preprint arXiv:1806.07832,
2018.

[83] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong Liu. A
novel neural source code representation based on abstract syntax tree. In 2019 IEEE/ACM 41st
International Conference on Software Engineering (ICSE), pages 783–794. IEEE, 2019.

[84] Daniel Zügner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan Günnemann.
Language-agnostic representation learning of source code from structure and context.
In
International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=Xh5eMZVONGF.

14

Appendix A Edge Prediction Results for Variable Misuse

(a) Precision

(b) Recall

(c) F score

Figure 6: Results on the validation and test splits for the variable misuse dataset [29].

We include edge prediction results for clean code in the variable misuse task’s dataset. In Figure 6,
we see compared to the results for the code completion task’s dataset, the model performs slightly
worse on various low frequency edge types.

15

CFGNEXTLASTREADLASTWRITECOMPUTEDFROMRETURNSTOFORMALARG NAMEFIELDNEXTSYNTAXLASTLEXICALCALLSEdge type0.00.20.40.60.81.0PrecisionCFGNEXTLASTREADLASTWRITECOMPUTEDFROMRETURNSTOFORMALARG NAMEFIELDNEXTSYNTAXLASTLEXICALCALLSEdge type0.00.20.40.60.81.0RecallCFGNEXTLASTREADLASTWRITECOMPUTEDFROMRETURNSTOFORMALARG NAMEFIELDNEXTSYNTAXLASTLEXICALCALLSEdge type0.00.20.40.60.81.0F scorevaltestAppendix B Additional Results on Fine-tuning the Edge Prediction Model

Appendix B.1 Setup for Fine-tuning the Edge Prediction Model

For simplicity, given a well-trained edge prediction model, we ﬁne-tune by jointly optimizing the
loss with respect to parameters of the edge model and the task model with the same learning rate. We
select the temperature τ based on validation performance, grid-searching over values in the range of
[0.01, 1].

Appendix B.2 Results on the Variable Misuse Task in the Literature

Many papers in the literature report results for the variable misuse task based on clean examples.
While our experiments in Section 4.4 and the following also report standard metrics on this version,
the results here only serve to improve our general understanding of relation model ﬁne-tuning. Our
primary focus is still on work-in-process code that may be broken or incomplete.

We note that well-formed code is in general much easier to model, and it is therefore unsurprising that
much better performance can be attained with additional tricks such as preprocessing code chunks
into graphs after parsing them into ASTs [34] and more structured approaches designed with formal
language theory in mind [34]. One may also expect that ﬁne-tuning a large model pretrained on
well-formed code to work well for tasks that also involve only well-formed code [35].

Lastly, we emphasize that our relation-aware model with ground truth edges results in accuracy
numbers in the same range as those reported by Hellendoorn et al. [29] for clean code (our test
accuracy is 72.7% and 78.5% for sequences with at most 512 tokens, respectively for repair and
localization; their reported results are 73.1% and 76.9% for sequences of fewer than 1000 tokens).

Appendix B.3 Plots Complementing Section 4.4

(a) Repair Accuracy (Small Clean)

(b) Repair Accuracy (Small WIP, k = 5)

Figure 7: Test set repair accuracy for clean and work-in-progress code when edge model is ﬁne-tuned
on small training set.

(a) Localization Accuracy (Small Clean)

(b) Localization Accuracy (Small WIP, k = 5)

Figure 8: Test set localization accuracy for clean and work-in-progress code when edge model is
ﬁne-tuned on small training set.

16

010000200003000040000500006000070000Iterations0.00.10.20.30.40.50.60.70.8Repair Accuracyagnosticaware-random-initaware-trueaware-pretrain-init01000020000300004000050000Iterations0.00.10.20.30.40.50.60.70.8Repair Accuracyagnostic (k=5)aware-random-init (k=5)aware-fixed (k=5)aware-pretrain-init (k=5)020000400006000080000Iterations0.00.10.20.30.40.50.60.70.80.9Localization Accuracyagnosticaware-random-initaware-trueaware-pretrain-init010000200003000040000500006000070000Iterations0.00.10.20.30.40.50.60.70.80.9Localization Accuracyagnostic (k=5)aware-random-init (k=5)aware-fixed (k=5)aware-pretrain-init (k=5)(a) Localization Accuracy (Large Clean)

(b) Localization Accuracy (Large WIP, k = 5)

Figure 9: Test set localization accuracy for clean and work-in-progress code when edge model is
ﬁne-tuned on large training set.

Appendix B.4 Additional Plots for WIP Code with k = 1 and k = 2

(a) Localization Accuracy (Small WIP, k = 1)

(b) Repair Accuracy (Small WIP, k = 1)

(c) Localization Accuracy (Large WIP, k = 1)

(d) Repair Accuracy (Large WIP, k = 1)

Figure 10: Test set localization and repair accuracies for clean and work-in-progress code when edge
model is ﬁne-tuned.

17

020000040000060000080000010000001200000Iterations0.00.10.20.30.40.50.60.70.80.9Localization Accuracyagnosticaware-random-initaware-trueaware-pretrain-init0200000400000600000800000100000012000001400000Iterations0.00.10.20.30.40.50.60.70.80.9Localization Accuracyagnostic (k=5)aware-random-init (k=5)aware-fixed (k=5)aware-pretrain-init (k=5)020000400006000080000100000Iterations0.00.10.20.30.40.50.60.70.80.9Localization Accuracyagnostic (k=1)aware-random-init (k=1)aware-fixed (k=1)aware-pretrain-init (k=1)01000020000300004000050000600007000080000Iterations0.00.10.20.30.40.50.60.70.8Repair Accuracyagnostic (k=1)aware-random-init (k=1)aware-fixed (k=1)aware-pretrain-init (k=1)02000004000006000008000001000000Iterations0.00.10.20.30.40.50.60.70.80.9Localization Accuracyagnostic (k=1)aware-random-init (k=1)aware-fixed (k=1)aware-pretrain-init (k=1)0100000200000300000400000500000600000700000Iterations0.00.10.20.30.40.50.60.70.8Repair Accuracyagnostic (k=1)aware-random-init (k=1)aware-fixed (k=1)aware-pretrain-init (k=1)(a) Localization Accuracy (Small WIP, k = 2)

(b) Repair Accuracy (Small WIP, k = 2)

(c) Localization Accuracy (Large WIP, k = 2)

(d) Repair Accuracy (Large WIP, k = 2)

Figure 11: Test set localization and repair accuracies for clean and work-in-progress code when edge
model is ﬁne-tuned.

18

020000400006000080000Iterations0.00.10.20.30.40.50.60.70.80.9Localization Accuracyagnostic (k=2)aware-random-init (k=2)aware-fixed (k=2)aware-pretrain-init (k=2)01000020000300004000050000600007000080000Iterations0.00.10.20.30.40.50.60.70.8Repair Accuracyagnostic (k=2)aware-random-init (k=2)aware-fixed (k=2)aware-pretrain-init (k=2)0200000400000600000800000Iterations0.00.10.20.30.40.50.60.70.80.9Localization Accuracyagnostic (k=2)aware-random-init (k=2)aware-fixed (k=2)aware-pretrain-init (k=2)0100000200000300000400000500000600000700000Iterations0.00.10.20.30.40.50.60.70.8Repair Accuracyagnostic (k=2)aware-random-init (k=2)aware-fixed (k=2)aware-pretrain-init (k=2)Appendix C Which Edges are Most Important for Code Completion?

This section is devoted to studying which edge types are most helpful for the relation-aware model to
achieve improved performance compared to its relation-agnostic counterpart on the code completion
task. Meanwhile, we also uncover a reason behind the mediocre performance of relation-aware
models when the fed edges are ﬁxed and come from a learned edge prediction model.

We split all edge types described in Section 3.1 into those that are easy to predict (5 types with highest
validation F score) and those that are hard to predict (5 remaining types) based on the predictions
of an edge prediction model trained as according to Section 4.1. Then, we train relation-aware
models using only ground truth edges from the former or latter category on the small training split.
Figure 12 demonstrates that the downstream performance of the relation-aware model using only
the easy-to-predict edges is nearly the same as the relation-agnostic model, and that using only
hard-to-predict edges is nearly the same as the relation-aware model trained with all edge types.

This experiment conﬁrms the intuition that downstream accuracy is non-uniformly affected by
different edge types and explains the poor performance of using our ﬁxed edge prediction model.

Lastly, we comment that the edge types most difﬁcult to predict are CFGNext, ReturnsTo, For-
malArgName, Field, and Calls. These edges are derived from the associated control ﬂow and data
ﬂow graphs of a piece of code.

(a) Cross entropy

(b) Top-1 accuracy

Figure 12: Results on next token prediction for models trained on the small training split. “Aware
(only easy)” is nearly the same as “Agnostic”, and “Aware (only hard)” is nearly the same as “Aware
(all)”. Results based on early stopping with a patience of 10k iterations on the validation split.

19

0500010000150002000025000Iterations3×1004×1005×1006×100Cross EntropyAgnosticAware (all)Aware (only easy)Aware (only hard)Aware (fixed)Aware (fine-tuned)020000400006000080000Iterations0.5500.5750.6000.6250.6500.6750.7000.7250.750Top-1 AccuracyAgnosticAware (all)Aware (only easy)Aware (only hard)Aware (fixed)Aware (fine-tuned)Appendix D Subword-level Results for Code Completion

Model

Perplexity (↓)

Top-1 accuracy in % (↑)

Small

Large

Small

Large

Transformer (stride=100) 6.157 ± 0.155
2.822 ± 0.019
Transformer (stride=5) 6.128 ± 0.155 2.806 ± 0.019

66.933 ± 0.522
66.973 ± 0.529

78.641 ± 0.135
78.741 ± 0.137

Agnostic
Aware-ﬁxed
Aware-tuned

Aware-true

80.008 ± 0.068
7.297 ± 0.061
79.922 ± 0.027
7.217 ± 0.016
6.380 ± 0.072 2.513 ± 0.009 69.740 ± 0.099 81.035 ± 0.072

68.642 ± 0.226
69.053 ± 0.097

2.654 ± 0.010
2.663 ± 0.006

6.690 ± 0.083

2.535 ± 0.002

70.226 ± 0.155

81.223 ± 0.012

Table 3: Results for next subword prediction. Small and Large respectively refer to when the small
and large training split is used. Numbers in bold are the best results in each column. We exclude the
Aware-true setting in our comparisons, since this setting cannot be executed in practice given any
unseen code preﬁx.

20

