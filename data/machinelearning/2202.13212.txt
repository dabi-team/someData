2
2
0
2

r
p
A
7
1

]

G
L
.
s
c
[

2
v
2
1
2
3
1
.
2
0
2
2
:
v
i
X
r
a

Faster One-Sample Stochastic Conditional Gradient Method for
Composite Convex Minimization

Gideon Dresdner
ETH Zürich, Switzerland

Maria-Luiza Vladarean
EPFL, Switzerland

Gunnar Rätsch
ETH Zürich, Switzerland

Francesco Locatello
Amazon Web Services

Volkan Cevher
EPFL, Switzerland

Alp Yurtsever
Umeå University, Sweden

Abstract

We propose a stochastic conditional gradient
method (CGM) for minimizing convex ﬁnite-
sum objectives formed as a sum of smooth and
non-smooth terms. Existing CGM variants for
this template either suﬀer from slow conver-
gence rates, or require carefully increasing the
batch size over the course of the algorithm’s
execution, which leads to computing full gra-
dients.
In contrast, the proposed method,
equipped with a stochastic average gradient
(SAG) estimator, requires only one sample per
iteration. Nevertheless, it guarantees fast con-
vergence rates on par with more sophisticated
variance reduction techniques. In applications
we put special emphasis on problems with a
large number of separable constraints. Such
problems are prevalent among semideﬁnite
programming (SDP) formulations arising in
machine learning and theoretical computer
science. We provide numerical experiments
on matrix completion, unsupervised cluster-
ing, and sparsest-cut SDPs.

continuous with constant Lf ), A is an m × d matrix,
and g : Rm → R ∪ {+∞} is convex but possibly non-
smooth. The function g(Aw) can capture constraints
of the form Aw = b (or Aw ∈ K, for closed, convex
sets K ⊆ Rm) via indicator functions δ{b} (resp., δK
which takes 0 for all points in K and +∞ everywhere
else). Throughout, we assume that g is either Lipschitz
continuous or an indicator function.

We study conditional gradient methods (CGM, also
known as the Frank-Wolfe Algorithm) tailored for Prob-
lem (1). For computational eﬃciency, we suppose linear
minimization over W is easy. We separately focus on
two speciﬁc settings of g:

(S1) g admits an eﬃcient prox-operator,
(S2) g is a ﬁnite-sum of the form g (cid:44) 1
m

i w),
where each gi : R → R ∪ {+∞} is convex and aT
i
is the i-th row of A. This separable ﬁnite-sum
structure allows us to tackle g stochastically and
therefore more eﬃciently when m is large.

i=1 gi(aT

(cid:80)m

Our problem template covers a variety of applications
in machine learning, statistics and signal processing,
including the ﬁnite-sum formulations that arise in M-
estimation and empirical risk minimization problems.

1

INTRODUCTION

Application Focus: Strongly Constrained SDPs

Consider the following composite ﬁnite-sum template:

(cid:40)
F (w) (cid:44) 1
n

min
w∈W

n
(cid:88)

i=1

(cid:41)

fi(xT

i w) + g(Aw)

.

(1)

W ⊂ Rd is a compact and convex set, each fi : R → R
is convex and Lf -smooth (i.e., its derivative is Lipschitz

Proceedings of the 25th International Conference on Artiﬁ-
cial Intelligence and Statistics (AISTATS) 2022, Valencia,
Spain. PMLR: Volume 151. Copyright 2022 by the au-
thor(s).

A particular example of our model problem is the stan-
dard semideﬁnite programming (SDP) template:

min
W ∈Sd×d
+

(cid:104)X, W (cid:105)

subj. to (cid:104)Ai, W (cid:105) (cid:47) bi,

i = 1, . . . , m,

(2)

+

where Sd×d
denotes the set of symmetric positive
semideﬁnite matrices, X ∈ Sd×d is the symmetric cost
matrix, (Ai, bi) ∈ Sd×d ×R characterize the constraints,
and ‘(cid:47)’ represents either equality ‘=’ or inequality ‘≤’
operations.

 
 
 
 
 
 
Faster One-Sample SCGM for Composite Convex Minimization

SDPs are ubiquitous in theoretical computer science.
Examples include relaxations of combinatorial optimiza-
tion problems such as maximum cut (Goemans and
Williamson, 1995), quadratic assignment (Zhao et al.,
1998), and sparsest cut (Arora et al., 2009). SDPs are
also found in machine learning problems such as matrix
completion (Alfakih et al., 1999), unsupervised cluster-
ing (Kulis et al., 2007), certifying robustness of neural
networks (Raghunathan et al., 2018) and estimating
their Lipschitz constants (Latorre et al., 2020).

The remarkable ﬂexibility of SDPs comes at the cost of
severe computational challenges. The cone constraint
itself poses a major challenge for a majority of the ﬁrst-
order methods because projection onto positive semidef-
inite cone requires expensive eigen-decompositions.
CGM is popular in this setting (see Hazan (2008);
Jaggi and Sulovsk`y (2010); Garber (2016); Yurtsever
et al. (2018)) since it avoids projection by leveraging
the so-called linear minimization oracle (lmo) which
computes only the top eigenvectors rather than the
full spectrum. Additionally, CGM is also used to re-
duce storage cost (Yurtsever et al., 2015; Freund et al.,
2017; Yurtsever et al., 2021), which is often a critical
bottleneck for solving SDPs in large scale.

However, scalable approaches to solving SDPs with a
large number of constraints, which we term as strongly-
constrained SDPs, remain largely unexplored. This
gap can be bridged by developing CGM variants which
handle linear constraints in a randomized fashion.

Contributions. We propose a new CGM variant for
convex ﬁnite-sum problems. The proposed method
extends the recent work on stochastic Frank-Wolfe
(Négiar et al., 2020) to the composite template in Prob-
lem (1). In particular:

(cid:46) In (S1), our algorithm ﬁnds an ε-suboptimal solu-
tion after O(ε−2) iterations (see Optimality Con-
ditions, Sec. 3 for the deﬁnition of ε-suboptimal).

(cid:46) In (S2), our algorithm ﬁnds an ε-suboptimal solu-
tion after O(ε−2) iterations, matching the iteration
complexity in Vladarean et al. (2020). However,
we achieve this rate without using an increasing
batch-size strategy. Thus, our algorithm enjoys
a total cost of O(ε−2d) which is independent of
m. In contrast, the cost in Vladarean et al. (2020)
is O(ε−2dm).

Finally, we present numerical experiments on matrix
completion, k-means clustering, and sparsest cut prob-
lems. In these experiments, the proposed algorithm
performs on par with the state-of-the-art variance re-
duced CGM variants. Importantly, however, our al-
gorithm does not require computing full gradients or
increasing the batch size.

2 RELATED WORK

CGM for Smooth Objectives. CGM is intro-
duced by Frank and Wolfe (1956) for minimizing a
convex quadratic function over a polytope. Later, the
analysis is extended to general convex smooth func-
tions and arbitrary convex and compact sets by Levitin
and Polyak (1966). Clarke (1990) and Hazan (2008)
propose CGM as an eﬀective method to tackle simplex
and spectrahedron constraints respectively. We refer
to Jaggi (2013) for an excellent survey on the eﬃciency
of CGM for machine learning applications.

The last decade has witnessed a surge of interest in
the CGM framework for machine learning applications
which has prompted researchers to study stochastic
extensions of CGM. Unlike gradient descent, CGM
does not immediately work when the gradient in the
algorithm is replaced with an unbiased stochastic gradi-
ent estimator with bounded variance. To address this
problem, several stochastic CGM variants have been
proposed by combining CGM with existing variance re-
duction techniques (Reddi et al., 2016; Hazan and Luo,
2017; Mokhtari et al., 2018; Yurtsever et al., 2019b;
Shen et al., 2019; Zhang et al., 2020) and more recently
in Négiar et al. (2020).

In general, the convergence rate of an algorithm is de-
termined by the stochastic gradient estimator. Hazan
and Luo (2017) develop an estimator with small vari-
ance, resulting in a fast O(ε−3/2) iteration complexity
but at the cost of exponentially increasing batch sizes.
Mokhtari et al. (2018) and Zhang et al. (2020) main-
tain a constant batch size but have slower convergence
rates of O((cid:15)−3) and O((cid:15)−2), respectively. We refer to
Yurtsever et al. (2019b) for a detailed comparison of
the existing stochastic CGM variants.

Our work draws from (Négiar et al., 2020) where the
authors propose a stochastic CGM with an iteration
complexity of O(ε−1) which is on par with deterministic
CGM. This is achieved by assuming a separable ﬁnite-
sum model and using the Stochastic Average Gradient
(SAG) estimation technique (Schmidt et al., 2017).

CGM for Composite Objectives. CGM is not
directly applicable to problems with a non-smooth ob-
jective (see Section 2 in (Nesterov, 2018) for a counter-
example). Lan (2013) tackle this problem in the case of
Lipschitz continuous non-smooth functions by combin-
ing CGM with Nesterov smoothing (Nesterov, 2005).
Yurtsever et al. (2018) further extend it for indicator
functions through a quadratic penalty technique, which
they call Homotopy CGM.

Locatello et al. (2019) extend Homotopy CGM to
stochastic objectives but only for the case in which

Dresdner, Vladarean, Rätsch, Locatello, Cevher, Yurtsever

Algorithm

Reference

HCGM, CGAL Yurtsever et al. (2018; 2019a)

Iteration Complexity
O(ε−2)

Total Cost
O(ε−2d max{n, m})

SHCGM

Locatello et al. (2019)

MOST-FW Akhtar and Rajawat (2021)

H-SAG-CGM v1

This Paper

H-1SFW
H-SPIDER-FW
MOST-FW+
H-SAG-CGM v2

Vladarean et al. (2020)
Vladarean et al. (2020)
Akhtar and Rajawat (2021)
This Paper

O(ε−3)
O(ε−2)
O(ε−2)

O(ε−6)
O(ε−2)
O(ε−4)
O(ε−2)

O(ε−3d m)
O(ε−2d m)
O(ε−2d m)

O(ε−6d)
O(ε−2d m)
O(ε−4d)
O(ε−2d)

Fixed Batch Size

N/A

N/A
N/A
N/A

(cid:88)
(cid:55)
(cid:88)
(cid:88)

Table 1: This table presents asymptotic costs of ﬁnding an ε-suboptimal solution to a given problem, i.e., we
treat problem parameters d, n and m as constants and characterize the behavior as ε → 0. O notation hides the
parameters Lf , (cid:107)A(cid:107), DW , and the absolute constants. We tailor the cost of existing methods for Problem (1),
their cost for other problems can be diﬀerent. The last column indicates whether the algorithm has increasing or
ﬁxed batch size.

the non-smooth part g is deterministic. More recently,
Vladarean et al. (2020) proposed new variants that can
handle stochastic constraints. They provide algorithms
for an arbitrary number of constraints under minimal
assumptions. However, for the common practical set-
ting of a ﬁnite number of constraints, their algorithm
requires full passes over the constraints.

This paper works in the same vein by proposing a
randomized algorithm for the ﬁnite-sum template in
Problem (1). Our algorithm for deterministic g in (S1)
outperforms the method of Locatello et al. (2019) both
in theory and in practice. Our algorithm for separable
g in (S2) performs on par with the methods described
in (Vladarean et al., 2020). However, in contrast to
the previous work, it maintains a constant batch size.

After submitting this paper, we became aware of the
recent work of Akhtar and Rajawat (2021). They study
a similar problem and also propose an algorithm with
two variants to address the cases of deterministic and
stochastic g. In the case of deterministic g, the cost of
their algorithm is O(ε−2dm); the same as our method.
However, in the case of stochastic g, their method’s
cost is O(ε−4d). In contrast, our algorithm achieves
O(ε−2d) by taking advantage of the separable ﬁnite-
sum structure.

Proximal Methods. A growing body of work aims to
address strongly constrained problems through proxi-
mal methods in various settings (Patrascu and Necoara,
2017; Fercoq et al., 2019; Mishchenko and Richtárik,
2019; Xu, 2020). These algorithms process a random
subset of constraints at each iteration and converge to
a feasible point asymptotically, similar to (Vladarean
et al., 2020) and the algorithm that we propose in
this paper. However, when applied to SDPs, proximal
methods require a costly eigenvalue decomposition at
each iteration. Hence, these methods are not practical
for solving SDPs in large scale.

Primal vs. Dual Problem. When there are many
constraints, solving the dual problem can be more
plausible from a computational perspective. However,
converting a dual solution to a primal solution is a
non-trivial problem itself, especially in large-scale set-
ting where we are restricted from using projection or
proximal operators. Moreover, since our problem is
stochastic, we can expect ﬁnding only a rough estimate
of the dual solution. In this work, we assume that we
are interested in the primal variable and that it is large.
To this end, we focus on solving the primal formulation.

3 PRELIMINARIES

Notation. The operator norm of a matrix A is writ-
ten (cid:107)A(cid:107) and the Euclidean inner-product is denoted
(cid:104)·, ·(cid:105). We deﬁne the diameter of W as

DW = max
x,y∈W

(cid:107)x − y(cid:107)2

(3)

and the (cid:96)1 and (cid:96)∞ diameters with respect to the column
space of a matrix M as

D1(M ) (cid:44) max
u,v∈W
D∞(M ) (cid:44) max
u,v∈W

(cid:107)M (u − v)(cid:107)1

(cid:107)M (u − v)(cid:107)∞.

(4)

(5)

The linear minimization oracle of set W is given by

lmoW (v) (cid:44) arg min

(cid:104)u, v(cid:105).

u∈W

(6)

The proximal operator of g : Rm → R ∪ {+∞} is

proxg(z) (cid:44) arg min
y∈Rm

g(y) +

1
2

(cid:107)y − z(cid:107)2
2.

(7)

When g is the indicator function of a convex set K, its
proximal operator is equal to the Euclidean projection,
proxδK(z) = projK(z).

Faster One-Sample SCGM for Composite Convex Minimization

Assumption. When g is an indicator function we
assume that strong duality holds. Slater’s condition is
a well-known suﬃcient condition for strong duality.

Optimality Conditions. We denote a solution of
Problem (1) by w(cid:63):

F (cid:63) (cid:44) F (w(cid:63)) ≤ F (w),

∀w ∈ W.

(8)

If g is continuous valued on W, we say that wk ∈ W is
an ε-suboptimal solution when it satisﬁes

EF (wk) − F (cid:63) ≤ ε.

(9)

If g = δK is an indicator function, the F (wk) − F (cid:63) can
be +∞ even when wk is arbitrarily close to a solution.
To this end, we relax the deﬁnition of an ε-suboptimal
solution in this case and say that wk ∈ W is an ε-
suboptimal solution of Problem (1) if it satisﬁes

|Ef (wk) − F (cid:63)| ≤ ε and E[dist(Awk; K)] ≤ ε. (10)

Our algorithm guarantees at every iteration that wk is
in W and asymptotically that Awk ∈ K.

3.1 Smoothing

Building on the existing Homotopy CGM framework
(Yurtsever et al., 2018; Locatello et al., 2019; Vladarean
et al., 2020), we use the smoothing technique of Nes-
terov (2005) and its extension to indicator functions
as studied in Tran-Dinh et al. (2018). Speciﬁcally,
given a convex (possibly non-smooth) function g, its
approximation is deﬁned as

gβ(z) (cid:44) sup
y

(cid:104)y, z(cid:105) − g∗(y) −

β
2

(cid:107)y(cid:107)2,

(11)

where g∗(y) (cid:44) supx(cid:104)x, y(cid:105)−g(x) is the Fenchel conjugate
of g. Importantly, gβ is 1
β -smooth (Nesterov, 2005).
When g = δK for some closed and convex set K, its
approximation becomes gβ(z) = 1
If g
allows for an eﬃcient prox operator, we can compute
the gradient of gβ as

2β dist(z, K)2.

∇gβ(Aw) = β−1 (cid:0)Aw − proxβg(Aw)(cid:1) .

(12)

4 ALGORITHM & CONVERGENCE

4.1 Stochastic Homotopy-Based CGM for

Separable Problems

First, we transform the objective in Problem (1) using
the smoothing technique summarized in Section 3.1 to
obtain the following smooth surrogate objective:

Algorithm 1 H-SAG-CGM
1: Input: β0 > 0, w0 ∈ W, α0 ∈ Rn, γ0 ∈ Rm,

4:
5:

6:

7:
8:

0 ∈ Rd, vg
vf
0 ∈ Rd
2: for k = 1, 2, . . . do
3:

ηk = 2
k+1
βk = β0/
Sample j ∼ Uniform[1, 2, . . . , n]

k + 1

√

αk,i =

(cid:40) 1

j wk)

n f (cid:48)
j(xT
αk−1,i

i = j
i (cid:54)= j

vf
k = vf
k−1 + (αk,j − αk−1,j)xi
vg
k ← use Variant 1 or Variant 2
vk = vf
k + vg
sk = lmoW (vk)
wk+1 = wk + ηk(sk − wk)

k

9:
10:
11:
12: end for

Variant 1 Non-separable Constraints
1: return 1
βk

AT (Awk − proxβkg(Awk))

Variant 2 Randomized Constraints
1: Sample l ∼ Uniform[1, 2, . . . , m]
βk,l(aT

l wk)

(cid:40) 1

m g(cid:48)
γk−1,q
k−1 + (γk,l − γk−1,l)al

q = l
q (cid:54)= l

3: return vg

2: γk,q =

In particular, if we consider (S2) in which g is separable,
then the smooth approximation gβ is also separable:

gβ(Aw) =

1
m

m
(cid:88)

j=1

gβ,j(aT

j w).

(14)

This will allow for a fully randomized algorithm (H-
SAG-CGM/v2) which can tackle strongly constrained
SDPs with a non-increasing batch size.

The fundamental mechanism of homotopy CGM is
to enforce a theoretically-determined schedule for βk
such that Fβk → F asymptotically. Broadly speaking,
stochastic homotopy CGMs perform these three steps
at each iteration:

(1) Compute a gradient estimator vk of the smooth
surrogate function Fβk (lines 5-8 of Alg. 1, imple-
mented in Variant 1 and Variant 2).

(2) Perform a conditional gradient update by solving
lmoW (vk) (Alg. 1, line 10) and moving the current
estimate towards this solution (Alg. 1, line 11).

(3) Decrease βk to enforce feasibility (line 4) and go

to Step (1).

Fβ(w) (cid:44) 1
n

n
(cid:88)

i=1

fi(xT

i w) + gβ(Aw).

(13)

The main contribution of our algorithm is Step (1)
where we use a SAG estimator for f and either the full
gradient of gβk (Variant 1) or another SAG estimator

Dresdner, Vladarean, Rätsch, Locatello, Cevher, Yurtsever

for gβk (Variant 2). This key innovation over previous
work in Vladarean et al. (2020) yields comparable,
state-of-the-art complexity bounds without requiring
full passes over the set of constraints. Then, Step (2)
comes from the classical CGM and Step (3) is the
homotopy smoothing step from Yurtsever et al. (2018).

In the following section, we give an overview of the
theoretical analysis.

Recently, Négiar et al. (2020) showed that optimal
convergence guarantees can be obtained for separable
objectives by considering a SAG-like gradient estimator
(Schmidt et al., 2017). By combining this idea with
the homotopy framework, we are able to provide an
improved randomized algorithm (in two variants) for
composite objectives. We now proceed by deﬁning the
SAG estimators and presenting their useful properties.

4.2 Analysis of Stochastic homotopy CGMs

The analysis is composed of two main parts. First, we
establish the convergence rate for the smoothed-gap

Sβk (wk+1) (cid:44) E[Fβk (wk+1) − F (cid:63)].

(15)

Then, in the second part that we present in Section 4.4,
we translate convergence of the smoothed-gap Sβk into
guarantees for the original problem based on the tech-
niques described in (Tran-Dinh et al., 2018).

For the ﬁrst part, we rely on a recursive inequality in-
volving Sβk (wk+1) which appears with slight variations
in (Locatello et al., 2019; Vladarean et al., 2020). A
generic version of this lemma is presented below.

Lemma 4.1. For both variants of H-SAG-CGM, and
for all k ≥ 1 it holds that

Sβk (wk+1) ≤ (1 − ηk)Sβk−1 (wk)

+ ηkDW E(cid:107)∇Fβk (wk) − vk(cid:107)

+

kD2
η2
W LFβk
2

,

where LFβk
represents the smoothness constant of the
surrogate objective Fβk . If we consider the setting (S1)
n +
and Variant 1 of the algorithm, then LFβk
(cid:107)A(cid:107)
. Otherwise, if g is separable as in (S2) and we use
βk
Variant 2, then LFβk

n + (cid:107)A(cid:107)
βkm .

= (cid:107)X(cid:107)Lf

= (cid:107)X(cid:107)Lf

See Appendix B for the proof.

Discussion. Lemma 4.1 shows how the convergence
rate depends on the variance of the stochastic gradient
estimator and the design parameters ηk and βk. Since
we can choose ηk and βk to get the best possible rates
in the analysis, this leaves the variance of the stochastic
gradient estimator as the decisive term. By combining
this lemma with two gradient estimators, corresponding
to Variants 1 and 2 of Algorithm 1, we get convergence
rates on Sβk which we present in Theorem 4.1.

Existing stochastic homotopy CGMs (Locatello et al.,
2019; Vladarean et al., 2020) rely on variance-reduced
gradient estimators devised to handle arbitrary stochas-
tic objectives, thus failing to exploit the separable ﬁnite-
sum structure often encountered in practice.

4.3 Stochastic Average Gradient (SAG)

Error Bounds

The following two SAG estimators approximate the
two parts of the gradient of Fβ.

At each iteration of Algorithm 1, the j-th coordinate
of the gradient of f is updated using a SAG estima-
tor (lines 5-7):

αk,i =

(cid:40) 1

i wk)

n f (cid:48)(xT
αk−1,i

i = j,
i (cid:54)= j,

(16)

In particular, if we consider setting (S2) with a separa-
ble g, then we can use Variant 2 of the algorithm which
employs another SAG estimator and updates the l-th
coordinate of the gradient of gβk :

γk,q =

(cid:40) 1

m g(cid:48)
γk−1,q

βk,l(aT

l wk)

q = l,
q (cid:54)= l.

(17)

Otherwise, in setting (S1) with a non-separable non-
smooth g, we use full gradients of gβk as in Variant 1.

In summary, Variant 1 assumes stochastic ∇f ap-
proximated by αk and a non-separable g whose gra-
dient is fully computed. Thus, the stochastic gradi-
ent is a sum of a stochastic and deterministic terms:
vk = X T αk + AT ∇gβk (Awk).

On the other hand, Variant 2 assumes that g separable
in addition to f , hence ∇gβk can be approximated by γk.
Thus, the overall gradient term vk in this case is the sum
of two stochastic terms given by vk = X T αk + AT γk.

We now present two lemmas characterizing the errors
of αk and γk in (cid:96)1-norm.
Lemma 4.2. [Lemma 3 in (Négiar et al., 2020)]
Consider H-SAG-CGM, with the SAG estimator αk
deﬁned in (16). Then, for all k ≥ 2,

E [(cid:107)∇f (Xwk) − αk(cid:107)1] ≤ (cid:0)1 − 1

n

(cid:1)k

+ C1

(cid:0)1 − 1

(cid:107)∇f (Xw0) − α0(cid:107)1
C2
k

log k +

(cid:1)k/2

n

,

where C1 = 2n−1Lf D1(X), C2 = 4n−1(n−1)Lf D1(X)
and the expectation is taken over all previous steps in
the algorithm.

Faster One-Sample SCGM for Composite Convex Minimization

Lemma 4.3. Consider Variant 2 of H-SAG-CGM with
the SAG estimators deﬁned in (16) and (17). Then,
for all k ≥ 2,

E[(cid:107)∇gβk (Awk) − γk(cid:107)1]
≤ (cid:0)1 − 1

(cid:1)k

m

(cid:107)∇gβ0(Aw0) − γ0(cid:107)1 +

C
√
k

where C = 10β−1
over all previous steps of the algorithm.

0 D1(A) and the expectation is taken

We refer to (Négiar et al., 2020) for the proof of
Lemma 4.2. We present the proof of Lemma 4.3 in Ap-
pendix D under the assumption that g is an indicator
function or a Lipschitz continuous function.

Discussion. Lemma 4.2 shows that the SAG-like
estimator provides an error bound in (cid:96)1-norm that
decays as O(1/k) in expectation. This decay does
not carry over to the separable case in Variant 2, as
demonstrated by Lemma 4.3, due to the 1
-factor
βk
associated with the smoothed approximation gβk .

4.4 Convergence Rates

Combining Lemmas 4.2 and 4.3 with Lemma 4.1 gives
the convergence rates for the two variants of H-SAG-
CGM which we now present.

Theorem 4.1. The sequence generated by H-SAG-
CGM (Algorithm 1) satisﬁes, for all k ≥ 2,

Sβk (wk+1) ≤

C1√
k

+

C2
k

+

C3
k2 .

The constants are deﬁned for H-SAG-CGM/v1 as fol-
lows:

W (cid:107)A(cid:107)β−1

(cid:46) C1 = 2D2
(cid:46) C2 = 8Lf D1(X)D∞(X) + 2n−1Lf (cid:107)X(cid:107)D2
W
(cid:46) C3

2n2D∞(X)(cid:0)(cid:107)∇f (Xw1) − α0(cid:107)1 +

0

=
32Lf D1(X)(cid:1)

and for Variant 2 as follows:
0 (2D2

(cid:46) C1 = β−1
(cid:46) C2 = 8Lf D1(X)D∞(X) + 2n−1Lf (cid:107)X(cid:107)D2
W
(cid:46) C3

2n2D∞(X)(cid:0)(cid:107)∇f (Xw1) − α0(cid:107)1 +

W (cid:107)A(cid:107) + 10D1(A)).

32Lf D1(X)(cid:1) + 2m2D∞(A)(cid:107)∇gβ0(Aw1) − γ0(cid:107)1

=

Using the techniques described in (Tran-Dinh et al.,
2018), we translate this bound to convergence guaran-
tees on the original problem in the following corollaries.
Corollary 4.1. Suppose g : Rm → R is Lg-Lipschitz
continuous. Then, the estimates generated by H-SAG-
CGM (Algorithm 1) satisfy

E[F (wk+1) − F (cid:63)] ≤

C1√
k

+

C2
k

+

C3
k2 +

β0L2
g
√
k
2

where the constants C1, C2 and C2 are deﬁned in The-
orem 4.1.

Corollary 4.2. Suppose g is the indicator function of a
closed and convex set K. Then, for H-SAG-CGM (Algo-
rithm 1), we have a lower bound on the suboptimality as
E [f (Xwk+1) − f (Xw∗)] ≥ −(cid:107)y∗(cid:107)E [dist(Awk+1, K)]
and the following upper bounds on the suboptimality
and feasibility:

E [f (Xwk+1) − f (Xw∗)] ≤

E [dist(Awk+1, K)] ≤

C1 + β0√
k
√

C4√
k

+

+

C2
k
2C2
k3/4

+

C3
k2 , and
+
√
2C3
k5/4

where the constants C1, C2 and C3 are deﬁned in The-
orem 4.1 and C4 = ( 3β0(cid:107)y∗(cid:107)

2C1).

√

+

2

Discussion. Even in the deterministic setting stud-
ied in (Yurtsever et al., 2018), the convergence rates
of Homotopy CGM is bounded below by Ω(1/
k), as
demonstrated theoretically in (Lan, 2013) and practi-
cally in (Kerdreux et al., 2021). Corollaries 4.1 and 4.2
show that both variants of our algorithm achieves this
lower bound.

√

H-SAG-CGM/v1 provides an order of magnitude im-
provement (from O(ε−3) to O(ε−2)) over the previous
state-of-the-art in deterministic constraints, Locatello
et al. (2019).

While H-SAG-CGM/v2 and H-SPIDER-FW Vladarean
et al. (2020) enjoy a similar overall rate, the latter
requires an exponentially increasing batch size. Com-
bined with occasional full passes, this quickly becomes
impractical for strongly constrained problems. As an
alternative, Vladarean et al. (2020) propose H-1SFW
which does use a ﬁxed batch size but at the cost of
an impractical O(ε−6) rate. In stark contrast, our al-
gorithm enjoys the optimal rate without resorting to
increasing the batch size.

5 NUMERICAL EXPERIMENTS

This section demonstrates the empirical performance
of the proposed method across a number of diﬀer-
ent problems: matrix completion, k-means clustering
and uniform sparsest cut. We performed these experi-
ments in MATLAB R2019b and the codes are pub-
licly available at https://github.com/ratschlab/
faster-hcgm-composite.

Baselines. We compare the proposed method against
the following methods:
(cid:46) SHCGM (Locatello et al., 2019)
(cid:46) H-SPIDER-FW (Vladarean et al., 2020)
(cid:46) H-1SFW (Vladarean et al., 2020)

Dresdner, Vladarean, Rätsch, Locatello, Cevher, Yurtsever

Figure 1: Empirical comparison of H-SAG-CGM/v1 with SHCGM on matrix completion with inequality con-
straints (18) with the MovieLens-100k dataset.

Figure 2: Empirical comparison of H-SAG-CGM/v1 with SHCGM on matrix completion with (cid:96)1-regularization (19)
with the MovieLens-100k dataset.

Note that SHCGM only works in the case of determin-
istic g and is hence a natural baseline comparison for
H-SAG-CGM/v1. H-SPIDER-FW can handle stochas-
tic g so it is used to compare to H-SAG-CBM/v2 but
importantly in this case, H-SPIDER-FW requires an
increasing batch size.

Challenges. The parameter β determines a trade-oﬀ
between convergence in the objective residual and the
infeasibility error. However, since we do not know the
optimal value a priori, β0 is not always easy to interpret
given a particular task. This leaves practitioners to
develop the intuition on how to tune this parameter.
This challenge is not unique to H-SAG-CGM but is
shared among homotopy CGM approaches (Yurtsever
et al., 2018; Locatello et al., 2019; Vladarean et al.,
2020). Automating β0-tuning is an important direction
for future research.

5.1 Matrix Completion

We consider two diﬀerent formulations of the matrix
completion problem. First, we focus on matrix com-
pletion with hard inequality constraints studied in Lo-

catello et al. (2019):
(cid:88)

min
(cid:107)w(cid:107)(cid:63)≤ζ

(i,j)∈Ω

(wij − Xij)2 subject to 1 ≤ w ≤ 5 (18)

where Ω is the observed entries of the input data X,
and (cid:107)X(cid:107)(cid:63) denotes the nuclear norm. The inequality
constraints 1 ≤ w ≤ 5 are hard thresholds which specify
that all the entries of w must lie between 1 and 5.
For X, we used the Movielens-100k dataset∗ con-
taining approximately 100,000 integer valued movie
ratings between 1 and 5, assigned by 1682 users to 943
movies. We used the ub.train and ub.test partitions
provided with the original data for the train/test split.

This numerical setup was studied also in Locatello
et al. (2019). We used the parameter setting that
they reported without any further tuning. We set
ζ = 7e3 for the nuclear norm bound, β0 = 10 for the
initial smoothing parameter, and we compute gradient
estimators with 1000 iid samples at each iteration.

Figure 1 compares the performance of H-SAG-CGM/v1
against SHCGM (Locatello et al., 2019) in terms of

∗F.M. Harper,

J.A. Konstan. — Available at

https://grouplens.org/datasets/movielens

Faster One-Sample SCGM for Composite Convex Minimization

Figure 3: Comparing H-SAG-CGM/v2 to state-of-the-art baselines on two distinct SDP-relaxation tasks,
k-means (20) and sparsest cut (22). The x-scale is in terms of the constraint epochs. One constraint epoch
corresponds to a full pass over all the constraints. Note that for the k-means clustering experiment, we deliberately
restricted H-SPIDER-FW to not perform full passes over all of the constraints resulting in noticeable degradation
in performance.

train and test root mean squared error (RMSE) and
infeasibility error. The comparison is based on the iter-
ation counter, which is an arguably fair representation
of the time cost of the algorithms since both methods
use the same number of samples per iteration.

Next, we test our algorithm for a setting in which g
is Lipschitz continuous by performing experiments on
matrix completion with (cid:96)1-regularization:

min
(cid:107)w(cid:107)(cid:63)≤ζ

(cid:88)

(i,j)∈Ω

(wij − Xij)2 + λ(cid:107)w(cid:107)1.

(19)

We use the same dataset and parameter settings
as in (18) with the regularization parameter set to
λ = 0.1. Figure 2 presents the train and test RMSE
obtained in this experiment. Note that the estimates
remain feasible in this experiment since g is not an
indicator function.

5.2 k-Means Clustering

In this experiment, we test H-SAG-CGM/v2. The
goal in k-means is to assign n data points to k clus-
ters. We consider the following SDP relaxation of this

problem (Peng and Wei, 2007):

min
w∈X

(cid:104)w, C(cid:105)

subject to w(cid:126)1 = (cid:126)1, and w ≥ 0

(20)

+

| Tr(w) ≤ 1

n }, (cid:126)1 = [1, 1, . . . , 1] ∈
where X = {w ∈ Sn×n
Rn, and w ≥ 0 denotes entry-wise non-negativity. The
problem is strongly constrained with a total of n2 + n
constraints — n equality and n2 inequality constraints.

This problem is also studied in the related works on
homotopy CGM in (Yurtsever et al., 2018; Locatello
et al., 2019; Vladarean et al., 2020). We use the same
test setup: For the input data C, we use mnist dataset†
with the preprocessing considered in Mixon et al. (2016).
We set β0 = 7.

We compare the methods based on the number of
epochs (an epoch corresponds to a full pass over the
constraints) since diﬀerent methods use diﬀerent batch
sizes in this experiment. The ﬁrst two plots in Fig-
ure 3 present the outcomes of this experiment. We
measure the objective residual as relative suboptimal-
ity |f (wk) − f (cid:63)|/|f (cid:63)| and the infeasibility error as the
Euclidean distance to the feasible set dist(Awk, K).

†http://yann.lecun.com/exdb/mnist/

Dresdner, Vladarean, Rätsch, Locatello, Cevher, Yurtsever

5.3 Uniform Sparsest Cut

6 CONCLUSION

In this experiment, we test H-SAG-CGM/v2 on the
uniform sparsest cut SDP. This problem is particularly
interesting because of the O(n3) number of constraints.

Let G = (V, E) be a graph with n nodes |V | = n and a
set of edges E. The goal in uniform sparsest cut is to
split vertices into two partitions (S, ¯S) that minimize

|E(S, ¯S)|
|S|| ¯S|

(21)

where E(S, ¯S) ⊆ E is the set of edges between the
nodes in S and ¯S.

This canonical problem has applications across many
ﬁelds including VLSI circuit layout design, the topo-
logical design of communication networks, image seg-
mentation, and many others. In machine learning, it
is a sub-problem of hierarchical clustering (Dasgupta,
2016; Chatziafratis et al., 2018).
√

Arora et al. (2009) propose a O(
log n)-approximation
algorithm for this problem based on an SDP relaxation
with O(n3) triangle inequality constraints. We adapt
their formulation to our SDP model (2):

(cid:104)L, w(cid:105)

min
w∈Sn×n
+
Tr(w)≤n
subj. to n Tr(w) − Tr(1n×nw) =

n2
2

wij + wjk − wik − wjj ≤ 0 ∀i, j, k ∈ V

(22)

where L is the graph Laplacian of G.

We used three datasets from the Network Repository
(Rossi and Ahmed, 2015):‡ 25mammalia-primate-
associate-13, 55n-insecta-ant-colony1-day37,
and 102n-insecta-ant-colony4-day10. These three
datasets diﬀer in size by a factor of ten. See Table S1
in the Appendix for more details. We use β0 = 100 for
all three network datasets.

Figure 3 presents the results of this experiment. As in
the k-means experiment, the objective residual infeasi-
bility error represent |f (wk)−f (cid:63)|/|f (cid:63)| and dist(Awk, K)
respectively. H-SPIDER-FW is aﬀected by the growing
number of constraints because of its increasing batch
size strategy. Other methods, with constant batch size,
are less aﬀected. H-SAG-CGM/v2 performs compet-
itively against H-SPIDER-FW without requiring an
increasing batch size.

‡https://networkrepository.com

We developed a fast randomized conditional gradient
method for solving convex composite ﬁnite-sum prob-
lems. The proposed method is particularly suitable for
solving SDPs with a large number of aﬃne constraints.
Theoretically, the proposed method has favorable scal-
ing properties compared to the previous state-of-the-art.
Empirically, it performs on par with more sophisticated
variance reduction techniques.

The proposed method takes advantage of a structural
assumption on the separability of the objective by ap-
plying randomization. For the non-smooth term, the
proposed method tackles the two subcases of deter-
ministic and stochastic separately. If the non-smooth
term is deterministic, the proposed method obtains
an ε-suboptimal solution after O(ε−2dm) arithmetic
operations (where d is the dimensionality of the de-
cision variable and m is the number of constraints
comprising g). This improves the previous complexity
of O(ε−3dm) found in Locatello et al. (2019).

If we further assume that the non-smooth part is also
separable, then we can employ a fully randomized
scheme to ﬁnd an ε-suboptimal solution after O(ε−2d)
arithmetic operations. This total cost complexity is
independent of m and thus represents a signiﬁcant im-
provement compared over previous work (Vladarean
et al., 2020) which has a total cost of O(ε−2dm).

Acknowledgments

The authors thank Vincent Fortuin for his helpful feed-
back on an initial draft of this work and the anony-
mous reviewers for their detailed comments. This work
started while F.L. was at ETH Zürich and is based on
the research done outside of Amazon.

This work was supported by ETH core funding to G.R.
(funding G.D.). V.C. has received funding from the
European Research Council (ERC) under the Euro-
pean Union’s Horizon 2020 research and innovation
programme (grant agreement n◦ 725594 – time-data).
M.L.V. was supported by the Swiss National Science
Foundation (SNSF) for the project “Theory and Meth-
ods for Storage-Optimal Optimization” grant number
200021_178865. A.Y. received support from the Wal-
lenberg AI, Autonomous Systems and Software Pro-
gram (WASP) funded by the Knut and Alice Wallen-
berg Foundation.

References

Zeeshan Akhtar and Ketan Rajawat.

“Zeroth and
First Order Stochastic Frank-Wolfe Algorithms
for Constrained Optimization”.
arXiv preprint
arXiv:2107.06534, 2021.

Faster One-Sample SCGM for Composite Convex Minimization

Abdo Y Alfakih, Amir Khandani, and Henry Wolkow-
icz. Solving Euclidean distance matrix completion
problems via semideﬁnite programming. Computa-
tional optimization and applications, 1999.

Brian Kulis, Arun C Surendran, and John C Platt.
Fast low-rank semideﬁnite programming for embed-
ding and clustering. In Artiﬁcial Intelligence and
Statistics. PMLR, 2007.

Sanjeev Arora, Satish Rao, and Umesh Vazirani. Ex-
pander ﬂows, geometric embeddings and graph par-
titioning. Journal of the ACM (JACM), 2009.

Guanghui Lan. The complexity of large-scale con-
vex programming under a linear optimization oracle.
arXiv preprint arXiv:1309.5550, 2013.

Vaggos Chatziafratis, Rad Niazadeh, and Moses
Charikar. Hierarchical clustering with structural
constraints. In International Conference on Machine
Learning. PMLR, 2018.

Frank H Clarke. Optimization and nonsmooth analysis.

SIAM, 1990.

Sanjoy Dasgupta. A cost function for similarity-based
hierarchical clustering. In Proceedings of the forty-
eighth annual ACM symposium on Theory of Com-
puting, 2016.

Olivier Fercoq, Ahmet Alacaoglu, Ion Necoara, and
Volkan Cevher. Almost surely constrained convex
optimization. Proceedings of the 36th International
Conference on Machine Learning, 2019.

Marguerite Frank and Philip Wolfe. An algorithm
for quadratic programming. Naval research logistics
quarterly, 1956.

Robert M Freund, Paul Grigas, and Rahul Mazumder.
An Extended Frank–Wolfe Method with “In-Face”
Directions, and Its Application to Low-Rank Matrix
Completion. SIAM Journal on optimization, 27(1):
319–346, 2017.

Dan Garber. Faster Projection-free Convex Optimiza-
tion over the Spectrahedron. Advances in Neural
Information Processing Systems, 29:874–882, 2016.

Michel X Goemans and David P Williamson. Improved
approximation algorithms for maximum cut and sat-
isﬁability problems using semideﬁnite programming.
Journal of the ACM (JACM), 1995.

Elad Hazan. Sparse approximate solutions to semidef-
inite programs. In Latin American symposium on
theoretical informatics. Springer, 2008.

Elad Hazan and Haipeng Luo. Variance-Reduced
and Projection-Free Stochastic Optimization. arXiv
preprint arXiv:1602.02101, 2017.

Martin Jaggi. Revisiting Frank-Wolfe: Projection-free
sparse convex optimization. In International Confer-
ence on Machine Learning. PMLR, 2013.

Martin Jaggi and Marek Sulovsk`y. A simple algorithm
for nuclear norm regularized problems. In Proceedings
of the 27th International Conference on International
Conference on Machine Learning, 2010.

Thomas Kerdreux, Alexandre d’Aspremont, and Sebas-
tian Pokutta. Local and Global Uniform Convexity
Conditions. ArXiv, abs/2102.05134, 2021.

Fabian Latorre, Paul Rolland, and Volkan Cevher.
Lipschitz constant estimation of neural networks
via sparse polynomial optimization. arXiv preprint
arXiv:2004.08688, 2020.

E.S. Levitin and B.T. Polyak. Constrained minimiza-
tion methods. USSR Computational Mathematics
and Mathematical Physics, 1966. ISSN 0041-5553.

Francesco Locatello, Alp Yurtsever, Olivier Fercoq,
and Volkan Cevher. Stochastic Frank-Wolfe for Com-
posite Convex Minimization. Advances in Neural
Information Processing Systems, 2019.

Konstantin Mishchenko and Peter Richtárik. A stochas-
tic decoupling method for minimizing the sum of
smooth and non-smooth functions. arXiv:1905.11535,
2019.

Dustin G Mixon, Soledad Villar, and Rachel Ward.
Clustering subgaussian mixtures by semideﬁnite pro-
gramming. arXiv preprint arXiv:1602.06612, 2016.

Aryan Mokhtari, Hamed Hassani, and Amin Karbasi.
Stochastic Conditional Gradient Methods: From
Convex Minimization to Submodular Maximization.
The Journal of Machine Learning Research (JMLR),
2018.

Geoﬀrey Négiar, Gideon Dresdner, Alicia Yi-Ting Tsai,
Laurent El Ghaoui, Francesco Locatello, and Fabian
Pedregosa. Stochastic Frank-Wolfe for Constrained
Finite-Sum Minimization. The International Con-
ference on Machine Learning (ICML), 2020.

Yu. Nesterov. Smooth minimization of non-smooth

functions. Mathematical Programming, 2005.

Yu. Nesterov. Complexity bounds for primal-dual
methods minimizing the model of objective function.
Mathematical Programming, 2018.

Andrei Patrascu and Ion Necoara. Nonasymptotic
convergence of stochastic proximal point methods
for constrained convex optimization. The Journal of
Machine Learning Research, 2017.

Jiming Peng and Yu Wei. Approximating k-means-
type clustering via semideﬁnite programming. SIAM
journal on optimization, 2007.

Aditi Raghunathan, Jacob Steinhardt, and Percy S
Liang. Semideﬁnite relaxations for certifying robust-
ness to adversarial examples. In S. Bengio, H. Wal-
lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,

Dresdner, Vladarean, Rätsch, Locatello, Cevher, Yurtsever

Alp Yurtsever, Joel A Tropp, Olivier Fercoq, Madeleine
Udell, and Volkan Cevher. Scalable semideﬁnite
programming. SIAM Journal on Mathematics of
Data Science, 3(1):171–200, 2021.

Mingrui Zhang, Zebang Shen, Aryan Mokhtari, Hamed
Hassani, and Amin Karbasi. One sample stochastic
frank-wolfe. In International Conference on Artiﬁcial
Intelligence and Statistics. PMLR, 2020.

Qing Zhao, Stefan E Karisch, Franz Rendl, and Henry
Wolkowicz. Semideﬁnite programming relaxations
for the quadratic assignment problem. Journal of
Combinatorial Optimization, 1998.

and R. Garnett, editors, Advances in Neural In-
formation Processing Systems, volume 31. Curran
Associates, Inc., 2018.

Sashank J Reddi, Suvrit Sra, Barnabás Póczos, and
Alex Smola. Stochastic Frank-Wolfe methods for non-
convex optimization . In 54th Annual Allerton Conf.
Communication, Control, and Computing, 2016.

Ryan Rossi and Nesreen Ahmed. The network data
repository with interactive graph analytics and visu-
alization. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, 2015.

Mark Schmidt, Nicolas Le Roux, and Francis Bach.
Minimizing Finite Sums with the Stochastic Average
Gradient. Mathematical Programming, 2017.

Zebang Shen, Cong Fang, Peilin Zhao, Junzhou Huang,
and Hui Qian. Complexities in Projection-Free
Stochastic Non-Convex Minimization . In Proc. 22nd
Int. Conf. Artiﬁcial Intelligence and Statistics, 2019.

Quoc Tran-Dinh, Olivier Fercoq, and Volkan Cevher.
A Smooth Primal-Dual Optimization Framework
for Nonsmooth Composite Convex Minimiza-
tion.
SIAM Journal on Optimization, 2018.
doi:10.1137/16M1093094.

Maria-Luiza Vladarean, Ahmet Alacaoglu, Ya-Ping
Hsieh, and Volkan Cevher. Conditional gradient
methods for stochastically constrained convex mini-
mization. arXiv preprint arXiv:2007.03795, 2020.

Yangyang Xu. Primal-dual stochastic gradient method
for convex programs with many functional con-
straints. SIAM Journal on Optimization, 2020.

Alp Yurtsever, Ya-Ping Hsieh, and Volkan Cevher.
Scalable convex methods for phase retrieval.
In
2015 IEEE 6th International Workshop on Computa-
tional Advances in Multi-Sensor Adaptive Processing
(CAMSAP), 2015.

Alp Yurtsever, Olivier Fercoq, Francesco Locatello, and
Volkan Cevher. A conditional gradient framework
for composite convex minimization with applications
to semideﬁnite programming. Proceedings of the
35th International Conference on Machine Learning,
2018.

Alp Yurtsever, Olivier Fercoq, and Volkan Cevher. A
Conditional-Gradient-Based Augmented Lagrangian
Framework. Proceedings of the 36th International
Conference on Machine Learning, 97:7272–7281, 09–
15 Jun 2019a.

Alp Yurtsever, Suvrit Sra, and Volkan Cevher. Con-
ditional gradient methods via stochastic path-
integrated diﬀerential stimator. In Proceedings of the
36th International Conference on Machine Learning,
2019b.

Faster One-Sample SCGM for Composite Convex Minimization

Supplementary Material:
Faster One-Sample Stochastic Conditional Gradient Method for
Composite Convex Minimization

A Background on Smoothing

This section recalls some useful properties about the smoothing technique (Nesterov, 2005). We present these
known properties in this section for completeness, since we use them in our analysis.

Let g : Rm → R ∩ {+∞} be a proper, closed and convex function. The smooth approximation of g is deﬁned by

gβ(z) = max
y∈Rd

(cid:26)

(cid:104)z, y(cid:105) − g∗(y) −

(cid:27)

(cid:107)y(cid:107)2

β
2

(23)

where g∗ denotes the Fenchel conjugate and β > 0 is the smoothing parameter. Then, gβ is convex and 1
Let y∗

β(z) denote the solution of the maximization sub-problem in (23), i.e.,

β -smooth.

y∗
β(z) = arg max
y∈Rd

(cid:26)

(cid:104)z, y(cid:105) − g∗(y) −

(cid:27)

(cid:107)y(cid:107)2

β
2

(cid:104)z, y(cid:105) +

1
2

(cid:107)y(cid:107)2 +

(cid:27)

z(cid:107)2

1
2

(cid:107)

1
β

(cid:107)y −

(cid:27)

z(cid:107)2

1
β

1
β
1
2

(cid:26) 1
β
(cid:26) 1
β

= arg min
y∈Rd

g∗(y) −

g∗(y) +

= arg min
y∈Rd
= proxβ−1g∗ (β−1z)
(cid:0)z − proxβg(z)(cid:1)

=

1
β

where the last line is the Moreau decomposition. Then, the followings hold ∀z1, z2 ∈ Rm and ∀β, γ > 0

gβ(z1) ≥ gβ(z2) + (cid:104)∇gβ(z2), z1 − z2(cid:105) +

g(z1) ≥ gβ(z2) + (cid:104)∇gβ(z2), z1 − z2(cid:105) +

gβ(z1) ≤ gγ(z1) +

γ − β
2

(cid:107)y∗

β(z1)(cid:107)2

β
2
β
2

(cid:107)y∗

β(z2) − y∗

β(z1)(cid:107)2

(cid:107)y∗

β(z2)(cid:107)2

We refer to Lemma 10 in (Tran-Dinh et al., 2018) for the proofs.
Suppose that g is Lg-Lipschitz continuous. Then, for ∀β > 0 and ∀z ∈ Rm,

gβ(z) ≤ g(z) ≤ gβ(z) +

β
2

L2
g,

(24)

(25)

(26)

(27)

(28)

(29)

(30)

(31)

(32)

The proof follows immediately from Equation (2.7) in (Nesterov, 2005) with a remark on the duality between
bounded domain and Lipschitz continuity.

Dresdner, Vladarean, Rätsch, Locatello, Cevher, Yurtsever

B Proof of Lemma 4.1

We follow the steps laid out in Theorem 4.1 in (Vladarean et al., 2020), which in turn builds upon Theorem 9 in
(Locatello et al., 2019).

We use the quadratic upper bound ensured by the fact that Fβk is LFβk

-smooth:

Fβk (wk+1) ≤ Fβk (wk) + (cid:104)∇Fβk (wk), wk+1 − wk(cid:105) +

≤ Fβk (wk) + ηk(cid:104)∇Fβk (wk), sk − wk(cid:105) +

(cid:107)wk+1 − wk(cid:107)2

LFβk
2
η2
kLFβk
2

D2
W

where the second line follows from the boundedness of W.

Next, we use the rule for change of β in smoothing (see (31)), which gives

Fβk (wk+1) ≤ Fβk−1 (wk) +

βk−1 − βk
2

(cid:107)y(cid:63)
βk

(Awk)(cid:107)2 + ηk(cid:104)∇Fβk (wk), sk − wk(cid:105) +

η2
kLFβk
2

D2
W

,

where y(cid:63)
βk

is deﬁned as in (24).

Then, we bound the term (cid:104)∇Fβk (wk), sk − wk(cid:105) as follows:

(cid:104)∇Fβk (wk),sk − wk(cid:105) = (cid:104)∇Fβk (wk) − vk, sk − wk(cid:105) + (cid:104)vk, sk − wk(cid:105)

= (cid:104)∇Fβk (wk) − vk, sk − w(cid:63)(cid:105) + (cid:104)∇Fβk (wk) − vk, w(cid:63) − wk(cid:105) + (cid:104)vk, sk − wk(cid:105)
≤ (cid:104)∇Fβk (wk) − vk, sk − w(cid:63)(cid:105) + (cid:104)∇Fβk (wk) − vk, w(cid:63) − wk(cid:105) + (cid:104)vk, w(cid:63) − wk(cid:105)
= (cid:104)∇Fβk (wk) − vk, sk − w(cid:63)(cid:105) + (cid:104)∇Fβk (wk), w(cid:63) − wk(cid:105)

where the inequality follows by the deﬁnition of sk.
Now, we focus on the term (cid:104)∇Fβk (wk), w(cid:63) − wk(cid:105) and bound it as follows:

(cid:104)∇Fβk (wk), w(cid:63) − wk(cid:105) = (cid:104)X T ∇f (Xwk) + AT ∇gβk (Awk), w(cid:63) − wk(cid:105)

= (cid:104)∇f (Xwk), X(w(cid:63) − wk)(cid:105) + (cid:104)∇gβk (Awk), A(w(cid:63) − wk)(cid:105)

≤ f (Xw(cid:63)) − f (Xwk) + g(Aw(cid:63)) − gβk (Awk) −

βk
2

(cid:107)y(cid:63)
βk

(Awk)(cid:107)2

= F (cid:63) − Fβk (wk) −

βk
2

(cid:107)y(cid:63)
βk

(Awk)(cid:107)2,

where the inequality holds due to the convexity of f and g and the smoothing property in (30).

Combining all these bounds and subtracting F (cid:63) from both sides, we get

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

(42)

(43)

Fβk (wk+1) − F (cid:63) ≤ (1 − ηk) (cid:0)Fβk−1 (wk) − F (cid:63)(cid:1) + ηk(cid:104)∇Fβk (wk) − vk, sk − w(cid:63)(cid:105)
η2
kLFβk
2

((1 − ηk)(βk−1 − βk) − ηkβk)(cid:107)y(cid:63)
βk

(Awk)(cid:107)2

2 +

1
2

+

D2
W

(44)

We cannot bound (cid:107)y(cid:63)
βk
and βk = β0√

k+1

for an arbitrary β0 > 0. Then,

(Awk)(cid:107)2

2 in general, so we choose ηk and βk carefully to vanish this term. Let ηk = 2
k+1

(1 − ηk)(βk−1 − βk) − ηkβk =

(cid:32)

k − 1
k + 1

β0√
k

(cid:33)

√

k
k + 1

−

√

< 0,

for all k ≥ 1.

(45)

Finally, taking expectation on both sides and applying the deﬁnition of Sβ(w) (cid:44) E [Fβ(w) − F (cid:63)] we arrive at our
stated result:

Sβk (wk+1) ≤ (1 − ηk)Sβk−1(wk) + ηkE[(cid:104)∇Fβk (wk) − vk, sk − w(cid:63)(cid:105)] +

η2
kLFβk
2

D2
W

.

(46)

Faster One-Sample SCGM for Composite Convex Minimization

C Proof of Theorem 4.1

Our aim is to get a rate on the smoothed gap Sβk (wk+1). We start from Lemma 4.1:

Sβk (wk+1) ≤ (1 − ηk)Sβk−1 (wk) + ηkE[(cid:104)∇Fβk (wk) − vk, sk − w(cid:63)(cid:105)] +

η2
k
2

(cid:18) (cid:107)X(cid:107)Lf
n

(cid:19)

+

(cid:107)A(cid:107)
βk

D2

W .

(47)

Multiply both sides by k(k + 1) and unroll the recurrence to get

k(k + 1)Sβk (wk+1) ≤ (k − 1)kSβk−1 (wk) + 2kE[(cid:104)∇Fβk (wk) − vk, sk − w(cid:63)(cid:105)] +

2k
k + 1

(cid:18) (cid:107)X(cid:107)Lf
n

(cid:19)

+

(cid:107)A(cid:107)
βk

D2
W

≤

k
(cid:88)

i=1
(cid:124)

2iE[(cid:104)∇Fβi(wi) − vi, si − w(cid:63)(cid:105)]

+

(cid:123)(cid:122)
A

(cid:125)

2i
i + 1

k
(cid:88)

i=1
(cid:124)

(cid:18) (cid:107)X(cid:107)Lf
n
(cid:123)(cid:122)
B

(cid:19)

+

(cid:107)A(cid:107)
βi

D2
W

.

(cid:125)

First, we get an upper-bound on the variance term A as follows:

E[(cid:104)∇Fβk (wk) − vk, sk − w(cid:63)(cid:105)] = E[(cid:104)X T (∇f (Xwk) − αk) + AT (∇gβk (Awk) − γk), sk − w(cid:63)(cid:105)]

= E[(cid:104)∇f (Xwk) − αk, X(sk − w(cid:63))(cid:105)]
≤ E[(cid:107)∇f (Xwk) − αk(cid:107)1 (cid:107)X(sk − w(cid:63))(cid:107)∞]
≤ E[(cid:107)∇f (Xwk) − αk(cid:107)1] D∞(X)

(48)

(49)

(50)

(51)

(52)

where, the ﬁrst inequality is the Hölder’s inequality, and the second one is based on the boundedness of W.

Then, by Lemma 4.2, we have

E[(cid:107)∇f (Xwk) − αk(cid:107)1] ≤ (cid:0)1 − 1

n

(cid:1)k

(cid:107)∇f (Xw1) − α0(cid:107)1 +

(cid:18)

2Lf D1(X)
n

(cid:0)1 − 1

n

(cid:1)k/2

log k +

2(n − 1)
k

(cid:19)

.

(53)

Finally, we combine (52) and (53) to get

(cid:34)

A ≤ 2D∞(X)

(cid:107)∇f (Xw1) − α0(cid:107)1

k
(cid:88)

i=1

i (cid:0)1 − 1

n

(cid:1)i

+

2Lf D1(X)
n

k
(cid:88)

(cid:16)

i=1

i (cid:0)1 − 1

n

(cid:1)i/2

log i + 2(n − 1)

(cid:35)

(cid:17)

(cid:20)
(cid:107)∇f (Xw1) − α0(cid:107)1 n2 +

≤ 2D∞(X)
≤ 2D∞(X) (cid:2)(cid:107)∇f (Xw1) − α0(cid:107)1 n2 + 4Lf D1(X) (cid:0)8n2 + k(cid:1)(cid:3)

2Lf D1(X)
n

(cid:0)16n3 + 2(n − 1)k(cid:1)

(cid:21)

where we use Lemma F.1 for the second line.

Next, we focus on the term B , and we use once again Lemma F.1 and obtain

B = 2D2
W

(cid:32)

(cid:107)X(cid:107)Lf
n

k
(cid:88)

i=1

i
i + 1

+

(cid:107)A(cid:107)
β0

(cid:33)

k
(cid:88)

i=1

√

i
i + 1

≤ 2D2
W

(cid:18) (cid:107)X(cid:107)Lf
n

k +

√

(cid:107)A(cid:107)
β0

k

(cid:19)

k + 1

.

(54)

(55)

(56)

(57)

To ﬁnalize, we substitute the bounds on A and B into (48) and divide both sides by k(k + 1) to get the desired
bound on Sβk (wk+1):

Sβk (wk+1) ≤

≤

2D∞(X)
k(k + 1)
C3
k(k + 1)

(cid:2)(cid:107)∇f (Xw1) − α0(cid:107)1 n2 + 4Lf D1(X) (cid:0)8n2 + k(cid:1)(cid:3) +

2D2
W
k(k + 1)

(cid:18) (cid:107)X(cid:107)Lf
n

k +

√

(cid:107)A(cid:107)
β0

k

(cid:19)

k + 1

+

C2
k + 1

+

C1√

k + 1

, where C3 = 2n2D∞(X)(cid:0)(cid:107)∇f (Xw1) − α0(cid:107)1 + 32Lf D1(X)(cid:1)
C2 = 8Lf D1(X)D∞(X) + 2n−1Lf (cid:107)X(cid:107)D2
W
C1 = 2D2

W (cid:107)A(cid:107)β−1
0 .

Dresdner, Vladarean, Rätsch, Locatello, Cevher, Yurtsever

C.1 Proof of Corollary 4.1

Suppose g is Lg-Lipschitz continuous. Then, from (32) we get

EF (xk+1) − F (cid:63) = E[f (Xwk+1) + g(Awk+1)] − F (cid:63)

≤ E[f (Xwk+1) + gβk (Awk+1)] − F (cid:63) +
β0L2
g
√
k + 1
2

= Sβk (wk+1) +

.

βkL2
g
2

C.2 Proof of Corollary 4.2

Suppose g(z) = δK(z), the indicator function of a closed and convex set. We can write the Lagrangian as

L(w, r, y) (cid:44) f (Xw) + (cid:104)Aw − r, y(cid:105),

w ∈ W, r ∈ K.

From the Lagrange saddle point theory, we have

f (Xw(cid:63)) ≤ L(w, r, y(cid:63)) ≤ f (Xw) + (cid:107)Aw − r(cid:107)(cid:107)y(cid:63)(cid:107),

∀w ∈ W and ∀r ∈ K.

(58)

(59)

(60)

(61)

(62)

Letting w = wk+1 ∈ W and r = projK(Awk+1) ∈ K, taking expectation on both sides and rearranging, we get

E [f (Xwk+1) − f (Xw∗)] ≥ −(cid:107)y(cid:63)(cid:107) E [dist(Awk+1, K)]

This is the desired lower-bound on objective residual.

Next, we derive an upper bound on objective residual. By deﬁnition of gβ (see (23)) for δK,

Note that f (Xw(cid:63)) = F (w(cid:63)) since g(Aw(cid:63)) = 0. Then,

gβ(Aw) =

1
2β

dist(Aw, K)2.

E[f (Xwk+1) − f (Xw(cid:63))] = E[Fβk (wk+1) − F (cid:63) − gβk (Awk+1)]

≤ Sβk (wk+1) −

≤ Sβk (wk+1).

1
2βk

E[dist(Awk+1, K)2]

Finally, we derive convergence rate of the infeasibility error. To this end, we combine (63) and (66):

−(cid:107)y(cid:63)(cid:107) E [dist(Awk+1, K)] ≤ Sβk (wk+1) −

1
2βk

E[dist(Awk+1, K)2]

(63)

(64)

(65)

(66)

(67)

(68)

We rearrange and apply Jensen’s inequality to E[dist(Awk+1, K)2], and we get a second order inequality with
respect to E[dist(Awk+1, K)]:

1
2βk

E[dist(Awk+1, K)]2
(cid:124)
(cid:123)(cid:122)
(cid:125)
t2

−(cid:107)y(cid:63)(cid:107) E[dist(Awk+1, K)]
(cid:125)

(cid:124)

(cid:123)(cid:122)
t

−Sβk (wk+1) ≤ 0.

(69)

By solving this inequality for t, we achieve the desired bound:

E[dist(Awk+1, K)] ≤ βk

(cid:107)y(cid:63)(cid:107) +

(cid:107)y(cid:63)(cid:107)2 +

(cid:32)

(cid:115)

(cid:33)

2Sβk (wk+1)
βk

≤ 2βk(cid:107)y(cid:63)(cid:107) +

(cid:113)

2βkSβk (wk+1),

(70)

where we used

√

a2 + b2 ≤ a + b for a, b ≥ 0 in the last inequality to simplify the terms.

Faster One-Sample SCGM for Composite Convex Minimization

D Proof of Lemma 4.3

The following Lemma will be needed in the subsequent characterization of the estimator variance.
Lemma D.1. Let ρ ∈ (0, 1), C ∈ R and {uk}k∈N be a sequence such that

Then, it holds that

Proof. Unrolling the recurrence yields

uk ≤ ρ(uk−1 +

1
√
k

C).

uk ≤ ρku1 +

√

2Cρ
k(1 − ρ)

.

uk ≤ ρk−1u1 + C

k
(cid:88)

i=2

ρk−i+1
√
i

Observe that ρk+1−i is a monotonically increasing with i because ρ ∈ (0, 1). Therefore,

1

(cid:80)k

i=1

1√
i

k
(cid:88)

i=1

ρk−i+1
√
i

≤

1
k

k
(cid:88)

i=1

ρk−i+1 =

1
k

k
(cid:88)

i=1

ρi

(71)

(72)

(73)

(74)

since the left side of the inequality is a weighted average of ρk−i+1 with decreasing weights and the right side is
the simple average with uniform weights. The equality holds simply by change of indices. Now, we rearrange as

k
(cid:88)

i=1

ρk−i+1
√
i

≤

1
k

(cid:32) k

(cid:88)

i=1

1
√
i

(cid:33) (cid:32) k

(cid:88)

(cid:33)

ρi

≤

√

i=1

2ρ
k(1 − ρ)

We complete the proof by combining (73) and (75).

D.1 Proof of Lemma 4.3 for indicator functions

First, we prove Lemma 4.3 for the case in which g is an indicator function. Observe that

Ek[|∇gβk (Awk)j − γk,j|] =

1
m

0 +

m − 1
m

|∇gβk (Awk)j − γk−1,j|.

Summing over all coordinates gives

E[(cid:107)∇gβk (Awk) − γk(cid:107)1] =

=

≤

m − 1
m
m − 1
m
m − 1
m

E[(cid:107)∇gβk (Awk) − γk−1(cid:107)1]

E[(cid:107)∇gβk (Awk) − ∇gβk−1(Awk−1) + ∇gβk−1 (Awk−1) − γk−1(cid:107)1]
(cid:17)
(cid:16)
E[(cid:107)∇gβk−1 (Awk−1) − γk−1(cid:107)1] + E[(cid:107)∇gβk (Awk) − ∇gβk−1(Awk−1)(cid:107)1]

.

Now, we focus on the last term and bound it as follows:

(cid:107)∇gβk (Awk) − ∇gβk−1(Awk−1)(cid:107)1 = (cid:107)∇gβk (Awk) ± ∇gβk (Awk−1) − ∇gβk−1 (Awk−1)(cid:107)1

≤ (cid:107)∇gβk (Awk) − ∇gβk (Awk−1)(cid:107)1 + (cid:107)∇gβk (Awk−1) − ∇gβk−1(Awk−1)(cid:107)1

(cid:107)A(wk−1 − wk)(cid:107)1 +

≤

≤

≤

1
mβk
ηk−1
mβk
D1(A)
m

D1(A) +

1
m

(cid:16) 1
βk

(cid:16) ηk−1
βk

+

1
βk

−

(cid:16) 1
βk
(cid:17)

−

1
m
1
βk−1
(cid:17)
1
βk−1

−

(cid:17)

1
βk−1

(cid:107)Awk−1 − projK(Awk−1)(cid:107)1

(cid:107)Awk−1 − Aw(cid:63)(cid:107)1

(75)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

(83)

(84)

Dresdner, Vladarean, Rätsch, Locatello, Cevher, Yurtsever

where the third inequality is due to the fact that K = K1 × K2 × · · · × Km. Simplifying further: ηk−1
βk
2
k

β0k + k+1

, gives

< 2
k

+ 2

k+1
β0

k+1
β0

k+1
β0

k
β0

+

−

+

√

√

√

√

√

√

√

k+1
√
k

< 5
√
β0

< 2
√
β0

− k
√
β0

− k
√
β0

k
β0

β0

k

k

k

k

k

+ 1
βk

− 1

βk−1

=

(cid:107)∇gβk (Awk) − ∇gβk−1(Awk−1)(cid:107)1 ≤

5D1(A)
k
mβ0

√

.

Substituting this back into (79), we get

E[(cid:107)∇gβk (Awk) − γk(cid:107)1] ≤

(cid:16)

m − 1
m

E[(cid:107)∇gβk−1 (Awk−1) − γk−1(cid:107)1] +

√

5D2(A)
√
k
β0

m

(cid:17)

.

This is in the form of (71). We conclude the proof by applying Lemma D.1:

E[(cid:107)∇gβk (Awk) − γk(cid:107)1] ≤

(cid:19)k

(cid:18) m − 1
m

E[(cid:107)∇gβ0(Aw0) − γ0(cid:107)1] +

10D2(A)
β0

√
m(m − 1)
√
k

.

D.2 Proof of Lemma 4.3 for Lipschitz continuous functions

Suppose g is Lipschitz continuous with parameter Lg. Then, from (32), we get

f (Xwk+1) + g(Awk+1)
(cid:123)(cid:122)
(cid:125)
(cid:124)
F (wk+1)

≤ f (Xwk+1) + gβk (Awk+1)
(cid:123)(cid:122)
(cid:125)
Fβk (wk+1)

(cid:124)

+

βk
2

L2

g = Fβk (wk+1) +

β0L2
g
√
k + 1
2

.

We achieve the desired bound by subtracting F (cid:63) and taking expectation on both sides:

E[F (wk+1) − F (cid:63)] ≤ Sβk (wk+1) +

β0L2
g
√
k + 1

.

2

(85)

(86)

(87)

(88)

(89)

To bound Sβk , we can follow the proof of Lemma 4.3 up to (81), which we repeat here for convenience:

(cid:107)∇gβk (Awk) − ∇gβk (Awk−1)(cid:107)1 + (cid:107)∇gβk (Awk−1) − ∇gβk−1(Awk−1)(cid:107)1

Recall that ∇gβ(z) = β−1(z − proxβg(z)). The ﬁrst term can be bounded using the 1/β-smoothness of gβ. For
the second term, recall the well-established fact that proxg(z) = λ proxg/λ(x/λ) for any λ > 0. Thus,

∇gβk (Awk−1) = βk

= βk

−1(Awk−1 − proxβkg(Awk−1))
βk
−1(Awk−1 −
βk−1

proxβk−1g(

βk−1
βk

Awk−1))

Thus,

= ∇gβk−1 (

βk−1
βk

Awk−1)

(cid:107)∇gβk (Awk) − ∇gβk (Awk−1)(cid:107)1 + (cid:107)∇gβk (Awk−1) − ∇gβk−1(Awk−1)(cid:107)1

≤

≤

≤

1
mβk
ηk−1
mβk
D1(A)
m

(cid:107)A(wk − wk−1)(cid:107)1 +

D1(A) +
(cid:18) ηk−1
βk

1
m

+

(

1
βk
1
βk

−

−

1
mβk−1
1
βk−1
(cid:19)
1
βk−1

(

βk−1
βk

− 1)(cid:107)Awk−1(cid:107)1

)(cid:107)Awk−1(cid:107)1

(90)

(91)

(92)

(93)

(94)

(95)

(96)

Note that this is identical to (84) in Lemma D.1. Thus, the rest of Lemma D.1 can be applied to arrive at the
same bound.

Faster One-Sample SCGM for Composite Convex Minimization

E Proof of Theorem 4.1 for H-SAG-CGM/v2

The proof is same until (48). Then, get an upper-bound on the variance term A as follows:

E[(cid:104)∇Fβk (wk) − vk,sk − w(cid:63)(cid:105)] = E[(cid:104)X T (∇f (Xwk) − αk) + AT (∇gβk (Awk) − γk), sk − w(cid:63)(cid:105)]

= E[(cid:104)∇f (Xwk) − αk, X(sk − w(cid:63))(cid:105) + (cid:104)∇gβk (Awk) − γk, A(sk − w(cid:63))(cid:105)]
≤ E[(cid:107)∇f (Xwk) − αk(cid:107)1 (cid:107)X(sk − w(cid:63))(cid:107)∞ + (cid:107)∇gβk (Awk) − γk(cid:107)1 (cid:107)A(sk − w(cid:63))(cid:107)∞]
≤ E[(cid:107)∇f (Xwk) − αk(cid:107)1] D∞(X) + E[(cid:107)∇gβk (Awk) − γk(cid:107)1] D∞(A)

(97)

(98)

(99)

(100)

where, the ﬁrst inequality is the Hölder’s inequality, and the second one is based on the boundedness of W.

Then, by Lemma 4.2, we have

E[(cid:107)∇f (Xwk) − αk(cid:107)1] ≤ (cid:0)1 − 1

n

(cid:1)k

(cid:107)∇f (Xw1) − α0(cid:107)1 +

2Lf D1(X)
n

And by Lemma 4.3, we have

(cid:18)

(cid:0)1 − 1

n

(cid:1)k/2

log k +

(cid:19)

2(n − 1)
k

(101)

E[(cid:107)∇gβk (Awk) − γk(cid:107)1] ≤ (cid:0)1 − 1

m

(cid:1)k E[(cid:107)∇gβ0(Aw1) − γ0(cid:107)1] +

10D2(A)
β0

√
m(m − 1)
√
k

.

(102)

Finally, we substitute (101) and (102) back into (100) to get

(cid:34)

A ≤ 2D∞(X)

(cid:107)∇f (Xw1) − α0(cid:107)1

k
(cid:88)

i=1

i (cid:0)1 − 1

n

(cid:1)i

+

2Lf D1(X)
n

k
(cid:88)

(cid:16)

i (cid:0)1 − 1

n

(cid:1)i/2

log i + 2(n − 1)

(cid:35)

(cid:17)

(cid:34)

+ 2D∞(A)

(cid:107)∇gβ0(Aw1) − γ0(cid:107)1

k
(cid:88)

i=1

i (cid:0)1 − 1

m

(cid:1)i

+

10D2(A)

m(m − 1)

i=1

√

β0

k
(cid:88)

√

(cid:35)
i

i=1

≤ 2D∞(X)

(cid:20)
(cid:107)∇f (Xw1) − α0(cid:107)1 n2 +

+ 2D∞(A)

(cid:20)
(cid:107)∇gβ0 (Aw1) − γ0(cid:107)1 m2 +

2Lf D1(X)
n
10D2(A)

(cid:0)16n3 + 2(n − 1)k(cid:1)

√

β0

m(m − 1)

k3/2

(cid:21)

(cid:21)

≤ 2D∞(X) (cid:2)(cid:107)∇f (Xw1) − α0(cid:107)1 n2 + 4Lf D1(X) (cid:0)8n2 + k(cid:1)(cid:3)

(cid:20)

+ 2D∞(A)

(cid:107)∇gβ0 (Aw1) − γ0(cid:107)1 m2 +

10D2(A) m3/2
β0

(cid:21)

k3/2

where we use Lemma F.1 for the second inequality.

Combining this with the bound on the smoothness term B from (48) gives the desired result:

Sβk (wk+1) ≤

(cid:40)

2D∞(X)
k(k + 1)

(cid:107)∇f (Xw1) − α0(cid:107)1 n2 + 4Lf D1(X) (cid:0)8n2 + k(cid:1)

+ 2D∞(A)

(cid:20)
(cid:107)∇gβ0 (Aw1) − γ0(cid:107)1 m2 +

10D2(A) m3/2
β0

k3/2

(cid:21)(cid:41)

(103)

(104)

(105)

+

≤

2D2
W
k(k + 1)
C3
k(k + 1)

(cid:18) (cid:107)X(cid:107)Lf
n
C2
k + 1

+

k +

(cid:107)A(cid:107)
β0
C1√

k + 1

+

√

k

k + 1

(cid:19)

, where C3 = 2n2D∞(X)(cid:0)(cid:107)∇f (Xw1) − α0(cid:107)1 + 32Lf D1(X)(cid:1)

+ 2m2D∞(A)(cid:107)∇gβ0 (Aw1) − γ0(cid:107)1
C2 = 8Lf D1(X)D∞(X) + 2n−1Lf (cid:107)X(cid:107)D2
W
C1 = β−1

W (cid:107)A(cid:107) + 10D1(A)).

0 (2D2

Dresdner, Vladarean, Rätsch, Locatello, Cevher, Yurtsever

F Supporting Lemmas

Lemma F.1. Let ρn = 1 − 1

n and ρm = 1 − 1

m , m, n ≥ 1. We present the following bounds:

a)

b)

k
(cid:80)
i=1

k
(cid:80)
i=1

iρi

n < n2

and

k
(cid:80)
i=1

iρi

m < m2

iρi/2

n log i < 16n3

Proof. a) Note that since ρn ∈ [0, 1), (cid:80)k

i=1 iρi

n ≤ (cid:80)k

i=1 iρi−1

n . Furthermore,

k
(cid:88)

i=1

iρi−1

n ≤

∞
(cid:88)

i=1

iρi−1

n =

∞
(cid:88)

i=1

∂ρi
n
∂ρn

=

∂

ρi
n

∞
(cid:80)
i=1
∂ρn

∂

(cid:104) 1
1−ρn
∂ρn

=

(cid:105)
− 1

=

1

(1 − ρn)2 = n2,

(106)

where the inequality comes from all terms being non-negative, and the second equality comes from the fact that
the inﬁnite sum exists for any ρn ∈ (−1, 1) and is the Taylor series expansion of

.

1
1−ρn

b) Use the loose bound log i < i + 1 and the fact that

√

ρn ∈ [0, 1):

k
(cid:88)

i=1

iρi/2

n log i ≤

∞
(cid:88)

i=1

iρi/2

n log i ≤

∞
(cid:88)

i(i + 1)

√

i−1

ρn

i=1
∂2 (cid:80)∞
i=2
√
∂(

pn)2

√

i

ρn

=

1
√

∂2

1−

=

√

−

pn)2

ρn
∂(

√

ρn − 1

=

(1 −

2
√

ρn)3

where the inequalities and equalities follow the same reasoning as in point a). Further noting that

2
√

ρn)3 =

(1 −

√

ρn)3

2(1 +
(1 − ρn)3 = 2n3(1 +
(cid:123)(cid:122)
≤2

(cid:124)

√

)3 ≤ 16n3
ρn
(cid:125)

(107)

(108)

(109)

G Uniform Sparsest Cut Datasets

Table S1: Datasets used for Uniform Sparsest Cut experiments. “Deg.” stands for “Degree,” “# Constraints”
refers to the number of constraints in the SDP relaxation, and the dimension of the decision variable w is n × n.

mammalia-primate-association-13
insecta-ant-colony1-day37
insecta-ant-colony4-day10

25
55
102

181
1e3
4e3

14
42
79

19
53
99

≈ 6.90e3
≈ 7.87e4
≈ 5.15e5

|V | = n

|E| Avg. Node Deg. Max. Node Deg. # Constraints

