From {Solution} Synthesis to {Student Attempt} Synthesis
for Block-Based Visual Programming Tasks∗

Adish Singla
MPI-SWS
adishs@mpi-sws.org

Nikitas Theodoropoulos
MPI-SWS
ntheodor@mpi-sws.org

2
2
0
2

n
u
J

0
2

]
I

A
.
s
c
[

2
v
5
6
2
1
0
.
5
0
2
2
:
v
i
X
r
a

ABSTRACT
Block-based visual programming environments are increas-
ingly used to introduce computing concepts to beginners.
Given that programming tasks are open-ended and concep-
tual, novice students often struggle when learning in these
environments. AI-driven programming tutors hold great
promise in automatically assisting struggling students, and
need several components to realize this potential. We inves-
tigate the crucial component of student modeling, in par-
ticular, the ability to automatically infer students’ miscon-
ceptions for predicting (synthesizing) their behavior. We in-
troduce a novel benchmark, StudentSyn, centered around
the following challenge: For a given student, synthesize the
student’s attempt on a new target task after observing the
student’s attempt on a ﬁxed reference task. This challenge
is akin to that of program synthesis; however, instead of syn-
thesizing a {solution} (i.e., program an expert would write),
the goal here is to synthesize a {student attempt} (i.e., pro-
gram that a given student would write). We ﬁrst show that
human experts (TutorSS) can achieve high performance
on the benchmark, whereas simple baselines perform poorly.
Then, we develop two neuro/symbolic techniques (NeurSS
and SymSS) in a quest to close this gap with TutorSS.

Keywords
block-based visual programming, programming education,
program synthesis, neuro-symbolic AI, student modeling

INTRODUCTION

1.
The emergence of block-based visual programming platforms
has made coding more accessible and appealing to beginners.
Block-based programming uses “code blocks” that reduce the
burden of syntax and introduces concepts in an interactive
way. Led by initiatives like Hour of Code by Code.org [10,
8] and the popularity of languages like Scratch [41], block-
based programming has become integral to introductory CS
∗This article is a longer version of the paper from the EDM
2022 conference. Authors are listed alphabetically.

education. Considering the Hour of Code initiative alone,
over one billion hours of programming activity has been
spent in learning to solve tasks in such environments [8].

Programming tasks on these platforms are conceptual and
open-ended, and require multi-step deductive reasoning to
solve. Given these aspects, novices often struggle when
learning to solve these tasks. The diﬃculties faced by novice
students become evident by looking at the trajectory of stu-
dents’ attempts who are struggling to solve a given task. For
instance, in a dataset released by Code.org [10, 8, 35], even
for simple tasks where solutions require only 5 code blocks
(see Figure 2a), students submitted over 50, 000 unique at-
tempts with some exceeding a size of 50 code blocks.

AI-driven programming tutors have the potential to sup-
port these struggling students by providing personalized as-
sistance, e.g., feedback as hints or curriculum design [37].
To eﬀectively assist struggling students, AI-driven systems
need several components, a crucial one being student mod-
eling. In particular, we need models that can automatically
infer a student’s knowledge from limited interactions and
then predict the student’s behavior on new tasks. However,
student modeling in block-based visual programming envi-
ronments can be quite challenging because of the following:
(i) programming tasks are conceptual, and there is no well-
deﬁned skill-set or problem-solving strategy for mastery [23];
(ii) there could be a huge variability in behaviors and a long-
tail distribution of students’ attempts for a task [51]; (iii) the
objective of predicting a student’s behavior on new tasks is
not limited to coarse-grained success/failure indicators (e.g.,
[49])—ideally, we should be able to do ﬁne-grained synthesis
of attempts for a given student.

Beyond the above-mentioned challenges, there are two criti-
cal issues arising from limited resources and data scarcity for
a given domain. First, while the space of tasks that could be
designed for personalized curriculum is intractably large [1],
the publicly available datasets of real-world students’ at-
tempts are limited; e.g., for the Hour of Code: Maze Chal-
lenge domain, we have datasets for only two tasks [35]. Sec-
ond, when a deployed system is interacting with a new stu-
dent, there is limited prior information [15], and the system
would have to infer the student’s knowledge by observing
behavior on a few reference tasks, e.g., through a quiz [21].
These two issues, in turn, limit the applicability of state-
of-the-art techniques that rely on large-scale datasets across
tasks or personalized data per student (e.g., [49, 28, 29,

 
 
 
 
 
 
def Run(){
move
turnLeft
move
turnRight
move

}

def Run(){
move
turnLeft
move
move

}

?

(a) Reference task T4 with solution code and datasets

(b) stu’s attempt for T4

(c) Target task T4x

(d) stu’s attempt for T4x

Figure 1: Illustration of our problem setup and objective for the task Maze#4 in the Hour of Code: Maze [9] by Code.org [8].
As explained in Section 2.2, we consider three distinct phases in our problem setup to provide a conceptual separation in
terms of information and computation available to a system. (a) In the ﬁrst phase, we are given a reference task T4 along
with its solution code C(cid:63)
T4 and data resources (e.g., a real-world dataset of diﬀerent students’ attempts); reference tasks are
ﬁxed and the system can use any computation a priori. (b) In the second phase, the system interacts with a student, namely
stu, who attempts the reference task T4 and submits a code, denoted as Cstu
T4 . (c, d) In the third phase, the system seeks to
synthesize the student stu’s behavior on a target task T4x, i.e., a program that stu would write if the system would assign
T4x to the student. Importantly, the target task T4x is not available a priori and this synthesis process would be done in
real-time. Furthermore, the system may have to synthesize stu’s behavior on a large number of diﬀerent target tasks (e.g., to
personalize the next task in a curriculum). Section 2 provides further details about the problem setup and objective; Section 3
introduces the StudentSyn benchmark comprising of diﬀerent types of students and target tasks for the reference task.

def Run(){

RepeatUntil(goal){
If(pathAhead){
move
}
Else{

turnLeft

}

}

}

RepeatUntil(goal){

def Run(){
move
turnLeft
move
turnLeft
move

}

}

?

(a) Reference task T18 with solution code and datasets

(b) stu’s attempt for T18

(c) Target task T18x

(d) stu’s attempt for T18x

Figure 2: Analogous to Figure 1, here we illustrate the setup for the task Maze#18 in the Hour of Code: Maze Challenge [9].

36])—we need next-generation student modeling techniques
for block-based visual programming that can operate under
data scarcity and limited observability. To this end, this
paper focuses on the following question:

For a given student, can we synthesize the stu-
dent’s attempt on a new target task after observ-
ing the student’s attempt on a ﬁxed reference task?

1.1 Our Approach and Contributions
Figures 1 and 2 illustrate this synthesis question for two
scenarios in the context of the Hour of Code: Maze Chal-
lenge [9] by Code.org [8]. This question is akin to that of
program synthesis [20]; however, instead of synthesizing a
{solution} (i.e., program an expert would write), the goal
here is to synthesize a {student attempt} (i.e., program that
a given student would write). This goal of synthesizing stu-
dent attempts, and not just solutions, requires going beyond
state-of-the-art program synthesis techniques [3, 4, 25]; cru-
cially, we also need to deﬁne appropriate metrics to quan-
titatively measure the performance of diﬀerent techniques.
Our approach and contributions are summarized below:

(1) We formalize the problem of synthesizing a student’s at-
tempt on target tasks after observing the student’s be-
havior on a ﬁxed reference task. We introduce a novel
benchmark, StudentSyn, centered around the above
synthesis question, along with generative/discriminative
performance measures for evaluation. (Sections 2, 3.1, 3.2)

(2) We showcase that human experts (TutorSS) can achieve
high performance on StudentSyn, whereas simple base-
lines perform poorly. (Section 3.3)

(3) We develop two techniques inspired by neural (NeurSS)
and symbolic (SymSS) methods, in a quest to close the
gap with human experts (TutorSS). (Sections 4, 5, 6)

(4) We publicly release the benchmark and implementations

to facilitate future research.1

1.2 Related Work
Inferring the knowledge state of a stu-
Student modeling.
dent is an integral part of AI tutoring systems and rele-
vant to our goal of predicting a student’s behavior. For
close-ended domains like vocabulary learning ([42, 36, 22])
and Algebra problems ([12, 40, 43]), the skills or knowl-
edge components for mastery are typically well-deﬁned and
we can use Knowledge Tracing techniques to model a stu-
dent’s knowledge state over time [11, 33]. These model-
ing techniques, in turn, allow us to provide feedback, pre-
dict solution strategies, or infer/quiz a student’s knowledge
state [40, 21, 43]. Open-ended domains pose unique chal-
lenges to directly apply these techniques (see [23]); however,
there has been some progress in this direction.
In recent
works [28, 29], models have been proposed to predict hu-
man behavior in chess for speciﬁc skill levels and to recog-
nize the behavior of individual players. Along these lines,
[7] introduced methods to perform early prediction of strug-
gling students in open-ended interactive simulations. There
has also been work on student modeling for block-based pro-
gramming, e.g., clustering-based methods for misconception

1The StudentSyn benchmark and implementation of
the techniques are available at https://github.com/
machine-teaching-group/edm2022_studentsyn.

Datasets forreference taskDatasets forreference taskdiscovery [18, 44], and deep learning methods to represent
knowledge and predict future performance [49].

AI-driven systems for programming education. There has
been a surge of interest in developing AI-driven systems for
programming education, and in particular, for block-based
programming domains [37, 38, 50]. Existing works have
studied various aspects of intelligent feedback, for instance,
providing next-step hints when a student is stuck [35, 52, 31,
15], giving data-driven feedback about a student’s miscon-
ceptions [45, 34, 39, 51], or generating/recommending new
tasks [2, 1, 19]. Depending on the availability of datasets and
resources, diﬀerent techniques are employed: using historical
datasets to learn code embeddings [34, 31], using reinforce-
ment learning in zero-shot setting [15, 46], bootstrapping
from a small set of expert annotations [34], or using expert
grammars to generate synthetic training data [51].

Neuro-symbolic program synthesis. Our approach is related
to program synthesis, i.e., automatically constructing pro-
grams that satisfy a given speciﬁcation [20]. In recent years,
the usage of deep learning models for program synthesis has
resulted in signiﬁcant progress in a variety of domains in-
cluding string transformations [16, 14, 32], block-based vi-
sual programming [3, 4, 13, 47], and competitive program-
ming [25]. Program synthesis has also been used to learn
compositional symbolic rules and mimic abstract human
learning [30, 17]. Our goal is akin to program synthesis and
we leverage the work of [3] in our technique NeurSS, how-
ever, with a crucial diﬀerence: instead of synthesizing a so-
lution program, we seek to synthesize a student’s attempt.

2. PROBLEM SETUP
Next, we introduce deﬁnitions and formalize our objective.

2.1 Preliminaries
The space of tasks. We deﬁne the space of tasks as T; in
this paper, T is inspired by the popular Hour of Code: Maze
Challenge [9] from Code.org [8]; see Figures 1a and 2a. We
deﬁne a task T ∈ T as a tuple (Tvis, Tstore, Tsize), where Tvis
denotes a visual puzzle, Tstore the available block types, and
Tsize the maximum number of blocks allowed in the solu-
tion code. For instance, considering the task T in Figure 2a,
we have the following speciﬁcation: the visual puzzle Tvis
comprises of a maze where the objective is to navigate the
“avatar” (blue-colored triangle) to the “goal” (red-colored
star) by executing a code; the set of available types of blocks
Tstore is {move, turnLeft, turnRight, RepeatUntil(goal),
IfElse(pathAhead), IfElse(pathLeft), IfElse(pathRight)},
and the size threshold Tsize is 5 blocks; this particular task
in Figure 2a corresponds to Maze#18 in the Hour of Code:
Maze Challenge [9], and has been studied in a number of
prior works [35, 15, 1].

The space of codes.2 We deﬁne the space of all possible codes
as C and represent them using a Domain Speciﬁc Language
(DSL) [20]. In particular, for codes relevant to tasks con-
sidered in this paper, we use a DSL from [1]. A code C ∈ C
has the following attributes: Cblocks is the set of types of
code blocks used in C, Csize is the number of code blocks
used, and Cdepth is the depth of the Abstract Syntax Tree

2Codes are also interchangeably referred to as programs.

of C. Details of this DSL and code attributes are not cru-
cial for the readability of subsequent sections; however, they
provide useful formalism when implementing diﬀerent tech-
niques introduced in this paper.

Solution code and student attempt. For a given task T, a
T ∈ C should solve the visual puzzle; addi-
solution code C(cid:63)
tionally, it can only use the allowed types of code blocks
(i.e., Cblocks ⊆ Tstore) and should be within the speciﬁed size
threshold (i.e., Csize ≤ Tsize). We note that a task T ∈ T in
general may have multiple solution codes; in this paper, we
typically refer to a single solution code that is provided as
input. A student attempt for a task T refers to a code that is
written by a student (including incorrect or partial codes).
A student attempt could be any code C ∈ C as long as it uses
the set of available types of code blocks (i.e., Cblocks ⊆ Tstore);
importantly, it is not restricted by the size threshold Tsize—
same setting as in the programming environment of Hour of
Code: Maze Challenge [9].

2.2 Objective
Distinct phases. To formalize our objective, we introduce
three distinct phases in our problem setup that provide a
conceptual separation in terms of information and compu-
tation available to a system. More concretely, we have:

(1) Reference task Tref: We are given a reference task Tref
for which we have real-world datasets of diﬀerent stu-
dents’ attempts as well as access to other data resources.
Reference tasks are ﬁxed and the system can use any
computation a priori (e.g., compute code embeddings).

(2) Student stu attempts Tref: The system interacts with a
student, namely stu, who attempts the reference task Tref
and submits a code, denoted as Cstu
Tref . At the end of this
phase, the system has observed stu’s behavior on Tref and
we denote this observation by the tuple (Tref, Cstu

Tref ).3

(3) Target task Ttar: The system seeks to synthesize the stu-
dent stu’s behavior on a target task Ttar. Importantly,
the target task Ttar is not available a priori and this syn-
thesis process would be done in real-time, possibly with
constrained computational resources. Furthermore, the
system may have to synthesize stu’s behavior on a large
number of diﬀerent target tasks from the space T (e.g.,
to personalize the next task in a curriculum).4

Granularity level of our objective. There are several diﬀer-
ent granularity levels at which we can predict the student
stu’s behavior for Ttar, including: (a) a coarse-level binary
prediction of whether stu will successfully solve Ttar, (b) a
medium-level prediction about stu’s behavior w.r.t. a pre-
deﬁned feature set (e.g., labelled misconceptions); (c) a ﬁne-
level prediction in terms of synthesizing Cstu
Ttar , i.e., a program
that stu would write if the system would assign Ttar to the
student. In this work, we focus on this ﬁne-level, arguably
also the most challenging, synthesis objective.
3In practice, the system might have more information, e.g.,
the whole trajectory of edits leading to Cstu
Tref or access to
some prior information about the student stu.
4Even though the Hour of Code: Maze Challenge [9] has
only 20 tasks, the space T is intractably large and new tasks
can be generated automatically, e.g., when providing feed-
back or for additional practice [1].

def Run(){
move
turnLeft
move
turnRight
move

}

(a) Reference task T4 with solution code and datasets

(b) Three target tasks for T4: T4x, T4y, and T4z

def Run(){
move
turnRight
move
turnLeft
move

}

def Run(){
move
turnLeft
move

}

def Run(){
move
turnRight
turnLeft
turnRight
move
turnLeft
turnLeft
turnRight
move

}

def Run(){
move
move
move
turnLeft
move
move
move
move
turnRight
move

}

def Run(){
move
turnLeft
move
move

}

def Run(){
move
move
turnLeft
turnRight
turnRight
turnLeft
move
turnLeft
turnRight
...
(many more blocks)

}

(c) Example codes (i)–(vi) corresponding to six types of students’ behaviors when attempting T4, each capturing diﬀerent misconceptions

Figure 3: Illustration of the key elements of the StudentSyn benchmark for the reference task T4 shown in (a)—same as
in Figure 1a. (b) Shows three target tasks associated with T4; these target tasks are similar to T4 in a sense that the set
of available block types is same as T4
store and the nesting structure of programming constructs in solution codes is same as
T4 . (c) Shows example codes corresponding to six types of students’ behaviors when attempting T4, each capturing a
in C(cid:63)
diﬀerent misconception as follows: (i) confusing left/right directions when turning, (ii) partially solving the task in terms of
getting closer to the “goal”, (iii) misunderstanding of turning functionality and writing repetitive turn commands, (iv) adding
more than the correct number of required move commands, (v) forgetting to include some turns needed in the solution, (vi)
attempting to randomly solve the task by adding lots of blocks. See details in Section 3.1.

def Run(){

RepeatUntil(goal){
If(pathAhead){
move
}
Else{

turnLeft

}

}

}

(a) Reference task T18 with solution code and datasets

(b) Three target tasks for T18: T18x, T18y, and T18z

def Run(){

RepeatUntil(goal){
If(pathAhead){

move

}
Else{

turnRight

}

}

}

def Run(){

RepeatUntil(goal){
If(pathLeft){
turnLeft
move

}
Else{
move

}

}

}

def Run(){

RepeatUntil(goal){
If(pathAhead){
turnLeft

}
Else{

}
move

turnLeft

}

}

def Run(){

RepeatUntil(goal){

move
turnLeft
move
turnLeft
move

}

}

def Run(){
move
If(pathAhead){

move

}
Else{

turnLeft

}

}

def Run(){
move
turnLeft
move
move
move
move
turnRight
move
move
move
move
move

}

(c) Example codes (i)–(vi) corresponding to six types of students’ behaviors when attempting T18, each capturing diﬀerent misconceptions

Figure 4: Analogous to Figure 3, here we illustrate the key elements of the StudentSyn benchmark for the reference
task T18 shown in (a)—same as in Figure 2a. (b) Shows three target tasks associated with T18. (c) Shows example codes
corresponding to six types of students’ behaviors when attempting T18, each capturing a diﬀerent misconception as follows:
(i) confusing left/right directions when turning or checking conditionals, (ii) following one of the wrong path segments, (iii)
misunderstanding of IfElse structure functionality and writing the same blocks in both the execution branches, (iv) ignoring
the IfElse structure when solving the task, (v) ignoring the While structure when solving the task, (vi) attempting to solve
the task by using only the basic action blocks in {turnLeft, turnRight, move}. See details in Section 3.1.

Performance evaluation. So far, we have concretized the syn-
thesis objective; however, there is still a question of how
to quantitatively measure the performance of a technique
set out to achieve this objective. The key challenge stems
from the open-ended and conceptual nature of program-
ming tasks. Even for seemingly simple tasks such as in Fig-
ures 1a and 2a, the students’ attempts can be highly diverse,
thereby making it diﬃcult to detect a student’s misconcep-
tions from observed behaviors; moreover, the space of mis-

conceptions itself is not clearly understood. To this end, we
begin by designing a benchmark to quantitatively measure
the performance of diﬀerent techniques w.r.t. our objective.

3. BENCHMARK AND INITIAL RESULTS
In this section, we introduce our benchmark, StudentSyn,
and report initial results highlighting the gap in performance
of simple baselines and human experts.

Datasets forreference taskDatasets forreference task?

stu’s attempt for T18x
in Figure 2

def Run(){
move
move
turnLeft
RepeatUntil(goal){
If(pathRight){
turnRight
move

}
Else{
move

}

}

}

def Run(){
move
move
turnLeft
move
move
move
move
turnRight
move
move
move
move

}

def Run(){
move
move
turnLeft
RepeatUntil(goal){
If(pathLeft){
turnLeft
move

}
Else{
move

}

}

}

def Run(){

RepeatUntil(goal){
If(pathLeft){
turnLeft
move

}
Else{
move

}

}

}

def Run(){

RepeatUntil(goal){

move
turnLeft
move
turnRight
move

}

}

option (a)

option (b)

option (c)

option (d)

option (e)

def Run(){
move
move
turnLeft
If(pathRight){
turnRight
move

}
Else{
move

}

}

def Run(){
move
move
turnLeft
RepeatUntil(goal){
If(pathRight){

move

}
Else{
move

}
turnRight

}

}

def Run(){
move
turnLeft
move
move
move
move
move
turnRight
turnRight
turnLeft
move

}

def Run(){
turnLeft
move
move
If(pathRight){
turnRight
move

}
Else{
move

}

}

def Run(){
move
move
turnLeft
RepeatUntil(goal){

turnRight
turnLeft
turnLeft
move

}

}

option (f)

option (g)

option (h)

option (i)

option (j)

Figure 5: Illustration of the generative and discriminative objectives in the StudentSyn benchmark for the scenario shown
in Figure 2. For the generative objective, the goal is to synthesize the student stu’s behavior on the target task T18x, i.e., a
program that stu would write if the system would assign T18x to the student. For the discriminative objective, the goal is to
choose one of the ten codes, shown as options (a)–(j), that corresponds to the student stu’s attempt. For each scenario, ten
options are created systematically as discussed in Section 3.2; in this illustration, option (a) corresponds to the solution code
C∗
T18x for the target task and option (e) corresponds to the student stu’s attempt as designed in the benchmark.

3.1 STUDENTSYN: Data Curation
We begin by curating a synthetic dataset for the benchmark,
designed to capture diﬀerent scenarios of the three distinct
phases mentioned in Section 2.2. In particular, each scenario
Tref , Ttar, Cstu
corresponds to a 4-tuple (Tref, Cstu
Ttar ), where Cstu
Tref
(observed by the system) and Cstu
Ttar (to be synthesized by
the system) correspond to a student stu’s attempts.

Reference and target tasks. We select two reference tasks
for this benchmark, namely T4 and T18, as illustrated in
Figures 1a and 2a. These tasks correspond to Maze#4 and
Maze#18 in the Hour of Code: Maze Challenge [9], and have
been studied in a number of prior works [35, 15, 1], because
of the availability of large-scale datasets of students’ at-
tempts for these two tasks. For each reference task, we man-
ually create three target tasks as shown in Figures 3b and 4b;
as discussed in the ﬁgure captions, these target tasks are sim-
ilar to the corresponding reference task in a sense that the
set of available block types is same and the nesting structure
of programming constructs in solution codes is same.

Types of students’ behaviors and students’ attempts. For
a given reference-target task pair (Tref, Ttar), next we seek
to simulate a student stu to create stu’s attempts Cstu
Tref
and Cstu
Ttar . We begin by identifying a set of salient stu-
dents’ behaviors and misconceptions for reference tasks T4
and T18 based on students’ attempts observed in the real-
world dataset of [35]. In this benchmark, we select 6 types of
students’ behaviors for each reference task—these types are
highlighted in Figures 3c and 4c for T4 and T18, respectively.5

5In real-world settings, the types of students’ behaviors and
their attempts have a much larger variability and complexi-
ties with a long-tail distribution; in future work, we plan to
extend our benchmark to cover more scenarios, see Section 7.

For a given pair (Tref, Ttar), we ﬁrst simulate a student stu
by associating this student to one of the 6 types, and then
manually create stu’s attempts Cstu
Ttar . For a given
scenario (Tref, Cstu
Ttar is not ob-
served and serves as a ground truth in our benchmark for
evaluation purposes;
in the following, we interchangeably
write a scenario as (Tref, Cstu

Ttar ), the attempt Cstu

Tref , Ttar, Cstu

Tref and Cstu

Tref , Ttar, ?).

Total scenarios. We create 72 scenarios (Tref, Cstu
Ttar )
in the benchmark corresponding to (i) 2 reference tasks, (ii)
3 target tasks per reference task, (iii) 6 types of students’
behaviors per reference task, and (iv) 2 students per type.
This, in turn, leads to a total of 72 (= 2 × 3 × 6 × 2) unique
scenarios.

Tref , Ttar, Cstu

3.2 STUDENTSYN: Performance Measures
We introduce two performance measures to capture our syn-
thesis objective. Our ﬁrst measure, namely generative per-
formance, is to directly capture the quality of ﬁne-level syn-
thesis of the student stu’s attempt—this measure requires
a human-in-the-loop evaluation. To further automate the
evaluation process, we then introduce a second performance
measure, namely discriminative performance.

Generative performance. As a generative performance mea-
sure, we introduce a 4-point Likert scale to evaluate the
quality of synthesizing stu’s attempt Cstu
Ttar for a scenario
(Tref, Cstu
Tref , Ttar, ?). The scale is designed to assign scores
based on two factors: (a) whether the elements of the stu-
dent’s behavior observed in Cstu
Tref are present, (b) whether
the elements of the target task Ttar (e.g., parts of its solu-
tion) are present. More concretely, the scores are assigned as
follows (with higher scores being better): (i) Score 1 means
the technique does not have synthesis capability; (ii) Score 2

Method

Generative Performance

Discriminative Performance

Reference task Reference task Reference task Reference task

RandD
EditD
EditEmbD

TutorSS
TutorSS1
TutorSS2
TutorSS3

T4

1.00
1.00
1.00

3.85
3.89
3.72
3.94

T18

1.00
1.00
1.00

3.91
3.94
3.89
3.89

T4

10.15
30.83
42.94

89.81
91.67
91.67
86.11

T18

10.10
47.06
47.11

85.19
83.33
88.89
83.33

Table 1: This table shows initial results on StudentSyn
in terms of the generative and discriminative performance
measures. The values are in the range [1.0, 4.0] for gen-
erative performance and in the range [0.0, 100.0] for dis-
criminative performance—higher values being better. Hu-
man experts (TutorSS) can achieve high performance on
both the measures, whereas simple baselines perform poorly.
The numbers reported for TutorSS are computed by av-
eraging across three separate human experts (TutorSS1,
TutorSS2, and TutorSS3). See Section 3.3 for details.

means the synthesis fails to capture the elements of Cstu
Tref and
Ttar; (iii) Score 3 means the synthesis captures the elements
Tref or of Ttar, but not both; (iv) Score 4 means the
only of Cstu
synthesis captures the elements of both Cstu

Tref and Ttar.

Discriminative performance. As the generative performance
requires human-in-the-loop evaluation, we also introduce a
disciminative performance measure based on the prediction
accuracy of choosing the student attempt from a set. More
Tref , Ttar, ?), the discrimi-
concretely, given a scenario (Tref, Cstu
native objective is to choose Cstu
Ttar from ten candidate codes;
see Figure 5. These ten options are created automatically in
a systematic way and include the following: (a) the ground-
truth Cstu
Ttar ,
(c) ﬁve codes Cstu(cid:48)
Ttar from the benchmark associated with other
students stu(cid:48) whose behavior type is diﬀerent from stu, and
(iv) three randomly constructed codes obtained by editing
the solution code C∗

Ttar from the benchmark, (b) the solution code C(cid:63)

Ttar .

3.3 Initial Results
As a starting point, we design a few simple baselines and
compare their performance with that of human experts.

Simple baselines. The simple baselines that we develop here
are meant for the discriminative-only objective; they do not
have synthesis capability. Our ﬁrst baseline RandD simply
chooses a code from the 10 options at random. Our next two
baselines, EditD and EditEmbD, are deﬁned through a dis-
tance function DTref (C, C(cid:48)) that quantiﬁes a notion of distance
between any two codes C, C(cid:48) for a ﬁxed reference task. For a
Tref , Ttar, ?) and ten option codes, these base-
scenario (Tref, Cstu
Tref ). EditD
lines select the code C that minimizes DTref (C, Cstu
uses a tree-edit distance between Abstract Syntax Trees as
Tref . EditEmbD extends
the distance function, denoted as Dedit
EditD by considering a distance function that combines
Dedit
Tref and a code-embedding based distance function Demb
Tref ;
in this paper, we trained code embeddings with the method-
ology of [15] using a real-world dataset of student attempts
on Tref. EditEmbD then uses a distance function as a con-
vex combination (cid:0)α·Dedit
Tref (C, C(cid:48))(cid:1) where
α is optimized for each reference task separately. For mea-

Tref (C, C(cid:48))+(1−α)·Demb

suring the discriminative performance, we randomly sample
a scenario, create ten options, and measure the predictive
accuracy of the technique—the details of this experimental
evaluation are provided in Section 6.2.

Human experts. Next, we evaluate the performance of hu-
man experts on the benchmark StudentSyn, and refer to
this evaluation technique as TutorSS. These evaluations
are done through a web platform where an expert would
provide a generative or discriminative response to a given
Tref , Ttar, ?). In our work, TutorSS involved
scenario (Tref, Cstu
participation of three independent experts for the evalua-
tion; these experts have had experience in block-based pro-
gramming and tutoring. We ﬁrst carried out generative per-
formance evaluations where an expert had to write the stu-
dent attempt code; afterwards, we carried out discriminative
performance evaluations where an expert would choose one
of the options. In total, each expert participated in 36 gen-
erative evaluations (18 per reference task) and 72 discrimi-
native evaluations (36 per reference task). Results in Table 1
highlight the huge performance gap between the human ex-
perts and simple baselines; further details are provided in
Section 6.

4. NEURAL SYNTHESIZER NEURSS
Our ﬁrst technique, NeurSS (Neural Program Synthesis for
StudentSyn), is inspired by recent advances in neural pro-
gram synthesis [3, 4].
In our work, we use the neural ar-
chitecture proposed in [3]—at a high-level, the neural syn-
thesizer model takes as input a visual task T, and then se-
quentially synthesizes a code C by using programming to-
kens in Tstore. However, our goal is not simply to synthesize
a solution code, instead, we want to synthesize attempts
of a given student that the system is interacting with at
real-time/deployment. To achieve this goal, NeurSS oper-
ates in three stages as illustrated in Figure 6. Each stage
is in line with a phase of our objective described in Sec-
tion 2.2. At a high-level, the three stages of NeurSS are
as follows: (i) In Stage1, we are given a reference task and
its solution (Tref, C(cid:63)
Tref ), and train a neural synthesizer model
that can synthesize solutions for any task similar to Tref; (ii)
In Stage2, the system observes the student stu’s attempt
Cstu
Tref and initiates continual training of the neural synthe-
sizer model from Stage1 in real-time; (iii) In Stage3, the
system considers a target task Ttar and uses the model from
Stage2 to synthesize Cstu
Ttar . In the following paragraphs, we
provide an overview of the key ideas and high-level imple-
mentation details for each stage.

NEURSS-Stage1.i. Given a reference task and its solution
(Tref, C(cid:63)
Tref ), the goal of this stage is to train a neural synthe-
sizer model that can synthesize solutions for any task similar
to Tref. In this stage, we use a synthetic dataset Dtasks
com-
prising of task-solution pairs (T, C(cid:63)
T ); the notion of similarity
here means that Tstore is the same as Tref
store and the nesting
structure of programming constructs in C(cid:63)
T is the same as in
C(cid:63)
Tref . To train this synthesizer, we leverage recent advances
in neural program synthesis [3, 4]; in particular, we use the
encoder-decoder architecture and imitation learning proce-
dure from [3]. The model we use in our experiments has
deep-CNN layers for extracting task features and an LSTM
for sequentially generating programming tokens. The input
to the synthesizer is a one-hot task representation of the vi-

Tref

Figure 6: Illustration of the three diﬀerent stages in NeurSS, our technique based on neural synthesis; details in Section 4.

sual grid denoting diﬀerent elements of the grid (e.g., “goal”,
“walls”, and position/orientation of the “avatar”), as well as
the programming tokens synthesized by the model so far.
To generate the synthetic dataset Dtasks
, we use the task
generation procedure from [1]. For the reference task T4, we
of size 50, 000; for the reference task T18,
generated Dtasks
we generated Dtasks

of size 200, 000.

Tref

T4

T18

NEURSS-Stage1.ii. Given a reference task Tref, the goal of
this stage is to train a code embedding network that maps
an input code C to a feature vector φ(C). This code em-
bedding space will be useful later in NEURSS-Stage2 when
we observe the student stu’s attempt. For each Tref, we use
a real-world dataset of students’ attempts Dattempts
on Tref
to train this embedding network using the methodology of
[15]. To train this embedding network, we construct a set
with triplets (C, C(cid:48), Dedit
and
Dedit
Tref computes the tree-edit distance between Abstract Syn-
tax Trees of two codes (see Section 3.3). The embedding
network is trained so the embedding space preserves given
distances, i.e., ||φ(C) − φ(C(cid:48))|| ≈ Dedit
Tref (C, C(cid:48)) for a triplet.
Following the setup in [15], we use a bidirectional LSTM
architecture for the network and use R80 embedding space.

Tref (C, C(cid:48))) where C, C(cid:48) ∈ Dattempts

Tref

Tref

NEURSS-Stage2. In this stage, the system observes the stu-
dent stu’s attempt Cstu
Tref and initiates continual training of
the neural synthesizer model from Stage1.i in real-time. More
concretely, we ﬁne-tune the pre-trained synthesizer model
from Stage 1.i with the goal of transferring the student stu’s
behavior from the reference task Tref to any target task Ttar.
Here, we make use of the embedding network from Stage1.ii
that enables us to ﬁnd neighboring codes C ∈ Dattempts
such
that φ(C) is close to φ(Cstu
Tref ). More formally, the set of neigh-
bors is given by {C ∈ Dattempts
Tref ) − φ(C)||2 ≤ r}
where the threshold r is a hyperparameter. Next, we use
these neighboring codes to create a small dataset for contin-
ual training: this dataset comprises of the task-code pairs
(C, Tref) where C is a neighboring code for Cstu
Tref and Tref is
the reference task. There are two crucial ideas behind the
design of this stage. First, we do this continual training
using a set of neighboring codes w.r.t. Cstu
Tref instead of just

: ||φ(Cstu

Tref

Tref

using Cstu
Tref —this is important to avoid overﬁtting during the
process. Second, during this continual training, we train
for a small number of epochs (a hyperparameter), and only
ﬁne-tune the decoder by freezing the encoder—this is impor-
tant so that the network obtained after continual training
still maintains its synthesis capability. The hyperparame-
ters in this stage (threshold r, the number of epochs and
learning rate) are obtained through cross-validation in our
experiments (see Section 6.2)

NEURSS-Stage3. In this stage, the system observes Ttar and
uses the model from Stage2 to synthesize Cstu
Ttar . More con-
cretely, we provide Ttar as an input to the Stage2 model
and then synthesize a small set of codes as outputs using
a beam search procedure proposed in [3]. This procedure
allows us to output codes that have high likelihood or prob-
ability of synthesis with the model. In our experiments, we
use a beam size of 64; Figures 9e and 10e illustrate Top-3
synthesized codes for diﬀerent scenarios obtained through
this procedure. The Top-1 code is then used for generative
performance evaluation. For the discriminative performance
evaluation, we are given a set of option codes; here we use
the model of Stage2 to compute the likelihood of provided
options and then select one with the highest probability.

5. SYMBOLIC SYNTHESIZER SYMSS
In the previous section, we introduced NeurSS inspired by
neural program synthesis. NeurSS additionally has syn-
thesis capability in comparison to the simple baselines in-
troduced earlier; yet, there is a substantial gap in the per-
formance of NeurSS and human experts (i.e., TutorSS).
An important question that we seek to resolve is how much
of this performance gap can be reduced by leveraging do-
main knowledge such as how students with diﬀerent behav-
iors (misconceptions) write codes. To this end, we introduce
our second technique, SymSS (Symbolic Program Synthesis
for StudentSyn), inspired by recent advances in using sym-
bolic methods for program synthesis [24, 51, 26]. Similar in
spirit to NeurSS, SymSS operates in three stages as illus-
trated in Figure 7. Each stage is in line with a phase of our
objective described in Section 2.2. At a high-level, the three

T"#$,M’"(NEURSS-Stage2: Continual training at deploymentNEURSS-Stage3: Student attempt synthesis at deploymentNEURSS-Stage1.i: Training a solution synthesizer networkNEURSS-Stage1.ii: Training a code embedding networkC𝜙(C)TC-⋆Inputs:§Reference task and solution T$/0,C-123⋆§Synthetic dataset 𝒟-123"#’5’	of tasks and solutions T,C-⋆	s.t.Tis similar toT$/0Computation:§Train a solution synthesizer networkInputs:§Real-world dataset 𝒟-123#""/78"’of different students’ attempts Cfor T$/0Computation:§Train a code embedding networkInputs:§Student attempt C-123’"(of student stuComputation:§Find neighboring codes C∈𝒟-123#""/78"’s.t. 𝜙(C)is close to 𝜙(C-123’"()§Continual training of Stage1.i networkT$/0CInputs:§Target task T"#$Computation:§Use Stage2 network to synthesize the      attempt C-=>1’"(of student stu	for T"#$T"#$C-=>1’"(gStart:=

p1−→ gR gM gL gM gM gM gL gM
p2−→ gR gM gL gM gM gM gM
p2−→ gR gM gM gM gM gL gM
p2−→ gM gL gM gM gM gL gM
p3−→ gR gM gM gM gM gM
p3−→ gM gL gM gM gM gM
p3−→ gM gM gM gM gL gM
p4−→ gM gM gM gM gM

T4x for T4x is {Run {turnRight; move;
The solution code C(cid:63)
turnLeft; move; move; move; turnLeft; move}}. These rules
for gStart are speciﬁc to the behavior type Mstu that cor-
responds to forgetting to include some turns in the solution
and are created automatically w.r.t. C(cid:63)

T4x .

gM:=

gL:=

gR:=

p5−→ gRepM gRepM
p6−→ move
p5−→ turnLeft
p5−→ turnRight
p5−→ gRepL gRepL
p5−→ move
p6−→ turnLeft
p5−→ turnRight
p5−→ gRepR gRepR
p5−→ move
p5−→ turnLeft
p6−→ turnRight

gRepM:=

gRepL:=

gRepR:=

p7−→ gRepM gRepM
p7−→ move
p8−→ turnLeft
p8−→ turnRight
p7−→ gRepL gRepL
p8−→ move
p7−→ turnLeft
p8−→ turnRight
p7−→ gRepR gRepR
p8−→ move
p8−→ turnLeft
p7−→ turnRight

These rules are speciﬁc to the behavior type Mstu and inde-
pendent of the task and solution code (T, C(cid:63)

T ).

Figure 8: PCFG corresponding to GT4 (T4x, C(cid:63)
T4x , Mstu)—this
PCFG is automatically created in SYMSS-Stage3 for the sce-
nario in Figure 1 and is used to synthesize student stu’s at-
tempt. We deﬁne the following symbols: (i) the start sym-
bol gStart; (ii) the non-terminal symbols gM, gL, gR, gRepM,
gRepL, gRepR; (iii) the terminal symbols move, turnLeft,
turnRight. For this scenario, the behavior type Mstu corre-
sponds to forgetting to include some turns in the solution.
The production rules for gStart are speciﬁc to the behavior
type Mstu and are automatically created w.r.t.
the solu-
tion code C(cid:63)
include all
T4x , leading to the following rules:
the turns (p1), omit one turn (p2), omit two turns (p3), or
omit all of the three turns (p4). We designed the proba-
bility of keeping a turn as pk = 1/3 and omitting a turn
as po = 1 − pk = 2/3; this leads to the following probabil-
ity values for gStart’s production rules: {p1 = p3
k, p2 =
p2
k · po, p3 = pk · p2
o}. The production rules for gM,
gL, gR, gRepM, gRepL, gRepR are speciﬁc to the behavior type
Mstu and independent of the task and solution code (T, C(cid:63)
T )
in GT4 (T, C(cid:63)
T , Mstu). The symbols gRepM, gRepL, gRepR are in-
troduced to allow variability in the students’ attempts; intu-
itively stu may further diverge from the behavior type by (i)
transforming tokens, (ii) replicating tokens, or (iii) adding a
sequence of tokens. For these rules, the probabilities values
are deﬁned as: {p5 = 0.1, p6 = 0.7, p7 = 0.4, p8 = 0.1}.

o, p4 = p3

bolic synthesizer GTref (T, C(cid:63)
T , M) is designed as a grammar
creator module:
internally, it ﬁrst automatically creates a
speciﬁc grammar corresponding to its input arguments and
then generates codes based on this grammar.

Figure 7: Illustration of the three diﬀerent stages in SymSS,
our technique based on symbolic synthesis; see Section 5.

stages of SymSS are as follows: (i) In Stage1, we are given
(Tref, C(cid:63)
Tref ), and design a symbolic synthesizer model using
Probabilistic Context Free Grammars (PCFG)s to encode
how students of diﬀerent behavior types M write codes for
any task similar to Tref [5, 27, 51]; (ii) In Stage2, the system
observes the student stu’s attempt Cstu
Tref and makes a predic-
tion about the behavior type Mstu ∈ M; (iii) In Stage3, the
system considers a target task Ttar and uses the model from
Stage1 to synthesize Cstu
In
the following paragraphs, we provide an overview of the key
ideas and high-level implementation details for each stage.

Ttar based on the inferred Mstu.

SYMSS-Stage1 (High-level design). For a given reference task
and its solution (Tref, C(cid:63)
Tref ), the goal of this stage is to create
a symbolic program synthesizer that encodes domain knowl-
edge. In this stage, we need access to the following: (i) a
set of types of students’ behaviors the system is expected to
encounter at deployment, denoted by M in Figure 7; (ii) an
expert with domain knowledge.6 The expert then designs
a symbolic program synthesizer GTref (T, C(cid:63)
T , M) for the refer-
ence task Tref that operates as follows: (a) as ﬁrst input,
it takes a task-solution pair (T, C(cid:63)
T ) where T is expected to
be similar to Tref; (b) as second input, it takes a type of
student behavior M ∈ M; (c) as outputs, it synthesizes a
code C along with the probability p of synthesis. This sym-

6We note that SymSS is the only technique that requires
access to the types of students’ behaviors; in our implemen-
tation and experiments, we considered M to be the same as
the types of students’ behaviors in StudentSyn. In prac-
tice, there could potentially be a large number of types of
behaviors, that manifest in students’ attempts in an un-
known way; hence, SymSS in a real-world setting could per-
form worse than the performance reported on StudentSyn.
Also, we note that human experts in TutorSS were not told
about the types of students’ behaviors in StudentSyn.

T"#$,M’"(SYMSS-Stage2: Predict misconception type at deploymentSYMSS-Stage3: Student attempt synthesis at deploymentSYMSS-Stage1: Expert designs a symbolic synthesizerInputs:§Reference task and solution T$)*,C,-./⋆§Set ℳ	of misconception typesComputation:§Expert designs a symbolic synthesizer 𝒢,-./	§Given a similar T,C,⋆and M∈ℳ, 𝒢,-./	synthesizes an attempt Cwith probability 𝑝Inputs:§Student attempt C,-./’"(of student stuComputation:§Predict M’"(as M∈ℳ	with highest probability 𝑝C,-./’"(	|	MInputs:§Target task and solution (T"#$,C,;<-⋆)§Predicted type M’"(from Stage2Computation:§Synthesize student attempt C,;<-’"(of student stu	for T"#$T,C,⋆C𝒢,-./	M𝑝T$)*,C,-./⋆C,-./’"(𝒢,-./	M𝑝T"#$,C,;<-⋆C,;<-’"(𝒢,-./	M’"(𝑝Method

Generative Performance

Discriminative Performance

Required Inputs and Domain Knowledge

Reference task Reference task Reference task Reference task Ref. task dataset: Ref. task dataset: Student Expert

Expert

RandD
EditD
EditEmbD

NeurSS
SymSS

TutorSS

T4

1.00
1.00
1.00

3.28
3.72

3.85

T18

1.00
1.00
1.00

2.94
3.83

3.91

T4

T18

student attempts

similar tasks

types

grammars evaluation

10.15 ± 0.2
30.83 ± 1.1
42.94 ± 2.1

40.10 ± 0.7
87.17 ± 0.7

10.10 ± 0.2
47.06 ± 0.3
47.11 ± 0.8

55.98 ± 1.5
67.83 ± 1.0

89.81 ± 1.9

85.19 ± 1.9

-
-
(cid:55)

(cid:55)
-

-

-
-
-
(cid:55)
-

-

-
-
-

-
(cid:55)

-

-
-
-

-
(cid:55)

-

-
-
-

-
-
(cid:55)

Table 2: This table expands on Table 1 and additionally provides results for NeurSS and SymSS. The columns under
“Required Inputs and Domain Knowledge” highlight information used by diﬀerent techniques ((cid:55) indicates the usage of the
corresponding input/knowledge). NeurSS and SymSS signiﬁcantly improve upon the simple baselines introduced in Sec-
tion 3.3; yet, there is a gap in performance in comparison to that of human experts. See Section 6 for details.

SYMSS-Stage1 (PCFG). Inspired by recent work on model-
ing students’ misconceptions via Probabilistic Context Free
Grammars (PCFG)s [51], we consider a PCFG family of
grammars inside GTref .7 More concretely, given a reference
task Tref, a task-solution pair (T, C(cid:63)
T ), and a type M, the
expert has designed an automated function that creates a
PCFG corresponding to GTref (T, C(cid:63)
T , M) which is then used to
sample/synthesize codes. This PCFG is created automati-
cally and the production rules are based on: the type M, the
input solution code C(cid:63)
T , and optionally features of T. In our
implementation, we designed two separate symbolic synthe-
sizers GT4 and GT18 associated with two reference tasks. As
a concrete example, consider the scenario in Figure 1: the
PCFG created internally at SymSS-Stage3 corresponds to
GT4 (T4x, C(cid:63)
T4x , Mstu) and is illustrated in Figure 8; details are
provided in the caption and as comments within the ﬁgure.

SYMSS-Stage2. In this stage, the system observes the stu-
dent stu’s attempt Cstu
Tref and makes a prediction about the
behavior type Mstu ∈ M. For each behavior type M ∈ M
speciﬁed at Stage1, we use GTref with arguments (Tref, C(cid:63)
ref, M)
to calculate the probability of synthesizing Cstu
Tref w.r.t. M, re-
ferred to as p(Cstu
Tref |M). This is done by internally creating a
corresponding PCFG for GTref (Tref, C(cid:63)
ref, M). To predict Mstu,
we pick the behavior type M with the highest probability.
As an implementation detail, we construct PCFGs in a spe-
cial form called the Chmosky Normal Form (CNF) [5, 27]
(though the PCFG illustrated in Figure 8 is not in CNF).
This form imposes constraints to the grammar rules that
add extra diﬃculty in grammar creation, but enables the
eﬃcient calculation of p(Cstu
Tref |M).

Ttar , Mstu) to synthesize Cstu

SYMSS-Stage3. In this stage, the system observes a target
task Ttar along with its solution C(cid:63)
Ttar . Based on the behavior
type Mstu inferred in Stage2, it uses GTref with input argu-
ments (Ttar, C(cid:63)
Ttar . More concretely,
we use GTref (Ttar, C(cid:63)
Ttar , Mstu) to synthesize a large set of codes
as outputs along with probabilities.
In our implementa-
tion, we further normalize these probabilities appropriately
by considering the number of production rules involved. In
our experiments, we sample a set of 1000 codes and keep the

7Context Free Grammars (CFG)s generate strings by apply-
ing a set of production rules where each symbol is expanded
independently of its context [27]. These rules are deﬁned
through a start symbol, non-terminal symbols, and termi-
nal symbols. PCFGs additionally assign a probability to
each production rule; see Figure 8 as an example.

codes with highest probabilities; Figures 9f and 10f illustrate
the Top-3 synthesized codes for two scenarios, obtained with
this procedure. The Top-1 code is then used for generative
performance evaluation. For the discriminative performance
evaluation, we are already given a set of option codes; here
we directly compute the likelihood of the provided options
and then select one with the highest probability.

6. EXPERIMENTAL EVALUATION
In this section, we expand on the evaluation presented in
Section 3 and include results for NeurSS and SymSS.

6.1 Generative Performance
Evaluation procedure. As discussed in Section 3.2, we eval-
uate the generative performance of a technique in the fol-
lowing steps: (a) a scenario (Tref, Cstu
Tref , Ttar, ?) is picked; (b)
the technique synthesizes stu’s attempt, i.e., a program that
stu would write if the system would assign Ttar to the stu-
dent; (c) the generated code is scored on the 4-point Likert
scale. The scoring step requires human-in-the-loop evalua-
tion and involved an expert (diﬀerent from the three experts
that are part of TutorSS). Overall, each technique is eval-
uated for 36 unique scenarios in StudentSyn—we selected
18 scenarios per reference task by ﬁrst picking one of the 3
target tasks and then picking a student from one of the 6
diﬀerent types of behavior. The ﬁnal performance results in
Table 2 are reported as an average across these scenarios;
for TutorSS, each of the three experts independently re-
sponded to these 36 scenarios and the ﬁnal performance is
computed as a macro-average across experts.

Quantitative results. Table 2 expands on Table 1 and reports
results on the generative performance per reference task for
diﬀerent techniques. As noted in Section 3.3, the simple
baselines (RandD, EditD, EditEmbD) do not have a syn-
thesis capability and hence have a score 1.00. TutorSS,
i.e., human experts, achieves the highest performance with
aggregated scores of 3.85 and 3.91 for two reference tasks
respectively; as mentioned in Table 1, these scores are re-
ported as an average over scores achieved by three diﬀerent
experts. SymSS also achieves high performance with ag-
gregated scores of 3.72 and 3.83—only slightly lower than
that of TutorSS and these gaps are not statistically signif-
icant w.r.t. χ2 tests [6]. The high performance of SymSS
is expected given its knowledge about types of students in
StudentSyn and the expert domain knowledge inherent in
its design. NeurSS improves upon simple baselines and

?

def Run(){
turnRight
move
turnLeft
move
move
move
turnLeft
move

}

def Run(){
turnRight
move
turnLeft
move
move
move
move

}

def Run(){
turnRight
move
move
move
move
turnLeft
move

}

(a) Attempt Cstu
T4x

(b) Solution C(cid:63)

T4x

(c) Benchmark code

(d) TutorSS

def Run(){
turnRight
move
move
turnLeft
move
move

}

def Run(){
turnRight
move
move
move
turnLeft
move

}

def Run(){
turnRight
move
turnLeft
move
move
move

}

def Run(){
move
move
move
move
move

}

def Run(){
move
move
move
move
turnLeft
move

}

def Run(){
turnRight
move
move
move
move
move

}

(e) NeurSS – Top-3 synthesized codes in decreasing likelihood

(f) SymSS – Top-3 synthesized codes in decreasing likelihood

Figure 9: Illustration of the qualitative results in terms of the generative objective for the scenario in Figure 1. (a) The
goal is to synthesize the student stu’s behavior on the target task T4x. (b) Solution code C(cid:63)
T4x for the target task. (c) Code
provided in the benchmark as a possible answer for this scenario. (d) Code provided by one of the human experts. (e, f ) Codes
synthesized by our techniques NeurSS and SymSS—Top-3 synthesized codes in decreasing likelihood are provided here. See
Section 6.1 for details.

?

def Run(){
move
move
turnLeft
RepeatUntil(goal){
If(pathRight){
turnRight
move
}
Else{
move
}

}

}

def Run(){

RepeatUntil(goal){

move
turnLeft
move
turnRight
move

}

}

def Run(){

RepeatUntil(goal){

move
move
turnLeft
move
move
turnRight
move
move

}

}

(a) Attempt Cstu
T18x

(b) Solution C(cid:63)

T18x

(c) Benchmark code

(d) TutorSS

def Run(){

def Run(){

def Run(){

RepeatUntil(goal){

RepeatUntil(goal){

RepeatUntil(goal){

move
turnLeft
move
turnLeft
move

}

}

move
turnLeft
move

}

}

move
turnLeft
move
turnLeft

}

}

def Run(){
move
move
turnLeft
RepeatUntil(goal){

def Run(){
move
move
turnLeft
RepeatUntil(goal){

def Run(){
move
move
turnLeft
RepeatUntil(goal){

turnRight
move
move

}

}

move
turnRight
move
move

}

}

turnRight
move
move
move

}

}

(e) NeurSS – Top-3 synthesized codes in decreasing likelihood

(f) SymSS – Top-3 synthesized codes in decreasing likelihood

Figure 10: Analogous to Figure 9, here we illustrate results in terms of the generative objective for the scenario in Figure 2.

achieves aggregated scores of 3.28 and 2.94; however, this
performance is signiﬁcantly worse (p ≤ 0.001) compared to
that of SymSS and TutorSS w.r.t. χ2 tests.8

the Top-3 codes synthesized by NeurSS in Figure 10e only
capture the elements of the student’s behavior in Cstu
Tref and
miss the elements of the target task Ttar.

Qualitative results. Figures 9 and 10 illustrate the quali-
tative results in terms of the generative objective for the
scenarios in Figures 1 and 2, respectively. As can be seen
in Figures 9d and 10d, the codes generated by human ex-
perts in TutorSS are high-scoring w.r.t. our 4-point Likert
scale, and are slight variations of the ground-truth codes in
StudentSyn shown in Figures 9c and 10c. Figures 9f and 10f
show the Top-3 codes synthesized by SymSS for these two
scenarios – these codes are also high-scoring w.r.t. our 4-
point Likert scale. In contrast, for the scenario in Figure 2,

8χ2 tests reported here are conducted based on aggregated
data across both the reference tasks.

6.2 Discriminative Performance
Evaluation procedure: Creating instances. As discussed in
Section 3.2, we evaluate the discriminative performance of
a technique in the following steps: (a) a discriminative in-
Tref , Ttar, ?) picked
stance is created with a scenario (Tref, Cstu
from the benchmark and 10 code options created automati-
cally; (b) the technique chooses one of the options as stu’s
attempt; (c) the chosen option is scored either 100.0 when
correct, or 0.0 otherwise. We create a number of discrimina-
tive instances for evaluation, and then compute an average
predictive accuracy in the range [0.0, 100.0]. We note that
the number of discriminative instances can be much larger
than the number of scenarios because of the variability in

creating 10 code options. When sampling large number of
instances in our experiments, we ensure that all target tasks
and behavior types are represented equally.

for programming tasks. We believe that the benchmark will
facilitate further research in this crucial area of student mod-
eling for block-based visual programming environments.

Evaluation procedure: Details about ﬁnal performance. For
TutorSS, we perform evaluation on a small set of 72 in-
stances (36 instances per reference task), to reduce the ef-
fort for human experts. The ﬁnal performance results for
TutorSS in Table 2 are reported as an average predictive
accuracy across the evaluated instances—each of the three
experts independently responded to the instances and the
ﬁnal performance is computed as a macro-average across ex-
perts. Next, we provide details on how the ﬁnal performance
results are computed for the techniques RandD, EditD,
EditEmbD, NeurSS, and SymSS. For these techniques,
we perform numEval = 5 independent evaluation rounds,
and report results as a macro-average across these rounds;
these rounds are also used for statistical signiﬁcance tests.
Within one round, we create a set of 720 instances (360 in-
stances per reference task). To allow hyperparameter tuning
by techniques, we apply a cross-validation procedure on the
360 instances per reference task by creating 10-folds whereby
1 fold is used to tune hyperparameters and 9 folds are used to
measure performance. Within a round, the performance re-
sults are computed as an average predictive accuracy across
the evaluated instances.

Quantitative results. Table 2 reports results on the discrim-
inative performance per reference task for diﬀerent tech-
niques. As noted in Section 3.3, the initial results showed a
huge gap between the human experts (TutorSS) and sim-
ple baselines (RandD, EditD, EditEmbD). As can be seen
in Table 2, our proposed techniques (NeurSS and SymSS)
have reduced this performance gap w.r.t. TutorSS. SymSS
achieves high performance compared to simple baselines and
NeurSS; moreover, on the reference task T4,
its perfor-
mance (87.17) is close to that of TutorSS (89.81). The
high performance of SymSS is partly due to its access to
types of students in StudentSyn; in fact, this information
is used only by SymSS and is not even available to human
experts in TutorSS—see column “Student types” in Ta-
ble 2. NeurSS outperformed simple baselines on the ref-
erence task T18; however, its performance is below SymSS
and TutorSS for both the reference tasks. For the three
techniques NeurSS, SymSS, and EditEmbD, we did sta-
tistical signiﬁcance tests based on results from numEval = 5
independent rounds w.r.t. Tukey’s HSD test [48], and ob-
tained the following: (a) the performance of NeurSS is sig-
niﬁcantly better than EditEmbD on the reference task T18
(p ≤ 0.001); (b) the performance of SymSS is signiﬁcantly
better than NeurSS and EditEmbD on both the reference
tasks (p ≤ 0.001).

7. CONCLUSIONS AND OUTLOOK
We investigated student modeling in the context of block-
based visual programming environments, focusing on the
ability to automatically infer students’ misconceptions and
synthesize their expected behavior. We introduced a novel
benchmark, StudentSyn, to objectively measure the gen-
erative as well as the discriminative performance of diﬀer-
ent techniques. The gap in performance between human
experts (TutorSS) and our techniques (NeurSS, SymSS)
highlights the challenges in synthesizing student attempts

There are several important directions for future work, in-
cluding but not limited to: (a) incorporating more diverse
tasks and student misconceptions in the benchmark; (b)
scaling up the benchmark and creating a competition with
a public leaderboard to facilitate research; (c) developing
new neuro-symbolic synthesis techniques that can get close
to the performance of TutorSS without relying on expert
inputs; (d) applying our methodology to other programming
environments (e.g., Python programming).

8. ACKNOWLEDGMENTS
This work was supported in part by the European Research
Council (ERC) under the Horizon Europe programme (ERC
StG, grant agreement No. 101039090).

9. REFERENCES
[1] U. Z. Ahmed, M. Christakis, A. Efremov,

N. Fernandez, A. Ghosh, A. Roychoudhury, and
A. Singla. Synthesizing Tasks for Block-based
Programming. In NeurIPS, 2020.

[2] F. Ai, Y. Chen, Y. Guo, Y. Zhao, Z. Wang, G. Fu,
and G. Wang. Concept-Aware Deep Knowledge
Tracing and Exercise Recommendation in an Online
Learning System. In EDM, 2019.

[3] R. Bunel, M. J. Hausknecht, J. Devlin, R. Singh, and
P. Kohli. Leveraging Grammar and Reinforcement
Learning for Neural Program Synthesis. In ICLR,
2018.

[4] X. Chen, C. Liu, and D. Song. Execution-Guided

Neural Program Synthesis. In ICLR, 2019.
[5] N. Chomsky. On Certain Formal Properties of

Grammars. Information and control, 2:137–167, 1959.
[6] W. G. Cochran. The χ2 Test of Goodness of Fit. The
Annals of Mathematical Statistics, pages 315–345,
1952.

[7] J. Cock, M. Marras, C. Giang, and T. K¨aser. Early

Prediction of Conceptual Understanding in Interactive
Simulations. In EDM, 2021.

[8] Code.org. Code.org – Learn Computer Science.

https://code.org/.

[9] Code.org. Hour of Code – Classic Maze Challenge.

https://studio.code.org/s/hourofcode.

[10] Code.org. Hour of Code Initiative.
https://hourofcode.com/.

[11] A. T. Corbett and J. R. Anderson. Knowledge

Tracing: Modeling the Acquisition of Procedural
Knowledge. User Modeling and User-Adapted
Interaction, 4(4):253–278, 1994.

[12] A. T. Corbett, M. McLaughlin, and K. C. Scarpinatto.
Modeling Student Knowledge: Cognitive Tutors in
High School and College. User Model. User Adapt.
Interact., 2000.

[13] J. Devlin, R. Bunel, R. Singh, M. J. Hausknecht, and

P. Kohli. Neural Program Meta-Induction. In
NeurIPS, 2017.

[14] J. Devlin, J. Uesato, S. Bhupatiraju, R. Singh,
A. Mohamed, and P. Kohli. Robustﬁll: Neural

Program Learning under Noisy I/O. In D. Precup and
Y. W. Teh, editors, ICML, 2017.

[15] A. Efremov, A. Ghosh, and A. Singla. Zero-shot

Learning of Hint Policy via Reinforcement Learning
and Program Synthesis. In EDM, 2020.

[16] K. Ellis, M. I. Nye, Y. Pu, F. Sosa, J. Tenenbaum,
and A. Solar-Lezama. Write, Execute, Assess:
Program Synthesis with a REPL. In NeurIPS, 2019.

[17] K. Ellis, C. Wong, M. I. Nye, M. Sabl´e-Meyer, L. Cary,

L. Morales, L. B. Hewitt, A. Solar-Lezama, and J. B.
Tenenbaum. Dreamcoder: Growing Generalizable,
Interpretable Knowledge with Wake-Sleep Bayesian
Program Learning. CoRR, abs/2006.08381, 2020.
[18] A. Emerson, A. Smith, F. J. Rodr´ıguez, E. N. Wiebe,

B. W. Mott, K. E. Boyer, and J. C. Lester.
Cluster-Based Analysis of Novice Coding
Misconceptions in Block-Based Programming. In
SIGCSE, 2020.

[19] A. Ghosh, S. Tschiatschek, S. Devlin, and A. Singla.

Adaptive Scaﬀolding in Block-based Programming via
Synthesizing New Tasks as Pop Quizzes. In AIED,
2022.

[20] S. Gulwani, O. Polozov, and R. Singh. Program

Synthesis. Foundations and Trends® in Programming
Languages, 2017.

[21] J. He-Yueya and A. Singla. Quizzing Policy Using
Reinforcement Learning for Inferring the Student
Knowledge State. In EDM, 2021.

[22] A. Hunziker, Y. Chen, O. M. Aodha, M. G.

Rodriguez, A. Krause, P. Perona, Y. Yue, and
A. Singla. Teaching Multiple Concepts to a Forgetful
Learner. In NeurIPS, 2019.

[23] T. K¨aser and D. L. Schwartz. Modeling and Analyzing

Inquiry Strategies in Open-Ended Learning
Environments. Journal of AIED, 30(3):504–535, 2020.
[24] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum.
Human-level Concept Learning through Probabilistic
Program Induction. Science, 2015.
[25] Y. Li, D. Choi, J. Chung, N. Kushman,

J. Schrittwieser, R. Leblond, J. Keeling, F. Gimeno,
A. D. Lago, T. Hubert, P. Choy, and C. de.
Competition-Level Code Generation with AlphaCode.
2022.

[26] A. Malik, M. Wu, V. Vasavada, J. Song, M. Coots,

J. Mitchell, N. D. Goodman, and C. Piech. Generative
Grading: Near Human-level Accuracy for Automated
Feedback on Richly Structured Problems. In EDM,
2021.

[27] J. C. Martin. Introduction to Languages and the

Theory of Computation, volume 4. McGraw-Hill NY,
1991.

[28] R. McIlroy-Young, S. Sen, J. M. Kleinberg, and

A. Anderson. Aligning Superhuman AI with Human
Behavior: Chess as a Model System. In KDD, 2020.

[29] R. McIlroy-Young and R. Wang. Detecting Individual

Decision-Making Style: Exploring Behavioral
Stylometry in Chess. In NeurIPS, 2021.

[30] M. I. Nye, A. Solar-Lezama, J. Tenenbaum, and B. M.

Lake. Learning Compositional Rules via Neural
Program Synthesis. In NeurIPS, 2020.

[31] B. Paaßen, B. Hammer, T. W. Price, T. Barnes,

S. Gross, and N. Pinkwart. The Continuous Hint
Factory - Providing Hints in Continuous and Inﬁnite
Spaces. Journal of Educational Data Mining, 2018.

[32] E. Parisotto, A. Mohamed, R. Singh, L. Li, D. Zhou,
and P. Kohli. Neuro-Symbolic Program Synthesis. In
ICLR, 2017.

[33] C. Piech, J. Bassen, J. Huang, S. Ganguli, M. Sahami,

L. J. Guibas, and J. Sohl-Dickstein. Deep Knowledge
Tracing. In NeurIPS, pages 505–513, 2015.

[34] C. Piech, J. Huang, A. Nguyen, M. Phulsuksombati,
M. Sahami, and L. J. Guibas. Learning Program
Embeddings to Propagate Feedback on Student Code.
In ICML, 2015.

[35] C. Piech, M. Sahami, J. Huang, and L. J. Guibas.

Autonomously Generating Hints by Inferring Problem
Solving Policies. In L@S, 2015.

[36] L. Portnoﬀ, E. N. Gustafson, K. Bicknell, and
J. Rollinson. Methods for Language Learning
Assessment at Scale: Duolingo Case Study. In EDM,
2021.

[37] T. W. Price and T. Barnes. Position paper:

Block-based Programming Should Oﬀer Intelligent
Support for Learners. In 2017 IEEE Blocks and
Beyond Workshop (B B), 2017.

[38] T. W. Price, Y. Dong, and D. Lipovac. iSnap:

Towards Intelligent Tutoring in Novice Programming
Environments. In SIGCSE, pages 483–488, 2017.
[39] T. W. Price, R. Zhi, and T. Barnes. Evaluation of a
Data-driven Feedback Algorithm for Open-ended
Programming. EDM, 2017.

[40] A. N. Raﬀerty, R. Jansen, and T. L. Griﬃths. Using

Inverse Planning for Personalized Feedback. In EDM,
2016.

[41] M. Resnick, J. Maloney, A. Monroy-Hern´andez,
N. Rusk, E. Eastmond, K. Brennan, A. Millner,
E. Rosenbaum, J. Silver, B. Silverman, et al. Scratch:
Programming for All. Communications of the ACM,
2009.

[42] B. Settles and B. Meeder. A Trainable Spaced

Repetition Model for Language Learning. In ACL,
2016.

[43] A. Shakya, V. Rus, and D. Venugopal. Student
Strategy Prediction using a Neuro-Symbolic
Approach. EDM, 2021.

[44] Y. Shi, K. Shah, W. Wang, S. Marwan, P. Penmetsa,

and T. W. Price. Toward Semi-Automatic
Misconception Discovery Using Code Embeddings. In
LAK, 2021.

[45] R. Singh, S. Gulwani, and A. Solar-Lezama.

Automated Feedback Generation for Introductory
Programming Assignments. In PLDI, pages 15–26,
2013.

[46] A. Singla, A. N. Raﬀerty, G. Radanovic, and N. T.
Heﬀernan. Reinforcement Learning for Education:
Opportunities and Challenges. CoRR, abs/2107.08828,
2021.

[47] D. Trivedi, J. Zhang, S. Sun, and J. J. Lim. Learning

to Synthesize Programs as Interpretable and
Generalizable policies. CoRR, abs/2108.13643, 2021.

[48] J. W. Tukey. Comparing Individual Means in the

Analysis of Variance. Biometrics, 5 2:99–114, 1949.

[49] L. Wang, A. Sy, L. Liu, and C. Piech. Learning to
Represent Student Knowledge on Programming
Exercises Using Deep Learning. In EDM, 2017.

[50] D. Weintrop and U. Wilensky. Comparing Block-based

and Text-based Programming in High School
Computer Science Classrooms. ACM Transactions of
Computing Education, 18(1):1–25, 2017.

[51] M. Wu, M. Mosse, N. D. Goodman, and C. Piech. Zero

Shot Learning for Code Education: Rubric Sampling
with Deep Learning Inference. In AAAI, 2019.

[52] J. Yi, U. Z. Ahmed, A. Karkare, S. H. Tan, and
A. Roychoudhury. A Feasibility Study of Using
Automated Program Repair for Introductory
Programming Assignments. In ESEC/FSE, 2017.

