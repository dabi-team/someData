2
2
0
2

r
a

M
1

]

R
C
.
s
c
[

1
v
4
9
1
0
0
.
3
0
2
2
:
v
i
X
r
a

Private Frequency Estimation via Projective Geometry

Vitaly Feldman*

Jelani Nelson†

Huy L. Nguyen‡

Kunal Talwar§

March 2, 2022

Abstract

In this work, we propose a new algorithm ProjectiveGeometryResponse (PGR) for locally
differentially private (LDP) frequency estimation. For a universe size of k and with n users,
our ε-LDP algorithm has communication cost (cid:100)log2 k(cid:101) bits in the private coin setting and
ε log2 e + O(1) in the public coin setting, and has computation cost O(n + k exp(ε) log k) for
the server to approximately reconstruct the frequency histogram, while achieving the state-of-
the-art privacy-utility tradeoff. In many parameter settings used in practice this is a signiﬁcant
improvement over the O(n + k2) computation cost that is achieved by the recent PI-RAPPOR
algorithm (Feldman and Talwar; 2021). Our empirical evaluation shows a speedup of over 50x
over PI-RAPPOR while using approximately 75x less memory for practically relevant param-
eter settings. In addition, the running time of our algorithm is within an order of magnitude
of HadamardResponse (Acharya, Sun, and Zhang; 2019) and RecursiveHadamardResponse (Chen,
Kairouz, and Ozgur; 2020) which have signiﬁcantly worse reconstruction error. The error of
our algorithm essentially matches that of the communication- and time-inefﬁcient but utility-
optimal SubsetSelection (SS) algorithm (Ye and Barg; 2017). Our new algorithm is based on
using Projective Planes over a ﬁnite ﬁeld to deﬁne a small collection of sets that are close to be-
ing pairwise independent and a dynamic programming algorithm for approximate histogram
reconstruction on the server side. We also give an extension of PGR, which we call HybridPro-
jectiveGeometryResponse, that allows trading off computation time with utility smoothly.

1 Introduction

In the so-called federated setting, user data is distributed over many devices which each communi-
cate to some central server, after some local processing, for downstream analytics and/or machine
learning tasks. We desire such schemes which (1) minimize communication cost, (2) maintain
privacy of the user data while still providing utility to the server, and (3) support efﬁcient al-
gorithms for the server to extract knowledge from messages sent by the devices. Such settings
have found applications to training language models for such applications as autocomplete and
spellcheck, and other analytics applications in Apple iOS [TVV+17] and analytics on settings in
Google Chrome [EPK14].

*Apple Inc.
†UC Berkeley. minilek@berkeley.edu. Supported by NSF grant CCF-1951384, ONR grant N00014-18-1-2562, and

ONR DORECG award N00014-17-1-2127.

‡Northeastern University. hu.nguyen@northeastern.edu. Supported in part by NSF CAREER grant CCF-1750716

and NSF grant CCF-1909314.

§Apple Inc. ktalwar@apple.com.

1

 
 
 
 
 
 
The gold standard for protecting privacy is for a scheme to satisfy differential privacy. In the
so-called local model that is relevant to the federated setting, there are n users with each user i
holding some data di ∈ D. Each user then uses its own private randomness ri and data di to run
a local randomizer algorithm that produces a random message Mi to send to the server. We say the
scheme is ε-differentially private if for all users i, any possible message m, and any d (cid:54)= d(cid:48),

P(Mi = m|di = d) (cid:54) eε P(Mi = m|di = d(cid:48)).

Note a user could simply send an unambiguous encoding of di, which allows the server to learn
di exactly (perfect utility), but privacy is not preserved; such a scheme does not preserve ε-DP for
any ﬁnite ε. On the opposite extreme, the user could simply send a uniformly random message
that is independent of di, which provides zero utility but perfect privacy (ε = 0). One can hope to
develop schemes that smoothly increase utility by relaxing privacy (i.e., by increasing ε).

This work addresses the problem of designing efﬁcient schemes for locally differentially pri-
vate frequency estimation. In this problem, one deﬁnes a histogram x ∈ Rk where xd is the number
of users i with di = d, and k = |D|. From the n randomized messages it receives, the server would
like to approximately reconstruct the histogram, i.e., compute some ˜x such that (cid:107)x − ˜x(cid:107) is small
with good probability over the randomness r = (r1, . . . , rn), for some norm (cid:107) · (cid:107). Our goal is to de-
sign schemes that obtain the best-known privacy-utility trade-offs, while being efﬁcient in terms
of communication, computation time, and memory. In this work we measure utility loss as the
mean squared error (MSE) Er
2], with lower MSE yielding higher utility. Note that such
a scheme should specify both the local randomizer employed by users, and the reconstruction
algorithm used by the server.

1
k [(cid:107)x − ˜x(cid:107)2

There are several known algorithms for this problem; see Table 1. To summarize, the best
known utility in prior work is achieved by SubsetSelection and slightly worse utility is achieved
by the RAPPOR algorithm [EPK14] that is based on the classical binary randomized response
[War65]. Unfortunately, both RAPPOR and Subset Selection have very high communication cost
of ≈ kH(1/(eε + 1)), where H is the binary entropy function and server-side running time of
˜O(nk/ exp(ε)). Large k is common in practice, e.g., k may be the size of a lexicon when estimating
word frequencies to train language models. This has led to numerous and still ongoing efforts to
design low-communication protocols for the problem [HKR12, EPK14, BS15, KBR16, WHN+19,
WBLJ17, YB17, ASZ19, BNS19, BNST20, CK ¨O20, FT21, SCB+21].

One simple approach to achieve low communication and computational complexity is to use
a simple k-ary RandomizedResponse algorithm (e.g. [WBLJ17]). Unfortunately, its utility loss is
suboptimal by up to an Ω(k/eε) factor; recall k is often large and ε is at most a small constant,
and thus this represents a large increase in utility loss. In the ε < 1 regime asymptotically opti-
mal utility bounds are known to be achievable with low communication and computational costs
[BS15, BNS19, BNST20]. The ﬁrst low-communication algorithm that achieves asymptotically op-
It communicates O(ε) bits and relies
timal bounds in the ε > 1 regime is given in [WBLJ17].
on shared randomness. However, it matches the bounds achieved by RAPPOR only when eε is
an integer and its computational cost is still very high and comparable to that of RAPPOR. Two
algorithms, HadamardResponse [ASZ19] and RecursiveHadamardResponse [CK ¨O20], show that it is
possible to achieve low communication, efﬁcient computation (only Θ(log k) slower than Random-
izedResponse) and asymptotically optimal utility. However, their utility loss in practice is subopti-
mal by a constant factor (e.g. our experiments show that these algorithms have an MSE that is

2

scheme name
RandomizedResponse

RAPPOR [EPK14]
SubsetSelection [YB17, WHN+19]
PI-RAPPOR [FT21]

communication
(cid:100)log2 k(cid:101)
k
k
eε (ε + O(1))
(cid:100)log2 k(cid:101) + O(ε)

utility loss
n(2eε+k)
(eε−1)2
4neε
(eε−1)2
4neε
(eε−1)2
4neε
(eε−1)2

HadamardResponse [ASZ19]
RecursiveHadamardResponse [CK ¨O20]

ProjectiveGeometryResponse

HybridProjectiveGeometryResponse

(cid:100)log2 k(cid:101)
(cid:100)log2 k(cid:101)
(cid:100)log2 k(cid:101)
(cid:100)log2 k(cid:101)

36neε
(eε−1)2
8neε
(eε−1)2
4neε
(eε−1)2
q−1 ) 4neε
(eε−1)2

(1 + 1

server time
n + k

nk
n k
eε

min(n + k2, n k

eε ), or

n + ke2ε log k (this work)
n + k log k
n + k log k

n + keε log k
n + kq log k

Table 1: Known local-DP schemes for private frequency estimation compared with ours. Utility
bounds are given up to 1 + ok(1) multiplicative accuracy for ease of display and running times are
asymptotic. For brevity we only state bounds for ε (cid:54) log k. Some of algorithms assume k is either
a power of 2 or some other prime power and otherwise potentially worsen in some parameters
due to round-up issues; we ignore this issue in the table. The communication and server time for
RAPPOR are random variables which are never more than k and nk, respectively, but RAPPOR
can be implemented so that in expectation the communication and runtimes are asymptotically
equal to SubsetSelection. For HybridProjectiveGeometryResponse, q can be chosen as any prime in
[2, exp(ε) + 1]. The utility loss here is the proven upper bound on the variance for PGR, HR and
RHR, and the analytic expression for the variance for the others. The communication bounds are in
the setting of private coin protocols. As with RHR, PGR and HPGR can also both achieve improved
communication in the public coin model; see Appendix B.

over 2× higher for ε = 5 than SubsetSelection; see Fig. 2).

Recent work of Feldman and Talwar [FT21] describes a general technique for reducing com-
munication of a local randomizer without sacriﬁcing utility and, in particular, derives a new low
communication algorithm for private frequency estimation via pairwise independent derandom-
ization of RAPPOR. Their algorithm, referred to as PI-RAPPOR, achieves the same utility loss as
RAPPOR and has the server-side running time of ˜O(min(n + k2, nk/ exp(ε))). The running time
of this algorithm is still prohibitively high when both n and k are large.

We remark that while goals (1)-(3) from the beginning of this section are all important, goal
(2) of achieving a good privacy/utility tradeoff is unique in that poor performance cannot be mit-
igated by devoting more computational resources (more parallelism, better hardware, increased
bandwidth, etc.). After deciding upon a required level of privacy ε, there is a fundamental limit as
to how much utility can be extracted given that level of privacy; our goal in this work is to under-
stand whether that limit can be attained in a communication- and computation-efﬁcient way.

Our main contributions. We give a new private frequency estimation algorithm ProjectiveGeom-
etryResponse (PGR) that maintains the best known utility and low communication while signiﬁ-
cantly improving computational efﬁciency amongst algorithms with similarly good utility. Using

3

our ideas, we additionally give a new reconstruction algorithm that can be used with the PI-
RAPPOR mechanism to speed up its runtime from O(k2/ exp(ε)) to O(k exp(2ε) log k) (albeit, this
runtime is still slower than PGR’s reconstruction algorithm by an exp(ε) factor). We also show a
general approach that can further improve the server-side runtime at the cost of slightly higher
reconstruction error, giving a smooth tradeoff: for any prime 2 (cid:54) q (cid:54) exp(ε) + 1, we can get
running time O(n + qk log k) with error only (1 + 1/(q − 1)) times larger than the best known
bound1. Note that for q = 2 we recover the bounds achieved by HR and RHR. Our mechanisms re-
quire (cid:100)log2 k(cid:101) per device in the private coin model, or ε log2 e + O(1) bits in the public coin model
(see Appendix B). As in previous work, our approximate reconstruction algorithm for the server is
also parallelizable, supporting linear speedup for any number of processors P (cid:54) min{n, k exp(ε)}.
We also perform an empirical evaluation of our algorithms and prior work and show that indeed
the error of our algorithm matches the state of the art will still being time-efﬁcient.

As has been observed in previous work [ASZ19], the problem of designing a local randomizer
is closely related to the question of existence of set systems consisting of sets of density ≈ exp(−ε)
which are highly symmetric, and do not have positive pairwise dependencies. The size of the
set system then determines the communication cost, and its structural properties may allow for
efﬁcient decoding. We show that projective planes over ﬁnite ﬁelds give us set systems with the
desired properties, leading to low communication and state-of-the-art utility. We also show a
novel dynamic programming algorithm that allows us to achieve server runtime that is not much
worse than the fastest known algorithms.

As in a lot of recent work on this problem, we have concentrated on the setting of moderately
large values for the local privacy parameter ε. This is a setting of interest due to recent work in
privacy ampliﬁcation by shufﬂing [BEM+17, CSU+19, EFM+19, BBGN19, FMT21] that shows that
local DP responses, when shufﬂed across a number of users so that the server does not know which
user sent which messages, satisfy a much stronger central privacy guarantee. Asymptotically, ε-

), δ)-DP. The hidden constants
DP local randomizers aggregated over n users satisfy (O(
here are small: as an example with n = 10, 000 and ε = 6, shufﬂing gives a central DP guarantee of
(0.3, 10−6)-DP. This motivation from shufﬂing is also the reason why our work concentrates on the
setting of private coin protocols, as shared randomness seems to be incompatible with shufﬂing
of private reports. We note that while constant factors improvement in error may seem small,
these algorithm are typically used for discovering frequent items from power law distributions. A
constant factor reduction in variance of estimating any particular item frequency then translates
to a corresponding smaller noise ﬂoor (for a ﬁxed false positive rate, say), which then translates to
a constant factor more items being discovered.

(cid:113)

eε ln 1
δ
n

1.1 Related Work

A closely related problem is ﬁnding “heavy hitters”, namely all elements j ∈ [k] with counts higher
than some given threshold; equivalently, one wants to recover an approximate histogram ˜x such
that (cid:107)x − ˜x(cid:107)∞ is small (the non-heavy hitters i can simply be approximated by ˜xi = 0). In this prob-
lem the goal is to avoid linear runtime dependence on k that would result from doing frequency

1For both PGR and HPGR we have stated runtime bounds assuming that certain quantities involving k, exp(ε) are
prime powers. If this is not the case, runtimes may increase by a factor of exp(ε) for PGR, or q for HPGR; we note that
PI-RAPPOR also has this feature.

4

estimation and then checking all the estimates. This problem is typically solved using a “fre-
quency oracle” which is an algorithm that for a given j ∈ [k] returns an estimate of the number of
j’s held by users (typically without computing the entire histogram) [BS15, BNST20, BNS19]. Fre-
quency estimation is also closely related to the discrete distribution estimation problem in which
inputs are sampled from some distribution over [k] and the goal is to estimate the distribution
[YB17, ASZ19]. Indeed, bounds for frequency estimation can be translated directly to bounds on
distribution estimation by adding the sampling error. We note that even for the problem of im-
plementing a private frequency oracle, our PGR scheme supports answering queries faster than
PI-RAPPOR by factor of Θ(exp(ε)).

2 Preliminaries

Our mechanisms are based on projective spaces, and below we review some basic deﬁnitions and
constructions of such spaces from standard vector spaces.

Deﬁnition 2.1. For a given vector space V, the projective space P (V) is the set of equivalence
classes of V \ {0}, where 0 denotes the zero vector, under the following equivalence relation: x ∼ y
iff x = cy for some scalar c. Each equivalence class is called a (projective) “point” of the projective
space. Let p : V \ {0} → P (V) be the mapping from each vector v ∈ V to its equivalence class. If
V has dimension t then P(V) has dimension t − 1.

We will also use subspaces of the projective space P (V).

Deﬁnition 2.2. A projective subspace W of P (V) is a subset of P (V) such that there is a subspace
U of V where p (U \ {0}) = W. If U has dimension t then W has dimension t − 1.

It should be noted that intersections of projective subspaces are projective subspaces. Let q be
(cid:17)

q the t-dimensional vector space over the ﬁeld Fq. We will work with P

Ft
q

(cid:16)

a prime power and Ft
and its subspaces.

Deﬁnition 2.3. A vector x ∈ Ft

q is called canonical if its ﬁrst non-zero coordinate is 1.

Each equivalence class can be speciﬁed by its unique canonical member.

3 ProjectiveGeometryResponse description and analysis

Our PGR scheme is an instantiation of the framework due to [ASZ19]. In their framework, the
local randomizer is implemented as follows. There is a universe U of outputs and each input v
corresponds to a subset S(v) of outputs. All the subsets S(v) for different values of v have the
same size. Given the input v, the local randomizer returns a uniformly random element of S(v)
|S(v)|eε+|U|−|S(v)| and a uniformly random element of U \ S(v) with probability
with probability
|U|−|S(v)|
|S(v)|eε+|U|−|S(v)| . The crux of the construction is in specifying the universe U and the subsets S(v).
q−1 for some integer t (other values of k need to be rounded up to the
q and the corre-
. We also identify the output values with projective points

nearest such value). We identify the k input values with k canonical vectors in Ft

PGR works for k = qt−1

sponding projective points in P

eε|S(v)|

(cid:17)

(cid:16)

Ft
q

5

(cid:16)

(cid:17)

(cid:16)

(cid:17)

. The subsets S(v) are the (t − 2)-dimensional projective subspaces of P

Ft
in P
q
are qt−1
q−1 (t − 2)-dimensional projective subspaces, which is the same as the number of projective
points. For a canonical vector v, the set S(v) is the (t − 2)-dimensional projective subspace such
that for all u ∈ p−1(S(v)), we have (cid:104)u, v(cid:105) = 0. Each (t − 2)-dimensional projective subspace con-
tains qt−1−1
q−1 messages out of the
universe of qt−1

q−1 projective points. In other words, each set S(v) contains qt−1−1

. There

Ft
q

q−1 messages.

An important property of the construction is the symmetry among the intersections of any two

subsets S(v).

Claim 3.1. Consider a t-dimensional vector space V. The intersection of any two (t − 2)-dimensional
projective subspaces of P (V) is a (t − 3)-dimensional projective subspace.

Proof. Let I be the intersection of two projective subspaces S1 and S2. Recall that I, S1, S2 are pro-
jective subspaces corresponding to subspaces of V. Assume for contradiction that the dimension
d − 1 of the intersection I is lower than t − 3. Starting from a basis v1, . . . , vd of p−1(I) ∪ {0}, we
can extend it with u1, . . . , ut−1−d to form a basis of the subspace p−1(S1) ∪ {0}. We can also ex-
tend v1, . . . , vd with w1, . . . , wt−1−d to form a basis of p−1(S2) ∪ {0}. Because d + 2(t − 1 − d) =
t + (t − 2 − d) > t, the collection of vectors v1, . . . , vd, u1, . . . , ut−1−d, w1, . . . , wt−1−d must be lin-
early dependent. There must exist nonzero coefﬁcients so that ∑i αivi + ∑j βjuj + ∑k γkwk = 0.
This means ∑k γkwk = − ∑i αivi − ∑j βjuj is a non-zero vector in p−1(S1) ∩ p−1(S2) but it is not in
p−1(I), which is a contradiction.

To ease the presentation we deﬁne cint = qt−2−1
q−1

q−1 denote the size of each subset S(v). Notice that c2
set

to be the size of the intersection of two sub-
(cid:62) k · cint i.e.

sets S(v) and let cset = qt−1−1
(cset/cint)2 (cid:62) k/cint.

Each user with input v sends a projective point e with probability eε p if e is in S(v) and proba-

bility p otherwise. We have

eε pcset + p(k − cset) = 1,

so that p =

1
(eε − 1) cset + k

.

The server keeps the counts on the received projective points in a vector y ∈ Zk. Thus, the

total server storage is O (k). We estimate xv by computing

˜xv = α

(cid:33)

(cid:32)

∑
u∈Sv

yu

+ β ∑

yu

u

where α and β are chosen so that it is an unbiased estimator. Note ∑u yu = n. We would like
E ˜xv = xv for all v. Notice that by linearity of expectation, it sufﬁces to focus on the contribution
to ˜xv from a single user.

If that user’s input is v, the expectation of the sum Q := ∑u∈Sv yu is eε pcset. On the other
hand, if the input is not v, the expectation of the sum ∑u∈Sv yu is eε pcint + p (cset − cint). We want
α · E [Q] + β = [[input is v]], where [[T]] is deﬁned to be 1 if T is true and 0 if false. Thus,

αeε pcset + β = 1,

6

and αp ((eε − 1) cint + cset) + β = 0.

Substituting p, we get

αeε

cset
(eε − 1) cset + k
(eε − 1)cint + cset
(eε − 1) cset + k

α

Solving for α, β, we get

+ β = 1

+ β = 0

α =

β = −

(eε − 1) cset + k
(eε − 1) (cset − cint)
(eε − 1) cset + k
(eε − 1) (cset − cint)

;

·

(eε − 1)cint + cset
(eε − 1) cset + k

= −

(eε − 1)cint + cset
(eε − 1) (cset − cint)

.

We next analyze the variance, which suggests that q should be chosen close to exp(ε) + 1 for

the best utility.

(cid:104)

(cid:107)x − ˜x(cid:107)2
2

(cid:105) (cid:54) neεc2

set/c2

int+n(k−1)((eε−1)+cset/cint)2
(eε−1)2(cset/cint−1)

. In particular, if cset/cint = eε + 1 then

Lemma 3.2. E
(cid:104) 1
k (cid:107)x − ˜x(cid:107)2

E

2

(cid:105) (cid:54) n

k + 4neε
(eε−1)2

Proof. By independence, we only need to analyze the variance when there is exactly one user with
input v. The lemma then follows from adding up the variances from all users.

E

(cid:104)

( ˜xv − 1)2(cid:105)

= eε pcset (α + β − 1)2 + p(k − cset) (β − 1)2

=

1 − β
α

(α + β − 1)2 +

α + β − 1
α

(1 − β)2

= (α + β − 1) (1 − β)

=

=

(cid:54)

=

eεcset
(eε − 1) (cset − cint)

·

−cset + k
(eε − 1) (cset − cint)
(−cset/cint + k/cint) eεcset/cint
(eε − 1)2 (cset/cint − 1)2

(cid:0)−cset/cint + c2

set/c2
int
(eε − 1)2 (cset/cint − 1)2

(cid:1) eεcset/cint

eεc2

set/c2
int
(eε − 1)2 (cset/cint − 1)

Let z = cset/cint. Note that
variance gets larger as q gets larger.

z2
z−1 is an increasing function for z ∈ [2, +∞) so this part of the

Next we analyze the contribution to the variance from coordinates u (cid:54)= v.

E (cid:2) ˜x2

u

(cid:3) = ((eε − 1) cint + cset) p (α + β)2 + (1 − eε pcint − p (cset − cint)) β2

= −

β
α

(α + β)2 +

(cid:18)

1 +

(cid:19)

β
α

β2

7

=

−β (α + β)2 + (α + β) β2
α

= −β (α + β)

=

=

(cid:54)

=

(eε − 2) cset + k − (eε − 1)cint
(eε − 1) (cset − cint)

·

(eε − 1)cint + cset
(eε − 1) (cset − cint)
(eε − 1) + cset/cint
(eε − 1) (cset/cint − 1)
((eε − 1) + z) (cid:0)(eε − 2) z + z2 − (eε − 1)(cid:1)
(eε − 1)2 (z − 1)2

·

(eε − 2) cset/cint + k/cint − (eε − 1)
(eε − 1) (cset/cint − 1)

((eε − 1) + z)2
(eε − 1)2 (z − 1)

Note that the function

is decreasing for z ∈ (0, eε + 1] and it is increasing for z ∈
[eε + 1, +∞) so this part of the variance is minimized when z = eε + 1. For z = eε + 1, we can
substitute and get

((eε−1)+z)2
(eε−1)2(z−1)

4eε
(eε−1)2 .

Next we discuss the algorithms to compute ˜xv. The naive algorithm takes O(kcset) = O(k2/q)
time and this is the algorithm of choice for t (cid:54) 3. For t > 3, we can use dynamic programming to
obtain a faster algorithm. Note in the below that q should be chosen close to exp(ε) + 1.

Theorem 3.3. In the ProjectiveGeometryResponse scheme, there exists an O((qt − 1)/(q − 1)tq) time
algorithm for server reconstruction, using O((qt − 1)/(q − 1)) memory. These bounds are at best O(ktq)
time and O(k) memory, and increase by at most a factor of q each if rounding up to the next power of q is
needed so that (qt − 1)/(q − 1) (cid:62) k.

Proof. We use dynamic programming. For a ∈ Fj
, z ∈ Fq, where a is further restricted to
have its ﬁrst nonzero entry be a 1 (it may also be the all-zeroes vector), and b is restricted to be a
canonical vector when j = 0, deﬁne

q, b ∈ Ft−j

q

f (a, b, z) =

∑
pre fj(u)=a
(cid:104)sufft−j(u),b(cid:105)=z

yu,

where prefi(u) denotes the length-i preﬁx vector of u, and suffi(u) denotes the length-i sufﬁx
vector of u. Then, we would like to compute

˜xv = α

(cid:33)

(cid:32)

∑
u∈Sv

yu

+ β ∑

u

yu = α · f (⊥, v, 0) + βn,

for all projective points v, where ⊥ denotes the length-0 empty vector. We next observe that f
satisﬁes a recurrence relation, so that we can compute the full array of values ( f (⊥, v, 0))v is canonical
efﬁciently using dynamic programming and then efﬁciently obtain ˜x ∈ Rk.

We now describe the recurrence relation. For w ∈ Fq and a vector v, let v ◦ w denote v with w
appended as one extra entry. If j denotes the length of the vector a, then the base case is j = t. In

8

this case, f (a, ⊥, z) = ya iff both a (cid:54)= 0 and z = 0; else, f (a, ⊥, z) = 0. The recursive step is then
when 0 (cid:54) j < t. Essentially, we have to sum over all ways to extend a by one more coordinate.
Let suff−1(b) denote the vector b but with the ﬁrst entry removed (so it is a vector of length one
shorter). There are two cases: a is the all-zeroes vector, versus it is not. In the former case, the
recurrence is

f (0, b, z) = f ((cid:126)0 ◦ 0, suff−1(b), z) + f ((cid:126)0 ◦ 1, suff−1(b), z − b1 mod q).

Note we are not allowed to append w ∈ {2, 3, . . . , q − 1} to a since that would not satisfy the
requirement that the ﬁrst argument to f either be all-zeroes or be canonical. The other case for the
recurrence relation is when a (cid:54)= 0, in which case the recurrence relation becomes

f (a, b, z) =

q−1
∑
w=0

f (a ◦ w, suff−1(b), z − d · b1 mod q).

We now analyze the running time and memory requirements to obtain all f (a, b, z) values via

dynamic programming. The runtime is proportional to

kq + ∑

q.

a,b,z,j(cid:54)=0

This is because for j > 0, for each a, b, z triple we do at most q work. When j = 0, there is only one
possible value for a (namely ⊥) and k = qt−1
q−1 values for v, plus we are only concerned with z = 0
in this case. For larger j, the number of possibilities for a is qj−1
q−1 + 1 (the additive 1 is since a can
be the all-zeroes vector), whereas the number of possibilities for b is qt−j. Thus the total runtime
is proportional to

kq +

(cid:32) t
∑
j=1

(cid:18) qj − 1
q − 1

(cid:19)

+ 1

(cid:33)

· qt−j

· q2 = O(ktq2).

For the memory requirement, note f (·) values for some ﬁxed j only depend on the values for
j + 1, and thus using bottom-up dynamic programming we can save a factor of t in the memory,
for a total memory requirement of only O(kq) (for any ﬁxed j there are only O(k) a, b pairs, and
there are q values for z).

Finally, we add an optimization which improves both the runtime and memory by a factor
of q. Speciﬁcally, suppose b is not canonical and is not the all-zeroes vector. Let the value of its
ﬁrst nonzero entry be ζ. Then f (a, b, z) is equal to f (a, b/ζ, z/ζ), where the division is over Fq.
Thus, we only need to compute f (·) for b either canonical or equal to the 0 vector. This reduces
the number of b from qt−j to (qt−j − 1)/(q − 1) + 1, which improves the runtime to O(ktq) and
the memory to O(k). Note ﬁnite ﬁeld division over Fq can be implemented in O(1) time after
preprocessing. First, factor q − 1 and generate all its divisors in o(q) time, from which we can ﬁnd
a generator g of F∗
q in o(q) expected time by rejection sampling (it is a generator iff gp (cid:54)≡ 1 mod q
for every nontrivial divisor p of q, and we can compute gp mod q in O(log q) time via repeated
squaring). Then, in O(q) time create a lookup table A[0 . . . q − 1] with A[i] := gi mod q. Then
create an inverse lookup table by for each 0 (cid:54) i < q, setting the inverse of A[i] to A[q − 1 − i].

9

4 HybridProjectiveGeometryResponse: trading off error and time

In this section, we describe a hybrid scheme using an intermediate value for the ﬁeld size q to
trade off between the variance and the running time. Roughly speaking, larger values for q lead
to slower running time but also smaller variance. The approach is similar to the way [ASZ19]
extended their scheme from the high privacy regime to the general setting. We choose h, q, t such
that they satisfy the following conditions:

• b = qt−1

q−1 and bh (cid:62) k > cseth.
q−1 , cint = qt−2−1
• Choose hz as close as possible to eε + 1.

• Let cset = qt−1−1

q−1 , and z = cset/cint. Note that c2
set

(cid:62) b · cint and q + 1 (cid:62) z (cid:62) q.

The input coordinates are partitioned into blocks of size at most b each. The algorithm’s response
consists of two parts: the index of the block and the index inside the block. First, the algorithm
uses the randomized response to report the block. Next, if the response has the correct block then
the algorithm uses the scheme described in the previous section with ﬁeld size q to describe the
coordinate inside the block. If the ﬁrst response has the wrong block then the algorithm uses a
uniformly random response in the second part.

More precisely, the algorithm works as follows. Each input value is identiﬁed with a pair (i, v)
where i ∈ Z
h and v is a canonical vector in Ft
q. If k < bh then we allocate up to (cid:100)k/h(cid:101) input
h and u is a canonical vector in Ft
values to each block. The response is a pair (j, u) where i ∈ Z
q
chosen as follows. For j = i and (cid:104)u, v(cid:105) = 0, the pair (j, u) is chosen with probability eε p. All other
choices are chosen with probability p each. Because all probabilities are either p or eε p, the scheme
is ε-private. We have

eε p · cset + p (bh − cset) = 1

p =

1
bh + (eε − 1) cset

Let ˜xi,v be our estimate for the frequency of input (i, v). The estimates are computed as follows.

˜xi,v = α

(cid:32)

∑
(cid:104)v,u(cid:105)=0

(cid:33)

(cid:32)

yi,u

+ β

(cid:33)

(cid:32)

yi,u

+ γ

(cid:33)

yj,u

∑
j,u

∑
u

We need to choose α, β and γ so that ˜xi,v is an unbiased estimator of xi,v. By linearity of ex-
pectation, we only need to consider the case with exactly one user. If the input is i, v then we
have

E [ ˜xi,v] = αeε pcset + βp ((eε − 1) cset + b) + γ = 1

If the input is not i, v but in the same block then

E [ ˜xi,v] = αp ((eε − 1) cint + cset) + βp ((eε − 1) cset + b) + γ = 0

Finally if the input is in a different block then

E [ ˜xi,v] = αpcset + βpb + γ = 0

10

We solve for α, β, γ and get

α =

1
p (eε − 1) (cset − cint)

=

bh + (eε − 1) cset
(eε − 1) (cset − cint)

cint/cset
p (eε − 1) (cset − cint)

β = −

= −

= −

αcint
cset
bh + (eε − 1) cset
(eε − 1) (cset − cint)

γ = −αpcset − βpb = −

·

cint
cset
cset − b · cint
cset
(eε − 1) (cset − cint)

(cid:54) 0

We note that α + β =

(cid:16)

1 − cint
cset

(cid:17)

α = bh/cset+(eε−1)

(eε−1)

.

Lemma 4.1.

(cid:104)

E

(cid:107)x − ˜x(cid:107)2
2

(cid:105) (cid:54) n

(cid:32)

1 +

(zh + (eε − 1))
(eε − 1)2 (z − 1)

+

2
(eε − 1)

+

+ n

(zh + (eε − 1)) z
(eε − 1)2 (z − 1)

(cid:18)

k − (cid:100)k/h(cid:101) + ((cid:100)k/h(cid:101) − 1)

(z + eε − 1)
z

(cid:33)

eε (zh − eε + 1)
(eε − 1)2
(cid:19)

In particular, if zh = eε + 1 then E

(cid:104) 1
k (cid:107)x − ˜x(cid:107)2

2

(cid:105) (cid:54) n

k + z

z−1 · n 4eε
(eε−1)2

Proof. By independence, we only need to analyze the variance when there is exactly one user with
input (i, v) and response (j, u). The lemma follows from adding up the variances from all users.

E

(cid:104)

( ˜xi,v − 1)2(cid:105) (cid:54)E

(cid:104)

( ˜xi,v − 1 − γ)2(cid:105)

= P [j (cid:54)= i] · (−1)2 + P [j = i ∧ (cid:104)u, v(cid:105) (cid:54)= 0] (β − 1)2 + P [j = i ∧ (cid:104)u, v(cid:105) = 0] (α + β − 1)2
= (1 − (eε − 1) pcset − pb) + p (b − cset) (β − 1)2 + eε pcset (α + β − 1)2
=1 + p (b − cset) (cid:0)β2 − 2β(cid:1) + eε pcset (α + β) (α + β − 2)

We expand the second and third terms individually:

p (b − cset) (cid:0)β2 − 2β(cid:1)

(cid:19)2

+

2cint/cset
p (eε − 1) (cset − cint)

(cid:33)

(cid:32)(cid:18)

= p (b − cset)

cint/cset
p (eε − 1) (cset − cint)
int/c2
set

=

=

(bh + (eε − 1) cset) (b − cset) c2

(eε − 1)2 (cset − cint)2

(bh/cset + (eε − 1)) (b/cset − 1)
(eε − 1)2 (cset/cint − 1)2

+

+

2 (b − cset) cint/cset
(eε − 1) (cset − cint)
2 (b/cset − 1)
(eε − 1) (cset/cint − 1)

(cid:54) (zh + (eε − 1))
(eε − 1)2 (z − 1)

+

2
(eε − 1)

11

and

bh/cset + (eε − 1)
(eε − 1)

·

=

eε pcset (α + β) (α + β − 2)
eεcset
bh + (eε − 1) cset
eε (bh/cset − eε + 1)
(eε − 1)2
(cid:54) eε (zh − eε + 1)
(eε − 1)2

=

·

bh/cset − (eε − 1)
(eε − 1)

When zh = eε + 1, we have
( ˜xi,v − 1)2(cid:105) (cid:54) 1 +

E

(cid:104)

2eε
(eε − 1)2 (z − 1)

+

2
(eε − 1)

+

2eε
(eε − 1)2

< 1 +

4eε
(eε − 1)2

z
z − 1

Next consider v(cid:48) (cid:54)= v.

E

(cid:104)

( ˜xi,v(cid:48) − 0)2(cid:105) (cid:54) E

(cid:104)

( ˜xi,v(cid:48) − γ)2(cid:105)

= P [j (cid:54)= i] · 0 + P (cid:2)j = i ∧ (cid:10)u, v(cid:48)(cid:11) (cid:54)= 0(cid:3) (β)2 + P (cid:2)j = i ∧ (cid:10)u, v(cid:48)(cid:11) = 0(cid:3) (α + β)2
= p (b + (eε − 2) cset − (eε − 1) cint) (β)2 + p ((eε − 1) cint + cset) (α + β)2

= (b + (eε − 2) cset − (eε − 1) cint)

1
p (eε − 1)2 (cset − cint)2

c2
int
c2
set

+

p ((eε − 1) cint + cset)

(1 − cint/cset)2
p2 (eε − 1)2 (cset − cint)2

= (b/cset + (eε − 2) − (eε − 1) cint/cset)

(bh/cset + (eε − 1))
(eε − 1)2 (1 − cint/cset)2

c2
int
c2
set

+

((eε − 1) cint/cset + 1)

(bh/cset + (eε − 1))
(eε − 1)2

(cid:54) (z + (eε − 2) − (eε − 1) /z)

(zh + (eε − 1))
(eε − 1)2 (z − 1)2

+ ((eε − 1) /z + 1)

(zh + (eε − 1))
(eε − 1)2

= (z + eε − 1)

(zh + (eε − 1))
(eε − 1)2 (z − 1)

When zh = eε + 1, the last expression is bounded by (z + eε − 1)

(z+eε−1)
(z−1)

·

2eε
(eε−1)2

(cid:54) 1+h
z

z
(z−1) ·

2eε
(eε−1)2

Finally, consider i(cid:48) (cid:54)= i and arbitrary v(cid:48).

2eε
(eε−1)2(z−1)z

+ (z + eε − 1)

2eε
(eε−1)2z

=

E

(cid:104)

( ˜xi(cid:48),v(cid:48) − 0)2(cid:105) (cid:54) E

(cid:104)

( ˜xi(cid:48),v(cid:48) − γ)2(cid:105)

= P (cid:2)j (cid:54)= i(cid:48)(cid:3) · 02 + P (cid:2)j = i(cid:48) ∧ (cid:10)u, v(cid:48)(cid:11) (cid:54)= 0(cid:3) (β)2 + P (cid:2)j = i(cid:48) ∧ (cid:10)u, v(cid:48)(cid:11) = 0(cid:3) (α + β)2
= p (b − cset) (β)2 + pcset (α + β)2

12

= p (b − cset)

= (b/cset − 1)

1
p2 (eε − 1)2 (cset − cint)2
(bh/cset + (eε − 1))
(eε − 1)2 (1 − cint/cset)2

c2
int
c2
set
c2
int
c2
set

+ pcset

(1 − cint/cset)2
p2 (eε − 1)2 (cset − cint)2

+

bh/cset + (eε − 1)
(eε − 1)2

+

zh + (eε − 1)
(eε − 1)2

(cid:54) (zh + (eε − 1))
(eε − 1)2 (z − 1)
(zh + (eε − 1)) z
(eε − 1)2 (z − 1)

=

When zh = eε + 1, the last expression is bounded by

There are bi

(cid:54) (cid:100)k/h(cid:101) (cid:54) b valid coordinates in the same block with the input (i, v). There are
k − bi coordinates in the other blocks. Thus the total variance across all coordinates except for
coordinate (i, v) is bounded by

2eε
(eε−1)2

1

z−1 + 2eε

(eε−1)2 = 2eε
(eε−1)2

z
z−1

(zh + (eε − 1)) z
(eε − 1)2 (z − 1)
(cid:18)

(cid:18)

k − bi + (bi − 1)

k − (cid:100)k/h(cid:101) + ((cid:100)k/h(cid:101) − 1)

(cid:54) (zh + (eε − 1)) z
(eε − 1)2 (z − 1)

(cid:19)

(cid:19)

(z + eε − 1)
z
(z + eε − 1)
z

For zh = eε + 1, we have

(z+eε−1)
z

(cid:54) 1 + h and k − bi + (bi − 1) (z+eε−1)

z

(cid:54) k − (cid:100)k/h(cid:101) + ((cid:100)k/h(cid:101) −

1)(1 + h) = k − (cid:100)k/h(cid:101) + (cid:100)k/h(cid:101) − 1 + h((cid:100)k/h(cid:101) − 1) < 2k.

Regarding the decoding algorithms, notice that the estimates are computed separately by
blocks except for an offset γ scaled by the total number of received messages across all blocks.
q(cid:100)logq(k/h)(cid:101)−1(cid:17)
Thus, using the naive algorithm, the time to estimate one count is O (cset) = O
.
Using the fast algorithm to estimate all counts takes O (bqt) time per block and in total, O (bqth) =
O

hq1+(cid:100)logq(k/h)(cid:101)(cid:17)

time.

(cid:16)(cid:108)

(cid:16)

(cid:109)

logq (k/h)

5 Experimental Results

In this section, we compare previously-known algorithms (RAPPOR, PI-RAPPOR, HadamardResponse
(HR), RecursiveHadamardResponse (RHR), SubsetSelection (SS)) and our new algorithms Projective-
GeometryResponse (PGR) and HybridProjectiveGeometryResponse (HPGR). As the variance upper
bound of these algorithms do not depend on the underlying data, we perform our experiments on
simple synthetic data that realize the worst case for variance. Our experiments show that Projec-
tiveGeometryOracle matches the best of these algorithms namely SS, RAPPOR, and PI-RAPPOR,
and achieves noticeably better MSE than other communication- and computation-efﬁcient ap-
proaches. At the same time it is signiﬁcantly more efﬁcient than those three in terms of server
computation time, while also achieving optimal communication.

All experiments were run on a Dell Precision T3600 with six Intel 3.2 GHz Xeon E5-1650 cores
running Ubuntu 20.04 LTS, though our implementation did not take advantage of parallelism. We
implemented all algorithms and ran experiments in C++, using the GNU C++ compiler version

13

scheme name
PI-RAPPOR
PGR
HPGR
RHR
HR
RR

runtime (in seconds)
1,893.82 (approximately 31.5 minutes)
36.92
5.94
1.20
0.64
0.02

Table 2: Server runtimes for ε = 5, k = 3,307,948. For HPGR, we chose the parameters h = 50, q =
3, t = 11, so that the mechanism rounded up the universe size to h(qt − 1)/(q − 1), which is about
34% larger than k.

9.3.0; code and details on how to run the experiments used to generate data and plots are in our
public repository at https://github.com/minilek/private_frequency_oracles/.

We ﬁrst performed one experiment to show the big gap in running times. We took ε = 5, a
practically relevant setting, and n = 10,000, k = 3,307,948; this setting of n is smaller than one
would see in practice, but the runtimes of the algorithms considered are all linear in n plus addi-
tional terms that depend on k, ε, and our aim was to measure the impact of these additive terms,
which can be signiﬁcant even for large n. Furthermore, in practice the server can immediately
process messages from each of the n users dynamically as the messages arrive asynchronously,
whereas it is the additive terms that must be paid at once at the time of histogram reconstruction.
For our settings, the closest prime to exp(ε) + 1 ≈ 149.4 is q = 151. Recall that PGR rounds up
to universe sizes of the form (qt − 1)/(q − 1); then (q4 − 1)/(q − 1) is less than 5% larger than
k, so that the negative effect of such rounding on the runtime of PGR is minimal. Meanwhile PI-
RAPPOR picks the largest prime q(cid:48) smaller than exp(ε) + 1, which in this case is q(cid:48) = 149, and
assumes universe sizes of the form q(cid:48)t − 1; in this case q(cid:48)3 − 1 = k exactly, so rounding issues do
not negatively impact the running time of PI-RAPPOR (we chose this particular value of k inten-
tionally, to show PI-RAPPOR’s performance in the best light for some fairly large universe size).
The runtimes of various algorithms with this setting of ε, k, n are shown in Table 2. Note RHR and
HR sacriﬁce a constant factor in utility compared to PI-RAPPOR and PGR, the former of which
is four orders of magnitude slower while the latter is only one order of magnitude slower and
approximately 51x faster than PI-RAPPOR. Meanwhile, HPGR’s runtime is of the same order of
magnitude (though roughly 5x slower) than RHR, but as we will see shortly, HPGR can provide
signiﬁcantly improved utility over RHR and HR.

Next we discuss error. Many of our experiments showing reconstruction error with ﬁxed ε
take ε = 5, a practically relevant setting, and universe size k = 22,000, for which the closest
prime to exp(ε) + 1 ≈ 149.4 is q = 151. Recall that PGR rounds up to universe sizes of the form
(qt − 1)/(q − 1); then (q3 − 1)/(q − 1) = 22,593 is not much larger than k, so that the runtime of
PGR is not severely impacted. Also, cset/cint as deﬁned in Section 3 is very close to exp(ε) + 1,
so that the MSE bound in Lemma 3.2 nearly matches that of SS. Furthermore for HPGR for this
setting of ε, k, if we choose q = 5, h = 30, t = 5, then h · (qt − 1)/(q − 1) = 23,430, which is
not much bigger than k so that the runtime of HPGR is not majorly impacted. Furthermore hz as
deﬁned in Section 4 is approximately 150.19, which is very close to exp(ε) + 1 as recommended

14

(a)

(b)

Figure 1: RandomizedResponse has signiﬁcantly worse error than other algorithms, even for moder-
ately large universes, followed by HadamardResponse and RecursiveHadamardResponse, which have
roughly double the error of state-of-the-art algorithms. HybridProjectiveGeometryResponse trades
off having slightly worse error than state-of-the-art for faster runtime.

by Lemma 4.1 to obtain minimal error. We ﬁrst draw attention to Figs. 2a and 2b. These plots
run RAPPOR, PI-RAPPOR, PGR, and SS with k, n, ε as in the ﬁgure and show that their error dis-
tributions are essentially equivalent. We show the plots for only one some particular parameter
settings, but the picture has looked essentially the same to us regardless of which parameters we
have tried. In Fig. 2a, we have n users each holding the same item in the universe (item 0); we
call this a spike distribution as noted in the plot. We have each user apply its local randomizer
to send a message to the server, and we ask the server to then reconstruct the histogram (which
should be (n, 0, . . . , 0)) and calculate the MSE. We repeat this experiment 300 times, and in this
plot we have 300 dots plotted per algorithm, where a dot at point (x, y) signiﬁes that the MSE
was at most y for x% of the trial runs; this, it is a plot of the CDF of the empirical error distribu-
tion. In Fig. 2b, we plot MSE as a function of increasing ε, where for each value of ε we repeat the
above experiment 10 times then plot the average MSE across those 10 trial runs. Because the error
performance of RAPPOR, PI-RAPPOR, SS, and PGR are so similar, in all other plots we do not in-
clude RAPPOR and PI-RAPPOR since their runtimes are so slow that doing extensive experiments
is very time-consuming computationally (note: our implementation of RAPPOR requires O(nk)
server time, though O(n(k/eε + 1)) expected time is possible by having each user transmit only a
sparse encoding of the locations of the 1 bits in its message). We ﬁnally draw attention to Figs. 2c
to 2h. Here we run several algorithms where the distribution over the universe amongst the users
is Zipﬁan (a power law), with power law exponent either 0.1 (an almost ﬂat distribution), or 3.0
(rapid decay). The HPGR algorithm was run with q = 5. As can be seen, the qualitative behavior
and relative ordering of all the algorithms is essentially unchanged by the Zipf parameter: PGR,SS
always have the best error, followed by HPGR, followed by RHR and HR. Figs. 2c and 2d show the
CDF of the empirical MSE over 300 independent trials, as discussed above. Figs. 2e and 2f is simi-
lar, but the y-axis denotes (cid:107)x − ˜x(cid:107)∞ instead of the MSE. Figs. 2g and 2h shows how the MSE varies
as ε is increased; in these last plots we do not include HPGR as one essentially one should select a
different q for each ε carefully to obtain a good tradeoff between runtime and error (as speciﬁed
by Lemma 4.1) due to round-up issues in powering q.

15

(a)

(c)

(e)

(b)

(d)

(f)

(g)

(h)

Figure 2: Error distributions from experiments.

16

Acknowledgments

We thank Noga Alon for pointing out the relevance of projective geometry for constructing the
type of set system our mechanism relies on.

References

[ASZ19]

Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Hadamard response: Estimating
distributions privately, efﬁciently, and with little communication. In Proceedings of
the 22nd International Conference on Artiﬁcial Intelligence and Statistics (AISTATS),
pages 1120–1129, 2019. 2, 3, 4, 5, 10

[BBGN19] Borja Balle, James Bell, Adri`a Gasc ´on, and Kobbi Nissim. The privacy blanket of the
shufﬂe model. In Alexandra Boldyreva and Daniele Micciancio, editors, Advances
in Cryptology – CRYPTO 2019, pages 638–667, Cham, 2019. Springer International
Publishing. 4

[BEM+17] Andrea Bittau, ´Ulfar Erlingsson, Petros Maniatis, Ilya Mironov, Ananth Raghunathan,
David Lie, Mitch Rudominer, Ushasree Kode, Julien Tinnes, and Bernhard Seefeld.
Prochlo: Strong privacy for analytics in the crowd.
In Proceedings of the 26th
Symposium on Operating Systems Principles, SOSP ’17, pages 441–459, 2017. 4

[BHO20]

Leighton Pate Barnes, Yanjun Han, and Ayfer ¨Ozg ¨ur. Lower bounds for learning
distributions under communication constraints via ﬁsher information.
Journal of
Machine Learning Research, 21(236):1–30, 2020. 20

[BNS19] Mark Bun, Jelani Nelson, and Uri Stemmer. Heavy hitters and the structure of local
privacy. ACM Transactions on Algorithms (TALG), 15(4):1–40, 2019. 2, 5

[BNST20] Raef Bassily, Kobbi Nissim, Uri Stemmer, and Abhradeep Thakurta. Practical locally

private heavy hitters. Journal of Machine Learning Research, 21(16):1–42, 2020. 2, 5

[BS15]

Raef Bassily and Adam Smith. Local, private, efﬁcient protocols for succinct his-
tograms. In Proceedings of the forty-seventh annual ACM symposium on Theory of
computing, pages 127–135, 2015. 2, 5

[CK ¨O20] Wei-Ning Chen, Peter Kairouz, and Ayfer ¨Ozg ¨ur. Breaking the communication-
In Proceedings of the 33rd Annual Conference on

privacy-accuracy trilemma.
Advances in Neural Information Processing Systems (NeurIPS), 2020. 2, 3

[CSU+19] Albert Cheu, Adam Smith, Jonathan Ullman, David Zeber, and Maxim Zhilyaev. Dis-
tributed differential privacy via shufﬂing. In Yuval Ishai and Vincent Rijmen, editors,
Advances in Cryptology – EUROCRYPT 2019, pages 375–403, Cham, 2019. Springer
International Publishing. 4

[EFM+19]

´Ulfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal Talwar,
and Abhradeep Thakurta. Ampliﬁcation by shufﬂing: From local to central differ-
In Proceedings of the Thirtieth Annual ACM-SIAM
ential privacy via anonymity.

17

[EPK14]

[FMT21]

[FT21]

[HKR12]

Symposium on Discrete Algorithms, SODA ’19, page 2468–2479, USA, 2019. Society
for Industrial and Applied Mathematics. 4

´Ulfar Erlingsson, Vasyl Pihurand, and Aleksandra Korolova. Rappor: Randomized
aggregatable privacy-preserving ordinal response. In Proceedings of the 2014 ACM
SIGSAC Conference on Computer and Communications Security (CCS), 2014. 1, 2, 3

Vitaly Feldman, Audra McMillan, and Kunal Talwar. Hiding among the clones:
In
A simple and nearly optimal analysis of privacy ampliﬁcation by shufﬂing.
Proceedings of the 62nd Annual IEEE Symposium on Foundations of Computer
Science (FOCS), 2021. arXiv:2012.12803 [cs.LG]. 4

Vitaly Feldman and Kunal Talwar. Lossless compression of efﬁcient private local ran-
domizers. In Proceedings of the 38th Annual Conference on International Conference
on Machine Learning (ICML), pages 3208–3219, 2021. 2, 3

Justin Hsu, Sanjeev Khanna, and Aaron Roth. Distributed private heavy hitters. In
International Colloquium on Automata, Languages, and Programming, pages 461–
472. Springer, 2012. 2

[KBR16]

Peter Kairouz, Keith Bonawitz, and Daniel Ramage. Discrete distribution estimation
under local privacy. arXiv preprint arXiv:1602.07387, 2016. 2

[SCB+21] Abhin Shah, Wei-Ning Chen, Johannes Balle, Peter Kairouz, and Lucas Theis. Op-
arXiv preprint

timal compression of locally differentially private mechanisms.
arXiv:2111.00092, 2021. 2

[TVV+17] Abhradeep Guha Thakurta, Andrew H. Vyrros, Umesh S. Vaishampayan, Gaurav
Kapoor, Julien Freudiger, Vivek Rangarajan Sridhar, and Doug Davidson. Learning
new words, 2017. US Patent 9,594,741. 1

[War65]

Stanley L Warner. Randomized response: A survey technique for eliminating evasive
answer bias. Journal of the American Statistical Association, 60(309):63–69, 1965. 2

[WBLJ17]

Tianhao Wang, Jeremiah Blocki, Ninghui Li, and Somesh Jha. Locally differentially
In 26th USENIX Security Symposium
private protocols for frequency estimation.
(USENIX Security 17), pages 729–745, Vancouver, BC, August 2017. USENIX Asso-
ciation. 2

[WHN+19] Shaowei Wang, Liusheng Huang, Yiwen Nie, Xinyuan Zhang, Pengzhan Wang,
Hongli Xu, and Wei Yang. Local differential private data aggregation for discrete
distribution estimation. IEEE Trans. Parallel Distributed Syst., 30(9):2046–2059, 2019.
2, 3

[YB17]

Min Ye and Alexander Barg. Optimal schemes for discrete distribution estimation
under local differential privacy. In Proceedings of the 14th Annual IEEE International
Symposium on Information Theory (ISIT), pages 759–763, 2017. 2, 3, 5

18

A Fast dynamic programming for PI-RAPPOR

In this section, we describe an adaptation of our dynamic programming approach to PI-RAPPOR.
First, we brieﬂy review the construction of PI-RAPPOR. We use Fq with the ﬁeld size q close to
eε + 1. Let t be the minimum integer such that k (cid:54) qt.
We identify the k input values with vectors in Ft

q. Let x ∈ Zqt

q. For each input v, we deﬁne a set S(v) ⊂ Ft

denote the input frequency vector
q × Fq

i.e. xv is the number of users with input v ∈ Ft
where (a, b) ∈ S(v) if and only if (cid:104)a, v(cid:105) + b = 0.

Each user with input v sends a random element e of Ft

q × Fq with probability eε p if e ∈ S(v)
eεqt+(q−1)qt . The server keeps the counts on the received
q × Fq. The total storage is O (cid:0)qt+1(cid:1). We estimate

1

and probability p if e (cid:54)∈ S(v). Thus, p =
elements in a vector y indexed by elements of Ft
the frequency vector x by computing

(cid:32)

˜xv = α

∑
u,w:(cid:104)u,v(cid:105)+w=0

yu,w

(cid:33)

+ β ∑
u,w

yu,w

where α and β are chosen so that this is an unbiased estimator. This condition implies two equa-
tions:

α

eεqt
eεqt + (q − 1)qt
eεqt−1 + (q − 1)qt−1
eεqt + (q − 1)qt

α

+ β = 1

+ β = 0

We obtain

eεq + (q − 1)q
(eε − 1)(q − 1)
eε + (q − 1)
(eε − 1)(q − 1)
Next, we describe a fast algorithm to compute ˜x with running time O (cid:0)tqt+2(cid:1). Speciﬁcally, for

β = −

α =

a ∈ Fj

q, b ∈ Ft−j

q

, z ∈ Fq, deﬁne

fj(a, b, z) =

∑
pre fj(u)=a
(cid:104)sufft−j(u),b(cid:105)+w=z

yu,w,

where prefi(u) denotes the length-i preﬁx vector of u, and suffi(u) denotes the length-i sufﬁx
vector of u. Then, we would like to compute

(cid:32)

˜xv = α

∑
u,w:(cid:104)u,v(cid:105)+w=0

yu,w

(cid:33)

+ β ∑
u,w

yu,w = α ∑

f0(⊥, v, 0) + βn,

w

for all v ∈ Ft
q, where ⊥ denotes the length-0 empty vector. We next observe that f satisﬁes a
recurrence relation, so that we can compute the full array of values f0(⊥, v, w) efﬁciently using
dynamic programming and then efﬁciently obtain ˜x ∈ Rk. We have

19

fj(a, b, z) =

∑
pre fj(u)=a
(cid:104)sufft−j(u),b(cid:105)+w=z

yu,w

=

=

q−1
∑
i=0

q−1
∑
i=0

∑
pre fj+1(u)=a◦i

yu,w

(cid:104)sufft−j−1(u),sufft−j−1(b)(cid:105)+w=z−i·b1

(mod q)

fj+1(a ◦ i, sufft−j−1(b), (z − i · b1) mod q)

Note that we have the base cases ft(a, ⊥, w) = ya,w. We need to compute the values of fj(a, b, z)
, z ∈ Fq and each value takes O(q) time so the total running

q, b ∈ Ft−j

q

for j ∈ {0, 1 . . . , t − 1}, a ∈ Fj
time is O(tqt+2).

B The public coin setting

We show that versions of PGR and HPGR can be implemented in the public coin setting in a
way that the communication is (cid:100)log2 q(cid:101) = ε log2 e + O(1) bits, which is asymptotically optimal to
achieve asymptotically optimal utility loss [BHO20, Corollary 7]. We begin with PGR.

Recall that as described, PGR associates each of the k input values with a canonical vector in
Ft
q. In the public coin variant we now describe, we further assume that the canonical vectors have
a non-zero last coordinate. This can be ensured by picking q, t such that k (cid:54) 1 + (1 − 1/q)((qt −
1)/(q − 1) − 1) = qt−1. We will use Cq,t to denote the set of canonical vectors in Ft
q,t to
denote those with a non-zero last coordinate.

q and C∗

With this setup, recall that each output in the set Sv can be associated with a vector u ∈ Cq,t
such that (cid:104)u, v(cid:105) = 0. Thus a user with input v sends a vector u ∈ Cq,t with probability eε p if
(cid:104)u, v(cid:105) = 0 and with probability p otherwise. For a vector u, let pre ft−1(u) denote its length (t − 1)
preﬁx. Note that for a vector u ∈ Cq,t, either pre ft−1(u) is itself a canonical vector in Cq,t−1, or
u = u∗ de f

= (0, . . . , 0, 1). Also note that for any v ∈ C∗

q,t, u∗ (cid:54)= Sv.

This then suggests the following algorithm. We use public randomness to select a vector w ∈
such that w = (0, . . . , 0) with probability p, and w is a random vector in Cq,t−1 otherwise.

Ft−1
q
Thus there are 1 + qt−1−1
a ∈ Fq such that (cid:104)v, w · a(cid:105) = 0 mod q. When w (cid:54)= (0, . . . , 0), a user with input v ∈ C∗
message a with probability
If w = (0, . . . , 0), the user always send 1.

q−1 possible values of w. Given a w ∈ Cq,t−1 and a v ∈ C∗

eε+q−1 if (cid:104)v, w · a(cid:105) = 0 mod q, and with probability

q,t, there is a unique
q,t sends
eε+q−1 otherwise.

eε

1

The server given w derived from the shared public randomness, and the message a ∈ Fq,

decodes it as

Dec(w, a) = w · a.

We claim that the distribution of Dec(w, a) is identical to the output in the private coin PGR. First
observe that by construction, Dec(w, a) ∈ Cq,t. Next notice that for any u, u(cid:48) ∈ S(v), we have
P(Dec(w, a) = u) = P(w = pre ft−1(u)) · P(a = ut | w = pre ft−1(u))

20

= P(w = pre ft−1(u)) ·

= P(w = pre ft−1(u(cid:48))) ·

eε
eε + q − 1
eε
eε + q − 1
= P(w = pre ft−1(u(cid:48))) · P(a = u(cid:48)
= P(Dec(w, a) = u(cid:48)).

(by uniformity of w over canonical vectors)

t | w = pre ft−1(u(cid:48)))

Similarly, for any u, u(cid:48) ∈ Cq,t \ Sv such that u, u(cid:48) (cid:54)= u∗, we can write

P(Dec(w, a) = u) = P(w = pre ft−1(u)) · P(a = ut | w = pre ft−1(u))

= P(w = pre ft−1(u)) ·

= P(w = pre ft−1(u(cid:48))) ·

1
eε + q − 1
1
eε + q − 1
= P(w = pre ft−1(u(cid:48))) · P(a = u(cid:48)
= P(Dec(w, a) = u(cid:48)).

(by uniformity of w over canonical vectors)

t | w = pre ft−1(u(cid:48)))

Further, an identical calculation shows that for u ∈ Sv, u(cid:48) ∈ Cq,t \ Sv with u(cid:48) (cid:54)= u∗, P[Dec(w, a) =
u] = eε · P(Dec(w, a) = u(cid:48)). Moreover, the distribution of w ensures that P(Dec(w, a) = u∗) = p.
It follows that for all u ∈ Cq,t, P(Dec(w, a) = u) is eε p if u ∈ Sv and p if u ∈ Cq,t \ Sv.

In other words, we have shown how to simulate the output distribution of PGR in the public

coin setting while sending only a single element from Fq.

An implementation of HPGR in the public coin model is similar. A message in HPGR is a pair
(j, u) where j ∈ {1, . . . , h} is the index of a block, and u ∈ Ft
q is the name of a canonical vector,
and as above in the public coin setting we will forbid u from being the all-zeroes vector (so that
now we need hqt−1 (cid:62) k). As described in Section 4, h, q are chosen so that hq ≈ eε + 1. In the
public coin model, the user selects j using private randomness and sends it explicitly then uses
the PGR public coin protocol described above to determine the ﬁrst t − 1 entries of u with no
communication required, then sends the ﬁnal entry of u to obey the HPGR distribution. The total
communication is (cid:100)hq(cid:101) = ε log2 e + O(1) bits.

21

