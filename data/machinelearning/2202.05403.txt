2
2
0
2

b
e
F
1
1

]

G
L
.
s
c
[

1
v
3
0
4
5
0
.
2
0
2
2
:
v
i
X
r
a

LEARNING TEMPORAL RULES FROM NOISY TIME-
SERIES DATA

Karan Samel ∗
Georgia Tech
ksamel@gatech.edu

Zelin Zhao
CUHK
sjtuytc@gmail.com

Binghong Chen
Georgia Tech
binghong@gatech.edu

Shuang Li
CUHK Shenzhen
lishuang@cuhk.edu.cn

Dharmashankar Subramanian
IBM Research AI
dharmash@us.ibm.com

Irfan Essa
Georgia Tech
Google
irfan@gatech.edu

Le Song
Biomap
MBZUAI
dasongle@gmail.com

ABSTRACT

Events across a timeline are a common data representation, seen in different
temporal modalities. Individual atomic events can occur in a certain temporal
ordering to compose higher level composite events. Examples of a composite event
are a patient’s medical symptom or a baseball player hitting a home run, caused
distinct temporal orderings of patient vitals and player movements respectively.
Such salient composite events are provided as labels in temporal datasets and
most works optimize models to predict these composite event labels directly. We
focus on uncovering the underlying atomic events and their relations that lead to
the composite events within a noisy temporal data setting. We propose Neural
Temporal Logic Programming (Neural TLP) which ﬁrst learns implicit temporal
relations between atomic events and then lifts logic rules for composite events,
given only the composite events labels for supervision. This is done through
efﬁciently searching through the combinatorial space of all temporal logic rules
in an end-to-end differentiable manner. We evaluate our method on video and
healthcare datasets where it outperforms the baseline methods for rule discovery.

1

INTRODUCTION

Complex time series data is present across many data modalities such as sensors, records, audio, and
video data. Typically there are composite events of interest in these time series which are composed
of other atomic events in a certain order (Liu et al., 1999; Chakravarthy et al., 1994; Hinze, 2003).
An example is a health symptom that can be observed in a doctor’s report. Atomic events, such as
patient vitals and medications, and their temporal relations dictate an underlying causal rule leading
to the composite event symptom. These rules may be unknown but useful to recover (Kovaˇcevi´c et al.,
2013; Guillame-Bert et al., 2017).

Recent methods leverage the advances in highly parameterized deep architectures to learn latent
representations of atomic event data (Pham et al., 2017; Chen et al., 2018; Choi et al., 2019),
with the increasing availability of large temporal datasets. Methods, such as LSTM (Hochreiter
& Schmidhuber, 1997) or Transformer (Vaswani et al., 2017) based architectures, provide state-
of-the-art performance in terms of composite event inference. However, it is uncertain whether
the latent representations learn the underlying causal sequence of events or overﬁt spurious signals
in the training data. Having representations faithful to causal mechanisms is advantageous for

∗Work partially conducted during an internship IBM Research

1

 
 
 
 
 
 
Figure 1: In ILP, the grounded background knowledge and known relational predicates between atoms
are provided to induce consistent rules. In the temporal case, we are operating over raw temporal
data samples, such as videos, with potentially multiple labels. Therefore the latent temporal structure
between the atomic events is recovered, and then the rules for composite events are learned.

interpretability, out-of-distribution generalization, and adapting to smaller data sets. Therefore it is
important to leverage parametric models that can handle data noise while providing a mechanism to
extract explicit temporal rules (Carletti et al., 2019).

Extracting explicit logic rules has been studied through Inductive Logic Programming (ILP)
methods (Muggleton, 1991; Muggleton & De Raedt, 1994) and have been leveraged in para-
metric fashions as well (Yang et al., 2017; Evans & Grefenstette, 2018; Rocktäschel & Riedel,
2017). ILP starts with set of background knowledge, consisting of grounded atoms (i.e. facts
which do not contain variables) such as location(Braves, Atlanta), where the predi-
cate location determines the relationship between the items Braves and Atlanta. There
are set of labels from which rules should be learned. The task is to construct a set of
rules, when executed over the background knowledge, entail the provided labels. Given the
label InLeague(Braves, NL East) and the background knowledge (Figure 1 ILP Input)
as input, a candidate rule is InLeague(Team, League) := Location(Team, City) ∧
Division(City, League). Here InLeague(Team, League) is the head of the rule con-
sisting of an atom with variables Team, League as items. The body consists of two atoms and
when these atoms exist on the background knowledge the rule is evaluated as true.We apply ILP over
real world temporal data, however learning such rules poses three key challenges.

Temporal Background Knowledge First, ILP methods operate over an existing grounded back-
ground knowledge. The temporal case does not have this knowledge when operating over raw time
series. For example in a baseball video, grounded atomic events pitch or swing, or grounded
predicates such as before(pitch, swing) are not explicitly provided. By nature, the video
would be labeled with a higher level composite event description, such as "Player A’s home run"
instead of individual atomic events and their corresponding temporal predicates. Such atoms can be
extracted using a model in a probabilistic fashion at each time point, and a temporal ILP method
should handle this uncertainty. The temporal predicates between these probabilistic atomic events can
be applied in a rule-based manner (ex. t1 < t2 → before), but due to the noisy nature of extracted
atomic events, the predicate predictions should be robust to consistent noise in the atomic event data.

Atomic Event Relevance Second, ILP works learn consistent rules that satisfy a path in the
background knowledge given the terms in the labels, such as InLeague(Braves, NL East).

2

ILPBravesAtlantaMetsNYC, QueensNL EastlocationlocationdivisondivisonPredicate LabelsInputInLeague(Braves, NL East)InLeague(Mets, NL East)Learned RulesInLeague(Team, League) := Location(Team, City) ^ Division(City, League)Temporal ILPPitchSwingbeforeLearned Temporal StructureHitRunbeforeafterbeforePitchSwingbeforeMissRunbeforebeforeComposite Event LabelsInputssteal, home runsteal, strikeLearned RulesSteal := Before(Run, Pitch)Strike := Before(Pitch, Swing) ^ Before(Swing, Miss)Atomic EventThe labels are nullary predicates in the temporal case, so the relevant source and target atomic events
and predicates to use for rule induction are unknown. In our example, we know from the video
we have a label strike, but are not told when it occurred or what other events, such as pitch,
swing, and miss are needed to compose a rule for strike.

Without a prior on which atomic events to search from, we must consider all pairwise temporal
relations between atomic events in the input. This leads to a combinatorial search of all pairwise
events for each predicate in the temporal rule body.

Multi-Event Labels Third, ILP domains work on disjoint labels, while in time series, multiple
composite events could occur in each input. In our baseball video, such as a highlight reel, composite
event labels strike, steal and their corresponding atomic events can co-occur in a single video.
This further extends the search space of atomic events we consider for each composite event rule.
We illustrate these differences in Figure 1 and further discuss these challenges regarding search
complexity in Appendix A. To address these challenges, Neural TLP operates on two key steps.

Parameter Learning First Neural TLP inputs probabilistic atomic events and learns parameters to
infer temporal predicates between atomic events. We represent the atomic event data in an interval-
based representation to efﬁciently predict all pairwise predicates between atomic events. The inferred
predicates are then projected to predict the composite event labels.

Structure Learning When the predicate parameters are learned, Neural TLP learns a sparse vector
to select the correct rule over the combinatorial space of possible rules. To prune the search space,
we use the learned projected weights to select candidate grounded predicates per composite event.

We evaluate our method on a synthetic video dataset to empirically test our temporal rule induction
performance. Additionally, we apply our framework to provide relevant rules in the healthcare
domain, which were veriﬁed by doctors.

2 PROBLEM FORMULATION

u=1 ⊆ X , and their associated time intervals Tr = {tu}N

We deﬁne the complete set of atomic events X = {x1, x2, . . . , x|X |} along a timeline T . These
atomic events can be existing features in time series data or user deﬁned features of interest. A
temporal logic rule r(Xr, Tr) can be deﬁned as using a subset of N ≤ |X | atomic events Xr =
{xu}N
u=1 ⊆ T . The time intervals consists
of start and end times tu = [tustart, tuend].
These intervals indicate durational events and we can also initialize instantaneous events occurring at
one time point where tustart = tuend. A rule is evaluated as true if the corresponding atomic events xu
are present and are in correct ordering with respect to the intervals tv of other events xv:
(cid:94)

(cid:94)

r(Xr, Tr) := (

(cid:94)
(

xu)

pi(tu, tv))

The temporal predicates pi ∈ {before, during, after} = P represent a simpliﬁed subset
of Allen’s Temporal Algebra (Allen, 1983). We simplify the notation of the rules as a conjunction of
temporal predicates between observed events, where the event time intervals are implicit:

xu∈Xr

tu,tv∈Tr

n
(cid:94)

r :=

xu,xv∈Xr

pi(xu, xv)

(1)

For example, the grounded predicate before(pitch[2,2.7], swing[3,3.5]) would evaluate to true.
These underlying causal rules r induce the composite event labels r → yr seen in the data. Multiple
composite events of interest can co-occur during the same time series sample T which we denote as
y = {yr}|R| ∈ {0, 1}|R|. Any yr = 1 indicates the latent rule r occurred over T resulting in label
yr.
While T contains precise atomic event interval information, the observed time series ˜T consists
of a sequence of probabilistic atomic events from times [1, T ]. Potentially k different objects ˜T i

3

Figure 2: The ﬁrst step (1) of Neural TLP involves learning the convolution and predicate model
parameters from the raw time series and labels. Then the structure learning step (2) is learning
attention s over a sparse combinatorial matrix to infer the labels. This attention and sparse matrix is
then used to carry out the ﬁnal rule induction (3).

compose the ﬁnal time series data ˜T = (cid:83)k
˜T i. Examples of objects can be multiple concurrent
sensor data, or tracking multiple people moving within a video. Then the input ˜T is formulated as
MT ∈ [0, 1]k×|X |×T across object, atomic event, and probability dimensions respectively.

i=1

The temporal ILP task is to recover all underlying rules R given m samples of inputs and labels
{(MTi, yi)}m
i=1. In Neural TLP this involves learning parameters for the predicates between atomic
events and then learning the combination of grounded predicates that induce each r ∈ R.

3 NEURAL TEMPORAL LOGIC PROGRAMMING

Neural TLP operates in two stages. The parameter learning stage learns how to compress the temporal
data and learns parameterized temporal predicates. Once these parameters are learned, the structure
learning stage learns which conjunctive combination of pairwise atomic event predicates is associated
with each composite event label. This conjunction composes the rule r for label yr and is jointly
computed for all R. An overview of the framework is presented in Figure 2.

3.1 PARAMETER LEARNING STAGE

Temporal Compression Starting from the raw probabilistic atomic event data, we ﬁrst compress
the timeline through convolution. This 1D convolution over the temporal dimension compresses and
smooths the timeline to mitigate noise from spurious events. Here the convolution kernel K|X |×l of
length l is learned per atomic event. We also parameterize α as an extra degree of freedom to scale
these convolved scores, which is useful when computing the intermediate predicates downstream.

MC ∈ Rk×|X |×t = α · conv_1D(MT ∈ [0, 1]k×|X |×T , K)

(2)

The time information is incorporated by multiplying the time dimension MD into compressed events:
MA ∈ Rk×|X |×t = MC (cid:12) MD. Here MD has the same dimensions as MC, but the temporal
dimension is enumerated from [1, t], where MD:,:,l = l. This can be thought as a positional encoding.
For example if we look at the sample compressed scores for a single object i and atomic
event j MCi,j,6:10 = [.01, .05, .7, .7, .03] and MDi,j,6:10 = [6, 7, 8, 9, 10] then MAi,j,6:10 =
[.06, .35, 5.6, 6.3, .3]. Intuitively we can see that from MAi,j,6:10 that (1) atomic event j occurs
when the scores are high at 5.6 and 6.3 and that (2) score 6.3 occurs after score 5.6 due to the
multiplied time index. This temporal representation provides a path to compute precise time intervals
of atomic event occurrences and deﬁne predicates to compare atomic event intervals.

4

conv 1Dprobabilistic atomic eventscompression and smoothing[6, 7.9][.8, 2.4][4.1, 6]interval extractionpredicatepredictiondense inferencesparse selection(1) parameter learning(2) structure learning(3) rule inductionNeural TLPOverviewtimetimeFigure 3: An overview of how intervals are computed from raw data. First the compressed atomic
event scores (1) are multiplied with the time scalar (2) to compute MA (3), where we observe a single
sample vector ti
u, representing the end of the interval, and use
this value to initialize the mask in step 5 (Equation 3). When steps 4 and 5 are summed in step 6, we
get a representation whose min corresponds to the start of the interval as shown in Equation 4.

u. In step 4 we ﬁnd the max value of ti

Predicate Modeling From the compressed timelines, we determine the temporal predicates be-
tween atomic events. These relations are computed in a pairwise manner for all atomic events
∀xu, xv ∈ X occurring in object i through a small network which we call Temporal Predicate
Network (TPN). For notation sake here, we represent the atomic event u’s timeline for object i
u = MAi,u,: ∈ Rt and correspondingly for atomic event v. We denote TPN as gθ(ti
as ti
v),
which takes pairwise atomic event timelines and predicts a temporal predicate p ∈ P to indicate the
relationship between the atomic events.

u, ti

Methods such as Temporal Relation Networks (Zhou et al., 2018) learn these predicates between
video events by sampling frames throughout the video. The timelines can be long in our setting, and
events can occur sparsely, making sampling timelines expensive and noisy. To efﬁciently compute
these relations, we would like to recover each event’s underlying start and end time intervals. From
intervals, we can encode strong inductive biases to predict the predicates.We are working with
continuous time series scores in ti
v, so the intervals have to be extracted as the ﬁrst step in TPN.
To compute the start of an event interval, we create a mask to identify the atomic event noise.
Those values will be below some small value (cid:15), corresponding to noise in the timeline. We learn the
convolution scalar α from Equation 2 to scale scores corresponding to active atomic event occurrences
above (cid:15) while keeping scores corresponding to atomic event noise below (cid:15). Then the mask is added to
the time series, and a min is performed to get the start of the active atomic event interval. Afterwards
the min of the mask is subtracted to remove any effect of the mask on the start value.

u, ti

tmask = (max(ti
ustart = min(ti

u) + (cid:15)) · (ti
u + tmask) − min(tmask)

u < (cid:15))

(3)

(4)

To get the end of the event interval we simply compute uend = max(ti
u) since we multiplied the event
scores with the time index earlier. This interval computation from the input time series is visualized
in Figure 3. This is computed similarly for the other pairwise event v: [vstart, vend]. Given the start
and end times for the event pairs u, v, the un-normalized predicate scores are computed as:

before(u, v) = vstart − uend
after(u, v) = ustart − vend

during(u, v) = min({vend − ustart, uend − vstart})

(5)
(6)
(7)

Although we use 3 predicates in our model, similar scores can be developed for more ﬁne grained
predicates. Then the values are aggregated as p = [before(u, v); during(u, v); after(u, v)] to compute
normalized predictions as p = softmax( p−β
γ ). Here β and γ and scale and shift parameters learned
from data. Our predicates scores assume that intervals for both u and v occur, so if either event

5

t t t t max1t0tmaxtmaxmin12 3456event probtimeFigure 4: In the parameter learning stage (1) we learn the most relevant grounded predicates used for
predicting each label yr through W. In the structure learning stage (2) we select the most relevant
predicates and construct the combinatorial matrix C, where each column indicates a conjunction of
predicates. Vector s is learned to select the most likely conjunction to induce the rule r.

doesn’t occur we suppress all predicate predictions:

supp = min({uend − ustart, vend − vstart})

pi = min({pi, supp})

(8)
(9)

Since we leverage a simple interval representation to compare atomic event objects, we can scale
comparing atomic events within the object and between the other k−1 objects: xu ∈ X , xv ∈ (X ×k).
This second-order interaction information is useful if we want to know if, for example, two events
occurred simultaneously within different objects. For a single object i, these relations are computed for
all pairwise predicates through TPN in MP ∈ R|X |×(|X |×k)×|P| = Rk×|X |×|X |×|P|. Aggregating
over all objects k, we get MQ = [MP1; . . . ; MPk] ∈ Rk2×|X |×|X |×|P|. We marginalize over the
object dimension to get our ﬁnal pairwise relation matrix MR = (cid:80)

i MQi,:,:,: ∈ R|X |×|X |×|P|.

Composite Event Prediction The ﬁnal inference step from the pairwise relational predicates to
the composite events labels is carried out by fφ. This is a linear projection function fφ(MR) :=
σ(dropout(vec(MR))W) used to infer the composite event labels ˆy.
Here we ﬂatten MR as vec(MR) ∈ R|X |·|X |·|P| and regularize it by randomly masking out the
grounded predicates (Srivastava et al., 2014). This representation is then projected to the label space
using W ∈ R(|X |·|X |·|P|)×|y| before passing the un-normalized results through a sigmoid function
σ. W learns what grounded relational predicates pi(xu, xv), such as before(pitch, swing),
correspond to each composite event label. These weights will also be useful for extracting the rules,
in the structure learning stage.

3.2 STRUCTURE LEARNING STAGE

Predicate Selection To induce the logic rules R, one method is to look at our projection weights
W. Here we can select the highest weighted entries corresponding to grounded predicates for each
label yr. One can use a conjunction of these predicates to construct the rule r for yr. However, in
most cases, the number of predicates needed to compose a rule is unknown apriori. Additionally,
setting thresholds for information gain splits is heuristic-based and error-prone, especially when one
cannot observe the underlying rules for veriﬁcation.

Instead of setting thresholds, we directly optimize over the space of

Combinatorial Inference
combinatorial rules to infer our composite event label yr.
Starting from the predicted grounded predicates vec(MR) ∈ Rd where d = |X | · |X | · |P| we
initialize a combinatorial matrix up to a max rule body length n:

C = [C1; ...; Cn] ; Ci ∈ Rd×(d
i)

(10)

6

grounded predicates index123(1) Paramter Learning(2) Structure Learning123i

Here for each unique column in Ci will have indicators for the i chosen predicates, corresponding to
one possible combination. Since (cid:0)d
(cid:1) can be quite large, we sample the top c < d predicate weights in
W:,yr . Then combinations can be initialized over those c predicate indices Ci ∈ Rd×(c
i).
Selecting the most relevant combination across all combinations is done through an attention vector
a = softmax(s) where s ∈ R(cid:80)n
j ajC:,j
and the label can be inferred by ˆyr = c(cid:62)vec(MR), thus ˆy = [ ˆyr for each r ∈ R]. Note that we
maintain separate attention parameters s and unique C for each label yr ∈ y, since each rule relies
on different predicates.

i). It is used to weight the combinations c ∈ Rd = (cid:80)

i=1 (c

Rule Induction To extract the rule we simply choose the column in Cj where j = arg max s
corresponds to the maximum attention value. Each indicator value i in Ci
j correspond to a grounded
temporal predicate pi ∈ P between two events xi

v ∈ X, which are used to construct a rule:

u, xi

r :=

n
(cid:94)

i=1

pi(xi

u, xi
v)

for pi, xi

u, xi

v ∈ predicate(Ci
j)

(11)

This is followed for every composite event label yr to provide our ﬁnal set of rules R. This full
process is illustrated in Figure 4.

3.3 OPTIMIZATION

Now that we have deﬁned our inference procedure to obtain ˆy from both parameter and structure
learning stages, we describe the overall training. To train over the data {(MT, y)}m
i=1, each stage
minimizes the standard cross entropy loss between y and ˆy, denoted as Lent, using the Adam
optimizer (Kingma & Ba, 2014) (details in Appendix B).

The parameter learning stage is trained ﬁrst over the convolution α, K, predicate β, γ, and projection
W parameters. For W we add L1 regularization (denoted as L1) and project the weights between
[0, 1] to mimic logic weights (Chorowski & Zurada, 2014; Riegel et al., 2020). We also constrain
the convolution scalar α between [0, 1]. This gives us our ﬁnal stage objective to optimize L =
Lent + λ1L1 where λ1 = 0.1. The structure learning stage is trained next to optimize Lent over all
attention vectors s corresponding to each label yr, while freezing all other parameters. Each stage is
trained in an end-to-end differential manner, after which the temporal logic rules R are induced.

4 EXPERIMENTS

4.1 CATER

We explore composite event prediction over complex videos in the CATER dataset (Girdhar &
Ramanan, 2019). CATER consists of videos containing objects moving around a scene. The object
movements correspond to |X | = 14 distinct atomic events. Every combination of predicates between
atomic events yields a rule of length n = 1, and when this combination occurs in the video, it induces
that corresponding label. There are |R| = 301 rules to recover and the average number of labels per
video ¯y = 1
m

yr = 53 out of 301.

(cid:80)m

(cid:80)

yr∈yi

i=1

In previous works on the CATER, the main metric is mean average precision (mAP) over the labels.
Due to its synthetic nature, we know the ground truth rules used to induce the labels, so we can
also empirically evaluate how well our models can recover these rules in its top k rule predictions
(Hits@k). The overall task for CATER is illustrated in Figure 5.

Baselines We test Neural TLP against two baselines. For the ﬁrst baseline, we input our MT
matrix into attention-based LSTM (Hochreiter & Schmidhuber, 1997) and predict the composite
events. Since we are dealing with only single predicate rules, we synthesize each rule combination
within MT and assign it to the highest weighted label.

We deﬁne the second baseline as Temporal MAP, which uses the same Neural TLP model. We freeze
the parameters and the weight W is a count of co-occurring grounded relational predicates and labels.

7

Figure 5: In CATER, generative rules are used to synthesize the labels from the videos. Neural TLP
can then learn these rules from the raw atomic event data and labels. We then verify our rule induction
performance over the ground truth rules.

This setup is akin to processing atomic event relations deterministically and computing W through
MLE. The rule extraction follows the same methods as Neural TLP, and additional baseline details
are laid out in Appendix C.

4.1.1 RESULTS

Model

Hits@1 Hits@5

LSTM
LSTM Attn
Temporal MAP
Neural TLP

.00
.00
.27
.91

.04
.04
.28
.95

Table 1: Hits when the inputs are inferred
(probabilistic) the atomic events. All scores
have a reported variance of ≤ .01.

Model

I3D/R3D
TSN
TSM

Inputs

ResNet Features + Optical Flow
RGB Difference + Optical Flow
ResNet Features

LSTM Attn
Neural TLP

Inferred Atomic Events
Inferred Atomic Events

mAP

.44
.64
.73

.75
.69

Table 2: mAP scores versus video baselines.

Model

¯y = 53 (orig)

¯y = 40

¯y = 30

¯y = 20

¯y = 10

LSTM Attn
Neural TLP

.75
.69

.37
.49

.34
.47

.31
.45

.27
.42

Table 3: We observe mAP performance when testing on out of distribution data with respect to the
average number of labels per sample.

We experiment where the atomic events are obtained from a noisy environment or inferred from
a process upstream, such as our baseball video. To infer the atomic events, we detect the objects
through a Faster R-CNN (Ren et al., 2015) and use its cropped image feature and optical ﬂow to
predict the shape and movement. These atomic events are predicted and cached prior to inputting
them in our models.

From the results in Table 1 we see that our more structured method is the most optimal for extracting
rules. MAP provides a coarse representation of the labels and enumerated rules with no parameters.
It can express these rules better than LSTMs but lags behind our method. As the number of free
parameters increases, the LSTM models are more likely to pick up spurious signals in the data that
are useful from a cross-entropy optimization perspective but deviate from the underlying generative
rule representation (Hits). This can be seen with highly parameterized LSTM models and video
models: I3D (Carreira & Zisserman, 2017), TSN (Wang et al., 2016), and TSM (Lin et al., 2019),
leading to larger gains in mAP in Table 2.

8

contain conepick-place cubepick-place conerotate snitch...VideoLabels12,2,4,94,32,56,Neural TLPInputsGround Truth RulesGenerateRecover + VerifyEven for mAP, we show that our underlying rule representation is useful when generalizing out of
distribution. Here we ﬁx our trained LSTM and Neural TLP models and test on out of distribution
data where the frequency of labels is changed in Table 3. We show that Neural TLP performance
degrades gracefully as it is exposed to out of distribution data.

We further ablate our Neural TLP model architecture and hyperparameters in D.2. We show the
effectiveness of our method on recovering longer dynamic length rules over Temporal MAP in
Appendix D.3. Now that we tested Neural TLP on synthetic tasks to empirically verify the rule
accuracy, we explore its capabilities on real-world healthcare data.

4.2 HEALTHCARE DATA

Model

#@50 MRR

Model

Urine Output mAP

Neural TLP
Temporal MAP

3
0

.04
0

Logistic Regression
LSTM Attn (L)
Neural TLP

.75
.74
.77

Table 4: We compute the number of relevant
rules in top 50 (#@50) as well as the mean
reciprocal ranking (MRR) of the correct rules.

Table 5: Inference results for the urine output
task.

We test the rules recovered from Neural TLP on patient data in MIMIC-III (Johnson et al., 2016).
We speciﬁcally look at 2023 patients admitted for sepsis (severe infection) and recover the rules
corresponding to stable vitals. This is done through predicting urine output as the composite event, an
auxiliary variable indicative of the state of the patient’s ﬂuids and circulatory system (Komorowski
et al., 2018). There are |X | = 82 different atomic events, composed of drugs administered and
patient vitals. The vitals are made into boolean events through logic rules provided by doctors, which
indicate the vital severity: low, normal, or high. The model’s task is to learn rules corresponding to
normal urine outﬂow. We present the top rules from Neural TLP and MAP to doctors for veriﬁcation.

From the results in Table 4, we see this is a difﬁcult problem due to a large number of atomic events
and small sample size. However, it indicates that the learning done by Neural TLP is useful to learn
and to rank important predicates before rule training. Temporal MAP fails at this task with the
increased number of atomic events and longer timelines, which led to many grounded predicates
(2-3k) per sample. Therefore the W weights are very coarse and ﬁlled with common relations. In
addition to relevant rules, Neural TLP maintains good inference performance (Table 5), which is also
an important metric for this domain.

From the doctors’ feedback, we also present the rules learned in Appendix E, but emphasize that these
are observed facts. This means that the rules are partially explained by a subset of predicates and
not considered treatment rules, as the rules didn’t contain any drugs administered. The doctors did
conﬁrm that we captured important explanatory variables in our proposed rules, so we are optimistic
about using our framework for feature selection in complex temporal environments.

5 RELATED WORK

Structured Temporal Prediction To optimize over the composite events given observed atomic
events, Hidden Markov Models (HMMs) are commonly used to model the latent rules that emit
consistent atomic events to induce the composite events. Variants of HMMs have been developed to
handle symbolic atomic events (Mutschler & Philippsen, 2012; Kersting et al., 2006; Liu et al., 2017)
to more perception-based events, such as videos (Tang et al., 2012). Deep models have shown to
incorporate temporal logic constraints within their outputs by leveraging Transformers architectures
(Finkbeiner et al., 2020), knowledge distillation (Ma et al., 2020), and by representing temporal logic
as a differentiable loss (Innes & Ramamoorthy, 2020). However, interpreting latent representations
of deep models in order to extract explicit rules is still being researched (Arras et al., 2019; Chefer
et al., 2021; Lal et al., 2021).

Rule Representations and Inference To explicitly induce rules over data, a space temporal logic
rules can be searched to determine appropriate rules. Temporal rules are induced using satisﬁability

9

(SAT) based methods (Neider & Gavran, 2018; Camacho & McIlraith, 2019) given strong priors
to the rule structure and limited data noise. In the presence of noise, softening logic rules using
Markov Logic Networks (Richardson & Domingos, 2006; Song et al., 2013) or probabilistic logic
(ProbLog) (De Raedt et al., 2007; Kimmig et al., 2011) perform approximate inference well, but
are intractable for rule training. Recently Yan & Julius (2021) proposed a network to learn sparse
weights for temporal rules given a ﬁxed rule format.

Inductive Logic Programming Extracting expressive rules is explored through Inductive Logic
Programming (ILP) methods (Muggleton, 1991; Muggleton & De Raedt, 1994) and within Statistical
Relational Learning literature (Koller et al., 2007; De Raedt & Kersting, 2010). Given ﬁxed back-
ground knowledge, parameterized models softly select the relevant facts used to derive the labels
and therefore lift logic rules (Yang et al., 2017; Evans & Grefenstette, 2018). Atoms can also be
represented as latent vectors to provide probabilistic facts for increased generalizability (Rocktäschel
& Riedel, 2017). Dong et al. (2019) present a more neural architecture to represent ﬁrst-order logic
and scale to larger rule search spaces.

6 CONCLUSION

Composite event extraction is a common task across many temporal domains. It is important to
understand the underlying atomic events and their predicates that induce the composite event. We
propose Neural TLP that learns these composite event rules even when provided noisy temporal data.
It ﬁrst learns parameters for atomic event timeline compression and pairwise predicate prediction.
Then once grounded predicates are reliably inferred, the structure learning stage learns over the space
of combinatorial predicates to induce the ﬁnal rules. We veriﬁed our method on synthetic video tasks
and explored rule recovery in a real-world healthcare dataset.

10

Ethics Statement Neural TLP, like any data-driven model, has potential societal impacts that
could include extracting unfairly biased rules or relations. In such cases, we envision using such a
framework for knowledge discovery over end decision making. We operate on hospital patient data,
through MIMIC-III which has been de-identiﬁed to avoid leaking privileged patient information.

Reproducibility Statement We provide the entire model code for our framework in the supple-
mentary materials. The dataset for CATER is publicly available, and we have provided the processing
code to generate our custom CATER data. We are in the process of sharing the experimental code for
the MIMIC-III experiments as well. We further describe our optimization procedures (Appendix B),
hyperparameters (Appendix D.2), and baseline model architectures (Appendix C) for clarity.

REFERENCES

James F Allen. Maintaining knowledge about temporal intervals. Communications of the ACM, 26

(11):832–843, 1983.

Leila Arras, José Arjona-Medina, Michael Widrich, Grégoire Montavon, Michael Gillhofer, Klaus-
Robert Müller, Sepp Hochreiter, and Wojciech Samek. Explaining and interpreting lstms. In
Explainable ai: Interpreting, explaining and visualizing deep learning, pp. 211–238. Springer,
2019.

Alberto Camacho and Sheila A McIlraith. Learning interpretable models expressed in linear temporal
logic. In Proceedings of the International Conference on Automated Planning and Scheduling,
volume 29, pp. 621–630, 2019.

Mattia Carletti, Chiara Masiero, Alessandro Beghi, and Gian Antonio Susto. Explainable machine
learning in industry 4.0: Evaluating feature importance in anomaly detection to enable root cause
analysis. In 2019 IEEE International Conference on Systems, Man and Cybernetics (SMC), pp.
21–26. IEEE, 2019.

Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics
dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
6299–6308, 2017.

Sharma Chakravarthy, Vidhya Krishnaprasad, Eman Anwar, and Seung-Kyum Kim. Composite
events for active databases: Semantics, contexts and detection. In VLDB, volume 94, pp. 606–617,
1994.

Hila Chefer, Shir Gur, and Lior Wolf. Generic attention-model explainability for interpreting bi-modal

and encoder-decoder transformers. arXiv preprint arXiv:2103.15679, 2021.

Pudi Chen, Shenghua Liu, Chuan Shi, Bryan Hooi, Bai Wang, and Xueqi Cheng. Neucast: Seasonal

neural forecast of power grid time series. In IJCAI, pp. 3315–3321, 2018.

Edward Choi, Zhen Xu, Yujia Li, Michael W Dusenberry, Gerardo Flores, Yuan Xue, and Andrew M
Dai. Graph convolutional transformer: Learning the graphical structure of electronic health records.
arXiv preprint arXiv:1906.04716, 2019.

Jan Chorowski and Jacek M Zurada. Learning understandable neural networks with nonnegative
weight constraints. IEEE transactions on neural networks and learning systems, 26(1):62–69,
2014.

Luc De Raedt and Kristian Kersting. Statistical relational learning. 2010.

Luc De Raedt, Angelika Kimmig, and Hannu Toivonen. Problog: A probabilistic prolog and its

application in link discovery. In IJCAI, volume 7, pp. 2462–2467. Hyderabad, 2007.

Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural logic

machines. arXiv preprint arXiv:1904.11694, 2019.

Richard Evans and Edward Grefenstette. Learning explanatory rules from noisy data. Journal of

Artiﬁcial Intelligence Research, 61:1–64, 2018.

11

Bernd Finkbeiner, Christopher Hahn, Markus N Rabe, and Frederik Schmitt. Teaching temporal

logics to neural networks. arXiv preprint arXiv:2003.04218, 2020.

Rohit Girdhar and Deva Ramanan. Cater: A diagnostic dataset for compositional actions and temporal

reasoning. arXiv preprint arXiv:1910.04744, 2019.

Mathieu Guillame-Bert, Artur Dubrawski, Donghan Wang, Marilyn Hravnak, Gilles Clermont, and
Michael R Pinsky. Learning temporal rules to forecast instability in continuously monitored
patients. Journal of the American Medical Informatics Association, 24(1):47–53, 2017.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016.

Annika Hinze. Efﬁcient ﬁltering of composite events. In British National Conference on Databases,

pp. 207–225. Springer, 2003.

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):

1735–1780, 1997.

Craig Innes and Subramanian Ramamoorthy. Elaborating on learned demonstrations with temporal

logic speciﬁcations. arXiv preprint arXiv:2002.00784, 2020.

Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-Wei, Mengling Feng, Mohammad
Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a
freely accessible critical care database. Scientiﬁc data, 3(1):1–9, 2016.

Kristian Kersting, Luc De Raedt, and Tapani Raiko. Logical hidden markov models. Journal of

Artiﬁcial Intelligence Research, 25:425–456, 2006.

Angelika Kimmig, Bart Demoen, Luc De Raedt, Vitor Santos Costa, and Ricardo Rocha. On the
implementation of the probabilistic logic programming language problog. Theory and Practice of
Logic Programming, 11(2-3):235–262, 2011.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

Daphne Koller, Nir Friedman, Sašo Džeroski, Charles Sutton, Andrew McCallum, Avi Pfeffer, Pieter
Abbeel, Ming-Fai Wong, Chris Meek, Jennifer Neville, et al. Introduction to statistical relational
learning. MIT press, 2007.

Matthieu Komorowski, Leo A Celi, Omar Badawi, Anthony C Gordon, and A Aldo Faisal. The
artiﬁcial intelligence clinician learns optimal treatment strategies for sepsis in intensive care.
Nature medicine, 24(11):1716–1720, 2018.

Aleksandar Kovaˇcevi´c, Azad Dehghan, Michele Filannino, John A Keane, and Goran Nenadic.
Combining rules and machine learning for extraction of temporal expressions and events from
clinical narratives. Journal of the American Medical Informatics Association, 20(5):859–866,
2013.

Vasudev Lal, Arden Ma, Estelle Aﬂalo, Phillip Howard, Ana Simoes, Daniel Korat, Oren Pereg,
Gadi Singer, and Moshe Wasserblat. Interpret: An interactive visualization tool for interpreting
transformers. In Proceedings of the 16th Conference of the European Chapter of the Association
for Computational Linguistics: System Demonstrations, pp. 135–142, 2021.

Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efﬁcient video understanding.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 7083–7093, 2019.

Guangtian Liu, Aloysius K Mok, and Eric J Yang. Composite events for network event correlation. In
Integrated Network Management VI. Distributed Management for the Networked Millennium. Pro-
ceedings of the Sixth IFIP/IEEE International Symposium on Integrated Network Management.(Cat.
No. 99EX302), pp. 247–260. IEEE, 1999.

12

Yu-Ying Liu, Alexander Moreno, Shuang Li, Fuxin Li, Le Song, and James M Rehg. Learning
continuous-time hidden markov models for event data. In Mobile Health, pp. 361–387. Springer,
2017.

Meiyi Ma, Ji Gao, Lu Feng, and John A Stankovic. Stlnet: Signal temporal logic enforced multivariate

recurrent neural networks. In NeurIPS, 2020.

Stephen Muggleton. Inductive logic programming. New generation computing, 8(4):295–318, 1991.

Stephen Muggleton and Luc De Raedt. Inductive logic programming: Theory and methods. The

Journal of Logic Programming, 19:629–679, 1994.

Christopher Mutschler and Michael Philippsen. Learning event detection rules with noise hidden
markov models. In 2012 NASA/ESA Conference on Adaptive Hardware and Systems (AHS), pp.
159–166. IEEE, 2012.

Daniel Neider and Ivan Gavran. Learning linear temporal properties. In 2018 Formal Methods in

Computer Aided Design (FMCAD), pp. 1–10. IEEE, 2018.

Trang Pham, Truyen Tran, Dinh Phung, and Svetha Venkatesh. Predicting healthcare trajectories
from medical records: A deep learning approach. Journal of biomedical informatics, 69:218–229,
2017.

Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. In Advances in neural information processing systems,
pp. 91–99, 2015.

Matthew Richardson and Pedro Domingos. Markov logic networks. Machine learning, 62(1-2):

107–136, 2006.

Ryan Riegel, Alexander Gray, Francois Luus, Naweed Khan, Ndivhuwo Makondo, Ismail Yunus
Akhalwaya, Haifeng Qian, Ronald Fagin, Francisco Barahona, Udit Sharma, et al. Logical neural
networks. arXiv preprint arXiv:2006.13155, 2020.

Tim Rocktäschel and Sebastian Riedel. End-to-end differentiable proving.

arXiv preprint

arXiv:1705.11040, 2017.

Young Chol Song, Henry Kautz, James Allen, Mary Swift, Yuncheng Li, Jiebo Luo, and Ce Zhang.
A markov logic framework for recognizing complex events from multimodal data. In Proceedings
of the 15th ACM on International conference on multimodal interaction, pp. 141–148, 2013.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. The journal of machine
learning research, 15(1):1929–1958, 2014.

Kevin Tang, Li Fei-Fei, and Daphne Koller. Learning latent temporal structure for complex event
detection. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1250–1257.
IEEE, 2012.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.

Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool.
Temporal segment networks: Towards good practices for deep action recognition. In European
conference on computer vision, pp. 20–36. Springer, 2016.

Ruixuan Yan and Agung Julius. Neural network for weighted signal temporal logic. arXiv preprint

arXiv:2104.05435, 2021.

Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical rules for knowledge

base reasoning. arXiv preprint arXiv:1702.08367, 2017.

Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba. Temporal relational reasoning in
videos. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 803–818,
2018.

13

A TEMPORAL ILP COMPARISON

Neural LP In typical ILP problems, such as Neural LP (Yang et al., 2017), the rules are induced
over a static knowledge base. This involves learning the walks along the static graph GS = (ES, RS).
Starting at entity node ex ∈ ES Neural LP learns the associated edge relations RS to traverse in
order to reach the corresponding ending entity node ey. Since between two entities there can be
many arbitrary paths, given the data, the most likely path is found. This is done by learning the
attention α over relational predicates matrices MRk to traverse from vx to vy. Here vx, vy are one
hot embeddings of the start and entities respectively.

ˆvy = vx

(cid:62)

T
(cid:89)

|R|
(cid:88)

t=1

k

αk

t MRk

Then the objective is to maximize the score ˆv(cid:62)
selected by αk
to the original paper for full details and implementation (Yang et al., 2017).

y vy of selecting the correct end entity ey through paths
t . The edges along this path compose the predicates of the rule between ex, ey. Refer

Knowledge Representation In temporal rule learning one can also compose a graph between the
entities (events) on the timeline. However the structure of such a dynamic graph GD = (ED, RD)
over time is different. The dynamic graph typically contains fewer events |ED| < |ES|, and the
graph is dense as every event has some temporal relation with respect to all other events. The
edge relations may not be provided in uniﬁed knowledge base, where the relations can vary per
sample m: RD = {R1
D }. Furthermore these temporal relations between samples are
rarely annotated with the relations of interest, while in static case, the relations RS are usually
predetermined.

D, . . . , Rm

D, R2

Rule Induction When learning rules in the temporal case, many rules don’t conform to the chain
like rule structure. This can be seen in the form f := after(a, b) ∧ before(c, d) where
there is no path between the ﬁrst predicate and the second (see the last rule in Table 9 as an example).
Without a path between two events to guide the rule construction, the rule can potentially involve
any events and relations. Formulating the problem in terms of a walk is challenging to evaluate if we
consider all relations between the disjoint events and selecting the most likely rule:

arg max
r

(cid:88)

pi∈P

after(a, b)∧before(c, d)∧pi(X, Y ) ∀X, Y ∈ disjoint((a, b), (c, d))

Here, events between two pairs of predicates are disjoint if there does not exist a common event
between the two predicates. If not, we would have to sample all combinations of X, Y from each
predicate respectively: (a, c), (a, d), (b, c), (b, d). Instead of expensive marginalization,
we aim to search for combinatorial combinations of grounded predicates in a differentiable fashion.
This search space for Temporal ILP grows faster with larger numbers of atomic events and relations.

Lemma. In Temporal ILP, given the space of events E = |X |, predicates P = |P|, and a rule with n
predicates, the search space for a rule r is generally O((P E2)n). If P consists of symmetric relations,
such as before(u, v) = after(v, u), and an equivalent relation during(u, v) = during(v, u),
we keep (cid:98) P
2 (cid:99) of the symmetric relations and the unique values in the equivalent relation. Then the
bound can be tightened to Θ(((cid:98) P

)n) unique combinations of rules.

2 (cid:99)E2 + E(E+1)

2

In ILP given the knowledge base, to construct a rule for predicates with a pair of entity variables
E1, E2 and average node degree D, the search space is Θ(Dn).

The corresponding temporal models and rule extraction methods has to reﬂect these differences. We
focus on structured time series tasks where we have to learn temporal predicate parameters in addition
to the temporal logic formulas from the samples. Our model Neural TLP learns these temporal
relations between pairwise atomic events, with considerations of event noise as well as computational
complexity. Then given the inferred relations, the combinations of relations can be learned such that
the correct composite event is inferred.

14

B OPTIMIZATION

We used the Adam (Kingma & Ba, 2014) optimizer with a learning rate of 0.001 for all our experi-
ments and the default parameters described in their paper. The batch size during relational training is
set to 256. Each experiment was run for 100 epochs to train the relation parameters and weights W.
For variable rule length search we tested longer epoch lengths, but didn’t see much improvement in
the validation past the ﬁrst epoch. Therefore 1 epoch of tuning was done to optimize the combinatorial
attention weights s while freezing all other relation parameters. All experiments were conducted on a
server with a Nvidia 2080TI GPU with 11GB of VRAM.

We made sure all our model conﬁgurations could ﬁt on a single GPU of this size. The memory
intensive component came from having different attention weights s and combinatorial matrices C
for each rule r ∈ R learned. This meant that we had to limit the number of predicates combinations
we search over. Here c is the number of most relevant predicates searched per rule and n is the max
number of predicates, so the number of combinations for the max rule length is (cid:0)c
(cid:1). So for each
variable rule length of 1,2,3,4 we chose c = 100, 100, 30, 25 respectively. Due to the larger memory
requirements, we reduce the batch size during rule search to 64.

n

C BASELINES

C.1 LSTM

Since we are working with multi-hot labels, and co-occurring atomic events, it is difﬁcult to
parametrize existing HMM variants. Therefore we test a more neural recurrent baseline, LSTM
(Hochreiter & Schmidhuber, 1997), over the raw atomic event stream. To account for object invariance,
we marginalize over the timelines to produce the inputs ˜MT = (cid:80)
k max(1, MTk,:,:) ∈ R|X |×T . The
compressed timelines and temporal indexing are done over ˜MT to produce MA. From MA ∈ R|X |×t
we input x ∈ R|X | at each step t: xi, hi, ci = LSTM(xi−1, hi−1, ci−1), where c is the cell state.
Then we perform classiﬁcation over the labels through a linear layer using the last hidden state
ˆy = Wht + b. We test both a large (L) and a small (S) version with hidden dimensions of |h| = 512
and |h| = 160 respectively.

We also test an attention based variant where the attention value for each hidden state is computed
using the hidden state as well as the input atomic events at that step. This helps the model focus on
time steps that are not empty over long frame sequences, and led to better convergence.

ri = [hi; xi]
ai = tanh(b(cid:62)
a = softmax(a)

1 (riW1))

(cid:88)

h =

hi · ai

i

ˆy = W2h + b2

Here W1 ∈ R(|h|+|x|)×d and b1 ∈ Rd project the concatenated data into attention dimension d,
which was set to d = 32 for all our experiments.

To extract the rules, we ﬁrst enumerate all possible composite events. Then we synthesize each
composite events as raw event timeline data. Unlike the original training data where there are
multiple atomic events occurring simultaneously, for each composite event we have two atomic events
occurring unambiguously before, during, or after one another. Then we pass the synthesized
events into the model and take the argmax prediction as the corresponding rule label.

C.2 TEMPORAL MAP

While LSTMs optimize for label prediction, a baseline to test rule induction is to maximize the
posterior distribution of the enumerated composite event rules given the training labels. This is
done by using Neural TLP but with two changes. First we freeze all model parameters as done

15

in a deterministic setting. Second, instead of learning the attention weights W we count the co-
occurrences of relations computed through MR and the training labels for each sample.
To compute W, we start with all the observed time series samples T, Y = {T }m
i=1, {y}m
i=1. Instead
of operating over T we are interested in the grounded relational data R = {MR}m
i=1, which can be
extracted through the prior stages of the Neural TLP pipeline through the TLN network gθ with ﬁxed
relations, as described in 3.1.
Given the inferred MR data, we have pairs of Mk
compute the co-occurrences of grounded predicates p and labels y as:

R, yk for each sample of k ∈ [1, m]. Then we

Θi,j =

(cid:88)

1

pi∈Mk

R,yj ∈yk ∀k ∈ [1, m]

ˆWi,j =

Mk

R,yk
Θi,j
j Θi,j

(cid:80)

This W is used for rule learning in the same manner as Neural TLP for both ﬁxed and variable length
strategies. To compute mAP we always predict the top ¯y most frequently occurring labels in the
training set.

D CATER

We explore composite action prediction over complex videos in the CATER data set (Girdhar &
Ramanan, 2019). The CATER data set provides synthetic videos of multiple objects performing
different actions simultaneously over the duration of the videos. The atomic events are a con-
junction of these movements ∈ {rotate, slide pick-place, contain} and objects
∈ {cone, cube, sphere, snitch}. Such an atomic event is slide cone ∈ X where
|X | = 14 and temporal predicates r ∈ {before, during, after}.

The composite event rules are composed of a single grounded temporal predicate (n = 1) be-
tween two atomic events. For example the underlying composite events in the video such as
before(pick-place cube,rotate snitch) is assigned to label 2, providing |y| = 301
unique composite events. Only the label along with the videos are provided during training, while the
rules are unknown.

Furthermore all these atomic events occur randomly during the video, thus multiple labels correspond-
ing the the underlying temporal rule are active. This provides a difﬁcult, yet practical challenge: we
know what coarse composite events occur during a timeline, but we want to recover the underlying
atomic events and predicates (rules) leading to each of these composite events (labels) as shown
in Figure 5. Since the videos are generated with underlying rule templates, we can objectively test
our models to recover these rules. For our experiments we used the train, validation, and test splits
provided in original dataset.

D.1 PREDICTED ATOMIC EVENTS

Modality

Overall Acc Rotate

Slide

Pick-Place

RGB
RGB + Flow

85.7%
96.6%

17.6% 3.9%
76.9% 88.2%

91.3%
96.9%

Table 6: Here are the accuracies for the predicted atomic events. The results show accuracy predicting
the event only using the RGB image features and with the optical ﬂow information. Further rules
were used to predict contain.

Using the original CATER video data we performed experiments where the atomic events were
provided and where we inferred the atomic events. The latter case is more difﬁcult, yet more realistic
for rule recovery over collected data where noise exists. To infer the atomic events we ﬁrst tune a
Faster R-CNN (Ren et al., 2015) over the object bounding boxes. In real world use cases it may

16

Loss

Lent
Lent
Lent
Lent
Lent
Lent
Lent + L1
Lent + L1

Relation Parameters

Projection mAP Rules@1 Rules@5

T = t = 301; no conv
t = 150; kernel= 14 × 3, stride= 2
t = 50; kernel= 14 × 7, stride= 6

t = 150; freeze conv
t = 150; freeze α
t = 150; freeze β, γ

t = 150
t = 150

(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:51)

.693
.682
.679

.669
.656
.658

.681
.675

.830
.883
.873

.869
.671
.681

.915
.913

.966
.953
.960

.953
.950
.913

.958
.956

Table 7: We ablate the different components of Neural TLP over the CATER predicted atomic events
data. We start by determining the efﬁcacy of quantization, where t is the dimension after quantization.
Then we test the contribution of each parameter in relation learning. Finally we test additional
optimization strategies.

be possible to use pre-trained detectors to lift these bounding boxes, but they typically don’t cover
synthetic objects. We use these bounding boxes to generate a visual feature over the cropped image
using a pre-trained ResNet-50 (He et al., 2016). We use the image feature and optical ﬂow from the
previous and next frame to predict the shape and movement.

There is additional difﬁculty performing this inference and we present the event accuracy in Table
6. Furthermore, the action contain is hard to distinguish from pick-place, so additional rules
are used to disambiguate pick-place from contain, leading to additional uncertainty. The rule
classiﬁed contain if the movement is a pick-place and the bounding box of the moving object
is close to the bounding box of a static object. Since each object has its own atomic actions we
assigned a timeline for each object, tracking a max of k = 30 objects.

D.2 MODEL ABLATION

We ablate our model over the predicted atomic event data in Table 7. In the ﬁrst section we test
different convolution kernel sizes which contains a 1d covolution weight that is learned per atomic
event |X | = 14 and a stride. We see that convolution smoothing is beneﬁcial at the reduced dimension
of t = 150 and use this for further ablations.

In the next section we test how much each relation parameter contributes to the result. The convolution
weights are ﬁxed to 1, and we see that in this case they contribute minimally to rule induction. In real
world cases with more complex events, these learned convolutions could potentially be more useful if
systematic noise exists. For the convolution scalar α it is crucial to shift the compressed timeline to
correctly identify event time intervals, as seen when it is ﬁxed to 1. Similarly when shift β = 0 and
scale γ = 1, it misses on variations in the relation data.

In the ﬁnal section we test our L1 regularization to produce sparser results for W. This shows to
enable better rule induction. We test projecting W between [0, 1] and while we got similar results, we
notice that the optimization converges faster and in a more stable fashion. We primarily focused on
rule precision by looking at Hits@1, while increasing recall produced similar results for all methods
in the setting where the rules only have n = 1 predicate.

D.3 DATA ABLATION

To test different rule lengths n, training samples m, and event complexity ¯y, we generated more
CATER-like data to explore these dataset statistics. Here we use the same atomic events and relations
as in the original dataset. Due to the number of possible combinations of the dataset statistics, it is
expensive to generate the video data and run our vision pipeline to generate the predicted atomic
events. Instead we opted to simulate the atomic event predictions directly. For each sample we:

1. Randomly sample j rules containing up to n predicates.

17

Max Len n

¯y

Model Variable

Len 1

Len 2

Len 3

Len 4

1

2

3

4

1.0

4.0

5.3

10.0

TLP
MAP

TLP
MAP

TLP
MAP

TLP
MAP

.97±.04
.53±.07
.44±.08
.17±.00

.15±.02
.11±.00
.10±.01
.09±.01

.97±.04
.53±.07
.80±.15
.34±.00
.35±.06
.33±.00
.36±.05
.33±.04

-
-

.09±.01
.00±.01
.05±.02
.01±.01
.02±.03
.04±.03

-
-

-
-

.01±.01
.00±.00
.00±.01
.00±.01

-
-

-
-

-
-

.00±.00
.00±.00

Table 8: For j = 1 samples we compare model Hits@10 performances across different rule lengths.
We observe the total variable length Hits for rules of all lengths that have to be learned. We break
down the combined performance into the individual performance per ﬁxed rule length n = Len i, that
compose the total variable performance.

Figure 6: We compare Neural TLP (Blue) and Temporal MAP (Red) with varying number of active
labels and samples. Each curve represents the overall variable length accuracy up to rule length n. On
the left we compare the variable length performance when we sampled more event rules per video,
increasing the active labels and timeline noise with ﬁxed 10000 samples. On the right we compare
performances as the number of samples increase and ﬁx j = 1.

2. Synthesize a timeline for each rule, where atomic events are placed in the timeline consistent
with the rule. Each atomic event prediction was sampled from a normal distribution, with
means centered around the atomic event detection accuracies in Table 6.

3. Based on the synthesized timeline over sampled rules, add any consistent rules induced by
the synthesized timeline. These additional consistent rules contribute to our j ≤ ¯y estimate.

4. Gaussian noise was also added along the timeline where events did not occur to simulate

detection noise.

Given the synthesized timeline and the consistent rules, the corresponding MT and labels y can
be generated for each sample. For validation and testing data we generated 2500 samples each,
regardless of the number of training samples.

From this base data set, we ﬁrst vary the max rule length n of the rule samples and ﬁx j = 1 to isolate
the effect of the rule length. For every level n we sample 100 total rules up to the max length n for
consistency. Breaking down the variable length accuracy in Table 8 we see Neural TLP provides
better performance for shorter rules, while rule learning becomes more difﬁcult for longer rules.

We also test the event j and data points m sample complexity in Figure 6. As we sample more events
j, more noise is added to our timeline and makes it harder for models to recover the underlying rules.
Inversely we also show the performance increases with the number of samples, where Neural TLP is
more sample efﬁcient.

18

1.01.52.02.53.03.54.04.55.0Sampled Events j0.00.20.40.60.81.0Hits@10Score versus Sampled EventsTLP Len 1MAP Len 1TLP Len 2MAP Len 2TLP Len 3MAP Len 3TLP Len 4MAP Len 40200040006000800010000Samples0.00.20.40.60.81.0Hits@10Score versus Sample ComplexityE MIMIC-III

after(oral water, spo2_sao2 high) ∧ after(oral water, paco2 high)
after(hco3 high, spo2_sao2 high) ∧ after(spo2_sao2 high, calcium high)
after(hco3 high, spo2_sao2 high) ∧ before(hco3 normal, pao2 low)

Table 9: Induced rules for normal urine, veriﬁed as correct or plausible by doctors.

oral water
spo2_sao2
pao2
paco2
hco3
calcium

patient drank water
pulse oximetry SpO2 and blood gas SaO2 in oxygen
partial pressure of blood oxygen PaO2
partial pressure of carbon dioxide in the blood PaCO2
body metabolic bicarbonate HCO3
patient calcium indication

Table 10: Descriptions of MIMIC atomic events in the lifted rules from Table 9.

For the healthcare data we use the MIMIC-III dataset (Johnson et al., 2016). The dataset contains
measurements, vitals, and medications for intensive care patients, and is already de-identiﬁed. We
ﬁrst ﬁlter the patients containing ICD codes corresponding to sepsis and sample 2k patients.

Instead of inferring patient survival directly, the medical doctors suggested to look at circulatory
indicators, speciﬁcally urine ﬂow. Additionally they helped us identify a subset of patient vitals and
measurements to use for urine ﬂow, leading to our |X | = 82 boolean atomic events. From the 2k
patients we identify timepoints containing urine information. This serves as a label, and all events
before the urine event are the timeseries inputs. Since patients had multiple indicators of urine during
their stay we have 3.8k samples of time series and urine label (normal or low). During training we
created an 80/20 split for training and validation respectively.

After training and iterative feedback from the doctors, we successfully lifted useful indicators of
urine ﬂow as presented in Table 9. A description of the atomic events is presented here in Table 10.

19

