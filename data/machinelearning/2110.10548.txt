1
2
0
2

v
o
N
6
1

]
L
P
.
s
c
[

2
v
8
4
5
0
1
.
0
1
1
2
:
v
i
X
r
a

SYNTHESIZING OPTIMAL PARALLELISM PLACEMENT AND REDUCTION
STRATEGIES ON HIERARCHICAL SYSTEMS FOR DEEP LEARNING

Ningning Xie 1 Tamara Norman 2 Dominik Grewe 2 Dimitrios Vytiniotis 2

ABSTRACT
We present a novel characterization of the mapping of multiple parallelism forms (e.g. data and model parallelism)
onto hierarchical accelerator systems that is hierarchy-aware and greatly reduces the space of software-to-hardware
mapping. We experimentally verify the substantial effect of these mappings on all-reduce performance (up to
448×). We offer a novel syntax-guided program synthesis framework that is able to decompose reductions over
one or more parallelism axes to sequences of collectives in a hierarchy- and mapping-aware way. For 69% of
parallelism placements and user requested reductions, our framework synthesizes programs that outperform the
default all-reduce implementation when evaluated on different GPU hierarchies (max 2.04×, average 1.27×). We
complement our synthesis tool with a simulator exceeding 90% top-10 accuracy, which therefore reduces the need
for massive evaluations of synthesis results to determine a small set of optimal programs and mappings.

1

INTRODUCTION

To facilitate efﬁcient training of large-scale deep learning
models, numerous parallelism techniques have been success-
fully employed. Common forms of parallelism include data
parallelism (Krizhevsky et al., 2012), where each device
has a copy of the full model to process a portion of the train-
ing data, and model parallelism (Dean et al., 2012), which
partitions a training model over available devices, such as
parameter sharding (Shoeybi et al., 2020) and pipeline par-
allelism (Huang et al., 2019). More recent studies explore
combinations of parallelism forms to maximize training
throughput (Jia et al., 2019; Narayanan et al., 2021), where
each form of parallelism is referred to as a parallelism axis.

While the aforementioned forms of parallelism and their
combinations have greatly improved training throughput,
they may still incur signiﬁcant communication cost. For
example, in the simplest form of data parallelism, parameter
gradients for each device must be reduced and replicated for
each iteration (Amodei et al., 2016), which is typically im-
plemented using the collective operation AllReduce (Thakur
et al., 2005). State-of-the-art parameter sharding for trans-
formers (Shoeybi et al., 2020) introduces sharded layers
where each involves several AllReduce operations. Com-
munication overhead is especially important for distributed
deep learning, as the more devices we have, computation
time reduces, and the communication cost becomes more

1University of Cambridge 2DeepMind. Correspondence to:

Ningning Xie <nx213@cam.ac.uk>.

Submitted to the 5 th MLSys Conference, Santa Clara, CA, USA,
2022. Copyright 2022 by the author(s).

(a) Combining
parameter sharding
and data parallelism

(b) Reduction
along the axis of
parameter sharding

(c) Reduction
along the axis of
data parallelism

Figure 1: Parallelism combination

prominent (Sergeev & Balso, 2018; Goyal et al., 2018).

To reduce communication overhead, one particular chal-
lenge posed by multiple parallelism axes is parallelism
placement. That is, how we map parallelism over devices
decides which devices communicate with each other along
each parallelism axis, and therefore decides the communi-
cation overhead. For example, Figure 1a presents a com-
bination of parameter sharding and data parallelism, for
which reduction along the axis of parameter sharding (or
data parallelism), referred to as the reduction axis, is shown
in Figure 1b (or Figure 1c, respectively). Now, suppose we
map each box in the ﬁgure to devices. In that case, different
mappings correspond to different reduction device groups,
which can have a signiﬁcant impact on the communication
overhead depending on the network topology.

In this work, we present P 2, a tool for parallelism placement
and placement-aware synthesis of recduction strategies. In
particular, we offer the following contributions:

 
 
 
 
 
 
Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning

(a) [(rack, 1), (server, 2), (CPU, 2), (GPU, 4)]

(b)

(cid:21)

(cid:20)1 2 2 1
1 1 1 4

(c)

(cid:21)

(cid:20)1 2 1 2
1 1 2 2

(d)

(cid:21)

(cid:20)1 1 2 2
1 2 1 2

Figure 2: (a): A system. (b), (c), (d): Possible (non-exhaustive) parallelism placements for (a) under data parallelism of size
4 and 4 parameter shards. For clarity, we show only the 16 GPUs but omit interconnects. Device marker n/m indicates data
batch n and parameter shard m.

• Parallelism placement synthesis: Given the parallelism
axes, the reduction axes, and a hierarchical system
topology, P 2 automatically synthesizes hierarchical
parallelism placements, where a parallelism placement
is modelled as a parallelism matrix mapping from par-
allelism axes to the system hierarchy (Section 3.1).
The notion of parallelism matrices greatly reduces the
space of parallelism placements contrary to a naive
implementation.

• Reduction strategy synthesis: For each parallelism
placement, P 2 utilizes the system hierarchy to fur-
ther synthesize a wide variety of reduction strategies
to implement reductions using common collective op-
erations. To achieve this, we introduce: (a) a formal
semantics for collectives (Section 3.2) based on Hoare
triples (Hoare, 1969); (2) a domain-speciﬁc language
(DSL) that can express possibly simultaneous reduc-
tions amongst groups of devices based on the system
hierarchy (Section 3.3); and (b) a lowering of our DSL
into sequences of collective operations. We use the
formal semantics to guide a syntax-directed synthesis
procedure on our DSL.

• Synthesis hierarchy: We show how the parallelism ma-
trix, which determines a candidate parallelism place-
ment, can be put to good use by the synthesizer to mas-
sively reduce the space of programs considered without
missing any semantically valid programs – provably
(Section 3.4).

• Evaluation: We evaluate the parallelism matrices and
reduction strategies synthesized by P 2 on two differ-
ent GPU systems available on Google Cloud Platform
(GCP) (Section 4). We use collective operations as
implemented by NVIDIA’s NCCL communication li-
brary (NVidia, 2021), exposed through XLA. The eval-
uation demonstrates (1) the impact of parallelism place-
ment: the performance of a single AllReduce across
different parallelism matrices differs up to 448.5×;
and (2) the effectiveness of custom reduction strate-
gies: for 69% of all parallelism mapping matrices, a

synthesized reduction outperforms AllReduce with up
to 2.04× speedup (average 1.27×).

• Simulation: P 2 synthesizes all mapping and hierarchy-
aware reduction strategies, but evaluating hundreds or
thousands of them to identify the best can be expensive.
We therefore introduce a simulator for predicting the
end-to-end performance of a parallelism matrix and re-
duction strategy (Section 5). The simulator is aware of
the network topology including different bandwidths
for different interconnects and networks (e.g., NVLink
and ethernet / data-center network in GPU topologies),
predicting with reasonable accuracy the communica-
tion overhead for each parallelism placement and re-
duction strategy. The validation – over all mappings
and synthesized programs for each mapping, and for
each of the two GPU systems we considered – demon-
strates that the simulator has 52%, 72%, and 92% of
top-1, top-5 and top-10 accuracy, respectively, making
it practical for identifying a much smaller subset of
programs for actual evaluation.

P 2 is helpful for ML practitioners to speed up their models
by improving placement and synthesizing reduction strate-
gies tailored to their system hierarchies. For instance, we
have used P 2 to improve Resnet-50 (He et al., 2016) data-
parallel training by 15% across 4 nodes, each with 8 V100
GPUs. (See Section 4 for the details of this system.)

2 OVERVIEW

This section outlines the key design in P 2. First, a system
consists of two entities: (1) a hardware hierarchy, where
each level has a name and a cardinality; and (2) a set of
switched interconnects. The system hierarchy is expected
to reﬂect how devices are arranged. Figure 2a describes
an example system with 16 GPUs (Cho et al., 2019). The
hierarchy is one-dimensional: a rack has 2 servers, each
with 2 CPUs connecting 4 GPUs. Interconnects specify
how devices are connected with each other and the latency
and bandwidth constraints. In this case, we have exactly

Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning

one kind of interconnect in each level, but, in general, the
interconnect topology can be more complex: there can be
multiple interconnects in one level, and an interconnect can
connect devices (and other interconnects) across levels.

2.1 Parallelism Placement

Parallel placement decides which parts of a partitioned pro-
gram will execute on which parts of a system. However,
synthesizing all arbitrary device mappings, as well as run-
ning experiments with them, can be extremely expensive if
implemented naively. For example, if we have data paral-
lelism of size 4 and 4 parameter shards for the system in
Figure 2a, then there will be (4 ∗ 4)! > 244 possibilities to
decide which partitioned program maps to which GPU.

To explore the search space efﬁciently, the critical idea of
P 2 is to partition parallelism axes over the system hierarchy
to generate topology-aware parallelism placements, while
still being able to systematically generate a wide range of
parallelism placements. Speciﬁcally, a result of parallelism
placement synthesis is a parallelism matrix, where each
element is a parallelism factor representing the number of a
speciﬁc level in the hierarchy that a parallelism form splits
the computation across. Figures 2b, 2c and 2d show exam-
ples of parallelism matrices synthesized by P 2, where we
have data parallelism of size 4 and 4 parameter shards. In
Figure 2b, the ﬁrst row (cid:2)1 2 2 1(cid:3) corresponds to a factoriza-
tion of data parallelism on each system level. Speciﬁcally,
we ﬁrst assign all data parallelism (of size 4) into 1 rack
(each with data parallelism of size 4/1 = 4). Then each
rack assigns data parallelism of size 4 into 2 servers (each
with data parallelism of size 4/2 = 2). Next, each server as-
signs data parallelism of size 2 into 2 CPUs (each with data
parallelism of size 2/2 = 1). Finally, each CPU assigns data
parallelism of size 1 into 1 GPU. The second row (cid:2)1 1 1 4(cid:3)
corresponds to a factorization of parameter sharding: each
rack, server, and CPU gets assigned all parameter shards (of
size 4), and each CPU then assigns 4 parameter shards into
4 GPUs, each GPU level with 4/4 = 1 shard. Therefore,
in the resulting placement, each CPU corresponds to one
replica (data parallelism) where each GPU has one parame-
ter shard. We can interpret Figure 2c and 2d accordingly.

Note how parallelism matrices decide communication re-
quirements. Consider reduction along parameter sharding
(i.e., reduce devices n/m with the same n but different m).
In Figure 2b, this can be done by communication over only
S0, while in 2c, half of the data can be reduced by only S0,
but the rest of the reduction requires communication over
S0/S1/S2. We discuss the impact of parallelism placements
on communication cost in detail in Section 4.

(a) AllReduce (b) AllReduce-AllReduce (c) Reduce-AllReduce-Broadcast

Figure 3: Example reduction strategies.

(a) ReduceScatter - AllReduce

(b) AllReduce - AllReduce

Figure 4: Semantically invalid reduction. (a): Reduce data
that should not be reduced. (b): Reduce the same data twice.

2.2 Reduction Strategy

For each parallelism matrix, P 2 further synthesizes
topology-aware reduction strategies using common collec-
tive operations, which allows us to ﬁnd the optimal reduction
strategy for any given parallelism matrix.

To illustrate the idea, consider the parallelism matrix in Fig-
ure 2d, and the goal is to reduce along parameter sharding.
As shown in Figure 3a, an obvious choice to perform the
reduction is a single AllReduce within reduction groups.
However, such reduction may be suboptimal, as it does not
utilize the topology of the system. Figure 3b and 3c show
two reduction strategies, among others, synthesized by P 2.
Figure 3b ﬁrst performs a step of AllReduce which commu-
nicates over only S0, and then AllReduce that communicates
over S0/S1/S2. Figure 3c ﬁrst performs Reduce that puts
the reduction result in the root device, then AllReduce be-
tween root devices, and ﬁnally Broadcast that broadcasts
data from the root device. Of particular interest in these
two reduction strategies is that no one is strictly better than
the other, as the communication overhead depends on the
network: 3c takes more steps, but has fewer data to be trans-
ferred over S1/S2, which may outperform 3b if S0 has high
bandwidth while communication over S1/S2 is expensive.

P 2 gives us a systematic way to synthesize and compare
a wide range of topology-aware reduction strategies. In
particular, synthesized reduction strategies can outperform a
single step AllReduce, with speedup up to 2.05×. However,
synthesizing reduction strategies also imposes challenges,
which we outline in the rest of this section.

2.3 Formalism of Collective Operations

To synthesize reduction strategies, we ﬁrst need to formal-
ize the semantics of collective operations, since not all se-
quences of operationally valid collective operations corre-
spond to semantically correct implementations of the end-to-
end reduction requested by the user. For example, consider

Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning

Figure 5

Figure 6: Device structured as the reduction axis

the (incomplete) reduction steps given in Figure 4 for the re-
quested reduction across parameter shards. Both programs
can be executed successfully, e.g., by NCCL (NVidia, 2021).
Unfortunately, they are both semantically invalid. In particu-
lar, we consider reduction steps which result in device states
that can never reach the ﬁnal desired state to be semanti-
cally invalid. Speciﬁcally, in 4a, the ﬁrst ReduceScatter
will reduce, among others, device A0 and A1 (recall that
GPUs are named in Figure 2a), and put the ﬁrst half of the
result on A0 and the second half on A1. Then the second
AllReduce will reduce A0 and A1 – so the ﬁrst and the sec-
ond half of the result get reduced while they should not!
Now, we can never reach the desired ﬁnal state. 4b is also
invalid as it reduces the data on A0 and C0 twice.

P 2 provides a concise and novel formalism of common
collective operations (Section 3.2) that captures semantic
correctness and rules out semantically invalid programs,
massively reducing the synthesis space. Speciﬁcally, each
device state is deﬁned as a state matrix describing what kind
of data a device has. The semantics of collective operations
is deﬁned with Hoare triples (Hoare, 1969), where a collec-
tive takes the state of each device as a pre-condition and
returns a new state as a post-condition.

2.4 Reduction Communication Patterns

Even though the formalism of collective operations rules
out semantically invalid reduction steps, the search space of
reduction strategies is still quite large. One reason is that
we need to decide which devices form a reduction group for
each reduction step. For example, the ﬁrst step in Figure 3b
reduces over {A0, A1}, {A2, A3} (among others). We may
randomly generate all possible groups, but that would sig-
niﬁcantly increase the search space. Also, many of them
would be immediately thrown away after semantic checks.

To synthesize reduction strategies effectively, P 2 uses a
domain-speciﬁc language (Section 3.3) that explores the
hierarchy to generate hierarchical communication patterns.
The reduction language, together with the synthesis hierar-
chy (explained next), can model many common communi-
cation patterns, including those in Figure 3 and 4.

2.5 Synthesis Hierarchy

To generate hierarchical reduction communication patterns,
the DSL reduction instruction needs to know the synthesis
hierarchy. For example, a possible instruction is to reduce

(cid:21)

(cid:20)1 1 2 2
1 2 1 2





1 2 3
4 5 6
7 8 9





column-based
row-based
reduction axis
column-based
row-based
reduction axes
collapsed

(cid:2)1 1 1 2 2 1 2 2(cid:3)
(cid:2)1 1 2 2 1 2 1 2(cid:3)
(cid:2)1 2 1 2(cid:3)
(cid:2)1 4 7 2 5 8 3 6 9(cid:3)
(cid:2)1 2 3 4 5 6 7 8 9(cid:3)
(cid:2)1 2 3 7 8 9(cid:3)
(cid:2)7 16 27(cid:3)

1
2
3

Table 1: Synthesis hierarchy (reduction axes highlighted)

all ”GPUs” connected to the same ”CPU”. Now, an impor-
tant design decision to consider is which hierarchy to use
for synthesis, as it decides what kind of instructions we can
produce. One obvious choice is the hardware hierarchy, i.e.,
(cid:2)1 2 2 4(cid:3) for our running example (we ignore level names
as that can be randomly generated). But the system hierar-
chy is not ﬁne-grained enough for the requested reduction:
for example, the reduction in Figure 3 requires reducing half
of the GPUs connected to a CPU. To do that, we need to
take parallelism axes into consideration. In this case, the

parallelism matrix is

(cid:20)1 1 2 2
1 2 1 2

(cid:21)
, which splits GPU into 2

by 2, allowing us to reduce 2 GPUs connected to a CPU.
However, to form a synthesis hierarchy from the parallelism
matrix, we have two options (Table 1): 1 puts parallelism
factors by columns, which essentially expands the system
hierarchy corresponding to the parallelism matrix; or 2
puts factors by rows, which expands the parallelism axes. In
this work, we prove that 2 is more expressive than 1 , i.e.,
2 can generate all semantically valid reduction strategies
that can be generated by 1 . The result might be some-
what counter-intuitive, as 1 seems a natural way to expand
the system hierarchy. The critical insight is that as 2 puts
parallelism axes consecutively, it can more easily generate
semantically correct reduction, while 1 can more easily
reduce devices laid out consequently but those reduction
can be semantically invalid as it partitions parallelism axes.

It turns out we can further optimize the synthesis hierarchy.
In particular, note that we reduce along the axis (cid:2)1 2 1 2(cid:3),
but 2 includes the full matrix (i.e., including (cid:2)1 1 2 2(cid:3)).
With a full matrix, we can generate reduction like Figure 5,
which, however, is not useful for this speciﬁc case, as we
should not reduce device A0 and A2. The key observation
here is that each reduction group is essentially structured
according to the reduction axis (cid:2)1 2 1 2(cid:3), and this structure
is repeated for the rest of the matrix, as shown in Figure 6.
Based on this observation, P 2 uses the synthesis hierarchy
3 formed by parallelism factors from only the reduction
axis and then lowers synthesized programs to the full system
hierarchy. We prove that 3 , while largely reducing the
search space, is actually more expressive than 2 .

Exploration with multiple reduction axes. The same obser-

Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning

vation applies for reduction over multiple axes. An example
is given in the second half of Table 1. In this case, the reduc-
tion axes based synthesis hierarchy is (cid:2)1 2 3 7 8 9(cid:3). Note
that some parallelism factors are from the same hardware
level: 1 and 7, 2 and 8, and 3 and 9. Since for switched
networks, splitting hardware hierarchies does not bring ben-
eﬁts in most cases, we can collapse parallelism factors of
the same hardware hierarchies. In this example, the ﬁnal
synthesis hierarchy is (cid:2)7 16 27(cid:3).

3 PROGRAM SYNTHESIS

We now present the program synthesis algorithm in P 2.

3.1 Parallelism Placement

Parallelism placement partitions parallelism axes over the
system hierarchy. With the novel notion of the parallelism
matrix and its interpretation (Section 2.1), synthesizing par-
allelism matrices is straightforward. Consider
H = (cid:2)h0 · · · hn
P = (cid:2)p0 · · · pm
then a parallelism matrix is

(cid:3) is the system hierarchy (e.g., (cid:2)1 2 2 4(cid:3)),
(cid:3) is the parallelism axes (e.g., (cid:2)4 4(cid:3)),






x0,0 x0,1 . . . x0,n
...
...
...
xm,0 xm,1 · · · xm,n

. . .






subject to:
m
(cid:89)

xi,j = hj, j = 0, ..., n (1)

i=0
n
(cid:89)

xi,j = pi, i = 0, ..., m (2)

j=0

Equation (1) requires the product of a column to be equiva-
lent to the corresponding system hierarchy cardinality, while
Equation (2) requires the product of a row to be equivalent
to the corresponding parallelism axis.

3.2 Collective Operations

This section deﬁnes the semantics of collective operations.
In this work we focus on the common ones: AllReduce,
ReduceScatter, AllGather, Reduce and Broadcast.

Notations We ﬁrst deﬁne the notations.

d
s
G
C

Bk×k
∈
:= di : si
:= AllReduce | ReduceScatter
|

AllGather | Reduce | Broadcast

device
device state
state context

We use d to denote a device, whose state s is represented as
a boolean matrix of dimensions k × k; k being the number
of devices. In particular we treat the data as being split in
k chunks. The ith row of a state matrix represents the ith
chunk. s[i][j] = 1 means that device j has contributed its

3 data chunks

reduced from device 0 and 1
reduced from device 1 and 2

reduced from device 2 and 3

Figure 7: A device state. Assume we have in total 4 devices
(i.e., device 0,1,2 and 3), so each device state is a 4 × 4
matrix. s[i][j] is colored if s[i][j] = 1. The device state has
3 non-empty rows, meaning that it has 3 data chunks. Each
data chunk describes where the data is reduced from. For
example, the ﬁrst data chunk is the reduction result between
the original ﬁrst data chunk of device 0 and 1.

original ith chunk to the reduction result. Figure 7 gives an
example. A state context G maps devices to their states.

Note ﬁnally that Reduce and Broadcast typically take a root
device to reduce to or broadcast from. Since we focus on
hierarchical systems, we always use the ﬁrst device in a
reduction group as the root without loss of generality.

Semantics Figure 8 deﬁnes the semantics of collective
operations, which is closely based on Hoare rules (Hoare,
1969). Each reduction takes the form of a Hoare triple
{ G1 } C { G2 }, which means that from the pre-condition
state G1, a step of reduction C yields to the post-condition
state G2. Explanations of auxiliary functions are given in
the ﬁgure. To better illustrate the semantics, the right of
Figure 8 provides examples of each collective operation.

At a high level, these rules capture the constraints for
a reduction step to be semantically correct. Rule R-
ALLREDUCE ﬁrst checks that the data contained in each
device (denoted as rows representing the non-empty rows)
should have the same data chunks. Moreover, columns in
any speciﬁc chunk should be disjoint. Both constraints are
essential for the reduction result to be valid: we should not
reduce data from different chunks or reduce the same data
twice (as discussed in Section 2.3). Finally, we generate the
resulting state (cid:93)si for each device by adding up all matri-
ces. Rule S-REDUCESCATTER and S-REDUCE are similar
to rule S-ALLREDUCE, except that S-REDUCESCATTER
scatters the reduction result over devices, where scatter
raises an error if the number of data chunks in s is not di-
visible by the number of devices; and S-REDUCE puts the
result only in the ﬁrst device and clears up the rest of the
devices. Rule S-ALLGATHER simply needs all data rows to
be disjoint. Rule S-BROADCAST overrides the data of every
device with the data from the ﬁrst one. As an optimization,
the rule enforces information increase, i.e., the data to be
broadcasted must be as informative as data in other devices
and more informative than at least one other device.

Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning

{ G1 } C { G2 } (Reduction: from the pre-condition state G1, C yields to the post-condition state G2 )

before after

R-ALLREDUCE
∀i j, si.rows = sj.rows

∀i j k, i (cid:54)= j =⇒ si[k] (cid:13)(cid:63) sj[k]

s = (cid:93)si

R-REDUCESCATTER
∀i j, si.rows = sj.rows

∀i j k, i (cid:54)= j =⇒ si[k] (cid:13)(cid:63) sj[k]

{ di : si } AllReduce { di : s }

{ di : si } ReduceScatter { di : s(cid:48)

s = (cid:93)si
i }

s(cid:48)
i = scatter(s, i)[i]

R-ALLGATHER
∀i j, i (cid:54)= j =⇒ si.rows (cid:13)(cid:63) sj.rows

∀i j, |si.rows| = |sj.rows|

s = (cid:93)si

{ di : si } AllGather { di : s }

R-REDUCE
∀i j, si.rows = sj.rows

∀i j k, i (cid:54)= j =⇒ si[k] (cid:13)(cid:63) sj[k]

s = (cid:93)si

{ di : si } Reduce { d0 : s, di : {}

i(cid:54)=0

}

disjoint
(cid:13)(cid:63)
(cid:93) addition
scatter(s, i) scatters non-empty rows in s over devices i

non-empty rows
length

rows
| · |

R-BROADCAST
∀i, si ≤ s0

∃i, si < s0

{ di : si } Broadcast { di : s0 }

Figure 8: Semantics of collective operations. with the right presents examples of each operation. For those examples, we
have in total 4 devices .e., device 0,1,2, and 3), so each device state is a 4 × 4 matrix. We assume the reduction happens
between only device 0 and 1. The pre-condition states of device 0 (top) and 1 (bottom) are on the left, and after a step of
reduction, their states turn into the post-condition states on the right.

3.3 Reduction Programs

We now turn to our reduction language which is built on top
of the formalism of collective operations.

program ∈
reduction ∈
slice
f orm

[reduction]
slice × f orm × C

:= e
:= InsideGroup | Parallel(e) | Master(e)

A reduction strategy is represented as a program, which
is essentially a list of reduction instructions. A reduction
instruction consists of a slice, a f orm, and a collective
operation C. We use e to represent a level in the synthesis
hierarchy. The slice chooses a level. The f orm has three
patterns: InsideGroup, Parallel(e), and Master(e). Inside a
reduction, the e carried in the form must be an ancestor of
the one carried in the slice. The slice and the form together
decide the device groups that will perform the operation C.

It turns out that slice and f orm are quite expressive
and can encode many common hierarchical communi-
cation patterns. Table 2 demonstrates several examples
using the system hierarchy in Figure 2a. Speciﬁcally, a
slice divides devices into reduction groups, and f orm
decides the reduction form happening for the reduction
groups. For example, consider that the slice is CPU,
i.e.,
then we get reduction groups within each CPU,

{A0, A1, A2, A3}, {B0, B1, B2, B3}, {C0, C1, C2, C3}, {D0, D1, D2, D3}.
Now, if the form is InsideGroup, then we perform reduction
within each reduction group. If the form is Parallel(e), we
perform reduction over the ﬁrst device in each group, the
second device in each group, etc, if they connect to the
same e. Thus, Parallel(server) generates {A0, B0}, {A1, B1},
etc., whereas Parallel(rack) generates {A0, B0, C0, D0} etc.
Master generates the device groups in the same way as
Parallel, but only reduces over the ﬁrst device group.

Note that Table 2 presents device groups for reduction over
the system hierarchy [(rack, 1), (server, 2), (CPU, 2), (GPU,
4)]. As we discussed in Section 2.5, reduction over speciﬁc
parallelism axes will use the synthesis hierarchy formed by
parallelism factors, and we will get reduction groups for
that particular reduction axis like {A0, A1}, {A2, A3} etc.

Supposing slice and f orm derive the device groups Gi,
which are disjoint by construction, we deﬁne the semantics
of a reduction instruction as:

(slice, f orm) derives Gi

{ Gi, G } (slice, f orm, C) { G(cid:48)

{ Gi } C { G(cid:48)
i, G }

i }

where each device group participating in the reduction gets
the device states updated according to the semantics of
collective operations, and devices not participating in the
reduction have their states unchanged. A reduction program

Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning

slice

f orm

groups(slice, f orm)

CPU

InsideGroup

Parallel(server)

Parallel(rack)

server

Master(rack)
InsideGroup

Parallel(rack)

rack

InsideGroup

{A0, A1, A2, A3}, {B0, B1, B2, B3},
{C0, C1, C2, C3}, {D0, D1, D2, D3}
{A0, B0}, {A1, B1}, {A2, B2}, {A3, B3}
{C0, D0}, {C1, D1}, {C2, D2}, {C3, D3}
{A0, B0, C0, D0}, {A1, B1, C1, D1},
{A2, B2, C2, D2}, {A3, B3, C3, D3}
{A0, B0, C0, D0}
{A0, A1, A2, A3, B0, B1, B2, B3},
{C0, C1, C2, C3, D0, D1, D2, D3}
{A0, C0}, {A1, C1}, {A2, C2}, {A3, C3}
{B0, D0}, {B1, D1}, {B2, D2}, {B3, D3}
{A0, A1, A2, A3, B0, B1, B2, B3,
C0, C1, C2, C3, D0, D1, D2, D3}

Table 2: Hierarchical communication patterns for Figure 2a.

then iteratively applies each reduction:

program = reduction

i∈n

{ Gi } reductioni { Gi+1 }

{ G0 } program { Gn+1 }

3.4 Synthesis Hierarchy

In Section 2.5, we have proposed and compared different
synthesis hierarchies for synthesizing reduction programs:
(a) System hierarchy ((cid:2)1 2 2 4(cid:3))
(b) Column-based parallelism factors ((cid:2)1 1 1 2 2 1 2 2(cid:3))
(c) Row-based parallelism factors ((cid:2)1 1 2 2 1 2 1 2(cid:3))
(d) Reduction axis parallelism factors ((cid:2)1 2 1 2(cid:3))

P 2 uses (d). Here, we formally prove the theorem that justi-
ﬁes our choice. First, every reduction instruction essentially
decides the device groups G and the operation C. Therefore,
a program can be lowered to a sequence (G1, C1), (G2, C2),
..., (Gn, Cn). Since (d) includes only the reduction axis,
lowering for (d) applies the generated grouping patterns to
non-reduction axes when forming device groups.

Deﬁnition 3.1 (Expressive power of synthesis hierarchy).
A synthesis hierarchy is more expressive than (≥) another, if
every valid lowered program L synthesized using the latter
can be synthesized using the former.

Theorem 3.2. (d) ≥ (c) ≥ (b) ≥ (a).

Proof. For space reasons, we show one example that il-
lustrates the key proof strategy, and we refer the reader
to the appendix for the full proof. Consider proving
(c) ≥ (b), where (b) synthesizes a valid reduction step
(e2, Parallel(e1), C). For the reduction to be valid, every
reduction group must be partitioned only over the reduc-
tion axis. Therefore, all non-reduction parallelism factors
column-wisely between e1 (exclusive) and e2 (inclusive)
can only be 1. An example is given below on the left.
Now we construct a reduction instruction for (c) that ex-
presses the same reduction. Suppose the reduction step
in (b) covers parallelism factors ei, ..., ej on the reduction
axis. Let e(cid:48)
1 be the level corresponding to the parallelism

factor right before ei row-wisely, and let e(cid:48)
(e(cid:48)

1), C) is a desired reduction instruction.

2, Parallel(e(cid:48)

2 be ej. Then

e1
reduction
axis






1
1

1 x0,3
x0,0
x1,0
1 x1,3
x2,0 x2,1 x2,2 x2,3
1 x3,3
1

1





e2






1
1

1
x0,3
x0,0
1 x1,3
x1,0
x2,0 x2,1 x2,2 x2,3
x3,3
1

1

1






e(cid:48)
2

e(cid:48)
1

3.5 Program Synthesis for Reduction Programs

So far, we have given the constraint for generating paral-
lelism matrices (Section 3.1) and how we can obtain a syn-
thesis hierarchy from a parallelism matrix (Section 3.4). The
last missing piece is how to synthesize reduction programs.

To formalize the synthesis problem, we need an initial pre-
condition state as the beginning state and a post-condition
state as the ﬁnal desired state. Initially, every device only
holds its own data, and therefore device i has 1 in the ith
column, and 0 in any other position. In the ﬁnal desired
state, a device should have 1 in all columns corresponding
to devices in its reduction group, and 0 in any other position.
An extra indirection is caused from using the reduction axis
parallelism factors as the synthesis hierarchy (Section 3.4),
which only includes part of the system, and then lowers
the program to the full system. Therefore, our goal is to
synthesize a program, whose lowering L subjects to:






di :







i
0 . . . 1 . . . 0
.
.
.
.
.
.
.
.
.
0 · · · 1 . . . 0

. . .

. . .












L






di :

i

j







0 . . . 1 . . . 0 . . . 1 . . . 0
.
.
.
.
.
.
.
.
.
0 · · · 1 . . . 0 · · · 1 · · · 0

. . .

. . .

. . .

. . .

.
.
.

.
.
.












supposing di reduces with devices j.

Given the syntax and the semantics of reduction programs,
we use syntax-guided program synthesis (Alur et al., 2013)
to synthesize programs in increasing order of program size.

4 EXPERIMENTS

We implement P 2 to synthesize parallelism matrices and re-
duction programs, and lower the programs into sequences of
XLA collective operations, which in turn result in sequences
of NCCL calls on the XLA GPU backend. We measure the
execution time of the compiled programs. The experiments
aim to answer the following research questions:

RQ1 What is the impact of parallelism placement on
reduction algorithms?

RQ2 Are our various techniques for taming the search
space effective so that we can quickly enumerate a wide
variety of reduction programs?

RQ3 Given a parallelism placement, can we ﬁnd reduc-
tion strategies that outperform the default implementation

Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning

Parallelism
axes

Parallelism
matrix

Reduction on
the 0th axis
Ring Tree

Reduction on
the 1st axis
Ring Tree

(a) 2 nodes, each with 16 A100 GPUs sharing one NVSwitch and
one NIC, and all NICs are connected in a data center

(b) 2 nodes, each with 8 V100 GPUs forming a ring via NVLink
and connected via PCIe switches. Each node consists of two CPUs
(each owning 4 GPUs) with one NIC to the DCN. A shared NIC
connecting the two CPUs is a modeling simpliﬁcation – in reality
cross-domain communication is through shared memory.

Figure 9: System topology models for 2 nodes. For the
experiments, we run on both 2 and 4 nodes.

(i.e., AllReduce), and if so what is their form?

The experiments ran on two different GPU system conﬁg-
urations available on Google Cloud Platform (GCP) (see
Figure 9): (i) NVIDIA A100, where each node consists of
16 GPUs sharing one NVSwitch and one NIC connecting to
the data center network; and (ii) NVIDIA Tesla V100, where
each node consists of 8 GPUs forming a ring via NVLink;
each pair of GPUs are connected via PCIe switches, and
each of the two CPUs of the node has 4 GPUS in its PCIe
domain. We experiment with both NCCL ring reduction and
tree reduction (Sanders et al., 2009), set by NCCL ALGO.

We run experiments with 2 and 4 nodes. For A100, the
system hierarchy is (cid:2)2 16(cid:3) or (cid:2)4 16(cid:3). For V100, since the
NVLink ring connects all 8 GPUs, and the NVLink ring
has much higher bandwidth than PCIe bridges, we put 8
GPUs inside one layer, and so with 2 or 4 nodes, the system
hierarchy is (cid:2)2 8(cid:3) or (cid:2)4 8(cid:3), respectively. Each GPU carries
a large amount of data ((229× nodes) of ﬂoat32) to reduce
the impact of latency, and each program runs 10 times to
reduce the impact of network noise.

For each system, we synthesize parallelism mappings and
reduction programs for (1) a single parallelism axis; (2) all
combinations of two parallelism axes, with reduction on one
of the axes; and (3) three parallelism axes, with reduction
on the ﬁrst and the third axes. We can easily scale to more
axes, though up to three axes are quite common in practical
settings, and many observations can already be illustrated.

Next, we discuss the results and insights from the experi-
ments. For space reasons, we present only representative

(cid:2)2 32(cid:3)

(cid:2)4 16(cid:3)

(cid:2)(cid:2)1 2(cid:3) (cid:2)4 8(cid:3)(cid:3)
0.12
(cid:2)(cid:2)2 1(cid:3) (cid:2)2 16(cid:3)(cid:3) 37.16
(cid:2)(cid:2)1 4(cid:3) (cid:2)4 4(cid:3)(cid:3)
0.15
(cid:2)(cid:2)2 2(cid:3) (cid:2)2 8(cid:3)(cid:3) 28.77
(cid:2)(cid:2)4 1(cid:3) (cid:2)1 16(cid:3)(cid:3) 56.13
(cid:2)(cid:2)1 8(cid:3) (cid:2)4 2(cid:3)(cid:3)
0.17
(cid:2)(cid:2)2 4(cid:3) (cid:2)2 4(cid:3)(cid:3) 16.52
(cid:2)(cid:2)4 2(cid:3) (cid:2)1 8(cid:3)(cid:3) 34.05

4 nodes, each with 16 A100
A1
A2
B1
B2
B3
C1
C2
C3
4 nodes, each with 8 V100
E1
E2
E3

(cid:2)(cid:2)1 8(cid:3) (cid:2)4 1(cid:3)(cid:3)
0.28
(cid:2)(cid:2)2 4(cid:3) (cid:2)2 2(cid:3)(cid:3) 14.25
(cid:2)(cid:2)4 2(cid:3) (cid:2)1 4(cid:3)(cid:3) 14.84

(cid:2)8 8(cid:3)

(cid:2)8 4(cid:3)

8.74
0.17
36.94
4.81
0.20 17.70
8.39
19.81
89.70
0.18
0.21 33.92
9.18 15.68
0.17

41.23

0.39 21.74
15.48 10.98
2.96
19.90

9.89
3.41
19.03
4.99
0.22
41.06
9.43
0.21

30.42
7.34
0.43

Table 3: Reduction time in seconds of running AllReduce.

cases, and we put the full experiment results in the appendix.

4.1 Synthesizing Parallelism Placement

Result 1 (RQ 1): The performance of AllReduce differs
signiﬁcantly among parallelism matrices, up to 448.5×.

The experiment results are given in Table 3. For a particular
parallelism axis (e.g., A), we compare the reduction time
for difference parallelism matrices (e.g., A1 and A2) with
each NCCL algorithm and the reduction axis. Notably, for
reducion on the 0th axis and with the Tree algorithm, B3
(89.70s) is slower than B1 (0.20s) by 448.5×.

The difference is due to the fact that different parallelism
matrices lead to different data placement. In B1, the ﬁrst
row of the the matrix ((cid:2)1 4(cid:3)) means that devices to be re-
duced are inside a single node, where the local NVSwitch
can perform the reduction efﬁciently. For B3, the ﬁrst row
((cid:2)4 1(cid:3)) puts reduction groups across nodes, going through
the slow data-center network. However, B3 can still be use-
ful for a diffferent reduction: since it puts the 1st reduction
axis inside a single node, for a reduction on the 1st axis, B3
(0.22s on Tree) is 86.5× faster than B1 (19.03s). In prac-
tice, models with multiple parallelism forms (e.g., Shoeybi
et al. (2020)) involve reductions across both axes, and the
selection of a mapping should take all of them into account.

4.2 Synthesizing Reduction Programs

Now we turn to the reduction programs synthesized for each
parallelism matrix. Table 4 presents experiment results.

Result 2 (RQ 2): Our pruning techniques are effective for
the synthesizer to achieve fast synthesis time.

With our formalism, a program cannot be arbitrarily large,
since our carefully crafted semantics of collective operations
enforces a form of information increase for every operation.

Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning

NCCL
algo

Parallelism
axes

Synthesis
time (s)

Programs
outperforming
AllReduce / total
programs

Tree

Ring

Ring

(cid:2)8 4(cid:3)

(cid:2)4 16(cid:3)

(cid:2)16 2 2(cid:3)

2 nodes, each with 16 A100
F1
F2
4 nodes, each with 16 A100
G1
G2
H1
H2
I1
I2
J1
4 nodes, each with 8 V100
(cid:2)8 2 2(cid:3)
K1
K2
L1

(cid:2)2 2 16(cid:3)

(cid:2)64(cid:3)

(cid:2)32(cid:3)

Ring

Ring

Ring

Tree

0.03

0.04

0.97

0.93

1.16

0.24

0.06

14/47

10/53

25/235

29/235

5/47

17/188

11/47

Parallelism matrix

(cid:2)(cid:2)1 8(cid:3)(cid:2)2 2(cid:3)(cid:3)
(cid:2)(cid:2)2 4(cid:3)(cid:2)1 4(cid:3)(cid:3)

(cid:2)(cid:2)1 4(cid:3)(cid:2)4 4(cid:3)(cid:3)
(cid:2)(cid:2)4 1(cid:3)(cid:2)1 16(cid:3)(cid:3)
(cid:2)(cid:2)1 16(cid:3)(cid:2)2 1(cid:3)(cid:2)2 1(cid:3)(cid:3)
(cid:2)(cid:2)2 8(cid:3)(cid:2)2 1(cid:3)(cid:2)1 2(cid:3)(cid:3)
(cid:2)(cid:2)2 1(cid:3)(cid:2)2 1(cid:3)(cid:2)1 16(cid:3)(cid:3)
(cid:2)(cid:2)1 2(cid:3)(cid:2)2 1(cid:3)(cid:2)2 8(cid:3)(cid:3)
(cid:2)(cid:2)4 16(cid:3)(cid:3)

(cid:2)(cid:2)2 4(cid:3)(cid:2)2 1(cid:3)(cid:2)1 2(cid:3)(cid:3)
(cid:2)(cid:2)1 8(cid:3)(cid:2)2 1(cid:3)(cid:2)2 1(cid:3)(cid:3)
(cid:2)(cid:2)4 8(cid:3)(cid:3)

AllReduce
(bold if the
optimal
AllReduce)

Optimal
(bold if
overall
optimal)

Speedup

0.17
16.84

0.20
89.70
4.79
4.91
4.82
5.28
5.75

4.80
4.40
4.83

0.17
9.19

0.17
56.13
4.63
3.10
2.99
4.77
4.74

2.35
4.40
3.45

1×
1.83×

1.17×
1.60×
1.03×
1.58×
1.61×
1.11×
1.21×

2.04×
1×
1.4×

Table 4: Reduction time in seconds for running AllReduce and the synthesized optimal reduction strategy (reduction on the
0th axis for parallelism axes of size 1 and 2, and on the 0th and 2rd axes for parallelism axes of size 3).

In our experiments, we set 5 as the program size limit for
the synthesizer, which turns out to be sufﬁcient to generate
interesting reduction patterns. With this setup, the longest
synthesis time is under 2 seconds (for up to 235 programs).
Increasing the size limit makes the synthesis slightly slower,
but, for most cases, does not generate new programs.

If the reduction axes can be put within
Result 3 (RQ 3):
one node, then a single step AllReduce inside that node is
the most performant reduction due to fast local bandwidth.

We observe this result from the difference between F1 and
F2. F1 assigns the reduction axis to the GPU level, and thus
AllReduce is the most performant reduction, outperforming
F2, which requires cross-node reduction, by 99.06×.

Result 4 (RQ3): Synthesized programs can help mitigate
the impact of parallelism placement.

Consider G1 and G2. As discussed before, G2’s AllReduce
(89.7s) is 448.5× slower than G1 (0.20s). Synthesized
programs have helped bridge the gap: G2’s optimal program
is only 330.2× slower. However, the performance difference
here is signiﬁcant and the help is limited. The case of H1
and H2 is more interesting: H1’s AllReduce is 1.03× slower
than H2, but its optimal program is 1.49× faster than H2!

On the other hand, it is also possible that synthesis aggra-
vates the impact of parallelism placement. For example, for
I1 and I2, the performance difference jumps from 1.10× for
AllReduce to 1.60× for the optimal program.

Result 5 (RQ3): For reduction across nodes, a topology-
aware reduction program tends to outperform a single step
AllReduce, with speedup on average 1.28×, upto 2.04×.

Table 4 shows that when cross-node communication is
needed the optimal program tends to outperform AllReduce.
For example, the speedup is 1.84× in F2, and 2.04× in K2.
For 69% of all mappings across both systems, synthesized
programs outperform AllReduce by 1.27× on average.

We present common optimal reduction programs applied
to our running examples (Section 2) in Figure 10. (i) Fig-
ure 10i ﬁrst reduces local data to a root device, performs
AllReduce between root devices, and broadcasts the result
from the root device to each device. (ii) Figure 10ii ﬁrst
performs ReduceScatter between local devices, and then
AllReduce between remote devices, and ﬁnally AllGather
between local devices. Both reduction programs utilize the
topology, by performing local communication ﬁrst, which
is often more efﬁcient due to local high bandwidth. Now,
the data to be reduced across nodes in the intermediate step
is signiﬁcantly smaller. The ﬁnal step is again local commu-
nication. Thus, the reduction programs have overall better
performance than AllReduce. It turns out that both reduc-
tion programs have been recently proposed: program (i)
has been used in Goyal et al. (2018); Jia et al. (2018a), and
program (ii) has been proposed by Cho et al. (2019).

Furthermore, the experiments suggest that program (ii) is
more often the optimal one and outperforms (i) by a larger
speedup. Speciﬁcally, when (i) is optimal, it outperforms
(ii) by only about 1.1× (up to 1.12×); when (ii) is optimal,
it outperforms (i) by about 1.3× (up to 2.73×).

Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning

(i) Reduce-AllReduce-Broadcast (ii) ReduceScatter-AllReduce-AllGather

Figure 10: Common optimal reduction programs

(a) 4 nodes of V100 with NCCL Ring and parallelism axes (cid:2)2 16(cid:3),
reduction on the 1st axis. Synthesis 0.12s, and simulation 0.54s.

(b) 4 nodes of A100 with NCCL Tree and parallelism axes (cid:2)4 2 8(cid:3),
reduction on 0th and 2rd axes. Synthesis 2.86s, simulation 3.09s.

Figure 11: Simulation results, in increasing order of experi-
ment time. Measurements are ’•’ and solid, and simulations
are ’×’ and translucent. Colors denote parallelism matrices.

5 SIMULATION

In this section, we apply simulation to the system topologies
in Figure 9, and show that P 2 can identify near-optimal
programs, to reduce the need for evaluation over hundreds
or thousands of synthesized mappings and strategies.

Assumptions. We use 100Gbps NICS, which we assume
were utilized at 60%, yielding an effective 8GB/s. For
the PCIe switches we assumed 32GB/s. For the V100
NVLink ring we assume 135GB/s in each direction – an
optimistic 90% of the nominal uni-directional bandwidth
(150GB/s) (Jia et al., 2018c). For the A100 NVLink switch,
we assume 270GB/s uni-directional bandwidth – again 90%
of the nominal value (300GB/s in each direction) (NVIDIA).

Results. Figure 11 shows that P 2 predictions follow the

Top-1 Top-2 Top-3 Top-5 Top-6 Top-10
A100 46.8% 71.0% 72.6% 74.2% 90.3% 94.4%
V100 60.5% 67.1% 71.1% 76.3% 76.3% 88.1%
Total 52.0% 69.5% 72.0% 75.0% 85.0% 92.0%

Table 5: Prediction accuracy.

same trend as the V100 experiments (11a), and are very
close to the absolute performance for A100 (11b). The
main reason for reduced absolute accuracy in V100 is the
imperfect modeling of cross-domain communication. Note
that there exist a few programs for which the simulation
result is notably slower than the experiments, mostly due
to XLA optimizations. For example, program 10 and 11
(in 11a) are all 2 steps of AllReduce, which XLA optimizes
to a single step AllReduce. We do not extend P 2 with any
optimizations, as optimized programs are themselves valid
synthesizable programs; in this speciﬁc example program 9.
P 2 successfully predicts the performance of program 9.

Table 5 summarizes the accuracy over all experiments for
the two GPU systems. Overall, P 2 delivers 52% top-1
accuracy, 75% top-5 accuracy, and 92% top-10 accuracy.

6 RELATED WORK

Parallelism forms. Recent work has explored combinations
of parallelism forms. Jia et al. (2018b) propose layer-wise
parallelism that allows each network layer to use a differ-
ent parallelization strategy. FlexFlow (Jia et al., 2019) uses
guided randomized search to ﬁnd a fast parallelism combina-
tion. Narayanan et al. (2021) combine pipeline, tensor and
data parallelism to scale transformer training to thousands of
GPUs, extending previous model parallelism work (Shoeybi
et al., 2020). ZeRO-DP (Rajbhandari et al., 2020) includes
three optimization stages partitioning over optimizer state,
gradient, and parameters. To our best knowledge, no prior
work has discussed parallelism placement, and typically
commit to a speciﬁc placement (e.g. model parallelism
within a host, batch parallelism across (Narayanan et al.,
2021).) which can get involved when multiple axes oc-
cur. Finally, there exists a rich body of work on operator
mapping, e.g. (Addanki et al., 2019; Gao et al., 2018; Mirho-
seini et al., 2017), but the focus there does not include the
structured forms of parallelism and reductions we address.

Program synthesis for communication. For a given topol-
ogy, SCCL (Cai et al., 2021) synthesizes optimal collective
algorithms as a sequence of sends, but the work has focused
on the single-node setting. SCCL takes a more ﬁne-grained
system topology as a graph with bandwidth constraints on
GPUs and edges, and uses an SMT solver to synthesize algo-
rithms. It is possible for P 2 to use SMT, but we found that
the structure we imposed on the problem already enables
fast enumerative syntax-guided synthesis. Blink (Wang

Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning

et al., 2020) performs the synthesis based on an approxi-
mate spanning-tree packing algorithm for intra-node com-
munication, but always uses program (i) (Figure 10ii) for
inter-node communication. PLink (Luo et al., 2020) probes
the network locality and groups nodes by physical afﬁnity,
and performs (i) for intra-group reduction. By contrast, P 2
synthesizes hierarchical strategies using collectives.

Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M.,
Le, Q. V., Mao, M. Z., Ranzato, M., Senior, A., Tucker,
P., Yang, K., and Ng, A. Y. Large scale distributed deep
networks. In Proceedings of the 25th International Con-
ference on Neural Information Processing Systems - Vol-
ume 1, NIPS’12, pp. 1223–1231, Red Hook, NY, USA,
2012. Curran Associates Inc.

Reduction strategies. BlueConnect (Cho et al., 2019) pro-
pose program (ii) (Figure 10i) for hierarchical systems,
which is later generalized by FlexReduce (Lee et al., 2020)
to asymmetric topologies. On the other hand, P 2 systemati-
cally generates and compares a wide range of hierarchical
reduction strategies for different parallelism placements.

7 CONCLUSION

We have presented a framework to synthesize structured
forms of parallelism mappings and hierarchy-aware reduc-
tion strategies. Our tool can be useful for speeding up ML
models, and also for establishing projections about commu-
nication costs when investigating new system hierarchies.

REFERENCES

Addanki, R., Venkatakrishnan, S. B., Gupta, S., Mao, H.,
and Alizadeh, M. Placeto: Learning Generalizable
Device Placement Algorithms for Distributed Machine
Learning. Curran Associates Inc., Red Hook, NY, USA,
2019.

Alur, R., Bodik, R., Juniwal, G., Martin, M. M.,
Raghothaman, M., Seshia, S. A., Singh, R., Solar-
Lezama, A., Torlak, E., and Udupa, A. Syntax-guided
synthesis. IEEE, 2013.

Amodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J.,
Battenberg, E., Case, C., Casper, J., Catanzaro, B., Cheng,
Q., Chen, G., et al. Deep speech 2: end-to-end speech
In Proceedings
recognition in english and mandarin.
of the 33rd International Conference on International
Conference on Machine Learning-Volume 48, pp. 173–
182, 2016.

Cai, Z., Liu, Z., Maleki, S., Musuvathi, M., Mytkowicz,
T., Nelson, J., and Saarikivi, O. Synthesizing optimal
collective algorithms. In Proceedings of the 26th ACM
SIGPLAN Symposium on Principles and Practice of Par-
allel Programming, PPoPP ’21, pp. 62–75, New York,
NY, USA, 2021. Association for Computing Machinery.
ISBN 9781450382946. doi: 10.1145/3437801.3441620.

Cho, M., Finkler, U., and Kung, D. Blueconnect: Novel
hierarchical all-reduce on multi-tired network for deep
learning. In Proceedings of the 2nd SysML Conference,
2019.

Gao, Y., Chen, L., and Li, B. Spotlight: Optimizing de-
In
vice placement for training deep neural networks.
Dy, J. G. and Krause, A. (eds.), Proceedings of the 35th
International Conference on Machine Learning, ICML
2018, Stockholmsm¨assan, Stockholm, Sweden, July 10-15,
2018, volume 80 of Proceedings of Machine Learning
Research, pp. 1662–1670. PMLR, 2018. URL http://
proceedings.mlr.press/v80/gao18a.html.

GCP. Gpus on compute engine. https://cloud.
google.com/compute/docs/gpus. Accessed:
2021-10-07.

Goyal, P., Doll´ar, P., Girshick, R., Noordhuis, P.,
Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and
He, K. Accurate, large minibatch sgd: Training imagenet
in 1 hour, 2018.

He, K., Zhang, X., Ren, S., and Sun, J. Deep resid-
In 2016 IEEE
ual learning for image recognition.
Conference on Computer Vision and Pattern Recogni-
tion, CVPR 2016, Las Vegas, NV, USA, June 27-30,
2016, pp. 770–778. IEEE Computer Society, 2016. doi:
10.1109/CVPR.2016.90. URL https://doi.org/
10.1109/CVPR.2016.90.

Hoare, C. A. R. An axiomatic basis for computer program-
ming. Commun. ACM, 12(10):576–580, October 1969.
ISSN 0001-0782. doi: 10.1145/363235.363259.

Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen,
M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe:
Efﬁcient training of giant neural networks using pipeline
parallelism. Advances in neural information processing
systems, 32:103–112, 2019.

Jia, X., Song, S., He, W., Wang, Y., Rong, H., Zhou, F.,
Xie, L., Guo, Z., Yang, Y., Yu, L., et al. Highly scal-
able deep learning training system with mixed-precision:
arXiv preprint
Training imagenet in four minutes.
arXiv:1807.11205, 2018a.

Jia, Z., Lin, S., Qi, C. R., and Aiken, A. Exploring
hidden dimensions in accelerating convolutional neural
In Dy, J. and Krause, A. (eds.), Proceed-
networks.
ings of the 35th International Conference on Machine
Learning, volume 80 of Proceedings of Machine Learn-
ing Research, pp. 2274–2283. PMLR, 10–15 Jul 2018b.
URL https://proceedings.mlr.press/v80/
jia18a.html.

Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning

Sanders, P., Speck, J., and Tr¨aff, J. L. Two-tree algorithms
for full bandwidth broadcast, reduction and scan. Parallel
Computing, 35(12):581–594, 2009.

Sergeev, A. and Balso, M. D. Horovod: fast and easy

distributed deep learning in tensorﬂow, 2018.

Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,
J., and Catanzaro, B. Megatron-lm: Training multi-
billion parameter language models using model paral-
lelism, 2020.

Thakur, R., Rabenseifner, R., and Gropp, W. Optimization
of collective communication operations in mpich. The
International Journal of High Performance Computing
Applications, 19(1):49–66, 2005.

Wang, G., Venkataraman, S., Phanishayee, A., Thelin, J.,
Devanur, N. R., and Stoica, I. Blink: Fast and generic col-
lectives for distributed ML. In Dhillon, I. S., Papailiopou-
los, D. S., and Sze, V. (eds.), Proceedings of Machine
Learning and Systems 2020, MLSys 2020, Austin, TX,
USA, March 2-4, 2020. mlsys.org, 2020. URL https:
//proceedings.mlsys.org/book/299.pdf.

Jia, Z., Maggioni, M., Staiger, B., and Scarpazza, D. P.
Dissecting the NVIDIA volta GPU architecture via mi-
crobenchmarking. CoRR, abs/1804.06826, 2018c. URL
http://arxiv.org/abs/1804.06826.

Jia, Z., Zaharia, M., and Aiken, A. Beyond data and model
parallelism for deep neural networks. In Talwalkar, A.,
Smith, V., and Zaharia, M. (eds.), Proceedings of Machine
Learning and Systems, volume 1, pp. 1–13, 2019.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classiﬁcation with deep convolutional neural networks.
Advances in neural information processing systems, 25:
1097–1105, 2012.

Lee, J., Hwang, I., Shah, S., and Cho, M. Flexreduce:
Flexible all-reduce for distributed deep learning on asym-
metric network topology. In 2020 57th ACM/IEEE De-
sign Automation Conference (DAC), pp. 1–6, 2020. doi:
10.1109/DAC18072.2020.9218538.

Luo, L., West, P., Krishnamurthy, A., Ceze, L., and Nel-
son, J. Plink: Discovering and exploiting locality for
accelerated distributed training on the public cloud. In
Dhillon, I. S., Papailiopoulos, D. S., and Sze, V. (eds.),
Proceedings of Machine Learning and Systems 2020, ML-
Sys 2020, Austin, TX, USA, March 2-4, 2020. mlsys.org,
2020. URL https://proceedings.mlsys.org/
book/293.pdf.

Mirhoseini, A., Pham, H., Le, Q. V., Steiner, B., Larsen,
R., Zhou, Y., Kumar, N., Norouzi, M., Bengio, S., and
Dean, J. Device placement optimization with reinforce-
ment learning. In Proceedings of the 34th International
Conference on Machine Learning - Volume 70, ICML’17,
pp. 2430–2439. JMLR.org, 2017.

Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Pat-
wary, M., Korthikanti, V. A., Vainbrand, D., Kashinkunti,
P., Bernauer, J., Catanzaro, B., Phanishayee, A., and Za-
haria, M. Efﬁcient large-scale language model training
on gpu clusters using megatron-lm. In The International
Conference for High Performance Computing, Network-
ing, Storage, and Analysis, 2021.

NVIDIA. Nvidia nvlink and nvswitch. https://www.
nvidia.com/en-gb/data-center/nvlink/.
Accessed: 2021-10-07.

NVidia. The nvidia collective communication library (nccl),
2021. URL https://developer.nvidia.com/
nccl.

Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero:
Memory optimizations toward training trillion parameter
models, 2020.

Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning

A EXPERIMENTS

Parallelism
axes

Reduce
axes

Synthesis
time

2 nodes each with 16 A100

Simulation time

Programs

Parallelism matrix

AllReduce

Optimal

Speedup

Ring

Tree

Ring

Tree

Ring

Tree Ring

Tree

Ring

Tree

[32]
[2 16]

[4 8]

[8 4]

[16 2]

[0]
[0]

[1]

[0]

[1]

[0]

[1]

[0]

[1]

0.244
0.004

0.458
0.014

0.860
0.019

9/47
0/6

3/47
1/6

0.065

0.019

0.208

10/50

1/50

0.026

0.125

0.137

11/50

5/50

0.030

0.135

0.149

12/50

3/50

0.033

0.136

0.151

14/50

2/50

0.026

0.124

0.136

11/50

7/50

0.067

0.195

0.214

10/50

2/50

0.005

0.011

0.012

0/6

1/6

4 nodes each with 16 A100

[64]
[2 32]

[4 16]

[0]
[0]

[1]

[0]

1.161
0.006

1.868
0.019

2.01
0.022

6/47
0/6

5/47
1/6

0.463

0.160

1.27

18/94

10/94

0.043

0.259

0.287

12/53

10/53

[1]

0.133

0.619

0.704

21/97

9/97

[8 8]

[0]

0.091

0.512

0.590

20/97

7/97

[1]

0.084

0.508

0.598

19/97

17/97

[16 4]

[0]

0.149

0.631

0.712

21/97

17/97

[1]

0.042

0.261

0.297

13/53

5/53

[32 2]

[0]

[1]

0.483

1.183

1.30

20/94

15/94

0.008

0.020

0.022

0/6

1/6

[2 16]
[[1 2] [2 8]]
[[2 1] [1 16]]
[[2 1] [1 16]]
[[1 2] [2 8]]
[[1 4] [2 4]]
[[2 2] [1 8]]
[[2 2] [1 8]]
[[1 4] [2 4]]
[[1 8] [2 2]]
[[2 4] [1 4]]
[[2 4] [1 4]]
[[1 8] [2 2]]
[[1 16] [2 1]]
[[2 8] [1 2]]
[[2 8] [1 2]]
[[1 16] [2 1]]

[[4 16]]
[[1 2] [4 8]]
[[2 1] [2 16]]
[[2 1] [2 16]]
[[1 2] [4 8]]
[[1 4] [4 4]]
[[2 2] [2 8]]
[[4 1] [1 16]]
[[4 1] [1 16]]
[[2 2] [2 8]]
[[1 4] [4 4]]
[[1 8] [4 2]]
[[2 4] [2 4]]
[[4 2] [1 8]]
[[4 2] [1 8]]
[[2 4] [2 4]]
[[1 8] [4 2]]
[[1 16] [4 1]]
[[2 8] [2 2]]
[[4 4] [1 4]]
[[4 4] [1 4]]
[[2 8] [2 2]]
[[1 16] [4 1]]
[[2 16] [2 1]]
[[4 8] [1 2]]
[[4 8] [1 2]]
[[2 16] [2 1]]

[16 2 2]

[0 2]

0.968

2.36

2.55

[8 2 4]

[0 2]

1.107

2.88

3.08

[4 2 8]

[0 2]

1.143

2.86

3.09

28/235 22/235

25/188 21/188 [[1 16] [2 1] [2 1]]
[[2 8] [2 1] [1 2]]
[[2 8] [1 2] [2 1]]
[[4 4] [1 2] [1 2]]
[[1 8] [2 1] [2 2]]
[[2 4] [2 1] [1 4]]
[[2 4] [1 2] [2 2]]
[[4 2] [1 2] [1 4]]
[[1 8] [1 2] [4 1]]
[[2 2] [2 1] [1 8]]
[[1 4] [2 1] [2 4]]
[[4 1] [1 2] [1 8]]
[[1 4] [1 2] [4 2]]

32/235 24/235

3.57
0.17

3.18
0.12

2.83
4.74
0.12
0.15
36.69 37.18 36.69 37.18
0.20
0.18
0.22
0.18
4.82
8.44
4.82
4.77
0.17
0.15
0.20
0.15
18.5
28.91 19.61 18.48
0.18
0.17
0.21
0.17
9.01
8.97
9.15
16.04
0.19
0.17
0.21
0.17
9.10
9.19
9.26
16.84
0.15
0.17
0.15
0.20
28.78 18.93 18.48 18.00
0.19
0.18
8.86
5.21
0.14
0.11
36.84 36.82 36.84 36.82

0.22
5.34
0.17

0.18
5.05
0.11

5.75
0.17

4.29
0.12

0.22
5.42

0.21
9.18

0.18
5.01

0.17
9.21

0.18
4.81

0.17
8.92

0.21
9.43

0.22
4.99

3.05
6.91
0.15

3.41
9.89
0.20

5.18
4.74
0.15
0.12
37.16 36.94 37.04 36.94
3.07
4.81
8.00
8.74
0.15
0.17
28.77 19.81 18.24 18.55
56.13 89.70 55.99 56.13
0.19
0.18
8.39
4.82
17.70 19.03 13.38 14.85
0.18
0.17
16.52
9.18
34.05 41.23 28.86 29.79
0.18
0.17
15.68
9.22
33.92 41.06 27.93 29.36
0.2
0.18
8.81
5.25
18.30 20.13 14.13 14.90
0.15
0.18
28.68 18.47 19.09 18.41
57.13 85.22 56.23 55.63
3.13
4.74
7.71
9.37
0.11
0.15
37.18 37.10 37.18 37.10
2.71
4.79
2.93
4.91
9.46
9.05
7.87
9.14
2.67
4.80
3.08
4.82
9.29
8.91
7.69
9.19
8.72
9.21
2.99
4.74
3.54
4.77
8.12
8.73
8.80
9.12

3.69
3.97
10.29
10.32
3.62
3.87
9.68
10.24
10.37
3.54
3.77
9.81
9.98

4.63
3.10
9.03
7.08
4.64
3.12
8.91
7.02
5.50
3.04
4.48
7.00
8.65

3.99
10.41
0.17

3.14
7.23
0.11

0.15

0.20

1.49
1
1
1
1.77
1
1.56
1
1.79
1
1.83
1
1.56
1
1.75
1
1

1.21
1
1.003
1.58
1.26
1
1.57
1.003
1
1.74
1.32
1
1.79
1.18
1
1.76
1.21
1
1.76
1.30
1
1.50
1.02
1.51
1.30
1
1
1.03
1.58
1.002
1.29
1.03
1.54
1
1.31
1.7
1.56
1.06
1.25
1.05

1.26
1.13
1
1.1
1
1.18
1.06
1.17
1.02
1.11
1.02
1.18
1.05
1.16
1.02
1.21
1

1.21
1.13
1
1.11
1.24
1.18
1.07
1.60
1.16
1.04
1.28
1.17
1
1.38
1.17
1.02
1.40
1.1
1.03
1.35
1.11
1.003
1.53
1.27
1.35
1.13
1
1.36
1.35
1.09
1.31
1.36
1.26
1.04
1.33
1.19
1.18
1.06
1.21
1.13

Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning

[2 2 16]

[0 2]

0.927

2.32

2.51

[[2 2] [1 2] [2 4]]
29/188 16/188 [[2 1] [2 1] [1 16]]
[[1 2] [2 1] [2 8]]
[[2 1] [1 2] [2 8]]
[[1 2] [1 2] [4 4]]

9.12
4.82
5.28
9.32
9.81

10.36
3.91
4.29
9.61
9.79

9.02
2.99
4.77
7.10
9.81

9.77
3.00
3.66
7.97
9.37

2 nodes each with 8 V100

[16]
[2 8]

[4 4]

[8 2]

[0]
[0]

[1]

[0]

[1]

[0]

[1]

0.058
0.0035

0.158
0.014

0.178
0.134

12/47
0/6

0/47
0/6

0.0257

0.206

0.128

6/50

2/50

0.0181

0.243

0.161

0/50

8/50

0.0182

0.181

0.116

10/50

2/50

0.0272

0.137

0.304

1/50

4/50

0.0036

0.009

0.023

0/6

1/6

4 nodes each with 8 V100

[32]
[2 16]

[2 16]

[4 8]

[0]
[0]

[1]

[0]

0.209
0.0043

0.507
0.024

0.770
0.027

11/47
0/6

7/47
1/6

0.121

0.536

0.621

10/94

7/94

0.027

0.303

0.356

1/53

9/53

[4 8]

[1]

0.096

0.421

0.498

15/97

11/97

[8 4]

[0]

0.103

0.579

0.701

2/97

13/97

[1]

0.0435

0.213

0.241

11/53

2/53

[16 2]

[0]

[1]

0.168

0.597

0.719

12/94

7/94

0.004

0.016

0.019

0/6

1/6

[2 2 8]

[0 2]

0.229

1.105

14/188

[8 2 2]

[0 2]

0.243

1.184

17/188

[[2 8] ]
[[1 2] [2 4]]
[[2 1] [1 8]]
[[2 1] [1 8]]
[[1 2] [2 4]]
[[1 4] [2 2]]
[[2 2] [1 4]]
[[2 2] [1 4]]
[[1 4] [2 2]]
[[1 8] [2 1]]
[[2 4] [1 2]]
[[2 4] [1 2]]
[[1 8] [2 1]]

[[4 8] ]
[[1 2] [4 4]]
[[2 1] [2 8]]
[[2 1] [2 8]]
[[1 2] [4 4]]
[[1 4] [4 2]]
[[2 2] [2 4]]
[[4 1] [1 8]]
[[4 1] [1 8]]
[[2 2] [2 4]]
[[1 4] [4 2]]
[[1 8] [4 1]]
[[2 4] [2 2]]
[[4 2] [1 4]]
[[4 2] [1 4]]
[[2 4] [2 2]]
[[1 8] [4 1]]
[[2 8] [2 1]]
[[4 4] [1 2]]
[[4 4] [1 2]]
[[2 8] [2 1]]
[[1 2] [2 1] [2 4]]
[[2 1] [2 1] [1 8]]
[[2 1] [1 2] [2 4]]
[[1 2] [1 2] [4 2]]
[[1 8] [2 1] [2 1]]
[[2 4] [2 1] [1 2]]
[[4 2] [1 2] [1 2]]
[[2 4] [1 2] [2 1]]

1.06
1.30
1.17
1.21
1.04

1
1
1
1.33
1
1
1.19
1
1.02
1.33
1.07
1
1.02

1.25
1
1.0007
1.004
1.14
1
1.19
1.42
1.33
1.02
1.28
1.3
1.07
1.30
1
1
1.40
1
1.17
1
1.02

1.01
1.61
1.11
1.31
1

1.90
1
1
1
1.10
1
1
1.29
1.46
1
1.003
1
1

0.28
6

2.30
8.44

2.41
9.37

0.40
3.70

2.3
4.58
9.37
8.44
14.53 14.55 14.53 14.55
0.28
0.30
3.70
6.59
12.55 13.01 12.55 13.01
13.5
13.11 16.11 13.11
0.43
2.29
0.43
2.96
7.26
7.52
10.95
7.42
0.30
0.28
0.40
0.28
14.48
14.2
14.24 15.47
0.32
0.33
0.32
0.33
14.51 14.81 14.51 14.47

0.3

0.28

0.39

4.83
9.17

4.57
8.42

0.40
3.82

2.42
7.16

2.26
8.10

0.28
5.78

1
3.66
4.83
1
9.17
8.42
1
14.47 14.52 14.47 14.51
1.81
4.37
2.25
1.003
7.10
7.18
1
12.60 13.00 12.60 13.00
1
13.69 16.07 13.69 13.56
1.001
22.21 30.55 22.04 21.49
1
0.28
0.30
1.14
3.74
6.60
1.15
12.94 15.47 11.21 12.12
1
0.28
1.001
14.25 15.48 14.23 14.49
1
15.33
14.84 19.90
1.29
0.43
0.43
2.96
1.45
10.98
7.34
7.34
1
21.74 30.42 21.74 21.71
4.47
1.83
2.25
2.44
15.00 17.58 14.99 15.01 1.0007
0.33
0.32
0.32
14.53 14.89 14.53 14.66
4.29
4.57
7.17
9.41
4.40
4.80
9.36
15.02

1
1
1
1.90
1.03
1
1
2.04
1.001
1.005

4.29
2.4
6.95
9.41
4.40
2.35
9.35
14.95

1.84
2.29
7.58

0.33

2.25

Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning

B PROOF OF EXPRESSIVENESS BETWEEN

SYNTHESIS HIERARCHIES

Recall our deﬁnitions of synthesis hierarchies:
(a) System hierarchy ((cid:2)1 2 2 4(cid:3))
(b) Column-based parallelism factors ((cid:2)1 1 1 2 2 1 2 2(cid:3))
(c) Row-based parallelism factors ((cid:2)1 1 2 2 1 2 1 2(cid:3))
(d) Reduction axis parallelism factors ((cid:2)1 2 1 2(cid:3))

Note that while we can assume all system hierarchies (i.e.,
(a), and thus (b) and (c)) start with a level of 1, e.g., [(rack,
1), (server, 2), (CPU, 2), (GPU, 4)], it may not be the case
for (d). To make it a complete synthesis hierarchy, we will
append a (root, 1) as the root of (d).

Our goal is to prove

Theorem 3.2. (d) ≥ (c) ≥ (b) ≥ (a).

(1) (b) ≥ (a) (Ap-
We split our goal into three parts:
pendix B.1 Lemma B.1); (2) (c) ≥ (b) (Appendix B.2
Lemma B.7); (2) (d) ≥ (c) (Appendix B.3 Lemma B.8),
and prove them separately.

During the proof, we often use parallelism factors (x) and
their corresponding levels (e) interchangeably.

B.1 Part 1

Lemma B.1. (b) ≥ (a).

Proof. This lemma is intuitive, as (b) expands (a), so can
be used to express any communication patterns that can be
formed by (a).
Suppose (a) is (cid:2)x0 x1 · · · xn
and the parallelism axes has size 0..m,
then (b) is (cid:2)x00 · · · x0m x10 · · · x1m · · · xn0 · · · xnm

(cid:3),

(cid:3),

m
(cid:89)

xij = xi.

with

j=0

Now we show that every reduction instruction given by (a),
a reduction instruction can be formed by (b) expressing the
same reduction.

Case 1 : (a) uses (xi, InsideGroup, C).

Then (b) can use (xi0, InsideGroup, C) to express the
same reduction.
To see why this is true, note that xij, forall j, repre-
sents the same system hierarchy as xi. So xi0 and
InsideGroup form the same device groups as xi and
InsideGroup.

Case 2 : (a) uses (xi, Parallel(xj), C).

Then (b) can use (xi0, Parallel(xjm), C) to express the
same reduction.

Here, the use of xi0 ensures that we form the same
reduction groups, and then we use xjm (instead of xj0)
to get the ﬁrst (second/etc) device from each reduction
group that connects to the same xjm. As xjm is the
last parallelism factor for xj, this makes sure that we
only connect devices that connect to the same xj.

Case 3 : (a) uses (xi, Master(xj), C).

Then (b) can use (xi0, Master(xjm), C) to express the
same reduction. The case is similar as the above one.

B.2 Part 2

B.2.1

Semantically valid reduction

Lemma B.2 (Parallel reduction). Consider a synthesis hi-
(cid:3), and the device groups
erarchy (cid:2)x0 x1 · · · xj · · · xi · · · xn
formed by (xi, Parallel(xj), C), we have:

1. a device group to be reduced has size xj+1 × ... × xi,
and we have x0 × x1 × ... × xj × xi+1 × ... × xn
number of such groups.

2. a device group has been partitioned over xj+1, ..., xi.

Proof. We can derive the observation from the semantics
of Parallel. In particular, we ﬁrst form reduction groups for
devices connected to the same xi. So each reduction group
has size size1 = xi+1 × ... × xn.

Then, because of Parallel, we reduce all ﬁrst (second/etc)
devices in the reduction group if they are connected to the
same xj. Since each xj owns size2 = xj+1 × ... × xi
different reduction groups, the device group we form is
exactly of size2. And for each xj, we have size1 of such
device groups. Since we have size3 = x0 ×...×xj different
xj, we have in total size3 × size1 device groups.

The second observation is similar. In particular, since device
groups are formed by each xj, which owns size2 different
xis with different xj+1, ..., xi, the device groups we form
are exactly partitioned over xj+1, ..., xi.

Lemma B.3 (Partitioning over reduction axes). Given re-
duction axes, for a reduction instruction to be semantically
valid, all device groups to be reduced must only be parti-
tioned over the reduction axes.

Proof. Suppose we want to reduce over reduction axes A,
and we reduce between dj and dj that have been, possibly
among others, partitioned over B (different from A). Then,
after reduction both devices contain data that differs in B.

Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning

Now the desired ﬁnal state (where data should only be re-
duced if they have different A) becomes unreachable for
both devices, as we can never recover the state of the device
since according to the semantics, once a row has grown it
will never get reduced back.

Corollary B.4 (Semantically valid parallel reduction). Con-
sider a synthesis hierarchy (cid:2)x0 x1 · · · xj · · · xi · · · xn
(cid:3),
given some reduction axes, a reduction instruction
(xi, Parallel(xj), C) is only semantically valid if all of
xj+1, ..., xi are either 1, or on the reduction axes.

Proof. By Lemma B.2, we know each device group is parti-
tioned over xj+1, ..., xi. However, Lemma B.3 shows that
for each device group to be semantically valid, they can
only be partitioned over the reduction axes. Therefore, all
xs in xj+1, ..., xi. should either be 1, or on the reduction
axes.

Lemma B.5 (Semantically valid InsideGroup reduction).
Consider a synthesis hierarchy (cid:2)x0 x1 · · · xj · · · xi · · · xn
(cid:3),
given some reduction axes, a reduction instruction
(xi, InsideGroup, C) is only semantically valid if all of
xi+1, ..., xn are either 1, or on the reduction axes.

Proof. Similar as Lemma B.2, except in this case we know
that we are reducing devices over xi+1, ..., xn.

Lemma B.6 (Semantically valid master reduction). Con-
sider a synthesis hierarchy (cid:2)x0 x1 · · · xj · · · xi · · · xn
(cid:3),
given some reduction axes, a reduction instruction
(xi, Master(xj), C) is only semantically valid if all of
xj+1, ..., xn are either 1, or on the reduction axes.

Proof. Note that in this case we require all parallelism fac-
tors up until xn (as with InsideGroup, rather than xi as with
Parallel). The trickiness here is that the case of Master is
different from Parallel: while in Parallel we know that we
will reduce in parallel everything that is not in the range
of Parallel (Lemma B.2), with Master we reduce only the
ﬁrst reduction group. So it is important to guarantee that we
form exactly the same ﬁrst inner reduction groups.

We prove the result by contradiction. Suppose xj+1, ..., xn
contains a parallelism factor x(cid:48) that is not 1 nor is on the
reduction axes. Then there are two possibilities.

(1) x(cid:48) is part of xj+1, ..., xi, i.e., it is covered by the reduc-
tion. Then we will reduce devices of different x(cid:48). According
to lemma B.3, the reduction is invalid.

(2) x(cid:48) is part of xi+1, ..., xn, i.e., it is not covered by the
reduction, but it affects the reduction groups we form. Sup-
pose reduction axes are A, and x(cid:48) is on level B, with B (cid:54)= A.

Then since x(cid:48) is part of xi+1, ..., xn, we can ﬁnd within
xi+1, ..., xn a device d0B, that has only different B with the
very ﬁrst device d0.

Suppose d0 reduces with some device d1 in this master
reduction. Since the reduction is valid, d0 and d1 differ only
in A.

Then in the same way we ﬁnd ddB to d0, we can ﬁnd a
device d1B, that is only different with d1 in B.

However, note that this is a master reduction, and d0 and
d0B belong to the same xi+1, ..., xn group, so the master
reduction will only reduce d0 and d1, but only d0B and d1B.

Now we can show that the ﬁnal desired state becomes un-
reachable. In particular, note that d0B and d1B will never
get reduced: every reduction that reduces d0B and d1B will
reduce d0 and d1 as well (Figure 6 is useful here). But since
d0 and d1 has been reduced already, re-reducing the devices
is an invalid reduction step. So we will never be able to
reduce d0B and d1B again. And thus the master reduction
is invalid.

Lemma B.7. (c) ≥ (b).

Proof. Case 1 (b) has (e2, Parallel(e1), C).

Based on Corollary B.4, all non-reduction parallelism
factors column-wisely between e1 (exclusive) and e2
(inclusive) can only be 1.

Now we construct a reduction instruction for (c) that
expresses the same reduction. Suppose the reduction
step in (b) covers parallelism factors ei, ..., ej on the
reduction axis. Note that if the reduction step in (b)
does not cover any parallelism factors, that means it
forms groups of one device, and in that case this does
not form a reduction and won’t be generated by P 2.
Let e(cid:48)
1 be the level right before ei row-wisely in
the synthesis hierarchy (c), and let e(cid:48)
2 be ej. Then
(e(cid:48)
1), C) is a desired reduction instruction.
Indeed, we can derive from Lemma B.2 that this re-
duction instruction forms the same device groups as
(b).

2, Parallel(e(cid:48)

The example from the paper is repeated below, with
the reduction axis highlighted.

e1






1
1

x0,0
1 x0,3
1 x1,3
x1,0
x2,0 x2,1 x2,2 x2,3
1 x3,3

1

1





e2






1
1

x0,0
1
x0,3
1 x1,3
x1,0
x2,0 x2,1 x2,2 x2,3
x3,3
1

1

1






e(cid:48)
2

e(cid:48)
1

Case 2 (b) has (e, InsideGroup, C).

This case can be reasoned in a similar way as the pre-
vious case, by using Lemma B.5.

Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning

We construct a reduction instruction for (c) that ex-
presses the same reduction. Suppose the reduction
step in (b) covers parallelism factors ei, ..., ej on the
reduction axis. Let e(cid:48) be the level corresponding to
the parallelism factor right before ei row-wisely. Then
(e(cid:48), InsideGroup, C) is a desired reduction instruction.

Below we give an example, with the reduction axis
highlighted.

Case 1 (c) has (e, InsideGroup, C).

We construct a reduction instruction for (d) that ex-
presses the same reduction.

If the reduction covers the whole parallelism factor,
then (root, InsideGroup, C) is a desired reduction in-
struction.

e






1
1

x0,0
x1,0
x2,0 x2,1 x2,2 x2,3

1
1

1
1











1
1

x0,0
1
x1,0
1
x2,0 x2,1 x2,2 x2,3

1
1






e(cid:48)

e






x0,0 x0,1 x0,2 x0,3
x1,0 x1,1
x2,0 x2,1 x2,2 x2,3

1

1






1

1

1

1

1

1

1

1

1

1

1

1

On the other hand, we can also show a counterex-
ample of (b) ≥ (c), where in the following hierarchy,
(e(cid:48), InsideGroup, C) is a valid reduction in (c) but there
is no way in (b) that can simulate the same reduction.

If the reduction covers some (but not all) parallelism
factors on the reduction axis, then e itself is on the
reduction aixs.

Then (e, InsideGroup, C) is a desired reduction instruc-
tion.






x0,0 x0,1 x0,2 x0,3
x1,0 x1,1 x1,2 x1,3
x2,0 x2,1 x2,2 x2,3






e(cid:48)

1

1

1

1

e






x0,0 x0,1 x0,2 x0,3
x1,0 x1,1
1
x2,0 x2,1 x2,2 x2,3
1
1

1

1

1






Similar counterexamples can also be shown for other
cases.

Case 3 (b) has (e2, Master(e1), C).

This case can be reasoned in a similar way as the pre-
vious case, by using Lemma B.6.

We construct a reduction instruction for (c) that ex-
presses the same reduction. The choice of e(cid:48)
1 and e(cid:48)
2
is the same as Case 1. Then (e(cid:48)
2, Parallel(e(cid:48)
1), C) is a
desired reduction instruction.

Below we give an example, with the reduction axis
highlighted.

Again, if the reduction does not cover any parallelism
factors on the reduction axis, then it forms reduction
groups of size 1, and thus this does not form a reduction
and won’t be generated by P 2.

Below we also show a counterexample of (c) ≥ (d),
where in the following hierarachy, (e, InsideGroup, C)
is a valid reduction in (d) but there is no way in (c)
that can simulate the same reduction since x3 can be
arbitrary numbers.










e2

1
1

1
1

x0,0
1
x1,0
1
x2,0 x2,1 x2,2 x2,3
1
1

1

1






e(cid:48)
1
e(cid:48)
2

e






x0,0 x0,1 x0,2 x0,3
x1,0 x1,1
1
x2,0 x2,1 x2,2 x2,3
x3,0 x3,1 x3,2 x3,3

1






e1






1
1

x0,0
x1,0
x2,0 x2,1 x2,2 x2,3

1
1

1
1

1

1

1

1

B.3 Part3

Lemma B.8. (d) ≥ (c).

Proof. Most reasoning is the same as Lemma B.7. For
each case of a (c) reduction instruction, we show how we
construct the reduction instruction for (d) that expresses the
same reduction. Remember that we attach a (root, 1) to
synthesis hierarchy (d).

Case 2 (c) has (e2, Parallel(e1), C).

Similar as the previous case, we construct a reduction
instruction for (d) that expresses the same reduction.

Suppose the reduction step in (c) covers parallelism
factors ei, ..., ej on the reduction axis. Let e(cid:48)
1 be the
level in the synthesis hierarchy (d) right before ei row-
wisely. If the reduction covers the whole parallelism
factor, then e(cid:48) would be the root. Let e(cid:48)
2 be ej. Then
(e(cid:48)
1), C) is a desired reduction instruction.

2, Parallel(e(cid:48)

In the following example, e(cid:48)

1 = root.

Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning


e1



x0,0 x0, 1 x0,2 x0,3
x1,0 x1,1
x2,0 x2,1 x2,2 x2,3
1 x3,3

1

1

1

1





e2






1

1 x0,3
1

x0,0
x1,0 x1,1
1
x2,0 x2,1 x2,2 x2,3
1 x3,3
1

1






e(cid:48)
2

Case 3 (c) has (e2, Master(e1), C).

This case is exactly the same as the case for Parallel.

