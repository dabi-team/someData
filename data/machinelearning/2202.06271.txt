Model-based Testing of Scratch Programs

Katharina G¨otz
University of Passau
Passau, Germany

Patric Feldmeier
University of Passau
Passau, Germany

Gordon Fraser
University of Passau
Passau, Germany

2
2
0
2

b
e
F
3
1

]
E
S
.
s
c
[

1
v
1
7
2
6
0
.
2
0
2
2
:
v
i
X
r
a

Abstract—Learners are often introduced to programming via
dedicated languages such as SCRATCH, where block-based com-
mands are assembled visually in order to control the interactions
of graphical sprites. Automated testing of such programs is
an important prerequisite for supporting debugging, providing
hints, or assessing learning outcomes. However, writing tests for
SCRATCH programs can be challenging: The game-like and ran-
domised nature of typical SCRATCH programs makes it difﬁcult
to identify speciﬁc timed input sequences used to control the pro-
grams. Furthermore, precise test assertions to check the resulting
program states are incompatible with the fundamental principle
of creative freedom in programming in SCRATCH, where correct
program behaviour may be implemented with deviations in the
graphical appearance or timing of the program. The event-
driven and actor-oriented nature of SCRATCH programs, however,
makes them a natural ﬁt for describing program behaviour
using ﬁnite state machines. In this paper, we introduce a model-
based testing approach by extending WHISKER, an automated
testing framework for SCRATCH programs. The model-based
extension describes expected program behaviour in terms of
state machines, which makes it feasible to check the abstract
behaviour of a program independent of exact timing and pixel-
precise graphical details, and to automatically derive test inputs
testing even challenging programs. A video demonstrating model-
based testing with WHISKER is available at the following URL:

https://youtu.be/edgCNbGSGEY

Index Terms—Model-based Testing, Scratch, GUI Testing

I. INTRODUCTION

With more than 85 million programs created and published
by young learners1, SCRATCH [1] represents the most popular
block-based programming language. Languages like SCRATCH
support learners by allowing them to drag and drop pro-
gramming statements to visually arrange programs without
having to remember programming syntax. As a consequence,
all programs are immediately syntactically correct, but they
may still be semantically incorrect. This creates a need for
automated testing tools that help learners implement, debug,
and test their programs by providing valuable feedback.

Figure 1a shows an example SCRATCH program, in which a
player can move a bowl sprite horizontally using the left and
right arrow keys, with the goal of catching as many fruit sprites
falling from the top as possible. Fig. 1b shows a code excerpt
that handles part of the bowl’s behaviour by ﬁrst checking in
an inﬁnite loop whether the left key was pressed, and then
moving the bowl to the left if the check passes. This behaviour
can be examined using the automated WHISKER [2] test shown

1[October 2021] https://scratch.mit.edu/statistics/

(a) Fruit catching game.

(b) Part of the SCRATCH code
for bowl sprite.

const test = async function(t) {

let sprite = t.getSprite(’Bowl’);
let oldX = sprite.x;
t.inputImmediate({device: ’keyboard’,
key: ’Left’, isDown: true});

await t.runForTime(1000);
t.assert.ok(oldX >= sprite.x);
t.end();

}

(c) WHISKER test for bowl left movement.

(d) Model testing the left movement of the bowl.

Fig. 1: Fruit catching game in Scratch.

in Fig. 1c. The test retrieves the bowl sprite, stores the current
x-position, then presses the left cursor key for 1000ms, waits,
and ﬁnally checks whether the bowl is now to the left of where
it was previously. While this is an intuitive and easy test, there
are several challenges:

• Being able to write such a test requires knowledge
of JavaScript, which may be problematic if the user
writing tests is a school teacher without thorough software
engineering training.

• The bowl may make arbitrary wrong movements that will
not be detected as long as the ﬁnal position after 1000ms
is somewhere to the left.

• It is challenging to estimate a precise waiting duration
before checking the resulting behaviour, as differences in
the computer as well as implementation choices may lead
to different execution times. For example, the duration of
an animation will depend on the current sprite position
as well as its size and shape.

• A further problem not shown by this example is that
test assertions on absolute positions are difﬁcult, as it is

Leftarrow|Bowl.xnew<=Bowl.xold1 
 
 
 
 
 
common and accepted behaviour in SCRATCH for young
learners to use arbitrary sprites, shapes, and sizes.

To address these issues, in this paper we introduce the
notion of model-based testing for SCRATCH. The behaviour
of the bowl sprite in Fig. 1b can easily be described using
the extended ﬁnite state machine shown in Fig. 1d: There is
a single state which is entered when the program is started,
and then whenever the left cursor key is pressed a transition is
triggered in which the expected effect is that the x-position of
the bowl has changed to the left. This model can immediately
serve to check the behaviour of the program during execution
in conjunction with any automated tests, or even automated
test generation [3]. While it is unlikely that an automated test
generator will produce sequences of events that will properly
play the fruit catching game, it is possible to also specify the
input behaviour using ﬁnite state machines.

In detail, the contributions of this paper are as follows:
• We introduce the notion of model-based testing for

SCRATCH.

• We extend the WHISKER testing framework for SCRATCH
with an editor to create models for testing SCRATCH
programs.

• We provide a fully automated testing solution for
SCRATCH programs by allowing users not only to model
the desired program behaviour, but to also derive test
inputs from user models.

• We demonstrate the feasibility of the approach using the
fruit catching game as a case study and apply model-based
testing to 38 students’ implementations of the game.

II. BACKGROUND

SCRATCH [1] is a popular block-based programming lan-
guage intended to support
learners new to the world of
programming. Programs are created by dragging and dropping
blocks from drawers containing all available blocks, such that
learners do not have to memorise all available commands
initially. Furthermore, since blocks have different shapes, only
syntactically valid combinations are possible, thus avoiding
syntax errors. Programs created this way consist of multiple
scripts that deﬁne the behaviour of sprites interacting on top
of a graphical stage. The resulting programs can usually be
controlled by user inputs, and high-level program statements
make it possible to quickly and easily create fun and engaging
programs and games.

While syntax errors are prevented by SCRATCH, learners
may still struggle to produce functionally correct programs, and
teachers may need help in supporting their students. Therefore,
automated tools are an important means for supporting both
of these groups. For example, there are tools that report code
smells [4]–[8] or bug patterns [9], but such tools are limited
to generic feedback independent of a task at hand. In order to
produce feedback on whether a speciﬁc target functionality is
satisﬁed, or to generate hints on how to proceed to get there,
automated tests are an important prerequisite [10].

Testing interactive, graphical programs like those typically
created with SCRATCH comes with multiple challenges, such

as the heavily randomised nature of game-like programs, long
execution times caused by animations and story-like segments
of programs, or their event-driven and graphical nature. The
ITCH [11] tool attempts to provide testing functionality by
translating a subset of the language features to Python, but is
restricted to checking textual dialogue behaviour. We have
therefore introduced the WHISKER testing framework [2]
which makes it possible to write automated tests for SCRATCH
programs using a JavaScript API as shown in Fig. 1c. To support
users, WHISKER can also automatically generate test inputs
(e.g. randomly or using search [3]), but in order to detect
faults there nevertheless has to be a speciﬁcation to check
against, which so far had to be written in JavaScript. Both,
writing tests as well as the speciﬁcations, are non-trivial tasks,
in particular considering that an important target audience are
users who may only have limited software engineering training
(e.g. teachers or creators of SCRATCH tutorials).

In many software engineering domains, model-based ap-
proaches have been successfully applied to improve testing [12].
Among the many notations available to specify system be-
haviour [13], variants of ﬁnite-state machine notations are
among the most common and longest established ones [14].
Finite-state models have also been a frequent subject of research
for automatically deriving tests [15]. However, to the best of our
knowledge, neither the use of model-based testing in general,
nor the use of ﬁnite-state machine notation speciﬁcally, have
been explored in the context of block-based programming
languages like SCRATCH yet.

III. MODEL-BASED TESTING FOR SCRATCH PROGRAMS

A. Modelling Scratch Programs

We aim to model the state-based behaviour of SCRATCH pro-
grams as ﬁnite state machines. The concrete state of a SCRATCH
program is deﬁned by the values assigned to variables and
attributes of sprites contained within a program (e.g. position,
rotation, size, visibility, graphics effect, costume). To keep the
size of models small, we use abstract states, where an abstract
state (1) refers to a subset of the variables and attributes of a
SCRATCH program, and (2) can represent multiple concrete
states of the remaining variables and attributes. This means that
there can be multiple models for a single SCRATCH program,
each representing different aspects of behaviour.

We deﬁne a model for a SCRATCH program as an extended

ﬁnite state machine (S, q0, Q0, Q1, Σ, E), where:
• S denotes a ﬁnite set of abstract states.
• q0 ∈ S is the initial state of the machine.
• Q0 ⊂ S speciﬁes a set of stop states that halt the execution

of the corresponding model.

• Q1 ⊂ Q0 deﬁnes an additional set of stop states, where
each node stops the execution of all models for the given
SCRATCH program.

• Σ : P → {0, 1} deﬁnes a set of predicates that may hold
for a given concrete program state P . As the SCRATCH
language is event-driven and action-oriented, this set
consists of predicates describing the presence or absence

Fig. 2: Notation for state machines.

of user inputs like key presses and mouse clicks, and of
graphical program events such as two sprites touching
each other. Additionally, we include time events such as
‘two seconds elapsed’.

• E : S × 2Σ → S × 2Σ is a ﬁnite set of state transitions,
where t(s, C) : (s(cid:48), C (cid:48)) denotes that from state s ∈ S
given all of c ∈ C ⊂ Σ evaluate to true, we move in a
step to s(cid:48) ∈ S with all c(cid:48) ∈ C (cid:48) ⊂ Σ evaluating to true. The
state transitions E enforce chronological dependencies
between all conditions c ∈ C and between all effects
c(cid:48) ∈ C (cid:48).

We represent models using the notation shown in
Fig. 2: States are represented as circles, and a transition
t(1, Conditions) : (2, Effects) is represented as a directed edge
from state 1 to state 2, where we separate conditions and effects
using the | symbol. For example, the model corresponding to
the bowl sprite shown in Fig. 1d has one state with a single
self transition bound to the condition ‘left arrow was pressed’.
If the condition evaluates to true, the program is tested against
the effect of the bowl moving to the left.

A program model is a model of a SCRATCH program
where the pseudo-transition to the initial state is activated
by the greenﬂag-event, which represents the user initiating
the program execution by pressing the greenﬂag button in
SCRATCH. Program models describe the behaviour until the
program execution is stopped, that is when the speciﬁcation
deﬁnes an abort or a valid end, represented by the stop states.
In order to simplify the description of various program
behaviours, we further introduce an alternative to program
models called end models, in which the pseudo-transition to
the initial state is activated as soon as all program models have
halted. For the testing process, end models are optional, but
they are useful for testing conditions that should hold after any
stopping condition in the speciﬁcation is reached, such as not
changing the player’s points anymore. For program and end
models, we restrict the global stop states Q1 to only halt state
machines of the same type.

SCRATCH models can be executed in parallel

to both,
manually written WHISKER tests as well as tests automatically
generated by one of WHISKER’s test-generation algorithms.
Additionally, we introduce user models that are capable of
specifying possible user inputs as ﬁnite state machines. More
speciﬁcally, a user model U = (S, q0, Q0, Q1, Σ, ΣI , E) is an
extended ﬁnite state machine which differs from a program
model in two ways:

• ΣI is the set of possible user inputs, such as key presses,

mouse movement, mouse clicks.

• E : S × 2Σ → S × 2ΣI deﬁnes the transition relation;
unlike program models the effects of an edge represent
sets of user events from ΣI rather than predicates.

Fig. 3: Model-based SCRATCH testing embedded within the
WHISKER step function.

When executing a user model U together with a SCRATCH
program under test, whenever an edge of U is traversed, the
set of user inputs that describe the effects of that transition is
applied to the program under test by WHISKER. Consequently,
by applying a user model and a set of program models the
process of testing SCRATCH programs can be automated. By
providing conditions that are taken with a deﬁned probability
also non-deterministic user models can be built.

B. Testing Scratch Programs with Models

In order to use models for testing SCRATCH programs we
execute the program under test and the models in parallel.
WHISKER starts the model test by simultaneously sending a
greenﬂag event to the project under test and setting all program
models to their initial states. The parallel execution of the
models and the program is achieved by interleaving program
execution steps with model updates.

The execution of SCRATCH programs is based on a step
function that is invoked at a regular interval, executing all active
scripts (each active script represents one execution thread). The
WHISKER testing framework wraps this step function in order
to apply test inputs and check properties [2]. Fig. 3 illustrates
the abstract procedure of a WHISKER step.

Model-based testing extends the workﬂow by inserting a
model input phase before and a model step after the SCRATCH
step. User models generating inputs are triggered in the model
input phase to provide the input for the model steps as well
as for the SCRATCH Virtual machine (VM). In the model step
after a SCRATCH step we update the states of all active models
using the current program state and elapsed time.

In addition, we instrumented the SCRATCH VM such that
each time a predicate p ∈ Σ changes its value WHISKER is
informed, and all active model states are updated based on their
transition relation E. Note that every unique state transition

21Conditions|EffectsScratchWhiskerStepperFetch InputWhiskerModelScratch StepUpdate Based onPredicate Change 𝑝Model StepFetch InputScratch StepModel StepStep xStep x+1Update Based onPredicate Change 𝑝 Scratch CheckModel StepCheck Scratch CheckModel StepChecke ∈ E can only be triggered once per WHISKER step in order
to traverse loops in the model only a single time.

A SCRATCH model may have multiple transitions enabled in
the same state. To avoid non-determinism, the transitions are
implemented as an ordered list in WHISKER, implying a priority
on the state transitions. The user can specify the order for each
state when deﬁning the models. Additionally, to reduce errors
in the modelling process WHISKER tests all simultaneously
tested effects of every active model for contradicting logic and
removes them from the testing set. Contradicting effects can
be literally opposites, i.e., one excludes the other and vice
versa like testing a boolean for true and false at the same time.
WHISKER also checks for contradicting ranges of results, e.g.
sprite.x > 0 and sprite.x < −1.

Whenever a model executes a transition t(s, C) : (s(cid:48), C (cid:48)),
we check whether all predicates c(cid:48) ∈ C (cid:48) are satisﬁed by the
resulting program state. The SCRATCH program’s behaviour is
heavily dependent on time, and the exact moment at which an
expected effect manifests in the program state will depend on
the computations performed as well as the rendering process of
SCRATCH. Consequently, this raises the question of when and
how long to apply the check to the resulting program state. We
therefore check the effect c(cid:48) for an interval of time depending
on when the transition was triggered: When a predicate change
originates from a transition within the SCRATCH step, then the
effect c(cid:48) is checked for the remainder of the current SCRATCH
execution step. Fig. 3 displays this as the Scratch Check. If a
state transition is executed during the model step, the effect is
checked in the next SCRATCH step until the following model
step, shown in Fig. 3 as the Model Step Check. In general,
the time frame for evaluating the effect of a SCRATCH step
covers at most one SCRATCH step. The only exception are
effects depending on speech bubbles which are checked for
two SCRATCH steps, as we have observed that the SCRATCH
GUI suffers from delays when rendering these.

Overall, this allows accurate testing of dependencies between
conditions and effects. If any of the predicates are not satisﬁed
after their time frame, then the model has found a failure in
the implementation under test. Once all models have reached
a stop state, all available end models are started. The test ends
when all program and end models have stopped.

IV. THE WHISKER TESTING TOOL

WHISKER provides a web interface where the user can load
and execute a project including corresponding tests (Fig. 4). We
extended WHISKER with an intuitive and straightforward editor
embedded directly in the web interface, as shown in Fig. 5. The
tabs at the top can be used for switching between models and
adding new ones. The left panel shows an interactive graph,
allowing users to modify the position of nodes and change the
focus of the view by zooming in and out. Pressing the Fit zoom
button at the bottom resets the zoom and node positions to show
the complete graph within the panel. The editor also supports
reordering models in a tree layout, facilitating the editing of
smaller as well as bigger models by sorting the graph left-
to-right or top-to-bottom beginning at the start node. Nodes

Fig. 4: Main WHISKER GUI.

Fig. 5: GUI for model-based testing in WHISKER.

are labelled and coloured depending on their purpose (e.g.
start, stop or stop all). Edge labels, on the other hand, consist
of the order in which the edge transitions should be tested,
assigned names, the number of conditions and the number of
effects. Nodes or edges may be added to the graph by using the
New node or respectively New edge button on the bottom left
of the user interface. The content of the right panel depends
on the user’s current selection:

• In case nothing is selected, the interface shows the model
settings, which allow deﬁning the model name and type.
• Clicking on a node highlights the node including its
adjacent edges on the graph and shows the node option
panel instead of the model settings on the right. The node
options, shown in Fig. 6a, allow changing a node’s name,
its functionality and the testing order of the outgoing
adjacent edges. Furthermore, to enforce the presence of
exactly one start node in each model, setting a node type

(a) Failures and summary of an executed testing run.

(a) Option
panel
shown when a node
was clicked.

(b) Option
panel
shown when an edge
was clicked.

(c) Adding/Editing
checks for an edge.

Fig. 6: Based on the user selection, the right panel of the GUI
changes to allow conﬁguring the selected part of a model.

to a start node is not allowed since start nodes are added
automatically during the creation of new models.

• As depicted in Fig. 6b, selecting an edge offers options
for customising the attributes of an edge. The upper half
of the panel allows changing the name and setting a
time constraint on the edge. The time constraint can be
deﬁned to either test an edge after a ﬁxed time passed
since the start of the testing process or to be based on the
time passed since the last edge transition in the respective
model. The lower half shows an overview of the conditions
and effects of the currently chosen edge.

• When adding a new check or selecting an existing check,
the panel switches to the check options shown in Fig. 6c
with the input ﬁelds for the arguments shown depending on
the chosen check type. While input ﬁelds for checks with
ﬁxed values such as keypresses offer predeﬁned values
to the user, ﬁelds with numeric arguments, such as RGB
values, support users with range checks and hints.

Using the WHISKER interface, users can download created
models from the editor as a JSON ﬁle or load existing ones into
the testing framework to start the testing process immediately.
These functions offer users the comfortable workﬂow of loading
existing models, modifying them to match their desired program
behaviour, and ﬁnally saving them again on their system. When
clicking the play button on the left in Fig. 4, the test framework
starts testing the currently loaded project ﬁle. Depending on the
uploaded ﬁles, the program under test can either be tested using
a test suite written in JavaScript, models or both. However,
if the user decides to test a given implementation using both
testing approaches, he/she has to ensure that only one of these
approaches provides user inputs. Moreover, the interface allows
users to observe the testing process since the game canvas
is constantly updated according to the sent user inputs. The
combined results of the JavaScript tests and models are shown

(b) Model details corresponding to an executed testing run.

Fig. 7: The test results including a summary shown by
WHISKER at the end of a test.

with a different detail level in two textual representations. Short
excerpts are shown in Fig. 7a and Fig. 7b. WHISKER, including
the new model-based testing feature, can also be executed on
the command-line, where it executes regular and/or model-
based tests in a headless browser.

V. CASE STUDY

We conducted a case study based on the fruit catching game
shown in Fig. 1a, which originates from SCRATCH lessons
for school children. The fruit catching game consists of an
apple and a banana sprite which drop down from the top of
the screen, and the aim of the game is to prevent falling fruit
from touching a red bar located at the bottom of the screen
by catching the falling fruit with a bowl that can be moved
horizontally using the cursor keys. The game comes with a
detailed textual speciﬁcation of the desired behaviour, as well
as a very detailed test suite consisting of 27 WHISKER tests [2],
which makes it well suited for formalisation with models. In
total, we used 19 program models, one end model and one user
model with input generation based on randomness to capture
all possible aspects of the program.

Names of sprites or variables can be described in our tool
by regular expressions. All references of apple or bananas in
the depicted models are simpliﬁed for clarity but actually are
given by ‘/(Apple|Apfel)/’ and ‘/Banan/’ in the implemented
models. Using regular expressions for names and also for the
content of speech bubbles allows for a choosable degree of
accuracy when testing with a wider range of implementations.
Our tool also offers testing strings with case sensitivity, which
is deactivated by default.

Fig. 10: Model deﬁning the apple’s behaviour.

the beginning of an edge. For example, when a speech bubble
appears and the left key is pressed at the same time, the
transition to the ‘text’ state will have a higher priority. Finally,
the edge (text, end) tests the removal of the speech bubble
after 1 second. The state ‘end’ is a stop state for all program
models as this is the game end.

is not

The edge (start, text) is annotated with a timed condition
‘force at 31s’ in brackets. This is an abbreviated form to
denote that the edge has to be taken within 31 seconds
after the program has started, and if it
taken by
the program under test,
then an error is reported. More
generally, each edge in our implementation can have a timed
condition from game start denoted with ‘force at’ and a
timed condition, described by ‘force after’, indicating the
maximal time difference between edge transitions. Note that
the WHISKER testing framework allows users to accelerate
program executions using a customisable acceleration factor. In
such cases, timed conditions are automatically modiﬁed with
respect to the chosen acceleration factor.

Figure 8b shows an erroneous implementation produced by
a student, in which the bowl is missing an action on right key
presses, and the speech bubble at the end is shown for two
seconds instead of one. The following failures are reported by
the bowl model in Fig. 9:

• Bowl.x+ missed: When the model is given a right key as
input, it takes the transition with the ‘right key’ condition.
This expects an increase of Bowl.x as effect, which is not
reported by the SCRATCH VM. The model then reports
this as a missing functionality of the program under test.
• No output of Bowl (End) after 1s: The edge (text, end) is
triggered after 1s and checks whether the bowl still has a
speech bubble containing ‘/End/’. The model reports the
output is not removed after 1s.

B. Apple Model

The behaviour of the apple sprite is represented by Fig. 10.
Initially, the model expects the apple to be moved to the top
of the screen and to start falling down. The model then waits
for the apple to touch either the bowl, or the red bar at the
bottom of the screen. If the bowl is touched, the model moves
to the state ‘bowl’, which expects the points to increase by ﬁve
within 1.5 seconds. Hereafter, edge (points, start) represents
the expected placement of the apple to the top of the canvas by
testing y > 100, moving the apple up again for another round of
the game as the speciﬁcation states that the sprite should be set

(a) Sample solution for bowl be-
haviour.

(b) Code example with mistakes
for bowl sprite.

Fig. 8: Code examples for the bowl sprite.

Fig. 9: Program model deﬁning the behaviour of the bowl sprite.

A. Bowl Model

The behaviour of the bowl sprite consists of two aspects:
1) The user can control

the bowl sprite by moving it

horizontally with the left and right cursor keys.

2) The bowl should display a speech bubble when a 30

seconds timer has elapsed.

The expected implementation of the bowl is shown in Fig. 8a,
and Fig. 9 shows the corresponding program model. The ﬁrst
edge (init, start) is always taken because of its ‘true’ condition
and no effects are checked. We use this initial transition to give
the program time to initialise variables and sprite positions
before applying any checks.

In the ‘start’ state there are four outgoing transitions: Two
of them are self-loops that test if the x-position of the bowl
increases/decreases in case the cursor keys are pressed and
the bowl is not touching a limiting vertical edge of the
screen. Another self-loop checks whether the x-position remains
unchanged if the right/left cursor keys are not pressed. The
edge (start, text) is triggered as soon as the bowl displays a
speech bubble which text conforms to the regular expression
‘/End/’ testing that 30 seconds have elapsed. The four outgoing
edges are ordered by priority, indicated by a grey number at

starttextendrightkey&!leftkey&!Bowlatrightverticaledge|Bowl.x+!rightkey&!leftkey|Bowl.xnochangeleftkey&!rightkey&!Bowlatverticalleftedge|Bowl.x−Bowlsays’/End/’|(forceat31s)!Bowlsays’/End/’initafter1s|1234true|-30selapsedstartbowlpointsAppletouchesBowl|-Points+5|-(forceafter1.5s)Apple.y>100|-12endAppleonred|Applesays’/over/’after1s|!Applesays”/over/”redinitApple.y-|-(a) Sample solution for apple be-
haviour.

(b) Code example with mistakes
for apple sprite.

Fig. 11: Code examples for the apple sprite.

to y = 170 if dropping into the bowl. Testing for the absolute
value of y == 170 is disadvantageous on edge (points, start)
since a faulty implementation that sets the apple to value
y = 150 when touching the bowl, would lead to traversing
both edges (start, bowl) and (bowl, points) and waiting for the
condition of y == 170 to be fulﬁlled, effectively halting the
model in state ‘points’. Thus, to make the depicted model more
robust against faulty implementations, we test against a value
greater than 100 and move the check for the correct value of
y == 170 to another model. When the apple hits the red line
at the bottom of the canvas a ‘Game over’ message should be
displayed, represented by the regular expression ‘/over/’. The
model further expects this speech bubble to disappear again
after a second for a clean state at the end of the game.

Given a correct implementation of the apple’s speciﬁcations
in Fig. 11a and a wrong implementation in Fig. 11b, where the
increase in points is missing and the speech bubble is shown
too long, the apple model reports the following problems:

• Points+5 missed: When the apple touches the bowl the
model changes its state to ‘bowl’. After 1.5s the model
reports that the transition to ‘points’ has not been triggered
and reports the condition as a failure.

apple

• No output of

after 1000ms: The
(over)
edge (red, end) is triggered after 1s and checks whether
the apple still has a speech bubble containing ‘/over/’.
The model reports the output is not removed after 1s.

C. Bananas Model

For the bananas sprite the speciﬁcation states that the player’s
points should be reduced by 8 when the bananas touch the red
bar at the bottom and the game should resume by spawning the
bananas again at the top of the canvas. When the bananas are
caught by the bowl sprite the player’s points are increased by 8.
The model is shown in Fig. 12. Again testing for a y value
greater than 100 allows the continuous test of these models
even if the fruit is not spawned to y == 170 as speciﬁed.

Fig. 12: Model deﬁning the bananas’ behaviour.

Fig. 13: Model started after game end. Checks that no sprites
are spawned or variables change.

D. End Model

End models make it possible to describe the behaviour at the
end of program execution in a single model, rather than having
to distribute it across multiple models. For testing constraints
after the fruit catching game has ended we implemented the
end model shown in Fig. 13. This model tests for two seconds
that the sprite positions and the points and timer variables do
not change, before ending the model test with the ‘stop’ state.
While the speciﬁcation does not include any constraints for
what should happen after a game ends, we found that this end
model provides useful feedback for programs that do not halt.

E. User model

In order to fully automate the testing process, we also deﬁned
the user model shown in Fig. 14, which generates inputs for
the game based on randomness. For this purpose, we generate a
random double in the range of [0, 1] and select the transition to
take, including the corresponding input based on the randomly
generated value. From the ‘start’ state, the model offers four
types of player behaviours: The model can choose to not move
the bowl in state ‘dontMove’ at all, dodge the apple in state
‘dodge’ by moving away from the apple to intentionally loose,
catch the apple continuously in state ‘onlyApple’ to win or
decide randomly between the apple and bananas sprite in state
‘fruits’. When deciding to catch a fruit the position of the bowl
and the respective fruit are compared and a self-looping edge
inputs the correct key to move to the fruit. When the difference
in positions is smaller than ten it stops as the bowl should
move in steps of ten. Only when the fruit is caught and it
respawns at the top, the next input is computed. We assume
here that a respawn of a fruit sets the fruit to a new random
x-position and therefore, react to it. The conditions y == 170
and y > 100 may either not be triggered or stop the movement
towards the sprite too early.

startbowlBananastouchBowl|-Bananas.y>100|-redpointsBananasonred|-Points+8|-(forceafter1.5s)21inittrue|-pointsPoints−8|-(forceafter1.5s)Bananas.y>100|-stopwaittrue|Pointsnochange&Timernochange&Bananas.xnochange&Apple.xnochange2saftergameend|-21contains poor student submissions with only 2 points but also
perfectly working submissions with maximal points. All in all,
the dataset consists of 39 fruit catching programs sampled from
a real-world application scenario, which reﬂects our targeted
application scenario of helping a teacher in assessing many
student solutions simultaneously.

B. Methodology

Model-based SCRATCH testing aims to simplify the process
of evaluating countless student submissions by offering a
convenient alternative to writing error-prone JavaScript tests.
Teachers can build visual models of their desired program
behaviour, which are less susceptible to implementation errors
and do not require users to be familiar with the JavaScript
language. When providing additional user models the testing
process can be made independent of JavaScript tests.

For our research questions we compare the manually written
JavaScript test suite by Stahlbauer et al. [2], which contains
ﬁxed user inputs as well as intricate JavaScript code to react to
various game situations, with the models described in Section V.
Two experiment settings are needed for this evaluation: The
ﬁrst one excludes our user model and only uses the 19 program
models and one end model for the test runs while obtaining the
user inputs from the JavaScript test suite. The other experiment
is independent of the test suite and provides inputs based on
our user model in Fig. 14. Both test suite and models are based
on the same textual speciﬁcation and sample solution.

To minimize the inﬂuence of randomised behaviour in the
fruit catching game or in the inputs, we repeat each experiment
30 times, where each run seeds the random number generator
differently by WHISKER. When executing one run on a student
implementation with model-based inputs we repeat the input
generation 20 times to get different input sequences. The
same seeds are used for the different experiment settings. The
WHISKER testing framework offers the option to accelerate
test execution by a user-deﬁned acceleration factor. With the
aim of decreasing the time required for evaluating all projects
of the dataset, we speed up test execution by a factor of 10.

RQ1: To evaluate how well model-based input generation
can test a SCRATCH program we compare the achieved
block coverages of model-generated inputs against the inputs
originating from the manually written test suite. We report the
achieved mean coverage values of both approaches together
with the Vargha and Delaney (A12) effect size for every project
of the dataset. We consider a result to be statistically signiﬁcant
if a Mann-Whitney-U-test reports a p-value smaller than 0.05.

RQ2: For model-based testing in SCRATCH to be a valid
alternative approach its test runs have to be able to detect
faulty program state transitions as reliable as the manually
written JavaScript test suite. For this, we compare the number
of erroneous program state transitions found by our deﬁned
models against the test suite. To prevent the used method of
input generation to inﬂuence the results, we performed the
experiment once with model input generation and once with
inputs from the test suite.

Fig. 14: User model for the fruit catching game.

VI. EVALUATION

To evaluate the effectiveness of model-based SCRATCH

testing we aim to answer the following research questions:

• RQ1: Are user models capable of exercising SCRATCH

programs thoroughly?

• RQ2: Is model-based SCRATCH testing capable of detect-

ing faulty program states?

The proposed tool for testing SCRATCH programs in a model-
based manner is fully integrated into the WHISKER testing
framework and available on GitHub2.

A. Dataset

The dataset used for evaluating the model-based SCRATCH
testing approach originates from a prior study [2], in which
pupils visiting the sixth and seventh grades were taught
programming in SCRATCH. As a ﬁnal assignment at the end of
the workshop, each student had to implement the fruit catching
program on his/her own given a textual speciﬁcation of the
game. To form a dataset, we ﬁrst gathered every submitted
student solution of the workshop which resulted in 41 fruit
catching programs. Then, similar to Stahlbauer et al. [2] we
removed three projects starting with a key press such as space
instead of the greenﬂag event, because WHISKER assumes the
projects to start on greenﬂag and the three projects thus would
not react to any inputs. In order to evaluate if the models meet
the requirements deﬁned within the textual speciﬁcation, we
also added the sample solution to the dataset. Thus, altogether
the dataset consists of 39 fruit catching versions, covering
many different implementation approaches as the number of
used SCRATCH blocks ranges from 3 to 81. Furthermore,
in a manual grading process [16], a teacher assigned each
student submission points in an interval of 0 to 30. The dataset

2[October 2021] https://github.com/se2p/whisker

startdontMoveRandom.double≤0.05|-dodgeRandom.double≤0.1|-Appleonred&after1s|-Apple.x≥0|Apple.x<0|12onlyAppleRandom.double≤0.2|-3ALARfruitstrue|-4ALARbananasBLBRappleRandom.double≤0.5|-12Apple.xchange|-true|-Bananas.xchange|-AL:=Bowl.x>Apple.x&|Bowl.x−Apple.x|>10|InputleftkeyAR:=Bowl.x<Apple.x&|Bowl.x−Apple.x|>10|InputrightkeyBL:=Bowl.x>Bananas.x&|Bowl.x−Bananas.x|>10|InputleftkeyBR:=Bowl.x<Bananas.x&|Bowl.x−Bananas.x|>10|InputrightkeyInputrightkeyInputleftkeyC. Threats to Validity

External Validity: Our dataset, composed of 39 fruit catching
programs, contains implementations that are very similar to the
sample solution as well as fruit catching versions that deviate
signiﬁcantly from the desired program behaviour. Additionally,
the grades manually awarded by a teacher, ranging from 2 to 30
points, indicate that the dataset contains many different program
behaviours, of which some are entirely correct and others
mostly faulty. However, due to the lack of data originating
from similar application scenarios, the experiments were only
conducted on various fruit catching versions and thus might
not generalise well to other programs.

Internal Validity: Because the fruit catching game as well as
the inputs derived from the user model include randomised
behaviour, we repeat each experiment 30 times and report the
achieved averages and A12 effect sizes across all repetitions.
Another threat to internal validity is the use of manually
developed models and test cases, as already minor changes in
both may already lead to signiﬁcant variations in the experiment
results. The results of experiments with model-based inputs
also depends on the quality of the user model. Because our
user model is based on randomness, we chose 20 repetitions
to create different input sequences, which could vary with
different probabilities on the transitions and less repetitions.

Construct Validity: The achieved block coverage, a measure
similar to statement coverage in mature programming lan-
guages, is used to determine how well a given approach can
exercise SCRATCH programs. However, because most program
states are often already covered by simply starting a given
SCRATCH program, block coverage may not always be a good
indicator of how thoroughly a given approach explores a student
submission. Furthermore, we compare the effectiveness of the
model-based approach in ﬁnding faulty program behaviours
against a manually written test suite by counting the number
of reported erroneous program states. Even though the models
and the manually written test suite were developed with the
intention to reﬂect the textual speciﬁcation of the fruit catching
game, a human evaluator may not always agree that reported
program states are indeed faulty.

D. RQ1: Are User Models Capable of Exercising SCRATCH
Programs Thoroughly?

User models offer a less error-prone alternative to explicitly
specifying SCRATCH inputs within a test suite written in
JavaScript. However, those user models are only of use if
they are able to exercise SCRATCH programs as thoroughly as
manually written tests.

For each project of the dataset and across all 30 experiment
repetitions, Table I compares the achieved block coverages of
inputs derived from a user model against inputs obtained by
the test suite. Furthermore, we report the A12 effect size for
each project individually and highlight statistically signiﬁcant
results with a p-value < 0.05 in boldface. Every project of
the dataset, except the sample solution, starts with the letter C

TABLE I: Mean coverage and A12 effect size: UserModel-
Inputs vs. TestSuite-Inputs. Values in boldface indicate strong
statistical signiﬁcance with p < 0.05.

l
e
d
o
M

r
e
s
U

70
73
94
97
87
32
94
80
100
33
98
97
69
80
85
100
100
94
74
92

e
t
i
u
S
t
s
e
T

68
73
94
94
88
35
98
80
100
33
97
97
69
80
89
98
100
91
65
91

Project

C6 01
C6 02
C6 03
C6 04
C6 05
C6 06
C6 07
C6 08
C6 09
C6 10
C6 11
C6 12
C6 13
C6 14
C6 15
C6 16
C6 17
C6 18
C7 01
C7 02

2
1
A

0.65
0.5
0.5
0.88
0.47
0.42
0.0
0.5
0.5
0.5
0.74
0.5
0.5
0.5
0.0
1.0
0.5
1.0
1.0
0.58

Project

C7 03
C7 04
C7 05
C7 06
C7 07
C7 08
C7 09
C7 10
C7 11
C7 12
C7 13
C7 14
C7 15
C7 16
C7 17
C7 18
C7 19
C7 20
Sample
Summary

l
e
d
o
M

r
e
s
U

57
74
88
77
87
95
71
99
99
100
88
99
95
91
93
52
97
67
100
84.27

e
t
i
u
S
t
s
e
T

57
73
82
75
87
89
71
98
100
100
84
99
90
90
94
52
93
50
100
82.87

2
1
A

0.5
0.53
1.0
0.75
0.5
0.98
0.5
0.97
0.48
0.5
1.0
0.5
1.0
0.55
0.45
0.42
0.97
1.0
0.5
17/15/39

followed by a number, which encodes the class of the student
who created the given project.

Because the test suite and the user model were implemented
to reﬂect the desired program behaviour deﬁned within the
textual speciﬁcation, both approaches reach full project cov-
erage on the provided sample solution. With a mean block
coverage of 84%, model-generated inputs covered slightly more
blocks than the manually written test suite. Regarding the A12
effect size, in 17/38 student submissions, with 15 of them
being statistically signiﬁcant results, the user model beneﬁts
from its ﬂexible input generation approach and reaches higher
coverage values than the static test suite, which is speciﬁcally
tailored to the control ﬂow of the sample solution. Furthermore,
in 14/38 student projects, the user model and the test suite
achieve the exact same coverage. This may indicate that the
program coverage may not be improvable when changing input
generation techniques. The fact that the program coverage
is also inﬂuenced by dead code blocks, unreachable code
and wrong thread starting hats supports this. For example, in
C6 S08 we found dead code blocks without connected thread-
starting hats that will never be executed independent from
the input. Other implementations had unreachable code based
on semantics e.g. putting the apple on red in the initial step,
stopping with the testing on red in the second step and not
being able to go into other code. Without such problematic
scenarios the program coverage would be higher.

Some projects have smaller project coverage when using
model-based inputs. Projects C6 06 and C6 15 are setting
their fruit sprites at ﬁxed positions with at least one in the
middle within range of the bowl without movement. As the
user model is ﬂexible and tests the distance of bowl to fruit
before providing any input, the model does only generate either
left or right key and misses the other one. The code to the

Fig. 15 shows the number of detected program states that
deviate from the textual speciﬁcation across all projects and
experiment repetitions. Note that every approach contains at
least one project in which not a single faulty program state
was detected. This project corresponds to the sample solution
and indicates that neither the test suite nor the models tend to
generate false positives. Overall, the combination of user model
input generation and model-based testing (UI-MF) detects the
most failures with a median of 11.9 reported program errors.
Furthermore, since both model-based approaches (UI-MF &
TI-MF) signiﬁcantly outperform the manually written test suite
(TI-TF), the results show strong evidence that model-based
SCRATCH testing is indeed capable of ﬁnding far more faulty
program behaviours than manually written tests, regardless of
how test inputs were generated.

Fig. 15: Number of detected erroneous program states:
UserModelInput-ModelFailures (UI-MF) vs. TestSuiteInput-
TestFailures (TI-TF) vs. TestSuiteInput-ModelFailures (TI-MF).

Summary RQ2: Model-based SCRATCH testing is a
convenient alternative to manually written JavaScript
tests and can detect far more faulty program states.

missing key is therefore never triggered and the code coverage
is lower. Another difference between the inputs generated is
that the model can choose to catch bananas, while the test suite
only concentrates on the apple. In student submission C6 05
we have the problem that catching the bananas effectively halts
the program, again only producing left inputs.

Projects C6 07 and C7 17 do not stop when the apple
touches red and also do not move the apple up when touching
the bowl. The model test stops shortly after the ﬁrst apple
touches red, never playing for the entire duration of 30 seconds.
The code coverage of the model inputs therefore never reaches
the code for event ‘timer == 0’ and has lower coverage for
these projects. This could be easily prevented by changing the
end model to test for at least 32 seconds.

VII. CONCLUSIONS
The game-like and randomised nature of visual programming
languages can make testing challenging. In this paper we pro-
posed a model-based approach for testing SCRATCH programs
in the WHISKER framework. Together with model-based input
generation this provides a fully automated testing approach,
the effectiveness of which was indicated by our experiments.
The WHISKER tool contains an integrated GUI to create
models for SCRATCH programs, and to apply model-based
testing to programs under test. Our hope is that a convenient
tool will make building models easy and approachable for
the SCRATCH target audience, such as teachers and content
providers creating tutorials. We provide WHISKER as an open
source tool at the following URL:

Summary RQ1: Since the user model performs equally
well or even better than a manually written test suite
in 31/38 student submissions, inputs derived from a
model pose a powerful alternative to error-prone tests
encoded in JavaScript.

E. RQ2: Is Model-Based SCRATCH Testing Capable of Detect-
ing Faulty Program States?

Similar to the process of generating inputs, model-based
SCRATCH testing offers an alternative to manually written
test suites for evaluating the functional correctness of a given
SCRATCH program. However, to be of any use, models must
be capable of detecting faulty program states reliably. Thus, to
explore the effectiveness of model-based SCRATCH testing, we
compare the number of found program failures of the model-
based approach against the failed test cases in the manually
written test suite.

https://github.com/se2p/whisker

While even simple models can quickly reveal faults in
implementations of SCRATCH programs, our case study shows
that creating a faithful representation of a speciﬁcation can
be non-trivial. A central aspect of our modelling approach
is therefore to allow the creation of many small models,
rather than a single big model. However, the large number
of 21 models of our case study program with a very detailed
speciﬁcation suggest that further support may be necessary for
users. In particular, given that an example solution is usually
available in our application scenario, model inference seems
like a promising direction for future research. Users could also
be supported by techniques like graph minimisation to provide
further insights into eliminating erroneous state transitions.
Another promising avenue of further research is using the
models to automatically derive hints and feedback for learners.

ACKNOWLEDGEMENTS

This work is supported by DFG project FR 2955/3-1 “Testing,

Debugging, and Repairing Blocks-based Programs”.

UI-MFTI-TFTI-MF051015# Detected Erroneous Program States11.98.411.2REFERENCES

[1] J. Maloney, M. Resnick, N. Rusk, B. Silverman, and E. Eastmond, “The
scratch programming language and environment,” ACM Trans. Comput.
Educ., vol. 10, no. 4, pp. 16:1–16:15, 2010.

[2] A. Stahlbauer, M. Kreis, and G. Fraser, “Testing scratch programs
automatically,” Proceedings of the 2019 27th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, pp. 165–175, 2019.

[3] A. Deiner, C. Fr¨adrich, G. Fraser, S. Geserer, and N. Zantner, “Search-
based testing for scratch programs,” in International Symposium on
Search Based Software Engineering. Springer, 2020, pp. 58–72.
[4] F. Hermans, K. T. Stolee, and D. Hoepelman, “Smells in block-based
IEEE, 2016, pp. 68–72.
[5] B. Boe, C. Hill, M. Len, P. Dreschler, Greg a nd Conrad, and D. Franklin,
“Hairball: Lint-inspired static analysis of scratch projects,” in Proc.
SIGCSE, 2013, pp. 215–220.

programming languages,” in Proc. VL/HCC.

[6] J. Moreno-Le´on and G. Robles, “Dr. scratch: A web tool to automatically
evaluate scratch projects,” in Proc. WIPSCE, 2015, pp. 132–133.
[7] P. Techapalokul and E. Tilevich, “Quality hound—an online code smell
analyzer for scratch programs,” in 2017 IEEE Symposium on Visual
Languages and Human-Centric Computing (VL/HCC).
IEEE, 2017, pp.
337–338.

[8] Z. Chang, Y. Sun, T.-Y. Wu, and M. Guizani, “Scratch analysis tool
(sat): a modern scratch project analysis tool bas ed on antlr to assess
computational thinking skills,” in 2018 14th International Wireless
Communications & Mobile Computing Conference (IWCMC).
IEEE,
2018, pp. 950–955.

[9] C. Fr¨adrich, F. Oberm¨uller, N. K¨orber, U. Heuer, and G. Fraser, “Common
bugs in scratch programs,” in Proceedings of the 2020 ACM Conference

on Innovation and Technology in Computer Science Education, 2020,
pp. 89–95.

[10] H. Keuning, J. Jeuring, and B. Heeren, “Towards a systematic review
of automated feedback generation for programming exercises,” in
Proceedings of the 2016 ACM Conference on Innovation and Technology
in Computer Science Education, 2016, pp. 41–46.

[11] D. E. Johnson, “Itch: Individual testing of computer homework for scratch

assignments,” in Proc. SIGCSE, 2016, pp. 223–227.

[12] A. C. Dias Neto, R. Subramanyan, M. Vieira, and G. H. Travassos,
“A survey on model-based testing approaches: a systematic review,”
in Proceedings of the 1st ACM international workshop on Empirical
assessment of software engineering languages and technologies: held
in conjunction with the 22nd IEEE/ACM International Conference on
Automated Software Engineering (ASE) 2007, 2007, pp. 31–36.
[13] M. Utting, A. Pretschner, and B. Legeard, “A taxonomy of model-based
testing approaches,” Software testing, veriﬁcation and reliability, vol. 22,
no. 5, pp. 297–312, 2012.

[14] D. Lee and M. Yannakakis, “Principles and methods of testing ﬁnite
state machines-a survey,” Proceedings of the IEEE, vol. 84, no. 8, pp.
1090–1123, 1996.

[15] S. Anand, E. K. Burke, T. Y. Chen, J. Clark, M. B. Cohen, W. Grieskamp,
M. Harman, M. J. Harrold, P. McMinn, A. Bertolino et al., “An
orchestrated survey of methodologies for automated software test case
generation,” Journal of Systems and Software, vol. 86, no. 8, pp. 1978–
2001, 2013.

[16] S. Keller, M. Krafft, G. Fraser, N. Walkinshaw, K. Otto, and B. Sabitzer,
“Improving scratch programming with crc-card design,” in Proceedings
of the 14th Workshop in Primary and Secondary Computing Education,
2019, pp. 1–4.

