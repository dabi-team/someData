Eﬃcient diﬀerentiable quadratic programming layers: an ADMM
approach

Andrew Butler and Roy H. Kwon
University of Toronto
Department of Mechanical and Industrial Engineering

December 15, 2021

Abstract

Recent advances in neural-network architecture allow for seamless integration of convex opti-
mization problems as diﬀerentiable layers in an end-to-end trainable neural network. Integrating
medium and large scale quadratic programs into a deep neural network architecture, however, is
challenging as solving quadratic programs exactly by interior-point methods has worst-case cubic
complexity in the number of variables.
In this paper, we present an alternative network layer
architecture based on the alternating direction method of multipliers (ADMM) that is capable of
scaling to problems with a moderately large number of variables. Backward diﬀerentiation is per-
formed by implicit diﬀerentiation of the residual map of a modiﬁed ﬁxed-point iteration. Simulated
results demonstrate the computational advantage of the ADMM layer, which for medium scaled
problems is approximately an order of magnitude faster than the OptNet quadratic programming
layer. Furthermore, our novel backward-pass routine is eﬃcient, from both a memory and com-
putation standpoint, in comparison to the standard approach based on unrolled diﬀerentiation
or implicit diﬀerentiation of the KKT optimality conditions. We conclude with examples from
portfolio optimization in the integrated prediction and optimization paradigm.

Keywords: Data driven stochastic-programming, diﬀerentiable neural networks, quadratic pro-

gramming, ADMM

1 Introduction

Many problems in engineering, statistics and machine learning require solving convex optimization
programs.
In many real-world applications, the solution to the convex optimization program is a
single component in a larger decision-making process (see for example [14, 13, 19]). Recent advances
in neural-network architecture allow for seamless integration of convex optimization programs as dif-
ferentiable layers in an end-to-end trainable neural network [1, 2, 3].

In this paper, we consider convex programming layers that take the form of parametric quadratic

programs (PQPs) with linear equality and box inequality constraints:

1
2
0
2

c
e
D
4
1

]

C
O
.
h
t
a
m

[

1
v
4
6
4
7
0
.
2
1
1
2
:
v
i
X
r
a

minimize
z

1
2
subject to A(θ) z = b(θ),

zT Q(θ) z + zT p(θ)

l(θ) ≤ z ≤ u(θ)

(1)

1

 
 
 
 
 
 
Here z ∈ Rdz denotes the decision variable and Q(θ), p(θ), A(θ), b(θ), l(θ), u(θ) are the parameter-
ized problem variables. Program (1) occurs in many applications to statistics [41, 42], machine-learning
[25, 29], signal-processing [30] and ﬁnance [8, 35, 34].

In general, a diﬀerentiable convex optimization layer can be viewed as a function that maps the
program input variables to optimal primal(-dual) solution(s). Therefore, in a fully integrated system,
optimizing problem variables by backpropogation ultimately requires computing the action of the
Jacobian of the optimal solution(s) with respect to the corresponding program input variables. For
example, the OptNet layer, proposed by Amos and Kolter [3] is a specialized diﬀerentiable optimization
layer that uses a primal-dual interior-point method for small scale batch quadratic programs. The
authors demonstrate that the solution to the system of equations provided by the KKT conditions
at optimality provide a system of equations for implicit diﬀerentiation with respect to all relevant
problem variables. Therefore, by strategically caching the KKT matrix factorization(s) (obtained
during the forward-pass), then the gradients required for backpropogation are obtained at little extra
Indeed, the authors note that the OptNet layer is computationally eﬃcient and
additional cost.
therefore practical for small-scale problems (dz < 100). However, solving convex quadratic programs
exactly by interior-point methods has worst-case time complexity on the order of O(d3
z) [27]. Therefore,
for medium (100 < dz < 1000) and large (dz > 1000) scale problems, embedding an OptNet layer in
a larger neural network can be computationally intractable.

In this paper, we address the computational challenges in the medium to large scale limit and
propose an alternative diﬀerentiable network architecture for batch constrained quadratic programs
of the form of Program (1). Our diﬀerentiable quadratic programming layer is built on top of the al-
ternating direction method of multipliers (ADMM) algorithm, which recently has become increasingly
popular for solving large-scale convex optimization problems [11, 36, 39, 40, 38]. Indeed, embedding
the ADMM algorithm in a larger neural-network system has been fundamental to recent innovations
in signal processing, compressed sensing, imaging and statistics (see for example [16, 45, 46]). The
standard ADMM-network implementation computes the relevant gradients by unrolling the ADMM
computational graph, which is memory ineﬃcient and typically requires substantially larger networks.
Furthermore, when the number of ADMM iterations is large in the forward-pass then the unrolled
gradient can be computationally demanding. Alternatively, many diﬀerentiable convex optimization
layers compute the relevant gradients by implicitly diﬀerentiating the Karush–Kuhn–Tucker (KKT)
optimality conditions. Indeed diﬀerentiating the KKT system of equations is possible but unfortu-
nately requires solving a system of equations of dimension R3dz×3dz , which can also be computationally
impractical. As an alternative, we present a novel modiﬁed backward-pass routine that is eﬃcient from
both a memory and computational standpoint. Speciﬁcally, we recast the ADMM algorithm as a ﬁxed-
point iteration and apply the implicit function theorem to the resulting residual map in order to obtain
the relevant backpropogation gradients. A primary advantage of our ﬁxed-point diﬀerentiation is that
the ﬁxed-point iteration is of dimension dz and therefore the resulting system of equations is approxi-
mately 3 times smaller than the equivalent KKT system. Finally, our diﬀerentiable ADMM-layer and
all algorithmic implementations is made available as an open-source R package, available here:

https://github.com/adsb85/lqp

The remainder of the paper is outlined as follows. We begin with the problem motivation and a
brief discussion of related work in the ﬁeld of diﬀerentiable convex optimization layers. In Section 2

2

we review the ADMM algorithm and present our ADMM-based architecture for forward solving batch
PQPs. We review the KKT based implicit diﬀerentiation and then present the ﬁxed-point iteration
and derive the expression for the relevant gradients. In Section 3 we perform several simulations (under
numerous model speciﬁcations) and compare the computational eﬃciency and performance accuracy
of our ADMM-layer with the state-of-the-art OptNet layer. We demonstrate that for medium-scale
problems, our ADMM-layer implementation is approximately an order of magnitude faster than the
OptNet layer and provides solutions that are equally as optimal. Moreover, we compare the com-
putational eﬃciency of the backpropogation routines based on unrolled diﬀerentiation, KKT implicit
diﬀerentiation and ﬁxed-point implicit diﬀerentiation. We demonstrate that the ﬁxed-point implicit
diﬀerentiation is universally more eﬃcient than the KKT implicit diﬀerentiation, and under certain
conditions is preferred to the unrolled diﬀerentiation. We conclude with a real world application of
the ADMM-layer to a medium scale portfolio optimization problem in the integrated prediction and
optimization paradigm.

1.1 Related work:

1.1.1 Problem Motivation

Many problems in machine learning, statistics, engineering and operations research involve both pre-
dictive forecasting and decision based optimization. Recent advances in neural network architecture
embed convex optimization programs as diﬀerentiable layers in a larger neural network structure. A
fully integrated prediction and optimization (IPO) architecture therefore enables the integration of
predictive forecasting and convex optimization and allows for the minimization of the total decision er-
ror induced by the forecast estimates (see for example [13, 14, 19, 20, 21]). This is in contrast to a more
traditional ‘predict, then optimize’ approach which would ﬁrst ﬁt the predictive models (for example
by maximum likelihood or least-squares) and then ‘plug-in’ those predictions to the corresponding
decision-based optimization program. While it is true that a perfect predictive model would lead to
optimal decision making, in reality, all predictive models do make some error, and thus an ineﬃciency
exists in the ‘predict, then optimize’ paradigm. With the widespread adoption of machine-learning
and data science in operations research, there has been a growing body of literature on data-driven
decision making and the relative merits of decoupled versus integrated predictive decision-making
[4, 7, 28, 33, 32, 43].

The preliminary ﬁndings of the aforementioned work advocate strongly for an IPO approach. In-
deed, IPO models typically exhibit lower model complexity and demonstrate statistically signiﬁcant
improvements in out-of-sample performance in comparison to more traditional ‘predict, then optimize’
models. Unfortunately, training medium and large scale IPO models can be computationally demand-
ing. For example, Elmachtoub and Grigas [20], Elmachtoub et al. [21] report that their ‘smart predict,
then optimize’ models can take several hours to train medium and large scale problems, while tradi-
tional prediction methods typically take seconds or minutes to train. Butler and Kwon [13] provide
details on the computational complexity of IPO models trained with the OptNet layer and demon-
strate that when dz > 100 then ﬁtting IPO models can be computationally impractical. Improving
the eﬃciency of the integrated framework is therefore an open and important area of research.

3

1.1.2 Diﬀerentiable convex optimization layers:

Diﬀerentiable convex optimization layers provide an eﬃcient and seamless framework for integrating
predictive forecasting with downstream decision-based optimization in a end-to-end trainable neural
network. Modern neural network technology (such as torch or tensorﬂow ) require that every layer in
the network inherits a forward and backward-pass routine. For diﬀerentiable convex optimization lay-
ers , the forward-pass is typically an iterative optimization algorithm that converts problem variables
into optimal primal(-dual) solution(s). The backward-pass routine therefore computes the action of
the Jacobian of the optimal solution(s) with respect to all problem variables, and returns the left
matrix-vector product of the Jacobian with the previous backward-pass gradient(s).

For example, the state-of-the-art OptNet layer implements a primal-dual interior-point method
for solving small-scale batch constrained quadratic programs [3]. For backward diﬀerentiation, the
authors implement a novel and eﬃcient argmin diﬀerentiation routine that implicitly diﬀerentiates
the KKT system of equations at optimality. By strategically caching the factorized KKT left-hand
side matrix then the resulting method is shown to be computationally tractable for small problems
within the context of deep neural network architectures. The author’s acknowledge, however, that
the OptNet layer may be impractical for optimization problems with a moderate to large number of
variables.

More recently, Agrawal et al. [2] provide a general framework for diﬀerentiable convex cone pro-
gramming. Their forward-pass recasts the conic program in its equivalent homogeneous self-dual
embedding form, which is then solved by operator splitting [36]. In the backward-pass, the relevant
gradients are obtained by implicit diﬀerentiation of the residual map provided by the homogeneous
self-dual embedding. The resulting diﬀerentiable cone programming layer is ﬂexible, but requires the
user to transform their problem into a canonical form, which is often time-consuming, prone to error
and requires a certain level of domain expertise.

Alternatively, Agrawal et al. [1], provide a domain-speciﬁc language for diﬀerentiable disciplined
convex programs. Their approach abstracts away the process of converting problems to canonical
form with minimal loss in computational eﬃciency in comparison to specialized convex optimization
layers. They also provide an eﬃcient sparse matrix solver, which for sparse quadratic programs is on
average an order of magnitude faster than the OptNet layer. Similarly, Blondel et al. [9] provide an
eﬃcient and modular approach for implicit diﬀerentiation of optimization problems. They consider
KKT, proximal gradient and mirror descent ﬁxed-point implicit diﬀerentiation and provide a software
infrastructure for eﬃciently integrating their modular implicit diﬀerentiation routines with state-of-
the-art optimization solvers. That said, for solving batches of convex optimization problems it is
often preferred and more eﬃcient to avail of optimization solvers that have the ability to exploit fast
GPU-based batch solves.

1.1.3 ADMM and unrolled diﬀerentiation:

The alternating direction method of multipliers (ADMM) algorithm, ﬁrst proposed by Gabay and
Mercier [24] and Glowinski and Marroco [26], is well suited to many large-scale and distributed prob-
lems common to applications of statistics, machine learning, control and ﬁnance. We note that the
ADMM algorithm is closely related to algorithms such as dual ascent, the augmented Lagrangian
method of multipliers, and operator (Douglas–Rachford) splitting and refer to Boyd et al. [11] for a

4

comprehensive overview.

Embedding the above mentioned algorithms in larger neural-network structures has been funda-
mental to recent innovations in signal processing, compressed sensing, imaging and statistics (see for
example [6, 16, 17, 23, 31]). Perhaps most closely related to our own work, the ADMM-Net, ﬁrst
proposed by Yang et al. [46], recasts and embeds the iterative ADMM procedure as a fully learnable
network graph. The authors provide examples from compressive sensing magnetic resonance imaging
and demonstrate that their ADMM-Net achieves state-of-the-art model accuracy and computationally
eﬃciency. More recently, the Diﬀerentiable Linearized ADMM (D-LADMM) algorithm, proposed by
[45], generalizes the ADMM-Net and is capable of solving general deep learning problems with equality
constraints. The authors show that there exists a set of learnable parameters for D-LADMM to gen-
erate global solutions and provide the relevant convergence analysis. However, in all cases mentioned
above the authors consider either unconstrained or linear equality constrained least-squares problems
and do not consider inequality constraints. Furthermore, they perform the action of argmin diﬀerenti-
ation by unrolling the ‘inner-loop’ of the convex optimization routine, which necessitates substantially
larger and less eﬃcient networks [3, 4].

Our ADMM-layer derives inspiration from both the diﬀerentiable convex optimization and ADMM
network literature. To our knowledge, our ADMM-layer is the ﬁrst implementation of its kind that
can eﬃciently handle medium to large scale diﬀerentiable constrained PQPs. In this paper we demon-
strate that solving medium and large scale QPs by interior-point methods can be computationally
burdensome. An obvious course of action is to replace the interior-point algorithm in the forward-pass
with a more computationally eﬃcient ﬁrst-order method, such as ADMM. However, implementing
an eﬃcient backward-pass routine by implicit diﬀerentiation of the KKT conditions is challenging as
the ADMM algorithm does not explicit solve the KKT system of equations. Therefore, unlike the
OptNet implementation, at each ‘outer’ iteration (hereafter referred to as epochs) we must form and
solve the resulting KKT system, which for large scale problems creates a computational bottleneck.
Unrolled diﬀerentiation of the ADMM algorithm is appropriate for small scale problems. However, for
larger scale problems or for problems that require solving the convex optimization problem to a high
degree of accuracy, an unrolled diﬀerentiation approach can also be computationally impractical. In
contrast, our novel ﬁxed-point implicit diﬀerentiation method is shown to be computationally eﬃcient
and invariant to the number of ‘inner’ iterations performed in the ADMM forward-pass. Furthermore,
as mentioned earlier, a primary advantage of the ﬁxed-point diﬀerentiation is that the ﬁxed-point iter-
ative scheme is of dimension dz and is therefore approximately 3 times smaller than the KKT system.
We demonstrate that in the absence of a pre-factorized KKT system, the ﬁxed-point implicit diﬀer-
entiation is preferred to the KKT implicit diﬀerentiation and is competitive with the eﬃcient KKT
factorization caching provided in the OptNet layer. Furthermore, we demonstrate that for small-scale
problems, our ADMM-layer is competitive with the state-of-the-art OptNet layer in terms of accuracy
and computational eﬃciency. For medium and large scale problems, our ADMM layer is shown to be
approximately an order of magnitude faster than the OptNet layer. Of course, our ADMM-layer is
not without its own limitations and areas for improvement, discussed in detail in Section 4.

5

2 Methodology

In general, the ADMM algorithm is applied to problems of the form:

minimize

f (x) + g(z)

subject to A x + B z = c

(2)

with decision variables x ∈ Rdx, z ∈ Rdz and problem variables A ∈ Rdeq×dx, B ∈ Rdeq×dz and
c ∈ Rdeq . In order to guarantee convergence we assume that f : Rdx → R and g : Rdz → R are closed,
proper convex functions [11]. The augmented Lagrangian of Program (2) is given by:

Lρ(x, z, y) = f (x) + g(z) + λT (r) +

ρ
2

(cid:107)r(cid:107)2
2,

(3)

with Lagrange dual variable λ, residual r = A x + B z − c and user-deﬁned penalty parameter ρ > 0.
We denote µ = ρ−1 λ and therefore the well-known scaled ADMM iterations are as follows:

xk+1 = argmin

f (x) +

x

zk+1 = argmin

g(z) +

z

(cid:107)A x + B zk − c +µk(cid:107)2
2

ρ
2
ρ
(cid:107)A xk+1 + B z − c +µk(cid:107)2
2
2

µk+1 = µk + A xk+1 + B zk+1 − c

(4)

where xk and zk denote the decision variables at iteration k.

We denote rk = A xk + B zk − c and sk = ρ AT B(zk − zk−1) as the primal and dual residual at
iteration k. Let (cid:15)p > 0 and (cid:15)d > 0 be the user deﬁned stopping tolerances for the primal and dual
residuals, respectively. Therefore a reasonable stopping criteria would be:

rk ≤ (cid:15)p

and

sk ≤ (cid:15)d.

2.1 ADMM for parametric quadratic programs

We consider convex parametric quadratic programs (PQPs) of the form:

minimize
z

1
2
subject to A(θ) z = b(θ),

zT Q(θ) z + zT p(θ)

l(θ) ≤ z ≤ u(θ),

(5)

(6)

with decision variable z ∈ Rdz . The objective function is therefore deﬁned by a vector p(θ) ∈ Rdz and
symmetric positive deﬁnite matrix Q(θ) ∈ Rdz×dx. Here, A(θ) ∈ Rdeq×dz , b(θ) ∈ Rdeq , l(θ) ∈ Rdz
and u(θ) ∈ Rdz deﬁne the linear equality and box inequality constraints. We assume that all problem
variables are parameterized by θ and are therefore trainable when integrated in an end-to-end neural
network; rather than simply being supplied by the user.

6

2.1.1 ADMM-layer: forward-pass

Our ADMM-layer solves Program (6) in the forward-pass by applying the ADMM algorithm as outlined
in Section 2. Note that for ease of notation we temporarily drop the parameterization, θ.

We deﬁne

1
2
with domain {x | A x = b}. Similarly, we deﬁne

f (x) =

xT Q x + xT p,

g(z) = Il≤z≤u(z)

(7)

(8)

where Il≤z≤u(z) denotes the indicator function with respect to the linear inequality constraints. Pro-
gram (6) is then recast to the following convex optimization Program:

minimize

f (x) + g(z)

subject to x − z = 0.

(9)

Applying the ADMM iterations, as deﬁned by Equations (4), to Program (9) therefore gives the
following iterative optimization algorithm:

1
2

xT Q x + xT p +

ρ
2

(cid:107)x − zk +µk(cid:107)2
2

xk+1 = argmin
{x | A x=b}
ρ
2

zk+1 = argmin
{l≤z≤u}
µk+1 = µk + xk+1 − zk+1

(cid:107)xk+1 − z +µk(cid:107)2
2

(10a)

(10b)

(10c)

The ADMM algorithm, as deﬁned by Equations (10), allows for eﬃcient optimization of medium
and large scale quadratic programs. Firstly, we note that (10b) is a least squares problem with box-
inequality constraints, and therefore can be solved analytically. Speciﬁcally, we deﬁne the euclidean
projection onto a set of box constraints as:

Π(x) =


lj

xj
uj



if xj < lj
if lj ≤ xj ≤ lj
if xj > uj

.

The analytic solution to Program (10b) is therefore given by:

zk+1 = Π(xk+1 +µk).

(11)

(12)

Furthermore, Program (10a) is an equality constrained quadratic program, which can also be solved
analytically. Speciﬁcally, the KKT optimality conditions of Program (10a) can be expressed as the
solution to the following linear system of equations:

(cid:20)Q +ρ Ix AT
0

A

(cid:21) (cid:20)xk+1
ηk+1

(cid:21)

= −

(cid:21)
(cid:20)p −ρ(zk − µk)
− b

,

(13)

7

with identity matrix Ix ∈ Rdx×dx. Applying Equations (12) and (13) allows us to express the ADMM
iterations in a simpliﬁed form:

(cid:21)

(cid:20)xk+1
ηk+1

= −

(cid:20)Q +ρ Ix AT
0

A

(cid:21)−1 (cid:20)p −ρ(zk − µk)

(cid:21)

− b

zk+1 = Π(xk+1 +µk)
µk+1 = µk + xk+1 − zk+1

(14a)

(14b)

(14c)

Observe that the per-iteration cost of the ADMM algorithm is dominated by solving the system of
Equations (13). Note, however, that this linear system is in general smaller than the Newton system
found in a standard primal-dual interior-point solvers by a factor of approximately 5, and therefore
remains tractable for medium and large scale problems. Furthermore, if ρ is static then the left-hand
side matrix in Equation (13) remains unchanged at each iteration and therefore is factorized only once
at the onset of the ADMM algorithm. Furthermore, as we demonstrate below, this matrix factorization
can subsequently be cached and invoked during backpropogation to compute the relevant gradients.
Lastly, we note that if the matrices Q and A remain unchanged at each epoch of gradient descent,
then the left-hand-side matrix needs to only be factorized once during the entire training process.

2.1.2 ADMM-layer: unrolled diﬀerentiation

Note that the standard unrolled diﬀerentiation approach ‘unrolls’ the iterations in Equation (14) by
standard backpropogation [37] and requires that each operation in Equation (14) be diﬀerentiable.
We refer to Domke [17] and Diamond et al. [16] for a more comprehensive overview of unrolled
diﬀerentiation. Our unrolled diﬀerentiation is invoked by the standard auto-diﬀerentiation routine in
the torch library.

2.1.3 ADMM-layer: KKT implicit diﬀerentiation

As an alternative to unrolled diﬀerentiation, we note that the system of equations provided by the
KKT conditions at optimality is a ﬁxed point mapping. As outlined by [3, 4], it is therefore possible to
apply the implicit function theorem and derive the gradient of the primal-dual variables with respect
to the problem variables in Program (6). In this section we derive the KKT implicit diﬀerentiation
for our ADMM solver. We begin with a few deﬁnitions.

Deﬁnition 1. Let F : Rdv × Rdθ → Rdv be a continuously diﬀerentiable function with variable v and
parameter θ. We deﬁne v∗ as a ﬁxed-point of F at (v∗, θ) if:

F (v∗, θ) = v∗ .

Deﬁnition 2. The residual map, G : Rdv × Rdθ → Rdv of a ﬁxed point, (v∗, θ), of F is given by:

G(v∗, θ) = F (v∗, θ) − v∗ = 0.

The implicit function theorem, as deﬁned by Dontchev and Rockafellar [18], then provides the

conditions on G for which the Jacobian of the solution mapping with respect to θ is well deﬁned.

8

Theorem 1. Let G : Rdv × Rdθ → Rdv be a continuously diﬀerentiable function in a neighborhood
of (v∗, θ) such that G(v∗, θ) = 0. Denote the nonsingular partial Jacobian of G with respect to
v as ∇vG(v∗, θ). Then v(θ) is an implicit function of θ and is continuously diﬀerentiable in a
neighborhood, Θ, of θ with Jacobian:

∇θ v(θ) = −[∇vG(v(θ), θ)]−1∇θG(v(θ), θ) ∀

θ ∈ Θ.

(15)

Corollary 1. Let F : Rdv ×Rdθ → Rdv be a continuously diﬀerentiable function with ﬁxed-point (v∗, θ).
Then v(θ) is an implicit function of θ and is continuously diﬀerentiable in a neighborhood, Θ, of θ
with Jacobian:

∇θ v(θ) = [Iv −∇vF (v(θ), θ)]−1∇θF (v(θ), θ) ∀

θ ∈ Θ.

(16)

For constrained quadratic programming, let us denote the primal-dual solution at optimality by
, η∗), where z∗ and η∗ are deﬁned by Equations (13) at optimality. Note that the dual

ν∗ = (z∗, ˜λ
variables associated with the inequality constraints are given by ˜λ

= (λ∗

∗

∗

−, λ∗

+) with:

− = − min(ρ µ∗, 0)
λ∗

and λ∗

+ = max(ρ µ∗, 0).

(17)

We deﬁne the box inequality constraints as G z ≤ h, where

G =

(cid:21)

(cid:20)− Ix
Ix

and h =

(cid:21)

(cid:20)− l
u

.

Note that all constraints are aﬃne and therefore Slater’s condition reduces to feasibility. The KKT
conditions for stationarity, primal feasibility, and complementary slackness therefore deﬁnes a ﬁxed-
point at optimality ν∗ given by:

G(ν∗, θ) =





p + Q z∗ + GT ˜λ
diag(˜λ

∗

)(G z∗ − h)

∗

+ AT η∗



 =







(18)



0
0
0

A z∗ − b

Applying Theorem 1, we take the diﬀerential of these conditions to give the following system of
equations:





Q
diag(˜λ
A

∗

GT

) G diag(G z∗ − h)

0





AT
0
0





dz
d λ
d ν



 = −





dQ z∗ +dp + dGT ˜λ
∗

∗

diag(˜λ

+ dAT η ∗
∗

)dh



 .

(19)

)dG z∗ − diag(˜λ
dA z∗ −db

Observe that the left side matrix gives the optimality conditions of the convex quadratic problem,
which, when solving by interior-point methods, must be factorized in order to obtain the solution
to the nominal program [10]. The right side gives the diﬀerentials of the relevant functions at the
achieved solution with respect to any of the problem variables. In practice, however, we never explicitly
form the right-side Jacobian matrix directly. Instead we follow the work of Amos and Kolter [3] and
compute the left matrix-vector product of the Jacobian with the previous backward-pass gradient,
∂(cid:96)
∂ z∗ , as outlined below:

9



 = −


¯dz
¯dλ

¯dη





∗

Q GT diag(˜λ
G diag(G z∗ − h)
A

) AT
0
0

0



−1 





(cid:0) ∂(cid:96)
∂ z∗
0
0

(cid:1)T



 .

(20)

Equation (20) allows for eﬃcient computation of the gradients with respect to any of the relevant
problem variables. This is particularly true when using interior-point methods as the required gradients
are eﬀectively obtained ‘for free’ upon factorization of the left matrix when obtaining the solution, z∗,
in the forward-pass. In the ADMM algorithm, however, we never explicitly solve the KKT system of
equations and therefore we must form and factorize the left side KKT matrix during the backward
pass routine. Observe that the left-side matrix in Equation (20) is of dimension 3dz +deq and therefore
for large scale problems solving this system of equations can be computationally burdensome. Finally,
for the reader’s interest, we state the gradients for all problem variables and refer the reader to Amos
and Kolter [3] for their derivation.

∂(cid:96)
∂ Q
∂(cid:96)
∂ A
∂(cid:96)
∂ G

=

1
2

(cid:16)¯dz z∗T + z∗ ¯dT

z

(cid:17)

= ¯dη z∗T + η∗ ¯dT
z

= diag(λ∗)¯dλ z∗T + λ∗ ¯dT
z

∂(cid:96)
∂ p
∂(cid:96)
∂ b
∂(cid:96)
∂ h

= ¯dz

= −¯dη

= − diag(λ∗)¯dλ

(21)

2.1.4 ADMM-layer: ﬁxed-point implicit diﬀerentiation

In this section we demonstrate that the ADMM iterations in Equation (13) can be cast as a ﬁxed-
point iteration of dimension dz + deq. In many applications, deq is typically much smaller than dz,
and therefore the proposed ﬁxed-point diﬀerentiation will almost certainly decrease the computational
overhead of the backward-pass routine. We begin with the following proposition. Note that all proofs
are available in the Appendix.

Proposition 1. Let vk = xk+1 + µk, ˜vk = (vk, ηk) and deﬁne F : Rd˜v ×Rdθ → Rd˜v . Then the ADMM
iterations in Equation (13) can be cast as a ﬁxed-point iteration of the form F (˜v, θ) = ˜v given by:

(cid:21)

(cid:20)vk+1
ηk+2

= −

(cid:20)Q +ρ Iv AT
0

A

(cid:21)
(cid:21)−1 (cid:20)p −ρ(2Π(vk) − vk)

− b

(cid:21)

(cid:20) vk
ηk+1

−

(cid:21)

(cid:20)Π(vk)
ηk+1

.

+

We follow Busseti et al. [12] and deﬁne the derivative of the projection operator, Π as:

DΠ(x) =


0

1

0

if xj < lj
if lj ≤ xj ≤ lj
if xj > uj

.

(22)

(23)

Observe that DΠ(x) is not continuously diﬀerentiable when xj = lj or xj = uj.
In practice, we
can overcome the non-diﬀerentiability of Π by introducing a small perturbation to x, thus moving x

10

away from the boundaries. Alternatively, smooth sigmoid based approximations to Π(x) may also be
suitable. In all experiments below, however, we invoke DΠ(x) directly as deﬁned in Equation (23).

The Jacobian, ∇˜vF , is therefore deﬁned as:

∇˜vF = −

(cid:20)Q +ρ Iv AT
0

A

(cid:21)−1 (cid:20)−ρ(2DΠ(v) − Iv) 0
0
0

(cid:21)

(cid:20)Iv
0

+

(cid:21)

0
Iη

(cid:20)DΠ(v)
0

−

(cid:21)

.

0
Iη

Corollary 1 therefore gives the desired Jacobian, ∇θ ˜v(θ), with respect to the parameter θ:

∇θ ˜v(θ) = [I˜v −∇˜vF (˜v(θ), θ)]−1∇θF (˜v(θ), θ)

From the deﬁnition of v we have that the Jacobians ∇θ x(θ) and ∇θ η(θ) are given by:

(cid:21)

(cid:20)∇θ x(θ)
∇θ η(θ)

(cid:20)DΠ(v)
0

=

(cid:21) (cid:104)

0
Iη

I˜v −∇˜vF (˜v(θ), θ)

(cid:105)−1

∇θF (˜v(θ), θ)

(24)

(25)

(26)

As before we never form the Jacobians ∇θ x(θ) and ∇θ η(θ) directly. Instead, we compute the
∂ z∗ , as outlined

left matrix-vector product of the Jacobian with the previous backward-pass gradient, ∂(cid:96)
below.
Proposition 2. Let ˆdx and ˆdη be deﬁned as:
(cid:20)ˆdx
ˆdη

(cid:20)Q +ρ Iv AT
0

I˜v −∇˜vF (˜v(θ), θ)

(cid:105)−T (cid:20)DΠ(v)

(cid:21)−1 (cid:104)

0
Iη

=

(cid:35)

(cid:21)

0

A
(cid:34) (cid:20)DΠ(v)
0

=

(cid:21) (cid:20)Q +ρ Iv AT
0
A

0
Iη

(cid:21)

+

(cid:20)−ρ(2DΠ(v) − Iv) 0
0
0

(cid:1)T

(cid:21) (cid:34)(cid:0) − ∂(cid:96)
∂ z∗
0
(cid:21) (cid:35)−1 (cid:20)DΠ(v)

0

(cid:21) (cid:34)(cid:0) − ∂(cid:96)
∂ z∗
0

0
Iη

(cid:35)

(cid:1)T

.

(27)

Then the gradients of the loss function, (cid:96), with respect to problem variables Q, p, A and b are given
by: .

∂(cid:96)
∂ Q
∂(cid:96)
∂ A

=

1
2

(cid:16)ˆdx x∗T + x∗ ˆd

T
x

(cid:17)

= ˆdη x∗T + η∗ ˆd

T
x

∂(cid:96)
∂ p
∂(cid:96)
∂ b

= ˆdx

= −ˆdη

(28)

Computing the gradients of the loss with respect to the box constraint variables, l and u is also

straightforward.
Proposition 3. Deﬁne ˜µ∗ and ˆdλ as:

˜µ∗

j =

(cid:40)

µ∗
j
1

if µ∗
j (cid:54)= 0
otherwise,

and

ˆdλ = diag(ρ ˜µ∗)−1(cid:16)

−

(cid:17)T

(cid:16) ∂(cid:96)
∂ z∗

− Q ˆdx − AT ˆdη

(cid:17)

.

Then the gradients of the loss function, (cid:96), with respect to problem variables l and u are given by:

∂(cid:96)
∂ l

= diag(λ∗

−)ˆdλ

= − diag(λ∗

+)ˆdλ.

∂(cid:96)
∂ u

11

(29)

(30)

(31)

We now have a framework for computing the gradient with respect to all problem variables by
implicit diﬀerentiation of the ﬁxed-point mapping of the transformed ADMM iterations. We re-iterate
that the implicit diﬀerentiation of the KKT conditions requires solving a system of equations on the
order of 3dz + deq. In contrast, the ﬁxed-point iteration, presented in Equation (22) is of dimension
dz + deq. As we will demonstrate shortly, reducing the dimension of the ﬁxed-point mapping results
in a considerable improvement in computational eﬃciency in the backward-pass.

3 Computational experiments

We present several experimental results that highlight the computational eﬃciency and performance
accuracy of the ADMM-layer. In all experiments, computational eﬃciency is measured by the average
runtime (in seconds), required to implement the forward-pass and backward-pass algorithms of each
model. We compare across 4 models:

1. ADMM Unroll: ADMM in the forward-pass and unrolled diﬀerentiation in the backward-pass.

2. ADMM KKT: ADMM in the forward-pass and KKT implicit diﬀerentiation in the backward-

pass.

3. ADMM FP: ADMM in the forward-pass and ﬁxed-point implicit diﬀerentiation in the backward-

pass.

4. OptNet: primal-dual interior-point method in the forward-pass and eﬃcient KKT implicit

diﬀerentiation in the backward-pass.

Both the ADMM and interior-point solvers terminate when the L2 norms of the primal and dual
residuals are suﬃciently small (i.e. less than some user-deﬁned tolerance (cid:15)tol). In many applications,
however, it is not always necessary to solve the batch QPs exactly during training. We therefore
consider and compare the computational eﬃciency of each model over several stopping tolerances:
(cid:15)tol ∈ {10−1, 10−3, 10−5}. Going forward, the model label ‘ADMM FP 3’, for example, denotes the
ADMM FP model with a stopping tolerance of 10−3.

Lastly, ﬁrst-order methods are known to be vulnerable to ill-conditioned problems and the resulting
convergence rates can vary signiﬁcantly when the data and algorithm parameters (ρ) are poorly scaled.
Many ﬁrst-order solvers therefore implement a preconditioning and problem scaling initialization step
(see for example [36, 38, 40]). In our case, however, the QP problem variables are parameterized and
are therefore expected to change at each epoch. Scaling and conditioning the problem variables at
each epoch would potentially result in excessive computational overhead. As a result, our ADMM-
layer implementation does not include a preconditioning step. Instead, in all experiments presented
below, we normalize the problem data to have approximately unit standard deviation (on average) and
manually scale the problem variables: p, Q, A and b where appropriate. We ﬁnd that for unit-scaled
problem data, a value of ρ ∈ {0.10, 1.0} provides a consistent rate of convergence. Indeed, an eﬃcient
and dynamic preconditioning and scaling routine is an interesting area of future research.

All experiments are conducted on an Apple Mac Pro computer (2.7 GHz 12-Core Intel Xeon E5,128
GB 1066 MHz DDR3 RAM) running macOS ‘Catalina’. All computations are run on an unloaded,
single-threaded CPU. The software was written using the R programming language (version 4.0.0)
and torch (version 0.6.0).

12

3.1 Experiment 1: ADMM-layer performance

We conduct an experiment comparing the computational eﬃciency of the ADMM and OptNet models
with various stopping tolerances. We randomly generate problem data of dimension:

dz ∈ {10, 50, 100, 250, 500, 1000}.

and for each trial implement the forward and backward algorithms on a mini-batch size of 128. Problem
UT U where entries of U ∈ R2dz×dz are sampled
variables are generated as follows. We set Q = 1
2dz
from a standard normal distribution. We randomly generate p by sampling from the standard normal
distribution and randomly generate l and u by randomly sampling from the uniform distribution with
domain [−2, −1] and [1, 2], respectively . Finally we set A = 1 and b = 1.

Figure 1 provides the average runtime and 95%-ile conﬁdence interval, evaluated over 10 trials,
of the forward and backward-pass algorithms. We make several important observations. First, for
small scale problems (dz < 100), there is negligible performance diﬀerences across all methods of the
same stopping tolerance. As expected, the total runtime increases as the required stopping tolerance
decreases. For medium scaled problems (100 < dz < 1000) we observe a substantial performance degra-
dation in both the ADMM-KKT models and the OptNet models, in comparison to the ADMM-FP and
ADMM-Unroll models. Speciﬁcally, the ADMM-KKT model exhibits an increase in computation time
that is anywhere from 4 to 16 times larger than the corresponding ADMM ﬁxed-point backward-pass
implementation. This result is not surprising as the ADMM-KKT backward-pass algorithm must ﬁrst
form and then factorize the KKT system of equations, which is of dimension 3dz + deq. In contrast,
the ﬁxed-point backward-pass routine solves a system of equations of size dz + deq and is shown to
be comparable in computational eﬃciency to the OptNet backward-pass algorithm. Furthermore,
for problems of size dz ≥ 250, we note that the ADMM-FP and ADMM-Unroll models are approx-
imately an order of magnitude more eﬃcient than the corresponding OptNet models. For example,
when dz = 1000 and (cid:15)tol = 10−3, the total runtime for the OptNet model is 150 seconds, whereas
the total runtime for the ADMM model is less than 10 seconds; over an order of magnitude faster.
This increase in computational performance will ultimately enable training architectures that can
practically support substantially larger quadratic optimization problems. Lastly, we note that while
the ADMM-unroll algorithm is relatively eﬃcient, it requires a signiﬁcantly larger memory footprint,
which may be impractical in some settings. Furthermore, we observe that the as the stopping tolerance
decreases the computation time of the unrolled backward-pass increases. In contrast, the ﬁxed-point
implicit diﬀerentiation method is invariant to the number of ‘inner’ iterations performed in the ADMM
forward-pass. Going forward, we choose to work with the ADMM ﬁxed-point model as it is eﬃcient
from both a computational and memory standpoint.

13

(a) dz = 10.

(b) dz = 50.

(c) dz = 100.

(d) dz = 250.

(e) dz = 500.

(f) dz = 1000.

Figure 1: Computational performance of ADMM-FP, ADMM-KKT, ADMM-Unroll and Optnet for
various problem sizes, dz, and stopping tolerances. Batch size = 128.

3.2 Experiment 2: learning p

We now consider a full training experiment whereby the objective is to learn a parameterized model for
the variable p, that is optimal in the context of the remaining QP problem variables. This problem was
considered by Donti et al. [19] with applications to power scheduling and battery storage, and more
recently by Butler and Kwon [14] for optimal return forecasting within the context of a mean-variance
portfolio. We refer to the aforementioned work for more details.

14

1.4x1.0x0.6x1.1x1.2x1.2x0.000.050.100.150.200.250.300.35ADMM FP 1ADMM FP 3ADMM FP 5ADMM KKT 1ADMM KKT 3ADMM KKT 5ADMM Unroll 1ADMM Unroll 3ADMM Unroll 5OptNet 1OptNet 3OptNet 5Time (s) BackwardForwardTotal2.0x1.6x1.2x2.6x2.5x2.4x0.000.050.100.150.200.250.300.35ADMM FP 1ADMM FP 3ADMM FP 5ADMM KKT 1ADMM KKT 3ADMM KKT 5ADMM Unroll 1ADMM Unroll 3ADMM Unroll 5OptNet 1OptNet 3OptNet 5Time (s) BackwardForwardTotal3.5x3.0x2.9x4.5x4.4x4.5x0.00.20.40.60.81.0ADMM FP 1ADMM FP 3ADMM FP 5ADMM KKT 1ADMM KKT 3ADMM KKT 5ADMM Unroll 1ADMM Unroll 3ADMM Unroll 5OptNet 1OptNet 3OptNet 5Time (s) BackwardForwardTotal 8.5x 8.8x10.0x 8.6x 8.5x 8.1x02468ADMM FP 1ADMM FP 3ADMM FP 5ADMM KKT 1ADMM KKT 3ADMM KKT 5ADMM Unroll 1ADMM Unroll 3ADMM Unroll 5OptNet 1OptNet 3OptNet 5Time (s) BackwardForwardTotal10.9x12.2x22.4x12.6x12.7x13.4x0102030405060ADMM FP 1ADMM FP 3ADMM FP 5ADMM KKT 1ADMM KKT 3ADMM KKT 5ADMM Unroll 1ADMM Unroll 3ADMM Unroll 5OptNet 1OptNet 3OptNet 5Time (s) BackwardForwardTotal13.7x16.6x16.4x16.3x16.3x16.4x050100150200250ADMM FP 1ADMM FP 3ADMM FP 5ADMM KKT 1ADMM KKT 3ADMM KKT 5ADMM Unroll 1ADMM Unroll 3ADMM Unroll 5OptNet 1OptNet 3OptNet 5Time (s) BackwardForwardTotalThe learning process can be posed as a bi-level optimization program where the objective is to
learn a parameter θ in order to minimize the average QP loss induced by the optimal decision policies
{z∗(θ)(i)}m
i=1. Program (32) is referred to as an integrated predict and optimize (IPO) model as the
prediction model for p is fully integrated with the resulting down-stream decision-based optimization
model.

minimize
θ

subject to

1
m

m
(cid:88)

(cid:16)

i=1

z∗(θ)T (i)

p(i) +

1
2

z∗(θ)T (i)

Q(i) z∗(θ)(i) (cid:17)

z∗(θ)(i) = argmin

− zT ˆp(θ)(i) +

z
A z∗(θ)(i) = b ∀i = 1, ..., m
l ≤ z∗(θ)(i) ≤ u ∀i = 1, ..., m.

1
2

(i)

zT ˆQ

z

∀i = 1, ..., m

(32)

Here p(i) and Q(i) denote the ground truth problem data and are generated as follows. We let
p(i) ∼ N (wT (i) θ0 +τ (cid:15)(i), Q) where Q ∈ Rdz×dz has entry (j, k) equal to ρ|j−k|
. We set ρp = 0.50
and generate the auxiliary feature data from the standard normal distribution, w(i) ∼ N (0, Iw). The
residuals, (cid:15)(i) ∼ N (0, Q), preserve the desired correlation structure and the scalar value τ controls
the signal-to-noise level. All experiments target a signal-to-noise level of 0.10.
We let ˆp(θ)(i) denote the estimate of p(i) according to the linear model:

p

ˆp(θ)(i) = wT (i)

θ .

(33)

The bound constraints, l and u, are generated by randomly sampling from the uniform distribution
with domain [−1, 0] and [0, 1], respectively and we set A = 1 and b = 1. In all experiments we set
the stopping tolerance to (cid:15)tol = 10−3.

We randomly generate problem data of dimension dz ∈ {250, 500, 1000}. The training process
for each trial consists of 30 epochs with a mini-batch size of 32. Figures 2(a) - 4(a) report the
average training loss at each epoch and the 95%-ile conﬁdence interval, evaluated over 10 independent
trials. Observe that the loss curves for the ADMM model and OptNet model are almost identical
at each epoch, suggesting that the training accuracy of the ADMM model and OptNet model are
equivalent. Conversely, Figures 2(b) - 4(b), compare the average and 95%-ile conﬁdence interval time
spent executing the forward and backward pass algorithms. When dz = 250 the ADMM model is
shown to be approximately 5 times faster than the OptNet model. Furthermore, when dz = 500
and dz = 1000, the ADMM model is a full order of magnitude faster than the OptNet model. More
concretely, when dz = 1000 the entire learning process takes approximately 1600 seconds to train the
OptNet model, but less than 130 seconds to train the ADMM model to an equal level of accuracy.

15

(a) Training Loss.

(b) Computational Performance.

Figure 2: Training loss and computational performance for learning p. Batch size = 32 and dz = 250.

(a) Training Loss

(b) Computational Performance

Figure 3: Training loss and computational performance for learning p. Batch size = 32 and dz = 500.

(a) Training Loss

(b) Computational Performance

Figure 4: Training loss and computational performance for learning p. Batch size = 32 and dz = 1000.

16

−0.75−0.50−0.250.000102030EpochNormalized QP Loss ADMMOptNet5.1x1.6x5.6x0102030405060ADMMOptNetTime (s) BackwardForwardTotal−0.75−0.50−0.250.000102030EpochNormalized QP Loss ADMMOptNet 9.0x 1.3x11.0x050100150200250ADMMOptNetTime (s) BackwardForwardTotal−0.75−0.50−0.250.000102030EpochNormalized QP Loss ADMMOptNet12.4x 0.8x16.5x050010001500ADMMOptNetTime (s) BackwardForwardTotal3.3 Experiment 3: learning A

We now present a real-world experiment from portfolio optimization whereby the objective is to learn
a parameterized model for the variable A. We consider an asset universe of dz = 255 liquid US stocks
traded on major U.S. exchanges (NYSE, NASDAQ, AMEX, ARCA). The universe is summarized in
Table 3, with representative stocks from each of the Global Industry Classiﬁcation Standard (GICS)
sectors. Weekly price data is given from January 1990 through December 2020, and is provided by
Quandl.

We denote the matrix of weekly return observations as A = [a(1), a(2), ..., a(m)] ∈ Rm×dz with
m > dz. Let Q(i) ∈ Rdz×dz denote the symmetric positive deﬁnite covariance matrix of asset returns.
We deﬁne the portfolio z(i) ∈ Rdz , where the element, z(i)
j, denotes the proportion of total capital
invested in the jth asset at time i.

We deﬁne the Sharpe ratio at observation i as the ratio of portfolio return to portfolio risk, where

risk is measured by the portfolio volatility (standard deviation).

SR

(i) =

aT (i) z(i)

(cid:113)

zT (i) Q(i) z(i)

(34)

We consider a long-only (z(i) ≥ 0), fully invested (1T z(i) = 1) max-Sharpe portfolio optimization,
presented in Program (35):

maximize
z

aT (i) z

(cid:113)

zT Q(i) z

subject to

1T z = 1,

0 ≤ z ≤ 1

(35)

Observe, however, that the Sharpe ratio is not convex in z but is homogeneous of degree zero. We
follow Cornuejols and Tutuncu [15] and re-cast Program (35) as a convex quadratic optimization
program:

minimize
z

1
2
subject to

zT Q(i) z

aT (i)

z = 1,

z ≥ 0.

(36)

Note that the fully-invested constraint can be enforced by normalizing the optimal weights, z∗.

As before, the learning process can be posed as a bi-level optimization program where the objective
is to learn a parameter θ and the associated constraints, ˆa(θ)(i), in order to maximize the average
realized Sharpe ratio induced by the optimal decision policies {z∗(θ)(i)}m

i=1.

minimize
θ

−

1
m

m
(cid:88)

i=1

(cid:113)

aT (i) z∗(θ)

Q(i) z∗(θ)(i)

z∗(θ)T (i)
1
2

z

subject to

z∗(θ) = argmin

zT Q(i) z

∀i = 1, ..., m

(37)

ˆa(θ)T (i)

z∗(θ)(i) = 1,

1T z∗(θ)(i) = 1,

z∗(θ)(i) ≥ 0 ∀i = 1, ..., m

17

In reality, we do not know the true value of a(i) at decision time and instead we estimate a(i)
through associated auxiliary feature variables w(i) ∈ Rdw . Again we consider a linear model of the
form:

ˆa(i) = θT w(i) .

(38)

In this experiment, asset returns, a(i) are modelled using the well-known Famma-French Five (FF5)
factor model [22], provided by the Kenneth R. French data library.

The goal is to observe the training and out-of-sample performance of the ADMM model in compar-
ison to the OptNet model. As a benchmark, we include the out-of-sample performance of an equally
weighted portfolio, and a max-Sharpe portfolio where θ is ﬁt by ordinary least-squares (OLS). All ex-
periments are trained on data from January 1990 through December 2014. The out-of-sample period
begins in January 2015 and ends in December 2020. Portfolios are formed at the close of each week,
and rebalanced on a weekly basis.

The training process for each trial consists of 500 epochs with a mini-batch size of 32. Portfolio
models are ﬁt to an accuracy of (cid:15)tol = 10−4 in training, and a higher accuracy of (cid:15)tol = 10−6 in
the out-of-sample period in order to guarantee strict adherence to the constraint set. Figure 5(a)
reports the average training loss at each epoch and the 95%-ile conﬁdence interval, evaluated over 10
independent trials. Once again, we observe that the loss curves for the ADMM model and OptNet
model are very similar. Interestingly, we observe that the ADMM model produces a consistently lower
average training loss. Recall that both models use implicit diﬀerentiation to compute the relevant
gradient, which assumes an exact ﬁxed point at each optimal solution z∗(θ)(i).
In practice, each
z∗(θ)(i) is only approximately optimal, to within a tolerance (cid:15)tol, and therefore diﬀerentiating at a
solution that is not an exact ﬁxed point will result in small errors in the gradient that likely explain
the observed diﬀerence. That said, the training loss proﬁle of the ADMM and OptNet models are
very similar, and the ﬁnal models achieve approximately equal loss after 500 epochs. Figure 5(b)
compares the average and 95%-ile conﬁdence interval of the total time spent executing the forward
and backward pass algorithms during training. Once again we observe that the ADMM model is
shown to be approximately 5 times faster than the OptNet model and requires less than 100 seconds
to train.

(a) Training Loss

(b) Computational Performance

Figure 5: Training loss and computational performance for learning A on US stock data.
Batch size = 32 and dz = 255.

18

−0.6−0.4−0.20.00100200300400500EpochSharpe Ratio LossADMMOptNet5.3x0.7x7.7x0100200300400500ADMMOptNetTime (s) BackwardForwardTotalFigure 6 reports the out-of-sample equity growth of the ADMM IPO max-Sharpe portfolio, Equal
Weight portfolio, OLS max-Sharpe portfolio and OptNet IPO max-Sharpe portfolio. The out-of-
sample economic performance metrics are reported in Table 1. First, observe that all max-Sharpe
models outperform the Equal Weight benchmark on an absolute and risk-adjusted basis. Further-
more, the ADMM and OptNet IPO max-Sharpe models achieve an out-of-sample Sharpe ratio that
is approximately 50% higher than that of the naive ‘predict, then optimize’ OLS max-Sharpe model,
thus highlighting the beneﬁt of training a fully integrated system. Lastly, the ADMM model achieves
a marginally higher out-of-sample Sharpe ratio in comparison to the OptNet model, though the dif-
ference is not statistically signiﬁcant.

Figure 6: Out-of-sample equity growth for ADMM IPO max-Sharpe portfolio, Equal Weight portfolio,
OLS max-Sharpe portfolio and OptNet IPO max-Sharpe portfolio.

Mean
Volatility
Sharpe Ratio

ADMM Equal Weight OLS
0.2382
0.1777
1.3407

0.0950
0.1950
0.4872

0.2122
0.2435
0.8747

OptNet
0.2413
0.1880
1.2836

Table 1: Out-of-sample economic performance metrics for ADMM IPO max-Sharpe portfolio, Equal
Weight portfolio, OLS max-Sharpe portfolio and OptNet IPO max-Sharpe portfolio.

3.4 Experiment 4: learning Q

We consider another real-world experiment from portfolio optimization whereby the objective is to
learn a parameterized model for the variable Q. We use the same asset universe of dz = 255 liquid
US stocks from Experiment 3 and an identical experimental design. Here, we consider the long-only,
fully-invested minimum variance portfolio optimization, described in Program (39).

1
minimize
2
z
subject to

zT Q(i) z

1T z = 1,

0 ≤ z ≤ 1.

(39)

19

0.00.51.01.52015201620172018201920202021Out−of−Sample EquityADMMEqual WeightOLSOptNetAs before, the learning process is posed as a bi-level optimization program where the objective is to
learn a parameter θ and the associated covariance matrix, ˆQ(θ)
, in order to minimize the average
realized variance induced by the optimal decision policies {z∗(θ)(i)}m
i=1.

(i)

minimize
θ

1
m

m
(cid:88)

i=1

z∗(θ)T (i)

Q(i) z∗(θ)(i)

subject to

z∗(θ) = argmin

z

1
2

zT ˆQ(θ)

(i)

z

∀i = 1, ..., m

1T z∗(θ)(i) = 1,

0 ≤ z∗(θ)(i) ≤ 1 ∀i = 1, ..., m

(40)

Asset returns, a(i) are modelled using the Famma-French Five (FF5) factor model. We follow
Butler and Kwon [13] and model w(i) according to a multivariate GARCH(1, 1) process with constant
correlation. We let ˆW
denote the the time-varying covariance estimate of the auxiliary feature
variables. We therefore model the stock covariance matrix as follows:

(i)

ˆa(i) = θT w(i),

(i)

ˆQ

= θT ˆW

(i)

θ + ˆF,

(41)

where ˆF denotes the diagonal matrix of residual variances.

Again, the goal is to observe the training and out-of-sample performance of the ADMM model
in comparison to the OptNet model. As a benchmark, we include the out-of-sample performance of
the equal weight portfolio, and a minimum variance portfolio where θ is ﬁt by OLS. The training
process for each trial consists of 200 epochs with a mini-batch size of 32. Portfolio models are ﬁt to an
accuracy of (cid:15)tol = 10−4 in training, and a higher accuracy of (cid:15)tol = 10−6 in the out-of-sample period.
Figure 7(a) reports the average training loss at each epoch and the 95%-ile conﬁdence interval,
evaluated over 10 independent trials. We observe that the loss curves for the ADMM model and
OptNet model are very similar, thus highlighting the accuracy of the ADMM-layer. Once again we
observe that the ADMM model produces a consistently lower average training loss and we refer to the
discussion in Experiment 3 for a likely explanation. Figure 7(b) compares the average and 95%-ile
conﬁdence interval of the total time spent executing the forward and backward pass algorithms in
training. As before, we observe that the ADMM model requires approximately 60 seconds to train
and is approximately 5 times faster than the OptNet model, which requires over 300 seconds.

20

(a) Training Loss

(b) Computational Performance

Figure 7: Training loss and computational performance for learning Q on US stock data.
Batch size = 32 and dz = 255.

Finally, Figure 8 reports the out-of-sample equity growth of the ADMM IPO minimum variance
portfolio, Equal Weight portfolio, OLS minimum variance portfolio and OptNet IPO minimum vari-
ance portfolio. The out-of-sample economic performance metrics are reported in Table 2. Again,
observe that all minimum-variance models outperform the Equal Weight benchmark on an absolute
and risk-adjusted basis. Furthermore, the ADMM and OptNet IPO minimum variance models achieve
an out-of-sample volatility that is approximately 25% lower and Sharpe ratio that is approximately
35% higher than that of the naive ‘predict, then optimize’ OLS minimum variance model. These
results are broadly consistent with the ﬁndings in Butler and Kwon [13], who consider an identical
stock universe but with considerably smaller portfolios (dz ≤ 100). Our ADMM model, on the other
hand, is able to overcome the computational challenges in the medium to large scale limit (described
in Butler and Kwon [13]), without any apparent loss in performance accuracy.

Figure 8: Out-of-sample equity growth for ADMM IPO minimum variance portfolio, Equal Weight
portfolio, OLS minimum variance portfolio and OptNet IPO minimum variance portfolio.

21

2e−043e−044e−045e−046e−0450100150200EpochVariance LossADMMOptNet5.2x1.4x6.2x050100150200250300350ADMMOptNetTime (s) BackwardForwardTotal0.00.20.40.60.82015201620172018201920202021Out−of−Sample EquityADMMEqual WeightOLSOptNetMean
Volatility
Sharpe Ratio

ADMM Equal Weight OLS
0.1058
0.1420
0.7446

0.0950
0.1950
0.4872

0.0984
0.1786
0.5510

OptNet
0.1070
0.1443
0.7418

Table 2: Out-of-sample economic performance metrics for ADMM IPO minimum variance portfo-
lio, Equal Weight portfolio, OLS minimum variance portfolio and OptNet IPO minimum variance
portfolio.

4 Conclusion and future work

In this paper, we provide a novel and eﬃcient framework for diﬀerentiable constrained quadratic
programming. Our diﬀerentiable quadratic programming layer is built on top of the ADMM algorithm,
which for medium to large scale problems is shown to be approximately an order of magnitude more
eﬃcient than the interior point implementation of the OptNet layer. The backward-pass algorithm
computes the relevant problem variable gradients by implicit diﬀerentiation of a modiﬁed ﬁxed-point
iteration, which is computationally favorable to KKT implicit diﬀerentiation and memory eﬃcient
in comparison to standard unrolled diﬀerentiation. Numerical results, using both simulated and real
problem data, demonstrates the eﬃcacy of the ADMM layer, which for medium to large scale problems
exhibits state-of-the art accuracy and improved computational performance.

Our experimental results should be interpreted as a proof-of-concept and we acknowledge that
further testing on alternative data sets or with diﬀerent problem variable assumptions is required in
order to better determine the eﬃcacy of the ADMM layer as a general purpose solver. Indeed, there is a
plethora of areas for active research and algorithm improvement. First, our ADMM layer currently only
supports linear equality and box inequality constraints, whereas the OptNet layer supports general
linear inequality constraints.
Indeed, incorporating more general inequality constraints as well as
augmenting the QP with parameterized regularization norms is an active area of research. Secondly,
as discussed earlier, the ADMM algorithm is known to be vulnerable to ill-conditioned problems and
the resulting convergence rates can vary signiﬁcantly when the data and algorithm parameter, ρ, are
poorly scaled. To overcome this, most ﬁrst-order solvers implement a preconditioning and scaling
initialization step. Currently, our ADMM-layer implementation does not support preconditioning and
scaling, which is challenged by virtue of the fact that the problem data is expected to change at each
epoch of training. Instead, we leave it to the user to select ρ and manually scale the problem data and
acknowledge that preconditioning, scaling and automatic parameter selection is an important area of
future development.

Furthermore, there are several heuristic methods that could be implemented in order to improve the
convergence rates and computational eﬃciency of our ADMM forward-pass. For example, acceleration
methods, such as Andersen acceleration [5], have recently been shown to improve the convergence rates
of ﬁrst-order solvers [39, 44]. Indeed, applying acceleration methods to the modiﬁed ﬁxed-point algo-
rithm presented in this paper provides an numerically eﬃcient scheme for potentially improving the
convergence rate of the ADMM algorithm and is an interesting area of future research. Alternatively,
methods that hybridize the eﬃciency of ﬁrst-order methods with the precision of interior-point meth-
ods, such solution polishing and reﬁnement [12], is another area of future exploration. Nonetheless,

22

our proposed ADMM layer is shown to be highly eﬀective and in its current form can be instrumental
for eﬃciently solving real-world medium and large scale learning problems.

References

[1] Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J. Zico
Kolter. Diﬀerentiable convex optimization layers. In Advances in Neural Information Processing
Systems, volume 32, pages 9562–9574. Curran Associates, Inc., 2019.

[2] Akshay Agrawal, Shane Barratt, Stephen Boyd, Enzo Busseti, and Walaa M. Moursi. Diﬀerenti-

ating through a cone program, 2020. URL http://arxiv.org/abs/1703.00443.

[3] Brandon Amos and J. Zico Kolter. Optnet: Diﬀerentiable optimization as a layer in neural

networks. CoRR, abs/1703.00443, 2017. URL http://arxiv.org/abs/1703.00443.

[4] Brandon Amos, Ivan Dario Jimenez Rodriguez, Jacob Sacks, Byron Boots, and J. Zico Kolter.

Diﬀerentiable mpc for end-to-end planning and control, 2019.

[5] Donald G. M. Anderson.

Iterative procedures for nonlinear integral equations. J. ACM, 12:

547–560, 1965.

[6] David Belanger, Bishan Yang, and Andrew McCallum. End-to-end learning for structured pre-

diction energy networks, 2017.

[7] Dimitris Bertsimas and Nathan Kallus. From predictive to prescriptive analytics. Management

Science, 66(3):1025–1044, 2020.

[8] F. Black and R. Litterman. Asset allocation combining investor views with market equilibrium.

Journal of Fixed Income, 1(2):7–18, 1991.

[9] Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer, Felipe Llinares-
Lopez, Fabian Pedregosa, and Jean-Philippe Vert. Eﬃcient and modular implicit diﬀerentiation,
2021.

[10] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press,

2004.

[11] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed opti-
mization and statistical learning via the alternating direction method of multipliers. Foundations
and Trends in Machine Learning, 3:1–122, 01 2011. doi: 10.1561/2200000016.

[12] E. Busseti, W. Moursi, and S. Boyd. Solution reﬁnement at regular points of conic problems,

2018.

[13] Andrew Butler and Roy Kwon. Covariance estimation for risk-based portfolio optimization: an

integrated approach. Social Science Research Network, 1(1), 2021.

23

[14] Andrew Butler and Roy H. Kwon. Integrating prediction in mean-variance portfolio optimization.

arXiv 2102.09287, pages 1–33, 2021.

[15] Gerard Cornuejols and Reha Tutuncu. Optimization Methods in Finance. Cambridge University

Press, 2006.

[16] Steven Diamond, Vincent Sitzmann, Felix Heide, and Gordon Wetzstein. Unrolled optimization

with deep priors, 2018.

[17] Justin Domke. Generic methods for optimization-based modeling. In AISTATS, 2012.

[18] Asen Dontchev and R Rockafellar.

Implicit Functions and Solution Mappings: A View from

Variational Analysis. 01 2009. ISBN 978-0-387-87820-1. doi: 10.1007/978-0-387-87821-8.

[19] Priya L. Donti, Brandon Amos, and J. Zico Kolter. Task-based End-to-end Model Learning.

CoRR, abs/1703.04529, 2017. URL http://arxiv.org/abs/1703.04529.

[20] Adam N. Elmachtoub and Paul Grigas. Smart predict, then optimize. arXiv, 2020.

[21] Adam N. Elmachtoub, Jason Cheuk Nam Liang, and Ryan McNellis. Decision trees for decision-

making under the predict-then-optimize framework, 2020.

[22] Eugene F. Fama and Kenneth R. French. A ﬁve-factor asset pricing model. Journal of Financial
Economics, 116(1):1 – 22, 2015. ISSN 0304-405X. doi: https://doi.org/10.1016/j.jﬁneco.2014.10.
010. URL http://www.sciencedirect.com/science/article/pii/S0304405X14002323.

[23] Jean Feng and Noah Simon. Gradient-based regularization parameter selection for problems with

non-smooth penalty functions, 2017.

[24] Daniel Gabay and Bertrand Mercier. A dual algorithm for the solution of nonlinear variational
problems via ﬁnite element approximation. Computers & Mathematics With Applications, 2:
17–40, 1976.

[25] R. Ganti and Alexander G. Gray. Cake: Convex adaptive kernel density estimation. In AISTATS,

2011.

[26] Roland Glowinski and A. Marroco. Sur l’approximation, par ´el´ements ﬁnis d’ordre un, et la
r´esolution, par p´enalisation-dualit´e d’une classe de probl`emes de dirichlet non lin´eaires. 1975.

[27] D. Goldfarb and Shucheng Liu. An o(n3l) primal interior point algorithm for convex quadratic

programming. Mathematical Programming, 49:325–340, 1991.

[28] Paul Grigas, Meng Qi, Zuo-Jun, and Shen. Integrated conditional estimation-optimization, 2021.

[29] Michael Ho, Zheng Sun, and Jack Xin. Weighted elastic net penalized mean-variance portfolio
design and computation. SIAM Journal on Financial Mathematics, 6(1):1220–1244, 2015.

[30] Seung-Jean Kim, K. Koh, M. Lustig, Stephen Boyd, and Dimitry Gorinevsky. An interior-point
method for large-scale l1-regularized least squares. Selected Topics in Signal Processing, IEEE
Journal of, 1:606 – 617, 01 2008. doi: 10.1109/JSTSP.2007.910971.

24

[31] Jonathan Lorraine and David Duvenaud. Stochastic hyperparameter optimization through hy-

pernetworks, 2018.

[32] Jayanta Mandi and Tias Guns. Interior point solving for lp-based prediction+optimisation, 2020.

[33] Jaynta Mandi, Emir Demirovic, Peter. J Stuckey, and Tias Guns. Smart predict-and-optimize

for hard combinatorial optimization problems, 2019.

[34] H. Markowitz. Portfolio selection. Journal of Finance, 7(1):77–91, 1952.

[35] Richard Michaud and Robert Michaud. Estimation error and portfolio optimization: A resampling

solution. Journal of Investment Management, 6(1):8–28, 2008.

[36] Brendan O’Donoghue, Eric Chu, Neal Parikh, and Stephen Boyd. Conic optimization via operator

splitting and homogeneous self-dual embedding, 2016.

[37] David E. Rumelhart, Geoﬀrey E. Hinton, and Ronald J. Williams. Learning representations by

back-propagating errors. Nature, 323:533–536, 1986.

[38] Michel Schubiger, Goran Banjac, and John Lygeros. Gpu acceleration of admm for large-scale
quadratic programming. Journal of Parallel and Distributed Computing, 144:55–67, 2020. ISSN
0743-7315. doi: https://doi.org/10.1016/j.jpdc.2020.05.021. URL https://www.sciencedirect.
com/science/article/pii/S0743731520303063.

[39] Pantelis Sopasakis, Krina Menounou, and Panagiotis Patrinos. Superscs: fast and accurate large-

scale conic optimization. pages 1500–1505, 06 2019. doi: 10.23919/ECC.2019.8796286.

[40] Bartolomeo Stellato, Goran Banjac, Paul Goulart, Alberto Bemporad, and Stephen Boyd. Osqp:
an operator splitting solver for quadratic programs. Mathematical Programming Computation,
12(4):637?672, Feb 2020. ISSN 1867-2957. doi: 10.1007/s12532-020-00179-2. URL http://dx.
doi.org/10.1007/s12532-020-00179-2.

[41] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statis-

tical Society, 58(1), 1996.

[42] Andrey Tikhonov. Solution of incorrectly formulated problems and the regularization method.

Soviet Mathematics, (4):1035–1038, 1963.

[43] Ayse Sinem Uysal, Xiaoyue Li, and John M. Mulvey. End-to-end risk budgeting portfolio opti-

mization with neural networks, 2021.

[44] Homer Walker and Peng Ni. Anderson acceleration for ﬁxed-point iterations. SIAM J. Numerical

Analysis, 49:1715–1735, 08 2011. doi: 10.2307/23074353.

[45] Xingyu Xie, Jianlong Wu, Zhisheng Zhong, Guangcan Liu, and Zhouchen Lin. Diﬀerentiable

linearized admm, 2019.

[46] Yan Yang, Jian Sun, Huibin Li, and Zongben Xu. Admm-net: A deep learning approach for

compressive sensing mri, 2017.

25

A Proof of Proposition 1

We deﬁne vk = xk+1 + µk. We can therefore express Equation (14b) as:

zk+1 = Π(xk+1 +µk) = Π(vk),

and Equation (14c) as:

µk+1 = µk + xk+1 − zk+1 = vk −Π(vk).

Substituting Equations (42) and (43) into Equation (14b) gives the desired ﬁxed-point iteration:

(cid:21)

(cid:20)vk+1
ηk+2

(cid:20)xk+2 + µk+1
ηk+2

(cid:21)

=

= −

= −

A

(cid:20)Q +ρ Iv AT
0
(cid:20)Q +ρ Iv AT
0

A

(cid:21)
(cid:21)−1 (cid:20)p −ρ(zk+1 − µk+1)

− b

(cid:21)
(cid:21)−1 (cid:20)p −ρ(2Π(vk) − vk)

− b

+

+

(cid:20)µk+1
0
(cid:20) vk
ηk+1

(cid:21)

(cid:21)

−

(cid:21)

(cid:20)Π(vk)
ηk+1

.

B Proof of Proposition 2

We deﬁne F : Rdv × Rdη → Rdv × Rdη as:

F (v, η) = −

(cid:20)Q +ρ Iv AT
0

A

(cid:21)
(cid:21)−1 (cid:20)p −ρ(2Π(v) − v)

− b

(cid:21)

(cid:20)v
η

+

(cid:21)

(cid:20)Π(v)
η

,

−

and let

Therefore we have

M =

(cid:20)Q +ρ Iv AT
0

A

(cid:21)

.

M F (v, η) = −

(cid:20)p −ρ(2Π(v) − v)
− b

(cid:21)

+ M

(cid:21)

(cid:20)v
η

− M

(cid:21)

(cid:20)Π(v)
η

.

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

Taking the partial diﬀerentials of Equation (50) with respect to the relevant problem variables therefore
gives:

M ∂F (v, η) = −

= −

= −

= −

(cid:21)

(cid:21)

(cid:20) ∂ p
−∂ b
(cid:20) ∂ p
−∂ b
(cid:20) ∂ p
−∂ b
(cid:20)∂ p + 1

(cid:21)

+ ∂ M

(cid:21)

(cid:20)v
η

(cid:34)

− ∂ M

− M−1

− ∂ M

(cid:21)

(cid:20)x∗
η∗

− ∂ M

(cid:21)

(cid:20)Π(v)
η

− ∂ M F (v, η)

(cid:20)p −ρ(2Π(v) − v)
− b

(cid:21) (cid:35)

(50)

2 (∂ Q x∗ +∂ QT x∗) + ∂ AT η∗

−∂ b +∂ A x∗

(cid:21)

.

26

From Equation (50) we have that the diﬀerential ∂F (v, η) is given by:

∂F (v, η) = − M−1

(cid:20)∂ p + 1

2 (∂ Q x∗ +∂ QT x∗) + ∂ AT η∗

−∂ b +∂ A x∗

(cid:21)

.

(51)

Substituting the gradient action of Equation (51) into Equation (26) and taking the left matrix-vector
product of the transposed Jacobian with the previous backward-pass gradient, ∂(cid:96)
∂ z∗ , gives the desired
result.

(cid:21)

(cid:20)ˆdx
ˆdη

(cid:20)Q +ρ Iv AT
0

A

=

(cid:21)−1 (cid:104)

I˜v −∇˜vF (˜v(θ), θ)

(cid:105)−T (cid:20)DΠ(v)

0

(cid:21) (cid:34)(cid:0) − ∂(cid:96)
∂ z∗
0

0
Iη

(cid:35)

(cid:1)T

.

From Equation (24) we have:

I˜v −∇˜vF (˜v(θ), θ) =

(cid:20)Q +ρ Iv AT
0

A

(cid:21)−1 (cid:20)−ρ(2DΠ(v) − Iv) 0
0
0

(cid:21)

(cid:20)DΠ(v)
0

+

(cid:21)

.

0
Iη

(52)

(53)

Simplifying Equation (52) with Equation (53) yields the ﬁnal expression:

(cid:21)

(cid:20)ˆdx
ˆdη

=

(cid:34) (cid:20)DΠ(v)
0

(cid:21) (cid:20)Q +ρ Iv AT
0
A

0
Iη

(cid:21)

(cid:20)−ρ(2DΠ(v) − Iv) 0
0
0

+

(cid:21) (cid:35)−1 (cid:20)DΠ(v)

0

(cid:21) (cid:34)(cid:0) − ∂(cid:96)
∂ z∗
0

0
Iη

(cid:35)

(cid:1)T

.

C Proof of Proposition 3

From the KKT system of equations (20) we have:

GT diag(˜λ

∗

)ˆdλ = diag(ρ µ∗)ˆdλ =

(cid:16)

−

(cid:16) ∂(cid:96)
∂ z∗

(cid:17)T

− Q ˆdx − AT ˆdη

(cid:17)

.

From Equation (21) it follows that:

∂(cid:96)
∂ l

= 0 ⇐⇒ λ∗

− > 0

and

∂(cid:96)
∂ u

= 0 ⇐⇒ λ∗

+ > 0,

(54)

(55)

(56)

and therefore Equation (55) uniquely determines the relevant non-zero gradients. Let ˜µ∗ be as deﬁned
by Equation (29), then it follows that:

ˆdλ = diag(ρ ˜µ∗)−1(cid:16)

−

(cid:17)T

(cid:16) ∂(cid:96)
∂ z∗

− Q ˆdx − AT ˆdη

(cid:17)

.

(57)

Substituting ˆdλ into Equation (21) gives the desired gradients.

27

D Data Summary

GICS Sector
Communication
Services

Consumer
Discretionary

Consumer
Staples

Energy

Financials

Health
Care

Industrials

Information
Technology

Materials

Real
Estate

Utilities

CBB
T

BBY
HD
MCD
TJX

ADM
CPB
MO
WBA

AE
HES

AFG
BK
PGR
USB

ABMD
CI
MRK
WST

ABM
CSL
GD
LMT
RPH
UNP

APD
IFF
SHW

ALX
WY

AEP
ED
NI
SO

CMCSA
VOD

DIS
VZ

Stock Symbols
FOX

IPG

LUMN MDP

NYT

CBRL
HOG
NKE
VFC

ALCO
FLO
PEP
WMK

APA
MRO

AFL
BXS
PNC
WFC

ABT
COO
OMI

AIR
CSX
GE
LUV
PNR

AVY
IP
SON

CCL
HRB
NVR
WHR

CAG
GIS
PG
WMT

BKR
OKE

AIG
C
RJF
WRB

AMGN
CVS
PFE

ALK
DE
GWW
MAS
ROK

ADI
INTC
TXN

BLL
MOS
VMC

F
JWN
NWL
WWW

CASY
HSY
SYY

BP
OXY

AJG
GL
SCHW
WTM

BAX
DHR
PKI

AME
DOV
HON
MMM
ROL

ADP
MSFT
TYL

CCK
NEM

GPC
LB
PHM

CHD
K
TAP

COP
SLB

AON
JPM
STT

BDX
HUM
SYK

AOS
EFX
IEX
NOC
RTX

GPS
LEG
PVH

GT
LEN
ROST

HAS
LOW
TGT

CL
KMB
TR

CLX
KO
TSN

COST
KR
UVV

CVX
EOG
VLO WMB

HAL
XOM

AXP
L

BAC
LNC
TROW TRV

BEN
MMC
UNM

BIO
JNJ
TFX

BA
EMR
ITW
NPK
SNA

BMY
LLY
TMO

CAH
MDT
VTRS

CAT
ETN
JCI
NSC
SWK

CMI
FDX
KSU
PCA
TXT

ADSK AMAT

MSI
WDC

CRS
NUE

MU
XRX

ECL
OLN

AMD
ORCL

GLW
ROG

FMC
PPG

GLT
SEE

FRT

GTY

HST

PEAK

PSA

VNO WRI

ATO
EIX
NJR
SWX

BKH
ETR
OGE
UGI

CMS
EVRG
PEG
WEC

CNP
EXC
PNM
XEL

D
LNT
PNW

DTE
NEE
PPL

DUK
NFG
SJW

AAPL
HPQ
SWKS

ADBE
IBM
TER

Table 3: U.S. stock data, sorted by GICS Sector. Data provided by Quandl.

28

