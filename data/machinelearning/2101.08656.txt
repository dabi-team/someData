1
2
0
2

n
a
J

0
2

]

G
L
.
s
c
[

1
v
6
5
6
8
0
.
1
0
1
2
:
v
i
X
r
a

Dive into Decision Trees and Forests: A
Theoretical Demonstration

Zhang Jinxiong

ID

jinxiongzhang@qq.com

Abstract

Based on decision trees, many ﬁelds have arguably made tremendous
progress in recent years. In simple words, decision trees use the strategy
of “divide-and-conquer” to divide the complex problem on the dependency
between input features and labels into smaller ones. While decision trees
have a long history, recent advances have greatly improved their perfor-
mance in computational advertising, recommender system, information
retrieval, etc.

We introduce common tree-based models (e.g., Bayesian CART, Bayesian

regression splines) and training techniques (e.g., mixed integer program-
ming, alternating optimization, gradient descent). Along the way, we
highlight probabilistic characteristics of tree-based models and explain
their practical and theoretical beneﬁts. Except machine learning and data
mining, we try to show theoretical advances on tree-based models from
other ﬁelds such as statistics and operation research. We list the repro-
ducible resource at the end of each method.

Keywords— Decision trees, decision forests, Bayesian trees, soft trees, dif-

ferentiable trees

Contents

1 Introduction

2 Decision trees

2.1 Algorithmic construction for decision trees . . . . . . . . . . . . .
2.2 Representation of decision tree . . . . . . . . . . . . . . . . . . .

3 Additive decision trees

3.1 Decision forest
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Boosted decision trees . . . . . . . . . . . . . . . . . . . . . . . .

4 Probabilistic decision trees and forest

4.1 Hierarchical mixtures of experts . . . . . . . . . . . . . . . . . . .
4.2 Mondrian trees and forests . . . . . . . . . . . . . . . . . . . . . .

1

2

4
4
5

7
7
8

9
10
13

 
 
 
 
 
 
5 Bayesian decision trees

5.1 Bayesian trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Bayesian additive regression trees . . . . . . . . . . . . . . . . . .
5.3 Bayesian MARS . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Optimal decision trees

6.1 Optimal classiﬁcation trees
. . . . . . . . . . . . . . . . . . . . .
6.2 Tree alternating optimization . . . . . . . . . . . . . . . . . . . .
6.3 Diﬀerentiable trees . . . . . . . . . . . . . . . . . . . . . . . . . .

7 Neural decision trees

7.1 Adaptive neural trees . . . . . . . . . . . . . . . . . . . . . . . . .
7.2 Tree ensemble layer . . . . . . . . . . . . . . . . . . . . . . . . . .
7.3 Neural-backed decision trees . . . . . . . . . . . . . . . . . . . . .

8 Regularized decision trees and forests

8.1 Regularized decision trees . . . . . . . . . . . . . . . . . . . . . .
8.2 Regularized additive trees . . . . . . . . . . . . . . . . . . . . . .

9 Conditional computation

9.1 Unbiased recursive partitioning . . . . . . . . . . . . . . . . . . .
9.2 Bonzai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10 Discussion

1

Introduction

15
15
17
18

19
19
23
26

30
30
31
32

33
34
35

35
36
37

37

Supervised learning is to ﬁnd optimal prediction when some observed samples
are given. It is based on the belief that we can imply the global relationship of
where n < ∞. In another
(x, y) based on a ﬁnite partial samples {(xi, yi)}n
word, it is to learn a function f : X → Y from the data set {(xi, yi) | xi ∈ X , y ∈
. Usually X is high dimensional and it is hardly to ﬁnd joint distribution
Y}n
of (x, y) ∈ X × Y. Unsupervised learning is to ﬁnd some inherent structure of
the data. These learning categories cover most tasks in machine learning.

i=1

i=1

Decision tree is a typical example of ‘divide-and-conquer’ strategy and widely
used in many ﬁelds as shown in [43, 88, 55, 90]. Based on decision trees, there
are improvements in diverse ﬁelds such as computational advertising[39], rec-
ommender system[61], information retrieval[65, 46, 25]. It is demonstrated the
eﬀectiveness of recursive partitioning specially for the tasks in health sciences
in [88]. In [92], it is to solve image recognition task based on the ensemble of
decision trees. Although decision trees are usually applied to regression and clas-
siﬁcation tasks, it is available to apply tree-based methods to unsupervised tasks
such as in [9, 23, 59, 76]. There is a cruated list of decision tree research papers
in https://github.com/benedekrozemberczki/awesome-decision-tree-papers.

There is a chronological literature review in [22], which summarizes the im-
portant work of decision trees in last century. In [23] applies tree-based methods

2

to diverse tasks such as classiﬁcation, regression, density estimation and man-
ifold learning. The [43] covers many aspects of very important technique in
decision tree from the data mining viewpoint. Here we will focus on the recent
advances on this ﬁelds.

It is usually represented in a tree structure visually.

Figure 1: A toy example of decision tree

Conventionally, visual representation of tree is in the top-to-down order as
shown in (1). And decision trees share many terms with tree data structure.
For example, the topmost node is called root node; if the node a is directly
under the node b, we say the node a is a child of the node b and the b is the
parent of node a; the nodes without any children are called leaves or terminal
nodes.

The decision tree is formally expressed as the simple function

T (x) =

J
(cid:88)

j

γjI(x ∈ Rj)

(1)

. The number of leaf nodes J is usually treated as
with parameters {γj, Rj}J
is a partitioning of the domain
a hyper-parameter. And the set family {Rj}J
of x. Each set Rj is parameterized with θj. The indicator function I(x ∈ Ri) is
the characteristic function of the set Ri,deﬁned as

j=1

j=1

I(x ∈ Ri) =

(cid:40)

if x ∈ Ri;
1,
0, otherwise.

Random forest is the sum of decision trees with some random factors. The
trees in random forest not only draw some samples from the train set but also

3

take a part of the sample features during tree induction. Boosting decision trees
method is to add up some well-designed decision trees with appreciate weights

N
(cid:88)

n

wnTn(x).

Some eﬃcient implementation of this boosting are open and free such as [17,
71, 53]. The weight wn depends on the error of Tn−1 for n ≥ 2 thus the trees
in such methods are trained in a sequential way.

Both above methods are additive trees as application of ensemble methods
to decision trees. With the help of Bayesian methods, we can invent the ‘multi-
plicative’ trees in some sense. And not only one term of the sum in T (x) can be
nonzero (associated with the leaf in which x falls at random) in Bayesian trees.
We will revisit methods of overcoming the deﬁciencies of classic decision trees
under the probabilistic len.

Additionally, we will revisit the relations of tree-baed models, Bayesian hier-
archical models and deep neural networks. And from a numerical optimization
perspective, we will review non-greedy optimization methods for decision trees.
We would review the following the topics to understand the properties of

decision trees:

1. Decision trees: basic introduction to decision trees and related ﬁelds.

2. Additive decision trees: linear aggregation of decision trees.

3. Probabilistic decision trees: probabilistic thoughts and methods in deci-

sion trees.

4. Bayesian decision trees: Bayesian ideas in tree-based models.

5. Optimal decision trees: non-greedy induction methods of decision trees.

6. Neural decision trees: the combination of decision trees and deep neural

networks.

7. Regularized decision trees: regularization in tree-based models.

8. Conditional computation: extension of tree-based models.

2 Decision trees

2.1 Algorithmic construction for decision trees

Decision tree is generally regarded as a stepwise procedure, consisting of two
phases- the induction or growth phase and the pruning phase. Both phases are
of the ‘if-then’ sentences so it is possible to deal with the mixed types of data.
During the induction or growth of decision tree, it is ﬁrst to recursively
partition the domain of input space. In another word, it is to ﬁnd the subset

4

Rj for j = 1, 2, · · · , J in (1). The second step is to determine the local optimal
model at each subset. In another word, it is to ﬁnd the best γj according to
some criteria in (1). Majority of the variants of decision tree is to improve the
ﬁrst phase. We quote the pseudocode for tree construction in [63].

Algorithm 1 Pseudocode for tree construction by exhaustive search
1: Start at the root node
2: For each X, ﬁnd the set S that minimizes the sum of the node impurities in
the two child nodes and choose the split X ∗ ∈ S∗ that gives the minimum
overall X and S.

3: If a stopping criterion is reached, exit. Otherwise, apply step 2 to each child

node in turn.

Diﬀerent node impurities and stopping criterion lead to diﬀerent tree con-
struction methods. Here we would not introduce the split creteria such as
entropy or the stopping criterion.

To avoid the over-ﬁtting and improve the generalization performance, prun-
ing is to decrease the size of the decision tree. In [33], there is the diﬀerences
between the backwards variable selection and pruning.

Decision tree is an adaptive computation method for function approximation.
However the framework[63] does not minimize the cost directly via numerical
optimization methods such as gradient descent because it only minimizes the
sum of the node impurities at each split. Mixed integer programming is used to
ﬁnd the optimal decision trees such as [5, 4, 80, 67, 3], which is discussed later.
We pay attention into the modiﬁcation of this framework and the connection

between decision trees and other models.

2.2 Representation of decision tree

Here we focus on the representation of decision tree. In another word, we pay
attention to inference of a trained decision tree rather than the traning of a
decision tree given some dataset.

Decision trees are named because of its graphical representation where every
input travels from the top (as known as root) to the terminal node (as known
as leaf). It is the recursive ‘divide-and-conquer’ nature which makes it diﬀerent
from other supervised learning methods.
It is implemented as a list of ‘IF-
THEN’ clauses.

In [33], the recursive partitioning regression, as binary regression tree, is

viewed in a more conventional light as a stepwise regression procedure:

fM (x) =

M
(cid:88)

Km(cid:89)

am

m=1

k=1

H[skm(xv(k,m)−tkm )]

(2)

where H(·) is the unit step function. The quantity Km, is the number of splits
that gave rise to basis function. The quantities skm take on values k1and in-
dicate the (right/left) sense of the associated step function. The v(k, m) la-

5

bel the predictor variables and the tkm, represent values on the corresponding
variables. The internal nodes of the binary tree represent the step functions
and the terminal nodes represent the ﬁnal basis functions.
It is hierarchical
model using a set of basis functions and stepwise selection. The product term
(cid:81)Km
k=1 H[skm(xv(k,m)−tkm )] is an explicit form of the indicator function I in (1).
Yosshua Bengio, Olivier Delalleau, and Clarence Simard in [2] deﬁne the

decision tree T : Rd → R as an additive model as below:

T (x) =

(cid:88)

gi(x)Ix∈Ri =

(cid:88)

gi(x)

(cid:89)

ISa(x)=ca,i

(3)

i∈leaves

i∈leaves

a∈ancestors(i)

where Ri ⊂ Rd is the region associated with leaf i of the tree, ancestors(i) is the
set of ancestors of leaf node i, ca,i is the child of node a on the path from a to
leaf i, and Sa is the n-ary split function at node a. The indicator function Ix∈Ri
is equal to 1 if x belongs to Ri otherwise it is equal to 0. gi(·) is the prediction
function associated with leaf i and is learned according to the training samples
in Ri.

The prediction function gi(x) is always restricted in a speciﬁc function family
such as polynomials [15]. Usually, gi is constant when we call (3) classical
decision trees. For example, decision tree in XGBoost[17] is expressed as

T (x) = wq(x), w ∈ RT , q : Rd → {1, 2, · · · , T }

(4)

where w is the vector of scores on leaves, wi is the score on leaf i for ∀i ∈
{1, 2, · · · , L}; q is a function assigning each data point to the corresponding
leaf, and T is the number of leaves.

In [54], each tree is transformed into a well-known polynomial form. It is to

express the decision tree in algebraic form

h(x) =

(cid:88)

w(cid:96)I{x ∈ (cid:96)}

(cid:96)∈leaves

(5)

where the indicator function is a product of indicators induced by splits along
the path from root to the terminal node:

I{x ∈ (cid:96)} =

(cid:89)

c(x)

(cid:89)

(1 − c(x)).

c∈right splits

c∈left splits

The key components of decision trees are (1) the split functions to parti-
tion the sample space at each leaf such as the split function Sa in (3); (2) the
prediction of each leaf or terminal node such as the decision function gi in (3).
Usually each split function is univariate thus decision tree is inherently sparse.
Note that exactly one term of the sum in T (x) (3) can be nonzero (associated
with the leaf in which x falls). In the term of mathematics, decision tree is ex-
actly the simple function when gi(x) is constant forall i. The training methods
are to ﬁnd proper gi(x) and Ri. Diverse tree-based methods are to modify the
training methods as shown in [64, 63, 43, 88].

6

3 Additive decision trees

Additive decision trees as well as its variant is to overcome the following funda-
mental limitation of decision tree via emsemble methods

• the lack of continuity,

• the lack of smooth decision boundary,

• the high instability with respect to minor perturbations in the training

data,

• the inability to provide good approximations to certain classes of simple

often-occurring functions.

We call the sum of decision trees as additive decision tree deﬁned in the

following form

N
(cid:88)

n=1

wnTn

(6)

where Tn is a decision tree and wn is greater than 0 for n = 1, 2, · · · , N . For con-
venience, we divide the additive decision trees into two categories: (1) decision
forests such as [23, 59, 90]; (2) boosting trees such as [17, 71, 53, 66].

Figure 2: Additive decision trees

The main diﬀerence between them are the methods to train the single de-
cision tree and the weights to combine them. Following Leo Breiman in [10],
the decision forest is to resample from the original training set to construct a
single decision tree and and then combined by voting; gradient boosting trees
are to adaptively resample and combine (hence the acronym–arcing) so that the
weights in the resampling are increased for those cases most often misclassiﬁed
and the combining is done by weighted voting. Leo Breiman gave his Wald Lec-
ture on this topic [8, 9]. Both methods are explained as interpolating classiﬁers
in [85].

3.1 Decision forest

A decision forest is a collection of decision trees by a simple averaging operation
[13]. Antonio Criminisi and his coauthors wrote two books on this topic [23, 22].

7

Forest-based classiﬁcation and prediction is also discussed in the Chapter 6 in
[88]. All trees in the decision forest are trained independently (and possibly
in parallel). Decision forests are designed to improve the stability of a single
decision tree.

And random forest will permute the training set and combine the trained
trees. Diﬀerent methods on the permutation of the training data leads to dif-
ferent random forests. The name ‘random forest‘ is initially proposed in [13].
Here we refer it to additive trees (6) which use the permuted data sets to train
the component tree. For examples, Leo Breiman in [13] use a bootstrap samples
and truncate the feature space on random to train each tree in order to decrease
the correlation between the trees, which can be regarded as a combination of
bagging and random subspace methods.

In an extremely randomized trees [35] this is made much faster by the fol-

lowing procedure.

1. The feature indices of the candidate splits are determined by drawing

max_features at random.

2. Then for each of these feature index, we select the split threshold by

drawing uniformly between the bounds of that feature.

When the set of candidate splits are obtained, as before we just return that

split that minimizes C.

Another direction in this ﬁeld is to decrease the number of decision trees with
low accuracy reduction. It is the model compression for forest-based models. For
example, it is shown that ensembles generated by a selective ensemble algorithm,
which selects some of the trained C4.5 decision trees to make up an ensemble,
may be not only smaller in the size but also stronger in the generalization
than ensembles generated by non-selective algorithms[91]. Heping Zhang and
Minghui Wang proposed a speciﬁc method to ﬁnd a sub-forest (e.g., in a single
digit number of trees) that can achieve the prediction accuracy of a large random
forest (in the order of thousands of trees) in [89].

Yi Lin and Yongho Jeon study random forests through their connection with

a new framework of adaptive nearest neighbor methods [57].

3.2 Boosted decision trees

Breiman in [12] summarized that the basic idea of AdaBoost [32] is to adap-
tively resample and combine (hence the acronym–arcing) so that the weights
in the resampling are increased for those cases most often misclassiﬁed and the
combining is done by weighted voting. Then in [11] it is found that arcing algo-
rithms are optimization algorithms which minimize some function of the edge.
A general gradient descent “boosting” paradigm is developed for additive ex-
pansions based on any ﬁtting criterion in [34] There are diverse algorithms and
theories on boosting as shown by Schapir and Freund in the monograph [74].

Multiple Additive Regression Trees (MARTTM) is an implementation of the
gradient tree boosting methods as described in [34] Although there are other

8

boosting methods as discussed in [8], gradient boosting decision trees are more
popular than others such as the open and free softwares [17, 71, 53, 83].

Algorithm 2 Gradient Boost Decision Trees
1: Input training data set {(xn, yn) | xn ∈ X ⊂ Rp, yn ∈ R, n = 1, 2, · · · , N }
2: Initialize f0 = arg minγ
3: for t = 1, 2, . . . , T do
4:

for i = 1, 2, . . . , N do

i=1 L(xi, γ)

(cid:80)N

Compute ri,t = −[ ∂L(yi,f (xi))

∂f (xi)

|f =F (t−1) ].

end for
Fit a regression tree to the targets ri,t giving terminal regions

Rj,m, j = 1, 2, . . . , Jm.

for j = 1, 2, . . . , Jm do

Compute γj,t = arg minγ

(cid:80)

end for
ft = (cid:80)Jm
Update F (t) = F (t−1) + νft, ν ∈ (0, 1)

j=1γj,tI(x ∈ Rj,m)

12:
13: end for
14: Output F (T ).

xi∈Rj,m

L(yi, F (t−1)(xi) + γ).

5:

6:
7:

8:
9:
10:

11:

4 Probabilistic decision trees and forest

We begin with John Ross Quinlan:

Decision trees are a widely known formalism for expressing classiﬁ-
cation knowledge and yet their straightforward use can be criticized
on several grounds. Because results are categorical, they do not
convey potential uncertainties in classiﬁcation. Small changes in the
attribute values of a case being classiﬁed may result in sudden and
inappropriate changes to the assigned class. Missing or imprecise
information may apparently prevent a case from being classiﬁed at
all.

As argued in [55], the probabilistic approach allows us to encode prior as-
sumptions about tree structures and share statistical strength between node
parameters; furthermore, it oﬀers a principled mechanism to obtain probabilis-
tic predictions which is crucial for applications where uncertainty quantiﬁcation
is important. We will revisit methods of overcoming these deﬁciencies under
the probabilistic len such as [55, 73]. Another motivation of probabilistic trees
is to soften their decision boundaries as well as the fuzzy decision trees.

9

Let us review the sum-product representation of decision tree

f (x) =

M
(cid:88)

Km(cid:89)

am

m=1

k=1

H[skm(xv(k,m)−tkm )]

where H[·] ∈ {0, 1} which can be regarded as binary probability distribution.
The product (cid:81)Km
k=1 H[skm(xv(k,m)−tkm )] is the probability reaching the leaf node.
In short, soft decision tree replace the indicator function with with continuous
functions in [0, 1] such as cumulant density function or membership function.
In particular, the output of probabilistic trees is deﬁned as below

f (x) =

M
(cid:88)

Km(cid:89)

am

m=1

k=1

H[skm(xv(k,m)−tkm )]

where H(·) is cumulant density function.

4.1 Hierarchical mixtures of experts

The mixtures-of-experts (ME) architecture is a mixture model in which the
mixture components are conditional probability distributions. The basic idea of
Probabilistic Decision Trees (also called Hierarchical Mixtures of Experts)[52] is
to convert the decision tree into a mixture model. Probabilistic Decision Trees
is a blend of Bayesian methods and artiﬁcial neural networks. To quote Michael
I Jordan [50]

Probabilistic Decision Trees

• drop inputs down the tree and use probabilistic models for de-

cisions;

• at leaves of trees use probabilistic models to generate outputs

from inputs;

• use a Bayes‘ rule recursion to compute posterior credit for non-

terminal nodes in the tree.

The routing and outputs of of probabilistic decision trees are probabilistic while
their structures are determined rather than probabilistic. In another word, the
number and positions of nodes are determined.

The architecture is a tree in which the gating networks sit at the non-terminal
nodes of the tree. These networks receive the vector x as input and produce
scalar outputs that are a partition of unity at each point in the input space.
The expert networks sit at the leaves of the tree. Each expert produces an
output vector pij for each input vector. These output vectors proceed up the
tree, being multiplied by the gating network outputs and summed at the non-
terminal nodes.

We follow the generative model interpretation of probabilistic decision trees
described by Christopher M. Bishop and Markus Svensen in [6]. Each gating

10

Table 1: Comparison of decision trees and probabilistic decision trees

Decision Trees
Test functions
Prediction functions
Recursively partitioning of the
input space
Simple functions

Determined routing

Probabilistic Decision Trees
Gating networks
Expert networks
Soft probabilistic splits of the in-
put space
Smoothed piecewise analogs of
corresponding generalized
the
linear models
Stochastic routing

node has an associated binary variable zi ∈ {0, 1} whose value is chosen with
probability given by

p(zi | X, vi) = σ(xT vi)zi(1 − σ(xT vi))1−zi

1
1+exp(−x)

where σ(x) =
for x ∈ R is the logistic sigmoid function, and vi is
vector of parameters governing the distribution. If zi = 1 we go down the left
branch while zi = 0 we go down the right branch. Starting at the top of the
tree, we thereby stochastically choose a path down to a single expert node j, and
then generate a value for t from conditional distribution for that expert. Given
the states of the gating variables, the HME model corresponds to a conditional
distribution for t of the form

p(t | x, W, τ, z) =

m
(cid:89)

j=1

N (t | X, Wjx, τ −1I)ζj

where M is the total number of experts, W denotes {Wj} and τ denotes {τj}.
Here we have deﬁned

ζj =

(cid:89)

˜zj

in which the product is taken over all gating nodes on the unique path from the
root node to the jth expert, and

i

(cid:40)

˜z =

if i is in the left sub-tree of the node i

zi,
1 − zi, otherwise.

Marginalizing over the gating variables z = {zi},

p(t | x, W, τ, v) =

(cid:88)

m
(cid:89)

N (t | X, Wjx, τ −1I)ζj (cid:89)

p(zi | x, vi)

z
(cid:88)

j=1
πj(x)N (t | X, Wjx, τ −1I)

=

i

j

(7)

11

so that the conditional distribution p(t | x, W, τ, v) is a mixture of Gaussians
in which the mixing coeﬃcients πj(x) for expert j is given by a product over
all gating nodes on the unique path from the root to expert j of factor σ(xT vi)
or 1 − σ(xT vi) according to whether the branch at the ith node corresponds to
zi = 1 or zi = 0.

4.1.1 Bayesian hierarchical mixtures of experts

Christopher M. Bishop and Markus Svensen combine ‘local’ and ‘global’ varia-
tional methods to obtain a rigorous lower bound on the marginal probability of
the data in [6].

We deﬁne a Gaussian prior distribution independently over of the parameters

vi for each of the gating nodes given by

p(vi | βi) = N (vi, β−1

i

I).

Similarly, for the parameters Wj of jth expert nodes we deﬁne priors given by

p(Wj | αj) =

d
(cid:89)

i

N (wji, α−1

j

I),

where j runs over the target variables, and d is the dimensionality of the target
space. The hyperparameter αj, βi and τj are given conjugate gamma distribu-
tions.

The variational inference algorithm for HME is extended by using automatic

relevance determination (ARD) priors in [68].

4.1.2 Hidden Markov decision trees

In [51], it is to combine the Hierarchical Mixtures of Experts and the hidden
Markov models to produce the hidden Markov decision tree for time series.

This architecture can be viewed in one of two ways: (a) as a time sequence of
decision trees in which the decisions in a given decision tree depend probabilis-
tically on the decisions in the decision tree at the preceding moment in time;
(b) as an HMM in which the state variable at each moment in time is factorized
and the factors are coupled vertically to form a decision tree structure.

4.1.3 Soft decision trees

As opposed to the hard decision node which redirects instances to one of its chil-
dren depending on the outcome of test functions, a soft decision node redirects
instances to all its children with probabilities calculated by a gating function
[48].

Learning the tree is incremental and recursive, as with the hard decision
tree. The algorithm starts with one node and ﬁts a constant model. Then, as
long as there is improvement, it replaces the leaf by a subtree. This involves

12

optimizing the gating parameters and the values of its children leaf nodes by
gradient-descent over an error function.

End-to-end Learning, End-to-End Learning of Decision Trees and Forests, is
an Expectation Maximization training scheme for decision trees that are fully
probabilistic at train time, but after a deterministic annealing process become
deterministic at test time.

Each leaf is reached by precisely one unique set of split outcomes, called a

path. We deﬁne the probability that a sample x takes the path to leaf (cid:96) as

µ(cid:96)(x) =

(cid:89)

s(fn)

(cid:89)

(1 − s(fn))

n∈AL((cid:96))

n∈AR((cid:96))

where fn is the split/test/decision function of the non-terminal node n such
as a linear combination of the input fn(x) = (cid:104)sn, x(cid:105) − bn; s is the probability
; AL((cid:96))(AR((cid:96))) is the set of ancestors
density function such as s(x) =
of (cid:96) whose left(right) branch has been followed on the path from the root node
to (cid:96). The prediction of the entire decision tree is given by multiplying the path
probability with the corresponding leaf prediction

1
1+exp(−x)

p(y) =

(cid:88)

(cid:96)∈TL

(π(cid:96))yµ(cid:96)(x)

where π(cid:96) is a categorical distribution over classes k ∈ {1, 2, · · · , K} associated
with the leaf node (cid:96) and so (cid:80)K
k=1(π(cid:96))k = 1. The code is in End-to-end Learning
of Deterministic Decision Trees.

4.2 Mondrian trees and forests

Mondrian trees and forests are based on The Mondrian Process, which is a
guillotine-partition-valued stochastic process. The split functions of these mod-
els are generated on random.

In Chapters 5 and 6 of [55], Mondrian forests are discussed, where Bayesian

inference is performed over leaf node parameters.

The Mondrian Process can be interpreted as probability distributions over
kd-tree data structures. In another word, the samples drawn from Mondrian
Processes are kd-trees. For example, a Mondrian process m ∼ M P (λ, (a, A), (b, B))
is given a constant λ and the rectangle (a, A) × (b, B).

More on the Mondrian process: The Mondrian Process, Stochastic geome-
try to generalize the Mondrian Process, Reversible Jump MCMC Sampler for
Mondrian Processes.

Similar to the extremely randomized trees, the learning of Mondrian trees

consist of two steps:

1. The split feature index f is drawn with a probability proportional to
ub[f ] − lb[f ] where ub and lb and the upper and lower bounds of all the
features.

13

Algorithm 3 Mondrian Processes
1: Input the speciﬁcation {λ, (a, A), (b, B)}
2: Draw E from an exponential distribution Exp(A − a + B − b);
3: if E ≥ λ then
4:

An axis-aligned cut is made uniformly at random along the combined
lengths of (a, A) and (b, B):
Choose dimension d with probability ∝ ud − (cid:96)d;
Choose cut location uniformly from (cid:96)d, ud;
Recurse on left and right subtrees with parameter λ − E.

the process halts, and returns the trivial partition (a, A) × (b, B);

5:
6:
7:
8: else
9:
10: end if

2. After ﬁxing the feature index, the split threshold δ is then drawn from a

uniform distribution with limits lb, ub.

In another word, the learning of Mondrian trees are a sample drawn form the
Mondrian process.

The prediction step of a Mondrian Tree is diﬀerent from the classic decision
trees. It takes into account all the nodes in the path of a new point from the root
to the leaf for making a prediction. This formulation allows us the ﬂexibility to
weigh the nodes on the basis of how sure/unsure we are about the prediction in
that particular node.

Mathematically, the distribution of P (Y |X) is given by

P (Y | X) =

(cid:88)

j

wjN (mj, vj)

where the summation is across all the nodes in the path from the root to the
leaf. The mean prediction becomes (cid:80)

j wjmj.
Formally, the weights are computed as following. If j is not a leaf, wj(x) =
pj(x) (cid:81)
k∈anc(j)(1 − pk(x)), where the ﬁrst one being the probability of splitting
away at that particular node and the second one being the probability of not
splitting away till it reaches that node. If j is a leaf, to make the weights sum
up to one, wj(x) = 1 − (cid:80)
k∈anc(j) wk(x). Here pj(x) denotes the probability
of a new point x splitting away from the node j; anc(j) denotes the ancestor
nodes of the node j. We can observe that for x that is completely within the
bounds of a node, wj(x) becomes zero and for a point where it starts branching
oﬀ, wj(x) = pj(x). So the Mondrian tree is expressed in the following form

P (Y | X) =

(cid:88)

j

(cid:89)

pj(x)

(1 − pk(x))N (mj, vj).

(8)

k∈anc(j)

The separation pj(x) of each node is computed in the following way.

1. ∆j = τj − τparent(j);

14

2. ηj(x) = (cid:80)

f (max(x[f ] − ubj[f ], 0) + max(0, lbj[f ] − x[f ]));

3. pj(x) = 1 − e−∆j ηj (x)).

The implementation is in Mondrian Tree Regressor in scikit-garden, code on
Modrian forests, Mondrian, Scornet talk in Mondrian Tree, a blog on mondrian-
trees.

Mondrian Forests is the sum of Mondrian trees such as Mondrian Forests,

AMF: Aggregated Mondrian Forests for Online Learning.

An variant of Mondrian forest is the Random Tessellation Forests.

5 Bayesian decision trees

As known, multiple adaptive regression spline is an extension of decision tree.
And decision trees are a two-layer network. Both ﬁelds introduce the Bayesian
methods. It is necessary to blend Bayesian methods and decision trees. And it
is important to introduce probabilistic methods to deal with the uncertainties in
decision trees. Recent history has seen a surge of interest in Bayesian techniques
for constructing decision tree ensembles [58].

The basic idea of Bayesian thought of tree-based methods is to have the
prior induce a posterior distribution which will guide a search towards more
promising tree-based models such as [19, 28, 84, 69, 36, 21, 40, 38].

5.1 Bayesian trees

From an abstract viewpoint, decision tree is a mapping D → R with the pa-
rameters Θ and d, where T guide the inputs to the terminal nodes and Θ is
the parameters of prediction associated with the terminal nodes. The Bayesian
statisticians would pre-specify a prior on the parameters Θ to reﬂect the pref-
erences or belief on diﬀerent models. Let f (x | Θ, T ) : x (cid:55)→ Y be a decision
tree.
In contrast to the conventional classiﬁcation decision tree, the decision
tree f outputs the probability distribution of targets. The conventional deci-
sion tree outputs the Dirac distribution or δ function on the mode of the targets
associated with the terminal nodes.

In the ensemble perspectives, Bayesian tree is a posterior

(cid:40)(cid:80)
(cid:82)

Θ f (x | Θ, T )P (Θ),
Θ T (x | Θ, T )dF (Θ),

if Θ is discrete;
if Θ is continuous.

Here F (Θ) is the cumulative probability function of the random variable Θ. As
in [69], the output Y is the probability drawn from the distribution Yx where the
distribution of Yx will determine the type of problem we are solving: a discrete
random variable translates into a classiﬁcation problem whereas a continuous
random variable translates into a regression problem. In another word, the treed
model then associates a parametric model for Y at each of the terminal nodes

15

of T . More precisely, for values x that are assigned to the ith terminal node of
T , the conditional distribution of Y | x is given by a parametric model

Y = f (x | Θ, T ) = P (y | x, Θ, T ) ∼ f (y | x, Θi, T ).

All points in the same set of a leaf node Π share the same outcome distribution,
i.e. Y does not depend on x but Θ, T . By using a richer structure at the
terminal nodes, the treed models in a sense transfer structure from the tree to
the terminal no des.

As presented in [19], association of the individual Y values with the ter-
minal nodes is indicated by letting Yij denote the jth observation of Y in the
ith partition (corresponding to the ith terminal node), i ∈ {1, 2, · · · , b}, j ∈
{1, 2, · · · , ni}. In this case, the CART model distribution for the data will be of
the form

P (Y | X, Θ) =

b
(cid:89)

i=1

f (Yi | θi) =

b
(cid:89)

ni(cid:89)

i=1

j=1

f (Yij | θi)

where we use f to represent a parametric family indexed by θi. Since a CART
model is identiﬁed by (Θ, T ), a Bayesian analysis of the problem proceeds by
specifying a prior probability distribution P (Θ, T ). This is most easily accom-
plished by specifying a prior P (T ) on the tree space and a conditional prior
P (θ | T ) on the parameter space, and then combining them via P (Θ, T ) =
P (T )P (Θ | T ). The features are also pointed out in[19]: (1) the choice of prior
for T does not depend on the form of the parametric family indexed by Θ; (2)
the conditional speciﬁcation of the prior on Θ more easily allows for the choice
of convenient analytical forms which facilitate posterior computation.

In [19], the tree prior P (T ) is implicitly speciﬁed by a tree-generating stochas-
tic process. As in [20], this prior is implicitly deﬁned by a tree-generating
stochastic process that grows trees from a single root tree by randomly splitting
terminal nodes. A tree’s propensity to grow under this process is controlled by
a two-parameter node splitting probability

P (node splits j | depth = d) = α(1 + d)−β,

where the root node has depth 0. The parameter α is a base probability of
growing a tree by splitting a current terminal node and β determines the rate
at which the propensity to split diminishes as the tree gets larger. The tree
prior P (T ) is completed by specifying a prior on the splitting rules assigned to
intermediate nodes. The stochastic process for drawing a tree from this prior
can be described in the following recursive manner:

1. Begin by setting T to be the trivial tree consisting of a single root (and

terminal) node denoted η.

2. Split the terminal node η with probability P (node splits j | depth = d).

3. If the node splits, assign it a splitting rule ρ according to the distribution
PRU LE(ρ | η, d), and create the left and right children nodes. Let T denote

16

the newly created tree, and apply steps 2 and 3 with η equal to the new
left and the right children (if nontrivial splitting rules are available).

Here PRU LE(ρ | η, d) is a distribution on the set of available predictors xi, and
conditional on each predictor, a distribution on the available set of split values or
category subsets. As a practical matter, we only consider prior speciﬁcation for
which the overall set of possible split values is ﬁnite. Thus, each PRU LE(ρ | η, d)
will always be a discrete distribution. Note also that because the assignment of
splitting rules will typically depend on X, the prior P (T ) will also depend on
X.

The speciﬁcation of P (Θ | T ) will necessarily be tailored to the particular
form of the model P (y | x, θ) under consideration. In particular, a key consider-
ation is to avoid conﬂict between P (Θ | T ) and the likelihood information from
the data. In [19, 18, 20], diverse priors were proposed. Starting with an initial
tree T 0, iteratively simulate the transitions from T i to T i+1 by the two steps:

1. Generate a candidate value T ∗ with probability distribution q(T i, T ∗).

2. Set T i+1 = T ∗ with probability

α(T i, T ∗) = min{

q(T ∗, T i)p(Y | X, T ∗)p(T ∗)
q(T i, T ∗)p(Y | X, T i)p(T i)

, 1}.

Otherwise, set T i+1 = T i.

We consider kernels q(T ∗, T i) which generate T ∗ from T by randomly choos-

ing among four steps:

• GROW: Randomly pick a terminal node. Split it into two new ones by
randomly assigning it a splitting rule according to pRU LE used in the prior.

• PRUNE: Randomly pick a parent of two terminal nodes and turn it into

a terminal node by collapsing the nodes below it.

• CHANGE: Randomly pick an internal node, and randomly reassign it a

splitting rule according to pRU LE used in the prior.

• SWAP: Randomly pick a parent-child pair which are both internal nodes.
Swap their splitting rules unless the other child has the identical rule. In
that case, swap the splitting rule of the parent with that of both children.

5.2 Bayesian additive regression trees

BART[21, 40, 38] is a nonparametric Bayesian regression approach which uses
dimensionally adaptive random basis elements. Motivated by ensemble methods
in general, and boosting algorithms in particular, BART is deﬁned by a statis-
tical model: a prior and a likelihood. BART diﬀers in both how it weakens
the individual trees by instead using a prior, and how it performs the iterative
ﬁtting by instead using Bayesian back-ﬁtting on a mixed number of trees.

17

We complete the BART model speciﬁcation by imposing a prior over all the
parameters of (6) including the total number of decision trees, all the bottom
node parameters as well as the tree structures and decision rules. For simplicity,
the tree components are set to be independent of each other and of error, and
the terminal node parameters of every tree are independent. In [21], each tree
follows the same prior as in [19]. And the errors are in normal distribution, i.e.,
(cid:15) ∼ N (0, σ) and σ use conjugate prior the inverse chi-square distribution.

BART can also be used for variable selection by simply selecting those vari-

ables that appear most often in the ﬁtted sum-of-trees models.

A modiﬁed version of BART is developed in [38], which is amenable to fast

posterior estimation.

The implementation of BART is in the project SoftBart. And we can use

BART to casual inference BARTC.

5.3 Bayesian MARS

The Bayesian approach is applied to univariate and multivariate adaptive re-
gression spline (MARS) such as [29, 42, 31].

Multivariate adaptive regression spline (MARS) [33], motivated by the re-

cursive partitioning approach to regression, is given by

k
(cid:88)

i=1

aiBi(x)

(9)

where x ∈ D and the ai(i = 1, · · · , k) are the suitably chosen coeﬃcients of the
basis functions Bi and k is the number of basis functions in the model. The Bi
are given by

(cid:40)

Bi =

1,
(cid:81)Ji

j [sij · (xµ(ij) − tij)]+,

i = 1
i = 2, 3, · · ·

(10)

where [x]+ = max(0, x); Ji is the degree of the interaction of basis Bi, the sji,
which we shall call the sign indicators, equal ±1, the µ(ij) give the index of the
predictor variable which is being split on the tji (known as knot points) give
the position of the splits. The µ(j, ·)(j = 1, J) are constrained to be distinct so
each predictor only appears once in each interaction term. See Section 3, [33]
for more details.

Bayesian MART[29] set up a probability distribution over the space of possi-
ble MARS structures. Any MARS model can be uniquely deﬁned by the number
of basis functions present, the coeﬃcients and the types of the basis functions,
together with the knot points and the sign indicators associated with each in-
teraction term. In another word, it is k, ai, sij, tij (10), (9) and the types of
basis function Bi Here the type of basis function Bi is described by Ti, which
just tells us which predictor variables we are splitting on, i.e. what the values
of µ(1, i), · · · , µ(Ji, i) are.

18

Table 2: Comparison of decision trees and multivariate adaptive regression
spline

Methods
Non-terminal functions
Terminal functions
Training methods

Decision Trees
Test functions
Constant functions
Induction and Pruning

Trained results

Simple functions

Routing
Hyperparameter

Determined routing
the number of terminal
nodes

MARS
Truncation functions
Polynomial function
Forwards search and back-
wards deletion
Piece-wise
functions
No routing
the number of basis func-
tions

polynomial

A truncated Poisson distribution (with parameter k) is used to specify the

prior probabilities for the number of basis functions, giving

p(k | λ) =

λk
αk!

, ∀k ∈ {1, 2, · · · , kmax}

where α is the normalization constant.

We can apply this Bayesian scheme directly into decision trees based on the

representation (2) in [33].

6 Optimal decision trees

Generally, the decision tree learning methods are greedy and recursively as
shown in [43, 64]. Here we will focus on numerical optimization methods for
decision trees. The optimal decision trees are expected as the best multivari-
ate tree with a given structure. However, such optimum condition is diﬃcult
to verify because of their comibinatorical structure. So we need optimization
techniques to minimize the error of the entire decision tree instaed of the greedy
or heuristic induction methods.

6.1 Optimal classiﬁcation trees

Bertsimas Dimitris and Dunn Jack [4] present Optimal Classiﬁcation Trees, a
novel formulation of the decision tree problem using modern MIO techniques
that yields the optimal decision tree. The core idea of the MIP formulation of
optimal decision tree is

• to construct the maximal tree of the given depth;

• to enforce the requirement of decision trees with the constraints;

19

• to minimize the cost function subject to the constraints.

It is ﬁrst to discuss the axes-aligned/univariate decision tree. If the depth
D is given, we will determine the structure of the maximal tree for this depth,
i.e., the variables A(t), AL(t), AR(t) and TB, TL. If a branch node does apply
a split, i.e. dt = 1, then the axes-aligned split at will select one and only one
attribute to test; If a branch node does not apply a split, i.e. dt = 0, then the
axes-aligned split at is set to be (cid:126)0 so that all points are enforced to follow the
right split at this node.1 These are enforced with the following constraints on
split functions:

p
(cid:88)

j=1

ajt = dt, ∀t ∈ TB,

0 ≤ bt ≤ dt ≤ 1, ∀t ∈ TB,

ajt ∈ {0, 1}, ∀t ∈ TB.

The hierarchical structure of the tree are enforced via

dt ≤ dp(t) ∀t ∈ TB − {1}

which means that the node t is likely to apply a split function only if its par-
ent applies a split function. The above constraints are not related with the
information of samples and they are just necessary requirement on the decision
trees.

We also force each point to be assigned to exactly one leaf

(cid:88)

t∈TL

zit = 1, i = 1, 2, · · · , n

and each leaf has at least the minimum number of samples

zit ≤ l(t),

n
(cid:88)

i=1

zit ≥ Nminlt, t ∈ TL ∪ TL.

Finally, we apply constraints enforcing the splits that are required by the struc-
ture of the tree when assigning points to leaves

aT
mxi + (cid:15) ≤ bm + M1(1 − zit), i = 1, · · · , n, ∀t ∈ TB, ∀m ∈ AL(t)

aT
mxi ≥ bm − M2(1 − zit), i = 1, · · · , n, ∀t ∈ TB, ∀m ∈ AR(t)
The largest valid value of (cid:15) is the smallest non-zero distance between adjacent
values of this feature, i.e.,

(cid:15) + i = max{xi+1

j − x(i)

j

| x(i+1)
j

(cid:54)= x(i)

j , i = 1, · · · , n, }

1Here the left split of the node t is atx < bt and the right split is atx ≤ bt.

20

Table 3: The Notation of MIP Formulation of Decision Trees

Notation
Nmin
Nx(l)

(xi, yi)

p(t)
A(t)
AL(t)

AR(t)

TB

TL

at ∈ Rp

bt ∈ R

dt

zit

lt

M1, M2
Nkt
Nt
{1, · · · , K}
Nt
Yik
Nkt
ckt

non-terminal

Deﬁnition
the minimum number of points of all nodes
the number of training points contained in leaf
node l
the training data, where xi ∈ [0, 1]p, yi ∈
{1, · · · , K} for i = 1, · · · , n.
the parent node of node t
the set of ancestors of node t
the set of ancestors of t whose left branch has
been followed on the path from the root node
to t
the set of right-branch ancestors of t, A(t) =
AL(t) ∪ AR(t)
branch node set, also a.k.a.
nodes, apply a linear split
leaf node set, also a.k.a. terminal nodes, make
a class prediction
the combination coeﬃcients of the split func-
tion at the node t
the threshold of the split function at the node
t
dt = I(the node t applies a split), the indica-
tor variables to track which branch nodes ap-
ply splits
zit = I(xi in the node t ),the indicator vari-
ables to track the points assigned to each leaf
node t
lt = I(the node t is not null),the indicator
variables to track cardinality of each leaf node
big constants
the number of points of label k in node t,
be the total number of points in node t
the ground truth set
the total number of points in node t
Kronecker delta function
the number of points of label k in node t
the variable to track the prediction of the node
t

21

where x{

j (i)} is the i-th largest value in the j-th feature.

The total number of points in node t and the number of points of label k in

node t: Nkt = 1
2

(cid:80)n

i=1(Yik + 1)zit, k = 1, · · · , K, t ∈ TL, and

Nt =

K
(cid:88)

k=1

Nkt =

n
(cid:88)

i=1

zit, t ∈ TL.

The optimal label of each leaf to predict is:

ct = arg

max
k∈{1,2,··· ,K}

Nkt

and we use binary variables ckt to track the prediction of each node where

ckt = Ict=k =

(cid:40)

if ct = k
1,
0, otherwise

.

We must make a single class prediction at each leaf node that contains points:

K
(cid:88)

k=1

ckt = lt, ∀t ∈ TL.

The misclassiﬁcation cost is

Lt = Nt − Nct t = Nt −

max
k∈{1,2,··· ,K}

Nkt =

min
k∈{1,2,··· ,K}

Nt − Nkt

which can be linearized to give

Lt ≥ Nt − Nkt − M (1 − ckt), k = 1, · · · , K, ∀t ∈ TL

Lt ≤ Nt − Nkt − M ckt, k = 1, · · · , K, ∀t ∈ TL

Lt ≥ 0, ∀t ∈ TL.
where again M is a suﬃciently large constant that makes the constraint inactive
depending on the value of ckt

22

min

(cid:88)

t∈TL

Lt + α

(cid:88)

dt

t∈TB

subject to Lt ≥ Nt − Nkt − M (1 − ckt), k = 1, · · · , K, ∀t ∈ TL
Lt ≤ Nt − Nkt − M ckt, k = 1, · · · , K, ∀t ∈ TL, Lt ≥ 0, ∀t ∈ TL

Nkt =

1
2

n
(cid:88)

(Yik + 1)zit, k = 1, · · · , K, t ∈ TL,

i=1

K
(cid:88)

Nt =

Nkt =

n
(cid:88)

zit, t ∈ TL,

K
(cid:88)

ckt = lt, ∀t ∈ TL,

i=1

k=1

k=1
aT
mxi + (cid:15) ≤ bm + M1(1 − zit), i = 1, · · · , n, ∀t ∈ TB, ∀m ∈ AL(t),
aT
mxi ≥ bm − M2(1 − zit), i = 1, · · · , n, ∀t ∈ TB, ∀m ∈ AR(t),
p
(cid:88)

ajt = dt, ∀t ∈ TB, 0 ≤ bt ≤ dt ≤ 1, ∀t ∈ TB,

j=1
ajt ∈ {0, 1}, ∀t ∈ TB, dt ≤ dp(t)∀t ∈ TB − {1},

(cid:88)

t∈TL

zit = 1, i = 1, 2, · · · , n, zit ≤ l(t),

n
(cid:88)

i=1

zit ≥ Nminlt, t ∈ TL ∪ TL.

Note that the indicator variables such as di, zit are in {0, 1} and bm ∈ R so

it is a mixed integer optimization problem.

For more on optimal decision trees see Dimitris Bertsimas and Romy Sh-
ioda[5], Hèlène Verhaeghe et al[80], Oktay et al[67], Kristin P. Bennettand
and Jennifer A. Blue[3], Sanjeeb Dash et al[24], Sicco Verwer and Yingqian
Zhang[81, 82], Murat Firat et al[30].

6.2 Tree alternating optimization

TAO [14] cycle over depth levels from the bottom (leaves) to the top (root) and
iterate bottom-top, bottom-top, etc. (i.e., reverse breadth-ﬁrst order). TAO
can actually modify the tree structure by ignoring the dead branches and pure
subtrees. And it shares the indirect pruning with MIO.

We want to optimize the the parameters of all nodes in the tree to minimize

the misclassiﬁcation cost jointly

L(T (x | Θ)) =

n
(cid:88)

i=1

(cid:96)(T (xi), yi)

where Θ = {at ∈ Rp, bt ∈ R, cl | ∀i ∈ TB, l ∈ TL}. The separability condition
allows us to optimize separately (and in parallel) over the parameters of any set
of nodes that are not descendants of each other (ﬁxing the parameters of the
remaining nodes). Optimizing the misclassiﬁcation cost over an internal node t

23

Table 4: Comparison of MIO and TAO for Decision Trees

MIO
maximum tree of given depth
ai = (cid:126)0, bi = 0
decision function
all their points have the same la-
bel
dt ≤ dp(t)∀t ∈ TB − {1}

by optimizing the coeﬃcients of
the split functions of the nodes
speciﬁed blood relatives of the
nodes

TAO
given tree structure
Dead branches
split function
Pure subtrees

dead branches and pure subtrees
are ignored
by reducing the size of the tree
to modify the tree structure
separability condition

is exactly equivalent to a reduced problem: a binary misclassiﬁcation loss for a
certain subset Ct (deﬁned below) of the training points over the parameters of
the test function.

Firstly, optimizing the misclassiﬁcation error over θt = (at, bt) in the mis-
classiﬁcation cost, where is summed over the whole training set, is equivalent to
optimizing it over the subset of training points St = {xi | zit = 1, i = 1, · · · , n}
that reach node t.

Next, the fate of a point x ∈ St depends only on which of t’s children it
follows because other parameters are ﬁxed. The modiﬁcation of the decision
function will only change some paths of samples in St called as altered travel-
ers, which only change partial results of the prediction. In another word, some
altered travelers eventually reach the diﬀerent leaf nodes with previous predic-
tion while some altered travelers eventually reach the same leaf as previously.

Then, we can deﬁne a new, binary classiﬁcation problem over the parameters
θt of the decision function on the altered travelers. Thus it is to optimize a
reduced problem.

There is no approximation guarantees for TAO at present. TAO does con-
verge to a local optimum in the sense of alternating optimization (as in k-means),
i.e., when no more progress can be made by optimizing one subset of nodes given
the rest.

For more information on TAO, see the following links.

1. Mcarreira Perpinan’s research on TAO;

2. Supplementary material for TAO;

3. An Experimental Comparison of Old and New Decision Tree Algorithms.

24

6.2.1 Surrogate objective

Mohammad Norouzi, Maxwell D. Collins, David J. Fleet, Pushmeet Kohli pro-
pose a novel algorithm for optimizing multivariate linear threshold functions
as split functions of decision trees to create improved Random Forest classi-
ﬁers. Mohammad Norouzi, Maxwell D. Collins, David J Fleet et al introduce
a tree navigation function f : Hm (cid:55)→ Im+1 that maps an m-bit sequence of
split decisions (Hm = {−1, +1}m) to an indicator vector that speciﬁes a 1-of-
(m + 1) encoding. However, it is not to formulate the dependence of navigation
function on binary tests. It is shown that the problem of ﬁnding optimal linear-
combination (oblique) splits for decision trees is related to structured prediction
with latent variables. The empirical loss function is re-expressed as

L(S, v; D) =

(cid:88)

(x,y)∈D

(cid:96)(vT f (ˆh(x)), y), s.t.ˆh(x) = arg max
h∈Hm

(hT Sx)

(11)

where f is the navigation function. And ˆh(x)i = (Sx)i
|(Sx)i|
on loss for an input-output pair, (x; y) takes the form

. And an upper bound

(cid:96)(vT f (

sgn(Sx))
(cid:125)
(cid:123)(cid:122)
(cid:124)
maxh∈Hm (hT Sx)

, y) ≤ max
g∈Hm

(gT Sx + (cid:96)(vT f (g)), y) − max
h∈Hm

(hT Sx).

(12)

To develop a faster loss-augmented inference algorithm, we formulate a slightly
diﬀerent upper bound on the loss, i.e.,

(cid:96)(vT f (sgn(Sx)), y) ≤

max
g∈∈B1(sgn(Sx)

(gT Sx + (cid:96)(vT f (g)), y) − max
h∈Hm

(hT Sx). (13)

where B1(sgn(Sx) denotes the Hamming ball of radius 1 around sgn(Sx). And
a regularizer is introduced on the norm of W when optimizing the bound

max
g∈Hm

max
g∈Hm

(agT Sx + (cid:96)(vT f (g)), y) − max
h∈Hm
(bgT Sx + (cid:96)(vT f (g)), y) − max
∈Hm

(ahT Sx) ≤

(bhT Sx).

(14)

for a > b > 0. Summing over the bounds for diﬀerent training pairs and
constraining the norm of rows of S, we the surrogate objective:

L(cid:48)(S, v; D) =

(cid:88)

(x,y)∈D
s.t. (cid:107)si(cid:107)2 ≤ ν ∀i = {1, 2, · · · , m.}

( max
g∈Hm

(bgT Sx + (cid:96)(vT f (g)), y) − max
h∈Hm

(bhT Sx))

(15)

where ν ∈ R+ is a regularization parameter and si is the i row of S.

We observe that after several gradient updates some of the leaves may end
up not being assigned to any data points and hence the full tree capacity may
not be exploited. We call such leaves inactive as opposed to active leaves that
are assigned to at least one training data point. An inactive leaf may become

25

active again, but this rarely happens given the form of gradient updates. To
discourage abrupt changes in the number of inactive leaves, we introduce a
variant of SGD, in which the assignments of data points to leaves are ﬁxed for a
number of gradient update steps. Thus, the bound is optimized with respect to
a set of data point to leaf assignment constraints. When the improvement in the
bound becomes negligible the leaf assignment variables are updated, followed
by another round of optimization of the bound. We call this algorithm Stable
SGD (SSGD) because it changes the assignment of data points to leaves more
conservatively than SGD.

Based on this surrogate objective, a Maximization-Minimization type method

is proposed as following

g = arg max
g∈Hm

{(gT Sx + (cid:96)(vT f (g)), y) − max
h∈Hm

(hT Sx)}

v = arg min
v∈Rm

{ max
g∈Hm

(gT Sx + (cid:96)(vT f (g)), y) − max
h∈Hm

(hT Sx)}

(16)

6.3 Diﬀerentiable trees

As put in End-to-End Learning of Decision Trees and Forests

One can observe that both neural networks and decision trees are
composed of basic computational units, the perceptrons and nodes,
respectively. A crucial diﬀerence between the two is that in a stan-
dard neural network, all units are evaluated for every input, while
in a reasonably balanced decision tree with I inner split nodes, only
O(log I) split nodes are visited. That is, in a decision tree, a sample
is routed along a single path from the root to a leaf, with the path
conditioned on the sample’s features.

Here we refer diﬀerentiable trees to the soft decision trees which can be

trained via gradient-based methods.

End-to-end Learning of Deterministic Decision Trees, End-to-End Learning
of Decision Trees and Forests, Deep Neural Decision Forests “soften” decision
functions in the internal tree nodes to make the overall tree function and tree
routing diﬀerentiable.

6.3.1 Deep neural decision trees

Yongxin Yang et al[86] construct the decision tree via Kronecker product ⊗

z = f1(x1) ⊗ f2(x2) ⊗ · · · ⊗ fD(xD)

where each feature xd is binned by its own neural network fd(xd). Here fd(·) is
a one-layer neural network with softmax as its activation function:

softmax(

wx + b
τ

)

26

Table 5: Comparison of decision trees and deep neural networks

Decision Trees
Test functions
Node to node
Recursively-partitioning-based
growth methods
Logical transparency
Probabilistic decision trees
Decision stump
Decision tree

Deep Neural Networks
Nonlinear activation functions
Layer by layer
Gradient-based
methods
High numerical eﬃciency
Bayesian Deep Learning
Perception
A hidden-layer neural network

optimization

where x ∈ R, w = [1, 2, · · · , n + 1], b = [0, −β1, −β1 − β2, · · · , (cid:80)n
i=1 −βi] and
τ > 0 is a temperature factor. As τ → ∞, the output tends to a one-hot vector.
Here z is now also an almost one-hot vector that indicates the index of the leaf
node where instance x arrives. Finally, we assume a linear classiﬁer at each leaf
z classiﬁes instances arriving there.

In summary, it is to bin the features instead of the recursive partitioning as
usual. For more details of its implementation see https://github.com/wOOL/DNDT.

6.3.2 Deep neural decision forests

Deep Neural Decision Forests uniﬁes classiﬁcation trees with the representation
learning functionality known from deep convolutional networks. Each decision
node (branching node as in MIO) is responsible for routing samples along the
tree. When a sample reaches a decision node n it will be sent to the left or
right subtree based on the output of decision/test/split function. In standard
decision forests, the decision function is binary and the routing is deterministic.
In order to provide an explicit form for the routing function we introduce the
following binary relations that depend on the tree’s structure:

• (cid:96) (cid:46) n: if the leaf (cid:96) belongs to the left subtree of node n;

• (cid:96) (cid:38) n: if the leaf (cid:96) belongs to the right subtree of node n.

We can now exploit these relations to express routing function ν(cid:96)(x | Θ) pro-
viding the probability that sample x will reach leaf (cid:96) as follows:
(cid:89)

µ(cid:96)(x | Θ) =

dn(x | Θ)I(cid:96)(cid:46)n ¯dn(x | Θ)I(cid:96)(cid:38)n

n∈TB

where ¯dn(x | Θ) = 1 − dn(x | Θ) and IP is an indicator function conditioned
on the argument P . And the ﬁnal prediction for sample x from tree T with
decision nodes parametrized by Θ is given by

PT [y | x, Θ, π] =

(cid:88)

(cid:96)∈TL

π(cid:96)yµ(cid:96)(x | Θ)

27

where π(cid:96)y denotes the probability of a sample reaching leaf (cid:96) to take on class y.

Figure 3: The Stochastic Routing Diagram

The decision functions delivering a stochastic routing are deﬁned as follows:

dn(x | Θ) = σ(fn(x | Θ))

1
1+exp(−x)

where σ(x) =
is the sigmoid function; fn(x | Θ) : X (cid:55)→ R is a
real-valued function depending on the sample and the parametrization Θ. Our
intention is to endow the trees with feature learning capabilities by embedding
functions fn within a deep convolutional neural network with parameters Θ. In
the speciﬁc, we can regard each function fn as a linear output unit of a deep
network that will be turned into a probabilistic routing decision by the action
of dn, which applies a sigmoid activation to obtain a response in the [0; 1] range.

Figure 4: The schematic illustration of decision nodes

6.3.3 TreeGrad

TreeGrad, an extension of Deep Neural Decision Forests, reframes decision trees

28

as a neural network through construction of three layers, the decision node layer,
routing layer and prediction (decision tree leaf) layer, and introduce a stochastic
and diﬀerentiable decision tree model.
In TreeGrad, we introduce a routing
matrix Q which is a binary matrix which describes the relationship between the
nodes and the leaves. If there are n nodes and (cid:96) leaves, then Q ∈ {0, 1}(cid:96)×2n,
where the rows of Q represents the presence of each binary decision of the n
nodes for the corresponding leaf (cid:96). We deﬁne the matrix containing the routing
probability of all nodes to be D(x; Θ). We construct this so that for each node
j = 1, · · · , n, we concatenate each decision stump route probability

D(x; Θ) = [d0+(x; θ0) ⊕ · · · ⊕ dn+(x; θn) ⊕ d0− (x; θ0) ⊕ · · · ⊕ dn−(x; θn),

where ⊕ is the matrix concatenation operation, and di+, di−
indicate the prob-
ability of moving to the positive route and negative route of node i respectively.
We can now combine matrix Q and D(x; Θ) to express µl as follows:

µl = exp(QT

(cid:96) log(D(x; Θ)))

where Q(cid:96) represents the binary vector for leaf (cid:96). Accordingly, the ﬁnal prediction
for sample x from the tree T with decision nodes parameterized by Θ is given
by

PT (y | x, Θ, π) = softmax(πT µ(x | Θ, Q))
where π represents the parameters denoting the leaf node values, and the row
of µ(x | Θ, Q) is the routing function which provides the probability that the
sample x will reach leaf (cid:96), The formulation consists of three layers:

1. decision node layer: H1 = ˜W x + ˜b;
2. probability routing layer: H2 = QT (φ2◦H1)(x) where φ2 = (log ◦ softmax)(x);

3. the leaf layer: H3 = πT (φ3 ◦ H2)(x) where φ3 = exp(x).
In short, this neural decision tree is expressed in as shown in ??

T (x) = πT exp(QT (log ◦ softmax) ◦ ( ˜W x − ˜b)).

(17)

6.3.4 Neural tree ensembles

Neural Decision Trees reformulate the random forest method of Breiman (2001)
into a neural network setting

Neural Oblivious Decision Ensembles (NODE) is designed to work with any
In a nutshell, the proposed NODE architecture general-
tabular data [70].
izes ensembles of oblivious decision trees, but beneﬁts from both end-to-end
gradient-based optimization and the power of multi-layer hierarchical represen-
tation learning. The NODE layer is composed of m diﬀerentiable oblivious
decision trees (ODTs) of equal depth d. Then, the tree returns one of the 2d
possible responses, corresponding to the comparisons result. The tree output is
deﬁned as

h(x) = R[I(f1(x) − b1), · · · , I(fd(x) − bd)]

29

where I(·) denotes the Heaviside step function and R is a d-dimensional tensor
of responses. We replace the splitting feature choice fi and the comparison
operator I(f1(x) − b1) by their continuous counterparts. The choice function
fi is hence replaced by a weighted sum of features, with weights computed as
entmax over the learnable feature selection matrix:

ˆfi(x) =

n
(cid:88)

j=1

xj entmaxα(Fij).

And we relax the Heaviside function I(fi(d) − bi) as a two-class entmax σα(x) =
entmaxα([0, x]). As diﬀerent features can have diﬀerent characteristic scales, we
use the scaled version

ci = σα(

fi(x) − bi
τi

)

where bi and τi are learnable parameters for thresholds and scales respectively.
we deﬁne a “choice” tensor C the outer product of all ci

(cid:18)

C =

(cid:19)

(cid:18)

⊗

c1
1 − c1

(cid:19)

c2
1 − c2

⊗ · · · ⊗

(cid:18)

(cid:19)

.

cd
1 − cd

The ﬁnal prediction is then computed as a weighted linear combination of re-
sponse tensor entries R with weights from the entries of choice tensor C

ˆh(x) =

(cid:88)

Ri1,··· ,id · Ci1,··· ,id (x).

i1,i2,··· ,id∈{0,1}d

There is a supplementary code for the above method https://github.com/Qwicen/node.

7 Neural decision trees

Here neural decision trees refer to the models unite the two largely separate
paradigms, deep neural networks and decision trees, in order to take the advan-
tages of both paradigms such as [1, 86].

7.1 Adaptive neural trees

Deep neural networks and decision trees are united via adaptive neural trees
(ANTs) [77] that incorporates representation learning into edges, routing func-
tions and leaf nodes of a decision tree, along with a backpropagation-based
training algorithm that adaptively grows the architecture from primitive mod-
ules (e.g., convolutional layers).

An ANT is deﬁned as a pair (T , O) where T deﬁnes the model topology,

and O denotes the set of operations on it.

An ANT is constructed based on three primitive modules of diﬀerentiable

operations:

30

1. Routers, R: the router of each internal leaf sends samples from the in-

coming edge to either the left or right child.

2. Transformers, T : every edge of the tree has one or a composition of mul-

tiple transformer module(s).

3. Solvers, S: each leaf node operates on the transformed input data and

outputs an estimate for the conditional distribution.

Each input x to the ANT stochastically traverses the tree based on decisions
of routers and undergoes a sequence of transformations until it reaches a leaf
node where the corresponding solver predicts the label y.

An ANT models the conditional distribution p(y | x) as a hierarchical mix-
ture of experts (HMEs) and beneﬁt from lightweight inference via conditional
computation.

Suppose we have L leaf nodes, the full predictive distribution is given by

p(y | x, Θ) =

L
(cid:88)

(cid:96)

p(z(cid:96) = 1 | x, θ, ψ)p(y | z(cid:96) = 1, x, φ, ψ)

(18)

where (θ, φ, ψ) summarize the parameters of router, transformer and solver mod-
ules in the tree. The mixing coeﬃcient πθ,ψ
(cid:96) = p(z(cid:96) = 1 | x, θ, ψ) quantiﬁes the
probability that x is assigned to leaf l and is given by a product of decision
probabilities over all router modules on the unique path P(cid:96) from the root to leaf
node (cid:96):

πθ,ψ
(cid:96) =

(cid:89)

j (xψ
rθ

j )I((cid:96)(cid:46)i)(1 − rθ

j (xψ

j ))1−I((cid:96)(cid:46)i).

i∈P(cid:96)

Here rθ
j : Xj (cid:55)→ [0, 1], parametrized by θ, is the router of internal node j; and
(cid:96) (cid:46) i is a binary relation and is only true if leaf (cid:96) is in the left subtree of internal
node j; xψ
is the feature representation of x at node j deﬁned as the result of
j
composite transformation

xψ
j = (tψ
en

◦ · · · tψ
e2

◦ tψ
e1

)(x)

where each transformer tψ
is a nonlinear function, parametrized by ψ, that
en
transforms samples from the previous module and passes them to the next one.
The leaf-speciﬁc conditional distribution p(y | z(cid:96) = 1, x, φ, ψ) is given by its
solver’s output sψ

parent((cid:96))) parametrized by ψ.

(cid:96) (xψ

See its codes in https://github.com/rtanno21609/AdaptiveNeuralTrees.

7.2 Tree ensemble layer

The Tree Ensemble Layer (TEL)[37] is an additive model of diﬀerentiable deci-
sion trees. The TEL is equipped with a novel mechanism to perform conditional
computation, during both training and inference, by introducing a new sparse
activation function for sample routing, along with specialized forward and back-
ward propagation algorithms that exploit sparsity.

31

Assuming that the routing decision made at each internal node in the tree
is independent of the other nodes, the probability that x reaches (cid:96) is given by:

P (x → (cid:96)) =

(cid:89)

i∈A((cid:96))

r(cid:96)(x, wi)

(19)

where r(cid:96)(x, wi) is the probability of node i routing x towards the subtree con-
taining leaf l, i.e.,

r(cid:96)(x, wi) = S((cid:104)x, wi(cid:105))I((cid:96)(cid:46)i)(1 − S((cid:104)x, wi(cid:105)))I((cid:96)(cid:38)i).

Generally, the activation function S can be any smooth cumulant probability
function.

For a sample x → Rp, we deﬁne the prediction of the tree as the expected

value of the leaf outputs, i.e.,

T (x) =

(cid:88)

(cid:96)→L

P (x → (cid:96))o(cid:96)

(20)

where S in (19) is so-called smooth-step activation function, i.e.,

S(t) =






0,
γ3 t3 + 3
2
1,

2γ t + 1

2 , − γ

t ≤ − γ
2 ,
2 < t < γ
t ≥ γ
2 .

2

We can implement true conditional computation by developing specialized

forward and backward propagation algorithms that exploit sparsity.

See its implementation in https://github.com/google-research/google-research.

7.3 Neural-backed decision trees

Neural-Backed Decision Trees (NBDTs)[41] are proposed to leverage the pow-
erful feature representation of convolutional neural networks and the inherent
interpretable of decision trees, which achieve neural network accuracy and re-
quire no architectural changes to a neural network. Simply speaking, NBDTs
use the convolutional network to extract the intrinsic representation and use the
decision trees to generate the output based the learnt features.

The neural-backed decision tree (NBDT), has the exact same architecture
as a standard neural network and a subset of a fully-connected layer represents
a node in the decision tree. The NBDT pipeline consists of four steps divided
into a training phase and an inference phase.

1. Build an induced hierarchy using the weights of a pre-trained network’s

last fully-connected layer;

2. Fine-tune the model with a tree supervision loss;

3. For inference, featurize samples with the neural network backbone;

32

4. And run decision rules embedded in the fully-connected layer.

The ﬁrst step of training phase is to learn hierarchical decision procedure
and the second step is to optimize the pre-trained network and decision tree
jointly.

Code and pretrained NBDTs can be found at https://github.com/alvinwan/neural-

backed-decision-trees.

8 Regularized decision trees and forests

The regularization techniques are widely used in machine learning community
to control the model complexity and overcome the over-ﬁtting problem. The
regularization of nonlinear or nonparametric models is more diﬃcult than the
linear model. There are two key factors to describe the complexity of decision
trees: its depth and the total number of its leaves. In univariate decision trees
each intermediate node is a associated with a single attribute.

We use the regularization techniques to select features such as [26, 60, 72]
and an interpretable model is learnt. And the pruning techniques are used to
ﬁnd smaller models in the purpose to avoid over-ﬁtting or deploy a lightweight
models.

Like other iterative optimization methods, we take one training step to re-
duce the overall loss in boosted trees. In another word, we add a new tree fK
to update the model:

L(fK) < L(fK−1).
After each iteration, the model is more complicated and its cost is lower. As a
byproduct, it is prone to overﬁt. In order to make a trade-oﬀ between bias and
variance, the following regularized objective is to minimize in XGBoost :

L(fK) =

(cid:88)

i

(cid:96)(ˆyi, yi) +

K
(cid:88)

k=1

Ω(fk)

(21)

where the complexity of tree is deﬁned as Ω(f ) = γT + 1
based on
the (4) and ˆyi = (cid:80)K
k=1 fk(xi). Here (cid:96)(ˆyi, yi) is usually a diﬀerentiable convex
loss function that measures the quality of prediction ˆyi on training data (xi, yi)
so that it is available to obtain the gradient gi = ∂(cid:96)(ˆyi,yi)
and the Hessian
hi = ∂2(cid:96)(ˆyi,yi)
k=1 fk(xi). It is in alternating approach to train
a new tree. First, we ﬁx the number of leaves T , we can reduce 21 via the
surrogate loss

when ˆyi = (cid:80)K−1

j=1 w2
j

∂2 ˆyi

∂ ˆyi

2 λ (cid:80)T

arg min

w

(cid:88)

i

(cid:96)(ˆyi, yi) + fK(xi)gi +
(cid:124)

(cid:123)(cid:122)

1
2

fK(xi)2hi
(cid:125)

taking Taylor expansion at ˆyi = (cid:80)K−1

k=1 fk(xi)

+γT +

1
2

λ

T
(cid:88)

j=1

w2

j +

K−1
(cid:88)

k=1

Ω(fk)

33

i.e.,

w∗

j = −

obj∗ = −

Gj
Hj + λ
T
(cid:88)

1
2

j=1

G2
j
Hj + λ

+ γT

(22)

where Gj = (cid:80)
into two leaves if the following gains is positive

qK (xi)=j gi and Hj = (cid:80)

qK (xi)=j hi. Second, we will split a leaf

Gain =

1
2

(cid:20) G2
L
HL + λ

+

G2
R
HR + λ

−

(GL + GR)2
HL + HR + λ

(cid:21)

− γ.

And it is equivalent to the pruning techniques in tree based models.

This regularization is suitable for the univariate decision trees. And the
training procedure is still greedy. In the following, we will review some regular-
ization techniques in tree-based methods.

8.1 Regularized decision trees

8.1.1 Regularized soft trees

We can directly apply the norm penalty to train soft trees [48] in diﬀeren-
tiable trees and neural trees as we apply norm penalty to deep learning. For
example, Olcay Taner Yıldız and Ethem Alpaydın introduce local dimension re-
duction via L1 and L2 regularization for feature selection and smoother ﬁtting
in [87]. And we can induce the sparse weighted oblique decision trees as shown
in SWOT. Another aim of sparse decision trees is to regularize with sparsity for
interpretability.

8.1.2 Sparse decision trees

Sparse decision trees are oblique or multivariate decision trees regularized by
sparsity-induced norms to avoid over-ﬁtting.

We can generate sparse trees in optimal trees such as [45, 7, 56]. The optimal
decision trees can learn the decision template from the data with theoretical
guarantee.

The OSDT [45], proposed by Xiyang Hu, Cynthia Rudin and Margo Seltzer

is desiigned for binary features.

Generalized and Scalable Optimal Sparse Decision Trees(GOSDT) provides
a general framework for decision tree optimization that addresses the two sig-
niﬁcant open problems in the area: treatment of imbalanced data and fully
optimizing over continuous variables.

And it is oﬃcial implementation in https://github.com/xiyanghu/OSDT and

https://github.com/Jimmy-Lin/GeneralizedOptimalSparseDecisionTrees.

34

8.2 Regularized additive trees

Regularized decision forests is not only the sum of regularized decision trees. For
example, Heping Zhang and Minghui Wang in [89] propose a speciﬁc method
to ﬁnd a sub-forest (e.g., in a single digit number of trees) that can achieve the
prediction accuracy of a large random forest (in the order of thousands of trees).
The complexity of the additive trees are the sum of the single tree complex-
ity. At one hand, we regularize the objective functions in order to control the
complexity oof the addtively trained trees as in xGboost. At another hand, we
want to regularize the whole additive trees in order to keep the balance between
the bias and variances.

8.2.1 Regularized random forests

Regularized random forests are used for feature selection specially in gene se-
lection [27, 60, 79, 72]. And there is its open implementation at https://cran.r-
project.org/web/packages/RRF/index.html

Sparse Projection Oblique Randomer Forests (SPORF)[78] is yet another
decision forest which recursively split along very sparse random projections.
SPORF uses very sparse random projections, i.e., linear combinations of a small
subset of features Its oﬃcial web is https://neurodata.io/sporf/.

8.2.2 Regularized Boosted Trees

We can construct the regularzied boosting machines based on trees such as
[49, 17, 16].

In https://arxiv.org/abs/1806.09762, we regularize gradient boosted trees
by introducing subsampling and employ a modiﬁed shrinkage algorithm so that
at every boosting stage the estimate is given by an average of trees.

9 Conditional computation

Conditional Computation refers to a class of algorithms in which each input
sample uses a diﬀerent part of the model, such that on average the compute,
latency or power (depending on our objective) is reduced. To quote Bengio et.
al

Conditional computation refers to activating only some of the units
in a network, in an input-dependent fashion. For example, if we
think we’re looking at a car, we only need to compute the activations
of the vehicle detecting units, not of all features that a network could
possible compute. The immediate eﬀect of activating fewer units is
that propagating information through the network will be faster,
both at training as well as at test time. However, one needs to be
able to decide in an intelligent fashion which units to turn on and
oﬀ, depending on the input data. This is typically achieved with

35

some form of gating structure, learned in parallel with the original
network.

Another natural property in trees is conditional computation, which refers to
their ability to route each sample through a small number of nodes (speciﬁcally,
a single root-to-leaf path). Conditional computation can be broadly deﬁned as
the ability of a model to activate only a small part of its architecture in an
input-dependent fashion.

Conditional Networks [47] is a fusion of conditional computation with rep-
resentation learning and achieve a continuum of hybrid models with diﬀerent
ratios of accuracy vs. eﬃciency.

9.1 Unbiased recursive partitioning

Wei-Yin Loh and Yu-Shan Shih present an algorithm called QUEST that has
negligible bias in [62].

A uniﬁed framework for recursive partitioning is proposed in [44, 75], which
embeds tree-structured regression models into a well deﬁned theory of condi-
tional inference procedures.

We focus on regression models describing the conditional distribution of a
response variable Y given the status of m covariates by means of tree-structured
recursive partitioning. The response Y from some sample space Y may be
multivariate as well. The m covariates X = (X1, · · · , Xm) are element of a
sample space X = X1 × · · · Xm. We assume that the conditional distribution
D(Y | X) of the response Y given the covariates X depends on a function f of
the covariates

D(Y | X) = D(y | X1, · · · , Xm) = D(y | f (X1, · · · , Xm))

where we restrict ourselves to partition based regression relationships. A re-
gression model of the relationship is to be ﬁtted based on a learning sample
{Y, X1, · · · , Xm}.

The following generic algorithm implements unbiased recursive binary par-

titioning:

1. For case weights w test the global null hypothesis of independence between
any of the m covariates X and the response Y . Stop if this hypothesis can
not be rejected. Otherwise select the j∗th covariate Xj∗ with strongest
association to Y .

2. Choose a set A∗ ⊂ X in order to split Xj∗ into two disjoint sets A∗ and

A∗ − Xj∗ .

3. Repeat recursively steps 1 and 2.

Each node of a tree is represented by a vector of case weights having non-
zero elements when the corresponding observations are element of the node and
are zero otherwise.

36

We need to decide whether there is any information about the response vari-
able covered by any of the m covariates. The fundamental problem of exhaustive
search procedures have been known for a long time is a selection bias towards
covariates with many possible splits or missing values.
Here are the R packages on recursive partitioning
https://www.rdocumentation.org/packages/partykit/versions/1.2-11.

9.2 Bonzai

Ashish Kumar, Saurabh Goyal, and Manik Varma develop a tree-based algo-
rithm called ‘Bonzai’

T (x) =

(cid:88)

k

Ik(x)W T

k Zx ◦ tanh(σV T

k Zx)

(23)

where ◦ denotes the element-wise Hadamard product, σ is a user tunable hyper-
parameter, Z is a sparse projection matrix and Bonsai’s tree is parameterized
by Ik , Wk and Vk where Ik(x) is an indicator function taking the value 1 if
node k lies along the path traversed by x and 0 otherwise and Wk and Vk are
sparse predictors learnt at node k. Bonsai computes Ik by learning a sparse
vector θ at each internal node such that the sign of θT Zx determines whether
point x should be branched to the node’s left or right child. In fact, the indicator
function is relaxed when implemented. A gradient descent based algorithm with
iterative hard threshold (IHT) was found to solve the optimization of Bonzai.

See its implementation in https://github.com/Microsoft/EdgeML.

10 Discussion

Decision trees for regression or classiﬁcation takes diverse forms as shown as
above. We study this ﬁeld from diﬀerent perspectives: ensemble methods,
Bayesian statistics, adaptive computation and conditional computation. It is
a theoretical overview on tree-based models associated with some implementa-
tion of new methods. Decision tree is a fast developing ﬁeld and interactive with
diverse ﬁelds. It seems simple and intuitive while powerful and insightful.

In the end, we identify some trend for future research.

• suﬃcient and compact representation of decision trees;

• combination of decision trees and deep learning;

• interpretation of tree-based models.

References

[1] Randall Balestriero. Neural decision trees. arXiv: Machine Learning, 2017.

37

[2] Yoshua Bengio, Olivier Delalleau, and Clarence Simard. Decision trees do
not generalize to new variations. Computational Intelligence, 26(4):449–
467, 2010.

[3] Kristin P Bennett and Jennifer A Blue. Optimal decision trees. Rensselaer

Polytechnic Institute Math Report, 214:24, 1996.

[4] Dimitris Bertsimas and Jack Dunn. Optimal classiﬁcation trees. Machine

Learning, 106(7):1039–1082, 2017.

[5] Dimitris Bertsimas and Romy Shioda. Classiﬁcation and regression via

integer optimization. Operations Research, 55(2):252–271, 2007.

[6] Christopher M Bishop and Markus Svenskn. Bayesian hierarchical mixtures
of experts. In Proceedings of the Nineteenth conference on Uncertainty in
Artiﬁcial Intelligence, pages 57–64, 2002.

[7] Rafael Blanquero, Emilio Carrizosa, Cristina Molero-Río, and Romero Do-
lores Morales. Sparsity in optimal randomized classiﬁcation trees. European
Journal of Operational Research, pages 255–272, 2019.

[8] Leo Breiman. Wald lecture i: Machine learning. https://www.stat.
berkeley.edu/users/breiman/wald2002-1.pdf. Accessed March 22,
2020.

[9] Leo Breiman. Wald lecture ii: Looking insidee the black box. https://www.
stat.berkeley.edu/users/breiman/wald2002-2.pdf. Accessed March
22, 2020.

[10] Leo Breiman. Bias, variance, and arcing classiﬁers. Technical report, Tech.
Rep. 460, Statistics Department, University of California, Berkeley . . . ,
1996.

[11] Leo Breiman. Arcing the edge. Technical report, Technical Report 486,

Statistics Department, University of California at . . . , 1997.

[12] Leo Breiman. Arcing classiﬁer (with discussion and a rejoinder by the

author). The annals of statistics, 26(3):801–849, 1998.

[13] Leo Breiman. Random forests. Machine learning, 45(1):5–32, 2001.

[14] Miguel A Carreira-Perpinán and Pooya Tavallali. Alternating optimization
of decision trees, with application to learning sparse oblique trees. In Ad-
vances in Neural Information Processing Systems, pages 1211–1221, 2018.

[15] Probal Chaudhuri, Min-Ching Huang, Wei-Yin Loh, and Ruji Yao.
Piecewise-polynomial regression trees. Statistica Sinica, pages 143–167,
1994.

[16] Tianqi Chen and Tong He. Higgs boson discovery with boosted trees.

HEPML@NIPS, pages 69–80, 2014.

38

[17] Tianqi Chen, Tong He, Michael Benesty, Vadim Khotilovich, and Yuan
Tang. Xgboost: extreme gradient boosting. R package version 0.4-2, pages
1–4, 2015.

[18] Hugh Chipman and Robert E McCulloch. Hierarchical priors for bayesian

cart shrinkage. Statistics and Computing, 10(1):17–24, 2000.

[19] Hugh A Chipman, Edward I George, and Robert E McCulloch. Bayesian
Journal of the American Statistical Association,

cart model search.
93(443):935–948, 1998.

[20] Hugh A Chipman, Edward I George, and Robert E McCulloch. Bayesian

treed models. Machine Learning, 48(1-3):299–320, 2002.

[21] Hugh A Chipman, Edward I George, Robert E McCulloch, et al. Bart:
Bayesian additive regression trees. The Annals of Applied Statistics,
4(1):266–298, 2010.

[22] Antonio Criminisi and Jamie Shotton. Decision forests for computer vision
and medical image analysis. Springer Science & Business Media, 2013.

[23] Antonio Criminisi, Jamie Shotton, Ender Konukoglu, et al. Decision forests:
A uniﬁed framework for classiﬁcation, regression, density estimation, man-
ifold learning and semi-supervised learning. Foundations and Trends® in
Computer Graphics and Vision, 7(2–3):81–227, 2012.

[24] Sanjeeb Dash, Dmitry M. Malioutov, and Kush R. Varshney. Learning
interpretable classiﬁcation rules using sequential rowsampling.
In 2015
IEEE International Conference on Acoustics, Speech and Signal Process-
ing, ICASSP 2015, South Brisbane, Queensland, Australia, April 19-24,
2015, pages 3337–3341. IEEE, 2015.

[25] Domenico Dato, Claudio Lucchese, Franco Maria Nardini, Salvatore Or-
lando, Raﬀaele Perego, Nicola Tonellotto, and Rossano Venturini. Fast
ranking with additive ensembles of oblivious and non-oblivious regression
trees. ACM Transactions on Information Systems (TOIS), 35(2):1–31,
2016.

[26] Houtao Deng and C. George Runger. Feature selection via regularized trees.

IJCNN, pages 1–8, 2012.

[27] Houtao Deng and George Runger. Gene selection with guided regularized

random forest. Pattern Recognition, pages 3483–3489, 2013.

[28] David GT Denison, Bani K Mallick, and Adrian FM Smith. A bayesian

cart algorithm. Biometrika, 85(2):363–377, 1998.

[29] David GT Denison, Bani K Mallick, and Adrian FM Smith. Bayesian mars.

Statistics and Computing, 8(4):337–346, 1998.

39

[30] Murat Firat, Guillaume Crognier, Adriana F. Gabor, Yingqian Zhang, and
Cor A. J. Hurkens. Constructing classiﬁcation trees using column genera-
tion. CoRR, abs/1810.06684, 2018.

[31] Devin Francom, Bruno Sansó, Ana Kupresanin, and Gardar Johannesson.
Sensitivity analysis and emulation for functional data using bayesian adap-
tive splines. Statistica Sinica, pages 791–816, 2018.

[32] Yoav Freund and Robert E Schapire. A desicion-theoretic generalization
of on-line learning and an application to boosting. In European conference
on computational learning theory, pages 23–37. Springer, 1995.

[33] JH Fridedman. Multivariate adaptive regression splines (with discussion).

Ann. Statist, 19(1):79–141, 1991.

[34] Jerome H Friedman. Greedy function approximation: a gradient boosting

machine. Annals of statistics, pages 1189–1232, 2001.

[35] Pierre Geurts, Damien Ernst, and Louis Wehenkel. Extremely randomized

trees. Machine Learning, 63(1):3–42, 2006.

[36] P Richard Hahn, Jared S Murray, Carlos M Carvalho, et al. Bayesian
regression tree models for causal inference: regularization, confounding,
and heterogeneous eﬀects. Bayesian Analysis, 2020.

[37] Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, and
Rahul Mazumder. The tree ensemble layer: Diﬀerentiability meets con-
ditional computation. 2020.

[38] Jingyu He, Saar Yalov, and P Richard Hahn. Xbart: Accelerated bayesian

additive regression trees. arXiv preprint arXiv:1810.02215, 2018.

[39] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin
Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, et al. Practical lessons
from predicting clicks on ads at facebook.
In Proceedings of the Eighth
International Workshop on Data Mining for Online Advertising, pages 1–
9, 2014.

[40] Belinda Hernández, Adrian E Raftery, Stephen R Pennington, and An-
drew C Parnell. Bayesian additive regression trees using bayesian model
averaging. Statistics and computing, 28(4):869–890, 2018.

[41] Daniel Ho. Nbdt: Neural-backed decision trees. Master’s thesis, EECS

Department, University of California, Berkeley, May 2020.

[42] Christopher C Holmes and DGT Denison. Classiﬁcation with bayesian

mars. Machine Learning, 50(1-2):159–173, 2003.

[43] Andreas Holzinger. Data mining with decision trees: Theory and applica-

tions. Online Information Review, 39(3):437–438, 2015.

40

[44] Torsten Hothorn, Kurt Hornik, and Achim Zeileis. Unbiased recursive par-
titioning: A conditional inference framework. Journal of Computational
and Graphical Statistics, 15(3):651–674, 2006.

[45] Xiyang Hu, Cynthia Rudin, and Margo Seltzer. Optimal sparse decision
trees. ADVANCES IN NEURAL INFORMATION PROCESSING SYS-
TEMS 32 (NIPS 2019), pages 7265–7273, 2019.

[46] Ziniu Hu, Yang Wang, Qu Peng, and Hang Li. Unbiased lambdamart:
An unbiased pairwise learning-to-rank algorithm. In The World Wide Web
Conference, pages 2830–2836, 2019.

[47] Yani Ioannou, Duncan Robertson, Darko Zikic, Peter Kontschieder, Jamie
Shotton, Matthew Brown, and Antonio Criminisi. Decision forests,
arXiv preprint
convolutional networks and the models in-between.
arXiv:1603.01250, 2016.

[48] Ozan Irsoy, Olcay Taner Yildiz, and Ethem Alpaydin. Soft decision trees.

In International Conference on Pattern Recognition, 2012.

[49] R. Johnson and Tong Zhang. Learning nonlinear functions using regu-
larized greedy forest. Pattern Analysis and Machine Intelligence, IEEE
Transactions , pages 942–954, 2013.

[50] Michael I Jorda. Bayesian learning in probabilistic decision trees. http:
//www.stats.org.uk/bayesian/Jordan.pdf. Accessed March 22, 2020.

[51] Michael I Jordan, Zoubin Ghahramani, and Lawrence K Saul. Hidden
markov decision trees. In Advances in neural information processing sys-
tems, pages 501–507, 1997.

[52] Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts

and the em algorithm. Neural computation, 6(2):181–214, 1994.

[53] Guolin Ke, Qi Meng, Thomas William Finley, Taifeng Wang, Wei Chen,
Weidong Ma, Qiwei Ye, and Tieyan Liu. Lightgbm: a highly eﬃcient
gradient boosting decision tree. pages 3149–3157, 2017.

[54] Igor Kuralenok, Vasilii Ershov, and Igor Labutin. Monoforest framework
for tree ensemble analysis. ADVANCES IN NEURAL INFORMATION
PROCESSING SYSTEMS 32 (NIPS 2019), pages 13780–13789, 2019.

[55] Balaji Lakshminarayanan. Decision trees and forests: a probabilistic per-

spective. PhD thesis, UCL (University College London), 2016.

[56] Jimmy Lin, Chudi Zhong, Diane Hu, Cynthia Rudin, and Margo Seltzer.
Generalized and scalable optimal sparse decision trees. international con-
ference on machine learning, 2020.

[57] Yi Lin and Yongho Jeon. Random forests and adaptive nearest neighbors.

Journal of the American Statistical Association, 101(474):578–590, 2006.

41

[58] Antonio R Linero. A review of tree-based bayesian methods. Communica-

tions for Statistical Applications and Methods, 24(6), 2017.

[59] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In 2008
Eighth IEEE International Conference on Data Mining, pages 413–422.
IEEE, 2008.

[60] Sheng Liu, Shamitha Dissanayake, Sanjay Patel, Xin Dang, Todd Mlsna,
Yixin Chen, and Dawn Wilkins. Learning accurate and interpretable mod-
els based on regularized random forests regression. BMC systems biology,
pages S5–S5, 2014.

[61] Yong Liu, Peilin Zhao, Aixin Sun, and Chunyan Miao. A boosting algo-
rithm for item recommendation with implicit feedback. In Twenty-Fourth
International Joint Conference on Artiﬁcial Intelligence, 2015.

[62] Wei-yin Loh and Yu-shan Shih. Split selection methods for classiﬁcation

trees. 1997.

[63] Weiyin Loh. Classiﬁcation and regression trees. Wiley Interdisciplinary

Reviews-Data Mining and Knowledge Discovery, 1(1):14–23, 2011.

[64] Weiyin Loh. Fifty years of classiﬁcation and regression trees. International

Statistical Review, 82(3):329–348, 2014.

[65] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raﬀaele
Perego, Nicola Tonellotto, and Rossano Venturini. Quickscorer: Eﬃcient
traversal of large ensembles of decision trees. In Joint European Confer-
ence on Machine Learning and Knowledge Discovery in Databases, pages
383–387. Springer, 2017.

[66] José Marcio Luna, Efstathios D Gennatas, Lyle H Ungar, Eric Eaton, Eric S
Diﬀenderfer, Shane T Jensen, Charles B Simone, et al. Building more
accurate decision trees with the additive tree. Proceedings of the National
Academy of Sciences of the United States of America, 116(40):19887, 2019.

[67] Matt Menickelly, Oktay Günlük, Jayant Kalagnanam, and Katya Schein-
berg. Optimal generalized decision trees via integer programming. CoRR,
abs/1612.03225, 2016.

[68] Iman Mossavat and Oliver Amft. Sparse bayesian hierarchical mixture of
experts. In 2011 IEEE Statistical Signal Processing Workshop (SSP), pages
653–656. IEEE, 2011.

[69] Giuseppe Nuti, Lluís Antoni Jiménez Rugama, and Andreea-Ingrid Cross.
A bayesian decision tree algorithm. arXiv preprint arXiv:1901.03214, 2019.

[70] Sergei Popov, Stanislav Morozov, and Artem Babenko. Neural oblivi-
ous decision ensembles for deep learning on tabular data. arXiv preprint
arXiv:1909.06312, 2019.

42

[71] Liudmila Ostroumova Prokhorenkova, Gleb Gusev, Aleksandr Vorobev,
Anna Veronika Dorogush, and Andrey Gulin. Catboost: unbiased boosting
with categorical features. arXiv: Learning, 2017.

[72] Edward Raﬀ, Jared Sylvester, and Steven Mills. Fair forests: Regularized
tree induction to minimize model bias. AIES, pages 243–250, 2018.

[73] Veselin Raychev, Pavol Bielik, and Martin Vechev. Probabilistic model for

code with decision trees. SIGPLAN Not., 51(10):731–747, October 2016.

[74] Robert E Schapire and Yoav Freund. Boosting: Foundations and Algo-

rithms. MIT Press, 2012.

[75] Lisa Schlosser, Torsten Hothorn, and Achim Zeileis. The power of unbiased
recursive partitioning: A unifying view of ctree, mob, and guide. arXiv:
Methodology, 2019.

[76] Carolin Strobl, Anne-Laure Boulesteix, Achim Zeileis, and Torsten
Hothorn. Bias in random forest variable importance measures: Illustra-
tions, sources and a solution. BMC bioinformatics, 8(1):25, 2007.

[77] Ryutaro Tanno, Kai Arulkumaran, Daniel Alexander, Antonio Criminisi,
and Aditya Nori. Adaptive neural trees. In Proceedings of the 36th Inter-
national Conference on Machine Learning (ICML), 2019.

[78] Tyler M. Tomita, James Browne, Cencheng Shen, Jaewon Chung, Jesse L.
Patsolic, Benjamin Falk, Carey E. Priebe, Jason Yim, Randal Burns,
Mauro Maggioni, and Joshua T. Vogelstein. Sparse projection oblique
randomer forests. Journal of Machine Learning Research, 21(104):1–39,
2020.

[79] fatih uzdilli, martin jaggi, zurich eth switzerland, dominic egger, pascal
julmy, leon derczynski, and mark cieliebak. Swiss-chocolate: Combining
ﬂipout regularization and random forests with artiﬁcially built subsystems
to boost text-classiﬁcation for sentiment. SemEval@NAACL-HLT, 2015.

[80] Hélène Verhaeghe, Siegfried Nijssen, Gilles Pesant, Claude-Guy Quimper,
and Pierre Schaus. Learning optimal decision trees using constraint pro-
gramming. In Katrien Beuls, Bart Bogaerts, Gianluca Bontempi, Pierre
Geurts, Nick Harley, Bertrand Lebichot, Tom Lenaerts, Gilles Louppe, and
Paul Van Eecke, editors, Proceedings of the 31st Benelux Conference on Ar-
tiﬁcial Intelligence (BNAIC 2019) and the 28th Belgian Dutch Conference
on Machine Learning (Benelearn 2019), Brussels, Belgium, November 6-8,
2019, volume 2491 of CEUR Workshop Proceedings. CEUR-WS.org, 2019.

[81] Sicco Verwer and Yingqian Zhang. Learning decision trees with ﬂexible
constraints and objectives using integer optimization. In International Con-
ference on AI and OR Techniques in Constraint Programming for Combi-
natorial Optimization Problems, pages 94–103. Springer, 2017.

43

[82] Sicco Verwer and Yingqian Zhang. Learning optimal classiﬁcation trees
using a binary linear program formulation.
In The Thirty-Third AAAI
Conference on Artiﬁcial Intelligence, AAAI 2019, The Thirty-First Inno-
vative Applications of Artiﬁcial Intelligence Conference, IAAI 2019, The
Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence,
EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages
1625–1632. AAAI Press, 2019.

[83] Zeyi Wen, Bingsheng He, Ramamohanarao Kotagiri, Shengliang Lu, and
Jiashuai Shi. Eﬃcient gradient boosted decision tree training on gpus. In
2018 IEEE International Parallel and Distributed Processing Symposium
(IPDPS), pages 234–243. IEEE, 2018.

[84] Yuhong Wu, Håkon Tjelmeland, and Mike West. Bayesian cart: Prior spec-
iﬁcation and posterior simulation. Journal of Computational and Graphical
Statistics, 16(1):44–66, 2007.

[85] Abraham J Wyner, Matthew Olson, Justin Bleich, and David Mease. Ex-
plaining the success of adaboost and random forests as interpolating clas-
siﬁers. The Journal of Machine Learning Research, 18(1):1558–1590, 2017.

[86] Yongxin Yang, Irene Garcia Morillo, and Timothy M Hospedales. Deep

neural decision trees. arXiv: Learning, 2018.

[87] Taner Olcay Yildiz and Ethem Alpaydin. Regularizing soft decision trees.

ISCIS, pages 15–21, 2013.

[88] Heping Zhang and Burton H Singer. Recursive partitioning and applica-

tions. Springer Science & Business Media, 2010.

[89] Heping Zhang and Minghui Wang. Search for the smallest random forest.

Statistics and its interface, pages 381–381, 2009.

[90] Heping Zhang, Chang-Yung Yu, and Burton Singer. Cell and tumor clas-
siﬁcation using gene expression data: construction of forests. Proceedings
of the National Academy of Sciences, 100(7):4168–4172, 2003.

[91] Zhi-Hua Zhou and Wei Tang. Selective ensemble of decision trees. In Inter-
national Workshop on Rough Sets, Fuzzy Sets, Data Mining, and Granular-
Soft Computing, pages 476–483. Springer, 2003.

[92] Zhihua Zhou and Ji Feng. Deep forest: Towards an alternative to deep

neural networks. arXiv: Learning, 2017.

44

