1
2
0
2

c
e
D
7

]
L
M

.
t
a
t
s
[

1
v
8
4
5
3
0
.
2
1
1
2
:
v
i
X
r
a

Private Robust Estimation by Stabilizing Convex Relaxations

Pravesh K. Kothari
praveshk@cs.cmu.edu

âˆ—

Pasin Manurangsi
pasin@google.com

â€ 

Ameya Velingker
ameyav@google.com

â€ 

December 8, 2021

Abstract

(

)

ğœ€, ğ›¿

We give the ï¬rst polynomial time and sample

-diï¬€erentially private (DP) algorithm to
estimate the mean, covariance and higher moments in the presence of a constant fraction of
adversarial outliers. Our algorithm succeeds for families of distributions that satisfy two well-
studied properties in prior works on robust estimation: certiï¬able subgaussianity of directional
moments and certiï¬able hypercontractivity of degree 2 polynomials. Our recovery guarantees
hold in the â€œright aï¬ƒne-invariant normsâ€: Mahalanobis distance for mean, multiplicative
spectral and relative Frobenius distance guarantees for covariance and injective norms for
higher moments. Prior works obtained private robust algorithms for mean estimation of
subgaussian distributions with bounded covariance. For covariance estimation, ours is the ï¬rst
eï¬ƒcient algorithm (even in the absence of outliers) that succeeds without any condition-number
assumptions.

Our algorithms arise from a new framework that provides a general blueprint for modifying
convex relaxations for robust estimation to satisfy strong worst-case stability guarantees in the
appropriate parameter norms whenever the algorithms produce witnesses of correctness in their
run. We verify such guarantees for a modiï¬cation of standard sum-of-squares (SoS) semideï¬-
nite programming relaxations for robust estimation. Our privacy guarantees are obtained by
combining stability guarantees with a new â€œestimate dependentâ€ noise injection mechanism in
which noise scales with the eigenvalues of the estimated covariance. We believe this framework
will be useful more generally in obtaining DP counterparts of robust estimators.

Independently of our work, Ashtiani and Liaw [AL21] also obtained a polynomial time and

sample private robust estimation algorithm for Gaussian distributions.

âˆ—

â€ 

Carnegie Mellon University
Google Research

1

 
 
 
 
 
 
1 Introduction

In this work, we consider the problem of eï¬ƒciently estimating the mean, covariance and, more
generally, the higher moments of an unknown high-dimensional probability distribution on â„ğ‘‘,
â„ğ‘‘, under two design constraints: outlier robustness and privacy.
given a sample ğ‘¦1, ğ‘¦2, . . . , ğ‘¦ğ‘›
The ï¬rst demands that we build estimators for such basic parameters of probability distributions
that tolerate a ï¬xed (dimension-independent) constant fraction of adversarial outliers in the input
data. The second demands that our estimators preserve the privacy of individual points ğ‘¦ğ‘–s (that
we model as being contributed by diï¬€erent individuals) participating in our input data.

âˆˆ

Sans privacy constraints, the problem of robustly estimating the basic parameters of an
unknown distribution has been the focus of intense research in algorithmic robust statistics
starting with the pioneering works of [DKK+16, LRV16] from 2016.
In addition to new
(and often, information-theoretically optimal) algorithms for several basic robust estimation
tasks [KS17b, KS17a, HL18, BK20a, DHKK20], this line of work has led to a deeper understanding
of the properties of the underlying distribution (algorithmic certiï¬cates of analytic properties such
as subgaussianity, hypercontractivity and anti-concentration, resilience [SCV18]) that make robust
estimation possible along with general frameworks such as outlier ï¬ltering and the sum-of-squares
(SoS) method for attacking algorithmic problems in robust statistics.

Sans outlier robustness constraints, the task of private estimation of the mean and covariance
of probability distributions has also seen considerable progress in the recent years. Diï¬€erential
privacy [DMNS06] has emerged as a widely-used standard for providing strong individual privacy
guarantees. Under diï¬€erential privacy, a single sample is not allowed to have too signiï¬cant of an
impact on the output distribution of an algorithm that operates on a dataset. Diï¬€erential privacy
has now been deployed in a number of production systems, including those at Google [EPK14,
BEM+17], Microsoft [DKY17], Apple [Gre16, App17], and the US Census Bureau [Abo18]. While
initial approaches to estimating the mean and covariance under diï¬€erential privacy required a
priori bounds on the support of the samples, a more recent work [KV18] managed to obtain the
ï¬rst private mean estimation algorithm for samples with unbounded support. Subsequent works
have built on this progress to obtain diï¬€erentially private algorithms for mean estimation and
covariance estimation (under assumptions on the condition number of the unknown covariance)
of Gaussian and heavy-tailed distributions [KLSU19, BS19, BKSW19, CWZ19, BDKU20, KSU20,
DFM+20, WXDX20, AAK21, BGS+21].

In this paper, we focus on the task of ï¬nding eï¬ƒcient estimation algorithms for mean, covariance
and, more generally, higher moments with recovery guarantees in multiplicative spectral distance
(i.e., an aï¬ƒne invariant guarantee necessary, for example, to whiten the data or put a set of
points in approximate isotropic position) and relative Frobenius distance (necessary for obtaining
total variation close estimates of an unknown high-dimensional Gaussian). A very recent work
of Liu, Kong, Kakade and Oh [LKKO21] found the ï¬rst private and robust algorithm for mean
estimation under natural distributional assumptions with bounded covariance. However, their
techniques do not appear to extend to covariance estimation. Informally, this is because in order to
obtain privacy guarantees, we need robust estimation algorithms that are stable, i.e., whose output
suï¬€ers from a bounded perturbation when a single data point is changed arbitrarily. When the

2

unknown covariance is bounded, one can eï¬€ectively assume that the change in a single data point
is bounded. However, in general, the covariance of the unknown distribution can be exponentially
(in the underlying dimension) varying eigenvalues which precludes such a method (even in the
outlier-free regime).

This work In this paper, we give the ï¬rst algorithms for diï¬€erentially private robust moment
estimation with polynomial time and sample complexity. Our algorithms, in fact, provide a gen-
eral blueprint for transforming any robust estimation algorithm into a diï¬€erentially private robust
moment estimation algorithm with similar accuracy guarantees as long as the robust estimation
algorithm satisï¬es two key properties: 1) the algorithm is â€œwitness-producing,â€ i.e., the algorithm
ï¬nds a sequence of â€œweightsâ€ on the input corrupted sample that induce a distribution with a
relevant property of the unknown distribution family (such as certiï¬able subgaussianity or hyper-
contractivity) and 2) the algorithm allows for ï¬nding weights that minimize a natural strongly
convex objective function in polynomial time. Such properties are naturally satisï¬ed by robust esti-
mation algorithms based on sum-of-squares semideï¬nite programs. Our main technical result is a
simple framework that transforms such an algorithm into one that satisï¬es worst-case stability under
input perturbation in the relevant norms on the parameters. The ï¬nal ingredient in our framework is
a new noise injection mechanism that uses the stability guarantees so obtained to derive privacy
guarantees. This mechanism allows obtaining privacy guarantees even though the distribution of
the noise being added depends on the unknown quantity being estimated. In particular, such a
subroutine allows us to obtain private robust covariance estimation without any assumptions on
the condition number. We note that even without the robustness constraints, a private covariance
estimation algorithm without any assumptions on the condition number was not known prior to
our work.

Robustness implies privacy? Our blueprint presents an intuitively appealing pictureâ€”that ro-
bustness, when obtained by estimators that satisfy some additional but generic conditions, implies
privacy via a generic transformation. This connection might even appear natural: privacy follows
by â€œadding noiseâ€ to the estimates obtained via algorithms that are insensitive or stable with
respect to changing any single point in the input, while robustness involves ï¬nding estimators
that are insensitive to the eï¬€ects of even up to a constant fraction of outliers. Despite this apparent
similarity, there are two key diï¬€erences that prevent such an immediate connection from being
true: 1) privacy is a worst-case guarantee while robustness guarantees are only sensible under
distributional assumptions, and, 2) privacy guarantees need insensitivity even against â€œinliers.â€
Nevertheless, our main result shows that robustness, when obtained via algorithms that satisfy
some natural additional conditions, does yield stable (or insensitive) algorithms as required for
obtaining diï¬€erentially private algorithms.

In what follows, we describe our results and techniques in more detail.

3

1.1 Our Results

Formally, our results provide diï¬€erentially private robust estimation algorithms in the strong
contamination model, which we deï¬ne below.

Deï¬nition 1.1 (Strong Contamination Model). Let ğœ‚ > 0 be the outlier rate. Given a distribution ğ·
on â„ğ‘‘ and a parameter ğ‘›
â„•, the strong contamination model with outlier rate ğœ‚ gives access to
â„ğ‘‘, an i.i.d. sample from ğ· of size
a set ğ‘Œ
âŠ†
ğ‘›, 2) Return any (potentially adversarially chosen) ğ‘Œ such that
ğ‘›. In this case, we
ğ‘Œ
say that ğ‘Œ is an ğœ‚-corruption of ğ‘‹.

â„ğ‘‘ of ğ‘› points generated as follows: 1) Generate ğ‘‹

1
(

ğ‘‹

>

âˆ©

âˆ’

âŠ†

âˆˆ

ğœ‚

)

|

|

In the context of analyzing privacy, we will say that two subsets of ğ‘› points ğ‘Œ, ğ‘Œâ€² âŠ†

â„ğ‘‘ (a.k.a.
databases) are adjacent if they diï¬€er in exactly one point (i.e
1.) We now present our
main theorem, which provides a diï¬€erentially private robust algorithm for moment estimation of
an unknown certiï¬ably subgaussian distribution in the strong contamination model.

ğ‘Œâ€²|

> ğ‘›

âˆ©

âˆ’

ğ‘Œ

|

ğ‘¡

2

)

(

)

)

h

i

ğ‘¥

ğœ‡

âˆ’

ğ·

, ğ‘£

ğ”¼ğ·
(

ğ‘¡ where ğœ‡

Our formal guarantees hold for moment estimation of certiï¬ably subgaussian distributions.
2ğ‘¡ 6

A distribution ğ· is ğ¶-subgaussian if for any direction ğ‘£ and any ğ‘¡
is the mean of the distribution ğ·. Certiï¬able subgaussianity
ğ·
ğ¶ğ‘¡
(
is a stricter version of such a property that additionally demands that the diï¬€erence between
the two sides of the inequality be a sum-of-squares (SoS) polynomial in the variable ğ‘£. Gaussian
distributions, uniform distributions on product domains, all strongly log-concave distributions and,
more generally, any distribution that satisï¬es a PoincarÃ© inequality with a dimension-independent
constant [KS17a] are known to satisfy certiï¬able subgaussianity. See Deï¬nition 3.22 and the
preliminaries for a detailed discussion.

â„•, ğ”¼ğ·

, ğ‘£

ğ·

âˆ’

ğœ‡

âˆˆ

ğ‘¥

h

i

)

)

(

(

Our ï¬rst result is an algorithm for moment estimation of certiï¬ably subgaussian distributions

that runs in polynomial time and has polynomial sample complexity.

â„•. Then, there exists an ğœ‚0 > 0 such that for any given outlier
Theorem 1.2. Fix ğ¶0 > 0 and ğ‘˜
rate 0 < ğœ‚ 6 ğœ‚0 and ğœ€, ğ›¿ > 0, there exists a randomized algorithm Alg that takes an input of ğ‘› >

âˆˆ

4

ln

1
ğ›¿
)ğœ€
/

(

+

2ğ‘˜
ğ‘˜
1

âˆ’

ln

1
ğ›¿
)ğœ€
/

(

ğ‘›0 =

Î©

ğ‘‘4ğ‘˜
ğœ‚2

1

+
(cid:16)
9
ğœ€ +

(cid:18)

(cid:18)
3
3 ln
(
/
e
ğœ€

ğ›¿

(cid:16)
1), runs in time
2ğ¶0 +
)
(
â„šğ‘‘, Ë†
Î£
either â€œrejectâ€ or estimates
ğœ‡
Ë†
âˆˆ
2ğ‘˜) satisfying the following guarantees:

(cid:17)
ğµğ‘›

+

âˆˆ

(cid:17)

ğ¶4ğ‘˜ ğ‘˜4ğ‘˜

6
+

points ğ‘Œ =

ğ‘¦1, ğ‘¦2, . . . , ğ‘¦ğ‘›

â„šğ‘‘ (where ğ¶ =

(cid:19)
ğ‘‚
(

(cid:19)

Â·
ğ‘˜
) (where ğµ is the bit complexity of the entries of ğ‘Œ) and outputs
ğ‘‘ (for all even ğ‘¡ < 2ğ‘˜ such that ğ‘¡ divides

} âŠ†

â„šğ‘‘

{

ğ‘‘

ğ‘¡

Ã—

Ã—Â·Â·Â·Ã—

)
â„šğ‘‘, and Ë†ğ‘€(

) âˆˆ

1. Privacy: Alg is

ğœ€, ğ›¿

-diï¬€erentially private with respect to the input ğ‘Œ, viewed as a ğ‘‘-dimensional
)

(

database of ğ‘› individuals.

2. Utility: Let ğ‘‹ =

{

with mean ğœ‡

ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘›

distribution
2âˆ’
ğ‘Œ =
is an ğœ‚-corruption of ğ‘‹, then with probability at least 9
/
and random choices of the algorithm, Alg does not reject and outputs estimates
and Ë†ğ‘€(

be an i.i.d. sample of size ğ‘› > ğ‘›0 from a certiï¬ably ğ¶0-subgaussian
for ğ‘¡ > 2. If
10 over the draw of ğ‘‹
ğ‘‘,
â„šğ‘‘, Ë†
Î£

ğ‘‘ (for all ğ‘¡ < 2ğ‘˜ such that ğ‘¡ divides 2ğ‘˜) satisfying the following guarantees:

)ğ¼, and moment tensors ğ‘€(
âˆ—

ğ’Ÿ
ğ‘¦1, ğ‘¦2, . . . , ğ‘¦ğ‘›

, covariance Î£

âˆ— (cid:23)

â„šğ‘‘

â„šğ‘‘

Ã—Â·Â·Â·Ã—

poly

ğœ‡
Ë†

âˆˆ

âˆˆ

{

}

}

Ã—

Ã—

ğ‘‘

ğ‘‘

âˆ—

)

(

ğ‘¡

ğ‘¡

) âˆˆ

ğ‘¢

âˆ€

âˆˆ

â„ğ‘‘ ,

ğœ‡
h Ë†

âˆ’

, ğ‘¢

ğœ‡

âˆ—

i

6 ğ‘‚

âˆšğ¶ ğ‘˜
(

2ğ‘˜
1
ğœ‚1
/
âˆ’
)

ğ‘¢âŠ¤Î£
âˆ—

ğ‘¢ ,

4

p

and,

1

ğ‘‚

((

âˆ’

ğ¶ ğ‘˜

)

ğ‘¡

2ğ‘˜
/

1
ğœ‚1
/
âˆ’

ğ‘˜

Î£
)

Î£
âˆ— (cid:22) Ë†

(cid:22)

and, for all even ğ‘¡ < 2ğ‘˜ such that ğ‘¡ divides 2ğ‘˜,

(cid:16)

(cid:17)

1

ğ‘‚

((

+

ğ¶ ğ‘˜

ğ‘¡

2ğ‘˜
/

ğ‘˜

1
ğœ‚1
/
âˆ’
)

)

,

Î£
âˆ—

(cid:17)

(cid:16)

1

ğ‘‚

(

âˆ’

ğ¶ ğ‘˜

ğ‘¡

2ğ‘˜
/

ğœ‚1
âˆ’
)

(cid:16)

(cid:17)

ğ‘¢ âŠ—

h

ğ‘¡, ğ‘€(

ğ‘¡
)
âˆ— i

6

ğ‘¢ âŠ—

ğ‘¡

ğ‘¡ , Ë†ğ‘€(

)

h

i

6

1

ğ‘‚

(

+

ğ¶ ğ‘˜

ğ‘¡

2ğ‘˜
/

ğœ‚1
âˆ’
)

(cid:16)

ğ‘¢ âŠ—

h

ğ‘¡ , ğ‘€(

ğ‘¡
)
âˆ— i

.

(cid:17)

In the above and subsequent theorems, we use the
ğœ€, and ln

ğ›¿

factors in ğ‘‘, ğ¶, ğ‘˜, 1
ğœ‚, 1
/
/

1
/
(

.
)

Î© notation to hide multiplicative logarithmic

e

Discussion Our algorithm above achieves an error guarantee in the â€œrightâ€ aï¬ƒne-invariant norms
similar to the robust moment estimation algorithm of [KS17b].
In particular, the error in the
mean in any direction scales proportional to the variance of the unknown distribution providing
recovery error bounds in the strong â€œMahalanobis error.â€ Similarly, the error in the covariance is
multiplicative in the LÃ¶wner ordering. Our algorithm succeeds in the standard word RAM model
of computation.
In particular, the lower bound assumption on the eigenvalue of the unknown
covariance in the statement above is entirely an artifact of numerical issues. Such an assumption
can be removed (and in particular, we can deal with rank deï¬cient covariances) if we assume that
the unknown covariance Î£
has rational entries with polynomial bit complexity. We choose to
âˆ—
make an assumption on the smallest eigenvalue of Î£
âˆ—

for the sake of simpler exposition.

Our algorithm above is obtained by applying a general blueprint that applies to any robust
estimation algorithms that use â€œone-shot roundingâ€ to produce a diï¬€erentially private version. We
explain our general blueprint in more detail in Section 2.

Applications Our diï¬€erentially private moment estimation algorithm immediately allows us
to obtain a diï¬€erentially private mechanism to implement an outlier-robust method of moments.
This allows us to learn parameters of statistical models that rely on the method of moments,
such as mixtures of spherical Gaussians with linearly independent means [HK13] (that rely on
decomposing 3rd moments) as well as independent component analysis [DLCC07] (that relies on
decomposing fourth moments). We direct the reader to the work on robust moment estimation
that details such applications [KS17b].

Covariance estimation in relative Frobenius error The above theorem provides a multiplicative
spectral guarantee. Such a guarantee, however, only yields a dimension-dependent bound on the
Frobenius norm of the error. While this is provably unavoidable for the class of certiï¬ably sub-
gaussian distributions, recent work [BK20b] showed that for distributions that satisfy the stronger
property of having certiï¬ably hypercontractive degree 2 polynomials (informally speaking, this
is the analog of certiï¬able subgaussianity for moments of degree 2 polynomials instead of linear
of the random variable ğ‘¥), one can obtain a dimension-independent bound on the
polynomials
Frobenius estimation error that vanishes as the fraction of outliers tends to zero. Their algorithm
relies on rounding an SoS relaxation with a slightly diï¬€erent constraint system. By working with

ğ‘¥, ğ‘£

h

i

5

their constraint system and applying our blueprint for obtaining a â€œstableâ€ version, we obtain a
version of the above theorem with the stronger Frobenius estimation guarantee (see Theorem 5.6).
By combining our privacy analysis above with the recent work that shows that the algorithm
in [BK20b] gives optimal estimation error when analyzed for corrupted samples from a Gaus-
sian distribution, we obtain the following stronger guarantees for private mean and covariance
estimation for Gaussian distributions.

Theorem 1.3 (Mean and Covariance Estimation for Gaussian Distributions). Fix ğœ€, ğ›¿ > 0. Then,
there exists an absolute constant ğœ‚0 > 0 such that for any given outlier rate 0 < ğœ‚ 6 ğœ‚0, there exists a
randomized algorithm Alg that takes an input of ğ‘› > ğ‘›0 =
â„šğ‘‘, runs in

points ğ‘Œ

Î©

1

ln

4

(

ğ‘‘8
ğœ‚4

(cid:18)

(cid:16)

+

1
ğ›¿
)ğœ€
/

(cid:19)

(cid:17)

âŠ†

(

ğ‘‚

time
ğœ‡
Ë†

1
) (where ğµ is the bit complexity of the entries of ğ‘Œ) and outputs either â€œrejectâ€ or estimates
ğµğ‘›
(
)
â„šğ‘‘ and Ë†
Î£
âˆˆ
1. Privacy: Alg is

-diï¬€erentially private with respect to the input ğ‘Œ, viewed as a ğ‘‘-dimensional
)

ğ‘‘ with the following guarantees:

ğœ€, ğ›¿

â„šğ‘‘

e

âˆˆ

Ã—

(

database of ğ‘› individuals.

{

2. Utility: Let ğ‘‹ =
with mean ğœ‡
probability at least 9
/
Î£
Ë†

â„šğ‘‘

Ã—

âˆ—

âˆˆ

and covariance Î£

ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘›

be an i.i.d. sample of size ğ‘› > ğ‘›0 from a Gaussian distribution
)ğ¼ such that ğ‘Œ is an ğœ‚-corruption of ğ‘‹. Then, with
2âˆ’
â„šğ‘‘ and

10 over the random choices of the algorithm, Alg outputs estimates

}
âˆ— (cid:23)

poly

ğ‘‘

(

ğœ‡
Ë†

âˆˆ

ğ‘‘ satisfying the following guarantees:

â„ğ‘‘,

ğ‘¢

âˆ€

âˆˆ

ğœ‡
h Ë†

âˆ’

, ğ‘¢

ğœ‡

âˆ—

i

log

ğ›¿

)

1
(
/
ğœ€

Â·

ğ‘¢âŠ¤Î£
âˆ—

ğ‘¢ ,

(cid:19) p

6

ğ‘‚

ğœ‚

(cid:18)

e

and,

2
1
Î£âˆ’
/
âˆ—

2
1
Î£Î£âˆ’
/
Ë†
âˆ—

ğ¼

âˆ’

ğ‘‚

ğœ‚

ğ¹ (cid:22)

log

ğ›¿

)

1
(
/
ğœ€

,

!

Â· r

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

e

ğœ‚. In particular, ğ‘‘TV
ğ‘‚ hides multiplicative logarithmic factors in 1
where the
/
ğ‘‚
1
ğœ‚ log
ğ›¿
/
(
(
e

.
)

)/

ğœ€

Î£
ğœ‡, Ë†
)

(ğ’©( Ë†

,

ğœ‡

ğ’©(

âˆ—

, Î£

âˆ—))

<

1.2 Related Work

e

Since the works of [DKK+16, LRV16], there has been a spate of works designing additional ro-
bust estimation algorithms for a wide variety of problems, including mean and covariance esti-
mation [DKK+17a, DKK+17b, CDGW19, DHL19, HLZ20, Hop20, LY20], mixture models [HL18,
KSS18, BK20c, DHKK20, BDH+20], principal component analysis (PCA) [KSKO20, JLT20], etc. (see
survey [DK19] for details on recent advances in robust statistics). Furthermore, the criterion of
reslience formulated in [SCV18] as a suï¬ƒcient condition for robustly learning a property of a dataset
was subsequently generalized in [ZJS19] in order to deal with a more general class of perturbations.
In the setting of high-dimensional parameter estimation, release of statistics can often reveal
signï¬cant information about individual data points, which can be problematic in a number of
applications in which it is desirable to protect the privacy of individuals while still providing useful
aggregate information (e.g., medical data or census data). Attacks exploiting such properties have

6

 
been investigated in a long line of works [DN03, BUV14, DSS+15, SU15, DSSU17, SSSS17]. In light
of such exploits, there has been much interest in designing statistical algorithms that protect the
privacy of individual samples in a dataset.

In the area of diï¬€erentially privacy, various works have explored private estimation per-
taining to Gaussian mixtures [NRS07, KSSU19], identity testing [CKM+20], Markov random
ï¬elds [ZKKW20], etc.

Concurrent related works The problem of private robust mean and covariance estimation has
been the subject of great interest resulting in a few concurrent and independent related works.
Kamath, Mouzakis, Singhal, Steinke, and Ullman [KMS+21] give a diï¬€erentially private (in the
outlier-free regime) algorithm for mean and covariance estimation of Gaussian without making
condition number assumptions on the covariance. The work of Liu, Kong, and Oh [LKO21]
gives a statistical feasibility of private robust estimation with optimal sample complexity via a
computationally ineï¬ƒcient algorithm. Finally, Hopkins, Kamath, and Majid [HKM21] also use
the sum-of-squares semideï¬nite programs to obtain private mean estimation (in the outlier-free
setting) algorithm for bounded covariance distribution with pure diï¬€erential privacy. Our result are
most directly related to the work of Asthiani and Liaw [AL21] that also obtains eï¬ƒcient private
and robust mean and covariance estimation for Gaussian distributions.

2 Technical Overview

In this section, we give a high-level overview of our general blueprint for obtaining diï¬€erentially
private versions of robust estimation algorithms. As a running example, we will focus on the
problem of obtaining private and robust mean and covariance estimators. Speciï¬cally, our goal is
â„ğ‘‘, along with an outlier
to design an algorithm that takes input consisting of ğ‘› points, say ğ‘Œ
ğœ€, ğ›¿
rate ğœ‚ and returns estimates of the mean and covariance. We would like the algorithm to be
-
)
diï¬€erentially private for every ğ‘Œ (i.e., a â€œworst-caseâ€ guarantee), viewed as a database in which
each ğ‘‘-dimensional point in ğ‘Œ is contributed by an individual. We would like the outputs of the
algorithm to provide faithful estimates whenever ğ‘Œ is an ğœ‚-corruption of a i.i.d. sample from a
distribution that has ğ¶-subgaussian fourth moments.

âŠ†

(

For the purpose of the ï¬rst part of this overview, we recommend the reader to ignore the
distinction between certiï¬able subgaussianity and â€œvanillaâ€ subgaussianity. Recall that a distri-
4 6
bution ğ· on â„ğ‘‘ has ğ¶-subgaussian fourth moments if for every ğ‘£
4ğ¶
from a ğ¶-subgussian distribution has 2ğ¶-subgaussian fourth moments.

2. It turns out that the uniform distribution on a ğ‘‚
)

ğ·
ğ·
(
âˆ¼
size i.i.d. sample ğ‘‹

â„ğ‘‘, ğ”¼ğ‘¥
ğ‘‘2

ğ”¼ğ‘¥
(

, ğ‘£

, ğ‘£

ğ·

âˆ’

âˆ’

ğœ‡

ğœ‡

âˆˆ

ğ‘¥

ğ‘¥

ğ·

h

i

h

i

âˆ¼

)

(

)

(

)

2

Stable robust estimation algorithms
In order to design diï¬€erentially private algorithms, we
need to ï¬nd robust moment estimation algorithms that are stable. Speciï¬cally, a robust moment
estimation algorithm Alg is stable if the outputs of Alg on any pair of adjacent inputs ğ‘Œ, ğ‘Œâ€² (i.e.,
inputs that diï¬€er in at most one point but arbitrarily so) are close. Such a guarantee must hold
over worst-case pairs ğ‘Œ, ğ‘Œâ€²â€”in particular, ğ‘Œ may not be obtained by taking an ğœ‚-corruption of an

7

i.i.d. sample from a distribution following our assumptions. This presents a problem at the outset
as robust moment estimation algorithms are typically analyzed under distributional assumptions.
The work of [LKKO21] addresses this issue by â€œopening upâ€ an iterative ï¬lter based algorithm for
robust moment estimation and eï¬€ectively making every step of the algorithm stable.

2.1 A Prototypical Robust Estimator to Privatize

To understand our ideas, it is helpful to work with a â€œprototypicalâ€ but ineï¬ƒcient robust estimation
algorithm that we can eventually swap with an eï¬ƒcent one. Let us thus start with a simple (but
ineï¬ƒcient) robust estimation algorithm that we call Alg in the discussion below.

Algorithm 2.1. Input: ğ‘Œ =

ğ‘¦1, ğ‘¦2, . . . , ğ‘¦ğ‘›

{

} âŠ†

â„ğ‘‘ and outlier rate ğœ‚ > 0.

Output: Estimates

Î£ of mean and covariance or â€œreject.â€

ğœ‡, Ë†
Ë†

Operation:

1. Find a witness set of ğ‘› points ğ‘‹â€² âŠ†
subgaussian fourth moments and

âˆ©
2. Return the mean and covariance of ğ‘‹â€².

|

â„ğ‘‘ such that the uniform distribution on ğ‘‹â€² has
ğ‘Œ

ğ‘›. Reject if no such ğ‘‹â€² exists.

>

ğœ‚

ğ‘‹â€²|

1
(

âˆ’

)

âˆˆ

Observe that the property of having subgaussian fourth moments requires verifying an inequal-
â„ğ‘‘, and, in general, there is no eï¬ƒcient (or even sub-exponential time) algorithm
ity for every ğ‘£
known (or expected, modulo the small-set expansion hypothesis) for this problem. Nevertheless,
in [KS17b] (see Section 2), the authors prove that a variant of the above program (which we discuss
this at the end of this overview) produces estimates that are guaranteed to be close to the mean and
covariance of ğ· if ğ‘Œ is an ğœ‚-corruption of an i.i.d. sample ğ‘‹ from ğ·. Note that, though ineï¬ƒcient,
such a result is suï¬ƒcient to establish statistical identiï¬ability of mean and covariance of ğ· from
ğ‘‚ğœ‚(
samples. The closeness guarantees in [KS17b] hold from a more general and basic result
that is useful to us in this exposition, which we note below:

ğ‘‘2

)

Fact 2.2 (See Section 2 of [KS17b], Parameter Closeness from Total Variation Closeness). Sup-
pose ğ·, ğ·â€² are two distributions such that 1) both have subgaussian fourth moments and 2) the to-
6
tal variation distance between ğ·, ğ·â€² is at most ğ›½. Then, for every ğ‘£
âˆˆ
Î£
ğ‘‚
ğ‘£âŠ¤(
ğ·
(
) +
4
ğœ‚3
that the means (covariances, respectively) of ğ·, ğ·â€² are close to within ğ‘‚
/
)
(
Mahalanobis distance, to summarize such a guarantee.

ğ·
ğ·â€²) âˆ’
ğœ‡
h
(
(
ğ·â€²))
ğ‘£. We will say
2
ğœ‚1
, respectively) in
/
)
(

Î£
ğ‘£ and ğ‘£âŠ¤(
(

â„ğ‘‘,
Î£
(
(ğ‘‚

Î£
ğ‘£âŠ¤(
(

ğ·â€²) âˆ’

ğ‘£ 6 ğ‘‚

4
ğœ‚3
/
(

ğ·â€²))

âˆšğœ‚
(

)
p

Î£
(

Î£
(

) +

, ğ‘£

ğ·

ğ·

))

ğœ‡

i

)

)

This fact eï¬€ectively says that if two distributions both have bounded fourth moments and
happen to be close in total variation distance, then their parameters (mean and covariance) must
be close. In fact, the closeness is in strong aï¬ƒne-invariant normsâ€”often called the Mahalanobis
distance for mean and covariance.

8

2.2 Robustness Implies Weak Stability of Alg with a Randomized Outlier Rate

Let us now consider the stability of the above ineï¬ƒcient algorithm. We are seemingly in trouble
at the outset: as written, there must be two adjacent ğ‘Œ, ğ‘Œâ€² such that Alg rejects on ğ‘Œ but not on
ğ‘Œâ€². Let us introduce our ï¬rst simple idea and show how to patch the algorithm to prevent it from
displaying such â€œdrasticâ€ change in its behavior.

Randomizing the outlier rate The following is a simple but useful observation: If Alg does not
reject on input ğ‘Œ with outlier rate ğœ‚, then, Alg must also not reject on ğ‘Œâ€² outlier rate ğœ‚
ğ‘›.
To see why, let ğ‘‹ be the set of points with subgaussian fourth moments that intersects ğ‘Œ in
ğ‘› points. Then, since ğ‘Œ and ğ‘Œâ€² diï¬€er in at most one point, ğ‘Œâ€² must intersect ğ‘‹ in at least
1
)
(
ğ‘› points. Thus, if, instead of a ï¬xed outlier rate ğœ‚, we ran Alg above with
ğ‘›
1
(
)
an appropriately â€œrandomizedâ€ outlier rate, we might expect the rejection probabilities of Alg on
ğ‘Œ, ğ‘Œâ€² to be similar. Such an argument can be made formal with a simple truncated Laplace noise
injection procedure.

ğœ‚
âˆ’
ğœ‚
âˆ’

1
/

1
/

1 =

1
(

âˆ’ (

))

+

âˆ’

+

ğ‘›

ğœ‚

âˆ©

1,

ğ‘Œâ€² is of size ğ‘›

Robustness implies weak stability in Mahalanobis norms We now address the issue of whether
the estimates computed on ğ‘Œ and ğ‘Œâ€² (assuming Alg does not reject on either of ğ‘Œ, ğ‘Œâ€²) are close.
We ï¬rst observe that the fact that Alg is outlier-robust already guarantees a weak stability property.
Speciï¬cally, suppose ğ‘‹ , ğ‘‹â€² are the sets of size ğ‘› generated by Alg when run on inputs ğ‘Œ, ğ‘Œâ€². Then,
since ğ‘Œ
1. Next, observe that intersection bound above
âˆ’
is equivalent to the uniform distributions on ğ‘‹ , ğ‘‹â€² having a total variation distance of at most
close in the
2ğœ‚
relative Mahalanobis distance deï¬ned above. Observe that this argument gives stability properties
in the right norms directly! However, this is a weak stability guarantee since it only provides a
ï¬xed constant distance guarantee instead of ğ‘œğ‘›
that one might expect given that ğ‘Œ and ğ‘Œâ€² diï¬€er
1
)
(
in at most 1 out of ğ‘› points. Nevertheless, our discussion shows that robustness, via the ineï¬ƒcient
algorithm above, immediately implies weak stability.

ğ‘›. Thus, from Fact 2.2, we know that the parameters of ğ‘‹ , ğ‘‹â€² are ğ‘‚

ğœ‚ğ‘‚
(

ğ‘‹â€²|

1
))

1
/

1
(

2ğœ‚

ğ‘‹

>

âˆ©

âˆ’

+

âˆ’

ğ‘›

)

|

(

2.3 A Simple Private Robust Mean Estimator from Weak Stability

Can we derive private algorithms from the weak stability guarantees? If the unknown covariance
happens to be spherical (i.e., has all of its eigenvalues equal to each other), then the Mahalanobis
distance guarantees are in fact equivalent (up to constant factor scaling) to Euclidean distance
guarantees. As a result, simply adding Gaussian noise calibrated to the sensitivity bounds yields
a private robust mean estimation algorithm! Indeed, 1) randomizing the outlier rate, 2) working
with the SoS relaxation of the above program and 3) adding Gaussian noise to the resulting
estimate, immediately yields a simple, straightforward private robust mean estimator that gives
essentially optimal sample complexity guarantees (i.e., matching those of the known non-private
robust estimators).

9

Weak stability is not enough for covariance estimation The challenge in using weak stability
to obtain private robust covariance estimators arise when the covariance is non-spherical (e.g.,
is rank deï¬cient or has eigenvalues of vastly diï¬€erent scales), in which case our Mahalanobis or
multiplicative spectral stability guarantee does not translate into Euclidean/spectral norm distance
guarantees. In particular, if we were to add Gaussian noise, we would end up scrambling all small
eigenvalues up and end up with no non-trivial recovery guarantee.

Indeed, the aforementioned challenge necessitates a rethink of noise injection mechanisms for
covariance estimation in generalâ€”standard noise addition mechanisms do not appear meaningful
in faithfully preserving eigenvalues of diï¬€erent scales. Prior works (e.g., [KLSU19]) deal with this
by iteratively computing some approximate preconditioning matrices. We have not investigated
robust variants of their method. We instead explore one-shot, blackbox noise injection mechanisms
that still provide us the right guarantees for covariance estimation.

2.4 Noise Injection in Estimate-Dependent Norms

If we wanted to faithfully preserve all eigenvalues (of varying scales) of the unknown covariance,
a natural mechanism would be to add noise linearly transformed with respect to the computed estimate.
Î£ is the computed estimate, we would like to consider the mechanism that returns
For example, if Ë†
2 where ğ‘ is a matrix of random Gaussians. The upshot of such a mechanism is that
Î£1
2ğ‘ Ë†
Î£
/
Ë†
Î£ğ‘£ is
Î£â€”directions where ğ‘£âŠ¤ Ë†
it adds noise that is scaled relative to the eigenvalues of the estimate Ë†
small get a smaller additive noise as against directions where the same quadratic form is large.

Î£1
/
+ Ë†

However, the distribution of the added noise in this mechanism depends on the non-privately

estimated quantity itself. Thus, a priori, it provides no useful privacy guarantee!

Key Observation: Nevertheless, our main idea to rescue the above plan is to note that the
mechanism above does indeed provide meaningful privacy guarantees (by standard computations
from the celebrated Gaussian mechanism) if we are able to guarantee that on any adjacent inputs
ğ‘Œ, ğ‘Œâ€², the non-privately computed estimates are ğ‘œğ‘›
close in relative Frobenius distance! This
follows from elementary arguments and is presented in Lemmas 4.20 and 4.21.

1
)
(

â†’ âˆ

The observation above crucially needs the distance between covariances (in relative Frobenius
norm) to tend to 0 as ğ‘›
; in fact, we need the rate to be inverse polynomial to achieve
polynomial sample complexity. Our weak stability guarantee above, however, guarantees only a
2
ğœ‚1
weak ğ‘‚
bound on multiplicative spectral distance which translates into a relative Frobenius
/
)
(
2âˆšğ‘‘
ğœ‚1
bound of ğ‘‚
.
/
â†’ âˆ
(
Thus, in order to use the above mechanism for covariance estimation, we must come up with
signiï¬cantly stronger (and asymptotically vanishing) stability guarantees. Let us investigate how
to obtain such guarantees next.

â€”not only does this not tend to 0 as ğ‘›
)

but it, in fact, explodes as ğ‘‘

â†’ âˆ

2.5 Strong Stability for Robust Estimation Algorithms

Lack of stability because of multiple diï¬€ering solutions There is an important barrier that
prevents Alg from oï¬€ering the strong stability guarantees we need in the covariance estimation
mechanism above. Consider the case when ğ‘Œ is an i.i.d. sample from a one-dimensional standard

10

)

ğœ‚

for a small enough constant ğ‘ is ğœ‚-close in total variation distance to

Gaussian distribution with mean 0 and variance 1 without any outliers added to it. Then,
Â±
0, 1
ğ‘ğœ‚
. By a straighforward
)
argument, this implies that we can choose ğ‘‹â€² to be an i.i.d. sample of size ğ‘› from
â€”
)
if ğ‘› is large enough, then ğ‘‹â€² will have subgaussian fourth moments and will intersect ğ‘Œ in
ğ‘› points. The two diï¬€erence distributions (and the corresponding samples ğ‘‹â€²) however,
1
(
have variances diï¬€ering by an additive ğ‘‚
â€”a ï¬xed constant independent of the sample size ğ‘›.
ğœ‚
)
(
This shows that even in one dimension, Alg has feasible solutions with variance both
))
. Observe that this issue concerns the output of Alg itself, which can belong to a range
and 1
)
that is signiï¬cantly larger than what we can tolerateâ€”we have not yet touched upon the issue of
what happens when we change ğ‘Œ to an adjacent ğ‘Œâ€².

0, 1

ğœ‚
(

ğœ‚
(

1
(

ğ’©(

ğ’©(

ğ’©(

0, 1

ğ‘ğœ‚

ğ‘‚

ğ‘‚

âˆ’

+

âˆ’

Â±

)

In order to modify Alg to output a canonical solution
Convexiï¬cation and entropy surrogates
(and with an eye for satisfying the stronger stability property), we wish to make the feasible solution
space of Alg belong to a convex set (instead of the discrete set of solutions ğ‘‹â€² that intersect with ğ‘Œ
ğ‘› points). With no fear of computational complexity, this is easy to do in a canonical way:
in
we search instead for a probability distribution over ğ‘‹â€² that satisfy the constraints that Alg imposes.
Unlike ğ‘‹â€², distributions on ğ‘‹â€² that satisfy the constraints are easily seen to form a convex set.

1
(

âˆ’

ğœ‚

)

Given such a convex set, we can resolve our diï¬ƒculty of not having canonical solutions for any
given ğ‘Œ by simply ï¬nding a solution (i.e., a probability distribution ğœ over ğ‘‹â€²) that minimizes an
appropriate strongly convex objective function. Speciï¬cally, for any ğ‘‹â€², let ğ‘¤ğ‘– be the 0-1 indicator
of those indices ğ‘– where ğ‘¥ğ‘– = ğ‘¦ğ‘–. Then, the constraints in Alg force
ğ‘›, and the
distribution ğœ can be thought to be over

in a natural way.

ğ‘– ğ‘¤ğ‘– >

1
(

âˆ’

ğœ‚

)

ğ‘‹â€², ğ‘¤

(

)

Ã

(

)

ğ‘¤

ğ‘‹â€², ğ‘¤

In order to ensure that Alg ï¬nds a canonical solution, a natural idea is to search over distri-
while minimizing some strongly convex function. We choose the simplest:
1. We think of this objective as a surrogate for ï¬nding â€œmaximum entropy solutionsâ€ as,
as deï¬ning a probability distribution over ğ‘¦ğ‘–, minimizing the â„“2 norm favors
2
2 is a convex function being minimized over
should

butions ğœ over
2
ğ”¼ğœ[
2
k
]k
when viewing ğ”¼
â€œspread-outâ€ or high entropy solutions. Since
convex set of expectations with respect to ğœ, we expect that the minimizing solution ğ”¼ğœ
be unique.

ğ”¼ğœ[

ğ‘¤ğ‘–

Ëœğœ[

]k

ğ‘¤

ğ‘¤

âˆ—[

k

]

]

This is not immediately true, however, as our Alg as stated outputs the mean of ğ‘‹â€² (there could

be â€œmultipleâ€ ğ‘‹â€² with the same intersection with ğ‘Œ, in principle).

Modifying the output of Alg In order to ï¬t our framework better, we modify the above blueprint
in Alg to instead output the weighted average of points in ğ‘Œ instead of ğ‘‹â€². While such a procedure
is not directly analyzed in [KS17b], the methods there can be naturally adapted without much
hiccup. As a result we obtain the following modiï¬ed version of Alg that we can now work with:

1The exponent of the polynomials appearing in our sample complexity bounds improve if we use a strongly convex
log ğ‘‘. Our interest is in presenting a general â€œprivatizingâ€
1
/

function with respect to 1-norm such as
blueprint so we continue with the simpler choice above in this work.

2
ğ‘ for ğ‘ = 1

+

ğ‘¥

k

k

11

Algorithm 2.3. Input: ğ‘Œ =

ğ‘¦1, ğ‘¦2, . . . , ğ‘¦ğ‘›

{

} âŠ†

â„ğ‘‘ and an outlier rate ğœ‚ > 0.

Output: Estimates

Î£ of mean and covariance or â€œreject.â€

ğœ‡, Ë†
Ë†

Operation:

1. Find a probability distribution ğœ over a witness set of ğ‘› points ğ‘‹â€² âŠ†

â„ğ‘‘ and inter-
section indicator ğ‘¤
ğ‘‹â€², ğ‘¤
)
such that 1) the uniform distribution on ğ‘‹â€² has subgaussian fourth moments and 2)

2
2 and is supported on

ğ‘› that minimizes

ğ”¼ğœ[

0, 1

âˆˆ {

]k

ğ‘¤

}

k

(

ğ‘– ğ‘¤ğ‘– >
2. Return
Ã

)

1
ğœ‚
âˆ’
(
ğœ‡ = 1
ğ‘
Ë†

ğ‘›. Reject if no such ğœ exists.
Î£ = 1
ğ‘– ğ”¼ğœ[
ğ‘

ğ‘– ğ”¼ğœ[

ğ‘¦ğ‘–, Ë†

ğ‘¤ğ‘–

ğ‘¤ğ‘–

ğ‘– ğ”¼ğœ[
With this modiï¬cation, Alg outputs a canonical single solution on any given ğ‘Œ (or rejects).

)âŠ¤ where ğ‘ =
ğœ‡
âˆ’ Ë†

ğœ‡
âˆ’ Ë†

Ã

Ã

Ã

ğ‘¦ğ‘–

ğ‘¦ğ‘–

](

)(

]

ğ‘¤ğ‘–

.

]

Stability of Alg from the stability of the entropy potential We now return to the issue of
stability. What happens if we switch the input ğ‘Œ of Alg above to ğ‘Œâ€²? The strongly convex objective
we imposed in the above discussion comes in handy here! Namely, by basic convex analysis
(see Proposition 3.20), it follows that if optimum entropy potential values of Alg on ğ‘Œ and ğ‘Œâ€²
-close, then, the vectors ğ”¼ğœ[
are say, ğ‘‚
close. Recall
1
)
(
that each ğ”¼ğœ[
ğ‘¤ğ‘–
0, 1
is a number in
and that these numbers add up to 1. Hence, intuitively
]
2
ğ”¼ğœ[
speaking, ğ‘‚
2 corresponds to constant perturbation in a constant number
-closeness of
1
)
(
of coordinates.

are themselves ğ‘‚

and ğ”¼ğœ[

ğ‘Œâ€²)
](

1
)
(

[
ğ‘¤

]k

ğ‘¤

ğ‘¤

](

ğ‘Œ

k

]

)

Thus, working with the strongly convex objective above reduces our stability analysis of Alg to
simply understanding how much can our entropy potential change when changing a single point
in ğ‘Œ.

Unfortunately, this change can be large in general.
ğœ‚

]k
2ğ‘›. The additive diï¬€erence between these two extremes is ğ‘‚
)

ğ‘¤

k

ğ”¼ğœ[

1
(

âˆ’

2
2 varies between
ğ‘‚
ğœ‚ğ‘›
(

.
1
)
(

) â‰«

1
(

ğœ‚

)

âˆ’

ğ‘› and

Stabilizing the entropy potential: private stable selection Before describing our key idea, we
ï¬rst make a simple observation: Fix an input ğ‘Œ and consider the optimum value of the entropy
potential of Alg when run with outlier rate ğœ‚. What happens if we change ğœ‚ to ğœ‚
ğ‘›? Clearly, the
potential cannot increase: any solution ğœ with outlier rate ğœ‚ is also a solution for outlier rate ğœ‚
ğ‘›.
The potential can decrease arbitrarily though.

1
/

1
/

+

+

More speciï¬cally, we show the following: in order to make the entropy potential stable under
such that the
is within an

a change of ğ‘Œ to an adjacent ğ‘Œâ€², it is enough to run ğ‘Œ with an outlier rate ğœ‚â€² = ğ‘‚
ğœ‚
(
)
entropy potential of Alg on ğ‘Œ for any outlier rate in the interval
ğ‘›
ğ¿
ğ‘›, ğœ‚â€² +
/
additive

of any other.

ğœ‚â€² âˆ’
[

ğ‘‚

ğ‘›

ğ¿

ğ¿

/

]

e

To see why this claim could be true, informally speaking, observe that if ğ‘Œâ€² is obtained from
ğ‘Œ by changing at most a single point, then a solution ğœ with outlier rate ğœ‚â€² can be modiï¬ed into a
ğ‘› by simplying zeroing out the ğ‘¤ğ‘– for the index ğ‘– where ğ‘Œâ€²
solution ğœâ€² for ğ‘Œâ€² with outlier rate ğœ‚â€² +
and ğ‘Œ diï¬€er. This allows us to relate the potentials for neighboring outlier rates on ğ‘Œ and ğ‘Œâ€². Under

1
/

(

/

)

12

the above assumption, the potential remains stable in an interval around ğœ‚â€² on ğ‘Œ. This allows us
to conclude that the same must be true for ğ‘Œâ€² for the interval

ğ‘›

ğ¿

ğ‘›

ğ¿

.

ğœ‚â€² âˆ’
[

/

+

1, ğœ‚â€² +

/

âˆ’

1
]

The above reasoning allows us to obtain strong stability guarantees if we can 1) show that a

stable interval as above exists and 2) ï¬nd such an interval via a stable process.

A stable selection procedure via the exponential mechanism We show that a stable interval as
above (for ğ¿ =
) exists via a simple Markov-like argument. Using an appropriate scoring rule,
1
)
(
we show that the standard exponential mechanism can then be used to produce a stable interval
like above via a stable algorithm (see Section 3.6.4).

ğ‘‚ğ‘›

e

ğ‘¤ğ‘–

Putting things together Altogether, we obtain a version of Alg that outputs a sequence of weights
(i.e., ğ”¼ğœ[
) that are stable under the modiï¬cation of a single point in ğ‘Œ. When viewed as a
âˆšğ‘›
distribution on ğ‘Œ, the stability guarantee we obtain corresponds to an â„“1-stability of
)
compared to the ğ‘‚
(a ï¬xed constant) stability that follows from any naive robust estimation
algorithm.

1
/
(

ğœ‚
(

ğ‘‚

e

]

)

We note that
ğ‘¥

function

k

k

âˆšğ‘›
ğ‘‚
1
/
(
2
ğ‘ for ğ‘ = 1
e

)
+

can be upgraded to
1
/

log ğ‘›.

e

ğ‘‚

ğ‘›

1
/
(

)

if we work with a more sophisticated potential

By applying Fact 2.2, we immediately get that if Alg does not reject on ğ‘Œ, ğ‘Œâ€², then the parameters
of the respective inputs must be close in the Mahalanobis distance up to a polynomially vanishing
function of ğ‘›, as desired. This allows us to implement the estimate-dependent noise injection
mechanism for covariance estimation!

We note that the discussion above can be formalized into an information-theoretic private identi-
ï¬ability algorithm (i.e., an ineï¬ƒcient private robust algorithm). We next discuss how to transform
the above blueprint result into an eï¬ƒcient algorithm.

2.6 From Ideal Algorithms to Eï¬ƒcient Algorithms

Let us now go back and summarize 1) facts about the idealized ineï¬ƒcient algorithm and 2) our
general blueprint for making such an algorithm Alg private.

1. Witness Production: We have used that the fact that Alg searches over witnesses ğ‘‹â€² that
share the relevant property of the distributional model we have chosen (e.g., subgaussianity
of fourth moments in the above discussion).

2. Strongly Convex Entropy Potential: We have minimized a strongly convex potential function

in order to ensure that Alg outputs a canonical solution.

3. Stable Outlier Rate Selection: We have implemented a randomized stable selection scheme
(via the exponential mechanism) for the outlier rate in order to argue that the optimum
entropy potential of Alg is stable under the modiï¬cation of a single point in the input ğ‘Œ.

We can apply this scheme to any algorithm that outputs a sequence of weights on the input sam-
ple ğ‘Œ, subject to the constraint that 1) the weights induce the relevant property of the distributional
model, and 2) they minimize a strongly convex potential function.

13

Witness-producing SoS-based robust estimation algorithms
It turns out that we can ensure
all the above properties for eï¬ƒcient robust estimation algorithms based on â€œone-shot roundingâ€
of convex relaxations. We speciï¬cally rely on the algorithms for robust estimation based on SoS
semideï¬nite programs in this work.

The SoS-based algorithms in the prior works that we use [BK20a, KS17b] almost ï¬t our

requirements except with two technical constraints:

1. The algorithms in the aforementioned prior works do not output weights on ğ‘Œ explicitly.
However, we are able to show that a natural modiï¬cation that outputs such weights on ğ‘Œ can
be analyzed by the same methods.

2. The algorithms in the aforementioned prior works were analyzed under distributional as-
sumptions on ğ‘Œ without the need to explicitly argue that the weights induce good witnesses
Indeed, arguing that these algorithms produce
(which we desire in our above analysis).
such witnesses on worst-case datasets ğ‘Œ (whenever they donâ€™t reject) appears challenging.
However, we are able to get by without such a statement by observing that we can adapt
the analyses of the algorithms in the prior works to infer the following statement:
if the
algorithm returns a good witness on ğ‘Œ, then under a small perturbation of the parameters,
it must also return a good witness on an adjacent ğ‘Œâ€².

While verifying the properties makes our transformation not entirely blackbox at the moment,
we strongly believe that our blueprint demonstrates a conceptually appealing connection between
robust algorithm design and private algorithm design. Concretly, we expect our blueprint to
be useful in designing more private (and robust) estimation algorithms. Indeed, we believe our
techniques immediately extend to other problems where SoS-based robust estimation algorithms
are known, such as linear regression [KKM18, BP20] and clustering spherical and non-spherical
mixtures [DHKK20, BK20a, HL18, KS17c, FKP19].

3 Preliminaries

In this work, we will deal with algorithms that operate on numerical inputs. In all such cases,
we will rely on the standard word RAM model of computation and assume that all the numbers
are rational represented as a pair of integers describing the numerator and the denominator. In
order to measure the running time of our algorithms, we will need to account for the length of the
numbers that arise during the run of the algorithm. The following deï¬nition captures the size of
the representations of rational numbers:

Deï¬nition 3.1 (Bit Complexity). The bit complexity of an integer ğ‘
complexity of a rational number ğ‘

â„¤ is 1
. The bit
â„¤ is the sum of the bit complexities of ğ‘ and ğ‘.

ğ‘ where ğ‘, ğ‘

log2 ğ‘

+ âŒˆ

âˆˆ

âŒ‰

/

âˆˆ

For any ï¬nite set ğ‘‹ of points in â„ğ‘‘, we will use ğœ‡

)(
covariance and the ğ‘¡-th moment tensor of the uniform distribution on ğ‘‹.

, Î£
(

, ğ‘€(

ğ‘‹

ğ‘‹

)

)

(

ğ‘¡

to denote the mean,

ğ‘‹

)

14

3.1 Pseudo-Distributions

Pseudo-distributions are generalizations of probability distributions and form dual objects to sum-
of-squares proofs in a precise sense that we will describe below.

Deï¬nition 3.2 (Pseudo-distribution, Pseudo-expectations, Pseudo-moments). A degree-â„“ pseudo-
2 >
distribution is a ï¬nitely-supported function ğ· : â„ğ‘›
)
0 for every polynomial ğ‘“ of degree at most â„“

ğ‘¥
(
2. (Here, the summations are over the support of ğœ‡.)
The pseudo-expectation of a function ğ‘“ on â„ğ‘‘ with respect to a pseudo-distribution ğ·, denoted

â„ such that

= 1 and

ğ‘¥ ğ·

ğ‘¥ ğ·

â†’

Ã

Ã

ğ‘¥

ğ‘¥

/

(

)

(

)

ğ‘“

ğ”¼ğ·

ğ‘¥

(

)

ğ‘“

ğ‘¥

, as
)

(

e

ğ”¼ğ·

ğ‘¥

(

)

ğ‘“

ğ‘¥

(

)

=

ğ·

ğ‘¥

ğ‘“

ğ‘¥

(

)

(

)

.

(3.1)

ğ‘¥

= ğ‘¥, i.e., ğœ‡

Ã•ğ‘¥
In particular, the mean ğœ‡ of a pseduo-distribution is deï¬ned naturally as the pseudo-expectation of
ğ‘“

)
The degree-â„“ moment tensor of a pseudo-distribution ğœ‡ is the tensor ğ”¼ğœ‡

â„“ .
)âŠ—
In particular, the moment tensor has an entry corresponding to the pseudo-expectation of every
monomial of degree at most â„“ in ğ‘¥.

1, ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘›

ğ”¼ğ·

ğ‘¥.

e

e

)(

(

ğ‘¥

ğ‘¥

(

)

(

Observe that if a pseudo-distribution ğœ‡ satisï¬es, in addition, that ğœ‡

> 0 for every ğ‘¥, then
it is a mass function of some probability distribution. Further, a straightforward polynomial-
pseudo-distribution satisï¬es ğœ‡ > 0 and is thus
interpolation argument shows that every degree-
an actual probability distribution. The set of all degree-â„“ moment tensors of probability distribution
is a convex set. Similarly, the set of all degree-â„“ moment tensors of degree-ğ‘‘ pseudo-distributions
is also convex.

âˆ

ğ‘¥

)

(

We now deï¬ne what it means for

ğ”¼ to (approximately) satisfy constraints.

{

e

Deï¬nition 3.3 (Satisfying constraints). For a polynomial ğ‘”, we say that a degree-ğ‘˜
exactly if for every polynomial ğ‘ of degree 6 ğ‘˜
ğ‘” = 0
constraint
}
ğ”¼
ğ‘
ğ‘ğ‘” ğ‘—
approximately if
[
|
every polynomial ğ‘ of degree 6 ğ‘˜
(cid:13)
(cid:13)
2
ğ”¼
2.
(cid:13)
(cid:13)

ğ”¼
,
deg
ğ‘”
[
)
(
ğ‘” > 0
ğ”¼ satisï¬es the constraint
{
e
ğ”¼

2. We say that
deg
2
ğ‘”
(

2, it holds that

6 ğœ

ğ‘2ğ‘”

ğ‘2ğ‘”

)/

(cid:13)
(cid:13)

(cid:13)
(cid:13)

e

e

e

>

âˆ’

âˆ’

âˆ’

]|

ğœ

ğ‘

/

ğ”¼ satisï¬es the
= 0 and ğœ-
ğ‘ğ‘”
]
e
exactly if for
}
> 0 and ğœ-approximately if

The following fact describes the precise sense in which pseudo-distributions are duals to sum-

[

]

]

[

e
of-squares proofs.

Fact 3.4 (Strong Duality, [JH16], see Theorem 3.70 in [FKP19] for an exposition). Let ğ‘1, ğ‘2, . . . , ğ‘ğ‘˜
be real-coeï¬ƒcient polynomials in ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘›. Suppose there is a degree-ğ‘‘ sum-of-squares refutation of
ğ‘–6ğ‘˜ . Then, there is no pseudo-distribution ğœ‡ of degree > ğ‘‘ satisfying
ğ‘–6ğ‘˜ .
the system
)
ğ‘–6ğ‘˜ .
On the other hand, suppose that there is a pseudo-distribution ğœ‡ of degree ğ‘‘ consistent with
)
ğ‘– ğ‘¥2
Suppose further that the set
ğ‘– for some ğ‘… > 0.
Then, there is no degree-ğ‘‘ sum-of-squares refutation of the system

contains the quadratic polynomial ğ‘…
ğ‘–6ğ‘˜ .

ğ‘1, ğ‘2, . . . , ğ‘ğ‘˜

> 0
> 0

> 0

> 0

ğ‘ğ‘–
ğ‘ğ‘–

ğ‘¥
ğ‘¥

{
{

}
}

(
(

ğ‘ğ‘–

ğ‘ğ‘–

âˆ’

ğ‘¥

ğ‘¥

}

}

{

{

(

)

{

(

)

}

Ã

Basic sum-of-squares (SoS) proofs

15

Fact 3.5 (Operator norm Bound). Let ğ´ be a symmetric ğ‘‘
and denominators upper-bounded by 2ğµ and ğ‘£ be a vector in â„ğ‘‘. Then, for every ğœ€ > 0,

Ã—

ğ‘‘ matrix with rational entries with numerators

ğ‘£
2

ğ‘£âŠ¤ğ´ğ‘£ 6

ğ´

ğ‘£

2
2 +

ğœ€

(cid:8)
The total bit complexity of the proof is poly
(

k
ğµ, ğ‘‘, log 1
/
Fact 3.6 (SoS HÃ¶lderâ€™s Inequality). Let ğ‘“ğ‘– , ğ‘”ğ‘– for 1 6 ğ‘– 6 ğ‘  be indeterminates. Let ğ‘ be an even positive
integer. Then,

(cid:9)

k

k2k
.
ğœ€
)

ğ‘“ ,ğ‘”
ğ‘2

1
ğ‘ 

ğ‘ 

ğ‘“ğ‘–ğ‘”

ğ‘
ğ‘–

ğ‘

1
âˆ’

6

!

1
ğ‘ 

ğ‘ 

ğ‘“

ğ‘
ğ‘–

1
ğ‘ 

!  

ğ‘ 

ğ‘”

ğ‘
ğ‘–

!

ğ‘

1
âˆ’

.

The total bit complexity of the SoS proof is ğ‘ ğ‘‚

ï£¼ï£´ï£´ï£½
ï£´ï£´
ï£¾
Observe that using ğ‘ = 2 yields the SoS Cauchy-Schwarz inequality.

ï£±ï£´ï£´ï£²
ï£´ï£´
ï£³

Ã•ğ‘–=1

Ã•ğ‘–=1

Ã•ğ‘–=1

).

ğ‘

(

Fact 3.7 (SoS Almost Triangle Inequality). Let ğ‘“1, ğ‘“2, . . . , ğ‘“ğ‘Ÿ be indeterminates. Then,

ï£±ï£´ï£´ï£²
The total bit complexity of the SoS proof is ğ‘Ÿğ‘‚
ï£´ï£´
ï£³

Ã•ğ‘–6ğ‘Ÿ
).

(

ğ‘¡

ğ‘“1 , ğ‘“2 ,..., ğ‘“ğ‘Ÿ
2ğ‘¡

2ğ‘¡

ğ‘“ğ‘–

!

6 ğ‘Ÿ2ğ‘¡

1
âˆ’

ğ‘“ 2ğ‘¡
ğ‘–

ğ‘Ÿ

Ã•ğ‘–=1

.

! ï£¼ï£´ï£´ï£½
ï£´ï£´
ï£¾

Fact 3.8 (SoS AM-GM Inequality, see Appendix A of [BKS15]). Let ğ‘“1, ğ‘“2, . . . , ğ‘“ğ‘š be indeterminates.
Then,

ğ‘“ğ‘– > 0

|

ğ‘– 6 ğ‘š

(cid:8)

(cid:9)

ğ‘“1 , ğ‘“2,..., ğ‘“ğ‘š
ğ‘š

1
ğ‘š

( 

Ã•ğ‘–=1

ğ‘š

ğ‘š

ğ‘“ğ‘–

!

> Î ğ‘–6ğ‘š ğ‘“ğ‘–

.

)

The total bit complexity of the SoS proof is exp
(
We will also use the following two consequence of the SoS AM-GM inequality:

.
))

ğ‘‚

ğ‘š

(

Proposition 3.9. Let ğ‘, ğ‘ be indeterminates. Then,

ğ‘,ğ‘
2ğ‘¡

ğ‘2ğ‘—ğ‘2ğ‘¡

2ğ‘— 6 ğ‘—ğ‘2ğ‘¡
âˆ’

ğ‘¡

+ (

âˆ’

ğ‘2ğ‘¡

ğ‘—

)

.

The total bit complexity of the SoS proof is exp
(
Proof. We apply the SoS AM-GM inequality with ğ‘“ğ‘– = ğ‘2 for ğ‘– = 1, . . . , ğ‘— and ğ‘“ğ‘– = ğ‘2 for ğ‘– = ğ‘—
We thus obtain:

.
))

ğ‘‚

(

ğ‘¡

(cid:9)

(cid:8)

âˆ’
By the SoS Almost Triangle inequality, we have:

/

(

(cid:8)

ğ‘,ğ‘
2ğ‘¡

ğ‘—

ğ‘¡ ğ‘2

1
+ (

ğ‘2

ğ‘—

ğ‘¡

)

/

)

ğ‘¡ > ğ‘2ğ‘—ğ‘2ğ‘¡

2ğ‘—

âˆ’

(cid:9)

1, . . . , ğ‘¡.

+

ğ‘,ğ‘
2ğ‘¡

ğ‘¡ ğ‘2

ğ‘—

(

/

1
+ (

âˆ’

ğ‘—

ğ‘¡

)

/

ğ‘2

)

ğ‘¡ 6

ğ‘—ğ‘2ğ‘¡

(

ğ‘¡

ğ‘—

)

âˆ’

+ (

ğ‘2ğ‘¡

)

Combining the above two claims completes the proof. The total bit complexity of the SoS proof
follows immediately by using the bounds for the two constituent inequalities used in the proof
(cid:3)
above.

(cid:9)

(cid:8)

16

 
 
 
 
Proposition 3.10. Let ğ‘, ğ‘ be indeterminates. Then, for any positive integers ğ‘–, ğ‘¡ such that ğ‘– is odd and
2ğ‘¡ > ğ‘–, we have:

ğ‘ğ‘–ğ‘2ğ‘¡

âˆ’

ğ‘– 6 1
2 (

ğ‘,ğ‘
2ğ‘¡

(cid:26)

ğ‘ğ‘–

1ğ‘2ğ‘¡
âˆ’

ğ‘–

1
+

âˆ’

ğ‘ğ‘–

1ğ‘2ğ‘¡
+

ğ‘–

1
âˆ’

âˆ’

+

.

)

(cid:27)

The total bit complexity of the SoS proof is exp
(
1 for some ğ‘Ÿ > 1. Then, we have: ğ‘ğ‘–ğ‘2ğ‘¡

.
))

ğ‘‚

(

ğ‘¡

Proof. Write ğ‘– = 2ğ‘Ÿ
AM-GM inequality with ğ‘“1 = ğ‘ğ‘Ÿ ğ‘ğ‘¡

âˆ’

ğ‘Ÿ and ğ‘“2 = ğ‘ğ‘Ÿ

1ğ‘ğ‘¡
âˆ’

ğ‘Ÿ

1, we thus have:
+

âˆ’

âˆ’

1 = ğ‘ğ‘Ÿ ğ‘ğ‘¡
âˆ’

ğ‘Ÿ ğ‘ğ‘Ÿ

1ğ‘ğ‘¡
âˆ’

ğ‘Ÿ

1. By the SoS
+

âˆ’

âˆ’

ğ‘ğ‘–ğ‘2ğ‘¡

ğ‘– = ğ‘ğ‘Ÿ ğ‘ğ‘¡

ğ‘Ÿ ğ‘ğ‘Ÿ

1ğ‘ğ‘¡
âˆ’

âˆ’

âˆ’

âˆ’

ğ‘Ÿ

ğ‘,ğ‘
2ğ‘¡

(cid:26)

1 6 1
2 (

+

ğ‘ğ‘–

1ğ‘2ğ‘¡
âˆ’

ğ‘–

1
+

âˆ’

ğ‘ğ‘–

1ğ‘2ğ‘¡
+

ğ‘–

1
âˆ’

âˆ’

+

.

)

(cid:27)

(cid:3)

Fact 3.11 (Cancellation within SoS, Constant RHS [BK20b]). Suppose ğ´ is indeterminate and ğ‘¡ > 1.
Then,

ğ´2ğ‘¡ 6 1

ğ´
2ğ‘¡

ğ´2 6 1

Further, the total bit complexity of the SoS proof is at most 2ğ‘‚

(cid:9)

(cid:8)

(cid:8)

ğ‘¡

(

).

(cid:9)

Lemma 3.12 (Cancellation within SoS [BK20b]). Suppose ğ´ and ğ¶ are indeterminates and ğ‘¡ > 1. Then,

(cid:8)
Further, the total bit complexity of the SoS proof is at most 2ğ‘‚

(cid:9)

(cid:8)
ğ‘¡
).
(

(cid:9)

ğ´ > 0

âˆª

ğ´ğ‘¡ 6 ğ¶ğ´ğ‘¡

1
âˆ’

ğ´,ğ¶
2ğ‘¡

ğ´2ğ‘¡ 6 ğ¶2ğ‘¡

.

3.2 Algorithms and Numerical Accuracy

The following fact follows by using the ellipsoid algorithm for semideï¬nite programming. The
resulting algorithm to compute pseudo-distributions approximately satisfying a given set of poly-
nomial constraints is called the sum-of-squares algorithm.

Fact 3.13 (Computing pseudo-distributions consistent with a set of constraints [Sho87, Par00, Nes00,
â„•, ğœ > 0,
Las01]). There is an algorithm with the following properties: The algorithm takes input ğµ
âˆˆ
and polynomials ğ‘1, ğ‘2, . . . , ğ‘ğ‘˜ of degree â„“ with rational coeï¬ƒcients of bit complexity ğµ.
If there is a
ğ‘–6ğ‘˜ , the algorithm in time
pseudo-distribution of degree ğ‘‘ consistent with the constraints
>
ğµğ‘›
(
0

outputs a pseudo-distribution ğœ‡ of degree ğ‘‘ that ğœ-approximately satisï¬es

1
) poly log
/
(

> 0

ğ‘ğ‘–

ğ‘ğ‘–

ğœ

ğ‘¥

ğ‘¥

{

{

}

ğ‘‚

(

)

)

)

(

ğ‘‘

(

)
ğ‘–6ğ‘˜ .

}

3.3 Tensors

Since we will deal with higher moments of distributions, which are naturally represented as
tensors, we will need to deï¬ne some related notation and conventions for the sake of clarity in our
exposition.
Let

for any natural number ğ‘›. We deï¬ne the following.

1, 2, . . . , ğ‘›

=

ğ‘›

[

]

{

}

17

Deï¬nition 3.14. Suppose we have an ğ‘š
to be the standard ğ‘šğ‘šâ€² Ã—
Moreover, for an ğ‘š
Ã—

ğ‘› matrix ğ‘€, we denote by ğ‘€ âŠ—

ğ‘› matrix ğ‘€ and an ğ‘šâ€² Ã—
ğ‘›ğ‘›â€² matrix given by the Kronecker product of ğ‘€ and ğ‘.

Ã—

ğ‘›â€² matrix ğ‘. We deï¬ne ğ‘€

ğ‘

âŠ—

ğ‘¡ the ğ‘¡-fold Kronecker product ğ‘€

ğ‘€

âŠ—

âŠ— Â· Â· Â· âŠ—

ğ‘€

ğ‘¡ times

(of dimension ğ‘¡ğ‘š

Given an ğ‘š

Ã—

ğ‘¡ğ‘›).

Ã—
ğ‘› matrix ğ‘€, we will also ï¬nd it convenient to index ğ‘€ âŠ—

1 6 ğ‘–1, ğ‘–2, . . . , ğ‘–ğ‘¡ 6 ğ‘š and 1 6 ğ‘—1, ğ‘—2, . . . , ğ‘—ğ‘¡ 6 ğ‘›, we can refer to the term ğ‘€ âŠ—
(

ğ‘¡
ğ‘˜=1 ğ‘€ğ‘–ğ‘¡ ,ğ‘—ğ‘¡ .
We also deï¬ne a useful ï¬‚attening operation on tensors:

|

{z

}
ğ‘¡ as follows: for any
=

ğ‘¡
ğ‘–1,ğ‘–2 ,...,ğ‘–ğ‘¡

ğ‘—1,ğ‘—2 ,...,ğ‘—ğ‘¡

,

)

(

)

Ã
Deï¬nition 3.15. Given an ğ‘š1 Ã—
ğ‘š1ğ‘š2 Â· Â· Â·
to be the
of ğ‘€ appearing in the natural lexicographic order on
ğ‘šğ‘¡
ğ‘š2] Ã— Â· Â· Â· Ã— [
for ğ‘˜ = 1, 2, . . . , ğ‘¡) in vec
entry ğ‘€ğ‘–1,ğ‘–2 ,...,ğ‘–ğ‘¡ appears before ğ‘€ğ‘—1,ğ‘—2 ,...,ğ‘—ğ‘¡ (where ğ‘–ğ‘˜ , ğ‘—ğ‘˜
(
only if there exists some 1 6 ğ‘˜ 6 ğ‘¡ such that ğ‘–ğ‘˜ < ğ‘—ğ‘˜ and ğ‘–ğ‘™ = ğ‘—ğ‘™ for all ğ‘™ < ğ‘˜.

ğ‘šğ‘¡ tensor ğ‘€, we deï¬ne the ï¬‚attening, or vectorization, of ğ‘€
ğ‘€
, whose entries are precisely the entries
(
)
ğ‘š1] Ã— [
. In other words, the
ğ‘šğ‘˜
if and
]
âˆˆ [

-dimensional vector, denoted vec
)

ğ‘š2 Ã— Â· Â· Â· Ã—

ğ‘šğ‘¡

ğ‘€

]

[

)

(

Deï¬nition 3.16. Given an ğ‘›-dimensional vector ğ‘¢ and an ğ‘›

ğ‘›

-dimensional tensor ğ‘€,

ğ‘›

Ã—

Ã— Â· Â· Â· Ã—

ğ‘¡ times

we deï¬ne
h
ğ‘›ğ‘¡-dimensional vectors) between the ï¬‚attenings of ğ‘¢ âŠ—

, vec
(

vec
(

to be

ğ‘¢ âŠ—

ğ‘¢ âŠ—

ğ‘€

h

i

)

ğ‘¡, ğ‘€

ğ‘¡

|
ğ‘¡ and ğ‘€.

)iâ„ğ‘‘, i.e., the value of the standard inner product (on

{z

}

A convenient fact we will use is a so-called â€œmixed productâ€ property for matrices.

Fact 3.17. Given an ğ‘š

ğ‘› matrix ğ´, ğ‘šâ€² Ã—

Ã—

ğ‘›â€² matrix ğµ, and ğ‘›

Ã—

ğ‘›â€² matrix ğ‘‰, we have that

ğ´ğ‘‰ ğµğ‘‡ =

ğ´

(

âŠ—

ğµ

ğ‘‰
vec
(

)

,

)

where the above is expressed as matrix-vector product.

Finally, we deï¬ne the moment tensor for a probability distribution.

Deï¬nition 3.18. Given a probability distribution
the ğ‘¡th moment tensor ğ‘€ to be a ğ‘‘

ğ‘‘

ğ‘‘

on â„ğ‘‘ and an integer ğ‘¡ > 1, we deï¬ne
tensor whose entries are given by ğ‘€ğ‘–1,ğ‘–2 ,...,ğ‘–ğ‘¡ =

ğ’Ÿ

ğ”¼ğ‘‹

ğ‘‹ğ‘–1 ğ‘‹ğ‘–2 Â· Â· Â·

ğ‘‹ğ‘–ğ‘¡ ]

for ğ‘–1, ğ‘–2, . . . , ğ‘–ğ‘¡
|

âˆˆ [

âˆ¼ğ’Ÿ[

3.4 Basic Convexity

Ã—

Ã— Â· Â· Â· Ã—
ğ‘¡ times
ğ‘‘
.
{z
]

}

We will use the following basic propositions about convexity in our analysis.

Proposition 3.19 (Neighborhoods of minimizers of convex functions). Let ğ¾ be a closed convex subset
of â„ğ‘ . Let ğ‘“ be a smooth convex function on â„ğ‘ . Let ğ‘¥ be a minimizer of ğ‘“ on ğ¾. Then, for every ğ‘¦
ğ¾,
ğ‘¦

> 0.

ğ‘¥,

âˆˆ

ğ‘¥

ğ‘“

(

âˆ’

âˆ‡

)i

h
Proof. If not, then for a small enough positive ğœ†,
1
(

ğœ†ğ‘¦

ğ¾.

+

âˆ’

ğœ†

âˆˆ

ğ‘¥

)

18

ğ‘“

ğ‘¥

(

ğœ†

ğ‘¦

(

+

ğ‘¥

))

âˆ’

< ğ‘“

ğ‘¥

. But, ğ‘¥
)

(

ğœ†

ğ‘¦

(

+

ğ‘¥

)

âˆ’

=
(cid:3)

                 
                 
            
            
            
            
Proposition 3.20 (Pythagorean theorem from strong convexity w.r.t 2 norm). Let ğ¾ be a convex
subset of â„ğ‘‘ for ğ‘‘
ğ¾. Then,
ğ‘¦
ğ‘“
ğ‘“

â„•. Let ğ‘¥ be a minimizer of the convex function ğ‘“
ğ‘¥

2
2 on ğ¾. Let ğ‘¦

>

=

âˆˆ

ğ‘¥

ğ‘¦

ğ‘¥

ğ‘¥

k

k

)

(

2
2.

âˆˆ
âˆ’
ğ‘¦

)

(

) âˆ’

(
(cid:13)
Proof. We have:
ğ‘¦
(cid:13)
sition 3.19 to observe that
(cid:13)
(cid:13)

âˆ’
ğ‘¦
h
We will also need the following basic bound:

2
2 + k
ğ‘¥, ğ‘¥
(cid:13)
i
âˆ’
(cid:13)

2
ğ‘¥
2 +
k
> 0.

2
(cid:13)
(cid:13)
2

(cid:13)
(cid:13)

(cid:13)
(cid:13)

âˆ’

=

ğ‘¥

ğ‘¦

2

h

ğ‘¥, ğ‘¥

. The proposition follows by applying Propo-
(cid:3)

i

Lemma 3.21. Suppose ğ‘¥, ğ‘¦
ğ‘¥ = ğ‘¥
ğ‘¥
Â¯
k1
k

ğ‘¦ = ğ‘¦
ğ‘¦1
Â¯
Â¯

and

ğ‘› such that
be the normalized versions of ğ‘¥, ğ‘¦. Then,

0, 1
]

ğ‘– ğ‘¥ğ‘– ,

âˆˆ [

ğ‘– ğ‘¦ğ‘– > ğ‘›

Ã

ğ‘¥
Â¯

ğ‘¦
âˆ’ Â¯

1

Ã
6 6ğ›½ .

2 and

ğ‘¥

/

ğ‘¦

1

âˆ’

6 ğ›½ğ‘› for ğ›½ 6 1
/

10. Let

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Proof. Suppose, without loss of generality, that
ğ‘¦
know that
âˆ’ Â¯
6 6ğ›½.
ğ›½ğ‘›2

= ğ‘2ğ‘› >

ğ‘›. Thus,

ğ‘1 âˆ’

ğ‘¥
Â¯

ğ›½

ğ‘¦

)

(

1

)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
k1 = ğ‘1ğ‘› > ğ‘2ğ‘› =
ğ‘¥
6 1
ğ‘¦
ğ‘¥
(cid:13)
ğ‘1 ğ‘2ğ‘›2 (
1 âˆ’
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

ğ‘¦

k
1

ğ‘¦
ğ‘¥
(cid:13)
(cid:13)

k

(cid:13)
(cid:13)

(cid:13)
(cid:13)

1 for ğ‘1, ğ‘2 > 1
/
6 1
ğ‘1ğ‘›
k1
ğ‘1ğ‘2ğ‘›2 (

1

2. Then, we

ğ‘¦

ğ‘¥

âˆ’

(cid:13)
(cid:13)

1 +
(cid:3)
(cid:13)
(cid:13)

3.5 Certiï¬able Subgaussianity

Deï¬nition 3.22 (Certiï¬able Subgaussianity). A distribution ğ· on â„ğ‘‘ with mean ğœ‡
is said to be 2ğ‘˜-
certiï¬ably ğ¶-subgaussian if there is a degree 2ğ‘˜ sum-of-squares proof of the following polynomial
inequality in ğ‘‘-dimensional vector-valued indeterminate ğ‘£:

âˆ—

ğ”¼
ğ‘¥

âˆ¼

ğ·h

ğ‘¥

, ğ‘£

ğœ‡

âˆ—

i

âˆ’

2ğ‘˜ 6

ğ¶ ğ‘˜

(

ğ‘˜

)

(cid:18)

ğ”¼
ğ‘¥

âˆ¼

ğ·h

2

ğ‘¥

, ğ‘£

ğœ‡

âˆ—

i

âˆ’

ğ‘˜

.

(cid:19)

Furthermore, we say that ğ· is certiï¬able ğ¶-subgaussian if it is 2ğ‘˜-certiï¬ably ğ¶-subgaussian for
every ğ‘˜

â„ğ‘‘ is said to be 2ğ‘˜-certiï¬able ğ¶-subgaussian if the uniform distribution on ğ‘‹

â„•.
A ï¬nite set ğ‘‹

âˆˆ

âŠ†

is 2ğ‘˜-certiï¬ably ğ¶-subgaussian.

0, 1
]

ğ‘› be weight vectors satisfying

Fact 3.23 (Consequence of Theorem 1.2 in [KS17b]). Let ğ‘Œ be a collection of ğ‘› points in â„ğ‘‘. Let
= ğœ. Suppose that the
ğ‘, ğ‘â€² âˆˆ [
distributions on ğ‘Œ where the probability of ğ‘– is ğ‘ğ‘– (ğ‘â€²ğ‘–, respectively) is 2ğ‘˜-certiï¬ably ğ¶1 (ğ¶2, respectively)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
subgaussian. Let ğœ‡ğ‘ =
â„• be
ğ‘¦ğ‘–
for every ğ‘¡
)(
(
ğ‘¡
the mean, covariance and ğ‘¡-th moment tensor of distribution deï¬ned ğ‘. Deï¬ne ğœ‡ğ‘â€² , Î£ğ‘â€² , ğ‘€(
)ğ‘â€²
the distribution corresponding to ğ‘â€².

(cid:13)
(cid:13)
)âŠ¤, and ğ‘€(

âˆˆ
similarly for

1,
(cid:13)
(cid:13)
ğœ‡ğ‘
âˆ’

ğ‘– ğ‘ğ‘– ğ‘¦ğ‘–, Î£ğ‘ =

= 1, and

ğ‘– ğ‘ğ‘– ğ‘¦âŠ—
ğ‘–

(cid:13)
(cid:13)
âˆ’

ğ‘¡
)ğ‘ =

ğ‘– ğ‘ğ‘–

ğœ‡ğ‘

Ã

Ã

Ã

ğ‘â€²

ğ‘â€²

ğ‘¦ğ‘–

(cid:13)
(cid:13)

âˆ’

ğ‘

ğ‘

1

1

ğ‘¡

Then, for every ğœ 6 ğœ‚0 for some absolute constant ğœ‚0, for every ğ‘¢

ğœ‡ğ‘

h

âˆ’

ğœ‡ğ‘â€² , ğ‘¢

i

1
(
ğ‘‚

(

1
(

âˆ’

1
ğœ1
/
âˆ’

ğ‘˜

ğ‘‚

ğ¶â€²ğ‘˜

âˆ’
ğ‘¡
ğ¶â€²

(
2ğ‘˜ğ‘¡
/

2
/

)
ğœ1
âˆ’

ğ‘¡

2ğ‘˜
/

)h

)

6 ğœ1
2ğ‘˜
1
/
âˆ’

ğ‘‚

âˆšğ¶ ğ‘˜
Â·
(
Î£ğ‘â€² (cid:22) (
1
(cid:22)
6
ğ‘¡ , ğ‘€(
ğ‘¢ âŠ—
)ğ‘ i

+
ğ‘¢ âŠ—

)
q
ğ‘‚
ğ¶â€²ğ‘˜
(
ğ‘¡ , Ë†ğ‘€(

Î£ğ‘
)

h

ğ‘¡

)
ğ‘¡
)ğ‘â€² i

â„ğ‘‘, ğ¶â€² = ğ¶1 +

âˆˆ
ğ‘¢âŠ¤Î£ğ‘ğ‘¢

,

)

1
ğœ1
/
âˆ’

ğ‘˜

Î£ğ‘â€² ,
)
ğ‘¡, ğ‘€(
ğ‘¢ âŠ—

ğ‘¡

)ğ‘ i

6

h

ğ¶2 and ğ‘¡ 6 ğ‘˜:

,

19

3.6 Diï¬€erential Privacy

In this section, we state a few tools from diï¬€erential privacy (DP) literature that will be used in our
algorithms. We start by recalling the deï¬nition of DP:

Deï¬nition 3.24 (Diï¬€erential Privacy [DMNS06]). An algorithm
diï¬€erentially private (or
ğ‘Œ, ğ‘Œâ€², we have

-DP) for ğœ€, ğ›¿ > 0 iï¬€, for every ğ‘†
)

ğœ€, ğ›¿

âŠ† ğ’ª

(

:

-
â„³
)
and every neighboring datasets

is said to be

ğ’´ â†’ ğ’ª

ğœ€, ğ›¿

(

â„™

ğ‘Œ

ğ‘†

6 ğ‘’ ğœ€

â„™

ğ‘Œâ€²

ğ‘†

ğ›¿.

Â·

]

) âˆˆ

) âˆˆ

] +

[â„³(

[â„³(
Throughout this work, our set ğ‘Œ will consist of ğ‘¦1, . . . , ğ‘¦ğ‘›

are neighbors iï¬€ they diï¬€er on a single data point, i.e., ğ‘¦â€²ğ‘—

and ğ‘Œâ€² =
= ğ‘¦ğ‘— for all ğ‘— â‰  ğ‘–. Note that
ğ‘¦â€²1, . . . , ğ‘¦â€²ğ‘›)
(
this is the so-called substitution variant of DP; another popular variant is the add/remove DP where
a neighboring ğ‘Œâ€² results from adding or removing an example from ğ‘Œ. We remark that it is not
ğ‘› of
hard to extend our algorithm to the add/remove DP setting, by ï¬rst computing a DP estimate
Ë†
ğ‘› and either throwing away random elements or adding zero vectors to arrive at an ğ‘›-size dataset
on which our algorithm can be applied.

â„ğ‘‘. ğ‘Œ =

ğ‘¦1, . . . , ğ‘¦ğ‘›

âˆˆ

(

)

3.6.1 Laplace Mechanism and Its Variants

The Laplace mechanism [DMNS06] is among the most widely used mechanisms in diï¬€erential
privacy. It works by adding a noise drawn from the Laplace distribution (deï¬ned below) to the
output of the function one wants to privatize.

Deï¬nition 3.25 (Laplace Distribution). The Laplace distribution with mean ğœ‡ and parameter ğ‘ on
ğ‘.
â„, denoted by Lap

ğœ‡

ğ‘¥

âˆ’

|/

ğœ‡, ğ‘
(

, has the PDF 1
)

2ğ‘ ğ‘’âˆ’|

We will also use the â€œtruncatedâ€ version of the Laplace mechanism where the noise distribution
is shifted and truncated to be non-negative. The precise deï¬nition of the noise distribution and
its guarantee is given below. For completeness, we provide the DP analysis (Lemma 3.27) in
Appendix A.1.

Deï¬nition 3.26 (Truncated Laplace Distribution). The (negatively) truncated Laplace distribution
with mean ğœ‡ and parameter ğ‘, denoted by tLap
conditioned on the
value being negative.

ğœ‡, ğ‘
is deï¬ned as Lap
(

ğœ‡, ğ‘
(

)

)

Lemma 3.27 (Truncated Laplace Mechanism). Let ğ‘“ :
Î”. Then the algorithm that adds tLap

Î”

1

ln

(

ğ’´ â†’
ğœ€

, Î”

1
ğ›¿
)ğœ€
/

âˆ’

+

to ğ‘“ satisï¬es

ğœ€, ğ›¿

-DP.
)

(

/

â„ be any function with sensitivity at most

(cid:17)
Finally, we also state a bound on the tail probability of the truncated Laplace distribution which

(cid:16)

(cid:17)

(cid:16)

will be useful in our subsequent analysis.

Lemma 3.28. Suppose ğœ‡ < 0 and ğ‘ > 0. Let ğ‘‹

ğœ‡, ğ‘
tLap
(

. Then, for ğ‘¦ < ğœ‡, we have that
)

âˆ¼

ğ‘’(
2

ğ‘¦

ğœ‡

ğ‘

âˆ’
)/
ğ‘’ğœ‡

/

.

ğ‘

âˆ’

ğ‘‹ < ğ‘¦

â„™

[

=

]

20

3.6.2 Composition Theorem

It will be convenient to also consider DP algorithms whose privacy guarantee holds only against
subsets of inputs. Speciï¬cally, we deï¬ne:

Deï¬nition 3.29 (Diï¬€erential Privacy Under Condition). An algorithm
ğœ€, ğ›¿
(
ğ‘†

-diï¬€erentially private under condition Î¨ (or
)
âŠ† ğ’ª

and every neighboring datasets ğ‘Œ, ğ‘Œâ€² both satisfying Î¨, we have

is said to be
:
-DP under condition Î¨) for ğœ€, ğ›¿ > 0 iï¬€, for every
)

ğ’´ â†’ ğ’ª

ğœ€, ğ›¿

â„³

(

â„™

ğ‘Œ

ğ‘†

]

) âˆˆ

6 ğ‘’ ğœ€

Â·

â„™

[â„³(

ğ‘Œâ€²

ğ›¿.

ğ‘†

] +

) âˆˆ

[â„³(

It is not hard to see that an analogue of the basic composition theorem still holds in this setting,
which we formalize below. We remark that this is similar to the composition theorem derived
in [DL09, Section 5]. However, since our composition theorem is slightly diï¬€erent, we provide its
proof in Appendix A.2.

Lemma 3.30 (Composition for Algorithm with Halting). Let

, . . . ,

ğ‘˜ :

ğ‘˜
ğ’ª2 âˆª {âŠ¥}
proceeds as follows (with ğ‘œ0 being empty): For ğ‘– = 1, . . . , ğ‘˜, compute ğ‘œğ‘– =
and output

. Finally, if the algorithm has not halted, then output ğ‘œğ‘˜.

1 Ã— ğ’´ â†’ ğ’ª
âˆ’

âˆª {âŠ¥}

â„³

ğ’ª

ğ‘˜

be algorithms. Furthermore, let

â„³1 :

,

ğ’´ â†’ ğ’ª1 âˆª {âŠ¥}
â„³
ğ‘œğ‘–
ğ‘–
(

â„³2 :
ğ’ª1 Ã— ğ’´ â†’
denote the algorithm that
and, if ğ‘œğ‘– =
, halt
1, ğ‘Œ
âˆ’

â„³

âŠ¥

)

âŠ¥

Suppose that:

â€¢ For any 1 6 ğ‘– < ğ‘˜, we say that ğ‘Œ satisï¬es the condition Î¨ğ‘– if running the algorithm on ğ‘Œ does not
ğ‘–.

result in halting after applying

â„³1,

â„³2, . . . ,

â„³

-DP.

â€¢

â€¢

ğœ€1, ğ›¿1)
(
ğœ€ğ‘– , ğ›¿ğ‘–

â„³1 is
ğ‘– is
â„³
for all ğ‘– =

(

{

}

-DP (with respect to neighboring datasets in the second argument) under condition Î¨ğ‘–
)
2, . . . , ğ‘˜

.

1
âˆ’

Then,

is

â„³

ğœ€ğ‘– ,

ğ‘–

ğ‘˜

]

âˆˆ[

ğ‘–

ğ‘˜

]

âˆˆ[

(cid:16)Ã

Ã

ğ›¿ğ‘–

-DP.

(cid:17)

3.6.3 Hockey-Stick Divergence

It will be convenient in our analysis to use an equivalent deï¬nition of DP based on the hockey-stick
ğ‘, 0
divergence. For ease of notation, let

for all ğ‘

= max

â„.

ğ‘

[

]+

{

âˆˆ

}
ğ‘¥

Deï¬nition 3.31 (Hockey-Stick Divergence). Let ğ‘
and ğ›¼ a non-negative real number. The Hockey-stick divergence ğ·ğ›¼(
as:

, ğ‘

ğ‘¥

(

(

)

)

)

be probability density functions on â„ğ‘‘,
between ğ‘, ğ‘ is deï¬ned

ğ‘, ğ‘

The following fact is simple to derive from the deï¬nition of DP and is often used in literature.

ğ‘, ğ‘

ğ·ğ‘’ ğœ€

(

=

)

â„ğ‘‘ [

âˆ«ğ‘¥

âˆˆ

ğ‘

ğ‘¥

(

) âˆ’

ğ›¼

ğ‘

ğ‘¥

(

Â·

)]+

ğ‘‘ğ‘¥ .

â„ğ‘‘ be a randomized
-DP from Hockey-Stick Divergence Bounds). Let
Fact 3.32 (
ğœ€, ğ›¿
)
(
-DP under condition Î¨ iï¬€ for any neighboring pair of databases ğ‘Œ, ğ‘Œâ€² both satisfying
ğœ€, ğ›¿
algorithm.
is
)
(
â„³
Î¨, we have ğ·ğ‘’ ğœ€
,
ğ‘Œ
)
(â„³(

ğ‘Œâ€²))

ğ’´ â†’

6 ğ›¿.

â„³(

â„³

:

21

We will need to bound the hockey-stick divergence between two distributions in terms of the
hockey-stick divergences to a third distribution. Unfortunately, the hockey-stick divergence does
not deï¬ne a metric and, therefore, does not admit the usual triangle inequality. However, it is
possible to prove a looser inequality, which we will ï¬nd useful:

Lemma 3.33. Suppose ğ‘

, ğ‘

ğ‘¥

(

)

ğ‘¥

, ğ‘Ÿ

ğ‘¥

are probability density functions on â„ğ‘‘. Then,

)
(
ğ·ğ‘’ ğœ€

(
)
ğ‘, ğ‘Ÿ

6 ğ·ğ‘’ ğœ€

ğ‘, ğ‘

2

ğ‘’ ğœ€

2
/

ğ·ğ‘’ ğœ€

2

ğ‘, ğ‘Ÿ

.

) +
We remark that such a bound is already implicit in the so-called group diï¬€erential privacy (see

(

)

(

(

)

Â·

/

/

e.g. [Vad17, Lemma 2.2]). Nonetheless, we provide a (short) proof in Appendix A.3.

3.6.4 Approximate-DP Selection

Finally, we will also use a DP algorithm for the selection problem, where the goal is to pick from
a (public) set of candidates one which has a high â€œscoreâ€. This problem can be solved using the
exponential mechanism [MT07]. The version of the algorithm we use deviates slightly from this
traditional version in that we also include a check (via truncated Laplace mechanism) to make sure
that the score is at least a certain threshold ğœ…; otherwise, the algorithmâ€™s properties are summarized
below. Its proof is deferred to Appendix A.4.

Theorem 3.34. Suppose ğœ€, ğ›¿
function for candidates as a function of the databases ğ‘Œ
There exists an algorithm Selection that satisï¬es the following properties:

be a set of candidates and let score :

be a scoring
, such that its sensitivity (w.r.t. ğ‘Œ) is at most Î”.

0, 1
]

ğ’ Ã— ğ’´

. Let

âˆˆ ğ’´

âˆˆ (

ğ’

1. Selection is

ğœ€, ğ›¿

-DP.
)

(

2. If the output of Selection is ğ‘âˆ— â‰ 

âŠ¥

, then score
(
> ğœ…

ğ‘, ğ‘Œ

ğ‘âˆ—, ğ‘Œ

> ğœ….

)

3. If there exists ğ‘

âˆˆ ğ’
probability at most ğ›½.

such that score
(

)

ğ‘‚

+

Î”
ğœ€ Â·

(cid:16)

log

|ğ’|ğ›½ğ›¿

, then Selection output

(cid:16)

(cid:17) (cid:17)

with

âŠ¥

4 Diï¬€erentially Private Robust Moment Estimation

In this section, we describe a diï¬€erentially private robust moment estimation algorithm. The
following is our main technical result:

â„•. Then, there
Theorem 4.1 (Diï¬€erentially Private Robust Moment Estimation). Fix ğ¶0 > 0 and ğ‘˜
exists an ğœ‚0 > 0 such that for any given outlier rate 0 < ğœ‚ 6 ğœ‚0 and ğœ€, ğ›¿ > 0, there exists a randomized
ğ‘‘4ğ‘˜
algorithm Alg that takes an input of ğ‘› > ğ‘›0 =
ğœ‚2

ğ¶4ğ‘˜ ğ‘˜4ğ‘˜

points

1
ğ›¿
)ğœ€
/

1
ğ›¿
)ğœ€
/

6
+

2ğ‘˜
ğ‘˜
1

Î©

âˆˆ

1

ln

ln

4

âˆ’

(

(

â„šğ‘‘ (where ğ¶ = ğ¶0 +

+
(cid:18)
(cid:16)
ğµğ‘›
1), runs in time
ğ‘Œ
e
(
â„šğ‘‘, Ë†
Î£
entries of ğ‘Œ) and outputs either â€œrejectâ€ or estimates
ğ‘¡ < 2ğ‘˜ such that ğ‘¡ divides 2ğ‘˜) with the following guarantees2:

9
ğœ€ +

3
(
/
ğœ€

ğœ‡
Ë†

3 ln

âŠ†

+

âˆˆ

(cid:18)

ğ›¿

)

)
âˆˆ

+

Â·
(cid:19)
(cid:17)
(cid:16)
) (where ğµ is the bit complexity of the
ğ‘‘ (for all even
ğ‘‘, and Ë†ğ‘€(

â„šğ‘‘

Ã—Â·Â·Â·Ã—

(cid:19)

(cid:17)

Ã—

Ã—

ğ‘‘

ğ‘¡

ğ‘‚

ğ‘˜

(
â„šğ‘‘

) âˆˆ

2The

Î© notation hides multiplicative logarithmic factors in ğ‘‘, ğ¶, ğ‘˜, 1
ğœ‚, 1
/
/

ğœ€, and ln
(

1
/

ğ›¿

)

.

e

22

1. Privacy: Alg is

ğœ€, ğ›¿

-diï¬€erentially private with respect to the input ğ‘Œ, viewed as a ğ‘‘-dimensional
)

(

database of ğ‘› individuals.

âˆ©

>

ğ‘‹

2. Utility: Suppose there exists a 2ğ‘˜-certiï¬ably ğ¶0-subgaussian set ğ‘‹

â„šğ‘‘ of ğ‘› > ğ‘›0 points such that
ğ‘¡
for 2 6 ğ‘¡ 6 ğ‘˜.
, covariance Î£
ğ‘› with mean ğœ‡
ğ‘Œ
ğœ‚
)
|
âˆ—
10 over the random choices of the algorithm, Alg outputs estimates
Then, with probability at least 9
/
ğ‘‘ (for all even ğ‘¡ < 2ğ‘˜ such that ğ‘¡ divides 2ğ‘˜) satisfying the
â„šğ‘‘
â„šğ‘‘
â„šğ‘‘, Ë†
Î£
ğœ‡
Ë†
following guarantees:

)ğ¼, and ğ‘¡-th moments ğ‘€(
âˆ—

ğ‘‘, and ğ‘€(

) âˆˆ

1
(

âˆ— (cid:23)

Ã—Â·Â·Â·Ã—

2âˆ’

poly

âŠ†

âˆ’

âˆˆ

âˆˆ

Ã—

Ã—

)

ğ‘‘

ğ‘‘

|

(

ğ‘¡

â„ğ‘‘ ,

ğ‘¢

âˆ€

âˆˆ

ğœ‡
h Ë†

âˆ’

, ğ‘¢

ğœ‡

âˆ—

i

and,

Î£
âˆ— (cid:22) Ë†
((
and, for every even ğ‘¡ < 2ğ‘˜ such that ğ‘¡ divides 2ğ‘˜,

1
ğœ‚1
/
âˆ’
)

2ğ‘˜
/

ğ¶ ğ‘˜

ğ‘‚

Î£

âˆ’

1

(cid:16)

(cid:17)

)

ğ‘˜

ğ‘¡

6 ğ‘‚

âˆšğ¶ ğ‘˜

2ğ‘˜
1
ğœ‚1
/
âˆ’

ğ‘¢âŠ¤Î£
âˆ—

ğ‘¢ ,

(cid:17)

1

ğ‘‚

((

+

ğ¶ ğ‘˜

(cid:16)

(cid:22)

(cid:16)

p
ğ‘¡

2ğ‘˜
/

)

ğ‘˜

1
ğœ‚1
/
âˆ’
)

,

Î£
âˆ—

(cid:17)

1

ğ‘‚

(

âˆ’

ğ¶ ğ‘˜

ğ‘¡

2ğ‘˜
/

ğœ‚1
âˆ’
)

ğ‘¢ âŠ—

h

ğ‘¡, ğ‘€(

ğ‘¡
)
âˆ— i

6

ğ‘¢ âŠ—

ğ‘¡

ğ‘¡ , Ë†ğ‘€(

)

h

i

6

1

ğ‘‚

(

+

ğ¶ ğ‘˜

ğ‘¡

2ğ‘˜
/

ğœ‚1
âˆ’
)

ğ‘¢ âŠ—

h

ğ‘¡ , ğ‘€(

ğ‘¡
)
âˆ— i

.

(cid:16)

(cid:17)

(cid:16)

(cid:17)

Moreover, the algorithm succeeds (i.e., does not reject) with probability at least 9
/
of the algorithm.

10 over the random choices

Observe that the privacy guarantees of the algorithm are (necessarily) worst-case. The utility

guarantees, however, hold only under the assumption that ğ‘Œ is an ğœ‚-corruption of a good set ğ‘‹.

The above theorem can also be translated into utility guarantees for points sampled from a given
distribution by recalling the well-known fact that points sampled from a certiï¬ably subgaussian
distribution are good with high probability:

Fact 4.2 (See Section 5 in [KS17b]). Suppose
poly
covariance Î£
(
be an i.i.d. sample from
probability at least 0.99 over the draw of ğ‘‹, the following all hold:

)ğ¼ and ğ‘¡-moment tensors ğ‘€(
of size ğ‘› > ğ‘›0 = ğ‘‚
ğ‘‘2ğ‘˜

) for ğ‘¡
ğœ‚2
/

âˆ— (cid:23)

2âˆ’

ğ’Ÿ

ğ’Ÿ

âˆˆ

(

ğ‘‘

ğ‘¡

. Then, for any ğ‘¡
)

âˆˆ

â„•. For any ğ‘˜

â„•, let ğ‘‹ =

ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘›

âˆˆ
}
{
â„• such that ğ‘¡ divides ğ‘˜, with

âˆ—

is a certiï¬ably ğ¶-subgaussian distribution with mean ğœ‡

and

1. ğ‘‹ is 2ğ‘˜-certiï¬ably 2ğ¶-subgaussian.

2.

2
1
Î£âˆ’
/
âˆ—

ğ‘‹

ğœ‡
(

(

) âˆ’

ğœ‡

6 ğœ‚.

2

âˆ—)
(cid:13)
(cid:13)
(cid:13)

.

(cid:13)
(cid:13)
3. Î£
(cid:13)
(
ğ‘£
2ğ‘˜

4.

ğ‘‹

1
) âˆˆ (

Â±
ğ‘¡ , ğ‘€(

ğœ‚

Î£
)
âˆ—
ğ‘¡

ğ‘‹

)(

ğ‘£âŠ—

h

n

1
)i âˆˆ (

Â±

ğœ‚

)h

ğ‘£âŠ—

ğ‘¡ , ğ‘€(

ğ‘¡
)
âˆ— i

.

o

We note that our main theorem for private robust moment estimation, Theorem 1.2, is an

immediate consequence of Theorem 4.1 and Fact 4.2.

For the rest of the section, we will work to prove Theorem 4.1. In Section 4.1, we will introduce
a witness-producing robust moment estimation algorithm that will be used as a subroutine for our
main algorithm and present relevant utility guarantees. In Section 4.2, we will then introduce our
main algorithm. After that, we will prove the necessary privacy guarantees in Section 4.3. Finally,
we will put together the pieces to prove our main theorem, Theorem 4.1, in Section 4.4.

23

4.1 Witness-Producing Version of Robust Moment Estimation Algorithm

ğ’œ

As a key building block, we will use the following (non-private) version of the robust moment
as in [KS17b]. Our
estimation algorithm of [KS17b] that uses the same constraint system
algorithm itself, however, makes one key change (we call our version â€œwitness-producingâ€ for
reasons that will soon become clear) to that of [KS17b] in order to obtain a private robust moment
estimation algorithm. Instead of outputting estimates of the moments of the unknown distribution,
our algorithm outputs a sequence of non-negative weights ğ‘1, ğ‘2, . . . , ğ‘ğ‘› forming a probability
distribution on the input set of points ğ‘Œ. The estimates can then be obtained by taking moments
of the ï¬nite set ğ‘Œ with respect to the probability distribution on ğ‘Œ deï¬ned by the weights ğ‘ğ‘–s. This
simple change is crucial to our worst-case analysis of the resulting algorithm (i.e. even when the
distributional assumption that ğ‘Œ is an ğœ‚-corruption of some good set ğ‘‹ is not met) and obtaining
our privacy guarantees. As we discuss, our blueprint for modifying convex optimization based
robust estimation algorithms appears to broadly applicable beyond the speciï¬c setting of robust
moment estimation.

The underlying constraint system

is shown below, and the witness-producing robust moment

estimation algorithm is shown as Algorithm 4.3.

ğ’œ

ğ¶,ğ‘˜,ğœ‚,ğ‘›

ğ’œ

1. ğ‘¤2
ğ‘–

ğ‘¦1, ğ‘¦2, . . . , ğ‘¦ğ‘›

({
= ğ‘¤ğ‘– for each 1 6 ğ‘– 6 ğ‘›,

})

: Constraint System for ğœ‚-Robust Moment Estimation

2.

ğ‘›

ğ‘–=1 ğ‘¤ğ‘– >

3. ğœ‡â€² = 1
Ã
ğ‘›

ğ‘›,

ğœ‚

)

âˆ’

1
(
ğ‘– ğ‘¥â€²ğ‘–,

= 0 for 1 6 ğ‘– 6 ğ‘›,

4. ğ‘¤ğ‘–

(

5. 1
ğ‘›

Ã
ğ‘¦ğ‘–
ğ‘¥â€²ğ‘– âˆ’

)
ğ‘¥â€²ğ‘– âˆ’

ğ‘›
ğ‘–=1h

ğ‘˜ 6

ğœ‡â€², ğ‘£

i

ğ¶ ğ‘˜

(

ğ‘˜

2
/

)

1
ğ‘›

ğ‘›
ğ‘–=1 h

ğ‘¥â€²ğ‘– âˆ’

ğœ‡â€², ğ‘£

2

i

Ã

(cid:0)

Ã

ğ‘˜

2
/

.

(cid:1)

Algorithm 4.3 (Witness-Producing Robust Moment Estimation).

Given: A set of points ğ‘Œ =

ğ‘¦1, ğ‘¦2, . . . , ğ‘¦ğ‘›

{

} âŠ†

â„šğ‘‘, ğœ‚ > 0, a parameter ğ‘˜

â„•.

âˆˆ

Output: Either â€œrejectâ€ or non-negative weights ğ‘1, ğ‘2, . . . , ğ‘ğ‘› s.t. ğ‘ğ‘– 6 1
1
ğœ‚
âˆ’

(

ğ‘› âˆ€

)

ğ‘– and

ğ‘– ğ‘ğ‘– = 1.

Operation:

Ã

1. Find a pseudo-distribution Ëœğœ of degree ğ‘‚

ğ‘˜
(
. If such a pseudo-distribution does not exist, then return â€œreject.â€
)

() satisfying the constraint system

ğ¶,ğ‘˜,ğœ‚,ğ‘›

ğ’œ

ğ‘Œ
(

)

2. Output weights ğ‘

0, 1
]

âˆˆ [

ğ‘› deï¬ned by ğ‘ğ‘– =

ğ”¼

ğ‘›
ğ‘–=1
e

ğ‘¤ğ‘–]
Ëœğœ[
ğ”¼
Ëœğœ[

ğ‘¤ğ‘–]

for each ğ‘–.

Analysis of the witness-producing robust estimation algorithm Robust estimation algorithms
that rely on the use of semideï¬nite programming are all analyzed under distributional assumptions

Ã

e

24

on the input set of points. Roughly speaking, such algorithms search over set of points that have a
large enough intersection with the input corrupted sample and satisfy certain relevant property of
the underlying family of distributions. In order to obtain privacy guarantee that holds for worst-
case inputs, we need to upgrade the analyses of such algorithms so that they not only provide
estimates of the target parameters, but also explicitly produce â€œwitnessesâ€â€”these are subsets of
the input corrupted sample that deï¬ne distributions with the estimated parameters and further,
satisfy the relevant property of the underlying family of distributions.

In this section, we verify that such a stronger guarantee can be obtained for robust moment
estimation algorithm of [KS17b]. Formally, their algorithm succeeds as long as the input is an
ğœ‚-corruption of a certiï¬ably subgaussian set.

The following guarantees for the algorithm above were shown in [KS17b].

Fact 4.4 (Lemmas 4.4, 4.5, and 4.8 in [KS17b]). Let ğ‘‹
ğ¶-subgaussian with mean ğœ‡
ğœ‚-corruption of ğ‘‹. Then, for ğœ‡â€² = 1
ğ‘›

, covariance Î£
âˆ—

ğ‘– ğ‘¥â€²ğ‘–, Î£â€² = 1

ğ‘›

âˆ—

âŠ†

ğ‘¡

)

â„ğ‘‘ be a set of size ğ‘› that is 2ğ‘˜-certiï¬ably
for ğ‘¡ evenly dividing 2ğ‘˜. Let ğ‘Œ be an
ğ‘¡, we have:
ğœ‡â€²)âŠ¤, and ğ‘€(
,

)â€² = 1
ğ‘›

ğ‘– ğ‘¥â€²ğ‘– âŠ—

ğ‘¢ ğ‘˜

Ã

ğ‘¡

and ğ‘¡-th moment ğ‘€(
âˆ—
ğ‘¥ğ‘–
âˆ’
ğ‘¢âŠ¤Î£
âˆ—

ğ‘–(
âˆ’
2ğ‘˜ 6 ğ‘‚
Ã

ğœ‡â€²)(
ğ¶ ğ‘˜ ğ‘˜ ğ‘˜

, ğ‘¢

ğ‘¥ğ‘–

âˆ—

ğœ‡

i
2

(
ğ‘˜ 6 ğ‘‚

)
ğ¶ ğ‘˜ ğ‘˜ ğ‘˜

(
ğ‘¡ 6 ğ‘‚

2ğ‘˜

/

,

(cid:9)
ğ‘¢ ğ‘˜

)
ğ¶ ğ‘˜ ğ‘˜ ğ‘˜

ğ‘¢âŠ¤Î£
âˆ—
(cid:9)
ğ‘¢âŠ¤Î£
âˆ—

ğ‘¢ ğ‘˜

.

âˆ’

ğ‘¢
ğ’œ 2ğ‘˜
ğ‘¢
ğ’œ 2ğ‘˜
ğ‘¢
ğ’œ 2ğ‘˜

h

Ã

ğœ‡â€²

h
(cid:8)
Î£â€²
h
ğ‘¡
(cid:8)
ğ‘€(

)â€²

Î£
âˆ—

âˆ’

ğ‘€(
âˆ—

, ğ‘¢ âŠ—

i
ğ‘¡
, ğ‘¢ âŠ—

ğ‘¡

)

âˆ’

i
Lemma 4.5 (Guarantees for Witness-Producing Robust Moment Estimation Algorithm). Given a
ğ‘˜
subset of of ğ‘› points ğ‘Œ
(
)
returns a sequence of weights 0 6 ğ‘1, ğ‘2, . . . , ğ‘ğ‘› satisfying
and either (a.) outputs â€œreject,â€ or (b.)
ğ‘ğ‘› = 1.
ğ‘1 +

â„šğ‘‘ whose entries have bit complexity ğµ, Algorithm 4.3 runs in time

ğµğ‘›

âŠ†

n

o

ğ‘‚

(

)

(

)

ğ‘¡

â„ğ‘‘ is 2ğ‘˜-certiï¬ably ğ¶-subgaussian with mean ğœ‡
ğœ‚
âˆ’
ğ‘¦ğ‘–
(

, covariance Î£
and in general, ğ‘¡-th
âˆ—
ğ‘›, then Algorithm 4.3 never rejects, and the corresponding
)âŠ¤ satisfy the following guarantees for ğ›½ğ‘¡ =

âŠ†
)âˆ— such that
ğ‘Œ
|
âˆ©
Î£ =
ğ‘– ğ‘ğ‘– ğ‘¦ğ‘– and Ë†

>
1
(
ğ‘›
ğ‘–=1 ğ‘ğ‘–

)
ğœ‡
âˆ’ Ë†

ğœ‡
âˆ’ Ë†

ğ‘¦ğ‘–

ğ‘‹

)(

|

âˆ—

ğ‘2 + Â· Â· Â· +
Moreover, if ğ‘‹
moment tensor ğ‘€(
estimates
2ğ‘˜ğ‘¡
ğ¶ ğ‘¡
ğ‘‚
/

ğœ‡ = 1
ğ‘›
Ë†
ğ‘¡
ğœ‚1
2
âˆ’
/
)

(

2ğ‘˜ for ğ‘¡ 6 ğ‘˜:
/
Ã

Ã

1. Mean Estimation:

ğ‘¢

âˆ€

âˆˆ

â„ğ‘‘ ,

ğœ‡
h Ë†

âˆ’

, ğ‘¢

ğœ‡

âˆ—

i

6 ğ‘‚

âˆšğ¶ ğ‘˜
(

2ğ‘˜
1
ğœ‚1
/
âˆ’
)

ğ‘¢âŠ¤Î£
âˆ—

ğ‘¢ ,

2. Covariance Estimation:

p

1
(

âˆ’

Î£
ğ›½2)

Î£
âˆ— (cid:22) Ë†

1
(cid:22) (

+

Î£
ğ›½2)
âˆ—

,

3. Moment Estimation: For all even ğ‘¡ < 2ğ‘˜ such that ğ‘¡ divides 2ğ‘˜,

âˆˆ
4. Witness: For ğ¶â€² 6 ğ¶

âˆ€

ğ‘¢

â„ğ‘‘ ,

ğ›½ğ‘¡

ğ‘¢ âŠ—

ğ‘¡ , ğ‘€(

ğ‘¡
)
âˆ— i

6

ğ‘¢ âŠ—

ğ‘¡

ğ‘¡ , Ë†ğ‘€(

)

h

i

6

1
(

+

ğ›½ğ‘¡

)h

ğ‘¢ âŠ—

ğ‘¡ , ğ‘€(

ğ‘¡
)
âˆ— i

1
(

ğ‘‚

)h

âˆ’
1
ğœ‚1
/
âˆ’
(
ğ‘›

ğ‘˜

,
))

1
(

+

1
ğ‘›

ï£±ï£´ï£´ï£²
ï£´ï£´
ï£³

ğ‘ğ‘–

ğ‘¦ğ‘–

h

ğœ‡
âˆ’ Ë†
i

2ğ‘˜ 6

ğ¶â€²ğ‘˜

(

ğ‘˜

)

Ã•ğ‘–=1

25

1
ğ‘›

ğ‘›

Ã•ğ‘–=1

ğ‘ğ‘–

ğ‘¦ğ‘–

h

ğœ‡
âˆ’ Ë†

i

ğ‘˜

2

!

ï£¼ï£´ï£´ï£½
ï£´ï£´
ï£¾

 
The ï¬rst three properties follow easily from an analysis similar to the one in [KS17b]. We verify

the last property below.

1
/

ğ’œ
ğ‘˜. Suppose there exists a 2ğ‘˜-certiï¬ably ğ¶1-subgaussian distribution ğ‘‹

Lemma 4.6. Let Ëœğœ be a pseudo-distribution of degree ğ‘‚
ğœ‚
â‰ª
such that
ğ‘Œ
|
where ğ‘Š =

âˆ’
, we have:

ğ‘›. Then, for ğœ‚ 6 ğœ‚0 for some absolute constant ğœ‚0 and for

consistent with

on input ğ‘Œ with outlier rate
of size ğ‘›
ğ‘¦ğ‘–
ğ‘¤ğ‘–

â„ğ‘‘ with mean ğœ‡
ğœ‡ = 1
ğ‘Š
Ë†

ğ‘›
ğ‘–=1

âˆ—
ğ”¼

Ëœğœ[

ğ”¼

>

âˆ©

âŠ†

ğœ‚

ğ‘˜

]

(

)

)

1
(
ğ‘¤ğ‘–

ğ‘‹
|
ğ‘›
ğ‘–=1

[

]

e

Ã

ğ‘¢
2ğ‘˜

1
ğ‘Š

ğ‘›

Ã•ğ‘–=1

ï£±ï£´ï£´ï£²
ï£´ï£´
2ğ‘˜
1
ğœ‚1
/
âˆ’
(
ï£³

)

for ğ¶â€² 6 ğ¶

ğ‘‚

1
(

+

Proof. We have:

ğ”¼

ğ‘¤ğ‘–

Ëœğœ[

]h

ğ‘¦ğ‘–

ğœ‡, ğ‘¢

âˆ’ Ë†

i

2ğ‘˜ 6

ğ¶â€²ğ‘˜

(

ğ‘˜

)

e
ğ‘˜

)

6 ğ¶

+

1 for small enough ğœ‚.

1
ğ‘Š

ğ‘›

Ã•ğ‘–=1

ğ”¼

ğ‘¤ğ‘–

Ëœğœ[

]h

ğ‘¦ğ‘–

ğœ‡, ğ‘¢

âˆ’ Ë†

e

e

Ã

,

ğ‘˜

2

i

!

ï£¼ï£´ï£´ï£½
ï£´ï£´
ï£¾

1
ğ‘›

ğ‘›

Ã•ğ‘–=1

e

ğ”¼

ğ‘¤ğ‘–

Ëœğœ[

]h

ğ‘¦ğ‘–

ğœ‡, ğ‘¢

âˆ’ Ë†

i!

2ğ‘˜

=

1
ğ‘›

ğ‘›

Ã•ğ‘–=1

e

ğ”¼

ğ‘¤ğ‘–

ğœ‡, ğ‘¢

ğ‘¥â€²ğ‘– âˆ’ Ë†

h

i

Ëœğœ[

2ğ‘˜

6 1
ğ‘›

]

ğ‘›

ğ”¼

ğ‘¥â€²ğ‘– âˆ’

Ëœğœ[h

ğœ‡â€²

+

ğœ‡â€²

ğœ‡, ğ‘¢

âˆ’ Ë†

2ğ‘˜

i

]

The ï¬rst term on the right-hand side above is at most
ğ‘‚

ğ‘¥â€²ğ‘– âˆ’
ğ‘¢ ğ‘˜ using certiï¬able subgaussianity constraints and Fact 4.4.

Ëœğœ[(

ğ¶ ğ‘˜

)

(

2ğ‘˜
ğœ‚1
1
/
âˆ’
(
Let us analyze the 2nd term above.

ğ‘˜ ğ‘¢âŠ¤Î£
âˆ—

ğ‘˜

)

)

ğ‘›
ğ‘–=1 h

Ã

ğœ‡â€², ğ‘¢

2

ğ‘˜

i

)

]

6

ğ¶

1
(

(

+

Ã•ğ‘–=1
1
ğ‘›

e

ğ‘˜

ğ”¼

ğ”¼

ğ‘¥â€²ğ‘– âˆ’

Ëœğœ[h

ğœ‡â€²

+

ğœ‡â€²

ğœ‡, ğ‘¢

âˆ’ Ë†

2ğ‘˜

i

]

e

ğ‘›

ğ‘¥â€²ğ‘– âˆ’

Ëœğœ[h

ğœ‡â€², ğ‘¢

2ğ‘˜

i

] +

2ğ‘˜

1
ğ‘›

ğ”¼

ğ‘¥â€²ğ‘– âˆ’

Ëœğœ[h

ğœ‡â€², ğ‘¢

2ğ‘˜

2
âˆ’

ğœ‡â€²

h

ğœ‡, ğ‘¢

âˆ’ Ë†

2

i

i

1
ğ‘›

ğ‘›

Ã•ğ‘–=1

ğ”¼

ğ‘¥â€²ğ‘– âˆ’

Ëœğœ[h

ğœ‡â€², ğ‘¢

2ğ‘˜

i

ğœ‡, ğ‘¢

âˆ’ Ë†

ğ‘—

i

]

e
ğœ‡â€², ğ‘¢
ğ‘¥â€²ğ‘– âˆ’

Ëœğœ[h

2ğ‘˜

i

] +

2ğ‘˜

1
ğ‘›

ğ”¼
(

ğ‘¥â€²ğ‘– âˆ’

Ëœğœ[h

ğœ‡â€², ğ‘¢

2ğ‘˜

(
)

i

2ğ‘˜

2ğ‘˜

2
)/
âˆ’

ğ”¼
(

Ëœğœ h

ğœ‡â€²

ğœ‡, ğ‘¢

âˆ’ Ë†

2ğ‘˜

2ğ‘˜
1
/
)

i

ğ”¼

ğ‘¥â€²ğ‘– âˆ’

Ëœğœ[h

ğœ‡â€², ğ‘¢

2ğ‘˜

i

âˆ’

h

ğœ‡, ğ‘¢

âˆ’ Ë†

ğ‘—

i

]

e

Ã•ğ‘–=1
ğ‘—

âˆ’

e
ğœ‡â€²

h

ğ‘›

Ã•ğ‘–=1
ğ‘—

e
ğœ‡â€²

ğ‘›

Ã•ğ‘–=1
1
ğ‘›

2ğ‘˜

1
ğ‘›

=

+

6 1
ğ‘›

2ğ‘˜

ğ‘›
e

ğ”¼

Ã•ğ‘–=1
e
2ğ‘˜
ğ‘—

Ã•ğ‘—=2 (cid:18)
ğ‘›

(cid:19)

ğ”¼

Ã•ğ‘–=1
e
2ğ‘˜
ğ‘—

(cid:19)

1
ğ‘›

ğ‘›

Ã•ğ‘–=1

+

Ã•ğ‘—=2 (cid:18)

e

(4.1)

(4.2)

(4.3)

(4.4)

(4.5)

Here, in the 2nd inequality, we used the HÃ¶lderâ€™s inequality for pseudo-distributions. Let us
analyze the 2nd term in the right-hand side above by observing the following that uses the bounds
from Fact 4.4:

ğ”¼
ğœ‡â€²
Ëœğœ[h
6 22ğ‘˜
e

(

ğœ‡, ğ‘¢
i
ğ‘˜ğœ‚2ğ‘˜

âˆ’ Ë†
ğ¶ ğ‘˜

)

ğ”¼

2ğ‘˜

6 22ğ‘˜
[
ğ‘¢ ğ‘˜

]
1ğ‘¢âŠ¤Î£
âˆ’
âˆ—

+
e

ğœ‡â€²
Ëœğœ[h
22ğ‘˜

âˆ’
ğ¶ ğ‘˜

ğœ‡

2ğ‘˜

, ğ‘¢
i
âˆ—
ğ‘˜ğœ‚2ğ‘˜
1
âˆ’

22ğ‘˜
ğœ‡
âˆ— âˆ’ Ë†
h
ğ‘˜ ğ‘¢âŠ¤Î£
ğ›½2)
âˆ—

ğœ‡, ğ‘¢
ğ‘¢ ğ‘˜

] +
1
(

+

2ğ‘˜

i

(4.6)

(4.7)

)

(

26

 
 
This allows us to infer that the 2nd term in (4.5) is at most
ğ¶ ğ‘˜

ğ‘›
ğ‘–=1h
Â·
ğ‘¢ ğ‘˜ using certiï¬able subgaussianity constraints and

ğ‘¢ 6 ğ‘‚

ğ‘¥â€²ğ‘– âˆ’

ğœ‡â€², ğ‘¢

2
)/
âˆ’

Ëœğœ[

](

ğ”¼

1
ğ‘›

2ğ‘˜

2ğ‘˜

2ğ‘˜

ğ‘˜

i

2ğ‘˜ âˆšğ‘¢âŠ¤Î£
1
2ğœ‚1
1
5ğ¶ ğ‘˜
/
âˆ’
/
(
)
âˆ—
Fact 4.4.

(

)(

2ğ‘˜ ğ‘¢âŠ¤Î£
ğ‘˜ğœ‚1
1
/
âˆ’
âˆ—

)

Ã

e

Letâ€™s now analyze the terms corresponding to ğ‘— > 2 in the right-hand side of (4.5). Each of
. Let us ï¬rst analyze

and

ğœ‡, ğ‘¢

ğœ‡â€², ğ‘¢

these terms corresponds to a â€œmixed monomialâ€ in
the even individual degree terms.

ğ‘¥â€²ğ‘– âˆ’

h

ğœ‡â€² âˆ’

h

i

i

First observe that by HÃ¶lderâ€™s inequality for pseudo-distributions again, we have:

1
ğ‘›

ğ‘›

Ã•ğ‘–=1

e

ğ”¼

ğ‘¥â€²ğ‘– âˆ’

Ëœğœ[h

ğœ‡â€², ğ‘¢

2ğ‘˜

2
âˆ’

ğœ‡â€²

h

ğœ‡, ğ‘¢

âˆ’ Ë†

2

i

]

i

6

ğ‘¥â€²ğ‘– âˆ’

h

ğœ‡â€², ğ‘¢

ğ‘˜

ğ‘˜

1
)/
âˆ’

2ğ‘˜

(
)

i

ğ”¼
(

Ëœğœ h

ğœ‡â€²

ğœ‡, ğ‘¢

âˆ’ Ë†

2ğ‘˜

ğ‘˜ .

1
/
)

i

(4.8)

By an analysis similar to the case of the ï¬rst term on the right-hand side of (4.5) above, we obtain
2ğ‘˜
1
ğœ‚1
that the right-hand side is at most: ğ‘‚
/
âˆ’
(

)
Next, letâ€™s analyze all terms corresponding to even ğ‘—. By Proposition 3.9, we have:

2ğ‘¢âŠ¤Î£
)
âˆ—

1
)(
(

ğ‘¢ ğ‘˜.

ğ¶ ğ‘˜

ğ‘˜

e

1
ğ‘›

ğ‘›

Ã•ğ‘–=1

e

ğ‘›

ğ”¼
(
Ã•ğ‘–=1
ğ‘›

e
ğ”¼
(

Ã•ğ‘–=1

6 2ğ‘˜
ğ‘›

ğ”¼

ğ‘¥â€²ğ‘– âˆ’

Ëœğœ[h

ğœ‡â€², ğ‘¢

2ğ‘˜

2ğ‘—

âˆ’

ğœ‡â€²

h

ğœ‡, ğ‘¢

âˆ’ Ë†

2ğ‘—

i

]

i

6 1
ğ‘›

ğœ‡â€²

Ëœğœ[h

ğœ‡, ğ‘¢

âˆ’ Ë†

2

i

ğ‘¥â€²ğ‘– âˆ’

h

ğœ‡â€², ğ‘¢

2ğ‘˜

2ğ‘—

âˆ’

ğœ‡â€²

h

ğœ‡, ğ‘¢

âˆ’ Ë†

i

2ğ‘—

2
âˆ’

]

i

ğœ‡â€²

Ëœğœ[h

ğœ‡, ğ‘¢

âˆ’ Ë†

2

i

(h

ğ‘¥â€²ğ‘– âˆ’

ğœ‡â€², ğ‘¢

2ğ‘˜

2
âˆ’

i

ğœ‡â€²

ğœ‡, ğ‘¢

âˆ’ Ë†

i

+ h

2ğ‘˜

2
âˆ’

)]

e
The ï¬rst term can now be upper bounded by the bound for (4.8) and the 2nd term by an application
of Fact 4.4.

The case of odd terms is similar with the ï¬rst step using Proposition 3.10.
Altogether, we obtain an upper bound of
+
On the other hand, using the sum-of-squares version of the Cauchy-Schwarz inequality along

2ğ‘˜
1
ğœ‚1
/
âˆ’
(

ğ‘˜ ğ‘¢âŠ¤Î£
âˆ—

ğ‘¢ ğ‘˜.

1
(

ğ‘‚

ğ¶

ğ‘˜

)

)

(

with the almost triangle inequality and invoking Fact 4.4 we have:

ğ‘¢
ğ’œ 2ğ‘˜

1
ğ‘›

(  

6 16ğœ‚ğ¶2

ğ‘›

1
(
Ã•ğ‘–=1
1
ğ‘›

ğ‘›

Ã•ğ‘–=1

ğ‘¤ğ‘–

ğœ‡, ğ‘¢

ğ‘¥â€²ğ‘– âˆ’ Ë†

i

)h

âˆ’

ğ‘¥â€²ğ‘– âˆ’

h

ğœ‡â€², ğ‘¢

2

i

+

2

2

6

!

1
ğ‘›

ğ‘›

Ã•ğ‘–=1

1
ğ‘›

ğ‘›

Ã•ğ‘–=1

1
(

âˆ’

ğ‘¤ğ‘–

2
)

!

2

1
ğ‘›

ğ‘›

Ã•ğ‘–=1

ğœ‡, ğ‘¢

ğ‘¥â€²ğ‘– âˆ’ Ë†

h

i

4

ğœ‡â€²

h

ğœ‡, ğ‘¢

âˆ’ Ë†

2

i

!

6 20ğœ‚ğ¶2

1
(

+

ğ›½2)

2ğ‘¢âŠ¤Î£
âˆ—

ğ‘¢2

)

Thus,

ğ‘¢
ğ’œ 2ğ‘˜

1
ğ‘›

( 

ğ‘›

Ã•ğ‘–=1

ğ”¼

ğ‘¤ğ‘–

ğ‘¦ğ‘–

h

Ëœğœ[

ğœ‡, ğ‘¢

âˆ’ Ë†

2

i

!

2

=

1
ğ‘›

ğ‘›

Ã•ğ‘–=1

2

2

i

!

] âˆ’  

1
ğ‘›

ğ‘›

Ã•ğ‘–=1

ğ”¼

ğ‘¦ğ‘–

Ëœğœ[h

ğœ‡, ğ‘¢

âˆ’ Ë†

e

ğ”¼

1
Ëœğœ[(

âˆ’

ğ‘¤ğ‘–

ğ‘¦ğ‘–

)h

ğœ‡, ğ‘¢

âˆ’ Ë†

2

ğ‘›

2

i

!

e

>

1
(

âˆ’

ğ‘‚

(

e
1
ğœ‚1
ğ¶ ğ‘˜
/
âˆ’
)

80ğœ‚ğ¶2

ğ‘˜

âˆ’

ğ‘¢âŠ¤Î£
âˆ—

)

ğ‘¢ .

)

The lemma now follows immediately for small enough ï¬xed constant ğœ‚.

(cid:3)

27

 
 
 
4.2 Private Robust Moment Estimation

We are now ready to present our main algorithm for private robust moment estimation. Our
algorithm uses the witness-producing algorithm (Algorithm 4.3) as a major building block while
augmenting it to search for pseudo-distributions that, in addition to satisfying the relevant set
of constraints, also minimize an appropriate strongly convex potential function. We deï¬ne the
relevant potential function Pot below in Deï¬nition 4.7.

2

ğ’œ
. Furthermore, let Potğ¶,ğ‘˜,ğ‘›

)

ğ‘Œ
(

ğœ‚

Deï¬nition 4.7 (Potential Function). Let ğ¶ > 0 and ğ‘›, ğ‘˜
degree 2 consistent with

for outlier rate ğœ‚ and input ğ‘Œ

âˆˆ

ğ¶,ğ‘˜,ğœ‚,ğ‘›

â„•. For any pseudo-distribution Ëœğœ of
be deï¬ned
ğœ‚, Ëœğœ

â„ğ‘‘, let Potğ¶,ğ‘˜,ğ‘›

ğ‘Œ
(

)

as

ğ”¼

ğ‘¤

Ëœğœ[

]

2
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)e

)
the potential as Ëœğœ ranges over all pseudo-distributions of degree 2ğ‘¡ consistent with
ğ‘Œ
no such pseudo-distribution exists, set Potğœ‚(

.
âˆ
When ğ¶ , ğ‘›, ğ‘˜ are understood from context, we may suppress these parameters and simply

. If
)

ğ¶,ğ‘˜,ğœ‚,ğ‘›

ğ‘Œ
(

ğ’œ

=

)

)

)

Ëœğœ sat

ğ‘Œ
ğ’œğ¶ ,ğ‘˜,ğœ‚,ğ‘›(

be the minimum value of

= min

ğ‘Œ
(

âŠ†
Potğ¶,ğ‘˜,ğ‘›
ğœ‚, Ëœğœ

ğ‘Œ
(

write Potğœ‚ and Potğœ‚, Ëœğœ.

Now, we are ready to describe our main private robust moment algorithm, which is listed as
Algorithm 4.8. The algorithm consists of three main steps. In the ï¬rst step, the randomized DP
selection algorithm (Theorem 3.34) is used to pick an outlier rate (according to a suitable scoring
function, as deï¬ned below in Deï¬nition 4.12). The second step invokes the witness-producing
algorithm (Algorithm 4.3) with the outlier rate chosen in step 1, after which one checks that the
outputted weights induce a certiï¬ably subgaussian distribution on the input dataset ğ‘Œ. Finally, in
the last step, one takes the estimates of the mean, covariance, and higher moments provided by
the resulting weight vector and adds suitable noise to guarantee diï¬€erential privacy.

Algorithm 4.8 (Private Robust Moment Estimation).

Given: A set of points ğ‘Œ =

ğ‘¦1, ğ‘¦2, . . . , ğ‘¦ğ‘›

â„šğ‘‘, parameters ğ¶ , ğœ‚, ğœ€, ğ›¿ > 0, ğ¿, ğ‘˜

â„•.

âˆˆ

} âŠ†

ğ‘¡

) (3 6 ğ‘¡ 6 ğ‘˜) for mean, covariance, and ğ‘¡-moments.

{
Î£, and Ë†ğ‘€(

ğœ‡, Ë†
Ë†

Output: Estimates

Operation:

1. Stable Outlier Rate Selection: Use the

2 to
with the scoring function as deï¬ned in Deï¬nition 4.12.

-DP Selection with ğœ… = ğ¿
3
)

3, ğ›¿

ğœ€

/

/

/

(

sample an integer ğœ
If ğœ =

ğœ‚ğ‘›

]

âˆˆ [

, then reject and halt. Otherwise, let ğœ‚â€² = ğœ

âŠ¥

ğ‘›.

/

ğ‘Œ
(

ğ‘Œ
and minimizing Potğœ‚â€², Ëœğœ(

2. Witness Checking: Compute a pseudo-distribution Ëœğœ of degree 2ğ‘˜ satisfying
and

)
ğ›¾. Check that the weight vector ğ‘ =

ğ¶,ğ‘˜,ğœ‚â€²,ğ‘›
ğ’œ
ğ¶â€² = ğ¶
gaussian distribution on ğ‘Œ. If not, reject immediately. Otherwise, let
Î£ =

]
(for all even ğ‘¡ < 2ğ‘˜ such that ğ‘¡ divides 2ğ‘˜) be the

induces a ğ¶â€²-certiï¬ably sub-
,

. Let ğ›¾
)

âˆ¼
ğ”¼
Ëœğœ[

, 3
/

, and

tLap

ğœ‡ =

3
(
/
ğœ€

Ëœğœ[

3 ln

ğ”¼

ğ”¼

ğ”¼

e

ğ‘¤

+

âˆ’

+

ğœ‡

1

ğœ€

(cid:16)

(cid:16)

(cid:17)

(cid:17)

]

ğ›¿

)

ğ‘¡

ğ‘¡

ğ‘€(

) =

ğ‘€(

Î£
]

Ëœğœ[

Ëœğœ[

)]

e

e

e

e

e

e

28

mean, covariance, and ğ‘¡th moment estimates, respectively, that are induced by the
pseudo-distribution Ëœğœ.

3. Noise Addition: Let ğ›¾1 = ğ‘‚
(
ğ‘‘ and ğ‘

1
2ğ‘˜ ) and ğ›¾ğ‘¡ = ğ‘‚
ğ¿
ğ‘›
ğ¶â€²ğ‘˜
2ğ‘˜ ) for
2 (
âˆ’
/
)
)
1
0, ğœ2)(
2 ), where we interpret ğ‘ as a symmetric
ğ‘‘ matrix with i.i.d. entries in the upper triangular portion. Similarly, for ğ‘¡ > 2,
), where we interpret ğ‘ as a symmetric ğ‘‘
tensor

ğ‘¡ > 2. Let ğ‘§
ğ‘‘
Ã—
let ğ‘(

)(
âˆ¼ ğ’©(

0, ğœ1)

1
2 (
âˆ’

âˆ¼ ğ’©(

0, ğœğ‘¡

ğ¶â€²ğ‘˜

2
/

ğ‘¡
+(
ğ‘¡

)(

((

ğ‘›

ğ¿

ğ‘‘

ğ‘‘

/

1
)

)

âˆ’

+

ğ‘¡

ğ‘¡

ğ‘‘

ğ‘‘

) âˆ¼ ğ’©(

)(

Ã—

Ã— Â· Â· Â·

1

1

1

ğ‘¡

independent â€œupper-triangularâ€ entries. Moreover, let

|

{z

}

ğ‘¡ times

with

ğ‘‘

1
)
âˆ’

ğ‘¡
ğ‘¡

+(

(cid:0)

(cid:1)

(

Then, output:

â€¢

â€¢

â€¢

ğœ‡ =
Ë†
Î£ =
Ë†
ğ‘¡
Ë†ğ‘€(

ğœ‡
Î£
e
) =
e

+

2ğ‘§.
Î£1
/
2ğ‘
Î£1
/
+
e
ğ‘¡
ğ‘€(
e
e

2.
Î£1
/
Î£
) + ((
e

+

e

e

e

4.3 Privacy Analysis

1

ğ‘¡

âˆ’
2

ğœğ‘— = 6ğ‘˜ğœ€âˆ’
ğœğ‘— = 6ğ‘˜ğœ€âˆ’

1ğ›¾ğ‘— ğ‘‘
1ğ›¾ğ‘—

7.5ğ‘˜
2 ln
(
ğ‘¡
1
ğ‘¡ ğ‘‘
p
)

/
2 ln

âˆ’
2

ğ›¿

,

)
7.5ğ‘˜
(

for ğ‘— = 1, 2

ğ›¿

,

)

/

for ğ‘— > 2

.

ğ¶â€²ğ‘˜

(

p

ğœ‡

ğœ‡ğ‘‡

2
1
/
)

)âŠ—

ğ‘¡

ğ‘¡ ğ‘(

), for all even ğ‘¡ < 2ğ‘˜ such that ğ‘¡ divides 2ğ‘˜.

Our analysis of the privacy of Algis based on a sequence of claims about each of the steps of Algthat
cumulatively establish the stability of the behavior of Algon adjacent inputs ğ‘Œ, ğ‘Œâ€². We will rely on
the following simple but key observation in our analysis. It is easy to verify using the deï¬nition of
pseudo-distributions.

Lemma 4.9 (Adjacent Pseudo-distributions). Let Ëœğœ be a pseudo-distribution of degree 2ğ‘˜ that satisï¬es all
â„ğ‘‘ be adjacent to ğ‘Œ. Deï¬ne
the constraints in 4.1 on input ğ‘Œ =
if
ğ‘¤ğ‘†ğ‘
an adjacent pseudo-distribution Ëœğœâ€² (that â€œzeroes out ğ‘¤ğ‘–â€) by
(
ğ‘– âˆ‰ ğ‘† and
= 0 if ğ‘–
ğ‘† for every polynomial ğ‘ in ğ‘‹â€² and other auxiliary indeterminates
in
. Then, Ëœğœâ€² is a pseudo-distribution of degree 2ğ‘˜ that satisï¬es all the constraints in 4.1 on both inputs ğ‘Œâ€²
and ğ‘Œ with outlier parameter ğœ‚

with outlier rate ğœ‚. Let ğ‘Œâ€² âŠ†
ğ”¼
ğ‘‹â€²,

ğ‘¦1, ğ‘¦2, . . . , ğ‘¦ğ‘›

Â· Â· Â· )]

Â· Â· Â· )]

Â· Â· Â· )]

ğ‘¤ğ‘†ğ‘

ğ‘¤ğ‘†ğ‘

ğ‘‹â€²,

ğ‘‹â€²,

Ëœğœâ€²[

Ëœğœâ€²[

Ëœğœ[

ğ’œ

ğ‘›.

ğ”¼

ğ”¼

e

e

e

=

âˆˆ

{

}

(

(

1
/

+

This allows us to conclude the following basic calculus of our potential function:

Lemma 4.10 (Basic Facts about Pot). Suppose that for some ğ‘Œ
4
ğœ‚â€² âˆˆ [
]
ğœ‚ > ğœ‚â€², the following holds:

, there is a pseudo-distribution of degree 2ğ‘¡ consistent with

0, ğœ‚0/

âŠ†

ğ’œ

â„ğ‘‘ of size ğ‘›, some ğ‘¡

â„• and
on input ğ‘Œ. Then, for every

âˆˆ

1. Monotonicity: Potğœ‚
subscript increases.

1
/
+

ğ‘Œ
ğ‘›(

)

6 Potğœ‚(
ğ‘Œ

.
)

In particular, Pot is monotonically decreasing as its

2. Lower Bound: Potğœ‚(
ğ‘Œ
3. Upper Bound: Potğœ‚(
ğ‘Œ

)

)

>

6

1
(
1
(

âˆ’

âˆ’

ğœ‚

2ğ‘›.
)
ğ‘›.

ğœ‚

)

29

         
         
ğ‘›
ğ‘–=1

Proof. The ï¬rst fact follows immediately from Lemma 4.9. For the second, observe that any
on input ğ‘Œ with outlier rate ğœ‚ must satisfy
pseudo-distribution Ëœğœ of degree 2ğ‘¡ consistent with
2 >
ğ‘› =
ğ”¼
[
6 1 for every ğ‘–. Thus,
e
(cid:3)

ğ‘›
ğ‘–=1
Ëœğœ[
2ğ‘›. This completes the proof. For the last part, observe that
ğœ‚
Ã
)
e
ğ”¼
ğ‘›.

ğ‘›. Thus, by Cauchy-Schwarz inequality,

(
Ã

ğ‘¤ğ‘–
ğ‘¤ğ‘–

2
])

ğ‘›
ğ‘–=1

2 6

1
Ã
(

1
(

ğ‘¤ğ‘–

ğ‘¤ğ‘–

ğ’œ

ğ”¼

ğ”¼

ğ”¼

>

âˆ’

=

ğœ‚

/

ğœ‚

]

]

]

)

âˆ’
ğ‘›
ğ‘–=1

ğ‘¤ğ‘–

]

Ëœğœ[

ğ‘›
ğ‘–=1

ğ‘¤ğ‘–

]

Ëœğœ[

1
(

âˆ’

)

[
ğ”¼
Ëœğœ[
e
e

e

e

Ã

Ã
Analysis of stable outlier rate selection The goal of the ï¬rst step of Algis to ï¬nd an outlier
rate ğœ‚â€² such that the strongly convex potential function Pot
on the pseudo-distribution we will
eventually compute (in Step 3) is close on adjacent input points ğ‘Œ, ğ‘Œâ€². We will later use the strong
ğ‘Œ
convexity of the Pot and the closeness guarantee on Pot on ğ‘Œ, ğ‘Œâ€² to infer that the weight vector ğ‘
(
and ğ‘

output by the algorithm themselves are close.

( Ëœğœ

)

)

Our key algorithmic trick to ensure the closeness of the strongly convex potential Pot is to ï¬nd
of outlier rates ğœ‚â€²â€² such that strongly convex potential
a â€œstable intervalâ€
function at near-optimal solutions must vary slowly as ğœ‚â€²â€² varies in the the interval. We ï¬nd such
an interval via a variant of the exponential mechanism.

ğ‘›, ğœ‚â€² +

ğœ‚â€² âˆ’
[

0.5ğ¿

0.5ğ¿

ğ‘›

/

/

]

ğ‘Œâ€²)
(

Deï¬nition 4.11 (Stability). Fix ğ¿
some ğ‘Œ
2ğ›¾ length interval centered at ğœ to be

â„ğ‘‘ of size ğ‘›, the constraint system

âŠ†

âˆˆ

â„•. Let ğœ, ğ›¾

ğ’œ((

âˆˆ {
ğœ
âˆ’

)/

)

0, . . . , ğ‘›
ğ‘›
ğ›¾

such that ğ›¾ 6 ğœ, ğ‘›

ğœ. Suppose for
is feasible. We deï¬ne the stability of the

âˆ’

}

stabğ‘Œ

ğœ, ğ›¾

= Pot
(

ğœ

ğ›¾

âˆ’

)/

ğ‘Œ
ğ‘›(

)

) âˆ’

Pot
(

ğœ

+

ğ›¾

)/

ğ‘Œ
ğ‘›(

)

(

Observe that if there is a pseudo-distribution consistent with

then there is a pseudo-distribution consistent with
stability above is well-deï¬ned.

ğ’œ

on ğ‘Œ with any outlier rate >

on ğ‘Œ with outlier rate
ğ›¾

ğ’œ

ğœ

ğ‘›
ğœ
ğ›¾
ğ‘›. Thus,

)/

âˆ’

(
)/

(

âˆ’

Deï¬nition 4.12 (Score Function). Fix ğ‘›, ğ‘˜
parameter ğ¿, we deï¬ne the following score function for every integer ğœ

â„• and ğ¶ > 0. Let ğ‘Œ

âŠ†

âˆˆ

â„ğ‘‘ be a set of size ğ‘›. For a

ğ‘›

:

]

âˆˆ [

ï£±ï£´ï£´ï£²
ï£´ï£´
ï£³

scoreğ‘›,ğ¶,ğ‘˜

ğœ, ğ‘Œ

=

)

(

0

max

ğ›¾
ğ‘Œ
ğ‘›,ğ‘›(

)

is feasible

min

{

ğ›¾, 20ğ¿

stabğ‘Œ

ğœ, ğ›¾

(

)}

âˆ’

ğ’œğ¶ ,ğ‘˜,

ğ›¾

ğœ

(

âˆ’

)/

if Alg
ğ‘Œ, ğœ
(
/
otherwise.

ğ‘›

)

is infeasible,

In the second case, we deï¬ne ğ›¾âˆ—ğ‘Œ(

:= arg max

ğœ

)

ğ’œğ¶ ,ğ‘˜,

ğ›¾

ğœ

(

âˆ’

)/

ğ›¾
ğ‘Œ
ğ‘›,ğ‘›(

)

is feasible

min

ğ›¾, ğ¿

{

âˆ’

stabğ‘Œ

ğœ, ğ›¾

.

)}

(

Lemma 4.13. Let ğœ, ğ›¾
system
ğ›¾
ğœ
)/
most one point. Then, for any ğœ, ğ›¾,

ğ’œ((

âˆˆ [

âˆ’

ğ‘›

ğ‘›

]

)

such that ğ›¾ 6 ğœ, ğ‘›

â„ğ‘‘ of size ğ‘›, the constraint
ğœ. Suppose for some ğ‘Œ
is feasible for both ğ‘Œ. Let ğ‘Œâ€² be any collection of ğ‘› points in â„ğ‘‘ diï¬€ering from ğ‘Œ in at

âˆ’

âŠ†

stabğ‘Œâ€²(

ğœ, ğ›¾

1
)

âˆ’

6 stabğ‘Œ

ğœ, ğ›¾

(

)

Proof. Using Lemma 4.9 and noting that if Ëœğœâ€² is adjacent to Ëœğœ then

Pot
(

ğœ

ğ›¾

1
)/
+

âˆ’

ğ‘Œâ€²

ğ‘›(

)

6 Pot
(

ğ›¾

ğœ

âˆ’

)/

,

ğ‘Œ
ğ‘›(

)

30

ğ‘¤

]

Ëœğœâ€²[

ğ”¼

(cid:13)
(cid:13)
(cid:13)e

2

2
(cid:13)
(cid:13)
(cid:13)

6

ğ”¼

(cid:13)
(cid:13)
(cid:13)e

ğ‘¤

]

Ëœğœ[

2

2
(cid:13)
(cid:13)
(cid:13)

, we have:

and

Pot
(
Combining the two equations yields

ğ›¾

ğœ

+

)/

ğ‘Œ
ğ‘›(

)

6 Pot
(

ğœ

ğ›¾

1
)/
âˆ’

+

ğ‘Œâ€²

ğ‘›(

.

)

stabğ‘Œâ€²(

ğœ, ğ›¾

1
)

âˆ’

ğ‘Œâ€²

= Pot
(
6 Pot
(

ğœ

ğ›¾

âˆ’

ğœ

ğ›¾

ğ‘›(
1
)/
+
ğ‘Œ
ğ‘›(

Pot
(

ğœ

+

ğœ

ğ›¾

) âˆ’
Pot
(

ğ›¾

1
âˆ’
)/
ğ‘Œ
ğ‘›(

ğ‘Œâ€²
ğ‘›(
)
= stabğ‘Œ

ğœ, ğ›¾

.

) âˆ’
Lemma 4.14 (Sensitivity of Score Function). Let ğ‘Œ, ğ‘Œâ€² be set of ğ‘› points in â„ğ‘‘ diï¬€ering at most in one
point, and ğœ

. Then, for every ğœ > 0,

ğ‘›

)/

)/

âˆ’

+

(

)

)

(cid:3)

âˆˆ [

]

Proof. It suï¬ƒces to prove that score
(
that score
(

> score
(
Consider the following two cases:

ğœ, ğ‘Œâ€²) âˆ’

ğœ, ğ‘Œ

)

ğœ, ğ‘Œ

|

score
(
ğœ, ğ‘Œâ€²)

) âˆ’
> score
(

) âˆ’
2, which establishes (4.9).

score
(

ğœ, ğ‘Œâ€²

)|

6 2.

(4.9)

ğœ, ğ‘Œ

2. A symmetric argument then proves

â€¢ Alg

ğœ

ğ‘Œ,
(

ğ‘›

1
)/

is infeasible for ğ‘Œ or ğ‘Œâ€². In this case, we have score
(

6 2, which implies

ğœ, ğ‘Œ

)

âˆ’
the desired bound.

(

)

ğœ

â€¢ Alg

ğ‘Œ,
(
âˆ’
Let ğ›¾âˆ— := ğ›¾âˆ—ğ‘Œ(

(

follows that

)

ğ‘›

is feasible for both ğ‘Œ and ğ‘Œâ€².

1
)/
. From Lemma 4.13, we know that stab
(
)

ğœ

ğœ, ğ›¾âˆ— âˆ’

1, ğ‘Œâ€²)

6 stabğ‘Œ

ğœ, ğ›¾âˆ—) +

(

2. Thus, it

score
(

ğœ, ğ‘Œâ€²

)

> min
> min

ğ›¾âˆ—

ğ›¾âˆ—

{

{

âˆ’

âˆ’

1, 20ğ¿

1, 20ğ¿

âˆ’

âˆ’

as desired.

stabğ‘Œâ€²(
stabğ‘Œ
(

ğœ, ğ›¾âˆ—

1
)}

âˆ’

ğœ, ğ›¾âˆ—

)}

> scoreğ‘Œ

ğœ

(

) âˆ’

1,

(cid:3)

ğœ‚

ğ’œ(

2
)

/

is feasible on ğ‘Œ. For every

Lemma 4.15 (Existence of a Good Stable Interval). Suppose
0, ğœ‚ğ‘›
ğ¿

, there is a ğœ

0, 0.25ğœ‚ğ‘›

> ğ¿.

ğœ, ğ‘Œ

such that score
(

)

âˆˆ [

]

Proof. Consider Potğœ‚
6
Potğœ‚

ğ‘Œ
Potğœ‚(

)

ğ‘Œ
2(
/

) âˆ’

âˆˆ [
2, Potğœ‚
2ğ¿
2
+
/
/
ğ‘›
2
1
ğœ‚
)
(

]
ğ‘› , . . . , Potğœ‚
/
1
ğœ‚
âˆ’ (

âˆ’

âˆ’

/

2ğ¿ğ‘Ÿ

2
+
/

ğ‘› where ğ‘Ÿ

:=
2ğ‘› 6 1.5ğœ‚ğ‘›. Therefore, there must exists ğ‘Ÿâˆ— âˆˆ [
)

0.25ğœ‚ğ‘›

ğ¿

/

âŒ‹

âŒŠ

/

.

Observe that
such that

ğ‘Ÿ

]

Potğœ‚

2ğ¿

2
+
/

ğ‘Ÿâˆ—âˆ’
1
)/

(

ğ‘› âˆ’

Potğœ‚

2ğ¿ğ‘Ÿâˆ—/
2
+
/

ğ‘› 6 1.5ğœ‚ğ‘›

ğ‘Ÿ

6 12ğ¿.

Let ğœ = ğœ‚

2

/

2ğ¿ğ‘Ÿâˆ— âˆ’

+ (

1
)/

ğ‘› and ğ›¾ = ğ¿. Then, we have stab
(

ğœ, ğ›¾

score
(

ğœ, ğ‘Œ

)

> max

ğ›¾, 20ğ¿

{

12ğ¿

âˆ’

6 12ğ¿ and, thus,

> ğ¿.

)

}

Lemma 4.16 (Utility of Score Function). Suppose

is feasible on ğ‘Œ. Let ğœ€, ğ›¿, ğ›½

0, 0.25ğœ‚ğ‘›

ğ‘›
ğ¿
ğ›½ğ›¿
function in Deï¬nition 4.12 and ğœ… = ğ¿

, if ğ¿ > ğ‘‚

1
ğœ€ Â·

log

âˆˆ [

]

(cid:16)

(cid:16)

, then with probability 1

ğ›½, Theorem 3.34, invoked with the score

2, does not reject, and the output ğœ satisï¬es stabğ‘Œ
(cid:17) (cid:17)
/

ğœ, ğ¿

2
)

/

(

< 20ğ¿.

ğœ‚

2
)

/

ğ’œ(

âˆ’

31

(cid:3)

. For every

0, 1
]

âˆˆ (

Proof. This follows from the guarantee of Selection (Theorem 3.34), Lemma 4.15 and the deï¬nition
(cid:3)
of score.

Lemma 4.17 (Potential Stability Under Good Coupling). Let ğœ‚, ğœ€, ğ›¿ > 0 and ğ‘˜, ğ¿
parameters such that 0.25ğœ‚ğ‘› > ğ¿ = Î©
not halt and chooses ğœ‚â€² = ğœ

â„• be given input
. Let ğ‘Œ, ğ‘Œâ€² be adjacent subsets of â„šğ‘‘. Suppose Algdoes

ğ‘› in Step 1 on input ğ‘Œ and ğ‘Œâ€². Then,
(cid:16)

1
ğœ€ Â·

log

ğ‘›
ğ›½ğ›¿

(cid:17) (cid:17)

âˆˆ

(cid:16)

/

Consequently, if ğ‘, ğ‘â€² are scalings of

ğ‘Œ
Potğœ‚â€²(
ğ‘¤

and

) âˆ’

ğ”¼

(cid:12)
ğ”¼
(cid:12)
Ëœğœ[

]

Potğœ‚â€²(
ğ‘¤

Ëœğœâ€²[

]

ğ‘Œâ€²

)
(cid:12)
so that
(cid:12)

6 20ğ¿ .

ğ‘

=

1

ğ‘â€²

1

= 1, then,

e

ğ‘

âˆ’

e
ğ‘â€²
1

6 120

(cid:13)
(cid:13)
ğ‘› .

(cid:13)
(cid:13)

ğ¿

/

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
ğ‘Œ
Proof. It is enough to prove that Potğœ‚â€²(
(cid:13)
other direction and completes the proof.

(cid:13)
Potğœ‚â€²(
(cid:13)

ğ‘Œâ€²)

) âˆ’

p
6 20ğ¿ as a symmetric argument proves the

2

ğ”¼

]

ğ‘¤

Ëœğœ[

while satisfying

Let Ëœğœ be the pseudo-distribution that minimizes

on ğ‘Œâ€² with outlier
rate ğœ‚â€² (computed in Step 3 of the algorithm on input ğ‘Œâ€²). Suppose ğ‘Œ and ğ‘Œâ€² diï¬€er on ğ‘–-th
(cid:13)
(cid:13)
(cid:13)e
sample point. Let Ëœğœğ‘ğ‘‘ğ‘— be the adjacent pseudo-distribution obtained by zeroing out ğ‘¤ğ‘–. Then,
ğ‘›.
from Lemma 4.9, we know that Ëœğœğ‘ğ‘‘ğ‘— is consistent with
ğ‘¤
Further,
Potğœ‚â€²+
(cid:12)
(cid:12)

ğ’œ
6
6 Potğœ‚â€²(
. Thus, Potğœ‚â€²+
)
6 20ğ¿. Therefore, we have Potğœ‚â€²(
(cid:13)
ğ‘Œ
(cid:13)
) âˆ’
(cid:13)e
Now, by Cauchy-Schwarz inequality, we immediately obtain that:

2
(cid:13)
(cid:13)
(cid:13)
on input ğ‘Œ with outlier rate ğœ‚â€² +
ğ‘Œâ€²)
Potğœ‚â€²(

. Further, Lemma 4.16 implies that
ğ‘Œâ€²)

6 20ğ¿ as desired.

]
2
(cid:13)
Potğœ‚â€²
(cid:13)
(cid:13)

(cid:13)
ğ‘Œ
ğ‘›(
(cid:13)
(cid:13)e

ğ‘Œ
ğ‘›(

2
(cid:13)
(cid:13)
(cid:13)

Ëœğœğ‘ğ‘‘ğ‘— [

1
/

) âˆ’

Ëœğœ[

ğ’œ

1
/

1
/

ğ”¼

ğ”¼

ğ‘¤

(cid:12)
(cid:12)

]

2

2

(cid:13)
(cid:13)
(cid:13)e
Thus, from Lemma 3.21, we have that:

ğ”¼

ğ‘¤

Ëœğœ[

] âˆ’

ğ”¼

ğ‘¤

]

Ëœğœâ€²[

e

1
(cid:13)
(cid:13)
(cid:13)

2

6 20ğ‘›ğ¿

ğ‘

âˆ’

ğ‘â€²

1

6 120

ğ‘› .

ğ¿

/

(cid:13)
(cid:13)

(cid:13)
(cid:13)

p

(cid:3)

)

)

ğ‘Œ
(

ğ‘Œ
(

is not too far from ğ‘ğ‘–

1-subgaussian distribution on ğ‘Œâ€².

Parameter closeness from potential stability The following lemma observes that if a sequence
of weights ğ‘ğ‘–
induces a 2ğ‘˜-certiï¬ably ğ¶â€²-subgaussian distribution on ğ‘Œ and ğ‘â€²ğ‘–(
ğ‘Œ
is a sequence
of weights on an adjacent ğ‘Œ such that ğ‘ğ‘–
ğ‘Œâ€²)
must also induce
(
a 2ğ‘˜-certiï¬ably ğ¶â€² +
Lemma 4.18. Let 0 6 ğ‘ğ‘–
ğ‘Œ
)
(
2ğ‘˜-certiï¬able ğ¶â€²-subgaussian distribution on ğ‘Œ. Let ğ‘ğ‘–
up to ğ‘› on ğ‘Œâ€² adjacent to ğ‘Œ such that
constant ğœ‚â€² > 0, ğ‘ğ‘–

be a sequence of non-negative weights adding up to ğ‘› that induce a
be a sequence of non-negative weights adding
ğ‘Œâ€²)
(
6 ğ›½ for ğ›½ 6 ğœ‚0. Then, for small enough absolute
ğ‘
ğ‘Œâ€²)
1
(
-subgaussian distribution on ğ‘Œ.
ğ¶â€² +
1
(cid:13)
)
(cid:13)
Proof Sketch. Letâ€™s ï¬rst describe the idea of the proof: the proof of Lemma 4.6 requires the existence
of a certiï¬ably subgaussian distribution that was close (in total variation distance) to the input ğ‘Œ.

) âˆ’
induces a 2ğ‘˜-certiï¬able

1
2ğœ‚â€²)
1
âˆ’

, then, ğ‘ğ‘–

ğ‘Œâ€²)
(

ğ‘Œâ€²)
(

ğ‘Œ
(

(cid:13)
(cid:13)

6

ğ‘

)

(

(

32

ğ‘›-close (the
Since ğ‘Œ is adjacent to ğ‘Œâ€², the 2ğ‘˜-certiï¬ably ğ¶â€²-subgaussian distribution is 1
ğ‘› comes from â€œremovingâ€ the index of the point where ğ‘Œ and ğ‘Œâ€² diï¬€er) in total variation
additive 2
/
distance to ğ‘Œ. Thus, the idea is to use the certiï¬ably subgaussian distribution supported on ğ‘Œ in
lieu of ğ‘‹ to repeat the argument. In order to apply Lemma 4.6, we need a â€œï¬‚atâ€ distributionâ€”but
this is easily achieved. Given a distribution with weights (without loss of generality, say, rational
numbers ğ‘Ÿğ‘–
ğ‘ ), we can consider a sample expansion to ğ‘›ğ‘  samples that has ğ‘Ÿğ‘– copies of sample
ğ‘¦ğ‘– for each ğ‘– and an analogous transformation to ğ‘Œâ€². And ï¬nally, given a pseudo-distribution
ğ‘Œâ€², we can transform to a pseudo-distribution on ğ‘›ğ‘  variables by each
on ğ‘¤1, ğ‘¤2, . . . , ğ‘¤ğ‘› on ğ‘Œ
(cid:3)
â€œcopyingâ€ ğ‘¤ğ‘– for ğ‘– such that ğ‘¦ğ‘– = ğ‘¦â€²ğ‘– ğ‘Ÿğ‘– times.

2
/

âˆ©

âˆ’

âˆ’

ğ›½

/

As an immediate corollary of Lemma 4.17 and Lemma 4.18, we obtain:

Corollary 4.19 (Parameter Closeness from Stability of Potential). Let ğœ‚, ğœ€, ğ›¿ > 0 and ğ‘˜, ğ¿
given input parameters to Algorithm 4.8 such that 0.25ğœ‚ğ‘› > ğ¿ = Î©
. Also, let ğ‘Œ, ğ‘Œâ€² be
adjacent subsets of â„šğ‘‘. Suppose Algdoes not reject in any of the 3 steps, uses the constant ğ¶â€² in Step 2 and
chooses ğœ‚â€² in Step 1 on input ğ‘Œ and ğ‘Œâ€².
ğ¿

Then, for every ğ‘¢

â„ğ‘‘ and ğœƒ =

ğ‘›, we have:

1
ğœ€ Â·

log

ğ‘›
ğ›½ğ›¿

(cid:17)(cid:17)

âˆˆ

(cid:16)

(cid:16)

â„• be

âˆˆ

/

p
âˆ’

ğœ‡ğ‘â€² , ğ‘¢

ğœ‡ğ‘

h

6 ğ‘‚

ğ¶â€²ğ‘˜

(

)

i

2ğ‘˜
1
ğœƒ1
/
âˆ’

ğ‘¢âŠ¤Î£ğ‘ğ‘¢ ,

q

1
(

âˆ’

ğ‘‚

ğ¶â€²ğ‘˜

1
ğœƒ1
/
âˆ’

ğ‘˜

(

)

Î£ğ‘
)

Î£ğ‘â€² (cid:22) (
1

(cid:22)

ğ‘‚

(

+

ğ¶â€²ğ‘˜

)

1
ğœƒ1
/
âˆ’

ğ‘˜

Î£ğ‘ ,
)

and, for every ğ‘¡ 6 ğ‘˜ such that ğ‘¡ divides 2ğ‘˜,

1
(

âˆ’

ğ‘‚

(

ğ‘¡
ğ¶â€²

2ğ‘˜ğ‘¡
/

2
/

ğ‘¡

ğœƒ1
âˆ’

2ğ‘˜
/

ğ‘¢ âŠ—

)h

))

ğ‘¡

ğ‘¡ , ğ‘€(

)ğ‘ i

6

ğ‘¢ âŠ—

h

ğ‘¡

ğ‘¡ , ğ‘€(

)ğ‘â€² i

6

1
(

+

ğ‘‚

(

ğ‘¡
ğ¶â€²

2ğ‘˜ğ‘¡
/

2
/

ğ‘¡

ğœƒ1
âˆ’

2ğ‘˜
/

)

ğ‘¢ âŠ—

)h

ğ‘¡

ğ‘¡ , ğ‘€(

)ğ‘ i

,

Ëœğœğ‘ğ‘‘ğ‘— satisï¬es

Proof. Let Ëœğœğ‘ğ‘‘ğ‘— be the adjacent pseudo-distribution of degree 2ğ‘˜ to Ëœğœ obtained by zeroing out ğ‘¤ğ‘–
where ğ‘– is the index of the point that ğ‘Œ and ğ‘Œâ€² diï¬€er on. Then, from Lemma 4.9, we know that
6 1,

on both inputs ğ‘Œ, ğ‘Œâ€² with outlier rate ğœ‚â€² +
6 1. Let ğ‘ğ‘ğ‘‘ğ‘— be the scaling of
ğ‘¤
ğ‘¤
Ëœğœâ€²[
2 |
(cid:13)
(cid:13)
ğ‘
ğ‘› (since ğœ‚â€² â‰ª
2). Further, applying Lemma 4.17 and triangle inequality, we have
(cid:13)
(cid:13)
âˆ’
(cid:13)e
(cid:13)
6 ğ‘‚
. Applying Fact 3.23 to ğ‘ğ‘ğ‘‘ğ‘— and ğ‘ on ğ‘Œ and ğ‘ğ‘ğ‘‘ğ‘— and ğ‘â€² on ğ‘Œâ€² and using
ğ‘›
ğ¿
that
(cid:13)
)
(
(cid:13)
(cid:3)
triangle inequality completes the proof.
p

] âˆ’
]
= 1. Then, clearly,

Ëœğœğ‘ğ‘‘ğ‘— [
ğ‘ğ‘ğ‘‘ğ‘—
ğ‘ğ‘ğ‘‘ğ‘—
(cid:13)
(cid:13)

ğ’œ
ğ”¼
6 2
/
e
ğ‘â€²
1

(cid:13)
(cid:13)
ğ‘ğ‘ğ‘‘ğ‘—
(cid:13)e
(cid:13)
(cid:13)

2 |
(cid:13)
(cid:13)
(cid:13)

so that

ğ‘› and

Ëœğœğ‘ğ‘‘ğ‘— [

Ëœğœğ‘ğ‘‘ğ‘— [

1
/

1
/

] âˆ’

Ëœğœ[

(cid:13)
(cid:13)

ğ”¼

ğ”¼

ğ”¼

ğ”¼

e

e

ğ‘¤

ğ‘¤

ğ‘¤

âˆ’

/

]

]

1

2

1

2

|

|

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Noise injection in estimate-dependent norms Our ï¬nal ingredient for obtaining privacy guaran-
tees for our robust estimation algorithms is a new noise injection mechanism where the distribution
of noise depends on the covariance estimated by our algorithm.

Lemma 4.20. Suppose ğœ€, ğ›¿ > 0. Let ğ´ be an invertible ğ‘‘
where ğ›½ 6

â„ğ‘‘ be a vector whose entries are i.i.d. from

. Let ğ‘§

Ã—

ğ‘‘ matrix that satisï¬es
ğ¼
1
ğ›½
(
)
âˆ’
. Then,
0, 1
)

ğ’©(

ğœ€
3ğ‘‘ ln

ğ‘‘

ğ›¿

/

)

(

âˆˆ

ğ´ğ´ğ‘‡

(cid:22)

1
(cid:22) (

+

ğ›½

)

ğ¼,

ğ‘§, ğ´ğ‘§

ğ·ğ‘’ ğœ€

(

)

6 ğ›¿.

33

Proof. Note that the probability distribution function of ğ´ğ‘§ at ğ‘¢

â„ğ‘‘ is

âˆˆ

1
âˆš2ğœ‹
(
)
2 = det
Moreover, det
ğ›½
(
)
(
ratio of the probability densities of ğ‘§ and ğ´ğ‘§ at ğ‘¢ is

1
det
(
2, since det
/
(

1
(

ğ´

ğ´

ğ´

6

+

)

)

)

ğ‘‘

ğ‘‘

ğ´âˆ’

1ğ‘¢

2
2/

2.

k

ğ‘’âˆ’k

ğ´

)

det

ğ´ğ‘‡

(

= det
(

)

ğ´ğ´ğ‘‡

)

6

1
(

ğ›½

)

+

ğ‘‘. Thus, the

ğ´

det
(

)

ğ‘’ k

ğ´âˆ’

1ğ‘¢

k

2
2
2/
âˆ’k

ğ‘¢

k

2
2/

2 6

6

6

6

Thus, note that if

6

ğ‘¢

k

kâˆ

2 ln

ğ‘‘

ğ›¿

, then
)

/

(

ğ‘¢

k

k2

)

)

)

)

ğ›½

ğ›½

ğ›½

ğ›½

+

+

1
(
1
(
1
(
1
(
+
6 âˆšğ‘‘

+

ğ´âˆ’

1ğ‘¢

ğ‘‘

ğ‘‘

/

2ğ‘’ k
2ğ‘’
/

1
2 ğ‘¢ğ‘‡

((

k
ğ´ğ´ğ‘‡

2
2
2/
âˆ’k

ğ‘¢

2
2
2/

k

1

)âˆ’

ğ‘¢

ğ¼

)

âˆ’

ğ‘‘

2ğ‘’
/

1
2 (

1
âˆ’

ğ›½

)âˆ’

1

k

ğ‘¢

2
2âˆ’

1
2 k

k

ğ‘¢

2
2

k

ğ‘‘

2ğ‘’
/

ğ›½
1
âˆ’

2
(

ğ›½

) k

ğ‘¢

2
2.

k

6

ğ‘¢

Â· k

kâˆ

2ğ‘‘ ln

ğ‘‘

ğ›¿

, and so,
)

/

(

p
det
(

ğ´

ğ‘’ k

)

ğ´âˆ’

1ğ‘¢

k

2
2
2/
âˆ’k

ğ‘¢

k

2
2/

2 6

1
(

+

ğ›½

)

p
ğ›½ ğ‘‘ ln
(

ğ›½

1
âˆ’

ğ‘‘

2ğ‘’
/

ğ‘‘

ğ›¿

/

) < ğ‘’ ğœ€,

.

since ğ›½ 6

ğœ€
3ğ‘‘ ln

ğ‘‘

ğ›¿

/

)

(

Moreover, by standard tail bounds of the normal distribution, we have that

with probability at most ğ›¿. This proves the claim.

ğ‘§

k

kâˆ

>

2 ln

p

ğ‘‘

ğ›¿
)
(cid:3)

/

(

Lemma 4.21. Suppose ğœ€, ğ›¿ > 0. Let ğ´ be a ğ‘‘
â„•. Moreover, let ğ‘

Ã—

Let ğ‘¡
1 6 ğ‘–1 6 ğ‘–2 6
ğ‘– =

âˆˆ

ğ‘–1, ğ‘–2, . . . , ğ‘–ğ‘‘
(
If ğ›½ 6
ğœ€
8ğ‘¡2ğ‘‘ğ‘¡ ln

Â· Â· Â·
)
ğ‘‘ğ‘¡

(

â„ğ‘‘ğ‘¡
6 ğ‘–ğ‘¡ 6 ğ‘‘, are i.i.d.
and permutation ğœ‹.

âˆˆ

, then

ğ›¿

)

/

ğ‘‘ matrix that satisï¬es

be a random vector indexed by
(cid:13)
(cid:13)
, and moreover, ğ‘ğ‘–1 ,ğ‘–2,...,ğ‘–ğ‘¡ = ğ‘ğ‘–ğœ‹
0, 1
from
)

ğ’©(

]

[

âˆ’
ğ‘¡, whose entries ğ‘ğ‘–1,ğ‘–2 ,...,ğ‘–ğ‘¡ , for
for any
,ğ‘–ğœ‹

,...,ğ‘–ğœ‹

(cid:13)
(cid:13)

ğ‘¡

1
)

(

2
)

(

(

)

6 ğ›½.

ğ¼

2

ğ´ğ´ğ‘‡
ğ‘‘

Proof. Let ğ¾ =

2 ln

ğ‘‘ğ‘¡

ğ›¿

/

(

p

ğ·ğ‘’ ğœ€

ğ‘, ğ´âŠ—

ğ‘¡ğ‘

6 ğ›¿.

(
. By standard tail bounds, note that
)

)

â„™

ğ‘

[k

kâˆ

> ğ¾

]

6 ğ›¿.

(4.10)

Let ğ‘† be the subspace of â„ğ‘‘ğ‘¡

consisting of all symmetric tensors, i.e.,

â„ğ‘‘ğ‘¡

âˆˆ

ğ‘† =

ğ‘¢

n

: ğ‘¢ğ‘–1,ğ‘–2 ,...,ğ‘–ğ‘¡ = ğ‘¢

ğ‘–ğœ‹

1
)

(

,ğ‘–ğœ‹

2
)

(

,...,ğ‘–ğœ‹

(

ğ‘¡

(

))

ğ‘– =

,

âˆ€

(

ğ‘–1, ğ‘–2, . . . , ğ‘–ğ‘¡

ğ‘‘

]

) âˆˆ [

ğ‘¡ , ğœ‹ a permutation on

ğ‘¡

[

]

.

o

Note that ğ‘† is an ğ‘‘â€²-dimension invariant subspace of ğ´âŠ—
ğ‘¡ be a representative set of indices of size
ğ‘‘
ğ‘…
]
ğ‘–1, ğ‘–2, . . . , ğ‘–ğ‘¡

ğ‘…
|
ğ‘¡, there exists a permutation ğœ‹ on
ğ‘† be the restriction of ğ´âŠ—

Now, let ğ‘€ = ğ´âŠ—

) âˆˆ [

âŠ† [

ğ‘‘

]

]

(

|

ğ‘¡

ğ‘‘

ğ‘¡, where ğ‘‘â€² =

ğ‘¡
6 ğ‘‘ğ‘¡. Moreover, let
1
âˆ’
+
ğ‘¡
= ğ‘‘â€², i.e., ğ‘… satisï¬es the property that for any
ğ‘¡

(cid:1)

(cid:0)
, ğ‘–ğœ‹

such that

ğ‘–ğœ‹
ğ‘¡
(
ğ‘¡ to the subspace ğ‘†. Moreover, let ğ‘ğ‘…

, . . . , ğ‘–ğœ‹

2
)

1
)

[

(

(

(

ğ‘….
â„ğ‘‘â€² denote

)) âˆˆ
âˆˆ

the projection of ğ‘ to indices in ğ‘….

|

34

Note that the probability distribution of ğ‘ can be equivalently viewed as the probability distri-
bution of ğ‘ğ‘…, since ğ‘ is uniquely determined by the projection ğ‘ğ‘…. Let ğ‘ be the probability density
function of ğ‘ğ‘… over â„ğ‘‘â€². Then, note that the probability distribution of ğ‘€ğ‘ğ‘… is ğ‘, given by

=

ğ‘

ğ‘£

(

)

1
ğ‘€
det
(

)

ğ‘

(

Â·

ğ‘€âˆ’

1ğ‘£

.

)

âˆˆ

for ğ‘£
by the ğ‘–th singular value of ğ´âŠ—
ğ¼
Moreover, by
Hence, the singular values of ğ‘€ also lie in
implies that

â„ğ‘‘â€². By standard properties, we know that the ğ‘–th singular value of ğ‘€ is bounded from above
ğ‘¡.
th singular value of ğ´âŠ—
ğ‘¡
2
2,
.
ğ›½
/
/
]
âˆ’
, which, together with ğ›½ğ‘¡ 6 1
4 ,

ğ‘‘ğ‘¡
ğ‘¡ and bounded from below by the
ğ‘‘â€²)
âˆ’
6 ğ›½, we know that the singular values of ğ´âŠ—
ğ‘¡ lie in
1
[(

ğ´ğ´ğ‘‡

1
[(

1
(

1
(

2,
/

2
/

(cid:13)
(cid:13)

(cid:13)
(cid:13)

âˆ’

+

+

+

âˆ’

ğ›½

ğ›½

ğ›½

]

)

)

)

)

(

ğ‘–

2

ğ‘¡

ğ‘¡

ğ‘¡

and so,

ğ‘€

det
(

6

2 = det
(
)
ğ‘¡ğ‘‘ğ‘¡

2.
/

2ğ›½ğ‘¡
ğ‘€
and so, det
1
)
(
+
(
â„ğ‘‘ğ‘¡
Let ğ‘¢
. Note that
ğ‘£

âˆˆ
Moreover, note that if

k

)
k
kâˆ
ğ‘
ğ‘£
(
ğ‘£
ğ‘
(

By (4.11), we have that

(

(cid:13)
(cid:13)

ğ‘€ ğ‘€ğ‘‡

ğ¼

2

âˆ’

6 2ğ›½ğ‘¡

(cid:13)
(cid:13)
ğ‘€ğ‘‡
)

= det
(

(cid:13)
(cid:13)
ğ‘€ ğ‘€ğ‘‡

ğ‘€

det
(

)

6

)

1
(

+

2ğ›½ğ‘¡

)

ğ‘‘â€² 6

ğ‘¡

Â·

2ğ›½ğ‘¡

1
(

+

ğ‘¡ğ‘‘ğ‘¡

,

)

(4.11)

(4.12)

6 ğ¾ if and only if ğ‘£

ğ‘¢
kâˆ
6 ğ¾, then

â„ğ‘‘â€² given by ğ‘£ = ğ‘¢

ğ‘… also satisï¬es

|

6 ğ¾.

ğ‘£

k

kâˆ

âˆˆ

6 det
(

ğ‘€

) Â·

)
)

6

1
(

+

2ğ›½ğ‘¡

)

ğ‘

(
ğ‘¡ğ‘‘ğ‘¡

2
/

Â·

ğ‘
ğ‘£
)
(
1ğ‘£
ğ‘€âˆ’

)
exp

1
2

6 ğ‘’ ğ›½ğ‘¡2ğ‘‘ğ‘¡

6 ğ‘’ ğ›½ğ‘¡2ğ‘‘ğ‘¡

Â·

Â·

exp

exp

(cid:18)

ğ‘€ ğ‘€ğ‘‡

1
)âˆ’

âˆ’

ğ¼

2

(cid:18)
6 2ğ›½ğ‘¡
2ğ›½ğ‘¡
1
âˆ’

(cid:18)
(cid:16)(cid:13)
(cid:13)
ğ‘€ ğ‘€ğ‘‡

1
âˆ’
)

1
2

ğ‘£ğ‘‡

((

ğ‘€âˆ’

1ğ‘£

ğ‘£

2
2

k

(cid:17) (cid:19)

2
2 âˆ’ k
(cid:13)
(cid:13)
âˆ’

ğ‘£

)

ğ¼

(cid:19)

(cid:0)

1
2 k

ğ‘£

2
2 Â·

k

(

ğ‘€ ğ‘€ğ‘‡

1
âˆ’
)

âˆ’

(cid:1)
ğ¼

2

(cid:19)

(cid:13)
ğ‘’ ğ›½ğ‘¡2ğ‘‘ğ‘¡
(cid:13)

Â·

exp

2ğ¾2ğ‘‘ğ‘¡ğ›½ğ‘¡

.

(cid:13)
(cid:13)

(cid:13)
(cid:13)
6 4ğ›½ğ‘¡, since ğ›½ğ‘¡ 6 1
4 . Therefore, (4.13) is at most

(4.13)

Thus, if ğ›½ 6
proves the desired claim.

ğœ€
2ğ¾2 ğ‘¡ğ‘‘ğ‘¡ =

ğœ€
8ğ‘¡2ğ‘‘ğ‘¡ ln

ğ‘‘ğ‘¡

(

ğ›¿

)

/

, the above quantity is at most ğ‘’ ğœ€. This, combined with (4.10),
(cid:3)

(cid:0)

(cid:1)

Remark 4.22. Note that Lemma 4.21 uses an assumption on the spectral norm of
2. However,
it is also possible to obtain a version of the lemma under an assumption on the Frobenius norm,
(cid:13)
6 ğ›½, then Eq. (4.12) instead
(cid:13)
ğ´ğ´ğ‘‡

In particular, if we assume that, instead,

ğ´ğ´ğ‘‡

(cid:13)
(cid:13)

âˆ’

ğ¼

ğ¼

ğ¼

ğ´ğ´ğ‘‡

âˆ’

ğ¹

ğ¹.

1

6

(cid:13)
(cid:13)

ğ‘¡ğ‘‘ğ‘¡

2
/

âˆ’
ğ›½
(cid:13)
ğ‘€
becomes det
(cid:13)
âˆšğ‘‘
(
ğœ†1, ğœ†2, . . . , ğœ†ğ‘‘ of ğ´ğ´ğ‘‡ satisfy
(cid:17)
is maximized when ğœ†1 = ğœ†2 =
Ã
ğ‘‘ğ‘¡ ğ‘¡-fold products of eigenvalues of ğ´ğ´ğ‘‡.

ğ‘‘
ğ‘–=1(
Â· Â· Â·

6 ğ‘’ ğ›½ğ‘¡ğ‘‘ğ‘¡

+

(cid:16)

)

1
2

âˆ’

2: This follows from the fact that (a.) the eigenvalues
/

(cid:13)
(cid:13)
2 6 ğ›½2, (b.) under the aforementioned constraint, ğœ†1ğœ†2 Â· Â· Â·
1
ğœ†ğ‘–
)
âˆ’
= ğœ†ğ‘‘ = 1

ğœ†ğ‘‘
ğ‘¡ are precisely the

, (c.) the eigenvalues of

ğ´ğ´ğ‘‡

(cid:13)
(cid:13)

(

)âŠ—

ğ›½
âˆšğ‘‘

+

35

Putting things together Now, we are ready to prove the main privacy guarantee provided by our
robust moment estimation algorithm, Algorithm 4.8.

Lemma 4.23 (Privacy Guarantee). Suppose ğ¶ , ğœ‚, ğœ€, ğ›¿ > 0 and ğ‘˜

2ğ‘˜
ğ‘˜
1

âˆ’

(cid:19)

(cid:1)(cid:17)

Î©

ğ¶ ğ‘˜4 ğ‘‘ğ‘˜
ğœ€

ln

6ğ‘˜ğ‘‘ğ‘˜
(

/

ğ›¿

) +

ğœ€
6ğ‘˜

. Then, Alg (given by Algorithm 4.8), invoked with ğ¿ = ğ‘‚

ğ‘›

log
(
(

/

ğ›¿

)/

ğœ€

,
)

(cid:18) (cid:16)
ğœ€, ğ›¿
(

is
-DP.
(cid:0)
e
)
Proof. Let ğœ€â€² = ğœ€
it suï¬ƒces to show that each step of the algorithm is
steps as parameter3). Let ğ‘Œ and ğ‘Œâ€² be any neighboring datasets.

3 and ğ›¿â€² = ğ›¿

ğœ€â€², ğ›¿â€²)

/

/

(

3. By our adaptive composition theorem under halting (Lemma 3.30),
-DP (given the outputs of the previous

â„•.

Suppose ğ‘› > ğ‘›0 =

âˆˆ

â€¢ Stable Outlier Rate Selection. Since this step invokes the

(Selection ), it immediately follows from Theorem 3.34 that this step is

ğœ€â€², ğ›¿â€²)

(

-DP Selection algorithm
-DP.

ğœ€â€², ğ›¿â€²)

(

â€¢ Witness Checking. Let ğ¶âˆ—(
ğ‘Œ

induces a 2ğ‘˜-certiï¬able
)
6 Î” for Î” = 1.
ğ¶âˆ—(
ğ¶âˆ—-subgaussian distribution on ğ‘Œ. Lemma 4.18 ensures that
Therefore, we may apply Lemma 3.27 with DP parameters ğœ€â€², ğ›¿â€² to conclude that this step is
also

denote the smallest ğ¶âˆ— for which ğ‘ğ‘–
ğ‘Œ
(
ğ‘Œ
ğ¶âˆ—(

)
) âˆ’

ğ‘Œâ€²)|

-DP.

|

ğœ€â€², ğ›¿â€²)

(

â€¢ Noise Addition. Since the algorithm has not halted in the previous step and the truncated
must induce 2ğ‘˜-certiï¬able ğ¶â€²-subgaussian distri-
ğœ‡â€² denote the corresponding mean estimates
Î£â€² denote the corresponding

Laplace noise is negative, ğ‘ğ‘–
ğ‘Œâ€²)
and ğ‘ğ‘–
(
butions on ğ‘Œ and ğ‘Œâ€² respectively. Let
ğœ‡ and
Î£ and
under ğ‘ğ‘–
covariance estimates. By Corollary 4.19, we have that, for all ğ‘¢

, respectively, and, similarly, let
e

and ğ‘ğ‘–

ğ‘Œâ€²)
(

ğ‘Œ
(

ğ‘Œ
(

e

)

)

and, for all 2 6 ğ‘¡ 6 ğ‘˜,

1
(

âˆ’

6 ğ›¾1

1
(cid:22) (

p
+

e
Î£ğ‘¢
ğ‘¢âŠ¤
Î£
ğ›¾2)
e
e

ğœ‡â€², ğ‘¢

i
Î£â€²

(cid:22)
e

ğœ‡
âˆ’
h
Î£
ğ›¾2)
e
e

e
ğ‘¡ ,

â„ğ‘‘,

âˆˆ
e

(4.14)

(4.15)

(4.16)

ğ‘¡ ,

ğ‘¡

ğ‘€(

,

)i

ğ‘¡

2ğ‘˜ for 2 6 ğ‘¡ 6 ğ‘˜. Moreover,
/

g

ğ›¾ğ‘¡

1
(
where ğœƒ =
ğ‘›, ğ›¾1 = ğ‘‚
ğ¿
/
2.
1
2
1
Î£â€²
Î£âˆ’
let ğµ =
/
/
p

âˆ’

ğ‘¢ âŠ—

ğ‘¡ ,

ğ‘¡

ğ‘€(

6

ğ‘¢ âŠ—

ğ‘¡

ğ‘€â€²(

h

)i
)i
2ğ‘˜ , and ğ›¾ğ‘¡ = ğ‘‚
1
ğœƒ1
(cid:157)
g
/
âˆ’

)h
ğ¶â€²ğ‘˜

(

)

((

6

1
(
ğ¶â€²ğ‘˜

+
ğ‘¡
2
/

)h
ğœƒ1
âˆ’

)

)

ğ›¾ğ‘¡

ğ‘¢ âŠ—

Note that in order to show that the noise addition step is

e

e

-DP, it suï¬ƒces to show that

ğœ€â€², ğ›¿â€²)
(
2ğ‘§,
Î£1
2ğ‘§
1
Î£â€²
ğœ‡â€²
/
/
+
2
1
2ğ‘
1
Î£â€²
Î£â€²
/
/
e
e
ğ‘¡ ğ‘(
2
1
Î£â€²
âŠ—
/
e
e

ğ·ğ‘’ ğœ€â€²â€² (
ğœ‡
+
2,
Î£1
2ğ‘
Î£1
Î£â€²
/
/
e
e
ğ‘¡
ğ‘€â€²(
),
) + (
e
e
ğ‘˜, since Fact 3.32 and standard DP composition [DKM+06] then
-DP. We now establish each of the above

Î£
ğ·ğ‘’ ğœ€â€²â€² (
+
2
Î£1
/
) + (
e

6 ğ›¿â€²â€²

(4.17)

(4.19)

(4.18)

(cid:157)

ğ‘¡ ğ‘(

g

ğ‘€(

âŠ—
e
)

e

+

)

)

)

)

)

ğ‘¡

ğ‘¡

ğ‘¡

6 ğ›¿â€²â€²
6 ğ›¿â€²â€²

2 < ğ‘¡ 6 ğ‘˜, ğ·ğ‘’ ğœ€â€²â€² (

âˆ€
ğ‘˜ and ğ›¿â€²â€² = ğ›¿â€²/
for ğœ€â€²â€² = ğœ€â€²/
e
imply that the entire noise addition step is
inequalities.

ğœ€â€², ğ›¿â€²)

(

3Note that we may also assume that the algorithm has not halted from the previous steps.

36

Noise addition for mean: We ï¬rst show (4.17). Note that

ğ·ğ‘’ ğœ€â€²â€² (
ğœ‡

+

2ğ‘§,
Î£1
/

ğœ‡â€²

2ğ‘§
1
Î£â€²
/

)

+

e

e

e

e

)

2ğ‘§
1
Î£â€²
/

ğµğ‘§

ğœ‡

= ğ·ğ‘’ ğœ€â€²â€² (
= ğ·ğ‘’ ğœ€â€²â€² (
ğ‘§,
e
= ğ·ğ‘’ ğœ€â€²â€²/
ğ‘§, ğ‘§
2
(
+
e
2ğ·ğ‘’ ğœ€â€²â€²/
ğ‘’ ğœ€â€²â€²/
2
+
= ğ·ğ‘’ ğœ€â€²â€²/
ğ‘§, ğ‘§

2ğ‘§,
Î£1
ğœ‡â€²
/
âˆ’
(
) +
2
1
Î£âˆ’
ğœ‡â€²
ğœ‡
/
âˆ’
(
) +
e
e
e
2
1
Î£âˆ’
ğœ‡
ğœ‡â€²
/
âˆ’
(
e
e
2
1
Î£âˆ’
ğ‘§
/
(
+
e
e
2
1
Î£âˆ’
ğœ‡â€²
/
(
e

âˆ’

+

(

2

))
ğœ‡â€²
(
e
ğœ‡
e

)

ğœ‡

2
1
Î£âˆ’
/

,

âˆ’

)
ğ·ğ‘’ ğœ€â€²â€²/
)) +
e
e

2

(

ğœ‡â€²
âˆ’
(
ğ‘§, ğµğ‘§
e

)

ğœ‡

) +

ğµğ‘§

)

,
e

(4.20)

(4.21)

note that

where (4.20) follows from Lemma 3.33. For the ï¬rst term on the right-hand side of (4.21), we
e
6 ğ›¾1 (which follows from plugging in ğ‘¢ =
Thus, by the standard hockey-stick divergence calculation for the Gaussian mechanism [DR14,
Appendix A], we have that

2
1
Î£âˆ’
/
(cid:13)
(cid:13)
(cid:13)e

ğœ‡
)
(cid:13)
(cid:13)
(cid:13)

into (4.14)).

ğœ‡â€² âˆ’
(

ğœ‡â€²)

1
Î£âˆ’

ğœ‡
(

e

e

e

e

e

e

e

âˆ’

2

ğ·ğ‘’ ğœ€â€²â€²/

2

(

ğ‘§, ğ‘§

+

2
1
Î£âˆ’
/

ğœ‡â€²
(

âˆ’

ğœ‡

))

< ğ›¿â€²â€²

2,

/

(4.22)

provided that

ğœ1 >

e
2ğ›¾1

e
e
2.5
2 ln
ğ›¿â€²â€²)
/
(
ğœ€â€²â€²

p

,

For the second term in (4.21), note that (4.15) implies that
Moreover, ğ›¾2 6

(cid:22)
, by the condition ğ‘› > ğ‘›0. Thus, by Lemma 4.20,

ğ›¾2)

1
(

âˆ’

ğ¼

3ğ‘‘ ln

ğœ€â€²â€²
2ğ‘‘

(

ğ›¿â€²â€²)

/

ğ·ğ‘’ ğœ€â€²/

2

(

ğ‘§, ğµğ‘§

)

6 ğ›¿â€²â€²

2.

/

Therefore, (4.22), (4.23),and (4.21) imply (4.17), as desired.

Noise addition for covariance: Next, we establish (4.18). Observe that

ğµğµğ‘‡

1
(cid:22) (

+

ğ¼.

ğ›¾2)

(4.23)

+

Î£
ğ·ğ‘’ ğœ€â€²â€² (
e

2ğ‘
Î£1
/

2,
Î£1
/

Î£â€²

2ğ‘
1
Î£â€²
/

2
1
Î£â€²
/

)

+

e

e

e

e

e

= ğ·ğ‘’ ğœ€â€²â€² (
= ğ·ğ‘’ ğœ€â€²â€² (
6 ğ·ğ‘’ ğœ€â€²â€²/
ğ‘’ ğœ€â€²â€²/
+
6 ğ·ğ‘’ ğœ€â€²â€²/

2

2

ğ¼

+
ğ‘,

ğ‘, ğµğµğ‘‡
ğµğµğ‘‡

+
ğ¼
âˆ’
) +
ğµğµğ‘‡

+ (
ğ‘
2
+ (
ğµğµğ‘‡

(

+ (

(
ğ‘, ğ‘
(
2ğ·ğ‘’ ğœ€â€²â€²/
ğ‘, ğ‘

(

ğµğ‘ğµğ‘‡

)
ğµğ‘ğµğ‘‡

)

ğ¼
âˆ’
ğµğµğ‘‡

))

ğ¼

âˆ’

,
)
(
ğ‘’ ğœ€â€²â€²/

ğµğµğ‘‡
ğ¼
âˆ’
2ğ·ğ‘’ ğœ€â€²â€²/

ğµğ‘ğµğ‘‡
) +
))
ğ‘, ğµğ‘ğµğ‘‡
2

(4.24)

,

(4.25)

(

))

ğ¼

âˆ’

)) +

where (4.24) follows from Lemma 3.33. To bound the right-hand side of (4.25), note that the
ï¬rst term is precisely the hockey-stick divergence computation corresponding to the Gaussian
mechanism (restricted to the upper triangular portion). Moreover, by (4.15),

Therefore ([DR14, Appendix A]), as long as

(cid:13)
(cid:13)

(cid:13)
(cid:13)

ğµğµğ‘‡

ğ¼

ğ¹

âˆ’

6 âˆšğ‘‘

ğµğµğ‘‡

ğ¼

2

âˆ’

6 ğ›¾2âˆšğ‘‘.

(cid:13)
(cid:13)

Â·

(cid:13)
(cid:13)

(4.26)

ğœ2 >

2ğ›¾2

2ğ‘‘ ln
2.5
/
(
ğœ€â€²â€²

p

ğ›¿â€²â€²)

,

37

we have that

ğ·ğ‘’ ğœ€â€²â€²/

2

ğ‘, ğ‘

ğµğµğ‘‡

ğ¼

6 ğ›¿â€²â€²

2.

+ (
1
2 ğ‘ has entries distributed in
For the second term in (4.25), note that ğœâˆ’
ğ‘â€² = vec
(
Fact 3.17, we know that ğµğ‘ğµğ‘‡ = ğµâŠ—

0, 1
. Moreover, let
)
be the ğ‘‘2-dimensional vector given by the ï¬‚attening of ğ‘ (see Deï¬nition 3.15). By

2ğ‘â€². Thus, by Lemma 4.21 applied with ğ‘¡ = 2,

ğ’©(

))

âˆ’

ğ‘

/

(

)

(4.27)

ğ·ğ‘’ ğœ€â€²â€²/

2

(

ğ‘, ğµğ‘ğµğ‘‡

))

as long as

2

(

= ğ·ğ‘’ ğœ€â€²â€²/
= ğ·ğ‘’ ğœ€â€²â€²/
6 ğ›¿â€²â€²

2ğ‘â€²
ğ‘â€², ğµâŠ—
)
2
1
2 ğ‘â€², ğµâŠ—
ğœâˆ’
(
2,
2ğ‘’ ğœ€â€²â€²/

2

/

1
2 ğ‘â€²
ğœâˆ’

(

))

(4.28)

ğ›¾2 <

32ğ‘‘2 ln

ğœ€â€²â€²
2
2ğ‘‘2ğ‘’ ğœ€â€²â€²/
(
which is true, since ğ‘› > ğ‘›0 by the conditions of the theorem. Thus, (4.27) and (4.28) imply that
(4.25) is at most ğ›¿â€²â€²/

= ğ›¿â€²â€²/
Noise addition for higher-order moments: Let 2 < ğ‘¡ 6 ğ‘˜. We write ğ‘… =
Î£â€² +
Observe that the injective/spectral norm
e

e
can be bounded as

2, which establishes (4.18).

ğ‘‡ for simplicity.

kÂ·kğœ of

ğœ‡ğ‘‡ and

2
2ğ‘’ ğœ€â€²â€²/

2
1
ğ‘…âˆ’
/

ğ›¿â€²â€²/

2
ğ‘’ ğœ€â€²â€²/

ğ›¿â€²â€²)

ğ‘…â€² =

ğ‘€â€²(

) âˆ’

ğ‘€(

)âŠ—

ğœ‡â€²

ğœ‡â€²

e

e

Î£

+

+

ğœ‡

2

/

(

)

,

)

ğ‘¡

ğ‘¡

ğ‘¡

e

e

2
1
ğ‘…âˆ’
/

ğ‘¡

âŠ—

)

(
(cid:13)
(cid:13)
(cid:13)

ğ‘¡

ğ‘€â€²(

) âˆ’

ğ‘¡

ğ‘€(

)

(cid:157)

g

(cid:16)

ğœ

(cid:17)(cid:13)
(cid:13)
(cid:13)

(
ğ‘‡

2
1
ğ‘…âˆ’
/

(

)

(cid:16)
(cid:157)
ğ‘¡
ğ‘€â€²(
âŠ—

ğ‘¡

(cid:17)
ğ‘¡
ğ‘€(

g

)

) âˆ’

(cid:1)
2ğ‘£
1
ğ‘…âˆ’
/

(cid:16)

(cid:157)
ğ‘€(

) âˆ’

ğ‘¡

)i

ğ‘¡ ,

âŠ—

ğ‘¡

ğ‘€â€²(

)

2ğ‘£
1
ğ‘…âˆ’
/

h(

)

g

(cid:157)
ğ‘¡ ,

âŠ—

ğ‘€(

ğ‘¡

)i

g

(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
ğ‘‡

ğ‘…

2ğ‘£
1
ğ‘…âˆ’
/

(cid:17)

(cid:16)

(cid:17) (cid:19)

(4.29)

ğ‘¡

2
/

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

g

2ğ‘£
1
ğ‘…âˆ’
/

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k

(cid:18) (cid:16)

ğ‘£

ğ‘¡
2

k

sup
â„ğ‘‘
ğ‘£
âˆˆ
k2=1
ğ‘£
k
sup
â„ğ‘‘
ğ‘£
âˆˆ
k2=1
ğ‘£

k

ğ‘¡

(cid:0)

h(

ğ‘£âŠ—

= sup
â„ğ‘‘
ğ‘£
k2=1 (cid:12)
âˆˆ
ğ‘£
(cid:12)
k
(cid:12)
6 sup
â„ğ‘‘
ğ‘£
k2=1 (cid:12)
âˆˆ
ğ‘£
(cid:12)
(cid:12)
sup
â„ğ‘‘
ğ‘£
k2=1 (cid:12)
âˆˆ
ğ‘£
(cid:12)
(cid:12)
ğ‘¡

k
6 ğ›¾ğ‘¡

Â·

k

6 ğ›¾ğ‘¡

ğ¶â€²ğ‘˜

Â· (

)

Â·

= ğ›¾ğ‘¡

ğ¶â€²ğ‘˜

ğ‘¡

)

Â·

Â· (

= ğ›¾ğ‘¡

ğ¶â€²ğ‘˜

ğ‘¡ ,

)
where (4.29) follows from the ğ¶â€²-subgaussianity property of the distribution induced by the weight
vector at the end of Step 2. Therefore, the Frobenius norm (or Hilbert-Schmidt norm) can be
bounded as (see Corollary 4.10 of [WDFS17])

Â· (

6 ğ‘‘

ğ‘¡

1

âˆ’
2

2
1
ğ‘…âˆ’
/

ğ‘¡

âŠ—

ğ‘¡

ğ‘€â€²(

ğ‘¡

ğ‘€(

)

(

) âˆ’
)
(
ğ¹
(cid:17)(cid:13)
(cid:13)
2, we have
1
2ğ‘…â€²
1
(cid:13)
(cid:13)
Moreover, letting ğ‘Š = ğ‘…âˆ’
g
/
/
(cid:13)
(cid:13)
ğ‘¡ ğ‘(

2
1
ğ‘…â€²
/

2
ğ‘…1
/

(cid:157)

ğ‘¡ ğ‘(

),

(cid:16)

âŠ—

âŠ—

Â·

ğ‘¡

ğ‘¡

ğ‘¡

ğ‘€â€²(

ğ‘€(

(cid:13)
(cid:13)
(cid:13)
ğ·ğ‘’ ğœ€â€²â€² (

) + (

) + (

)

)

g

(cid:157)

2
1
ğ‘…âˆ’
/

ğ‘¡

âŠ—

)

(cid:16)

ğ‘¡

ğ‘€â€²(

) âˆ’

(cid:157)

ğ‘¡

)

)

= ğ·ğ‘’ ğœ€â€²â€²

2
ğ‘…1
/

âŠ—

)

(

(cid:16)

38

ğ‘¡

ğ‘€(

)

g

(cid:17)(cid:13)
(cid:13)
(cid:13)
ğ‘¡
ğ‘¡ ğ‘(

6 ğ›¾ğ‘¡

ğ¶â€²ğ‘˜

ğ‘¡

)

Â·

Â· (

ğœ

ğ‘¡

1
2 .
âˆ’

ğ‘‘

(4.30)

),

ğ‘¡

ğ‘€â€²(

ğ‘¡

ğ‘€(

2
1
ğ‘…â€²
/

)

) + (

) âˆ’

ğ‘¡

ğ‘¡ ğ‘(

)

âŠ—

(cid:157)

g

(cid:17)

= ğ·ğ‘’ ğœ€â€²â€²

ğ‘(

ğ‘¡

),

2
1
ğ‘…âˆ’
/

(

)

ğ‘¡

âŠ—

ğ‘¡

ğ‘€â€²(

(cid:16)
2

6 ğ·ğ‘’ ğœ€â€²â€²/
ğ‘’ ğœ€â€²â€²/

+

2
1
ğ‘…âˆ’
/

(
6 ğ·ğ‘’ ğœ€â€²â€²/
ğ‘’ ğœ€â€²â€²/

2

+

ğ‘¡

ğ‘(

), ğ‘(

ğ‘¡

)

+ (

(cid:16)
2
1
ğ‘…âˆ’
(cid:157)
âŠ—
/
)
2
1
ğ‘…âˆ’
/

(cid:16)
2ğ·ğ‘’ ğœ€â€²â€²/

ğ‘(

ğ‘¡

)

2

(
ğ‘€â€²(

ğ‘¡

ğ‘¡

+ (
ğ‘€(

ğ‘¡

)

âŠ—

)

) âˆ’
(cid:16)
ğ‘¡
ğ‘¡
ğ‘(
), ğ‘(
(cid:157)
(cid:16)
2ğ·ğ‘’ ğœ€â€²â€²/

ğ‘(

)

ğ‘¡

2

+
(cid:17)
2
1
ğ‘…âˆ’
g
/

+ (
), ğ‘Š âŠ—

)
ğ‘¡
ğ‘¡ ğ‘(

(

) âˆ’
ğ‘¡

ğ‘¡

ğ‘€(

)

ğ‘¡
ğ‘€â€²(
g

+

(cid:17)
) âˆ’
ğ‘¡

ğ‘Š âŠ—

ğ‘¡

ğ‘¡ ğ‘(

)

(cid:17)

ğ‘¡

ğ‘€(

)

(cid:17) (cid:17)
ğ‘¡
)

,

ğ‘€(
g

(cid:16)
ğ‘¡

âŠ—

)

ğ‘Š âŠ—

) âˆ’

ğ‘€â€²(
(cid:157)
(cid:16)
ğ‘¡
ğ‘¡ ğ‘(
(cid:157)
)
))

(cid:17)

g

(4.31)

ğ‘¡

âŠ—

,

)

)

(cid:16)

ğ‘¡

ğ‘€â€²(

) âˆ’

ğ‘¡

ğ‘€(

)

(cid:157)

g

(cid:17) (cid:17)

(4.32)

where again we have used Lemma 3.33 in (4.31). In order to bound the right-hand side of (4.32),
note that the ï¬rst term is again the hockey-stick divergence computation corresponding to the
Gaussian mechanism (restricted according to symmetry conditions). Recalling (4.30), we see that
([DR14, Appendix A]) as long as

ğœğ‘¡ >

2ğ›¾ğ‘¡

ğ¶â€²ğ‘˜

(

)

1

ğ‘¡

âˆ’
2

ğ‘¡ ğ‘‘

ğœ€â€²â€²
p

2 ln

2.5
/
(

ğ›¿â€²â€²)

,

we have that

ğ·ğ‘’ ğœ€â€²â€²/

2

ğ‘¡

ğ‘(

), ğ‘(

ğ‘¡

)

2
1
ğ‘…âˆ’
/

ğ‘¡

âŠ—

)

+ (

ğ‘¡

ğ‘€â€²(

) âˆ’

ğ‘¡

ğ‘€(

)

6 ğ›¿â€²â€²

2.

/

(4.33)

1
ğ‘¡ ğ‘(
For the second term in (4.32), note that ğœâˆ’
that

0, 1
. Moreover, note
(cid:157)
)
6 ğ›¾2âˆšğ‘‘ by (4.26) and the fact that (4.15) implies

) has entries distributed in

ğ‘Šğ‘Š ğ‘‡

ğµğµğ‘‡

g

ğ’©(

6

ğ¼

ğ¼

ğ‘¡

(cid:16)

(cid:17)(cid:17)

(cid:16)

ğ¹

âˆ’

ğ¹

âˆ’

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

1
(

âˆ’

ğ‘…

ğ›¾2)

(cid:22)

ğ‘…â€²

1
(cid:22) (

+

ğ‘….

ğ›¾2)

Thus, by Lemma 4.21, we have that

ğ·ğ‘’ ğœ€â€²â€²/

2

ğ‘¡

ğ‘(

), ğ‘Š âŠ—

ğ‘¡

ğ‘¡ ğ‘(

)

(cid:16)

(cid:17)

as long as

2

= ğ·ğ‘’ ğœ€â€²â€²/
6 ğ›¿â€²â€²

(cid:16)
2ğ‘’ ğœ€â€²â€²/

1
ğ‘¡ ğ‘(
ğœâˆ’
2,

ğ‘¡

), ğ‘Š âŠ—

ğ‘¡

1
ğ‘¡ ğ‘(
ğœâˆ’

ğ‘¡

)

(

)

(cid:17)

(4.34)

/

ğœ€â€²â€²
2ğ‘‘ğ‘¡ ğ‘’ ğœ€â€²â€²/
2
(
which is true since ğ‘› > ğ‘›0, by the hypothesis of the lemma. Thus, (4.33) and (4.34) imply that (4.32)
(cid:3)
is at most ğ›¿â€²â€²/

= ğ›¿â€²â€², which establishes (4.19), as desired.

16ğ‘¡2ğ‘‘ğ‘¡ ln

2
2ğ‘’ ğœ€â€²â€²/

ğ›¿â€²â€²/

2
ğ‘’ ğœ€â€²â€²/

ğ›¿â€²â€²)

ğ›¾2 <

+

2

/

(

)

,

4.4 Proof of Theorem 4.1

We are now ready to prove our main theorem, Theorem 4.1.

Proof of Theorem 4.1. Choose ğ›½ = 1
/
in Lemma 4.16). Moreover, let ğ¶ = ğ¶0 +

30. Choose ğ¿ = Î©
3
(
/
ğœ€

9
ğœ€ +

3 ln

+

ğ›¿

)

ğ‘›
ğ›½ğ›¿

log

1
ğœ€ Â·
(cid:16)
1. Then, we claim that setting Alg to be

(according to the condition

(cid:17) (cid:17)

(cid:16)

39

Algorithm 4.8 with parameters ğ¶ , ğœ‚, ğœ€, ğ›¿, ğ¿, ğ‘˜ satisï¬es the desired conditions, as long as ğœ‚ 6 ğœ‚0,
where we set ğœ‚0 later.

with mean ğœ‡

, covariance Î£
âˆ—

Note that the desired privacy guarantees follow immediately from Lemma 4.23.
It remains to prove the utility guarantees. Suppose that there indeed exists a good set ğ‘‹
ğœ‚

, and ğ‘¡-th moments ğ‘€(
âˆ—
By Theorem 3.34, we have that Step 1 (stable outlier rate selection) rejects and halts with
2. In particular,
is feasible. By

30, and the resulting output ğœ satisï¬es score
(
2 for which

probability at most ğ›½ = 1
/
the latter condition implies that there exists some ğ›¾ > ğ¿
ğ‘›.
monotonicity,

is also feasible, where we let ğœ‚â€² = ğœ

for 3 6 ğ‘¡ 6 ğ‘˜, such that

> ğ¿
ğ›¾
ğœ
âˆ’
ğ‘›

âŠ†
ğ‘›.
)

ğœ, ğ‘Œ

1
(

â„šğ‘‘

ğ’œ

ğœ‚â€²

ğ‘‹

>

âˆ©

âˆ’

ğ‘Œ

/

/

)

|

|

âˆ—

)

ğ‘¡

(cid:0)

Hence, the invocation of Algorithm 4.3 in Step 2 does not yield â€œreject.â€ Moreover, note that
(cid:1)
by Lemma 3.28, we have that ğ¶â€² = ğ¶
30. In this case, the
computed weight vector ğ‘ induces a ğ¶â€²-certiï¬ably subgaussian distribution on ğ‘Œ. Hence, the
30.
probability of rejection in Step 2 is at most 1
/
ğ”¼
) =
Î£ =

(for 2 6 ğ‘¡ 6 ğ‘˜) be the estimates of the mean,
Î£
]
covariance, and ğ‘¡-th moments, respectively, that are outputted by the Algorithm 4.3 subroutine in
Step 2 of Algorithm 4.8. Then, by Lemma 4.5, we have

ğ›¾ > ğ¶0 with probability at least 29
/

, and

ğœ‡ =

Î£
]

Let

ğ‘€(

Ëœğœ[

Ëœğœ[

Ëœğœ[

ğ”¼

ğ”¼

e

e

e

e

e

+

ğœ‡

]

,

ğ‘¡

e

/

(cid:0)

(cid:1)

ğ’œ

â„ğ‘‘,

ğ‘¢

âˆ€

âˆˆ

ğœ‡

h

âˆ’

6 ğ‘‚

âˆšğ¶ ğ‘˜

2ğ‘˜
1
ğœ‚1
/
âˆ’

ğ‘¢âŠ¤Î£
âˆ—

ğ‘¢

ğœ‡

, ğ‘¢

âˆ—

i
Î£
ğ›½2)

1
(
e

Î£

(cid:16)
1
(cid:22) (

(cid:17)
Î£
ğ›½2)
+
âˆ—

p

âˆ— (cid:22)
and, for all even 2 6 ğ‘¡ 6 ğ‘˜ such that ğ‘¡ divides 2ğ‘˜,
e

âˆ’

where ğ›½ğ‘¡ = ğ›½ğ‘¡
= ğ‘‚
Note that this guarantees that ğ›½ğ‘¡ = ğ›½ğ‘¡

ğœ‚
(

ğ¶ ğ‘˜

((

)

)

â„ğ‘‘ ,

ğ‘¢

âˆ€

âˆˆ

1
(
ğ‘¡

2
/

ğ›½ğ‘¡

âˆ’
ğœ‚1
âˆ’
)

ğ‘¡

ğ‘¡
)
âˆ— i

ğ‘¢ âŠ—

ğ‘¡ , ğ‘€(

6

ğ‘¢ âŠ—

ğ‘¡ ,

ğ‘€(

ğ‘¡

)

6

+
h
)h
)h
6 1
2ğ‘˜ . We now set ğœ‚0 such that ğ›½ğ‘¡
ğœ‚0)
/
(

e

2 , since we are assuming ğœ‚ 6 ğœ‚0.

6 1

i

1
(

ğ›½ğ‘¡

ğ‘¢ âŠ—

ğ‘¡ , ğ‘€(

ğ‘¡
)
âˆ— i

,

ğœ‚
(

)

2 for all aforementioned ğ‘¡.

Now, consider the noise addition step, i.e., Step 3 of Algorithm 4.8. Note that by the Cauchy-

Schwarz Inequality, for any ğ‘¢

â„ğ‘‘, we have

âˆˆ
2ğ‘§, ğ‘¢
Î£1
/
h

i

e

=

6

=

6

2
2ğ‘§, Î£1
Î£1
/
/
âˆ—

ğ‘¢

2
1
Î£âˆ’
/
h
âˆ—
2
1
2ğ‘§
Î£1
Î£âˆ’
/
/
e
âˆ—
(cid:13)
ğ‘§ğ‘‡
1
2Î£âˆ’
Î£1
(cid:13)
/
e
(cid:13)
(
âˆ—

i
2
Î£1
/
âˆ—

ğ‘¢

ğ‘¢

2 Â·
2
(cid:13)
(cid:13)
(cid:13)
2ğ‘§
Î£1
ğ‘¢âŠ¤Î£
(cid:13)
(cid:13)
(cid:13)
/
(cid:13)
(cid:13)
(cid:13)
) Â·
âˆ—
2
Î£1
1
2Î£âˆ’
Î£1
p
/
/
e
âˆ—
(cid:13)
2ğ›½2
(cid:13)
(cid:13)e
+
Â·
ğ‘¢ğ‘‡Î£
ğ‘¢,
(cid:1)
âˆ—

ğ‘¢âŠ¤Î£
e
âˆ—

p

+

âˆ’
ğ‘¢

k

ğ‘§

ğ‘§

1

(cid:16)
1

2
2 Â·
e
k
2
2 Â·
k
2
ğ‘§
(cid:0)
2 Â·

k

k

6

k
6 2

ğ‘¢âŠ¤Î£
âˆ—

ğ‘¢

Â·

2

(cid:17)

p

ğ¼

(cid:13)
(cid:13)
(cid:13)

p
since ğ›½2 6 1
2 by our choice of ğœ‚0. Now, note that with probability at least 1
6 ğ‘‚
ğ‘§
ğœ1

, in which case it follows that

ğ‘‘ ln

ğ‘˜ğ‘‘

1
30ğ‘˜ , we have that

âˆ’

k

k2

(

)

(cid:16)

p

(cid:17)

2
2 = ğ‘‚

ğ‘§

k

k

ğ‘‘

(

) Â·

ğœ2
1 ln

ğ‘˜ğ‘‘

)

(

= ğ‘‚

âˆšğ¶ ğ‘˜
(

2ğ‘˜ ,
1
ğœ‚1
/
âˆ’
)

40

by our choice of ğ‘› > ğ‘›0. Thus, the mean estimate

ğœ‡
h Ë†

âˆ’

, ğ‘¢

ğœ‡

âˆ—

i

ğœ‡

ğœ‡

=

=

, ğ‘¢

i + h

ğœ‡ outputted by the Step 3 satisï¬es
Ë†
ğœ‡, ğ‘¢
ğœ‡
h Ë†
âˆ’
2ğ‘§, ğ‘¢
Î£1
/
h
e
âˆšğ¶ ğ‘˜
= ğ‘‚
e
(cid:16)

ğœ‡
i + h
âˆ’
e
2ğ‘˜
1
ğœ‚1
/
âˆ’
e

i
âˆ—
ğ‘¢ğ‘‡Î£
âˆ—

, ğ‘¢

ğ‘¢.

âˆ’

ğœ‡

(cid:17)

i

âˆ—

p

(4.35)

Next, we consider the utility guarantee for the covariance. Note that

ğ‘‚

ğœ2

ğ‘‘ ln

ğ‘˜ğ‘‘2

)

(

with probability at least 1

âˆ’

(cid:16)

p

Wigner matrices; see, for instance, [Tao12]), in which case, it follows that
Moreover, by our choice of ğ‘› > ğ‘›0 as well as ğœ‚0, we have that ğœˆ2 6 ğ›½2 6 1

(cid:17)

6 ğœˆ2 =
1
30ğ‘˜ (this follows from standard spectral properties of
Î£.

k2

Î£

ğ‘

k

ğœˆ2

ğœˆ2

2
Î£1
/

2ğ‘
Î£1
/
(cid:22)
âˆ’
2 . Thus, it follows that
e

e

e

(cid:22)

e

Î£
Ë†

1
(cid:22) (
1
(cid:22) (

+

+

Î£
ğ›½2)
2Î£
ğ›½2)
âˆ—
e
5
ğ›½2
2

(cid:19)
ğ¶ ğ‘˜

Î£
âˆ—

+

(cid:18)

1

1

(cid:22)

(cid:22)

ğ‘‚

(

+

(cid:16)
Î£
and by a similar argument, we also have Ë†

1

ğ‘‚

ğ¶ ğ‘˜

1
ğœ‚1
/
âˆ’

ğ‘˜

) Â·

ğ‘‚

1
ğœ‚1
/
âˆ’

ğ‘˜

ğ¶ ğ‘˜

(

) Â·

,

Î£
âˆ—

(cid:17)
1
ğœ‚1
/
âˆ’

ğ‘˜

1

ğ‘‚

ğ¶ ğ‘˜

(cid:1)
) Â·

1

(cid:23)
Î£

âˆ’
(cid:0)
Î£
âˆ— (cid:22) Ë†

, thus implying that

Î£
âˆ—
1
ğœ‚1
/
âˆ’

ğ‘˜

.

Î£
âˆ—

(4.36)

(

âˆ’

(cid:22)
Finally, we consider the utility guarantee for moment estimation. Suppose 2 6 ğ‘¡ 6 ğ‘˜ and ğ‘¡ is
. Note that for any 2 < ğ‘¡ 6 ğ‘˜, we
ğœ‡ğ‘‡
âˆ—
1
30ğ‘˜ . In this case, note that for any

âˆ— +
with probability at least 1

an even number dividing 2ğ‘˜. Let ğ´ =

ğœ‡ğ‘‡ and ğ´

ğœğ‘¡ ğ‘‘ğ‘¡

have

= ğ‘‚

= Î£

ğ‘˜ğ‘‘ğ‘¡

ğ‘(

ln

) Â·

Î£

+

+

ğœ‡

ğœ‡

(cid:17)

(cid:17)

(cid:16)

(cid:16)

(

âˆ—

âˆ—

)

ğ‘¡

ğ¹

2
/
â„ğ‘‘, we have the following (recall that
(cid:13)
(cid:13)

(cid:13)
(cid:13)

e

(cid:16)

(cid:17)

(

)

ğ‘¢

âˆˆ

p
2
ğ´1
/

ğ‘¡ ,

ğ‘¢ âŠ—

h

(

ğ‘¡

ğ‘¡ ğ‘(

âŠ—

)

)

i

=

6

âˆ’

e
e
kÂ·kğœ indicates the injective norm of a tensor):
2ğ‘¢
ğ´1
/

ğ‘¡ , ğ‘(

âŠ—

)

ğ‘¡

)

ğœ Â·

h(
ğ‘(

ğ‘¡

)

ğ‘(

6

(cid:13)
(cid:13)
= ğ‘‚
(cid:13)
(cid:13)
= ğ‘‚

= ğ‘‚

= ğ‘‚

(

(

(

(

(cid:13)
(cid:13)
(cid:13)
(cid:13)
p

ğ‘¡

)

(cid:13)
(cid:13)
ğ¹ Â·
ğœğ‘¡ ğ‘‘ğ‘¡
2
(cid:13)
/
(cid:13)
ğœğ‘¡ ğ‘‘ğ‘¡
2
/
ğ‘‘ğ‘¡

ğœğ‘¡

(

2
p
/

ğœğ‘¡

(

ğ‘‘

p
1
+
(
ğ‘¡
ğ‘‘ğ‘’ ğ›½2

i
2ğ‘¢
ğ´1
/

2ğ‘¢
ğ´1
/

ğ‘¡
2
ğ‘¡
(cid:13)
(cid:13)
2
ğ‘˜ğ‘‘ğ‘¡
(cid:13)
(cid:13)
)) Â·
ğ‘˜ğ‘‘ğ‘¡

(

ln

ln

(
ln

ğ‘˜ğ‘‘ğ‘¡

ğ‘¡

(
ğ›½2))
ln

2
/

ğ‘¡
2
ğ‘¡
(cid:13)
(cid:13)
)
ğ›½2)

2ğ‘¢
ğ´1
/

ğ‘¢ğ‘‡ ğ´ğ‘¢
(cid:13)
(cid:13)
)) Â· (

1
)) Â· ((
2
ln
/

+
ğ‘˜ğ‘‘ğ‘¡

ğ‘¡

2
/

ğ‘¡

ğ‘¢

)
ğ‘¢

ğ‘¢ğ‘‡ ğ´

âˆ—
2
ğ´1
/
âˆ—
(cid:13)
ğ‘¡ , ğ‘€(
(cid:13)
(cid:13)
,

ğ‘¡
)
âˆ— i

2
(cid:13)
(cid:13)
(cid:13)

(

)) Â·
ğ‘¢ âŠ—
ğ‘¡
ğ‘¡ , ğ‘€(
)
âˆ— i

ğœğ‘¡

= ğ‘‚

2
/
)
(
2ğ‘˜
ğ‘¡
ğœ‚1
p
/
âˆ’
)
where (4.37) follows from Jensenâ€™s Inequality, and (4.38) follows from our choice of ğ‘› > ğ‘›0. Thus,
the moment estimate Ë†ğ‘€(
ğ‘¡, Ë†ğ‘€(

) outputted by our algorithm satisï¬es

) + (
ğ‘¡ ,
ğ‘¢ âŠ—

(
ğ¶ ğ‘˜

(4.37)

(4.38)

2
ğ´1
/

2
ğ´1
/

= ğ‘‚

)) Â· h

ğ‘¡ ğ‘(

ğ‘¡ ğ‘(

) =

ğ‘€(

ğ‘€(

ğ‘¢ âŠ—

ğ‘¢ âŠ—

ğ‘¢ âŠ—

)âŠ—

Â· h

2
/

ğ‘¡,

((

âŠ—

)

(

)

)

)

ğ‘¡

ğ‘¡

ğ‘¡

ğ‘¡

ğ‘¡

ğ‘¡

ğ‘¡

i + h

(

)

i

6
e
h

i

h

p
ğ‘˜ğ‘‘ğ‘¡

e

41

6

=

ğ›½ğ‘¡

ğ‘‚

+

+

1
(

1

(cid:16)

ğ‘¡ , ğ‘€(

ğ‘¢ âŠ—

)h

ğ¶ ğ‘˜

ğ‘¡

2
/

)

((

ğ‘¡
)
âˆ— i +
ğ‘¡
ğœ‚1
âˆ’
)

2ğ‘˜
/

ğ¶ ğ‘˜

ğ‘‚

((

ğ‘¢ âŠ—

h

ğ‘¡

2
/

ğœ‚1
âˆ’
)
)
ğ‘¡ , ğ‘€(

ğ‘¡
)
âˆ— i

.

ğ‘¡

2ğ‘˜
/

ğ‘¢ âŠ—

ğ‘¡ , ğ‘€(

ğ‘¡
)
âˆ— i

Â· h

(cid:17)

âˆ’

(cid:0)
ğ‘¡

)

ğ‘¢ âŠ—

ğ‘¡, Ë†ğ‘€(

i

In a similar fashion, we also get that
that

h

ğ‘¢ âŠ—

ğ‘¡

ğ‘¡ , Ë†ğ‘€(

)i

>

1

ğ¶ ğ‘˜

ğ‘‚

((

ğ‘¡

2
/

ğœ‚1
âˆ’
)

)

ğ‘¡

2ğ‘˜
/

ğ‘¢ âŠ—

h

ğ‘¡ , ğ‘€(

ğ‘¡
)
âˆ— i

, thus implying

1

ğ‘‚

ğ¶ ğ‘˜

âˆ’

((

ğ‘¡

2
/

ğœ‚1
âˆ’
)

)

ğ‘¡

2ğ‘˜
/

h

ğ‘¢ âŠ—

ğ‘¡ , ğ‘€(

6

ğ‘¡
)
âˆ— i

h

6

1

ğ‘‚

ğ¶ ğ‘˜

+

((

ğ‘¡

2
/

)

ğ‘¡

2ğ‘˜
/

ğ‘¢ âŠ—

ğ‘¡ , ğ‘€(

.

(4.39)

ğ‘¡
)
âˆ— i

h

(cid:16)

(cid:17)

(cid:16)
Hence, (4.35), (4.36), and (4.39) imply the desired utility guarantees.
Moreover, recall that the rejection probabilities at Steps 1 and 2 are each at most 1

30 , and it is not
possible to reject in Step 3. Moreover, the 6 ğ‘˜ utility guarantees each fail with probability at most
1
30ğ‘˜ . Thus, by a union bound, it follows that the algorithm does not reject and, moreover, outputs
30ğ‘˜ = 9
10 .
estimates satisfying the desired utility guarantees with probability at least 1
) follows from the time complexity guarantee in
(
Lemma 4.5, as the invocation of Algorithm 4.3 in Step 2 is the bottleneck. Steps 1 and 3 are easily
(cid:3)
seen to run in

Finally, note that the running time of

) time. This completes the proof.

1
30 âˆ’

1
30 âˆ’

ğµğ‘›

ğµğ‘›

âˆ’

ğ‘˜

ğ‘‚

ğ‘‚

(cid:17)

(

)

1

Â·

ğ‘˜

ğ‘˜

(

(

)

(cid:1)
ğœ‚1
âˆ’
)

5 Robust Mean and Covariance Estimation for Certiï¬ably Hypercon-

tractive Distributions

In this section, we observe that we can upgrade our guarantees from the previous section for robust
estimation of moments of distributions that have certiï¬ably hypercontractive degree 2 polynomials.

Deï¬nition 5.1. A distribution ğ‘ğ· on â„ğ‘‘ with mean ğœ‡
certiï¬ably ğ¶-hypercontractive degree 2 polynomials if for a ğ‘‘
and

ğœ‡

,

âˆ—

Ã—

ğ‘¥ = ğ‘¥
Â¯

âˆ’

âˆ—

and covariance Î£
âˆ—

is said to have 2â„-
ğ‘‘ matrix-valued indeterminate ğ‘„

ğ‘„
2â„

ğ‘¥

(cid:26)

ğ”¼
âˆ¼

ğ‘¥âŠ¤ğ‘„

ğ·( Â¯

ğ‘¥
Â¯

âˆ’

ğ‘¥

ğ”¼
âˆ¼

ğ· Â¯

ğ‘¥âŠ¤ğ‘„

2â„ 6
)

ğ‘¥
Â¯

(

ğ¶ â„

2â„
)

2
Î£1
/
âˆ—

2
ğ‘„Î£1
/
âˆ—

.

2â„

ğ¹

(cid:27)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

The Gaussian distribution [KOTZ14], uniform distribution on the hypercube and more gen-
erally other product domains and their aï¬ƒne transforms are known to satisfy 2ğ‘¡-certï¬ably ğ¶-
hypercontractivity with an absolute constant ğ¶ for every ğ‘¡.

In order to derive this conclusion, we note the following analog of the witness-producing

algorithm and its guarantees:

Witness-producing version of the robust moment estimation algorithm We will use the follow-
ing (non-private) guarantees for the robust moment estimation algorithm in the previous section
that hold for a strengthening of the constraint system
with certiï¬able hypercontractivity con-
straints. Using the analysis of [DKK+16], the following guarantees were recently shown in [KMZ21]
for the case when the unknown distribution is Gaussian.

ğ’œ

For any ğ‘‘

Ã—

ğ‘‘ matrix-valued indeterminate ğ‘„, let Â¯ğ‘¥â€²ğ‘–

âŠ¤ğ‘„ Â¯ğ‘¥â€²ğ‘–

= ğ‘¥â€²âŠ¤ğ‘„ ğ‘¥â€² âˆ’

1
ğ‘›

ğ‘›
ğ‘–=1 ğ‘¥â€²ğ‘– âŠ¤ğ‘„ ğ‘¥â€².

Ã

42

: Constraint System for ğœ‚-Robust Moment Estimation

ğ’œ

1. ğ‘¤2
ğ‘–

= ğ‘¤ğ‘– for each 1 6 ğ‘– 6 ğ‘›,

2. Î 2 = 1
ğ‘›

3.

ğ‘›

ğ‘–=1 ğ‘¤ğ‘– >
Ã

4. ğœ‡â€² = 1
Ã
ğ‘›

ğ‘›
ğ‘–=1(
1
(
ğ‘– ğ‘¥â€²ğ‘–,

ğ‘¥â€²ğ‘–

ğœ‚

âˆ’

ğ‘¥â€²ğ‘–

ğœ‡â€²)âŠ¤,

âˆ’

ğœ‡â€²)(
âˆ’
ğ‘›,

)

5. ğ‘¤ğ‘–

(

6. 1
ğ‘›

Ã
ğ‘¦ğ‘–
ğ‘¥â€²ğ‘– âˆ’
ğ‘›
ğ‘–=1 Â¯ğ‘¥â€²ğ‘–

= 0 for 1 6 ğ‘– 6 ğ‘›,

)

âŠ¤ğ‘„ Â¯ğ‘¥â€²ğ‘–

2 6 ğ¶

Î ğ‘„Î 

k

2
ğ¹.

k

The following guarantees for the algorithm above were shown in [BK20a].

Ã

Fact 5.2 ([BK20a]). Let ğ‘‹
ğœ‚-corruption of ğ‘‹. Then, for ğœ‡â€² = 1
ğ‘›

âŠ†

ğ‘– ğ‘¥â€²ğ‘–, Î£â€² = 1

ğ‘›

â„ğ‘‘ be an i.i.d. sample of size ğ‘› > ğ‘›0 =

, Î£

ğœ‡

ğ’©(

âˆ—

âˆ—)

. Let ğ‘Œ be an

)

(

ğ‘‘2
ğ‘‚
from
ğœ‚
/
ğœ‡â€²)âŠ¤, we have:
âˆ’
e
ğ‘¢âŠ¤Î£
,
âˆ—

ğ‘¢2

ğ‘¥ğ‘–

ğ‘–(
6 ğ‘‚

ğ‘¥ğ‘–

ğœ‡â€²)(
âˆ’
2ğ‘˜
1
ğœ‚1
/
âˆ’
(
6 ğ‘‚

)
ğ‘˜

Ã
)

ğ‘¢
ğ‘˜
ğ’œ ğ‘‚
(
ğ‘¢
ğ‘˜
ğ’œ ğ‘‚

(

)

(cid:8)
h

ğœ‡â€²

âˆ’
h
ğ‘¢, Î£â€²

, ğ‘¢

ğœ‡

âˆ—

Ã
i

, ğ‘¢

Î£
âˆ—

i

âˆ’

ğ’œ ğ‘‚

ğ‘˜

(

2
1
Î£âˆ’
/
âˆ—

2
1
Î£â€²Î£âˆ’
/
âˆ—

ğ¼

âˆ’

(cid:8)
) (cid:26)(cid:13)
(cid:13)
(cid:13)

1
ğœ‚1
/
âˆ’
(
2

6 ğ‘‚

(cid:9)
ğ‘¢

,

ğ‘¢âŠ¤Î£
âˆ—

)

ğ‘˜

1
ğœ‚1
/
âˆ’
(

)

(cid:9)

.

(cid:27)

ğ¹

(cid:13)
(cid:13)
(cid:13)

The ï¬rst two guarantees of the lemma below were shown in [BK20a]. The third guarantee
follows from an argument similar to that of Lemma 4.6. Notice that the key diï¬€erence in the
guarantees below (compared to the ones in Lemma 4.5) is the bound on the Frobenius (instead of
the weaker spectral) distance between the estimated covariance and true unknown covariance.

âŠ†

Lemma 5.3 (Guarantees for Witness-Producing Robust Moment Estimation Algorithm). Given a
1
subset of of ğ‘› points ğ‘Œ
(
)
returns a sequence of weights 0 6 ğ‘1, ğ‘2, . . . , ğ‘ğ‘› satisfying
and either (a.) outputs â€œreject,â€ or (b.)
ğ‘ğ‘› = 1.
ğ‘1 +
ğ‘2 + Â· Â· Â· +
Moreover, if there exists a set ğ‘‹

â„šğ‘‘ whose entries have bit complexity ğµ, Algorithm 4.3 runs in time

, then Algorithm 4.3 does not reject, and the corresponding estimates

â„ğ‘‘ of points with 4-certiï¬ably ğ¶-hypercontractive degree 2 polynomials
ğœ‡ =
Ë†

ğµğ‘›

âŠ†

ğ‘‚

)

(

âˆ—

ğœ‡
âˆ’ Ë†

)(

ğ‘¦ğ‘–

ğœ‡
âˆ’ Ë†

)âŠ¤ satisfy the following guarantees:

with mean ğœ‡
1
ğ‘›

, covariance Î£
âˆ—
ğ‘›
Î£ =
ğ‘– ğ‘ğ‘– ğ‘¦ğ‘– and Ë†
ğ‘¦ğ‘–
ğ‘–=1 ğ‘ğ‘–
1. Mean Estimation:
Ã

Ã

(

ğ‘¢

âˆ€

âˆˆ

â„ğ‘‘ ,

ğœ‡
h Ë†

âˆ’

, ğ‘¢

ğœ‡

âˆ—

i

6 ğ‘‚

âˆšğ¶
(

4
ğœ‚3
/
)

ğ‘¢âŠ¤Î£
âˆ—

ğ‘¢ ,

2. Covariance Estimation:

3. Witness: For ğ¶â€² 6 ğ¶

ğ‘‚

1
(

+

2
1
Î£âˆ’
/
âˆ—

(cid:13)
(cid:13)
2
ğœ‚1
/
(cid:13)
(

,
))

p
2
ğ¶ğœ‚1
/

)

,

6 ğ‘‚

(

2
1
Î£Î£âˆ’
/
Ë†
âˆ—

ğ¼

âˆ’

ğ¹

(cid:13)
(cid:13)
(cid:13)

ğ‘ğ‘–

ğ‘¦ğ‘–

 h

ğœ‡, ğ‘„

âˆ’ Ë†

ğ‘¦ğ‘–

(

ğœ‡
âˆ’ Ë†

)i âˆ’

1
ğ‘›

ğ‘›

Ã•ğ‘–=1

ğ‘„

ï£±ï£´ï£´ï£²
ï£´ï£´
ï£³

ğ‘ğ‘–

ğ‘¦ğ‘–

h

ğœ‡, ğ‘„

âˆ’ Ë†

ğ‘¦ğ‘–

(

ğœ‡
âˆ’ Ë†
)i!

1
ğ‘›

ğ‘›

Ã•ğ‘–=1

43

2

6 ğ¶â€²

Î£1
/
Ë†

2
Î£1
2ğ‘„ Ë†
/

(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
(cid:13)

ğ¹ ï£¼ï£´ï£´ï£½
ï£´ï£´
ï£¾

We can now use the above witness-producing algorithm to obtain a stronger Frobenius norm
-privacy for Gaussian distributions. Notice that the only change
estimation guarantee with
)
and the corresponding change
from the previous section is in the choice of the constraint system
in the witness checking step.

ğœ€, ğ›¿

ğ’œ

(

Algorithm 5.4 (Private Robust Moment Estimation).

Given: A set of points ğ‘Œ =

ğ‘¦1, ğ‘¦2, . . . , ğ‘¦ğ‘›

{

} âŠ†

â„šğ‘‘, parameters ğœ‚, ğœ€, ğ›¿ > 0, ğ¿

â„•.

âˆˆ

Output: Estimates

ğœ‡ and Ë†
Ë†

Î£ for mean and covariance.

Operation:

1. Stable Outlier Rate Selection: Use the

2 to
with the scoring function as deï¬ned in Deï¬nition 4.12.

-DP Selection with ğœ… = ğ¿
3
)

3, ğ›¿

ğœ€

/

/

/

(

sample an integer ğœ
If ğœ =

ğœ‚ğ‘›

]

âˆˆ [

, then reject and halt. Otherwise, let ğœ‚â€² = ğœ

âŠ¥

ğ‘›.

/

3 ln

ğ’œ

ing

tLap

2. Witness Checking: Compute a pseudo-distribution Ëœğœ of degree ğ‘‚
1
)
(
. Let ğ›¾
)
ğ‘¤

ğ‘Œ
on input ğ‘Œ with outlier rate ğœ‚â€² and minimizing Potğœ‚â€², Ëœğœ(
ğ”¼
1
Ëœğœ[

. Check that the weight vector ğ‘ =
ğ¶

distribution on ğ‘Œ that has
-certiï¬ably hypercontractive polynomials. If not,
ğ›¾
(cid:17)
)
Î£
ğœ‡ =
reject immediately. Otherwise, let
]
ğ¿

3. Noise Addition: Let ğ›¾1 = ğ‘‚

âˆ¼
induces a

1
4 . Let ğ‘§

satisfy-

, 3
/

and

Î£ =

3
(
/
ğœ€

ğ”¼

ğ”¼

e

+

âˆ’

+

ğœ‡

ğ‘›

ğ‘›

ğœ€

(cid:16)

(cid:17)

(cid:16)

]

(

ğ‘‘

ğ›¿

.

)

]

Ëœğœ[
1
4 and ğ›¾2 = ğ‘‚
e
)

e

Ëœğœ[
ğ¶â€²)(
e

(

/

)

ğ‘‘

1

ğ¿
ğ¶â€²)(
/
e

(

+

âˆ¼ ğ’©(

0, ğœ2)(

and ğ‘
independent lower-triangular entries, and ğœğ‘— = 12ğœ€âˆ’
Then, output:

2 ), where we interpret ğ‘ has a symmetric ğ‘‘
15
/
(

1ğ›¾ğ‘—

2 ln

Ã—
ğ›¿
)

âˆ¼ ğ’©(

0, ğœ1)
ğ‘‘ matrix with
for 1 6 ğ‘— 6 2.

p

â€¢

â€¢

ğœ‡ =
Ë†
Î£ =
Ë†

+

+

ğœ‡
Î£
e
e

2ğ‘§.
Î£1
/
2ğ‘
Î£1
/
e
e

e

2.
Î£1
/

The parameter closeness from potential stability is also upgraded from Corollary 4.19:

Lemma 5.5 (Parameter Closeness from Stability of Potential). Let ğœ‚, ğœ€, ğ›¿ > 0 and ğ¿
âˆˆ
input parameters to Algorithm 5.4 such that 0.25ğœ‚ğ‘› > ğ¿ = Î©
. Also, let ğ‘Œ, ğ‘Œâ€² be adjacent
subsets of â„šğ‘‘. Suppose Algdoes not reject in any of the 3 steps, uses the constant ğ¶â€² in Step 2 and chooses ğœ‚â€²
in Step 1 on input ğ‘Œ and ğ‘Œâ€².

1
ğœ€ Â·

â„• be given

log

ğ‘›
ğ›½ğ›¿

(cid:17) (cid:17)

(cid:16)

(cid:16)

Then, for every ğ‘¢

â„ğ‘‘ and ğœƒ =

ğ¿

âˆˆ

ğ‘›, we have:

and

p
ğœ‡ğ‘

h

/
ğœ‡ğ‘â€² , ğ‘¢

âˆ’

6 ğ‘‚

4
ğœƒ3
/

ğ¶â€²
)

(

i

ğ‘¢âŠ¤Î£ğ‘ğ‘¢

q

1
2
ğ‘ Î£ğ‘â€²
Î£âˆ’
/

2
1
Î£âˆ’
/
ğ‘

ğ¼

âˆ’

6 ğ‘‚

ğ¶â€²

2 .
ğœƒ1
/

(

)

ğ¹
(cid:13)
The following theorem summarizes our privacy and utility guarantees for the algorithm above.
(cid:13)
(cid:13)
We specialize to the â€œbase case assumptionâ€ of 4-certiï¬able ğ¶-hypercontractivity of degree 2 poly-
nomials in order to derive explicit bounds here. Our analysis of the algorithm above follows mutatis

(cid:13)
(cid:13)
(cid:13)

44

mutandis with the key upgrade being the stronger Frobenius norm guarantees in Lemma 4.18 that
(this requires us
hold under certiï¬ably hypercontractivity constraints in our constraint system
ğ’œ
2; see
to use a version of Lemma 4.21 that makes use of a bound on
ğ¹ instead of
Î© notation hides logarthmic multiplicative
the remark at the end of Lemma 4.21). As before, the
(cid:13)
(cid:13)
factors in ğ‘‘, ğ¶, 1
ğœ€, and ln
ğœ‚, 1
/
/

1
/
(

ğ´ğ´ğ‘‡

ğ´ğ´ğ‘‡

.
)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

âˆ’

âˆ’

ğ›¿

ğ¼

ğ¼

Theorem 5.6 (Private Robust Mean and Covariance Estimation for Certiï¬ably Hypercontractive
Distributions). Fix ğ¶0 > 0. Then, there exists an ğœ‚0 > 0 such that for any given outlier rate 0 < ğœ‚ 6 ğœ‚0 and
ğœ€, ğ›¿ > 0, there exists a randomized algorithm Alg that takes an input of ğ‘› > ğ‘›0 =
ğ¶4
â„šğ‘‘ (where ğ¶ = ğ¶0 +

points ğ‘Œ =
bit complexity of the entries of ğ‘Œ) and outputs either â€œrejectâ€ or estimates
following guarantees:

(cid:18)
(cid:16)
ğ‘‚
1), runs in time
ğµğ‘›
(
e
(
)
â„šğ‘‘ and Ë†
Î£

+
(cid:19)
(cid:17)
1
) (where ğµ is the
ğ‘‘ with the

ğ‘¦1, ğ‘¦2, . . . , ğ‘¦ğ‘›

9
ğœ€ +

1
ğ›¿
)ğœ€
/

3
(
/
ğœ€

} âŠ†

â„šğ‘‘

ğ‘‘8
ğœ‚2

ğœ‡
Ë†

Î©

3 ln

+

âˆˆ

âˆˆ

1

ln

{

Ã—

4

ğ›¿

Â·

(

)

e

1. Privacy: Alg is

ğœ€, ğ›¿

-diï¬€erentially private with respect to the input ğ‘Œ, viewed as a ğ‘‘-dimensional
)

(

database of ğ‘› individuals.

2. Utility: Suppose there exists a 4-certiï¬ably ğ¶0-subgaussian set ğ‘‹ =
ğ‘‘
poly

and covariance Î£

2âˆ’

(

>

âˆ©
10 over the random choices of the algorithm, Alg outputs estimates

âˆ— (cid:23)

âˆ’

âˆ—

ğ‘‹

that
ğ‘Œ
|
least 9
/
satisfying the following guarantees:

ğœ‚0)

1
(

|

ğ‘› with mean ğœ‡

ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘›

â„šğ‘‘ such
{
)ğ¼. Then, with probability at
ğ‘‘
â„šğ‘‘ and Ë†
Î£

} âŠ†

â„šğ‘‘

Ã—

ğœ‡
Ë†

âˆˆ

âˆˆ

and,

ğ‘¢

âˆ€

âˆˆ

â„ğ‘‘ ,

ğœ‡
h Ë†

âˆ’

, ğ‘¢

ğœ‡

âˆ—

i

6 ğ‘‚

4
âˆšğ¶ğœ‚3
/
(

ğ‘¢âŠ¤Î£
âˆ—

ğ‘¢ ,

)
p

(cid:13)
(cid:13)
Moreover, the algorithm succeeds (i.e., does not reject) with probability at least 9
(cid:13)
/
of the algorithm.

(cid:13)
(cid:13)
(cid:13)

2
1
Î£âˆ’
/
âˆ—

2
1
Î£Î£âˆ’
/
Ë†
âˆ—

ğ¼

ğ‘‚

ğ¶âˆšğœ‚

.

âˆ’

ğ¹ (cid:22)

(

)

10 over the random choices

When specialized to Gaussian distributions, the Frobenius guarantee above is suboptimalâ€”
the robust estimation algorithms of [DKK+16] allow estimating the mean and covariance of the
unknown Gaussian distribution to an error
. We can in fact recover the stronger guarantees
ğœ‚
)
(
by relyong on the analysis in [KMZ21][Theorem 1 and 2] of the same constraint system above for
e
the case of Gaussian distributions (in the â€œutility caseâ€). This yields the following corollary:

ğ‘‚

Theorem 1.3 (Mean and Covariance Estimation for Gaussian Distributions). Fix ğœ€, ğ›¿ > 0. Then,
there exists an absolute constant ğœ‚0 > 0 such that for any given outlier rate 0 < ğœ‚ 6 ğœ‚0, there exists a
randomized algorithm Alg that takes an input of ğ‘› > ğ‘›0 =
â„šğ‘‘, runs in

points ğ‘Œ

Î©

1

ln

4

(

ğ‘‘8
ğœ‚4

(cid:18)

(cid:16)

+

1
ğ›¿
)ğœ€
/

(cid:19)

(cid:17)

âŠ†

(

ğ‘‚

time
ğœ‡
Ë†

1
) (where ğµ is the bit complexity of the entries of ğ‘Œ) and outputs either â€œrejectâ€ or estimates
ğµğ‘›
(
)
â„šğ‘‘ and Ë†
Î£
âˆˆ
1. Privacy: Alg is

-diï¬€erentially private with respect to the input ğ‘Œ, viewed as a ğ‘‘-dimensional
)

ğ‘‘ with the following guarantees:

ğœ€, ğ›¿

â„šğ‘‘

e

âˆˆ

Ã—

(

database of ğ‘› individuals.

45

{

2. Utility: Let ğ‘‹ =
with mean ğœ‡
probability at least 9
/
Î£
Ë†

â„šğ‘‘

Ã—

âˆ—

âˆˆ

and covariance Î£

ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘›

be an i.i.d. sample of size ğ‘› > ğ‘›0 from a Gaussian distribution
)ğ¼ such that ğ‘Œ is an ğœ‚-corruption of ğ‘‹. Then, with
2âˆ’
â„šğ‘‘ and

10 over the random choices of the algorithm, Alg outputs estimates

}
âˆ— (cid:23)

poly

ğ‘‘

(

ğœ‡
Ë†

âˆˆ

ğ‘‘ satisfying the following guarantees:

â„ğ‘‘,

ğ‘¢

âˆ€

âˆˆ

ğœ‡
h Ë†

âˆ’

, ğ‘¢

ğœ‡

âˆ—

i

log

ğ›¿

)

1
(
/
ğœ€

Â·

ğ‘¢âŠ¤Î£
âˆ—

ğ‘¢ ,

(cid:19) p

6

ğ‘‚

ğœ‚

(cid:18)

e

and,

2
1
Î£âˆ’
/
âˆ—

2
1
Î£Î£âˆ’
/
Ë†
âˆ—

ğ¼

âˆ’

ğ‘‚

ğœ‚

ğ¹ (cid:22)

log

ğ›¿

)

1
(
/
ğœ€

,

!

Â· r

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

e

where the
ğœ‚. In particular, ğ‘‘TV
ğ‘‚ hides multiplicative logarithmic factors in 1
/
ğ‘‚
1
ğœ‚ log
ğ›¿
/
(
(
e

.
)

)/

ğœ€

Î£
ğœ‡, Ë†
)

(ğ’©( Ë†

,

ğœ‡

ğ’©(

âˆ—

, Î£

âˆ—))

<

e
References

[AAK21]

Ishaq Aden-Ali, Hassan Ashtiani, and Gautam Kamath. On the sample complexity of
privately learning unbounded high-dimensional gaussians. In Vitaly Feldman, Katrina
Ligett, and Sivan Sabato, editors, Algorithmic Learning Theory, 16-19 March 2021, Virtual
Conference, Worldwide, volume 132 of Proceedings of Machine Learning Research, pages
185â€“216. PMLR, 2021. 2

[Abo18]

John M. Abowd. The u.s. census bureau adopts diï¬€erential privacy. KDD â€™18, page
2867, New York, NY, USA, 2018. Association for Computing Machinery. 2

[AL21]

Hassan Ashtiani and Christopher Liaw. Private and polynomial time algorithms for
learning gaussians and beyond. CoRR, abs/2111.11320, 2021. 1, 7

[App17]

Apple Diï¬€erential Privacy Team. Learning with privacy at scale. Apple Machine
Learning Journal, 2017. 2

[BDH+20] A. Bakshi, I. Diakonikolas, S. B. Hopkins, D. Kane, S. Karmalkar, and P. K. Kothari.
Outlier-robust clustering of gaussians and other non-spherical mixtures. In 61st IEEE
Annual Symposium on Foundations of Computer Science, FOCS 2020, pages 149â€“159. IEEE,
2020. 6

[BDKU20] Sourav Biswas, Yihe Dong, Gautam Kamath, and Jonathan R. Ullman. Coinpress:
Practical private mean and covariance estimation. In Hugo Larochelle, Marcâ€™Aurelio
Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances
in Neural Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 2

[BEM+17] Andrea Bittau, Ãšlfar Erlingsson, Petros Maniatis, Ilya Mironov, Ananth Raghunathan,
David Lie, Mitch Rudominer, Ushasree Kode, Julien TinnÃ©s, and Bernhard Seefeld.

46

 
Prochlo: Strong privacy for analytics in the crowd. In Proceedings of the 26th Symposium
on Operating Systems Principles, Shanghai, China, October 28-31, 2017, pages 441â€“459.
ACM, 2017. 2

[BGS+21] Gavin Brown, Marco Gaboardi, Adam D. Smith, Jonathan R. Ullman, and Lydia
Zakynthinou. Covariance-aware private mean estimation without private covariance
estimation. CoRR, abs/2106.13329, 2021. 2

[BK20a]

[BK20b]

[BK20c]

[BKS15]

A. Bakshi and P. Kothari. Outlier-robust clustering of non-spherical mixtures. CoRR,
abs/2005.02970, 2020. 2, 14, 43

Ainesh Bakshi and Pravesh Kothari. Outlier-robust clustering of non-spherical mix-
tures. 2020. 5, 6, 17

Ainesh Bakshi and Pravesh Kothari. Outlier-robust clustering of non-spherical mix-
tures. CoRR, abs/2005.02970, 2020. 6

B. Barak, J. A. Kelner, and D. Steurer. Dictionary learning and tensor decomposition
via the sum-of-squares method [extended abstract]. In STOCâ€™15â€”Proceedings of the
2015 ACM Symposium on Theory of Computing, pages 143â€“151. ACM, New York, 2015.
16

[BKSW19] Mark Bun, Gautam Kamath, Thomas Steinke, and Zhiwei Steven Wu. Private hypoth-
esis selection. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
dâ€™AlchÃ©-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information
Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019,
NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 156â€“167, 2019. 2

[BP20]

[BS19]

A. Bakshi and A. Prasad. Robust linear regression: Optimal rates in polynomial time.
arXiv preprint arXiv:2007.01394, 2020. 14

Mark Bun and Thomas Steinke. Average-case averages: Private algorithms for smooth
In Hanna M. Wallach, Hugo Larochelle, Alina
sensitivity and mean estimation.
Beygelzimer, Florence dâ€™AlchÃ©-Buc, Emily B. Fox, and Roman Garnett, editors, Ad-
vances in Neural Information Processing Systems 32: Annual Conference on Neural Informa-
tion Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,
pages 181â€“191, 2019. 2

[BUV14] Mark Bun, Jonathan Ullman, and Salil P. Vadhan. Fingerprinting codes and the price
of approximate diï¬€erential privacy. In STOC, pages 1â€“10. ACM, 2014. 7

[CDGW19] Yu Cheng, Ilias Diakonikolas, Rong Ge, and David P. Woodruï¬€. Faster algorithms for
high-dimensional robust covariance estimation. In Alina Beygelzimer and Daniel Hsu,
editors, Conference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA,
volume 99 of Proceedings of Machine Learning Research, pages 727â€“757. PMLR, 2019. 6

47

[CKM+20] ClÃ©ment L. Canonne, Gautam Kamath, Audra McMillan, Jonathan R. Ullman, and
Lydia Zakynthinou. Private identity testing for high-dimensional distributions. In
Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. 7

[CWZ19]

T. Tony Cai, Yichen Wang, and Linjun Zhang. The cost of privacy: Optimal rates of
convergence for parameter estimation with diï¬€erential privacy. CoRR, abs/1902.04495,
2019. 2

[DFM+20] Wenxin Du, Canyon Foot, Monica Moniot, Andrew Bray, and Adam Groce. Diï¬€eren-

tially private conï¬dence intervals. CoRR, abs/2001.02285, 2020. 2

[DHKK20]

Ilias Diakonikolas, Samuel B. Hopkins, Daniel Kane, and Sushrut Karmalkar. Robustly
learning any clusterable mixture of gaussians. CoRR, abs/2005.06417, 2020. 2, 6, 14

[DHL19]

Yihe Dong, Samuel B. Hopkins, and Jerry Li. Quantum entropy scoring for fast ro-
bust mean estimation and improved outlier detection. In Hanna M. Wallach, Hugo
Larochelle, Alina Beygelzimer, Florence dâ€™AlchÃ©-Buc, Emily B. Fox, and Roman Gar-
nett, editors, Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Van-
couver, BC, Canada, pages 6065â€“6075, 2019. 6

[DK19]

Ilias Diakonikolas and Daniel M. Kane. Recent advances in algorithmic high-
dimensional robust statistics. CoRR, abs/1911.05911, 2019. 6

[DKK+16]

I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Robust
estimators in high dimensions without the computational intractability. In Proc. 57th
IEEE Symposium on Foundations of Computer Science (FOCS), pages 655â€“664, 2016. 2, 6,
42, 45

[DKK+17a] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Al-
istair Stewart. Being robust (in high dimensions) can be practical. In ICML, volume 70
of Proceedings of Machine Learning Research, pages 999â€“1008. PMLR, 2017. 6

[DKK+17b] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and
Alistair Stewart. Robustly learning a gaussian: Getting optimal error, eï¬ƒciently. CoRR,
abs/1704.03866, 2017. 6

[DKM+06] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni
Naor. Our data, ourselves: Privacy via distributed noise generation. In Advances in
Cryptology - EUROCRYPT 2006, 25th Annual International Conference on the Theory and
Applications of Cryptographic Techniques, St. Petersburg, Russia, May 28 - June 1, 2006,
Proceedings, volume 4004 of Lecture Notes in Computer Science, pages 486â€“503. Springer,
2006. 36

48

[DKY17]

Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin. Collecting telemetry data
privately. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach,
Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural
Information Processing Systems 30: Annual Conference on Neural Information Processing
Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 3571â€“3580, 2017. 2

[DL09]

Cynthia Dwork and Jing Lei. Diï¬€erential privacy and robust statistics. In Michael
Mitzenmacher, editor, Proceedings of the 41st Annual ACM Symposium on Theory of
Computing, STOC 2009, Bethesda, MD, USA, May 31 - June 2, 2009, pages 371â€“380. ACM,
2009. 21, 54

[DLCC07] Lieven De Lathauwer, JosÃ©phine Castaing, and Jean-FranÃ§ois Cardoso. Fourth-order
cumulant-based blind identiï¬cation of underdetermined mixtures. IEEE Trans. Signal
Process., 55(6, part 2):2965â€“2973, 2007. 5

[DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating
noise to sensitivity in private data analysis.
In Shai Halevi and Tal Rabin, editors,
Theory of Cryptography, Third Theory of Cryptography Conference, TCC 2006, New York, NY,
USA, March 4-7, 2006, Proceedings, volume 3876 of Lecture Notes in Computer Science,
pages 265â€“284. Springer, 2006. 2, 20

[DN03]

[DR14]

[DSS+15]

Irit Dinur and Kobbi Nissim. Revealing information while preserving privacy.
PODS, pages 202â€“210. ACM, 2003. 7

In

Cynthia Dwork and Aaron Roth. The algorithmic foundations of diï¬€erential privacy.
Found. Trends Theor. Comput. Sci., 9(3-4):211â€“407, 2014. 37, 39

Cynthia Dwork, Adam D. Smith, Thomas Steinke, Jonathan R. Ullman, and Salil P.
Vadhan. Robust traceability from trace amounts. In Venkatesan Guruswami, editor,
IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS 2015, Berkeley,
CA, USA, 17-20 October, 2015, pages 650â€“669. IEEE Computer Society, 2015. 7

[DSSU17] Cynthia Dwork, Adam Smith, Thomas Steinke, and Jonathan Ullman. Exposed! a
survey of attacks on private data. Annual Review of Statistics and Its Application, 4(1):61â€“
84, 2017. 7

[EPK14]

[FKP19]

Ãšlfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. RAPPOR: Randomized
aggregatable privacy-preserving ordinal response. In CCS, pages 1054â€“1067, 2014. 2

Noah Fleming, Pravesh Kothari, and Toniann Pitassi. Semialgebraic proofs and eï¬ƒ-
cient algorithm design. Foundations and TrendsÂ® in Theoretical Computer Science, 14(1-
2):1â€“221, 2019. 14, 15

[Gre16]

Andy Greenberg. Appleâ€™s â€œdiï¬€erential privacyâ€ is about collecting your data â€“ but
not your data. Wired, June, 13, 2016. 2

49

[HK13]

Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical Gaussians: mo-
ment methods and spectral decompositions. In ITCSâ€™13â€”Proceedings of the 2013 ACM
Conference on Innovations in Theoretical Computer Science, pages 11â€“19. ACM, New York,
2013. 5

[HKM21]

Samuel B. Hopkins, Gautam Kamath, and Mahbod Majid. Eï¬ƒcient mean estimation
with pure diï¬€erential privacy via a sum-of-squares exponential mechanism. CoRR,
abs/2111.12981, 2021. 7

[HL18]

[HLZ20]

S. B. Hopkins and J. Li. Mixture models, robustness, and sum of squares proofs. In
Proc. 50th Annual ACM Symposium on Theory of Computing (STOC), pages 1021â€“1034,
2018. 2, 6, 14

Samuel B. Hopkins, Jerry Li, and Fred Zhang. Robust and heavy-tailed mean estima-
tion made simple, via regret minimization. In Hugo Larochelle, Marcâ€™Aurelio Ranzato,
Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural
Information Processing Systems 33: Annual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 6

[Hop20]

Samuel B. Hopkins. Mean estimation with sub-Gaussian rates in polynomial time.
The Annals of Statistics, 48(2):1193 â€“ 1213, 2020. 6

[JH16]

[JLT20]

CÃ©dric Josz and Didier Henrion. Strong duality in Lasserreâ€™s hierarchy for polynomial
optimization. Optim. Lett., 10(1):3â€“10, 2016. 15

Arun Jambulapati, Jerry Li, and Kevin Tian. Robust sub-gaussian principal component
analysis and width-independent schatten packing. In Hugo Larochelle, Marcâ€™Aurelio
Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances
in Neural Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 6

[KKM18] A. Klivans, P. Kothari, and R. Meka. Eï¬ƒcient algorithms for outlier-robust regression.

In Proc. 31st Annual Conference on Learning Theory (COLT), pages 1420â€“1430, 2018. 14

[KLSU19] Gautam Kamath, Jerry Li, Vikrant Singhal, and Jonathan R. Ullman. Privately learning
high-dimensional distributions. In Alina Beygelzimer and Daniel Hsu, editors, Con-
ference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA, volume 99
of Proceedings of Machine Learning Research, pages 1853â€“1902. PMLR, 2019. 2, 10

[KMS+21] Gautam Kamath, Argyris Mouzakis, Vikrant Singhal, Thomas Steinke, and Jonathan R.
Ullman. A private and computationally-eï¬ƒcient estimator for unbounded gaussians.
CoRR, abs/2111.04609, 2021. 7

[KMZ21]

Pravesh K. Kothari, Peter Manohar, and Brian Hu Zhang. Polynomial-time sum-of-
squares can robustly estimate mean and covariance of gaussians optimally, 2021. 42,
45

50

[KOTZ14] Manuel Kauers, Ryan Oâ€™Donnell, Li-Yang Tan, and Yuan Zhou. Hypercontractive
In SODA, pages 1644â€“1658. SIAM,

inequalities via sos, and the frankl-rÃ¶dl graph.
2014. 42

[KS17a]

[KS17b]

[KS17c]

P. K. Kothari and J. Steinhardt. Better agnostic clustering via relaxed tensor norms.
CoRR, abs/1711.07465, 2017. 2, 4

P. K. Kothari and D. Steurer. Outlier-robust moment-estimation via sum-of-squares.
CoRR, abs/1711.11581, 2017. 2, 5, 8, 11, 14, 19, 23, 24, 25, 26

Pravesh K. Kothari and Jacob Steinhardt. Better agnostic clustering via relaxed tensor
norms. CoRR, abs/1711.07465, 2017. 14

[KSKO20] Weihao Kong, Raghav Somani, Sham M. Kakade, and Sewoong Oh. Robust meta-
In Hugo Larochelle,
learning for mixed linear regression with small batches.
Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, edi-
tors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 6

[KSS18]

P. K. Kothari, J. Steinhardt, and D. Steurer. Robust moment estimation and improved
In Proc. 50th Annual ACM Symposium on Theory of
clustering via sum of squares.
Computing (STOC), pages 1035â€“1046, 2018. 6

[KSSU19] Gautam Kamath, Or Sheï¬€et, Vikrant Singhal, and Jonathan R. Ullman. Diï¬€erentially
private algorithms for learning mixtures of separated gaussians. In Hanna M. Wal-
lach, Hugo Larochelle, Alina Beygelzimer, Florence dâ€™AlchÃ©-Buc, Emily B. Fox, and
Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14,
2019, Vancouver, BC, Canada, pages 168â€“180, 2019. 7

[KSU20]

[KV18]

[Las01]

Gautam Kamath, Vikrant Singhal, and Jonathan R. Ullman. Private mean estimation
of heavy-tailed distributions. In Jacob D. Abernethy and Shivani Agarwal, editors,
Conference on Learning Theory, COLT 2020, 9-12 July 2020, Virtual Event [Graz, Austria],
volume 125 of Proceedings of Machine Learning Research, pages 2204â€“2235. PMLR, 2020.
2

Vishesh Karwa and Salil P. Vadhan. Finite sample diï¬€erentially private conï¬dence
In Anna R. Karlin, editor, 9th Innovations in Theoretical Computer Science
intervals.
Conference, ITCS 2018, January 11-14, 2018, Cambridge, MA, USA, volume 94 of LIPIcs,
pages 44:1â€“44:9. Schloss Dagstuhl - Leibniz-Zentrum fÃ¼r Informatik, 2018. 2

Jean B. Lasserre. New positive semideï¬nite relaxations for nonconvex quadratic pro-
grams. In Advances in convex analysis and global optimization (Pythagorion, 2000), vol-
ume 54 of Nonconvex Optim. Appl., pages 319â€“331. Kluwer Acad. Publ., Dordrecht,
2001. 17

51

[LKKO21] Xiyang Liu, Weihao Kong, Sham M. Kakade, and Sewoong Oh. Robust and diï¬€eren-

tially private mean estimation. CoRR, abs/2102.09159, 2021. 2, 8

[LKO21]

Xiyang Liu, Weihao Kong, and Sewoong Oh. Diï¬€erential privacy and robust statistics
in high dimensions. CoRR, abs/2111.06578, 2021. 7

[LRV16]

[LY20]

[MT07]

[Nes00]

[NRS07]

[Par00]

[SCV18]

K. A. Lai, A. B. Rao, and S. Vempala. Agnostic estimation of mean and covariance. In
Proc. 57th IEEE Symposium on Foundations of Computer Science (FOCS), pages 665â€“674,
2016. 2, 6

Jerry Li and Guanghao Ye. Robust gaussian covariance estimation in nearly-matrix
multiplication time. In Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-
Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual, 2020. 6

Frank McSherry and Kunal Talwar. Mechanism design via diï¬€erential privacy. In 48th
Annual IEEE Symposium on Foundations of Computer Science (FOCS 2007), October 20-23,
2007, Providence, RI, USA, Proceedings, pages 94â€“103. IEEE Computer Society, 2007. 22,
55, 56

In High
Yurii Nesterov. Squared functional systems and optimization problems.
performance optimization, volume 33 of Appl. Optim., pages 405â€“440. Kluwer Acad.
Publ., Dordrecht, 2000. 17

Kobbi Nissim, Sofya Raskhodnikova, and Adam D. Smith. Smooth sensitivity and
sampling in private data analysis. In David S. Johnson and Uriel Feige, editors, Pro-
ceedings of the 39th Annual ACM Symposium on Theory of Computing, San Diego, California,
USA, June 11-13, 2007, pages 75â€“84. ACM, 2007. 7

Pablo A Parrilo. Structured semideï¬nite programs and semialgebraic geometry methods in
robustness and optimization. PhD thesis, California Institute of Technology, 2000. 17

J. Steinhardt, M. Charikar, and G. Valiant. Resilience: A criterion for learning in the
presence of arbitrary outliers. In Proc. 9th Innovations in Theoretical Computer Science
Conference (ITCS), pages 45:1â€“45:21, 2018. 2, 6

[Sho87]

N. Z. Shor. Quadratic optimization problems. Izv. Akad. Nauk SSSR Tekhn. Kibernet.,
(1):128â€“139, 222, 1987. 17

[SSSS17]

Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership
inference attacks against machine learning models. In 2017 IEEE Symposium on Security
and Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017, pages 3â€“18. IEEE Computer
Society, 2017. 7

52

[SU15]

[Tao12]

[Vad17]

Thomas Steinke and Jonathan R. Ullman.
Interactive ï¬ngerprinting codes and the
hardness of preventing false discovery. In Peter GrÃ¼nwald, Elad Hazan, and Satyen
Kale, editors, Proceedings of The 28th Conference on Learning Theory, COLT 2015, Paris,
France, July 3-6, 2015, volume 40 of JMLR Workshop and Conference Proceedings, pages
1588â€“1628. JMLR.org, 2015. 7

T. Tao. Topics in Random Matrix Theory. Graduate studies in mathematics. American
Mathematical Society, 2012. 41

Salil P. Vadhan. The complexity of diï¬€erential privacy. In Yehuda Lindell, editor, Tutori-
als on the Foundations of Cryptography, pages 347â€“450. Springer International Publishing,
2017. 22

[WDFS17] Miaoyan Wang, Khanh Dao Duc, Jonathan Fischer, and Yun S. Song. Operator norm
inequalities between tensor unfoldings on the partition lattice. Linear algebra and its
applications, 520:44â€“66, 2017. 38

[WXDX20] Di Wang, Hanshen Xiao, Srinivas Devadas, and Jinhui Xu. On diï¬€erentially private
stochastic convex optimization with heavy-tailed data. In Proceedings of the 37th Interna-
tional Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume
119 of Proceedings of Machine Learning Research, pages 10081â€“10091. PMLR, 2020. 2

[ZJS19]

Banghua Zhu, Jiantao Jiao, and Jacob Steinhardt. Generalized resilience and robust
statistics. CoRR, abs/1909.08755, 2019. 6

[ZKKW20] Huanyu Zhang, Gautam Kamath, Janardhan Kulkarni, and Zhiwei Steven Wu. Pri-
vately learning markov random ï¬elds. In Proceedings of the 37th International Conference
on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceed-
ings of Machine Learning Research, pages 11129â€“11140. PMLR, 2020. 7

A Missing Proofs from Section 3.6

A.1 Proof of Lemma 3.27

Proof of Lemma 3.27. Consider any neighboring datasets ğ‘Œ, ğ‘Œâ€² and let
denote the truncated
Laplace mechanism (with parameter as speciï¬ed). Let ğ‘, ğ‘ denote the probability density functions
of

. Observe that ğ‘

. Thus, we have

for all ğ‘¥ < min

6 ğ‘’ ğœ€

â„³

, ğ‘“

ğ‘Œ

ğ‘¥

ğ‘¥

ğ‘

ğ‘“

,

(

)

Â·

(

)

ğ‘Œ
(

)

{

ğ‘Œâ€²)}
(

â„³(

)

â„³(

ğ‘Œâ€²)

ğ‘

ğ‘¥

(

â„[

) âˆ’

ğ‘’ ğœ€ğ‘

ğ‘¥

(

)]+

ğ‘‘ğ‘¥

âˆ«ğ‘¥

âˆˆ

âˆ«ğ‘¥>min

{

, ğ‘“

ğ‘“

ğ‘Œ
(

)

ğ‘Œâ€²)}
(

ğ‘

ğ‘¥

(

[

) âˆ’

ğ‘’ ğœ€ğ‘

ğ‘¥

(

)]+

ğ‘‘ğ‘¥

âˆ«ğ‘¥>min

{

, ğ‘“

ğ‘“

ğ‘Œ
(

)

ğ‘Œâ€²)}
(

ğ‘‘ğ‘¥

ğ‘

ğ‘¥

(

)

ğ‘, ğ‘

ğ·ğ‘’ ğœ€

(

)

=

=

6

53

Since sensitivity of ğ‘“ is at most Î”
(

)

Lemma 3.28
(

)

6

âˆ«ğ‘¥> ğ‘“

6 ğ›¿,

ğ‘‘ğ‘¥

ğ‘

ğ‘¥

(

)

ğ‘Œ
(

Î”

)âˆ’

which means that the truncated Laplace mechanism is indeed

ğœ€, ğ›¿

-DP.
)

(

(cid:3)

A.2 Proof of Lemma 3.30

The proof of the composition lemma follows from that of the standard adaptive composition of
ğ‘¥, 0
approximate DP proof [DL09, Theorem 16]. Below we use the notation
and ğ‘¥

ğ‘¦ to denote min

to denote max

ğ‘¥, ğ‘¦

]+

ğ‘¥

}

{

[

.

âˆ§

{

}

Proof of Lemma 3.30. It suï¬ƒces to prove the theorem for ğ‘˜ = 2 as we may then apply induction to
arrive at the statement for any positive integer ğ‘˜. To prove the case ğ‘˜ = 2, consider any ğ‘†
ğ‘‚2 âˆª {âŠ¥}
and any pair of neighboring datasets ğ‘Œ, ğ‘Œâ€².

âŠ†

For any ğ‘†1 âŠ† ğ’ª1 âˆª {âŠ¥}
(ğ’ª1)

Note that we have ğœ‡

:=
â„™
ğ‘†1)
, we deï¬ne the measure ğœ‡
[
6 ğ›¿1 due to our assumption that
â„³1 is

ğ‘Œ
[â„³1(
) âˆˆ
ğœ€1, ğ›¿1)
(

(

ğ‘†1] âˆ’
-DP.

Now consider four cases:

ğ‘’ ğœ€1 â„™

[â„³1(

ğ‘Œâ€²) âˆˆ

ğ‘†1]]+

.

â€¢ Both ğ‘Œ, ğ‘Œâ€² satisfy Î¨1.

In this case, we may appeal to

implies

ğœ€2, ğ›¿2)

-DP under Î¨1 of

(

â„³2 which

â„™

[â„³2(

ğ‘œ1, ğ‘Œ

ğ‘†

]

) âˆˆ

For ease of notation, let ğ‘ğ‘Œ :
probability density function of
Then, observe that

ğ’ª1 â†’
ğ‘Œ
â„³1(

)

6

ğ‘’ ğœ€2 â„™

ğ‘œ1, ğ‘Œâ€²

ğ‘†

[â„³2(

1
) +

ğ›¿2.

) âˆˆ

] âˆ§

(
â„+ denote the measure obtained by restricting the
ğ‘œ1)
to
).

ğ’ª1 (note that

ğ‘Œ
[â„³1(

ğ‘‘ğ‘œ1 = 1

âŠ¥]

ğ‘ğ‘Œ

â„™

=

âˆ’

(

)

(A.1)

1
ğ’ª
âˆ«

â„™

ğ‘Œ

[â„³(

ğ‘†

]

) âˆˆ

= 1

[âŠ¥âˆˆ

ğ‘†

]

â„™

ğ‘Œ
[â„³1(

)

=

âŠ¥] +

â„™

[â„³2(

ğ‘œ1, ğ‘Œ

ğ‘ğ‘Œ

ğ‘†

]

ğ‘œ1)

(

ğ‘‘ğ‘œ1

) âˆˆ

âˆ«ğ’ª
1

ğ‘’ ğœ€2 â„™

[â„³2(

ğ‘œ1, ğ‘Œâ€²

ğ‘†

1
) +

ğ›¿2)

ğ‘ğ‘Œ

ğ‘œ1)

(

ğ‘‘ğ‘œ1

] âˆ§

) âˆˆ

(A.1)
6 1

[âŠ¥âˆˆ

6 1

[âŠ¥âˆˆ

ğ‘†

]

+

1(
âˆ«ğ’ª
ğ‘†

[âŠ¥âˆˆ

](

6 1

+

1(
âˆ«ğ’ª
ğ‘†

[âŠ¥âˆˆ

](

6 1

+

1(
âˆ«ğ’ª
ğ‘†

[âŠ¥âˆˆ

](

6 1

1 ((

âˆ«ğ’ª

ğ‘†

â„™

]

ğ‘Œ
[â„³1(
ğ‘Œ
[â„³1(
ğ‘’ ğœ€2 â„™

)

â„™

[â„³2(

=

)

âŠ¥] +

ğ›¿2

=

âŠ¥] +
ğ‘œ1, ğ‘Œâ€²

ğ‘†

1
)

] âˆ§

ğ‘ğ‘Œ

) âˆˆ

ğ‘‘ğ‘œ1

ğ‘’ ğœ€1 â„™

[â„³1(

ğ‘’ ğœ€2 â„™

[â„³2(

=

ğ‘Œâ€²

)
ğ‘œ1, ğ‘Œâ€²

âŠ¥] +
ğ‘†

) âˆˆ

ğœ‡

1
)(

] âˆ§

({âŠ¥})) +

(

ğ‘œ1)
ğ›¿2
ğ‘’ ğœ€1 ğ‘ğ‘Œâ€²(

ğ‘’ ğœ€1 â„™

ğ‘’ ğœ€2 â„™

ğ‘’ ğœ€1

+

[â„³1(

[â„³2(
ğœ€2 â„™

ğœ‡

ğ‘Œâ€²

=

)
ğ‘œ1, ğ‘Œâ€²

âŠ¥]) +
ğ‘†

) âˆˆ

] âˆ§

[â„³1(

ğ‘Œâ€²

=

)

âŠ¥]) +

(ğ’ª1 âˆª {âŠ¥}) +
ğ‘’ ğœ€1 ğ‘ğ‘Œâ€²(
ğ‘œ1))
1
)(
ğ›¿2
ğ›¿1 +

ğ‘‘ğ‘œ1

ğ‘‘ğœ‡

ğ‘œ1))

(

ğ‘œ1)

ğ‘‘ğ‘œ1 +
ğ›¿2

54

ğ‘’ ğœ€1

+

ğœ€2 â„™

[â„³2(

ğ‘œ1, ğ‘Œâ€²

ğ‘†

ğ‘ğ‘Œâ€²(

ğ‘œ1)

]

ğ‘‘ğ‘œ1

) âˆˆ

+
6 ğ›¿1 +

âˆ«ğ’ª
1
ğ›¿2 +
â€¢ ğ‘Œ satisï¬es Î¨1 but ğ‘Œâ€² does not. In this case, we have â„™

[â„³(

ğœ€2 â„™

ğ‘’ ğœ€1

) âˆˆ

ğ‘Œâ€²

ğ‘†

+

]

.

=

= 1, which implies that

â„™

ğ‘Œ

[â„³(

ğ‘†

] âˆ’

) âˆˆ

ğ‘’ ğœ€1

+

ğœ€2 â„™

ğ‘Œâ€²

[â„³(

ğ‘†

]

) âˆˆ

6 â„™

ğ‘Œ

[â„³(

â‰ 

)

âŠ¥]

= â„™

[â„³(

ğ‘’ ğœ€1 â„™

ğ‘Œâ€²

[â„³(

â‰ 

)

âŠ¥]

6 ğ›¿1,

where the last inequality follows from the fact that

â„³1 is

â€¢ ğ‘Œâ€² satisï¬es Î¨1 but ğ‘Œ does not. In this case, we have â„™

â„™

ğ‘Œ

[â„³(

ğ‘†

] âˆ’

) âˆˆ

ğ‘’ ğœ€1

+

ğœ€2 â„™

ğ‘Œâ€²

[â„³(

ğ‘†

]

) âˆˆ

= 1, which implies that

[â„³(
=

ğ‘Œ

ğ‘Œ

)

)

=

âŠ¥] âˆ’

âŠ¥] âˆ’

âŠ¥]
ğ‘’ ğœ€1
+
ğ‘’ ğœ€1 â„™

ğœ€2 â„™

ğ‘Œâ€²
)
=

[â„³(
ğ‘Œâ€²

[â„³(

)

âŠ¥]]+

=

âŠ¥]]+

6

â„™

6
â„™
[
6 ğ›¿1,

[

[â„³(

[â„³(

[â„³(

ğ‘Œâ€²)
ğ‘Œ

âŠ¥]
â‰ 

)
ğœ€1, ğ›¿1)
(
=
ğ‘Œ

)

âŠ¥] âˆ’

-DP.

where the last inequality once again follows from the fact that

â€¢ Neither ğ‘Œ nor ğ‘Œâ€² satisfy Î¨1. In this case, both
ğ‘†

we have â„™

= â„™

ğ‘Œ

ğ‘†

.

ğ‘Œ

â„³(

)

and

ğ‘Œâ€²)

â„³(

]
Thus, in all cases, we have â„™

[â„³(

) âˆˆ

ğ‘Œâ€²) âˆˆ
ğ‘†

]
= ğ‘’ ğœ€1

[â„³(
ğ‘Œ

[â„³(

) âˆˆ

]

ğœ€2 â„™

+

ğ‘Œâ€²) âˆˆ

[â„³(

ğ‘†

ğ›¿1 +

] +

ğ›¿2 as desired.

-DP.

ğœ€1, ğ›¿1)
â„³1 is
always output

(

. Therefore,

âŠ¥

A.3 Proof of Lemma 3.33

Proof of Lemma 3.33. Then, note that

ğ·ğ‘’ ğœ€

ğ‘, ğ‘Ÿ

(

)

=

=

6

=

ğ‘

ğ‘¥

(

) âˆ’

â„ğ‘‘ [

ğ‘’ ğœ€ğ‘Ÿ

ğ‘¥

(

)]+

ğ‘‘ğ‘¥

ğ‘

ğ‘¥

(

) âˆ’

â„ğ‘‘ [(

ğ‘’ ğœ€

2ğ‘
/

ğ‘¥

(

)) + (

ğ‘’ ğœ€

2ğ‘
/

ğ‘¥

(

) âˆ’

ğ‘’ ğœ€ğ‘Ÿ

ğ‘¥

(

))]+

ğ‘‘ğ‘¥

âˆ«ğ‘¥

âˆˆ

âˆ«ğ‘¥

âˆˆ

ğ‘

ğ‘¥

(

) âˆ’

â„ğ‘‘ [(

ğ‘’ ğœ€

2ğ‘
/

ğ‘¥

(

))]+

ğ‘‘ğ‘¥

+

âˆ«ğ‘¥

âˆˆ

ğ‘

ğ‘¥

(

â„ğ‘‘ [(

ğ‘, ğ‘

2

(

/

) +

ğ‘’ ğœ€

2ğ‘
/

) âˆ’
ğ‘’ ğœ€

2
/

Â·

ğ‘¥

(
ğ·ğ‘’ ğœ€

ğ‘‘ğ‘¥

+

))]+

ğ‘, ğ‘Ÿ

,

)

2

(

/

âˆ«ğ‘¥
âˆˆ
= ğ·ğ‘’ ğœ€

âˆ«ğ‘¥
ğ‘’ ğœ€

âˆˆ
2
/

ğ‘’ ğœ€

2ğ‘
/

ğ‘¥

(

) âˆ’

ğ‘’ ğœ€ğ‘Ÿ

ğ‘¥

(

))]+

ğ‘‘ğ‘¥

â„ğ‘‘ [

ğ‘

ğ‘¥

(

) âˆ’

â„ğ‘‘ [

ğ‘’ ğœ€

2ğ‘Ÿ
/

ğ‘¥

(

))]+

ğ‘‘ğ‘¥

âˆ«ğ‘¥

âˆˆ

as desired.

A.4 Proof of Theorem 3.34

As stated earlier, the proof of Theorem 3.34 follows from applying the exponential mecha-
nism [MT07] and then use the truncated Laplace mechanism (Lemma 3.27) to check that the
score indeed exceeds ğœ….

Proof of Theorem 3.34. Selection works as follows:

55

(cid:3)

(cid:3)

1. First, run the

ğœ€
(
probability proportional to exp

-DP exponential mechanism [MT07], i.e.
2
)

ğ‘, ğ‘Œ

/

. Let ğ‘1 be the output of this procedure.

selecting each ğ‘

ğ¶ with

âˆˆ

ğœ€
4Î” Â·

score
(
ğ›¿

2 ln

2. Sample the noise ğ‘

)
(cid:1)
, 2Î”
ğœ€
.
âŠ¥
We will now prove each of the claimed properties:

)
âˆ¼
score > ğœ…, then output ğ‘1. Otherwise, output
(cid:17)

tLap

1
/
(
ğœ€

(cid:0)
Î”

+

âˆ’

1

(cid:16)

(cid:16)

(cid:157)

and compute

score = score
(

ğ‘1, ğ‘Œ

) +

ğ‘. If

(cid:17)

(cid:157)

1. The ï¬rst step satisï¬es

nism [MT07]. The second step is
composition theorem implies that Selection is

ğœ€

/

ğœ€

(

/

-DP via the standard privacy guarantee of the exponential mecha-
2
)
-DP due to Lemma 3.27. Thereby, applying the basic
)

2, ğ›¿

(

ğœ€, ğ›¿

-DP.
)

(

2. Since ğ‘ 6 0, we are guarantee that if the algorithm outputs ğ‘âˆ— âˆˆ ğ’

ğœ… as desired.

, we must have score
(

ğ‘, ğ‘Œ

)

>

3. For any ğ‘

, the standard utility analysis of the exponential mechanism [MT07] implies

âˆˆ ğ’
that, with probability 1
the tail bound of Laplace noise (Lemma 3.28) implies that with probability 1
(cid:17)(cid:17)
have ğ‘ >

0.5ğ›½, we have score
(

> score
(

Î”
ğœ€ ln

ğ‘1, ğ‘Œ

ğ‘, ğ‘Œ

) âˆ’

ğ¶
|ğ›½

ğ‘‚

ğ‘‚

ğ‘‚

>

Î”

âˆ’

1

ln

(cid:16)

(cid:16)

)

(

|

1
ğ›¿
)ğœ€
/

1
+
/
(
(cid:17) (cid:17)
, the probability that the algorithm outputs

âˆ’

âˆ’

ğ›½

(cid:17)

(cid:16)

(cid:16)

)

(cid:0)

(cid:1)

Î”
ğœ€ ln

1
ğ›¿ğ›½

Î”
ğœ€ ln

âˆ’
. Therefore, if score
(

. Moreover,
0.5ğ›½ we
>

ğ‘, ğ‘Œ

)

âˆ’
ğ¶
|ğ›¿ğ›½
|

(cid:16)

Î”
ğœ€ ln

ğ‘‚

ğœ…

+

âŠ¥

is at most ğ›½, as desired.

(cid:3)

(cid:16)

(cid:16)

(cid:17) (cid:17)

56

