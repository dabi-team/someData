1
2
0
2

c
e
D
7

]
L
M

.
t
a
t
s
[

1
v
8
4
5
3
0
.
2
1
1
2
:
v
i
X
r
a

Private Robust Estimation by Stabilizing Convex Relaxations

Pravesh K. Kothari
praveshk@cs.cmu.edu

∗

Pasin Manurangsi
pasin@google.com

†

Ameya Velingker
ameyav@google.com

†

December 8, 2021

Abstract

(

)

𝜀, 𝛿

We give the ﬁrst polynomial time and sample

-diﬀerentially private (DP) algorithm to
estimate the mean, covariance and higher moments in the presence of a constant fraction of
adversarial outliers. Our algorithm succeeds for families of distributions that satisfy two well-
studied properties in prior works on robust estimation: certiﬁable subgaussianity of directional
moments and certiﬁable hypercontractivity of degree 2 polynomials. Our recovery guarantees
hold in the “right aﬃne-invariant norms”: Mahalanobis distance for mean, multiplicative
spectral and relative Frobenius distance guarantees for covariance and injective norms for
higher moments. Prior works obtained private robust algorithms for mean estimation of
subgaussian distributions with bounded covariance. For covariance estimation, ours is the ﬁrst
eﬃcient algorithm (even in the absence of outliers) that succeeds without any condition-number
assumptions.

Our algorithms arise from a new framework that provides a general blueprint for modifying
convex relaxations for robust estimation to satisfy strong worst-case stability guarantees in the
appropriate parameter norms whenever the algorithms produce witnesses of correctness in their
run. We verify such guarantees for a modiﬁcation of standard sum-of-squares (SoS) semideﬁ-
nite programming relaxations for robust estimation. Our privacy guarantees are obtained by
combining stability guarantees with a new “estimate dependent” noise injection mechanism in
which noise scales with the eigenvalues of the estimated covariance. We believe this framework
will be useful more generally in obtaining DP counterparts of robust estimators.

Independently of our work, Ashtiani and Liaw [AL21] also obtained a polynomial time and

sample private robust estimation algorithm for Gaussian distributions.

∗

†

Carnegie Mellon University
Google Research

1

 
 
 
 
 
 
1 Introduction

In this work, we consider the problem of eﬃciently estimating the mean, covariance and, more
generally, the higher moments of an unknown high-dimensional probability distribution on ℝ𝑑,
ℝ𝑑, under two design constraints: outlier robustness and privacy.
given a sample 𝑦1, 𝑦2, . . . , 𝑦𝑛
The ﬁrst demands that we build estimators for such basic parameters of probability distributions
that tolerate a ﬁxed (dimension-independent) constant fraction of adversarial outliers in the input
data. The second demands that our estimators preserve the privacy of individual points 𝑦𝑖s (that
we model as being contributed by diﬀerent individuals) participating in our input data.

∈

Sans privacy constraints, the problem of robustly estimating the basic parameters of an
unknown distribution has been the focus of intense research in algorithmic robust statistics
starting with the pioneering works of [DKK+16, LRV16] from 2016.
In addition to new
(and often, information-theoretically optimal) algorithms for several basic robust estimation
tasks [KS17b, KS17a, HL18, BK20a, DHKK20], this line of work has led to a deeper understanding
of the properties of the underlying distribution (algorithmic certiﬁcates of analytic properties such
as subgaussianity, hypercontractivity and anti-concentration, resilience [SCV18]) that make robust
estimation possible along with general frameworks such as outlier ﬁltering and the sum-of-squares
(SoS) method for attacking algorithmic problems in robust statistics.

Sans outlier robustness constraints, the task of private estimation of the mean and covariance
of probability distributions has also seen considerable progress in the recent years. Diﬀerential
privacy [DMNS06] has emerged as a widely-used standard for providing strong individual privacy
guarantees. Under diﬀerential privacy, a single sample is not allowed to have too signiﬁcant of an
impact on the output distribution of an algorithm that operates on a dataset. Diﬀerential privacy
has now been deployed in a number of production systems, including those at Google [EPK14,
BEM+17], Microsoft [DKY17], Apple [Gre16, App17], and the US Census Bureau [Abo18]. While
initial approaches to estimating the mean and covariance under diﬀerential privacy required a
priori bounds on the support of the samples, a more recent work [KV18] managed to obtain the
ﬁrst private mean estimation algorithm for samples with unbounded support. Subsequent works
have built on this progress to obtain diﬀerentially private algorithms for mean estimation and
covariance estimation (under assumptions on the condition number of the unknown covariance)
of Gaussian and heavy-tailed distributions [KLSU19, BS19, BKSW19, CWZ19, BDKU20, KSU20,
DFM+20, WXDX20, AAK21, BGS+21].

In this paper, we focus on the task of ﬁnding eﬃcient estimation algorithms for mean, covariance
and, more generally, higher moments with recovery guarantees in multiplicative spectral distance
(i.e., an aﬃne invariant guarantee necessary, for example, to whiten the data or put a set of
points in approximate isotropic position) and relative Frobenius distance (necessary for obtaining
total variation close estimates of an unknown high-dimensional Gaussian). A very recent work
of Liu, Kong, Kakade and Oh [LKKO21] found the ﬁrst private and robust algorithm for mean
estimation under natural distributional assumptions with bounded covariance. However, their
techniques do not appear to extend to covariance estimation. Informally, this is because in order to
obtain privacy guarantees, we need robust estimation algorithms that are stable, i.e., whose output
suﬀers from a bounded perturbation when a single data point is changed arbitrarily. When the

2

unknown covariance is bounded, one can eﬀectively assume that the change in a single data point
is bounded. However, in general, the covariance of the unknown distribution can be exponentially
(in the underlying dimension) varying eigenvalues which precludes such a method (even in the
outlier-free regime).

This work In this paper, we give the ﬁrst algorithms for diﬀerentially private robust moment
estimation with polynomial time and sample complexity. Our algorithms, in fact, provide a gen-
eral blueprint for transforming any robust estimation algorithm into a diﬀerentially private robust
moment estimation algorithm with similar accuracy guarantees as long as the robust estimation
algorithm satisﬁes two key properties: 1) the algorithm is “witness-producing,” i.e., the algorithm
ﬁnds a sequence of “weights” on the input corrupted sample that induce a distribution with a
relevant property of the unknown distribution family (such as certiﬁable subgaussianity or hyper-
contractivity) and 2) the algorithm allows for ﬁnding weights that minimize a natural strongly
convex objective function in polynomial time. Such properties are naturally satisﬁed by robust esti-
mation algorithms based on sum-of-squares semideﬁnite programs. Our main technical result is a
simple framework that transforms such an algorithm into one that satisﬁes worst-case stability under
input perturbation in the relevant norms on the parameters. The ﬁnal ingredient in our framework is
a new noise injection mechanism that uses the stability guarantees so obtained to derive privacy
guarantees. This mechanism allows obtaining privacy guarantees even though the distribution of
the noise being added depends on the unknown quantity being estimated. In particular, such a
subroutine allows us to obtain private robust covariance estimation without any assumptions on
the condition number. We note that even without the robustness constraints, a private covariance
estimation algorithm without any assumptions on the condition number was not known prior to
our work.

Robustness implies privacy? Our blueprint presents an intuitively appealing picture—that ro-
bustness, when obtained by estimators that satisfy some additional but generic conditions, implies
privacy via a generic transformation. This connection might even appear natural: privacy follows
by “adding noise” to the estimates obtained via algorithms that are insensitive or stable with
respect to changing any single point in the input, while robustness involves ﬁnding estimators
that are insensitive to the eﬀects of even up to a constant fraction of outliers. Despite this apparent
similarity, there are two key diﬀerences that prevent such an immediate connection from being
true: 1) privacy is a worst-case guarantee while robustness guarantees are only sensible under
distributional assumptions, and, 2) privacy guarantees need insensitivity even against “inliers.”
Nevertheless, our main result shows that robustness, when obtained via algorithms that satisfy
some natural additional conditions, does yield stable (or insensitive) algorithms as required for
obtaining diﬀerentially private algorithms.

In what follows, we describe our results and techniques in more detail.

3

1.1 Our Results

Formally, our results provide diﬀerentially private robust estimation algorithms in the strong
contamination model, which we deﬁne below.

Deﬁnition 1.1 (Strong Contamination Model). Let 𝜂 > 0 be the outlier rate. Given a distribution 𝐷
on ℝ𝑑 and a parameter 𝑛
ℕ, the strong contamination model with outlier rate 𝜂 gives access to
ℝ𝑑, an i.i.d. sample from 𝐷 of size
a set 𝑌
⊆
𝑛, 2) Return any (potentially adversarially chosen) 𝑌 such that
𝑛. In this case, we
𝑌
say that 𝑌 is an 𝜂-corruption of 𝑋.

ℝ𝑑 of 𝑛 points generated as follows: 1) Generate 𝑋

1
(

𝑋

>

∩

−

⊆

∈

𝜂

)

|

|

In the context of analyzing privacy, we will say that two subsets of 𝑛 points 𝑌, 𝑌′ ⊆

ℝ𝑑 (a.k.a.
databases) are adjacent if they diﬀer in exactly one point (i.e
1.) We now present our
main theorem, which provides a diﬀerentially private robust algorithm for moment estimation of
an unknown certiﬁably subgaussian distribution in the strong contamination model.

𝑌′|

> 𝑛

∩

−

𝑌

|

𝑡

2

)

(

)

)

h

i

𝑥

𝜇

−

𝐷

, 𝑣

𝔼𝐷
(

𝑡 where 𝜇

Our formal guarantees hold for moment estimation of certiﬁably subgaussian distributions.
2𝑡 6

A distribution 𝐷 is 𝐶-subgaussian if for any direction 𝑣 and any 𝑡
is the mean of the distribution 𝐷. Certiﬁable subgaussianity
𝐷
𝐶𝑡
(
is a stricter version of such a property that additionally demands that the diﬀerence between
the two sides of the inequality be a sum-of-squares (SoS) polynomial in the variable 𝑣. Gaussian
distributions, uniform distributions on product domains, all strongly log-concave distributions and,
more generally, any distribution that satisﬁes a Poincaré inequality with a dimension-independent
constant [KS17a] are known to satisfy certiﬁable subgaussianity. See Deﬁnition 3.22 and the
preliminaries for a detailed discussion.

ℕ, 𝔼𝐷

, 𝑣

𝐷

−

𝜇

∈

𝑥

h

i

)

)

(

(

Our ﬁrst result is an algorithm for moment estimation of certiﬁably subgaussian distributions

that runs in polynomial time and has polynomial sample complexity.

ℕ. Then, there exists an 𝜂0 > 0 such that for any given outlier
Theorem 1.2. Fix 𝐶0 > 0 and 𝑘
rate 0 < 𝜂 6 𝜂0 and 𝜀, 𝛿 > 0, there exists a randomized algorithm Alg that takes an input of 𝑛 >

∈

4

ln

1
𝛿
)𝜀
/

(

+

2𝑘
𝑘
1

−

ln

1
𝛿
)𝜀
/

(

𝑛0 =

Ω

𝑑4𝑘
𝜂2

1

+
(cid:16)
9
𝜀 +

(cid:18)

(cid:18)
3
3 ln
(
/
e
𝜀

𝛿

(cid:16)
1), runs in time
2𝐶0 +
)
(
ℚ𝑑, ˆ
Σ
either “reject” or estimates
𝜇
ˆ
∈
2𝑘) satisfying the following guarantees:

(cid:17)
𝐵𝑛

+

∈

(cid:17)

𝐶4𝑘 𝑘4𝑘

6
+

points 𝑌 =

𝑦1, 𝑦2, . . . , 𝑦𝑛

ℚ𝑑 (where 𝐶 =

(cid:19)
𝑂
(

(cid:19)

·
𝑘
) (where 𝐵 is the bit complexity of the entries of 𝑌) and outputs
𝑑 (for all even 𝑡 < 2𝑘 such that 𝑡 divides

} ⊆

ℚ𝑑

{

𝑑

𝑡

×

×···×

)
ℚ𝑑, and ˆ𝑀(

) ∈

1. Privacy: Alg is

𝜀, 𝛿

-diﬀerentially private with respect to the input 𝑌, viewed as a 𝑑-dimensional
)

(

database of 𝑛 individuals.

2. Utility: Let 𝑋 =

{

with mean 𝜇

𝑥1, 𝑥2, . . . , 𝑥𝑛

distribution
2−
𝑌 =
is an 𝜂-corruption of 𝑋, then with probability at least 9
/
and random choices of the algorithm, Alg does not reject and outputs estimates
and ˆ𝑀(

be an i.i.d. sample of size 𝑛 > 𝑛0 from a certiﬁably 𝐶0-subgaussian
for 𝑡 > 2. If
10 over the draw of 𝑋
𝑑,
ℚ𝑑, ˆ
Σ

𝑑 (for all 𝑡 < 2𝑘 such that 𝑡 divides 2𝑘) satisfying the following guarantees:

)𝐼, and moment tensors 𝑀(
∗

𝒟
𝑦1, 𝑦2, . . . , 𝑦𝑛

, covariance Σ

∗ (cid:23)

ℚ𝑑

ℚ𝑑

×···×

poly

𝜇
ˆ

∈

∈

{

}

}

×

×

𝑑

𝑑

∗

)

(

𝑡

𝑡

) ∈

𝑢

∀

∈

ℝ𝑑 ,

𝜇
h ˆ

−

, 𝑢

𝜇

∗

i

6 𝑂

√𝐶 𝑘
(

2𝑘
1
𝜂1
/
−
)

𝑢⊤Σ
∗

𝑢 ,

4

p

and,

1

𝑂

((

−

𝐶 𝑘

)

𝑡

2𝑘
/

1
𝜂1
/
−

𝑘

Σ
)

Σ
∗ (cid:22) ˆ

(cid:22)

and, for all even 𝑡 < 2𝑘 such that 𝑡 divides 2𝑘,

(cid:16)

(cid:17)

1

𝑂

((

+

𝐶 𝑘

𝑡

2𝑘
/

𝑘

1
𝜂1
/
−
)

)

,

Σ
∗

(cid:17)

(cid:16)

1

𝑂

(

−

𝐶 𝑘

𝑡

2𝑘
/

𝜂1
−
)

(cid:16)

(cid:17)

𝑢 ⊗

h

𝑡, 𝑀(

𝑡
)
∗ i

6

𝑢 ⊗

𝑡

𝑡 , ˆ𝑀(

)

h

i

6

1

𝑂

(

+

𝐶 𝑘

𝑡

2𝑘
/

𝜂1
−
)

(cid:16)

𝑢 ⊗

h

𝑡 , 𝑀(

𝑡
)
∗ i

.

(cid:17)

In the above and subsequent theorems, we use the
𝜀, and ln

𝛿

factors in 𝑑, 𝐶, 𝑘, 1
𝜂, 1
/
/

1
/
(

.
)

Ω notation to hide multiplicative logarithmic

e

Discussion Our algorithm above achieves an error guarantee in the “right” aﬃne-invariant norms
similar to the robust moment estimation algorithm of [KS17b].
In particular, the error in the
mean in any direction scales proportional to the variance of the unknown distribution providing
recovery error bounds in the strong “Mahalanobis error.” Similarly, the error in the covariance is
multiplicative in the Löwner ordering. Our algorithm succeeds in the standard word RAM model
of computation.
In particular, the lower bound assumption on the eigenvalue of the unknown
covariance in the statement above is entirely an artifact of numerical issues. Such an assumption
can be removed (and in particular, we can deal with rank deﬁcient covariances) if we assume that
the unknown covariance Σ
has rational entries with polynomial bit complexity. We choose to
∗
make an assumption on the smallest eigenvalue of Σ
∗

for the sake of simpler exposition.

Our algorithm above is obtained by applying a general blueprint that applies to any robust
estimation algorithms that use “one-shot rounding” to produce a diﬀerentially private version. We
explain our general blueprint in more detail in Section 2.

Applications Our diﬀerentially private moment estimation algorithm immediately allows us
to obtain a diﬀerentially private mechanism to implement an outlier-robust method of moments.
This allows us to learn parameters of statistical models that rely on the method of moments,
such as mixtures of spherical Gaussians with linearly independent means [HK13] (that rely on
decomposing 3rd moments) as well as independent component analysis [DLCC07] (that relies on
decomposing fourth moments). We direct the reader to the work on robust moment estimation
that details such applications [KS17b].

Covariance estimation in relative Frobenius error The above theorem provides a multiplicative
spectral guarantee. Such a guarantee, however, only yields a dimension-dependent bound on the
Frobenius norm of the error. While this is provably unavoidable for the class of certiﬁably sub-
gaussian distributions, recent work [BK20b] showed that for distributions that satisfy the stronger
property of having certiﬁably hypercontractive degree 2 polynomials (informally speaking, this
is the analog of certiﬁable subgaussianity for moments of degree 2 polynomials instead of linear
of the random variable 𝑥), one can obtain a dimension-independent bound on the
polynomials
Frobenius estimation error that vanishes as the fraction of outliers tends to zero. Their algorithm
relies on rounding an SoS relaxation with a slightly diﬀerent constraint system. By working with

𝑥, 𝑣

h

i

5

their constraint system and applying our blueprint for obtaining a “stable” version, we obtain a
version of the above theorem with the stronger Frobenius estimation guarantee (see Theorem 5.6).
By combining our privacy analysis above with the recent work that shows that the algorithm
in [BK20b] gives optimal estimation error when analyzed for corrupted samples from a Gaus-
sian distribution, we obtain the following stronger guarantees for private mean and covariance
estimation for Gaussian distributions.

Theorem 1.3 (Mean and Covariance Estimation for Gaussian Distributions). Fix 𝜀, 𝛿 > 0. Then,
there exists an absolute constant 𝜂0 > 0 such that for any given outlier rate 0 < 𝜂 6 𝜂0, there exists a
randomized algorithm Alg that takes an input of 𝑛 > 𝑛0 =
ℚ𝑑, runs in

points 𝑌

Ω

1

ln

4

(

𝑑8
𝜂4

(cid:18)

(cid:16)

+

1
𝛿
)𝜀
/

(cid:19)

(cid:17)

⊆

(

𝑂

time
𝜇
ˆ

1
) (where 𝐵 is the bit complexity of the entries of 𝑌) and outputs either “reject” or estimates
𝐵𝑛
(
)
ℚ𝑑 and ˆ
Σ
∈
1. Privacy: Alg is

-diﬀerentially private with respect to the input 𝑌, viewed as a 𝑑-dimensional
)

𝑑 with the following guarantees:

𝜀, 𝛿

ℚ𝑑

e

∈

×

(

database of 𝑛 individuals.

{

2. Utility: Let 𝑋 =
with mean 𝜇
probability at least 9
/
Σ
ˆ

ℚ𝑑

×

∗

∈

and covariance Σ

𝑥1, 𝑥2, . . . , 𝑥𝑛

be an i.i.d. sample of size 𝑛 > 𝑛0 from a Gaussian distribution
)𝐼 such that 𝑌 is an 𝜂-corruption of 𝑋. Then, with
2−
ℚ𝑑 and

10 over the random choices of the algorithm, Alg outputs estimates

}
∗ (cid:23)

poly

𝑑

(

𝜇
ˆ

∈

𝑑 satisfying the following guarantees:

ℝ𝑑,

𝑢

∀

∈

𝜇
h ˆ

−

, 𝑢

𝜇

∗

i

log

𝛿

)

1
(
/
𝜀

·

𝑢⊤Σ
∗

𝑢 ,

(cid:19) p

6

𝑂

𝜂

(cid:18)

e

and,

2
1
Σ−
/
∗

2
1
ΣΣ−
/
ˆ
∗

𝐼

−

𝑂

𝜂

𝐹 (cid:22)

log

𝛿

)

1
(
/
𝜀

,

!

· r

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

e

𝜂. In particular, 𝑑TV
𝑂 hides multiplicative logarithmic factors in 1
where the
/
𝑂
1
𝜂 log
𝛿
/
(
(
e

.
)

)/

𝜀

Σ
𝜇, ˆ
)

(𝒩( ˆ

,

𝜇

𝒩(

∗

, Σ

∗))

<

1.2 Related Work

e

Since the works of [DKK+16, LRV16], there has been a spate of works designing additional ro-
bust estimation algorithms for a wide variety of problems, including mean and covariance esti-
mation [DKK+17a, DKK+17b, CDGW19, DHL19, HLZ20, Hop20, LY20], mixture models [HL18,
KSS18, BK20c, DHKK20, BDH+20], principal component analysis (PCA) [KSKO20, JLT20], etc. (see
survey [DK19] for details on recent advances in robust statistics). Furthermore, the criterion of
reslience formulated in [SCV18] as a suﬃcient condition for robustly learning a property of a dataset
was subsequently generalized in [ZJS19] in order to deal with a more general class of perturbations.
In the setting of high-dimensional parameter estimation, release of statistics can often reveal
signﬁcant information about individual data points, which can be problematic in a number of
applications in which it is desirable to protect the privacy of individuals while still providing useful
aggregate information (e.g., medical data or census data). Attacks exploiting such properties have

6

 
been investigated in a long line of works [DN03, BUV14, DSS+15, SU15, DSSU17, SSSS17]. In light
of such exploits, there has been much interest in designing statistical algorithms that protect the
privacy of individual samples in a dataset.

In the area of diﬀerentially privacy, various works have explored private estimation per-
taining to Gaussian mixtures [NRS07, KSSU19], identity testing [CKM+20], Markov random
ﬁelds [ZKKW20], etc.

Concurrent related works The problem of private robust mean and covariance estimation has
been the subject of great interest resulting in a few concurrent and independent related works.
Kamath, Mouzakis, Singhal, Steinke, and Ullman [KMS+21] give a diﬀerentially private (in the
outlier-free regime) algorithm for mean and covariance estimation of Gaussian without making
condition number assumptions on the covariance. The work of Liu, Kong, and Oh [LKO21]
gives a statistical feasibility of private robust estimation with optimal sample complexity via a
computationally ineﬃcient algorithm. Finally, Hopkins, Kamath, and Majid [HKM21] also use
the sum-of-squares semideﬁnite programs to obtain private mean estimation (in the outlier-free
setting) algorithm for bounded covariance distribution with pure diﬀerential privacy. Our result are
most directly related to the work of Asthiani and Liaw [AL21] that also obtains eﬃcient private
and robust mean and covariance estimation for Gaussian distributions.

2 Technical Overview

In this section, we give a high-level overview of our general blueprint for obtaining diﬀerentially
private versions of robust estimation algorithms. As a running example, we will focus on the
problem of obtaining private and robust mean and covariance estimators. Speciﬁcally, our goal is
ℝ𝑑, along with an outlier
to design an algorithm that takes input consisting of 𝑛 points, say 𝑌
𝜀, 𝛿
rate 𝜂 and returns estimates of the mean and covariance. We would like the algorithm to be
-
)
diﬀerentially private for every 𝑌 (i.e., a “worst-case” guarantee), viewed as a database in which
each 𝑑-dimensional point in 𝑌 is contributed by an individual. We would like the outputs of the
algorithm to provide faithful estimates whenever 𝑌 is an 𝜂-corruption of a i.i.d. sample from a
distribution that has 𝐶-subgaussian fourth moments.

⊆

(

For the purpose of the ﬁrst part of this overview, we recommend the reader to ignore the
distinction between certiﬁable subgaussianity and “vanilla” subgaussianity. Recall that a distri-
4 6
bution 𝐷 on ℝ𝑑 has 𝐶-subgaussian fourth moments if for every 𝑣
4𝐶
from a 𝐶-subgussian distribution has 2𝐶-subgaussian fourth moments.

2. It turns out that the uniform distribution on a 𝑂
)

𝐷
𝐷
(
∼
size i.i.d. sample 𝑋

ℝ𝑑, 𝔼𝑥
𝑑2

𝔼𝑥
(

, 𝑣

, 𝑣

𝐷

−

−

𝜇

𝜇

∈

𝑥

𝑥

𝐷

h

i

h

i

∼

)

(

)

(

)

2

Stable robust estimation algorithms
In order to design diﬀerentially private algorithms, we
need to ﬁnd robust moment estimation algorithms that are stable. Speciﬁcally, a robust moment
estimation algorithm Alg is stable if the outputs of Alg on any pair of adjacent inputs 𝑌, 𝑌′ (i.e.,
inputs that diﬀer in at most one point but arbitrarily so) are close. Such a guarantee must hold
over worst-case pairs 𝑌, 𝑌′—in particular, 𝑌 may not be obtained by taking an 𝜂-corruption of an

7

i.i.d. sample from a distribution following our assumptions. This presents a problem at the outset
as robust moment estimation algorithms are typically analyzed under distributional assumptions.
The work of [LKKO21] addresses this issue by “opening up” an iterative ﬁlter based algorithm for
robust moment estimation and eﬀectively making every step of the algorithm stable.

2.1 A Prototypical Robust Estimator to Privatize

To understand our ideas, it is helpful to work with a “prototypical” but ineﬃcient robust estimation
algorithm that we can eventually swap with an eﬃcent one. Let us thus start with a simple (but
ineﬃcient) robust estimation algorithm that we call Alg in the discussion below.

Algorithm 2.1. Input: 𝑌 =

𝑦1, 𝑦2, . . . , 𝑦𝑛

{

} ⊆

ℝ𝑑 and outlier rate 𝜂 > 0.

Output: Estimates

Σ of mean and covariance or “reject.”

𝜇, ˆ
ˆ

Operation:

1. Find a witness set of 𝑛 points 𝑋′ ⊆
subgaussian fourth moments and

∩
2. Return the mean and covariance of 𝑋′.

|

ℝ𝑑 such that the uniform distribution on 𝑋′ has
𝑌

𝑛. Reject if no such 𝑋′ exists.

>

𝜂

𝑋′|

1
(

−

)

∈

Observe that the property of having subgaussian fourth moments requires verifying an inequal-
ℝ𝑑, and, in general, there is no eﬃcient (or even sub-exponential time) algorithm
ity for every 𝑣
known (or expected, modulo the small-set expansion hypothesis) for this problem. Nevertheless,
in [KS17b] (see Section 2), the authors prove that a variant of the above program (which we discuss
this at the end of this overview) produces estimates that are guaranteed to be close to the mean and
covariance of 𝐷 if 𝑌 is an 𝜂-corruption of an i.i.d. sample 𝑋 from 𝐷. Note that, though ineﬃcient,
such a result is suﬃcient to establish statistical identiﬁability of mean and covariance of 𝐷 from
𝑂𝜂(
samples. The closeness guarantees in [KS17b] hold from a more general and basic result
that is useful to us in this exposition, which we note below:

𝑑2

)

Fact 2.2 (See Section 2 of [KS17b], Parameter Closeness from Total Variation Closeness). Sup-
pose 𝐷, 𝐷′ are two distributions such that 1) both have subgaussian fourth moments and 2) the to-
6
tal variation distance between 𝐷, 𝐷′ is at most 𝛽. Then, for every 𝑣
∈
Σ
𝑂
𝑣⊤(
𝐷
(
) +
4
𝜂3
that the means (covariances, respectively) of 𝐷, 𝐷′ are close to within 𝑂
/
)
(
Mahalanobis distance, to summarize such a guarantee.

𝐷
𝐷′) −
𝜇
h
(
(
𝐷′))
𝑣. We will say
2
𝜂1
, respectively) in
/
)
(

Σ
𝑣 and 𝑣⊤(
(

ℝ𝑑,
Σ
(
(𝑂

Σ
𝑣⊤(
(

𝐷′) −

𝑣 6 𝑂

4
𝜂3
/
(

𝐷′))

√𝜂
(

)
p

Σ
(

Σ
(

) +

, 𝑣

𝐷

𝐷

))

𝜇

i

)

)

This fact eﬀectively says that if two distributions both have bounded fourth moments and
happen to be close in total variation distance, then their parameters (mean and covariance) must
be close. In fact, the closeness is in strong aﬃne-invariant norms—often called the Mahalanobis
distance for mean and covariance.

8

2.2 Robustness Implies Weak Stability of Alg with a Randomized Outlier Rate

Let us now consider the stability of the above ineﬃcient algorithm. We are seemingly in trouble
at the outset: as written, there must be two adjacent 𝑌, 𝑌′ such that Alg rejects on 𝑌 but not on
𝑌′. Let us introduce our ﬁrst simple idea and show how to patch the algorithm to prevent it from
displaying such “drastic” change in its behavior.

Randomizing the outlier rate The following is a simple but useful observation: If Alg does not
reject on input 𝑌 with outlier rate 𝜂, then, Alg must also not reject on 𝑌′ outlier rate 𝜂
𝑛.
To see why, let 𝑋 be the set of points with subgaussian fourth moments that intersects 𝑌 in
𝑛 points. Then, since 𝑌 and 𝑌′ diﬀer in at most one point, 𝑌′ must intersect 𝑋 in at least
1
)
(
𝑛 points. Thus, if, instead of a ﬁxed outlier rate 𝜂, we ran Alg above with
𝑛
1
(
)
an appropriately “randomized” outlier rate, we might expect the rejection probabilities of Alg on
𝑌, 𝑌′ to be similar. Such an argument can be made formal with a simple truncated Laplace noise
injection procedure.

𝜂
−
𝜂
−

1
/

1
/

1 =

1
(

− (

))

+

−

+

𝑛

𝜂

∩

1,

𝑌′ is of size 𝑛

Robustness implies weak stability in Mahalanobis norms We now address the issue of whether
the estimates computed on 𝑌 and 𝑌′ (assuming Alg does not reject on either of 𝑌, 𝑌′) are close.
We ﬁrst observe that the fact that Alg is outlier-robust already guarantees a weak stability property.
Speciﬁcally, suppose 𝑋 , 𝑋′ are the sets of size 𝑛 generated by Alg when run on inputs 𝑌, 𝑌′. Then,
since 𝑌
1. Next, observe that intersection bound above
−
is equivalent to the uniform distributions on 𝑋 , 𝑋′ having a total variation distance of at most
close in the
2𝜂
relative Mahalanobis distance deﬁned above. Observe that this argument gives stability properties
in the right norms directly! However, this is a weak stability guarantee since it only provides a
ﬁxed constant distance guarantee instead of 𝑜𝑛
that one might expect given that 𝑌 and 𝑌′ diﬀer
1
)
(
in at most 1 out of 𝑛 points. Nevertheless, our discussion shows that robustness, via the ineﬃcient
algorithm above, immediately implies weak stability.

𝑛. Thus, from Fact 2.2, we know that the parameters of 𝑋 , 𝑋′ are 𝑂

𝜂𝑂
(

𝑋′|

1
))

1
/

1
(

2𝜂

𝑋

>

∩

−

+

−

𝑛

)

|

(

2.3 A Simple Private Robust Mean Estimator from Weak Stability

Can we derive private algorithms from the weak stability guarantees? If the unknown covariance
happens to be spherical (i.e., has all of its eigenvalues equal to each other), then the Mahalanobis
distance guarantees are in fact equivalent (up to constant factor scaling) to Euclidean distance
guarantees. As a result, simply adding Gaussian noise calibrated to the sensitivity bounds yields
a private robust mean estimation algorithm! Indeed, 1) randomizing the outlier rate, 2) working
with the SoS relaxation of the above program and 3) adding Gaussian noise to the resulting
estimate, immediately yields a simple, straightforward private robust mean estimator that gives
essentially optimal sample complexity guarantees (i.e., matching those of the known non-private
robust estimators).

9

Weak stability is not enough for covariance estimation The challenge in using weak stability
to obtain private robust covariance estimators arise when the covariance is non-spherical (e.g.,
is rank deﬁcient or has eigenvalues of vastly diﬀerent scales), in which case our Mahalanobis or
multiplicative spectral stability guarantee does not translate into Euclidean/spectral norm distance
guarantees. In particular, if we were to add Gaussian noise, we would end up scrambling all small
eigenvalues up and end up with no non-trivial recovery guarantee.

Indeed, the aforementioned challenge necessitates a rethink of noise injection mechanisms for
covariance estimation in general—standard noise addition mechanisms do not appear meaningful
in faithfully preserving eigenvalues of diﬀerent scales. Prior works (e.g., [KLSU19]) deal with this
by iteratively computing some approximate preconditioning matrices. We have not investigated
robust variants of their method. We instead explore one-shot, blackbox noise injection mechanisms
that still provide us the right guarantees for covariance estimation.

2.4 Noise Injection in Estimate-Dependent Norms

If we wanted to faithfully preserve all eigenvalues (of varying scales) of the unknown covariance,
a natural mechanism would be to add noise linearly transformed with respect to the computed estimate.
Σ is the computed estimate, we would like to consider the mechanism that returns
For example, if ˆ
2 where 𝑍 is a matrix of random Gaussians. The upshot of such a mechanism is that
Σ1
2𝑍 ˆ
Σ
/
ˆ
Σ𝑣 is
Σ—directions where 𝑣⊤ ˆ
it adds noise that is scaled relative to the eigenvalues of the estimate ˆ
small get a smaller additive noise as against directions where the same quadratic form is large.

Σ1
/
+ ˆ

However, the distribution of the added noise in this mechanism depends on the non-privately

estimated quantity itself. Thus, a priori, it provides no useful privacy guarantee!

Key Observation: Nevertheless, our main idea to rescue the above plan is to note that the
mechanism above does indeed provide meaningful privacy guarantees (by standard computations
from the celebrated Gaussian mechanism) if we are able to guarantee that on any adjacent inputs
𝑌, 𝑌′, the non-privately computed estimates are 𝑜𝑛
close in relative Frobenius distance! This
follows from elementary arguments and is presented in Lemmas 4.20 and 4.21.

1
)
(

→ ∞

The observation above crucially needs the distance between covariances (in relative Frobenius
norm) to tend to 0 as 𝑛
; in fact, we need the rate to be inverse polynomial to achieve
polynomial sample complexity. Our weak stability guarantee above, however, guarantees only a
2
𝜂1
weak 𝑂
bound on multiplicative spectral distance which translates into a relative Frobenius
/
)
(
2√𝑑
𝜂1
bound of 𝑂
.
/
→ ∞
(
Thus, in order to use the above mechanism for covariance estimation, we must come up with
signiﬁcantly stronger (and asymptotically vanishing) stability guarantees. Let us investigate how
to obtain such guarantees next.

—not only does this not tend to 0 as 𝑛
)

but it, in fact, explodes as 𝑑

→ ∞

2.5 Strong Stability for Robust Estimation Algorithms

Lack of stability because of multiple diﬀering solutions There is an important barrier that
prevents Alg from oﬀering the strong stability guarantees we need in the covariance estimation
mechanism above. Consider the case when 𝑌 is an i.i.d. sample from a one-dimensional standard

10

)

𝜂

for a small enough constant 𝑐 is 𝜂-close in total variation distance to

Gaussian distribution with mean 0 and variance 1 without any outliers added to it. Then,
±
0, 1
𝑐𝜂
. By a straighforward
)
argument, this implies that we can choose 𝑋′ to be an i.i.d. sample of size 𝑛 from
—
)
if 𝑛 is large enough, then 𝑋′ will have subgaussian fourth moments and will intersect 𝑌 in
𝑛 points. The two diﬀerence distributions (and the corresponding samples 𝑋′) however,
1
(
have variances diﬀering by an additive 𝑂
—a ﬁxed constant independent of the sample size 𝑛.
𝜂
)
(
This shows that even in one dimension, Alg has feasible solutions with variance both
))
. Observe that this issue concerns the output of Alg itself, which can belong to a range
and 1
)
that is signiﬁcantly larger than what we can tolerate—we have not yet touched upon the issue of
what happens when we change 𝑌 to an adjacent 𝑌′.

0, 1

𝜂
(

𝜂
(

1
(

𝒩(

𝒩(

𝒩(

0, 1

𝑐𝜂

𝑂

𝑂

−

+

−

±

)

In order to modify Alg to output a canonical solution
Convexiﬁcation and entropy surrogates
(and with an eye for satisfying the stronger stability property), we wish to make the feasible solution
space of Alg belong to a convex set (instead of the discrete set of solutions 𝑋′ that intersect with 𝑌
𝑛 points). With no fear of computational complexity, this is easy to do in a canonical way:
in
we search instead for a probability distribution over 𝑋′ that satisfy the constraints that Alg imposes.
Unlike 𝑋′, distributions on 𝑋′ that satisfy the constraints are easily seen to form a convex set.

1
(

−

𝜂

)

Given such a convex set, we can resolve our diﬃculty of not having canonical solutions for any
given 𝑌 by simply ﬁnding a solution (i.e., a probability distribution 𝜁 over 𝑋′) that minimizes an
appropriate strongly convex objective function. Speciﬁcally, for any 𝑋′, let 𝑤𝑖 be the 0-1 indicator
of those indices 𝑖 where 𝑥𝑖 = 𝑦𝑖. Then, the constraints in Alg force
𝑛, and the
distribution 𝜁 can be thought to be over

in a natural way.

𝑖 𝑤𝑖 >

1
(

−

𝜂

)

𝑋′, 𝑤

(

)

Í

(

)

𝑤

𝑋′, 𝑤

In order to ensure that Alg ﬁnds a canonical solution, a natural idea is to search over distri-
while minimizing some strongly convex function. We choose the simplest:
1. We think of this objective as a surrogate for ﬁnding “maximum entropy solutions” as,
as deﬁning a probability distribution over 𝑦𝑖, minimizing the ℓ2 norm favors
2
2 is a convex function being minimized over
should

butions 𝜁 over
2
𝔼𝜁[
2
k
]k
when viewing 𝔼
“spread-out” or high entropy solutions. Since
convex set of expectations with respect to 𝜁, we expect that the minimizing solution 𝔼𝜁
be unique.

𝔼𝜁[

𝑤𝑖

˜𝜁[

]k

𝑤

𝑤

∗[

k

]

]

This is not immediately true, however, as our Alg as stated outputs the mean of 𝑋′ (there could

be “multiple” 𝑋′ with the same intersection with 𝑌, in principle).

Modifying the output of Alg In order to ﬁt our framework better, we modify the above blueprint
in Alg to instead output the weighted average of points in 𝑌 instead of 𝑋′. While such a procedure
is not directly analyzed in [KS17b], the methods there can be naturally adapted without much
hiccup. As a result we obtain the following modiﬁed version of Alg that we can now work with:

1The exponent of the polynomials appearing in our sample complexity bounds improve if we use a strongly convex
log 𝑑. Our interest is in presenting a general “privatizing”
1
/

function with respect to 1-norm such as
blueprint so we continue with the simpler choice above in this work.

2
𝑞 for 𝑞 = 1

+

𝑥

k

k

11

Algorithm 2.3. Input: 𝑌 =

𝑦1, 𝑦2, . . . , 𝑦𝑛

{

} ⊆

ℝ𝑑 and an outlier rate 𝜂 > 0.

Output: Estimates

Σ of mean and covariance or “reject.”

𝜇, ˆ
ˆ

Operation:

1. Find a probability distribution 𝜁 over a witness set of 𝑛 points 𝑋′ ⊆

ℝ𝑑 and inter-
section indicator 𝑤
𝑋′, 𝑤
)
such that 1) the uniform distribution on 𝑋′ has subgaussian fourth moments and 2)

2
2 and is supported on

𝑛 that minimizes

𝔼𝜁[

0, 1

∈ {

]k

𝑤

}

k

(

𝑖 𝑤𝑖 >
2. Return
Í

)

1
𝜂
−
(
𝜇 = 1
𝑍
ˆ

𝑛. Reject if no such 𝜁 exists.
Σ = 1
𝑖 𝔼𝜁[
𝑍

𝑖 𝔼𝜁[

𝑦𝑖, ˆ

𝑤𝑖

𝑤𝑖

𝑖 𝔼𝜁[
With this modiﬁcation, Alg outputs a canonical single solution on any given 𝑌 (or rejects).

)⊤ where 𝑍 =
𝜇
− ˆ

𝜇
− ˆ

Í

Í

Í

𝑦𝑖

𝑦𝑖

](

)(

]

𝑤𝑖

.

]

Stability of Alg from the stability of the entropy potential We now return to the issue of
stability. What happens if we switch the input 𝑌 of Alg above to 𝑌′? The strongly convex objective
we imposed in the above discussion comes in handy here! Namely, by basic convex analysis
(see Proposition 3.20), it follows that if optimum entropy potential values of Alg on 𝑌 and 𝑌′
-close, then, the vectors 𝔼𝜁[
are say, 𝑂
close. Recall
1
)
(
that each 𝔼𝜁[
𝑤𝑖
0, 1
is a number in
and that these numbers add up to 1. Hence, intuitively
]
2
𝔼𝜁[
speaking, 𝑂
2 corresponds to constant perturbation in a constant number
-closeness of
1
)
(
of coordinates.

are themselves 𝑂

and 𝔼𝜁[

𝑌′)
](

1
)
(

[
𝑤

]k

𝑤

𝑤

](

𝑌

k

]

)

Thus, working with the strongly convex objective above reduces our stability analysis of Alg to
simply understanding how much can our entropy potential change when changing a single point
in 𝑌.

Unfortunately, this change can be large in general.
𝜂

]k
2𝑛. The additive diﬀerence between these two extremes is 𝑂
)

𝑤

k

𝔼𝜁[

1
(

−

2
2 varies between
𝑂
𝜂𝑛
(

.
1
)
(

) ≫

1
(

𝜂

)

−

𝑛 and

Stabilizing the entropy potential: private stable selection Before describing our key idea, we
ﬁrst make a simple observation: Fix an input 𝑌 and consider the optimum value of the entropy
potential of Alg when run with outlier rate 𝜂. What happens if we change 𝜂 to 𝜂
𝑛? Clearly, the
potential cannot increase: any solution 𝜁 with outlier rate 𝜂 is also a solution for outlier rate 𝜂
𝑛.
The potential can decrease arbitrarily though.

1
/

1
/

+

+

More speciﬁcally, we show the following: in order to make the entropy potential stable under
such that the
is within an

a change of 𝑌 to an adjacent 𝑌′, it is enough to run 𝑌 with an outlier rate 𝜂′ = 𝑂
𝜂
(
)
entropy potential of Alg on 𝑌 for any outlier rate in the interval
𝑛
𝐿
𝑛, 𝜂′ +
/
additive

of any other.

𝜂′ −
[

𝑂

𝑛

𝐿

𝐿

/

]

e

To see why this claim could be true, informally speaking, observe that if 𝑌′ is obtained from
𝑌 by changing at most a single point, then a solution 𝜁 with outlier rate 𝜂′ can be modiﬁed into a
𝑛 by simplying zeroing out the 𝑤𝑖 for the index 𝑖 where 𝑌′
solution 𝜁′ for 𝑌′ with outlier rate 𝜂′ +
and 𝑌 diﬀer. This allows us to relate the potentials for neighboring outlier rates on 𝑌 and 𝑌′. Under

1
/

(

/

)

12

the above assumption, the potential remains stable in an interval around 𝜂′ on 𝑌. This allows us
to conclude that the same must be true for 𝑌′ for the interval

𝑛

𝐿

𝑛

𝐿

.

𝜂′ −
[

/

+

1, 𝜂′ +

/

−

1
]

The above reasoning allows us to obtain strong stability guarantees if we can 1) show that a

stable interval as above exists and 2) ﬁnd such an interval via a stable process.

A stable selection procedure via the exponential mechanism We show that a stable interval as
above (for 𝐿 =
) exists via a simple Markov-like argument. Using an appropriate scoring rule,
1
)
(
we show that the standard exponential mechanism can then be used to produce a stable interval
like above via a stable algorithm (see Section 3.6.4).

𝑂𝑛

e

𝑤𝑖

Putting things together Altogether, we obtain a version of Alg that outputs a sequence of weights
(i.e., 𝔼𝜁[
) that are stable under the modiﬁcation of a single point in 𝑌. When viewed as a
√𝑛
distribution on 𝑌, the stability guarantee we obtain corresponds to an ℓ1-stability of
)
compared to the 𝑂
(a ﬁxed constant) stability that follows from any naive robust estimation
algorithm.

1
/
(

𝜂
(

𝑂

e

]

)

We note that
𝑥

function

k

k

√𝑛
𝑂
1
/
(
2
𝑞 for 𝑞 = 1
e

)
+

can be upgraded to
1
/

log 𝑛.

e

𝑂

𝑛

1
/
(

)

if we work with a more sophisticated potential

By applying Fact 2.2, we immediately get that if Alg does not reject on 𝑌, 𝑌′, then the parameters
of the respective inputs must be close in the Mahalanobis distance up to a polynomially vanishing
function of 𝑛, as desired. This allows us to implement the estimate-dependent noise injection
mechanism for covariance estimation!

We note that the discussion above can be formalized into an information-theoretic private identi-
ﬁability algorithm (i.e., an ineﬃcient private robust algorithm). We next discuss how to transform
the above blueprint result into an eﬃcient algorithm.

2.6 From Ideal Algorithms to Eﬃcient Algorithms

Let us now go back and summarize 1) facts about the idealized ineﬃcient algorithm and 2) our
general blueprint for making such an algorithm Alg private.

1. Witness Production: We have used that the fact that Alg searches over witnesses 𝑋′ that
share the relevant property of the distributional model we have chosen (e.g., subgaussianity
of fourth moments in the above discussion).

2. Strongly Convex Entropy Potential: We have minimized a strongly convex potential function

in order to ensure that Alg outputs a canonical solution.

3. Stable Outlier Rate Selection: We have implemented a randomized stable selection scheme
(via the exponential mechanism) for the outlier rate in order to argue that the optimum
entropy potential of Alg is stable under the modiﬁcation of a single point in the input 𝑌.

We can apply this scheme to any algorithm that outputs a sequence of weights on the input sam-
ple 𝑌, subject to the constraint that 1) the weights induce the relevant property of the distributional
model, and 2) they minimize a strongly convex potential function.

13

Witness-producing SoS-based robust estimation algorithms
It turns out that we can ensure
all the above properties for eﬃcient robust estimation algorithms based on “one-shot rounding”
of convex relaxations. We speciﬁcally rely on the algorithms for robust estimation based on SoS
semideﬁnite programs in this work.

The SoS-based algorithms in the prior works that we use [BK20a, KS17b] almost ﬁt our

requirements except with two technical constraints:

1. The algorithms in the aforementioned prior works do not output weights on 𝑌 explicitly.
However, we are able to show that a natural modiﬁcation that outputs such weights on 𝑌 can
be analyzed by the same methods.

2. The algorithms in the aforementioned prior works were analyzed under distributional as-
sumptions on 𝑌 without the need to explicitly argue that the weights induce good witnesses
Indeed, arguing that these algorithms produce
(which we desire in our above analysis).
such witnesses on worst-case datasets 𝑌 (whenever they don’t reject) appears challenging.
However, we are able to get by without such a statement by observing that we can adapt
the analyses of the algorithms in the prior works to infer the following statement:
if the
algorithm returns a good witness on 𝑌, then under a small perturbation of the parameters,
it must also return a good witness on an adjacent 𝑌′.

While verifying the properties makes our transformation not entirely blackbox at the moment,
we strongly believe that our blueprint demonstrates a conceptually appealing connection between
robust algorithm design and private algorithm design. Concretly, we expect our blueprint to
be useful in designing more private (and robust) estimation algorithms. Indeed, we believe our
techniques immediately extend to other problems where SoS-based robust estimation algorithms
are known, such as linear regression [KKM18, BP20] and clustering spherical and non-spherical
mixtures [DHKK20, BK20a, HL18, KS17c, FKP19].

3 Preliminaries

In this work, we will deal with algorithms that operate on numerical inputs. In all such cases,
we will rely on the standard word RAM model of computation and assume that all the numbers
are rational represented as a pair of integers describing the numerator and the denominator. In
order to measure the running time of our algorithms, we will need to account for the length of the
numbers that arise during the run of the algorithm. The following deﬁnition captures the size of
the representations of rational numbers:

Deﬁnition 3.1 (Bit Complexity). The bit complexity of an integer 𝑝
complexity of a rational number 𝑝

ℤ is 1
. The bit
ℤ is the sum of the bit complexities of 𝑝 and 𝑞.

𝑞 where 𝑝, 𝑞

log2 𝑝

+ ⌈

∈

⌉

/

∈

For any ﬁnite set 𝑋 of points in ℝ𝑑, we will use 𝜇

)(
covariance and the 𝑡-th moment tensor of the uniform distribution on 𝑋.

, Σ
(

, 𝑀(

𝑋

𝑋

)

)

(

𝑡

to denote the mean,

𝑋

)

14

3.1 Pseudo-Distributions

Pseudo-distributions are generalizations of probability distributions and form dual objects to sum-
of-squares proofs in a precise sense that we will describe below.

Deﬁnition 3.2 (Pseudo-distribution, Pseudo-expectations, Pseudo-moments). A degree-ℓ pseudo-
2 >
distribution is a ﬁnitely-supported function 𝐷 : ℝ𝑛
)
0 for every polynomial 𝑓 of degree at most ℓ

𝑥
(
2. (Here, the summations are over the support of 𝜇.)
The pseudo-expectation of a function 𝑓 on ℝ𝑑 with respect to a pseudo-distribution 𝐷, denoted

ℝ such that

= 1 and

𝑥 𝐷

𝑥 𝐷

→

Í

Í

𝑥

𝑥

/

(

)

(

)

𝑓

𝔼𝐷

𝑥

(

)

𝑓

𝑥

, as
)

(

e

𝔼𝐷

𝑥

(

)

𝑓

𝑥

(

)

=

𝐷

𝑥

𝑓

𝑥

(

)

(

)

.

(3.1)

𝑥

= 𝑥, i.e., 𝜇

Õ𝑥
In particular, the mean 𝜇 of a pseduo-distribution is deﬁned naturally as the pseudo-expectation of
𝑓

)
The degree-ℓ moment tensor of a pseudo-distribution 𝜇 is the tensor 𝔼𝜇

ℓ .
)⊗
In particular, the moment tensor has an entry corresponding to the pseudo-expectation of every
monomial of degree at most ℓ in 𝑥.

1, 𝑥1, 𝑥2, . . . , 𝑥𝑛

𝔼𝐷

𝑥.

e

e

)(

(

𝑥

𝑥

(

)

(

Observe that if a pseudo-distribution 𝜇 satisﬁes, in addition, that 𝜇

> 0 for every 𝑥, then
it is a mass function of some probability distribution. Further, a straightforward polynomial-
pseudo-distribution satisﬁes 𝜇 > 0 and is thus
interpolation argument shows that every degree-
an actual probability distribution. The set of all degree-ℓ moment tensors of probability distribution
is a convex set. Similarly, the set of all degree-ℓ moment tensors of degree-𝑑 pseudo-distributions
is also convex.

∞

𝑥

)

(

We now deﬁne what it means for

𝔼 to (approximately) satisfy constraints.

{

e

Deﬁnition 3.3 (Satisfying constraints). For a polynomial 𝑔, we say that a degree-𝑘
exactly if for every polynomial 𝑝 of degree 6 𝑘
𝑔 = 0
constraint
}
𝔼
𝑝
𝑝𝑔 𝑗
approximately if
[
|
every polynomial 𝑝 of degree 6 𝑘
(cid:13)
(cid:13)
2
𝔼
2.
(cid:13)
(cid:13)

𝔼
,
deg
𝑔
[
)
(
𝑔 > 0
𝔼 satisﬁes the constraint
{
e
𝔼

2. We say that
deg
2
𝑔
(

2, it holds that

6 𝜏

𝑝2𝑔

𝑝2𝑔

)/

(cid:13)
(cid:13)

(cid:13)
(cid:13)

e

e

e

>

−

−

−

]|

𝜏

𝑝

/

𝔼 satisﬁes the
= 0 and 𝜏-
𝑝𝑔
]
e
exactly if for
}
> 0 and 𝜏-approximately if

The following fact describes the precise sense in which pseudo-distributions are duals to sum-

[

]

]

[

e
of-squares proofs.

Fact 3.4 (Strong Duality, [JH16], see Theorem 3.70 in [FKP19] for an exposition). Let 𝑝1, 𝑝2, . . . , 𝑝𝑘
be real-coeﬃcient polynomials in 𝑥1, 𝑥2, . . . , 𝑥𝑛. Suppose there is a degree-𝑑 sum-of-squares refutation of
𝑖6𝑘 . Then, there is no pseudo-distribution 𝜇 of degree > 𝑑 satisfying
𝑖6𝑘 .
the system
)
𝑖6𝑘 .
On the other hand, suppose that there is a pseudo-distribution 𝜇 of degree 𝑑 consistent with
)
𝑖 𝑥2
Suppose further that the set
𝑖 for some 𝑅 > 0.
Then, there is no degree-𝑑 sum-of-squares refutation of the system

contains the quadratic polynomial 𝑅
𝑖6𝑘 .

𝑝1, 𝑝2, . . . , 𝑝𝑘

> 0
> 0

> 0

> 0

𝑝𝑖
𝑝𝑖

𝑥
𝑥

{
{

}
}

(
(

𝑝𝑖

𝑝𝑖

−

𝑥

𝑥

}

}

{

{

(

)

{

(

)

}

Í

Basic sum-of-squares (SoS) proofs

15

Fact 3.5 (Operator norm Bound). Let 𝐴 be a symmetric 𝑑
and denominators upper-bounded by 2𝐵 and 𝑣 be a vector in ℝ𝑑. Then, for every 𝜀 > 0,

×

𝑑 matrix with rational entries with numerators

𝑣
2

𝑣⊤𝐴𝑣 6

𝐴

𝑣

2
2 +

𝜀

(cid:8)
The total bit complexity of the proof is poly
(

k
𝐵, 𝑑, log 1
/
Fact 3.6 (SoS Hölder’s Inequality). Let 𝑓𝑖 , 𝑔𝑖 for 1 6 𝑖 6 𝑠 be indeterminates. Let 𝑝 be an even positive
integer. Then,

(cid:9)

k

k2k
.
𝜀
)

𝑓 ,𝑔
𝑝2

1
𝑠

𝑠

𝑓𝑖𝑔

𝑝
𝑖

𝑝

1
−

6

!

1
𝑠

𝑠

𝑓

𝑝
𝑖

1
𝑠

!  

𝑠

𝑔

𝑝
𝑖

!

𝑝

1
−

.

The total bit complexity of the SoS proof is 𝑠𝑂




Observe that using 𝑝 = 2 yields the SoS Cauchy-Schwarz inequality.





Õ𝑖=1

Õ𝑖=1

Õ𝑖=1

).

𝑝

(

Fact 3.7 (SoS Almost Triangle Inequality). Let 𝑓1, 𝑓2, . . . , 𝑓𝑟 be indeterminates. Then,


The total bit complexity of the SoS proof is 𝑟𝑂



Õ𝑖6𝑟
).

(

𝑡

𝑓1 , 𝑓2 ,..., 𝑓𝑟
2𝑡

2𝑡

𝑓𝑖

!

6 𝑟2𝑡

1
−

𝑓 2𝑡
𝑖

𝑟

Õ𝑖=1

.

! 



Fact 3.8 (SoS AM-GM Inequality, see Appendix A of [BKS15]). Let 𝑓1, 𝑓2, . . . , 𝑓𝑚 be indeterminates.
Then,

𝑓𝑖 > 0

|

𝑖 6 𝑚

(cid:8)

(cid:9)

𝑓1 , 𝑓2,..., 𝑓𝑚
𝑚

1
𝑚

( 

Õ𝑖=1

𝑚

𝑚

𝑓𝑖

!

> Π𝑖6𝑚 𝑓𝑖

.

)

The total bit complexity of the SoS proof is exp
(
We will also use the following two consequence of the SoS AM-GM inequality:

.
))

𝑂

𝑚

(

Proposition 3.9. Let 𝑎, 𝑏 be indeterminates. Then,

𝑎,𝑏
2𝑡

𝑎2𝑗𝑏2𝑡

2𝑗 6 𝑗𝑎2𝑡
−

𝑡

+ (

−

𝑏2𝑡

𝑗

)

.

The total bit complexity of the SoS proof is exp
(
Proof. We apply the SoS AM-GM inequality with 𝑓𝑖 = 𝑎2 for 𝑖 = 1, . . . , 𝑗 and 𝑓𝑖 = 𝑏2 for 𝑖 = 𝑗
We thus obtain:

.
))

𝑂

(

𝑡

(cid:9)

(cid:8)

−
By the SoS Almost Triangle inequality, we have:

/

(

(cid:8)

𝑎,𝑏
2𝑡

𝑗

𝑡 𝑎2

1
+ (

𝑏2

𝑗

𝑡

)

/

)

𝑡 > 𝑎2𝑗𝑏2𝑡

2𝑗

−

(cid:9)

1, . . . , 𝑡.

+

𝑎,𝑏
2𝑡

𝑡 𝑎2

𝑗

(

/

1
+ (

−

𝑗

𝑡

)

/

𝑏2

)

𝑡 6

𝑗𝑎2𝑡

(

𝑡

𝑗

)

−

+ (

𝑏2𝑡

)

Combining the above two claims completes the proof. The total bit complexity of the SoS proof
follows immediately by using the bounds for the two constituent inequalities used in the proof
(cid:3)
above.

(cid:9)

(cid:8)

16

 
 
 
 
Proposition 3.10. Let 𝑎, 𝑏 be indeterminates. Then, for any positive integers 𝑖, 𝑡 such that 𝑖 is odd and
2𝑡 > 𝑖, we have:

𝑎𝑖𝑏2𝑡

−

𝑖 6 1
2 (

𝑎,𝑏
2𝑡

(cid:26)

𝑎𝑖

1𝑏2𝑡
−

𝑖

1
+

−

𝑎𝑖

1𝑏2𝑡
+

𝑖

1
−

−

+

.

)

(cid:27)

The total bit complexity of the SoS proof is exp
(
1 for some 𝑟 > 1. Then, we have: 𝑎𝑖𝑏2𝑡

.
))

𝑂

(

𝑡

Proof. Write 𝑖 = 2𝑟
AM-GM inequality with 𝑓1 = 𝑎𝑟 𝑏𝑡

−

𝑟 and 𝑓2 = 𝑎𝑟

1𝑏𝑡
−

𝑟

1, we thus have:
+

−

−

1 = 𝑎𝑟 𝑏𝑡
−

𝑟 𝑎𝑟

1𝑏𝑡
−

𝑟

1. By the SoS
+

−

−

𝑎𝑖𝑏2𝑡

𝑖 = 𝑎𝑟 𝑏𝑡

𝑟 𝑎𝑟

1𝑏𝑡
−

−

−

−

𝑟

𝑎,𝑏
2𝑡

(cid:26)

1 6 1
2 (

+

𝑎𝑖

1𝑏2𝑡
−

𝑖

1
+

−

𝑎𝑖

1𝑏2𝑡
+

𝑖

1
−

−

+

.

)

(cid:27)

(cid:3)

Fact 3.11 (Cancellation within SoS, Constant RHS [BK20b]). Suppose 𝐴 is indeterminate and 𝑡 > 1.
Then,

𝐴2𝑡 6 1

𝐴
2𝑡

𝐴2 6 1

Further, the total bit complexity of the SoS proof is at most 2𝑂

(cid:9)

(cid:8)

(cid:8)

𝑡

(

).

(cid:9)

Lemma 3.12 (Cancellation within SoS [BK20b]). Suppose 𝐴 and 𝐶 are indeterminates and 𝑡 > 1. Then,

(cid:8)
Further, the total bit complexity of the SoS proof is at most 2𝑂

(cid:9)

(cid:8)
𝑡
).
(

(cid:9)

𝐴 > 0

∪

𝐴𝑡 6 𝐶𝐴𝑡

1
−

𝐴,𝐶
2𝑡

𝐴2𝑡 6 𝐶2𝑡

.

3.2 Algorithms and Numerical Accuracy

The following fact follows by using the ellipsoid algorithm for semideﬁnite programming. The
resulting algorithm to compute pseudo-distributions approximately satisfying a given set of poly-
nomial constraints is called the sum-of-squares algorithm.

Fact 3.13 (Computing pseudo-distributions consistent with a set of constraints [Sho87, Par00, Nes00,
ℕ, 𝜏 > 0,
Las01]). There is an algorithm with the following properties: The algorithm takes input 𝐵
∈
and polynomials 𝑝1, 𝑝2, . . . , 𝑝𝑘 of degree ℓ with rational coeﬃcients of bit complexity 𝐵.
If there is a
𝑖6𝑘 , the algorithm in time
pseudo-distribution of degree 𝑑 consistent with the constraints
>
𝐵𝑛
(
0

outputs a pseudo-distribution 𝜇 of degree 𝑑 that 𝜏-approximately satisﬁes

1
) poly log
/
(

> 0

𝑝𝑖

𝑝𝑖

𝜏

𝑥

𝑥

{

{

}

𝑂

(

)

)

)

(

𝑑

(

)
𝑖6𝑘 .

}

3.3 Tensors

Since we will deal with higher moments of distributions, which are naturally represented as
tensors, we will need to deﬁne some related notation and conventions for the sake of clarity in our
exposition.
Let

for any natural number 𝑛. We deﬁne the following.

1, 2, . . . , 𝑛

=

𝑛

[

]

{

}

17

Deﬁnition 3.14. Suppose we have an 𝑚
to be the standard 𝑚𝑚′ ×
Moreover, for an 𝑚
×

𝑛 matrix 𝑀, we denote by 𝑀 ⊗

𝑛 matrix 𝑀 and an 𝑚′ ×
𝑛𝑛′ matrix given by the Kronecker product of 𝑀 and 𝑁.

×

𝑛′ matrix 𝑁. We deﬁne 𝑀

𝑁

⊗

𝑡 the 𝑡-fold Kronecker product 𝑀

𝑀

⊗

⊗ · · · ⊗

𝑀

𝑡 times

(of dimension 𝑡𝑚

Given an 𝑚

×

𝑡𝑛).

×
𝑛 matrix 𝑀, we will also ﬁnd it convenient to index 𝑀 ⊗

1 6 𝑖1, 𝑖2, . . . , 𝑖𝑡 6 𝑚 and 1 6 𝑗1, 𝑗2, . . . , 𝑗𝑡 6 𝑛, we can refer to the term 𝑀 ⊗
(

𝑡
𝑘=1 𝑀𝑖𝑡 ,𝑗𝑡 .
We also deﬁne a useful ﬂattening operation on tensors:

|

{z

}
𝑡 as follows: for any
=

𝑡
𝑖1,𝑖2 ,...,𝑖𝑡

𝑗1,𝑗2 ,...,𝑗𝑡

,

)

(

)

Î
Deﬁnition 3.15. Given an 𝑚1 ×
𝑚1𝑚2 · · ·
to be the
of 𝑀 appearing in the natural lexicographic order on
𝑚𝑡
𝑚2] × · · · × [
for 𝑘 = 1, 2, . . . , 𝑡) in vec
entry 𝑀𝑖1,𝑖2 ,...,𝑖𝑡 appears before 𝑀𝑗1,𝑗2 ,...,𝑗𝑡 (where 𝑖𝑘 , 𝑗𝑘
(
only if there exists some 1 6 𝑘 6 𝑡 such that 𝑖𝑘 < 𝑗𝑘 and 𝑖𝑙 = 𝑗𝑙 for all 𝑙 < 𝑘.

𝑚𝑡 tensor 𝑀, we deﬁne the ﬂattening, or vectorization, of 𝑀
𝑀
, whose entries are precisely the entries
(
)
𝑚1] × [
. In other words, the
𝑚𝑘
if and
]
∈ [

-dimensional vector, denoted vec
)

𝑚2 × · · · ×

𝑚𝑡

𝑀

]

[

)

(

Deﬁnition 3.16. Given an 𝑛-dimensional vector 𝑢 and an 𝑛

𝑛

-dimensional tensor 𝑀,

𝑛

×

× · · · ×

𝑡 times

we deﬁne
h
𝑛𝑡-dimensional vectors) between the ﬂattenings of 𝑢 ⊗

, vec
(

vec
(

to be

𝑢 ⊗

𝑢 ⊗

𝑀

h

i

)

𝑡, 𝑀

𝑡

|
𝑡 and 𝑀.

)iℝ𝑑, i.e., the value of the standard inner product (on

{z

}

A convenient fact we will use is a so-called “mixed product” property for matrices.

Fact 3.17. Given an 𝑚

𝑛 matrix 𝐴, 𝑚′ ×

×

𝑛′ matrix 𝐵, and 𝑛

×

𝑛′ matrix 𝑉, we have that

𝐴𝑉 𝐵𝑇 =

𝐴

(

⊗

𝐵

𝑉
vec
(

)

,

)

where the above is expressed as matrix-vector product.

Finally, we deﬁne the moment tensor for a probability distribution.

Deﬁnition 3.18. Given a probability distribution
the 𝑡th moment tensor 𝑀 to be a 𝑑

𝑑

𝑑

on ℝ𝑑 and an integer 𝑡 > 1, we deﬁne
tensor whose entries are given by 𝑀𝑖1,𝑖2 ,...,𝑖𝑡 =

𝒟

𝔼𝑋

𝑋𝑖1 𝑋𝑖2 · · ·

𝑋𝑖𝑡 ]

for 𝑖1, 𝑖2, . . . , 𝑖𝑡
|

∈ [

∼𝒟[

3.4 Basic Convexity

×

× · · · ×
𝑡 times
𝑑
.
{z
]

}

We will use the following basic propositions about convexity in our analysis.

Proposition 3.19 (Neighborhoods of minimizers of convex functions). Let 𝐾 be a closed convex subset
of ℝ𝑁 . Let 𝑓 be a smooth convex function on ℝ𝑁 . Let 𝑥 be a minimizer of 𝑓 on 𝐾. Then, for every 𝑦
𝐾,
𝑦

> 0.

𝑥,

∈

𝑥

𝑓

(

−

∇

)i

h
Proof. If not, then for a small enough positive 𝜆,
1
(

𝜆𝑦

𝐾.

+

−

𝜆

∈

𝑥

)

18

𝑓

𝑥

(

𝜆

𝑦

(

+

𝑥

))

−

< 𝑓

𝑥

. But, 𝑥
)

(

𝜆

𝑦

(

+

𝑥

)

−

=
(cid:3)

                 
                 
            
            
            
            
Proposition 3.20 (Pythagorean theorem from strong convexity w.r.t 2 norm). Let 𝐾 be a convex
subset of ℝ𝑑 for 𝑑
𝐾. Then,
𝑦
𝑓
𝑓

ℕ. Let 𝑥 be a minimizer of the convex function 𝑓
𝑥

2
2 on 𝐾. Let 𝑦

>

=

∈

𝑥

𝑦

𝑥

𝑥

k

k

)

(

2
2.

∈
−
𝑦

)

(

) −

(
(cid:13)
Proof. We have:
𝑦
(cid:13)
sition 3.19 to observe that
(cid:13)
(cid:13)

−
𝑦
h
We will also need the following basic bound:

2
2 + k
𝑥, 𝑥
(cid:13)
i
−
(cid:13)

2
𝑥
2 +
k
> 0.

2
(cid:13)
(cid:13)
2

(cid:13)
(cid:13)

(cid:13)
(cid:13)

−

=

𝑥

𝑦

2

h

𝑥, 𝑥

. The proposition follows by applying Propo-
(cid:3)

i

Lemma 3.21. Suppose 𝑥, 𝑦
𝑥 = 𝑥
𝑥
¯
k1
k

𝑦 = 𝑦
𝑦1
¯
¯

and

𝑛 such that
be the normalized versions of 𝑥, 𝑦. Then,

0, 1
]

𝑖 𝑥𝑖 ,

∈ [

𝑖 𝑦𝑖 > 𝑛

Í

𝑥
¯

𝑦
− ¯

1

Í
6 6𝛽 .

2 and

𝑥

/

𝑦

1

−

6 𝛽𝑛 for 𝛽 6 1
/

10. Let

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Proof. Suppose, without loss of generality, that
𝑦
know that
− ¯
6 6𝛽.
𝛽𝑛2

= 𝑐2𝑛 >

𝑛. Thus,

𝑐1 −

𝑥
¯

𝛽

𝑦

)

(

1

)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
k1 = 𝑐1𝑛 > 𝑐2𝑛 =
𝑥
6 1
𝑦
𝑥
(cid:13)
𝑐1 𝑐2𝑛2 (
1 −
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

𝑦

k
1

𝑦
𝑥
(cid:13)
(cid:13)

k

(cid:13)
(cid:13)

(cid:13)
(cid:13)

1 for 𝑐1, 𝑐2 > 1
/
6 1
𝑐1𝑛
k1
𝑐1𝑐2𝑛2 (

1

2. Then, we

𝑦

𝑥

−

(cid:13)
(cid:13)

1 +
(cid:3)
(cid:13)
(cid:13)

3.5 Certiﬁable Subgaussianity

Deﬁnition 3.22 (Certiﬁable Subgaussianity). A distribution 𝐷 on ℝ𝑑 with mean 𝜇
is said to be 2𝑘-
certiﬁably 𝐶-subgaussian if there is a degree 2𝑘 sum-of-squares proof of the following polynomial
inequality in 𝑑-dimensional vector-valued indeterminate 𝑣:

∗

𝔼
𝑥

∼

𝐷h

𝑥

, 𝑣

𝜇

∗

i

−

2𝑘 6

𝐶 𝑘

(

𝑘

)

(cid:18)

𝔼
𝑥

∼

𝐷h

2

𝑥

, 𝑣

𝜇

∗

i

−

𝑘

.

(cid:19)

Furthermore, we say that 𝐷 is certiﬁable 𝐶-subgaussian if it is 2𝑘-certiﬁably 𝐶-subgaussian for
every 𝑘

ℝ𝑑 is said to be 2𝑘-certiﬁable 𝐶-subgaussian if the uniform distribution on 𝑋

ℕ.
A ﬁnite set 𝑋

∈

⊆

is 2𝑘-certiﬁably 𝐶-subgaussian.

0, 1
]

𝑛 be weight vectors satisfying

Fact 3.23 (Consequence of Theorem 1.2 in [KS17b]). Let 𝑌 be a collection of 𝑛 points in ℝ𝑑. Let
= 𝜏. Suppose that the
𝑝, 𝑝′ ∈ [
distributions on 𝑌 where the probability of 𝑖 is 𝑝𝑖 (𝑝′𝑖, respectively) is 2𝑘-certiﬁably 𝐶1 (𝐶2, respectively)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
subgaussian. Let 𝜇𝑝 =
ℕ be
𝑦𝑖
for every 𝑡
)(
(
𝑡
the mean, covariance and 𝑡-th moment tensor of distribution deﬁned 𝑝. Deﬁne 𝜇𝑝′ , Σ𝑝′ , 𝑀(
)𝑝′
the distribution corresponding to 𝑝′.

(cid:13)
(cid:13)
)⊤, and 𝑀(

∈
similarly for

1,
(cid:13)
(cid:13)
𝜇𝑝
−

𝑖 𝑝𝑖 𝑦𝑖, Σ𝑝 =

= 1, and

𝑖 𝑝𝑖 𝑦⊗
𝑖

(cid:13)
(cid:13)
−

𝑡
)𝑝 =

𝑖 𝑝𝑖

𝜇𝑝

Í

Í

Í

𝑝′

𝑝′

𝑦𝑖

(cid:13)
(cid:13)

−

𝑝

𝑝

1

1

𝑡

Then, for every 𝜏 6 𝜂0 for some absolute constant 𝜂0, for every 𝑢

𝜇𝑝

h

−

𝜇𝑝′ , 𝑢

i

1
(
𝑂

(

1
(

−

1
𝜏1
/
−

𝑘

𝑂

𝐶′𝑘

−
𝑡
𝐶′

(
2𝑘𝑡
/

2
/

)
𝜏1
−

𝑡

2𝑘
/

)h

)

6 𝜏1
2𝑘
1
/
−

𝑂

√𝐶 𝑘
·
(
Σ𝑝′ (cid:22) (
1
(cid:22)
6
𝑡 , 𝑀(
𝑢 ⊗
)𝑝 i

+
𝑢 ⊗

)
q
𝑂
𝐶′𝑘
(
𝑡 , ˆ𝑀(

Σ𝑝
)

h

𝑡

)
𝑡
)𝑝′ i

ℝ𝑑, 𝐶′ = 𝐶1 +

∈
𝑢⊤Σ𝑝𝑢

,

)

1
𝜏1
/
−

𝑘

Σ𝑝′ ,
)
𝑡, 𝑀(
𝑢 ⊗

𝑡

)𝑝 i

6

h

𝐶2 and 𝑡 6 𝑘:

,

19

3.6 Diﬀerential Privacy

In this section, we state a few tools from diﬀerential privacy (DP) literature that will be used in our
algorithms. We start by recalling the deﬁnition of DP:

Deﬁnition 3.24 (Diﬀerential Privacy [DMNS06]). An algorithm
diﬀerentially private (or
𝑌, 𝑌′, we have

-DP) for 𝜀, 𝛿 > 0 iﬀ, for every 𝑆
)

𝜀, 𝛿

⊆ 𝒪

(

:

-
ℳ
)
and every neighboring datasets

is said to be

𝒴 → 𝒪

𝜀, 𝛿

(

ℙ

𝑌

𝑆

6 𝑒 𝜀

ℙ

𝑌′

𝑆

𝛿.

·

]

) ∈

) ∈

] +

[ℳ(

[ℳ(
Throughout this work, our set 𝑌 will consist of 𝑦1, . . . , 𝑦𝑛

are neighbors iﬀ they diﬀer on a single data point, i.e., 𝑦′𝑗

and 𝑌′ =
= 𝑦𝑗 for all 𝑗 ≠ 𝑖. Note that
𝑦′1, . . . , 𝑦′𝑛)
(
this is the so-called substitution variant of DP; another popular variant is the add/remove DP where
a neighboring 𝑌′ results from adding or removing an example from 𝑌. We remark that it is not
𝑛 of
hard to extend our algorithm to the add/remove DP setting, by ﬁrst computing a DP estimate
ˆ
𝑛 and either throwing away random elements or adding zero vectors to arrive at an 𝑛-size dataset
on which our algorithm can be applied.

ℝ𝑑. 𝑌 =

𝑦1, . . . , 𝑦𝑛

∈

(

)

3.6.1 Laplace Mechanism and Its Variants

The Laplace mechanism [DMNS06] is among the most widely used mechanisms in diﬀerential
privacy. It works by adding a noise drawn from the Laplace distribution (deﬁned below) to the
output of the function one wants to privatize.

Deﬁnition 3.25 (Laplace Distribution). The Laplace distribution with mean 𝜇 and parameter 𝑏 on
𝑏.
ℝ, denoted by Lap

𝜇

𝑥

−

|/

𝜇, 𝑏
(

, has the PDF 1
)

2𝑏 𝑒−|

We will also use the “truncated” version of the Laplace mechanism where the noise distribution
is shifted and truncated to be non-negative. The precise deﬁnition of the noise distribution and
its guarantee is given below. For completeness, we provide the DP analysis (Lemma 3.27) in
Appendix A.1.

Deﬁnition 3.26 (Truncated Laplace Distribution). The (negatively) truncated Laplace distribution
with mean 𝜇 and parameter 𝑏, denoted by tLap
conditioned on the
value being negative.

𝜇, 𝑏
is deﬁned as Lap
(

𝜇, 𝑏
(

)

)

Lemma 3.27 (Truncated Laplace Mechanism). Let 𝑓 :
Δ. Then the algorithm that adds tLap

Δ

1

ln

(

𝒴 →
𝜀

, Δ

1
𝛿
)𝜀
/

−

+

to 𝑓 satisﬁes

𝜀, 𝛿

-DP.
)

(

/

ℝ be any function with sensitivity at most

(cid:17)
Finally, we also state a bound on the tail probability of the truncated Laplace distribution which

(cid:16)

(cid:17)

(cid:16)

will be useful in our subsequent analysis.

Lemma 3.28. Suppose 𝜇 < 0 and 𝑏 > 0. Let 𝑋

𝜇, 𝑏
tLap
(

. Then, for 𝑦 < 𝜇, we have that
)

∼

𝑒(
2

𝑦

𝜇

𝑏

−
)/
𝑒𝜇

/

.

𝑏

−

𝑋 < 𝑦

ℙ

[

=

]

20

3.6.2 Composition Theorem

It will be convenient to also consider DP algorithms whose privacy guarantee holds only against
subsets of inputs. Speciﬁcally, we deﬁne:

Deﬁnition 3.29 (Diﬀerential Privacy Under Condition). An algorithm
𝜀, 𝛿
(
𝑆

-diﬀerentially private under condition Ψ (or
)
⊆ 𝒪

and every neighboring datasets 𝑌, 𝑌′ both satisfying Ψ, we have

is said to be
:
-DP under condition Ψ) for 𝜀, 𝛿 > 0 iﬀ, for every
)

𝒴 → 𝒪

𝜀, 𝛿

ℳ

(

ℙ

𝑌

𝑆

]

) ∈

6 𝑒 𝜀

·

ℙ

[ℳ(

𝑌′

𝛿.

𝑆

] +

) ∈

[ℳ(

It is not hard to see that an analogue of the basic composition theorem still holds in this setting,
which we formalize below. We remark that this is similar to the composition theorem derived
in [DL09, Section 5]. However, since our composition theorem is slightly diﬀerent, we provide its
proof in Appendix A.2.

Lemma 3.30 (Composition for Algorithm with Halting). Let

, . . . ,

𝑘 :

𝑘
𝒪2 ∪ {⊥}
proceeds as follows (with 𝑜0 being empty): For 𝑖 = 1, . . . , 𝑘, compute 𝑜𝑖 =
and output

. Finally, if the algorithm has not halted, then output 𝑜𝑘.

1 × 𝒴 → 𝒪
−

∪ {⊥}

ℳ

𝒪

𝑘

be algorithms. Furthermore, let

ℳ1 :

,

𝒴 → 𝒪1 ∪ {⊥}
ℳ
𝑜𝑖
𝑖
(

ℳ2 :
𝒪1 × 𝒴 →
denote the algorithm that
and, if 𝑜𝑖 =
, halt
1, 𝑌
−

ℳ

⊥

)

⊥

Suppose that:

• For any 1 6 𝑖 < 𝑘, we say that 𝑌 satisﬁes the condition Ψ𝑖 if running the algorithm on 𝑌 does not
𝑖.

result in halting after applying

ℳ1,

ℳ2, . . . ,

ℳ

-DP.

•

•

𝜀1, 𝛿1)
(
𝜀𝑖 , 𝛿𝑖

ℳ1 is
𝑖 is
ℳ
for all 𝑖 =

(

{

}

-DP (with respect to neighboring datasets in the second argument) under condition Ψ𝑖
)
2, . . . , 𝑘

.

1
−

Then,

is

ℳ

𝜀𝑖 ,

𝑖

𝑘

]

∈[

𝑖

𝑘

]

∈[

(cid:16)Í

Í

𝛿𝑖

-DP.

(cid:17)

3.6.3 Hockey-Stick Divergence

It will be convenient in our analysis to use an equivalent deﬁnition of DP based on the hockey-stick
𝑎, 0
divergence. For ease of notation, let

for all 𝑎

= max

ℝ.

𝑎

[

]+

{

∈

}
𝑥

Deﬁnition 3.31 (Hockey-Stick Divergence). Let 𝑝
and 𝛼 a non-negative real number. The Hockey-stick divergence 𝐷𝛼(
as:

, 𝑞

𝑥

(

(

)

)

)

be probability density functions on ℝ𝑑,
between 𝑝, 𝑞 is deﬁned

𝑝, 𝑞

The following fact is simple to derive from the deﬁnition of DP and is often used in literature.

𝑝, 𝑞

𝐷𝑒 𝜀

(

=

)

ℝ𝑑 [

∫𝑥

∈

𝑝

𝑥

(

) −

𝛼

𝑞

𝑥

(

·

)]+

𝑑𝑥 .

ℝ𝑑 be a randomized
-DP from Hockey-Stick Divergence Bounds). Let
Fact 3.32 (
𝜀, 𝛿
)
(
-DP under condition Ψ iﬀ for any neighboring pair of databases 𝑌, 𝑌′ both satisfying
𝜀, 𝛿
algorithm.
is
)
(
ℳ
Ψ, we have 𝐷𝑒 𝜀
,
𝑌
)
(ℳ(

𝑌′))

𝒴 →

6 𝛿.

ℳ(

ℳ

:

21

We will need to bound the hockey-stick divergence between two distributions in terms of the
hockey-stick divergences to a third distribution. Unfortunately, the hockey-stick divergence does
not deﬁne a metric and, therefore, does not admit the usual triangle inequality. However, it is
possible to prove a looser inequality, which we will ﬁnd useful:

Lemma 3.33. Suppose 𝑝

, 𝑞

𝑥

(

)

𝑥

, 𝑟

𝑥

are probability density functions on ℝ𝑑. Then,

)
(
𝐷𝑒 𝜀

(
)
𝑝, 𝑟

6 𝐷𝑒 𝜀

𝑝, 𝑞

2

𝑒 𝜀

2
/

𝐷𝑒 𝜀

2

𝑞, 𝑟

.

) +
We remark that such a bound is already implicit in the so-called group diﬀerential privacy (see

(

)

(

(

)

·

/

/

e.g. [Vad17, Lemma 2.2]). Nonetheless, we provide a (short) proof in Appendix A.3.

3.6.4 Approximate-DP Selection

Finally, we will also use a DP algorithm for the selection problem, where the goal is to pick from
a (public) set of candidates one which has a high “score”. This problem can be solved using the
exponential mechanism [MT07]. The version of the algorithm we use deviates slightly from this
traditional version in that we also include a check (via truncated Laplace mechanism) to make sure
that the score is at least a certain threshold 𝜅; otherwise, the algorithm’s properties are summarized
below. Its proof is deferred to Appendix A.4.

Theorem 3.34. Suppose 𝜀, 𝛿
function for candidates as a function of the databases 𝑌
There exists an algorithm Selection that satisﬁes the following properties:

be a set of candidates and let score :

be a scoring
, such that its sensitivity (w.r.t. 𝑌) is at most Δ.

0, 1
]

𝒞 × 𝒴

. Let

∈ 𝒴

∈ (

𝒞

1. Selection is

𝜀, 𝛿

-DP.
)

(

2. If the output of Selection is 𝑐∗ ≠

⊥

, then score
(
> 𝜅

𝑐, 𝑌

𝑐∗, 𝑌

> 𝜅.

)

3. If there exists 𝑐

∈ 𝒞
probability at most 𝛽.

such that score
(

)

𝑂

+

Δ
𝜀 ·

(cid:16)

log

|𝒞|𝛽𝛿

, then Selection output

(cid:16)

(cid:17) (cid:17)

with

⊥

4 Diﬀerentially Private Robust Moment Estimation

In this section, we describe a diﬀerentially private robust moment estimation algorithm. The
following is our main technical result:

ℕ. Then, there
Theorem 4.1 (Diﬀerentially Private Robust Moment Estimation). Fix 𝐶0 > 0 and 𝑘
exists an 𝜂0 > 0 such that for any given outlier rate 0 < 𝜂 6 𝜂0 and 𝜀, 𝛿 > 0, there exists a randomized
𝑑4𝑘
algorithm Alg that takes an input of 𝑛 > 𝑛0 =
𝜂2

𝐶4𝑘 𝑘4𝑘

points

1
𝛿
)𝜀
/

1
𝛿
)𝜀
/

6
+

2𝑘
𝑘
1

Ω

∈

1

ln

ln

4

−

(

(

ℚ𝑑 (where 𝐶 = 𝐶0 +

+
(cid:18)
(cid:16)
𝐵𝑛
1), runs in time
𝑌
e
(
ℚ𝑑, ˆ
Σ
entries of 𝑌) and outputs either “reject” or estimates
𝑡 < 2𝑘 such that 𝑡 divides 2𝑘) with the following guarantees2:

9
𝜀 +

3
(
/
𝜀

𝜇
ˆ

3 ln

⊆

+

∈

(cid:18)

𝛿

)

)
∈

+

·
(cid:19)
(cid:17)
(cid:16)
) (where 𝐵 is the bit complexity of the
𝑑 (for all even
𝑑, and ˆ𝑀(

ℚ𝑑

×···×

(cid:19)

(cid:17)

×

×

𝑑

𝑡

𝑂

𝑘

(
ℚ𝑑

) ∈

2The

Ω notation hides multiplicative logarithmic factors in 𝑑, 𝐶, 𝑘, 1
𝜂, 1
/
/

𝜀, and ln
(

1
/

𝛿

)

.

e

22

1. Privacy: Alg is

𝜀, 𝛿

-diﬀerentially private with respect to the input 𝑌, viewed as a 𝑑-dimensional
)

(

database of 𝑛 individuals.

∩

>

𝑋

2. Utility: Suppose there exists a 2𝑘-certiﬁably 𝐶0-subgaussian set 𝑋

ℚ𝑑 of 𝑛 > 𝑛0 points such that
𝑡
for 2 6 𝑡 6 𝑘.
, covariance Σ
𝑛 with mean 𝜇
𝑌
𝜂
)
|
∗
10 over the random choices of the algorithm, Alg outputs estimates
Then, with probability at least 9
/
𝑑 (for all even 𝑡 < 2𝑘 such that 𝑡 divides 2𝑘) satisfying the
ℚ𝑑
ℚ𝑑
ℚ𝑑, ˆ
Σ
𝜇
ˆ
following guarantees:

)𝐼, and 𝑡-th moments 𝑀(
∗

𝑑, and 𝑀(

) ∈

1
(

∗ (cid:23)

×···×

2−

poly

⊆

−

∈

∈

×

×

)

𝑑

𝑑

|

(

𝑡

ℝ𝑑 ,

𝑢

∀

∈

𝜇
h ˆ

−

, 𝑢

𝜇

∗

i

and,

Σ
∗ (cid:22) ˆ
((
and, for every even 𝑡 < 2𝑘 such that 𝑡 divides 2𝑘,

1
𝜂1
/
−
)

2𝑘
/

𝐶 𝑘

𝑂

Σ

−

1

(cid:16)

(cid:17)

)

𝑘

𝑡

6 𝑂

√𝐶 𝑘

2𝑘
1
𝜂1
/
−

𝑢⊤Σ
∗

𝑢 ,

(cid:17)

1

𝑂

((

+

𝐶 𝑘

(cid:16)

(cid:22)

(cid:16)

p
𝑡

2𝑘
/

)

𝑘

1
𝜂1
/
−
)

,

Σ
∗

(cid:17)

1

𝑂

(

−

𝐶 𝑘

𝑡

2𝑘
/

𝜂1
−
)

𝑢 ⊗

h

𝑡, 𝑀(

𝑡
)
∗ i

6

𝑢 ⊗

𝑡

𝑡 , ˆ𝑀(

)

h

i

6

1

𝑂

(

+

𝐶 𝑘

𝑡

2𝑘
/

𝜂1
−
)

𝑢 ⊗

h

𝑡 , 𝑀(

𝑡
)
∗ i

.

(cid:16)

(cid:17)

(cid:16)

(cid:17)

Moreover, the algorithm succeeds (i.e., does not reject) with probability at least 9
/
of the algorithm.

10 over the random choices

Observe that the privacy guarantees of the algorithm are (necessarily) worst-case. The utility

guarantees, however, hold only under the assumption that 𝑌 is an 𝜂-corruption of a good set 𝑋.

The above theorem can also be translated into utility guarantees for points sampled from a given
distribution by recalling the well-known fact that points sampled from a certiﬁably subgaussian
distribution are good with high probability:

Fact 4.2 (See Section 5 in [KS17b]). Suppose
poly
covariance Σ
(
be an i.i.d. sample from
probability at least 0.99 over the draw of 𝑋, the following all hold:

)𝐼 and 𝑡-moment tensors 𝑀(
of size 𝑛 > 𝑛0 = 𝑂
𝑑2𝑘

) for 𝑡
𝜂2
/

∗ (cid:23)

2−

𝒟

𝒟

∈

(

𝑑

𝑡

. Then, for any 𝑡
)

∈

ℕ. For any 𝑘

ℕ, let 𝑋 =

𝑥1, 𝑥2, . . . , 𝑥𝑛

∈
}
{
ℕ such that 𝑡 divides 𝑘, with

∗

is a certiﬁably 𝐶-subgaussian distribution with mean 𝜇

and

1. 𝑋 is 2𝑘-certiﬁably 2𝐶-subgaussian.

2.

2
1
Σ−
/
∗

𝑋

𝜇
(

(

) −

𝜇

6 𝜂.

2

∗)
(cid:13)
(cid:13)
(cid:13)

.

(cid:13)
(cid:13)
3. Σ
(cid:13)
(
𝑣
2𝑘

4.

𝑋

1
) ∈ (

±
𝑡 , 𝑀(

𝜂

Σ
)
∗
𝑡

𝑋

)(

𝑣⊗

h

n

1
)i ∈ (

±

𝜂

)h

𝑣⊗

𝑡 , 𝑀(

𝑡
)
∗ i

.

o

We note that our main theorem for private robust moment estimation, Theorem 1.2, is an

immediate consequence of Theorem 4.1 and Fact 4.2.

For the rest of the section, we will work to prove Theorem 4.1. In Section 4.1, we will introduce
a witness-producing robust moment estimation algorithm that will be used as a subroutine for our
main algorithm and present relevant utility guarantees. In Section 4.2, we will then introduce our
main algorithm. After that, we will prove the necessary privacy guarantees in Section 4.3. Finally,
we will put together the pieces to prove our main theorem, Theorem 4.1, in Section 4.4.

23

4.1 Witness-Producing Version of Robust Moment Estimation Algorithm

𝒜

As a key building block, we will use the following (non-private) version of the robust moment
as in [KS17b]. Our
estimation algorithm of [KS17b] that uses the same constraint system
algorithm itself, however, makes one key change (we call our version “witness-producing” for
reasons that will soon become clear) to that of [KS17b] in order to obtain a private robust moment
estimation algorithm. Instead of outputting estimates of the moments of the unknown distribution,
our algorithm outputs a sequence of non-negative weights 𝑝1, 𝑝2, . . . , 𝑝𝑛 forming a probability
distribution on the input set of points 𝑌. The estimates can then be obtained by taking moments
of the ﬁnite set 𝑌 with respect to the probability distribution on 𝑌 deﬁned by the weights 𝑝𝑖s. This
simple change is crucial to our worst-case analysis of the resulting algorithm (i.e. even when the
distributional assumption that 𝑌 is an 𝜂-corruption of some good set 𝑋 is not met) and obtaining
our privacy guarantees. As we discuss, our blueprint for modifying convex optimization based
robust estimation algorithms appears to broadly applicable beyond the speciﬁc setting of robust
moment estimation.

The underlying constraint system

is shown below, and the witness-producing robust moment

estimation algorithm is shown as Algorithm 4.3.

𝒜

𝐶,𝑘,𝜂,𝑛

𝒜

1. 𝑤2
𝑖

𝑦1, 𝑦2, . . . , 𝑦𝑛

({
= 𝑤𝑖 for each 1 6 𝑖 6 𝑛,

})

: Constraint System for 𝜂-Robust Moment Estimation

2.

𝑛

𝑖=1 𝑤𝑖 >

3. 𝜇′ = 1
Í
𝑛

𝑛,

𝜂

)

−

1
(
𝑖 𝑥′𝑖,

= 0 for 1 6 𝑖 6 𝑛,

4. 𝑤𝑖

(

5. 1
𝑛

Í
𝑦𝑖
𝑥′𝑖 −

)
𝑥′𝑖 −

𝑛
𝑖=1h

𝑘 6

𝜇′, 𝑣

i

𝐶 𝑘

(

𝑘

2
/

)

1
𝑛

𝑛
𝑖=1 h

𝑥′𝑖 −

𝜇′, 𝑣

2

i

Í

(cid:0)

Í

𝑘

2
/

.

(cid:1)

Algorithm 4.3 (Witness-Producing Robust Moment Estimation).

Given: A set of points 𝑌 =

𝑦1, 𝑦2, . . . , 𝑦𝑛

{

} ⊆

ℚ𝑑, 𝜂 > 0, a parameter 𝑘

ℕ.

∈

Output: Either “reject” or non-negative weights 𝑝1, 𝑝2, . . . , 𝑝𝑛 s.t. 𝑝𝑖 6 1
1
𝜂
−

(

𝑛 ∀

)

𝑖 and

𝑖 𝑝𝑖 = 1.

Operation:

Í

1. Find a pseudo-distribution ˜𝜁 of degree 𝑂

𝑘
(
. If such a pseudo-distribution does not exist, then return “reject.”
)

() satisfying the constraint system

𝐶,𝑘,𝜂,𝑛

𝒜

𝑌
(

)

2. Output weights 𝑝

0, 1
]

∈ [

𝑛 deﬁned by 𝑝𝑖 =

𝔼

𝑛
𝑖=1
e

𝑤𝑖]
˜𝜁[
𝔼
˜𝜁[

𝑤𝑖]

for each 𝑖.

Analysis of the witness-producing robust estimation algorithm Robust estimation algorithms
that rely on the use of semideﬁnite programming are all analyzed under distributional assumptions

Í

e

24

on the input set of points. Roughly speaking, such algorithms search over set of points that have a
large enough intersection with the input corrupted sample and satisfy certain relevant property of
the underlying family of distributions. In order to obtain privacy guarantee that holds for worst-
case inputs, we need to upgrade the analyses of such algorithms so that they not only provide
estimates of the target parameters, but also explicitly produce “witnesses”—these are subsets of
the input corrupted sample that deﬁne distributions with the estimated parameters and further,
satisfy the relevant property of the underlying family of distributions.

In this section, we verify that such a stronger guarantee can be obtained for robust moment
estimation algorithm of [KS17b]. Formally, their algorithm succeeds as long as the input is an
𝜂-corruption of a certiﬁably subgaussian set.

The following guarantees for the algorithm above were shown in [KS17b].

Fact 4.4 (Lemmas 4.4, 4.5, and 4.8 in [KS17b]). Let 𝑋
𝐶-subgaussian with mean 𝜇
𝜂-corruption of 𝑋. Then, for 𝜇′ = 1
𝑛

, covariance Σ
∗

𝑖 𝑥′𝑖, Σ′ = 1

𝑛

∗

⊆

𝑡

)

ℝ𝑑 be a set of size 𝑛 that is 2𝑘-certiﬁably
for 𝑡 evenly dividing 2𝑘. Let 𝑌 be an
𝑡, we have:
𝜇′)⊤, and 𝑀(
,

)′ = 1
𝑛

𝑖 𝑥′𝑖 ⊗

𝑢 𝑘

Í

𝑡

and 𝑡-th moment 𝑀(
∗
𝑥𝑖
−
𝑢⊤Σ
∗

𝑖(
−
2𝑘 6 𝑂
Í

𝜇′)(
𝐶 𝑘 𝑘 𝑘

, 𝑢

𝑥𝑖

∗

𝜇

i
2

(
𝑘 6 𝑂

)
𝐶 𝑘 𝑘 𝑘

(
𝑡 6 𝑂

2𝑘

/

,

(cid:9)
𝑢 𝑘

)
𝐶 𝑘 𝑘 𝑘

𝑢⊤Σ
∗
(cid:9)
𝑢⊤Σ
∗

𝑢 𝑘

.

−

𝑢
𝒜 2𝑘
𝑢
𝒜 2𝑘
𝑢
𝒜 2𝑘

h

Í

𝜇′

h
(cid:8)
Σ′
h
𝑡
(cid:8)
𝑀(

)′

Σ
∗

−

𝑀(
∗

, 𝑢 ⊗

i
𝑡
, 𝑢 ⊗

𝑡

)

−

i
Lemma 4.5 (Guarantees for Witness-Producing Robust Moment Estimation Algorithm). Given a
𝑘
subset of of 𝑛 points 𝑌
(
)
returns a sequence of weights 0 6 𝑝1, 𝑝2, . . . , 𝑝𝑛 satisfying
and either (a.) outputs “reject,” or (b.)
𝑝𝑛 = 1.
𝑝1 +

ℚ𝑑 whose entries have bit complexity 𝐵, Algorithm 4.3 runs in time

𝐵𝑛

⊆

n

o

𝑂

(

)

(

)

𝑡

ℝ𝑑 is 2𝑘-certiﬁably 𝐶-subgaussian with mean 𝜇
𝜂
−
𝑦𝑖
(

, covariance Σ
and in general, 𝑡-th
∗
𝑛, then Algorithm 4.3 never rejects, and the corresponding
)⊤ satisfy the following guarantees for 𝛽𝑡 =

⊆
)∗ such that
𝑌
|
∩
Σ =
𝑖 𝑝𝑖 𝑦𝑖 and ˆ

>
1
(
𝑛
𝑖=1 𝑝𝑖

)
𝜇
− ˆ

𝜇
− ˆ

𝑦𝑖

𝑋

)(

|

∗

𝑝2 + · · · +
Moreover, if 𝑋
moment tensor 𝑀(
estimates
2𝑘𝑡
𝐶 𝑡
𝑂
/

𝜇 = 1
𝑛
ˆ
𝑡
𝜂1
2
−
/
)

(

2𝑘 for 𝑡 6 𝑘:
/
Í

Í

1. Mean Estimation:

𝑢

∀

∈

ℝ𝑑 ,

𝜇
h ˆ

−

, 𝑢

𝜇

∗

i

6 𝑂

√𝐶 𝑘
(

2𝑘
1
𝜂1
/
−
)

𝑢⊤Σ
∗

𝑢 ,

2. Covariance Estimation:

p

1
(

−

Σ
𝛽2)

Σ
∗ (cid:22) ˆ

1
(cid:22) (

+

Σ
𝛽2)
∗

,

3. Moment Estimation: For all even 𝑡 < 2𝑘 such that 𝑡 divides 2𝑘,

∈
4. Witness: For 𝐶′ 6 𝐶

∀

𝑢

ℝ𝑑 ,

𝛽𝑡

𝑢 ⊗

𝑡 , 𝑀(

𝑡
)
∗ i

6

𝑢 ⊗

𝑡

𝑡 , ˆ𝑀(

)

h

i

6

1
(

+

𝛽𝑡

)h

𝑢 ⊗

𝑡 , 𝑀(

𝑡
)
∗ i

1
(

𝑂

)h

−
1
𝜂1
/
−
(
𝑛

𝑘

,
))

1
(

+

1
𝑛





𝑝𝑖

𝑦𝑖

h

𝜇
− ˆ
i

2𝑘 6

𝐶′𝑘

(

𝑘

)

Õ𝑖=1

25

1
𝑛

𝑛

Õ𝑖=1

𝑝𝑖

𝑦𝑖

h

𝜇
− ˆ

i

𝑘

2

!





 
The ﬁrst three properties follow easily from an analysis similar to the one in [KS17b]. We verify

the last property below.

1
/

𝒜
𝑘. Suppose there exists a 2𝑘-certiﬁably 𝐶1-subgaussian distribution 𝑋

Lemma 4.6. Let ˜𝜁 be a pseudo-distribution of degree 𝑂
𝜂
≪
such that
𝑌
|
where 𝑊 =

−
, we have:

𝑛. Then, for 𝜂 6 𝜂0 for some absolute constant 𝜂0 and for

consistent with

on input 𝑌 with outlier rate
of size 𝑛
𝑦𝑖
𝑤𝑖

ℝ𝑑 with mean 𝜇
𝜇 = 1
𝑊
ˆ

𝑛
𝑖=1

∗
𝔼

˜𝜁[

𝔼

>

∩

⊆

𝜂

𝑘

]

(

)

)

1
(
𝑤𝑖

𝑋
|
𝑛
𝑖=1

[

]

e

Í

𝑢
2𝑘

1
𝑊

𝑛

Õ𝑖=1



2𝑘
1
𝜂1
/
−
(


)

for 𝐶′ 6 𝐶

𝑂

1
(

+

Proof. We have:

𝔼

𝑤𝑖

˜𝜁[

]h

𝑦𝑖

𝜇, 𝑢

− ˆ

i

2𝑘 6

𝐶′𝑘

(

𝑘

)

e
𝑘

)

6 𝐶

+

1 for small enough 𝜂.

1
𝑊

𝑛

Õ𝑖=1

𝔼

𝑤𝑖

˜𝜁[

]h

𝑦𝑖

𝜇, 𝑢

− ˆ

e

e

Í

,

𝑘

2

i

!





1
𝑛

𝑛

Õ𝑖=1

e

𝔼

𝑤𝑖

˜𝜁[

]h

𝑦𝑖

𝜇, 𝑢

− ˆ

i!

2𝑘

=

1
𝑛

𝑛

Õ𝑖=1

e

𝔼

𝑤𝑖

𝜇, 𝑢

𝑥′𝑖 − ˆ

h

i

˜𝜁[

2𝑘

6 1
𝑛

]

𝑛

𝔼

𝑥′𝑖 −

˜𝜁[h

𝜇′

+

𝜇′

𝜇, 𝑢

− ˆ

2𝑘

i

]

The ﬁrst term on the right-hand side above is at most
𝑂

𝑥′𝑖 −
𝑢 𝑘 using certiﬁable subgaussianity constraints and Fact 4.4.

˜𝜁[(

𝐶 𝑘

)

(

2𝑘
𝜂1
1
/
−
(
Let us analyze the 2nd term above.

𝑘 𝑢⊤Σ
∗

𝑘

)

)

𝑛
𝑖=1 h

Í

𝜇′, 𝑢

2

𝑘

i

)

]

6

𝐶

1
(

(

+

Õ𝑖=1
1
𝑛

e

𝑘

𝔼

𝔼

𝑥′𝑖 −

˜𝜁[h

𝜇′

+

𝜇′

𝜇, 𝑢

− ˆ

2𝑘

i

]

e

𝑛

𝑥′𝑖 −

˜𝜁[h

𝜇′, 𝑢

2𝑘

i

] +

2𝑘

1
𝑛

𝔼

𝑥′𝑖 −

˜𝜁[h

𝜇′, 𝑢

2𝑘

2
−

𝜇′

h

𝜇, 𝑢

− ˆ

2

i

i

1
𝑛

𝑛

Õ𝑖=1

𝔼

𝑥′𝑖 −

˜𝜁[h

𝜇′, 𝑢

2𝑘

i

𝜇, 𝑢

− ˆ

𝑗

i

]

e
𝜇′, 𝑢
𝑥′𝑖 −

˜𝜁[h

2𝑘

i

] +

2𝑘

1
𝑛

𝔼
(

𝑥′𝑖 −

˜𝜁[h

𝜇′, 𝑢

2𝑘

(
)

i

2𝑘

2𝑘

2
)/
−

𝔼
(

˜𝜁 h

𝜇′

𝜇, 𝑢

− ˆ

2𝑘

2𝑘
1
/
)

i

𝔼

𝑥′𝑖 −

˜𝜁[h

𝜇′, 𝑢

2𝑘

i

−

h

𝜇, 𝑢

− ˆ

𝑗

i

]

e

Õ𝑖=1
𝑗

−

e
𝜇′

h

𝑛

Õ𝑖=1
𝑗

e
𝜇′

𝑛

Õ𝑖=1
1
𝑛

2𝑘

1
𝑛

=

+

6 1
𝑛

2𝑘

𝑛
e

𝔼

Õ𝑖=1
e
2𝑘
𝑗

Õ𝑗=2 (cid:18)
𝑛

(cid:19)

𝔼

Õ𝑖=1
e
2𝑘
𝑗

(cid:19)

1
𝑛

𝑛

Õ𝑖=1

+

Õ𝑗=2 (cid:18)

e

(4.1)

(4.2)

(4.3)

(4.4)

(4.5)

Here, in the 2nd inequality, we used the Hölder’s inequality for pseudo-distributions. Let us
analyze the 2nd term in the right-hand side above by observing the following that uses the bounds
from Fact 4.4:

𝔼
𝜇′
˜𝜁[h
6 22𝑘
e

(

𝜇, 𝑢
i
𝑘𝜂2𝑘

− ˆ
𝐶 𝑘

)

𝔼

2𝑘

6 22𝑘
[
𝑢 𝑘

]
1𝑢⊤Σ
−
∗

+
e

𝜇′
˜𝜁[h
22𝑘

−
𝐶 𝑘

𝜇

2𝑘

, 𝑢
i
∗
𝑘𝜂2𝑘
1
−

22𝑘
𝜇
∗ − ˆ
h
𝑘 𝑢⊤Σ
𝛽2)
∗

𝜇, 𝑢
𝑢 𝑘

] +
1
(

+

2𝑘

i

(4.6)

(4.7)

)

(

26

 
 
This allows us to infer that the 2nd term in (4.5) is at most
𝐶 𝑘

𝑛
𝑖=1h
·
𝑢 𝑘 using certiﬁable subgaussianity constraints and

𝑢 6 𝑂

𝑥′𝑖 −

𝜇′, 𝑢

2
)/
−

˜𝜁[

](

𝔼

1
𝑛

2𝑘

2𝑘

2𝑘

𝑘

i

2𝑘 √𝑢⊤Σ
1
2𝜂1
1
5𝐶 𝑘
/
−
/
(
)
∗
Fact 4.4.

(

)(

2𝑘 𝑢⊤Σ
𝑘𝜂1
1
/
−
∗

)

Í

e

Let’s now analyze the terms corresponding to 𝑗 > 2 in the right-hand side of (4.5). Each of
. Let us ﬁrst analyze

and

𝜇, 𝑢

𝜇′, 𝑢

these terms corresponds to a “mixed monomial” in
the even individual degree terms.

𝑥′𝑖 −

h

𝜇′ −

h

i

i

First observe that by Hölder’s inequality for pseudo-distributions again, we have:

1
𝑛

𝑛

Õ𝑖=1

e

𝔼

𝑥′𝑖 −

˜𝜁[h

𝜇′, 𝑢

2𝑘

2
−

𝜇′

h

𝜇, 𝑢

− ˆ

2

i

]

i

6

𝑥′𝑖 −

h

𝜇′, 𝑢

𝑘

𝑘

1
)/
−

2𝑘

(
)

i

𝔼
(

˜𝜁 h

𝜇′

𝜇, 𝑢

− ˆ

2𝑘

𝑘 .

1
/
)

i

(4.8)

By an analysis similar to the case of the ﬁrst term on the right-hand side of (4.5) above, we obtain
2𝑘
1
𝜂1
that the right-hand side is at most: 𝑂
/
−
(

)
Next, let’s analyze all terms corresponding to even 𝑗. By Proposition 3.9, we have:

2𝑢⊤Σ
)
∗

1
)(
(

𝑢 𝑘.

𝐶 𝑘

𝑘

e

1
𝑛

𝑛

Õ𝑖=1

e

𝑛

𝔼
(
Õ𝑖=1
𝑛

e
𝔼
(

Õ𝑖=1

6 2𝑘
𝑛

𝔼

𝑥′𝑖 −

˜𝜁[h

𝜇′, 𝑢

2𝑘

2𝑗

−

𝜇′

h

𝜇, 𝑢

− ˆ

2𝑗

i

]

i

6 1
𝑛

𝜇′

˜𝜁[h

𝜇, 𝑢

− ˆ

2

i

𝑥′𝑖 −

h

𝜇′, 𝑢

2𝑘

2𝑗

−

𝜇′

h

𝜇, 𝑢

− ˆ

i

2𝑗

2
−

]

i

𝜇′

˜𝜁[h

𝜇, 𝑢

− ˆ

2

i

(h

𝑥′𝑖 −

𝜇′, 𝑢

2𝑘

2
−

i

𝜇′

𝜇, 𝑢

− ˆ

i

+ h

2𝑘

2
−

)]

e
The ﬁrst term can now be upper bounded by the bound for (4.8) and the 2nd term by an application
of Fact 4.4.

The case of odd terms is similar with the ﬁrst step using Proposition 3.10.
Altogether, we obtain an upper bound of
+
On the other hand, using the sum-of-squares version of the Cauchy-Schwarz inequality along

2𝑘
1
𝜂1
/
−
(

𝑘 𝑢⊤Σ
∗

𝑢 𝑘.

1
(

𝑂

𝐶

𝑘

)

)

(

with the almost triangle inequality and invoking Fact 4.4 we have:

𝑢
𝒜 2𝑘

1
𝑛

(  

6 16𝜂𝐶2

𝑛

1
(
Õ𝑖=1
1
𝑛

𝑛

Õ𝑖=1

𝑤𝑖

𝜇, 𝑢

𝑥′𝑖 − ˆ

i

)h

−

𝑥′𝑖 −

h

𝜇′, 𝑢

2

i

+

2

2

6

!

1
𝑛

𝑛

Õ𝑖=1

1
𝑛

𝑛

Õ𝑖=1

1
(

−

𝑤𝑖

2
)

!

2

1
𝑛

𝑛

Õ𝑖=1

𝜇, 𝑢

𝑥′𝑖 − ˆ

h

i

4

𝜇′

h

𝜇, 𝑢

− ˆ

2

i

!

6 20𝜂𝐶2

1
(

+

𝛽2)

2𝑢⊤Σ
∗

𝑢2

)

Thus,

𝑢
𝒜 2𝑘

1
𝑛

( 

𝑛

Õ𝑖=1

𝔼

𝑤𝑖

𝑦𝑖

h

˜𝜁[

𝜇, 𝑢

− ˆ

2

i

!

2

=

1
𝑛

𝑛

Õ𝑖=1

2

2

i

!

] −  

1
𝑛

𝑛

Õ𝑖=1

𝔼

𝑦𝑖

˜𝜁[h

𝜇, 𝑢

− ˆ

e

𝔼

1
˜𝜁[(

−

𝑤𝑖

𝑦𝑖

)h

𝜇, 𝑢

− ˆ

2

𝑛

2

i

!

e

>

1
(

−

𝑂

(

e
1
𝜂1
𝐶 𝑘
/
−
)

80𝜂𝐶2

𝑘

−

𝑢⊤Σ
∗

)

𝑢 .

)

The lemma now follows immediately for small enough ﬁxed constant 𝜂.

(cid:3)

27

 
 
 
4.2 Private Robust Moment Estimation

We are now ready to present our main algorithm for private robust moment estimation. Our
algorithm uses the witness-producing algorithm (Algorithm 4.3) as a major building block while
augmenting it to search for pseudo-distributions that, in addition to satisfying the relevant set
of constraints, also minimize an appropriate strongly convex potential function. We deﬁne the
relevant potential function Pot below in Deﬁnition 4.7.

2

𝒜
. Furthermore, let Pot𝐶,𝑘,𝑛

)

𝑌
(

𝜂

Deﬁnition 4.7 (Potential Function). Let 𝐶 > 0 and 𝑛, 𝑘
degree 2 consistent with

for outlier rate 𝜂 and input 𝑌

∈

𝐶,𝑘,𝜂,𝑛

ℕ. For any pseudo-distribution ˜𝜁 of
be deﬁned
𝜂, ˜𝜁

ℝ𝑑, let Pot𝐶,𝑘,𝑛

𝑌
(

)

as

𝔼

𝑤

˜𝜁[

]

2
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)e

)
the potential as ˜𝜁 ranges over all pseudo-distributions of degree 2𝑡 consistent with
𝑌
no such pseudo-distribution exists, set Pot𝜂(

.
∞
When 𝐶 , 𝑛, 𝑘 are understood from context, we may suppress these parameters and simply

. If
)

𝐶,𝑘,𝜂,𝑛

𝑌
(

𝒜

=

)

)

)

˜𝜁 sat

𝑌
𝒜𝐶 ,𝑘,𝜂,𝑛(

be the minimum value of

= min

𝑌
(

⊆
Pot𝐶,𝑘,𝑛
𝜂, ˜𝜁

𝑌
(

write Pot𝜂 and Pot𝜂, ˜𝜁.

Now, we are ready to describe our main private robust moment algorithm, which is listed as
Algorithm 4.8. The algorithm consists of three main steps. In the ﬁrst step, the randomized DP
selection algorithm (Theorem 3.34) is used to pick an outlier rate (according to a suitable scoring
function, as deﬁned below in Deﬁnition 4.12). The second step invokes the witness-producing
algorithm (Algorithm 4.3) with the outlier rate chosen in step 1, after which one checks that the
outputted weights induce a certiﬁably subgaussian distribution on the input dataset 𝑌. Finally, in
the last step, one takes the estimates of the mean, covariance, and higher moments provided by
the resulting weight vector and adds suitable noise to guarantee diﬀerential privacy.

Algorithm 4.8 (Private Robust Moment Estimation).

Given: A set of points 𝑌 =

𝑦1, 𝑦2, . . . , 𝑦𝑛

ℚ𝑑, parameters 𝐶 , 𝜂, 𝜀, 𝛿 > 0, 𝐿, 𝑘

ℕ.

∈

} ⊆

𝑡

) (3 6 𝑡 6 𝑘) for mean, covariance, and 𝑡-moments.

{
Σ, and ˆ𝑀(

𝜇, ˆ
ˆ

Output: Estimates

Operation:

1. Stable Outlier Rate Selection: Use the

2 to
with the scoring function as deﬁned in Deﬁnition 4.12.

-DP Selection with 𝜅 = 𝐿
3
)

3, 𝛿

𝜀

/

/

/

(

sample an integer 𝜏
If 𝜏 =

𝜂𝑛

]

∈ [

, then reject and halt. Otherwise, let 𝜂′ = 𝜏

⊥

𝑛.

/

𝑌
(

𝑌
and minimizing Pot𝜂′, ˜𝜁(

2. Witness Checking: Compute a pseudo-distribution ˜𝜁 of degree 2𝑘 satisfying
and

)
𝛾. Check that the weight vector 𝑝 =

𝐶,𝑘,𝜂′,𝑛
𝒜
𝐶′ = 𝐶
gaussian distribution on 𝑌. If not, reject immediately. Otherwise, let
Σ =

]
(for all even 𝑡 < 2𝑘 such that 𝑡 divides 2𝑘) be the

induces a 𝐶′-certiﬁably sub-
,

. Let 𝛾
)

∼
𝔼
˜𝜁[

, 3
/

, and

tLap

𝜇 =

3
(
/
𝜀

˜𝜁[

3 ln

𝔼

𝔼

𝔼

e

𝑤

+

−

+

𝜇

1

𝜀

(cid:16)

(cid:16)

(cid:17)

(cid:17)

]

𝛿

)

𝑡

𝑡

𝑀(

) =

𝑀(

Σ
]

˜𝜁[

˜𝜁[

)]

e

e

e

e

e

e

28

mean, covariance, and 𝑡th moment estimates, respectively, that are induced by the
pseudo-distribution ˜𝜁.

3. Noise Addition: Let 𝛾1 = 𝑂
(
𝑑 and 𝑍

1
2𝑘 ) and 𝛾𝑡 = 𝑂
𝐿
𝑛
𝐶′𝑘
2𝑘 ) for
2 (
−
/
)
)
1
0, 𝜎2)(
2 ), where we interpret 𝑍 as a symmetric
𝑑 matrix with i.i.d. entries in the upper triangular portion. Similarly, for 𝑡 > 2,
), where we interpret 𝑍 as a symmetric 𝑑
tensor

𝑡 > 2. Let 𝑧
𝑑
×
let 𝑍(

)(
∼ 𝒩(

0, 𝜎1)

1
2 (
−

∼ 𝒩(

0, 𝜎𝑡

𝐶′𝑘

2
/

𝑡
+(
𝑡

)(

((

𝑛

𝐿

𝑑

𝑑

/

1
)

)

−

+

𝑡

𝑡

𝑑

𝑑

) ∼ 𝒩(

)(

×

× · · ·

1

1

1

𝑡

independent “upper-triangular” entries. Moreover, let

|

{z

}

𝑡 times

with

𝑑

1
)
−

𝑡
𝑡

+(

(cid:0)

(cid:1)

(

Then, output:

•

•

•

𝜇 =
ˆ
Σ =
ˆ
𝑡
ˆ𝑀(

𝜇
Σ
e
) =
e

+

2𝑧.
Σ1
/
2𝑍
Σ1
/
+
e
𝑡
𝑀(
e
e

2.
Σ1
/
Σ
) + ((
e

+

e

e

e

4.3 Privacy Analysis

1

𝑡

−
2

𝜎𝑗 = 6𝑘𝜀−
𝜎𝑗 = 6𝑘𝜀−

1𝛾𝑗 𝑑
1𝛾𝑗

7.5𝑘
2 ln
(
𝑡
1
𝑡 𝑑
p
)

/
2 ln

−
2

𝛿

,

)
7.5𝑘
(

for 𝑗 = 1, 2

𝛿

,

)

/

for 𝑗 > 2

.

𝐶′𝑘

(

p

𝜇

𝜇𝑇

2
1
/
)

)⊗

𝑡

𝑡 𝑍(

), for all even 𝑡 < 2𝑘 such that 𝑡 divides 2𝑘.

Our analysis of the privacy of Algis based on a sequence of claims about each of the steps of Algthat
cumulatively establish the stability of the behavior of Algon adjacent inputs 𝑌, 𝑌′. We will rely on
the following simple but key observation in our analysis. It is easy to verify using the deﬁnition of
pseudo-distributions.

Lemma 4.9 (Adjacent Pseudo-distributions). Let ˜𝜁 be a pseudo-distribution of degree 2𝑘 that satisﬁes all
ℝ𝑑 be adjacent to 𝑌. Deﬁne
the constraints in 4.1 on input 𝑌 =
if
𝑤𝑆𝑝
an adjacent pseudo-distribution ˜𝜁′ (that “zeroes out 𝑤𝑖”) by
(
𝑖 ∉ 𝑆 and
= 0 if 𝑖
𝑆 for every polynomial 𝑝 in 𝑋′ and other auxiliary indeterminates
in
. Then, ˜𝜁′ is a pseudo-distribution of degree 2𝑘 that satisﬁes all the constraints in 4.1 on both inputs 𝑌′
and 𝑌 with outlier parameter 𝜂

with outlier rate 𝜂. Let 𝑌′ ⊆
𝔼
𝑋′,

𝑦1, 𝑦2, . . . , 𝑦𝑛

· · · )]

· · · )]

· · · )]

𝑤𝑆𝑝

𝑤𝑆𝑝

𝑋′,

𝑋′,

˜𝜁′[

˜𝜁′[

˜𝜁[

𝒜

𝑛.

𝔼

𝔼

e

e

e

=

∈

{

}

(

(

1
/

+

This allows us to conclude the following basic calculus of our potential function:

Lemma 4.10 (Basic Facts about Pot). Suppose that for some 𝑌
4
𝜂′ ∈ [
]
𝜂 > 𝜂′, the following holds:

, there is a pseudo-distribution of degree 2𝑡 consistent with

0, 𝜂0/

⊆

𝒜

ℝ𝑑 of size 𝑛, some 𝑡

ℕ and
on input 𝑌. Then, for every

∈

1. Monotonicity: Pot𝜂
subscript increases.

1
/
+

𝑌
𝑛(

)

6 Pot𝜂(
𝑌

.
)

In particular, Pot is monotonically decreasing as its

2. Lower Bound: Pot𝜂(
𝑌
3. Upper Bound: Pot𝜂(
𝑌

)

)

>

6

1
(
1
(

−

−

𝜂

2𝑛.
)
𝑛.

𝜂

)

29

         
         
𝑛
𝑖=1

Proof. The ﬁrst fact follows immediately from Lemma 4.9. For the second, observe that any
on input 𝑌 with outlier rate 𝜂 must satisfy
pseudo-distribution ˜𝜁 of degree 2𝑡 consistent with
2 >
𝑛 =
𝔼
[
6 1 for every 𝑖. Thus,
e
(cid:3)

𝑛
𝑖=1
˜𝜁[
2𝑛. This completes the proof. For the last part, observe that
𝜂
Í
)
e
𝔼
𝑛.

𝑛. Thus, by Cauchy-Schwarz inequality,

(
Í

𝑤𝑖
𝑤𝑖

2
])

𝑛
𝑖=1

2 6

1
Í
(

1
(

𝑤𝑖

𝑤𝑖

𝒜

𝔼

𝔼

𝔼

>

−

=

𝜂

/

𝜂

]

]

]

)

−
𝑛
𝑖=1

𝑤𝑖

]

˜𝜁[

𝑛
𝑖=1

𝑤𝑖

]

˜𝜁[

1
(

−

)

[
𝔼
˜𝜁[
e
e

e

e

Í

Í
Analysis of stable outlier rate selection The goal of the ﬁrst step of Algis to ﬁnd an outlier
rate 𝜂′ such that the strongly convex potential function Pot
on the pseudo-distribution we will
eventually compute (in Step 3) is close on adjacent input points 𝑌, 𝑌′. We will later use the strong
𝑌
convexity of the Pot and the closeness guarantee on Pot on 𝑌, 𝑌′ to infer that the weight vector 𝑝
(
and 𝑝

output by the algorithm themselves are close.

( ˜𝜁

)

)

Our key algorithmic trick to ensure the closeness of the strongly convex potential Pot is to ﬁnd
of outlier rates 𝜂′′ such that strongly convex potential
a “stable interval”
function at near-optimal solutions must vary slowly as 𝜂′′ varies in the the interval. We ﬁnd such
an interval via a variant of the exponential mechanism.

𝑛, 𝜂′ +

𝜂′ −
[

0.5𝐿

0.5𝐿

𝑛

/

/

]

𝑌′)
(

Deﬁnition 4.11 (Stability). Fix 𝐿
some 𝑌
2𝛾 length interval centered at 𝜏 to be

ℝ𝑑 of size 𝑛, the constraint system

⊆

∈

ℕ. Let 𝜏, 𝛾

𝒜((

∈ {
𝜏
−

)/

)

0, . . . , 𝑛
𝑛
𝛾

such that 𝛾 6 𝜏, 𝑛

𝜏. Suppose for
is feasible. We deﬁne the stability of the

−

}

stab𝑌

𝜏, 𝛾

= Pot
(

𝜏

𝛾

−

)/

𝑌
𝑛(

)

) −

Pot
(

𝜏

+

𝛾

)/

𝑌
𝑛(

)

(

Observe that if there is a pseudo-distribution consistent with

then there is a pseudo-distribution consistent with
stability above is well-deﬁned.

𝒜

on 𝑌 with any outlier rate >

on 𝑌 with outlier rate
𝛾

𝒜

𝜏

𝑛
𝜏
𝛾
𝑛. Thus,

)/

−

(
)/

(

−

Deﬁnition 4.12 (Score Function). Fix 𝑛, 𝑘
parameter 𝐿, we deﬁne the following score function for every integer 𝜏

ℕ and 𝐶 > 0. Let 𝑌

⊆

∈

ℝ𝑑 be a set of size 𝑛. For a

𝑛

:

]

∈ [





score𝑛,𝐶,𝑘

𝜏, 𝑌

=

)

(

0

max

𝛾
𝑌
𝑛,𝑛(

)

is feasible

min

{

𝛾, 20𝐿

stab𝑌

𝜏, 𝛾

(

)}

−

𝒜𝐶 ,𝑘,

𝛾

𝜏

(

−

)/

if Alg
𝑌, 𝜏
(
/
otherwise.

𝑛

)

is infeasible,

In the second case, we deﬁne 𝛾∗𝑌(

:= arg max

𝜏

)

𝒜𝐶 ,𝑘,

𝛾

𝜏

(

−

)/

𝛾
𝑌
𝑛,𝑛(

)

is feasible

min

𝛾, 𝐿

{

−

stab𝑌

𝜏, 𝛾

.

)}

(

Lemma 4.13. Let 𝜏, 𝛾
system
𝛾
𝜏
)/
most one point. Then, for any 𝜏, 𝛾,

𝒜((

∈ [

−

𝑛

𝑛

]

)

such that 𝛾 6 𝜏, 𝑛

ℝ𝑑 of size 𝑛, the constraint
𝜏. Suppose for some 𝑌
is feasible for both 𝑌. Let 𝑌′ be any collection of 𝑛 points in ℝ𝑑 diﬀering from 𝑌 in at

−

⊆

stab𝑌′(

𝜏, 𝛾

1
)

−

6 stab𝑌

𝜏, 𝛾

(

)

Proof. Using Lemma 4.9 and noting that if ˜𝜁′ is adjacent to ˜𝜁 then

Pot
(

𝜏

𝛾

1
)/
+

−

𝑌′

𝑛(

)

6 Pot
(

𝛾

𝜏

−

)/

,

𝑌
𝑛(

)

30

𝑤

]

˜𝜁′[

𝔼

(cid:13)
(cid:13)
(cid:13)e

2

2
(cid:13)
(cid:13)
(cid:13)

6

𝔼

(cid:13)
(cid:13)
(cid:13)e

𝑤

]

˜𝜁[

2

2
(cid:13)
(cid:13)
(cid:13)

, we have:

and

Pot
(
Combining the two equations yields

𝛾

𝜏

+

)/

𝑌
𝑛(

)

6 Pot
(

𝜏

𝛾

1
)/
−

+

𝑌′

𝑛(

.

)

stab𝑌′(

𝜏, 𝛾

1
)

−

𝑌′

= Pot
(
6 Pot
(

𝜏

𝛾

−

𝜏

𝛾

𝑛(
1
)/
+
𝑌
𝑛(

Pot
(

𝜏

+

𝜏

𝛾

) −
Pot
(

𝛾

1
−
)/
𝑌
𝑛(

𝑌′
𝑛(
)
= stab𝑌

𝜏, 𝛾

.

) −
Lemma 4.14 (Sensitivity of Score Function). Let 𝑌, 𝑌′ be set of 𝑛 points in ℝ𝑑 diﬀering at most in one
point, and 𝜏

. Then, for every 𝜏 > 0,

𝑛

)/

)/

−

+

(

)

)

(cid:3)

∈ [

]

Proof. It suﬃces to prove that score
(
that score
(

> score
(
Consider the following two cases:

𝜏, 𝑌′) −

𝜏, 𝑌

)

𝜏, 𝑌

|

score
(
𝜏, 𝑌′)

) −
> score
(

) −
2, which establishes (4.9).

score
(

𝜏, 𝑌′

)|

6 2.

(4.9)

𝜏, 𝑌

2. A symmetric argument then proves

• Alg

𝜏

𝑌,
(

𝑛

1
)/

is infeasible for 𝑌 or 𝑌′. In this case, we have score
(

6 2, which implies

𝜏, 𝑌

)

−
the desired bound.

(

)

𝜏

• Alg

𝑌,
(
−
Let 𝛾∗ := 𝛾∗𝑌(

(

follows that

)

𝑛

is feasible for both 𝑌 and 𝑌′.

1
)/
. From Lemma 4.13, we know that stab
(
)

𝜏

𝜏, 𝛾∗ −

1, 𝑌′)

6 stab𝑌

𝜏, 𝛾∗) +

(

2. Thus, it

score
(

𝜏, 𝑌′

)

> min
> min

𝛾∗

𝛾∗

{

{

−

−

1, 20𝐿

1, 20𝐿

−

−

as desired.

stab𝑌′(
stab𝑌
(

𝜏, 𝛾∗

1
)}

−

𝜏, 𝛾∗

)}

> score𝑌

𝜏

(

) −

1,

(cid:3)

𝜂

𝒜(

2
)

/

is feasible on 𝑌. For every

Lemma 4.15 (Existence of a Good Stable Interval). Suppose
0, 𝜂𝑛
𝐿

, there is a 𝜏

0, 0.25𝜂𝑛

> 𝐿.

𝜏, 𝑌

such that score
(

)

∈ [

]

Proof. Consider Pot𝜂
6
Pot𝜂

𝑌
Pot𝜂(

)

𝑌
2(
/

) −

∈ [
2, Pot𝜂
2𝐿
2
+
/
/
𝑛
2
1
𝜂
)
(

]
𝑛 , . . . , Pot𝜂
/
1
𝜂
− (

−

−

/

2𝐿𝑟

2
+
/

𝑛 where 𝑟

:=
2𝑛 6 1.5𝜂𝑛. Therefore, there must exists 𝑟∗ ∈ [
)

0.25𝜂𝑛

𝐿

/

⌋

⌊

/

.

Observe that
such that

𝑟

]

Pot𝜂

2𝐿

2
+
/

𝑟∗−
1
)/

(

𝑛 −

Pot𝜂

2𝐿𝑟∗/
2
+
/

𝑛 6 1.5𝜂𝑛

𝑟

6 12𝐿.

Let 𝜏 = 𝜂

2

/

2𝐿𝑟∗ −

+ (

1
)/

𝑛 and 𝛾 = 𝐿. Then, we have stab
(

𝜏, 𝛾

score
(

𝜏, 𝑌

)

> max

𝛾, 20𝐿

{

12𝐿

−

6 12𝐿 and, thus,

> 𝐿.

)

}

Lemma 4.16 (Utility of Score Function). Suppose

is feasible on 𝑌. Let 𝜀, 𝛿, 𝛽

0, 0.25𝜂𝑛

𝑛
𝐿
𝛽𝛿
function in Deﬁnition 4.12 and 𝜅 = 𝐿

, if 𝐿 > 𝑂

1
𝜀 ·

log

∈ [

]

(cid:16)

(cid:16)

, then with probability 1

𝛽, Theorem 3.34, invoked with the score

2, does not reject, and the output 𝜏 satisﬁes stab𝑌
(cid:17) (cid:17)
/

𝜏, 𝐿

2
)

/

(

< 20𝐿.

𝜂

2
)

/

𝒜(

−

31

(cid:3)

. For every

0, 1
]

∈ (

Proof. This follows from the guarantee of Selection (Theorem 3.34), Lemma 4.15 and the deﬁnition
(cid:3)
of score.

Lemma 4.17 (Potential Stability Under Good Coupling). Let 𝜂, 𝜀, 𝛿 > 0 and 𝑘, 𝐿
parameters such that 0.25𝜂𝑛 > 𝐿 = Ω
not halt and chooses 𝜂′ = 𝜏

ℕ be given input
. Let 𝑌, 𝑌′ be adjacent subsets of ℚ𝑑. Suppose Algdoes

𝑛 in Step 1 on input 𝑌 and 𝑌′. Then,
(cid:16)

1
𝜀 ·

log

𝑛
𝛽𝛿

(cid:17) (cid:17)

∈

(cid:16)

/

Consequently, if 𝑝, 𝑝′ are scalings of

𝑌
Pot𝜂′(
𝑤

and

) −

𝔼

(cid:12)
𝔼
(cid:12)
˜𝜁[

]

Pot𝜂′(
𝑤

˜𝜁′[

]

𝑌′

)
(cid:12)
so that
(cid:12)

6 20𝐿 .

𝑝

=

1

𝑝′

1

= 1, then,

e

𝑝

−

e
𝑝′
1

6 120

(cid:13)
(cid:13)
𝑛 .

(cid:13)
(cid:13)

𝐿

/

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
𝑌
Proof. It is enough to prove that Pot𝜂′(
(cid:13)
other direction and completes the proof.

(cid:13)
Pot𝜂′(
(cid:13)

𝑌′)

) −

p
6 20𝐿 as a symmetric argument proves the

2

𝔼

]

𝑤

˜𝜁[

while satisfying

Let ˜𝜁 be the pseudo-distribution that minimizes

on 𝑌′ with outlier
rate 𝜂′ (computed in Step 3 of the algorithm on input 𝑌′). Suppose 𝑌 and 𝑌′ diﬀer on 𝑖-th
(cid:13)
(cid:13)
(cid:13)e
sample point. Let ˜𝜁𝑎𝑑𝑗 be the adjacent pseudo-distribution obtained by zeroing out 𝑤𝑖. Then,
𝑛.
from Lemma 4.9, we know that ˜𝜁𝑎𝑑𝑗 is consistent with
𝑤
Further,
Pot𝜂′+
(cid:12)
(cid:12)

𝒜
6
6 Pot𝜂′(
. Thus, Pot𝜂′+
)
6 20𝐿. Therefore, we have Pot𝜂′(
(cid:13)
𝑌
(cid:13)
) −
(cid:13)e
Now, by Cauchy-Schwarz inequality, we immediately obtain that:

2
(cid:13)
(cid:13)
(cid:13)
on input 𝑌 with outlier rate 𝜂′ +
𝑌′)
Pot𝜂′(

. Further, Lemma 4.16 implies that
𝑌′)

6 20𝐿 as desired.

]
2
(cid:13)
Pot𝜂′
(cid:13)
(cid:13)

(cid:13)
𝑌
𝑛(
(cid:13)
(cid:13)e

𝑌
𝑛(

2
(cid:13)
(cid:13)
(cid:13)

˜𝜁𝑎𝑑𝑗 [

1
/

) −

˜𝜁[

𝒜

1
/

1
/

𝔼

𝔼

𝑤

(cid:12)
(cid:12)

]

2

2

(cid:13)
(cid:13)
(cid:13)e
Thus, from Lemma 3.21, we have that:

𝔼

𝑤

˜𝜁[

] −

𝔼

𝑤

]

˜𝜁′[

e

1
(cid:13)
(cid:13)
(cid:13)

2

6 20𝑛𝐿

𝑝

−

𝑝′

1

6 120

𝑛 .

𝐿

/

(cid:13)
(cid:13)

(cid:13)
(cid:13)

p

(cid:3)

)

)

𝑌
(

𝑌
(

is not too far from 𝑝𝑖

1-subgaussian distribution on 𝑌′.

Parameter closeness from potential stability The following lemma observes that if a sequence
of weights 𝑝𝑖
induces a 2𝑘-certiﬁably 𝐶′-subgaussian distribution on 𝑌 and 𝑝′𝑖(
𝑌
is a sequence
of weights on an adjacent 𝑌 such that 𝑝𝑖
𝑌′)
must also induce
(
a 2𝑘-certiﬁably 𝐶′ +
Lemma 4.18. Let 0 6 𝑝𝑖
𝑌
)
(
2𝑘-certiﬁable 𝐶′-subgaussian distribution on 𝑌. Let 𝑝𝑖
up to 𝑛 on 𝑌′ adjacent to 𝑌 such that
constant 𝜂′ > 0, 𝑝𝑖

be a sequence of non-negative weights adding up to 𝑛 that induce a
be a sequence of non-negative weights adding
𝑌′)
(
6 𝛽 for 𝛽 6 𝜂0. Then, for small enough absolute
𝑝
𝑌′)
1
(
-subgaussian distribution on 𝑌.
𝐶′ +
1
(cid:13)
)
(cid:13)
Proof Sketch. Let’s ﬁrst describe the idea of the proof: the proof of Lemma 4.6 requires the existence
of a certiﬁably subgaussian distribution that was close (in total variation distance) to the input 𝑌.

) −
induces a 2𝑘-certiﬁable

1
2𝜂′)
1
−

, then, 𝑝𝑖

𝑌′)
(

𝑌′)
(

𝑌
(

(cid:13)
(cid:13)

6

𝑝

)

(

(

32

𝑛-close (the
Since 𝑌 is adjacent to 𝑌′, the 2𝑘-certiﬁably 𝐶′-subgaussian distribution is 1
𝑛 comes from “removing” the index of the point where 𝑌 and 𝑌′ diﬀer) in total variation
additive 2
/
distance to 𝑌. Thus, the idea is to use the certiﬁably subgaussian distribution supported on 𝑌 in
lieu of 𝑋 to repeat the argument. In order to apply Lemma 4.6, we need a “ﬂat” distribution—but
this is easily achieved. Given a distribution with weights (without loss of generality, say, rational
numbers 𝑟𝑖
𝑠), we can consider a sample expansion to 𝑛𝑠 samples that has 𝑟𝑖 copies of sample
𝑦𝑖 for each 𝑖 and an analogous transformation to 𝑌′. And ﬁnally, given a pseudo-distribution
𝑌′, we can transform to a pseudo-distribution on 𝑛𝑠 variables by each
on 𝑤1, 𝑤2, . . . , 𝑤𝑛 on 𝑌
(cid:3)
“copying” 𝑤𝑖 for 𝑖 such that 𝑦𝑖 = 𝑦′𝑖 𝑟𝑖 times.

2
/

∩

−

−

𝛽

/

As an immediate corollary of Lemma 4.17 and Lemma 4.18, we obtain:

Corollary 4.19 (Parameter Closeness from Stability of Potential). Let 𝜂, 𝜀, 𝛿 > 0 and 𝑘, 𝐿
given input parameters to Algorithm 4.8 such that 0.25𝜂𝑛 > 𝐿 = Ω
. Also, let 𝑌, 𝑌′ be
adjacent subsets of ℚ𝑑. Suppose Algdoes not reject in any of the 3 steps, uses the constant 𝐶′ in Step 2 and
chooses 𝜂′ in Step 1 on input 𝑌 and 𝑌′.
𝐿

Then, for every 𝑢

ℝ𝑑 and 𝜃 =

𝑛, we have:

1
𝜀 ·

log

𝑛
𝛽𝛿

(cid:17)(cid:17)

∈

(cid:16)

(cid:16)

ℕ be

∈

/

p
−

𝜇𝑝′ , 𝑢

𝜇𝑝

h

6 𝑂

𝐶′𝑘

(

)

i

2𝑘
1
𝜃1
/
−

𝑢⊤Σ𝑝𝑢 ,

q

1
(

−

𝑂

𝐶′𝑘

1
𝜃1
/
−

𝑘

(

)

Σ𝑝
)

Σ𝑝′ (cid:22) (
1

(cid:22)

𝑂

(

+

𝐶′𝑘

)

1
𝜃1
/
−

𝑘

Σ𝑝 ,
)

and, for every 𝑡 6 𝑘 such that 𝑡 divides 2𝑘,

1
(

−

𝑂

(

𝑡
𝐶′

2𝑘𝑡
/

2
/

𝑡

𝜃1
−

2𝑘
/

𝑢 ⊗

)h

))

𝑡

𝑡 , 𝑀(

)𝑝 i

6

𝑢 ⊗

h

𝑡

𝑡 , 𝑀(

)𝑝′ i

6

1
(

+

𝑂

(

𝑡
𝐶′

2𝑘𝑡
/

2
/

𝑡

𝜃1
−

2𝑘
/

)

𝑢 ⊗

)h

𝑡

𝑡 , 𝑀(

)𝑝 i

,

˜𝜁𝑎𝑑𝑗 satisﬁes

Proof. Let ˜𝜁𝑎𝑑𝑗 be the adjacent pseudo-distribution of degree 2𝑘 to ˜𝜁 obtained by zeroing out 𝑤𝑖
where 𝑖 is the index of the point that 𝑌 and 𝑌′ diﬀer on. Then, from Lemma 4.9, we know that
6 1,

on both inputs 𝑌, 𝑌′ with outlier rate 𝜂′ +
6 1. Let 𝑝𝑎𝑑𝑗 be the scaling of
𝑤
𝑤
˜𝜁′[
2 |
(cid:13)
(cid:13)
𝑝
𝑛 (since 𝜂′ ≪
2). Further, applying Lemma 4.17 and triangle inequality, we have
(cid:13)
(cid:13)
−
(cid:13)e
(cid:13)
6 𝑂
. Applying Fact 3.23 to 𝑝𝑎𝑑𝑗 and 𝑝 on 𝑌 and 𝑝𝑎𝑑𝑗 and 𝑝′ on 𝑌′ and using
𝑛
𝐿
that
(cid:13)
)
(
(cid:13)
(cid:3)
triangle inequality completes the proof.
p

] −
]
= 1. Then, clearly,

˜𝜁𝑎𝑑𝑗 [
𝑝𝑎𝑑𝑗
𝑝𝑎𝑑𝑗
(cid:13)
(cid:13)

𝒜
𝔼
6 2
/
e
𝑝′
1

(cid:13)
(cid:13)
𝑝𝑎𝑑𝑗
(cid:13)e
(cid:13)
(cid:13)

2 |
(cid:13)
(cid:13)
(cid:13)

so that

𝑛 and

˜𝜁𝑎𝑑𝑗 [

˜𝜁𝑎𝑑𝑗 [

1
/

1
/

] −

˜𝜁[

(cid:13)
(cid:13)

𝔼

𝔼

𝔼

𝔼

e

e

𝑤

𝑤

𝑤

−

/

]

]

1

2

1

2

|

|

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Noise injection in estimate-dependent norms Our ﬁnal ingredient for obtaining privacy guaran-
tees for our robust estimation algorithms is a new noise injection mechanism where the distribution
of noise depends on the covariance estimated by our algorithm.

Lemma 4.20. Suppose 𝜀, 𝛿 > 0. Let 𝐴 be an invertible 𝑑
where 𝛽 6

ℝ𝑑 be a vector whose entries are i.i.d. from

. Let 𝑧

×

𝑑 matrix that satisﬁes
𝐼
1
𝛽
(
)
−
. Then,
0, 1
)

𝒩(

𝜀
3𝑑 ln

𝑑

𝛿

/

)

(

∈

𝐴𝐴𝑇

(cid:22)

1
(cid:22) (

+

𝛽

)

𝐼,

𝑧, 𝐴𝑧

𝐷𝑒 𝜀

(

)

6 𝛿.

33

Proof. Note that the probability distribution function of 𝐴𝑧 at 𝑢

ℝ𝑑 is

∈

1
√2𝜋
(
)
2 = det
Moreover, det
𝛽
(
)
(
ratio of the probability densities of 𝑧 and 𝐴𝑧 at 𝑢 is

1
det
(
2, since det
/
(

1
(

𝐴

𝐴

𝐴

6

+

)

)

)

𝑑

𝑑

𝐴−

1𝑢

2
2/

2.

k

𝑒−k

𝐴

)

det

𝐴𝑇

(

= det
(

)

𝐴𝐴𝑇

)

6

1
(

𝛽

)

+

𝑑. Thus, the

𝐴

det
(

)

𝑒 k

𝐴−

1𝑢

k

2
2
2/
−k

𝑢

k

2
2/

2 6

6

6

6

Thus, note that if

6

𝑢

k

k∞

2 ln

𝑑

𝛿

, then
)

/

(

𝑢

k

k2

)

)

)

)

𝛽

𝛽

𝛽

𝛽

+

+

1
(
1
(
1
(
1
(
+
6 √𝑑

+

𝐴−

1𝑢

𝑑

𝑑

/

2𝑒 k
2𝑒
/

1
2 𝑢𝑇

((

k
𝐴𝐴𝑇

2
2
2/
−k

𝑢

2
2
2/

k

1

)−

𝑢

𝐼

)

−

𝑑

2𝑒
/

1
2 (

1
−

𝛽

)−

1

k

𝑢

2
2−

1
2 k

k

𝑢

2
2

k

𝑑

2𝑒
/

𝛽
1
−

2
(

𝛽

) k

𝑢

2
2.

k

6

𝑢

· k

k∞

2𝑑 ln

𝑑

𝛿

, and so,
)

/

(

p
det
(

𝐴

𝑒 k

)

𝐴−

1𝑢

k

2
2
2/
−k

𝑢

k

2
2/

2 6

1
(

+

𝛽

)

p
𝛽 𝑑 ln
(

𝛽

1
−

𝑑

2𝑒
/

𝑑

𝛿

/

) < 𝑒 𝜀,

.

since 𝛽 6

𝜀
3𝑑 ln

𝑑

𝛿

/

)

(

Moreover, by standard tail bounds of the normal distribution, we have that

with probability at most 𝛿. This proves the claim.

𝑧

k

k∞

>

2 ln

p

𝑑

𝛿
)
(cid:3)

/

(

Lemma 4.21. Suppose 𝜀, 𝛿 > 0. Let 𝐴 be a 𝑑
ℕ. Moreover, let 𝑍

×

Let 𝑡
1 6 𝑖1 6 𝑖2 6
𝑖 =

∈

𝑖1, 𝑖2, . . . , 𝑖𝑑
(
If 𝛽 6
𝜀
8𝑡2𝑑𝑡 ln

· · ·
)
𝑑𝑡

(

ℝ𝑑𝑡
6 𝑖𝑡 6 𝑑, are i.i.d.
and permutation 𝜋.

∈

, then

𝛿

)

/

𝑑 matrix that satisﬁes

be a random vector indexed by
(cid:13)
(cid:13)
, and moreover, 𝑍𝑖1 ,𝑖2,...,𝑖𝑡 = 𝑍𝑖𝜋
0, 1
from
)

𝒩(

]

[

−
𝑡, whose entries 𝑍𝑖1,𝑖2 ,...,𝑖𝑡 , for
for any
,𝑖𝜋

,...,𝑖𝜋

(cid:13)
(cid:13)

𝑡

1
)

(

2
)

(

(

)

6 𝛽.

𝐼

2

𝐴𝐴𝑇
𝑑

Proof. Let 𝐾 =

2 ln

𝑑𝑡

𝛿

/

(

p

𝐷𝑒 𝜀

𝑍, 𝐴⊗

𝑡𝑍

6 𝛿.

(
. By standard tail bounds, note that
)

)

ℙ

𝑍

[k

k∞

> 𝐾

]

6 𝛿.

(4.10)

Let 𝑆 be the subspace of ℝ𝑑𝑡

consisting of all symmetric tensors, i.e.,

ℝ𝑑𝑡

∈

𝑆 =

𝑢

n

: 𝑢𝑖1,𝑖2 ,...,𝑖𝑡 = 𝑢

𝑖𝜋

1
)

(

,𝑖𝜋

2
)

(

,...,𝑖𝜋

(

𝑡

(

))

𝑖 =

,

∀

(

𝑖1, 𝑖2, . . . , 𝑖𝑡

𝑑

]

) ∈ [

𝑡 , 𝜋 a permutation on

𝑡

[

]

.

o

Note that 𝑆 is an 𝑑′-dimension invariant subspace of 𝐴⊗
𝑡 be a representative set of indices of size
𝑑
𝑅
]
𝑖1, 𝑖2, . . . , 𝑖𝑡

𝑅
|
𝑡, there exists a permutation 𝜋 on
𝑆 be the restriction of 𝐴⊗

Now, let 𝑀 = 𝐴⊗

) ∈ [

⊆ [

𝑑

]

]

(

|

𝑡

𝑑

𝑡, where 𝑑′ =

𝑡
6 𝑑𝑡. Moreover, let
1
−
+
𝑡
= 𝑑′, i.e., 𝑅 satisﬁes the property that for any
𝑡

(cid:1)

(cid:0)
, 𝑖𝜋

such that

𝑖𝜋
𝑡
(
𝑡 to the subspace 𝑆. Moreover, let 𝑍𝑅

, . . . , 𝑖𝜋

2
)

1
)

[

(

(

(

𝑅.
ℝ𝑑′ denote

)) ∈
∈

the projection of 𝑍 to indices in 𝑅.

|

34

Note that the probability distribution of 𝑍 can be equivalently viewed as the probability distri-
bution of 𝑍𝑅, since 𝑍 is uniquely determined by the projection 𝑍𝑅. Let 𝑝 be the probability density
function of 𝑍𝑅 over ℝ𝑑′. Then, note that the probability distribution of 𝑀𝑍𝑅 is 𝑞, given by

=

𝑞

𝑣

(

)

1
𝑀
det
(

)

𝑝

(

·

𝑀−

1𝑣

.

)

∈

for 𝑣
by the 𝑖th singular value of 𝐴⊗
𝐼
Moreover, by
Hence, the singular values of 𝑀 also lie in
implies that

ℝ𝑑′. By standard properties, we know that the 𝑖th singular value of 𝑀 is bounded from above
𝑡.
th singular value of 𝐴⊗
𝑡
2
2,
.
𝛽
/
/
]
−
, which, together with 𝛽𝑡 6 1
4 ,

𝑑𝑡
𝑡 and bounded from below by the
𝑑′)
−
6 𝛽, we know that the singular values of 𝐴⊗
𝑡 lie in
1
[(

𝐴𝐴𝑇

1
[(

1
(

1
(

2,
/

2
/

(cid:13)
(cid:13)

(cid:13)
(cid:13)

−

+

+

+

−

𝛽

𝛽

𝛽

]

)

)

)

)

(

𝑖

2

𝑡

𝑡

𝑡

and so,

𝑀

det
(

6

2 = det
(
)
𝑡𝑑𝑡

2.
/

2𝛽𝑡
𝑀
and so, det
1
)
(
+
(
ℝ𝑑𝑡
Let 𝑢
. Note that
𝑣

∈
Moreover, note that if

k

)
k
k∞
𝑝
𝑣
(
𝑣
𝑞
(

By (4.11), we have that

(

(cid:13)
(cid:13)

𝑀 𝑀𝑇

𝐼

2

−

6 2𝛽𝑡

(cid:13)
(cid:13)
𝑀𝑇
)

= det
(

(cid:13)
(cid:13)
𝑀 𝑀𝑇

𝑀

det
(

)

6

)

1
(

+

2𝛽𝑡

)

𝑑′ 6

𝑡

·

2𝛽𝑡

1
(

+

𝑡𝑑𝑡

,

)

(4.11)

(4.12)

6 𝐾 if and only if 𝑣

𝑢
k∞
6 𝐾, then

ℝ𝑑′ given by 𝑣 = 𝑢

𝑅 also satisﬁes

|

6 𝐾.

𝑣

k

k∞

∈

6 det
(

𝑀

) ·

)
)

6

1
(

+

2𝛽𝑡

)

𝑝

(
𝑡𝑑𝑡

2
/

·

𝑝
𝑣
)
(
1𝑣
𝑀−

)
exp

1
2

6 𝑒 𝛽𝑡2𝑑𝑡

6 𝑒 𝛽𝑡2𝑑𝑡

·

·

exp

exp

(cid:18)

𝑀 𝑀𝑇

1
)−

−

𝐼

2

(cid:18)
6 2𝛽𝑡
2𝛽𝑡
1
−

(cid:18)
(cid:16)(cid:13)
(cid:13)
𝑀 𝑀𝑇

1
−
)

1
2

𝑣𝑇

((

𝑀−

1𝑣

𝑣

2
2

k

(cid:17) (cid:19)

2
2 − k
(cid:13)
(cid:13)
−

𝑣

)

𝐼

(cid:19)

(cid:0)

1
2 k

𝑣

2
2 ·

k

(

𝑀 𝑀𝑇

1
−
)

−

(cid:1)
𝐼

2

(cid:19)

(cid:13)
𝑒 𝛽𝑡2𝑑𝑡
(cid:13)

·

exp

2𝐾2𝑑𝑡𝛽𝑡

.

(cid:13)
(cid:13)

(cid:13)
(cid:13)
6 4𝛽𝑡, since 𝛽𝑡 6 1
4 . Therefore, (4.13) is at most

(4.13)

Thus, if 𝛽 6
proves the desired claim.

𝜀
2𝐾2 𝑡𝑑𝑡 =

𝜀
8𝑡2𝑑𝑡 ln

𝑑𝑡

(

𝛿

)

/

, the above quantity is at most 𝑒 𝜀. This, combined with (4.10),
(cid:3)

(cid:0)

(cid:1)

Remark 4.22. Note that Lemma 4.21 uses an assumption on the spectral norm of
2. However,
it is also possible to obtain a version of the lemma under an assumption on the Frobenius norm,
(cid:13)
6 𝛽, then Eq. (4.12) instead
(cid:13)
𝐴𝐴𝑇

In particular, if we assume that, instead,

𝐴𝐴𝑇

(cid:13)
(cid:13)

−

𝐼

𝐼

𝐼

𝐴𝐴𝑇

−

𝐹

𝐹.

1

6

(cid:13)
(cid:13)

𝑡𝑑𝑡

2
/

−
𝛽
(cid:13)
𝑀
becomes det
(cid:13)
√𝑑
(
𝜆1, 𝜆2, . . . , 𝜆𝑑 of 𝐴𝐴𝑇 satisfy
(cid:17)
is maximized when 𝜆1 = 𝜆2 =
Í
𝑑𝑡 𝑡-fold products of eigenvalues of 𝐴𝐴𝑇.

𝑑
𝑖=1(
· · ·

6 𝑒 𝛽𝑡𝑑𝑡

+

(cid:16)

)

1
2

−

2: This follows from the fact that (a.) the eigenvalues
/

(cid:13)
(cid:13)
2 6 𝛽2, (b.) under the aforementioned constraint, 𝜆1𝜆2 · · ·
1
𝜆𝑖
)
−
= 𝜆𝑑 = 1

𝜆𝑑
𝑡 are precisely the

, (c.) the eigenvalues of

𝐴𝐴𝑇

(cid:13)
(cid:13)

(

)⊗

𝛽
√𝑑

+

35

Putting things together Now, we are ready to prove the main privacy guarantee provided by our
robust moment estimation algorithm, Algorithm 4.8.

Lemma 4.23 (Privacy Guarantee). Suppose 𝐶 , 𝜂, 𝜀, 𝛿 > 0 and 𝑘

2𝑘
𝑘
1

−

(cid:19)

(cid:1)(cid:17)

Ω

𝐶 𝑘4 𝑑𝑘
𝜀

ln

6𝑘𝑑𝑘
(

/

𝛿

) +

𝜀
6𝑘

. Then, Alg (given by Algorithm 4.8), invoked with 𝐿 = 𝑂

𝑛

log
(
(

/

𝛿

)/

𝜀

,
)

(cid:18) (cid:16)
𝜀, 𝛿
(

is
-DP.
(cid:0)
e
)
Proof. Let 𝜀′ = 𝜀
it suﬃces to show that each step of the algorithm is
steps as parameter3). Let 𝑌 and 𝑌′ be any neighboring datasets.

3 and 𝛿′ = 𝛿

𝜀′, 𝛿′)

/

/

(

3. By our adaptive composition theorem under halting (Lemma 3.30),
-DP (given the outputs of the previous

ℕ.

Suppose 𝑛 > 𝑛0 =

∈

• Stable Outlier Rate Selection. Since this step invokes the

(Selection ), it immediately follows from Theorem 3.34 that this step is

𝜀′, 𝛿′)

(

-DP Selection algorithm
-DP.

𝜀′, 𝛿′)

(

• Witness Checking. Let 𝐶∗(
𝑌

induces a 2𝑘-certiﬁable
)
6 Δ for Δ = 1.
𝐶∗(
𝐶∗-subgaussian distribution on 𝑌. Lemma 4.18 ensures that
Therefore, we may apply Lemma 3.27 with DP parameters 𝜀′, 𝛿′ to conclude that this step is
also

denote the smallest 𝐶∗ for which 𝑝𝑖
𝑌
(
𝑌
𝐶∗(

)
) −

𝑌′)|

-DP.

|

𝜀′, 𝛿′)

(

• Noise Addition. Since the algorithm has not halted in the previous step and the truncated
must induce 2𝑘-certiﬁable 𝐶′-subgaussian distri-
𝜇′ denote the corresponding mean estimates
Σ′ denote the corresponding

Laplace noise is negative, 𝑝𝑖
𝑌′)
and 𝑝𝑖
(
butions on 𝑌 and 𝑌′ respectively. Let
𝜇 and
Σ and
under 𝑝𝑖
covariance estimates. By Corollary 4.19, we have that, for all 𝑢

, respectively, and, similarly, let
e

and 𝑝𝑖

𝑌′)
(

𝑌
(

𝑌
(

e

)

)

and, for all 2 6 𝑡 6 𝑘,

1
(

−

6 𝛾1

1
(cid:22) (

p
+

e
Σ𝑢
𝑢⊤
Σ
𝛾2)
e
e

𝜇′, 𝑢

i
Σ′

(cid:22)
e

𝜇
−
h
Σ
𝛾2)
e
e

e
𝑡 ,

ℝ𝑑,

∈
e

(4.14)

(4.15)

(4.16)

𝑡 ,

𝑡

𝑀(

,

)i

𝑡

2𝑘 for 2 6 𝑡 6 𝑘. Moreover,
/

g

𝛾𝑡

1
(
where 𝜃 =
𝑛, 𝛾1 = 𝑂
𝐿
/
2.
1
2
1
Σ′
Σ−
let 𝐵 =
/
/
p

−

𝑢 ⊗

𝑡 ,

𝑡

𝑀(

6

𝑢 ⊗

𝑡

𝑀′(

h

)i
)i
2𝑘 , and 𝛾𝑡 = 𝑂
1
𝜃1
(cid:157)
g
/
−

)h
𝐶′𝑘

(

)

((

6

1
(
𝐶′𝑘

+
𝑡
2
/

)h
𝜃1
−

)

)

𝛾𝑡

𝑢 ⊗

Note that in order to show that the noise addition step is

e

e

-DP, it suﬃces to show that

𝜀′, 𝛿′)
(
2𝑧,
Σ1
2𝑧
1
Σ′
𝜇′
/
/
+
2
1
2𝑍
1
Σ′
Σ′
/
/
e
e
𝑡 𝑍(
2
1
Σ′
⊗
/
e
e

𝐷𝑒 𝜀′′ (
𝜇
+
2,
Σ1
2𝑍
Σ1
Σ′
/
/
e
e
𝑡
𝑀′(
),
) + (
e
e
𝑘, since Fact 3.32 and standard DP composition [DKM+06] then
-DP. We now establish each of the above

Σ
𝐷𝑒 𝜀′′ (
+
2
Σ1
/
) + (
e

6 𝛿′′

(4.17)

(4.19)

(4.18)

(cid:157)

𝑡 𝑍(

g

𝑀(

⊗
e
)

e

+

)

)

)

)

)

𝑡

𝑡

𝑡

6 𝛿′′
6 𝛿′′

2 < 𝑡 6 𝑘, 𝐷𝑒 𝜀′′ (

∀
𝑘 and 𝛿′′ = 𝛿′/
for 𝜀′′ = 𝜀′/
e
imply that the entire noise addition step is
inequalities.

𝜀′, 𝛿′)

(

3Note that we may also assume that the algorithm has not halted from the previous steps.

36

Noise addition for mean: We ﬁrst show (4.17). Note that

𝐷𝑒 𝜀′′ (
𝜇

+

2𝑧,
Σ1
/

𝜇′

2𝑧
1
Σ′
/

)

+

e

e

e

e

)

2𝑧
1
Σ′
/

𝐵𝑧

𝜇

= 𝐷𝑒 𝜀′′ (
= 𝐷𝑒 𝜀′′ (
𝑧,
e
= 𝐷𝑒 𝜀′′/
𝑧, 𝑧
2
(
+
e
2𝐷𝑒 𝜀′′/
𝑒 𝜀′′/
2
+
= 𝐷𝑒 𝜀′′/
𝑧, 𝑧

2𝑧,
Σ1
𝜇′
/
−
(
) +
2
1
Σ−
𝜇′
𝜇
/
−
(
) +
e
e
e
2
1
Σ−
𝜇
𝜇′
/
−
(
e
e
2
1
Σ−
𝑧
/
(
+
e
e
2
1
Σ−
𝜇′
/
(
e

−

+

(

2

))
𝜇′
(
e
𝜇
e

)

𝜇

2
1
Σ−
/

,

−

)
𝐷𝑒 𝜀′′/
)) +
e
e

2

(

𝜇′
−
(
𝑧, 𝐵𝑧
e

)

𝜇

) +

𝐵𝑧

)

,
e

(4.20)

(4.21)

note that

where (4.20) follows from Lemma 3.33. For the ﬁrst term on the right-hand side of (4.21), we
e
6 𝛾1 (which follows from plugging in 𝑢 =
Thus, by the standard hockey-stick divergence calculation for the Gaussian mechanism [DR14,
Appendix A], we have that

2
1
Σ−
/
(cid:13)
(cid:13)
(cid:13)e

𝜇
)
(cid:13)
(cid:13)
(cid:13)

into (4.14)).

𝜇′ −
(

𝜇′)

1
Σ−

𝜇
(

e

e

e

e

e

e

e

−

2

𝐷𝑒 𝜀′′/

2

(

𝑧, 𝑧

+

2
1
Σ−
/

𝜇′
(

−

𝜇

))

< 𝛿′′

2,

/

(4.22)

provided that

𝜎1 >

e
2𝛾1

e
e
2.5
2 ln
𝛿′′)
/
(
𝜀′′

p

,

For the second term in (4.21), note that (4.15) implies that
Moreover, 𝛾2 6

(cid:22)
, by the condition 𝑛 > 𝑛0. Thus, by Lemma 4.20,

𝛾2)

1
(

−

𝐼

3𝑑 ln

𝜀′′
2𝑑

(

𝛿′′)

/

𝐷𝑒 𝜀′/

2

(

𝑧, 𝐵𝑧

)

6 𝛿′′

2.

/

Therefore, (4.22), (4.23),and (4.21) imply (4.17), as desired.

Noise addition for covariance: Next, we establish (4.18). Observe that

𝐵𝐵𝑇

1
(cid:22) (

+

𝐼.

𝛾2)

(4.23)

+

Σ
𝐷𝑒 𝜀′′ (
e

2𝑍
Σ1
/

2,
Σ1
/

Σ′

2𝑍
1
Σ′
/

2
1
Σ′
/

)

+

e

e

e

e

e

= 𝐷𝑒 𝜀′′ (
= 𝐷𝑒 𝜀′′ (
6 𝐷𝑒 𝜀′′/
𝑒 𝜀′′/
+
6 𝐷𝑒 𝜀′′/

2

2

𝐼

+
𝑍,

𝑍, 𝐵𝐵𝑇
𝐵𝐵𝑇

+
𝐼
−
) +
𝐵𝐵𝑇

+ (
𝑍
2
+ (
𝐵𝐵𝑇

(

+ (

(
𝑍, 𝑍
(
2𝐷𝑒 𝜀′′/
𝑍, 𝑍

(

𝐵𝑍𝐵𝑇

)
𝐵𝑍𝐵𝑇

)

𝐼
−
𝐵𝐵𝑇

))

𝐼

−

,
)
(
𝑒 𝜀′′/

𝐵𝐵𝑇
𝐼
−
2𝐷𝑒 𝜀′′/

𝐵𝑍𝐵𝑇
) +
))
𝑍, 𝐵𝑍𝐵𝑇
2

(4.24)

,

(4.25)

(

))

𝐼

−

)) +

where (4.24) follows from Lemma 3.33. To bound the right-hand side of (4.25), note that the
ﬁrst term is precisely the hockey-stick divergence computation corresponding to the Gaussian
mechanism (restricted to the upper triangular portion). Moreover, by (4.15),

Therefore ([DR14, Appendix A]), as long as

(cid:13)
(cid:13)

(cid:13)
(cid:13)

𝐵𝐵𝑇

𝐼

𝐹

−

6 √𝑑

𝐵𝐵𝑇

𝐼

2

−

6 𝛾2√𝑑.

(cid:13)
(cid:13)

·

(cid:13)
(cid:13)

(4.26)

𝜎2 >

2𝛾2

2𝑑 ln
2.5
/
(
𝜀′′

p

𝛿′′)

,

37

we have that

𝐷𝑒 𝜀′′/

2

𝑍, 𝑍

𝐵𝐵𝑇

𝐼

6 𝛿′′

2.

+ (
1
2 𝑍 has entries distributed in
For the second term in (4.25), note that 𝜎−
𝑍′ = vec
(
Fact 3.17, we know that 𝐵𝑍𝐵𝑇 = 𝐵⊗

0, 1
. Moreover, let
)
be the 𝑑2-dimensional vector given by the ﬂattening of 𝑍 (see Deﬁnition 3.15). By

2𝑍′. Thus, by Lemma 4.21 applied with 𝑡 = 2,

𝒩(

))

−

𝑍

/

(

)

(4.27)

𝐷𝑒 𝜀′′/

2

(

𝑍, 𝐵𝑍𝐵𝑇

))

as long as

2

(

= 𝐷𝑒 𝜀′′/
= 𝐷𝑒 𝜀′′/
6 𝛿′′

2𝑍′
𝑍′, 𝐵⊗
)
2
1
2 𝑍′, 𝐵⊗
𝜎−
(
2,
2𝑒 𝜀′′/

2

/

1
2 𝑍′
𝜎−

(

))

(4.28)

𝛾2 <

32𝑑2 ln

𝜀′′
2
2𝑑2𝑒 𝜀′′/
(
which is true, since 𝑛 > 𝑛0 by the conditions of the theorem. Thus, (4.27) and (4.28) imply that
(4.25) is at most 𝛿′′/

= 𝛿′′/
Noise addition for higher-order moments: Let 2 < 𝑡 6 𝑘. We write 𝑅 =
Σ′ +
Observe that the injective/spectral norm
e

e
can be bounded as

2, which establishes (4.18).

𝑇 for simplicity.

k·k𝜎 of

𝜇𝑇 and

2
2𝑒 𝜀′′/

2
1
𝑅−
/

𝛿′′/

2
𝑒 𝜀′′/

𝛿′′)

𝑅′ =

𝑀′(

) −

𝑀(

)⊗

𝜇′

𝜇′

e

e

Σ

+

+

𝜇

2

/

(

)

,

)

𝑡

𝑡

𝑡

e

e

2
1
𝑅−
/

𝑡

⊗

)

(
(cid:13)
(cid:13)
(cid:13)

𝑡

𝑀′(

) −

𝑡

𝑀(

)

(cid:157)

g

(cid:16)

𝜎

(cid:17)(cid:13)
(cid:13)
(cid:13)

(
𝑇

2
1
𝑅−
/

(

)

(cid:16)
(cid:157)
𝑡
𝑀′(
⊗

𝑡

(cid:17)
𝑡
𝑀(

g

)

) −

(cid:1)
2𝑣
1
𝑅−
/

(cid:16)

(cid:157)
𝑀(

) −

𝑡

)i

𝑡 ,

⊗

𝑡

𝑀′(

)

2𝑣
1
𝑅−
/

h(

)

g

(cid:157)
𝑡 ,

⊗

𝑀(

𝑡

)i

g

(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
𝑇

𝑅

2𝑣
1
𝑅−
/

(cid:17)

(cid:16)

(cid:17) (cid:19)

(4.29)

𝑡

2
/

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

g

2𝑣
1
𝑅−
/

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k

(cid:18) (cid:16)

𝑣

𝑡
2

k

sup
ℝ𝑑
𝑣
∈
k2=1
𝑣
k
sup
ℝ𝑑
𝑣
∈
k2=1
𝑣

k

𝑡

(cid:0)

h(

𝑣⊗

= sup
ℝ𝑑
𝑣
k2=1 (cid:12)
∈
𝑣
(cid:12)
k
(cid:12)
6 sup
ℝ𝑑
𝑣
k2=1 (cid:12)
∈
𝑣
(cid:12)
(cid:12)
sup
ℝ𝑑
𝑣
k2=1 (cid:12)
∈
𝑣
(cid:12)
(cid:12)
𝑡

k
6 𝛾𝑡

·

k

6 𝛾𝑡

𝐶′𝑘

· (

)

·

= 𝛾𝑡

𝐶′𝑘

𝑡

)

·

· (

= 𝛾𝑡

𝐶′𝑘

𝑡 ,

)
where (4.29) follows from the 𝐶′-subgaussianity property of the distribution induced by the weight
vector at the end of Step 2. Therefore, the Frobenius norm (or Hilbert-Schmidt norm) can be
bounded as (see Corollary 4.10 of [WDFS17])

· (

6 𝑑

𝑡

1

−
2

2
1
𝑅−
/

𝑡

⊗

𝑡

𝑀′(

𝑡

𝑀(

)

(

) −
)
(
𝐹
(cid:17)(cid:13)
(cid:13)
2, we have
1
2𝑅′
1
(cid:13)
(cid:13)
Moreover, letting 𝑊 = 𝑅−
g
/
/
(cid:13)
(cid:13)
𝑡 𝑍(

2
1
𝑅′
/

2
𝑅1
/

(cid:157)

𝑡 𝑍(

),

(cid:16)

⊗

⊗

·

𝑡

𝑡

𝑡

𝑀′(

𝑀(

(cid:13)
(cid:13)
(cid:13)
𝐷𝑒 𝜀′′ (

) + (

) + (

)

)

g

(cid:157)

2
1
𝑅−
/

𝑡

⊗

)

(cid:16)

𝑡

𝑀′(

) −

(cid:157)

𝑡

)

)

= 𝐷𝑒 𝜀′′

2
𝑅1
/

⊗

)

(

(cid:16)

38

𝑡

𝑀(

)

g

(cid:17)(cid:13)
(cid:13)
(cid:13)
𝑡
𝑡 𝑍(

6 𝛾𝑡

𝐶′𝑘

𝑡

)

·

· (

𝜎

𝑡

1
2 .
−

𝑑

(4.30)

),

𝑡

𝑀′(

𝑡

𝑀(

2
1
𝑅′
/

)

) + (

) −

𝑡

𝑡 𝑍(

)

⊗

(cid:157)

g

(cid:17)

= 𝐷𝑒 𝜀′′

𝑍(

𝑡

),

2
1
𝑅−
/

(

)

𝑡

⊗

𝑡

𝑀′(

(cid:16)
2

6 𝐷𝑒 𝜀′′/
𝑒 𝜀′′/

+

2
1
𝑅−
/

(
6 𝐷𝑒 𝜀′′/
𝑒 𝜀′′/

2

+

𝑡

𝑍(

), 𝑍(

𝑡

)

+ (

(cid:16)
2
1
𝑅−
(cid:157)
⊗
/
)
2
1
𝑅−
/

(cid:16)
2𝐷𝑒 𝜀′′/

𝑍(

𝑡

)

2

(
𝑀′(

𝑡

𝑡

+ (
𝑀(

𝑡

)

⊗

)

) −
(cid:16)
𝑡
𝑡
𝑍(
), 𝑍(
(cid:157)
(cid:16)
2𝐷𝑒 𝜀′′/

𝑍(

)

𝑡

2

+
(cid:17)
2
1
𝑅−
g
/

+ (
), 𝑊 ⊗

)
𝑡
𝑡 𝑍(

(

) −
𝑡

𝑡

𝑀(

)

𝑡
𝑀′(
g

+

(cid:17)
) −
𝑡

𝑊 ⊗

𝑡

𝑡 𝑍(

)

(cid:17)

𝑡

𝑀(

)

(cid:17) (cid:17)
𝑡
)

,

𝑀(
g

(cid:16)
𝑡

⊗

)

𝑊 ⊗

) −

𝑀′(
(cid:157)
(cid:16)
𝑡
𝑡 𝑍(
(cid:157)
)
))

(cid:17)

g

(4.31)

𝑡

⊗

,

)

)

(cid:16)

𝑡

𝑀′(

) −

𝑡

𝑀(

)

(cid:157)

g

(cid:17) (cid:17)

(4.32)

where again we have used Lemma 3.33 in (4.31). In order to bound the right-hand side of (4.32),
note that the ﬁrst term is again the hockey-stick divergence computation corresponding to the
Gaussian mechanism (restricted according to symmetry conditions). Recalling (4.30), we see that
([DR14, Appendix A]) as long as

𝜎𝑡 >

2𝛾𝑡

𝐶′𝑘

(

)

1

𝑡

−
2

𝑡 𝑑

𝜀′′
p

2 ln

2.5
/
(

𝛿′′)

,

we have that

𝐷𝑒 𝜀′′/

2

𝑡

𝑍(

), 𝑍(

𝑡

)

2
1
𝑅−
/

𝑡

⊗

)

+ (

𝑡

𝑀′(

) −

𝑡

𝑀(

)

6 𝛿′′

2.

/

(4.33)

1
𝑡 𝑍(
For the second term in (4.32), note that 𝜎−
that

0, 1
. Moreover, note
(cid:157)
)
6 𝛾2√𝑑 by (4.26) and the fact that (4.15) implies

) has entries distributed in

𝑊𝑊 𝑇

𝐵𝐵𝑇

g

𝒩(

6

𝐼

𝐼

𝑡

(cid:16)

(cid:17)(cid:17)

(cid:16)

𝐹

−

𝐹

−

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

1
(

−

𝑅

𝛾2)

(cid:22)

𝑅′

1
(cid:22) (

+

𝑅.

𝛾2)

Thus, by Lemma 4.21, we have that

𝐷𝑒 𝜀′′/

2

𝑡

𝑍(

), 𝑊 ⊗

𝑡

𝑡 𝑍(

)

(cid:16)

(cid:17)

as long as

2

= 𝐷𝑒 𝜀′′/
6 𝛿′′

(cid:16)
2𝑒 𝜀′′/

1
𝑡 𝑍(
𝜎−
2,

𝑡

), 𝑊 ⊗

𝑡

1
𝑡 𝑍(
𝜎−

𝑡

)

(

)

(cid:17)

(4.34)

/

𝜀′′
2𝑑𝑡 𝑒 𝜀′′/
2
(
which is true since 𝑛 > 𝑛0, by the hypothesis of the lemma. Thus, (4.33) and (4.34) imply that (4.32)
(cid:3)
is at most 𝛿′′/

= 𝛿′′, which establishes (4.19), as desired.

16𝑡2𝑑𝑡 ln

2
2𝑒 𝜀′′/

𝛿′′/

2
𝑒 𝜀′′/

𝛿′′)

𝛾2 <

+

2

/

(

)

,

4.4 Proof of Theorem 4.1

We are now ready to prove our main theorem, Theorem 4.1.

Proof of Theorem 4.1. Choose 𝛽 = 1
/
in Lemma 4.16). Moreover, let 𝐶 = 𝐶0 +

30. Choose 𝐿 = Ω
3
(
/
𝜀

9
𝜀 +

3 ln

+

𝛿

)

𝑛
𝛽𝛿

log

1
𝜀 ·
(cid:16)
1. Then, we claim that setting Alg to be

(according to the condition

(cid:17) (cid:17)

(cid:16)

39

Algorithm 4.8 with parameters 𝐶 , 𝜂, 𝜀, 𝛿, 𝐿, 𝑘 satisﬁes the desired conditions, as long as 𝜂 6 𝜂0,
where we set 𝜂0 later.

with mean 𝜇

, covariance Σ
∗

Note that the desired privacy guarantees follow immediately from Lemma 4.23.
It remains to prove the utility guarantees. Suppose that there indeed exists a good set 𝑋
𝜂

, and 𝑡-th moments 𝑀(
∗
By Theorem 3.34, we have that Step 1 (stable outlier rate selection) rejects and halts with
2. In particular,
is feasible. By

30, and the resulting output 𝜏 satisﬁes score
(
2 for which

probability at most 𝛽 = 1
/
the latter condition implies that there exists some 𝛾 > 𝐿
𝑛.
monotonicity,

is also feasible, where we let 𝜂′ = 𝜏

for 3 6 𝑡 6 𝑘, such that

> 𝐿
𝛾
𝜏
−
𝑛

⊆
𝑛.
)

𝜏, 𝑌

1
(

ℚ𝑑

𝒜

𝜂′

𝑋

>

∩

−

𝑌

/

/

)

|

|

∗

)

𝑡

(cid:0)

Hence, the invocation of Algorithm 4.3 in Step 2 does not yield “reject.” Moreover, note that
(cid:1)
by Lemma 3.28, we have that 𝐶′ = 𝐶
30. In this case, the
computed weight vector 𝑝 induces a 𝐶′-certiﬁably subgaussian distribution on 𝑌. Hence, the
30.
probability of rejection in Step 2 is at most 1
/
𝔼
) =
Σ =

(for 2 6 𝑡 6 𝑘) be the estimates of the mean,
Σ
]
covariance, and 𝑡-th moments, respectively, that are outputted by the Algorithm 4.3 subroutine in
Step 2 of Algorithm 4.8. Then, by Lemma 4.5, we have

𝛾 > 𝐶0 with probability at least 29
/

, and

𝜇 =

Σ
]

Let

𝑀(

˜𝜁[

˜𝜁[

˜𝜁[

𝔼

𝔼

e

e

e

e

e

+

𝜇

]

,

𝑡

e

/

(cid:0)

(cid:1)

𝒜

ℝ𝑑,

𝑢

∀

∈

𝜇

h

−

6 𝑂

√𝐶 𝑘

2𝑘
1
𝜂1
/
−

𝑢⊤Σ
∗

𝑢

𝜇

, 𝑢

∗

i
Σ
𝛽2)

1
(
e

Σ

(cid:16)
1
(cid:22) (

(cid:17)
Σ
𝛽2)
+
∗

p

∗ (cid:22)
and, for all even 2 6 𝑡 6 𝑘 such that 𝑡 divides 2𝑘,
e

−

where 𝛽𝑡 = 𝛽𝑡
= 𝑂
Note that this guarantees that 𝛽𝑡 = 𝛽𝑡

𝜂
(

𝐶 𝑘

((

)

)

ℝ𝑑 ,

𝑢

∀

∈

1
(
𝑡

2
/

𝛽𝑡

−
𝜂1
−
)

𝑡

𝑡
)
∗ i

𝑢 ⊗

𝑡 , 𝑀(

6

𝑢 ⊗

𝑡 ,

𝑀(

𝑡

)

6

+
h
)h
)h
6 1
2𝑘 . We now set 𝜂0 such that 𝛽𝑡
𝜂0)
/
(

e

2 , since we are assuming 𝜂 6 𝜂0.

6 1

i

1
(

𝛽𝑡

𝑢 ⊗

𝑡 , 𝑀(

𝑡
)
∗ i

,

𝜂
(

)

2 for all aforementioned 𝑡.

Now, consider the noise addition step, i.e., Step 3 of Algorithm 4.8. Note that by the Cauchy-

Schwarz Inequality, for any 𝑢

ℝ𝑑, we have

∈
2𝑧, 𝑢
Σ1
/
h

i

e

=

6

=

6

2
2𝑧, Σ1
Σ1
/
/
∗

𝑢

2
1
Σ−
/
h
∗
2
1
2𝑧
Σ1
Σ−
/
/
e
∗
(cid:13)
𝑧𝑇
1
2Σ−
Σ1
(cid:13)
/
e
(cid:13)
(
∗

i
2
Σ1
/
∗

𝑢

𝑢

2 ·
2
(cid:13)
(cid:13)
(cid:13)
2𝑧
Σ1
𝑢⊤Σ
(cid:13)
(cid:13)
(cid:13)
/
(cid:13)
(cid:13)
(cid:13)
) ·
∗
2
Σ1
1
2Σ−
Σ1
p
/
/
e
∗
(cid:13)
2𝛽2
(cid:13)
(cid:13)e
+
·
𝑢𝑇Σ
𝑢,
(cid:1)
∗

𝑢⊤Σ
e
∗

p

+

−
𝑢

k

𝑧

𝑧

1

(cid:16)
1

2
2 ·
e
k
2
2 ·
k
2
𝑧
(cid:0)
2 ·

k

k

6

k
6 2

𝑢⊤Σ
∗

𝑢

·

2

(cid:17)

p

𝐼

(cid:13)
(cid:13)
(cid:13)

p
since 𝛽2 6 1
2 by our choice of 𝜂0. Now, note that with probability at least 1
6 𝑂
𝑧
𝜎1

, in which case it follows that

𝑑 ln

𝑘𝑑

1
30𝑘 , we have that

−

k

k2

(

)

(cid:16)

p

(cid:17)

2
2 = 𝑂

𝑧

k

k

𝑑

(

) ·

𝜎2
1 ln

𝑘𝑑

)

(

= 𝑂

√𝐶 𝑘
(

2𝑘 ,
1
𝜂1
/
−
)

40

by our choice of 𝑛 > 𝑛0. Thus, the mean estimate

𝜇
h ˆ

−

, 𝑢

𝜇

∗

i

𝜇

𝜇

=

=

, 𝑢

i + h

𝜇 outputted by the Step 3 satisﬁes
ˆ
𝜇, 𝑢
𝜇
h ˆ
−
2𝑧, 𝑢
Σ1
/
h
e
√𝐶 𝑘
= 𝑂
e
(cid:16)

𝜇
i + h
−
e
2𝑘
1
𝜂1
/
−
e

i
∗
𝑢𝑇Σ
∗

, 𝑢

𝑢.

−

𝜇

(cid:17)

i

∗

p

(4.35)

Next, we consider the utility guarantee for the covariance. Note that

𝑂

𝜎2

𝑑 ln

𝑘𝑑2

)

(

with probability at least 1

−

(cid:16)

p

Wigner matrices; see, for instance, [Tao12]), in which case, it follows that
Moreover, by our choice of 𝑛 > 𝑛0 as well as 𝜂0, we have that 𝜈2 6 𝛽2 6 1

(cid:17)

6 𝜈2 =
1
30𝑘 (this follows from standard spectral properties of
Σ.

k2

Σ

𝑍

k

𝜈2

𝜈2

2
Σ1
/

2𝑍
Σ1
/
(cid:22)
−
2 . Thus, it follows that
e

e

e

(cid:22)

e

Σ
ˆ

1
(cid:22) (
1
(cid:22) (

+

+

Σ
𝛽2)
2Σ
𝛽2)
∗
e
5
𝛽2
2

(cid:19)
𝐶 𝑘

Σ
∗

+

(cid:18)

1

1

(cid:22)

(cid:22)

𝑂

(

+

(cid:16)
Σ
and by a similar argument, we also have ˆ

1

𝑂

𝐶 𝑘

1
𝜂1
/
−

𝑘

) ·

𝑂

1
𝜂1
/
−

𝑘

𝐶 𝑘

(

) ·

,

Σ
∗

(cid:17)
1
𝜂1
/
−

𝑘

1

𝑂

𝐶 𝑘

(cid:1)
) ·

1

(cid:23)
Σ

−
(cid:0)
Σ
∗ (cid:22) ˆ

, thus implying that

Σ
∗
1
𝜂1
/
−

𝑘

.

Σ
∗

(4.36)

(

−

(cid:22)
Finally, we consider the utility guarantee for moment estimation. Suppose 2 6 𝑡 6 𝑘 and 𝑡 is
. Note that for any 2 < 𝑡 6 𝑘, we
𝜇𝑇
∗
1
30𝑘 . In this case, note that for any

∗ +
with probability at least 1

an even number dividing 2𝑘. Let 𝐴 =

𝜇𝑇 and 𝐴

𝜎𝑡 𝑑𝑡

have

= 𝑂

= Σ

𝑘𝑑𝑡

𝑍(

ln

) ·

Σ

+

+

𝜇

𝜇

(cid:17)

(cid:17)

(cid:16)

(cid:16)

(

∗

∗

)

𝑡

𝐹

2
/
ℝ𝑑, we have the following (recall that
(cid:13)
(cid:13)

(cid:13)
(cid:13)

e

(cid:16)

(cid:17)

(

)

𝑢

∈

p
2
𝐴1
/

𝑡 ,

𝑢 ⊗

h

(

𝑡

𝑡 𝑍(

⊗

)

)

i

=

6

−

e
e
k·k𝜎 indicates the injective norm of a tensor):
2𝑢
𝐴1
/

𝑡 , 𝑍(

⊗

)

𝑡

)

𝜎 ·

h(
𝑍(

𝑡

)

𝑍(

6

(cid:13)
(cid:13)
= 𝑂
(cid:13)
(cid:13)
= 𝑂

= 𝑂

= 𝑂

(

(

(

(

(cid:13)
(cid:13)
(cid:13)
(cid:13)
p

𝑡

)

(cid:13)
(cid:13)
𝐹 ·
𝜎𝑡 𝑑𝑡
2
(cid:13)
/
(cid:13)
𝜎𝑡 𝑑𝑡
2
/
𝑑𝑡

𝜎𝑡

(

2
p
/

𝜎𝑡

(

𝑑

p
1
+
(
𝑡
𝑑𝑒 𝛽2

i
2𝑢
𝐴1
/

2𝑢
𝐴1
/

𝑡
2
𝑡
(cid:13)
(cid:13)
2
𝑘𝑑𝑡
(cid:13)
(cid:13)
)) ·
𝑘𝑑𝑡

(

ln

ln

(
ln

𝑘𝑑𝑡

𝑡

(
𝛽2))
ln

2
/

𝑡
2
𝑡
(cid:13)
(cid:13)
)
𝛽2)

2𝑢
𝐴1
/

𝑢𝑇 𝐴𝑢
(cid:13)
(cid:13)
)) · (

1
)) · ((
2
ln
/

+
𝑘𝑑𝑡

𝑡

2
/

𝑡

𝑢

)
𝑢

𝑢𝑇 𝐴

∗
2
𝐴1
/
∗
(cid:13)
𝑡 , 𝑀(
(cid:13)
(cid:13)
,

𝑡
)
∗ i

2
(cid:13)
(cid:13)
(cid:13)

(

)) ·
𝑢 ⊗
𝑡
𝑡 , 𝑀(
)
∗ i

𝜎𝑡

= 𝑂

2
/
)
(
2𝑘
𝑡
𝜂1
p
/
−
)
where (4.37) follows from Jensen’s Inequality, and (4.38) follows from our choice of 𝑛 > 𝑛0. Thus,
the moment estimate ˆ𝑀(
𝑡, ˆ𝑀(

) outputted by our algorithm satisﬁes

) + (
𝑡 ,
𝑢 ⊗

(
𝐶 𝑘

(4.37)

(4.38)

2
𝐴1
/

2
𝐴1
/

= 𝑂

)) · h

𝑡 𝑍(

𝑡 𝑍(

) =

𝑀(

𝑀(

𝑢 ⊗

𝑢 ⊗

𝑢 ⊗

)⊗

· h

2
/

𝑡,

((

⊗

)

(

)

)

)

𝑡

𝑡

𝑡

𝑡

𝑡

𝑡

𝑡

i + h

(

)

i

6
e
h

i

h

p
𝑘𝑑𝑡

e

41

6

=

𝛽𝑡

𝑂

+

+

1
(

1

(cid:16)

𝑡 , 𝑀(

𝑢 ⊗

)h

𝐶 𝑘

𝑡

2
/

)

((

𝑡
)
∗ i +
𝑡
𝜂1
−
)

2𝑘
/

𝐶 𝑘

𝑂

((

𝑢 ⊗

h

𝑡

2
/

𝜂1
−
)
)
𝑡 , 𝑀(

𝑡
)
∗ i

.

𝑡

2𝑘
/

𝑢 ⊗

𝑡 , 𝑀(

𝑡
)
∗ i

· h

(cid:17)

−

(cid:0)
𝑡

)

𝑢 ⊗

𝑡, ˆ𝑀(

i

In a similar fashion, we also get that
that

h

𝑢 ⊗

𝑡

𝑡 , ˆ𝑀(

)i

>

1

𝐶 𝑘

𝑂

((

𝑡

2
/

𝜂1
−
)

)

𝑡

2𝑘
/

𝑢 ⊗

h

𝑡 , 𝑀(

𝑡
)
∗ i

, thus implying

1

𝑂

𝐶 𝑘

−

((

𝑡

2
/

𝜂1
−
)

)

𝑡

2𝑘
/

h

𝑢 ⊗

𝑡 , 𝑀(

6

𝑡
)
∗ i

h

6

1

𝑂

𝐶 𝑘

+

((

𝑡

2
/

)

𝑡

2𝑘
/

𝑢 ⊗

𝑡 , 𝑀(

.

(4.39)

𝑡
)
∗ i

h

(cid:16)

(cid:17)

(cid:16)
Hence, (4.35), (4.36), and (4.39) imply the desired utility guarantees.
Moreover, recall that the rejection probabilities at Steps 1 and 2 are each at most 1

30 , and it is not
possible to reject in Step 3. Moreover, the 6 𝑘 utility guarantees each fail with probability at most
1
30𝑘 . Thus, by a union bound, it follows that the algorithm does not reject and, moreover, outputs
30𝑘 = 9
10 .
estimates satisfying the desired utility guarantees with probability at least 1
) follows from the time complexity guarantee in
(
Lemma 4.5, as the invocation of Algorithm 4.3 in Step 2 is the bottleneck. Steps 1 and 3 are easily
(cid:3)
seen to run in

Finally, note that the running time of

) time. This completes the proof.

1
30 −

1
30 −

𝐵𝑛

𝐵𝑛

−

𝑘

𝑂

𝑂

(cid:17)

(

)

1

·

𝑘

𝑘

(

(

)

(cid:1)
𝜂1
−
)

5 Robust Mean and Covariance Estimation for Certiﬁably Hypercon-

tractive Distributions

In this section, we observe that we can upgrade our guarantees from the previous section for robust
estimation of moments of distributions that have certiﬁably hypercontractive degree 2 polynomials.

Deﬁnition 5.1. A distribution 𝑐𝐷 on ℝ𝑑 with mean 𝜇
certiﬁably 𝐶-hypercontractive degree 2 polynomials if for a 𝑑
and

𝜇

,

∗

×

𝑥 = 𝑥
¯

−

∗

and covariance Σ
∗

is said to have 2ℎ-
𝑑 matrix-valued indeterminate 𝑄

𝑄
2ℎ

𝑥

(cid:26)

𝔼
∼

𝑥⊤𝑄

𝐷( ¯

𝑥
¯

−

𝑥

𝔼
∼

𝐷 ¯

𝑥⊤𝑄

2ℎ 6
)

𝑥
¯

(

𝐶 ℎ

2ℎ
)

2
Σ1
/
∗

2
𝑄Σ1
/
∗

.

2ℎ

𝐹

(cid:27)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

The Gaussian distribution [KOTZ14], uniform distribution on the hypercube and more gen-
erally other product domains and their aﬃne transforms are known to satisfy 2𝑡-certﬁably 𝐶-
hypercontractivity with an absolute constant 𝐶 for every 𝑡.

In order to derive this conclusion, we note the following analog of the witness-producing

algorithm and its guarantees:

Witness-producing version of the robust moment estimation algorithm We will use the follow-
ing (non-private) guarantees for the robust moment estimation algorithm in the previous section
that hold for a strengthening of the constraint system
with certiﬁable hypercontractivity con-
straints. Using the analysis of [DKK+16], the following guarantees were recently shown in [KMZ21]
for the case when the unknown distribution is Gaussian.

𝒜

For any 𝑑

×

𝑑 matrix-valued indeterminate 𝑄, let ¯𝑥′𝑖

⊤𝑄 ¯𝑥′𝑖

= 𝑥′⊤𝑄 𝑥′ −

1
𝑛

𝑛
𝑖=1 𝑥′𝑖 ⊤𝑄 𝑥′.

Í

42

: Constraint System for 𝜂-Robust Moment Estimation

𝒜

1. 𝑤2
𝑖

= 𝑤𝑖 for each 1 6 𝑖 6 𝑛,

2. Π2 = 1
𝑛

3.

𝑛

𝑖=1 𝑤𝑖 >
Í

4. 𝜇′ = 1
Í
𝑛

𝑛
𝑖=1(
1
(
𝑖 𝑥′𝑖,

𝑥′𝑖

𝜂

−

𝑥′𝑖

𝜇′)⊤,

−

𝜇′)(
−
𝑛,

)

5. 𝑤𝑖

(

6. 1
𝑛

Í
𝑦𝑖
𝑥′𝑖 −
𝑛
𝑖=1 ¯𝑥′𝑖

= 0 for 1 6 𝑖 6 𝑛,

)

⊤𝑄 ¯𝑥′𝑖

2 6 𝐶

Π𝑄Π

k

2
𝐹.

k

The following guarantees for the algorithm above were shown in [BK20a].

Í

Fact 5.2 ([BK20a]). Let 𝑋
𝜂-corruption of 𝑋. Then, for 𝜇′ = 1
𝑛

⊆

𝑖 𝑥′𝑖, Σ′ = 1

𝑛

ℝ𝑑 be an i.i.d. sample of size 𝑛 > 𝑛0 =

, Σ

𝜇

𝒩(

∗

∗)

. Let 𝑌 be an

)

(

𝑑2
𝑂
from
𝜂
/
𝜇′)⊤, we have:
−
e
𝑢⊤Σ
,
∗

𝑢2

𝑥𝑖

𝑖(
6 𝑂

𝑥𝑖

𝜇′)(
−
2𝑘
1
𝜂1
/
−
(
6 𝑂

)
𝑘

Í
)

𝑢
𝑘
𝒜 𝑂
(
𝑢
𝑘
𝒜 𝑂

(

)

(cid:8)
h

𝜇′

−
h
𝑢, Σ′

, 𝑢

𝜇

∗

Í
i

, 𝑢

Σ
∗

i

−

𝒜 𝑂

𝑘

(

2
1
Σ−
/
∗

2
1
Σ′Σ−
/
∗

𝐼

−

(cid:8)
) (cid:26)(cid:13)
(cid:13)
(cid:13)

1
𝜂1
/
−
(
2

6 𝑂

(cid:9)
𝑢

,

𝑢⊤Σ
∗

)

𝑘

1
𝜂1
/
−
(

)

(cid:9)

.

(cid:27)

𝐹

(cid:13)
(cid:13)
(cid:13)

The ﬁrst two guarantees of the lemma below were shown in [BK20a]. The third guarantee
follows from an argument similar to that of Lemma 4.6. Notice that the key diﬀerence in the
guarantees below (compared to the ones in Lemma 4.5) is the bound on the Frobenius (instead of
the weaker spectral) distance between the estimated covariance and true unknown covariance.

⊆

Lemma 5.3 (Guarantees for Witness-Producing Robust Moment Estimation Algorithm). Given a
1
subset of of 𝑛 points 𝑌
(
)
returns a sequence of weights 0 6 𝑝1, 𝑝2, . . . , 𝑝𝑛 satisfying
and either (a.) outputs “reject,” or (b.)
𝑝𝑛 = 1.
𝑝1 +
𝑝2 + · · · +
Moreover, if there exists a set 𝑋

ℚ𝑑 whose entries have bit complexity 𝐵, Algorithm 4.3 runs in time

, then Algorithm 4.3 does not reject, and the corresponding estimates

ℝ𝑑 of points with 4-certiﬁably 𝐶-hypercontractive degree 2 polynomials
𝜇 =
ˆ

𝐵𝑛

⊆

𝑂

)

(

∗

𝜇
− ˆ

)(

𝑦𝑖

𝜇
− ˆ

)⊤ satisfy the following guarantees:

with mean 𝜇
1
𝑛

, covariance Σ
∗
𝑛
Σ =
𝑖 𝑝𝑖 𝑦𝑖 and ˆ
𝑦𝑖
𝑖=1 𝑝𝑖
1. Mean Estimation:
Í

Í

(

𝑢

∀

∈

ℝ𝑑 ,

𝜇
h ˆ

−

, 𝑢

𝜇

∗

i

6 𝑂

√𝐶
(

4
𝜂3
/
)

𝑢⊤Σ
∗

𝑢 ,

2. Covariance Estimation:

3. Witness: For 𝐶′ 6 𝐶

𝑂

1
(

+

2
1
Σ−
/
∗

(cid:13)
(cid:13)
2
𝜂1
/
(cid:13)
(

,
))

p
2
𝐶𝜂1
/

)

,

6 𝑂

(

2
1
ΣΣ−
/
ˆ
∗

𝐼

−

𝐹

(cid:13)
(cid:13)
(cid:13)

𝑝𝑖

𝑦𝑖

 h

𝜇, 𝑄

− ˆ

𝑦𝑖

(

𝜇
− ˆ

)i −

1
𝑛

𝑛

Õ𝑖=1

𝑄





𝑝𝑖

𝑦𝑖

h

𝜇, 𝑄

− ˆ

𝑦𝑖

(

𝜇
− ˆ
)i!

1
𝑛

𝑛

Õ𝑖=1

43

2

6 𝐶′

Σ1
/
ˆ

2
Σ1
2𝑄 ˆ
/

(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
(cid:13)

𝐹 



We can now use the above witness-producing algorithm to obtain a stronger Frobenius norm
-privacy for Gaussian distributions. Notice that the only change
estimation guarantee with
)
and the corresponding change
from the previous section is in the choice of the constraint system
in the witness checking step.

𝜀, 𝛿

𝒜

(

Algorithm 5.4 (Private Robust Moment Estimation).

Given: A set of points 𝑌 =

𝑦1, 𝑦2, . . . , 𝑦𝑛

{

} ⊆

ℚ𝑑, parameters 𝜂, 𝜀, 𝛿 > 0, 𝐿

ℕ.

∈

Output: Estimates

𝜇 and ˆ
ˆ

Σ for mean and covariance.

Operation:

1. Stable Outlier Rate Selection: Use the

2 to
with the scoring function as deﬁned in Deﬁnition 4.12.

-DP Selection with 𝜅 = 𝐿
3
)

3, 𝛿

𝜀

/

/

/

(

sample an integer 𝜏
If 𝜏 =

𝜂𝑛

]

∈ [

, then reject and halt. Otherwise, let 𝜂′ = 𝜏

⊥

𝑛.

/

3 ln

𝒜

ing

tLap

2. Witness Checking: Compute a pseudo-distribution ˜𝜁 of degree 𝑂
1
)
(
. Let 𝛾
)
𝑤

𝑌
on input 𝑌 with outlier rate 𝜂′ and minimizing Pot𝜂′, ˜𝜁(
𝔼
1
˜𝜁[

. Check that the weight vector 𝑝 =
𝐶

distribution on 𝑌 that has
-certiﬁably hypercontractive polynomials. If not,
𝛾
(cid:17)
)
Σ
𝜇 =
reject immediately. Otherwise, let
]
𝐿

3. Noise Addition: Let 𝛾1 = 𝑂

∼
induces a

1
4 . Let 𝑧

satisfy-

, 3
/

and

Σ =

3
(
/
𝜀

𝔼

𝔼

e

+

−

+

𝜇

𝑛

𝑛

𝜀

(cid:16)

(cid:17)

(cid:16)

]

(

𝑑

𝛿

.

)

]

˜𝜁[
1
4 and 𝛾2 = 𝑂
e
)

e

˜𝜁[
𝐶′)(
e

(

/

)

𝑑

1

𝐿
𝐶′)(
/
e

(

+

∼ 𝒩(

0, 𝜎2)(

and 𝑍
independent lower-triangular entries, and 𝜎𝑗 = 12𝜀−
Then, output:

2 ), where we interpret 𝑍 has a symmetric 𝑑
15
/
(

1𝛾𝑗

2 ln

×
𝛿
)

∼ 𝒩(

0, 𝜎1)
𝑑 matrix with
for 1 6 𝑗 6 2.

p

•

•

𝜇 =
ˆ
Σ =
ˆ

+

+

𝜇
Σ
e
e

2𝑧.
Σ1
/
2𝑍
Σ1
/
e
e

e

2.
Σ1
/

The parameter closeness from potential stability is also upgraded from Corollary 4.19:

Lemma 5.5 (Parameter Closeness from Stability of Potential). Let 𝜂, 𝜀, 𝛿 > 0 and 𝐿
∈
input parameters to Algorithm 5.4 such that 0.25𝜂𝑛 > 𝐿 = Ω
. Also, let 𝑌, 𝑌′ be adjacent
subsets of ℚ𝑑. Suppose Algdoes not reject in any of the 3 steps, uses the constant 𝐶′ in Step 2 and chooses 𝜂′
in Step 1 on input 𝑌 and 𝑌′.

1
𝜀 ·

ℕ be given

log

𝑛
𝛽𝛿

(cid:17) (cid:17)

(cid:16)

(cid:16)

Then, for every 𝑢

ℝ𝑑 and 𝜃 =

𝐿

∈

𝑛, we have:

and

p
𝜇𝑝

h

/
𝜇𝑝′ , 𝑢

−

6 𝑂

4
𝜃3
/

𝐶′
)

(

i

𝑢⊤Σ𝑝𝑢

q

1
2
𝑝 Σ𝑝′
Σ−
/

2
1
Σ−
/
𝑝

𝐼

−

6 𝑂

𝐶′

2 .
𝜃1
/

(

)

𝐹
(cid:13)
The following theorem summarizes our privacy and utility guarantees for the algorithm above.
(cid:13)
(cid:13)
We specialize to the “base case assumption” of 4-certiﬁable 𝐶-hypercontractivity of degree 2 poly-
nomials in order to derive explicit bounds here. Our analysis of the algorithm above follows mutatis

(cid:13)
(cid:13)
(cid:13)

44

mutandis with the key upgrade being the stronger Frobenius norm guarantees in Lemma 4.18 that
(this requires us
hold under certiﬁably hypercontractivity constraints in our constraint system
𝒜
2; see
to use a version of Lemma 4.21 that makes use of a bound on
𝐹 instead of
Ω notation hides logarthmic multiplicative
the remark at the end of Lemma 4.21). As before, the
(cid:13)
(cid:13)
factors in 𝑑, 𝐶, 1
𝜀, and ln
𝜂, 1
/
/

1
/
(

𝐴𝐴𝑇

𝐴𝐴𝑇

.
)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

−

−

𝛿

𝐼

𝐼

Theorem 5.6 (Private Robust Mean and Covariance Estimation for Certiﬁably Hypercontractive
Distributions). Fix 𝐶0 > 0. Then, there exists an 𝜂0 > 0 such that for any given outlier rate 0 < 𝜂 6 𝜂0 and
𝜀, 𝛿 > 0, there exists a randomized algorithm Alg that takes an input of 𝑛 > 𝑛0 =
𝐶4
ℚ𝑑 (where 𝐶 = 𝐶0 +

points 𝑌 =
bit complexity of the entries of 𝑌) and outputs either “reject” or estimates
following guarantees:

(cid:18)
(cid:16)
𝑂
1), runs in time
𝐵𝑛
(
e
(
)
ℚ𝑑 and ˆ
Σ

+
(cid:19)
(cid:17)
1
) (where 𝐵 is the
𝑑 with the

𝑦1, 𝑦2, . . . , 𝑦𝑛

9
𝜀 +

1
𝛿
)𝜀
/

3
(
/
𝜀

} ⊆

ℚ𝑑

𝑑8
𝜂2

𝜇
ˆ

Ω

3 ln

+

∈

∈

1

ln

{

×

4

𝛿

·

(

)

e

1. Privacy: Alg is

𝜀, 𝛿

-diﬀerentially private with respect to the input 𝑌, viewed as a 𝑑-dimensional
)

(

database of 𝑛 individuals.

2. Utility: Suppose there exists a 4-certiﬁably 𝐶0-subgaussian set 𝑋 =
𝑑
poly

and covariance Σ

2−

(

>

∩
10 over the random choices of the algorithm, Alg outputs estimates

∗ (cid:23)

−

∗

𝑋

that
𝑌
|
least 9
/
satisfying the following guarantees:

𝜂0)

1
(

|

𝑛 with mean 𝜇

𝑥1, 𝑥2, . . . , 𝑥𝑛

ℚ𝑑 such
{
)𝐼. Then, with probability at
𝑑
ℚ𝑑 and ˆ
Σ

} ⊆

ℚ𝑑

×

𝜇
ˆ

∈

∈

and,

𝑢

∀

∈

ℝ𝑑 ,

𝜇
h ˆ

−

, 𝑢

𝜇

∗

i

6 𝑂

4
√𝐶𝜂3
/
(

𝑢⊤Σ
∗

𝑢 ,

)
p

(cid:13)
(cid:13)
Moreover, the algorithm succeeds (i.e., does not reject) with probability at least 9
(cid:13)
/
of the algorithm.

(cid:13)
(cid:13)
(cid:13)

2
1
Σ−
/
∗

2
1
ΣΣ−
/
ˆ
∗

𝐼

𝑂

𝐶√𝜂

.

−

𝐹 (cid:22)

(

)

10 over the random choices

When specialized to Gaussian distributions, the Frobenius guarantee above is suboptimal—
the robust estimation algorithms of [DKK+16] allow estimating the mean and covariance of the
unknown Gaussian distribution to an error
. We can in fact recover the stronger guarantees
𝜂
)
(
by relyong on the analysis in [KMZ21][Theorem 1 and 2] of the same constraint system above for
e
the case of Gaussian distributions (in the “utility case”). This yields the following corollary:

𝑂

Theorem 1.3 (Mean and Covariance Estimation for Gaussian Distributions). Fix 𝜀, 𝛿 > 0. Then,
there exists an absolute constant 𝜂0 > 0 such that for any given outlier rate 0 < 𝜂 6 𝜂0, there exists a
randomized algorithm Alg that takes an input of 𝑛 > 𝑛0 =
ℚ𝑑, runs in

points 𝑌

Ω

1

ln

4

(

𝑑8
𝜂4

(cid:18)

(cid:16)

+

1
𝛿
)𝜀
/

(cid:19)

(cid:17)

⊆

(

𝑂

time
𝜇
ˆ

1
) (where 𝐵 is the bit complexity of the entries of 𝑌) and outputs either “reject” or estimates
𝐵𝑛
(
)
ℚ𝑑 and ˆ
Σ
∈
1. Privacy: Alg is

-diﬀerentially private with respect to the input 𝑌, viewed as a 𝑑-dimensional
)

𝑑 with the following guarantees:

𝜀, 𝛿

ℚ𝑑

e

∈

×

(

database of 𝑛 individuals.

45

{

2. Utility: Let 𝑋 =
with mean 𝜇
probability at least 9
/
Σ
ˆ

ℚ𝑑

×

∗

∈

and covariance Σ

𝑥1, 𝑥2, . . . , 𝑥𝑛

be an i.i.d. sample of size 𝑛 > 𝑛0 from a Gaussian distribution
)𝐼 such that 𝑌 is an 𝜂-corruption of 𝑋. Then, with
2−
ℚ𝑑 and

10 over the random choices of the algorithm, Alg outputs estimates

}
∗ (cid:23)

poly

𝑑

(

𝜇
ˆ

∈

𝑑 satisfying the following guarantees:

ℝ𝑑,

𝑢

∀

∈

𝜇
h ˆ

−

, 𝑢

𝜇

∗

i

log

𝛿

)

1
(
/
𝜀

·

𝑢⊤Σ
∗

𝑢 ,

(cid:19) p

6

𝑂

𝜂

(cid:18)

e

and,

2
1
Σ−
/
∗

2
1
ΣΣ−
/
ˆ
∗

𝐼

−

𝑂

𝜂

𝐹 (cid:22)

log

𝛿

)

1
(
/
𝜀

,

!

· r

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

e

where the
𝜂. In particular, 𝑑TV
𝑂 hides multiplicative logarithmic factors in 1
/
𝑂
1
𝜂 log
𝛿
/
(
(
e

.
)

)/

𝜀

Σ
𝜇, ˆ
)

(𝒩( ˆ

,

𝜇

𝒩(

∗

, Σ

∗))

<

e
References

[AAK21]

Ishaq Aden-Ali, Hassan Ashtiani, and Gautam Kamath. On the sample complexity of
privately learning unbounded high-dimensional gaussians. In Vitaly Feldman, Katrina
Ligett, and Sivan Sabato, editors, Algorithmic Learning Theory, 16-19 March 2021, Virtual
Conference, Worldwide, volume 132 of Proceedings of Machine Learning Research, pages
185–216. PMLR, 2021. 2

[Abo18]

John M. Abowd. The u.s. census bureau adopts diﬀerential privacy. KDD ’18, page
2867, New York, NY, USA, 2018. Association for Computing Machinery. 2

[AL21]

Hassan Ashtiani and Christopher Liaw. Private and polynomial time algorithms for
learning gaussians and beyond. CoRR, abs/2111.11320, 2021. 1, 7

[App17]

Apple Diﬀerential Privacy Team. Learning with privacy at scale. Apple Machine
Learning Journal, 2017. 2

[BDH+20] A. Bakshi, I. Diakonikolas, S. B. Hopkins, D. Kane, S. Karmalkar, and P. K. Kothari.
Outlier-robust clustering of gaussians and other non-spherical mixtures. In 61st IEEE
Annual Symposium on Foundations of Computer Science, FOCS 2020, pages 149–159. IEEE,
2020. 6

[BDKU20] Sourav Biswas, Yihe Dong, Gautam Kamath, and Jonathan R. Ullman. Coinpress:
Practical private mean and covariance estimation. In Hugo Larochelle, Marc’Aurelio
Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances
in Neural Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 2

[BEM+17] Andrea Bittau, Úlfar Erlingsson, Petros Maniatis, Ilya Mironov, Ananth Raghunathan,
David Lie, Mitch Rudominer, Ushasree Kode, Julien Tinnés, and Bernhard Seefeld.

46

 
Prochlo: Strong privacy for analytics in the crowd. In Proceedings of the 26th Symposium
on Operating Systems Principles, Shanghai, China, October 28-31, 2017, pages 441–459.
ACM, 2017. 2

[BGS+21] Gavin Brown, Marco Gaboardi, Adam D. Smith, Jonathan R. Ullman, and Lydia
Zakynthinou. Covariance-aware private mean estimation without private covariance
estimation. CoRR, abs/2106.13329, 2021. 2

[BK20a]

[BK20b]

[BK20c]

[BKS15]

A. Bakshi and P. Kothari. Outlier-robust clustering of non-spherical mixtures. CoRR,
abs/2005.02970, 2020. 2, 14, 43

Ainesh Bakshi and Pravesh Kothari. Outlier-robust clustering of non-spherical mix-
tures. 2020. 5, 6, 17

Ainesh Bakshi and Pravesh Kothari. Outlier-robust clustering of non-spherical mix-
tures. CoRR, abs/2005.02970, 2020. 6

B. Barak, J. A. Kelner, and D. Steurer. Dictionary learning and tensor decomposition
via the sum-of-squares method [extended abstract]. In STOC’15—Proceedings of the
2015 ACM Symposium on Theory of Computing, pages 143–151. ACM, New York, 2015.
16

[BKSW19] Mark Bun, Gautam Kamath, Thomas Steinke, and Zhiwei Steven Wu. Private hypoth-
esis selection. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information
Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019,
NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 156–167, 2019. 2

[BP20]

[BS19]

A. Bakshi and A. Prasad. Robust linear regression: Optimal rates in polynomial time.
arXiv preprint arXiv:2007.01394, 2020. 14

Mark Bun and Thomas Steinke. Average-case averages: Private algorithms for smooth
In Hanna M. Wallach, Hugo Larochelle, Alina
sensitivity and mean estimation.
Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Ad-
vances in Neural Information Processing Systems 32: Annual Conference on Neural Informa-
tion Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,
pages 181–191, 2019. 2

[BUV14] Mark Bun, Jonathan Ullman, and Salil P. Vadhan. Fingerprinting codes and the price
of approximate diﬀerential privacy. In STOC, pages 1–10. ACM, 2014. 7

[CDGW19] Yu Cheng, Ilias Diakonikolas, Rong Ge, and David P. Woodruﬀ. Faster algorithms for
high-dimensional robust covariance estimation. In Alina Beygelzimer and Daniel Hsu,
editors, Conference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA,
volume 99 of Proceedings of Machine Learning Research, pages 727–757. PMLR, 2019. 6

47

[CKM+20] Clément L. Canonne, Gautam Kamath, Audra McMillan, Jonathan R. Ullman, and
Lydia Zakynthinou. Private identity testing for high-dimensional distributions. In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. 7

[CWZ19]

T. Tony Cai, Yichen Wang, and Linjun Zhang. The cost of privacy: Optimal rates of
convergence for parameter estimation with diﬀerential privacy. CoRR, abs/1902.04495,
2019. 2

[DFM+20] Wenxin Du, Canyon Foot, Monica Moniot, Andrew Bray, and Adam Groce. Diﬀeren-

tially private conﬁdence intervals. CoRR, abs/2001.02285, 2020. 2

[DHKK20]

Ilias Diakonikolas, Samuel B. Hopkins, Daniel Kane, and Sushrut Karmalkar. Robustly
learning any clusterable mixture of gaussians. CoRR, abs/2005.06417, 2020. 2, 6, 14

[DHL19]

Yihe Dong, Samuel B. Hopkins, and Jerry Li. Quantum entropy scoring for fast ro-
bust mean estimation and improved outlier detection. In Hanna M. Wallach, Hugo
Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Gar-
nett, editors, Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Van-
couver, BC, Canada, pages 6065–6075, 2019. 6

[DK19]

Ilias Diakonikolas and Daniel M. Kane. Recent advances in algorithmic high-
dimensional robust statistics. CoRR, abs/1911.05911, 2019. 6

[DKK+16]

I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Robust
estimators in high dimensions without the computational intractability. In Proc. 57th
IEEE Symposium on Foundations of Computer Science (FOCS), pages 655–664, 2016. 2, 6,
42, 45

[DKK+17a] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Al-
istair Stewart. Being robust (in high dimensions) can be practical. In ICML, volume 70
of Proceedings of Machine Learning Research, pages 999–1008. PMLR, 2017. 6

[DKK+17b] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and
Alistair Stewart. Robustly learning a gaussian: Getting optimal error, eﬃciently. CoRR,
abs/1704.03866, 2017. 6

[DKM+06] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni
Naor. Our data, ourselves: Privacy via distributed noise generation. In Advances in
Cryptology - EUROCRYPT 2006, 25th Annual International Conference on the Theory and
Applications of Cryptographic Techniques, St. Petersburg, Russia, May 28 - June 1, 2006,
Proceedings, volume 4004 of Lecture Notes in Computer Science, pages 486–503. Springer,
2006. 36

48

[DKY17]

Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin. Collecting telemetry data
privately. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach,
Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural
Information Processing Systems 30: Annual Conference on Neural Information Processing
Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 3571–3580, 2017. 2

[DL09]

Cynthia Dwork and Jing Lei. Diﬀerential privacy and robust statistics. In Michael
Mitzenmacher, editor, Proceedings of the 41st Annual ACM Symposium on Theory of
Computing, STOC 2009, Bethesda, MD, USA, May 31 - June 2, 2009, pages 371–380. ACM,
2009. 21, 54

[DLCC07] Lieven De Lathauwer, Joséphine Castaing, and Jean-François Cardoso. Fourth-order
cumulant-based blind identiﬁcation of underdetermined mixtures. IEEE Trans. Signal
Process., 55(6, part 2):2965–2973, 2007. 5

[DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating
noise to sensitivity in private data analysis.
In Shai Halevi and Tal Rabin, editors,
Theory of Cryptography, Third Theory of Cryptography Conference, TCC 2006, New York, NY,
USA, March 4-7, 2006, Proceedings, volume 3876 of Lecture Notes in Computer Science,
pages 265–284. Springer, 2006. 2, 20

[DN03]

[DR14]

[DSS+15]

Irit Dinur and Kobbi Nissim. Revealing information while preserving privacy.
PODS, pages 202–210. ACM, 2003. 7

In

Cynthia Dwork and Aaron Roth. The algorithmic foundations of diﬀerential privacy.
Found. Trends Theor. Comput. Sci., 9(3-4):211–407, 2014. 37, 39

Cynthia Dwork, Adam D. Smith, Thomas Steinke, Jonathan R. Ullman, and Salil P.
Vadhan. Robust traceability from trace amounts. In Venkatesan Guruswami, editor,
IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS 2015, Berkeley,
CA, USA, 17-20 October, 2015, pages 650–669. IEEE Computer Society, 2015. 7

[DSSU17] Cynthia Dwork, Adam Smith, Thomas Steinke, and Jonathan Ullman. Exposed! a
survey of attacks on private data. Annual Review of Statistics and Its Application, 4(1):61–
84, 2017. 7

[EPK14]

[FKP19]

Úlfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. RAPPOR: Randomized
aggregatable privacy-preserving ordinal response. In CCS, pages 1054–1067, 2014. 2

Noah Fleming, Pravesh Kothari, and Toniann Pitassi. Semialgebraic proofs and eﬃ-
cient algorithm design. Foundations and Trends® in Theoretical Computer Science, 14(1-
2):1–221, 2019. 14, 15

[Gre16]

Andy Greenberg. Apple’s “diﬀerential privacy” is about collecting your data – but
not your data. Wired, June, 13, 2016. 2

49

[HK13]

Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical Gaussians: mo-
ment methods and spectral decompositions. In ITCS’13—Proceedings of the 2013 ACM
Conference on Innovations in Theoretical Computer Science, pages 11–19. ACM, New York,
2013. 5

[HKM21]

Samuel B. Hopkins, Gautam Kamath, and Mahbod Majid. Eﬃcient mean estimation
with pure diﬀerential privacy via a sum-of-squares exponential mechanism. CoRR,
abs/2111.12981, 2021. 7

[HL18]

[HLZ20]

S. B. Hopkins and J. Li. Mixture models, robustness, and sum of squares proofs. In
Proc. 50th Annual ACM Symposium on Theory of Computing (STOC), pages 1021–1034,
2018. 2, 6, 14

Samuel B. Hopkins, Jerry Li, and Fred Zhang. Robust and heavy-tailed mean estima-
tion made simple, via regret minimization. In Hugo Larochelle, Marc’Aurelio Ranzato,
Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural
Information Processing Systems 33: Annual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 6

[Hop20]

Samuel B. Hopkins. Mean estimation with sub-Gaussian rates in polynomial time.
The Annals of Statistics, 48(2):1193 – 1213, 2020. 6

[JH16]

[JLT20]

Cédric Josz and Didier Henrion. Strong duality in Lasserre’s hierarchy for polynomial
optimization. Optim. Lett., 10(1):3–10, 2016. 15

Arun Jambulapati, Jerry Li, and Kevin Tian. Robust sub-gaussian principal component
analysis and width-independent schatten packing. In Hugo Larochelle, Marc’Aurelio
Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances
in Neural Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 6

[KKM18] A. Klivans, P. Kothari, and R. Meka. Eﬃcient algorithms for outlier-robust regression.

In Proc. 31st Annual Conference on Learning Theory (COLT), pages 1420–1430, 2018. 14

[KLSU19] Gautam Kamath, Jerry Li, Vikrant Singhal, and Jonathan R. Ullman. Privately learning
high-dimensional distributions. In Alina Beygelzimer and Daniel Hsu, editors, Con-
ference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA, volume 99
of Proceedings of Machine Learning Research, pages 1853–1902. PMLR, 2019. 2, 10

[KMS+21] Gautam Kamath, Argyris Mouzakis, Vikrant Singhal, Thomas Steinke, and Jonathan R.
Ullman. A private and computationally-eﬃcient estimator for unbounded gaussians.
CoRR, abs/2111.04609, 2021. 7

[KMZ21]

Pravesh K. Kothari, Peter Manohar, and Brian Hu Zhang. Polynomial-time sum-of-
squares can robustly estimate mean and covariance of gaussians optimally, 2021. 42,
45

50

[KOTZ14] Manuel Kauers, Ryan O’Donnell, Li-Yang Tan, and Yuan Zhou. Hypercontractive
In SODA, pages 1644–1658. SIAM,

inequalities via sos, and the frankl-rödl graph.
2014. 42

[KS17a]

[KS17b]

[KS17c]

P. K. Kothari and J. Steinhardt. Better agnostic clustering via relaxed tensor norms.
CoRR, abs/1711.07465, 2017. 2, 4

P. K. Kothari and D. Steurer. Outlier-robust moment-estimation via sum-of-squares.
CoRR, abs/1711.11581, 2017. 2, 5, 8, 11, 14, 19, 23, 24, 25, 26

Pravesh K. Kothari and Jacob Steinhardt. Better agnostic clustering via relaxed tensor
norms. CoRR, abs/1711.07465, 2017. 14

[KSKO20] Weihao Kong, Raghav Somani, Sham M. Kakade, and Sewoong Oh. Robust meta-
In Hugo Larochelle,
learning for mixed linear regression with small batches.
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, edi-
tors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 6

[KSS18]

P. K. Kothari, J. Steinhardt, and D. Steurer. Robust moment estimation and improved
In Proc. 50th Annual ACM Symposium on Theory of
clustering via sum of squares.
Computing (STOC), pages 1035–1046, 2018. 6

[KSSU19] Gautam Kamath, Or Sheﬀet, Vikrant Singhal, and Jonathan R. Ullman. Diﬀerentially
private algorithms for learning mixtures of separated gaussians. In Hanna M. Wal-
lach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and
Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14,
2019, Vancouver, BC, Canada, pages 168–180, 2019. 7

[KSU20]

[KV18]

[Las01]

Gautam Kamath, Vikrant Singhal, and Jonathan R. Ullman. Private mean estimation
of heavy-tailed distributions. In Jacob D. Abernethy and Shivani Agarwal, editors,
Conference on Learning Theory, COLT 2020, 9-12 July 2020, Virtual Event [Graz, Austria],
volume 125 of Proceedings of Machine Learning Research, pages 2204–2235. PMLR, 2020.
2

Vishesh Karwa and Salil P. Vadhan. Finite sample diﬀerentially private conﬁdence
In Anna R. Karlin, editor, 9th Innovations in Theoretical Computer Science
intervals.
Conference, ITCS 2018, January 11-14, 2018, Cambridge, MA, USA, volume 94 of LIPIcs,
pages 44:1–44:9. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2018. 2

Jean B. Lasserre. New positive semideﬁnite relaxations for nonconvex quadratic pro-
grams. In Advances in convex analysis and global optimization (Pythagorion, 2000), vol-
ume 54 of Nonconvex Optim. Appl., pages 319–331. Kluwer Acad. Publ., Dordrecht,
2001. 17

51

[LKKO21] Xiyang Liu, Weihao Kong, Sham M. Kakade, and Sewoong Oh. Robust and diﬀeren-

tially private mean estimation. CoRR, abs/2102.09159, 2021. 2, 8

[LKO21]

Xiyang Liu, Weihao Kong, and Sewoong Oh. Diﬀerential privacy and robust statistics
in high dimensions. CoRR, abs/2111.06578, 2021. 7

[LRV16]

[LY20]

[MT07]

[Nes00]

[NRS07]

[Par00]

[SCV18]

K. A. Lai, A. B. Rao, and S. Vempala. Agnostic estimation of mean and covariance. In
Proc. 57th IEEE Symposium on Foundations of Computer Science (FOCS), pages 665–674,
2016. 2, 6

Jerry Li and Guanghao Ye. Robust gaussian covariance estimation in nearly-matrix
multiplication time. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-
Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual, 2020. 6

Frank McSherry and Kunal Talwar. Mechanism design via diﬀerential privacy. In 48th
Annual IEEE Symposium on Foundations of Computer Science (FOCS 2007), October 20-23,
2007, Providence, RI, USA, Proceedings, pages 94–103. IEEE Computer Society, 2007. 22,
55, 56

In High
Yurii Nesterov. Squared functional systems and optimization problems.
performance optimization, volume 33 of Appl. Optim., pages 405–440. Kluwer Acad.
Publ., Dordrecht, 2000. 17

Kobbi Nissim, Sofya Raskhodnikova, and Adam D. Smith. Smooth sensitivity and
sampling in private data analysis. In David S. Johnson and Uriel Feige, editors, Pro-
ceedings of the 39th Annual ACM Symposium on Theory of Computing, San Diego, California,
USA, June 11-13, 2007, pages 75–84. ACM, 2007. 7

Pablo A Parrilo. Structured semideﬁnite programs and semialgebraic geometry methods in
robustness and optimization. PhD thesis, California Institute of Technology, 2000. 17

J. Steinhardt, M. Charikar, and G. Valiant. Resilience: A criterion for learning in the
presence of arbitrary outliers. In Proc. 9th Innovations in Theoretical Computer Science
Conference (ITCS), pages 45:1–45:21, 2018. 2, 6

[Sho87]

N. Z. Shor. Quadratic optimization problems. Izv. Akad. Nauk SSSR Tekhn. Kibernet.,
(1):128–139, 222, 1987. 17

[SSSS17]

Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership
inference attacks against machine learning models. In 2017 IEEE Symposium on Security
and Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017, pages 3–18. IEEE Computer
Society, 2017. 7

52

[SU15]

[Tao12]

[Vad17]

Thomas Steinke and Jonathan R. Ullman.
Interactive ﬁngerprinting codes and the
hardness of preventing false discovery. In Peter Grünwald, Elad Hazan, and Satyen
Kale, editors, Proceedings of The 28th Conference on Learning Theory, COLT 2015, Paris,
France, July 3-6, 2015, volume 40 of JMLR Workshop and Conference Proceedings, pages
1588–1628. JMLR.org, 2015. 7

T. Tao. Topics in Random Matrix Theory. Graduate studies in mathematics. American
Mathematical Society, 2012. 41

Salil P. Vadhan. The complexity of diﬀerential privacy. In Yehuda Lindell, editor, Tutori-
als on the Foundations of Cryptography, pages 347–450. Springer International Publishing,
2017. 22

[WDFS17] Miaoyan Wang, Khanh Dao Duc, Jonathan Fischer, and Yun S. Song. Operator norm
inequalities between tensor unfoldings on the partition lattice. Linear algebra and its
applications, 520:44–66, 2017. 38

[WXDX20] Di Wang, Hanshen Xiao, Srinivas Devadas, and Jinhui Xu. On diﬀerentially private
stochastic convex optimization with heavy-tailed data. In Proceedings of the 37th Interna-
tional Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume
119 of Proceedings of Machine Learning Research, pages 10081–10091. PMLR, 2020. 2

[ZJS19]

Banghua Zhu, Jiantao Jiao, and Jacob Steinhardt. Generalized resilience and robust
statistics. CoRR, abs/1909.08755, 2019. 6

[ZKKW20] Huanyu Zhang, Gautam Kamath, Janardhan Kulkarni, and Zhiwei Steven Wu. Pri-
vately learning markov random ﬁelds. In Proceedings of the 37th International Conference
on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceed-
ings of Machine Learning Research, pages 11129–11140. PMLR, 2020. 7

A Missing Proofs from Section 3.6

A.1 Proof of Lemma 3.27

Proof of Lemma 3.27. Consider any neighboring datasets 𝑌, 𝑌′ and let
denote the truncated
Laplace mechanism (with parameter as speciﬁed). Let 𝑝, 𝑞 denote the probability density functions
of

. Observe that 𝑝

. Thus, we have

for all 𝑥 < min

6 𝑒 𝜀

ℳ

, 𝑓

𝑌

𝑥

𝑥

𝑞

𝑓

,

(

)

·

(

)

𝑌
(

)

{

𝑌′)}
(

ℳ(

)

ℳ(

𝑌′)

𝑝

𝑥

(

ℝ[

) −

𝑒 𝜀𝑞

𝑥

(

)]+

𝑑𝑥

∫𝑥

∈

∫𝑥>min

{

, 𝑓

𝑓

𝑌
(

)

𝑌′)}
(

𝑝

𝑥

(

[

) −

𝑒 𝜀𝑞

𝑥

(

)]+

𝑑𝑥

∫𝑥>min

{

, 𝑓

𝑓

𝑌
(

)

𝑌′)}
(

𝑑𝑥

𝑝

𝑥

(

)

𝑝, 𝑞

𝐷𝑒 𝜀

(

)

=

=

6

53

Since sensitivity of 𝑓 is at most Δ
(

)

Lemma 3.28
(

)

6

∫𝑥> 𝑓

6 𝛿,

𝑑𝑥

𝑝

𝑥

(

)

𝑌
(

Δ

)−

which means that the truncated Laplace mechanism is indeed

𝜀, 𝛿

-DP.
)

(

(cid:3)

A.2 Proof of Lemma 3.30

The proof of the composition lemma follows from that of the standard adaptive composition of
𝑥, 0
approximate DP proof [DL09, Theorem 16]. Below we use the notation
and 𝑥

𝑦 to denote min

to denote max

𝑥, 𝑦

]+

𝑥

}

{

[

.

∧

{

}

Proof of Lemma 3.30. It suﬃces to prove the theorem for 𝑘 = 2 as we may then apply induction to
arrive at the statement for any positive integer 𝑘. To prove the case 𝑘 = 2, consider any 𝑆
𝑂2 ∪ {⊥}
and any pair of neighboring datasets 𝑌, 𝑌′.

⊆

For any 𝑆1 ⊆ 𝒪1 ∪ {⊥}
(𝒪1)

Note that we have 𝜇

:=
ℙ
𝑆1)
, we deﬁne the measure 𝜇
[
6 𝛿1 due to our assumption that
ℳ1 is

𝑌
[ℳ1(
) ∈
𝜀1, 𝛿1)
(

(

𝑆1] −
-DP.

Now consider four cases:

𝑒 𝜀1 ℙ

[ℳ1(

𝑌′) ∈

𝑆1]]+

.

• Both 𝑌, 𝑌′ satisfy Ψ1.

In this case, we may appeal to

implies

𝜀2, 𝛿2)

-DP under Ψ1 of

(

ℳ2 which

ℙ

[ℳ2(

𝑜1, 𝑌

𝑆

]

) ∈

For ease of notation, let 𝑝𝑌 :
probability density function of
Then, observe that

𝒪1 →
𝑌
ℳ1(

)

6

𝑒 𝜀2 ℙ

𝑜1, 𝑌′

𝑆

[ℳ2(

1
) +

𝛿2.

) ∈

] ∧

(
ℝ+ denote the measure obtained by restricting the
𝑜1)
to
).

𝒪1 (note that

𝑌
[ℳ1(

𝑑𝑜1 = 1

⊥]

𝑝𝑌

ℙ

=

−

(

)

(A.1)

1
𝒪
∫

ℙ

𝑌

[ℳ(

𝑆

]

) ∈

= 1

[⊥∈

𝑆

]

ℙ

𝑌
[ℳ1(

)

=

⊥] +

ℙ

[ℳ2(

𝑜1, 𝑌

𝑝𝑌

𝑆

]

𝑜1)

(

𝑑𝑜1

) ∈

∫𝒪
1

𝑒 𝜀2 ℙ

[ℳ2(

𝑜1, 𝑌′

𝑆

1
) +

𝛿2)

𝑝𝑌

𝑜1)

(

𝑑𝑜1

] ∧

) ∈

(A.1)
6 1

[⊥∈

6 1

[⊥∈

𝑆

]

+

1(
∫𝒪
𝑆

[⊥∈

](

6 1

+

1(
∫𝒪
𝑆

[⊥∈

](

6 1

+

1(
∫𝒪
𝑆

[⊥∈

](

6 1

1 ((

∫𝒪

𝑆

ℙ

]

𝑌
[ℳ1(
𝑌
[ℳ1(
𝑒 𝜀2 ℙ

)

ℙ

[ℳ2(

=

)

⊥] +

𝛿2

=

⊥] +
𝑜1, 𝑌′

𝑆

1
)

] ∧

𝑝𝑌

) ∈

𝑑𝑜1

𝑒 𝜀1 ℙ

[ℳ1(

𝑒 𝜀2 ℙ

[ℳ2(

=

𝑌′

)
𝑜1, 𝑌′

⊥] +
𝑆

) ∈

𝜇

1
)(

] ∧

({⊥})) +

(

𝑜1)
𝛿2
𝑒 𝜀1 𝑝𝑌′(

𝑒 𝜀1 ℙ

𝑒 𝜀2 ℙ

𝑒 𝜀1

+

[ℳ1(

[ℳ2(
𝜀2 ℙ

𝜇

𝑌′

=

)
𝑜1, 𝑌′

⊥]) +
𝑆

) ∈

] ∧

[ℳ1(

𝑌′

=

)

⊥]) +

(𝒪1 ∪ {⊥}) +
𝑒 𝜀1 𝑝𝑌′(
𝑜1))
1
)(
𝛿2
𝛿1 +

𝑑𝑜1

𝑑𝜇

𝑜1))

(

𝑜1)

𝑑𝑜1 +
𝛿2

54

𝑒 𝜀1

+

𝜀2 ℙ

[ℳ2(

𝑜1, 𝑌′

𝑆

𝑝𝑌′(

𝑜1)

]

𝑑𝑜1

) ∈

+
6 𝛿1 +

∫𝒪
1
𝛿2 +
• 𝑌 satisﬁes Ψ1 but 𝑌′ does not. In this case, we have ℙ

[ℳ(

𝜀2 ℙ

𝑒 𝜀1

) ∈

𝑌′

𝑆

+

]

.

=

= 1, which implies that

ℙ

𝑌

[ℳ(

𝑆

] −

) ∈

𝑒 𝜀1

+

𝜀2 ℙ

𝑌′

[ℳ(

𝑆

]

) ∈

6 ℙ

𝑌

[ℳ(

≠

)

⊥]

= ℙ

[ℳ(

𝑒 𝜀1 ℙ

𝑌′

[ℳ(

≠

)

⊥]

6 𝛿1,

where the last inequality follows from the fact that

ℳ1 is

• 𝑌′ satisﬁes Ψ1 but 𝑌 does not. In this case, we have ℙ

ℙ

𝑌

[ℳ(

𝑆

] −

) ∈

𝑒 𝜀1

+

𝜀2 ℙ

𝑌′

[ℳ(

𝑆

]

) ∈

= 1, which implies that

[ℳ(
=

𝑌

𝑌

)

)

=

⊥] −

⊥] −

⊥]
𝑒 𝜀1
+
𝑒 𝜀1 ℙ

𝜀2 ℙ

𝑌′
)
=

[ℳ(
𝑌′

[ℳ(

)

⊥]]+

=

⊥]]+

6

ℙ

6
ℙ
[
6 𝛿1,

[

[ℳ(

[ℳ(

[ℳ(

𝑌′)
𝑌

⊥]
≠

)
𝜀1, 𝛿1)
(
=
𝑌

)

⊥] −

-DP.

where the last inequality once again follows from the fact that

• Neither 𝑌 nor 𝑌′ satisfy Ψ1. In this case, both
𝑆

we have ℙ

= ℙ

𝑌

𝑆

.

𝑌

ℳ(

)

and

𝑌′)

ℳ(

]
Thus, in all cases, we have ℙ

[ℳ(

) ∈

𝑌′) ∈
𝑆

]
= 𝑒 𝜀1

[ℳ(
𝑌

[ℳ(

) ∈

]

𝜀2 ℙ

+

𝑌′) ∈

[ℳ(

𝑆

𝛿1 +

] +

𝛿2 as desired.

-DP.

𝜀1, 𝛿1)
ℳ1 is
always output

(

. Therefore,

⊥

A.3 Proof of Lemma 3.33

Proof of Lemma 3.33. Then, note that

𝐷𝑒 𝜀

𝑝, 𝑟

(

)

=

=

6

=

𝑝

𝑥

(

) −

ℝ𝑑 [

𝑒 𝜀𝑟

𝑥

(

)]+

𝑑𝑥

𝑝

𝑥

(

) −

ℝ𝑑 [(

𝑒 𝜀

2𝑞
/

𝑥

(

)) + (

𝑒 𝜀

2𝑞
/

𝑥

(

) −

𝑒 𝜀𝑟

𝑥

(

))]+

𝑑𝑥

∫𝑥

∈

∫𝑥

∈

𝑝

𝑥

(

) −

ℝ𝑑 [(

𝑒 𝜀

2𝑞
/

𝑥

(

))]+

𝑑𝑥

+

∫𝑥

∈

𝑝

𝑥

(

ℝ𝑑 [(

𝑝, 𝑞

2

(

/

) +

𝑒 𝜀

2𝑞
/

) −
𝑒 𝜀

2
/

·

𝑥

(
𝐷𝑒 𝜀

𝑑𝑥

+

))]+

𝑞, 𝑟

,

)

2

(

/

∫𝑥
∈
= 𝐷𝑒 𝜀

∫𝑥
𝑒 𝜀

∈
2
/

𝑒 𝜀

2𝑞
/

𝑥

(

) −

𝑒 𝜀𝑟

𝑥

(

))]+

𝑑𝑥

ℝ𝑑 [

𝑞

𝑥

(

) −

ℝ𝑑 [

𝑒 𝜀

2𝑟
/

𝑥

(

))]+

𝑑𝑥

∫𝑥

∈

as desired.

A.4 Proof of Theorem 3.34

As stated earlier, the proof of Theorem 3.34 follows from applying the exponential mecha-
nism [MT07] and then use the truncated Laplace mechanism (Lemma 3.27) to check that the
score indeed exceeds 𝜅.

Proof of Theorem 3.34. Selection works as follows:

55

(cid:3)

(cid:3)

1. First, run the

𝜀
(
probability proportional to exp

-DP exponential mechanism [MT07], i.e.
2
)

𝑐, 𝑌

/

. Let 𝑐1 be the output of this procedure.

selecting each 𝑐

𝐶 with

∈

𝜀
4Δ ·

score
(
𝛿

2 ln

2. Sample the noise 𝑁

)
(cid:1)
, 2Δ
𝜀
.
⊥
We will now prove each of the claimed properties:

)
∼
score > 𝜅, then output 𝑐1. Otherwise, output
(cid:17)

tLap

1
/
(
𝜀

(cid:0)
Δ

+

−

1

(cid:16)

(cid:16)

(cid:157)

and compute

score = score
(

𝑐1, 𝑌

) +

𝑁. If

(cid:17)

(cid:157)

1. The ﬁrst step satisﬁes

nism [MT07]. The second step is
composition theorem implies that Selection is

𝜀

/

𝜀

(

/

-DP via the standard privacy guarantee of the exponential mecha-
2
)
-DP due to Lemma 3.27. Thereby, applying the basic
)

2, 𝛿

(

𝜀, 𝛿

-DP.
)

(

2. Since 𝑁 6 0, we are guarantee that if the algorithm outputs 𝑐∗ ∈ 𝒞

𝜅 as desired.

, we must have score
(

𝑐, 𝑌

)

>

3. For any 𝑐

, the standard utility analysis of the exponential mechanism [MT07] implies

∈ 𝒞
that, with probability 1
the tail bound of Laplace noise (Lemma 3.28) implies that with probability 1
(cid:17)(cid:17)
have 𝑁 >

0.5𝛽, we have score
(

> score
(

Δ
𝜀 ln

𝑐1, 𝑌

𝑐, 𝑌

) −

𝐶
|𝛽

𝑂

𝑂

𝑂

>

Δ

−

1

ln

(cid:16)

(cid:16)

)

(

|

1
𝛿
)𝜀
/

1
+
/
(
(cid:17) (cid:17)
, the probability that the algorithm outputs

−

−

𝛽

(cid:17)

(cid:16)

(cid:16)

)

(cid:0)

(cid:1)

Δ
𝜀 ln

1
𝛿𝛽

Δ
𝜀 ln

−
. Therefore, if score
(

. Moreover,
0.5𝛽 we
>

𝑐, 𝑌

)

−
𝐶
|𝛿𝛽
|

(cid:16)

Δ
𝜀 ln

𝑂

𝜅

+

⊥

is at most 𝛽, as desired.

(cid:3)

(cid:16)

(cid:16)

(cid:17) (cid:17)

56

