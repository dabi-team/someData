1
2
0
2

b
e
F
3

]

C
O
.
h
t
a
m

[

2
v
2
1
3
1
0
.
0
1
9
1
:
v
i
X
r
a

A sparse semismooth Newton based augmented
Lagrangian method for large-scale support vector
machines

Dunbiao Niu* Chengjing Wang† Peipei Tang‡ Qingsong Wang§ and Enbin Song¶

Abstract

Support vector machines (SVMs) are successful modeling and prediction tools with a va-
riety of applications. Previous work has demonstrated the superiority of the SVMs in dealing
with the high dimensional, low sample size problems. However, the numerical difﬁculties
of the SVMs will become severe with the increase of the sample size. Although there exist
many solvers for the SVMs, only few of them are designed by exploiting the special structures
of the SVMs. In this paper, we propose a highly efﬁcient sparse semismooth Newton based
augmented Lagrangian method for solving a large-scale convex quadratic programming prob-
lem with a linear equality constraint and a simple box constraint, which is generated from the
dual problems of the SVMs. By leveraging the primal-dual error bound result, the fast local
convergence rate of the augmented Lagrangian method can be guaranteed. Furthermore, by ex-
ploiting the second-order sparsity of the problem when using the semismooth Newton method,
the algorithm can efﬁciently solve the aforementioned difﬁcult problems. Finally, numerical
comparisons demonstrate that the proposed algorithm outperforms the current state-of-the-art
solvers for the large-scale SVMs.

Keywords: support vector machines, semismooth Newton method, augmented Lagrangian

method

1 Introduction

Support Vector Machines (SVMs), introduced by [1], are originally formulated for binary classiﬁ-
cation problems that aim to separate two data sets with the widest margin. Nowadays, the SVMs

*College of Mathematics, Sichuan University, No.24 South Section 1, Yihuan Road, Chengdu 610065, China.

(dunbiaoniu sc@163.com).

†Corresponding author. School of Mathematics, Southwest Jiaotong University, No.999, Xian Road, West Park,

High-tech Zone, Chengdu 611756, China. (renascencewang@hotmail.com).

‡School

of Computing Science,

Zhejiang University City College, Hangzhou

310015, China.

(tangpp@zucc.edu.cn).

§School of Mathematics, Southwest Jiaotong University, No.999, Xian Road, West Park, High-tech Zone, Chengdu

611756, China. (nothing2wang@hotmail.com).

¶College of Mathematics, Sichuan University, No.24 South Section 1, Yihuan Road, Chengdu 610065, China.

(e.b.song@163.com).

1

 
 
 
 
 
 
have been extended to solve a variety of pattern recognition and data mining problems such as
feature selection [2], text categorization [3], hand-written character recognition [4], image classiﬁ-
cation [5], and so on. Among the different applications of the SVMs, we ﬁrst introduce two speciﬁc
examples.

• The C-Support Vector Classiﬁcation (C-SVC) [6, 7]
Given a training set of instance-label pairs (˜xi, yi), i = 1, ..., n, where ˜xi ∈
+1,
{
H
to solve the following optimization problem

Rq and yi ∈
, the C-SVC aims to ﬁnd a hyperplane in a given reproducing kernel Hilbert space
[8] to separate the data set into two classes with the widest margin. In this model, we need

1
−

}

min
w,b,ξ

1
2 h

w, w

iH + C

n

ξi

i=1
X
w, φ(˜xi)
iH + b)
0, i = 1, . . . , n,

s.t. yi (
h
ξi ≥

ξi,

1

−

≥

(1.1)

and φ : Rq

h·
iH is a reproducing kernel of

·iH is the inner product in a reproducing ker-
where C > 0 is a regularization parameter,
is a feature map such that the function K(˜xi, ˜xj) :=
nel Hilbert space
Rq. For example,
for any data ˜xi, ˜xj ∈
φ(˜xi), φ(˜xj)
h
i ˜xj is a linear kernel and K(˜xi, ˜xj) = exp−||x||2/2α is a radial basis func-
K(˜xi, ˜xj) = ˜xT
tion (RBF) kernel, where α > 0 is a ﬁxed parameter called the width. As it has been shown
in [6], the dual of the problem (1.1) is the following quadratic programming problem

→ H

H

H

,

1
2

min
x∈Rn

xT Qx

−
s.t. yT x = 0,
xi ≤

≤

0

C, i = 1, . . . , n,

eT x

(1.2)

where e = [1, . . . , 1]T
Sn (the space of n
and Q
Qij = yiyjK(˜xi, ˜xj), i, j = 1, . . . , n.

×

∈

∈

Rn is a vector of all ones, y = [y1, . . . , yn]T

Rn is a label vector
n symmetric matrices) is a positive semideﬁnite matrix with

∈

• The Support Vector Regression (SVR) [9]

For a set of training points (˜xi, yi) , i = 1, ..., n, the regression problem is to predict the quan-
Rq. Given a regularization
titative response yi ∈
coefﬁcient C > 0 and an insensitivity parameter ǫ > 0, the standard form of the support
vector regression is

R on the basis of the predictor variable ˜xi ∈

n

n

ξi + C

ξ∗
i

min
w,b,ξ,ξ∗

1
2 h

w, w

s.t.

iH + C
iH + b
w, φ(˜xi)

i=1
X
yi ≥
−
b
iH −
≥
0, i = 1, . . . , n.

w, φ(˜xi)
h
yi − h
ξi, ξ∗
i ≥

i=1
X
ǫ + ξi,
ξ∗
i ,
ǫ

−

(1.3)

2

Furthermore, the dual of the problem (1.3) is

min
x,z∈Rn

1
2

[xT , zT ]

(cid:18)

K
K
−
K K

s.t. [eT ,

eT ]

= 0,

−
x
z

(cid:21)

−
(cid:20)
xi, zi ≤

0

≤

C, i = 1, . . . , n,

x
z

(cid:19) (cid:20)

n

+

(ε + yi)xi +

(cid:21)

i=1
X

n

i=1
X

(ε

−

yi)zi

(1.4)

∈

where K

Sn
+ is a positive semideﬁnite kernal matrix with Kij = K(˜xi, ˜xj), i, j = 1, . . . , n.
The above two problems (1.1) and (1.3) show that there are various formulations for the SVMs
in different scenarios. However, their dual problems (1.2) and (1.4) can be summarized as the
following uniﬁed form

min
x∈Rn

1
2

xT Qx + cT x

s.t. aT x = d,

(1.5)

l

x

≤

≤

u,

∈

Sn is a positive semideﬁnite matrix and c, a, l, u

R are given vectors
where Q
and scalar, respectively. Moreover, we assume that l < u, i.e., li < ui for all i
,
1, . . . , n
}
where li and ui are the ith elements of l and u, respectively. Currently most of the work on
computational aspects of the SVMs concentrate on solving the dual problem (1.5) since it is a
more general framework to handle the SVMs. But there still exist some algorithms that solve the
primal problem. For example, Yin and Li [10] proposed a semismooth Newton method to solve the
primal problems of L2-loss SVC model and ǫ-L2-loss SVR model with linear kernels recently.

Rn and d

∈ {

∈

∈

For the convex problem (1.5), many existing optimization algorithms including the accelerated
proximal gradient (APG) method, the alternating direction method of multipliers (ADMM) and the
interior-point method (IPM) et al., can be applied to solve it efﬁciently when the problem scale
is small or moderate. However, when facing the large-scale problems, the numerical difﬁculties
become severe. Speciﬁcally, when the dimension n is very large, the full storage of the n
n
dense matrix Q in (1.5) is very difﬁcult and even impossible for a typical computer. Therefore
we cannot apply the standard quadratic programming solvers which require the full storage of Q
directly. An alternative approach is to compute the elements of Q from the original data when
it is required. However, this becomes prohibitively time consuming since the elements of Q are
frequently required in the process of solving the problem (1.5).

×

Currently, one approach to avoid using the whole elements of the matrix Q is to adopt the
decomposition strategy [11–13].
In this kind of approach, only a small subset of variables in
each iteration need to be updated so that only a few rows of the matrix Q are involved in, which
signiﬁcantly reduces the computational cost. The Sequential Minimal Optimization (SMO) method
[13] is one of the well-known decomposition methods, in which only two variables are considered
in each iteration. The popular SVMs solver LIBSVM [14] also implements an SMO-type method
[15] to solve the problem (1.5).

Another widely used algorithm for the problem (1.5) is the gradient projection (GP) method.
Thanks to the low-cost algorithm of the projection onto the feasible set of (1.5) and the identiﬁca-
tion properties of the GP method [16], we only need to consider a reduced subproblem which is

3

related to the current active set of variables. Furthermore, inspired by the algorithm for the bound
constrained quadratic programming problem [17], the proportionality-based 2-phase gradient pro-
jection (P2GP) method [18] is derived to solve the problem (1.5). In addition, the fast APG (FAPG)
method [19] is another commonly used algorithm to solve various classiﬁcation models, including
the C-SVC, L2-loss SVC, and v-SVM et al..

The advantage of the decomposition and PG methods is that only a small subset of variables
are involved in each subproblem, i.e., only a small part of the elements of the matrix Q are required
in each iteration. However, as the numerical experiments shown in Section 4, both algorithms may
encounter the problem of slow convergence. If Q is positive deﬁnite and the nondegeneracy as-
sumption holds, the SMO-type decomposition methods are shown to be linear convergent to the
solution of (1.5) in [20]. It is well-known that the PG-type methods can exhibit (linear) sublin-
ear convergence when the objective function is (strongly) convex. These existing theories partly
explain why the convergences of these algorithms are not ideal.

In this paper, we aim to solve the problem (1.5) by applying the augmented Lagrangian (AL)
method [21] to the dual problem of (1.5). Meanwhile, a semismooth Newton (SsN) method is
used to solve the inner subproblems of the AL method. It is well known that to fully fulﬁll the
potential of the AL method, the inner subproblems should be solved accurately and efﬁciently.
Although the SsN method is an ideal approach to solve the inner subproblems, it is not proper
to be applied directly because the cost of the SsN method may be very high at the beginning
few iterations due to the lack of sparse structure of the generalized Hessian, especially for very
large-scale problems. To overcome this difﬁculty, we may use some algorithm, say, the random
Fourier features method [22] which will be introduced in Section 4, to produce a good initial point,
and then transfer to the semismooth Newton based augmented Lagrangian (SsNAL) method. The
generalized Hessian for the inner subproblem may probably has some kind of sparse structure.
Wisely exploiting this nice structure may largely reduce the computational cost and the memory
consumption in each SsN step. Hence, our proposed algorithm not only has the same advantage
as the decomposition method in [12] with memory requirements being linear to the number of
training examples and support vectors, but also has the fast local convergent rate in both inner and
outer iterations. Since our algorithm fully takes advantage of the sparse structure, we call it a Sparse
SsN based AL (SSsNAL) method. Besides, there are three main reasons why the SSsNAL method
can be implemented efﬁciently to solve the problem (1.5):

(I) The piecewise linear-quadratic structure of the problem (1.5) guarantees the fast local con-

vergence of the AL method [23, 24].

(II) There exist many efﬁcient algorithms [19, 25–28] to compute the value of the Euclidean pro-
jection of any given point onto the feasible set (1.5) due to its special structure. Furthermore,
the explicit formula of the generalized Jacobian, which is named HS-Jacobian [29], can be
easily derived.

(III) It is generally true for many SVMs that the number of the support vectors are much less than
that of the training examples, and many multiplier variables of the support vectors are at the
upper bound of the box constraint [12]. That is, very few of the components of the optimal
solution lie in the interior of the box constraint.

As will be shown later, the above (I) and (II) guarantee the inner subproblem can be solved by
the SsN method with a very low memory consumption in each iteration. And the above (I), (II) and

4

(III) together provide an insight into the compelling advantages of applying the SSsNAL method to
the problem (1.5). Indeed, for the large-scale problem (1.5), the numerical experiments in Section
4 will show that the SSsNAL method only needs at most a few dozens of outer iterations to reach
the desired solutions while all the inner subproblems can be solved without too much effort.

The rest of the paper is organized as follows. In Section 2, some preliminaries about the re-
stricted dual formulation and some necessary error bound results are provided. Section 3 is dedi-
cated to present the SSsNAL method for the restricted dual problem in details. Numerical experi-
ments are presented on real data in Section 4, which verify the performance of our SSsNAL method
against other solvers. Finally, Section 5 concludes this paper.

,

y, z

and

−∞

be two real ﬁnite dimensional Euclidean spaces. For any con-
], its conjugate function is denoted by p∗, i.e., p∗(x) =
(

Notations: Let
X
Y
vex function p : S
⊂ X →
, and its subdifferential at x is denoted by ∂p(x), i.e., ∂p(x) :=
p(y)
x, y
supy{h
i −
,
x
p(x) +
z
i
∈
−
distance from x to Ω by dist(x, Ω) := inf y∈Ω ||
ΠΩ(x) := arg miny∈Ω ||
y
x
Y
. We use In to denote the n
the graph of F , i.e., gph F :=
∈ X × Y |
matrix in Rn and A† to denote the Moore-Penrose pseudo-inverse of a given matrix A

≥
. For a given closed convex set Ω and a vector x, we denote the
and the Euclidean projection of x onto Ω by
, we use gph F to denote
n identity
Rn×n.

x
. For any set-valued mapping F :
(x, y)

dom(p)

F (x)

p(z)

}
∀

||
{

⇒

∞

−

−

X

∈

||

}

}

{

y

y

y

h

|

×
∈

2 Preliminaries

In this section, we present some necessary error bound results, which will be used in the conver-
gence rate analysis of the AL method in Section 3.1.

We denote the single linear constraint and the box constraint in the problem (1.5) by

L :=

x

{

∈

Rn

|

aT x = d

}

and K :=

Rn

x

{

∈

l

|

x

,

u
}

≤

≤

(2.1)

respectively. Then the problem (1.5) can be equivalently rewritten as

(P) min
x∈Rn

f (x) := 1
2h

x, Qx
i

+

c, x
i

h

+ δK T L (x)

,

(cid:8)

K

L, i.e., δK T L (x) = 0 if
where δK T L is the indicator function for the polyhedral convex set K
x
. Note that the problem (P) is already the dual formulation
of the SVMs in the introduction, but we still regard it as the primal problem based on our custom.
The dual of the problem (P) is

L, otherwise δK T L (x) = +

∞

∈

∩

(cid:9)
∩

(D)

min
w∈Rn, z∈Rn

w, Qw

1
2h

i

+ δ∗

K T L (z)

|

Qw + z + c = 0, w

Ran(Q)

,

∈

(cid:8)

(cid:9)

where δ∗
K T L is the conjugate of the indicator function δK T L , and Ran(Q) denotes the range space of
Ran⊥(Q),
Q. Note that the additional constraint w
w and w + w0 have the same objective function values and both satisfy the linear constraint in
(D). As will be shown in the next section, the constraint w
Ran(Q) in fact plays an important
role to guarantee that the subproblem has a unique solution and our designed algorithm is efﬁcient.
Since we restrict w in the range space artiﬁcially, we may also call (D) a restricted dual problem.

Ran(Q) is reasonable because, for any w0

∈

∈

∈

5

Correspondingly, the Karush-Kuhn-Tucker (KKT) condition associated with the problem (P) is
given by

x

−

ProxδK T L (x + z) = 0, Qw

−

Qx = 0, Qw + z + c = 0,

(2.2)

where the proximal mapping for a given closed proper convex function p : Rn
deﬁned by

(

−∞

, +

∞

] is

→

Proxp(u) := arg min

x

p(x) +

1
2||

u

2

x

||

−

u

,

∀

∈

Rn.

Moreover, for any given parameter λ > 0, we introduce the following Moreau identity, which will
be used frequently.

(cid:8)

(cid:9)

Proxλp(u) + λProxp∗/λ(u/λ) = u.

Let l be the ordinary Lagrangian function for the problem (D)

l(w, z, x) =

(

1
2h
+

w, Qw
,

∞

+ δ∗

K T L (z)

i

− h

x, Qw + z + c

i

, w

Ran(Q),

∈

otherwise.

(2.3)

(2.4)

Then, we deﬁne the maximal monotone operators

Tf (x) :=∂f (x) =

ux ∈

{

Rn

ux ∈

|

Tl [21] by
Tf and
Qx + c + ∂δK T L (x)

x

,

}

∀

∈

Rn

and

Tl(w, z, x) :=
∈
respectively. Correspondingly, the inverses of

(uw, uz, ux)

{

−1
f
T

(ux) :=

x

{

R3n

|
Tf and
Rn

∈

(uw, uz,

ux)

∂l(w, z, x)

,

−

∈
}
Tl are given, respectively, by
ux ∈
,

∂f (x)

}

|

and

−1

l
T

(uw, uz, ux) :=

(w, z, x)

R3n

(uw, uz,

ux)

∂l(w, z, x)

.

{

}
∈
] is said to be piecewise linear-
Recall that a closed proper convex function g :
quadratic if dom g is the union of ﬁnitely many polyhedral sets and on each of these polyhedral sets,
g is either an afﬁne or a quadratic function [30, Deﬁnition 10.20]. Hence the objective function f in
(P) is piecewise linear-quadratic. Meanwhile, by [30, Theorem 11.14] the support function δ∗
is piecewise linear-quadratic, which implies that l is also piecewise linear-quadratic.

X →

−∞

, +

∞

K T L

−

∈

(

|

⇒

Y

X

In addition, F :

Tl(w, z, x) are piecewise polyhedral multivalued mappings.

is called piecewise polyhedral if its graph is the union of ﬁnitely many
polyhedral convex sets. Therefore, according to the following proposition established in [24], both
Tf (x) and
Proposition 1 [24] Let
]
∞
be a closed proper convex function. Then θ is piecewise linear-quadratic if and only if the graph of
∂θ is piecewise polyhedral. Moreover, θ is piecewise linear-quadratic if and only if its conjugate
θ∗ is piecewise linear-quadratic.

be a ﬁnite-dimensional real Euclidean space and θ :

X →

−∞

, +

X

(

6

In [23], Robinson established the following fundamental property to describe the locally upper
Lipschitz continuity of a piecewise polyhedral multivalued mapping.

Proposition 2 [23] If the multivalued mapping F :
locally upper Lipschitz continuous at any x0
i.e., there exists a neighborhood V of x0 such that F (x)

is piecewise polyhedral, then F is
with modulus κ0 independent of the choice of x0.
x

∈ X

x0

V.

X

x

BY,

⇒

Y
F (x0) + κ0||

Therefore, the above Propositions 1 and 2 imply that
Lipschitz continuous. Combining [31, Theorem 3H.3], we have the following result.

⊆
Tf (x) and

−

||
Tl(w, z, x) are both locally upper

∈

∀

Proposition 3 Assume that the KKT system (2.2) has at least one solution. Let ( ¯w, ¯z, ¯x) be a
−1
solution of the KKT system (2.2). Then
l
T
is also metrically subregular at ( ¯w, ¯z, ¯x) for the origin, i.e., there exist a neighborhood of origin
and constants κl > 0, κf > 0 along with the neighborhoods Bδl( ¯w, ¯z, ¯x) and Bδf (¯x) such that

is metrically subregular at ¯x for the origin and

−1
f
T

V

dist

x,

(0)

−1
f
T

≤

κf dist

0,

Tf (x)

∩ V

holds for any x

Bδf (¯x) and

(cid:0)

∈

(cid:1)

(cid:0)

,

(cid:1)

dist

(w, z, x),

−1

(0)

l
T

holds for any (w, z, x)

Bδl( ¯w, ¯z, ¯x).

(cid:0)
∈

κldist

0,

Tl(w, z, x)

∩ V

(cid:0)

,

(cid:1)

≤

(cid:1)

Besides, we may go one step further to present the error bound condition in some semilocal sense.
Although Zhang et al. [32] presented a similar result without a proof, for the sake of completeness,
we still present the detailed results and the proof.

Proposition 4 For any r > 0 and ( ¯w, ¯z, ¯x)
that

−1

l
∈ T

(0), there exist κf (r) > 0 and κl(r) > 0 such

dist(x,

dist

(w, z, x),

hold for any x

(cid:0)
n with dist(x,

∈ R

−1
f
T

−1
f
T
−1
l
T
(0))

(0))

(0)

≤

κf (r) dist(0,
0,
κl(r)dist

≤
(cid:1)
r and

≤

(w, z, x)

(cid:0)

||

Tf (x)),
Tl(w, z, x)
(cid:1)
( ¯w, ¯z, ¯x)
|| ≤

−

.

(2.5a)

(2.5b)

r.

Proof For the sake of contradiction, we assume that the ﬁrst assertion about inequality (2.5a) is
false. Then for some ˜r > 0 and any κf (˜r) = k > 0, there exists xk
˜r
such that

Rn with dist(xk,

−1
f
T

(0))

≤

∈

Next, by using the fact that
fore, there exists a subsequence
we have

−1
f
T

(0) is compact, we know that
xkj

such that xkj

{

}

→

dist(xk,

−1
f
T

(0)) > kdist(0,

Tf (xk)).
xk
is a bounded sequence. There-
}
{
x∗ as kj →
. Then, together with (2.6),
+

(2.6)

∞

dist

0,

0

≤

Tf (x∗)

= lim

kj →+∞

dist(0,

Tf (xkj ))

≤

lim
kj→+∞

−1
dist(xkj ,
f
T
kj

(0))

lim
kj →+∞

≤

˜r
kj

= 0,

(cid:0)

(cid:1)

7

−1
f
∈ T

(0). Moreover, there exists ¯k > κf such that

which implies that x∗
δf holds
for all kj > ¯k, where the parameters κf and δf have been deﬁned in Proposition 3. Thus, for some
¯k, ˜r
ε > 0 and all kj > max
ε }
{
dist(xkj ,
Tf (xkj ))

∩ Bε(0)) = κf dist(0,
≤
which, together with kj > ¯k > κf , is a contradiction. Hence, the ﬁrst assertion is true.

Tf (xkj )),

Tf (xkj )

κf dist(0,

kjdist(0,

, we have

−1
f
T

(0))

|| ≤

xkj

x∗

−

≤

||

Similarly, for the sake of contradiction, we also assume that the second assertion about inequal-
(0), for any κl(˜r) = k > 0, such

ity (2.5b) is false. Then, there exist ˜r > 0 and ( ˜w, ˜z, ˜x)
that

l
∈ T

−1

dist

(wk, zk, xk),

l
T
(cid:0)
(wk, zk, xk)
Note that
}
such that (wki, zki, xki)

{

−1

(0)

> kdist

0,

Tl(wk, zk, xk)

,

∃

(wk, zk, xk)

∈ B˜r( ˜w, ˜z, ˜x).

(2.7)

is a bounded sequence. Hence, there exists a subsequence

(cid:1)

(cid:0)

(cid:1)

→

( ˜w∗, ˜z∗, ˜x∗) as ki →
Tl(wki, zki, xki)
Now taking the limits on both sides of the inequality (2.8), we have

(wki, zki, xki),

. Then we have

∞
dist

dist

0,

ki

<

+

(cid:0)

(cid:1)

(cid:0)

−1

(0)

l
T

.

(cid:1)

(wki, zki, xki)

}

{

(2.8)

dist

0,

0

≤

Tl( ˜w∗, ˜z∗, ˜x∗)

lim
ki→+∞

≤

dist

(wki, zki, xki),

−1

(0)

l
T

= 0,

ki

which implies that ( ˜w∗, ˜z∗, ˜x∗)
l
∈ T
for the origin. Then, for all ki sufﬁciently large and some ε > 0, we have

l
T

−1

−1

(cid:0)

(cid:1)
(0). Note that

(cid:0)

(cid:1)
is metrically subregular at ( ˜w∗, ˜z∗, ˜x∗)

dist

(wki, zki, xki),

−1

(0)

l
T

≤

κldist

0,

Tl(wki, zki, xki)

∩ Bε(0)

= κldist

0,

Tl(wki, zki, xki)

.

On the other hand, (2.7) implies that

(cid:1)

(cid:0)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

Tl(wki, zki, xki)
Thus, by taking ki > max(ε, κl), we obtain a contradiction. So (2.5b) is true.

(wki, zki, xki),

> kidist

dist

(0)

l
T

0,

(cid:0)

(cid:1)

(cid:0)

−1

.

(cid:1)

3 The SSsNAL method for the SVM problems

In this section, we detailedly discuss how to apply the SSsNAL method to solve the problem (D)
and establish its convergence theories.

3.1 The SSsNAL method for the problem (D)

Firstly, we provide the framework of the SSsNAL method. Given σ > 0, the AL function associated
with the problem (D) is given as follows

Lσ(w, z; x) =

1
2 h

w, Qw

i

+ δ∗

K T L (z) +

σ
2 ||

Qw + z + c

1
σ

2

x

||

−

1
2σ ||

2,

x

||

−

8

where (w, z, x)
∈
sketched as below.

Ran(Q)

n
× R

n. The SSsNAL method for solving the problem (D) can be

× R

Algorithm 1 : the SSsNAL method for the problem (D)

Let σ0 > 0 be a given parameter. Choose x0
and xk+1 by executing the following iterations:
Step 1. Apply the SsN method to compute

∈ R

n. For k = 0, 1, 2, . . . , generate (wk+1, zk+1)

(wk+1, zk+1)

argmin
w∈Ran(Q),z∈Rn

≈

Ψk(w, z) :=

Lσk(w, z; xk)

.

(3.1)

Step 2. Compute

and update σk+1.

(cid:8)

(cid:9)

xk+1 = xk

−

σk(Qwk+1 + zk+1 + c),

Notably, the inner subproblem (3.1) has no closed-form solution in general. So we consider

how to solve it approximately with the following stopping criteria introduced in [21, 33]:

(A) Ψk(wk+1, zk+1)

inf
w∈Ran(Q),z

−

Ψk(w, z)

(B) Ψk(wk+1, zk+1)

inf
w∈Ran(Q),z

−

Ψk(w, z)

≤

≤

Xk=1
xk+1

δ2
k/(2σk)

||

+∞

ǫ2
k/(2σk),

ǫk < +

,

∞

xk

2,

||

−

+∞

Xk=1

δk < +

.

∞

Since inf w∈Ran(Q),z Ψk(w, z) is unknown, in order to apply the stopping criteria (A) and (B) in
Algorithm 1, we need to further analyze the following optimization problem.

min

Ψk(w, z)

(w, z)

Ran(Q)

Rn

.

|

∈

×

Obviously, it is easily seen from the deﬁnition of Ψk(w, z) in (3.1) that the above problem has a
unique optimal solution in Ran(

Ran(Q), we deﬁne

Rn. For any w

(cid:9)

(cid:8)
)
Q

×

∈

Ψk(w, z)

ψk(w) := inf
z∈Rn
1
2 h

=

w, Qw

+ δ∗

K T L (Prox 1
σk

δ∗
K T L

i

(u(w)/σk))

σk
2 ||

Prox 1
σk

w, Qw

+

i

+

1
2 h

=

δ∗
K T L
1
2σk

(cid:0)

(u(w)/σk)

u(w)

||

2

||

− ||

−

u(w)/σk||
u(w)

−

2

−

1
2σk ||

xk

ΠK T L (u(w))

(3.2)

1
2σk ||

xk

2,

||

2,

||

2

||

−

(cid:1)

where u(w) := xk
(wk+1, zk+1) in Step 1 of Algorithm 1 can be obtained in the following manner

σk(Qw + c) and the last equality directly follows from (2.2) in [34]. Then

−

wk+1
≈
zk+1 = Prox 1
σk

arg min
{
δ∗
K T L

ψk(w)

w

Ran(Q)

,

|

∈

(u(wk+1)/σk) = σ−1
k

}

(cid:0)

9

u(wk+1)

−

ΠK T L (u(wk+1))

,

(cid:1)

(3.3a)
(3.3b)

where the last equality in (3.3b) follows from the Moreau identity (2.3). Moreover, in combination
with (3.3b), the update in Step 2 of Algorithm 1 can be simpliﬁed as

xk+1 = xk

−

σk

Qwk+1 + σ−1
k

u(wk+1)

−

ΠK T L (u(wk+1))

+ c

= ΠK T L (u(wk+1)). (3.4)

Finally, note that ψk(w) is continuously differentiable and strongly convex with modulus ˜λmin(Q)

(cid:1)

(cid:1)

(cid:0)

(cid:0)

in Ran(Q), where ˜λmin(Q) is the minimum nonzero eigenvalue of Q. Then we have

Ψk(wk+1, zk+1)

inf
w∈Ran(Q),z

−

Ψk(w, z)

= ψk(wk+1)

inf
w∈Ran(Q)

−

ψk(w)

1

≤

2˜λmin(Q) ||∇

ψk(wk+1)

2,

||

(3.5)

ψk(w∗) = 0 with
where the last inequality is due to Theorem 2.1.10 in [35] and the fact that
w∗ = arg minw∈Ran(Q) ψk(w). Therefore, we replace the above stopping criteria (A) and (B) by
the following easy-to-check criteria

∇

(A′)

||∇

ψk(wk+1)

|| ≤

˜λmin(Q)ǫk/√σk, ǫk ≥

q

ǫk < +

,

∞

(B′)

||∇

ψk(wk+1)

|| ≤

˜λmin(Q)δk/√σk||

q

xk+1

−

, δk ≥

||

+∞

0,

Xk=1
xk

0,

+∞

Xk=1

δk < +

.

∞

Next, we shall adapt the results developed in [21, 33, 36] to establish the convergence theory of
+∞
k=1 ǫk < r, then we can

the AL method for the problem (D). Take a positive scalar r such that
state the convergence theory as below.

P

{

(wk, zk, xk)

Theorem 1 Let
be any inﬁnite sequence generated by Algorithm 1 with stopping
}
criterion (A′) and (B′) for solving subproblem (3.3a). Let ΩP be the solution set of (P) and (w∗, z∗)
be the unique optimal solution of (D). Then the sequence
ΩP and the
sequence (wk, zk) converges to the unique optimal solution (w∗, z∗). Moreover, if dist(x0, ΩP )
r

converges to x∗

+∞
k=1 ǫk, then for all k > 0

xk

≤

∈

}

{

−

P

dist(xk+1, ΩP )
(wk+1, zk+1)

||

µkdist(xk, ΩP ),
µ′
(w∗, z∗)
k||

|| ≤

≤

−

xk+1

xk

,

||

−

(3.6a)

(3.6b)

where µk :=

δk + (1 + δk)κf (r)/

f (r) + σ2
κ2
k

/(1

µ∞ := κf (r)/

κ2
f (r) + σ2
∞,

δk)

−

→

k := κl(r)

µ′
the parameter r is determined by Proposition 4. Moreover, µk and µ′

µ′
∞ := κl(r)/σ∞, κf (r), κl(r) > 0 are constant and
.

σ∞ = +

k + ˜λmin(Q)δ2

(cid:2)
1/σ2

(cid:3)

q
k/σk →

q
k go to 0 as σk ↑

q

∞

Proof The statements on the global convergence directly follow from [21]. The proof for the ﬁrst
inequality (3.6a) follows from the similar idea of the proof in [32, Lemma 4.1], so we omit the
details. Next, to prove the second inequality (3.6b), for the given parameter r, we have

(wk+1, zk+1, xk+1)

(w∗, z∗, x∗)

r,

k

∀

≥

0,

|| ≤

−

||

10

which follows from the fact that (wk+1, zk+1, xk+1)
4, we have

→

(w∗, z∗, x∗). Therefore, from Proposition

(wk+1, zk+1)

(w∗, z∗)

+ dist(xk+1, ΩP )

||

||
which, together with the estimate (4.21) in [21], implies

−

≤

κl(r)dist

0,

Tl(wk+1, zk+1, xk+1)

,

k

∀

≥

0,

(cid:0)

(cid:1)

(wk+1, zk+1)

(w∗, z∗)

κl(r)

|| ≤

−

||

dist2(0, ∂Ψk(wk+1, zk+1)) + σ−2

k ||

xk+1

xk

||

−

1/2

2

.

(3.7)

Then, according to the update rule of (wk+1, zk+1) in (3.3) and the Danskin-type theorem [37], one
has

(cid:2)

(cid:3)

In combination of (3.7), (3.8) and the stopping criterion (B′), we obtain that for all k

dist(0, ∂Ψk(wk+1, zk+1)) =

ψk(wk+1)

.

||

||∇

(3.8)

0,

≥

(wk+1, zk+1)

||
k + ˜λmin(Q)δ2

(w∗, z∗)

xk+1

µ′
k||

xk

,

||

−

|| ≤

−

where µ′

k := κl(r)

1/σ2

k/σk. This completes the proof of the theorem.

q

3.2 The SsN method for the inner subproblem (3.1)

In this subsection, we propose the SsN method to solve the inner subproblem (3.1). Addition-
ally, we need to carefully study the structure of the projection operator ΠK T L and its associated
generalized Jacobian, which plays a fundamental role in the design of the SsN method.

3.2.1 The computation of the projection operator ΠK T L

Rn
In this part, we focus on how to compute the projection operator ΠK T L(v) efﬁciently, where v
is a given vector. Firstly, it follows from the deﬁnition of K and L in (2.1) that ΠK T L(v) is the
solution of the following optimization problem

∈

min
x∈Rn

x

1
2||
||
s.t. aT x = d,

−

v

2

Furthermore, by introducing a slack variable y, the problem (3.9) can be reformulated as

x

l

≤

≤

u.

min
x∈Rn,y∈Rn

x

1
2 ||
||
s.t. aT x = d,

−

v

2 + δK(y)

(3.9)

(3.10)

y = 0,

x

−

where δK is an indicator function for the polyhedral convex set K. Correspondingly, the dual
problem of (3.10) is

min
λ∈R,z∈Rn

1
2 ||

v

λa

z

||

−

−

2 + δ∗

K(z)

1
2 ||

v

||

−

2 + λd,

(3.11)

11

and the associated KKT condition is

x = ΠK(v

λa), z = v

λa

−
−
aT x = d, x = y, (x, y, λ, z)

−
Rn

∈

x,

R

K

×

×

×

Rn.

(3.12)

Hence, combining (3.10), (3.11) and (3.12), it is sufﬁcient for us to obtain ΠK T L(v) by solving the
problem (3.11). Let

1
2||
(cid:8)
which implies that the optimal solution (ˆλ, ˆz) of the problem (3.11) is given by

ϕ(λ) := inf
z

K(z) + λd

2 + δ∗

λa

−

−

v

(cid:9)

||

z

,

ˆλ = arg min
{

ϕ(λ)

λ

|

∈

R

,

}

(3.13)

and

ˆz = v

ˆλa

ΠK(v

aˆλ),

−
respectively. Since the function ϕ is convex and continuously differentiable with its gradient given
by

−

−

the optimal solution ˆλ of (3.13) is the zero point of

ϕ(λ) =

aT ΠK(v

λa) + d,

∇

−

−
ϕ, i.e.,

∇

It follows by [25] that
break points in the following set

∇

ϕ(λ) is a continuous non-decreasing piecewise linear function which has

ϕ(ˆλ) = 0.

∇

(3.14)

ui

,

vi −
ai

li

vi −
ai

|

T =

(cid:26)

i = 1, . . . , n

.

(cid:27)

Moreover, the range of

∇

ϕ(λ) is a closed and bounded interval

d +

(cid:20)

Xk∈Ia−

lk|

ak| −

Xj∈Ia+

uj|

aj|

, d +

lj|

aj|

,
(cid:21)

uk|

ak| −

Xj∈Ia+
Xk∈Ia−
ai < 0, i = 1, . . . , n
}

|

i

{

where Ia+ =

ai > 0, i = 1, . . . , n
}

i
{
Next, we introduce the algorithm in [25] to obtain the zero point of the equation (3.14). The
procedure consists of a binary search among the 2n break points until bracketing ˆλ between two
breakpoints. The algorithm can be summarized as follows

and Ia− =

|

.

12

Algorithm 2 : The breakpoint search algorithm for solving (3.14) [25]

Initialization: Sort all break points given in (3.2.1) with an ascending order

t(1)

≤

t(2)

. . .

t(2n),

t(i)

≤
≤
(cid:0)
ϕ(t(Il)), and fu =

T,

i = 1,

∀
∈
ϕ(t(Iu)).

, 2n

· · ·

(cid:1)

∇

∇

Il > 1 and t(Iu) > t(Il)
ϕ(t(Im)).

Given Il = 1, Iu = 2n, fl =
While Iu −
Im = [ Il+Iu
2
if fm ≥
0
else

Iu = Im, fu = fm.

], fm =

∇

Il = Im, fl = fm.

end

end
if fu = fl = 0
ˆλ = t(Il).

else

ˆλ = t(Il)

end

fl
fu−fl

(t(Iu)

t(Il)).

−

−

Remark 1 Compared with the bisection method used in [19], the breakpoint search algorithm in
our paper can avoid the complicated operations on some index sets. Hence, it is often faster than
the bisection method in the practical implementation.
Finally, suppose that the optimal solution ˆλ of (3.13) is obtained by Algorithm 2. Then, in combi-
nation with the KKT condition (3.12), we have

where

=

ΠK T L (v) = ΠK (v

ˆλa)

−
Π[l1,u1](v1 −
(cid:16)

Π[li,ui](vi −

ui
ˆλai) = 
vi −

li

ˆλai

ˆλa1), . . . , Π[ln,un](vn −

ˆλan)

T

,

(cid:17)

ui,
ˆλai < ui,

ˆλai ≥
if vi −
if li < vi −
ˆλai ≤
if vi −

li,

(i = 1, . . . , n).

3.2.2 The computation of the HS-Jacobian of ΠK T L



Rn. For the
In the following, we proceed to ﬁnd the HS-Jacobian of ΠK T L at a given point v
sake of clarity, we rewrite the box constraint in (3.9) as a general linear inequality constraint so that
the problem (3.9) becomes

∈

2

(3.15)

min
x∈Rn

x

v

−

1
2||
||
s.t. aT x = d,
g,
Ax

≥

13

(3.17)

(3.18)

(3.19)

where A = [I T
n ,

n ]T
I T

−

∈

R2n×n and g = [lT ,

uT ]T

R2n. Denote

(v) =

I

{

−

∈
AiΠK T L (v) = gi, i = 1, . . . , 2n
}

i
|
(v), where Ai is the ith row of the matrix A. Then, with the

(3.16)

,

and r =
notation introduced above, we may compute the HS-Jacobian by the following results.

, the cardinality of

(v)

|I

I

|

Theorem 2 For any v

Rn, let

I

∈

(v) be given in (3.16) and

Σ := In −

Diag(θ)

Rn×n,

∈

where θ

n is deﬁned as

∈ R

θi =

1
0
(

(v),
i
otherwise.

∈ I

i = 1, . . . , n.

Then

Σ(In −
Σ,
(
is the HS-Jacobian of ΠK T L at v.

P =

1

aT Σa aaT )Σ,

= 0,

if aT Σa
otherwise,

Proof Firstly, it follows from Theorem 1 in [38] that the following matrix

P0 = In −

AT

I(v) a

AI(v)
aT

(cid:21)

(cid:18) (cid:20)

AT

I(v) a

†

(cid:19)

(cid:20)

AI(v)
aT

(cid:21)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

is the HS-Jacobian of ΠK T L at v, where AI(v) is the matrix consisting of the rows of A indexed by
I(v)AI(v) =

(v) and AI(v) imply that AI(v)AT

I(v) = Ir and AT

Secondly, we focus on the calculation of the Moore-Penrose pseudo-inverse in (3.19). Above

Σ.

(v). Moreover, the deﬁnitions of
I
In −
all, it is easy to verify that

I

Then, on one hand, if aT Σa

det

AI(v)
aT

(cid:18) (cid:20)

(cid:2)
= 0, we have that

(cid:21)

P0 =In −

(cid:2)

=In −

AT

I(v) a

(cid:3)

AT

I(v) a

AI(v)
aT
Ir + 1

(cid:18) (cid:20)

(cid:18)
(cid:3)
aaT )Σ,

(cid:2)
=Σ(In −

1
aT Σa
where the last equality holds because Σ = In −

−

AT

I(v) a

= aT Σa.

(cid:19)

(cid:3)

−1

AT

I(v) a

AI(v)
aT
1

,

(cid:2)

(cid:21)

(cid:20)
(cid:19)
(cid:3)
aT ΣaAI(v)aaT AT
aT Σa AI(v)a
I(v) −
1
aT ΣaaT AT
aT Σa

I(v)

(cid:21)

1

AI(v)
aT

,

(cid:21)

(cid:19) (cid:20)

AT

I(v)AI(v) and Σ = Σ2.

14

6
6
On the other hand, if aT Σa = (Σa)T Σa = 0, i.e., Σa = 0, then

P0 =In −
=Σ,

(cid:2)

AT

I(v) a

Ir −

(cid:3)

aT a+2
(1+aT a)2 AI(v)aaT AT
(1+aT a)2 aT AT

I(v)

1

I(v)

1

(1+aT a)2 AI(v)a
aT a
(1+aT a)2

AI(v)
aT

! (cid:20)

(3.20)

(cid:21)

where the ﬁrst equality follows from the deﬁnition of the Moore-Penrose pseudo-inverse and (3.19),
and the last equality is owing to AT

I(v)AI(v)a = a. Thus, the proof is completed.

3.2.3 The SsN method for the inner subproblem (3.1)

In this part, we formally present the SsN method for the subproblem (3.1). Recall that we need to
solve the convex subproblem in each iteration of Algorithm 1, i.e.,

min
w∈Ran(Q)

ψk(w) :=

(cid:26)
Furthermore, note that

1
2h

w, Qw

1
2σk

+

i

||

(cid:0)

u(w)

2

||

− ||

u(w)

−

ΠK T L (u(w))

1
2σk ||

xk

2

||

−

.

(cid:27)

2

||

(cid:1)

ψk(w) = Qw

QΠK T L (u(w)),

(3.21)

−
which implies that the optimal solution ¯w can be obtained through solving the following nonsmooth
piecewise afﬁne equation

∇

Ran(Q) be any given point. We deﬁne the following operator

ψk(w) = 0, w

Ran(Q).

(3.22)

∇

∈

Let w

∈

ˆ∂2ψk(w) := Q + σkQP(u(w))Q,

where the multivalued mapping P : Rn ⇒ Rn×n is the HS-Jacobian of ΠK T L [29].

Now we are ready to state the SsN method as below.

Algorithm 3 : the SsN method for the problem (3.1)

∈

(0, 1

2), δ

(0, 1), τ

Given µ
∈
j = 0, 1, 2, . . . , iterate the following steps:
Step 1. Let
PjQ, where
Mj := Q + σQ
method to ﬁnd an approximate solution dwj to the following linear system

∈
P(u(wj)). Apply the conjugate gradient (CG)

(0, 1). Given an initial point w0

(0, 1] and η

Pj ∈

∈

∈

Rn. For

Mjdwj +

∇

ψk(wj) = 0, dwj

Ran(Q)

∈

(3.23)

such that

|| ≤
Step 2. (Line search) Set αj = δmj , where mj is the ﬁrst nonneigative integer m for which

||∇

∇

||

||Mjdwj +

ψk(wj)

min(η,

ψk(wj)

1+τ ).

ψk(wj + δmdwj )

ψk(wj) + µδm

ψk(wj), dwj

.

i

h∇

≤

Step 3. Set wj+1 = wj + αjdwj .

15

 
Mj is positive deﬁnite in Ran(Q) and

Remark 2
(i) Since the matrix
(3.23) has a unique solution in Ran(Q).
(ii) For the SVM problems with nonlinear kernels, we usually adopt some easy-to-implement algo-
rithm to warm start the SSsNAL method. This process is very essential because if we choose an
arbitrary initial point, when we implement Algorithm 3 we can hardly get the sparse structure in
the early Newton iterations and the cost of the Newton step may be very high. We will present some
necessary details in Section 4.

Ran(Q), the linear system

ψk(wj)

∇

∈

Before analyzing the local convergence rate of the SsN method, we ﬁrst consider the strong
ψk. For the deﬁnitions of semismoothness and γ-order semismoothness, one

semismoothness of
may see the below.

∇

Deﬁnition 1 (semismoothness) [39, 40] Let
nonempty and compact valued, upper-semicontinuous set-valued mapping, and F :
a locally Lipschitz continuous function. F is said to be semismooth at x
multifunction

if F is directionally differentiable at x and for any V

Rn ⇒ Rn×m be a
Rn be
O →
O with respect to the
0,

∈
(x + ∆x) with ∆x

Rn be an open set,

O ⊆

O ⊆

K

:

∈ K

→

F (x + ∆x)

F (x)

V ∆x = o(

∆x

).

−
Let γ be a positive constant. F is said to be γ-order (strongly, if γ = 1) semismooth at X with
respect to

if F is directionally differentiable at x and for any V

(x + ∆x) with ∆x

−

0,

||

||

K

K

−
According to [29, Lemma 2.1] and [41, Theorem 7.5.17], we can immediately obtain that

−

||

||

F (x + ∆x)

F (x)

V ∆x = O(

∆x

∈ K
1+γ).

→

ψk

∇

is strongly semismooth at w with respect to ˆ∂2ψk.

Theorem 3 Let
the unique optimal solution ˜w

be the inﬁnite sequence generated by Algorithm 3. Then
Ran(Q) to the problem (3.22) and

}

{

wj

wj

{

}

converges to

∈

wj+1

˜w

||

−

= O(

wj

||

˜w

||

−

1+τ ).

||

3.2.4 An efﬁcient implementation for solving the linear system (3.23)

In this part, we discuss how to solve the linear system of equations (3.23) efﬁciently. In Algorithm
3, the most time-consuming step is solving the linear system (3.23), so it is essential to make full
use of the the sparse structure of the coefﬁcient matrix to design an efﬁcient algorithm to obtain the
descent direction dwj . For the sake of convenience, we ignore the subscript and rewrite the linear
system (3.23) as follows

(Q + σQ

Q) d =

P

ψk(w), d

Ran(Q),

∈

−∇

(3.24)

P ∈

ψk(w)

P(u(w)) and u(w) = xk

σk(Qw + c). Although Q may be singular, the subspace
where
−
constraint and the fact that
Ran(Q) together imply that there exists a unique solution
to the linear system (3.24). It is extraordinarily time-consuming to solve (3.24) directly when the
dimension of Q is large. However, we only need to update Qd and dT Qd in every iteration of
Algorithm 3. Hence, as shown in the next proposition, instead of computing the solution ˆd of
(3.24) directly, we may choose to solve a relatively much smaller linear system to obtain Qˆd and
ˆdT Qˆd by deliberately employing the sparse structure of the HS-Jacobian.

∇

∈

16

Proposition 5 Let the index set
=
and p =
denotes the cardinality of
th diagonal element Σii is equal to 1 if i
is the solution to the linear system (3.24).

{
J
∈ J

|J |

J

/
, n
1,
}
. Moreover, Let Σ

· · ·

I

(u(w)), where

I

(u(w)) is deﬁned by (3.16)
Rn×n be a diagonal matrix whose i-
Ran(Q)

∈

, otherwise Σii is equal to 0. Assume that ˆd

∈

(a) If aT Σa

= 0, then we have

Qˆd =QT

J vJ (w)

ψk(w) + σ

− ∇

and

aT
J QJ J vJ (w)
aT
J aJ −

ψk
aT
J ∇
−
σaT
J QJ J aJ

J (w)

QT

J aJ

(3.25)

ˆdT Qˆd =vT

+

2vT
−
J QJ J aJ

J (w)QJ J vJ (w)
σ2aT
2σaT
J aJ −
aT
σaT
J aJ −
J QJ J aJ
ΠK T L (u(w)) and vJ (w)

(cid:0)

∈

J (w)

J (w) + (s(w))T Qs(w)
ψk

∇
2 (aT
J QJ J vJ (w)

aT
J ∇

−

J (w))2,
ψk

(3.26)

Rp is the solution to the following linear system
(cid:1)

where s(w) = w

−

Ip + QJ J +

1
σ

(cid:18)

σQJ J aJ aT
σaT
aT
J aJ −

J QJ J
J QJ J aJ (cid:19)

vJ (w) =

Ip +

(cid:18)

σQJ J aJ aT
J
σaT

aT
J aJ −

J QJ J aJ (cid:19)

ψk

J (w),

∇

(3.27)

Rp×p is the submatrix of Q with those rows and columns in

. Moreover,

Rp×n are matrices consisting of the rows of

J

∈
ψk(w), a and Q indexed by

∇

ψk

J (w)

∇

and QJ J ∈
Rp, aJ ∈
, respectively.
J

Rp and QJ ∈
(b) If aT Σa = 0, then we have

and

Qˆd =QT

J vJ (w)

ψk(w)

− ∇

(3.28)

ˆdT Qˆd =vT

J (w)QJ J vJ (w)

2vT

J (w)

∇

−

J (w) + (s(w))T Qs(w),
ψk

(3.29)

ψk
where QJ J , QJ ,
following linear system

∇

J (w) and aJ are same as those in (a) and vJ (w)

1
σ

(cid:18)

Ip + QJ J

vJ (w) =

(cid:19)

ψk

J (w).

∇

Rp is the solution to the

∈

(3.30)

1

= Σ(In −

Proof To prove part (a), combining with Theorem 2 and the condition aT Σa

= 0, we know that
aT ΣaaaT )Σ is an element in the HS-Jacobian of ΠK T L at u(w). From (3.21), it is
ψk(w) = Qs(w). Thus, without loss
Rn×r is a full collum

P
not difﬁcult to establish Then according to (3.21), we have
of generality, we assume that Q can be decomposed as Q = LLT where L
rank matrix and r = Rank(Q). Then, substituting Q = LLT into (3.24), we obtain that

∇

∈

Since L has a full collum rank and

(cid:0)

L

Ir + σLT

L

LT d =

P

LLT s(w), d

Ran(Q).

−
aT ΣaaaT )Σ, (3.31) is further equivalent to

∈

1

(3.31)

Ir + σLT Σ(In −
(cid:18)

LT d =

LT s(w), d

−

∈

Ran(Q).

(3.32)

(cid:1)
= Σ(In −
aaT )ΣL

P
1
aT Σa

(cid:19)

17

6
6
Since ˆd

∈

Ran(Q) is the solution to the linear system (3.24), we have

1
aT Σa

−1

aaT )ΣL

LT s(w)

(cid:19)

−1

J aJ aT

J LJ + σLT

J LJ

LT s(w)

LT ˆd =

=

=

=

=

−

−

−

(cid:18)

−

(cid:18)

−

Ir + σLT Σ(In −
(cid:18)
σ
Ir −
aT
J aJ
(cid:18)

LT

Ir +

σLT
aT
J aJ −
J +

(cid:18)
1
Ip + LJ LT
σ

LT s(w)

−

1
σ

Ip + QJ J +

LT s(w)

−

(cid:19)
LT s(w) +

Ir +

J LJ LT
J

J LJ
J LJ LT

σLT
aT
J aJ −

J aJ aT
σaT
J aJ (cid:19)
J aJ aT
σLJ LT
J LJ LT
σaT
aT
J aJ −
ψk
J aJ aT
J (w)
J ∇
σaT
J QJ J aJ
σQJ J aJ aT
J QJ J
σaT
aT
J QJ J aJ (cid:19)
J aJ −
ψk
J aJ aT
σLT
J (w)
J ∇
+ LT
J
σaT
aT
J QJ J aJ
J aJ −

J aJ (cid:19)
+ LT
J

(cid:18)

−1

(cid:18)

−1

LJ

Ip +

Ir +

(cid:18)

aT
J aJ −

σLT
aT
J aJ −

J LJ
J LJ LT
J aJ aT
σaT

J aJ aT
σaT
σLT
aT
J aJ −
σaJ aT
J QJ J
σaT
J QJ J aJ (cid:19)
σQJ J aJ aT
J
σaT
J QJ J
σaT
J QJ J aJ (cid:19)

J QJ J aJ (cid:19)

Ip +

(cid:18)

Ip +

(cid:18)

aT
J aJ −
σaJ aT

aT
J aJ −

LT
J

J aJ (cid:19)
J LJ
J LJ LT

J aJ (cid:19)

LT s(w)

ψk

J (w)

∇

vJ (w)

(3.33)

J LJ , aT Σa = aT

J aJ and aT ΣL = aT

Ir + σLT Σ(In −

aT Σa aaT )ΣL
where the ﬁrst equality is thanks to (3.31) and the nonsingularity of
the second equality follows from the special 0-1 structure of the diagonal matrix Σ and the fact that
(cid:1)
Rp×r is a matrix consisting
LT ΣΣL = LT
of the rows of L indexed by
; the third equality is obtained by using the Sherman-Morrison-
Woodbury formula [42] twice; the fourth equality is due to QJ J = LJ LT
J (w) =
LJ LT s(w). Finally, it is immediately follows from Qˆd = LLT ˆd, ˆdT Qˆd = (LT ˆd)T LT ˆd and
(3.33) that (3.25) and (3.26) hold true, thus concluding the proof of part (a).
= Σ

As for part (b), since aT Σa = 0, it follows from Theorem 2 that
P
to the proof for part (a), we can obtain the desired results (3.28)-(3.30).

J LJ , where LJ ∈

P(u(w)). Similar

J and

ψk

∇

J

∈

(cid:0)

1

;

Remark 3 In the proof of Proposition 5, we always assume that aT
σ
LT
the matrix Ir −
aT
J aJ
J aJ = aT Σa
because aT
the algorithm to avoid aT

= 0, i.e.,
J aJ aT
J LJ in (3.33) is invertible. Actually this assumption is reasonable
= 0 and σ can be adjusted in an appropriate way in the implementation of
J aJ −

J QJ J aJ 6

J QJ J aJ = 0.

J aJ −

σaT

σaT

From the above discussion, we can see that the computational costs for solving the Newton
linear system (3.23) are reduced signiﬁcantly from O(n3) to O(p3). According to the updating rule
for the primal variable xk in (3.4), the number p is also equal to the number of the unbounded
support vectors [43] in the SVMs, which is usually much smaller than the number of samples
n [44]. So we can always solve the linear system (3.27) or (3.30) at very low costs.

18

6
4 Numerical experiments

In this section, we demonstrate the performances of the SSsNAL method by solving the SVM prob-
lem (1.5) on the benchmark datasets from the LIBSVM data [14]. For a comparison, three state-of-
the-art solvers for solving the SVM problems: the P2GP method [18], the LIBSVM [14] and the
FAPG method [19] are used to solve the same problems. Note that the P2GP method is a two-phase
gradient-based method and its MATLAB code can be downloaded from https://github.com/diseraﬁ/P2GP.
The LIBSVM implements the sequential minimal optimization method [13] which is available on
https://www.csie.ntu.edu.tw/˜cjlin/libsvm/index.html, and the FAPG method is a general optimiza-
tion algorithm based on an accelerated proximal gradient method and the authors shared their code
[0, 1]q, (i = 1, 2, . . . , n).
to us. Moreover, we scale the LIBSVM datasets so that each sample ˜xi ∈
The details of the LIBSVM datasets (the size of the problem, the proportion of nonzeros in the data
and the type of the task) are presented in Table 1.

All experiments are implemented in MATLAB R2018b on a PC with the Intel Core i7-6700HQ

processors (2.60GHz) and 8 GB of physical memory.

4.1 The stopping criterion

We use the following relative KKT residual as a stopping criterion for all the algorithms which only
involves the variable x in the primal problem (1.5)

RKKT(x) = ||

x

−

ΠK T L(x
1 +

−
x
||
||

Qx

c)

−

||

< Tol,

(4.1)

where Tol = 10−31. We also set the maximum numbers of iterations for the SSsNAL, FAPG, P2GP
and LINBSVM to be 200, 20000, 20000 and 20000, respectively. Moreover, the initial values of
variables x0 and w0 are set to be zero vectors in our SSsNAL method.

4.2 Numerical experiments for the support vector classiﬁcation problems

with the linear kernel

In this subsection, the performances of all the methods on the C-SVC problem (1.2) with a linear
kernel function k(x, y) = xT y are presented. For each LIBSVM data set, we adopt the ten-fold
cross validation scheme to select the penalty parameter C. Moreover, the linear C-SVC model
is trained on the training set and the testing data is used to verify the classiﬁcation accuracy. As
for those datasets which do not have the corresponding testing data, we randomly and uniformly
sample 80% of the data from the dataset as a training set, and use the remained 20% as a testing set.
In order to eliminate the effects of randomness, we repeat the above process 10 times and obtain an
average result.

1Note that our SSsNAL method is easier and faster to generate a high accurate solution, say, a solution with the
KKT residual less than 10−6. But we found that the accuracy of the classiﬁcation with the highly accurate solution can
only improve very little, so here we only set Tol = 10−3.

19

Data
splice
madelon
ijcnn1
svmguide1
svmguide3
w1a
w2a
w3a
w4a
w5a
w6a
w7a
australian
breast-cancer
mushrooms
phishing
diabetes
ionosphere
heart
fourclass
colon-cancer
diabetes
a1a
a2a
a3a
a4a
a6a
a7a
cadata
abalone
space ga

41

600

(n+

2000)
296)

n−
train)
483)
1000)

4000 (2000,
(41,

test,
ntest
2175 (1131,
(300,

(n+
n−
test)
ntrain
train,
1044)
1000
(517,
2000 (1000,
300)
35000 (3415, 31585) 91701 (8712, 82989)
2000)
3089 (1089,
(947,
1243
0)
2405) 47272 (1407, 45865)
(72,
2477
3363) 46279 (1372, 44907)
(107,
3470
4769) 44837 (1336, 43501)
(143,
4912
7150) 42383 (1263, 41120)
(216,
7366
9607) 39861 (1198, 38663)
(281,
9888
(954, 31607)
(525, 16663) 32551
17188
(739, 24318)
4363) 25057
(740,
5103
383)
*)
*
(307,
690
444)
*)
*
(239,
683
4208)
*)
*
8124 (3916,
4898)
*)
*
11055 (6157,
268)
*)
*
(500,
768
126)
*)
*
(225,
651
150)
*)
*
(120,
270
555)
*)
*
(307,
862
40)
*)
*
(22,
62
268)
*)
*
(500,
768
1210)
*)
*
(395,
1605
1693)
*)
*
(572,
2265
2412)
*)
*
3185
(773,
*)
*
3593)
4781 (1188,
*)
*
11220 (2692,
8528)
*)
*
16100 (3918, 12182)
*)
*
*)
11080
*)
*
*)
4177
*)
*
*)
3107

(*,
(*,
(*,
(*,
(*,
(*,
(*,
(*,
(*,
(*,
(*,
(*,
(*,
(*,
(*,
(*,
(*,
(*,
(*,

(*,
(*,
(*,

q
60
500
22
4
22
300
300
300
300
300
300
300
14
10
112
68
8
34
13
2
2000
8
119
119
122
122
122
122
8
8
6

density
1.000
0.999
0.591
0.997
0.805
0.039
0.039
0.039
0.039
0.039
0.039
0.039
0.874
1.000
0.188
0.441
0.999
0.884
0.962
0.983
1.000
0.999
0.116
0.115
0.114
0.114
0.114
0.115
1.000
0.960
0.750

type
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
classiﬁcation
regression
regression
regression

Table 1: The details of the LIBSVM Datasets. ‘
∗
data or the features of the training data and testing data are not equal to each other.

’ means that it does not have the corresponding

20

×

10t” in all the following tables, e.g., 1.0-4 denotes 1.0

We report the numerical results in Table 2. For the numerical results, we report the data set
name (Data), the number of samples (n) and features (q), the value of penalty parameter (C), the
number of unbounded support vectors (suppvec), the relative KKT residual (RKKT), the computing
time (Time), the iteration number (Iter) and the percentage of the classiﬁcation accuracy on the
testing data set (Accuracy). Particularly, we use “s sign(t)
” to denote a number of the form
t
|
10−4. The computing time here is in
“s
the format of “hours:minutes:seconds”. Notably, “00” and “T” in the time column denote that the
elapsed time is less than 0.5 second and more than 2 hours, respectively. For the SSsNAL method,
we report the number of iterations in the form of “Iteralm(Iterssn)”, where Iteralm and Iterssn represent
the number of outer iterations and the average number of inner iterations, respectively. Note that
the P2GP method is an alternating method between two different phases, so we report its number
of iterations in the form of “Iter0(IterI, IterII)”, where Iter0 represents the alternating number of
phases. IterI and IterII represent the the average number of iterations in phase I and II, respectively.
Moreover, the reported unbounded support vectors are obtained from the SSsNAL method.

|
×

In Table 2, we present the performances of the four algorithms on various C-SVC problems
(1.2) with the linear kernel function. Note that our SSsNAL method not only achieves the desired
accuracy in all cases, but also outperforms the other three algorithms. For example, in the ‘w8a’
data set, only the SSsNAL and FAPG methods achieve the predetermined accuracy, but the SSs-
NAL method only takes 5 seconds to reach the desired accuracy while the FAPG method needs
more than three minutes and the other two solvers needs much more time. The test accuracy of
the classiﬁcation on most of problems are nearly the same for different algorithms except for the
LINBSVM. For the datasets ‘jcnn1’, ‘a4a’, ‘a8a’ and ‘a9a’, the accuracies of the LINBSVM are
worse than those of the other three algorithms, because its KKT residuals obviously fail to achieve
the required accuracy. Hence, to achieve a satisfactory classiﬁcation, an appropriate precision re-
quirement is necessary.

4.3 Numerical experiments for the Nystr¨om method and the random Fourier
features method to the RBF kernel support vector classiﬁcation prob-
lems

In this subsection, we compare the performances of the SSsNAL, FAPG and P2PG methods to the
approximated linear kernel support vector classiﬁcation problem (1.2) by the Nystr¨om method and
the random Fourier features method.

First, we give a brief introduction to the Nystr¨om method [45]. Recall that (˜xi, yi), i = 1, ..., n,
Rn×n be the RBF
denote a set of training examples, where ˜xi ∈
kernel matrix with Kij = exp−||˜xi−˜xj||2/2α (i, j = 1, . . . , n), where α > 0 is a given parameter.
Instead of computing the whole kernel matrix directly, the Nystr¨om method provides a reduced-
rank approximation to the kernel matrix K by choosing a landmark points set
consisting of the
column indicators of matrix K and then setting

Rp and yi ∈ {

. Let K

1
−

+1,

∈

I

}

K = Kn,rK −1

r,r K T

n,r,

where Kn,r ∈
also a submatrix of K with the rows and columns in

Rn×r is a submatrix consisting of the columns of K indexed by

, Kr,r ∈
and r is the cardinality of the set
I

e

I

I

Rr×r is
. Next,

21

2
2

Data

C

suppvec

madelon
ijcnn1
cod-rna
svmguide1
w1a
w2a
w3a
w4a
w5a
w6a
w7a
w8a
mushrooms
phishing
skin nonskin
a2a
a3a
a4a
a6a
a7a
a8a
a9a

0.031
1024
2
64
1
2
2
1
1
1
2
32
0.25
0.063
8192
0.25
0.5
256
0.03
0.5
2
1

RKKT
c
d
b
a
|
|
|
1.8-2
9.9-4
|
1.7-4
9.9-4
|
1.1-4
6.9-4
|
2.7-3
4.7-4
|
1.1-3
9.7-4
|
9.0-4
9.8-4
|
7.8-4
9.7-4
|
1.6-3
9.9-4
|
1.2-3
9.9-4
|
1.8-3
9.9-4
|
1.0-3
9.9-4
|
5.3-5
9.9-4
|
3.2-2
9.8-4
|
2.9-1
9.8-4
|
3.7-3
2.0-1
|
3.7-3
9.8-4
|
2.3-3
9.9-4
|
1.1-4
9.9-4
|
2.3-2
9.7-4
|
2.5-3
9.8-4
|
6.8-4
9.8-4
|
1.3-3
9.8-4
|

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

1.2-2
2.8-2
1.6-0
6.9-4
2.7-2
1.5-2
1.4-3
1.9-3
1.8-3
2.9-2
1.5-2
1.5-1
1.4-2
5.6-3
1.2-2
2.5-3
1.4-3
1.5-2
5.2-3
4.5-3
7.4-1
1.4-0

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

4.1-4
9.6-4
3.9-4
3.1-4
2.6-4
1.2-4
4.9-4
7.8-4
2.1-4
5.0-4
8.7-4
8.4-4
6.2-4
6.3-4
5.7-4
5.2-4
7.1-4
4.5-4
5.9-4
5.3-4
5.8-4
4.2-4

|

Time
d
c
b
a
|
|
|
4:45
01
|
|
1:28
T
|
75
T
|
07
01
01
02
02
06
05
01

43
|
6:04
|
13:27
|
31:57
|
1:18:32
|
T
|
T
|
33:10
|
T
1:03
|
1:02:25
T

01
45
9
|
01
|
01
|
01
|
02
|
03
|
05
|
16
|
02
|
3:19
2
|
2
|
00
01
01
06
01
04
10
15

01
02
02
00
01
01
01
01
01
02
01
05
01
01
00
00
01
01
01
01
01
01

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

|
01
|
40
|
00
01
01
08
04
02
07
06

01
4:04
9:29
24:50
T
T
T
T

20000

1103

|

|
1362

Iter
d
c
b
a
|
|
|
20000
3(15,37)
|
38(1266,16454)
|
17(400,8774)
636(4352,10850)
|
20000
5(30,287)
|
4(29,772)
20000
|
7(41,2072)
|
5(29,1841)
|
8(44,3321)
|
4(30,2072)
|
4(37,1250)

20000
20000
15315
4643
|
20000

|

15(330,11069)
20000

|

692

|

14644

|

2(63,149)
12(210,19888)
|
20000
1(9,17)
|
3(35,967)
3(37,819)
25(304,19667)
|
5(120,2722)
1(42,1086)
3(77,2904)
2(59,2056)

3(9)
|
5(4)
|
6(2)
|
6(3)
|
6(7)
|
7(7)
|
7(7)
|
5(10)
7(7)
|
6(11)
5(10)
10(8)
7(7)
6(6)
5(2)
6(5)
5(5)
6(9)
4(8)
6(6)
5(8)
5(9)

|
|
|
|
|
|
|
|
|
|

133
|
3386
801
|
301
|
461
|
673
|
809
|
906
|
|
1573
|
2784
|
875
|
|
11528
|
480
|
520
|
101
|
407
|
438
|
4638
456
|
1142
2228
2318

Accuracy (%)
c
a
d
b
|
|
|
59.0
59.0
|
|
91.4
92.1
|
|
95.2
95.2
|
|
94.6
94.8
|
|
97.7
97.7
|
|
98.0
98.0
|
|
98.2
98.2
|
|
98.4
98.4
|
|
98.5
98.5
|
|
98.7
98.7
|
|
97.2
97.2
|
|
98.7
98.7
|
|
100
100
|
|
94.1
|
83.3
|
80.9
|
84.0
|
83.9
|
84.2
|
84.6
|
84.5
|
84.8
|

|
89.6
82.1
81.0
84.0
83.5
84.2
84.6
84.6
84.8

|
|
|
|
|
|
|
|
|

|
|
|
|
|
|
|
|
|

59.0
91.5
95.2
94.7
97.7
98.0
98.2
98.4
98.5
98.7
97.3
98.7
100
94.1
83.1
81.0
84.0
83.9
84.2
84.6
84.5
84.8

58.6
84.2
0.0
94.6
97.7
98.0
98.2
98.4
98.5
98.7
97.3
87.7

|
|
|
|
|
|
|
|
|
|
|
|
100

93.8
83.3
81.9
83.6
57.7
84
84
67.6
67.6

|
|
|
|
|
|
|
|
Table 2: Comparisons of the SSsNAL, FAPG, P2GP, and LINBSVM solvers for the C-SVC problem (1.2) with the linear kernel function. In
the table, “a” denotes the SSsNAL method, “b” denotes the FAGP method, “c” denotes the P2GP method, and “d” denotes the LINBSVM
solver.

|
15893
9601
5229
2318

20000
20000

|
|
|
|
|
|
|
|

20000

|
|
|
|

|
|
|

|
|

134
16410
234
50
126
169
194
226
229
360
601
5145
376
220
156
107
102
191
178
102
182
190

we can use the reduced-rank approximation matrix ˜K to replace the original kernel matrix K in the
problem (1.2) and turn the problem into a linear kernel support vector classiﬁcation problem.

Next, we introduce the random Fourier features method [22] which is another commonly used
randomized algorithm for approximating the kernel matrix. Let k(x, y) = exp−||x−y||2/2α be the
RBF kernel function. Then, according to the Bochner’s theorem [46], it can be regarded as the
Fourier transform of the Gaussian probability density function p(ω) = (2π/αn)− n
2 exp−α||ω||2/2,
i.e.,

k(x, y) =

expiωT (x−y) p(ω) = Eω[zω(x)T zω(y)],

Rq

Z

R2 is a real-valued mapping. So, the ran-
where the function zω(x) := [cos(ωT x), sin(ωT x)]T
dom Fourier features method is constructed by ﬁrst sampling the independent and identically dis-
tributed vectors ω1, . . . , ωN ∈
to estimate k(x, y). If K
∈
method is to approximate K by

∈
Rq from p(ω) and then using the sample average 1
N
Rn×n is the RBF kernel matrix, then the random Fourier features

N
i=1 zωi(x)T zωi(y)

P

K = ZωZ T
ω ,

where

Zω := 

cos(ωT
...
cos(ωT

1 ˜x1)

1 ˜xn)

1 ˜x1)

sin(ωT
...
sin(ωT
1 ˜xn)

e

· · ·
. . .

N ˜x1)

cos(ωT
...
cos(ωT
N ˜xn)

sin(ωT
N ˜x1)
...
sin(ωT
N ˜xn)

Rn×2N ,



∈




· · ·
Rq (i = 1, . . . , n) are training samples. Similarly, we use the matrix

and ˜xi ∈
K to replace the
original kernel matrix K in the problem (1.2) and turn the problem into a linear kernel support
vector classiﬁcation problem with Zω as new training samples.




e

}

{

{

I

I

was given by r =

128, 256, 512, 1024

128, 256, 512, 1024

and the cardinality of

In the implementation of the Nystr¨om method, we replace Kr,r by Kr,r +σI with σ = 10−3 [47]
to avoid numerical instabilities. Moreover, we chose the k-means clustering algorithm [48] to de-
termine the landmark points set
.
}
Similarly, in the implementation of the random Fourier features method, we set the sampling num-
ber to be 2N =
. As for the parameters C and α, they are also selected by the
ten-fold cross validation. After approximating the kernel matrix by the Nystr¨om method and the
random Fourier features method, we apply the SSsNAL, FAPG and P2PG methods, respectively, to
solve the approximated linear C-SVC problems. Moreover, the termination criterion of the three al-
Kij, i, j = 1, . . . , n.
gorithms are the same as (4.1) except for replacing Q with
Tables 3 and 4 show the performances of the SSsNAL, FAPG and P2PG methods on several
approximated linearization RBF kernel C-SVC problems (1.2) obtained by the Nystr¨om method
and the random Fourier features method, respectively. For comparisons, we also report the number
of the unbounded support vectors and the classiﬁcation accuracy of the SSsNAL method on the
original RBF kernel C-SVC problems in the last column of the two tables. Above all, similar to the
previous subsection, the SSsNAL method takes less time to solve the approximation problems than
the FAPG and P2PG methods with the same stopping criterion. Indeed, as we see in the next sub-
section, for all of the three algorithms the computing times for solving the approximation problems
are signiﬁcantly less than those for solving the original problems. However, the test accuracies for
solving the original problems are more ideal. In addition, the numbers of the unbounded support
vectors obtained by solving the approximation problems are also quite different from the true num-
bers. As is explained in [49], if the Nystr¨om and the random Fourier features methods are used to

Qij = yiyj

Q, where

b

b

b

23

Data
C:α

r

suppvec

svmguide1
0.063:1

w5a
256:256

w6a
16:16

phishing
2:4

a6a
32:16

a7a
32:4

128
256
512
1024
128
256
512
1024
128
256
512
1024
128
256
512
1024
128
256
512
1024
128
256
512
1024

35
44
44
44
14
15
20
16
7
9
13
15
42
43
60
76
43
46
66
75
48
46
72
78

RKKT
c
b
a
|
|
9.5-4
1.0-3
9.9-4
9.9-4
1.0-3
9.9-4
9.5-4
9.7-4
9.0-4
9.9-4
9.9-4
9.9-4
8.3-4
7.1-4
9.1-4
9.3-4
9.9-4
9.5-4
9.9-4
9.9-4
9.9-4
9.9-4
9.8-4
9.9-4

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

1.3-4
1.1-4
8.7-4
1.1-4
1.2-4
2.8-4
1.5-4
2.7-4
1.0-4
4.5-4
5.5-4
5.6-4
1.5-4
1.3-4
1.5-4
1.7-4
1.6-4
1.6-4
1.7-4
1.9-4
6.6-4
6.6-4
6.4-4
6.8-4

8.0-4
8.8-4
6.9-4
3.1-4
9.6-4
8.9-4
8.9-4
8.4-4
8.4-4
4.0-4
7.7-4
6.9-4
8.2-4
5.2-4
6.6-4
6.2-4
7.2-4
7.9-4
7.8-4
6.9-4
7.1-4
8.1-4
7.3-4
6.4-4

01
00
00
01
00
00
01
01
01
01
01
03
00
01
01
02
00
00
01
02
01
01
01
02

Time
c
b
a
|
|
01
01
01
02
03
03
05
07
05
07
11
17
01
01
02
03
03
04
06
10
05
07
10
17

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

Iter
c
b
a
|
|
1(6,100)
1(6,72)
1(6,129)
1(6,107)
1(5,173)
1(5,85)
4(20,261)
1(5,96)
2(26,149)
1(25,80)
1(24,85)
1(24,70)
10(162,7188)
59(487,6628)
3(18,128)
3(60,42)
1(5,403)
1(6,421)
1(5,358)
1(7,367)
1(5,468)
1(5,477)
1(5,363)
3(17,540)

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

5(5)
6(4)
5(5)
6(4)
5(3)
6(3)
6(3)
6(3)
9(2)
9(2)
8(3)
8(2)
6(3)
6(4)
6(4)
5(5)
9(2)
5(3)
5(4)
5(4)
7(4)
6(4)
6(5)
6(5)

403
403
372
376
460
376
372
360
592
573
561
536
263
268
254
255
551
535
532
528
647
640
625
602

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

01
00
00
01
01
01
03
02
01
01
02
03
15
34
01
01
01
02
03
06
02
03
05
14

Accuracy (%)
a
c
b
|
|
73.7
76.5
77.6
77.0
90.8
90.1
97.0
96.8
64.8
93.6
93.3
94.8
76.7
82.2
85.3
77.1
60.3
42.0
59.8
34.5
58.6
58.6
58.6
72.6

74.0
76.5
77.1
76.7
97.0
89.7
97.0
97.0
64.6
94.6
93.4
94.9
76.7
75.2
89.4
75.5
77.1
34.5
69.7
32.3
58.6
75.7
70.3
75.7

73.7
76.7
77.5
76.9
90.8
96.9
97.0
97.3
64.8
93.6
93.3
94.9
79.2
83.8
83.6
74.7
71.3
41.3
54.1
41.3
58.6
59.7
54.8
70.6

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

Original
Ra
Rs
|

96.3

36

|

1390

2613

3528

98.5

98.8

97.4

|

|

|

681

460

|

|

84.6

84.9

Table 3: Comparisons of the SSsNAL, FAPG and P2GP methods for the standard C-SVM problem (1.2) with the RBF
kernel function by the Nystr¨om method. In the table, “a” denotes the SSsNAL method, “b” denotes the FAGP method
and “c” denotes the P2PG method. In the last column, “Rs” denotes the reference number of unbounded support vectors
and “Ra” denotes the reference classiﬁcation accuracy on the testing data set (%).

24

reduce the size of training sets by selecting the candidate vectors (i.e., the support vectors), it may
produce large errors. This also shows the necessity and importance of solving the original problem.
Although the Nystr¨om method and the random Fourier features method have some disadvan-
tages, they are very suitable to be implemented to generate an initial iteration point. Therefore, in
the following subsection we adopt the random Fourier features method with 2N = 1024 samples
to warm start the SSsNAL method.

Data
C:α

svmguide1
0.063:1

w5a
256:256

w6a
16:16

phishing
2:4

a6a
32:16

a7a
32:4

2N

suppvec

128
256
512
1024
128
256
512
1024
128
256
512
1024
128
256
512
1024
128
256
512
1024
128
256
512
1024

40
38
35
39
904
441
492
368
460
454
2031
2139
1053
466
554
690
1069
443
538
729
409
438
550
747

RKKT
a
c
b
|
|
1.0-3
9.5-4
9.9-4
1.0-3
5.4-4
5.0-4
4.8-4
4.0-4
9.8-4
1.0-3
1.0-3
1.0-3
4.8-4
5.0-4
6.4-4
8.3-4
5.0-4
3.2-4
3.6-4
3.3-4
5.8-4
5.2-4
5.2-4
5.1-4

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

1.5-4
2.2-4
1.9-4
2.1-4
4.7-4
3.4-4
6.9-4
5.0-4
7.8-4
1.1-4
1.2-4
1.3-4
1.9-4
1.9-4
1.9-4
1.6-4
6.9-4
6.9-4
6.9-4
7.7-4
1.2-4
1.2-4
1.2-4
1.5-4

3.1-4
8.2-4
2.2-4
4.2-4
5.4-4
7.2-4
6.5-4
8.1-4
6.2-4
7.5-4
2.4-4
4.9-4
6.4-4
3.8-4
4.1-4
2.9-4
8.3-4
6.6-4
5.2-4
5.4-4
6.9-4
7.5-4
3.7-4
4.8-4

01
01
00
01
00
01
01
03
01
01
03
08
00
01
02
03
00
00
01
02
00
01
01
03

Time
c
b
a
|
|
01
00
10
01
01
01
02
03
06
03
05
10
01
01
02
02
01
01
02
03
01
02
02
04

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

01
00
00
01
10
07
04
07
16
10
06
12
38
64
47
21
01
02
04
08
15
02
03
05

3(9)
|
4(8)
|
4(7)
|
4(7)
|
8(2)
|
10(2)
10(2)
12(2)
6(4)
6(4)
6(6)
6(5)
4(5)
5(5)
6(7)
6(9)
6(2)
6(2)
7(2)
6(2)
4(3)
5(3)
6(3)
5(4)

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

Iter
a
c
b
|
|
2(55,150)
2(55,104)
3(60,97)
2(55,92)
10(50,3965)
5(25,1555)
3(15,655)
4(20,589)
4(122,4239)

|
|
|
|
5(25,542)
5(25,542)
4(20,517)
6(144,20952)
13(389,22091)
8(172,9024)
5(69,2049)
1(50,744)
1(43,710)
2(45,601)
2(43,681)
3(129,5375)
1(48,361)
1(26,315)
2(55,280)

296
290
294
314
301

|
|
|
|
|
301
|
301
|
301
|
1296
501
485
495
434
301
281
247
434
368
401
301
301
301
301
301

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

Accuracy (%)
a
c
b
|
|
96.6
95.0
94.7
95.3
52.8
50.8
62.7
87.4
47.6
62.4
71.0
92.8
92.4
92.7
92.7
94.3
73.5
78.8
79.6
82.1
76.6
80.1
82.9
83.6

96.5
94.8
94.8
95.3
54.1
51.0
64.7
88.8
50.9
61.0
88.6
92.9
92.5
92.8
92.7
94.3
73.6
78.7
79.5
82.1
76.8
80.1
82.8
83.5

96.6
95.1
94.5
95.4
49.4
55.1
58.1
89.4
43.0
63.0
69.2
92.6
92.4
93.8
92.7
94.3
74.1
78.5
79.7
82.1
76.0
80.0
82.9
83.5

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

Original
Ra
Rs
|

96.3

36

|

1390

2613

3528

98.5

98.8

97.4

|

|

|

681

460

|

|

84.6

84.9

Table 4: Comparisons of the SSsNAL, FAPG and P2GP methods for the standard C-SVM problem (1.2) with the RBF
kernel function by the random Fourier features method. In the table, “a” denotes the SSsNAL method, “b” denotes the
FAGP method and “c” denotes the P2PG method. In the last column, “Rs” denotes the reference number of unbounded
support vectors and “Ra” denotes the reference classiﬁcation accuracy on the testing data set (%).

4.4 Numerical experiments for the support vector classiﬁcation problems

with the RBF kernel

In this subsection, we present the numerical experiments of all the algorithms on the C-SVC prob-
lem (1.2) with the RBF kernel function.

In Table 5, we present the performances of the four algorithms on the various problems. In order
n dense kernel matrix Q, which may cause an out-of-memory exception, we

to avoid saving the n

×

25

Data

C:α

suppvec

svmguide1
w1a
w2a
w3a
w4a
w5a
w6a
mushrooms
phishing
a2a
a3a
a4a
a6a
a7a

2
6

0.063:1
64:64
16:16
32:128
32:128
256:256
16:16
1:0.063
2:4
128:512
128:32
32:4
32:16
32:4

30
474
713
889
1193
548
2668
4040
3529
398
157
148
628
405

RKKT
c
a
d
b
|
|
|
9.4-4
9.6-4
|
3.0-5
9.9-4
|
9.9-5
9.9-4
|
1.7-5
9.9-4
|
1.8-3
9.9-4
|
5.3-3
7.2-4
|
7.5-1
2.5-2
|
1.5-2
5.8-4
|
1.0-3
9.8-4
|
2.6-6
9.2-4
|
3.1-5
6.9-4
|
2.7-4
3.2-4
|
3.8-2
4.4-4
|
2.9-1
5.2-4
|

|
|
|
|
|
|
|
|
|
|
|
|
|
|

|
|
|
|
|
|
|
|
|
|
|
|
|
|

9.2-1
5.3-2
2.5-1
3.5-2
4.6-2
1.3-2
9.3-1
7.2-1
1.2-0
2.4-2
4.4-1
1.3-0
1.6.0
1.2-0

4.1-4
7.8-4
5.6-4
6.0-4
6.9-4
9.4-4
2.9-4
4.7-4
5.3-4
6.9-4
7.3-4
5.7-4
6.2-4
5.2-4

Time
a
d
c
b
|
|
|
3
6
28
|
|
9
9
4:17
|
|
8:06
13
19
|
|
16:24
27
32
|
|
40:46
T
4:11
|
|
1:10:11
T
29:46
|
|
T
T
T
|
|
|
5:26
11:13
|
5:19
32:00
8
|
3
|
7
|
|
18:30
|
1:11:43

1
|
6
|
11
|
22
|
37
|
31
|
1:03:04
2:07
|
1:38
|
4
2
|
|
4
3
|
|
6
13
|
1:31
7:17

|
2:07
4:19

9:48
T

|
T

|
|

|
|

1:11:27
T

|

|

44:31
1:21:37

Iter
a
d
c
b
|
|
|
3(60,113)
|
5(38,265)
|
4(58,187)
|
4(52,443)
|
2(12,182)
|
1(5,73)
|
157
|
150
195
201
121
195
332
301

|
1(5,30)
|
1(5,23)
|
2(18,122)
|
4(59,1584)
2(41,149)
2(45,105)
1(45,194)
1(39,62)

327
7(2)
|
234
6(3)
|
333
8(6)
|
378
9(2)
|
482
9(3)
|
9(1)
301
|
13(3)
8(3)
9(2)
4(1)
4(1)
7(2)
8(2)
8(2)

|
|
|
|
|
|
|

|
|
|
|
|
|
|

|

728
20000
20000
20000
20000

|
|
|
|
|
20000
8142
|
20000

20000

20000

|
20000
|
20000
|
20000
|
16871

Accuracy (%)
c
a
d
b
|
|
|
96.3
96.3
|
97.8
97.8
|
98.1
98.1
|
98.2
98.1
|
98.4
98.4
|
98.3
98.5
|
42.8
98.8
|
99.8
99.8
|
97.4
97.4
|
82.3
81.8
|
83.3
83.3
|
84.5
84.5
|
83.1
84.3
|
81.8
84.7
|

|
|
|
|
|
|
|
|
|
|
|
|
|
|

|
|
|
|
|
|
|
|
|
|
|
|
|
|

84.1
81.1
56.7
50.9
46.9
75.2
37.6
92.4
93.4
71.4
70.1
72.0
71.3
71.4

96.3
97.8
98.1
98.1
98.4
98.5
98.8
99.8
97.4
82.1
83.3
84.5
84.4
84.6

Table 5: Comparisons of the SSsNAL, FAPG, P2GP, and LINBSVM solvers for the standard C-SVM problems (1.2) with the RBF kernel
function. In the table, “a” denotes the SSsNAL method, “b” denotes the FAGP method, “c” denotes the P2GP method, and “d” denotes the
LINBSVM solver.

{

n,

⌋}

Rn×p, where p = min

3.6
⌊

only save partial columns of Q, denoted by QP. Note that QP ∈
×
107/n
. So the number of entries in QP is less than 60002. Then, we compute the rest entries of Q
on demand in each iteration of the FAGP method and the P2GP method. We can see the LIBSVM
solver fails to produce a reasonably accurate solution for all the problems and its accuracies of the
classiﬁcation are worse than those of the other three solvers. Meanwhile, the SSsNAL, FAGP and
P2GP methods can solve all the problems successfully. Nevertheless, in most cases, the FAGP and
P2GP methods require much more time than the SSsNAL method. One can also observe that for
large-scale problems, the SSsNAL method outperforms the FAGP and P2GP methods by a large
margin (sometimes up to a factor of 10
100). In fact, in almost all cases, when the iteration
point is near the solution point, the number of unbounded support vectors is much smaller than the
number of training data. This implies that the subproblems in our SSsNAL method can be solved
by the semismooth Newton method very efﬁciently because the Newton direction can be obtained
just by solving a much smaller linear system. The superior numerical performance of the SSsNAL
method shows that it is a robust, high-performance algorithm for large-scale SVM problems.

∼

4.5 Numerical experiments for the support vector regression problems with

the RBF kernel

Three experiments based on the benchmark datasets from the LIBSVM data are performed to
compare the SSsNAL method with the FAPG and P2PG methods on the SVR problems (1.4)
with the RBF kernel. In these experiments, we also randomly partition each dataset
∈
Rq
Dte in the proportion 8:2 and normalize
Dtr and a testing set
each variable (˜xi, yi) into [0, 1]q+1. Additionally, the spread parameter α in the Gaussian kernel
k(x, y) = exp−||x−y||2/2α and the regularization parameter C are selected by the ten-fold cross val-
idation, while the insensitivity parameter ε is determined directly by the method proposed in [50].
The mean square error, deﬁned by

R , i = 1, ..., n
}

into a training set

(˜xi, yi)

×

{

MSE =

1
nte

kT
˜xi

w∗

yi −

−

b∗

2

,

(cid:1)

Xi∈Dte

(cid:0)

R is
is used to characterize the prediction accuracy on the testing set, where (w∗, b∗)
∈
Rntr with
the optimal solution of the SVR problem (1.3), k˜xi = [k(˜xj1, ˜xi), . . . , k(˜xjntr , ˜xi)]T
˜xjk ∈ Dtr, k = 1, . . . , ntr, is a given vector and ntr and nte are the cardinality of the training set
and a testing set, respectively. Considering that solving the SVR problems usually requires higher
accuracy, we set the tolerance of the KKT residual Tol = 10−6 in the experiments.

Rntr

×

∈

In Table 6, we repeat the random partition for 10 times to yield the mean performances of the
three algorithms. As we see in the table, the SSsNAL method not only uses less time but also needs
fewer iterations than the other two algorithms under the same stopping criterion. This is due to
the fast linear convergence rate of the SSsNAL method and the fact that the inner subproblem can
be solved very quickly and cheaply by the semismooth Newton method, which shows again the
superiorities of the SSsNAL method in solving the large-scale SVM problems.

27

Data
C:α:ε
cadata
512:0.0313:0.0686

abalone
512:0.0625:0.0559
space ga
64:0.250:0.0630

suppvec

2

2

4

RKKT
c
b
a
|
|
9.9-8

2.8-7

2.2-7

|

|

|

9.8-8

2.9-7

1.4-7

|

|

|

Time
c
b
a
|
|
2:15

|

|

4.9-7

29

Iter
c
b
a
|
|
68(340,595)

|

2.88-2

2:07

8(3)

149

|

6.9-7

2.7-7

9

2

|

|

1:11

2(9)

216

|

|

114(635,2406)

4.63-2

7

1

|

|

24

9(3)

661

|

|

14(124,2603)

1.15-1

MSE
c
b
a
|
|
2.88-2

4.63-2

1.16-1

|

|

|

2.88-2

4.63-2

1.16-1

|

|

|

Table 6: Comparisons of the SSsNAL, FAPG and P2GP methods for the SVR problems (1.4) with the RBF kernel
function. In the table, “a” denotes the SSsNAL method, “b” denotes the FAGP method, and “c” denotes the P2GP
method.

5 Conclusion

In this paper, we proposed a highly efﬁcient SSsNAL method for solving the large-scale convex
quadratic programming problem generated from the dual problem of the SVMs. By leveraging the
primal-dual error bound result, the fast local convergence property of the AL method can be guar-
anteed, and by exploiting the second-order sparsity of the Jacobian when using the SsN method,
the algorithm can efﬁciently solve the problems. Finally, numerical experiments demonstrated the
high efﬁciency and robustness of the SSsNAL method.

Acknowledgements

We would like to thank Prof. Akiko Takeda at the University of Tokyo and Dr. Naoki Ito at
Fast Retailing Co., Ltd. for sharing the codes of the FAPG method and providing some valuable
comments for our manuscript. Additionally, we also thank Prof. Ingo Steinwart at University of
Stuttgart for his valuable suggestions to improve the manuscript.

References

[1] V. N. Vapnik and A. Lerner, “Pattern recognition using generalized portrait method,” Autom.

Remote Control, vol. 24, pp. 774–780, 1963.

[2] H. Taira and M. Haruno, “Feature selection in SVM text categorization.,” in Proceedings
of the Sixteenth National Conference on Artiﬁcial Intelligence and Eleventh Conference on
Innovative Applications of Artiﬁcial Intelligence, pp. 480–486, 1999.

[3] M. Goudjil, M. Koudil, M. Bedda, and N. Ghoggali, “A novel active learning method us-
ing SVM for text classiﬁcation,” International Journal of Automation and Computing, no. 3,
pp. 1–9, 2018.

[4] R. Azim, W. Rahman, and M. F. Karim, “Bangla hand written character recognition using
support vector machine,” International Journal of Engineering Works, vol. 3, no. 6, pp. 36–
46, 2016.

28

[5] Y. Lin, F. Lv, S. Zhu, and M. Yang, “Large-scale image classiﬁcation: Fast feature extraction
and SVM training,” IEEE Conference on Computer Vision and Pattern Recognition, vol. 1,
no. 14, pp. 1689–1696, 2011.

[6] C. Cortes and V. Vapnik, “Support-vector networks,” Machine Learning, vol. 20, no. 3,

pp. 273–297, 1995.

[7] B. E. Boser, I. M. Guyon, and V. N. Vapnik, “A training algorithm for optimal margin classi-
ﬁers,” in The Fifth Annual Workshop on Computational Learning Theory, pp. 144–152, 1992.

[8] N. Aronszajn, “Theory of reproducing kernels,” Transactions of the American Mathematical

Society, vol. 68, no. 3, pp. 337–404, 1950.

[9] V. Vapnik, “Statistical learning theory,” Encyclopedia of the Sciences of Learning, vol. 4,

pp. 3185–3189, 1998.

[10] J. Yin and Q. N. Li, “A semismooth Newton method for support vector classiﬁcation and
regression,” Computational Optimization and Applications, vol. 73, no. 2, pp. 477–508, 2019.

[11] E. Osuna, R. Freund, and F. Girosi, “An improved training algorithm for support vector ma-
chines,” in Neural networks for signal processing VII. Proceedings of the 1997 IEEE signal
processing society workshop, pp. 276–285, IEEE, 1997.

[12] T. Joachims, “Making large-scale SVM learning practical,” Technical Reports, vol. 8, no. 3,

pp. 499–526, 1998.

[13] J. C. Platt, Fast Training of Support Vector Machines Using Sequential Minimal Optimization.

MIT Press, 1999.

[14] C. C. Chang and C. J. Lin, “LIBSVM : A library for support vector machines,” ACM Trans-

actions on Intelligent Systems and Technology, vol. 2, no. 3, pp. 1–27, 2011.

[15] R. E. Fan, P. H. Chen, and C. J. Lin, “Working set selection using second order information
for training support vector machines,” Journal of Machine Learning Research, vol. 6, no. 4,
pp. 1889–1918, 2005.

[16] P. H. Calamai and J. J. Mor´e, “Projected gradient methods for linearly constrained problems,”

Mathematical Programming, vol. 39, no. 1, pp. 93–116, 1987.

[17] J. J. Mor´e and G. Toraldo, “On the solution of large quadratic programming problems with

bound constraints,” SIAM Journal on Optimization, vol. 1, no. 1, pp. 93–113, 1991.

[18] D. di Seraﬁno, G. Toraldo, M. Viola, and J. Barlow, “A two-phase gradient method for
quadratic programming problems with a single linear constraint and bounds on the variables,”
SIAM Journal on Optimization, vol. 28, no. 4, pp. 2809–2838, 2018.

[19] N. Ito, A. Takeda, and K. C. Toh, “A uniﬁed formulation and fast accelerated proximal gradi-
ent method for classiﬁcation,” Journal of Machine Learning Research, vol. 18, no. 1, pp. 510–
558, 2017.

29

[20] P. H. Chen, R. E. Fan, and C. J. Lin, “A study on smo-type decomposition methods for support
vector machines,” IEEE Transactions on Neural Networks, vol. 17, no. 4, pp. 893–908, 2006.

[21] R. T. Rockafellar, “Augmented Lagrangians and applications of the proximal point algorithm
in convex programming,” Mathematics of Operations Research, vol. 1, no. 2, pp. 97–116,
1976.

[22] A. Rahimi and B. Recht, “Random features for large-scale kernel machines,” in Advances in

neural information processing systems, pp. 1177–1184, 2008.

[23] S. M. Robinson, “Some continuity properties of polyhedral multifunctions,” Mathematical

Programming at Oberwolfach, vol. 14, no. 5, pp. 206–214, 1981.

[24] J. Sun, On monotropic piecewise quadratic programming. PhD thesis, Department of Mathe-

matics, University of Washington, 1986.

[25] R. Helgason, J. Kennington, and H. Lall, “A polynomially bounded algorithm for a singly
constrained quadratic program,” Mathematical Programming, vol. 18, no. 1, pp. 338–343,
1980.

[26] P. Brucker, “An O(n) algorithm for quadratic knapsack problems,” Operations Research Let-

ters, vol. 3, no. 3, pp. 163 – 166, 1984.

[27] S. Cosares and D. S. Hochbaum, “Strongly polynomial algorithms for the quadratic trans-
portation problem with a ﬁxed number of sources,” Mathematics of Operations Research,
vol. 19, no. 1, pp. 94–111, 1994.

[28] K. C. Kiwiel, “Variable ﬁxing algorithms for the continuous quadratic knapsack problem,”
Journal of Optimization Theory and Applications, vol. 136, no. 3, pp. 445–458, 2008.

[29] J. Han and D. Sun, “Newton and Quasi-Newton methods for normal maps with polyhedral

sets,” Journal of Optimization Theory and Applications, vol. 94, no. 3, pp. 659–676, 1997.

[30] R. T. Rockafellar and R. J. B. Wets, Variational Analysis. Springer, 1998.

[31] A. L. Dontchev and R. T. Rockafellar, Implicit Functions and Solution Mappings. World

Book Inc, 2013.

[32] Y. J. Zhang, N. Zhang, D. F. Sun, and K. C. Toh, “An efﬁcient Hessian based algorithm for
solving large-scale sparse group lasso problems,” Mathematical Programming, vol. 3, pp. 1–
41, 2018.

[33] R. T. Rockafellar, “Monotone operators and the proximal point algorithm,” SIAM Journal on

Control and Optimization, vol. 14, no. 5, pp. 877–898, 1976.

[34] J. B. Hiriart-Urruty, J. J. Strodiot, and V. H. Nguyen, “Generalized Hessian matrix and second-
order optimality conditions for problems with C 1,1 data,” Applied Mathematics and Optimiza-
tion, vol. 11, no. 1, pp. 43–56, 1984.

30

[35] Y. Nesterov, Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Aca-

demic Publishers, 2004.

[36] F. J. Luque, “Asymptotic convergence analysis of the proximal point algorithm,” SIAM Jour-

nal on Control and Optimization, vol. 22, no. 2, pp. 277–293, 1984.

[37] J. M. Danskin, “The theory of max-min, with applications,” SIAM Journal on Applied Math-

ematics, vol. 14, no. 4, pp. 641–664, 1966.

[38] X. D. Li, D. F. Sun, and K. C. Toh, “On the efﬁcient computation of a generalized Jacobian

of the projector over the Birkhoff polytope,” arXiv preprint arXiv:1702.05934, 2017.

[39] B. Kummer, “Newton’s method for non-differentiable functions,” Advances in Mathematical

Optimization, vol. 45, no. 1988, pp. 114–125, 1988.

[40] L. Qi and J. Sun, “A nonsmooth version of Newton’s method,” Mathematical programming,

vol. 58, no. 1-3, pp. 353–367, 1993.

[41] F. Facchinei and J. S. Pang, Finite-Dimensional Variational Inequalities and Complementarity

Problems. Springer, 2003.

[42] G. H. Golub and C. F. Van Loan, Matrix Computations (3rd Ed.). Johns Hopkins University

Press, 1996.

[43] S. Abe, Support Vector Machines for Pattern Classiﬁcation, Second Edition. Springer, 2005.

[44] S. S. Keerthi, O. Chapelle, and D. Decoste, “Building support vector machines with reduced
classiﬁer complexity,” Journal of Machine Learning Research, vol. 7, no. 2006, pp. 1493–
1515, 2006.

[45] C. K. Williams and M. Seeger, “Using the Nystr¨om method to speed up kernel machines,” in

Advances in neural information processing systems, pp. 682–688, 2001.

[46] W. Rudin, Fourier Analysis on Groups. Wiley Online Library, 1962.

[47] R. M. Neal, “Regression and classiﬁcation using Gaussian process priors (with discussion),”

In Bernardo, J.M., et al. (eds.) Bayesian statistics, vol. 6, pp. 475–501, 1998.

[48] K. Zhang, I. W. Tsang, and J. T. Kwok, “Improved Nystr¨om low-rank approximation and
error analysis,” in Proceedings of the 25th international conference on Machine learning,
pp. 1232–1239, 2008.

[49] J. Nalepa and M. Kawulok, “Selecting training sets for support vector machines: a review,”

Artiﬁcial Intelligence Review, vol. 52, no. 2, pp. 857–900, 2019.

[50] V. Cherkassky and Y. Ma, “Practical selection of svm parameters and noise estimation for

svm regression,” Neural networks, vol. 17, no. 1, pp. 113–126, 2004.

31

