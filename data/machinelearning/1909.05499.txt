1
2
0
2

r
p
A
9
1

]
S
D
.
s
c
[

5
v
9
9
4
5
0
.
9
0
9
1
:
v
i
X
r
a

Online Linear Programming: Dual Convergence, New

Algorithms, and Regret Bounds

Xiaocheng Li†

Yinyu Ye†

†Department of Management Science and Engineering, Stanford University

{chengli1, yyye}@stanford.edu

Abstract

We study an online linear programming (OLP) problem under a random input model in which the

columns of the constraint matrix along with the corresponding coeﬃcients in the objective function

are generated i.i.d.

from an unknown distribution and revealed sequentially over time. Virtually

existing online algorithms were based on learning the dual optimal solutions/prices of the linear

programs (LP), and their analyses were focused on the aggregate objective value and solving the

packing LP where all coeﬃcients in the constraint matrix and objective are nonnegative. However,

two major open questions were: (i) Does the set of LP optimal dual prices learned in the existing

algorithms converge to those of the “oﬄine” LP, and (ii) Could the results be extended to general LP

problems where the coeﬃcients can be either positive or negative. We resolve these two questions by

establishing convergence results for the dual prices under moderate regularity conditions for general

LP problems. Speciﬁcally, we identify an equivalent form of the dual problem which relates the

dual LP with a sample average approximation to a stochastic program. Furthermore, we propose

a new type of OLP algorithm, Action-History-Dependent Learning Algorithm, which improves the

previous algorithm performances by taking into account the past input data as well as the past

decisions/actions. We derive an O(log n log log n) regret bound (under a locally strong convexity and

smoothness condition) for the proposed algorithm, against the O(

n) bound for typical dual-price

√

learning algorithms, where n is the number of decision variables. Numerical experiments demonstrate

the eﬀectiveness of the proposed algorithm and the action-history-dependent design.

1

Introduction

Sequential decision making has been an increasingly attractive research topic with the advancement of

information technology and the emergence of new online marketplaces. As a key concept appearing

widely in the ﬁelds of operations research, management science, and artiﬁcial intelligence, sequential

decision making concerns the problem of ﬁnding the optimal decision/policy in a dynamic environment

where the knowledge of the system, in the form of data and samples, amasses and evolves over time. In

this paper, we study the problem of solving linear programs in a sequential setting, usually referred to as

online linear programming (OLP) (See e.g., (Agrawal et al., 2014)). The formulation of OLP has been

widely applied in the context of online Adwords/advertising (Mehta et al., 2005), online auction market

(Buchbinder et al., 2007), resource allocation (Asadpour et al., 2019), packing and routing (Buchbinder

and Naor, 2009), and revenue management (Talluri and Van Ryzin, 2006). One common feature in these

application contexts is that the customers, orders, or queries arrive in a forward sequential manner, and

the decisions need to be made on the ﬂy with no future data/information available at the decision/action

point.

1

 
 
 
 
 
 
The OLP problem takes a standard linear program as its underlying form (with n decision variables
and m constraints), while the constraint matrix is revealed column by column with the corresponding
coeﬃcient in the linear objective function. In this paper, we consider the standard random input model

(See (Goel and Mehta, 2008; Devanur et al., 2019)) where the orders, represented by the columns of the

constraint matrix together with the corresponding objective coeﬃcients, are sampled independently and
identically from an unknown distribution P. At each timestamp, the value of the decision variable
needs to be determined based on the past observations and cannot be changed afterward. The goal is to

minimize the gap (formally deﬁned as regret) between the objective value solved in this online fashion

and the “oﬄine” optimal objective value where one has the full knowledge of the linear program data.

There were many algorithms and research results on OLP in the past decade due to its wide ap-

plications. Virtually all existing online algorithms were based on learning the LP dual optimal solu-

tions/prices, and their analyses of OLP were focused on the aggregate objective value and solving the

packing LP where all coeﬃcients in the constraint matrix and objective are nonnegative. Two major open
questions in the literature were: (1) Does the set of LP optimal dual prices of OLP converge to those

of the oﬄine LP, and (2) Could the results be extended to general LP problems where the coeﬃcients

can be either positive or negative. We resolve these two questions in this paper as part of our results.

Moreover, we propose a new type of OLP algorithm and develop tools to analyze the regret of OLP

algorithms. Our key results and main contributions are summarized as follows.

1.1 Key Results and Main Contributions

Dual convergence of online linear programs. We establish convergence results for the dual optimal
solutions of a sequence of linear programs in Section 3. We ﬁrst derive an equivalent form of the

dual LP and discover that the sampled dual LP, under the random input model, can be viewed as a

Sample Average Approximation (SAA) (Kleywegt et al., 2002; Shapiro et al., 2009) of a constrained

stochastic programming problem. The stochastic program is deﬁned by the LP constraint capacity
and the distribution P that generates the input of the LP. Our key result states that, under moderate
regularity conditions, the optimal solution of the sampled dual LP will converge to the optimal solution

of the stochastic program as the number of (primal LP) decision variables goes to inﬁnity. Speciﬁcally, we
establish that the L2 distance between the two solutions is ˜O
under the random input model where
m is the number of constraints and n is the number of decision variables. Moreover, the convergence
results are not only pertaining to online packing LPs, but also hold for general LPs where the input data

m√
n

(cid:16) √

(cid:17)

coeﬃcients can be either positive or negative.

Action-history-dependent learning algorithm. We develop a new type of OLP algorithm –
Action-history-dependent Learning Algorithm in Section 4.5. This new algorithm is a dual-based algo-

rithm (as the algorithms in (Devanur et al., 2011; Agrawal et al., 2014; Gupta and Molinaro, 2014), etc.)

and it utilizes our results on the convergence of the sampled dual optimal solution. One common pattern
in the design of most existing OLP algorithms is that the choice of the decision variable at time t only de-
pends on the past input data, i.e., the coeﬃcients in the constraints and the objective function revealed,
but not the decisions already made (until time t − 1). Our new action-history-dependent algorithm con-
siders both the past input data and the past choice of decision variables. Similar idea was considered in a

few speciﬁc problems such as network revenue management and online auction. Compared to the exist-

ing OLP algorithms, our new algorithm is more conscious of the constraints/resources consumed by the

past decisions, and thus the decisions can be made in a more dynamic, closed-loop, and non-stationary

way. We demonstrate in both theory and numerical experiments that this actions-history-dependent

mechanism signiﬁcantly improves the online performance than existing OLP algorithms without this

mechanism.

2

Regret bounds for OLP. We analyze the worst-case gap (regret) between the expected online
objective value and the “oﬄine” optimal objective value. Speciﬁcally, we study the regret in an asymptotic
regime where the number of constraints m is ﬁxed as a constant and the LP right-hand-side input scales
linearly with the number of decision variables n. As far as we know, this is the ﬁrst regret analysis
result in the general OLP formulation. We derive an O(log n log log n) regret upper bound (under a
locally strong convexity and smoothness assumption) for the proposed action-history-dependent learning
algorithm, which has a similar order of magnitude as the best achievable lower bound Ω(log n) of the
problem even with the exact knowledge of the underlying distribution (Bray, 2019). Our regret analysis

provides an algorithmic insight for the constrained online optimization problem: a successful algorithm

should have good control of the binding constraints (resources) consumption – not exhausting those

constraints too early or having too much remaining at the end, an aspect usually overlooked by typical

dual-price learning algorithms in OLP literature. The analysis extends the ﬁndings in the network

revenue management literature (for example, the adaptive resource control in (Jasin and Kumar, 2012;
Jasin, 2015)) to a more general non-parametric context. Moreover, the results and methodologies (such

as Theorem 2) are potentially applicable to other online learning and online decision making problems.

1.2 Literature Review

Online optimization/learning/decision problems have been long studied in the community of operations

research and theoretical computer science. We refer readers to the papers (Borodin and El-Yaniv, 2005;

Buchbinder et al., 2009; Hazan et al., 2016) for a general overview of the topic and the recent develop-

ments. The OLP problem has been studied mainly under two models: the random input model (Goel

and Mehta, 2008; Devanur et al., 2019) and the random permutation model (Molinaro and Ravi, 2013;

Agrawal et al., 2014; Gupta and Molinaro, 2014). In this paper, we consider the random input model (also

known as stochastic input model) where the columns of constraint matrix are generated i.i.d. from an
unknown distribution P. In comparison, the random permutation model assumes the columns are arriv-
ing in random order and the arrival order is uniformly distributed over all the permutations. Technically,

the i.i.d. assumption in the random input model is stronger than the random permutation assumption in

that the random input model can be viewed as a special case of the random permutation model ((Mehta,

2013)). Practically, the random input model can be motivated from the online advertising problem, net-

work revenue management problems in ﬂight and hotel bookings, or the online auction problem. In these

application contexts, each column in the constraint matrix together with the corresponding coeﬃcient in

the objective function represents an individual order/bid/query. In this sense, the random input model

can be interpreted as an independence assumption across diﬀerent customers.

Our paper diﬀers from the existing OLP literature in the right-hand-side assumption, i.e., the con-
straint capacity. A stream of OLP papers (Devanur and Hayes, 2009; Molinaro and Ravi, 2013; Agrawal

et al., 2014; Kesselheim et al., 2014; Gupta and Molinaro, 2014) studied the trade-oﬀ between the

algorithm competitiveness and the constraint capacity. They investigated the necessary and suﬃ-
cient condition on the right-hand-side of the LP and the number of constraints m for the existence
of a (1 − (cid:15))-competitive OLP algorithm. Speciﬁcally, Agrawal et al. (2014) established the necessary
part by constructing a worst-case example, stating that the right-hand-side should be no smaller than
. Kesselheim et al. (2014); Gupta and Molinaro (2014) developed algorithms that achieve (1−(cid:15))-
Ω
competitiveness under this necessary condition and thus completed the suﬃcient part. In this paper, we

(cid:16) log m
(cid:15)2

(cid:17)

research an alternative question, when the right-hand-side grows linearly with the number of decision
variables n, whether the algorithm could achieve a better performance than (1 − (cid:15))-competitiveness. In
general, this linear growth regime will render the optimal objective value growing linearly with n as well.
Consequently, a (1 − (cid:15))-competitiveness performance guarantee will potentially incur a gap that is linear

3

in n between the online objective value and the oﬄine optimal value. Thus we consider regret instead of
competitiveness ratio as the performance measure and will analyze three algorithms that achieve sublin-

ear regret under the linear growth regime. Moreover, a typical assumption in the OLP literature requires

the data entries in the constraint matrix and the objective to be non-negative. We do not make this

assumption in our model so that our model and analysis can capture a double-sided market with both

buying and selling orders.

Another stream of research originates from the revenue management literature and studies a param-

eterized type of the OLP problem. It models the network revenue management problem where hetero-

geneous customers arrive sequentially to the system and the customers can be divided into ﬁnitely many

classes based on their requests and prices. In the language of OLP, the columns of the constraint matrix

with the corresponding coeﬃcients in the objective follow a well-parameterized distribution and have a

ﬁnite and known support. Reiman and Wang (2008); Jasin and Kumar (2012, 2013); Bumpensanti and

Wang (2018) among others studied the problem under a setting where the model parameters are known
and discussed the performance of a re-solving technique that dynamically solves the certainty-equivalent

problem according to the current state of the system. This line of work highlights the eﬀectiveness of the

re-solving technique in an environment with known parameters and investigates the desired re-solving

frequency. In a similar spirit, Jasin (2015) analyzed the performance of the re-solving based algorithm in

an unknown parameter setting, and Ferreira et al. (2018) studied a slightly diﬀerent pricing-based revenue

management problem. The OLP model generalizes the network revenue management problem in that it

does not impose any parametric structure on the distribution. For the OLP problem, the distribution of

the customer request and price may have an inﬁnite support (such as secretary problem/Adwords prob-

lem) and negative values are allowed (such as a two-sided auction market). Comparatively, the revenue

management literature focuses on the case where the underlying distribution has a parametric structure

and ﬁnite support. For example, Jasin (2015) studied an unknown setting, but the paper still assumed

the knowledge of the distribution’s support. Consequently, the parameter learning in (Jasin, 2015) was

reduced to a simple intensity estimation problem for a homogeneous Poisson process and the algorithm

therein relied on the estimation together with the re-solving technique. In fact, the algorithms developed

along this line of literature fail for the more general OLP problem because when the distribution is fully

non-parametric and unknown, there is no way to ﬁrst estimate the distribution parameter and then to

solve a certainty-equivalent optimization problem based on the estimated parameter. The dual-based al-

gorithms developed in our paper can thus be viewed as a combination of this ﬁrst-estimate-then-optimize

procedure into one single step. In particular, our action-history-dependent algorithm implements the idea

of re-solving technique in a non-parametric setting, and our algorithm analysis reinforces the eﬀectiveness

of the adaptive and re-solving design (mainly discussed in the revenue management literature) in a more

general online optimization context.

Another line of research investigated the multi-secretary problem ((Kleinberg, 2005; Arlotto and

Gurvich, 2019; Bray, 2019) among others). The multi-secretary problem is a special form of OLP problem

that has only one constraint and all the coeﬃcients in the constraint matrix are one. Arlotto and Gurvich

(2019) showed that when the reward distribution is unknown and if no additional assumption is imposed
n). A subsequent
on the distribution, the regret lower bound of the multi-secretary problem is Ω(
work (Bray, 2019) further noted that even when the distribution is known, the regret lower bound
is Ω(log n). Balseiro and Gur (2019) studied a similar one-constraint but multi-agent online learning
formulation motivated from the repeated auction problem. The results in our paper are positioned in a

√

more general context and consistent with this line of works. We identify a group of assumptions that
admits an ˜O(log n) regret upper bound for the more general OLP problem. Our action-history-dependent
algorithm can also be viewed as a generalization of the elegant adaptive algorithms developed in (Arlotto

and Gurvich, 2019; Balseiro and Gur, 2019). Together with (Bray, 2019), our results indicate that the

4

action-history-dependent algorithm achieves a near-optimal regret performance for the multi-secretary

problem even in a comparison with the optimal dynamic algorithm developed with knowledge of the

distribution.

The OLP problem is also related to the general online optimization problem. Compared to the

standard online optimization problem, our OLP problem is special in two aspects: (i) the presence

of the constraints and (ii) a dynamic oracle as the regret benchmark. For the literature on online

optimization with constraints (Mahdavi et al., 2012; Agrawal and Devanur, 2014b; Yu et al., 2017; Yuan

and Lamperski, 2018), the common approach is to employ a bi-objective performance measure and report

the regret and constraint violation separately. We contribute to this line of research by developing a

machinery to analyze the regret of a feasible online algorithm. For the second aspect, the dynamic oracle

allows the decision variables (in OLP) of diﬀerent time steps to take diﬀerent values. This is a stronger

oracle than the static oracle (Mahdavi et al., 2012; Agrawal and Devanur, 2014b; Yu et al., 2017; Yuan

and Lamperski, 2018) that requires the decision variables at diﬀerent time steps to take the same (static)
value. In other words, the OLP regret is computed against a stronger benchmark. This explains why

OLP literature considers mainly the random input model and the random permutation model, instead

of an adversarial setting.

2 Problem Formulation

In this section, we formulate the OLP problem and deﬁne the objective. Consider a generic LP problem

max

s.t.

n
(cid:88)

j=1
n
(cid:88)

j=1

rjxj

(1)

aijxj ≤ bi,

i = 1, ..., m

0 ≤ xj ≤ 1,

j = 1, ..., n

where rj ∈ R, aj = (a1j, ..., amj)(cid:62) ∈ Rm, and b = (b1, ..., bm)(cid:62) ∈ Rm. Without loss of generality, we
assume bi > 0 for i = 1, ..., m. Throughout this paper, we use bold symbols to denote vectors/matrices
and normal symbols for scalars.

In the online setting, the parameters of the linear program are revealed in an online fashion and one
needs to determine the value of decision variables sequentially. Speciﬁcally, at each time t, the coeﬃcients
(rt, at) are revealed, and we need to decide the value of xt instantly. Diﬀerent from the oﬄine setting,
at time t, we do not have the information of the following coeﬃcients to be revealed. Given the history
, the decision of xt can be expressed as a policy function of the history and the
Ht−1 = {rj, aj, xj}t−1
j=1
coeﬃcients observed in the current time period. That is,

xt = πt(rt, at, Ht−1).

(2)

The policy function πt can be time-dependent and we denote policy π = (π1, ..., πn). The decision variable
xt must conform to the constraints

t
(cid:88)

j=1

aijxj ≤ bi,

i = 1, ..., m,

0 ≤ xt ≤ 1.

The objective is to maximize the objective (cid:80)n

j=1 rjxj.

5

We illustrate the problem setting through the following practical example. Consider a market making
company receiving both buying and selling orders, and the orders arrive sequentially. At each time t,
we observe a new order, and we need to decide whether to accept or reject the order. The order is a

buying/selling request for the resources, or it could be a mixed request, e.g., selling the ﬁrst resource
for 1 unit and buying the second resource for 2 units with a total order price of $1. Once our decision
is made, the order will leave the system, and it is either fulﬁlled or rejected. In this example, the term
bi can be interpreted as the total available inventory for the resource i, and the decision variables xt’s
can be interpreted as the acceptance and rejection of an order. In particular, we do not allow shorting

of resources along the process. Our goal is to maximize the total revenue.

We assume the LP parameters (rj, aj) are generated i.i.d.

denote the oﬄine optimal solution of linear program (1) as x∗ = (x∗
objective value as R∗
n

(Rn). Speciﬁcally,

from an unknown distribution P. We
n)(cid:62), and the oﬄine (online)

1, ..., x∗

R∗

n :=

Rn(π) :=

n
(cid:88)

j=1
n
(cid:88)

j=1

rjx∗
j

rjxj.

in which online objective value depends on the policy π. The quantity R∗
n
of the realization (of the randomness), and it is also known as hindsight oracle in the literature of online
learning and robust optimization. In this paper, we consider a ﬁxed m and large n regime, and focus on
the worst-case gap between the online and oﬄine objective. We deﬁne the regret

assumes the full knowledge

∆P

n (π) := EP [R∗

n − Rn(π)]

and the worst-case regret

∆n(π) := sup
P∈Ξ

∆P

n (π) = sup
P∈Ξ

EP [R∗

n − Rn(π)]

where Ξ denotes a family of distributions satisfying some regularity conditions (to be speciﬁed later).
Throughout this paper, we omit the subscript P in the expectation notation when there is no ambiguity.
The worst-case regret takes the supremum regret over a family of distributions so that it is suitable
as a performance guarantee when the distribution P is unknown. We remark that the oﬄine optimal
solution R∗
can be interpreted as a dynamic oracle that allows the optimal decision variables to take
n
diﬀerent values at diﬀerent time steps. This is an important distinction between the OLP problem and

the problem of online convex optimization with constraints (Mahdavi et al., 2012; Agrawal and Devanur,

2014b; Yu et al., 2017; Yuan and Lamperski, 2018).

2.1 Notations

Throughout this paper, we use the standard big-O notations where O(·) and Ω(·) represent upper and
lower bound, respectively. The notation ˜O(·) further omits the logarithmic factor, such as ˜O(
n) =
n log n) and ˜O(log n) = O(log n log log n). The following list summarizes the notations used in this
O(
paper:

√

√

• m: number of constraints; n: number of decision variables

• i: index for constraint; j, t: index for the decision variables

• rj: the j-th coeﬃcient in the objective function

6

• aj: the j-th column in the constraint matrix

• ¯r, ¯a: upper bound on |rj|’s and (cid:107)aj(cid:107)2’s

• P: distribution of (rj, aj)’s; P ∈ Ξ: a family of distributions to be deﬁned in the next section

• λ, µ: Parameters that convey the meaning of strong convexity and smoothness; they are related to

the condition distribution of r|a to be deﬁned in the next section

• x∗ = (x∗

1, ..., x∗

n)(cid:62): the oﬄine primal optimal solution

• x = (x1, ..., xn)(cid:62): the online solution

• p∗
n

: the (random) oﬄine dual optimal solution

• p∗: the (deterministic) optimal solution of the stochastic program (7)

• R∗
n

: the oﬄine optimal objective value

• Rn(π) (sometimes Rn when the policy is clear): the online return/revenue under policy π

• b = (b1, ..., bm)(cid:62): the right-hand-side, constraint capacity

• d: the average constraint capacity, i.e., d = b/n

• d, ¯d: lower and upper bounds for d

• Ωd: the region for d, deﬁned by (cid:78)m

i=1(d, ¯d)

• IB and IN : the index sets for binding and non-binding constraints deﬁned by the stochastic program

(7), respectively

• bt: the remaining constraint capacity at the end of the t-th period, t = 1, ..., n

• dt: the remaining average constraint capacity at the end of the t-th period, i.e., dt = bt/(n − t),

t = 1, ..., n − 1

• ∧: minimum operator, y ∧ z := min{y, z} for y, z ∈ R

• ∨: maximum operator, y ∨ z := max{y, z} for y, z ∈ R

• I(·): indicator function; I(E) = 1 when E is true and I(E) = 0 otherwise

3 Dual Convergence

Many OLP algorithms rely on solving the dual problem of the linear program (1). However, there is still

a lack of theoretical understanding of the properties of the dual optimal solutions. In this section, we

establish convergence results on the OLP dual solutions and lay foundations for the analyses of the OLP

algorithms.

To begin with, the dual of the linear program (1) is

m
(cid:88)

i=1
m
(cid:88)

min

s.t.

bipi +

n
(cid:88)

j=1

yj

aijpi + yj ≥ rj,

j = 1, ..., n.

i=1
pi, yj ≥ 0 for all i, j.

7

(3)

Here the decision variables are p = (p1, ..., pm)(cid:62) and y = (y1, ..., yn)(cid:62).

Let (p∗

n, y∗

n) be an optimal solution for the dual LP (3). From the complementary slackness condition,

we know the primal optimal solution satisﬁes

x∗
j =




1,



0,

rj > a(cid:62)
rj < a(cid:62)

j p∗
n
j p∗
n.

(4)

j p∗
n

When rj = a(cid:62)
optimal solution largely depends on the dual optimal solution p∗
n
optimal dual solutions. An equivalent form of the dual LP can be obtained by plugging the constraints

may take non-integer values. This tells us that the primal

and thus motivates our study of the

, the optimal solution x∗
j

into the objective function (3):

m
(cid:88)

min

bipi +

(cid:32)

rj −

n
(cid:88)

j=1

m
(cid:88)

i=1

(cid:33)+

aijpi

(5)

i=1
s.t. pi ≥ 0,

i = 1, ..., m.

where (·)+ is the positive part function, also known as the ReLu function. The optimization problem
(5), despite not being a linear program, has a convex objective function. It has the advantage of only
involving p which is closely related to the optimal primal solution. More importantly, the random
summands in the second part of the objective function (5) are independent of each other, and therefore

the sum (after a normalization) will converge to a certain deterministic function. To better make this
point, let di = bi/n and divide the objective function in (5) by n. Then the optimization problem can
be rewritten as

min fn(p) :=

m
(cid:88)

i=1

dipi +

1
n

n
(cid:88)

j=1

(cid:32)

rj −

m
(cid:88)

i=1

(cid:33)+

aijpi

s.t. pi ≥ 0,

i = 1, ..., m.

The second term in the objective function (6) is a summation of n i.i.d. random functions.

Consider the following stochastic program

min f (p) := d(cid:62)p + E (cid:2)(r − a(cid:62)p)+(cid:3)
s.t. p ≥ 0,

(6)

(7)

where the expectation is taken with respect to (r, a). In the rest of the paper, unless otherwise stated,
the expectation is always taken with respect to (r, a). Evidently,

Efn(p) = f (p)

for all p. This observation casts the dual convergence problem in the form of a stochastic programming
problem. The function fn(p) in (6) can be viewed as a sample average approximation (SAA) (See
(Kleywegt et al., 2002; Shapiro et al., 2009)) of the function f (p). Speciﬁcally, the dual program associated
with a primal linear program with n decision variables is then an n-sample approximation of the stochastic
program (7). We denote the optimal solutions to the n-sample approximation problem (6) and the
and p∗, respectively. In this section, we provide a ﬁnite-sample analysis
stochastic program (7) with p∗
n
for the convergence of p∗
to p∗. We ﬁrst introduce the assumptions and then formally establish the
n
convergence.

8

3.1 Assumptions and Basics

The ﬁrst group of assumptions concerns the boundedness and the linear growth of the constraints.

Assumption 1 (Boundedness and Linear Growth Capacity). We assume

(a) {(rj, aj)}n

j=1

are generated i.i.d. from distribution P.

(b) There exist constants ¯r, ¯a > 0 such that |rj| ≤ ¯r and (cid:107)aj(cid:107)2 ≤ ¯a almost surely.

(c) di = bi/n ∈ (d, ¯d) for d, ¯d > 0, i = 1, ..., m. Denote Ωd = (cid:78)m

i=1(d, ¯d).

(d) n > m.

Throughout this paper, (cid:107) · (cid:107)2 denotes the L2-norm of a vector.

Assumption 1 (a) states that parameters (coeﬃcients in the objective function and columns in the
constraint matrix) of linear program (1) are generated i.i.d.
from an unknown distribution P. The
vectors {(rj, aj), j = 1, ..., n} are independent of each other, but their components may be dependent.
Assumption 1 (b) requires the parameters are bounded. The bound parameters ¯a and ¯r are introduced
only for analysis purposes and will not be used for algorithm implementation. Assumption 1 (c) requires
the right-hand-side of the LP constraints grows linearly with n. This guarantees that for the (optimal)
solutions, a constant proportion of the xj’s could be 1. It means the number of orders/requests that can
be fulﬁlled is on the order of n and thus ensures a constant service level (percentage of orders satisﬁed).
On the contrary, if this is not true and all the requests are buying orders (aij > 0), the service level
may go to zero when the business running period n goes to inﬁnity. The parameter di = bi/n has the
interpretation of available constraint/resource per period. Also, we require that the number of decision
variables n is larger than the number of constraints m. While discussing the dual convergence, the
dimension of the dual variable p is equal to the number of constraints m and the number of primal
decision variables n can be viewed as the number of samples used to approximate p∗. The assumption
of n > m restricts our attention to a low-dimensional setting.

Proposition 1 summarizes several basic properties related to the dual LP (3) and the stochastic

program (7). It states that both the objective functions in the SAA problem and the stochastic program

are convex. In addition, the optimal solutions to these two problems are bounded.

Proposition 1. Under Assumption 1, we have the following results on fn and f (with probability 1).

(a) The optimal solution set of problem (5) is identical to the optimal solution set of problem (3).

(b) Both fn(p) and f (p) are convex.

(c) The optimal solutions p∗
n

and p∗ satisfy

d(cid:62)p∗

n ≤ ¯r,

d(cid:62)p∗ ≤ ¯r.

Given the boundedness of the optimal solutions, we deﬁne

(cid:26)

Ωp :=

p ∈ Rm : p ≥ 0, e(cid:62)p ≤

(cid:27)

¯r
d

where e ∈ Rm is an all-one vector. We know that Ωp covers all possible optimal solutions to (6) and (7).
Next, we introduce the second group of assumptions on the distribution P. Here and hereafter, I(·)

denotes the indicator function.

9

Assumption 2 (Non-degeneracy). We assume

(a) The second-order moment matrix M := E(r,a)∼P [aa(cid:62)] is positive-deﬁnite. Denote its minimum

eigenvalue with λmin.

(b) There exist constants λ and µ such that if (r, a) ∼ P,

λ|a(cid:62)p − a(cid:62)p∗| ≤ (cid:12)

(cid:12)P(r > a(cid:62)p|a) − P(r > a(cid:62)p∗|a)(cid:12)

(cid:12) ≤ µ|a(cid:62)p − a(cid:62)p∗|

holds for any p ∈ Ωp.

(c) The optimal solution p∗ to the stochastic optimization problem (7) satisﬁes p∗

i = 0 if and only if

di − E(r,a)∼P [aiI(r > a(cid:62)p∗)] > 0.

Assumption 2 (a) is mild in that the matrix E[aa(cid:62)] is positive semi-deﬁnite by deﬁnition; the positive
deﬁniteness holds as long as the constraint matrix A of the LP always has a full row rank, which is a
typical assumption for solving linear programs. Assumption 2 (b) states that the cumulative distribution
function of r|a should not grow too fast or too slowly. Assumption 2 (c) imposes a strict complementarity
for the stochastic program. Essentially, Assumption 2 altogether imposes a non-degeneracy condition

for both the primal and dual LPs. It can be viewed as a generalization of the non-degeneracy condition

in (Jasin and Kumar, 2012; Jasin, 2015) and as a stochastic version of the general position condition in
(Devanur and Hayes, 2009; Agrawal et al., 2014).

We remark that all three parts of Assumption 2 are crucial for the analyses in the rest of the paper.

While Assumption 2 (a) and (c) are not necessarily true for all the stochastic programs, a slight per-
turbation of the distribution P (for example, through adding small random noises to rj and aij) would
result in them being satisﬁed. In addition, we provide three examples that satisfy Assumption 2 (b). In
Example 1, we can simply choose λ = α and µ = ¯α and then Assumption 2 (b) is satisﬁed. The analyses
of Example 2 and Example 3 are postponed to Section A3.

Example 1. Consider a multi-secretary problem where m = 1 and all the constraint coeﬃcients a1j = 1
for j = 1, ..., n. The reward rj is a continuous random variable and its distribution Pr has a density
function fr(x) s.t. α ≤ fr(x) ≤ ¯α for x ∈ [0, 1]. Here ¯α ≥ α ≥ 0.

Example 2. Consider (r, a) ∼ P such that r = a(cid:62)p∗ + (cid:15) where (cid:15) is a continuous random variable
independent with a and with bounded support.
In addition, the distribution P(cid:15) of (cid:15)’s has a density
function f(cid:15)(x) such that there exists ¯α, α, c(cid:15) > 0 such that f(cid:15)(x) ≥ α for x ∈ [−c(cid:15), c(cid:15)] and f(cid:15)(x) ≤ ¯α for
all x.

Example 3. Consider (r, a) ∼ P such that the conditional distribution r|a has a density function fr|a(x)
and the density function satisﬁes α ≤ fr|a(x) ≤ ¯α for x ∈ [−¯r, ¯r] with α, ¯α > 0, and fr|a(x) = 0 for
x /∈ [−¯r, ¯r]. In addition, there exists δr > 0 such that a(cid:62)p∗ ∈ [−¯r + δr, ¯r − δr] almost surely.

According to Assumption 2 (c), we deﬁne two index sets

IB := (cid:8)i : di − E(r,a)∼P [aiI(r > a(cid:62)p∗)] = 0(cid:9) ,

IN := (cid:8)i : di − E(r,a)∼P [aiI(r > a(cid:62)p∗)] > 0(cid:9) ,

where the subscripts “B” and “N” are short for binding and non-binding, respectively. Assumption 2 (c)
implies IB ∩ IN = ∅ and IB ∪ IN = {1, ..., m}. Throughout the paper, the binding and non-binding
constraints of the OLP problem are always deﬁned according to the stochastic program (7).

10

Now, we derive more basic properties for the stochastic program (7) based on Assumption 2. First,

deﬁne a function h : Rm × Rm+1 → R,

h(p, u) :=

m
(cid:88)

i=1

(cid:32)

dipi +

u0 −

(cid:33)+

m
(cid:88)

i=1

uipi

and function φ : Rm × Rm+1 → Rm,

φ(p, u) :=

∂h(p, u)
∂p

= (d1, ..., dm)(cid:62) − (u1, ..., um)(cid:62) · I

u0 >

(cid:32)

(cid:33)

uipi

m
(cid:88)

i=1

where u = (u0, u1, ..., um)(cid:62) and p = (p1, ..., pm)(cid:62). The function φ is the partial sub-gradient of the
function h with respect to p; in particular, φ(p, u) = d when u0 = (cid:80)m

i=1 uipi. We know that

f (p) = Eu∼P [h(p, u)].

and we deﬁne

∇f (p) := E [φ(p, u)]

(8)

where both expectations are taken with respect to u = (r, a) ∼ P. The caveat is that the function f
is not necessarily diﬀerentiable under our current assumptions. As we will see shortly in Lemma 1 and
Proposition 2, the deﬁnition of ∇f (p) constitutes a meaning of sub-gradient for function f .

Lemma 1 represents the diﬀerence between f (p) and f (p∗) with the sub-gradient function φ. Note
that the identity (9) holds regardless of the distribution P. Its derivation shares the same idea with
the Knight’s identity in (Knight, 1998). Intuitively, the lemma can be viewed as a second-order Taylor
expansion for the function f. The ﬁrst and second term on the right-hand side of the identity can be
interpreted as the ﬁrst- and second- order term in Taylor expansion. They are disguised in this special

form due to the non-diﬀerentiability of the positive part function at the origin.

Lemma 1. For any p ≥ 0, we have the following identity,

f (p) − f (p∗) = ∇f (p∗)(p − p∗)
(cid:125)
(cid:124)

(cid:123)(cid:122)
First-order

(cid:34)(cid:90) a(cid:62)p∗

a(cid:62)p

+ E

(cid:124)

(cid:0)I(r > v) − I(r > a(cid:62)p∗)(cid:1) dv

(cid:123)(cid:122)
Second-order

(cid:35)

.

(cid:125)

(9)

where the expectation is taken with respect to (r, a) ∼ P.

Proposition 2 applies Assumption 2 to the identity (9) and it leads to a local property around p∗ for
the function f (p). Our motivation for the proposition is to provide more intuitions for Assumption 2
from a technical perspective. Moreover, the proposition asserts the uniqueness of p∗ which makes our
notion of convergence to p∗ well-deﬁned.

Proposition 2 (Growth and Smoothness of f (p)). Under Assumption 1 and 2, for p ∈ Ωp,

λλmin
2

(cid:107)p − p∗(cid:107)2

2 ≤ f (p) − f (p∗) − ∇f (p∗)(p − p∗) ≤

µ¯a2
2

(cid:107)p − p∗(cid:107)2
2.

(10)

Moreover, the optimal solution p∗ to the stochastic program (7) is unique.

The proposition gives a technical interpretation for the distributional conditions in Assumption 2.
Essentially, the role of the assumption on the distribution P is to impose a locally strong convexity and
smoothness around p∗. This is weaker than the classic notion of strong convexity and smoothness for

11

convex functions which requires the inequality (10) to hold globally. Assumption 2 (b) and Proposition
2 both concern a local property for the optimal solution p∗. As we will see in the later chapter, this
local property on f (p) is crucial and suﬃcient to ensure a fast convergence rate of p∗
and a sharp regret
n
bound for OLP algorithms.

We use the notation Ξ to denote the family of distributions that satisfy Assumption 1 and 2. In
the rest of the paper except for Section 4.5, all the theoretical results on the dual convergence and the

analyses of OLP algorithms are established under Assumption 1 and 2. In Section 4.5, we will present a

stronger version of Assumption 2 and analyze the action-history-dependent algorithm accordingly.

3.2 Dual Convergence

Now, we discuss the convergence of p∗
n

to p∗. First, the SAA function fn(p) can be expressed by

fn(p) =

1
n

n
(cid:88)

j=1

h(p, uj)

where uj = (rj, aj) and the function h is as deﬁned earlier. Lemma 2 is a sample average version of
Lemma 1 and it represents the diﬀerence between fn(p) and fn(p∗) with the sub-gradient function φ.

Lemma 2. For any p ∈ Rm, we have the following identity,

fn(p) − fn(p∗) =

n
(cid:88)

j=1

1
n

(cid:124)

φ(p∗, uj)(cid:62)(p − p∗)

+

(cid:123)(cid:122)
First-order

(cid:125)

n
(cid:88)

(cid:90) a(cid:62)

j p∗

j=1

a(cid:62)

j p

1
n

(cid:124)

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv

.

(11)

(cid:123)(cid:122)
Second-order

(cid:125)

In the following, we will establish the convergence of p∗
n

based on an analysis of the identity (11). The

idea is to show that the right-hand-side of (11) concentrates around its expectation as on the right-hand-

side of (9). The following two propositions analyze the ﬁrst-order and second-order terms respectively.
Proposition 3 tells that the sample average sub-gradient 1
j=1 φ(p∗, uj) stays close to its expectation
n
∇f (p) (8) evaluated at p∗ with high probability. Proposition 4 states that the second-order term – the
integral on the right hand of (11), is uniformly lower bounded by a strongly convex quadratic function
with high probability. Intuitively, the analysis only involves the local property of the function fn around
p∗. This explains why we impose only local (but not global) conditions on the function f in Assumption
2. Speciﬁcally, Assumption 2 (a) and (b) concern the Hessian, while Assumption 2 (c) concerns the
gradient, with both being evaluated at p∗.

(cid:80)n

Proposition 3. We have

P





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

j=1

φ(p∗, uj) − ∇f (p∗)



(cid:18)

≤ (cid:15)

 ≥ 1 − 2m exp

−

(cid:19)

n(cid:15)2
2¯a2m

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

hold for any (cid:15) > 0, all n > m and P ∈ Ξ.

Proposition 3 is obtained by a direct application of a concentration inequality. Notably, the probability
bound on right-hand-side is not dependent on the distribution P and the inequality holds for any (cid:15) > 0.
From the optimality condition of the stochastic program, we know (∇f (p∗))i = 0 for i ∈ IB and
(∇f (p∗))i > 0 for i ∈ IN . Therefore, the proposition implies that the sample average sub-gradient
(ﬁrst-order term in (11)) concentrates around zero for binding dimensions and concentrates around a

positive value for non-binding dimensions. As noted earlier, the binding and non-binding dimensions are

deﬁned by the stochastic program (7).

12

Proposition 4. We have

(cid:32)

P

1
n

n
(cid:88)

(cid:90) a(cid:62)

j p∗

j=1

a(cid:62)

j p

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv ≥ −(cid:15)2 − 2(cid:15)¯a(cid:107)p∗ − p(cid:107)2 +

λλmin
32

(cid:107)p∗ − p(cid:107)2
2

(12)

for all p ∈ Ωp

(cid:33)

(cid:18)

≥ 1 − m exp

−

(cid:19)

min

nλ2
4¯a2

− 2 (2N )m · exp

(cid:19)

(cid:18)

−

n(cid:15)2
2

holds for any (cid:15) > 0, n > m and P ∈ Ξ. Here

(cid:36)

N =

logq

(cid:19)(cid:37)

(cid:18) d(cid:15)2
√
¯a¯r

m

+ 1,

q = max






1
1 + 1√
m

,






1
(cid:16) λλmin
8µ¯a2

(cid:17) 1

3

1 + 1√
m

where (cid:98)·(cid:99) is the ﬂoor function.

Proposition 4 discusses the second-order term in (11). The inequality (12) tells that the second-order

term is uniformly lower bounded by a quadratic function with high probability. To prove the inequality
for a ﬁxed p can be easily done by a concentration argument as in Proposition 3. The challenging part is
to show that the inequality (12) holds uniformly for all p ∈ Ωp. The idea here is to ﬁnd a collection of sets
that covers Ωp and then analyze each covering set separately. We utilize the same covering scheme as in
(Huber et al., 1967) which is originally developed to analyze the consistency of non-standard maximum

likelihood estimators. The advantage of this covering scheme is that it provides a tighter probability
bound than the traditional (cid:15)-covering scheme. The parameters involved in the proposition are dependent
on the parameters in Assumption 1 and 2: ¯a, ¯r and d are speciﬁed in Assumption 1; λmin, λ and µ are
speciﬁed in Assumption 2. For the newly introduced parameters, q can be viewed as a constant parameter
m log m. All of these parameters are not dependent on the speciﬁc distribution
and N is on the order of
P and thus the result creates convenience for our later regret analysis of OLP algorithms. Importantly,
both probability bounds in Proposition 3 and 4 have an exponential term of n, and we utilize this fact
to establish the convergence rate of p∗
n

as follows.

√

With Proposition 3 and 4, the identity (11) can be written heuristically as

fn(p) − fn(p∗) ≥ ∇f (p∗)(cid:62)(p − p∗) − (cid:15)(cid:107)p∗ − p(cid:107)2 − (cid:15)2 − 2(cid:15)¯a(cid:107)p∗ − p(cid:107)2 +

≥ −(cid:15)2 − (2¯a + 1)(cid:15)(cid:107)p∗ − p(cid:107)2 +

λλmin
32

(cid:107)p∗ − p(cid:107)2
2

λλmin
32
uniformly for all p ∈ Ωp

(cid:107)p∗ − p(cid:107)2
2

(13)

with high probability, that is, (13) is violated with a probability decreasing exponentially in (cid:15). Note that
the right side in above is a quadratic function of p. From the optimality of p∗
n

,

fn(p∗

n) ≤ fn(p∗).

(14)

By putting together (13) with (14) and then integrating with respect to (cid:15), we can obtain the following
theorem.

Theorem 1. Under Assumption 1 and 2, there exists a constant C such that

E (cid:2)(cid:107)p∗

n − p∗(cid:107)2
2

(cid:3) ≤

Cm log m log log n
n

holds for all n ≥ max{m, 3}, m ≥ 2, and distribution P ∈ Ξ. Additionally,

E [(cid:107)p∗

n − p∗(cid:107)2] ≤ C

(cid:114)

m log m log log n
n

.

13

For the case of m = 1, both results still hold without the log m term on the right-hand-side.

Theorem 1 states the convergence rate in terms of the L2 distance. The second inequality in the
theorem can be directly implied from the ﬁrst one, and here we present both inequalities for later usage.

Also, to simplify the notations in the rest of the paper, we adopt the same constant for both inequalities

without loss of generality. The theorem formally connects the dual LP and the stochastic program, by

characterizing the distance between the optimal solutions of the two problems. As mentioned earlier,

it resolves the open question of the convergence of dual optimal solutions in the OLP problem. The

conclusion and the proof of the theorem are all based on ﬁnite-sample argument, in parallel with the

classic stochastic programming literature where asymptotic results are derived (Shapiro, 1993; Shapiro

et al., 2009). The ﬁnite-sample property is crucial in the regret analysis of the OLP algorithms. In the

Theorem 2 of the next section, we will show how the regret of an OLP algorithm can be upper bounded by
the L2 approximation error of p∗. This explains why we focus on the convergence and the approximation
error of p∗
n). As for the convergence rate, recall that Assumption 2 (a)
n
and (b) impose a curvature condition around the optimal solution p∗. They are critical to the quadratic
form (12) in Proposition 4 and consequently the convergence rate in Theorem 1. An alternative proof

instead of the function value f (p∗

of the convergence results in Theorem 1 may also be obtained by using the notion of local Rademacher

Complexity. Bartlett et al. (2005) proposed this local and empirical version of Rademacher complexity

and employed the notion to derive the fast rate of estimator convergence. We remark that this possible

alternative treatment also requires certain growth and smoothness condition around the optimal solution
p∗ as Assumption 2 (a) and (b).

will converge to p∗, and that when n gets large, p∗
n

Now, we discuss some implications of Theorem 1 on the OLP problem. The theorem tells that the
stays closer to p∗. From an algorithmic
sequence of p∗
n
perspective, an OLP algorithm can form an approximate for p∗ based on the observed t inputs at each
should be very close to both p∗ and p∗
time t. If t is suﬃciently large, the approximate dual price p∗
n.
t
) to decide the value of the current
Then, the algorithm can use p∗
t
decision variable. This explains why OLP algorithms in literature always solve a scaled version of the
oﬄine LP (based on the observations available at time t). However, in the literature, due to the lack
of convergence knowledge of the dual optimal solutions, papers devised other approaches to analyze the

(as an approximate to the optimal p∗
n

online objective value. Our convergence result explicitly characterizes the rate of the convergence and

thus provides a powerful and natural instrument for the theoretical analysis of the online algorithms.

The dual convergence result also contributes to the literature of approximate algorithms for large-

scale LPs. Speciﬁcally, we can perform one-time learning with the ﬁrst t inputs and then use p∗
t
. In this way, we obtain an approximate algorithm for solving the original LP
an approximation for p∗
n
j=1. The approximate algorithm can be viewed
problem by only accessing the ﬁrst t columns {(rj, aj)}t
as a constraint sampling procedure (De Farias and Van Roy, 2004; Lakshminarayanan et al., 2017) for

as

the dual LP. It also complements the recent work (Vu et al., 2018) in which approximate algorithms for

large-scale LP are developed under certain statistical assumptions on the coeﬃcients of the LP problem.

4 Learning Algorithms for OLP

4.1 Dual-based Policy, Constraint Process, and Stopping Time

In this section, we present several online algorithms based on the dual convergence results. We ﬁrst

revisit the deﬁnition of online policies and narrow down our scope to a class of dual-based policies that
rely on the dual solutions. Speciﬁcally, at each time t, a vector pt is computed based on historical data

pt = ht(Ht−1)

14

where Ht−1 = {rj, aj, xj}t−1
j=1
to set

. Inspired by the optimality condition of the oﬄine/static LP, we attempt

˜xt =




1,



0,

if rt > a(cid:62)
if rt ≤ a(cid:62)

t pt,

t pt.

In other words, a threshold is set by the dual price vector pt. If the reward rt is larger than the threshold,
we intend to accept the order. Then, we check the constraints satisfaction and assign

xt =




˜xt,



0,

if (cid:80)t−1

j=1 aijxj + ait ˜xt ≤ bi,

for i = 1, ..., m,

otherwise.

We formally deﬁne policies with this structure as a dual-based policy. We emphasize that in this dual-
based policy class, pt is ﬁrst computed based on history (up to time t − 1), and then (rt, at) is observed.
This creates a natural conditional independence

(˜xt, rt, at) ⊥ Ht−1

(cid:12)
(cid:12)pt.

This matches the setting in online convex optimization where at each time t, the online player makes her
decision before we observe the function ft (See (Hazan et al., 2016)). We will frequently resort to this
conditional independence in the regret analysis. In this policy class, an online policy π could be fully
speciﬁed by the sequence of mappings ht’s, i.e., π = (h1, ..., hn). To facilitate our analysis, we introduce
the constraint process {bt}n

,

t=1

b0 = b = nd

bt = bt−1 − atxt.

In this way, bt = (b1t, ..., bmt)(cid:62) represents the vector of remaining resources at the end of the t-th period.
In particular, bn = (b1n, ..., bmn)(cid:62) represents the remaining resources at the end of the horizon. By the
deﬁnition of OLP, bt ≥ 0 for t = 1, ..., n. Also, the process of {bt}n
is pertaining to the policy π. Based
on the constraint process, we deﬁne

t=0

τs := min{n} ∪

t ≥ 1 : min

(cid:110)

(cid:111)

bit < s

i

for s > 0. In this way, τs denotes the ﬁrst time that there are less than s units for some type of constraints.
Precisely, τs is a stopping time adapted to the process {bt}n
. Similar to the process bt, the stopping
time τs is also pertaining to the policy π. When executing an online policy, we do not close the business
at the ﬁrst time that some constraints are violated. This is because we are considering double-sided

t=1

problems that include both buying and selling orders.

If a certain type of resource is exhausted, we

may accept selling orders containing that resource as a way of replenishment. We will see that a careful

design of the algorithm will ensure the constraint violation/resource depletion only happens at the very

end. So, the decisions afterward will not aﬀect the cumulative revenue signiﬁcantly.

In the rest of this section, we ﬁrst derive a generic upper bound for the regret of OLP algorithms and

then present three OLP algorithms. These three algorithms all belong to the dual-based policy class,

and their regret analyses all rely on the dual convergence and the generic upper bound. We restrict our
attention to large-n and small-m setting, and the regret bounds will be presented with big-O notation
which treats m and the parameters in Assumption 1 and 2 as constants.

15

4.2 Upper Bound for OLP Regret

We ﬁrst construct an upper bound for the oﬄine optimal objective value. Consider the optimization

problem

max
p≥0

E (cid:2)rI(r > a(cid:62)p)(cid:3)

s.t. E (cid:2)aI(r > a(cid:62)p)(cid:3) ≤ d

(15)

where the expectation is taken with respect to (r, a) ∼ P. There are two ways to interpret this optimiza-
tion problem. On one hand, we can interpret this problem as a “deterministic” relaxation of the primal

LP (1). We substitute both the objective and constraints of (1) with an expectation form expressed
in dual variable p. On the other hand, we can view this optimization problem as the primal problem
of the stochastic program (7). The consideration of a deterministic form for an online decision making

problem has appeared widely in the literature of network revenue management (Talluri and Van Ryzin,

1998; Jasin and Kumar, 2013; Bumpensanti and Wang, 2018), dynamic pricing (Besbes and Zeevi, 2009;

Wang et al., 2014; Lei et al., 2014; Chen and Gallego, 2018), and bandits problem (Wu et al., 2015).

The idea is that when analyzing the regret of an online algorithm in such problems, the oﬄine optimal

value usually does not have a tractable form (such as the primal LP problem (1)). The deterministic

formulation serves as a tractable upper bound for the oﬄine optimal value, and then the gap between

the deterministic optimal and the online objective values is an upper bound for the regret of the online

algorithm. Diﬀerent from the literature, we consider the Lagrangian of the deterministic formulation to
remove the constraints. Speciﬁcally, deﬁne

g(p) := E

(cid:104)
rI(r > a(cid:62)p) + (cid:0)d − aI(r > a(cid:62)p)(cid:1)(cid:62)

p∗(cid:105)

,

where the expectation is taken with respect to (r, a) ∼ P and p∗ is the optimal solution to the stochastic
program (7). We can view g(p) as the Lagrangian of the optimization problem (15) with a speciﬁcation of
the multiplier by p∗. Lemma 3 establishes that the deterministic formulation indeed provides an upper
bound for the oﬄine optimal value and that the optimization problems (7) and (15) share the same

optimal solution.

Lemma 3. Under Assumption 1 and 2, we have

ER∗

n ≤ ng(p∗)

g(p∗) ≥ g(p)

for any p ≥ 0. Here p∗ is the optimal solution to the stochastic program (7). Additionally,

g(p∗) − g(p) ≤ µ¯a2(cid:107)p∗ − p(cid:107)2
2

(16)

holds for all p ∈ Ωp and all the distribution P ∈ Ξ.

The presence of constraints makes the regret analysis challenging, because the way the constraints

aﬀect the objective value in an online setting is elusive and problem-dependent. The Lagrangian form
g(p) resolves the issue by incorporating the constraints into the objective. Intuitively, it assigns a cost to
the constraint consumption and thus uniﬁes the two seemingly conﬂicting sides – revenue maximization

and constraint satisfaction. The importance of Lemma 3 is two-fold. First, it provides a deterministic

upper bound for the expected oﬄine optimal objective value. The upper bound is not dependent on the

realization of a speciﬁc OLP instance, so it is more convenient to analyze than the original oﬄine optimal

16

objective value. Second, a dual-based online algorithm employs a dual price pt at time t and its instant
reward at time t can be approximated by g(pt). Then the single-step regret of the algorithm at time t
can be upper bounded with (16). Theorem 2 builds upon Lemma 3 and compares the online objective
value ERn(π) against ng(p∗). A generic upper bound is developed for dual-based online policies. The
upper bound consists of three components: (i) the cumulative “approximation” error, (ii) the remaining

periods after the constraint is almost exhausted, and (iii) the remaining resources at the end of time
period n. The ﬁrst component relates the regret with E (cid:2)(cid:107)pt − p∗(cid:107)2
justiﬁes why the dual convergence (Theorem 1) is studied in the last section. The second component
concerns τ¯a, where ¯a is deﬁned in Assumption 1 and it is the maximal possible constraint consumption
per period. The intuition for τ¯a is that an order (rt, at), if necessary, can always be fulﬁlled when t ≤ τ¯a.
Though selling order (with negative aij’s) may still be accepted after τ¯a, from the standpoint of deriving
regret upper bound, we simply ignore all the orders that come after τ¯a. The third component considers
the constraint leftovers for binding constraints. Intuitively, binding constraints are the bottleneck for

(cid:3) studied in the last section.

It

2

producing revenue, so wasting those resources at the end of the horizon will induce a cost.

Theorem 2. Under Assumption 1 and 2, there exists a constant K such that the worst-case regret under
policy π,

∆n(π) ≤ K · E

(cid:34) τ¯a(cid:88)

t=1

(cid:107)pt − p∗(cid:107)2

2 + (n − τ¯a) +

(cid:88)

i∈IB

bin

(cid:35)

holds for all n > 0. Here IB is the set of binding constraints speciﬁed by the stochastic program (7), pt
is speciﬁed by the policy π, and p∗ is the optimal solution of the stochastic program (7).

Theorem 2 provides important insights for the design of an online policy. First, a dual-based policy
should learn p∗. Meanwhile, the online policy should have stable control of the resource/constraint
consumption. Exhausting the constraints too early may result in a large value of the term n − τ¯a while
the remaining resources at the end of the horizon may also induce regret through the term (cid:80)
bin.
Essentially, both components stem from the ﬂuctuation of the constraint consumption. The ideal case,
although not possible due to the randomness, is that the i-th constraint is consumed by exactly di units
in each time period. The following corollary states that the result in Theorem 2 holds for a general class

i∈IB

of stopping times.

Corollary 1. For any given bt-adapted stopping time τ, if P(τ ≤ τ¯a) = 1,

∆n(π) ≤ K · E

(cid:34)τ −1
(cid:88)

t=1

(cid:107)pt − p∗(cid:107)2

2 + (n − τ ) +

(cid:35)

bin

(cid:88)

i∈IB

holds for all n > 0 with the same constant K as in Theorem 2. Here IB is the set of binding constraints,
pt is speciﬁed by the policy π, and p∗ is the optimal solution of the stochastic program (7).

Remark. Theorem 2 and Corollary 1 reduce the derivation of regret upper bound to the analysis
of approximation errors and the analysis of the constraint process. To highlight the usefulness of this

reduction, we brieﬂy discuss the existing techniques developed for analyzing online learning problems

with the presence of constraints. The simplest approach is to propose a bi-objective performance measure:
one for the objective value and one for the constraint violation. The bi-objective performance measure

is adopted in the problem of online convex optimization with constraints (Mahdavi et al., 2012; Agrawal

and Devanur, 2014b; Yu et al., 2017; Yuan and Lamperski, 2018). In these works, the regret for the

objective value and the constraint violation are reported separately. Our results provide a method to

convert the bi-objective results into one performance measure. When the upper bound on the constraint
coeﬃcients ¯a is known, one approach to combine the bi-objective performance measure is to penalize

17

the excessive usage of the constraints, such as the regret analysis in (Ferreira et al., 2018). However,

this approach only works for non-adaptive algorithms (like Algorithm 2) but fails when the constraint

process aﬀects the decision (like Algorithm 3 and other re-solving algorithms). In this light, Theorem 2

and Corollary 1 provide a uniﬁed treatment for non-adaptive and adaptive algorithms. Another approach

to deal with the constraints is to use the “shrinkage” trick. The idea is to perform the online learning as if
the constraint is shrunk by a factor of 1 − (cid:15), and then the output online solution will be feasible with high
probability for the original problem. This shrinkage trick is used in the OLP literature (Agrawal et al.,

2014; Kesselheim et al., 2014) and also in the bandits with knapsacks problem (Agrawal and Devanur,

2014a). The downside of the shrinkage is that it will probably result in an over-conservative decision

and may have too many remaining resources at the end of the horizon. The regret analysis closest to

our approach is the analysis in (Jasin and Kumar, 2012; Jasin, 2015; Balseiro and Gur, 2019). The

regret derivation therein also involves analyzing the stopping time related to the constraint depletion.

Our generic upper bound can be viewed as a generalization of their analysis for the case when no prior
knowledge on the support of (rj, aj)’s is available. For the network revenue management problem, the
support of customer orders (rj, aj)’s is assumed to be ﬁnite and known. In this case, the oﬄine optimal
solution can be explicitly speciﬁed by the optimal quantity of each type of customer orders that should

be accepted. Then the performance of an online algorithm can be analyzed by the deviation from the

optimal quantity. In a general OLP problem, there is no explicit representation of the oﬄine optimal
solution. Theorem 2 and Corollary 1 resolve the issue by identifying the approximation error of p∗
as a non-parametric generalization of the deviation from the optimal quantity in the network revenue

management context. A subsequent work (Lu et al., 2020) also applied similar ideas from Theorem 2

and Corollary 1 in their regret analysis for an online allocation problem which has a ﬁnite support of
(rj, aj)’s.

4.3 When the Distribution is Known

We ﬁrst present an algorithm for the situation when we know the distribution P that generates the LP
coeﬃcients. With the knowledge of P, the stochastic programming problem (7) is well-speciﬁed. We
study the algorithm mainly for benchmark purposes, so we do not discuss the practicability of knowing
the distribution P. Moreover, we assume that the stochastic programming problem (7) can be solved
exactly. In Algorithm 1, the optimal solution p∗ is computed before the online procedure and can be
viewed as prior knowledge. So, there is “no need to learn” the dual price throughout the procedure, and
the pre-computed p∗ can be used for thresholding rule.

Algorithm 1 No-Need-to-Learn Algorithm
1: Input: n, d1, ..., dm, Distribution P
2: Compute the optimal solution of the stochastic programming problem

p∗ = arg min d(cid:62)p + E(r,a)∼P

(cid:2)(r − a(cid:62)p)+(cid:3)

s.t. p ≥ 0.

3: for t = 1, ..., n do
4:

If constraints are not violated, choose

5: end for

xt =

(cid:40)

1,
0,

if rt > a(cid:62)
if rt ≤ a(cid:62)

t p∗
t p∗

18

Theorem 3. With the online policy π1 speciﬁed by Algorithm 1,

∆n(π1) ≤ O(

√

n).

√

Theorem 3 tells that the worst-case regret under an online policy with the knowledge of full distri-
bution is O(
n). Recall the generic regret upper bound in Theorem 2, there is no approximation error
for Algorithm 1 because p∗ is employed as the dual price. So the regret of Algorithm 1 stems only from
the ﬂuctuation of the constraint process. Intuitively, the ﬂuctuation is on the order of O(
n) when it
approaches the end of the horizon. Our next algorithm and its according regret analysis show that the
contribution of the approximation error of p∗ (when the distribution is unknown) is indeed dominated
by the regret caused by the ﬂuctuation of the constraint process.

√

4.4 Simpliﬁed Dynamic Learning Algorithm

Algorithm 2 approximates the dual price p∗ in Algorithm 1 by solving an SAA for the stochastic program
based on the past observations. From the OLP perspective, it can be viewed as a simpliﬁed version of
the dynamic learning algorithm in (Agrawal et al., 2014). In Algorithm 2, the dual price vector pt is
updated only at geometric time intervals and it is computed based on solving the t-sample approxi-
mation, i.e., minimizing ft(p). The key diﬀerence between this simpliﬁed algorithm and the dynamic
learning algorithm in (Agrawal et al., 2014) is that we get rid of the shrinkage term
in the

(cid:16)

(cid:17)

1 − (cid:15)

(cid:113) n
tk

constraints. Speciﬁcally, the algorithm in that paper considers

tkdi on the right-hand side
of the constraints in Step 6 of Algorithm 2. In the random input model, the shrinkage factor results in
an over-estimated dual price pt and hence will be more conservative in accepting orders. The conserva-
tiveness is probably helpful under the random permutation model studied in (Agrawal et al., 2014) but
may cause the remaining resources to be linear in n under the random input model.

1 − (cid:15)

(cid:16)

(cid:17)

(cid:113) n
tk

Theorem 4. With the online policy π2 speciﬁed by Algorithm 2,

∆n(π2) ≤ O(

√

n log n).

Theorem 4 tells that the policy incurs a worst-case regret of O(

√

n log n). Its proof relies on an analysis
can be

of the three components in the regret bound in Theorem 2. The summation (cid:80)τ¯a
t=1 (cid:107)pt − p∗(cid:107)2
2
analyzed by the dual convergence result. Speciﬁcally, the dual price ptk
in Algorithm 2 is computed
based on a tk-sample approximation to the stochastic program (7), and therefore Theorem 1 can be
. It reiterates the importance of studying the dual
employed to upper bound the distance (cid:107)ptk − p∗(cid:107)2
2
convergence and expressing the approximation error in L2 distance. The two components related to the
stopping time and remaining resources are studied based on a careful analysis of the process bt. The
detailed proof can be found in Section C2.

4.5 Action-History-Dependent Learning Algorithm

Now, we present our action-history-dependent learning algorithm. In Algorithm 2, the dual price pt is
a function of the past inputs {(rj, aj)}t−1
but it does not consider the past actions (x1, ..., xt−1). In
j=1
contrast, Algorithm 3 integrates the past actions into the constraints of the optimization problem of pt.
are observed. Algorithm 2 normalizes
At the beginning of period t + 1, the ﬁrst t inputs {(xj, rj, aj)}t
n bi = tdi for the right-hand-side of the LP, while Algorithm 3 normalizes the remaining resource
bi to t
bit for the right-hand-side of the LP (Step 6 of Algorithm 3). The intuition is that if we happen to

j=1

19

Algorithm 2 Simpliﬁed Dynamic Learning Algorithm
1: Input: d1, ..., dm where di = bi/n
2: Initialize: Find δ ∈ (1, 2] and L > 0 s.t. (cid:98)δL(cid:99) = n.
3: Let tk = (cid:98)δk(cid:99), k = 1, 2, ..., L − 1 and tL = n + 1
4: Set x1 = ... = xt1 = 0
5: for k = 1, 2, ..., L − 1 do
6:

Specify an optimization problem

tk(cid:88)

j=1

tk(cid:88)

max

s.t.

rjxj

aijxj ≤ tkdi,

i = 1, ..., m

j=1
0 ≤ xj ≤ 1,

j = 1, ..., tk

7:

Solve its dual problem and obtain the optimal dual variable p∗
k

p∗

k = arg min

p

m
(cid:88)

i=1

dipi +

1
tk

tk(cid:88)

j=1

(cid:32)

rj −

m
(cid:88)

i=1

(cid:33)+

aijpi

s.t. pi ≥ 0,

i = 1, ..., m.

8:
9:

for t = tk + 1, ..., tk+1 do

If constraints permit, set

xt =

(cid:40)

1,
0,

if rt > a(cid:62)
if rt ≤ a(cid:62)

t p∗
k
t p∗
k

Otherwise, set xt = 0
If t = n, stop the whole procedure.

10:
11:
12:
13: end for

end for

consume too much resource in the past periods, the remaining resource bit will shrink, and Algorithm
3 will accordingly push up the dual price and be more inclined to reject an order. On the contrary, if

we happen to reject a lot of orders at the beginning and it results in too much remaining resource, the

algorithm will lower down the dual price so as to accept more orders in the future. This pendulum-like

design in Algorithm 3 incorporates the past actions in computing dual prices indirectly through the

remaining resources.

From an algorithmic standpoint, Algorithm 3 implements the re-solving technique in a learning

environment, while the idea was implemented in a known-parameter environment by (Reiman and Wang,

2008; Jasin and Kumar, 2012; Bumpensanti and Wang, 2018). Unlike the work (Jasin, 2015) which

explicitly estimated the arrival intensity and fed the estimate into a certainty-equivalent problem, the

learning part of Algorithm 3 is implicit and integrated into the optimization part. For the optimization

problem in Step 6 of Algorithm 3, the left-hand-side is speciﬁed by the history, while the right-hand-
as the average remaining
side is speciﬁed by the real-time constraint capacity. If we deﬁne dit = bit
n−t
resource capacity, then the optimization problem at time t can be viewed as a t-sample approximation to
a stochastic program speciﬁed by dt = (d1t, ..., dmt)(cid:62). Importantly, the targeted stochastic program at
each time period is dynamically changing according to the constraint process, while Algorithm 2 and all
the preceding analyses in this paper focus on a static stochastic program speciﬁed by the ﬁxed initial d.
To apply the dual convergence result in a changing d setting, we need a uniform version of Assumption

20

Algorithm 3 Action-history-dependent Learning Algorithm
1: Input: n, d1, ..., dm
2: Initialize the constraint bi0 = ndi for i = 1, ..., m
3: Initialize the dual price p1 = 0.
4: for t = 1, ..., n do
5:

Observe (rt, at) and set

xt =

if the constraints are not violated

Update the constraint vector

(cid:40)

1,
0,

if rt > a(cid:62)
if rt ≤ a(cid:62)

t pt
t pt

6:

7:

bit = bi,t−1 − aitxt

for i = 1, ..., m

Specify an optimization problem

t
(cid:88)

j=1

t
(cid:88)

max

s.t.

rjxj

aijxj ≤

j=1
0 ≤ xj ≤ 1,

tbit
n − t

,

i = 1, ..., m

j = 1, ..., t

8:

If t < n, solve its dual problem and obtain the dual price pt+1

pt+1 = arg min

p

m
(cid:88)

i=1

bitpi
n − t

+

1
t

t
(cid:88)

j=1

(cid:32)

rj −

m
(cid:88)

i=1

(cid:33)+

aijpi

s.t. pi ≥ 0,

i = 1, ..., m.

9: end for

2. Speciﬁcally, for d(cid:48) = (d(cid:48)

1, ..., d(cid:48)

m)(cid:62) ∈ Ωd, deﬁne

fd(cid:48)(p) := d(cid:48)(cid:62)p + E(r,a)∼P

(cid:2)(r − a(cid:62)p)+(cid:3)

and denote p∗(d(cid:48)) denotes its optimal solution. Assumption 3 shares the same part (a) with Assumption
2 and extends the part (b) and (c) of Assumption 2 to a uniform condition for all d(cid:48) ∈ Ωd. Accordingly,
we update the deﬁnition of Ξ to denote the family of distributions that satisfy Assumption 1 and 3; for
this subsection, we consider the distribution P within this updated Ξ.

Assumption 3 (Uniform version of Assumption 2). We assume

(a) The second-order moment matrix M := E(r,a)∼P [aa(cid:62)] is positive-deﬁnite. Denote its minimum

eigenvalue with λmin.

(b) There exist constants λ and µ such that if (r, a) ∼ P,

λ|a(cid:62)p − a(cid:62)p∗(d(cid:48))| ≤ (cid:12)

(cid:12)P(r > a(cid:62)p|a) − P(r > a(cid:62)p∗(d(cid:48))|a)(cid:12)

(cid:12) ≤ µ|a(cid:62)p − a(cid:62)p∗(d(cid:48))|

holds for any p ∈ Ωp and d(cid:48) ∈ Ωd.

(c) The optimal solution p∗(d(cid:48)) satisﬁes p∗

i (d(cid:48)) = 0 if and only if d(cid:48)

i − E(r,a)∼P [aiI(r > a(cid:62)p∗(d(cid:48)))] > 0

for any d(cid:48) ∈ Ωd.

21

Theorem 5 states that Algorithm 3 incurs a worst-case regret of O(log n log log n). Technically, the
proof builds upon an analysis of the three components of the generic upper bound in Theorem 2. A

caveat is that the SAA problem and the underlying stochastic program in Algorithm 3 are dynamically
changing over time. To analyze the algorithm, we identify a subset D ⊂ Ωd around the initial d = b/n
and show that (i) when dt = d(cid:48) ∈ D, Assumption 1 and 3 are satisﬁed, and more importantly, all the
stochastic programs speciﬁed by d(cid:48) share the same binding and non-binding dimensions; (ii) the process
dt exits the region D only at the very end of the horizon. Since the constraint process bt and dt are more
complicated, we adopt a diﬀerent approach than the proof of Theorem 4. The proof of the theorem and

more discussions are deferred to Section C3.

Theorem 5. With the online policy π3 speciﬁed by Algorithm 3,

∆n(π3) ≤ O(log n log log n).

(a) Algorithm 2

(b) Algorithm 3

(c) Algorithm 2

(d) Algorithm 3

Figure 1: Constraint Consumption: m = 4 and n = 2000. Both rj and aij are generated from
Uniform[0, 1]. The bottom two plots zoom in to the last 250 periods in the top two plots.

Figure 1 visualizes the constraint consumption under Algorithm 2 and Algorithm 3. We run 10
simulation trials for each algorithm and plot the constraint consumption of a binding constraint from

each random trial. The top two ﬁgures show that both algorithms seem to perform well in balancing

the resource consumption: not to exhaust the constraint too early or to have too much leftover at the

end. But if we zoom into the last 250 periods as the bottom two ﬁgures, the advantage of Algorithm
3 becomes signiﬁcant. For Algorithm 2, some trials exhaust the constraint O(
n) periods prior to the

√

22

n) remaining at the end. Interestingly, for some curves (like the grey
end, while some trials have an O(
and green ones) in Figure 1c, the remaining resource level stops decreasing O(
n) periods prior to the
end, though the remaining resource level is strictly positive. This is because some other constraint(s)

√

√

has been exhausted at that time, and from that point on, we can not accept more orders even though

there are still remaining resources for the plotted constraint. In comparison, the constraint consumption

of Algorithm 3 is much more stable.

5 Experiments and Discussions

5.1 Numerical Experiments

We implement the three proposed algorithms on three diﬀerent models, with model details given in Table
1. In the ﬁrst model (Random Input I), the constraint coeﬃcients aj’s and objective coeﬃcients rj’s
are i.i.d. generated as bounded random variables. All di’s are set to be 0.25.
In the second model
(Random Input II), the constraint coeﬃcient aij is generated from a normal distribution, which violates
the boundedness assumption. The assignment of rj is deterministic conditional of aj and thus violates
Assumption 2 (b). Both aij and rj take negative values with a positive probability. In Random Input
II, we set di’s alternatively to be 0.2 and 0.3. In the third model, we consider a random permutation
model, the same as the worst-case example in (Agrawal et al., 2014). The number of decision variables
n is a random variable itself in this permutation model, so we specify its expectation to be 100 and 300
in the experiment.

Model
Random Input I
Random Input II
Permutation

aj
aij ∼ Uniform[-0.5,1]
aij ∼ Normal(0.5, 1)

rj
rj ⊥ aj and rj ∼ Uniform[0, 10]

rj = (cid:80)m

i=1 aij

(Agrawal et al., 2014)

Table 1: Models used in the experiments

Table 2 reports the estimated regrets of the three algorithms under diﬀerent combinations of m and
n. The estimation is based on 200 simulation trials and in each simulation trial, a problem instance (aij’s
and rj’s) is generated from the corresponding model. While solving the stochastic program in Algorithm
1, we use an SAA scheme with 106 samples. We have the following observations based on the experiment
results. First, Table 2 shows that Algorithm 3 performs uniformly better than Algorithm 1 and Algorithm

2.

Importantly, Algorithm 3 also excels in the models of Random Input II and Permutation where

the assumptions for theoretical analysis are violated. In particular, the random permutation problem

instance is used in Agrawal et al. (2014) to establish the necessary condition on the constraint capacity

and can thus be viewed as one of the most challenging OLP problems. As far as we know, Algorithm 3
is the ﬁrst algorithm that employs the action-history-dependent mechanism in a generic OLP setting. In

this regard, Algorithm 1 and Algorithm 2 stand for typical algorithms in OLP literature (Molinaro and

Ravi, 2013; Agrawal et al., 2014; Gupta and Molinaro, 2014; Devanur et al., 2019), and the experiments

demonstrate the eﬀectiveness of the action-history-dependent design compared with these works. Besides,
the advantage of Algorithm 3 becomes smaller as the ratio m/n goes up. This can be explained by the
fact that the dual convergence rate is of order (cid:112)m/n and therefore a dual-based algorithm like Algorithm
3 would be more eﬀective in a large-n and small-m regime.

To illustrate how the regret scales with n, we ﬁx m = 4 and run the experiments for diﬀerent
n(= 25, 50, 100, 250, 500, 1000, 2000). The results are presented in Figure 2, where the curves are plotted
by connecting the sample points. For the left panel, the curves verify the regret results in Theorem 3, 4

and 5. Meanwhile, the right panel looks interesting in that the regrets of Algorithm 1 and 2 scale linearly

23

Model

Random Input I

Random Input II

Permutation

Algorithm

A1

A2

A3

A1

A2

A3

A1

A2

A3

m = 4, n = 100
m = 4, n = 300
m = 16, n = 100
m = 16, n = 300
m = 64, n = 100
m = 64, n = 300

28.17
60.17
30.21
60.76
36.84
67.78

37.68
86.33
45.16
88.91
40.52
87.68

27.14
45.01
27.59
46.30
34.77
52.90

11.75
37.59
93.34
184.4
493.2
1017.6

23.85
76.43
81.02
160.4
461.7
881.9

5.29
5.47
52.69
49.13
414.5
611.1

33.17
109.6
24.96
140.5
37.70
145.9

37.62
51.52
21.96
59.56
20.31
47.22

7.42
11.48
12.88
14.87
15.97
18.76

Table 2: Regret performance: A1, A2, and A3 stand for Algorithm 1 (No-need-to-learn), Algorithm 2
(Simpliﬁed Dynamic Learning), and Algorithm 3 (Action-history-dependent), respectively.

with n while the regret of Algorithm 3 is O(1) (a horizontal line can be ﬁtted). This phenomenon is
potentially caused by the deterministic assignment of rj’s in the Random Input II model.

(a) Random Input I

(b) Random Input II

Figure 2: Regret curves with m = 4

In the numerical experiments (Figure 2 (a)), we observe empirically that the regrets for Algorithm 1
n. This indicates that the geometric interval is already suﬃcient
for Algorithm 2. In other words, the performance of Algorithm 2 cannot be further improved by simply

and for Algorithm 2 are both of order

√

increasing learning frequency. Furthermore, it means the consideration of past actions is necessary if we
want to reduce the regret to O(log n). The same observation has been made for the network revenue
management problem and thus motivates the study of the re-solving technique.

Table 3 reports numerical experiments based on the network revenue management problem (Jasin,

2015). The network revenue management problem therein can be formulated as an OLP problem by

associating each arriving customer with a binary decision variable which denotes the decision of accep-

tance or rejection. The parameters in our experiment are the same as the numerical experiment in (Jasin,

2015): demand rate, deterministic price, itinerary structure, and ﬂight capacity. We implemented the

PAC (Probabilistic Allocation Control) algorithm in (Jasin, 2015) and the re-solving algorithm in (Jasin

and Kumar, 2012). Two versions of the re-solving algorithm are implemented, one with the knowledge

of true parameters (Re-solving (I)) and the other one with an imprecise parameter estimate (Re-solving

(II)). Speciﬁcally, we perturb the arrival probability estimate by a magnitude of 0.005 for each customer

type and re-normalize the perturbed probability. The imprecise estimate aims to capture the scenario

that the decision maker has no access to the true parameters and can only calibrate the arrival proba-

bility through history data. Diﬀerent re-solving frequencies and diﬀerent horizons are tested for all four

algorithms and the performance is reported based on an average over 200 simulation trials. Speciﬁcally,
for the PAC algorithm, we set the initial learning period t0 = 15 which means accepting all the customers

24

when t ≤ t0.

Algorithm 3

PAC

Re-solving (I)

Re-solving (II)

Re-solving freq.

2

10

2

10

2

10

2

10

Horizon

n = 200
n = 400
n = 800
n = 1600

63.73
82.67
114.57
122.81

64.83
92.01
111.50
131.76

123.49
141.32
179.93
192.18

168.32
183.14
192.22
221.03

60.04
121.62
205.67
383.37

84.61
129.68
235.31
402.52

62.59
131.29
226.55
477.73

87.34
140.52
266.97
486.78

Table 3: Regret performance on network revenue management experiment (Jasin, 2015). PAC stands
for the Probabilistic Allocation Control algorithm proposed in (Jasin, 2015). Re-solving (I) stands for
the re-solving based algorithm proposed in (Jasin and Kumar, 2012). Re-solving (II) stands for the re-
solving based algorithm proposed in (Jasin and Kumar, 2012) but with an imprecise parameter estimate.
Re-solving frequency denotes the frequency with which the dual or primal control is updated for all
four algorithms. Horizon denotes the number of periods (total number of customers in network revenue
management problem/decision variables in OLP). The unit for the numbers in the table is $100.

Our action-history-dependent algorithm (Algorithm 3) has a better empirical performance than the
other three algorithms. The result is a bit surprising in that the PAC algorithm can be viewed as a

primal version of our action-history-dependent algorithm and it also has the adaptive design through

re-solving and re-optimization.

In addition, the Re-solving (I) algorithm even utilizes the knowledge

of true parameter values. We believe the advantage of our algorithm is only up to a constant factor
when n is suﬃciently large, and we provide two explanations for the observation. First, there are 41
diﬀerent itinerary routes and 14 connecting ﬂights (m = 14) in this experiment. It means that the PAC
algorithm needs to estimate 41 parameters, while Algorithm 3 only needs to estimate 14 parameters.

Also, the nature of the problem may result in a smaller variance when estimating the optimal dual

price. Speciﬁcally, the 41 parameters in the PAC algorithms are all probabilities and the average value

is approximately 0.02. So the estimation suﬀers from the eﬃciency issues in the rare-event simulation

(Asmussen and Glynn, 2007).

In addition, the two-step procedure of the PAC algorithm feeds the

estimation as input for an optimization problem and the optimization procedure may further amplify the

estimation error. Second, all three re-solving algorithms are primal-based while our Algorithm 3 is dual-

based. Note that the primal-based algorithms output a randomized allocation rule from the optimization

procedure, and consequently this randomized rule induces more randomness when deciding the values of

the primal variables. The additional randomness may cause more ﬂuctuation of the constraint process.

In contrast, our Algorithm 3 is dual-based and can thus be viewed as a smoother version of the three

primal-based algorithms.

5.2 Lower Bound and Open Questions

Now, we present a lower bound result for the OLP problem for dual-based policies. Bray (2019) estab-
lished that the worst-case regret of the multi-secretary problem is Ω(log n) even with the knowledge of
underlying distribution. We provide an alternative proof for the lower bound for two goals. First, the

proof mimics the derivation of the lower bounds in (Keskin and Zeevi, 2014; Besbes and Muharremoglu,

2013) and the core part is based on van Trees inequality (Gill and Levit, 1995) – a Bayesian version of the

Cramer-Rao bound. Thus, it extends the previous lower bound analysis from an unconstrained setting

to a constrained setting. Second, recall that we develop a generic regret upper bound in Theorem 2. The

lower bound proof shows that under certain conditions, the generic regret upper bound is rather tight.
Intuitively, this indicates that an eﬀective learning of p∗ and stable control of the constraint process are
not only suﬃcient but also probably necessary to guarantee a sharp regret bound.

25

Theorem 6. There exist constants C and n0 > 0 such that

∆n(π) ≥ C log n

holds for all n ≥ n0 and any dual-based policy π.

Based on the experiments and the theoretical results developed in this paper, we raise several open
questions for future study. First, how does the regret depend on m? In the experiments of Random
Input I, we observe that the regret increases but does not scale up much with m. This is apparently not
the case for Random Input II where the regret increases signiﬁcantly as m grows larger (See Section D6
for more experiments). A possible explanation is that the generation of rj’s in Random Input II causes
that rj scales with m and consequently the oﬄine optimal objective value scales linearly with mn. A
natural question then is what are the conditions that render the regret dependent on m and in what
way the regret depends on m. Second, is it possible to relax the assumptions and extend the theoretical
results for more general random input models and permutation models? We observe a good performance

of Algorithm 3 when the assumptions are violated and even in the permutation model. For example, we
observe an O(1) regret of Algorithm 3 in the experiment under Random Input II (Figure 2). Since the
assumptions are violated, the lower bound does not hold in Random Input II. Also, it is an interesting

question to ask if the dual convergence and the regret results still hold under the random permutation

model. This question entails a proper deﬁnition of the stochastic program (7) in the permutation context.
Third, in Assumption 1 (c), we require that the constraints scale linearly with n. We have not answered
the question of whether this linear growth rate is necessary for establishing the dual convergence results.

In other words, how can the dual convergence and regret bounds be extended to a limited-resource

regime? Besides, Algorithm 3 updates the dual price in every period. This raises the question if it is

possible to have a less frequent updating/learning scheme but still to achieve the same order of regret.

In the network revenue management setting where the distribution is known, Bumpensanti and Wang

(2018) has shown the eﬀectiveness of an infrequent updating scheme. The analysis therein highly relies

on the ﬁniteness of the distribution’s support as well as the knowledge of the distribution. We believe it is

both interesting and challenging to derive a similar low-regret result with an infrequent updating scheme

for the general OLP problem. The last question is on the type of regret bound: all the algorithm bounds

we provide in this paper are on regret expectation. An interesting question is how a high-probability

regret bound can be derived for the OLP problem (possibly with extra logarithmic terms). In fact, the

results of dual convergence in our paper are probabilistic (as in Proposition 3 and 4), which may serve
as a good starting point for deriving a high-probability regret bound.

Acknowledgment

We thank Yu Bai, Simai He, Peter W. Glynn, Daniel Russo, Zizhuo Wang, Zeyu Zheng, and seminar participants

at Stanford AFTLab, NYU Stern, Columbia DRO, Chicago Booth, Imperial College Business School and ADSI

summer school for helpful discussions and comments. We thank Chunlin Sun, Guanting Chen, and Zuguang Gao

for proofreading the proof and Yufeng Zheng for assistance in the simulation experiments.

References

Abernethy, Jacob, Peter L Bartlett, Alexander Rakhlin, Ambuj Tewari. 2008. Optimal strategies and

minimax lower bounds for online convex games .

26

Agrawal, Shipra, Nikhil R Devanur. 2014a. Bandits with concave rewards and convex knapsacks. Pro-

ceedings of the ﬁfteenth ACM conference on Economics and computation. 989–1006.

Agrawal, Shipra, Nikhil R Devanur. 2014b. Fast algorithms for online stochastic convex programming.

Proceedings of the twenty-sixth annual ACM-SIAM symposium on Discrete algorithms. SIAM, 1405–

1424.

Agrawal, Shipra, Zizhuo Wang, Yinyu Ye. 2014. A dynamic near-optimal algorithm for online linear

programming. Operations Research 62(4) 876–890.

Arlotto, Alessandro, Itai Gurvich. 2019. Uniformly bounded regret in the multisecretary problem.

Stochastic Systems .

Asadpour, Arash, Xuan Wang, Jiawei Zhang. 2019. Online resource allocation with limited ﬂexibility.

Management Science .

Asmussen, Søren, Peter W Glynn. 2007. Stochastic simulation: algorithms and analysis, vol. 57. Springer

Science & Business Media.

Balseiro, Santiago R, Yonatan Gur. 2019. Learning in repeated auctions with budgets: Regret minimiza-

tion and equilibrium. Management Science 65(9) 3952–3968.

Bartlett, Peter L, Olivier Bousquet, Shahar Mendelson, et al. 2005. Local rademacher complexities. The

Annals of Statistics 33(4) 1497–1537.

Besbes, Omar, Alp Muharremoglu. 2013. On implications of demand censoring in the newsvendor

problem. Management Science 59(6) 1407–1424.

Besbes, Omar, Assaf Zeevi. 2009. Dynamic pricing without knowing the demand function: Risk bounds

and near-optimal algorithms. Operations Research 57(6) 1407–1420.

Borodin, Allan, Ran El-Yaniv. 2005. Online computation and competitive analysis. cambridge university

press.

Boucheron, Stéphane, Gábor Lugosi, Pascal Massart. 2013. Concentration inequalities: A nonasymptotic

theory of independence. Oxford university press.

Bray, Robert L. 2019. Does the multisecretary problem always have bounded regret? arXiv preprint

arXiv:1912.08917 .

Buchbinder, Niv, Kamal Jain, Joseph Seﬃ Naor. 2007. Online primal-dual algorithms for maximizing

ad-auctions revenue. European Symposium on Algorithms. Springer, 253–264.

Buchbinder, Niv, Joseph Naor. 2009. Online primal-dual algorithms for covering and packing. Mathe-

matics of Operations Research 34(2) 270–286.

Buchbinder, Niv, Joseph Seﬃ Naor, et al. 2009. The design of competitive online algorithms via a
primal–dual approach. Foundations and Trends® in Theoretical Computer Science 3(2–3) 93–263.

Bumpensanti, Pornpawee, He Wang. 2018. A re-solving heuristic with uniformly bounded loss for network

revenue management. arXiv preprint arXiv:1802.06192 .

Chen, Ningyuan, Guillermo Gallego. 2018. A primal-dual learning algorithm for personalized dynamic

pricing with an inventory constraint. Available at SSRN .

27

De Farias, Daniela Pucci, Benjamin Van Roy. 2004. On constraint sampling in the linear programming
approach to approximate dynamic programming. Mathematics of operations research 29(3) 462–478.

Devanur, Nikhil R, Thomas P Hayes. 2009. The adwords problem: online keyword matching with

budgeted bidders under random permutations. Proceedings of the 10th ACM conference on Electronic

commerce. ACM, 71–78.

Devanur, Nikhil R, Kamal Jain, Balasubramanian Sivan, Christopher A Wilkens. 2011. Near optimal

online algorithms and fast approximation algorithms for resource allocation problems. Proceedings of

the 12th ACM conference on Electronic commerce. ACM, 29–38.

Devanur, Nikhil R, Kamal Jain, Balasubramanian Sivan, Christopher A Wilkens. 2019. Near optimal

online algorithms and fast approximation algorithms for resource allocation problems. Journal of the
ACM (JACM) 66(1) 7.

Ferreira, Kris Johnson, David Simchi-Levi, He Wang. 2018. Online network revenue management using

thompson sampling. Operations research 66(6) 1586–1602.

Gill, Richard D, Boris Y Levit. 1995. Applications of the van trees inequality: a bayesian cramér-rao

bound. Bernoulli 1(1-2) 59–79.

Goel, Gagan, Aranyak Mehta. 2008. Online budgeted matching in random input models with applications

to adwords. Proceedings of the nineteenth annual ACM-SIAM symposium on Discrete algorithms.
Society for Industrial and Applied Mathematics, 982–991.

Gupta, Anupam, Marco Molinaro. 2014. How experts can solve lps online. European Symposium on

Algorithms. Springer, 517–529.

Hazan, Elad, et al. 2016.

Introduction to online convex optimization. Foundations and Trends® in

Optimization 2(3-4) 157–325.

Huber, Peter J, et al. 1967. The behavior of maximum likelihood estimates under nonstandard conditions.

Jasin, Stefanus. 2015. Performance of an lp-based control for revenue management with unknown demand

parameters. Operations Research 63(4) 909–915.

Jasin, Stefanus, Sunil Kumar. 2012. A re-solving heuristic with bounded revenue loss for network revenue

management with customer choice. Mathematics of Operations Research 37(2) 313–345.

Jasin, Stefanus, Sunil Kumar. 2013. Analysis of deterministic lp-based booking limit and bid price

controls for revenue management. Operations Research 61(6) 1312–1320.

Keskin, N Bora, Assaf Zeevi. 2014. Dynamic pricing with an unknown demand model: Asymptotically

optimal semi-myopic policies. Operations Research 62(5) 1142–1167.

Kesselheim, Thomas, Andreas Tönnis, Klaus Radke, Berthold Vöcking. 2014. Primal beats dual on

online packing lps in the random-order model. Proceedings of the forty-sixth annual ACM symposium

on Theory of computing. ACM, 303–312.

Kleinberg, Robert. 2005. A multiple-choice secretary algorithm with applications to online auctions. Pro-

ceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial

and Applied Mathematics, 630–631.

Kleywegt, Anton J, Alexander Shapiro, Tito Homem-de Mello. 2002. The sample average approximation

method for stochastic discrete optimization. SIAM Journal on Optimization 12(2) 479–502.

28

Knight, Keith. 1998. Limiting distributions for l1 regression estimators under general conditions. Annals

of statistics 755–770.

Lakshminarayanan, Chandrashekar, Shalabh Bhatnagar, Csaba Szepesvári. 2017. A linearly relaxed

approximate linear program for markov decision processes. IEEE Transactions on Automatic Control
63(4) 1185–1191.

Lei, Yanzhe Murray, Stefanus Jasin, Amitabh Sinha. 2014. Near-optimal bisection search for nonpara-

metric dynamic pricing with inventory constraint. Ross School of Business Paper (1252).

Mackey, Lester, Michael I Jordan, Richard Y Chen, Brendan Farrell, Joel A Tropp, et al. 2014. Matrix
concentration inequalities via the method of exchangeable pairs. The Annals of Probability 42(3)
906–945.

Mahdavi, Mehrdad, Rong Jin, Tianbao Yang. 2012. Trading regret for eﬃciency: online convex opti-
mization with long term constraints. Journal of Machine Learning Research 13(Sep) 2503–2528.

Mehta, Aranyak. 2013. Online matching and ad allocation. Foundations and Trends® in Theoretical

Computer Science 8(4) 265–368.

Mehta, Aranyak, Amin Saberi, Umesh Vazirani, Vijay Vazirani. 2005. Adwords and generalized on-line

matching. 46th Annual IEEE Symposium on Foundations of Computer Science (FOCS’05). IEEE,

264–273.

Molinaro, Marco, Ramamoorthi Ravi. 2013. The geometry of online packing linear programs. Mathe-

matics of Operations Research 39(1) 46–59.

Reiman, Martin I, Qiong Wang. 2008. An asymptotically optimal policy for a quantity-based network

revenue management problem. Mathematics of Operations Research 33(2) 257–282.

Revuz, Daniel, Marc Yor. 2013. Continuous martingales and Brownian motion, vol. 293. Springer Science

& Business Media.

Rockafellar, R Tyrrell. 1970. Convex analysis, vol. 28. Princeton university press.

Shapiro, Alexander. 1993. Asymptotic behavior of optimal solutions in stochastic programming. Math-

ematics of Operations Research 18(4) 829–845.

Shapiro, Alexander, Darinka Dentcheva, Andrzej Ruszczyński. 2009. Lectures on stochastic programming:

modeling and theory. SIAM.

Talluri, Kalyan, Garrett Van Ryzin. 1998. An analysis of bid-price controls for network revenue man-

agement. Management science 44(11-part-1) 1577–1593.

Talluri, Kalyan T, Garrett J Van Ryzin. 2006. The theory and practice of revenue management, vol. 68.

Springer Science & Business Media.

Vu, Ky, Pierre-Louis Poirion, Leo Liberti. 2018. Random projections for linear programming. Mathe-

matics of Operations Research 43(4) 1051–1071.

Wang, Zizhuo, Shiming Deng, Yinyu Ye. 2014. Close the gaps: A learning-while-doing algorithm for

single-product revenue management problems. Operations Research 62(2) 318–331.

Wu, Huasen, Rayadurgam Srikant, Xin Liu, Chong Jiang. 2015. Algorithms with logarithmic or sublinear

regret for constrained contextual bandits. Advances in Neural Information Processing Systems. 433–

441.

29

Yu, Hao, Michael Neely, Xiaohan Wei. 2017. Online convex optimization with stochastic constraints.

Advances in Neural Information Processing Systems. 1428–1438.

Yuan, Jianjun, Andrew Lamperski. 2018. Online convex optimization for cumulative constraints. Ad-

vances in Neural Information Processing Systems. 6137–6146.

30

Appendices

A Proofs for Section 2

A1 Proof of Proposition 1

Proof.

(a) The original dual problem (3) can be recovered by substituting yj = (rj − (cid:80)m

i=1 aijpi)+ in
the objective function (5). Then the feasible solutions and the objective functions are matched.

Therefore, these two problems share the same optimal solution.

(b) We know that each component in the ﬁrst summation is linear and each component in the second

summation is convex. Also, the summation operation preserves convexity (See Chapter 3.2.1
(Rockafellar, 1970)). So, both fn and f are convex functions.

(c) If d(cid:62)p > ¯r, then

f (p) ≥ d(cid:62)p > ¯r ≥ E[r] = f (0).

Hence p cannot be the optimal solution. In the same way, we can show the result for p∗
n.

A2 Proof of Lemma 1

Proof. We only need to show the following equality (before taking the expectation),

h(p, (r, a)) − h(p∗, (r, a)) = φ(p∗, (r, a))(cid:62)(p − p∗) +

(cid:90) a(cid:62)p∗

a(cid:62)p

(cid:0)I(r > v) − I(r > a(cid:62)p∗)(cid:1) dv.

Indeed,

h(p, (r, a)) − h(p∗, (r, a)) − φ(p∗, (r, a))(cid:62)(p − p∗)

=d(cid:62)p + (r − a(cid:62)p)+ − d(cid:62)p∗ − (r − a(cid:62)p∗)+ − (d − a · I(rj > a(cid:62)p∗))(cid:62)(p − p∗)
=(r − a(cid:62)p)+ − (r − a(cid:62)p∗)+ + I(r > a(cid:62)p∗) · (a(cid:62)p − a(cid:62)p∗)

=

=

(cid:90) a(cid:62)p∗

a(cid:62)p
(cid:90) a(cid:62)p∗

a(cid:62)p

I(r > v)dv + I(r > a(cid:62)p∗) · (a(cid:62)p − a(cid:62)p∗)

(cid:0)I(r > v) − I(r > a(cid:62)p∗)(cid:1) dv.

By taking expectation with respect to (r, a) ∼ P, we obtain the identity (9).

A3 Examples for Assumption 2 (b)

In Example 2, it is easy to see that for p such that |a(cid:62)p − a(cid:62)p∗| ≤ c(cid:15). We have

α|a(cid:62)p − a(cid:62)p∗| ≤ (cid:12)

(cid:12)P(r > a(cid:62)p|a) − P(r > a(cid:62)p∗|a)(cid:12)

(cid:12) ≤ ¯α|a(cid:62)p − a(cid:62)p∗|.

For a general p ∈ Ωp, we know

|a(cid:62)p − a(cid:62)p∗| ≤ ¯a(cid:107)p − p∗(cid:107)2 ≤ ¯a(cid:107)p − p∗(cid:107)1 ≤

¯a¯r
d

31

where the last inequality comes from Proposition 1 (c). Without loss of generality, we consider a(cid:62)p <
a(cid:62)p∗ − c(cid:15), then

(cid:12)P(r > a(cid:62)p|a) − P(r > a(cid:62)p∗|a)(cid:12)
(cid:12)

(cid:12) ≥ αc(cid:15) ≥

|a(cid:62)p − a(cid:62)p∗|.

αc(cid:15)d
¯a¯r

So, we can choose µ = ¯α and λ = min

(cid:110)

α, αc(cid:15)d
¯a¯r

(cid:111)

and Assumption 2 (b) is satisﬁed.

The implication for this second example is that for the existence of µ, we only need that the density
function has a ﬁnite upper bound, and for the existence of λ, we can impose a locally lower bound for
the density function and extend the condition to a more general support through the above derivation.

Then, Example 3 follows the same intuition and analysis as Example 2.

A4 Proof of Proposition 2

Proof. To see the result, from Lemma 1, we only need to analyze the second-order term

(cid:34)(cid:90) a(cid:62)p∗

E

(cid:0)I(r > v) − I(r > a(cid:62)p∗)(cid:1) dv

(cid:35)

a(cid:62)p
(cid:34)(cid:90) a(cid:62)p∗

(cid:34)

E

(cid:0)I(r > v) − I(r > a(cid:62)p∗)(cid:1) dv

a(cid:62)p
(cid:34)(cid:90) a(cid:62)p∗

P(r > v|a) − P(r > a(cid:62)p∗|a)dv

a(cid:62)p
(cid:34)(cid:90) a(cid:62)p∗

a(cid:62)p

(cid:35)

µ(a(cid:62)p∗ − v)dv

E (cid:2)(a(cid:62)p − a(cid:62)p∗)2(cid:3)

(cid:107)p − p∗(cid:107)2
2

=E

=E

≤E

=

≤

µ
2
µ¯a2
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:35)

(cid:35)(cid:35)

a

(17)

(18)

where the ﬁrst inequality comes from Assumption 2 (b) and the second inequality comes from the
boundedness of a – Assumption 1 (b). With a similar argument, we can show that

(cid:34)(cid:90) a(cid:62)p∗

E

a(cid:62)p

(cid:0)I(r > v) − I(r > a(cid:62)p∗)(cid:1) dv

(cid:35)

≥

λλmin
2

(cid:107)p − p∗(cid:107)2
2

where λmin is speciﬁed in Assumption 2 (a) and λ is speciﬁed in Assumption 2 (b).

To see the uniqueness of p∗, we ﬁrst show that

∇f (p∗) · p∗ = 0

(19)

(20)

where the operator · denotes the element-wise product. In addition, ∇f (p∗) ≥ 0. These results can be
shown via a standard argument through the sub-gradient and the optimality condition (See Chapter 23

of (Rockafellar, 1970)). For completeness, we provide a self-contained proof here.

To see ∇f (p∗) ≥ 0, we note from (18) that

f (p) − f (p∗) ≤ ∇f (p∗)(p − p∗) +

µ¯a2
2

(cid:107)p − p∗(cid:107)2
2.

If the i(cid:48)-th entry (∇f (p∗))i(cid:48) < 0 for some i(cid:48), then we can choose p(cid:48) = (p(cid:48)

1, ..., p(cid:48)

m)(cid:62) ≥ 0 such that p(cid:48)

i = p∗
i

32

for i (cid:54)= i(cid:48) and p(cid:48)

i(cid:48) = p∗

i(cid:48) − (∇f (p∗))i(cid:48)

µ¯a2

. It is easy to verify that

f (p(cid:48)) − f (p∗) = −

2(∇f (p∗))2
i(cid:48)
µ¯a2

< 0

which contradicts the optimality of p∗.

Similarly, if (∇f (p∗))i(cid:48) · p∗

and p(cid:48)
i(cid:48) = (p∗
optimality of p∗.

i(cid:48) − (∇f (p∗))i(cid:48)

µ¯a2

i(cid:48) > 0, we can choose p(cid:48) = (p(cid:48)

for i (cid:54)= i(cid:48)
) ∨ 0. It is easy to verify that f (p(cid:48)) − f (p∗) < 0 which also contradicts the

m)(cid:62) ≥ 0 such that p(cid:48)

1, ..., p(cid:48)

i = p∗
i

Consequently, the uniqueness follows from

f (p) − f (p∗) ≥ ∇f (p∗)(p − p∗) +

λλmin
2

(cid:107)p − p∗(cid:107)2
2.

Speciﬁcally, let p(cid:48) (cid:54)= p∗ be another optimal solution to the problem. Then the left-hand-side is zero. But
for the right-hand-side, the ﬁrst term is non-negative from (20) and the fact that ∇f (p∗) ≥ 0, and the
second term is strictly positive, which leads to a contradiction.

A5 Proof of Lemma 2

Proof. The proof is the same as the proof of Lemma 1 and it is completed by doing the analysis for each
term in the summation of fn.

A6 Proof of Proposition 3

First, we introduce the Hoeﬀding’s inequality for scalar random variables.

Lemma 4 (Hoeﬀding’s inequality). Let X1, ..., Xn be independent random variables such that Xi takes
its values in [ui, vi] almost surely for all i ≤ n. Then for every t > 0,

P

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

Xi − EXi

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

(cid:18)

≥ t

≤ 2 exp

−

2n2t2
i=1(ui − vi)2

(cid:80)n

(cid:19)

Proof. We refer to Chapter 2 of the book (Boucheron et al., 2013).

Now, we present the proof of Proposition 3.

Proof. We use φ(p∗, uj)i to denote the i-th coordinate of the gradient vector φ(p∗, uj). By the deﬁnition
of φ, we know that

From the boundedness aj’s (Assumption 1 (b)), we know that

Eφ(p∗, uj)i = (∇f (p∗))i .

|φ(p∗, uj)i| ∈ [di − ¯a, di + ¯a].

Then, by applying the Hoeﬀding’s inequality, we obtain

P


(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

1
n

n
(cid:88)

j=1

φ(p∗, uj)i − (∇f (p∗))i



(cid:18)

≥ (cid:15)

 ≤ 2 exp

−

(cid:19)

.

n(cid:15)2
2¯a2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

33

In fact,






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

j=1

φ(p∗, uj) − ∇f (p∗)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2






⊂


(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)


1
n

m
(cid:91)

i=1

n
(cid:88)

j=1

≥ (cid:15)

φ(p∗, uj)i − (∇f (p∗))i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥

(cid:15)
√
m






.

Applying the union bound,

P





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

j=1

φ(p∗, uj) − (∇f (p∗))i

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2


 ≤ mP

≥ (cid:15)


(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

1
n

n
(cid:88)

j=1

φ(p∗, uj)i − (∇f (p∗))i





≥

(cid:15)
√
m

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18)

≤ 2m exp

−

(cid:19)

.

n(cid:15)2
2¯a2m

Thus we obtain Proposition 3.

A7 Proof of Proposition 4

We ﬁrst introduce a matrix version for the Hoeﬀding’s inequality.

Lemma 5 (Matrix Hoeﬀding’s Inequality). Let X1, ..., Xn ∈ Rd be i.i.d. random vectors with E(XkX (cid:62)
M . Also, we assume (cid:107)Xk(cid:107)2

2 ≤ B almost surely. Let

k ) =

Z =

1
n

n
(cid:88)

k=1

XkX (cid:62)
k .

P ((cid:107)M − Z(cid:107)S ≥ t) ≤ d · exp

(cid:19)

(cid:18) −t2
Bn

Then

for all t > 0.

Proof. We refer to the Corollary 4.2 (Matrix Hoeﬀding Inequality) of (Mackey et al., 2014). The proof

of this lemma simply reduces the matrix in Corollary 4.2 to a vector setting.

Now, we prove Proposition 4.

Proof of Proposition 4. We complete the proof in three steps:

Step 1. We show that the quantity Mn = 1
n

(cid:3) with
high probability. Intuitively, this removes the randomness on aj’s. In the later part of the proof,
we will see that the matrix M works approximately as the Hessian matrix of the function fn(p).
Assumption 2 (b) states that the minimum eigenvalue of the Hessian matrix is λmin.

concentrates around its mean M = E (cid:2)aja(cid:62)

j=1 aja(cid:62)
j

(cid:80)n

j

Step 2. To establish the uniform result, we introduce a ﬁnite set of representative points pkl’s (the indices
k and l to be speciﬁed later) for the set Ωp. As mentioned in the main body of the paper, for each
p ∈ Ωp, the function value

n
(cid:88)

(cid:90) a(cid:62)

j p∗

j=1

a(cid:62)

j p

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv

is a random variable dependent on (rj, aj)’s. Though we can apply the concentration inequality to
analyze the function value for each speciﬁc p, the argument does not go through for all (uncountably
many) points in Ωp as in the proposition. The representative points serve as an intermediary
between the point-wise argument and the uniform argument. The idea is:

34

Part (i). We ﬁrst show the quantity (cid:80)n

j=1

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv concentrates around

(cid:82) a(cid:62)
a(cid:62)

j p∗
j pkl

its mean for all representative points pkl’s

Part (ii). We then establish that for each p ∈ Ωp, there is a pkl near p such that the diﬀerence

n
(cid:88)

(cid:90) a(cid:62)

j p∗

a(cid:62)

j pkl

j=1
(cid:124)

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv

−

(cid:123)(cid:122)
what has been analyzed by the above Part (i)

(cid:125)

n
(cid:88)

j=1
(cid:124)

(cid:90) a(cid:62)

j p∗

a(cid:62)

j p

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv

(cid:123)(cid:122)
the goal of the proposition

(cid:125)

=

n
(cid:88)

(cid:90) a(cid:62)

j p

j=1

a(cid:62)

j pkl

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv

is small with high probability.

In this way, we make the argument through for all p ∈ Ω.

Step 3. We combine the two parts in Step 2 and then put the result together with Step 1.

For Step 1, consider

Mn =

1
n

n
(cid:88)

j=1

aja(cid:62)
j

M = E (cid:2)aja(cid:62)

j

(cid:3)

where the expectation is taken with respect to (rj, aj) ∼ P. We know from Assumption 2 (a) that the
minimum eigenvalue of M is λmin. Also,

λmin − λmin(Mn) ≤ λmax(M − Mn) ≤ (cid:107)M − Mn(cid:107)S

where λmin(·) and λmax(·) refer to the smallest and largest eigenvalue of a matrix, respectively. Denote
event

Applying Lemma 5,

E0 =

λmin(Mn) ≤

(cid:26)

(cid:27)

.

λmin
2

(cid:18)

P(E0) = P

λmin(Mn) ≤

(cid:19)

λmin
2

≤ P

(cid:18)

(cid:107)M − Mn(cid:107)S ≥

(cid:19)

λmin
2

≤ m · exp

(cid:18) −nλ2
4¯a2

min

(cid:19)

.

(21)

where ¯a is the upper bound on aj from Assumption 1 (b). So, we complete Step 1 by showing that the
random matrix Mn has a minimum eigenvalue larger than λmin
2

with high probability.

For Step 2, we ﬁrst present how we select the representative set of points pkl’s. From Proposition 1
p ∈ Rm(cid:12)
(cid:110)
(cid:111)
(cid:12)(cid:107)p − p∗(cid:107)∞ ≤ ¯r
(cid:12)
n ∈ Ωp ⊂ ¯Ω almost surely. We only need to show the results for the larger set ¯Ω. The region ¯Ω can

and p∗ is bounded. Deﬁne set ¯Ω =

(c), we know that the optimal solution p∗
n
and p∗
be split into a union of disjoint sets

d

¯Ω =

N
(cid:91)

lk(cid:91)

k=1

l=1

Ωkl.

The splitting scheme is inspired by (Huber et al., 1967). Speciﬁcally, these sets are divided layer by layer.
The set ¯Ωk =
for k = 0, ..., N. Here N and q ∈ (0, 1) will be determined
d
later. The k-th layer ¯Ωk−1 \ ¯Ωk is further divided into disjoint cubes {Ωkl}lk
with edges of length
l=1
for k = 1, ..., N − 1 and l = 1, ..., lk. The center cube is simply ¯ΩN = ΩN 1 with edge of
(1 − q)qk−1 ¯r
d

p ∈ Rm(cid:12)
(cid:110)
(cid:12)(cid:107)p − p∗(cid:107)∞ ≤ qk ¯r
(cid:12)

(cid:111)

35

and lN = 1. Also, the length q is adjusted in a way that the splitting scheme cut the region
length qN ¯r
d
¯Ω into an integer number of cubes. Figure 3 gives a visualization of the splitting scheme. In total, there
are no more than (2N )m cubes (See Lemma 3 in (Huber et al., 1967)). In the following, we will complete
the two parts in Step 2 for the cubes. First, we analyze the outer cubes Ωkl, for k = 1, ..., N − 1 and
l = 1, ..., lk and then we treat the center cube ΩN 1 separately.

Figure 3: Visualization of the splitting scheme

Let pkl be the center of the cube Ωkl, p

from p∗, respectively. That is,

and ¯pkl be the points in Ωkl that are closest and furthest

kl

p

kl

= arg min

p∈Ωkl

(cid:107)p − p∗(cid:107)2,

¯pkl = arg max

p∈Ωkl

(cid:107)p − p∗(cid:107)2.

Now, we derive the Part (i) of Step 2. To obtain an upper bound for the usage of Hoeﬀding’s

Inequality, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90) a(cid:62)

j p∗

a(cid:62)

j pkl

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ (cid:12)

(cid:12)a(cid:62)

j pkl − a(cid:62)

j p∗(cid:12)

(cid:12) ≤ ¯a(cid:107)p∗ − ¯pkl(cid:107)2

where the right-hand-side is a deterministic quantity that does not depend on (rj, aj)’s. We deﬁne the
following event (for the cube Ωkl and the point pkl indexed by k, l) that the integral deviates from its
mean,

Ekl,1 =

(cid:40)

1
n

n
(cid:88)

j=1

(cid:90) a(cid:62)

j p∗

a(cid:62)
j pkl
(cid:34)(cid:90) a(cid:62)

j p∗

a(cid:62)

j pkl

1
n

n
(cid:88)

E

j=1

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv ≤

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv

(cid:35)

a1, ..., an

− (cid:15)¯a(cid:107)p∗ − ¯pkl(cid:107)2

(cid:41)
.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Note that r1, ..., rn are independent conditional on a1, ..., an. By applying Hoeﬀding’s inequality, we

36

know,

P(Ekl,1|a1, ..., an) ≤ exp

(cid:19)

(cid:18)

−

n(cid:15)2
2

(22)

for a1, ..., an, k = 1, ..., N − 1 and l = 1, ..., lk. Consequently, we know P (Ekl,1) ≤ exp
by
integrating with respect to (a1, ..., an). This completes Part (i) of Step 2, establishing that the random
integral from a(cid:62)

j p∗ concentrates around its (conditional) mean with high probability.

− n(cid:15)2
2

(cid:16)

(cid:17)

j pkl to a(cid:62)
For Part (ii) of Step 2, deﬁne

Γkl(rj, aj) = max
p∈Ωkl

(cid:90) a(cid:62)

j pkl

a(cid:62)

j p

I(rj > v) − I(rj > a(cid:62)

j p∗)dv.

We know that

E[Γkl(rj, aj)|a1, ..., an] = E

(cid:34)

(cid:34)

max
p∈Ωkl

≤ E

max
p∈Ωkl

(cid:90) a(cid:62)

j pkl

a(cid:62)

j p
(cid:90) a(cid:62)

j pkl

a(cid:62)

j p

I(rj > v) − I(rj > a(cid:62)

j p∗)dv

I(v < rj ≤ a(cid:62)

j p∗)dv

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

a1, ..., an

(cid:35)

a1, ..., an

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

(cid:107)p − pkl(cid:107)2 · E

(cid:34)

max
p∈Ωkl

I(a(cid:62)

j p < rj ≤ a(cid:62)

j p∗)dv

(cid:107)p − pkl(cid:107)2 · µ¯a(cid:107)p∗ − ¯pkl(cid:107)2

≤ ¯a max
p∈Ωkl

≤ ¯a max
p∈Ωkl

(cid:35)

a1, ..., an

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= µ¯a2(cid:107)p∗ − ¯pkl(cid:107)2 max
p∈Ωkl

(cid:107)p − pkl(cid:107)2.

Here the second line comes from the deﬁnition of the indicator function. The third line singles out the
two limits of the integral. The fourth line comes from Assumption 2 (b) and the deﬁnition of ¯pkl (the
furthest point in Ωkl from p∗.) Speciﬁcally, the parameter ¯a comes from Assumption 1 (b) and µ comes
from Assumption 2 (b). Also,

|Γkl(rj, aj)| ≤ max
p∈Ωkl

(cid:12)
(cid:12)a(cid:62)

j p − a(cid:62)

j pkl

(cid:12)
(cid:12) ≤ ¯a max
p∈Ωkl

(cid:107)p − pkl(cid:107)2

holds for all k = 1, ..., N − 1 and l = 1, ..., lk. Let

Ekl,2 =






(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

j=1

Γkl(rj, aj) −

1
n

n
(cid:88)

j=1

(cid:12)
(cid:12)
E[Γkl(rj, aj)|a1, ..., an]
(cid:12)
(cid:12)
(cid:12)

≥ 2(cid:15)¯a max
p∈Ωkl

(cid:107)p − pkl(cid:107)2






.

By applying Hoeﬀding’s Inequality with the independence of Γkl(rj, aj)’s conditional on a1, ..., an,

P (Ekl,2|a1, ..., an) ≤ exp

(cid:19)

(cid:18) −n(cid:15)2
2

(23)

for all a1, ..., an, k = 1, ..., N − 1 and l = 1, ..., lk. Consequently, P (Ekl,2) ≤ exp

(cid:16) −n(cid:15)2
2

(cid:17)

.

Next, we handle Step 3 and combine the previous parts together. First, we analyze the conditional

37

expectation (22) on the right hand side of Ekl,1. Conditional on event E0,





1
n

E

n
(cid:88)

j=1

(cid:90) a(cid:62)

j p∗

a(cid:62)

j pkl

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv

a1, ..., an, E0





(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

≥

=

≥

P(rj > v|aj) − P(rj > a(cid:62)

j p∗|aj)dv

n
(cid:88)

(cid:90) a(cid:62)

j p∗

a(cid:62)

j pkl

1
n

j=1
n
(cid:88)

j=1

λ
2n

(a(cid:62)

j p∗ − a(cid:62)

j pkl)2

λ
2

(p∗ − pkl)(cid:62)





1
n

n
(cid:88)

j=1


 (p∗ − pkl)

aja(cid:62)
j

λλmin
4

(cid:107)p∗ − pkl(cid:107)2
2

(24)

where the third line comes from Assumption 2 (b) and the last line comes from the deﬁnition of E0
(earlier in (21)). Based on the splitting scheme, we ﬁnd the relationship between pkl, ¯pkl, p∗, and an
arbitrary p ∈ Ωkl. Speciﬁcally, since the cubes on the k-th layer shrink the whole region with a factor of
qk,

max
p∈Ωkl

(cid:107)p − pkl(cid:107)2 =

√

m(1 − q)qk−1 ¯r
d

,

for all k = 1, ..., N − 1 and l = 1, ..., lk. As a result,

(cid:107)p∗ − pkl(cid:107)2 ≥ qk ¯r
d

,

(cid:107)p∗ − ¯pkl(cid:107)2 ≤ (cid:107)p∗ − pkl(cid:107)2 + max
p∈Ωkl
(cid:18)
(cid:19)

√

(cid:107)p − pkl(cid:107)2

m(1 − q)
q

(cid:107)p∗ − pkl(cid:107)2

and

≤

1 +

max
p∈Ωkl

(cid:107)p − pkl(cid:107)2 ≤

√

m(1 − q)
q

(cid:107)p∗ − pkl(cid:107)2 ≤

√

m(1 − q)
q

(cid:107)p∗ − ¯pkl(cid:107)2 .

(25)

(26)

With (24), (25) and (26), we can compare the diﬀerence (when the event E0 happens) between the
conditional expectations appearing in Ekl,1 and Ekl,2,





1
n

E

n
(cid:88)

j=1

(cid:90) a(cid:62)

j p∗

a(cid:62)

j pkl

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

a1, ..., an


 − E





1
n

n
(cid:88)

j=1

(cid:12)
(cid:12)
(cid:12)
Γkl(rj, aj)
(cid:12)
(cid:12)





a1, ..., an

≥

λλmin
4

≥

λλmin
4

(cid:107)p∗ − pkl(cid:107)2

2 − µ¯a2(cid:107)p∗ − ¯pkl(cid:107)2 max
p∈Ωkl

(cid:107)p − pkl(cid:107)2





1
√
m(1−q)
q

1 +



2



(cid:107)p∗ − ¯pkl(cid:107)2

2 − µ¯a2

√

m(1 − q)
q

(cid:107)p∗ − ¯pkl(cid:107)2
2,

(Applying (26))

where the last line represents the diﬀerence in a quadratic form of (cid:107)p∗ − ¯pkl(cid:107). By choosing

q = max






1
1 + 1√
m

,






,

1
(cid:16) λλmin
8µ¯a2

(cid:17) 1

3

1 + 1√
m

38

we have





1
n

E

n
(cid:88)

j=1

(cid:90) a(cid:62)

j p∗

a(cid:62)

j pkl

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

a1, ..., an


−E





1
n

n
(cid:88)

j=1

(cid:12)
(cid:12)
(cid:12)
Γkl(rj, aj)
(cid:12)
(cid:12)



a1, ..., an

 ≥

λλmin
32

(cid:107)p∗ − ¯pkl(cid:107)2
2 .

(27)
Intuitively, the choice of q ensures that the distance between the cube center pkl and the furthest point
from pkl in the cube Ωkl is dominated by the distance between pkl and p∗. In this way, the integral from
j p for any p ∈ Ωkl. And the
j p∗ to a(cid:62)
a(cid:62)
former integral approximately takes a quadratic form based on the concentration argument.

j pkl in the above will dominate the integral from a(cid:62)

j pkl to a(cid:62)

Now, we are ready to return to the main objective and analyze (cid:80)n

j=1

(cid:82) a(cid:62)
j p∗
a(cid:62)
j p

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv.

In Part 2 of the proof, we decompose it into two parts and derive concentration results for the two parts
respectively. The above inequality (27) puts together the expectation terms in the two concentration
results. By connecting the dots, we know that on event E0 ∩ E c

,

kl,1 ∩ E c

kl,2

(cid:90) a(cid:62)

j p∗

a(cid:62)

j p
(cid:90) a(cid:62)

j p∗

a(cid:62)

j pkl

(cid:90) a(cid:62)

j p∗

a(cid:62)

j pkl

(cid:90) a(cid:62)

j p∗

a(cid:62)

j pkl

(a)
=

(b)
≥

(c)
=

1
n

1
n

1
n

1
n

n
(cid:88)

j=1

n
(cid:88)

j=1

n
(cid:88)

j=1

n
(cid:88)

j=1

−

1
n

j=1


(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv +

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv −

1
n

1
n

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv − E



n
(cid:88)

j=1

n
(cid:88)

j=1


1
n

(cid:90) a(cid:62)

j pkl

a(cid:62)

j p

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv

Γkl(rj, aj)

n
(cid:88)

j=1

(cid:90) a(cid:62)

j p∗

a(cid:62)

j pkl

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv





n
(cid:88)

Γkl(rj, aj) + E





1
n

n
(cid:88)

j=1



Γkl(rj, aj)



+ E



n
(cid:88)

j=1

(cid:90) a(cid:62)

j p∗

a(cid:62)

j pkl

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv


 − E





1
n

n
(cid:88)

j=1



Γkl(rj, aj)



1
n

(d)
≥ − 2(cid:15)¯a(cid:107)p∗ − ¯pkl(cid:107)2 +

(cid:107)p∗ − ¯pkl(cid:107)2
2

(e)
≥ − 2(cid:15)¯a(cid:107)p∗ − p(cid:107)2 +

(cid:107)p∗ − p(cid:107)2
2

λλmin
32
λλmin
32

(28)

holds for any p ∈ Ωkl, k = 1, ..., N − 1 and l = 1, ..., lk. Here (a) decomposes the integral in two parts.
(b) is from the deﬁnition of Γkl. (d) comes from applying (22), (23) and (27) to the three lines in (c). (e)
comes from the deﬁnition of ¯pkl and the property of a quadratic function. To interpret this result (28),
it provides a quadratic lower bound (with high probability) for the integral of interest and the quadratic
lower bound has a dominating second-order part plus a linear part with small coeﬃcient (cid:15).

Note the above results hold for all the cubes with k ≤ N − 1. We need some special treatment for

the center cube ΩN 1. Speciﬁcally, its center is p∗, and

1
n

n
(cid:88)

(cid:90) a(cid:62)

j p∗

j=1

a(cid:62)

j p

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv ≥ −¯a(cid:107)p∗ − ¯pN 1(cid:107)2 = −¯a

√

mqN ¯r
d

where the last equality comes from the splitting scheme.

Intuitively, this center cube is not signiﬁcant because we can always choose N – the number of layers
(of cubes) so that the eﬀect of the center cube is small enough and it is no greater than (cid:15)2. To achieve

39

this, we choose

so that

for all p ∈ ΩN 1.

(cid:36)

N =

logq

(cid:19)(cid:37)

(cid:18) d(cid:15)2
√
¯a¯r

m

+ 1

1
n

n
(cid:88)

(cid:90) a(cid:62)

j p∗

j=1

a(cid:62)

j p

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv ≥ −(cid:15)2

(29)

Therefore, we obtain from (28) and (29) that, on the event ∩N

k=1 ∩lk

l=1 (E c

kl,1 ∩ E c

kl,2) ∩ E0,

1
n

n
(cid:88)

(cid:90) a(cid:62)

j p∗

j=1

a(cid:62)

j p

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv ≥ −(cid:15)2 − 2(cid:15)¯a(cid:107)p∗ − p(cid:107)2 +

λλmin
32

(cid:107)p∗ − p(cid:107)2
2

for all p ∈ ¯Ω.

We complete the proof by computing the probability,

(cid:32) N
(cid:92)

lk(cid:92)

(cid:16)

1 − P

E c
kl,1

(cid:92)

E c
kl,2

(cid:17) (cid:92)

(cid:33)

E0

= P

(cid:32) N
(cid:91)

lk(cid:91)

(cid:16)

Ekl,1

(cid:91)

Ekl,2

(cid:17) (cid:91)

E c
0

(cid:33)

k=1

l=1

k=1

l=1

≤ P(E c

0) +

N
(cid:88)

lk(cid:88)

k=1

l=1

(P(Ekl,1) + P(Ekl,2))

(cid:18)

≤ m exp

−

(cid:19)

min

nλ2
4¯a2

(cid:18)

+ 2 exp

−

(cid:19)

n(cid:15)2
2

· (2N )m .

A8 Proof of Theorem 1

Proof. Let event

From Proposition 3,

Let event

E2 =






1
n

n
(cid:88)

(cid:90) a(cid:62)

j p∗

j=1

a(cid:62)

j p

From Proposition 4,

E1 =






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

j=1

(cid:13)
(cid:13)
(cid:13)
φ(p∗, uj) − ∇f (p∗)
(cid:13)
(cid:13)
(cid:13)2






.

≤ (cid:15)

P (E c

1) ≤ 2m exp

(cid:18)

−

(cid:19)

.

n(cid:15)2
2¯a2m

(cid:0)I(rj > v) − I(rj > a(cid:62)

j p∗)(cid:1) dv ≥ −(cid:15)2 − 2(cid:15)¯a(cid:107)p∗ − p(cid:107)2 +

λλmin
32

(cid:107)p∗ − p(cid:107)2
2






.

P (E c

2) ≤ m exp

(cid:19)

(cid:18)

−

nλmin
4¯a2

+ 2 (2N )m · exp

(cid:19)

(cid:18)

−

n(cid:15)2
2

where N is deﬁned in Proposition 4.

On the event E1 ∩ E2, the following inequality holds for all p ∈ Ωp

fn(p) − fn(p∗) ≥ −(cid:15)2 − (cid:15)(2¯a + 1)(cid:107)p∗ − p(cid:107)2 +

λλmin
32

(cid:107)p∗ − p(cid:107)2
2 .

(30)

This comes from a combination of Lemma 2 with Assumption 2 (b) and (c). Speciﬁcally, we plug in the

40

event E2 for the second-order term in Lemma 2 and we apply Assumption 2 (b) and (c) with the event
E1 for the ﬁrst-order term in Lemma 2.

By the deﬁnition of p∗
n

(that minimizes fn), (30) leads to

−(cid:15)2 − (cid:15)(2¯a + 1)(cid:107)p∗ − p∗

n(cid:107)2 +

λλmin
32

(cid:107)p∗ − p∗

n(cid:107)2

2 ≤ fn(p∗

n) − fn(p∗) ≤ 0

and this implies,

with

(cid:107)p∗

n − p∗(cid:107)2 ≤ κ(cid:15)

(cid:113)

2¯a + 1 +

(2¯a + 1)2 + λλmin

8

λλmin/16

.

κ =

The intuition for the above result is that on the event E1 ∩ E2, the diﬀerence fn(p∗
n) − fn(p∗) is lower
bounded by a convex quadratic function and thus the optimality condition entails p∗ should stay close
. The probability
to p∗
of the “good” event

n. Thus we can view the event E1 ∩ E2 as a “good” event that ensures p∗ close to p∗
n

P(E1 ∩ E2) ≥ 1 − P(E c

1) − P(E c
2)
(cid:18)
n(cid:15)2
2¯a2m

−

≥ 1 − 2m exp

(cid:19)

(cid:18)

− m exp

−

(cid:19)

nλmin
4¯a2

− 2 (2N )m · exp

(cid:18)

−

(cid:19)

.

n(cid:15)2
2

We emphasize that this probability bound holds for all (cid:15) > 0. So, if we let (cid:15)(cid:48) = (cid:15)2, we have

P

(cid:18) (cid:107)p∗

n − p∗(cid:107)2
2
κ2

(cid:19)

> (cid:15)(cid:48)

= P

(cid:18) (cid:107)p∗

(cid:19)

> (cid:15)2

n − p∗(cid:107)2
2
κ2
(cid:18)

≤ 2m exp

−

(cid:18)

= 2m exp

−

n(cid:15)2
2¯a2m
n(cid:15)(cid:48)
2¯a2m

(cid:19)

(cid:19)

(cid:18)

+ m exp

−

(cid:18)

+ m exp

−

(cid:19)

(cid:19)

nλmin
4¯a2
nλmin
4¯a2

+ 2 (2N )m · exp

+ 2 (2N )m · exp

(cid:18)

(cid:18)

−

−

n(cid:15)2
2
n(cid:15)(cid:48)
2

(cid:19)

(cid:19)

.

With this probability bound, we can upper bound the L2 distance between p∗
n
Speciﬁcally, given that (cid:107)p∗
n − p∗(cid:107)2 ≤ ¯r
d ,

and p∗ by integration.

1
κ2

E(cid:107)p∗

n − p∗(cid:107)2

2 =

≤

(cid:90) ¯r2
d2

0
(cid:90) ¯r2
d2

P

(cid:18)

0

(cid:18) (cid:107)p∗

n − p∗(cid:107)2
2
κ2

(cid:19)

d(cid:15)(cid:48)

> (cid:15)(cid:48)

(cid:18)

2m exp

−

(cid:19)

n(cid:15)(cid:48)
2¯a2m

(cid:18)

+ m exp

−

(cid:19)

nλmin
4¯a2

+ 2 (2N )m · exp

(cid:19)(cid:19)

(cid:18)

−

n(cid:15)(cid:48)
2

∧ 1d(cid:15)(cid:48)

where y ∧ z = min{y, z} for y, z ∈ R. Next, we analyze the integral (31) term by term.

First, with (cid:15)(cid:48) = m log m

n

· ε,

(cid:90) ¯r2
d2

(cid:18)

(cid:18)

2m exp

−

(cid:19)(cid:19)

∧ 1d(cid:15)(cid:48)

n(cid:15)(cid:48)
2¯a2m
(cid:18)

2m exp

−

(cid:19)(cid:19)

ε log m
2¯a2

∧ 1dε

(cid:18)

(cid:18)

2

exp

log m −

(cid:19)(cid:19)

ε log m
2¯a2

(cid:114)

∧ 1dε ≤ c

m log m
n

.

0

≤

m log m
n

≤

m log m
n

(cid:90) ∞

(cid:18)

0
(cid:90) ∞

0

(31)

(32)

where c is dependent only on ¯a. The inequality on the last line is referred to Lemma 6 in the following

41

subsection.

Second,

(cid:90) ¯r2
d2

0

(cid:18)

m exp

−

(cid:19)

nλmin
4¯a2

d(cid:15)(cid:48) =

m¯r2
d2 exp

(cid:18)

−

nλmin
4¯a2

(cid:19)

≤ c(cid:48) m
n

(33)

where c(cid:48) is dependent only on ¯a, ¯r, d, and λmin.

Third, we can show that there exists constant c0 such that

2N ≤ c0

√

m log

(cid:19)

(cid:18) √
m
(cid:15)(cid:48)

from the deﬁnition of N in Proposition 4. Hence,

(cid:90) ¯r2
d2

0

(cid:18)

(cid:18)

1 ∧

2 exp

−

(cid:19)

n(cid:15)(cid:48)
2

· (2N )m

(cid:19)

d(cid:15)(cid:48) ≤

=

≤

(cid:90) ∞

0
(cid:90) ∞

0
(cid:90) ∞

0

(cid:18)

(cid:18)

1 ∧

2 exp

−

(cid:18)

(cid:18)

1 ∧

2 exp

−

(cid:18)

(cid:18)

1 ∧

2 exp

−

n(cid:15)(cid:48)
2
n(cid:15)(cid:48)
2
n(cid:15)(cid:48)
2

(cid:19)

(cid:18)

·

c0

√

m log

(cid:18) √
m
(cid:15)(cid:48)

(cid:19)(cid:19)m(cid:19)

d(cid:15)(cid:48)

(cid:18)

+ m log

c0

(cid:18)

+ m log

c0

√

√

m log

m log

(cid:18) √
m
(cid:15)(cid:48)
(cid:18) √
m
(cid:15)(cid:48)

(cid:19)(cid:19)(cid:19)(cid:19)

(cid:19)(cid:19)(cid:19)(cid:19)

d(cid:15)(cid:48)

d(cid:15)(cid:48)

Let (cid:15)(cid:48) = m log m log log n

n

· ε and use ε to replace (cid:15)(cid:48) in above. We have,

(cid:19)

· (2N )m

(cid:19)

d(cid:15)(cid:48)

(cid:90) ¯r2
d2

0

(cid:18)

1 ∧

2 exp

(cid:18)

−

n(cid:15)(cid:48)
2
(cid:90) ∞

≤

≤

m log m log log n
n
0
c(cid:48)(cid:48)m log m log log n
n

,

1 ∧

(cid:16)

2 exp

(cid:16)

−

ε
2

m log m log log n + m log

√

(cid:16)

c0

m log

(cid:17)(cid:17)(cid:17)(cid:17)

dε

(cid:16) n
ε

(34)

where c(cid:48)(cid:48) depends only on c0. The inequality in the last line is deferred to Lemma 7 in the following
subsection.

Combining (31), (32), (33) and (34), we conclude that

E(cid:107)p∗

n − p∗(cid:107)2

2 ≤ κ2(c + c(cid:48) + c(cid:48)(cid:48))

m log m log log n
n

holds for all n > m and P ∈ Ξ. From the concavity of the square root function, we know

E(cid:107)p∗

n − p∗(cid:107)2 ≤ κ

√

c + c(cid:48) + c(cid:48)(cid:48) ·

(cid:114)

m log m log log n
n

.

Thus we complete the proof.

A8.1 Two inequalities used in the proof of Theorem 1

We introduce two inequalities that will be used in the proof of Theorem 1. The proof for these two
inequalities are based on basic calculus. We use ∧ to denote the minimum operator, i.e., y∧z = min{y, z}.

Lemma 6. The inequality

holds for all m ≥ 2.

(cid:90) ∞

0

(exp (log m − x log m)) ∧ 1dx ≤ 2

42

Proof. We have

(cid:90) ∞

(exp (log m − x log m)) ∧ 1dx

0
(cid:90) 1

0
(cid:90) 1

0

=

≤

≤1 +

1dx +

(cid:90) ∞

1

(exp (log m − x log m)) ∧ 1dx +

(cid:90) ∞

1

(exp (log m − x log m)) ∧ 1dx

(cid:90) ∞

1

exp (log m − x log m) dx (Splitting the integral in two parts)

exp (log 2 · (1 − x)) dx ≤ 1 +

1
log 2

,

where in the last line the log m term is replaced with its lower bound log 2.

Lemma 7. The inequality

(cid:90) ∞

0

(cid:16)

exp

(cid:16)

1 ∧

−xm log m log log n + m log

(cid:16)√

m log

(cid:16) n
x

(cid:17)(cid:17)(cid:17)(cid:17)

dx ≤ 2

holds for all n ≥ max{m, 3} and m ≥ 2.

Proof. We have

(cid:90) ∞

(cid:16)

exp

(cid:16)

1 ∧

−xm log m log log n + m log

(cid:16)√

m log

(cid:17)(cid:17)(cid:17)(cid:17)

(cid:16) n
x
(cid:17)(cid:17)(cid:17)

dx

(cid:16) n
x

(cid:16)

exp

−xm log m log log n + m log

(cid:16)√

m log

dx (Splitting the integral in two parts)

exp (cid:0)−xm log m log log n + m log (cid:0)√

m log n(cid:1)(cid:1) dx (Using the fact that x > 1)

exp (−xm log m log log n + m log m log log n) dx

(cid:90) ∞

1
(cid:90) ∞

1
(cid:90) ∞

1

0

≤1 +

≤1 +

≤1 +

≤1 +

1
m log m log log n

≤ 3

where the last line follows the same argument as the last inequality in Lemma 6.

B Proof for Generic Regret Upper Bound

B1 Proof of Lemma 3

Proof. First, we show g(p∗) provides an upper bound for ER∗
n.



n
(cid:88)

ER∗

n = E





rjx∗
j



j=1


nd(cid:62)p∗

n +

= E


nd(cid:62)p∗ +

≤ E

n
(cid:88)

j=1

n
(cid:88)

j=1

= ng(p∗)

(cid:0)rj − a(cid:62)

j p∗
n

(cid:1)+


 (From the strong duality)

(cid:0)rj − a(cid:62)

j p∗(cid:1)+


 (From the optimality of p∗
n

)

(35)

where the expectation is taken with respect to (rj, aj)’s.

43

Then, by taking the diﬀerence between g(p∗) and g(p),

g(p∗) − g(p) = E

p∗(cid:105)
(cid:104)
rI(r > a(cid:62)p∗) + (cid:0)d − aI(r > a(cid:62)p∗)(cid:1)(cid:62)
= E (cid:2)(cid:0)r − a(cid:62)p∗(cid:1) (cid:0)I(r > a(cid:62)p∗) − I(r > a(cid:62)p)(cid:1)(cid:3)
= E (cid:2)(cid:0)a(cid:62)p∗ − r(cid:1) I(a(cid:62)p∗ ≥ r > a(cid:62)p)(cid:3) + E (cid:2)(cid:0)r − a(cid:62)p∗(cid:1) I(a(cid:62)p∗ < r ≤ a(cid:62)p)(cid:3) ≥ 0

− E

rI(r > a(cid:62)p) + (cid:0)d − aI(r > a(cid:62)p)(cid:1)(cid:62)

(cid:104)

p∗(cid:105)

where the expectation is taken with respect to (r, a). The last line is true because when the indicator
functions are positive, the terms go before the indicators must be non-negative accordingly. This proves
the maximum of g(p) is achieved at p∗. Furthermore, with a more careful analysis, we have

g(p∗) − g(p) = E (cid:2)(cid:0)a(cid:62)p∗ − r(cid:1) I(a(cid:62)p∗ ≥ r > a(cid:62)p)(cid:3) + E (cid:2)(cid:0)r − a(cid:62)p∗(cid:1) I(a(cid:62)p∗ < r ≤ a(cid:62)p)(cid:3)

≤ E (cid:2)(cid:0)a(cid:62)p∗ − a(cid:62)p(cid:1) I(a(cid:62)p∗ ≥ r > a(cid:62)p)(cid:3) + E (cid:2)(cid:0)a(cid:62)p − a(cid:62)p∗(cid:1) I(a(cid:62)p∗ < r ≤ a(cid:62)p)(cid:3)
= E (cid:2)(cid:0)a(cid:62)p∗ − a(cid:62)p(cid:1) (cid:0)P(r > a(cid:62)p∗|a) − P(r > a(cid:62)p|a)(cid:1) I(a(cid:62)p∗ > a(cid:62)p)(cid:3)

+ E (cid:2)(cid:0)a(cid:62)p − a(cid:62)p∗(cid:1) (cid:0)P(r > a(cid:62)p|a) − P(r > a(cid:62)p∗|a)(cid:1) I(a(cid:62)p∗ < a(cid:62)p)(cid:3)

≤ µE

(cid:104)(cid:0)a(cid:62)p∗ − a(cid:62)p(cid:1)2(cid:105)

≤ µ¯a2(cid:107)p∗ − p(cid:107)2
2

where the expectation is taken with respect to (r, a) ∼ P. Here the second line is because when a(cid:62)p∗ ≥
r > a(cid:62)p is true, we have a(cid:62)p∗ − r ≤ a(cid:62)p∗ − a(cid:62)p; the same for the second part of this line. The third
line comes from taking conditional expectation with respect to r. The fourth line applies Assumption 2
(b) and the last line applies the upper bound on a in Assumption 1 (b).

B2 Proof of Theorem 2

Proof. For any dual-based online policy π, its expected revenue

ERn(π) = E

= E

= E

= E

= E

(cid:35)

rtxt

rtxt + b(cid:62)

n p∗

(cid:35)

− E (cid:2)b(cid:62)

n p∗(cid:3)

(cid:34) n
(cid:88)

t=1
(cid:34) n
(cid:88)

t=1





n
(cid:88)

t=1
(cid:34) n
(cid:88)

t=1

(cid:34) τ¯a(cid:88)

t=1

(cid:32)

rtxt +

nd −

n
(cid:88)

t=1

(cid:33)(cid:62)


 − E (cid:2)b(cid:62)

n p∗(cid:3)

p∗

atxt

(By the deﬁnition of bn)

(cid:0)rtxt + d(cid:62)p∗ − a(cid:62)

t p∗xt

(cid:1)

(cid:0)rtxt + d(cid:62)p∗ − a(cid:62)

t p∗xt

(cid:1)

(cid:35)

(cid:35)

− E (cid:2)b(cid:62)

n p∗(cid:3)

(cid:34) n
(cid:88)

+ E

t=τ¯a+1

(cid:0)rtxt + d(cid:62)p∗ − a(cid:62)

t p∗xt

(cid:1)

(cid:35)

− E (cid:2)b(cid:62)

n p∗(cid:3)

(36)

where the expectation is taken with respect to (rt, at)’s.

We analyze the ﬁrst two terms in (36) separately. For the ﬁrst term in (36), it could be represented

44

by the Lagrangian function g(·) as in Lemma 3,

(cid:34) τ¯a(cid:88)

E

t=1

(cid:0)rtxt + d(cid:62)p∗ − a(cid:62)

j p∗xt

(cid:1)

(cid:35)

= E

(cid:34) n
(cid:88)

t=1

(cid:0)rtxt + d(cid:62)p∗ − a(cid:62)

t p∗xt

(cid:35)
(cid:1) I(τ¯a ≥ t)

n
(cid:88)

(a)
=

(b)
=

(c)
=

t=1
n
(cid:88)

t=1
n
(cid:88)

t=1

E (cid:2)(cid:0)rtxt + d(cid:62)p∗ − a(cid:62)

t p∗xt

(cid:1) I(τ¯a ≥ t)(cid:3)

E (cid:2)E (cid:2)(cid:0)rtxt + d(cid:62)p∗ − a(cid:62)

t p∗xj

(cid:1) I(τ¯a ≥ t)|bt−1, Ht−1

(cid:3)(cid:3)

E [g(pt)I(τ¯a ≥ t)]

(d)
= E

(cid:34) n
(cid:88)

t=1

(cid:35)

g(pt)I(τ¯a ≥ t)

= E

(cid:35)

g(pt)

(cid:34) τ¯a(cid:88)

t=1

(37)

where pt’s are the dual price vectors speciﬁed by the policy π and the expectation is taken with respect
to (rt, at)’s. Here (a) and (d) come from the exchange of summation and expectation. (b) comes from
nesting a conditional expectation. (c) is from two facts: ﬁrst, on the event τ¯a ≥ t, the remaining inventory
bt−1 (at the end of time period t − 1) is enough to satisfy the t-th order; second, the dual-based policy
is adopting the price vector pt in deciding the value of xt, i.e., xt = I(rt > a(cid:62)

t pt).

For the second term in (36), we show that it is lower bounded by −E[n − τ¯a] up to some constant.
from Proposition 1 and (cid:107)at(cid:107)2 ≤ ¯a from Assumption 1. Combining these two

We know that (cid:107)p∗(cid:107) ≤ ¯r
d
facts,

(cid:34) n
(cid:88)

E

t=τ¯a+1

(cid:0)rtxt + d(cid:62)p∗ − a(cid:62)

t p∗xt

(cid:1)

(cid:35)

≥ E

(cid:34) n
(cid:88)

t=τ¯a+1

(cid:35)

(cid:0)rtxt − a(cid:62)

t p∗xt

(cid:1)

≥ −E[n − τ¯a] ·

(cid:18)

¯r +

(cid:19)

.

¯r¯a
d

where the ﬁrst lines comes from the fact that d(cid:62)p∗ ≥ 0.

Plugging (37) and (38) into (36), we obtain

ERn(π) ≥ E


 − E[n − τ¯a] ·

(cid:18)

¯r +

g(pj)


τ¯a(cid:88)


j=1

(cid:34)

− E

(cid:19)

¯r¯a
d

¯r
d

·

(cid:88)

i∈IB

(cid:35)

bin

.

(38)

(39)

To obtain an upper bound on the regret, we simply take the diﬀerence between ER∗
n

and (39), and

then apply Lemma 3 for an upper bound on ER∗
n

,

ER∗

n − ERn(π) ≤ E

µ¯a2(cid:107)pj − p∗(cid:107)2
2


 + E[n − τ¯a] ·

(cid:18)

¯r +


τ¯a(cid:88)


j=1

(cid:34)

+ E

(cid:19)

¯r¯a
d

¯r
d

·

(cid:88)

i∈IB

(cid:35)

bin

holds for all n > 0 and distribution P ∈ Ξ. By choosing

K = max

(cid:26)

µ¯a2, ¯r +

(cid:27)

,

¯r¯a
d

,

¯r
d

we ﬁnish the proof.

45

B3 Proof of Corollary 1

Proof. From the proof of Theorem 2, the role that the stopping time τ¯a plays is to guarantee the orders
coming before τ¯a can always be satisﬁed. When P(τ ≤ τ¯a) = 1, the stopping time τ has the same
property. Therefore, the derivations in the proof of Theorem 2 still hold for τ .

C Regret Analyses for OLP Algorithms

C1 Proof of Theorem 3

Proof. The proof of Theorem 3 builds upon the generic regret upper bound in Theorem 2.

For the ﬁrst part in the generic upper bound in Theorem 2, since we apply p∗ as the decision

rule, i.e., pt = p∗,

(cid:34) n
(cid:88)

E

t=1

(cid:35)

(cid:107)pt − p∗(cid:107)2
2

= 0.

(40)

So, we only need to focus on E [n − τ¯a] and E (cid:2)(cid:80)

i∈IB

(cid:3) . Deﬁne

bin

τ i
¯a = min{n} ∪






t ≥ 1 :

t
(cid:88)

j=1

aijI(rj > a(cid:62)

j p∗) > ndi − ¯a






.

From the optimality condition on p∗, we know that the expected constraint consumption of Algorithm
1 under the dual price p∗ has an upper bound





t
(cid:88)

E

aijI(rj > a(cid:62)

 ≤ tdi


j p∗)

j=1

for i = 1, ..., m and t = 1, ..., n. That is, ∇f (p∗) ≥ 0, and this can be derived from the proof of
Proposition 2. In addition, the variance of the constraint consumption has a trivial upper bound due to

the independence across diﬀerent time periods,





t
(cid:88)

aijI(rj > a(cid:62)

j p∗)


 ≤ ¯a2t.

Var

for i = 1, ..., m and t = 1, ..., n. In the following, we use these two upper bounds to derive upper bounds
for the second and third part of the generic upper bound in Theorem 2.

j=1

46

For the second part of the generic upper bound,

E[n − τ i

¯a] ≤

=

n
(cid:88)

t=1

n
(cid:88)

P(τ i

¯a ≤ t)





t
(cid:88)

P

aijI(rj > a(cid:62)

j p∗) ≥ ndi − ¯a





aijI(rj > a(cid:62)

j p∗) − E





t
(cid:88)





aijI(rj > a(cid:62)

j p∗)

 ≥ (n − t)di − ¯a



t=1

j=1

n
(cid:88)

P

≤





t
(cid:88)

j=1

¯a2t
((n − t)di − ¯a)2

(cid:19)

∧ 1 (Applying the Chebyshev’s inequality)

t=1

j=1

n0(cid:88)

(cid:18)

≤n − n0 +

(cid:18)

≤

2 +

t=1
(cid:19) √

n

¯a
di

where n0 = (cid:98)n − ¯a
di

(cid:99). We refer the last line to Lemma 8 in the following subsection. Therefore,

E[n − τ¯a] = E[max

{n − τ i

¯a}]

i

E[n − τ i
¯a]

≤

m
(cid:88)

i=1
(cid:18)

≤

2 +

(cid:19)

√

m

n

¯a
d

(41)

and the second line comes from a replacement
where the ﬁrst line comes from the deﬁnition of τ¯a and τ i
¯a
of the maximum of m (non-negative) random variables with an upper bound of their summation. Next,
for the third part of the generic upper bound,





E[bin] = E



ndi −

n
(cid:88)

j=1

aijI(rj > a(cid:62)

j p∗)





+
 ≤ E


(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

ndi −

n
(cid:88)

j=1

aijI(rj > a(cid:62)

(cid:12)
(cid:12)
j p∗)
(cid:12)
(cid:12)
(cid:12)





= E


(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

n
(cid:88)

j=1

(cid:0)di − aijI(rj > a(cid:62)

j p∗)(cid:1)





(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

E

≤


(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

n
(cid:88)

j=1

(cid:0)di − aijI(rj > a(cid:62)

j p∗)(cid:1)

2



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

Var

=

√

≤ ¯a

n





n
(cid:88)



aijI(rj > a(cid:62)

j p∗)



j=1

(42)

holds for all i ∈ IB and n > 0. The second line comes from absorbing the term ndi into the summation,
j p∗)] for a binding constraint i, and the last
the fourth line comes from the fact that di = E[aijI(rj > a(cid:62)
line comes from the independence between diﬀerent time periods. Combining (40), (41), and (42) with

Theorem 2, we complete the proof.

47

C1.1

Inequality in the Proof of Theorem 3

Lemma 8. The following inequality holds

n − n0 +

n0(cid:88)

(cid:18)

t=1

¯a2t
((n − t)di − ¯a)2

(cid:19)

(cid:18)

∧ 1 ≤

2 +

(cid:19) √

n

¯a
di

where n0 = (cid:98)n − ¯a
di

(cid:99).

Proof. We have

n − n0 +

=n − n0 +

n0(cid:88)

(cid:18)

(cid:32)

t=1

n0(cid:88)

t=1

√

≤

n + 1 +

√

≤

n + 1 +

√

≤

n + 1 +

(cid:98)n−

√
(cid:88)

t=1
√
(cid:88)

(cid:98)n−

t=1
(cid:90) n−

√

n

0

¯a2t
((n − t)di − ¯a)2

(cid:19)

∧ 1

(cid:33)

∧ 1

(cid:33)

n(cid:99)

(cid:32)

t
¯a − 1(cid:1)2
(cid:0)(n − t) di
t
¯a − 1(cid:1)2
(cid:0)(n − t) di
(cid:33)

(cid:32)

n(cid:99)

2t
(cid:0)(n − t) di

¯a

(cid:1)2

2x
(cid:0)(n − x) di
(cid:90) n−

¯a
n

√

(cid:1)2 dx

√

≤

√

=

n + 1 + (n −

√

n) ·

n + 1 + (n −

0

¯a
√

≤

n

√

n)

di

(cid:1)2 dx

1
(cid:0)(n − x) di
¯a
(cid:19) √
(cid:18)
¯a
di

2 +

n.

C2 Proof of Theorem 4

Proof. We prove the theorem by using the results in Theorem 2 and Corollary 1. The proof is similar

but more complicated than Theorem 3, because the dual price is computed based on SAA in Algorithm

2. Speciﬁcally, we analyze the three parts in the generic regret upper bound separately.

First, deﬁne

τ i
¯a = min{n} ∪






t ≥ 1 :

t
(cid:88)

j=1

aijI(rj > a(cid:62)

j pj) > ndi − ¯a






where pj’s are speciﬁed by Algorithm 2. Here the stopping time τ i
¯a
process under policy π1. In this way,

is associated with the constraint

τ¯a = min

i

τ i
¯a.

From the dual convergence result in Theorem 1, we know that there exists a constant C, such that

E(cid:107)ptk − p∗(cid:107)2

2 ≤

Cm
tk

log log tk

holds for all k ≥ 1 and distribution P ∈ Ξ. Here pk is the dual price used in Algorithm 2 and tk appears
on the right-hand-side instead of k because Algorithm 2 updates the dual price only in periods tk’s.

48

First, for the ﬁrst part of the generic regret bound,

n
(cid:88)

j=1

L−1
(cid:88)

E(cid:107)pj − p∗(cid:107)2
2

tk+1
(cid:88)

E(cid:107)ptk − p∗(cid:107)2
2

k=1

t=tk+1

L−1
(cid:88)

(tk+1 − tk) ·

Cm
tk

log log tk

E


τ¯a(cid:88)


j=1



(cid:107)pj − p∗(cid:107)2
2

 ≤

≤

≤

≤

k=1

L−1
(cid:88)

k=1

Cm(δk+1 − δk + 1)
δk

log log n (Plugging in tk’s value)

= 2Cm(δ − 1)L log log n ≤ 3Cm log n log log n,

(43)

where the last line comes from the fact that δ ∈ (1, 2] and n = (cid:98)δL(cid:99). As we will see shortly, the
contribution of this ﬁrst part is daunted by the later two parts.

Next, we analyze the second part in the generic regret bound – the stopping time τ¯a. Speciﬁ-

cally, we consider the constraint process bit. From the deﬁnition of τ i
¯a,

(cid:8)τ i

¯a ≤ t(cid:9) =




t(cid:48)
(cid:88)



j=1

aijI(rj > a(cid:62)

j pj) ≥ ndi − ¯a for some 1 ≤ t(cid:48) ≤ t






(44)

where pj’s are speciﬁed by Algorithm 2. To obtain an upper bound of E[n − τ i
¯a], we only need to analyze
the probability of the event on the right hand side. Notice that from the optimality condition of the

stochastic programming problem,

t
(cid:88)

j=1

E (cid:2)aijI(rj > a(cid:62)

j p∗)(cid:3) ≤ tdi

where the expectation is taken with respect to (rj, aj) ∼ P. The equality holds for the binding constraints
(where the binding and non-binding constraints are deﬁned according to the stochastic program (7)). It
tells that if we apply the dual price p∗, then the constraints will not be exhausted until the last step.
The idea is to use the fact that pj’s are close to p∗ to show that if we apply the dual price pj’s, the
constraints will also not be exhausted until the very end of the horizon.

Deﬁne function

We know

g0(p) = E (cid:2)aijI(rj > a(cid:62)

j p)(cid:3) .

|g0(p) − g0(p∗)| = |E (cid:2)aijI(rj > a(cid:62)
= |E (cid:2)aijP(rj > a(cid:62)
≤ E (cid:2)|aijP(rj > a(cid:62)
≤ ¯aE (cid:2)|P(rj > a(cid:62)
≤ ¯a2µ(cid:107)p − p∗(cid:107)2

j p)(cid:3) − E (cid:2)aijI(rj > a(cid:62)
j p|aj) − aijP(rj > a(cid:62)
j p|aj) − aijP(rj > a(cid:62)

j p∗)(cid:3) |
j p∗|aj)(cid:3) |
j p∗|aj)|(cid:3)

j p|aj) − P(rj > a(cid:62)

j p∗|aj)|(cid:3)

(45)

for any p ∈ Ωp and distribution P ∈ Ξ. The parameter µ in the last line comes from Assumption 2 (b).
The above inequality states that the diﬀerence in terms of constraint consumption is upper bounded by

the diﬀerence between dual prices (up to a constant factor).

49

The expectation of i-th constraint consumption up to time t under Algorithm 2,

E





t
(cid:88)

j=1



aijI(rj > a(cid:62)

j pj)

 =

≤

t
(cid:88)

j=1

t
(cid:88)

j=1

≤ ¯a2µ

= ¯a2µ

≤ ¯a2µ

E (cid:2)aijI(rj > a(cid:62)

j pj)(cid:3)

(cid:0)E (cid:2)aijI(rj > a(cid:62)

j pj)(cid:3) − E (cid:2)aijI(rj > a(cid:62)

j p∗)(cid:3)(cid:1) + tdi

t
(cid:88)

j=1

L
(cid:88)

E(cid:107)pj − p∗(cid:107)2 + tdi

tk+1
(cid:88)

E(cid:107)pj − p∗(cid:107)2I(j ≤ t) + tdi

k=1

j=tk+1

L
(cid:88)

tk+1
(cid:88)

√
C
√

m
tk

(cid:112)log log tkI(j ≤ t) + tdi

k=1
√
≤ 5C¯a2µ

m

j=tk+1
√

t(cid:112)log log t + tdi,

(46)

for i = 1, ..., m and t = 1, ..., n. The constant C is the coeﬃcient of the dual convergence from Theorem
1. Here the second line comes from plugging in the feasibility of p∗ for the stochastic program. The
third line comes from the analysis of the function g0(p). The ﬁfth line comes from the dual convergence
result. The detailed derivation of the last line is defered to Lemma 9 in the following subsection.
The variance of i-th constraint consumption up to time t has the following decomposition,





t
(cid:88)

aijI(rj > a(cid:62)

j pj)


 = E

Var

j=1





t
(cid:88)

aijI(rj > a(cid:62)

j pj) −

E (cid:2)aijI(rj > a(cid:62)

j pj)|pj


2
(cid:3)


t
(cid:88)

j=1

j=1


+ Var



t
(cid:88)



E[aijI(rj > a(cid:62)

j pj)|pj]

 .

(47)

This is because, for two random variables X1 and X2, we have

j=1

Var[X1] = E[X1 − EX1]2

= E [X1 − E[X1|X2] + E[X1|X2] − EX1]2
= E [X1 − E[X1|X2]]2 + Var[E[X1|X2]].

Let Zj := aijI(rj > a(cid:62)
sequence adapted to {Ht}n
Speciﬁcally, we have

j pj) − E (cid:2)aijI(rj > a(cid:62)

t=1. Recall that Ht is deﬁned as the σ-algebra generated by {(rj, aj)}t

(cid:3) . It is easy to see that Zj’s is a martingale diﬀerence
j=1.

j pj)|pj

Then, for the ﬁrst term in (47),

E[Zj] < ∞ and E[Zj|Hj−1] = 0.

E





t
(cid:88)

j=1

aijI(rj > a(cid:62)

j pj) −

E (cid:2)aijI(rj > a(cid:62)

j pj)|pj


2
(cid:3)


t
(cid:88)

j=1

=

t
(cid:88)

j=1

E (cid:2)aijI(rj > a(cid:62)

j pj) − E (cid:2)aijI(rj > a(cid:62)

j pj)|pj

(cid:3)(cid:3)2

≤ ¯a2t.

(48)

50

For the second term in (47),





t
(cid:88)

E[aijI(rj > a(cid:62)

j pj)|pj]


 ≤ E





t
(cid:88)

E[aijI(rj > a(cid:62)

j pj)|pj] −

Var

j=1

j=1


¯a2µ

≤ E

t
(cid:88)

j=1


2

(cid:107)pj − p∗(cid:107)2



≤ C¯a4µ2mt log t log log t

E[aijI(rj > a(cid:62)


2
j p∗)]


t
(cid:88)

j=1

(49)

where the last line is referred to Lemma 10 in the following subsection and the constant C is the coeﬃcient
of the dual convergence from Theorem 1. Putting together (48) and (49),





t
(cid:88)

aijI(rj > a(cid:62)

j pj)


 ≤ ¯a2t + C¯a4µ2mt log t log log t,

(50)

Var

j=1

for i = 1, ..., m and t = 1, ..., n.

To summarize, (46) and (50) provide upper bounds for the expectation and variance of the constraint

consumption under Algorithm 2. With these two bounds, we can proceed to analyze the right-hand-side

of (44),





t(cid:48)
(cid:88)

aijI(rj > a(cid:62)

j pj) ≥ ndi − ¯a for some 1 ≤ t(cid:48) ≤ t





P

j=1

(cid:32) t(cid:48)
(cid:88)

= P

j=1

aijI(rj > a(cid:62)

j pj) − E

for some 1 ≤ t(cid:48) ≤ t

(cid:33)

(cid:32) t(cid:48)
(cid:88)

≤ P

j=1

aijI(rj > a(cid:62)

j pj) − E

for some 1 ≤ t(cid:48) ≤ t

(cid:33)

t(cid:48)
(cid:88)





aijI(rj > a(cid:62)

j pj)


 ≥ ndi − ¯a − E

t(cid:48)
(cid:88)







aijI(rj > a(cid:62)

j pj)



j=1

j=1

aijI(rj > a(cid:62)

j pj)


 ≥ (n − t)di − ¯a − 5C¯a2µ

√

t(cid:48)
(cid:88)





j=1

√

m

t(cid:112)log log t

(51)

where the last line comes from plugging in the upper bound (46) of the expected constraint consumption.

We can view the process

Mt =

t
(cid:88)

j=1

aijI(rj > a(cid:62)

j pj) − E





t
(cid:88)



aijI(rj > a(cid:62)

j pj)



j=1

as a martingale adapted to the ﬁltration Ht generated by {(rj, aj)}t

j=1

. Applying Doob’s martingale

51

inequality, when (n − t)di − ¯a − 5C¯a2µ

√

√

√

t

m

log log t > 0, we have,





P

t(cid:48)
(cid:88)

aijI(rj > a(cid:62)

j pj) ≥ ndi − ¯a for some 1 ≤ t(cid:48) ≤ t





j=1
(cid:16)

≤ P

Mt(cid:48) ≥ (n − t)di − ¯a − 5C¯a2µ

t(cid:112)log log t for some 1 ≤ t(cid:48) ≤ t

(cid:17)

√

√

m
(cid:105)

(cid:104)(cid:80)t

Var

(cid:0)(n − t)di − ¯a − 5C¯a2µ

j=1 aijI(rj > a(cid:62)
j pj)
√
√
t

m

√

log log t(cid:1)2

¯a2t + C¯a4µ2mt log t log log t

(cid:0)(n − t)di − ¯a − 5C¯a2µ

√

√

√
t

log log t(cid:1)2 .

m

≤

≤

where the second line is (51), the third line applies Doob’s Martingale inequality (Revuz and Yor, 2013)

and the fourth line plugs in the variance upper bound (50).

Then, we complete the analysis of the second part of the generic upper bound,

E[n − τ i

¯a] ≤

=

n
(cid:88)

t=1

n
(cid:88)

P(τ i

¯a ≤ t)





t(cid:48)
(cid:88)

aijI(rj > a(cid:62)

j pj) ≥ ndi − ¯a for some 1 ≤ t(cid:48) ≤ t





P

t=1

j=1

(cid:32)

n0(cid:88)

≤n − n0 +

≤C (cid:48)√

√

m

t=1

n log n

¯a2t + C¯a4µ2mt log t log log t

(cid:0)(n − t)di − ¯a − 5C¯a2µ

√

√

√

m

t

log log t(cid:1)2

(cid:33)

∧ 1

for some constant C (cid:48) dependent on ¯a, d, ¯d, µ and C. Here n0 is the largest index t such that (n − t)di −
√
log log t > 0,. We refer the derivation of the last line to Lemma 11 in the following
¯a − 5C¯a2µ
t
subsection. Then,

m

√

√

E[n − τ¯a] = E[max

{n − τ i

¯a}]

i

m
(cid:88)

≤

E[n − τ i
¯a]

i=1
≤ C (cid:48)m

3
2

√

n log n.

(52)

where the ﬁrst two lines follow the same argument as (41) in Theorem 3. The last thing is the third
part of the generic regret bound – E [bin] for i ∈ IB. Indeed,



bin =

ndi −

n
(cid:88)

j=1



+

aijI(rj > a(cid:62)

j pj)



≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ndi −

n
(cid:88)

j=1

aijI(rj > a(cid:62)

(cid:12)
(cid:12)
(cid:12)
j pj)
(cid:12)
(cid:12)

.

52

Therefore,

E [bin] ≤ E


(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

ndi −

n
(cid:88)

j=1

aijI(rj > a(cid:62)

j pj)





(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

E


(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤

ndi −

n
(cid:88)

j=1

aijI(rj > a(cid:62)

(cid:12)
(cid:12)
(cid:12)
j pj)
(cid:12)
(cid:12)

2



(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)


E

=



ndi −

n
(cid:88)

j=1




2



aijI(rj > a(cid:62)

j pj)





+ Var

ndi −

n
(cid:88)

j=1



aijI(rj > a(cid:62)

j pj)



(cid:112)

≤

25C 2¯a4µ2m2n log log n + ¯a2n + C¯a4µ2mn log n log log n

≤ 6C¯a2µm

√

n(cid:112)log log n

(53)

where the second line comes from Cauchy-Schwartz inequality and the fourth line comes from plugging

in the upper bounds on expectation and variance of constraint consumption, namely, (46) and (50).

Combining the three inequalities (43), (52), and (53) which correspond to the three parts in the upper

bound from Theorem 2, we complete the proof.

C2.1

Inequalities in the Proof of Theorem 4

Lemma 9. The following inequality holds for 3 ≤ t < n,

L
(cid:88)

tk+1
(cid:88)

k=1

j=tk+1

1
√
tk

(cid:112)log log tkI(j ≤ t) ≤ 5

√

t(cid:112)log log t.

Proof. Let L(cid:48) be the smallest integer such that δL(cid:48)

> t. By its deﬁnition, δL(cid:48)−1 ≤ t < δL(cid:48)

.

L
(cid:88)

tk+1
(cid:88)

k=1

j=tk+1

L(cid:48)
(cid:88)

tk+1
(cid:88)

k=1

j=tk+1

1
√
tk

1
√
tk

(cid:112)log log tkI(j ≤ t)

(cid:112)log log t

≤

=

=

=

L(cid:48)
(cid:88)

k=1
L(cid:48)
(cid:88)

k=1
L(cid:48)
(cid:88)

tk+1 − tk√
tk

(cid:112)log log t

δ − 1
δk/2

(cid:112)log log t

(Plugging in the value of tk’s)

(δ − 1)δk/2(cid:112)log log t

k=1

=δ1/2(δ − 1)

≤δ(δ1/2 + 1)

√

≤5

t(cid:112)log log t

(cid:112)log log t

δL(cid:48)/2 − 1
δ1/2 − 1
√
t(cid:112)log log t

(Using the fact that δL(cid:48)−1 ≤ t)

where the last line comes from the choice of δ ∈ (1, 2].

53

Lemma 10. The following inequality holds for 3 ≤ t ≤ n,





t
(cid:88)


2

(cid:107)pj − p∗(cid:107)2



≤ Cmt log t log log t

E

where C is the coeﬃcient of dual convergence in Theorem 1.

j=1

We have

E





t
(cid:88)


2

(cid:107)pj − p∗(cid:107)2



≤tE





t
(cid:88)



(cid:107)pj − p∗(cid:107)2
2



j=1

j=1

(Use the dual convergence result)

L
(cid:88)

tk+1
(cid:88)

≤t

k=1

j=tk+1

Cm
tk

log log tkI(j ≤ t)

(Use the same method as Lemma 9)

≤t

=t

=t

L(cid:48)
(cid:88)

tk+1
(cid:88)

k=1

j=tk+1

Cm
tk

log log tk

Cm(tk+1 − tk)
tk

log log tk

Cm(δ − 1) log log tk

L(cid:48)
(cid:88)

k=1
L(cid:48)
(cid:88)

k=1

≤Cmt log t log log t

where L(cid:48) is the smallest integer such that δL(cid:48)

> t. By this deﬁnition, δL(cid:48)−1 ≤ t < δL(cid:48)

.

Lemma 11. The following inequality holds

n − n0 +

(cid:32)

n0(cid:88)

t=1

mt log t log log t
√

√

√
t

log log t(cid:1)2

(cid:0)n − t −

m

(cid:33)

∧ 1 ≤6(cid:112)mn log n log log n

where n0 is the largest index t such that n − t −

√

√

m

√

t

log log t > 0.

Proof. First, we choose n1 = n − 3
generality, we assume n1 > 0.

√

mn log n log log n. It is easy to show that n0 < n1. Without loss of

(cid:32)

n0(cid:88)

n − n0 +

mt log t log log t
√

√

√

(cid:0)n − t −

m

t

log log t(cid:1)2

(cid:33)

∧ 1

≤n − n1 +

t=1

n1(cid:88)

t=1

mt log t log log t
√

√

√
t

log log t(cid:1)2

(cid:0)n − t −

m

n1(cid:88)

t=1

1
√

√

m

(cid:0)n − t −
3
mn log n log log n

√

log log t(cid:1)2

t

≤3(cid:112)mn log n log log n + mn log n log log n

≤3(cid:112)mn log n log log n + mn log n log log n

√

≤6(cid:112)mn log n log log n.

54

C3 Proof of Theorem 5

Proof. As the proof of Theorem 3 and Theorem 4, we utilize again the generic upper bound in Theorem
2. We ﬁrst deﬁne the stochastic process of the constraint consumption. Deﬁne dt = (d1,t, ..., dm,t)(cid:62)
where

dit :=

bit
n − t

as the remaining resource per period after the end of t-th period, for i = 1, ..., m and t = 1, ..., n − 1. The
reason that we analyze the process dt instead of the original process bt is that a more careful analysis is
required in the proof. Speciﬁcally, we deﬁne d0 = d.

Throughout this proof, we will reserve the vector d to denote the initial average resource and use ˜d
and d(cid:48) to denote a general vector in Ωd. Assumption 3 states uniform conditions on ˜d ∈ Ωd. In Lemma
12 and Lemma 13, we ﬁrst build on Assumption 3 and establish the existence of δd > 0 such that
D := (cid:78)m
i=1[di − δd, di + δd] and for all ˜d ∈ D, the stochastic program f ˜d(p) speciﬁed with ˜d shares
the same binding and non-binding sets (IB and IN ) with the original d. In comparison, Assumption 3
deﬁnes a set Ωd, and here D ⊂ Ωd is a set of d(cid:48) that not only satisﬁes Assumption 3 but also shares the
same binding and non-binding dimensions with the initial d.

The property of sharing binding/non-binding dimensions will create great convenience in this proof.

Intuitively, the existence of δd is due to the continuity of

fd(cid:48)(p) := d(cid:48)(cid:62)p + E(r,a)∼P

(cid:2)(r − a(cid:62)p)+(cid:3)

with respect to d(cid:48) together with Assumption 3 (b). We emphasize that the constant δd is pertaining to
the stochastic program and it is not dependent on n. The statement and the proof of Lemma 12 and
Lemma 13 are deferred to the following subsection.

Now, we deﬁne a stopping time τ according to D. As we will see, the deﬁnition of D guarantees that
for t < τ , the binding and non-binding dimensions will not switch and this creates great convenience for
the analysis.

Deﬁne

(cid:26)

τ = min

n − (cid:100)

(cid:27)

¯a
d

(cid:101)

∪ {t ≥ 0 : dt /∈ D},

where (cid:100)·(cid:101) represents the ceiling function.
Intuitively, τ is the ﬁrst time that the binding/nonbinding
structure of the problem may be changed, i.e., some binding constraints become non-binding or some
non-binding constraints become binding. To put it in another way, it means for the i-th constraint, the
average remaining resource level dit deviates from the initial di by a constant. This is more strict than
the stopping time τ¯a deﬁned earlier in Theorem 2. Comparatively, τ¯a denotes the time that a certain type
of constraint is almost exhausted while τ here characterizes the time that a certain type of constraint
deviates certain amount from its original level di. Since d is a lower bound for all ˜d ∈ Ωd, the deﬁnition
ensures that τ ≤ τ¯a.

We ﬁrst work on the second part of the generic upper bound and derive an upper bound for

E[n − τ ]. Deﬁne

d(cid:48)
it =




dit,



d(cid:48)
i,t−1,

if t ≤ τ

if t > τ

t=1

can be interpreted as an auxiliary process associated
for i = 1, ..., m and t = 1, ..., n. The process {d(cid:48)
and it freezes the value of dit from the time τ. The motivation for deﬁning this auxiliary
with {dit}n
process is that the original process dit may behave irregularly after time τ (the binding and non-binding
dimensions switch), but the new process d(cid:48)
remains the same value after time τ . In such a way, we
it
separate the eﬀect of irregularity across diﬀerent constraints and single out one constraint for analysis.

it}n

t=1

55

Since our objective here is to analyze E[τ ] and the two processes take the same value before time τ , so
it makes no diﬀerence to study the more “regular” process d(cid:48)
it

.

As in the proof of Theorem 4, we deﬁne a stopping time for each constraint,

(cid:26)

τi = min

n − (cid:100)

(cid:27)

¯a
d

(cid:101)

∪ {t ≥ 1 : d(cid:48)

it /∈ [di − δd, di + δd]} .

It is easy to see that τ = mini τi, so we only need to study τi and E[n − τi].

Deﬁne ˜p∗

t+1

be the optimal solution to the following optimization problem

min fdt(p) := d(cid:62)
s.t. p ≥ 0,

t p + E (cid:2)(r − a(cid:62)p)+(cid:3)

(54)

where dt = (d1,t, ..., dm,t)(cid:62). The problem (54) is diﬀerent from the original stochastic program (7) in
terms of d. The speciﬁcation of dt makes the stochastic program (54) correspond to the SAA problem
solved at time t in Algorithm 3. Assumption 3 ensures that the dual convergence result in Theorem 1
extends to pt+1 (the dual price used in Algorithm 3) and ˜p∗

t+1.

Now, we analyze the dynamics of d(cid:48)
it

. With the execution of Algorithm 3, we have

bt+1 = bt − at+1I(rt+1 > a(cid:62)

t+1pt+1)

for t = 0, 1, ..., n − 1. Normalizing both sides,

i,t+1 = d(cid:48)
d(cid:48)

itI(τ < t) +

(n − t)dit − ai,t+1I(rt+1 > a(cid:62)

t+1pt+1)

= d(cid:48)

itI(τ < t) + ditI(τ > t) +

n − t − 1
dit − ai,t+1I(rt+1 > a(cid:62)

t+1pt+1)

I(τ ≥ t)

n − t − 1

I(τ ≥ t),

for t = 0, ..., n − 1 and i = 1, ..., m.

Since τ is deﬁned by the deviation from di, we can take oﬀ di on both sides,

i,t+1 − di = (d(cid:48)
d(cid:48)

it − di)I(τ < t) + (dit − di)I(τ ≥ t) +

dit − ai,t+1I(rt+1 > a(cid:62)

t+1pt+1)

n − t − 1

= (d(cid:48)

it − di)I(τ < t) + (dit − di)I(τ ≥ t) +

ai,t+1

+

(cid:0)I(rt+1 > a(cid:62)

t+1 ˜p∗

t+1) − I(rt+1 > a(cid:62)

t+1pt+1)(cid:1)

n − t − 1

n − t − 1

I(τ ≥ t).

dit − ai,t+1I(rt+1 > a(cid:62)

t+1 ˜p∗

t+1)

I(τ ≥ t)

I(τ ≥ t)

Taking square for both sides and take expectation,

E

(cid:104)(cid:0)d(cid:48)

i,t+1 − di

(cid:1)2(cid:105)

= E (cid:2)(d(cid:48)

it − di)2I(τ < t)(cid:3) + E (cid:2)(dit − di)2I(τ ≥ t)(cid:3)
(cid:34) (cid:0)dit − ai,t+1I(rt+1 > a(cid:62)
(n − t − 1)2

t+1)(cid:1)2

t+1 ˜p∗

I(τ ≥ t)

(cid:35)

(cid:34) (cid:0)ai,t+1I(rt+1 > a(cid:62)

t+1 ˜p∗

t+1) − ai,t+1I(rt+1 > a(cid:62)
(n − t − 1)2

t+1pt+1)(cid:1)2

(cid:35)

I(τ ≥ t)

(cid:34) (cid:0)ai,t+1I(rt+1 > a(cid:62)

t+1 ˜p∗

t+1) − ai,t+1I(rt+1 > a(cid:62)

t+1pt+1)(cid:1) (dit − di)

t+1 ˜p∗

t+1) − ai,t+1I(rt+1 > a(cid:62)

t+1pt+1)(cid:1) (cid:0)dit − ai,t+1I(rt+1 > a(cid:62)

t+1 ˜p∗

t+1)(cid:1)

n − t − 1

+ 2E
(cid:34) (cid:0)ai,t+1I(rt+1 > a(cid:62)

+2E

+ E

+ E

I(τ ≥ t)

(cid:35)

(cid:35)

I(τ ≥ t)

.

(55)

(n − t − 1)2

56

where the expectation is taken with respect to the online process, i.e., (rt, at)’s. Here the cross terms
that contain both I(τ < t) and I(τ ≥ t) will cancel out because these two events are exclusive to each
other.

We analyze (55) separately for binding and non-binding dimensions. We emphasize that binding and
non-binding dimensions are deﬁned according to the original (initial) d but extend to all ˜d ∈ D. For
binding dimensions (i ∈ IB), the following cross term disappears

2E

(cid:34) (cid:0)dit − ai,t+1I(rt+1 > a(cid:62)
n − t − 1

t+1 ˜p∗

t+1)(cid:1) (dit − di)

(cid:35)

I(τ ≥ t)

(cid:34)

=2E

E

(cid:34) (cid:0)dit − ai,t+1I(rt+1 > a(cid:62)
n − t − 1

t+1 ˜p∗

t+1)(cid:1) (dit − di)

dit − ai,t+1I(rt+1 > a(cid:62)

t+1 ˜p∗

t+1)

n − t − 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:34)

(cid:34)

=2E

E

=0.

(cid:12)
(cid:12)
(cid:12)
I(τ ≥ t)
(cid:12)
(cid:12)

(cid:35)

(cid:35)(cid:35)

d1,t, ..., dm,t

(cid:35)

d1,t, ..., dm,t

(dit − di)I(τ ≥ t)

The last line is because the deﬁnition of τ ensures that the binding and non-binding dimensions will not
switch before time τ and therefore, for the binding dimensions, we always have

dit = E(r,a)∼P [aiI(r > a(cid:62) ˜p∗

t+1)|dt]

due to the optimality condition of the stochastic program (54) and the deﬁnition of ˜p∗
it means the stochastic program will output a solution ˜p∗
sumption under ˜p∗

so that the future average constraint con-
is always equal to the current average constraint level dit for binding dimensions.

. Intuitively,

t+1

t+1

t+1

Back to (55), we analyze the right hand side term by term.

1) Combine two terms,

E (cid:2)(d(cid:48)

it − di)2I(τ < t)(cid:3) + E (cid:2)(dit − di)2I(τ ≥ t)(cid:3) = E (cid:2)(d(cid:48)

it − di)2(cid:3)2

.

2) Utilize the upper bound on di’s and aj’s,

E

(cid:34) (cid:0)dit − ai,t+1I(rt+1 > a(cid:62)
(n − t − 1)2

t+1 ˜p∗

t+1)(cid:1)2

(cid:35)

I(τ ≥ t)

≤

( ¯d + ¯a)2
(n − t − 1)2 .

3) The following cross term characterizes the diﬀerence between the constraint consumption under

pt+1 (computed by SAA in Algorithm 3) and ˜p∗
(54)).

t+1

(the optimal solution of the stochastic program

(cid:34) (cid:0)ai,t+1I(rt+1 > a(cid:62)

t+1 ˜p∗

t+1) − ai,t+1I(rt+1 > a(cid:62)

t+1pt+1)(cid:1) (dit − di)

(cid:35)

I(τ ≥ t)

2E

n − t − 1

(cid:113)

E(d(cid:48)

it − di)2

¯a2µ(cid:107) ˜p∗
√

t+1 − pt+1(cid:107)2

2
n − t − 1
√
2¯a2µC

≤

≤

m
(n − t − 1)

log log t
t

√

(cid:113)

·

E [(d(cid:48)

it − di)2] (Applying the dual convergence result)

where the second line applies the Cauchy–Schwartz inequality and the constant C is the coeﬃcient
of the dual convergence.

4) The following term also characterizes the diﬀerence in constraint consumption as the last one, but

57

since the denominator is larger than the last one, we directly apply the trivial upper bound for the

numerator.

(cid:34) (cid:0)ai,t+1I(rt+1 > a(cid:62)

t+1 ˜p∗

E

t+1) − ai,t+1I(rt+1 > a(cid:62)
(n − t − 1)2

t+1pt+1)(cid:1)2

(cid:35)

I(τ ≥ t)

≤

¯a2
(n − t − 1)2 .

5) With the same reason as before, the denominator is large. So, we can apply the trivial upper bound

for the numerator as follows.

2E

(cid:34) (cid:0)ai,t+1I(rt+1 > a(cid:62)

t+1 ˜p∗

t+1) − ai,t+1I(rt+1 > a(cid:62)

t+1pt+1)(cid:1) (cid:0)dit − ai,t+1I(rt+1 > a(cid:62)

t+1 ˜p∗

t+1)(cid:1)

(n − t − 1)2

(cid:35)

I(τ ≥ t)

≤

2¯a(¯a + ¯d)
(n − t − 1)2 .

Combining the above ﬁve components (upper bounds) into (55), we obtain

E

(cid:104)(cid:0)d(cid:48)

i,t+1 − di

(cid:1)2(cid:105)

≤ E

(cid:104)

it − di)2(cid:105)
(d(cid:48)

+

4¯a2 + ¯d2 + 4¯a ¯d
(n − t − 1)2 +

√

√

2¯a2µC

m
(n − t − 1)

log log t
t

√

(cid:114)

E

·

(cid:104)

it − di)2(cid:105)
(d(cid:48)

for i = 1, ..., m and t = 0, 1, ..., n − 1. Also, the above inequality holds for all n > 0 and distribution
P ∈ Ξ. From Lemma 14 in the following subsection, there exists a constant α such that

E

(cid:104)
it − di)2(cid:105)
(d(cid:48)

≤ αm log n log log n

n
(cid:88)

t=1

(56)

holds for binding dimensions i ∈ IB, n > 0 and distribution P ∈ Ξ. Then, we can analyze the stopping
time associated with the i-th constraint (binding),

E[n − τi] ≤

n
(cid:88)

t=1

P(τi ≤ t)

≤1 +

≤1 +

≤1 +

¯a
d

¯a
d

¯a
d

+

+

+

n
(cid:88)

t=1
n
(cid:88)

t=1
α
δ2
d

P (|d(cid:48)

it − di| ≤ δd)

E (cid:2)(d(cid:48)

it − di)2(cid:3)
δ2
d

(Applying the Chebyshev’s Inequality)

m log n log log n.

(57)

Here the second line (with the extra term ¯a
d

) comes from the deﬁnition of τi’s, and the third line applies

Chebyshev’s Inequality for the probabilities in the second line.

Now, we analyze the process dit and the stopping time τi for the non-binding dimensions, i.e.
i ∈ IN . In fact, the non-binding dimensions are easier to analyze. This is by the deﬁnition of δd and the
binding/non-binding dimensions, the average resource consumption (under the optimal solution to (54)
for any ˜d ∈ D) will not exceed di − δd for non-binding dimensions. Thus there is a safety region of nδd
that avoids the non-binding constraints to be exhausted too early. Also, Theorem 2 tells that there is no

need to worry about the left-overs of non-binding dimensions at the end of the horizon. Speciﬁcally, for

a non-binding dimension, we apply the same argument as (51) and (52) in the proof of Theorem 4. For

the expectation of the resource consumption of non-binding constraints, we adopt a similar derivation of

58

(46),

E





t
(cid:88)



aijI(rj > a(cid:62)

j pj)

 =

j=1

≤

t
(cid:88)

j=1

t
(cid:88)

j=1

E (cid:2)aijI(rj > a(cid:62)

j pj)(cid:3)

(cid:0)E (cid:2)aijI(rj > a(cid:62)

j pj)(cid:3) − E (cid:2)aijI(rj > a(cid:62)

j ˜p∗

j )(cid:3)(cid:1) + t(di − δd)

t
(cid:88)

j=1

t
(cid:88)

≤ ¯a2µ

≤ ¯a2µ

j=1
≤ 5C¯a2µ

√

m

C

√
√

m
j
√

E(cid:107)pj − ˜p∗

j (cid:107)2 + t(di − δd)

(cid:112)log log j + t(di − δd)

t(cid:112)log log t + t(di − δd).

For the variance of the resource consumption of non-binding constraints, with the same analysis as in

(50),

Var





t
(cid:88)

aijI(rj > a(cid:62)

j pj)


 ≤ ¯a2t + C 2¯a4µ2mt log log t.

Putting together (58) and (59),

j=1

E[n − τi] ≤

=

≤

≤

n
(cid:88)

t=1

n
(cid:88)

t=1

n
(cid:88)

t=1
α(cid:48)
δ2
d

P(τi ≤ t)





t(cid:48)
(cid:88)

aijI(rj > a(cid:62)

j pj) ≥ t(di − δd) + nδd for some 1 ≤ t(cid:48) ≤ t





P

j=1

(cid:32)

¯a2t + C 2¯a4µ2mt log log t

(cid:0)nδd − C¯a2µ

√

√

√

m

t

log log t(cid:1)2

(cid:33)

∧ 1

m log n log log n.

(58)

(59)

(60)

There exists a constant α(cid:48) dependent on ¯a, µ and C such that the above inequality holds for all non-
binding constraints i ∈ IN . Here the last line comes from a similar derivation as Lemma 11. Technically,
the bound here is tighter than the previous bound (52) because we utilize the knowledge that the non-
and thus it creates a gap of nδd in
binding constraint will not be consumed more than di − δd under ˜p∗
t
n log log n) in
the second line above. This extra term (on the order of n) reduces the bound from O(
(52) to O(log n log log n) here in (60).

√

We complete the analysis of the second part in the generic regret upper bound by combining

(57) and (60),

E[n − τ ] =E[max

i

{n − τi}]

≤

≤

m
(cid:88)

E[n − τi]

i=1
max{α, α(cid:48)}
δ2
d

m2 log n log log n + m

(cid:19)

+ 1

.

(cid:18) ¯a
d

(61)

59

Now, we analyze the ﬁrst part in the generic regret upper bound. First, note that

(cid:34) τ

(cid:88)

E

t=1

(cid:34) τ

(cid:88)

t=1

≤E

(cid:35)

(cid:107)pt − p∗(cid:107)2
2

(cid:107)pt − ˜p∗

t (cid:107)2

2 +

(cid:35)

(cid:107) ˜p∗

t − p∗(cid:107)2
2

.

τ
(cid:88)

t=1

(62)

For the ﬁrst summation in (62), we can apply the dual convergence result in Theorem 1 which is always
valid when t ≤ τ,

(cid:34) τ

(cid:88)

E

t=1

(cid:35)

(cid:107)pt − ˜p∗

t (cid:107)2
2

≤ E

(cid:34) n
(cid:88)

(cid:35)

(cid:107)pt − ˜p∗

t (cid:107)2
2

t=1
n
(cid:88)

t=1

m
t

≤ C

log t log t

where the constant C is the coeﬃcient of the dual convergence.

For the second summation in (62), we apply Lemma 12 and our previous analysis of the process dt,

≤ Cm log n log log n

(63)

(cid:34) τ

(cid:88)

E

t=1

(cid:35)

(cid:107) ˜p∗

t − p∗(cid:107)2
2

≤

≤

(cid:34)

E

(cid:88)

τ
(cid:88)

(dit − di)2

(cid:35)

i∈IB

t=1

αm2 log n log log n

1
λ2λ2

min

1
λ2λ2

min

(64)

where the ﬁrst line comes from Lemma 12 and the second line comes from plugging in the result (56) –
noting that dit = d(cid:48)
it

when t ≤ τ . Plugging (63) and (64) into (62), we obtain

(cid:34) τ

(cid:88)

E

t=1

(cid:107)pt − p∗(cid:107)2
2

(cid:35)

(cid:18)

≤

Cm +

(cid:19)

αm2

1
λ2λ2

min

log n log log n

(65)

for all n and P ∈ Ξ. Thus we complete the analysis of the second part of the generic upper bound.

Now, we analyze the third part of the generic upper bound. From the deﬁnition of τi, we know

for i ∈ IB. Consequently,

bin ≤ (di + δd)(n − τi)

E [bin] ≤ E [(di + δd)(n − τi)] ≤

¯dα
δ2
d

m log n log log n + ¯d

(cid:19)

+ 1

,

(cid:18) ¯a
d

(cid:34)

E

(cid:88)

i∈IB

(cid:35)

bin

≤

¯dα
δ2
d

m2 log n log log n + m ¯d

(cid:19)

+ 1

(cid:18) ¯a
d

(66)

for all n > 0 and P ∈ Ξ.

Combining (65), (61), and (66) with Corollary 1, we complete the proof.

60

C3.1 Lemmas and Inequalities in the Proof of Theorem 5

Lemma 12. Under Assumption 1 and 3, for any ˆd, ˜d ∈ Ωd, let

ˆp∗ ∈ arg min

ˆf (p) := ˆd(cid:62) ˆp + E (cid:2)(r − a(cid:62) ˆp)+(cid:3) ,

ˆp≥0

˜p∗ ∈ arg min

˜f ( ˜p) := ˜d(cid:62) ˜p + E (cid:2)(r − a(cid:62) ˜p)+(cid:3)

˜p≥0

be the optimal solution to the according optimization problem. Then,

(cid:107) ˆp∗ − ˜p∗(cid:107)2

2 ≤

1
λ2λ2

min

(cid:107) ˆd − ˜d(cid:107)2
2.

In addition, if ˆf and ˜f deﬁne the same binding and non-binding dimensions, IB and IN respectively,
and the binding and non-binding dimensions are strictly complimentary to each other (as Assumption 3

(c)), then

(cid:107) ˆp∗ − ˜p∗(cid:107)2

2 ≤

1
λ2λ2

min

(cid:88)

i∈IB

( ˆdi − ˜di)2.

Proof. From Proposition 2, we know

ˆf ( ˜p∗) − ˆf ( ˆp∗) ≥

˜f ( ˆp∗) − ˜f ( ˜p∗) ≥

λλmin
2

λλmin
2

(cid:107) ˜p∗ − ˆp∗(cid:107)2
2

(cid:107) ˜p∗ − ˆp∗(cid:107)2
2.

This is because the ﬁrst-order term in Proposition 2 is non-negative, and both Assumption 3 (b) and
Proposition 2 hold for all ˆd, ˜d ∈ Ωd. Adding up the two inequalities, we have

ˆd(cid:62) ˜p∗ − ˆd(cid:62) ˆp∗ + ˜d(cid:62) ˆp∗ − ˜d(cid:62) ˜p∗ ≥ λλmin(cid:107) ˜p∗ − ˆp∗(cid:107)2
2

where the expectation terms in f and ˜f are cancelled out. Then,

In addition,

λλmin(cid:107) ˜p∗ − ˆp∗(cid:107)2

2 ≤ ( ˜d − ˆd)(cid:62)( ˆp∗ − ˜p∗)
≤ (cid:107) ˆd − ˜d(cid:107)2 · (cid:107) ˜p∗ − ˆp∗(cid:107)2.

λλmin(cid:107) ˜p∗ − ˆp∗(cid:107)2

2 ≤ ( ˜d − ˆd)(cid:62)( ˆp∗ − ˜p∗)

(cid:115) (cid:88)

≤

i∈IB

( ˆdi − ˜di)2 · (cid:107) ˜p∗ − ˆp∗(cid:107)2

where only the binding dimensions remain in the last line because both ˜p∗ and ˆp∗ are zero-valued on
the non-binding dimensions.

Lemma 13. Recall that the binding and non-binding dimensions speciﬁed by the stochastic program (7)
with parameter d = (d1, ..., dm)(cid:62) as IB and IN . Under Assumption 1 and 3, there exists a constant
δd > 0 such that for all ˜d ∈ D = (cid:78)m
i=1[di − δd, di + δd] ⊂ Ωd, the stochastic program (7) speciﬁed with
the parameter ˜d share the same binding and non-binding dimensions.

Proof. From Lemma 12, we know that for any ˜d ∈ Ωd and the corresponding optimal solution ˜p∗, we

61

have

If the conclusion in the lemma does not hold, there are two cases:

(cid:107)p∗ − ˜p∗(cid:107)2

2 ≤

1
λ2λ2

min

(cid:107)d − ˜d(cid:107)2
2.

(i) There is an index i ∈ IB but i is a non-binding constraint for the stochastic program speciﬁed by

˜d, i.e., p∗

i > 0 and ˜p∗

i = 0.

(ii) There is an index i ∈ IN but i is a binding constraint for the stochastic program speciﬁed by ˜d,

i.e.,

di > E[aijI(rj > a(cid:62)
˜di = E[aijI(rj > a(cid:62)

j p∗)],
j ˜p∗)].

For case (i), denote p = min{p∗

i : i ∈ IB}. Then, we must have

(cid:107)d − ˜d(cid:107)2

2 ≥ λ2λ2

min(cid:107)p∗ − ˜p∗(cid:107)2

2 ≥ p2λ2λ2

min.

For case (ii), from the inequality (45) in the proof of Theorem 4, we know that

|E[aijI(rj > a(cid:62)

j ˜p∗)] − E[aijI(rj > a(cid:62)

j p∗)]| ≤ ¯a2µ(cid:107) ˜p∗ − p∗(cid:107)2.

(67)

(68)

Recall that from Assumption 3 (c),

for i ∈ IN . Denote

di > E[aijI(rj > a(cid:62)

j p∗)]

γ = min
i∈IN

(cid:8)di − E[aijI(rj > a(cid:62)

j p∗)](cid:9) .

From Assumption 3 (c), we know γ > 0. From (67) and (68), we know that if |di − ˜di| ≤ γ
2
have

, then we must

(cid:107) ˜p∗ − p∗(cid:107)2 ≥

=

≥

≥

and consequently,

1
¯a2µ
1
¯a2µ
1
¯a2µ
γ
2¯a2µ

|E[aijI(rj > a(cid:62)

j ˜p∗)] − E[aijI(rj > a(cid:62)

j p∗)]|

| ˜di − E[aijI(rj > a(cid:62)

j p∗)]|

(from the condition (67) in case (ii))

(cid:16)

|di − E[aijI(rj > a(cid:62)

j p∗)]| − |di − ˜di|

(cid:17)

,

(cid:107)d − ˜d(cid:107)2

2 ≥ λ2λ2

min(cid:107)p∗ − ˜p∗(cid:107)2

2 ≥

γ2λ2λ2
4¯a4µ2

min

.

Combining the two aspects, we know that when

(cid:107)d − ˜d(cid:107)2

2 < min

(cid:26)

p2λ2λ2

min,

γ2λ2λ2
4¯a4µ2

min

,

γ2
4

(cid:27)

.

Then we have the two stochastic programs speciﬁed by d and ˜d must share the same binding and
non-binding dimensions. And the existence of δd is implied by the last inequality.

62

Lemma 14. If a sequence {zt}n

t=0

satisﬁes

zt+1 = zt +

√

m log log t
(n − t − 1)

√
√

zt
t

+

1
(n − t − 1)2 ,

and z0 = 0. Then we have

n
(cid:88)

t=1

zt ≤ 32 log n log log n

holds for all n ≥ 3.

Proof. First, we only need to prove that if

zt+1 = zt +

√

zt

(n − t − 1)

+

√

t

1
(n − t − 1)2 ,

(69)

then

n
(cid:88)

t=1

zt ≤ 32 log n.

This is because we could change the variable by introducing z(cid:48)
results for z(cid:48)

t. So we analyze the sequence zt under (69) now. Note that if zt ≤

t = zt · m log log t and prove the above
(n−t−1)2 ,
4t

zt+1 ≤

4t
(n − t − 1)2 +

3
(n − t − 1)2 ≤

4(t + 1)
(n − t − 2)2 .

Also, we have z0 = 0. Thus, when t ≤ n/2, an induction argument leads to

zt ≤

4t
(n − t − 1)2

and speciﬁcally, zn/2+1 ≤ 10/n.

Now, we analyze the case of t > n/2. Note that if

zt ≤

16
n − t − 1

and t ≥ n/2, we have

zt+1 ≤

16
n − t − 1

+

1
(n − t − 1)2 +

4

(n − t − 1)3/2

≤

√

t

16
n − t − 2

.

Also, we have

Thus, with an induction argument for t ≥ n/2, we obtain

zn/2+1 ≤

10
n

≤

16
n − n
2 − 1

.

zt ≤

16
n − t − 2

.

63

Now, combining the two parts together,

n−3
(cid:88)

i=1

zt ≤

n−3
(cid:88)

i=n/2

16
n − t − 2

+

n/2
(cid:88)

i=1

4(t + 1)
(n − t − 3)2

≤ 32 log n +

n
n − n/2 − 1

+ log(n − 1 − n/2) +

1
n − 3

≤ 32 log n,

where n > 3 and the second inequality is obtained by approximating the sum by integral.

D One-Constraint Case and Lower Bound

D1 One-Constraint Case

In this section, we discuss the OLP regret lower bound by relating the OLP problem with a statistical
estimation problem. Speciﬁcally, we consider a one-constraint LP where m = 1 in LP (1). We set a1j = 1
for j = 1, ..., n. Then the optimization problem becomes

max

s.t.

n
(cid:88)

j=1
n
(cid:88)

j=1

rjxj

(70)

xj ≤ nd, xj ∈ [0, 1].

This one-constraint case of OLP has been discussed extensively and is known as the multi-secretary

problem (Kleinberg, 2005; Arlotto and Gurvich, 2019; Bray, 2019). To simplify our discussion, we
assume nd to be an integer. There exists an integer-valued optimal solution of (70), given by

x∗
t =




1,



0,

rt ≥ ˆQn(1 − d)
rt < ˆQn(1 − d)

where ˆQn(η) deﬁnes the sample η-quantile of {rj}n
sample quantile ˆQn(1 − d) is indeed the dual optimal solution p∗
n
this optimal solution allocates resources to the proportion of orders with highest returns. We restate

. The
Intuitively,

, i.e. ˆQn(η) = inf

in the general setting.

v ∈ R :

≥ η

j=1

j=1 I(v>rj )
n

(cid:110)

(cid:80)n

(cid:111)

Assumption 1 and 2 in this one-constraint case as follows.

Assumption 4. Assume d ∈ (0, 1) and {rj}n
is a sequence of i.i.d. random variables supported on
[0, 1]. Assume it has a density function fr(x) s.t. λ(cid:48) ≤ fr(x) ≤ µ(cid:48) for x ∈ [0, 1] with λ(cid:48), µ(cid:48) > 0. Denote
the set of all distributions Pr satisfying the above assumptions as Ξr.

j=1

We restrict our attention to a class of thresholding policies as discussed in Section 4.1. At each time
t, we compute a dual price from the history inputs, pt = ht(r1, x1, ..., rt−1, xt−1) and if the constraint
permits, set

xt =




1,

rt > pt.



0,

rt ≤ pt.

If (cid:80)t
j=1 xt = nd for some t, we require ps = hs(r1, x1, ..., rs−1, xs−1) = 1 for all s > t. In this way,
all the future orders will be automatically rejected. Similar to the general OLP setting, an online

64

algorithm/policy in this one-constraint problem can be speciﬁed by the sequence of functions ht’s, i.e.,
π = (h1, ..., hn).

D2 Lower Bound

Theorem 7 establishes a regret lower bound for the one-constraint problem. The inequality tells that if
we view the threshold pj at each step as an statistical estimator, the regret of this one-constraint problem
is no less than the cumulative estimation error of a certain quantile of the distribution p∗ = Qτ (1 − d).
The signiﬁcance of this inequality lies in the fact that the term on its right-hand side does not involve

the constraint.

In an online optimization/learning setting, a violation of the binding constraint will

potentially improve the reward and reduce the regret; however, it does not necessarily help decrease the

term on the right-hand side. Therefore, while studying the lower bound, we can focus on the right-hand

side and view it as the estimation error from an unconstrained problem.

Theorem 7. The following inequality holds

EPr [R∗

n − Rn(π)] ≥

λ(cid:48)
2

· EPr

(cid:34) n
(cid:88)

(pt − p∗)2

(cid:35)

t=1

+ EPr [R∗

n] − ng(p∗)

(71)

holds for any the thresholding policy π and any distribution Pr ∈ Ξr. Here pt = ht(r1, x1, ..., rt−1, xt−1)
is speciﬁed by the policy π, and p∗ = Qr(1 − d) is the (1 − d)-quantile of the random variable rj with
Qr(η) := inf {v ∈ R : P(r ≤ v) ≥ η} . The parameter λ(cid:48) comes from Assumption 4. The function g(·) is
the same as deﬁned in Section 4.2.

The proof of Theorem 7 follows the same approach as the derivation of the upper bound in Theorem
2. We establish ng(p∗) as an upper bound for E [R∗
n] and compare E [Rn(π)] against ng(p∗). Therefore,
n] and ng(p∗) will aﬀect the tightness of this lower bound. Fortunately, this gap
the gap between EPr [R∗
should be small in many cases; for example, in the proof of Theorem 6, we use the dual convergence

result in Theorem 1 and show that

EPr [R∗

n] − ng(p∗) ≥ −C log log n

for this one-constraint problem. The following corollary presents the lower bound in a more similar form
as the upper bound. The right hand side of the lower bound in Corollary 2 also involves E[n − τ0].
Note that the terms E[n − τ0] and E (cid:2)(cid:80)
(cid:3) symmetrically capture the overuse and underuse of the
constraints, and therefore they should be on the same order. Corollary 2 tells that the upper bound

bin

i∈IB

given in Theorem 2 is rather tight, and that a stable control of the resource consumption is indispensable
because the term E[n − τ0] also appears in the lower bound of the regret.

Corollary 2. The following inequality holds,

EPr [R∗

n − Rn(π)] ≥

λ(cid:48)
2

· EPr

(cid:34) τ0(cid:88)

t=1

(cid:35)

(pt − p∗)2

+

λ(cid:48)
2

(1 − p∗)2 E[n − τ0] + EPr [R∗

n] − ng(p∗)

holds for any the thresholding policy π and any distribution Pr ∈ Ξr. Here pt = ht(r1, x1, ..., rt−1, xt−1)
is speciﬁed by the policy π, and p∗ = Qr(1 − d). The stopping time

τ0 = min{n} ∪






t :

t
(cid:88)

j=1

xj = nd






represents the ﬁrst time that the resource is exhausted.

65

Based on Theorem 7, we can derive a lower bound for the OLP problem by analyzing the right-
hand-side of (71). The idea is to ﬁnd a parametric family of distributions and we relate p∗ with the
parameter that speciﬁes the distribution Pr. The t-th term on the right-hand side then can be viewed
as the approximation error of the parameter p∗ using the ﬁrst t − 1 observations. Theorem 6 states the
lower bound for the OLP problem. The proof considers a family of truncated exponential distributions

that satisﬁes Assumption 4, and it mimics the derivation of lower bounds in (Keskin and Zeevi, 2014;

Besbes and Muharremoglu, 2013). The core part is the usage of van Trees inequality (Gill and Levit,

1995) – a Bayesian version of the Cramer-Rao bound.

Theorem. There exist constants C and n0 > 0 such that

holds for all n ≥ n0 and any dual-based policy π.

∆n(π) ≥ C log n

Theorem 6 indicates that Algorithm 3 is an asymptotically near-optimal algorithm for the OLP
problem under the ﬁxed-m and large-n regime. The lower bound O(log n) is also consistent with the
lower bound of the unconstrained online convex optimization problem (Abernethy et al., 2008).

D3 Proof of Theorem 7

Proof. First,

E [Rn(π)] = E

(cid:34) n
(cid:88)

(cid:35)

rtxt

(a)
≤ E

t=1
(cid:34) n
(cid:88)

t=1

(cid:32)

rtxt −

nd −

(cid:33)

(cid:35)

xt

p∗

n
(cid:88)

t=1

(cid:34) n
(cid:88)

(cid:35)
(rtxt + dp∗ − xtp∗)

= E

t=1

n
(cid:88)

E [rtxt + dp∗ − xtp∗]

t=1
n
(cid:88)

t=1

E [g(pt)] .

=

(b)
=

where the expectation is taken with respect to rt ∼ Pr. (a) comes from that the constraint must be
satisﬁed and (b) comes from the deﬁnition of g(·). We do not need to consider the stopping time because
after the resource is exhausted, the setting of ps = 1 is consistent with the enforcement of xs = 0.

Then,

ng(p∗) − E [Rn(π)] ≥

=

≥

n
(cid:88)

t=1
n
(cid:88)

E[g(p∗) − g(pt)]

E [(rt − p∗)I(rt > p∗) − (rt − p∗)I(rt > pt)]

t=1
λ(cid:48)
2

n
(cid:88)

t=1

E (cid:2)(pt − p∗)2(cid:3)

where the expectation is taken with respect to rt ∼ Pr and the last line comes from Assumption 4.

66

D4 Proof of Corollary 2

Proof. Since the algorithm enforces pt = 1 for t > τ0. The result follows by splitting the summation on
the right-hand-side of (6).

EPr [R∗

n − Rn(π)] ≥

=

=

λ(cid:48)
2

λ(cid:48)
2

λ(cid:48)
2

EPr

EPr

EPr

(cid:34) n
(cid:88)

t=1

(cid:34) τ0(cid:88)

t=1

(cid:34) τ0(cid:88)

t=1

(pt − p∗)2

(cid:35)

(cid:35)

(pt − p∗)2

+

(cid:35)

(pt − p∗)2

+

+ EPr [R∗

n] − ng(p∗)

(cid:34) n
(cid:88)

EPr

(cid:35)

(pt − p∗)2

+ EPr [R∗

n] − ng(p∗)

t=τ0+1

(1 − p∗)2 E[n − τ0] + EPr [R∗

n] − ng(p∗).

λ(cid:48)
2

λ(cid:48)
2

D5 Proof of Theorem 6

We ﬁrst introduce the van Tree inequality and refer its proof to (Gill and Levit, 1995).

Lemma 15 (Gill and Levit (1995)). Let (X , F, Pθ : θ ∈ Θ) be a dominated family of distributions on
some sample space X ; denote the dominating measure by µ. The parameter space Θ is a closed interval
on the real line. Let f (x|θ) denote the density of Pθ with respect to µ. Let λ(θ) denote the density
function of θ. Suppose that λ and f (x|·) are both absolutely continuous, and that λ converges to zero
at the endpoints of the interval Θ. Consider φ : Θ → R a ﬁrst-order diﬀerentiable function. Let ˆφ(X)
denote any estimator of φ(θ). Then,

E[ ˆφ(X) − φ(θ)]2 ≥

[Eφ(cid:48)(θ)]2
E[I(θ)] + I(λ)

where the expectation on the left hand side is taken with respect to both X and θ, and the expectation on
the right hand side is taken with respect to θ. I(θ) and I(λ) denote the Fisher information for θ and λ,
respectively,

I(θ) := E

I(λ) := E

(cid:105)

(log f (X|θ)(cid:48))2 (cid:12)
(cid:104)
(cid:12)
(cid:12)θ
(log λ(θ)(cid:48))2(cid:105)

(cid:104)

.

Now, we proceed to prove Theorem 6.

Proof of Theorem 6. First, we analyze the gap between E[R∗

n] and ng(p∗).

ng(p∗) − E[R∗

n] ≤

=

n
(cid:88)

t=1
n
(cid:88)

t=1

(E [(p∗ − p∗

n) I(p∗ ≥ rt > p∗

n)] + E [(p∗

n − p∗) I(p∗ < rt ≤ p∗

n)])

(E [(p∗ − p∗

n) P(p∗ ≥ rt > p∗

n|p∗

n)] + E [(p∗

n − p∗) P(p∗ < rt ≤ p∗

n|p∗

n)])

≤ nµ(cid:48)E (cid:2)|p∗ − p∗
≤ Cµ(cid:48) log log n

n|2
2

(cid:3)

(72)

where the constant C is the coeﬃcient of dual convergence in Theorem 1. Here the ﬁrst line comes from
the proof of Lemma 3 and the third line comes from Assumption 4. The last comes for applying the dual

67

convergence result to this special one-constraint case. Next, we derive a lower bound for





E

n
(cid:88)

(pj − p∗)2





with the help of the van Tree’s inequality in Lemma 15. For the lower bound, we only need to derive
under a speciﬁc distribution. Consider a truncated exponential distribution for rj’s

j=1

f (r|θ) =

θe−θrI(r ∈ [0, 1])
1 − e−θ

and the Beta distribution as the priori for the parameter θ

λ(θ) = (θ − 1)2(2 − θ)2

with the support Θ = [1, 2]. Let d = 1/2 and then

p∗ = Q 1

2

(r) = φ(θ) :=

1
θ

log

(cid:18) 1
2

+

(cid:19)

.

1
2

θ

Additionally,

[Eφ(cid:48)(θ)]2 =

(cid:20)(cid:90) 2

(cid:21)2

φ(cid:48)(θ)λ(θ)dθ

:= c1 ≈ 0.006.

1
The Fisher information I(θ) and I(r; λ) can be computed according to the deﬁnition.

E [I(r; θ)] = E

(cid:104)

(cid:104)
E

(log f (r|θ)(cid:48))2 (cid:12)
(cid:12)
(cid:12)θ

(cid:105)(cid:105)

(cid:90) 2

(cid:90) 1

=

(log f (r|θ)(cid:48))2 f (r|θ)λ(θ)drdθ := c2 > 0.

1
(cid:104)
(log λ(θ)(cid:48))2(cid:105)
I(λ) = E

0

=

(cid:90) 1

0

(log λ(θ)(cid:48))2 λ(θ)dθ := c3 > 0.

In above, c1, c2 and c3 are deterministic real numbers that can be computed from the corresponding
integrals.

Then, pj can be viewed as an estimator of p∗ = φ(θ) with the ﬁrst j − 1 samples. Consider the fact

that

We apply Lemma 15 and obtain,

E (cid:2)I(r1:(j−1); θ)(cid:3) = (j − 1)E [I(r; θ)] .





n
(cid:88)

j=1

(pj − p∗)2



 ≥

n
(cid:88)

j=2

c1
c2(j − 1) + c3

≥ c4 log n

(73)

E

sup
θ

for all n > 0 with some constant c4 dependent on c1, c2, and c3. Combining (72) and (73) with Theorem
7, there exist a θ and a distribution P(r|θ) such that

ER∗

n − ERn(π) ≥

c4λ(cid:48)
2

log n − Cµ(cid:48) log log n.

Set n0 = min {n ≥ 0 : c4λ(cid:48) log n ≤ 4Cµ(cid:48) log log n} and C = c4λ(cid:48)

4

. The lower bound result follows.

68

D6 More Discussions on the Algorithms

In the paper, we present three diﬀerent algorithms and derive corresponding regret bounds with the help

of Theorem 2 and Corollary 1. As an summary, Table 4 presents the regret upper bounds of the algorithms

separately with respect to the three components in Theorem 2. Table 5 summarizes the prior knowledge
and the computational cost of the three algorithms. Algorithm 1 uses p∗ as the dual price and it serves
for a benchmark purpose. Strictly speaking, Algorithm 1 is not an OLP algorithm in that it assumes the
knowledge of the distribution P and that to compute p∗ exactly is hardly practical. Algorithm 2 is both
a learning version of Algorithm 1 and a simpliﬁed version of the dynamic learning algorithm proposed

in (Agrawal et al., 2014). Algorithm 3 introduces a history-action-dependent mechanism to stabilize

the constraint consumption. It is an adaptive version of Algorithm 2 and an extension of the re-solving

technique (in network revenue management literature) to a learning and more general context.

Algorithm E (cid:2)(cid:80)τ

t=1 (cid:107)pt − p∗(cid:107)2
2

Algorithm 1
Algorithm 2
Algorithm 3

0
˜O(log n)
˜O(log n)

bin

(cid:3) E [n − τ ] E (cid:2)(cid:80)
˜O(
n)
˜O(
n)
˜O(log n)

˜O(
n)
˜O(
n)
˜O(log n)

i∈IB
√
√

√
√

(cid:3)

Regret
√
√

˜O(
n)
˜O(
n)
˜O(log n)

Table 4: Three components of the upper bound in Theorem 2/Corollary 1.

Algorithm

Prior Knowledge Computational Cost

Algorithm 1
Algorithm 2
Algorithm 3

(d1, ..., dm), P, p∗
(d1, ..., dm)
(d1, ..., dm), n

O(1)
O(log n)
O(n)

Regret
√

O(
√

n)
n log n)

O(

O(log n log log n)

Table 5: Algorithm summary and comparison

The table shows that the bottleneck for Algorithm 1 and 2 lies in the control of constraint con-

sumption. Intuitively, these two algorithms are not adaptive enough and the constraint consumption in
each period is “independent” of the current constraint level. This causes a ﬂuctuation of O(
n) after n
periods, and that is essentially the reason why the last two terms are O(
n) for these two algorithms.
Moreover, Algorithm 3, in contrast with the geometrically updating scheme in Algorithm 2, updates

√

√

the dual price after every period. The analysis of Algorithm 3 indicates the goal of this more frequent
updating scheme is not to further reduce the approximation error of p∗, but to stabilize the constraint
consumption.

In Table 5, the computational cost is measured by the number of LPs or optimization problems that

need to be solved throughout the process. Algorithm 1 utilizes the distribution knowledge and thus

only needs to optimize once. Algorithm 2 is notably more computationally eﬃcient than Algorithm 3.
However, the O(n) computational cost of Algorithm 3 can be signiﬁcantly curtailed in practice by using
pt as the initial point while solving the optimization problem for pt+1 (as in papers (Gupta and Molinaro,
2014; Agrawal and Devanur, 2014b; Devanur et al., 2019)). The dual convergence result tells us that pt
and pt+1 are close to each other and this makes pt as a good warm start for pt+1.

Also, we further illustrate the performance of the algorithms with respect to the number of constraints
m. Table 6 reports the performance under the model of Random Input I and Random Input II with
ﬁxed n = 500 but diﬀerent values of m. From the table, we conjecture that under Random Input I, the
regret increases sublinearly as m grows, while under Random Input II, the regret grows linearly as m
grows.

69

Model

Random Input I

Random Input II

Algorithm

A1

A2

A3

A1

A2

A3

m = 5, n = 500
m = 10, n = 500
m = 50, n = 500
m = 100, n = 500
m = 200, n = 500

90.64
135.55
228.43
251.95
281.72

109.07
152.28
255.74
296.96
319.55

31.89
38.23
56.22
70.34
76.51

36.87
62.65
853.98
2189.28
4975.79

120.27
174.04
647.83
1732.12
4291.45

8.73
25.52
369.99
1197.30
3351.86

Table 6: Regret performance: A1, A2, and A3 stand for Algorithm 1 (No-need-to-learn), Algorithm 2
(Simpliﬁed Dynamic Learning), and Algorithm 3 (Action-history-dependent), respectively.

70

