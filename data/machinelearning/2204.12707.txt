Accelerated Continuous-Time Approximate Dynamic Programming via
Data-Assisted Hybrid Control(cid:63)

aDepartment of Electrical, Energy and Computer Engineering. University of Colorado Boulder, Boulder, 80305, Colorado, USA

Daniel E. Ochoa∗, Jorge I. Poveda

2
2
0
2

r
p
A
7
2

]

C
O
.
h
t
a
m

[

1
v
7
0
7
2
1
.
4
0
2
2
:
v
i
X
r
a

Abstract

We introduce a new closed-loop architecture for the online solution of approximate optimal control problems in the
context of continuous-time systems. Speciﬁcally, we introduce the ﬁrst algorithm that incorporates dynamic mo-
mentum in actor-critic structures to control continuous-time dynamic plants with an afﬁne structure in the input.
By incorporating dynamic momentum in our algorithm, we are able to accelerate the convergence properties of the
closed-loop system, achieving superior transient performance compared to traditional gradient-descent based tech-
niques. In addition, by leveraging the existence of past recorded data with sufﬁciently rich information properties,
we dispense with the persistence of excitation condition traditionally imposed on the regressors of the critic and the
actor. Given that our continuous-time momentum-based dynamics also incorporate periodic discrete-time resets that
emulate restarting techniques used in the machine learning literature, we leverage tools from hybrid dynamical sys-
tems theory to establish asymptotic stability properties for the closed-loop system. We illustrate our results with a
numerical example.

Keywords: Approximate dynamic programming, concurrent learning, hybrid systems, Lyapunov theory.

1. Introduction

Recent technological advances in computation and sensing have incentivized the development and implementa-
tion of data-assisted feedback control techniques previously deemed intractable due to their computational complexity.
Among these techniques, reinforcement learning (RL) has emerged as a practically viable tool with remarkable de-
grees of success in robotics [1], autonomous driving [2], water-distribution systems [3], among other cyber-physical
applications, see [4]. These types of algorithms, are part of a large landscape of adaptive systems that aim to control a
plant while simultaneously optimizing a performance index in a model-free way, with closed-loop stability guarantees.
In this paper, we focus on a particular class of inﬁnite horizon RL problems from the perspective of approximate
optimal control and approximate adaptive dynamic programming (AADP). Speciﬁcally, we study the optimal con-
trol problem for nonlinear continuous-time and control-afﬁne deterministic plants, interconnected with approximate
adaptive optimal controllers [5] in an actor-critic conﬁguration. These types of adaptive controllers aim to ﬁnd, in real
time, the solution to the Hamilton-Jacobi-Bellman (HJB) equation by measuring the output of the nonlinear dynamical
system while making use of two approximation structures:

• a critic, used to estimate the optimal value function of the optimal control problem, and

• an actor, used to estimate the optimal feedback controller.

Our goal is to design online adaptive dynamics for the real-time tuning of the aforementioned structures, while si-
multaneously achieving closed-loop stability and high transient performance. To achieve this, and motivated by

(cid:63)Research supported in part by NSF grant number CNS-1947613.
∗Corresponding Author.
Email addresses: daniel.ochoa@colorado.edu (Daniel E. Ochoa ), jorge.poveda@colorado.edu (Jorge I. Poveda)

 
 
 
 
 
 
the widespread usage of momentum-based gradient dynamics in practical RL settings [6], we study continuous-time
actor-critic dynamics inspired by a class of ordinary differential equations (ODEs) that can be seen as continuous-time
counterparts of Nesterov’s accelerated optimization algorithm [7]. Such types of algorithms have gained popularity
in optimization and related ﬁelds due to the fact that they can minimize smooth convex functions at a rate of order
(1/t2) [8]. The main source for the acceleration property in these ODEs comes from the addition of momentum to
O
gradient-based dynamics, in conjunction with a vanishing dynamic damping coefﬁcient. However, as recently shown
in [9] and [10], the non-uniform convergence properties that emerge in these types of dynamics complicates their use
in feedback systems with plant dynamics in the loop. In this paper, we overcome these challenges by incorporating
resets into the proposed momentum-based algorithms, similar to restarting heuristics studied in the machine learning
literature, see [11] and [7]. Our resulting actor-critic controller is naturally modeled by a hybrid dynamical system
that incorporates continuous-time and discrete-time dynamics, which we analyze using tools from [12].

A traditional assumption in the literature of continuous-time actor-critic RL is that the regressors used in the pa-
rameterizations satisfy a persistence of excitation condition along the trajectories of the plant. However, in practice,
this condition can be difﬁcult to verify a priori. To circumvent this issue, in this paper we consider a data-assisted
approach, where a ﬁnite amount of past “sufﬁciently rich” recorded data is used to guarantee asymptotic learning
in the closed-loop system. As a consequence, the resulting data-assisted hybrid control algorithm concurrently uses
real-time and recorded data, similar in spirit to concurrent-learning (CL) techniques [13]. By using Lyapunov-based
tools for hybrid dynamical systems, we analyze the interconnection of an actor-critic neural-network (NN) controller
and the nonlinear plant, establishing that the trajectories of the closed-loop system remain ultimately bounded around
the origin of the plant and the optimal actor and critic NN parameters. Since the resulting closed-loop system has
suitable regularity properties in terms of continuity of the dynamics, our stability results are in fact robust with respect
to arbitrarily small additive disturbances that can be adversarial in nature, or that can arise due to numerical imple-
mentations. To the best knowledge of the authors, these are the ﬁrst theoretical stability guarantees of continuous-time
accelerated actor-critic algorithms for neural network-based adaptive dynamic programming controllers in nonlinear
deterministic settings.

The rest of this paper is organized as follows: Section 2 presents the notation and some concepts on hybrid
dynamical systems, Section 3 presents the problem statement and some preliminaries on optimal control. Section
4 introduces the hybrid momentum-based dynamics for the update of the critic NN, Section 5 presents the update
dynamics for the actor NN, and Section 6 studies the properties of closed-loop system.
In Section 7 we study a
numerical example illustrating our theoretical results.

2. Preliminaries

|

z

⊂

|·|

z
|

A
|

A := mins∈A
|

to denote its usual vector norm. Given A

Notation: We denote the real numbers by R, and we use R≥0

R to denote the non-negative real line. We use
Rn×n, we
to denote the induced 2-norm for matrices, and we infer its distinction with the vector norm depending on the
Rn,
. We also use rB to denote a closed ball
Rn×n to denote the identity matrix,
R≥0 is said
R≥0 is
R≥0, it is non-increasing in its second argument, and
R is deﬁned as a column
→
Rm×n to denote its Jacobian

Rn to represent the n-dimensional Euclidean space and
use
context. We use Tr (A) to denote the trace operator on matrices. Given a compact set
we use
A
in the Euclidean space, of radius r > 0, and centered at the origin. We use In
and (x, y) for the concatenation of the vectors x and y, i.e., (x, y) := [x(cid:62), y(cid:62)](cid:62). A function γ : R≥0
(γ
to be of class-
said to be of class-
lims→∞ β(r, s) = 0 for each r
vector and denoted by
matrix.

), if it is continuous, zero at zero, and nondecreasing. A function β : R≥0
(β

∈
R≥0. The gradient of a real valued function f : Rn

f . For a vector valued function g : Rn

to represent the minimum distance of z to

Rm, we use ∂g(x)

Rn and a vector z

∈ KL
∈

∈ K
KL

for each s

∂x ∈

, s)
·

→
R≥0

) if β(

A ⊂

∈ K

s
|

→

→

∇

×

−

K

∈

∈

∈

|

Hybrid Dynamical Systems: To study our algorithms, we will use tools from hybrid dynamical systems (HDS)
theory [12]. A HDS with state x

Rn, has dynamics

∈
where F : Rn
Rn is called the ﬂow map, G : Rn
are closed sets, called the ﬂow set and the jump set, respectively. We use

→

→

∈

Rn is called the jump map, and C

Rn
⊂
= (C, F, D, G) to denote the elements

Rn and D

⊂

C,

˙x = F (x),

and x

D, x+ = G(x),

(1)

∈
x

2

H

H

. Solutions x : dom(x)

Rn to system (1) are indexed by a continuous-time parameter t, which
of the HDS
increases continuously during ﬂows, and a discrete-time index j, which increases by one during jumps. Thus, the
notation ˙x in (1) represents the derivative dx(t,j)
; and x+ in (1) represents the value of x after an instantaneous jump,
Rn to system (1) are deﬁned on hybrid time domains. For a
i.e., x(t, j + 1). Therefore, solutions x : dom(x)
precise deﬁnition of hybrid time domains and solutions to HDS of the form (1), we refer the reader to [12, Ch.2]. The
following deﬁnitions will be instrumental to study the stability and convergence properties of systems of the form (1).

→

→

dt

Deﬁnition 1. The compact set
β

A ⊂
and r > 0 such that every solution x with x(0, 0)

D is said to be uniformly asymptotically stable (UAS) for system (1) if
D) satisﬁes:

rB

(C

C

∪

∃

∈ KL

x(t, j)
|
|

A

β(

x(0, 0)
|
|

≤

∩
∈
A, t + j),

∪
(t, j)

∀

dom(x).

∈

When β(r, s) = c1re−c2s for some c1, c2 > 0, the set

A

is said to be uniformly exponentially stable (UES).

3. Problem Statement

Consider a control-afﬁne nonlinear dynamical plant

˙x = f (x) + g(x)u,

(2)

(cid:3)

(3)

Rn is the state of the system, u

Rn×m are
where x
locally Lipschitz functions. Our goal is to design a stable algorithm able to ﬁnd –in real time– a control law u∗ that
minimizes the cost functional V : Rn

Rm is the input, and f : Rn

Rn and g : Rn

R given by:

→

→

⊂

U

∈

∈

V

× U

→

V (x0, u) :=

(cid:90) ∞

0

(cid:16)

r

x(cid:0)τ (cid:1), u (x(τ ))

(cid:17)

dτ,

(4)

where x(cid:0)t(cid:1) represents a solution to (3) from the initial condition x(0) = x0, that results from implementing a feedback
law u, belonging to a class of admissible control laws

V characterized as follows:

Deﬁnition 2. [14, Deﬁnition 1] Given the dynamical system in (3), a feedback control u : Rn
with respect to the cost functional V in (4) if

• u is continuous,

• u renders system (3) UAS,

• V (x0, u) <

for all x0

∞

Rn.

∈

We denote the set of admissible feedback laws as

U

V .

U

Rm is admissible

→

(cid:3)

In (4), we consider cost functions r : Rn
×
given by Q(x) := x(cid:62)Πxx with Πx
optimal control law that minimizes (4), we study the Hamiltonian function H : Rn
and (4), given by

R of the form r(x, u) := Q(x) + R(u), where the state-cost is
0. To ﬁnd the
(cid:31)
R related to (3)

0, and the control-cost is given by R(u) := u(cid:62)Πuu with Πu

Rm

Rm

Rn

→

→

×

×

(cid:31)

Using (5), a necessary optimality condition for u∗ is given by Pontryagin’s maximum principle [15]:

H(x, u,

V ) :=

∇

∇

V (cid:62)(f (x) + g(x)u) + Q(x) + R(u).

u∗(x) = arg min

H(x, u,

u∈UV

V ∗) =

⇒

∇

u∗(x) =

1
2

−

Π−1

u g(x)(cid:62)

V ∗(x),

∇

(5)

(6)

where V ∗ represents the optimal value function:

V ∗(x) := inf
u∈UV

))
V (x, u(
·

3

On the other hand, under the assumption that V ∗ is continuously differentiable, the optimal value function can be
shown to satisfy the Hamilton-Jacobi-Bellman equation [5, Ch. 1.4]:

∂V ∗
∂t

=

−

H(x, u∗,

V ∗)

∇

Rn.

x

∀

∈

Since the functional in (4) does not have an explicit dependence on t, it follows that ∂V ∗
0, meaning that for all x

Rn, the following holds:

∂t = 0, and hence H(x, u∗,

∈

V ∗(cid:62) (cid:0)f (x) + g(x)u∗(x)(cid:1) + Q(x) + R

(cid:16)

(cid:17)

u∗(x)

= 0.

∇

V ∗) =

∇

(7)

The time-invariant Hamilton-Jacobi-Bellman equation in (7), allows for a state-dependent characterization of opti-
mality. Therefore, by using the optimal control law in (6), and assuming that the system dynamics (3) are known,
the form (7) could be leveraged to ﬁnd V ∗. Unfortunately, ﬁnding an explicit closed-form expression for V ∗, and
thus for the optimal control law, is, in general, an intractable problem. However, the utility of (7) is not completely
lost. As we shall show in the following sections, online and historical “measurements” of (7) can be leveraged in real
time to estimate the optimal control law u∗ while concurrently rendering a neighborhood of the origin of system (3)
asymptotically stable.

4. Data-Assisted Critic Dynamics

To leverage the form of (7), we consider the following parameterization of the optimal value function V ∗(x):

V ∗(x) = θ∗(cid:62)

c φc(x) + (cid:15)c(x)

x

∀

∈

K,

(8)

Rn is a compact set, θ∗

Rlc is a vector of continuously differentiable basis functions,
where K
⊂
R is the approximation error. The parameterization (8) is always possible on compact sets due to the
and (cid:15)c : Rn
continuity properties of V and the universal approximation theorem [16]. This parametrization results in an optimal
Hamiltonian of the form H ∗

Rlc, φc : Rn

(cid:15)c) given by:

c ∈

p := H(x, u∗, ∂φc
∂x

θ∗
c +

→

→

(cid:62)

∇

H ∗

p (x) = θ∗(cid:62)

c ψ(x, u∗(x)) + Q(x) + R (u∗(x)) +

(cid:15)c(x)(cid:62) (f (x) + g(x)u∗(x)) ,

∇

where we deﬁned ψ : Rn

Rm

×

→

Rlc as:

ψ(x, u) :=

∂φc(x)
∂x

(f (x) + g(x)u) .

(9)

(10)

×

→

Rm

We note that the explicit dependence of ψ : Rn
Rlc on the control action u, deﬁned in (10), is a fundamental
departure from the previous approaches studied in the context of concurrent learning (CL) NN actor-critic controllers,
such as those considered in [17] and [18]. In particular, we note that in the context of CL the data used to estimate
the optimal value function V ∗ is generated from measurements of the optimal Hamiltonian which, by deﬁnition,
incorporates the optimal control law u∗. Hence, the need to include u as part of the regressor vectors ψ becomes
crucial; this dependence characterizes how far our recorded measurements of a Hamiltonian are from the optimal
Hamiltonian H ∗
p . Indeed, this distance will explicitly emerge in our convergence and stability analysis. Naturally, the
dependence of (10) on u will impose stronger conditions on the recorded data needed to estimate V ∗.
Assuming we have access to φc, we can deﬁne a critic neural network as:

ˆV (x) := θ(cid:62)

c φc(x),

x

∀

∈

K,

(11)

which will serve as an approximation of the optimal value function V ∗ in (8). This critic NN results in an estimated
Hamiltonian:

(cid:16)

H

(cid:17)

ˆV

x, u,

∇

:= θ(cid:62)

c ψ (x, u) + Q(x) + R(u),

(12)

4

which we will use to design the update dynamics of the critic parameters θc. In particular, our goal is to use previously
θ∗
,
recorded data from trajectories of the plant to ensure asymptotic stability of the set of optimal critic parameters
c }
{
while simultaneously enabling the incorporation of instantaneous measurements from the plant. Towards this end, we
will assume enough “richness” properties in the recorded data, a notion that is captured by a relaxed (and ﬁnite-time)
version of persistence of excitation (PE); see [13] and [19].

Assumption 1. Let

ψ (xk, u∗(xk))
{

N
k=1 be a sequence of recorded data, and deﬁne:
}

N
(cid:88)

Λ :=

Ψ(xk, u∗(xk))Ψ(xk, u∗(xk))(cid:62), Ψ(x, u) :=

ψ(x, u)
1 + ψ(x, u)(cid:62)ψ(x, u)

.

There exists λ

∈

k=1
R>0 such that Λ

(cid:23)

λIn, i.e., the data is λ-sufﬁciently-rich (λ-SR).

(13)

(cid:3)

Remark 1. In this paper, we study reinforcement learning dynamics that do not make explicit usage of exploration
signals with standard PE properties, which can be difﬁcult to guarantee in practice. Instead, we assume access to
samples obtained by observing the action of optimal values u∗(xk) acting on the plant. Note however that this does
not imply knowledge of the optimal control policy as a whole, but only of a ﬁnite number of demonstrations from an
“expert” policy. Similar requirements commonly arise in the literature of imitation learning, or inverse reinforcement
learning, and have been recently shown in practice to reduce the exploratory requirements of online reinforcement
learning algorithms, with mild assumptions in the sampling of the demonstrations. For recent discussions on these
topics in the discrete-time stochastic reinforcement learning setting we refer the reader to [20] and [21].

Now, we consider the instantaneous and data-dependent errors of the estimated Hamiltonian with respect to the opti-
mal one:

ei (θc, x, u) := H

x, u,

(cid:16)

(cid:17)

ˆV

H (x, u∗(x),

V ∗)

∇

∇

−
= θ(cid:62)
c ψ (x, u) + Q(x) + R (u) ,
(cid:16)

(cid:17)

xk, u∗(xk),

ˆV

H (xk, u∗(xk),

V ∗)

ed
k(θc) := H
= θ(cid:62)

∇

∇
−
c ψ (xk, u∗(xk)) + Q(xk) + R (u∗(xk)) ,
V ∗) = 0. Moreover, we deﬁne the joint instantaneous and data-dependent

where we used the fact that H (x, u∗(x),
error as:

∇

e (θc, x, u) :=

(cid:32)

ρi

(cid:16)

1
2

ei (x, θc, u)2

1 +

ψ(x, u)
|

|

2(cid:17)2 + ρd

N
(cid:88)

k=1

(cid:16)

1 +

k(θc)2
ed
ψ (xk, u∗(xk))
|
|

2(cid:17)2

(cid:33)
,

(14)

R>0 are tunable gains. Since we are interested in designing real-time training dynamics

c , we compute the the gradient of (14) with respect to θc as follows:

where ρi
for the estimation of the optimal parameters θ∗

R≥0 and ρd

∈

∈

θce(θc, x, u) = ρi

∇


 Ψ(x, u)Ψ(x, u)(cid:62)θc +

ψ(x, u) [Q(x) + R(u)]
(1 + ψ(x, u)(cid:62)ψ(x, u))2







+ ρd


Λθc +

N
(cid:88)

k=1

ψ(xk, u∗(xk)) [Q(xk) + R (u∗(xk))]
(cid:17)2
1 + ψ (xk, u∗(xk))(cid:62) ψ (xk, u∗(xk))

(cid:16)




 ,

(15)

where Λ and Ψ are deﬁned in Assumption 1.
The “propagated” error to the HJB equation that results from the approximate parametrization of V ∗ in (8), is given
by:

(cid:15)HJB(x) := H(x, u∗(x),

V ∗)

∇

−

H

x, u∗,

(cid:32)

(cid:62)

(cid:33)

θ∗
c

∂φc(x)
∂x

5

The following assumption is standard, and it is satisﬁed when the involved functions are continuous and K is compact.

(cid:16)

(cid:15)(cid:62)
c (x)

=

−∇

f (x) + g(x)u∗(x)

(cid:17)

.

(16)

Assumption 2. There exist φc, dφc, (cid:15)c, d(cid:15)c, (cid:15)HJB, g

R>0 such that

∈

φc(x)

| ≤

|

φc,

(cid:15)c(x)

| ≤

|∇

(cid:12)
(cid:12)
(cid:12)
(cid:12)
d(cid:15)c,

∂φc(x)
∂x

(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤
(cid:15)HJB(x)
|

dφc,

(cid:15)c(x)
|
(cid:15)HJB,

| ≤
g(x)
|

| ≤

(cid:15)c,

| ≤

where K is the same set considered in (8).

g

x

∀

∈

K,

(cid:3)

4.1. Critic Dynamics via Data-Driven Hybrid Momentum-Based Control

To design fast asymptotically stable dynamics for the estimate θc, we propose a new class of momentum-based
critic dynamics inspired by accelerated gradient ﬂows with restarting mechanisms, such as those studied in [7] and
[11]. Speciﬁcally, we consider the following hybrid dynamics of the form (1), with state y := (θc, p, τ ) and elements:

Cc := (cid:8)y

Dc := (cid:8)y

∈

∈

R2lc+1 : τ

[T0, T ](cid:9) ,

∈

Fc(y, x, u) :=





2
τ (p

θc)
θce(θc, x, u)

2kc

−

∇

−
1
2

R2lc+1 : τ = T (cid:9) ,

Gc(y) :=



 ,





θc
θc
T0



 ,

(17a)

(17b)

R>0 is a tunable gain, and (p, τ ) are auxiliary states that are periodically reset every time τ = T via the
where kc
∈
> T > T0 > 0. The dynamical system in (17) ﬂows in continuous time according to (17a)
jump map (17b), with
whenever the timer variable τ is in [T0, T ]. As soon as τ hits T , the algorithm (17) resets the timer variable to T0,
as well as the momentum variable p to θc, while leaving θc unaffected. Accordingly, after the ﬁrst reset, the system
exhibits periodic resets every ∆T = 2(T
T0) intervals of time. The following assumption provides data-dependent
tuning guidelines for the resetting frequency of the timer variable τ , which will be leveraged in our stability results.

∞

−

Assumption 3. The tunable parameters (T0, T, kc, ρi, ρd) satisfy 2ρdλ > ρi and

T 2
0 +

1
2kcλρd

< T 2 <

8ρdλ
kcρ2
i

,

where λ is the level of richness of the recorderd data deﬁned in Assumption 1.

For system (17), we study stability properties with respect to the compact set:

∈
The following theorem is the ﬁrst main result of this paper. All the proofs are presented in the Appendices.

A

c :=

θc,p

A
A
×
θc,p := (cid:8)(θc, p)

[T0, T ],

R2lc : pc = θc, θc = θ∗
c

(cid:9) .

(18)

(cid:3)

(19a)

(19b)

Rn,
Theorem 1. Given a number lc of basis functions φc parametrizing the critic NN, and a compact set K
suppose that Assumptions 1, 2 and 3 are satisﬁed. Then, there exists (κ, c)
∞ functions γ1
and γ2, such that for every solution y = (θc, p, τ ) to (17) with initial condition y(0, 0) = (θc(0, 0), p(0, 0), τ (0, 0)),
and using the control policy u(

V on the plant, the critic parameters θc satisfy

R>0 and class-

R>0

⊂

×

K

∈

where ˜u(x(t, j)) := u(x(t, j))

u∗(x(t, j)), for all (t, j)

−

∈
6

κe−c(t+j)

y(0, 0)

|

|Ac

+ γ2 (
|
dom (y)

˜u(x(t, j))

) + γ1((cid:15)HJB),
|

(20)

(cid:3)

)
·
θc(t, j)
|

−

∈ U
θ∗
c | ≤

Figure 1: Proposed Hybrid Momentum Based Dynamics for the training of the Critic subsystem

u∗(x)
The presence of a residual optimal-control mismatch term in (20) of the form γ2(
), represents a crucial
|
|
difference with respect to previous CL adaptive dynamic approaches, such as those studied in [17] and [5, Ch. 4 ].
This term is a direct byproduct of our deﬁnition of ψ in (10), its dependence on the control action u, and its appearance
in the error gradient (15). In principle, the emergence of this term in Theorem 1 is agnostic to the particular gradient-
based update dynamics for the critic NN, regardless of the inclusion or not of momentum. Since γ2
, the larger
the difference between the nominal input u and the optimal feedback law u∗, the greater the residual error in the
convergence of θc. In particular, the bound (20) describes a semi-global practical input-to-state stability property that,
to the best knowledge of the authors, is novel in the context of CL-based RL. In the next section we will show that the
residual error γ2(
|

) can be removed by incorporating an additional actor NN in the system.
˜u
|

u(x)

∈ K

−

Remark 2. In contrast to standard data-driven gradient-descent dynamics for the estimation of the optimal value
function V ∗, which can achieve exponential rates of convergence proportional to λ (cf. [18, 13]), under the assump-
tions of Theorem 1 the critic update dynamics (17) can achieve exponential convergence with rates proportional to
√λ. As shown in [9], momentum-based dynamics of this form can achieve these rates using the restarting parameter
(cid:114) 1

(21)

T = T ∗ := e

+ T 2
0 .

2kcρdλ

This property is particularly useful in settings where the level of richness of the data-set is limited, i.e., when λ
which is common in practical applications.

(cid:28)

1,

that deﬁne the op-
Theorem 1 guarantees exponential convergence to a neighborhood of the optimal parameters
timal value function V ∗. Consequently, by continuity, and on compact sets, ˆV would converge to an (cid:15)-approximation
of V ∗, which can be leveraged by the control law (6) to stabilize system (3). However, as noted in [22], implementing
only critic structures for the control of nonlinear dynamical systems of the form (3) can lead to poor closed-loop
transient performance. To tackle this issue, we consider an auxiliary dynamical system, called the actor, which will
serve as an estimator of the optimal controller that acts on the plant.

{

θ∗
c }

5. Actor Dynamics

Using the optimal value parametrization described in Section 4 the optimal control law can written as:

u∗(x) =

1
2

−

Π−1

u g(x)(cid:62)

(cid:34)

(cid:62)

∂φc(x)
∂x

(cid:35)

θ∗
c +

∇

(cid:15)c(x)

,

x

∀

∈

K.

Therefore, using ∂φc(x)

∂x

and g(x) we can implement an actor neural-network given by:

ˆu(x) = ω(x)(cid:62)θu,

7

(22)

(23)

where ω : Rn

→

Rlc×m is deﬁned as:

Figure 2: Actor Subsystem

ω(x) :=

1
2

∂φc(x)
∂x

−

g(x)Π−1
u .

(24)

To guarantee convergence of ˆu to u∗, we design update dynamics for θu

ε(x, θc, θu) :=

(cid:34)

1
2

α1

εa(x, θc, θu)(cid:62)εa(x, θc, θu)
1 + Tr (ω(x)(cid:62)ω(x))

∈

Rlc based on the minimization of the error:
(cid:35)
+ α2εb(θc, θu)(cid:62)εb(θc, θu)

,

which satisﬁes:

where

εa(x, θc, θu) := ˆu(x)
εb(θc, θu) := θu

−

ω(x)(cid:62)θc = ω(x)(cid:62) (θu

−
θc,

θc) ,

−

θu ε(x, θc, θu) = Ω(x)(θu

θc),

−

∇

Ω(x) := α1

ω(x)ω(x)(cid:62)
1 + Tr (ω(x)(cid:62)ω(x))

+ α2I

∈

Rlc×lc

Rn.

x

∀

∈

Based on these deﬁnitions, we consider the following gradient-descent dynamics for the actor neural-network:

where ku

∈

R>0 is a tunable gain. A scheme representing these update dynamics is shown in Figure 2.

˙θu = Fu(θu, x, θc) :=

ku

−

∇

θu ε(x, θc, θu),

(25)

(26)

(27)

6. Momentum-Based Actor-Critic Feedback System

Consider the closed-loop resulting from the interconnection between the plant (3), the critic update dynamics (17),

the actor update dynamics (27) and the feedback law in (23) shown in Figure 3a, and given by:

˙x = f (x) + g(x)ˆu(x),

˙y = Fc(y, x, ˆu(x)),
˙θu = Fu(θu, x, θc),

x+ = x,
y+ = Gc(y),
θ+
u = θu,

(28a)

(28b)

(28c)

and with ﬂow set and jump set given by C = Rn
×
are as deﬁned in (17). Let z := (x, y, θu) be the overall state of the closed-loop system, and deﬁne:

Rlc and D = Rn

Dc

Cc

×

×

×

Rlc respectively, where Cc and Dc

The following is the main result of this paper.

:=

0
} × A

c

{

A

.

θ∗
c }

× {

8

(a) Closed-Loop System

(b) Convergence of the critic (left) and actor (right) neural networks’ weights to the optimal values.

Figure 3: Closed-Loop System Diagram and Numerical Example

Ky

Theorem 2. Given the vector of basis functions φc : Rn
Kz := K
×
Then, there exists β
z = (x, y, θu) to the closed-loop system (28), with initial condition z(0, 0) = (x(0, 0), y(0, 0), θu(0, 0))
exists ˜T > 0 such that for all (t, j)

Rlc parametrizing the critic NN and a compact set
Rlc, where K is given as in (8), suppose that Assumption 1-3 are satisﬁed.
×
and tunable parameters (ρi, ρd, kc, ku, α1, α2), such that for every solution
Kz, there

⊂
∈ KL

dom(z):

R2lc+1

∈ K

×
, γ

Kθ

Rn

→

×

∈

∈
z(t, j)
|

|A ≤

z(0, 0)
β(
|

|A , t + j) + γ((cid:12)

(cid:0)(cid:15)HJB, d(cid:15)c
(cid:12)

(cid:1)(cid:12)
(cid:12)) + ν,

for all 0

t + j

≤

≤

˜T , and

for some ν > 0 constant.

z(t, j)
|

|A ≤

γ((cid:12)
(cid:12)

(cid:0)(cid:15)HJB, d(cid:15)c

(cid:1)(cid:12)
(cid:12)) + ν,

˜T

∀

≤

t + j,

(cid:3)

0 from any
Theorem 2 establishes asymptotic convergence to a neighborhood of the compact set
compact set Kz modulo some error ν, under a suitable choice of tunable parameters. To the best knowledge of the
authors this is the ﬁrst result providing stability certiﬁcates for continuous-time actor-critic reinforcement learning
using recorded data and accelerated value-function estimation dynamics with momentum.
In addition, since the
resulting closed-loop system in (28) is given by a well-posed hybrid system, the stability results are robust with
respect to arbitrarily small additive disturbances on the states and dynamics [12, Ch. 7].

→

A

as (cid:0)(cid:15)HJB, d(cid:15)c

(cid:1)

7. Numerical Example

In this section, we present a numerical experiment that illustrates our theoretical results. In particular, we study

the following nonlinear control-afﬁne plant:

˙x = f (x) + g(x)u,


f (x) =




−

1
2

(cid:32)

x1

x2

−

−
(cid:16)
1

g(x) :=

(cid:20)

0
cos(2x1) + 2

(cid:21)

,

x1 + x2

cos(2x1 + 2)2(cid:17)

−

(cid:33)




 ,

(29a)

(29b)

(29c)

with local state and control costs given by Q(x) = x(cid:62)x and R(u) = u2 [18]. The optimal value function for this
setting is given by V ∗(x) = 1
(cos(2x1) + 2)x2. Using this
information, we choose φc(x) = (x2
2), and we implement the prescribed hybrid momentum-based dynamics

2 with optimal control law given by u∗(x) =

1, x1x2, x2

1 + x2

2 x2

−

9

02040Time0.00.20.40.60.81.0θc02040Time−0.50.00.51.0θuCriticGradientUpdateCriticMomentum-BasedUpdateθ∗c−

10, 10), θc(0, 0) = (1, 1, 1) and θu

in (17) for the update of the critic neural network, and the update dynamics for the actor described in (27). We obtain
[0, 1]3. We compare the
the results shown in Figure 3b with x(0, 0) = (
results with the case in which the critic neural-network is updated with the gradient-descent dynamics of [17], and
where the sufﬁciently rich data is a set of 16 data points obtained by sampling the dynamics (29) in a grid around the
origin of size 4
4. In our simulations we use T0 = 0.1, T = 5.5 for the momentum-based dynamics in (17). These
particular values are obtained by using the level of richness λ of the data-set, and the inequalities in (18) in order to
ensure compliance with Assumption 3. For both reinforcement learning dynamics we use kc = 1, ku = 1, ρd = 1 and
ρi = 1. As shown in the ﬁgure both update dynamics are able to converge to
c = (1/2, 0, 1) describing
the optimal value function V ∗. However, the hybrid-based dynamics are able to signiﬁcantly improve the transient
performance of the learning mechanism.1

, with θ∗

θ∗
c }

×

∈

{

8. Conclusions

In this paper, we introduced the ﬁrst stability guarantees for deterministic continuous-time actor-critic reinforce-
ment learning with accelerated training of neural network structures. To do so, we studied a novel hybrid momentum-
based estimation dynamical system for the critic NN, which estimates, in real time, the optimal value function. Our
stability analysis leveraged the existence of rich recorded data taken from a ﬁnite number of samples along optimal
trajectories and inputs of the system. We showed that this ﬁnite sequence of samples can be used to train the controller
to achieve online optimal performance with fast transient performance. Closed-loop stability was established using
tools from hybrid dynamical systems theory. Potential extensions include the study of similar accelerated training
dynamics for the actor subsystem, as well as considering reinforcement learning problems in hybrid plants.

References

[1] J. Ibarz, J. Tan, C. Finn, M. Kalakrishnan, P. Pastor, and S. Levine, “How to train your robot with deep reinforcement learning: lessons we

have learned,” The International Journal of Robotics Research, vol. 40, no. 4-5, pp. 698–721, 2021.

[2] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. Al Sallab, S. Yogamani, and P. P´erez, “Deep reinforcement learning for autonomous

driving: A survey,” IEEE Transactions on Intelligent Transportation Systems, 2021.

[3] J. Martinez-Piazuelo, D. E. Ochoa, N. Quijano, and L. F. Giraldo, “A multi-critic reinforcement learning method: An application to multi-tank

water systems,” IEEE Access, vol. 8, pp. 173227–173238, 2020.

[4] K. G. Vamvoudakis, Y. Wan, F. L. Lewis, and D. Cansever, “Handbook of reinforcement learning and control,” 2021.
[5] R. Kamalapurkar, P. Walters, J. Rosenfeld, and W. E. Dixon, Reinforcement learning for optimal feedback control: A Lyapunov-based

approach. Springer, 2018.

[6] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al.,

“Human-level control through deep reinforcement learning,” nature, vol. 518, no. 7540, pp. 529–533, 2015.

[7] W. Su, S. Boyd, and E. Candes, “A differential equation for modeling Nesterov’s accelerated gradient method: Theory and insights,” J. of

Machine Learning Research, vol. 17, no. 153, pp. 1–43, 2016.

[8] A. Wibisono, A. C. Wilson, and M. I. Jordan, “A variational perspective on accelerated methods in optimization,” Proceedings of the National

Academy of Sciences, vol. 113, no. 47, pp. E7351–E7358, 2016.

[9] J. I. Poveda and N. Li, “Robust hybrid zero-order optimization algorithms with acceleration via averaging in continuous time,” Automatica,

vol. 123, 2021.

[10] J. I. Poveda and A. R. Teel, “The heavy-ball ode with time-varying damping: Persistence of excitation and uniform asymptotic stability,” in

2020 American Control Conference (ACC), pp. 773–778, IEEE, 2020.

[11] O’Donoghue and E. J. Cand`es, “Adaptive restart for accelerated gradient schemes,” Foundations of Computational Mathematics, vol. 15,

no. 3, pp. 715–732, 2013.

[12] R. Goebel, R. G. Sanfelice, and A. R. Teel, “Hybrid dynamical systems: modeling stability, and robustness,” 2012.
[13] G. Chowdhary and E. Johnson, “Concurrent learning for convergence in adaptive control without persistency of excitation,” in 49th IEEE

Conference on Decision and Control (CDC), pp. 3674–3679, IEEE, 2010.

[14] R. W. Beard, G. N. Saridis, and J. T. Wen, “Galerkin approximations of the generalized hamilton-jacobi-bellman equation,” Automatica,

vol. 33, no. 12, pp. 2159–2177, 1997.

[15] D. Liberzon, Calculus of variations and optimal control theory. Princeton university press, 2011.
[16] K. Hornik, M. Stinchcombe, and H. White, “Universal approximation of an unknown mapping and its derivatives using multilayer feedforward

networks,” Neural networks, vol. 3, no. 5, pp. 551–560, 1990.

[17] K. G. Vamvoudakis, M. F. Miranda, and J. P. Hespanha, “Asymptotically stable adaptive–optimal control algorithm with saturating actuators

and relaxed persistence of excitation,” IEEE transactions on neural networks and learning systems, vol. 27, no. 11, pp. 2386–2398, 2015.

1The code used to implement this simulation can be found in the following repository: https://github.com/deot95/Accelerated-Continuous-

Time-Approximate-Dynamic-Programming-through-Data-Assisted-Hybrid-Control

10

[18] R. Kamalapurkar, P. Walters, and W. E. Dixon, “Model-based reinforcement learning for approximate optimal regulation,” Automatica,

vol. 64, pp. 94–104, 2016.

[19] K. J. Astrom and B. Wittenmark, Adaptive Control. Addison-Wesley Publishing Company, 1989.
[20] K. Ciosek, “Imitation learning by reinforcement learning,” in International Conference on Learning Representations, 2022.
[21] P. Rashidinejad, B. Zhu, C. Ma, J. Jiao, and S. Russell, “Bridging ofﬂine reinforcement learning and imitation learning: A tale of pessimism,”

Advances in Neural Information Processing Systems, vol. 34, 2021.

[22] K. Doya, “Reinforcement learning in continuous time and space,” Neural computation, vol. 12, no. 1, pp. 219–245, 2000.
[23] C. Cai and A. R. Teel, “Characterizations of input-to-state stability for hybrid systems,” Systems & Control Letters, vol. 58, no. 1, pp. 47–53,

2009.

[24] H. K. Khalil, Nonlinear Systems. Upper Saddle River, NJ: Prentice Hall, 2002.
[25] D. E. Ochoa, J. I. Poveda, A. Subbaraman, G. S. Schmidt, and F. R. Pour-Safaei, “Accelerated concurrent learning algorithms via data-driven

hybrid dynamics and nonsmooth odes,” in Learning for Dynamics and Control, pp. 866–878, PMLR, 2021.

Appendix A. Proof Theorem 1

Appendix A.1. Gradient of Critic Error-Function in Deviation Variables

First, using (16) together with H(x, u∗(x),

V ∗) = 0 for all x, we obtain:

∇
c + Q(x) + R (u∗(x)) = (cid:15)HJB(x).
ψ(x, u∗(x))(cid:62)θ∗

Thus, using (15) and (A.1), we can rewrite the gradient of e(θc, x, u) as follows:

θce(θc, x, u) = Θ(x, u) (θc

∇

θ∗
c ) + υ(cid:15)(x, u) + χ(x, u),

−

Θ(x, u) := ρiΨ(x, u)Ψ(x, u)(cid:62) + ρdΛ,

where

and

υ(cid:15)(x, u) := ρi

2(cid:17)2 + ρd

N
(cid:88)

k=1

ψ(x, u)(cid:15)HJB(x)

(cid:16)

1 +

|

ψ(x, u)
|
(cid:104) ∂φc(x)

ψ(xk, u∗(xk))(cid:15)HJB(xk)
(cid:16)

1 +

ψ(xk, u∗(xk))
|
(cid:105)(cid:62)

2(cid:17)2 ∈
|

Rlc ,

ρiψ(x, u)

∂x g(x) (u

u∗(x))

θ∗
c

(cid:16)

1 +

ψ(x, u)
|

−
2(cid:17)2
|

ρiψ(x, u) [R(u)

+

(cid:16)

1 +

R(u∗(x))]
−
2(cid:17)2
ψ(x, u)
|
|

χ(x, u) :=

(A.1)

(A.2)

(A.3)

(A.4)

Rlc ,

∈

(A.5)

which, by using the fact that

r
(1+r2)2 ≤

√
3
3
16 ,

r
∀

∈

R≥0, satisfy:

(cid:15)HJB (ρi + N ρd) ,

υ(cid:15)(x, u)
|

| ≤

χ(x, u)
|

| ≤

3√3
16
3√3
16

ρi

(cid:32)

g (cid:0)dφc [1 +

θ∗
c |
|

] + d(cid:15)c

(cid:1)

u

|

−

u∗(x)
|

+ λmax (Πu)

u
|

−

u∗(x)

2
|

(A.6a)

(A.6b)

(cid:33)
.

The following Lemma will be instrumental for our results.

Lemma 1. If the data is λ-sufﬁciently-rich, then there exist Θ, Θ

R>0 such that

∈

ΘIn

Θ(x, u)

ΘIn

x

Rn,

u

Rm.

(cid:22)
Rlc be arbitrary. Since, by assumption, the data is λ-SR it follows that:

(cid:22)

∈

∈

∀

∀

Proof. Let θ

∈

θ(cid:62)Θ(x, u)θ = θ(cid:62)ρiΨ(x, u)Ψ(x, u)(cid:62)θ + θ(cid:62)ρdΛθ

ρdλ

2
|

θ
|

≥

11

where Θ := ρdλ. On the other hand, using the fact that (cid:12)
(cid:12)Ψ(x, u)Ψ(x, u)(cid:62)(cid:12)
(cid:12)

(cid:12) =

a

Rn, we obtain that:

2
Ψ(x, u)
|

|

≤

|
1,

∀
∈
(x, u)

∀

Rn

×

Rm,

∈

Θ(x, u)

=

⇒

(x, u)

Rn

∈

×

Rm,

(A.7)

ΘIlc,

(cid:23)
(cid:12)aa(cid:62)(cid:12)

(cid:12) =

∀
2 ,
a
|

we obtain:

θ(cid:62)Θ(x, u)θ = θ(cid:62)ρiψ(x, u)ψ(x, u)(cid:62)θ + θ(cid:62)ρdΛθ
(ρi + ρdλmax (Λ))
|
ΘIlc,

Θ(x, u)

(x, u)

≤
=

θ

2

|

Rn

⇒

(cid:22)

∀

∈

where Θ := ρi + ρdλmax (Λ).

Appendix A.2. Lyapunov-Based Analysis

Rm,

×

(cid:4)

Recall from Section 4 that y = (θc, p, τ ), suppose that the assumptions of Theorem 1 hold and consider the

Lyapunov candidate function Vc : Rlc

Rlc

R>0

R≥0 given by:

Vc(y) := |

p

×
θc
−
4

×

2
|

p

+ |

→
θ∗
c |
−
4

2

+ kcρdτ 2 (θc

−

θ∗
c )

Λ (θc
(cid:62)
2

θ∗
c )

,

−

where Λ was deﬁned in Assumption 1 and which satisﬁes:

2
y
c
Ac ≤
|
|
kcρdT 2
0 λ
2

(cid:26) 1
4

,

c := min

Vc(y)
(cid:27)

,

≤
c :=

,

2
y
c
Ac
|
|
(cid:26) 3
1
2
4

,

(cid:0)1+kcρdT 2λ(cid:1)

(cid:27)

,

where λ := λmax (Λ). Now, let u
of the critic subsystem, i.e., ˙Vc =
˙Vc can be shown to satisfy

V , and consider the time derivative of Vc along the continuous-time evolution
∈ U
yVc(y)(cid:62) ˙y. Then, by using (A.2) and Lemma 1, and some algebraic manipulation,
∇

˙Vc

(cid:0)

p
|

θc

|

−

≤ −

θc
|

−

(cid:1) M (τ )

θ∗
c |

(cid:18)

p
|
θc
|

−
−

(cid:19)

θc
|
θ∗
c |

+ 2√2kcyAc

(cid:0)

υ(cid:15)(x)
|
|

+

|

χ(x, u(x))

(cid:1),

|

where

M (τ ) :=

(cid:18) 2

ρi
kcτ 2 −
2
ρi
Θ
2
−

(cid:19)

,

c was deﬁned in Section 4. Since 2ρdλ > ρi and T 2 < 8ρdλ
kcρ2
i

and
A
[T0, T ],
ρi
Θ
2 . Hence, from (A.10) and using (A.6), we obtain that:

(t, j)

∈

∀

−

dom (y) by construction of the critic update dynamics (17), it follows that M (τ )

by means of Asssumption 2, and τ (t, j)

∈
r with r :=

(cid:23)

˙Vc

r
≤ −

y
|

|

2
Ac

+

y
|

|Ac

(cid:16)

u(x)
γν ((cid:15)HJB) + γχ (
|

−

u∗(x)

(cid:17)

,

)
|

(A.12)

where γν, γχ

∞ are given by:

∈ K

γν(r) :=

cχ :=

3√6
8
3√6
8

(ρi + N ρd) r,

γχ(r) := cχ(r + r2),

ρi max (cid:8)g (cid:0)dφc [1 +

] + d(cid:15)c

θ∗
c |

|

(cid:1) , λmax (Πu)(cid:9) .

Thus, letting dc

∈

(0, 1), and using (A.9), (A.12):

˙Vc

≤ −

r(1

dc)

−
c

Vc(y),

y
∀ |

|Ac ≥

(cid:16)

1
dc
12

γν ((cid:15)HJB) + γχ (
|

u(x)

−

u∗(x)

(cid:17)

.

)
|

(A.13a)

(A.8)

(A.9)

(A.10)

(A.11)

On the other hand, the change of Vc during the jumps in the update dynamics for the critic (17), satisﬁes:

(cid:0)y+(cid:1)

Vc

Vc(y)

−

ηVc(y),

≤ −

(A.14)

T 2
0
T 2 −

1

with η := 1
(0, 1) by means of Assumption 2. Together, (A.13) and (A.14),
in conjuction with the quadratic bounds of (A.9), imply the results of Theorem 1 via [23, Prop 2.7] and the fact that
(cid:4)
θc(t, j)
|

2kcρdλT 2 which satisﬁes η

(θc(t, j), p(t, j))

θ∗
c | ≤ |

for all (t, j)

|Ac ≤ |

dom (y).

|Aθc ,p

y(t, j)

−

−

∈

∈

Appendix B. Proof of Theorem 2

Appendix B.1. Gradient of Actor Error-Function in Deviation Variables

First, note that we we can write (25) as:

θu εa(x, θc, θu) = Ω(x) (θu

∇

θ∗
c −

−

(θc

−

θ∗
c )) ,

and consider the following Lemma, instrumental for our results.

Lemma 2. There exists Ω, Ω

R>0 such that

∈

ΩIlc (cid:22)

Ω(x)

(cid:22)

ΩIlc.

Proof. Let θ

∈

Rlc be arbitrary. Then, by the deﬁnition of Ω : Rn

Rlc×lc in (26), it follows that:

→

θ(cid:62)Ω(x)θ = α1

2

(cid:12)ω(x)(cid:62)θ(cid:12)
(cid:12)
(cid:12)
1 + Tr (ω(x)(cid:62)ω(x))

+ α2

2
|

θ
|

α2

θ
|

|

≥

2 =

⇒

Ω(x)

ΩIlc,

(cid:23)

Rn,

x

∀

∈

where Ω := α2. On the other hand, we obtain:

(cid:32)

θ(cid:62)Ω(x)θ =

α1

(cid:33)

+ α2

ω(x)
|
1 +

2
|
ω(x)
|

2
F
|

2
|

θ
|

Ω

2 =
|

⇒

θ
|

≤

Ω(x)

ΩIlc,

(cid:22)

Rn,

x

∀

∈

A

where Ω := α1 + α2,
r
∀

r2
1+r2 ≤
Now, consider the Lyapunov function

R.

∈

1

|

|F represents the Frobenius norm and where we used

(z) := Vo(x) + Vc(y) + Va(θu),

V
Vo(x) := V ∗(x),

Va(θu) :=

1
2 |

θu

2 ,

θ∗
u|

−

A
|

| ≤ |

A

|F ,

A

∀

∈

Rlc×lc and
(cid:4)

(B.1a)

(B.1b)

where Vc was deﬁned in (A.8) and where we recall that z = (x, y, θu). By [24, Lemma 4.3], and since Vo = V ∗ is a
continuous and positive deﬁnite function in Rn, there exist γ
). Hence,
o
|
is in turn of class
using (A.9), and the fact that sum of class

Vo(x)
≤
such that:

, there exist γ

such that γ

γo(

x
|

≤

, γo ∈ K
K

o

(
x
)
|
|
, γV ∈ K

V

K

Now, the time derivative of ˙Vo =

∇

γ

V

z

(
|

|A)

(z)

z
γV (
|

|A)

≤

≤ V

Vo(x)(cid:62) ˙x along the trajectories of (28) satisﬁes:

˙Vo

Q(x) +

≤ −

(cid:0)Π−1
g2λmax
2

u

(cid:1)

(cid:0)dφc |
θ∗
c |

+ d(cid:15)c

(cid:1) (cid:0)dφc

θu
|

−

θ∗
c |

+ d(cid:15)c

(cid:1) .

On the other hand, making use of Lemma 2, for the time derivative of ˙Va =

θu Va(θu)(cid:62)θu we obtain:

˙Va

kuα2

≤ −

θu

|

−

θ∗
c |

2 + kuΩ

θu

|

−

13

∇
θc

θ∗
c | |

.

θ∗
c |

−

(B.2)

(B.3)

(B.4)

Hence, using (A.10), (B.3), and (B.4), together with the upper bounds in (A.6), we obtain that the time derivative of

along the trajectories of the closed-loop system satisﬁes:

V

˙
V ≤ −

+ cy

Q(x)

−
y
|Ac
|
+ cyu2

r

y

2
Ac −
|
|
θu
+ cu
|
θu

|Ac |

y
|

θu
kuα2
|
−
θ∗
+ cyu
c |
2 + c0,
θ∗
c |

−

−

2
θ∗
c |
θu
|

−

θ∗
y
c | |

|Ac

(B.5)

where

cy :=

3√6
8

(cid:32)

kc

(cid:15)HJB (ρi + N ρd) +

(cid:34)

g2ρi

λmax

1
2

(cid:0)Π−1

u

(cid:1) (cid:0)dφc [1 +

θ∗
c |
|

] + d(cid:15)c

(cid:1) d(cid:15)c + λmax (Πu) λmax

(cid:0)Π−1

u

(cid:1)2

2

d(cid:15)c

(cid:35)(cid:33)

,

+ d(cid:15)c

(cid:1) g2λmax

(cid:0)Π−1

u

(cid:1) dφc,

2kuΩ + g2ρiλmax

(cid:0)Π−1

u

(cid:1) (cid:0)dφc [1 +

θ∗
c |
|

] + d(cid:15)c

(cid:33)
,

(cid:1) dφc

kcg2ρiλmax (Πu) λmax

(cid:0)Π−1

u

(cid:1)2

dφc

2

,

+ d(cid:15)c

(cid:1) g2λmax

(cid:0)Π−1

u

(cid:1) d(cid:15)c.

cu :=

cyu :=

cyu2 :=

c0 :=

1
(cid:0)dφc |
θ∗
c |
2
(cid:32)
3√6kc
16

3√6
16
1
2

(cid:0)dφc |
θ∗
c |
θu
|

−

Then, for all
bounded as:

θ∗
c | ≤

cyu
cyu2

, by using Q(x) = x(cid:62)Πxx and letting d1

(0, 1), from (B.5), ˙
V

∈

can be further upper

˙
V ≤ −

λmin (Πx)

+ cy

−

y

|
(cid:0)

|Ac
y
|

|Ac

x

2
|
|
+ cu

(1

(cid:16)

r

d1)

−
θu

−

−
θ∗
+ c0
c |
(cid:18) d1r
(cid:1)
cyu

−
θ∗
c |

|
θu

|

−

y
|

2
Ac
|

+ kuα2

θu

|

−

θ∗
c |

2(cid:17)

cyu
−
d1kuα2

(cid:19) (cid:18)

|

(cid:19)

.

y
|
θu

|Ac
θ∗
c |
−

c2
yu
d2
1kuα2

≥
(cid:26) c0
2dyu

(cid:27)

,

,

2dyu
d2dz

θu

|

−

θ∗
c | ≤

cyu
cyu2

,

(B.6)

(B.7a)

Now, pick a set of tunable parameters (ρi, ρd, kc, ku) such that r

so that from (B.6), we obtain:

˙
V ≤ −

(1

−

d2)dz

z
|

2
A ,
|

z
∀ |

|A ≥

max

with

dz := min
λmin (Πx) , (1
{
dyu := max
2cyu, c0

,

−
d2

d1)r, kuα2
(0, 1).

,

}

}
Notice that for every compact set Kθ of initial conditions for θu we can pick suitable ρi, ρd, α1, α2, kc, ku to satisfy
Kθ. Now, during jumps x and θu do not
Kθ
change, and hence

cyu
cyu2 B such that (B.7) holds for every trajectory with θu(0, 0)

satisﬁes:

⊂

∈

∈

{

V

(z+)

V

− V

(z) = Vc(y+)

Vc(y)

−

ηVc(y).

≤ −
during ﬂows outside a neighborhood of

(B.8)

The result of the theorem follows by using the strong-decrease of
scribed in (B.7), the non-increase of
are a well-posed HDS which experiences periodic jumps followed by intervals of ﬂow of length T
[25]), and by following the same arguments of [12, Prop 3.27] and [23, Prop. 2.7].

de-
during jumps given in (B.8), by noting that, by design, the closed-loop dynamics
T0 > 0 (c.f.
(cid:4)

A

−

V

V

14

