0
2
0
2

t
c
O
9
2

]

G
L
.
s
c
[

1
v
1
4
4
5
1
.
0
1
0
2
:
v
i
X
r
a

Self-awareness in intelligent vehicles: Feature

based dynamic Bayesian models for abnormality
detection

Divya Thekke Kanapram 1,3, Pablo Marin-Plaza 2, Lucio Marcenaro 1, David
Martin, 2 Arturo de la Escalera 2, Carlo Regazzoni1

1Department of Electrical, Electronics and Telecommunication Engineering and
Naval Architecture, University of Genova, Italy. divya.thekkekanapram@edu.unige.it,
{carlo.regazzoni,lucio.marcenaro}@unige.it
2Intelligent Systems Lab, Universidad Carlos III, Leganes Spain. {pamarinp,
dmgomez, escalera}@ing.uc3m.es
3Centre for Intelligent Sensing, School of Electronic Engineering and Computer
Science (EECS), Queen Mary University of London, UK.

Abstract. The evolution of Intelligent Transportation Systems in re-
cent times necessitates the development of self-awareness in agents. Be-
fore the intensive use of Machine Learning, the detection of abnormalities
was manually programmed by checking every variable and creating huge
nested conditions that are very diﬃcult to track. This paper aims to in-
troduce a novel method to develop self-awareness in autonomous vehicles
that mainly focuses on detecting abnormal situations around the consid-
ered agents. Multi-sensory time-series data from the vehicles are used to
develop the data-driven Dynamic Bayesian Network (DBN) models used
for future state prediction and the detection of dynamic abnormalities.
Moreover, an initial level collective awareness model that can perform
joint anomaly detection in co-operative tasks is proposed.
The GNG algorithm learns the DBN models’ discrete node variables;
probabilistic transition links connect the node variables. A Markov Jump
Particle Filter (MJPF) is applied to predict future states and detect
when the vehicle is potentially misbehaving using learned DBNs as ﬁlter
parameters.
In this paper, datasets from real experiments of autonomous vehicles
performing various tasks used to learn and test a set of switching DBN
models.

Keywords: Intelligent Transportation System (ITS), Autonomous ve-
hicles, Dynamic Bayesian Network (DBN), Hellinger distance, Abnor-
mality detection.

 
 
 
 
 
 
2

1

D.T. Kanapram, P. Marin-Plaza, L. Marcenaro et al.

Introduction

Due to the rise of the population in the cities, the vehicles grow exponentially,
leading to congestion and pollution. Consequently, road accidents increase dra-
matically due to numerous reasons such as lack of contextual data, distracted
and reckless driving, adverse weather conditions, animal crossing, unsafe lane
changes, etc. These factors show the importance of making the vehicles ”self-
aware” in order to ensure safety in driving. The future autonomous vehicles will
use these self-awareness processes to solve diﬀerent tasks in diﬀerent environ-
ments that never were considered previously for decision making or problem-
solving, such as trajectory planning or collision avoidance, or abnormality de-
tection. The new learning capabilities from observing the environment or the
autonomous vehicles’ own states of behavior create new possibilities for problem-
solving in new situations of the real traﬃc scenarios. So, developing self-aware
models will improve the general decision and the navigation in the autonomous
vehicles facilitating the improvement of the incremental self-capabilities, such as
fault-tolerant decisions based on own perception or communication capabilities
in dynamic environments.

Self-awareness (SA) is a broad concept that describes the cognitive property
of an agent. In the case of artiﬁcial agents like intelligent vehicles (IVs) [1], the
concept of SA is an ability to observe themselves and the surrounding environ-
ment through the various exteroceptive and proprioceptive sensors and process
the sensory data to learn and maintain a contextual representation of the system
[2]. Nowadays, the emergent techniques and algorithms in Machine learning allow
for the learning of data-driven models that can provide self-awareness function-
alities. Self-awareness functionality can be extended to ’collective self-awareness’
by utilizing the shared data between the agents and learn models from the multi-
sensory data collected from agents performing co-operative tasks. However, the
in-depth analysis of communication schemes is beyond the scope of this paper.
Here we have shown the co-operative communication scheme used in the experi-
mental scenarios to collect data sets and an initial level collective self-awareness
model to represent agents’ joint tasks.

Dynamic Bayesian Networks(DBNs) are probabilistic models representing
the multi-sensory temporal data sequences at diﬀerent abstraction levels [3]. In
this work, we have used a speciﬁc category of DBN models called Switching Lin-
ear Dynamic Systems (SLDS) [4]. In SLDS, a sequential combination of linear
dynamic models can be used to represent a non-linear dynamic model. The dis-
crete variables in the higher levels of the SLDS represent switching variables (or

Self-awareness in IV: Feature based abnormality detection

3

superstates). These switching variables are associated with the linear dynamic
models deﬁned in the DBN’s continuous state level.

The proposed methodology for learning the data-driven models for agents’
self-awareness functionality has been veriﬁed with the data sets collected from
autonomous vehicles performing co-operative and individual driving tasks of
diﬀerent scenarios.

The main contributions of this paper can be summarized as follows:

1. A method to learn switching Dynamic Bayesian Network (DBN) models
from the pairs of exteroceptive and proprioceptive sensory data sequences. Such
learned models can detect abnormal behaviors in real-time (online phase) by
testing the models with the data sets extracted from diﬀerent experiences.

2. Performance analysis of the pair based DBN models was conducted to
check the best pair based feature to detect abnormal situations in the surround-
ing environment. Considered the data sets collected from co-operative driving
scenarios along with a scenario consists of one agent.

The remainder of this paper is organized as follows. Section 2 presents a
survey of the related work. In section 3, the proposed method is described by
deﬁning principles exploited in the training phase and the anomaly detection
steps involved in the model the test phase. Section 4 summarizes the experimen-
tal setup in addition to the description of the research platform used. Section 5
presented abnormality measurement results of model testing, made analysis, and
comparison. Finally, section 6 concludes the paper by including possible future
research lines.

2 State of the art

This section explains some of the related work regarding the development of self-
awareness in agents. Over many years, self-awareness has been studied in multiple
research disciplines, such as cognitive sciences, psychology, and philosophy [5, 6,
7, 8]. The concept of self-awareness is widely studied in biology, which has been
reproduced in artiﬁcial systems to enrich the capability of autonomy in diﬀerent
ﬁelds, including machine learning and robotics [9, 10]. Moreover, in [11, 12], the
diﬀerent aspects of self-awareness are discussed.
Here we consider DBNs learned using sensorial data recorded during training
experiences as generative models capable of allowing a SA agent to predict states
in future similar testing experiences. Additional probabilistic inference features
related to DBN models allow the SA of the agent to detect possible abnormalities
in new experiences. Prediction, estimation, and abnormality detection are the

4

D.T. Kanapram, P. Marin-Plaza, L. Marcenaro et al.

emergent (i.e., data-driven) SA features discussed in this paper as a collective
property of a set of agents aiming to perform the same task.

Developing self-awareness in agents has been shown to help an agent in-
crease conﬁdence when executing autonomously tasks and explainability of its
own actions in terms of emergent SA models learned. Human eﬀorts in diﬀerent
areas can take advantage, and in some ﬁelds, this can increase conﬁdence in
autonomous systems so allowing the human to reduce its overcontrol work. An
Intelligent vehicle [13] can be seen as a straightforward example of an agent: it
perceives information from the surrounding environment and uses obtained in-
formation to make decisions autonomously in diﬀerent situations. However, this
does not always imply that the IV can explain to itself and to the human user
the causal sequence of events that carried it to make decisions. Self-awareness
addresses within Artiﬁcial Intelligence the set of techniques/models that allow
agents/machines to describe the relationship between perceptions and actions
the agent has to do to perform a task. In this context, Machine Learning and
Deep Learning, etc. are increasingly used to obtain SA models in a data-driven
way, and self- driving vehicles [14] can beneﬁt from such methods. Machine
Learning techniques capable of dealing with uncertainty to learn the SA model
from multisensory signals coming from the vehicle’s sensors are particularly use-
ful. Such models can be of the generative type, so allowing predictions of future
or lower-level states of the agent in consideration to be made when analyzing
new data sequences.

In recent years, the research in intelligent and autonomous vehicles occupies
a prominent place in the ﬁeld of ITS. In [15], the authors propose an approach
to develop a multilevel self-awareness model learned from an agent’s multisen-
sory data. Such a learned model allows the agent to detect abnormal situations
present in the surrounding environment. In another work [16], the learning of
self-awareness models for autonomous vehicles is based on the data collected by
diﬀerent maneuvering tasks performed by a human driver. In this work, visual
perception and position data are used as modalities, and the cross-correlation
between diﬀerent modalities is analyzed for detecting abnormal situations. On
the other hand, in [17], the authors propose a new architecture for mobile robots
with a model for risk assessment and decision making when detecting diﬃcul-
ties in the autonomous robot design. In [18], the authors proposed a model of
driving behavior awareness (DBA) that can infer driving behaviors such as lane
change. In [19], an approach to detect abnormalities in dynamic systems by the
models learned from the diﬀerent features of an agent is presented. Moreover,
it examines the most precise model to detect abnormal situations. However, it

Self-awareness in IV: Feature based abnormality detection

5

involves one agent and doesn’t highlight the issue of co-operative driving.
In most of the related works, either the data from one entity is used, or the
objective was limited; for example, in [18], the aim was to detect lane changes
either on the left or right side of the considered vehicles. In this work, we have
considered the data from the real vehicles and developed pair based switching
DBN models for each vehicle and, ﬁnally, obtained the performance comparison.
Moreover, shown an initial level collective DBN model and performance com-
parison has been made with the results obtained from single pair based DBN
models.

3 Proposed method

This section discusses how to model “intelligence” and “awareness” into vehicles
to generate “ Self-aware, intelligent vehicles.” Such intelligent vehicles should be
provided of sensors to perceive internal as well as external states. Intelligence in
this context is often related to the capability of using perceptions to allow the
vehicle to adapt to variations in external situations. However, the system cannot
be considered self-aware until the vehicle is not provided with the capability to
observe perception and actions it has successfully done until a given moment.
Then, organize them in a data-driven way into models capable to statistically
predict at diﬀerent hierarchical levels future states in case a similar situation
and task to perform once again. This prediction capability should be inherent to
the SA model, which has to be capable of deriving conditional models based on
available knowledge (e.g., state at a previous instant) to derive predictions (e.g.,
future state). Models that have this capability are called generative models [20].

The concept of anomaly arises when the SA model has a further inference
capability: it can estimate whether current observations are in line or not with
predictions of variables at whatever level in the model. This work uses this frame-
work to deﬁne abnormality: a dynamic anomaly is found by a SA model when
the states the model predicts mismatch the current sensory observations. In this
sense, the emergent self-awareness property in vehicles derives from two aspects:
1) to learn generative SA models from normal sensor experiences (Dynamic
Bayesian Network (DBN) models are here used); 2) to associate to such models
a general inference mechanism capable of performing predictions and of detect-
ing dynamic anomalies. The latter capability can be deﬁned as self-evaluation
of models available in the vehicle: if a given SA model has the capability of pre-
dicting the future states eﬃciently, i.e., the ground truth observations from the
vehicle’s sensors conﬁrm predictions, than the vehicle is ensured that a certain

6

D.T. Kanapram, P. Marin-Plaza, L. Marcenaro et al.

level of homeostasis (dynamic stability) will occur in next moments performing
the same actions it did in past experiences.

The capability of understanding when homeostasis conditions cannot be met
with available models, i.e., anomaly detection, is so an important self-awareness
step to allow an agent to start making decisions based on diﬀerent strategies than
normal known ones. The intelligent vehicles used in this work are equipped with
one lidar of 16 layers and 360 degrees of Field of view(FOV), a stereo camera,
and position/control encoder devices to make available information about the
internal and position state during diﬀerent tasks being performed. In this work,
we mainly concentrated on such latter low dimensional multi-sensory observed
data: namely the position (odometry) and control signals of the vehicle (e.g.,
steering, power, and rotor velocity ). In this work, position data are considered
exteroceptive information as the relative position is used with respect to the
environment around it. This information can be obtained, e.g., by GPS or by
Localization techniques using exteroceptive sensors like lidar and cameras. How-
ever, here we consider as already available in low dimensions (coordinates with
respect to environment) such data as extracted by sensor ad hoc algorithms.
On the other hand, the agents’ control signal is here used as an example of
proprioceptive sensory data.

Considering directly low dimensional data sensor observations makes it easier
here to analyze how to learn pair-based switching DBN models, i.e., the genera-
tive models here used, and to associate anomaly detection tools to them. Exten-
sion to higher dimensional data (i.e., directly mapping lidar and video cameras
observations as random variables into DBNs) requires methods based on prob-
abilistic versions of tools like Variational Autoencoders(VAEs) and Generative
Adversarial Networks(GANs) to be studied in detail to allow joint learning of
mapping of high dimensional sensory observations into DBN states and predic-
tion models as shown in [21, 22]. The extension to such high dimensional sensors
of the SA model goes beyond the scope of this paper. Instead, here, we use
low dimensional sensory data to explore the simultaneous SA awareness onto
multiple vehicles jointly cooperating in a task. This case shows how collective
awareness can rise as a direct extension of appropriate self-awareness models of
individual agents in each vehicle.

The proposed method is divided into two phases: oﬄine training and online
testing. A block diagram representation is shown in Fig. 1. In the oﬄine train-
ing phase, the vehicles learn probabilistic ﬁltering models from the multisensory
data sequence while they perform a reference situation task. Each vehicle learns
a set of models from the diﬀerent combinations of the vehicle’s control and po-

Self-awareness in IV: Feature based abnormality detection

7

sition information. In the online test phase, each DBN model inside a vehicle
is used as parametric knowledge of a ﬁlter that makes inference on the corre-
sponding sensory data. Each ﬁlter is provided with an additional computational
method that allows it to estimate its own level of ﬁtness(anomaly detection) by
comparing the states it can generatively predict with the sensors’ current obser-
vations. The abnormality occurs in the environment are detected as deviations
from expected modeled behaviors. A ﬁlter called Markov Jump Particle Filter
(MJPF) [23, 24] is applied to the learned DBN models to perform the ﬁltering
operation of prediction and update. The ﬁlter is modiﬁed to allow it to also es-
timate abnormalities at diﬀerent abstraction levels as a required SA capability.

3.1 Oﬄine training phase

In this phase, the considered agents (i.e., vehicles) learn the vocabulary and
dynamic/hierarchical models of the switching DBN from the observed extero-
ceptive and proprioceptive sensorial data that originate from the current agent’s
experience deﬁning their normal behavior.

DBN models are of a generative type, and learning them implies learning the
conditional probabilities that relate their variables. Therefore, observing a se-
quence of data and learning conditional functions within the DBN will allow the
agent to predict variables not yet observed based on evidence collected until that
moment. This makes the model appropriate for capturing causal relationships
at the basis of SA.

The grey shaded area in Fig 1 shows the block diagram representation of
the training phase. The various steps involved in switching DBN model learning
have been described below.

Estimation of Generalized states
The method assumes a synchronization operation is available on the considered
multisensory data. Exteroceptive based position data and three pairs of vehicle
control signals are considered in this paper as a case study. An initial generalized
ﬁlter [25] is applied to the sequence of data variables to estimate so-called gen-
eralized states (GSs). Generalized states are the variables here used to describe
anomalies with respect to to a given DBN model. Using GSs allows the agent
to estimate a new DBN model capable of predicting in a better way in future
those anomalies coming from sequences statistically similar to the one that has
generated such anomalies. Generalized states are generalized coordinates that
allow each variable in the DBN to describe its own dynamics as part of the

8

D.T. Kanapram, P. Marin-Plaza, L. Marcenaro et al.

F
i
g
.
1
:

B
l
o
c
k

d
i
a
g
r
a
m

:

t
r
a
i

i

n
n
g

a
n
d

t
e
s
t

p
h
a
s
e

o
f

s
w
i
t
c
h

i

n
g
D
B
N
m
o
d
e
l

Observations from vehiclePre-processingof dataGeneralized error estimation using initial filterClustering by GNGsState transition matrix estimationDBN models learnedDBN 2(S-P)DBN 3(V-P)DBN 4(S-V)Training phase: Vehicle iCab N(offline)OdometrySteering angle-power(S-P)MJPF 1MJPF 2MJPF 4Abnormality measurements: X-Y,S-P, V-P, S-VFuture trajectoriesPreprocessing of observed dataTest phase: Vehicle iCab N(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:3)(cid:6)(cid:7)Ground truth observations (pair based features) : X-Y, S-P, V-P, S-VOdometry(X-Y positions)Steering angle (S)Velocity (V)Power (P)Steering angle-velocity(S-V)Velocity-power(V-P)DBN 1(X-Y)MJPF 3Odometry(X-Y)Steering angle(S)Velocity (V)Power (P)GNG 1GNG 2...GNG nGNG 1GNG 2...GNG nGNG 1GNG 2...GNG nGNG 1GNG 2...GNG nSelf-awareness in IV: Feature based abnormality detection

9

random information. Therefore, conditional probabilities in a DBN formed by
GS variables allow predictions of the dynamic evolution of states such variables
represent with advantages in the inference process.

The generalized state of a continuous state variable associated with a given

sensory data c can be deﬁned as:

˜X c
tk

= [X c
tk

˙X c
tk

¨X c
tk

· · · X c,(L)
tk

](cid:124),

where (L) indicates the L-th time derivative of the state.
The l-th time derivative of GS at the time tk can be written as:

X (l)
tk

=

X (l−1)c
tk

− X (l−1)c
tk−1
∆tk

,

(1)

(2)

where X (0)
tk

= Xtk and ∆tk is the uniform sampling time for all variables
considered. In this work, the derivatives inside the generalized state(GS) are
limited L=1, i.e., only the state and its derivative are included in the GS. This
is because we used a DBN with limited memory (a two-slice DBN) so that
dynamical models depend directly only on variables in the previous slice. In
the case of low dimensional observations (and consequently states), this implies
that local interslice dynamic models can be considered as well approximated by
linear equations between GSs. So the model learned with GSs can capture the
considered agent dynamics as piecewise linear. DBNs with longer memories imply
higher-order derivatives so that additional learning tools would be required with
respect to those here discussed. In the future, the work here presented could be
extended by including higher-order GSs.

In a vehicle agent, an initial DBN has to be assumed as known that embeds
basic dynamic conditions. In this case, a DBN is used that assumes that there
will not be any force acting on the agent between two consecutive time instants
but random Gaussian perturbations. The corresponding ﬁlter can be deﬁned in
terms of GSs: the state vector at tth
k+1 time instance will remain same w.r.t the
state vector at tth
k . So that the ﬁlter produces derivatives of states as errors. So
GSs estimated by such an initial ﬁlter represent jointly for each given state X,
errors, i.e., derivatives of X. GSs can therefore be seen as pairs (state, error)
that are produced in this case by an initial ﬁlter associated with initial DBN.
Mainly we have considered four pairs of GSs data for DBN model learning,
such as odometry(X − Y ), steering-power (S − P ), velocity-power (V − P ), and
steering-velocity (S − V ).

10

D.T. Kanapram, P. Marin-Plaza, L. Marcenaro et al.

Construction of discrete level of DBN model
This section explains the process involved in learning the vocabularies of dis-
crete level variables, as well as dynamic models, transition probabilities, and
co-occurrence matrices between variables of the DBN model, i.e. all conditional
probabilities that can decompose the joint probability of the DBN in a set of
causally meaningful conditional probabilities, making it a generative model.

This work considers a hierarchical switching 2-Time Slice DBN (2T-DBN)[26]
as a generative ﬁlter that can predict future states of the considered entities
(i.e., vehicles). Fig.2 shows the representation of the DBN model, continuous
level constitutes the generalized states, and discrete level represents the hierar-
chically higher semantic vocabulary and correlation among them. Each of the
discrete level variables belongs to a particular dynamical model in the continuous
state space. Thus, the considered switching DBN model can represent non-linear
dynamics by a set of linear dynamic models.

To model the discrete level of the DBN (orange shaded area in Fig. 2), the
discrete switching variables that represent the meaningful information have to
be learned from the generalized states estimated from the outcomes of the initial
ﬁlter on a given data sequence. A clustering algorithm called Growing Neural
Gas (GNG) [27] is used to group generalized states samples collected as output
of a previous ﬁlter associated with a DBN (in this case, the initial ﬁlter). Each
cluster of samples deﬁnes a diﬀerent switching variable of the newly learned
DBN.

In general, clustering can be described as the process of organizing a collec-
tion of k-dimensional vectors into groups whose members share similar features
in some way. A k-dimensional vector represents each one of such groups called
a code vector (other names used are centroid and node). There are many algo-
rithms available for clustering: K-means [28], Self Organising Map(SOM) [29],
Neural Gas(NG) [30], Growing Neural Gas(GNG) [27], Density-based spatial
clustering of applications with noise (DBSCAN)[31], etc. The SOM algorithm
can compress large multidimensional datasets into a ﬁxed number of represen-
tative units. However, the dimension of the representative units (clusters) needs
to be deﬁned before, and it may sometimes cause not intuitive for representing
the characteristics of data structure.

In contrast to the SOM, GNG is an unsupervised, adaptive, and incremen-
tal neural network that learns topologies; it grows during the learning process
and does not require users to deﬁne the number of representative units called
nodes beforehand. Such a dynamic property is an advantage over other cluster-
ing algorithms for using it in many applications. DBSCAN is a density-based

Self-awareness in IV: Feature based abnormality detection

11

clustering algorithm that ﬁnds a number of clusters starting from the estimated
density distribution of corresponding nodes. Although it has many advantages,
such as discovering arbitrarily shaped clusters and robust detection of outliers,
the algorithm sometimes fails to identify clusters in those situations of varying
density of considered data or if the dataset is too sparse. The dataset considered
in this work is sparse and multidimensional so that we have chosen the GNG
algorithm by considering its advantages over other clustering algorithms.

The GNG produces a set of nodes, and each of the nodes represents a cluster.
The cluster can be described in terms of global statistical properties of the
samples grouped in it: mean and co-variance is parameters here used. In the
proposed method, GSs components (i.e., state and errors) ( Eq.1) obtained from
the initial ﬁlter are separately clustered by the GNG. For example, in the case
of control S − P modality clustering is applied separately to GSs components
written as below:

GN G1 = [stk ptk ](cid:124)
˙ptk ](cid:124)
GN G2 = [ ˙stk

(3a)

(3b)

In GNG, each node groups a subset of data samples that are closer with
respect to a deﬁned distance metric to the centroid of the node. The nodes
obtained by each GNG deﬁne a set of letters that can be used to describe the
new DBN to be learned. The collection of nodes generated by the GNGs of
modality S − P can be written as below:

V 0
SP = {a1, a2, ..., ap}
V 1
SP = {b1, b2, ..., bq}

(4a)

(4b)

where p and q represents the index of clusters obtained by the state and
SP represent the group of letters(nodes) produced

derivative GNGs, V 0
by GNG1 and GNG2 respectively.

SP and V 1

Each GNG produces clusters (i.e., vocabularies of letters) that have to be
coupled to deﬁne a single dynamic behavior: to deﬁne how to couple letters
describing states and derivatives; time co-occurrence is used. Clusters of states
and derivatives that are activated simultaneously are associated with a pair of
letters to generate diﬀerent switching variables for the DBN model. Each pair
can form a word and an example belong to S − P modality is provided below:

where V 0

m, V 1
n ]
m represents the mth element of the set of nodes generated by GNG1
n is the nth element of node belong to GNG2. Each pair can be called as a

i = [V 0

W SP

(5)

and V 1

12

D.T. Kanapram, P. Marin-Plaza, L. Marcenaro et al.

word, and a unique label is assigned to each word. The list of words, along with
assigned labels, form a dictionary as below.

Dsp = {β, β1, · · · βL}

(6)

where βl is Wl. Gl and Gl represents the unique label assigned for word Wl.
Dictionary is a high-level hierarchy switching variable that explains the system
states from a semantic viewpoint.

Furthermore, the transition probability matrices have been estimated based
on the timely evolution of such letters. The transition matrix consists of the
probability of activating nodes from state and derivative space at the next time
instance by knowing the currently activated nodes. The green and blue coloured
arrows in Fig. 2 represents the transition probability between discrete variables.
The model above can be extended to the case of collective awareness: when
two GSs DBNs associated with two agents in a swarm in a collective ensemble are
available. Collective awareness can be modeled as additional, conditional prob-
abilities describing how one agent’s GS can be predicted from another agent’s
GS. As an example of how this additional, conditional probability can be repre-
sented by a transition matrix between words of two diﬀerent agents, the coupled
co-occurrence matrix is here used that was obtained by considering the vocabu-
lary (letters) learned from the position data of agents. As in the previous case,
ﬁrstly, each entity’s position GSs have been separately clustered by GNG to
generate respective vocabularies (i.e., letters). Then, the co-occurrence matrix
that encodes the probabilities of passing from words describing agent1 task to
the words describing agent2 and vice versa. Each element of the coupled co-
occurrence matrix represents the probability of activation of one agent’s discrete
vocabulary(letters) conditioned to the other agent’s GS clusters. The coupled
co-occurrence matrix can be represented as below:

Coupled co-occurrence matrix, T =







M11 M12 . . . M1n
...

. . .

Mm1 Mm2 . . . Mmn







(7)

where m and n represent the maximum number of clusters (obtained from
position data) belongs to agent1 and agent2, respectively. Each entry in the
matrix represents the co-occurrence probability between agent1 and agent2. For
example, M12 (refer Eq. 7)the co-occurrence probability from node 1 (mean value
of ﬁrst cluster) belong to agent1 to node 2 (mean value of second cluster) belong
to agent2. This information of couple probability helps to model the collective
awareness for the agents jointly performing tasks.

Self-awareness in IV: Feature based abnormality detection

13

For each switching variable, a diﬀerent dynamic model is learned that de-
ﬁnes the dynamics of generalized states ˜X k in the region of the state covered
by each switching word. As we limited GSs to L=1 each dynamic model can
consider only clusters that collect as samples GSs including state and derivative,
˙Xk](cid:124). For such a case, mean state (and related covari-
such that ˜X k = [Xk
ance) individuate sparse regions in the state space, while mean derivative (and
its covariance) deﬁnes local linear dynamics around that state region. On this
basis, for each switching variable in the word vocabulary, it is possible to learn
continuous dynamic models of random mean velocity equal to the corresponding
word for tracking generalized states’ dynamics, such that:

˜X k+1 = A ˜X k + BUk + wk

(8)

where

A =

(cid:35)

(cid:34)

Ij 0j,j
0j,j 0j,j

; B =

(cid:35)

(cid:34)

0j,j
Ij∆k

The variable j indexes the number of states in the data combination in con-
sideration. Ij is an identity matrix of dimension j. 0j,j is a zero j × j matrix.
wk ∼ N (0, σ), encodes the system noise. Uk = E(Wi,k+1|Wi,k), where E(·) is the
expectation operator. The control vector Uk contains the agents velocity when
it’s state falls within a discrete space formed by the GNG clustering.

Pair-based DBN models
All the previous steps are involved in learning the pair based switching DBN
models from the multisensory position and control data. The number of DBNs
learned by the vehicle m can be written as:

DBN m = {DBN1, · · · , DBNn}.

(9)

where m represents the mth vehicle in the network and n is the total number
DBN learned by the mth vehicle. The same DBN architecture is considered
for making inferences with diﬀerent sensory data combinations belong to each
vehicle considered. The learned DBN can be represented, as shown in Fig.2. The
DBN has mainly two levels, such as continuous and discrete levels.

Coupled DBN models
The coupled co-occurrence matrix learned from two agents’ position data helps
to model the interacting agents’ joint/collective awareness. Each agent esti-
mates the co-occurrence matrix in the training phase by jointly considering the

14

D.T. Kanapram, P. Marin-Plaza, L. Marcenaro et al.

Fig. 2: Proposed switching DBN model

agent1 and agent2 vocabulary(letters). In the test phase, the intercommunica-
tion scheme proposed in Section 4 shares the observed position data between the
agents at each time instance. With this information, the coupled DBN inside each
agent estimates the discrete level coupled letters (i.e., word ). The coupled pair of
letters at each time instance points to a speciﬁc cell in the co-occurrence matrix
(refer Eq. 7) to check the probability of ﬁring the current word. If the probability
value is high, it means the agents’ current experience matches with the one used
to learn the model, and the situation is considered normal. When the value goes
lower, the agents pass through unseen dynamics, and the situation is consid-
ered abnormal. The coupled DBN models are specially used in situations where
the pair-based self-awareness DBN models fail to detect the anomaly happens
around other agents in cooperative scenarios.

3.2 Online test phase

The block diagram representation for the online test phase is shown as a pink
shaded area in Fig.1. In this phase, a probabilistic switching model called Markov

((cid:3038)(cid:2878)(cid:2869)/(cid:3038))Particle filterKalman filter (cid:2868),(cid:3038)(cid:2869),(cid:3038)(cid:2868),(cid:3038)(cid:2878)(cid:2869)(cid:2869),(cid:3038)(cid:2878)(cid:2869)(cid:3038)(cid:2878)(cid:2869)(cid:3038)(cid:3038)(cid:3038)(cid:2878)(cid:2869)(cid:3038)(cid:3038)(cid:2878)(cid:2869)((cid:3038)(cid:2878)(cid:2869)/(cid:3038))Self-awareness in IV: Feature based abnormality detection

15

Jump Particle Filter (MJPF) [32, 19] has been chosen to make inferences on the
DBN models (refer Fig.2) learned in the training phase.

The ﬁltering algorithms like Markov Jump Particle Filter (MJPF) [33, 34]
and Interacting multiple models (IMM) ﬁlters [35] allow an agent to predict and
estimate target motion according to multiple probabilistic models. The ﬁlters
diﬀer in the inference methods they use to perform prediction and update steps.
While MJPF uses particle ﬁlters at discrete levels together with Kalman ﬁlters
at continuous levels, IMM ﬁlters can use diﬀerent approaches. For example, in
IMM ﬁlters [36, 37], a model-driven approach is performed to fuse Kalman Fil-
ters. In general, IMM ﬁlters can be coupled with parameter estimation learning
methods speciﬁc to the inference approach used that can be used on training se-
quences. However, parameters are often chosen by design, and ﬁxed discrete state
transition probabilities are provided oﬄine from the discrete variables switches’
frequency. The number of models is generally a priori ﬁxed, limiting the descrip-
tors of the agents’ dynamics. In this work, we used MJPF, a type of Markov
Jump Linear System (MJLS) that uses a parametrized couple of Particle Filter
and Kalman ﬁlters that can be learned from data. This allows as in IMM infer-
ences on a Dynamic Bayesian Network jointly at continuous and discrete levels.
However, the data-driven approach used in this work is based on a free energy
minimization approach that allows a varying number of dynamic models to be
estimated together with temporal transition probabilities that characterize the
models’ discrete temporal evolution.

In IMM, model switching is mainly dependant on a time-independent tran-
sition probability matrix; in MJPF here used, co-occurrence probability and
transition models learned are time-dependent, so allowing a time-variant transi-
tion probability, speciﬁc for each dynamic model, to be employed. In used MJPF,
the number of particles employed at the discrete level is deﬁned as proportional
to the number of dynamic models. The method explore in parallel an alternative
set of dynamic models predictions by evaluating the best choices depending on
anomaly detection capabilities added.

Estimation of future states and abnormality detection
The preprocessed data sequences collected from the experiences of agent vehicles
not included in the training set are given as input to the MJPF in the online
phase. In a Markov Jump Particle Filter (MJPF), the posterior probability den-
sity function can be written as:

p(Wk+1, ˜X k+1/Zk+1) = p( ˜X k+1/Wk+1, Zk+1)p(Wk+1/Zk+1)

(10)

16

D.T. Kanapram, P. Marin-Plaza, L. Marcenaro et al.

where Wk+1 is the word in the higher hierarchical level and ˜X k+1 is the con-
tinuous state in the state space at time instant k. p(Wk+1/Zk+1) is estimated by
using particle ﬁlter [38] at the discrete vocabulary level. Dynamic models learned
in the training phase as in Eq. 8 are used as a dynamic model equation of Kalman
Filters at the continuous level associated with the discrete switching variable. A
diﬀerent gaussian velocity associated with the corresponding switching variable
cluster obtained by GNG is represented with the control vector as a variable Uk
(refer Eq. 11). This means that the predictions p( ˜X k+1/ ˜X k) of particles at the
continuous state level are made by considering a bank of Kalman Filters built
according to the discrete vocabulary where diﬀerent Gaussian velocity models
are valid for each switching variable.

The goal is to use a ﬁlter capable of performing inference jointly at contin-
uous and discrete levels(refer Fig. 2). The MJPF allows the agent to predict
its future states, starting from the learned DBN generative model’s prediction
components. Moreover, the ﬁlter is provided with an additional inference feature
that allows it to use bayesian prediction and evidence messages exchanged by
DBN nodes to compute abnormality indicators at diﬀerent DBN levels. Such
abnormality measurements allow the agent to detect when its own prediction
model components of the generative DBN ﬁt with new data sequences processed
by the modiﬁed MJPF. Anomaly is here deﬁned as a time signal that indicates
when the probabilistically predicted states mismatch the current noisy sensory
observations. A detailed description of the modiﬁed MJPF version with abnor-
mality detection here used is described in section II of [39]. However, in that
paper, the authors used a diﬀerent clustering algorithm(SOM) for estimating
the switching variables associated with diﬀerent dynamic models from general-
ized errors. Moreover, a diﬀerent approach is used here in learning the discrete
vocabulary and the correlation among them than the one used in [39].

In the prediction step of the MJPF, a Sequential importance Resampling
(SIR) PF [40, 41] is used to predict the new set of particles iteratively. The time-
variant transition probability model is used to perform particle sampling at a
new time instant at the discrete level. Gaussian proposal function p(Wk+1/Wk)
is used. In the update step, the new sample from the testing sequence is provided
to the ﬁlter. It allows ﬁrstly to update the prediction message at the continuous
level and to obtain the new posterior. The update is then propagated at the
particle level to obtain new weights based on the observations that such poste-
∗,is associated with
rior gives to the discrete variable itself. As each particle Wk
the Kalman ﬁlter having the dynamic model of the particle word Wk than the

Self-awareness in IV: Feature based abnormality detection

17

corresponding dynamic model is used within the particle to predict continuous
state p( ˜X k+1/ ˜X k, Wk

∗)

The prediction performed at the continuous level can match with updates
to a diﬀerent extent. Measuring such a matching level by a probabilistic dis-
tance measurement allows the modiﬁed MJPF to estimate the anomaly that can
be present. In probability theory, a statistical distance quantiﬁes the distance
between two statistical functions, which can be either two random variables
or two probability distributions. Some important statistical distances used be-
tween two distributions include: Bhattacharya distance [42], Hellinger Distance
[43], Jensen–Shannon divergence [44], Kullback–Leibler (KL) divergence [45] etc.
Hellinger distance (HD) is a symmetric distance used to quantify the distance
vectors having only positive or zero elements [46]. Here such a distance is used to
measure the distance between prediction and evidence at the continuous level.
The works in [47] and [24] also proposed to use Hellinger distance (HD) as an
abnormality measurement.
c
Accordingly, let p( ˜X
k−1) be the predicted generalized states and p(Zk| ˜X
be the observation evidence. The Hellinger distance (HD) can be written as:

k| ˜X

c
k)

c

k = (cid:112)1 − λc
θc
k,

where λc

k is deﬁned as the Bhattacharyya coeﬃcient [48], such that:

(cid:90) (cid:113)

λc
k =

p( ˜X

c

k| ˜X

c
k−1)p(Z c

k| ˜X

c

k) d ˜X

c
k.

(11)

(12)

When MJPF processes a testing set sequences, an abnormality measurement is
computed at each time instant at the continuous level, as in the equation (11).
The variable θc
k ∈ [0, 1], where values close to 0 indicate that the ground truth
measurements match with predictions, whereas values close to 1 reveal the pres-
ence of an abnormality in the environment. Once estimated the anomaly by HD
metric, it is possible to check the complementarity among diﬀerent DBN models
learned by measuring how well the pair based models diﬀer in performance of
detecting environmental abnormalities.

In addition to the HD abnormality measurements, the co-occurrence of events
with a low probability of occurring in coupled DBNs of the two agents provides
a further collective abnormality measurement. This measurement can be used to
measure the reciprocal inﬂuence of agent states in case of collective awareness. In
this way, when an anomaly happens around one agent, other agents can under-
stand this as a low probability co-occurrent event can happen. In this paper, we
have explored how this works using only odometry data in this part of discrete

18

D.T. Kanapram, P. Marin-Plaza, L. Marcenaro et al.

level joint anomaly detection by coupled DBN model to provide an insight of
possible future developments of the presented method. The coupled DBN model
inside each agent ﬁrst checks the ﬁring of a pair of letters (word) formed from
both agents’ currently observed position data. Then record the probability value
inside the corresponding cell of the coupled co-occurrence matrix. A high proba-
bility value means that the current experience matched the agent’s experience in
the training phase when the model was learned. A low probability value detects
the presence of a collective anomaly that is occurring to the agents’ team. For
the better representation, we have taken 1-(the value of co-occurrence probabil-
ity) and plotted the resultant signal in Sec 5. This discrete level anomaly based
on coupled co-occurrence matrix can be estimated with the following equation:

δk = 1 − M k
ij

(13)

where M k
occurrence matrix at kth time instance.

ij is the probability value in the ith row and jth column of the co-

This discrete level anomaly metric encodes the information of collective
awareness. However, this part will require further work to improve the collec-
tive awareness functionality by considering the system’s control data and other
relevant features. In this paper, we have shown a preliminary level collective
awareness model and the obtained results.

4 Experimental Setup and employed datasets

In order to validate the proposed method, the datasets collected from two in-
telligent research platform with autonomous capabilities called iCab (Intelligent
Campus AutomoBile)[49] (see Fig.3a) performing co-operative as well as in-
dividual driving tasks were considered. The multisensory data extracted from
co-operative driving tasks used to learn the DBN models and test the models
self/collective awareness functionality, whereas a single-vehicle scenario datasets
exploited to learn and test DBN model’s self-awareness capability.

The considered iCab vehicles are equipped with two powerful computers and
a screen for debugging and Human-Machine Interaction (HMI) purposes of nav-
igating through the environment, as displayed in Fig. 3b. The software pro-
totyping tool used is ROS [50]. The exteroceptive position and proprioceptive
control data sets collected from the iCab vehicles performing tasks are ﬁrstly
synchronized to match their time stamps. Additionally, the multisensory data
normalized to bring the numeric columns in the data set to a common scale by
not distorting the diﬀerences in the ranges of values or losing information.

Self-awareness in IV: Feature based abnormality detection

19

(a) The autonomous vehicles (iCab)

(b) The environment

Fig. 3: The agents and the environment used for the experiments.

The intercommunication scheme proposed in [51] was used to share all the po-
sition data and their respective timestamp information over the Virtual Private
Network (VPN) during the co-operative driving experiments of iCab vehicles.

Both vehicles perform a Perimeter Monitoring Task (PMT) jointly , which
consists of the autonomous movement of platooning around a square building
(see Fig.3b). The 2D data of exteroceptive odometry (X − Y ) and the diﬀerent
pairs of the proprioceptive control variables such as Steering angle-Velocity (S −
V ), Steering angle-Power (S − P ), and Velocity-Power (V − P ) are the main
features considered to learn and test the models. The dimension of the movement
trace (Fig.3b) in the testing environment is 38mX33m.

(a) Perimeter monitoring

(b) Emergency stop

Fig. 4: Odometry data for iCab1

-20-10010200510152025303540Odometry trajectory for iCab1-20-10010200510152025303540Emergency stop trajectory for iCab1Emergency stop20

D.T. Kanapram, P. Marin-Plaza, L. Marcenaro et al.

(a) Perimeter monitoring

(b) Emergency stop

Fig. 5: Odometry data for iCab2

4.1 Scenarios

To generate the required data for the model learning and testing, we have con-
ducted two types of experiments with four diﬀerent scenarios shown in Fig. 9
and the description below.

– Scenario I: Perimeter monitoring task (PMT)

The two iCab vehicles perform the platooning operation in a counterclock-
wise direction by following a rectangular trajectory in a closed environment
(refer Scenario I in Fig. 9), four laps in total. Fig.4a and Fig.5a show the plots
of odometry data for the perimeter monitoring task for iCab1 and iCab2 re-
spectively. Moreover, Fig.7a and Fig.7b show the steering angle (S) and rotor
velocity(V ) data respectively of iCab1 plotted w.r.t it’s own odometry posi-
tion data. The rotor power data plotted w.r.t iCab1’s position is illustrated
in Fig.8. The paired data combinations from this scenario are used for model
learning in the training phase.

– Scenario II: Emergency stop 1 (ES1)

Both vehicles perform the same experiment of perimeter monitoring task
(PMT), but now a random pedestrian crosses in front of the leader vehi-
cle(i.e., iCab1) (refer to Scenario II in Fig. 9). When the leader vehicle de-
tects the dynamic obstacle (i.e., randomly crossing pedestrian), it automati-
cally executes an emergency stop and waits until the pedestrian fully moves
out from the danger zone. Meanwhile, the follower (i.e., iCab2) detects the

-20-10010200510152025303540Odometry trajectory for iCab2-20-10010200510152025303540Emergency stop trajectory for iCab2Emergency stopSelf-awareness in IV: Feature based abnormality detection

21

Fig. 6: Odometry data for pedestrian avoidance (iCab)

(a) Steering angle(s) w.r.t position

(b) Velocity(v ) w.r.t position

Fig. 7: Control data for iCab1 for perimeter monitoring task

-20-10010200510152025303540Odometry trajectory22

D.T. Kanapram, P. Marin-Plaza, L. Marcenaro et al.

Fig. 8: Rotor power(p) w.r.t position for iCab1 for perimeter monitoring task

Fig. 9: Scenarios for PMT

Self-awareness in IV: Feature based abnormality detection

23

leader’s emergency brake action (by receiving the trajectory position data
from iCab1 ) and mimics the same action of an emergency stop. Once the
leader continues the PMT, the follower also starts moving and continues the
PMT. Fig.4b and Fig.5b show the plots of odometry data for the emergency
stop criteria for iCab1 and iCab2 respectively. The intervals that the iCab
vehicles perform an emergency stop is marked as a red box.

– Scenario III: Emergency stop 2 (ES2)

In this scenario, a random pedestrian crosses in front of the follower(iCab2)
vehicle, as shown in Fig 9 (Scenario III), while they perform the co-operative
driving task of PMT. As soon as the follower vehicle detects the dynamic
pedestrian’s presence, it executes an emergency brake operation. However,
the leader doesn’t stop as in Scenario II; instead, it continues PMT.

– Scenario IV: Pedestrian avoidance

This scenario considers a standstill pedestrian appears along the path in two
diﬀerent locations that interfere with the perimeter monitoring task per-
forms by an iCab vehicle. In this case, the pedestrian is a static obstacle.
When a pedestrian appears in front of the vehicle, it executes an avoiding
maneuver (the red part of the trajectory in Scenario IV in Fig.9) and contin-
ues its perimeter monitoring task. The odometry data plot of the pedestrian
avoidance scenario is shown in Fig.6.

5 Results

The learned models have been tested with the data set collected from diﬀer-
ent co-operative tasks and a single-vehicle scenario task. The models’ anomaly
detection capability has compared to know the best pair-based feature of the
vehicles. The overall training time (which includes synchronization, GNG clus-
tering, vocabulary generation, and transition probability estimation) for all the
four modalities was about 56 seconds. On average, each modality consumed 14
seconds for model learning, of the training data size of 3200X2. The computation
time of the MJPF algorithm can mainly depend on the number of particles used
and the test data size. The average calculation time of anomaly by the MJPF
belongs to each pair based model for the test data size of 800X2 (one complete
lap) is 20 seconds in the test phase. Therefore, the average computation time for
each abnormality sample is 0.025 seconds, includes the time for state prediction,
anomaly estimation, and updation of states.

The abnormality threshold value is ﬁxed based on the concept of validating
the learned generative model. The prediction is distributed according to mul-

24

D.T. Kanapram, P. Marin-Plaza, L. Marcenaro et al.

tivariate Gaussian, and the covariance matrix characterizes the shape of the
function. If more percentage of the evidence(observation) falls under the predic-
tion distribution, it is considered normal. In this work, we have assumed that the
generative model’s prediction could be inside 60 % of the distribution. If at least
50% of the observation falls inside this conﬁdence region, such a situation is con-
sidered normal. On the other hand, if more percentage of the observed evidence
falls outside the 60% of prediction, it is regarded as abnormal. By considering
the above conditions, the anomaly threshold value is automatically identiﬁed
and is 0.3.

The model testing phase consists of two parts: Phase I presented the results
and analysis of the self-awareness functionality of the pair based DBN models
and the initial level collective awareness functionality developed by the coupled
DBN model. The coupled DBN model can be useful in situations like the pair
based DBN model fails to detect the anomaly happens around other agents while
performing co-operative tasks.

On the other hand, Phase II gave focus to test the models learned from
single-vehicle experience, and the abnormality estimation results obtained from
diﬀerent pair based DBN models analyzed to check the best pair based feature.
The cyan shaded area from Fig.10 to Fig.16 shows the intervals where vehicles
encountered static/dynamic obstacles.

5.1 Test phase I

In this phase, we used the data sets collected from two diﬀerent co-operatives
driving task scenarios. The results of Hellinger Distance(HD) abnormality mea-
surements estimated by the DBN models learned from four diﬀerent pair-based
features of the vehicles presented. Moreover, the usefulness of coupled DBN
models collective awareness functionality examined by estimating discrete level
global anomaly.

Emergency stop scenario I The anomaly estimation results by MJPF,
whereas paired data sequences from Scenario II (described in Sec 4.1), fed as
input to MJPF presented in this part. The results from each pair-based DBN
models and the coupled DBN model learned from odometry position data are
described below.

(i) Odometry (X − Y ): The switching DBN model in this work is designed
for the control part of the vehicles. However, we have considered odometry
data and tested the performance of the learned DBN. Fig. 10 shows the plots

Self-awareness in IV: Feature based abnormality detection

25

(a)

(b)

Fig. 10: Abnormality measurements for odometry: (a) iCab1, (b) iCab2

(a)

(b)

Fig. 11: Abnormality measurements for control (SV): (a) iCab1, (b) iCab2

(a)

(b)

Fig. 12: Abnormality measurements for control (SP): (a) iCab1, (b) iCab2

(a)

(b)

Fig. 13: Abnormality measurements for control (VP): (a) iCab1, (b) iCab2

10020030040050060070080000.20.40.60.81Odometry iCab110020030040050060070080000.20.40.60.81Odometry iCab210020030040050060070080000.20.40.60.81Control SV iCab110020030040050060070080000.20.40.60.81Control SV iCab210020030040050060070080000.20.40.60.81Control SP iCab110020030040050060070080000.20.40.60.81Control SP iCab210020030040050060070080000.20.40.60.81Control VP iCab110020030040050060070080000.20.40.60.81Control VP iCab226

D.T. Kanapram, P. Marin-Plaza, L. Marcenaro et al.

of abnormality measures by examining odometry data for the vehicle leader
(iCab1) and the vehicle follower (iCab2), respectively. During the interval
(cyan shaded area) while a random pedestrian crosses in front of the leader
vehicle (icab1), and it stops, there isn’t any signiﬁcant diﬀerence in HD value
for iCab1 (leader) as well as iCab2 (follower). This behavior is because, dur-
ing that interval, the vehicles are always inside the normal trajectory range.
However, speciﬁc intervals when the vehicles deviate from the normal range
with respect to the trajectory data used to learn models; the HD measures
provided a value of about 0.2 during those intervals. It means that the odom-
etry DBN model was able to predict if any trajectory deviation occurred.

(ii) Steering angle-Velocity (S − V ): It is necessary to check diﬀerent pairs
of 2D sensory data combinations to know the vehicles’ best pair-based fea-
tures. The models learned from the steering-velocity pair doesn’t notice the
abnormal situation happens in the environment. This is an expected result
as the vehicles’ steering values don’t change much when they perform an
emergency stop. The HD abnormality plots for iCab1 and iCab2 vehicles are
depicted in Fig. 11, not detecting pedestrians’ presence. So that, this pair
is not considered a useful feature for the abnormality detection purpose for
similar scenarios.

(iii) Steering angle-Power (S − P ): When a pedestrian crosses in front of
the leader vehicle (iCab1), the HD value is shown high during that interval,
as shown in Fig. 12 (a). The follower(iCab2) receives the shared trajectory
data from the leader, understands the presence of anomaly, and performs an
emergency stop operation to keep a minimum distance with the leader. So
that the DBN model inside the follower also detects the anomaly, and the
HD value becomes high in that interval, as shown in Fig. 12 (b). Thus S − P
is a useful feature of the vehicle in detecting an anomaly for similar tasks.

(iv) Velocity-Power (V − P ): The last pair tested is velocity and power con-
sumption, which are highly related. In Fig. 13 (a), the HD anomaly when a
pedestrian cross in front of the leader is shown in cyan color. The highest
HD values(closer to 1) are shown in this modality w.r.t other modalities.
For the follower vehicle, the abnormality measurement is very signiﬁcant,
as shown in Fig. 13 (b) due to the brake operation performed by following
the action of the leader vehicle. The consecutive peaks(that are closer to
0.3) in the plots are caused by the high acceleration when the leader vehicle
starts moving, but the current distance between them is still lower than the
desired.

Self-awareness in IV: Feature based abnormality detection

27

To summarize, the pair-based DBN learned from S −P and V −P data were able
to predict the unusual situations present; however, odometry and S − V pairs
of control did not show the right combination to detect the agents abnormal
behavior.

Emergency stop scenario II This part analyzes the anomaly signal esti-
mation by the models considering the sensory data of Scenario III (refer Sec
4.1). In this special case scenario, when a pedestrian crosses in front of the
follower(iCab2) vehicle, it performs the emergency stop, and the leader vehi-
cle continues the PMT. The DBN model inside the leader doesn’t show any
anomaly; only the DBN models inside the follower vehicle detect pedestrians’
presence.

In such situations, the coupled DBN model plays an important role. The
model learned from both vehicle’s position data represents the collective situa-
tion and estimates anomaly that considers both vehicle’s behavior. The model
will perform a discrete level anomaly estimation based on the co-occurrence prob-
ability matrix that infers the collective awareness information. For simplicity, we
have only presented the results from S − P modality to show the self-awareness
functionality and odometry modality for collective self-awareness.

(a)

(b)

Fig. 14: Abnormality measurements for control SP: (a) iCab1, (b) iCab2

(i) Pair based Steering angle-Power (S − P ) model: The HD anomaly
measurements for iCab1 and iCab2 for Steering-power (S − P ) modality is
shown in Fig.14 (a) and Fig.14 (b) respectively. Contrary to the previous
case (Emergency stop scenario I), the HD anomaly indicator of the DBN
model inside the leader(iCab1) vehicle doesn’t show any high peaks as the
vehicle doesn’t stop anywhere during the PMT, and this is an expected

10020030040050060070080000.20.40.60.81Control SP iCab110020030040050060070080000.20.40.60.81Control SP iCab228

D.T. Kanapram, P. Marin-Plaza, L. Marcenaro et al.

result. Simultaneously, the DBN models inside the follower((iCab2) vehicle
detect abnormality when the vehicle executed emergency brake operation
during the dynamic pedestrian crosses in front of it. We haven’t included
the results of models learned from other pairs and assumed that results
would be are similar to the previous shown co-operative scenario.

(ii) Collective self-awareness: Odometry (X − Y ): The situations in which
the pair based self-awareness models fail to detect anomaly around other
agents, the collective awareness model (i.e., coupled DBN) plays a vital role.
For instance, the pair based DBN model of S − P modality inside iCab1
vehicle , tested with the data sets of Scenario III (refer Sec 4.1), didn’t show
any high peaks in estimated Hellinger Distance(HD) values. Therefore the
leader vehicle (iCab1) was not aware of the anomaly happening around the
follower vehicle (iCab2). In this situation, the collective awareness model in-

Fig. 15: Abnormality measurements based on coupled co-occurrence matrix

side iCab1 plays an essential role in showing anomaly present around iCab2.
The coupled DBN model inside iCab 1 was able to estimate the anomaly
(presence of dynamic pedestrian) happens around the follower vehicle. The
discrete probabilistic anomaly signal shows high values, as in Fig.15 dur-
ing the interval when the pedestrian is inside the danger zone. The collective
DBN model inside any of the vehicles can track the anomaly happens around
any of the other vehicles that are part of the co-operative driving task.

5.2 Test phase II :Pedestrian avoidance scenario

In this part, the abnormality measurements obtained when the model tested
with the data from a pedestrian avoidance task (refer to Scenario IV in Fig.9 )
have been presented. A total of four pairs of data combinations were examined
and described below.

Self-awareness in IV: Feature based abnormality detection

29

(a)

(b)

(c)

(d)

Fig. 16: Abnormality measurements for pedestrian avoidance: (a) OdometryXY ,
(b) Control SV (c) Control SP (d) Control V P

010020030040050060070080000.20.40.60.81Odometry iCab010020030040050060070080000.20.40.60.81Control SV iCab010020030040050060070080000.20.40.60.81Control SP iCab010020030040050060070080000.20.40.60.81Control VP iCab30

D.T. Kanapram, P. Marin-Plaza, L. Marcenaro et al.

(i) Odometry (X − Y ): The HD abnormality measurements from the odome-
try test data of scenario IV plotted in Fig. 16(a). The model inside the iCab
vehicle detects abnormality during the vehicle performs avoidance maneu-
ver by detecting the presence of a static pedestrian in front of it. As the
trajectory deviates from the normal one, the abnormality peaks are high as
expected.

(ii) Steering Angle-Velocity (S − V ): When used SV pair test data from
scenario IV, the HD abnormality measurements are shown in Fig. 16(b).
Like the odometry data, the model learned from the S − V pair also shows
abnormality peaks during those intervals. This high value in the HD metric is
because when a pedestrian appears, the vehicle takes the trajectory (shown
as the red dotted line in Fig.9 ) that is diﬀerent than the one in the training
phase, so that there would be signiﬁcant deviations in the steering angle of
the vehicle.

(iii) Steering Angle-Power (S − P ): Fig. 16(c) shows the HD measurement
plot of SP pair. When the vehicle encounters a pedestrian in two diﬀerent
locations, it shows high peaks in the HD plot (cyan shaded area) during those
intervals. Other small spikes indicated the sensor noise and abnormalities in
the environment.

(iv) Velocity-Power (V − P ): The HD abnormality measurements from the
V P pair test data for the considered vehicles plotted in Fig. 16(d). Here,
the HD measures do not show any peaks during those intervals because
the considered vehicle’s velocity and power don’t change much when they
execute the avoidance maneuver.

The pair based models inside the vehicles play an essential role in detecting
anomaly(presence of dynamic and static obstacles) happens around itself, and
also some instances of co-operative scenarios. On the other hand, the collective
awareness model (coupled DBN) detects anomaly occurs around any of the ve-
hicles that are part of co-operative driving tasks by exploiting the data shared
by the communication scheme.

6 Conclusion and future work

This paper proposed a method to develop self-awareness models considering pair
based features of the vehicles. To learn the switching DBN models, low dimen-
sional multisensory data describe the normal behavior of the vehicles used. Such
a learned data-driven DBN model can automatically detect abnormal situations

instead of deﬁning the metric’s upper and lower limits. A set of DBN models
learned inside each entity by considering pair based features helps conﬁrm the
best model that ﬁts to detect abnormalities. The obtained results from each
of the pair based DBN models show that our method works well in detecting
the dynamic anomalies in the surrounding environment of the vehicle. We have
considered data sets from diﬀerent co-operative driving scenarios as well as a
single-vehicle scenario. A performance comparison was performed of the models
learned from various pair-based features of the vehicles.
Additionally, an initial level collective awareness model(i.e., coupled DBN ) is
proposed to detect collective anomaly when agents perform co-operative tasks.
When the anomaly happens around one vehicle, the coupled DBN models in-
side other vehicles can detect the presence of anomaly. However this work needs
additional work in future to improve the collective awareness functionality. The
future work can include eﬃcient communication schemes between the objects to
share essential data among the vehicles to exploit collective awareness by con-
sidering diﬀerent modalities. Such models can make the mutual prediction of the
future states of the objects involved in the task. The model performance under
the inﬂuence of wireless communication channels and the diﬀerent communi-
cation protocols standards, environmental conditions, etc. need to be carefully
studied. Additionally, the classiﬁcation of detected abnormality by considering
diﬀerent test scenarios and comparing abnormality detection performance using
other abnormality metrics could be considered.

Acknowledgement

Research supported by the Spanish Government through the CICYT projects
(TRA2016-78886-C3-1-R and RTI2018-096036-B-C21), Universidad Carlos III
of Madrid through (PEAVAUTO-CM-UC3M) and the Comunidad de Madrid
through SEGVAUTO-4.0-CM (P2018/EMT-4362). We gratefully acknowledge
the support of NVIDIA Corporation with the donation of the GPUs used for
this research.

Bibliography

[1] Xiong G, Zhou P, Zhou S, Zhao X, Zhang H, Gong J, Chen H (2010)
Autonomous driving of intelligent vehicle bit in 2009 future challenge of

32

D.T. Kanapram, P. Marin-Plaza, L. Marcenaro et al.

china. In: Intelligent Vehicles Symposium (IV), 2010, IEEE, pp 1049–1053
[2] Regazzoni C, Marcenaro L, Campo D, Rinner B (2020) Multisensorial gen-
erative and descriptive self-awareness models for autonomous systems. Pro-
ceedings of the IEEE

[3] Murphy KP, Russell S (2002) Dynamic bayesian networks: representation,

inference and learning

[4] Fox E, Sudderth EB, Jordan MI, Willsky AS (2009) Nonparametric bayesian
learning of switching linear dynamical systems. In: Advances in Neural In-
formation Processing Systems, pp 457–464

[5] Bajgar J, Ciarrochi J, Lane R, Deane FP (2005) Development of the levels
of emotional awareness scale for children (leas-c). British Journal of Devel-
opmental Psychology 23(4):569–586

[6] Asendorpf JB, Warkentin V, Baudonniere PM (1996) Self-awareness and
other-awareness. ii: Mirror self-recognition, social contingency awareness,
and synchronic imitation. Developmental Psychology 32(2):313

[7] Baker S (1897) The identiﬁcation of the self. Psychological Review 4(3):272
[8] Tawney G (1902) Feeling and self-awareness. Psychological Review 9(6):570
[9] Rinner B, Esterle L, Simonjan J, Nebehay G, Pﬂugfelder R, Dominguez GF,
Lewis PR (2015) Self-aware and self-expressive camera networks. Computer
48(7):21–28

[10] Winﬁeld A (2014) Robots with internal models: A route to self-

safer

and hence

aware
robots.
https://www.scopus.com/inward/
10.1142/9781783264186 0016, URL
record.uri?eid=2-s2.0-85013472750&doi=10.1142%2f9781783264186 0016&
partnerID=40&md5=08b7605c309b77d861784f2026d5d6f7, cited By 7

Imperial College Press, DOI

[11] Duval S, Wicklund RA (1972) A theory of objective self awareness.
[12] Goukens C, Dewitte S, Warlop L (2009) Me, myself, and my choices: The
inﬂuence of private self-awareness on choice. Journal of Marketing Research
46(5):682–692

[13] Bishop R (2000) Intelligent vehicle applications worldwide. IEEE Intelligent

Systems and Their Applications 15(1):78–81

[14] Berger C (2014) From a competition for self-driving miniature cars to a stan-
dardized experimental platform: Concept, models, architecture, and evalu-
ation. 1406.7768

[15] Baydoun M, Ravanbakhsh M, Campo D, Marin P, Martin D, Marcenaro L,
Cavallaro A, Regazzoni CS (2018) a multi-perspective approach to anomaly
detection for self -aware embodied agents. In: 2018 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP), pp 6598–
6602

Self-awareness in IV: Feature based abnormality detection

33

[16] Ravanbakhsh M, Baydoun M, Campo D, Marin P, Martin D, Marcenaro
L, Regazzoni CS (2018) Learning multi-modal self-awareness models for
autonomous vehicles from human driving. In: 2018 21st International Con-
ference on Information Fusion (FUSION), pp 1866–1873

[17] Leite A, Pinto A, Matos A (2018) A safety monitoring model for a faulty

mobile robot. Robotics 7:32, DOI 10.3390/robotics7030032

[18] Xie G, Gao H, Huang B, Qian L, Wang J (2018) A driving behavior
awareness model based on a dynamic bayesian network and distributed
genetic algorithm. International Journal of Computational Intelligence Sys-
tems 11(1):469–482

[19] Kanapram D, Campo D, Baydoun M, Marcenaro L, Bodanese EL, Regaz-
zoni C, Marchese M (2019) Dynamic bayesian approach for decision-making
in ego-things. In: 2019 IEEE 5th World Forum on Internet of Things (WF-
IoT), pp 909–914, DOI 10.1109/WF-IoT.2019.8767204

[20] Jebara T (2012) Machine learning: discriminative and generative, vol 755.

Springer Science & Business Media

[21] Slavic G, Campo D, Baydoun M, Marin P, Martin D, Marcenaro L, Regaz-
zoni C (2020) Anomaly detection in video data based on probabilistic latent
space models. In: 2020 IEEE Conference on Evolving and Adaptive Intelli-
gent Systems (EAIS), pp 1–8

[22] Ravanbakhsh M, Baydoun M, Campo D, Marin P, Martin D, Marcenaro L,
Regazzoni CS (2018) Hierarchy of gans for learning embodied self-awareness
model. In: 2018 25th IEEE International Conference on Image Processing
(ICIP), pp 1987–1991

[23] Costa OLV, Fragoso MD, Marques RP (2006) Discrete-time Markov jump

linear systems. Springer Science & Business Media

[24] Kanapram D, Marin-Plaza P, Marcenaro L, Martin D, de la Escalera A,
Regazzoni C (2019) Self-awareness in intelligent vehicles: Experience based
abnormality detection. In: Iberian Robotics conference, Springer, pp 216–
228

[25] Campo D, Betancourt A, Marcenaro L, Regazzoni C (2017) Static force
ﬁeld representation of environments based on agents’ nonlinear motions.
EURASIP Journal on Advances in Signal Processing 2017(1):13

[26] Koller D, Lerner U (2001) Sampling in factored dynamic systems. In: Se-

quential Monte Carlo Methods in Practice

[27] Fritzke B (1995) A growing neural gas network learns topologies. In: Ad-

vances in neural information processing systems, pp 625–632

[28] MacQueen J, et al (1967) Some methods for classiﬁcation and analysis of
multivariate observations. In: Proceedings of the ﬁfth Berkeley symposium

34

D.T. Kanapram, P. Marin-Plaza, L. Marcenaro et al.

on mathematical statistics and probability, Oakland, CA, USA, vol 1, pp
281–297

[29] Kohonen T (1990) The self-organizing map. Proceedings of the IEEE

78(9):1464–1480

[30] Martinetz T, Schulten K (1991) A ”neural-gas” network learns topologies.

Artiﬁcial neural networks 1:397–402

[31] Ester M, Kriegel HP, Sander J, Xu X, et al (1996) A density-based algorithm
for discovering clusters in large spatial databases with noise. In: Kdd, vol 96,
pp 226–231

[32] Driessen H, Boers Y (2005) Eﬃcient particle ﬁlter for jump markov nonlin-
ear systems. IEE Proceedings - Radar, Sonar and Navigation 152(5):323–326

[33] Han C, Feng G, Zhang H (2011) Optimal markov jump ﬁlter for stochastic
systems with markovian transmission delays. In: Proceedings of the 30th
Chinese Control Conference, pp 4566–4571

[34] Doucet A, Gordon NJ, Krishnamurthy V (2001) Particle ﬁlters for state
estimation of jump markov linear systems. IEEE Transactions on Signal
Processing 49(3):613–624

[35] P Blom HA (1984) An eﬃcient ﬁlter for abruptly changing systems. In: The

23rd IEEE Conference on Decision and Control, pp 656–658

[36] Blom HAP, Bar-Shalom Y (1988) The interacting multiple model algorithm
for systems with markovian switching coeﬃcients. IEEE Transactions on
Automatic Control 33(8):780–783

[37] Youn W, Myung H (2019) Robust interacting multiple model with modeling
uncertainties for maneuvering target tracking. IEEE Access 7:65,427–65,443

[38] Gordon NJ, Salmond DJ, Smith AFM (1993) Novel approach to
nonlinear/non-gaussian bayesian state estimation. IEE Proceedings F -
Radar and Signal Processing 140(2):107–113, DOI 10.1049/ip-f-2.1993.0015

[39] Baydoun M, Campo D, Sanguineti V, Marcenaro L, Cavallaro A, Regaz-
zoni C (2018) Learning switching models for abnormality detection for au-
tonomous driving. In: 2018 21st International Conference on Information
Fusion (FUSION), pp 2606–2613

[40] Rubin DB (1987) The calculation of posterior distributions by data aug-
mentation: Comment: A noniterative sampling/importance resampling al-
ternative to the data augmentation algorithm for creating a few impu-
tations when fractions of missing information are modest: The sir algo-
rithm. Journal of the American Statistical Association 82(398):543–546,
URL http://www.jstor.org/stable/2289460

Self-awareness in IV: Feature based abnormality detection

35

[41] Gordon NJ, Salmond DJ, Smith AF (1993) Novel approach to
nonlinear/non-gaussian bayesian state estimation. In: IEE proceedings F
(radar and signal processing), IET, 2, pp 107–113

[42] Bhattacharyya A (1943) On a measure of divergence between two statistical
populations deﬁned by their probability distributions. Bull Calcutta Math
Soc 35:99–109

[43] Pardo L (2005) Statistical inference based on divergence measures. Chap-

man and Hall/CRC

[44] Endres DM, Schindelin JE (2003) A new metric for probability distribu-

tions. IEEE Transactions on Information theory

[45] Hershey JR, Olsen PA (2007) Approximating the kullback leibler divergence
between gaussian mixture models. In: 2007 IEEE International Conference
on Acoustics, Speech and Signal Processing-ICASSP’07, IEEE, vol 4, pp
IV–317

[46] Abdi H, Salkind NJ (2007) Encyclopedia of measurement and statistics.
Thousand Oaks, CA: Sage Agresti, A(1990) Categorical data analysis, New
York: Wiley Agresti, A(1992) A survey of exact inference for contingency
tables Statist Sci 7:131–153

[47] Lourenzutti R, Krohling RA (2014) The hellinger distance in multicriteria
decision making: An illustration to the topsis and todim methods. Expert
Syst Appl 41(9):4414–4421, DOI 10.1016/j.eswa.2014.01.015, URL http://
dx.doi.org/10.1016/j.eswa.2014.01.015

[48] Bhattacharyya A (1943) On a measure of divergence between two statis-
tical populations deﬁned by their probability distributions. Bulletin of the
Calcutta Mathematical Society 35:99–109

[49] Marin-Plaza P, Hussein A, Martin D, Escalera Adl (2018) Global and lo-
cal path planning study in a ros-based research platform for autonomous
vehicles. Journal of Advanced Transportation 2018

[50] Quigley M, Conley K, Gerkey B, Faust J, Foote T, Leibs J, Wheeler R, Ng
AY (2009) Ros: an open-source robot operating system. In: ICRA workshop
on open source software, Kobe, Japan, vol 3, p 5

[51] Kokuti A, Hussein A, Mar´ın-Plaza P, de la Escalera A, Garc´ıa F (2017) V2x
communications architecture for oﬀ-road autonomous vehicles. In: Vehicular
Electronics and Safety (ICVES), 2017 IEEE International Conference on,
IEEE, pp 69–74

