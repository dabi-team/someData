0
2
0
2

t
c
O
2
2

]
L
P
.
s
c
[

1
v
1
7
0
2
1
.
0
1
0
2
:
v
i
X
r
a

Translating Recursive Probabilistic Programs
to Factor Graph Grammars

DAVID CHIANG, University of Notre Dame, USA
CHUNG-CHIEH SHAN, Indiana University, USA

It is natural for probabilistic programs to use conditionals to express alternative substructures in models, and
loops (recursion) to express repeated substructures in models. Thus, probabilistic programs with conditionals
and recursion motivate ongoing interest in efficient and general inference. A factor graph grammar (FGG)
generates a set of factor graphs that do not all need to be enumerated in order to perform inference. We
provide a semantics-preserving translation from first-order probabilistic programs with conditionals and
recursion to FGGs.

1 INTRODUCTION
Probabilistic models often contain repeated substructures, such as the time steps of a hidden
Markov model, as well as alternative substructures, such as different grammar productions that
generate the same string. Loops (recursion) and conditionals naturally and compactly represent
such substructures in probabilistic programming. However, efficient and general inference on these
representations remains a longstanding challenge.

Factor graph grammars (FGGs) [Chiang and Riley 2020] are hyperedge replacement grammars
(HRGs) [Bauderon and Courcelle 1987; Drewes et al. 1997; Habel and Kreowski 1987] for generating
sets of factor graphs. They have recently been proposed as a unified formalism that can describe
both repeated and alternative substructures. Moreover, it turns out that inference can be performed
on an FGG (by variable elimination) without enumerating all the factor graphs generated.

This paper explores the relationship of this formalism to probabilistic programming languages
(PPLs). When random variables are limited to finite domains, what kinds of models can and can’t be
expressed using FGGs? We answer this question by defining a simple PPL and giving a translation
from this language to FGGs. This translation extends that of van de Meent et al. [2018], which
translates a PPL without higher-order functions or recursion to a factor graph. Our source language
does allow recursion, and our translation uses the added power of FGGs to handle recursion and
conditionals cleanly.

2 DEFINITIONS
To establish notation, we briefly define hypergraphs, factor graphs, HRGs, and finally FGGs.

Fix a finite set 𝐿 of edge labels, and a function arity : 𝐿 → Z≥0.
Definition 1. A (hyper)graph is a tuple (𝑉 , 𝐸, lab, att, ext), where

• 𝑉 names a finite set of nodes.
• 𝐸 names a finite set of (hyper)edges.
• lab : 𝐸 → 𝐿 assigns to each edge a label.
• att : 𝐸 → 𝑉 ∗ assigns to each edge 𝑒 a sequence of arity (lab (𝑒)) attachment nodes.
• ext ∈ 𝑉 ∗ is a sequence of zero or more external nodes.

Definition 2. A factor graph [Kschischang et al. 2001] is a hypergraph (𝑉 , 𝐸, lab, att, ext) together
with a set 𝐷 and a function 𝐹 :

• 𝐷 is the set of possible values that each node can take. For simplicity, we use a single 𝐷 for

all nodes, unlike in the previous definition [Chiang and Riley 2020].

• 𝐹 maps each edge label ℓ to a function 𝐹 (ℓ) : 𝐷 arity (ℓ) → R≥0. For brevity, we write 𝐹 (𝑒) for

𝐹 (lab (𝑒)). An edge 𝑒 together with its function 𝐹 (𝑒) is called a factor.

1

 
 
 
 
 
 
Example 3. Below is a factor graph for trees of a certain shape generated by a PCFG with start
symbol 𝑆. Variables N range over nonterminal symbols and W over terminal symbols.

David Chiang and Chung-chieh Shan

𝑝 (N2 → W4)

N2

W4

N1 = 𝑆

N1

𝑝 (N1 → N2N3)

N3

𝑝 (N3 → W5)

W5

We draw a node as a circle with its name inside, and an edge as a square with lines (called
tentacles) to its attachment nodes. To reduce clutter, we don’t show the ordering of tentacles. We
draw a factor 𝑒 as a small square with 𝐹 (𝑒) next to it, as an expression in terms of its attachment
nodes’ names. A Boolean expression has value 1 if true and 0 if false.

Definition 4. An assignment of a factor graph 𝐻 = (𝑉 , 𝐸, lab, att, ext) is a mapping 𝜉 : 𝑉 → 𝐷 from
nodes to values. We write Ξ𝐻 for the set of all assignments of 𝐻 . The weight of an assignment 𝜉 is

𝑤𝐻 (𝜉) =

(cid:214)

𝑒 ∈𝐸

𝐹 (𝑒) (cid:0)𝜉 (att (𝑒)1), . . . , 𝜉 (att (𝑒) |att (𝑒) |)(cid:1).

We also define the marginal distribution over assignments to external nodes, 𝜉 : ext → 𝐷:

𝑤ext (𝜉) =

∑︁

𝑤𝐻 (𝜉 ′).

𝜉 ′ ∈Ξ𝐻
𝜉 ′ |ext =𝜉

(1)

(2)

Definition 5. A hyperedge replacement graph grammar (HRG) is a tuple (𝑁 ,𝑇 , 𝑃, 𝑆), where

• 𝑁 ⊆ 𝐿 is a finite set of nonterminal symbols.
• 𝑇 ⊆ 𝐿 is a finite set of terminal symbols, disjoint from 𝑁 .
• 𝑃 is a finite set of productions (or rules) of the form (𝑋 → 𝑅), where 𝑋 ∈ 𝑁 , and 𝑅 is a

hypergraph with edge labels from 𝑁 ∪ 𝑇 and exactly arity (𝑋 ) external nodes.

• 𝑆 ∈ 𝑁 is a distinguished start nonterminal symbol.

Most definitions require arity (𝑆) = 0, including the previous definition of FGGs [Chiang and
Riley 2020], but here we relax this requirement, to allow the graphs generated by an HRG to have
external nodes.

Definition 6. If 𝐺 is an HRG and 𝑋 is a nonterminal symbol, define the set of 𝑋 -derivation trees
of 𝐺, D𝐺 (𝑋 ), to be the smallest set containing all pairs (𝑅, 𝑠) such that (𝑋 → 𝑅) ∈ 𝐺 and 𝑠 is
a mapping from each nonterminal edge 𝑒 in 𝑅 to a lab (𝑒)-derivation tree of 𝐺. Define the set of
derivation trees of 𝐺 to be D𝐺 = D𝐺 (𝑆).

An 𝑋 -derivation tree 𝑇 = (𝑅, 𝑠) yields a graph with the same arity (𝑋 ) external nodes as 𝑅. We
define this graph by induction on 𝑇 as follows. For each 𝑒 ∈ 𝑅, let 𝐻𝑒 be the graph yielded by 𝑠 (𝑒).
Replace 𝑒 with 𝐻𝑒 , identifying each attachment node att (𝑒)𝑖 of 𝑒 with the 𝑖th external node of 𝐻𝑒 ;
the resulting node is external iff att (𝑒)𝑖 was.

Definition 7. An HRG for factor graphs, or a factor graph grammar (FGG) for short, is an HRG
together with a set 𝐷 and and a function 𝐹 , as in the definition of factor graphs (Definition 2),
except that 𝐹 is defined on terminal edge labels only.

Example 8. Below is an FGG for derivations of a PCFG in Chomsky normal form. The start symbol
of the FGG is S′.

2

Translating Recursive Probabilistic Programs to Factor Graph Grammars

N1 = 𝑆

S′ −→

N1

X

N1

X −→ N1

𝑝 (N1 → N2N3)

N1

X −→ N1

W2

𝑝 (N1 → W2)

N2

X

This FGG generates an infinite number of factor graphs, including the one in Example 3, whose

derivation tree looks like:

N3

X

N1

X

N1

N2

X

N3

X

N1

N1

W2

W2

We draw external nodes in black. We draw an edge with nonterminal label 𝑋 as a square with
𝑋 inside. Although the left-hand side is just a nonterminal symbol, we draw it like an edge, with
replicas of the external nodes as attachment nodes.

The graphs generated by an FGG can be viewed, together with 𝐷 and 𝐹 , as factor graphs, each
of which defines a distribution over assignments. We also want each nonterminal of the FGG to be
thought of as a distribution over derivation trees and assignments; for present purposes, we only
consider the marginal distribution over assignments to external nodes:
Definition 9. An assignment of a nonterminal edge label 𝑋 is a sequence 𝜉 ∈ 𝐷 arity (𝑋 ) of values.
The weight of an assignment 𝜉 is

𝑤𝑋 (𝜉) =

∑︁

𝑤ext𝑇 (𝜉),

𝑇 ∈D (𝑋 )

(3)

where ext𝑇 is the list of external nodes of the graph yielded by 𝑇 .

To query some variables, we can make them external in the generated graphs (because 𝑆 can
have nonzero arity), and 𝑤𝑆 is their joint distribution. Chiang and Riley [2020] discuss how to
compute 𝑤𝑆 ; in particular, if all variables have finite domain, then 𝑤𝑆 can be computed in time
linear in the size of the FGG and exponential in the maximum treewidth of its right-hand sides.

3 A SIMPLE PROBABILISTIC PROGRAMMING LANGUAGE
The source language of our translation has the following syntax. Because the language is first-order,
variable names 𝑥 are distinct from function names 𝑓 .

Programs

Expressions

𝑞 ::= 𝑒
𝑒 ::= 𝑥 |
|

| fun 𝑓 (𝑥1, . . . , 𝑥𝑛) = 𝑒; 𝑞

let 𝑥1 = 𝑒2 in 𝑒3
if 𝑒1 then 𝑒2 else 𝑒3

𝑓 (𝑒1, . . . , 𝑒𝑛)

|
| case 𝑒1 of inl(𝑥2) ⇒ 𝑒2 | inr (𝑥3) ⇒ 𝑒3

| sample 𝑒1

| observe 𝑒1 ← 𝑒2

We assume a number of built-in functions and constants: =, ≠, true, false, pairing, fst, snd, inl, inr,
unit. Boolean operations and and or can be treated as built-in functions, or if short-circuiting is
desired, defined as syntactic sugar:

𝑒1 and 𝑒2 ≡ if 𝑒1 then 𝑒2 else false

𝑒1 or 𝑒2 ≡ if 𝑒1 then true else 𝑒2

The only probabilistic constructs are sample and observe. Generatively speaking, sample 𝑒1
evaluates 𝑒1 to a distribution and then nondeterministically samples from the distribution, whereas
observe 𝑒1 ← 𝑒2 evaluates 𝑒2 to a distribution and then multiplies the weight of the current branch

3

David Chiang and Chung-chieh Shan

of computation by the density of the distribution at the value of 𝑒1. Other constructs can be defined
as syntactic sugar in terms of observe, by setting 𝑒2 to a common distribution such as Bernoulli
or exponential. In particular, a fail expression unconditionally terminates the current branch of
computation, multiplying its weight by zero.

We assume that 𝐷 is countable, and define the denotations of the language in a relatively standard
way. Without recursion, we would directly define the denotation of each expression 𝑒 to be an s-
finite kernel from environments to values [Staton 2017], where an environment is a tuple that maps
each free variable of 𝑒 to its value. With recursion, we have to first define this denotation relative
to an interpretation of each function 𝑓 as itself an s-finite kernel from environments to values,
where an environment is a tuple that maps each argument of 𝑓 to its value. Thus, the denotation of
a program’s function definitions maps an interpretation of each function to an interpretation of
each function. Finally, we take the least fixed point of this monotonic map [Kozen 1981].

4 COMPILING PROBABILISTIC PROGRAMS TO FGGS
Every subexpression 𝑒 or function 𝑓 of a program can be translated into an HRG production (or
two) whose left-hand-side nonterminal is 𝑒 or 𝑓 , respectively. If the subexpression 𝑒 occurs in an
environment with 𝑘 bound variables, or if the function 𝑓 takes 𝑘 arguments, then we translate it
to a nonterminal with arity 𝑘 + 1. The external nodes are 𝑥1, . . . , 𝑥𝑘 for the variables and 𝑣 for the
result. Such a nonterminal is pictured below.

𝑥𝑖

𝑒

𝑣

𝑖 = 1, . . . , 𝑘

We use plate notation to depict multiple nodes. Unless otherwise indicated, each plate has an
instance for each 𝑖 = 1, . . . , 𝑘. We stress, however, that plates are only meta-notation; as will be
clear in the example below, actual FGG rules do not use plates.

Our translation preserves semantics in the sense that the denotation of a subexpression 𝑒 or
function 𝑓 , regarded as a weight function on (𝑘 + 1)-tuples, is equal to the weight function 𝑤𝑒
or 𝑤 𝑓 of its translation (Definition 9).

4.1 Random variables
The sample and observe expressions are translated by turning the density of the distribution into
a factor:

𝑥𝑖

sample 𝑒1

𝑣 −→ 𝑥𝑖

𝑥𝑖

observe 𝑒1 ← 𝑒2

𝑣 −→ 𝑥𝑖

𝑒1

𝑒1

𝑒2

𝑣1

𝑣2

𝑣1 (𝑣)

𝑣2 (𝑣)

𝑣

𝑣

4.2 Conditionals and disjoint unions
A conditional expression translates to two productions, one for the case where the condition is true
and one for the case where the condition is false:

𝑥𝑖

if 𝑒1 then 𝑒2 else 𝑒3

𝑣 −→ 𝑥𝑖

4

𝑣1 = true

𝑒1

𝑒2

𝑣1

𝑣

Translating Recursive Probabilistic Programs to Factor Graph Grammars

𝑥𝑖

if 𝑒1 then 𝑒2 else 𝑒3

𝑣 −→ 𝑥𝑖

𝑣1 = false

𝑒1

𝑒3

𝑣1

𝑣

This is the only place a left-hand side has more than one right-hand side (along with the related
translation of case). So the FGG has one derivation tree for each possible code path.

This translation avoids a problem that van de Meent et al. [2018, Section 3.1] encounter when
translating conditional expressions to factor graphs. In a factor graph, both arms of the conditional
must be translated, and every assignment to the factor graph must assign values to variables in both
arms, even though only one can be active at a time. Their translation requires some complicated
machinery to work around this problem. But our translation to an FGG does not have this problem,
because it generates a different graph for each arm.

The translation of case is similar to if:

𝑥𝑖

𝑥𝑖

case 𝑒1 of inl(𝑦2) ⇒ 𝑒2
| inr (𝑦3) ⇒ 𝑒3

𝑣 −→ 𝑥𝑖

case 𝑒1 of inl(𝑦2) ⇒ 𝑒2
| inr (𝑦3) ⇒ 𝑒3

𝑣 −→ 𝑥𝑖

𝑒1

𝑣1

𝑣1 = inl (𝑦2)

𝑒1

𝑣1

𝑒2

𝑒3

𝑣

𝑣

𝑦2

𝑦3

𝑣1 = inr (𝑦3)

4.3 Functions and local variables
A function definition fun 𝑓 (𝑥1, . . . , 𝑥𝑛) = 𝑒 becomes the production:

𝑥𝑖

𝑓

𝑣 −→ 𝑥𝑖

𝑒

𝑣

Inside the function body, variables evaluate to their value in the environment:

𝑥𝑖

𝑥 𝑗

𝑣 −→ 𝑥𝑖

𝑥 𝑗

𝑖 ≠ 𝑗

𝑣

𝑣 = 𝑥 𝑗

And function calls become:

𝑥𝑖

𝑓 (𝑒1, . . . , 𝑒𝑛)

𝑣 −→ 𝑥𝑖

𝑒 𝑗

𝑣 𝑗

𝑓

𝑣

𝑗 = 1, . . . , 𝑛

Unlike in the translation of van de Meent et al. [2018], the function can be recursive: the definition
of 𝑓 can include calls to 𝑓 . Infinite recursion is disallowed because the definition of FGG doesn’t
allow infinite-sized derivation trees. But a program can have an infinite number of branches of
computation, in which the depth of recursion is unbounded. The resulting FGG has an infinite
number of derivation trees, and the inference algorithm does sum over, or at least approximate the
sum over, all of them.

5

David Chiang and Chung-chieh Shan

The translation of let is:

𝑥𝑖

let 𝑥 = 𝑒1 in 𝑒2

𝑣 −→ 𝑥𝑖

𝑒2

𝑣

𝑒1

𝑥

4.4 Built-in functions and constants
Built-in functions and constants

𝑐 ::= = | ≠ |

true |

false |

(, )

|

fst

|

snd |

inl

|

inr

| unit

can be translated into FGG rules that give rise to terminal edges:

𝑥𝑖

𝑐 (𝑒1, . . . , 𝑒𝑛)

𝑣 −→ 𝑥𝑖

𝑒 𝑗

𝑣 𝑗

𝑗 = 1, . . . , 𝑛

𝑣

𝑣 = 𝑐 (𝑣1, . . . , 𝑣𝑛)

4.5 Programs
Finally, if 𝑒 is the top-level expression at the end of a program (𝑞), create the rule

𝑆
where 𝑆 is the start symbol of the FGG.

𝑣 −→ 𝑒

𝑣

5 EXAMPLE: PCFG
The code in Figure 1a nondeterministically samples a string from a PCFG in Chomsky normal form.
If 𝑋 is a left-hand side, let 𝑝 [𝑋 ] be a distribution over right-hand sides, which can be inl(𝑎) for a
single terminal symbol 𝑎 or inr (𝑌, 𝑍 ) for two nonterminals 𝑌 and 𝑍 .

This translates to the FGG in Figure 2; note the similarity to Example 8. We have made a few

optimizations for greater readability:

• We “inline” all rules except those for if, case, and functions.
• Consecutive terminal edges arising from built-in functions and constants are composed into

• In the translation of variables, the factor 𝑣 = 𝑥 𝑗 can usually be contracted, merging the nodes

a single terminal edge.

for 𝑥 𝑗 and 𝑣.

• Rules that must have zero weight are omitted.
Next, we modify this program to take a string 𝑤 as input and constrain the derivations to those
that yield 𝑤, shown in Figure 1b. This translates to the FGG in Figure 3. Computing 𝑤𝑆 on this
grammar [Chiang and Riley 2020] is equivalent to CKY. In the last rule, the nodes 𝑤, 𝑤 ′, and 𝑣25
range over suffixes of the input string, so summing over assignments to the right-hand side takes
𝑂 (𝑛3) time, just as in CKY. Unlike a typical implementation of CKY, however, the programs in
Figure 1 can be easily modified to efficiently handle, say, the intersection of a PCFG with a regular
language [Hale 2004; Lang 1988], or a PCFG whose nonterminals are structured as tuples [Collins
1997; Klein and Manning 2003].

6 CONCLUSION
The above translation has been implemented, but only outputs LATEX code to produce the diagrams
shown in this paper. We plan to properly implement the translation to output an FGG, but first we
need to implement FGGs and inference algorithms for them.

The translation only produces FGGs in which each rule has exactly one output external node. As
far as FGGs are concerned, a rule can have more than one output external node; the nonterminal

6

Translating Recursive Probabilistic Programs to Factor Graph Grammars

fun d(x) =

case sample p[x] of

inl a =>
unit
| inr yz =>

let u = d(fst(yz)) in
d(snd(yz));

d(S)

fun d(w, x) =

case sample p[x] of

inl a =>

if w != nil and car(w) = a then

cdr(w)

else

fail
| inr yz =>

let w' = d(w, fst(yz)) in
d(w', snd(yz));

if d(w, S) = nil then unit else fail

(a)

(b)

Fig. 1. Code for (a) all derivations of a PCFG in Chomsky normal form; (b) all derivations yielding a given
string 𝑤.

𝑆

𝑑

𝑥

𝑣1 −→

𝑣2

𝑑

𝑣1

𝑣2 = 𝑆

𝑣7 −→ 𝑥

𝑣5

𝑣4

𝑎

𝑣7

𝑣5 = 𝑝 [𝑥 ]

𝑣5 (𝑣4)

𝑣4 = inl (𝑎)

𝑣7 = unit

𝑥

𝑑

𝑣12 −→ 𝑥

𝑣5

𝑣4

𝑦𝑧

𝑣5 = 𝑝 [𝑥 ]

𝑣5 (𝑣4)

𝑣4 = inr (𝑦𝑧)

𝑣13

𝑣13 = snd (𝑦𝑧)

𝑣10 = fst (𝑦𝑧)

𝑣10

𝑑

𝑑

𝑢

𝑣12

Fig. 2. Translation of the program in Figure 1a.

𝑆

𝑣6 −→

𝑣4 = 𝑤

𝑣5 = 𝑆

𝑣6 = unit

𝑤

𝑑

𝑣17 −→

𝑥

𝑣10 = 𝑝 [𝑥 ]

𝑣4

𝑣5

𝑣6

𝑣10

𝑣17

𝑣17 = cdr (𝑤)

𝑑

𝑣3

𝑣2

𝑣2 = 𝑣3 = nil

𝑣2 = true

𝑣9

𝑣10 (𝑣9)

𝑣9 = inl (𝑎)

𝑣13

𝑎
𝑣13 = 𝑤 ≠ nil ∧
car (𝑤) = 𝑎

𝑣13 = true

𝑑

𝑣25 −→

𝑤

𝑥

𝑣23 = fst (𝑦𝑧)

𝑣23

𝑑

𝑤 ′

𝑣10

𝑣9

𝑦𝑧

𝑑

𝑣25

𝑣10 = 𝑝 [𝑥 ]

𝑣10 (𝑣9)

𝑣9 = inr (𝑦𝑧)

𝑣27

𝑣27 = snd (𝑦𝑧)

𝑤

𝑥

𝑤

𝑥

Fig. 3. Translation of the program in Figure 1b.

7

David Chiang and Chung-chieh Shan

edge that “calls” it could even have an input node that depends on one of its output nodes. What
kinds of functions would translate into such rules?

ACKNOWLEDGMENTS
This material is based upon work supported by the National Science Foundation under Grant
Nos. 2019291 and 2019266. Any opinions, findings, and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily reflect the views of the National
Science Foundation.

REFERENCES
Michel Bauderon and Bruno Courcelle. 1987. Graph expressions and graph rewriting. Mathematical Systems Theory 20

(1987), 83–127.

David Chiang and Darcey Riley. 2020. Factor Graph Grammars. In Proc. Conference on Neural Information Processing Systems.
Michael Collins. 1997. Three Generative, Lexicalised Models for Statistical Parsing. In Proc. Annual Meeting of the Association
for Computational Linguistics and European Chapter of the Association for Computational Linguistics (ACL-EACL). 16–23.
Frank Drewes, Hans-Jörg Kreowski, and Annegret Habel. 1997. Hyperedge Replacement Graph Grammars. In Handbook of

Graph Grammars and Computing by Graph Transformation, Grzegorz Rozenberg (Ed.). World Scientific, 95–162.

Annegret Habel and Hans-Jörg Kreowski. 1987. May we introduce to you: Hyperedge replacement. In Proc. Third International
Workshop on Graph Grammars and Their Application to Computer Science (Lecture Notes in Computer Science, Vol. 291).
Springer, 15–26.

John Hale. 2004. The Information-Processing Difficulty of Incremental Parsing. In Proceedings of the Workshop on Incremental

Parsing: Bringing Engineering and Cognition Together. 58–65.

Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proc. Annual Meeting of the Association for

Computation Linguistics. 423–430.

Dexter Kozen. 1981. Semantics of Probabilistic Programs. J. Comput. System Sci. 22, 3 (1981), 328–350.
Frank R. Kschischang, Brendan J. Frey, and Hans-Andrea Loeliger. 2001. Factor Graphs and the Sum-Product Algorithm.

IEEE Trans. Information Theory 47, 2 (2001), 498–519.

Bernard Lang. 1988. Parsing Incomplete Sentences. In Proc. International Conference on Computational Linguistics (COLING).

365–371.

Sam Staton. 2017. Commutative Semantics for Probabilistic Programming. In Programming Languages and Systems:
Proceedings of ESOP 2017, 26th European Symposium on Programming (Lecture Notes in Computer Science, 10201), Yang
Hongseok (Ed.). Springer, 855–879.

Jan-Willem van de Meent, Brooks Paige, Hongseok Yang, and Frank Wood. 2018. An Introduction to Probabilistic Program-

ming. arXiv:1809.10756.

8

