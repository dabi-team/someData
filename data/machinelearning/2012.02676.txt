0
2
0
2

c
e
D
4

]

G
L
.
s
c
[

1
v
6
7
6
2
0
.
2
1
0
2
:
v
i
X
r
a

Community detection using fast low-cardinality
semideﬁnite programming

Po-Wei Wang
Machine Learning Department
Carnegie-Mellon University
Pittsburgh, PA
poweiw@cs.cmu.edu

J. Zico Kolter
Department of Computer Science
Carnegie-Mellon University &
Bosch Center for Artiﬁcial Intelligence
Pittsburgh, PA
zkolter@cs.cmu.edu

Abstract

Modularity maximization has been a fundamental tool for understanding the com-
munity structure of a network, but the underlying optimization problem is noncon-
vex and NP-hard to solve. State-of-the-art algorithms like the Louvain or Leiden
methods focus on different heuristics to help escape local optima, but they still de-
pend on a greedy step that moves node assignment locally and is prone to getting
trapped. In this paper, we propose a new class of low-cardinality algorithm that
generalizes the local update to maximize a semideﬁnite relaxation derived from
max-k-cut. This proposed algorithm is scalable, empirically achieves the global
semideﬁnite optimality for small cases, and outperforms the state-of-the-art algo-
rithms in real-world datasets with little additional time cost. From the algorithmic
perspective, it also opens a new avenue for scaling-up semideﬁnite programming
when the solutions are sparse instead of low-rank.

1 Introduction

Community detection, that is, ﬁnding clusters of densely connected nodes in a network, is a fun-
damental topic in network science. A popular class of methods for community detection, called
modularity maximization [34], tries to maximize the modularity of the cluster assignment, the qual-
ity of partitions deﬁned by the difference between the number of edges inside a community and
the expected number of such edges. However, optimizing modularity is NP-hard [14], so modern
methods focus on heuristics to escape local optima. A very popular heuristic, the Louvain method
[8], greedily updates the community membership node by node to the best possible neighboring
community that maximizes the modularity function’s gain. Then it aggregates the resulting parti-
tion and repeats until no new communities are created. The Louvain method is fast and effective
[48], although it still gets trapped at local optima and might even create disconnected communities.
A follow-up work, the Leiden method [43], resolves disconnectedness by an additional reﬁnement
step, but it still relies on greedy local updates and is prone to local optima.

In this paper, we propose the Locale (low-cardinality embedding) algorithm, which improves the
performance of community detecion above the current state of the art. It generalizes the greedy
local move procedure of the Louvain and Leiden methods by optimizing a semideﬁnite relaxation of
modularity, which originates from the extremal case of the max-k-cut semideﬁnite approximation
[22, 20, 2] when k goes to inﬁnity. We provide a scalable solver for this semideﬁnite relaxation by ex-
ploiting the low-cardinality property in the solution space. Traditionally, semideﬁnite programming
is considered unscalable. Recent advances in Riemannian manifold optimization [38, 16, 1] provide
a chance to scale-up by optimizing directly in a low-rank solution space, but it is not amenable in
many relaxations like the max-k-cut SDP, where there are nonnegativity constraints on all entries of

34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

 
 
 
 
 
 
the semideﬁnite variable X. However, due to the nonnegativity constraints, the solution X is sparse
and a low-cardinality solution in the factorized space V sufﬁces. These observations lead to our
ﬁrst contribution, which is a scalable solver for low-cardinality semideﬁnite programming subject
to nonnegative constraints. Our second contribution is using this solver to create a generalization of
existing community detection methods, which outperforms them in practice because it is less prone
to local optima.

We demonstrate in the experiments that our proposed low-cardinality algorithm is far less likely
to get stuck at local optima than the greedy local move procedure. On small datasets that are
solvable with a traditional SDP solver, our proposed solver empirically reaches the globally op-
timal solution of the semideﬁnite relaxation given enough cardinality and is orders of magni-
tude faster than traditional SDP solvers. Our method uniformly improves over both the stan-
dard Louvain and Leiden methods, which are the state-of-the-art algorithms for community de-
tection, with 2.2x time cost. Additionally, from the perspective of algorithmic design, the low-
cardinality formulation opens a new avenue for scaling up semideﬁnite programming when the so-
lutions tend to be sparse instead of low-rank. Source code for our implementation is available at
https://github.com/locuslab/sdp_clustering.

2 Background and related work

Notation. We use upper-case letters for matrices and lower-case letters for vectors and scalars. For
0, the entry-wise nonnegative
a matrix X, we denote the symmetric semideﬁnite constraint as X
for the
constraint as X
≥
2-norm, and top+
k (v) for the sparsiﬁed vector of the same shape containing the largest k nonnegative
coordinates of v. For example, top+
2)) = (0, 0). For a
function Q(V ), we use Q(vi) for the same function taking the column vector vi while pinning all
other variables. We use [r] for the set

0. For a vector v, we use card(v) for the number of nonzero entries,

, and e(t) for the basis vector of coordinate t.

1, 3)) = (0, 3), and top+

1, . . . , r

2 ((

1 ((

1,

−

−

(cid:23)

−

k

k

v

{

}

Modularity maximization. Modularity was proposed in [34] to measure the quality of densely
connected clusters. For an undirected graph with a community assignment, its modularity is given
by

Q(c) :=

1
2m Xij

aij

(cid:20)

−

didj
2m (cid:21)

δ(ci = cj),

(1)

∈

P

ij aij/2 is the sum of edge weights, and ci

where aij is the edge weight connecting nodes i and j, di =

j aij is the degree for node i, m =
[r] is the community assignment for node i among
the r possible communities. The notation δ(ci = cj) is the Kronecker delta, which is one when ci =
P
cj and zero otherwise. The higher the modularity, the better the community assignment. However, as
shown in [14], optimizing modularity is NP-hard, so researchers instead focus on different heuristics,
including spectral methods [33], simulated annealing [39], linear programming and semideﬁnite
programming [2, 26]. The most popular heuristic, the Louvain method [8], initializes each node
with a unique community and updates the modularity Q(c) cyclically by moving ci to the best
neighboring communities [27, 33]. When no local improvement can be made, it aggregates nodes
with the same community and repeats itself until no new communities are created. Experiments
show that it is fast and performant [48] and can be further accelerated by choosing update orders
wisely [36, 4]. However, the local steps can easily get stuck at local optima, and it may even
create disconnected communities [43] containing disconnected components. In follow-up work, the
Leiden method [43] ﬁxes the issue of disconnected communities by adding a reﬁnement step that
splits disconnected communities before aggregation. However, it still depends on greedy local steps
and still suffers from local optima. Beyond modularity maximization, there are many other metrics
to optimize for community detection, including asymptotic suprise [42], motif-aware [30] or higher
order objectives [49].

Semideﬁnite programming and clustering. Semideﬁnite programming (SDP) has been a power-
ful tool in approximating NP-complete problems [28, 37]. Speciﬁcally, the max-k-cut SDP [22, 20]
deeply relates to community detection, where max-k-cut maximizes the sum of edge weights be-
tween partitions, while community detection maximizes the sum inside partitions. The max-k-cut

2

SDP is given by the optimization problem

maximize
X

− Xij

aij xij , s.t. X

0, X

1/(k

−

≥ −

(cid:23)

1), diag(X) = 1.

(2)

{

≥

(cid:23)

X

0, X

−
0, Xii = 1

1, values of xij become discrete and are either 1 or

1),
When limiting the rank of X to be k
which works similarly to a Kronecker delta δ(ci = cj) [20]. If k goes to inﬁnity, the constraint set
reduces to
, and Swamy [41] provides a 0.75 approximation ratio
to correlation clustering on the relaxation (the bound doesn’t apply to modularity maximization).
However, these SDP relaxations are less practical because known semideﬁnite solvers don’t scale
with the numerous constraints, and the runtime of the rounding procedure converting the continuous
variables to discrete assignments grows with O(n2k). By considering max-2-cut iteratively at every
hierarchical level, Agarwal and Kempe [2] is able to perform well on small datasets by combining
SDPs with the greedy local move, but the method is still unscalable due to the SDP solver. DasGupta
and Desai [17] discussed the theoretical property of SDPs when there are only 2 clusters. Other
works [44, 26] also apply SDPs to solve clustering problems, but they don’t optimize modularity.

1/(k

−

−

}

Low-rank methods for SDP. One trick to scale-up semideﬁnite programming is to exploit its low-
rank structure when possible. The seminal works by Barvinok and Pataki [38, 6] proved that when
there are m constraints in an SDP, there must be an optimal solution with rank less than √2m, and
Barvinok [5] proved the bound to be tight. Thus, when the number of constraints m is subquadratic,
we can factorize the semideﬁnite solution X as V T V , where the matrix V requires only n√2m
memory instead of the original n2 of X. Burer and Monteiro [16] ﬁrst exploited this property and
combined it with L-BFGS to create the ﬁrst low-rank solver of SDPs. Later, a series of works on
Riemannian optimization [1, 12, 11, 45] further established the theoretical framework and extended
the domain of applications for the low-rank optimization algorithms. However, many SDPs, includ-
ing the max-k-cut SDP approximation that we use in this paper, have entry-wise constraints on X
like X

0, and thus the low-rank methods is not amenable to those problems.

≥

Copositive programming. The constraint DNN =
is connected to an area called “copositive programming” [31, 19, 15], which focuses on the sets

in our SDP relaxation

0, X

X

X

(cid:23)

≥

}

{

0

|

vT Xv

CP =

|

{

0,

X

(3)
Interestingly, both CP and CP ∗ are convex, but the set membership query is NP-hard. The coposi-
tive sets relate to semideﬁnite cone S =
vT Xv
DNN
CP

by the hierarchy

0, V

v
∀

r
∀

X

0,

≥

≥

≥

≥

N

∈

∈

V

S

}

{

0

}

{

|

|

.

and CP ∗ =

V T V

Rn×r,

v
∀
}
CP ∗.

(4)

⊇

⊇

⊇

4, the set DNN = CP ∗, but DNN ) CP ∗ for n
For low dimensions n
5 [23]. Optimization
over the copositive set is hard in the worst case because it contains the class of binary quadratic
programming [15]. Approximation through optimizing the V space has been proposed in [9, 24],
but it is still time-consuming because it requires a large copositive rank r = O(n2) [10].

≤

≥

3 The Locale algorithm and application to community detection

In this section, we present the Locale (low-cardinality embedding) algorithm for community detec-
tion, which generalizes the greedy local move procedure from the Louvain and Leiden methods. We
describe how to derive the low-cardinality embedding from the local move procedure, its connection
to the semideﬁnite relaxation, and then how to round the embedding back to the discrete commu-
nity assignments. Finally, we show how to incorporate this algorithm into full community detection
methods.

3.1 Generalizing the local move procedure by low-cardinality embeddings

State-of-the-art community detection algorithms like the Louvain and Leiden methods depend on a
core component, the local move procedure, which locally optimizes the community assignment for
a node. It was originally proposed by Kernighan and Lin [27] for graph cuts, and was later adopted
by Newman [33] to maximize the modularity Q(c) deﬁned in (1). The local move procedure in
[33, 8] ﬁrst initializes each node with a unique community, then updates the community assignment

3

Figure 1: An illustration of the low cardinality relaxation, where the discrete cluster assignment for
nodes is relaxed into a continuous and smooth space containing the original discrete set. The param-
eter k controls the cardinality, or equivalently the maximum number of overlapping communities a
node may belong to. When k = 1, we recover the original discrete set.

node by node and changes ci to a neighboring community (or an empty community) that maximizes
the increment of Q(ci). That is, the local move procedure is an exact coordinate ascent method on
the discrete community assignment c. Because it operates on the discrete space, it is prone to local
optima. To improve it, we will ﬁrst introduce a generalized maximum modularity problem such that
each node may belong to more than one community.

A generalized maximum modularity problem. To assign a node to more than one community,
we need to rewrite the Kronecker delta δ(ci = cj) in Q(c) as a dot product between basis vectors.
Let e(t) be the basis vector for community t with one in e(t)t and zeros otherwise. By creating an
assignment vector vi = e(ci) for each node i, we have δ(ci = cj) = vT
i vj , and we reparameterize
the modularity function Q(c) deﬁned in (1) as

Q(V ) :=

aij

1
2m Xij
[r], where r is the upper-bounds on number of communities,
. And the set becomes equivalent to the below unit norm and unit

didj
2m (cid:21)

vT
i vj.

(5)

−

∈

(cid:20)

Notice that the original constraint ci
becomes vi
t
e(t)
cardinality constraint in the nonnegative orthant.

∈ {

[r]

∈

}

|

e(t)

t

[r]

=

|

∈

}

{

vi

{

|

vi

∈

Rr
+,

vi

k

k

= 1, card(vi)

1

.

(6)

≤

}

vi

= 1) and the cardinality constraint (card(vi)

The constraint can be interpreted as the intersection between the curved probability simplex (vi
∈
Rr
1), where the latter constraint controls
+,
how many communities may be assigned to a node. Naturally, we can generalize the maximum
modularity problem by relaxing the cardinality constraint from 1 to k, where k is the maximun
number of overlaying communities a node may belong to. The generalized problem is given by

≤

k

k

maximize
V

Q(V ) :=

1
2m Xij

aij

(cid:20)

−

didj
2m (cid:21)

vT
i vj, s.t. vi

Rr
+,

∈

vi

k

k

= 1, card(vi)

k,

i.

∀

≤

(7)
The larger the k, the smoother the problem (7). When k = r the cardinality constraint becomes
trivial and the feasible space of V become smooth. The original local move procedure is simply
an exact coordinate ascent method when k = 1, and we now generalized it to work on arbitrary k
in a smoother feasible space of V . We call the resulting V the low cadinality embeddings and the
generalized algorithm the Locale algorithm. An illustration is given in Figure 1.

The Locale algorithm for low-cardinality embeddings. We ﬁrst prove that, just like the local
move procedure, there is a closed-form optimal solution for the subproblem Q(vi), where we opti-
mize on variable vi and pin all the other variables.
Proposition 1. The subproblem for variable vi
Q(vi), s.t. vi

= 1, card(vi)

(8)

vi

k

maximize
vi

Rr
+,

∈

k

k

≤

admits the following optimal solution

vi = g/

g

k

k

, where g =

e(t) with the max (
top+
k (

Q(vi))

(cid:26)

∇

∇

Q(vi))t

if
Q(vi)
∇
otherwise

0

,

≤

(9)

4

5

4

 local optimum in greedy local moves is escaped

gram matrix 

3

1

2

6

7

8

(1:1)
(2:1)
(3:1)
(4:1)
(5:1)
(6:1)
(7:1)
(8:1)
iter 0

(3:0.7  4:0.7)
(6:0.7  7:0.7)
(4:0.8  5:0.5)
(4:0.7  5:0.7)
(4:0.9  5:0.5)
(7:0.8  8:0.5)
(7:0.6  8:0.7)
(7:0.9  8:0.5)
iter 1

(4:0.8  5:0.6)
(7:0.8  8:0.6)
(4:0.8  5:0.6)
(4:0.8  5:0.6)
(4:0.8  5:0.6)
(7:0.8  8:0.6)
(7:0.8  8:0.6)
(7:0.8  8:0.6)
iter 2

Figure 2: An example that the Locale algorithm escapes the local optimum in greedy local move pro-
cedure. Numbers in the parentheses are the low-cardinality embeddings in a sparse index : value
format, where we compress a sparse vector with its top-k nonzero entries. The above bottleneck
graph was used in the Leiden paper [43] to illustration local optima, where a greedy local move
procedure following the order of the nodes gets stuck at the local optima in the red box, splitting
node 1 and 2 from the correct communities because of its unit cardinality constraint. In contrast,
the Locale (low-cardinality embeddings) algorithm escapes the local optima because it has an addi-
tional channel for the top-k communities to cross the bottleneck. The gram matrix of the resulting
embeddings shows that it perfectly identiﬁes the communities.

where top+
For the special case
there are multiple t with maximum (

Q(vi)

∇

≤

k (q) is the sparsiﬁed vector containing the top-k-largest nonnegative coordinates of q.
0, we choose the t with maximum (vi)t from the previous iteration if

Q(vi))t.

∇

We list the proof in Appendix A. With the close-form solution for every subproblem, we can now
generalize the local move procedure to a low-cardinality move procedure that works on arbitrary k.
We ﬁrst initialize every vector vi with a unique vector e(i), then perform the exact block coordinate
ascent method using the optimal update (9) cycling through all variables vi, i = 1, . . . , n, till con-
vergence. We could also pick coordinate randomly, and because the updates are exact, we have the
following guarantee.
Theorem 2. Applying the low-cardinality update iteratively on random coordinates1, the projected
gradient of the iterates converges to zero at O(1/T ) rate, where T is the number of iterations.

−

Q(vi) by (

j djvj and compute
card(A)

We list the proof in Appendix B. When implementing the Locale algorithm, we store the matrix V
in a sparse format since it has a ﬁxed cardinality upper bound, and perform all the summation using
sparse vector operations. We maintain a vector z =
−
∇
di
P
divi). This way, updating all vi once takes O
time, where the log k term
2m (z
comes from the partial sort to implement the top+
) operator. Taking a small k (we pick k = 8
(cid:0)
k (
·
in practice), the experiments show that it scales to large networks without too much additional time
cost to the greedy local move procedure. Implementation-wise, we choose the updating order by the
smart local move [36, 4]. We initialize r to be the number of nodes and increase it when
0
and there is no free coordinate. This corresponds to the assignment to a new “empty community”
in the Louvain method [8] (which also increases the r). At the worst case, the maximum r is n
k,
but we have never observed this in the experiments, where in practice r is always less than 2n. For
illustration, we provide an example from Leiden method [43] in Figure 2 showing that, because of
the relaxed cardinality constraint, the Locale algorithm is less likely to get stuck at local optima
compared to the greedy local move procedure.

j aij vj)

k log k

Q(vi)

P

∇

≤

(cid:1)

·

·

Connections to correlation clustering SDP and copositive programming. Here we connect the
Locale solution to an SDP relaxation of the generalized modularity maximization problem (7). Let
r to be large enough2 and let k = r to drop the cardinality constraint, the resulting feasible gram
matrix of V becomes (the dual of) the copositive constraint

V T V

{

V

|

≥

0, diag(V T V ) = 1

,

}

(10)

1The proof can also be done with a cyclic order using Lipschitz continuity, but for simplicity we focus on

the randomized version in our proof, which contains largely the same arguments and intuition.

2At the worse case r = n(n + 1)/2 − 4 sufﬁces [10, Theorem 4.1].

5

r
o
r
r
e

e
v
i
t
a
e
R

l

100

10−1

10−2

10−3

10−4

10−5

10−6

zachary  =34 deg=4.6

100 polbook  =105 deg=8.4

football  =115 deg=10.7

10−1

10−2

10−3

10−4

10−5

10−6

10−3

10−1

10−1

10−2

10−3

10−4

10−5

10−6

10−3

10−1

101

ru  i g time (in sec)

10−3

10−1

101

SCS

Greedy local moves

Locale, k=8

Locale, k= 

Figure 3: Comparing the relative error to optimal objective values and the running time in the
semideﬁnite relaxation of maximum modularity. The optimal values are obtained by running the
SCS [35], a splitting conic solver, for 3k iterations. The greedy local move procedure gets stuck
pretty early at a local optimum (even for the original modularity maximization problem). The Locale
algorithm is able to give a good approximation with cardinality k = 8, and is able to reach the global
optima with k = n. Further, it is 100 to 1000 times faster than SCS, which is already orders of
magnitude faster the than state-of-the-art interior point methods.

which can be further relaxed to the semideﬁnite constraint

X

X

0, X

0, diag(X) = 1

.

(11)

≥
This semideﬁnite constraint has been proposed as a relaxation for correlation clustering in [41]. With
these relaxations, the complete SDP relaxation for the (generalized) maximum modularity problem
is

(cid:23)

{

}

|

maximize

X Xij

aij

(cid:20)

−

didj
2m (cid:21)

xij , s.t. X

0, X

(cid:23)

≥

0, diag(X) = 1.

(12)

We use the SDP relaxation to certify whether the Locale algorithm reaches the global optima, given
enough cardinality k. That is, if the objective value given by the Locale algorithm meets the SDP
relaxation (solvable via an SDP solver), it certiﬁes the globally optimality of (7). In the experiments
for small datasets that is solvable via SDP solvers, we show that a very low cardinality k = 8 is
enough to approximate the optimal solution to a difference of 10−4, and running the Locale algo-
rithm with k = n recovers the global optimum. In addition, our algorithm is orders-of-magnitude
faster than the state-of-the-art SDP solvers.

3.2 Rounding by changing the cardinality constraint

After obtaining the embeddings for the generalized modularity maximization algorithm (7), we need
to convert the embedding back to unit cardinality to recover the community assignment for the
original maximum modularity problem. This is achieved by running the Locale algorithm with the
k = 1 constraint, starting at the previous solution. Also, since the rounding procedure reduces all
embeddings to unit cardinality after the ﬁrst sweep, this is equivalent to running the local move
procedure of the Louvain method, but initialized with higher-cardinality embeddings. Likewise,
we could also increase the cardinality constraint to update a unit cardinality solution to a higher
cardinality solution. These upgrade and downgrade steps can be performed iteratively to increase
the quality of the solution slowly, but we ﬁnd that it is more efﬁcient to only do the downgrade steps
in the overall multi-level algorithm. The rounding process has the same complexity as the Locale
algorithm since it is a special case of the algorithm with k = 1.

3.3 The Leiden-Locale algorithm for community detection

Here, we assemble all the aforementioned components and build the Leiden-Locale algorithm for
community detection. We use the Leiden method [43] as a framework and replace the local move
procedure with the Locale algorithm followed by the rounding procedure. While the results are bet-
ter with more inner iterations of the Locale algorithm, we found that two inner iterations followed

6

 
by the rounding procedure is more efﬁcient in the overall multi-level algorithm over multiple itera-
tions, while substantially improving over past works. We list the core pseudo-code below, and the
subroutines can be found in the Appendix E.

do

Algorithm 1 The Leiden-Locale method
1: procedure LEIDEN-LOCALE(Graph G, Partition P )
2:
3:
4:
5:
6:
7:
8: end procedure

E
←
P
←
G, P , done
while not done
return P

LocaleEmbeddings(G, P )
LocaleRounding(G, P , E )

←

⊲ Replace the LocalMove(G, P ) in Leiden

LeidenRefineAggregate(G, P )

⊲ [43, Algorithm A.2, line 5-9]

Because we still use the reﬁnement step from the Leiden algorithm, we have the following guarantee.

Theorem 3. [43, Thm. 5] The communities obtained from the Leiden-Locale algorithm are con-
nected.

Since we only perform two rounds of updates of the Locale algorithm, it adds relatively little over-
head and complexity to the Leiden algorithm. However, experiments show that the boost is signif-
icant. The Leiden-Locale algorithm gives consistently better results than the other state-of-the-art
methods.

4 Experimental results

In this section, we evaluate the Locale algorithm with other state-of-the-art methods. We show that
the Locale algorithm is effective on the semideﬁnite relaxation of modularity maximization, im-
proves the complexity from O(n6) to O(card(A)k log k) over SDP solvers, and scales to millions
of variables. Further, we show that on the original maximum modularity problem, the Locale al-
gorithm signiﬁcantly improves the greedy local move procedure. When used on the community
detection problem, the combined Leiden-Locale algorithm provides a 30% additional performance
increase over ten iterations and is better than all the state-of-the-art methods on the large-scale
datasets compared in the Leiden paper [43], with 2.2x the time cost to the Leiden method. The code
for the experiment is available at the supplementary material.

Comparison to SDP solvers on the semideﬁnite relaxation of maximum modularity. We com-
pare the Locale algorithm to the state-of-the-art SDP solver on the semideﬁnite relaxation (12) of
the maximum modularity problem. We show that it converges to the global optimum of SDP on
the veriﬁable datasets and is much faster than the SDP method. We use 3 standard toy networks
that are small enough to be solvable via an SDP solver, including zachary [50], polbook [34], and
football [21]. Typically, primal-dual interior-point methods [40] have cubic complexity in the
number of variables, which is n2 in our problem, so the total complexity is O(n6). Moreover, the
canonical SDP solver requires putting the nonnegativity constraint on the diagonal of the semideﬁ-
nite variable X, leading a much higher number of variables. For fairness, we choose to compare with
a new splitting conic solver, the SCS [35], which supports splitting variables into Cartesian products
of cones, which is much more efﬁcient than pure SDP solver in this kind of problem. We run the
SCS solver for 3k iterations for the reference optimal objective value. Also, since the solution of
SCS might not be feasible, we project the solution back to the feasible set by iterative projections.

Figure 3 shows the plot of difference to the optimal objective value and the running time. The greedy
local move procedure gets stuck at a local optimum early in the plot, but the Locale algorithm gives
a decent approximation at a low cardinality k = 8. At k = n, both the Locale algorithm and the
SCS solver reach the optimum for the SDP relaxation (12), but Locale is 193x faster than SCS (in
average) for reaching 10−4 difference to the optimum, and the speedup scales with the dimensions.

The results demonstrate the effectiveness and the orders of speedup of the proposed low-cardinality
method, opening a new avenue for scaling-up semideﬁnite programming when the solution is low-
cardinality instead of low-rank.

7

Table 1: Overview of the empirical networks and the modularity after the greedy local move proce-
dure (running till convergence) and the Locale algorithm (running for 2 rounds or till convergence).

Dataset

Nodes Degree

DBLP
Amazon
IMDB
Youtube
Live Journal

317 080
334 863
374 511
1 134 890
3 997 962

6.6
5.6
80.2
5.3
17.4

Greedy
local moves

The Locale algorithm
full update
2 rounds

0.5898
0.6758
0.6580
0.6165
0.6658

0.6692
0.7430
0.6697
0.6294
0.6540

0.8160
0.9154
0.6852
0.7115
0.7585

Comparison with the local move procedure.
In this experiment, we show that the Locale al-
gorithm scales to millions of nodes and signiﬁcantly improves the local move procedure in em-
pirical networks. We compare to 5 large-scale networks, including DBLP, Amazon, Youtube [47],
IMDB[46],and Live Journal [3, 29]. These are the networks that were also studied in the Lei-
den [43] and Louvain papers [8]. For the greedy local move procedure, we run it iteratively till
convergence (or till the function increment is less than 10−8 after n consecutive changes to avoid
ﬂoating-point errors). For the Locale algorithm, we test two different settings: running only two
rounds of updates or running it till convergence, followed by the rounding procedure.

Table 1 shows the comparison results. With only 2 rounds of updates, the Locale algorithm already
improves the greedy local move procedure except for the Live Journal dataset. When running a
full update, the algorithm signiﬁcantly outperforms the greedy local moves on all datasets. Moreover,
it is even comparable with running a full (multi-level) iteration of the Louvain and Leiden methods,
as we will see in the next experiments. The results suggest that the Locale algorithm indeed improves
the greedy local move procedure. With the algorithm, we create a generalization of the Leiden
method and show that it outperforms the Leiden methods in the next experiment.

Comparison with state-of-the-art community detection algorithms.
In the experiments, we
show that the Leiden-Locale algorithm (Locale in the table) outperforms the Louvain and Leiden
methods, the state-of-the-art community detection algorithms. For more context, the Louvain and
Leiden methods are the state-of-the-art multi-level algorithm that performs the greedy local move
procedure, reﬁnement (for Leiden only), and aggregation of graph at every level until convergence.
Further, they can be run for multiple iterations using previous results as initialization3, so we con-
sider the settings of running the algorithm once or for 10 iterations, where one iteration means
running the whole multi-level algorithm until convergence. Speciﬁcally, for the 10-iteration setting,
we take the best results over 10 random trials for the Louvain and Leiden methods shown in the
Leiden paper [43]. Since the Locale algorithm is less sensitive to random seeds, we only need to
run it once in the setting. This make it much faster than the Leiden method with 10 trials, while
performing better. Further, we run the inner update twice for the Locale algorithm since we found
that it is more efﬁcient in the overall multi-level algorithm over multiple iterations.

Table 2 shows the result of comparisons. In the one iteration setting, the Locale algorithm out-
performs both the Louvain and Leiden methods (except for the Live Journal dataset), with an
2.2x time cost in average. Using the Louvain method as a baseline, the Locale method provides
a 0.0052 improvement in average and the Leiden method provides a 0.0034 improvement. These
improvement are signiﬁcant since little changes in modularity can give completely different com-
munity assignments [2]. For the 10 iteration setting, we uniformly outperform both Louvain and
Leiden methods in all datasets and provide a 30% additional improvement over the Leiden method
using the Louvain method as a baseline.

5 Conclusion

In this paper, we have presented the Locale (low-cardinality embeddings) algorithm for community
detection. It generalizes the greedy local move procedure from the Louvain and Leiden methods by
approximately solving a low-cardinality semideﬁnite relaxation. The proposed algorithm is scalable

3The performance with low-number of iterations is also important because the Leiden method takes a default

of 2 iterations in the leidenalg package and 10 iterations in the paper [43].

8

Table 2: Overview of the empirical networks and the maximum modularity, running for 1 iterations
(running the full multi-level algorithm till no new communities are created) and for 10 iterations
(using results from the previous iteration as initialization). Note that results of Louvain [8] and
Leiden [43] methods are obtained in additional 10 random trials.

Max. modularity

1 iter

10 iters 10 trials [43]

10 iters

Dataset

Nodes Degree Louvain Leiden Locale Louvain

Leiden Locale

DBLP
Amazon
IMDB
Youtube
Live Journal

317 080
334 863
374 511
1 134 890
3 997 962

6.6
5.6
80.2
5.3
17.4

0.8201
0.9261
0.6951
0.7179
0.7528

0.8206
0.9252
0.7046
0.7254
0.7576

0.8273
0.9273
0.7054
0.7295
0.7531

0.8262
0.9301
0.7062
0.7278
0.7653

0.8387
0.9341
0.7069
0.7328
0.7739

0.8397
0.9344
0.7070
0.7355
0.7747

(orders of magnitude faster than the state-of-the-art SDP solvers), empirically reaches the global
optimum of the SDP on small datasets that we can verify with an SDP solver. Furthermore, it
improves the local move update of the Louvain and Leiden methods and outperforms the state-of-
the-art community detection algorithms.

The Locale algorithm can also be interpreted as solving a generalized modularity problem (7) that
allows assigning at most k communities to each node, and this may be intrinsically a better ﬁt for
practical use because in a social network, a person usually belongs to more than one community.
Further, the Locale algorithm hints a new way to solve heavily constrained SDPs when the solution
is sparse but not low-rank. It scales to millions of variables, which is well beyond previous ap-
proaches. From the algorithmic perspective, it also opens a new avenue for scaling-up semideﬁnite
programming by the low-cardinality embeddings.

6 Broader impact

Although most of the work focuses on the mathematical notion of modularity maximization, the
community detection algorithms that result from these methods have a broad number of applica-
tions with both potentially positive and negative beneﬁts. Community detection methods have been
used extensively in social networks (e.g., [18]), where they can be used for advertisement, track-
ing, attribute learning, or recommendation of new connections. These may have positive effects
for the networks, of course, but as numerous recent studies also demonstrate potential negative con-
sequences of such social network applications [13]. In these same social networks, there are also
of course many positive applications of community detection algorithms. For example, researchers
have used community detection methods, including the Louvain method, to detect bots in social
networks [7], an activity that can bring much-needed transparency to the interactions that are be-
coming more common. While we do not explore such applications here, it is possible that the multi-
community methods we discuss can also have an impact on the design of these methods, again for
both positive or negative effects.

Ultimately, the method we present here does largely on community detection from a purely algo-
rithmic perspective, focusing on the modularity maximization objective, and provides gains that
we believe can improve the quality of existing algorithms as a whole. Ultimately, we do believe
that presenting these algorithms publicly and evaluating them fairly, we will at least be able to bet-
ter establish the baseline best performance that these algorithms can achieve. In other words, we
hope to separate the algorithmic goal of modularity maximization (which our algorithm addresses),
which is an algorithmic question, from the more applied question of what can be done with the clus-
ters assigned by this “best available” modularity maximization approach. Speciﬁcally, if we could
achieve the true maximum modularity community assignment for practical graphs, what would this
say about the resulting communities or applications? We hope to be able to study this in future
work, as our approach and others push forward the boundaries on how close we can get to this “best”
community assignment.

9

Acknowledgments

Po-Wei Wang is supported by a grant from the Bosch Center for Artiﬁcial Intelligence.

References

[1] P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization algorithms on matrix manifolds.

Princeton University Press, 2009.

[2] G. Agarwal and D. Kempe. Modularity-maximizing graph communities via mathematical

programming. The European Physical Journal B, 66(3):409–418, 2008.

[3] L. Backstrom, D. Huttenlocher, J. Kleinberg, and X. Lan. Group formation in large social
In Proceedings of the 12th ACM SIGKDD

networks: membership, growth, and evolution.
international conference on Knowledge discovery and data mining, pages 44–54, 2006.

[4] S.-H. Bae, D. Halperin, J. D. West, M. Rosvall, and B. Howe. Scalable and efﬁcient ﬂow-
based community detection for large-scale graph analysis. ACM Transactions on Knowledge
Discovery from Data (TKDD), 11(3):1–30, 2017.

[5] A. Barvinok. A remark on the rank of positive semideﬁnite matrices subject to afﬁne con-

straints. Discrete & Computational Geometry, 25(1):23–31, 2001.

[6] A. I. Barvinok. Problems of distance geometry and convex properties of quadratic maps. Dis-

crete & Computational Geometry, 13(2):189–202, 1995.

[7] D. M. Beskow and K. M. Carley. Bot conversations are different: leveraging network metrics
In 2018 IEEE/ACM International Conference on Advances in

for bot detection in twitter.
Social Networks Analysis and Mining (ASONAM), pages 825–832. IEEE, 2018.

[8] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre. Fast unfolding of communities
in large networks. Journal of statistical mechanics: theory and experiment, 2008(10):P10008,
2008.

[9] I. M. Bomze, F. Jarre, and F. Rendl. Quadratic factorization heuristics for copositive program-

ming. Mathematical Programming Computation, 3(1):37–57, 2011.

[10] I. M. Bomze, P. J. Dickinson, and G. Still. The structure of completely positive matrices
according to their cp-rank and cp-plus-rank. Linear algebra and its applications, 482:191–
206, 2015.

[11] N. Boumal. A riemannian low-rank method for optimization over semideﬁnite matrices with

block-diagonal constraints. arXiv preprint arXiv:1506.00575, 2015.

[12] N. Boumal, B. Mishra, P.-A. Absil, R. Sepulchre, et al. Manopt, a matlab toolbox for optimiza-

tion on manifolds. Journal of Machine Learning Research, 15(1):1455–1459, 2014.

[13] E. Bozdag and J. van den Hoven. Breaking the ﬁlter bubble: democracy and design. Ethics

and Information Technology, 17(4):249–265, 2015.

[14] U. Brandes, D. Delling, M. Gaertler, R. Gorke, M. Hoefer, Z. Nikoloski, and D. Wagner. On
modularity clustering. IEEE transactions on knowledge and data engineering, 20(2):172–188,
2007.

[15] S. Burer. A gentle, geometric introduction to copositive optimization. Mathematical Program-

ming, 151(1):89–116, 2015.

[16] S. Burer and R. D. Monteiro. Local minima and convergence in low-rank semideﬁnite pro-

gramming. Mathematical Programming, 103(3):427–444, 2005.

[17] B. DasGupta and D. Desai. On the complexity of newman’s community ﬁnding approach for
biological and social networks. Journal of Computer and System Sciences, 79(1):50–67, 2013.

[18] N. Du, B. Wu, X. Pei, B. Wang, and L. Xu. Community detection in large-scale social networks.
In Proceedings of the 9th WebKDD and 1st SNA-KDD 2007 workshop on Web mining and
social network analysis, pages 16–25, 2007.

[19] M. D¨ur. Copositive programming–a survey. In Recent advances in optimization and its appli-

cations in engineering, pages 3–20. Springer, 2010.

10

[20] A. Frieze and M. Jerrum. Improved approximation algorithms for max k-cut and max bisection.
In International Conference on Integer Programming and Combinatorial Optimization, pages
1–13. Springer, 1995.

[21] M. Girvan and M. E. Newman. Community structure in social and biological networks. Pro-

ceedings of the national academy of sciences, 99(12):7821–7826, 2002.

[22] M. X. Goemans and D. P. Williamson. Improved approximation algorithms for maximum cut
and satisﬁability problems using semideﬁnite programming. Journal of the ACM (JACM), 42
(6):1115–1145, 1995.

[23] L. J. Gray and D. G. Wilson. Nonnegative factorization of positive semideﬁnite nonnegative

matrices. Linear algebra and its applications, 31:119–127, 1980.

[24] P. Groetzner and M. D¨ur. A factorization method for completely positive matrices. Linear

Algebra and its Applications, 591:1–24, 2020.

[25] A. Hagberg, P. Swart, and D. S Chult. Exploring network structure, dynamics, and function us-
ing networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United
States), 2008.

[26] A. Javanmard, A. Montanari, and F. Ricci-Tersenghi. Phase transitions in semideﬁnite relax-
ations. Proceedings of the National Academy of Sciences, 113(16):E2218–E2223, 2016.
[27] B. W. Kernighan and S. Lin. An efﬁcient heuristic procedure for partitioning graphs. The Bell

system technical journal, 49(2):291–307, 1970.

[28] J. B. Lasserre. Global optimization with polynomials and the problem of moments. SIAM

Journal on optimization, 11(3):796–817, 2001.

[29] J. Leskovec, K. J. Lang, A. Dasgupta, and M. W. Mahoney. Community structure in large
networks: Natural cluster sizes and the absence of large well-deﬁned clusters. Internet Mathe-
matics, 6(1):29–123, 2009.

[30] P.-Z. Li, L. Huang, C.-D. Wang, and J.-H. Lai. Edmot: An edge enhancement approach for
motif-aware community detection. In Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, pages 479–487, 2019.

[31] J. E. Maxﬁeld and H. Minc. On the matrix equation X T X = A. Proceedings of the Edinburgh

Mathematical Society, 13(2):125–129, 1962.

[32] A. Medus, G. Acu˜na, and C. O. Dorso. Detection of community structures in networks via
global optimization. Physica A: Statistical Mechanics and its Applications, 358(2-4):593–604,
2005.

[33] M. E. Newman. Finding community structure in networks using the eigenvectors of matrices.

Physical review E, 74(3):036104, 2006.

[34] M. E. Newman and M. Girvan. Finding and evaluating community structure in networks.

Physical review E, 69(2):026113, 2004.

[35] B. O’Donoghue, E. Chu, N. Parikh, and S. Boyd. Conic optimization via operator splitting and
homogeneous self-dual embedding. Journal of Optimization Theory and Applications, 169(3):
1042–1068, June 2016. URL http://stanford.edu/~boyd/papers/scs.html.

[36] N. Ozaki, H. Tezuka, and M. Inaba. A simple acceleration method for the louvain algorithm.

International Journal of Computer and Electrical Engineering, 8(3):207, 2016.

[37] P. A. Parrilo. Semideﬁnite programming relaxations for semialgebraic problems. Mathematical

programming, 96(2):293–320, 2003.

[38] G. Pataki. On the rank of extreme matrices in semideﬁnite programs and the multiplicity of

optimal eigenvalues. Mathematics of operations research, 23(2):339–358, 1998.

[39] J. Reichardt and S. Bornholdt. Statistical mechanics of community detection. Physical review

E, 74(1):016110, 2006.

[40] J. F. Sturm. Using sedumi 1.02, a matlab toolbox for optimization over symmetric cones.

Optimization methods and software, 11(1-4):625–653, 1999.

[41] C. Swamy. Correlation clustering: maximizing agreements via semideﬁnite programming. In
Proceedings of the ﬁfteenth annual ACM-SIAM symposium on Discrete algorithms, pages 526–
527. Society for Industrial and Applied Mathematics, 2004.

11

[42] V. A. Traag, R. Aldecoa, and J.-C. Delvenne. Detecting communities using asymptotical sur-

prise. Physical Review E, 92(2):022816, 2015.

[43] V. A. Traag, L. Waltman, and N. J. van Eck. From louvain to leiden: guaranteeing well-

connected communities. Scientiﬁc reports, 9(1):1–12, 2019.

[44] J. Wang, T. Jebara, and S.-F. Chang. Semi-supervised learning using greedy max-cut. Journal

of Machine Learning Research, 14(Mar):771–800, 2013.

[45] P.-W. Wang, W.-C. Chang, and J. Z. Kolter. The mixing method: low-rank coordinate descent
for semideﬁnite programming with diagonal constraints. arXiv preprint arXiv:1706.00476,
2017.

[46] D. J. Watts and S. H. Strogatz. Collective dynamics of ‘small-world’networks. nature, 393

(6684):440, 1998.

[47] J. Yang and J. Leskovec. Deﬁning and evaluating network communities based on ground-truth.

Knowledge and Information Systems, 42(1):181–213, 2015.

[48] Z. Yang, R. Algesheimer, and C. J. Tessone. A comparative analysis of community detection

algorithms on artiﬁcial networks. Scientiﬁc reports, 6:30750, 2016.

[49] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich. Local higher-order graph clustering.
In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pages 555–564, 2017.

[50] W. W. Zachary. An information ﬂow model for conﬂict and ﬁssion in small groups. Journal of

anthropological research, 33(4):452–473, 1977.

12

A Proof of Proposition 1

Note that in problem (7) the term (aii
subproblem Q(vi) for variable vi, we can ignore the constant term and write the gradient

i vi is constant because

= 1. Thus, in the
Q(vi) as

didi/(2m))vT

vi

−

k

k

∇

Q(vi) =

∇

1
2m Xj6=i (cid:0)

aij

didj
2m

−

(cid:1)

vj.

(13)

Further, since there is no vi term in
variable vi becomes qT vi with q =
subproblem reduces to

∇

Q(vi), the objective function Q(vi) for the subproblem of
∇
Q(vi), up to a constant. For simplicity, denote vi as v, and the

maximize
v

qT v, s.t. v

Rr
+,

v

= 1, card(v)

k.

∈
Let v∗ be the optimal solution of the above subproblem (8) (existence by compactness). When q
0, and
we have max(q)

0. With

≤

k

k

v

v

v

≤

k
max(q) = max(q)
k

k2 = 1, v
v
k2 ≥

≥
max(q)
k

k2 ≤ k
k
k1 = max(q)

v

k1, there is
vt

Xt

≥ Xt

qtvt = qT v.

(14)

0,

≤

(15)

Thus, e(t) with the max qt is the optimal solution in the ﬁrst case. For the second case, there is
at least one coordinate p such that qp > 0. Now we exclude the following two cases of inactive
coordinates by contradictions.

t = 0. Otherwise, suppose there is a v∗

t > 0 with qt < 0.

0, selecting v∗ = e(p) violates the optimality of v∗, a contradiction.

(When qt < 0) We know v∗
If qT v∗
If qT v∗ > 0, we have

≤

0 < qT v∗ < qT (v∗

e(t)v∗
t )

qT (v∗

e(t)v∗

t )/

v∗

−

≤

−

e(t)v∗
t k

−

k

,

(16)

also a contradiction to the optimality of v∗, because the last term is a feasible solution.

(When qt < q[k], where q[k] is the k-th largest value) We know v∗
be a coordinate j in the top-k-largest value that is not selected (v∗
way, we have

t = 0. Otherwise, there must
k. This

j = 0) because card(v∗)

≤

which contradicts to the optimality of v∗ because (v∗

qT v∗ < qT (v∗

e(t)v∗

−

t + e(j)v∗
e(t)v∗

t ),
t + e(j)v∗

t ) is a feasible solution.

(17)

Thus, by removing the inactive coordinates,
top+

k (q)T v∗, and the optimal solution follows from

the effective objective function qT v∗ becomes

−

v∗
i k

k

= 1 and top+

k (q)

0.

≥

13

B Proof of Theorem 2

Deﬁne the projected gradient (for maximization) as

grad(V ) = PΩ(V +

Q(V ))

V,

(18)

∇

−

where PΩ is the projection (under 2-norm) to the constraint set Ω of the optimization problem (7)

Ω =

V

{

|

vi

∈

Rr
+,

vi

k

k

= 1, card(vi)

k,

i = 1, . . . , n

,

(19)

≤

∀

}

and denote Ωi as the constraint for vi for the separable Ω. Because the cardinality constraint is an
union between ﬁnite hyperplanes, it is a closed set, which implies the constraint of the optimization
problem is a compact set. Thus, by the Weierstrass extreme value theorem, the function Q(V ) is
upper-bounded and must attain global maximum over the constraint.
Now we connect the exact update in the Locale algorithm with the projected gradient. Denote v+
i as
the update taken for the subproblem Q(vi). Because the Locale algorithm performs an exact update
(Proposition 1), we have

Q(vi)T v+
u

Q(vi)T u,

u

Ωi.

(20)

k

∀

∈

i ≥ ∇
2 = 1, we have

v+
i k

Further, because

∇
2 = 1 and
k
v+
i − ∇
This means that the update v+
i
update with the projected gradient, we need the following lemma.
Lemma 4. Denote the projection (under 2-norm) of a point x on a closed constraint set Ω as PΩ(x).
Then for any scalar α > 1 and vector q, we have
qT (PΩ(x + αq)

k
Q(vi)
k
is the projection of

Q(vi) to the constraint set Ωi. To connect the

PΩ(x + q))

Q(vi)
k

− ∇

(21)

≤ k

Ωi.

2,

∇

∈

u

u

∀

k

0

2

−

≥

The proof is listed in Appendix C. Taking the lemma with α

and let q =

0

qT (PΩi (vi + αq)

lim
−
α→0
where the last equation follows because v+
i
Further, apply the deﬁnition of projection PΩi (vi + q) again on the feasible vi, we have

is the projection of q on Ωi with

PΩi (vi + q)),

i −

k · k

≤

→ ∞
PΩi (x + q)) = qT (v+

Q(vi), we have

∇

(22)

= 1 constraint 4.

and after rearranging there is

PΩi (vi + q)

k

(vi + q)
k

−

2

vi

(vi + q)
k

−

≤ k

2,

k
Applying (22) to the equation above, we have

−

k

PΩi (vi + q)

2

vi

2qT (PΩi (vi + q)

vi).

−

≤

PΩi(vi + q)
The right hand side of the above equation equals the function increment Q(v+
i )

i −

vi).

vi

−

≤

k

k

2

2qT (v+

−
Now, taking expectation over the random coordinate i, we have

≤

−

k

k

PΩi(vi + q)

2

vi

2(Q(v+
i )

Q(vi)).

(23)

(24)

(25)

Q(vi). Thus,

−

(26)

1
n k

PΩ(V +

Q(V ))

∇

V

2 = E
k

k

−

PΩi (vi + q)

vi

−

2

k

≤

2E(Q(v+
i )

−

Q(vi)) = Q(V t+1)

Q(V t).
(27)
1
Q(V 0), where V ∗ is the global

−

−

Further, since Q(V t+1)
forms a telescoping sum, which is upper-bounded by Q(V ∗)
optimal solution of Q(V ). Substitute the deﬁnition of projected gradient (18), we have

Q(V t) is monotonic increasing, summing them over iterations 0 to T

−

−

T
n

min

t k

grad(V t)
k

2

≤

1
n

T −1

Xt=0

grad(V t)
k

k

2

≤

2(Q(V ∗)

−

Q(V 0)).

(28)

Thus, the projected gradient grad(V ) converges to zero at a O(1/T ) rate.

4Note that in Proposition 1, when q ≤ 0 and there are multiple maximum qt, we further select the t with

the maximum (vi)t in the previous iteration. This makes the limit to hold on the corner case q = 0.

14

C Proof for Lemma 4

By deﬁnition of the projection PΩ(x + q), we have

PΩ(x + q)

k

(x + q)
k

−

2

≤ k

PΩ(x + αq)

(x + q)
k

−

2.

Take out the q term out of the norm and rearrange, there is

PΩ(x + q)

PΩ(x + αq)

≤ k
Similarly, by deﬁnition of the projection PΩ(x + αq), there is

−

−

−

k

2

x
k

2

x
k

2qT (PΩ(x + αq)

PΩ(x + q)).

(29)

−

PΩ(x + αq)

x
k
−
Sum (29) and (30), the norms cancel, and we have

PΩ(x + q)

x
k

≤ k

−

k

2

2αqT (PΩ(x + q)

2

−

−

PΩ(x + αq)).

(30)

which implies

Thus, the result holds.

2(α

−

1)qT (PΩ(x + αq)

PΩ(x + q))

0,

≥

−

qT (PΩ(x + αq)

PΩ(x + q))

0.

≥

−

(31)

D Experiments on networks with ground truth

In this section, we compare results from the Leiden-Locale method on data with the ground truth for
partitions. The result is listed in Figure 4.

11

17

21

12

19

0

1

7

2

3

13

27

23

24

25

14

31

28

22

8

30

15

32

33

20

29

18

9

26

16

5

6

4

10

46

51

0

7

6

4

28

18

69

104

76

103

48

1

34

38

19

17
36

5

53

29

77

50

24
26

20

55

15

39

33
10

16

32

42
23

21

44

43

35

37

13

8

40

12
3
47

41
14

11

9

45

54

27

56

22

25

58

2

49

52

57

65

68

67

102

61

95

85
94

93

64

79
71

101
30
73

82

83

75

31

78

59

86

60

81

99

62

63

97

84

66
100
96
88 89

87

74

72

70

91 92

80

98

90

(a) zachary (ground truth = 4 clusters)

(b) polbook (ground truth = 3 clusters)

Figure 4: The comparison of the results from Leiden-Locale method to ground-truth partitions in the
zachary and polbook datasets. The position of each node is arranged using the 2D Fruchterman-
Reingold force-directed algorithm from the ground-truth using networkx [25], and the color of
each node indicates the solution community given by Leiden-Locale algorithm. The red edges
between nodes indicates the case when two nodes are inside the same cluster in the ground truth but
wasn’t assigned so in our algorithm. For zachary, the Leiden-Locale algorithm returns a perfect
answer comparing to the ground truth with a perfect modularity of 0.4197 [32]. For polbook, it
misclassiﬁes 18 over 105 nodes, but still attains a best known modularity of 0.5272 [2].

15

E Pseudo-code for the Leiden-Locale algorithm

Here we list the pseudo-code for the Leiden-Locale method. Note that we reuse Algorithm 2 in
Algorithm 3–4 for rounding and reﬁnement by changing its constraint and initialization. And in the
actual code, Algorithm 3–4 are combined as a single subroutine.

Algorithm 2 Optimization procedure for the Locale algorithm
1: procedure LOCALEEMBEDDINGS(Graph G, Partition P )
2:
3:
4:
5:
6:
7:

Initialize V with vi = e(i), i = 1, . . . , n.
Initialize the ring queue R with indices i = 1, . . . , n.
Let z =
while not yet converged do

P
i = R.pop()
Q(vi) =

n
j=1 djvj.

⊲ Pick an index from the ring queue
⊲ Sums only j in the same partition of i

−

di
2m (z
−
Q(vi))t,

∇
gi =

j∈P (i) aijvj
P
e(t) with the max (
top+
k (
vold
i = vi, vi = gi/
k
vold
i )
z = z + di(vi
Push all neighbors j with nonzero aij into the ring queue R if it is not already inside.

⊲ Perform the closed-form update
⊲ Maintain the z

divi)
if
Q(vi)
otherwise.

Q(vi)),
gi

∇

∇

∇

0,

≤

−

(cid:26)

k

8:

9:
10:
11:
12:
13:
14: end procedure

end while
return the embedding V

Algorithm 3 Rounding procedure for the Locale algorithm
1: procedure LOCALEROUNDING(Graph G, Partition P , Embedding E )
2:
3:
4:
5:
6: end procedure

Initialize V with input E.
Run line 3–12 of Algorithm 2 with cardinality constraint k = 1.
Let the index of the 1-sparse embedding above be the new partition P ′.
return P ′

Algorithm 4 Reﬁne and Aggregate procedure from the Leiden algorithm
1: procedure LEIDENREFINEAGGREGATE(Graph G, Partition P )
Reﬁne P ′
2:
←
Forms a hypergraph G ′ by merging nodes inside the same partitions in P ′ and simplify P ′.
3:
P
done
equals
4:
|
|
return G ′, P ′, done
5:
6: end procedure

LocaleRounding(G, P ) by restricting the local move within its partition.5

← |

G ′

|

.

5This is the reﬁnement step implemented in the package python-leiden.

16

