0
2
0
2

t
c
O
2
1

]

G
L
.
s
c
[

2
v
6
3
2
4
0
.
9
0
9
1
:
v
i
X
r
a

Online Planning with Lookahead Policies

Yonathan Efroni ∗ † ‡

Mohammad Ghavamzadeh §

Shie Mannor ‡ ¶

Abstract

Real Time Dynamic Programming (RTDP) is an online algorithm based on
Dynamic Programming (DP) that acts by 1-step greedy planning. Unlike DP,
RTDP does not require access to the entire state space, i.e., it explicitly handles the
exploration. This fact makes RTDP particularly appealing when the state space is
large and it is not possible to update all states simultaneously. In this we devise
a multi-step greedy RTDP algorithm, which we call h-RTDP, that replaces the
1-step greedy policy with a h-step lookahead policy. We analyze h-RTDP in its
exact form and establish that increasing the lookahead horizon, h, results in an
improved sample complexity, with the cost of additional computations. This is
the ﬁrst work that proves improved sample complexity as a result of increasing
the lookahead horizon in online planning. We then analyze the performance of
h-RTDP in three approximate settings: approximate model, approximate value
updates, and approximate state representation. For these cases, we prove that the
asymptotic performance of h-RTDP remains the same as that of a corresponding
approximate DP algorithm, the best one can hope for without further assumptions
on the approximation errors.

1

Introduction

Dynamic Programming (DP) algorithms return an optimal policy, given a model of the environment.
Their convergence in the presence of lookahead policies [4, 13] and their performance in different
approximate settings [4, 25, 27, 17, 1, 14] have been well-studied. Standard DP algorithms require
simultaneous access to the entire state space at run time, and as such, cannot be used in practice when
the number of states is too large. Real Time Dynamic Programming (RTDP) [3, 29] is a DP-based
algorithm that mitigates the need to access all states simultaneously. Similarly to DP, RTDP updates
are based on the Bellman operator, calculated by accessing the model of the environment. However,
unlike DP, RTDP learns how to act by interacting with the environment. In each episode, RTDP
interacts with the environment, acts according to the greedy action w.r.t. the Bellman operator, and
samples a trajectory. RTDP is, therefore, an online planning algorithm.

Despite the popularity and simplicity of RTDP and its extensions [5, 6, 24, 8, 29, 22], precise
characterization of its convergence was only recently established for ﬁnite-horizon MDPs [15].
While lookahead policies in RTDP are expected to improve the convergence in some of these
scenarios, as they do for DP [4, 13], to the best of our knowledge, these questions have not been
addressed in previous literature. Moreover, previous research haven’t addressed the questions of how
lookahead policies should be used in RTDP, nor studied RTDP’s sensitivity to possible approximation
errors. Such errors can arise due to a misspeciﬁed model, or exist in value function updates, when
e.g., function approximation is used.

∗Part of this work was done during an internship in Facebook AI Research
†Microsoft Research, New York, NY
‡Technion, Israel
§Google Research
¶Nvidia Research

34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

 
 
 
 
 
 
In this paper, we initiate a comprehensive study of lookahead-policy based RTDP with approximation
errors in online planning. We start by addressing the computational complexity of calculating
lookahead policies and study its advantages in approximate settings. Lookahead policies can be
computed naively by exhaustive search in O(Ah) for deterministic environments or O(ASh) for
stochastic environments. Since such an approach is infeasible, we offer in Section 3 an alternative
approach for obtaining a lookahead policy with a computational cost that depends linearly on a
natural measure: the total number of states reachable from a state in h time steps. The suggested
approach is applicable both in deterministic and stochastic environments.

In Section 5, we introduce and analyze h-RTDP, a RTDP-based algorithm that replaces the 1-step
greedy used in RTDP by a h-step lookahead policy. The analysis of h-RTDP reveals that the sample
complexity is improved by increasing the lookahead horizon h. To the best of our knowledge, this is
the ﬁrst theoretical result that relates sample complexity to the lookahead horizon in online planning
setting. In Section 6, we analyze h-RTDP in the presence of three types of approximation: when
(i) an inexact model is used, instead of the true one, (ii) the value updates contain error, and ﬁnally
(iii) approximate state abstraction is used. Interestingly, for approximate state abstraction, h-RTDP
convergence and computational complexity depends on the size of the abstract state space.

In a broader context, this work shows that RTDP-like algorithms could be a good alternative to Monte
Carlo tree search (MCTS) [7] algorithms, such as upper conﬁdence trees (UCT) [21], an issue that
was empirically investigated in [22]. We establish strong convergence guarantees for extensions of
h-RTDP: under no assumption other than initial optimistic value, RTDP-like algorithms combined
with lookahead policies converge in polynomial time to an optimal policy (see Table 1), and their
approximations inherit the asymptotic performance of approximate DP (ADP). Unlike RTDP, MCTS
acts by using a (cid:112)log N/N bonus term instead of optimistic initialization. However, in general, its
convergence can be quite poor, even worse than uniformly random sampling [9, 26].

2 Preliminaries

Finite Horizon MDPs. A ﬁnite-horizon MDP [4] with time-independent dynamics6 is a tuple
M = (S, A, r, p, H), where S and A are the state and action spaces with cardinalities S and A,
respectively, r(s, a) ∈ [0, 1] is the immediate reward of taking action a at state s, and p(s(cid:48)|s, a) is the
probability of transitioning to state s(cid:48) upon taking action a at state s. The initial state in each episode
is arbitrarily chosen and H ∈ N is the MDP’s horizon. For any N ∈ N, denote [N ] := {1, . . . , N }.

A deterministic policy π : S × [H] → A is a mapping from states and time step indices
to actions. We denote by at
:= πt(s) the action taken at time t at state s according to a
policy π. The quality of a policy π from a state s at time t is measured by its value function,
t(cid:48)=t r(st(cid:48), πt(cid:48)(st(cid:48))) | st = s(cid:3), where the expectation is over all the randomness
i.e., V π
in the environment. An optimal policy maximizes this value for all states s ∈ S and time steps
t ∈ [H], i.e., V ∗

t (s), and satisﬁes the optimal Bellman equation,

t (s) := E(cid:2) (cid:80)H

t (s) := maxπ V π

t (s) = T V ∗
V ∗

= max

a

t+1(s) := max
E(cid:2)r(s1, a) + V ∗

(cid:0)r(s, a) + p(·|s, a)V ∗
t+1(s2) | s1 = s(cid:3).

a

t+1

(cid:1)

(1)

By repeatedly applying the optimal Bellman operator T , for any h ∈ [H], we have

t (s) = T hV ∗
V ∗

t+h(s) = max

a

(cid:0)r(s, a) + p(·|s, a)T h−1V ∗

t+h

(cid:1)

= max

πt,...,πt+h−1

(cid:104) h
(cid:88)

E

t(cid:48)=1

r(st(cid:48), πt+t(cid:48)−1(st(cid:48))) + V ∗

(cid:105)
t+h(sh+1) | s1 = s

.

(2)

We refer to T h as the h-step optimal Bellman operator. Similar Bellman recursion is deﬁned for the
value of a given policy, π, i.e., V π, as V π
π V π
V π
t+h,
where T h
π is the h-step Bellman operator of policy π.
h-Lookahead Policy. An h-lookahead policy w.r.t. a value function V ∈ RS returns the optimal ﬁrst
action in an h-horizon MDP. For a state s ∈ S, it returns

t+h(s) := r(s, πt(s)) + p(·|s, πt(s))T h−1

t (s) = T h

π

6The results can also be applied to time-dependent MDPs, however, the notations will be more involved.

2

ah(s) ∈ arg max

a

(cid:0)r(s, a) + p(·|s, a)T h−1V (cid:1)

= arg max
π1(s)

max
π2,...,πh

(cid:104) h
(cid:88)

E

t=1

(cid:105)
r(st, πt(st)) + V (sh+1)|s1 = s

.

(3)

We can see V represent our ‘prior-knowledge’ of the problem. For example, it is possible to show [4]
that if V is close to V ∗, then the value of a h-lookahead policy w.r.t. V is close to V ∗.

For a state s ∈ S and a number of time steps h ∈ [H], we deﬁne the set of reachable states
from s in h steps as Sh(s) = {s(cid:48) | ∃π : pπ(sh+1 = s(cid:48) | s1 = s, π) > 0}, and denote by Sh(s) its
cardinality. We deﬁne the set of reachable states from s in up to h steps as S T ot
t=1St(s), its
cardinality as ST ot
t=1 St(s), and the maximum of this quantity over the entire state space
as ST ot
the maximum number of accessible
states in 1-step (neighbors) from any state.

h (s). Finally, we denote by N := ST ot

h = maxs ST ot

h (s) := (cid:80)h

h (s) := ∪h

1

Regret and Uniform-PAC. We consider an agent that repeatedly interacts with an MDP in a
t and ak
sequence of episodes [K]. We denote by sk
t , the state and action taken at the time step
t of the k’th episode. We denote by Fk−1, the ﬁltration that includes all the events (states,
actions, and rewards) until the end of the (k − 1)’th episode, as well as the initial state of the
k’th episode. Throughout the paper, we denote by πk the policy that is executed during the
k’th episode and assume it is Fk−1 measurable. The performance of an agent is measured by
1)(cid:1), as well as by the Uniform-PAC
its regret, deﬁned as Regret(K) := (cid:80)K
1 (sk
criterion [10], which we generalize to deal with approximate convergence. Let (cid:15), δ > 0 and
1) ≥ (cid:15)(cid:9) be the number of episodes in which the algorithm outputs
N(cid:15) = (cid:80)∞
a policy whose value is (cid:15)-inferior to the optimal value. An algorithm is called Uniform-PAC, if
Pr(∃(cid:15) > 0 : N(cid:15) ≥ F (S, 1/(cid:15), log 1/δ, H)) ≤ δ, where F (·) depends polynomially (at most) on its
parameters. Note that Uniform-PAC implies ((cid:15), δ)-PAC, and thus, it is a stronger property. As we
analyze algorithms with inherent errors in this paper, we use a more general notion of ∆-Uniform-
1) ≥ ∆+(cid:15)(cid:9), where ∆ > 0.
PAC by deﬁning the random variable N ∆
k=1
Finally, we use ˜O(x) to represent x up to constants and poly-logarithmic factors in δ, and O(x) to
represent x up to constants.

(cid:15) =(cid:80)∞

1) − V πk

1) − V πk

1)−V πk

1(cid:8)V ∗

1(cid:8)V ∗

1 (sk

1 (sk

1 (sk

1 (sk

1 (sk

(cid:0)V ∗

k=1

k=1

3 Computing h-Lookahead Policies

Computing an action returned by a h-lookahead policy at a certain state is a main component in
the RTDP-based algorithms we analyze in Sections 5 and 6. A ‘naive’ procedure that returns such
action is the exhaustive search. Its computational cost is O(Ah) and O(ASh) for deterministic and
stochastic systems, respectively. Such an approach is impractical, even for moderate values of h or S.

Instead of the naive approach, we formulate a Forward-Backward DP (FB-DP) algorithm, whose
pseudo-code is given in Appendix 10. The FB-DP returns an action of an h-lookahead policy from a
given state s. Importantly, in both deterministic and stochastic systems, the computation cost of FB-
DP depends linearly on the total number of reachable states from s in up to h steps, i.e., ST ot
h (s). In
the worst case, we may have Sh(s) = O(min(cid:0)Ah, S(cid:1)). However, when ST ot
h (s) is small, signiﬁcant
improvement is achieved by avoiding unnecessary repeated computations.

FB-DP has two subroutines. It ﬁrst constructs the set of reachable states from state s in up to h steps,
{St(s)}h
t=1, in the ‘forward-pass’. Given this set, in the second ‘backward-pass’ it simply applies
backward induction (Eq. 3) and returns an action suggested by the h-lookahead policy, ah(s). Note
that at each stage t ∈ [h] of the backward induction (applied on the set {St(s)}h
t=1) there are St(s)
states on which the Bellman operator is applied. Since applying the Bellman operator costs O(N A)
h (s)(cid:1).
computations, the computational cost of the ‘backward-pass’ is O(cid:0)N AST ot
In Appendix 10, we describe a DP-based approach to efﬁciently implement ‘forward-pass’ and analyze
its complexity. Speciﬁcally, we show the computational cost of the ‘forward-pass’ is equivalent
to that of the ‘backward-pass’ (see Propsition 8). Meaning, the computational cost of FB-DP is
O(cid:0)N AST ot

h (s)) - same order as the cost of backward induction given the set S T ot

h (s).

3

4 Real-Time Dynamic Programming

Real-time dynamic programming (RTDP) [3] is a well-known online planning algorithm that assumes
access to a transition model and a reward function. Unlike DP algorithms (policy, value iteration,
or asynchronous value iteration) [4] that solve an MDP using ofﬂine calculations and sweeps over
the entire states (possibly in random order), RTDP solves it in real-time, using samples from
the environment (either simulated or real) and DP-style Bellman updates from the current state.
Furthermore, unlike DP algorithms, RTDP needs to tradeoff exploration-exploitaion, since it interacts
with the environment via sampling trajectories. This makes RTDP a good candidate for problems in
which having access to the entire state space is not possible, but interaction is.

t+1(s) = H − t ≥ V ∗

Algorithm 1 contains the pseudo-code of RTDP in ﬁnite-horizon MDPs. The value is initialized
optimistically, ¯V 0
t+1(s). At each time step t ∈ [H] and episode k ∈ [K], the
agent updates the value of the current state sk
t by the optimal Bellman operator. It then acts greedily
w.r.t. the current value at the next time step ¯V k−1
t+1 . Finally, the next state, sk
t+1, is sampled either from
the model or the real-world. When the model is exact, there is no difference in sampling from the
model and real-world, but these are different in case the model is inexact as in Section 6.1.

The following high probability bound on the regret of a Decreasing Bounded Process (DBP), proved
in [15], plays a key role in our analysis of exact and approximate RTDP with lookahead policies
in Sections 5 and 6. An adapted process {Xk, Fk}k≥0 is a DBP, if for all k ≥ 0, (i) Xk ≤ Xk−1
almost surely (a.s.), (ii) Xk ≥ C2, and (iii) X0 = C1 ≥ C2. Interestingly, contrary to the standard
regret bounds (e.g., in bandits), this bound does not depend on the number of rounds K.
Theorem 1 (Regret Bound of a DBP [15]). Let {Xk, Fk}k≥0 be a DBP and RK = (cid:80)K
E[Xk | Fk−1] be its K-round regret. Then,

k=1 Xk−1 −

Pr{∃K > 0 : RK ≥ 9(C1 − C2) ln(3/δ)} ≤ δ.

5 RTDP with Lookahead Policies

In this section, we devise and analyze a lookahead-based RTDP algorithm, called h-RTDP, whose
pseudo-code is shown in Algorithm 2. Without loss of generality, we assume that H/h ∈ N. We
divide the horizon H into H/h intervals, each of length h time steps. h-RTDP stores HS/h values in
the memory, i.e., the values at time steps H = {1, h + 1, . . . , H + 1}.7 For each time step t ∈ [H], we
denote by hc ∈ H, the next time step for which a value is stored in the memory, and by tc = hc − t,
the number of time steps until there (see Figure 1). At each time step t of an episode k ∈ [K], given
the current state sk

t returned by the tc-lookahead policy w.r.t. ¯V k−1

,

hc

t , h-RTDP selects an action ak
(cid:104) tc(cid:88)
E

t ) ∈ arg max
π1(sk
t )

max
π2,...,πtc

t(cid:48)=1

t = atc (sk
ak

r(st(cid:48), πt(cid:48)(st(cid:48))) + ¯V k−1

hc

(stc+1) | s1 = sk
t

(cid:105)
.

(4)

Thus, h-RTDP uses a varying lookahead horizon
tc that depends on how far the current time step
is to the next one for which a value is stored.
Throughout the paper, with an abuse of notation, we
refer to this policy as a h-lookahead policy. Finally,
it can be seen that h-RTDP generalizes RTDP as
they are equal for h = 1.

guarantees

for h-RTDP;

We are now ready to establish ﬁnite-sample
performance
see
Appendix 11 for the detailed proofs. We start
with two lemmas from which we derive the main
convergence result of this section.
Lemma 2. For all s ∈ S, n ∈ {0} ∪ [ H
h ],
and k ∈ [K], the value function of h-RTDP is (i)
Optimistic: V ∗
nh+1(s), and (ii) Non-
Increasing: ¯V k
nh+1(s).

nh+1(s) ≤ ¯V k
nh+1(s) ≤ ¯V k−1

t

t = 1

h+1 = 4

H +1 = 7

Figure 1: Varying lookahead horizon of a h-greedy
policy in h-RTDP (see Eq. 4) with h = 3 and H = 6.
The blue arrows show the lookahead horizon from
a speciﬁc time step t, and the red bars are the
time steps for which a value is stored in memory,
i.e., H = {1 , h + 1 = 4 , 2h + 1 = H + 1 = 7}.

7In fact, h-RTDP does not need to store V1 and VH+1, they are only used in the analysis.

4

Algorithm 1 Real-Time DP (RTDP)
init: ∀s ∈ S, ∀t ∈ {0} ∪ [H],
¯V 0
t+1(s) = H − t
for k ∈ [K] do
Initialize sk
for t ∈ [H] do
t ) = T ¯V k−1
¯V k
t (sk
t+1 (sk
t )
t ∈ arg maxa r(sk
t , a)+p(·|sk
ak
Act by ak

1 arbitrarily

t , observe sk

t , a) ¯V k−1
t+1
t , ak
t )

t+1 ∼ p(· | sk

end for

end for

Algorithm 2 RTDP with Lookahead (h-RTDP)

init:: ∀s ∈ S, n ∈ {0} ∪ [ H

h ],

¯V 0
nh+1(s) = H − nh
for k ∈ [K] do
Initialize sk
for t ∈ [H] do

1 arbitrarily

if (t − 1) mod h = 0 then

t ) = T h ¯V k−1

hc

(sk
t )

hc = t + h
¯V k
t (sk
end if
ak
t ∈
arg maxa r(sk
Act by ak

t , a) + p(·|sk

t , a)T hc−t−1 ¯V k−1

hc

t , observe sk

t+1 ∼ p(· | sk

t , ak
t )

end for

end for

Lemma 3 (Optimality Gap and Expected Decrease). The expected cumulative value update at the k’th
1) = (cid:80) H
episode of h-RTDP satisﬁes ¯V k
nh+1(s) | Fk−1].

nh+1(s) − E[ ¯V k
¯V k−1

1) − V πk

1 (sk

h −1
n=1

1 (sk

(cid:80)

s∈S

1 (sk
1 (sk

Properties (i) and (ii) in Lemma 2 show that { ¯V k
nh+1(s)}k≥0 is a DBP, for any s and n. Lemma 3
1) (LHS) to the expected decrease in ¯V k at the k’th episode (RHS). When the
relates ¯V k
1) − V πk
1 (sk
LHS is small, then ¯V k
1), due to the optimism of ¯V k
1) (cid:39) V ∗
1 , and h-RTDP is about to converge
to the optimal value. This is why we refer to the LHS as the optimality gap. Using these two lemmas
and the regret bound of a DBP (Theorem 1), we prove a ﬁnite-sample convergence result for h-RTDP
(see Appendix 11 for the full proof).
Theorem 4 (Performance of h-RTDP). Let (cid:15), δ > 0. The following holds for h-RTDP:

1 (sk

1. With probability 1 − δ, for all K > 0, Regret(K) ≤ 9SH(H−h)

h

ln(3/δ).

2. Pr

(cid:110)
∃(cid:15) > 0 : N(cid:15) ≥ 9SH(H−h) ln(3/δ)

h(cid:15)

(cid:111)

≤ δ.

Proof Sketch. Applying Lemmas 2 and 3, we may write
H
h −1
(cid:88)

K
(cid:88)

K
(cid:88)

¯V k
1 (sk

1) − V πk

1 (sk

1) =

Regret(K) ≤

(cid:88)

nh+1(s) − E[ ¯V k
¯V k−1

nh+1(s) | Fk−1]

k=1

n=1

s

Xk−1 − E[Xk | Fk−1].

(5)

k=1

K
(cid:88)

k=1

=

(cid:80)

(cid:80)

(cid:80)
s

s V 0

s V ∗

h −1
n=1

h −1
n=1

h −1
n=1

nh+1(s) ≤ SH(H −h)/h, and applying Theorem 1.

¯V k−1
nh+1(s) and use linearity of expectation. By Lemma 2,
nh+1(s) ≥ 0. We conclude the

Where we deﬁne Xk := (cid:80) H
{Xk}k≥0 is decreasing and bounded from below by (cid:80) H
proof by observing that X0 ≤ (cid:80) H
Remark 1 (RTDP and Good Value Initialization). A closer look into the proof of Theorem 4 shows
we can easily obtain a stronger result which depends on the initial value V 0. The regret can be
bounded by Regret(K) ≤ ˜O
, which formalizes the intuition the
algorithm improves as the initial value V 0 better estimates V ∗. For clarity purposes we provide the
worse-case bound.
Remark 2 (Computational Complexity of h-RTDP). Using FB-DP (Section 3) as a solver of a
h-lookahead policy, the per-episode computation cost of h-RTDP amounts to applying FB-DP for H
time steps, i.e., it is bounded by O(HN AST ot
h ). Since ST ot
– the total number of reachable states in
up to h time steps – is an increasing function of h, the computation cost of h-RTDP increases with h,
as expected. When ST ot
is signiﬁcantly smaller than S, the per-episode computational complexity of

nh+1(s)(cid:1)(cid:17)

nh+1(s) − V ∗

(cid:16)(cid:80) H

h −1
n=1

(cid:0)V 0

h

h

5

h-RTDP is S independent. As discussed in Section 3, using FB-DP, in place of exhaustive search,
can signiﬁcantly improve the computational cost of h-RTDP.
Remark 3 (Improved Sample Complexity of h-RTDP). Theorem 4 shows that h-RTDP improves the
sample complexity of RTDP by a factor 1/h. This is consistent with the intuition that larger horizon
of the applied lookahead policy results in faster convergence (less samples). Thus, if RTDP is used in
a real-time manner, one way to boost its performance is to combine with lookahead policies.
Remark 4 (Sparse Sampling Approaches). In this work, we assume h-RTDP has access to a h-
lookahead policy (3) solver, such as FB-DP presented in Section 3. We leave studying the sparse
sampling approach [19, 28] for approximately solving h-lookahead policy for future work.

6 Approximate RTDP with Lookahead Policies

In this section, we consider three approximate versions of h-RTDP in which the update deviates from
its exact form described in Section 5. We consider the cases in which there are errors in the 1) model,
2) value updates, and when we use 3) approximate state abstraction. We prove ﬁnite-sample bounds
on the performance of h-RTDP in the presence of these approximations. Furthermore, in Section 6.3,
given access to an approximate state abstraction, we show that the convergence of h-RTDP depends
on the cardinality of the abstract state space – which can be much smaller than the original one. The
proofs of this section generalize that of Theorem 4, while following the same ‘recipe’. This shows
the generality of the proof technique, as it works for both exact and approximate settings.

6.1 h-RTDP with Approximate Model (h-RTDP-AM)

In this section, we analyze a more practical scenario in which the transition model used by h-RTDP
to act and update the values is not exact. We assume it is close to the true model in the total variation
(T V ) norm, ∀(s, a) ∈ S × A, ||p(·|s, a) − ˆp(·|s, a)||1 ≤ (cid:15)P , where ˆp denotes the approximate model.
Throughout this section and the relevant appendix (Appendix 12), we denote by ˆT and ˆV ∗ the optimal
Bellman operator and optimal value of the approximate model ˆp, respectively. Note that ˆT and ˆV ∗
satisfy (1) and (2) with p replaced by ˆp. h-RTDP-AM is exactly the same as h-RTDP (Algorithm 2)
with the model p and optimal Bellman operator T replaced by their approximations ˆp and ˆT . We
report the pseudocode of h-RTDP-AM in Appendix 12.

Although we are given an approximate model, ˆp, we are still interested in the performance of
(approximate) h-RTDP on the true MDP, p, and relative to its optimal value, V ∗. If we solve the
approximate model and act by its optimal policy, the Simulation Lemma [20, 30] suggests that the
regret is bounded by O(H 2(cid:15)P K). For h-RTDP-AM, the situation is more involved, as its updates
are based on the approximate model and the samples are gathered by interacting with the true MDP.
Nevertheless, by properly adjusting the techniques from Section 5, we derive performance bounds
for h-RTDP-AM. These bounds reveal that the asymptotic regret increases by at most O(H 2(cid:15)P K),
similarly to the regret of the optimal policy of the approximate model. Interestingly, the proof
technique follows that of the exact case in Theorem 4. We generalize Lemmas 2 and 3 from Section 5
to the case that the update rule uses an inexact model (see Lemmas 9 and 10 in Appendix 12). This
allows us to establish the following performance bound for h-RTDP-AM (proof in Appendix 12).
Theorem 5 (Performance of h-RTDP-AM). Let (cid:15), δ > 0. The following holds for h-RTDP-AM:

1. With probability 1 − δ, for all K > 0, Regret(K) ≤ 9SH(H−h)

h

ln(3/δ) + H(H − 1)(cid:15)P K.

2. Let ∆P = H(H − 1)(cid:15)P . Then, Pr

(cid:110)

∃(cid:15) > 0 : N ∆P

(cid:15) ≥ 9SH(H−h) ln(3/δ)

h(cid:15)

(cid:111)

≤ δ.

These bounds show the approximate convergence resulted from the approximate model. However,
the asymptotic performance gaps – both in terms of the regret and Uniform PAC – of h-RTDP-
AM approach those experienced by an optimal policy of the approximate model. Interestingly,
although h-RTDP-AM updates using the approximate model, while interacting with the true MDP, its
convergence rate (to the asymptotic performance) is similar to that of h-RTDP (Theorem 4).

6.2 h-RTDP with Approximate Value Updates (h-RTDP-AV)

Another important question in the analysis of approximate DP algorithms is their performance under
approximate value updates, motivated by the need to use function approximation. This is often
modeled by an extra noise |(cid:15)V (s)| ≤ (cid:15)V added to the update rule [4]. Following this approach, we

6

study such perturbation in h-RTDP. Speciﬁcally, in h-RTDP-AV the value update rule is modiﬁed
such that it contains an error term (see Algorithm 2),

hc

t ).

(sk

¯V k
t (sk

t ) = (cid:15)V (sk

t ) + T h ¯V k−1
For (cid:15)V (sk
t ) = 0, the exact h is recovered. The pseudocode of h-RTDP-AV is supplied in Appendix 13.
Similar to the previous section, we follow the same proof technique as for Theorem 4 to establish the
following performance bound for h-RTDP-AV (proof in Appendix 13).
Theorem 6 (Performance of h-RTDP-AV). Let (cid:15), δ > 0. The following holds for h-RTDP-AV:
δ ) + 2H

1. With probability 1 − δ, for all K > 0, Regret(K) ≤ 9SH(H−h)
(cid:110)
(cid:15) ≥ 9SH(H−h)(1+ ∆V

2. Let ∆V = 2H(cid:15)V . Then, Pr

h (cid:15)V ) ln( 3
(cid:111)

∃(cid:15) > 0 : N

h (cid:15)V K.

2h ) ln( 3
δ )

(1 + H

≤ δ.

∆V
h

h

h(cid:15)

As in Section 6.1, the results of Theorem 6 exhibit an asymptotic linear regret O(H(cid:15)V K/h). As
proven in Proposition 20 in Appendix 16, such performance gap exists in ADP with approximate value
updates. Furthermore, the convergence rate in S to the asymptotic performance of h-RTDP-AV is
similar to that of its exact version (Theorem 4). Unlike in h-RTDP-AM, the asymptotic performance
of h-RTDP-AV improves with h. This quantiﬁes a clear beneﬁt of using lookahead policies in online
planning when the value function is approximate.

6.3 h-RTDP with Approximate State Abstraction (h-RTDP-AA)

h

We conclude the analysis of approximate h-RTDP with exploring the advantages of combining it
with approximate state abstraction [1]. The central result of this section establishes that given an
approximate state abstraction, h-RTDP converges with sample, computation, and space complexity
independent of the size of the state space S, as long as ST ot
is smaller than S (i.e., when performing
h-lookahead is S independent, Remark 2). This is in contrast to the computational complexity of ADP
in this setting, which is still O(HSA) (see Appendix 16.3 for further discussion). State abstraction
has been widely investigated in approximate planning [12, 11, 16, 1], as a means to deal with large
state space problems. Among existing approximate abstraction settings, we focus on the following
one. For any n ∈ {0} ∪ [ H
h − 1], we deﬁne φnh+1 : S → Sφ to be a mapping from the state space S
to reduced space Sφ, Sφ = |Sφ| (cid:28) S. We make the following assumption:
Assumption 1 (Approximate Abstraction, [23], deﬁnition 3.3). For any s, s(cid:48) ∈ S and n ∈ {0} ∪
[ H
h − 1] for which φnh+1(s) = φnh+1(s(cid:48)), we have |V ∗
Let us denote by { ¯V k
n=0 the values stored in memory by h-RTDP-AA at the k’th episode.
Unlike previous sections, the value function per time step contains Sφ entries, ¯V k
φ,1+nh ∈ RSφ. Note
that if (cid:15)A = 0, then optimal value function can be represented in the reduced state space Sφ. However,
if (cid:15)A is positive, exact representation of V ∗ is not possible. Nevertheless, the asymptotic performance
of h-RTDP-AA will be ‘close’, up to error of (cid:15)A, to the optimal policy.

nh+1(s(cid:48))| ≤ (cid:15)A.

nh+1(s) − V ∗

φ,nh+1}H/h

Furthermore, the deﬁnition of the multi-step Bellman operator (2) and h-greedy policy (3) should be
revised, and with some abuse of notation, deﬁned as

ak
t ∈ arg max
π0(sk
t )

max
π1,...,πtc−1

T h
φ

¯V k−1
φ,hc

(sk

t ) := max

π0,...,πh−1

(cid:34)tc−1
(cid:88)

E

rt(cid:48) + ¯V k−1
φ,hc

(φhc (stc)) | s0 = sk
t

,

(cid:35)

t(cid:48)=0
(cid:34)h−1
(cid:88)

E

t(cid:48)=0

rt(cid:48) + ¯V k−1

φ,t+h(φt+h(sh)) | s0 = sk

t

(6)

(7)

(cid:35)

.

Eq. (6) and (7) indicate that similar to (3), the h-lookahead policy uses the given model to plan for h
time steps ahead. Differently from (3), the value after h time steps is the one deﬁned in the reduced
state space Sφ. Note that the deﬁnition of the h-greedy policy for h-RTDP-AA in (6) is equivalent to
the one used in Algorithm 8, obtained by similar recursion as for the optimal Bellman operator (2).

h-RTDP-AA modiﬁes both the value update and the calculation of the h-lookahead policy (the value
update and action choice in algorithm 2). The h-lookahead policy is replaced by h-lookahead deﬁned
in (6). The value update is substituted by (7), i.e, ¯V k
t ). The full pseudocode
of h-RTDP-AA is supplied in Appendix 14. By similar technique, as in the proof of Theorem 4, we
establish the following performance guarantees to h-RTDP-AA (proof in Appendix 14).

t )) = T h
φ

φ,t(φt(sk

¯V k−1
φ,hc

(sk

7

Setting
Exact (5)
App. Model (6.1)
App. Value (6.2)
App. Abstraction (6.3)

h-RTDP Regret (This work)
˜O(cid:0)SH(H −h)/h(cid:1)
˜O(cid:0)SH(H −h)/h+∆P K(cid:1)

˜O(cid:0)SH(H −h)g(cid:15)
H/h/h+∆V K/h(cid:1)
˜O(cid:0)SφH(H −h)/h + ∆AK/h(cid:1)

ADP Regret [4]
0
∆P K
∆V K/h
∆AK/h

UCT
Ω(exp(exp(H))) [9]
N.A
N.A
N.A

Table 1: The lookhead horizon is h and the horizon of the MDP is H. We denote g(cid:15)
H/h = (1 + H(cid:15)V /h),
∆P = H(H − 1)(cid:15)P , ∆V = 2H(cid:15)V , and ∆A = H(cid:15)A. The table summarizes the regret bounds of the h-
RTDP settings studied in this work and compares them to those of their corresponding ADP approaches. The
performance of ADP is based on standard analysis, supplied in Propositions 19, 20, 21 in Appendix 16.

Theorem 7 (Performance of h-RTDP-AA). Let (cid:15), δ > 0. The following holds for h-RTDP-AA:

1. With probability 1 − δ, for all K > 0, Regret(K) ≤ 9SφH(H−h)

h

ln(3/δ) + H(cid:15)A

h K.

2. Let ∆A = H(cid:15)A. Then, Pr

∃(cid:15) > 0 : N

(cid:110)

∆A
(cid:15) ≥ 9SφH(H−h) ln(3/δ)
h

(cid:111)

≤ δ.

h(cid:15)

Theorem 7 establishes S-independent performance bounds that depend on the size of the reduced state
space Sφ. The asymptotic regret and Uniform PAC guarantees are approximate, as the state abstraction
is approximate. Furthermore, they are improving with the quality of approximation (cid:15)A, i.e., their
asymptotic gap is O(H(cid:15)A/h) relative to the optimal policy. Moreover, the asymptotic performance
of h-RTDP-AA improves as h is increased. Importantly, since the computation complexity of
each episode of h-RTDP is independent of S (Section 3), the computation required to reach the
approximate solution in h-RTDP-AA is also S-independent. This is in contrast to the computational
cost of DP that depends on S and is O(SHA) (see Appendix 16.3 for further discussion).

7 Discussion and Conclusions

RTDP vs. DP. The results of Sections 5 and 6 established ﬁnite-time convergence guarantees for
the exact h-RTDP and its three approximations. In the approximate settings, as expected, the regret
has a linear term of the form ∆K, where ∆ is linear in the approximation errors (cid:15)P , δ, and (cid:15)A, and
thus, the performance is continuous in these parameters, as we would desire. We refer to ∆K as the
asymptotic regret, since it dominates the regret as K → ∞.

nh+1}H/h

A natural measure to evaluate the quality of h-RTDP in the approximate settings is comparing its regret
to that of its corresponding approximate DP (ADP). Table 1 summarizes the regrets of the approximate
h-RTDPs studied in this paper and their corresponding ADPs. ADP calculates approximate values
{V ∗
n=0 by backward induction. Based on these values, the same h-lookahead policy by which
h-RTDP acts is evaluated. In the analysis of ADP, we use standard techniques developed for the
discounted case in [4]. From Table 1, we reach the following conclusion: the asymptotic performance
(in terms of regret) of approximate h-RTDP is equivalent to that of a corresponding approximate DP
algorithm. Furthermore, it is important to note that the asymptotic error decreases with h for the
approximate value updates and approximate abstraction settings for both RTDP and DP algorithms. In
these settings, the error is caused by approximation in the value function. By increasing the lookahead
horizon h, the algorithm uses less such values and relies more on the model which is assumed to be
correct. Thus, the algorithm becomes less affected by the value function approximation.

In this paper, we formulated h-RTDP, a generalization of RTDP that acts by a
Conclusions.
lookahead policy, instead of by a 1-step greedy policy, as in RTDP. We analyzed the ﬁnite-sample
performance of h-RTDP in its exact form, as well as in three approximate settings. The results
indicate that h-RTDP converges in a very strong sense. Its regret is constant w.r.t. to the number of
episodes, unlike in, e.g., reinforcement learning where a lower bound of ˜O(
SAHT ) exists [2, 18].
Furthermore, the analysis reveals that the sample complexity of h-RTDP improves by increasing the
lookahead horizon h (Remark 3). Moreover, the asymptotic performance of h-RTDP was shown to
be equivalent to that of ADP (Table 1), which under no further assumption on the approximation
error, is the best we can hope for.

√

We believe this work opens interesting research venues, such as studying alternatives to the solution
of the h-greedy policy (see Section 10), studying a Receding-Horizon extension of RTDP, RTDP with

8

function approximation, and formulating a Thompson-Sampling version of RTDP, as the standard
RTDP is an ‘optimistic’ algorithm. As the analysis developed in this work was shown to be quite
generic, we hope that it can assist with answering some of these questions. On the experimental side,
more needs to be understood, especially comparing RTDP with MCTS and studying how RTDP can
be combined with deep neural networks as the value function approximator.

8 Broader Impact

Online planning algorithms, such as A∗ and RTDP, have been extensively studied and applied in AI
for well over two decades. Our work quantiﬁes the beneﬁts of using lookahead-policies in this class
of algorithms. Although lookahead-policies have also been widely used in online planning algorithms,
their theoretical justiﬁcation was lacking. Our study sheds light on the beneﬁts of lookahead-policies.
Moreover, the results we provide in this paper suggest improved ways for applying lookahead-policies
in online planning with beneﬁts when dealing with various types of approximations. This work
opens up the room for practitioners to improve their algorithms and base lookahead policies on solid
theoretical ground.

9 Acknowledgements

We thank the reviewers for their helpful comments and feedback.

References

[1] David Abel, D. Hershkowitz, and Michael Littman. Near optimal behavior via approximate state
abstraction. In Proceedings of the 33rd International Conference on International Conference
on Machine Learning, pages 2915–2923, 2016.

[2] Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for
In Proceedings of the 34th International Conference on Machine

reinforcement learning.
Learning-Volume 70, pages 263–272. JMLR. org, 2017.

[3] Andrew Barto, Steven Bradtke, and Satinder Singh. Learning to act using real-time dynamic

programming. Artiﬁcial intelligence, 72(1-2):81–138, 1995.

[4] D. Bertsekas and J. Tsitsiklis. Neuro-dynamic programming. Athena Scientiﬁc, 1996.

[5] Blai Bonet and Hector Geffner. Planning with incomplete information as heuristic search in
belief space. In Proceedings of the Fifth International Conference on Artiﬁcial Intelligence
Planning Systems, pages 52–61. AAAI Press, 2000.

[6] Blai Bonet and Hector Geffner. Labeled rtdp: Improving the convergence of real-time dynamic

programming. In ICAPS, volume 3, pages 12–21, 2003.

[7] Cameron Browne, Edward Powley, Daniel Whitehouse, Simon Lucas, Peter Cowling, Philipp
Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A
survey of Monte Carlo tree search methods. IEEE Transactions on Computational Intelligence
and AI in games, 4(1):1–43, 2012.

[8] Vadim Bulitko and Greg Lee. Learning in real-time search: A unifying framework. Journal of

Artiﬁcial Intelligence Research, 25:119–157, 2006.

[9] Pierre-Arnaud Coquelin and Rémi Munos. Bandit algorithms for tree search. arXiv preprint

cs/0703062, 2007.

[10] Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying pac and regret: Uniform pac
bounds for episodic reinforcement learning. In Advances in Neural Information Processing
Systems, pages 5713–5723, 2017.

[11] Thomas Dean, Robert Givan, and Sonia Leach. Model reduction techniques for computing
approximately optimal solutions for Markov decision processes. In Proceedings of the 13th
conference on Uncertainty in artiﬁcial intelligence, pages 124–131, 1997.

[12] Richard Dearden and Craig Boutilier. Abstraction and approximate decision-theoretic planning.

Artiﬁcial Intelligence, 89(1-2):219–283, 1997.

9

[13] Y. Efroni, G. Dalal, B. Scherrer, and S. Mannor. How to combine tree-search methods in
reinforcement learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, pages
3494–3501, 2019.

[14] Yonathan Efroni, Gal Dalal, Bruno Scherrer, and Shie Mannor. Multiple-step greedy policies in
approximate and online reinforcement learning. In Advances in Neural Information Processing
Systems, pages 5238–5247, 2018.

[15] Yonathan Efroni, Nadav Merlis, Mohammad Ghavamzadeh, and Shie Mannor. Tight regret
bounds for model-based reinforcement learning with greedy policies. In Advances in Neural
Information Processing Systems, pages 12203–12213, 2019.

[16] Eyal Even-Dar and Yishay Mansour. Approximate equivalence of markov decision processes.

In Learning Theory and Kernel Machines, pages 581–594, 2003.

[17] Matthieu Geist and Olivier Pietquin. Algorithmic survey of parametric value function
approximation. IEEE Transactions on Neural Networks and Learning Systems, 24(6):845–
867, 2013.

[18] Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably
efﬁcient? In Advances in Neural Information Processing Systems, pages 4863–4873, 2018.
[19] Michael Kearns, Yishay Mansour, and Andrew Ng. A sparse sampling algorithm for near-
optimal planning in large Markov decision processes. Machine learning, 49(2-3):193–208,
2002.

[20] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.

Machine learning, 49(2-3):209–232, 2002.

[21] Levente Kocsis and Csaba Szepesvári. Bandit based Monte-Carlo planning. In European

conference on machine learning, pages 282–293, 2006.

[22] Andrey Kolobov, Daniel S Weld, et al. Lrtdp versus uct for online probabilistic planning. In

Twenty-Sixth AAAI Conference on Artiﬁcial Intelligence, 2012.

[23] L. Li, T. Walsh, and M. Littman. Towards a uniﬁed theory of state abstraction for MDPs. In
Proceedings of the 9th International Symposium on Artiﬁcial Intelligence and Mathematics,
pages 531–539, 2006.

[24] Brendan McMahan, Maxim Likhachev, and Geoffrey Gordon. Bounded real-time dynamic
programming: Rtdp with monotone upper bounds and performance guarantees. In Proceedings
of the 22nd international conference on Machine learning, pages 569–576. ACM, 2005.
[25] Rémi Munos. Performance bounds in l_p-norm for approximate value iteration. SIAM journal

on control and optimization, 46(2):541–561, 2007.

[26] Rémi Munos. From bandits to Monte-Carlo tree search: The optimistic principle applied to
optimization and planning. Foundations and Trends® in Machine Learning, 7(1):1–129, 2014.
[27] Bruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon, and Matthieu Geist. Approximate
modiﬁed policy iteration. In Proceedings of the 29th International Conference on Machine
Learning, pages 1207–1214, 2012.

[28] Aaron Sidford, Mengdi Wang, Xian Wu, and Yinyu Ye. Variance reduced value iteration and
faster algorithms for solving Markov decision processes. In Proceedings of the 29th Annual
ACM-SIAM Symposium on Discrete Algorithms, pages 770–787, 2018.

[29] A. Strehl, L. Li, and M. Littman. PAC reinforcement learning bounds for RTDP and rand-RTDP.

In Proceedings of AAAI workshop on learning for search, 2006.

[30] Alexander Strehl, Lihong Li, and Michael Littman. Reinforcement learning in ﬁnite MDPs:

PAC analysis. Journal of Machine Learning Research, 10(Nov):2413–2444, 2009.

10

10 Per-Episode Complexity of h-RTDP

In this section, we deﬁne and analyze the Forward-Backward DP by which an h-greedy policy can be
calculated from a current state sk
t according to (3). Observe that the algorithm is based on a ‘local’
information, i.e., it does not need access to the entire state space, but to a portion of the state space in
the ‘vicinity’ of the current state sk
t . Furthermore, it does not assume prior knowledge on this vicinity.

10.1 Forward-Backward Dynamic Programming Approach

Algorithm 3 h-Forward-Backward DP

Input: s, transition p, reward r, lookahead horizon h, value at the end of lookahead horizon ¯V
{St(cid:48)(s)}h+1
t(cid:48)=1 = Forward-Pass(s,p, h)
action = Backward-Pass({St(cid:48)(s)}h+1
return: action

t(cid:48)=1, r, p, h, ¯V )

Algorithm 4 Forward-Pass

Algorithm 5 Backward-Pass

Input: Starting state s, p, h
Init: S1 = {s}, ∀t(cid:48) ∈ [h]/{1}, St(cid:48)(s) =
{}
for t(cid:48) = 2, 3, . . . , h + 1 do
for st(cid:48)−1 ∈ St(cid:48)−1(s) do

# acquire possible next states from
st(cid:48)−1
for a ∈ A do
St(cid:48)(s)
{s(cid:48) : p(s(cid:48) | s, a) > 0}

St(cid:48)(s)

=

∪

end for

end for

end for
return: {St(cid:48)(s)}h+1
t(cid:48)=1

t(cid:48)=1, r, p, h, ¯V

Input: {St(cid:48)(s)}h+1
# initialize values by arbitrary value C
Init: ∀t(cid:48) ∈ [h − 1], ∀s ∈ St(cid:48)(s), Vt(cid:48)(s) = C
# Assign the value at t(cid:48) = h to the current value,
V .
for s ∈ Sh+1(s) do
Vh+1(s) = ¯V (s)

end for
for t(cid:48) = h, h − 1, . . . , 2 do

for s ∈ St(cid:48)(s) do

Vt(cid:48)(s) = maxa r(s, a) + p(· | s, a)Vt(cid:48)+1

end for

end for
return: arg maxa r(s, a) + p(· | s, a)V2

The Forward-Backword DP (Algorithm 3) approach is built on the following observation: would
we known the accessible state space from s in next h time steps we could use Backward Induction
(i.e., Value Iteration) on a ﬁnite-horizon MDP, with an horizon of h, and calculate the optimal policy
from s. Unfortunately, as we do not assume such a prior knowledge, we have to calculate this set
before applying the backward induction step. Thus, Forward-Backword DP ﬁrst build this set (in the
ﬁrst, ‘Forward’ stage) and later applies standard backward induction (in the ‘Backward’ stage). In
Proposition 8, we establish that calculating the set of accessible states can be done efﬁciently

Let us ﬁrst analyze the computational complexity of Algorithm 3 using the following deﬁnitions. Let
St(cid:48)(s) be the set of reachable states from state s in t(cid:48) times steps, formally,

St(cid:48)(s) = {s(cid:48) | ∃π : pπ(st(cid:48) = s(cid:48) | s0 = s, π) > 0},

where pπ(st(cid:48) = s(cid:48) | s0 = s, π) = E[1{st(cid:48) = s(cid:48)} | s0 = s, π]. The cardinality of this set is denoted
by |St(cid:48)(s)|. let N := maxs |S2(s)| be the maximal number of accessible states in 1-step (maximal
‘nearest neighbors’ from any state). Furthermore, let the total reachable states in h time steps from
state s be ST ot
h (s) is small, as we establish in this section, local
search up to an horizon of h can be done efﬁciently with the Forward-Backward DP, unlike the
exhaustive search approach.

t(cid:48)=1 |St(cid:48)(s)|. When ST ot

h (s) = (cid:80)h

Based on the above deﬁnitions we analyze the computational complexity of Forward-Backward DP
starting from the Forward-Pass stage.

Proposition 8 (Computation Cost of Forward-Pass). The Forward-Pass stage of FB-DP can be
implemented with the computation cost of O(cid:0)N AST ot

h (s)(cid:1).

11

Proof. Calculating the set {s(cid:48) : p(s(cid:48) | s, a) > 0} cost is upper bounded by O(N ) as we need
to enumerate at most all possible O(N ) next-states. We assume that St(cid:48) = St(cid:48)(s) ∪
{s(cid:48) : p(s(cid:48) | s, a) > 0} can be done by O(N ), e.g., when using a hash-table for saving St(cid:48) in memory.
As we need to repeat this operation A times, the complexity for each t(cid:48) ∈ {2, 3, ., , h + 1} is upper
bounded by O(N A|St(cid:48)−1(s)|). Summing over all t(cid:48) we get that the computational complexity of the
Forward pass is upper bounded by

(cid:32)

O

N A

h+1
(cid:88)

(cid:33)

|St(cid:48)−1(s)|

t(cid:48)=2

= O(cid:0)N A(cid:12)

(cid:12)ST ot

h (s)(cid:12)
(cid:12)

(cid:1),

where the second equality holds by deﬁnition of total number of accessible states in h time steps.

The computational complexity of the backward passage is the computational complexity of Backward
Induction, which is the total number of states in which actions can be taken times the number of
actions per state, i.e.,

where the origin of the factor N is due to the need to calculate the sum (cid:80)
each (s, a) pair, and, by deﬁnition, this sum contain at most N elements.

O(AN ST ot

h (s)).,

(8)
s(cid:48) p(s(cid:48) | s, a)V (s(cid:48)) for

Using Proposition 8 and (8) we get that for every t ∈ [H], the computational complexity of calculating
an h-lookahead policy from a state s using the Forward-Backward DP is bounded by,

O((N A + N A)ST ot

h (s)) = O(N AST ot

h ),

where the last relation holds by deﬁnition, ST ot

h = maxs ST ot

h (s).

Finally, the space complexity of Forward-Backward DP is the space required the save in memory
the possible visited states in h time steps (their identity in the Forward-Pass and their values in the
Backward-Pass). By deﬁnition it is at most O(hSh).

12

11 Real Time Dynamic Programming with Lookahead

This section contains the full proofs of all the results of Section 5 in chronological order.
Lemma 2. For all s ∈ S, n ∈ {0}∪[ H
nh+1(s) ≤ ¯V k
V ∗

h ], and k ∈ [K], the value function of h-RTDP is (i) Optimistic:
nh+1(s).

nh+1(s), and (ii) Non-Increasing: ¯V k

nh+1(s) ≤ ¯V k−1

Proof. Both claims are proven using induction.

h ]. By the initialization, ∀s, n, V ∗

(i) Let n ∈ {0} ∪ [ H
for the ﬁrst (k − 1) episodes. Let sk
at which a value update takes place, i.e., t = nh + 1, for some n ∈ {0} ∪ [ H
of Algorithm 2 and (2), we have
t ) = (T h ¯V k−1
¯V k
t (sk

nh+1(s). Assume the claim holds
t be the state of the algorithm at a time step t of the k’th episode
h ]. By the value update

t ) = (T h ¯V k−1

nh+1(s) ≤ V 0

t ) ≥ (T hV ∗

t ) = V ∗

t+h)(sk

t (sk

t+h )(sk

)(sk

t ).

hc

The inequality holds by the induction hypothesis and the monotonicity of T h, a consequence of the
monotonicity of T , the optimal Bellman operator [4]. The last equality holds by the fact that the
recursion is satisﬁed by the optimal value function (2). Thus, the induction step is proven for the ﬁrst
claim.

(ii) Let n ∈ {0} ∪ [ H
the base case, we use the optimistic initialization. Let s1
step of the ﬁrst episode. By the update rule, we have

h ] and t = nh + 1 be a time step in which a value update takes place. To prove
t be the state of the algorithm in the t’th time

¯V 1
t (s1

t ) = (T h ¯V 0

t+h)(s0

t ) = max

a0,...,ah−1

(cid:34)h−1
(cid:88)

E

t(cid:48)=0

r(st(cid:48), at(cid:48)) + ¯V 0

t+h(sh) | s0 = s0
t

(cid:35)

(a)
≤ h + H − (t + h − 1) = H − (t − 1) (b)= ¯V 0
(a) holds since r(s, a) ∈ [0, 1] and by the optimistic initialization.
(b) observe that H − (t − 1) is the value of the optimistic initialization.

t (s1

t ).

t (sk

¯k−1
t+h )(sk

Assume that the claim holds for the ﬁrst (k − 1) episodes. Let sk
t be the state of the algorithm at a time
step t of the k’th episode at which a value update takes place, i.e., t = nh+1, for some n ∈ {0}∪[ H
h ].
By the value update rule of Algorithm 2, we have ¯V k
t ) = (T h ¯V k−1
t ). If
t was previously updated, let ¯k be the last episode in which the update occurred, i.e., ¯V ¯k
sk
t ) =
(T h ¯V
(s).
Using the monotonicity of T h, we may write
¯V k
t (sk
t+h )(sk
t ) ≤ ¯V k−1(sk
t (sk

t ) = (T h ¯V k−1
t ) and the induction step is proved. If sk

t ). In this case, the induction hypothesis implies that ∀s(cid:48), ¯V k−1

Thus, ¯V k
¯V k−1
(sk
t
and the result is proven similarly to the base case.

t was not previously updated, then
t+h(s(cid:48))

t ). By the induction hypothesis, we have that ∀s, t, ¯V

t+h )(sk
t (sk
(s) ≥ ¯V k−1

t (sk
t ) = ¯V 0

t ) = (T h ¯V k−1

t+h (s(cid:48)) ≤ ¯V 0

t ) ≤ (T h ¯V

t ) = ¯V k−1

t ) = ¯V k−1

¯k−1
t+h )(sk

¯k−1
t

)(sk

(sk

(sk

t ).

hc

t

t

t

Lemma 3 (Optimality Gap and Expected Decrease). The expected cumulative value update at the k’th
1) = (cid:80) H
episode of h-RTDP satisﬁes ¯V k
nh+1(s) | Fk−1].

nh+1(s) − E[ ¯V k
¯V k−1

1) − V πk

1 (sk

h −1
n=1

1 (sk

(cid:80)

s∈S

Proof. Let n ∈ {0} ∪ [ H
deﬁnition of the update rule, the following holds for the value update at the visited state sk
t :
¯V k
t (sk

h ] and t = nh + 1 be a time step in which a value update takes place. By the

t ) = (T h ¯V k−1

t+h )(sk
t )

= (T πk(t) · · · T πk(t+h−1) ¯V k−1

t+h )(sk

t ) = E

(cid:34)t+h−1
(cid:88)

r(st(cid:48), at(cid:48)) + ¯V k−1

t+h (st+h) | πk, st = sk

t

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (sk

t+h) | Fk−1, sk
t

(cid:35)

.

(10)

t(cid:48)=t

(cid:34)t+h−1
(cid:88)

(a)= E

t(cid:48)=t

13

(9)
(cid:35)

(a) We prove this passage for each reward element r(st(cid:48), at(cid:48)) in the expectation. The proof for
the expectation of ¯V k−1
t+h (st+h) follows in a similar manner. Since the ﬁrst expectation is w.r.t. the
dynamics of the true model, a consequence of updating by the true model, for any t(cid:48) ≥ t, we may
write
E(cid:2)r(st(cid:48), at(cid:48)) | πk, st = sk

t , πk)r(st(cid:48), πk(st(cid:48), t(cid:48)))

p(st(cid:48) | st = sk

(cid:3) =

(cid:88)

t

(i)=

st(cid:48) ∈S
(cid:88)

sk
t(cid:48) ∈S

p(sk

t(cid:48) | sk

t , Fk−1)r(sk

t(cid:48), πk(sk

t(cid:48), t(cid:48))) = E(cid:2)r(sk

t(cid:48), ak

t(cid:48)) | Fk−1, sk
t

(cid:3),

t , πk) is the probability of starting at state sk

t , following πk, and reaching state st(cid:48) in

t , πk) = p(sk
t the probability for a state sk

t(cid:48) | sk

t , Fk−1), in words, given the policy πk (which is
t(cid:48) with t(cid:48) ≥ t is independent of the rest of the

where p(st(cid:48) | sk
t(cid:48) − t steps.
(i) We use the fact that p(st(cid:48) | sk
Fk−1 measurable) and sk
history.

Now that we proved (a), we take the conditional expectation of (9) w.r.t. Fk−1 and use the tower rule
to obtain

E(cid:2) ¯V k

t (sk

t ) | Fk−1

(cid:34)t+h−1
(cid:88)

(cid:3) = E

t(cid:48)=t

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (sk

t+h) | Fk−1

(cid:35)
.

(11)

Summing (11) for all n ∈ {0} ∪ [ H
¯V k
H+1(s) = 0 for all s, k, we have

h ], and using the linearity of expectation and the fact that

E(cid:2) ¯V k

nh+1(sk

nh+1) | Fk−1

(cid:34) H
(cid:88)

(cid:3) = E

r(sk

t , ak

t ) | Fk−1

H
h −1
(cid:88)

n=0

E(cid:2) ¯V k−1

nh+1(sk

nh+1) | Fk−1

(cid:3)

E(cid:2) ¯V k

nh+1(sk

t ) | Fk−1

t=1

(cid:34) H
(cid:88)

(cid:3) = E

t=1

r(sk

t , ak

t ) | Fk−1

+

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k−1

nh+1(sk

nh+1) | Fk−1

(cid:3)

(cid:35)

+

H
h −1
(cid:88)

n=1
(cid:35)

⇐⇒ ¯V k

1 (sk

1) +

⇐⇒ ¯V k

1 (sk

1) +

H
h −1
(cid:88)

n=1

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k

nh+1(sk

t ) | Fk−1

(cid:3) = V πk (sk

1) +

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k−1

nh+1(sk

nh+1) | Fk−1

(cid:3)

⇐⇒ ¯V k

1 (sk

1) − V πk (sk

1) =

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k−1

nh+1(sk

nh+1) − ¯V k

nh+1(sk

nh+1) | Fk−1

(cid:3).

(12)

The second line holds by the fact that sk

1 is measurable w.r.t. Fk−1 The third line holds since

(cid:34) H
(cid:88)

V πk
1 (sk

1) = E

t=1

r(sk

t , ak

t ) | sk

1, πk

(cid:35)

(cid:34) H
(cid:88)

= E

t=1

r(sk

t , ak

t ) | Fk−1

(cid:35)

.

Applying Lemma 15 from Appendix 15 with gk

t = ¯V k

t for t = nh + 1, we obtain

(12) =

H
h −1
(cid:88)

(cid:88)

n=1

s∈S

nh+1(s) − E[ ¯V k
¯V k−1

nh+1(s) | Fk−1],

which concludes the proof. Note that the update of ¯V k
update rule uses ¯V k−1

t+h , i.e., it is measurable w.r.t. Fk−1, and thus, it is valid to apply Lemma 15.

t occurs only at the visited state sk

t and the

Theorem 4 (Performance of h-RTDP). Let (cid:15), δ > 0. The following holds for h-RTDP:

1. With probability 1 − δ, for all K > 0, Regret(K) ≤ 9SH(H−h)

h

ln(3/δ).

2. Pr

(cid:110)
∃(cid:15) > 0 : N(cid:15) ≥ 9SH(H−h) ln(3/δ)

h(cid:15)

(cid:111)

≤ δ.

14

Proof. We start by proving Claim (1). We know that the following bounds hold on the regret:

Regret(K) :=

K
(cid:88)

k=1

1 (sk
V ∗

1) − V πk

1 (sk
1)

(a)
≤

K
(cid:88)

k=1

¯V k
1 (sk

1) − V πk

1 (sk
1)

K
(cid:88)

H
h −1
(cid:88)

(cid:88)

(b)=

k=1

n=1

s∈S

¯V k−1
nh+1(s) − E[ ¯V k

nh+1(s) | Fk−1].

(13)

(a) is by the optimism of the value function (Lemma 2), and (b) is by Lemma 3.

We would like to show that (13) is the regret of a Decreasing Bounded Process (DBP). We start by
deﬁning

Xk :=

H
h −1
(cid:88)

(cid:88)

n=1

s∈S

¯V k
nh+1(s).

(14)

We now prove that {Xk}k≥0 is a DBP. Note that {Xk}k≥0

1. is decreasing, since ∀s, t, ¯V k

t (s) ≤ ¯V k−1

t

decreasing, and

(s) by Lemma 2, and thus, their sum is also

2. is bounded since ∀s, t ¯V k

t (s) ≥ V ∗

t (s) ≥ 0 by Lemma 2, and thus, the sum is bounded from

below by 0.

We can show that the initial value X0 is also bounded as

X0 =

H
h −1
(cid:88)

(cid:88)

n=1

s∈S

¯V 0
nh+1(s) ≤

H
h −1
(cid:88)

(cid:88)

n=1

s∈S

H =

SH(H − h)
h

.

Using the linearity of expectation and the deﬁnition (14), we observe that (13) can be written as

Regret(K) ≤ (13) =

K
(cid:88)

k=1

Xk−1 − E[Xk | Fk−1],

which is regret of a DBP. Applying the bound on the regret of a DBP, Theorem 1, we conclude the
proof of the ﬁrst claim.

We now prove Claim (2). Here we use a different technique than the one used in [15]. The technique
allows us to prove uniform-PAC bounds for the approximate versions of h-RTDP described in
Section 6. For these approximate versions, the uniform-PAC result is not a corollary of the regret
bound and more careful analysis should be used.

For all (cid:15) > 0, the following relations hold:

1(cid:8)V ∗

1 (sk

1) − V πk

1 (sk

1) ≥ (cid:15)(cid:9)(cid:15)

(a)

≤ 1(cid:8) ¯V k

(b)

≤ 1(cid:8) ¯V k

1 (sk

1) − V πk

1 (sk

1 (sk

1) − V πk

1 (sk

1) ≥ (cid:15)(cid:9)(cid:15)
1) ≥ (cid:15)(cid:9)(cid:0) ¯V k

(c)= 1(cid:8) ¯V k

1 (sk

1) − V πk

1 (sk

1) ≥ (cid:15)(cid:9)





1 (sk
H
h −1
(cid:88)

(cid:88)

1) − V πk

1 (sk

1)(cid:1)

nh+1(s) − E[ ¯V k
¯V k−1

nh+1(s) | Fk−1]





(d)= 1(cid:8) ¯V k

1 (sk

1) − V πk

1 (sk

1) ≥ (cid:15)(cid:9)(Xk−1 − E[Xk | Fk−1]).

(15)

n=1

s∈S

(a) holds since for all t, s, ¯V k
holds by Lemma 3. (d) holds by the deﬁnition of Xk from (14) and the linearity of expectation.

t (s) by Lemma 2. (b) holds by the indicator function. (c)

t (s) ≥ V ∗

15

Let deﬁne N(cid:15)(K) = (cid:80)K
k=1
(cid:15) at the ﬁrst K episodes. For all (cid:15) > 0, we may write

1) − V πk

1 (sk

1 (sk

1(cid:8)V ∗

1) ≥ (cid:15)(cid:9) as the number of times V ∗

1 (sk

1)−V πk

1 (sk

1) ≥

N(cid:15)(K)(cid:15) (a)=

(c)
≤

K
(cid:88)

k=1

K
(cid:88)

k=1

1(cid:8)V ∗

1 (sk

1) − V πk

1 (sk

1) ≥ (cid:15)(cid:9)(cid:15)

(b)
≤

K
(cid:88)

k=1

Xk−1 − E[Xk | Fk−1],

1(cid:8) ¯V k

1 (sk

1) − V πk

1 (sk

1) ≥ (cid:15)(cid:9)(Xk−1 − E[Xk | Fk−1])

(a) holds by the deﬁnition of N(cid:15)(K). (b) follows from (15). (c) holds because {Xk}k≥0 is a DBP,
and thus, Xk−1 − E[Xk | Fk−1] ≥ 0 a.s. Therefore, the following relation holds:
(cid:40)

(cid:41)

9SH(H − h)
h

ln

3
δ

(cid:26)

⊆

∀(cid:15) > 0 : N(cid:15)(K)(cid:15) ≤

9SH(H − h)
h

ln

(cid:27)

,

3
δ

K
(cid:88)

∀K > 0 :

Xk−1 − E[Xk | Fk−1] ≤

k=1

from which we obtain that for any K > 0,

(cid:18)

Pr

∀(cid:15) > 0 : N(cid:15)(K)(cid:15) ≤

9SH(H − h)
h

ln

(cid:19)

3
δ

(cid:32)

≥ Pr

∀K > 0 :

K
(cid:88)

k=1

Xk−1 − E[Xk | Fk−1] ≤

9SH(Hh)
h

ln

3
δ

(cid:33)

(a)
≥ 1 − δ.

(a) holds because of the bound on the regret of DBP (see Theorem 1). Equivalently, for any K > 0,

(cid:18)

Pr

∃(cid:15) > 0 : N(cid:15)(K)(cid:15) ≥

9SH(H − h)
h

ln

(cid:19)

3
δ

≤ δ.

(16)

Note that for all (cid:15) > 0, K1 ≥ K2, 1{N(cid:15)(K2)(cid:15) ≥ C} = 1 implies 1{N(cid:15)(K1)(cid:15) ≥ C} = 1, and thus,
1{N(cid:15)(K)(cid:15) ≥ C} ≤ limK→∞ 1{N(cid:15)(K)(cid:15) ≥ C}. Furthermore, 1{N(cid:15)(K)(cid:15) ≥ C} ≥ 0 by deﬁnition.
Thus, we can apply the Monotone Convergence Theorem to conclude the proof:

(cid:18)

Pr

∃(cid:15) > 0 : N(cid:15)(cid:15) ≥

9SH(H − h)
h

ln

(cid:19)

3
δ

(cid:18)

(cid:26)

= Pr

lim
K→∞

∃(cid:15) > 0 : N(cid:15)(K)(cid:15) ≥

9SH(H − h)
h

ln

(cid:20)
= E

lim
K→∞

(cid:26)

1

(cid:18)

∃(cid:15) > 0 : N(cid:15)(K)(cid:15) ≥

9SH(H − h)
h

ln

= lim
K→∞

Pr

∃(cid:15) > 0 : N(cid:15)(K)(cid:15) ≥

9SH(H − h)
h

ln

3
δ

(a)= lim
K→∞

(cid:20)

E

1

(cid:26)

∃(cid:15) > 0 : N(cid:15)(K)(cid:15) ≥

(cid:27)(cid:21)

3
δ
(cid:19) (b)

≤ δ.

(cid:27)(cid:19)

3
δ
9SH(H − h)
h

(cid:27)(cid:21)

ln

3
δ

(a) is by the Monotone Convergence Theorem by which E[limk→∞ Xk] = limk→∞ E[Xk], for
Xk ≥ 0 and Xk ≤ limk→∞ Xk. (b) holds by (16).

16

12 h-RTDP with Approximate Model

Algorithm 6 h-RTDP with Approximate Model (h-RTDP-AM)

init: ∀s ∈ S, ∀n ∈ {0} ∪ [ H
for k ∈ [K] do
Initialize sk
1
for t ∈ [H] do

h ], ¯V 0

nh+1(s) = H − nh

if (t − 1) mod h == 0 then

t ) = ˆT h ¯V k−1

hc = t + h
¯V k
t (sk
end if
ak
t ∈ arg maxa r(sk
Act with ak

hc

(sk
t )

t , a) + ˆp(·|sk

t , a) ˆT hc−t−1 ¯V k−1
t , ak
t )

t+1 ∼ p(· | sk

hc

t and observe sk

end for

end for

Algorithm 6 contains the pseudocode of h-RTDP with approximate model. The algorithm is exactly
the same as h-RTDP (Algorithm 2) with the model p and optimal Bellman operator T replaced by
their approximations ˆp and ˆT . Meaning, h-RTDP is agnostic whether it uses the true or approximate
model.

We now provide the full proofs of all results in Section 6.1 in their chronological order. We use the
notation E ˆP to denote expectation w.r.t. the approximate model, i.e., w.r.t. the dynamics ˆp(s(cid:48) | s, a)
instead according to p(s(cid:48) | s, a).
Lemma 9. For all s ∈ S, n ∈ {0} ∪ [ H

h ], and k ∈ [K]:

(i) Bounded / Optimism: ˆV ∗

nh+1(s) ≤ ¯V k

nh+1(s).

(ii) Non-Increasing: ¯V k

nh+1(s) ≤ ¯V k−1

nh+1(s).

Proof. Both claims are proven using induction.

h − 1] and denote ˆT , ˆV ∗ as the optimal Bellman operators and optimal value of the

(i) Let n ∈ [0, H
approximate MDP (S, A, ˆp, r, H). See that they satisfy usual Bellman equation 2.
By the initialization, ∀s, t, ˆV ∗
1+hn(s). Assume the claim holds for k − 1 episodes. Let
sk
t be the state the algorithm is at in the t = 1 + hn time step of the k’th episode, i.e., at a time step
in which a value update is taking place. By the value update of Algorithm 6,
t ) = ( ˆT h ¯Vt+h)(sk
t (sk

1+hn(s) ≤ V 0

t ) ≥ ( ˆT h ˆV ∗

t ) = ˆV ∗

t+h)(sk

¯V k
t (sk

t ).

The second relation holds by the induction hypothesis and the monotonicity of ˆT h, a consequnce of
the monotonicity of ˆT , the optimal Bellman operator [4]. The third relation holds by the recursion
satisﬁed by the optimal value function (2). Thus, the induction step is proven for the ﬁrst claim.

h − 1] and let t = 1 + hn be a time step in which a value update is taking place.
t be the state the

(ii) Let n ∈ [0, H
To prove the base case of the second claim we use the optimistic initialization. Let s1
algorithm is at in the t’th time step of the ﬁrst episode. By the update rule,
t+h)(s0
t )

t ) = ( ˆT h ¯V 0

¯V 1
t (s1

(1)
=

max
π0,π1,..,πh−1

h−1
(cid:88)

E ˆP (cid:48)[

t(cid:48)=0

r(s(cid:48)

t, πt(cid:48)(s(cid:48)

t)) + ¯V 0

t+h(sh) | s0 = s0
t ]

(2)
≤ h + H − (t + h − 1) = H − (t − 1)

(3)
= ¯V 0

t (s1

t ).

Relation (1) is by the update rule (see Algorithm 6), when the expectation is taken place w.r.t. the
approximate model ˆP . Relation (2) holds since r(s, a) ∈ [0, 1] and and by the optimistic initialization

17

(see that for t the values at times step t + h were not updated and keep their initial value). For (3)
observe that H − (t − 1) is the value of the optimistic initialization.

Assume the second claim holds for k − 1 episodes. Let sk
t be the state that the algorithm is at in the
t’th time step of the k’th episode. Again, assume that t = 1 + hn, a time step in which a value update
is being done. By the value update of Algorithm 6, we have

¯V k
t (sk

t ) = ( ˆT h ¯V k−1

t+h )(sk

t ).

t was previously updated, let ¯k be the previous episode in which the update occured. By the
t (s) ≥ ¯V k−1
(s). Using the monotonicity of T h (due to

If sk
induction hypothesis, we have that ∀s, t, ¯V ¯k
the monotonicity of the Bellman operator),

t

( ˆT h ¯V k−1

t+h )(sk

t ) ≤ ( ˆT h ¯V

¯k
t+h)(sk

t ) = ¯V k−1

t

(sk

t ).

Thus, ¯V k
¯V k−1
(sk
t
and the result is proven similarly to the base case.

t ) ≤ ¯V k−1(sk
t (sk

t (sk
t ) = ¯V 0

t ) and the induction step is proved. If sk

t ). In this case, the induction hypothesis implies that ∀s(cid:48), ¯V k−1

t was not previously updated, then
t+h(s(cid:48))

t+h (s(cid:48)) ≤ ¯V 0

Lemma 10. The expected cumulative value update at the k’th episode of h-RTDP-AM satisﬁes the
following relation:

¯V k
1 (sk

1) − V πk

1 (sk

1) =

H(H − 1)
2

(cid:15)P

+

H
h −1
(cid:88)

(cid:88)

n=1

s∈S

nh+1(s) − E[ ¯V k
¯V k−1

nh+1(s) | Fk−1].

Proof. Let n ∈ [0, H
By the deﬁnition of the update rule, the following holds for the update at the visited state sk
t :

h − 1] and let t = 1 + hn be a time step in which a value update is taking place.

t ) = ( ˆT h ¯V k−1

¯V k
t (sk
= ( ˆT πk(t) · · · ˆT πk(t+h−1) ¯V k−1

t+h )(sk
t )

t+h )(sk
t )

(cid:34)t+h−1
(cid:88)

= EP (cid:48)

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (sk

t+h) | πk, sk
t

(cid:35)

t(cid:48)=t
(cid:34)t+h−1
(cid:88)

= E

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (sk

t+h) | πk, sk
t

(cid:35)

t(cid:48)=t

+ EP (cid:48)

(cid:34)t+h−1
(cid:88)

t(cid:48)=t

(cid:34)t+h−1
(cid:88)

= E

t(cid:48)=t

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (sk

t+h) | πk, sk
t

(cid:35)

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (sk

t+h) | πk, sk
t

(17)

(cid:35)

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (sk

t+h) | πk, sk
t

(cid:35)

(cid:34)t+h−1
(cid:88)

− E

t(cid:48)=t

t+h−1
(cid:88)

(cid:88)

(cid:16)

+

P πk (st(cid:48) | sk

t ) − ˆP πk (st(cid:48) | sk
t )

(cid:17)

r(sk

t(cid:48), ak

t(cid:48)) +

t(cid:48)=t
(cid:34)t+h−1
(cid:88)

≤ E

st(cid:48)

(cid:35)

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (sk

t+h) | πk, sk
t

(cid:88)

(cid:16)

st+h

P πk (st+h | sk

t ) − ˆP πk (st+h | sk
t )

(cid:17) ¯V k−1

t+h (sk

t+h))

t(cid:48)=t

t+h−1
(cid:88)

t(cid:48)=t

+

(cid:12)
(cid:88)
(cid:12)P πk (st(cid:48) | sk
(cid:12)

st(cid:48)

t ) − ˆP πk (st(cid:48) | sk
t )

(cid:12)
(cid:12)
(cid:12) + (H − (t + h − 1))

(cid:88)

st+h

(cid:12)
(cid:12)P πk (st+h | sk
(cid:12)

t ) − ˆP πk (st+h | sk
t )

(cid:12)
(cid:12)
(cid:12).

18

Applying Lemma 16 we bound the above by,

(cid:34)t+h−1
(cid:88)

(17) ≤ E

t(cid:48)=t

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (sk

t+h) | πk, sk
t

(cid:35)

+

t+h−1
(cid:88)

(t(cid:48) − t)(cid:15)P + (H − (t + h − 1))h(cid:15)P

(cid:34)t+h−1
(cid:88)

= E

t(cid:48)=t
(cid:34)t+h−1
(cid:88)

= E

t(cid:48)=t

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (sk

t+h) | πk, sk
t

(cid:35)

−

(cid:35)

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (sk

t+h) | Fk−1, sk
t

t(cid:48)=t

1
2

(h − 1)h(cid:15)P + (H − t)h(cid:15)P

−

1
2

(h − 1)h(cid:15)P + (H − t)h(cid:15)P .

(18)

Where the second relation holds by using the close form of the arithmetic sum and by algebraic
manipulations. For the third relation, we observe that given πk, sk
t(cid:48) with t(cid:48) ≥ t is
independent of the past episodes (see 10),

t the state sk

(cid:34)t+h−1
(cid:88)

E

t(cid:48)=t

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (sk

t+h) | πk, sk
t

(cid:35)

(cid:34)t+h−1
(cid:88)

= E

t(cid:48)=t

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (sk

t+h) | Fk−1, sk
t

(cid:35)

Taking the conditional expectation w.r.t. Fk−1 of both (17) and its RHS (18), using the tower
property and the fact for all s, ¯VH+1(s) = 0 we get,

E(cid:2) ¯V k

t (sk

t ) | Fk−1

(cid:34)t+h−1
(cid:88)

(cid:3) ≤E

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (sk

t+h) | Fk−1

(cid:35)

t(cid:48)=t
1
2

(h − 1)h(cid:15)P + (H − t)h(cid:15)P

−

Let us denote dn := − 1
using linearity of expectation, and the fact ¯V k

2 (h − 1)h(cid:15)P + (H − n)h(cid:15)P . Summing the above relation for all n ∈ [ H

h ] − 1,

H+1(s) = for all s, k,

H
h −1
(cid:88)

n=0

E(cid:2) ¯V k

1+nh(sk

t ) | Fk−1

(cid:34) H
(cid:88)

(cid:3) = E

t=1

r(sk

t , ak

t ) | Fk−1

(cid:35)

+

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k−1

1+nh(sk

1+nh) | Fk−1

H
h −1
(cid:88)

(cid:3) +

d1+nh

n=0
(19)

By simple algebraic manipulation we get (cid:80) H
(19) has the following equivalent forms, by which we conclude the proof of this lemma.

h −1
n=0 d1+nh = 1

2 H(H − 1)(cid:15)P (see Lemma 18). Thus,

⇐⇒ ¯V k

1 (sk

1) +

⇐⇒ ¯V k

1 (sk

1) +

H
h −1
(cid:88)

n=1

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k

1+nh(sk

t ) | Fk−1

E(cid:2) ¯V k

1+nh(sk

t ) | Fk−1

(cid:34) H
(cid:88)

(cid:3) = E

t=1

r(sk

t , ak

t ) | Fk−1

(cid:35)

+

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k−1

1+nh(sk

1+nh) | Fk−1

(cid:3) +

1
2

H(H − 1)(cid:15)P

(cid:3) = V πk (sk

1) +

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k−1

1+nh(sk

1+nh) | Fk−1

(cid:3) +

1
2

H(H − 1)(cid:15)P

⇐⇒ ¯V k

1 (sk

1) − V πk (sk

1) =

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k−1

1+nh(sk

1+nh) − ¯V k

1+nh(sk

1+nh) | Fk−1

(cid:3) +

1
2

H(H − 1)(cid:15)P

⇐⇒ ¯V k

1 (sk

1) − V πk (sk

1) =

K
(cid:88)

H
h −1
(cid:88)

(cid:88)

k=1

n=1

s

nh+1(s) − E[ ¯V k
¯V k−1

nh+1(s) | Fk−1] +

1
2

H(H − 1)(cid:15)P

The second line holds by the fact sk

1 is measurable w.r.t. Fk−1, the third line holds since

(cid:34) H
(cid:88)

V πk
1 (sk

1) = E

r(sk

t , ak

t ) | Fk−1

(cid:35)

.

t=1

19

The forth line holds by Lemma 15 with ¯V k
only at the visited state sk
is valid to apply the lemma.

t and the update rule uses ¯V k−1

t = gk

t for t = nh + 1. See that the update of ¯V k

t occurs
t+1 , i.e., it is measurable w.r.t. to Fk−1, and it

Theorem 5 (Performance of h-RTDP-AM). Let (cid:15), δ > 0. The following holds for h-RTDP-AM:

1. With probability 1 − δ, for all K > 0, Regret(K) ≤ 9SH(H−h)

h

ln(3/δ) + H(H − 1)(cid:15)P K.

2. Let ∆P = H(H − 1)(cid:15)P . Then, Pr

(cid:110)

∃(cid:15) > 0 : N ∆P

(cid:15) ≥ 9SH(H−h) ln(3/δ)

h(cid:15)

(cid:111)

≤ δ.

Proof. We start by proving claim (1). The following bounds on the regret hold.

Regret(K) :=

K
(cid:88)

k=1

1 (sk
V ∗

1) − V πk

1 (sk
1)

≤

≤

K
(cid:88)

k=1

K
(cid:88)

k=1

ˆV ∗
1 (sk

1) − V πk

1 (sk

1) +

¯V k
1 (sk

1) − V πk

1 (sk

1) +

H(H − 1)
2

(cid:15)P

H(H − 1)
2

(cid:15)P

= H(H − 1)(cid:15)P K +

K
(cid:88)

H
h −1
(cid:88)

(cid:88)

k=1

n=1

s

nh+1(s) − E[ ¯V k
¯V k−1

nh+1(s) | Fk−1]

(20)

The second relation holds by Lemma 17 which relates the optimal value of the approximate model
to the optimal value of the environment. The third relation is by the optimism of the value function
(Lemma 9), and the forth relation is by Lemma 10.

We now observe the regret is a regret of a Decreasing Bounded Process. Let

Xk :=

H
h −1
(cid:88)

(cid:88)

n=1

s

¯V k
nh+1(s),

(21)

and observe that {Xk}g≥0 is a Decreasing Bounded Process.

1. It is decreasing since for all s, t ¯V k

t (s) ≤ ¯V k−1

t

decreasing.

(s) by Lemma 9. Thus, their sum is also

2. It is bounded since for all s, t ¯V k

t (s) ≥ V ∗

t (s) ≥ 0 by Lemma 9. Thus, the sum is bounded

from below by 0.

See that the initial value can be bounded as follows,

X0 =

H
h −1
(cid:88)

(cid:88)

n=1

s

¯V 0
nh+1(s) ≤

H
h −1
(cid:88)

(cid:88)

n=1

s

H =

SH(H − h)
h

.

Using linearity of expectation and the deﬁnition (21) we observe that (20) can be written,

Regret(K) ≤ (20) = H(H − 1)(cid:15)P K +

K
(cid:88)

k=1

Xk−1 − E[Xk | Fk−1],

which is regret of A Bounded Decreasing Process. Applying the regret bound on DBP, Theorem 1,
we conclude the proof of the ﬁrst claim.

20

We now prove the claim (2) using the proving technique at Theorem 4. Denote ∆P = H(H − 1)(cid:15)P .
The following relations hold for all (cid:15) > 0.
(cid:27)(cid:18)

(cid:26)

(cid:19)

∆P
2

+ (cid:15)

∆P
2

(cid:15) +

(cid:27)(cid:18)

+ (cid:15)

(cid:15) +

(cid:19)

∆P
2

1) − V πk

1 (sk

1) ≥

1

ˆV ∗
1 (sk
(cid:26)

≤ 1

¯V k
1 (sk

1) − V πk

1 (sk

1) ≥

(cid:26)

(cid:26)

(cid:26)

≤ 1

= 1

= 1

¯V k
1 (sk

1) − V πk

1 (sk

1) ≥

¯V k
1 (sk

1) − V πk

1 (sk

1) ≥

¯V k
1 (sk

1) − V πk

1 (sk

1) ≥

∆P
2
∆P
2

∆P
2

∆P
2

(cid:27)

+ (cid:15)

(cid:0) ¯V k

1 (sk

1) − V πk

1 (sk

1)(cid:1)

(cid:27)





+ (cid:15)

H
h −1
(cid:88)

(cid:88)

n=1

s

¯V k−1
nh+1(s) − E[ ¯V k

nh+1(s) | Fk−1] +





∆P
2

(cid:27)(cid:18)

Xk−1 − E[Xk | Fk−1] +

+ (cid:15)

(cid:19)

.

∆P
2

The ﬁrst relation holds since for all t, s, ¯V k
t (s) ≥ ˆV ∗
t (s) by Lemma 9. The second relation holds
by the indicator function and the third relation holds by Lemma 10. The forth relation holds by the
deﬁnition of Xk (21) and linearity of expectation. Using an algebraic manipulation the above leads
to the following relation,

(cid:26)

1

ˆV ∗
1 (sk

1) − V πk

1 (sk

1) ≥

(cid:27)

+ (cid:15)

(cid:15) ≤ 1

(cid:26)

∆P
2

¯V k
1 (sk

1) − V πk

1 (sk

1) ≥

(cid:27)

+ (cid:15)

∆P
2

(Xk−1 − E[Xk | Fk−1])

(22)

As we wish the ﬁnal performance to be compared to V ∗ and not ˆV we use the the ﬁrst claim of
Lemma 17, by which for all s, ˆV ∗

1(cid:8)V ∗

1 (sk

1) − V πk

1 (sk

1 (s) − ∆P
1 (s) ≥ V ∗
2 . This implies that
(cid:26)
1) ≥ ∆P + (cid:15)(cid:9) ≤ 1

1) − V πk

ˆV ∗
1 (sk

1 (sk

1) ≥

(cid:27)

+ (cid:15)

.

∆P
2

(23)

Combining all the above, we get

1(cid:8)V ∗
1 (sk
(cid:26)

1) − V πk

1 (sk

1) ≥ ∆P + (cid:15)(cid:9)(cid:15)

≤ 1

ˆV ∗
1 (sk

1) − V πk

1 (sk

1) ≥

(cid:26)

≤ 1

¯V k
1 (sk

1) − V πk

1 (sk

1) ≥

(cid:27)

+ (cid:15)

(cid:15)

(cid:27)

+ (cid:15)

(Xk−1 − E[Xk | Fk−1]).

(24)

∆P
2
∆P
2

The ﬁrst relation is by (23) and the second relation by (22).
1(cid:8)V ∗
Deﬁne N(cid:15)(K) = (cid:80)K
V πk
1 (sk
denote we get that for all (cid:15) > 0

1) −
1) ≥ ∆P + (cid:15) at the ﬁrst K episodes. Summing the above inequality (24) for all k ∈ [K] and

1) ≥ ∆P + (cid:15)(cid:9) as the number of times V ∗

1) − V πk

1 (sk

1 (sk

1 (sk

k=1

N(cid:15)(K)(cid:15) =

K
(cid:88)

k=1

1(cid:8)V ∗

1 (sk

1) − V πk

1 (sk

1) ≥ ∆P + (cid:15)(cid:9)(cid:15)

K
(cid:88)

(cid:26)

1

≤

¯V k
1 (sk

1) − V πk

1 (sk

1) ≥

k=1

K
(cid:88)

k=1

≤

Xk−1 − E[Xk | Fk−1].

(cid:27)

+ (cid:15)

∆P
2

(Xk−1 − E[Xk | Fk−1])

The ﬁrst relation holds by deﬁnition, the second by (24) and the third relation holds as {Xk}k≥0 is a
DBP (21) and, thus, Xk−1 − E[Xk | Fk−1] ≥ 0 a.s. . Thus, the following relation holds
(cid:40)

(cid:41)

∀K > 0 :

Xk−1 − E[Xk | Fk−1] ≤

9SH(H − h)
h

ln

3
δ

(cid:26)

⊆

∀(cid:15) > 0 : N(cid:15)(K)(cid:15) ≤

9SH(H − h)
h

ln

(cid:27)

,

3
δ

K
(cid:88)

k=1

21

from which we get that for any K > 0

(cid:18)

Pr

∀(cid:15) > 0 : N(cid:15)(K)(cid:15) ≤

9SH(H − h)
h

ln

(cid:19)

3
δ

(cid:32)

≥ Pr

∀K > 0 :

K
(cid:88)

k=1

Xk−1 − E[Xk | Fk−1] ≤

(cid:33)

9SH(Hh)
h

ln

3
δ

≥ 1 − δ,

and the third relation holds the bound on the regret of DBP, Theorem 1. Equivalently, for any K > 0,

(cid:18)

Pr

∃(cid:15) > 0 : N(cid:15)(K)(cid:15) ≥

9SH(H − h)
h

ln

(cid:19)

3
δ

≤ δ.

(25)

Applying the Monotone Convergence Theorem as in the proof of Theorem 4 we conclude the proof.

22

Algorithm 7 h-RTDP with Approximate Value Updates (h-RTDP-AV)

init: ∀s ∈ S, n ∈ {0} ∪ [ H
for k ∈ [K] do
Initialize sk
1
for t ∈ [H] do

h ], ¯V 0

nh+1(s) = H − nh

if (t − 1) mod h == 0 then

t ) + T h ¯V k−1

hc

(sk

t ) ;

¯V k
t (sk

t ) ← min(cid:8) ¯V k

t (sk

t ), ¯V k−1

t

(sk

t )(cid:9) ;

t ) = (cid:15)V (sk

hc = t + h
¯V k
t (sk
end if
t ∈ arg maxa r(sk
ak
Act with ak

end for

end for

t , a) + p(·|sk

t , a)T hc−t−1 ¯V k−1
t , ak
t )

t+1 ∼ p(· | sk

hc

t and observe sk

;

13 h-RTDP with Approximate Value updates

Lemma 11. For all s ∈ S, n ∈ {0}∪[ H

h ], and k ∈ [K]:

(i) Bounded / Optimism:

nh+1(s) ≤ ¯V k
V ∗

nh+1(s)+(cid:15)V (

H
h

−n).

(ii) Non-Increasing: ¯V k

nh+1(s) ≤ ¯V k−1

nh+1(s).

Proof. We prove the ﬁrst claim by induction. The second claim holds by construction.

h ]. By the optimistic initialization, ∀s, n, V ∗

(i) Let n ∈ {0}∪[ H
1+hn(s) ≤
V 0
1+hn(s). Assume the claim holds for k − 1 episodes. Let sk
t be the state the algorithm is at in the
t = 1 + hn time step of the k’th episode, i.e., at a time step in which a value update is taking place.
Let e ∈ RS be the constant vector of ones. By the value update of Algorithm 7,

1+hn(s)−(cid:15)V ( H

h −n) ≤ V ∗

¯V k
t (sk

t ) = min(cid:8)(cid:15)V (sk

t ) + T h ¯V k−1

hc

(sk

t ), ¯V k−1

t

(sk

t )(cid:9).

(26)

If the minimal value is ¯V k−1
assumption. If (cid:15)V (sk

(sk
t ) + T h ¯V k−1

t

t ) then ¯V k

t (sk

t ) satisﬁes the induction hypothesis by the induction
t ) is the minimal value in (26), then the following relation holds,

(sk

hc

¯V k
t (sk

t ) = (cid:15)V (sk

≥ −(cid:15)V + T h ¯V k−1

t ) + T h ¯V k−1
t+h (sk
t )
t+h (sk
t )
(cid:18)

≥ −(cid:15)V + T h

V ∗
t+h − e(cid:15)V (

= −(cid:15)V + T hV ∗

t+h(sk

t ) − (cid:15)V (

− n − 1)

− n − 1)

(cid:19)

(sk
t )

H
h
H
h

= T hV ∗

t+h(sk

H
h

− n)

t ) − (cid:15)V (
H
h

= V ∗

t (sk

t ) − (cid:15)V (

− n).

The second relation holds by the assumption |(cid:15)V (sk
t )| ≤ (cid:15)V . The third relation by the induction
hypothesis and the monotonicity of T h. The forth relation holds since for any constant α ∈ R and
V ∈ Rs, T (V + αe) = T V + α (e.g.,[4]) and thus T h(V + αe) = T hV + α. Lastly, the ﬁfth relation
holds by the Bellman equations (2).

(ii) The second claim holds by construction of the update rule ¯V k
which enforces ¯V k

t ) ← min(cid:8) ¯V k
(s) for every updated state, and thus for all s and t.

t (sk

t (s) ≤ ¯V k−1

t

t (sk

t ), ¯V k−1

t

(sk

t )(cid:9)

23

Lemma 12. The expected cumulative value update at the k’th episode of h-RTDP-AV satisﬁes the
following relation:

¯V k
1 (sk

≤

H
h

1) − V πk
1 (sk
1)
H
h −1
K
(cid:88)
(cid:88)

(cid:15)V +

k=1

n=1

s∈S

(cid:88)

¯V k−1
nh+1(s) − E[ ¯V k

nh+1(s) | Fk−1].

Proof. Let n ∈ {0} ∪ [ H
h − 1] and let t = 1 + hn be a time step in which a value update is taking
place. By the deﬁnition of the update rule, the following holds for the update at the visited state sk
t :

¯V k
t (sk

t ) = (cid:15)V (sk

t ) + (T h ¯V k−1

t+h )(sk
t )

≤ (cid:15)V + (T πk(t) · · · T πk(t+h−1) ¯V k−1

t+h )(sk
t )

(cid:34)t+h−1
(cid:88)

= (cid:15)V + E

t(cid:48)=t

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (sk

t+h) | Fk−1, sk
t

(cid:35)
.

Where the third relation holds by the same argument as in (10). Taking the conditional expectation
w.r.t. Fk−1, using the tower property and the fact for all s, ¯VH+1(s) = 0 we get,

E(cid:2) ¯V k

t (sk

t ) | Fk−1

(cid:3) ≤ (cid:15)V + E

(cid:34)t+h−1
(cid:88)

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (sk

t+h) | Fk−1

(cid:35)

.

t(cid:48)=t
Summing the above relation for all n ∈ {0} ∪ [ H
¯V k
H+1(s) = for all s, k,

h − 1], using linearity of expectation, and the fact

H
h −1
(cid:88)

n=0

E(cid:2) ¯V k

1+nh(sk

t ) | Fk−1

(cid:3) ≤

H
h

(cid:34) H
(cid:88)

(cid:15)V + E

t=1

r(sk

t , ak

t ) | Fk−1

(cid:35)

+

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k−1

1+nh(sk

1+nh) | Fk−1

(cid:3)

⇐⇒ ¯V k

1 (sk

1) +

⇐⇒ ¯V k

1 (sk

1) +

H
h −1
(cid:88)

n=1

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k

1+nh(sk

t ) | Fk−1

E(cid:2) ¯V k

1+nh(sk

t ) | Fk−1

(cid:3) ≤

(cid:3) ≤

H
h

H
h

(cid:34) H
(cid:88)

(cid:15)V + E

t=1

r(sk

t , ak

t ) | Fk−1

(cid:35)

+

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k−1

1+nh(sk

1+nh) | Fk−1

(cid:3)

(cid:15)V + V πk (sk

1) +

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k−1

1+nh(sk

1+nh) | Fk−1

(cid:3)

⇐⇒ ¯V k

1 (sk

1) − V πk (sk

1) ≤

⇐⇒ ¯V k

1 (sk

1) − V πk (sk

1) ≤

H
h

H
h

(cid:15)V +

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k−1

1+nh(sk

1+nh) − ¯V k

1+nh(sk

1+nh) | Fk−1

(cid:3)

(cid:15)V +

K
(cid:88)

H
h −1
(cid:88)

(cid:88)

k=1

n=1

s

¯V k−1
nh+1(s) − E[ ¯V k

nh+1(s) | Fk−1]

The second line holds by the fact sk

1 is measurable w.r.t. Fk−1, and the third line holds since

(cid:34) H
(cid:88)

V πk
1 (sk

1) = E

r(sk

t , ak

t ) | Fk−1

(cid:35)

.

The ﬁfth line holds by by Lemma 15 with ¯V k
only at the visited state sk
is valid to apply the lemma.

t and the update rule uses ¯V k−1

t for t = nh + 1. See that the update of ¯V k

t occurs
t+1 , i.e., it is measurable w.r.t. to Fk−1, and it

t=1
t = gk

Theorem 6 (Performance of h-RTDP-AV). Let (cid:15), δ > 0. The following holds for h-RTDP-AV:
δ ) + 2H

1. With probability 1 − δ, for all K > 0, Regret(K) ≤ 9SH(H−h)
(cid:110)
(cid:15) ≥ 9SH(H−h)(1+ ∆V

2. Let ∆V = 2H(cid:15)V . Then, Pr

h (cid:15)V ) ln( 3
(cid:111)

∃(cid:15) > 0 : N

2h ) ln( 3
δ )

(1 + H

≤ δ.

∆V
h

h

h(cid:15)

h (cid:15)V K.

24

Proof. We start by proving claim (1). The following bounds on the regret hold.

Regret(K) :=

K
(cid:88)

k=1

1 (sk
V ∗

1) − V πk

1 (sk
1)

≤

=

K
(cid:88)

k=1

2H
h

¯V k
1 (sk

1) − V πk

1 (sk

1) +

H
h

(cid:15)V

(cid:15)V K +

K
(cid:88)

H
h −1
(cid:88)

(cid:88)

k=1

n=1

s

¯V k−1
nh+1(s) − E[ ¯V k

nh+1(s) | Fk−1]

(27)

The second relation is by the approximated optimism of the value function when approximate value
updates are used (Lemma 11). The third relation is by Lemma 12.

We now observe the regret is a regret of a Decreasing Bounded Process. Let

Xk :=

H
h −1
(cid:88)

(cid:88)

n=1

s

¯V k
nh+1(s),

(28)

and observe that {Xk}g≥0 is a Decreasing Bounded Process.

1. It is decreasing since for all s, t ¯V k

t (s) ≤ ¯V k−1

t

decreasing.

(s) by Lemma 11. Thus, their sum is also

2. It is bounded since for all s, n ∈ [ H

h ] − 1,

¯V k
1+hn(s) ≥ V ∗

1+hn(s) − (cid:15)V (

H
h

− n) ≥ −(cid:15)V (

H
h

− n) ≥ −(cid:15)V

H
h

by Lemma 11. Thus, X0 which is a sum of the above terms is bounded from below by
− (cid:15)V
h

SH(H−h)
h

.

See that the initial value can be bounded as follows,

X0 =

H
h −1
(cid:88)

(cid:88)

n=1

s

¯V 0
nh+1(s) ≤

H
h −1
(cid:88)

(cid:88)

n=1

s

H =

SH(H − h)
h

.

Using linearity of expectation and the deﬁnition (14) we observe that (27) can be written,

Regret(K) ≤ (27) =

2H
h

(cid:15)V K +

K
(cid:88)

k=1

Xk−1 − E[Xk | Fk−1],

which is regret of A Bounded Decreasing Process. Applying the regret bound on DBP, Theorem 1 we
conclude the proof of the ﬁrst claim.

We now prove claim (2) using the proving technique at Theorem 4. Denote ∆V = 2H(cid:15)V . The
following relations hold for all (cid:15) > 0.

(cid:26)

1

¯V k
1 (sk
(cid:26)

1) − V πk

1 (sk

1) ≥

∆V
2h

(cid:27)(cid:18)

+ (cid:15)

(cid:15) +

(cid:19)

∆V
2h

≤ 1

¯V k
1 (sk

1) − V πk

1 (sk

1) ≥

(cid:26)

(cid:26)

= 1

= 1

¯V k
1 (sk

1) − V πk

1 (sk

1) ≥

¯V k
1 (sk

1) − V πk

1 (sk

1) ≥

∆V
2h

∆V
2h

∆V
2h

(cid:27)

+ (cid:15)

(cid:0) ¯V k

1 (sk

1) − V πk

1 (sk

1)(cid:1)

(cid:27)





+ (cid:15)

H
h −1
(cid:88)

(cid:88)

n=1

s

nh+1(s) − E[ ¯V k
¯V k−1

nh+1(s) | Fk−1] +





∆V
2h

(cid:27)(cid:18)

Xk−1 − E[Xk | Fk−1] +

+ (cid:15)

(cid:19)

.

∆V
2h

25

The ﬁrst relation holds by the indicator function and the second relation by Lemma 12. The
third relation holds by the deﬁnition of Xk (28) and linearity of expectation. Using an algebraic
manipulation the above leads to the following relation,
∆V
2h

(Xk−1 − E[Xk | Fk−1])

1) − V πk

1) − V πk

¯V k
1 (sk

¯V k
1 (sk

1 (sk

1 (sk

∆V
2h

(cid:15) ≤ 1

1) ≥

1) ≥

+ (cid:15)

+ (cid:15)

(cid:27)

(cid:27)

(cid:26)

(cid:26)

1

(29)

As we wish the ﬁnal performance to be compared to V ∗ we use the the ﬁrst claim of Lemma 11, by
which for all s, k, ¯V k
1 (s) − ∆V
2h . This implies that
∆V
h

1 (s) ≥ V ∗

1) − V πk

1) − V πk

¯V k
1 (sk

1 (sk
V ∗

1 (sk

1 (sk

∆V
2h

1) ≥

1) ≥

≤ 1

(30)

+ (cid:15)

+ (cid:15)

(cid:26)

(cid:27)

(cid:27)

(cid:26)

1

.

Combining the above we get

(cid:26)

1

1 (sk
V ∗
(cid:26)

1) − V πk

1 (sk

1) ≥

(cid:27)

+ (cid:15)

(cid:15)

∆V
h

≤ 1

¯V k
1 (sk

1) − V πk

1 (sk

1) ≥

(cid:26)

≤ 1

¯V k
1 (sk

1) − V πk

1 (sk

1) ≥

(cid:27)

+ (cid:15)

(cid:15)

(cid:27)

+ (cid:15)

(Xk−1 − E[Xk | Fk−1]).

(31)

∆V
2h
∆V
2h

The ﬁrst relation is by (30) and the second relation by (29).
1(cid:8)V ∗
Deﬁne N(cid:15)(K) = (cid:80)K
V πk
1) ≥ ∆V
1 (sk
denote we get that for all (cid:15) > 0

1) −
h + (cid:15) at the ﬁrst K episodes. Summing the above inequality (31) for all k ∈ [K] and

h + (cid:15)(cid:9) as the number of times V ∗

1) − V πk

1) ≥ ∆V

1 (sk

1 (sk

1 (sk

k=1

N(cid:15)(K)(cid:15) =

K
(cid:88)

(cid:26)

1

k=1

1 (sk
V ∗

1) − V πk

1 (sk

1) ≥

(cid:27)

+ (cid:15)

(cid:15)

∆V
h

K
(cid:88)

(cid:26)

1

≤

¯V k
1 (sk

1) − V πk

1 (sk

1) ≥

k=1

K
(cid:88)

k=1

≤

Xk−1 − E[Xk | Fk−1].

(cid:27)

+ (cid:15)

∆V
2h

(Xk−1 − E[Xk | Fk−1])

The ﬁrst relation holds by deﬁnition, the second by (31) and the third relation holds as {Xk}k≥0 is a
DBP (28) and, thus, Xk−1 − E[Xk | Fk−1] ≥ 0 a.s. . Thus, the following relation holds
(cid:41)

(cid:40)

K
(cid:88)

∀K > 0 :

Xk−1 − E[Xk | Fk−1] ≤

9SH(H − h)
h

(1 +

(cid:15)V ) ln

H
h

3
δ

k=1

(cid:26)

⊆

∀(cid:15) > 0 : N(cid:15)(K)(cid:15) ≤

9SH(H − h)
h

(1 +

H
h

(cid:15)V ) ln

(cid:27)

,

3
δ

from which we get for any K > 0

(cid:18)

Pr

∀(cid:15) > 0 : N(cid:15)(K)(cid:15) ≤

9SH(H − h)
h

(1 +

H
h

(cid:15)V ) ln

(cid:19)

3
δ

(cid:32)

≥ Pr

∀K > 0 :

K
(cid:88)

k=1

Xk−1 − E[Xk | Fk−1] ≤

9SH(Hh)
h

(1 +

H
h

(cid:15)V ) ln

(cid:33)

3
δ

≥ 1 − δ,

and the third relation holds the bound on the regret of DBP, Theorem 1. Equivalently, for any K > 0,
9SH(H − h)
h
Applying the Monotone Convergence Theorem as in the proof of Theorem 4 we conclude the proof.

∃(cid:15) > 0 : N(cid:15)(K)(cid:15) ≥

(cid:15)V ) ln

H
h

(1 +

≤ δ.

(32)

3
δ

Pr

(cid:18)

(cid:19)

26

Algorithm 8 h-RTDP with Approximate State Abstraction (h-RTDP-AA)

init: ∀sφ ∈ Sφ, n ∈ {0} ∪ [ H
for k ∈ [K] do
Initialize sk
1
for t ∈ [H] do

h ], ¯V 0

φ,nh+1(sφ) = H − nh

if (t − 1) mod h == 0 then
¯V k
φ,t(φt(sk
(cid:110) ¯V k

hc = t + h ;
¯V k
φ,t(φt(sk
end if
ak
t ∈ arg maxa r(sk
Act with ak

t and observe sk

t )) ← min

t , a) + p(·|sk

t )) = T h
φ

¯V k−1
(sk
t ) ;
φ,hc
t )), ¯V k−1
φ,t (φt(sk

φ,t(φt(sk

t ))

(cid:111)

;

t , a)T hc−t−1
φ
t , ak
t+1 ∼ p(· | sk
t )

¯V k−1
φ,hc

;

end for

end for

14 h-RTDP with Approximate State Abstraction

In this section we analyze the performance of h-RTDP performance which uses approximate
abstraction. For clarity we restate the assumption we make on the approximate abstraction and
the deﬁnition of equivalent set under abstraction.
Assumption 1 (Approximate Abstraction, [23], deﬁnition 3.3). For any s, s(cid:48) ∈ S and n ∈ {0} ∪
[ H
h − 1] for which φnh+1(s) = φnh+1(s(cid:48)), we have |V ∗

nh+1(s(cid:48))| ≤ (cid:15)A.

nh+1(s) − V ∗

An important quantity in our analysis is the set of states equivalent to a given state s under φnh+1.
Deﬁnition 1 (Equivalent Set Under Abstraction). For any s ∈ S and n ∈ {0} ∪ [ H
h − 1], we deﬁne
the set of states equivalent to s under φnh+1 as Φnh+1(s) := {s(cid:48) ∈ S : φnh+1(s) = φnh+1(s(cid:48))}.

t when using abstraction. Unlike the usual deﬁnition of ¯V k
φ,t is a mapping from the abstract state space to the reals, i.e., ¯V k

Before we supply with the proof we emphasize an important difference in the deﬁnition of the
value function ¯V k
: S → R, in case of
abstraction ¯V k
φ,t : Sφ → R. Meaning,
¯V k
φ,t is deﬁned on the abstract state space. Given a state s ∈ S we need to query φt to obtain its value
at time t by ¯V k
Lemma 13. For all s ∈ S, n ∈ {0} ∪ [ H

t (φt(s)).

t

h ], and k ∈ [K]:

(i) Optimism:

max
s(cid:48)∈Φnh+1(s)

nh+1(s(cid:48)) ≤ ¯V k
V ∗

nh+1(φnh+1(s)) + (cid:15)A(

H
h

− n).

(ii) Bounded: ¯V k

nh+1(φnh+1(s)) ≥ 0.

(iii) Non-Increasing: ¯V k

nh+1(φnh+1(s)) ≤ ¯V k−1

nh+1(φnh+1(s)).

Proof. We prove the ﬁrst claim by induction. The second and third claims hold by construction.

(i) Let n ∈ {0} ∪ [ H
h − 1]. By the optimistic initialization, ∀s, n, V ∗
h − n) ≤
1+hn(s) ≤ V 0
V ∗
1+hn(φ1+hn(s)). Assume the claim holds for k − 1 episodes. Let sk
t be the state the
algorithm is at in the t = 1 + hn time step of the k’th episode, i.e., at a time step in which a value
update is taking place. By the value update of Algorithm 8,

1+hn(s) − (cid:15)A( H

¯V k
t (φ(sk

t )) = min(cid:8)T h ¯V k−1

hc

(sk

t ), ¯V k−1

t

(φ(sk

t ))(cid:9).

(33)

If the minimal value is ¯V k−1
induction assumption. If T h ¯V k−1

(φ(sk
(sk

t

t )) then ¯V k
t )) satisﬁes the induction hypothesis by the
t ) is the minimal value in (33), then the following relation holds,

t (φ(sk

hc

27

¯V k
t (φt(sk

t )) =

max
π0,π1,..,πh−1

≥

max
π0,π1,..,πh−1

=

max
π0,π1,..,πh−1

≥

max
π0,π1,..,πh−1

h−1
(cid:88)

E[

t(cid:48)=0

h−1
(cid:88)

E[

t(cid:48)=0

h−1
(cid:88)

E[

t(cid:48)=0

h−1
(cid:88)

E[

t(cid:48)=0

r(s(cid:48)

t, πt(cid:48)(s(cid:48)

t)) + ¯V k−1

t+h (φ(sh)) | s0 = sk
t ]

r(s(cid:48)

t, πt(cid:48)(s(cid:48)

t)) + max

s(cid:48)∈Φt+h(sh)

t+h(s(cid:48)) − (cid:15)A
V ∗

(cid:18) H
h

(cid:19)

− n − 1

r(s(cid:48)

t, πt(cid:48)(s(cid:48)

t)) + max

s(cid:48)∈Φt+h(sh)

t+h(s(cid:48)) | s0 = sk
V ∗

t ] − (cid:15)A

r(s(cid:48)

t, πt(cid:48)(s(cid:48)

t)) + V ∗

t+h(sh) | s0 = sk

t ] − (cid:15)A

(cid:18) H
h

− n − 1

| s0 = sk
t ]

(cid:19)

− n − 1

(cid:18) H
h

(cid:19)

= V ∗

t (sk

t ) − (cid:15)A

(cid:18) H
h

(cid:19)

− n − 1

≥ max
s(cid:48)∈Φt(sk
t )

t (sk
V ∗

t ) − (cid:15)A − (cid:15)A

= max
s(cid:48)∈Φt(sk
t )

t (sk
V ∗

t ) − (cid:15)A

(cid:18) H
h

(cid:19)

− n − 1

(cid:18) H
h
(cid:19)
.

− n

The ﬁrst relation is the deﬁnition of the update rule. The second relation holds by the monotonicity of
the max operator together with the induction assumption. The third relation as the extracted term
out of the max is constant. The forth relation holds by the deﬁnition of the max operation. The ﬁfth
relation by the Bellman equations V ∗

t satisﬁes (2), and the sixth relation by Assumption 1.

(ii) The second claim holds by construction of the update rule ¯V k
which enforces ¯V k

t ) ← min(cid:8) ¯V k
(s) for every updated state, and thus for all s and t.

t (sk

t (s) ≤ ¯V k−1

t

t (sk

t ), ¯V k−1

t

(sk

t )(cid:9)

(iii) The third claim holds since V k
itself and positive elements, as r(s, a) ≥ 0. Thus, it remains positive a.s. .

t (φt(s)) is initialized with positive elements and is updated by

Lemma 14. The expected cumulative value update at the k’th episode of h-RTDP-AA satisﬁes the
following relation:

¯V k
1 (φ(sk
K
(cid:88)

1)) − V πk
H
h −1
(cid:88)

(cid:88)

1 (sk
1)

≤

nh+1(sφ) − E[ ¯V k
¯V k−1

nh+1(sφ) | Fk−1].

k=1

n=1

sφ∈Sφ

Proof. Let n ∈ {0} ∪ [ H
h − 1] and let t = 1 + hn be a time step in which a value update is taking
place. By the deﬁnition of the update rule, the following holds for the update at the visited state sk
t :

¯V k
t (φt(sk

t )) ≤ E

(cid:34)t+h−1
(cid:88)

t(cid:48)=t
(cid:34)t+h−1
(cid:88)

= E

t(cid:48)=t

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (φt+h(sk

t+h)) | πk, sk
t

(cid:35)

(cid:35)

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (φt+h(sk

t+h)) | Fk−1, sk
t

where the last relation follows by the same argument as in (10).

Taking the conditional expectation w.r.t. Fk−1 and using the tower property we get,
(cid:34)t+h−1
(cid:88)

E(cid:2) ¯V k

t (φt(sk

t )) | Fk−1

(cid:3) ≤ E

r(sk

t(cid:48), ak

t(cid:48)) + ¯V k−1

t+h (φt+h(sk

t+h)) | Fk−1

(cid:35)

.

t(cid:48)=t

28

φ,t := φt(sk
Denote sk
expectation, and the fact ¯V k

H+1(φH+1(s)) = 0 for all s, k,

t ). Summing the above relation for all n ∈ {0} ∪ [ H

h − 1], using linearity of

H
h −1
(cid:88)

n=0

E(cid:2) ¯V k

1+nh(sk

φ,1+nh) | Fk−1

(cid:34) H
(cid:88)

(cid:3) ≤ E

t=1

r(sk

t , ak

t ) | Fk−1

(cid:35)

+

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k−1

1+nh(sk

φ,1+nh) | Fk−1

(cid:3)

⇐⇒ ¯V k

1 (sk

φ,1) +

⇐⇒ ¯V k

1 (sk

φ,1) +

H
h −1
(cid:88)

n=1

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k

1+nh(sk

φ,1+nh) | Fk−1

E(cid:2) ¯V k

1+nh(sk

φ,1+nh) | Fk−1

(cid:34) H
(cid:88)

(cid:3) ≤ E

t=1

r(sk

t , ak

t ) | Fk−1

(cid:35)

+

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k−1

1+nh(sk

φ,1+nh) | Fk−1

(cid:3)

(cid:3) ≤ V πk (sk

1) +

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k−1

1+nh(sk

φ,1+nh) | Fk−1

(cid:3)

⇐⇒ ¯V k

1 (sk

φ,1) − V πk (sk

1) ≤

H
h −1
(cid:88)

n=1

E(cid:2) ¯V k−1

1+nh(sk

φ,1+nh) − ¯V k

1+nh(sk

φ,1+nh) | Fk−1

(cid:3)

⇐⇒ ¯V k

1 (sk

φ,1) − V πk (sk

1) ≤

K
(cid:88)

H
h −1
(cid:88)

(cid:88)

k=1

n=1

sφ∈Sφ

nh+1(sφ) − E[ ¯V k
¯V k−1

nh+1(sφ) | Fk−1]

The second line holds by the fact sk

1 is measurable w.r.t. Fk−1, the third line holds since

(cid:34) H
(cid:88)

V πk
1 (sk

1) = E

t=1

r(sk

t , ak

t ) | Fk−1

(cid:35)

.

The ﬁfth line holds by Lemma 15 with ¯V k
to be Sφ. See that the update of ¯V k
state space. Furthermore, the update rule uses ¯V k−1
valid to apply the lemma.

t = gk

t occurs only at the visited state sk

t for t = nh + 1. Furthermore, we set ˜S of Lemma 15
φ,t = φ(sk
t ) of the abstracted
φ,t+1, i.e., it is measurable w.r.t. to Fk−1, and it is

Theorem 7 (Performance of h-RTDP-AA). Let (cid:15), δ > 0. The following holds for h-RTDP-AA:

1. With probability 1 − δ, for all K > 0, Regret(K) ≤ 9SφH(H−h)

h

ln(3/δ) + H(cid:15)A

h K.

2. Let ∆A = H(cid:15)A. Then, Pr

∃(cid:15) > 0 : N

(cid:110)

∆A
(cid:15) ≥ 9SφH(H−h) ln(3/δ)
h

(cid:111)

≤ δ.

h(cid:15)

Before supplying with the proof observe the following remark.

Proof. We start by proving claim (1). The following bounds on the regret hold.

Regret(K) :=

K
(cid:88)

k=1

1 (sk
V ∗

1) − V πk

1 (sk
1)

≤

≤

K
(cid:88)

k=1

K
(cid:88)

k=1

max
s∈Φ1(sk
1 )

1 (s) − V πk
V ∗

1 (sk
1)

¯V k
1 (φ1(sk

1)) − V πk

1 (sk

1) + (cid:15)A

H
h

≤ (cid:15)A

H
h

K +

K
(cid:88)

H
h −1
(cid:88)

(cid:88)

k=1

n=1

sφ∈Sφ

¯V k−1
nh+1(sφ) − E[ ¯V k

nh+1(sφ) | Fk−1]

(34)

The second relation holds the deﬁnition of the max operator and since sk
1) (by deﬁnition
we have that s ∈ Φt(s), as φt(s) = φt(s) for any t). The third relation holds by the approximate
optimism of the value function (Lemma 13), and the forth relation is by Lemma 14.

1 ∈ φ(sk

29

We now observe the regret is a regret of a Decreasing Bounded Process. Let

Xk :=

H
h −1
(cid:88)

(cid:88)

n=1

sφ∈Sφ

¯V k
nh+1(sφ),

(35)

and observe that {Xk}g≥0 is a Decreasing Bounded Process.

1. It is decreasing since for all sφ ∈ Sφ, t ¯V k

t (sφ) ≤ ¯V k−1

t

sum is also decreasing.

(sφ) by Lemma 13. Thus, their

2. It is bounded since for all s ∈ Sφ, t ¯V k

t (sφ) ≥ 0 by Lemma 13. Thus, the sum is bounded

from below by 0.

See that the initial value can be bounded as follows,

X0 =

H
h −1
(cid:88)

(cid:88)

n=1

sφ∈Sφ

¯V 0
nh+1(sφ) ≤

H
h −1
(cid:88)

(cid:88)

n=1

sφ∈Sφ

H =

SφH(H − h)
h

.

Using linearity of expectation and the deﬁnition (14) we observe that (34) can be written,

Regret(K) ≤ (34) = (cid:15)A

H
h

K +

K
(cid:88)

k=1

Xk−1 − E[Xk | Fk−1],

which is regret of A Bounded Decreasing Process. Applying the bound on the regret of a DRP,
Theorem 1, we conclude the proof of the ﬁrst claim.

We now prove claim (2) using the proving technique at Theorem 4. Denote ∆A = H(cid:15)A. The
following relations hold for all (cid:15) > 0.

1 (φ1(sk

1(cid:8) ¯V k
≤ 1(cid:8) ¯V k

1)) − V πk

1 (sk
1)) − V πk

1) ≥ (cid:15)(cid:9)(cid:15)
1 (sk

1 (φ1(sk

1) ≥ (cid:15)(cid:9)(cid:0) ¯V k

≤ 1(cid:8) ¯V k

1 (φ1(sk

1)) − V πk

1 (sk

1) ≥ (cid:15)(cid:9)





1 (φ1(sk
H
h −1
(cid:88)

(cid:88)

1)) − V πk

1 (sk

1)(cid:1)

nh+1(sφ) − E[ ¯V k
¯V k−1

nh+1(sφ) | Fk−1]





= 1(cid:8) ¯V k

1 (sk

1) − V πk

1 (sk

1) ≥ (cid:15)(cid:9)(Xk−1 − E[Xk | Fk−1]).

n=1

sφ∈Sφ

(36)

The ﬁrst relation holds by the indicator function and the second relation holds by Lemma 14. The
forth relation holds by the deﬁnition of Xk (35) and linearity of expectation.
As we wish the ﬁnal performance to be compared to V ∗ we use the the ﬁrst claim of Lemma 13, by
which for all s, k, ¯V k

1 (φ1(s)) ≥ V ∗

h . This implies that

1 (s) − ∆A

(cid:26)

1

1 (sk
V ∗

1) − V πk

1 (sk

1) ≥

(cid:27)

+ (cid:15)

∆A
h

Combining the above we get

≤ 1(cid:8) ¯V k

1 (φ1(sk

1)) − V πk

1 (sk

1) ≥ (cid:15)(cid:9).

(cid:26)

1

1 (sk
V ∗
≤ 1(cid:8) ¯V k
≤ 1(cid:8) ¯V k

1) − V πk

1 (sk

1) ≥

(cid:27)

(cid:15)

+ (cid:15)

∆A
h
1) ≥ (cid:15)(cid:9)(cid:15)
1 (sk
1) ≥ (cid:15)(cid:9)(Xk−1 − E[Xk | Fk−1]).
1 (sk

1)) − V πk
1)) − V πk

1 (φ1(sk
1 (φ1(sk
The ﬁrst relation is by (37) and the second relation by (36).
1(cid:8)V ∗
Deﬁne N(cid:15)(K) = (cid:80)K
V πk
1) ≥ ∆A
1 (sk

1) ≥ ∆A

1 (sk

1 (sk

1) −
h + (cid:15) at the ﬁrst K episodes. Summing the above inequality (38) for all k ∈ [K] and

h + (cid:15)(cid:9) as the number of times V ∗

1) − V πk

1 (sk

k=1

(37)

(38)

30

denote we get that for all (cid:15) > 0

N(cid:15)(K)(cid:15) =

K
(cid:88)

(cid:26)

1

k=1

1 (sk
V ∗

1) − V πk

1 (sk

1) ≥

(cid:27)

+ (cid:15)

(cid:15)

∆A
h

≤

≤

K
(cid:88)

k=1

K
(cid:88)

k=1

1(cid:8) ¯V k

1 (φ1(sk

1)) − V πk

1 (sk

1) ≥ (cid:15)(cid:9)(Xk−1 − E[Xk | Fk−1])

Xk−1 − E[Xk | Fk−1].

The ﬁrst relation holds by deﬁnition, the second by (38) and the third relation holds as {Xk}k≥0 is a
DBP (35) and, thus, Xk−1 − E[Xk | Fk−1] ≥ 0 a.s. . Thus, the following relation holds
(cid:40)

(cid:41)

K
(cid:88)

∀K > 0 :

Xk−1 − E[Xk | Fk−1] ≤

9SH(H − h)
h

ln

3
δ

(cid:26)

⊆

∀(cid:15) > 0 : N(cid:15)(K)(cid:15) ≤

9SH(H − h)
h

ln

(cid:27)

,

3
δ

k=1

from which we get that for any K > 0

(cid:18)

Pr

∀(cid:15) > 0 : N(cid:15)(K)(cid:15) ≤

9SH(H − h)
h

ln

(cid:19)

3
δ

(cid:32)

≥ Pr

∀K > 0 :

K
(cid:88)

k=1

Xk−1 − E[Xk | Fk−1] ≤

(cid:33)

9SH(Hh)
h

ln

3
δ

≥ 1 − δ,

and the third relation holds the bound on the regret of DBP, Theorem 1. Equivalently, for any K > 0,

(cid:18)

Pr

∃(cid:15) > 0 : N(cid:15)(K)(cid:15) ≥

9SH(H − h)
h

ln

(cid:19)

3
δ

≤ δ.

(39)

Applying the Monotone Convergence Theorem as in the proof of Theorem 4 we conclude the proof.

31

15 Useful Lemmas

The following lemma is a generalization of Lemma 34 in [15].
Lemma 15 (On Trajectory Regret to Uniform Regret). For any t ∈ [H], let (cid:8)sk
random process where (cid:8)sk
set of all possible realizations of sk
entry of the vector as gk
which is Fk−1 measurable, i.e.,

k≥0 is adapted to the ﬁltration {Fk}k≥0 and sk
t with cardinally ˜S := | ˜S|. Let gk

k≥0 be a
t , Fk
t ∈ ˜S where ˜S is a ﬁnite
t ∈ R ˜S and denoting the s ∈ ˜S
t by an update rule

t (s) be updated only at the state sk

t (s). Furthermore, let gk

(cid:9)

(cid:9)

t

gk
t (s) =

(cid:26)f k−1
t
gk−1
t

(s), if s = sk
t ,
(s), o.w..

Where f k−1

t

(s) is an update rule Fk−1 measurable. Then,

K
(cid:88)

k=1

E[gk−1
t

(sk

t ) − gk

t (sk

t ) | Fk−1] =

K
(cid:88)

(cid:88)

k=1

s∈ ˜S

gk−1
t

(s) − E[gk

t (s) | Fk−1]

Proof. The following relations hold.

K
(cid:88)

H
(cid:88)

k=1

t=1

E[gk−1
t

(sk

t ) − gk

t (sk

t ) | Fk−1]

=

(1)
=

(2)
=

(3)
=

(4)
=

K
(cid:88)

H
(cid:88)

(cid:88)

E[1(cid:8)s = sk

t

(cid:9)gk−1

t

(s) − 1(cid:8)s = sk

t

(cid:9)gk

t (s) | Fk−1]

k=1

t=1

s∈ ˜S

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

H
(cid:88)

(cid:88)

s∈ ˜S
K
(cid:88)

t=1

s∈ ˜S

k=1

E[1(cid:8)s = sk

t

(cid:9)gk−1

t

(s) − 1(cid:8)s = sk

t

(cid:9)f k−1

t

(s) | Fk−1]

E[1(cid:8)s = sk

t

(cid:9)gk−1

t

(s) + 1(cid:8)s (cid:54)= sk

t

(cid:9)gk−1

t

(s) | Fk−1]

− E[1(cid:8)s = sk

t

(cid:9)f k−1

t

(s) + 1(cid:8)s (cid:54)= sk

t

(cid:9)gk−1

t

(s) | Fk−1]

H
(cid:88)

(cid:88)

K
(cid:88)

t=1

s∈ ˜S

k=1

H
(cid:88)

(cid:88)

K
(cid:88)

t=1

s∈ ˜S

k=1

gk−1
t

(s) − E[1(cid:8)s = sk

t

(cid:9)f k−1

t

(s) + 1(cid:8)s (cid:54)= sk

t

(cid:9)gk−1

t

(s) | Fk−1]

gk−1
t

(s) − E[gk

t (s) | Fk−1].

(40)

Relation (1) holds since for s = sk
adding and subtracting 1(cid:8)s (cid:54)= sk
for any event 1{A} + 1{Ac} = 1 and since gk−1
the update rule,

t the vector gt
(cid:9)gk−1

t

t

t

k is updated according by f k−1. Relation (2) holds by
(s) while using the linearity of expectation. (3) holds since
is Fk−1 measurable. (4) holds by the deﬁnition of

(cid:9)f k−1

(s) + 1(cid:8)s (cid:54)= sk

t

t

E[1(cid:8)s = sk
= E[1(cid:8)s = sk
= Pr(sk

(cid:9) | Fk−1]f k−1

t

t
t = s | Fk−1)f k−1

t

(s) + Pr(sk

t

(cid:9)gk−1
(s) + E[1(cid:8)s (cid:54)= sk

t

(s) | Fk−1]

t
t (cid:54)= s | Fk−1)gk−1

t

(cid:9) | Fk−1]gk−1

t

(s)

(s) = E[gk

t (s) | Fk−1].

Where we used that gk−1
in the ﬁrst relation.

t

(s) is Fk−1 measurable and the assumption that f k−1

t

(s) is Fk−1 measurable

The following lemma is a variant of a well known error propagation analysis in case of an approximate
model.

32

Lemma 16 (Model Error Propagation). Let (cid:107)(P (· | s, a) − ˆP (· | s, a))(cid:107) ≤ (cid:15)P for any s, a. Then,
for any policy π,

∀s1 ∈ S,

(cid:88)

sn

(cid:12)
(cid:12)
(cid:12)P π(sn | s1) − ˆP π(sn | s1)
(cid:12)
(cid:12)
(cid:12) ≤ n(cid:15)P

Proof. We prove the claim by induction. For the base case n = 1 we get that for any s1 ∈ S

(cid:12)
(cid:88)
(cid:12)P π(s2 | s1) − ˆP π(s2 | s1)
(cid:12)

s2

(cid:12)
(cid:12)
(cid:12)

π(a | s1)

(cid:16)

(cid:17)
P (s2 | s1, a) − ˆP π(s2 | s1, a)

(cid:88)

a

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

π(a | s1)

(cid:12)
(cid:88)
(cid:12)P (s2 | s1, a) − ˆP π(s2 | s1, a)
(cid:12)

s2

(cid:12)
(cid:12)
(cid:12)

π(a | s1)(cid:107)P (· | s1, a) − P (· | s1, a)(cid:107)1 ≤ (cid:15)P .

=

≤

=

(cid:12)
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)
s2
(cid:88)

a
(cid:88)

a

Assume the induction step, i.e., assume the claim holds for k = n − 1. We now prove the induction
step, i.e., for k = n

(cid:12)
(cid:12)P π(sn | s1) − ˆP π(sn | s1)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:88)

sn

=

≤

≤

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
s2
(cid:88)

(cid:88)

sn
(cid:88)

sn
(cid:88)

s2
(cid:88)

sn

s2

P π(sn | s2)P π(s2 | s1) − ˆP π(sn | s2) ˆP π(s2 | s1)

(cid:12)
(cid:12)P π(sn | s2)P π(s2 | s1) − ˆP π(sn | s2) ˆP π(s2 | s1)
(cid:12)
(cid:12)
(cid:12)P π(sn | s2)P π(s2 | s1) − ˆP π(sn | s2)P π(s2 | s1)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆP π(sn | s2) ˆP π(s2 | s1) − ˆP π(sn | s2)P π(s2 | s1)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:88)

≤

(cid:88)

sn

s2

P π(s2 | s1)

(cid:12)
(cid:12)P π(sn | s2) − ˆP π(sn | s2)
(cid:12)
(cid:12)
+ ˆP π(sn | s2)
(cid:12)
(cid:12)

ˆP π(s2 | s1) − P π(s2 | s1)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:88)

≤

s2
(cid:124)

(cid:32)

P π(s2 | s1)

max
s(cid:48)
2

(cid:12)
(cid:88)
(cid:12)P π(sn | s(cid:48)
(cid:12)

(cid:12)
2) − ˆP π(sn | s(cid:48)
(cid:12)
2)
(cid:12)

sn

(cid:33)

(cid:123)(cid:122)
=1

(cid:88)

+

(cid:125)
(cid:32)

(cid:88)

s2

sn

ˆP π(sn | s2)

(cid:33)
(cid:12)
(cid:12)
(cid:12)

ˆP π(s2 | s1) − P π(s2 | s1)

(cid:12)
(cid:12)
(cid:12)

(cid:124)

(cid:123)(cid:122)
=1
(cid:12)
(cid:12)P π(sn | s2) − ˆP π(sn | s2)
(cid:12)

(cid:125)

= max

s2

(cid:88)

sn

(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:88)
(cid:12)
(cid:12)

s2

ˆP π(s2 | s1) − P π(s2 | s1)

(cid:12)
(cid:12)
(cid:12).

By the induction hypothesis and the base case,

(cid:12)
(cid:88)
(cid:12)P π(sn | s(cid:48)
(cid:12)

(cid:12)
2) − ˆP π(sn | s(cid:48)
(cid:12)
(cid:12) ≤ (cid:15)(n − 1)
2)

sn
ˆP π(s2 | s1) − P π(s2 | s1)

(cid:12)
(cid:12)
(cid:12) ≤ (cid:15)P ,

max
s(cid:48)
2

(cid:12)
(cid:88)
(cid:12)
(cid:12)

s2

33

from which we prove the induction step,

∀s1 ∈ S, (cid:107)P π(· | s1)1 − ˆP π(· | s1)(cid:107) =

(cid:12)
(cid:12)
(cid:88)
(cid:12)P π(sn | s1) − ˆP π(sn | s1)
(cid:12)
(cid:12)
(cid:12) ≤ n(cid:15)P .

sn

Lemma 17. Let V ∗
t (s), ˆV π
V π

t (s), ˆV ∗

t (s) be the optimal values on the MDP M, ˆM, respectively, and let

t (s) be the value of a ﬁxed policy π on the MDP M, ˆM, respectively. Then,

i) ||V ∗

1 − ˆV ∗

1 ||∞ ≤

ii) ∀π, (cid:107)V π

1 − ˆV π

1 (cid:107)∞ ≤

(cid:15)P ,

H(H − 1)
2
H(H − 1)
2

(cid:15)P .

Proof. Both claims follow standard techniques based on the Simulation Lemma [20, 30].

(i) Let ∆t(s) := ˆV ∗
∆H (s) = max

t (s) − V ∗
(cid:88)

r(s, a) +

a

t (s), ∆t = maxs|∆t(s)|. For t = H we have that for all s
P (s(cid:48) | s, a)V ∗

ˆP (s(cid:48) | s, a) ˆV ∗

H+1(s(cid:48)) − max

r(s, a) +

(cid:88)

H+1(s(cid:48))

a

= max

a

r(s, a) +

s(cid:48)
(cid:88)

s(cid:48)

ˆP (s(cid:48) | s, a) · 0 − max

a

r(s, a) +

(cid:88)

s(cid:48)

s(cid:48)

P (s(cid:48) | s, a) · 0 = 0,

(41)

and the base case holds. Assume the claim holds for any t ≥ k + 1, we now prove it holds for t = k.
The following relations hold for any s,

∆t(s) = max

a

r(s, a) +

(cid:88)

ˆP (s(cid:48) | s, a) ˆV ∗

t+1(s(cid:48)) − max

≤ r(s, a∗) +

(cid:88)

s(cid:48)
ˆP (s(cid:48) | s, a∗) ˆV ∗

t+1(s(cid:48)) − r(s, a∗) +

r(s, a) +

(cid:88)

s(cid:48)

P (s(cid:48) | s, a)v∗

t+1(s(cid:48))

a

P (s(cid:48) | s, a∗)V ∗

t+1(s(cid:48))

(cid:88)

s(cid:48)

s(cid:48)
ˆP (s(cid:48) | s, a∗) ˆV ∗

t+1(s(cid:48)) − P (s(cid:48) | s, a∗)V ∗

t+1(s(cid:48))

ˆP (s(cid:48) | s, a∗)

(cid:12)
ˆV ∗
t+1(s(cid:48)) − V ∗
(cid:12)
(cid:12)
(cid:124)

(cid:123)(cid:122)
:=∆t+1(s(cid:48))

(cid:12)
t+1(s(cid:48))
(cid:12)
(cid:12)
(cid:125)

(cid:12)
(cid:12)
(cid:12)P (s(cid:48) | s, a∗) − ˆP (s(cid:48) | s, a∗)
(cid:12)V ∗
(cid:12)
(cid:12)

t+1(s(cid:48))

+

ˆP (s(cid:48) | s, a∗)|∆t+1(s(cid:48))| + (H − t)(cid:15)P

=

≤

(cid:88)

s(cid:48)
(cid:88)

s(cid:48)

(cid:88)

≤

s(cid:48)

≤ ∆t+1

(cid:88)

s(cid:48)

ˆP (s(cid:48) | s, a∗) + (H − t)(cid:15)P = ∆t+1 + (H − t)(cid:15)P

The second relation holds by choosing a∗ to maximize the ﬁrst term ﬁrst. The forth relation by
adding and subtracting ˆP (s(cid:48) | s, a∗) ˆV ∗
t+1(s(cid:48)) and standard inequalities. The ﬁfth relation by the fact
t+1(s) ≤ H − t and the assumption that for all s, a (cid:107)P (· | s, a) − ˆP (· | s, a)(cid:107) ≤ (cid:15)P . The sixth by
V ∗
the fact ˆP (· | s, a) is a probability distribution and thus sums to 1.
Lower bounding ∆t(s) using similar technique with opposite inequalities yields,

and thus,

∆t(s) ≥ −(∆t+1 + (H − t)(cid:15)P ),

|∆t(s)| ≤ ∆t+1 + (H − t)(cid:15)P .

As the above holds for any s it holds for the maximizer. Thus,
∆t ≤ ∆t+1 + (H − t)(cid:15)P .

Iterating on this relation while using ∆H (s) = 0 by (41),

||V ∗

1 − ˆV ∗

1 ||∞ = ∆1 ≤

H
(cid:88)

(H − t)(cid:15)P = (cid:15)P

t=1

H−1
(cid:88)

t=1

t =

H(H − 1)
2

(cid:15)P .

34

(ii) The proof of the second claim follows the same proof of the ﬁrst claim, without while replacing
the max operator with the a ﬁxed policy π.
Lemma 18 (Total Contribution of Approximate Model Errors). Let dn := − 1
n)h(cid:15)P . Then,

2 (h − 1)h(cid:15)P + (H −

H
h −1
(cid:88)

n=0

d1+nh =

1
2

H(H − 1)(cid:15)P .

Proof. The following relations hold.

H
h −1
(cid:88)

n=0

d1+nh = −

= −

= −

= −

= −

1
2

1
2

1
2
1
2
1
2

H(h − 1)(cid:15)P +

H
h −1
(cid:88)

(H − 1 − nh)h(cid:15)P

n=0

H(h − 1)(cid:15)P + H(H − 1)(cid:15)P − h2(cid:15)P

H
h −1
(cid:88)

n=0

n

H(h − 1)(cid:15)P + H(H − 1)(cid:15)P −

H(h − 1)(cid:15)P + H(H − 1)(cid:15)P −

h2(cid:15)P (

H − h
h

)

H
h

H(H − h)(cid:15)P

H(H − 1)(cid:15)P + H(H − 1)(cid:15)P =

H(H − 1)(cid:15)P .

1
2
1
2
1
2

35

16 Approximate Dynamic Programming in Finite-Horizon MDPs

Algorithm 9 (Exact) h-DP
init: ∀s ∈ S, ∀n ∈ [ H
H − nh
for n = H

for s ∈ S do

h − 1, H

h − 2, . . . , 1 do
Vnh+1(s) = (cid:0)T hV(n+1)h+1

(cid:1)(s)

h ], Vnh+1(s) =

end for

end for
return: {Vnh+1}H/h
n=1

Algorithm 10 h-DP with Approximate
Model

h ], Vnh+1(s) =

init: ∀s ∈ S, ∀n ∈ [ H
H − nh
for n = H

h − 1, H

for s ∈ S do

Vnh+1(s) =

end for

end for
return: {Vnh+1}H/h
n=1

h − 2, . . . , 1 do
(cid:16) ˆT hV(n+1)h+1

(cid:17)

(s)

In this section, we follow standard analysis [20, 30] and establish bounds on the performance of
approximate DP algorithms which update by an h-step optimal Bellman operator (2). We abbreviate
this class of algorithms by h-ADP. See that unlike previous analysis [20, 30], we focus on ﬁnite
horizon MDPs, which is the setup in which h-RTDP is analyzed. The different approximation
setting we analyze in this section corresponds to the ones anlayzed for h-RTDP: approximate model,
approximate value update, and approximate state abstraction.

As a reminder and for the sake of completeness, we start by considering h-DP Algorithm 9, which is
the exact, approximate-free, version of the following h-ADP algorithms. The algorithm uses backward
induction and a h-step optimal Bellman operator T h by which it outputs the values {Vnh+1}
n=2.
(cid:9) H
Notice that it holds {Vnh+1}
n=2 by standard arguments on the Backward Induction
algorithm. Furthermore, T h can be solved by Backward induction with the total computational
complexity of O(SAh) by using Backward Induction. Thus, the total computational complexity of
h-DP is O(SAH) similar to the one of standard DP, e.g., Backward Induction.

n=2 = (cid:8)V ∗

nh+1

H
h

H
h

h

H
h

In terms of space complexity, h-DP stores in memory O(S H

h ) value entries. Observe that an h-greedy
policy (3) w.r.t. {Vnh+1}
n=2 is an optimal policy, as these values are the optimal values as previously
observed. Ultimately, one would like using these values to act in the environment by the optimal
policy. If one uses the Forward-Backward DP (Section 10) to calculate such an h-greedy policy, then
an extra O(hSh) space should be used, which results in total O(S H
h + hSh) space complexity (as
in h-RTDP) that decrease in h if Sh is not too big (see Remark 2). Furthermore, the computational
complexity of such approach is O(HhAShS1) which increases in h.

In next sections, we consider approximate settings of h-DP and establish that an h-greedy policy (3)
w.r.t. the output values {Vnh+1}
n=2 has an equivalent performance to the asymptotic policy by which
h-RTDP acts.

H
h

16.1 h-ADP with an Approximate Model

In the case of an approximate model, we replace the Bellman operator T used in h-DP with ˆT , the
Bellman operator of the approximate model ˆp instead the true one p (we assume r is exactly known,
which correspond to the assumption made in Section 6.1). This results in Algorithm 10. Similarly
to Section 6.1, we assume (cid:107)ˆp(· | s, a) − p(· | s, a)(cid:107)T V ≤ (cid:15)P , for all (s, a) ∈ S × A. Furthermore,
denote π∗

P as the optimal policy of the approximate MDP.

Equivalently to h-DP, Algorithm 10 returns the optimal values of the approximate model
(Algorithm 10 can be interpreted as exact h-DP applied on the approximate model). Thus, the
h-greedy policy w.r.t.
n=2 is the optimal policy of the
P . The performance of π∗
approximate MDP, π∗
P is measured by relatively to the performance of
π∗
the optimal policy, i.e., we wish to bound (cid:107)V ∗
p
1 (cid:107)∞. This term represents the performance
1 − V
gap between the optimal policy of the ‘real’ MDP to the performance of the optimal policy of the
approximate MDP evaluated on the real MDP, and is bounded in the following proposition.

to the outputs of Algorithm 10 {Vnh+1}

H
h

36

Proposition 19. Assume for all (s, a) ∈ S × A : (cid:107)ˆp(· | s, a) − p(· | s, a)(cid:107)T V ≤ (cid:15)P and let π∗
the optimal policy of the approximate MDP. Then,

P be

(cid:107)V ∗

1 − V π∗

P

1 (cid:107)∞ ≤ H(H − 1)(cid:15)P .

Proof. Let ˆV π∗
ﬁrst and second claim of Lemma 17 we conclude the proof,

P be the optimal value on the approximate MDP. By using the triangle inequality, the

(cid:107)V ∗

1 − V π∗

P

1 (cid:107)∞ ≤ (cid:107)V ∗

1 − ˆV π∗

1 (cid:107)∞ + (cid:107) ˆV π∗

1 − V π∗

P

P

P

1 (cid:107)∞ ≤ H(H − 1)(cid:15)P .

Algorithm 11 h-DP with Approximate
Value Updates

h ], Vnh+1(s) =

init: ∀s ∈ S, ∀n ∈ [ H
H − nh
for n = H

h − 1, H

for s ∈ S do

h − 2, . . . , 1 do
t (s) = (cid:15)V (s) + (cid:0)T hVt+h
¯V k
end for

(cid:1)(s)

end for
return: {Vnh+1}H/h
n=1

Algorithm 12 h-DP with Approximate State
abstraction

init: ∀s ∈ S, ∀n ∈ [ H
for n = H

for s ∈ S do

h − 1, H
¯Vnh+1(φ(s))
min(cid:8)(cid:0)T hV(n+1)h+1

end for

h ], Vnh+1(s) = H − nh

h − 2, . . . , 1 do

(cid:1)(s), ¯V k

nh+1(φ(s))(cid:9)

=

end for
return: {Vnh+1}H/h
n=1

16.2 h-DP with Approximate Value Updates

In the case of a approximate value updates Algorithm 9 is replaced by an value updates with added
noise (cid:15)V (s), by which Algorithm 11 is formulated. Similarly to the assumption used for h-RTDP
with approximate value updates (see Section 6.2) we assume for all s ∈ S, |(cid:15)V (s)| ≤ (cid:15)V > 0. The
following proposition bounds the performance of an h-greedy policy w.r.t. the values output by
Algorithm 11.

Proposition 20. Assume for all s ∈ S, |(cid:15)V (s)| ≤ (cid:15)V . Let π∗
Algorithm 11. Then,

V be the h-greedy policy (3) w.r.t. output

(cid:107)V ∗

1 − V π∗

V

1 (cid:107)∞ ≤

2H
h

(cid:15)V .

(cid:110) ˆV ∗

(cid:111)H/h

Proof. Let
similarity to the two claims of Lemma 17. Combining the two we prove the result.

denote the output of Algorithm 11. We establish two claims which are of

nh+1

n=1

37

(i) The following relations hold for all s ∈ S and n ∈ {0} ∪ [ H
∆1+nh(s) := ˆV ∗
= (cid:15)V (s) + T h ˆV ∗

1+(1+n)h(s(cid:48))

h − 1].

1+nh(s) − V ∗
1+nh(s)
1+(n+1)h(s) − T hV ∗
(cid:34)h−1
(cid:88)

E

r(st(cid:48), at(cid:48)(st(cid:48))) + ˆV ∗

(cid:35)
t+h(sh) | s0 = s

= (cid:15)V (s) + max

a0,...,ah−1

t(cid:48)=0

− max

a0,...,ah−1

(cid:34)h−1
(cid:88)

E

t(cid:48)=0

r(st(cid:48), at(cid:48)(st(cid:48))) + V ∗

(cid:35)
t+h(sh) | s0 = s

(42)

The second relation holds by the updating equation and the third relation by deﬁnition (2). Let
{ˆa0, ˆa1, .., ˆah−1} be the set of policies maximizes the second terms, then, by plugging this sequence
to the third term we necessarily decrease it. Thus,
(cid:34)h−1
(cid:88)

(cid:35)

r(st(cid:48), at(cid:48)(st(cid:48))) + ˆV ∗

t+h(sh) | s0 = s, {at(cid:48)}h−1

t(cid:48)=0 = {ˆat(cid:48)}h−1

t(cid:48)=0

(42) ≤ (cid:15)V (s) + E

(cid:34)h−1
(cid:88)

− E

t(cid:48)=0

(cid:35)

r(st(cid:48), at(cid:48)(st(cid:48))) + V ∗

t+h(sh) | s0 = s, {at(cid:48)}h−1

t(cid:48)=0 = {ˆat(cid:48)}h−1

t(cid:48)=0

t(cid:48)=0
(cid:104) ˆV ∗
= (cid:15)V (s) + E
(cid:104)
= (cid:15)V (s) + E

t+h(sh) − V ∗

t+h(sh) | s0 = s, {at(cid:48)}h−1

(cid:105)

t(cid:48)=0 = {ˆat(cid:48)}h−1
(cid:105)

t(cid:48)=0

∆1+(n+1)h(s) | s0 = s, {at(cid:48)}h−1

t(cid:48)=0 = {ˆat(cid:48)}h−1

t(cid:48)=0

≤ (cid:15)V + (cid:107)∆1+(n+1)h(cid:107)∞.

The second relation holds by linearity of expectation, the third relation by deﬁnition, and the forth by
assumption on (cid:15)V (s) and by the standard bounded E[X] ≤ (cid:107)X(cid:107)∞.

Repeating the above arguments while choosing the sequence which maximizes the third term in (42)
allows us to lower bound (42) as follows

and thus,

(42) ≥ −(cid:15)V − (cid:107)∆1+(n+1)h(cid:107)∞,

(cid:107)∆1+nh(cid:107)∞ ≤ (cid:15)V + (cid:107)∆1+(n+1)h(cid:107)∞.

Solving the recursion while using ∆H+1(s) = 0 for all s ∈ S we get

(cid:107)∆1(cid:107)∞ ≤

H
h

(cid:15)V .

(43)

(ii) The following relations hold for all s ∈ S and n ∈ [ H

h ].

∆π∗

V

1+nh(s) := ˆV ∗

1+nh(s) − V π∗

V

1+nh(s)

= (cid:15)V (s) + max

a0,..,ah−1

(cid:34)h−1
(cid:88)

E

t(cid:48)=0

r(st(cid:48), at(cid:48)(st(cid:48))) + ˆV ∗

(cid:35)
1+(n+1)h(sh) | s0 = s

(cid:34)h−1
(cid:88)

− E

t(cid:48)=0

r(st(cid:48), at(cid:48)(st(cid:48))) + V π∗

V

1+(n+1)h(sh) | s0 = s, π∗

V

(cid:35)

.

By deﬁnition, the sequence which maximizes the second term is π∗
ˆV ∗. Using the linearity of expectation we get

V as it is the h-greedy policy w.r.t.

∆π∗

V

(cid:104) ˆV ∗
1+(n+1)h(sh) − V π∗
1+nh(s) = (cid:15)V (s) + E
(cid:104)
∆π∗
= (cid:15)V (s) + E
1+(n+1)h(s1+(n+1)h) | s0 = s, π∗

V

V

V

1+(n+1)h(sh) | s0 = s, π∗

V

(cid:105)
.

(cid:105)

As for all s, |(cid:15)V (s)| ≤ (cid:15)V , using the triangle inequality and E[X] ≤ (cid:107)X(cid:107)∞ we get the following
recursion,

(cid:107)∆π∗

1+nh(cid:107)∞ ≤ (cid:15)V + (cid:107)∆π∗

V

V

1+(n+1)h(cid:107)∞.

38

Using (cid:107)∆π∗

V

1+H (cid:107)∞ = 0 we arrive to its solution,

(cid:107)∆π∗

V

1 (cid:107)∞ ≤

H
h

(cid:15)V .

(44)

which proves the second needed result.

Finally, using the triangle inequality and the two proven claims, (43) and (44), we conclude the proof.

(cid:107)V ∗

1 − V π∗

V

1 (cid:107)∞ ≤ (cid:107)V ∗

1 − ˆV1(cid:107)∞ + (cid:107) ˆV ∗

1 − V π∗

1 (cid:107)∞ = (cid:107)∆1(cid:107)∞ + (cid:107)∆π∗

V

V

1 (cid:107)∞ ≤ 2

H
h

(cid:15)V .

16.3 h-DP with Approximate State Abstraction

H
h −1
When an approximate state abstraction {φ1+nh}
n=0 is given, Algorithm 9 can be replaced by an
exact value update in the reduced state space Sφ, as given in Algorithm 12. This corresponds to
updating a value V ∈ RSφ, instead a value RS. An obvious advantage of such an algorithm, relatively
to h-DP, is its reduced space complexity, as it only needs to store O( H
h Sφ) value entries, instead of
O( H

h S) as h-DP.

Yet, as seen in Algorithm 12, its computational complexity remains O(SAH) as it needs to uniformly
update on the entire (non-abstracted) state space. Would have we being given a representative from
each equivalence classes under φ1+nh for every n ∈ {0} ∪ [ H
h ]8 we could suggest an alternative
Backward Induction algorithm with computational complexity of O(SφAH). However, as we do
not assume access to this knowledge, we are obliged to scan the entire state space, without further
assumptions.

The following proposition bounds the performance of an h-greedy policy w.r.t. the values output by
Algorithm 12. Similarly to the analysis of the corresponding h-RTDP algorithm (see Section 6.3), we
assume {φ1+nh}

H
h −1
n=0 satisfy Assumption 1.

Proposition 21. Let {φ1+nh}
Algorithm 12 and let π∗

H
h −1
n=0 satisfy Assumption 1. Let

(cid:110) ˆV ∗

nh+1

(cid:111) H

h

denote the output of

n=1

A be the h-greedy policy w.r.t. these approximate values (3). Then,
H
h

1 − V π∗

1 (cid:107)∞ ≤

(cid:107)V ∗

(cid:15)A.

A

Proof. We establish two claims which are of similarily to the two claims of Lemma 17 and
Proposition 20. Combining the two we prove the result.

(i) The following relations hold for any s ∈ S.

ˆV ∗
1+nh(φ1+nh(s)) − V ∗
= T h ˆV ∗

1+nh(s)
φ,1+(n+1)h(s) − T hV ∗

1+(1+n)h(s)

(cid:34)h−1
(cid:88)

E

r(st(cid:48), at(cid:48)(st(cid:48))) + ˆV ∗

(cid:35)
1+(n+1)h(φ1+(n+1)h(sh)) | s0 = s

= max

a0,..,ah−1

t(cid:48)=0
(cid:34)h−1
(cid:88)

E

t(cid:48)=0

r(st(cid:48), at(cid:48)(st(cid:48))) + V ∗

(cid:35)
1+(n+1)h(sh) | s0 = s

(45)

− max

a0,..,ah−1

The second and third relation holds by the updating rule of Algorithm 12. Let {ˆa0, ˆa1, .., ˆah−1} be
the set of policies which maximizes the ﬁrst term. Then, by plugging this sequence to the second

8Differently put, if we interpret φ as clustering multiple states s ∈ S together, we would require a single

representative from each such a cluster.

39

term we necessarily decrease it, and the following holds.

(cid:34)h−1
(cid:88)

(45) ≤ E

t(cid:48)=0

r(st(cid:48), at(cid:48)(st(cid:48))) + ˆV ∗

1+(n+1)h(φ1+(n+1)h(sh)) | s0 = s, {at(cid:48)}h−1

t(cid:48)=0 = {ˆat(cid:48)}h−1

t(cid:48)=0

(cid:35)

(cid:34)h−1
(cid:88)

− E

r(st(cid:48), at(cid:48)(st(cid:48))) + V ∗

1+(n+1)h(sh) | s0 = s, {at(cid:48)}h−1

t(cid:48)=0 = {ˆat(cid:48)}h−1

t(cid:48)=0

(cid:35)

t(cid:48)=0
1+(n+1)h(φ1+(n+1)h(sh)) − V ∗

(cid:104) ˆV ∗

= E

1+(n+1)h(sh) | s0 = s, {at(cid:48)}h−1

t(cid:48)=0 = {ˆat(cid:48)}h−1

t(cid:48)=0

(cid:105)

(46)

Where the second relation holds by linearity of expectation. By Assumption 1 the following inequality
holds,

(cid:20)
ˆV ∗
(46) ≤ E
1+(n+1)h(φ1+(n+1)h(sh)) −

max
¯sh∈Φ1+(n+1)h(sh)

1+(n+1)h(¯sh) + (cid:15)A | s0 = s, {at(cid:48)}h−1
V ∗

t(cid:48)=0 = {ˆat(cid:48)}h−1

t(cid:48)=0

(cid:20)
ˆV ∗
= (cid:15)A + E
1+(n+1)h(φ1+(n+1)h(sh)) −

max
¯sh∈Φ1+(n+1)h(sh)

1+(n+1)h(¯sh) | s0 = s, {at(cid:48)}h−1
V ∗

t(cid:48)=0 = {ˆat(cid:48)}h−1

t(cid:48)=0

≤ (cid:15)A + max

s

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˆV ∗
1+(n+1)h(φ1+(n+1)h(s)) −

max
¯s∈Φ1+(n+1)h(s)

(cid:12)
(cid:12)
V ∗
1+(n+1)h(¯s)
(cid:12)
(cid:12)

(47)

(cid:21)

(cid:21)

By choosing the sequence of polices which maximizes the second term in (45) and repeating similar
arguments to the above we arrive to the following relations.
(45) ≥ E
(cid:20)
ˆV ∗
≥ E
1+(n+1)h(φ1+(n+1)h(sh)) −

1+(n+1)h(sh) | s0 = s, {at(cid:48)}h−1

1+(n+1)h(φ1+(n+1)h(sh)) − V ∗

t(cid:48)=0 = {ˆat(cid:48)}h−1

t(cid:48)=0 = {ˆat(cid:48)}h−1

(cid:104) ˆV ∗

t(cid:48)=0

t(cid:48)=0

(cid:105)

max
¯sh∈Φ1+(n+1)h(sh)

1+(n+1)h(¯sh) | s0 = s, {at(cid:48)}h−1
V ∗
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

V ∗
1+(n+1)h(¯sh)

| s0 = s, {at(cid:48)}h−1

t(cid:48)=0 = {ˆat(cid:48)}h−1

t(cid:48)=0

(cid:21)

(cid:35)

≥ −E

(cid:34)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˆV ∗
1+(n+1)h(φ1+(n+1)h(sh)) −

max
1+(n+1)h(sh)

¯sh∈φ−1

≥ − max

s

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˆV ∗
1+(n+1)h(φ1+(n+1)h(s)) −

max
¯s∈Φ1+(n+1)h(s)

V ∗
1+(n+1)h(¯s)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(48)

Let ∆φ,1+nh(s) := ˆV ∗
holds,

1+nh(φ1+nh(s)) − max¯s∈Φ1+nh(s) V ∗

1+nh(¯s). The following upper bound

∆φ,1+nh(s) := ˆV ∗

1+nh(φ1+nh(s)) − max

¯s∈φ−1

1+nh(s)

V ∗
1+nh(¯s)

≤ ˆV ∗

1+nh(φ1+nh(s)) − V ∗

1+nh(s)

≤ max

s

| ˆV ∗

1+(n+1)h(φ1+(n+1)h(s)) −

max
¯s∈Φ1+(n+1)h(s)

V ∗
1+(n+1)h(¯s)| + (cid:15)A

= (cid:107)∆φ,1+(n+1)h(cid:107)∞ + (cid:15)A.

where the third relation is by (47). Furthermore, the following lower bounds holds,

∆φ,1+nh(s) := ˆV ∗

1+nh(φ1+nh(s)) − max

¯s∈Φ1+nh(s)

V ∗
1+nh(¯s)

≥ ˆV ∗

1+nh(φ1+nh(s)) − V ∗

1+nh(s) − (cid:15)A

≥ − max

s

ˆV ∗
1+(n+1)h(φ1+(n+1)h(s)) −

max
¯s∈Φ1+(n+1)h(s)

V ∗
1+(n+1)h(¯s)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

− (cid:15)A

= −(cid:107)∆φ,1+(n+1)h(cid:107)∞ − (cid:15)A,

where the second relation holds by Assumption 1 and the third by (48).

By the upper and lower bounds on ∆φ,1+nh(s) which holds for all s we conclude that

(cid:107)∆φ,1+nh(cid:107)∞ ≤ (cid:107)∆φ,1+(n+1)h(cid:107)∞ + (cid:15)A.

Using (cid:107)∆φ,H+1(cid:107)∞ = 0 we solve the recursion and conclude that

(cid:107)∆φ,1(cid:107)∞ ≤

H
h

(cid:15)A.

40

(49)

(ii) The following relations hold based on similar arguments as in (42). Let ∆π∗
maxs ˆV ∗

1+nh(s). For all s the following relations hold.

A

1+nh :=

A

1+nh(φ1+nh(s)) − V π∗
1+nh(φ(s)) − V π∗
ˆV ∗
(cid:34)h−1
(cid:88)

≤ max

a0,..,ah−1

E

A

1+nh(s)

− Eπ∗

A

(cid:34)h−1
(cid:88)

t(cid:48)=0

r(st(cid:48), at(cid:48)(st(cid:48))) + ˆV ∗

(cid:35)
1+(n+1)h(φ(sh)) | s0 = s

t(cid:48)=0

r(st(cid:48), at(cid:48)(st(cid:48))) + V π∗

(cid:35)
1+(n+1)h(sh) | s0 = s
,

A

(50)

the ﬁrst relation holds by the updating rule which update by the (see Algorithm 12), and since
V π
t = (T π)hV π
t+h, similarly to the optimal Bellman operator (2).
By deﬁnition, the sequence which maximizes the ﬁrst term is π∗
ˆV ∗. Using the linearity of expectation we get

A as it is the h-greedy policy w.r.t.

(51)

(52)

(50) = Eπ∗

A

≤ max

s

(cid:104) ˆV ∗

1+(n+1)h(φ(sh)) − V π∗
1+(n+1)h(φ(s)) − V π∗
ˆV ∗

(cid:105)
1+(n+1)h(sh) | s0 = s
1+(n+1)h(s) := ∆π∗

1+(n+1)h.

A

A

A

Since (51) for all s it also holds for the maximum, i.e.,

∆π∗

A

1+nh := max

s

1+nh(φ(s)) − V π∗
ˆV ∗

1+nh(s) ≤ ∆π∗

A

A

1+(n+1)h.

As ∆π∗

A

H+1 = 0 and iterating on the above recursion we get,

We are now ready to prove the proposition. For any s the following holds,

∆π∗

A

1 ≤ 0.

1 (s) − V π∗
V ∗

1

A

(s) = V ∗

1 (s) − ˆV1(φ(s))
(cid:123)(cid:122)
(cid:125)
(A)

(cid:124)

+ ˆV1(φ(s)) − V π∗

A

1

(cid:124)

(cid:123)(cid:122)
B

.

(s)
(cid:125)

By (49)

By (52),

(A) ≤ max

¯s∈Φ1(s)

1 (¯s) − ˆV1(φ1(s))
V ∗

:= −∆φ,1(s) ≤ (cid:107)∆φ,1(cid:107)∞ ≤

H
h

(cid:15)A.

ˆV1(φ1(s)) − V π∗

A

1

(s) ≤ max

¯s

(cid:16) ˆV1(φ1(¯s)) − V π∗

A

1

(cid:17)

(¯s)

= ∆π∗

A

1 ≤ 0.

Lastly, combining the above and using V ∗ ≥ V π, we get that for all s

0 ≤ V ∗

→ (cid:107)V ∗

A

1

1 (s) − V π∗
1 − V π∗

A

1 (cid:107)∞ ≤

(s) ≤

H
h

(cid:15)A.

H
h

(cid:15)A.

41

