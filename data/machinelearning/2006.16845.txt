0
2
0
2

n
u
J

6
2

]
P
S
.
s
s
e
e
[

1
v
5
4
8
6
1
.
6
0
0
2
:
v
i
X
r
a

A GRU-based Mixture Density Network for Data-Driven
Dynamic Stochastic Programming

Xiaoming Lia,c,d, Chun Wanga, Xiao Huangb, Yimin Nied

aDepartment of Information and System Engineering, Concordia University, Montreal,
QC, Canada
bJohn Molson School of Business, Concordia University, Montreal, QC, Canada
cSchool of Computer, Shenyang Aerospace University, Shenyang, Liaoning, P.R. China
dEricsson INC. Global Artiﬁcial Intelligence Accelerator (GAIA) innovation hub,
Montreal, QC, Canada

Abstract

The conventional deep learning approaches for solving time-series problem
such as long-short term memory (LSTM) and gated recurrent unit (GRU)
both consider the time-series data sequence as the input with one single unit
as the output (predicted time-series result). Those deep learning approaches
have made tremendous success in many time-series related problems, how-
ever, this cannot be applied in data-driven stochastic programming problems
since the output of either LSTM or GRU is a scalar rather than probability
distribution which is required by stochastic programming model. To ﬁll the
gap, in this work, we propose an innovative data-driven dynamic stochastic
programming (DD-DSP) framework for time-series decision-making problem,
which involves three components: GRU, Gaussian Mixture Model (GMM) and
SP. Speciﬁcally, we devise the deep neural network that integrates GRU and
GMM which is called GRU-based Mixture Density Network (MDN), where
GRU is used to predict the time-series outcomes based on the recent historical
data, and GMM is used to extract the corresponding probability distribution
of predicted outcomes, then the results will be input as the parameters for SP.
To validate our approach, we apply the framework on the car-sharing reloca-
tion problem. The experiment validations show that our framework is superior
to data-driven optimization based on LSTM with the vehicle average moving
lower than LSTM.

Keywords: Data-Driven Optimization (DDO), Stochastic Programming
(SP), Gated Recurrent Unit (GRU), Gaussian Mixture Model (GMM),
Mixture Density Network (MDN),

Preprint submitted to Transportation Research Part C

July 1, 2020

 
 
 
 
 
 
1. Introduction

A variety of deep learning (DL) models and algorithms have been proposed

and successfully solved a great many of applications ﬁelds such as computer

vision, natural language processing, data analysis etc. Due to its great suc-

cess, recently, leveraging machine learning (ML) and deep learning approaches

to support decision-making problem has attracted huge attentions in opera-

tions research community. Unlike regular ML approaches, DL methods are

capable of dealing with intrinsic and potential features that are hidden behind

the complex data. In data-driven optimization frameworks, the uncertainty

is modeled based on complex data which may has great impact on the opti-

mization solutions. The inaccurate parameters that are derived from complex

data may lead the optimizations model sub-optimal or even infeasible. In this

sense, data-driven optimization could be beneﬁt from utilizing DL tools.

Recently, decision-making under uncertainty has been applied in various

ﬁelds such as intelligent transportation, network optimization, scheduling prob-

lems, supply chain management etc. In this work, we focus on stochastic pro-

gramming (SP) technique which is aiming to ﬁnd the optimal solution that

maximize / minimize the expected value of objective function while satisfying

all the scenarios that are obtained from uncertain parameters. Conventionally,

SP assumes that the probability distribution of uncertain parameters is known

from perfect knowledge. In reality, however, it is diﬃcult even impossible to

obtain the accurate probability distribution from complex data. It is worth

noting that the inaccurate probability distribution may lead the optimization

solution to be sub-optimal even infeasible, therefore, it is quite necessary to

integrate ML / DL approaches to improve the solution quality of SP.

In this paper, we consider using SP to solve time-series decision-making

problems, which has been extensively studied and widely applied in intelligent

transportation domain. As discussed in the previous section, probability dis-

tribution is required by SP, however, the existing deep learning approaches for

2

time-series predication such as long-short term memory (LSTM)[1] and gated

recurrent unit (GRU)[2, 3], both of them return single unit (scalar) output

as the predicted results which cannot be used as the parameters for SP. To

ﬁll the gap, we devise an innovative deep neural network which involves GRU

and mixture density network (MDN) [4] called GRU-MDN for the time-series

probability distribution prediction. Further, we propose a novel data-driven

dynamic stochastic programming framework that integrates GRU-MDN along

with SP to solve time-series decision-making problems under uncertainty. To

best of our knowledge, this is the ﬁrst work that combine DL and SP for

time-series decision-making problems.

The contributions in this work are two-folded: (1) a novel GRU-MDN deep

neural network is devised to predict probability distribution of time-series

data, (2) stochastic programming is seamlessly integrated with GRU-MDN

to formulate relevant problems. The remainder of this paper is organized as

follows, the details of data-driven dynamic stochastic programming framework

is discussed in section 2, next we apply the framework using a toy example

which is in section 3, the conclusions and future work is summarized in section

4.

2. GRU-MDN stochastic programming framework

To make data-driven SP that is capable of solving time-series problem, we

propose a GRU-base mixture density network called GRU-MDN. The frame-

work involves three components, GRU is in charge of predicting customer

demands, GMM focuses on the probability distribution that are based on the

outcomes from GRU, the SP is in charge of modeling uncertainty. We will

elaborate our framework in this section.

2.1. Gated Recurrent Unit

Unlike the single and simple building block in RNNs, LSTM uses forgetting

and gating mechanisms to select and ﬁlter information that is necessary for

3

Figure 1: Gated Recurrent Unit

future computation. Figure 1 shows the detail gating mechanism of GRU.

The state of GRU for each time step t is given by the following equations.

zt = σg (Wzxt + Uzht−1 + bz)
rt = σg (Wrxt + Urht−1 + br)
ht = zt (cid:12) ht−1 + (1 − zt) (cid:12) φh (Whxt + Uh (rt (cid:12) ht−1) + bh)

(1)

where xt is the input vector, ht is the output vecotr, zt is the update gate

vector, rt is the reset gate vector, W, U, b are the parameter matrices and

vector. σg and φh are activation functions in sigmoid and hyperbolic types,

respectively.

2.2. Mixture Density Network

MDN is a variant of a neural network whose output is probability distribu-

tion(s) rather than single unit for most of neural networks. The basic idea of

MDN is combining a deep neural network (DNN) and a (group of ) mixture of

distributions. Actually, most of the modern DNN architectures such as CNN,

RNN and LSTM can be extended to become the special MDNs. The DNN

provides the parameters for multiple distributions, which are then mixed by

some weights. Also, These weights are provided by the DNN. Notice that the

4

true distribution of the input data can be any type which cannot be described

by the parametric methods (e.g. Gaussian distribution, Poisson distribution).

Actually, the non-parametric method such as kernel density estimation (KDE)

is able to handle with the arbitrary probability distribution learning problem.

However, KDE method cannot be directly applied for probability distribution

storage in MDN since KDE is a non-parametric approach which implies that

it may contain inﬁnite parameters that cannot be store by ﬁnite number of

neurons. Therefore, the semi-parametric approach - Gaussian mixture model

(GMM) is adopted to overcome the problem in MDN, which is formulated as

follows.

p(X(cid:12)

(cid:12)θ) =

K
(cid:88)

i=1

wiN (cid:0)X(cid:12)

(cid:12)µi, Σi

(cid:1)

where θ = (W, µ, Σ), W = (w1, w2, · · · , wK), µ = (µ1, µ2, · · · , µK) and Σ =

(Σ1, Σ2, · · · , ΣK). K is the number of Gaussian distributions. Generally,

GMM can be considered as a group of Gaussian distributions with diﬀerent

weights, where the i − th Gaussian is determined by weight wi, means µi and

covariance matrix Σi (variance for univariate Gaussian). Then the predicted

probability distribution can be represented using GMM by adjusting the pa-

rameter θ. In this work, we use expectation maximization (EM)[5] algorithm

to determine the parameter of GMM, which is summarized as follows.

The structure of MDN can be shown in the ﬁgure2.

One of the signiﬁcant contribution of this work is to combine GRU and

MDN, our proposed architecture of GRU-MDN can be shown in ﬁgure 3.

2.3. Data-driven dynamic stochastic programming framework

Finally, we come to summarize our data-driven SP framework which is

displayed in the ﬁgure 4.

There are four major components involving in the proposed framework.

Speciﬁcally, the time-series data is the input of the framework, then the pre-

5

Algorithm 1 EM

Input: The GMM (cid:80)K
Output: GMM

k=1 πkN (x|µk, σk)

1: Initialize µj, Σj and πj, j = 1, · · · , K
2: E-step. Compute

γnj =

πjN (ξn|µj, Σj)
i=1 πiN (ξn|µi, Σi)

(cid:80)K

3: M-step. Re-estimate

1
Nj

N
(cid:88)

n=1

µnew
j =

where

γnjξn, Σnew

j =

1
Nj

N
(cid:88)

n=1

γnj

(cid:0)ξn − µnew

j

(cid:1) (cid:0)ξn − µnew

j

(cid:1)(cid:62) , πnew

j =

Nj
N

Nj =

N
(cid:88)

n=1

γnj

4: Check whether the convergence is satisﬁed. If not, return to step 2.

Figure 2: Architecture of Mixture Density Network

6

Figure 3: The Structure of GRU-MDN

Figure 4: GRU-MDN SP Framework

7

dicted time-series probability distribution is obtained via GUR-MDN, the dis-

tribution is input as the parameters of SP, after that various model reformu-

lation and decomposition algorithms can be applied to solve the SP model.

3. Case Study

To validate our data-driven dynamic SP framework, we investigate the car-

sharing relocation problem (CSRP) which is referred in [6]. Speciﬁcally, we

will use the same stochastic programming model and data sets.

3.1. Experimental Setting

We use real data from New York taxi trip record data set from January

2017 to June 20191. The entire data set is split into training set (from January

2017 to March 2019), and testing set (from April 2019 to June 2019). We

compare our proposed approach with data-driven deterministic optimization

model where the demand is obtained from the typical time-series prediction

approach - LSTM.

The GRU-MDN models are implemented using Python 3.7 + tensorﬂow

2.1 under the platform CUDA 10.2 GPU, 16GB RAM, Ubuntu 18.04, the

mathematical models are solved by Gurobi 2 9.0 academic version using Python

3.7 under the platform Intel i7 CPU, 32GB RAM, Windows 10.

3.2. Experimental Validation

In order to compare the optimization performance of GRU-MDN and

LSTM. We select the similar deep neural network stricture which is shown

in Table 1.

We select the window size equals 10 as the input layer for both GRU-

MDN and LSTM. There are two hidden layers for each neural network with

1https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page
2https://www.gurobi.com/downloads/gurobi-software/

8

Table 1: The structures of two deep neural network

GRU-MDN
LSTM

Input Layer Hidden Layers Output Layer
(256, 128)
(256, 128)

ws = 10
ws = 10

(3*3, 1)
1

Table 2: Comparison between GRU-MDN Stochastic Programming Model and LSTM De-
terministic Model

GRU-MDN
LSTM

Average Revenue Average Cost Average Moving
$415601.5
$438014.2

$947552.6
$921922.4

231.4615
248.7143

the number of neurons 256 and 128, respectively. Since GRU-MDN returns a

probability distribution, we use GMM as the output which involves 3 Gaussian.

It is worth noting that although the structures of both deep neural network

are quite similar, the output of GRU-MDN is a predicted time-series distribu-

tion, which integrates a two-stage stochastic programming model, while the

output of LSTM is a predicted time-series value, which integrates a deter-

ministic model. Then we use the ﬁrst-stage solutions that are obtained from

GRU-MDN and LSTM to test on the testing set (91 days from April 1, 2019

to June 30, 2019). The comparison of the experiment results is shown in Table

2.

The experiment results show that GRU-MDN with SP model is able to yield

more average revenue with relatively lower average cost compared to LSTM

with deterministic model. Additionally, the moving average of GRU-MDN is

6.94% lower than LSTM.

4. Concluding Remarks

In this work, we developed a practical data-driven dynamic stochastic

programming framework for time-series problem. The approach integrates

a GRU-MDN deep neural network along with a two-stage stochastic program-

ming model. Our proposed methodology provides a very eﬃcient framework

9

for data-driven dynamic SP technique. Furthermore, the framework does not

apply in the discussed example only. Actually, as a potential extension, the

framework can be applied in a number of diﬀerent applications by replacing the

components. For instance, the component of SP can be replaced by distribu-

tionally robust optimization [7] (DRO) which relies on the ambiguity set that

contains a family of probability distributions. We believe that the framework

is capable of dealing with DRO modeling problems by minor modiﬁcations

on GRU-MDN. Additionally, diﬀerent model decomposition algorithms such

as L-shape, column generation can be adopted for model solving according to

the characteristics of mathematical models.

Although the proposed framework utilize the historical data to solve the

time-series decision-making under uncertainty, it does not consider the prior

probability distribution which may be quite informative for SP. We believe that

using prior probability distribution information may improve the SP solution

in a very eﬀective way, therefore, we will investigate Bayesian learning with

SP for data-driven SP framework in our future work.

References

[1] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural compu-

tation 9 (1997) 1735–1780.

[2] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,

H. Schwenk, Y. Bengio, Learning phrase representations using rnn encoder-

decoder for statistical machine translation, arXiv preprint arXiv:1406.1078

(2014).

[3] J. Chung, C. Gulcehre, K. Cho, Y. Bengio, Empirical evaluation of

gated recurrent neural networks on sequence modeling, arXiv preprint

arXiv:1412.3555 (2014).

[4] C. M. Bishop, Mixture density networks (1994).

10

[5] A. P. Dempster, N. M. Laird, D. B. Rubin, Maximum likelihood from

incomplete data via the em algorithm, Journal of the Royal Statistical

Society: Series B (Methodological) 39 (1977) 1–22.

[6] X. Li, C. Wang, X. Huang, Ddksp: A data-driven stochastic pro-

gramming framework for car-sharing relocation problem, arXiv preprint

arXiv:2001.08109 (2020).

[7] E. Delage, Y. Ye, Distributionally robust optimization under moment

uncertainty with application to data-driven problems, Operations research

58 (2010) 595–612.

11

