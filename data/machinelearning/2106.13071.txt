1
2
0
2

n
u
J

4
2

]

V
C
.
s
c
[

1
v
1
7
0
3
1
.
6
0
1
2
:
v
i
X
r
a

ChaLearn Looking at People: Inpainting and Denoising challenges*

Sergio Escalera1, Marti Soler2, Stephane Ayache3, Umut Guclu4, Jun Wan5, Meysam Madadi6,
Xavier Bar´o7, Hugo Jair Escalante8, Isabelle Guyon9
1 Universitat de Barcelona and Computer Vision Center, Barcelona, Spain
2 Universitat de Barcelona, Barcelona, Spain
3 Aix Marseille Univ, CNRS, LIF, Marseille, France
4 Radboud University, Donders Institute for Brain, Cognition and Behaviour, Nijmegen,
Netherlands
5 National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese
Academy of Sciences (CASIA)
6 Compter Vision Center, Spain
7 Universitat Oberta de Catalunya, Spain
8 INAOE, Mexico
9 Universit´e Paris-Saclay, France, ChaLearn, Berkeley, CA, USA

Abstract

1. Introduction

Dealing with incomplete information is a well studied
problem in the context of machine learning and computa-
tional intelligence. However, in the context of computer
vision, the problem has only been studied in speciﬁc sce-
narios (e.g., certain types of occlusions in speciﬁc types of
images), although it is common to have incomplete infor-
mation in visual data. This chapter describes the design
of an academic competition focusing on inpainting of im-
ages and video sequences that was part of the competition
program of WCCI2018 and had a satellite event collocated
with ECCV2018. The ChaLearn Looking at People Inpaint-
ing Challenge aimed at advancing the state of the art on
visual inpainting by promoting the development of methods
for recovering missing and occluded information from im-
ages and video. Three tracks were proposed in which visual
inpainting might be helpful but still challenging: human
body pose estimation, text overlays removal and ﬁngerprint
denoising. This chapter describes the design of the chal-
lenge, which includes the release of three novel datasets,
and the description of evaluation metrics, baselines and
evaluation protocol. The results of the challenge are ana-
lyzed and discussed in detail and conclusions derived from
this event are outlined.

*This is a preprint of an article published in Inpainting and Denois-
ing Challenges. The Springer Series on Challenges in Machine Learn-
ing. Springer, Cham. The ﬁnal authenticated version is available online at:
https://doi.org/10.1007/978-3-030-25614-2 2

The problem of dealing with missing or incomplete in-
formation in machine learning and computer vision arises
naturally in many applications. Recent strategies make use
of generative models to complete missing or corrupted data.
Advances in computer vision using deep generative models
have found applications in image/video processing, such as
denoising [7], restoration [18], super-resolution [4], or in-
painting [17, 13].

We focus on image and video inpainting tasks, that might
beneﬁt from novel methods such as Generative Adversarial
Networks (GANs) [14] or Residual connections [6, 11]. So-
lutions to the inpainting problem may be useful in a wide
In this study, we focus
variety of computer vision tasks.
on three important applications of image and video inpaint-
ing: human pose estimation, video de-captioning and ﬁn-
gerprint recognition. Regarding the former task, it is chal-
lenging to perform human pose recognition in images con-
taining occlusions. It still is a present problem in most real-
istic scenarios. Since tracking human pose is a prerequisite
for human behavior analysis in many applications, replac-
ing occluded parts may help the whole processing chain.
Regarding the second task, in the context of news media,
video entertainment and broadcasting programs from var-
ious languages (such as news, series or documentaries),
there are frequently text captions or embedded commercials
or subtitles. These reduce visual attention and occlude parts
of frames, potentially decreasing the performance of auto-
matic understanding systems. Finally, the problem of vali-

1

 
 
 
 
 
 
dation of ﬁngerprints in images with occlusions which has
applications mostly in forensics and security systems. De-
spite recent advances in machine learning, it is still chal-
lenging to aim at fast (real time) and accurate automatic re-
moval of occlusions (text, objects or stain) in images and
video sequences.

In this chapter we study recent advances in each given
task. Speciﬁcally, we conduct the ChaLearn Looking at
People Inpainting Challenge that comprised three tracks,
one for each task. Such event had a satellite event collocated
with ECCV2018. Herein we explain the challenge setups,
provided datasets, baselines and challenge results for each
task. The main contributions of this challenge are: the or-
ganization of an academic competition that attracted a num-
ber of participants to deal with the inpainting problem; the
release of three novel datasets that will foster research in vi-
sual inpainting; an evaluation protocol that will remain open
so that new methodologies can be compared with those pro-
posed in the challenge. The data can be downloaded from:

• Still images of humans:

https://chalearnlap.cvc.uab.cat/dataset/30/description/

• Video decaptioning:

https://chalearnlap.cvc.uab.cat/dataset/31/description/

• Fingerprint reconstruction and denoising:

https://chalearnlap.cvc.uab.cat/dataset/32/description/

The remainder of the chapter is organized as follows. In
Section 2 we explain the inpainting of human images track
In Sec-
where images are occluded with random blocks.
tion 3 we elaborate on the problem of inpainting of video
clips subtitles with different size and color. Next, we de-
scribe the inpainting of noisy ﬁngerprints track in Section 4,
where noise is added artiﬁcially with the aid of different pat-
terns. Finally, we discuss conclusions and future directions
of research in Section 5.

2. Inpainting still images of humans

Human subjects are important targets in many images
and photographs. In this sense, having an image with clear
details and without noise is critical for some applications in
uncontrolled environments. For instance, it is quite likely to
remove an object that occludes part of the body and replace
it with a realistic background. This has an instant applica-
tion in media production or security cameras. It can also be
used as an intermediate task to help other applications like
human pose or gesture estimation.

The human pose has many degrees of freedom, giv-
ing the human apperance very high variability. This cou-
pled with cluttered backgrounds, different viewpoints and
differences in foreground/background illumination, pose
great challenge to the task of inpainting parts of the hu-
man body in still images. The lack of temporal context

in highly masked images adds an extra difﬁculty for these
approaches. Therefore, an algorithm must be aware of
global context while generating consistent and realistic lo-
cal patches for masked areas.

In this track we tackle inpainting still images of humans
with different level of missing areas. As a standard ap-
proach in the inpainting domain, we artiﬁcially mask out
several blocks around body joints with random position and
size. Given an RGB image and its mask in training time, a
participant’s method must be able to accurately reconstruct
the masked region in test time. Evaluation is done based on
standard similarity metrics. Given inpainting as an interme-
diate task, we also evaluate accuracy of each reconstructed
image in human pose. That is, a pretrained method is used
to estimate body pose based on reconstructed image and an
error is computed based on estimated 2D joints.

2.1. Data

For this dataset we collected images from multiple
sources that can be seen in Table 1. They have been selected
because they provide a high variability of content while be-
ing representative enough to allow the resulting methods to
generalize. We randomly split samples into train/val/test
sets by 70/15/15 % ratio, respectively.

Table 1: Image source overview, note that the number of im-
ages used is not the same as the original size of the dataset,
this is due to extracting multiple human crops from the
original image and ﬁltering out some not suitable for the
planned task.

Name

#Images Used Cropped

MPII Human Pose Dataset [2]
Leeds Sports Pose Dataset [9]
Synchronic Activities Stickmen V [5]
Short BBC Pose [3]
Frames Labelled In Cinema [15]

26571
2000
1128
996
10381

Yes
No
Yes
No
Yes

Preprocessing To make the joint annotations consistent
we made sure that all the selected datasets used the same
criteria when annotating each body part. It is important to
note that while not every image has all joints annotated, the
annotations of all the joints of the same type are consis-
tent. In Figure 1 one can see which joint positions we con-
sider valid. In Table 2 we can see which joints we extracted
from each dataset. The only ones that appear through all the
datasets are wrists, elbow and shoulder.
Some adjustments had to be done to ensure that the annota-
tions were consistent:

• We removed the information about right and left of the

Table 2: Check-marks indicate that either the joint appears in some images of the dataset, or it can be extracted from the
original annotations. Joints refer to 1-Ankles, 2-Knees, 3-Hips, 4-Pelvis, 5- Thorax, 6-Upper Neck, 7-Head Top, 8-Wrists,
9-Elbows, 10-Shoulders, 11-Eyes, 12-Nose

Name

#Joints

MPII
LSP
Synch
BBC
FLIC

19
8
6
4
6

6

3

7

4

5

8

2

10
1
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
(cid:88) (cid:88) (cid:88)
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
(cid:88) (cid:88) (cid:88) (cid:88)

9

11

12

(cid:88)

(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)

size s ranging from

min(w, h)
20

< s <

min(w, h)
3

where w and h are the width and height of the image, re-
spectively. The blocks cover joints in part and are randomly
positioned around the center of the joint. The blocks do not
overlap and have a margin of 100px from the image’s edge.
At most 70% of the image is masked.

2.2. Baselines

The baselines decided for track 1 are adaptations of
[14, 19, 20] that all involve deep convolutional architec-
tures. Below are descriptions of the considered baselines:

• Context encoders. [14]. This method uses an autoen-
coder model along with a generative adversarial model
(GAN). It was trained by a conjunction of a reconstruc-
tion loss (normalized masked L2) and an adversarial
loss provided by the discriminator.

• Multi-scale neural patches [19]. This baseline is fo-
cused on obtaining high resolution results. To do so
it uses a model based on two networks, a content net-
work tasked with creating an initial approximation and
a texture network which is tasked with adding high fre-
quency details by constraining the texture of the gener-
ated image according to the texture of the non masked
area. Context Encoders are used as the global con-
tent prediction network which serves as initialization
for the multi-scale algorithm.

This model

• Semantic inpainting [20].

learns to
generate new samples from the dataset, which means
it learns the manifold where the dataset exists. Then
they infer a reconstruction by ﬁnding the point on this
manifold closest to the encoded masked image.

We trained these baselines on the same train set used for the
competition. Some qualitative results are shown in Table 3.
As one can see, the baselines perform quite well on small

Figure 1: Position of the selected joints.

joints since some of the datasets were using the camera
as reference and others the human subject.

• In the Synchronic dataset we did not have a thorax
joint, but, from what we could see, it contained a verti-
cal line through the upper body and the middle of that
line was pretty similar to the thorax position, so we
decided to use those instead.

Masking For each image we generated a different mask
to hide parts of the image. Algorithms will be evaluated
on how well they can restore the parts of the image oc-
cluded by this mask. To avoid having ﬁxed bounds we set
the following procedure: masks consist of N blocks, where
0 < N < 11 for each image, each block being a square of

parts but visual quality must be improved when inferring
bigger missing regions.

2.3. Competition results

To evaluate results, we used traditional image similar-
ity metrics such as mean squared error (MSE) and struc-
tural dissimilarity (DSSIM) [16]. Additionally, we pro-
posed a new highly domain speciﬁc metric, Weakly Nor-
malized Joint Distance (henceforth WNJD), which evalu-
ates the performance of a state of the art pose estimation
technique on the image before and after the reconstruction.
It can be calculated for each image with:

W N JD =

(cid:80)N

i=0 |OJointi − P Jointi|
N ||(w, h)||

,

where OJoint and P Joint are vectors containing the orig-
inal and predicted joints, respectively. N is the number of
labeled joints and w, h are the width and height of the im-
age. Finally, due to the emphasis on the human pose do-
main of the problem the submissions were evaluated using
a mean rank of multiple metrics.

We used [12] as pose estimation algorithm, which
presents an architecture called hourglass which is stated
to be able to capture information from all possible scales.
Each hourglass module is an autoencoder with residual con-
nections from encoder convolutional layers to correspond-
ing decoder ones. A number of hourglass modules are
stacked sequentially, a loss is applied on the output of each
stack and the whole model is trained jointly. Finally, the
joints are extracted from output joint heatmaps. In the com-
petition, this information was withheld from the participants
to avoid unfair advantages.

The ﬁnal results on the competition test set can be seen
in Table 4. We also show some qualitative images of partic-
ipants methods in Figure 5.

2.4. Discussion

From the results of the competition we can see that most
of the approaches, from participants or baselines, involved
the use of a GANs. The main differences with the solutions
were on the Loss or mixture of Losses used to train the GAN
and the architecture. These differences can be seen in Ta-
ble 6.

Another interesting point to note is that while both
approaches transferred knowledge from networks trained in
other domains, they used different methods to do so. One
of them used pretraining and the other one incorporated the
knowledge into the loss function, by using VGG to evaluate
the quality of the reconstruction.
We believe that these results indicate that GANs have con-
solidated their place as a strong competitor for inpainting
problems, and that new methods live and die by their ability
to correctly evaluate the quality of the reconstruction. We

can see the ﬁeld moving from using the original image
as the only solution and instead evaluating each solution
on their own merits,
independently of how much they
resemble the original. This is probably one of the main
contributing factors behind the predominance of GANs.
We can also see that there is no generalized standard to
evaluate these possible solutions which causes most of the
methods to focus heavily on loss engineering.

In this competition though we did not want to only eval-
uate the image reconstruction quality. We also wanted to
check how capable an inpainting model was to store infor-
mation from an speciﬁc domain such as human poses. To
that end we can see a more ﬁne grained breakdown of the
mean WNJD for each joint type on Table 7

As we can see there was a deﬁnitive difference between
the quality of the human pose reconstruction between the
approaches, as one model outperformed the other in ev-
ery type of joint reconstructed. It is notable that no model
added any kind of handcrafted feature or used pretrained
networks speciﬁc to the human domain. All the human do-
main knowledge was learned from this dataset.

3. Video decaptioning

Video data is nowadays collected everywhere, as well
as shared on various internet platforms in an extraordinary
exponential growing., This offers a wide diversity of
contents to ﬁnal users, e.g. more than 400 hours of new
video are uploaded to Youtube every single minute[1]. Live
streaming from social media platforms also offer a huge
quantity of video to ﬁnal users, that for some poor network
reasons might be degraded to low resolution.
In various
situations, available video data might beneﬁt from recent
advances in computer vision to enhance its quality and
improving the user experience.

Existing similar tasks, such as automatic video de-
noising or video coloration allow for restoring ancient
archives that constitute an effective way of preserving such
rich source of cultural knowledge. Video inpainting has
multiple forms because of the spatio-temporal nature of
such signal: in the temporal axis, inpainting might consists
in inferring entire frames within a given video sequence,
leading to artiﬁcial slow motion effect [8]. For both spatial
and temporal axis, video inpainting replaces missing or
occluding parts in video sequences with semantically
In particular, removing occluding parts
relevant pixels.
in video sequences might lead to better automatic object
detection and indexing systems, especially for autonomous
vehicle vision systems. From the user point of view, text
overlays in news or media programs reduce the visual at-
tention, making hard to focus on the original video message.

Image

Masked

[14]

[20]

[19]

Pose Estimation

DSSIM:

DSSIM:

DSSIM:

-

-

-

0.306

0.351

0.312

0.135

0.231

0.172

0.353

0.477

0.353

-

-

-

Table 3: Qualitative and quantitative samples of chosen baselines for track 1 (resized). DSSIM to the original has been added
below the reconstructions. Pose estimation (last column) done over [19] reconstruction.

Table 4: Test set results for track 1.

# User

Rank MSE

DSSIM

WNJD

1 UNLU
2

Inception

1.3
1.7

0.0158 (1)
0.0158 (2)

0.2088 (2)
0.2048 (1)

0.1489 (1)
0.1495 (2)

This section introduces the ChaLearn LaP Inpainting
Challenge Track on video decaptioning. The video decap-
tioning task might be viewed as a simpliﬁed but challenging
and realistic scenario of the video inpainting task. We deﬁne
the video decaptioning task in a supervised setting, where
the goal is to generate a decaptioned version of a captioned
video sequence. Addressing this task must resolve the com-
plex task of replacing overlaid captions with original set of
semantically coherent pixels in the video sequence. We pro-
pose the ﬁrst large dataset of paired training data that con-
tains both degraded and groundtruth data from natural, wide
and diverse video sequences. We hope that such large video

dataset can push deep learning based model to resolve this
challenging task.

3.1. Data

We collected a large brand new dataset for the video in-
painting track of the ChaLearn LaP Inpainting Challenge.
The dataset is composed of pairs of the form (X, Y), where
X is a 5 seconds video clip (containing embedded text) and
Y the corresponding target video clip (without embedded
text, see Figure 2). The participants in this track must de-
velop a method that receives as input the video with cap-
tions and provide as output the decaptioned video.

Sources We collected about 150 hours of diverse videos
of TV movies of various genres with associated generated
subtitles available from YouTube. The various video con-
tents are intended to be as diverse as possible, making
the dataset almost generic for subtitle removal task. Data
sources were checked to fulﬁll the required copyrights to be
reused during this challenge for research purposes.

Ground truth

UNLU

Inception

Table 5: Qualitative samples for track 1.

Table 6: Main approach differences between participant approaches.

# Work Name

Input Size

Pretraining

People Inpainting with
Generative Adversarial
Networks

256x256

Generative Image Inpainting
with Contextual Attention
(Places2)

Generator

S1 D2

Loss

Discriminators

N

Y

Generalized
Loss-Sensitive GAN

Global + Local

Generative Image
Inpainting for Person
Pose Generation

1 Skip-connections
2 Dilated Convolutions

128x128

None
(VGG used to
calculate loss)

Y

Y

Mixture:
[L1 & Adversarial
& Perceptual(VGG)]

Global

1

2

Table 7: Mean WNJD for all predictions of each predicted joint type in the test set. Lower is better. RK - Right Knee, RH -
Right Hip, RW - Right Wrist,RE - Right Elbow, RS - Right Shoulder, RA - Right Ankle, P - Pelvis, LE - Left Elbow, LS -
Left Shoulder, LK - Left Knee,LH - Left Hip, LW Left Wrist, T - Torso, LA - Left Ankle

# User

RK RH RW RE

RS

RA

P

LE

LS

LK LH LW T

1 UNLU
2

Inception

0.07
0.51

0.16
0.46

0.24
0.40

0.13
0.38

0.12
0.39

0.16
0.57

0.06
0.32

0.11
0.28

0.10
0.26

0.06
0.34

0.09
0.31

0.19
0.31

0.08
0.47

LA

0.10
0.35

Preprocessing Raw videos were processed in order to ob-
tain a set of well formed short video sequences: we removed
letter-boxing area (top/down black bars where subtitles
could be printed), normalized frame rate to 25 frames per
second and rescaled frames to width=256 keeping aspect
ratio, then cropped the central region with size 256 × 256.
In order to create the set X of corrupted videos, we hard
printed subtitles by applying several variations in fonts and
text style (size, color, italic/bold, background box transpar-
ent/opaque). Finally, we split the original and corrupted
videos in 5 seconds non-overlapped video clips. We kept
70% of the clips that contain the most subtitles that repre-
sents about 90,000 video clips with 125 frames of 256x256
pixels. Finally, we split the resulting video clips into train-
ing (60% of clips), validation (20% ) and, test (20%) sets.

In Figure 2 a few examples are shown. The ﬁgure illus-
trates the diversity of video content, style and resolution, as
well as the vaeried font transformations and subtitle over-
lays. We also notice a few examples containing text or logo
in both corrupted and ground truth clips.

3.2. Novelty of the dataset

This dataset is the largest and more diverse video dataset
especially designed for the inpainting task. The nature and
the diversity of video content make it both, challenging
and of practical relevance, Large variations of text overlays
offers various kind of inpainting contributions, from de-
noising methods (for samples where overlay is small, font
is well contrasted and background transparent), to large

missing region (where background is opaque and overlay
is large). Dealing with such variations and video contents
lead to a difﬁcult and challenging dataset.

Missing parts (i.e., captioned area) in the video dataset have
various sizes, since overlays size vary with the font size as
well as length of subtitles. Figure 3 shows the distribution
of missing parts ratio, highlighting the various difﬁculty of
video samples, up to 50% of frames to replace. Variations
in number of pixels makes the proposed task challenging
but realistic.
We provide metadata with this dataset, consisting of every
video sources, timestamped subtitles text, overlay size and
background style for each video clip. Such metadata allows
for analyzing systems performance according to overlays
variations.

3.3. Baselines

The video track’s main objective can be achieved by
multiple auxiliary tasks, i.e.: detect regions that contain
text, video segmentation, video/patch generation, etc. We
implemented baselines from recent state of the art meth-
ods from the image inpainting domain. We adapted mod-
els from [14, 19, 20], which involve deep convolutional
encoder-decoder architectures with reconstruction loss. We
do not use temporal information, neither adversarial loss,
giving room for lots of possible improvements during the
challenge.

Figure 2: Few dataset samples. On top, corrupted X frames. At bottom, groundtruth Y frames

split into non-overlapped patches. Then if the clas-
siﬁer informs about the presence of text, the patch is
passed through the decoder. Patches classiﬁed as non-
text are simply copied to the generated output frame.
This approach is faster to train and efﬁcient to remove
text with no background overlays. However, since it
ignores the context, it cannot perform well on patches
containing only missing values.

3.4. Competition results

Tables 8 and 9 show the results of this track obtained
in both validation and test phases of the Challenge, re-
spectively. We report three evaluated metrics: MSE, peak
signal-to-noise ratio (PSNR) and structural similarity index
(SSIM). During the ﬁrst phase, 35 different participants
registered and downloaded the starting kit. Among them,
18 did at least one submission (not always valid), and 7 par-
ticipants kept active. In total, we received 50 submissions,
Table 8 and 9 list scores reached by active participants.

At the test phase, some participants merged their submis-
sions, constituting a new team, while some did not submit
any factsheets or codes as required to ofﬁcially claim prizes.
Table 9 list every participants submission (in italic we show
discarded participants).

3.5. Discussion

Most of active participants did succeed in improving
baseline methods. Next we brieﬂy discuss on differences
of proposed methods and their impact on inpainting perfor-
mance.

Figure 3: Distribution of different missing parts on the
video dataset.

• Baseline 1 considers the whole frames as global con-
text to infer missing parts, such as the complete spa-
tial structure to generate complete frames. We use a
deep convolutional autoencoder, with a VGG-like en-
coder consisting of 3 blocks of 2 convolutional layers,
batch normalization and relu non-linearities, with re-
spectively 64, 128 and 256 ﬁlters of size 3x3 pixels.
The decoder is composed of 4 blocks of convolutional,
upsampling, batch normalization and relu activation.
This model is heavy in terms of parameters and mem-
ory usage, since it assumes inputs and outputs of size
256x256x3.

• Baseline 2 processes frames by patches of 32x32 pix-
els, allowing to reduce the number of parameters. A
patch auto-encoder (similar but lighter than ﬁrst base-
line) is jointly learned with a patch classiﬁer that de-
tects the presence of text. At inference, a frame is ﬁrst

Table 8: Validation Results of track 2.

# User

Rank MSE

PSNR

SSIM

hcilab
1
arnavkj95
2
dhkim
3
4
anubhap93
5 mcahny01
6
7
8

ucs
baseline2
yangchris11

3.3
3.6
4.0
4.3
4.6
4.6
5.3
6.0

0.0013 (1)
0.0014 (2)
0.0015 (3)
0.0016 (6)
0.0018 (5)
0.0021 (4)
0.0023 (7)
0.0045 (8)

32.6001 (1)
31.9629 (2)
31.0613 (3)
30.5311 (4)
29.9306 (6)
28.6957 (7)
30.0993 (5)
27.0632 (8)

0.0439 (8)
0.0512 (7)
0.0516 (6)
0.0610 (5)
0.0751 (3)
0.0935 (1)
0.0621 (4)
0.0876 (2)

Table 9: Test set Results of track 2.

# User

Rank MSE

PSNR

SSIM

1
2
3
4
5
6

SanghyunWoo
hcilab
ucs
anubhap93
arnavkj95
baseline2

2.6
3.0
3.3
3.6
4.0
4.3

0.0011 (1)
0.0012 (3)
0.0011 (2)
0.0012 (4)
0.0012 (5)
0.0022 (6)

33.3527 (1)
33.0228 (2)
33.0052 (3)
32.0021 (5)
32.1713 (4)
30.1856 (6)

0.0404 (6)
0.0424 (4)
0.0410 (5)
0.0499 (2)
0.0482 (3)
0.0613 (1)

• SanghyunWoo used a U-Net based architecture
combined with 3D and 2D gated convolutions to
predict the middle frame of N given frames. Gated
convolution allows for learning an attention mecha-
nism in intermediate feature maps, while temporal
dependence aims at taking advantage of the video
structure. Training is performed by optimizing a
loss that combines L1 reconstruction term, SSIM
perceptual score, and Gradient L1 loss.

• hcilab trained a U-Net model in a multi-stage proce-
dure, where output of the previous stage is given as
input of next stage, in a recurrent manner. The use of
skip connections from U-Net in addition to an iterative
scheme increase signiﬁcantly the inpainting perfor-
mance, compared to initial baseline2 performance.

• anubhap93 used an almost standard U-Net archi-
tecture with dilated convolutions. Their model was
trained by combining MSE, SSIM and GAN losses.
Dilated convolutions seem to signiﬁcantly improve
the generation quality.

• arnavkj95 used a standard U-Net architecture, trained

with MSE loss.

We notice that only the winning team did take account

of temporal context in video sequences to perform decap-
tioning. However, it is not clear whether they reached top
performance due to this characteristic, since they are also
the only team using gated convolutions. GANs were not
intensively considered during this challenge, only one team
combined Adversarial loss with MSE and SSIM losses,
with a sensibly low impact, probably due to the difﬁculty
and unstable nature of such training. The U-Net model has
been widely considered as an effective encoder-decoder
architecture. Most participants extended this model to
improve inpainting performance.

Figure 4 shows a qualitative comparison of participants
outputs for three different examples that belong to three lev-
els of difﬁculty with respect to the size of missing part to
inpaint: easy samples mostly contain subtitles with small
font, transparent background and few text; medium samples
have bigger font, semi-transparent background, more text ;
difﬁcult samples contain a big part to replace (up to 50% of
frame) corresponding to subtitles with black-opaque back-
ground. We note that all methods perform well for easy
sample. From the medium sample, we remark small tex-
ture variations in the curtain reconstruction from partici-
pants outputs, where the baseline output fails to reconstruct
well the right texture. The U-Net architecture seems here
to bring a signiﬁcant positive impact, since only our base-
line method did not use any skip connections within the
encoder-decoder architecture. Hard samples are extreme
case of inpainting problem where a large part must be re-
placed. All the methods fail in getting acceptable recon-
struction and generate blurry pixels. However, watching
carefully, it appears that all the methods (except baseline)
succeed in reconstructing the global structure of missing
parts (face, hairs, neck, jacket). More carefully, we can ob-
serve that ’hcilab’ seemed to generate details slightly better
than others, especially considering the shape of face, where
’arnavkj95’ missed more details than others. The blurry
effect in this hard case might mostly come from the MSE
loss used by most participants. In this case, GANs objec-
tive should help in generating sharper and more accurate
regions.

4. Fingerprint reconstruction and denoising

Biometrics play an increasingly important role in secu-
rity. They ensure privacy and identity veriﬁcation, as evi-
denced by the increasing prevalence of ﬁngerprint sensors
on mobile devices. Fingerprint retrieval keeps also being
an important law enforcement tool used in forensics. How-
ever, much remains to be done to improve the accuracy of
veriﬁcation, both in terms of false negatives (in part due to
poor image quality when ﬁngers are wet or dirty) and false
positives (due to the ease of forgery). This track involves re-
ducing noise (denoising) and/or replacing missing parts (in-

Figure 4: Qualitative comparison from the middle frame of three test samples.

painting) due to various kinds of alterations or sensor fail-
ures in ﬁngerprint images. This is viewed as a preprocess-
ing step to ease veriﬁcation carried out either by humans or
existing third party software. To protect privacy and have
full control over the experimental design, we synthesized a
(very) large dataset of realistic artiﬁcial ﬁngerprints for the
purpose of training learning machines.

Thus, the objective of participants was to develop al-
gorithms that can inpaint and denoise ﬁngerprint images
that contain artifacts like noise, scratches, etc. Such pro-
cedures can be applied to improve the performance of sub-
sequent operations like ﬁngerprint veriﬁcation. Developed
algorithms were evaluated based on reconstruction perfor-
mance. That is, participants were required to reconstruct
the degraded ﬁngerprint images using their developed algo-
rithms and submit the reconstructed ﬁngerprint images. Af-
ter the submission, the reconstructed images were compared
against the corresponding ground-truth ﬁngerprint images
in the pixel space to determine the quality of the reconstruc-
tions.

4.1. Data

We generated images of ﬁngerprints by ﬁrst degrading
synthetic ﬁngerprints with a distortion model (blur, bright-
ness, contrast, elastic transformation, occlusion, scratch,
resolution, rotation), then overlaying the ﬁngerprints on top
of various backgrounds. The resulting images were typical
of what law enforcement agents have to deal with. For train-
ing participants got pairs of original and distorted images.
Their goal was to recover original images from distorted
image on a test dataset.

Training set was constructed in the following two steps:
i) 75,600 275 × 400 pixel ground-truth ﬁngerprint images
without any noise or scratches, but with random transforma-
tions (at most ﬁve pixels translation and +/-10 degrees ro-
tation) were generated by using the software Anguli: Syn-
ii) 75,600 275 × 400 pixel
thetic Fingerprint Generator.
degraded ﬁngerprint images were generated by applying
random artifacts (blur, brightness, contrast, elastic transfor-
mation, occlusion, scratch, resolution, rotation) and back-
grounds to the ground-truth ﬁngerprint images.
In total,
it contains 151,200 ﬁngerprint images (75,600 ﬁngerprints,
and two impressions - one ground-truth and one degraded -
per ﬁngerprint).

Validation and test sets were constructed similarly to
the training set and were used to evaluate the reconstruc-
tion performance. In total, each set contains 16,800 ﬁnger-
print images (8,400 ﬁngerprints and two impressions - one
ground-truth and one degraded - per ﬁngerprint). Since this
set was used for the purpose of evaluating the reconstruction
performance, only the degraded and not the ground-truth
ﬁngerprint images were provided to participants.

Table 10: Validation Results of track 3.

#

1
2
3
4
5
6
7
8
9
10
11

User

Rank MSE

PSNR

SSIM

CVxTz
rgsl888
hcilab
sukeshadigav
baseline
ﬁnlouarn
Xiaojing
BriceRauby
go
yashkotadia
yg

1.0
2.3
3.3
4.0
5.0
5.3
7.0
8.0
9.0
10.0
11.0

0.0191 (1)
0.0239 (2)
0.0241 (3)
0.0276 (6)
0.0252 (5)
0.0251 (4)
0.0381 (7)
0.0398 (8)
0.0414 (9)
0.0564 (10)
0.7282 (11)

17.6640 (1)
16.8363 (2)
16.6062 (3)
16.4162 (4)
16.4098 (5)
16.3992 (6)
14.6347 (7)
14.1740 (8)
14.1710 (9)
12.7785 (10)
1.3781 (11)

0.8426 (1)
0.8069 (3)
0.8031 (4)
0.8234 (2)
0.7954 (5)
0.7904 (6)
0.6990 (7)
0.6954 (8)
0.6709 (9)
0.6417 (10)
0.0001 (11)

Table 11: Test results of track 3.

# User

Rank MSE

PSNR

SSIM

1 CVxTz
rgsl888
2
hcilab
3
sukeshadigav
4
baseline
5

1.0
2.3
3.3
3.3
5.0

0.0189 (1)
0.0231 (2)
0.0238 (3)
0.0268 (4)
0.0241 (5)

17.6968 (1)
16.9688 (2)
16.6465 (3)
16.5534 (4)
16.4160 (5)

0.8427 (1)
0.8093 (3)
0.8033 (4)
0.8261 (2)
0.8234 (5)

4.2. Baselines

A deep learning based baseline was used for reconstruct-
ing/enhancing the degraded ﬁngerprint images with a stan-
dard deep convolutional neural network (CNN). To this end,
a CNN was trained on the synthetic training set by minimiz-
ing the MSE loss function with Adam. The CNN comprises
four convolutional layers, ﬁve residual blocks, two decon-
volutional layers and another convolutional layer. Each of
the ﬁve residual blocks comprises two convolutional layers.
All of the layers except for the last layer are followed by
batch normalization and rectiﬁed linear units. The last layer
is followed by batch normalization and hyperbolic tangent
units. The model is implemented in Chainer.

4.3. Competition results

Similar to the other tracks, the results of this track were
evaluated based on three different metrics. The metrics
were mean squared error (MSE), peak signal-to-noise ra-
tio (PSNR) and structural similarity index (SSIM). Table 10
and 11 show the ﬁnal results of the validation phase and the
test phase. Figure 5 shows qualitative results of a number of
submissions. In total, there were 11 submissions in the val-
idation phase and ﬁve submissions in the test phase. In the
test phase, only four out of a potential 10 submissions out-
performed the baseline, whereas in the validation phase, the
baseline was outperformed by all four submissions. Inter-
estingly, the only submissions in the test phase were those
that were ranked above the baseline in the validation phase.

Figure 5: Sample results of three Track 3 submissions.

Table 12: Summary of the winning methods.

# User

Model

Preprocessing Training

1 CVxTz

U-Net

Normalization,
rescaling,
resizing

Vanilla backprop with MSE,
data augmentation
(afﬁne, contrast, saturation)

-

Vanilla backprop with MSE

U-Net,
dilated conv
Baseline

2

3

4

5

rgsl888

hcilab

baseline

sukeshadigav M-Net

-
Normalization,
padding,
resizing
Residual encoder Resizing

Iterative backprop with MSE

Vanilla backprop with MSE,
SSIM

Vanilla backprop with MSE

4.4. Discussion

There were a number of similarities and differences be-
tween the winning methods and the baseline. A summary
thereof is presented in Table 12.

Three out of four of the winning methods used a U-net
like architecture and the remaining method used the same
architecture as the baseline. Given that the most striking
difference between the baseline and such U-net like archi-
tectures are skip connections and bottleneck layers, these
results demonstrate the importance of these architectural
components in the task of inpainting and denoising for ﬁn-
gerprint veriﬁcation.

Next to the architectural differences, there were other im-
portant factors that differentiated the higher ranking meth-
ods from the rest. In particular, it seems that the extensive
use of data augmentation by the ﬁrst placed team and the
use of dilated convolutions by the second placed team were
the main factors that separated them from the rest of the
teams.

Another interesting observation was that almost all
teams exclusively used the MSE loss function, which is no-
torious for causing artefacts in image generation tasks, sug-
gesting that the noise ceiling on this dataset was not reached
as the methods were held back by suboptimal loss functions.
Taken together, the results are expected to be improved

with more realistic forward models, adversarial/feature loss
functions and perhaps variational methods. It would also be
interesting to see how the current methods or such improved
methods compare with physical models and transfer to real
data.

5. Conclusion and future directions of research

We have described the design and results of a novel chal-
lenge focusing on inpainting in images and video. Three
tracks were proposed, each associated to a realistic sce-
nario. For each track we prepared new datasets and base-
lines, exposing the complexity of the tasks and showing the
feasibility of solving them. Results obtained by participants
were presented and analyzed. In all tracks, participants suc-
ceeded at improving the performance of baseline methods.
Also, it is impressive the qualitative performance of pro-
posed solutions.

We can outline the following conclusions and corre-
sponding future research directions as derived from the
challenge:

• Methodological similarity among solutions. Results
obtained by participants together with the undeniable
similarity among solutions, all of them based on deep
learning, conﬁrm the establishment of speciﬁc archi-
tectures of deep learning (e.g., U-NET like architec-
tures) for visual inpainting.
In fact, the difference
among solutions and their performance in some cases
was only due to hyperparameter or subtle changes in
the overall architecture of the models. This ﬁnding
seems to suggest that a promising line of future re-
search is that of the automatic construction of deep
learning models (AutoDL) [10]. That is, methods that
automatically can determine the architecture and hy-
perparameters of a model given a speciﬁc task. This is
in fact a growing area not only in terms of academic
research but of industrial importance.

• Feasibility of the approached tasks and potential
impact. The quantitative, but mainly the qualitative
performance of solutions evidences the feasibility of
the approached task:
for the three proposed tracks,
top ranked solutions represent satisfactory solutions
that could be put in practice in short time.
In this
sense, the challenge was successful in providing aca-
demically challenging problems, but also in organizing
tracks around problems of practical relevance. What is
more, solutions are based on what can be considered
domain public knowledge. This will surely motivate
other researchers and practitioners to approach similar
tasks.

• Quality and quantity on newly released datasets.
As previously mentioned, a major contribution of the

organization of the inpainting challenge lies in the re-
sources being generated. We provided novel datasets,
associated to realistic scenarios with a large potential
of application. The released datasets are now public
and comprise the largest resources to date in their cor-
responding application. We foresee these datasets will
be decisive in the progress of the ﬁeld in the forthcom-
ing years. Likewise, we think that a promising line
of research has to do with the generation of resources
for visual inpainting, including novel ways of labeling
huge amounts of data.

• Evaluation metric / loss function. Although we have
used SSIM as structural similarity metric, the qualita-
tive results show such a metric is not sufﬁcient to eval-
uate realistic inpainting and perceptions. Training a
network with SSIM or VGG-based perceptual loss can
not guarantee a realistic inpainting to the unseen test
data. This can be an interesting and important research
topic in the future.

Acknowledgements

The sponsors of ChaLearn Looking at People inpainting
and denoising events are Google, ChaLearn, Amazon, and
Disney Research. This work has been partially supported by
the Spanish project TIN2016-74946-P (MINECO/FEDER,
UE) and CERCA Programme / Generalitat de Catalunya.
This work was also partially funded by the French na-
tional research agency (grant number ANR16-CE23-0006).
We gratefully acknowledge the support of NVIDIA Cor-
poration with the donation of the GPU used for this re-
search. This work is partially supported by ICREA under
the ICREA Academia programme. We thank all challenge
participants for their excellent contributions.

References

video

of
as of

uploaded
[1] Hours
minute
july 2015.
statista.com/statistics/259477/
hours-of-video-uploaded-to-youtube-every-minute/,
2019. 4

every
https://www.

youtube

to

[2] Mykhaylo Andriluka, Leonid Pishchulin, Peter
Gehler, and Bernt Schiele. 2d human pose estimation:
New benchmark and state of the art analysis. In IEEE
Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2014. 2

[3] James Charles, Tomas Pﬁster, Derek R Magee,
David C Hogg, and Andrew Zisserman. Domain
adaptation for upper body pose tracking ian signed tv
broadcasts. In BMVC, 2013. 2

[14] Deepak Pathak, Philipp Kr¨ahenb¨uhl, Jeff Donahue,
Trevor Darrell, and Alexei Efros. Context encoders:
In Computer Vision
Feature learning by inpainting.
and Pattern Recognition (CVPR), 2016. 1, 3, 5, 7

[15] Benjamin Sapp and Ben Taskar. Modec: Multimodal
decomposable models for human pose estimation. In
In Proc. CVPR, 2013. 2

[16] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and
Eero P Simoncelli.
Image quality assessment: from
error visibility to structural similarity. IEEE transac-
tions on image processing, 13(4):600–612, 2004. 4

[17] Junyuan Xie, Linli Xu, and Enhong Chen. Image de-
noising and inpainting with deep neural networks. In
Advances in neural information processing systems,
pages 341–349, 2012. 1

[18] Li Xu, Jimmy SJ Ren, Ce Liu, and Jiaya Jia. Deep
convolutional neural network for image deconvolu-
tion. In Advances in Neural Information Processing
Systems, pages 1790–1798, 2014. 1

[19] Chao Yang, Xin Lu, Zhe Lin, Eli Shechtman, Oliver
Wang, and Hao Li. High-resolution image inpainting
using multi-scale neural patch synthesis. In The IEEE
Conference on Computer Vision and Pattern Recogni-
tion (CVPR), July 2017. 3, 5, 7

[20] Raymond A. Yeh∗, Chen Chen∗, Teck Yian Lim,
Schwing Alexander G., Mark Hasegawa-Johnson, and
Minh N. Do. Semantic image inpainting with deep
generative models. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition,
2017. ∗ equal contribution. 3, 5, 7

[4] Chao Dong, Chen Change Loy, Kaiming He, and Xi-
aoou Tang. Image super-resolution using deep convo-
lutional networks. IEEE transactions on pattern anal-
ysis and machine intelligence, 38(2):295–307, 2016.
1

[5] Marcin Eichner and Vittorio Ferrari. Human pose
IEEE transac-
co-estimation and applications.
tions on pattern analysis and machine intelligence,
34(11):2282–2288, 2012. 2

[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 770–778, 2016.
1

[7] Viren Jain and Sebastian Seung. Natural image de-
noising with convolutional networks. In Advances in
Neural Information Processing Systems, pages 769–
776, 2009. 1

[8] Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-
Hsuan Yang, Erik G. Learned-Miller, and Jan Kautz.
Super slomo: High quality estimation of multiple in-
In 2018
termediate frames for video interpolation.
IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2018, Salt Lake City, UT, USA,
June 18-22, 2018, pages 9000–9008, 2018. 4

[9] Sam Johnson and Mark Everingham. Clustered pose
and nonlinear appearance models for human pose esti-
mation. In Proceedings of the British Machine Vision
Conference, 2010. doi:10.5244/C.24.12. 2

[10] Zhengying Liu, Olivier Bousquet, Andr´e Elisseeff,
Sergio Escalera, Isabelle Guyon, Julio Jacques Jr.,
Adrien Pavao, Danny Silver, Lisheng Sun-Hosoya,
Sebastien Treguer, Wei-Wei Tu, Jingsong Wang, and
Quanming Yao. Autodl challenge design and beta
tests: towards automatic deep learning. In Submitted
to NIPS Workshop on Meta-Learning, 2018. 13

[11] Xiao-Jiao Mao, Chunhua Shen, and Yu-Bin Yang.
Image restoration using convolutional auto-encoders
arXiv preprint
with symmetric skip connections.
arXiv:1606.08921, 2016. 1

[12] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked
hourglass networks for human pose estimation. In Eu-
ropean Conference on Computer Vision, pages 483–
499. Springer, 2016. 4

[13] Alasdair Newson, Andr´es Almansa, Matthieu Fradet,
Yann Gousseau, and Patrick P´erez. Video inpainting
of complex scenes. SIAM Journal on Imaging Sci-
ences, 7(4):1993–2019, 2014. 1

