On a class of data-driven mixed-integer programming problems under
uncertainty: a distributionally robust approach

Sergey S. Ketkov†a, Andrei S. Shilova

aLaboratory of Algorithms and Technologies for Networks Analysis, National Research University
Higher School of Economics (HSE), Bolshaya Pecherskaya st., 25/12, Nizhny Novgorod, 603155, Russia

2
2
0
2

y
a
M
8
1

]

C
O
.
h
t
a
m

[

3
v
9
3
1
4
1
.
5
0
1
2
:
v
i
X
r
a

Abstract

In this study we analyze linear mixed-integer programming problems, in which the distribution of the

cost vector is only observable through a ﬁnite training data set. In contrast to the related studies, we

assume that the number of random observations for each component of the cost vector may vary. Then

the goal is to ﬁnd a prediction rule that converts the data set into an estimate of the expected value

of the objective function and a prescription rule that provides an associated estimate of the optimal

decision. We aim at ﬁnding the least conservative prediction and prescription rules, which satisfy

some speciﬁed asymptotic guarantees as the sample size tends to inﬁnity. We demonstrate that under

some mild assumption the resulting vector optimization problems admit a Pareto optimal solution

with some attractive theoretical properties. In particular, this solution can be obtained by solving a

distributionally robust optimization (DRO) problem with respect to all probability distributions with

given component-wise relative entropy distances from the empirical marginal distributions. It turns

out that the outlined DRO problem can be solved rather eﬀectively whenever there exists an eﬀective

algorithm for the respective deterministic problem. In addition, we perform numerical experiments

where the out-of-sample performance of the proposed approach is analyzed.

Keywords:

stochastic programming; distributionally robust optimization; data-driven optimization;

incomplete information; relative entropy distance

1. Introduction

We consider a class of linear mixed-integer programming problems where the objective criterion is

a function of both decision variables and uncertain problem parameters. The decision-maker cannot

observe the nominal distribution of uncertain parameters, but has access to some ﬁnite training data

set obtained from this distribution; see, e.g., the studies in [1, 2]. Ideally, we endeavor to solve the

following stochastic programming problem:

f (x, Q∗) := EQ∗{γ(c, x)},

min
x∈X

(1)

where X ⊆ Zn1 × Rn2 is a linear mixed-integer set of feasible decisions, γ(c, x) is a given loss function
and c ∈ Rn with n = n1 + n2 is a vector of uncertain problem parameters (which is also referred to as

†Corresponding author. Email: sketkov@hse.ru; phone: +7 910 382-27-32.

Preprint submitted to Elsevier

May 20, 2022

 
 
 
 
 
 
a cost vector ) governed by some nominal probability distribution Q∗. As outlined above, we assume
that the nominal distribution Q∗ is only observable through a ﬁnite data set. Furthermore, in contrast
to the related literature, the data set is supposed to be incomplete, i.e., the components of the cost

vector c may be explored not to the same degree. The latter assumption is motivated by some online

combinatorial optimization and machine learning problem settings discussed within Section 1.2.

From a computational perspective, if the nominal distribution Q∗ is not available to the decision-
maker, then we cannot even calculate the objective function value in (1) for a ﬁxed decision x ∈ X.

For this reason, the related solution approaches attempt to construct a fairly good approximation of

the objective function value in (1) based on a set of available data.

1.1. Related literature

In this section we brieﬂy discuss how a ﬁnite set of random observations can be transformed to a

solution of the stochastic programming problem (1). In the related literature this problem is usually

addressed withing the framework of sample average approximation (SAA) or distributionally robust

optimization (DRO).

In the former approach the expected value of the loss function in (1) is approximated, for example,

by the sample mean and the resulting function is optimized over the set X of feasible decisions [3]. In

general, SAA methods provide deterministic formulations that enjoy strong asymptotic performance

guarantees due to the central limit theorem. In other words, the solutions obtained from SAA converge,

in a sense, to the nominal solution of (1) as the sample size tends to inﬁnity. It can also be argued that

some robust modiﬁcations of sample average approximation methods may provide both asymptotic

and ﬁnite sample performance guarantees; see, e.g., [4].

The second approach to evaluate the expected value of the loss function in (1) is to construct an

ambiguity set (or a family) of probability distributions, which is consistent with the available training

data set. In other words, we must guarantee that the nominal distribution of uncertain parameters,
Q∗, belongs to the obtained ambiguity set with a suﬃciently high probability. Then the decision-maker
may solve a DRO problem, where the expected loss in (1) is optimized under the worst-case possible

distribution of uncertain parameters; see, e.g., [5, 6, 7].

In general, the problem of constructing an ambiguity set from data observations is considered by

relatively many authors. For example, Delage and Ye [5] design conﬁdence sets for the support, mean

and covariance matrix of uncertain parameters based on a ﬁnite set of independent training samples.

Alternatively, several studies explore a distance metric in the space of probability distributions “cen-

tered” at the empirical distribution of the training samples; we refer to the studies in [8, 2] and the

references therein. Next, in our previous study [9] we explore a situation where random observations

of the cost vector c are not known exactly, but reside within some speciﬁed aﬃne manifolds. However,

the modeling paradigm of [9] is related to the shortest path problem and is restricted by a rather

speciﬁc class of ambiguity sets. Eventually, we refer to Gupta [10], where the author introduces a

class of Bayesian ambiguity sets that are near-optimal in some speciﬁed asymptotic sense, i.e., when

the sample size tends to inﬁnity.

The major drawback of the outlined solution techniques is that they decouple a prediction stage,

i.e., estimation of the objective function value in (1) for a ﬁxed decision, and a prescription stage

2

where the obtained estimate is minimized over a given set of feasible decisions. In this regard, Van

Parys et al. [11] propose a meta-optimization problem, which aims at ﬁnding, in a sense, optimal way

of transforming the training data set to a solution of the stochastic programming problem (1).

The authors in [11] explore the case, in which the set of feasible decisions in (1) is continuous and

the training data set is formed by independent observations of the vector of uncertain parameters.

Van Parys et al. [11] seek the least conservative approximations of the expected loss in (1), for which

some predeﬁned asymptotic performance guarantees hold. It is proved that the optimal approximation

in the aforementioned sense is unique and can be obtained by solving a DRO problem subject to all

distributions within a ball with respect to the relative entropy distance. Speciﬁcally, the radius of the

ball controls the quality of asymptotic performance guarantees and the center of the ball is at the

empirical distribution of the data. In addition, we refer to Sutter et al. [12], who applied the idea of

Van Parys et al. [11] to non-i.i.d. training data sets and more general stochastic programming problems.

1.2. Our approach and contribution

In this paper we propose another extension of the problem setting in [11] with respect to the

construction of the training data set. The key modeling assumption of our study is that the number

of observations for each particular component of the cost vector c is not necessarily the same and,

thus, the overall data set is not complete. This assumption can be justiﬁed by a number of practical

discrete optimization problem settings, in which the components of the cost vector c are explored not

to the same degree.

At ﬁrst, it is often the case that historical data is collected by trial and errors through multiple

decision epochs. For example, in the stochastic multi-armed bandit problem [13, 14] the decision-

maker has access to a ﬁnite set of actions with unknown expected rewards and attempts to maximize

its cumulative reward over some ﬁnite time horizon. Naturally, in this problem setting it is more

favorable for the decision-maker to explore the actions with higher expected rewards, which implies

that some of the actions are explored more often than the others. The same idea can be applied to

online combinatorial optimization; see, e.g., [15, 16], where each action relates to a subset of ground

items (for example, a routing decision in the context of network optimization problems corresponds to
a sequence of arcs in a given network.) Then given a binary set of feasible decisions X ⊆ {0, 1}n the
decision-maker may take a decision x ∈ X and observe only the costs associated with this decision,

i.e., with nonzero components of x.

Another possible application of incomplete data sets dates back to statistical analysis with missing

data; see, e.g., [17] and the references therein. The existence of missing values in a training data set

may depend on the data itself and hence, this fact is usually modeled as a probabilistic phenomenon.

Depending on a concrete application missing values can be, for example, imputed based on some

probabilistic model, ignored (parameters of the model are estimated based on the available data) or

dropped from the analysis leaving only complete observations of the vector of uncertain parameters;

see, e.g., [18]. With respect to discrete optimization problems, the source of missing values can be

related to measurement errors, non-detects or simply a lack of information about some components of

the cost vector c.

3

In general, our modeling approach is motivated and similar to the approach of Van Parys et al. [11].

Formally, we seek an approximate solution of the stochastic programming problem (1) by introducing

prediction and prescription rules. A prediction rule converts the training data set into an estimate of

the objective function value in (1) for some ﬁxed decision x ∈ X, whereas a prescription rule evaluates

the optimal decision by minimizing the predicted value over the set X of feasible decisions. The choice

of prediction and prescription rules is induced by the following research questions:

Q1. Are there prediction and prescription rules that can be applied to incomplete data sets and are

also optimal in some predeﬁned sense?

Q2. If such optimal prediction and prescription rules exist, can we compute them rather eﬀectively

using oﬀ-the-shelf mixed-integer programming (MIP) solvers?

Q3. How the sample size and the form of the nominal distribution aﬀect the out-of-sample perfor-

mance of the proposed model?

In order to address the ﬁrst research question, Q1, we need to deﬁne some notion of optimality

for the prediction and prescription rules. Following [11] the quality of these rules is estimated using

an out-of-sample disappointment, i.e., the probability that the nominal expected loss in (1), i.e., the
expected loss under the nominal distribution Q∗, is underestimated by the predicted expected loss.
Next, we formulate prediction and prescription problems, which goal is to ﬁnd the least conservative

prediction and prescription rules, whose out-of-sample disappointment decays exponentially for every
thinkable data-generating distribution Q∗, as the sample size tends to inﬁnity.

It turns out that the outlined prediction and prescription problems are, in fact, vector optimization

problems over a functional space; see, e.g., [19]. For this reason, we distinguish between strongly and

weakly optimal solutions. A strongly optimal solution of a vector optimization problem is strictly

better than all other feasible solutions with respect to some speciﬁed partial order and a weakly optimal

solution, in turn, is not dominated by any other feasible solution. We prove that under some additional
assumptions on the objective criterion in (1) and the nominal distribution Q∗, there exists a rather
broad class of weakly optimal solutions for both prediction and prescription problems. In particular,

one of these solutions can be obtained by solving a DRO problem subject to all probability distributions

with given component-wise relative entropy distances from the empirical marginal distributions. We

demonstrate that this solution inherits some properties of a strongly optimal solution and thoroughly

analyze its theoretical properties.

With respect to the second research question, Q2, our solution procedure for the aforementioned

DRO problem is divided into two consecutive stages.

In the ﬁrst stage we retrieve the worst-case

expected costs by solving univariate convex optimization problems for each component of the cost

vector; in the second stage we solve a deterministic version of the underlying mixed-integer program-

ming (MIP) problem. As a result, the DRO problem can be solved rather eﬀectively whenever there

exists an eﬀective algorithm for the nominal MIP problem.

One natural limitation of our approach is that the objective criterion in (1) is assumed to be linear

in the uncertain parameters. Intuitively, under this assumption we may shrink the probability space to

4

the set of univariate distributions and consider component-wise ambiguity sets. It can also be argued

that a straightforward implementation of the DRO approach from [11] with a discrete set of feasible

decisions results in a substantially non-linear MIP problem, which cannot be solved at hand using

oﬀ-the-shelf solvers; see Section 4.1 for more details.

Finally, we consider the research question Q3 in our numerical study, where the out-of-sample

performance of the proposed model in analyzed with respect to several classes of combinatorial opti-

mization problems. From the practical perspective, we demonstrate that solutions with a reasonably

good quality can be obtained whenever the items with lower expected costs are observed more often

than the items with higher expected costs. The latter observation seems to be relevant, e.g., if the

historical data is collected by trial and errors through multiple decision epochs; recall our discussion of

the online learning problem settings at the beginning of this section. In addition, we compare the DRO

approach with several benchmark solution approaches based on measure concentration inequalities or

truncation of the training data set and using the model of Van Parys et al. [11].

The remaining structure of the paper is summarized as follows:

• In Section 2 we introduce our prediction and prescription problems (that are motivated and
similar to the related meta-optimization problems in [11]) under the assumption that the data

set is incomplete. We also deﬁne problem-speciﬁc asymptotic and ﬁnite sample performance

guarantees.

• Section 3 provides some analysis of the prediction and prescription problems. In particular, we

focus on the quality and computational tractability of the proposed DRO approach.

• In Section 4 we provide numerical experiments with applications to data-driven shortest path and
unweighted knapsack problems. The DRO approach is also compared with several benchmark

approaches in terms of their average out-of-sample performance.

• Finally, Section 5 concludes the paper and outlines possible directions for future research.

Notation. All vectors and matrices are labelled by bold letters. The natural logarithm for some
q ∈ R>0 is denoted as ln(q). Furthermore, we adapt the conventions 0 ln( 0
0 ) = ∞
for any q, q(cid:48) ∈ R>0. The space of all probability distributions with a support S ⊆ Rn, n ∈ Z>0, is
denoted by Q0(S) and for any distribution Q ∈ Q0(S) we denote by Q = (Q1, . . . , Qn)(cid:62) a vector
of the associated marginal distributions. We use 1{Z} as an indicator of some logical statement Z.
Finally, the probability of Z with respect to some distribution Q ∈ Q0(S) is referred to as PrQ{Z}
(the index Q is omitted, if the data-generating distribution is clear from the context).

q ) = 0 and q(cid:48) ln( q(cid:48)

2. Problem formulation

2.1. Modeling assumptions and terminology

As brieﬂy outlined in Section 1, we consider the stochastic programming problem (1) with a mixed-
integer set of feasible decisions X ⊆ Zn1 × Rn2 and a random cost vector c ∈ Rn1+n2. For simplicity
of exposition we set n := n1 + n2 and deﬁne a set of indexes A := {1, . . . , n}. The cost vector c is

5

assumed to be governed by some unknown nominal distribution Q∗ ∈ Q0(Rn), which is only observable
through a ﬁnite data set

(cid:98)C := {((cid:98)ca,1, . . . , (cid:98)ca,Ta)(cid:62), a ∈ A},
Speciﬁcally, for each component ca, a ∈ A, of the cost vector c we presume existence of Ta ∈ Z>0
independent identically distributed (i.i.d.) samples from the related marginal distribution Q∗
a.

(2)

Throughout the paper we make the following modeling assumptions:

A1. Each component of the cost vector c is strictly positive and has a ﬁnite discrete support, i.e.,

ca ∈ Sa := {za,1, . . . , za,da}

(3)

for some za,i > 0, i ∈ {1, . . . , da} and a ∈ A.

A2. Any feasible decision is nontrivial, nonnegative and bounded, i.e., 0 /∈ X and for every x ∈ X

there exists some u ∈ Rn

+ such that xa ∈ [0, ua], a ∈ A.

A3. We assume that γ(c, x) = c(cid:62)x in the deﬁnition of the stochastic optimization problem (1).

The motivation behind Assumption A1 is two-fold. First, it is argued in [11] that the case of

continuous support requires more subtle mathematical techniques and makes the problem of ﬁnding

optimal prediction and prescription rules substantially more diﬃcult. For this reason, we leave an

extension of our problem setting to the case of continuous support as a future research direction.

Secondly, Assumption A1 indicates that each component of the cost vector c has its individual support

and, hence, there is no initial information about some correlation between the components. The need

of individual support sets can be motivated in the context of online combinatorial optimization [15, 16],

where the expected rewards are estimated individually for each action and there is no correlation

information involved. On the other hand, the related assumption of Van Parys et al. [11] requires

complete knowledge of the support of the cost vector c. Put diﬀerently, even in the absence of the

training data set, the decision-maker in [11] has some initial information about possible relations

between the components of vector c.

Next, Assumption A2 is rather standard in the discrete optimization literature and holds for the

majority of practical decision-making problems; see, e.g., [20]. Finally, Assumptions A1 and A3 imply
that the objective function in (1) is completely deﬁned by the marginal probability distributions Q∗
a,
a ∈ A, induced by the nominal distribution Q∗. In other words, we observe that

f (x, Q∗) = EQ∗{γ(c, x)} =

(cid:88)

a∈A

EQ∗

a{ca}xa

(4)

Furthermore, if the loss function γ(c, x) does not admit such a component-wise decomposition, then

we cannot even compute the empirical mean of γ(c, x) as the associated training data set is incomplete.

At the same time, some more complicated forms of the objective criteria in (1) are brieﬂy discussed

within Section 5.

6

Below we provide some technical deﬁnitions that are used to formulate our prediction and prescrip-

tion problems. Admittedly, our deﬁnitions are somewhat similar to the terminology of Van Parys et al.

[11] but account incomplete knowledge of the training data set.

First, in view of Assumption A1, the set of all marginal probability distributions for each a ∈ A

can be deﬁned as:

Qa :=

(cid:110)

Q ∈ Q0(Sa) : 0 ≤ qa,i ≤ 1 ∀i ∈ {1, . . . , da},

qa,i = 1

(cid:111)
,

da(cid:88)

i=1

(5)

where qa,i denotes the probability that ca equals to za,i, i ∈ {1, . . . , da}. Furthermore, taking into
account the form of the objective function (4) it suﬃces to consider the following ambiguity set of

joint distributions:

Q :=

(cid:110)

Q ∈ Q0(S1 × . . . × Sn) : Qa ∈ Qa ∀a ∈ A

(cid:111)

(6)

As outlined earlier, for any joint distribution Q ∈ Q we denote by Q = (Q1, . . . , Qn)(cid:62) the vector of
associated marginal distributions.

The key idea of our theoretical analysis is to ﬁnd, in a sense, optimal approximations of the nominal
expected loss f (x, Q∗) and an optimal decision x∗(Q∗) ∈ argminx∈X f (x, Q∗) in (1) by some functions
of empirical marginal distributions.

Deﬁnition 1 (Empirical marginal distributions). A univariate probability distribution (cid:98)Qa,Ta ∈
Qa such that

(cid:98)qa,i := Pr{ca = za,i} =

1
Ta

Ta(cid:88)

j=1

1{(cid:98)ca,j = za,i} ∀i ∈ {1, . . . , da}

(7)

(cid:3)

is referred to as an empirical marginal distribution of ca.

In other words, by leveraging the available data set (2) we construct empirical estimators (cid:98)Qa,Ta of the
nominal marginal distributions Q∗
a for each a ∈ A. Any vector of empirical marginal distributions
that satisﬁes equation (7) is denoted as

(cid:98)Q(T1, . . . , Tn) := ( (cid:98)Q1.T1, . . . , (cid:98)Qn,Tn)(cid:62),

where the dependence on T1, . . . , Tn is usually omitted in order to streamline the notations.

As a result, the deﬁnitions of prediction and prescription rules can be formalized as follows; we

also refer to the related deﬁnition in [11].

Deﬁnition 2 (Prediction and prescription rules). A function ˆf : X ×Q1 ×. . .× Qn → R is called
a data-driven prediction rule, if ˆf (x, (cid:98)Q) is used as an approximation of f (x, Q∗). Furthermore, a func-
ˆf (x, Q)
tion ˆx : Q1 ×. . .×Qn → X is an associated data-driven prescription rule, if ˆx(Q) ∈ argminx∈X
for any feasible Q ∈ Q and ˆx( (cid:98)Q) is used as an approximation of x∗(Q∗).
(cid:3)

Following [11] we assess the quality of approximation by using an out-of-sample disappointment.

7

Deﬁnition 3 (Out-of-sample disappointment). For any prediction rule ˆf and feasible decision
x ∈ X an out-of-sample prediction disappointment is given by:

PrQ∗

(cid:111)
(cid:110)
f (x, Q∗) > ˆf (x, (cid:98)Q)
,

Similarly, for a given prediction-prescription pair ( ˆf , ˆx)

PrQ∗

(cid:111)
(cid:110)
f (ˆx( (cid:98)Q), Q∗) > ˆf (ˆx( (cid:98)Q), (cid:98)Q)

is reﬀered to as an out-of-sample prescription disappointment.

(8)

(9)

(cid:3)

The out-of-sample disappointment for ﬁxed x ∈ X quantiﬁes the probability that the nominal
expected loss f (x, Q∗) is underestimated by the predicted expected loss ˆf (x, (cid:98)Q). It can also be argued
that, if a decision-maker aims at minimizing its loss, then underestimated losses are usually more

harmful than overestimated losses; see, e.g., [21] for some basic concepts of worst-case analysis in

classical decision theory.

As outlined in Section 1.2, we focus on a class of prediction rules that cater an exponential decay rate

for the out-of-sample prediction disappointment irrespective of a decision x ∈ X and a data-generating
distribution Q∗ ∈ Q, as the sample size tends inﬁnity. In other words, we set Tmin := mina∈A Ta we
exploit the following asymptotic and ﬁnite sample performance guarantees:

• Asymptotic guarantee. The out-of-sample prediction disappointment (8) decays exponen-

tially at some rate r > 0 when Tmin goes to inﬁnity, i.e.,

lim sup
Tmin→+∞

1
Tmin

(cid:16)

(cid:110)
f (x, Q∗) > ˆf (x, (cid:98)Q(T1, . . . , Tn))

(cid:111)(cid:17)

Pr

ln

≤ −r ∀x ∈ X, ∀Q∗ ∈ Q (AG)

• Finite sample guarantee. Let g(T ) be a given function that decays exponentially at some
rate r > 0 as T goes to inﬁnity. Then for any Ta ∈ Z>0, a ∈ A, the out-of-sample prediction
disappointment is bounded from above by g(Tmin), i.e.,

(cid:111)
(cid:110)
f (x, Q∗) > ˆf (x, (cid:98)Q(T1, . . . , Tn))

Pr

≤ g(Tmin) ∀x ∈ X, ∀Q∗ ∈ Q, ∀Ta ∈ Z>0, a ∈ A (FG)

In the following example we demonstrate that the ﬁnite sample guarantee (FG) holds for a rather

simple class of prediction rules. That is, we may approximate the expected costs in (1) by the sum of

empirical mean and some positive constants.

Example 1 (Hoeﬀding bounds). Let za = mini∈{1,...,da} za,i, za = maxi∈{1,...,da} za,i for each a ∈ A
and Ax := {a ∈ A : xa > 0} (cid:54)= ∅; recall Assumption A2. We assume that the prediction rule ˆf for
some ﬁxed x ∈ X is deﬁned as:

ˆf (x, (cid:98)Q) =

min

(cid:88)

a∈A

(cid:110) 1
Ta

Ta(cid:88)

j=1

(cid:98)ca,j + εa; za

(cid:111)

xa

(10)

8

where εa > 0 are some positive constants deﬁned below and za serves as an upper bound on the
expected costs for each a ∈ A. Using equation (4) we observe that the out-of-sample prediction

disappointment (8) can be bounded from above as:

(cid:110)
f (x, Q∗) > ˆf (x, (cid:98)Q)

(cid:111)

Pr

= Pr

(cid:110) (cid:88)

(cid:16)

EQ∗

a{ca} − min

(cid:110) 1
Ta

Ta(cid:88)

j=1

(cid:98)ca,j + εa; za

(cid:111)(cid:17)

(cid:111)

xa > 0

a∈A

(cid:110) (cid:95)

(cid:16)

Pr

EQ∗

a{ca} − min

(cid:110) 1
Ta

Ta(cid:88)

j=1

(cid:98)ca,j + εa; za

(cid:111)

(cid:17)(cid:111)

> 0

≤

a∈Ax

(cid:88)

Pr

a∈Ax

(cid:110)

EQ∗

a{ca} > min

(cid:110) 1
Ta

Ta(cid:88)

j=1

(cid:98)ca,j + εa; za

(cid:111)(cid:111)

≤

(cid:88)

Pr

a∈Ax

(cid:88)

(cid:16)

a∈Ax

Pr

(cid:110)(cid:16)

EQ∗

a{ca} >

1
Ta

Ta(cid:88)

j=1

(cid:98)ca,j + εa

(cid:17) (cid:95) (cid:16)

EQ∗

a{ca} > za

(cid:17)(cid:111)

≤

(cid:110)

EQ∗

a{ca} >

1
Ta

Ta(cid:88)

j=1

(cid:98)ca,j + εa

(cid:111)

+ Pr

(cid:110)

EQ∗

a{ca} > za

(cid:111)(cid:17)

=

≤

(11)

(cid:88)

Pr

a∈Ax

(cid:110)

EQ∗

a{ca} >

1
Ta

Ta(cid:88)

j=1

(cid:111)

(cid:98)ca,j + εa

The ﬁrst inequality in (11) exploits Assumption A2 and can be checked by contradiction, while

the second inequality is implied by the standard union bound. In the following we use, respectively,
the deﬁnition of minimum, the union bound and the fact that EQ∗

a{ca} ≤ za for every a ∈ A.

Next, for any given decay rate r > 0 and δa ∈ (0, 1), a ∈ A, such that (cid:80)

a∈A δa = 1 we may set

εa = (za − za)

(cid:114) rTmin − ln δa
2Ta

(12)

By using Hoeﬀding’s inequality [22] for the sum of bounded i.i.d. random variables we observe that

(cid:110)

EQ∗

a{ca} −

Pr

1
Ta

Ta(cid:88)

j=1

(cid:98)ca,j > −εa

(cid:111)

(cid:16)

≤ exp

− 2Ta

(cid:17)2(cid:17)

(cid:16)

εa
za − za

= δae−rTmin

and, thus, for any T1, . . . , Tn ∈ Z>0

(cid:110)
f (x, Q∗) > ˆf (x, (cid:98)Q)

(cid:111)

≤

Pr

(cid:88)

δae−rTmin ≤ e−rTmin

a∈Ax

The upper bounding function g(Tmin) := e−rTmin has an exponential decay rate r, i.e.,

lim sup
Tmin→+∞

1
Tmin

ln g(Tmin) = −r

We conclude that the prediction rule (10) with the parameters εa, a ∈ A, given by equation (12)
satisﬁes both the ﬁnite-sample and asymptotic performance guarantees, (FG) and (AG).
(cid:3)

9

2.2. Prediction and prescription problems

In this section we introduce prediction and prescription problems, which are also motivated by the

related formulations in [11]. First, we impose a partial order on the space of prediction rules. That
is, a prediction rule ˆf1 not strictly dominates a prediction rule ˆf2, i.e., ˆf1 (cid:22) ˆf2, if and only if

ˆf1(x, Q1, . . . , Qm) ≤ ˆf2(x, Q1, . . . , Qm) ∀x ∈ X, ∀Q ∈ Q

Also, let F be a set of all real-valued functions on the set X × Q1 × . . . × Qm. Then a prediction
problem can be expressed as:

ˆf (·)

min
ˆf (·)∈F

( w.r.t. (cid:22))

(P)

s.t.

lim sup
Tmin→+∞

1
Tmin

(cid:16)

(cid:110)
f (x, Q∗) > ˆf (x, (cid:98)Q)

(cid:111)(cid:17)

Pr

ln

≤ −r ∀x ∈ X, ∀Q∗ ∈ Q

Formally, in the vector optimization problem (P) we aim at ﬁnding the least conservative prediction

rule (with respect to the partial order deﬁned above) that satisﬁes the asymptotic guarantee (AG).

Analogously, we deﬁne a partial order on the space of prescription rules and introduce a prescription
problem. That is, a prediction-prescription pair ( ˆf1, ˆx1) not strictly dominates a pair ( ˆf2, ˆx2), if the
following set of inequalities holds:

ˆf1(ˆx1(Q), Q) ≤ ˆf2(ˆx2(Q), Q) ∀Q ∈ Q,

(14)

where we recall that Q = (Q1, . . . , Qn)(cid:62) is a vector of marginal distributions induced by some Q ∈ Q.
Let X be a set of all pairs ( ˆf , ˆx), where ˆf ∈ F and ˆx is the prescription rule induced by ˆf . Then a
prescription problem can be formulated as follows:

( ˆf , ˆx)

( w.r.t. (cid:22))

min
( ˆf ,ˆx)∈X

(P (cid:48))

s.t.

lim
Tmin→+∞

sup

1
Tmin

(cid:16)

(cid:110)
f (ˆx( (cid:98)Q), Q∗) > ˆf (ˆx( (cid:98)Q), (cid:98)Q)

(cid:111)(cid:17)

Pr

ln

≤ −r ∀Q∗ ∈ Q

Table 1 outlines some comparison of the prediction and prescription problems, (P) and (P (cid:48)), with
the related meta-optimization problems in [11]. In fact, the key diﬀerence is stipulated by our con-

struction of the data set (2) and the objective criterion in (1); recall Assumption A3. Furthermore, in

view of Deﬁnition 2, our prediction and prescription rules are modeled as some functions of marginal

distributions, but not a joint distribution of the cost vector c. In contrast to [11], we cannot guar-
antee that the vector optimization problems (P) and (P (cid:48)) admit a unique strongly optimal solution.
However in the next section we demonstrate that both problems (P) and (P (cid:48)) admit a particular class
of weakly optimal solutions that possess some properties of a strongly optimal solution.

10

Properties

Our formulation

Formulation of Van Parys et al. [11]

Support set

a discrete set for each component ca, a ∈ A

a discrete or continuous set
for the cost vector c

Sample size

Ta ∈ Z>0 for each a ∈ A

the same size T ∈ Z>0 for each a ∈ A

Set of feasible decisions

X is a bounded subset of Zn1

+ × Rn2

+

Objective function in (1)

γ(c, x) = c(cid:62)x

Prediction rules

an arbitrary function
of decisions and
marginal distributions

X is bounded subset of Rn

γ(c, x) is any continuous function
with respect to x ∈ X

a continuous function
of decisions and a joint distribution

Table 1: The table summarizes the diﬀerence between our formulations (P) and (P (cid:48)) and the related formulations in [11].

Despite the fact that our approach yields some simpliﬁcations compared to the approach of

Van Parys et al. [11], we may capture a number of discrete optimization problem settings, for which

an application of the model in [11] is substantially limited; recall our discussion of the research ques-

tions Q1 and Q2 in Section 1.2. As a byproduct, our construction of the training data set gives rise

to some practical insights concerning a relation between the sample size for each component of the

cost vector and the form of the nominal distribution; see the research question Q3 and Section 4 for

further details.

3. Solution techniques

In this section we analyze a particular class of distributionally robust prediction and prescription

rules based on a relative entropy distance from empirical marginal distributions. More precisely, for

any ﬁxed a ∈ A the relative entropy distance or Kullback–Leibler divergence [23] between an empirical
distribution (cid:98)Qa,Ta and some other distribution Qa ∈ Qa is deﬁned as follows:

DKL( (cid:98)Qa,Ta (cid:107) Qa) =

da(cid:88)

i=1

(cid:98)qa,i ln (cid:98)qa,i
qa,i

The idea is to account all marginal distributions within relative entropy balls of some radius
ra ∈ R>0 centered at the empirical marginal distributions (cid:98)Qa,Ta, a ∈ A. Speciﬁcally, we consider the
following component-wise and joint ambiguity sets:

(cid:98)Qa :=

(cid:110)

Qa ∈ Q0(Sa) : 0 ≤ qa,i ≤ 1 ∀i ∈ {1, . . . , da},

(cid:98)Q :=

(cid:111)
(cid:110)
Q ∈ Q0(S1 × . . . × Sn) : Qa ∈ (cid:98)Qa ∀a ∈ A

qa,i = 1, DKL( (cid:98)Qa,Ta (cid:107) Qa) ≤ ra

(cid:111)

da(cid:88)

i=1

(16)

(17)

The parameters ra, a ∈ A, are used to control the exponential decay rate induced by the asymptotic
guarantee (AG); see Theorem 1 and Section 3.3 for further details.

Based on the deﬁnitions above, for some ﬁxed x ∈ X we may deﬁne the following distributionally

11

robust prediction and prescription rules:

ˆfDR(x, (cid:98)Q) = max
Q∈ (cid:98)Q

EQ{c(cid:62)x} =

(cid:88)

(cid:16)

a∈A
ˆfDR(x, (cid:98)Q)

(cid:17)
EQa{ca}

xa

max
Qa∈ (cid:98)Qa

(18a)

(18b)

ˆxDR( (cid:98)Q) ∈ argminx∈X

The remainder of this section is organized as follows. In Sections 3.1 and 3.2 we establish some

theoretical properties of (18a) and (18b) in relation to the prediction and prescription problems (P)
and (P (cid:48)), respectively. In particular, we introduce a dual reformulation of the maximization problem in
(18a) and thereby obtain a linear MIP reformulation of the DRO problem in (18b). Finally, Section 3.3
provides some additional discussion on the choice of the parameters ra, a ∈ A, in the ambiguity set (17).

3.1. Properties of the distributionally robust prediction rule

3.1.1. Dual reformulation

In this section we demonstrate that optimization problem (18a) admits a dual reformulation. In

addition, the related DRO problem

min
x∈X

ˆfDR(x, (cid:98)Q) = min
x∈X

max
Q∈ (cid:98)Q

EQ{c(cid:62)x}

(DRO)

can be solved by leveraging n one-dimensional convex optimization problems and a single deterministic

MIP problem. The following result holds.

Proposition 1. The distributionally robust optimization problem (DRO) is equivalent to the following

minimization problem:

min
x∈X

(cid:88)

a∈A

min
βa≥za

(cid:16)

βa − e−ra

da(cid:89)

(βa − za,i)(cid:98)qa,i

(cid:17)

xa,

i=1

(19)

where za = maxi∈{1,...da} za,i for each a ∈ A.

Proof. Note that a min-min reformulation of (DRO) can be obtained by dualizing the worst-case

expectation problems for each particular a ∈ A, that is,

max
qa

EQa{ca} =

da(cid:88)

i=1

za,iqa,i

s.t. 0 ≤ qa,i ≤ 1 ∀i ∈ {1, . . . , da}

da(cid:88)

i=1

da(cid:88)

i=1

qa,i = 1

(cid:98)qa,i ln (cid:98)qa,i
qa,i

≤ ra

(20a)

(20b)

(20c)

(20d)

It is rather straightforward to verify that the feasible set in (20) is convex and the objective function

is linear. By Proposition 2 in [11] the dual problem can be reduced to a one-dimensional convex

12

optimization problem of the form:

(cid:16)

min
βa

βa − e−ra

da(cid:89)

(βa − za,i)(cid:98)qa,i

(cid:17)

i=1

s.t. βa ≥ za

(21a)

(21b)

If we combine the minimization over x ∈ X and the minimization over the dual variables βa, a ∈ A,
then the result follows.

Proposition 1 provides a two-stage solution approach for the DRO problem (DRO); recall the

research question Q2. In the ﬁrst stage we obtain the worst-case expected costs for each a ∈ A by

solving univariate convex optimization problems (21), and in the second stage we solve an instance

of the deterministic MIP problem induced by (19). Despite the fact that the feasible set in (21) is

unbounded, it can be argued that the optimal solution is unique; we refer to the proof of Proposition 2

in [11] for more details.

3.1.2. Asymptotic and ﬁnite sample guarantees

It turns out that the distributionally robust prediction rule (18a) with an appropriate choice of
the parameters ra, a ∈ A, satisﬁes both ﬁnite sample and asymptotic guarantees, (FG) and (AG). In
particular, this assertion follows from a strong large deviation principle; see, e.g., [24, 11], which is

formulated and brieﬂy discussed below.

Proposition 2 (Strong LDP, univariate case). Assume that a ∈ A is ﬁxed and the samples
(cid:98)ca,1, . . . , (cid:98)ca,Ta are drawn independently from some marginal distribution Q∗
a ∈ Qa, where Qa is given
by equation (5). Then for every Borel set Da ⊆ Qa and any Ta ∈ N we have:

(cid:110)
(cid:98)Qa,Ta ∈ Da

Pr

(cid:111)

≤ (Ta + 1)da exp

(cid:16)

− Ta

inf
Qa∈Da

DKL(Qa (cid:107) Q∗
a)

(cid:17)

(LDP 1)

Proof. See Theorem 2 in [11].

In other words, if Q∗

a /∈ Da, then the empirical distribution (cid:98)Qa,Ta belongs to Da with a probability
that decays exponentially in the sample size, Ta. In the next result we mimic the idea of Example 1.
That is, the out-of-sample prediction disappointment (8) with respect to (18a) is bounded from above
by some function that decays exponentially in Tmin.

Theorem 1. Assume that r > 0 is a required exponential decay rate, Tmin = mina∈A Ta and δa ∈ (0, 1)
are such that (cid:80)

a∈A δa = 1. Then the prediction rule (18a) with the parameters

ra =

(cid:16)

1
Ta

da ln(Ta + 1) + rTmin − ln δa

(cid:17)

∀a ∈ A

(22)

is feasible in (P).

13

Proof. We observe that ˆfDR ∈ F by construction and, thus, it suﬃces to prove that ˆfDR satisﬁes the
asymptotic guarantee (AG). Let Ax = {a ∈ A : xa > 0} (cid:54)= ∅ for each x ∈ X. We observe that the
out-of-sample prediction disappointment (8) is bounded from above as:

(cid:110)
f (x, Q∗) > ˆfDR(x, (cid:98)Q)

(cid:111)

Pr

= Pr

(cid:110)

EQ{c(cid:62)x} > 0

(cid:111)

=

(cid:110) (cid:88)

(cid:16)

Pr

a∈A

EQ∗

a{ca} − max
Qa∈ (cid:98)Qa

(cid:17)

EQa{ca}

(cid:111)

xa > 0

a{ca} − max
Qa∈ (cid:98)Qa

EQa{ca} > 0

(cid:17)(cid:111)
,

EQ∗{c(cid:62)x} − max
Q∈ (cid:98)Q
EQ∗

(cid:110) (cid:95)

≤ Pr

(cid:16)

a∈Ax

where the last inequality exploits Assumption A2 and can be checked by contradiction. Furthermore,

the inequality

EQ∗

a{ca} − max
Qa∈ (cid:98)Qa

EQa{ca} > 0

for some a ∈ A implies that Q∗
observe that

a /∈ (cid:98)Qa. Hence, by leveraging the union bound and Proposition 2 we

(cid:110) (cid:95)

(cid:16)

Pr

EQ∗

a∈Ax
(cid:88)

a∈Ax

Pr

a{ca} − max
Qa∈ (cid:98)Qa
(cid:111)

Q∗

a /∈ (cid:98)Qa

=

(cid:110)

EQa{ca} > 0

(cid:17)(cid:111)

(cid:88)

Pr

≤

(cid:110)

EQ∗

(cid:88)

Pr

(cid:110)
DKL( (cid:98)Qa,Ta (cid:107) Q∗

a) > ra

a∈Ax

a{ca} − max
Qa∈ (cid:98)Qa

(cid:111)
EQa{ca} > 0

≤

(cid:111)

≤

(cid:88)

a∈Ax

(Ta + 1)dae−Tara

a∈Ax

By leveraging equation (22) we conclude that

(Ta + 1)dae−Tara =

(cid:88)

a∈Ax

(cid:88)

a∈Ax

δae−Tminr ≤ e−Tminr

Hence,

lim sup
Tmin→∞

1
Tmin

(cid:16)

(cid:110)
f (x, Q∗) > ˆfDR(x, (cid:98)Q)

(cid:111)(cid:17)

Pr

ln

≤ lim sup
Tmin→∞

1
Tmin

ln(e−Tminr) = −r

and the proof follows.

We infer that the distributionally robust prediction rule (18a) with the parameters ra, a ∈ A, given
by equation (22) is feasible in the prediction problem (P). In the next section we demonstrate that

(18a) is, in fact, a weakly optimal solution of (P).

3.1.3. Weak optimality

At ﬁrst, we introduce some problem-speciﬁc deﬁnitions of weak and strong optimality. In particu-

lar, we introduce a class of weakly optimal solutions that dominate all other feasible solutions at some

given set of points.

Deﬁnition 4 (Weak optimality, prediction). A prediction rule ˆf ∈ F is called weakly optimal
(W -optimal) for (P), if it is feasible in (P) and any other prediction rule ˆf (cid:48) ∈ F that satisﬁes

ˆf (cid:48)(x, Q) < ˆf (x, Q) ∀x ∈ X, ∀Q ∈ Q

14

is infeasible in (P). Furthermore, ˆf ∈ F is strongly optimal with respect to a nonempty set (cid:101)Z ⊆ X × Q
(S -optimal w.r.t. (cid:101)Z), if it is feasible in (P) and any other feasible prediction rule, ˆf (cid:48) ∈ F, satisﬁes

ˆf (cid:48)(x, Q) ≥ ˆf (x, Q) ∀(x, Q) ∈ (cid:101)Z

(cid:3)

As a remark, if (cid:101)Z = X × Q in the deﬁnition above, then the related prediction rule is strongly
optimal in a common sense. It can be also argued that an S -optimal w.r.t (cid:101)Z (cid:54)= ∅ prediction rule is
also W -optimal, but not vice versa. We illustrate this fact with the following counterexample.

Example 2. We consider the following vector optimization problem:

φ(y)

(23)

min
φ(·)
(cid:90) 1

s.t.

φ(y)dy ≥

0

φ(0) ≥ 0

1
2

where φ : [0, 1] → R is an integrable function on the interval [0, 1]. It is also assumed that φ1 (cid:22) φ2 if
and only if φ1(y) ≤ φ2(y) for all y ∈ [0, 1].

We note the function φ1(y) := 0.5 is a W -optimal solution of (23) as the function (cid:101)φ1(y) := 0.5 − ε
does not satisfy the ﬁrst constraint in (23) for any ε > 0. However this solution is manifestly not
S -optimal w.r.t. any nonempty set (cid:101)Z ⊆ [0, 1], as the function φ1(y) can be decreased at any ﬁxed
point y0 ∈ [0, 1] without changing the value of the integral in (23). On the other hand, the func-
tion φ2(y) := y is an S -optimal w.r.t. (cid:101)Z = {0} solution of (23) as any function (cid:101)φ2(y) such that
(cid:101)φ2(0) < φ2(0) = 0 is infeasible in (23).

In summary, we observe that the functions φ1(y) and φ2(y) are not comparable in general. However
the function φ2(y) can be posed as the best solution at least at one point, i.e., y = 0. One possible
interpretation is that the more “rich” the set (cid:101)Z, the more advantage an S-optimal w.r.t. (cid:101)Z solution
(cid:3)
has compared to some typical W -optimal solutions.

It turns out that W -optimality holds for a rather broad class of prediction rules including the

prediction rule (10) from Example 1 and the distributionally robust prediction rule (18a). The following

result takes place.

Proposition 3. Let za = maxi∈{1,...,da} za,i for any a ∈ A. Then any prediction rule ˆf ∈ F that is
feasible in (P) and satisﬁes the following upper bound:

ˆf (x, Q) ≤

(cid:88)

a∈A

zaxa ∀x ∈ X, ∀Q ∈ Q

(24)

is W-optimal in (P).

Proof. See Appendix A.

15

Proposition 3 does not exploit the structure of ˆf , but only requires that the estimate of expected
loss, ˆf (x, Q), satisﬁes the asymptotic guarantee (AG) and does not exceed the maximal possible loss;
see the right hand side of equation (24). In the following we demonstrate that the distributionally
robust prediction rule (18a) is also S -optimal w.r.t. to some speciﬁed set (cid:101)Z ⊆ X × Q. The subsequent
theoretical results exploit the following additional assumption on the individual support sets Sa, a ∈ A:

A1’. For any a, b ∈ A, a (cid:54)= b, we have da = db and there exist some wa,b ∈ R+ and va,b ∈ R such that

zb,i = wa,b za,i + va,b for any i ∈ {1, . . . , da}.

In fact, Assumption A1’ entails an aﬃne dependence between the elements of individual support
sets. Besides that, the number of elements da, a ∈ A, is assumed to be the same for all components of
the cost vector c. While Assumption A1’ limits the number of candidate data-generating distributions,
we may still account diﬀerent cost ranges for the components of c by adapting the constants wa,b and
va,b for a, b ∈ A, a (cid:54)= b. The following result holds.

Theorem 2. Assume that r > 0 is a required exponential decay rate and Assumption A1’ holds. Also,

let

(cid:101)Z = X ×

(cid:110)

Q ∈ Q : qa,i = qb,i > 0 ∀i ∈ {1, . . . , da}, ∀a, b ∈ A, a (cid:54)= b

(25)

(cid:111)
,

where qa,i are rational and denote the probability that ca equals za,i. Then the distributionally robust
prediction rule (18a) with parameters ra, a ∈ A, given by equation (22) with δa = 1
n is S-optimal w.r.t.
(cid:101)Z for (P).

Proof. We need to show for any ﬁxed ((cid:101)x, (cid:101)Q) ∈ (cid:101)Z every other prediction rule ˆf (cid:48) ∈ F that satisﬁes

ε := ˆfDR((cid:101)x, (cid:101)Q) − ˆf (cid:48)((cid:101)x, (cid:101)Q) > 0

(26)

is infeasible in (P); recall the deﬁnition of S -optimality.

Step 1. At the ﬁrst step bound the out-of-sample disappointment

(cid:111)
(cid:110)
f (x, Q∗) > ˆf (cid:48)(x, (cid:98)Q)

Pr

from below at x = (cid:101)x by selecting some marginal probability distributions Q∗
In
this regard, for each a ∈ A we construct a relative entropy ball (cid:98)Qa of the radius ra centered at the
marginal distribution (cid:101)Qa; see equation (16) and Figure 1 for details. In particular, by deﬁnition of
(cid:101)Z the marginal distributions (cid:101)Qa, a ∈ A, are the same probability distributions but equipped with
diﬀerent support sets Sa.

a for each a ∈ A.

Then we set b ∈ argmina∈A Ta and deﬁne the worst-case marginal distributions as:

Q(w)

a ∈ argmaxQa∈ (cid:98)Qa

EQa{ca}

In view of Assumption A1’, we have da = d ∈ Z>0 for each a ∈ A. Furthermore, substituting δa = 1
n ,

16

(cid:101)Qb

r b

Q∗
a

Q(w)
b

Figure 1: A rough illustration of the notations used in the ﬁrst step of the proof of Theorem 3. The relative entropy ball
(cid:98)Qb is depicted as an Euclidean ball of the radius rb. The red dot corresponds to the nominal marginal distribution Q∗
a,
which is the same for all a ∈ A.

a ∈ A, into equation (22) implies that

ra =

(cid:16)

1
Ta

d ln(Ta + 1) + rTmin − ln

(cid:17)

1
n

∀a ∈ A

Hence, there exists T0 ∈ Z>0 such that for every Tmin = Tb ≥ T0

rb ≥ ra ∀a ∈ A, a (cid:54)= b

Then using Assumption A1’ for any ﬁxed a ∈ A, a (cid:54)= b, and Tmin ≥ T0 we observe that

E

Q(w)
a

{ca} =

d
(cid:88)

i=1

q(w)
a,i za,i = max
Qa∈ (cid:98)Qa

d
(cid:88)

i=1

wb,a max
Qa∈ (cid:98)Qa

(cid:16) d
(cid:88)

i=1

(cid:17)

qa,izb,i

d
(cid:88)

qa,iza,i = max
Qa∈ (cid:98)Qa
(cid:16) d
(cid:88)

i=1

+ vb,a ≤ wb,a max
Qb∈ (cid:98)Qb

i=1

qa,i(wb,azb,i + vb,a) =

(cid:17)

qb,izb,i

+ vb,a =

(27)

max
Qb∈ (cid:98)Qb

(cid:16) d
(cid:88)

i=1

qb,i(wb,azb,i + vb,a)

(cid:17)

= E

Q(w)
b

{ca}

The inequality follows from the fact that wb,a > 0 and the relative entropy balls (cid:98)Qa and (cid:98)Qb have the
same center (recall the deﬁnition of (cid:101)Z).

Next, we deﬁne

Q∗

a := λ (cid:101)Qb + (1 − λ)Q(w)

b

∀a ∈ A

(28)

for some λ ∈ (0, 1). By deﬁnition of (cid:101)Z we have (cid:101)qb,i > 0, i ∈ {1, . . . , d}, and, thus, the nominal
probabilities q∗
a,i are also positive and independent of a ∈ A. Consequently, the relative entropy
distance DKL( (cid:101)Qb (cid:107) Q∗

a) is ﬁnite.

We observe that convexity of the relative entropy distance implies that:

DKL( (cid:101)Qb (cid:107) Q∗

a) ≤ λDKL( (cid:101)Qb (cid:107) (cid:101)Qb) + (1 − λ)DKL( (cid:101)Qb (cid:107) Q(w)

b

) ≤ (1 − λ)rb < rb

(29)

17

Furthermore, for any ε(cid:48) > 0 and a ∈ A we may guarantee that for Tmin ≥ T0

EQ∗

a{ca} = λE

(cid:101)Qb

{ca} + (1 − λ)E

Q(w)
b

{ca} > E

Q(w)
b

{ca} − ε(cid:48) ≥ E

Q(w)
a

{ca} − ε(cid:48),

(30)

where the strict inequality can be always satisﬁed with a suﬃciently small λ > 0 and the last inequality

exploits (27).

Let ε(cid:48) = ε

nu , where u = max{ua, a ∈ A} is a maximal upper bound for the decision variables
induced by Assumption A2. Then by leveraging (26) and (30) it can be observed that for Tmin ≥ T0

(cid:88)

EQ∗

f ((cid:101)x, Q∗) =
= ˆfDR((cid:101)x, (cid:101)Q) − ε(cid:48) (cid:88)

a∈A

a∈A

a{ca}(cid:101)xa >

(cid:88)

(cid:16)

a∈A

max
Qa∈ (cid:98)Qa

(cid:48)

(cid:101)xa = ˆf

((cid:101)x, (cid:101)Q) + ε −

EQa{ca} − ε(cid:48)(cid:17)

(cid:101)xa =

ε
nu

(cid:88)

a∈A

(cid:48)

(cid:101)xa ≥ ˆf

((cid:101)x, (cid:101)Q)

(31)

In conclusion, we need to show that (cid:101)Q can be posed as some vector of empirical marginal distribu-
tions. In this particular case using the inequality (31) we may bound the out-of-sample disappointment

from below as:

Pr

(cid:111)
(cid:110)
f ((cid:101)x, Q∗) > ˆf (cid:48)((cid:101)x, (cid:98)Q)

≥ Pr

(cid:110)

(cid:111)

(cid:98)Q = (cid:101)Q

(LB1)

Indeed, by deﬁnition of (cid:101)Z the probabilities (cid:101)qa,i for i ∈ {1, . . . , d} and any a ∈ A are rational.
Therefore, the vector (cid:101)Q can be posed as vector of the same empirical marginal distributions on K ∈ Z>0
samples, where K is the smallest common multiple for the denominators of (cid:101)qa,i, i ∈ {1, . . . , d}.
Furthermore, it is rather straightforward to verify that the same empirical distributions can be obtained
from 2K, 3K, . . . samples as well and without loss of generality we may select K ≥ T0. Eventually,
since the asymptotic guarantee (AG) must hold for any T1, . . . , Tn ∈ Z>0 as Tmin = mina∈A Ta goes
to inﬁnity, we consider the case, in which

Ta ∈ K := {K, 2K, . . .},

∀a ∈ A

(32)

That is, for any T1, . . . , Tn that satisfy equation (32) the lower bound (LB1) holds.
Step 2. The goal of this step is to construct a joint distribution Q∗ with the marginal distributions
deﬁned by equation (28) such that the asymptotic guarantee (AG) for ˆf (cid:48) does not hold at x = (cid:101)x. First,
we assume that the vector of marginal distributions (cid:101)Q is induced by some joint distribution (cid:101)J ∈ Q,
which is itself an empirical distribution on T ∈ K samples. Formally, as the marginal distributions
(cid:101)Qa, a ∈ A, coincide, for any set of indexes (i1, . . . , in)(cid:62) ∈ {1, . . . , d}n and any ﬁxed T ∈ K we may set

Pr

(cid:101)J{c = (z1,i1, . . . , zn,in)(cid:62)} = ji1,...,in =






(cid:101)q1,i, if i1 = . . . = in = i for i ∈ {1, . . . , d}
0, otherwise

Analogously, since the marginal distributions Q∗

a coincide for each a ∈ A, we may deﬁne the nominal

18

distribution, Q∗, as:

PrQ∗{c = (z1,i1, . . . , zn,in)(cid:62)} = q∗

i1,...,in =






q∗
1,i, if i1 = . . . = in = i for i ∈ {1, . . . , d}
0, otherwise

We conclude that for T ∈ K the following lower bound holds:

(cid:110)

Pr

(cid:98)Q = (cid:101)Q

(cid:111)

(T + 1)−dn

≥ Pr

(cid:111)
(cid:110)
(cid:98)J = (cid:101)J
e−T DKL((cid:101)Qb (cid:107) Q∗

≥ (T + 1)−dn

e−T DKL((cid:101)J (cid:107) Q∗) =

b ) ≥ (T + 1)−dn

e−(1−λ)rbT

(LB2)

In particular, the ﬁrst inequality in (LB2) follows from the fact the joint distribution (cid:101)J enforces the
vector of empirical marginal distributions (cid:101)Q. The second inequality exploits that:

(i ) the number of sequences of size T that give rise to the same empirical joint distribution is

bounded from below by (T + 1)−|S|, where |S| = dn is a cardinality of the support of c;

(ii ) the probability of each sequence coincides with e−T DKL((cid:101)J (cid:107) Q∗).

We omit the proof of (i ) and (ii ) for brevity and refer to the proof of Theorem 1 in [11] for details.
The equality in (LB2) is implied by the deﬁnition of the relative entropy distance. Finally, the last
inequality follows from (29).

By combining the lower bounds (LB1) and (LB2) for Tmin := T (k) = kK → +∞, k ∈ {1, 2, . . .},

we observe that

lim sup
Tmin→+∞

lim
k→+∞

1
Tmin
(cid:16)
1
T (k)

(cid:16)

ln

Pr

(cid:110)
f ((cid:101)x, Q∗) > ˆf (cid:48)((cid:101)x, (cid:98)Q)

(cid:111)(cid:17)

≥ lim

k→+∞

1
T (k)

(cid:16)

ln

(T (k) + 1)−dn

e−(1−λ)rbT (k)(cid:17)

=

− dn ln(T (k) + 1) − (1 − λ)(cid:0)d ln(T (k) + 1) + rT (k) − ln

(cid:1)(cid:17)

1
n

= −(1 − λ)r > −r

Thus, the prediction rule ˆf (cid:48) is infeasible in (P) and the result follows.

As outlined earlier, the set (cid:101)Z deﬁned by equation (25) contains a class of points (x, Q), where
x ∈ X is any feasible decision and Q ∈ Q is a joint distributions with the same marginals. At the
same time, the proof of Theorem 2 cannot be applied directly to the case when the parameters da,
a ∈ A, are diﬀerent. In this regard, we formulate an auxiliary result, which exploits some relaxed
version of Assumption A1’ and thereby holds for any da ∈ Z>0. More speciﬁcally, we make the
following assumption:

A1” Suppose that the values za,1, . . . , za,da are sorted in a decreasing order for each a ∈ A. Then for

any a, b ∈ A, a (cid:54)= b, there exist some wa,b ∈ R+ and va,b ∈ R such that

for any i ≤ dmin, where dmin = mina∈A da.

zb,i = wa,b za,i + va,b

19

The following result holds.

Theorem 3. Assume that r > 0 is a required exponential decay rate and Assumption A1” holds.

Also, let

(cid:101)Z = X ×

(cid:110)

Q ∈ Q : qa,i =




qb,i ∈ R>0, if i ≤ dmin



0, otherwise

∀a, b ∈ A, a (cid:54)= b

(cid:111)
,

(33)

where qa,i are rational and denote the probability that ca equals za,i. Then the distributionally robust
prediction rule (18a) with parameters ra, a ∈ A, given by equation (22) with δa = 1
n is S-optimal w.r.t.
(cid:101)Z for (P).

Proof. See Appendix A.

In conclusion, it is still an open question whether (18a) is a strongly optimal solution for (P) or

not. We did not manage to answer this question by using the methodology of Theorem 2 and, thus,

we leave it as a possible direction of future research.

3.2. Properties of the distributionally robust prescriptor

In this section we consider analogous properties of the prescription rule (18b), which is deﬁned by
a pair of functions ( ˆfDR, ˆxDR). As a corollary of Theorem 1, we observe that the pair ( ˆfDR, ˆxDR) is
feasible in (P (cid:48)). That is, the asymptotic guarantee (AG) holds for any feasible decision x ∈ X and, in
particular, for the optimal estimate

ˆxDR( (cid:98)Q) ⊆ argminx∈X

ˆfDR(x, (cid:98)Q)

Next, we slightly modify the deﬁnitions of weak and strong optimality in the context of the prescription
problem (P (cid:48)).

Deﬁnition 5 (Weak and strong optimality, prescription). A prediction-prescription pair ( ˆf , ˆx) is
called weakly optimal (W -optimal) for (P (cid:48)), if it is feasible in (P (cid:48)) and any other prediction-prescription
pair ( ˆf (cid:48), ˆx(cid:48)) that satisﬁes

ˆf (cid:48)(ˆx(cid:48)(Q), Q) < ˆf (ˆx(Q), Q) ∀Q ∈ Q

is infeasible in (P (cid:48)). Furthermore, ( ˆf , ˆx) is strongly optimal with respect to a nonempty set (cid:101)Z ⊆ Q
(S -optimal w.r.t. (cid:101)Z), if it is feasible in (P) and any other feasible pair ( ˆf (cid:48), ˆx(cid:48)) satisﬁes

ˆf (cid:48)(ˆx(cid:48)(Q), Q) ≥ ˆf (ˆx(cid:48)(Q), Q) ∀Q ∈ (cid:101)Z

(cid:3)

Analogously, if (cid:101)Z = Q, then the outlined prediction-prescription pair ( ˆf , ˆx) is strongly optimal in
a common sense. We show that (18b) is strongly optimal with respect to some set (cid:101)Z ⊆ Q for the
prescription problem (P (cid:48)). The following result holds.

20

Theorem 4. Assume that r > 0 is a required exponential decay rate and Assumption A1’ holds. Also,

let

(cid:101)Z =

(cid:110)

Q ∈ Q : qa,i = qb,i > 0 ∀i ∈ {1, . . . , da}, ∀a, b ∈ A, a (cid:54)= b

(34)

(cid:111)
,

where qa,i are rational and denote the probability that ca equals za,i. Then the prediction-prescription
pair ( ˆfDR, ˆxDR) with parameters ra, a ∈ A, given by equation (22) with δa = 1
n is S-optimal w.r.t. (cid:101)Z
for (P (cid:48)).

Proof. The proof, in a sense, reiterates the proof of Theorem 2 with some minor changes. First, we as-
sume that there exists a prediction-prescription pair ( ˆf (cid:48), ˆx(cid:48)), which is less conservative than ( ˆfDR, ˆxDR)
at (cid:101)Q = ( (cid:101)Q1, . . . , (cid:101)Qn) deﬁned as in the proof of Theorem 2, i.e.,

ε := ˆfDR(ˆxDR( (cid:101)Q), (cid:101)Q) − ˆf (cid:48)(ˆx(cid:48)( (cid:101)Q), (cid:101)Q) > 0

In view of Deﬁnition 5, we need to demonstrate that the pair ( ˆf (cid:48), ˆx(cid:48)) is infeasible in (P (cid:48)).

In order to simplify the notations we set (cid:101)x := ˆxDR( (cid:101)Q) and x(cid:48) := ˆx(cid:48)( (cid:101)Q), where (cid:101)x, x(cid:48) ∈ X. Then we

consider the out-of-sample prescription disappointment (9) given by:

(cid:110)
f (ˆx(cid:48)( (cid:98)Q), Q∗) > ˆf (cid:48)(ˆx(cid:48)( (cid:98)Q), (cid:98)Q)

(cid:111)

Pr

and revise (31) as follows:

f (x(cid:48), Q∗) =

(cid:88)

a∈A

ˆfDR(x(cid:48), (cid:101)Q) − ε(cid:48) (cid:88)

a ≥ ˆfDR((cid:101)x, (cid:101)Q) − ε(cid:48) (cid:88)
x(cid:48)

a∈A

a∈A

(x(cid:48), (cid:101)Q) + ε −

ε
nu

(cid:88)

a∈A

(cid:48)

a ≥ ˆf
x(cid:48)

(x(cid:48), (cid:101)Q),

EQ∗

a{ca}x(cid:48)

a >

(cid:88)

(cid:16)

EQa{ca} − ε(cid:48)(cid:17)

x(cid:48)
a =

max
Qa∈ (cid:98)Qa

a∈A
a = ˆf
x(cid:48)

(cid:48)

where the second inequality follows from the deﬁnition of (cid:101)x. We conclude that

(cid:110)
f (ˆx(cid:48)( (cid:98)Q), Q∗) > ˆf (cid:48)(ˆx(cid:48)( (cid:98)Q), (cid:98)Q)

(cid:111)

Pr

≥ Pr

(cid:110)

(cid:98)Q = (cid:101)Q

(cid:111)

and the result follows from the second step in the proof of Theorem 2.

Finally, we formulate the result, which holds for any da ∈ Z>0, a ∈ A; recall Theorem 3.

Theorem 5. Assume that r > 0 is a required exponential decay rate and Assumption A1” holds.

Also, let

(cid:110)

Q ∈ Q : qa,i =

(cid:101)Z =




qb,i ∈ R>0, if i ≤ dmin



0, otherwise

∀a, b ∈ A, a (cid:54)= b

(cid:111)
,

(35)

where qa,i are rational and denote the probability that ca equals za,i. Then the distributionally robust
prescription rule (18b) with parameters ra, a ∈ A, given by equation (22) with δa = 1
n is S-optimal
w.r.t. (cid:101)Z for (P (cid:48)).

Proof. The result follows from Theorems 3 and 4.

21

Theorems 2 and 4 establish that the prediction and prescription rules (18a) and (18b) are optimal
in the asymptotic sense whenever the empirical marginal distributions (cid:98)Qa,Ta, a ∈ A, have aﬃnely
dependent support sets but the same probability mass functions; recall equation (25). Admittedly,

in practice this modeling assumption cannot be realized as the decision-maker is not able to control

the initial data set. Nevertheless, in our numerical experiments we attempt to relax the outlined
ideal situation by leveraging a complete data set (2), i.e., with Ta = T , a ∈ A, obtained from
independent marginal distributions of the same type. If the parameter T is suﬃciently large, then the

obtained empirical marginal distributions usually have a similar form up to some aﬃne transformation.

We demonstrate numerically that in this case our approach outperforms several other benchmark

approaches in terms of their out-of-sample performance; see Section 4.3 and, in particular, Figure 7

for further details.

As we outlined before, most of the previous theoretical results focus on asymptotic performance

guarantees. Alternatively, in the next section we propose potential improvements to the ﬁnite sample

guarantee (FG) provided via the proof of Theorem 1 and used in our numerical experiments.

3.3. Improved ﬁnite sample guarantees

In this section we assume that Assumption A1’ holds and, thus, the conditions of Theorems 2

and 4 are satisﬁed. Following the proof of Theorem 1 we observe that the out-of-sample prediction

disappointment for (18a) is bounded from above as follows:

(cid:110)
f (x, Q∗) > ˆfDR(x, (cid:98)Q)

(cid:111)

≤

Pr

(cid:88)

a∈Ax

(cid:110)

Pr

DKL(Qa (cid:107) Q∗

a) > ra

(cid:111)

≤

(cid:88)

a∈A

(Ta + 1)dae−Tara,

where the last inequality exploits the upper bound (LDP 1) and the fact that Ax ⊆ A for any x ∈ X.
However it can be argued that the upper bound (LDP 1) is not tight.

Recently Mardia et al. [25] has proposed some tighter upper bound that can be used to provide
less conservative estimates of the parameters ra, a ∈ A. That is, by Theorem 3 in [25] for any da ≥ 2
and Ta ≥ 2 the following upper bound holds:

(cid:110)

Pr

DKL(Qa (cid:107) Q∗

a) > ra

(cid:111)

≤

(cid:16) 3u1
u2

da−2
(cid:88)

j=0

Kj−1(

√
e

Ta

2π

)j(cid:17)

e−Tara,

(LDP 2)

where u0 = π, u1 = 2, K−1 = 1,

ui =






π × 1×3×...×(i−1)
2 × 2×4×...×(i−1)

2×4×...×i

1×3×...×i

, if i is even and i ≥ 2

, if i is odd and i ≥ 3

and Kj =

j
(cid:89)

i=0

ui ∀j ≥ 1

In particular, it is illustrated in [25] that for most parameter settings the upper bound (LDP 2)
is tighter than (LDP 1) and some other existing upper bounds; see, e.g., [26]. By using the proof of
Theorem 1, in our numerical experiments we select the parameters ra, a ∈ A, by setting the right-hand
side of (LDP 2) equal to δae−rTmin. In particular, if Assumption A1’ holds and δa = 1
n , a ∈ A, then

22

it is rather easy to check that

(cid:111)
(cid:110)
f (x, Q∗) > ˆfDR(x, (cid:98)Q)

Pr

≤ e−rTmin

and both Theorems 2 and 4 remain valid with the aforementioned choice of ra, a ∈ A.

4. Computational study

The primarily goal of this section is to explore the quality of distributionally robust decisions

induced by the prescription rule (18b). In particular, we provide a detailed numerical comparison of

our approach with Hoeﬀding bounds and some modiﬁcation of the DRO approach described in [11].

Following the research question Q3 we examine diﬀerent parameter settings with respect to both the

form of the nominal distribution and the sample size.

For a given nominal distribution Q∗ ∈ Q and a vector of empirical marginal distributions (cid:98)Q, we

measure the quality of a prescription rule ˆx by using a nominal relative loss, that is,

ρ(Q∗, (cid:98)Q) =

f (ˆx( (cid:98)Q), Q∗)
minx∈X f (x, Q∗)

(36)

In fact, the nominal relative loss (36) evaluates the out-of-sample performance of ˆx( (cid:98)Q) ∈ X under the
nominal distribution Q∗. Ideally, we have ρ = 1, while in general ρ ≥ 1 as long as the decision-maker
operates with a limited information about Q∗.

With respect to the nominal problem in (1), we consider two types of combinatorial optimization

problems, namely, the shortest path and unweighted knapsack problems. As outlined in Proposition 1,

computation of (18b) requires solving n univariate convex optimization problems and a unique deter-

ministic MIP problem. For this reason, we do not consider some more complicated MIP problems,

but instead focus on some qualitative insights implied by our construction of the nominal distribution

and the training data set. In particular, some practical inference from estimation of the worst-case

expected costs for each component of the cost vector c is provided.

The remainder of this section is organized as follows. In Section 4.1 we design several benchmark

approaches based on standard measure concentration inequalities or truncation of the training data set.

Section 4.2 provides the test instances and parameter settings. Finally, in Section 4.3 we report and

discuss our numerical results.

4.1. Benchmark approaches

Hoeﬀding bounds. Following Example 1 we deﬁne prediction and prescription rules as:

ˆfhoef (x, (cid:98)Q) =

min

(cid:88)

a∈A

(cid:110) 1
Ta

ˆxhoef ( (cid:98)Q) ∈ argminx∈X

j=1
ˆfhoef (x, (cid:98)Q),

Ta(cid:88)

(cid:98)ca,j + εa; za

(cid:111)

xa

(37a)

(37b)

where εa, a ∈ A, are given by equation (12) with δa = 1
n .

23

Truncated DRO methods (DRO1 and DRO2). We consider two alternative approaches,
namely, DRO1 and DRO2, based on a truncation of the training data set (2). In the truncated DRO1
approach we exploit the ﬁrst Tmin random observations of ca for each a ∈ A and apply the model
[11] assuming that the obtained complete data set is generated from some joint
of Van Parys et al.
distribution Q∗. In this model the decision-maker aims at minimizing its worst-case expected loss with
respect to all probability distributions within a relative entropy ball centered at the joint empirical
distribution of the data, say, ˆJ(Tmin). Then the outlined prediction and prescription rules can be
described by their dual formulations as follows (we omit some minor technical details for brevity; see

Proposition 2 in [11]):

ˆftrunc(x, ˆJ) = min
β

(cid:110)

β − e−(cid:101)r

(cid:101)d
(cid:89)

(β − ((cid:98)c(i))(cid:62)x)(cid:98)ji : β ≥

ˆxtrunc(ˆJ) ∈ argminx∈X

i=1
ˆftrunc(x, ˆJ)

(cid:111)

zaxa

(cid:88)

a∈A

(38a)

(38b)

Here, (cid:101)d = (cid:81)
a∈A da is a number of possible realizations of c, (cid:98)ji is an empirical probability that c = (cid:98)c(i)
for i ∈ {1, . . . , (cid:101)d} and (cid:101)r is a radius of the relative entropy ball centered at (cid:98)J. Since the prediction rule
(38a) obeys ﬁnite sample guarantees similar to those obtained in Theorem1 (see Theorem 5 in [11]),
we compute (cid:101)r for some ﬁxed decay rate r > 0 using the upper bound (LDP 2) adapted to the joint
distributions. In particular, as the value of (cid:101)d is typically large, the sum in (LDP 2) is computed by
eliminating zero terms in the sense of a ﬂoating-point precision.

On the other hand, in the truncated DRO2 approach we apply our prediction and prescriptions

rules, (18a) and (18b), to the truncated data set. In other words, we set Ta = Tmin for each a ∈ A.

While the truncated DRO2 approach preserves the complexity of the distributionally robust opti-
mization problem (DRO), the application of DRO1 approach in our problem setting is substantially
In fact, the objective function in (38b) is non-linear
limited; recall our discussion in Section 1.2.

and the set of feasible decisions, X, is generally non-convex. Hence, computation of (38b) results

in a non-linear MIP problem. In our computational experiments we simply enumerate all decisions

x ∈ X and solve the resulting univariate convex optimization problems in the parameter β. In view
of the discussion above, a further implementation of the truncated DRO1 approach is restricted to
comparatively small instances of the test problems.

4.2. Test instances

Nominal problems. As outlined earlier, we focus on two types of combinatorial optimization

problems. First, we consider the shortest path problem in a fully-connected layered graph with h

intermediate layers and w nodes at each layer i ∈ {1, . . . , h}. The ﬁrst and the last layers consist

of unique nodes, which are the source and the destination nodes, respectively. An example with

h = w = 3 is depicted in Figure 2. Speciﬁcally, we observe that the indexes A = {1, . . . , n} are related
to the set of directed arcs, whereas a decision x ∈ {0, 1}n encodes a simple path between the source
and destination nodes. Hence, X is described by the standard path ﬂow constraints [27], which explicit

form is omitted for brevity.

24

1

2

3

4

5

6

7

8

9

10

11

Figure 2: A fully-connected layered graph with h = 3 intermediate layers and w = 3 nodes at each layer.

Joint distribution

Marginal distributions

Support

Mean

Variance

Product of marginals

Binomial(pa, d − 1)

ca ∈ {1, . . . , d}

(d − 1)pa + 1

dpa(1 − pa)

Multinomial(p, d − 1)

Product of marginals

Binomial(pa, d − 1)
with (cid:80)
a∈A pa = 1

ca ∈ {1, . . . , d},
a∈A ca = d − 1 + |A|

(cid:80)

Discretization of normal
with mean µa and variance σ2
a

ca ∈ {1, . . . , d}

(d − 1)pa + 1

dpa(1 − pa)

(cid:80)d

i=1 iq∗

a,i

(cid:80)d
(cid:16) (cid:80)d

i=1 i2q∗
i=1 iq∗

a,i−
(cid:17)2

a,i

Table 2: The table summarizes discrete distributions that are used to model Q∗. The values of binomial and multinomial
distributions are shifted to the right by one. The support, mean and variance are component-wise. Parameters q∗
a,i, a ∈ A, of the
normal distributions are computed using (40).

Second, we consider a stochastic version of the unweighted knapsack problem (UNP), where the
decision-maker attempts to minimize its expected loss by selecting a decision vector x ∈ {0, 1}n with at
least K ∈ {1, . . . , n} nonzero components. Formally, we consider the stochastic programming problem

(1) with a set of feasible decisions given by:

XU N P =

(cid:110)

x ∈ {0, 1}n :

(cid:111)

xi ≥ K

n
(cid:88)

i=1

(39)

As a remark, this problem can be posed as an oﬄine version of the best arm identiﬁcation problem in

online learning; see, e.g., [28] and recall our discussion of the application settings in Section 1.2.

All experiments are performed on a PC with CPU i7-9700 and RAM 32 GB. The deterministic

versions of combinatorial optimization problems are solved in Python with CPLEX 12.10.0.0. The

dual formulations in (21) are solved using the function scipy.optimize.minimize() and the method of

Nelder-Mead with default parameters. In particular, we verify that the strong duality holds by solving

the primal optimization problems (20) with CVX 1.0.31.

Data-generating distributions. We assume that Sa = {1, . . . , d}, d ∈ Z>0, for each a ∈ A,
i.e., both Assumptions A1 and A1’ are satisﬁed. Then several classes of discrete distributions are

examined; see Table 2. The binomial and multinomial distributions are standard, but shifted to the

25

Distribution of Ta, a ∈ A

Support

Parameters

Uniform

[ (cid:101)Tmin, (cid:101)Tmax]

-

Binomial1(pa, (cid:101)Tmax − (cid:101)Tmin)

[ (cid:101)Tmin, (cid:101)Tmax]

pa =

EQ∗
a
maxb∈A EQ∗
b

{ca}−minb∈A EQ∗
b

{cb}
{cb}−minb∈A EQ∗
b

{cb}

Binomial2(pa, (cid:101)Tmax − (cid:101)Tmin)

[ (cid:101)Tmin, (cid:101)Tmax]

pa = 1 −

EQ∗
a
maxb∈A EQ∗
b

{ca}−minb∈A EQ∗
b

{cb}
{cb}−minb∈A EQ∗
b

{cb}

Table 3: The table summarizes the ways to pick Ta, a ∈ A. The binomial distributions are shifted to the right by (cid:101)Tmin.

right by one. The additional support constraint

(cid:88)

a∈A

ca = d − 1 + |A|

for the multinomial distribution is not known to the decision-maker due to Assumption A1.

In addition, we note that the variance of the binomial and multinomial distirbutions for each a ∈ A

depends on the mean and, thus, cannot be controlled directly; see the last column of Table 2. In this

regard, we consider a naive discretization of univariate normal distributions, in which the variance for
each a ∈ A can be controlled by an external parameter σa. That is, for a given µa ∈ [1, d] and σa ≥ 0
the nominal probabilities q∗

a,i, i ∈ {1, . . . , d}, are computed as follows:

q∗
a,i =

C
√

σa

(cid:90) i+0.5

2π

i−0.5

(cid:17)2

(cid:16)

t−µa
σa

− 1
2

e

dt,

(40)

Speciﬁcally, C > 0 is a normalization constant and the integrals are computed numerically using the

function scipy.integrate.quad() in Python.

Sample size. In order to pick the values of Ta, a ∈ A, we ﬁx some (cid:101)Tmin ∈ Z>0 and set

(cid:101)Tmax = (cid:101)Tmin + ∆,

where ∆ ∈ Z>0 is some positive integer constant. By leveraging the values of (cid:101)Tmin and (cid:101)Tmax we set
the values of Ta, a ∈ A, in three diﬀerent ways; see Table 3.

The intuition behind the choice of Ta is as follows. The Binomial1 distribution sorts the mean
values of Ta, a ∈ A, in an increasing order with respect to their nominal expected costs. Oppositely,
the Binomial2 distribution sorts the mean values of Ta, a ∈ A, in the decreasing order. In other words,
in the former situation the components with higher expected costs can be observed suﬃciently often; in

the latter situation the same holds for the components with lower expected costs. Finally, the uniform
distribution is somewhat in the middle between the Binomial1 and Binomial2 distributions. In the
next section we demonstrate that the distribution of Ta, a ∈ A, substantially aﬀects the out-of-sample
tests both for Hoeﬀding bounds and the baseline DRO approach induced by (18b).

26

(a) Binomial distribution.

(b) Multinomial distribution.

Figure 3: Average relative loss (36) and MADs as a function of (cid:101)Tmin, (cid:101)Tmin ∈ {5, 7 . . . , 35}, with ∆ = 10 under the (a)
binomial and (b) multinomial distributions. The distribution of Ta, a ∈ A, is uniform, the parameters of the graph are
given by h = 7 and w = 4.

(a) Binomial distribution.

(b) Multinomial distribution.

Figure 4: Average nominal expected costs (in an increasing order) and the associated worst-case expected costs under
the (a) binomial and (b) multinomial distributions with (cid:101)Tmin = 25, ∆ = 10. The distribution of Ta, a ∈ A, is uniform,
the parameters of the graph are given by h = 7 and w = 4.

4.3. Results and discussion

In view of the discussion above, we compare the aforementioned solution approaches in terms of

the nominal relative loss (36). Speciﬁcally, we compute the average relative loss and median absolute
deviations around the mean (MADs) over N0 = 200 randomly generated test instances.

Next, we ﬁx a conﬁdence level α = e−rTmin instead of the exponential decay rate, r, for convenience
and set α = 0.05 in all experiments. In addition, we set da = d = 50 and δa = 1
n for each a ∈ A; recall
the conditions of Theorems 2 and 4. The default parameters of the shortest path problem are given

by h = 7 and w = 4, if other is not speciﬁed. Finally, we set n = 100 and customize the parameter K

in the unweighted knapsack problem.

4.3.1. Results for the shortest path problem

Dependence on the form of the nominal distribution. In this paragraph we assume that
the distribution of Ta, a ∈ A, is uniform and compare the baseline DRO approach (18a)-(18b) with
Hoeﬀding bounds (37a)-(37b). First, we consider the nominal relative loss (36) as a function of (cid:101)Tmin
with ∆ = 10 under the binomial and multinomial distributions; see Figures 3a and 3b, respectively.
In particular, the parameters pa of binomial (multinomial) distributions for each a ∈ A are uniformly

27

Figure 5: Average relative loss (36) and MADs as a function of σ, σ ∈ {1, 3, . . . , 49}, under the discretized normal
distribution with (cid:101)Tmin = 25 and ∆ = 10 . The distribution of Ta, a ∈ A, is uniform, the parameters of the graph are
given by h = 7 and w = 4.

distributed over the interval [0, 1]; for the multinomial distribution the sum of pa, a ∈ A, is additionally
normalized to one.

As expected, the out-of-sample performance decreases as a function of (cid:101)Tmin; recall that the conﬁ-
dence level α is ﬁxed. Furthermore, both methods provide similar results for the binomial distribution,

but Hoeﬀding bounds demonstrate better out-of-sample performance in the case of the multinomial

distribution. Some intuition behind this fact can be provided in terms of the worst-case expected costs.

That is, in Figures 4a and 4b we consider the nominal expected costs (sorted in an increasing order)
and the associated worst-case expected costs averaged over N0 = 200 instances with (cid:101)Tmin = 25 and
∆ = 10 . The key observation is that the lower expected costs are better estimated using Hoeﬀding

bounds, while the higher costs are better estimated using the baseline DRO approach. As outlined in

Table 2, the multinomial distribution has an additional support constraint, which implies lower ex-

pected costs than those for the binomial distribution. This observation provides a practical evidence

behind the plots in Figures 3a and 3b.

In the second experiment we set µa uniformly distributed over [1, d] and σa = σ for each a ∈ A;
recall Table 2. We consider the nominal relative loss (36) as a function of σ under the discretized
normal distribution with (cid:101)Tmin = 25 and ∆ = 10; see Figure 5. Naturally, the quality of obtained solu-
tions decreases as the variance increases. However the baseline DRO approach and Hoeﬀding bounds

demonstrate similar out-of-sample performance across the considered values of variance. Therefore,

no general conclusions are made regarding a comparison of the considered solution approaches.

Dependence on the sample size. In this paragraph the data-generating distribution is supposed
to be binomial with the parameters pa, a ∈ A, uniformly distributed over [0, 1]. We explore how the
distribution of Ta, a ∈ A, aﬀects the out-of-sample performance of the baseline DRO approach and
Hoeﬀding bounds. Speciﬁcally, we consider the nominal relative loss (36) as a function of (cid:101)Tmin with
∆ = 10 under the Binomial1 and Binomial2 distributions from Table 3. The corresponding plots are
depicted in Figures 6a and 6b, respectively.

We conclude that the DRO approach outperforms Hoeﬀding bounds only in the case of Binomial2
distribution. This observation can be motivated in the context of Figure 4a. That is, under the
Binomial2 distribution the estimates of lower expected costs become less conservative and, thus, the

28

(a) Binomial distribution with Binomial1 Ta, a ∈ A.

(b) Binomial distribution with Binomial2 Ta, a ∈ A.

Figure 6: Average relative loss (36) and MADs as a function of (cid:101)Tmin, (cid:101)Tmin ∈ {5, 7 . . . , 35}, with ∆ = 10 under
the binomial distribution. The distributions of Ta, a ∈ A, are assumed to be (a) Binomial1 and (b) Binomial2; the
parameters of the graph are given by h = 7 and w = 4

worst-case expected costs obtained from the DRO approach are, in a sense, more consistent with the

nominal expected costs. The same holds for Hoeﬀding bounds and higher expected costs under the
Binomial1 distribution.

As we outlined in Section 1.2, the Binomial2 distribution arises naturally, if the data is collected by
trial and errors through multiple decision epochs. More precisely, if the decision-maker takes a decision
x ∈ X ⊆ {0, 1}n and observes only the costs associated with this decision, then lower expected costs
are more preferable for the decision-maker due to the objective criterion in (1). This observation

reveals some novel motivation behind the proposed distributionally robust optimization approach.

In the next experiment we assume that the distribution of Ta, a ∈ A, is uniform and explore the
nominal relative loss (36) as a function of ∆ for (cid:101)Tmin = 10 ; see Figure 7. We note that for ∆ = 0
the nominal relative loss (36) under the baseline DRO approach is close to one despite the number of
samples is suﬃciently small, i.e., we have Ta = 10 for each a ∈ A.

Some intuition behind this result can be provided by Theorems 2 and 4. That is, if ∆ = 0, then
all empirical marginal distributions contain the same number of samples, (cid:101)Tmin, and are obtained from
nominal marginal distributions of the same type, i.e., binomial distributions. We expect that in this

case the empirical marginal distributions have, in a sense, a similar form; recall our discussion after

the proof of Theorem 4 in Section 3.2. Therefore, in view of Theorems 2 and 4, we may anticipate

that the aforementioned parameters setting provides some advantage to the baseline DRO approach.

Surprisingly, the increase of ∆ and, thus, the use of additional data for particular components of

c does not improve the out-of-sample performance of the baseline DRO approach; recall Figure 7. In

this regard, we make the following additional observations:

• The increase of ∆ implies a larger ﬂuctuation of the worst-case expected costs obtained from

both solution approaches.

• In the case of the baseline DRO approach the worst-case expected costs increase rather slowly in
average as a function of the nominal expected costs; recall Figure 4a. Hence, a large ﬂuctuation

of the worst-case expected costs may lead to non-uniform estimates of the nominal expected

costs and, thus, to low-quality decisions in terms of the nominal relative loss.

29

Figure 7: Average relative loss (36) and MADs as a function of ∆, ∆ ∈ {0, 2, . . . , 40}, under the binomial distribution
with (cid:101)Tmin = 10. The distribution of Ta, a ∈ A, is uniform, the parameters of the graph are given by h = 7 and w = 4.

• In the case of Hoeﬀding bounds the worst-case expected costs increase faster as a function of
the nominal costs. For this reason, the ﬂuctuations in the case of suﬃciently large ∆ can be

eliminated, in a sense, by less conservative estimates of the nominal expected costs; see Figure

7 and note that with the increase of ∆ the sample size also tends to grow.

• Furthermore, if ∆ = 0, then the worst-case expected costs obtained from Hoeﬀding bounds usu-
ally achieve the upper bound, d, which results in a poor out-of-sample performance of (37a)-(37b).

We conclude that the baseline DRO approach outperforms Hoeﬀding bounds whenever the relative

gap between sample sizes, ∆, is suﬃciently small. The obtained results for ∆ = 0 are extended in the
next paragraph, where the truncated DRO1 and DRO2 methods are examined.

Figure 8: Average relative loss (36) and MADs as a function of ∆, ∆ ∈ {0, 2, . . . , 40}, under the discretized normal
distribution with (cid:101)Tmin = 10 and σ = d
4 . The distribution of Ta, a ∈ A, is uniform and the parameters of the graph are
given by w = h = 3.

Truncation of the data.

In this paragraph we assume that the nominal distribution is dis-
cretized normal with the parameters µa uniformly distributed over [1, d] and σa = σ for each a ∈ A.
Following our discussion of the truncated DRO1 approach in Section 4.1 we consider comparatively
small instances of the shortest path problem with h = w = 3.

First, we assume that the distribution of Ta, a ∈ A, is uniform. The dependence on ∆ is of interest
since the truncated methods do not take into account a part of the data. That is, in Figure 8 we
depict the nominal relative loss (36) as a function of ∆ with (cid:101)Tmin = 10 and σ = d
4 . As a remark, the

30

(a) Normal distribution with Binomial2 Ta, a ∈ A, and σ = d
4 .

(b) Normal distribution with uniform Ta, a ∈ A, and σ = 3d
4 .

Figure 9: Average relative loss (36) and MADs as a function of ∆, ∆ ∈ {0, 2, . . . , 40}, under the discretized normal
distribution with (a) Binomial2 Ta, a ∈ A, σ = d
4 . In both cases we set (cid:101)Tmin = 10
and w = h = 3.

4 and (b) uniform Ta, a ∈ A, σ = 3d

results for the truncated DRO1 and DRO2 approaches may also depend on ∆ as it is not necessarily
the case that (cid:101)Tmin = Tmin; recall Table 3.

The numerical results indicate that both methods with the truncated data set substantially out-

perform the baseline DRO approach for all considered values of parameter ∆. This observation for
the truncated DRO2 approach is explained via the previous experiment; recall Figure 7. At the same
time, the truncated DRO1 approach demonstrates a similar out-of-sample performance as the DRO2
approach. Intuitively, the latter observation can be also provoked by the fact that the DRO1 approach
is asymptotically optimal; see Theorem 7 in [11].

Finally, we note that the baseline DRO approach may outperform the DRO1 and DRO2 methods,
e.g., if the parameters Ta, a ∈ A, are governed by the Binomial2 distribution or the variance σ is
relatively large; see Figures 9a and 9b, respectively. The intuition behind the former observation is

discussed in the previous experiments (Figure 6b), while the latter observation can be explained as

follows. The number of random samples in the truncated data set is relatively small. Hence, a large

variance may result in biased estimates of the nominal expected costs that, in turn, results in a higher
nominal relative loss for the truncated DRO1 and DRO2 approaches.

4.3.2. Results for the unweighted knapsack problem

In this section we demonstrate that the obtained practical insights remain valid for a class of un-

weighted knapsack problems. For the sake of brevity, we focus on a comparison of the baseline DRO

approach (18a)-(18b) and Hoeﬀding bounds (37a)-(37b). Also, the nominal distribution and the dis-
tribution of Ta, a ∈ A, are supposed to be component-wise binomial and uniform, respectively; recall
Tables 2 and 3. As in the previous section, the parameters pa, a ∈ A, of the binomial distribution are
distributed uniformly over the interval [0, 1].

First, in Figure 10 we consider the nominal relative loss (36) as a function of K for (cid:101)Tmin = 10 and
∆ = 10; recall deﬁnition (39). We observe that the baseline DRO approach outperforms Hoeﬀding

bounds only if the parameter K is suﬃciently large.

This observation can be explained as follows. If K is suﬃciently small, then we need to select only

the components of c with relatively low expected costs. As outlined in Figure 4a, these costs are better

31

Figure 10: Average relative loss (36) and MADs as a function of K, K ∈ {10, 15, . . . , 90}, with (cid:101)Tmin = 10, ∆ = 10 and n = 100
under the binomial distribution. The distribution of Ta, a ∈ A, is uniform and n = 100.

estimated using Hoeﬀding bounds. On the other hand, if K is suﬃciently large, than the decision-

maker needs to select all components of c with low expected costs and a part of the components with

relatively high expected costs. Since the baseline DRO approach provides better estimates of higher

expected costs, we conclude that it performs better for suﬃciently large values of K.

Finally, we show that the aforementioned results do not depend on the choice of (cid:101)Tmin. That is, we
consider the nominal relative loss (36) as a function of (cid:101)Tmin with ∆ = 10. The plots for K = 20 and
K = 80 are depicted in Figures 11a and 11b, respectively.

(a) Binomial distribution and K = 20.

(b) Binomial distribution and K = 80.

Figure 11: Average relative loss (36) and MADs as a function of (cid:101)Tmin, (cid:101)Tmin ∈ {5, 7, . . . , 35}, with ∆ = 10, (a) K = 20
and (b) K = 80 under the binomial distribution. The distribution of Ta, a ∈ A, is uniform and n = 100.

Summary. Advantages of the baseline DRO approach (18a)-(18b) can be summarized as follows.

The proposed approach outperforms Hoeﬀding bounds (37a)-(37b) whenever relatively low expected

costs can be observed suﬃciently often or the gap between the sample sizes for the components of c

is comparatively small. Furthermore, if the variance of the data-generating distribution is suﬃciently

small, then a truncation of the data set may substantially improve the out-of-sample performance of

the baseline DRO approach. In contrast to the optimization model in [11], the outlined eﬀect can be

achieved “for free”, i.e., by leveraging the same DRO model for the truncated data set.

32

5. Conclusion

In this paper we consider a class of linear mixed-integer programming problems, where the cost
vector is governed by some unknown probability distribution Q∗. The decision-maker attempts to
minimize its expected loss under Q∗ using some ﬁnite training data set obtained from this distribution.
In contrast to the related study of Van Parys et al. [11], we assume that the components of the cost

vector are explored not to the same degree. In particular, the proposed modeling approach is motivated

by a number of online combinatorial optimization and machine learning problem settings, where the

data is collected by trial and errors through multiple decision epochs.

For the constructed stochastic programming problem we seek a prediction rule that converts the

data set into an estimate of the expected value of the objective function and a prescription rule that

provides an associated estimate of the optimal decision. The goal is to ﬁnd the least conservative pre-

diction and prescription rules, which also hedge against underestimated losses whenever the sample

size tends to inﬁnity. We demonstrate that under some mild assumption the associated prediction

and prescription problems admit a weakly optimal solution with a number of attractive theoretical

properties. First, there is a class of decisions and joint distributions, for which the proposed pre-

diction (prescription) rules are not dominated by any other feasible prediction (prescription) rules.

Second, the aforementioned solution can be obtained by solving a number of component-wise convex

distributionally robust optimization problems and a unique instance of the nominal problem.

We perform numerical experiments, where the out-of-sample performance of the proposed approach

is analyzed with respect to several classes of synthetic combinatorial optimization problems. Impor-

tantly, we exploit the abovementioned theoretical results to provide some intuition for a numerical

validation of our approach. In particular, it turns out that solutions with a reasonably good quality

can be obtained whenever the relative diﬀerence between sample sizes is suﬃciently small or the data

set is biased towards the lower expected costs.

Admittedly, our theoretical results exploit a component-wise decomposition of the objective cri-

terion. Therefore, it would be interesting to consider some other risk measures, especially those that

do not account for any covariance information among the cost coeﬃcients (but may account for some

variance information). To the best of our knowledge, an accurate estimation of the covariance matrix

for incomplete data sets is rather challenging; we refer, e.g., to [29] where this problem is addressed

for a particular class of data-generating distributions. In addition, a rather natural step forward is to

extend the current results to the case of a continuous support. We refer the reader to Section 5 in [11]

for the related discussion in the case of complete data. Finally, it remains an open question whether

the proposed approach provides strongly optimal prediction (prescription) rules or not.

Acknowledgments. The authors would like to thank Dr. Oleg Prokopyev for his helpful com-

ments and suggestions. The article was prepared within the framework of the Basic Research Program

at the National Research University Higher School of Economics (Sections 1-2). The research is funded
by RSF project №22-11-00073 (Sections 3-5).

Conﬂict of interest: The authors declare that they have no conﬂict of interest.

33

References

[1] D. Bertsimas and A. Thiele, “Robust and data-driven optimization: modern decision making

under uncertainty,” in Models, methods, and applications for innovative decision making, pp. 95–

122, INFORMS, 2006.

[2] P. M. Esfahani and D. Kuhn, “Data-driven distributionally robust optimization using the wasser-

stein metric: Performance guarantees and tractable reformulations,” Mathematical Programming,

vol. 171, no. 1-2, pp. 115–166, 2018.

[3] A. J. Kleywegt, A. Shapiro, and T. Homem-de Mello, “The sample average approximation method

for stochastic discrete optimization,” SIAM Journal on Optimization, vol. 12, no. 2, pp. 479–502,

2002.

[4] D. Bertsimas, V. Gupta, and N. Kallus, “Robust sample average approximation,” Mathematical

Programming, vol. 171, no. 1, pp. 217–282, 2018.

[5] E. Delage and Y. Ye, “Distributionally robust optimization under moment uncertainty with ap-

plication to data-driven problems,” Operations Research, vol. 58, no. 3, pp. 595–612, 2010.

[6] J. Goh and M. Sim, “Distributionally robust optimization and its tractable approximations,”

Operations Research, vol. 58, no. 4-part-1, pp. 902–917, 2010.

[7] W. Wiesemann, D. Kuhn, and M. Sim, “Distributionally robust convex optimization,” Operations

Research, vol. 62, no. 6, pp. 1358–1376, 2014.

[8] G. Bayraksan and D. K. Love, “Data-driven stochastic programming using phi-divergences,” in

The Operations Research Revolution, pp. 1–19, INFORMS, 2015.

[9] S. S. Ketkov, O. A. Prokopyev, and E. P. Burashnikov, “An approach to the distributionally

robust shortest path problem,” Computers & Operations Research, vol. 130, no. 105212, 2021.

[10] V. Gupta, “Near-optimal bayesian ambiguity sets for distributionally robust optimization,” Man-

agement Science, vol. 65, no. 9, pp. 4242–4260, 2019.

[11] B. P. Van Parys, P. M. Esfahani, and D. Kuhn, “From data to decisions: Distributionally robust

optimization is optimal,” Management Science, 2020.

[12] T. Sutter, B. P. Van Parys, and D. Kuhn, “A general framework for optimal data-driven opti-

mization,” arXiv preprint arXiv:2010.06606, 2020.

[13] P. Auer, N. Cesa-Bianchi, and P. Fischer, “Finite-time analysis of the multiarmed bandit prob-

lem,” Machine Learning, vol. 47, no. 2-3, pp. 235–256, 2002.

[14] T. L. Lai and H. Robbins, “Asymptotically eﬃcient adaptive allocation rules,” Advances in Ap-

plied Mathematics, vol. 6, no. 1, pp. 4–22, 1985.

34

[15] J.-Y. Audibert, S. Bubeck, and G. Lugosi, “Regret in online combinatorial optimization,” Math-

ematics of Operations Research, vol. 39, no. 1, pp. 31–45, 2014.

[16] B. Kveton, Z. Wen, A. Ashkan, and C. Szepesvari, “Tight regret bounds for stochastic combina-

torial semi-bandits,” in Artiﬁcial Intelligence and Statistics, pp. 535–543, PMLR, 2015.

[17] J. L. Schafer and J. W. Graham, “Missing data: our view of the state of the art.,” Psychological

Methods, vol. 7, no. 2, p. 147, 2002.

[18] T. D. Pigott, “A review of methods for missing data,” Educational Research and Evaluation,

vol. 7, no. 4, pp. 353–383, 2001.

[19] J. Jahn, Vector Optimization. Springer, 2009.

[20] M. Conforti, G. Cornu´ejols, and G. Zambelli, Integer Programming, vol. 271. Springer, 2014.

[21] M. Sniedovich, “A classical decision theoretic perspective on worst-case analysis,” Applications

of Mathematics, vol. 56, no. 5, p. 499, 2011.

[22] W. Hoeﬀding, “Probability inequalities for sums of bounded random variables,” in The collected

works of Wassily Hoeﬀding, pp. 409–426, Springer, 1994.

[23] S. Kullback, Information theory and statistics. Courier Corporation, 1997.

[24] P. Groeneboom, J. Oosterhoﬀ, and F. H. Ruymgaart, “Large deviation theorems for empirical

probability measures,” The Annals of Probability, pp. 553–586, 1979.

[25] J. Mardia, J. Jiao, E. T´anczos, R. D. Nowak, and T. Weissman, “Concentration inequalities for

the empirical distribution of discrete distributions: beyond the method of types,” Information

and Inference: A Journal of the IMA, vol. 9, no. 4, pp. 813–850, 2020.

[26] R. Agrawal, “Finite-sample concentration of the multinomial in relative entropy,” IEEE Trans-

actions on Information Theory, vol. 66, no. 10, pp. 6297–6302, 2020.

[27] R. K. Ahuja, T. L. Magnanti, and J. B. Orlin, Network ﬂows. Cambridge, Mass.: Alfred P. Sloan

School of Management, Massachusetts, 1988.

[28] S. Kalyanakrishnan and P. Stone, “Eﬃcient selection of multiple bandit arms: Theory and prac-

tice,” in ICML, 2010.

[29] J. Liu and D. P. Palomar, “Regularized robust estimation of mean and covariance matrix for

incomplete data,” Signal Processing, vol. 165, pp. 278–291, 2019.

35

Appendix A. Supplementary material

The proof of Proposition 3. Assume that there exists another prediction rule ˆf (cid:48), which is less
conservative than ˆf and satisﬁes the asymptotic guarantee (AG). Therefore, for any decision x ∈ X
and any joint distribution Q ∈ Q there exists ε ∈ R>0 such that

ˆf (cid:48)(x, Q) ≤ ˆf (x, Q) − ε,

In the remainder of the proof we show that ˆf (cid:48) is infeasible in (P).

First, we bound the out-of-sample disappointment (8) from below as:

(cid:110)
f (x, Q∗) > ˆf (cid:48)(x, (cid:98)Q)

(cid:111)

Pr

≥ Pr

(cid:110)
f (x, Q∗) > ˆf (x, (cid:98)Q) − ε

(cid:111)
,

(A.1)

Without loss of generality let za,1 = za for each a ∈ A. Since the asymptotic guarantee (AG) must
be satisﬁed for any thinkable data-generating distribution Q∗ ∈ Q, we may assume that for each a ∈ A

q∗
a,i =




1, if i = 1,



0, otherwise

Next, we observe that

f (x, Q∗) =

(cid:88)

a∈A

EQ∗

a{ca}xa =

(cid:88)

a∈A

zaxa > ˆf (x, Q) − ε,

for any feasible Q ∈ Q (recall that Q is a vector of marginal distributions induced by Q), where the
last inequality holds due to the upper bound (24). Hence,

(cid:110)
f (x, Q∗) > ˆf (cid:48)(x, (cid:98)Q)

(cid:111)

Pr

≥ Pr

(cid:110)
f (x, Q∗) > ˆf (x, (cid:98)Q) − ε

(cid:111)

= 1

and ˆf (cid:48) does not satisfy the asymptotic guarantee (AG).

The proof of Theorem 3. The proof follows the proof of Theorem 2 excluding the following

minor changes. First, we set b ∈ argmina∈A Ta and note that the fact ((cid:101)x, (cid:101)Q) ∈ (cid:101)Z implies that

(cid:101)qa,i = (cid:101)qb,i > 0 ∀i ∈ {1, . . . , dmin}, ∀a ∈ A, a (cid:54)= b

and (cid:101)qa,i = 0 for any i ∈ {dmin + 1, . . . , da} and a ∈ A; recall deﬁnition (33). Then the expected cost
of ca under the worst-case distribution

Q(w)

a ∈ argmaxQa∈ (cid:98)Qa

EQa{ca}

36

can be expressed as follows:

E

Q(w)
a

{ca} =

da(cid:88)

i=1

q(w)
a,i za,i = max
Qa∈ (cid:98)Qa

da(cid:88)

i=1

qa,iza,i = max
Qa∈ (cid:98)Qa

dmin(cid:88)

i=1

qa,iza,i =

max
Qa∈ (cid:98)Qa

dmin(cid:88)

i=1

qa,i(wb,azb,i + vb,a) = wb,a max
Qa∈ (cid:98)Qa

(cid:16) dmin(cid:88)

i=1

(cid:17)

qa,izb,i

+ vb,a ≤

wb,a max
Qb∈ (cid:98)Qb

(cid:16) dmin(cid:88)

(cid:17)

qb,izb,i

i=1

+ vb,a = max
Qb∈ (cid:98)Qb

(cid:16)

db(cid:88)

i=1

(cid:17)

qb,iza,i

= E

Q(w)
b

{ca}

In contrast to (27), we additionally use the deﬁnition of the relative entropy ball, i.e.,

da(cid:88)

i=1

q(w)
a,i za,i = max

da(cid:88)

i=1

qa,iza,i

s.t. qa,i > 0 ∀i ∈ {1, . . . , da}

(A.2)

da(cid:88)

qa,i = 1,

i=1

dmin(cid:88)

i=1

(cid:101)qa,i ln (cid:101)qa,i
qa,i

≤ ra

That is, since (cid:101)qa,i ln (cid:101)qa,i
qa,i
optimal solution of (A.2) must satisfy q(w)
Finally, for each a ∈ A we deﬁne

is a decreasing function of qa,i and za,1 ≥ . . . ≥ za,da by construction, the
a,i = 0 for i ∈ {dmin + 1, . . . , da} and any a ∈ A.

Q∗

a := λ (cid:101)Qb + (1 − λ)Q(w)

b

for a suﬃciently small λ > 0. In particular, we observe that q∗
a,i > 0 for any
i ∈ {1, . . . , dmin} and a ∈ A and q∗
a,i = 0, otherwise. Thus, in each support set Sa, a ∈ A, we may
leave only the ﬁrst dmin components and then exploit the proof of Theorem 2 for d = dmin. This
observation concludes the proof.

a,i = λ(cid:101)qa,i + (1 − λ)q(w)

37

