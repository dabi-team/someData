Highlights

CDiNN -Convex DiÔ¨Äerence Neural Networks
Parameswaran Sankaranarayanan,Raghunathan Rengaswamy

‚Ä¢ Introduce new Neural network architecture for eÔ¨Écient decision making without signiÔ¨Åcant loss of representa-

tional capability

‚Ä¢ Use of DiÔ¨Äerence of Convex (DC) programming in decision making involving neural networks

‚Ä¢ DC optimization produces better result at each iteration and guarantees convergence with the proposed neural

network architecture

‚Ä¢ Optimization problem at each step reduces to Linear Programming problem with the proposed network archi-

tecture

‚Ä¢ Illustration of advantages of this neural network structure through several case studies

1
2
0
2

r
p
A
5

]

G
L
.
s
c
[

2
v
1
3
2
7
1
.
3
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
CDiNN -Convex DiÔ¨Äerence Neural Networks

Parameswaran Sankaranarayanana, Raghunathan Rengaswamya,‚àó

aRobert Bosch Centre for Data Science and ArtiÔ¨Åcial Intelligence, Indian Institute of Technology Madras, Chennai, India

A R T I C L E I N F O

A B S T R A C T

Keywords:
DiÔ¨Äerence of convex optimization
Input Convex Neural Networks
Deep learning
Neural Networks
Optimal control

Neural networks with ReLU activation function have been shown to be universal function approx-
imators and learn function mapping as non-smooth functions. Recently, there is considerable
interest in the use of neural networks in applications such as optimal control. It is well-known
that optimization involving non-convex, non-smooth functions are computationally intensive and
have limited convergence guarantees. Moreover, the choice of optimization hyper-parameters
used in gradient descent/ascent signiÔ¨Åcantly aÔ¨Äect the quality of the obtained solutions. A new
neural network architecture called the Input Convex Neural Networks (ICNNs) learn the output
as a convex function of inputs thereby allowing the use of eÔ¨Écient convex optimization methods.
Use of ICNNs for determining the input for minimizing output has two major problems: learning
of a non-convex function as a convex mapping could result in signiÔ¨Åcant function approximation
error, and we also note that the existing representations cannot capture simple dynamic struc-
tures like linear time delay systems. We attempt to address the above problems by introduction
of a new neural network architecture, which we call the CDiNN, which learns the function as a
diÔ¨Äerence of polyhedral convex functions from data. We also discuss that, in some cases, the
optimal input can be obtained from CDiNN through diÔ¨Äerence of convex optimization with con-
vergence guarantees and that at each iteration, the problem is reduced to a linear programming
problem.

1. Introduction

Neural networks have been used for function approximation and classiÔ¨Åcation tasks in several applications. The
fact that these networks are shown to be universal function approximators is a nice theoretical guarantee for possible
representational capability. SpeciÔ¨Åcally, in [17], the authors have shown the universal approximation capability of
neural networks with unbounded activation functions like ReLU. Further, in recent years, complex problems have
been solved using deep architectures (multiple layers) with RectiÔ¨Åed Linear Unit (ReLU) activation functions. This
has spawned a tremendous interest in the study of these architectures across all engineering domains. However, while
these properties are theoretically nice, the actual performance depends on solving optimization problems in multiple
contexts while neural networks are used. Optimization is important in neural networks in two contexts:

1. The Ô¨Årst context is the training of these networks, where the network parameters are identiÔ¨Åed by minimizing a
loss function. The most popular approach is the back-propagation approach, which is a gradient descent algo-
rithm. Several variants of this algorithm have been evaluated for neural network training. Since the loss function
is usually non-convex, issues related to convergence and local minima problems need to be addressed. In spite
of all these, there are several applications where neural networks have found great success.

2. The second context is where the trained network is used in model-based control or model-based optimization
studies. These are problems where optimal decisions have to be made such as understanding the eÔ¨Äect of inputs
on certain neurons, inputs for optimal control and so on. Since the identiÔ¨Åed neural network model is non-convex,
the use of such models in any control and/or optimization application is beset with local minima problems.
Global optimization solutions are computationally prohibitive for use in real-time or near real-time applications.
Further, there is very little theoretical convergence guarantees for many of these techniques. An additional
diÔ¨Éculty is the trial-and-error process for choosing parameters in the optimization procedure (referred to as
hyper-parameter tuning), which leads to considerable Ô¨Çuctuations in the quality of results. This could possibly
make the whole process non-standard.

‚àóCorresponding author
paramesh282@gmail.com (P. Sankaranarayanan); raghur@mail.iitm.com (R. Rengaswamy)
ORCID(s):

Parameswaran S et al.: Preprint submitted to Elsevier

Page 1 of 19

CDiNN

All nonlinear problems are not complex from an optimization perspective, it is the non-convexities that lead to
diÔ¨Éculties. On the other hand, algorithms dealing with convex functions have guarantees regarding convergence
and identiÔ¨Åcation of global optimum. As a result, use of convex optimization ideas in neural networks is gaining
prominence. In [1] and [4], the authors propose an architecture called the Input Convex Neural Network (ICNN)
in which the output is learnt as a convex function of the inputs. When a network like that is used in optimization
studies, particularly when the output function is directly optimized, then the optimization problem becomes a convex
optimization problem. As a result, eÔ¨Écient convex optimization algorithms can be applied to solve these types of
problems. Further, an extension of the ICNN as a recurrent neural network has also been proposed; this structure can
be used to model time series data.

While there are several advantages to ICNN, one of the main drawbacks of the architecture is the representational
capability. There can be considerable loss in accuracy when approximating a non-convex function using a convex
function. Additionally, we show that the ICNN architecture, due to the manner in which it is constructed, cannot be
used to model time delay systems. To address these diÔ¨Éculties, while retaining some of the advantages in working
with convex functions in optimization, we propose a new neural network architecture called the CDiNN architecture.
In this architecture, any given function that needs to be modeled is represented as a diÔ¨Äerence of convex functions. This
enhances the representation capability tremendously. Further, as in ICNN, when the outputs are directly optimized,
one could use diÔ¨Äerence of convex algorithms (DCA), which provide much better theoretical guarantees than the
standard gradient descent algorithms. In this paper, we describe the CDiNN architecture, analyze its properties and
show application examples to demonstrate the versatility of this architecture.

The rest of the paper is organized as follows. ICNN and their properties, advantages and disadvantages are dis-
cussed in the next section. After that we describe the proposed CDiNN architecture, explore its properties and compare
the proposed architecture with ICNN. Application case studies that corroborate the theoretical claims follows this dis-
cussion. We conclude with thoughts on further development of the CDiNN architecture.

2. Input Convex Neural Network - ICNN

Input Convex Neural Network (ICNN) [1] is designed to learn function mapping from a class of convex functions
generated using a neural network architecture. In this neural network structure, outputs are convex functions of inputs.
This is achieved through appropriate choices for activation functions and constraining the weights. In this section,
we start with a basic description of the ICNN architecture. Properties of ICNN and some of the representational
issues in the use of the ICNN architecture are then described. Training strategies for identifying the weights in the
ICNN structure are outlined. Finally, applications of ICNN studied in the literature and the disadvantages of the ICNN
structure are explored.

2.1. ICNN Structure

and output of hidden layer is connected to the next layer through weights ùëä (ùëß)

Input Convex Neural Network is a modiÔ¨Åcation of feed-forward neural networks with constraints on the weights
and the choice of activation functions [1]. The architecture of the neural network is shown in Figure 2. The input to
the network (ùë•) is passed through hidden layers (possibly deep) and ùëßùëò
is the output of the ùëòùë°‚Ñé layer. The activation
function used in the network is a convex non-decreasing function. RectiÔ¨Åed Linear Unit (ReLU), shown in Figure 1 is
an example of a possible activation function that can be used. The input ùë• is connected to hidden layers through weights
ùëä (ùë•)
. The network structure depicted
ùëñ
in Figure 2 reduces to the standard feed-forward structure, except for the direct connections from the inputs to every
layer of the network. It can be observed that when the weights connecting the output of one hidden layer of neural
network to the next layer (ùëä (ùëß)
) is constrained to be positive and the activation function is convex non-decreasing
function, every element in the output layer (ùëßùêæ
) is a convex function of the input ùë•. This is because a non-negative
sum of convex functions is a convex function and a composition of non-decreasing convex function ùëîùëñ
on a convex
function results in a convex function. Given that the weights are constrained to be positive, the pass-through layers
(connections from the inputs directly to every layer) provided by ùëä (ùë•)

are required to permit identity mapping.

ùëñ

ùëñ

ùëñ

ùëßùëñ+1 = ùëîùëñ(ùëä (ùëß)

ùëñ ùëßùëñ + ùëä (ùë•)

ùëñ ùë• + ùëèùëñ);

(1)

It is also possible easily extend this framework to construct a Partial Input Convex Neural Network (PICNN) where

Parameswaran S et al.: Preprint submitted to Elsevier

Page 2 of 19

CDiNN

Figure 1: ReLU activation function output = max(0, input)

Figure 2: Input Convex Neural Network proposed in [1]

Figure 3: Partial Input Convex Neural Network

the output is a convex function of only a subset of the input terms. In the Figure 3, each element of ùëßùëò
a convex function of x and a non-convex function of u. Only the weights ùëä (ùëß)

are constrained to be positive.

can be learnt as

ùëñ

2.2. Recurrent ICNN

We discuss the Recurrent ICNN architecture proposed in [4]. The architecture of Recurrent ICNN is shown in

Figure 4 and described by equation 2

ùëßùë° = ùëÖùëíùêøùëà (ùëà ùë•ùë° + ùëçùëßùë°‚àí1 + ùê∑ùë•ùë°‚àí1)

Parameswaran S et al.: Preprint submitted to Elsevier

Page 3 of 19

CDiNN

Figure 4: Recurrent Input Convex Neural Network expanded in time (time=1 to time=t)

ùë¶ùë° = ùëìùëé(ùëÄùëßùë° + ùëÅùëßùë°‚àí1 + ùëâ ùë•ùë°)

(2)

where ùë•ùë°
is the input vector at time t in the format ùë• =
is a non-decreasing convex activation function (Linear, ReLU etc.) and ùê∑, ùëÅ, ùëâ are the weights of
at time t, ùëìùëé
pass-through layers. It can be observed that the output ùë¶ùë° = ùëì (ùë•1, ùë•2, ..., ùë•ùë°; ùúÉ) is a convex function of the inputs.
ùúÉ = [ùëà , ùëâ , ùëÄ, ùëÅ, ùê∑, ùëç] are network parameters and are constrained to be non-negative in this formulation.

is the output of the neural network

[ ùë¢
‚àíùë¢

]
, u is the input, ùë¶ùë°

2.3. Properties

The authors of [4] compare the eÔ¨Éciency of representation for a convex function ùëì as ICNN with piece-wise aÔ¨Éne
representation. It is argued that input convex neural networks provide a compact approach to learn a function as a
maximum of aÔ¨Éne functions.

Theorem 1. For any Lipschitz convex function over a compact domain, there exists an input convex neural network
with non-negative weights (of ùëä (ùëß)

) and ReLU activation functions that approximates it within some ùúñ. [4]

ùëñ

Remark. Any Lipschitz convex function can be approximated by a maximum of Ô¨Ånite number of aÔ¨Éne functions.The
proof of the above theorem involves showing that a convex function in the form of maximum of ùëò aÔ¨Éne functions can
be represented by ICNN with ùëò hidden layers, each with one neuron and ReLU activation function [4] .

2.4. Learning of the network parameters

Two methods for learning network parameters are described in [1].

1. Direct functional Ô¨Åtting - Gradient descent: Gradient descent (or its variants) are used to minimize a loss
are forced to be positive by

function - for example, mean square error. At each iteration, the weights ùëä (ùëß)
replacing the negative weights by 0 or multiplication with ‚àí0.5.

ùëñ

2. Argmin diÔ¨Äerentiation: In this method, the authors propose to learn the network parameters by minimizing the
diÔ¨Äerence between true optimal input and the estimated input that minimizes the output of the network. This is
suitable for cases where the true optimal input is known at the time of training.

Parameswaran S et al.: Preprint submitted to Elsevier

Page 4 of 19

CDiNN

Figure 5: Learning of sine function by ICNN of one hidden layer and 10 hidden neurons. The green curve is the learnt
function, the yellow points represent the training data. Mean square Loss of training data is 0.008 and number of training
epochs is 800.

2.5. Application of ICNN to optimal control

In [4], the authors discuss two conÔ¨Ågurations to use ICNN in optimal control. In the Ô¨Årst conÔ¨Åguration, the ICNN
is used to learn the state transition as a function of actions and states in reinforcement learning in feedforward conÔ¨Ågu-
ration. In the second conÔ¨Åguration, the authors used Input Convex Recurrent Neural Network (ICRNN) for modelling
the cost as a dynamic function of inputs and states. The authors then show results for a model predictive controller
that minimizes the positively weighted sum of cost functions (ICRNNs) with respect to the inputs ùë•.

In [21], the authors used ICNN for model predictive control of Van de Vusse reaction. The authors observe that
ICNN does not perform well in modelling the reaction (with a large MSE); this reiterates the fact that the choice
of ICNN for modeling nonconvex functions can be problematic. However, in this study it is shown that the control
performance with ICNN is reasonably good for this problem. Nonetheless, it is not possible to guarantee that this
can be generalized for other problems. The authors argue that given the predictability of results and no local optima
points, ICNN could be a good choice for modelling provided the model-mismatch is not signiÔ¨Åcant enough to generate
infeasible controllers. A few other examples where ICNN is used for optimal control are [3] and [5], where ICNN is
applied for room temperature control and optimal voltage regulation respectively.

2.6. Discussions

In this section, we discuss the disadvantages of using ICNN in decision making process by the use of simple

functions. We also show that Recurrent ICNN cannot represent time delay systems.

2.6.1. 1D non-convex functions

While ICNNs provide an interesting framework for using neural networks in optimization, we note that many
interesting optimization problems involve non-convex functions and in such scenarios, convex function approximations
could lead to signiÔ¨Åcant errors. Consider the use of ICNN for approximating a non-convex function ùë¶ = ùë†ùëñùëõ(5ùë•)‚àï5
where, ùë• ‚àà [‚àí1, 1]. The approximation results are shown in Figure 5 and two important observations can be made.

1. Consider optimization involving ICNN to Ô¨Ånd minima with initial point as x=0.5. From the Figure 5, we see
that optimizing using ICNN would result in an optimal value for ùë• = ‚àí0.5, while ùë• = 1.0 is the closest best
optimum. Hence it could be observed that if ICNN is used for control, it might result in optimization solutions
that could be very far away from the present operating region (while skipping an equivalent solution that might
be nearby).

2. The global optimum value pointed by the ICNN in Figure 5 is ùë• = ‚àí0.5 whereas the true global optima occurs

at ùë• = ‚àí0.314. Hence we see that global optimum found by ICNN may not be the true optimum value.

Parameswaran S et al.: Preprint submitted to Elsevier

Page 5 of 19

CDiNN

Figure 6: Output predicted (red) vs truth (black) by Recurrent ICNN (without bias) as proposed in [4]

2.6.2. Modelling time delay systems

The recurrent ICNN architecture is empirically shown to model dynamical systems well in [4]. We now study
for illustration. We focus on
this characteristic of ICNN through an example. Let us consider modelling of ùë¶ùë° = ùë¢ùë°‚àí5
. Consider ùë° = 5 for the
the ability of the Recurrent ICNN shown in Figure 4 to represent the negative values of ùë¢ùë°‚àí5
analysis. Only at time steps ùë° = 0 and ùë° = 1, the hidden layers receive the input ùë¢0
directly. Further, we note that
the output of the hidden layers are non-negative because ReLU clips negative values. Hence it would be diÔ¨Écult to
only via hidden layers. Interestingly, we observe that, in the
model the negative values of ùë¢0
presence of bias and bounded inputs ùë¢, this function can be learnt by the network due to the following. With enough
hidden nodes to allow for dynamical mapping, the network could map ùë¢0
by shifting all the inputs in the input
layer to positive (by a bias of ùëè) and shifting back the values at output layer to input values by a bias of ‚àíùëè, thereby
preserving the information.

is connected to ùë¢0

since ùë¶5

to ùë¶5

Thus, we investigate if this network could still learn the time-delay functions in absence of bias. We show the results
empirically with Recurrent ICNN with 10 recurrent neurons and disabled biases. The input that is used to excite the
system is a random uniform signal uniformly sampled from [-1,1]. The output is ùë¶ùë° = ùë¢ùë°‚àí4 + ùë¢ùë°‚àí3 + ùë¢ùë°‚àí2 + ùë¢ùë°‚àí1
. There
is no noise in the output signal. The input to the RNN is time delayed input signal with delay ‚àà [0, 4]. It is seen from
the Figure 6 that ICNN does not capture the dynamic behaviour of this system. Hence, we attribute the performance
of Recurrent ICNN in time-delay systems to shifting of inputs by biases. While this is acceptable in training data,
this could result in poor generalization when the test data explores a diÔ¨Äerent region of the input than in the training
data. Further, learning of complex models from inputs might be diÔ¨Écult because shifting of input by bias could make
saturation of neurons (value ‚â§ 0) diÔ¨Écult.

2.6.3. Summary of discussions on ICNN

1. ICNN performs well when used to learn convex functions. In such cases, optimization over ICNN to minimize

an output is advantageous as a convex optimization problem is solved.

2. When ICNN is used for non-convex functions, approximation errors may be quite high. In particular, the global
optimum identiÔ¨Åed by ICNN need not necessarily be close to true global optimum or true local optimum.
3. Recurrent ICNN cannot model time delayed inputs at the output in the absence of bias. In the presence of bias,
the performance could be attributed to the shifting of the inputs, which might result in poor generalization.

Parameswaran S et al.: Preprint submitted to Elsevier

Page 6 of 19

CDiNN

Figure 7: Psuedo multilayer network. Each box is one Unary single layer network and grey circles are summations

3. CDiNN- Convex DiÔ¨Äerence Neural Network

We now describe a neural network architecture based on the idea of modeling arbitrary nonlinear functions as a
diÔ¨Äerence of polyhedral convex functions. We call this structure the CDiNN (Convex DiÔ¨Äerence Neural Network)
since this network learns the output as diÔ¨Äerence of convex functions from its constituent ICNN units. In this section,
we start with a brief description of diÔ¨Äerence of convex (DC) functions and their properties. We then focus on a method
called the Convex Concave Procedure (CCP) [12] used in the optimization of DC functions. The properties of this
network structure and extensions to a recurrent form are described.

Before we proceed, we point out one attempt [15], where the use of DC optimization with neural networks was
discussed. In this work, a given function is decomposed as unary functions and each unary function is represented as a
single hidden layer network. A library of unary functions is described. With this representation and the decomposition
scheme, the original function can be represented as a "pseudo multilayer neural network" (Figure 7). This multilayer
network can be rewritten manually as a DC function. A few example functions are used to demonstrate this idea in
this thesis. However, the focus of the work was not on learning from data but more on exploring the representational
capability. Even from a representational viewpoint, the procedure is not automatic. Nonetheless, it is worthwhile to
point out that researchers had identiÔ¨Åed the potential use of DC functions in neural networks.

In another work, [16] describe an ICNN architecture for binary classiÔ¨Åcation, where the output is a two dimensional
vector. Each element of this vector is a convex function of the inputs. Argmax of this output vector predicts the class
of the input. This results in a decision boundary, which is a diÔ¨Äerence of convex function. Further, they explore an
ensemble of ICNNs for multi-class classiÔ¨Åcation. A gating layer allows the choice of ICNN to be used based on the
region of interest. However, they do not explicitly formulate a diÔ¨Äerence of convex neural network architecture. As a
result, the use of diÔ¨Äerence of convex optimization techniques cannot be applied to Ô¨Ånd optimal inputs. Further, the
use of the diÔ¨Äerence of convex structure for regression and optimization are not discussed. Additionally, we believe
that the use of diÔ¨Äerence of convex optimization approaches will be of critical importance in further development of
training approaches for such neural network structures.

3.1. DiÔ¨Äerence of Convex functions and properties

DiÔ¨Äerence of Convex (DC) functions are those class of functions in which the output is represented as a diÔ¨Äerence
of convex functions of inputs as in Equation 3. In this equation, ùëì1
are convex functions of input x. Many
nonlinear functions can be represented as DiÔ¨Äerence of Convex (DC) functions [8]. For example, theorem 2 holds
good for any twice diÔ¨Äerentiable function. A convex function that is represented as a maximum of Ô¨Ånite number of
aÔ¨Éne functions is said to be a polyhedral convex function. A DC function where either or both of the convex functions
is(are) polyhedral convex function(s) is said to be a diÔ¨Äerence of polyhedral convex functions.

and ùëì2

ùëì (ùë•) = ùëì1(ùë•) ‚àí ùëì2(ùë•)

Parameswaran S et al.: Preprint submitted to Elsevier

(3)

Page 7 of 19

CDiNN

Theorem 2. Any twice diÔ¨Äerentiable function ùëì ‚à∂ ùë•‚àí > ùë¶ can be represented as a diÔ¨Äerence of two convex functions.
[12]

3.2. DiÔ¨Äerence of Convex (DC) optimization

DC Optimization deals with minimizing an objective function with constraints, when both the objective and con-
straints are represented by diÔ¨Äerence of convex functions. There are diÔ¨Äerent algorithms that solve DC optimization
problems. Here, we discuss a speciÔ¨Åc algorithm called Convex-Concave Procedure (CCP) which can be considered as
a version of DiÔ¨Äerence of Convex Algorithm (DCA) [12]. DCA involves solving alternating sequences of primal and
dual problem whereas CCP solves the problem directly as a primal formulation. More details on comparison between
DCA and CCP and their extensions can be found in [12]. Consider the DC optimization problem,

minimize ùëì0(ùë•) ‚àí ùëî0(ùë•)
subject to ùëìùëñ(ùë•) ‚àí ùëîùëñ(ùë•) ‚â§ 0,

ùëñ = 1, ‚Ä¶ , ùëö,

(4)

CCP is an iterative procedure that involves linearization of the concave part of the objective function (‚àíùëî0(ùë•)) and
constraints (‚àíùëîùëñ(ùë•)) to get a convex problem at each iteration. Then this convex problem is solved to global optimum to
get an initial point for the next iteration. The linearization used at each iteration is a Ô¨Årst order Taylor‚Äôs series expansion
around the point ùë•0
is the initial guess or the optimum value of x from
the previous iteration. This algorithm converges to a local optimum. Further, it is noted that at each iteration, CCP is
guaranteed to produce a better result than the previous iteration. This is because the convexiÔ¨Åed function ÃÇùëì (ùë•) ‚â• ùëì (ùë•),
since linearization of a concave function provides an upper bound to that function. This means that the value of the
true objective function is always less than the value of the modiÔ¨Åed objective. This result helps establish convergence
as explained in detail in [12].

, ùëî0(ùë•) = ùëî0(ùë•0) + ‚àá(ùëî0(ùë•0)) ‚ãÖ (ùë• ‚àí ùë•0), where ùë•0

In particular, it is easy to observe that if the DC function is a diÔ¨Äerence of polyhedral convex functions with ùëì0(ùë•)
in the form of maximum of aÔ¨Éne functions and when constraints are either absent or aÔ¨Éne, then the solution to the
convex problem at each iteration is a Linear Programming (LP) problem as given below. Further, we note that some
DC optimizations methods have nice properties when the convex functions in DC representation are polyhedral. For
example, DCA has Ô¨Ånite time convergence guarantee for diÔ¨Äerence of polyhedral convex functions. [11], [7]

ùë•ùëúùëùùë° = ùëéùëüùëîùëöùëñùëõ(ùëöùëéùë•(ùê¥1ùë• + ùëè1, ùê¥2ùë• + ùëè2...ùê¥ùëõùë• + ùëèùëõ))

This can be written as linear problem as:

ùë•ùëúùëùùë° = ùëéùëüùëîùëöùëñùëõ(ùë°)

with the constraints,

ùê¥1ùë• + ùëè1 ‚â§ ùë°
ùê¥2ùë• + ùëè2 ‚â§ ùë°
...
ùê¥ùëõùë• + ùëèùëõ ‚â§ ùë°

(5)

(6)

3.3. Design of CDiNN: Convex DiÔ¨Äerence Neural Network

We propose a Neural network architecture called Convex DiÔ¨Äerence Neural Network (CDiNN) where the output
is learnt as a diÔ¨Äerence of polyhedral convex functions of the input. We discuss two ways in which CDiNN can be
designed in this section, which are both based on the ICNN structure. Before that, we discuss the necessity of pass-
through layers in the ICNNs and investigate options to remove them.

3.3.1. Pass-through layers

Consider ICNN with a linear activation function at the output layer. In the absence of pass-through layers, ICNN
learns the function as a maximum of aÔ¨Éne representations, that is, ùëöùëéùë•(ùê¥1ùë• + ùëè1, ùê¥2ùë• + ùëè2...ùê¥ùëõùë• + ùëèùëõ, 0) + ùëèùëõ+1 where
ùëèùëõ+1 is the bias of output layer. This is because ReLU activation function clips the negative values at each hidden
layer. In the presence of pass-through layers, the pass-through at the output layer generates an additional aÔ¨Éne term

Parameswaran S et al.: Preprint submitted to Elsevier

Page 8 of 19

CDiNN

Figure 8: Parameter-Constrained ReLU (PC-ReLU) output = max (ùëéùë•, ùë•). This function is convex non-decreasing as
ùëé ‚àà [0, 1]

that renders the output to ùëöùëéùë•((ùê¥1ùë• + ùëè1, ùê¥2ùë• + ùëè2...ùê¥ùëõùë• + ùëèùëõ, 0) + (ùê¥ùëõ+1ùë• + ùëèùëõ+1)). Hence removal of pass-through
layers restricts the class of functions that can be learnt by ICNN. In particular, when biases are ignored, even identity
mapping cannot be learnt by ICNN without pass-through layers. One approach to address this problem is suggested in
[16], where it is proposed to use Leaky ReLU or ELU as activation function. ELU activation function is given by output
= (ùõº ‚ãÖ(ùëíùë• ‚àí1) if ùë• < 0, and ùë• if ùë• >= 0) and Leaky ReLU is given by output = (ùõºùë• if ùë• < 0, and ùë• if ùë• >= 0), where ùë• is
the input and ùõº is a hyperparameter. In that work, results are generated using ELU activation. It could be seen that with
the use of ELU activation function, convexity with respect to inputs is retained as long as ùõº ‚â§ 1. We propose the use
of Parametric ReLU as shown in Figure 8 [9]. The activation function is represented by output = (ùëéùë• if ùë• < 0, and ùë• if
ùë• >= 0), where ùë• is the input. It has one learnable parameter ‚Äôùëé‚Äô. Parametric-ReLU is similar to leaky ReLU, except
that instead of choosing a small value for ùëé, this parameter is learnt as a part of network training. If we constrain this
learnable parameter to be non-negative, then Parametric-ReLU is a convex non-decreasing function. We can represent
this function as ùëöùëéùë•(ùëéùë•, ùë•) with ùëé ‚àà [0, 1]. We call this as Parameter-Constrained ReLU (PC-ReLU). Thus, the use of
PC-ReLU allows for negative values as inputs in the inner hidden layers without aÔ¨Äecting the convexity.

It can be seen that PC-ReLU acts as a simple pass-through if ùëé = 1. As a result, if the dimension of the input is m
and if there are at least m neurons at each hidden layer, the network could learn the activation functions and weights such
that the inputs are passed through to hidden layers. From Theorem 1 and remark 2.3, we see that ICNN with one neuron
and a pass-through connection at each hidden layer and ReLU activation function can approximate any Lipschitz convex
function. Hence, ICNN without pass-through layer and PC-ReLU activation function can approximate any Lipschitz
convex function if there are at least ùëö + 1 neurons at each hidden layer and if the network has suÔ¨Écient hidden layers.
In essence, PC-ReLU activation function permits the network to learn implicit pass-through layers whenever needed.
It is not clear if ELU or Leaky-ReLU would have similar representational capability.

3.3.2. Architecture of Feedforward CDiNN

Simple extensions of the ICNN architecture allows the design of neural networks that can learn functions as dif-
ference of convex functions. We observe that it would be suÔ¨Écient to relax the non-negativity constraint at the last
layer in an ICNN to produce a diÔ¨Äerence of convex mapping. If ùëßùëò
represents the outputs of neurons in the last layer
layer is convex with respect to the inputs. Knowing that non-negative sum of convex
in an ICNN, every mapping in ùëßùëò
functions is convex and a negated convex function is a concave function, we can realize a CDiNN structure by adding
an output layer after ùëßùëò
layer to this output
layer are unconstrained,

layer in the ICNN structure. In this case, the connecting weights from ùëßùëò

ùë¶ = ùëì (ùëßùëò)
‚àë
ùë¶ =

(ùëä ùëù(ùëß)
ùëò

ùëßùëòùëù) +

‚àë

(ùëä ùëõ(ùëß)
ùëò

ùëßùëòùëõ)

(7)

(8)

Parameswaran S et al.: Preprint submitted to Elsevier

Page 9 of 19

CDiNN

Figure 9: CDiNN-1 architecture. All the weights except ùëä (ùë•)

0

,ùëä (ùëß)
ùëò

are constrained to be non-negative

‚Ä≤(ùë•)
Figure 10: CDiNN-2 architecture. All the weights except ùëä
0

‚Ä≤‚Ä≤(ùë•)
, ùëä
0

are constrained to be non-negative

ùëò

ùëò

where, ùëä ùëù(ùëß)

are the positive weights and ùëä ùëõ(ùëß)

are the negative weights connecting the last layer ùëßùëò

to output
layer. This formulation can be considered as sum/diÔ¨Äerence of outputs of multiple ICNNs. This is shown in Figure
9. We denote the CDiNN architecture which has a single neural network that follows equation 8 as CDiNN-1. In
the second extension, we propose two ICNNs without interconnections in hidden layers whose outputs are subtracted
at the Ô¨Ånal layer as shown in Figure 10. This CDiNN architecture with two independent ICNNs as CDiNN-2. We
note that CDiNN-1 conÔ¨Åguration has lesser number of nodes in hidden layer in comparison to CDiNN-2 for the same
number of network parameters and depth. This is due to the absence of interconnections in the hidden layer between
two ICNNs in CDiNN-2.

3.3.3. Pass-through layers and choice of activation function

We showed that through the use of PC-ReLU, pass-through layers might not be needed in ICNN structure that
forms the basis for CDiNN. We described in section 3.3.1 that ICNN with ReLU activation function clips the negative
values from the aÔ¨Éne functions when there are no pass-through layers and biases are disabled. However, when these
ICNNs are used in CDiNN, the convex functions are subtracted which permits negative values of aÔ¨Éne functions to
be represented. This shows that after the removal of pass-through layers, CDiNN has better representational ability
than ICNN even if ReLU is used as an activation function. Not withstanding this, in this paper, we use PC-ReLU as
the activation function for CDiNN.

3.3.4. Design of Recurrent CDiNN

We extend the recurrent ICNN architecture to design recurrent CDiNN as follows. With the removal of pass-
through layers and choice of PC-ReLU as activation function, recurrent convex functions in the recurrent CDiNN is
modeled by equation 9, where weights Z are non-negative. As M is unconstrained, ùë¶ùë°
is in the DC form. The recurrent
CDiNN is shown in Figure 11.

Parameswaran S et al.: Preprint submitted to Elsevier

Page 10 of 19

CDiNN

Figure 11: Recurrent CDiNN

ùëßùë° = PC-ReLU(ùëà ùë•ùë° + ùëçùëßùë°‚àí1)
ùë¶ùë° = (ùëÄùëßùë°)

(9)

3.4. Properties of CDiNN

1. Representation capability. Any function that is capable of being decomposed into diÔ¨Äerence of convex func-
tions can be learnt by CDiNN. We Ô¨Ånd from Theorem 1 that any convex function can be learnt by ICNN. Since
CDiNN learns the output as diÔ¨Äerence of outputs of ICNNs, we see that the structure of CDiNN is capable of
learning of any function that can be written as diÔ¨Äerence of two convex functions. We Ô¨Ånd from Theorem 2 that
any twice diÔ¨Äerentiable function can be represented as diÔ¨Äerence of convex functions. Thus, it is can be seen
that any twice diÔ¨Äerentiable function can be learnt by CDiNN.

Remark. It is observed that any neural network with one hidden layer of ReLU activation function and linear
output layer is a CDiNN Network. This is because at the output of hidden layer, each function is convex and the
output is a weighted sum and can be written as a diÔ¨Äerence of convex functions as in Equation 8.

2. CDiNN learns the function as diÔ¨Äerence of polyhedral convex functions We note that ICNN learns a function
in the form of maximum of Ô¨Ånite number of aÔ¨Éne functions. Since CDiNN learns the function mapping as
diÔ¨Äerence of outputs of multiple ICNNs, the function represented by CDiNN is in the form of diÔ¨Äerence of
polyhedral convex functions.

3.4.1. Training of CDiNN

In this paper, a modiÔ¨Åcation to the training procedure used for ICNN is pursued while training the CDiNN archi-
tecture. This removes the step that heuristically forces the weights to be non-negative in ICNN training. To constrain
the weights that connects layers to be non-negative, we modify the forward function by use of squared parameters. In
the context of this paper, in all the examples used in the next section on discussions and applications, (ùëä (ùëß)
)2 are used
as weights in the hidden layers of CDiNN. Adam optimizer [10] with Xavier-normal initialization [6] of weights and
zero initialization of biases are used in training all the neural networks described hereon.

ùëñ

Parameswaran S et al.: Preprint submitted to Elsevier

Page 11 of 19

CDiNN

4. Discussions and Applications

In this section, the properties of CDiNN presented previously are explored. First, we empirically show the repre-
sentational capability of CDiNN. We then discuss the results of optimization with CDiNN and compare it with ICNN
and Standard ReLU neural networks. Finally, we present one simple engineering example to highlight the advantages
of the CDiNN structure. In this Ô¨Årst paper on CDiNN, the intent of these examples is to describe the fundamental as-
pects of this architecture in detail. Benchmarking studies on large engineering examples and theoretical and application
studies on optimizing the network structure for performance will be pursued in the future.

4.1. EÔ¨Äect of removing Pass-through layers

We compare the performance of ICNN and CDiNN in the absence of pass-through layers. We discussed in Section
3.3.1 that ICNN with ReLU activation function would not even be able to learn identity mapping ùë¶ = ùëì (ùë¶) in absence
of pass-through layers when biases are ignored. But when such ICNNs are used in CDiNN, convex functions could be
learnt as ùëì1 = ùëÖùëíùêøùëà (ùë•) and ùëì2 = ùëÖùëíùêøùëà (‚àíùë•) and the output can be learnt such that ùëúùë¢ùë°ùëùùë¢ùë° = ùëì1 ‚àíùëì2 = ùë• . This shows
that in the absence of pass-through layers, CDiNN is better at learning even convex/aÔ¨Éne functions in comparison to
ICNN. We further show empirically in Table 1 (through simple nonlinear functions) that the removal of pass-through
layers in the individual ICNNs in the CDiNN structure does not signiÔ¨Åcantly impact the representational capability.

Table 1
Fit MSE for 1D and 2D synthetic function on CDiNN network with and without Pass-through layers

Function

Fit MSE with Pass-through layer

Fit MSE without Pass-through layer

1D Sine
1D Quadratic
1D Cubic
2D Circles
2D Moons

0.0007
0.0024
0.0032
0.069
0.0385

0.0003
0.0016
0.0021
0.069
0.0375

4.2. Non-linear function approximation

Synthetic 1D and 2D functions are used to show that CDiNN is capable of learning nonlinear functions.

4.2.1. 1D Regression

Consider the non-convex functions: ùë¶ = ùë†ùëñùëõ(5ùë•)‚àï5, ùë¶ = ùë•2 and ùë¶ = ùë•3 where, ùë• ‚àà [‚àí1, 1]. The Ô¨Åt of the function
for CDiNN Network is shown in Figures 12 and 13. We note that CDiNN is able to model all the three functions with
reasonable accuracy as can be seen from Table 1.

4.2.2. 2D classiÔ¨Åcation

In Figure 14, classiÔ¨Åcation performance of CDiNN on a 2D dataset is illustrated using a two-dimensional binary
classiÔ¨Åcation problem from the sci-kit-learn toolkit [14]. The points in the picture represent training data with output
as either 0 or 1 and the regions are coloured based on the value of predictions ‚àà [0, 1]. It could be seen that the CDiNN
is able to learn complex decision boundaries.

4.3. Time delay system

= ùëÖùëíùêøùëà (‚àíùë¢ùë°‚àíùëö). Hence ùë¶ùë° = ùë¶ùë°1

In section 2.6.2, we saw that the recurrent version of ICNN cannot model time delay systems eÔ¨Äectively. We now
proceed to show that CDiNN network can model time-delay systems. Assume, ùë¶ùë° = ùë¢ùë°‚àíùëö, ùëö ‚â• 0. Let PC-ReLU
parameter be set to 0 so that activation function is ReLU. We see that the network could learn ùë¶ùë°1
= ùëÖùëíùêøùëà (ùë¢ùë°‚àíùëö) and
can be learnt. We also discuss the experimental results on the
= ùë¢ùë°‚àíùëö
ùë¶ùë°ùëß
response of CDiNN with 10 recurrent neurons. This has similar number of learnable parameters to ICNN discussed
earlier. In this case as well, we disable the learning of bias in recurrent and feed-forward layers to avoid eÔ¨Äects due
to shifting of input. The input that is used as excitation signal is ùë¢ùë° = randomUniform() ‚àà [‚àí1, 1], which is random
signal uniformly sampled from [-1,1]. The output is ùë¶ùë° = ùë¢ùë°‚àí4 + ùë¢ùë°‚àí3 + ùë¢ùë°‚àí2 + ùë¢ùë°‚àí1
and the number of samples given
per sequence as inputs to the network is 5. We see that the CDiNN is able to capture the pattern of noise in the input
at output ùë¶ùë°

from Figure 15.

‚àí ùë¶ùë°2

Parameswaran S et al.: Preprint submitted to Elsevier

Page 12 of 19

CDiNN

Figure 12: Learning of sine by CDiNN. The green curve is the learnt function, the yellow points represent the training
data. Mean square Loss of training data is 0.0002 and number of training epochs is 800.

Figure 13: Learning of quadratic and cubic functions by CDiNN . The green curve is the learnt function, the yellow points
represent the training data. Steps and loss indicate number of training epochs (800) and Mean square loss of training
data respectively

Figure 14: ClassiÔ¨Åcation decision boundaries of CDiNN on 2D synthetic data

4.4. Discussions on Optimization

We now demonstrate the utility of CDiNN in solving standard optimization functions. We Ô¨Årst learn the optimiza-
tion function in DC form using CDiNN and use CCP to solve it. We compare the results of optimization to ICNN and
Standard-ReLU neural networks.

4.4.1. Test setup

We use three optimization test functions from [19]. We add a constant value of 5 to each of the functions. We train

the network by using scaled inputs and outputs (so that inputs and outputs ‚àà [-1,1]).

Parameswaran S et al.: Preprint submitted to Elsevier

Page 13 of 19

CDiNN

Figure 15: Output predicted (red) vs truth (black) by CDiNN RNN. The red line and black curves almost coincide with
each other

‚Ä¢ ùëì = Three humped camel function +5: This is a Non-convex 2D input function. Global minimum is 5. The

maximum value of function in training set is 2.1E+03, which is scaled to 1 for training the network.

‚Ä¢ ùëì = Sumpower +5: This is a convex 5D input function. True minimum is 5. The maximum value of function

in training set is 10, which is scaled to 1 for training the network.

‚Ä¢ ùëì = Matya+5: This is a convex 2D input function. True minimum is 5. The maximum value of function in

training set is 105, which is scaled to 1 for training the network.

The experiment involves training the following networks.

‚Ä¢ Standard Parametric-ReLU network of 30 hidden layer neurons

‚Ä¢ CDiNN-1 network with 30 hidden layer neurons

‚Ä¢ CDiNN-2 network with 15 hidden layer neurons in each of its ICNNs

All the networks have only one learnable hidden layer and almost equal number of learnable network parameters.

After training, we optimize the input for minimizing the output. The initial value for optimization was [-1,-1] for
SumPower function and [1,1] for Matya and Camel function. The optimization used for standard ReLU network is to
use Filtered-ùõΩ (sub-)gradient descent [18] [2]. The algorithm is stopped based on a certain convergence criterion or
after a maximum number of iterations. At every iteration k,

ùë†ùëò = (1 ‚àí ùõΩ) ‚ãÖ ùëîùëò + ùõΩ ‚ãÖ ùë†ùëò‚àí1

ùë•ùëò+1 = ùë•ùëò ‚àí ùõºùëò ‚ãÖ ùë†ùëò

is step size at iteration k, ùõΩ is set to 0.25 and ùëîùëò

where ùõºùëò
is the gradient of output with respect to input at iteration
k. The step size used is either Constant step size (ùõº = ùëêùëúùëõùë†ùë°ùëéùëõùë°) or "Square summable but not summable" step size
(ùõº = ùõº‚àïùëò). The experiments are run with diÔ¨Äerent starting step sizes [0.01, 0.1, 5, 10]. The optimization for CDiNN
network is performed using CCP procedure as explained in Section 3.2. Standard linear programming optimizers from
CVXOPT Modelling [13] library in python are used to Ô¨Ånd the optimum value of the convexiÔ¨Åed function at each
iteration. The number of maximum iterations is set to 200 or 500. The stopping criteria for convergence is that the
diÔ¨Äerence of function values between successive iterations is either 1ùëí‚àí3 or 1ùëí‚àí5. Since training of the non-convex
neural network can lead to diÔ¨Äerent parameters due to local optima, we train the network three times and report the
results as follows. Choose the network that reported best optimization results in CCP for CDiNN-1 and CDiNN-2
and choose the network that reported best optimization results in Gradient descent for Standard ReLU network. This
is to done to ensure that results of optimization are not aÔ¨Äected due to local optima problems in training the neural
networks. 1

1The time taken for optimization needs to be understood with caution since the algorithm is not run on dedicated processors and these might

Parameswaran S et al.: Preprint submitted to Elsevier

Page 14 of 19

CDiNN

Table 2
Fit and optimization performance for Optimization test functions with stopping precision = 0.001

Func.

Type

Algorithm Fit MSE

Optimization parameters

Camel
Camel
Camel
Camel
Sum power
Sum power
Sum power
Sum power
Matya
Matya
Matya
Matya

CDiNN-1
CDiNN-2
Std. ReLU
Std. ReLU
CDiNN-1
CDiNN-2
Std. ReLU
Std. ReLU
CDiNN-1
CDiNN-2
Std. ReLU
Std. ReLU

CCP
CCP
GD
GD
CCP
CCP
GD
GD
CCP
CCP
GD
GD

3.26-05
1.9E-05
3.7E-05
3.7E-05
1.29e-05
8.2E-04
8.7E-05
8.7E-05
7.8e-06
1.8e-05
4.1E-06
4.1E-06

ùõº0 = 0.1, ùõºùëõ+1 = ùõºùëõ
ùõº0 = 5, ùõºùëõ+1 = ùõºùëõ

ùõº0 = 5, ùõºùëõ+1 = ùõºùëõ
ùõº0 = 0.01, ùõºùëõ+1 = ùõºùëõ‚àïùëõ

ùõº0 = 10, ùõºùëõ+1 = ùõºùëõ‚àïùëõ
ùõº0 = 10, ùõºùëõ+1 = ùõºùëõ

ùë¶ùëúùëùùë°

5.07
6.97
8.31
2001.46
5.02
5
5
9.86
5.15
5.23
5.08
104.80

Exec. time (s)

0.04
0.041
3.69
1.37
0.07
0.06
6
0.06
0.08
0.08
1.78
4.62

Table 3
Fit and optimization performance for Optimization test functions with stopping precision = 0.00001

Func.

Type

Algorithm

Fit MSE

Optimization parameters

Camel
Camel
Camel
Camel
Camel

CDiNN-1
CDiNN-2
Std. ReLU
Std. ReLU
Std. ReLU

CCP
CCP
GD
GD
GD

3.26-05
1.9E-05
3.7E-05
3.7E-05
3.7E-05

ùõº0 = 0.1, ùõºùëõ+1 = ùõºùëõ
ùõº0 = 10, ùõºùëõ+1 = ùõºùëõ‚àïùëõ
ùõº0 = 5, ùõºùëõ+1 = ùõºùëõ

ùë¶ùëúùëùùë°

5.07
6.97
8.31
8.98
2001.46

Exec. time (s)

0.04
0.04
3.69
3.99
1.37

Figure 16: Optimization using CCP on the classiÔ¨Åcation dataset. The initial points (cross) and the Ô¨Ånal solution (star)
are marked for a) Standard NN b) ICNN c) CDiNN

4.4.2. Test result Discussion

We discuss the performance of CDiNN and Standard ReLU neural networks from Table 2 and 3.

1. Learning the function: We observe that Ô¨Åt performances of CDiNN-1 and CDiNN-2 network are on par with

Standard ReLU neural networks.

2. Execution time: Execution times of CDiNN-1 and CDiNN-2 are also comparable to each other with the avail-
able results. No conclusions are drawn based on execution times between gradient descent and CCP since the
codes are not optimized versions of these algorithms. But we observe that the execution time of CCP optimiza-
tion algorithm on CDiNN network shows little variation for a given neural network, which is ideal for real-time
applications. In real time applications, it is desirable to have a fairly consistent execution time rather than un-
predictable execution times.

not be the best implementations of the corresponding optimization algorithms.

Parameswaran S et al.: Preprint submitted to Elsevier

Page 15 of 19

CDiNN

Figure 17: Partial input optimization using CCP on the classiÔ¨Åcation dataset along horizontal axis. The initial points
(cross) and the Ô¨Ånal solution (star) are marked

3. Variability in results: We see that CCP has no hyper-parameter tuning (except for choice of convergence pre-
cision) and the results from Standard ReLU network show a higher variance with respect to optimization hyper-
parameters.

4. Optimization results: In general, we see that CDiNN-CCP pair produces results that are very close to the
optimally tuned gradient descent algorithm. However, the tuning parameters in the CDiNN-CCP solution have
very little eÔ¨Äect on the Ô¨Ånal solution from the viewpoints of accuracy and execution time.

4.4.3. Finding partial inputs

Now we apply CCP in Ô¨Ånding partial inputs. For illustration, we Ô¨Åx one of the coordinates (y coordinate) in the
circles 2D classiÔ¨Åcation task and run the optimization to Ô¨Ånd the other coordinate value that minimizes the output. The
results are given in Figure 17.

4.5. Estimating time and location of Pollutant spill

In this section, we compare the performance of ICNN, CDiNN and Standard ReLU Neural network in estimating
the time and location of pollutant spill from simulated data. We use the non-smooth function in [19] for simulation
which captures scaled pollutant concentration as a function of distance and time.

4.5.1. Problem description

The problem under study involves two-spills, Ô¨Årst spill at location = 0 and time = 0 and the second spill at location

= 0.8 and time = 10 and the system is simulated for ùëôùëúùëêùëéùë°ùëñùëúùëõ ‚àà [0.01, 1] and ùë°ùëñùëöùëí ‚àà [0.01, 15].

Given the pollutant concentration measurements at some locations at diÔ¨Äerent times, we would like to compare the
Ô¨Åt of diÔ¨Äerent neural networks and optimization to identify the location and time of the spill. The data for training the
neural network is sampled at an interval of 0.02 along each dimension (location and time) and for test data both the
inputs are sampled at an interval of 0.05. A weight of (-1) is added at the output of ICNN to enable it learn the output as
concave function. The inputs are normalized to [-1,1] before training. Optimization is performed using (sub-) gradient
ascent for standard ReLU, standard convex optimizers in python [13] are used for ICNN neural networks, and CCP
algorithm is used on negated CDiNN output to determine the location and time of the spill. We also add the constraint
that the feasible solution should be such that distance > 0.35 and time > 5.25. We use the starting point as d=1.2 and
t = 15.2 and the optimum local solution is at d=0.8 and t=10.02. The optimal solution for time is considered as 10.02
seconds because the spill happens immediately after 10 seconds and the training data resolution is 0.02 seconds. We
run the experiment two times and report the best optimization result in each network. This helps in reducing the eÔ¨Äects
of local minima in training the neural networks to aÔ¨Äect the optimization results on the inputs.

4.5.2. Test result discussion

1. Learning the function: The Ô¨Åt performance is shown in Table 4. The performance of ICNN, CDiNN with the
comparable number of learnable parameters is shown in Figure 19. Further, the following observations show
the ability of CDiNN to learn function mapping similar to standard ReLU networks. The true function is a
non-smooth (not twice-diÔ¨Äerentiable) and discontinuous function at the location of second spill. It is known
that neural networks with continuous activation functions like ReLU wouldn‚Äôt be able to model discontinuities.

Parameswaran S et al.: Preprint submitted to Elsevier

Page 16 of 19

CDiNN

Figure 18: (a) The scaled model response captured by ICNN (b) The scaled model response captured by CDiNN-1

Figure 19: (a) The scaled model response captured by Standard Parametric-ReLU neural network (b) The scaled model
response captured by CDiNN-2

This is because ReLU is a continuous function and composition or summation of continuous functions is con-
tinuous. We see from Figure 19 that the value of concentration at the point of discontinuity (concentration =
3.24) [d=0.8,t=10.02] is not learnt well in all the networks. But we observe that CDiNN is able to capture the
function in diÔ¨Äerence of convex form with performances similar to that of Standard ReLU network.

2. Optimization: The local maxima in time is expected immediately after 10 seconds. We consider time = 10.02
as the local solution since the training dataset is sampled at the resolution of 0.02 seconds. From Table 5, we
observe that ICNN performs poorly due to poor Ô¨Åt, the results from Standard ReLU NN-gradient ascent depends
heavily on the choice of hyper-parameters but the optimization with CDiNN-CCP consistently provides the best
results.

Table 4
Fit of diÔ¨Äerent neural networks for Pollutant spill function mapping

Network

Standard Parametric-ReLU NN
ICNN
CDiNN-1
CDiNN-2

MSE of pollutant spill truth vs prediction

0.02
0.05
0.02
0.02

Parameswaran S et al.: Preprint submitted to Elsevier

Page 17 of 19

CDiNN

Table 5
Optimization performance for identiÔ¨Åcation of location and time of pollutant spill

Title

Initial value ùë•ùëú

Step size

Final value ùë•ùëúùëùùë°

ùë¶ùëúùëùùë°

ùë°ùëíùë•ùëíùëê (seconds)

Expected local maximum
Standard Parametric-ReLU NN
Standard Parametric-ReLU NN
Standard Parametric-ReLU NN
ICNN
CDiNN-1
CDiNN-2

d=0.8 t=10.02
d=1.2, t=15.2
d=1.2, t=15.2
d=1.2, t=15.2
d=1.2, t=15.2
d=1.2, t=15.2
d=1.2, t=15.2

-
0.01
10
0.1
-
-
-

-
d=0.87 t=10.26
d=0.57 t=5.25
d=0.92 t=11.86
d=0.91 t=12.95
d=0.84 t=10.23
d=0.87 t=10.31

3.24
0.24
‚àí0.77
‚àí0.43
‚àí0.52
0.35
0.16

-
0.2
0.21
0.2
0.02
0.04
0.03

5. Conclusion and future work

In this paper, we discussed the disadvantages of ICNN and conclude that while ICNN provides a framework for
capturing convex functions, use of ICNN for non-convex function approximation and subsequent use in control or any
other application is not desirable since global optimum as learnt by ICNN might not be close to true global optimum.
We also discussed that optimization results using (sub-)gradient descent methods on standard ReLU based feed-forward
networks heavily depend on the choice of a number of hyper-parameters, with very little convergence guarantees. As
a solution to these issues, we introduced a new neural network architecture called CDiNN. We showed that the use of
Convex-Concave procedure with the new architecture helps capture the non-linearity better and produces good quality
local solutions with guarantees on convergence. We believe that the proposed network can be a good candidate to
solve optimal control problems and other problems that involve Ô¨Ånding the optimum input(s) to the neural network
for a desired output. We anticipate that the application of global optimization techniques on CDiNN could lead to
signiÔ¨Åcantly improved results in several applications.

References
[1] Amos, B., Xu, L., Kolter, J.Z., 2017. Input convex neural networks, in: Precup, D., Teh, Y.W. (Eds.), Proceedings of the 34th International
Conference on Machine Learning, PMLR, International Convention Centre, Sydney, Australia. pp. 146‚Äì155. URL: http://proceedings.
mlr.press/v70/amos17b.html.

[2] Boyd, S., Park, J., . subgrad_method_notes.dvi. https://stanford.edu/class/ee364b/lectures/subgrad_method_notes.pdf.

(Accessed on 03/09/2021).

[3] B√ºnning, F., Schalbetter, A., Aboudonia, A., de Badyn, M.H., Heer, P., Lygeros, J., 2020. Input convex neural networks for building mpc.

arXiv:2011.13227.

[4] Chen, Y., Shi, Y., Zhang, B., 2019. Optimal control via neural networks: A convex approach, in: International Conference on Learning

Representations. URL: https://openreview.net/forum?id=H1MW72AcK7.

[5] Chen, Y., Shi, Y., Zhang, B., 2020. Input convex neural networks for optimal voltage regulation. arXiv:2002.08684.
[6] Glorot, X., Bengio, Y., 2010. Understanding the diÔ¨Éculty of training deep feedforward neural networks, in: In Proceedings of the International

Conference on ArtiÔ¨Åcial Intelligence and Statistics (AISTATS‚Äô10). Society for ArtiÔ¨Åcial Intelligence and Statistics.

[7] Hang, N.T., Yen, N.D., 2016. On the Problem of Minimizing a DiÔ¨Äerence of Polyhedral Convex Functions Under Linear Constraints. Journal
of Optimization Theory and Applications 171, 617‚Äì642. URL: https://ideas.repec.org/a/spr/joptap/v171y2016i2d10.1007_
s10957-015-0769-x.html, doi:10.1007/s10957-015-0769-x.

[8] Hartman, P., 1959. On functions representable as a diÔ¨Äerence of convex functions.

PaciÔ¨Åc J. Math. 9, 707‚Äì713. URL: https:

//projecteuclid.org:443/euclid.pjm/1103039111.

[9] He, K., Zhang, X., Ren, S., Sun, J., 2015. Delving deep into rectiÔ¨Åers: Surpassing human-level performance on imagenet classiÔ¨Åcation, in:
Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), IEEE Computer Society, USA. p. 1026‚Äì1034. URL:
https://doi.org/10.1109/ICCV.2015.123, doi:10.1109/ICCV.2015.123.

[10] Kingma, D.P., Ba, J., 2017. Adam: A method for stochastic optimization. arXiv:1412.6980.
[11] Le Thi, H.A., Tao, P., 2005. The dc (diÔ¨Äerence of convex functions) programming and dca revisited with dc models of real world nonconvex

optimization problems. Annals of Operations Research 133, 23‚Äì46. doi:10.1007/s10479-004-5022-1.

[12] Lipp, T., Boyd, S., 2015. Variations and extension of the convex‚Äìconcave procedure. Optimization and Engineering 17. doi:10.1007/

s11081-015-9294-x.

[13] Martin Andersen, J.D., Vandenberghe, L., . Home ‚Äî cvxopt. https://cvxopt.org/. (Accessed on 03/09/2021).
[14] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Van-
derplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E., 2011. Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research 12, 2825‚Äì2830.

Parameswaran S et al.: Preprint submitted to Elsevier

Page 18 of 19

CDiNN

[15] Roddier, N., 1994. Global optimization via neural networks and D.C. programming. Ph.D. thesis. The University of Arizona. URL: http:

//hdl.handle.net/10150/186617.

[16] Sivaprasad, S., Singh, A., Manwani, N., Gandhi, V., 2020. The curious case of convex neural networks. arXiv:2006.05103.
[17] Sonoda, S., Murata, N., 2015. Neural network with unbounded activation functions is universal approximator. Applied and Computational

Harmonic Analysis doi:10.1016/j.acha.2015.12.005.

[18] Stephen Boyd, L.X., Mutapcic, A., . subgrad_method.pdf. https://web.stanford.edu/class/ee392o/subgrad_method.pdf. (Ac-

cessed on 03/09/2021).

[19] Surjanovic, S., Bingham, D., . Virtual library of simulation experiments: Test functions and datasets. Retrieved July 10, 2020, from http:

//www.sfu.ca/~ssurjano.

[20] user147263, . The maximum of several aÔ¨Éne functions is a polyhedral function. Mathematics Stack Exchange. URL: https://math.

stackexchange.com/q/1381592. uRL:https://math.stackexchange.com/q/1381592 (version: 2015-08-02).

[21] Yang, S., Bequette, B.W., 2021. Optimization-based control using input convex neural networks. Computers and Chemical Engineering
144, 107143. URL: https://www.sciencedirect.com/science/article/pii/S0098135420308942, doi:https://doi.org/10.
1016/j.compchemeng.2020.107143.

[22] Z, R., . Real analysis - a c2-function as diÔ¨Äerence of two convex functions - mathematics stack exchange. https://math.stackexchange.

com/questions/2693451/a-c2-function-as-difference-of-two-convex-functions. (Accessed on 02/25/2021).

Parameswaran Sankaranarayanan is a doctoral research scholar at Indian Institute of Technology Madras. He does his
research with Robert Bosch Centre for Data Science and ArtiÔ¨Åcial Intelligence and Systems Engineering of Natural and
ArtiÔ¨Åcial group at Indian Institute of Technology Madras. He completed his bachelors in Electronics and Instrumenta-
tion Engineering in Madras Institute of Technology, Anna University Chennai. He has 7 years of industrial experience in
automotive electronics and has done a number of projects involving embedded control systems.

Prof. Raghunathan Rengaswamy is a Professor at Indian Institute of Technology Madras. Prior to this, he was a Professor
and co-director of the Process Control and Optimization Consortium (PCOC) at Texas Tech University, Lubbock, TX USA,
Associate and full Professor at Clarkson University, Potsdam, NY and Assistant Professor at IIT Bombay, Mumbai, India.
Rengaswamy was the recipient of the Young Engineer Award for the year 2000 awarded by the Indian National Academy
of Engineering (INAE). A paper that he co-authored was chosen by the International Federation of Automatic Control
(IFAC) for the Best Paper Prize, for the years 2002-2005, in Engineering Applications of ArtiÔ¨Åcial Intelligence Journal in
the category ‚Äì Application-oriented paper on Symbolic AI Approaches. He was elected a Fellow of INAE in 2017.

Parameswaran S et al.: Preprint submitted to Elsevier

Page 19 of 19

