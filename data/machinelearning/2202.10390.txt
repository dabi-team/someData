2
2
0
2

b
e
F
1
2

]

B
D
.
s
c
[

1
v
0
9
3
0
1
.
2
0
2
2
:
v
i
X
r
a

Optimizing Recursive Queries with Program Synthesis

Yisu Remy Wang
University of Washington
relationalAI
USA

Mahmoud Abo Khamis
relationalAI
USA

Hung Q. Ngo
relationalAI
USA

Reinhard Pichler
TU Wien
Austria

Dan Suciu
University of Washington
relationalAI
USA

ABSTRACT
Most work on query optimization has concentrated on loop-free
queries. However, data science and machine learning workloads to-
day typically involve recursive or iterative computation. In this
work, we propose a novel framework for optimizing recursive
queries using methods from program synthesis. In particular, we
introduce a simple yet powerful optimization rule called the “FGH-
rule” which aims to find a faster way to evaluate a recursive program.
The solution is found by making use of powerful tools, such as a
program synthesizer, an SMT-solver, and an equality saturation sys-
tem. We demonstrate the strength of the optimization by showing
that the FGH-rule can lead to speedups up to 4 orders of magnitude
on three, already optimized Datalog systems.

1 INTRODUCTION
Most database systems are designed to support primarily non-
recursive (loop-free) queries. Their optimizers are based on the
rule-driven, cost-based Volcano architecture, designed specifically
for optimizing non-recursive query plans. However, most data sci-
ence and machine learning workloads today involve some form
of recursion or iteration. Examples include finding the connected
components of a graph, computing the page rank, computing the
network centrality, minimizing an objective function using gradi-
ent descent, etc. The importance of supporting recursive queries
has been noted by system designers. Some modern data analytics
systems, like Spark or Tensorflow, support for-loops. The SQL stan-
dard defines a limited form of recursive queries, using the with
construct, and some popular engines, like Postgres or SQLite, do
support this restricted form of recursion.

Datalog is a language designed specifically for recursive queries,
and it is gaining in popularity [3, 12, 14, 22, 36, 38, 39, 49, 50].
But the optimization problem for recursive queries is much less
studied. A datalog program consists of multiple rules, defining
several, mutually recursive relations, and one distinguished relation
name which is the output of the program. The effect of the program
consist of repeatedly applying the rules, sometimes called the body
of the program, until a fixpoint is reached, then it returns the output
relation. Datalog engines typically optimize the loop body, without
optimizing the actual loop. The few systems that do (for example
Soufflé) apply only limited optimization techniques, like magic set
optimization and semi-naive evaluation, which are mainly restricted
to positive Datalog queries.

In this paper we describe a new query optimization framework
for recursive queries. Our framework replaces a recursive program
with another, equivalent recursive program, whose body may be
quite different, and thus focuses on optimizing the recursive pro-
gram as a whole, not on optimizing its body in isolation; the latter
can be done separately, using standard query optimization tech-
niques. Our optimization is based on a novel rewrite rule for re-
cursive programs, called the FGH-rule, which we implement using
program synthesis, a technique developed in the programming lan-
guages and verification communities. We introduce a new method
for inferring loop invariants, which extends the reach of the FGH-
rule, and also show how to use global constraints on the data for
semantic optimizations using the FGH-rule. We explain these points
in some details next.

The FGH-Rule At the core of our approach is a novel, yet
very simple rewrite rule, called the FGH-rule (pronounced fig-rule),
which can be used to prove that two recursive programs are equiv-
alent, even when their loop bodies are quite different. We show
that the FGH-rule can express previously known optimizations
for Datalog, including magic sets and semi-naive evaluation, and
also a wide range of new optimizations. The optimized program is
often significantly more efficient than the original program, and
sometimes can have a strictly lower asymptotic complexity. We
implemented a source-to-source optimizer using the FGH-rule, eval-
uated its effectiveness on several Datalog systems, and observed
speedups of up to 4 orders of magnitude (Sec. 8).

For a taste of the FGH-optimization, consider the following ex-
ample, from [55, 56]: compute the connected components of an
undirected graph 𝐸 (𝑥, 𝑦). The Datalog program in Fig. 1 (a) achieves
this by first computing the transitive closure relation 𝑇𝐶 (𝑥, 𝑦), then
computing a min-aggregate query assigning to every node 𝑥 the
smallest label 𝐿[𝑦] of all nodes 𝑦 reachable from 𝑥. In contrast, the
optimized program in Fig. 1 (b) computes directly the CC label of
every node 𝑥 as the minimum of its own label and the smallest
CC label of its neighbors, using a single recursive rule with min-
aggregation. The space complexity of the transitive closure is 𝑂 (𝑛2),
which, in practice, is prohibitively expensive on large graphs. On
the other hand, the optimized query has space complexity 𝑂 (𝑛).

Pattern Matching vs. Query Synthesis Applying the FGH-
rule is an instance of query rewriting using views. In that problem
we are given a set of view expressions and a query, and the task
is to rewrite the query to use the view expressions rather than
the base relations. This problem has been extensively studied in
the literature [21], and today’s database systems perform it using

 
 
 
 
 
 
𝑇𝐶 (𝑥, 𝑦) :- [𝑥 = 𝑦] ∨ ∃𝑧 (𝐸 (𝑥, 𝑧) ∧ 𝑇𝐶 (𝑧, 𝑦))

𝐶𝐶 [𝑥] :- min

𝑦

{𝐿[𝑦] | 𝑇𝐶 (𝑥, 𝑦)}

(a)

𝐶𝐶 [𝑥] :- min(𝐿[𝑥], min

𝑦

{𝐶𝐶 [𝑦] | 𝐸 (𝑥, 𝑦)})

(b)

Figure 1: Unoptimized (a) and optimized (b) Datalog pro-
gram for the connected components of an undirected graph.

pattern matching [16]. This is a form of transformational synthesis,
where every candidate query rewriting is guaranteed to be correct,
because it is obtained by applying a limited set of manually crafted
rules (patterns), which are guaranteed to be correct. However, the
FGH-rule often requires exploring a very large space, which cannot
be covered by a limited set of rules. In this paper we propose to use
counterexample-guided inductive synthesis (CEGIS) for this purpose,
which is a technique designed for program sketching [44, 47]. When
applied to our context, we call this technique query synthesis. Unlike
pattern matching, query synthesis explores a much larger space,
by examining rewritings that are not necessarily correct, and need
to be checked for correctness by a verifier (z3 in our system). The
verifier also produces a small counterexample database for each
rejected candidate, and these counterexamples are collected by the
synthesizer and used to produce only candidate rewritings that
pass all the previous counterexamples, which significantly prunes
the search space of the synthesizer. We report in Sec. 8 synthesis
times of less than 1 second, even for complex queries that use global
constraints and require inferring loop invariants.

Monotone Queries and Semiring Semantics Datalog is, by
definition, restricted to monotone queries. This ensures that ev-
ery query has a well-defined semantics, namely the least fixpoint
of its immediate consequence operator. Existing optimizations for
Datalog, like semi-naive evaluation and magic set rewriting, apply
mainly to monotone queries. Even stratified negation can (if at
all) only be handled by imposing appropriate restrictions [45]. But
queries that contain aggregates or negation (expressed in SQL via
subqueries) are not monotone, and most systems that support re-
cursion prohibit the combination of aggregates and recursion. This
has two shortcomings: it limits what kind of queries the user can
express, and also prevents many of our FGH-rewritings. For exam-
ple, the simple computation of connected components in Fig. 1 (a)
can be expressed in PostgreSQL, or in SQLite, or in Soufflé, because
the first rule uses only recursion and the second rule uses only
aggregation. However, none of these systems accepts the query in
Fig. 1 (b), because it combines recursion and aggregation.1 In order
to express such queries, in this paper we propose an extension of
Datalog, following the approach in [18], where the relations are
interpreted over ordered semirings.

1Prior work [15, 38] has proposed extending Datalog with min and max aggregates by
explicitly re-defining the semantics of recursive rules with aggregates. Our approach
keeps the standard least fixpoint semantics, but generalizes the semiring.

Yisu Remy Wang, Mahmoud Abo Khamis, Hung Q. Ngo, Reinhard Pichler, and Dan Suciu

A semiring is an algebraic structure with two operations, ⊕, ⊗.
Traditional Datalog corresponds to the Boolean semiring, where
these two operators are ∨, ∧, while the query in Fig. 1 (b) is over the
Tropical semiring, where the two operators are min, + (reviewed
in Sec. 2). We call this extension of Datalog to ordered semirings
Datalog◦, pronounced “Datalogo”, where the circle represents the
semiring. In Datalog◦ recursion is still restricted to monotone2
queries, but monotone queries in Datalog◦ include queries with
aggregates, over an appropriate semiring. The query in Fig. 1 (b) is
monotone over the (ordered) tropical semiring.

Loop Invariants One difficulty in reasoning about loops in
programming languages is the need to discover loop invariants.
Some (but not all) applications of the FGH-rule also require the
discovery of loop invariants. We describe a novel technique for
inferring loop invariants for Datalog◦ programs, by combining
symbolic execution with equality saturation, and using a verifier.
We execute symbolically the recursive program for a very small
number of iterations (five in our system), obtain query expressions
for the IDBs (the recursive predicates), and construct all identities
satisfied by the IDBs. Then, we retain only candidates that hold at
each iteration, and check each candidate for correctness using the
SMT solver. By inferring and using loop invariants we show that we
can significantly improve some instances of magic-set optimizations
from the literature: we call the new optimization beyond magic.

Constraints and Semantic Optimizations Optimizations that
are conditioned on certain constraints on the database are known
as semantic optimizations [33]. SQL optimizers routinely use key
constraints and foreign key constraints to optimize queries. More
powerful optimizations can be performed using the chase and back-
chase framework [10, 31], and these include optimizations under
inclusion constraints, or conditional functional dependencies, or tu-
ple generating constraints. However, all constraints that are useful
for optimizing non-recursive queries are local. In contrast, the FGH-
rule optimizes recursive queries, and therefore it can also exploit
global constraints. For example, suppose the database represents a
graph, and the global constraint states that the graph is a tree. This
global constraint does not help optimize non-recursive queries, but
can be used to great advantage to optimize some recursive queries;
we give details in Sec. 3.3.

Equality Saturation Systems Throughout our optimizer we
need to manage symbolic expressions of queries, and their equiva-
lence classes, as defined by a set of rules. We uses for this purpose a
state-of-the-art Equality Saturation System (EQSAT), EGG [54]. We
show how to use EQSAT for checking equality under constraints, in-
ferring loop invariants, and “denormalization” (which is essentially
query rewriting using views).

Related Work Our work was partially inspired by the PreM con-
dition, described by Zaniolo et al. [55], which, as we shall explain, is
a special case of the FGH-rule. Unlike our system, their implemen-
tation required the programmer to check the PreM manually, then
perform the corresponding optimization. Seveal prior systems lever-
aged SMT-solvers to reason about query languages [8, 19, 37, 48, 51];
but none of these consider recursive queries. Datalog synthesizers
have been described in [2, 32, 42, 43, 52]. Their setting is different
from ours: the specification is given by input-output examples, and

2This monotonicity is over the partial order from the ordered semiring.

Optimizing Recursive Queries with Program Synthesis

the synthesizer needs to produce a program that matches all ex-
amples. A design choice that we made, and which sets us further
aside from the previous systems, is to use an existing CEGIS system,
Rosette; thus, we do not aim to improve the CEGIS system itself,
but optimize the way we use it.

Contributions In summary, the main contribution of this paper
consists of a new, principled and powerful method for optimizing
recursive queries. We make the following specific contributions:

• We introduce a simple optimization rule for recursive queries,

called the FGH-rule (Sec. 3).

• We show how the FGH-rule captures known optimizations
(magic sets, PreM, semi-naive), (Sec. 3.1), some new optimiza-
tions (Sec. 3.2), and optimizations under global constraints
(Sec. 3.3).

• We present our novel framework for query optimization via

the FGH-rule (Sec. 4).

• We describe how an SMT solver (Sec. 5) and a CEGIS system
(Sec. 6) can be profitably integrated into our FGH-optimizer.
• We describe how to use an EQSAT system for various tasks
in the FGH optimizer: loop-invariant inference, denormal-
ization, and checking equivalence under constraints (Sec. 7).

2 BACKGROUND
Datalog A relation of arity 𝑘 is a finite subset of 𝐷𝑘 , where 𝐷 is a
fixed domain. The abbreviations EDB and IDB stand for Extensional
Database and Intensional Database, and represent the base relations
and the computed relations respectively. A Datalog rule has the
form:

𝑅0 (vars) :- 𝑅1 (vars1) ∧ · · · ∧ 𝑅𝑚 (vars𝑚)
where 𝑅0 is an IDB, and 𝑅1, . . . , 𝑅𝑚 are IDBs or EDBs. The rule
is safe if every variable occurs in at least some predicate in the
body, and the rule is linear if its body contains at most one IDB. A
Datalog program consists of a set of possibly mutually recursive
rules. Usually, only a subset of the IDB predicates are returned to
the user, and we will call them the answer IDBs. The Immediate
Consequence Operator, ICO, is the mapping on the IDB predicates
that consists of one application of all the Datalog rules. The seman-
tics of a Datalog program is given by the least fixpoint of its ICO.
The naive evaluation algorithm consists of repeatedly applying the
ICO until the IDBs no longer change.

In this paper we will combine multiple rules with the same head
into a single rule by OR-ing their bodies, and writing explicitly all
existential quantifiers. This is a common convention used in the
literature, see e.g., [13]. For example the following datalog program,
which computes the transitive closure of a relation 𝐸,

𝑇𝐶 (𝑥, 𝑦) :- 𝐸 (𝑥, 𝑦)
𝑇𝐶 (𝑥, 𝑦) :- 𝐸 (𝑥, 𝑧) ∧ 𝑇𝐶 (𝑧, 𝑦)

becomes 𝑇𝐶 (𝑥, 𝑦) :- 𝐸 (𝑥, 𝑦) ∨ ∃𝑧 (𝐸 (𝑥, 𝑧) ∧ 𝑇𝐶 (𝑧, 𝑦)).

(Pre-)Semirings A pre-semiring is a tuple 𝑺 = (𝑆, ⊕, ⊗, ¯0, ¯1)
where ⊕ is commutative, both ⊕, ⊗ are associative, have identi-
ties ¯0 and ¯1 respectively, and ⊗ distributes over ⊕. When ⊗ is
commutative, then we call 𝑺 a commutative pre-semiring. All pre-
semirings in this paper are commutative, and we will simply refer
to them as pre-semirings. When the equality 𝑥 ⊗ ¯0 = ¯0 holds

for all 𝑥, then it is called a semiring. An ordered pre-semiring is a
pre-semiring with a partial order ⪯, where both ⊕, ⊗ are mono-
tone operations. When the partial order is defined by 𝑥 ⪯ 𝑦 iff
∃𝑧, 𝑥 ⊕ 𝑧 = 𝑦 then it is called the natural order. Examples of or-
dered (pre-)semirings are the Booleans B = ({0, 1}, ∨, ∧, 0, 1), the
closed natural numbers N∞ = (N ∪ {∞}, +, ∗, 0, 1), the tropical
semiring Trop = (N ∪ {∞}, min, +, ∞, 0), the reversed tropical
semiring Trop𝑟 = (N, max, +, 0, 0), the lifted naturals and lifted
reals N⊥ = (N ∪ {⊥}, +, ∗, 0, 1), R⊥ = (R ∪ {⊥}, +, ∗, 0, 1), where
⊥+𝑥 = ⊥∗𝑥 = ⊥. The structures B, N∞, Trop are semirings, the oth-
ers are pre-semirings. B, N∞, Trop, and Trop𝑟 are naturally ordered.
Confusingly (!!), the order relation on Trop is the reverse one: ∞ is
the smallest, and 0 is the largest element. The order relation in N⊥
and R⊥ is given by ⊥ ⪯ 𝑥 for all 𝑥: they are ordered pre-semirings
but not naturally ordered.3

𝑺-relations An 𝑺-relation 𝑅 is a function that associates to each
tuple 𝑡 ∈ 𝐷𝑘 a value in the semiring, 𝑅 [𝑡] ∈ 𝑺. In this context, 𝑺 is
called the value space of the relation 𝑅, while the domain 𝐷 of its
attributes is called the key space. 𝑺-relations were first introduced4
by Green et al. [18] in order to model data provenance. A B-relation
is a set, an N∞-relation is a bag (with possibly infinite multiplicities),
an R⊥-relation is a tensor (with possibly undefined entries).

Queries Consider a relational schema 𝑅1, 𝑅2, . . . over a pre-
semiring 𝑺. A positive (relational algebra) query is a relational
algebra expression using selections, projections, joins, and unions
(no difference operator in the positive fragment). The most com-
mon definition of the relational algebra restricts the predicates used
in selections to equality predicates, 𝑥 = 𝑦. In this paper we fol-
low [18] and allow arbitrary predicates 𝑝 (𝑥, 𝑦, . . .) over the value
space, including disequality 𝑥 ≠ 𝑦, inequality 𝑥 < 𝑦, or any other
interpreted predicate. Green [18] showed that positive relational
algebra extends naturally to an arbitrary semiring 𝑺. When 𝑺 is
the Boolean semiring, then this coincides with the set semantics of
relational algebra, and when 𝑺 is the semiring of natural numbers,
then it coincides with bag semantics.

Normal Forms Alternatively, a query can be described using

rules, as follows. A sum-product query is an expression

𝑇 (𝑥1, . . . , 𝑥𝑘 ) :- (cid:202)

𝐴1 ⊗ · · · ⊗ 𝐴𝑚

(1)

𝑥𝑘+1,...,𝑥𝑝 ∈𝐷

, . . . , 𝑥𝑡𝑘𝑖 ),
where each 𝐴𝑢 is a relational atom of the form 𝑅𝑖 (𝑥𝑡1𝑖
or some interpreted predicate such as 𝑥𝑖 > 5𝑥 𝑗 + 3. The variables
𝑥1, . . . , 𝑥𝑘 are called free variables, or head variables, and the others
are called bound variables. A sum-sum-product query has the form:
(2)
𝑄 (𝑥1, . . . , 𝑥𝑘 ) :- 𝑇1 (𝑥1, . . . , 𝑥𝑘 ) ⊕ · · · ⊕ 𝑇𝑞 (𝑥1, . . . , 𝑥𝑘 )
where 𝑇1,𝑇2, . . . ,𝑇𝑞 are sum-product expressions with the same
head variables 𝑥1, . . . , 𝑥𝑘 . When the semiring is B, N∞ and the
interpreted predicates are restricted to equality predicates, then
these queries are (Unions of) Conjunctive Queries (UCQs) under
set semantics, or under bag semantics; when the semiring is R⊥,
then the sum-products are tensor expressions, sometimes called
Einsum expressions [35]. Every positive relational algebra query 𝑄

3Note that we define Trop and Trop𝑟 over the natural numbers rather than the reals.
The motivation for this slight deviation from the standard definition of these semirings
will become clear in Section 5: the support of integer theories by the SMT-solver z3.
4Under the name 𝐾 -relations.

can be converted into a sum-sum-product expression, which we
call the normal form of 𝑄.

Datalogo Let 𝑺 be an ordered pre-semiring. A Datalog◦ program
consists of a set of (possibly recursive) sum-sum-product rules (2)
over 𝑺-relations. We allow two extensions to the expressions (1)
and (2): the summation in (1) may be restricted by some Boolean
predicate, and we also allow an atom 𝐴 in (1) to be an interpreted
function. One important interpreted function is the cast operator
[−]¯1
: B → 𝑺, which maps 0 to ¯0 and 1 to ¯1 and therefore, for any
¯0
predicate 𝑃, [𝑃]¯1
is an atom in the pre-semiring 𝑺. For example,
¯0
[𝑥 < 𝑦]¯1
is ¯0 ∈ 𝑺 when 𝑥 ≥ 𝑦 and ¯1 ∈ 𝑺 when 𝑥 < 𝑦; when
¯0
¯0, ¯1 are clear from the context, we drop them and write simply
[𝑥 < 𝑦]. We treat interpreted functions in a similar way to negation
in standard Datalog, and require a program to be stratified, such
that the interpreted functions are applied only to EDBs or to IDBs
defined in earlier strata. This implies that the ICO of that stratum
is a monotone function in the IDBs defined by that stratum, and
its semantics is defined as its least fixpoint. Abo Khamis et al. [23]
proved that any Datalog◦ program over the semirings discussed
in this section (except for N∞ and Trop𝑟 ) converges in polynomial
time in the size of the input database.

Example 2.1. Consider the body of the rule in Fig. 1(b). The re-
lations 𝐿, 𝐶𝐶 are over the tropical semiring, while 𝐸 is over the
Boolean semiring. Formally, its body is a sum-sum-product expres-
sion, with a Boolean predicate:
(cid:202)

𝐿[𝑥] ⊕

{𝐶𝐶 [𝑦] | 𝐸 (𝑥, 𝑦)}

𝑦

Here the summation (cid:201)
the predicate 𝐸 (𝑥, 𝑦). Equivalently, we can rephrase it as:
(cid:16)
𝐶𝐶 [𝑦] ⊗ [𝐸 (𝑥, 𝑦)]0
∞

𝑦 is restricted to those values 𝑦 that satisfy

𝐿[𝑥] ⊕

(cid:202)

(cid:17)

𝑦

∞ is the cast operator from B to Trop; it maps 0, 1 to ∞, 0
where [−]0
respectively. Alternatively, suppose that we represent a label 𝑣 =
𝐿[𝑥] using a standard, Boolean-valued relation 𝐿(𝑥, 𝑣), where 𝑥 is a
key, and 𝑣 is the numerical value (label). Then, instead of the atom
𝐿[𝑥] we would write (cid:201)
(cid:1).
Here 𝑣 is considered to be an atom.

𝑣 {𝑣 | 𝐿(𝑥, 𝑣)}, or (cid:201)

(cid:0)𝑣 ⊗ [𝐿(𝑥, 𝑣)]0
∞

𝑣

3 THE FGH-RULE
In this section we introduce a simple rewrite rule that allows us to
rewrite an iterative program to another, possibly more efficient pro-
gram. Then, we illustrate how this rule, when applied to Datalog◦
programs, can express several known optimizations in the literature,
as well as some new ones.

Consider an iterative program that repeatedly applies a func-
tion 𝐹 until some termination condition is satisfied, then applies a
function 𝐺 that returns the final answer 𝑌 :

𝑋 ← 𝑋0
loop 𝑋 ← 𝐹 (𝑋 ) end loop
𝑌 ← 𝐺 (𝑋 )

(3)

We call this an FG-program. The FGH-rule (pronounced FIG-rule)
provides a sufficient condition for the final answer 𝑌 to be computed

Yisu Remy Wang, Mahmoud Abo Khamis, Hung Q. Ngo, Reinhard Pichler, and Dan Suciu

by the alternative program, called the GH-program:

𝑌 ← 𝐺 (𝑋0)
loop 𝑌 ← 𝐻 (𝑌 ) end loop

(4)

Theorem 3.1 (The FGH-Rule). If the following identity holds:
𝐺 (𝐹 (𝑋 )) = 𝐻 (𝐺 (𝑋 ))

(5)

then the FG-program (3) is equivalent to the GH-program (4).

Proof. Let 𝑋0, 𝑋1, 𝑋2, . . . denote the intermediate values of the
FG-program, and 𝑌0, 𝑌1, 𝑌2, . . . those of the GH-program. By the
FGH-rule, the following diagram commutes, proving the claim:

𝑋0
𝐺

𝑌0

𝐹

𝐻

𝑋1
𝐺

𝑌1

𝐹

𝐻

𝑋2
𝐺

𝑌2

𝐹

𝐻

· · ·

· · ·

𝐹

𝐻

𝑋𝑛

𝐺

𝑌𝑛

□

In this paper we will apply the FGH-rule to optimize Datalog◦
programs. In this context, 𝐹 is the ICO of the Datalog◦ program,
𝑋 is the tuple of all its IDB predicates, and 𝑌 are the answer-IDB
predicates. We will also make the natural assumption that 𝐺 maps
the initial state 𝑋0 of the IDBs of the program (3) to the initial state
𝑌0 of (4). For example, if both programs are traditional Datalog
programs, then the initial state consists of all IDBs being the empty
set, which we denote, with some abuse, by 𝑋0 = ∅, even when
𝑋 consists of several mutually recursive IDBs. Similarly, 𝑌0 = ∅.
Typically, 𝐺 is a conjunctive query, which maps ∅ to ∅, and in that
case the theorem implies that, if Eq. (5) holds, then the following
Datalog◦ programs Π1, Π2 return the same answer 𝑌 :

Π1 :

𝑋 :- 𝐹 (𝑋 )
𝑌 :- 𝐺 (𝑋 )

Π2 :

𝑌 :- 𝐻 (𝑌 )

(6)

More generally, however, the theorem does not care about the
termination condition of the FG-programs (3). It only assumes that
the GH-program is executed the same number of iterations as the
FG-program. However, it follows immediately that, if 𝐹 reaches a
fixpoint, then so does 𝐻 :

Corollary 3.2. If the FG-program reaches a fixpoint after 𝑛 steps
(meaning: 𝑋𝑛 = 𝑋𝑛+1) then the GH-program also reaches a fixpoint
after 𝑛 steps (𝑌𝑛 = 𝑌𝑛+1). The converse fails: the GH-program may
converge much faster than the FG-program.

In summary, the optimization proceeds as follows. Given an
FG-program defined by the query expressions 𝐹 and 𝐺, find a new
query expression 𝐻 such that the identity 𝐺 ◦ 𝐹 = 𝐻 ◦𝐺 holds, then
replace the FG-program with the GH-program. We will describe
this process in detail in Sec. 4. In the remainder of this section we
present several examples showing that the FGH-rule can express
several known optimizations, like magic set rewriting, and new
optimizations, like semantic optimizations using global constraints.

3.1 Simple Examples

Example 3.3 (Connected Components). Consider the computation
of the connected components of a graph, which is a well-known
target of query optimization in the literature, see e.g., [56]. The

Optimizing Recursive Queries with Program Synthesis

𝐶𝐶1 [𝑥 ]

def
= min
𝑦

= min
𝑦

{𝐿 [𝑦 ] | 𝑇𝐶′ (𝑥, 𝑦) }

{𝐿 [𝑦 ] | [𝑥 = 𝑦 ] ∨ ∃𝑧 (𝐸 (𝑥, 𝑧) ∧ 𝑇𝐶 (𝑧, 𝑦)) }

= min(𝐿 [𝑥 ], min

𝑦

= min(𝐿 [𝑥 ], min
𝑦,𝑧

{𝐿 [𝑦 ] | ∃𝑧 (𝐸 (𝑥, 𝑧) ∧ 𝑇𝐶 (𝑧, 𝑦)) })

{𝐿 [𝑦 ] | 𝐸 (𝑥, 𝑧) ∧ 𝑇𝐶 (𝑧, 𝑦) })

𝐶𝐶2 [𝑥 ]

def
= min(𝐿 [𝑥 ], min

𝑦

{𝐶𝐶 [𝑦 ] | 𝐸 (𝑥, 𝑦) })

= min(𝐿 [𝑥 ], min

𝑦

{min
𝑦′

{𝐿 [𝑦′ ] | 𝑇𝐶 (𝑦, 𝑦′) } | 𝐸 (𝑥, 𝑦) })

= min(𝐿 [𝑥 ], min
𝑦′,𝑦

{𝐿 [𝑦′ ] | 𝐸 (𝑥, 𝑦) ∧ 𝑇𝐶 (𝑦, 𝑦′) })

Figure 2: Computing 𝐶𝐶1 and 𝐶𝐶2 from Example 3.3.

program is given in Fig. 1 (a), and its optimized version in Fig. 1 (b).
The three transformations 𝐹, 𝐺, 𝐻 are as follows:

𝐹 (𝑇𝐶)

𝐺 (𝑇𝐶)

def
=𝑇𝐶′
def
= 𝐶𝐶

where

𝑇𝐶′ (𝑥, 𝑦)

where

𝐶𝐶 [𝑥 ]

def
= [𝑥 = 𝑦 ] ∨ ∃𝑧 (𝐸 (𝑥, 𝑧) ∧ 𝑇𝐶 (𝑧, 𝑦))
def
= min
𝑦

{𝐿 [𝑦 ] | 𝑇𝐶 (𝑥, 𝑦) }

𝐻 (𝐶𝐶)

def
= 𝐶𝐶′

where

𝐶𝐶′ [𝑥 ]

def
= min(𝐿 [𝑥 ], min

𝑦

{𝐶𝐶 [𝑦 ] | 𝐸 (𝑥, 𝑦) })

def
= 𝐺 (𝐹 (𝑇𝐶)) = 𝐺 (𝑇𝐶 ′),
To check the FGH-rule, we compute 𝐶𝐶1
def
then compute 𝐶𝐶2
= 𝐻 (𝐺 (𝑇𝐶)) = 𝐻 (𝐶𝐶), both shown in Fig. 2,
and observe that it becomes identical to 𝐶𝐶1 after renaming the
variables 𝑦 ′, 𝑦 to 𝑦, 𝑧 respectively.

Example 3.4 (PreM Property). Zaniolo et al. [55] define the Pre-
mappability rule (PreM), and prove that, under this rule, one Data-
log program with ICO 𝐹 is equivalent to another program with a
simpler ICO. The PreM property is a restricted form of the FGH-rule,
more precisely it asserts that the identity 𝐺 (𝐹 (𝑋 )) = 𝐺 (𝐹 (𝐺 (𝑋 )))
holds. In this case one can simply define 𝐻 as 𝐻 (𝑋 ) = 𝐺 (𝐹 (𝑋 )),
and the FGH-rule holds. The PreM rule is more restricted than the
FGH-rule, in two ways. First, the types of the IDBs of the F-program
and the H-program must be the same. Second, the new query 𝐻
def
= 𝐺 ◦ 𝐹 . While this simplifies the
is uniquely defined, namely 𝐻
optimizer significantly, it also limits the type of optimizations that
are possible under PreM.

Example 3.5 (Simple Magic). The simplest application of magic
set optimization [5, 29, 30] converts transitive closure to reachability.
More precisely, it rewrites this program:

Π1 :

𝑇𝐶 (𝑥, 𝑦) :- [𝑥 = 𝑦] ∨ ∃𝑧 (𝑇𝐶 (𝑥, 𝑧) ∧ 𝐸 (𝑧, 𝑦))

𝑄 (𝑦) :- 𝑇𝐶 (𝑎, 𝑦)

where 𝑎 is some constant, into this program:

Π2 :

𝑄 (𝑦) :- [𝑦 = 𝑎] ∨ ∃𝑧 (𝑄 (𝑧) ∧ 𝐸 (𝑧, 𝑦))

(7)

(8)

This is a powerful optimization, because it reduces the run time
from 𝑂 (𝑛2) to 𝑂 (𝑛). Several Datalog systems support some form
of magic set optimizations. We check that (7) is equivalent to (8) by
verifying the FGH-rule. The functions 𝐹, 𝐺, 𝐻 are shown in Fig. 3.
One can verify that 𝐺 (𝐹 (𝑇𝐶)) = 𝐻 (𝐺 (𝑇𝐶)), for any relation 𝑇𝐶.
Indeed, after converting both expressions to normal form, we obtain
𝐺 (𝐹 (𝑇𝐶)) = 𝐻 (𝐺 (𝑇𝐶)) = 𝑃, where:

𝑃 (𝑦)

def
= [𝑦 = 𝑎] ∨ ∃𝑧 (𝑇𝐶 (𝑎, 𝑧) ∧ 𝐸 (𝑧, 𝑦))

𝐹 (𝑇𝐶)

𝐺 (𝑇𝐶)

𝐻 (𝑄)

def
=𝑇𝐶′ where 𝑇𝐶′ (𝑥, 𝑦)
def
= 𝑄 where
def
= 𝑄′ where

𝑄′ (𝑦)

𝑄 (𝑦)

def
= [𝑥 = 𝑦 ] ∨ ∃𝑧 (𝑇𝐶 (𝑥, 𝑧) ∧ 𝐸 (𝑧, 𝑦))
def
=𝑇𝐶 (𝑎, 𝑦)
def
= [𝑦 = 𝑎] ∨ ∃𝑧 (𝑄 (𝑧) ∧ 𝐸 (𝑧, 𝑦))

Figure 3: Expressions 𝐹, 𝐺, 𝐻 in Example 3.5.

We prove in the full version of this paper that, given a sideways
information passing strategy (SIPS) [6] every magic set optimiza-
tion [4] over a Datalog program can be proven correct using a
sequence of applications of the FGH-rule.

Example 3.6 (Generalized Semi-Naive Evaluation). The naïve eval-
uation algorithm for (positive) Datalog re-discovers each fact from
step 𝑡 again at steps 𝑡 + 1, 𝑡 + 2, . . . The semi-naive algorithm aims at
avoiding this, by computing only the new facts. We generalize the
semi-naive evaluation from the Boolean semiring to any ordered
pre-semiring 𝑺, and prove its correctness using the FGH-rule. We re-
quire 𝑺 to be a complete distributive lattice and ⊕ to be idempotent,
def
= (cid:211){𝑐 | 𝑏 ⪯ 𝑎 ⊕ 𝑐},
and define the “minus” operation as: 𝑏 ⊖ 𝑎
then prove using the FGH-rule that the following two programs
are equivalent:

Π1 :

𝑋0 := ∅;
loop 𝑋𝑡 := 𝐹 (𝑋𝑡 −1);

Π2 :

𝑌0 := ∅; Δ0 := 𝐹 (∅) ⊖ ∅;
loop

𝑌𝑡 := 𝑌𝑡 −1 ⊕ Δ𝑡 −1;
Δ𝑡 := 𝐹 (𝑌𝑡 ) ⊖ 𝑌𝑡 ;

//= 𝐹 (∅)

def
= (𝑋, 𝐹 (𝑋 ) ⊖ 𝑋 ),
To prove their equivalence, we define 𝐺 (𝑋 )
def
𝐻 (𝑋, Δ)
= (𝑋 ⊕ Δ, 𝐹 (𝑋 ⊕ Δ) ⊖ (𝑋 ⊕ Δ)), and then we prove that
𝐺 (𝐹 (𝑋 )) = 𝐻 (𝐺 (𝑋 )) by exploiting the fact that 𝑺 is a complete
distributive lattice. In practice, we compute the difference Δ𝑡 =
𝐹 (𝑌𝑡 ) ⊖𝑌𝑡 = 𝐹 (𝑌𝑡 −1 ⊕ Δ𝑡 −1) ⊖ 𝐹 (𝑌𝑡 −1) using an efficient differential
rule that computes 𝛿𝐹 (𝑌𝑡 −1, Δ𝑡 −1) = 𝐹 (𝑌𝑡 −1 ⊕ Δ𝑡 −1) ⊖ 𝐹 (𝑌𝑡 −1),
where 𝛿𝐹 is an incremental update query for 𝐹 , i.e., it satisfies the
identity 𝐹 (𝑌 ) ⊕ 𝛿𝐹 (𝑌, Δ) = 𝐹 (𝑌 ⊕ Δ).

Thus, semi-naive query evaluation generalizes from standard
Datalog over the Booleans to Datalog◦ over any complete distribu-
tive lattice with idempotent ⊕, and, moreover, is a special case of the
FGH-rule. However, the semi-naive program (more precisely, func-
tion 𝐻 ) is no longer monotone, while our synthesizer (described
in Sec. 6) is currently restricted to infer only monotone functions
𝐻 . For that reason we do not synthesize the semi-naive algorithm;
instead we apply it using pattern-matching as the last optimization
step.

3.2 Loop Invariants
More advanced uses of the FGH-rule require a loop-invariant, 𝜙 (𝑋 ).
By refining Theorem 3.1 with a loop invariant we obtain the fol-
lowing corollary:

Corollary 3.7. Let 𝜙 (𝑋 ) be any predicate satisfying the following

three conditions:

𝜙 (𝑋0)
𝜙 (𝑋 ) ⇒ 𝜙 (𝐹 (𝑋 ))
𝜙 (𝑋 ) ⇒ (𝐺 (𝐹 (𝑋 )) = 𝐻 (𝐺 (𝑋 )))

(9)

(10)

(11)

then the FG-program (3) is equivalent to the GH-program (4).

To prove the corollary, we consider the restriction of the function
𝐹 to values 𝑋 that satisfy 𝜙. Conditions (9) and (10) state that 𝜙 is
a loop invariant for the FG-program (3), while condition (11) is the
FGH-rule applied to the restriction of 𝐹 to 𝜙.

Example 3.8 (Beyond Magic). By using loop-invariants, we can
perform optimizations that are more powerful than standard magic
set rewritings. For a simple illustration, consider the following
program:

Π1 :

𝑇𝐶 (𝑥, 𝑦) :- [𝑥 = 𝑦] ∨ ∃𝑧 (𝐸 (𝑥, 𝑧) ∧ 𝑇𝐶 (𝑧, 𝑦))

(12)

𝑄 (𝑦) :- 𝑇𝐶 (𝑎, 𝑦)

which we want to optimize to:

Π2 :

𝑄 (𝑦) :- [𝑦 = 𝑎] ∨ ∃𝑧 (𝑄 (𝑧) ∧ 𝐸 (𝑧, 𝑦))

(13)

Unlike the simple magic program in Example 3.5, here rule (12) is
right-recursive. As shown in [6], the magic set optimization using
the standard sideways information passing optimization [1] yields
a program that is more complicated than our program (13). Indeed,
consider a graph that is simply a directed path 𝑎0 → 𝑎1 → · · · →
𝑎𝑛 with 𝑎 = 𝑎0. Then, even with magic set optimization, the right-
recursive rule (12) needs to derive quadratically many facts of the
form 𝑇 (𝑎𝑖, 𝑎 𝑗 ) for 𝑖 ≤ 𝑗, whereas the optimized program (13) can
be evaluated in linear time. Note also that the FGH-rule cannot be
applied directly to prove that the program (12) is equivalent to (13).
To see this, denote by 𝑃1
observe that 𝑃1, 𝑃2 are defined as:

def
= 𝐺 (𝐹 (𝑇𝐶)) and 𝑃2

def
= 𝐻 (𝐺 (𝑇𝐶)), and

𝑃1 (𝑦)

𝑃2 (𝑦)

def
= [𝑦 = 𝑎] ∨ ∃𝑧 (𝐸 (𝑎, 𝑧) ∧ 𝑇𝐶 (𝑧, 𝑦))
def
= [𝑦 = 𝑎] ∨ ∃𝑧 (𝑇𝐶 (𝑎, 𝑧) ∧ 𝐸 (𝑧, 𝑦))

In general, 𝑃1 ≠ 𝑃2. The problem is that the FGH-rule requires that
𝐺 (𝐹 (𝑇𝐶)) = 𝐻 (𝐺 (𝑇𝐶)) for every input 𝑇𝐶, not just the transitive
closure of 𝐸. However, the FGH-rule does hold if we restrict 𝑇𝐶 to
relations that satisfy the following loop-invariant 𝜙 (𝑇𝐶):
∃𝑧1 (𝐸 (𝑥, 𝑧1) ∧ 𝑇𝐶 (𝑧1, 𝑦)) ⇔ ∃𝑧2 (𝑇𝐶 (𝑥, 𝑧2) ∧ 𝐸 (𝑧2, 𝑦))

(14)

If 𝑇𝐶 satisfies this predicate, then it follows immediately that 𝑃1 =
𝑃2, allowing us to optimize the program (12) to (13). It remains
to prove that 𝜙 is indeed an invariant for the function 𝐹 . The
base case (9) holds because both sides of (14) are empty when
𝑇𝐶 = ∅. It remains to check 𝜙 (𝑇𝐶) ⇒ 𝜙 (𝐹 (𝑇𝐶)). Let us denote
𝑇𝐶 ′ def
= 𝐹 (𝑇𝐶), then we need to check that, if (14) holds, then the
def
= ∃𝑧1 (𝐸 (𝑥, 𝑧1) ∧ 𝑇𝐶 ′(𝑧1, 𝑦)) is equivalent to
predicate Ψ1 (𝑥, 𝑦)
def
= ∃𝑧2 (𝑇𝐶 ′(𝑥, 𝑧2) ∧ 𝐸 (𝑧2, 𝑦)). We expand
the predicate Ψ2 (𝑥, 𝑦)
both predicates in Fig. 4, where we renamed 𝑧 to 𝑧2 in the last line
of Ψ1, and renamed 𝑧 to 𝑧1 in Ψ2. Their equivalence follows from
the assumption (14).

Yisu Remy Wang, Mahmoud Abo Khamis, Hung Q. Ngo, Reinhard Pichler, and Dan Suciu

Ψ1 (𝑥, 𝑦) ≡∃𝑧1 (𝐸 (𝑥, 𝑧1) ∧ ( [𝑧1 = 𝑦 ] ∨ ∃𝑧 (𝐸 (𝑧1, 𝑧) ∧ 𝑇𝐶 (𝑧, 𝑦))))

≡∃𝑧1 (𝐸 (𝑥, 𝑧1) ∧ [𝑧1 = 𝑦 ] ∨ 𝐸 (𝑥, 𝑧1) ∧ ∃𝑧 (𝐸 (𝑧1, 𝑧) ∧ 𝑇𝐶 (𝑧, 𝑦)))
≡𝐸 (𝑥, 𝑦) ∨ ∃𝑧1 (𝐸 (𝑥, 𝑧1) ∧ ∃𝑧 (𝐸 (𝑧1, 𝑧) ∧ 𝑇𝐶 (𝑧, 𝑦)))
≡𝐸 (𝑥, 𝑦) ∨ ∃𝑧1 (𝐸 (𝑥, 𝑧1) ∧ ∃𝑧2 (𝐸 (𝑧1, 𝑧2) ∧ 𝑇𝐶 (𝑧2, 𝑦)))
Ψ2 (𝑥, 𝑦) ≡∃𝑧2 ( ( [𝑥 = 𝑧2 ] ∨ ∃𝑧 (𝐸 (𝑥, 𝑧) ∧ 𝑇𝐶 (𝑧, 𝑧2))) ∧ 𝐸 (𝑧2, 𝑦))
≡𝐸 (𝑥, 𝑦) ∨ ∃𝑧, 𝑧2 (𝐸 (𝑥, 𝑧) ∧ 𝑇𝐶 (𝑧, 𝑧2) ∧ 𝐸 (𝑧2, 𝑦))
≡𝐸 (𝑥, 𝑦) ∨ ∃𝑧 (𝐸 (𝑥, 𝑧) ∧ ∃𝑧2 (𝑇𝐶 (𝑧, 𝑧2) ∧ 𝐸 (𝑧2, 𝑦)))
≡𝐸 (𝑥, 𝑦) ∨ ∃𝑧1 (𝐸 (𝑥, 𝑧1) ∧ ∃𝑧2 (𝑇𝐶 (𝑧1, 𝑧2) ∧ 𝐸 (𝑧2, 𝑦)))

Figure 4: Predicates Ψ1 and Ψ2 from Example 3.8.

3.3 Semantic Optimization Under Constraints
Semantic optimization refers to optimization rules that hold when
the database satisfies certain constraints [33]. For example, most
database systems today can optimize key/foreign-key joins by sim-
ply removing the join when the table containing the key is not used
anywhere else in the query.

A priori knowledge on the structure of the underlying data may
often provide additional potential for optimization. For instance,
in [5], the counting and reverse counting methods are presented
to further optimize the same-generation program if it is known
that the underlying graph is acyclic. We present a principled way
of exploiting such a priori knowledge. As we show here, recursive
queries have the potential to use global constraints on the data
during semantic optimization; for example, the query optimizer may
exploit the fact that the graph is a tree, or the graph is connected.
Let Γ denote a set of constraints on the EDBs. Then, the FGH-
rule (5) needs to be be checked only for EDBs that satisfy Γ. We
illustrate this with an example:

Example 3.9 (Semantic Optimization). Consider a hierarchy of
subparts consisting of two relations: SubPart(𝑥, 𝑦) indicates that
𝑦 is a subpart of 𝑥, and Cost[𝑥] ∈ N represents the cost of the
part 𝑥. We want to compute, for each 𝑥, the total cost 𝑄 [𝑥] of all
its subparts, sub-subparts, etc. Since the hierarchy can, in general,
be a DAG, we first need to compute the transitive closure, before
summing up the costs of all subparts, sub-subparts, etc:

Π1 :

𝑆 (𝑥, 𝑦) :- [𝑥 = 𝑦] ∨ ∃𝑧 (𝑆 (𝑥, 𝑧) ∧ SubPart(𝑧, 𝑦))
𝑄 [𝑥] :- ∑︁
𝑦

{Cost[𝑦] | 𝑆 (𝑥, 𝑦)}

(15)

The first rule, defining the 𝑆 predicate, is over the B semiring, while
the second rule, defining 𝑄, is over the N⊥ semiring. Consider now
the case when our subpart hierarchy is a tree. Then, we can compute
the total cost much more efficiently, using the following program:

Π2 :

𝑄 [𝑥] :- Cost[𝑥] +

∑︁

𝑧

{𝑄 [𝑧] | SubPart(𝑥, 𝑧)}

(16)

Optimizing the program (15) to (16) is an instance of semantic
optimization, since this only holds if the database instance is a tree.
We do this in three steps. We define the constraint Γ stating that the
data is a tree; using Γ we infer a loop-invariant Φ of the program
Π1; using Γ and Φ we prove the FGH-rule, concluding that Π1 is
equivalent to Π2.

Optimizing Recursive Queries with Program Synthesis

𝑃1 [𝑥 ] =

∑︁

𝑦

{Cost[𝑦 ] | [𝑥 = 𝑦 ] ∨ ∃𝑧 (𝑆 (𝑥, 𝑧) ∧ SubPart(𝑧, 𝑦)) }

=Cost[𝑥 ] +

∑︁

𝑦

{Cost[𝑦 ] | ∃𝑧 (𝑆 (𝑥, 𝑧) ∧ SubPart(𝑧, 𝑦)) }

∑︁

−

𝑦

{Cost[𝑦 ] | [𝑥 = 𝑦 ] ∧ ∃𝑧 (𝑆 (𝑥, 𝑧) ∧ SubPart(𝑧, 𝑦)) }

=Cost[𝑥 ] +

=Cost[𝑥 ] +

∑︁

𝑦
∑︁

{Cost[𝑦 ] | ∃𝑧 (𝑆 (𝑥, 𝑧) ∧ SubPart(𝑧, 𝑦)) }

∑︁

{Cost[𝑦 ] | (𝑆 (𝑥, 𝑧) ∧ SubPart(𝑧, 𝑦)) }

𝑦

𝑧

Figure 5: Transformation of 𝑃1

def= 𝐺 (𝐹 (𝑆)) in Example 3.9.

The constraint Γ is the conjunction of the following statements:

∀𝑥1, 𝑥2, 𝑦 (SubPart(𝑥1, 𝑦) ∧ SubPart(𝑥2, 𝑦) ⇒ 𝑥1 = 𝑥2)
∀𝑥, 𝑦 (SubPart(𝑥, 𝑦) ⇒ 𝑇 (𝑥, 𝑦))
∀𝑥, 𝑦, 𝑧 (𝑇 (𝑥, 𝑧) ∧ SubPart(𝑧, 𝑦) ⇒ 𝑇 (𝑥, 𝑦))
∀𝑥, 𝑦 (𝑇 (𝑥, 𝑦) ⇒ 𝑥 ≠ 𝑦)

(17)

(18)

(19)

(20)

The first asserts that 𝑦 is a key in SubPart(𝑥, 𝑦). The last three
are an Existential Second Order Logic (ESO) statement: they assert
that there exists some relation 𝑇 (𝑥, 𝑦) that contains SubPart, is
transitively closed, and irreflexive. Next, we infer the following
loop-invariant of the program Π1:

(21)

def
= 𝐺 (𝐹 (𝑆)) and 𝑃2

Φ : 𝑆 (𝑥, 𝑦) ⇒ [𝑥 = 𝑦] ∨ 𝑇 (𝑥, 𝑦)
Finally, we check the FGH-rule, under the assumptions Γ, Φ. Denote
def
by 𝑃1
= 𝐻 (𝐺 (𝑆)). To prove 𝑃1 = 𝑃2 we
simplify 𝑃1 using the assumptions Γ, Φ, as shown in Fig. 5. We
explain each step. Line 2-3 are inclusion/exclusion. Line 4 uses
the fact that the term on line 3 is = 0, because the loop invariant
implies:
𝑆 (𝑥, 𝑧) ∧ SubPart(𝑧, 𝑦) ⇒ ( [𝑥 = 𝑧 ] ∨ 𝑇 (𝑥, 𝑧)) ∧ SubPart(𝑧, 𝑦)

by (21)

≡ SubPart(𝑥, 𝑦) ∨ (𝑇 (𝑥, 𝑧) ∧ SubPart(𝑧, 𝑦))
⇒ 𝑇 (𝑥, 𝑦) ∨ 𝑇 (𝑥, 𝑦)
≡ 𝑇 (𝑥, 𝑦)
⇒ 𝑥 ≠ 𝑦

by (19)

by (20)
Line 5 follows from the fact that 𝑦 is a key in SubPart(𝑧, 𝑦). A
direct calculation of 𝑃2 = 𝐻 (𝐺 (𝑆)) results in the same expression
as line 5 of Fig. 5, proving that 𝑃1 = 𝑃2.

4 ARCHITECTURE OF FGH-OPTIMIZATION
In the rest of the paper we describe our synthesis-based FGH-
optimizer, whose architecture is shown in Fig. 6. We optimize
one stratum at a time. We denote by Π1 one stratum of the input
program, denote by 𝑋 its recursive IDBs, by 𝑌 its output IDBs,
and by 𝐹, 𝐺 the ICO and the output operator respectively; see
Eq. (6). The optimizer also takes as input a database constraint,
Γ. The optimizer starts by inferring the loop invariant Φ; this is
discussed in Sec. 7. Next, the optimizer needs to find 𝐻 such that
Γ ∧ Φ |= (𝐺 (𝐹 (𝑋 )) = 𝐻 (𝐺 (𝑋 ))). To reduce clutter we will often
abbreviate this to Γ |= (𝐺 (𝐹 (𝑋 )) = 𝐻 (𝐺 (𝑋 ))), assuming that Γ in-
corporates Φ. The optimizer makes two attempts at synthesizing 𝐻 :
it first tries using a simpler rule-based synthesizer, and, if that fails,
then it tries the state-of-the-art Counterexample-Guided Inductive

Figure 6: The architecture of the FGH-optimizer. The input
is the unoptimized program Π1, consisting of the functions
𝐹, 𝐺 and the database constraint Γ. The output consists of
the optimized program Π2, see Eq. (6). Blue boxes are de-
scribed in Section 6 and the green boxes in Section 7. The yel-
low box (generalized semi-naive optimization) is described
in Section 3.1. The red boxes represent three state-of-the-art
systems: Rosette is a CEGIS system [44, 46, 47], z3 is an SMT
solver [9], and EGG is an EQSAT system [54].

Synthesis (CEGIS). This is described in Sec. 6. Finally, 𝐻 (or the orig-
inal program if the FGH-optimization failed) is further transformed
using generalized semi-naive optimization, as we already described
in Sec. 3.1. Notice that stratification ensures that no interpreted
functions are applied to the IDBs 𝑋 ; they can still be applied to the
EDBs, or occur in predicates.

def
= 𝐺 (𝐹 (𝑋 )) and 𝑉

The FGH-optimization is an instance of query rewriting using
def
views [16, 21]. Denoting by 𝑄
= 𝐺 (𝑋 ), one
has to rewrite the query 𝑄 using the view(s) 𝑉 , in other words
𝑄 = 𝐻 (𝑉 ). This is a total rewriting, in the sense that 𝐻 is no longer
allowed to refer to the IDBs 𝑋 . This problem is NP-complete for
UCQs with set semantics [26], in NP for UCQs with bag semantics5,
and undecidable for realistic SQL queries that include aggregates
and arithmetic [16]. Systems that support query rewriting using
views are rule-based, and apply a set of hand crafted, predefined
patterns; our first attempt to synthesize 𝐻 is also rule-based. Such
synthesizers usually cannot take advantage of database constraints,
but we will show in Sec. 7 how to exploit the constraint Γ in the
rule-based synthesizer. However, rule-based rewriting explores a
limited space, which is insufficient for many FGH-optimizations.
In a seminal paper [44] Solar-Lezama proposed an alternative to
rule-based transformation, called Counterexample-Guided Inductive
Synthesis, CEGIS: the synthesizer produces potentially incorrect
candidates, and an SMT solver verifies their correctness. In the FGH-
optimizer we use a program synthesizer, Rosette [46], to synthesize
𝐻 .

At a conceptual level, program synthesis has two abstract steps:
generate 𝐻 , and verify 𝐺 (𝐹 (𝑋 )) = 𝐻 (𝐺 (𝑋 )). While the verifier is

5This follows from the fact that, under bag semantics, two UCQ queries are equivalent
iff they are isomorphic. [17, 53].

Counterexample- based synthesisInvariant inferenceRule-based synthesisGrammar generatorFailCEGIS (Rosette)DenormalizeSuccessGeneralized Semi-Naive rewriting (GSN)FailSuccessEQSAT (EGG)SMT Solver (Z3)𝐶𝐶1 [𝑥 ] =

(cid:202)

𝑦

𝐿 [𝑦 ] ⊗ (cid:0) [𝑥 = 𝑦 ]0

∞ ⊕

(cid:202)

𝑧

[𝐸 (𝑥, 𝑧) ]0

∞ ⊗ [𝑇𝐶 (𝑧, 𝑦) ]0
∞

(cid:1)

𝐶𝐶2 [𝑥 ] = 𝐿 [𝑥 ] ⊕

(cid:202)

𝑦

(cid:0) (cid:202)
𝑦′

𝐿 [𝑦′ ] ⊗ [𝑇𝐶 (𝑦, 𝑦′) ]0
∞

(cid:1) ⊗ [𝐸 (𝑥, 𝑦) ]0
∞

Yisu Remy Wang, Mahmoud Abo Khamis, Hung Q. Ngo, Reinhard Pichler, and Dan Suciu

Example 5.1 (APSP100). Consider a labeled graph 𝐸 where 𝐸 [𝑥, 𝑦]
represents the cost of the edge 𝑥, 𝑦. The following query over Trop
computes the all-pairs shortest path up to length of 100:

𝐷 [𝑥, 𝑦 ] :- if 𝑥 = 𝑦 then 0 else min

𝑧

(𝐷 [𝑥, 𝑧 ] + 𝐸 [𝑧, 𝑦 ])

𝑄 [𝑥, 𝑦 ] :- min(𝐷 [𝑥, 𝑦 ], 100)

(27)

Figure 7: 𝐶𝐶1 and 𝐶𝐶2 in semiring notation; their normal
forms are isomorphic.

The program is inefficient because it first computes the full path
length, only to cap it later to 100. By using the FGH-rule we get:

not used explicitly, it is used implicitly in the synthesizer, and we
describe it in Sec. 5. Then we describe the synthesizer in Sec. 6.

5 VERIFICATION
We introduced the FGH-rule in Sec. 3 and showed several examples.
In order to apply the rule, one needs to check the identity (5),
𝐹 (𝐺 (𝑋 )) = 𝐺 (𝐻 (𝑋 )). In this section we describe how we verify
this identity. This step is implicit in both boxes Rule-based Synthesis
and CEGIS in Fig 6. The identity can be checked in one of two
ways: by applying a predefined set of identity rules (as currently
done by most query optimizers), or by using an SMT solver.

5.1 Rule-based Test
Let 𝑃1 = 𝐺 (𝐹 (𝑋 )), 𝑃2 = 𝐻 (𝐺 (𝑋 )). To check 𝑃1 = 𝑃2, the rule-
based test first normalizes both expressions into a sum-sum-product
expression (Eq. (2)) via the semiring axioms, then checks if the
expressions are isomorphic: if yes, then 𝑃1 = 𝑃2, otherwise we
assume 𝑃1 ≠ 𝑃2. The treatment of a constraint Γ will be discussed
in Sec. 7. This test can be visualized as follows:

axioms
−−−−−−→ normalize(𝑃1) ≃ normalize(𝑃2)

axioms
←−−−−−− 𝑃2

𝑃1

(22)

where ≃ denotes isomorphism. The Rule-based test is sound. When
both 𝑃1, 𝑃2 are over the N∞ semiring and have no interpreted func-
tions then it is also complete [17, 53]. This simple test motivates
the need for a complete set of axioms that allows any semiring
expression to be normalized. The axioms include standard semir-
ing axioms, and axioms about summations and free variables fv.
For example, in order to prove 𝐶𝐶1 = 𝐶𝐶2 in Example 3.3 (with
semiring notation in Figure 7) one needs all three axioms below:

(cid:202)

(cid:202)

𝑥

𝑦

(· · · ) =

𝐴 ⊗

(cid:202)

𝐵 =

(cid:202)

𝑥,𝑦
(cid:202)

𝑥
(𝐴(𝑥) ⊗ [𝑥 = 𝑦]) = 𝐴(𝑦)

𝑥

(cid:202)

𝑥

(· · · )

𝐴 ⊗ 𝐵 when 𝑥 ∉ fv(𝐴)

(23)

(24)

(25)

5.2 SMT Test
When the expressions 𝑃1, 𝑃2 are over a semiring other than N∞,
or they contain interpreted functions, then the rule-based test is
insufficient and we use an SMT solver for our verifier. We still
normalize the expressions using our axioms, because today’s solvers
cannot reason about bound/free variables (as needed in axioms (23)-
(25)). The SMT test is captured by the following figure:

axioms
−−−−−−→ normalize(𝑃1)

SMT
←−−→normalize(𝑃2)

axioms
←−−−−−− 𝑃2

𝑃1

(26)

𝑄 [𝑥, 𝑦 ] :- if 𝑥 = 𝑦 then 0 else min

(cid:16)

min
𝑧

(𝑄 [𝑥, 𝑧 ] + 𝐸 [𝑧, 𝑦 ]) , 100

(cid:17)

(28)

We show how to verify that (28) is equivalent to (27). Denote
def
by 𝑃1
= 𝐻 (𝐺 (𝐷)) (where 𝐹, 𝐺, 𝐻 are the
obvious functions in the two programs defining 𝑄). After we de-
sugar, convert to semiring expressions, and normalize, they become:

def
= 𝐺 (𝐹 (𝐷)) and 𝑃2

𝑃1 [𝑥, 𝑦 ] = (cid:0)0 ⊗ [𝑥 = 𝑦 ]0
∞

(cid:1) ⊕

𝑃2 [𝑥, 𝑦 ] = (cid:0)0 ⊗ [𝑥 = 𝑦 ]0
∞

(cid:1) ⊕

(cid:32)

(cid:32)

(cid:202)

𝑧

(cid:202)

𝑧

𝐷 [𝑥, 𝑧 ] ⊗ 𝐸 [𝑧, 𝑦 ]

(cid:33)

(cid:33)

⊕ 100

(cid:32)

𝐷 [𝑥, 𝑧 ] ⊗ 𝐸 [𝑧, 𝑦 ]

⊕

100 ⊗

(cid:33)

𝐸 [𝑧, 𝑦 ]

⊕ 100

(cid:202)

𝑧

In the normalized expressions we push the summations past the
joins, i.e., we apply rule (24) from right to left, thus we write 100 ⊗
(cid:201)(· · · ) instead of (cid:201)(100 ⊗ · · · ): we give the rationale below. At
this point, the normalized 𝑃1 and 𝑃2 are not isomorphic, yet they
are equivalent if they are interpreted in Trop. We explain below in
detail how the solver can check that. In this particular semiring, the
identity 100 = (cid:0)100 ⊗ (cid:201)
𝑧 𝐸 [𝑧, 𝑦](cid:1) ⊕ 100 holds since it becomes
100 = min(100 + min𝑧 𝐸 [𝑧, 𝑦], 100) with 𝐸 [𝑧, 𝑦] ≥ 0, once we
replace the uninterpreted operators ⊕, ⊗ with min, +.

Implementation We describe how we implemented the SMT
test Γ |= 𝑃1 = 𝑃2 using a solver, now also taking the database
constraint Γ into account, where 𝑃1, 𝑃2 are the expressions 𝐺 ◦ 𝐹
and 𝐻 ◦ 𝐺. We used the z3 solver [9], but our discussion applies to
other solvers as well. We need to normalize 𝑃1, 𝑃2 before using the
solver, because solvers require all axioms to be expressed in First
Order Logic. They cannot encode the axioms (23)-(25), because they
are referring to free variables, which is a meta-logical condition
not expressible in First Order Logic. Once normalized, we encode
the equality as a first-order logic formula, and assert its negation,
asking the solver to check if Γ ∧ (𝑃1 ≠ 𝑃2) is satisfiable. The solver
returns UNSAT, a counterexample, or UNKNOWN. UNSAT means
the identity holds. When it returns a counterexample, then the
identity fails, and the counterexample is given as input to the syn-
thesizer (Sec. 6). UNKNOWN means that it could neither prove nor
disprove the equivalence and we assume 𝑃1 ≠ 𝑃2. For the theory
of reals with +, ∗, despite its decidability, z3 often timed out in our
experiments. We therefore used the theory of integers, and z3 never
timed out or returned UNKNOWN in our experiments.

We encode every 𝑺-relation 𝑅(𝑥1, . . . , 𝑥𝑛) as an uninterpreted
function 𝑅 : N × · · · × N → 𝑺, where 𝑺 is the interpreted semiring,
i.e., B, Trop, N∞, etc. We represent natural numbers as integers
with nonnegativity assertions, and represent the sets N∞, N⊥, R⊥
as union types. Operators supported by the solver, like +, ∗, min, −,
are entered unchanged; we treat other operators as uninterpreted
functions. Unbounded aggregation, like (cid:201)
𝑥 𝑒 (𝑥), poses a challenge:
there is no such operation in any SMT theory. Here we use the fact

Optimizing Recursive Queries with Program Synthesis

that 𝑃1 and 𝑃2 are normalized sum-sum-product expressions:

(cid:32)

(cid:202)

(cid:33)

(cid:32)

𝑒1

⊕

𝑃1 =

𝑥1

(cid:33)

𝑒2

⊕ · · ·

(cid:202)

𝑥2

𝑃2 = (cid:169)
(cid:173)
(cid:171)

𝑒 ′
1

(cid:202)

𝑥 ′
1

(cid:170)
(cid:174)
(cid:172)

⊕ (cid:169)
(cid:173)
(cid:171)

𝑒 ′
2

(cid:202)

𝑥 ′
2

(cid:170)
(cid:174)
(cid:172)

⊕ · · ·

Assume first that each 𝑥𝑖 is a single variable. We ensure that all the
variables 𝑥1, 𝑥2, . . . in 𝑃1 are distinct, by renaming them if necessary.
Next, we replace each expression (cid:201)
𝑒𝑖 with 𝑢 (𝑥𝑖, 𝑒𝑖 ) where 𝑢 is
an uninterpreted function. Finally, we ask the solver to check

𝑥𝑖

Γ |= (cid:0)𝑢 (𝑥1, 𝑒1) ⊕ 𝑢 (𝑥2, 𝑒2) ⊕ · · · = 𝑢 (𝑥 ′
1

, 𝑒 ′

1) ⊕ 𝑢 (𝑥 ′

2

2) ⊕ · · · (cid:1)
, 𝑒 ′

𝑥 𝑒 = (cid:201)

This procedure is sound, because if the identity 𝑢 (𝑥, 𝑒) = 𝑢 (𝑥 ′, 𝑒 ′)
holds, then 𝑥 = 𝑥 ′ (they are the same variable) and 𝑒 = 𝑒 ′, which
means that (cid:201)
𝑥 ′ 𝑒 ′. Moreover, when synthesizing 𝑃2,
we will ensure that the generator includes the variables 𝑥1, 𝑥2, . . .
present in 𝑃1 to achieve a limited form of completeness, see Sec. 6.
Finally, if a summation is over multiple variables, we simply nest
the uninterpreted function, i.e., write (cid:201)

𝑥,𝑦 𝑒 as 𝑢 (𝑥, 𝑢 (𝑦, 𝑒)).

Example 5.2. We now finish Example 5.1. After introducing the

uninterpreted functions described above, we obtain:

𝑃1 = min(0 + 𝑤 (𝑥, 𝑦), 𝑢 (𝑧, 𝐷 [𝑥, 𝑧] + 𝐸 [𝑧, 𝑦]), 100)
𝑃2 = min(0 + 𝑤 (𝑥, 𝑦), 𝑢 (𝑧, 𝐷 [𝑥, 𝑧] + 𝐸 [𝑧, 𝑦]), 100 + 𝑢 (𝑧, 𝐸 [𝑧, 𝑦]), 100)
where 𝑤 (𝑥, 𝑦) is an uninterpreted function representing [𝑥 = 𝑦]0
∞,
and 𝑢 is our uninterpreted function encoding summation. The solver
proves that the two expressions are equal, given that 𝑤 ≥ 0 and
𝑢 ≥ 0. Notice that it was critical to factorize the term 100: had
we not done that, then the expression 100 + 𝑢 (𝑧, 𝐸 [𝑧, 𝑦]) would be
𝑢 (𝑧, 100 + 𝐸 [𝑧, 𝑦]) and the identity 𝑃1 = 𝑃2 no longer holds.

Discussion Readers unfamiliar with First Order Logic may be
puzzled by our statement that the identity 𝑢 (𝑥, 𝑒) = 𝑢 (𝑥 ′, 𝑒 ′) holds
iff 𝑥 = 𝑥 ′ and 𝑒 = 𝑒 ′. In order to explain this, it helps to first review
the basic definitions of validity and satisfiability in logic. A state-
ment is “valid” if it is true for all interpretations of its uninterpreted
symbols. For example, the equality 𝑓 (𝑥) +𝑦 = 𝑦 + 𝑓 (𝑥) is valid over
integers, because it holds for all function 𝑓 and all values of 𝑥 and
𝑦. A statement is “satisfiable” if there exists interpretations of its
uninterpreted symbols that make the statement true. A statement
is valid iff its negation is not satisfiable. In our case, the statement
𝑢 (𝑥, 𝑒) = 𝑢 (𝑥 ′, 𝑒 ′) is valid if the equality is true for all possible inter-
pretations of 𝑢, 𝑥, 𝑥 ′. For example, suppose we asked the solver to
check whether 𝑢 (𝑥, 2(𝑥 + 1)) = 𝑢 (𝑦, 2𝑦 + 2) is valid. To answer this
question, we negate the statement and ask the z3 solver whether
the negation is satisfiable: 𝑢 (𝑥, 2(𝑥 +1)) ≠ 𝑢 (𝑦, 2𝑦 +2). One can eas-
ily satisfy this with pen and paper, e.g., 𝑥 = 1, 𝑦 = 2, 𝑢 (𝑎, 𝑏) = 𝑎 + 𝑏,
then 𝑢 (𝑥, 2(𝑥 + 1)) = 5, 𝑢 (𝑦, 2𝑦 + 2) = 8. z3 also answers “yes”, and
provides the following example for the inequality6:

𝑥 = 0, 𝑦 = 38, 𝑢 (𝑎, 𝑏) = if 𝑎 = 38 ∧ 𝑏 = 78 then 6 else 4
Therefore, the identity 𝑢 (𝑥, 2(𝑥 + 1)) = 𝑢 (𝑦, 2𝑦 + 2) is not valid.
In contrast, suppose we asked the solver whether 𝑢 (𝑥, 2(𝑥 + 1)) =
𝑢 (𝑥, 2𝑥 + 2) is valid. Its negation is 𝑢 (𝑥, 2(𝑥 + 1)) ≠ 𝑢 (𝑥, 2𝑥 + 2),
and z3 returns UNSAT, which means that the identity is valid. In
general, the identity 𝑢 (𝑥, 𝑒) = 𝑢 (𝑥 ′, 𝑒 ′) is valid iff 𝑥 = 𝑥 ′ and 𝑒 = 𝑒 ′.

6Please refer to the documentation of z3 for how models for uninterpreted functions
are constructed.

6 SYNTHESIS
We have seen in Sec. 5 how to use an SMT solver to check the
identity 𝐺 (𝐹 (𝑋 )) = 𝐻 (𝐺 (𝑋 )). We are now ready to discuss the
core of the FGH-optimizer: given only the query expressions 𝐹, 𝐺,
find an expression 𝐻 such that the identity 𝐺 (𝐹 (𝑋 )) = 𝐻 (𝐺 (𝑋 ))
holds; recall that we denote these expressions by 𝑃1, 𝑃2. As for
verification, this can be done by using only rewriting, or using
program synthesis with an SMT solver. We are also given a database
constraint Γ, and we assume that we have already added to it the
loop invariant Φ.

6.1 Rule-based Synthesis
The optimizer first attempts to synthesize 𝐻 using rule-based rewrit-
ing. This process is akin to our initial verifier that relies only on
normalization and isomorphism checking.

axioms
−−−−−−→ normalize(𝑃1)

axioms
−−−−−−→ 𝑃2

𝑃1

(29)

There is no obvious way to “denormalize” an expression, since
many expressions can share the same normal form. We used for
this purpose an equality saturation system (EQSAT), which we also
used for multiple tasks of the FGH-optimizer, see Fig 6. We describe
EQSAT in Sec. 7.

6.2 Counterexample-based Synthesis
The rule-based synthesis (29) explores only correct rewritings 𝑃2,
but its space is limited by the hand-written axioms. The alternative
approach, pioneered in the programming language community [44],
is to synthesize candidate programs 𝑃2 from a much larger space,
then using an SMT solver to verify their correctness. This technique,
called Counterexample-Guided Inductive Synthesis, or CEGIS, can
find rewritings 𝑃2 even in the presence of interpreted functions,
because it exploits the theory of the underlying domain. As a first
attempt it can be described as follows (we will revise it below):

axioms
−−−−−−→ normalize(𝑃1)

𝑃1

CEGIS
−−−−−→ 𝑃2

(30)

6.2.1 Brief Overview of CEGIS. We give a brief overview of the
CEGIS system, Rosette [46, 47], that we used in our optimizer. Un-
derstanding its working is important in order to optimize its usage
for FGH-optimization. The input to Rosette consists of a specifica-
tion and a grammar, and the goal is to synthesize a program defined
by the grammar and that satisfies the specification. The main loop is
implemented with a pair of dueling SMT-solvers, the generator and
the checker. In our setting, the inputs are the query 𝑃1, the database
constraint Γ, and a small grammar Σ (described below). The speci-
fication is Γ |= (𝑃1 = 𝑃2), where 𝑃2 is defined by the grammar Σ.
The generator generates syntactically correct programs 𝑃2, and the
verifier checks Γ |= (𝑃1 = 𝑃2). In the most naive attempt, the gen-
erator could blindly generate candidates 𝑃2, 𝑃 ′
, . . ., until one is
2
found that the verifier accepts. This is hopelessly inefficient. The
first optimization in CEGIS is that the verifier returns a small coun-
terexample database instance 𝐷 for each unsuccessful candidate
𝑃2, i.e., 𝑃1 (𝐷) ≠ 𝑃2 (𝐷). When considering a new candidate 𝑃2, the
generator checks that 𝑃1 (𝐷𝑖 ) = 𝑃2 (𝐷𝑖 ) holds for all previous coun-
terexamples 𝐷1, 𝐷2, . . ., by simply evaluating the queries 𝑃1, 𝑃2 on
the small instance 𝐷𝑖 . This significantly reduces the search space
of the generator.

, 𝑃 ′′
2

CEGIS applies a second optimization, where it uses the SMT
solver itself to generate the next candidate 𝑃2, as follows. It requires
a fixed recursion depth for the grammar Σ; in other words we can
assume w.l.o.g. that Σ is non-recursive. Then it associates a symbolic
Boolean variable 𝑏1, 𝑏2, . . . to each choice of the grammar. The
grammar Σ can be viewed now as a BDD (binary decision diagram)
where each node is labeled by a choice variable 𝑏 𝑗 , and each leaf by a
completely specified program 𝑃2. The search space of the generator
is now completely defined by the choice variables 𝑏 𝑗 , and Rosette
uses the SMT solver to generate values for these Boolean variables
such that the corresponding program 𝑃2 satisfies 𝑃1 (𝐷𝑖 ) = 𝑃2 (𝐷𝑖 ),
for all counterexample instances 𝐷𝑖 . This significantly speeds up
the choice of the next candidate 𝑃2.

6.2.2 Using Rosette. To use Rosette, we need to define the specifica-
tion and the grammar. A first attempt is to simply define some gram-
mar for 𝐻 , with the specification Γ |= (𝐺 (𝐹 (𝑋 )) = 𝐻 (𝐺 (𝑋 ))). This
does not work, since Rosette uses the SMT solver to check the iden-
tity: as explained in Sec. 5.2, modern SMT solvers have limitations
that require us to first normalize 𝐺 (𝐹 (𝑋 )) and 𝐻 (𝐺 (𝑋 )) before
checking their equivalence. Even if we modify Rosette to normalize
𝐻 (𝐺 (𝑋 )) during verification, there is still no obvious way to incor-
porate normalization into the program generator driven by the SMT
solver. Instead, we define a grammar Σ for normalize(𝐻 (𝐺 (𝑋 )))
rather than for 𝐻 , and then specify:

Γ |= normalize(𝐺 (𝐹 (𝑋 ))) = normalize(𝐻 (𝐺 (𝑋 )))

Then, we denormalize the result returned by Rosette, in order to
extract 𝐻 , using the denormalization module in Fig. 6, described in
Sec. 7. In summary, our CEGIS-approach for FGH-optimization can
be visualized as follows:

axioms
−−−−−−→ normalize(𝑃1)

𝑃1

CEGIS
−−−−−→ normalize(𝑃2)

axioms
−−−−−−→ 𝑃2

(31)

The choice of the grammar Σ is critical for the FGH-optimizer. If
it is too restricted, then the optimizer will be limited too, if it is
too general, then the optimizer will take a prohibitive amount of
time to explore the entire space. We briefly describe our design at
a high level. Recall that 𝑋 denotes multiple IDBs, and the query
𝐺 (𝑋 ) may also return multiple intermediate relations. In our system
𝐺 (𝑋 ) is restricted to return a single relation, so we will assume that
𝑌 = 𝐺 (𝑋 ) is a single IDB. The expression 𝐺 is known to us, and is
a sum-sum-product expression, see Eq. (2),

𝐺 (𝑋 ) =𝐺1 (𝑋 ) ⊕ · · · ⊕ 𝐺𝑚 (𝑋 )

where each 𝐺𝑖 (𝑋 ) is a sum-product expression, Eq. (1), using the
IDBs 𝑋 and/or the EDBs.

To generate normalize(𝐻 (𝐺 (𝑋 ))), we group its sum-products

by the number of occurrences of 𝑌 :

normalize(𝐻 (𝑌 )) =𝐻 (0) ⊕ 𝐻 (1) (𝑌 ) ⊕ · · · ⊕ 𝐻 (𝑘max) (𝑌 )

where 𝐻 (𝑘) is a sum-sum-product 𝐻 (𝑘) = 𝑄1 ⊕ 𝑄2 ⊕ · · · s.t. each
𝑄𝑖 contains exactly 𝑘 occurrences of 𝑌 , and an arbitrary number
of EDBs (it may not contain the IDBs 𝑋 ). We choose 𝑘max as the
largest number of recursive IDBs 𝑋 that occur in any rule of the
original program 𝐹 (𝑋 ), e.g., if the original program was linear, then

Yisu Remy Wang, Mahmoud Abo Khamis, Hung Q. Ngo, Reinhard Pichler, and Dan Suciu

,

𝐴 → 𝐴0 ⊕ 𝐴1 ⊕ · · · ⊕ 𝐴𝑘max
𝐴0 → 𝑄0 | 𝑄0 ⊕ 𝐴0, 𝑄0 → 𝑢 (𝑍, 𝑄0) | 𝑄0 ⊗ 𝑄0 | 𝐸 (𝑍, 𝑍, · · · , 𝑍 ),
𝐴1 →𝐴11 ⊕ · · · ⊕ 𝐴1𝑚, 𝐴2 → 𝐴211 ⊕ · · · ⊕ 𝐴2𝑚𝑚, 𝐴3 → 𝐴3111 ⊕ . . .
𝐴1𝑖 → 𝑄1𝑖 | 𝑄1𝑖 ⊕ 𝐴1𝑖,
𝐴2𝑖 𝑗 → 𝑄2𝑖 𝑗 ⊕ 𝐴2𝑖 𝑗 ,
𝐴3𝑖 𝑗 ℓ → 𝑄3𝑖 𝑗 ℓ ⊕ 𝐴3𝑖 𝑗 ℓ ,

𝑄1𝑖 → 𝑢 (𝑍, 𝑄1𝑖 ) | 𝑄1𝑖 ⊗ 𝑄0 | 𝐺𝑖 (𝑋 ),
𝑄2𝑖 𝑗 → 𝑢 (𝑍, 𝑄2𝑖 𝑗 ) | 𝑄1𝑖 ⊗ 𝐺 𝑗 (𝑋 ),
𝑄3𝑖 𝑗 ℓ → 𝑢 (𝑍, 𝑄3𝑖 𝑗 ℓ ) | 𝑄2𝑖 𝑗 ⊗ 𝐺ℓ (𝑋 ),

𝑖 =1, 𝑚
𝑖, 𝑗 =1, 𝑚
𝑖, 𝑗, ℓ =1, 𝑚

Figure 8: Grammar Σ for normalize(𝐻 (𝐺 (𝑋 ))), for 𝑘max = 3.

𝑘max

def
= 1. We obtain:
normalize(𝐻 (𝐺 (𝑋 ))) =

, 𝑧 ′
2

𝐻 (0) ⊕ normalize(𝐻 (1) (𝐺 (𝑋 ))) ⊕ · · · ⊕ normalize(𝐻 (𝑘max ) (𝐺 (𝑋 )))
The grammar Σ is shown in Fig. 8. The start symbol, 𝐴, generates
a sum matching the expression above. 𝐴0 generates 𝐻 (0) , which is
a sum of sum-product terms without any occurrence of 𝑌 . Recall
from Sec. 5.2 that the expression 𝑢 (𝑧, 𝑄) denotes (cid:201)
𝑧 𝑄. 𝐸 is one
of the EDBs, and 𝑍 is a non-terminal for which we define rules
𝑍 → 𝑧1|𝑧2| · · · |𝑧𝑚 |𝑧 ′
1|𝑧 ′
2 · · · where 𝑧1, . . . , 𝑧𝑚 are variables that
already occur in normalize(𝐺 (𝐹 (𝑋 ))), and 𝑧 ′
, . . . is some fixed
1
set of fresh variable names. 𝐴𝑘 generates normalize(𝐻 (𝑘) (𝐺 (𝑋 ))),
which is a sum of sum-products, each with exactly 𝑘 occurrence of
𝑌 . As stated in Fig. 8, the rules for 𝐴𝑘 are incorrect. For example con-
sider 𝐴1: the 𝑚 non-terminals 𝐴11, . . . , 𝐴1𝑚 should have identical
derivations, instead of being expanded independently. For example,
assume 𝐺 = 𝐺1 ⊕𝐺2 (thus 𝑚 = 2) and we want 𝐻 to be one of 𝐸1 ⊗𝑌
or 𝐸2 ⊗ 𝑌 or 𝐸3 ⊗ 𝑌 . Then, normalize(𝐻 (𝐺 (𝑋 ))) can be one of the
following three expressions 𝐸1 ⊗𝐺1 ⊕ 𝐸1 ⊗𝐺2 or 𝐸2 ⊗𝐺1 ⊕ 𝐸2 ⊗𝐺2
or 𝐸3 ⊗ 𝐺1 ⊕ 𝐸3 ⊗ 𝐺2. However, the grammar 𝐴1 → 𝐴11 ⊕ 𝐴12
also generates incorrect expressions 𝐸1 ⊗ 𝐺1 ⊕ 𝐸2 ⊗ 𝐺2, because
𝐴11, 𝐴12 can choose independently the IDB 𝐸1, 𝐸2, or 𝐸3. We fix
this by exploiting the choice variables in Rosette: we simply use the
same variables in 𝐴11, 𝐴12, . . . ensuring that all these non-terminals
make exactly the same choices. We note that our current system is
restricted to linear programs, hence 𝑘max = 1.
6.2.3 Discussion. Even though our grammar is restricted to 𝑘max =
1, it is more complex than Fig 8, in order to further reduce the search
space. We use more non-terminals to better control which variables
𝑧 can be used where, and we also consider the choice of including
entire subexpressions that occur in the original program 𝑃1, since
they are often reused in the optimized program. The synthesizer
would require many trials to find them, had we not included them
explicitly.

7 EQUALITY SATURATION
Throughout the FGH-optimizer we need to manipulate expressions,
apply rules, and manage equivalent expressions. This problem is
common to all query optimizers. Instead of implementing our own
expression manager, we adopt a state-of-the-art rewriting system
dubbed Equality Saturation (EQSAT). Specifically, we used EGG [54]
to implement the green boxes in the architecture shown in Fig. 6.
An EQSAT system maintains a data structure called an e-graph
that compactly represents a set of expressions, together with an
equivalence relation over this set. Each e-graph consists of a set
of e-classes, each e-class consists of a set of e-nodes, and each

Optimizing Recursive Queries with Program Synthesis

Figure 9: Example e-graph.

e-node is a function symbol with e-classes as children. Figure 9
shows an e-graph representing the two expressions in Eq. (24),
their subexpressions, and other equivalent expressions. Each e-
class (dotted box) represents a class of equivalent expressions. For
example e-class 5 represents 𝐴 ⊗ 𝐵 and 𝐵 ⊗ 𝐴, which are equivalent
by commutativity. e-class 6 represents four equivalent expressions
(including the two choices in e-class 5).

The EQSAT system maintains separately a collection of rules,
each represented by a pair of patterns. For example, one rule may
state that ⊗ is commutative: 𝑥 ⊗𝑦 = 𝑦 ⊗𝑥. The e-graph can efficiently
add a new expression to its collection, insert a new rule, and match
a given expression against the e-graph.

We describe how we use EGG in the FGH-optimizer. First, we use
it to extend the Rule-based test (Sec. 5.1) to account for a constraint
Γ. By design, the e-graph makes it easy to infer the equivalence
𝑃1 = 𝑃2 from a set of rules. Suppose we want to check such an
equivalence conditioned on Γ. We may assume w.l.o.g. that Γ is
a logical implication, Δ ⇒ Θ since all database constraints are
expressed this way. We convert it into an equivalence Δ ∧ Θ = Δ,
and insert it into the e-graph, then check for equivalence 𝑃1 = 𝑃2.
Second, we use the e-graph to denormalize an expression. More
precisely, recall from Sec. 6.1 that we attempt to synthesize 𝐻 by
def
denormalizing 𝑃1
= normalize(𝐹 (𝐺 (𝑋 ))), in other words, writing
it in the form 𝐻 (𝐺 (𝑋 )). For that we add 𝐺 (𝑋 ) to the e-graph,
observe in which e-class it is inserted, and replace that e-class
with a new node 𝑌 . The root of the new e-graph represents many
equivalent expressions, and each of them is a candidate for 𝐻 . We
choose the expression 𝐻 that has the smallest AST and does not
have any occurrence of the IDBs 𝑋 .

Finally, we use the e-graph to infer the loop invariants. We do
this by symbolically executing the recursive program 𝐹 for up to
5 iterations, and compute the symbolic expressions of the IDBs
𝑋 : 𝑋0, 𝑋1, . . . Using an e-graph we represent all identities satisfied
by these (distinct!) expressions. The identities that are satisfied by
every 𝑋𝑖 are candidate loop invariants: for each of them we use the
SMT solver to check if they satisfy Eq. (10) from Sec. 3.2.

8 EVALUATION
We implemented a source-to-source FGH-optimizer, based on Fig. 6.
The input is a program Π1, given by 𝐹, 𝐺, and a database constraint
Γ, and the output is an optimized program 𝐻 . We evaluated it on
three Datalog systems, and several programs from benchmarks pro-
posed by prior research [12, 39]; we also propose new benchmarks
that perform standard data analysis tasks. We did not modify any
of the three Datalog engines. We asked two major questions:

(1) How effective is our source-to-source optimization, given
that each system already supports a range of optimizations?

(2) How much time does the actual FGH optimization take?

8.1 Setup
There is a great number of commercial and open-source Datalog
engines in the wild, but only a few support aggregates in recursion.
We were able to identify five major systems with such support:
SociaLite [38], Myria [50], the DeALS family of systems (DeALS [41],
BigDatalog [40], and RaDlog [20]), RecStep [12], and Dyna [14].
Prior work [39] reports SociaLite and Myria are consistently slower
than newer systems, so we do not include them in our experiments.
Dyna is designed to experiment with novel language semantics and
not for data analytics, and we were not able to run our benchmarks
without errors using it. Systems in the DeALS family are similar
to each other; we pick BigDatalog because it is open source and
runs our benchmarks without errors; we include RecStep for the
same reasons. Both BigDatalog and RecStep are multi-core systems.
Finally, we run experiments on an unreleased commercial system
X, which is single core. As we shall discuss, X is the only one that
supports all features for our benchmarks.

We conducted all experiments on a server running CentOS
8.3.2011. The server has a total of 1008GB memory, and 4 Intel
Xeon CPU E7-4890 v2 2.80GHz CPUs, each with 15 cores and 30
threads. We ran seven benchmarks, shown in Fig 10. BM and CC
are Examples 3.8 and 3.3; MLM is basically Example 3.9. CC, SSSP
and MLM are from [39], the others are designed by us. R and MLM
require a database constraint stating that the data is a tree. BM, R,
and MLM each have a non-trivial loop invariant that is inferred by
the optimizer. Our optimizer requires each program to consist of
two rules, one each for 𝐹 and 𝐺, and so a meaningful metric for
program size is the number of semiring operations. These numbers
are listed in the last column of Fig 10. Our benchmark programs
are comparable in size to those used in prior work [12, 39]. All pro-
grams are available in our git repository. The real-world datasets
twitter [28], epinions [34], and wiki [24] are from the popular SNAP
collection [25]. We follow the setting in [12, 39] when generating
the synthetic graphs. We additionally generate random recursive
trees with an exponential decay, modeling the decay of associa-
tion in multi-level marketing [11]. For WS, we input the vector
[1, . . . , 𝑛], since the values of the entries do not affect run time. In
general, we used smaller datasets than [12, 39] because some of our
experiments run single-threaded.

8.2 Run Time Measurement
For each program-dataset pair, we measure the run times of three
programs: original, with the FGH-optimization, and with the FGH-
optimization and the generalized semi-naive (GSN, for short) trans-
formation. We report only the speedups relative to the original
program in Fig. 11 and 12. In some cases the original program timed
out our preset limit of 3 hours, where we report the speedup against
the 3 hours mark. In some other cases the original program ran
out of memory and we mark them with “o.o.m.” in the figure. The
absolute runtimes are irrelevant for our discussion, since we want
to report the effect of adding our optimizations. (We also do not
have permission to report the runtimes of X.) All three systems

123456Yisu Remy Wang, Mahmoud Abo Khamis, Hung Q. Ngo, Reinhard Pichler, and Dan Suciu

Program
Beyond Magic (BM)
Connected Components (CC)
Single Source Shortest Path (SSSP)
Sliding Window Sum (WS)
Betweenness Centrality (BC)
Graph Radius (R)
Multi-level Marketing (MLM)

Synthesis Type
rule-based
rule-based
rule-based
CEGIS
CEGIS
CEGIS
CEGIS

Constraint?
No
No
No
No
No
Yes
Yes

Invariant?
Yes
No
No
Yes
No
Yes
Yes

Dataset
twitter, epinions, wiki
twitter, epinions, wiki
twitter, epinions, wiki
Vector of Numbers
Erdős–Rényi Graphs
Random Recursive Trees
Random Recursive Trees

Size (# ops)
6
6
17
15
43
12
6

Figure 10: Experimental Setup

Figure 11: Speedup of the optimized v.s. original program; higher is better; t.o. means the original program timed out after 3
hours, in which case we report the speedup against 3 hours; o.o.m. means the original program ran out of memory.

Figure 12: Runtime increase as a function of the data size; lower is better.

Program
Invariance inference
Synthesis
Total
Opt. / Exec. (max-min)

BM
0.092
0.004
0.096
.82% - .16%

CC
0
0.005
0.005
.04%-.01%

SSSP
0
0.004
0.004
.24%-.002%

R
0.129
0.284
0.413
.41%-.07%

MLM
0.132
0.299
0.431
.76%-.09%

BC
0
1.2
1.2
6.3%-.51%

WS
0
0.821
0.821
7.4%-.66%

Program
Search space

R MLM BC WS
94
10

132

20

Figure 13: Optimization time in seconds, optimization time over execution time, and size of the search space.

2M4M6M8M10M1234567relative timeR (random recursive tree)originalFGHFGH+GSN2M4M6M8M10M123456789MLM (random recursive tree)originalFGHFGH+GSN.2k.4k.6k.8k1k1.2k1.4k1.6k1.8k2k020406080100120BCoriginalFGHFGH+GSN1k2k3k4k5k6k7k8k9k10k020406080relative timeR (recursive tree w/ exp. decay)originalFGHFGH+GSN1k2k3k4k5k6k7k8k9k10k01020304050607080MLM (recursive tree w/ exp. decay)originalFGHFGH+GSN.2k.4k.6k.8k1k1.2k1.4k1.6k1.8k2k05101520253035WSoriginalFGHFGH+GSNOptimizing Recursive Queries with Program Synthesis

already perform semi-naive evaluation on the original program,
since that is expressed over the Boolean semiring. But the FGH-
optimized program is over a different semiring (except for BM), and
GSN has non-stratifiable rules with negation, which are supported
only by system X; we report GSN only for system X. While the
benchmarks in Fig. 11 were on real datasets, those in Fig. 12 use
synthetic data, for multiple reasons: we did not have access to a
good tree dataset needed in the R and MLM benchmarks, BC timed
out on our real data (BC is computationally expensive), and WS uses
only a simple array. A benefit of synthetic data is that we can report
how the optimizations scale with the data size. Unfortunately, the
FGH-optimized programs in Fig. 12 require recursion with SUM
aggregation, which is not supported by BigDatalog or RecStep; this
is in contrast with those in Fig. 11, which require recursion with
MIN aggregation which is supported by all systems.

Findings. Figure 11 shows the results of the first group of
8.2.1
benchmarks optimized by the rule-based synthesizer. Overall, we
observe our optimizer provides consistent and significant (up to 4
orders of magnitude) speedup across systems and datasets. Only a
few datapoints indicate the optimization has little effect: BM and
CC on wiki under BigDatalog, and SSSP on wiki under X. This
is due to the small size of the wiki dataset: both the optimized
and unoptimized programs finish very quickly, so the run time is
dominated by system overhead which cannot be optimized away.
We also note that (under X) GSN speeds up SSSP but slows down CC
(note the log scale). The latter occurs because the Δ-relations for CC
are very large, and as a result the semi-naive evaluation has the same
complexity as the naive evaluation; but the semi-naive program
is more complex and incurs a constant slowdown. GSN has no
effect on BM because the program is in the boolean semiring, and X
already implements the standard semi-naive evaluation. Optimizing
BM with FGH on BigDatalog sees a significant speedup even though
the systems already implements magic set rewrite, because the
optimization depends on a loop invariant.7 Overall, both the semi-
naive and naive versions of the optimized program are significantly
faster than the unoptimized program.

Figure 12 shows the results of the second group of benchmarks,
which required CEGIS. Since we used synthetic data, we examined
here the asymptotic behavior of the optimization as a function of
the data size. The most advanced optimization was for BC, which
leads essentially to Brandes’ algorithm [7]: its effect is dramatic. R
and MLM rely on semantic optimization for a tree. We generated
two synthetic trees, a random recursive tree with expected depth
of 𝑂 (log 𝑛) and one with exponential decay with expected depth of
𝑂 (𝑛). Since the benefit of the optimization depends on the depth,
we see a much better asymptotic behavior in the second case. Here,
too, the optimizations were always improving the runtime.

8.3 Optimization Time and the Size of the

Search Space

CEGIS can quickly become very expensive if its search space is
large, and, for that reason, we have designed the grammar genera-
tor carefully to reduce the search space without losing generality.
Fig. 13 reports the runtime of the synthesizer (in seconds) for both

7BigDatalog can optimize the left-recursive version of BM (7) to obtain similar speedup,
via the classic magic set rewrite.

rule-based synthesis and CEGIS, and the size of the search space.
The rule-based synthesizer runs in milliseconds, while CEGIS took
over 1s for BC (our hardest benchmark). These numbers are close
to those demanded by modern query optimizers, and represent
only a tiny portion of the total runtime of the optimized query.
Optimization time takes less than 1% of the query run time for all
benchmarks except for BC and WS on the smallest input data. To
our surprise, our grammar managed to narrow the search space
considerably, to no more than 132 candidates, which (in hindsight)
explains the low optimization times. The search space can grow
rapidly, and even exponentially, as the size of the input program
grows. Our optimizer optimizes a single stratum at a time, focusing
on improving critical “basic blocks” of a program. Our benchmark
programs demonstrate a wide range of data analysis computation
can be expressed succinctly using just a few semiring operations,
and optimization can have a dramatic impact on performance.

8.4 Summary
We conclude that our optimizer can significantly speedup already
optimized Datalog systems, either single-core or multi-core. GSN
can, sometimes, further improve the runtime. We achieved this
using a rather small search space, which led to fast optimization.

9 CONCLUSION
We have presented a new optimization method for recursive queries,
which generalizes many previous optimizations described in the
literature. We implemented it using a CEGIS and an EQSAT system.
Our experiments have shown that this optimization is beneficial,
regardless of what other optimizations a Datalog system supports.
We discuss here some limitations and future work.

Our current implementation is restricted to linear programs, but
our techniques apply to nonlinear programs as well. Non-linear
programs require a more complex grammar Σ; this is likely to
increase the search space, and possibly increase the optimization
time. We leave this exploration to future work.

Our current optimizer is heuristic-based, and future work needs
to integrate it with a cost model. This, however, will be challenging,
because very little work exists for estimating the cost of recursive
queries. This paper applies a simple cost-model. We use the arity
of the IDB predicate as a proxy for a simple asymptotic cost model,
because 𝑁 arity is the size bound of the output, when 𝑁 is the size
of the active domain. This simple cost-model is currently used by
the commercial DB system mentioned in the paper. If the optimized
program reduces the arity, then it is assessed to have lower cost.

Two limitations of our current implementation are the fact that
we currently do not “invent” new IDBs for the optimized query,
and do not apply the FGH-optimizer repeatedly. Both would be
required in order to support more advanced instances of magic set
optimizations.

Our initial motivation for this work came from a real application,
which consists of a few hundred Datalog rules that were compu-
tationally very expensive, and required a significant amount of
manual optimizations. Upon close examination, at a very high level,
the manual optimization that we performed could be described, ab-
stractly, as a sliding window optimization (WS in Fig. 10), which is
one of the simplest instantiations of the FGH-rule. Yet, our current

system is far from able to optimize automatically programs with
hundreds of rules: we leave that for future work.

ACKNOWLEDGMENTS
Suciu and Wang were partially supported by NSF IIS 1907997 and
NSF IIS 1954222. Pichler was supported by the Austrian Science
Fund (FWF):P30930.

REFERENCES
[1] Serge Abiteboul, Richard Hull, and Victor Vianu. 1995. Foundations of Databases.

Addison-Wesley. http://webdam.inria.fr/Alice/

[2] Aws Albarghouthi, Paraschos Koutris, Mayur Naik, and Calvin Smith. 2017.
Constraint-Based Synthesis of Datalog Programs. In Principles and Practice of
Constraint Programming - 23rd International Conference, CP 2017, Melbourne, VIC,
Australia, August 28 - September 1, 2017, Proceedings (Lecture Notes in Computer
Science, Vol. 10416), J. Christopher Beck (Ed.). Springer, 689–706. https://doi.org/
10.1007/978-3-319-66158-2_44

[3] Peter Alvaro, William R. Marczak, Neil Conway, Joseph M. Hellerstein, David
Maier, and Russell Sears. 2010. Dedalus: Datalog in Time and Space. In Datalog
Reloaded - First International Workshop, Datalog 2010, Oxford, UK, March 16-19,
2010. Revised Selected Papers (Lecture Notes in Computer Science, Vol. 6702), Oege
de Moor, Georg Gottlob, Tim Furche, and Andrew Jon Sellers (Eds.). Springer,
262–281. https://doi.org/10.1007/978-3-642-24206-9_16

[4] Isaac Balbin, Graeme S. Port, Kotagiri Ramamohanarao, and Krishnamurthy
Meenakshi. 1991. Efficient Bottom-UP Computation of Queries on Stratified
Databases. J. Log. Program. 11, 3&4 (1991), 295–344. https://doi.org/10.1016/0743-
1066(91)90030-S

[5] François Bancilhon, David Maier, Yehoshua Sagiv, and Jeffrey D. Ullman. 1986.
Magic Sets and Other Strange Ways to Implement Logic Programs. In Proceedings
of the Fifth ACM SIGACT-SIGMOD Symposium on Principles of Database Systems,
March 24-26, 1986, Cambridge, Massachusetts, USA, Avi Silberschatz (Ed.). ACM,
1–15. https://doi.org/10.1145/6012.15399

[6] Catriel Beeri and Raghu Ramakrishnan. 1991. On the Power of Magic. J. Log.
Program. 10, 3&4 (1991), 255–299. https://doi.org/10.1016/0743-1066(91)90038-Q
[7] Ulrik Brandes. 2001. A faster algorithm for betweenness centrality. Journal of

mathematical sociology 25, 2 (2001), 163–177.

[8] Shumo Chu, Chenglong Wang, Konstantin Weitz, and Alvin Cheung. 2017.
Cosette: An Automated Prover for SQL. In 8th Biennial Conference on Inno-
vative Data Systems Research, CIDR 2017, Chaminade, CA, USA, January 8-11,
2017, Online Proceedings. www.cidrdb.org. http://cidrdb.org/cidr2017/papers/p51-
chu-cidr17.pdf

[9] Leonardo de Moura and Nikolaj Bjørner. 2008. Z3: An Efficient SMT Solver. In
Tools and Algorithms for the Construction and Analysis of Systems, C. R. Ramakr-
ishnan and Jakob Rehof (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg,
337–340.

[10] Alin Deutsch, Lucian Popa, and Val Tannen. 1999. Physical Data Independence,
Constraints, and Optimization with Universal Plans. In VLDB’99, Proceedings of
25th International Conference on Very Large Data Bases, September 7-10, 1999, Edin-
burgh, Scotland, UK, Malcolm P. Atkinson, Maria E. Orlowska, Patrick Valduriez,
Stanley B. Zdonik, and Michael L. Brodie (Eds.). Morgan Kaufmann, 459–470.
http://www.vldb.org/conf/1999/P44.pdf

[11] Yuval Emek, Ron Karidi, Moshe Tennenholtz, and Aviv Zohar. 2011. Mechanisms
for multi-level marketing. In Proceedings of the 12th ACM conference on Electronic
commerce. 209–218.

[12] Zhiwei Fan, Jianqiao Zhu, Zuyu Zhang, Aws Albarghouthi, Paraschos Koutris, and
Jignesh M. Patel. 2019. Scaling-Up In-Memory Datalog Processing: Observations
and Techniques. Proc. VLDB Endow. 12, 6 (2019), 695–708. https://doi.org/10.
14778/3311880.3311886

[13] Melvin Fitting. 1991. Bilattices and the Semantics of Logic Programming. J. Log.
Program. 11, 1&2 (1991), 91–116. https://doi.org/10.1016/0743-1066(91)90014-G
[14] Matthew Francis-Landau, Tim Vieira, and Jason Eisner. 2020. Evaluation of Logic
Programs with Built-Ins and Aggregation: A Calculus for Bag Relations. In 13th
International Workshop on Rewriting Logic and Its Applications. 49–63.

[15] Sumit Ganguly, Sergio Greco, and Carlo Zaniolo. 1991. Minimum and Maximum
Predicates in Logic Programming. In Proceedings of the Tenth ACM SIGACT-
SIGMOD-SIGART Symposium on Principles of Database Systems, May 29-31, 1991,
Denver, Colorado, USA, Daniel J. Rosenkrantz (Ed.). ACM Press, 154–163. https:
//doi.org/10.1145/113413.113427

[16] Jonathan Goldstein and Per-Åke Larson. 2001. Optimizing Queries Using Ma-
terialized Views: A practical, scalable solution. In Proceedings of the 2001 ACM
SIGMOD international conference on Management of data, Santa Barbara, CA,
USA, May 21-24, 2001, Sharad Mehrotra and Timos K. Sellis (Eds.). ACM, 331–342.
https://doi.org/10.1145/375663.375706

Yisu Remy Wang, Mahmoud Abo Khamis, Hung Q. Ngo, Reinhard Pichler, and Dan Suciu

[17] Todd J. Green. 2009. Containment of conjunctive queries on annotated relations.
In Database Theory - ICDT 2009, 12th International Conference, St. Petersburg,
Russia, March 23-25, 2009, Proceedings (ACM International Conference Proceeding
Series, Vol. 361), Ronald Fagin (Ed.). ACM, 296–309. https://doi.org/10.1145/
1514894.1514930

[18] Todd J. Green, Gregory Karvounarakis, and Val Tannen. 2007. Provenance
semirings. In Proceedings of the Twenty-Sixth ACM SIGACT-SIGMOD-SIGART
Symposium on Principles of Database Systems, June 11-13, 2007, Beijing, China,
Leonid Libkin (Ed.). ACM, 31–40. https://doi.org/10.1145/1265530.1265535
[19] Shelly Grossman, Sara Cohen, Shachar Itzhaky, Noam Rinetzky, and Mooly Sagiv.
2017. Verifying Equivalence of Spark Programs. In Computer Aided Verification -
29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017,
Proceedings, Part II (Lecture Notes in Computer Science, Vol. 10427), Rupak Majum-
dar and Viktor Kuncak (Eds.). Springer, 282–300. https://doi.org/10.1007/978-3-
319-63390-9_15

[20] Jiaqi Gu, Yugo H. Watanabe, William A. Mazza, Alexander Shkapsky, Mohan Yang,
Ling Ding, and Carlo Zaniolo. 2019. RaSQL: Greater Power and Performance for
Big Data Analytics with Recursive-aggregate-SQL on Spark. In Proceedings of
the 2019 International Conference on Management of Data, SIGMOD Conference
2019, Amsterdam, The Netherlands, June 30 - July 5, 2019, Peter A. Boncz, Stefan
Manegold, Anastasia Ailamaki, Amol Deshpande, and Tim Kraska (Eds.). ACM,
467–484. https://doi.org/10.1145/3299869.3324959

[21] Alon Y. Halevy. 2001. Answering queries using views: A survey. VLDB J. 10, 4

(2001), 270–294. https://doi.org/10.1007/s007780100054

[22] Shan Shan Huang, Todd Jeffrey Green, and Boon Thau Loo. 2011. Datalog and
Emerging Applications: An Interactive Tutorial. In Proceedings of the 2011 ACM
SIGMOD International Conference on Management of Data (Athens, Greece) (SIG-
MOD ’11). Association for Computing Machinery, New York, NY, USA, 1213–1216.
https://doi.org/10.1145/1989323.1989456

[23] Mahmoud Abo Khamis, Hung Q. Ngo, Reinhard Pichler, Dan Suciu, and
Convergence of Datalog over (Pre-) Semirings.

Yisu Remy Wang. 2021.
arXiv:2105.14435v1 [cs.DB]

[24] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. 2010. Signed networks
in social media. In Proceedings of the SIGCHI conference on human factors in
computing systems. 1361–1370.

[25] Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford Large Network

Dataset Collection. http://snap.stanford.edu/data.

[26] Alon Y. Levy, Alberto O. Mendelzon, Yehoshua Sagiv, and Divesh Srivastava.
1995. Answering Queries Using Views. In Proceedings of the Fourteenth ACM
SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, May
22-25, 1995, San Jose, California, USA, Mihalis Yannakakis and Serge Abiteboul
(Eds.). ACM Press, 95–104. https://doi.org/10.1145/212433.220198

[27] Paolo Mascellani and Dino Pedreschi. 2002. The Declarative Side of Magic. In
Computational Logic: Logic Programming and Beyond, Essays in Honour of Robert
A. Kowalski, Part II (Lecture Notes in Computer Science, Vol. 2408), Antonis C.
Kakas and Fariba Sadri (Eds.). Springer, 83–108. https://doi.org/10.1007/3-540-
45632-5_4

[28] Julian J McAuley and Jure Leskovec. 2012. Learning to discover social circles in

ego networks.. In NIPS, Vol. 2012. Citeseer, 548–56.

[29] Inderpal Singh Mumick, Sheldon J. Finkelstein, Hamid Pirahesh, and Raghu
Ramakrishnan. 1990. Magic is Relevant. In Proceedings of the 1990 ACM SIGMOD
International Conference on Management of Data, Atlantic City, NJ, USA, May
23-25, 1990, Hector Garcia-Molina and H. V. Jagadish (Eds.). ACM Press, 247–258.
https://doi.org/10.1145/93597.98734

[30] Inderpal Singh Mumick and Hamid Pirahesh. 1994. Implementation of Magic-
sets in a Relational Database System. In Proceedings of the 1994 ACM SIGMOD
International Conference on Management of Data, Minneapolis, Minnesota, USA,
May 24-27, 1994, Richard T. Snodgrass and Marianne Winslett (Eds.). ACM Press,
103–114. https://doi.org/10.1145/191839.191860

[31] Lucian Popa, Alin Deutsch, Arnaud Sahuguet, and Val Tannen. 2000. A Chase
Too Far?. In Proceedings of the 2000 ACM SIGMOD International Conference on
Management of Data, May 16-18, 2000, Dallas, Texas, USA, Weidong Chen, Jeffrey F.
Naughton, and Philip A. Bernstein (Eds.). ACM, 273–284. https://doi.org/10.
1145/342009.335421

[32] Mukund Raghothaman, Jonathan Mendelson, David Zhao, Mayur Naik, and
Bernhard Scholz. 2020. Provenance-guided synthesis of Datalog programs. Proc.
ACM Program. Lang. 4, POPL (2020), 62:1–62:27. https://doi.org/10.1145/3371130
[33] Raghu Ramakrishnan and Divesh Srivastava. 1994. Semantics and Optimization
IEEE Data Eng. Bull. 17, 2 (1994), 14–17.

of Constraint Queries in Databases.
http://sites.computer.org/debull/94JUN-CD.pdf

[34] Matthew Richardson, Rakesh Agrawal, and Pedro Domingos. 2003. Trust man-
agement for the semantic web. In International semantic Web conference. Springer,
351–368.

[35] Tim Rocktäschel. [n.d.]. Einsum is all you need - Einstein summation in deep

learning. https://rockt.github.io/2018/04/30/einsum.

[36] Timothy Roscoe and Boon Thau Loo. 2018. Declarative Networking. In Encyclo-
pedia of Database Systems, Second Edition, Ling Liu and M. Tamer Özsu (Eds.).
Springer. https://doi.org/10.1007/978-1-4614-8265-9_1220

Optimizing Recursive Queries with Program Synthesis

[37] Matthias Schlaipfer, Kaushik Rajan, Akash Lal, and Malavika Samak. 2017. Op-
timizing Big-Data Queries Using Program Synthesis. In Proceedings of the 26th
Symposium on Operating Systems Principles, Shanghai, China, October 28-31, 2017.
ACM, 631–646. https://doi.org/10.1145/3132747.3132773

[38] Jiwon Seo, Stephen Guo, and Monica S. Lam. 2015. SociaLite: An Efficient Graph
Query Language Based on Datalog. IEEE Trans. Knowl. Data Eng. 27, 7 (2015),
1824–1837. https://doi.org/10.1109/TKDE.2015.2405562

[39] Alexander Shkapsky, Mohan Yang, Matteo Interlandi, Hsuan Chiu, Tyson Condie,
and Carlo Zaniolo. 2016. Big Data Analytics with Datalog Queries on Spark.
In Proceedings of the 2016 International Conference on Management of Data (San
Francisco, California, USA) (SIGMOD ’16). Association for Computing Machinery,
New York, NY, USA, 1135–1149. https://doi.org/10.1145/2882903.2915229
[40] Alexander Shkapsky, Mohan Yang, Matteo Interlandi, Hsuan Chiu, Tyson Condie,
and Carlo Zaniolo. 2016. Big Data Analytics with Datalog Queries on Spark. In
Proceedings of the 2016 International Conference on Management of Data, SIGMOD
Conference 2016, San Francisco, CA, USA, June 26 - July 01, 2016, Fatma Özcan,
Georgia Koutrika, and Sam Madden (Eds.). ACM, 1135–1149. https://doi.org/10.
1145/2882903.2915229

[41] Alexander Shkapsky, Mohan Yang, and Carlo Zaniolo. 2015. Optimizing re-
cursive queries with monotonic aggregates in DeALS. In 31st IEEE Interna-
tional Conference on Data Engineering, ICDE 2015, Seoul, South Korea, April
13-17, 2015, Johannes Gehrke, Wolfgang Lehner, Kyuseok Shim, Sang Kyun
Cha, and Guy M. Lohman (Eds.). IEEE Computer Society, 867–878.
https:
//doi.org/10.1109/ICDE.2015.7113340

[42] Xujie Si, Woosuk Lee, Richard Zhang, Aws Albarghouthi, Paraschos Koutris, and
Mayur Naik. 2018. Syntax-guided synthesis of Datalog programs. In Proceedings
of the 2018 ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2018,
Lake Buena Vista, FL, USA, November 04-09, 2018, Gary T. Leavens, Alessandro
Garcia, and Corina S. Pasareanu (Eds.). ACM, 515–527. https://doi.org/10.1145/
3236024.3236034

[43] Xujie Si, Mukund Raghothaman, Kihong Heo, and Mayur Naik. 2019. Synthesizing
Datalog Programs using Numerical Relaxation. In Proceedings of the Twenty-
Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao,
China, August 10-16, 2019, Sarit Kraus (Ed.). ijcai.org, 6117–6124. https://doi.org/
10.24963/ijcai.2019/847

[44] Armando Solar-Lezama, Liviu Tancau, Rastislav Bodík, Sanjit A. Seshia, and
Vijay A. Saraswat. 2006. Combinatorial sketching for finite programs. In Pro-
ceedings of the 12th International Conference on Architectural Support for Pro-
gramming Languages and Operating Systems, ASPLOS 2006, San Jose, CA, USA,
October 21-25, 2006, John Paul Shen and Margaret Martonosi (Eds.). ACM, 404–415.
https://doi.org/10.1145/1168857.1168907

[45] K. Tuncay Tekle and Yanhong A. Liu. 2019. Extended Magic for Negation: Effi-
cient Demand-Driven Evaluation of Stratified Datalog with Precise Complexity
Guarantees. In Proceedings 35th International Conference on Logic Programming
(Technical Communications), ICLP 2019 Technical Communications, Las Cruces,
NM, USA, September 20-25, 2019 (EPTCS, Vol. 306), Bart Bogaerts, Esra Erdem,
Paul Fodor, Andrea Formisano, Giovambattista Ianni, Daniela Inclezan, Ger-
mán Vidal, Alicia Villanueva, Marina De Vos, and Fangkai Yang (Eds.). 241–254.

https://doi.org/10.4204/EPTCS.306.28

[46] Emina Torlak and Rastislav Bodík. 2013. Growing solver-aided languages with
rosette. In ACM Symposium on New Ideas in Programming and Reflections on
Software, Onward! 2013, part of SPLASH ’13, Indianapolis, IN, USA, October 26-31,
2013, Antony L. Hosking, Patrick Th. Eugster, and Robert Hirschfeld (Eds.). ACM,
135–152. https://doi.org/10.1145/2509578.2509586

[47] Emina Torlak and Daniel Jackson. 2007. Kodkod: A Relational Model Finder. In
Tools and Algorithms for the Construction and Analysis of Systems, 13th Interna-
tional Conference, TACAS 2007, Held as Part of the Joint European Conferences on
Theory and Practice of Software, ETAPS 2007 Braga, Portugal, March 24 - April 1,
2007, Proceedings (Lecture Notes in Computer Science, Vol. 4424), Orna Grumberg
and Michael Huth (Eds.). Springer, 632–647. https://doi.org/10.1007/978-3-540-
71209-1_49

[48] Margus Veanes, Pavel Grigorenko, Peli de Halleux, and Nikolai Tillmann. 2009.
Symbolic Query Exploration. In Formal Methods and Software Engineering, 11th
International Conference on Formal Engineering Methods, ICFEM 2009, Rio de
Janeiro, Brazil, December 9-12, 2009. Proceedings (Lecture Notes in Computer Science,
Vol. 5885), Karin K. Breitman and Ana Cavalcanti (Eds.). Springer, 49–68. https:
//doi.org/10.1007/978-3-642-10373-5_3

[49] Victor Vianu. 2021. Datalog Unchained. In Proceedings of the 40th ACM SIGMOD-
SIGACT-SIGAI Symposium on Principles of Database Systems (Virtual Event, China)
(PODS’21). Association for Computing Machinery, New York, NY, USA, 57–69.
https://doi.org/10.1145/3452021.3458815

[50] Jingjing Wang, Magdalena Balazinska, and Daniel Halperin. 2015. Asynchronous
and Fault-Tolerant Recursive Datalog Evaluation in Shared-Nothing Engines.
Proc. VLDB Endow. 8, 12 (Aug. 2015), 1542–1553. https://doi.org/10.14778/2824032.
2824052

[51] Yuepeng Wang, Isil Dillig, Shuvendu K. Lahiri, and William R. Cook. 2018. Veri-
fying equivalence of database-driven applications. Proc. ACM Program. Lang. 2,
POPL (2018), 56:1–56:29. https://doi.org/10.1145/3158144

[52] Yuepeng Wang, Rushi Shah, Abby Criswell, Rong Pan, and Isil Dillig. 2020. Data
Migration using Datalog Program Synthesis. Proc. VLDB Endow. 13, 7 (2020),
1006–1019. https://doi.org/10.14778/3384345.3384350

[53] Yisu Remy Wang, Shana Hutchison, Dan Suciu, Bill Howe, and Jonathan Leang.
2020. SPORES: Sum-Product Optimization via Relational Equality Saturation
for Large Scale Linear Algebra. Proc. VLDB Endow. 13, 11 (2020), 1919–1932.
http://www.vldb.org/pvldb/vol13/p1919-wang.pdf

[54] Max Willsey, Chandrakana Nandi, Yisu Remy Wang, Oliver Flatt, Zachary Tatlock,
and Pavel Panchekha. 2021. egg: Fast and extensible equality saturation. Proc.
ACM Program. Lang. 5, POPL (2021), 1–29. https://doi.org/10.1145/3434304
[55] Carlo Zaniolo, Mohan Yang, Ariyam Das, Alexander Shkapsky, Tyson Condie,
and Matteo Interlandi. 2017. Fixpoint semantics and optimization of recursive
Datalog programs with aggregates. Theory Pract. Log. Program. 17, 5-6 (2017),
1048–1065. https://doi.org/10.1017/S1471068417000436

[56] Carlo Zaniolo, Mohan Yang, Matteo Interlandi, Ariyam Das, Alexander Shkapsky,
and Tyson Condie. 2018. Declarative BigData Algorithms via Aggregates and
Relational Database Dependencies. In Proceedings of the 12th Alberto Mendelzon
International Workshop on Foundations of Data Management, Cali, Colombia, May
21-25, 2018 (CEUR Workshop Proceedings, Vol. 2100), Dan Olteanu and Barbara
Poblete (Eds.). CEUR-WS.org. http://ceur-ws.org/Vol-2100/paper2.pdf

Yisu Remy Wang, Mahmoud Abo Khamis, Hung Q. Ngo, Reinhard Pichler, and Dan Suciu

A GRAMMAR REFINEMENTS
As discussed in Section 6.2.3, we implement further refinements of the grammar in Figure 8 to limit the search space. First, we allow the
user to specify a type for each attribute of a relation. For example, a weighted edge relation 𝐸 (𝑥 : ID, 𝑦 : ID, 𝑧 : int) has 2 attributes of
type ID and the last attribute typed int. Although they all share the same concrete type (think “machine type”), the abstract types help
guide the synthesizer to never use the same variables for attributes of different types. Second, we allow the user to define helper functions.
One example is the definition of 𝐷 in Figure 18 which computes the distance between two vertices. 𝐷 is used repeatedly in the remaining
definition in Figure 18, and it is a common practice for programmers to abstract out such recurring patterns. We leverage good practice like
this to aid synthesis: if a user defines a helper function with head relation 𝑅, we will include 𝑅 as a base relation in our grammar because it
is likely that the helper function is also helpful in the optimized query. When generating a candidate, we simply inline the definition of
the helper function, thereby including an entire sub-expression of the original query. Concretely, the refinements augment the grammar
in Figure 8 with types and adds the following cases to the production rule of 𝑄0:

where 𝑍𝑖𝑛𝑡 are variables of type int and 𝐹 is a user-defined helper function.

𝑄0 → · · · | 𝑍𝑖𝑛𝑡 | 𝐹 (𝑍, 𝑍, . . . , 𝑍 )

B BENCHMARK PROGRAMS
Figures 14 to 20 contain the benchmark queries used in the experiments. In each query, 𝑉 is the set of input vertices and 𝐸 the set of input
edges. A binary edge relation is unweighted, and a ternary edge relation is weighted with the weight in the third position. The domain of all
relations is integers. Each query outputs the head relation of the last rule.

𝑇𝐶 (𝑥, 𝑦) = 𝑉 (𝑥) ∧ [𝑥 = 𝑦] ∨ ∃𝑡 (𝑇𝐶 (𝑥, 𝑡) ∧ 𝐸 (𝑡, 𝑦))

𝑅(𝑦) = 𝑇𝐶 (a, 𝑦)

Figure 14: Beyond Magic (BM). a is a constant vertex ID, chosen uniformly at random during experiments.

𝑇𝐶 (𝑥, 𝑦) = 𝑉 (𝑥) ∧ [𝑥 = 𝑦] ∨ ∃𝑡 (𝑇𝐶 (𝑥, 𝑡) ∧ 𝐸 (𝑡, 𝑦))
𝑆𝐶𝐶 [𝑥] = min

{𝑣 | 𝑇𝐶 (𝑥, 𝑣)}

𝑣

Figure 15: Connected Components (CC). Note the vertex ID itself is used as the label for that vertex, instead of 𝐿(𝑣) in fig. 1.

𝐷 (𝑥, 𝑑) = [𝑥 = a] ∧ [𝑑 = 0]

∨ ∃(𝑦, 𝑑1, 𝑑2 : 𝐷 (𝑦, 𝑑1) ∧ 𝐸 (𝑦, 𝑥, 𝑑2) ∧ [𝑑 = 𝑑1 + 𝑑2])

𝑆𝑃 [𝑥] = min

𝑑

{𝑑 | 𝐷 (𝑥, 𝑑)}

Figure 16: Single-source Shortest Paths (SSSP). a is a constant vertex ID, chosen uniformly at random during experiments.

𝑊 (𝑡, 𝑗, 𝑤) = 𝐴( 𝑗, 𝑤) ∧ [𝑡 = 𝑗]

∨ ∃(𝑠 : [𝑡 = 𝑠 + 1] ∧ 𝑊 (𝑠, 𝑗, 𝑤) ∧ [1 ≤ 𝑗 < 𝑡])

𝑃 [𝑡] =

∑︁

𝑗,𝑤

{𝑤 | 𝑊 (𝑡, 𝑗, 𝑤)}

𝑆 [𝑡] = 𝑃 [𝑡] − 𝑃 [𝑡 − 10]

Figure 17: Window Sum (WS) with a window size of 10. 𝐴 is an array, and 𝐴(𝑖, 𝑣) is true when 𝐴 holds 𝑣 at index 𝑖, and arrays
are 1-indexed.

Optimizing Recursive Queries with Program Synthesis

𝐷 (𝑠, 𝑡, 𝑘) = 𝑉 (𝑥) ∧ [𝑠 = 𝑡] ∧ [𝑘 = 0]
∨ [𝑘 = 1 + min
𝑣,𝑙

{𝑙 | 𝐸 (𝑣, 𝑡) ∧ [𝑠 ≠ 𝑡] ∧ 𝐷 (𝑠, 𝑣, 𝑙)}]

𝜎 (𝑠, 𝑡, 𝑛) = 𝑉 (𝑠) ∧ [𝑠 = 𝑡] ∧ [𝑛 = 1]

∨ 𝐸 (𝑣, 𝑡) ∧ 𝐷 (𝑠, 𝑣, 𝑑𝑠𝑣) ∧ 𝐷 (𝑠, 𝑡, 𝑑𝑠𝑡 )
∧ [𝑑𝑠𝑣 = 𝑑𝑠𝑡 + 1] ∧ [𝑠 ≠ 𝑡] ∧ 𝜎 (𝑠, 𝑣, 𝑚)

𝐵 [𝑣] =

∑︁

𝑠,𝑡,𝑏

{𝜎𝑠𝑣 × 𝜎𝑣𝑡 /𝜎𝑠𝑡 | [𝑠 ≠ 𝑡] ∧ [𝑠 ≠ 𝑣] ∧ [𝑡 ≠ 𝑣]

∧ 𝐷 (𝑠, 𝑡, 𝑑𝑠𝑡 ) ∧ 𝐷 (𝑠, 𝑣, 𝑑𝑠𝑣) ∧ 𝐷 (𝑣, 𝑡, 𝑑𝑣𝑡 ) ∧ [𝑑𝑠𝑡 = 𝑑𝑠𝑣 + 𝑑𝑣𝑡 ]
∧ 𝜎 (𝑠, 𝑣, 𝜎𝑠𝑣) ∧ 𝜎 (𝑣, 𝑡, 𝜎𝑣𝑡 ) ∧ 𝜎 (𝑠, 𝑡, 𝜎𝑠𝑡 )}

Figure 18: Betweenness Centrality (BC). Intuitively, 𝐷 computes the distance between two vertices, and 𝜎 computes the number
of shortest paths between two vertices.

𝑇𝐶 (𝑥, 𝑦, 𝑤) = 𝑉 (𝑥) ∧ [𝑥 = 𝑦] ∧ [𝑤 = 0]

∨ ∃(𝑧, 𝑤1 : 𝑇𝐶 (𝑥, 𝑧, 𝑤1) ∧ 𝐸 (𝑧, 𝑦) ∧ [𝑤 = 𝑤1 + 1])

𝑆𝑃 [𝑥, 𝑦] = min
𝑤

{𝑤 | 𝑇𝐶 (𝑥, 𝑦, 𝑤)}

𝑅 [𝑥] = max

𝑦

{𝑆𝑃 [𝑥, 𝑦] | }

Figure 19: Graph Radius (R). 𝑅 [𝑥] computes the length of the longest shortest-path between 𝑥 and any other vertex in the
graph. Intuitively, it is the diameter with one vertex fixed.

𝑇𝐶 (𝑥, 𝑦) = 𝑉 (𝑥) ∧ [𝑥 = 𝑦] ∨ ∃(𝑧 : 𝑇𝐶 (𝑥, 𝑧) ∧ 𝐸 (𝑧, 𝑦))

𝑀 [𝑥] =

∑︁

𝑣

{𝑣 | 𝑇𝐶 (𝑥, 𝑣)}

Figure 20: Multi-level Marketing (MLM). Intuitively, each vertex 𝑣 represents a participant who makes 𝑣 amount of profit; the
query 𝑀 [𝑥] computes the total profit of the sub-network under participant 𝑥.

C MAGIC SET OPTIMIZATION
We describe here a general form of magic set optimization and show that its correctness can be proven by a verifier using only three rules:
the FGH rule, the Stratification Rule, and the Fixpoint rule, described below. Each of the rewrite rules can be proven by our verifier, but our
synthesizer cannot synthesize the magic rewritings in general; it is currently restricted to relatively simple rewritings, like those illustrated
earlier in the paper.

Notations. In this section we restrict the discussion to the Boolean semiring and monotone functions. When we write 𝐹 (𝑋 ) we assume
that 𝑋 is a tuple of IDB relations, e.g. 𝑋 = (𝑅, 𝑆,𝑇 ), and 𝐹 (𝑋 ) returns a tuple of relations of the same arities. A datalog program has the form
𝑋 :- 𝐹 (𝑋 ), and we denote by ¯𝑋 its least fixpoint. Given two tuples of relations 𝑋, 𝑌 of the same type, i.e. the same number of relations and of
the same arities, we write 𝑋 ⇒ 𝑌 to mean component-wise set inclusion. For example, if 𝑋 = (𝑅1, 𝑆1,𝑇1) and 𝑌 = (𝑅2, 𝑆2,𝑇2) then 𝑋 ⇒ 𝑌
means 𝑅1 ⊆ 𝑅2, 𝑆1 ⊆ 𝑆2, 𝑇1 ⊆ 𝑇2.

C.1 The Three Rules

Simplified FGH Rule. We consider the following simplified version of the FGH rule. Given two datalog programs:

Π1 : 𝑋 :- 𝐹 (𝑋 )

Π2 : 𝑌 :- 𝐻 (𝑌 )

a homomorphism from Π1 to Π2 is a function 𝐺 satisfying 𝐺 (∅) = ∅ and 𝐺 (𝐹 (𝑋 )) = 𝐻 (𝐺 (𝑋 )) for all 𝑋 . If such a homomorphism exists,
then the FGH-rule in Theorem 3.1 implies the following:

𝐺 ( ¯𝑋 ) =¯𝑌

(32)

The only difference from the FGH-rule in Theorem 3.1 is that we do not ask for Π1 to return 𝐺 (𝑋 ).

Yisu Remy Wang, Mahmoud Abo Khamis, Hung Q. Ngo, Reinhard Pichler, and Dan Suciu

Stratification Rule. Assume 𝐾 (𝑍 ), 𝐹 (𝑍, 𝑋 ) are two monotone functions over tuples of relations, such that the output of 𝐾 has the same

type as 𝑍 , and the output of 𝐹 has the same type as 𝑋 . Then the following two programs are equivalent:

Π :

𝑍 :-
𝑋 :-

𝐾 (𝑍 )
𝐹 (𝑍, 𝑋 )

Π′ : 𝑍 :-
𝑋 :-

𝐾 (𝑍 )
𝐹 ( ¯𝑍, 𝑋 )

// Stratum 0: let ¯𝑍 be its fixpoint
// Stratum 1: note the use of ¯𝑍

The program Π computes both sets of rules 𝐾, 𝐹 in a single stratum. Program Π′ separates them into two strata: first it computes the
fixpoint ¯𝑍 of 𝐾, then uses it as an EDB to compute the fixpoint of 𝐹 ( ¯𝑍, 𝑋 ). This stratification rule is well known for monotone datalog, and
def
we omit the proof. A formal statement asserts that, for any 𝜔-continuous functions 𝐾, 𝐹 , denoting 𝐿(𝑍, 𝑋 )
= (𝐾 (𝑍 ), 𝐹 (𝑍, 𝑋 )), we have
lfp(𝐿) = (lfp(𝐾), lfp(𝜆𝑋 .𝐹 (lfp(𝐾), 𝑋 ))).

Fixpoint Rule. Consider a datalog program, and suppose that its stratum 𝑠 is the following:

𝑋 :- 𝐹 (𝑋 )

Let ¯𝑋 be the fixpoint of stratum 𝑠. Then the following constraint holds in all strata 𝑠 ′ > 𝑠:

𝐹 ( ¯𝑋 ) ⇒ ¯𝑋

(33)

C.2 Running Example
Throughout this section we will illustrate using the following example.

Example C.1. We show below a program Π and its magic set optimized program Π𝑂 .
𝑄 ′
0 () :-
𝑅′
𝑅′
𝑂 (𝑦) :-
𝑂 (𝑥) ∧ 𝑇 (𝑥, 𝑦, 𝑧)
𝑅′
𝑅′
𝑂 (𝑧) :-
𝑂 (𝑥) ∧ 𝑇 (𝑥, 𝑦, 𝑧) ∧ 𝑅𝑂 (𝑦)
𝑂 (𝑥) :- 𝑄 ′
𝑅′
𝑂 () ∧ 𝐺 (𝑥)
𝑅′
𝑅𝑂 (𝑥) :-
𝑂 (𝑥) ∧ 𝑉 (𝑥)
𝑅′
𝑅𝑂 (𝑥) :-
𝑂 (𝑥) ∧ 𝑇 (𝑥, 𝑦, 𝑧) ∧ 𝑅(𝑦) ∧ 𝑅(𝑧)
𝑄𝑂 (𝑥) :- 𝑄 ′
𝑂 () ∧ 𝐺 (𝑥) ∧ 𝑅𝑂 (𝑥)

𝑅(𝑥) :-
𝑅(𝑥) :-
𝑄 (𝑥) :- 𝐺 (𝑥) ∧ 𝑅(𝑥)

𝑉 (𝑥)
𝑇 (𝑥, 𝑦, 𝑧) ∧ 𝑅(𝑦) ∧ 𝑅(𝑧)

Π𝑂 :

Π :

Π computes an IDB 𝑅(𝑥), then returns 𝑄 (𝑥) which is a restriction of 𝑅. The optimized program Π𝑂 computes the IDB 𝑅𝑂 (𝑥) only on a
subset of the nodes 𝑥 that are sufficient to answer 𝑄, namely the set defined by 𝑅′
𝑂 (𝑥) is called the magic predicate.

𝑂 (𝑥). The predicate 𝑅′

C.3 Definition of Magic Set Rewriting
We use the elegant definition of magic set rewriting by Mascellani and Pedreschi [27].

Notation. An atom 𝐴 is a predicate symbol followed by variables and/or constants, e.g. 𝐴 can be 𝑅(′𝑎′,′ 𝑏 ′, 𝑥, 𝑦). Single atoms are denoted
𝐴, 𝐵, . . . and sequences (possibly empty) of atoms by A, B, . . . Each datalog rule has the form 𝐴 :- A. Following the convention used in [1], a
query is given by a datalog program Π and a query predicate 𝑄 not occurring in Π, such that 𝑄 is defined by a single rule 𝑟𝑄 of the form
𝑟𝑄 : 𝑄 (v) :- A, where A does not contain the predicate 𝑄.

Modes. For an 𝑛-ary relation symbol 𝑅, a mode is a string {+, −}𝑛. Intuitively a + represents an input, and a − represents an output.
Given a datalog program Π, we fix a moding for each relational symbol 𝑅. The moding can be arbitrary, with a single restriction: the mode of
the output predicate 𝑄 must be (−, −, · · · , −), i.e. all its positions are output positions. To simplify the notations, we will assume w.l.o.g. that,
for each relational symbol 𝑅 the input positions precede the output positions, i.e. its mode is + · · · + − · · · −. Hence, an atom 𝐴 of the form
𝑅(u, v) has input arguments u and output arguments v.

Magic set transformation. To each relational symbol 𝑅 we associate two new symbols. A magic symbol 𝑅′

𝑂 , whose arity is the number of
def
= 𝑅′(u)
𝑂 and 𝑅𝑂 respectively. Similarly, if 𝑨 is a sequence of atoms, then we
𝑂 and 𝑨𝑂 the corresponding sequences of atoms. If 𝑅 is an EDB, then we define 𝑅𝑂 to be the same EDB, and we will often

input positions in the mode of 𝑅, and an optimized symbol 𝑅𝑂 , of the same arity as 𝑅. If 𝐴 is the atom 𝑅(u, v), then we denote by 𝐴′
𝑂
and 𝐴𝑂
denote by 𝑨′
remove the subscript 𝑂.

def
= 𝑅𝑂 (𝒖, 𝒗) the atoms with the corresponding symbols 𝑅′

Definition C.2. [27, Definition 3] Let Π be a datalog program with an output predicate 𝑄. The magic set transformation is the program Π𝑂

obtained from Π by the following transformation steps:

(1) For every rule in Π fix an order of the atoms in its body, i.e. the rule becomes:

For every atom 𝐵ℓ above add the new rule:

𝑟 : 𝐴 :- 𝐵1 ∧ 𝐵2 ∧ · · · ∧ 𝐵𝑘

ℓ : 𝐵′
𝑟 ′

ℓ,𝑂 :- 𝐴′

𝑂 ∧ 𝐵1,𝑂 ∧ · · · ∧ 𝐵ℓ−1,𝑂

(34)

Optimizing Recursive Queries with Program Synthesis

(2) Add the following rule with an empty body (i.e. the body is true): 𝑄 ′
(3) Replace each original rule 𝐴 :- A in Π by the new rule 𝐴𝑂 :- 𝐴′
𝑂, A𝑂 .

𝑂 () :- .

Example C.3. In Example C.1 we use the modes 𝑅(+), and 𝑄 (−). We associate to the atoms 𝑅(𝑥), 𝑄 (𝑥) the magic atoms 𝑅′

𝑂 (𝑥), 𝑄 ′
𝑂 () and
the optimized atoms 𝑅𝑂 (𝑥), 𝑄𝑂 (𝑥). The modes for the EDBs 𝑉 ,𝑇 , 𝐺 can be arbitrary, since the magic symbols 𝑉 ′
𝑂 are never used in
any rule, and hence they were omitted from Π𝑂 ; they are useful for us only to simplify the statement of Lemma C.6 below, and for that
reason we illustrate them here, assuming that their modes are −, −, −. Then, according to item 1 of Def. C.2, the optimized program should
include these rules:

𝑂, 𝐺 ′

𝑂,𝑇 ′

𝑂 () :- 𝑅′
𝑉 ′
𝑂 (𝑥)
𝑇 ′
𝑂 () :- 𝑅′
𝑂 (𝑥)
𝑂 () :- 𝑄 ′()
𝐺 ′

If we add these rules to the program Π𝑂 in Example C.1, then we observe that, at fixpoint, ¯𝑅′
𝑂 () = ¯𝑇 ′
¯𝑉 ′
Correctness. We restate here the theorem from [27, Theorem 4]:

𝑂 () = true.

𝑂 () = ¯𝐺 ′

𝑂 ≠ ∅ (assuming 𝐺 ≠ ∅), and therefore,

Theorem C.4. Let Π be a datalog program with query predicate 𝑄. Fix any moding of its symbols, and let Π𝑂 , be the corresponding magic

program. Then, at fixpoint, the IDB ¯𝑄 computed by Π equals the IDB ¯𝑄𝑂 computed by Π𝑂 .

We will re-prove the theorem by showing, importantly, that the equivalence of the two programs follows from the three rules, FGH,

Stratification, and Fixpoint, and therefore can be checked automatically by a verifier.

C.4 Property B
To prove the equivalence of Π and Π𝑂 we only need one property of the optimized program, which we call Property B. We prove here
that the specific rewriting in Def. C.2 ensures that the resulting program Π0 satisfies Property B; later we will show that any program Π𝑂
satisfying Property B is equivalent to Π. Our proof here consists of several applications of the chase procedure, which we briefly review here.

The Chase. Suppose that the following constraint holds: ∀𝑥 (Φ(𝑥) ⇒ Ψ(𝑥)). Then the following equivalence holds:

∀𝑥 (Φ(𝑥) ∧ Γ(𝑥) ≡Φ(𝑥) ∧ Ψ(𝑥) ∧ Γ(𝑥))

By “applying the chase” we mean rewriting the formula Φ(𝑥) ∧ Γ(𝑥) to Φ(𝑥) ∧ Ψ(𝑥) ∧ Γ(𝑥). The “back-chase” proceeds in reverse, i.e.
it removes Ψ(𝑥). Both the chase and the back-chase can be encoded in an EQSAT system (see Sec. 7), and therefore proofs based on
chase/back-chase can be derived and checked automatically.

Consider a program Π and its magic-set rewriting Π𝑂 in Definition C.2. Consider the least fixpoint of Π𝑂 ; as usual we denote by ¯𝑅′

𝑂, ¯𝑅𝑂

the instances in this least fixpoint; this notation extends to the case when 𝑅 is an EDB symbol, then simply ¯𝑅𝑂

def
= 𝑅.

Definition C.5 (Boundedness). Let 𝑅 be an IDB symbol occurring in the program Π. We say that an instance 𝑅 is bounded w.r.t. Π𝑂 if it

satisfies:

This definition holds trivially for each EDB relation 𝑅: each such relation is bounded, because ¯𝑅𝑂 = 𝑅 by definition.

Lemma C.6 (Property B). Consider an instance of all IDBs of the program Π that is bounded w.r.t. Π𝑂 . Then, for every rule 𝑟 in the original

program Π:

∀𝑢∀𝑣 ( ¯𝑅′

𝑂 (𝑢) ∧ 𝑅(𝑢, 𝑣) ⇒ ¯𝑅𝑂 (𝑢, 𝑣))

(35)

the following equivalence holds:

𝑟 : 𝐴 :- 𝑩

𝑂 ∧ ¯𝑩′
To help the reader parsing Eq. (36), we note that the atoms in the sequence 𝑩 refer to the bounded relational instance (the symbols

𝑂 ∧ 𝑩 ≡ ¯𝐴′
¯𝐴′

𝑂 ∧ 𝑩

(36)

appearing in Π), while the atoms ¯𝐴′

𝑂 and ¯𝑩′

𝑂 refer to the fixpoint of the program Π𝑂 .

Proof. This proof is the place where we use the magic rule (34) in item 1 of Definition C.2. Specifically, we write 𝑩 = 𝐵1 ∧ · · · ∧ 𝐵𝑘 ,
where the order of the atoms is that chosen in Def. C.2 item 1. At fixpoint, for each ℓ = 1, 𝑘, the magic rule (34) becomes the following
implication by the Fixpoint rule (see Eq. (33)):

(cid:219)

¯𝐴′

𝑂 ∧

¯𝐵𝑖,𝑂 ⇒ ¯𝐵′

ℓ,𝑂

𝑖=1,ℓ−1

(37)

We will chase repeatedly the LHS of (36) with the implications (37) and (35) to arrive at the RHS.

Yisu Remy Wang, Mahmoud Abo Khamis, Hung Q. Ngo, Reinhard Pichler, and Dan Suciu

For each ℓ = 0, 𝑘, denote by Φℓ the following sentence:

Φℓ

def
= ¯𝐴′ ∧

(cid:219)

(cid:16) ¯𝐵′

𝑖,𝑂 ∧ 𝐵𝑖

(cid:17)

∧

(cid:219)

𝐵𝑖

𝑖=1,ℓ

𝑖=ℓ+1,𝑘

Eq. (36) asserts that Φ0 ≡ Φ𝑘 , and we prove it by showing that, for every ℓ = 1, 𝑘, the following holds:

This follows from the following chase steps:

¯𝐴′

𝑂 ∧

(cid:219)

(cid:16) ¯𝐵′

𝑖,𝑂 ∧ 𝐵𝑖

(cid:17)

∧ 𝐵ℓ ≡ ¯𝐴′

𝑂 ∧

(cid:219)

(cid:16) ¯𝐵′

𝑖,𝑂 ∧ ¯𝐵𝑖,𝑂 ∧ 𝐵𝑖

Φℓ−1 ≡Φℓ

𝑖=1,ℓ−1

≡ ¯𝐴′

𝑂 ∧

≡ ¯𝐴′

𝑂 ∧

𝑖=1,ℓ−1
(cid:219)

(cid:16) ¯𝐵′

𝑖,𝑂 ∧ ¯𝐵𝑖,𝑂 ∧ 𝐵𝑖

𝑖=1,ℓ−1
(cid:16) ¯𝐵′
(cid:219)

𝑖,𝑂 ∧ 𝐵𝑖

(cid:17)

𝑖=1,ℓ

(cid:17)

(cid:17)

∧ 𝐵ℓ

Chase with (35)

∧ ¯𝐵′

ℓ,𝑂 ∧ 𝐵ℓ

Chase with (37)

Back-chase with (35)

By conjoining both sides of the equivalence above with (cid:211)𝑖=ℓ+1,𝑘 𝐵𝑖 we obtain Φℓ−1 ≡ Φℓ , as required.

□

Example C.7. We describe the Property B for the running Example C.1. Let ¯𝑅′

𝑂, ¯𝑄 ′

𝑂 be the outputs of the magic predicates of the program

Π𝑂 . Notice that 𝑄 ′

𝑂 () ≡ true. The Property B asserts the following: if 𝑅, 𝑄 are two bounded instances, meaning that they satisfy:

then the following holds (one constraint for each of the 3 rules of Π):

∀𝑥 ( ¯𝑅′

𝑂 (𝑥) ∧ 𝑅(𝑥) ⇒ ¯𝑅𝑂 (𝑥))

∀𝑥 ( ¯𝑄 ′

𝑂 () ∧ 𝑄 (𝑥) ⇒ ¯𝑄𝑂 (𝑥))

(38)

We saw in Example C.3 that ¯𝑉 ′

𝑂 () ≡ true, hence to check Property B it suffices to check only two equivalences:

∀𝑥 ( ¯𝑅′

∀𝑥 ( ¯𝑄 ′

∀𝑥 ( ¯𝑅′
𝑂 (𝑥) ∧ 𝑉 (𝑥) ≡ ¯𝑅′
𝑂 (𝑥) ∧ 𝑇 (𝑥, 𝑦, 𝑧) ∧ 𝑅(𝑦) ∧ 𝑅(𝑧) ≡ ¯𝑅′
𝑂 () ∧ 𝐺 (𝑥) ∧ 𝑅(𝑥) ≡ ¯𝑄 ′
𝑂 () ≡ ¯𝐺 ′

𝑂 () ≡ ¯𝑇 ′
𝑂 (𝑥) ∧ 𝑇 (𝑥, 𝑦, 𝑧) ∧ 𝑅(𝑦) ∧ 𝑅(𝑧) ≡ ¯𝑅′
𝑂 () ∧ 𝐺 (𝑥) ∧ 𝑅(𝑥) ≡ ¯𝑄 ′

∀𝑥 ( ¯𝑄 ′

∀𝑥 ( ¯𝑅′

𝑂 (𝑥) ∧ ¯𝑉 ′
𝑂 (𝑥) ∧ ¯𝑇 ′
𝑂 () ∧ ¯𝐺 ′

𝑂 () ∧ 𝑉 (𝑥))
𝑂 () ∧ 𝑇 (𝑥, 𝑦, 𝑧) ∧ ¯𝑅′
𝑂 () ∧ 𝐺 (𝑥) ∧ ¯𝑅′

𝑂 (𝑥) ∧ 𝑅(𝑥))

𝑂 (𝑦) ∧ 𝑅(𝑦) ∧ ¯𝑅′

𝑂 (𝑧) ∧ 𝑅(𝑧))

𝑂 (𝑥) ∧ 𝑇 (𝑥, 𝑦, 𝑧) ∧ ¯𝑅′
𝑂 () ∧ 𝐺 (𝑥) ∧ ¯𝑅′

𝑂 (𝑥) ∧ 𝑅(𝑥))

𝑂 (𝑦) ∧ 𝑅(𝑦) ∧ ¯𝑅′

𝑂 (𝑧) ∧ 𝑅(𝑧))

We leave it up to the reader to check that both constraints can be derived by repeated chase and back-chase using the constraints (38) and
the following constraints derived from the property of the fixpoint of Π𝑂 (see Eq. (33)):

∀𝑥 ( ¯𝑅′

𝑂 (𝑥) ∧ 𝑇 (𝑥, 𝑦, 𝑧) ⇒ ¯𝑅′
𝑂 (𝑥) ∧ 𝑇 (𝑥, 𝑦, 𝑧) ∧ ¯𝑅𝑂 (𝑦) ⇒ ¯𝑅′

𝑂 (𝑦))
𝑂 (𝑧))

∀𝑥 ( ¯𝑅′

C.5 Correctness Proof of Magic Set Rewriting
We prove a stronger claim than Theorem C.4: we prove the correctness of magic set rewriting using only the FGH, the Stratification, and
the Fixpoint rules. While at a high level our proof is inspired by Mascellani and Pedreschi [27], it differs in that we use the least fixpoint
semantics of a datalog program rather than the minimal model semantics. This allows us to prove the equivalence Π ≡ Π𝑂 using only the
three rules, FGH, Stratification, and Fixpoint. Moreover, our proof here is independent of the particular definition of the magic set rewriting
used to define Π𝑂 (Def. C.2), and, instead, applies to any program Π𝑂 that satisfies Property B.

Let 𝑋 denote the tuple of IDBs of the original program Π, and fix a moding. Then we let 𝑋 ′ denote the tuple of the magic IDBs, in other

words:

We denote by 𝑋 ′ ∧ 𝑋 their pairwise conjunction:

𝑋 =(𝑅1 (𝒖1, 𝒗1), 𝑅2 (𝒖2, 𝒗2), . . .)
𝑋 ′ =(𝑅′

2 (𝒖2), . . .)

1 (𝒖1), 𝑅′

𝑋 ′ ∧ 𝑋

Consider two programs Π and Π𝑂 :

Π : 𝑋 :- 𝐹 (𝑋 )

def
= (𝑅′

1 (𝒖1) ∧ 𝑅1 (𝒖1, 𝒗1), 𝑅′

2 (𝒖2) ∧ 𝑅2 (𝒖2, 𝒗2), . . .)

Π𝑂 : 𝑋 ′

𝑂 :- 𝐹 ′(𝑋 ′
𝑋𝑂 :- 𝑋 ′

𝑂, 𝑋𝑂 )
𝑂 ∧ 𝐹 (𝑋𝑂 )

(39)

Optimizing Recursive Queries with Program Synthesis

where 𝐹, 𝐹 ′ are two monotone functions. The second line in Π𝑂 corresponds directly to the optimized rules in item 3 of Definition C.2. The
first line corresponds to the magic predicates. For the moment we allow 𝐹 ′, which should not be confused with a derivative8, to be any
monotone function.

Let ¯𝑋 ′

𝑂, ¯𝑋𝑂 denote the fixpoint of the program Π𝑂 . We generalize Definition C.5:

Definition C.8 (Boundedness). We say that 𝑋 is bounded w.r.t. Π𝑂 if it satisfies:

¯𝑋 ′
𝑂 ∧ 𝑋 ⇒ ¯𝑋𝑂

Definition C.9 (Property B). We say that Π𝑂 satisfies Property B, if every bounded 𝑋 satisfies:

¯𝑋 ′
𝑂 ∧ 𝐹 (𝑋 ) = ¯𝑋 ′

𝑂 ∧ 𝐹 ( ¯𝑋 ′

𝑂 ∧ 𝑋 )

Theorem C.10. Consider two programs Π, Π𝑂 where Π𝑂 satisfies Property B. Then the following holds:

where ¯𝑋 is the fixpoint of Π and ¯𝑋 ′

𝑂, ¯𝑋𝑂 is the fixpoint of Π𝑂 .

¯𝑋 ′
𝑂 ∧ ¯𝑋 = ¯𝑋𝑂

(40)

(41)

Theorem C.10 immediately implies Theorem C.4. Indeed, if 𝐹 (𝑋 ) is the ICO of the original datalog program and 𝐹 ′ defines the magic
predicates, as per items 1 and 2 of Def. C.2, then the optimized program satisfies Property B, by Lemma C.6. Thus, the identity (41) holds,
and, in particular, the following for the query predicate 𝑄:

𝑄 ′

𝑂 () ∧ 𝑄 (¯𝑢) =𝑄𝑂 (¯𝑢)

Theorem C.4 follows from the fact that 𝑄 ′

𝑂 () ≡ true; this is the only place where we need item 2 of Definition C.2.

In the rest of this subsection we prove Theorem C.10.

Proof of Theorem C.10. We use four steps.

Step 1: FGH rule for Π𝑂 ≡ Πcopy. We start by using the FGH-rule to prove that the program Π𝑂 is equivalent to the following program:
𝑋 ′
𝑂 :- 𝐹 ′(𝑋 ′
𝑋copy :- 𝑋 ′
𝑋𝑂 :- 𝑋 ′

𝑂, 𝑋copy)
𝑂 ∧ 𝐹 (𝑋copy)
𝑂 ∧ 𝐹 (𝑋𝑂 )

Πcopy :

The new program creates a copy 𝑋copy of 𝑋𝑂 . Intuitively, it is obvious that the new program computes the same IDBs as the original program.
Formally, one can check that the following function 𝐺 is a homomorphism mapping the state (𝑋 ′
𝑂, 𝑋copy, 𝑋𝑂 )
of Πcopy: 𝐺 (𝑋 ′
𝑂, 𝑋𝑂, 𝑋𝑂 ). If the fixpoint of Π𝑂 is ( ¯𝑋 ′
𝑂, ¯𝑋𝑂 ), then the homomorphism implies that the fixpoint of Πcopy is
( ¯𝑋 ′
𝑂, ¯𝑋𝑂, ¯𝑋𝑂 ).
Step 2: Stratification rule for Πcopy ≡ Π1. We apply the stratification rule to Πcopy, and write it as:

𝑂, 𝑋𝑂 ) of Π𝑂 to the state (𝑋 ′

def
= (𝑋 ′

𝑂, 𝑋𝑂 )

Π0 :

𝑋 ′
𝑂 :-
𝑋copy :-

𝐹 ′(𝑋 ′
𝑂, 𝑋copy)
𝑋 ′
𝑂 ∧ 𝐹 (𝑋copy)

Π1 :

𝑋𝑂 :-

¯𝑋 ′
𝑂 ∧ 𝐹 (𝑋𝑂 )

(42)

We denote by ¯𝑋 ′

𝑂, ¯𝑋copy (= ¯𝑋𝑂 ) the fixpoint of the first stratum Π0. Importantly, the second stratum Π1 uses the fixpoint ¯𝑋 ′
Step 3: Fixpoint rule for the invariant Φ(𝑋 ). Next, we prove that the state 𝑋 of the original program Π satisfies the following invariant:

𝑂 as an EDB.

The invariant holds trivially when 𝑋 = ∅. Assuming that it holds for 𝑋 , we check that it also holds for 𝐹 (𝑋 ):

Φ(𝑋 ) ≡( ¯𝑋 ′

𝑂 ∧ 𝑋 ⇒ ¯𝑋𝑂 )

𝑂 ∧ 𝐹 (𝑋 ) ≡ ¯𝑋 ′
¯𝑋 ′
⇒ ¯𝑋 ′
⇒ ¯𝑋𝑂

𝑂 ∧ 𝑋 )

𝑂 ∧ 𝐹 ( ¯𝑋 ′
𝑂 ∧ 𝐹 ( ¯𝑋𝑂 )

By Property B, (40)

Induction hypothesis Φ(𝑋 )
Fixpoint rule: ¯𝑋𝑂 is the least fixpoint of Π𝑂

8We use the notation 𝐹 ′ to follow the convention in [27] where the magic predicate for 𝑅 is denoted 𝑅′.

Step 4: FGH-rule for Π ≡ Π1. Consider now the original program Π and the program Π1: their states are 𝑋 and 𝑋𝑂 respectively. We

claim that, under the invariant Φ(𝑋 ), the following function 𝐺 is a homomorphism from Π to Π𝑂 :

𝐺 (𝑋 )

def
= ¯𝑋 ′

𝑂 ∧ 𝑋

Yisu Remy Wang, Mahmoud Abo Khamis, Hung Q. Ngo, Reinhard Pichler, and Dan Suciu

We prove 𝐺 (𝐹 (𝑋 )) = 𝐻 (𝐺 (𝑋 )) where 𝐻 is the ICO of the program Π1, Eq. (42). We expand both sides:
𝑂 ∧ 𝐹 ( ¯𝑋 ′

𝐻 (𝐺 (𝑋 )) = ¯𝑋 ′

𝐺 (𝐹 (𝑋 )) = ¯𝑋 ′

𝑂 ∧ 𝐹 (𝑋 )

𝑂 ∧ 𝑋 )

Their equality follows immediately from Property B (40).

By the FGH-rule (32), it follows that ¯𝑋 ′

the fixpoint of Π1 is equal to that of the magic optimized program Π𝑂 , and this completes the proof.

𝑂 ∧ ¯𝑋 = ¯𝑋𝑂 , where ¯𝑋, ¯𝑋𝑂 are the fixpoints of Π and Π1 respectively. We have already shown that
□

C.6 Discussion

Necessity of Stratification. A question is whether the stratification rule is redundant, more precisely whether the correctness proof of
the magic set rewriting could be completed using only the FGH rule. The answer is no. To see this, observe that if two datalog programs are
proven equivalent by the FGH rule, then the number of iterations needed by the second program to reach a fixpoint is at most equal to that
needed by the first program, see Corollary 3.2. However, in some cases, the magic-set optimized program may require a significantly larger
number of iterations, proving that the FGH-rule is insufficient to prove their equivalence.

For a concrete example, consider the programs Π and Π𝑂 in Example C.1, and assume that 𝑇 (𝑥, 𝑦, 𝑧) is a complete binary tree of depth 𝑛
and with 2𝑛 nodes, where 𝑥 is the parent and 𝑦 and 𝑧 are the two children. The original program Π reaches its fixpoint after 𝑛 iterations,
while the optimized program Π𝑂 may require 2𝑛 iterations, since it performs a left-deep traversal of the tree.

The Flexibility of Moding. A nice feature of the framework introduced by Mascellani and Pedreschi [27] is that it decouples the
correctness proof from the performance consideration. In practice, the moding, which is also called adornment or binding pattern, is
determined by a Sideway Information Passing (SIP) algorithm. However, the correctness proof holds for any moding, even if it is not the
result of a SIP. We illustrate this decoupling with a classic example.

Example C.11. The same generation program, and its magic rewriting, are the following:

Π :

𝑆 (𝑥, 𝑦) :- 𝐻 (𝑥, 𝑦)
𝑆 (𝑥, 𝑦) :-
𝑄 (𝑦) :-

𝑈 (𝑥, 𝑝) ∧ 𝑆 (𝑝, 𝑞) ∧ 𝐷 (𝑞, 𝑦)
𝑆 (𝑎, 𝑦)

The EDBs 𝑈 , 𝐷, 𝐻 stand for “up”, “down”, and “horizontal”.

Π𝑂 :

𝑄 ′
𝑂 () :-
𝑆 ′
𝑆 ′
𝑂 (𝑝) :-
𝑂 (𝑥) ∧ 𝑈 (𝑥, 𝑝)
𝑂 (𝑎) :- 𝑄 ′
𝑆 ′
𝑂 ()
𝑆 ′
𝑂 (𝑥) ∧ 𝐻 (𝑥, 𝑦)
𝑆 ′
𝑂 (𝑥) ∧ 𝑈 (𝑥, 𝑝) ∧ 𝑆𝑂 (𝑝, 𝑞) ∧ 𝐷 (𝑞, 𝑦)
𝑄𝑂 (𝑦) :- 𝑄 ′
𝑂 () ∧ 𝑆𝑂 (𝑎, 𝑦)

𝑆𝑂 (𝑥, 𝑦) :-
𝑆𝑂 (𝑥, 𝑦) :-

The SIP-based magic rewriting will adorn 𝑆 with +−, because 𝑄 (𝑦) = 𝑆 (𝑎, 𝑦) where 𝑎 is a constant, and will order the atoms in the rule
𝑆 (𝑥, 𝑦) :- 𝑈 (𝑥, 𝑝) ∧ 𝑆 (𝑝, 𝑞) ∧ 𝐷 (𝑞, 𝑦) as shown, so as to facilitate sideways information passing. This leads to the optimized program Π𝑂
shown above.

What happens if we chose a different order in the rule for 𝑆 (𝑥, 𝑦)? Assuming the order is 𝑆 (𝑥, 𝑦) :- 𝑈 (𝑥, 𝑝) ∧ 𝐷 (𝑞, 𝑦) ∧ 𝑆 (𝑝, 𝑞). The new

magic set rewriting will have a modified rule for 𝑆 ′

𝑂 (𝑝):
𝑂 (𝑝) :- 𝑆 ′
𝑆 ′

𝑂 (𝑥) ∧ 𝑈 (𝑥, 𝑝) ∧ 𝐷 (𝑞, 𝑦)

We have introduced a redundant cartesian product with 𝐷 (𝑞, 𝑦): the new magic program is still correct, but less efficient.

What happens if we choose the adornment −+ for 𝑆? Also, assume that we reorder the atoms in the second rule for 𝑆 to: 𝑆 (𝑥, 𝑦) :- 𝐷 (𝑞, 𝑦) ∧

𝑆 (𝑝, 𝑞) ∧ 𝑈 (𝑥, 𝑝). Then the magic rewriting becomes

𝑄 ′
𝑂 () :-
𝑂 (𝑞) :- 𝑆 ′(𝑦) ∧ 𝐷 (𝑞, 𝑦)
𝑆 ′
𝑂 (𝑦) :- 𝑄 ′
𝑆 ′
𝑂 ()
𝑆𝑂 (𝑥, 𝑦) :- 𝑆 ′
𝑂 (𝑦) ∧ 𝐻 (𝑥, 𝑦)
𝑆𝑂 (𝑥, 𝑦) :- 𝑆 ′
𝑂 (𝑦) ∧ 𝐷 (𝑞, 𝑦), ∧𝑆𝑂 (𝑝, 𝑞) ∧ 𝑈 (𝑥, 𝑝)
𝑄𝑂 (𝑦) :- 𝑄 ′
𝑂 () ∧ 𝑆𝑂 (𝑎, 𝑦)

The third rule above9, 𝑆 ′
original one.

𝑂 (𝑦) :- 𝑄 ′

𝑂 (), defines 𝑆 ′

𝑂 as the entire domain. The “optimized” program is still correct, but less efficient than the

9Strictly speaking the rule is unsafe. We allow it here for illustration.

Optimizing Recursive Queries with Program Synthesis

Multiple Modings. Finally, we explain how to circumvent an apparent limitation of the framework of Mascellani and Pedreschi [27]:
the fact that each predicate symbol 𝑅 can have a single mode. When multiple modes are needed, then this can be achieved by making copies
of the IDBs and moding them differently. We illustrate this with another classic example of magic set rewriting.

Example C.12. Consider the reverse-same-generation program:

Π :

𝑆 (𝑥, 𝑦) :- 𝐻 (𝑥, 𝑦)
𝑆 (𝑥, 𝑦) :- 𝑈 (𝑥, 𝑝) ∧ 𝑆 (𝑞, 𝑝) ∧ 𝐷 (𝑞, 𝑦)
𝑄1 (𝑦) :- 𝑆 (𝑎, 𝑦)

The only change is that 𝑆 (𝑝, 𝑞) is replaced by 𝑆 (𝑞, 𝑝) in the second rule. The SIP algorithm requires us to adorn 𝑆 in two ways, both +− and
−+. To achieve that it suffices to create two copies of 𝑆, the left 𝑆𝑙 and the right 𝑆𝑟 . More precisely, consider the program:

Π′ :

𝑆𝑙 (𝑥, 𝑦) :- 𝐻 (𝑥, 𝑦)
𝑆𝑟 (𝑥, 𝑦) :- 𝐻 (𝑥, 𝑦)
𝑆𝑙 (𝑥, 𝑦) :- 𝑈 (𝑥, 𝑝) ∧ 𝑆𝑟 (𝑞, 𝑝) ∧ 𝐷 (𝑞, 𝑦)
𝑆𝑟 (𝑥, 𝑦) :- 𝐷 (𝑞, 𝑦) ∧ 𝑆𝑙 (𝑞, 𝑝) ∧ 𝑈 (𝑥, 𝑝)

𝑄 (𝑦) :- 𝑆𝑙 (𝑎, 𝑦)

The FGH-rule proves formally that Π and Π′ are equivalent. More precisely, consider the following function 𝐺 mapping the state 𝑆 of Π to
def
the state 𝑆𝑙, 𝑆𝑟 or Π′: 𝐺 (𝑆)
= (𝑆, 𝑆). One can check immediately that 𝐺 is a homomorphism. It follows that Π and Π′ compute the same IDB
𝑆 = 𝑆𝑙 = 𝑆𝑟 .

Following SIP, we define the following modings for Π′, 𝑆𝑙 (+−), 𝑆𝑟 (−+), 𝑄 (−), and also use the ordering of the rules as show above, where,
in the rule for 𝑆𝑟 , we have switched the order of 𝐷 and 𝑈 . Then, the magic set transformation in Def. C.2 produces the following optimized
program:

Π𝑂 : 𝑄 ′
𝑂 () :-
𝑆𝑟 ′
𝑂 (𝑝) :- 𝑆𝑙 ′
𝑆𝑙 ′
𝑂 (𝑞) :- 𝑆𝑟 ′
𝑆𝑙 ′
𝑂 (𝑎) :- 𝑄 ′
𝑆𝑙𝑂 (𝑥, 𝑦) :- 𝑆𝑙 ′
𝑆𝑟𝑂 (𝑥, 𝑦) :- 𝑆𝑟 ′
𝑆𝑙𝑂 (𝑥, 𝑦) :- 𝑆𝑙 ′
𝑆𝑟𝑂 (𝑥, 𝑦) :- 𝑆𝑟 ′
𝑄𝑂 (𝑦) :- 𝑄 ′

𝑂 (𝑥) ∧ 𝑈 (𝑥, 𝑝)
𝑂 (𝑦) ∧ 𝐷 (𝑞, 𝑦)
𝑂 ()
𝑂 (𝑥) ∧ 𝐻 (𝑥, 𝑦)
𝑂 (𝑦) ∧ 𝐻 (𝑥, 𝑦)
𝑂 (𝑥) ∧ 𝑈 (𝑥, 𝑝) ∧ 𝑆𝑟𝑂 (𝑞, 𝑝) ∧ 𝐷 (𝑞, 𝑦)
𝑂 (𝑦) ∧ 𝐷 (𝑞, 𝑦) ∧ 𝑆𝑙𝑂 (𝑞, 𝑝) ∧ 𝑈 (𝑥, 𝑝)
𝑂 (), 𝑆𝑙𝑂 (𝑎, 𝑦)

