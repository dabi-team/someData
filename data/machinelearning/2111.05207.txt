1
2
0
2

v
o
N
9

]
S
M

.
s
c
[

1
v
7
0
2
5
0
.
1
1
1
2
:
v
i
X
r
a

Computing Sparse Jacobians and Hessians Using
Algorithmic Diﬀerentiation

Bradley M. Bell1 and Kasper Kristensen2

1IHME, University of Washington, Seattle, USA,
bradbell@seanet.com
2DTU Aqua, Technical University of Denmark, DK, kaskr@dtu.dk

November 10, 2021

Abstract

Stochastic scientiﬁc models and machine learning optimization esti-
mators have a large number of variables; hence computing large sparse
Jacobians and Hessians is important. Algorithmic diﬀerentiation (AD)
greatly reduces the programming eﬀort required to obtain the sparsity
patterns and values for these matrices. We present forward, reverse,
and subgraph methods for computing sparse Jacobians and Hessians.
Special attention is given the the subgraph method because it is new.
The coloring and compression steps are not necessary when computing
sparse Jacobians and Hessians using subgraphs. Complexity analysis
shows that for some problems the subgraph method is expected to be
much faster. We compare C++ operator overloading implementations
of the methods in the ADOL-C and CppAD software packages using
some of the MINPACK-2 test problems. The experiments are set up
in a way that makes them easy to run on diﬀerent hardware, diﬀerent
systems, diﬀerent compilers, other test problem and other AD pack-
ages. The setup time is the time to record the graph, compute sparsity,
coloring, compression, and optimization of the graph. If the setup is
necessary for each evaluation, the subgraph implementation has similar
run times for sparse Jacobians and faster run times for sparse Hessians.

Keywords: automatic diﬀerentiation, sparse, minpack-2, cppad, adolc, c++
AMS classiﬁcations: 05C15, 65F50, 90C30
ACM classiﬁcations: F.2.2, G.2.2

1

 
 
 
 
 
 
1

INTRODUCTION

1

Introduction

This paper concerns the computation of sparse Jacobians and Hessians for
a function f : Rn → Rm that can be evaluated using a computer program.
We use x ∈ Rn to denote the inputs and y ∈ Rm the outputs of the program.

1.1 The Computational Graph Representation

We use v ∈ R(cid:96) to denote all intermediate values that depend on the inputs

v = (v1, . . . , v(cid:96))T ∈ Rn

The independent variable subvector (input vector) is

x = (v1, . . . , vn)T ∈ Rn .

The dependent variable subvector (output vector) is

y = (v(cid:96)−m+1, . . . , v(cid:96))T ∈ Rm .

We decompose the computations to a sequence of elementary functions φk :
R2 → R. For k = n + 1 to (cid:96)

vk = φk(va[k], vb[k])

where the positive integer a[k] and b[k] are the variable index of the left and
right operands for the k-th elementary function. It follows that a[k] < k
and b[k] < k, i.e., computation of the k-th variable only depends on the in-
dependent variables and variables that have been computed previously. The
variables are nodes in the computation graph. Each elementary operation
corresponds to two arcs, one from variable va[k] to variable vk and the other
from variable vb[k] to variable vk. We also write a[k] ≺ k and b[k] ≺ k which
means variable k directly depends on variables a[k] and b[k]. This is very
similar to the notation in [Griewank and Walther, 2008, Section 2.2]. Some
references call nodes a[k], b[k] predecessors of node k and node k a successor
of a[k], b[k]; for example see [Gower and Mello, 2014], [Wang et al., 2016].

We represent a unary elementary function using a[k] = b[k] together with
a binary function φk(va[k], vb[k]) that does not depend on it second argument.
One example of a unary representation is φk(va[k], vb[k]) = sin(va[k]). Another
example is φk(va[k], vb[k]) = va[k] which can be used to set yi = v(cid:96)−m+i =
va[(cid:96)−m+i]. It is not necessary to have the extra node for v(cid:96)−m+i in the graph
for this purpose but it makes this presentation simpler.

2

1

INTRODUCTION

1.2 Dependency Relation

Example 1.1 Consider the computer program deﬁned by the following pseudo
code:

function y = f (x)

(v1, v2, v3) = x
v4
v5
y

= v1 + v2
= v3 ∗ v4
= (v4, v5)

The corresponding function f : R3 → R2 is given by

f1(x) = x1 + x2
f2(x) = x3(x1 + x2)

The following is a diagram of the corresponding computational graph:

(cid:27)(cid:24)

(cid:27)(cid:24)

v4

(cid:26)(cid:25)
(cid:64)(cid:73)+

(cid:64)

(cid:0)(cid:18)

(cid:0)

(cid:45)

v5

(cid:26)(cid:25)
*

(cid:54)

(cid:27)(cid:24)

(cid:0)

(cid:64)

(cid:27)(cid:24)

(cid:27)(cid:24)

v1

(cid:26)(cid:25)

v2

(cid:26)(cid:25)

v3

(cid:26)(cid:25)

If the last line of the program pseudo code were changed to y = v4, then
f : R3 → R, f (x) = x3(x1 + x2) and the diagram above would not change.

The dimension of the range of φk might be greater than one, and its
domain might be greater than two. For example, in some operator over-
loaded AD tools it is possible to record the operations for one function
and make that function an elementary operation in another function. This
is closely related to checkpointing; see [Griewank and Walther, 2008, Tables
12.3, 12.4]. We only include binary elementary functions in order to simplify
this presentation.

1.2 Dependency Relation

There may be a diﬀerence between sparsity patterns and dependency pat-
terns. For example, suppose that

φk(va[k], vb[k]) =

(cid:26) 0 if va[k] ≤ 0
1 otherwise

.

3

2 SPARSE JACOBIANS

It follows that, where φk is diﬀerentiable, its derivative is zero. Some AD
packages might return zero for the derivative everywhere while others might
return (+∞, 0) when va[k] = 0. In any event, φk(va[k], vb[k]) depends on the
value of va[k]. Hence, a dependency pattern might include more possibly
non-zero elements than a Jacobian sparsity pattern. For the purpose of this
presentation, we do not distinguish Jacobian sparsity patterns and depen-
dency patterns. We use the notation a[k] ≺ k and b[k] ≺ k to denote the
fact that vk directly depends on va[k] and vb[k]. Furthermore, we use ≺∗
to denote the transitive closure of this relation; e.g., if p ≺ q and q ≺ k
then p ≺∗ k; see [Griewank and Walther, 2008, Section 2.2]. Note that the
φk(va[k], vb[k]) deﬁned above does not depend on vb[k]; i.e., it represents a
unary function. In the text below, unary functions correspond to the case
where φk does not depend on its second argument and a[k] = b[k].

2 Sparse Jacobians

In this section we present a forward algorithm, reverse algorithm, and re-
verse subgraph algorithm that compute Jacobian sparsity patterns. We also
present an algorithm that obtains a sorted subgraph and discuss computing
Jacobians and the computational complexity of the subgraph algorithms.

2.1 Forward Mode Sparsity

We use J to denote the subset of independent variable indices that are of in-
terest, J ⊂ {1, . . . , n}. For the independent variable indices, k ∈ {1, . . . , n},
we deﬁne Xk to be the singleton {k}, if k ∈ J, otherwise the empty set. For
k > n we deﬁne Xk to be the indices of independent variables in J that the
variable vk depends on; i.e.,

Xk =






{k}
∅
{j : j ∈ J and j ≺∗ k} otherwise

if k ≤ n and k ∈ J
if k ≤ n and k /∈ J

.

The forward Jacobian sparsity calculation is given by Algorithm 2.1; see
[Griewank and Walther, 2008, Eq. (7.4)]. Note that for unary functions,
a[k] = b[k] and the union in the algorithm is not necessary. The sparsity
pattern for the Jacobian f (1)(x) is given by the observation that for i =
1, . . . , m, j ∈ J, and x ∈ Rn,

[f (1)
i

(x)]j (cid:54)= 0 ⇒ j ∈ X(cid:96)−m+i .

4

2 SPARSE JACOBIANS

2.2 Reverse Mode Sparsity

Algorithm 1 Forward Jacobian Sparsity
{ X1 , . . . , X(cid:96) } = function(J)

for k = 1, . . . , n

if k ∈ J set Xk = {k}
else

set Xk = ∅

for k = n + 1, . . . , (cid:96)

set Xk = Xa[k] ∪ Xb[k]

We note that there are (cid:96) sets Xk and at most |J| elements in each of these
sets where |J| ≤ n is the number of elements in set J.

2.2 Reverse Mode Sparsity

We use I to denote the subset of dependent variable indices that are of
interest, I ⊂ {1, . . . , m}. For the corresponding variable indices k ∈ {(cid:96) −
m + 1, . . . , (cid:96)} we deﬁne Yk to be the singleton {k − (cid:96) + m}, if k − (cid:96) + m ∈ I,
otherwise the empty set. For k ≤ (cid:96) − m we deﬁne Yk to be the indices of
dependent variables that the variable in I that are aﬀected by vk; i.e.,

Yk =






{k − (cid:96) + m}
∅
{i ∈ I : k ≺∗ (cid:96) − m + i} otherwise

if k > (cid:96) − m and k − (cid:96) + m ∈ I
if k > (cid:96) − m and k − (cid:96) + m /∈ I

.

We use ∪= for the operator that sets the left hand side to the right hand side
union the previous value of the left hand side. The reverse Jacobian sparsity
calculation is given by Algorithm 2.2; see [Griewank and Walther, 2008, Eq.
(7.8)]. Note that for unary functions, a[k] = b[k] and the second union in
the algorithm is not necessary. The sparsity pattern for the Jacobian f (1)(x)
is given by the observation that for i ∈ I, j = 1, . . . , n, and x ∈ Rn,

[f (1)
i

(x)]j (cid:54)= 0 ⇒ i ∈ Yj .

We note that there are (cid:96) sets Yk and at most |I| elements in each of these
sets where |I| ≤ m is the number of elements in set I.

2.3 Reverse Subgraph Sparsity

We call this a reverse subgraph method because on each pass through the
graph it only visits the nodes that aﬀect a selected dependent variable. (A

5

2.3 Reverse Subgraph Sparsity

2 SPARSE JACOBIANS

Algorithm 2 Reverse Jacobian Sparsity
{ Y1 , . . . , Y(cid:96) } = function(I)

for k = (cid:96), . . . , (cid:96) − m + 1
if k − (cid:96) + m ∈ I
else

set Yk = {k − (cid:96) + m}
set Yk = ∅

for k = (cid:96) − m, . . . , 1
set Yk = ∅
for k = (cid:96) − m, . . . , 1

set Ya[k]
set Yb[k]

∪= Yk
∪= Yk

forward subgraph method would only visit nodes that are aﬀected by a
selected independent variable.) We present two versions of the algorithm.
The ﬁrst is the ‘pattern only’ variant which ﬁnds the sparsity pattern of the
Jacobian matrix. The second variant is an extension that ﬁnds the actual
subgraph required to calculate the non-zero values of the Jacobian.

We use I ⊂ {1, . . . , m} to denote the subset of dependent variable in-
dices that are of interest. We use J ⊂ {1, . . . , n} to denote the subset of
independent variable indices that are of interest. The set Si accumulates the
independent variables in J that aﬀect the dependent variable yi = v(cid:96)−m+i.
The stack K contains the nodes in the subgraph that have not yet been
processed for this independent variable index i. Let {Xk} be the output
corresponding to Algorithm 2.1 with input J. It is not necessary to have
the sequence of sets {Xk}, just the integer sequence {ck} initialized by

if Xk = ∅ set ck = m + 1 else set ck = 0 .

The value ck = i < m + 1 is used to indicate that node k has already
been processed for this independent variable index i. The reverse subgraph
Jacobian sparsity calculation is deﬁned by Algorithm 2.3.

The input value for the sequence {ck} can be computed using Algo-
rithm 2.1 with the sets Xk replaced by integer values ck that are m + 1 for
empty Xk and 0 for non-empty Xk. The complexity of this calculation is
O((cid:96)), (cid:96) is the number of nodes in the graph. Note that for unary functions,
a[k] = b[k] and the condition for the second if block in the algorithm is false.
The sparsity pattern for the Jacobian f (1)(x) is given by the observation
that for i ∈ I, j ∈ J, and x ∈ Rn,

[f (1)
i

(x)]j (cid:54)= 0 ⇒ j ∈ Si .

6

2 SPARSE JACOBIANS

2.4 Reverse Subgraph Sorting

Algorithm 3 Reverse Subgraph Jacobian: Return Sparsity Pattern
{Si : i ∈ I} = function(I, c1, . . . , c(cid:96))

for i ∈ I

set done = i, ignore = m + 1
set K = ∅
set Si = ∅
push (cid:96) − m + i into K
while K (cid:54)= ∅

pop k from K
if ca[k] /∈ {done, ignore}
set ca[k] = done
if a[k] ≤ n set Si
else push a[k] into K

∪= {a[k]}

if cb[k] /∈ {done, ignore}
set cb[k] = done
if b[k] ≤ n set Si
else push b[k] into K

∪= {b[k]}

We note that there are |I| sets Si, at most |J| elements in each of these sets,
and |I| ≤ m , |J| ≤ n. We contrast this with the forward and reverse mode
sparsity patterns which have (cid:96) sets; i.e., a set for each node in the graph.

Remark 2.1 Algorithm 2.3 is a non-recursive depth-ﬁrst search in reverse
direction from each dependent variable. It uses a mark vector c, of already
processed nodes, to avoid multiple placements of the same node in the stack
K. The mark vector need not be cleared between consecutive searches because
the dependent variable index is used as unique mark for each search. This
is important when the subgraph is much smaller than the full graph. The
‘pop k from K’ in the algorithm could be replaced by extracting any element
from K and the algorithm would still work. However, extracting the most
recently pushed element (the pop) reduces the worst case space requirement
for K from l, the size of the full graph, to the maximum vertex depth of the
graph.

2.4 Reverse Subgraph Sorting

Algorithm 2.3 computes the sparsity patterns {Si : i ∈ I}. For each i, the
subgraph corresponding to the node order of the pops from K would have to

7

2.5 Computing Jacobians

2 SPARSE JACOBIANS

be sorted in dependency order to be used for computations. For example, to
calculate reverse mode derivatives along the subgraph a dependency order
is required. In a worst case scenario, the subgraphs are as big as the full
graph and the cost of a reverse sweep is O(l). The cost of sorting the
subgraph is O(l × log(l)). Hence, the sorting could asymptotically become
expensive compared to other parts of the algorithm. It is therefore relevant
to avoid the sort. With a minor modiﬁcation of Algorithm 2.3 we can directly
obtain a dependency sorted subgraph. If vd[1], . . . , vd[|G|] is the ordering for
a subgraph G, we say that G is dependency sorted or topologically sorted if

vd[p] ≺∗ vd[q] ⇒ p < q .

The main change to the algorithm is as follows: When the top of the stack
is a node, for which all the nodes it depends on are already in the subgraph,
the top node of the stack is moved to the end of the subgraph. A new mark
−i is used to signify that a node has been moved to the subgraph. The
result is that the nodes in the subgraph are always dependency sorted.

Remark 2.2 Algorithm 2.4 can be used to ﬁnd all the subgraphs, one by
one, as needed to perform a full Jacobian calculation. When a Gi has been
used for a sweep it is not needed anymore and can be discarded. Note that
Gi is only guarantied to be dependency sorted. In particular, one cannot as-
sume that the independent variable indices (node indices ≤ n) in the sparsity
pattern of the i-th row of the Jacobian are at the beginning of Gi.

2.5 Computing Jacobians

Once a sparsity pattern for a Jacobian is available, a row (column) com-
pression technique could be used to by reverse mode (forward mode) to
multiple rows (columns) of the Jacobian during one pass of the compu-
tational graph. This requires an approximate solution of a graph color-
ing problem; see [Griewank and Walther, 2008, Eq. 8.6 and Section 8.3],
[Coleman and Verma, 1998].

The reverse subgraph Jacobian method does not require coloring or a
compression step. In this way it is similar to the edge pushing algorithm; see
[Petra et al., 2018], [Gower and Mello, 2014]. Reference [Gower and Mello, 2014]
deﬁnes the apex-induced subgraph corresponding to a ﬁxed node as the
nodes that the ﬁxed node depends on (plus the corresponding arcs). The
subgraphs in this paper are apex-induced subgraphs corresponding to de-
pendent variables. The subgraph Jacobian calculation method for a selected
dependent variable is the same as for normal reverse pass, except that only

8

2 SPARSE JACOBIANS

2.5 Computing Jacobians

Algorithm 4 Reverse Subgraph Jacobian: Return Sorted Subgraph
{Gi : i ∈ I} = function(I, c1, . . . , c(cid:96))

for i ∈ I

set visited = i, done = −i, ignore = m + 1
set Gi = ∅, K = ∅
push (cid:96) − m + i into K
while K (cid:54)= ∅

set k = top (K)
comment If all inputs to node k are done move k to Gi
if ca[k] ∈ {done, ignore} and cb[k] ∈ {done, ignore}

pop k from K
push k into Gi
set ck = done

else

comment vmax may depend on vmin
for ν = max(a[k], b[k]), min(a[k], b[k])
if cν /∈ {visited, done, ignore}

if ν ≤ n

else

push ν into Gi
set cν = done

push ν into K
set cν = visited

9

2.6 Complexity of the Subgraph Methods

2 SPARSE JACOBIANS

the subset of nodes that aﬀect a selected dependent variable yi are pro-
cessed. The subgraph and normal reverse mode calculations are so similar
that CppAD uses iterators to implement both calculations with the same
code; see [Bell, 2020].

2.6 Complexity of the Subgraph Methods

The asymptotic complexity of the subgraph algorithms 2.3 and 2.4 is easy to
assess. Both algorithms loop across subgraphs and nodes in each subgraph.
For each subgraph, each subgraph node is visited once in Algorithm 2.3
and twice in Algorithm 2.4. The work per node is bounded by a constant
independent of the computational graph because there are no more than
two arguments (incoming edges) and one result (variable) for each node. It
follows that the complexity of both algorithms is

(cid:32)

(cid:88)

O

i∈I

(cid:33)

|Gi|

+ O((cid:96)) ,

whether computing patterns only or numerical entries of the Jacobian. The
set I is the independent variables of interest and |I| is less than or equal m.
The term O((cid:96)) is for the complexity of initializing the sequence c1 , . . . , c(cid:96).
If few independent variables are of interest, the term (cid:80)
i∈I |Gi| could be less
than (cid:96), the number of nodes in the entire graph.

The formula above tells us that, for a given problem, the eﬃciency of
the subgraph method is not directly determined by sparsity of the Jacobian
matrix. What really matters is the amount of overlap between the sub-
graphs. The less overlap (smaller |Gi ∩ Gj|) the faster are the algorithms.
This is in contrast with the graph coloring approach for which the eﬃciency
is determined by the sparsity pattern of the Jacobian. We highlight these
diﬀerences by two simple examples:

Example 2.1 The Jacobian f (1)(x) for this example is a dense matrix. Let
A be a random n-by-n matrix and consider the matrix vector multiplication
function f (x) = Ax. The computational graph essentially consists of n2
multiply-add instructions. The size of the full graph is thus O(n2). The size
of the i-th subgraph is O(n) because yi is only aﬀected by row i of A. The
time to calculate the Jacobian by each of the methods using n reverse sweeps
is:

10

2 SPARSE JACOBIANS

2.6 Complexity of the Subgraph Methods

Bound
O(n3)
O(n3)

Method
n full sweeps
Coloring
Algorithm 2.3 O(n2 log(n))
Algorithm 2.4 O(n2)

Clearly, no method can be faster than O(n2) for this problem. Although
this example was chosen to show the beneﬁt of the subgraph methods, it
demonstrates that there exist non-sparse problems where Algorithm 2.4 is
asymptotically optimal.

Example 2.2 The Jacobian f (1)(x) for this example is the identity matrix
plus a matrix that is zero except for column n. Suppose that for k = 1, . . . , n,
φk is a unary function (in the sense of this paper) and

vn+k = φk(vn+k−1, vn+k−1) .

Recall that vn = xn and deﬁne f : Rn → Rn by

yk = v2n + xk ,

for k = 1, . . . , n. The computational graph of f and all its subgraphs are
of size O(n). The sparsity pattern of the Jacobian is {n} for the n-th row
and {n, k} for rows k = 1, . . . , n − 1. This Jacobian can be recovered using
a combination of one forward pass and one reverse pass. The asymptotic
complexity of the diﬀerent methods is:

Bound
O(n2)
O(n)

Method
n full sweeps
Coloring
Algorithm 2.3 O(n2 log(n))
Algorithm 2.4 O(n2)

The subgraph method is ineﬃcient for this problem because of the high sub-
graph overlap. The coloring approach is most eﬃcient for this example be-
cause of the special sparsity pattern.

The examples 2.1 and 2.2 are also useful for testing that implementations

of a subgraph algorithm scales as expected for various values of n.

11

3 Sparse Hessians

3 SPARSE HESSIANS

In this section we consider computing the sparsity pattern for the Hessian
of

m
(cid:88)

g(x) =

wifi(x) ,

(1)

i=1
where w ∈ Rm. We use I to denote the dependent variable indices for which
wi (cid:54)= 0; i.e.,

i ∈ I ⇔ i ∈ {1, . . . , m} and wi (cid:54)= 0 .

(2)

In addition, we use J to denote the subset of independent variables that
are of interest, J ⊂ {1, . . . , n}. Let {Xk} be the output corresponding
to Algorithm 2.1 with input J. Let {Yk} be the output corresponding to
Algorithm 2.2 with input I. It is not necessary to have the sequence of sets
{Yk}, just the reverse mode activity analysis Boolean sequence {dk} deﬁned
by

dk = (Yk (cid:54)= ∅) .

Here and below ∂ph(u) is the partial of h with respect to the p-th com-
ponent of its argument vector evaluated at u. We use ∂p,q to abbreviate
∂p∂q. We say that the elementary function φk is left nonlinear if ∂1,1φk(u)
is possibly non-zero for some u. It is right nonlinear if ∂2,2φk(u) is possibly
non-zero for some u. It is jointly nonlinear if ∂1,2φk(u) is possibly non-zero
for some u. We assume that ∂1,2φk(u) = ∂2,1φk(u).

3.1 Forward Mode Sparsity

Forward mode for function, derivative, and Hessian values starts with the
zero, ﬁrst, and second order values for the independent variables; i.e., xj,
˙xj, ¨xj for j = 1, . . . , n. It computes the zero, ﬁrst, and second order values
for the other variables using the following equations for k = n + 1, . . . , (cid:96):

vk = φk(va[k], vb[k])
˙vk = ∂1φk(va[k], vb[k]) ˙va[k] + ∂2φk(va[k], vb[k]) ˙vb[k]
¨vk = ∂1φk(va[k], vb[k])¨va[k] + ∂2φk(va[k], vb[k])¨vb[k]
a[k] + ∂2,2φk(va[k], vb[k]) ˙v2

+ ∂1,1φk(va[k], vb[k]) ˙v2
+ 2 ∂1,2φk(va[k], vb[k]) ˙va[k] ˙vb[k] .

b[k]

(3)

The forward mode Hessian sparsity calculation is deﬁned by Algorithm 3.1.

This is similar to the algorithm [Walther, 2008, Algorithm II]. One diﬀer-
ence is using {dk} to avoid the ‘dead end’ nodes mentioned in the reference

12

3 SPARSE HESSIANS

3.1 Forward Mode Sparsity

and nodes that are not included in the Hessian because the corresponding
wi is zero; see Eq 1 and Eq 2. This is probably the reason that the adolc
implementation has a larger nnz (more possibly non-zero values) than the
other implementations in Table 5 and Table 7.

The set Nj, in the algorithm, accumulates the nonlinear interactions
between the j-th independent variable and other independent variables. The

Algorithm 5 Forward Hessian Sparsity
{ N1 , . . . , Nn } = function(X1, d1, . . . , X(cid:96), d(cid:96))

for j = 1, . . . , n set Nj = ∅
for k = n + 1, . . . , (cid:96) if dk
if φk is left nonlinear

for j ∈ Xa[k] set Nj

if φk is right nonlinear

for j ∈ Xb[k] set Nj
if φk is jointly nonlinear
for j ∈ Xa[k] set Nj
for j ∈ Xb[k] set Nj

∪= Xa[k]

∪= Xb[k]

∪= Xb[k]
∪= Xa[k]

nonlinear interactions are initialized as empty. This corresponds to the
second order values for the independent variables being zero; i.e., 0 = ¨x ∈
R(cid:96). In the case where φk is jointly nonlinear and left nonlinear, the algorithm
[Walther, 2008, Algorithm II] uses the fact that Xk = Xa[k]∪Xb[k] to combine
two of the unions into one. A similar optimization is done for the case where
φk is jointly nonlinear and right nonlinear,

The sparsity pattern for the Hessian g(2)(x) is given by the observation

that for j ∈ J, p ∈ J, and x ∈ Rn,

[g(2)(x)]j,p (cid:54)= 0 ⇒ p ∈ Nj .

Given the second order forward mode equation for ¨vk in Eq 3, a proof for
this assertion would be similar to the proof for Algorithm 3.2. The Boolean
vector d has length (cid:96). There are (cid:96) sets Xk and at most |J| elements in each
of these sets. There are n sets Nj and at most |J| elements in each of these
sets.

13

3.2 Reverse Mode Sparsity

3 SPARSE HESSIANS

3.2 Reverse Mode Sparsity

The reverse mode Hessian sparsity calculation is deﬁned by Algorithm 3.2.
This is similar to the table [Griewank and Walther, 2008, Table 7.4], but
includes more general nonlinear binary functions; e.g., pow(x, y) = xy. In
addition, the algorithm and proof show how to extend the algorithm to
functions with more than two arguments.

Algorithm 6 Reverse Hessian Sparsity
{ M1 , . . . , M(cid:96) } = function(X1, d1, . . . , X(cid:96), d(cid:96))

for k = 1, . . . , (cid:96) set Mk = ∅
for k = (cid:96), . . . , n + 1 if dk

∪= Mk
∪= Mk

set Ma[k]
set Mb[k]
if φk is left nonlinear
∪= Xa[k]
∪= Xa[k]
if φk is right nonlinear
∪= Xb[k]
∪= Xb[k]

set Ma[k]
set Mb[k]

set Ma[k]
set Mb[k]

if φk is jointly nonlinear

set Ma[k]
set Mb[k]

∪= Xb[k]
∪= Xa[k]

As with Algorithm 3.1 when φk is both left nonlinear and jointly non-
linear (or right nonlinear and jointly nonlinear) two of the unions in Algo-
rithm 3.2 can be combined into one. We include a theorem and proof for
this algorithm below.

Theorem 3.1 For j ∈ J, p ∈ J, and x ∈ Rn,

[g(2)(x)]j,p (cid:54)= 0 ⇒ p ∈ Mj .

Proof : We deﬁne the sequence of scalar valued functions F(cid:96), . . . , Fn by

F(cid:96)(v1, . . . , v(cid:96)) =

m
(cid:88)

i=1

wiv(cid:96)−m+i ,

14

3 SPARSE HESSIANS

3.2 Reverse Mode Sparsity

and for k = (cid:96), . . . , n + 1,

Fk−1(v1, . . . , vk−1) = Fk[v1, . . . , vk−1, φk(va[k], vb[k])] .

The function Fn(x) is the same as g(x). Reverse mode computes the deriva-
tives Fk with respect to its arguments for k = (cid:96) − 1, . . . , n. The derivative
of Fn with respect to its arguments is equal to g(1)(x) and is the ﬁnal value
for ¯x = (¯v1, . . . , ¯vn) in the algorithm below. We use += for the operator that
sets the left hand side to the right hand side plus the previous value of the
left hand side.

for k = 1, . . . , (cid:96) − m set ¯vk = 0
for i = 1, . . . , m set ¯v(cid:96)−m+i = wi
for k = (cid:96), . . . , n + 1
set ¯va[k]
set ¯vb[k]

+= ∂1φk(va[k], vb[k])¯vk
+= ∂2φk(va[k], vb[k])¯vk

Diﬀerentiating the algorithm above with respect to x, and using the forward
mode equation for ˙vk in Eq 3, we obtain

for k = 1, . . . , (cid:96) set ˙¯vk = 0
for k = (cid:96), . . . , n + 1

comment diﬀerentiate setting of ¯va[k]
+= ∂1φk(va[k], vb[k]) ˙¯vk
set ˙¯va[k]
+= ∂1,1φk(va[k], vb[k])¯vk ˙va[k]
set ˙¯va[k]
+= ∂1,2φk(va[k], vb[k])¯vk ˙vb[k]
set ˙¯va[k]
comment diﬀerentiate setting of ¯vb[k]
+= ∂2φk(va[k], vb[k]) ˙¯vk
set ˙¯vb[k]
+= ∂1,2φk(va[k], vb[k])¯vk ˙va[k]
set ˙¯vb[k]
+= ∂2,2φk(va[k], vb[k])¯vk ˙vb[k]
set ˙¯vb[k]

Suppose that in Eq 3 ˙x is the j-th elementary vector. It follows that ˙¯xp =
∂p,jg(x) for p = 1, . . . n. We claim that at the beginning of the iteration k,
in the algorithm above and in Algorithm 3.2, for p = 1, . . . , k

˙¯vp (cid:54)= 0 ⇒ j ∈ Mp .

Proving this claim will complement the proof of the theorem. For the ﬁrst
iteration, k = (cid:96) and ˙¯vp = 0 for all p. Hence the claim is true for k = (cid:96).

15

3.2 Reverse Mode Sparsity

3 SPARSE HESSIANS

Suppose the claim is true at the beginning of the k-th iteration, it suﬃces
to show it is true at the beginning of iteration k − 1. If p (cid:54)= a[k] and p (cid:54)= b[k]
then ˙¯vp and Mp are the same at the beginning of iteration k and k − 1, so
we are done. The two cases p = a[k] and p = b[k] are symmetric. It suﬃces
to show the case p = a[k]; i.e., at the end of iteration k

˙¯va[k] (cid:54)= 0 ⇒ j ∈ Ma[k] .

If ˙¯va[k] (cid:54)= 0 at the beginning of iteration k then by induction j ∈ Ma[k] at
the beginning of iteration k and by Algorithm 3.2 it also true at the end of
iteration k − 1.

Consider the remaining case where ˙¯va[k] = 0 at the beginning of iteration
k and ˙¯va[k] (cid:54)= 0 at the end of iteration k. This implies that the right hand
side was non-zero in one of the three assignments to ˙¯va[k] above. This in
turn implies that dk is true (otherwise ¯vk and ˙¯vk would be zero). Suppose
the ﬁrst assignment to ˙¯va[k] is non-zero,

0 (cid:54)= ∂1φk(va[k], vb[k]) ˙¯vk .

This implies that ˙¯vk (cid:54)= 0 which, by induction, implies that j ∈ Mk which, by
Algorithm 3.2, implies j ∈ Ma[k] at the end of iteration k. This completes
the case where the ﬁrst assignment to ˙¯va[k] is non-zero.
Suppose the second assignment to ˙¯va[k] is non-zero,

0 (cid:54)= ∂1,1φk(va[k], vb[k])¯vk ˙va[k] .

This implies that all three terms in the product on the right hand side are
non-zero. Hence ˙va[k] = ∂jva[k](x) is non-zero and j ∈ Xa[k]. Furthermore
φk is left nonlinear. Hence, at the end of iteration k, Xa[k] ⊂ Ma[k] and
j ∈ Ma[k] This completes the case where the second assignment to ˙¯va[k] is
non-zero.

Suppose the third assignment to ˙¯va[k] is non-zero,

0 (cid:54)= ∂1,2φk(va[k], vb[k])¯vk ˙vb[k] .

This implies that all three terms in the product on the right hand side are
non-zero. Hence ˙vb[k] = ∂jvb[k](x) is non-zero and j ∈ Xb[k]. Hence, at the
end of iteration k, Xb[k] ⊂ Ma[k] and j ∈ Ma[k]. This completes the case
where the third assignment to ˙¯va[k] is non-zero. Q.E.D.

Remark 3.1 The edge pushing algorithm [Gower and Mello, 2014, Algo-
rithm 3.1] is an alternative reverse mode algorithm for computing Hessian

16

4 EXPERIMENTS

3.3 Subgraph Sparsity

sparsity patterns. It is fundamentally diﬀerent from the reverse mode Hes-
sian sparsity algorithm in this paper because it does not use ﬁrst order for-
ward mode sparsity patterns to reduce the computed index set sizes to the
number of independent variables that are of interest.

3.3 Subgraph Sparsity

We are given a function f : Rn → Rm and deﬁne g : Rn → R by

g(x) =

m
(cid:88)

i=1

wifi(x) .

Using reverse mode we can compute g(1)(x) in O((cid:96)) operations.
In addi-
tion we can obtain the corresponding computational graph; for example see
[Kristensen et al., 2016, Tape T2 on pp. 7], [Wang et al., 2016, Section 3.1].
We use this computational graph to deﬁne the function h : Rn → Rn

h(x) = g(1)(x) =

m
(cid:88)

i=1

wif (1)
i

(x) .

The Jacobian of h(x) is the Hessian of g(x). We can apply the reverse
subgraph Jacobian algorithm to the computational graph for h(x) to obtain
the sparsity pattern for the Hessian g(2)(x).

3.4 Computing Hessians

Once a sparsity pattern for a Hessian is available, the values in the Hessian
are computed in a manner similar to how the Jacobians are calculated; see
Section 2.5.

4 Experiments

The speed tests reported below were run using the following hardware and
software:

17

4.1 CSV File

4 EXPERIMENTS

processor:
operating system:
Linux kernel:
compiler:
memory:

i5-3470 3.2GHz 64bit
fedora 34
5.13.6
clang 12.0.0
8GB

disk: ST500DM002-1BD14

ADOL-C [Griewank et al., 1999]:
CppAD [Bell, 2020]:
ColPack [Gebremedhin et al., 2013]:

git hash 25a69c4
git hash 5c86b18
version v1.0.10

The ADOL-C git hash corresponds to its master branch on 2020-03-31,

the CppAD git hash corresponds to its master branch on 2020-05-23.

4.1 CSV File

Each run of the speed test program adds a row to a csv ﬁle with the following
columns:

KB : This is the average memory usage, in kilobytes, where each kilobyte
is 1000 bytes (not 1024 bytes). A separate run of the speed test program by
valgrind’s massif tool is used to determine these values.

implement:

adolc A forward or reverse algorithm implemented by the ADOL-C pack-

age

cppad A forward or reverse algorithm implemented by the CppAD pack-

age

subgraph A CppAD implementation of Algorithm 2.3. Note that the poten-
tially faster Algorithm 2.4 is not implemented in CppAD and therefore
not part of the test.

problem: This is the minpack2 test source code used to compute f (x);
see [Averick et al., 1992]. The source code was converted to C using f2c and
then to C++ so it could be used with ADOL-C and CppAD. The available
problem values are:

dficfj The ﬂow in a channel Jacobian problem

dierfj The incompressible elastic rod Jacobian problem

deptfg The elastic-plastic torsion Hessian problem

18

4 EXPERIMENTS

4.1 CSV File

dgl1fg One Dimensional Ginzburg-Landau Hessian problem

colpack : If true, the ColPack package was used to solve the coloring sub-
problem. Otherwise a greedy distance two coloring algorithm (inspired by
the algorithm [Gebremedhin et al., 2005, Algorithm 3.2] and implemented
in the CppAD package) is used. In either case, if this is a Hessian problem, a
special version of the coloring algorithm that takes advantage of the symme-
try, is used. The colpack option must be true when implement is adolc. It
must be false when implement is subgraph because it does not use coloring.

indirect: If this is true, an indirect method was used to get more com-
pression out of the coloring problem. It can only be true when implement is
adolc and problem corresponds to computing a Hessian. This must be false
when implement is cppad because it does not support this recovery option.
It must also be false when implement is subgraph because it does not use a
compression technique.

optimize: Was the computation graph was optimized. This must be false

when implement is adolc because it does not have this option.

setup: Does the sec result include the setup time; e.g., recording the
graph, computing sparsity, coloring, compression, and optimization of the
graph.
If the computational graph (see Section 1.1) corresponding to a
function does not depend on the argument to the function, the setup opera-
tions can be preformed once and re-used for any argument value. Note that
optimization is not included in the setup time when implement is adolc.
Coloring and compression are not included when implement is subgraph.

reverse: If true (false) reverse mode (forward mode) was used for com-
puting both the sparsity pattern and derivative values. This option must be
true when implement is subgraph.

onepass: If true, the derivative values were computed using one pass of
graph with multiple directions at the same time. Otherwise, each directional
derivative is computed using a separate pass. This option must be false when
implement is subgraph. The onepass option must be true when implement
is adolc and reverse is false.

n: This is the size of the domain space for f (x).

m: This is the size of the range space for f (x). If m = 1 (m > 1), this

is a Hessian (Jacobian) problem.

nnz : This is the number of possibly non-zero values in sparse matrix
calculated by the implementation. For a Jacobian problem, the matrix is

19

4.2 Result Tables

4 EXPERIMENTS

the entire Jacobian. For a Hessian problem, the matrix is the upper triangle
of the Hessian. Note that one implementation may compute a more eﬃcient
sparsity pattern than another (have fewer possibly non-zero values).

sec: This is the number of seconds for each calculation of the sparse

Jacobian or Hessian.

4.2 Result Tables

There is one table for each problem and value for setup. For each implement,
only the combination of options that result in the smallest sec (smallest time
and fastest execution) is included in the table. Given this selection, options
that are the same for all the implementations in a table are reported in the
caption at the top of the table. The other options are reported in each row
of the table. The rows for each table are sorted so that the sec column is
monotone non-decreasing.

4.2.1 Jacobian Without Setup

Table 1:
m=3200, nnz =24787

problem=dficfj,

setup=false,

indirect=false, n=3200,

KB implement
adolc
cppad
subgraph

12108
13361
6480

colpack
true
true
false

optimize
false
true
true

reverse
false
false
true

onepass
true
true
false

sec
0.00186
0.00233
0.00586

Table 2:
m=3003, nnz =31600

problem=dierfj,

setup=false,

indirect=false, n=3003,

KB implement
adolc
cppad
subgraph

11561
5797
4049

colpack
true
true
false

optimize
false
true
true

reverse
false
false
true

onepass
true
true
false

sec
0.0015
0.00182
0.00411

Table 1 (Table 2) compares the time, sec, and memory, KB, required to
compute the Jacobian where problem is dficfj (dierfj) and setup is false.
The adolc and cppad implementations use the same algorithms, hence their
similar run times is not surprising. The subgraph implementation takes

20

4 EXPERIMENTS

4.2 Result Tables

about twice as long.
(The subgraph implementation does not require a
sparsity calculation or solving a coloring sub-problem and these tasks are
part of the setup for the adolc and cppad implementations.) The number
of possibly non-zeros in the Jacobian sparsity pattern, nnz, depends on the
problem but does not depend on the implementation; i.e., the sparsity pat-
tern eﬃciency is the same for all the implementations. The fastest option
choices for the cppad implementation are optimize true, reverse false, and
onepass true. Using colpack true with the cppad implementation is faster
for one problem and slower for the other. The indirect true option is not
available for Jacobian problems. The onepass true option results in more
memory usage but we have not included those details in the result tables.
The subgraph implementation uses less memory than the other implemen-
tations.

4.2.2 Jacobian With Setup

Table 3: problem=dficfj, setup=true, indirect=false, optimize=false,
n=3200, m=3200, nnz =24787

KB implement
adolc
cppad
subgraph

9807
18430
8859

colpack
true
true
false

reverse
false
false
true

onepass
true
true
false

sec
0.0109
0.013
0.0141

Table 4: problem=dierfj, setup=true, indirect=false, optimize=false,
n=3003, m=3003, nnz =31600

KB implement
adolc
subgraph
cppad

9585
5151
13645

colpack
true
false
false

reverse
false
true
false

onepass
true
false
true

sec
0.00862
0.00887
0.0116

Table 3 (Table 4) is similar to Table 1 (Table 2) with the diﬀerence
being that setup is true instead of false. The time for the adolc, cppad and
subgraph implementations are all close. The fastest option choices for the
cppad implementation are optimize false, reverse false, and onepass true.
Using colpack true with the cppad implementation is faster for one problem
and slower for the other. The subgraph implementation uses less memory

21

4.2 Result Tables

4 EXPERIMENTS

than the other implementations.

4.2.3 Hessian Without Setup

Table 5: problem=deptfg, setup=false, onepass=false, n=3600, m=1

KB implement
subgraph
cppad
adolc

5207
4364
15115

colpack
false
true
true

indirect
false
false
true

optimize
true
true
false

reverse
true
false
false

nnz
10680
10680
14161

sec
0.00499
0.00523
0.0226

Table 6: problem=dgl1fg, setup=false, onepass=false, n=5000, m=1,
nnz =10000

KB implement
cppad
subgraph
adolc

6086
9806
16679

colpack
true
false
true

indirect
false
false
true

optimize
true
true
false

reverse
true
true
false

sec
0.00535
0.00982
0.0243

Table 5 (Table 6) compares the time, sec, and and memory, KB, required
to compute the Hessian where problem is deptfg (dgl1fg) and setup is
false. The cppad and subgraph implementations have similar times and the
adolc implementation takes twice as long. The number of possibly non-
zeros in the adolc implementation for the deptfg problem is signiﬁcantly
larger than in the other implementations; i.e., its sparsity pattern is not as
eﬃcient as the other implementations. The fastest option choices for the
cppad implementation are colpack true, optimize true, and reverse true.
The indirect true option is not available with the cppad implementation.
No implementation uses less memory for both problems.

4.2.4 Hessian With Setup

Table 7 (Table 8) is similar to Table 5 (Table 6) with the diﬀerence being that
setup is true instead of false. The subgraph implementation is the fastest for
this case and the cppad implementation is signiﬁcantly slower. The fastest
option choices for the cppad implementation are colpack true, optimize false,
and reverse true. The adolc implementation uses less memory than the
other implementations.

22

5 CONCLUSIONS

Table 7: problem=deptfg, setup=true, onepass=false, n=3600, m=1

KB implement
subgraph
adolc
cppad

7809
15299
4921

colpack
false
true
true

indirect
false
true
false

optimize
true
false
true

reverse
true
false
false

nnz
10680
14161
10680

sec
0.0258
0.0453
0.0843

Table 8: problem=dgl1fg, setup=true, onepass=false, n=5000, m=1,
nnz =10000

KB implement
subgraph
cppad
adolc

33709
7917
19144

colpack
false
true
true

indirect
false
false
true

optimize
false
true
false

reverse
true
false
false

sec
0.0603
0.111
0.114

4.2.5 Reproducing Results

The source code that produced the results corresponds to the tag 20210803
of the following git repository:

https://github.com/bradbell/sparse_ad .

5 Conclusions

If the computational graph (see Section 1.1) corresponding to a function does
not depend on the argument to the function, the setup can be done once
and reused many times. In the other case, when the setup operations are
computed for each argument to a sparse Jacobian, all the implementation
have similar timing results. When computing the setup operations for each
argument to a sparse Hessian, the subgraph implementation is signiﬁcantly
faster and cppad is signiﬁcantly slower.

Further testing is called for. We have provided a GitHub repository with
the source code used to obtain the results in this paper. This facilitates re-
production of the results as well as extension of the tests to other cases.
Other cases include diﬀerent computer hardware, operating systems, com-
pilers, and versions of the AD packages. The tests can also be extended to
other AD packages as well as other problems and problem sizes. The tests
also provide example source code that implements the algorithms presented
in this paper.

23

REFERENCES

References

REFERENCES

[Averick et al., 1992] Averick, B. M., Carter, R. G., Mor´e, J. J., and Xue,
G.-L. (1992). The MINPACK-2 test problem collection. Technical Report
Preprint MCS-P153-0692, Mathematics and Computer Science Division,
Argonne National Laboratory.

[Bell, 2020] Bell, B. M. (2020). CppAD: a package for C++ algorithmic dif-
ferentiation. Computational infrastructure for operations research: coin-
or.

[Coleman and Verma, 1998] Coleman, T. F. and Verma, A. (1998). The
eﬃcient computation of sparse jacobian matrices using automatic diﬀer-
entiation. SIAM Journal on Scientiﬁc Computing, 19(4):1210–1233.

[Gebremedhin et al., 2013] Gebremedhin, A., Nguyen, D., Patwary, M.
M. A., and Pothen, A. (2013). Colpack: Software for graph coloring and
related problems in scientiﬁc computing. ACM Transactions on Mathe-
matical Software (TOMS), 40:1–31.

[Gebremedhin et al., 2005] Gebremedhin, A. H., Manne, F., and Pothen,
A. (2005). What color is your jacobian? graph coloring for computing
derivatives. SIAM Review, 47(4):629–705.

[Gower and Mello, 2014] Gower, R. M. and Mello, M. P. (2014). Computing
the sparsity pattern of Hessians using automatic diﬀerentiation. ACM
Transactions on Mathematical Software, 40(2):10:1–10:15.

[Griewank et al., 1999] Griewank, A., Juedes, D., Mitev, H., Utke, J., Vo-
gel, O., and Walther, A. (1999). ADOL-C: A package for the automatic
diﬀerentiation of algorithms written in C/C++. Technical report, In-
stitute of Scientiﬁc Computing, Technical University Dresden. Updated
version of the paper published in ACM Trans. Math. Software 22, 1996,
131–167.

[Griewank and Walther, 2008] Griewank, A. and Walther, A. (2008). Eval-
uating Derivatives: Principles and Techniques of Algorithmic Diﬀerenti-
ation. SIAM, Philadelphia, PA, 2nd edition.

[Kristensen et al., 2016] Kristensen, K., Nielsen, A., Berg, C. W., Skaug,
H., and Bell, B. M. (2016). TMB: Automatic diﬀerentiation and Laplace
approximation. Journal of Statistical Software, 70(5):1–21.

24

REFERENCES

REFERENCES

[Petra et al., 2018] Petra, C. G., Qiang, F., Lubin, M., and Huchette, J.
(2018). On eﬃcient hessian computation using the edge pushing algorithm
in julia. Optimization Methods and Software, 33(4–6):1010–1029.

[Walther, 2008] Walther, A. (2008). Computing sparse hessians with au-
tomatic diﬀerentiation. ACM Transactions on Mathematical Software,
34(1).

[Wang et al., 2016] Wang, M., Pothen, A., and Hovland, P. (2016). Edge
pushing is equivalent to vertex elimination for computing Hessians.
In
Gebremedhin, A., Boman, E., and Ucar, B., editors, 2016 Proceedings
of the Seventh SIAM Workshop on Combinatorial Scientiﬁc Computing,
Albuquerque, New Mexico, USA, pages 102–111, Philadelphia, PA, USA.
SIAM.

25

