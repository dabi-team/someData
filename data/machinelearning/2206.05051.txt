2
2
0
2

n
u
J

9

]

G
L
.
s
c
[

1
v
1
5
0
5
0
.
6
0
2
2
:
v
i
X
r
a

Temporal Inductive Logic Reasoning

Yuan Yang, Siheng Xiong, James C Kerce and Faramarz Fekri
Georgia Institute of Technology
{yyang754@,sxiong45@}gatech.edu
{clayton.kerce@gtri.,faramarz.fekri@ece.}gatech.edu

Abstract

Inductive logic reasoning is one of the fundamental tasks on graphs, which seeks
to generalize patterns from the data. This task has been studied extensively for
traditional graph datasets such as knowledge graphs (KGs), with representatitve
techniques such as inductive logic programming (ILP). Existing ILP methods
typically assume learning from KGs with static facts and binary relations. Beyond
KGs, graph structures are widely present in other applications such as video
instructions, scene graphs and program executions. While inductive logic reasoning
is also beneﬁcial for these applications, applying ILP to the corresponding graphs
is nontrivial: they are more complex than KGs, which usually involve timestamps
and n-ary relations, effectively a type of hypergraph with temporal events.
In this work, we study two of such applications and propose to represent them
as hypergraphs with time intervals. To reason on this graph, we propose the
multi-start random B-walk that traverses this hypergraph. Combining it with a path-
consistency algorithm, we propose an efﬁcient backward-chaining ILP method that
learns logic rules by generalizing from both the temporal and the relational data.

1

Introduction

The task of inductive reasoning concerns generalizing concepts or patterns from data. This task is
studied extensively in knowledge graphs (KG)s where techniques such as inductive logic programming
(ILP) are proposed. A typical knowledge graph such as FB15K [Toutanova and Chen, 2015] and
WN18 [Bordes et al., 2013] represents commonsense knowledge as a set of nodes and edges, where
entities are the nodes and the facts are represented as the edges that connect the entities. For example,
Father(Bob, Amy) is a fact stating that “Bob is the Father of Amy”; this is represented as an edge
of type Father connecting the two entities Bob and Amy. Many ILP techniques are proposed to
reason on the KGs that learn ﬁrst-order logic rules from the graph. Learning these rules is beneﬁcial
as they are interpretable and data efﬁcient [Yang and Song, 2020].

Apart from KGs, the graph is widely used in other applications to represent structured data, such as
temporal events that happened among a set of entities [Boschee et al., 2015, Leetaru and Schrodt,
2013]; the cooking instruction of a video (Figure 1); the abstract syntax tree (AST) of a program;
and a scene graph from autonomous driving sensors [Caesar et al., 2020]. Learning explicit rules
from these graphs is also beneﬁcial. However, these graphs are more complex than standard graphs,
and therefore, existing ILP methods are not readily applicable. Speciﬁcally, (1) for temporal data
such as video, the facts or events are labeled with time intervals indicating their start and end times.
While some temporal KGs such as ICEWS [Boschee et al., 2015] also have time labels, each event
only is labeled with a single time point. This is less expressive than a time interval because it cannot
characterize the temporal relations of events with duration. For example, “cook the soup while cutting
the lettuce”. (2) Many events require n-ary relations, for example “mixing onion, garlic, and oil
together”. Such a relation corresponds to an edge that connects to more than two nodes, and a graph
with such edges is a hypergraph. While it is possible to convert hypergraph to graph with various

Preprint. Under review.

 
 
 
 
 
 
Figure 1: Representing a video instruction of making spaghetti & meatball with temporal hypergraph.

techniques such as clique expansion, the conversions are lossy and lead to an exponential number of
edges.

In this work, we aim to extend the traditional ILP methods to hypergraphs whose events are labeled
with time intervals. In the following section, we ﬁrst formally deﬁne this representation, namely
temporal hypergraph. Then, we discuss the random walk (RW) algorithm on a hypergraph, as RW is
the fundamental mechanism of a family of widely used ILP methods, that is the backward-chaining
methods. We revisit the notion of B-connectivity on hypergraphs and propose the multi-start random
B-walk algorithm that explores the hypergraph given a set of starting points. To generalize the
temporal relation, we incorporate Allen’s interval algebra [Allen, 1983] and characterize events with
the interval operators such as Before and After. We learn temporal relations by resolving the time
constraints with a constraint propagation algorithm modiﬁed from the path consistency algorithm.

We summarize our contributions as follows:

• We propose temporal hypergraph, a graph that supports n-ary relations and temporal events
with time intervals. We showcase that this is a natural and expressive representation for
many applications.

• We propose a backward-chaining ILP method for the temporal hypergraph, which learns
to generalize on both temporal and relational data. This is realized with a novel random
B-walk algorithm and a time constraint propagation algorithm.

• We showcase our framework by creating two temporal hypergraph datasets from the
YouCook2 cooking recipe dataset [Zhou et al., 2018] and nuScenes autonomous driv-
ing dataset [Caesar et al., 2020] and demonstrating the effectiveness of the proposed ILP
method.

To the best of our knowledge, the temporal inductive logic reasoning (TILR) is the ﬁrst framework
that addresses the ILP problem on hypergraphs with time interval labels.

2 Related Work

ILP methods. Many ILP methods are proposed for inductive logic reasoning on graphs. These
methods are categorized into two types. Forward-chaining methods [Galárraga et al., 2015, Evans and
Grefenstette, 2018, Payani and Fekri, 2019] learn by searching in the rule space. It supports inferring
difﬁcult tasks but suffers from exponential complexity and does not scale to large graphs. On the
other hand, backward-chaining methods [Campero et al., 2018, Yang and Song, 2020, Yang et al.,
2017] learn rules by searching in the graph space, while the rule is usually limited to a chain-like

2

0:180:341:031:332:123:160:00Spaghetti & Meatball“Put one jar of spaghetti sauce in the pan and let it simmer” “Sauté chopped onion  and garlic with oil”“Add the beef, breadcrumbs, parmesan, … to the bowl and mix them together”“Mix the meatballs and sauce in the spaghetti”“Make small balls of beef and fry them in vegetable oil and olive oil in a pan”𝑆𝑎𝑢𝑡é(𝑂𝑛𝑖𝑜𝑛,𝐺𝑎𝑟𝑙𝑖𝑐,𝑂𝑖𝑙)𝑀𝑖𝑥𝑆𝑎𝑢𝑐𝑒,𝑂𝑖𝑙∧𝑆𝑖𝑚𝑚𝑒𝑟(𝑆𝑎𝑢𝑐𝑒)𝑀𝑖𝑥(𝐵𝑒𝑒𝑓,𝐵𝑟𝑒𝑎𝑑𝑐𝑟𝑢𝑚𝑏𝑠,𝑃𝑎𝑟𝑚𝑒𝑠𝑎𝑛,…)𝑀𝑎𝑘𝑒𝐵𝑒𝑒𝑓,𝐵𝑎𝑙𝑙𝑠∧𝐹𝑟𝑦(𝐵𝑎𝑙𝑙𝑠,𝑂𝑖𝑙)𝑀𝑖𝑥(𝐵𝑎𝑙𝑙𝑠,𝑆𝑎𝑢𝑐𝑒,𝑆𝑝𝑎𝑔ℎ𝑒𝑡𝑡𝑖)OnionGarlicOilSauceBeefBread-crumbs…OilSpaghetti𝑆𝑎𝑢𝑡é𝑀𝑖𝑥𝑆𝑖𝑚𝑚𝑒𝑟𝑀𝑖𝑥𝑀𝑎𝑘𝑒𝐹𝑟𝑦𝑀𝑖𝑥Hypergraphpath in the graph, this leads to better scalability. For temporal hypergraphs, none of the existing ILP
methods are readily applicable. In this work, we propose the ILP method for temporal hypergraphs
under the backward-chaining paradigm.

Temporal reasoning There are many ways for reasoning on temporal data. Some temporal KGs
have events with single time points. And some recent methods such as TLogic [Liu et al., 2022]
were proposed for solving ILP on these graphs. However, time point algebra is limited as it cannot
represent temporal relations of events with duration. A more expressive representation is proposed in
time interval algebra with Allen’s interval algebra [Allen, 1983] being the representative schema. In
this work, we show how to incorporate the algebra for learning temporal relations.

3 Temporal Hypergraph and ILP

For an ordinary graph, is can be represented by a set of triples of the form P (xh, xt), where P is the
predicate and xh and xt are the head and tail entities. To represent more complex structures with
n-ary relations (i.e., relations that have arity greater than 2) and timestamps. We need to introduce the
temporal hypergraph.

A temporal hypergraph consists of the following components: Entities: x ∈ X is the unique entity
in the temporal data. Deﬁnitions. Predicate: P ∈ P is the temporal predicate. Event: an event is
deﬁned as e = P (xh, xt, t), where xh, xt are the head entities and tail entities1, and t = (cid:104)ts, te(cid:105)
consists of the start and end time of an event. Therefore, a temporal hypergraph is deﬁned as
G = (cid:104)X , P, E(cid:105), where E = {e1, ..., en} is a collection of events.

3.1 First-Order Logic

Given a graph, one can express knowledge as ﬁrst-order logic (FOL) rules. A FOL rule consists of
(i) a set of predicates deﬁned in P, (ii) a set of logical variables such as X and Y , and (iii) logical
operations {∧, ∨, ¬}. For example,

(1)

GrandFatherOf(X, X (cid:48)) ←FatherOf(X, Y )∧
MotherOf(Y, X (cid:48))
involves predicates GrandFatherOf, FatherOf and MotherOf.
Components such as
FatherOf(X, Y ) are called atoms which correspond to the predicates that apply to the logical
variables. Each atom can be seen as a lambda function with its logical variables as input. This
function can be evaluated by instantiating the logical variables such as X into the object in X . For
example, let X = {Amy, Bob, Charlie}, we can evaluate FatherOf(Bob/X, Amy/Y ) by instantiat-
ing X and Y into Bob and Amy respectively. This yields 1 (i.e. True) if “Bob is the father of Amy”.
The outputs of all atoms are combined using logical operations {∧, ∨, ¬} and the imply operation
p ← q is equivalent to p ∨ ¬q. Thus, when all variables are instantiated, the rule will produce an
output as the speciﬁed combinations of those from the atoms. By using the logical variables, the
rule encodes the “lifted” knowledge that does not depend on the speciﬁc data. Such representation is
beneﬁcial because (i) the rules are highly interpretable and can be translated into natural language for
human assessment, and (ii) the rules guarantee to generalize to many examples.

3.2 Temporal Inductive Logic Reasoning

The Temporal Inductive Logic Reasoning (TILR) concerns solving the ILP problem for temporal
data. Speciﬁcally, given a set of positive and negative queries Q+ and Q−, TILR learns the logic
rules that predict (or entail) positive ones and do not predict the negative ones.

For example, one wants to summarize a recipe of making Spaghetti & Meatball from a set of
videos video1, ..., videon similar to Figure 1. Let G1, ..., Gn be the temporal graphs of the videos,
then, this tasks corresponds to learning a logic rule that successfully predict the positive video
labels Q+ = {S&M (video1), ..., S&M (videon)} and does not predict the negative video labels
Q− = {S&M (videoi)|i (cid:54)= 1, ..., n}.

Similarly, one can perform standard deductive reasoning on the graph, that is, link prediction or graph
completion. In this case, positive queries consist of a set of events Q+ = {e1, ..., en} (or triples, if G

1Note that in some work such as [Gallo et al., 1993], head and tail are used in a reversed manner

3

Algorithm 1: Multi-start Random B-walk
Init: rule queue q = {s}, edge counter ∀e ∈ E, k[e] = 0, probability

∀x ∈ X , t = 1...T, p[x][t] = 0, length t = 0

while q is not empty do
x = q.dequeue()
t+ = 1
Child probabilities childP rob = []
for e ∈ U nvisitedEdgeF rom(x) do

k[e].add(x)
// visit e if all tail vertices of e are visited
if k[e] == T (e) then
Mark e as visited
x(cid:48) = H(e) // because it’s B-Graph
1
p[x(cid:48)][t]+ =
q.enqueue(x(cid:48))
childP rob.append(p[x(cid:48)][t])

OutDegree(T (e)) minx(cid:48)(cid:48)∈T (e) p[x(cid:48)(cid:48)][t − 1]

end

end

end

is a traditional KG), and the negative queries consist of another set of events. The task is to learn a
set of rules that predict positive events and not negative events.

There are many ways to solve ILP. One can learn a logic program consisting of multiple rules, where
by applying them repeatedly to the graph, the positive queries will be entailed (and the opposite for
the negative ones). This is referred to as forward-chaining method [Campero et al., 2018, Evans and
Grefenstette, 2018]. On the other hand, one can solve each query independently, where a logic rule
is proposed to predict a given query. This is referred to as backward-chaining method [Yang et al.,
2017, Yang and Song, 2020]. In other words, one learns a function f (q; G) that maps a query q into a
logic rule. In this work, we consider the backward-chaining approach.

Deﬁnition 3.1 (Temporal Inductive Logic Reasoning) Given a temporal hypergraph G, and a set
of positive and negative queries Q+ and Q−, ﬁnd a model f (q; G) that maps the query q into a logic
rule R such that

R(q) = 1[q ∈ Q+], for q ∈ Q+ ∪ Q−.

4 Proposed Method

Graph random walk is employed in many backward-chaining methods on traditional KG. Given a
triple query P (xh, xt), one searches a path from xh to xt, which can be represented as a chain-like
rule

P (xh, xt) ← P (1)(xh, x(1)) ∧ P (2)(x(1), x(2)) ∧ ... ∧ P (n)(x(n), xt).

For temporal hypergraph with events P (xh, xt, t), we need to address two challenges: 1) How to
perform random walk on hypergraph?; and 2) How to incorporate the temporal relation while walking
on the graph? We discuss these issues in this section.

4.1 Reasoning via multi-start random B-walk

Many random walk techniques have been proposed for hypergraphs. For In [Chitra and Raphael,
2019], a ﬁxed weight RW method is proposed. In [Chitra and Raphael, 2019], an edge-dependent
vertex weight RW is proposed. There are also RW with nonlinear operators proposed to preserve
properties of the RW on ordinary graphs[Chan et al., 2018, Li et al., 2020]. However, none of the
existing work is designed for ILP and concerns the important property, that is, the B-connectivity. In
this work, we propose a novel RW method namely, the multi-start random B-walk (MRBW).

4

B-connectivity Consider a n-ary relation in the recipe example in Figure 1, that is, Mix(xh, xt, t),
where xh denotes the ingredients and xt denotes the mixed output (therefore, in this case, dim(xt) =
1). Let x(t) be the set of nodes that are reached at (t)th step in a walk. Then, it is natural to say that
“x(t) is connected to xt by Mix if and only if xh ⊆ x(t)”. In other words, the tail nodes cannot be
reached until all the head nodes are reached.

This property is referred to as B-connectivity [Gallo et al., 1993]. Formally, let P (xh, xt, t) be a
hyperedge of predicate P . Then, a hyperedge is a B-edge if dim(xh) ≥ 1 and dim(xt) = 1. In other
words, P is a many-to-one relation. And a path is B-connected if all of its B-edges are B-connected.
A hypergraph consisting of only B-edges is called a B-graph. In this work, we consider the random
walk on B-graphs.

Multi-start Random B-walk. For the traditional random walk, one starts from a node xh and
randomly transits to the tail nodes with equal probability. In MRBW, we consider starting from a set
of nodes, and random walking on the graph while maintaining the B-connectivity.

Let x(t−1) be the nodes reached at (t − 1)th step and X(t−1) be the union of nodes from (0) to (t − 1),
and P (t) be the type of hyperedge by which transition is performed at (t)th step. For ∀x(t)
t ⊆ X that
are reachable at (t)th step We have

p(x(t)

t ) =

(cid:88)

xh∈X(t−1)

min
xh∈xh

1
OutDegree(xh)

p(x(t−1)
h

)

4.2 Temporal relations

So far we have been discussing how to address the random walk with n-ary relations. Here, we
consider how to integrate the temporal relations introduced in [Allen, 1983] into the random walk,
e.g., BEFORE, EQUAL, MEETS and etc. Instead of treating temporal relations as a separate operation
from conjunction ∧. we consider them as composite conjunction of the following form
BIN (t, t(cid:48))

P (xh, xt, t) BIN P (cid:48)(x(cid:48)

P (xh, xt) ∧ P (cid:48) (x(cid:48)

∧

(cid:17)

(cid:17)

(cid:16)

(cid:16)

h, x(cid:48)

t, t(cid:48)) =

h, x(cid:48)
t)

,

where BIN ∈ {BEFORE, EQUAL, MEETS, ...}. In order to generalize the temporal relations from the
data, one keeps track of the set of applicable temporal relations for each pair of the events in the
rule. Whenever a positive query is matched by the rule, one updates the temporal relations to
satisfy the time constraints posed in the corresponding subgraph of the query. This is the traditional
path-consistency (PC) algorithm introduced in [Allen, 1983]. In this work, we incorporate the PC
algorithm and propose the modiﬁed PC for MRBW. The algorithm is shown Algorithm 2. Here,
ResolveT ime is the standard PC algorithm. Intuitively, the modiﬁed PC algorithm resolves time
constraints whenever the two random walk paths merges together. This way, one does not need to
update the relations at each step of the random walk, because the events within each path is already
ordered.

5 Learning

By applying the MRBW and the PC algorithm, one can develop many ILP methods that learn temporal
logic rules from the data. Here, we propose a simplistic approach that follows the setup introduced
in [Lao et al., 2011] that learns a linear model to weigh the paths in answering the query.

Let R1, ..., Rn be the n candidate rules for reasoning. We deﬁne a linear model that predicts the score
of answering the query q = e = P (xh, xt, t), that is

fθ(e; G) =

(cid:88)

i

θiRi(xh, xt, t),

where θ1, ..., θn are the learnable parameters. Then the parameters can be learned by optimizing a
cross-entropy loss

L(θ; G) =

N
(cid:88)

j

yilogfθ(ei; G) + (1 − yi)log(1 − fθ(ei; G)),

where N is the number of training queries, and yi ∈ 0, 1 is the binary label indicating if the query is
positive.

5

Algorithm 2: Path-consistency for temporal relation generalization.
Init: Path set P = {x1, x2, ...}, T imeOP [pi][pj] = 0, ∀pi, pj ∈ G, max search depth D
while maxLen(P ) < D do
Xheads = HeadsOf (P )
e ∼ U nvisitedEdgeF rom(Xheads)
if e is B-connected then

P (cid:48) ← P athsEndW ith(HeadsOf (e))
for {pi, pj} ∈ P ‘ × P (cid:48) do

T imeOP [pi][pj] = ResolveT ime(pi, pj)

end
for pi ∈ P ‘ do

T imeOP [e][pj] = ResolveT ime(e, pj)

end
p∗ ← BuildP ath(e, P (cid:48), T imeOP )
P/P ‘; P ∪ {p∗}

end
return P

end

6 Experiment

6.1 Recipe summarization

YouCook2 cooking recipe dataset [Zhou et al., 2018] consists of instructional videos of 89 cooking
recipes such as spaghetti & meatballs. Each recipe has 22 videos and each video is annotated by a
sequence of natural language sentences that describe the procedure steps, as shown in Figure 1.

The dataset serves as a challenging benchmark for evaluating the generalizability of both temporal
and relational data. To see this, instructions belonging to the same class can have different procedures
and varied ingredients. For example, one can make BLT sandwich by ﬁrst putting the lettuce on the
bread and then the ham, or in the reserve order. This requires the ILP method to learn generalized
temporal relations for the temporal events. On the other hand, instructions can differ by the actual
steps and ingredients used. For example, some instructions of BLT sandwich involve mayonnaise
while others do not. This requires the ILP method to learn rules that generalize these patterns.

Table 1: Statistics of the temporal hypergraph benchmark based on the YouCook2 recipe instructions.

Predicate Type

# Predicates

Relation

Unary
Binary
N-ary

Class

208
51
1

1136

# Facts

Per Graph

Total

58
29
69

10426
5213
12417

Examples

Cut, fry
Put
MixInto

257

45881 Oil, Oinon, Noodle

Hypergraph construction. We construct the temporal hypergraph from the instruction sentences.
For each clip, we run NLTK tools and extract the nouns and verbs from the sentences. We collect
verbs as temporal relations. After pre-processing and grouping synonyms, there are 208 unary
relations, 51 binary relations, and 1 n-ary relation. There are many possible n-ary relations in the
raw data that describe the similar action, such as “put together” and “stir together”. Given the low
frequency of each data, we consider merging them into a single n-ary relation, that is MixInto.
Statistics are shown in Table 1.

Additionally, we create another version of the dataset with ordinary graphs and time point labels.
This is done by: 1) converting the hypergraph via clique-expansion; 2) dropping the end time of the
time intervals and treating the start time as the time point label.

Task. The goal of recipe summarization is to generalize a structured procedure for each distinct
recipe from the hypergraph data. Formally, Let (G, y) be a hypergraph and its recipe type label (e.g.

6

Table 2: MRR and Hits of three MRBW methods on recipe summarization task together with that of
three GNN-based methods and two static graph ILP methods.

Method

GNN-GCN
GNN-Cheb
CTDNE

RW
NeuralLP

MRBW
MRBW+PC
MRBW+PC (train)

Clique-Expansion

Hypergraph

MRR H@3 H@10 MRR H@3 H@10

0.76
0.78
0.81

0.19
0.36

0.25
0.30
0.42

79.2
80.7
85.3

18.8
37.1

20.4
27.9
38.2

82.1
82.9
87.6

20.6
39.5

23.7
31.1
42.5

-
-
-

-
-

-
-
-

-
-

-
-
-

-
-

0.35
0.60
0.72

29.4
55.8
76.0

36.3
59.1
79.4

BLT sandwich). Let y∗ be the target type for which we want to summarize; the background, positive
and negative sets of y∗ are

Q+ = {(G, y)|y = y∗},

Q− = {(G, y)|y (cid:54)= y∗}.

(2)

The positive set is essentially the set of positive hypergraphs, as the task is to learn rules from the
hypergraph which can entail the positive hypergraphs of recipe y∗ and do not entail the negative ones.
Since the goal is to generate

However, Eq.(2) is not sufﬁcient for generating rules for summarization. Without additional con-
straints Eq.(2) essentially becomes a graph classiﬁcation task, and solving it can lead to rules with
good accuracy but poor interpretability. For examples, one can learn rules that classify spaghetti &
meatballs easily by solely checking if the Spaghetti and Meatball class labels are present in the
graph. To solve this, we put the constraint that the rules must cover most of the time span of the target
hypergraphs. In other words, it needs to ﬁnd rules that generalize to a signiﬁcant subgraph in the
hypergraph. This can be done by keeping track of the earliest and the latest events when a subgraph
is matched by a rule. If the duration of the subgraph is less than that of the entire graph then the rule
is discarded.

Methods. We evaluate three versions of our ILP method: 1) MRBW: this version performs vanilla
MRBW without the path-consistency (PC) algorithm; it proposes the best rule for each recipe by
counting the occurrences and picking the most frequent one. 2) MRBW+PC: this version is the same
as MRBW but with PC algorithm. 3) MRBW+PC (train): this version performs MRBW and PC
algorithm during search and learns rule via the parametric model introduced in section 5.

We also compare with methods of two categories: 1) GNN-GCN [Kipf and Welling, 2016] and
GNN-Cheb Defferrard et al. [2016]. GNN methods use the clique-expansion graph and ignores the
time labels. The goal for including neural models is to provide a reference for score comparison: the
neural models treat the task as a graph classiﬁcation task; they do not produce interpretable results,
but they tend to outperform the symbolic and neuro-symbolic counterparts. 2) CTDNE [Nguyen
et al., 2018]. A graph embedding methods that utilize the time point labels in clique-expansion
graph. 3) random walk (RW) and NeuralLP [Yang et al., 2017]. These are two ILP methods via
counting and differentiable learning respectively. We implement the RW method as a simplistic
baseline that performs personalized random walks (that is starting from the given start point) on the
clique-expansion graph while ignoring the time labels; it counts the frequency of paths that answer
the query. All experiments are done on a PC with i7-8700K and one GTX1080ti. We use the Mean
Reciprocal Rank (MRR) and Hits@3 (H@3) and Hits@10 (H@10) as the metrics,

Results. The results are shown in Table 2, For clique-expansion graph, the three GNN-based methods
achieved the best performance. By inspecting the data (Appendix B), we found that the class labels in
the hypergraph are highly correlated with the recipe types. For example, Lettuce and Ham frequently
co-occur with BLT; and Wasabi co-occurs with Salmon Sashimi. This suggests the GNN-based
models have learned to rely on this information to classify the graph. On the other hand, for the ILP
methods with the constraint that ﬁlters out the trivial rules, the scores are lower. The ﬁrst two versions
of MRBW obtain a comparative score with the ILP baselines and the trained MRBW achieves the
best performance within the ILP category.

7

Table 4: MRR and Hits of the three versions of MRBW on the nuScenes hypergraph benchmark.

Method

nuScenes Hypergraph
MRR H@3 H@10

MRBW
MRBW+PC
MRBW+PC (train)

0.19
0.22
0.46

20.8
24.1
45.3

31.4
35.5
52.7

For the temporal hypergraph representation, only MRBW is applicable. And the performance of the
three versions of the method improves signiﬁcantly. This suggests the hypergraph representation
retains useful higher-order information and our MRBW is capable of learning it.

6.2 Driving behavior explanation

Table 3: Statistics of the temporal hypergraph benchmark based
on the nuScenes autonomous driving datasets.

Predicate Type

# Predicates

# Facts

Per Graph

Total

Relation

Unary

Binary

N-ary

Class

13

6

1

23

4631

3473257

1859

1692

1394253

1269004

257

45881

Examples

ego.throttle
ego.brake
in_front
behind
Between

vehicle.car
human.pedestrian.adult
ped_crossing

Figure 2: Example driving logs
in nuScenes.

The nuScenes autonomous driving dataset [Caesar et al., 2020] consists of 750 driving scenes in the
training set (Figure 2). Each scene is a 20s video clip; each frame in the clip is annotated with 2D
bounding boxes and class labels. The dataset also provides the lidar and ego information such as
absolute position, brake, throttle, and acceleration.

Hypergraph construction. We construct the temporal hypergraph for each driving scene.
For class labels, we use the provided 23 object classes,
including vehicle.car and
human.pedestrian.adult. For the predicate, the ego information together with the object at-
tribute such as vehicle.moving and pedestrian.moving are considered the unary predicates.
The original dataset does not provide relations for the objects. Here, we create the relative spatial rela-
tions by inferring from the absolute spatial information. This includes In_front, Behind, Between
and etc. The statistics are shown in Table 3.

Task. We apply TILR to explain the driving behavior with temporal logic rules. Such explanation is
beneﬁcial as it can provide insight of why certain behavior happens, for example, one can explain
that in what situation will the driver hit the brake; the evidence is likely to be there is a pedestrian
crossing the street. Formally, we consider the positive queries to be the events of ego.brake and
ego.throttle and the rest of the events are the negative queries, that is

Q+ = {P (xh, xt, t)|P ∈ {ego.brake, ego.throttle}},
Q− = {P (xh, xt, t)|P /∈ {ego.brake, ego.throttle}}.

Similar to the recipe summarization. We evaluate three versions of our ILP method: MRBW,
MRBW+PC, and MRBW+PC (train).

Results. The results are shown in Table 4. The results suggest that the gap between the trained and
untrained version of the MRBW are more signiﬁcant in this benchmark. This is because the driving
scenes are more diverse and many situations can lead to the target event. This makes it difﬁcult to
learn rules by counting and picking the best one. In this case, a learned model that proposes the rule
for individual scenes yields a higher score.

8

7 Conclusion

In this work, we propose temporal inductive logic reasoning (TILR). A novel framework that
extends the ILP technique to applications beyond knowledge graphs. We ﬁrst introduce the temporal
hypergraph and show that this is a powerful representation for many applications; we then propose
the multi-start random B-walk (MRBW) algorithm that serves as the foundation of developing ILP
methods on this hypergraph. Finally, we propose an ILP method based on MRBW and a path-
consistency algorithm that generalizes on both the temporal and the relational data. The TILR is
evaluated on the YouCook2 and nuScenes datasets and demonstrates strong performance.

References

James F Allen. Maintaining knowledge about temporal intervals. Communications of the ACM, 26

(11):832–843, 1983.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in neural information
processing systems, pages 2787–2795, 2013.

Elizabeth Boschee, Jennifer Lautenschlager, Sean O’Brien, Steve Shellman, James Starz, and Michael

Ward. ICEWS Coded Event Data, 2015. URL https://doi.org/10.7910/DVN/28075.

Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush
Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for
autonomous driving. In CVPR, 2020.

Andres Campero, Aldo Pareja, Tim Klinger, Josh Tenenbaum, and Sebastian Riedel. Logical rule
induction and theory learning using neural theorem proving. arXiv preprint arXiv:1809.02193,
2018.

T-H Hubert Chan, Anand Louis, Zhihao Gavin Tang, and Chenzi Zhang. Spectral properties of
hypergraph laplacian and approximation algorithms. Journal of the ACM (JACM), 65(3):1–48,
2018.

Uthsav Chitra and Benjamin Raphael. Random walks on hypergraphs with edge-dependent vertex
weights. In International Conference on Machine Learning, pages 1172–1181. PMLR, 2019.

Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral ﬁltering. Advances in neural information processing systems,
29, 2016.

Richard Evans and Edward Grefenstette. Learning explanatory rules from noisy data. Journal of

Artiﬁcial Intelligence Research, 61:1–64, 2018.

Luis Galárraga, Christina Teﬂioudi, Katja Hose, and Fabian M Suchanek. Fast rule mining in
ontological knowledge bases with amie+. The VLDB Journal—The International Journal on Very
Large Data Bases, 24(6):707–730, 2015.

Giorgio Gallo, Giustino Longo, Stefano Pallottino, and Sang Nguyen. Directed hypergraphs and

applications. Discrete applied mathematics, 42(2-3):177–201, 1993.

Woojeong Jin, Meng Qu, Xisen Jin, and Xiang Ren. Recurrent event network: Autoregressive
structure inference over temporal knowledge graphs. arXiv preprint arXiv:1904.05530, 2019.

Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.

arXiv preprint arXiv:1609.02907, 2016.

Ni Lao, Tom Mitchell, and William Cohen. Random walk inference and learning in a large scale
knowledge base. In Proceedings of the 2011 conference on empirical methods in natural language
processing, pages 529–539, 2011.

Kalev Leetaru and Philip A Schrodt. Gdelt: Global data on events, location, and tone, 1979–2012. In

ISA annual convention, volume 2, pages 1–49. Citeseer, 2013.

9

Pan Li, Niao He, and Olgica Milenkovic. Quadratic decomposable submodular function minimization:

Theory and practice. J. Mach. Learn. Res., 21:106–1, 2020.

Yushan Liu, Yunpu Ma, Marcel Hildebrandt, Mitchell Joblin, and Volker Tresp. Tlogic: Temporal
logical rules for explainable link forecasting on temporal knowledge graphs. arXiv preprint
arXiv:2112.08025, 2022.

Giang Hoang Nguyen, John Boaz Lee, Ryan A Rossi, Nesreen K Ahmed, Eunyee Koh, and Sungchul
Kim. Continuous-time dynamic network embeddings. In Companion Proceedings of the The Web
Conference 2018, pages 969–976, 2018.

Ali Payani and Faramarz Fekri. Inductive logic programming via differentiable deep neural logic

networks. arXiv preprint arXiv:1906.03523, 2019.

Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text
inference. In Proceedings of the 3rd Workshop on Continuous Vector Space Models and their
Compositionality, pages 57–66, 2015.

Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical rules for knowledge
base reasoning. In Advances in Neural Information Processing Systems, pages 2319–2328, 2017.

Yuan Yang and Le Song. Learn to explain efﬁciently via neural logic inductive learning.

In
International Conference on Learning Representations, 2020. URL https://openreview.net/
forum?id=SJlh8CEYDB.

Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from
web instructional videos. In AAAI Conference on Artiﬁcial Intelligence, pages 7590–7598, 2018.
URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17344.

10

A Temporal KG completion

Table 5: MRR and Hits of MSWR on three temporal KG datasets together with baseline methods of
two categories.

Method

DistMult
RotatE

TA-DistMult
RE-NET
TLogic

MRBW

ICEWS14

ICEWS18

GDELT

MRR H@3 H@10 MRR H@3 H@10 MRR H@3 H@10

0.19
0.30

0.21
0.46
0.43

0.32

22.0
32.9

22.8
49.1
48.3

36.1

36.4
42.7

35.3
59.1
61.2

50.3

0.22
0.23

0.29
0.43
0.30

0.31

26.0
27.6

31.6
45.5
33.95

33.1

42.2
38.7

45.0
55.8
48.53

46.8

0.19
0.22

0.29
0.40
-

0.34

20.1
23.9

31.6
43.4
-

37.5

32.6
32.3

41.4
53.7
-

45.6

The proposed multi-start random B-walk (MRBW) is also applicable to the Temporal KG datasets.
We evaluate it on the ICEWS [Boschee et al., 2015] and the GDELT [Leetaru and Schrodt, 2013]
datasets.

The temporal KG dataset consists of snapshots of graphs for a series of time points. To apply MRBW,
we modify the graph as followings: 1) we change time point labels into time intervals with the same
start and end time; and 2) we add edges s IsSameEnt(x, x(cid:48), t) to the graphs. The edge connects the
entities of the same IDs in the temporal KGs with start and end time match the time points of the two
connected graph snapshots. As the original graphs at different time points are disconnected, these
edges essentially create paths that go across different time points. Note that we keep the conversion
minimal and avoid introducing additional information into the graph for a fair comparison. The goal
of this study is not to propose a state-of-the-art reasoning method for temporal KGs, as the MRBW is
not designed for this dataset. Instead, this serves as a sanity check on how the algorithm performs on
the common benchmarks.

We evaluate MRBW together with baselines of two categories on the link forecasting task deﬁned
in [Liu et al., 2022]. We used the data split provided in [Jin et al., 2019]. The results are shown in
Table 5; baseline results are taken from [Liu et al., 2022, Jin et al., 2019]. The results suggest that
MRBW has comparable performance with the popular static and temporal graph reasoning models.
To improve the performance, one can introduce a parametric model for rule learning just as that in
section 5. We leave this investigation in future work.

B Example hypergraphs

Figure 3: Example temporal hypergraph for recipe Shepherd’s Pie.

11

