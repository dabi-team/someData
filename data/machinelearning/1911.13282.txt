9
1
0
2

v
o
N
9
2

]
h
p
-
t
n
a
u
q
[

1
v
2
8
2
3
1
.
1
1
9
1
:
v
i
X
r
a

Quantum Computation with Machine-Learning-Controlled Quantum Stuﬀ

Lucien Hardy and Adam G. M. Lewis∗
Perimeter Institute for Theoretical Physics,
31 Caroline Street North, Waterloo,
Ontario N2L 2Y5, Canada
(Dated: December 2, 2019)

We describe how one may go about performing quantum computation with arbitrary “quantum
stuﬀ”, as long as it has some basic physical properties. Imagine a long strip of stuﬀ, equipped with
regularly spaced wires to provide input settings and to read oﬀ outcomes. After showing how the
corresponding map from settings to outcomes can be construed as a quantum circuit, we provide a
machine learning algorithm to tomographically “learn” which settings implement the members of a
universal gate set. At optimum, arbitrary quantum gates, and thus arbitrary quantum programs,
can be implemented using the stuﬀ.

I.

INTRODUCTION

Imagine we have some “stuﬀ” with quantum properties.
Can we use it as a quantum computer? Join us in pictur-
ing a long strip of Plasticine-like stuﬀ, whose unknown
properties are accessible only via regularly spaced setting
and outcome wires. By appropriate choices of settings,
followed by correct interpretations of outcomes, is it pos-
sible to implement quantum computation?
If the stuﬀ secretly contains matter known to be capable
of quantum computation - say, a row of trapped ions
- it is obviously possible. Suppose, for example, that
the setting wires were to manipulate the local magnetic
ﬁeld and the timing of measurements, whose results were
transmitted by the outcome wires. Any given quantum
program could then be implemented, given knowledge of
the details.
Normally, one would ﬁrst decide upon a correspondence
between physical and logical operations, and then en-
gineer a computer to respect it. Here, we discuss the
reverse task, that of mapping arbitrary quantum compu-
tation onto the ﬁxed physics of initially uncharacterized
matter: stuﬀ.
Our iterative approach to this problem begins with a hy-
pothesized map between physical and logical operations.
For example, we might guess that feeding ﬁrst 2 and then
3 to the ﬁrst input wire implements the quantum CNOT
gate. This guess is likely to be wrong, but we will demon-
strate that given a few assumptions about the internal
dynamics of the stuﬀ, it is possible to determine just how
wrong. We can then converge towards a correct guess by
gradient descent, applied to a specially constructed set
of neural networks.
The essential problem we are concerned with, the deter-
mination of the unknown quantum dynamics of a black
box system (in our case, the arbitrary length strip of
stuﬀ), might be viewed as a slight generalization of so-
called “quantum process tomography” [1–6]. We are

∗ alewis@perimeterinstitute.ca

aware of two major diﬀerences. First, we assume no
ability to interact with the quantum stuﬀ except via the
classical settings and outcomes on the wires. In partic-
ular, we do not presume advance knowledge of how to
prepare or to measure states. This echoes previous work
on randomized gate benchmarking [7–10], self-consistent
quantum process tomography [11, 12], and especially op-
erational process tomography [13].
Second, we will never explicitly obtain the process matrix
of the stuﬀ, but only the ability to map chosen quan-
tum circuits onto it. Presumably, the process matrix
would need to be somehow encoded in the weights of any
successfully optimized neural networks, but we make no
attempt to characterize such an encoding. This situates
our work within the rapidly developing ﬁeld of “quantum
machine learning” [14, 15], and, more speciﬁcally, within
the subﬁeld concerned with applying machine learning
approaches to quantum tomography [16–21].
In Section I to follow, we introduce the essential ideas
required to construe operations upon quantum stuﬀ as a
user-deﬁned circuit mapped onto spacetime. These ideas
are then reﬁned and formulated precisely in Section IV.
The iterative process of determining which operations
correspond to which circuit, or “bootstrap tomography”,
is formulated in Section V. Finally, in Section VII, we
propose a machine learning algorithm to realize this to-
mographic process.

II. QUANTUM STUFF AND COMPUTATION

Picture a strip length-L strip of stuﬀ. Fix two wires
at each site xl = l∆X, l = (0, 1, 2, . . .), one to send
in classical settings, and another to read oﬀ classical
outcomes. Binning the duration of these interactions
into discrete time intervals centred at times tn = n∆T ,
n = (0, 1, 2, . . .) deﬁnes a discrete set of spacetime points,
x = (x, t) = (l∆x, n∆t), each labelling a setting-outcome
pair. We refer this structure of setting-outcome pairs at
discrete events as the computational lattice.
The physical dynamics of the stuﬀ might be viewed as a

 
 
 
 
 
 
2

outcome wires

outcome wires

STUFF

STUFF

setting wires

setting wires

t

x

FIG. 1. On the left we see a section of a length of stuﬀ with input and output wires placed at regular intervals. A time-gated
sequence of inputs is fed into the input wires and a similar time-gated sequence of outputs is read oﬀ the wires. We represent
the input/output at position x and time t by a dot as shown. On the right we see the same ﬁgure with the dots divided up
into octagons and squares. Points lying on the boundary between octagons are assigned to the upper octagon.

map from sequences of settings to probability distribu-
tions over sequences of outcomes. To use the stuﬀ as a
quantum computer, we need to know the pertinent as-
pects of that mapping, construed as one between known
mathematical objects. This knowledge can be usefully
decomposed into that of two functions: an encoder to
translate a given program - the “logical input” - into
physical settings, and a decoder to translate probability
distributions over physical outcomes into mathematical
objects, the “logical output”. The encoder/decoder pair
has functioned correctly if and only if the emitted logical
output is, accounting for the probabilistic nature of the
physical outcomes, indeed that dictated by the logical
input.
Suppose, for example, we wish to perform the compu-
tation qout = 2qin. Then the encoder must map from
the logical input “2 × qin” to some sequence of physical
settings. The decoder must then map the correspond-
ing sequence of physical outcomes into the logical output
qout. The encoder/decoder pair is correct, in this case,
when the emitted qout approaches 2qin, having smoothed
(e.g. averaged over) probabilistic ﬂuctuations.
One might reasonably expect the discovery of a correct

encoder/decoder pair to be a fairly daunting task. How-
ever, in this case and, as we will see, in general, checking
whether a given encoder/decoder pair is correct is quite
simple. Furthermore, one can easily construct a smooth
measure, or loss function, of how far from correct a given
encoder/decoder pair is: the RMS error between qout and
2qin, for example, or some monotonic function of it [22].
A correct encoder/decoder pair will minimize this loss
function. Later, we will detail a means to accomplish
this functional minimization automatically on a classical
computer, via a bespoke neural network parameteriza-
tion of the encoder and decoder.

III. GATES, CIRCUITS, AND TESSELATIONS

We would like to implement arbitrary quantum compu-
tations, not just multiplication by 2.
It is well known
that arbitrary quantum computations can be decom-
posed into elemental quantum gates forming a “universal
gate set” (UGS). It would thus be suﬃcient for our en-
coder/decoder pair to correctly implement all members
of some UGS.

H

H

P

P

R

R

3

Subset 1

Subset 2

Subset 3

0

0

0

1

0

1

TABLE I. A set of gates that is universal with respect to the given lattice. H stands for the Hadamard gate, P stands for the

phase gate which acts as

, and R stands for the π

8 gate on the left which acts as

(cid:19)

(cid:18) 1 0
0 i

(cid:18) 1

0
π
0 e
8

(cid:19)
.

Quantum gates, however, map between quantum states,
not classical information. By assumption, we do not have
direct access to the internal quantum dynamics of the
stuﬀ. We will thus instead concern ourselves with quan-
tum circuits, wirings together of quantum gates that are
entirely characterized by classical settings and outcomes.
Any given quantum computation may be expressed as a
quantum circuit. Doing so further reﬁnes the formulation
of our task. The logical input thus becomes the classi-
cal settings and gate labels deﬁning a particular quan-
tum circuit. The logical output is that circuit’s theoreti-
cal outcome distribution. The encoder/decoder pair has
functioned correctly if the emitted outcome distribution
matches the theoretical one. The loss function can be any
of various distance metrics between those distributions.
Later we will demonstrate that, given a few physical
assumptions about the stuﬀ, there exist “tomograph-
If an en-
ically complete” sets of quantum circuits.
coder/decoder pair, acting upon stuﬀ with the assumed
properties, correctly implements all the circuits in a to-
mographically complete circuit set, it correctly imple-
ments all the gates in a UGS. The combination of en-
coder, stuﬀ, and decoder in that case forms a universal
computer. The degree by which it fails to do so can be
taken, most primitively, as the summed losses of all the
circuits in the set.
We want the stuﬀ not only to implement an arbitrary
computation, but to restrict the required number of set-
ting/outcome retrievals to do so. To achieve this, we
will require the encoder’s implementation of each indi-
vidual gate to occupy only a ﬁnite volume of spacetime.
Speciﬁcally, we group together events in the computa-
tional lattice x = (l∆x, x∆t), depicted in the left panel
of Figure 1, into a tesselation of octagons and squares,
depicted in the right panel of Figure 1.
Each octagon, or tessel, will be used to implement one
of a universal set of two qubit gates. The qubits for
these gates will, under this implementation, be input at
the lower slanted edges, and output at the upper slanted
edges. Octagons are convenient because they adjoin to
form causal-diamond-like structures. The squares appear
by geometric necessity and will implement a ﬁxed “do

nothing” gate, diﬀerent from the identity. We will even-
tually view the encoder and decoder as neural networks
to be trained by a machine learning algorithm. One fail-
ure mode of this algorithm will occur if the tesselation
encloses too few events per gate to implement the UGS
properly.
In that case, we can uniformly scale up the
tesselation and try again.
We label each octagon by its midpoint, x. Let Xoctagons
be the set of octagon positions we consider in some given
tessellation over the length L and some time duration,
T .
The two qubit gates situated on the octagons will form
a regular lattice which we will call the gate lattice. A
universal gate set (UGS) must include enough gates to do
universal quantum computation with respect to the gate
lattice. An example of a universal set of gates, complete
with respect to this lattice is given in Table I. Each of the
gates in the table is a two-qubit gate. The ﬁrst subset of
gates are the usual gates included in a UGS adapted for
the lattice.
Because we assume control over the stuﬀ only through
the classical settings, we need to include two more sub-
sets of gates not usually mentioned as part of a UGS. The
second subset are the identity and swap gates which allow
us to transport qubits around the circuit. The third sub-
set of gates are the preparation and measurement gates.
These gates are non-unitary. The ﬁrst gate in subset
3 performs the identity measurement on the incoming
qubits and then prepares two qubits, each in the 0 state.
The second gate in subset 3 projects the left qubit onto
the 0 basis while leaving the right qubit unchanged. We
have similar notation for the other gates. Unlike the
other gates in the table, these measurement gates have
outcomes associated with them.
It is necessary to include the second and third subsets of
gates in our UGS because we need to train our stuﬀ to
implement them the same as any other gate. They do
not come for free.
We can connect gates together to form what we will call
a fragment (denoted by F). Whenever we build a frag-
ment, there must be some gates having one or two open
inputs. We will call such gates initial gates. A circuit, C,

4

0

1

0

1

0

0

H

H

0

0

0

0

0

0

0

0

FIG. 2. On the left we see an example of a circuit built with gates from our universal gate set. Note that the circuit is closed
oﬀ from external inﬂuences because, at the bottom, input signals are absorbed by the identity measurement and, at the sides,
quantum information coming into the circuit is shunted back out. On the right we see how gates can be assigned to octagons.

is a special case of a fragment for which all initial gates
are of the type that ignore any open inputs. The gates in
the table provide two ways to do this. First, the prepara-
tion gate in subset 3 simply absorbs incoming quantum
systems so they do not aﬀect probabilities for the circuit.
Second, any gates that include an identity map can be
used to shunt quantum information coming through an
open input back out through an open output so probabil-
ities for the circuit are not aﬀected. The circuit in Figure
2 contains examples of both types. We can calculate a
probability for a circuit using the rules of Quantum The-
ory. A fragment that is not a circuit is subject to outside
inﬂuences and so will not necessarily have a probability
associated with it.
We will choose some particular UGS, call it G, to pro-
ceed. It does not have to be the one described here, and
there might UGS’s that are more suited to this project.
However, the gate set must have elements that enable us
to close oﬀ in the manner just described. There must
also be some gates with outcomes, so we can read oﬀ the
results of the computation.

IV. BUILDING CIRCUITS

In this section, we describe how sequences of operations
upon the stuﬀ may be arranged in such a way to permit
comparison with a given quantum circuit.
Thus, within each octagon of the tesselation, we will en-
code (by inputting an appropriate sequence of signals into
the setting wires attached to the stuﬀ) and decode (by
selecting on an appropriate sequence of output signals
from the outcome wires attached to the stuﬀ) in an at-
tempt to implement a putative element of the UGS, G.

Initially we do not know what the appropriate encoding
and decoding are. Thus we start with some initial choice
and then, through the machine learning algorithm, train
until we settle on encodings and decodings that minimize
the loss function.
Let the putative encoding (decoding) for gate g ∈ G be
denoted En[x, g] (Dn[x, g]) for the nth iterative step in
the training for the octagon at x. We write Y n[x, g] =
(En[x, g], Dn[x, g]). We are, then, admitting the possi-
bility that the same gate might require diﬀerent encod-
ings and decodings in diﬀerent regions of spacetime. One
could imagine eventually adjusting the machine learning
algorithm to be introduced to exploit any assumed homo-
geneity, but we will not explore this issue further. At the
nth step we will have a particular encoding and decoding
scheme

{(x, Y n[x, g]) : ∀ x ∈ Xoctagons, ∀ g ∈ G}

(1)

This speciﬁes an encoding for every element of G for every
octagon. We will attempt to implement various quantum
circuits using this encoding. Then, using the machine
learning algorithm trained upon the resulting empirical
information, we will iterate the encoding and decoding
scheme, obtaining a new one to be used during the (n +
1)th step.
We consider a fragment, F, made from the gates, g, in
our UGS with locations assigned to the positions of some
of the octagons in some octagon square tessellation. The
fragment F is speciﬁed by

F = {(x, g) : ∀x ∈ X ⊆ Xoctagons}

where X is the set of positions of octagons positions in
the tessellation at which a gate is placed. A circuit, C, is
a special case of a fragment in which every initial gate is
of the type that ignores any open inputs into it.

The attempted implementation of circuit C during the
nth step is now given by

Y n[C] = {(x, Y n[x, g]) : ∀x ∈ X}

(2)

in the octagons. Actually, this is not entirely suﬃcient,
since we must also specify what happens operationally in
regions of spacetime outside the given octogons (tessels).
We will return to this point at the end of this section.
Each gate g in C maps quantum information to classi-
cal outcomes with various probabilities. The full circuit,
however, ﬁxes this internal quantum information (by in-
cuding state preparation and measurement “gates”, for
example), and is thus characterized by a single proba-
bility pC for a speciﬁc set of classical outcomes to be
observed. Quantum mechanics can be formulated [23] as
an assignment of a pC to every possible circuit C.
In the next section, we will show that under certain as-
sumptions, the converse is also true. That is, if each cir-
cuit within a tomographically complete circuit set indeed
occurs with its predicted pC, the underlying operational
map is indeed that speciﬁed by the relevant sequence of
gates.
In order to certify that the computation within the
stuﬀ is indeed quantum mechanical, we thus seek en-
coder/decoder pairs that perform this same assignment
via the stuﬀ. That is, the theoretical map, from circuit
settings and gate labels C to outcome probabilities pC, is
also the operational one, from the tesselation of C and a
given Y n[C] to the observed outcome probability pn
C .
We can then deﬁne one of several loss functions measur-
ing how closely Y n[C] indeed implements some C,

L[Y n[C], C] = err(pn

C , pC),

(3)

where err(x, y) is some convenient positive function with
a global minimum at x = y, for example err = |x2 − y2|.
Now suppose we have ﬁxed a set S of circuits that we
wish to simultaneously implement, for example because
they form a tomographically complete circuit set. A loss
function over the full set can then be written as

L[Y n, S] =

(cid:88)

C∈S

L[Y n[C], C].

(4)

Since the internal quantum information of the stuﬀ might
depend upon occurrences outside the spacetime region we
have identiﬁed with our circuit, for this to work in prac-
tice we must also specify what happens at the spacetime
points, ¯X, that are not in the octagons labelled by x ∈ X.
This includes those in the squares and those that occur
elsewhere.
One strategy out of potentially many is to choose a “null”
encoding E0 at each “external” spacetime point, and to
simply ignore signals on the outcome wires there, so that
we need not specify any decoding. The null encoding
might also be iterated (“trained”) in order to, for ex-
ample, appropriately “zero out” quantum information in

5

the external regions; in that case it would be denoted En
0 .
Similar considerations apply to the squares. We do not
need to concern ourselves with the encoding for space-
time points in the future of the circuit, C, via the causal
assumption that inﬂuences cannot travel backwards in
time.

V. BOOTSTRAP TOMOGRAPHY

Now let us discuss the construction of so-called “tomo-
graphically complete circuit sets”.
If the loss function, L[Y n, S], is minimized when summed
over a big enough set of test circuits, we would like it to
be the case that any circuit gives the correct probabili-
ties (to within some small error). In this section we state
a theorem (proven in Appendix 1) that if the empirical
probabilities, pn
C , are exactly equal to the ideal proba-
bilities, pC (so the loss function (4) is minimized), for a
certain set of circuits, Stom, then this is true for all cir-
cuits. The circuits in Stom have the property that they
are bounded in size. We conjecture that a robust version
of this theorem also holds - namely that the loss function
(4) over Stom bounds the loss function for all circuits.
We will need the important notion of a bounded fragment
(or circuit). This is one that ﬁts inside a box of some
constant size, ∆L and ∆T , where this box size does not
increase in size as we increase L and T .
It is shown in Appendix 1 that we can associate a vec-
tor, rn
X [F], with any fragment in region X. This vector
linearly relates the given fragment to a tomographically
complete set of fragments for the given region. In the case
of Quantum Theory, the vector rX [F] is linearly related
to the superoperator associated with the fragment. The
vectors, rn
X [F], are used to calculate the probabilities for
circuits.
We can determine the vectors, rX [F], by doing tomog-
raphy on a set of circuits F ∪ ¯F for diﬀerent ¯F. We say
we have fragment tomography boundedness if we can do
tomography on a bounded set of fragments (pertaining
to X) by means of a bounded set of circuits.
In Appendix 1 we deﬁne a composition tomograph,
ΛΛΛX1,X2,...
which tells us how to combine r vectors per-
taining to non-overlapping (though possibly adjacent) re-
gions X1, X2, . . . to obtain the tomographic information
pertaining to the region X = X1 ∪ X2 ∪ . . . .
We say we have composition tomography boundedness
if the composition tomograph for a composite region
formed from any number of regions can be determined
from the composition tomographs for composites that ﬁt
inside bounded boxes. In other words, we can do the cal-
culation for any circuit from calculations pertaining to
smaller bounded parts of that circuit.
In Appendix 1 the following theorem is proven.

X

Theorem 1.

If we have fragment to-

mography boundedness and composition to-
mography boundedness, then there exists a
bounded set of circuits Stom such that if, at
iteration ˜n, we have

p˜n
C = pC ∀C ∈ Stom

then p˜n

C = pC for any C ∈ Scircuits.

Stom is then called a “tomographically complete circuit
set”. We conjecture (but do not prove) that a robust
version of this theorem holds.
Conjecture 1.
If we have fragment to-
mography boundedness and composition to-
mography boundedness, then there exists a
bounded set of circuits Stom such that if, at
iteration ˜n, we have

L[Y ˜n, Stom] ≤ (cid:15)

6

stuﬀ fails to have the properties. For example, the prop-
erties will fail if the stuﬀ has hidden signalling between
far separated locations. For example, the bit of the stuﬀ
at x might send a radio frequency signal to the bit of
stuﬀ at some x(cid:48) in the deep future outside any bound-
ing box where this radio signal cannot tomographically
probed by circuits that live in a bounding box.
While it would be satisfying to prove Conjecture 1 math-
ematically, it is possibly more useful to test it empiri-
cally. The conjecture does, in any case, necessarily in-
volve assuming the boundedness properties which are,
themselves, in need of empirical investigation. We test
the conjecture empirically by determining the extent to
which minimizing the loss function on various bounded
sets of circuits allows us to reproduce the probabilities
for sets of larger circuits.

then

VI. RANDOM CIRCUIT SAMPLING

err(pn

C , pC) ≤ (cid:15)BNC

∀ C

where B is a constant and NC the number of
gates in C.

The motivation for this conjecture is that the tomogra-
phy process will ﬁx the parameters in the gates to some
error. If we enlarge the set, Stom, we can expect to get
a better bound on the error since we then collect more
information.
The causaloid framework [24–27] is used to prove The-
orem 1. This framework was originally developed as a
framework for modelling indeﬁnite causal structure in the
context of Quantum Gravity. These theorems mean that
we can rely on the quantum stuﬀ to implement an ar-
bitrary circuit as long as the measured probabilities for
circuits in Stom are close enough to the ideal probabilities
calculated from Quantum Theory. This is good because
it would not be practical to measure the probabilities
for all C ∈ Scircuits since the rank of this set grows very
rapidly with L and T .
For the two tomographic boundedness properties to hold
requires in each case that (i) that a mathematical prereq-
uisite holds and then (ii) that the physics of the stuﬀ ac-
cords. The mathematical prerequisite is that the proper-
ties hold for ideal circuits constructed on the given lattice
from the given universal gate set. To check this requires a
mathematical calculation. Since we have a universal gate
set, it is immediately clear that this mathematical pre-
requisite holds for fragment tomography boundedness (as
we can use the UGS to construct a tomographically com-
plete set of fragments that are bounded). We conjecture
in Appendix 1 that the mathematical prerequisite holds
for composition tomography boundedness for any UGS.
If the mathematical prerequisites hold, then we can con-
sider whether the physical properties accord. This will,
most likely, be settled through working with the stuﬀ.
However, we can always cook up situations in which the

In the preceding sections we have illustrated that mini-
mization of the loss function (4) by an implementation
Y n[C], acting upon the stuﬀ with respect to a tomograph-
ically complete set of quantum circuits Stom, certiﬁes that
each individual gate in a UGS has also been correctly im-
plemented by Y n[C]. The loss function (4) is inconvenient
to use directly, however, because a) computing a proba-
bility for every circuit in Stom for every training iteration
is likely to be expensive and b) the diﬀering contributions
to the overall gradients by the (possibly many) terms in
the sum over Stom are expected to confound gradient de-
scent optimization.
We will instead operate upon randomly constructed cir-
cuits, with each training iteration acting upon either a
single example or perhaps a small minibatch of them.
Random circuits can be constructed in various ways. For
example, we could start by randomly assigning gates
from the UGS to a few positions, resulting in a frag-
ment. Next, we can identify the locations of open wires,
and close the circuit by assigning random preparation
measurement gates to each. Alternatively, one could be-
gin with the preparation gates, randomly add gates from
the UGS capable of receiving inputs from those already
present, and at some also-random point close the con-
struction with a measurement gate, repeating the process
from scratch if it has failed to deliver a circuit. Any such
random constructions will be controlled by some param-
eters which dictate the average size of the circuits.
Consider a set (minibatch), Srand, of circuits generated
randomly by some such technique. We make the follow-
ing random sampling assumption

Assump: Random circuit sampling.
There exists a bounded set of circuits Srand
such that if, at iteration ˜n, we have

L[Y ˜n, Srand] ≤ (cid:15)

then

err(pn

C , pC) ≤ (cid:15)DNC

∀ C

where D is a constant and NC the number of
gates in C.

For this assumption to hold good, we must have a ma-
chine learning algorithm that does not know what ran-
dom set of circuits is going to be chosen from one iteration
to the next. In other words, our procedure must indeed
be “random” in the sense that the algorithm never learns
to predict its output in advance.
The practical loss function is thus

Lrand[Y n, Stom] =

(cid:88)

(cid:88)

L[Y n[C], C],

(5)

Srand⊂Stom

C∈Srand

where Srand is a randomly constructed subset of Stom.
Since Srand would canonically contain only one or a few
elements, the interior sum is much simpler than that of
(4). In addition, by virtue of the random circuit sampling
assumption, the exterior sum can be treated by individ-
ually optimizing each of its summands. In other words,
repeatedly optimizing L[Y n[C], C] with respect to ran-
domly constructed C from Stom also optimizes (5) and,
by assumption, (4).

VII. MACHINE LEARNING ALGORITHM

A. Neural network implementation of
encoder/decoder

We have repeatedly alluded to our intention to view the
encoder/decoder pair as neural networks to be trained
by a machine learning algorithm. In this section we will
elaborate upon this process. Recall that in the preceding
sections we have formulated the problem of translating
arbitrary quantum programs into operations upon the
stuﬀ as bootstrap tomography: the functional minimiza-
tion of the loss function (4) (in practice (5)) obtained by
comparing the observed and predicted outcome distribu-
tions with respect to the encoder/decoder pair at train-
ing iteration n, En[x, g] and Dn[x, g]. Here g = g(t, x)
is the gate label assigned to the point x = (t, x) by the
tesselation of the given circuit.
The problem of automatically varying a function to op-
timize a loss function is the central concern of machine
learning. The subﬁeld of deep learning [28, 29] has be-
come explosively popular in the past decade or so, as suf-
ﬁcient computational resources have become available to
produce nearly magical - and highly proﬁtable - results
in the most impressive cases. Deep learning becomes
especially useful relative to other approaches when the
function to be optimized represents, or depends upon, a
complicated probability distribution, as one might expect
the encoder/decoder pair to.

7

FIG. 3. Depiction of the simulation algorithm. Circuit Sim-
ulation: The user decides on a program (circuit), which via
the tesselation assigns a gate label to each spacetime event.
Each spatial point is assigned an encoder, mapping these gate
labels to inputs to the stuﬀ. Output from the stuﬀ is then
processed by the decoder into simulated logical output from
the program. The decoder receives the gate label g and the
central spatial point of the tessel x in addition to its depicted
input. Training: by choosing circuits from a tomographically
complete circuit set, the comparison between predicted and
actual output from each can be used as a loss function for the
encoder and decoder, such that all circuits are correctly im-
plemented at optimum. This is achieved by representing the
encoder and decoder as neural networks, and descending their
weights towards this optimum, using randomly constructed
circuits as input.

A deep learning optimization, or “training”, begins by
representing the target function as a linear composition
of smooth activation functions called a neural network.
Diﬀerent network structures are deﬁned by diﬀerent ar-
rangements of activation functions, with “deep” learn-
ing being somewhat vaguely deﬁned by its focus upon
network structures formed of “many” successive layers.
The activation functions themselves are parameterized by
their weights θ so that diﬀerent functions decompose into
a given network structure by diﬀerent choices of weights.
Every function theoretically has some neural network
representation [29]. While this in itself is not especially
impressive, the linearity and smoothness of a neural net-
work’s activation functions permit functional derivatives
with respect to the full network to be expressed as sums
of partial derivatives of the weights, essentially via the
chain rule. The gradient of the full function with respect
to some loss can then by eﬃciently descended by descend-
ing those of the individual activation functions, a process
known as backpropagation [29–31].
Precisely how backpropagation is best applied in a given
situation is somewhat problem-dependent. We will save
detailed consideration of this matter in future studies,
when we implement bootstrap tomography on a small,
classically simulated spin chain.
For now we will

instead depict the problem at the

Algorithm 1 Schematized optimization of the encoders and decoder.

8

for number of training iterations do

(cid:9) , θD)

x

1: procedure Optimize((cid:8)θE
2:
3:
4:
5:
6:
7:
end for
8:
9: end procedure

Circuit ← a randomly generated circuit.
LogicalOutput ← a batch of m calls to SimulateCircuit((cid:8)θE
P redictedOutput ← m predicted circuit outcomes.
Loss ← distance metric between LogicalOutput and P redictedOutput per (3).
Descend the gradients of (cid:8)θE

(cid:9) , θD, circuit).

x

x

(cid:9) and θD to minimize Loss, as dictated by the chosen optimization strategy.

more abstract level depicted in Figure 3. We will de-
note the neural network representation of the decoder as
D[θD; x, g], so that

Dn[x, g] = D[θD; x, g].

(6)

The network representation D itself is ﬁxed for all n.
Training updates are instead implemented by varying the
weights θD between training iterations - they are other-
wise ﬁxed.
The encoder En[x, g] controls the internal quantum in-
formation of the stuﬀ, and thus unlike the decoder must
interact with it in real time.
It nevertheless needs to
share information within tessels in order to track that
quantum information’s ﬂow. This point will be elabo-
rated upon in the upcoming Subsection B. For now we
brieﬂy note that we handle this problem by representing
En[x, g] with a “ﬂeet” “recurrent” neural networks, one
at each spatial point x. We denote the encoder weights at
some x as θE
x , and the full set of weights over all spatial
points as (cid:8)θE
x

(cid:9). Thus

En[x, g] = E[(cid:8)θE

x

(cid:9) , M; x, g].

(7)

x

The network representation E is again ﬁxed for all train-
ing iterations n, with variation between iterations instead
(cid:9). The
implemented by manipulations of the weights (cid:8)θE
output of the network additionally depends upon a mem-
ory vector M. This vector is passed between diﬀerent
θE
x within a tessel, allowing encoders following diﬀerent
constant-x “worldlines” to communicate. We will elabo-
rate upon this point in Subsection B.
Let us now follow the logic of Figure 3 in words. The user
selects a program, which is mapped by the tesselation
into a ﬁeld of gate labels g(t, x). Each gate label at x is
passed along with the appropriate memory vector M to
the encoder at that same x, E[θE
x , M; x, g]. This yields a
raw input signal, to be sent to the setting wire at x. Once
an entire tessel has been implemented, the corresponding
“raw” outcome settings from the stuﬀ are passed into the
decoder, which maps them into “logical” output.
(cid:9) are optimal, as indicated by
If the weights θD and (cid:8)θE
the loss function (5) reaching its global minimum, the log-
ical output may be interpreted as the correct result of the
program. Otherwise, the gradients of the weights in the
direction of decreasing (5) can be calculated, as by the

x

“optimizer” in Figure 3, and descended along to obtain
new, better optimized weights. In machine learning par-
lance, this process is called “training” the networks. The
next iteration operates upon a new randomly constructed
circuit, and the process is repeated until a desired con-
vergence threshold is reached. Illustrative pseudocode is
provided as Algorithm 1.

B. RNN ﬂeet implementation of encoder

As discussed previously, the encoder En[x, g] and de-
coder Dn[x, g] have diﬀering relationships with the real
time behaviour of the stuﬀ, which suggests a particular
network implementation for the encoder that we call an
“RNN ﬂeet”. We will elaborate upon this point here.
Figure 4 depicts several aspects of the behaviour of the
encoder/decoder algorithm in spacetime. In the top left
panel, we see a single tessel, assigned to a gate labelled
g. The top middle panel depicts this same tessel, imple-
mented as operations upon the encoders and thus upon
the stuﬀ. As the stuﬀ proceeds through time, the gate
label g is fed to the encoder at each (t, x) in the tessel.
As we see in the top right panel, the output from the
tessel is collected into a vector. Once all of it has been
collected, it is sent to the decoder, which produces the
logical output of the gate.
Since nothing in this procedure depends upon the logical
output directly, the precise time at which the decoder
Dn[x, g] operates is not especially important, within rea-
son. The encoder En[x, g], on the other hand, controls
the internal quantum information of the stuﬀ. The order
in which its inputs are processed therefore critical.
In
addition, it must be synchronized within a tessel.
The problem of processing “time series” with a speciﬁc
temporal ordering occurs repeatedly in machine learn-
ing. The prototypical example is machine translation,
since correct translations depend upon prior context. A
genre of network structures known as “recurrent neural
networks”, or RNNs [29, 30, 32, 33], are adapted to re-
member such context. In addition to their weights, which
are ﬁxed except during training, such networks maintain
a “memory” vector M. The output of the RNN depends
on the memory as well as upon the weights and the input.

9

FIG. 4. Top Left: each point (t, x) in spacetime is assigned a gate label g by the tesselation. g is constant within each
tessel, and takes a uniform “null” value outside of a tessel. Bottom Left: each spatial point x is assigned a recurrent neural
network (RNN) “Encoder”, mapping g and a “memory” vector M to the input to the stuﬀ, along with a new Mn+1 (note this
n paramaterizes subsequent RNN calls, not training iterations). The map is governed by “weights” θE
x , local to x and held
ﬁxed except during training. Top Middle, Top Right: the stuﬀ advances through time, receiving encoded input dictated
by the gate labels. Its raw outputs O(t, x) at each point in each tessel are collected into a vector, and then fed along with the
gate label g to the decoder. The decoder, another neural network with weights θD, emits the simulated “logical output” of the
gate. Bottom Middle, Bottom Right: two strategies to allow the encoder RNNs to collaborate over a region of spacetime.
Rasterized memory (Bottom Middle) involves passing Mn in left-right order throughout a tessel. Causal memory (Bottom
Right) involves passing it forward within each encoder’s future light cone, achieving, per the locality assumption, the same end
in a shorter timescale.

But unlike the weights, the memory is modiﬁed between
successive function calls, allowing it to represent contex-
tual information: which contextual information to record
is in turn determined by the weights.
We thus implement the encoder En[x, g] at each separate
spatial point x as an RNN = E[θE
x , M; x, g]. as depicted
in the bottom left panel of Figure 4. Each E[θE
x , M; x, g]
follows a particular constant-x worldline through the tes-
selation, partly motivating the term “RNN ﬂeet”.
Additional motivation for the term comes from the need
of the various RNNs to be synchronized within a tes-
sel. Thus, the memory vector Mn is not simply passed
forward along a worldline, but is instead shared within
each tessel between networks of diﬀerent weights. Conse-
quently, we need to synchronize the processing of several

time series at diﬀerent spatial points.
The bottom middle and bottom right panels depict two
strategies for synchronizing within tessels. We call the
ﬁrst and simplest strategy, in the bottom middle panel
of 4, rasterized memory.
In this paradigm, encoders
E[θE
x , M; x, g] are called sequentially at each timestep
from left to right. Starting from a ﬁxed null value, mem-
ory is thus passed within the zig-zagging “spacelike” lines
depicted in the bottom middle panel of Figure 4, which
resemble the “rasterized” beam path of a cathode ray
tube television. Circuit simulation using the rasterized
strategy is illustrated by the pseudocode of Algorithm 2.
The illusion of motion created by such televisions re-
lies upon the time required for the beam to traverse the
screen being much shorter than the processing time of

the eye. The ability of the rasterized memory strategy
to eﬀectively synchronize encoders within a tessel corre-
spondingly depends upon the processing time of the en-
coders being much shorter than that between simulation
timesteps.
The causal memory strategy depicted in the bottom right
of Figure 4 relaxes this assumption. It is based on the
assumption that the error incurred by sending encoder
input out of sequence falls oﬀ with the spacetime distance
between the disordered events.
Instead of passing memory sideways between every event
in the tessel, we thus pass it forward in spacetime within
a ﬁxed “light cone” of predetermined width s, unless do-
ing so would cross a tessel. Given locality, this should
synchronize just as well as the rasterized strategy, with-
out putting impositions upon the relative timescales of
the encoder and the simulations. Circuit simulation us-
ing the causal strategy is illustrated by the pseudocode
of Algorithm 3.

VIII. ACKNOWLEDGEMENTS

A. G. M. Lewis is supported by the Tensor Network
Initiative at Perimeter Institute. Research at Perime-
ter Institute is supported by the Government of Canada
through the Department of Innovation, Science and Eco-
nomic Development Canada and by the Province of On-
tario through the Ministry of Research, Innovation and
Science.

APPENDIX 1

In this appendix we provide deﬁnitions of
fragment
tomography boundedness and composition tomography
boundedness and we prove Theorem 1. The idea is to
consider tomography on fragments. We consider two
types of tomography. First, we have fragment tomog-
raphy, whereby we obtain a mathematical object (rX [F]
below) associated with the fragment. Second, we con-
sider composition tomography whereby we obtain the rule
for composing these mathematical objects for composite
regions such as X1 ∪ X2 ∪ X3. If the tomography bound-
edness properties hold then we only need to consider frag-
ment tomography for fragments up to a certain size and
composition tomography for composites up to a certain
size. To do tomography on these fragments and com-
posite fragments are completed into circuits. Hence it
follows that we only need to consider circuits need be
no bigger than a certain size. This provides our set,
Stom ⊂ Scircuits of circuits.
If we obtain probabilities
pn
C = pC for C ∈ Stom (to within some bounded error)
then it follows that pn
C = pC (to within some bounded
error) for all circuits.
For a circuit, C = F ∪ ¯F we can always write the proba-

bility as

F ∪ ¯F = rn
pn

X [F] · pn

X [ ¯F]

where we deﬁne the ordered set

pn[ ¯F] = (pn

F k∪ ¯F : for k ∈ ΩY )

10

(8)

(9)

where k ∈ ΩX labels the elements, F k, of some tomo-
graphic set, T tom
X ⊆ TX of minimal possible rank. Here
TX is the set of all possible fragments at X.
In gen-
eral, the choice of tomographic set, T tom
X , is not unique.
We can always write (8) because, in the worst case, we
X = TX and then the vector rn
can choose T tom
Y [F] is just
a list of 0’s except with a 1 at position k.
In general,
however, there will be some linear relationships between
these probabilities so that we can use a proper subset,
X [ ¯F] as the generalized
X ⊂ TX . We can think of pn
T tom
state prepared by ¯F for region X. And we can think of
rn
X [F] as the generalized eﬀect associated with fragment
F performed in region X. The choice of tomographic
set, T tom
X , must be good for calculating the probability
for any circuit in any region X ∪ ¯X (that is for any ¯X
associated with a fragment ¯F).
Using simple linear algebra [34] we can obtain the set
{rn
X [F] : ∀ F ∈ TX } if we are given enough empirical
X . The set, Stom
information in the form of pn
X ,
has to be big enough to make this possible - namely it has
X [ ¯F] vectors. In
to generate ΩX linearly independent pn
this case we will say Stom
X is tomographically complete for
X. We can be sure this is true simply by choosing S tom
to be the set of all circuits in Sall circuits having elements
at positions in X. This is not very useful however as this
set grows very rapidly with L and T .
To obtain a more useful notion we deﬁne a bounded set
of circuits or fragments as one for which each element
ﬁts inside a box with bounded spatial and temporal di-
mensions which do not scale with L and T . Consider the
following property.

C for C ∈ Stom

X

Fragment
tomography boundedness.
We say we have fragment tomography bound-
edness if, for any bounded set of fragments,
there exists a bounded and tomographically
complete set of circuits.

We can motivate the assumption that this property holds
by ﬁniteness and locality. First, note that the opera-
tionally accessed part of the Hilbert space associated with
the inputs and outputs for any F ∈ T should be ﬁnite so
only require a ﬁnite number of circuits for tomography.
Furthermore, by locality we should be able to do tomog-
raphy on this Hilbert space by means of circuits that are
not too much bigger than the fragments.
Consider a composite region, X1 ∪ X2 (where X1 and X2
are disjoint). Then, for the circuit C = F1 ∪ F2 ∪ ¯F, we
can write the probability as
F1∪F2∪ ¯F = rn
pn

[F1 ∪ F2] · pn

X1∪X2

X1∪X2

(10)

[ ¯F]

where
pn

X1∪X2

[ ¯F] = (pn

F k1 ∪F k2 ∪ ¯F : for k ∈ ΩX1∪X2 )

(11)

11

x

(cid:9) , θD, circuit)

for t in the tesselation do

for x in the tesselation do

nT ← a separate memory vector for each T .

g(t, x) ← the gate labels assigned to each spacetime point by the tesselation of circuit.
nT ← a separate point counter for each tessel T with ﬁxed g.

Algorithm 2 Circuit simulation using rasterized memory, passed sequentially between each point in each tessel.
1: procedure SimulateCircuitRasterizedMemory((cid:8)θE
2:
3:
4: MT
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: end procedure

Determine which tessel T the point (t, x) inhabits.
Retrieve this g, nT , and MT
InputSignal, M T
RawOutput[nT ] ← the corresponding output from stuﬀ at x.
nT ← nT + 1.

LogicalOutput[T ] ← output mapped from RawOutput, g, and x by decoder with weights θD.

end for
if this is the last point in a tessel then

nT +1 ← output mapped from g and MT

nT by encoder at x with weights θE
x .

end for
return LogicalOutput

end if

nT .

Algorithm 3 Circuit simulation using causal memory, passed within forward lightcones, conﬁned by tessels.

x

for t in the tesselation do

for x in the tesselation do

(cid:9) , θD, circuit, light cone width s.)

1: procedure SimulateCircuitCausalMemory((cid:8)θE
g(t, x) ← the gate labels assigned to each spacetime point by the tesselation of circuit.
2:
nT ← a separate point counter for each tessel T with ﬁxed g.
3:
4: Mt=0,x ← a separate memory vector for each spatial point x.
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: end procedure

Retrieve g(t, x) and Mt−s,x−s:x+s, the memories between x − s and x + s.
˜Mt,x ← pointwise multiplication of Mt−s,x−s:x+s within the same tessel as (t, x).
InputSignal, Mt,x ← output mapped from g and ˜Mt,x by encoder at x with weights θE
x .
RawOutput[nT ] ← the corresponding output from stuﬀ at x.
nT ← nT + 1.

end for
if this is the last point in a tessel then

end for
return LogicalOutput

LogicalOutput[T ] ← output mapped from RawOutput, g, and x by decoder with weights θD.

end if

It can be shown [24] that for the composite region X1∪X2
we can choose a tomographic set of fragments of the form
F k1 ∪ F k2 where

ΩX1∪X2 ⊆ ΩX1 × ΩX2

(12)

and, correspondingly, that

Composition tomography extends in the obvious way to
more than two regions. Thus, we can obtain ΛΛΛX1,X2,X3
,
X1∪X2∪X3
ΛΛΛX1,X2,X3,X4
, and so on. It turns out that these ΛΛΛ’s are
related by mathematical identities if the Ω sets associated
with the X’s have certain relationships with one another
(see Sec. 23 of [24]). Consider the following property

X1∪X2∪X3∪X4

rn
X1∪X2

[F1 ∪ F2] = ΛΛΛX1,X2
X1∪X2

(rn
X1

[F1] ⊗ rn
X2

[F2])

(13)

[F]⊗rn
X2

where ΛΛΛX1,X2
(which we called a composition tomogram
X1∪X2
in Sec. V) linearly projects vectors, rn
[F] living
X1
in a space of dimension |ΩX1 × ΩX2 | down to the vectors
rn
[F1 ∪ F2] living in a space of dimension |ΩX1∪X2|.
X1∪X2
If we do fragment tomography to obtain rn
[F2],
X1
and rn
[F1 ∪ F2] then we can ﬁnd an appropriate
ΩX1∪X2 set and solve for ΛΛΛX1,X2
. This last step com-
X1∪X2
pletes the composition tomography for X1 ∪ X2.

[F1], rn
X2

X1∪X2

Composition tomography boundedness.
We will say we have composition tomogra-
phy boundedness for circuits formed on a
given lattice using a given gate set, G, if we
can obtain ΛΛΛX1,X2,...
for any X from math-
ematical identities concerning only ΛΛΛ’s per-
taining to boxes bounded by some constant
size (∆Lbounded, ∆Tbounded) (which do not in-
crease with L and T ).
Here we say that ΛΛΛX1,X2,...,XM

X

X1∪X2∪···∪XM

pertains to a box

bounded by (∆Lbounded, ∆Tbounded) if X1 ∪X2 ∪· · ·∪XM
ﬁts inside such a box. From the results in Sec. 29 of
[24] it is clear this conjecture is true for a speciﬁc gate
set, G, consisting of gates for which the associated op-
erators span the full space of operators acting on the
Hilbert space associated with two qubits. In this case,
|G| = 256. The gate set illustrated in the above table has
only |G| = 14. We conjecture that circuits formed on the
above lattice with this gate set also have the property of
composition tomography boundedness. More generally,
we conjecture that circuits formed with respect to any
lattice with any universal gate set have this property.
We can calculate the probability for any circuit, C using
only r vectors in the following way. First we consider

F ∪ ¯F = rn
pn

X [F] · pn

X [ ¯F]

(14)

where X is the region for fragment F, and ¯F is another
fragment. Also consider

prob( ¯F|Tchoice) =

(cid:88)

l

pn
F [l]∪ ¯F =

(cid:88)

l

X [F[l]] · pn
rn

X [ ¯F]

(15)
where F = F[1] and Tchoice = {F[l] : ∀l} are a set of frag-
ments for X with mutually exclusive and exhaustive set
of outcomes so that there probabilities must add to one.
We denote this set with the subscript “choice” because
it represents the choice we make in X (a diﬀerent choice
would correspond to a diﬀerent mutually exclusive and
exhaustive set). The vector pX [ ¯F] depends on ¯F and is
independent of F[l] in region X. By Bayes rule

probn(F| ¯F, Tchoice) =

X [ ¯F]
X [F] · pn
rn
X [F[l]] · pn
l rn

X [ ¯F]

(cid:80)

(16)

Hence, iﬀ

rn
X [F]

is proportional to

rn
X [F[l]]

(17)

(cid:88)

l

then probn(F| ¯F, Tchoice) is independent of ¯F. We denote
this probability by pn

and it is given by

F |Tchoice

X [F] = pn
rn

F |Tchoice

rn
X [F[l]]

(cid:88)

l

(18)

We can apply this approach to the case where F and ¯F
are circuits. If we have ideal circuits then it is true in

12

Quantum Theory (and circuit theories in general [35])
that

pC∪ ¯C = pCp ¯C

(19)

From this property it can be proven that the proportion-
ality condition in (17) must hold and hence the probabil-
ity for a circuit is given by

rX [C] = pC

(cid:88)

rX [C[l]]

(20)

l

Here we have dropped the possible dependence of pC on
the choice of mutually exclusive circuit set (it is possible
to prove that there is no such dependence). For non-ideal
circuits, however, the proportionality condition of (17)
may fail even when F is a circuit because we have not
fully trained it to behave like a circuit. Nevertheless, if
the proportionality condition holds approximately, then
we can bound the probability [27].
We now prove the following theorem.

Theorem 1.
If we have fragment to-
mography boundedness and composition to-
mography boundedness, then there exists a
bounded set of circuits Stom such that if, at
some iteration n = ˜n, we have

p˜n
C = pC ∀C ∈ Stom

then p˜n

C = pC for any C ∈ Scircuits.

X

Recall that pC are the probabilities for idealised circuits
when the gates g are actually those in our UGS, G. We
can obtain these probabilities by calculation using Quan-
tum Theory. To calculate p˜n
C for an arbitrary circuit we
need to be able to calculate the rn
X vectors for the re-
gion associated with C. We can do this in the following
way. If we have ΛΛΛX1,X2,...
where X1, X2, . . . live inside
bounded boxes and we can also calculate rX1, r2, . . . for
these same bounded boxes and then we can calculate rX
vectors for region X, and from these we can calculate
pC. We can obtain these rXi vectors from probabilities
for bounded circuits by fragment tomography bounded-
ness. If we have composition tomography boundedness
then we can obtain ΛΛΛX1,X2,...
from ΛΛΛ’s that pertain to
bounded boxes. Furthermore, we can determine these
ΛΛΛ’s that live in bounded boxes by constructing circuits
that live in bigger, but still bounded, boxes by fragment
tomographic locality. Thus, all the circuits we need to
do tomography live in bounded boxes. If we obtain the
probabilities p˜n
C = pC for this set of circuits then, by the
above argument, we will obtain rn
X [F] = rX [F] for any X
and F. We can calculate the probability for any circuit
using these r vectors and hence this proves the theorem.

X

[1] I. L. Chuang and M. A. Nielsen, Journal of Modern Op-

tics 44, 2455 (1997).

[2] J. L. O’Brien, G. J. Pryde, A. Gilchrist, D. F. V. James,
N. K. Langford, T. C. Ralph, and A. G. White, Phys.

Rev. Lett. 93, 080502 (2004).

org.

[3] J. F. Poyatos, J. I. Cirac, and P. Zoller, Phys. Rev. Lett.

[30] D. Rumelhart, G. Hinton, and R. Williams, Nature 323,

78, 390 (1997).

533 (1986).

13

[31] D. E. Rumelhart and J. L. McClelland, “Learning in-
ternal representations by error propagation,” in Parallel
Distributed Processing: Explorations in the Microstruc-
ture of Cognition: Foundations (MITP, 1987) pp. 318–
362.

[32] S. Hochreiter and J. Schmidhuber, Neural Comput. 9,

1735 (1997).

[33] A.

Sherstinsky, CoRR abs/1808.03314

(2018),

arXiv:1808.03314.

[34] P. G. Mana, arXiv preprint quant-ph/0305117 (2003).
[35] L. Hardy, arXiv preprint arXiv:1104.2066 (2011).

[4] L. M. Artiles, R. D. Gill, and M. I. Guta, J. Royal Stat.

Soc. 67, 109 (2005).

[5] G. M. D’Ariano, M. G. A. Paris, and M. F. Sacchi, Adv.

Imag. Elect. Phys. 128, 205 (2003).

[6] M. A. Nielsen and I. L. Chuang, Quantum Computation
and Quantum Information: 10th Anniversary Edition,
10th ed. (Cambridge University Press, New York, NY,
USA, 2011).

[7] E. Knill, D. Leibfried, R. Reichle, J. Britton, R. B.
Blakestad, J. D. Jost, C. Langer, R. Ozeri, S. Seidelin,
and D. J. Wineland, Physical Review A 77, 012307
(2008).

[8] J. Emerson, M. Silva, O. Moussa, C. Ryan, M. Laforest,
J. Baugh, D. G. Cory, and R. Laﬂamme, Science 317,
1893 (2007).

[9] J. Emerson, R. Alicki, and K. ˙Zyczkowski, Journal of Op-
tics B: Quantum and Semiclassical Optics 7, S347 (2005).
[10] E. Magesan, J. M. Gambetta, and J. Emerson, Physical

review letters 106, 180504 (2011).

[11] S. T. Merkel, J. M. Gambetta, J. A. Smolin, S. Poletto,
A. D. C´orcoles, B. R. Johnson, C. A. Ryan, and M. Stef-
fen, Phys. Rev. A 87, 062119 (2013).

[12] R. Blume-Kohout, J. K. Gamble, E. O. Nielsen,

J. Mizrahi, J. D. Sterk, and P. Maunz (2013).

[13] O.

di Matteo,

PIRSA:19070074

(available

at

http://pirsa.org/19070074/) (2019).

[14] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost,
N. Wiebe, and S. Lloyd, Nature 549, 195 (2017).
[15] M. Schuld, I. Sinayskiy, and F. Petruccione, Contempo-

rary Physics 56, 172 (2015).

[16] H. Jaﬀali and L. Oeding, ArXiv abs/1908.10247

(2019).

[17] J. Carrasquilla, G. Torlai, R. Melko, and L. Aolita, Na-

ture Machine Intelligence 1, 155 (2019).

[18] M. J. S. Beach, I. D. Vlugt, A. Golubeva, P. Huembeli,
B. Kulchytskyy, X. Luo, R. G. Melko, E. Merali, and
G. Torlai, SciPost Phys. 7, 9 (2019).

[19] G. Torlai, G. Mazzola, J. Carresquilla, M. Troyer,
R. Melko, and G. Carleo, Nature Phys 14, 447 (2018).
[20] Y. Quek, S. Fort, and H. K. Ng, ArXiv abs/1812.06693

(2018).

[21] Q. Xu and S. Xu, ArXiv abs/1811.06654 (2018).
[22] The loss function will, strictly speaking, be a functional
of the encoder/decoder pair. These will eventually be rep-
resented as neural networks, however, parameterized a
ﬁnite set of weights. The loss will be a plain old function
of these.

[23] L. Hardy, Philos. Trans. R. Soc. A 370 (2012),

http://doi.org/10.1098/rsta.2011.0326.

[24] L. Hardy, arXiv preprint gr-qc/0509120 (2005).
[25] L. Hardy, Journal of Physics A: Mathematical and The-

oretical 40, 3081 (2007).

[26] L. Hardy, in Quantum reality, relativistic causality, and
closing the epistemic circle (Springer, 2009) pp. 379–401.
[27] S. Markes and L. Hardy, in Journal of Physics: Confer-
ence Series, Vol. 306 (IOP Publishing, 2011) p. 012043.
[28] Y. L. Cun, Y. Bengio, and G. Hinton, Nature 521, 436

(2015).

[29] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learn-
ing (MIT Press, 2016) http://www.deeplearningbook.

