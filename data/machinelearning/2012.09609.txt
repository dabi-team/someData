0
2
0
2

c
e
D
2
1

]

G
L
.
s
c
[

1
v
9
0
6
9
0
.
2
1
0
2
:
v
i
X
r
a

DRAW YOUR NEURAL NETWORKS

Jatin Sharma
Microsoft Research
Redmond, WA 98052
jatin.sharma@microsoft.com

Shobha Lata
Microsoft Corporation
Redmond, WA 98052
shobha.lata@microsoft.com

ABSTRACT

Deep Neural Networks are the basic building blocks of modern Artiﬁcial Intelligence. They are
increasingly replacing or augmenting existing software systems due to their ability to learn directly
from the data and superior accuracy on variety of tasks. Existing Software Development Life Cycle
(SDLC) methodologies fall short on representing the unique capabilities and requirements of AI
Development and must be replaced with Artiﬁcial Intelligence Development Life Cycle (AIDLC)
methodologies. In this paper, we discuss an alternative and more natural approach to develop neural
networks that involves intuitive GUI elements such as blocks and lines to draw them instead of
complex computer programming. We present Sketch framework, that uses this GUI-based approach
to design and modify the neural networks and provides interoperability with traditional frameworks.
The system provides popular layers and operations out-of-the-box and could import any supported pre-
trained model making it a faster method to design and train complex neural networks and ultimately
democratizing the AI by removing the learning curve.

Keywords Graphical User Interfaces · Deep Neural Networks · AI-Development Life Cycle

1

Introduction

Deep Neural Networks have high expressibility and learn directly from the data without any feature engineering.
This makes them highly versatile and the foundational blocks for the modern Artiﬁcial Intelligence. These networks
comprise of dozens and sometimes hundreds of self-learning layers stacked one after another. Each of these layers
have weights, biases and several other learnable parameters that are adjusted through the back-propagation algorithm
[1][2] during the network’s training phase to learn useful patterns. Creation of these networks is generally complex and
requires expertise in computer programming, mathematics and application-speciﬁc domain. For example, deep neural
networks for computer vision generally deploy convolutional layers [3] which look at spatial patterns, however, the
networks for time-series analysis or language modeling require recurrent layers (e.g. RNN [1][4][5], LSTM [6][5],
BERT [7]) which gather temporal patterns and long-term dependencies. A network for video processing may use both
convolutional and recurrent layers. Some systems may deploy attention [8] based layers that look out for correlations
between input and desired output. Therefore, it requires a high learning curve, in-depth technical background and years
of experience before making any meaningful contribution in designing novel architectures. There exist numerous deep
learning frameworks such as PyTorch [9], Tensorﬂow [10], Keras [11], Caffe [12], Torch [13], MXNet [14] and so
on that provide extensive capabilities to design and train neural networks but they are programming-based and are
generally not interoperable. Recently, ONNX [15] project has initiated a formal discussion and collaboration across the
industry to encourage interoperability between these platforms. There is also a lack of change management system that
could track the modiﬁcations in the existing networks and provide a version control such that it becomes possible to go
back and forth between the generations of the trained neural networks.

This paper proposes Sketch framework that try to bridge this gap in the technology and provides an alternative approach
for the AI development. Instead of computer programming which requires language speciﬁc syntax and structure, the
system uses intuitive GUI elements such as blocks and lines to represent layers and their interconnections. Conceptually,
it is similar to ﬂow-charts where a user can just draw a neural network instead of programming it and the resulting
neural network is a visual sketch instead of a long monolithic piece of code. Thus providing an intuitive and much faster
approach to neural network development. The system can also import and export models from major deep learning
frameworks (e.g. PyTorch, Torch, ONNX) providing a standard interface for interoperability. Most of the applications

 
 
 
 
 
 
require some standard layers (e.g. convolution layer, fully-connected layer, pooling, non-linearity), loss functions (e.g.
mean square error, absolute loss) and other operations (e.g. batch normalization, dropout, identity). The proposed
system come up with these standard layers and operations out-of-the-box and thus provides a much faster method to
design and train complex neural networks without the technical complexities.

2 Related Work

Computer programming based neural network development has been the prominent approach in AI-Development so
far. There are several deep learning frameworks which are publicly available. Some of them include PyTorch [9],
Tensorﬂow [10], Keras [11], Caffe [12], Torch [13] and MXNet [14]. PyTorch and Tensorﬂow has been the most
popular for their ease-of-use and large community support. Another leading reason for the popularity has been attributed
to their support to Python as primary development language which is widely used in the research community due to its
user-friendly syntax and faster learning curve. These frameworks have their custom implementations and generally lack
interoperability which presents a huge roadblock moving from prototyping to production phase. In this direction, Open
Neural Network Exchange (ONNX) [15] is the ﬁrst multi-organization attempt to encourage framework-interoperability
and shared optimization. Today more than 40 organizations support the ONNX initiative which highlights the necessity
for interoperability in AI.

However, there has not been signiﬁcant development in GUI-based AI development and interoperability by design.
There exist some well supported graphical interfaces like Caffe GUI Tool [16], Nvidia Digits [17], Sony Neural Network
Console [18], Mathwork nnstart [19] and Expresso [20] but they lack capabilities of a comprehensive framework. The
authors in [21] developed Barista as an Open Source tool for designing and training neural networks. However, it is
based on Caffe and therefore limited in the user base. Researchers at MIT developed Elegant Neural Network User
Interface (ENNUI) [22] - a browser-based neural network editor that presents a drag and drop interface to create the
models and export to python code. However, this also lacks general usability through the ability to import pre-existing
and pre-trained models and export to interoperable formats. Recently, another tool – Deep Recognition [23] – is
launched as the ﬁrst commercially available GUI based editor for neural network development which provides a more
comprehensive set of functionality including support for autoML and multi-GPU training however it is not open source
and limited to PyTorch and Keras only. We believe that all these tools are paving a path towards a comprehensive
graphical interface based neural network editor that could cater the needs of the AI-Development Life Cycle.

3 AI Development Life Cycle

Software Development Life Cycle (SDLC) is a well established methodology in the ﬁeld of computer science. It
recommends clearly deﬁned processes for creating high quality software. With the rise of Artiﬁcial Intelligence (AI)
and its increasing applications in software development, progress towards an AI-Development Life Cycle (AIDLC) is
natural and roughly follows the same underlying principles as SDLC. However, it also poses unique challenges in 1)
Method of development, 2) Interoperability, 3) Explainability and Debugging, 4) Collaborative Development and 5)
Version Control and Maintenance. A comprehensive AI-Development framework should provide basic functionality
to support these areas. In our current work, we focus on Method of development, Interoperability and Maintenance.
We develop Sketch1– a graphical interface based AI development framework where a user can draw neural networks
instead of writing complex computer code. The system supports importing existing model architectures along with
their pre-trained weights to further improve them by training or benchmarking against new datasets. The system also
supports a plug and play kernel based approach where the user-drawn model is stored as an abstract graph representation
and could be converted to any framework of choice including PyTorch, Torch (Lua) and ONNX. Thus, providing
interoperability by design.

4 System Description

The proposed system has ﬁve major components as depicted in ﬁgure 1. Next, we would discuss each of these
components in detail.

4.1 Graphical User Interface

This component provides the primary interaction between the user and the system. It encompasses all the UI elements
that help in drawing the neural networks and compiling them. Figure 2 demonstrates a screenshot of the system’s

1Please visit https://github.com/jatinsha/sketch for further details and code related to Sketch project.

2

Figure 1: A block diagram illustrating the major components of the Sketch system

Figure 2: A typical example of Sketch’s Graphical User Interface

3

typical graphical user interface. At the very top, there is Title Bar (A) which has minimize, maximize and close buttons
along with the name of the tool. Beneath it lies the Menu Bar (B) which contains various menu options to import/export
models, open/save ﬁles, choose target kernel and update/reset conﬁguration settings. Toolbox (C) is located at the left
side and it contains popular layers. Layers from the toolbox can be used through a ‘Drag & Drop’ method to a Canvas
on the Graphical Editor (D). These individual network layers which look like rectangular blocks could be connected by
clicking on their edges and dragging the link to another layer and thus forming arrow-like inter-connections.

When a network is compiled with the help of a kernel, an output model ﬁle is generated which contains both model
architecture and its parameters. A representation text output is populated in the Text Editor (E). The Sequential layer is
used to create a group of layers or custom computation blocks that could further be edited through Group Canvas (F).
The parameters (e.g. channels, kernel size, stride, padding) of any individual layer could be modiﬁed by clicking at a
layer on the canvas and editing the corresponding values in the Node Editor (G). Located on the bottom right corner,
Directory Tree (H) displays an interactive list of current working directory and can be used to open any ﬁle.

4.2 Graph Abstraction

A user draws a neural network on a canvas in the Graphical Editor using various layers from the Toolbox and linking
them together through inter-connections. To create an inter-connection, the user can click anywhere on the border
of a layer and drag it to another layer. As the user draws the network, an abstract Graph representation of the same
it additively created by the Graph Abstraction component. To accomplish this an Adjacency List representation is
maintained where the layers and their inter-connections are used as graph nodes and edges. Each node contains its
nodeId and lists for prior and next connections. An extensive set of processing happens in the background for various
events such as key bindings for shortcuts, selection and grouping of multiple elements, their positioning and deletion –
making it intuitive for the user to work with the system. Figure 3 illustrates an example adjacency list for the network
shown in ﬁgure 2.

Figure 3: An example of adjacency list representation used in Sketch’s Graph Representation

4.3 Binder

The binder is the most crucial component of the Sketch system. It acts similar to a compiler and converts the high-level
abstract graph representation of the deep neural network into a framework-speciﬁc model architecture. The choice of
target framework is deﬁned through a kernel which could be changed by the user at any time. For an instance, if we
draw a sequential neural network containing Convolution →BatchNormalization →ReLU →MaxPool →Convolution
layers and pick PyTorch as the kernel then compiling the canvas would generate a PyTorch model as *.pth ﬁle along
with a textual PyTorch representation in textual editor. If we switch the kernel to ONNX and re-compile the same
canvas then it would output an *.onnx model and ONNX representation of the underlying neural network.

The Binder interface exposes two methods – exportModel and importModel. The exportModel method provides basic
capability to convert the Graph Abstraction into framework-speciﬁc representation. On the other hand, importModel
enables loading any pre-existing model from the supported frameworks and converting them into the system’s Graph
Abstraction and a sketch on the canvas. This provides a tremendous capability to this tool to not only create new
model architectures but also to use and modify huge number of pre-trained models available in the research community.
This Binder interface is implemented by various kernels which provide the actual low-level conversion logic and
weight transfer between the frameworks. The Sketch system currently supports PyTorch, Torch (Lua) and ONNX
through implementing corresponding kernels. Similar kernels could be implemented for TensorFlow, Caffe or any other
framework pretty easily as a plug and play method.

4.4 State Manager

The State Manager is responsible for tracking the system state. At all time, the crucial state information such as system
version, current working directory path and list of open tabs, canvas state and so on is tracked. When the editor is closed
this state information is saved to the disk and reloaded whenever the edited is reopened. Using this mechanism, Sketch
editor try to restore the open ﬁles and canvases which were being worked on in the previous session. Every modiﬁcation
on the canvas is recorded by a checkpointing mechanism under the state manager and is used to provide the undo and
redo functionalities. Figure 4 illustrates they key steps in Sketch-based AI development.

4

Figure 4: A ﬂowchart diagram illustrating the key steps in Sketch-based AI development

4.5 Logs

To provide a debugging and maintenance mechanism crucial programmatical events (e.g. loading/saving ﬁles, changing
kernel, compiling canvas) are logged. During an exception, the appropriate stack trace is logged as well. This provides
a sequential trace of various actions and system state which could be used to debug and enhance the system.

5 Conclusions

In the present work, we have discussed the need for research towards AI-Development Life Cycle as existing Software
Development Life Cycle Strategies may not completely represent the unique challenges faced by the AI development.
From computer coding based method to framework-speciﬁc implementations and from trace based debugging to version
control, there exist several areas that need to be reworked in accordance with the unique characteristics of the AI. We
developed Sketch framework which – (1) provides an alternative and natural approach to develop neural networks, and
(2) brings interoperability by design through providing import and export capabilities for major frameworks in a plug &
play fashion. This removes dependency upon the extensive knowledge of underlying deep learning framework and
computer programming language syntax and provides a lego-like functionality to build over existing operators. We
believe such an approach could democratize the AI development. In future, we intend to work towards implementing
In-built Training, Version Control, Collaborative Development and Visual Debugging towards a comprehensive and
one-stop framework for AI Development.

5

References

[1] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by back-propagating

errors. nature, 323(6088):533–536, oct 1986.

[2] Yann LeCun. A theoretical framework for back-propagation. In D. Touretzky, G. Hinton, and T. Sejnowski,
editors, Proceedings of the 1988 Connectionist Models Summer School, CMU, Pittsburg, PA, pages 21–28. Morgan
Kaufmann, 1988.

[3] Yann LeCun and Yoshua Bengio. Convolutional Networks for Images, Speech, and Time Series, page 255–258.

MIT Press, Cambridge, MA, USA, 1998.

[4] M I Jordan. Serial order: a parallel distributed processing approach. technical report, june 1985-march 1986. 5

1986.

[5] Alex Sherstinsky. Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm) network.

Physica D: Nonlinear Phenomena, 404:132306, Mar 2020.

[6] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9:1735–80, 12 1997.
[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short
Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
[8] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, undeﬁnedukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference
on Neural Information Processing Systems, NIPS’17, page 6000–6010, Red Hook, NY, USA, 2017. Curran
Associates Inc.

[9] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito,
Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.
Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelz-
imer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32,
pages 8024–8035. Curran Associates, Inc., 2019.

[10] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay
Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore,
Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and
Xiaoqiang Zheng. Tensorﬂow: A system for large-scale machine learning. In Proceedings of the 12th USENIX
Conference on Operating Systems Design and Implementation, OSDI’16, page 265–283, USA, 2016. USENIX
Association.

[11] François Chollet and others. Keras: The Python Deep Learning library, June 2018.
[12] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama,
and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the 22nd
ACM International Conference on Multimedia, MM ’14, page 675–678, New York, NY, USA, 2014. Association
for Computing Machinery.

[13] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for machine learning. In

BigLearn, NIPS Workshop, 2011.

[14] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang,
and Zheng Zhang. Mxnet: A ﬂexible and efﬁcient machine learning library for heterogeneous distributed systems.
CoRR, abs/1512.01274, 2015.

[15] Junjie Bai, Fang Lu, Ke Zhang, et al. Onnx: Open neural network exchange. https://github.com/onnx/onnx,

2019.

[16] Caffe gui tool. https://github.com/Chasvortex/caffe-gui-tool. Accessed: 2020-12-03.
[17] Nvidia digits. https://developer.nvidia.com/digits. Accessed: 2020-12-03.
[18] Sony neural network console. https://dl.sony.com/. Accessed: 2020-12-03.
[19] Mathwork nnstart. https://www.mathworks.com/help/deeplearning/ref/nnstart.html. Accessed:

2020-12-03.

[20] Ravi Kiran Sarvadevabhatla and R. Venkatesh Babu. Expresso : A user-friendly gui for designing, training and

exploring convolutional neural networks, 2015.

6

[21] Soeren Klemm, Aaron Scherzinger, Dominik Drees, and Xiaoyi Jiang. Barista - a graphical tool for designing and

training deep neural networks, 2018.

[22] Jesse Michel, Zack Holbrook, Stefan Grosser, and Rikhav Shah. Elegant neural network user interface. https:

//github.com/martinjm97/ENNUI.

[23] Deep cognition. https://deepcognition.ai/. Accessed: 2020-12-03.

7

