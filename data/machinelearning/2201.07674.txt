Code Sophistication
From Code Recommendation to Logic Recommendation

Jessie Galasso
jessie.galasso-
carbonnel@umontreal.ca
Université de Montréal, DIRO
Canada

Michalis Famelis
michalis.famelis@umontreal.ca
Université de Montréal, DIRO
Canada

Houari Sahraoui
houari.sahraoui@umontreal.ca
Université de Montréal, DIRO
Canada

2
2
0
2

n
a
J

9
1

]
E
S
.
s
c
[

1
v
4
7
6
7
0
.
1
0
2
2
:
v
i
X
r
a

ABSTRACT
A typical approach to programming is to first code the main execu-
tion scenario, and then focus on filling out alternative behaviors
and corner cases. But, almost always, there exist unusual condi-
tions that trigger atypical behaviors, which are hard to predict in
program specifications, and are thus often not coded. In this pa-
per, we consider the problem of detecting and recommending such
missing behaviors, a task that we call code sophistication. Previous
research on coding assistants usually focuses on recommending
code fragments based on specifications of the intended behavior. In
contrast, code sophistication happens in the absence of a specifica-
tion, aiming to help developers complete the logic of their programs
with missing and unspecified behaviors. We outline the research
challenges to this problem and present early results showing how
program logic can be completed by leveraging code structure and
information about the usage of input parameters.

KEYWORDS
Code sophistication, Code Completion, Automatic Program Repair,
Program Synthesis, Code search

1 INTRODUCTION
When developing a program to solve a task, developers usually be-
gin by coding typical scenarios, i.e., by implementing the behaviors
that should be followed under normal conditions. Subsequently,
they concentrate on atypical scenarios (e.g., failures, corner cases)
by expanding the program with the appropriate alternative be-
haviors. Because these atypical scenarios correspond to unusual
conditions, they may be omitted from specification documents [41],
and are difficult to predict during implementation [48]. Even taking
into account testing, such behaviors are difficult to detect as they
are rarely triggered during executions [19, 37]. Unsurprisingly, they
are one of the largest source of software defects [7, 11, 31, 41]. It
is thus crucial to help developers uncover such omitted scenarios
and handle them by coding the appropriate corresponding program
behaviors. These alternative behaviors may include error handling,
variable assignations, early returns or more complex operations.

In this paper, we outline the problem of recommending unspec-
ified behaviors to handle omitted scenarios, which we call code
sophistication, and present the results of our initial attempt to ad-
dress it. The goal of code sophistication is to recommend missing
parts of a program; we argue that finding missing behaviors cannot
be properly handled by existing code recommendation approaches
(see Section 2). Such approaches typically rely on the existence of

some kind of specification of what the program should be doing,
and use it to suggest appropriate code fragments. But in the case
of code sophistication, no specification of the missing behaviors
can be assumed. Thus the scope of code sophistication goes beyond
offering developers assistance about how to code but also assis-
tance about what to code to complete the logic of their program.
We propose an approach to leverage the knowledge embedded in
large repositories of existing code to learn how to complete the
logic of programs. We then present a preliminary implementation
with promising results that allows using this knowledge to infer
potential missing behaviors in new programs.

The paper is organized as follows: We cover related code rec-
ommendation techniques in Section 2. We define and motivate the
problem of code sophistication, and outline the research directions
for addressing it in Section 3. We present our preliminary approach
and results in Section 4, and conclude in Section 5.

2 RELATED WORK
We can put techniques for assisting developers into completing their
programs into four categories: code completion, program synthesis,
automatic program repair, and code search. We explain why these
techniques do not address code sophistication.

Code completion focuses on predicting the end of a token
or the next tokens at a particular location in the code by relying
on the code already written [2]. Source code language models ex-
tracted from large code bases represent probability distributions
over sequences of tokens; they were used to predict the next tokens
given the previous ones [18, 20, 50], identifiers [5, 30], function-
calls [53] or to infer meaningful code units (e.g., method call, binary
expression) [39]. Similar methods were used for more advanced pre-
dictions, such as API calls [6, 40], sequence of API invocations [44]
or even syntactic templates for API usages [38]. Code completion
approaches do not rely on some explicit specification of what parts
of the code should be recommended. However they usually leverage
low level syntactic and lexical features that are not sufficient to
infer the logic of a program, much less to complete it, which is the
aim of code sophistication. The approach closest to sophistication
is the work on inferring syntactic templates [38] but it is restricted
to specific API usages.

Program synthesis is about "discovering programs that real-
ize user intent expressed in the form of some specifications" [14].
The latter may be logical specifications, descriptions in natural
language, input-output examples or test cases. Early approaches
relied on extensive specifications [12], but more recent inductive
approaches rather aim at producing generalization from partial
specifications [28]. Inductive program synthesis is usually seen as

 
 
 
 
 
 
Code Sophistication

Galasso, Famelis and Sahraoui

a search problem, where the goal is to explore the set of programs
to find the one corresponding to the provided specification. Ap-
proaches based on deep learning extract knowledge from large code
bases, for instance to generate domain specific programs based on
input/output examples [15, 47]. Other approaches rely on neural
machine translation to translate functionality descriptions in natu-
ral language into executable code in domain specific languages [55],
general purpose languages for API usage sequences [13], or more
generalized purpose [32, 54]. Automatic program repair is a sub-
field of program synthesis defined as "the transformation of an
unacceptable behavior of a program into a acceptable one according
to a specification" [36]. The goal of automatic program repair is to
recommend appropriate patches to modify the part of the program
responsible of an identified defect. Some approaches use repair
templates to address specific classes of defects [4, 24, 33], while
others rely on genetic programming and mutations to find appro-
priate patches [3, 52]. Code Phage [48] is a system transferring
correct code from programs passing a set of test cases to programs
in which defects were detected. Program synthesis and program re-
pair approaches rely on specifications that characterize the missing
behavior, such as test cases and textual descriptions. Code recom-
mendation is then akin to suggesting code fragments implementing
an outlined behavior. In other words, it helps developers determine
how to code for a particular scenario. On the contrary, code sophis-
tication aims to suggest missing conditional paths corresponding
to unspecified behaviors, relying only on the program under devel-
opment. Note that some approaches in automatic program repair
generate patches without relying on specifications [4, 17, 51]; how-
ever, they only target specific classes of defects.

When facing unfamiliar programming tasks, developers often
seek code snippet examples; code search aims to suggest such
relevant snippets. Most code search approaches are based on in-
formation retrieval techniques to find relevant code snippets de-
pending on a query either formulated directly by the developer
or inferred from the code [1]. Some approaches focus on recom-
mending specific types of snippets, such as framework usages [21],
Java methods [1], exception handling examples [43], auxiliary func-
tionalities [29] or API usages [42], while others are general pur-
pose [22, 23, 25, 35, 45, 49]. Approaches inferring a query from the
code can be based on tokens and/or statements similarity [1, 49],
structural code features [21, 25, 35, 43], or lexical ones [43]. Code
search approaches which are the closest to code sophistication are
the ones in which no query is formulated directly and which rely
only on the code under development to suggest code fragments.
These approaches search for similar fragments and use the provided
program as a specification of the intended behavior. In contrast,
code sophistication focuses on completing the logic of a program
with behaviors that were missed. These behaviors are by definition
assumed to be absent from the code. Code search approaches are
thus not applicable for sophistication.

3 CODE SOPHISTICATION
In this section, we first lay down and motivate the problem of code
sophistication. We then give directions for addressing it.

3.1 Definitions and Motivations
The set of behaviors of a program, sometimes referred as the pro-
gram’s logic, can be outlined through execution paths, pseudocode,
or use cases. Execution paths offer an interesting perspective in our
case, because we can think of each path as one behavior, depending
on the conditions that delimit in which scenario it is executed [26].
In other words, the choice of an execution path is determined by the
program’s inputs and the conditions they satisfy. Those inputs may
be explicit (e.g., parameters, attributes) or implicit (e.g., time). Each
conditional path thus defines how to behave in a scenario circum-
scribed by the values of the inputs. Common scenarios correspond
to combinations of input values that are typically observed, whereas
atypical scenarios correspond to uncommon combinations which
happen rarely. Recommending alternative behaviors to handle omit-
ted scenarios can thus be defined as suggesting missing conditional
paths targeting specific combinations of input’s values.

In software testing, missing conditional paths are known as a
particular class of defects which do not reside in the written code,
but in the absence of a particular fragment [37]. Several studies have
analyzed bug fixes of large projects and report that missing paths
are a particularly abundant class of defects [7, 11, 31, 41], known
to be hard to predict and detect. In The Art of Software Testing [37],
Myers et al. report that exhaustive path testing, a common approach
to test the logic of a program, might not uncover errors caused by
missing paths. Hemmati showed that indeed the largest category of
errors that remain undetected by code coverage criteria are "faults
of omission" related to missing conditional paths [19]. Chen et al.
studied dormant bugs, i.e., bugs introduced in a version of a system
and not reported before after the release of the next version [9].
They found that 52% of dormant bugs are related to corner cases
and control flow, while only 11% of the non-dormant bugs concern
these categories, suggesting they take longer to be exposed. The
reasons for this are that scenarios which are not present in the
requirements are difficult to detect due to a lack of "local clues about
the omission" [8] or because they necessitate particular conditions
to be triggered [41]. These suggest that missing conditional paths
indeed correspond to omitted scenarios.

To sum up, the literature provides evidence that missing condi-
tional paths corresponding to omitted scenarios are one of the most
occurring class of defects found in software projects, and are par-
ticularly difficult to detect by both manual and automated software
testing approaches. Detecting and patching missing behaviors is
thus an important and challenging issue. In addition, we showed
in Section 2 that existing code recommendation approaches, while
providing valuable avenues to investigate, are not sufficient to rec-
ommend missing behaviors in the general case. In what follows,
we provide directions for addressing this problem.

3.2 Toward Logic Recommendation
We hypothesize that knowledge about programs’ logic can be de-
rived from available code in large project repositories. More specifi-
cally, we aim at learning sophistication patterns from recurring code
changes across project histories. We consider that commits adding
conditional paths, as illustrated in the Python method in Fig. 1 (a),
are good candidates to learn sophistication patterns. Such patterns

Code Sophistication

Galasso, Famelis and Sahraoui

Figure 1: Processing a commit’s method1 to detect extension points (level 1) and characterize missing behaviors (level 2).

should characterize alternative behaviors (i.e., what should be added)
and their context (i.e., where and when it should be added).

To achieve generalizability of sophistication patterns across dif-
ferent projects and domains, it is necessary to work with high level
characterization of behaviors, independent from implementation
concerns. For example, consider two applications, one for the re-
serving travel tickets, one for banking. The reservation application
tests if the date of departure is before the date of return; the banking
application tests if the amount of money to be withdrawn exceeds
the bank balance. Despite having domain specific implementations,
these behaviors are logically similar: they both raise an error if one
input value is superior to another. Their characterization must be
at a sufficient level of abstraction to reflect this similarity.

Because a program’s logic pertains to its structure, leveraging
structural information is crucial to depict the context of alternative
behaviors. We are currently investigating the use of structural rep-
resentations of the code, notably the control flow graph (CFG). A
CFG is a graph representing the flow (directed edges) between the
statements (nodes) of a program, portraying its different execution
paths, as shown in Fig. 1 (b) for the same Python method. The
nodes and edges in green represent the path added in the commit.
Predicate nodes have two outgoing edges and divide their ingoing
path into two paths which depend on a condition. Adding a new
path in a program entails adding a predicate node in its correspond-
ing CFG: thus, we consider that each edge of a CFG is a potential
extension point, on which a predicate node can be added to divide
the current path and add an alternative behavior. In Fig 1 (b), we
can see that the added predicate node (represented by an hexagon)
splits the edge coming from the first node to insert a new path: this
edge is an extension point. Also, because the executed behaviors de-
pend on the inputs of the program, we hypothesize that conditional
paths may depend on the usage of inputs through the program, and
we take these usages into account when describing the context in
sophistication patterns.

1Original
a9f620e2508cf9de4f5a405449673d4d50804d80

commit:

https://github.com/3b1b/manim/commit/

Based on these assumptions, we outline the problem of recom-
mending missing behaviors as three levels of increasing complexity.
(Level 1:) Suggest possible extension points in a program. Extension
points show where the current control flow should be diverted
when a certain condition is met to introduce a new behavior. (Level
2:) Given an extension point in the program, suggest a high level
description characterizing the potentially missing behavior. Instead
of suggesting code fragments representing specific implementation
of a behavior, recommended solutions must rather characterize this
behavior with a sufficient level of abstraction to be independent
of domain specific implementation concerns. (Level 3:) Given an
extension point and a characterization of the missing behavior, sug-
gest code templates of proposed implementations of the missing
behavior.

4 IMPLEMENTATION AND VALIDATION
We propose a preliminary implementation of a code sophistication,
based on Graph Convolutional Networks. We present early results
for Levels 1 and 2: detecting potential extension points and char-
acterizing missing behaviors. Figure 1 presents an overview of the
proposed approach.

4.1 Collecting and Encoding Data
The change history of a software project is an important indicator
of the defects which were uncovered and fixed by developers. To
gather information about patterns of program logic completion,
we focused on commits adding conditional statements, as they
can show where a developer added a conditional path that was
missing [41]. We analyzed all commits in the history of 250 Github
projects written in Python, for a total of 1.2M commits, and extract
methods in which an if statement block was added, as illustrated
in Fig. 1 (a). To leverage structural information in the extracted
methods, we construct their control flow graph (CFG), as shown
in Fig.1 (b). In each CFG, we removed the nodes corresponding
to the added path (Fig 1 (c)): the pruned CFG thus corresponds to
the CFG of the method before the commit. We keep track of the

Added statements:{Assign, Call}Inputs: {self.points, point}def start_at(self, point):...self.points[0] = pointreturnselfExtension point?FunctionDefSubscript: self.pointsAssign: self.points, pointReturn: /def start_at(self, point):...if len(self.points) == 0:self.points = np.zeros((1, 3))self.points[0] = pointreturn selfLEVEL 1LEVEL 2(b)(c)(d)(a)Code Sophistication

Galasso, Famelis and Sahraoui

edge corresponding to the extension point (represented in orange
in Fig 1 (c)), as well as the statements present in the path added by
the commit (here an Assign and a Call). Finally, we annotate each
node of the CFG with the usage of the inputs, as illustrated in Fig 1
(d). We considered two types of inputs: the method’s parameters
and the attributes used in its body. In our example, the method has
two inputs: the parameter point and the attribute self.points.
We characterize the usage of inputs by the type of statements in
which they appear. In the middle node of Fig. 1 (c), self.points is
used in Assign and Subscript statements, and point in an Assign
statement. The Return statement of the bottom node does not use
any input. For each method, we thus have a CFG annotated with the
usage of inputs, where one of its edges is identified as an extension
point, associated with the statements from the removed path.

4.2 Implementation of Learning Models
For the two experiments we built classification models taking, as
input, a CFG annotated with input usages. We used the StellarGraph
library [10] to build the graph classification models based on the
graph convolutional layers from Kipf and Welling [27]. Because we
aim to represent an edge of the graph, we divide the CFG in two
parts (before and after the tagged edge, representing the structured
inputs’ usage before and after the extension point, as shown in Fig 1
(d)) and feed the models with these two sub-graphs.

To detect extension points (level 1), we trained a binary clas-
sifier to detect if an edge of a given CFG is an extension point. For
positive examples, we used the extracted graphs for which we know
the edge on which a path was added. For negative examples, we
mined methods for which commits added lines that do not corre-
spond to new paths. We created the same annotated CFGs, but with
edges which did not correspond to extension points. We evaluate
the model with the following metrics: accuracy, precision, recall, F1
measure and area under the curve (AUC). The AUC score measures
the probability that our classifier will rank an edge corresponding
to an extension point higher that an edge which does not.

To characterize the missing behavior (level 2), we trained a
multi-label classifier to identify the types of statements that should
be used in the behavior to be added at a given extension point. We
selected 8 recurring types of statements as defined in the Python
AST module 2 as our classes – Return, Assign, AugAssign, Raise,
If, Call, Subscript and BinOp – and labeled each method with
the statements appearing in the commit added lines. We compared
the results with two state-of-the-art models for code completion in
Python, GraphCodeBERT [16] and CodeGPT [34]. Because these
two models do not suggest types of statements, we generate for each
method the same number of tokens added in the commit. We then
extracted the types of statements from the models’ suggestions to
obtain a baseline for comparison. We evaluate the three models with
the following metrics: AUC, macro and micro F1, and Hamming
loss. AUC scores, in the multilabel case, represent an average of the
AUC of each class. Macro F1 is the average of F1 measure for each
class, while micro F1 is the F1 measure computed on all examples
and for all classes. The Hamming loss represents how many times
a pair edge-label is misclassified: the lower the loss the better.

2https://docs.python.org/3/library/ast, accessed in July 2021

Accuracy AUC Precision Recall
.717

.727

.817

.730

F1
.721

Table 1: Results for the detection of extension points.

Metrics
AUC
Macro F1
Micro F1
Hamming loss

CC(CodeGPT) CC(GraphCodeBERT)
.561
.072
.095
.264

.564
.309
.333
.308

CS
.750
.358
.397
.237

Table 2: Results for the characterization of behaviors.

4.3 Results
For the two experiments, we performed a 5-fold cross validation: the
presented results are the averages for the 5 folds. Table 1 presents
the results for the model detecting extension points. A classifier is
often considered suitable if its AUC score is above 0.7 [46]. Table 2
compares the results obtained with the two state-of-the-art models
for code completion (denoted CC) and our multilabel classifier
for code sophistication (denoted CS). We can see that our model
provides better results than the two baselines.

In both cases, we have encouraging results suggesting that a)
knowledge about programs’ logic can be learned from code repos-
itories and b) structural code information and input’s usage are
adequate to identify extension points and characterize missing
behaviors, without relying on specifications.

5 CONCLUSION AND FUTURE WORK
We defined and motivate the problem of code sophistication, i.e.,
completing programs with missing behaviors that were neither
specified nor predicted. We discussed how existing code recommen-
dation approaches are not suitable for this problem and proposed
an approach to recommending appropriate behaviors that relies
on knowledge learned from large code repositories. We presented
early results demonstrating we could detect extension points and
characterize missing behaviors by learning from the structure and
the usage of inputs from commits adding conditional paths.

Our next steps are to study in detail the added lines in the com-
mits of our dataset to provide a characterization of the missing
alternative behaviors, with descriptions independent from imple-
mentation and domain concerns. This knowledge is crucial to better
understand what parts of program logic we should aim to infer for
code sophistication. We are currently investigating the approaches
used for related tasks, especially code completion and code search,
with the goal of adapting and extending the more relevant ones to
recommend alternative behaviors and code templates (Level 3). This
will help us further determine which code properties are decisive
to detect and infer missing behaviors. In the long term, we aim
to study how code sophistication could be efficiently carried out
by recommendation systems to successfully assist developers into
handling omitted scenarios.

REFERENCES
[1] Lei Ai, Zhiqiu Huang, Weiwei Li, Yu Zhou, and Yaoshen Yu. 2019. Sensory: Lever-
aging code statement sequence information for code snippets recommendation.
In 43rd Ann. Computer Software and Applications Conference, Vol. 1. IEEE, 27–36.
[2] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018.
A survey of machine learning for big code and naturalness. ACM Computing

Code Sophistication

Galasso, Famelis and Sahraoui

Surveys (CSUR) 51, 4 (2018), 1–37.

[3] Andrea Arcuri. 2011. Evolutionary repair of faulty software. Applied soft com-

puting 11, 4 (2011), 3494–3514.

[4] Johannes Bader, Andrew Scott, Michael Pradel, and Satish Chandra. 2019. Getafix:
Learning to fix bugs automatically. Proceedings of the ACM on Programming
Languages 3, OOPSLA (2019), 1–27.

[5] Avishkar Bhoopchand, Tim Rocktäschel, Earl Barr, and Sebastian Riedel. 2016.
Learning python code suggestion with a sparse pointer network. arXiv preprint
arXiv:1611.08307 (2016).

[6] Marcel Bruch, Martin Monperrus, and Mira Mezini. 2009. Learning from examples
to improve code completion systems. In Proceedings of the 7th joint meeting of
the European software engineering conference and the ACM SIGSOFT symposium
on the foundations of software engineering. 213–222.

[7] Ray-Yaung Chang, Andy Podgurski, and Jiong Yang. 2007. Finding what’s not
there: a new approach to revealing neglected conditions in software. In Proc. of
the 2007 international symposium on Software testing and analysis. 163–173.
[8] Ray-Yaung Chang, Andy Podgurski, and Jiong Yang. 2008. Discovering neglected
IEEE Transactions on

conditions in software by mining dependence graphs.
Software Engineering 34, 5 (2008), 579–596.

[9] Tse-Hsun Chen, Meiyappan Nagappan, Emad Shihab, and Ahmed E Hassan.
2014. An empirical study of dormant bugs. In Proceedings of the 11th Working
Conference on Mining Software Repositories. 82–91.

[10] CSIRO’s Data61. 2018. StellarGraph Machine Learning Library. https://github.

com/stellargraph/stellargraph.

[11] Joao A Duraes and Henrique S Madeira. 2006. Emulation of software faults: A
field data study and a practical approach. Ieee transactions on software engineering
32, 11 (2006), 849–867.

[12] Cordell Green. 1981. Application of theorem proving to problem solving. In

Readings in Artificial Intelligence. Elsevier, 202–222.

[13] Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim. 2016. Deep
API learning. In Proceedings of the 2016 24th ACM SIGSOFT International Sympo-
sium on Foundations of Software Engineering. 631–642.

[14] Sumit Gulwani. 2010. Dimensions in program synthesis. In 12th Int. ACM SIG-
PLAN Symposium on Principles and practice of declarative programming. 13–24.
[15] Sumit Gulwani, William R Harris, and Rishabh Singh. 2012. Spreadsheet data

manipulation using examples. Commun. ACM 55, 8 (2012), 97–105.

[16] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long
Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. 2020. Graphcodebert:
Pre-training code representations with data flow. arXiv:2009.08366 (2020).
[17] Hideaki Hata, Emad Shihab, and Graham Neubig. 2018. Learning to generate

corrective patches using neural machine translation. arXiv:1812.07170 (2018).

[18] Vincent J Hellendoorn and Premkumar Devanbu. 2017. Are deep neural networks
the best choice for modeling source code?. In Proceedings of the 2017 11th Joint
Meeting on Foundations of Software Engineering. 763–773.

[19] Hadi Hemmati. 2015. How effective are code coverage criteria?. In 2015 Interna-

tional Conference on Software Quality, Reliability and Security. IEEE, 151–156.

[20] Abram Hindle, Earl T Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu.
2016. On the naturalness of software. Commun. ACM 59, 5 (2016), 122–131.
[21] Reid Holmes and Gail C Murphy. 2005. Using structural context to recommend
source code examples. In 27th Int. Conf. on Software engineering. 117–125.
[22] He Jiang, Liming Nie, Zeyi Sun, Zhilei Ren, Weiqiang Kong, Tao Zhang, and
Xiapu Luo. 2016. Rosf: Leveraging information retrieval and supervised learning
for recommending code snippets. Trans. on Services Computing 12 (2016), 34–46.
[23] Renhe Jiang, Zhengzhao Chen, Zejun Zhang, Yu Pei, Minxue Pan, and Tian Zhang.
2018. Semantics-based code search using input/output examples. In 18th Int.
Working Conf. on Source Code Analysis and Manipulation (SCAM). IEEE, 92–102.
[24] Dongsun Kim, Jaechang Nam, Jaewoo Song, and Sunghun Kim. 2013. Automatic
patch generation learned from human-written patches. In 2013 35th International
Conference on Software Engineering (ICSE). IEEE, 802–811.

[25] Kisub Kim, Dongsun Kim, Tegawendé F Bissyandé, Eunjong Choi, Li Li, Jacques
Klein, and Yves Le Traon. 2018. FaCoY: a code-to-code search engine. In Proceed-
ings of the 40th International Conference on Software Engineering. 946–957.
[26] James C King. 1976. Symbolic execution and program testing. Commun. ACM

19, 7 (1976), 385–394.

[27] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph

convolutional networks. arXiv preprint arXiv:1609.02907 (2016).

[28] Emanuel Kitzelmann. 2009. Inductive programming: A survey of program syn-
thesis techniques. In International workshop on approaches and applications of
inductive programming. Springer, 50–73.

[29] Otávio Augusto Lazzarini Lemos, Sushil Bajracharya, Joel Ossher, Paulo Cesar
Masiero, and Cristina Lopes. 2011. A test-driven approach to code search and
its application to the reuse of auxiliary functionality. Information and Software
Technology 53, 4 (2011), 294–306.

[30] Jian Li, Yue Wang, Michael R Lyu, and Irwin King. 2017. Code completion with

neural attention and pointer networks. arXiv preprint arXiv:1711.09573 (2017).

[31] Zhenmin Li, Lin Tan, Xuanhui Wang, Shan Lu, Yuanyuan Zhou, and Chengxiang
Zhai. 2006. Have things changed now? An empirical study of bug character-
istics in modern open source software. In Proceedings of the 1st workshop on

Architectural and system support for improving software dependability. 25–33.

[32] Wang Ling, Phil Blunsom, Edward Grefenstette, Karl Moritz Hermann, Tomás
Kociský, Fumin Wang, and Andrew W. Senior. 2016. Latent Predictor Networks
for Code Generation. In Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics, ACL 2016.

[33] Fan Long and Martin Rinard. 2015. Staged program repair with condition syn-

thesis. In 10th Joint Meeting on Foundations of Software Engineering. 166–178.

[34] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambro-
sio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021.
CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding
and Generation. arXiv preprint arXiv:2102.04664 (2021).

[35] Sifei Luan, Di Yang, Celeste Barnaby, Koushik Sen, and Satish Chandra. 2019.
Aroma: Code recommendation via structural code search. Proceedings of the ACM
on Programming Languages 3, OOPSLA (2019), 1–28.

[36] Martin Monperrus. 2018. Automatic software repair: a bibliography. ACM

Computing Surveys (CSUR) 51, 1 (2018), 1–24.

[37] Glenford J Myers, Corey Sandler, and Tom Badgett. 2011. The art of software

testing. John Wiley & Sons.

[38] Anh Tuan Nguyen and Tien N Nguyen. 2015. Graph-based statistical language
model for code. In 37th Int. Conf. on Software Engineering, Vol. 1. IEEE, 858–868.
[39] Tung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N Nguyen.
2013. A statistical semantic language model for source code. In Proceedings of the
9th Joint Meeting on Foundations of Software Engineering. 532–542.

[40] Sebastian Proksch, Johannes Lerch, and Mira Mezini. 2015.

Intelligent code
completion with Bayesian networks. ACM Transactions on Software Engineering
and Methodology (TOSEM) 25, 1 (2015), 1–31.

[41] Shruti Raghavan, Rosanne Rohana, David Leon, Andy Podgurski, and Vinay
Augustine. 2004. Dex: A semantic-graph differencing tool for studying changes
in large code bases. In 20th Int. Conf. on Software Maintenance. IEEE, 188–197.

[42] Mukund Raghothaman, Yi Wei, and Youssef Hamadi. 2016. Swim: Synthesizing
what i mean-code search and idiomatic snippet synthesis. In 2016 IEEE/ACM 38th
International Conference on Software Engineering (ICSE). IEEE, 357–367.

[43] Mohammad Masudur Rahman and Chanchal K Roy. 2014. On the use of context in
recommending exception handling code examples. In 2014 IEEE 14th International
Working Conference on Source Code Analysis and Manipulation. IEEE, 285–294.

[44] Veselin Raychev, Martin Vechev, and Eran Yahav. 2014. Code completion with
statistical language models. In Proceedings of the 35th ACM SIGPLAN Conference
on Programming Language Design and Implementation. 419–428.

[45] Steven P Reiss. 2009. Semantics-based code search. In 2009 IEEE 31st International

Conference on Software Engineering. IEEE, 243–253.

[46] Daniele Romano and Martin Pinzger. 2011. Using source code metrics to pre-
dict change-prone java interfaces. In 2011 27th IEEE international conference on
software maintenance (ICSM). IEEE, 303–312.

[47] Reed Scott and N Freitas. 2015. Neural programmer-interpreters. In International

Conference on Learning Representations.

[48] Stelios Sidiroglou-Douskos, Eric Lahtinen, Fan Long, and Martin Rinard. 2015.
Automatic error elimination by horizontal code transfer across multiple appli-
cations. In Proceedings of the 36th ACM SIGPLAN Conference on Programming
Language Design and Implementation. 43–54.

[49] Watanabe Takuya and Hidehiko Masuhara. 2011. A spontaneous code recommen-
dation tool based on associative search. In Proc. of the 3rd International Workshop
on search-driven development: Users, infrastructure, tools, and evaluation. 17–20.
[50] Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu. 2014. On the localness
of software. In 22nd Int. Symp. on Foundations of Software Engineering. 269–280.
[51] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin
White, and Denys Poshyvanyk. 2019. An empirical study on learning bug-fixing
patches in the wild via neural machine translation. ACM Transactions on Software
Engineering and Methodology (TOSEM) 28, 4 (2019), 1–29.

[52] Westley Weimer, ThanhVu Nguyen, Claire Le Goues, and Stephanie Forrest. 2009.
Automatically finding patches using genetic programming. In 2009 IEEE 31st
International Conference on Software Engineering. IEEE, 364–374.

[53] Martin Weyssow, Houari Sahraoui, Benoıt Frénay, and Benoıt Vanderose. 2020.
Combining Code Embedding with Static Analysis for Function-Call Completion.
arXiv preprint arXiv:2008.03731 (2020).

[54] Pengcheng Yin and Graham Neubig. 2017. A Syntactic Neural Model for General-
Purpose Code Generation. In 55th Ann. Meet. of the Association for Computational
Linguistics, Regina Barzilay and Min-Yen Kan (Eds.). ACL, 440–450.

[55] Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating
structured queries from natural language using reinforcement learning. arXiv
preprint arXiv:1709.00103 (2017).

