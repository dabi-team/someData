FOLD-SE: Scalable Explainable AI

Huaduo Wang and Gopal Gupta
Computer Science Department
The University of Texas at Dallas, Richardson, USA
{huaduo.wang, gupta}utdallas.edu

2
2
0
2

g
u
A
6
1

]

G
L
.
s
c
[

1
v
2
1
9
7
0
.
8
0
2
2
:
v
i
X
r
a

Abstract

FOLD-R++ is a highly efﬁcient and explainable rule-based
machine learning algorithm for binary classiﬁcation tasks. It
generates a stratiﬁed normal logic program as an (explain-
able) trained model. We present an improvement over the
FOLD-R++ algorithm, termed FOLD-SE, that provides scal-
able explainability (SE) while inheriting all the merits of
FOLD-R++. Scalable explainability means that regardless of
the size of the dataset, the number of learned rules and learned
literals stay small and, hence, understandable by human be-
ings, while maintaining good performance in classiﬁcation.
FOLD-SE is competitive in performance with state-of-the-
art algorithms such as XGBoost and Multi-Layer Perceptrons
(MLP). However, unlike XGBoost and MLP, the FOLD-SE
algorithm generates a model with scalable explainability. The
FOLD-SE algorithm outperforms FOLD-R++ and RIPPER
algorithms in efﬁciency, performance, and explainability, es-
pecially for large datasets. The FOLD-RM algorithm is an
extension of FOLD-R++ for multi-class classiﬁcation tasks.
An improved FOLD-RM algorithm built upon FOLD-SE is
also presented.

1 Introduction
Dramatic success of machine learning has led to a torrent of
Artiﬁcial Intelligence (AI) applications. However, the effec-
tiveness of these systems is limited by the machines’ cur-
rent inability to explain their decisions and actions to human
users. That is mainly because the statistical machine learn-
ing methods produce models that are complex algebraic so-
lutions to optimization problems such as risk minimization
or data likelihood maximization. Lack of intuitive descrip-
tions makes it hard for users to understand and verify the
underlying rules that govern the model. Also, these methods
cannot produce a justiﬁcation for a prediction they compute
for a new data sample.

Rule-based machine learning (RBML) algorithms have
been devised that learn a set of relational rules that collec-
tively represent the logic of the concept encapsulated in the
data. The generated rules are more comprehensible to hu-
mans compared to the complicated deep learning model or
complicated formulas. Examples of such algorithms include
the FOIL (Quinlan 1990) and RIPPER (Cohen 1995). Some
of these algorithms allow the knowledge learned to be incre-
mentally extended without retraining the entire model. The

learned symbolic rules make it easier for users to understand
and verify them.

The RBML problem can be regarded as a search problem
for a set of rules from the training examples. Usually, the
search for rules can be processed either top-down or bottom-
up. A bottom-up approach starts by creating most-speciﬁc
rules from training examples and exploring the hypothe-
sis space by employing generalization techniques. Bottom-
up approaches are not applicable for large datasets. A top-
down approach starts by building the most-general rules
from training examples and then specializes them into a ﬁ-
nal rule set. Most of the RBML algorithms are not efﬁcient
for large datasets even if they follow a top-down approach.
Some of them, e.g., TREPAN (Craven and Shavlik 1995)
extract rules from statistical machine learning models, but
their performance and efﬁciency is limited by the target
machine learning model. Yet other algorithms train mod-
els with a logic program solver, for example, most of
the Inductive Logic Programming (ILP) based algorithms
(Cropper and Dumancic 2020).

An important aspect of RBML systems is that they should
be scalable, meaning that they should work with large
datasets, and learn the rules in a reasonable amount of time.
For RBML algorithms, the size of the rule-set—represented
by the number of rules and number of conditional predicates
(features) involved in the rules—has a big impact on human-
understanding of the rules (interpretability) and explaining
predictions (explainability). The more rules and predicates
a rule-set representing a model contains, the harder it is for
a human to understand. Generally, as the size of dataset in-
creases, the number of rules and conditional predicate in-
creases. Ideally, we would like for the rule-set size to not
increase with dataset size. We call this concept scalable ex-
plainability, i.e., the size of the rule-set is a small constant
regardless of the dataset size. Thus, even when size of the
input training data is very large, the rule-set representing the
model should be small enough for a human to comprehend.
The FOIL algorithm by Quinlan (Quinlan 1990) is a pop-
ular top-down RBML algorithm. FOIL uses heuristics from
information theory called weighted information gain. The
use of greedy heuristics allows FOIL to run much faster than
bottom-up approaches and scale up much better. The FOLD
algorithm by Shakerin (Shakerin, Salazar, and Gupta 2017)
is inspired by the FOIL algorithm, it learns a default the-

 
 
 
 
 
 
ory with exceptions represented as a stratiﬁed normal logic
program. The FOLD algorithm incrementally generates lit-
erals for default rules that cover positive examples while
avoiding covering negative examples. It then swaps the pos-
itive and negative examples and calls itself recursively to
learn exceptions to the default when there are still posi-
tive examples uncovered. Subsequently, we developed the
FOLD-R++ algorithm (Wang and Gupta 2022) on top of
the FOLD algorithm that utilizes the preﬁx sum compu-
tation technique with a special comparison operators to
speed up literal selection while avoiding one-hot encoding
for mixed-type data. FOLD-R++ also introduced a hyper-
parameter called ratio to speed up training while reduc-
ing the number of generated rules (Wang and Gupta 2022;
Wang, Shakerin, and Gupta 2022).

The FOLD-R++ algorithm is able to generate much fewer
rules than the well-known rule-based ML algorithm RIP-
PER while outperforming it in classiﬁcation accuracy. For
very large datasets, most of the RBML algorithms are not
scalable. They fail to ﬁnish training in a reasonable time.
Even RIPPER and FOLD-R++ algorithms often generate
too many rules making them incomprehensible to humans.
For example, Rain in Australia is a large dataset with over
140K training examples. With the same target class ‘No’,
RIPPER generates 180 rules with over 700 literals that
achieve 63% accuracy while FOLD-R++ generates 48 rules
with around 120 literals that achieve 79% accuracy. That’s
too many rules, arguably, for a human to understand. An-
other explainability-related problem with these RBML al-
gorithms is that the rules they generate change signiﬁcantly
when a small percentage of training data changes. Thus, the
learned rule-set will be different for different splits of the
dataset for training and testing. We would like for the rule-
set to not change as the training dataset changes.

To deal with the above explainability issue on large
datasets, this paper presents an improved FOLD-R++ algo-
rithm that employs a newly created heuristic for literal se-
lection that greatly reduces the number of rules and predi-
cates in the generated rule-set. In addition, we also improve
explainability by introducing a rule pruning mechanism in
the training process. The pruning mechanism ameliorates
the long-tail effect, namely, that rules generated later in the
learning process cover fewer examples than those generated
earlier. Finally, we add two comparison operators that im-
prove the literal selection process for sparse datasets con-
taining many missing values. The improved learning algo-
rithm, that we call FOLD-SE, provides scalable explainabil-
ity. FOLD-SE generates a rule-set that uses a small number
of rules and predicates (features) regardless of the dataset
size. Also, the generated rule-set is almost the same re-
gardless of the training/testing split used. Our experimen-
tal results indicate that FOLD-SE is competitive in accu-
racy and efﬁciency with well-known machine learning al-
gorithms such as XGBoost classiﬁer and Multi-Layer Per-
ceptron (MLP). The FOLD-SE algorithm signiﬁcantly out-
performs FOLD-R++, on which it is based, as well as the
RIPPER algorithm. In addition, the FOLD-SE implementa-
tion provides justiﬁcation for a prediction.

2 Background
that

it

FOLD-SE represents the rule-set
learns from a
dataset as a default theory that is expressed as a normal
logic program, i.e., logic programming with negation. The
logic program is stratiﬁed in that there are no recursive
calls in the rules. we brieﬂy describe default logic be-
low. We assume that the reader is familiar with logic pro-
gramming (Bratko 2012) as well as classiﬁcation problems
(Bishop 2006).

Default Logic (Reiter 1980) is a non-monotonic logic to
formalize commonsense reasoning. A default D is an ex-
pression of the form

A : MB
Γ
which states that the conclusion Γ can be inferred if pre-
requisite A holds and B is justiﬁed. MB stands for “it is
consistent to believe B”. Normal logic programs can encode
a default theory quite elegantly (Gelfond and Kahl 2014). A
default of the form:

α1 ∧ α2 ∧ · · · ∧ αn : M¬β1, M¬β2 . . . M¬βm
γ

can be formalized as the normal logic programming rule:

γ :- α1, α2, . . . , αn, not β1, not β2, . . . , not βm.
where α’s and β’s are positive predicates and not represents
negation-as-failure. We call such rules default rules. Thus,
the default bird(X):M¬penguin(X)
will be represented as the
following default rule in normal logic programming:

f ly(X)

fly(X) :- bird(X), not penguin(X).
We call bird(X), the condition that allows us to jump to
the default conclusion that X can ﬂy, the default part of the
rule, and not penguin(X) the exception part of the rule.

3 FOLD-SE algorithm

3.1 Heuristic for Literal Selection
Most top-down RBML algorithms employ a heuristic to
guide the literal selection process; information gain (IG) and
its variations are the most popular ones. Every selected lit-
eral leads to a split on input examples during training. The
heuristic used in split-based classiﬁers greatly impacts the
accuracy and structure of the learned model, whether its
rule-based or decision tree-based. Speciﬁcally, the heuris-
tic used in literal selection of RBML algorithms impacts the
number of generated rules and literals, therefore it has an
impact on explainability.

FOLD-SE employs a heuristic that we have newly cre-
ated called Magic Gini Impurity (MGI). MGI is inspired by
Gini Impurity (GI) heuristic to guide the literal selection pro-
cess. It helps reduce the number of generated rules and lit-
erals while maintaining competitive performance compared
to using information gain (IG). GI for binary classiﬁcation
is deﬁned as:

GI(tp, f n, tn, f p) =

tp × f p
(tp + f p)2 +

tn × f n
(tn + f n)2

(1)

To verify MGI’s effectiveness, a comparison experiment of
4 different heuristics (MGI, information gain, weighted Gini

Data Set

Magic Gini Impurity

Information Gain

Weighted Gini Index

Chi-Square

4.0
1.0
3
3.3
0.93
8.0
51
3.9
65
0.75 38.8
7.7
302 0.88 19.0
7.5
0.98
34
8.0
4.6
0.95 23.9
28
6.9
172 0.80 75.4
10.4
0.92 35.3
34
10.1
7.6
0.92 46.1
53
10.4 22,224 0.81 34.9

Rows Cols Acc. Nodes Depth T(ms) Acc. Nodes Depth T(ms) Acc. Nodes Depth T(ms) Acc. Nodes Depth T(ms)
Name
4.0
1.0
7
120
acute
14 0.97
5.8
178
wine
14 0.73 38.4
270
heart
35 0.88 19.1
351
ionosphere
25 1.0
7.1
400
kidney
17 0.94 24.2
435
voting
16 0.81 78.2
690
credit-a
10 0.93 35.9
699
breast-w
704
18 0.93 46.3
autism
765 754 0.83 33.7
parkinson
236
9 0.69 119.5 11.1
768
diabetes
62
9.9
1728
1.0
7
cars
585
10.1
kr vs. kp
3196 37 1.0
mushroom 8124 23 1.0
1,160
5.1
churn-model 10000 11 0.80 1260.9 17.2 4,975 0.80 1225.6 15.5 5,610 0.79 1286.2 15.1 5,432 0.79 1266.9 14.2 6,492
12330 18 0.87 884.3 19.3 7,007 0.86 843.2 15.3 6,886 0.86 921.3 14.3 8,378 0.86 884.3 13.4 8,440
intention
14980 15 0.84 1135.7 17.6 11,634 0.85 1141.9 15.9 11,820 0.84 1290.5 15.9 12,266 0.84 1236.5 13.8 11,757
eeg
30000 24 0.73 4011.9 27.3 68,232 0.73 3895.5 22.5 62,112 0.73 4241.7 21.8 58,831 0.73 4101.6 18.7 78,661
credit card
32561 15 0.82 4273.5 29.2 38,266 0.82 4064.6 24.7 40,943 0.82 4220.0 22.8 47,001 0.82 4173.7 20.5 46,194
adult

4.2
1.0
2
3.3
0.89
9.8
59
4.2
38
0.70 42.2
7.0
377 0.91 22.7
7.0
0.97 11.6
53
4.3
0.94 25.4
27
6.4
137 0.81 81.9
9.4
0.94 34.5
48
8.6
7.4
0.90 51.8
48
7.3 18,253 0.78 42.6
166 0.69 125.3
0.99 57.1
53
50.8
422
1.0
14.3
1,463 1.0

4.2
1.0
2
3.3
0.89
9.4
48
4.5
45
0.74 40.3
7.1
413 0.89 21.5
7.1
0.98 10.3
61
5.1
0.94 24.9
25
6.3
135 0.80 79.9
8.8
0.93 35.7
37
7.9
7.5
0.91 48.0
52
8.2 30,081 0.81 40.3
165 0.70 118.5
9.9
150 0.99 56.0
9.6
49.3
456
9.9
1.0
12.9
1,262 1.0
5.5

3.3
4.5
6.9
6.4
4.8
6.3
8.5
7.8
7.4
7.0 37,881
9.2
9.6
9.9
5.4

186 0.68 117.4 10.1
9.6
51
10.1
456
5.1
972

2
63
63
512
87
28
201
46
57

47.9
43.9
11.9

47.2
43.3
11.0

1.0
1.0
1.0

Table 1: Decision tree with different heuristics on various Datasets

Index, Chi-Square) with 2-way split decision trees on vari-
ous datasets is performed. The weighted Gini Impurity is
also tested but its result is exactly the same as that for
weighted Gini Index because they are equivalent in compar-
ison, mathematically. The decision tree employs the literal
selection process of FOLD-R++ for splitting nodes. The ac-
curacy, numbers of generated tree nodes, the average depth
of leaf nodes, and time consumption of the 10-fold cross-
validation test are averaged and reported. This comparison
experiment is performed on a small form factor desktop with
an Intel i7-8705G CPU and 16 GB RAM. As we can see
from the results shown in Table 1, all the heuristics have
equivalent splitting performance on decision trees. The num-
bers of generated tree nodes and the average depth of leaf
nodes are also very close.

Next, we use Magic Gini Impurity (MGI) as the heuris-
tic in the FOLD-R++ algorithm and compare it to FOLD-
R++ with the Information Gain (IG) heuristic. Interestingly,
the number of rules and predicates in the rules is reduced
drastically. As we can see in Table 2, the number of gen-
erated rules and generated literals is reduced signiﬁcantly
with MGI compared to IG, while performance remains the
same. Thus, Magic Gini Impurity signiﬁcantly improves in-
terpretability and explainability of the FOLD-R++ algorithm
while preserving performance.

3.2 Comparison of Feature Values

During the learning process, FOLD-SE has to compare cat-
egorical and numerical data values. FOLD-SE employs a
carefully designed comparison operator, which is an exten-
sion of the comparison operator of FOLD-R++ for compar-
ing categorical and numerical values. This gives FOLD-SE
the ability to elegantly handle mixed-type values and, thus,
learn from datasets that may have features containing both
numerical and categorical values (a missing value is consid-

ered as a categorical value). The comparison between two
numerical values or two categorical values in FOLD-R++ is
straightforward, as commonsense would dictate, i.e., two nu-
merical (resp. categorical) values are equal if they are iden-
tical, otherwise they are unequal. The equality between a
numerical value and a categorical value is always false, and
the inequality between a numerical value and a categorical
value is always true. In addition, numerical comparisons (≤
and >) between a numerical value and a categorical value is
always false. However, the numerical comparisons ≤ and >
are not complementary to each other with this comparison
assumption. For example, x ≤ 4 means that x is a number
and x is less than or equal to 4. The opposite of x ≤ 4 should
be x is a number greater than 4 or x is not a number. Without
the opposite of these two numerical comparisons being used,
the literal selection process of FOLD-R++ would be limited.
The FOLD-SE algorithm, thus, extends the comparison op-
erators with (cid:2) and ≯ as the opposites of ≤ and >, respec-
tively. The literals with (cid:2) and ≯ will be candidate literals in
the literal selection process but converted to their opposites,
≤ and >, in the ﬁnal results. An example is shown in Table
3.

Given E+={3,4,4,5,x,x,y}, E−={1,1,1,2,3,y,y,z}, and
literal(i, >, 3) in Table 4, the true positive example Etp,
false negative examples Ef n, true negative examples Etn,
and false positive examples Ef p implied by the literal
are {4,4,5}, {3,x,x,y}, {1,1,1,2,3,y,y,z}, Ø respectively.
literal(i, >, 3) is calculated as
Then,
MGI(i,>,3)(3,4,8,0)=-0.38 through Formula 1.

the heuristic of

3.3 Literal Selection

The FOLD-R++ algorithm starts the learning process with
the candidate rule p(...):- true., where p(...) is
the target predicate to learn. It specializes the rule by adding
literals to its body during the training process. It adds a lit-

Data Set

FOLD-R++

FOLD-R++ with MGI

2
38
275
16
23
84
34
62

3.0 0.99 1.0 0.99 0.99
2.7
15.9 32.2 0.74 0.77 0.78 0.77
12.4 19.7 0.91 0.89 0.98 0.93
4.9
5.9 0.98 1.0 0.97 0.98
10.0 27.2 0.95 0.92 0.96 0.94
10.3 23.3 0.85 0.93 0.78 0.85
10.5 18.6 0.95 0.98 0.94 0.96
25.4 54.8 0.92 0.95 0.94 0.94

Rows Cols Acc Prec Rec F1 T(ms) Rules Preds Acc Prec Rec F1 T(ms) Rules Preds
Name
3.0
120
2.7
acute
0.99 1.0 0.99 0.99
7
8.9
4.3
270
heart
14 0.77 0.80 0.80 0.79
7.1
5.1
351
ionosphere
35 0.90 0.92 0.93 0.92
6.3
400
5.2
kidney
25 0.99 1.0 0.98 0.99
20.2
7.8
435
voting
17 0.94 0.92 0.93 0.92
3.8
2.2
690
credit-a
16 0.83 0.90 0.78 0.83
8.1
12.9
699
breast-w
10 0.95 0.97 0.85 0.96
19.4 43.4
704
autism
18 0.93 0.95 0.95 0.95
13.2
7.4
765
parkinson
754 0.82 0.85 0.93 0.89 10,757 13.7 21.2 0.81 0.82 0.96 0.89 7,469
4.8
11.6
768
diabetes
66
8.3
9
19.4 0.74 0.78 0.84 0.81
0.74 0.79 0.83 0.80
17.5
8.8
1728
cars
31
12.3 29.8 0.96 1.0 0.95 0.97
0.96 1.0 0.95 0.97
7
11.4 27.6
kr vs. kp
3196
19.3 46.7 0.97 0.97 0.97 0.97
226
37 0.99 1.0 0.99 0.99
7.0
11.9 1.0 1.0 1.0 1.0
13.4
mushroom 8124
7.9
23
281
1.0 1.0 1.0 1.0
11.4
4.4
18 0.90 0.95 0.93 0.94 1,085
12330
intention
23.0 0.90 0.95 0.93 0.94
8.4
15 0.72 0.76 0.72 0.74 2,735 69.1 152.6 0.68 0.75 0.64 0.69 1,353 18.7 40.8
eeg
14980
24 0.82 0.83 0.96 0.89 5,954 19.1 48.8 0.82 0.83 0.96 0.89 3,827 10.9 25.5
credit card 30000
13.2
adult
15 0.84 0.86 0.95 0.90 2,508 16.8 46.7 0.84 0.86 0.95 0.90 1,414
32561
rain in aus 145460 24 0.79 0.87 0.84 0.86 26,203 48.2 115.8 0.80 0.86 0.87 0.86 15,003 14.6 57.9

1
11
94
25
25
39
32
41

48
35
170
257
691

6.8

Table 2: Comparison of Magic Gini impurity (MGI) and information gain (IG) in FOLD-R++

comparison
10 = ‘cat’
10 ≤ ‘cat’
10 (cid:2) ‘cat’

evaluation
False
False
True

comparison
10 6= ‘cat’
10 > ‘cat’
10 ≯ ‘cat’

evaluation
True
False
True

Table 3: Comparing numerical and categorical values

E+
E−
Etp(i,>,3)
Efn(i,>,3)
Etn(i,>,3)
Efp(i,>,3)

ith feature values count

3 4 4 5 x x y
1 1 1 2 3 y y z
4 4 5
3 x x y
1 1 1 2 3 y y z
Ø

7
8
3
4
8
0

Table 4: Evaluation and count for literal(i, >, 3).

eral that maximizes information gain. FOLD-SE extends the
literal selection process of FOLD-R++ by employing MGI
as a heuristic instead of IG. In addition, candidate literals of
the form m (cid:2) n and m ≯ n are also considered. The literal
selection process of FOLD-SE is summarized in Algorithm
1. In line 2, cnt+ and cnt− are dictionaries that hold, re-
spectively, the numbers of positive and negative examples
of each unique value. In line 3, setn, setc are sets that hold,
respectively, the unique numerical and categorical values. In
line 4, tot+
n are the total number of, respectively,
positive and negative examples with numerical values; tot+
c
and tot−
c are the total number of, respectively, positive and
negative examples with categorical values. In line 6, the pre-
ﬁx sums of numerical values have been computed as prepa-
ration for calculating heuristics of candidate literals. After
the preﬁx sum calculation process, cnt+[x] and cnt−[x] rep-
resents the number of positive examples and negative ex-
amples that have a value less than or equal to x. Preparing
parameters correctly is essential to calculating MGI values

n and tot−

n − cnt+[x] + tot+

n − cnt+[x] + tot+

for candidate literals. In line 11, the MGI value for literal
(i, ≤, x) is computed by taking parameters cnt+[x] as num-
ber of true positive examples, tot+
c as the
number of false positive examples, tot−
n − cnt−[x] + tot−
c
as the number of true negative examples, and cnt−[x] as the
number of false positive examples. The reason for this is as
follows: for the literal (i, ≤, x), only numerical values that
are less than or equal to x can be evaluated as positive, other-
wise negative. tot+
c represents the number
of positive examples that have a value greater than x plus the
total number of positive examples with categorical values.
tot−
c represents the number of negative ex-
amples that have a value greater than x plus the total number
of negative examples with categorical values. cnt−[x] rep-
resents the number of negative examples that have a value
less than or equal to x. The heuristic calculation for other
candidate literals also follows the same comparison regime
mentioned above. Finally, the best literal on attr
function returns the best heuristic score and the correspond-
ing literal except the literals that have been used in current
rule-learning process.

n − cnt−[x]+ tot−

Example 1 Given positive and negative examples in Table
3, E+, E−, with mixed type of values on ith feature, the target
is to ﬁnd the literal with the best information gain on the
given feature. There are 7 positive examples, their values
on ith feature are [3, 4, 4, 5, x, x, y], and the values on ith
feature of the 8 negative examples are [1, 1, 1, 2, 3, y, y, z].

With the given examples and speciﬁed feature, the num-
ber of positive examples and negative examples for each
unique value are counted ﬁrst, which are shown as count+,
count− in Table 5. Then, the preﬁx sum arrays are calcu-
lated for computing heuristic as sum+
pfx. Table 6
shows the MGI heuristic for each candidate literal and the
literal(i, (cid:2), 2) gets selected as it has the highest score.

pfx, sum−

Algorithm 1 FOLD-SE Algorithm, Find Best Literal func-
tion
Input: E+: positive examples, E−: negative examples, used:

used literals

n , tot+

c , tot−

n , tot−

← count class(E+, E−, i)

cnt+, cnt−
setn, setc ← unique values(E+, E−, i)
tot+
num ← couting sort(setn)
for j ← 1 to size(num) do

Output: literal: the best literal that has the best heuristic score
1: function BEST LITERAL ON ATTR(E+, E−, i, used)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

cnt+[numj ] ← cnt+[numj ] + cnt+[numj−1]
cnt−
[numj−1]
end for
for x ∈ setn do

c ← count total(E+, E−, i)

⊲ H function computes MGI
n − cnt+[x] +

[numj ] ← cnt−

⊲ compute preﬁx sum

[numj ] + cnt−

score[(i, ≤, x)] ← H(cnt+[x], tot+
c , cnt−
n − cnt−
score[(i, >, x)] ← H(tot+
n − cnt−

[x])
n − cnt+[x], cnt+[x] +
[x])

[x] + tot−

c , cnt−

c , tot−

c , tot−

tot+

tot+

tot+

c , cnt+[x], cnt−

[x], tot−

← H(tot+
n − cnt−

[x] + tot−
c )

n − cnt+[x] +

score[(i, ≯, x)] ← H(cnt+[x] + tot+

c , tot+

n −

[x] + tot−
score[(i, (cid:2), x)]

[x], cnt−

[x] + tot−
c )

n − cnt−

cnt+[x], tot−
end for
for c ∈ sets do

c − cnt+[c] +

c − cnt+[c] +

tot+

n , tot−

score[(i, =, c)] ← H(cnt+[c], tot+
c − cnt−
score[(i, 6=, c)]

n , cnt−
← H(tot+

[c] + tot−

[c])

[c] + tot−
n )

tot+

[c], tot−

c − cnt−

n , cnt+[c], cnt−
end for
h, literal ← best pair(score, used)
return h, literal

19:
20:
21:
22: end function
23: function FIND BEST LITERAL(E+, E−, used)
best h, literal ← −∞, invalid
24:
for i ← 1 to N do
25:
26:
27:
28:
29:
30:
31:
32: end function

end for
return literal

best h, literal ← h, lit

end if

h, lit ← BEST LITERAL ON ATTR(E+,E−,i,used)
if best h < h then

⊲ N is the number of features

⊲ best score and its literal

12:

13:

14:

15:
16:
17:

18:

3.4 Rule Pruning
The FOLD-R++ algorithm (Wang and Gupta 2022) is a re-
cent rule-based ML algorithm for binary classiﬁcation that
generates a normal logic program in which all the default
rules have the same rule head (target predicate). An exam-
ple is covered means that it is predicted as positive. An ex-
ample covered by any default rule in the set would imply
the rule head is true. The FOLD-R++ algorithm generates a
model by learning one rule at a time. After learning a rule,
the already covered examples would be ruled out for better
literal selection of remaining examples. If the ratio of false
positive examples to true positive examples drops below the
preset threshold, it would next learn exceptions by swap-
ping remaining positive and negative examples then calling
itself recursively. The ratio stands for the upper bound on
the number of true positive examples to the number of false
positive examples implied by the default part of a rule. It

E+
E−

value
count+
sum+
pfx
count−
sum−

pfx

3
1
1
0
0
3
3

4
1
2
0
0
1
4

4
1
3
1
1
1
5

x
y
x
2

ith feature values
x
z
3
z
5
1
0
4 NA NA NA
0
1
5 NA NA NA

5
2
4
2
3
0
5

y
y
y
1

0

2

Table 5: Top: Examples and values on ith feature. Bottom:
positive/negative count and preﬁx sum on each value

heuristic
5
4

y

2

3

1

x

value
z
≤ value −∞ −∞ −∞ −∞ −∞ NA NA NA
> value -0.47 -0.44 -0.38 -0.46 -0.50 NA NA NA
(cid:2) value -0.39 -0.35 -0.43 -0.49 -0.50 NA NA NA
≯ value −∞ −∞ −∞ −∞ −∞ NA NA NA
= value NA NA NA NA NA -0.42 −∞ −∞
6= value NA NA NA NA NA −∞ -0.49 -0.47

Table 6: The heuristic on ith feature with given examples

helps speed up the training process and reduces the num-
ber of rules learned. The training process of FOLD-R++ is
also a process of ruling out already covered examples. Later
generated rules cover fewer examples than the early gener-
ated ones. In other words, FOLD-R++ suffers from long-tail
effect. Here is an example:

Example 2 The “Adult Census Income” is a classical clas-
siﬁcation task that contains 32,561 records. We treat 80%
of the data as training examples and 20% as testing exam-
ples. The task is to learn the income status of individuals
(more/less than 50K/year) based on features such as gender,
age, education, marital status, etc. FOLD-R++ generates
the following program that contains 9 rules:

(1) income(X,’<=50K’) :-

[3428] not marital_status(X,’Married-civ-spouse’),

not ab3(X,’True’).

(2) income(X,’<=50K’) :-

[1999] marital_status(X,’Married-civ-spouse’),

education_num(X,N1), N1=<12.0, capital_gain(X,N2),
N2=<5013.0, not ab5(X,’True’), not ab6(X,’True’).

(3) income(X,’<=50K’) :- occupation(X,’Farming-fishing’),

[1]

workclass(X,’Self-emp-not-inc’),
education_num(X,N1), N1>12.0, capital_gain(X,N2),
N2>5013.0.

(4) ab1(X,’True’) :- not workclass(X,’Local-gov’),

[2]

capital_gain(X,N2), N2=<7978.0, education_num(X,N1),
N1=<10.0.

(5) ab2(X,’True’) :- capital_gain(X,N2), N2>27828.0,

[0]

N2=<34095.0.

(6) ab3(X,’True’) :- capital_gain(X,N2), N2>6849.0,

[0]

age(X,N3), N3>20.0, not ab1(X,’True’),
not ab2(X,’True’).

(7) ab4(X,’True’) :- workclass(X,’Local-gov’),

[0]

native_country(X,’United-States’).

(8) ab5(X,’True’) :- not race(X,’Amer-Indian-Eskimo’),

[0]

education_num(X,N1), N1=<8.0, capital_loss(X,N4),
N4>1735.0, N4=<1902.0, not ab4(X,’True’).

(9) ab6(X,’True’) :- occupation(X,’Tech-support’),

[0]

not education(X,’11th’), education_num(X,N1),
N1>5.0, N1=<8.0, age(X,N3), N3=<36.0.

The above generated rules achieve 0.85 accuracy and 0.90
F1 score. The ﬁrst rule covers 3428 test examples and the
second rule covers 1999 test examples. Subsequent rules
only cover small number of test examples. This long-tail ef-
fect is due to the overﬁtting on the training data. FOLD-
SE introduces a hyper-parameter tail to limit the mini-
mum number/percentage of training examples that a rule can
cover. It helps reduce the number of generated rules and gen-
erated literals by reducing overﬁtting of outliers. This rule
pruning is not a post-process after training, rather rules are
pruned during the training process itself which helps speed-
up training. With the tail parameter, FOLD-SE can be easily
tuned to obtain a trade-off between accuracy and explain-
ability. The FOLD-SE algorithm is summarized in Algo-
rithm 2. The added rule pruning process is carried out in
Line 32–37 of Algorithm 2. When a learned rule cannot
cover enough training examples, the learn rule func-
tion returns. Except for the input parameter tail and the
rule pruning process, FOLD-R++ and FOLD-SE have the
same algorithmic framework. Of course, the heuristic used
in FOLD-SE is MGI, while FOLD-R++ uses IG.

3.5 Complexity Analysis
If M is the number of training examples and N is the number
of features that have been included in the training, the time
complexity of ﬁnding the best literal of a feature is O(M ),
assuming that counting sort is used at line 5 in Algorithm
1. Therefore, the complexity of ﬁnding the best literal of all
features is O(M N ). The worst training case is that each gen-
erated rule only covers one training example and each literal
only help exclude one example. In this case, O(M 2) literals
would be selected in total. Hence, the worst case time com-
plexity of FOLD-SE is O(M 3N ). Additionally, it is easy to
prove that the FOLD-SE algorithm always terminates (proof
is omitted due to lack of space).

4 Experimental Results
We next present our experiments on UCI benchmarks
and Kaggle datasets. The XGBoost Classiﬁer is a well-
known classiﬁcation model and used as a baseline model in
our experiments. Multi-Layer Perceptron (MLP) is another
widely-used model that is able to deal with generic classiﬁ-
cation tasks. The settings used for XGBoost and MLP mod-
els is kept simple without limiting their performance. How-
ever, Both XGBoost and MLP models cannot directly per-
form training on mixed-type (numerical and categorical val-
ues in a row or a column) data. For mixed-type data, one-hot
encoding has been used for data preparation because label
encoding would add non-existing numerical relation to cate-
gorical values. RIPPER system is another rule-induction al-
gorithm that generates formulas in conjunctive normal form
as an explainable model. FOLD-R++ is the foundation of
our new FOLD-SE algorithm. Both RIPPER and FOLD-
R++ are capable of dealing with mixed-type data and are
used as baseline to compare explainability. For binary clas-

Algorithm 2 FOLD-SE Algorithm
Input: E+: positive examples, E−: negative examples, ratio: ex-

ception ratio, tail: covering limit

break

⊲ rule out covered

⊲ learned rule invalid

end if
E+ ← Ef n
R ← R ∪ r

R ← Ø
while |E+| > 0 do

L, r ← Ø, EmptyRule
while true do

r ←LEARN RULE(E+, E−, used, tail)
Ef n ← cover(r, E+, false)
if |Ef n| = |E+| then

Output: R = {r1, ..., rn}: a set of defaults rules with exceptions
1: function LEARN RULE SET(E+, E−, used, tail)
2:
3:
4:
5:
6:
7:
8:
9:
10:
end while
11:
return R
12:
13: end function
14: function LEARN RULE(E+, E−, used, tail)
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
end if
37:
return r
38:
39: end function

l ←FIND BEST LITERAL(E+, E−, used)
L ← L ∪ l
r.default ← L
E+ ← cover(r, E+, true)
E−
← cover(r, E−, true)
if l is InvalidRule or |E−
if l is InvalidRule then
r.default ← L \ l

E+ ← covers(r, E+, true)
if |E+| < tail then

end if
end while
if tail > 0 then

| ≤ |E+| ∗ ratio then

return InvalidRule

end if
break

⊲ set the default part of rule as L

⊲ remove invalid literal l

⊲ rule pruning

end if

else

ab ← LEARN RULE SET(E−, E+, used + L, tail)
r.exception ← ab

⊲ set exception part as ab

siﬁcation tasks, accuracy, precision, recall, and F1 score
have been used as evaluation metrics of classiﬁcation per-
formance. For multi-category classiﬁcation, accuracy and
weighted F1 score have been reported. The number of gen-
erated rules and the number of generated literals (predicates)
have been used as evaluation metrics of explainability.

The FOLD-SE algorithm does not need any data encod-
ing for training, a feature that it inherits from FOLD-R++.
After specifying the numerical features, both FOLD-R++
and FOLD-SE can deal with mixed-type data directly. Even
missing values are handled and do not need to be provided.
FOLD-SE has been implemented in Python. We have down-
loaded FOLD-R++, also coded in Python, from GitHub. The
hyper-parameter ratio of these two algorithms is simply
set to default value of 0.5 for all experiments. The hyper-
parameter tail of the FOLD-SE algorithms is set to default
percentage 0.5% of training data size. All the training pro-

Data Set

FOLD-SE

FOLD-R++

2
38
275
16
23
84
34
62

95
317
1,161
750
172
944
319
359

Rows Cols Acc F1
Name
2.0
0.93 0.92
7
120
acute
4.0
14 0.76 0.77
270
heart
3.6
35 0.72 0.73
351
ionosphere
4.9
25 0.98 0.98
400
kidney
17 0.95 0.92
7.3
435
voting
2.4
16 0.89 0.89
690
credit-a
3.5
10 0.93 0.90
699
breast-w
9.9
18 0.93 0.95
704
autism
5.7
765
754 0.70 0.78 159,556
parkinson
2.7
768
diabetes
7.2
1728
cars
5.0
3196
kr vs. kp
5.7
mushroom
8124
2.9
churn-model 10000
2.0
12330
intention
5.1
14980
eeg
2.0
30000
credit card
2.0
32561
adult
145460 24 0.63 0.70 3118,025 180.1 776.4 0.79 0.86 26,203 48.2 115.8 0.82 0.89 10,243 2.5
rain in aus

RIPPER
T(ms) Rules Preds Acc F1 T(ms) Rules #Preds Acc F1 T(ms) Rules Preds
3.0
2.0
2.7
4.0 0.99 0.99
9.1
12.9 0.77 0.79
15.9
5.4
7.1
12.4
13.9 0.90 0.92
8.5
4.9
6.1
8.5 0.99 0.99
7.1
4.1
8.9 0.94 0.92
20.2
10.0
5.8
10.3
10.1 21.4 0.83 0.83
6.3
14.4 19.9 0.95 0.96
10.5
23.6
10.3 25.2 0.93 0.95
25.4
12.5
13.4 0.82 0.89 10,757 13.7
8.9
38
5.9
66
8.3
8.7
14.8 0.74 0.80
20
14.0
31
12.3
14.2 39.8 0.96 0.97
152
10.4
16.2 0.99 0.99
226
19.3
8.1
254
10.6
7.9
281
8.3
12.7 1.0 1.0
9.1
600
11.6 39.2 0.85 0.91
28.1
987
5.1
25.2 91.6 0.90 0.94 1,085
661
8.4
12.1
43.4 134.7 0.72 0.74 2,735 69.1 152.6 0.67 0.68 1,227
3.0
48.8 0.82 0.89 3,513
36.5 150.7 0.82 0.89 5,954 19.1
5.0
46.7 0.84 0.90 1,746
41.4 168.4 0.84 0.90 2,508 16.8
6.1

1.0 1.0
3.0
32.2 0.74 0.77
19.7 0.91 0.93
1.0 1.0
5.9
27.2 0.95 0.94
23.3 0.85 0.85
18.6 0.94 0.92
54.8 0.91 0.94
21.2 0.82 0.89 9,691
19.4 0.75 0.81
29.8 0.96 0.97
46.7 0.97 0.97
11.9
1.0 1.0
66.9 0.85 0.91
23.0 0.90 0.94

511
9
0.58 0.56
0.99 0.99
385
7
37 0.99 0.99
609
923
23
1.0 1.0
9,941
11 0.54 0.60
18 0.88 0.93
8,542
15 0.55 0.36 12,996
24 0.76 0.84 49,940
15 0.71 0.77 63,480

1
13
119
16
11
36
9
29

Table 7: Comparison of RIPPER, FOLD-R++, and FOLD-SE on various Datasets

Data Set

XGBoost

MLP

FOLD-SE

22
95

122
247

273
149
720
186
236

0.99 0.99
0.99
0.76 0.79 0.79 0.78

Rows Cols Acc Prec Rec F1 T(ms) Acc Prec Rec F1 T(ms) Acc Prec Rec F1 T(ms)
Name
1.0 1.0 1.0 1.0
1.0 1.0 1.0 1.0
7
1
120
acute
14 0.82 0.83 0.85 0.83
0.74 0.77 0.78 0.77
270
heart
35 0.88 0.87 0.95 0.91 2,206 0.79 0.91 0.74 0.81 1,771 0.91 0.89 0.98 0.93
351
ionosphere
0.99 1.0 0.99 0.99
1.0 1.0 1.0 1.0
25 0.99 0.99 0.99 0.99
400
kidney
0.95 0.92 0.96 0.94
17 0.95 0.93 0.95 0.93
0.95 0.92 0.94 0.93
435
voting
0.85 0.92 0.79 0.85
16 0.85 0.86 0.86 0.86
0.82 0.84 0.84 0.84
690
credit-a
0.97 0.98 0.97 0.98
10 0.95 0.96 0.98 0.96
0.94 0.88 0.97 0.92
699
breast-w
18 0.97 0.98 0.98 0.98
0.96 0.99 0.96 0.97
0.91 0.94 0.94 0.94
704
autism
754 0.76 0.79 0.93 0.85 270,336 0.60 0.77 0.67 0.71 152,056 0.82 0.82 0.96 0.89 9,691
765
parkinson
38
0.75 0.78 0.85 0.81
0.66 0.73 0.74 0.73
9
0.66 0.71 0.81 0.76
768
diabetes
20
1.0 1.0 1.0 1.0
0.96 1.0 0.94 0.97
0.99 1.0 1.0 1.0
7
1728
cars
152
0.99 0.99 1.0 0.99
37 0.99 0.99 1.0 0.99
0.97 0.96 0.97 0.97
3196
kr vs. kp
254
1.0 1.0 1.0 1.0
1.0 1.0 1.0 1.0
23
1.0 1.0 0.99 1.0
mushroom
8124
600
11 0.85 0.87 0.96 0.91 97,727 0.81 0.90 0.86 0.88 18,084 0.85 0.87 0.95 0.91
churn-model 10000
18 0.90 0.93 0.95 0.94 171,480 0.81 0.89 0.88 0.89 41,992 0.90 0.95 0.93 0.94
661
12330
intention
15 0.64 0.64 0.81 0.71 46,472 0.69 0.72 0.71 0.71 9,001 0.67 0.74 0.63 0.68 1,227
14980
eeg
NA 0.82 0.83 0.96 0.89 3,513
24 NA NA NA NA
30000
credit card
15 0.87 0.89 0.95 0.92 424,686 0.81 0.88 0.87 0.87 300,380 0.84 0.86 0.95 0.90 1,746
32561
adult
145460 24 0.84 0.85 0.96 0.90 385,456 0.81 0.86 0.89 0.88 243,990 0.82 0.85 0.94 0.89 10,243
rain in aus

1
13
119
16
11
36
9
29

218
43
356
48
56

NA NA NA NA

368
83
273
394

839
210
403
697

NA

Table 8: Comparison of XGBoost, MLP, and FOLD-SE on various Datasets

cesses have been performed on a small form factor desktop
with Intel i7-8705G and 16 GB RAM. To have good perfor-
mance test, we performed 10-fold cross-validation test on
each dataset. We report average classiﬁcation metrics and
execution times.

The experimental results shown in Table 7 indicates that
the FOLD-R++ and FOLD-SE outperform RIPPER algo-
rithm in accuracy and explainability (the numbers of gener-
ated rules and literals/predicates). The FOLD-SE algorithm
outperforms FOLD-R++ in explainability while maintain-
ing comparable performance in classiﬁcation, especially for
large datasets. With enough data, the FOLD-SE algorithm

can generate really concise rules that can capture patterns
in datasets. The most dramatic result is that FOLD-SE gen-
erates a model with 2.5 rules on average with 6.1 literals
in these rules on average with average accuracy of 0.82,
while RIPPER and FOLD-R++ report much higher values
for number of rules and literals (180.1 rules and 776.4 lit-
erals for RIPPER and 48.2 rules and 115.8 predicates for
FOLD-R++) and lower value for accuracy (0.63 for RIPPER
and 0.79 for FOLD-R++).

The experimental results comparing FOLD-SE with XG-
Boost and MLP are listed in Table 8. FOLD-SE always takes
less time to train compared to XGBoost and MLP, espe-

Data Set

FOLD-RM

FOLD-SE-M

7.5 0.95 0.95 6.5

Rows Cols Acc F1 Rules Preds Acc F1 Rules Preds
Name
7.6
178
14 0.93 0.94 7.5
wine
9 0.79 0.80 43.9 60.0 0.80 0.79 24.0 45.2
336
ecoli
4024 155 1.0 1.0 14.7 16.9 1.0 1.0 7.0 10.6
weight-lift
5456 25 0.99 0.99 29.6 41.6 0.99 0.99 7.1 15.8
wall-robot
page-blocks 5473 11 0.97 0.97 74.8 161.9 0.96 0.96 8.5 15.0
12960 9 0.96 0.96 59.5 142.0 0.92 0.92 18.4 39.9
nursery
13611 17 0.91 0.91 189.4 353.7 0.90 0.90 15.1 31.4
dry-bean
5.0
58000 10 1.0 1.0 27.2 35.4 1.0 0.99 4.0
shuttle

Table 9: Comparison of FOLD-RM, and FOLD-SE-M

cially for large mixed-type datasets with many unique val-
ues. FOLD-SE can achieve equivalent scores wrt accuracy
and F1 score. For the “credit card” dataset, XGBoost and
MLP failed training because of 16 GB memory limitation
of the testing machine, the one-hot encoding process needs
around 39 GB memory consumption. However, FOLD-SE
only consumes 53 MB memory at peak for training on the
same dataset. Compared to XGBoost and MLP, the FOLD-
SE algorithm is more efﬁcient and light-weight.

is

the

The

algorithm
FOLD-RM
(Wang, Shakerin, and Gupta 2022)
FOLD-R++
algorithm with the “M” extension for multi-class classi-
ﬁcation tasks. The “M” extension can also work with the
FOLD-SE algorithm, we only need to add another parameter
tail. The extended FOLD-SE is called FOLD-SE-M. Table
9 compares only FOLD-RM and FOLD-SE-M algorithms
(we didn’t ﬁnd open source RIPPER implementation for
multi-class classiﬁcation). The hyper-parameter is also set
as 0.5% of training data size. The FOLD-SE algorithm
helps the resulting rule set stay small.

5 Explainability
With the new heuristic, extended comparison operator, and
rule pruning, the FOLD-SE algorithm pushes interpretabil-
ity and explainability to a higher level. For Example 2 (UCI
Adult Dataset), it generates the following logic program
with only two rules:

(1) income(X,’<=50K’) :-

[3457] not marital_status(X,’Married-civ-spouse’),

capital_gain(X,N1), N1=<6849.0.

(2) income(X,’<=50K’) :-

[1998] marital_status(X,’Married-civ-spouse’),
capital_gain(X,N1), N1=<5013.0,
education_num(X,N2), N2=<12.0.

The above rules achieve 0.85% accuracy, 0.86% precision,
0.95% recall, and 0.91% F1 score, the ﬁrst rule covers 3457
test examples and the second rule covers 1998 test exam-
ples. The generated rule set can be understood easily due to
the symbolic representation: Who makes less than 50K dol-
lars a year: (1) unmarried people with capital gain less than
$6,849; (2) married people with capital gain less than $5,013
and education level not over 12. The generated rule-set for
this dataset in the 10-fold cross validation test are almost all
identical, only the values in the literals change slightly.

Example 3 The “Rain in Australia” is another classiﬁca-
tion task that contains 145,460 records with 24 features. We
treat 80% of the data as training examples and 20% as test-
ing examples. The task is to ﬁnd out if it is not rainy tomor-
row, the FOLD-SE generates following rules:

(1) raintomorrow(X,’No’) :- humidity3pm(X,N1), N1=<64.0,

rainfall(X,N2), N2=<182.6.

(2) raintomorrow(X,’No’) :- rainfall(X,N2), N2=<2.2,

humidity3pm(X,N1), not(N1=<64.0), not(N1>81.0).

The generated rules achieve 0.83% accuracy, 0.85% preci-
sion, 0.94% recall, and 0.89% F1 score.

is able to generate normal

6 Related Work and Conclusions
Rule-base Machine Learning is a long-standing interest of
the community. Some RBML algorithms perform train-
ing directly on the input data: ALEPH (Srinivasan 2001)
is a well-known Inductive Logic Programming algorithm
that induces rules by using bottom-up approach, it can-
not handle numerical features;
those have to be han-
dled manually. ILASP (Law 2018) system is another ILP
algorithm that
logic pro-
it needs to work with a solver and requires a
gram,
rule set
to describe the hypothesis space. Some other
RBML algorithms rely on statistical machine learning mod-
els: SVM+ProtoTypes (N´u˜nez, Angulo, and Catal`a 2002)
extracts
rule from Support Vector Machine (SVM)
models by using K-Means clustering algorithm. Rule-
Fit (Friedman, Popescu, and others 2008) algorithm learns
weighted rules from ensemble models of shallow deci-
sion trees. TREPAN (Craven and Shavlik 1995) produces
a decision tree from trained Neural Networks by query-
ing. Support Vector ILP (Muggleton et al. 2005) uses ILP-
learned clauses as the kernel in dual form of SVM. nFOIL
(Landwehr, Kersting, and Raedt 2005) system employs the
naive Bayes criterion to guide its rule induction. The kFOIL
(Landwehr et al. 2006) algorithm integrates the FOIL sys-
tem with kernel methods. Compared to the above systems,
our approach is more efﬁcient and scalable due to being
top-down and using preﬁx-sum technique for literal selec-
tion. Thus, the rule-set learned in our approach is more con-
cise because of the use of default rules with exceptions. Fi-
nally, our approach is able to provide scalable explainability
which, to the best of our knowledge, no other RBML algo-
rithm achieves.

In this paper, we presented a highly efﬁcient rule-based
ML algorithm with scalable explainability, called FOLD-
SE, to generate a normal logic program for classiﬁcation
tasks. It is built upon the FOLD-R++ algorithm with a newly
created heuristic for literal selection and a rule pruning
mechanism. Our experimental results show that the gener-
ated logic rule-set provides performance comparable to XG-
Boost and MLP but better training efﬁciency, interpretabil-
ity, and explainability. Unlike its predecessor FOLD-R++
and other RBML algorithms, the explainability of FOLD-SE
is scalable which means the number of generated logic rules
and generated literals stay small regardless of the size of the
training data. The tail hyper-parameter added to FOLD-SE
can be easily adjusted to obtain a trade-off between accu-

[Srinivasan 2001] Srinivasan, A. 2001. The aleph manual.
http://web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/.
[Wang and Gupta 2022] Wang, H., and Gupta, G.

2022.
FOLD-R++: A scalable toolset for automated inductive
In Func-
learning of default theories from mixed data.
tional and Logic Programming: 16th International Sympo-
sium, FLOPS 2022, 224–242. Berlin, Heidelberg: Springer-
Verlag.

[Wang, Shakerin, and Gupta 2022] Wang, H.; Shakerin, F.;
and Gupta, G. 2022. FOLD-RM: A scalable, efﬁcient, and
explainable inductive learning algorithm for multi-category
classiﬁcation of mixed data. Theory and Practice of Logic
Programming 1–20.

racy and explainability. We also presented an extension of
the FOLD-SE algorithm that can, similarly, handle multi-
class classiﬁcation tasks.

References
[Bishop 2006] Bishop, C. M. 2006. Pattern Recognition
and Machine Learning (Information Science and Statistics).
Berlin, Heidelberg: Springer-Verlag.

[Bratko 2012] Bratko, I. 2012. Prolog Programming for
Artiﬁcial Intelligence (4th ed.). Harlow, England: Addison
Wesley.

[Cohen 1995] Cohen, W. W. 1995. Fast effective rule in-
duction. In Proceedings of the Twelfth International Con-
ference on International Conference on Machine Learning,
ICML’95, 115–123. San Francisco, CA, USA: Morgan
Kaufmann Publishers Inc.

[Craven and Shavlik 1995] Craven, M. W., and Shavlik,
1995. Extracting tree-structured representations
J. W.
In Proceedings of the 8th Interna-
of trained networks.
tional Conference on Neural Information Processing Sys-
tems, NIPS’95, 24–30. Cambridge, MA, USA: MIT Press.
[Cropper and Dumancic 2020] Cropper, A., and Dumancic,
S. 2020. Inductive logic programming at 30: a new intro-
duction. https://arxiv.org/abs/2008.07912.

[Friedman, Popescu, and others 2008] Friedman,

J. H.;
Popescu, B. E.; et al. 2008. Predictive learning via rule
ensembles. The Annals of Applied Statistics 2(3):916–954.
[Gelfond and Kahl 2014] Gelfond, M., and Kahl, Y. 2014.
Knowledge representation, reasoning, and the design of in-
telligent agents: The answer-set programming approach.
Cambridge University Press.

[Landwehr et al. 2006] Landwehr, N.; Passerini, A.; Raedt,
L. D.; and Frasconi, P. 2006. kFOIL: Learning simple rela-
tional kernels. In Proc. AAAI, 389–394.

[Landwehr, Kersting, and Raedt 2005] Landwehr, N.; Kerst-
ing, K.; and Raedt, L. D. 2005. nFOIL: Integrating na¨ıve
bayes and FOIL. In Proc. AAAI, 795–800.

[Law 2018] Law, M. 2018.

Inductive learning of answer
set programs. Ph.D. Dissertation, Imperial College London,
UK.

[Muggleton et al. 2005] Muggleton, S.; Lodhi, H.; Amini,
A.; and Sternberg, M. J. E. 2005. Support vector induc-
tive logic programming. In Hoffmann, A.; Motoda, H.; and
Scheffer, T., eds., Discovery Science. Berlin, Heidelberg:
Springer Berlin Heidelberg.

[N´u˜nez, Angulo, and Catal`a 2002] N´u˜nez, H.; Angulo, C.;
and Catal`a, A. 2002. Rule extraction from support vector
In In Proceedings of European Symposium on
machines.
Artiﬁcial Neural Networks, 107–112.

[Quinlan 1990] Quinlan, J. R. 1990. Learning logical deﬁni-

tions from relations. Machine Learning 5:239–266.

[Reiter 1980] Reiter, R. 1980. A logic for default reasoning.

Artiﬁcial Intelligence 13(1-2):81–132.

[Shakerin, Salazar, and Gupta 2017] Shakerin, F.; Salazar,
E.; and Gupta, G. 2017. A new algorithm to automate induc-
tive learning of default theories. TPLP 17(5-6):1010–1026.

