SoftNeuro: Fast Deep Inference using
Multi-platform Optimization

Masaki Hilaga, Yasuhiro Kuroda, Hitoshi Matsuo, Tatsuya Kawaguchi,
Gabriel Ogawa, Hiroshi Miyake and Yusuke Kozawa
Morpho, Inc.
Tokyo, Japan
(m-hilaga, y-kuroda, h-matsuo, t-kawaguchi, g-ogawa, h-miyake, y-kozawa)@morphoinc.com

1
2
0
2

t
c
O
2
1

]

G
L
.
s
c
[

1
v
7
3
0
6
0
.
0
1
1
2
:
v
i
X
r
a

Abstract—Faster inference of deep learning models is highly
demanded on edge devices and even servers, for both ﬁnancial
and environmental reasons. To address this issue, we propose
SoftNeuro, a novel, high-performance inference framework with
efﬁcient performance tuning. The key idea is to separate algo-
rithmic routines from network layers. Our framework maximizes
the inference performance by proﬁling various routines for each
layer and selecting the fastest path. To efﬁciently ﬁnd the best
path, we propose a routine-selection algorithm based on dynamic
programming. Experiments show that the proposed framework
achieves both fast inference and efﬁcient tuning.

Index Terms—SoftNeuro, Inference engine, deep learning, op-

These toolsets maximize performance by carefully tuning
operations for each device and optimizing models with well-
known techniques such as quantization and pruning. Mean-
while, hardware vendors have also released inference engines
such as OpenVINO [9], TensorRT [23], and SNPE [26]. These
engines would be most effective when the deployment target
hardware is predetermined, as each vendor optimizes for their
own hardware. Furthermore, although these inference libraries
are successful in optimizing individual operations of DNNs,
they do not provide sufﬁcient optimization at the graph level.

timization

I. INTRODUCTION

Deep neural networks (DNNs) have shown remarkable
accuracy improvement in various domains, including computer
vision, speech recognition, and natural language processing. In
the ﬁeld of computer vision, models based on convolutional
neural networks (CNNs) have produced state-of-the-art results
in object recognition, detection, and segmentation [19], [21],
[35]. Text generation and translation using the transformer
mechanism are also attracting attention in natural language
processing [4]. More and more real-world applications have
come to take advantage of these technologies.

DNN models can be trained by well-known frameworks
such as PyTorch [24], TensorFlow [1], and MXNet [6]. These
frameworks achieve efﬁcient training by taking advantage of
well-optimized libraries. However,
those libraries are only
tuned for a narrow range of environments with sufﬁcient
resources such as GPUs. These frameworks will not run as
well on edge devices, such as smartphones, AR/VR devices,
and industrial equipment, or even less powerful servers, due
to limited computational resources and memory. Improving
computational capability is often prohibitively expensive or
impractical, be it from material costs, thermal issues, or even
power consumption related costs or environment-friendliness.
Inference optimization is thus an important task; with sufﬁ-
ciently fast inference, those devices can be used practically,
without additional computation capability.

With the above motivation, several inference focused li-
braries have been released for fast inference on edge devices.
TensorFlow Lite [13] and PyTorch Mobile [11] are repre-
sentative examples of such toolsets, and efﬁciently perform
inference of models trained by the aforementioned frameworks.

Another framework, named TVM [7], optimizes inference
on various devices by appropriately scheduling operations.
AutoTVM [8] has also been proposed to tune parameters using
machine learning. However, the search space is limited to a
small part of the many possible scheduling combinations, often
making optimization insufﬁcient. Moreover, its tuning is quite
time-consuming, as shown in our experiments.

In this paper, we address the aforementioned issues by
introducing a framework that enables efﬁcient tuning over
large search spaces at the graph level. The framework performs
fast inference of deep learning models trained by the learning
frameworks on a wide range of platforms, including CPUs,
GPUs, and DSPs. The main idea behind efﬁcient tuning is
separating algorithmic routines from network layers of DNN
models (Section III).

Our framework maximizes inference performance by using
actual execution measurements on target devices. This opti-
mization takes into account multiple aspects: target deploy-
ment devices (e.g., CPUs, GPUs, and DSPs), data types (e.g.,
ﬂoat32, ﬂoat16, and qint8), data layout (e.g., channels-last and
channels-ﬁrst), and algorithms used (e.g., direct, Winograd,
and sparse). Since a naive algorithm to ﬁnd the best routines at
the graph level may hit combinatorial explosions, we propose
an optimization algorithm based on dynamic programming
(Section IV).

Experiments in Section V show that our framework is as
much as 3 times faster than other inference frameworks on a
Galaxy S8 smartphone equipped with a Snapdragon 835 SOC.
In addition, the tuning of our framework can be 500 times
faster than that of TVM for the VGG16 model [29].

 
 
 
 
 
 
II. RELATED WORK

We review related work from the viewpoint of accelerating
the convolution operation because it occupies considerable
inference time, making its efﬁcient calculation essential [20].
There are two important factors towards accelerating con-
volutions’ computation [2]: (1) how computations are per-
formed (algorithms) and (2) how computations are divided
and in which order they are executed (scheduling). Scheduling
optimization is described in Section II-A, and algorithmic
improvements are summarized in Sections II-B and II-C.

A. Automatic Scheduling

Most inference frameworks simply select computation meth-
ods based on some heuristics [13], [16]. Other frameworks
such as Halide [27] and TVM [7] optimize scheduling inde-
pendently of employed algorithms. Halide [27] uses a genetic
algorithm to search for an approximate optimal schedule solu-
tion. In [2], improvemrnts to Halide are proposed by enabling
larger search spaces of schedules and employing a tree search
algorithm based on beam search. TVM [7] separates hardware
intrinsics from algorithms and scheduling, supporting unseen
devices. To optimize scheduling, they predict running times of
schedules with machine learning models based on XGBoost
[5] by using features such as access to various memories.

While the above methods can optimize model inference
as a whole, the number of scheduling combinations is quite
large and expensive to search, so limiting the scheduling space
and improving search efﬁciency are still important issues. We
address these issues by introducing the concepts of routines
and layers with an efﬁcient optimization algorithm based on
proﬁling.

B. Winograd Algorithm

Winograd’s minimal ﬁltering [33] was originally proposed
as a method to minimize the number of multiplications during
convolution in the context of digital signal processing. Never-
theless, it can also be extended to convolution in CNNs [18],
and is implemented by various inference frameworks. The
Winograd algorithm simultaneously calculates convolution
over tiles for a speciﬁc tile size, but the optimal tile size
needs to be properly chosen for the best performance. Some
inference frameworks estimate the appropriate tile size based
on input characteristics such as data sizes [16]. We address
this issue by measuring the performance with multiple tile
sizes and selecting the best setting.

C. Quantization

Quantization of weights and activations is another common
technique to reduce the model size and computational cost
of DNNs [30]. Quantization expresses values (e.g., 32-bit
ﬂoating point number) at a lower-precision (e.g., 8-bit) integer
and a small number of parameters. Some training frameworks
such as TensorFlow [1] have introduced a quantization-aware
training mechanism to improve accuracy [17]. In addition,
inference engines such as TensorFlow Lite [13], PyTorch
Mobile [11], and NVIDIA TensorRT [23] support 8-bit integer

quantization. SoftNeuro also implements quantization-based
routines to accelerate convolutions, as well as other compu-
tations.

III. FRAMEWORK FEATURES

This section gives an overview of SoftNeuro features:
how deep learning models are deployed in SoftNeuro (Sec-
tion III-A), how it represents deep learning models (Sec-
tion III-B), and how it optimizes model execution on various
platforms (Section III-C). It also introduces childnets, struc-
tures that facilitate the implementation of additional, high-
performance routines by reusing existing ones (Section III-D).

A. Model Deployment

A DNN model can be deployed with SoftNeuro in three
major steps: (1) Import a DNN model trained by popular
deep learning frameworks, converting it into SoftNeuro’s dnn
format. (2) Tune the model to maximize the inference speed
on the target platform. (3) Perform inference using the tuned
model. A brief description of the main components (importers,
proﬁler, and optimizer) needed for deployment follows.

Importers are responsible for two tasks: (1) Convert a
given DNN model into SoftNeuro’s format, and (2) Optimize
the computational graph of the model. Importers currently
support conversion from TensorFlow [1] and ONNX1 (which
in turn supports Caffe2 [10], Chainer [31], Microsoft Cognitive
Toolkit [34], MXNet [6], PyTorch [24], and PaddlePaddle [3]).
When converting a model, importers automatically optimize
the model’s computational graph. For instance, ReLU acti-
vation and batch normalization layers are fused into their
previous layers, reducing computational cost without affecting
accuracy.

The proﬁler and optimizer tune models to run on a given
target platform as fast as possible. To this end, SoftNeuro
separately considers layers and routines. Layers are an abstract
concept that deﬁne only what kind of computation should be
performed (e.g., ReLU and 2D convolution). On the other
hand, routines are actual implementations of layer compu-
tations optimized for various hardware platforms (e.g., Intel
CPUs, NVIDIA GPUs) and data types (e.g., ﬂoat32, qint8). A
dnn model can be regarded as a graph of layers, and tuning
is accomplished by selecting each layer’s fastest routine. For
tuning, the proﬁler measures the processing times of available
routines for each layer, and by using the proﬁling data, the
optimizer selects the best routines for layers. Further details
can be read in Sections III-B and III-C.

B. Model Representation

A dnn model consists of a net, a directed acyclic graph
(DAG) of layers. Nodes of the graph are layers, and edges
represent computational dependencies between layers. A net
handles input in the ﬁrst layer, and the net output is obtained
from the last layer. Other layers between input and output
perform computations to calculate the ﬁnal output.

1https://onnx.ai/

A layer is an abstract structure that takes one or more n-
dimensional tensors, or blobs, performs some computations,
and outputs the resulting blob. Every layer has a type, which
deﬁnes what kind of computation should be performed. For
instance, a layer of the conv2 type takes an input blob, carries
out 2D convolution with a layer-speciﬁc kernel and bias, and
outputs the convolution result.

Layers contain two kinds of attributes, namely layer param-
eters and weights. Layer parameters specify how to execute the
layer type’s computation, and weights are trained parameters
speciﬁc to the layer. For instance, layer parameters of a conv2
layer include dilations, strides, and padding, while the weights
would be the kernel and bias.

C. Routines and Proﬁling

Routines are concrete implementations of layers, and many
implementations can exist in SoftNeuro for each layer. To dis-
criminate routines, we introduce routine descriptors. A descrip-
tor speciﬁes the platform, algorithm, and data type each routine
was made for. For instance, the cuda:float16/cudnn
routine for a conv2 layer uses the cuDNN library [22] to
perform 2D convolution in half-precision on a compatible
GPU.

The part before the slash (cuda:float16 in the example)
is called routine schema. It deﬁnes a routine’s platform and
data type. If schemas are different between routines of two
adjacent layers,
the data format or data location must be
changed appropriately. For instance, if a layer has a cpu
routine and the next layer has a cuda routine, data must
be transferred to a GPU before computing the second layer.
To this end, SoftNeuro automatically inserts an adapt layer
between such two layers. Adapt layers handle three kinds
of jobs: (1) data transfer between heterogeneous devices, (2)
casting data types, and (3) changing data layout.

Given there are multiple routines, we need to select each
layer’s best routine when deploying models. However, each
routine’s performance heavily depends on its algorithm and
various characteristics of the target platform (e.g., cache
structures). Thus, it
is practically impossible to determine
in advance the best combination of routines for all possible
platforms. We address this issue by proﬁling available routines
ﬁrst, and then selecting the best-performing routine.

To accommodate diverse characteristics of multiple devices,
we add routine parameters to each routine. Routine param-
eters specify values of internal variables used in routines.
For instance, the cpu/woc64_avx routine2 has two routine
parameters: cache size and task granularity. Each parameter is
a list of integers, and the proﬁler considers all of the possible
combinations of the parameters, thereby enabling SoftNeuro to
select the best performing routines on many kinds of devices.
Actual proﬁling is divided into two steps: (1) unit proﬁling,
which proﬁles routines in a layer-by-layer fashion, and (2)
integrated proﬁling, which considers all of a net’s routines as
a whole. This division aims to reduce the computational cost

2A conv2 routine that utilizes AVX for computation.

of integrated proﬁling. Ideally we would perform integrated
proﬁling from scratch, but the search space for a whole net
can be combinatorially large. On the other hand, performing
only unit proﬁling can be an alternative strategy, but it is
insufﬁcient due to possible differing performance between unit
and integrated cases, mainly due to cache behavior. Therefore
we opt for using unit proﬁling to ﬁlter out unpromising routine
patterns, reducing the combinations for integrated proﬁling.
Speciﬁcally, our current implementation selects routine pat-
terns performing better in unit proﬁling so that the number
of patterns in integrated proﬁling is judiciously limited to a
searchable amount.

D. Childnet

We introduce a mechanism called childnet, which enables
us to implement a new routine by reusing existing ones, for the
following two objectives: (1) to facilitate the implementation
of additional routines, and (2) to allow for efﬁcient proﬁling.
In the following, we describe how childnets achieve the
objectives by using Figure 1 as examples.

In Figure 1(a), we demonstrate that childnets can easily
implement a dense routine. Three sub-layers compose the rou-
tine: reshape, conv2, and reshape. The ﬁrst reshape
layer changes the input shape from C to 1 × 1 × C. The
conv2 layer uses the reshaped blob for 2D convolution with
a kernel of shape 1×1×C ×K. The result of convolution is
reshaped from 1×1×K to K in the last layer. In this way, a
new, efﬁcient routine can be easily constructed by combining
existing routines.

In addition, childnets allow for efﬁcient proﬁling of speciﬁc
routines by exploiting the integrated-proﬁling mechanism. An
example of such routines is a Winograd routine for 2D
convolution. A naive implementation of the routine without
childnet contains four kinds of routine parameters. In this
case, proﬁling needs to be performed for all combinations of
routine parameters. On the other hand, a Winograd routine can
also be implemented using the childnet mechanism as shown
in Figure 1(b). This Winograd routine consists of three sub-
layers: wgenc, wgconv, and wgdec. Each routine of these
layers has only a single routine parameter, and the Winograd
routine itself also has a routine parameter. In this case, we can
reduce the number of patterns to be proﬁled with the ﬁltering
mechanism of integrated proﬁling, thereby achieving efﬁcient
proﬁling.

IV. ALGORITHM FOR PERFORMANCE OPTIMIZATION

In this section, we describe the proposed algorithm for per-
formance optimization of routines using the proﬁling results.
If we could assume only a single routine schema, it would be
sufﬁcient to use a simple algorithm that selects each layer’s
fastest routine. However, the difﬁculty arises when we handle
multiple schemas with adapt layers, because we also need to
incur the cost of adaptation performed by the adapt layers
(e.g., data format conversion). Such tuning scenarios are called
hybrid tuning in this paper, and our algorithm for hybrid tuning

B. Straight Net

We ﬁrst consider the simplest case illustrated as the net A
in Figure 2, over the routine schemas Λ = {α, β}. In this case,
the size of search space for S∗ is O(2n) because we have two
options (i.e., α and β routines) for each of n layers. To reduce
the computational complexity to O(n), we employ a simple
dynamic-programming approach, formulated as the following
recurrence relation:

Si(α) = {Fi(α)} ∪ fastest 


Si−1(α),
Si−1(β)∪
{F a

i−1,i(β → α)}

,

(3)


where F a
i,j (β → α) denotes the fastest adapt routine for layer
i to layer j, and at i = 1, S1(λ) = {F1(λ)} for λ ∈ Λ. The
fastest construct is an operation that takes routine paths and
ﬁnds the fastest path.

The relation means that the optimal routine subpath at layer
i can be obtained by selecting the fastest routine for layer
i and computing the optimal routine subpath at layer i − 1.
The latter is either Si−1(α) or Si−1(β) with an adapt routine,
because we need to include the adaptation cost when schemas
change. While the above relation is for schema α, a similar
relation for schema β also holds. By using the relations, we
can show the following proposition.
Proposition 1 (Optimality of DP). The routine path ˆS =
fastestλ∈Λ Sn(λ) is optimal.

Proof. It is shown by induction that for any i, Si(λ) is the
optimal routine subpath of L1, . . . , Li, where routine ri has
schema λ. This induction is clear from the equation 3.

C. Branching Net

We then examine networks where a layer output is branched
into two layers, as shown in Figure 2’s net B. By extending
the idea for the straight case, we can calculate the optimal
routine subpath S5(λ5) at layer 5 as the union of S3(λ3) and
S4(λ4) with adapt routines connecting layers 3 and 4 to layer
5. However, this computation requires that schemas for layer
1 in S3(λ3) and S4(λ4) are identical; otherwise S5(λ5) may
have two routines for layer 1 and is invalid. To handle such
cases, we modify the deﬁnition of Si(λ) to include a constraint
Ci as Si(λ; Ci). For example, S5(λ; λ1 = α) means that the
routine for layer 1 must have schema α.

With this extension, the optimal routine subpath at layer 5

can be obtained incrementally by the following equation:




S5(α; λ1 = α) = {F5(α)} ∪ fastest
S3(α; λ1 = α) ∪ S4(α; λ1 = α),
S3(α; λ1 = α) ∪ S4(β; λ1 = α) ∪ {F a
S3(β; λ1 = α) ∪ {F a
S3(β; λ1 = α) ∪ {F a

4,5(β → α)},
3,5(β → α)} ∪ S4(α; λ1 = α),
3,5(β → α)}∪



S4(β; λ1 = α) ∪ {F a

4,5(β → α)}

(4)
The fastest term considers union of the optimal routine
subpaths at layers 3 and 4 (i.e., S3 and S4). For each subpath,
there are two candidates depending on the selected schema, as
in the straight case. Thus the fastest term evaluates the four

(a) A dense routine.

(b) A Winograd routine.

Fig. 1. Routines implemented by childnets.

is named dynamic programming for routine selection, or DPRS
for short.

In the following, we ﬁrst give basic notations and deﬁ-
nitions in Section IV-A. Then Sections IV-B–IV-E explain
the algorithm from the simplest case to gradually complex
cases. Section IV-F summarizes the algorithm workﬂow by
describing its pseudo-code.

A. Notations and Deﬁnitions

Our algorithm’s objective is to select the best routine for
each layer, given a net, a set of routine schemas, and proﬁling
data. To formulate the problem, we begin with a formal
deﬁnition of nets.

A net is a DAG with one input and one output (1-in-1-
out net) consisting of n layers. The layers are denoted as
L1, . . . , Ln in topological order, where L1 is the input layer
and Ln is the output layer. Each layer Li has a set Ri of
routines. A set Ri(λ) is the subset of Ri that contains the
routines with schema λ. We deﬁne the fastest routine Fi(λ)
within Ri(λ) as

Fi(λ) ≡ arg min
r∈Ri(λ)

T (r),

(1)

where T (r) is the execution time of routine r measured by
proﬁling.

Given these notations, we provide the deﬁnition of routine

path as follows.

Deﬁnition 1 (Routine path). A routine path S is a possible
set of routine combinations for a given net. It consists of each
layer’s routine as S = {ri ∈ Ri | 1 ≤ i ≤ n}. When the edge
(Li, Lj) exists in the net and their routines (ri and rj ) have
different schemas, the adapt routine ra
i,j is also included in the
set S.

Based on this deﬁnition, we deﬁne the optimal routine path,

which is the desired result for hybrid tuning.

Deﬁnition 2 (Optimal routine path). The optimal routine path
S∗ is the path with the minimum overall processing time
among all routine paths for a net. In other words, it is the
solution of the following combinatorial optimization:

S∗ = arg min

T (S),

S

(2)

where T (S) =

r∈S T (r) for routine path S.

P

✭✝✮ ❙✞✟✠✐❣❤✞

✭❈✮ ▼☛❧✞✐✲❜✟✠✡❝❤✐✡❣

✭

✮ ▼☛❧✞✐✲✐✡

☛✞

☛✞

☛✞

❉

☞

✴♦

☞

✭❇✮ ❇✟✠✡❝❤✐✡❣

▲❛②❡r ✶

▲❛②❡r ✶

▲❛②❡r ✶

❆✉①✳ ■♥♣✉t

▲❛②❡r

✷

▲❛②❡r

✷

▲❛②❡r

✷

▲❛②❡r ✶

▲❛②❡r

✷

▲❛②❡r ✹

▲❛②❡r

✻

▲❛②❡r ✸

▲❛②❡r ✸

▲❛②❡r ✸

▲❛②❡r ✹

▲❛②❡r ✸

▲❛②❡r

✺

▲❛②❡r

✺

▲❛②❡r ✹

▲❛②❡r

✺

▲❛②❡r

✼

▲❛②❡r ✽

(cid:0)✁✂✄ ❖✁☎✆✁☎

Fig. 2. Network structure examples.

candidates in total. Similar relations for S5 exist for different
combinations of the schema and constraint.

already satisﬁed at layer 5, when merging layers 3 and 4. Thus,
the constraint on layer 2 can be relaxed at layer 5 as follows:

More generally, the constraint can be regarded as an element
of the mth Cartesian power of routine schemas Λ when there
are m constraint layers. The mth Cartesian power of Λ is

Λm ≡ Λ × · · · × Λ

.

(5)

m
{z

}

|

When we let A = {a1, . . . am} be a set of indices of m
constrained layers, a tuple λA = (λa1 , . . . , λam ) ∈ Λm
represents a combination of schema constraints for the layers.
When λA is in the constraint, the fastest routine for layer j
with schema λj, Fj (λj), must be included in the constrained
optimal routine path, Si(λi; λA). Based on this notation, we
can show the following proposition.

Proposition 2 (Optimality of constrained DP). The routine
path ˆS obtained by the constrained optimal routine subpath
as follows is optimal.

ˆS =

fastest
λ∈Λ,λCn ∈Λ|Cn |

SN (λ; λCn ).

(6)

Proof. Derived by the deﬁnition of constrained DP variables
and generalizing the recurrence formula (Equation 4) in merg-
ing branches.

S5(α; λ1 = α) = fastest

S5(α; λ1 = α, λ2 = α),
S5(α; λ1 = α, λ2 = β).

(cid:26)

(7)

In this equation, we remove the constraint of λ2 by selecting
the faster subpath. Similar relaxation can also be applied for
S5(α; λ1 = β), S5(β; λ1 = α), and S5(β; λ1 = β). A
more general condition for the relaxation is summarized as
the following proposition.

Proposition 3 (Condition for relaxing constraints). Let Ai be a
set of layers through which all downstream paths of branching
layer Li pass. The constraint on Li at Lj can be relaxed if
and only if Lj ∈ Ai.

Proof. If Lj ∈ Ai, then all downstream paths of Li go through
Lj. At the downstream of Lj, merging with a constraint on
Li does not occur. Hence we can relax the constraint on Lj
in this case.

Conversely, if Lj 6∈ Ai, then there is a path ρ that passes
through Li without travelling Lj. Besides, there exists a path
ρ′ at the downstream of Lj that intersects ρ at Lk(j < k ≤ n),
because a net has only one output. Since the constraint on
Li is required at the intersecting point, we cannot relax the
constraint at Lj.

D. Multi-branching Net

E. Multi-input, Multi-output Net

Now we inspect networks with multiple branches illustrated
by the net C in Figure 2. As a naive extension, we could
apply the idea for branching nets by constraining each branch.
For instance, layers 1 and 2 need to be constrained in the
example. In this case, we need to evaluate eight patterns in
total to compute the optimal routine subpaths of layers at the
downstream of layer 2 (i.e., layers 3, 4, 5, and 8), because
the unconstrained layers also have two schema options. In
short, a naive extension of the branching case results in 2b
combinations of schemas for nets with b branches.

To alleviate this cost, we relax the constraint when merging
branches. For example, when merging the subpaths of layers 5
and 7 for layer 8, the necessary constraint is only the schema
of layer 1. This is because the constraint on layer 2 must be

So far, we have explored only 1-in-1-out networks, but
networks with multiple inputs and outputs have recently been
utilized for tasks such as synthesizing images and multi-task
learning [12], [32]. To adapt the above discussion to such
cases as well, we introduce auxiliary input and output layers
as shown in the net D (Figure 2). The auxiliary input layer
connects to all input layers, and the auxiliary output layer
merges all output layers. The auxiliary layers have a single
routine with zero processing time, so there is no impact on
performance optimization.

F. Algorithm

Algorithm 1 shows the pseudo-code for hybrid tuning, a
generalization of the above discussion. The algorithm receives

TABLE I
AVERAGE INFERENCE TIMES IN MILLISECONDS WITH STANDARD DEVIATIONS IN PARENTHESES, UNDER VARIOUS TUNING OPTIONS.

Tuning option

Network

ﬂoat32 (w/o tune)

ﬂoat32

qint8

hybrid

VGG16
ResNet50
MobileNetV2
MobileNetV3

548.442 (±16.904)
168.179 (±0.094)
20.295 (±0.044)
7.832 (±0.034)

219.603 (±0.963)
111.344 (±1.606)
16.246 (±0.056)
5.722 (±0.038)

372.658 (±1.251)
119.607 (±0.527)
15.409 (±0.017)
9.142 (±0.040)

198.453 (±3.758)
102.544 (±1.373)
14.931 (±0.019)
5.695 (±0.066)

TABLE II
AVERAGE INFERENCE TIMES OF INFERENCE FRAMEWORKS, SHOWN IN MILLISECONDS WITH STANDARD DEVIATIONS IN PARENTHESES. TENSORFLOW
LITE AND PYTORCH MOBILE MODELS WERE QUANTIZED TO 8-BIT. SOFTNEURO USED HYBRID TUNING OF FLOAT32 AND QINT8.

Network

TensorFlow Lite

PyTorch Mobile

Ours

VGG16
ResNet50
MobileNetV2
MobileNetV3

403.062 (±12.053)
129.698 (±0.871)
25.730 (±0.639)
7.479 (±0.036)

N/A
152.172 (±0.766)
55.653 (±0.031)
N/A

198.453 (±3.758)
102.544 (±1.373)
14.931 (±0.019)
5.695 (±0.066)

a 1-in-1-out net; thus a network with multiple inputs and
outputs is converted, with layers re-indexed in topological
order, before applying the algorithm. Meanwhile, if the input
net has a childnet, the algorithm is applied recursively starting
from the innermost childnet. For simplicity, we always add
adapt routines at line 12, and insert a void adapt routine
(T (ra
i,j) = 0) when it is not needed. Note that the constraint
when obtaining S∗ at line 33 is always an empty set, because
at the output layer, all constraints should already be satisﬁed.

V. EXPERIMENTS

We evaluate inference performance on the VGG16 [29],
ResNet50 [14], MobileNetV2 [28] and MobileNetV3 [15]
models, which are commonly used for classiﬁcation or as back-
bones for other tasks. Inference performance was measured on
a Samsung Galaxy S8 containing a Snapdragon 835 SOC [25].
We run inference 20 times per experiment for obtaining the
averages and standard deviations. We also compare the tuning
and inference speeds of SoftNeuro with those of TVM.

A. Hybrid Tuning Performance

We compare the inference speeds of four cases: (1) without
tuning, (2) tuning only for ﬂoat32 routines, (3) tuning only for
qint8 routines, and (4) hybrid tuning of ﬂoat32 and qint8. The
result is summarized in Table I.

Hybrid tuning achieves the best result for all tested models,
by as much as 9.6% compared to ﬂoat32-only tuning on
VGG16. This is because it considers not only the inference
speed of each routine, but also the conversion costs between
ﬂoat32 and qint8, allowing for the optimal combination of
ﬂoat32 and qint8 routines. It is of note that ﬂoat32-only tuning
achieved better results than qint8, except for MobileNetV2.
This reveals that ﬂoat32 routines can be faster than qint8

TABLE III
SELECTED ROUTINE DESCRIPTORS AND PARAMETERS FOR VGG16 BY
HYBRID TUNING.

Layer

Routine Descriptor

Routine Parameters

cache:8192
task ops:32768
tile size:4

tile size:8
tile size:6

tile size:8
tile size:8
tile size:8

tile size:8
tile size:4
tile size:4

tile size:6
tile size:6
tile size:6

block1 conv1

cpu/owc32 neon

block1 conv2
block1 pool
block2 conv1
block2 conv2
block2 pool
block3 conv1
block3 conv2
block3 conv3
block3 pool
block4 conv1
block4 conv2
block4 conv3
block4 pool
block5 conv1
block5 conv2
block5 conv3
block5 pool
ﬂatten
fc1
fc2
fc3
softmax

cpu/wg2
cpu/neon
cpu/wg2
cpu/wg2
cpu/neon
cpu/wg2
cpu/wg2
cpu/wg2
cpu/neon
cpu/wg2
cpu/wg2
cpu/wg2
cpu:qint8/neon
cpu/wg2
cpu/wg2
cpu/wg2
cpu:qint8/neon
cpu:qint8
cpu:qint8
cpu:qint8
cpu:qint8
cpu/naive

routines in some conditions, in this case, due to Winograd’s
efﬁciency for ﬂoat32. In fact, qint8 routines were selected for
only six layers of VGG16, as shown in Table III. Specif-
ically, the qint8 routines are used for block4 pooling,
block5 pooling, flatten, fc1, fc2, and fc3. Note

Algorithm 1 DPRS for Hybrid Tuning

1: Input: 1-in-1-out net {Li}, Layer-wise optimal routines

{Fi}, {F a

i,j}, Routine schema set Λ

S0(λ; ∅) ← ∅

2: Output: Optimal routine path S∗
3: C0 ← ∅
4: for λ ∈ Λ do
5:
6: end for
7: for i in 1, . . . , N do
8: H ← Indices of layers directly connected to Li
9:

Ci ← Ch from one of h ∈ H (All Ch are equal)
for λ ∈ Λ do

10:
11:

12:

13:
14:

15:

16:
17:

18:

19:
20:

21:
22:

23:
24:

25:

26:
27:

28:

29:
30:

for λCi ∈ Λ|Ci| do

Si(λ; λCi ) ← {Fi(λ)}∪

fastest
λH ∈Λ|H|

end for

Sj∈H

Sj(λj ; λCi ) ∪ {F a

j,i(λj → λ)}

end for
if Li is a merging layer then

K ← Subset of Ci to be relaxed at Li
Ci ← Ci\K
for λ ∈ Λ do

for λCi ∈ Λ|Ci| do

Si(λ; λCi ) ← fastest
λK ∈Λ|K|

end for

Si(λ; λCi , λK)

end for

end if
if Li is a branching layer then

for λ ∈ Λ do

for λCi ∈ Λ|Ci| do

Si(λ; λCi , λi = λ) ← Si(λ; λCi )

end for

end for
Ci ← Ci ∪ {i}

end if
31:
32: end for
33: S∗ ← fastest

λ∈Λ

SN (λ; ∅)

that block4 pooling is situated between ﬂoat32 routines.
Such routine selection is enabled by hybrid tuning; the tuning
algorithm determines that
the qint8 routine is faster than
ﬂoat32 routines even if quantization and dequantization need
to be performed.

B. Comparison with Other Frameworks

We compare SoftNeuro inference speeds with two infer-
ence frameworks: TensorFlow Lite (ver. r2.4) and PyTorch
Mobile (ver. 1.7.1). Table II shows the result of experiments.
PyTorch Mobile does not have readily available VGG16 and
MobileNetV3 models, so they were not measured. For a fair
comparison with hybrid tuning, quantized models were used
in TensorFlow Lite and PyTorch Mobile.

SoftNeuro outperforms TensorFlow Lite and PyTorch Mo-
bile for all models. Notably, SoftNeuro is more than 2 times

TABLE IV
TUNING AND AVERAGE INFERENCE TIMES.

Tuning Time (s)

Inference Time (ms)

Network

TVM

Ours

TVM

Ours

VGG16
ResNet50

33,240
127,900

66
14

84.928
23.881

18.498
11.901

faster than TensorFlow Lite on VGG16. Compared with
PyTorch Mobile, SoftNeuro is more than 3 times faster on
MobileNetV2.

C. Tuning Speed

We measured the tuning and inference speeds for VGG16
and ResNet50 using TVM and SoftNeuro on an Intel Xeon
Gold 6126 CPU (Table IV). TVM was set to quantize models
to 8-bit and tune with 2,000 trials per operation, matching the
conﬁguration used in the TVM paper [7]. SoftNeuro shows
more than an order of magnitude faster tuning over TVM while
also achieving as much as thrice faster inference speeds.

VI. CONCLUSIONS AND FUTURE WORK

In this paper, we have described a novel, performance-
optimized inference framework. The core idea is to separate
routines from layers, thereby enabling efﬁcient and ﬂexible
performance optimization. The optimization has been formu-
lated as a problem of ﬁnding the fastest routine at each layer
while taking into account the adaptation cost. To efﬁciently
solve the problem, we have proposed the DPRS algorithm
based on dynamic programming. Our experiments show that
the proposed framework achieves faster inference and more
efﬁcient tuning than other frameworks.

In future we plan to optimize the processing time with
restrictions of accuracy and memory usage. SoftNeuro cur-
rently considers only processing time when tuning the perfor-
mance. However, quantization may degrade accuracy beyond
acceptable limits. Meanwhile, the Winograd algorithm may
consume a large amount of memory due to kernel sizes
that increase with tile sizes. Such accuracy degradation and
excessive memory consumption should be alleviated in the
future.

REFERENCES

[1] Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
Ghemawat, S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga,
R., Moore, S., Murray, D. G., Steiner, B., Tucker, P., Vasudevan, V.,
Warden, P., Wicke, M., Yu, Y., and Zheng, X. (2016). TensorFlow: A
system for large-scale machine learning. In OSDI, pp. 265–283.

[2] Adams, A., Ma, K., Anderson, L., Baghdadi, R., Li, T.-M., Gharbi, M.,
Steiner, B., Johnson, S., Fatahalian, K., Durand, F., and Ragan-Kelley,
(2019). Learning to optimize halide with tree search and random
J.
programs. ACM Trans. Graph., 38(4), Article 121.

[3] Baidu.

(2021).
https://github.com/PaddlePaddle/Paddle

PaddlePaddle.

Retrieved

from

[29] Simonyan, K. and Zisserman, A.

(2015).

tional networks for large-scale image recognition.
arXiv:1409.1556 [cs.CV].

Very deep convolu-
arXiv preprint,

[30] Sze, V., Chen, Y., Yang, T., and Emer, J. S. (2017). Efﬁcient Processing
of Deep Neural Networks: A Tutorial and Survey. Proc. IEEE, 105(12),
2295–2329.

[31] Tokui, S., Okuta, R., Akiba, T., Niitani, Y., Ogawa, T., Saito, S., Suzuki,
S., Uenishi, K., Vogel, B., and Yamazaki Vincent, H. (2019). Chainer:
A Deep Learning Framework for Accelerating the Research Cycle. In
KDD pp. 2002–2011,

[32] Vandenhende, S., Georgoulis, S., Van Gansbeke, W., Proesmans, M., Dai,
D., and Van Gool, L. (2021). Multi-Task Learning for Dense Prediction
Tasks: A Survey. IEEE Trans. Pat. Anal. Mach. Intel., Early Access.

[33] Winograd, S. (1978). Complexity Of Computations. In ACM, pp. 138–

141.

[34] Yu, D., Eversole, A., Seltzer, M., Yao, K., Kuchaiev, O., Zhang, Y.,
Seide, F., Huang, Z., Guenter, B., Wang, H., Droppo, J., Zweig, G.,
Rossbach, C., Gao, J., Stolcke, A., Currey, J., Slaney, M., Chen, G.,
Agarwal, A., Basoglu, C., Padmilac, M., Kamenev, A., Ivanov, V.,
Cypher, S., Parthasarathi, H., Mitra, B., Peng, B., and Huang, X. (2014).
An Introduction to Computational Networks and the Computational
Network Toolkit. Technical Report, MSR-TR-2014-112.

[35] Zeng, D., Liao, M., Tavakolian, M., Guo, Y., Zhou, B., Hu, D.,
(2021). Deep Learning for Scene

Pietik¨ainen, M., and Liu, L.
Classiﬁcation: A Survey. arXiv preprint, arXiv:2101.10531 [cs.CV].

[4] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-
Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,
Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A.,
Sutskever, I., and Amodei, D. (2020). Language Models are Few-Shot
Learners. arXiv preprint, arXiv:2005.14165 [cs.CL].

[5] Chen, T. and Guestrin, C. (2016). Xgboost: A scalable tree boosting

system. In KDD, pp. 785–794.

[6] Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Xu,
B., Zhang, C., and Zhang, Z. (2015). MXNet: A Flexible and Efﬁcient
Machine Learning Library for Heterogeneous Distributed Systems. In
LearningSys.

[7] Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen, H., Cowan,
M., Wang, L., Hu, Y., Ceze, L., Guestrin, C., and Krishnamurthy, A.
(2018a). TVM: An Automated End-to-End Optimizing Compiler for
Deep Learning. In OSDI, pp. 578–594.

[8] Chen, T., Zheng, L., Yan, E., Jiang, Z., Moreau, T., Ceze, L., Guestrin, C.,
and Krishnamurthy, A. (2018b). Learning to Optimize Tensor Programs.
In NIPS, pp. 3393–3404.

[9] Demidovskij, A., Gorbachev, Y., Fedorov, M., Slavutin, I., Tugarev, A.,
Fatekhov, M., and Tarkan, Y.
(2019). OpenVINO Deep Learning
Workbench: Comprehensive Analysis and Tuning of Neural Networks
Inference. In ICCVW, pp. 783–787.

[10] Facebook. (2018). Caffe2. Retrieved from https://caffe2.ai/.
[11] Facebook.

PyTorch Mobile.

(2021).

Retrieved

from

https://pytorch.org/mobile/home/.

[12] Gatys, L. A., Ecker, A. S., and Bethge, M. (2016). Image Style Transfer
Using Convolutional Neural Networks. In CVPR, pp. 2414–2423.

[13] Google.

(2021).

TensorFlow Lite.

Retrieved

from

https://www.tensorﬂow.org/lite.

[14] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep Residual Learning

for Image Recognition. In CVPR, pp. 770–778.

[15] Howard, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., Wang,
(2019). Searching for

W., Zhu, Y., Pang, R., Vasudevan, V., et al.
MobileNetV3. In ICCV, pp. 1314–1324.

[16] Jiang, X., Wang, H., Chen, Y., Wu, Z., Wang, L., Zou, B., Yang, Y., Cui,
Z., Cai, Y., Yu, T., Lv, C., and Wu, Z. (2020). MNN: A Universal and
Efﬁcient Inference Engine. In MLSys.

[17] Krishnamoorthi, R.

(2018). Quantizing deep convolutional networks
for efﬁcient inference: A whitepaper. arXiv preprint, arXiv:1806.08342
[cs.LG].

[18] Lavin, A. and Gray, S. (2016). Fast Algorithms for Convolutional Neural

Networks In CVPR, pp. 4013–4021.

[19] Liu, L., Ouyang, W., Wang, X., Fieguth, P., Chen, J., Liu, X., and
Pietik¨ainen, M. (2020). Deep Learning for Generic Object Detection:
A Survey. Int. J. Comput. Vis., 128(2), 261–318.

[20] Liu, Y., Wang, Y., Yu, R., Li, M., Sharma, V., and Wang, Y.

(2019).
Optimizing CNN Model Inference on CPUs. In USENIX ATC, pp. 1025–
1040.

[21] Minaee, S., Boykov, Y., Porikli, F., Plaza, A., Kehtarnavaz, N., and
Image Segmentation Using Deep Learning:

Terzopoulos, D.
A Survey. arXiv preprint, arXiv:2001.05566 [cs.CV].

(2020).

[22] NVIDIA. (2020a). NVIDIA cuDNN Documentation. Retrieved from

https://docs.nvidia.com/deeplearning/cudnn/index.html.

[23] NVIDIA. (2020b). NVIDIA TensorRT Documentation. Retrieved from
https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html.
[24] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G.,
Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A.,
Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner,
B., Fang, L., Bai, J., and Chintala, S. (2019). PyTorch: An Imperative
Style, High-Performance Deep Learning Library. In NeurIPS, pp. 8026–
8037.

[25] Qualcomm. (2017). Snapdragon 835 Mobile Platform. Retrieved from
https://www.qualcomm.com/products/snapdragon-835-mobile-platform.
Snapdragon Neural Processing Engine SDK.

[26] Qualcomm.

(2019).

Retrieved from https://developer.qualcomm.com/docs/snpe/index.html.

[27] Ragan-Kelley, J., Barnes, C., Adams, A., Paris, S., Durand, F., and Ama-
rasinghe, S. (2013). Halide: A Language and Compiler for Optimizing
Parallelism, Locality, and Recomputation in Image Processing Pipelines.
In PLDI, pp. 519–530.

[28] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C.
(2018). MobileNetV2: Inverted Residuals and Linear Bottlenecks.
In
CVPR, pp. 4510–4520.

