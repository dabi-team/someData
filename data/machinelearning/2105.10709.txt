Machine Learning manuscript No.
(will be inserted by the editor)

Inclusion of Domain-Knowledge into GNNs using
Mode-Directed Inverse Entailment

Tirtharaj Dash · Ashwin Srinivasan ·
A Baskar

1
2
0
2

g
u
A
4
1

]

G
L
.
s
c
[

2
v
9
0
7
0
1
.
5
0
1
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract We present a general technique for constructing Graph Neural Net-
works (GNNs) capable of using multi-relational domain knowledge. The tech-
nique is based on mode-directed inverse entailment (MDIE) developed in In-
ductive Logic Programming (ILP). Given a data instance e and background
knowledge B, MDIE identiﬁes a most-speciﬁc logical formula ⊥B(e) that con-
tains all the relational information in B that is related to e. We represent ⊥B(e)
by a “bottom-graph” that can be converted into a form suitable for GNN im-
plementations. This transformation allows a principled way of incorporating
generic background knowledge into GNNs: we use the term ‘BotGNN’ for this
form of graph neural networks. For several GNN variants, using real-world
datasets with substantial background knowledge, we show that BotGNNs per-
form signiﬁcantly better than both GNNs without background knowledge and
a recently proposed simpliﬁed technique for including domain knowledge into
GNNs. We also provide experimental evidence comparing BotGNNs favourably
to multi-layer perceptrons (MLPs) that use features representing a “proposi-
tionalised” form of the background knowledge; and BotGNNs to a standard
ILP based on the use of most-speciﬁc clauses. Taken together, these results
point to BotGNNs as capable of combining the computational eﬃcacy of GNNs
with the representational versatility of ILP.

T. Dash · A. Srinivasan
APPCAIR, BITS Pilani, India

T. Dash · A. Srinivasan · A. Baskar
Department of CS & IS
BITS Pilani, K.K. Birla Goa Campus
Goa 403726, India
E-mail: {tirtharaj,ashwin,abaskar}@goa.bits-pilani.ac.in

 
 
 
 
 
 
2

1 Introduction

Tirtharaj Dash et al.

Scientiﬁc progress is largely a cumulative enterprise: hypotheses are devised
and experiments conducted based on what is already known in the ﬁeld. Re-
cent ambitious developments (see the Nobel-Turing Grand Challenge (Kitano,
2016)) seek to accelerate signiﬁcantly the process of scientiﬁc discovery, by
automating the conjectures-and-refutations part of the scientiﬁc approach. At
the heart of the approach is the use of Machine Learning (ML) methods to
generate hypotheses about the data (we are now in the 3rd generation of such
“robot scientists” (King et al., 2009), which are being used to hypothesise can-
didate drugs for tropical diseases like malaria: (Williams et al., 2015)). The
use of domain knowledge is a necessary part of such ML methods. Indeed, a
recent extensive report on AI for Science (Stevens et al., 2020) has listed the
inclusion of domain knowledge as the ﬁrst of 3 Grand Challenges for ML and
AI:

“ML and AI are generally domain-agnostic . . . Oﬀ-the-shelf practice
treats [each of these] datasets in the same way and ignores domain
knowledge that extends far beyond the raw data itself—such as physi-
cal laws, available forward simulations, and established invariances and
symmetries–that is readily available . . . Improving our ability to sys-
tematically incorporate diverse forms of domain knowledge can impact
every aspect of AI, from selection of decision variables and architecture
design to training data requirements, uncertainty quantiﬁcation, and
design optimization.”

ML methods such as Deep Neural Networks (DNNs) have been shown to be
extremely successful at tackling prediction problems across a range of domains
such as image classiﬁcation (Krizhevsky et al., 2017), machine translation (Wu
et al., 2016), audio generation (Oord et al., 2016), visual reasoning (Johnson
et al., 2017), etc. This has largely been possible due to: (a) the availability
and easy access to large amount of data, which can be represented as numeric
tensors; (b) the availability of computational hardware to allow the massively
parallel computations required for large-scale neural learning. The capacity
to learn from large amounts of vectorised data—a welcome development in
itself—does pose some issues for a class of real-world problems, which includes
many concerned with scientiﬁc discovery. These issues are: (a) The available
data is structured: often represented as graphs of entities and their relation-
ships; (b) The available data is scarce, with instances ranging from a few 10s
to a few 100s to 1000s. What is available in compensation, however, is a large
amount of domain-knowledge, that can act as prior information. Examples
of such problems abound in the natural sciences, medicine, and in the social
sciences involving human studies.

There have been several proposals for a kind of DNNs devised speciﬁcally
to deal with graph-structured data, called Graph Neural Networks (GNNs:
(Wu et al., 2020)). The usual approach of transfer learning could alleviate
the problem of learning from small amount of data if the domains of the

Inclusion of Domain-Knowledge into GNNs using MDIE

3

source- and the target-problem are closely related. The other well-established
route for dealing with small amounts of data involves the use of prior domain
knowledge. Examples of problems with small amounts of observational data,
but with large amount of compensatory prior-knowledge abound in the nat-
ural sciences, medicine, and in the social sciences involving human studies.
However, general-purpose ways of incorporating such knowledge into neural
networks remains elusive. In contrast, Symbolic machine learning techniques
such as Inductive Logic Programming (ILP: (Muggleton and De Raedt, 1994))
have developed generic techniques for incorporating background knowledge–
albeit encoded as logical statements–into the model-construction process. This
has an immediate importance to problems of scientiﬁc discovery, given the his-
torical focus of scientiﬁc disciplines on mathematical and logical models: as a
consequence, a substantial amount of what is known can be codiﬁed in some
logical form. Further, the logical representation used by ILP systems is suﬃ-
ciently expressive for representing scientiﬁc domain knowledge. However, the
subsequent model-construction process can be computationally expensive, re-
quiring a combinatorial search that cannot exploit the recent developments
in specialised hardware and software libraries. In this paper, we adopt the
key technique used to incorporate domain knowledge by one of the most suc-
cessful form of ILP, namely, that based on mode-directed inverse entailment
(MDIE: (Muggleton, 1995)). This form of ILP usually involves a saturation
procedure, which eﬃciently identiﬁes all the relations entailed by the domain
knowledge for a speciﬁc data instance. This maximal-set of relations–called a
bottom clause–is then used by an ILP engine to ﬁnd useful logical explana-
tions for the data. Here, we develop a corresponding saturation procedure for
GNNs, which results in a maximally-speciﬁc bottom graph, which is then used
for subsequent GNN model construction. The main contributions of this paper
are as follows:

– To the ﬁeld of graph neural networks, the paper proposes a systematic

technique for incorporating symbolic domain-knowledge into GNNs.

– To the ﬁeld of neuro-symbolic learning, the paper provides substantial em-
pirical evidence using over 70 real-world datasets and domain-knowledge
consisting over hundreds of symbolic relations that the incorporation of
symbolic domain knowledge into graph-based neural networks can make a
signiﬁcant diﬀerence to their predictive performance.

The rest of the paper is organised as follows. We provide a brief speciﬁca-
tion and a general working principle of GNNs in Sec. 2. The basic details of
saturation as used in MDIE are in Sec. 3. Our adaptation of the saturation
step to construct bottom graphs is in Sec. 4. Section 5 contains an empirical
evaluation of BotGNNs. We outline some of the related works in Sec. 6 pro-
viding some relevant methods for incorporating domain-knowledge into deep
neural networks. Section 7 concludes the paper. The Appendices contain some
conceptual, implementation- and application-related details relevant to the
content in the main body of the paper.

4

Tirtharaj Dash et al.

2 Graph Neural Networks (GNNs)

GNNs are a class of deep neural networks suitable for learning from graph-
structured data. GNNs were ﬁrst introduced in (Gori et al., 2005), and re-
cently being popularised by their use in molecular property prediction prob-
lems (Gilmer et al., 2017). In this section, we provide a general working princi-
ple of GNNs concerned with graph classiﬁcation. We assume that the reader is
familiar with the basics of graphs, primarily, directed and undirected graphs,
labelled graphs, the role of a neighbourhood function in a graph, etc. However,
these details are very well discussed in (Dash et al., 2021c, Sec. 2).

GNNs are concerned with labelled graphs, that is, each vertex (and each
edge) of a graph are associated with a numeric feature-vector, essentially de-
scribing some properties of that vertex (or that edge). Here it suﬃces to state
that the deﬁning property of a GNN is that it uses some form of neural
message-passing in which messages (in vector form) are exchanged between
vertices of a graph and updated using a neural network (Gilmer et al., 2017).
For this purpose, a GNN treats the features associated with a labelled graph
as ‘messages’ and, in the message-passing process, it updates these messages.
The message-passing process is conceptually implemented by using a function
called Relabel (this is deﬁned in (Dash et al., 2021c, Defn. 4)). This involves an
iterative update of the vertex- (and edge-) labels. The output of the function
is a relabelled graph, where the vertex- (and edge-) labels are updated.

In our present work, we are concerned with problems involving classiﬁcation
of (molecular) graphs. This requires a graph-level representation (also called
graph-embedding (Hamilton, 2020)), meaning, every input graph is encoded
as a d-dimensional feature vector (in Rd). This is conceptually implemented
using a function called V ec (refer (Dash et al., 2021c, Defn. 5)) that vectorises
a relabelled graph. This feature vector is then input to a (multi-layered) neural
network, denoted by a function N N that maps the feature vector to a set of
class-labels. The whole pipeline of graph classiﬁcation is shown in Fig. 1. For
completeness of presentation, we now describe how these three functions are
related to the general working principle of GNNs.

Fig. 1: A diagrammatic representation of graph classiﬁcation using a GNN.
Graphs are of tuples of the form (V, E, σ, ψ, (cid:15)), where V is a set of vertices;
E is a set of edges; σ is some neighbourhood function; ψ is a vertex-labelling;
and (cid:15) is an edge-labelling. Often σ is left out, and derived from the edges
in E. Also, for GNNs ψ is a mapping from vertices to feature-vectors. Many
GNN implementations, including the ones used in experiments here, assume
the graph to be undirected and ignore the edge-labelling in (cid:15).

Inclusion of Domain-Knowledge into GNNs using MDIE

5

2.1 General working principle of GNNs

Let G = (V, E, σ, ψ, (cid:15)) denote a graph where V is a set of vertices; E is a set of
edges; σ is some neighbourhood function; ψ is a vertex-labelling; and (cid:15) is an
edge-labelling. As mentioned earlier, we are concerned with graph classiﬁcation
problems. That is, given a graph G, a GNN predicts its class-label.

In a graph G, let Xv denote a vector that represents the initial labelling
(ψ) of a vertex v ∈ V . That is, Xv is the feature-vector associated with the
vertex v. The relabelling function Relabel : (V, E, σ, ψ, (cid:15)) → (V, E, σ, ψ(cid:48), (cid:15)) (it-
eratively) updates the labelling of the vertices in G. This process involves two
procedures: (a) AGGREGATE: for every vertex, this procedure aggregates the
information from neighboring vertices; and (b) COMBINE: this procedure up-
dates the label of the vertex by combining its present label with its neighbors’.
Mathematically, at some iteration k, the labelling of a vertex v (denoted by
hv) is updated as:

v = AGGREGATE(k) (cid:16)(cid:110)
a(k)
v = COMBINE(k) (cid:16)
h(k−1)
h(k)
v

h(k−1)
u

, a(k)
v

(cid:111)(cid:17)

: u ∈ N (v)
(cid:17)

,

where, N (v) denotes the set of vertices adjacent to v. Initially (at k = 0),
h(0)
v = Xv.

The vectorisation function V ec : (V, E, σ, ψ(cid:48), (cid:15)) → Rd constructs a vector
representation of the entire graph (also called the graph embedding). This step
is carried out after the representations of all the vertices are relabelled by some
iterations over AGGREGATE and COMBINE. The vectorised representation of
the entire graph can be obtained using a READOUT procedure that aggregates
vertex features from the ﬁnal iteration (k = K):

hG = READOUT

(cid:16)(cid:110)

h(K)
v

(cid:111)(cid:17)

| v ∈ G

In practice, AGGREGATE and COMBINE procedures are implemented us-
ing graph convolution and pooling operations. The READOUT procedure is
usually implemented using a global or hierarchical pooling operation (Xu
et al., 2019). Variants of GNNs result from modiﬁcations to these 3 proce-
dures: AGGREGATE, COMBINE and READOUT.

2.2 Note on GNN variants used in this paper

In our present work, the GNN variants considered are a result of diﬀerent graph
convolution methods (we refer the reader to Appendix B for the mathematical
aspects of these diﬀerent approaches): (1) spectral graph convolution (Kipf
and Welling, 2017), (2) multistage graph convolution (Morris et al., 2019),
(3) graph convolution with attention (Veli˘ckovi´c et al., 2018), (4) simple-and-
aggregate graph convolution (Hamilton et al., 2017), and (5) graph convolution

6

Tirtharaj Dash et al.

with auto-regressive moving average (Bianchi et al., 2021). In addition to the
graph convolution methods mentioned above, we adopt the method of graph
pooling with structural-attention (Lee et al., 2019) to apply down-sampling to
graphs. We use the hierarchical graph-pooling approach proposed by Cangea
et al. (2018) to implement the READOUT procedure that outputs a ﬁxed-
length representation for an input graph. This representation is then input to
a multilayer perceptron (MLP) that outputs a class-label.

At this point, the complete architectural speciﬁcs of the GNN including
details on various hyperparameters are not relevant. We defer these details to
Sec. 5.3. We refer the reader to Appendix B for a detailed mathematical de-
scription on the ﬁve variants of graph neural networks, where we describe how
the graph-convolution and graph-pooling methods are implemented followed
by an elaborate description on the construction of graph-representation using
the hierarchical pooling approach.

3 Mode-Directed Inverse Entailment

Mode-directed Inverse Entailment (MDIE) was introduced by Stephen Mug-
gleton in (Muggleton, 1995), as a technique for constraining the search for
explanations for data in Inductive Logic Programming (ILP). For this pa-
per, it is suﬃcient to focus on variants of ILP that conforms to the following
input-output requirements:1
Given: (i) B, a set of clauses (constituting background- or domain-) knowl-
edge; (ii) a set of clauses E+ = {p1, p2, . . . , pN } (N > 0), denoting a con-
junction of “positive examples”; and (iii) a set of clauses E− = {n1, n2, . . . , nM }
(M ≥ 0), denoting a conjunction of “negative examples”, s.t.
Prior Necessity. B (cid:54)|= E+

Find: A ﬁnite set of clauses (usually in a subset of ﬁrst-order logic), H =

{D1, D2, . . . , Dk} s.t.
Weak Posterior Suﬃciency. For every Dj ∈ H, B∪{Dj} |= p1∨p2∨· · ·∨pN
Strong Posterior Suﬃciency. B ∪ H |= E+
Posterior Satisﬁability. B ∪ H ∪ E− (cid:54)|= (cid:50)

Here |= denotes logical consequence and (cid:50) denotes a contradiction. MDIE
implementations attempt to ﬁnd the most-probable H, given B and the data
E+, E−.2

The key concept used in (Muggleton, 1995) is to constrain the identiﬁ-
cation of the Dj using a most-speciﬁc clause. The following is adapted from
(Muggleton, 1995).

1 In the following, clauses will usually be in some subset of ﬁrst-order logic (usually Horn-
or deﬁnite-clauses). When we refer to a set of clauses, we will usually assume it to be ﬁnite.
A set of clauses C = {D1, D2, . . . , Dk} will often be used interchangebly with the logical
formula C = D1 ∧ D2 ∧ · · · ∧ Dk.

2 Usually, the entailment relation |= is used to identify logical consequences of some set
of logical sentences P . That is, what are the e’s s.t. P |= e? Here, we are given the e’s and
B, and are asking what is an H s.t. P = B ∪ H. In this sense, H is said to be the result of
inverting entailment (IE).

Inclusion of Domain-Knowledge into GNNs using MDIE

7

Remark 1 (Most-Speciﬁc Clause) Given background knowledge B and a
data-instance e (it does not matter at this point if e ∈ E+ or e ∈ E−), any
clause D s.t. B ∪ {D} |= e will satisfy D |= B ∪ e. This follows directly from
the Deduction Theorem. Let A = a1 ∧ a2 · · · ∧ an be the conjunction of ground
literals3 true in all models of B ∪ e. Hence B ∪ e |= a1 ∧ a2 ∧ · · · ∧ an. That
is, a1 ∧ a2 ∧ · · · ∧ an |= B ∪ e. Let ⊥B(e) denote a1 ∧ a2 ∧ · · · ∧ an. That is,
⊥B(e) is the clause ¬a1 ∨ ¬a2 ∨ · · · ∨ ¬an. Further, since the ais are ground,
⊥B(e) is a ground clause.

For any clause D, if D |= ⊥B(e) then D |= B ∪ e. Thus any such D sat-
isﬁes the Weak Posterior Suﬃciency condition stated earlier. ⊥B(e) is called
the most-speciﬁc clause (or “bottom clause”) for e, given B.

Thus, bottom clause construction for a data-instance e provides a mechanism
for inclusion of all the ground logical consequences given the domain-knowledge
B and the instance e.

Example 1 In the following, capitalised letters like X, Y denote variables. Let

B:

e:

parent(X, Y ) ← f ather(X, Y )
parent(X, Y ) ← mother(X, Y )

mother(jane, alice) ←

gparent(henry, john) ←

f ather(henry, jane),
mother(jane, john)

Here “←” should be read as “if ” and the commas (“,”) as “and”. So, the def-
inition for gparent is to be read as: “henry is a grandparent of john if henry
is the father of jane and jane is the mother of john.”

The conjunction A of ground literals true in all models of B ∪ e is:

¬gparent(henry, john) ∧ f ather(henry, jane) ∧ mother(jane, john) ∧
mother(jane, alice) ∧ parent(henry, jane) ∧ parent(jane, alice) ∧
parent(jane, john)

⊥B(e) = A:

gparent(henry, john) ←

f ather(henry, jane), mother(jane, john), mother(jane, alice),
parent(henry, jane), parent(jane, john), parent(jane, alice)

The above clause is logically equivalent to the disjunct: gparent(henry, john)
∨ ¬f ather(henry, jane) ∨ · · · ∨ ¬parent(jane, alice). We will also write
clause like this as the set: { gparent(henry, john), ¬f ather(henry, jane), . . . ,
¬parent(jane, alice) }.

⊥B(e) thus “extends” the example e to include relations in the background
knowledge provided: our interest is in the inclusion of the parent/2 relation.
The literals in the correct deﬁnition of gparent is a “generalised” form of

3 In theory, the number of ground literals can be inﬁnite. Practical constraints that restrict

this number to a ﬁnite size are described shortly.

8

Tirtharaj Dash et al.

subset of the literals in ⊥B(e). Of course, to ﬁnd the subset and its gener-
alised form eﬃciently is a diﬀerent matter, and is the primary concern of ILP
systems used to implement MDIE.

It is common to call the non-negated literal in the disjunct (gparent(henry, john))
as the “head” literal, and the negated literals in the disjunct as the “body”
literals. In this paper, we will restrict ourselves to ⊥B(e)’s that are deﬁnite-
clauses (clauses with exactly one head literal). This is for practical reasons,
and not a requirement of the MDIE formulation of most-speciﬁc clauses.

Construction of ⊥B(e) is called a saturation step, reﬂecting the extension of
the example by all potentially relevant facts that are derivable using B and the
example e. The domain-knowledge can encode signiﬁcantly more information
than simple binary relations (like parent above):

Example 2 Suppose data consist of the atom-and-bond structure of molecules
that are known to be toxic. Each toxic molecule can be represented by a clausal
formula. For example, a toxic molecule m1 could be represented by the logical
formula (here a1, a2 are atoms, c denotes carbon, ar denotes aromatic, and
so on):

toxic(m1) ←

atom(m1, a1, c),
atom(m1, a2, c),
...
bond(m1, a1, a2, ar),
bond(m1, a2, a3, ar),
...

The above clause can be read as: molecule m1 is toxic, if it contains atom
a1 of type carbon, atom a2 of type carbon, there is an aromatic bond between
a1 and a2, and so on. We will see later that this is a deﬁnite-clause encoding
of a graph-based representation of the molecule m1.

Given background knowledge deﬁnitions (for example, of rings and func-
tional groups), ⊥B(e) would extend the logical deﬁnition of e with relevant
parts of the background knowledge:
toxic(m1) ←

atom(m1, a1, c),
atom(m1, a2, c),
...
bond(m1, a1, a2, ar),
bond(m1, a2, a3, ar),
...
benzene(m1, [a1, a2, a3, a4, a5, a6]),
benzene(m1, [a3, a4, a8, a9, a10, a11]),
...
f used(m1, [a1, a2, a3, a4, a5, a6], [a3, a4, a8, a9, a10, a11]),

Inclusion of Domain-Knowledge into GNNs using MDIE

9

...
methyl(m1, [. . .]),
...

As seen from this example, the size of ⊥B(·) can be large. More problematically,
for complex domain knowledge, ⊥B(e) may not even be ﬁnite. To address this,
MDIE introduces the notion of a depth-bounded bottom-clause, using mode
declarations.

3.1 Modes

Practical ILP systems like Progol (Muggleton, 1995) use a depth-bounded bot-
tom clause constructed within a mode-language. We ﬁrst illustrate a simple
example of a mode-language speciﬁcation.

Example 3 A “mode declaration” for an n-arity predicate P (often written
as P/n) is one of the following kinds: (a) modeh(P (a1, a2, . . . , an)); or (b)
modeb(P (a1, a2, . . . , an)). A set of mode-declarations for the predicates in the
gparent example is: M = { modeh(gparent(+person, −person)), modeb(f ather(+person, −person)),
modeb(mother(+person, −person)), modeb(parent(+person, −person)) }.

The modeh speciﬁes details about literals that can appear in the head of
a clause in the mode-language and the modeb’s specify details about literals
that can appear in the body of a clause. A “mode declaration” refers to ei-
ther a modeh or modeb statement. Based on the mode-language speciﬁed in
(Muggleton, 1995), each argument ai in the mode declarations above is one
of: (1) +person, denoting that the argument in that literal is an ‘input’ vari-
able of type person.4 That is, the variable must have appeared either as a
−person variable in a literal that appears earlier in the body of the clause or
as a +person variable in the head of the clause; (2) −person, denoting that
the variable in the literal is an ‘output’ variable of type person. If an output
variable appears in the head of a clause, it must appear as an output variable
of some literal in the body. There are no special constraint on output variables
in body-literals. That is, they can either be a new variable, or any variable (of
the same type) that has appeared earlier in the clause. Later we will see how
mode-declarations allow the appearance of ground terms.

Example 4 Continuing Example 3, in the following X, Y, Z are variables
of type person. These clauses are all within the mode language speciﬁed in
(Muggleton, 1995): (a) gparent(X, Y ) ← parent(X, Y ); (b) gparent(X, Y ) ←
parent(X, X); (c) gparent(X, Y ) ← mother(X, Y ); and (d) gparent(X, Y ) ←
parent(X, Z), parent(Z, Y ).

4 Informally, “a variable of type γ” will mean that ground substitutions for the variable
are from some set γ. Here, γ is the set person = {henry, jane, alice, john, . . .}: that is,
person is a unary-relation.

10

Tirtharaj Dash et al.

But the following clauses are all not within the mode language in (Mug-
gleton, 1995): (e) gparent(X, Y ) ← parent(Y, Z) (Y does not appear be-
fore); (f ) gparent(X, Y ) ← parent(X, Y ), parent(Z, Y ) (Z does not appear
before); (g) gparent(henry, Y ) ← parent(henry, Z), parent(Z, Y ) (+ argu-
ments have to be variables, not ground terms); and (h) gparent(X, Y ) ←
parent(Z, jane), parent(Z, Y ) (− arguments have to be variables, not ground
terms).

We refer the reader to (Muggleton, 1995) for more details on the use of
modes. Here we conﬁne ourselves to the details necessary for the material in
this paper. We ﬁrst reproduce the notion of a place-number of a term in a
literal following (Plotkin, 1972).

Deﬁnition 1 (Term Place-Numbering) Let π = (cid:104)i1, . . . , ik(cid:105) be a sequence
of natural numbers. We say that a term τ is in place-number π of a literal λ
iﬀ: (1) π (cid:54)= (cid:104)(cid:105); and (2) τ is the term at place-number (cid:104)i2, . . . , ik(cid:105) in the term at
1 argument of λ. τ is at a place-number π in term τ (cid:48): (1) if π = (cid:104)(cid:105) then
the ith
τ = τ (cid:48); and (2) if π = (cid:104)i1, . . . , ik(cid:105) then τ (cid:48) is a term of the form f (t1, . . . , tm),
i1 ≤ m and τ is in place-number (cid:104)i2, . . . , ik(cid:105) in ti1.

Example 5 (a) In the literal λ = gparent(henry, john), the term henry
occurs in the ﬁrst argument of λ and john occurs in the second argument of
λ. The place-numbering of henry in λ is (cid:104)1(cid:105) and of john in λ is (cid:104)2(cid:105).
(b) As a more complex example, let λ = mem(a, [a, b, c]) denote the statement
that a is a member of the list [a, b, c]. The second argument of λ is short-
hand for the term list(a, list(b, list(c, nil))) (usually, the function list/2 is
represented as ‘.’/2 in the logic-programming literature). Then the term a is a
term that occurs in two place-numbers in λ: (cid:104)1(cid:105), and (cid:104)2, 1(cid:105). The term b occurs
at place-number (cid:104)2, 2, 1(cid:105) in λ; the term c occurs at place-number (cid:104)2, 2, 2, 1(cid:105) in
λ; and the term nil occurs at place-number (cid:104)2, 2, 2, 2(cid:105) in λ.

We ﬁrst present the syntactic aspects constituting a mode-language. The
meaning of these elements is deferred to the next section.

1, mt(cid:48)

2, . . . , mt(cid:48)

Deﬁnition 2 (Mode-Declaration) (a) Let Γ be a set of type names. A
mode-term is deﬁned recursively as one of: (i) +γ, −γ or #γ for some
γ ∈ Γ ; or (ii) φ(mt(cid:48)
j), where φ is a function symbol of arity
j, and the mt(cid:48)
ks are mode-terms. We will call mode-terms of type (i) simple
mode-terms and mode-declarations of type (ii) structured mode-terms;5
(b) A mode-declaration µ is of the form modeh(λ(cid:48)) or modeb(λ(cid:48)). Here λ(cid:48)
is a ground-literal of the form p(mt1, mt2, . . . , mtn) where p is a predicate
name with arity n, and the mti are mode-terms. We will say µ is a modeh-
declaration (resp. modeb-declaration) for the predicate-symbol p/n.6 We
will also use M odeLit(µ) to denote λ(cid:48).

5 For all experiments in this paper, modes consist only of simple mode-terms.
6 In general there can be several modeh or modeb-declarations for a predicate-symbol p/n.
If there is exactly one mode-declaration for a predicate symbol p/n, we will say the mode
declaration for p/n is determinate.

Inclusion of Domain-Knowledge into GNNs using MDIE

11

(c) µ is said to be a mode-declaration for a literal λ iﬀ λ and M odeLit(µ)

have the same predicate symbol and arity.

(d) Let τ be the term at place-number π in µ, We deﬁne

M odeT ype(µ, π) =






(+, γ)

(−, γ)

if τ = +γ

if τ = −γ

(#, γ)

if τ = #γ

unknown otherwise

(e) If µ is a mode-declaration for literal λ, M odeT ype(µ, π) = (+, γ) for some
place-number π, τ is the term at place π in λ, then we will say τ is an
input-term of type γ in λ given µ (or simply τ is an input-term of type
γ). Similarly we deﬁne output-terms and constant-terms.

3.2 Depth-Limited Bottom Clauses

Returning now to the most-speciﬁc clause ⊥B(e) for a data-instance e, given
background knowledge B, it is suﬃcient for our purposes to understand that
the input-output speciﬁcations in a set of mode-declarations result in a natural
notion of the depth at which any term ﬁrst appears in ⊥B(e) (terms that
appear in the head of the clause are at depth 0, terms that appear in literals
whose input terms depend only on terms in the head are at depth 1, and so
on. A formal deﬁnition follows below.) By ﬁxing an upper-bound d on this
depth, we can restrict ourselves to a ﬁnite-subset of ⊥B(e).7 This is called the
depth-limited bottom clause. Given a set of mode-declarations M, we denote
this depth-limited clause by ⊥B,M,d(e) (or simply ⊥d(e)), where d is a (pre-
speciﬁed) depth-limit. We will refer to the corresponding mode-language as a
depth-limited mode-language and denote it by LM,d. We ﬁrst illustrate this
with an example before deﬁning depth formally.

Example 6 Using the modes M in Example 3, we obtain the following most-
speciﬁc clauses for the gparent example (Example 1):

⊥B,M,1(e):

gparent(henry, john) ←

f ather(henry, jane),
parent(henry, jane)

⊥B,M,2(e):

gparent(henry, john) ←

f ather(henry, jane),
mother(jane, john),
mother(jane, alice),
parent(henry, jane),
parent(jane, john),
parent(jane, alice)

We now formally deﬁne type-deﬁnitions and depth for ground-terms.

7 In fact, additional restrictions are also needed on the number of times a relation can
occur at any depth. In implementations like (Muggleton, 1995), this is usually provided as
part of the mode declaration.

12

Tirtharaj Dash et al.

Deﬁnition 3 (Type Deﬁnitions) Let Γ be a set of types and T be a set of
ground-terms. For γ ∈ Γ we deﬁne a set of ground-terms Tγ = {τ1, τ2, . . .},
where τi ∈ T. We will say a ground-term τi is of type γ if τi ∈ Tγ, and denote
by TΓ the set {Tγ : γ ∈ Γ }. TΓ will be called a set of type-deﬁnitions.

Deﬁnition 4 (Depth of a term) Let M be a set of modes. Let C be a ground
clause. Let λi be a literal in C and let τ be an input- or output-term of type
γ in λi given some µ ∈ M. Let Yτ be the set of all other terms in body literals
of C that contain τ as an output-term of type γ. Then,

depth(τ ) =






0

if τ is an input-term of type γ

in a head literal of C

minτ (cid:48)∈Yτ depth(τ (cid:48)) + 1

otherwise

Example 7 In the previous example for C = ⊥B,M,2(e), depth(henry) = 0,
depth(jane) = depth(henry) + 1 = 1, depth(john) = depth(jane) + 1 = 2,
and depth(alice) = depth(jane) + 1 = 2.

A set of mode-declarations M (see Defn. 2), a set of type-deﬁnitions TΓ , and a
depth-limit d together deﬁne a set of acceptable ground clauses LTΓ ,M,d. Infor-
mally, LTΓ ,M,d consists of ground clauses in which: (a) all terms are correctly
typed; (b) all input terms in a body literal have appeared as output terms in
previous body literals or as input terms in any head literal; and (c) all output
terms in any head literal appear as output terms in some body literals. In this
paper, we will mainly be interested in deﬁnite-clauses (that is, m = 1 in the
deﬁnition that follows).

Deﬁnition 5 (λµ-Sequence) Assume a set of type-deﬁnitions TΓ , modes M,
and a depth-limit d. Let C = {l1, . . . , lm, ¬lm+1, . . . , ¬lk} be a clause with
k ground literals. Then (cid:104)(λ1, µ1), (λ2, µ2), . . . , (λk, µk)(cid:105) is said to be a λµ-
sequence for C iﬀ it satisﬁes the following constraints:

(a) (i) The λ’s are all distinct and (ii) For j = 1 . . . k, µj is a mode-declaration
for λj; (iii) For j = 1 . . . m, λj = lj and µj = modeh(·); (iv) For j =
(m + 1) . . . k, λj = li where ¬li ∈ C, and µj = modeb(·)

(b) If τ is an input-term of type γ in λj given µj, then:

(i) τ ∈ Tγ; and
(ii) if j > m:

– There is an input-term τ of type γ in one of λ1, . . . , λm given

µ1, . . . , µm; or

– There is an output-term τ of type γ in λi (m < i < j) given µi

(c) If τ is an output-term of type γ in λj given µj, then

(i) τ ∈ Tγ; and
(ii) if j ≤ m:

– τ is an output-term of type γ for some λi (m < i ≤ k) given µi

(d) If τ is a constant-term of type γ in λj given µj then τ ∈ Tγ
(e) There is no term τ at any place π in any λj s.t. the depth(τ ) > d.

Inclusion of Domain-Knowledge into GNNs using MDIE

13

Deﬁnition 6 (Mode-Language) Assume a set of type-deﬁnitions TΓ , modes
M, and a depth-limit d. The mode-language LTΓ ,M,d for TΓ , M, d is {C : either
C = ∅ or there exists a λµ-sequence for C}.

Example 8 Let M be the set of modes {µ1, µ2, µ3, µ4, µ5, µ6} where µ1 =
modeh(p(+int)), µ2 = modeh(p(+real)), µ3 = modeb(q(+int)), µ4 = modeb(
q(+real)), µ5 = modeb(r(+int)), µ6 = modeb(r(+real)). Let the depth-limit
d = 1. Let C be a ground deﬁnite-clause p(1) ← q(1), r(1). That is, C =
{p(1), ¬q(1), ¬r(1)}. Let: λ1 = p(1), λ2 = q(1), λ3 = r(1). Then, C is in LM,d.
The λµ-sequences for C are: (cid:104)(λ1, µ1), (λ2, µ3), (λ3, µ5)(cid:105); (cid:104)(λ1, µ1), (λ3, µ5),
(λ2, µ3)(cid:105); (cid:104)(λ1, µ2), (λ2, µ4), (λ3, µ6)(cid:105); (cid:104)(λ1, µ2), (λ3, µ6), (λ2, µ4)(cid:105);

We note that Def. 6 does not allow the following to be λµ-sequences:
(cid:104)(λ1, µ1), (λ2, µ4), (λ3, µ5)(cid:105), (cid:104)(λ1, µ2), (λ2, µ4), (λ3, µ5)(cid:105), since a 1 of type int
is treated as being diﬀerent to a 1 of type real.

We note that although the meanings of +, − and # are the same here as in
(Muggleton, 1995), clauses in LTΓ ,M,d here are restricted to being ground (in
(Muggleton, 1995), clauses are required to have variables in + and − places
of literals).

4 BotGNNs

In this section, we describe a method to translate the depth-limited most-
speciﬁc clauses of the previous section (⊥B,M,d(·)’s) into a form that can be
used by standard variants of GNNs. We illustrate the procedure ﬁrst with an
example.

Example 9 Consider ⊥B,M,2(e) in Example 6. The tabulation below shows
the literals in ⊥B,M,2(e) and matching modes

S.No.

Literal (λ)

Mode (µ)

1
2
3
4
5
6
7

gparent(henry, john) modeh(gparent(+person, −person))
f ather(henry, jane)
mother(jane, john)
mother(jane, alice)
parent(henry, jane)
parent(jane, john)
parent(jane, alice)

modeb(f ather(+person, −person))
modeb(mother(+person, −person))
modeb(mother(+person, −person))
modeb(parent(+person, −person))
modeb(parent(+person, −person))
modeb(parent(+person, −person))

The table below shows the ground-terms (τ ’s) in literals appearing in ⊥B,M,2(e)

and their types (γ’s), obtained from the corresponding term-place number in
the matching mode:

The information in these tables can be represented as a directed bipartite
graph as shown in Fig. 2. The square-shaped vertices represent (λ, µ) pairs

14

Tirtharaj Dash et al.

S.No. Term (τ ) Type (γ)

1
2
3
4

henry
john
jane
alice

person
person
person
person

in the ﬁrst table, and the round-shaped vertices represent (τ, γ) pairs in the
second table. Arcs from a (λ, µ) (square-) vertex to a (τ, γ) (round-) vertex
indicates term τ is designated by mode µ as an output or constant term (− or
#) of type γ in literal λ. Conversely, an arc from an (τ, γ) vertex to an (λ, µ)
vertex indicates that term τ is designated by mode µ as an input term (+) of
type γ in literal λ.

(a)

(b)

Fig. 2: For the gparent example: (a) depth-limited bottom-clause ⊥B,M,2(e);
and (b) the corresponding clause-graph where the vertex-labels (λ, µ)s and
(τ, γ)s are as provided in the preceding tables. The “dashed” square-box and
the “dashed” arrow are shown to indicate the vertex specifying the head of
the clause. The subscripts used in the labels correspond to the S.No. in the
tables, for example, (λ3, µ3) refers to the third-row in the ﬁrst table in this
example; and, similarly, (τ4, γ4) refers to the fourth row in the second table.

The structure in Fig. 2 is called a bottom-graph in this paper. BotGNNs are
GNN models constructed from graphs based on such clause-graphs. We ﬁrst
clarify some details needed for the construction of clause-graphs.

4.1 Notations and Assumptions

Sets. We use the following notations:

(a) Set E to deﬁne a set of relational data-instances8;

8 In this paper, this set consists of deﬁnite clauses.

Inclusion of Domain-Knowledge into GNNs using MDIE

15

(b) Sets P, F, K to denote predicate-symbols, function-symbols, and constant-

symbols, respectively;

(c) Λ to denote the set of all positive ground-literals that can be constructed
using P, F, K; and T to denote the set of all ground-terms that can be
constructed using F, K;9

(d) ΛC to denote the set of all literals in a clause C;
(e) B to denote the set of predicate-deﬁnitions constituting background

knowledge;

(f) M to denote the set of modes for the predicate-symbols in P ;
(g) Let Γ (cid:48) to denote the set of type-names used by modes in M. In addition,
we assume a special type-name R to denote a numeric type. We denote
by Γ the set Γ (cid:48) ∪ {#t : t ∈ Γ (cid:48) s.t. #t occurs in some mode µ ∈ M};
(h) LM to denote the set {(λ, µ) : λ ∈ Λ, µ ∈ M, µ is a mode-declaration
for λ }; and ET to denote the set {(τ, γ) : τ ∈ T, γ ∈ Γ, τ is of type γ};
(i) Xs to denote the set {x1, . . . , x|LM |} and Y s to denote the set {y1, . . . , y|ET |};
(j) B to denote the set of bipartite graphs10 of the form (X, Y, E) where

X ⊆ Xs, Y ⊆ Y s, and E ⊆ (Xs × Y s) ∪ (Y s × Xs);

(k) G to denote the set of labelled bipartite graphs ((X, Y, E), ψ) where

(X, Y, E) ∈ B and ψ : (Xs ∪ Y s) → (LM ∪ ET ).

(l) We will use CG(cid:62) to denote the special graph ((∅, ∅, ∅), ∅) ∈ G.
Functions. We assume bijections hx : LM → Xs; and hy : ET → Y s;
Implementation. We will assume the following implementation details:

(a) The elements of Γ are assumed to be unary predicate symbols, and
the type-deﬁnitions TΓ in Defn. 3 will be implemented as predicate-
deﬁnitions in B. That is, if a ground-term τ is of type γ ∈ Γ (that
is τ ∈ Tγ in Defn. 3) then γ(τ ) ∈ B. We will therefore refer to the
mode-language LTΓ ,M,d in Defn. 6 as LB,M,d;

(b) An MDIE implementation that, given B, M, d, ensures for any ground
deﬁnite-clause e returns a unique ground deﬁnite-clause ⊥B,M,d(e) ∈
LB,M,d if it exists or ∅ otherwise. In addition, if ⊥B,M,d(e) ∈ LB,M,d,
we assume the MDIE implementation has been extended to return at
least one matching λµ-sequence for ⊥B,M,d.

4.2 Construction of Bottom-Graphs

We now deﬁne the graph-structures or simply, the graphs constructed from
the depth-limited bottom-clauses.

Deﬁnition 7 (Literals Set) Given background knowledge B, a set of modes
M, a depth-limit d, let C be a clause in LB,M,d. We deﬁne LitsB,M,d(C), or
simply Lits(C) as follows:

9 A term is deﬁned recursively as a constant from K, a variable, or a function symbol

from F applied to term. A ground term is a term without any variables.
10 A directed graph G = (V, E) is called bipartite if there is a 2-partition of V into sets
X, Y , s.t. there are no vertices a, b ∈ X (resp. Y ) s.t. (a, b) ∈ E. We will sometimes denote
such a bipartite graph by (X, Y, E), where it is understood that V = X ∪ Y .

16

Tirtharaj Dash et al.

(i) If C = ∅ then Lits(C) = ∅;
(ii) If C (cid:54)= ∅, let LM be the set of all λµ-sequences for C. Then Lits(C) =

{(λi, µi) : S ∈ LM and (λi, µi) is in sequence S}

The deﬁnition for Lits(·) requires all λµ-sequences to ensure that Lits is well-
deﬁned. In practice, we restrict ourselves to the λµ-sequences identiﬁed by
the MDIE implementation. If these are a subset of all λµ-sequences, then
the resulting clause-graph will be “more general” than that obtained with all
λµ-sequences (see Appendix A).

Example 10 We revisit the gparent example. Let M = {µ1, µ2, µ3, µ4}, where
µ1 = modeh(gparent(+person, −person)); µ2 = modeb(f ather(+person, −person));
µ3 = modeb(mother( +person, −person); µ4 = modeb(parent(+person, −person)).
Let background knowledge B contain the type-deﬁnitions: person(henry), person(john),
person(jane), person(alice); and let depth-bound d = 2, Let C = ⊥B,M,d(e)
as in Example 6.

1. Here C = {gparent(henry, john), ¬f ather(henry, jane), ¬mother(jane, john),

¬mother(jane, alice), ¬parent(henry, jane), ¬parent(jane, john), ¬parent(jane, alice)}.

2. ΛC = { λ1, λ2, . . . , λ7} where: λ1 = gparent(henry, john), λ2 = f ather(henry, jane),
λ3 = mother(jane, john), λ4 = mother(jane, alice), λ5 = parent(henry, jane),
λ6 = parent(jane, john), λ7 = parent(jane, alice)

3. C ∈ LB,M,d because S = (cid:104)(λ1, µ1), (λ2, µ2), (λ3, µ3), (λ4, µ3), (λ5, µ4),
(λ6, µ4), (λ7, µ4)(cid:105) is a λµ-sequence for C. Some other permutations of
S will also be λµ-sequences. The reader can verify that the terms in λ-
components of S are correctly typed; input terms in the body literals ap-
pear after corresponding output terms in body-literals earlier in the λ-
components of S, or as input-terms in λ1; the output-term in λ1 appears
as an output-term in some λ later in the sequence S.

4. Then Lits(C) = {(λ1, µ1), (λ2, µ2), (λ3, µ3), (λ4, µ3), (λ5, µ4), (λ6, µ4),

(λ7, µ4)}.

Deﬁnition 8 (Terms Set) Given background knowledge B, a set of modes
M, a depth-limit d, let C ∈ LB,M,d. We deﬁne T ermsB,M,d(C), or simply
T erms(C) as follows.

If Lits(C) = ∅, then T erms(C) = ∅. Otherwise, for any pair (λ, µ) ∈
Lits(C), let T s((λ, µ)) = {(λ, µ, π) : π is a place-number s.t. M odeT ype(µ, π) =
(·, γ) for some γ ∈ Γ }. Then T erms(C) = (cid:83)

x∈Lits(C) T s(x).

Example 11 In Example 10, Lits(C) = {(λ1, µ1), (λ1, µ1), . . . , (λ7, µ4)}.
Therefore, T erms(C) = {(λ1, µ1, (cid:104)1(cid:105)), (λ1, µ1, (cid:104)2(cid:105)), (λ2, µ2, (cid:104)1(cid:105)), (λ2, µ2, (cid:104)2(cid:105)),
. . . (λ7, µ4, (cid:104)1(cid:105)), (λ7, µ4, (cid:104)2(cid:105))}.

Deﬁnition 9 (Clause-Graphs) Given background knowledge B, a set of modes
M, and a depth-limit d, we deﬁne a function ClauseT oGraph : LB,M,d → G
as follows.

If C = ∅ then ClauseT oGraph(C) = CG(cid:62) (see Sec. 4.1). Otherwise,

ClauseT oGraph(C) = ((X, Y, E), ψ) ∈ G where:

Inclusion of Domain-Knowledge into GNNs using MDIE

17

(a) X = {xi : (λ, µ) ∈ Lits(C), xi = hx((λ, µ))};
(b) Y = {yj : (λ, µ, π) ∈ T erms(C), T ermT ype((λ, µ, π)) = (τ, γ), M odeT ype(µ, π) ∈

{(+, γ), (−, γ)}, yj = hy((τ, γ))} ∪ {yj : (λ, µ, π) ∈ T erms(C), T ermT ype((λ, µ, π)) =
(τ, γ), M odeT ype(µ, π) = (#, γ), yj = hy((τ, #γ))};

(c) E = Ein ∪ Eout, where:

Ein = {(yj, xi) : (λ, µ, π) ∈ T erms(C), xi = hx((λ, µ)),
(τ, γ) = T ermT ype((λ, µ, π)), yj = hy((τ, γ)),
M odeT ype(µ, π) = (+, γ)},
and, Eout = {(xi, yj) : (λ, µ, π) ∈ T erms(C), xi = hx((λ, µ)),
(τ, γ) = T ermT ype((λ, µ, π)), yj = hy((τ, γ)),
M odeT ype(µ, π) ∈ {(−, γ), (#, γ)}}

and ψ is a vertex-labelling function deﬁned as follows:

(d) For v ∈ X, ψ(v) = h−1
(e) For v ∈ Y , ψ(v) = h−1

x (v);
y (v)

In Appendix A, we show ClauseT oGraph(·) is an injective function.

Example 12 We continue Example 11. Recall T erms(C) = { (λ1, µ1, (cid:104)1(cid:105)),
(λ1, µ1, (cid:104)2(cid:105)), (λ2, µ2, (cid:104)1(cid:105)), . . . , (λ7, µ4, (cid:104)2(cid:105))}. Then, in Defn. 9, T ermT ype((λ1, µ1,
(cid:104)1(cid:105))) = (henry, person), T ermT ype((λ1, µ1, (cid:104)2(cid:105))) = (john, person), T ermT ype(
(λ2, µ2, (cid:104)1(cid:105))) = (henry, person), . . . , T ermT ype((λ7, µ4, (cid:104)2(cid:105))) = (alice, person).
Then ClauseT oGraph(C) is as follows:

– G = (X, Y, E) where:

– X = {x1, x2, . . . , x7}, where: x1 = hx((λ1, µ1)); x2 = hx((λ2, µ2));

. . . x7 = hx((λ7, µ4))

– Y = {y1, y2, y3, y4} where: y1 = hy((henry, person)); y2 = hy((john, person));

y3 = hy((jane, person)); y4 = hy((alice, person))

– E = Ein ∪ Eout, where

Ein = {(y1, x1), (y1, x2), (y1, x5), (y3, x3), (y3, x4), (y3, x6), (y3, x7)}
Eout = {(x1, y2), (x2, y3), (x3, y2), (x4, y4), (x5, y3), (x6, y2), (x7, y4)}
– The vertex-labelling ψ is s.t. ψ(x1) = (λ1, µ1); ψ(x2) = (λ2, µ2); ψ(x3)
= (λ3, µ3); ψ(x4) = (λ4, µ3); ψ(x5) = (λ5, µ4); ψ(x6) = (λ6, µ4); ψ(x7)
= (λ7, µ4); ψ(y1) = (henry, person); ψ(y2) = (john, person); ψ(y3) =
(jane, person); ψ(y4) = (alice, person).

The reader can compare this to the graph shown diagrammatically in Fig. 2.

Example 13 Examples 10–12 do not illustrate what happens when we have
multiple matching mode-declarations. To illustrate this we repeat the exercise
with Example 8 (for consistency, we now use R instead of real) In that exam-
ple, M = {µ1, µ2, µ3, µ4, µ5, µ6} where µ1 = modeh(p(+int)), µ2 = modeh(p(+R)),
µ3 = modeb(q(+int)), µ4 = modeb(q(+R)), µ5 = modeb(r(+int)), µ6 =
modeb(r(+R)). Let the depth-limit d = 1.

1. Here C = { p(1), ¬q(1), ¬r(1) };

18

Tirtharaj Dash et al.

2. ΛC = {λ1, λ2, λ3}, where λ1 = p(1), λ2 = q(1), λ3 = r(1).
3. C ∈ LB,M,d since there is at least one λµ-sequence for C (in fact, there are

4 matching λµ-sequences: see Example 8).

4. Lits(C) = {(λ1, µ1), (λ2, µ3), (λ3, µ5), (λ1, µ2), (λ2, µ4), (λ3, µ6)}.
5. We note that the term 1 is at place-number (cid:104)1(cid:105) in all the three literals.
6. Then T erms(C) = {(λ1, µ1, (cid:104)1(cid:105)), (λ2, µ3, (cid:104)1(cid:105)), . . . , (λ3, µ6, (cid:104)1(cid:105))}
7. Then, in Defn. 9, T ermT ype((λ1, µ1, (cid:104)1(cid:105))) = (1, int), T ermT ype((λ2, µ3, (cid:104)1(cid:105)))

= (1, int), . . . , T ermT ype((λ3, µ6, (cid:104)1(cid:105))) = (1, R).

The reader can verify that ClauseT oGraph(C) = (G, ·) where G = (X, Y, E)
s.t.

– X = {x1, x2, . . . , x6}, where x1 = hx((λ1, µ1)), x2 = hx((λ2, µ3)),. . . , x6 =

hx((λ3, µ6))

– Y = {y1, y2}, where y1 = hy((1, int)) and y2 = hy((1, R))
– E = {(y1, x1), (y1, x2), (y1, x3), ((y2, x4), (y2, x5), (y2, x6)}

It is now straightforward to deﬁne graphs from most-speciﬁc clauses.

Deﬁnition 10 (Bottom-Graphs) Given a data instance e ∈ E and B, M, d
as before, let ⊥B,M,d(e) be the (depth-bounded) most-speciﬁc ground deﬁnite-
clause for e. We deﬁne BotGraphB,M,d(e) : E → G, or simply BotGraph(e)
as follows: BotGraph(e) = ClauseT oGraph(⊥B,M,d(e)).

Example 14 For our gparent/2 example described through out this paper, the
bottom-graph for the most-speciﬁc clause with d = 2 is written as: BotGraph(e) =
ClausetoGraph(⊥B,µ,2(e)), which is shown in the diagram below (the “dashed”
square-box and the “dashed” arrow are shown to indicate the vertex specifying
the head of the clause):

The vertex-labelling in the above graph is as obtained in Example 12, where γ1
denotes the type-name person, τ1, . . . , τ4 denote the terms henry, john, jane,
alice respectively. The reader can verify that the diagram above is consistent
with the bottom-graph shown in Fig. 2.

Some properties of clause-graphs are in Appendix A. The bottom-graphs
deﬁned here are not immediately suitable for GNNs for the task of graph-
classiﬁcation. Some graph-transformations are needed before providing them
as input to a GNN. We describe these transformations next.

Inclusion of Domain-Knowledge into GNNs using MDIE

19

4.3 Transformations for Graph Classiﬁcation by a GNN

We now describe functions used to transform bottom-graphs into a form suit-
able for the GNN implementations we consider in this paper. The deﬁnite-
clause representation of graphs that we use (an example follows below) contains
all the information about the graph in the antecedent of the deﬁnite-clause.
The following function extracts the corresponding parts of the bottom-graph.

Deﬁnition 11 (Antecedent-Graphs) We deﬁne function Antecedent : G →
G as follows. Let (G, ψ) ∈ G, where G = (X, Y, E) is a directed bipartite graph.
Let Xh = {x : x ∈ X, ψ(x) = (λ, µ), µ = modeh(·)}. We deﬁne (G(cid:48), ψ(cid:48)) where
G(cid:48) = (X (cid:48), Y (cid:48), E(cid:48)) and

– X (cid:48) = X − Xh
– Y (cid:48) = {y : y ∈ Y, ∃x ∈ X (cid:48) s.t. (x, y) ∈ E or (y, x) ∈ E}
– E(cid:48) = E − {(vi, vj) : vi ∈ Xh} − {(vj, vi) : vi ∈ Xh}

and, ψ(cid:48)(vi) = ψ(vi) for all vi ∈ X (cid:48) ∪ Y (cid:48). Then Antecedent((G, ψ)) = (G(cid:48), ψ(cid:48)).

Most GNN implementations, including those used in this paper, require
graphs to be undirected (Hamilton, 2020). Furthermore, an undirected graph
representation allows an easy exchange of messages across multiple relations
(the X-nodes) resulting in unfolding their internal dependencies. We deﬁne a
function that converts directed clause-graphs to undirected clause-graphs.

Deﬁnition 12 (Undirected Clause-Graphs) We deﬁne a function U Graph :
G → G as follows. Let (G, ψ) ∈ G, where G = (X, Y, E) is a directed bipartite
graph. We deﬁne (G(cid:48), ψ(cid:48)), where G(cid:48) = (X (cid:48), Y (cid:48), E(cid:48)) and

– X (cid:48) = X
– Y (cid:48) = Y
– E(cid:48) = E ∪ {(vj, vi) : (vi, vj) ∈ E}

and ψ(cid:48)(vi) = ψ(vi) for all vi ∈ X (cid:48) ∪ Y (cid:48). Then U Graph((G, ψ)) = (G(cid:48), ψ(cid:48)).

In fact, graphs for GNNs are not actually in G. GNN implementations usu-
ally require vertices in a graph to be labelled with numeric feature-vectors.
This requires a modiﬁcation of the vertex-labelling to be a function from ver-
tices to real-vectors of some ﬁnite length. The ﬁnal transformation converts
the vertex-labelling of a graph in G into a suitable form.

Deﬁnition 13 (Vectorise) Let (G, ψ) ∈ G, where G = (X, Y, E). Assume we
are given a set of modes M. Let Γ# be the set of all type-names γ ∈ Γ −{R, #R}
such that #γ in some mode µ ∈ M. Let T# ⊆ T be the set of ground-terms of
types in Γ#.11

11 That is, Γ# is the set of all #-ed, non-numeric type-names in M. and T# is the set of
all ground-terms of #-ed non-numeric types.

20

Tirtharaj Dash et al.

Let us deﬁne the following four functions from X ∪ Y to the set of all real

vectors of ﬁnite length. For v ∈ X ∪ Y :

fρ(v) =

fγ(v) =

fτ (v) =

(cid:40)

onehot(P, r) if v ∈ X, h(v) = (λ, ·) and predsym(λ) = r

0|P |

otherwise

(cid:40)

onehot(Γ, γ) if v ∈ Y and h(v) = (τ, γ)

0|Γ |

otherwise

(cid:40)

onehot(T#, τ ) if v ∈ Y and h(v) = (τ, #γ) and γ (cid:54)∈ R

0|T#|

otherwise

fR(v) =

(cid:40)

[τ ]
01

if v ∈ Y and h(v) = (τ, #R)
otherwise

where 0d denotes the zero-vector of length d; predsym(l) is a function that
returns the name and arity of literal l; and onehot(S, x) denotes a one-hot
vector encoding of x ∈ S.12

Let V ectorise be a function deﬁned on G as follows: V ectorise((G, ψ)) =
(G, ψ(cid:48)) where ψ(cid:48)(v) = fρ(v) ⊕ fγ(v) ⊕ fτ (v) ⊕ fR(v) for each v ∈ X ∪ Y . Here
⊕ denotes vector concatenation.

We note that the vectors in the vertex-labelling from V ectorise should not
be confused with the vector obtained using the V ec function employed within
a GNN (see Fig. 1) in Sec. 2. The purpose of that function is to obtain a
low-dimensional real-valued vector representation for an entire graph (usually
for problems of graph-classiﬁcation).

Example 15 Recall the most-speciﬁc clause for the gparent(henry, john) in
Example 1: gparent(henry, john) ← f ather(henry, jane), mother(jane, john),
mother(jane, alice), parent(henry, jane), parent(jane, john), parent(jane, alice).
The clause-graph and corresponding antecedent-graph are shown below.
Assume the following sets: P = {gparent/2, f ather/2, mother/2, parent/2},
Γ = {person}, Γ# = ∅, T# = ∅.

Additionally, since the mode-language in Example 1 does not have any
#’ed arguments, T# = ∅. So: fρ is a 4-dimensional (one-hot encoded) vector
(since |P | = 4); fγ is a 1-dimensional vector (since |Gamma| = 1); fτ is a
1-dimensional vector containing 0 (since |T#| = 0); and fR is a 1-dimensional
vector containing 0 (since there are no #’ed numeric terms)). A full tabulation
of the vectors involved is provided below, along with the new vertex-labelling
that results. In the table, the vertex labels are as obtained in Example 12; γ1
is used to denote the type person.

12 A one-hot vector encoding of an element x in a set S assumes a 1-1 mapping N from
elements of S to {1, . . . , |S|}. If x ∈ S and onehot(S, x) = v then v is a vector of dimension
|S| s.t. N (x)’th entry in v is 1 and all other entries in v are 0.

Inclusion of Domain-Knowledge into GNNs using MDIE

21

v
x2
x3
x4
x5
x6
x7
y1
y2
y3
y4

ψ(v)
(λ2, µ2)
(λ3, µ3)
(λ4, µ3)
(λ5, µ4)
(λ6, µ4)
(λ7, µ4)
(τ1, γ1)
(τ2, γ1)
(τ3, γ1)
(τ4, γ1)

fρ(v)(cid:62)
[0, 1, 0, 0]

[0, 0, 1, 0]

[0, 0, 1, 0]

[0, 0, 0, 1]

[0, 0, 0, 1]

[0, 0, 0, 1]

[0, 0, 0, 0]

[0, 0, 0, 0]

[0, 0, 0, 0]

[0, 0, 0, 0]

fγ(v)(cid:62) fτ (v)(cid:62) fR(v)(cid:62)
[0.0]
[0]

[0]

ψ(cid:48)(v)(cid:62)
[0, 1, 0, 0, 0, 0, 0.0]

[0]

[0]

[0]

[0]

[0]

[1]

[1]

[1]

[1]

[0]

[0]

[0]

[0]

[0]

[0]

[0]

[0]

[0]

[0.0]

[0.0]

[0.0]

[0.0]

[0.0]

[0.0]

[0.0]

[0.0]

[0.0]

[0, 0, 1, 0, 0, 0, 0.0]

[0, 0, 1, 0, 0, 0, 0.0]

[0, 0, 0, 1, 0, 0, 0.0]

[0, 0, 0, 1, 0, 0, 0.0]

[0, 0, 0, 1, 0, 0, 0.0]

[0, 0, 0, 0, 1, 0, 0.0]

[0, 0, 0, 0, 1, 0, 0.0]

[0, 0, 0, 0, 1, 0, 0.0]

[0, 0, 0, 0, 1, 0, 0.0]

The following ﬁgures show: (a) the antecedent graph and (b) the vectorised,
undirected, antecedent graph for the gparent example. We call the structure in
(b) as a BotGNNGraph, the deﬁnition of which is provided later.

The example above does not have any #-ed arguments in the modes M. In
the following example, we consider modes that have #-ed arguments (of types:
R and not R) and repeat the same exercise: starting with the construction of
the bottom-graph. Then we show how the function V ectorise results in a
vectorised graph suitable for a GNN.
Example 16 Let M be the set of modes {µ1, µ2, µ3} where µ1 = modeh(p(+R)),
µ2 = modeb(q(+R, #colour)), µ3 = modeb(r(#colour, #R)). Let the depth-
limit d = 1 and that the background knowledge contains the type-deﬁnitions
colour(white) and colour(black). Let C be a ground deﬁnite-clause p(1.0) ←
q(1.0, white), r(white, 1.0). The following are obtained based on the deﬁnitions:

– C = {p(1.0), ¬q(1.0, white), ¬r(white, 1.0)}.
– ΛC = {λ1, λ2, λ3}, where λ1 = p(1.0), λ2 = q(1.0, white), λ3 = r(white, 1.0).
– C is in LB,M,d since there is at least one λµ-sequence for C. Here we have

one such sequence: (cid:104)(λ1, µ1), (λ2, µ2), (λ3, µ3)(cid:105)

– Lits(C) = {(λ1, µ1), (λ2, µ2), (λ3, µ3)}
– T erms(C) = {(λ1, µ1, (cid:104)1(cid:105)), (λ2, µ2, (cid:104)1(cid:105)), (λ2, µ2, (cid:104)2(cid:105)), (λ3, µ3, (cid:104)1(cid:105))(λ3, µ3, (cid:104)2(cid:105))}
– T ermT ype((λ1, µ1, (cid:104)1(cid:105))) = (1.0, R), T ermT ype((λ2, µ2, (cid:104)1(cid:105))) = (1.0, R),
T ermT ype((λ2, µ2, (cid:104)2(cid:105))) = (white, #colour), T ermT ype((λ3, µ3, (cid:104)1(cid:105))) =
(1.0, #R) T ermT ype((λ3, µ3, (cid:104)2(cid:105))) = (white, #colour)

22

Tirtharaj Dash et al.

Then, ClauseT oGraph(C) = (G, ψ), where G = (X, Y, E) s.t.

– X = {x1, x2, x3}, where x1 = hx((λ1, µ1)), x2 = hx((λ2, µ2)) and x3 =

– Y = {y1, y2, y3}, where y1 = hy((1.0, R)), y2 = hy((white, #colour)), y3 =

hx((λ3, µ3))

hy((1.0, #R))

– E = {(y1, x1), (y1, x2), (x2, y2), (x3, y2), (x3, y3)}

and, the vertex-labelling ψ is as follows: ψ(x1) = (λ1, µ1), ψ(x2) = (λ2, µ2),
ψ(x3) = (λ3, µ3), ψ(y1) = (1.0, R), ψ(y2) = (white, #class), ψ(y3) = (1.0, #R).
In this example, we assume the following sets: P = {p/1, q/2, r/2}, Γ =
{R, #colour, #R}, Γ# = {#colour}, T# = {white, black}.

The graph (G, ψ) constructed above is the bottom-graph for this particu-
lar example. The feature-vectors obtained from the functions in V ectorise are
tabulated below. In the table, τ1 = 1.0, τ2 = white, γ1 = R, γ2 = #colour,
γ3 = #R.

v
x2
x3
y1
y2
y3

ψ(v)
(λ2, µ2)
(λ3, µ3)
(τ1, γ1)
(τ2, γ2)
(τ1, γ3)

fρ(v)(cid:62)
[0, 1, 0]

fγ(v)(cid:62) fτ (v)(cid:62) fR(v)(cid:62)
[0.0]
[0, 0]
[0, 0, 0]

ψ(cid:48)(v)(cid:62)
[0, 1, 0, 0, 0, 0, 0, 0, 0.0]

[0, 0, 1]

[0, 0, 0]

[0, 0, 0]

[1, 0, 0]

[0, 0, 0]

[0, 1, 0]

[0, 0, 0]

[0, 0, 1]

[0, 0]

[0, 0]

[1, 0]

[0, 0]

[0.0]

[0.0]

[0.0]

[1.0]

[0, 0, 1, 0, 0, 0, 0, 0, 0.0]

[0, 0, 0, 1, 0, 0, 0, 0, 0.0]

[0, 0, 0, 0, 1, 0, 1, 0, 0.0]

[0, 0, 0, 0, 0, 1, 0, 0, 1.0]

The following ﬁgure shows how the ﬁnal vectorised graph is constructed from
the bottom-graph (the dotted square-box and the dotted arrow are shown to
indicate the vertex specifying the head of the clause C):

The functions Antecedent, U Graph and V ectorise transform bottom-graphs
into a form suitable for GNNs by straightforward composition:

Deﬁnition 14 (Graph Transformation) We deﬁne a transformation over
G as follows: T ransf ormGraph(G) = V ectorise(U Graph(Antecedent((G, ψ)))).

Inclusion of Domain-Knowledge into GNNs using MDIE

23

We now have all the pieces for obtaining graphs suitable for GNNs:

Deﬁnition 15 (BotGNN Graphs) Given a data instance e ∈ E and B, M, d
as before, we deﬁne BotGN N GraphB,M,d(e), or simply BotGN N Graph(e) =
T ransf ormGraph(BotGraphB,M,d(e))

Figure 3 summarises the sequence of computations used in this paper. We will
use the term BotGN N to describe GNNs constructed from BotGNN graphs.

Fig. 3: Construction and use of bottom-graphs for use by GNNs in this paper.
We note that constituting the transformation of bottom-graphs are for the
GNN implementations used in this paper.

Procedures 1, 2 use the deﬁnitions we have introduced to construct and test
BotGN N models.13 The procedures assume that data provided as graphs can
be represented as deﬁnite clauses (Steps 1 in Procedure 1 and 1 in Procedure
2). We illustrate this with an example.

Example 17 The chemical Tacrine is a drug used in the treatment of Alzheimer’s
disease. It’s molecular formula is C13H14N2, and its molecular structure is
shown in diagrammatic form below:

One representation of this molecular graph as a deﬁnite clause is:

graph(tacrine) ←

atom(tacrine, a1, c),
atom(tacrine, a2, c),
...
atom(tacrine, a13, c),
atom(tacrine, a14, n),
...
bond(tacrine, a1, a2, 1),
bond(tacrine, a2, a3, 2),
...

13 In practice, Step 2 of Procedure 1 and Step 2 of Procedure 2 involve some pre-processing
that converts the information in BotGNN graphs into a syntactic form suitable for the im-
plementations used. We do not describe these pre-processing details here: they are available
as code accompanying the paper.

24

Tirtharaj Dash et al.

More generally, a graph g = (V, E, ψ, φ) (where V denotes the vertices, E
denotes the edges, ψ and φ are vertex and edge-label mappings) can be trans-
formed into deﬁnite clause of the form graph(g) ← Body, where Body is
a conjunction of ground-literals of the form vertex(g, v1), vertex(g, v2), . . . ;
edge(g, e1), edge(g, e2), . . . ; vlabel(g, v1, ψ(v1)), vlabel(g, v2, ψ(v2)), . . . ; and
elabel(g, e1, ψ(e1)), elabel(g, e2, ψ(e2)), . . . and so on where V = {v1, v2, . . .},
E = {e1, e2, . . .}. More compact representations are possible, but in the ex-
perimental section following, we will be using this kind of simple transforma-
tion (for molecules: the transformation is done automatically from a standard
molecular representation).

Procedure 1: (TrainBotGNN) Construct a BotGN N model, given
training data {(gi, yi)}N
1 , where each gi is a graph and yi is the class-
label for gi.

Data: Background knowledge B, modes M, depth-limit d, training data
1 , and some procedure T rainGN N that trains a

Dtr = {(gi, yi)}N
graph-based neural network

Result: A BotGN N
tr = { (g(cid:48)
1. D(cid:48)
g(cid:48)
i = BotGN N GraphB,M,d(ei) }
2. Let BotGN N = T rainGN N (D(cid:48)
3. return BotGN N

tr)

i, yi) : (gi, yi) ∈ Dtr, ei be a ground deﬁnite-clause representing gi,

Procedure 2: (TestBotGNN) Obtain predictions of a BotGN N
model on a data set

Data: A BotGN N model, background knowledge B, modes M, depth-limit d, and

data D consisting of a set of graphs {gi}N
1

Result: {(gi, ˆyi)}N
1. Let D(cid:48) = {(gi, g(cid:48)

1 where the ˆyi are predictions by BotGN N
i) : gi ∈ D, ei is the deﬁnite-clause representation of gi,

g(cid:48)
i = BotGN N GraphB,M,d(ei)}

2. Let P red = {(gi, ˆyi) : (gi, g(cid:48)
3. return P red

i) ∈ D(cid:48), ˆyi = BotGN N (g(cid:48))}

4.4 Note on Diﬀerences to Vertex-Enrichment

While we defer most related work to a later section (Sec. 6), it is useful to clar-
ify here some diﬀerences of BotGNNs with the approach of vertex-enrichment
in GNNs (or VEGNNs). These were introduced in (Dash et al., 2021c) with the
same goal of incorporating symbolic domain-knowledge into GNNs. An imme-
diate diﬀerence is in the nature of the graphs handled by the two approaches.
Broadly, VEGNNs require data in a graphical form. VEGNNs retain the most

Inclusion of Domain-Knowledge into GNNs using MDIE

25

of the original graph-structure, but modify the feature-vectors associated with
each vertex of the graph (more on this below). BotGNNs on the other hand
do not require data to be a graph. Instead, any data representable as a def-
inite clause are reformulated using the bottom-clause into BotGNN graphs.
Recall these are bipartite-graphs, in which both vertices and their labels have
a diﬀerent meaning to the graphs in VEGNNs.

A subtler diﬀerence between BotGNNs and VEGNNs arises from how the
relational information is included within the graphs constructed in each case.
The diﬀerence is best illustrated by example.

Example 18 Suppose we consider a molecule containing the atoms and bonds
shown on the left below, and we want to include the 6-ary relation of a benzene
ring (the corresponding hyper-edge is shown dotted on the right below).

In VEGNNs (Dash et al., 2021c), graphs are represented as tuples of the
form (V, E, σ, ψ, φ), where V is the set of vertices (here atoms in the molecule);
E denotes the edges (bonds in the molecule); σ is a neighbourhood function;
ψ denotes an initial vertex-labelling; and φ denotes an initial edge-labelling.
For each v ∈ V , let ψ(v) be a real-valued vector of ﬁnite dimension. In (Dash
et al., 2021c), any n-ary relation in domain-knowledge is treated as a hyper-
edge, where a hyperedge is a set of n vertices in the graph. For any vertex
v in a graph, let h(v) denote the set of predicate symbols such that the cor-
responding hyper-edge contains v. Let g/1 be a function that maps sets of
predicate-symbols to a ﬁxed-length Boolean-valued vector (a “multi-hot” en-
coding). Thus, in (Dash et al., 2021c), a VEGNN is a GNN that operates
on graphs obtained from labelled graphs of the form (V, E, σ, ψV , φ), where
ψV (v) = ψ(v) ⊕ g(h(v)) (here ⊕ denotes a concatenation operation). In a
VEGNN h(v) is {Benzene/6} for v = v1, . . . , v10 in the graph below (repre-
senting the compound naphthalene):

Thus, the information that v3, v4 are members of 2 diﬀerent benzene rings
is not captured in the VEGNNs vertex-labelleing, and we have to rely on the
GNN machinery to re-derive this information from the graph structure (if this
information is needed). In a BotGNN on the other hand, the two benzene
rings are separate vertices in the bipartite graph, which share edges to vertices
representing v3 and v4. The broad structure of the VEGNN (only vertex-labels

26

Tirtharaj Dash et al.

are shown for clarity) and the BotGNN graphs for naphthalene are shown below
in (a) and (b) respectively:

ψV /1 in (a) refers to the vertex-encoding function in (Dash et al., 2021c), and
ψ(cid:48) in (b) refers to the function deﬁned in Defn. 13. For the experimental data
in this paper, the vertex-encoding in (Dash et al., 2021c) results in vectors
whose dimensions are about 10 times more than the ψV /1 from Defn. 13.

The approach to n-ary relations employed by VEGNNs is thus somewhat
akin to a clique-expansion of the graph containing vertices for terms. In a
clique-expansion, all vertices in a hyper-edge–elements of some n-ary relation–
are connected together by a labelled hyper-edge. This can introduce a lot of
new edges, and some mechanism is needed to distinguish between multiple
occurrences of the same relation (an example is the multiple occurrences of
benzene rings above). VEGNNs can be seen as achieving the eﬀect of such
a clique-expansion, without explicitly adding the new edges, but they do not
address the problem of multiple occurrences. BotGNNs can be seen instead
as a star-expansion of the graph containing vertices for terms. In such a star-
expansion, new nodes denoting the relation are introduced, along with edges
between the relation-vertex and the term-vertices that are part of the relation
(that is, the hyper-edge). Star-expansions of graphs thus contain 2 kinds of
vertices, which is similar to the graph constructed by a BotGNN.

5 Empirical Evaluation

5.1 Aims

Our aim in this section is to investigate the following claims:

1. GNNs based on bottom-graphs constructed with domain knowledge (BotGN N s)
have a higher predictive performance than GNNs that do not use domain
knowledge;

2. BotGN N s have a higher predictive performance than vertex-enriched GNNs
(V EGN N s) that employ a simpliﬁcation to provide domain knowledge to
GNNs (Dash et al., 2021c).

Inclusion of Domain-Knowledge into GNNs using MDIE

27

5.2 Materials

5.2.1 Data

For the empirical evaluation of our proposed BotGNNs, we use 73 bench-
mark datasets arising in the ﬁeld of drug-discovery. Each dataset represents
extensive drug evaluation eﬀort at the NCI14 to experimentally determine the
eﬀectiveness of anti-cancer activity of a compound against a number of cell
lines (Marx et al., 2003). The datasets correspond to the concentration param-
eter GI50, which is the concentration that results in 50% growth inhibition.
Each dataset consists of a set of chemical compounds, which are then converted
into bottom-graphs.

Each bottom-graph can be represented using (G, ·), where G = (X, Y, E),
where X represents the vertices corresponding to the relations, Y represents
the vertices corresponding to ground terms in the bottom-clause constructed
by MDIE, and E represents the edges between X and Y . Table 1 summarises
the datasets.

# of
datasets

Avg # of
instances

avg. of
|X|

avg. of
|Y |

avg. of
|E|

73

3032

81

42

937

Table 1: Dataset summary (The last 3 columns are the average number of X,
Y and E in each bottom-graph in a dataset)

5.2.2 Background Knowledge

The initial version of the background knowledge was used in (Van Crae-
nenbroeck et al., 2002; Ando et al., 2006). This BK is a collection of logic
programs (written in Prolog) deﬁning almost 100 relations for various func-
tional groups (such as amide, amine, ether, etc.) and various ring structures
(such as aromatic, non-aromatic etc.). A functional group is represented by
functional group/4 predicate and a ring is represented by ring/4. There are
also higher-level relations deﬁned on the top of the above two relations. These
are: the presence of fused rings, connected rings and substructures.

has struc(CompoundId, Atoms, Length, Struc) This relation is T RU E if a com-
pound identiﬁed by CompoundId contains a structure Struc of length Length
containing a set of atoms in Atoms.

fused(CompoundId, Struc1, Struc2) This relation is T RU E if a compound
identiﬁed by CompoundId contains a pair of fused structures Struc1 and
Struc2 (that is, there is at least 1 pair of common atoms).

14 The National Cancer Institute (https://www.cancer.gov/)

28

Tirtharaj Dash et al.

connected(CompoundId, Struc1, Struc2) This relation is T RU E if a com-
pound identiﬁed by CompoundId contains a pair structures Struc1 and
Struc2 that are not fused but connected by a bond between an atom in
Struc1 and an atom in Struc2.

5.2.3 Algorithms and Machines

The datasets and the BK are written in Prolog. We use Inductive Logic Pro-
gramming (ILP) engine, Aleph (Srinivasan, 2001) to construct the bottom-
clause using MDIE. A Prolog program then extracts the relations and ground
terms from the bottom-clause. We use YAP compiler for execution of all our
Prolog programs. These are parsed by UNIX and MATLAB scripts to con-
struct bottom-graph datasets in the format prescribed in (Kersting et al.,
2016), which are mainly representations of adjacency matrix, vertex labels
(feature vector), class labels, etc.

The GNN variants used here are described in the next section. All the
experiments are conducted in a Python environment. The GNN models have
been implemented by using the PyTorch Geometric library (Fey and Lenssen,
2019)–a popular geometric deep learning extension for PyTorch (Paszke et al.,
2019) enabling easier implementations of various graph convolution and pool-
ing methods.

For all the experiments, we use a machine with Ubuntu (16.04 LTS) op-
erating system, and hardware conﬁguration such as: 64GB of main memory,
16-core Intel Xeon processor, a NVIDIA P4000 graphics processor with 8GB
of video memory.

5.3 Method

Let D be a set of data-instances represented as graphs {(g1, y1), . . . , (gN , yN )},
where yi is a class label associated with the graph gi. We also assume that we
have access to background-knowledge B, a set of modes M, a depth-limit d. Our
method for investigating the performance of BotGN N s uses is straightforward:

(1) Randomly split D into DT r and DT e;
(2) Let BotGN N be the model from Procedure 1 (TrainBotGNN) with back-
round knowledge B, modes M, depth-limit d, training data DT r and some
GNN implementation (see below);

(3) Let GN N be the model from the GNN implementation without back-

ground knowledge, and with DT r;

(4) Let V EGN N be the model using the GNN implementation with vertex-

enrichment using the background knowledge B, and with DT r;

(5) Let D(cid:48)
(6) Obtain the predictions for D(cid:48)

T e = {gi : (gi, yi) ∈ DT e}

GNN) with background knowledge B, modes M, and depth-limit d;

(7) Obtain the predictions for D(cid:48)
(8) Compare the performance of BotGN N , GN N and V EGN N .

T e using GN N and V EGN N ; and

T e of BotGN N using Procedure 2 (TestBot-

Inclusion of Domain-Knowledge into GNNs using MDIE

29

The following additional details are relevant. We closely follow the method
used in (Dash et al., 2021c) for the construction of GNNs. The general work-
ﬂow involved in GNNs was described in Sec. 2. A diagram of the components
involved in implementing that workﬂow is shown in Fig. 4.

Fig. 4: Components involved in implementing the workﬂow in Sec. 2 for Bot-
GNN models. ‘Conv’ and ‘Pool’ refer to the graph-convolution and graph-
pooling operations, respectively. The ‘Readout’ operation constructs a graph-
representation by accumulating information from all the vertex in the graph
obtained after the pooling operation. The ﬁnal graph-representation is ob-
tained in the READOUT block by an element-wise sum (shown as ⊕) of the
individual graph-representations obtained after each AGGREGATE-COMBINE
block. MLP stands for Multilayer Perceptron.

– We have used a 70:30 train-test split for each of the datasets. 10% of the

train-set is used as a validation set for hyperparameter tuning.

– Each GNN consists of three graph convolution blocks and three graph
pooling blocks. The convolution and pooling blocks interleave each other
(that is, C-P-C-P-C-P) as shown in Fig. 4.

– The convolution blocks can be of one of the following ﬁve variants: GCN (Kipf
and Welling, 2017), k-GNN (Morris et al., 2019), GAT (Veli˘ckovi´c et al.,
2018), GraphSAGE (Hamilton et al., 2017), and ARMA (Bianchi et al.,
2021). The mathematical details on these graph convolution operations
are provided in Appendix B.

– The graph pooling block uses self-attention pooling (Lee et al., 2019) with
a pooling ratio of 0.5. We use the graph-convolution formula proposed in
(Kipf and Welling, 2017) for calculating the self-attention scores.

– Due to the large number of experiments (resulting from multiple datasets
and multiple GNN variants), the hyperparameters in the convolution blocks
are set to the default values within the PyTorch Geometric library.

– We use a hierarchical pooling architecture that uses the readout mechanism
proposed by Cangea et al. (Cangea et al., 2018). The readout block aggre-
gates node features to produce a ﬁxed size intermediate representation for

30

Tirtharaj Dash et al.

the graph. The ﬁnal ﬁxed-size representation for the graph is obtained by
element-wise addition of the three readout representations.

– The representation length (2m) is determined by using a validation-based
approach. The parameter grid for m is: {8, 128}, representing a small and
a large embedding, respectively.

– The ﬁnal representation is then fed as input to a 3-layered MLP. We use a
dropout layer with a ﬁxed dropout rate of 0.5 after the ﬁrst layer of MLP.
– The input layer of the MLP contains 2m units, followed by two hidden
layers with m units and (cid:98)m/2(cid:99) units, respectively. The activation func-
tion used in the hidden layers is relu. The output layer uses logsoftmax
activation.

– The loss function used is the negative log-likelihood between the target

class-labels and the predictions from the model.

– We denote the BotGN N variants as: BotGN N1,...,5 based on the type of

graph convolution method used.

– We use the Adam (Kingma and Ba, 2015) optimiser for training the Bot-
GNNs (BotGN N1,...,5). The learning rate is 0.0005, weight decay param-
eter is 0.0001, the momentum factors are set to the default values of
β1,2 = (0.9, 0.999).

– The maximum number of training epochs is 1000. The batch size is 128.
– We use an early-stopping mechanism (Prechelt, 1998) to avoid overﬁtting
during training. The resulting model is then saved and can be used for eval-
uation on the independent test-set. The patience period for early-stopping
is ﬁxed at 50.

– The predictive performance of a BotGNN model refers to its predictive

accuracy on the independent test-set.

– Comparison of the predictive performance of BotGNNs against GNNs and
VEGNNs is conducted using the Wilcoxon signed-rank test, using the stan-
dard implementation within MATLAB (R2018b).

5.4 Results

The quantitative comparisons of predictive performance of BotGN N s against
baseline GN N s and V EGN N s are presented in Fig. 6. The tabulation shows
number of datasets on which BotGN N has higher, lower or equal predic-
tive accuracy. The principal conclusions from these tabulations are these: (1)
BotGN N s perform signiﬁcantly better than their corresponding counterparts
that do not have access to any information other than the atom-and-bond
structure of a molecule achieving a gain in predictive accuracy of 5-8% across
variants as shown in the qualitative comparison shown in Fig. 5. This is ir-
respective of the variant of GNN used, suggesting that the technique is able
to usefully integrate domain knowledge; (2) BotGN N s perform signiﬁcantly
better than V EGN N s with access to the same background knowledge. This
suggests that BotGN N s do more than the vertex-enrichment approach used
by V EGN N s.

Inclusion of Domain-Knowledge into GNNs using MDIE

31

(a) GN N 1 (median gain ≈ 6%)

(b) GN N 2 (median gain ≈ 5%)

(c) GN N 3 (median gain ≈ 8%)

(d) GN N 4 (median gain ≈ 7%)

(e) GN N 5 (median gain ≈ 6%)

Fig. 5: Qualitative comparison of predictive performance of BotGNNs against
Baseline (that is, GNN variants without access to domain-relations). Per-
formance refers to estimates of predictive accuracy (obtained on a holdout
set), and all performances are normalised against that of baseline performance
(taken as 1). No signiﬁcance should be attached to the line joining the data
points: this is only for visual clarity.

In a previous section (Sec. 4.4) we have described diﬀerences between Bot-
GNNs and VEGNNs arising from an encoding of the data into a bipartite
graph representation. Possible reasons for this diﬀerence in performance are
twofold: (1) The GNN variants are unable to use edge-label information. In the
VEGNN-style graphs for the data, this information corresponds to the type
of bonds. However, this information is contained in vertices associated with
the bond-literals in BotGNN-style graphs, which can be used by the GNN-
variants; and (2) The potential loss in relational information in VEGNN-style

32

Tirtharaj Dash et al.

GNN
Variant

Accuracy (BotGN N )
Higher/Lower/Equal (p-value)
V EGN N

GN N

1
2
3
4
5

59/5/9 (< 0.001)
59/8/6 (< 0.001)
61/2/10 (< 0.001)
63/1/9 (< 0.001)
60/4/9 (< 0.001)

54/11/8 (< 0.001)
61/9/3 (< 0.001)
54/10/9 (< 0.001)
55/11/7 (< 0.001)
52/9/12 (< 0.001)

Fig. 6: Comparison of predictive performance of BotGN N s. The tabulations
are the number of datasets on which BotGN N has higher, lower or equal
predictive accuracy (obtained on a holdout set) than GN N and V EGN N .
Statistical signiﬁcance is computed by the Wilcoxon signed-rank test.

graphs as described in Sec. 4.4. A further diﬀerence, not apparent from tabu-
lations of performance is the diﬀerences in the feature-vectors associated with
each vertex. For the data here, vertex-labels for VEGNNs described in (Dash
et al., 2021c) results in each vertex being associated with a 1400-dimensional
vector. For BotGNNs, this is about 130.

The signiﬁcant improvement in predictive performance with the inclusion
of symbolic domain-knowledge into GNNs is consistent with similar obser-
vations we obtain with multilayer perceptrons (MLPs). For the latter, one
well-established way of inclusion of domain-knowledge is through the use of
the technique of propositionalisation (Lavraˇc et al., 1991). This represents rela-
tional data, like graphs, in the form of some numeric vector (usually, Boolean).
For instance, a simple and eﬀective method known as “bottom-clause proposi-
tionalisation” or BCP (Fran¸ca et al., 2014) is a propositionalisation approach
serving as an extension to one of the pioneering works on neural-symbolic
learning systems proposed in (Garcez and Zaverucha, 1999). For a set of (re-
lational) data-instances, BCP obtains a set of (unique) literals from the most-
speciﬁc (or bottom) clauses constructed by an ILP engine. It then proposi-
tionalises each bottom-clause based on the literal set. This process results in
each relational data-instance being represented as a Boolean feature-vector.
These feature-vectors are then input to an MLP for further processing, al-
lowing the symbolic domain-information available in the bottom-clauses to be
easily incorporated into the MLP. Some other related studies have shown that
the most-speciﬁc clauses can also be treated as a source of relational features,
described by conjunctions of literals. These relational features can be used to
construct a Boolean-vector representation of the (relational) data-instances.
These features can form the input feature-vectors for standard statistical mod-
els (as in (Saha et al., 2012)) or for multilayer perceptrons (MLPs). When
used with MLPs, the resulting model is called as “deep relational machines”
or DRMs, as introduced by (Lodhi, 2013) and studied extensively in (Dash
et al., 2018, 2019). In Fig. 7 we also observe gains in performance of MLPs

Inclusion of Domain-Knowledge into GNNs using MDIE

33

by incorporating domain-knowledge through the use of some form of proposi-
tionalisation.

(a) DRM

(b) BCP+MLP

Fig. 7: Improvements in predictive performance of MLPs, when provided with
domain-knowledge through propositionalisation. Baselines (“1”) are the mod-
els without domain-knowledge. The DRM here is an MLP that uses simple
Boolean propositions indicating the presence or absence of relations provided
as domain-knowledge. The structure and parameters of the MLP are obtained
using the Adam optimiser (Kingma and Ba, 2015). BCP+MLP use Boolean
propositions constructed using the bottom-clause propositionalisation method
in (Fran¸ca et al., 2014). “MLP” refers to the use of these features by a multi-
layer perceptron. The structure and parameters for the MLP are obtained
using the same approach as the DRM. The domain-knowledge is the same as
that used for the construction of BotGNNs.

5.5 Some Additional Results

We turn now to two questions that are not within the scope of the experimen-
tal goals listed in Sec. 5.1, but are nevertheless of practical interest (details
relevant to the results are in Appendix C). First, the question of whether in-
corporating domain-knowledge using the route of bottom-graphs and GNNs
is better than propositions and MLPs? A straightforward comparison of the
BotGN N models against those used in Fig. 7 would seem to suggest that the
answer is “yes” (Fig. 8). However, we caution against drawing such a conclu-
sion for at least the following reasons: (a) Inclusion of propositions based on
stochastic sampling of complex relational features in (Dash et al., 2019) can
result in signiﬁcantly better DRM models. It is possible also that BCP could
be augmented with the same sampling methods to yield more informative
propositions; (b) It is also possible that a BotGN N obtained with access to
sampled relational features could improve its performance over what is shown
here. We note that propositionalisation is not conﬁned to the use of MLPs:

34

Tirtharaj Dash et al.

so it would be not be surprising if the BotGNN performance was bettered by
some ML method using the data provided to the DRM or BCP+MLP.

A more useful diﬀerence between the BotGNN approach and proposition-
alisation is that techniques relying on the latter usually separate the feature-
and model-construction steps. A BotGN N , like any GNN, constructs a vector-
space embedding for the graphs it is provided. However, this embedding is
obtained as part of an end-to-end model construction process. This can be
substantially more compact than the representation used by methods that
employ a separate propositionalisation step (see Fig. 9).

GNN
Variant

Accuracy (BotGN N )
Higher/Lower/Equal (p-value)

DRM

BCP+MLP

1
2
3
4
5

47/22/4 (< 0.001)
41/29/3 (0.065)
45/19/9 (< 0.001)
50/19/4 (< 0.001)
51/16/6 (< 0.001)

58/10/5 (< 0.001)
58/11/4 (< 0.001)
61/6/6 (< 0.001)
62/6/5 (< 0.001)
60/6/7 (< 0.001)

Fig. 8: Comparison of predictive performance of BotGNN with DRM and
BCP+MLP. The tabulations are the number of datasets on which BotGN N
has higher, lower or equal predictive accuracy (obtained on a holdout set) than
DRM and BCP+MLP. DRM and BCP+MLP refer to the models in Fig. 7

Method

Vector Representation Vector Dimension (Range)

BotGNN
DRM
BCP+MLP

Real, dense
Boolean, sparse
Boolean, very sparse

16–256
1400–1400
18000–52000

Fig. 9: Characterisation of vector-representation used for model-construction
by BotGNNs, DRMs and BCP+MLP. Minimum/maximum values of the range
are only shown to 3 meaningful digits (the actual values are not relevant here).
The graph-representations (also, called graph-embeddings) for BotGNNs are
constructed internally by the GNN. By “sparse” we mean that there are many
0-values, and by “very sparse”, we mean the values are mostly 0.

The second question of practical interest is how a BotGNN’s performance
compares to an ILP learner that uses bottom-clauses directly. Fig. 10(a) shows
a routine comparison against the Aleph system (Srinivasan, 2001) conﬁgured
to perform greedy set-covering for identifying rules using bottom-clauses (in ef-
fect, a Progol-like approach: see (Muggleton, 1995)). Again, we caution against

Inclusion of Domain-Knowledge into GNNs using MDIE

35

drawing the obvious conclusion, since the results are obtained without at-
tempting to optimise any parameters of the ILP learner (only the minimum
accuracy of clauses was changed from the default setting of 1.0 to 0.7: this
latter value has been shown to be more appropriate in many previous ex-
perimental studies with Aleph). A better indication is in Fig. 10(b), which
compares BotGNN performance on older benchmarks for which ILP results
after parameter optimisation are available. These suggest that BotGNN per-
formance to be comparable to an ILP approach with optimised parameter
settings.15

GNN

Accuracy (BotGN N )

Variant Higher/Lower/Equal (p-value)

1
2
3
4
5

62/7/4 (< 0.001)
60/9/4 (< 0.001)
61/7/5 (< 0.001)
62/6/5 (< 0.001)
62/4/7 (< 0.001)

(a)

Dataset

ILP

BotGNN

DssTox
Mutag
Canc
Amine
Choline
Scop
Toxic

0.73
0.88
0.58
0.80
0.77
0.67
0.87

(b)

0.76
0.89
0.64
0.84
0.72
0.65
0.85

Fig. 10: Comparison of predictive performance of BotGNNs with an ILP
learner (Aleph system): (a) Without hyperparameter tuning in Aleph; (b)
With hyperparameter tuning. In (a), the tabulations are the number of
datasets on which BotGN N has higher, lower or equal predictive accuracy
(obtained on a holdout set) than the ILP learner. In (b), each entry is the
average of the accuracy obtained across 10-fold validation splits (as in (Srini-
vasan et al., 2003))

6 Related Work

One of the oldest approaches that incorporate domain knowledge into feature-
based machine learning is LINUS (Lavraˇc et al., 1991), which proposed the
idea of an attribute-value learning system based on a method called ‘proposi-
tionalisation’. This is a simple and eﬀective approach to construct a numeric
feature-vector representation for domain-relations (Lavraˇc et al., 2021), which
can then be used as input features for deep neural networks. One such ex-
ample is Deep Relational Machines (DRMs: (Lodhi, 2013)). Some large-scale

15 We note that parameter screening and optimisation is not routinely done in ILP. In
(Srinivasan and Ramakrishnan, 2011) it is noted: “Reports in the [ILP] literature rarely
contain any discussion of sensitive parameters of the system or their values. Of 100 ex-
perimental studies reported in papers presented between 1998 and 2008 to the principal
conference in the area, none attempt any form of screening for relevant parameters.”

36

Tirtharaj Dash et al.

studies show that propositionalisation of relational (ﬁrst-order) features re-
sults in an eﬀective way of incorporating symbolic domain-knowledge into
deep networks (Dash et al., 2018, 2019). The pioneering work on connectionist
inductive learning and logic programming (CIL2P) by Garcez and Zaverucha
(1999) is extended in an interesting idea of propositionalisation called ‘Bot-
tom Clause Proposionalisation (BCP)’ (Fran¸ca et al., 2014). BCP converts
a bottom-clauses in ILP into Boolean vectors, which can then be used to
learn a neural network more eﬃciently and faster than its predecessor. Al-
though propositionalisation approaches are a simple and straight-forward way
of constructing a feature-vector that encodes both relational data and domain-
knowledge, the present forefront of deep networks dealing directly with rela-
tional data (graphs) are GNNs, where we can represent relational knowledge
in neural networks directly without a propositionalisation step.

Other approaches of incorporating domain-knowledge include a modiﬁca-
tion to the loss function that is optimised during training a deep network (Xu
et al., 2018; Fischer et al., 2019). In these approaches, the primary structure
of the underlying deep network stays roughly unaltered. One such example
is: Domain-adapted neural network (DANN) that introduces an (additional)
domain-based loss term to the neural network loss function. In particular, the
domain-speciﬁc rules are approximated using two kinds of constraints: approx-
imation constraint and monotonicity constraint. It is claimed that incorporat-
ing domain-knowledge in this manner enforces the network not only to learn
from the available training data but also domain-speciﬁc rules (Muralidhar
et al., 2018).

Under the category of statistical relational learning (SRL), there are some
interesting proposals to learn from relational data (often, graph-structured),
and symbolic domain-knowledge. For instance, the work on kLog by Fras-
coni et al. (2014) introduced a method called ‘graphicalisation’ to convert
relational structures (ﬁrst-order logic interpretations) to graphs. By this con-
version, it enables the use of graph-kernel methods, which measures the simi-
larity between two graphs16. kLog generates propositional features (based on
the graph kernel) for use in SRL. kLog and BotGNN share some common
characteristics: (1) at the ﬁrst level of modelling, they both construct bipar-
tite graphs representing the relational instances: kLog constructs undirected
bipartite graphs that are related to ‘Probabilistic Entity-Relationship model’
(Heckerman et al., 2007), whereas our approach here constructs directed bi-
partite graph, and the construction is diﬀerent from the former representation
to a great extent, relying heavily on a vertex-labelling based on the concept
of a mode-language used within ILP; (2) at the second level of learning, kLog
employs graph kernels to construct propositional features and uses those to
learn a statistical learner, whereas BotGNN leverages GNNs, which are su-
perior to graph kernels in practice (Du et al., 2019). Though there are some
similarities and diﬀerences between kLog and BotGNN, one should note that

16 Graph Kernel: A kernel function compares substructures of graphs that are computable
in polynomial time (Vishwanathan et al., 2010).

Inclusion of Domain-Knowledge into GNNs using MDIE

37

the primary aim of the former is a proposal for a language for statistical re-
lational learning, whereas our primary aim is to propose a principled way to
incorporate symbolic domain-knowledge into GNNs.

Falling under the same umbrella of SRL is work on the integration of re-
lational learning with GNNs (ˇSourek et al., 2021), which demonstrates how
simple relational logic programs can capture advanced graph convolution oper-
ations in a tightly integrated manner, requiring the use of a language of Lifted
Relational Neural Networks (LRNNs) (Sourek et al., 2018). The integration
of logic programs and GNNs in this manner results in an interesting GNN-
based neuro-symbolic model (Lamb et al., 2020). The input representation for
LRNN is a weighted logic program or template, which is mainly diﬀerent from
the input representation used for BotGNNs.

Knowledge-graphs (KGs) are a rich source of domain-knowledge and are a
representation of binary relations. In a KG, each vertex represents an entity.
The relation between two vertices is represented as an edge between them. In
the last few years, several methods have attempted to incorporate the infor-
mation (relations) encoded in KGs into deep neural networks. For instance,
Schlichtkrull et al. (2018) introduced Relational Graph Convolutional Net-
works (R-GCNs) that models a information exchange among diﬀerent entities
via a relation with the help of the message-passing in a GCN. The approach
was intended for entity classiﬁcation and link prediction (discovering the miss-
ing relation between two entities). R-GCNs are related to our BotGNNs in one
sense that they are able to model multi-relational information. However, R-
GCNs are restricted to KGs to deal only with binary relations, albeit they
could be extended to go beyond binary relations using hypergraph neural net-
works (Feng et al., 2019; Bai et al., 2021). A method proposed for incorporating
knowledge-graphs into deep networks is termed as “knowledge-infused learn-
ing” (Kursuncu et al., 2020; Sheth et al., 2019). The work examines techniques
for incorporating relations at various layers of deep networks (the authors cat-
egorise these as: shallow, semi-deep and deep infusion). A GNN can directly
operate on knowledge-graphs for constructing node and graph representations
that are useful for further learning (Wang et al., 2019). We note that Bot-
GNNs could be seen as performing a generalised form of knowledge-infused
learning, in which: (a) Data can contain n-ary relations; and (b) Domain-
knowledge can encode arbitrary relations of possible relevance to the data.
In the original work on knowledge-infusion, data are knowledge-graphs that
only employ binary relations, and there is no possibility of including additional
domain-knowledge (that is, B = ∅).

A method (Xie et al., 2019) proposed that the symbolic knowledge can
be represented as formulas in Conjunctive Normal Form (CNF) and decision-
Deterministic Decomposable Negation Normal Form (d-DNNF). These for-
mulas can naturally be viewed as graph structures. Learning on these graph
structures is then carried out using a GNN. A recently proposed method uses
the idea of treating symbolic domain-relations as hyperedges (Dash et al.,
2021c). These hyperedges can then be used to construct the labelling for the
nodes of a graph using a method called ‘vertex-enrichment’, which is a sim-

38

Tirtharaj Dash et al.

pliﬁed approach to incorporate symbolic domain-knowledge into GNNs. This
form of GNNs are called Vertex-Enriched GNNs (VEGNNs). A detailed note
on the diﬀerences between VEGNNs and BotGNNs are already provided in
an earlier section. Brieﬂy, there are three main diﬀerences: (1) VEGNNs re-
quire data to be represented as graphs; whereas, BotGNNs can deal with any
data that can be represented as deﬁnite clauses; (2) VEGNNs introduce sym-
bolic domain-relations by modifying only the vertex-labelling of the graphs
while maintaining the original graph structure of the data; whereas, BotGNNs
combine data and background knowledge (using MDIE) to construct Bot-
GNN graph representations, which are bipartite graphs; and (3) VEGNNs
do not allow some of the crucial information about a vertex to be automat-
ically conveyed via the vertex-labellings, for example, a vertex is a member
of two diﬀerent benzene rings; whereas, in BotGNNs this information is read-
ily available from the bipartite graph-structure. A recent systematic review
on various methods of incorporating domain-knowledge into deep neural net-
works categorises domain-knowledge into two classes of constraints: logical and
numerical (Dash et al., 2021b). Our present work falls under the former cate-
gory and aims to constrain the structure of the graph (here, bipartite graph).
An extended version of this survey can be found in (Dash et al., 2021a) which
categories the methods of incorporating domain-knowledge into deep networks
based on whether (a) the input-representation is changed, (b) the loss-function
is changed, or (c) the structure of the deep network is changed. Our proposed
BotGNN approach falls under the ﬁrst category where each (relational) data-
instance is changed to a bipartite graphs representation.

7 Conclusions

The Domain-Knowledge Grand Challenge in (Stevens et al., 2020) calls for
systematically incorporating diverse forms of domain knowledge. In this pa-
per, we have proposed a systematic way of incorporating domain-knowledge
encoded in a powerful subset of ﬁrst-order logic. The technique explicitly ad-
dresses the requirement in (Stevens et al., 2020) of “extending” the raw data
for a class of neural networks that deal with graphs. The signiﬁcant improve-
ments in performance that we have observed support the statement on the
importance of the role of domain knowledge. We have also provided additional
results that suggest that the technique may be doing more than a simple
“propositionalisation”.

On the face of it, it would seem that logic programs are necessary to con-
struct the BotGNNs proposed here. We distinguish here between the principle
we have proposed and its implementation using logic programs. The construc-
tion of a most-speciﬁc clause, as is done by MDIE, is necessary for the con-
struction of a BotGNN. However, the construction of this clause need not be
done by using logic programming technology: it has been shown, for example,
in (Bravo et al., 2005) how this same formula can be obtained entirely us-
ing operations on relational databases. In many ways, such an implementation

Inclusion of Domain-Knowledge into GNNs using MDIE

39

would be more convenient, since the information in modes can be incorporated
(and even augmented) by the schema of a relational database. Looking further
ahead, it is possible that domain-knowledge could even be communicated as
unstructured statements in a natural language. However, for problems of sci-
entiﬁc discovery it would appear to be more useful if domain-knowledge was
represented in some structured, mathematical form.

We do not view BotGNNs as an alternative to ILP. Instead, the purpose in
this paper is to show that techniques developed in ILP can be used to incor-
porate symbolic domain knowledge into deep neural networks, with signiﬁcant
improvement in predictive performance. In fact, relational deﬁnitions found
by an ILP engine may be able to improve a BotGNN’s performance further,
possibly by augmenting bottom-graphs. This can be done by either simple in-
clusion of the new relations as background knowledge, or through an extension
of clause-graphs from bipartite to more general k-partite graphs.

The linking of symbolic and neural techniques allows the possibility of
providing logical explanations for predictions made by the neural model. This
potential has long been recognised, and demonstrated (see for example: (Besold
et al., 2017; Garcez et al., 2019)). Here, we expect the relationships shown in
Remark 4 (see Appendix A) may open the interesting possibility of linking
clausal explanations to GNNs. The improvement in predictive performance of
GNNs by the incorporation of domain knowledge is a necessary part of their
use as tools for scientiﬁc discovery. But that is not suﬃcient: explanations in
terms of relevant domain-concepts will also be needed. We intend to look at
this in future.

Data and Code Availability

Data and codes used in our experiments are available at: https://github.
com/tirtharajdash/BotGNN.

Acknowledgements A.S. is a Visiting Professorial Fellow at School of CSE, UNSW Syd-
ney. He is also the Class of 1981 Chair Professor at BITS Pilani. We thank Gustav ˇSourek,
Czech Technical University, Prague for providing the dataset information; and researchers
at the DTAI, University of Leuven, for suggestions on how to use the background knowledge
within DMAX. We also thank Oghenejokpeme I. Orhobor and Ross D. King for providing
us with the initial set of background-knowledge deﬁnitions. We thank Artur d’Avila Garcez,
City, University of London for providing information on the usage of BCP within their
system CILP++.

References

Ando HY, Dehaspe L, Luyten W, Van Craenenbroeck E, Vandecasteele H,
Van Meervelt L (2006) Discovering h-bonding rules in crystals with inductive
logic programming. Molecular pharmaceutics 3(6):665–674

Bai S, Zhang F, Torr PH (2021) Hypergraph convolution and hypergraph

attention. Pattern Recognition 110:107637

40

Tirtharaj Dash et al.

Besold TR, Garcez Ad, Bader S, Bowman H, Domingos P, Hitzler P,
K¨uhnberger KU, Lamb LC, Lowd D, Lima PMV, et al. (2017) Neural-
symbolic learning and reasoning: A survey and interpretation. ArXiv
abs/1711.03902

Bianchi FM, Grattarola D, Livi L, Alippi C (2021) Graph neural networks
with convolutional arma ﬁlters. IEEE Transactions on Pattern Analysis and
Machine Intelligence pp 1–1, DOI 10.1109/TPAMI.2021.3054830

Bravo HC, Page D, Ramakrishnan R, Shavlik J, Costa VS (2005) A frame-
work for set-oriented computation in inductive logic programming and its
application in generalizing inverse entailment. In: International Conference
on Inductive Logic Programming, Springer, pp 69–86

Cangea C, Veliˇckovi´c P, Jovanovi´c N, Kipf T, Li`o P (2018) Towards sparse

hierarchical graph classiﬁers. ArXiv abs/1811.01287

Chollet F, et al. (2015) Keras. https://keras.io
Dash T, Srinivasan A, Vig L, Orhobor OI, King RD (2018) Large-scale assess-
ment of deep relational machines. In: International Conference on Inductive
Logic Programming, Springer, pp 22–37

Dash T, Srinivasan A, Joshi RS, Baskar A (2019) Discrete stochastic search
and its application to feature-selection for deep relational machines. In: In-
ternational Conference on Artiﬁcial Neural Networks, Springer, pp 29–45
Dash T, Chitlangia S, Ahuja A, Srinivasan A (2021a) How to tell deep neural

networks whatwe know. ArXiv abs/2107.10295

Dash T, Chitlangia S, Ahuja A, Srinivasan A (2021b) Incorporating domain

knowledge into deep neural networks. ArXiv abs/2103.00180

Dash T, Srinivasan A, Vig L (2021c) Incorporating symbolic domain knowl-

edge into graph neural networks. Machine Learning pp 1–28

Du SS, Hou K, Salakhutdinov RR, Poczos B, Wang R, Xu K (2019) Graph
neural tangent kernel: Fusing graph neural networks with graph kernels.
Advances in Neural Information Processing Systems 32:5723–5733

Feng Y, You H, Zhang Z, Ji R, Gao Y (2019) Hypergraph neural networks.
In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol 33,
pp 3558–3565, DOI 10.1609/aaai.v33i01.33013558

Fey M, Lenssen JE (2019) Fast graph representation learning with PyTorch
Geometric. In: ICLR Workshop on Representation Learning on Graphs and
Manifolds

Fischer M, Balunovic M, Drachsler-Cohen D, Gehr T, Zhang C, Vechev M
(2019) Dl2: Training and querying neural networks with logic. In: Interna-
tional Conference on Machine Learning, PMLR, pp 1931–1941

Fran¸ca MV, Zaverucha G, Garcez ASd (2014) Fast relational learning using
bottom clause propositionalization with artiﬁcial neural networks. Machine
learning 94(1):81–104

Frasconi P, Costa F, De Raedt L, De Grave K (2014) klog: A language for
logical and relational learning with kernels. Artiﬁcial Intelligence 217:117–
143

Garcez Ad, Gori M, Lamb LC, Seraﬁni L, Spranger M, Tran SN (2019)
Neural-symbolic computing: An eﬀective methodology for principled in-

Inclusion of Domain-Knowledge into GNNs using MDIE

41

tegration of machine learning and reasoning. FLAP 6(4):611–632, URL
https://collegepublications.co.uk/ifcolog/?00033

Garcez ASA, Zaverucha G (1999) The connectionist inductive learning and

logic programming system. Applied Intelligence 11(1):59–77

Gilmer J, Schoenholz SS, Riley PF, Vinyals O, Dahl GE (2017) Neural mes-
sage passing for quantum chemistry. In: International conference on machine
learning, PMLR, pp 1263–1272

Gori M, Monfardini G, Scarselli F (2005) A new model for learning in graph do-
mains. In: Proceedings. 2005 IEEE International Joint Conference on Neural
Networks, 2005., IEEE, vol 2, pp 729–734

Hamilton W, Ying Z, Leskovec J (2017) Inductive representation learning on
large graphs. In: Advances in neural information processing systems, pp
1024–1034

Hamilton WL (2020) Graph representation learning. Synthesis Lectures on

Artiﬁcal Intelligence and Machine Learning 14(3):1–159

Heckerman D, Meek C, Koller D (2007) Probabilistic entity-relationship mod-
els, prms, and plate models. Introduction to statistical relational learning
pp 201–238

Jankovics V (2020) vakker/cilp. https://github.com/vakker/CILP
Johnson J, Hariharan B, van der Maaten L, Fei-Fei L, Lawrence Zitnick C,
Girshick R (2017) Clevr: A diagnostic dataset for compositional language
and elementary visual reasoning. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp 2901–2910

Kersting K, Kriege NM, Morris C, Mutzel P, Neumann M (2016) Benchmark
data sets for graph kernels. http://graphkernels.cs.tu-dortmund.de
King RD, Rowland J, Oliver SG, Young M, Aubrey W, Byrne E, Liakata M,
Markham M, Pir P, Soldatova LN, et al. (2009) The automation of science.
Science 324(5923):85–89

Kingma DP, Ba J (2015) Adam: A method for stochastic optimization. In:

ICLR (Poster), URL http://arxiv.org/abs/1412.6980

Kipf TN, Welling M (2017) Semi-supervised classiﬁcation with graph convo-
lutional networks. In: 5th International Conference on Learning Represen-
tations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings

Kitano H (2016) Artiﬁcial intelligence to win the nobel prize and beyond:

Creating the engine for scientiﬁc discovery. AI magazine 37(1):39–49

Krizhevsky A, Sutskever I, Hinton GE (2017) Imagenet classiﬁcation with deep
convolutional neural networks. Communications of the ACM 60(6):84–90
Kursuncu U, Gaur M, Sheth A (2020) Knowledge infused learning (k-
il): Towards deep incorporation of knowledge in deep learning. ArXiv
abs/1912.00512

Lamb LC, Garcez Ad, Gori M, Prates MO, Avelar PH, Vardi MY (2020)
Graph neural networks meet neural-symbolic computing: A survey and per-
spective. In: Bessiere C (ed) Proceedings of the Twenty-Ninth International
Joint Conference on Artiﬁcial Intelligence, IJCAI-20, International Joint
Conferences on Artiﬁcial Intelligence Organization, pp 4877–4884, DOI

42

Tirtharaj Dash et al.

10.24963/ijcai.2020/679, survey track

Lavraˇc N, Dˇzeroski S, Grobelnik M (1991) Learning nonrecursive deﬁnitions
of relations with linus. In: European Working Session on Learning, Springer,
pp 265–281

Lavraˇc N, Podpeˇcan V, Robnik-ˇSikonja M (2021) Propositionalization
of Relational Data, Springer International Publishing, Cham, pp 83–
105. DOI 10.1007/978-3-030-68817-2 4, URL https://doi.org/10.1007/
978-3-030-68817-2_4

Lee J, Lee I, Kang J (2019) Self-attention graph pooling. In: International

Conference on Machine Learning, pp 3734–3743

Lodhi H (2013) Deep relational machines. In: International Conference on

Neural Information Processing, Springer, pp 212–219

Marx KA, O’Neil P, Hoﬀman P, Ujwal M (2003) Data mining the nci cancer
cell line compound gi50 values: identifying quinone subtypes eﬀective against
melanoma and leukemia cell classes. Journal of chemical information and
computer sciences 43(5):1652–1667

Morris C, Ritzert M, Fey M, Hamilton WL, Lenssen JE, Rattan G, Grohe M
(2019) Weisfeiler and leman go neural: Higher-order graph neural networks.
In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol 33,
pp 4602–4609

Muggleton S (1995) Inverse entailment and progol. New generation computing

13(3-4):245–286

Muggleton S, De Raedt L (1994) Inductive logic programming: Theory and

methods. The Journal of Logic Programming 19:629–679

Muralidhar N, Islam MR, Marwah M, Karpatne A, Ramakrishnan N (2018)
Incorporating prior domain knowledge into deep neural networks. In: 2018
IEEE International Conference on Big Data (Big Data), IEEE, pp 36–45
Oord Avd, Dieleman S, Zen H, Simonyan K, Vinyals O, Graves A, Kalchbren-
ner N, Senior A, Kavukcuoglu K (2016) Wavenet: A generative model for
raw audio. ArXiv abs/1609.03499

Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen T,
Lin Z, Gimelshein N, Antiga L, et al. (2019) Pytorch: An imperative style,
high-performance deep learning library. In: Advances in Neural Information
Processing Systems, pp 8024–8035

Plotkin G (1972) Automatic methods of inductive inference. Ph.d. dissertation,

The University of Edinburgh

Plotkin GD (1970) A note on inductive generalization. Machine intelligence

5(1):153–163

Prechelt L (1998) Early stopping-but when? In: Neural Networks: Tricks of

the trade, Springer, pp 55–69

Saha A, Srinivasan A, Ramakrishnan G (2012) What kinds of relational fea-
tures are useful for statistical learning? In: International Conference on In-
ductive Logic Programming, Springer, pp 209–224

Schlichtkrull M, Kipf TN, Bloem P, Van Den Berg R, Titov I, Welling M (2018)
Modeling relational data with graph convolutional networks. In: European
semantic web conference, Springer, pp 593–607

Inclusion of Domain-Knowledge into GNNs using MDIE

43

Sheth A, Gaur M, Kursuncu U, Wickramarachchi R (2019) Shades of
knowledge-infused learning for enhancing deep learning. IEEE Internet
Computing 23(6):54–63

Sourek G, Aschenbrenner V, Zelezny F, Schockaert S, Kuzelka O (2018) Lifted
relational neural networks: Eﬃcient learning of latent relational structures.
Journal of Artiﬁcial Intelligence Research 62:69–100

ˇSourek G, ˇZelezn`y F, Kuˇzelka O (2021) Beyond graph neural networks with

lifted relational neural networks. Machine Learning pp 1–44

Srinivasan A (2001) The aleph manual. https://www.cs.ox.ac.uk/

activities/programinduction/Aleph/aleph.html

Srinivasan A, Ramakrishnan G (2011) Parameter screening and optimisation
for ilp using designed experiments. Journal of Machine Learning Research
12(2)

Srinivasan A, King RD, Bain ME (2003) An empirical study of the use of
relevance information in inductive logic programming. Journal of Machine
Learning Research 4(Jul):369–383

Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R (2014)
Dropout: a simple way to prevent neural networks from overﬁtting. The
journal of machine learning research 15(1):1929–1958

Stevens R, Taylor V, Nichols J, Maccabe AB, Yelick K, Brown D (2020) Ai
for science. Tech. rep., Argonne National Lab.(ANL), Argonne, IL (United
States)

Van Craenenbroeck E, Vandecasteele H, Dehaspe L (2002) Dmax’s functional
group and ring library. https://dtai.cs.kuleuven.be/software/dmax/
Veli˘ckovi´c P, Cucurull G, Casanova A, Romero A, Li`o P, Bengio Y (2018)
Graph attention networks. In: International Conference on Learning Repre-
sentations, URL https://openreview.net/forum?id=rJXMpikCZ

Vishwanathan SVN, Schraudolph NN, Kondor R, Borgwardt KM (2010)

Graph kernels. Journal of Machine Learning Research 11:1201–1242

Wang H, Zhao M, Xie X, Li W, Guo M (2019) Knowledge graph convolutional
networks for recommender systems. In: The world wide web conference, pp
3307–3313

Williams K, Bilsland E, Sparkes A, Aubrey W, Young M, Soldatova LN,
De Grave K, Ramon J, De Clare M, Sirawaraporn W, et al. (2015)
Cheaper faster drug development validated by the repositioning of drugs
against neglected tropical diseases. Journal of the Royal society Interface
12(104):20141289

Wu Y, Schuster M, Chen Z, Le QV, Norouzi M, Macherey W, Krikun M, Cao
Y, Gao Q, Macherey K, et al. (2016) Google’s neural machine translation
system: Bridging the gap between human and machine translation. ArXiv
abs/1609.08144

Wu Z, Pan S, Chen F, Long G, Zhang C, Philip SY (2020) A comprehensive
survey on graph neural networks. IEEE Transactions on Neural Networks
and Learning Systems

Xie Y, Xu Z, Kankanhalli MS, Meel KS, Soh H (2019) Embedding symbolic
knowledge into deep networks. In: Advances in Neural Information Process-

44

Tirtharaj Dash et al.

ing Systems, pp 4233–4243

Xu J, Zhang Z, Friedman T, Liang Y, Broeck G (2018) A semantic loss function
for deep learning with symbolic knowledge. In: International Conference on
Machine Learning, PMLR, pp 5502–5511

Xu K, Hu W, Leskovec J, Jegelka S (2019) How powerful are graph neural
networks? In: International Conference on Learning Representations, URL
https://openreview.net/forum?id=ryGs6iA5Km

A Some Properties of Clause-Graphs

We note the following properties about clause-graphs. For these properties, we assume back-
ground knowledge B, a set of modes M, and a depth-limit d as before. Clause-graphs are
elements of the set G and are structures of the form ((X, Y, E), ψ) where (X, Y, E) are
bipartite graphs from the set B (see Sec. 4.1). We assume G contains the element CG(cid:62) =
((∅, ∅, ∅), ∅). We also deﬁne the following equality relation over elements of G: (Xi, Yi, Ei), ψi)
= (Xj , Yj , Ej ), ψj ) iﬀ Xi = Xj , Yi = Yj , Ei = Ej and ψi = ψj . Also, given a clause C =
{l1, . . . , lm, ¬lm+1, . . . , ¬lk}, 1 ≤ m < k, ΛC is the set {l1, . . . , lm+1, . . . , lk}.

Deﬁnition 16 ((cid:22)cg) Let CG1 = (G1, ψ1), CG2 = (G2, ψ2) be elements of G, where G1 =
(X1, Y1, E1) and G2 = (X2, Y2, E2) Then CG1 (cid:22)cg CG2 iﬀ: (a) X1 ⊆ X2; (b) Y1 ⊆ Y2;
(c) E1 ⊆ E2; and (d) ψ1 ⊆ ψ2.

Proposition 1 (cid:104)G, (cid:22)cg(cid:105) is partially ordered.

Proof: In the following, let CG = ((X, Y, E), ψ), and CGi = ((Xi, Yi, Ei), ψi).

Reﬂexive. If CG ∈ G then CG (cid:22)cg CG. This follows trivially since X ⊆ X, Y ⊆ Y , E ⊆ E

and ψ ⊆ ψ.

Anti-Symmetric. Let CG1, CG2 ∈ G. If CG1 (cid:22)cg CG2 and CG2 (cid:22)cg CG1 then CG1 =
CG2. Since CG1 (cid:22)cg CG2, and CG2 (cid:22)cg CG1 X1 ⊆ X2 and X2 ⊆ X1. Therefore
X1 = X2. Similarly Y1 = Y2, E1 = E2 and ψ1 = ψ2, and therefore CG1 = CG2;

Transitive. Let CG1, CG2, CG3 ∈ G. If CG1 (cid:22)cg CG2 and CG2 (cid:22)cg CG3 then CG1 (cid:22)cg
CG3. Since CG1 (cid:22)cg CG2 and CG2 (cid:22)cg CG3 then X1 ⊆ X2 and X2 ⊆ X3. Therefore
X1 ⊆ X3. Similarly, Y1 ⊆ Y3, E1 ⊆ E3 and ψ1 ⊆ ψ3. Therefore, CG1 (cid:22)cg CG3.

(cid:50)

For CG1, CG2 ∈ G, if CG1 (cid:22)cg CG2, then we will say CG1 is more general than CG2. We
note without formal proof that if CG ∈ G then CG(cid:62) (cid:22)cg CG.

Remark 2 We note the following consequences of the Defns. 7–9, and Defn. 16 above:

(i) Let C, D ∈ LB,M,d, CG1 = ClauseT oGraph(C), and CG2 = ClauseT oGraph(D)
where: CG1 = ((X1, Y1, E1), ψ1) and CG2 = ((X2, Y2, E2), ψ2). If (X1 ⊆ X2) then
CG1 (cid:22)cg CG2. By construction, X1 ⊆ X2 iﬀ Lits(C) ⊆ Lits(D). It follows that
T erms(C) ⊆ T erms(D), and Y1 ⊆ Y2. Since E1 contains all the relevant arcs between
X1 and Y1 and E2 contains all the relevant arcs between X2 and Y2, E2 will contain
all the elements of E1. Since hx, hy are bijections, ψ1 ⊆ ψ2. That is CG1 (cid:22)cg CG2;
(ii) Let C, D ∈ LB,M,d, CG1 = ClauseT oGraph(C) = ((X1, Y1, E1), ψ1) and CG2 =
ClauseT oGraph(D) = ((X2, Y2, E2), ψ2). Let LM1 be the set of λµ-sequences for C
and LM2 be the set of λµ-sequences for D. If LM1 ⊆ LM2 then CG1 (cid:22)cg CG2. It
is evident that Lits(C) ⊆ Lits(D). Therefore X1 ⊆ X2, and from the observation (i)
above, CG1 (cid:22)cg CG2).

Lemma 1 (Lits) The function Lits : LB,M,d → 2LM (deﬁned in Defn. 7) is well-deﬁned.
That is, if C = D, then Lits(C) = Lits(D).

Inclusion of Domain-Knowledge into GNNs using MDIE

45

Proof: Assume the contrary. That is, C = D and Lits(C) (cid:54)= Lits(D). Since C = D,
ΛC = ΛD. Further, since Lits(C) (cid:54)= Lits(D), for some λi ∈ ΛC , ΛD, there must exist
µi ∈ M s.t. (λi, µi) ∈ Lits(C) and (λi, µi) (cid:54)∈ Lits(D) or vice versa. This is not possible
(cid:50)
since Lits(C) and Lits(D) contain all λµ-sequences for C, D.

Lemma 2 Let C, D ∈ LB,M,d. Let CG1 = ClauseT oGraph(C) and CG2 = ClauseT oGraph(D).
if C = D then CG1 = CG2.

Proof: The result holds trivially if C = D = ∅; and we consider C, D (cid:54)= ∅. Let CG1 =
((X1, Y1, E1), ψ1) and CG2 = ((X2, Y2, E2), ψ2). Since C = D, by Lemma 1 Lits(C) =
Lits(D). From Defn. 8, (T erms(C) = T erms(D)) iﬀ (Lits(C) = Lits(D)). From Defn.
9, (X1 = X2) iﬀ (Lits(C) = Lits(D)) and (Y1 = Y2) iﬀ (T erms(C) = T erms(D)). If
(X1 = X2) and (Y1 = Y2) then (E1 = E2). Since hx, hy are bijections, ψ1 = ψ2. That is,
(cid:50)
CG1 = CG2.

Proposition 2 (ClauseT oGraph) The function ClauseT oGraph : LB,M,d → G (deﬁned
in Defn. 9) is injective.

Proof: Let C and D in LB,M,d, and CG1 = ClauseT oGraph(C) and CG2 = ClauseT oGraph(D).
We need to show that if CG1 = CG2 then C = D.

Let CG1 = (G1, ψ1) and and CG2 = (G2, ψ2), where G1 = (X1, Y1, E1) and G2 =
(X2, Y2, E2). Since CG1 = CG2, (G1, ψ1) = (G2, ψ2). That is, X1 = X2, Y1 = Y2 and ψ1
= ψ2. Suppose C (cid:54)= D. Then, either there is some literal in C that is not in D or vice
versa. Let λi ∈ C, and λi (cid:54)∈ D. Let λi be the corresponding literal in ΛC , and λi (cid:54)∈ ΛD.
Then since C ∈ LB,M,d there must be at least one µi ∈ M s.t. (λi, µi) ∈ Lits(C). Let
x = hx((λi, µi)) ∈ X1. Since hx is a bijection, and λi (cid:54)∈ D, there will be no other λ and µ
such that hx((λ, µ)) = x. Hence x (cid:54)∈ X2. This is a contradiction, since X1 = X2. Similarly
(cid:50)
for λi ∈ D and λi (cid:54)∈ C.

Proposition 3 (Left-Inverse) ClauseT oGraph(·) has a left-inverse.

Proof: We show that there is a function GraphtoClause : G → LB,M,d s.t. for all C ∈
LB,M,d, GraphT oClause(ClauseT oGraph(C)) = C.
Let CG = ClauseT oGraph(C). So, CG = (G, ψ), where G = (X, Y, E). For each xi ∈ X:
1. Let L+ = {λi : xi ∈ X, ψ(xi) = (λi, µi), µi = modeh(·)};
2. Let L− = {¬λi : xi ∈ X, ψ(xi) = (λi, µi), µi = modeb(·)}
Let GraphT oClause(CG) = C(cid:48) where C(cid:48) = L+ ∪ L−. We claim C = C(cid:48). Assume C (cid:54)= C(cid:48).
Then there must be some literal li ∈ C s.t. li (cid:54)∈ C(cid:48) (or vice versa). Let the corresponding
literal in ΛC be λi. Since C ∈ LB,M,d, there must be some λµ-sequence (Defn. 5) for C s.t.
some (λi, µi) ∈ Lits(C) (Defn. 7) and hx((λi, µi)) ∈ X (Defn. 9). Then, by the construction
above, li ∈ C(cid:48), which is a contradiction. Suppose li ∈ C(cid:48) and li (cid:54)∈ C. Then there cannot
be any (λi, µi) s.t. hx((λi, µi) ∈ X. By construction, li (cid:54)∈ C(cid:48), which is a contradiction.
(cid:50)
Therefore there is no li ∈ C and li (cid:54)∈ C(cid:48), or vice versa, and C = C(cid:48).

Remark 3 We note the following without formal proofs:

(i) In Defn. 10, if there exists a unique ⊥B,M,d(e) ∈ LB,M,d then BotGraphB,M,d(e) is
unique. The proof follows from BotGraphB,M,d(e) = ClauseT oGraph(⊥B,M,d(e)).

(ii) In Defn. 11, Antecededent : G → G is well-deﬁned. That is, if Antecedent(CG1) (cid:54)=
Antecedent(CG2) then CG1 (cid:54)= CG2. Again the proof follows from the contrapositive
which is easily seen to hold. Also, we note that Antecedent is many-to-one, that is is
possible that CG1 (cid:54)= CG2, and Antecedent(CG1) = Antecedent(CG2).

(iii) In Defn. 12, U Graph : G → G is well-deﬁned. That is, if U Graph(CG1) (cid:54)= U Graph(CG2)

then CG1 (cid:54)= CG2. This follows from the contrapositive which is easily shown to hold
(that is, if CG1 = CG2 then U Graph(CG1) = U Graph(CG2)).

46

Tirtharaj Dash et al.

Finally, we relate the clausal explanations found by some MDIE systems using the ordering
(cid:22)θ deﬁned over clauses in (Plotkin, 1970).17 Given background knowledge B and a clause
e, we will say a clause C is a clausal explanation for e if B ∪ {C} |= e.

Remark 4 (Relation to Clausal Explanations) Let ⊥B,M,d(e) be the ground most-
speciﬁc deﬁnite-clause using MDIE. Let C be a clause (not necessarily ground). We show
the following: If Cθ ⊆ ⊥B,M,d(e) and Cθ ∈ LB,M,d, then there exists a clause-graph CG(cid:48)
s.t. CG(cid:48) (cid:22)cg ClauseT oGraph( ⊥B,M,d(e) and GraphT oClause(CG(cid:48)) = Cθ.

Denoting ⊥B,M,d(e) as ⊥(e) and ClauseT oGraph(⊥B,M,d) as CG⊥(e) the relationships

described in this remark is shown diagrammatically as:

Let ClauseT oGraph(⊥B,M,d(e)) = (G, ψ), with G = (X, Y, E). In the following, P os(l) =

p if l = ¬p is a negative literal, otherwise P os(l) = l. Consider the structure CG(cid:48) = (G(cid:48), ψ(cid:48)),
with G(cid:48) = (X (cid:48), Y (cid:48), E(cid:48)) obtained as follows.

(a) X (cid:48) = {xi : xi ∈ X, li ∈ Cθ, λi = P os(li), ψ(xi) = (λi, µi)};
(b) E(cid:48) = {(xi, yj ) : xi ∈ X (cid:48), (xi, yj ) ∈ E} ∪ {(yj , xi) : xi ∈ X (cid:48), (yj , xi) ∈ E};
(c) Y (cid:48) = {yj : (xi, yj ) ∈ E(cid:48) or (yj , xi) ∈ E(cid:48)};
(d) For v ∈ X (cid:48) ∪ Y (cid:48), ψ(cid:48)(v) = ψ(v)

It is evident that G(cid:48) is a directed bipartite graph, and ψ(cid:48) is deﬁned for every vertex
in G(cid:48). So, CG(cid:48) ∈ G. By construction, CG(cid:48) has the following properties: (i) X (cid:48) ⊆ X and
Y (cid:48) ⊆ Y ; (ii) E(cid:48) ⊆ E; and (iii) ψ(cid:48) ⊆ ψ. Therefore CG(cid:48) (cid:22)cg (G, ψ). Since the vertices in X (cid:48)
are obtained using only the literals in Cθ, it follows that GraphT oClause(CG) = Cθ.

Since Cθ ⊆ ⊥B,M,d(e), Cθ |= ⊥B,M,d(e). Further, since C (cid:22)θ Cθ, C |= Cθ. It follows
that B ∪ C |= B ∪ ⊥B,M,d(e). Since B ∪ ⊥B,M,d(e) |= e, then B ∪ C |= e. That is, C is a
clausal explanation for e.

B Implementation Details on GNNs

We provide some basic mathematical details on how various graph convolutions and pooling
operations are implemented in the ﬁve diﬀerent GNN variants considered in our work. This
section is meant for completeness only. The reader should refer to the primary sources of
these implementations for a detailed information on each of these approaches.

B.1 Graph Convolution

For each graph convolution method, we only describe how the AGGREGATE-COMBINE pro-
cedure (as described in Sec. 2) is implemented.

17 C1 (cid:22)θ C2 if there exists some substitution θ s.t. C1θ ⊆ C2. By convention C1 is said
to be more-general than C2, and C2 is said to be more-speciﬁc than C1. It is known that if
C1 (cid:22)θ C2, then C1 |= C2.

Inclusion of Domain-Knowledge into GNNs using MDIE

47

Variant 1: GCN

Based on the spectral-based graph convolution as proposed by Kipf and Welling (2017), this
graph convolution uses a layer-wise (or iteration-wise) propagation rule for a graph with N
vertices as:

H(k) = σ

(cid:16) ˜D− 1

2 ˜A ˜D− 1

2 H(k−1)Θ(k−1)(cid:17)

(1)

where, H (k) ∈ RN ×D denotes the matrix of vertex representations of length D, ˜A = A + I is
the adjacency matrix representing an undirected graph G with added self-connections, A ∈
RN ×N is the graph adjacency matrix, IN is the identity matrix, ˜Dii = (cid:80)
˜Aij , and Θ(k−1)
is the iteration-speciﬁc trainable parameter matrix, σ(·) denotes the activation function e.g.
ReLU(·) = max(0, ·), H(0) = X, X is the matrix of feature-vectors of the vertices, where
each vertex i is associated with a feature-vector Xi.

j

Variant 2: k-GNN

This graph convolution passes messages (vertex feature-vectors) directly between subgraph
structures inside a graph (Morris et al., 2019). At iteration k, the feature representation of
a vertex is computed by using

h(k)
u = σ


h(k−1)

u

· Θ(k)

1 +

(cid:88)

h(k−1)
v

· Θ(k)
2



v∈N (u)



(2)

where, hk
u denotes the vertex-representation of a vertex u at iteration k, N denotes the
neighborhood function, σ is a non-linear transfer function applied component wise to the
function argument, Θs are the layer-speciﬁc learnable parameters of the network.

Variant 3: GAT

This variant is based on aggregating information from neighbours with attention. This ap-
proach is popularly known as Graph Attention Network (GAT: (Veli˘ckovi´c et al., 2018)).
This network assumes that the contributions of neighboring vertices to the central vertex
are not pre-determined which is the case in the Graph Convolutional Network (Kipf and
Welling, 2017). This adopts attention mechanisms to learn the relative weights between two
connected vertices. The graph convolutional operation at iteration k is thereby deﬁned as:

h(k)
u = σ





(cid:88)

uv Θ(k)h(k−1)
α(k)

u





v∈N (u)∪u

(3)

u denotes the vertex-representation of a vertex u at iteration k; h(0)
where, hk
u = Xu (the
initial feature-vector associated with a vertex u). The connective strength between the vertex
u and its neighbor vertex v is called attention weight, which is deﬁned as

α(k)

uv = softmax

(cid:16)

LeakyReLU

(cid:16)
aT (cid:104)

Θ(k)h(k−1)
u

(cid:107) Θ(k)h(k−1)

v

(cid:105)(cid:17)(cid:17)

(4)

where, a is the set of learnable parameters of a single layer feed-forward neural network, ||
denotes the concatenation operation.

Variant 4: GraphSAGE

This graph convolution is based on inductive representation learning on large graphs (Hamil-
ton et al., 2017), which is primarily used to generate low-dimensional vector representations
for vertices. It adopts two steps: First, it samples a neighbourhood vertices of a vertex;

48

Tirtharaj Dash et al.

Second, aggregate the feature-information from these sampled vertices. GraphSAGE is used
to found to be very useful for graphs with vertices associated with rich feature-vectors. The
following is an iterative update of the vertex representations in a graph:

h(k)
u = σ


h(k−1)

u

· Θ(k)

1 +

1
|N (u)|



(cid:88)

h(k−1)
v

· Θ(k)
2



v∈N (u)

(5)

where, hk
u denotes the vertex-representation of a vertex u at iteration k, σ is a non-linear
transfer function applied component wise to the function argument, N denotes the neigh-
borhood function, Θs are the layer-speciﬁc learnable parameters of the network.

Variant 5: ARMA

This graph convolution is inspired by the auto-regressive moving average (ARMA) ﬁlters
that are considered to be more robust than polynomial ﬁlters (Bianchi et al., 2021). The
ARMA graph convolutional operation is deﬁned as:

H(k) =

1
M

M
(cid:88)

m=1

H(K)
m

(6)

where, Hk denotes the vertex-representation matrix at iteration k, M is the number of
parallel stacks, K is the number of layers; and H(K)

m is recursively deﬁned as

H(k+1)
m

= σ

(cid:16) ˆLH(k)

m Θ(k)

2 + H(0)Θ(k)

2

(cid:17)

(7)

where, σ is a non-linear transfer function, ˆL = I − L is the modiﬁed Laplacian. The Θ
parameters are learnable parameters.

B.2 Graph Pooling

Graph pooling methods apply the idea of downsampling mechanisms to graphs. This oper-
ation allows us to obtain reﬁned graph representations at each layer. The primary aim of
including a graph pooling operation after each graph convolution is that this operation can
reduce the graph representation while ideally preserving important structural information.
In this work, we use a recently proposed graph pooling method based on self-attention (Lee
et al., 2019). This method uses the graph convolution deﬁned in Eq. (1) to obtain a self-
attention score as given in Eq. 8 with the trainable parameter replaced by Θatt ∈ RN ×1,
which is a set of trainable parameters in the pooling layer.

Z = σ

(cid:16) ˜D− 1

2 ˜A ˜D− 1

2 XΘatt

(cid:17)

(8)

Here, σ(·) is the activation function e.g. tanh.

B.3 Hierarchical Graph Pooling

The graph-convolution and graph-pooling operations described in the preceding two sub-
sections allows a GNN are concerned with construction of vertex-representations. To deal
with the problem of graph classiﬁcation (as is the case in this work), we need to represent
an input graph as a “ﬂattened” ﬁxed-length feature-vector that can then be used with a
standard fully-connected multilayer neural network (e.g. Multilayer Perceptron) to produce

Inclusion of Domain-Knowledge into GNNs using MDIE

49

a class-label. To construct this graph-representation (mostly, a dense real-valued feature-
vector, also called a graph-embedding), we use hierarchical graph-pooling method proposed
by Cangea et al. (2018). This method is implemented with two operations: (a) global average
pooling, that averages all the learnt vertex representations in the ﬁnal (readout) layer; (b)
augmenting the representation obtained in (a) with the representation obtained using global
max pooling, that seek to obtained the most relevant information and could strengthen the
graph-representation. The term “hierarchical” refers to the fact that the above two oper-
ations (a) and (b) are carried out after each conv-pool block in the GNN (refer Fig. 4).
The ﬁnal graph representation is an aggregate of all the layer-wise representations by taking
their sum.

The output graph after each conv-pool block can be represented by a concatenation of

the global average pool representation and the global max pool representation as:

H (k)

G = avg(H(k)) || max(H(k))

(9)

where, H k
G denotes the graph-representation at iteration k; H(k) denotes the matrix of
vertex-representations after conv-pool operations at iteration k as mathematically described
in Sec. B.1 and Sec. B.2; avg and max denote the average and max operations, which are
computed as follows:

avg(H(k)) =

1
N

N
(cid:88)

i=1

H(k)
i

(10)

N
max
i=1
i denotes the representation for the ith vertex of the graph; N is the number of

max(H(k)) =

H(k)
i

(11)

Here, Hk
nodes in the graph.

The ﬁnal ﬁxed-length representation after iteration K for the whole input graph is then
computed by the element-wise sum, denoted as ⊕, of these intermediate graph-representations
in Eq. 9:

H (K)

G = ⊕K

k=1H (k)

G

(12)

In our present work, K = 3 (since, we use 3 conv-pool blocks in our GNN). The graph-
representation H (K)

G is then input to a multilayer perceptron as described in Sec. 5.3.

C Application Details

C.1 Mode-Declarations

We use the ILP engine, Aleph (Srinivasan, 2001) to construct the most-speciﬁc clause for a
relational data instance given background-knowledge, mode speciﬁcations and a depth. The
mode-language used for our main experiments in the paper is given below:

:- modeb(*,bond(+mol,-atomid,-atomid,#atomtype,#atomtype,#bondtype)).
:- modeb(*,has_struc(+mol,-atomids,-length,#structype)).
:- modeb(*,connected(+mol,+atomids,+atomids)).
:- modeb(*,fused(+mol,+atomids,+atomids)).

The ‘#’-ed arguments in the mode declaration refers to type, that is, #atomtype refers to
the type of atom, #bondtype refers to the type of bond, and #structype refers to the type
of the structure (functional group or ring) associated with the molecule.

C.2 Logical Representation of the Data and Background Knowledge

Data in the primary set of experiments are molecules. At the lowest level, the atoms and
bonds in each molecule are represented as a set of ground deﬁnitions of the bond/6 predicate.

50

Tirtharaj Dash et al.

Thus bond(m1,27,24,o2,car,1) denotes that in instance m1 there is an oxygen atom (id 27),
and a carbon atom (id 24) connected by a single bond (car denotes a carbon atom in an
aromatic ring). Deﬁnitions of functional-groups and ring-structures are written as clausal
deﬁnitions that use the deﬁnitions of this low-level bond/6 predicate. There are about 100
such higher-level deﬁnitions. The result of inference about the presence of functional groups
and rings is pre-compiled for eﬃciency. For example:

functional_group(m1,[27],1,oxide).
ring(m1,[25,28,30,29,26,23],6,benzene_ring).

Here, the 1 and 6 denote number of atoms involved in the group or ring. Access to groups
and rings in a molecule uses the has struc/4 predicate:

has_struc(Mol,Atoms,Length,Type):-

ring(Mol,Atoms,Length,Type).

has_struc(Mol,Atoms,Length,Type):-

functional_group(Mol,Atoms,Length,Type).

...

In addition to functional groups and rings, there are predicates for computing if structures
are connected or fused. An example of the most-speciﬁc clause for a data instance is:

class(m411,pos):-

bond(m411,18,13,c3,o2,1),
bond(m411,13,18,o2,c3,1),
bond(m411,12,16,nar,nar,ar),...,
has_struc(m411,[5,7,10,6,4],5,pyrrole_ring),
has_struc(m411,[7,11,16,12,8,5],6,pyridazine_ring),...,
connected(m411,[15],[9,14,13]),
connected(m411,[15],[7,11,16,12,8,5]),...,
fused(m411,[7,11,16,12,8,5],[5,7,10,6,4]),...,
lteq(1,1),lteq(3,3),...,
gteq(1,1),gteq(3,3),....

C.3 Propositionalisation Experiments

In a propositionalisation approach (Lavraˇc et al., 1991), each data instance is represented
using a Boolean vector of 0’s and 1’s, depending on the value of propositions (constructed
manually or automatically) for the data instance (the value of the ith dimension is 0 if the
ith proposition is false for the data-instance and 1 otherwise). The resulting dataset is then
used to construct an MLP model. The following details are relevant:

– The MLP is implemented using Tensorﬂow-Keras (Chollet et al., 2015).
– The number of layers in MLP is tuned using a validation-based approach. The parameter

grid for number of hidden layers is: {1, 2, 3, 4}.

– Each layer has ﬁxed number of neurons: 10.
– The dropout rate is 0.5. We apply dropout (Srivastava et al., 2014) after every layer in

the network except the output layer.

– The activation function used in each hidden layer is relu.
– The training is carried out using the Adam optimiser (Kingma and Ba, 2015) with

learning rate 0.001.

– Additionally, we use early-stopping (Prechelt, 1998) to control over-ﬁtting during train-

ing.

For the DRM, propositions simply denote whether any speciﬁc relation in the background
knowledge is true or false for the data instance. BCP (Fran¸ca et al., 2014) constructs proposi-
tions using the most-speciﬁc clauses returned by the ILP system Aleph given the background-
knowledge B, modes M and depth-limit d. For the construction of Boolean features using
BCP, we use the code available at (Jankovics, 2020).

Inclusion of Domain-Knowledge into GNNs using MDIE

51

C.4 Experiments with ILP Benchmarks

The seven datasets are taken from (Srinivasan et al., 2003). These datasets are some of the
most popular benchmark datasets to evaluate various techniques within ILP studies. For
the construction of BotGNNs the following details are relevant:

– There is a BK for each dataset.
– There are 10 splits for each dataset. Therefore, for each test-split we construct BotGNNs
(all 5 variants), using 8 of rest splits as training set and the remaining 1 split as a
validation set.

– Since these datasets are small (few hundreds of data instances), we could manage
to perform some hyperparameter tuning for construction of our BotGNNs. The pa-
rameter grids for this are: m : {8, 16, 32, 64, 128}; batch-size: {16, 32}; learning rate:
{0.0001, 0.0005, 0.001}.

– Other details are same as described in the main BotGNN experiments.
– In Section 5.5 we report the test accuracy from the best performing BotGNN variant.

