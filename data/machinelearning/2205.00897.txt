2
2
0
2

n
u
J

0
1

]

C
O
.
h
t
a
m

[

2
v
7
9
8
0
0
.
5
0
2
2
:
v
i
X
r
a

Fast Continuous and Integer L-shaped
Heuristics Through Supervised Learning

Eric Larsen ∗

Emma Frejinger ∗†

Bernard Gendron ∗

Andrea Lodi ‡

June 14, 2022

Abstract

We propose a methodology at the nexus of operations research and
machine learning (ML) leveraging generic approximators available from
ML to accelerate the solution of mixed-integer linear two-stage stochastic
programs. We aim at solving problems where the second stage is highly
demanding. Our core idea is to gain large reductions in online solution
time while incurring small reductions in ﬁrst-stage solution accuracy by
substituting the exact second-stage solutions with fast, yet accurate super-
vised ML predictions. This upfront investment in ML would be justiﬁed
when similar problems are solved repeatedly over time, for example, in
transport planning related to ﬂeet management, routing and container
yard management.

Our numerical results focus on the problem class seminally addressed
with the integer and continuous L-shaped cuts. Our extensive empirical
analysis is grounded in standardized families of problems derived from
stochastic server location (SSLP) and stochastic multi knapsack (SMKP)
problems available in the literature. The proposed method can solve the
hardest instances of SSLP in less than 9% of the time it takes the state-
of-the-art exact method, and in the case of SMKP the same ﬁgure is 20%.
Average optimality gaps are in most cases less than 0.1%.

Keywords: stochastic programming, machine learning, integer L-shaped,

integer linear programming, supervised learning

1 Introduction

Decision-making problems in a broad range of application domains are subject
to uncertainty. Prominent examples are supply chain management and trans-
port planning that during the COVID-19 pandemic received media attention for
their widespread disruptions and lack of resiliency. Real-life instances of related

∗Department of Computer Science and Operations Research and CIRRELT, Université de

Montréal

†Corresponding author. Email: frejinger.umontreal@gmail.com
‡CERC, Polytechnique Montréal and Jacobs Technion-Cornell Institute, Cornell Tech and

Technion

1

 
 
 
 
 
 
decision-making problems, such as ﬂeet management and network design prob-
lems, are known to be hard to solve, even in a deterministic form. Stochastic
formulations are therefore rarely used in practice, despite their well-documented
advantages when there is uncertainty in problem parameters.

We consider a general stochastic linear two-stage problem (P) of the form

(notation borrowed from Angulo et al., 2016)

min
x,z,θ

{cx + dz + θ}

s.t. Ax + Cz ≤ b,
Q(x) − θ ≤ 0,
x ∈ {0, 1}n,
z ≥ 0, z ∈ Z,

where the second-stage subproblem (S) is

Q(x) :≡ Eξ[min
y

{qξy : Wξy ≥ hξ − Tξx, y ∈ Y}],

(1)
(2)

(3)
(4)

(5)

and Z and Y may embody integrality constraints and a number of additional
constraints pertaining to z and y, respectively. The coupling constraints Wξy ≥
hξ − Tξx tie the ﬁrst and second stages. We concentrate on formulations where
(i) stochastic coeﬃcients ξ ≡: {qξ, Wξ, Tξ, hξ} with ﬁnite support can occur in
the second-stage objective or constraints, (ii) both stages can feature integer or
continuous variables, (iii) the variables coupling the ﬁrst and second stage are
binary-valued, and (iv) both (S) and its primal continuous relaxation (RS)

Q(x) :≡ Eξ[min
y
e

{qξy : Wξy ≥ hξ − Tξx, y ∈

Y}]
e

(6)

feature relatively complete recourse (
every integrality constraint).

Y is equal to Y except for the removal of
e

Common to many real-world settings is the solution of instances of (P) shar-
ing similar patterns. Consider for example the tactical-operational planning of
transport systems. This similarity lends itself to machine learning (ML) where
data on the distribution of instances of interest could be leveraged oﬄine to
gain online speed-up. In this context, our goal is to devise a general ML-based
matheuristic to solve mixed-integer linear two-stage stochastic programs, where
the second stage is highly demanding computationally. More precisely, our core
idea is to gain large reductions in online solution time while incurring small
reductions in ﬁrst-stage solution accuracy by substituting the solutions of the
latter stages of Benders decomposition with fast, yet accurate approximations
arising from supervised ML.

1.1 Relations with existing literature

Let us turn our attention (i) to exact methods currently available in the oper-
ations research (OR) literature addressing problem class (P) and to heuristic
methods from the (ii) OR and the (iii) ML literatures and to the relations of
our own heuristic method therewith.

2

1.1.1 Exact methods

Problem (P) is seminally addressed in Laporte and Louveaux (1993) with the
integer L-shaped method operating on the Benders decomposition of (P). Let
L denote a lower bound on Q(x). The master (ﬁrst-stage) problem (M) is then

min
x,z,θ

{cx + dz + θ}

s.t. (1), (3), (4),

Πx − 1θ ≤ π0,
θ ≥ L,

(7)

(8)

where 1 denotes a column vector of ones. The set of constraints (7) is ini-
tially (i.e., at the root node of the algorithm) empty (unless initial cuts have
been added, for instance, in a multi-phase solution process), and progressively
populated with optimality cuts as a branch-and-Benders-cut process advances.
Laporte and Louveaux (1993) apply the integer L-shaped optimality cut at a
candidate solution x∗ ∈ {0, 1}n

(Q(x∗) − L) (cid:16) X

i∈S(x∗)

xi −

X
i /∈S(x∗)

xi − |S(x∗)|(cid:17) + Q(x∗) ≤ θ,

(9)

where S(x∗) :≡ {i : x∗
whenever x∗ turns out to be an invalid ﬁrst-stage solution.

i = 1} and Q(x∗) is the optimal value of (S) at x∗,

To the best of our knowledge, Angulo et al. (2016) is currently the most
up-to-date and complete overview available about the application of Benders
decomposition methods to problem class (P). They propose an algorithm that
alternates between continuous and integer L-shaped cuts. At a given candidate
integral ﬁrst-stage solution x∗ that turns out to be invalid, their alternating
cuts strategy ﬁrst computes a subgradient cut derived from (RS). In particular,
if Y only imposes that second-stage variables in subproblem (S) are binary,
then a continuous L-shaped optimality mono-cut (Birge and Louveaux, 2011,
pp. 183-184) is deﬁned by

Eξ[φξ(hξ − Tξx) − 1′ψξ] ≤ θ,

(10)

where φξ, ψξ are the solutions to the duals (DRS) corresponding to RS, evaluated
at x∗,

{φξ(hξ − Tξx∗) − 1′ψξ : φξWξ − ψξ ≤ qξ, φξ ≥ 0, ψξ ≥ 0},

(11)

max
φξ,ψξ

∀ξ. Here, the dual variables ψξ, ∀ξ are attached to the upper bound (equal to
1) embodied in

Y and constraining the second-stage variables in (RS).
e

The alternating algorithm of Angulo et al. (2016) resorts to the calculation
of the integer L-shaped cut (9) (which is then imposed in the ﬁrst-stage prob-
lem) only if the continuous L-shaped mono-cut (10) fails to separate the invalid
ﬁrst-stage solution. Otherwise, the continuous L-shaped cut is added to the ﬁrst
stage. This algorithm is generally faster than the standard integer L-shaped al-
gorithm since (i) calculating the solution of the integral second-stage problem
usually requires considerably more time than calculating the solution of its con-
tinuous relaxation and (ii) a binding continuous L-shaped cut will generally
create a deeper separation than the corresponding integer L-shaped cut.

3

Angulo et al. (2016) also propose a new type of integer optimality cut, based
on disjunctive programming, that can be used in place of (9), either in the in-
teger L-shaped method or in their alternating algorithm. However, their exper-
imental results indicate that the introduction of these cuts has a marginal or
relatively small impact on computing times, depending on the class of problems
considered.

1.1.2 Heuristic methods from OR

We turn our attention to heuristic algorithms that have been proposed to solve
(P). A strong alternative to the aforementioned exact methods from the OR lit-
erature is the progressive hedging (PH) algorithm (Rockafellar and Wets, 1991,
Watson and Woodruﬀ, 2011) in view of its broad applicability, the interest it
has attracted, its advanced and continuing development, and of the availabil-
ity of an extensive computational platform. The dual decomposition algorithm
(Carøe and Schultz, 1999) is also a possible contender. However, the docu-
mented performance of PH, provided it is favorably tuned, apparently sur-
passes that of dual decomposition for certain classes of problems, notably that
of stochastic server location problems on which we draw as a test bench in our
experiments (Torres et al., 2022, Woodruﬀ, 2016).

In OR, the idea of ap-
Commonalities of the proposed method in OR.
proximating all or part of a complex programming problem or of its solution
has chieﬂy been investigated in simulation optimization and, to a much lesser
extent, in bilevel programming. In our method, an ML predictor plays a role
comparable to the objects alternatively known as metamodels, surrogate models
or response surfaces (Barton, 2020, Barton and Meckesheimer, 2006) that are
encountered in simulation optimization (Amaran et al., 2016). Likewise, our ML
predictor describes a map between the inputs and output(s) of an optimization
process whose exact form is ﬁtted to data. In contrast, we are neither perform-
ing optimization over the ML predictor itself as a surrogate for the map between
inputs and output(s) of the original optimization problem nor inserting it as a
surrogate constraint as a stand in for a subproblem occurring in the original op-
timization problem. It is used, instead, as an auxiliary instrument to accelerate
computations by generating at a high speed the intermediate inputs that are
required in the broader optimization process governed by a Benders decomposi-
tion. This use as an auxiliary instrument is exploited in Larsen et al. (2022) and
Frejinger and Larsen (2019). From this perspective, and to our knowledge, our
closest neighbors are Sinha et al. (2016) and Sinha et al. (2020), where meta-
models approximate the lower-level reaction set mapping and the lower-level
value function in evolutionary algorithms handling deterministic, bilevel pro-
gramming problems featuring continuous variables.

1.1.3 Heuristic methods from ML

There is an abundant and steadily growing literature in ML about constrained
It is surveyed in
optimization, and combinatorial optimization in particular.
Bengio et al. (2021), Kotary et al. (2021a,b), Vesselinova et al. (2020). Bengio et al.
(2021) and Kotary et al. (2021a) distinguish two main areas in which ML oper-
ates. The ﬁrst one, ML alongside optimization algorithms (Bengio et al., 2021),

4

i.e., ML-augmented constrained optimization (Kotary et al., 2021a), designates
the utilization of ML to inform the decisions made by optimization algorithms
in order to enhance their eﬃciency. Our contribution is not quite of this nature.
The second main area is end-to-end learning, whose deﬁning characteristic is
simply the application of ML within calculations towards the solution of opti-
mization problems. This better corresponds to our matheuristic algorithm: we
utilize ML to approximate a core component of the Benders decomposition, that
is the solution of second stage, given ﬁrst-stage coupling variables, to achieve
an approximate ﬁrst-stage solution for the stochastic programming problem at
hand.

Kotary et al. (2021a) further distinguish two subareas in end-to-end con-
strained optimization learning:
(i) predicting solutions and (ii) predict-and-
optimize. On the one side, (i) designates the application of supervised or rein-
forcement learning (RL) to predict the overall solutions of constrained optimiza-
tion problems. Cappart et al. (2020), Mazyavkina et al. (2021), Wang and Tang
(2021) detail the application of RL in this context. Graphical neural net-
works (GNN) are especially well-adapted to the representation of combinatorial
problems and their application is detailed in Cappart et al. (2021), Peng et al.
(2021), Schuetz et al. (2022). On the other side, (ii) designates the search for
solutions to partially deﬁned problems while accounting jointly for their un-
known aspects or parameters. Predecessors of methods currently in (ii) belong
to an area known as predict-then-optimize, where prediction of the unknown
aspects of the optimization problem and optimization itself are performed se-
quentially. Recent information on (ii) is available in Elmachtoub and Grigas
(2022), Fioretto (2022), Zhang et al. (2022). Whereas methods in subarea (ii)
involve the application of mathematical programming solvers, those in (i) rely
entirely on optimization of ML architectures. It is worth noting that an assign-
ment of papers to speciﬁc subareas is often not totally precise, because some
works can be interpreted diﬀerently and might ﬁt into multiple categories.

The following recent publications lie at the intersection of stochastic pro-
gramming and ML. Given a set of scenarios, Bengio et al. (2020) propose to
predict a representative scenario to form a smaller surrogate problem that is
easier to solve with a general purpose MIP solver. The deﬁnition of a repre-
sentative scenario depends on the structure of the problem. This approach can
be assigned to both ML-augmented constrained optimization and end-to-end
learning. In constrast, Nair et al. (2018) and Dai et al. (2021) belong to sub-
area (i) of end-to-end learning. Nair et al. (2018) propose an RL algorithm for
two-stage stochastic programs with unconstrained binary decisions and hence
does not address the general problem class (P). Dai et al. (2021) address multi-
stage stochastic programs. Although this class is a generalization of (P), the
respective contexts and solution approaches are distant: The authors exploit
the commonalities between the problems successively solved by the stochastic
dual dynamic programming algorithm to progressively train an eﬃcient approx-
imator of the value function. In a very recent paper, Dumouchelle et al. (2022)
propose to approximate the second-stage solution value with a feed-forward neu-
ral network (in a fashion similar to Larsen et al., 2022). Using the results in
Fischetti and Jo (2018), they represent the feed-forward network as a mixed-
integer programming (MIP) problem, which allows them to integrate it in the
MIP formulation of the ﬁrst-stage problem.

5

Relation of proposed method with ML. The method we propose falls
outside of subareas (i) and (ii) currently explored in ML. Since it involves an
ML predictor whose role is to inform an overall solution process handled by a
solver, it is somewhat more closely connected to (ii). Yet, the uncertainty of the
problems it addresses is fully speciﬁed rather than discovered through prediction
and, instead of missing data, our method predicts intermediate values (e.g.,
expectations) that are instrumental to the overall optimization process governed
by the underlying mathematical programming algorithm. It also optimizes the
predictor and the overall solution sequentially, rather than jointly. To the best
of our knowledge, Dumouchelle et al. (2022) is the only other work addressing
the general problem class (P) with ML. Beyond approximating the second-
stage solution value with ML, the diﬀerences between their approach and the
methodology we propose run deep.

1.2 Contributions

We propose an ML-based matheuristic version of the L-shaped method that we
call ML-L-Shaped. It is designed to eﬀectively solve problems of class (P) fea-
turing computationally demanding second-stage problems. More precisely, ML-
L-Shaped is a heuristic version of both the standard integer L-shaped method
(Laporte and Louveaux, 1993) and the algorithm with alternating continuous
and integer L-shaped cuts (Angulo et al., 2016), where we replace the costly
computations, that is, solution values of (S) – Q(x) – and (RS) –
Q(x) – by
fast ML predictions, and solution values of (DRS) – ψξ, φξ, ∀ξ – by fast ML
e
predictions of low-dimensional reductions.

Focusing on input-output structure and dimension reduction, we describe
how to generate labeled data about the second-stage problems so as to use stan-
dard supervised ML. We also discuss how supervised ML can utilize a method
presented in Larsen et al. (2022) to reduce the cost of generating labeled data.
Our numerical analyses are grounded in the two standard classes of problems
that are examined in Angulo et al. (2016): the stochastic server location prob-
lems (SSLP) and the stochastic multi knapsack problems (SMKP). We compare
the performance of ML-L-Shaped to the exact counterpart as well as to a PH
algorithm. Our method can solve the hardest instances of SSLP in less than
9% of the time it takes the state-of-the-art exact method, and in the case of
SMKP the same ﬁgure is 20%. Average optimality gaps are in most cases less
than 0.1%.

1.3 Structure of the paper

The remainder of the paper is structured as follows: Section 2 introduces the
ML-L-Shaped method. Section 3 describes how we operationalize the method,
i.e., the experimental setting: We review the standardized problem families on
which the detailed analysis is based, the generation of data from each family
and the input-output structures of the ML approximators. Section 4 outlines
the training and validation of ML predictors for the solutions of integral and
continuously-relaxed second-stage problems. Section 5 details and compares
the experimental results achieved by the exact and ML-assisted versions of the
algorithms presented in Section 2. We compare their performances to that of

6

an alternative heuristic algorithm (PH) in Section 6. Section 7 summarizes our
ﬁndings and outlines possible extensions and implementations of our core idea.

2 ML-based L-shaped method

This section introduces our ML-L-Shaped method. We distinguish between
the heuristic versions of the standard (Std) and alternating (Alt) cut strategies
in the argument of the main procedure of the branch-and-Benders-cut process
(Algorithm 1) by setting isAlt to true in the case of alternating cuts, and to
false otherwise. Those strategies are standard and essentially correspond to
the ones of Angulo et al. (2016), except for the heuristic callback (Algorithm 2)
introduced in Step 20.

Algorithm 1 Benders decomposition: Main
1: procedure Main(isAlt, µ, ν)
2:

3:

4:
5:

6:

7:
8:

9:

10:
11:

12:

13:

14:
15:

16:
17:

18:

19:
20:

21:

22:

Compute or retrieve the lower bound L for the objective value of (P).
Initialize a branch-and-cut process with a global node tree for (M). This
creates the repository of leaf nodes, say R. The latter initially contains
only the root node.

UB ← ∞
(x∗∗, z∗∗) ← ∅
if R = ∅ then
go to 22

else

⊲ First-stage upper bound
⊲ First-stage incumbent solution

Select a node from R.

end if
Compute the current optimal solution (x∗, θ∗) to (M) for the node at
hand.
if (cx∗ + dz∗ + θ∗) ≥ UB then
Discard the node from R.
go to 6

end if
if (x∗, z∗) is not integral then

Partition the domain of (x, z) in (M) or add MIP-based cuts. Ac-
cordingly add newly deﬁned nodes to R or update existing nodes
in R.
go to 6

end if
HeuristicCallback(isAlt, µ, ν)
go to 6
Retrieve the ﬁnal ﬁrst-stage incumbent solution (x∗∗, z∗∗). Compute
the ﬁnal overall value cx∗∗ + dz∗∗ + Q(x∗∗).

23: end procedure

Let QML(x∗) denote the value of Q(x∗) predicted by ML. We deﬁne a heuris-

tic version of the exact integer L-shaped cut (9) as

QML
(cid:0)

(x∗) − L

xi −

(cid:1) (cid:16) X

i∈S(x∗)

X
i /∈S(x∗)

xi − |S(x∗)|(cid:17) + QML

(x∗) ≤ θ.

(12)

7

Similarly, let

Q(x∗) predicted by ML in (6).
Now, we might consider deﬁning a heuristic version of the exact continuous
e
L-shaped mono-cut (10) as

QML(x∗) denote the value of
e

Eξ[φML

ξ

(hξ − Tξx) − 1′ψML

ξ

] ≤ θ,

(13)

where φML
ξ
pearing in (11), ∀ξ.

and ψML

ξ would denote predictions of φξ and ψξ, respectively, ap-

However, it is advantageous to note that detailed knowledge of the poten-
tially high-dimensional φξ and ψξ, ∀ξ is not necessary to construct the mono-cut
(10). The three reductions Eξ[φξhξ], Eξ[φξTξ] and Eξ[1′ψξ] suﬃce. Further-
more, whenever hξ or Tξ are non-stochastic, that is hξ ≡ h, ∀ξ or Tξ ≡ T, ∀ξ,
it is suﬃcient to calculate Eξ[φξ] and Eξ[ψξ], respectively. By exploiting these
reductions, the task of building an ML approximation for (10) can be eased.
Hence, we deﬁne the following heuristic version of the exact continuous L-shaped
mono-cut (10) as

{Eξ[φξhξ]}ML − {Eξ[φξTξ]}MLx − {Eξ[1′ψξ]}ML ≤ θ,

(14)

where {Eξ[φξhξ]}ML, {Eξ[φξTξ]}ML and {Eξ[1′ψξ]}ML are ML predictions of
the reductions Eξ[φξhξ], Eξ[φξTξ] and Eξ[1′ψξ], respectively.

Alternatively, whenever hξ and Tξ are non-stochastic, that is hξ ≡ h, ∀ξ or
Tξ ≡ T, ∀ξ, simpliﬁcations ensue and we calculate the heuristic version of the
exact continuous L-shaped mono-cut (10) as

{Eξ[φξ]}ML(h − T x) − {Eξ[1′ψξ]}ML ≤ θ,

(15)

where {Eξ[φξ]}ML and {Eξ[1′ψξ]}ML are ML predictions of the reductions
Eξ[φξ] and Eξ[1′ψξ], respectively. These simpliﬁcations occur in the applica-
tion reported in Section 3.2.2.

Since the predictions occurring at Step 5 of Algorithm 2 may be yielded by a
single prediction model, we compute all of them simultaneously. In practice, at
Steps 5 and 10 of Algorithm 2, C-language bindings launch GPU computations
returning ML predictions.

In addition to the argument determining the variant of the algorithm (stan-
dard integer L-shaped or alternating cuts strategy), there are two hyperparam-
eters: 0 ≪ µ ≤ 1 and 0 ≪ ν ≤ 1. They control the likelihood of introducing
incorrect integer L-shaped or continuous L-shaped cuts. This could occur even
if the predictions yielded by ML are of high quality and feature low average ab-
solute relative errors. Concretely, since we are solving minimization problems,
QML(x∗) and QML(x∗) overestimate the
we wish to control the likelihood that
corresponding exact values. We achieve this by uniformly shifting down the ML
e
predictions. The exact values of these shifts speciﬁed with µ and ν are estimated
from a preliminary tuning process on an independent set of problem instances,
starting with a value of one. Note that we obtain the exact counterparts of
Q(x∗),
the heuristic algorithm by ﬁxing µ = ν = 1 and by using exact values
Eξ[φξhξ], Eξ[φξTξ], Eξ[1′ψξ], Eξ[φξ], Eξ[1′ψξ], Q(x∗) instead of predictions, as
e
well as cuts (10) and (9) in Steps 7 and 17 of Algorithm 2, respectively.

Algorithm 1 terminates with a feasible solution, provided that Step 13 of
Algorithm 2 is reached at least once. The likelihood of reaching that step is
controlled by the aforementioned shift coeﬃcients. In the unlikely event that

8

Algorithm 2 Benders decomposition: Heuristic callback
1: procedure HeuristicCallback(isAlt, µ, ν)
2:

if !isAlt then
go to 10

3:

4:
5:

6:

7:
8:

9:

10:
11:

12:

13:
14:

15:
16:

17:

⊲ Alternating cut strategy

end if
Compute predictions
QML(x∗), {Eξ[φξhξ]}ML, {Eξ[φξTξ]}ML, {Eξ[1′ψξ]}ML
or
e
QML(x∗), {Eξ[φξ]}ML, {Eξ[1′ψξ]}ML.
e
if ν

QML(x∗) > θ∗ then
Add a heuristic continuous L-shaped mono-cut (14) or (15).
e
return

end if
Compute prediction QML(x∗).
if µQML(x∗) ≤ θ∗ then

if cx∗ + dz∗ + θ∗ < UB then
UB ← cx∗ + dz∗ + θ∗
(x∗∗, z∗∗) ← (x∗, z∗)

end if

else

⊲ Integer L-shaped method

⊲ Update upper bound
⊲ Update incumbent solution

Add a heuristic integer L-shaped cut (12).

end if
18:
19: end procedure

the algorithm terminates without a feasible solution, it can be reapplied, after
decreasing the values of µ and ν. This occurred twice over all problem instances
considered in Section 5.

We now turn to the diﬀerences between the heuristic integer L-shaped and
alternating cut algorithms. First, recall that the alternating cut strategy was
proposed by Angulo et al. (2016) to avoid costly computations of Q(x∗). On the
contrary, predictions QML(x∗) are fast to compute (in the order of a few millisec-
onds). Moreover, as we further discuss in Section 3, the task of predicting Q(x∗)
Q(x∗) and the reductions of φξ, ψξ, ∀ξ. This a
is easier than that of predicting
priori favors the heuristic version of the standard integer L-shaped method over
e
the alternating cuts strategy. However, the latter is likely to be useful when
the ﬁrst-stage problem is hard. Indeed, integer L-shaped cuts suppress only one
ﬁrst-stage solution at a time whereas continuous L-shaped cuts can be stronger.
This can make a diﬀerence when the number of heuristic integer L-shaped cuts
is very large if used alone. In these circumstances, the solution process can be
hindered by their large number despite the very high speed of the individual
computations.

We consider two further variants of the proposed algorithm. Both involve
two phases, where Algorithm 1 is used in a ﬁrst phase to produce a feasible
solution. In the ﬁrst variant, this solution is used alone to warm start the exact
standard integer L-shaped method, or the one with alternating cuts. The re-
sulting approach is then exact. In the second variant, in addition to supplying
a warm-start solution, we introduce a probabilistic lower bound on the value
of the ﬁrst-stage objective in the exact solution process. We obtain this prob-

9

abilistic lower bound for a given problem family by computing the empirical
distribution of exact objective values from a preliminary, independent set of
instances and calculating a 10% one-sided Chebyshev lower conﬁdence bound
(Ngo, 2011). Such conﬁdence bounds are pessimistic and the resulting solutions
can be interpreted as having a probability of at least 90% of being exact.

3 Experimental setting: problem families, data
generation and input-output structure of ML
predictors

We ground our numerical analysis of the ML-L-Shaped method in the standard
problem sets of classes SSLP and SMKP that are examined in Angulo et al.
(2016) and made available online (Ahmed et al., 2015). As far as we know,
problems in class SSLP were originally deﬁned and solved in Ntaimo and Sen
(2005). Section 3.1 brieﬂy describes the problem families we focus on. From
these families, we select instances that are reported as being hard to solve in
the literature. We parameterize these instances so that we can generate sam-
ples of instances featuring similar characteristics. Section 3.2 details the data
generation process and the input-output structures of our ML predictors.

3.1 Problem families

The SSLP and SMKP specialize problem (P) described in Section 1 as follows:

1. In both SSLP and SMKP, z is absent, i.e., Z = ∅, and Y imposes only

binary restrictions.

2. In SSLP, qξ ≡ q, Wξ ≡ W , Tξ ≡ T , ∀ξ, i.e., all second-stage coeﬃcients

are deterministic, except the right-hand sides of some constraints.

3. In SMKP, Wξ ≡ W , hξ ≡ h, Tξ ≡ T , ∀ξ, i.e., all second-stage coeﬃcients

are deterministic, except those appearing in the objective.

From SSLP (Section 5.1 Angulo et al., 2016), we select problems SSLP(10,
50, 2000) and SSLP(15, 45, 15), where SSLP(a, b, c) features a servers, b
clients and c second-stage scenarios. According to the computations reported
in Angulo et al. (2016), these are the most diﬃcult to solve exactly among the
problems in SSLP whose detailed statements have been made publicly available.
The second stages of these problems are also among the most diﬃcult to solve
in SSLP.

From SMKP (Section 5.2 Angulo et al., 2016), we select problems indexed
by 29 and 30, denoted SMKP(29) and SMKP(30). Similarly, these are the most
diﬃcult to solve exactly according to Angulo et al. (2016) and the second stages
of these problems are also among the most diﬃcult in SMKP. We note that no
solution was found to SMKP(30) in Angulo et al. (2016) within the allocated
time.

The diﬀerence in the nature of the problems belonging to SSLP and SMKP
families is deep:
in comparison with problems in SSLP, problems in SMKP
feature considerably harder ﬁrst stages and considerably easier integral second-
stage problems. Clearly, the harder the second-stage problems are to solve

10

exactly, the likelier is the expected gain in speed from ML-L-Shaped. Therefore,
SSLP is a better candidate for our method than SMKP. In addition, because
of the diﬀerences between these families, we expect the alternating cuts variant
of ML-L-Shaped to perform better with SMKP than the standard integer L-
shaped cut variant, whereas we expect the opposite to hold for SSLP. These
opposing dominances are conﬁrmed in our experiments.

Recall that our goal is to solve in short online computing time individual
problems stemming from a distribution of problem instances sharing common
characteristics. With this goal in mind, and the objective to draw statistical
evidence from our numerical experiments, we turn our attention to the param-
eterization of these problems. We parameterize problems SSLP(10, 50, 2000)
and SSLP(15, 45, 15) by allowing the individual deterministic capacities of the
servers to vary. These capacities are speciﬁed in a deterministic subvector of hξ.
Thus, whereas all servers of a problem have identical capacities in Angulo et al.
(2016), these being respectively equal to 188 and 112 in SSLP(10, 50, 2000) and
SSLP(15, 45, 15), we assume in eﬀect that the problem instances that might be
encountered are characterized by server-speciﬁc capacities ranging between 75
and 300. We shall let SSLPF(10, 50, 2000) and SSLPF(15, 45, 15) denote the
derived families of problem instances obtained by parameterizing the original
individual problems SSLP(10, 50, 2000) and SSLP(15, 45, 15).

Similarly, we parameterize problems SMKP(29) and SMKP(30) of Angulo et al.

(2016) by allowing the coeﬃcients of the deterministic technology matrix T and
the deterministic right-hand side values h appearing in the coupling constraints
to vary, whereas the recourse matrix W remains ﬁxed across simulations. The in-
dividual coeﬃcients of T are integer-valued and range between 1 and 40 whereas
the individual values in h can vary freely, provided the resulting second-stage
problems feature relatively complete recourse, given the values taken by W and
T . This gives rise to the two derived families of problem instances SMKPF(29)
and SMKPF(30).

We also deﬁne two additional families: SSLPF(15, 45, 150) and SSLPF(15,
80, 15). These enable us to assess the eﬀects of moderate increases in the
complexity of second stage on the relative performances of the exact and ap-
proximate versions of the algorithms. These families feature exactly the same
ﬁrst-stage problem (M ) as that of SSLPF(15, 45, 15). Whereas SSLPF(15, 45,
150) shares the same recourse matrix W with SSLPF(15, 45, 15) but features
150 instead of 15 scenarios in its second stage, SSLPF(15, 80, 15) shares the
same 15 scenarios with SSLPF(15, 45, 15) but features 80 clients instead of 15.
In SSLPF(15, 80, 15), the coeﬃcients of the new recourse matrix W are gener-
ated according to the same distribution as that originally used in Angulo et al.
(2016) for generating those of SSLP(15, 45, 15).

3.2 Data generation

Selection of data sets made up of (input, label) examples is critical for supervised
ML. In the present context, an input is a statement of a second-stage problem
(S), (RS) or (DRS) and a label is the optimal solution thereof. The following
details the choices governing the generation of our sets of examples.

11

3.2.1 Families of SSLP instances

Generation of the data required for training and validating an ML predictor of
the optimal objective value of second stage is performed in a similar fashion for
each one of the standardized families SSLPF(10, 50, 2000), SSLPF(15, 45, 15),
SSLPF(15, 45, 150) and SSLPF(15, 80, 15). Keeping all coeﬃcients appearing
in the second stage ﬁxed except server capacities, the second-stage problems
are simulated by pseudo-randomly sampling individual server capacities from
independent discrete uniform distributions with support [75, 300] and by pseudo-
randomly sampling from independent discrete uniform distributions the values
of the binary coupling variables shared by the ﬁrst and second stages. Then, the
corresponding optimal solution of second stage is computed exactly. Each such
(problem statement, problem solution) pair constitutes a supervised example
available to ML.

Problem statements pertaining to SSLPF(10, 50, 2000) are summarized by
vectors in N20 (10 integral servers capacities + 10 coupling binaries), whereas
problem statements pertaining to SSLPF(15, 45, 15), SSLPF(15, 45, 150) and
SSLPF(15, 80, 15) are summarized by vectors in N30 (15 integral servers capac-
ities + 15 coupling binaries). As expected, preliminary experiments have shown
that the variant of Algorithm 1with isAlt set to false dominates the one where
isAlt is set to true in connection with the SSLP families. Hence, ML predictions
QML(x) required in building cuts (12) suﬃce and all solutions pertaining to
SSLP families can be described by points in R.

Similarly to Larsen et al. (2022), we deliberately generate very large numbers
of examples to ensure that availability of data does not impinge on learning.
For each one of the families, we produce a set of 1M examples. The data
sets are partitioned according to proportions 64%, 16%, 20% between training,
validation and test sets.

We also generate a second data set for the SSLPF(10, 50, 2000) family that
we identify as SSLPF-indx(10, 50, 2000). Examples in this set comprise the
same problem statements as those of SSLPF(10, 50, 2000). However, instead
of calculating the solution as an expectation over all second-stage scenarios, we
calculate it for a single randomly selected scenario.
It is considerably faster
to generate each example in this fashion: the time required for producing 1M
examples of the latter is approximately 1/80th of that required to generate 1M
examples of SSLPF(10, 50, 2000). Larsen et al. (2022) have argued and provided
evidence that the resulting ML predictors should have predictive properties close
to those of ML predictors computed from examples where the solutions reﬂect
all scenarios, calling this phenomenon “implicit aggregation” and the resulting
predictor “implicit predictor”. We later compare the performances achieved
in approximating ﬁrst-stage solutions through ML predictors originating from
SSLPF(10, 50, 2000) and SSLPF-indx(10, 50, 2000) data.

Average generation time per example of each SSLP family is below 0.6 second
for all families except SSLPF(10, 50, 2000) where the average is 3.24 seconds.
We report additional detail in Appendix A (Table 10).

3.2.2 Families of SMKP instances

Generation is performed in a distinct manner for the SMKP families.
If the
statements of the problems in SMKPF(29) and SMKPF(30) were to be described

12

by stating separately the values of each binary coupling variable, each coeﬃcient
of the technology matrix T and each element of the right-hand side h, then a
vector in N600 – since there are 100 individual coupling variables in x and T is of
dimension (5×100) – and a vector in R5 – since h is of dimension (5×1) – would
be required. However, problems in SMKPF(29) and SMKPF(30) can be stated
compactly with a vector in R5 since each problem is fully described by the real
vector h + T x of dimension (5 × 1). Reducing the input in this fashion makes it
far less demanding for ML. In general, similar input reductions are applicable
in contexts where a detailed problem description stating all individual input
elements would hamper the learning process.

As expected, our experiments show that, in contrast with families in SSLP,
ML-L-Shaped with isAlt set to true should be used with the SMKP families.
Therefore, we train and validate two ML predictors. One yields the predictor
of the expected objective value of the original second-stage subproblem (S),
QML(x∗), (ii)
QML(x), required in building cuts (12). Another outputs (i)
{Eξ[φξ]}ML and (iii) {Eξ[1′ψξ]}ML. (i), (ii) and (iii) respectively predict the
e
optimal expected objective value of the continuously-relaxed second-stage sub-
problem (RS), the expectation of the dual variables attached to the coupling
constraints and the expectation of the summation of the dual variables attached
to the upper bounds (equal to 1) of all second-stage variables.
In regard to
SMKPF(29) and SMKPF(30), (i) has value in R, (ii) has value in R5 and (iii)
has value in R. Hence, the output can be completely described by a vector in
R7. In contrast, an extensive description of the solution to the optimal solution
of the continuously-relaxed second stage that would account for every dual value
and every scenario would involve several hundred elements. Summarizing the
statement of the solution in this manner is far less demanding for ML. Similar
output reductions are applicable in contexts where a description of all individual
output elements would be overly detailed.

For each of SMKPF(29) and SMKPF(30), two data sets are generated. The
two data sets share the same reduced problem statements. However, one data
set comprises the solutions to the integral second-stage problem (S), whereas
the other comprises the reduced descriptions of solutions to its continuous relax-
ation (DRS). All four resulting data sets are partitioned according to proportions
64%, 16%, 20% between training, validation and test sets. The average gener-
ation time per example is less than 0.13 second. We report additional detail in
Table 10 of Appendix A.

4 ML predictors

Based on the data sets generated according to Section 3.2, we build the ML-
predictors yielding the approximate second-stage solutions called at Steps 5 and
10 of Algorithm 2. We proceed to detail their characteristics, the process fol-
lowed in their training/validation and to report their predictive performance
over their respective test sets.

As our primary goal is to assess the usefulness of the ML-L-Shaped algo-
rithm in solving problems of class (P), we implement standard feed-forward
neural network architectures. These generic ML approximators feature a well-
documented performance (in terms of speed and accuracy) and are known to
scale well to wide ranges of input and output lengths and values. Similarly,

13

our hyperparameter search, inclusive of the characteristics of the feed-forward
networks, remains limited in view of our goal of illustrating the implementation
of our approximation methods and probing their potential beneﬁts rather than
extracting maximal predictive accuracy or data-eﬃciency. We employ GPUs for
training/validation and generating predictions. Whereas these operations can
run on CPUs, the gains in performance oﬀered by GPUs make their use prac-
tically indispensable, despite the additional challenges associated, for example,
with the calls from CPU threads occurring at prediction time at Steps 5 and 10
of Algorithm 2.

In view of their heterogeneous ranges, the non-binary inputs of the predictors
for the SSLP families are rescaled in [0, 1] to ensure better numerical condition-
ing during training. In contrast, the transformation applied to the inputs of the
predictors for the SMKP families makes rescaling unnecessary.

We minimize L1 error over the training set with stochastic mini-batch gra-
dient descent equipped with Adam learning rate adaptation and mini-batch size
equal to 128. Weighting inversely proportional to the sample averages of the
output values measured on the training set is applied to the individual L1 errors
of the networks outputting multiple values when calculating the training and
validation errors.

Early stopping is applied based on L1 error (weighted when there are mul-
tiple outputs) over the full validation set with a forgiving patience of at least
several hundred epochs. Networks outputting a single value QML(x) (SSLP and
SMKP families) are equipped with 10 hidden layers of 800 units each. Networks
outputting multiple values (SMKP families) are equipped with 15 hidden layers
of 1,000 units each. All units except those in last hidden and output layers are
ﬁtted with rectiﬁed linear activations. Units in last hidden and output layers
are ﬁtted with linear activations. ML training and validation is performed with
Python PyTorch 1.71 on a single Nvidia RTX 3090 GPU.

Problem family

IP/LP

Input # Hid.
layers
length
10
20
10
20
10
30
10
30
10
30
10
5
15
5
10
5
15
5

SSLPF(10,50,2000)
SSLPF-indx(10,50,2000)
SSLPF(15,45,15)
SSLPF(15,45,150)
SSLPF(15,80,15)
SMKPF(29)
SMKPF(29)
SMKPF(30)
SMKPF(30)
IP, LP: output is solution of integral or relaxed 2nd stage problem.
Abs. rel. error: average absolute relative prediction error made on ML test set.
The test set is same for SSLPF-indx(10,50,2000) and SSLPF(10,50,2000).

IP
IP
IP
IP
IP
IP
LP
IP
LP

units/
hid. layer
800
800
800
800
800
800
1000
800
1000

Output Abs. rel.
error [%]
length
0.87
1
5.31
1
0.23
1
0.12
1
0.40
1
0.071
1
6.64
7
0.072
1
7.41
7

Table 1: Performance of ML predictors

Table 1 reports the characteristics of the ML predictors and their perfor-
mance on their respective test sets. Total of training and validation times is
reported for each predictor in Table 11 of Appendix A. Crucially, the compu-
tational advantage presented by the ML-L-Shaped method hinges on the time
within which a prediction can be generated. For every feed-forward network
considered here, this time is nearly constant and at most equal to a few millisec-

14

onds. The predictive performance appears to be good or excellent. Measured in
absolute relative percentage error, it is better than 1% for models that output
QML(x) and are trained on the most accurate data (see rows with IP label).
A larger error (5.31% vs 0.87%) ensues when performing implicit aggregation
(Larsen et al., 2022). The tasks related to the continuous cuts (14) are harder
(see SMKP rows with LP label), and errors vary in this context between 6%
and 8%.

5 Experimental results

This section compares the performances achieved by the ML-L-Shaped heuristic
and the best performing exact method. The results we report are based on
independently generated instances of the SSLP and SMKP families. Section 6
is dedicated to a comparison between ML-L-Shaped and an alternative heuristic
(PH).

Our comparisons pertain to computing time, optimality gap, number of ﬁrst-
stage nodes, number and duration of integral second-stage problems, number
and duration of continuously-relaxed second-stage problems. We also present
evidence regarding the use of heuristic solutions as warm-start incumbents. All
computations reported in this section are performed on an Intel i9-10980XE
processor using the Java programming language adjoined with the Java version
of CPLEX, version 12.10. ML predictions are called through the Java bindings
of TorchScript and performed on a single Nvidia RTX 3090 GPU.

Tables 2 to 7 detail the results of our experiments for both SSLP and SMKP
families whereas the relevant analyses are split between Sections 5.1 and 5.2.
We open with a number of background remarks. First, the shorthand notations
Std-L and Alt-L stand for respectively the exact standard integer L-shaped
method and the one with alternating cuts. Similarly, ML-Std-L and ML-Alt-L
designate the two variants of the ML-L-Shaped method. Second, for each one
of the SSLP and SMPK families, the reported statistics are calculated from
100 independently sampled instances that have not been previously involved
either in ML or in preliminary experiments or calibrations. This guards against
a well-known source of overoptimism. Third, our results about SSLPF(10, 50,
2000), SSLPF(15, 45, 15), SMKPF(29), SMKPF(30) are aligned with those
reported in Angulo et al. (2016): We ﬁnd out that Alt-L achieves computing
times that are smaller or equal to those of Std-L. Hence, Alt-L is reported in
the tables as the benchmark for any comparison with ML-L-Shaped. Moreover,
the hierarchy of our computing times between SSLPF(10, 50, 2000), SSLPF(15,
45, 15), SMKPF(29), SMKPF(30) is similar to that reported in Angulo et al.
(2016). Fourth, the shift coeﬃcients are as follows: µ = 1.0 in connection with
instances belonging to the SSLP families, µ = 0.98 and ν = 0.95 in connection
with instances belonging to the SMKP families. We encountered one instance of
SMKPF(29) and one instance of SMKPF(30) where these values were too large
to yield a feasible solution. For each of these two instances, we reapplied ML-
L-Shaped with decreasing values of µ and ν, as indicated in Section 2, reaching
solutions with µ = ν = 0.7.

15

5.1 Families of SSLP instances

This section focuses on the results related to the SSLP families. Those are
reported in the ﬁrst ﬁve rows of Tables 2 to 7 and reﬂect the application of
the ML-Std-L variant of ML-L-Shaped. That ML-Std-L dominates ML-Alt-L
on SSLP instances was established by independent, preliminary experiments, as
expected.

Computing times. As evidenced in Table 2, ML-Std-L achieves computing
times far smaller than those required by Alt-L. For instance, the average ra-
tios of computing times over 100 generated instances of families SSLPF(10, 50,
2000) and SSLPF(15, 45, 15) are respectively equal to 0.57% and 8.67% (repre-
senting speed-ups of 175x and 12x). Noticing that the second-stage problems in
family SSLPF(15, 45, 15) are quite simple, leading to an average overall exact
solution time of 5.25 seconds, we present complementary results for the families
SSLPF(15, 45, 150) and SSLPF(15, 80, 15) whose second stages are moder-
ately more complex. The corresponding average ratios of computing times,
respectively equal to 1.57% and 7.30% (representing speed-ups of 64x and 14x),
illustrate how ML-Std-L becomes more advantageous as the complexity of the
second-stage problems increases. As expected, since ML prediction times are
nearly invariant, the ratio of computing times obtained for SSLPF-indx(10, 50,
2000), equal to 0.52%, is nearly equal to that of SSLPF(10, 50, 2000).

Problem family

SSLPF(10,50,2000)

Alt-L

0.05
131.63

Quantiles

0.5
150.73

0.95
186.04

SSLPF-indx(10,50,2000)

131.63

150.73

186.04

SSLPF(15,45,15)

4.28

5.00

7.12

SSLPF(15,45,150)

27.96

34.16

43.93

SSLPF(15,80,15)

36.20

47.24

130.70

SMKPF(29)

151.82

742.89

3996.73

SMKPF(30)

212.54

1459.19

6785.80

Standard error of estimate is reported between parentheses.

ML-L-Shaped

Quantiles
0.5
0.64

0.05
0.52

0.95
2.59

0.46

0.60

2.39

0.37

0.43

0.60

0.39

0.53

0.79

2.32

3.22

13.90

28.02

109.12

510.91

37.91

194.34

935.81

Avg
0.93
(0.08)
0.85
(0.08)
0.45
(0.01)
0.55
(0.01)
4.12
(0.29)
174.55
(18.05)
329.82
(55.97)

Avg
156.06
(2.89)
156.06
(2.89)
5.25
(0.11)
34.96
(0.57)
58.55
(3.15)
1233.74
(123.09)
2132.51
(254.70)

Table 2: Computing times (seconds)

ML-L-Shaped/Alt-L ratio

Quantiles

9.99%

1.42%

7.55%

0.31%

0.42%

8.68%

0.95
1.49%

0.05
0.34%

0.5
0.43%

Avg
0.57%
(0.03%)
0.52%
(0.03%)
8.67%
(0.07%)
1.57%
(0.03%)
7.30%
(0.36%)
4.87% 16.51% 51.22% 19.99%
(1.47%)
3.66% 15.70% 57.38% 19.34%
(1.46%)

6.67% 14.54%

1.51%

1.32%

2.13%

3.60%

First-stage objective values. As evidenced by Table 3, the performance
of ML-Std-L in regard to the values achieved for the objective of ﬁrst stage is
excellent. The average optimality gaps over the instances of families SSLPF(10,
50, 2000), SSLPF(15, 45, 15), SSLPF(15, 45, 150) and SSLPF(15, 80, 15), vary
between nearly zero and 1.94%. In addition, equal to 2.61%, the small ﬁrst-
stage optimality gap yielded by the implicit ML predictor of the value of the
second-stage problems of SSLPF(10, 50, 2000) (see SSLPF-indx(10, 50, 2000))
provides evidence in favor of implicit second-stage predictors when generating
second-stage data based on all scenarios is highly time-consuming. (Recall that
generating SSLPF-indx(10, 50, 2000) requires the solution to the second-stage
problem of a single scenario.)

16

Problem family

SSLPF(10,50,2000)

Alt-L

0.05
-353.85

Quantiles
0.5
-347.14

0.95
-333.17

SSLPF-indx(10,50,2000)

-353.85

-347.14

-333.17

SSLPF(15,45,15)

-313.20

-308.74

-296.62

SSLPF(15,45,150)

-314.16

-306.42

-294.74

SSLPF(15,80,15)

-632.21

-614.67

-584.42

SMKPF(29)

7736.92

8176.1

8567.36

SMKPF(30)

8288.24

8790.53

9160.83

Standard error of estimate is reported between parentheses.

ML-L-Shaped

0.05
-353.85

Quantiles
0.5
-347.14

0.95
-333.17

-348.92

-339.00

-315.25

-313.20

-308.67

-296.48

-314.15

-306.27

-281.51

-630.52

-614.67

-584.42

7736.92

8178.5

8570.25

8228.24

8790.53

9164.16

Avg
-345.58
(0.70)
-336.57
(1.06)
-306.95
(0.60)
-300.01
(3.61)
-612.97
(1.34)
8156.18
(25.58)
8754.73
(28.19)

Avg
-345.60
(0.70)
-345.60
(0.70)
-307.15
(0.59)
-305.89
(0.66)
-613.43
(1.34)
8155.42
(27.60)
8754.33
(28.18)

Optimality gap

Quantiles

0.05

0.95
0.000% 0.000% 0.036%

0.5

0.000% 2.173% 7.850%

0.000% 0.000% 0.608%

0.000% 0.000% 1.021%

0.000% 0.000% 0.538%

0.000% 0.000% 0.050%

0.000% 0.000% 0.027%

Avg
0.006%
(0.003%)
2.609%
(0.242%)
0.064%
(0.019%)
1.943%
(1.150%)
0.075%
(0.018%)
0.009%
(0.002%)
0.005%
(0.001%)

Table 3: First-stage values and optimality gaps

Number of ﬁrst-stage problems. Table 4 indicates that the average ratio of
the numbers of ﬁrst-stage nodes between approximate and exact solutions varies
over the SSLP families between 50% and 76%, thus excluding simpliﬁcation of
the ﬁrst-stage problems as the main source of reduction in computing times
between Alt-L and ML-Std-L.

Problem family

SSLPF(10,50,2000)

Alt-L

0.05
5.51E+02

Quantiles
0.5
6.21E+02

0.95
7.16E+02

SSLPF-indx(10,50,2000)

5.51E+02

6.21E+02

7.16E+02

SSLPF(15,45,15)

1.82E+03

2.05E+03

2.64E+03

SSLPF(15,45,150)

1.83E+03

2.15E+03

2.79E+03

SSLPF(15,80,15)

1.27E+04

1.42E+04

1.73E+04

SMKPF(29)

1.20E+06

6.04E+06

3.12E+07

SMKPF(30)

1.71E+06

1.17E+07

4.47E+07

Standard error of estimate is reported between parentheses.

Avg
6.26E+02
(4.96E+00)
6.26E+02
(4.96E+00)
2.13E+03
(2.62E+01)
2.19E+03
(2.88E+01)
1.44E+04
(1.32E+02)
9.59E+06
(9.18E+05)
1.59E+07
(1.49E+06)

ML-L-Shaped

Quantiles
0.5
4.27E+02

0.95
4.83E+02

0.05
3.85E+02

3.29E+02

4.32E+02

5.14E+02

1.33E+03

1.58E+03

2.03E+03

1.16E+03

1.61E+03

2.04E+03

5.56E+03

6.76E+03

1.45E+04

2.21E+05

8.21E+05

3.91E+06

2.72E+05

1.54E+06

7.39E+06

Avg
4.29E+02
(3.39E+00)
4.27E+02
(4.92E+00)
1.62E+03
(2.30E+01)
1.59E+03
(3.33E+01)
7.29E+03
(2.20E+02)
1.35E+06
(1.43E+05)
2.51E+06
(3.90E+05)

Table 4: Number of ﬁrst-stage nodes

ML-L-Shaped/Alt-L ratio

Quantiles

0.5

0.05

0.95

Avg
64.42% 68.50% 72.60% 68.52%
(0.24%)
56.93% 68.97% 78.33% 68.26%
(0.67%)
68.82% 76.20% 82.52% 75.95%
(0.41%)
64.28% 74.52% 82.02% 72.97%
(1.13%)
40.75% 48.12% 81.04% 49.96%
(1.02%)
4.47% 15.42% 49.92% 19.16%
(1.40%)
3.65% 14.53% 56.20% 18.62%
(1.44%)

Second-stage integral and relaxed problems. Tables 5 and 6 report the
numbers of integral and relaxed second-stage problems and the total times spent
in the latter. Their content reﬂects the parallelism taking place in CPLEX’s
branch-and-cut computations, which entails that the net computing times ap-
pearing in Table 2 cannot be directly inferred from it. Average times per indi-
vidual integral problem can, however, be estimated by dividing the average total
time and the average number of problems (reported in lower and upper panels
of Table 5). Average times per individual relaxed problem can be estimated in
a similar fashion from Table 6.

By construction, Std-L only requires the solution of integral second-stage
problems. Table 5 shows that, when it is applied to the SSLP families, ML-Std-
L must compute at most a few thousand approximate integer L-shaped cuts and
that the latter are generated within a few milliseconds on average, including the
time required to predict the value of second stage with ML. In contrast, Table 6
shows that Alt-L must solve exactly a comparable number of relaxed second-
stage problems at a cost of several milliseconds each. In addition, according to
Table 5, it must solve exactly several integral second-stage problems at a cost
of one or more seconds each.

The latter ﬁndings point to the main source of the reduction in computing

17

time achieved by ML-Std-L in comparison to the fastest exact algorithm avail-
able (Alt-L), when the number of ﬁrst-stage nodes is in the order of thousands
as in the SSLP families: The high speed with which predictions of the value of
the integral second-stage problem are generated with ML is suﬃcient to oﬀset a
larger number of integral second-stage problems in comparison with Alt-L. This
speed is even higher than the speed at which the exact solutions of the relaxed
second-stage problems can be computed by Alt-L.

Problem family

Alt-L

Quantiles
0.5

0.05

0.95

Avg

0.05

0.5

0.95

Avg

0.05

ML-L-Shaped

Quantiles

ML-L-Shaped/Alt-L ratio

Quantiles
0.5

0.95

Avg

SSLPF(10,50,2000)

56.00

65.00

82.90

SSLPF-indx(10,50,2000)

56.00

65.00

82.90

SSLPF(15,45,15)

52.00

63.00

79.95

SSLPF(15,45,150)

57.05

70.00

86.00

SSLPF(15,80,15)

37.10

72.00

89.95

SMKPF(29)

SMKPF(30)

35.00

64.00

177.65

36.15

71.00

238.65

SSLPF(10,50,2000)

26439.20

36704.00

63863.85

SSLPF-indx(10,50,2000)

26439.20

36704.00

63863.85

SSLPF(15,45,15)

269.50

511.50

1434.75

SSLPF(15,45,150)

2140.10

3851.00

7380.75

SSLPF(15,80,15)

2492.65

9765.00

84545.40

SMKPF(29)

323.20

1390.50

13930.55

SMKPF(30)

367.10

1964.50

12432.40

Standard error of estimate is reported between parentheses.

Number of problems
417.00

381.00

455.95

327.10

402.00

467.00

1075.05

1243.50

1601.55

1006.25

1259.00

1582.85

4952.95

6052.00

13922.15

13.00

20.00

33.00

14.00

24.50

42.90

66.73
(0.88)
66.73
(0.88)
64.06
(0.96)
70.29
(0.84)
70.26
(1.48)
77.12
(4.47)
99.65
(8.88)

709.55

772.35

2268.75

2339.10

10086.90

530.25

568.05

643.50

622.00

1845.50

1606.10

Total time spent (milliseconds)
42256.11
(2867.43)
42256.11
(2867.43)
633.63
(34.56)
4097.17
(166.85)
20430.97
(2999.29)
3247.54
(483.22)
4633.48
(1269.85)

1539.75

4882.90

1891.50

5599.50

13.05

13.00

21.00

27.00

54.70

62.95

419.17
(2.73)
401.16
(3.98)
1289.88
(17.59)
1266.55
(24.91)
6641.81
(221.40)
21.54
(0.74)
29.66
(4.76)

632.82
(6.44)
647.42
(7.89)
1876.13
(21.33)
1886.55
(33.91)
5921.83
(131.59)
25.02
(1.26)
35.08
(5.67)

756.01%

733.87%

514.06%

643.20%

621.40%

475.16%

1289.20% 1791.57%

1563.66% 1973.50%

636.07%
(7.14%)
608.65%
(8.06%)
2749.83% 2052.07%
(38.41%)
2556.04% 1828.42%
(42.47%)
6587.86% 8504.23% 18138.67% 9875.65%
(362.88%)
33.66%
(1.51%)
35.43%
(2.10%)

13.09%

31.55%

31.23%

77.69%

62.96%

9.78%

1.02%

1.77%

2.32%

1.09%

1.73%

2.34%

141.95%

357.32%

684.90%

27.87%

48.97%

87.26%

8.76%

57.08%

245.03%

0.19%

1.53%

7.94%

0.28%

1.21%

7.37%

1.69%
(0.04%)
1.73%
(0.04%)
370.39%
(16.43%)
51.92%
(1.82%)
90.41%
(10.39%)
2.33%
(0.22%)
2.30%
(0.23%)

Table 5: Integral second-stage problems

Two-phase variants. We now direct our attention to variants of the solution
process where ML-L-Shaped outputs a feasible, approximate solution that is
used as a warm-start incumbent ﬁrst-stage solution in Alt-L. We report total
computing times in Table 7 along with the average ratio between the two-phase
variants of ML-L-Shaped and Alt-L. Since this average ratio varies between 83%
and 111% in the upper panel of the table, we conclude that the application of
this two-phase mechanism is not advantageous by itself.

The lower panel of the table reports a similarly calculated average ratio when
a probabilistic lower bound on the value of the ﬁrst-stage objective is introduced
in addition in the exact solution process. The joint eﬀect of introducing the
warm-start incumbent solution and the probabilistic lower bound on the average
time ratio is then moderately yet unambiguously advantageous. (Notice that
the application of the probabilistic lower bound alone was shown in a distinct
experiment not to be advantageous.)

5.2 Families of SMKP instances

We turn to the analysis of the results related to the SMKP families. Those
are reported in the rows below those pertaining to SSLP families in Tables 2
to 7. (We do not report results for warm-start only in Table 7 given that the
performance with warm start and probabilistic bound is poor.) We generate

18

Problem family

Alt-L

Quantiles
0.5

0.05

0.95

Avg

0.05

Quantiles
0.5

0.95

Avg

0.05

0.5

0.95

Avg

Quantiles

ML-L-Shaped

ML-L-Shaped/Alt-L ratio

Number of problems

SSLPF(10,50,2000)

364.00

406.00

460.95

SSLPF-indx(10,50,2000)

364.00

406.00

460.95

SSLPF(15,45,15)

933.15

1047.50

1347.20

SSLPF(15,45,150)

924.70

1089.00

1392.25

SSLPF(15,80,15)

5432.45

6234.00

7711.50

SMKPF(29)

SMKPF(30)

55.15

108.50

330.20

55.05

124.00

420.80

408.13
(2.97)
408.13
(2.97)
1084.63
(14.04)
1115.20
(15.40)
6308.44
(65.66)
137.04
(8.79)
174.40
(16.52)

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

17.00

24.00

48.55

18.00

29.00

48.00

SSLPF(10,50,2000)

591539.95

667071.00

Total time spent (milliseconds)
0.00

0.00

749049.35

0.00

SSLPF-indx(10,50,2000)

591539.95

667071.00

749049.35

SSLPF(15,45,15)

22277.30

25686.50

34244.55

SSLPF(15,45,150)

142335.25

172721.50

223728.10

SSLPF(15,80,15)

180629.90

209324.00

250452.65

SMKPF(29)

SMKPF(30)

104.35

238.50

894.25

236.10

535.50

1947.50

Standard error of estimate is reported between parentheses.

664969.61
(5065.41)
664969.61
(5065.41)
26816.72
(480.28)
177046.35
(2736.33)
210419.66
(2140.48)
317.21
(24.37)
783.28
(78.32)

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

41.00

74.50

281.3

45.05

78.50

381.55

0.00
(0.00)
0.00
(0.00)
0.00
(0.00)
0.00
(0.00)
0.00
(0.00)
29.21
(3.60)
34.62
(4.78)

0.00
(0.00)
0.00
(0.00)
0.00
(0.00)
0.00
(0.00)
0.00
(0.00)
103.47
(16.11)
147.17
(35.82)

0.00%

0.00%

0.00%

0.00%

0.00%

0.00%

0.00%

0.00%

0.00%

0.00%

0.00%
(0.00%)
0.00%
(0.00%)
0.00%
(0.00%)
0.00%
(0.00%)
0.00%
(0.00%)
8.11% 22.88% 51.11% 27.95%
(4.17%)
6.68% 21.56% 59.62% 25.59%
(1.64%)

0.00%

0.00%

0.00%

0.00%

0.00%

0.00%

0.00%

0.00%

0.00%

0.00%

0.00%

0.00%

0.00%

0.00%

0.00%

0.00%
(0.00%)
0.00%
(0.00%)
0.00%
(0.00%)
0.00%
(0.00%)
0.00%
(0.00%)
9.78% 31.65% 71.10% 42.08%
(7.65%)
4.30% 14.03% 51.11% 20.38%
(2.30%)

0.00%

0.00%

0.00%

0.00%

0.00%

Table 6: Relaxed second-stage problems

Problem family

Alt-L

Quantiles

Two-phase and bound
Quantiles

(Two-phase and bound)/Alt-L ratio

Quantiles

0.05

0.5

0.95

Avg

0.05

0.5

0.95

Avg

0.05

0.5

0.95

Avg

SSLPF(10,50,2000)

131.63

150.73

186.04

SSLPF-indx(10,50,2000)

131.63

150.73

186.04

SSLPF(15,45,15)

4.28

5.00

7.12

SSLPF(15,45,150)

27.96

34.16

43.93

SSLPF(15,80,15)

36.20

47.24

130.70

With warm start only
127.05

113.54

143.25

125.57

141.74

162.68

4.40

5.04

6.38

32.14

38.46

50.46

13.12

52.88

76.42

SSLPF(10,50,2000)

131.63

150.73

With warm start and probabilistic bound
99.27

130.71

112.97

186.04

SSLPF-indx(10,50,2000)

131.63

150.73

186.04

SSLPF(15,45,15)

4.28

5.00

7.12

SSLPF(15,45,150)

27.96

34.16

43.93

SSLPF(15,80,15)

36.20

47.24

130.70

SMKPF(29)

151.82

742.89

3996.73

SMKPF(30)

212.54

1459.19

6785.80

Standard error of estimate is reported between parentheses.

107.54

125.34

150.39

3.52

4.02

5.31

24.46

29.82

41.39

13.57

30.56

55.99

167.95

859.10

4184.03

251.40

1623.59

7141.88

156.06
(2.89)
156.06
(2.89)
5.25
(0.11)
34.96
(0.57)
58.55
(3.15)

156.06
(2.89)
156.06
(2.89)
5.25
(0.11)
34.96
(0.57)
58.55
(3.15)
1233.74
(123.09)
2132.51
(254.70)

127.84
(1.17)
142.03
(1.43)
5.37
(0.23)
38.78
(0.62)
48.96
(2.49)

113.53
(1.08)
126.95
(1.43)
4.32
(0.15)
30.20
(0.50)
35.29
(1.57)
1305.54
(122.34)
2350.84
(244.89)

91.93%

84.36%

79.88%

72.97%

92.63% 102.30%

82.85%
(0.67%)
92.02%
(0.82%)
86.52% 100.64% 109.28% 101.13%
(1.34%)
99.40% 111.71% 120.48% 111.22%
(0.84%)
94.48%
(6.07%)

98.83% 142.27%

27.15%

95.12%

83.93%

88.53%

74.97%

82.59%

81.24%

60.70%

67.78%

72.82%

78.59%

73.67%
(0.72%)
82.29%
(0.87%)
81.62%
(0.81%)
86.47%
(0.53%)
66.73%
(2.98%)
70.16% 114.83% 159.15% 112.98%
(2.40%)
79.25% 115.25% 152.57% 115.85%
(1.85%)

63.75% 114.51%

18.79%

86.72%

95.08%

Table 7: Total computing times with warm start from approximate solution
(seconds)

19

all results related to SMKP by applying the ML-Alt-L variant of ML-L-Shaped.
That ML-Alt-L dominates ML-Std-L on SMKP instances was established by
independent, preliminary experiments, as expected.

The accuracy of ML-Alt-L reported in Table 3 is excellent. The speed-
ups reported in Table 2 are signiﬁcant but not of the magnitude witnessed in
connection with the SSLP families. The latter is unsurprising in view of the
much simpler second-stage problems featured by the SMKP families. Indeed,
Tables 5 and 6 show in regard to SMKPF(29) and SMKPF(30) that Alt-L solves
the integral and relaxed second-stage problems in only about 50 ms and 5 ms on
average, respectively. In general, large reductions in computing time can take
place in contexts where the integral second-stage problems are suﬃciently hard.
We proceed to illustrate this in the context of the SMKP families.

Provided the two following suﬃcient conditions hold, the diﬀerence in com-
puting times between Alt-L and ML-Alt-L can be made arbitrarily large by
increasing the complexity of the second-stage problems in SMKPF(29) and
SMKPF(30). First, the number of nodes deﬁned by ML-Alt-L and Alt-L must
be similar or the computing times of Alt-L and ML-Alt-L must be similar when
they handle families SMKPF(29) and SMKPF(30). Second, the nearly invari-
ant times required to compute approximate solutions of the integral and relaxed
problems of second stage with ML must be at most in the order of 50 ms and
5 ms, respectively. That the ﬁrst condition holds is illustrated in Table 4. As
regards the second condition, it is shown to hold from calculations made from
Tables 5 and 6. Crucially, to make the solution yielded by ML-Alt-L worthwhile,
we still need to show that it is suﬃciently accurate. Table 3 evidences this.

In general, for families of problems characterized by a diﬃcult ﬁrst stage,
ML-Alt-L becomes increasingly advantageous in comparison with its exact ver-
sion when the diﬃculty of the second-stage problems increases. The gains in
online computing times resulting from ML-Alt-L would become arbitrarily large
as the complexity of the second-stage problems would grow unboundedly. Prac-
tically, this means that the quantiles and averages of the ratios reported in
Table 2 in regard to families SMKPF(29) and SMKPF(30) would decrease as
their numbers of scenarios would grow.
In other words, the speed-up would
increase as the number of scenarios would grow. Clearly, a practical upper limit
on the complexity of the second-stage problems is set by the computation of
the corresponding exact solutions when generating the data required for train-
ing/validating the ML predictors. One particular manner in which this practical
upper limit may be oﬀset is the use of the implicit ML predictor discussed in
Section 3.2.1.

6 Comparison with an alternative heuristic

Whereas Section 5 compares the performance of ML-L-Shaped to that of its
exact counterpart,
it is desirable to also draw a parallel between the per-
formance of ML-L-Shaped and that of a best alternative heuristic algorithm.
As highlighted in Section 1, the PH algorithm (Rockafellar and Wets, 1991,
Watson and Woodruﬀ, 2011) is a strong candidate for this purpose. Tables 8
and 9 compare the performance of the PH algorithm with that of ML-L-Shaped
for each family of problems in classes SSLP and SMKP.

PH computations are performed on the same workstation as that used to

20

produce the results reported in Section 5 and are implemented with version 3.8
of the Python language using the Pyomo 5.7.3 optimization library and its PH
subpackage PySP (Watson et al., 2012). PySP relies on CPLEX 12.10 to solve
the scenario-speciﬁc optimization problems deﬁned by the PH algorithm. Par-
allel computations are handled through MPI by the Mpi4py package. We select
the speciﬁc settings of the PH algorithm in PySP so as to closely match those
demonstrated in Woodruﬀ (2016) to yield a better performance in connection
with problems in the SSLP class. We veriﬁed that the computing times thus
obtained with PH for the original problems SSLP(10, 50, 2000) and SSLP(15,
45, 15) are in the vicinity of those reported in Woodruﬀ (2016).

From Table 8 we conclude that ML-L-Shaped on the one hand and the PH
algorithm on the other hand both approximate the ﬁrst-stage optimal solution
values very well for every family of problems considered in classes SSLP and
SMKP. For example, whereas the ML-Std-L achieves a gap of 0.006% over the
instances of family SSLPF(10, 50, 2000), the corresponding ﬁgure yielded by
the PH algorithm is equal to 0.078%.

Problem family

Quantiles

Quantiles

ML-L-Shaped (From Table 3)

PH algorithm

0.05

0.5

0.95

Avg

0.05

0.5

0.95

Avg

SSLPF(10,50,2000)

0.000

0.000

0.036

SSLPF(15,45,15)

0.000

0.000

0.608

SSLPF(15,45,150)

0.000

0.000

1.021

SSLPF(15,80,15)

0.000

0.000

0.538

SMKPF(29)

0.000

0.000

0.050

SMKPF(30)

0.000

0.000

0.027

0.006
(0.003)
0.064
(0.019)
1.943
(1.150)
0.075
(0.018)

0.009
(0.002)
0.005
(0.001)

0.003

0.036

0.345

0.001

0.001

0.024

0.000

0.033

0.163

0.000

0.021

0.205

0.085

0.211

0.396

0.098

0.216

0.384

0.078
(0.011)
0.005
(0.001)
0.053
(0.006)
0.119
(0.052)

0.223
(0.009)
0.224
(0.008)

Standard error of estimate is reported between parentheses.

Table 8: Comparison of optimality gaps (percentage) between PH and ML-L-
Shaped

From the ﬁgures reported in Table 9, we conclude that online computing
times are signiﬁcantly smaller for ML-L-Shaped than for PH with respect to
instances in the SSLP class. This is particularly manifest when the number of
scenarios is high. For example, in regard to SSLPF(10, 50, 2000), the average
ratio between computing times is 28,242%, representing a 282x speed-up for
ML-L-Shaped. Comparing the average computing times for the PH algorithm
in Table 9 with those of the exact algorithm in Table 2, we note that the former
are strictly greater or nearly equal for all four families in the SSLP class.

In contrast, the computing times achieved by the PH algorithm over the
families in the SMKP class are smaller than those achieved by ML-Alt-L. PH
achieves average ratios of 25.01% and 16.99% representing average speed-ups of
respectively 4x and 6x compared to ML-Alt-L.

However, the computing times of ML-L-Shaped are arguably invariant with
respect to the number of second-stage scenarios whereas the computing times
of the PH algorithm are monotonically increasing. Therefore, online computing
times of ML-L-Shaped will eventually be exceeded by those of the PH algorithm

21

ML-L-Shaped (from Table 2)

Problem family

Quantiles

SSLPF(10,50,2000)

0.05
0.52

0.5
0.64

0.95
2.59

SSLPF(15,45,15)

0.37

0.43

0.60

SSLPF(15,45,150)

0.39

0.53

0.79

SSLPF(15,80,15)

2.32

3.22

13.90

SMKPF(29)

28.02

109.12

510.91

SMKPF(30)

37.91

194.34

935.81

Avg
0.93
(0.08)
0.45
(0.01)
0.55
(0.01)
4.12
(0.29)
174.55
(18.05)
329.82
(55.97)

Standard error of estimate is reported between parentheses.

PH algorithm

Quantiles

0.05
156.43

0.5
181.70

0.95
505.67

17.17

19.56

44.05

27.72

31.31

55.98

19.54

29.20

188.65

17.58

20.27

26.80

17.84

20.18

30.12

Avg
225.44
(12.29)
24.80
(2.26)
36.69
(2.08)
48.88
(4.59)
21.17
(0.33)
22.49
(1.00)

PH/ML-L-Shaped ratio

Quantiles
0.5

0.95

0.05

7559%

3416%

3956%

4759%

6570% 10548%

Avg
13120% 28174% 49685% 28242%
(1024%)
5305%
(340%)
7271%
(569%)
1338%
(129%)
25.01%
(2.77%)
16.99%
(1.89%)

3.81% 18.94% 68.63%

2.34% 11.08% 57.72%

5376%

464%

948%

Table 9: Comparison of computing times (seconds) between PH and ML-L-
Shaped

when the number of scenarios occurring in families of the SMKP class reaches
a suﬃcient magnitude. Still, it must be ascertained that this threshold does
not have a size so large that it is rendered practically irrelevant. To gain useful
evidence about this, we assessed the impact on computing times with the PH
algorithm of increasing to 2,000 the number of scenarios in SMKPF(29) and
SMKPF(30) from their base value of 20. In regard to SMKPF(29) the average
computing times rises from 21.17 to 484.57 seconds, and for SMKPF(30) it rises
from 22.49 to 372.60 seconds. (Table 12 in Appendix B reports detailed results.)
It thus appears that the critical number of scenarios for which the computing
time associated with the PH algorithm exceeds that of ML-L-Shaped would be
in the order of a few thousand. This number is suﬃciently small to be practically
consequential.

7 Conclusion

We proposed ML-L-Shaped, a matheuristic leveraging the strong capabilities
of generic ML approximators to accelerate the online solution of mixed-integer
linear two-stage stochastic programming problems. We aimed to solve problems
where the second stage is highly demanding computationally. Our core idea was
to substitute the solutions of the latter stages of Benders decomposition with
fast, yet accurate predictions arising from supervised ML. We veriﬁed the prac-
tical usefulness of ML-L-Shaped by applying it to problems seminally addressed
in Laporte and Louveaux (1993). We conducted an extensive empirical analy-
sis grounded in families of problems derived from the SSLP and SMKP classes
examined in Angulo et al. (2016). The SSLP instances featured relatively hard
second-stage problems and hence correspond to the type of problems for which
ML-L-Shaped is designed.

Our results appear to substantiate convincingly the usefulness of ML-L-
Shaped: compared to an exact approach, predicting the second-stage solutions
with ML can bring about large reductions in online computing time while sac-
riﬁcing small losses in ﬁrst-stage solution quality. We reported optimality gaps
close to zero and average speed-ups between 5x and 192x. We also compared
the online performance with that of the PH algorithm. The results showed
that ML-L-Shaped and PH reach solutions of comparable quality. The comput-
ing times achieved by ML-L-Shaped over the SSLP instances are considerably
smaller than those yielded by PH. In contrast, PH achieved smaller comput-

22

ing times over the SMKP instances. We showed, however, that ML-L-Shaped,
whose computing times are invariant with respect to the number of scenarios,
would display an advantage when the latter is large.

We remark that the PH algorithm could in principle also be equipped with
an ML predictor. This predictor would return heuristic solution values at a
high speed for the scenario-speciﬁc integer programming problems that must
currently be solved repeatedly with an exact solver. Since the PH algorithm
must obtain one solution per scenario instead of one expected solution over all
scenarios as in Std-L and Alt-L, the computing times of the resulting hybrid PH-
ML algorithm could not be invariant with respect to the number of scenarios.
However, the relation between number of scenarios and computing time could be
made considerably ﬂatter in comparison with the original PH algorithm through
the introduction of the high-speed ML-predictions.

An important direction for future research consists in drawing the eﬃcient
frontier between (i) the total time required by data generation and ML learning,
(ii) the accuracy of the resulting heuristic ﬁrst-stage solutions and (iii) the time
required for their computation. This endeavour would require exploring the
outcomes of broad ranges of approximators, hyperparameter settings and data
set sizes. In this context, it would also seem opportune to explore predict-and-
optimize approaches performing joint optimization of the ML predictor and the
ﬁrst-stage solution of problem (P).

Finally, we note that the idea of introducing approximate substitutes for the
second-stage solutions of two-stage decompositions is also applicable in princi-
ple to the latter stages of multi-stage decompositions. ML predictions for the
solution of a given stage would then be made conditional on coupling variables
originating from previous stages and on free parameters in the current stage.

Acknowledgments

This research was funded by the Canadian National Railway Company (CN)
Chair in Optimization of Railway Operations at Université de Montréal and
a Collaborative Research and Development Grant from the Natural Sciences
and Engineering Research Council of Canada (CRD-477938-14). Computations
were made on the supercomputer Béluga, managed by Calcul Québec and Com-
pute Canada. The operation of this supercomputer is funded by the Canada
Foundation for Innovation (CFI), the Ministère de l’Économie, de la Science
et de l’Innovation du Québec (MESI) and the Fonds de recherche du Québec
- Nature et technologies (FRQ-NT). The research was also partially funded by
the “IVADO Fundamental Research Project Grants” under the project entitled
“Machine Learning for (Discrete) Optimization”.

References

Ahmed, S., Garcia, R., Kong, N., Ntaimo, L., Parija, G., Qiu, F., and Sen, S.
SIPLIB: A stochastic integer programming test problem library, 2015. URL
https://www2.isye.gatech.edu/~sahmed/siplib.

Amaran, S., Sahinidis, N. V., Sharda, B., and Bury, S. J. Simulation optimiza-

23

tion: a review of algorithms and applications. Annals of Operations Research,
240(1):351–380, 2016.

Angulo, G., Ahmed, S., and Dey, S. S. Improving the integer L-shaped method.

INFORMS Journal on Computing, 28(3):483–499, 2016.

Barton, R. R. Tutorial: Metamodeling for simulation. In 2020 Winter Simula-

tion Conference (WSC), pages 1102–1116, 2020.

Barton, R. R. and Meckesheimer, M. Chapter 18 metamodel-based simulation
optimization.
In Henderson, S. G. and Nelson, B. L., editors, Simulation,
volume 13 of Handbooks in Operations Research and Management Science,
pages 535–574. Elsevier, 2006.

Bengio, Y., Frejinger, E., Lodi, A., Patel, R., and Sankaranarayanan, S.
A learning-based algorithm to quickly compute good primal solutions for
stochastic integer programs. In Hebrard, E. and Musliu, N., editors, Inte-
gration of Constraint Programming, Artiﬁcial Intelligence, and Operations
Research, pages 99–111, Cham, 2020. Springer International Publishing.

Bengio, Y., Lodi, A., and Prouvost, A. Machine learning for combinatorial opti-
mization: A methodological tour d’horizon. European Journal of Operational
Research, 290(2):405–421, 2021.

Birge, J. R. and Louveaux, F. Introduction to Stochastic Programming. Springer

Publishing Company, 2nd edition, 2011.

Cappart, Q., Moisan, T., Rousseau, L., Prémont-Schwarz, I., and Ciré, A. A.
Combining reinforcement learning and constraint programming for combina-
torial optimization. ArXiv e-prints arXiv:2006.01610, 2020.

Cappart, Q., Chételat, D., Khalil, E. B., Lodi, A., Morris, C., and Velickovic,
P. Combinatorial optimization and reasoning with graph neural networks.
ArXiv e-prints arXiv:2102.09544, 2021.

Carøe, C. C. and Schultz, R. Dual decomposition in stochastic integer program-

ming. Operations Research Letters, 24(1):37–45, 1999.

Dai, H., Xue, Y., Syed, Z., Schuurmans, D., and Dai, B. Neural stochastic dual

dynamic programming. ArXiv e-prints arXiv:2112.00874, 2021.

Dumouchelle, J., Patel, R., Khalil, E. B., and Bodur, M. Neur2sp: Neural
two-stage stochastic programming. ArXiv e-prints arXiv:2205.12006, 2022.

Elmachtoub, A. N. and Grigas, P. Smart “predict, then optimize”. Management

Science, 68(1):9–26, 2022.

Fioretto,

F.

Integrating

machine

timization
https://web.ecs.syr.edu/~ffiorett/files/papers/Fioretto-IJCAI22-EC.pdf.

making,

decision

boost

to

learning
2022.

and

op-
URL

Fischetti, M. and Jo, J. Deep neural networks and mixed integer linear opti-

mization. Constraints, 23:296–309, 2018.

24

Frejinger, E. and Larsen, E. A language processing algorithm for predicting tac-
tical solutions to an operational planning problem under uncertainty. ArXiv
e-prints arXiv:1910.08216, 2019.

Kotary, J., Fioretto, F., Hentenryck, P. V., and Wilder, B. End-to-end con-
strained optimization learning: A survey. ArXiv e-prints arXiv:2103.16378,
2021a.

Kotary, J., Fioretto, F., and Van Hentenryck, P. Learning hard optimization
problems: A data generation perspective. ArXiv e-prints arXiv:2106.02601,
2021b.

Laporte, G. and Louveaux, F. V. The integer l-shaped method for stochastic
integer programs with complete recourse. Operations Research Letters, 13(3):
133–142, 1993.

Larsen, E., Lachapelle, S., Bengio, Y., Frejinger, E., Lacoste-Julien, S., and
Lodi, A. Predicting tactical solutions to operational planning problems un-
der imperfect information. INFORMS Journal on Computing, 34(1):227–242,
2022.

Mazyavkina, N., Sviridov, S., Ivanov, S., and Burnaev, E. Reinforcement learn-
ing for combinatorial optimization: A survey. Computers & Operations Re-
search, 134:105400, 2021.

Nair, V., Dvijotham, D., Dunning, I., and Vinyals, O. Learning fast optimiz-
ers for contextual stochastic integer programs. In Proceedings of the Thirty-
Fourth Conference on Uncertainty in Artiﬁcial Intelligence, UAI 2018, Mon-
terey, California, USA, August 6-10, 2018, pages 591–600, 2018.

Ngo, H. Q.

Tail

and concentration inequalities,

2011.

URL

http://www.cse.buffalo.edu/~hungngo/classes/2011/Spring-694/lectures/l4.pdf.

Ntaimo, L. and Sen, S. The million-variable “march” for stochastic combinatorial

optimization. Journal of Global Optimization, 32(3):385–400, 2005.

Peng, Y., Choi, B., and Xu, J. Graph learning for combinatorial optimization:
A survey of state-of-the-art. Data Science and Engineering, 6(2):119–141,
2021.

Rockafellar, R. T. and Wets, R. J.-B. Scenarios and policy aggregation in
optimization under uncertainty. Mathematics of Operations Research, 16(1):
119–147, 1991.

Schuetz, M. J. A., Brubaker, J. K., and Katzgraber, H. G. Combinatorial
optimization with physics-inspired graph neural networks. Nature Machine
Intelligence, 4(4):367–377, 2022.

Sinha, A., Malo, P., and Deb, K. Solving optimistic bilevel programs by it-
In 2016 IEEE

eratively approximating lower level optimal value function.
Congress on Evolutionary Computation (CEC), pages 1877–1884, 2016.

Sinha, A., Lu, Z., Deb, K., and Malo, P. Bilevel optimization based on iterative
approximation of multiple mappings. J. Heuristics, 26(2):151–185, 2020.

25

Torres, J. J., Li, C., Apap, R. M., and Grossmann, I. E. A review on the
performance of linear and mixed integer two-stage stochastic programming
software. Algorithms, 15(4), 2022.

Vesselinova, N., Steinert, R., Perez-Ramirez, D. F., and Boman, M. Learning
combinatorial optimization on graphs: A survey with applications to network-
ing. IEEE Access, 8:120388–120416, 2020.

Wang, Q. and Tang, C. Deep reinforcement learning for transportation network
combinatorial optimization: A survey. Knowledge-Based Systems, 233:107526,
2021.

Watson, J.-P. and Woodruﬀ, D. L. Progressive hedging innovations for a class of
stochastic mixed-integer resource allocation problems. Computational Man-
agement Science, 8(4):355–370, 2011.

Watson, J.-P., Woodruﬀ, D. L., and Hart, W. PySP: modeling and solving
stochastic programs in Python. Mathematical Programming Computation, 4
(2):109–149, 2012.

Woodruﬀ, D.

for
https://github.com/DLWoodruff/sslp-pysp/blob/master/sslpfortime.pdf.

Wall-clock

SSLP,

2016.

URL

time

the

Zhang, B., Ong, Y. J., and Nakamura, T. SimPO: Simultaneous prediction and

optimization. ArXiv e-prints arXiv:2204.00062, 2022.

26

Appendix A: Data generation and ML times

IP/LP Avg time per example

Problem family
SSLPF(10,50,2000)
SSLPF-indx(10,50,2000)
SSLPF(15,45,15)
SSLPF(15,45,150)
SSLPF(15,80,15)
SMKPF(29)
SMKPF(29)
SMKPF(30)
SMKPF(30)
IP, LP: output is solution of integral or relaxed 2nd stage problem.

3.24
0.0405
0.0493
0.151
0.582
0.129
0.0188
0.0743
0.0269

IP
IP
IP
IP
IP
IP
LP
IP
LP

Table 10: Data generation times (seconds)

IP/LP Training + Validation time

Problem family
SSLPF(10,50,2000)
SSLPF-indx(10,50,2000)
SSLPF(15,45,15)
SSLPF(15,45,150)
SSLPF(15,80,15)
SMKPF(29)
SMKPF(29)
SMKPF(30)
SMKPF(30)
IP, LP: output is solution of integral or relaxed 2nd stage problem.

25.17
4.185
24.45
33.59
32.19
28.78
39.69
23.62
40.95

IP
IP
IP
IP
IP
IP
LP
IP
LP

Table 11: ML times (hours)

27

Appendix B: PH times with additional scenarios

Problem family

Quantiles
0.5

0.05

0.95

Avg

20.27

17.58

26.80

300.83

187.67

SMKPF(29)
(20 scen.)
SMKPF(29)
(2000 scen.)
SMKPF(30)
(20 scen.)
SMKPF(30)
(2000 scen.)
Standard error of estimate is reported between parentheses.

21.17
(0.33)
484.57
(98.27)
22.49
(1.00)
372.60
(80.58)

232.61

586.05

678.51

177.11

30.12

20.18

17.84

Table 12: PH times with additional scenarios (seconds)

28

