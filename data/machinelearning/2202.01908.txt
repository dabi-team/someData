2
2
0
2

t
c
O
5
1

]

G
L
.
s
c
[

2
v
8
0
9
1
0
.
2
0
2
2
:
v
i
X
r
a

Sampling with Riemannian Hamiltonian Monte Carlo
in a Constrained Space

Yunbum Kook
Georgia Tech
yb.kook@gatech.edu

Yin Tat Lee
Microsoft Research,
University of Washington
yintat@uw.edu

Ruoqi Shen
University of Washington
shenr3@cs.washington.edu

Santosh S. Vempala
Georgia Tech
vempala@gatech.edu

Abstract

We demonstrate for the ﬁrst time that ill-conditioned, non-smooth, constrained dis-
tributions in very high dimension, upwards of 100,000, can be sampled efﬁciently
in practice. Our algorithm incorporates constraints into the Riemannian version
of Hamiltonian Monte Carlo and maintains sparsity. This allows us to achieve a
mixing rate independent of condition numbers.

On benchmark data sets from systems biology and linear programming, our al-
gorithm outperforms existing packages by orders of magnitude. In particular, we
achieve a 1,000-fold speed-up for sampling from the largest published human
metabolic network (RECON3D). Our package has been incorporated into the
COBRA toolbox.

1

Introduction

Sampling is Fundamental. Sampling algorithms arise naturally in models of statistical physics,
e.g., Ising, Potts models for magnetism, Gibbs model for gases, etc. These models directly suggest
Markov chain algorithms for sampling the corresponding conﬁgurations. In the Ising model where
the vertices of a graph are assigned a spin, i.e., ±1, in each step, we pick a vertex at random and ﬂip
its spin with some probability. The probability is chosen so that the distribution of the vector of all
spins approaches a target distribution where the probability exponentially decays with the number
of agreements in spin for pairs corresponding to edges of the graph. In the Gibbs model, particles
move randomly with collisions and their motion is often modeled as reﬂecting Brownian motion.
Sampling with Markov chains is today the primary algorithmic approach for high-dimensional
sampling. For some fundamental problems, sampling with Markov chains is the only known efﬁcient
approach or the only approach to have guarantees of efﬁciency. Two notable examples are sampling
perfect matchings of a bipartite graph and sampling points from a convex body. These are the core
subroutines for estimating the permanent of a nonnegative matrix and estimating the volume of
a convex body, respectively. The solution space for these problems scales exponentially with the
dimension. In spite of this, polynomial-time algorithms have been discovered for both problems. The
current best permanent algorithm scales as n7 (time) [2, 23], while the current best volume algorithm
scales as n3 (number of membership tests) [24]. For the latter, the ﬁrst polynomial-time algorithm
had a complexity of n27 [16], and the current best complexity is the result of many breakthrough
discoveries, including general-purpose algorithms and analysis tools.

36th Conference on Neural Information Processing Systems (NeurIPS 2022).

 
 
 
 
 
 
Sampling is Ubiquitous. The need for efﬁcient high-dimensional sampling arises in many ﬁelds.
A notable setting is metabolic networks in systems biology. A constraint-based model of a metabolic
network consists of m metabolites and n reactions, and a set of equalities and inequalities that deﬁne
a set of feasible steady state reaction rates (ﬂuxes):

Ω = (cid:8)v ∈ Rn | Sv = 0, l ≤ v ≤ u, cT v = α(cid:9) ,

where S is a stoichiometric matrix with coefﬁcients for each metabolite and reaction. The linear
equalities ensure that the ﬂuxes into and out of every node are balanced. The inequalities arise from
thermodynamical and environmental constraints. Sampling constraint-based models is a powerful tool
for evaluating the metabolic capabilities of biochemical networks [33, 46]. While the most common
distribution used is uniform over the feasible region, researchers have also argued for sampling from
the Gaussian density restricted to the feasible region; the latter has the advantage that the feasible set
does not have to be bounded. A previous approach to sampling, using hit-and-run with rounding [20],
has been incorporated into the COBRA package [21] for metabolic systems analysis (Bioinformatics).

A second example of mathematical interest is the problem of computing the volume of the Birkhoff
polytope. For a given dimension n, the Birkhoff polytope is the set of all doubly stochastic n × n
matrices (or the convex hull of all permutation matrices). This object plays a prominent role in alge-
braic geometry, probability, and other ﬁelds. Computing its volume has been pursued using algebraic
representations; however exact computations become intractable even for n = 11, requiring years
of computation time. Hit-and-run has been used to show that sampling-based volume computation
can go to higher dimension [11], with small error of estimation. However, with existing sampling
implementations, going beyond n = 20 seems prohibitively expensive.

A third example is from machine learning, a ﬁeld that is increasingly turning to sampling models
of data according to their performance in some objective. One such commonly used criterion is
the logistic regression function. The popularity of logistic regression has led to sampling being
incorporated into widely used packages such as STAN [44], PyMC3 [41], and Pyro [3]. However,
those packages in general do not run on the constraint-based models we are interested in.

Problem Description.
densities are of the form

In this paper, we consider the problem of sampling from distributions whose

e−f (x) subject to Ax = b, x ∈ K
where f is a convex function and K is a convex body. We assume that a self-concordant barrier φ
for K is given. Note that any convex body has a self-concordant barrier [32] and there are explicit
barriers for convex bodies that come up in practical applications [37], so this is a mild assumption.
We introduce an efﬁcient algorithm for the problem when K is a product of convex bodies Ki, each
with small dimension. Many practical instances can be written in this form. As a special case, the
algorithm can handle K in the form of {x ∈ Rn : li ≤ xi ≤ ui for all i ∈ [n]} with li ∈ R ∪ {−∞}
and ui ∈ R ∪ {+∞}, which is the common model structure in systems biology. Moreover, any
generalized linear model exp(− (cid:80) fi(a(cid:62)
i x − bi)), e.g., the logistic model, can be rewritten in the
form

(1.1)

(cid:88)

exp(−

ti) subject to Ax = b + s, (s, t) ∈ K

(1.2)

where K = ΠKi and each Ki = {(si, ti) : fi(si) ≤ ti} is a two-dimensional convex body.

The Challenges of Practical Sampling. High dimensional sampling has been widely studied in
both the theoretical computer science and the statistics communities. Many popular samplers are
ﬁrst-order methods, such as MALA [40], basic HMC [36, 14] and NUTS [22], which update the
Markov chain based on the gradient information of f . The runtime of such methods can depend
on the condition number of the function f [15, 30, 7, 8, 42]. However, the condition number of
real-world applications can be very large. For example, RECON1 [27], a reconstruction of the human
metabolic network, can have condition number as large as 106 due to the dramatically different orders
of different chemicals’ concentrations. Motivated by sampling from ill-conditioned distributions,
another class of samplers use higher-order information such as Hessian of f to take into account
the local structure of the problems [43, 9]. However, such samplers cannot handle non-smooth
distributions, such as hinge-loss, lasso, or uniform densities over polytopes.

For non-smooth distributions, the best polytime methods are based on discretizations of Brownian
motion, e.g., the Ball walk [25] (and its afﬁne-invariant cousin, the Dikin walk [26]), which takes a

2

random step in a ball of a ﬁxed size around the current point. Hit-and-Run [34] builds on these by
avoiding an explicit step size and going to a random point along a random line through the current
point. Both approaches hit the same bottleneck — in a polytope that contains a unit ball, the step size
should be O(1/
n) to avoid stepping out of the body with large probability. This leads to quadratic
bounds (in dimension) on the number of steps to “mix”.

√

Due to the reduction mentioned in (1.2), non-smooth distributions can be translated to the form in (1.1)
with constraint K. Both the ﬁrst and higher-order sampler and the polytime non-smooth samplers
have their limitations in handling distributions with non-smooth objective function or constraint K.
Given the limitations of all previous samplers, a natural question we want to ask is the following.

Question. Can we develop a practically efﬁcient sampler that can handle the constrained problem
in (1.1) and preserve sparsity1 with mixing time independent of the condition number?

In some applications, smoothness and condition number can be controlled with tailor-made models.
Our goal here is to propose a general solver that can sample from any non-smooth distributions as
given. For traditional samplers such as the Ball walk and Hit-and-Run, as mentioned earlier, the
step size needs to be small so that the process does not step out. An approach that gets around this
bottleneck is Hamiltonian Monte Carlo (HMC), where the next step is given by a point along a
Hamiltonian-preserving curve according to a suitably chosen Hamiltonian. It has two advantages.
First, the steps are no longer straight lines in Euclidean space, and we no longer have the concern of
“stepping out”. Second, the process is symplectic (so measure-preserving), and hence the ﬁltering step
is easy to compute. It was shown in [31] that signiﬁcantly longer steps can be taken and the process
with a convergence analysis in the setting of Hessian manifolds, leading to subquadratic convergence
for uniformly sampling polytopes.

To make this practical, however, is a formidable challenge. There are two high-level difﬁculties. One
is that many real-world instances are highly skewed (far from isotropic) and hence it is important
to use the local geometry of the density function. This means efﬁciently computing or maintaining
second-order information such as a Hessian of the logarithm of the density. This can be done in
the Riemannian HMC (RHMC) framework [17, 31], but the computation of the next step requires
solving the Hamiltonian ODE to high accuracy, which in turn needs the computation of leverage
scores, a procedure that takes at least matrix-multiplication time in the worst case. Another important
difﬁculty is maintaining hard linear constraints. Existing high-dimensional packages do not allow
for constraints (they must be somehow incorporated into the target density), and RHMC is usually
considered with a full-dimensional feasible region such as a full-dimensional polytope. This can also
be done in the presence of linear equalities by working in the afﬁne subspace deﬁned by the equalities,
but this has the effect of losing any sparsity inherent in the problem and turning all coefﬁcient matrices
and objective coefﬁcients into dense objects, thereby potentially incurring a quadratic blow-up.

Our Solution: Constrained Riemannian Hamiltonian Monte Carlo (CRHMC). We develop a
constrained version of RHMC, maintaining both sparsity and constraints. Our reﬁnement of RHMC
ensures that the process satisﬁes the given constraints throughout, without incurring a signiﬁcant
overhead in time or sparsity. It works even if the resulting feasible region is poorly conditioned. Since
many instances in practice are ill-conditioned and have degeneracies, we believe this is a crucial
aspect. Our algorithm outperforms existing packages by orders of magnitude.

In Section 2, we give the main ingredients of the algorithm and discuss how we overcome the
challenges that prevent us from sampling efﬁciently in practice. Following that, in Section 3, we
present empirical results on several benchmark datasets, showing that CRHMC successfully samples
much larger models than previously known to be possible, and is signiﬁcantly faster in terms of rate
of convergence (“number of steps”) and total sampling time. Our complete package is available on
GitHub. We refer the reader to Appendix for theory, notations, and deﬁnitions.

1When A is sparse, preserving the sparsity of A can greatly enhance both the runtime and the space efﬁciency.

3

2 Algorithm: Constrained RHMC

In this section, we propose a constrained Riemannian Hamiltonian Monte Carlo (CRHMC2) algorithm
to sample from a distributions of the form

e−f (x) subject to c(x) = 0 and x ∈ K for some convex body K,
where the constraint function c : Rn → Rm satisﬁes the property that the Jacobian Dc(x) has full
rank for all x such that c(x) = 0. It is useful to keep in mind the case when c(x) = 0 is an afﬁne
subspace Ax = b, in which case Dc(x) = A, and the full-rank condition simply says that the rows of
A are independent.

We refer readers to [1, 4, 39] for preliminary versions of CRHMC called the constrained Hamiltonian
Monte Carlo (CHMC). In particular, a framework in [4] can be extended to CRHMC when K = Rn,
and in fact they mention CRHMC as a possible variant. However, their algorithm for CRHMC
requires eigenvalue decomposition and is not efﬁcient for large problems, which takes n3 time and
n2 space per MCMC step in practice. In this section, we propose an algorithm that overcomes those
limitations and satisﬁes the additional constraint K by using a local metric induced by the Hessian of
self-concordant barriers, leading to n1.5 time and n space in practice.

2.1 Basics of CRHMC

To introduce our algorithm, we ﬁrst recall the RHMC algorithm (Algorithm 1). In RHMC, we extend
the space x to the pair (x, v), where v denotes the velocity. Instead of sampling from e−f (x), RHMC
samples from the distribution e−H(x,v), where H(x, v) is the Hamiltonian, and then outputs x. To
make sure the distribution is correct, we choose the Hamiltonian such that the marginal of e−H(x,v)
along v is proportional to e−f (x). One common choice of H(x, v) is

H(x, v) = f (x) +

1
2

v(cid:62)M (x)−1v +

1
2

log det M (x),

(2.1)

where M (x) is a position-dependent positive deﬁnite matrix deﬁned on Rn.

Algorithm 1: Riemannian Hamiltonian Monte Carlo (RHMC)
Input: Initial point x(0), step size h
for k = 1, 2, · · · do

// Step 1: resample v
Sample v(k− 1

2 ) ∼ N (0, M (x(k−1))) and set x(k− 1

2 ) ← x(k−1).

// Step 2: Hamiltonian dynamics
Solve the ODE

∂H(x, v)
∂x
with H deﬁned in (2.1) and the initial point given by (x(k− 1
Set x(k) ← x(h) and v(k) ← v(h).

∂H(x, v)
∂v

dx
dt

dv
dt

= −

=

,

2 ), v(k− 1

2 )).

(2.2)

end
Output: x(k)

To extend RHMC to the constrained case, we need to make sure both Step 1 and Step 2 satisfy the
constraints, so the Hamiltonian dynamics has to maintain c(x) = 0 throughout Step 2. Note that

d
dt

c(xt) = Dc(xt) ·

dxt
dt

= Dc(xt) ·

∂H(xt, vt)
∂vt

,

(2.3)

where Dc(x) is the Jacobian of c at x. With H deﬁned in (2.1), Condition (2.3) be-
comes Dc(x)M (x)−1v = 0.
if M (x) is invertible,
then Range(v) = Range(N (0, M (x))) = Rn immediately violates this condition due to

for full rank Dc(x),

However,

2pronounced “crumch”.

4

dim(Null(Dc(x)M −1(x))) = n − m. To get around this issue, we use a non-invertible matrix
M (x) with its pseudo-inverse M (x)† to satisfy Dc(x)M (x)†v = 0 for any v ∈ Range(M (x)).
Since we want the step to be able to move in all directions satisfying c(x) = 0, we impose the
following condition with Range(M (x)) = Range(M (x)†) in mind:

Range(M (x)) = Null(Dc(x)) for all x ∈ Rn,

(2.4)

which can be achieved by M (x) proposed soon.

Under the condition (2.4), we sample v from N (0, M (x)) in Step 1, which is equivalent to sampling
from e−H(x,v) subject to v ∈ Range(M (x)) = Null(Dc(x)). Also, the stationary distribution of
CRHMC should be proportional to

e−H(x,v) subject to c(x) = 0 and v ∈ Null(Dc(x)).
Here, to maintain v ∈ Null(Dc(x)) during Step 2 we add a Lagrangian term to H. Without the
Lagrangian term, vt would escape from Null(Dc(xt)) = Range(M (xt)) in Step 2 as seen in the
proof of Lemma 1, which contradicts Range(vt) = Range(N (0, M (xt))) = Range(M (xt)). The
constrained Hamiltonian we propose is (See its rigorous derivation in Lemma 1)

H(x, v) = H(x, v) + λ(x, v)(cid:62)c(x) with H(x, v) = f (x) +

v(cid:62)M (x)†v + log pdet(M (x))
(2.5)
where λ(x, v) = (Dc(x)Dc(x)(cid:62))−1 (cid:16)
D2c(x)[v, dx
. Here, pdet denotes
pseudo-determinant and λ(x, v) is picked so that v ∈ Null(Dc(x)). An algorithmic description of
CRHMC is the same as Algorithm 1 with the constrained H in place of the unconstrained H. We
show the convergence of CRHMC to the correct distribution exp(−f (x)) in Appendix B.3.

dt ] − Dc(x) ∂H(x,v)

∂x

(cid:17)

1
2

Choice of M via Self-concordant Barriers. The construction of the Hamiltonian (2.5) relies
on having a family of positive semi-deﬁnite matrix M (x) satisfying the condition (2.4) (i.e.,
Range(M (x)) = Null(Dc(x))). One natural choice is the orthogonal projection to Null(Dc(x)):
Q(x) = I − Dc(x)(cid:62)(Dc(x)Dc(x)(cid:62))−1Dc(x),

(2.6)

which is similar to the choice in [4].

For the problem we care about, there are additional constraints on x other than {c(x) = 0}. In
the standard HMC algorithm, we have dx
dt ∼ N (0, M (x)−1). For example, for a simple constraint
K = [0, 1], to ensure every direction is moving towards/away from x = 0 multiplicatively, a natural
choice of M is M (x) = diag(x−2). For general convex body K, we can use a self-concordant
barrier, a function deﬁned on K such that φ(x) is self-concordant and φ(x) → +∞ as x → ∂K.
Using the barrier φ, we can deﬁne the local metric based on g(x) = ∇2φ(x). Intuitively, as the
sampler approaches ∂K, the local metric stretches accordingly so that the Hamiltonian dynamics
never passes the barrier, respecting x ∈ K throughout.

In summary, we need M (x) to have its range match the null space of Dc(x) and agree with g(x) in
its range. We can verify that M (x) = Q(x)(cid:62)g(x)Q(x), where Q(x) is the symmetric matrix deﬁned
in (2.6), satisﬁes these two constraints.

2.2 Efﬁcient Computation of ∂H/∂x and ∂H/∂v

With M (x) = Q(x)(cid:62)g(x)Q(x), we have all the pieces of the algorithm. However, using this naive
algorithm to compute ∂H/∂x and ∂H/∂v, we face several challenges.

1. The algorithm involves computing the pseudo-inverse and its derivatives, which takes O(n3)

except for very special matrices.

2. The Lagrangian term in the constrained Hamiltonian dynamics requires additional computation

such as the second-order derivative of c(x).

3. A naive approach to computing leverage scores in ∂H/∂x results in a very dense matrix.

Those challenges make the algorithm hard to implement and inefﬁcient, especially when the dimension
is high. In the following paragraphs, we give an overview of how we overcome each of the challenges
above. We defer a more detailed discussion of our approaches and the proofs to Appendix B.2.

5

Avoiding Pseudo-inverse and Pseudo-determinant. We are able to show equivalent formulas for
M (x)† and log pdetM (x) that can take advantage of sparse linear system solvers. In particular, we
show that M (x)† = g(x)− 1

2 · (I − P (x)) · g(x)− 1

2 , where

P (x) = g(x)− 1

2 · Dc(x)(cid:62)(Dc(x) · g(x)−1 · Dc(x)(cid:62))−1Dc(x) · g(x)− 1
2 .

(2.7)

As mentioned earlier, a majority of convex bodies appearing in practice are of the form K = (cid:81)
i Ki,
where Ki are constant dimensional convex bodies. In this case, we will choose g(x) to be a block
diagonal matrix with each block of size O(1). Hence, the bottleneck of applying P (x) to a vector is
simply solving a linear system of the form (Dc · g−1 · Dc(cid:62))u = b for some b. The existing sparse
linear system solvers can solve large classes of sparse linear system much faster than O(n3) time
[13]. For log pdetM (x), we show
log pdet(M (x)) = log det g(x)+log det (cid:0)Dc(x) · g(x)−1 · Dc(x)(cid:62)(cid:1)−log det (cid:0)Dc(x) · Dc(x)(cid:62)(cid:1) .
(2.8)
This simpliﬁcation allows us to take advantage of sparse Cholesky decomposition. We prove (2.7)
and (2.8) in Lemma 2 and Lemma 3 in Appendix B.2.1. The formulas (2.7) and (2.8) avoid the
expensive pseudo-inverse and pseudo-determinant computations, and signiﬁcantly improve the
practical performance of our algorithm.

Simpliﬁcation for Subspace Constraints. For the case c(x) = Ax − b, the Hamiltonian is now

H(x, v) = f (x)+

1
2

v(cid:62)g− 1

2 (I − P ) g− 1

2 v+

(cid:0)log det g + log det Ag−1A(cid:62) − log det AA(cid:62)(cid:1)+λ(cid:62)c,

1
2

2 A(cid:62)(Ag−1A(cid:62))−1Ag− 1

where P = g− 1
2 . The key observation is that the algorithm only needs to
know x(h) in the HMC dynamics, and not v(h). Thus, we can replace H by any other that produces
the same x(h). We show in Lemma 4 (Appendix B.2.2) that the dynamics corresponding to H above
is equivalent to the dynamics that corresponds to a much simpler Hamiltonian:

H(x, v) = f (x) +

1
2

Furthermore, we have

v(cid:62)g− 1

2 (I − P ) g− 1

2 v +

(cid:0)log det g + log det Ag−1A(cid:62)(cid:1) .

1
2

dx
dt

= g− 1

2 (I − P ) g− 1

2 v,

dv
dt

= −∇f (x) +

1
2

Dg

(cid:21)

(cid:20) dx
dt

,

dx
dt

−

1
2

Tr(g− 1

2 (I − P ) g− 1

2 Dg).

2 (I − P ) g− 1

2 A(cid:62)(Ag−1A(cid:62))−1Ag− 1

2 Dg) in dv
2 to compute dv

Efﬁcient Computation of Leverage Score. Even after simplifying the Hamiltonian as above, we
still have a term for the leverage scores, Tr(g− 1
dt so that we need to compute
the diagonal entries of P = g− 1
dt . Since (Ag−1A(cid:62))−1 can
be extremely dense even when A is very sparse, a naive approach such as direct computation of the
inverse can lead to a dense-matrix multiplication. To avoid dense-matrix multiplication, our approach
is based on the fact that certain entries of (Ag−1A(cid:62))−1 can be computed as fast as computing
sparse Cholesky decomposition of Ag−1A(cid:62) [45, 5], which can be O(n) time faster than computing
(Ag−1A(cid:62))−1 in many settings. We ﬁrst compute the Cholesky decomposition to obtain a sparse
triangular matrix L such that LL(cid:62) = Ag−1A(cid:62). Then, we show that only entries of Ag−1A(cid:62) in
sp(L) ∪ sp(L(cid:62)) matter in computing diag(A(cid:62)(Ag−1A(cid:62))−1A), where sp(L) is the sparsity pattern
of L. We give the details of our approach in Appendix B.2.3.

2.3 Discretization

Explicit integrators such as leapfrog integrator, which are commonly used for Hamiltonian Monte
Carlo, are no longer symplectic on general Riemannian manifolds (see Appendix C.1). Even though
there have been some attempts [38] to make explicit integrators work in the Riemannian setting, its
variants do not work for ill-conditioned problems.

Our algorithm uses the implicit midpoint method (Algorithm 3) to discretize the Hamiltonian process
into steps of step size h and run the process for T iterations. This integrator is reversible and
symplectic (so measure-preserving) [19], which allows us to use a Metropolis ﬁlter to ensure the

6

distribution is correct so that we no longer need to solve ODE to accuracy to maintain the correct
stationary distribution. We write H(x, v) = H 1(x, v) + H 2(x, v), where

H 1(x, v) = f (x) +

1
2

(cid:0)log det g(x) + log det Ag(x)−1A(cid:62)(cid:1) ,

H 2(x, v) =

1
2

v(cid:62)g(x)− 1

2 (I − P (x)) g(x)− 1

2 v.

Starting from (x0, v0), in the ﬁrst step of the integrator, we run the process on the Hamiltonian H 1
with step size h
2 to get (x1/3, v1/3). In the second step of the integrator, we run the process on H 2
with step size h by solving

x 2
3

= x 1
3

+ h

∂H 2
∂v

(cid:18) x 1

3

+ x 2
3

2

v 1
3

+ v 2
3

(cid:19)

2

,

,

v 2
3

= v 1
3

− h

∂H 2
∂x

(cid:18) x 1

3

+ x 2
3

2

v 1
3

+ v 2
3

(cid:19)

2

,

,

iteratively using the Newton’s method. This step involves computing the Cholesky decomposition of
(Ag−1A(cid:62))−1 using the Cholesky decomposition of Ag−1A(cid:62). In the third step, we run the process
on the Hamiltonian H 1 with step size h
2 again to get (x1, v1).
We state the complete algorithm (Algorithm 2 and Algorithm 3) with details on the step size in
Appendix C.1 and give the theoretical guarantees in Appendix C.2 (convergence of implicit midpoint
method) and Appendix D (independence of condition number).

3 Experiments

In this section, we demonstrate the efﬁciency of our sampler using experiments on real-world datasets
and compare our sampler with existing samplers. We demonstrate that CRHMC is able to sample
larger models than previously known to be possible, and is signiﬁcantly faster in terms of rate of
convergence and sampling time in Section 3.2, along with convergence test in Section 3.4. We
examine its behavior on benchmark instances such as simplices and Birkhoff polytopes in Section 3.3.

3.1 Experimental Setting

Settings. We performed experiments on the Standard DS12 v2 model from MS Azure cloud, which
has a 2.1GHz Intel Xeon Platinum 8171M CPU and 28GB memory. In the experiments, we used our
MATLAB and C++ implementation of CRHMC3, which is available here and has been integrated
into the COBRA toolbox.

We used twelve constraint-based metabolic models from molecular systems biology in the COBRA
Toolbox v3.0 [21] and ten real-world LP examples randomly chosen from NETLIB LP test sets. A
polytope from each model is deﬁned by {x ∈ Rn : Ax = b, l ≤ x ≤ u} for A ∈ Rm×n, b ∈ Rm,
and l, u ∈ Rn, which is input to CRHMC for uniform sampling. We describe in Appendix A how we
preprocessed these dataset, along with full information about the datasets in Table 2.

Comparison. We used as a baseline the Coordinate Hit-and-Run (CHAR) implemented in two
different languages. The former is Coordinate Hit-and-Run with Rounding (CHRR) written in
MATLAB [11, 20] and the latter is the same algorithm (CDHR) with an R interface and a C++ library,
VolEsti [6]. We refer readers to Appendix A for the details of these algorithms and our comparison
setup. We note that popular sampling packages such as STAN and Pyro were not included in the
experiments as they do not support constrained-based models. Even after transforming our dataset to
their formats, the transformed dataset were too ill-conditioned for those algorithms to run. CHMC in
[4] works only for manifolds implicitly deﬁned by {c(x) = 0} for continuously differentiable c(x)
with Dc(x) full-rank everywhere, so we could not use it for comparison.

Measurements. To evaluate the quality of sampling methods, we measured two quantities, the
number of steps per effective sample (i.e., mixing rate) and the sampling time per effective sample,
Ts. The effective sample size (ESS)4 can be thought of as the number of actual independent samples,

3Our package can be run to sample from general logconcave densities and has a feature for parallelization.
4We use the minimum of the ESS of each coordinate.

7

Figure 3.1: Mixing rate of CRHMC and the com-
petitors. Mixing rate of CRHMC was sub-linear in
dimension and the nnz of a preprocessed matrix A
in a model, whereas the others needed quadratically
many steps to converge to uniform distribution. In
particular for our dataset, CRHMC mixed up to 6
orders of magnitude earlier than the others. Note that
mixing rate of CHAR was very close to quadratic
growth when using the full-dimensional scale (the
ﬁrst column in Table 2).

Figure 3.2: Sampling time of CRHMC and the competi-
tors. The sampling time per effective sample of CRHMC
was sub-quadratic in dimension and the nnz of a prepro-
cessed matrix A in a model, while the others indicates at
least a cubic dependency on dimension. In particular for
our dataset, CRHMC was able to obtain a statistically inde-
pendent sample up to 4 orders of magnitude faster than the
others. This beneﬁt of speed-up was actually straightfor-
ward from the ﬁgure, since CHRR could not obtain enough
samples from instances with more than 5000 variables until
it ran out of time.

taking into account correlation of samples from a target distribution. Thus the number of steps per
effective sample is estimated by the total number of steps divided by the ESS, and the sampling time
Ts is estimated as the total sampling time until termination divided by the ESS.

Each algorithm attempted to draw 1000 uniform samples, with limits on running time set to 1 day
(3 days for the largest instance ken_18) and memory usage to 6GB. If an algorithm passes either
the time or the memory limit, we stop the algorithm and measure the quantities of interest based on
samples drawn until that moment. After getting uniform samples, we thinned the samples twice to
ensure independence of samples; ﬁrst we computed the ESS of the samples, only kept ESS many
samples, and repeated this again. We estimated the above quantities only if the ESS is more than 10
and an algorithm does not run into any error while running5.

3.2 Mixing Rate and Sampling Time

Sub-linear Mixing Rate. We examined how the number of steps per effective sample grows
with the number of nonzeros (nnz) of matrix A (after preprocessing) and the number of variables
(dimension in the plots). To this end, we counted the total number of steps taken until termination of
algorithms and divided it by the effective sample size of drawn samples. Note that we thinned twice
to ensure independence of samples used.

The mixing rate of CRHMC was sub-linear in both dimension and nnz, whereas previous implemen-
tations based on CHAR required at least n2 steps per sample as seen in Figure 3.1. On the dataset,

5When running CDHR from the VolEsti package on some instances, we got an error message “R session

aborted and R encountered a fatal error”.

8

102103104105Dimension101102103104105106107108109Step/SampleMixing RateCRHMC: dim0.52 CHRR: dim2.71 CDHR: dim2.14102103104105106NNZ102103Step/SampleMixing RateCRHMC: nnz0.53102103104105Dimension10-410-310-210-1100101102103104105Time/Sample (s)Sampling TimeCRHMC: dim1.50 CHRR: dim3.14 CDHR: dim3.16102103104105106NNZ10-310-210-1100101102103104Time/Sample (s)Sampling TimeCRHMC: nnz1.50Bio Model Vars (n)

ecoli
cardiac_mit
Aci_D21
Aci_MR95
Abi_49176
Aci_20731
Aci_PHEA
iAF1260
iJO1366
Recon1
Recon2
Recon3

95
220
851
994
1069
1090
1561
2382
2583
3742
7440
13543

nnz
291
228
1758
2859
2951
2946
4640
6368
7284
8717
19791
48187

CRHMC
0.0098
0.0100
0.4257
0.9624
0.9608
0.1540
0.3701
4.4355
4.1608
0.7184
2.6116
31.114

CHRR
0.0365
0.0059
0.6884
2.0668
1.9395
2.3014
12.06
3687.2
70.5
208.5
10445*
29211*

CDHR
0.0022
0.0005
0.2974
0.5237
0.9622
1.1086
-
-
35.556
-
-
-

LP Model Vars (n)

israel
gfrd_pnc
25fv47
pilot_ja
sctap2
ship08l
cre_a
woodw
80bau3b
ken_18

316
1160
1876
2267
2500
4363
7248
8418
12061
154699

nnz
2519
2393
10566
11886
7334
9434
17368
23158
22341
295946

CRHMC
0.1186
0.2199
0.8159
1.3490
0.6752
0.6258
2.2205
2.0689
11.881
1616.3

CHRR
1.2224
40.988
199.9
5059*
520.2
6512
30455*
30307*
47432*
-

CDHR
0.4426
18.468
-
-
-
-
-
-
-
-

Table 1: Sampling time per effective sample of CHRR and CRHMC. We note that CRHMC is 1000 times faster
than CHRR on the latest metabolic network (Recon3). Sampling time with asterisk (*) indicates that the effective
sample size is less than 10.

mixing rate attained was up to 6 orders of magnitude faster for CRHMC compared to CHAR, imply-
ing that CRHMC converged to uniform distribution substantially faster than the other competitors.
This gap in mixing rate increased super-linearly in dimension, enabling CRHMC to run on large
instances of dimension up to 100000.

Sub-quadratic Sampling Time. We next examined the sampling time Ts in terms of both the
nnz of A and the dimension of the instance. We computed the runtime of algorithms until their
termination divided by the effective sample size of drawn samples, where we ignored the time it takes
for preprocessing. Note that the sampling time Ts is essentially multiplication of the mixing rate and
the per-step complexity (i.e., how much time each step takes).

As shown in Figure 3.2 and Table 1, we found that the per-step complexity of CRHMC was small
enough to make the sampling time sub-quadratic in both dimension and nnz, whereas CHAR had at
least a cubic dependency on dimension, despite of a low per-step complexity. On our dataset, the
sampling time of CRHMC was up to 4 orders of magnitude less than that of CHRR and CDHR.
While CHRR can be used on dimension only up to a few thousands, increasing beneﬁts of sampling
time in higher dimension allows CRHMC to run on dimension up to 0.1 million.

3.3 CRHMC on Structured Instances

To see the behavior of CRHMC on very large instances, we ran the algorithm on three families of
structured polytopes – hypercube, simplex, and Birkhoff polytope – up to dimension half-million. We
attempted to draw 500 uniform samples with a 1 day time limit (except for 2 days for half-million-
dimensional Birkhoff polytope). The deﬁnitions of these polytopes are shown in Appendix A.1.

Figure 3.3: Mixing rate and sampling time on structured polytopes including hybercubes, simplices, and
Birkhoff polytopes. CRHMC is scalable up to 0.5 million dimension on hypercubes and simplices and up to 0.1
million dimension on Birkhoff polytopes. We note that on the 0.5 million dimensional Birkhoff polytope the
ESS is only 16, which is not reliable compared to the ESS on the other instances.

To the best of our knowledge, this is the ﬁrst demonstration that it is possible to sample such a
large model. As seen in Figure 3.3, CRHMC can scale smoothly up to half-million dimension on
hypercubes and simplices and up to dimension 105 for Birkhoff polytopes (we could not obtain
a reliable estimate of mixing rate and sampling time on the half-million dimensional Birkhoff

9

101102103104105106Dimension101102103104Step/SampleMixing RateCube: dim0.34 Simplex: dim0.33 Birkhoff: dim 0.43101102103104105106Dimension10-310-210-1100101102103104Time/Sample (s)Sampling TimeCube: dim0.76 Simplex: dim0.83 Birkhoff: dim 1.08polytope, as the ESS is only 16 after 2 days). However, we believe that one can ﬁnd room for further
improvement of CRHMC by tuning parameters or leveraging engineering techniques. We also expect
that CRHMC enables us to estimate the volume of Bn for n ≥ 20, going well beyond the previously
best possible dimension.

3.4 Uniformity Test

We used the following uniformity test to check whether samples from CRHMC form the uniform
distribution over a polytope P : check that the fraction of the samples in the scaled set x · P is
proportional to xdim. As seen in Figure 3.4, the empirical CDFs of the radial distribution to the power
of (1/dim) are close to the CDFs of the uniform distribution over those polytopes.

Figure 3.4: We plot the empirical cumulative distribution function of the radial distribution to the power of
(1/dim) with 1000 ESS obtained by running CRHMC on ATCC-49176 (952 × 1069, left) and Aci-PHEA
(1319 × 1561, right), and in the plot x-axis is the scaling factor. We can observe the CDFs are very close to the
CDFs of the uniform distribution over the polytopes deﬁned by two instances.

Acknowledgement. The authors are grateful to Ben Cousins for helpful discussions, and to Ronan
Fleming, Ines Thiele and their research groups for advice on metabolic models. This work was
supported in part by NSF awards DMS-1839116, DMS-1839323, CCF-1909756, CCF-2007443 and
CCF-2134105.

References

[1] Hans C Andersen. Rattle: A “velocity” version of the shake algorithm for molecular dynamics

calculations. Journal of computational Physics, 52(1):24–34, 1983.

[2] Ivona Bezáková, Daniel Štefankoviˇc, Vijay V Vazirani, and Eric Vigoda. Accelerating simulated
annealing for the permanent and combinatorial counting problems. SIAM Journal on Computing
(SICOMP), 37(5):1429–1454, 2008.

[3] Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis
Karaletsos, Rohit Singh, Paul A. Szerlip, Paul Horsfall, and Noah D. Goodman. Pyro: Deep
Universal Probabilistic Programming. Journal of Machine Learning Research (JMLR), 20:28:1–
28:6, 2019.

[4] Marcus Brubaker, Mathieu Salzmann, and Raquel Urtasun. A family of MCMC methods on
implicitly deﬁned manifolds. In Artiﬁcial intelligence and statistics (AISTATS), pages 161–172,
2012.

[5] Yogin E Campbell and Timothy A Davis. Computing the sparse inverse subset: an inverse

multifrontal approach. University of Florida, Technical Report TR-95-021, 1995.

[6] Apostolos Chalkis and Vissarion Fisikopoulos. volEsti: Volume approximation and sampling

for convex polytopes in R. arXiv preprint arXiv:2007.01578, 2020.

[7] Yuansi Chen, Raaz Dwivedi, Martin J Wainwright, and Bin Yu. Fast mixing of Metropolized
Hamiltonian Monte Carlo: Beneﬁts of multi-step gradients. Journal of Machine Learning
Research (JMLR), 21:92–1, 2020.

10

00.20.40.60.81x00.20.40.60.81F(x)Empirical CDF of the Radial DistributionEmpirical CDFUniform00.20.40.60.81x00.20.40.60.81F(x)Empirical CDF of the Radial DistributionEmpirical CDFUniform[8] Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan. Underdamped
Langevin MCMC: a non-asymptotic analysis. In Conference on Learning Theory (COLT),
pages 300–323. PMLR, 2018.

[9] Sinho Chewi, Thibaut Le Gouic, Chen Lu, Tyler Maunu, Philippe Rigollet, and Austin Stromme.
Exponential ergodicity of mirror-Langevin diffusions. Advances in Neural Information Process-
ing Systems (NeurIPS), 33:19573–19585, 2020.

[10] Adam D Cobb, Atılım Güne¸s Baydin, Andrew Markham, and Stephen J Roberts. Introducing
an explicit symplectic integration scheme for Riemannian manifold Hamiltonian Monte Carlo.
arXiv preprint arXiv:1910.06243, 2019.

[11] Ben Cousins and Santosh Vempala. A practical volume algorithm. Mathematical Programming

Computation, 8(2):133–160, 2016.

[12] Timothy A Davis. Direct methods for sparse linear systems. SIAM, 2006.

[13] James W Demmel. Applied numerical linear algebra. SIAM, 1997.

[14] Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid Monte

Carlo. Physics letters B, 195(2):216–222, 1987.

[15] Raaz Dwivedi, Yuansi Chen, Martin J Wainwright, and Bin Yu. Log-concave sampling:
Metropolis-Hastings algorithms are fast! In Conference on Learning Theory (COLT), pages
793–797. PMLR, 2018.

[16] Martin Dyer, Alan Frieze, and Ravi Kannan. A random polynomial-time algorithm for approxi-

mating the volume of convex bodies. Journal of the ACM (JACM), 38(1):1–17, 1991.

[17] Mark Girolami and Ben Calderhead. Riemann manifold Langevin and Hamiltonian Monte
Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
73(2):123–214, 2011.

[18] Andreas Griewank and Andrea Walther. Evaluating derivatives: principles and techniques of

algorithmic differentiation. SIAM, 2008.

[19] Ernst Hairer, Marlis Hochbruck, Arieh Iserles, and Christian Lubich. Geometric numerical

integration. Oberwolfach Reports, 3(1):805–882, 2006.

[20] Hulda S Haraldsdóttir, Ben Cousins, Ines Thiele, Ronan MT Fleming, and Santosh Vempala.
Chrr: coordinate hit-and-run with rounding for uniform sampling of constraint-based models.
Bioinformatics, 33(11):1741–1743, 2017.

[21] Laurent Heirendt, Sylvain Arreckx, Thomas Pfau, Sebastián N Mendoza, Anne Richelle, Almut
Heinken, Hulda S Haraldsdóttir, Jacek Wachowiak, Sarah M Keating, Vanja Vlasov, et al.
Creation and analysis of biochemical constraint-based models using the COBRA Toolbox V.
3.0. Nature protocols, 14(3):639–702, 2019.

[22] Matthew D Hoffman, Andrew Gelman, et al. The No-U-Turn sampler: adaptively setting
path lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research (JMLR),
15(1):1593–1623, 2014.

[23] Mark Jerrum, Alistair Sinclair, and Eric Vigoda. A polynomial-time approximation algorithm for
the permanent of a matrix with nonnegative entries. Journal of the ACM (JACM), 51(4):671–697,
2004.

[24] He Jia, Aditi Laddha, Yin Tat Lee, and Santosh Vempala. Reducing isotropy and volume to KLS:
an O∗(n3ψ2) volume algorithm. In Proceedings of the 53rd Annual ACM SIGACT Symposium
on Theory of Computing (STOC), pages 961–974, 2021.

[25] Ravi Kannan, László Lovász, and Miklós Simonovits. Random walks and an O∗(n5) volume

algorithm for convex bodies. Random Structures & Algorithms, 11(1):1–50, 1997.

[26] Ravindran Kannan and Hariharan Narayanan. Random walks on polytopes and an afﬁne interior
point method for linear programming. Mathematics of Operations Research, 37(1):1–20, 2012.

11

[27] Zachary A King, Justin Lu, Andreas Dräger, Philip Miller, Stephen Federowicz, Joshua A
Lerman, Ali Ebrahim, Bernhard O Palsson, and Nathan E Lewis. BiGG Models: A platform
for integrating, standardizing and sharing genome-scale models. Nucleic acids research,
44(D1):D515–D522, 2016.

[28] Yunbum Kook, Yin Tat Lee, Ruoqi Shen, and Santosh S. Vempala. Condition-number-
independent Convergence Rate of Riemannian Hamiltonian Monte Carlo with Numerical
Integrators. arXiv preprint arXiv:2210.07219, 2022.

[29] Aditi Laddha and Santosh Vempala. Convergence of Gibbs sampling: Coordinate Hit-and-Run
mixes fast. The 37th International Symposium on Computational Geometry (SoCG), 2021.

[30] Yin Tat Lee, Ruoqi Shen, and Kevin Tian. Logsmooth gradient concentration and tighter
In Conference on Learning Theory

runtimes for metropolized Hamiltonian Monte Carlo.
(COLT), pages 2565–2597. PMLR, 2020.

[31] Yin Tat Lee and Santosh S Vempala. Convergence rate of Riemannian Hamiltonian Monte Carlo
and faster polytope volume computation. In Proceedings of the 50th Annual ACM SIGACT
Symposium on Theory of Computing (STOC), pages 1115–1121, 2018.

[32] Yin Tat Lee and Man-Chung Yue. Universal barrier is n-self-concordant. Mathematics of

Operations Research, 2021.

[33] Nathan E Lewis, Harish Nagarajan, and Bernhard O Palsson. Constraining the metabolic
genotype–phenotype relationship using a phylogeny of in silico methods. Nature Reviews
Microbiology, 10(4):291–305, 2012.

[34] László Lovász and Santosh Vempala. Hit-and-run from a corner. SIAM Journal on Computing

(SICOMP), 35(4):985–1005, 2006.

[35] Hariharan Narayanan and Piyush Srivastava. On the mixing time of coordinate Hit-and-Run.

Combinatorics, Probability and Computing, pages 1–13, 2021.

[36] Radford M Neal et al. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte

Carlo, 2(11):2, 2011.

[37] Yurii Nesterov and Arkadii Nemirovskii.

Interior-point polynomial algorithms in convex

programming. SIAM, 1994.

[38] Pauli Pihajoki. Explicit methods in extended phase space for inseparable hamiltonian problems.

Celestial Mechanics and Dynamical Astronomy, 121(3):211–231, 2015.

[39] Sebastian Reich. Symplectic integration of constrained Hamiltonian systems by Runge-Kutta

methods. University of British Columbia, Department of Computer Science, 1993.

[40] Gareth O Roberts and Richard L Tweedie. Exponential convergence of Langevin distributions

and their discrete approximations. Bernoulli, pages 341–363, 1996.

[41] John Salvatier, Thomas V Wiecki, and Christopher Fonnesbeck. Probabilistic programming in

Python using PyMC3. PeerJ Computer Science, 2:e55, 2016.

[42] Ruoqi Shen and Yin Tat Lee. The randomized midpoint method for log-concave sampling.

Advances in Neural Information Processing Systems (NeurIPS), 32, 2019.

[43] Umut Simsekli, Roland Badeau, Taylan Cemgil, and Gaël Richard. Stochastic quasi-newton
Langevin Monte Carlo. In International Conference on Machine Learning (ICML), pages
642–651. PMLR, 2016.

[44] Stan Development Team. RStan: the R interface to Stan, 2020. R package version 2.21.2.

[45] Kazuhiro Takahashi. Formation of sparse bus impedance matrix and its application to short

circuit study. In Proceeding of PICA Conference, June, 1973, 1973.

[46] Ines Thiele, Neil Swainston, Ronan MT Fleming, Andreas Hoppe, Swagatika Sahoo, Maike K
Aurich, Hulda Haraldsdottir, Monica L Mo, Ottar Rolfsson, Miranda D Stobbe, et al.
A community-driven global reconstruction of human metabolism. Nature biotechnology,
31(5):419–425, 2013.

12

Experiments details
(App A)

Continuous
CRHMC
(App B)

Theory

Notation, Deﬁnition
(App E)

Discretized
CRHMC
(App C)

App B.1: Derivation

App B.2: Computational tricks

App B.3: Correctness

App D: Condition # independence
(Mixing rate)

App C.1: Implicit midpoint method

App C.2: Correctness & Efﬁciency

A Additional Experiment Details

Dataset. We summarize in Table 2 the dataset used in experiments. If a model is unbounded,
we make it bounded by setting l = max(l, −107) and u = min(u, 107). As existing packages
require full-dimensional representations of polytopes (i.e., {x : A(cid:48)x ≤ b(cid:48)}), we transformed all
constraint-based models to prepare instances for them as follows: (1) ﬁrst preprocess each model
by removing redundant constraints and appropriately scaling it, (2) ﬁnd its corresponding full-
dimensional description, and (3) round it via the maximum volume ellipsoid (MVE) algorithm
making the polytope more amenable to sampling. We note that a full-dimensional polytope can
be transformed into a constraint-based polytope and vice versa, so CRHMC can be run on either
representation.

Bio Model

Full-dim

Consts (m)

Vars (n)

nnz

ecoli
cardiac_mit
Aci_D21
Aci_MR95
Abi_49176
Aci_20731
Aci_PHEA
iAF1260
iJO1366
Recon1
Recon2
Recon3

24
12
103
123
157
164
328
572
590
932
2430
5335

72
230
856
917
952
1009
1319
1668
1805
2766
5063
8399

95
220
851
994
1069
1090
1561
2382
2583
3742
7440
13543

291
228
1758
2859
2951
2946
4640
6368
7284
8717
19791
48187

LP Model

Full-dim

Consts (m)

Vars (n)

nnz

israel
gfrd_pnc
25fv47
pilot_ja
sctap2
ship08l
cre_a
woodw
80bau3b
ken_18

142
544
1056
1002
1410
2700
3703
4656
9233
49896

174
616
821
940
1090
778
3516
1098
2262
105127

316
1160
1876
2267
2500
4363
7248
8418
12061
154699

2519
2393
10566
11886
7334
9434
17368
23158
22341
295946

Table 2: Constraint-based models. Each constraint-based model has a form of {x ∈ Rn : Ax = b, l ≤ x ≤ u}
for A ∈ Rm×n, b ∈ Rm and l, u ∈ Rn, where the rows and columns correspond to constraints and variables
respectively. The full-dimension of each model is obtained by transforming its degenerate subspace to a full
dimensional representation (i.e., A(cid:48)x ≤ b(cid:48)), and we count the number of nonzero (nnz) entries of a preprocessed
matrix A.

Preprocessing. We preprocessed each constrained-based model prior to sampling. This preprocess-
ing consists mainly of simplifying polytopes, scaling properly for numerical stability, and ﬁnding a
feasible starting point. To simplify a given polytope, we check if li = ui for each i ∈ [n] and then
incorporate such variables xi into Ax = b. Any dense column is split into several columns with less
non-zero entries by introducing additional variables. Then we remove dependent rows of A by the
Cholesky decomposition. Then we ﬁnd the Dikin ellipsoid of the polytope. If the width along some
axis is smaller than a preset tolerance, then we ﬁx variables in such directions, reducing columns of
A. Lastly, we run the primal-dual interior-point method with the log-barrier to ﬁnd an analytic center
of the polytope, which will be used as a starting point in sampling. When ﬁnding the analytic center
of the simpliﬁed polytope, if a coordinate of the analytic center is too close to a boundary (to be

13

precise, smaller than a preset tolerance boundary 10−8), then we assume that the inequality constraint
(either xi ≤ ui or li ≤ xi) is tight, and we collapse such a variable by moving it into the constraints
Ax = b. We go back to the step for removing dependent rows and repeat until no more changes are
made to A. Along with simpliﬁcation, we keep rescaling A, b, l, u for numerical stability.

Coordinate Hit-and-Run (CDHR). We brieﬂy explain how CHRR works. First, rounding via the
MVE algorithm ﬁnds the maximum volume ellipsoid inscribed in the polytope and applies, to the
polytope, an afﬁne transformation that makes this ellipsoid a unit ball. This procedure puts a possibly
highly-skewed polytope into John’s position, which guarantees that the polytope contains a unit ball
and is contained in a ball of radius n. This position still has a beneﬁcial effect on sampling in practice
in the sense that the random walk can converge in fewer steps. After the transformation, the random
walk based on Coordinate Hit-and-Run (CHAR) chooses a random coordinate and moves to a random
point on the line through the current point along the chosen coordinate.
When running CHRR and CDHR, we recorded a sample every n2 steps. The mixing rate (i.e., the
number of steps required to get a sample from a target distribution) of Hit-and-Run (HAR), a general
version of CHAR choosing a random direction (unit vector) instead of a random coordinate, is
O∗(n2R2) for a polytope P with Bn ⊆ P ⊆ R · Bn, where Bn is the unit ball in Rn [34]. It was
proved only recently that CHAR mixes in O∗(n9R2) steps on such a polytope [29, 35]. Even though
this bound is not as tight as the mixing-rate bound for HAR, it was reported in [20] that CHRR mixes
in the same number of steps as HAR empirically. Moreover, the per-step complexity of CHAR can
be n times faster than that of HAR, so CHAR brings a signiﬁcant speed-up in practice.

Comparison Setup. We set the parameters of CRHMC to values in default_options.m in
the experiments. For the competitors, we proceeded with the following additional steps for fair
comparison. First, as the VolEsti package does not support the MVE rounding, we rounded each
polytope by the MVE algorithm in the CHRR package and then transformed the rounded polytope so
that the R interface can read the data ﬁle. Next, we limited all algorithms to a single core, since the R
interface uses a single core as a default whereas MATLAB uses as many available cores as possible.

A.1 Polytope Deﬁnition

Hypercube. The n-dimensional hypercube is deﬁned by {x ∈ Rn : − 1
Note that it has no equality constraint and its full-dimension is n.

2 ≤ xi ≤ 1

2 for all i ∈ [n]}.

Simplex. The n-dimensional simplex is deﬁned by {x ∈ Rn : 0 ≤ xi for all i ∈ [n], (cid:80)n
1}. Note that its full-dimension is n − 1.

i=1 xi =

Birkhoff Polytope. The nth Birkhoff polytope Bn is the set of all doubly stochastic n × n matrices
(or the convex hull of all permutation matrices), which is deﬁned as

Bn = {(Xij)i,j∈[n] :

(cid:88)

j

Xij = 1 for all i ∈ [n],

(cid:88)

i

Xij = 1 for all j ∈ [n], and Xij ≥ 0}.

Namely, Bn is deﬁned in a constrained Rn2
1) = (n − 1)2. We ran CRHMC on B√
Birkhoff polytope.

-dimensional space, and its full-dimension is n2 − (2n −
n to examine its efﬁciency on (roughly) n-dimensional

B Deferred details of CRHMC

In this section, we present all technical details behind an idealized version of our algorithm, CRHMC,
together with correctness of CRHMC. Subsequently in Appendix C, we provide details on a discretized
version of CRHMC.

14

B.1 Deferred details of Section 2.1

Recall that in Section 2.1 we mention that the following constrained Hamiltonian satisﬁes the
Hamiltonian ODE

(cid:16) dx

, dv

(cid:17)

:

dt = − ∂H(x,v)

∂x

dt = ∂H(x,v)

∂v

H(x, v) = H(x, v) + λ(x, v)(cid:62)c(x) with H(x, v) = f (x) +

1
2

v(cid:62)M (x)†v + log pdet(M (x))

where

λ(x, v) = (Dc(x)Dc(x)(cid:62))−1

(cid:18)

D2c(x)[v,

dx
dt

] − Dc(x)

∂H(x, v)
∂x

(cid:19)

.

Lemma 1. Consider the constrained Hamiltonian deﬁned by (2.5) with Range(M (x)) =
Null(Dc(x)) and

λ(xt, vt) = (Dc(xt)Dc(xt)(cid:62))−1

(cid:18)

D2c(xt)[vt,

dxt
dt

] − Dc(xt)

∂H(xt, vt)
∂x

(cid:19)

.

When the initial point satisﬁes c(x0) = 0, the ODE solution of (2.2) satisﬁes c(xt) = 0 and
Dc(xt)vt = Dc(x0)v0 for all t.

Proof. First we compute

d
dt

c(xt) = Dc(xt) ·

dxt
dt

= Dc(xt) ·

∂H(xt, vt)
∂vt

= Dc(xt)M (xt)†v + Dc(xt)Dvλ(xt, vt)(cid:62)c(xt)
= Dc(xt)Dvλ(xt, vt)(cid:62)c(xt)

where we used Range(M (x)†) = Range(M (x)) = Null(Dc(x)). Since c(x0) = 0, by the unique-
ness of the ODE solution, we have that c(xt) = 0 for all t. Next we compute

dvt
dt

= −

= −

= −

∂H(xt, vt)
∂x
∂H(xt, vt)
∂x
∂H(xt, vt)
∂x

− Dc(xt)(cid:62)λ(xt, vt) − Dxλ(xt, vt)(cid:62)c(xt)

− Dc(xt)(cid:62)λ(xt, vt)

where we used c(xt) = 0. Hence, we have

d
dt

Dc(xt)vt = D2c(xt)[vt,

= D2c(xt)[vt,

dxt
dt
dxt
dt

] + Dc(xt)

] − Dc(xt)

dvt
dt
∂H(xt, vt)
∂x

− Dc(xt)Dc(xt)(cid:62)λ(xt, vt).

By setting λ(xt, vt) = (Dc(xt)Dc(xt)(cid:62))−1(D2c(xt)[vt, dxt
d
dt Dc(xt)vt = 0 and Dc(xt)vt = Dc(x0)v0 for all t (i.e., vt ∈ Null(Dc(xt)) during Step 2).

dt ] − Dc(xt) ∂H(xt,vt)

∂x

), we have

B.2 Deferred details of Section 2.2

In Section 2.2, we mention that a naive algorithm computing ∂H/∂x and ∂H/∂v is bound to face
the following challenges, especially in high-dimensional regime, and brieﬂy explain how we address
each of them. In this section, we give full details on our computational tricks.

1. Computation of the pseudo-inverse and its derivatives takes O(n3), except for very

special matrices =⇒ Find equivalent formulas (Appendix B.2.1).

2. The Lagrangian term in the constrained Hamiltonian entails extra computation such

as D2c(x) =⇒ Simplify the constrained Hamiltonian (Appendix B.2.2).

3. A naive approach to computing leverage scores in ∂H/∂x results in a very dense

matrix =⇒ Track sparsity pattern (Appendix B.2.3).

15

B.2.1 Avoiding pseudo-inverse and pseudo-determinant

We start with a formula for M (x)†.
Lemma 2. Let M (x) = Q(x) · g(x) · Q(x) where Q(x) = I − Dc(x)(cid:62)(Dc(x) · Dc(x)(cid:62))−1Dc(x)
is the orthogonal projection to the null space of Dc(x). Then, Dc(x) · M (x)† = 0 and M (x)† =
g(x)− 1

2 · (I − P (x)) · g(x)− 1

2 with

P (x) = g(x)− 1

2 · Dc(x)(cid:62)(Dc(x) · g(x)−1 · Dc(x)(cid:62))−1Dc(x) · g(x)− 1
2 .

Proof. Recall that Range(M (x)†) = Range(M (x)). Hence, for any u ∈ Rn, we have
that M (x)†u ∈ Range(M (x)). Since Range(M (x)) ⊆ Range(Q(x)) and Range(Q(x)) =
Null(Dc(x)) due to the deﬁnition of the orthogonal projection Q(x), it follows that Dc(x) ·
M (x)†u = 0 for all u.
2 P g− 1
For the formula of M (x)†, we simplify the notation by ignoring the parameter x. Let N = g− 1
and J = Dc(x). The goal is to prove that M † = N . First, we show some basic identities about Q
and N :

2

QN =Qg− 1

2 (I − g− 1

2 J (cid:62)(Jg−1J (cid:62))−1Jg− 1
=(I − J (cid:62)(JJ (cid:62))−1J)(g−1 − g−1J (cid:62)(Jg−1J (cid:62))−1Jg−1)
=g−1 − J (cid:62)(JJ (cid:62))−1Jg−1

2 )g− 1

2

− (g−1J (cid:62)(Jg−1J (cid:62))−1Jg−1 − J (cid:62)(JJ (cid:62))−1Jg−1J (cid:62)(Jg−1J (cid:62))−1Jg−1)

=N.

(B.1)

Similarly, we have N Q = N , QgN = Q, and N gQ = Q. To prove that M † = N , we need to check
that M N and N M are symmetric, M N M = M , and N M N = N .

For symmetry of M N and N M , we note that M N = QgQN = QgN = Q and N M = N QgQ =
N gQ = Q. For the formula of M N M and N M N , we note that that Q is a projection matrix and
hence

M N M = QM = QQgQ = QgQ = M,
N M N = QN = N.

Therefore, we have M † = N .

Another bottleneck of the algorithm is to compute log pdetM (x). The next lemma shows a simpler
formula that can take advantage of sparse Cholesky decomposition.
Lemma 3. We have that
log pdet(M (x)) = log det g(x)+log det (cid:0)Dc(x) · g(x)−1 · Dc(x)(cid:62)(cid:1)−log det (cid:0)Dc(x) · Dc(x)(cid:62)(cid:1) .

Proof. We simplify the notation by ignoring the parameter x and letting J = Dc(x). Let

f1(g) = log pdet(Q · g · Q),
f2(g) = log det g + log det Jg−1J (cid:62) − log det JJ (cid:62).

Clearly, f1(I) = f2(I) = 0, and hence it sufﬁces to prove that their derivatives are the same.
Note that Range(Q · g · Q) = Null(J) and Range(J (cid:62)) is the orthogonal complement of Null(J).
Since J (cid:62)(JJ (cid:62))−1J is the orthogonal projection to Range(J (cid:62)), all of its eigenvectors in Range(J (cid:62))
have eigenvalue 1 and all the rest in Null(J) have eigenvalue 0. Therefore, by padding eigenvalue 1
on Range(J (cid:62)) = Null(J)⊥ = Range(QgQ)⊥, we have

pdet(Q · g · Q) = det(Q · g · Q + J (cid:62)(JJ (cid:62))−1J)

= det(Q · g · Q + (I − Q)).

Using D log det A(g)[u] = Tr(A(g)−1DA(g)[u]), the directional derivative of f1 on direction u is
Df1(g)[u] = Tr(cid:0)(Q · g · Q + (I − Q))−1Q · u · Q(cid:1).

16

Let N = (Q · g · Q)†. As shown in the proof of Lemma 2, we have N Q = QN = N and QgN = Q.
By using these identities, we can manually check that (Q · g · Q + (I − Q))−1 = N + (I − Q).
Hence,

Df1(g)[u] = Tr ((N + (I − Q))Q · u · Q) = Tr(N uQ)

= Tr(QN u) = Tr(N u)

where we used idempotence of the projection matrix Q (i.e., Q2 = Q).

On the other hand, we have

Df2(g)[u] = Tr(g−1u) − Tr (cid:0)(Jg−1J (cid:62))−1(Jg−1ug−1J (cid:62))(cid:1)

= Tr (cid:0)(g−1 − g−1J (cid:62)(Jg−1J (cid:62))−1Jg−1)u(cid:1)
= Tr(N u)

where we used the alternative formula of N in Lemma 2. This shows that the derivative of f1
equals to that of f2 at any point g (cid:31) 0. Since the set of positive deﬁnite matrices is connected and
f1(I) = f2(I), this implies that f1(g) = f2(g) for all g (cid:31) 0.

Combining Lemma 2 and Lemma 3, we have the following formula of the Hamiltonian.
H(x, v) =H0(x, v) + λ(x, v)(cid:62)c(x),
1
2

H0(x, v) =f (x) +

I − g(x)− 1

v(cid:62)g(x)− 1

(cid:16)

2

2 · Dc(x)(cid:62)(Dc(x) · g(x)−1 · Dc(x)(cid:62))−1Dc(x) · g(x)− 1

2

+

1
2

(cid:0)log det g(x) + log det (cid:0)Dc(x) · g(x)−1 · Dc(x)(cid:62)(cid:1) − log det (cid:0)Dc(x) · Dc(x)(cid:62)(cid:1)(cid:1) .

B.2.2 Simpliﬁcation for subspace constraints

For the case c(x) = Ax − b, the constrained Hamiltonian is

H(x, v) =f (x) +

1
2

v(cid:62)g− 1

2 (I − P ) g− 1

2 v +

1
2

(cid:0)log det g + log det Ag−1A(cid:62) − log det AA(cid:62)(cid:1) + λ(cid:62)c

(B.2)

2 A(cid:62)(Ag−1A(cid:62))−1Ag− 1

where P = g− 1
2 . The following lemma shows that the dynamics correspond-
ing to H above is equivalent to a simpler Hamiltonian. The key observation is that the algorithm only
needs to know x(h) in the HMC dynamics, and not v(h). Thus we can replace H by any other H
that produces the same x(h).
Lemma 4. The Hamiltonian dynamics of x corresponding to (B.2) is same as the dynamics of x
corresponding to

(cid:17)

g(x)− 1

2 v

H(x, v) = f (x) +

v(cid:62)g− 1

1
2
2 A(cid:62)(Ag−1A(cid:62))−1Ag− 1

where P = g− 1

2 . Furthermore, we have

2 (I − P ) g− 1

2 v +

(cid:0)log det g + log det Ag−1A(cid:62)(cid:1)

(B.3)

1
2

dx
dt
dv
dt

= g− 1

2 (I − P ) g− 1

= −∇f (x) +

1
2

Dg

2 v,
(cid:20) dx
dt

(cid:21)

−

,

dx
dt

1
2

Tr(g− 1

2 (I − P ) g− 1

2 Dg).

Proof. Note that the dynamics of x corresponding to (B.2) is given by

= g− 1

2 (I − P ) g− 1

2 v + (Dvλ)(cid:62)c

dx
dt

=

∂H
∂v
= g− 1

2 (I − P ) g− 1

2 v

where we used that c(x) = 0 (Lemma 1).

Now let us compute the dynamics of v. Note that

v(cid:62)g− 1

2 (I − P ) g− 1

2 v = v(cid:62)g−1v − v(cid:62)g−1A(cid:62)(A · g−1 · A(cid:62))−1Ag−1v.

17

(B.4)

(B.5)

(B.6)

v(cid:62)g−1 · Dg · g−1v + v(cid:62)g−1 · Dg · g−1A(cid:62)(A · g−1 · A(cid:62))−1Ag−1v

v(cid:62)g−1A(cid:62)(A · g−1 · A(cid:62))−1A · g−1 · Dg · g−1 · A(cid:62)(A · g−1 · A(cid:62))−1Ag−1v

2 · Dg · g− 1

2 (I − P )g− 1

2 v

Hence, we have
(cid:18) 1
2

Dx

v(cid:62)g− 1

2 (I − P ) g− 1

2 v

(cid:19)

= −

−

= −

= −

1
2
1
2
1
2
1
2

2 (I − P )g− 1
(cid:21)

,

,

Dg

v(cid:62)g− 1
(cid:20) dx
dx
dt
dt
dt = g− 1
∂H
∂x

= −

dv
dt

where we used dx

2 (I − P )g− 1

2 v in (B.6). Therefore, it follows that

− (Dvλ)(cid:62)c − A(cid:62)λ
(cid:21)

= −∇f (x) +

1
2

Dg

1
2
Tr (cid:0)(Ag(x)−1A(cid:62))−1Ag(x)−1 · Dg · g(x)−1A(cid:62)(cid:1) − A(cid:62)λ

Tr(g−1Dg)

dx
dt

−

,

(cid:20) dx
dt

+

1
2

= −∇f (x) +

1
2

Dg

(cid:21)

(cid:20) dx
dt

,

dx
dt

−

1
2

Tr(g− 1

2 (I − P ) g− 1

2 Dg) − A(cid:62)λ

(B.7)

(B.8)

where we used that c = 0 again in the second equality.
2 (I − P ) g− 1

Recall that dx

dt = g− 1

2 v. In this formula, let us perturb v by A(cid:62)y for any y as follows.

(I − P ) g− 1

2 (v + A(cid:62)y) = (I − P ) g− 1

2 v +

(cid:16)

I − g− 1

2 A(cid:62)(Ag−1A(cid:62))−1Ag− 1

2

(cid:17)

g− 1

2 A(cid:62)y

= (I − P ) g− 1
= (I − P ) g− 1
= (I − P ) g− 1

2 v.

2 v + (g− 1
2 v + (g− 1

2 A(cid:62)y − g− 1
2 A(cid:62)y − g− 1

2 A(cid:62)(Ag−1A(cid:62))−1(Ag−1A(cid:62))y)
2 A(cid:62)y)

Hence, removing A(cid:62)λ from dv
dt in (B.7) does not change the dynamics of x, and thus we have the
new dynamics given simply by (B.4) and (B.5). By repeating this proof, one can check that the
simpliﬁed Hamiltonian (B.3) also yields (B.4) and (B.5).

B.2.3 Efﬁcient Computation of Leverage Score

In this section, we discuss how we efﬁciently compute the diagonal entries of A(cid:62)(Ag−1A(cid:62))−1A. Our
idea is based on the fact that certain entries of (Ag−1A(cid:62))−1 can be computed as fast as computing
sparse Cholesky decomposition of Ag−1A(cid:62) [45, 5], which can be O(n) time faster than computing
(Ag−1A(cid:62))−1 in many settings.

For simplicity, we focus on the case g(x) as a diagonal matrix, since we use the log-barrier φ(x) =
− (cid:80)m
i=1(log(xi − li) + log(ui − xi)) in implementation. We ﬁrst note that we maintain a “sparsity
pattern” sp(M ) of a sparse matrix M so that we handle only these entries in downstream tasks. The
sparsity pattern indicates “candidates” of nonzero entries of a matrix (i.e., sp(M ) ⊇ nnz(M ) =
{(i, j) : Mij (cid:54)= 0}). For instance, it is obvious that sp(cc(cid:62)) = {(i, j) : cicj (cid:54)= 0} = nnz(cc(cid:62)) for a
column vector c and that sp(Ag−1A(cid:62)) = (cid:83)
i ) follows from the equality Ag−1A(cid:62) =
(cid:80)n
i , where Mi denote the ith column of M (See Theorem 2.1 in [12]). Then
2 )(cid:62)
we compute the Cholesky decomposition to obtain a sparse triangular matrix L such that LL(cid:62) =
Ag−1A(cid:62) with a property sp(Ag−1A(cid:62)) ⊆ sp(L(cid:62)) ∪ sp(L) (See Theorem 4.2 in [12]).
Once the sparsity pattern of L is identiﬁed, we compute S := (Ag−1A(cid:62))−1|sp(L), the restriction of
S to sp(L), that is, the inverse matrix S is computed only for entries in sp(L). [45, 5] showed that
this matrix S can be computed as fast as the Cholesky decomposition of Ag−1A(cid:62).

i∈[n] sp(AiA(cid:62)

i=1(Ag− 1

2 )i(Ag− 1

18

For completeness, we explain how they compute S efﬁciently. Let L0DL(cid:62)
of Ag−1A(cid:62) such that the diagonals of L0 is one and so L = L0D 1

0 be the LDL decomposition

2 , and it easily follows that

S = D−1L−1

0 + (I − L(cid:62)

0 )S = D− 1

2 L−1 + (I − L(cid:62)D− 1

2 )−1S.

0

is lower triangular and I − L(cid:62)

2 . [45, 5] showed that the total cost of computing S is O((cid:80)n

Since D−1L−1
0 is strictly upper triangular, symmetry of S implies
that S can be computed from the bottom row to the top row one by one. We note that the computation
of S on any entry in sp(L) only requires previously computed S on entries in sp(L), due to the
sparsity pattern of I − L(cid:62)D− 1
i=1 n2
i )
for backward substitution, where ni is the number of nonzeros in the ith column of L. This exactly
matches the cost of computing L. In our experiments, for many sparse matrices A, we found that
O((cid:80)n
We have presented methods to save computational cost, avoiding full computation of the inverse
(Ag−1A(cid:62))−1. This attempt is justiﬁed by the fact that only entries of Ag−1A(cid:62) in sp(L) ∪ sp(L(cid:62))
matter in computing diag(A(cid:62)SA) = diag(A(cid:62)(Ag−1A(cid:62))−1A).
Lemma 5. Computation of diag(A(cid:62)(Ag−1A(cid:62))−1A) involves accessing only entries of
(Ag−1A(cid:62))−1 in sp(Ag−1A(cid:62)).

i ) is roughly O(n1.5) and it is much faster than dense matrix inverse.

i=1 n2

Proof. Let M := (Ag−1A(cid:62))−1 ∈ Rm×m, σi := (A(cid:62)(Ag−1A(cid:62))−1A)ii for i ∈ [n], and ai be the
ith column of A. Observe that
σi = a(cid:62)

i (Ag−1A(cid:62))−1ai = Tr(a(cid:62)

i M ai) = Tr(M aia(cid:62)

i ).

As the entries of M only in sp(aia(cid:62)
of M used for computing σi for all i ∈ [n] are included in (cid:83)n

i ) matter when computing the trace, we have that all the entries
i=1 sp(aia(cid:62)

i ) = sp(Ag−1A(cid:62)).

Now let us divide the diagonals of S by 2. Then we have (Ag−1A(cid:62))−1|sp(L)∪sp(L(cid:62)) = S + S(cid:62) and
thus

diag(A(cid:62)(Ag−1A(cid:62))−1A) = diag(A(cid:62)(Ag−1A(cid:62))−1|sp(L)∪sp(L(cid:62))A)
= diag(A(cid:62)SA + A(cid:62)S(cid:62)A) = 2 · diag(A(cid:62)SA)

and the last term can be computed efﬁciently using S. In our experiment, the cost of computing
leverage score is roughly twice the cost of computing Cholesky decomposition in all datasets.

Finally, we discuss another approach to compute leverage score with the same asymptotic complexity.
We consider the function

V (g) = log det Ag−1A(cid:62)
where g is a sparse matrix g ∈ Rsp(g) and V is deﬁned only on Rsp(g). Note that V (g) can be computed
using Cholesky decomposition of A(cid:62)g−1A(cid:62) and multiplying the diagonal of the decomposition.
Next, we note that

∇V (g) = −(g−1A(cid:62)(Ag−1A(cid:62))−1Ag−1)|sp(g).
Hence, we can compute leverage score by ﬁrst computing ∇V (g) via automatic differentiation, and
the time complexity of computing ∇V is only a small constant factor more than the time complexity of
computing V [18]. The only problem with this approach is that the Cholesky decomposition algorithm
is an algorithm involving a large loop and sparse operations and existing automatic differentiation
packages are not efﬁcient to differentiate such functions.

B.3 Stationarity of CRHMC

Now, the ideal CRHMC (or the continuous CRHMC) is the same as Algorithm 1 with the simpliﬁed
constrained Hamiltonian H in place of the unconstrained Hamiltonian. In this section, we prove that
the Markov chain deﬁned by the ideal CRHMC projected to x satisﬁes detailed balance with respect
to its target distribution proportional to e−f (x) subject to c(x) = 0, leading to the target distribution
being stationary.
To this end, we introduce a few notations here. Let M = {x ∈ Rn : c(x) = 0} be a manifold
in Rn and π(x) be a desired distribution on M proportional to e−f (x) satisfying (cid:82)
M π(x)dx = 1

19

(to be precise, the Radon-Nikodym derivative of π w.r.t. the Hausdorff measure on the manifold
M is proportional to e−f (x)). We denote the set of velocity v at x ∈ M (i.e., cotangent space)
by TxM = Null(Dc(x)) = {v ∈ Rn : Dc(x)M (x)†v = 0}. Let Th be the map sending (x, v)
to (x(cid:48), v(cid:48)) = (x(h), y(h)) in the Hamiltonian ODE (Step 2 of Algorithm 1) and deﬁne Fx,h(v) :=
(π1 ◦ Th)(x, v) = x(cid:48), where π1(x, v) := x is the projection to the position space x. For a matrix A,
we denote by |A| the absolute value of its determinant | det(A)|.

Note that we check the detailed balance of the induced chain on the “original (x)” space without
moving to the “phase (x, v)” space, unlike Brubaker’s proof [4].
Theorem 6. For x, x(cid:48) ∈ M, let Px(x(cid:48)) be the probability density of the one-step distribution to x(cid:48)
starting at x in CRHMC (i.e., transition kernel from x to x(cid:48)). It satisﬁes detailed balance with respect
to the desired distribution π (i.e., π(x)Px(x(cid:48)) = π(x(cid:48))Px(cid:48)(x)).

Proof. Fix x and x(cid:48) in M. Let C1 be the normalization constant of e−f (x) (i.e., π(x) = C1e−f (x)).
The transition kernel Px(x(cid:48)) is characterized as the pushforward by Fx,h of the probability measure
v ∼ N (0, M (x)) on TxM, so it follows that
e− 1

2 log pdet(M (x))− 1

2 v(cid:62)M (x)†v

(cid:90)

Px(x(cid:48)) = C2

Vx

|DFx,h(v)|
2 log pdet(M (x))− 1

dv,

2 v(cid:62)M (x)†v and Vx = {v ∈ TxM :
where C2 is the normalization constant of e− 1
Fx,h(v) = x(cid:48)} is the set of velocity in cotangent space at x such that the Hamiltonian ODE with step
size h sends (x, v) to (x(cid:48), v(cid:48)). (Further details for deducing the 1-step distribution can be found in
Lemma 10 of [31]) As c(x) = 0 for x ∈ M, it follows that

π(x)Px(x(cid:48))
(cid:90)

= C1C2

Vx

e−f (x)− 1

2 log pdet(M (x))− 1

2 v(cid:62)M (x)†v−λ(x,v)(cid:62)c(x)

|DFx,h(v)|

dv = C1C2

(cid:90)

Vx

e−H(x,v)
|DFx,h(v)|

dv.

Going forward, we use three important properties of the Hamiltonian dynamics including reversibility,
Hamiltonian preservation, and volume preservation, which still hold for the constrained Hamiltonian
H. Due to reversibility T−h(x(cid:48), v(cid:48)) = (x, v), we can write

π(x(cid:48))Px(cid:48)(x) = C1C2

(cid:90)

e−H(x(cid:48),v(cid:48))
|DFx(cid:48),−h(v(cid:48))|

dv(cid:48),

Vx(cid:48)

where Vx(cid:48) = {v(cid:48) ∈ Tx(cid:48)M : Fx(cid:48),−h(v(cid:48)) = x} is the counterpart of Vx. From reversibility T−h ◦ Th =
I, the inverse function theorem implies DT−h = (DTh)−1. Now let us denote

DTh(x, v) =

(cid:21)

(cid:20) A B
C D

& DT−h(x(cid:48), v(cid:48)) =

(cid:20) A(cid:48) B(cid:48)
C (cid:48) D(cid:48)

(cid:21)

,

where each entry is a block matrix with the same size. Note that DFx,h(v) = B and DFx(cid:48),−h(v(cid:48)) =
B(cid:48) hold by the deﬁnition of Jacobian. Together with DT−h = (DTh)−1, a formula for the inverse of
a block matrix results in

|DFx(cid:48),−h(v(cid:48))| = |B(cid:48)| =

|B|
|D||A − BD−1C|

=

|B|
|DTh(x, v)|

= |B| = |DFx,h(v)|,

where we use the property of volume preservation in the fourth equality (i.e., |DTh(x, v)| = 1).
Finally, the property of Hamiltonian preservation implies H(x, v) = H(x(cid:48), v(cid:48)) and thus

e−H(x(cid:48),v(cid:48))
|DFx(cid:48),−h(v(cid:48))|
Therefore, π(x)Px(x(cid:48)) = π(x(cid:48))Px(cid:48)(x) holds.

Vx(cid:48)

(cid:90)

dv(cid:48) =

(cid:90)

Vx

e−H(x,v)
|DFx,h(v)|

dv.

Similar reasoning as Theorem 3 and Lemma 1 in [4] gives π-irreducibility and aperiodicity of the
process, so CRHMC converges to the unique stationary distribution π ∝ e−f (x).

20

C Discretization

We discuss how to implement our Hamiltonian dynamics using the implicit midpoint method in
Section C.1 and present theoretical guarantees of correctness and efﬁciency of the discretized CRHMC
in Section C.2.

C.1 Discretized CRHMC based on Implicit Midpoint Integrator

In our algorithm, we discretize the Hamiltonian process into steps of step size h and run the process
for T iterations (see Algorithm 2). Rather than resampling the velocity at every step, we may change
the velocity more gradually, using the following update:

v(cid:48) ← (cid:112)βv + (cid:112)1 − βz
where z ∼ N (0, M (x)) and β is a parameter. We note that this step is time-reversible, i.e.,
P(v|x)P(v → v(cid:48)) = P(v(cid:48)|x)P(v(cid:48) → v) (see Theorem 8). Starting from (x(0), v(0)), let (x(t), v(t))
be the point obtained after iteration t. In the beginning of each iteration, we compute the Cholesky
decomposition of Ag(x)−1A(cid:62) for later use and resample the velocity with momentum. As noted
previously in Lemma 4, for c(x) = Ax − b we can just use the simpliﬁed Hamiltonian in (B.3),

H(x, v) = f (x) +

1
2

v(cid:62)g(x)− 1

2 (I − P (x)) g(x)− 1

2 v +

(cid:0)log det g(x) + log det Ag(x)−1A(cid:62)(cid:1)

1
2

instead of the constrained Hamiltonian H + λ(cid:62)c. We solve the Hamiltonian dynamics for H by the
implicit midpoint method, which we will discuss below, and then use a Metropolis ﬁlter on H to
ensure the distribution is correct.

Implicit Midpoint Method. For general Riemannian manifolds, explicit integrators such as the
leapfrog method (LM) are not symplectic, unlike IMM. LM is symplectic when the Hamiltonian
equations are separable (i.e., each of dx/dt and dv/dt is a function of either x or v only). However,
in the general Riemannian manifold setting, where dx/dt depends on position x due to mass matrices
(which is g(x) in our paper) as well as velocity v, the Hamiltonian is no longer separable, which
prevents us from using LM. We refer interested readers to Section 3 and Section 4.1 in [10].

We now elaborate on how the implicit midpoint integrator works (see Algorithm 3), which is
symplectic (so measure-preserving) and reversible [19]. Let us write H(x, v) = H 1(x, v)+H 2(x, v),
where

H 1(x, v) = f (x) +

1
2

(cid:0)log det g(x) + log det Ag(x)−1A(cid:62)(cid:1) ,

H 2(x, v) =

1
2

v(cid:62)g(x)− 1

2 (I − P (x)) g(x)− 1

2 v.

Starting from (x0, v0), in the ﬁrst step of the integrator, we run the process on the Hamiltonian H 1
∂H 1
2 to get (x1/3, v1/3), and this discretization leads to x1/3 = x0 + h
with step size h
∂v (x0, v0) and
∂H 1
∂x (x0, v0). Note that x1/3 = x0 due to ∂H 1
v1/3 = v0 − h
∂v = 0. In the second step of the integrator,
2
we run the process on H 2 with step size h by solving
(cid:18) x 1

(cid:19)

2

x 2
3

= x 1
3

+ h

∂H 2
∂v

v 2
3

= v 1
3

− h

(cid:18) x 1

3

∂H 2
∂x

3

+ x 2
3

2
+ x 2
3

2

v 1
3

+ v 2
3

,

,

(cid:19)

.

2
+ v 2
3

2

v 1
3

,

(cid:17)

(cid:16) x1/3+x2/3
2

To this end, starting from x2/3 = x1/3 and v2/3 = v1/3, we apply x2/3 ← x1/3 +
h ∂H 2
iteratively with
∂v

, v1/3+v2/3
, v1/3+v2/3
and v2/3 ← v1/3 − h ∂H 2
∂x
2
2
∂v and ∂H 2
the following subroutine for computing ∂H 2
∂x . According to Lemma 4, this compu-
tation involves solving g(x)−1A(cid:62) (cid:0)Ag(x)−1A(cid:62)(cid:1)−1
Ag(x)−1v for some v and x. To compute
(cid:0)Ag(x)−1A(cid:62)(cid:1)−1
Ag(x)−1v, we use the Newton’s method, which iteratively computes ν ←
ν + M −1Ag(x)−1 (cid:0)v − A(cid:62)ν(cid:1) for some M . Note that the Newton’s method guarantees that ν

(cid:16) x1/3+x2/3
2

(cid:17)

21

converges to M −1Ag(x)−1v if M is invertible. Here, we choose M = Ag(x(t))−1A(cid:62) to ensure fast
convergence. Since we have already computed the Cholesky decomposition of M in the beginning,
M −1Ag(x)−1 (cid:0)v − A(cid:62)ν(cid:1) can be computed efﬁciently by backward and forward substitution. In the
third step of the integrator, we run the process on the Hamiltonian H 1 with step size h
2 again to get
(x1, v1), which results in x1 = x2/3 and v1 = v2/3 − h
2
We note that CRHMC is afﬁne-invariant and provably independent of condition number (Theorem 15),
and thus the step size and momentum only need to depend on the dimension. In practice, we set the
momentum to roughly 1 − h, and for the step size h, we decrease it until the acceptance probability
is close enough to 1 during the warm-up phase. Empirically, we found that the step size stays
between 0.05 and 0.2 in practice even for high dimensional ill-conditioned polytopes. This step size
is remarkable, given that for these instances a standard package like STAN ends up selecting a small
step size like 10−8 and thus fails to converge.

∂H 1
∂x (x1, v2/3).

Putting Algorithm 2 and Algorithm 3 together, we obtain discretization of constrained Riemannian
Hamiltonian Monte Carlo algorithm.

Algorithm 2: Discretized Constrained Riemannian Hamiltonian Monte Carlo with Momentum
Input: Initial point x(0), velocity v(0), record frequency T , step size h, ODE steps K
for t = 1, 2, · · · , T do

Let v = v(t−1) and x = x(t−1).
// Step 1: Resample v with momentum
Let z ∼ N (0, M (x)). Update v:

v ← (cid:112)βv + (cid:112)1 − βz.

// Step 2: Solve dx

dt = ∂H(x,v)

∂v

dt = − ∂H(x,v)
, dv

∂x

via the implicit midpoint

method

Use Implicit Midpoint Method(x, v, h, K) to ﬁnd (x(cid:48), v(cid:48)) such that

v 1
3

= v −

h
2

x(cid:48) = x + h

∂H 1(x, v)
∂x
∂H 2( x+x(cid:48)
2

, v1/3+v2/3
2
∂v
∂H 1(x(cid:48), v 2
∂x

)

.

3

v(cid:48) = v 2

3

−

h
2

,

(C.1)

)

, v 2
3

= v 1
3

− h

∂H 2( x+x(cid:48)
2

, v1/3+v2/3
2
∂x

)

,

// Step 3: Filter
(cid:110)
1, e−H(x(cid:48),v(cid:48) )

With probability min
Otherwise, set x(t) ← x and v(t) ← −v.

e−H(x,v)

(cid:111)

, set x(t) ← x(cid:48) and v(t) ← v(cid:48).

end
Output: x(T )

C.2 Theoretical Guarantees

In terms of efﬁciency, we ﬁrst show that one iteration of Algorithm 2 incurs the cost of solving
a few Cholesky decomposition and O(K) sparse triangular systems. We also show in Lemma 9
that the implicit midpoint integrator converges to the solution of Eq. (C.1) in logarithmically many
iterations. Regarding correctness, Theorem 8 and Lemma 9 together show that the discretized
CRHMC (Algorithm 2) converges to the stationary distribution indeed (see Remark 10).

Theorem 7. The cost of each iteration of Algorithm 2 is solving O(1) Cholesky decomposition and
O(K) triangular systems, where K is the number of iterations in Algorithm 3.

22

Algorithm 3: Implicit Midpoint Method
Input: Initial point x, velocity v, step size h, ODE steps K
dt = ∂H 1(x,v)
// Step 1: Solve dx
← x and v 1
Set x 1

dt = − ∂H 1(x,v)
, dv

∂v
∂H 1(x,v)
∂x

← v − h
2

∂x

.

3

3

(cid:16)

// Step 2: Solve dx
Set ν ← 0.
for k = 1, 2, · · · , K do
Let xmid ← 1
+ x 2
2
3
Set ν ← ν + (cid:0)LL(cid:62)(cid:1)−1
← x 1
Set x 2
3
← v 1
and v 2
3

x 1
3

3

3

end

dt = ∂H 2(x,v)

∂v

dt = − ∂H 2(x,v)
, dv

∂x

via implicit midpoint

(cid:17)

(cid:16)

(cid:17)

and vmid ← 1
2

+ v 2
3
Ag(xmid)−1 (cid:0)vmid − A(cid:62)ν(cid:1)

v 1
3

+ hg(xmid)−1 (cid:0)vmid − A(cid:62)ν(cid:1)
+ h

2 Dg(xmid) (cid:2)g(xmid)−1 (cid:0)vmid − A(cid:62)ν(cid:1) , g(xmid)−1 (cid:0)vmid − A(cid:62)ν(cid:1)(cid:3)

// Step 3: Solve dx
and v1 ← v 2
Set x1 ← x 2
Output: x1,v1

3

dt = ∂H 1(x,v)
∂v
∂H 1
− h
∂x (x 2
2

dt = − ∂H 1(x,v)
, dv
, v 2
3

).

∂x

3

3

Proof. We ﬁrst solve the Cholesky decomposition to get Lt−1L(cid:62)
beginning of iteration. Recall that

t−1 = Ag(x(t−1))−1A(cid:62) at the

H(x, v) = H 1(x, v) + H 2(x, v)

(cid:18)

=

f (x) +

1
2

(log det g(x) + log det Ag(x)−1A(cid:62))

(cid:19)

+

(cid:18) 1
2

v(cid:62)g(x)− 1

2

(cid:16)

I − g(x)− 1

2 A(cid:62)(Ag(x)−1A(cid:62))−1Ag(x)− 1

2

(cid:17)

g(x)− 1

2 v

(cid:19)

.

t−1(L−1

t−1(Ag(x)− 1

The value of H(x(t−1), v(t−1)) should be computed later for the ﬁlter step and can be efﬁciently
t−1 = Ag(x(t−1))−1A(cid:62)and solving two sparse triangular systems
computed by the given Lt−1L(cid:62)
(i.e., L−(cid:62)
2 ))). We need the same cost (i.e., Cholesky decomposition and solving two
triangular systems) for the value of H(x(cid:48), v(cid:48)), where (x(cid:48), v(cid:48)) is the output of Algorithm 3. We note
that L inherits sparsity of A and thus each triangular system can be solved efﬁciently by backward
and forward substitution.
In the implicit midpoint method, one main component is computation of ∂H 1(x,v)
in Step 1 and
∂H 1
) in Step 3 due to leverage scores. As seen in Section B.2.3, the cost for these computa-
∂x (x 2
tions is within a constant factor of solving the Cholesky decomposition for Ag(x(t−1))−1A(cid:62) and
)−1A(cid:62). Another component is solving O(K) triangular systems to update ν in Step 2.
Ag(x 2
3

, v 2
3

∂x

3

Adding up all these costs, each iteration of Algorithm 2 only requires solving O(1) Cholesky
decomposition and O(K) sparse triangular systems.

Theorem 8. The Markov chain deﬁned by Algorithm 2 projected to x has a stationary density pro-
portional to exp(−f (x)), and is irreducible and aperiodic. Therefore, this Markov chain converges
to the stationary distribution.

Proof. Each iteration consists of two stages: resampling velocity with momentum in Step 1 (i.e.,
(x, v) to (x, v)) and solving ODE followed by the ﬁlter in Step 2 and 3 (i.e., (x, v) to (x(cid:48), v(cid:48))). To
prove the claim, we show that Step 1 is time-reversible with respect to the conditional distribution
π(v|x) and that Step 2 followed by Step 3 is also time-reversible with respect to π(x, v).

We begin with the ﬁrst part. We have π(v|x) = N (0, M (x)) due to the deﬁnition of H. Since v|x ∼
1 − βz
N (0, M (x)) and z ∼ N (0, M (x)) are independent Gaussians, the update rule v =

βv +

√

√

23

implies π(v|x) = N (0, M (x)). Let P(z) be the probability density and C be the normalization
constant for Gaussian N (0, M (x)). Then, the time-reversibility w.r.t. π(v|x) is immediate from the
following computation:

π(v|x)P(v → v) = C 2 exp(−

1
2

v(cid:62)M †v) · exp(−

= C 2 exp

= C 2 exp

(cid:18)

(cid:18)

−

−

1
2

1
2

(cid:18)

v(cid:62)M †v +

(cid:18) v(cid:62)M †v
1 − β

+

= C 2 exp

= C 2 exp

(cid:18)

(cid:18)

−

−

1
2

1
2

(cid:18)

v(cid:62)M †v +

(cid:18) v(cid:62)M †v
1 − β

+

(v(cid:62)M †v + v(cid:62)M †v)

(cid:19)(cid:19)

√

+

−
√

+

−

(v −

1
2
v(cid:62)M †v
1 − β
v(cid:62)M †v
1 − β

(v −

1
2
v(cid:62)M †v
1 − β
v(cid:62)M †v
1 − β

−

βv)(cid:62)M †(v −
1 − β
βv(cid:62)M †v
1 − β
√

β
1 − β
βv)(cid:62)M †(v −
1 − β
βv(cid:62)M †v
1 − β
√

−

β
1 − β

√

βv)

)

√

β
1 − β

βv)

)

√

β
1 − β

(v(cid:62)M †v + v(cid:62)M †v)
√

(cid:19)(cid:19)

,

(v(cid:62)M †v + v(cid:62)M †v)

(cid:19)(cid:19)

(v(cid:62)M †v + v(cid:62)M †v)

(cid:19)(cid:19)

π(v|x)P(v → v) = C 2 exp(−

1
2

v(cid:62)M †v) · exp(−

=⇒ π(v|x)P(v → v) = π(v|x)P(v → v).

The second part follows from a stronger statement due to symmetry of v in H(x, v): In the space
where (x, v) and (x, −v) are identiﬁed, the Markov chain deﬁned by Step 2 and 3 satisﬁes detailed
balance with respect the density π([x, v]) proportional to exp(−H(x, v)), where [x, v] denotes the
identiﬁed point for (x, v) and (x, −v). Consider the pairs [x, v] = {(x, v), (x, −v)} and [x(cid:48), v(cid:48)] =
{(x(cid:48), v(cid:48)), (x(cid:48), −v(cid:48))} where in Step 2 (x, v) goes to (x(cid:48), v(cid:48)) and (x(cid:48), −v(cid:48)) goes to (x, −v) due to
reversibility of the implicit midpoint method. We now verify that the ﬁltering probability is the same
in either direction, using the measure-preserving property of Step 2

π(x, v)P ((x, v) → (x(cid:48), v(cid:48))) = π(x, v) min

(cid:27)

(cid:26)

1,

π(x(cid:48), v(cid:48))
π(x, v)
= min {π(x, v), π(x(cid:48), v(cid:48))}
= min {π(x, −v), π(x(cid:48), −v(cid:48))}

= π(x(cid:48), −v(cid:48)) min

(cid:26)

1,

(cid:27)

π(x, −v)
π(x(cid:48), −v(cid:48))

= π(x(cid:48), −v(cid:48))P ((x(cid:48), −v(cid:48)) → (x, −v)) .

for any two pairs [x, v] and [x(cid:48), v(cid:48)], we have π([x, v])P ([x, v] → [x(cid:48), v(cid:48)]) =
Therefore,
π([x(cid:48), v(cid:48)]P ([x(cid:48), v(cid:48)] → [x, v]), and thus this detailed balance implies that the target density is station-
ary.

Its irreducibility is implied by the non-zero lower bound on the conductance of the discretized
CRHMC (Theorem 15). To see this, let A and B be two subsets of positive measure such that one
subset is not reachable from another in inﬁnitely many steps. Take the set R of reachable points from
A via running the Markov chain, and note that R and Rc(⊇ B) have non-zero measures. However,
the non-zero conductance, meaning that there must be a positive probability of stepping out of R,
which contradicts the deﬁnition of R. Now for aperiodicity, as assumed at the beginning of the mixing
rate proof (Appendix D), we consider a lazy version of the discretized CRHMC instead, which makes
the chain stay where it is at with probability 1/2 at each iteration, which prevents potential periodicity
of the process. Note that this modiﬁcation worsens the mixing rate only by a factor of 2.

Putting these three together, we can show that the discretized CRHMC converges to the target
distribution.

Now we show in Lemma 9 that the implicit midpoint method (Algorithm 3) converges to the solution
of (C.1) in logarithmically many iterations. To show the convergence of Algorithm 3, we denote by
T the map induced by one iteration of Step 2.

24

Deﬁnition 1. Let

T (x, v, ν) =





v 1
3

+ hg(xmid)−1(vmid − A(cid:62)λ1)
2 Dg(xmid)[g(xmid)−1(vmid − A(cid:62)λ1), g(xmid)−1(vmid − A(cid:62)λ1)]

x 1
3

+ h

λ1



 ,

where xmid = 1
, v∗
Let (x∗

+ x), vmid = 1

2 (v 1
, ν∗) be the ﬁxed point of T .

2 (x 1

3

3

2
3

2
3

+ v), and λ1 = ν + (LL(cid:62))−1Ag(xmid)−1 (cid:0)vmid − A(cid:62)ν(cid:1).

We assume that g is given by the Hessian of a highly self-concordant barrier φ (see E.2). Note that
the log-barrier is highly self-concordant. We can show that for small enough step size h, Algorithm 3
can solve (C.1) to δ-accuracy in logarithmically many iterations.
Lemma 9. Suppose g(x) = ∇2φ(x) for some highly self-concordant barrier φ. For any input
, ν(k)) be points obtained after k iterations in Step 2 of Algorithm 3. Let
(x 1
3
((cid:101)x 2

, v(k)
), let (x(k)
) be the solution for (x 2

) in the following equation

, v 1
3
, (cid:101)v 2

, v 2
3

2
3

2
3

3

3

3

x 2

3

= x 1

3

+ h

∂H 2
∂v

(cid:18) x 1

3

+ x 2
3

2

v 1
3

+ v 2
3

(cid:19)

2

,

, v 2

3

= v 1

3

− h

∂H 2
∂x

(cid:18) x 1

3

+ x 2
3

2

v 1
3

+ v 2
3

(cid:19)

2

.

,

√

Let (cid:107)x(cid:107)A :=

x(cid:62)Ax for a matrix A. For any (x, v, ν), deﬁne the norm

(cid:107)(x, v, λ)(cid:107) := (cid:107)x(cid:107)g(x 1

3

) + (cid:107)v(cid:107)g(x 1

3

)−1 + h(cid:107)A(cid:62)ν(cid:107)g(x 1

3

)−1.

If

(cid:13)
(cid:13)(x(0)
(cid:13)

2
3

, v(0)

2
3

, ν(0)) − ((cid:101)x 2

3

, (cid:101)v 2

3

, ν∗)

(cid:13)
(cid:13) ≤ r with h ≤ r ≤ min( 1
(cid:13)
10 ,

√
h
4 ,

(cid:107)v∗(cid:107)g(x0)−1
4

), then

(cid:13)
(cid:13)(x(L)
(cid:13)

2
3

, v(L)

2
3

, ν(L)) − ((cid:101)x 2

3

, (cid:101)v 2

3

, ν∗)

(cid:13)
(cid:13)
(cid:13) ≤ δ

for some L = O

(cid:16)

log1/C

(cid:17)

r
δ

, where C = On(h) is the Lipschitz constant of the map T .

Proof. Since (x∗

2
3

, v∗

2
3

, ν∗) is the ﬁxed point of T (i.e., ν∗ = λ1), we have

ν∗ = ν∗ + (LL(cid:62))−1Ag(xmid)−1 (cid:0)vmid − A(cid:62)ν∗(cid:1)

and thus Ag(xmid)−1vmid = Ag(xmid)−1A(cid:62)ν∗. For invertible Ag(xmid)−1A(cid:62), we have

ν∗ = (cid:0)Ag(xmid)−1A(cid:62)(cid:1)−1
Similarly by using the deﬁnition of the ﬁxed point and this new formula for ν∗,

Ag(xmid)−1vmid.

x∗

2
3

= x 1
3

= x 1
3

+ hg(xmid)−1vmid − hg(xmid)−1A(cid:62)ν∗
+ hg(xmid)−1vmid − hg(xmid)−1A(cid:62) (cid:0)Ag(xmid)−1A(cid:62)(cid:1)−1

Ag(xmid)−1vmid

= x 1
3

+ h

∂H 2
∂v

(xmid, vmid)

and

v∗

2
3

= v 1
3

+

h
2

Dg(xmid)[g(xmid)−1(vmid − A(cid:62)ν∗), g(xmid)−1(vmid − A(cid:62)ν∗)]

= v 1
3

− h

∂H 2
∂x

(xmid, vmid)

which shows that (x∗

2
3

, v∗

2
3

) is exactly the solution for (x, v) in the equation

x = x 1

3

+ h

∂H 2
∂v

(cid:18) x 1

3

+ x

2

v 1

3

,

+ v

(cid:19)

2

, v = v 1

3

− h

∂H 2
∂x

(cid:18) x 1

3

+ x

2

v 1

3

,

+ v

(cid:19)

2

.

25

Next, we
(cid:13)
(cid:13)(x(0)
, v(0)
(cid:13)

2
3

show that

, ν(0)) − (x∗

, v∗

2
3

2
3

2
3
(cid:13)
(cid:13)(x((cid:96))
(cid:13)

2
3

, v((cid:96))

2
3

, ν((cid:96))) − (x∗

to (x∗

2
3

, v∗

2
3

, ν∗).

If

the

, v∗

in Step 2 converges

iterations
(cid:13)
(cid:13)
, ν∗)
(cid:13) ≤ r for some C = On(h), we have
(cid:13)
(cid:13)T (x((cid:96)−1)
(cid:13)
(cid:13)
(cid:13)(x((cid:96)−1)
(cid:13)
≤ C
≤ C (cid:96) (cid:13)
(cid:13)(x(0)
(cid:13)

, v((cid:96)−1)

, v((cid:96)−1)

(cid:13)
(cid:13)
(cid:13) =

, v(0)

, ν∗)

2
3

2
3

2
3

2
3

2
3

2
3

2
3

2
3

, ν((cid:96))) − T (x∗

, v∗

2
3

2
3

, ν((cid:96)−1)) − (x∗

, ν(0)) − (x∗

, v∗

2
3

2
3

, ν∗)

(cid:13)
, ν∗)
(cid:13)
(cid:13)
(cid:13)
, ν∗)
(cid:13)
(cid:13)

2
3

, v∗
2
3
(cid:13)
(cid:13)
(cid:13) ,

where the ﬁrst equality follows from (x∗

2
3

, v∗

2
3

, ν∗) is the ﬁxed point of T and the second inequality

(cid:13)
(cid:13)(x(L)
(cid:13)

, v(L)

(cid:13)
(cid:13)
(cid:13) ≤ δ for L =

, v∗

, ν∗)

, ν(L)) − (x∗

2
3

2
3

2
3

2
3

r
δ

(cid:1) .

follows from Lemma 12. Therefore, we have
O (cid:0)logC
Remark 10. Lemma 9 shows that Algorithm 3 converges to the solution of (C.1) in logarithmically
many iterations for small enough step size h. In Step 1 of Algorithm 2, v is resampled so that every
iteration of Algorithm 2 is a non-degenerate map. Then, the total variation distance between the
distributions generated by solving (C.1) using Algorithm 3 and solving (C.1) exactly in one iteration
of Algorithm 2 can be bounded by error due to Algorithm 3. Theorem 8 shows that the process
will converge to the exact stationary distribution. Therefore, in order for the accumulated error of
Algorithm 2 to remain bounded for polynomially many steps, it sufﬁces to run logarithmically many
iterations in Algorithm 3. Any small bias due to the numerical error in the ODE computation is
corrected by the ﬁlter, and maintaining as small error as possible is important to keep the acceptance
probability high.

C.3 Deferred Proof

Lemma 11 ([28], Lemma 28). Suppose g(x) = ∇2φ(x) for some highly self-concordance barrier φ.
Then, we have that

• (1 − (cid:107)y − x(cid:107)g(x))2g(x) (cid:22) g(y) (cid:22)

1

(1−(cid:107)y−x(cid:107)g(x))2 g(x).

• (cid:107)Dg(x)[v, v](cid:107)g(x)−1 ≤ 2(cid:107)v(cid:107)2

g(x).

• (cid:107)Dg(x)[v, v] − Dg(y)[v, v](cid:107)g(x)−1 ≤

(1−(cid:107)y−x(cid:107)g(x))3 (cid:107)v(cid:107)2

6

g(x)(cid:107)y − x(cid:107)g(x).

• (cid:107)Dg(x)[v, v] − Dg(x)[w, w](cid:107)g(x)−1 ≤ 2 (cid:107)v − w(cid:107)g(x) (cid:107)v + w(cid:107)g(x).

Lemma 12. Let g(x) = ∇2φ(x) for some highly self-concordance barrier φ. Given x0, v0 and L
such that LL(cid:62) = Ag(x0)−1A(cid:62), consider the map

T (x, v, λ) =





x0 + hg(x1/2)−1(v1/2 − A(cid:62)λ1)

v0 + h

2 Dg(x1/2)[g(x1/2)−1(v1/2 − A(cid:62)λ1), g(x1/2)−1(v1/2 − A(cid:62)λ1)]

λ1





where x1/2 = (x0 + x)/2, v1/2 = (v0 + v)/2 and λ1 = λ + (LL(cid:62))−1Ag(x1/2)−1 (cid:0)v1/2 − A(cid:62)λ(cid:1).
Let (x∗, v∗, λ∗) be a ﬁxed point of T . For any x, v, λ, we deﬁne the norm

(cid:107)(x, v, λ)(cid:107) = (cid:107)x(cid:107)g(x0) + (cid:107)v(cid:107)g(x0)−1 + h(cid:107)A(cid:62)λ(cid:107)g(x0)−1 .

Let Ω = {(x, v, λ) : (cid:107)(x, v, λ) − (x∗, v∗, λ∗)(cid:107) ≤ r} with h ≤ r ≤ min( 1
10 ,
Suppose that (x0, v0, 0) ∈ Ω. Then, for any (x, v, λ), (x, v, λ) ∈ Ω, we have

√
h
4 ,

(cid:107)v∗(cid:107)g(x0)−1
4

).

(cid:107)T (x, v, λ) − T (x, v, λ)(cid:107) ≤ C(cid:107)(x, v, λ) − (x, v, λ)(cid:107)

h + (cid:107)v∗(cid:107)g(x0)−1)(400r + 18h(cid:107)v∗(cid:107)g(x0)−1 ).

where C = ( 3r
Remark 13. Note that we should think r = Θn(h) because that is the distance between (x0, v0, 0)
and (x∗, v∗, λ∗). In that case, the Lipschitz constant of T is On(h(cid:107)v∗(cid:107)2
g(x0)−1 ) = On(h). Hence, if
the step size h is small enough, then T is a contractive mapping. In practice, we can take h close to a
constant because g is decomposable into barriers in each dimension and the bound can be improved
using this.

26

Proof. We use T (x, v, λ)x to denote the x component of T (x, v, λ) and similarly for T (x, v, λ)v
and T (x, v, λ)λ. For simplicity, we write g0 = g(x0), g1/2 = g(x1/2) and g1/2 = g(x1/2). By the
assumption, we have that

(cid:107)x − x0(cid:107)g0 ≤ (cid:107)x − x∗(cid:107)g0 + (cid:107)x∗ − x0(cid:107)g0 ≤ 2r.

Similarly, (cid:107)x − x0(cid:107)g0 ≤ 2r.
We ﬁrst bound T (x, v, λ)λ. Note that

T (x, v, λ)λ − T (x, v, λ)λ = α1 + α2 + α3 + α4

where

0 A(cid:62))(λ − λ),

α1 = (I − (LL(cid:62))−1Ag−1
α2 = (LL(cid:62))−1Ag−1
α3 = (LL(cid:62))−1A(g−1
α4 = (LL(cid:62))−1A(g−1

0 (v1/2 − v1/2),
1/2 − g−1
1/2 − g−1

0 )((v1/2 − A(cid:62)λ) − (v1/2 − A(cid:62)λ)),
1/2)(v1/2 − A(cid:62)λ).

Using that LL(cid:62) = Ag(x0)−1A(cid:62), we have α1 = 0. For α2, we have

(cid:107)A(cid:62)α2(cid:107)2

g−1
0

= (v1/2 − v1/2)(cid:62)g−1
= (v1/2 − v1/2)(cid:62)g−1
≤ (v1/2 − v1/2)(cid:62)g−1

0 A(cid:62)(LL(cid:62))−1Ag−1
0 A(cid:62)(Ag−1
0 (v1/2 − v1/2)

0 A(cid:62))−1Ag−1

0 (v1/2 − v1/2)

0 A(cid:62)(L(cid:62)L)−1Ag−1

0 (v1/2 − v1/2)

=

(cid:107)v − v(cid:107)2

1
4
where we use LL(cid:62) = Ag(x0)−1A(cid:62) and g−1/2
for B = Ag−1/2

g−1
0

0

0

. For α3, by self-concordance of g (Lemma 11) and (cid:107)x − x0(cid:107)g0 ≤ 2r, we have

A(cid:62)(Ag−1

0 A(cid:62))−1Ag−1/2

0

= B(cid:62)(BB(cid:62))−1B (cid:22) I

(1 − r)2g0 (cid:22) g1/2 (cid:22)

1
(1 − r)2 g0

(C.2)

)2 (cid:22) ((1 − r)−2 − 1)2I. Using this and P =

0 )g1/2

0

(cid:22) I, we have

and hence (g1/2
g−1/2
0

0
A(cid:62)(Ag−1

(g−1
1/2 − g−1
0 A(cid:62))−1Ag−1/2
= (cid:107)g1/2
(cid:107)A(cid:62)α3(cid:107)g−1
0
≤ (cid:107)g1/2
0

0

0

(g−1

1/2 − g−1
1/2 − g−1

0 )((v1/2 − A(cid:62)λ) − (v1/2 − A(cid:62)λ))(cid:107)P
0 )((v1/2 − A(cid:62)λ) − (v1/2 − A(cid:62)λ))(cid:107)2

(g−1

≤ ((1 − r)−2 − 1)(cid:107)g−1/2

((v1/2 − A(cid:62)λ) − (v1/2 − A(cid:62)λ))(cid:107)2

0
1
2

(cid:107)v − v(cid:107)g−1

0

+ (cid:107)A(cid:62)(λ − λ)(cid:107)g−1

0

).

≤ ((1 − r)−2 − 1)(

Using r ≤ 1/10, we have

(cid:107)A(cid:62)α3(cid:107)g−1

0

≤ 1.2r(cid:107)v − v(cid:107)g−1

0

+ 2.4r(cid:107)A(cid:62)(λ − λ)(cid:107)g−1

0

.

For α4, similarly, we have

(cid:107)A(cid:62)α4(cid:107)g−1

0

≤ ((1 − 0.5(cid:107)x − x(cid:107)g1/2 )−2 − 1)(cid:107)v1/2 − A(cid:62)λ(cid:107)g−1
≤ ((1 − 0.6(cid:107)x − x(cid:107)g0 )−2 − 1)(cid:107)v1/2 − A(cid:62)λ(cid:107)g−1
≤ 1.5(cid:107)x − x(cid:107)g0(cid:107)v1/2 − A(cid:62)λ(cid:107)g−1

0

0

0

where we used g1/2 (cid:22) 1.2g0 (by (C.2)) in the second inequality and (cid:107)x − x(cid:107)g0 ≤ (cid:107)x − x∗(cid:107)g0 +
(cid:107)x − x∗(cid:107)g0 ≤ 1

5 at the end. Combining everything, we have

(cid:107)A(cid:62)(T (x, v, λ)λ − T (x, v, λ)λ)(cid:107)g−1

0

= (cid:107)A(cid:62)(λ1 − λ1)(cid:107)g−1

0

27

≤ 0.7(cid:107)v − v(cid:107)g−1

0

+ 2.4r(cid:107)A(cid:62)(λ − λ)(cid:107)g−1

0

+ 1.5(cid:107)x − x(cid:107)g0(cid:107)v1/2 − A(cid:62)λ(cid:107)g−1

0

.

(C.3)

Now we bound T (x, v, λ)x. Note that

where

T (x, v, λ)x − T (x, v, λ)x = hβ1 + hβ2

β1 =g−1
β2 =(g−1

1/2((v1/2 − A(cid:62)λ1) − (v1/2 − A(cid:62)λ1)),
1/2 − g−1
1/2)(v1/2 − A(cid:62)λ1).

By a proof similar to above, we have

(cid:107)β1(cid:107)g0 ≤ 1.2((cid:107)v1/2 − v1/2(cid:107)g−1
(cid:107)β2(cid:107)g0 ≤ 0.6(cid:107)x − x(cid:107)g0 (cid:107)v1/2 − A(cid:62)λ1(cid:107)g−1

+ (cid:107)A(cid:62)(λ1 − λ1)(cid:107)g−1
.

0

0

0

),

and thus

(cid:107)T (x, v, λ)x − T (x, v, λ)x(cid:107)g0
≤ 0.6h(cid:107)v − v(cid:107)g−1

+ 1.2h(cid:107)A(cid:62)(λ1 − λ1)(cid:107)g−1

0

+ 0.6h(cid:107)x − x(cid:107)g0 (cid:107)v1/2 − A(cid:62)λ1(cid:107)g−1

0

.

0

Finally, we bound T (x, v, λ)v. We split the term

T (x, v, λ)v − T (x, v, λ)v =

h
2

γ1 +

h
2

γ2

where

γ1 =Dg(x1/2)[g−1

1/2(v1/2 − A(cid:62)λ1), g−1

1/2(v1/2 − A(cid:62)λ1)]

− Dg(x1/2)[g−1

1/2(v1/2 − A(cid:62)λ1), g−1

1/2(v1/2 − A(cid:62)λ1)],

γ2 =Dg(x1/2)[g−1

1/2(v1/2 − A(cid:62)λ1), g−1

1/2(v1/2 − A(cid:62)λ1)]

− Dg(x1/2)[g−1
1/2(v1/2 − A(cid:62)λ1) and η = g−1

1/2(v1/2 − A(cid:62)λ1), g−1
1/2(v1/2 − A(cid:62)λ1)].
1/2(v1/2 − A(cid:62)λ1). For γ1, we have that

Let η = g−1

(cid:107)Dg(x1/2)[η, η] − Dg(x1/2)[η, η](cid:107)g−1

1/2

≤ 2(cid:107)Dg(x1/2)[η − η, η](cid:107)g−1
≤ 4(cid:107)η − η(cid:107)g1/2 (cid:107)η(cid:107)g1/2 + 2(cid:107)η − η(cid:107)2
where we use Lemma 11. Using g1/2 (cid:22) 1.2g0 (by (C.2)),

1/2

g1/2

+ (cid:107)Dg(x1/2)[η − η, η − η](cid:107)g−1

1/2

(cid:107)γ1(cid:107)g−1

0

≤4(cid:107)(v1/2 − A(cid:62)λ1) − (v1/2 − A(cid:62)λ1)(cid:107)g−1
+ 2(cid:107)(v1/2 − A(cid:62)λ1) − (v1/2 − A(cid:62)λ1)(cid:107)2

0

.

g−1
0

(cid:107)v1/2 − A(cid:62)λ1(cid:107)g−1

0

For γ2, we use Lemma 11 and get

(cid:107)γ2(cid:107)g−1

0

≤

4

(1 − 0.6(cid:107)x − x(cid:107)g0)3 (cid:107)v1/2 − A(cid:62)λ1(cid:107)2

g−1
0

(cid:107)x − x(cid:107)g0

≤ 6(cid:107)v1/2 − A(cid:62)λ1(cid:107)2

g−1
0

(cid:107)x − x(cid:107)g0.

Combining everything, we have

(cid:107)T (x, v, λ)v − T (x, v, λ)v(cid:107)g−1

0

≤ 2h(cid:107)(v1/2 − A(cid:62)λ1) − (v1/2 − A(cid:62)λ1)(cid:107)g−1
+ h(cid:107)(v1/2 − A(cid:62)λ1) − (v1/2 − A(cid:62)λ1)(cid:107)2
+ 3h(cid:107)v1/2 − A(cid:62)λ1(cid:107)2

(cid:107)x − x(cid:107)g0

0

g−1
0

g−1
0

(cid:107)v1/2 − A(cid:62)λ1(cid:107)g−1

0

28

Combining the bounds for Tλ, Tx, Tv, we have

(cid:107)T (x, v, λ) − T (x, v, λ)(cid:107)

≤ 0.7h(cid:107)v − v(cid:107)g−1

0

+ 2.4rh(cid:107)A(cid:62)(λ − λ)(cid:107)g−1

0

+ 1.5h(cid:107)x − x(cid:107)g0 (cid:107)v1/2 − A(cid:62)λ(cid:107)g−1

0

0

+ 1.2h(cid:107)A(cid:62)(λ1 − λ1)(cid:107)g−1
+ 0.6h(cid:107)v − v(cid:107)g−1
+ 2h(cid:107)(v1/2 − A(cid:62)λ1) − (v1/2 − A(cid:62)λ1)(cid:107)g−1
+ h(cid:107)(v1/2 − A(cid:62)λ1) − (v1/2 − A(cid:62)λ1)(cid:107)2
+ 3h(cid:107)v1/2 − A(cid:62)λ1(cid:107)2

(cid:107)x − x(cid:107)g0.

g−1
0

0

0

g−1
0

+ 0.6h(cid:107)x − x(cid:107)g0 (cid:107)v1/2 − A(cid:62)λ1(cid:107)g−1
(cid:107)v1/2 − A(cid:62)λ1(cid:107)g−1

0

0

To simplify the terms, we note that
(cid:107)v1/2 − A(cid:62)λ1(cid:107)g−1

0

0

≤(cid:107)v1/2 − A(cid:62)λ(cid:107)g−1
=(cid:107)v1/2 − A(cid:62)λ(cid:107)g−1
≤(cid:107)v1/2 − A(cid:62)λ(cid:107)g−1
≤3(cid:107)v1/2 − A(cid:62)λ(cid:107)g−1

0

0

0

.

+ (cid:107)A(cid:62)(LL(cid:62))−1Ag−1
1/2

(cid:0)v1/2 − A(cid:62)λ(cid:1) (cid:107)g−1

0

+ (cid:107)g1/2

+ (cid:107)g1/2

0 g−1
1/2
0 g−1
1/2

(cid:0)v1/2 − A(cid:62)λ(cid:1) (cid:107)P
(cid:0)v1/2 − A(cid:62)λ(cid:1) (cid:107)2

Using this and simplifying, we have

(cid:107)T (x, v, λ) − T (x, v, λ)(cid:107)

≤1.3h(cid:107)v − v(cid:107)g−1

0

+ 2.4rh(cid:107)A(cid:62)(λ − λ)(cid:107)g−1

0

+ 3.3h(cid:107)x − x(cid:107)g0(cid:107)v1/2 − A(cid:62)λ(cid:107)g−1

0

+ 1.2h(cid:107)A(cid:62)(λ1 − λ1)(cid:107)g−1

0

(cid:107)v − v(cid:107)g−1

0

+ (cid:107)A(cid:62)(λ1 − λ1)(cid:107)g−1

0

)(cid:107)v1/2 − A(cid:62)λ(cid:107)g−1

0

+ 6h(

1
2
+ h(cid:107)v − v(cid:107)2

+ 2h(cid:107)A(cid:62)(λ1 − λ1)(cid:107)2

g−1
0

g−1
0

+ 27h(cid:107)v1/2 − A(cid:62)λ(cid:107)2

g−1
0

(cid:107)x − x(cid:107)g0.

Next, we note that

(cid:107)v1/2 − A(cid:62)λ(cid:107)g−1

0

≤

1
2

+

1
2

(cid:107)v0 − v∗(cid:107)g−1

0

+ (cid:107)v∗(cid:107)g−1

0

0

(cid:107)v − v∗(cid:107)g−1
1
2

+

+

(cid:107)A(cid:62)λ − A(cid:62)λ∗(cid:107)g−1
r
1
2h
2

r + (cid:107)v∗(cid:107)g−1

+

0

0

(cid:107)A(cid:62)λ − A(cid:62)λ∗(cid:107)g−1
r
2h

3r
h

r
h

≤

+

0

+ (cid:107)v∗(cid:107)g−1

0

+ (cid:107)A(cid:62)λ∗(cid:107)g−1

0

≤

1
2

r +

1
2

+

Using this, (C.3), h ≤ r, r2 ≤ h

16 , r ≤ (cid:107)v∗(cid:107)g−1

0

/4, we have

(cid:107)A(cid:62)(λ1 − λ1)(cid:107)g−1

0

≤ (cid:107)v − v(cid:107)g−1

0

3r2
h
Hence, we can further simplify it to

≤ r +

+

(cid:107)T (x, v, λ) − T (x, v, λ)(cid:107)

+ 3r(cid:107)A(cid:62)(λ − λ)(cid:107)g−1
5r2
h

+ 2r(cid:107)v∗(cid:107)g−1

≤

0

0

8r2
h

+ (

5r
h

+ 2(cid:107)v∗(cid:107)g−1

0

)(cid:107)x − x(cid:107)g0

+ 2r(cid:107)v∗(cid:107)g−1

0

≤ 1

≤2.3h(cid:107)v − v(cid:107)g−1

0

+ 2.4rh(cid:107)A(cid:62)(λ − λ)(cid:107)g−1

0

+ 3.3h(cid:107)x − x(cid:107)g0(cid:107)v1/2 − A(cid:62)λ(cid:107)g−1

0

+ 3.2h(cid:107)A(cid:62)(λ1 − λ1)(cid:107)g−1

0

+ 6h(

1
2

(cid:107)v − v(cid:107)g−1

0

+ (cid:107)A(cid:62)(λ1 − λ1)(cid:107)g−1

0

)(cid:107)v1/2 − A(cid:62)λ(cid:107)g−1

0

+ 27h(cid:107)v1/2 − A(cid:62)λ(cid:107)2
3r
h

+ (cid:107)v∗(cid:107)g−1

0

≤(

)(6h(cid:107)v − v(cid:107)g−1

0

(cid:107)x − x(cid:107)g0

g−1
0

+ 9h(cid:107)A(cid:62)(λ1 − λ1)(cid:107)g−1

0

+ 31h(cid:107)x − x(cid:107)g0)

29

Proven in [28]

Ideal RHMC
O(mn7/6)

Discretized RHMC
O(mn3)

Reduce

Reduce

Ideal CRHMC (D.1)
O(mk7/6)

Discretized CRHMC (D.2)
O(mk3)

Figure D.1: Proof outline for the mixing rates of CRHMC

+ 2.4rh(cid:107)A(cid:62)(λ − λ)(cid:107)g−1
≤ 3r

0

where we used (cid:107)v1/2−A(cid:62)λ(cid:107)g−1
we have

0

h +(cid:107)v∗(cid:107)g−1

0

and r ≥ h. Using the bound on (cid:107)A(cid:62)(λ1−λ1)(cid:107)g−1

0

,

(cid:107)T (x, v, λ) − T (x, v, λ)(cid:107)

)(15h(cid:107)v − v(cid:107)g−1

0

+ 27rh(cid:107)A(cid:62)(λ − λ)(cid:107)g−1

0

+ 9h(

36r
h

+ 2(cid:107)v∗(cid:107)g−1

0

)(cid:107)x − x(cid:107)g0)

≤(

≤(

≤(

0

+ (cid:107)v∗(cid:107)g−1

3r
h
+ 2.4rh(cid:107)A(cid:62)(λ − λ)(cid:107)g−1
3r
h
3r
h

1
4r
+ (cid:107)v∗(cid:107)g−1

)(15h(cid:107)v − v(cid:107)g−1

+

0

0

0

+ 30rh(cid:107)A(cid:62)(λ − λ)(cid:107)g−1

0

+ 9h(

36r
h

+ 2(cid:107)v∗(cid:107)g−1

0

)(cid:107)x − x(cid:107)g0)

)(400r + 18h(cid:107)v∗(cid:107)g−1

0

)(cid:107)(x, v, λ) − (x, v, λ)(cid:107).

D Condition Number Independence via Self-concordant Barrier

In this section, we analyze the convergence rates of the ideal CRHMC and discretized RHMC in our
setting respectively, showing that both are independent of condition numbers. We only show the case
when f is linear,

π(x) ∝ e−f (x) = e−α(cid:62)x, for some α ∈ Rn.
However, recall that all logconcave densities can be reduced to this linear case (see (1.2)). We also
focus on when a manifold M is a polytope in the form of {x ∈ Rn : A(cid:48)x ≥ b(cid:48), Ax = 0} for
full-rank A(cid:48) ∈ Rm×n, A ∈ Rp×n and b(cid:48) ∈ Rm, with the Riemannian metric induced by the Hessian
of the logarithmic barrier of the polytope. For simplicity, we consider the case when there is no
momentum (i.e., β = 0) in Algorithm 2. In addition, we consider a lazy version of Algorithm 2 to
avoid a uniqueness issue of the stationary distribution of the Markov chain. The lazy version of the
Markov chain, at each step, does nothing with probability 1
2 (in other words, stays at where it is and
does not move). Note that this change for the purpose of proof worsens the mixing rate only by a
factor of 2.

In this setting, we show that the mixing rates of the ideal CRHMC and the discretized CRHMC
(Algorithm 2) are O (cid:0)mk7/6 log2 Λ
(cid:1), where Λ is a warmness parameter and
k is the dimension of the constrained space deﬁned by {x ∈ Rn : Ax = 0}. We remark that our
algorithm is actually independent of condition number (i.e., no dependency on (cid:107)α(cid:107)2 and the geometry
of polytope). This is the key reason that our sampler is much more efﬁcient for skewed instances than
previous samplers.

(cid:1) and O (cid:0)mk3 log3 Λ

(cid:15)

(cid:15)

We ﬁrst shed light on how RHMC and CRHMC can be related (see Figure D.1), establishing a
correspondence between RHMC (full-dimensional space) and CRHMC (constrained space). This
connection enables us to refer to the mixing rates of the ideal RHMC and discretized RHMC proven
in [28]. To be precise, we prove in Section D.1 the following theorem on the mixing rate of the ideal
CRHMC, which can solve the Hamiltonian equations accurately without any error.

30

Theorem 14 (Mixing rate of ideal CRHMC). Let πT be the distribution obtained after T iterations of
π0(S)
the ideal CRHMC on a convex body M = {x ∈ Rn : A(cid:48)x ≥ b(cid:48), Ax = 0}. Let Λ = supS⊆M
π(S)
be the warmness of the initial distribution π0. For any (cid:15) > 0, there exists T = O (cid:0)mk7/6 log2 Λ
(cid:1)
such that (cid:107)πT − π(cid:107)TV ≤ (cid:15), where k is the dimension of the constrained space deﬁned by {x ∈ Rn :
Ax = 0}.

(cid:15)

We then prove in Section D.2 the convergence rate of the discretized RHMC (Algorithm 2).
Theorem 15 (Mixing rate of discretized CRHMC). Let πT be the distribution obtained after T
iterations of Algorithm 2 on a convex body M = {x ∈ Rn : A(cid:48)x ≥ b(cid:48), Ax = 0}. Let Λ =
π0(S)
π(S) be the warmness of the initial distribution π0. For any (cid:15) > 0, there exists T =
supS⊆M
O (cid:0)mk3 log3 Λ
(cid:1) such that (cid:107)πT − π(cid:107)TV ≤ (cid:15), where k is the dimension of the constrained space
(cid:15)
deﬁned by {x ∈ Rn : Ax = 0}.

We believe there is room for improvement on the n-dependence via a more careful analysis.

D.1 Convergence rate of ideal CRHMC

Lee and Vempala [31] ﬁrst analyzed Riemannian Hamiltonian Monte Carlo (RHMC) on n-
dimensional polytopes embedded in Rn, with an invertible metric induced by the Hessian of the
logarithmic barrier of the polytopes. They bounded the mixing rate in terms of smoothness parameters
that depend on the manifold. In particular for uniform sampling, they showed that the convergence
rate of RHMC is O(mn2/3). Subsequently, [28] extended their analysis to exponential densities
and further analyzed the convergence rate of RHMC discretized by the implicit midpoint method,
showing that the mixing rates are O(mn7/6) and O(mn3), respectively.

However, our metric M (x) deﬁned for the constrained space could be singular in the underlying space
Rn, so we cannot directly refer to any theoretical results from [31, 28]. To address this challenge, we
establish a formalism that allows us to reduce the ideal CRHMC to the ideal RHMC, obtaining the
mixing rate through this reduction.
Even though our convex body M = {x ∈ Rn : A(cid:48)x ≥ b(cid:48), Ax = 0} of dimension k is embedded in
Rn, we can handle it with an invertible metric g on M properly deﬁned as if it is embedded in Rk.
To this end, we use {u1, ..., uk} to denote an orthonormal basis of the constrained space (which is
the null space of A) and extend it to an orthonormal basis of Rn denoted by {u1, ..., uk, ..., un}. We
also deﬁne two matrices Uk ∈ Rn×k and U ∈ Rn×n by

Uk = [ u1

· · · uk ] & U = [ Uk uk+1

· · · un ] .

Using this orthonormal basis {u1, ..., uk}, we can consider a new coordinate system y =
(y1, ..., yk) ∈ Rk on the k-dimensional manifold M. Moreover, there exists one-to-one corre-
spondence between y and x; for any x ∈ M there is a unique y such that x = Uky, and we can
recover this y by multiplying U (cid:62)

k (i.e., y = U (cid:62)

k x).

Let us deﬁne the invertible local metric g at y ∈ M by

g(y)(ui, uj) def= g(x)(ui, uj)

for i, j ≤ k.

With abuse of notations, we also use g(y) to denote the k × k matrix with its (i, j)-entry being
g(y)(ui, uj). We ﬁrst establish relationships between g(y) (and its inverse g−1) and M (x) (and its
pseudoinverse W def= M (x)†). We recall that for the orthogonal projection Q to the null space of A

Lemma 16. We have g(y) = U (cid:62)

M (x) = Q(cid:62)g(x)Q, W (x) = g(x)− 1
k M (x)Uk = U (cid:62)

2 (I − P (x))g(x)− 1
2 .
k g(x)Uk and g(y)−1 = U (cid:62)

k W (x)Uk.

Proof. It is immediate from the deﬁnition of g that g(y) = U (cid:62)
M (x) and g(x) agree on the constrained space, we also have g(y) = U (cid:62)
For g−1, we deﬁne two matrices Pk ∈ Rn×k and Pr ∈ Rn×(n−k) by
(cid:20) 0k×(n−k)
(cid:20)
In−k

Ik
0(n−k)×k

, Pr =

Pk =

(cid:21)

(cid:21)

k g(x)Uk. Since the quadratic forms of

k M (x)Uk.

31

where 0(n−k)×k is the zero matrix of size (n − k) × k, Ik is the identity matrix of size k × k and so
on. Due to Uk = U Pk, the upper-left k × k submatrix of g(cid:48)(x) := U (cid:62)g(x)U is exactly g(y). Let us
represent the inverse of g(cid:48) in the form of block matrix:

g(cid:48)(x)−1 =

(cid:20) B1 B2
B(cid:62)
2 B3

(cid:21)

,

for B1 ∈ Rk×k, B2 ∈ Rk×(n−k) and B3 ∈ R(n−k)×(n−k). Using the formula of the inverse of block
matrices (see App. E.3),

g(y)−1 = B1 − B2B−1

3 B(cid:62)
2 .

It is straightforward to check

B1 = P (cid:62)
= U (cid:62)
B2 = P (cid:62)
B3 = P (cid:62)

k g(cid:48)(x)−1Pk = P (cid:62)
k g(x)−1Uk,
k g(cid:48)(x)−1Pr = U (cid:62)
r g(cid:48)(x)−1Pr = U (cid:62)
· · · un ] ∈ Rn×(n−k). Therefore,

k U (cid:62)g(x)−1U Pk

k g(x)−1Ur,
r g(x)−1Ur,

for Ur = [ uk+1

g(y)−1 = U (cid:62)
= U (cid:62)
k

k g(x)−1Uk − U (cid:62)

k g(x)−1Ur(U (cid:62)

r g(x)−1Ur)−1U (cid:62)

r g(x)−1Uk

(cid:0)g(x)−1 − g(x)−1Ur(U (cid:62)

r g(x)−1Ur)−1U (cid:62)

r g(x)−1(cid:1) Uk.

2 Ur(U (cid:62)

Since g(x)− 1
r g(x)− 1
U (cid:62)
orthogonal projection matrices implies

r g(x)−1Ur)−1U (cid:62)

2 and this row space is the same with the row space of Ag(x)− 1

2 is the orthogonal projection to the row space of
2 , the uniqueness of

r g(x)− 1

g(x)− 1

2 A(cid:62)(Ag(x)−1A(cid:62))−1Ag(x)− 1

2 = g(x)− 1

2 Ur(U (cid:62)

r g(x)−1Ur)−1U (cid:62)

r g(x)− 1
2 .

Therefore,

g(y)−1 = U (cid:62)
k
= U (cid:62)
k

(cid:0)g(x)−1 − g(x)−1A(cid:62)(Ag(x)−1A(cid:62))−1Ag(x)−1(cid:1) Uk
(cid:16)
2 A(cid:62)(Ag(x)−1A(cid:62))−1Ag(x)− 1
g(x)− 1

I − g(x)− 1

(cid:16)

2

2

(cid:17)

g(x)− 1

2

(cid:17)

Uk

(cid:16)

= U (cid:62)
k

g(x)− 1

2 (I − P (x))g(x)− 1

2

(cid:17)

Uk

= U (cid:62)

k W (x)Uk.

We can now view the ideal CRHMC with the metric M (x) as the ideal RHMC with the metric g on
the k-dimensional manifold. Note that we have to ensure that the local metric g is also induced by
the Hessian of a logarithmic barrier, in order to refer to results from it.
Lemma 17. Let A(cid:48) = A(cid:48)Uk and ψ(y) be the logarithmic barrier of the k-dimensional polytope
deﬁned by {y ∈ Rk : A(cid:48)y ≤ b(cid:48)}. Then ∇2
yψ(y) = g(y).

Proof. Observe that {y ∈ Rk : A(cid:48)y ≥ b(cid:48)} is the new representation of M = {x ∈ Rn : A(cid:48)x ≥
b(cid:48), Ax = 0} in the y-coordinate system. Due to A(cid:48)y = Ax, we have Sx = Diag(A(cid:48)x − b(cid:48)) =
Diag(A(cid:48)y − b) = Sy. For the logarithmic barrier φ(x) of {x ∈ Rn : A(cid:48)x ≥ b}, direct computation
results in

∇2

yψ(y) = A(cid:48)(cid:62)
= U (cid:62)

k A(cid:48)(cid:62)S−2

x A(cid:48)Uk = U (cid:62)

y A(cid:48) = U (cid:62)
S−2
k g(x)Uk = g(y),
x A(cid:48) in the third equality and Lemma 16 in the last equality.

xφ(x)Uk

k ∇2

where we used ∇2

xφ(x) = A(cid:48)(cid:62)S−2

Most importantly, we prove that this ideal RHMC on the k-dimensional manifold with the metric
g(y) is equivalent to the ideal CRHMC with the metric M (x).

32

Lemma 18. The dynamics (x, v) and (y, u) of the ideal CRHMC in Rn and the ideal RHMC in Rk
are equivalent in a sense that the Hamiltonian equations for (x, v) can be obtained by lifting up the
Hamiltonian equations for (y, u) from Rk to Rn via multiplying Uk. That is, when we lift up the
dynamics (y, u) in Rk to the dynamics (x, v) in Rn deﬁned by x = Uky and v = Uku, it follows that

dx
dt

=

dx
dt

,

dv
dt

=

dv
dt

and v, v ∼ N (0, M (x)).

Proof. We ﬁrst recall from the proof of Lemma 4 that the Hamiltonian equations of (x, v) are

dx
dt
dv
dt

= W (x)v,

= −∇xf (x) −

1
2
1
2
− A(cid:62) (cid:0)AA(cid:62)(cid:1)−1

= −∇xf (x) −

Tr [W (x)Dg(x)] +

Tr [W (x)Dg(x)] +

1
2
1
2

Dg(x)

Dg(x)

(cid:20) dx
dt
(cid:20) dx
dt

,

,

(cid:21)

(cid:21)

dx
dt
dx
dt

− A(cid:62)λ(x, v)

(cid:18)

A

−∇xf (x) −

1
2

Tr [W (x)Dg(x)] +

1
2

Dg(x)

(cid:20) dx
dt

,

= (I − A(cid:62) (cid:0)AA(cid:62)(cid:1)−1

A)

(cid:18)

−∇xf (x) −

1
2

Tr [W (x)Dg(x)] +

(cid:18)

= UkU (cid:62)
k

−∇xf (x) −

1
2

Tr [W (x)Dg(x)] +

1
2

Dg(x)

(cid:20) dx
dt

,

1
2
dx
dt

Dg(x)

(cid:21)(cid:19)

,

(cid:21)(cid:19)

dx
dt
(cid:20) dx
dt

(cid:21)(cid:19)

,

dx
dt

where the last equality follows from that UkU (cid:62)
From Lemma 7 of [31], the Hamiltonian equations of (y, u) are

k is the orthogonal projection to the null space of A.

dy
dt
du
dt

= g(y)−1u,

= −∇yf (Uky) −

Tr (cid:2)g(y)−1Dg(y)(cid:3) +

1
2

1
2

Dg(y)

(cid:20) dy
dt

,

dy
dt

(cid:21)

.

From the deﬁnitions of (x, v), we have

dx
dt

dv
dt

= Ukg(y)−1u = UkU (cid:62)

k W (x)Uku = UkU (cid:62)

k W (x)v

W (x)v,

=
(∗)

(cid:18)

= Uk

−∇yf (Uky) −

Tr (cid:2)g(y)−1Dg(y)(cid:3) +

1
2

1
2

Dg(y)

(cid:20) dy
dt

,

dy
dt

(cid:21)(cid:19)

,

where (∗) follows from that W (x)v is already in the constrained space (i.e., the null space of A). Let
us examine each term in dv/dt separately.

∇yf (Uky) = U (cid:62)

k ∇xf (x),

Tr (cid:2)g(y)−1Dyg(y)(cid:3) = Tr (cid:2)U (cid:62)

(cid:1)(cid:3)
k g(Uky)Uk
k Dxg(x))(cid:3)

(cid:0)U (cid:62)
k (U (cid:62)
k Dxg(x))(cid:3)

k W (x)Uk · Dy
k W (x)UkU (cid:62)

k Dxg(x))(cid:3)

= Tr (cid:2)UkU (cid:62)
= Tr (cid:2)W (x)UkU (cid:62)
= Tr (cid:2)(UkU (cid:62)
= Tr (cid:2)W (x)(U (cid:62)
= U (cid:62)

k (U (cid:62)
k W (x))(cid:62)(U (cid:62)
k Dxg(x))(cid:3)
k Tr [W (x)Dxg(x)] ,
(cid:20) dy
dy
dt
dt
k Dxg(x))Uk

k (U (cid:62)
= (cid:0)v(cid:62)W (x)Uk

k Dxg(x))Uk

k (U (cid:62)

= U (cid:62)

(cid:1) U (cid:62)

(cid:21)

,

(cid:0)U (cid:62)

k W (x)v(cid:1)

33

Dyg(y)

(cid:21)

(cid:20) dy
dt

,

dy
dt

= v(cid:62)W (x)(U (cid:62)

= U (cid:62)

k Dxg(x)

k Dxg(x))W (x)v
(cid:21)
(cid:20) dx
dx
dt
dt

.

,

Putting all these together, the Hamiltonian equations of (x, v) can be written as

dx
dt
dv
dt

= W (x)v,

(cid:18)

= UkU (cid:62)
k

−∇f (x) −

1
2

Tr [W (x)Dg(x)] +

1
2

Dg(x)

(cid:20) dx
dt

,

dx
dt

(cid:21)(cid:19)

.

Therefore, the Hamiltonian equations of (x, v) and (x, v) are exactly the same. In addition, v = Uku
leads to v ∼ N (0, Ukg(y)U (cid:62)

k ) = N (0, M (x)).

Using these three lemmas, we conclude that the dynamics of the ideal CRHMC on the constrained
space is equivalent to that of the ideal RHMC on the corresponding k-dimensional polytope. There-
fore, Theorem 14 immediately follows from Corollary 3 in [28].
Corollary. Let π be a target distribution on a polytope with m constraints in Rn such that dπ
dx ∼
e−α(cid:62)x for α ∈ Rn. Let M be the Hessian manifold of the polytope induced by the logarithmic
π0(S)
barrier of the polytope. Let Λ = supS⊂M
π(S) be the warmness of the initial distribution π0. Let
πT be the distribution obtained after T iterations of the ideal RHMC on M. For any ε > 0 and step
size h = O

(cid:17)
, there exists T = O (cid:0)mn7/6 log2 Λ

(cid:1) such that (cid:107)πT − π(cid:107)T V ≤ ε.

(cid:16)

ε

1
n7/12 log1/2 Λ
ε

D.2 Convergence rate of discretized CRHMC

We attempt to demonstrate a similar reduction of the discretized CRHMC. However, it is trickier
than that of the ideal RHMC, since Algorithm 2 uses the simpliﬁed Hamiltonian, which omits the
Lagrangian term c(x)(cid:62)λ(x, v), in place of the full Hamiltonian.

We look into the reduction in two steps. First of all, we show in Section D.2.1 that the dynamics
of x is the same under the discretized CRHMC via IMM with the simpliﬁed Hamiltonian, and the
discretized CRHMC via IMM with the full Hamiltonian, and that the acceptance probabilities are the
same as well. Next in Section D.2.2, we show a correspondence between the discretized CRHMC via
IMM and the discretized RHMC via IMM, just as we did for the ideal case in Section D.1.

D.2.1 Simpliﬁed Hamiltonian and full Hamiltonian in constrained space

We recall that the simpliﬁed Hamiltonian H(x, v) is the sum of two parts deﬁned by

H 1(x, v) = f (x) +

log pdetM (x)

1
2
1
2
v(cid:62)W (x)v.

H 2(x, v) =

1
2

= f (x) +

(cid:0)log det g(x) + log det Ag(x)−1A(cid:62) − log det AA(cid:62)(cid:1) ,

The full Hamiltonian H(x, v) with the Lagrangian term c(x)(cid:62)λ(x, v) can be also written as the sum
of two parts deﬁned by

H1(x, v) = f (x) +

1
2

(cid:0)log det g(x) + log det Ag(x)−1A(cid:62) − log det AA(cid:62)(cid:1)

− x(cid:62)A(cid:62)(AA(cid:62))−1A

(cid:18)

∇f (x) +

1
2

Tr [W (x)Dg(x)]

,

(cid:19)

H2(x, v) =

1
2

v(cid:62)W (x)v +

1
2

x(cid:62)A(cid:62)(AA(cid:62))−1ADg(x)

(cid:20) dx
dt

,

dx
dt

(cid:21)

,

and IMM with this Hamiltonian is implemented as in (C.1). We note from the proof of Lemma 18
that

∂H1
∂x

(x, v) = UkU (cid:62)
k

∂H 1
∂x

(x, v),

34

∂H2
∂x
∂H2
∂v

(x, v) = UkU (cid:62)
k

(x, v) = UkU (cid:62)
k

∂H 2
∂x
∂H 2
∂v

(x, v),

(x, v).

Lemma 19. For step size h, let (x, v) and (x, v) be the outputs of IMM with the simpliﬁed Hamil-
tonian and with the full Hamiltonian starting from (x0, v0), respectively. Then x = x, and
e−H(x0 ,v0 ) = e−H(x,v)
e−H(x,v)
e−H(x0,v0) .

3

(= x0), x 2

(= x) and v 1

Proof. We use x 1
of IMM with the full Hamiltonian. We similarly deﬁne xi and vi for i = 1
starting point, UkU (cid:62)
W (z)UkU (cid:62)

, v to denote the points obtained during one step
3 . As (x0, v0) is a
k v0 = v0. Due to Null(W (x)) = row(A), we have that

k w = W (z)w. By comparing the ﬁrst step of IMM for each Hamiltonian,

k x0 = x0 and UkU (cid:62)

3 , 2

, v 2
3

3

3

UkU (cid:62)

k v 1

3

= v0 −

h
2

UkU (cid:62)
k

∂H 1
∂x

(x0, v0) = v0 −

h
2

∂H1
∂x

(x0, v0) = v 1

3

,

and thus UkU (cid:62)

k v 1

3

= v 1
3

.

From the second step of IMM, x 2
3
+ v 2
(x 1
3
3

)/2 and vmid = (v 1

+ x 2
3

3

= x0, xmid =
is already in the null space of A. For x 1
)/2, the second step of IMM with the simpliﬁed Hamiltonian is

= x 1
3

3

x 2
3

= x 1
3

+ hUkU (cid:62)
k

∂H 2
∂v

(xmid, vmid) = x 1

3

+ hUkU (cid:62)

k W (xmid)vmid

= x 1
3

+ hUkU (cid:62)

k W (xmid)UkU (cid:62)

k vmid = x 1

3

= x 1
3

+ h

∂H2
∂v

(xmid, UkU (cid:62)

k vmid),

+ hUkU (cid:62)
k

∂H 2
∂v

(xmid, UkU (cid:62)

k vmid)

(xmid, vmid)

UkU (cid:62)

k v 2

3

= v 1
3

+ hUkU (cid:62)
k

= v 1
3

+ hUkU (cid:62)
k

= v 1
3

+ hUkU (cid:62)
k

∂H 2
∂x
(cid:18)

−

(cid:18)

−

1
2
1
2

Dg(xmid) [W (xmid)vmid, W (xmid)vmid]

(cid:19)

Dg(xmid) (cid:2)W (xmid)UkU (cid:62)

k vmid, W (xmid)UkU (cid:62)

k vmid

(cid:19)
(cid:3)

= v 1
3

+ hUkU (cid:62)
k

∂H 2
∂x

(xmid, UkU (cid:62)

k vmid)

= v 1
3

+ h

∂H2
∂x

(xmid, UkU (cid:62)

k vmid).

k vmid = (UkU (cid:62)

Note that UkU (cid:62)
second step is characterized as a unique ﬁxed-point, it follows that (x 2
, we can obtain that UkU (cid:62)
so x = x. In the same way we analyzed v 2

)/2 and xmid = (x 2

+ x 1
3

+ v 1
3

k v 2

3

3

3

3

k v = v.

)/2. Since the solution of this
) and

, UkU (cid:62)

) = (x 2
3

k v 2

, v 2
3

3

We now compare the acceptance probabilities. We clearly have H(x0, v0) = H(x0, v0) due to
c(x0) = 0 and have H 1(x, v) = H1(x, v) due to x = x. For H 2,

v(cid:62)W (x)v = v(cid:62)UkU (cid:62)

k W (x)UkU (cid:62)

k v = v(cid:62)W (x)v,

and so H 2(x, v) = H2(x, v).

D.2.2 CRHMC and RHMC discretized by IMM

In this section, we show that there is a correspondence between the dynamics of CRHMC discretized
by IMM and that of RHMC discretized by IMM.
Lemma 20. The discretized CRHMC via IMM in Rn and the discretized RHMC via IMM in Rk are
equivalent. That is, the output (x1, v1) given by the discretized CRHMC starting from (x, v) is the

35

same with (Uky1, Uku1), where (y1, u1) is the output of the discretized RHMC starting from (y, u)
satisfying (x, v) = (Uky, Uku). Moreover, the acceptance probabilities are the same due to

e−H c(x1,v1)
e−H c(x,v)

=

e−H r(y1,u1)
e−H r(y,u)

,

where H c(x, v) and H r(y, u) are the Hamiltonians of CRHMC and RHMC respectively.

Proof. We ﬁrst recall that H c(x, v) can be rewritten as the sum of two parts deﬁned by

H c

1(x, v) = f (x) +

1
2

log pdetM (x)

− x(cid:62)A(cid:62)(AA(cid:62))−1A

(cid:18)

∇f (x) +

1
2

Tr [W (x)Dg(x)]

,

(cid:19)

H c

2(x, v) =

1
2

v(cid:62)W (x)v +

1
2

x(cid:62)A(cid:62)(AA(cid:62))−1ADg(x)

(cid:20) dx
dt

,

dx
dt

(cid:21)

.

Similarly for H r, we can represent it by the sum of two parts deﬁned by

H r

H r

1 (y, u) = f (Uky) +
1
2

2 (y, u) =

u(cid:62)g(y)−1u.

1
2

log det g(y),

For the ﬁrst claim, we need to show that each step of IMM for RHMC and CRHMC is equivalent,
thus it sufﬁces to check that for any (y, u) ∈ Rk × Rk

∂H c

1(Uky, Uku)

∂x

∂H c

2(Uky, Uku)

∂x

∂H c

2(Uky, Uku)

∂v

= Uk

= Uk

= Uk

∂H r

∂H r

∂H r

1 (y, u)
∂y
2 (y, u)
∂y
2 (y, u)
∂u

,

,

.

These computations were already checked in the proof of Lemma 18.

For the second claim, we note that the Lagrangian term vanishes due to c(x) = c(Uky) = 0. Then
the second claim follows from

log det g(y(cid:48)) = log det U (cid:62)

k M (Uky(cid:48))Uk

(Lemma 16)

= log pdetM (Uky(cid:48)),

u(cid:48)(cid:62)g(y(cid:48))−1u(cid:48) = u(cid:48)(cid:62)U (cid:62)

k W (Uky(cid:48))Uku(cid:48).

(Lemma 16)

The previous two lemmas imply that the dynamics of the discretized CRHMC via IMM on the
constrained space is equivalent to that of the discretized RHMC via IMM on the corresponding
k-dimensional polytope. Therefore, Theorem 15 follows from Corollary 4 in [28].
Corollary. Let π be a target distribution on a polytope with m constraints in Rn such that dπ
dx ∼
e−α(cid:62)x for α ∈ Rn. Let M be the Hessian manifold of the polytope induced by the logarithmic
π0(S)
barrier of the polytope. Let Λ = supS⊂M
π(S) be the warmness of the initial distribution π0. Let
πT be the distribution obtained after T iterations of RHMC discretized by IMM on M. For any ε > 0
, there exists T = O (cid:0)mn3 log3 Λ
and step size h = O

(cid:1) such that (cid:107)πT − π(cid:107)T V ≤ ε.

(cid:17)

(cid:16)

ε

1
n3/2 log Λ
ε

E Missing Notations and Deﬁnitions

E.1 Notations

• We use N (µ, Σ) to denote Gaussian distribution with mean µ and covariance Σ.

36

• We use Null(A) and Range(A) to denote the null space and image space of a matrix or

linear operator A.

• We use ∇2f ∈ Rn×n to denote the Hessian of a function f : Rn → R.

• We use (cid:107)·(cid:107) to denote (cid:96)2-norm unless speciﬁed otherwise, and deﬁne (cid:107)x(cid:107)A :=

a vector x ∈ Rn and a matrix A ∈ Rn×n.

√

x(cid:62)Ax for

• We use ∂K to denote the boundary of the set K.
• For a matrix g(x) with x ∈ Rn, we use Dg(x) to denote the derivative of g(x) with respect
to x. This can be thought of as the n × n × n tensor such that (Dg(x))(i, j, k) = ∂(g(x))ij
.
In other words, (Dg(x))(·, ·, k) is the matrix, each of entries is the derivative of g(x) with
respect to xk. In addition, for a vector v ∈ Rn, Dg(x)[v, v] is a vector in Rn such that
(Dg(x)[v, v])i = v(cid:62)Dg(x)(·, ·, i)v.

∂xk

• For a matrix A of size n × n, we use A · Dg(x) to denote a n × n × n tensor such that
(A · Dg(x))(·, ·, i) = A · (Dg(x))(·, ·, i). We use Tr(A · Dg(x)) to denote a vector in Rn
such that (Tr(A · Dg(x)))i = Tr((A · Dg(x))(·, ·, i)).

E.2 Deﬁnitions

Convex body. A convex body is a compact and convex set.

Isotropy. A random variable X is said to be in isotropic position if EX = 0 and EXX (cid:62) = I.

Pseudo-inverse. For a matrix A ∈ Rm×n, it is well known that there always exists the unique
pseudo-inverse matrix A† that satisﬁes the following conditions:

1. A†AA† = A†.
2. AA†A = A.
3. AA† and A†A are symmetric.

It is also well known that Null(A†) = Null(A(cid:62)) and Range(A†) = Range(A(cid:62)).

Pseudo-determinant. For a square matrix A, its pseudo-determinant pdet(A) is deﬁned as the
product of non-zero eigenvalues of A.

Leverage score. For a matrix A ∈ Rm×n, the leverage score of the ith row is (A(A(cid:62)A)†A(cid:62))ii
for i ∈ [m]. When A is full-rank, it is simply (A(A(cid:62)A)−1A(cid:62))ii.

Log-barrier & Dikin ellipsoid. For a polytope P = {x ∈ Rn : Ax ≤ b} where A ∈ Rm×n and
b ∈ Rm, let us denote the ith row of A by ai and the ith row of b by bi. The log-barrier of P is
deﬁned by

m
(cid:88)

φ(x) = −

log(bi − a(cid:62)

i x).

For x ∈ P , the Dikin ellipsoid at x is deﬁned by D(x) := {y ∈ Rn : (y − x)(cid:62)∇2φ(x)(y − x) ≤ 1}.
The Dikin ellipsoid is always contained in P .

i=1

Analytic center. The analytic center xac of the polytope P is the point minimizing the log-barrier
(i.e., xac = arg min φ(x)).

Self-concordant function. A function f
(cid:12)D3f (x)[h, h, h](cid:12)
(cid:12)

(cid:12) ≤ 2 (cid:0)D2f (x)[h, h](cid:1)3/2

for all h ∈ Rn and x ∈ Rn.

: Rn → R is self-concordant

if it satisﬁes

Highly self-concordant function. A barrier φ is called highly self-concordant if it satisﬁes for all
h ∈ Rd and x ∈ Rd
(cid:12)D3φ(x)[h, h, h](cid:12)
(cid:12)

(cid:12) ≤ 2 (cid:0)D2φ(x)[h, h](cid:1)3/2

(cid:12) ≤ 6 (cid:0)D2φ(x)[h, h](cid:1)2

(cid:12)D4φ(x)[h, h, h, h](cid:12)
(cid:12)

and

.

37

Total variation. For two probability distributions P and Q on support K, the total variation
distance of P and Q is

(cid:107)P − Q(cid:107)TV

def= sup
A⊆K

(P (A) − Q(A)).

E.3 Details

Inverse and Determinant of block matrix. For a square matrix M =

(cid:21)

(cid:20) A B
C D

with blocks

A, B, C, D of same size, if D and A − BD−1C are invertible, then its inverse and determinant can
be computed by

(cid:20)

M −1 =

(A − BD−1C)−1

−(A − BD−1C)−1BD−1

−D−1C(A − BD−1C)−1 D−1 + D−1C(A − BD−1C)−1BD−1

(cid:21)

,

det(M ) = det(D) det(A − BD−1C).

Orthogonal projection. Let S = {x ∈ Rn : Ax = b} for A ∈ Rm×n and b ∈ Rm, and x0 be a
point in S. Thus S − x0 is the null space of A, due to A(x − x0) = 0. The orthogonal projection P
to this null space is

P = I − A(cid:62)(AA(cid:62))−1A.
Note that the range of P always lies in the null space because

A(P v) = A(I − A(cid:62)(AA(cid:62))−1A)v = Av − AA(cid:62)(AA(cid:62))−1Av = Av − Av = 0.

I − P is also an orthogonal projection matrix, and eigenvalues of orthogonal projection matrices are
either 0 or 1.

Matrix calculus. Let U (x) be a n × n matrix with a parameter x ∈ Rn.

∂U −1(x)
∂xi

= −U (x)

∂U (x)
∂xi

U (x).

Hence using the notation Dg, we can write in a more compact way as

For log det,

In other words,

DU −1(x) = −U (x)DU (x)U (x).

∂ log det U (x)
∂xi

(cid:18)

U −1(x)

= Tr

(cid:19)

.

∂U (x)
∂xi

D(log det U (x)) = Tr(U −1(x)DU (x)).

Cholesky decomposition. For a symmetric positive deﬁnite matrix A, there exists a lower triangular
matrix L such that LL(cid:62) = A.

Newton’s method. For f convex and twice differentiable in Rn, consider an unconstrained convex
optimization minx f (x). Given a starting point x0 ∈ Rn, the Newton’s method repeats

xi = xi−1 − (∇2f (xi−1))−1∇f (xi−1) ∀i ∈ N

to solve the optimization problem.

38

