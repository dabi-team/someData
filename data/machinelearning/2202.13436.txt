2
2
0
2

b
e
F
7
2

]

G
L
.
s
c
[

1
v
6
3
4
3
1
.
2
0
2
2
:
v
i
X
r
a

Neural-Progressive Hedging: Enforcing Constraints
in Reinforcement Learning with Stochastic
Programming

Supriyo Ghosh∗
Microsoft Research, India
supriyoghosh@microsoft.com

Laura Wynter
IBM Research AI, Singapore
lwynter@sg.ibm.com

Shiau Hong Lim
IBM Research AI, Singapore
shonglim@sg.ibm.com

Duc Thien Nguyen∗
Singapore Management University
dtnguyen.2014@phdis.smu.edu.sg

Abstract

We propose a framework, called neural-progressive hedging (NP), that leverages
stochastic programming during the online phase of executing a reinforcement
learning (RL) policy. The goal is to ensure feasibility with respect to constraints and
risk-based objectives such as conditional value-at-risk (CVaR) during the execution
of the policy, using probabilistic models of the state transitions to guide policy
adjustments. The framework is particularly amenable to the class of sequential
resource allocation problems since feasibility with respect to typical resource
constraints cannot be enforced in a scalable manner. The NP framework provides
an alternative that adds modest overhead during the online phase. Experimental
results demonstrate the efﬁcacy of the NP framework on two continuous real-
world tasks: (i) the portfolio optimization problem with liquidity constraints for
ﬁnancial planning, characterized by non-stationary state distributions; and (ii) the
dynamic repositioning problem in bike sharing systems, that embodies the class
of supply-demand matching problems. We show that the NP framework produces
policies that are better than deep RL and other baseline approaches, adapting to
non-stationarity, whilst satisfying structural constraints and accommodating risk
measures in the resulting policies. Additional beneﬁts of the NP framework are
ease of implementation and better explainability of the policies.

1

Introduction

Reinforcement learning (RL) experienced a surge in popularity when deep models demonstrated
superior performance in game playing with Deep Q-learning Networks (DQN) [Mnih et al., 2013].
The role of RL was cemented when it was used to beat the reigning Go world champion [Silver
et al., 2017]. Improvements to deep RL algorithms have abounded, including RL for continuous
state and action spaces, with DDPG [Lillicrap et al., 2015], TRPO [Schulman et al., 2015], and PPO
[Schulman et al., 2017]. In spite of these advances, the dominance of RL for real-world problems has
lagged. We believe that this is due to three shortcomings.

First, RL policies cannot enforce business rules, or constraints during policy execution. Yet, often
structural constraints must be respected for a policy to be implementable. Existing methods, such
as constrained policy gradient [Achiam et al., 2017] or “safe" RL methods [Garcıa and Fernández,
2015] do not prevent constraint violations during policy execution. Moreover, these methods can

∗This work was done while the authors were with IBM Research AI, Singapore Lab.

 
 
 
 
 
 
often be difﬁcult to train and not scalable to large problems. Second, there is a natural trade-off
between expected reward and risk. The majority of RL algorithms seek to maximize the expected
return. While there have been RL algorithms that optimize for various risk measures, doing so in a
scalable manner and under constraints is still challenging. Third, the sample inefﬁciency of RL has
posed an impediment to solving problems where high-ﬁdelity simulators are not available to generate
sufﬁciently large number of sample trajectories. A hope to overcoming this is through the judicious
use of models to explore more sparingly the state and action spaces.

We introduce a framework to address these issues for problems with continuous state and action
spaces. An unconstrained RL policy is ﬁrst trained ofﬂine. During the online execution phase,
a stochastic program (SP) is used to re-optimize the given RL policy under constraints and risk
measures over a short-term future trajectory. Once the next action is chosen, the process repeats in a
rolling-horizon fashion using updated state information. We call this neural-progressive hedging (NP).
The NP method aims to exploit the generalization ability of RL to unseen scenarios jointly with the
ability of SP to exploit models and enforce scenario-dependent constraints as well as incorporating
risk measures. Since the NP framework relies on a model-based online phase, it is most useful in
problem settings where closed-form models of state transitions are relatively good approximations to
the true state transitions. Sequential and dynamic resource allocation problems are a key example in
which the NP framework excels.

Empirically, we show that the NP method results in policies that offer substantial improvements in
reward under various risk measures whilst satisfying hard constraints. Moreover, we observe that the
NP policy, with its more sample-efﬁcient initial RL policy followed by the online ﬁne-tuning phase,
is able to outperform the fully-trained (and data-hungry) RL policy. An additional beneﬁt of the
framework is ease of implementation: it can be implemented using existing deep RL algorithms and
off-the-shelf optimization packages. To that end, the key contributions of the paper are as follows:

1. We deﬁne a novel method, neural-progressive hedging, combining stochastic programming
model-based online planning with ofﬂine, deep RL for a continuous policy that satisﬁes
hard constraints during execution;

2. We incorporate risk-measures such as CVaR without sacriﬁcing model structure or decom-

position algorithm; and

3. We demonstrate the efﬁcacy of the NP method on the class of resource allocation models,
including two real-world problems: (i) liquidity-constrained portfolio optimization with
a CVaR objective; and (ii) dynamic repositioning in a bike sharing system, where the NP
method outperforms deep RL, both constrained and unconstrained as well as other baselines.

2 Preliminaries

Consider the problem of learning a deterministic policy π : S → A in a Markov Decision Process
(MDP) given by (S, A, p, f, γ, T, G), with continuous states s ∈ S, continuous actions x ∈ A,
transition probability distribution p(st+1|st, xt), cost function f (st, xt, st+1) ∈ R, discount factor
γ ∈ [0, 1], decision horizon T , and constraint set G. We allow T = ∞ whenever γ < 1. The
constraint set G contains a set of K additional cost functions g1 . . . gK where gk(st, xt, st+1) ∈ R
and constants β1 . . . βK ∈ R. Our constrained MDP setting follows that of Altman [1999], where we
aim to solve the following problem:

minimize
π

Eπ

s.t. Eπ

(cid:34) T

(cid:88)

t=1
(cid:34) T

(cid:88)

t=1

γt−1f (st, xt, st+1)

(cid:35)

(1)

(cid:35)
γt−1gk(st, xt, st+1)
≤ βk,

k = 1 . . . K.

Without loss of generality we assume a ﬁxed initial state s1. The expectation Eπ is taken with respect
to randomness induced by the transitions st+1 ∼ p(·|st, xt) by taking xt = π(st), for all t. Problem
(1) is very challenging to solve for general MDPs with continuous states and actions. We shall now
put this constrained MDP in the context of stochastic programming (SP) from which we borrow many
of the algorithmic tools in this work.

2

The key assumption from SP is that all the randomness or uncertainty in the system comes from
external sources. This decoupling of randomness allows us to employ powerful optimization tools in
solving the main problem. Assume T is ﬁnite and let ξ1 . . . ξT be random variables such that the next
state st+1 is given by st+1 = ˜p(st, xt, ξt) where ˜p is a deterministic function once ξt is ﬁxed. We
call each realization of ξ = (ξ1 . . . ξT ) a scenario. Given a particular scenario ξ 2, one can ﬁnd the
best action sequence in “hindsight” by solving the following problem:

where we deﬁne

minimize
x=(x1...xT )
s.t.

˜f (x, ξ)

˜gk(x, ξ) ≤ βk,

k = 1 . . . K

˜f (x, ξ) :=

T
(cid:88)

t=1

γt−1f [st, xt, ˜p(st, xt, ξt)]

˜gk(x, ξ) :=

T
(cid:88)

t=1

γt−1gk[st, xt, ˜p(st, xt, ξt)].

(2)

If, for each ξ, the functions ˜f and ˜gk for all k are all convex in x, then each scenario sub-problem
can be readily solved using existing convex optimization tools. To simplify notation, we deﬁne the
constraint set G(ξ) := {x|˜gk(x, ξ) ≤ βk, k = 1 . . . K}, so problem (2) can be stated simply as
minimizex∈G(ξ)

˜f (x, ξ).

Suppose that one starts with a ﬁnite set Ξ of scenarios, with known probability distribution q(ξ)
where (cid:80)
ξ∈Ξ q(ξ) = 1. One can solve problem (2) for each individual ξ ∈ Ξ to obtain a mapping
x(·) that provides a solution x(ξ) = (x1(ξ) . . . xT (ξ)) for each ξ ∈ Ξ. Suppose that the action space
A ⊆ Rn and |Ξ| = N , then x(·) ∈ AN ×T ⊆ RN ×T ×n. How could we then reconcile the various
xt(ξ) across all ξ ∈ Ξ, at time t? For the resulting solutions to be implementable, one needs to
enforce a nonanticipative property which states that xt must only depend on information available
at time t. From an MDP point of view, the state st captures all observations available up to time t,
represented by ξ1 . . . ξt−1, and therefore xt must only depend on these if it is to be implementable,
i.e., xt(ξ) = xt(ξ1, . . . , ξt−1) and x1(ξ) must be the same for all ξ. All solutions x(·) that satisfy
this nonanticipative property can be expressed as:

x(ξ) = (x1, x2(ξ1), . . . , xT (ξ1, . . . , ξT −1)),

∀ξ ∈ Ξ.

ξ q(ξ) (cid:80)T

We use M to denote the space of all nonanticipative mappings. Deﬁne an inner product on AN ×T by
(cid:104)x(·), w(·)(cid:105) := (cid:80)
t=1(cid:104)xt(ξ), wt(ξ)(cid:105) where (cid:104)xt(ξ), wt(ξ)(cid:105) is the standard inner product in
Rn. Given any ˆx(·) ∈ AN ×T , one can ﬁnd a nonanticipative version x(·) = PM[ˆx(·)] where PM
is the orthogonal projection onto M given by the conditional expectation xt(ξ) = Eξ|ξ1...ξt−1 ˆxt(ξ)
for all t and ξ. Note that PM can be computed via simple averaging over the appropriate subsets of
scenarios.
Deﬁne G ⊆ AN ×T such that x(·) ∈ G iff x(ξ) ∈ G(ξ) for all ξ. We then aim to solve the following
global problem:

minimize
x(·)∈G∩M

Eξ

˜f (x(ξ), ξ)

(3)

˜f (x(ξ), ξ) = (cid:80)

ξ∈Ξ q(ξ) ˜f (x(ξ), ξ). Without the constraint x(·) ∈ M, problem (3) would
where Eξ
in fact be separable and could be decomposed into solving individual scenarios as in problem (2).
This problem, however, can still be solved in an iterative manner where each iteration involves solving
a slightly modiﬁed version of problem (2) for each scenario. This “progressive hedging” algorithm
by Rockafellar and Wets [1991], which is an application of the proximal point algorithm, involves
keeping track of the solution xi(·) as well as a Lagrange multiplier λi(·) in each iteration i, until
convergence. It also involves a parameter νi > 0, which may be constant for all i. Each iteration
involves solving the following steps:

1. At iteration i, solve the following for each scenario ξ:

ˆxi(ξ) ∈ arg min

x(ξ)∈G(ξ)

˜f (x(ξ), ξ) + (cid:104)λi(ξ), x(ξ)(cid:105) +

νi
2

(cid:107)x(ξ) − xi(ξ)(cid:107)2

(4)

2We abuse notation slightly by using ξ to refer to both the random variable and its particular realizations.

3

2. Compute xi+1(·) = PM[ˆxi(·)].
3. Update the Lagrange multiplier λi+1(·) = λi(·) + νi[ˆxi(·) − xi+1(·)].

In the case where ˜f and G are both convex, the algorithm is guaranteed to converge to an optimal
solution x∗(·) of problem (3) starting from arbitrary x1(·) and λ1(·). Local convergence to a stationary
point for nonconvex ˜f was shown by Rockafellar [2019].

The SP framework can be adapted to measures of risk. Consider CVaR, the conditional value-at-risk,
a popular measure for ﬁnding risk-averse solutions. The CVaR of a random variable Z at level
α ∈ [0, 1) can be written as:

CVaRα(Z) := min
y∈R

(cid:26)

y +

1
1 − α

EZ [max{0, Z − y}]

(cid:27)

.

CVaR at α = 0 gives the expectation. We solve the CVaR version of problem (3), replacing the
expectation Eξ with CVaRα, by following a modiﬁed progressive hedging algorithm [Rockafellar,
2018] with an introduction of an additional variable yi(ξ) ∈ R and the corresponding dual ui(ξ) ∈ R
for each ξ. Instead of equation (4), we solve equation (5) in step 1 with corresponding changes in
steps 2 and 3.

(ˆyi(ξ), ˆxi(ξ)) ∈ arg

min
y(ξ)∈R,x(ξ)∈G(ξ)

(cid:110)

y(ξ) +

1
1 − α

· max{0, ˜f (x(ξ), ξ) − y(ξ)}+

νi
2

|y(ξ) − yi(ξ)|2 + ui(ξ)y(ξ) + (cid:104)λi(ξ), x(ξ)(cid:105) +

(cid:107)x(ξ) − xi(ξ)(cid:107)2(cid:111)

(5)

νi
2

3 Neural-Progressive Hedging

We introduce Neural-Progressive Hedging (NP) method combining the generalization capability of
ofﬂine RL with the ability of SP through an online phase to exploit models while enforcing scenario-
dependent constraints and risk measures. The key steps of the NP method are shown compactly in
Algorithm 1.

The NP method works as follows: an unconstrained RL policy πθ, parameterized by θ, is obtained by
solving (1), or its risk-aware counterpart, without constraints. In each time-step τ , the NP method
observes current state s(τ ) and queries RL policy πθ to get initial action xπ(·). The new NP policy
is guided by the initial RL policy via a convex combination parameter κ so that, at convergence,
the executed actions satisfy constraints of G and the risk measures. Inner iterations are denoted by
i = 1, . . .; at each iteration i, the SP sub-problems are solved for each scenario ξ ∈ Ξ with updated
Lagrangian multipliers λi, ui to obtain the dual solution ˆxi(ξ) and ˆyi(ξ). Then, we project ˆxi(ξ)
onto a feasible space PM[ˆxi(·)], that satisﬁes the nonanticipative property, by averaging over all the
scenarios. The primal solution xi+1(·) is obtained as a convex combination with the initial RL policy
xπ(·) then projected with PM[ˆxi(·)]. We then update multipliers λi, ui, and parameters κi, and νi.
This iterative process continues until the difference between primal and dual solutions, δi, is below a
pre-deﬁned threshold (cid:15).

Resource Allocation Problems: The NP approach is particularly effective for the class of resource
allocation problems. In such applications, the main source of uncertainty is external – consider stock
price changes or customer demands – and to a large extent such random variables are unaffected
by the actions of the policy. A scenario generator can hence be readily trained using historical data.
The set of scenarios, Ξ, is obtained by sampling from such a scenario generator. Given a scenario, ξ,
this policy can then be queried at any state st to obtain the corresponding action xt. Given a ﬁnite
scenario set Ξ, we can obtain from πθ its solution xπ(·) ∈ M.

3.1 Theoretical Analysis

The parameter κi blends the ofﬂine RL policy with the solution from SP (Step 2 in Algorithm 1). The
assumption below covers the settings of warm start, where κ1 = 1 and ˆı = 2, and imitation learning,
where κi is a decreasing sequence such as (1 + i)−2, where 1 ≤ ˆı < ∞.

Assumption 1 (Imitation learning and warm start) Let κi → 0 as i → ∞. Furthermore, there
exists an ˆı such that for all i ≥ ˆı, κi = 0.

4

Algorithm 1 Neural-Progressive Hedging Algorithm

Initialization: Obtain RL policy πθ. Deﬁne inner convergence criterion (cid:15), convex combination
parameters κi and penalty parameters νi > 0 for i > 0.
for τ = 1, 2, . . ., do

Observe state s(τ ). Sample scenario set Ξ, and query πθ to obtain xπ(·). Set x1(·) = xπ(·). Set
λ1(·) = 0, u1(·) = 0 and i = 1.
while convergence criterion δi > (cid:15) do

1. Solve, for each ξ ∈ Ξ, (5) (or (4) for the risk-neutral case) to obtain ˆxi(ξ) and ˆyi(ξ).
2. Set xi+1(·) = κixπ(·) + (1 − κi)PM[ˆxi(·)]. Set yi+1(·) = Eξ[ˆyi(·)].
3. Update multipliers: λi+1(·) = λi(·)+νi(ˆxi(·)−xi+1(·)) and ui+1(·) = ui(·)+νi(ˆyi(·)−
yi+1(·)).
4. Update κi, νi.
5. Convergence test: δi+1 := (cid:107)ˆxi(·) − xi(·)(cid:107) + (cid:107)ˆyi(·) − yi(·)(cid:107)
6. Set i ← i + 1, continue.

end while
From converged solution x∗(·), obtain and execute x∗
1.

end for

Assumption 2 (Existence and local convexity) Assume that the solution set of equation (5) for a
CVaR objective, or equation (4) otherwise, is nonempty and ﬁnite, G(ξ) is convex and compact, the
gradients of ˜f are locally Lipschitz for each ξ and that the dual penalty parameters νi are sufﬁciently
large for all i.

Lemma 1 Under Assumption 1, the NP algorithm is equivalent to the progressive hedging algorithm
over an inﬁnite number of iterations.

Proof: Assumption 1 states that there exists a ﬁnite iterate ˆı such that for all i ≥ ˆı, κi = 0. Since
xi+1(·) = κixπ(·) + (1 − κi)PM[ˆxi(·)], for all i(cid:48) ≥ ˆı, xi(cid:48)
(·)], and hence the update of
the primal variable of the algorithm reduces to the progressive hedging update. (cid:4)

(·) = PM[ˆxi(cid:48)

Instances of stochastic programming typically make use of discretized support Ξ. We thus deﬁne the
problem (3) in terms of a discrete Ξ and refer to this problem for the remainder of this section.

Assumption 3 (Discrete support) Let Ξ be a discrete support and let 1 . . . K index each scenario
corresponding to a random variable ξ ∈ Ξ, with probability pk = 1/K. Then, problem (3) can be
expressed as:

min
xk∈Gk;xk∈M

1
K

(cid:88)

˜fk(xk).

k=1...K

(6)

Theorem 1 (Convergence of Alg. 1 for Convex ˜f ) Under Assumptions 1, 2 and 3, along with the
convexity of ˜f , the sequence of iterates (xi(·), yi(·), λi(·), ui(·)) generated by the NP algorithm is
such that

(cid:107)xi+1 − xi(cid:107)2 + (cid:107)yi+1 − yi(cid:107)2 + (1/ν2)(cid:107)λi+1 − λi(cid:107)2 + (1/ν2)(cid:107)ui+1 − ui(cid:107)2 <
(cid:107)xi − xi−1(cid:107)2 + (cid:107)yi − yi−1(cid:107)2 + (1/ν2)(cid:107)λi − λi−1(cid:107)2 + (1/ν2)(cid:107)ui − ui−1(cid:107)2, and

|xi+1 − x∗|2 + |yi+1 − y∗(cid:107)2 + (1/ν2)(cid:107)λi+1 − λ∗(cid:107)2 + (1/ν2)(cid:107)ui+1 − u∗(cid:107)2 <
|xi − x∗|2 + |yi − y∗|2 + (1/ν2)(cid:107)λi − λ∗(cid:107)2 + (1/ν2)(cid:107)ui − u∗(cid:107)2

with equality at (x∗(·), y∗) in the case of ﬁnite convergence, and thus converges to a local solution
(x∗(·), y∗) with (λ∗(·), u∗(·)) as i → ∞.

Proof: From Lemma 1, Algorithm 1 is equivalent to the Progressive Hedging Algorithm of Rockafel-
lar [2019] when run for an inﬁnite number of iterations. The convergence of the Progressive Hedging
Algorithm to a solution (x∗(·), y∗(·)) is thus guaranteed under Assumptions 2 and 3 along with the
convexity of ˜f . (cid:4)

5

Theorem 2 (Convergence of Alg. 1 for Nonconvex ˜f ) Let Assumptions 1, 2 and 3, hold and let
(xi(·), yi(·)) be a locally optimal solution to each subproblem (5). If sequences {xi, yi, λi, ui}
converge to point {x∗, y∗, λ∗, u∗}, then (x∗(·), y∗(·)) generated by the NP algorithm is a locally
optimal solution to problem (3).

Proof: From Lemma 1, Algorithm 1 is equivalent to the Progressive Hedging Algorithm of Rock-
afellar [2019] when run for an inﬁnite number of iterations. For nonconvex ˜f , when the Progressive
Hedging Algorithm converges to a point, under Assumptions 2 and 3, it was shown in Rockafellar
and Wets [1991] that the point is a stationary point of the problem (3). (cid:4)

The NP algorithm uses a decomposition of the measurability constraints on the scenario tree from the
scenario-speciﬁc constraints, and then proceeds to solve the SP by standard optimization methods. It
should be noted however that the structure and theoretical properties of the NP hold equally with
sample average approximation [Bertsimas et al., 2018].
When combining the unconstrained policy xπ(·) with the constrained solution PM(ˆxi(·)), we also
show how the quality of xi+1 evolves as a function of xπ(·) and PM(ˆxi(·)).

Proposition 1 Let ˜f be Lipschitz ∀ξ, i.e., (cid:107) ˜f (x(ξ), ξ) − ˜f (x(cid:48)(ξ), ξ)(cid:107) ≤ L(cid:107)x(ξ) − x(cid:48)(ξ)(cid:107). We have
the following bound as a function of κi and Lipschitz constant L:

E[ ˜f (xi+1(·), ·)] ≤ E[ ˜f (xπ(·), ·)] + L(1 − κi) · (cid:107)PM(ˆxi(·)) − xπ(·)(cid:107).

(7)

Proof: For each scenario ξ, we have

˜f (xi+1(ξ), ξ) − ˜f (xπ(ξ), ξ) ≤ L(cid:107)xi+1(ξ) − xπ(ξ)(cid:107) ≤ L(cid:107)κixπ(ξ)+

(1 − κi) · PM(ˆxi(ξ)) − xπ(ξ)(cid:107) ≤ L(1 − κi)(cid:107)PM(ˆxi(ξ)) − xπ(ξ)(cid:107)(cid:4)

Naturally, we expect an unconstrained RL solution to achieve a higher objective value, but the
executed solution may include constraint violations and excessive risk. The parameter κ, thus controls
the trade-off between a higher objective value and constraint satisfaction and risk aversion.

4 Experimental Results

To evaluate the performance of the proposed neural-progressive hedging (NP) approach, we conduct
experiments on two real-world domains where risk measures and constraints are an integral part of
implementable policies: (i) Liquidity management through portfolio optimization which seeks to
optimally reinvest earnings based on the CVaR whilst maintaining sufﬁcient liquidity; and (ii) Online
repositioning which seeks to dynamically match supply-demand when resources (here, represented
by bikes in a bike-sharing system) must be continuously rebalanced to meet changes in demand
whilst respecting the station capacity constraints.

We compare performance of NP method with Constrained Policy Optimization (CPO) [Achiam et al.,
2017], and Lagrangian-relaxed Proximal Policy Optimization (PPO-L) [Ray et al., 2019]. DDPG
[Lillicrap et al., 2015] is used to solve the unconstrained RL problems. Note that when κ = 1, the NP
approach returns the DDPG solution. Similarly, when κ = 0, the NP method returns the results of a
pure stochastic program (SP), computed using progressive hedging method [Rockafellar, 2019].

Experiment settings: We perform all the experiments on Ubuntu 18.04 virtual machines with
32-core CPU, 64 GB of RAM, and a single Nvidia Tesla P100 GPU. The distributed Ray framework
and RLlib [Liang et al., 2017] were used for the DDPG method. The pure SP and NP methods
with linear and non-linear objective function are solved using IBM ILOG CPLEX 12.9 and IPOPT
[Wächter and Biegler, 2006], respectively. The CPO and PPO-L methods are solved using OpenAI
safe RL implementation [Ray et al., 2019].

The unconstrained RL policy used as an expert is computed at each time step t using the DDPG
algorithm [Lillicrap et al., 2015]. We use a recurrent neural network (RNN) architecture for training
the DDPG method with 1 hidden layer consisting of 25 hidden predictor nodes and a tanh nonlinear
activation function. In addition, a long short-term memory (LSTM) model is used to represent the
RNN architecture with LSTM cell size 256 and maximum sequence length of 20. Parameter values

6

are as follows: the discounting factor γ = 0.99, minibatch size b = 50 and learning rate lr = 3e−5.
For both constrained RL methods (i.e., CPO and PPO-L), we use a neural network with 2 hidden
layers, each consisting of 256 hidden nodes with tanh nonlinear activation function3.

A discretized scenario tree is used in each decision epoch to solve the NP method for the experiments.
For the ﬁnancial planning example, in each decision period t, we generate a two layer scenario tree
where the ﬁrst layer consists of a root node and the second layer includes 1000 nodes, giving rise to
1000 scenarios. The interest rates for each of the scenarios are sampled from a multi-dimensional
log normal distribution whose mean and covariance matrix are estimated from the training data set
of price movements in the S&P500. For the liquidity constraints, we sample 10 liquidity demand
processes from a Gaussian distribution with µ = 0.025 and σ = 0.01, giving rise to 10,000 scenarios
in the second layer of the scenario tree. For the bike sharing problem, due to its complex non-linear
objective function, we generate a two-layer tree with 200 leaf nodes, giving rise to 200 scenarios.
The demand values at stations for each of the scenarios are sampled from a multi-variate normal
distribution whose mean and covariance matrix are learnt from 60 days of training demand data
[Ghosh et al., 2019]. Additional implementation details regarding discretization of the pure SP and
NP scenario tree are provided in Appendix B.

4.1 Liquidity-constrained portfolio optimization

The liquidity management problem seeks to optimally reinvest earnings in a portfolio based on the
CVaR whilst maintaining sufﬁcient liquidity. Too much liquidity means loss of potential returns
and too little incurs borrowing costs. Model-based forecasts of the price movements and liquidity
process are generally available in practice. The overall problem thus involves computing allocations
across a universe of ﬁnancial instruments, given observed rewards, prices, and model-based forecasts
of the price and liquidity processes. We have one risk-free liquid instrument. In each time step a
constraint requires that the amount in the liquid account to satisfy forecasted demand. We consider
four portfolios, each with nine stocks and one risk-free instrument. The state at time t includes the
current allocation, observed price changes and liquidity demands up to time t. The action is a vector,
xt = (xt,1, ..., xt,J ), of allocations across J instruments at time t, where j = 1 is the liquid asset.
Let ξt,1 be the cumulative liquidity requirement and Wt the wealth at the beginning of time t. The
constraint set is G(ξ) := {x|Wt · xt,1 ≥ ξt,1, (cid:80)
j xt,j = 1, t = 1 . . . T }. The liquidity requirement
(cid:96)t(ξ) for time t is sampled from a Gaussian (cid:96) ∼ N (µ, σ); µ(cid:96) = 0.025, σ(cid:96) = 0.01 and accumulates
over time, i.e., ξt,1 = Lt−1 + (cid:96)t, where Lt−1 denotes the accumulated realized liquidity requirement.
We use 11 years of S&P500 daily data from 2009–2019. The data from 2009–2016 is used for
training the unconstrained RL policy πθ and price movement model. For the SP and NP, in each time
step, we sample 1000 scenarios from a multi-variate log-normal distribution whose parameters are
learnt from the training data. Hyperparameter tuning of πθ is done using data of 2017–2018. Tests
are done on two consecutive 30 working day periods in 2019 (Jan 1-Feb 11, and Feb 12-Mar 25). It
should be noted that the experiments for these two testing datasets are done independently, where we
assume that the initial investment starts with 1 unit of liquid asset at the ﬁrst day.

Figure 1(a)-(b) shows the mean and standard error in returns of NP with CVaR α = 0.95, 0.99, along
with the unconstrained RL policy and the pure SP policy, over four asset universes. The NP policies
with CVaR α = 0.95, 0.99 signiﬁcantly outperforms the pure SP policy and improves the average
return by 14% and 18% over the DDPG policy. It should be noted that the variance (demonstrated
by the light shaded area) arises from differences in return rates for 4 different asset universes, but
our NP method always outperforms other baseline methods for individual asset universe. Table 1
provides performance metrics including the Sharpe ratio, volatility and maximum daily drawdown
(MDD), as well as the performance of well-known baseline trading strategies. Speciﬁcally, we
compare against four state-of-the-art online universal portfolio selection algorithms: (i) A uniform
constant rebalancing portfolio (uCRP) approach [Cover, 2011]; (ii) Online moving average reversion
(OLMAR) [Li and Hoi, 2012] (iii) Passive-aggressive mean reversion (PAMR) [Li et al., 2012] and
(iv) Robust median reversion (RMR) [Huang et al., 2016]. We use a grid search to optimize the
two key hyper-parameters of these universal portfolio algorithms: namely the lookback window
w and threshold parameter (cid:15)4. Average returns and Sharpe ratios of the NP are higher than all the

3The source codes for the CPO and PPO-L can be found at: https://github.com/openai/safety-starter-agents.
4The
at

algorithms

selection

portfolio

online

found

can

the

be

codes
https://github.com/Marigold/universal-portfolios.

source

for

7

benchmark approaches. In Figure 1(c), we demonstrate the sample efﬁciency of our expert-guided
NP approach. For this experiment, we train a DDPG policy with fewer samples (referred as “DDPG-
LS", “LS=less samples") obtained after 0.5 million training steps, and use it as the expert policy to
guide our NP approach. Despite having less training data, NP still provides better returns than the
sample-hungry DDPG policy, which is trained to convergence at 1.5 million steps. Additional results
on unconstrained portfolio optimization problem are provided in Appendix A.1.

(a)

(b)

(c)

Figure 1: (a)–(b) Returns without liquidity constraints. NP policies outperform DDPG, SP; (c)
Sample efﬁciency of NP.

Second 30 days, annualized values
Volatility MDD

First 30 days, annualized values

Algorithms
SP-0.0
SP-0.95
SP-0.99
NP-0.0
NP-0.95
NP-0.99
DDPG
uCRP
OLMAR
PAMR
RMR

Returns
11.84
11.83
0.0
22.47
22.47
21.64
19.33
12.08
10.4
6.35
10.68

Sharpe
3.58
3.58
-0.54
4.44
4.44
4.29
4.36
5.68
4.65
2.45
4.72

Volatility
27.33
27.37
0.0
40.29
40.29
40.38
35.63
17.16
18.26
22.08
18.45

MDD
7.63
7.65
0.0
10.46
10.46
10.44
9.52
5.26
5.97
6.03
5.97

Returns
2.3
0.87
0.0
7.44
7.44
7.4
6.08
1.38
-4.17
-8.02
-4.56

Sharpe
1.16
0.51
-3.78
2.22
2.22
2.09
2.12
0.97
-2.69
-3.39
-2.82

17.8
17.32
0.0
29.1
29.1
30.96
24.86
12.5
12.98
20.1
13.58

4.8
4.85
0.0
7.41
7.41
7.99
5.93
3.77
3.54
5.19
3.83

Table 1: Performance without liquidity constraints. NP policies nearly always outperform all other
strategies. SP with α = 0.99 puts all funds in cash, hence MDD and volatility are 0, but returns are 0
as well.

(a)

(b)

(c)

Figure 2: (a)-(b) Returns with liquidity constraints. NP with CVaRα=95 and CVaRα=99 are nearly
identical in (b). NP policies outperform CPO, PPO-L and DDPG-H; (c) Average liquidity in each
policy, Liquidity constraint is shown in red.

A signiﬁcant beneﬁt of the NP framework is the ability to enforce constraints, otherwise difﬁcult to
handle in an RL policy. Figure 2(a)-(b) show the mean and standard error in returns under liquidity
constraints. We compare against a heuristic we call DDPG-H that uses DDPG, but reserves µ(cid:96) + 3σ(cid:96)
of the funds for the 0-interest cash account by re-normalizing the remaining allocations. DDPG-H
thus provides a conservative, but constraint-feasible policy, by construction. The constrained NP
policy outperforms CPO, PPO-L, DDPG-H and even the unconstrained DDPG policy. In Figure
2(c), we show constraint violations; the red increasing line shows cumulative liquidity demand in
each period. Although PPO-L, unlike CPO, was able to learn the constraints during training (see
Appendix A.2 for constraint violations during training), both CPO and PPO-L are unable to come
close to satisfying the liquidity constraints in testing. Only DDPG-H and NP satisfy the constraints in
testing, but the DDPG-H method over-allocates to the liquid account, thereby reducing net returns.

8

051015202530Time Step1.001.051.101.151.201.251.30Return ValueSP-0.95SP-0.99NP-0.95NP-0.99DDPG051015202530Time Step0.951.001.051.101.151.20Return ValueSP-0.95SP-0.99NP-0.95NP-0.99DDPG051015202530Time Step1.001.051.101.151.201.251.30Return ValueNP-0.95NP-0.99DDPG-LSDDPG051015202530Time Step1.001.051.101.151.201.25Return ValueNP-0.95NP-0.99DDPG-HCPOPPO-L051015202530Time Step0.981.001.021.041.061.081.101.12Return ValueNP-0.95NP-0.99DDPG-HCPOPPO-L051015202530Time Step0.00.20.40.6Cash_AmountCash_ReqNP-0.95NP-0.99DDPG-HCPOPPO-L4.2 Online repositioning in bike-sharing systems

The bike repositioning problem is a form of online resource matching in an uncertain environment.
Uncoordinated movements of users in bike or electric vehicle sharing, along with demand uncertainty,
results in the need to often reposition the resources [Ghosh et al., 2017, Schuijbroek et al., 2017,
Ghosh et al., 2016, Ghosh and Varakantham, 2017]. We use an RL-based simulator from Bhatia et al.
[2019] built upon the dataset of Hubway bike sharing system in Boston, consisting of 95 base stations
and 760 bikes. The state at time t includes the current allocated bikes in each station j ∈ {1 . . . J}
and ξt,j is the random customer demand at station j. The action is a vector, xt = {xt,1, ..., xt,J },
that represents the percent allocations of bikes across all stations while respecting the constraint set
G(ξ) := {x| ˇCj ≤ N · xt,j ≤ ˆCj, (cid:80)
j xt,j = 1, t = 1 . . . T }, where ˇCj and ˆCj denote the minimum
and maximum bounds on the number of allocated bikes at station j, and N denotes the total number
of bikes present in the system. The objective function is represented by:

max
x

(cid:88)

(cid:88)

t

j

− L(xt,j, ξ)(1 + log(1 + L(xt,j, ξ)) − R(xt,j, ξ) sin(π · R(xt,j, ξ))

where L(xt,j, ξ) represents the amount of unfulﬁlled demand and R(xt,j, ξ) is the number of bikes
picked up or dropped off at station j at time t in action x, which incurs a repositioning cost. We
use two months of data to train the models. We evaluate the learnt policies on 3 consecutive days
during the morning peak period (6AM–12PM) with 12 decision epochs, each having a duration of 30
minutes.

Figure 3(a) shows mean and standard error in cumulative reward over 3 test days from NP with
CVaR of α = 0.0, 0.99, CPO and PPO-L, and unconstrained DDPG. After training CPO and PPO-L
methods for 1 million episodes, they fail to perform at par with NP. NP using expected reward
(i.e., α = 0.0) and with CVAR of α = 0.99 improves cumulative reward by 11.3% and 7.9% over
unconstrained DDPG. Figure 3(b) shows the cumulative constraint violation where capacity violation
per station costs 1 unit. Only NP variants satisfy the constraints, while both CPO and PPO-L fail to
satisfy the capacity constraints during the test period.

(a)

(b)

Figure 3: Performance comparison on the online bike repositioning problem for 12 time steps (6AM-
12PM), averaged over 3 testing days: (a) cumulative reward value; (b) cumulative constraint violation
cost.

(a)

(b)

Figure 4: Sensitivity analysis results by varying (a) convex combination parameter, κ; and (b) number
of scenarios.

Finally, we provide sensitivity analysis by varying (a) convex combination parameter, κ; and (b)
number of scenarios sampled from the demand distribution. Figure 4(a) shows the mean and standard
error in cumulative reward for different κ that decreases over iterations of NP. Recall that at κ = 1
and 0, NP reduces to DDPG and pure SP, respectively. The best performance is achieved with κ = 1
i2 ,

9

0246810Time Step6005004003002001000Cummulative RewardDDPGNP-0.0NP-0.99CPOPPO-L0246810Time Step051015202530Cummulative CostDDPGNP-0.0NP-0.99CPOPPO-L89101112Time Step450400350300250200150Cummulative Reward=0=1=1i=1i=1i2=1i389101112Time Step450400350300250200150Cummulative RewardN=5N=10N=50N=100N=150N=200which is our default setting in the experiments. Figure 4(b) shows the cumulative reward of NP in the
last ﬁve time steps, varying the number of scenarios. NP performance improves with the number of
scenarios, but in a concave manner. We thus use 200 scenarios for both the pure SP and NP in the
experiments.

5 Related work

The neural-progressive hedging algorithm combines ofﬂine policy search with an online model-
based phase to ﬁne-tune the policy so as to satisfy constraints and risk measures such as CVaR. We
categorize the existing relevant research into three threads: (a) Combining model-free and model-
based methods for performance improvement; (b) Constrained and risk-sensitive RL methods; and
(c) Improving sequential decisions through warm starting and imitation learning.

Ensemble of model-free and model-based methods: Model-based methods are prized for sample
efﬁciency, but, as noted by Feinberg et al. [2018], high-capacity models are “prone to over-ﬁtting in
the low-data regimes where they are most needed", implying that the combination of model-based
and model-free methods will be important for good performance in complex settings. They propose,
as do Buckman et al. [2018], to rollout the learned model for use in value estimation of a model-free
RL, in the latter reference using an ensemble of such models to estimate variance. Lu et al. [2019],
Amos et al. [2018], Tamar et al. [2017], Kahn et al. [2017] combine (online) planning models with
model-free RL to explore more sparingly the state and action spaces. Amos et al. [2018] propose to
differentiate through a planning model using analytical expressions for the derivatives of a convex
approximation of a non-convex model. Mansard et al. [2018] suggest a structure similar to ours for
controlling dynamical systems using models to initialize a model predictive control formulation, as a
warm-start. Lu et al. [2019] develop Adaptive Online Planning (AOP) with a continuous model-free
RL algorithm, TF3 [Fujimoto et al., 2018]. The goal is similar to ours – leveraging the responsiveness
of online planning with reactive off-policy learning to make better decisions. The approach is
however different from ours – AOP uses a model-based policy when uncertainty is high and a reactive
model-free policy when habitual behavior should sufﬁce.

Constrained and risk-sensitive RL methods: Garcıa and Fernández [2015] surveyed safe RL
methods which they classify as either optimization-based or handling safety in the exploration
process. Pham et al. [2018] suggest after each policy update to project the current iterate onto the
feasible set of safety constraints; since they assume that the safety constraints may not be known
in advance, they propose a method to learn the parameters of a linear polytope. Yang et al. [2019]
extend CPO method to solve constrained RL by optimizing the reward function using TRPO and then
projecting the solution onto the feasible region deﬁned by safety constraints, similar to Pham et al.
[2018]. Chow et al. [2015, 2017] model risk-constrained MDPs with a CVaR objective or chance
constraints, and solve it by relaxing the constraints and using a policy gradient algorithm. However,
similar to CPO and Lagrangian-relaxed PPO, the constraints are not enforced during execution and
need not be satisﬁed. Most “safe" RL methods use an initial infeasible, unconstrained policy and
iteratively render it feasible and locally optimal, e.g., Berkenkamp et al. [2017] deﬁne an expanding
“region of attraction" to guide safe exploration to improve the policy, whilst remaining feasible.

Imitation learning and warm start: NP can be viewed through the lens of imitation learning. Gu
et al. [2016] use synthetic model-based “imagination" rollouts in the early iterations of deep RL
training, which can be considered as a model-based warm-start. This is the opposite of our approach,
we propose a longer-horizon deep RL to warm start the online stochastic program. Aggravate [Ross
and Bagnell, 2014] and Aggravated [Sun et al., 2017], building on the seminal DAgger [Ross et al.,
2011], involve iteratively mixing the learning step of a policy with an expert policy, in that, at iteration
n, πn = βnπ∗ + (1 − βn)ˆπn, where β → 0 as n → ∞. This is similar to the update step of NP
which uses a convex combination of the expert and the learner policies, with damping. Cheng et al.
[2018] take this one step further by deﬁning a framework with a mirror descent gradient update
that reduces to imitation learning-based RL, depending on the choice of the gradient estimator; they
introduce SLOLS, where the gradient is a convex combination of a policy gradient and an expert
gradient. Sun et al. [2018] propose combining imitation learning and RL with the aim of faster
learning and improving beyond a sub-optimal expert. The advantages achieved by the NP method
in inverting the roles of expert and learner are the ability of the SP to enforce hard constraints and
incorporate risk measures, and doing so in an explainable manner. The NP warm start serves as an

10

external expert to guide the SP in the early iterations to encourage convergence to a better solution by
reshaping the objective itself.

6 Conclusion

The neural-progressive hedging (NP) method starts from an ofﬂine, unconstrained RL policy and
iteratively enforces constraints and risk requirements using model-based stochastic programming.
The framework is thus a type of external point method. We demonstrate the efﬁcacy of NP method
on two real-world applications, in ﬁnance and logistics. The NP method signiﬁcantly outperforms
both constrained and unconstrained RL whilst handling both resource constraints and risk measures.
An important beneﬁt of the framework is its ease of implementation: NP method can be implemented
using existing deep RL algorithms and commercial off-the-shelf optimization packages, and provides
added transparency and explainability on the constraint satisfaction of the policy.

Appendix

A Additional Numerical Results

In this section, we provide additional empirical results on the convergence of NP method, the effect
of warm-starting NP with an ofﬂine RL solution, and episodic constraint violation cost for CPO and
PPO-L benchmarks during the training process.

A.1 Convergence and effect of warm-starting

Figure 5(a) shows the convergence of the NP method, in the presence of and after damping to zero
the expert guidance. According to Theorem 1, initial iterates may decrease non-monotonically but for
iterations i ≥ ˆı = 20 progression to an optimum is monotonic Figure 5(b) compares the warm-start
(called NP-WS) version with the damped-guidance, or imitation-learning-type expert guidance (called
NP). Both warm-starting and imitation learning with damping for CVaR α = 0.95 and 0.99 perform
far better than the baseline DDPG policy.

(a)

(b)

Figure 5: (a) Convergence of the neural-progressive hedging algorithm with CVaRα=95; (b) Per-
formance comparison of two versions of the algorithm: warm start (κ1 = 1, ˆı = 1) vs. imitation
learning (κi = (1 + i)−2, ˆı = 20, in this example).

A.2 Constraint violation for benchmark constrained RL

Figure 6 illustrates the episodic constraint violation cost for two benchmark constrained RL algo-
rithms, CPO of Achiam et al. [2017], and PPO-Lagrangian of Ray et al. [2019], for the ﬁnancial
planning domain. Each episode duration is 30 time steps and in each time step t, we enforce a cost
of 1 if the amount available in the liquid instrument is less than the cumulative account payable up
through time t. Observe that CPO fails to learn the constraints during training. The PPO-Lagrangian
method is able to bring down the episodic cost to 0 during training (the limit of the episodic cost is
set to 0), but as shown in the main paper (see Figure 2(c)), the learned PPO-L policy is not able to
satisfy the constraints during execution.

11

01020304050#Iterations0.0000.0250.0500.0750.1000.1250.1500.175Optimality GapUniverse 0Universe 1Universe 2Universe 3051015202530Time Step1.001.051.101.151.201.251.30Return ValueA = NP-0.95A = NP-0.99A = DDPGA = NP-WS-0.95A = NP-WS-0.99(a)

(b)

Figure 6: Episodic (episode length = 30) constraint violation cost during training for (a) CPO; and (b)
PPO-Lagrangian.

B Additional Implementation Details

Discretization of the stochastic program scenario tree: Consider a ﬁnite scenario tree formula-
tion of a stochastic programming problem, such that the set of nodes in the scenario tree at time
stage t are denoted Nt. A node denotes a point in time when a realisation of the random process
becomes known and a decision is taken. Each node replicates the data of the optimization problem,
conditioned on the probability of visiting that node from its parent node. A path from the root
to each leaf node is referred to as a scenario; its probability of occurrence, ps, is the product of
the conditional probabilities of visiting each of the nodes on that scenario path. The discretized
model-based stochastic program is thus:

(cid:88)

max

Fs(x, ξ) :=

(cid:88)

ps

(cid:88)

ft(xs(t)).

s=1...S

s=1...S

t=1...T

(8)

The non-anticipativity constraints are critical for the implementability of the policy but they couple
the scenario sub-problems by requiring that the action xt at time t is the same across scenarios (i.e.,
sample paths) sharing the sample path up to and including time t. For each ξ ∈ Ξ, these coupling
constraints are expressed as:

x(ξ) = (x1, x2(ξ1), x3(ξ1, ξ2), . . . xT (ξ1 . . . ξT −1).

(9)

Using the discretized formulation of (8), and following Rosa and Ruszczy´nski [1996] we can rewrite
(9) in a manner that facilitates relaxation of those constraints: Deﬁne the last common stage of two
scenarios s1 and s2 as

tmax(s1, s2) := max{ˆt : s1(t) = s2(t), t = 1, . . . ˆt},
and then re-order the scenarios s = 1 . . . S, so that at every s, the scenario s + 1 has the largest
common stage with scenario i for all scenarios s(cid:48) > s, that is tmax(s, s + 1) := max{tmax(u, v) :
v > u}. Then, deﬁne the sibling of scenario s at time stage t as a permutation ν(s, t) := s + 1 if
tmax(s, s + 1) ≥ t and ν(s, t) := min{t(cid:48) : tmax(s, t(cid:48)) ≥ t} otherwise. The inverse permutation shall
be denoted ν−1(s, t). Note that the sibling of a scenario depends upon the time stage, and that a
scenario with no shared decisions at a time stage has by deﬁnition itself as sibling. Using the above,
Rosa and Ruszczy´nski [1996] re-deﬁne the constraints enforcing measurability in terms of the sibling
function as follows:

(10)

xs(t) = xν(s,t)(t) ∀(s, t), s (cid:54)= ν(s, t).
(11)
Equation (11) is convenient in the primal-dual formulation in terms of discrete scenarios, presented
next. We are interested in maintaining the separability of the subproblems which depend only on
individual scenarios of the random variable to facilitate handling large problems via scenario-based
decomposition. To do so, we relax the constraints using the following formulation

M := {x : M1x1(ξ) + . . . + MSxS(ξ) = 0},
where the matrices in (12) are deﬁned so that each Ms is a matrix of -1, 0 and 1 such that at the
root node x11 = x12, x12 = x23 = · · · x1,s−1 = x1,s, at the stage t = 2, there are as many such

(12)

12

0.250.500.751.001.251.50TotalEnvInteracts1e6051015202530AverageEpCostexp_cpo_pf300.250.500.751.001.251.501.75TotalEnvInteracts1e6051015202530AverageEpCostexp_ppol_pf30sets of equalities as children nodes emanating from the root node, and so on up to stage T − 1. At
stage T , all nodes are leaves and no such linking constraints are required. The projection of a point
xi onto the subspace M, PM[xi(·)] can be computed by taking the conditional expectation of xi,
Eξ | ξ1,...ξi−1. Lagrange relaxation of the measurability constraints (11) gives rise to the following
Lagrange function, in terms of the discrete scenarios s = 1 . . . S:

L(x, λ) =

(cid:88)

ps

(cid:88)

ft(xs(t)) +

(cid:88)

(cid:88)

λs(t)(xs(t) − xν(s,t)(t)).

(13)

s=1...S

t=1...T

s=1...S

t=1...T −1

The scenario subproblems are re-deﬁned as a function of the inverse permutation of the sibling
function:

min
xs∈G(cid:48)
s

Ls(xs, λs) = ps

(cid:88)

t=1...T

ft(xs(t)) +

(cid:88)

(λs(t) − λν−1(s,t)(t))xs(t)

(14)

t=1...T −1

for each s = 1 . . . S. The dual problem is given by

max
λ

D(λ) := min
x∈G(cid:48)

L(x, λ).

(15)

It is possible to further speed up convergence of our NP algorithm in practice using the approach of
Zehtabian and Bastin [2016]. This approach monitors the primal and dual gap terms in convergence
criteria separately to update the penalty parameters so as to reduce the convergence gap quickly.

References

Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
Proceedings of the 34th International Conference on Machine Learning, pages 22–31. JMLR. org,
2017.

Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.

Brandon Amos, Ivan Jimenez, Jacob Sacks, Byron Boots, and J Zico Kolter. Differentiable mpc for
end-to-end planning and control. In Advances in Neural Information Processing Systems, pages
8289–8300, 2018.

Felix Berkenkamp, Matteo Turchetta, Angela P Schoellig, and Andreas Krause. Safe model-based re-
inforcement learning with stability guarantees. In International Conference on Neural Information
Processing Systems, pages 908–919, 2017.

Dimitris Bertsimas, Vishal Gupta, and Nathan Kallus. Robust sample average approximation.

Mathematical Programming, 171(1-2):217–282, 2018.

Abhinav Bhatia, Pradeep Varakantham, and Akshat Kumar. Resource constrained deep reinforcement
learning. In International Conference on Automated Planning and Scheduling, volume 29, pages
610–620, 2019.

Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-
efﬁcient reinforcement learning with stochastic ensemble value expansion. In Advances in Neural
Information Processing Systems, pages 8224–8234, 2018.

Ching-An Cheng, Xinyan Yan, Nolan Wagener, and Byron Boots. Fast policy learning through

imitation and reinforcement. arXiv preprint arXiv:1805.10413, 2018.

Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-sensitive and robust decision-
making: a cvar optimization approach. In Proceedings of the 28th International Conference on
Neural Information Processing Systems, pages 1522–1530, 2015.

Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained
reinforcement learning with percentile risk criteria. The Journal of Machine Learning Research,
18(1):6070–6120, 2017.

Thomas M Cover. Universal portfolios. In The Kelly Capital Growth Investment Criterion: Theory

and Practice, pages 181–209. World Scientiﬁc, 2011.

Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I Jordan, Joseph E Gonzalez, and Sergey Levine.
Model-based value estimation for efﬁcient model-free reinforcement learning. arXiv preprint
arXiv:1803.00101, 2018.

13

Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in

actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.

Javier Garcıa and Fernando Fernández. A comprehensive survey on safe reinforcement learning.

Journal of Machine Learning Research, 16(1):1437–1480, 2015.

Supriyo Ghosh and Pradeep Varakantham. Incentivizing the use of bike trailers for dynamic reposi-
tioning in bike sharing systems. In Proceedings of the International Conference on Automated
Planning and Scheduling, volume 27, 2017.

Supriyo Ghosh, Michael Trick, and Pradeep Varakantham. Robust repositioning to counter unpre-
dictable demand in bike sharing systems. International Joint Conference on Artiﬁcial Intelligence
(IJCAI), 2016.

Supriyo Ghosh, Pradeep Varakantham, Yossiri Adulyasak, and Patrick Jaillet. Dynamic repositioning
to reduce lost demand in bike sharing systems. Journal of Artiﬁcial Intelligence Research, 58:
387–430, 2017.

Supriyo Ghosh, Jing Yu Koh, and Patrick Jaillet. Improving customer satisfaction in bike sharing sys-
tems through dynamic repositioning. In International Joint Conferences on Artiﬁcial Intelligence
(IJCAI), 2019.

Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning with
model-based acceleration. In International Conference on Machine Learning, pages 2829–2838,
2016.

Dingjiang Huang, Junlong Zhou, Bin Li, Steven CH Hoi, and Shuigeng Zhou. Robust median
reversion strategy for online portfolio selection. IEEE Transactions on Knowledge and Data
Engineering, 28(9):2480–2493, 2016.

Gregory Kahn, Tianhao Zhang, Sergey Levine, and Pieter Abbeel. Plato: Policy learning using adap-
tive trajectory optimization. In 2017 IEEE International Conference on Robotics and Automation
(ICRA), pages 3342–3349. IEEE, 2017.

Bin Li and Steven CH Hoi. On-line portfolio selection with moving average reversion. arXiv preprint

arXiv:1206.4626, 2012.

Bin Li, Peilin Zhao, Steven CH Hoi, and Vivekanand Gopalkrishnan. Pamr: Passive aggressive mean

reversion strategy for portfolio selection. Machine learning, 87(2):221–258, 2012.

Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Joseph Gonzalez, Ken
Goldberg, and Ion Stoica. Ray rllib: A composable and scalable reinforcement learning library.
arXiv preprint arXiv:1712.09381, 2017.

Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.

Kevin Lu, Igor Mordatch, and Pieter Abbeel. Adaptive online planning for continual lifelong learning.

arXiv preprint arXiv:1912.01188, 2019.

Nicolas Mansard, Andrea DelPrete, Mathieu Geisert, Steve Tonneau, and Olivier Stasse. Using
a memory of motion to efﬁciently warm-start a nonlinear predictive controller. In 2018 IEEE
International Conference on Robotics and Automation (ICRA), pages 2986–2993. IEEE, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.

Tu-Hoa Pham, Giovanni De Magistris, and Ryuki Tachibana. Optlayer-practical constrained opti-
mization for deep reinforcement learning in the real world. In 2018 IEEE International Conference
on Robotics and Automation (ICRA), pages 6236–6243. IEEE, 2018.

Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking safe exploration in deep reinforcement

learning. Technical report, Open AI, 2019.

R. Tyrrell Rockafellar. Solving stochastic programming problems with risk measures by progressive

hedging. Set-Valued and Variational Analysis, 26:759–768, 2018.

R Tyrrell Rockafellar. Progressive decoupling of linkages in optimization and variational inequalities
with elicitable convexity or monotonicity. Set-Valued and Variational Analysis, 27(4):863–893,
2019.

14

R Tyrrell Rockafellar and Roger J-B Wets. Scenarios and policy aggregation in optimization under

uncertainty. Mathematics of operations research, 16(1):119–147, 1991.

Charles H Rosa and Andrzej Ruszczy´nski. On augmented lagrangian decomposition methods for

multistage stochastic programs. Annals of Operations Research, 64(1):289–309, 1996.

Stephane Ross and J Andrew Bagnell. Reinforcement and imitation learning via interactive no-regret

learning. arXiv preprint arXiv:1406.5979, 2014.

Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured
prediction to no-regret online learning. In Proceedings of the fourteenth international conference
on artiﬁcial intelligence and statistics, pages 627–635, 2011.

Jasper Schuijbroek, Robert C Hampshire, and W-J Van Hoeve. Inventory rebalancing and vehicle
routing in bike sharing systems. European Journal of Operational Research, 257(3):992–1004,
2017.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pages 1889–1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy

optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. Nature, 550(7676):354–359, 2017.

Wen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron Boots, and J Andrew Bagnell. Deeply
aggrevated: Differentiable imitation learning for sequential prediction. In Proceedings of the 34th
International Conference on Machine Learning, pages 3309–3318. JMLR. org, 2017.

Wen Sun, J Andrew Bagnell, and Byron Boots. Truncated horizon policy search: Deep combination
of reinforcement and imitation. In International Conference on Learning Representations, 2018.
Aviv Tamar, Garrett Thomas, Tianhao Zhang, Sergey Levine, and Pieter Abbeel. Learning from the
hindsight plan—episodic mpc improvement. In 2017 IEEE International Conference on Robotics
and Automation (ICRA), pages 336–343. IEEE, 2017.

Andreas Wächter and Lorenz T Biegler. On the implementation of an interior-point ﬁlter line-search
algorithm for large-scale nonlinear programming. Mathematical programming, 106(1):25–57,
2006.

Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J Ramadge. Projection-based
constrained policy optimization. In International Conference on Learning Representations, 2019.
Shohre Zehtabian and Fabian Bastin. Penalty parameter update strategies in progressive hedging

algorithm. CIRRELT, 2016.

15

