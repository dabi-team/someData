Estimating Predictive Uncertainty Under Program
Data Distribution Shift

Yufei Li
University of Texas at Dallas
Dallas, USA
yxl190090@utdallas.edu

Simin Chen
University of Texas at Dallas
Dallas, USA
simin.chen@utdallas.edu

Wei Yang
University of Texas at Dallas
Dallas, USA
wei.yang@utdallas.edu

1
2
0
2

l
u
J

3
2

]

G
L
.
s
c
[

1
v
9
8
9
0
1
.
7
0
1
2
:
v
i
X
r
a

Abstract—Deep learning (DL) techniques have achieved great
success in predictive accuracy in a variety of tasks, but deep
neural networks (DNNs) are shown to produce highly overconﬁ-
dent scores for even abnormal samples. Well-deﬁned uncertainty
indicates whether a model’s output should (or should not) be
trusted and thus becomes critical in real-world scenarios which
typically involves shifted input distributions due to many factors.
Existing uncertainty approaches assume that testing samples
from a different data distribution would induce unreliable model
predictions thus have higher uncertainty scores. They quantify
model uncertainty by calibrating DL model’s conﬁdence of a
given input and evaluate the effectiveness in computer vision (CV)
and natural language processing (NLP)-related tasks. However,
their methodologies’ reliability may be compromised under
programming tasks due to difference in data representations and
shift patterns. In this paper, we ﬁrst deﬁne three different types of
distribution shift in program data and build a large-scale shifted
Java dataset. We implement two common programming language
tasks on our dataset to study the effect of each distribution
shift on DL model performance. We also propose a large-scale
benchmark of existing state-of-the-art predictive uncertainty on
programming tasks and investigate their effectiveness under data
distribution shift. Experiments1 show that program distribution
shift does degrade the DL model performance to varying de-
grees and that existing uncertainty methods all present certain
limitations in quantifying uncertainty on program dataset.

Index Terms—model uncertainty, distribution shift, program-

ming task, deep learning

I. INTRODUCTION

Recent success of deep learning (DL) [1] application in a
wide range of domains such as computer vision (CV), natural
language processing (NLP) has attracted huge attention from
researchers [2]. Due to the superiority of DL techniques,
they have also been broadly applied in nowadays software
engineering (SE)-related tasks including autonomous driving
testing, malware detection and programming language tasks.
With the implementation of deep neural networks (DNNs), one
can leverage the well-trained model to make predictions on the
test dataset. The effectiveness of DL techniques is based on
an important assumption that the test dataset is independently
and identically distributed (i.i.d.) with the training dataset [3].
However, in practical scenarios once a model is deployed,
the distribution over observed data may shift due to many
factors including natural evolution [4], noises [5] and artiﬁcial

1Our implementation is available at https://github.com/GAET-embedding/

Uncertainty Study.git

adversarial attack [6], and may eventually become much
different from the original distribution. A typical example
in programming language is software
of distribution shift
evolution [7] which leads to various forms of code distribution
shift, e.g., programming language gets updated to a more
recent library version, or the same project is taken over by
another programmer with different writing habit.

Intuitively, test inputs from a shifted distribution can reduce
DL model performance, but it is also critical to learn the
speciﬁc impact that different types of data distribution shift
has on DL models in terms of decision making. For example,
if we know that the distribution shift caused by programming
language update has little effect on program-analysis model
performance, we do not need to retrain a new one. Retraining
a DL model on a shifted dataset needs labelling on the
dataset which requires large effort. And we need to strike a
balance between the retraining cost and model performance
degradation. Studying the impact of distribution shift on a
model can facilitate us to understand when and how to adapt
the model to the shifted dataset.

Moreover, when the testing distribution differs from the
training distribution, DL models, though exhibit poor perfor-
mance, still tend to assume their prediction is accurate and
becomes untrustworthy [8]. Existing work [9]–[15] designs
calibrated predictive uncertainty to evaluate the reliability of
model’s prediction of a given input. They assume abnormal
samples such as out-of-distribution (OOD) and adversarial in-
puts are more likely to induce unreliable DL model predictions
and thus have higher uncertainty scores. These uncertainty
methods are widely used as an input validation mechanism
to ﬁlter out uncertain inputs and satisfy accuracy requirement
of (safety or security-critical) systems [16].

However, to our knowledge all the existing state-of-the-art
work evaluates the effectiveness of their uncertainty methods
on CV and NLP tasks. Their conclusions may not be adaptive
to programming language hypothesis due to the domain gap.
The speciality of programming language task compared to
CV and NLP in terms of distribution shift mainly lies in
two aspects. First, their representations are different. Image
models handle input pixel matrices, and the distribution shift
over pixel matrix is generalizable statistical models such as
Gaussian noise or linear matrix transformations such as im-
age rotation, scaling, shear, etc. [17]; while program-analysis

 
 
 
 
 
 
uncertainty estimates on program tasks and datasets for
future methodology reﬁnement.

Paper Organization. The reminder of this paper is orga-
nized as follows. Section II introduces necessary preliminaries.
Section III conducts study of effect of distribution shift on
model performance. Section IV evaluates the effectiveness of
5 popular uncertainty methods. Section V presents possible
threats to validity of this article. Section VI introduces some
related work we follow in this paper. Section VII proposes
some future work. Section VIII summarizes our conclusion.

II. PRELIMINARIES

DL models are used in DL software mainly for decision-
making, which concerns the model training and input predica-
tion phases. Below we introduce the preliminaries of the two
phases as well as the related uncertainty background in DL
applications.

A. Uncertainty in Deep Learning

With the deployment of DL systems in real-life settings,
it becomes more critical to understand what a model does
not know or cannot handle. DL systems can learn powerful
representations but these mappings are often taken blindly
and assumed to be accurate, which is not always the case.
For example, In May 2016, there was the ﬁrst fatality from
an assisted driving system, caused by the perception system
confusing the white side of a trailer for the bright sky.
This disaster was caused by the ignorance of the model’s
capability for recognizing white background. Building reliable
and truthful DL systems requires knowing the conﬁdence
behind the model’s predictive probabilities. In other words,
it requires digging deeply into uncertainty measurement.

The uncertainty in DL models arises from two sources, the
aleatoric uncertainty and the epistemic uncertainty [19]. The
former one arises from the noise in the observed labels (e.g.,
natural evolution, artiﬁcial corruption), while the latter one
comes from the selection of model parameters and model
structures (e.g., [20] proposes that invertible ResNet can be
more generative for input samples from different distribution
datasets). Bayesian machine learning [21]–[23] which works
with probabilistic models and uncertainty, deﬁnes probability
distributions over functions and are used to learn the more and
less likely ways to generalize from observed data. Existing
uncertainty measurements [2], [15], [24] which enhance the
effectiveness and efﬁciency of Bayesian machine learning
indeed achieve some progress.

B. Problem Deﬁnition

Let x ∈ Rn denotes a space of n-dimensional features and
y ∈ {1, · · ·, C} be its label for C-class classiﬁcation. Suppose
that a training dataset D is generated from an unknown true
distribution p∗(x, y) and contains M i.i.d. samples D =
{(xm, ym)}M
m=1, also known as the data generation process
[3]. In our programming language classiﬁcation tasks, the true
distribution is proposed to be a discrete distribution over the
C classes, and the observed label y ∈ {1, · · ·, C} is sampled

Fig. 1: The architecture of input validation on DL software

models, though may use the same DL architecture as NLP
models such recurrent neural network (RNN), represent code
snippets with a structured nature of their syntax such as
abstract syntax tree (AST) rather than a linear sequence of
tokens [18]. Therefore, the distribution shift on both NLP
and programming data is not standard generalizable manip-
ulations as on CV data. Moreover, the distribution shift on
code snippets contains additional structural relationships (e.g.,
logic operations, function calls) compared to natural language.
Second, their distribution shift sources are different. Shifted
image datasets can be another dataset with total different
discipline (e.g., from pet image dataset to MNIST). Shifted
natural language dataset can also be a text for another topic
with different composition of words. OOD datasets in both
CV and NLP can be chosen from a different topic and present
totally different meanings, while code data distribution shift
has to follow the programming language grammar constraints
and are generally milder such as language versions update,
projects content change or programmer change.

To investigate the effectiveness of existing predictive un-
certainty on programming language applications under data
distribution shift, we deﬁne three different types of program
data distribution shift based on real-world software develop-
ment scenarios. Then we conduct a comprehensive study of
the impact that each type of distribution shift has on DL model
performance as well as on existing predictive uncertainty
methods. We also analyze the advantages and limitations of
each uncertainty method based on the evaluation results under
program distribution shift for future study.
Contributions. The main contributions of this work are:

• We build three shifted programming distribution datasets
based on the real-world scenarios for future uncertainty
study in software engineering.

• We systematically study the effect that data distribution
has on the DL model performance in two programming
language classiﬁcation tasks.

• We conduct a comprehensive study of the effectiveness of
existing uncertainty methods on programming language
tasks and the impact of data distribution on these uncer-
tainty estimates.

• We summarize and analyze the limitations of existing

TrainingTrainingDatasetNormalData(cid:0)NoisyDataOODDataAdversaryDataTestingDatasetUncertaintyCalculateDecisionMakingDLModelValidatedInputDLSoftwareFilterOutTABLE I. Dataset for different versions (timelines)

Dataset

Train
Test1
Test2
Test3

Version

Elasticsearch

Gradle

Presto

Wildﬂy

Hadoop

Hibernate-orm Spring-framework

v.0.90.6
v.2.3.2
v.6.6.2
v.7.11.1

REL 1.9
REL 2.13
v.5.3.0
v.6.8.3

0.53
0.145
0.220
0.248

8.0.0.Beta1
10.1.0.CR1
17.0.0.Alpha1
23.0.0.Beta1

REL 2.2.0
YARN-2928
OZONE-0.4.0
REL 3.2.2

4.3.0.CR1
4.2.23.Final
5.3.0
5.4.29

v.3.2.5.REL
v.3.2.17.REL
v.5.2.0.M2
v.5.3.4

Release Time

Nov 2013
Apr 2016
Apr 2019
Feb 2021

TABLE II. Dataset for different projects

Dataset

Project

Version

Release Time

Dataset1

Dataset2

Dataset3

Train
Test

Train
Test

Train
Test

Spring-framework
Gradle

v.5.3.4
v.6.8.3

Hibernate-orm
Hadoop

5.4.29
REL 3.2.2

Presto
Elasticsearch

0.248
v.7.11.1

Feb 2021

Feb 2021

Feb 2021

TABLE III. Dataset for different authors

Dataset

Author

Project

Version

Release Time

Train
Test1
Test2
Test3

jasontedor
martijnvg
s1monw
kimchy

Elasticsearch

5.4.29

Feb 2021

from the conditional distribution p∗(y|x). Let f (·) = pθ(y|x)
denotes a neural network estimating the parameter θ through
the training dataset. In the testing phase, we evaluate the model
predictions on a test dataset that is sampled from the same
distribution as the training dataset and also on test datasets
that are sampled from a shifted distribution ˆp(x, y) (cid:54)= p∗(x, y).
Note that in CV and NLP tasks, they typically consider a
complete different OOD dataset where the ground truth label
is not one of the C classes, while in the
in test dataset
programming language task we generally consider the shifted
dataset where the ground truth label belongs to one of the
C classes. This is because in the program dataset, only the
variable names are different and user-deﬁned while the special
and key tokens are the same across different code snippets. In
the next section, we introduce three types of distribution shift
on the Java-based program dataset and evaluate the pre-trained
model on the test dataset with different distribution shift.

III. DISTRIBUTION SHIFT IN PROGRAMMING TASKS

In this section, we propose three real-world program project
dataset distribution shift scenarios and build three corre-
sponding shifted datasets, then we implement two common
programming language classiﬁcation tasks and evaluate with
the three shifted datasets. For each program task we follow
standard training, validation and testing protocols except that
we additionally evaluate results on increasingly shifted dataset.
To show the correlation between different data distribution
shift and model performance, we ﬁrst illustrate our datasets
conﬁguration in Section III-B and then introduce two down-

stream programming language tasks and report the model’s
prediction accuracy under different shifted datasets in Sec-
tion III-C to illustrate the pattern of distribution shift.

A. Research Questions

We aim to answer the following two research questions in

this section:
RQ1.1: Does distribution shift of program data affect the DL
model performance? And how much impact each type of
distribution shift has on the effectiveness of DL model?
RQ1.2: What factors may decide the effect of distribution shift
on DL model performance?

B. Datasets conﬁguration

Suppose that a company designed a DL model to automat-
ically check program misspell on a certain project P , as time
goes this project has been committed for multiple times, e.g.,
ﬁle addition and deletion, code modiﬁcation, etc., and updated
to a newer version which we denote as P (cid:48). It is critical to
investigate the pre-trained model performance on the project
P (cid:48)
to ensure DL software’s reliability on distribution shift
across different timelines. On the other hand, the company may
also want to know if the pre-trained model can work well on
other projects so that they can save unnecessary training effort,
which we denotes as the distribution shift across different
projects. Furthermore, some of the new commits on project P
are done by other employees that do not originally participate
in this project. Given the observation that different program-
mers may have different program writing habits, these commits
implemented by new authors may also bring distribution shift
and it is also necessary to evaluate the pre-trained model
performance on the code snippets written by different authors.
To simulate the three program data distribution shift scenar-
ios, we pull 7 Java projects from GitHub, namely, elasticsearch
[25], gradle [26], presto [27], wildﬂy [28], hadoop [29],
hibernate-orm [30], spring-framework [31] and extract all Java
ﬁles for the later programming tasks. For the ﬁrst scenario,
we choose four release time periods for all the 7 projects to
represent the distribution shift across timelines. We combine
all 7 projects released at each timeline to be the training, test1,
test2 and test3 dataset, respectively. Intuitively, the degree
of distribution shift is increasing as the time span enlarges.
The detailed project versions and release timelines are shown
in Table I. To present the second scenario, we choose three
pairs of different projects that are all in the latest version.
For each pair of dataset, one project
is used for training
and the other is for testing purpose. The detailed dataset

conﬁgurations including project, version and release time are
shown in Table II. For the third scenario, we investigate a
speciﬁc project elasticsearch by partitioning its Java ﬁles into
four parts where ﬁles in each part are written by an author.
Note that the project we choose is relatively large and contains
hundreds of contributors, so we choose the four authors who
have the most commits in history and the corresponding Java
ﬁles as the dataset. The detailed information about authors is
shown in Table III.

To visualize the distribution shift in programming data, we
select one speciﬁc function (named extends IMutation) and
show its two versions written by author jasontedor and kimchy
in Listing 1 and Listing 2, respectively. We omit some parts
of the ﬁrst program for saving space. From this example we
can conclude that different authors may write totally different
programs even for realizing the same functionality, which
incurs program distribution shift.

1 private Collection<? extends IMutation> getMutations

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

(BatchQueryOptions options, boolean local, long
now)
throws RequestExecutionException,
RequestValidationException
{

Set<String> tablesWithZeroGcGs = null;
UpdatesCollector collector = new

UpdatesCollector(updatedColumns, updatedRows());

for (int i = 0; i < statements.size(); i++)
{

... // omitted part

}
if (tablesWithZeroGcGs != null)
{

... // omitted part

}

collector.validateIndexedColumns();
return collector.toMutations();

}

Listing 1: Java code written by author jasontedor

1 private Collection<? extends IMutation> getMutations

2

3

4

5

6

7

(QueryOptions options, boolean local, long now)
{

UpdatesCollector collector = new

UpdatesCollector(Collections.singletonMap(cfm.
cfId, updatedColumns), 1);

addUpdates(collector, options, local, now);
collector.validateIndexedColumns();
return collector.toMutations();

}

Listing 2: Java code written by author kimchy

C. Downstream Tasks

In this section, we demonstrate the effect of data distribution
shift on model performance under two programming language
tasks that
target different properties of source code: code
summarization (CS) [18] and code completion (CC). They
are both implemented using the datasets in Section III-B.
For clariﬁcation, there are other programming language tasks
such as authorship identiﬁcation, API search or code clone
detection, but the datasets used in these tasks requires large
labelling effort. Since our study focus on distribution shift and

our shifted data are manually generated, we only consider CS
and CC in this paper.
Code Summarization. The ﬁrst task is source code summa-
rization and more speciﬁcally, we consider predicting method
names according to method bodies. We follow [18] to conﬁg-
ure a path-attention network architecture for code prediction
tasks and evaluate by the accuracy metric following work [32].
Code Completion. The second task is to predict the missing
code based on existing context. We follow [33] to conﬁgure a
multilayer perceptron (MLP) architecture for code prediction
tasks and apply accuracy as the evaluation metric.

D. Experiment Settings

We describe how we train the two task models on which

we will evaluate their performance:

The path-attention model [18] (also refer to as Code2Vec)
for CS task contains two embedding layers (node embedding,
path embedding), one dropout layer, one attention layer and
one fully-connected (FC) layer. The MLP model [33] (also
refer to as Word2Vec) for CC task consists of one embedding
layer and one FC layer. Their model parameters are shown in
Table IV.

TABLE IV. Parameters for Code2Vec and Word2Vec models

Parameter

Code2Vec Model Word2Vec Model

Learning Rate
Embedding Dimension
Dropout
Optimizer
Batch Size
Epochs

0.001
100
0.5
Adam
512
300

0.001
100
-
Adam
512
300

E. Evaluation Results

The model prediction accuracy (%) is shown in Table V.
We also report the accuracy drop ratio for each test dataset
to illustrate the impact of distribution shift on model perfor-
mance.

Finding 1: All three types of code data distribution shift
can degrade the DL model performance in programming
language tasks.

In both CS and CC tasks, the DL model performance is
declined on each shifted test dataset compared to the validation
accuracy under the three shift types. However, among the
three types of program shift, distribution shift across timeline
(version) has little effect on the DL performance, which
presents the robustness of DL models under certain program
data shift assumptions.

Finding 2: The degree of data distribution shift decides the
impact of distribution shift on model performance.

As we see in Table V, distribution shift across different
timelines has pretty mild effect on both task performance, but
the prediction accuracy on test1, test2, test3 dataset decrease
in order as the corresponding release time span (degree of

TABLE V. Programming task performance (accuracy) on 3 types of data distribution shift

Shift Type

Timeline

Project

Author

CS

CC

Validation
29.96

Test1
29.86(-0.33%)

Test2
29.7(-0.87%)

Test3
29.14(-2.74%)

Validation
45.45

Test1
45.4(-0.11%)

Test2
44.9(-1.21%)

Test3
44.29(-2.55%)

Validation
Test

Dataset1
28.47
27.25(-4.29%)

Dataset2
55.02
28.38(-48.42%)

Dataset3
47.39
20.11(-57.56%)

Validation
Test

Dataset1
48.35
40.13(-17.00%)

Dataset2
50.83
39.15(-22.98%)

Dataset3
50.44
40.10(-20.50%)

Validation
45.66

Test1
45.22(-0.96%)

Test2
24.93(-45.40%)

Test3
23.31(-48.95%)

Validation
47.81

Test1
47.67(-0.29%)

Test2
44.79(-6.32%)

Test3
44.41(-7.11%)

distribution shift) grows up, which means the severer shift we
have on test dataset, the poorer DL model performance will
be. Moreover, for distribution shift across different authors,
the drop ratio of test1 dataset is much lower than that in test2
and test3 dataset, which indicates that author jasontedor and
author martijnvg may have similar writing habits and the two
data distributions are similar.

IV. UNCERTAINTY ESTIMATION

We implement our evaluation of the existing popular un-
certainty measurements on our previously deﬁned datasets
and well-trained models in Section III. For each program
task we evaluate the effectiveness of the following popular
uncertainty methods through multiple metrics in terms of both
error/success prediction and in-/out-of-distribution detection.

Finding 3: The type of data distribution shift also decides
the impact of distribution shift.

From Table V we can see the overall drop ratios of model
prediction accuracy on the shifted test datasets under the
three shift types are totally different. With distribution shift
across different timelines having the slightest effect where
the reduction of accuracy is less than 3%, distribution shift
across different projects having the severest impact where the
maximum deduction of prediction accuracy reaches 57% on
Dataset3. This actually ﬁts in the practical scenarios where
most of the Java ﬁles in the same project are unchanged
through timelines, but different projects or projects written
by different authors may contain totally different composition
and distribution of program snippets.

Finding 4: The impact of data distribution shift on DL model
performance also relies on the model architecture. Simple
DNN architecture is more robust to data distribution shift.

Comparing the overall drop ratios under the three shift
types on both CS and CC tasks, we ﬁnd that the impact of
distribution shift is more obvious on CS than CC for each
dataset, e.g., the drop ratios under author shift for test1, test2
and test3 dataset are 0.96%, 45.40%, 48.95% for CS and
0.29%, 6.32%, 7.11% for CC. We assume that the two DL
models under both CS and CC tasks are well-trained, then
the different impact of distribution shift on the classiﬁcation
task mainly raises from the different model architectures which
have different sensitivity to distribution shift. Note that the DL
model architecture in CC is just one FC-layer MLP which is
much shallower and simpler than the path-attention network
architecture in CS. Given that larger number of parameters in
a more complex and deeper neural network are more likely
to cause overﬁtting in the training data distributing, we can
conclude that a simple DNN architecture should be more
robust to distribution shift.

A. Research Questions

We aim to answer the following four research questions in

this section:
RQ2.1: How effective are the existing uncertainty methods
in error/success prediction on shifted program data? Which
method(s) perform(s) relatively well (or bad) and why?
RQ2.2: How effective are the existing uncertainty methods
in distinguishing in/out-of-distribution program inputs? Which
method(s) perform(s) relatively well (or bad) and why?
RQ2.3: Are the existing uncertainty methods sensitive to
program distribution shift?
RQ2.4: Are the performance of existing uncertainty methods
consistent on CS and CC? If not consistent, what are the
possible reasons?

B. Uncertainty Methods

In recent years, lots of researches about uncertainty mea-
surement for DL models have been proposed. We select a
subset of uncertainty metrics from the existing literature for
their prevalence, scalability, and practical applicability. The
selected work includes:

• Vanilla [12]: Vanilla proposes that the maximum softmax
probability could be used as the model conﬁdence or
uncertainty to distinguish the in- and OOD inputs.

• Temp Scale [34]: Temp scale utilizes the post-hoc calibra-
tion named temperature scaling on a validation set to cal-
ibrate the gap between model predictive conﬁdence and
accuracy, the calibrated DL model’s predictive probability
could represent better ground-truth correctness likelihood
and thus be used for uncertainty measurement.

• MC-Dropout [2]: Monte-Carlo Dropout with rate p as ap-
proximate Bayesian inference in deep Gaussian process.
It shows the dropout process can distinguish between in-
and OOD samples.

• mMutate [6]: mMutant integrates statistical hypothesis
testing and model mutation testing to check whether an
input sample is likely to be in- or OOD at runtime by

TABLE VI. Uncertainty approach evaluation (error/success prediction)

Task

Approach

Different Timelines

AUC

AUPR

Brier

AUC

Vanilla
Temp Scale
mMutate
MC-Dropout
Dissector

Vanilla
Temp Scale
mMutate
MC-Dropout
Dissector

56.32
56.42
67.28
75.04
73.10

84.12
84.00
67.44
50.23
61.44

49.35
49.42
69.89
66.05
61.97

83.47
83.59
72.86
72.83
61.49

27.98
28.02
46.02
32.90
32.01

16.62
16.23
43.81
54.34
27.82

82.08
78.87
83.63
89.69
87.11

83.57
83.30
63.04
50.03
61.35

CS

CC

Dataset1
AUPR

67.57
62.79
72.99
77.57
76.39

83.89
83.78
74.80
74.06
63.89

Brier

42.38
28.43
28.61
12.02
18.91

17.06
16.92
44.48
51.89
27.60

Different Projects
Dataset2
AUPR

Brier

AUC

78.85
51.70
78.55
89.64
93.57

85.01
84.72
66.64
50.21
60.54

82.67
77.83
85.37
89.37
93.89

86.95
86.87
78.16
76.04
66.99

38.09
54.88
26.62
12.90
15.13

16.52
16.11
38.90
47.92
28.64

Dataset3
AUPR

71.94
73.70
81.67
86.36
87.71

88.12
87.89
76.11
75.10
69.63

AUC

61.18
50.06
81.69
86.70
88.48

86.01
85.75
60.80
50.10
63.60

Different Authors

Brier

AUC

AUPR

Brier

50.46
47.30
22.77
15.02
18.77

15.86
15.54
44.97
49.80
25.98

87.11
86.94
74.33
84.67
83.30

85.15
84.84
64.66
50.01
63.61

85.61
85.30
78.55
84.52
83.84

85.44
85.43
73.50
73.66
65.60

24.00
15.83
37.37
23.58
26.39

16.02
16.54
44.36
52.68
27.50

measuring its sensitivity to the model mutation. Based on
different mutation operators, mMutant is conﬁgured into
four sub-techniques, named as, mMutant-GF, mMutant-
NAI, mMutant-WS and mMutant-NS. They evaluate the
uncertainty in terms of Label Changing Rate (LCR).
• Dissector [35]: Dissector proposes a model-speciﬁc un-
certainty evaluation approach based on assessing the
model’s cross-layer conﬁdence about a given input. Based
on different weight growth types, Dissector was con-
ﬁgured into three sub-techniques, named as, Dissector-
linear, Dissector-log and Dissector-exp.

C. Uncertainty Experimental Setup

1) Datasets and Models: We continue using the datasets

and well-trained DL models in Section III.

2) Uncertainty Methods Setup: For Vanilla, no additional
conﬁguration is needed since it leverages the softmax prob-
ability of the DL models to measure predictive uncertainty.
For Temp Scale, we use the BFGS optimizer [36] to train the
calibration temperature on additional validation dataset which
has the same distribution as the training set with a learning
rate of 0.01. For MC-Dropout, we follow [2] and set dropout
probability as 0.5. For mMutate, we follow [6] and use four
mutation operators, namely, Gaussian Fuzzing (GF), Weight
Shufﬂing (WS), Neuron Switch (NS) and Neuron Activation
Inverse (NAI), and report the best score of the four in our
experimental results. We also conﬁgure the mutation degree as
0.05 in our study. For Dissector, it needs to select interior NN
layers for sub-model generation. For the path-attention model
in CS task, we adapt the hidden feature from both embedding
layer and attention layer. For the MLP model in CC task, we
select the embedding layer of each token as the feature to train
the sub-model since there is only one FC layer. Similarly, we
report the best evaluation scores of the three sub-techniques
of Dissector in Section IV-E.

D. Evaluation Metrics

We evaluate the above methods on both CS and CC tasks
with our created datasets. We use arrows to indicate which
direction is better for each of the metric. The evaluation
metrics contains:

• AUC↑ [6]: the Area Under the Receiver Operating Char-
acteristic curve (AUC) is a threshold-independent per-
formance evaluation metric [37]. The ROC curve is a
ﬁgure showing the relation between true positive rate and
false positive rate. AUC represents the probability that
a positive example has a larger predictive score than a
negative one [38]. A random classiﬁer corresponds to an
AUC of 50% and a ”perfect” classiﬁer corresponds to
100%.

• AUPR↑ [39]: the Area Under the Precision-Recall curve
(AUPR) better handles the situation when the positive
class and negative class have greatly differing base rates
compared to AUC. The PR curve plots the relationship
between precision and recall. The baseline classiﬁer has
to the precision [40] and a
an AUPR roughly equal
”perfect” classiﬁer corresponds to an AUPR of 100%.
• Brier Score↓ [41]: proper scoring rule representing the
accuracy of predicted probabilities. It measures the mean
squared error of the predicted probability assigned to
the possible outcomes for each sample and the actual
outcome. Therefore, the lower the Brier score is for a set
of predictions, the better the predictions are calibrated.

E. Evaluation Results

1) Error/Success Prediction: Table VI shows the error/-
success prediction results of the 5 uncertainty metrics all
the validation sets. We mark the best metric score of the
5 uncertainty methods in bold format. Fig. 2 and Fig. 3
respectively show the uncertainty evaluation on CS and CC
tasks under not only the validation set but also shifted datasets.
Ground truths. The ground truths of uncertainty prediction
are inputs that
the model can handle (within inputs) and
those the model cannot handle (beyond inputs). Based on our
previous discussion that beyond-inputs are likely to cause a DL
model’s prediction to be misleading or wrong. Therefore, we
consider within inputs (positive) as those correctly-predicted
samples and beyond inputs (negative) as those incorrectly-
predicted samples.

Finding 5: The effectiveness of softmax-based uncertainty
estimates highly rely on the DL model performance.

(a) Timeline Shift (time view)

(b) Timeline Shift (accuracy view)

(c) Author Shift (accuracy view)

(d) Project Shift1

(e) Project Shift2

(f) Project Shift3

Fig. 2: Results on Code Summary Task: 2a and 2b show AUC and Brier scores as the data is increasingly shifted (across
timeline). 2c also presents the two scores as the data is increasingly shifted (across author). To investigate the effectiveness of
uncertainty methods, we also explore the ﬁltered Number of samples (Count) and AUC versus conﬁdence score on the OOD
data in 2d, 2e and 2f. In the three project shift datasets, the number of samples decreases as the conﬁdence score grows. Temp
Scale has lower AUC on three shifted projects since all of its conﬁdence scores are in a very small range around 0. MC-Dropout
and Dissector have the overall decent AUC and Brier. Their effectiveness is stable as the intensity of shift increases.

In Table V, the model’s prediction accuracy on validation set
are relatively low in CS task under timeline shift and under
project shift dataset1. In both cases, the model’s prediction
accuracy is less than 30%. Correspondingly in Table VI, the
overall performance of Vanilla and Temp Scale in these two
situations are also relatively poor compared to MC-Dropout
and Dissector. In comparison, the model’s prediction accuracy
in CC task are more decent with most values larger than
40%, the corresponding Vanilla and Temp Scale uncertainty
evaluation scores are much better than other uncertainty
methods. One reason is that Vanilla and Temp scale both
focus on leveraging the DNN’s softmax layer to represent the
conﬁdence score, given that softmax layer is the output layer
that is directly correlated with the model’s prediction, their
calibrated conﬁdence score has more reliance on the original
model’s prediction accuracy than those uncertainty methods
that focus more on the DNN’s interior layer activation values.

Finding 6: Layer-level uncertainty estimates perform well in
complex or deep neural network but struggle with shallow
DL model.

In Table VI we ﬁnd that MC-Dropout and Dissector perform
well in CS task under nearly all different datasets but perform
poorly in CC task under all cases. This is because the two
methods are based on calculating the consistency between

network units in each layer, and these layer-level predictive
uncertainty methods require deeper neural network architec-
ture to ensure ﬁdelity. Since the MLP model used in CC task
contains only one fully-connected (FC) layer, all uncertainty
estimates of MC-Dropout and Dissector under CC task get
compromised. In comparison, the path-attention model used
in CS task contains 4 interior layers, which provides more
versatile cross-layer conﬁdence about a given input and thus
makes the calibrated uncertainty of the layer-level uncertainty
methods more reliable.

Finding 7: Adversarial-based uncertainty estimates can
hardly detect beyond inputs from an in-distribution dataset.

Table VI demonstrates that mMutate’s overall evaluation
scores are relatively low, this results from that mMutate is
proposed for detecting adversarial samples that are closer to
the model decision boundary and sensitive to the mutation
operations. While in the error/success prediction case, all
inputs are from the validation set and few of them are adver-
sarial samples or out-of-distribution samples. Distinguishing
inputs may not be enough for well-calibrating
adversarial
model’s prediction conﬁdence.

(a) Timeline Shift (time view)

(b) Timeline Shift (accuracy view)

(c) Author Shift (accuracy view)

(d) Project Shift1

(e) Project Shift2

(f) Project Shift3

Fig. 3: Results on Code Completion Task: 3a and 3b show AUC and Brier score as the data is increasingly shifted (across
timeline). 3c also presents the two scores as the data is increasingly shifted (across author). We also explore the ﬁltered Number
of samples (Count) and AUC versus conﬁdence threshold on the shifted data in 3d, 3e and 3f. In the three shifted project
datasets, the number of samples decreases as the conﬁdence score grows. MC-Dropout has lower AUC on three shift scenarios
since all of its conﬁdence scores are in a very small range around 1. Vanilla and Temp Scale have the overall decent AUC and
Brier. Their effectiveness is stable as the intensity of shift increases.

Finding 8: The overall evaluation scores on different metrics
reﬂect the effectiveness of an uncertainty method rather than
only a single metric.

Finding 9: Layer-level uncertainty methods are more robust
to dataset shift on CS task, while softmax-based uncertainty
methods are more robust to dataset shift on CC task.

In Table VI we mark the best evaluation score of the 5
uncertainty methods as bold. Experiments illustrate that the
best evaluation scores of the three metrics not always belong
to the same uncertainty method. For example, in CS task under
timeline distribution shift, MC-Dropout has the best AUC of
75.05, mMutate has the best AUPR of 69.89 while Vanilla
has the best Brier of 27.98. This is due to that different
metrics evaluate different aspect of prediction accuracy. Par-
ticularly, Brier score measures the marginal uncertainty over
labels and are insensitive to predicted probabilities associated
with in/frequent events, thus Temp Scale which calibrates the
conﬁdence scores into a small interval with high frequency
can achieve decent Brier score; AUPR compromises with the
positive and negative label rates, consequently, the imbalanced
ground truths in CS task under different timelines and under
different projects dataset1 due to the low model prediction
causes the AUPR to be much lower than AUC. Therefore,
one should evaluate the effectiveness of an uncertainty based
on different metric scores.

Generally, a model uncertainty that is well-calibrated on the
training and validation distributions would ideally remain so
on shifted dataset [3]. But as shown in Fig. 2c and Fig. 3c,
all the ﬁve uncertainty methods exhibit varying degrees of
AUC and Brier decline as the model’s prediction accuracy
decreases on the increasing shifted test dataset, which indicates
that existing uncertainty metrics still need further improvement
for programming language applications. We also explore that
layer-level uncertainty such as MC-Dropout and Dissector
are more robust to dataset shift on code summary task as
evidenced by a lower overall conﬁdence Fig. 2d, Fig. 2e,
Fig. 2f (the number of samples that are larger than the
conﬁdence threshold are smaller than other methods) and
higher overall AUC and lower Brier as shown in Fig. 2a,
Fig. 2b, Fig. 2c, while softmax-based uncertainty such as
Vanilla and Temp Scale are more robust to dataset shift on code
completion task as evidenced by a lower overall conﬁdence
Fig. 3d, Fig. 3e, Fig. 3f and higher overall AUC and lower
Brier as shown in Fig. 3a, Fig. 3b, Fig. 3c. This ﬁnding is
actually consistent with our previous ﬁndings that softmax-
based calibrated uncertainty works well on pre-trained models
with high prediction accuracy, while layer-level uncertainty

works well on deep and complex DL models (path-attention
model in CS task).

TABLE VII. Uncertainty approach evaluation on different
timelines (in-/out-of-distribution detection)

Dataset
In/OOD

val/test1

val/test2

val/test3

Approach

Vanilla
Temp Scale
mMutate
MC-Dropout
Dissector

Vanilla
Temp Scale
mMutate
MC-Dropout
Dissector

Vanilla
Temp Scale
mMutate
MC-Dropout
Dissector

AUC

49.04
49.04
50.51
50.97
49.75

50.13
50.14
50.67
51.00
49.71

51.38
51.38
50.46
51.61
50.85

CS
AUPR

48.38
48.38
65.39
54.92
48.82

49.51
49.52
65.80
55.21
49.50

51.34
51.34
66.27
56.32
51.40

Brier

AUC

37.60
37.52
44.08
39.84
41.38

37.47
37.36
43.63
39.48
41.51

37.34
37.21
43.19
38.70
40.57

49.67
49.72
54.98
51.07
49.48

47.35
47.79
50.82
50.88
50.06

48.86
49.40
43.08
50.95
49.74

CC
AUPR

52.43
52.40
68.29
76.37
51.92

48.59
48.79
65.49
75.59
50.34

49.80
49.99
61.88
75.25
49.64

Brier

34.15
32.74
40.32
47.26
33.31

35.03
34.02
43.54
48.83
33.25

33.87
32.96
46.11
49.50
33.33

TABLE VIII. Uncertainty approach evaluation on different
projects (in-/out-of-distribution detection)

Dataset
In/OOD

val1/test1

val2/test2

val3/test3

Approach

Vanilla
Temp Scale
mMutate
MC-Dropout
Dissector

Vanilla
Temp Scale
mMutate
MC-Dropout
Dissector

Vanilla
Temp Scale
mMutate
MC-Dropout
Dissector

AUC

50.99
47.82
50.48
54.02
54.28

62.47
50.96
61.11
67.90
65.73

49.33
50.07
55.15
58.71
57.86

CS
AUPR

60.51
57.47
64.23
60.51
62.14

57.93
68.87
62.37
61.66
59.75

54.02
65.05
43.60
37.02
38.25

Brier

AUC

33.70
58.23
34.86
38.69
39.83

53.87
38.41
42.52
27.09
35.66

68.27
30.08
41.24
26.60
39.95

56.42
57.07
53.95
50.06
54.16

58.12
58.52
57.54
50.08
55.09

59.03
59.38
59.72
50.02
53.18

CC
AUPR

67.99
68.40
76.95
80.88
65.26

51.44
52.04
63.72
70.19
45.66

59.41
58.57
71.70
73.61
51.83

Brier

32.24
28.33
34.46
38.24
30.03

26.12
28.17
49.74
59.62
30.59

27.45
27.22
45.97
52.77
30.46

2) In-/out-of-distribution detection: Table VII, Table VIII,
Table IX respectively shows the in-/OOD detection results of
the 5 uncertainty metrics on CS and CC task under each type
of distribution shift. For simpliﬁcation, we denote ”validation”
as ”val”. We also mark the best metric score of the 5 uncer-
tainty methods in bold format. Note that the distribution shift
under programming data is relatively mild compared to those
complete OOD datasets used in CV tasks, thus it is tougher for
existing uncertainty methods to distinguish between in-/OOD
datasets in our experiments which means lower evaluation
scores in this part are still reasonable.

Ground truths. The ground truths of in-/OOD detection
are inputs that are within the training data distribution (in-
distribution inputs) and those follow a different distribution
from the training data (OOD inputs). Based on our setting,
all test datasets contain more or less distribution shift should
be considered as OOD data. Therefore, we consider in-
distribution inputs (positive) as those samples in the validation
set and OOD inputs (negative) as those samples in the shifted
test sets.

TABLE IX. Uncertainty approach evaluation on different
authors (in-/out-of-distribution detection)

Dataset
In/OOD

val/test1

val/test2

val/test3

Approach

Vanilla
Temp Scale
mMutate
MC-Dropout
Dissector

Vanilla
Temp Scale
mMutate
MC-Dropout
Dissector

Vanilla
Temp Scale
mMutate
MC-Dropout
Dissector

AUC

50.79
50.88
49.74
50.51
50.93

59.42
58.71
55.38
59.35
57.73

60.06
59.49
53.61
59.20
58.51

CS
AUPR

54.93
55.06
67.81
60.19
55.23

74.55
74.21
75.81
74.30
73.54

72.03
71.68
72.41
70.94
70.99

Brier

AUC

37.31
39.31
40.86
37.54
38.75

30.57
37.06
32.29
30.59
33.43

31.54
35.13
35.67
31.70
34.11

50.60
50.36
59.88
50.03
50.74

51.44
51.95
49.85
50.09
52.99

52.57
53.08
59.71
50.11
53.19

CC
AUPR

50.60
50.54
70.50
75.17
50.60

60.88
61.14
72.11
79.59
60.70

63.41
63.78
77.49
80.40
62.80

Brier

33.12
32.74
41.87
49.67
32.03

34.06
30.79
37.15
40.82
30.05

33.90
30.04
34.22
39.21
29.75

Finding 10: Existing predictive uncertainty methods are
sensitive to data distribution shift, especially when the level
of distribution shift is high.

From Table V we know the degree of distribution shift
across authors is increasing in order of test1, test2 and test3
as the prediction accuracy decreases, correspondingly in Ta-
ble IX the overall evaluation performance of the 5 uncertainty
methods also becomes better in order of val/test1, val/test2
and val/test3. The pattern is same for distribution shift across
projects as the drop ratio increases in the order of Dataset1,
Dataset2 and Dataset3, the 5 uncertainty methods perform bet-
ter in distinguish between the corresponding in/OOD dataset
pair, e.g., val1/test1, val2/test2 and val3/test3. This pattern
demonstrates that existing predictive uncertainty are well-
calibrated and sensitive to distribution shift. As the level
of distribution shift increases, these uncertainty methods can
more precisely distinguish between the in-distribution dataset
and shifted OOD dataset.

Finding 11: Adversarial-based uncertainty estimates per-
form more effectively in distinguish in-/OOD inputs than
predicting error/success inputs.

In comparison of the error/success prediction and in-/OOD
detection results, we ﬁnd that the overall performance of mMu-
tate over the other 4 uncertainty methods on OOD detection
is relatively better than on error/success prediction, which
further indicates that inputs from a shifted distribution are
more sensitive to model mutation operations and this approach
becomes more effective when detecting adversarial inputs or
inputs from a different distribution.

Finding 12: Dissector is not sensitive to distribution shift
and falls short in detecting program OOD inputs.

As shown in Table VII, Table VIII, Table IX, Dissector
struggles with distinguishing in-/OOD inputs in all
three
distribution shift cases. Speciﬁcally, most of its AUC and

AUPR scores are around or below 50, which means the
calibrated conﬁdence scores on both validation set and shifted
set are similar. This result also indicates that inputs from
a different distribution shift are not necessary to cause the
neural network’s interior layer-level intermediate prediction
inconsistency. And DL model’s cross-layer conﬁdence about
a given inputs might be insensitive to data distribution shift.

V. THREATS TO VALIDITY

Inappropriate selection of datasets and uncertainty methods
may weaken the external validity of experimental conclusions.
We try to mitigate this threat by the following approaches:
(1) Select sufﬁcient number of train, validation and test ﬁles
that are versatile with different distributions [42], [43], e.g.,
Datasets for different timelines contain total 36,588 Java ﬁles,
among which 26,436 ﬁles for the train set, 2,538 ﬁles for the
validation set and each of the three shifted test set. Datasets
for different projects contains total 31,115 Java ﬁles in three
pairs. Speciﬁcally, 4,615 ﬁles for train set1, 1,977 ﬁles for
validation set1 and 2,558 ﬁles for test set1; 5,810 ﬁles for
train set2, 2,489 ﬁles for validation set2 and 2,834 ﬁles for
test set2; 4,684 ﬁles for train set3, 2,007 ﬁles for validation
set3 and 4,141 ﬁles for test set3. Datasets for different authors
contain total 18,014 Java ﬁles, among which 7,137 ﬁles for
train set, 3,059 ﬁles for validation set, 2,760 ﬁles for test
set1, 2,378 ﬁles for test set2 and 2,680 ﬁles for test set3;
(2) Deﬁne three data distribution shift types to manifest the
change of model performance; (3) Choose 5 state-of-the-art
uncertainty methods with diverse architectures and evaluate
their effectiveness in terms of both error/success prediction as
well as in/out-of-distribution detection.

Our internal

threat mainly arises from shifted datasets
conﬁguration. Existing uncertainty evaluation in terms of in-
/OOD detection on CV or NLP tasks uses complete OOD
datasets from different discipline, while in software engineer-
ing cases, we focus on program distribution shift and in which
the OOD dataset has relatively lower shift intensity. Thus in
our experiment, existing uncertainty’s effectiveness may get
compromised compared to their original experimental results.
However, our experiment only focus on the pattern of change
of uncertainty effectiveness as the degree of distribution shift
increases. In this assumption as long as the uncertainty can
more precisely distinguish between in- and OOD samples, it
is sensitive to distribution shift.

VI. RELATED WORK

A. Predictive Uncertainty for DL Application

Uncertainty is a natural part of any predictive system, thus
modeling uncertainty is of crucial importance. Existing work
has been developed for quantifying predictive uncertainty in
DL models and divided into two categories: Bayesian methods
and Non-Bayesian methods. Popular Bayesian approximation
approaches include Laplace approximation [22], variational
inference [44] and dropout-based variational inference [2],
[45] which activates the dropout layer in the testing phase
to measure the uncertainty. For Non-Bayesian methods, [12]

ﬁrst propose a baseline for detecting misclassiﬁed samples,
which leverages the maximum value of softmax layer as the
uncertainty score. [34] propose re-calibration of probabilities
on a held-out validation set through Temp Scale.

B. Adversarial Attacks

Adversarial attack techniques [46], [47] could generate
adversarial perturbations to fool DL models. Adversarial per-
turbations are unnoticeable for human-beings, but they could
change the model prediction when applied to normal inputs.
One well-known example is the Fast Gradient Method (FSGM)
search along the gradients’ direction to generate adversarial
samples. Besides the gradient search techniques, another type
of attack algorithm [48], [49] models the adversarial sample
generation as an optimization problem, which targets to mini-
mize the norm between normal input and adversarial samples
that satisfy the constraints.

C. Veriﬁcation for DL Application

The following work is proposed to formally verify DL
models to ensure their effectiveness. [50], [51] apply symbolic
techniques on the hidden neurons of the DL model to abstract
the input space. However, the cost of symbolic techniques
that limits this type of techniques can hardly be applied to
large DL models. Some other techniques try to validate inputs
at the running time. For example, [35] propose a validation
technique by measuring the PV-score of each input, [6] mutate
the original model to measure the label change rate (LCR) to
identify the adversarial samples at the running time.

VII. FUTURE WORK

Although our work, to our best knowledge, is the ﬁrst to
systematically research uncertainty under program distribution
shift problem, we mainly focus on Java language. In the
future, we would further explore distribution shift across
different programming languages. For example, evaluate the
performance and uncertainty of Python program classiﬁer on
Java snippets. Moreover, we plan to further study program
distribution shift problem under more program-analysis tasks
such as code authorship identiﬁcation (AI) [52], [53], code
API search [54], code clone detection [55], etc., to make our
conclusion more comprehensive and convincing. Finally, from
our conclusion in Section IV, all existing uncertainty present
certain limitations under program distribution shift scenarios.
One main reason is that their observation and assumption
on CV and NLP datasets may not be adaptive to program
data. Future work could try to enhance the performance of
uncertainty measurements following this direction.

VIII. CONCLUSIONS

Distribution shift is prevalent in software engineering sys-
tem which not only degrades the DL model performance but
also induce unreliable overconﬁdent predictions. Our com-
prehensive study illustrates the speciﬁc impact of different
types of program distributing shift on DL software and the
effectiveness of existing state-of-the-art uncertainty methods

under program distributing shift. We show that the impact of
distribution shift on DL models depends on various factors
such as the degree of shift, the type of shift, the DL model
architecture, etc. and could be mild or severe. Furthermore, ex-
isting uncertainty methods originally designed for quantifying
model uncertainty under CV and NLP tasks, though exhibit
reasonable effectiveness under both error/success prediction
and in-/OOD detection, all presents certain limitations in
program applications. For example,
layer-level uncertainty
such as Dissector and MC-Dropout perform poorly on simple
or shallow neural networks, softmax-based uncertainty such as
Vanilla and Temp Scale highly rely on the model performance
and are sometimes vulnerable to distribution shift. Further im-
provement is needed for adapting existing uncertainty methods
to software engineering.

REFERENCES

[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521,

no. 7553, pp. 436–444, 2015.

[2] Y. Gal and Z. Ghahramani, “Dropout as a bayesian approximation:
Representing model uncertainty in deep learning,” in international
conference on machine learning, 2016, pp. 1050–1059.

[3] Y. Ovadia, E. Fertig, J. Ren, Z. Nado, D. Sculley, S. Nowozin, J. V.
Dillon, B. Lakshminarayanan, and J. Snoek, “Can you trust your model’s
uncertainty? evaluating predictive uncertainty under dataset shift,” arXiv
preprint arXiv:1906.02530, 2019.

[4] J. Gama, I. ˇZliobait˙e, A. Bifet, M. Pechenizkiy, and A. Bouchachia, “A
survey on concept drift adaptation,” ACM computing surveys (CSUR),
vol. 46, no. 4, pp. 1–37, 2014.

[5] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
IEEE,

networks,” in 2017 ieee symposium on security and privacy (sp).
2017, pp. 39–57.

[6] J. Wang, G. Dong, J. Sun, X. Wang, and P. Zhang, “Adversarial sample
detection for deep neural network through model mutation testing,” in
2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE).

IEEE, 2019, pp. 1245–1256.

[7] T. Mens, “Introduction and roadmap: History and challenges of software

evolution,” in Software evolution. Springer, 2008, pp. 1–11.

[8] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman,
and D. Man´e, “Concrete problems in ai safety,” arXiv preprint
arXiv:1606.06565, 2016.

[9] B. Lakshminarayanan, A. Pritzel, and C. Blundell, “Simple and scalable
predictive uncertainty estimation using deep ensembles,” arXiv preprint
arXiv:1612.01474, 2016.

[10] K. Lee, K. Lee, H. Lee, and J. Shin, “A simple uniﬁed framework
for detecting out-of-distribution samples and adversarial attacks,” in
Advances in Neural Information Processing Systems, 2018, pp. 7167–
7177.

[11] J. Ren, P. J. Liu, E. Fertig, J. Snoek, R. Poplin, M. Depristo, J. Dillon,
and B. Lakshminarayanan, “Likelihood ratios for out-of-distribution
detection,” in Advances in Neural Information Processing Systems, 2019,
pp. 14 707–14 718.

[12] D. Hendrycks and K. Gimpel, “A baseline for detecting misclassiﬁed
and out-of-distribution examples in neural networks,” arXiv preprint
arXiv:1610.02136, 2016.

[13] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo,
R. Desai, T. Zhu, S. Parajuli, M. Guo et al., “The many faces of
robustness: A critical analysis of out-of-distribution generalization,”
arXiv preprint arXiv:2006.16241, 2020.

[14] D. Hendrycks, X. Liu, E. Wallace, A. Dziedzic, R. Krishnan, and
D. Song, “Pretrained transformers improve out-of-distribution robust-
ness,” arXiv preprint arXiv:2004.06100, 2020.

[15] W. J. Maddox, P. Izmailov, T. Garipov, D. P. Vetrov, and A. G.
Wilson, “A simple baseline for bayesian uncertainty in deep learning,” in
Advances in Neural Information Processing Systems, 2019, pp. 13 153–
13 164.

[16] E. G. Barrantes, D. H. Ackley, S. Forrest, T. S. Palmer, D. Stefanovic,
and D. D. Zovi, “Randomized instruction set emulation to disrupt binary
code injection attacks,” in Proceedings of the 10th ACM conference on
Computer and communications security, 2003, pp. 281–289.

[17] Y. Tian, K. Pei, S. Jana, and B. Ray, “Deeptest: Automated testing
of deep-neural-network-driven autonomous cars,” in Proceedings of the
40th international conference on software engineering, 2018, pp. 303–
314.

[18] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, “code2vec: Learning
distributed representations of code,” Proceedings of the ACM on Pro-
gramming Languages, vol. 3, no. POPL, pp. 1–29, 2019.

[19] Y. Xiao and W. Y. Wang, “Quantifying uncertainties in natural language
processing tasks,” in Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, vol. 33, 2019, pp. 7322–7329.

[20] J. Behrmann, W. Grathwohl, R. T. Chen, D. Duvenaud, and J.-H.
Jacobsen, “Invertible residual networks,” in International Conference on
Machine Learning, 2019, pp. 573–582.

[21] D. J. MacKay, “Information-based objective functions for active data
selection,” Neural computation, vol. 4, no. 4, pp. 590–604, 1992.
[22] ——, “Bayesian interpolation,” Neural computation, vol. 4, no. 3, pp.

415–447, 1992.

[23] ——, “A practical bayesian framework for backpropagation networks,”

Neural computation, vol. 4, no. 3, pp. 448–472, 1992.

[24] J. M. Hern´andez-Lobato and R. Adams, “Probabilistic backpropagation
for scalable learning of bayesian neural networks,” in International
Conference on Machine Learning, 2015, pp. 1861–1869.

[25] “elasticsearch,” https://github.com/elastic/elasticsearch.
[26] “gradle,” https://github.com/gradle/gradle.
[27] “presto,” https://github.com/prestodb/presto.
[28] “wildﬂy,” https://github.com/wildﬂy/wildﬂy.
[29] “hadoop,” https://github.com/apache/hadoop.
[30] “hibernate-orm,” https://github.com/hibernate/hibernate-orm.
[31] “spring-framework,”

https://github.com/spring-projects/

spring-framework.

[32] M. Allamanis, H. Peng, and C. Sutton, “A convolutional attention
network for extreme summarization of source code,” in International
conference on machine learning. PMLR, 2016, pp. 2091–2100.
[33] X. Rong, “word2vec parameter learning explained,” arXiv preprint

arXiv:1411.2738, 2014.

[34] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On calibration of
modern neural networks,” arXiv preprint arXiv:1706.04599, 2017.
[35] H. Wang, J. Xu, C. Xu, X. Ma, and J. Lu, “Dissector: input validation for
deep learning applications by crossing-layer dissection,” in Proceedings
of the ACM/IEEE 42nd International Conference on Software Engineer-
ing, 2020, pp. 727–738.

[36] R. Battiti and F. Masulli, “Bfgs optimization for faster and auto-
mated supervised learning,” in International neural network conference.
Springer, 1990, pp. 757–760.

[37] J. Davis and M. Goadrich, “The relationship between precision-recall
and roc curves,” in Proceedings of the 23rd international conference on
Machine learning, 2006, pp. 233–240.

[38] T. Fawcett, “An introduction to roc analysis,” Pattern recognition letters,

vol. 27, no. 8, pp. 861–874, 2006.

[39] C. Manning and H. Schutze, Foundations of statistical natural language

processing. MIT press, 1999.

[40] T. Saito and M. Rehmsmeier, “The precision-recall plot is more informa-
tive than the roc plot when evaluating binary classiﬁers on imbalanced
datasets,” PloS one, vol. 10, no. 3, p. e0118432, 2015.

[41] G. W. Brier, “Veriﬁcation of forecasts expressed in terms of probability,”

Monthly weather review, vol. 78, no. 1, pp. 1–3, 1950.

[42] D. Hendrycks, N. Mu, E. D. Cubuk, B. Zoph, J. Gilmer, and B. Lak-
shminarayanan, “Augmix: A simple data processing method to improve
robustness and uncertainty,” arXiv preprint arXiv:1912.02781, 2019.

[43] S. Liang, Y. Li, and R. Srikant, “Enhancing the reliability of out-
of-distribution image detection in neural networks,” arXiv preprint
arXiv:1706.02690, 2017.

[44] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, “Weight
uncertainty in neural networks,” arXiv preprint arXiv:1505.05424, 2015.
[45] D. P. Kingma, T. Salimans, and M. Welling, “Variational dropout and
the local reparameterization trick,” in Advances in neural information
processing systems, 2015, pp. 2575–2583.

[46] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in the

physical world,” arXiv preprint arXiv:1607.02533, 2016.

[47] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami, “The limitations of deep learning in adversarial settings,” in
2016 IEEE European symposium on security and privacy (EuroS&P).
IEEE, 2016, pp. 372–387.

[48] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
networks,” in 2017 ieee symposium on security and privacy (sp).
IEEE,
2017, pp. 39–57.

[49] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: a simple
and accurate method to fool deep neural networks,” in Proceedings of
the IEEE conference on computer vision and pattern recognition, 2016,
pp. 2574–2582.

[50] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer,
“Reluplex: An efﬁcient smt solver for verifying deep neural networks,”
in International Conference on Computer Aided Veriﬁcation. Springer,
2017, pp. 97–117.

[51] L. Pulina and A. Tacchella, “An abstraction-reﬁnement approach to
veriﬁcation of artiﬁcial neural networks,” in International Conference
on Computer Aided Veriﬁcation. Springer, 2010, pp. 243–257.
[52] H. J. Kang, T. F. Bissyand´e, and D. Lo, “Assessing the generalizability

of code2vec token embeddings,” in 2019 34th IEEE/ACM International
Conference on Automated Software Engineering (ASE).
IEEE, 2019,
pp. 1–12.

[53] M. Abuhamad, T. AbuHmed, A. Mohaisen, and D. Nyang, “Large-scale
and language-oblivious code authorship identiﬁcation,” in Proceedings
of the 2018 ACM SIGSAC Conference on Computer and Communica-
tions Security, 2018, pp. 101–114.

[54] X. Gu, H. Zhang, and S. Kim, “Deep code search,” in 2018 IEEE/ACM
40th International Conference on Software Engineering (ICSE).
IEEE,
2018, pp. 933–944.

[55] M. White, M. Tufano, C. Vendome, and D. Poshyvanyk, “Deep learning
code fragments for code clone detection,” in 2016 31st IEEE/ACM
International Conference on Automated Software Engineering (ASE).
IEEE, 2016, pp. 87–98.

