2
2
0
2

p
e
S
5

]
L
M

.
t
a
t
s
[

1
v
7
5
8
1
0
.
9
0
2
2
:
v
i
X
r
a

Statistical Comparisons of Classiﬁers by
Generalized Stochastic Dominance

Christoph Jansen
Department of Statistics
Ludwig-Maximilians-Universit¨at
Ludwigstr. 33, 80539 Munich, Germany

Malte Nalenz
Department of Statistics
Ludwig-Maximilians-Universit¨at
Ludwigstr. 33, 80539 Munich, Germany

Georg Schollmeyer
Department of Statistics
Ludwig-Maximilians-Universit¨at
Ludwigstr. 33, 80539 Munich, Germany

Thomas Augustin
Department of Statistics
Ludwig-Maximilians-Universit¨at
Ludwigstr. 33, 80539 Munich, Germany

christoph.jansen@stat.uni-muenchen.de

malte.nalenz@stat.uni-muenchen.de

georg.schollmeyer@stat.uni-muenchen.de

thomas.augustin@stat.uni-muenchen.de

Abstract
Although being a question in the very methodological core of machine learning, there is
still no unanimous consensus on how to compare classiﬁers. Every comparison framework is
confronted with (at least) three fundamental challenges: the multiplicity of quality criteria,
the multiplicity of data sets and the randomness / arbitrariness of the selection of data sets.
In this paper, we add a fresh view to the vivid debate by adopting recent developments in
decision theory. Our resulting framework, based on so-called preference systems, ranks clas-
siﬁers by a generalized concept of stochastic dominance, which powerfully circumvents the
cumbersome, and often even self-contradictory, reliance on aggregates. Moreover, we show
that generalized stochastic dominance can be operationalized by solving easy-to-handle
linear programs and statistically tested by means of an adapted two-sample observation-
randomization test. This indeed yields a powerful framework for the statistical comparison
of classiﬁers with respect to multiple quality criteria simultaneously. We illustrate and
investigate our framework in a simulation study and with standard benchmark data sets.
Keywords:
algorithm comparison, statistical test, generalized stochastic dominance,
preference system, decision theory

1. Introduction

1.1 Background

With a surge of new classiﬁcation algorithms, a statistically sound way to decide if a method
improves on its competitors is of great importance. This task has eo ipso a multi-dimensional
structure: one typically compares several classiﬁers over several data sets relying on sev-
eral criteria, with accuracy, AUC, and Brier score being popular choices. Depending on

©— Christoph Jansen, Malte Nalenz, Georg Schollmeyer and Thomas Augustin.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.

 
 
 
 
 
 
Jansen, Nalenz, Schollmeyer and Augustin

the speciﬁc ﬁeld, also other criteria, such as model size, interpretability, and computa-
tional demand may be of interest (see, e.g., the systematic taxonomy given in Lavesson and
Davidsson (2007) and the minimum requirements discussed in Yu and Kumbier (2020)). In
addition, in the last years, following Demˇsar (2006), attention has also been paid to the
question of statistical signiﬁcance of observed diﬀerences, understanding the investigated
data sets as a sample from a virtual universe of potential data sets.

If the quality criterion is commensurable over data sets, then tests on the diﬀerences of
means may be used, e.g., pairwise t-tests. An immediate way to reach conclusions from
a multidimensional structure is to summarize the diﬀerent components by a real-valued
quantity, for instance by considering rank aggregates of some quality criteria. Then also
tests in the vain of Demˇsar (2006) can be used to judge statistical signiﬁcance. Typical
studies proceeding that way include, for instance, Fern´andez-Delgado et al. (2014), where
algorithms are ranked based on their mean accuracy on each data set, Ismail Fawaz et al.
(2019), where time series classiﬁers are compared, or Graczyk et al. (2010), where diﬀerent
neural networks for regression are investigated.

Although being quite intuitive at ﬁrst glance, such approaches also have been shown to have
severe shortcomings. Of course, the used aggregation procedure suﬀers from substantial ar-
bitrariness. Moreover, even rather intuitively plausible aggregation procedures may show
quite paradoxical behavior; see, e.g., Benavoli et al. (2016), who substantially relativize
any comparison based on rank aggregation by demonstrating that adding classiﬁers to the
evaluation can change the comparison between the originally compared classiﬁers.

A systematic discussion and a clariﬁed understanding of the problem of classiﬁer compar-
ison could beneﬁt from embedding this problem into the framework of choice theory (see,
e.g., Brams and Fishburn (2002) for a classical review), where the proper aggregation of
preferences is a well-established topic; see, for instance, Eugster et al. (2012) or Mersmann
et al. (2015) for the connection between benchmarking algorithms and social choice theory.
However, relying on the classical setting of choice theory typically brings negative results,
but helps to understand paradoxical results better and to elaborate on the principled funda-
mental diﬃculties of our comparison task. Indeed, following Arrow’s impossibility theorem
(Arrow, 1950), in general, there is no aggregation rule that satisﬁes a set of seemingly intu-
itive minimum requirements. More precisely, any function aggregating tuples of preference
relations to a single one while satisfying a Pareto and an independence condition can be
shown to be dictatorial:
implicitly, only one of the quality criteria is considered. This
very general statement includes the Borda rule (originally proposed in de Borda (1781)),
in which classiﬁers are compared on the basis of their average ranks with respect to the
various quality criteria.

Unfortunately, even leaving the Arrovian framework and searching for seemingly convincing
aggregation rules outside it may not be successful. In particular, also the Condorcet rule
(originally proposed in de Condorcet (1785)), where a ranking between any pair of classi-
ﬁers is derived by counting which one performs better with respect to more quality criteria,
has undesirable properties. It is known to produce intransitive and – even worse – cyclic
rankings for certain constellations.

2

Comparing classifiers by stochastic dominance

Before we look deeper into recent developments of modern choice and decision theory found-
ing our proposal, it seems helpful to precisely formalize our problem.

1.2 Speciﬁcation and Formalization of the Problem

To formally describe the diﬀerent levels at which problems can arise when comparing
classiﬁers, we use the following notation, which will be revisited in Section 3: Let C de-
note the set of classiﬁers available in the problem under consideration and D denote a
set of data sets with respect to which the classiﬁers are to be compared. Let further be
φ1, . . . , φn : C × D → R diﬀerent criteria to measure the goodness of classiﬁcation of the dif-
ferent classiﬁers on the diﬀerent data sets (note that the assumption of all criteria mapping
to R is only for the simplicity of presentation and will be dropped later in Section 3). The
structure of the problem is summarized in Table 1 for the situation that C = {C1, . . . Cq}
and D = {D1, . . . , Ds}. A closer look at Table 1 now directly shows three diﬀerent levels of

data sets

classiﬁer

C1

...

Cq

D1











φ1(C1, D1)
...
φn(C1, D1)
...
φ1(Cq, D1)
...
φn(Cq, D1)

. . .




 . . .

...




 . . .

Ds











φ1(C1, Ds)
...
φn(C1, Ds)
...
φ1(Cq, Ds)
...
φn(Cq, Ds)











Table 1: A schematic presentation of the problem of comparing diﬀerent classiﬁers over
multiple data sets with respect to multiple quality criteria simultaneously.

challenges when comparing classiﬁers:

Level 1: In the case of multiple quality criteria, two classiﬁers can generally not be trivially
compared already on one single data set. For instance, consider a situation with conﬂicting
quality criteria such as φ1(C1, D) > φ1(C2, D) but at the same time φ2(C1, D) < φ2(C2, D).
This problem is also strongly connected with multi-objective optimization, where one can
ﬁnd and analyze the Pareto-optimal points (M¨ussel et al., 2012; Deb, 2014). However, with-
out further assumptions, no decision between the classiﬁers can be made in such situations:
The component-wise dominance relation is only a partial order (compare Section 2 for de-
tails). Another solution is the introduction of measures that combine and trade oﬀ multiple
dimensions into one single number (e.g. Brazdil et al. (2003) or Marler and Arora (2010)).

Level 2: Even if the problem in Level 1 can be circumvented somehow (for instance if we
indeed happen to have component-wise dominance), the rank order of classiﬁers that holds
over one ﬁxed data set may change or even completely reverse over another data set. For
instance it might hold that φi(C1, D1) > φi(C2, D1) for all i ∈ {1, . . . , n} but there exists
some i0 ∈ {1, . . . , n} such that φi0(C1, D2) < φi0(C2, D2). This makes the comparison of

3

Jansen, Nalenz, Schollmeyer and Augustin

classiﬁers a decision problem under uncertainty about the data sets and, of course, this
uncertainty should be adequately included in any further analysis of the problem.

Level 3: Since both the set of all relevant data sets and their probability distribution will
in general be unknown, it is often impossible to analyze the decision problem from Level
2 in practice. Instead, one can only analyze an empirical analogon of the problem over a
sample of data sets. This means that even if one has found ways to meaningfully solve the
problems of Levels 1 and 2 and thus could deﬁne a meaningful order of classiﬁers for the
concrete sample of data sets, a diﬀerent order of classiﬁers could occur as soon as another
sample of data sets is considered. The solution of such an empirical decision problem is
subject to statistical uncertainty. It is desirable to be able to control this statistical uncer-
tainty by constructing a suitable statistical test. For single criteria evaluation tests have
been proposed in Demˇsar (2006), see also Corani et al. (2017) for a Bayesian variant.

1.3 Sketch of our Proposal and Overview

In the present paper, we propose a framework that allows a comparison of classiﬁers with
respect to multiple quality criteria over multiple data sets simultaneously (therefore account-
ing for the problems of Levels 1 and 2). For also accounting for the statistical uncertainty
arising from the speciﬁc selection of the benchmark data sets (as described in Level 3), we
also propose a permutation-based statistical test for our criterion. Our comparison crite-
rion is based on a generalized notion of ﬁrst-order stochastic dominance and, in general,
provides only an incomplete ranking of the classiﬁers. Here, however, the ordering power of
our dominance relation can be explicitly modelled by a parameter whose increase attenuates
each quality dimension to the same extent instead of being limited to only one dimension.

Generally, addressing problems of multiplicity by stochastic dominance appears promising.
However, while approaches in this spirit are common in several ﬁelds ranging from biomet-
rics (e.g., Davidov and Peddada (2013)) to econometrics (e.g., Whang (2019)), the authors
are not aware of such contributions in machine learning.

To derive and discuss our framework, the paper is organized as follows: Section 2 recalls
the required mathematical deﬁnitions. Section 3 introduces the concept of δ-dominance be-
tween classiﬁcation algorithms, while Section 4 gives an algorithm for detecting δ-dominance
and discusses how to test for it if only a sample of data sets is available. Sections 5 and 6
demonstrate the ideas presented on simulated data and with standard benchmark data sets.
Section 7 concludes by elaborating on some promising perspectives for future research.

2. Preliminiaries

Throughout the paper, we will consider binary relations at several points, be it on the set
of all quality vectors as in Equation (1), on a binary relation itself as in Equation (2), or on
the set of all classiﬁers as in Deﬁnition 6. We, therefore, begin with a compilation of some
important concepts in this context. First, recall that a binary relation R on a non-empty
set M is a subset of the Cartesian product of the set with itself, that is R ⊆ M ×M . Several
(potential) properties of binary relation occur in what follows: R ⊆ M × M is called

• reﬂexive, if (m, m) ∈ R,

4

Comparing classifiers by stochastic dominance

• transitive, if (m1, m2), (m2, m3) ∈ R implies (m1, m3) ∈ R,

• antisymmetric, if (m1, m2), (m2, m1) ∈ R implies m1 = m2 ,

• complete, if (m1, m2) ∈ R or (m2, m1) ∈ R (or both)

for arbitrary elements m, m1, m2, m3 ∈ M . A preference relation is a binary relation that
is complete and transitive; a pre-order is a binary relation that is reﬂexive and transitive;
a linear order is a preference relation that is antisymmetric; a partial order is a pre-order
that is antisymmetric.

Equipped with these concepts, we can now deﬁne the central ordering structure for us,
so-called preference systems. With the help of these systems, it is possible to model ordered
sets on which the scale of measurement can vary locally: The (potential partial) ordinal
part of the system is modelled by a pre-order on the set itself, while the (potential partial)
metric part of the system is modelled by a pre-order on the ordinal relation, which covers
those parts of the set for which a strength of order can also be speciﬁed. The following
Deﬁnitions 1, 2, and 3 have been introduced in a decision-theoretic context by Jansen
et al. (2018) and are also discussed in Jansen et al. (2022). As these form the basis of our
generalized stochastic dominance concept, they are listed here for further reference.

Deﬁnition 1 Let A be a non-empty set and let R1 ⊆ A × A denote a pre-order on A.
Moreover, let R2 ⊆ R1 × R1 denote a pre-order on R1. Then the triplet A = [A, R1, R2] is
called a preference system on A.

Originally stemming rather from a decision-theoretic context, the relations R1 and R2 are
usually given a behavioristic interpretation: If (a, b) ∈ R1, this is interpreted as a being at
least as desirable as b, whereas if ((a, b), (c, d)) ∈ R2, this is interpreted as exchanging b by a
being at least as desirable as exchanging d by c. However, such an interpretation is far from
mandatory: As will be shown in Section 3, preference systems are also perfectly suited to
analyze multidimensional data structures with both ordinal and metric dimensions. Note
that the speciﬁc preference system based on the comparison of multiple quality measures
for classiﬁers simultaneously will be deﬁned in Equations (1), (2) and (3).

Since the formal deﬁnition of a preference system does not restrict the interaction of the
involved relations R1 and R2 in any way, usually, an additional consistency criterion is
introduced. Here, for a pre-order R ⊆ M × M on a set M , we denote by PR ⊆ M × M its
strict part and by IR ⊆ M × M its indiﬀerence part, respectively deﬁned by

(m1, m2) ∈ PR ⇔ (m1, m2) ∈ R ∧ (m2, m1) /∈ R,

(m1, m2) ∈ IR ⇔ (m1, m2) ∈ R ∧ (m2, m1) ∈ R.

This leads to the following deﬁnition.

Deﬁnition 2 The preference system A = [A, R1, R2] is consistent if there exists a func-
tion u : A → [0, 1] such that for all a, b, c, d ∈ A we have:

i) If (a, b) ∈ R1, then u(a) ≥ u(b) with = iﬀ (a, b) ∈ IR1.

ii) If ((a, b), (c, d)) ∈ R2, then u(a) − u(b) ≥ u(c) − u(d) with = iﬀ ((a, b), (c, d)) ∈ IR2.

5

Jansen, Nalenz, Schollmeyer and Augustin

The set of all such representations u satisfying i) and ii) is denoted by UA.

In the case of a consistent preference system that possesses R1-minimal and R1-maximal
elements in A, it may be useful to consider only representations that measure the utility of
consequences on the same scale. This proves of particular importance for the parameter δ
from Deﬁnitions 5 and 6 to have a meaningful interpretation in terms of regularization. We
have the following deﬁnition.

Deﬁnition 3 Let A = [A, R1, R2] be a consistent preference system containing a∗, a∗ ∈ A
such that (a∗, a) ∈ R1 and (a, a∗) ∈ R1 for all a ∈ A. Then

(cid:110)

NA :=

u ∈ UA : u(a∗) = 0 ∧ u(a∗) = 1

(cid:111)

is called the normalized representation set of A. Further, for a number δ ∈ [0, 1), we
denote by N δ

A the set of all u ∈ NA satisfying

u(a) − u(b) ≥ δ

∧ u(c) − u(d) − u(e) + u(f ) ≥ δ

for all (a, b) ∈ PR1 and for all ((c, d), (e, f )) ∈ PR2. We call A δ-consistent if N δ

A (cid:54)= ∅.

Finally, we will deﬁne the notion of an automorphism in the context of preference systems,
as we need this notion later in Proposition 1 and Proposition 2.

Deﬁnition 4 Let A = [A, R1, R2] be a preference system. A mapping T : A −→ A is called
automorphism if it is bijective and if furthermore for arbitrary a, b, c, d ∈ A we have

(a, b) ∈ R1 ⇔ (T (a), T (b)) ∈ R1 and
((a, b), (c, d)) ∈ R2 ⇔ ((T (a), T (b)), (T (c), T (d)) ∈ R2.

3. Comparing Classiﬁers by Generalized Stochastic Dominance

We now propose a criterion with which it is possible to compare classiﬁcation algorithms
with respect to multiple quality measures on multiple data sets simultaneously. The basic
idea of our criterion is based on the concept of ﬁrst-order stochastic dominance for random
variables over partially ordered sets, which (essentially) states: a variable X is stochastically
greater or equal than another variable Y if the expectation of the variable u ◦ X is greater
or equal than the expectation of the variable u ◦ Y for every real-valued representation
u of the underlying partial order (see, e.g., Mosler and Scarsini (1991) for more details).
Intuitively spoken, this means that X has a greater or equal expected value in every metric
interpretation of the underlying space that does not contradict the given ordering structure.

In the context of comparing classiﬁcation algorithms, the variables under consideration are
functions associated with the various classiﬁers that assign a vector of quality values, or short
quality vector, to each possible data set. The set of all these quality vectors is then partially
ordered by the component-wise greater or equal relation (see Equation (1) below). However,
since some of the considered quality measures may also be interpretable on a metric scale
(e.g. if one considers prediction accuracy as a quality measure), there is even more structure.
To be able to exploit this partial cardinal structure, we suitably deﬁne a preference system

6

Comparing classifiers by stochastic dominance

on the set of quality vectors (see Equation (2) below). A natural generalization of stochastic
dominance is then to require the expectation dominance mentioned above no longer for all
representations of the component-wise partial order, but only for all representations of the
constructed preference system in the sense of Deﬁnitions 2 and 3.

3.1 Generalized Stochastic Dominance

Before turning to the construction just described in detail, we give the following central
deﬁnition of generalized stochastic dominance over preference systems for arbitrary random
variables. This relation can be viewed as a generalization of ﬁrst-order stochastic dominance
in two respects: First, as just discussed, the relation R2 can also include partial metric
information. Second, the parameter δ allows to explicitly model from which threshold on a
diﬀerence in utility should be included in the analysis of the random variables.

Deﬁnition 5 Let A = [A, R1, R2] be a δ-consistent preference system and let [S, σ(S), π]
be a probability space. Denote by

(cid:110)

F(A,S) :=

X ∈ AS : u ◦ X is σ(S)-BR([0, 1])-measurable for all u ∈ UA

(cid:111)

For random variables X, Y ∈ F(A,S), we say that X (A, π, δ)-dominates Y , abbreviated
with X ≥(A,π,δ) Y , whenever it holds that

for all normalized representations u ∈ N δ

A respecting the threshold δ.

Eπ(u ◦ X) ≥ Eπ(u ◦ Y )

Remark 1 Consider again the situation of Deﬁnition 5. For the special case of R2 = ∅ and
δ = 0 the relation ≥(A,π,0) essentially reduces to classical ﬁrst-order stochastic dominance on
partially ordered sets. Further, for the special case of δ = 0 and relations R1 and R2 that are
compatible in the sense of satisfying the axioms in Krantz et al. (1971, Deﬁnition 1, p. 147)
and, thus, admitting a representation that is unique up to positive linear transformations,
the relation ≥(A,π,0) essentially reduces to the classical principle of maximizing expected
utility. Finally, again setting δ = 0, the relation ≥(A,π,0) can be viewed as that special case
of the relation R∀∀ from Jansen et al. (2018, p. 123), where the there mentioned set of
probability measures M is chosen to consist solely of π, that is M = {π} is a singleton.

3.2 Utilizing Generalized Stochastic Dominance for Comparing Classiﬁers

As indicated at the beginning of the section, we now show how the relation ≥(A,π,δ) can be
utilized to compare classiﬁcation algorithms with respect to multiple quality measures on
multiple data sets simultaneously. This requires some additional notation. Let

• D denote the set of all data sets that are relevant for the classiﬁcation task in question,

• C denote the set of all classiﬁers that intend to classify the data sets from D,

• φi : C × D → Qi denote a criterion of classiﬁcation quality for every i ∈ {1, . . . , n},

7

Jansen, Nalenz, Schollmeyer and Augustin

• φ := (φ1, . . . , φn) : D × C → Q, where Q := Q1 × · · · × Qn is the set of quality vectors.1

Speciﬁcally, for a data set D ∈ D and a classiﬁer C ∈ C, the (not necessarily numerical)
reward φi(C, D) is interpreted as the quality of the classiﬁer C for data set D with respect
to the classiﬁcation quality criterion φi. Importantly, note that the diﬀerent reward sets
Q1, . . . , Qn are not assumed to be of the same scale of measurement. In particular, this
implies that some of the sets are of ordinal scale (i.e. are equipped with a preference order
but no metric), while others allow for a metric interpretation (i.e. are equipped with both
a preference order and a metric). However, all of them are assumed to be of at least
ordinal scale and to possess minimal and maximal elements. For every i ∈ {1, . . . , n},
the preference order of the space Qi will be denoted by ≥i. Note already now that these
assumptions directly imply that the set Q possesses minimal and maximal elements w.r.t. R1
from Equation (1), ensuring the normalized representation set (see Deﬁnition 3) of the
preference system C from Equation (3) to be well-deﬁned.

Without loss of generality, we assume the quality criteria (φ1, . . . , φn) to be arranged such
that there exists k ∈ {1, . . . , n} for which the sets Q1, . . . , Qk are of metric scale, equipped
with metrics di : Qi × Qi → R, i = 1, . . . , k, respectively, whereas the remaining sets are
of ordinal scale not allowing for any meaningful metric interpretation. We then deﬁne a
preference system on the set Q of all quality vectors by setting

(cid:110)

R1 :=

(q, p) ∈ Q × Q : qi ≥i pi

for all i = 1, . . . , n

(cid:111)

(cid:110)

R2 :=

((q, p), (r, s)) ∈ R1 × R1 : di(qi, pi) ≥ di(ri, si) for all i = 1, . . . , k

(1)

(2)

(cid:111)

We denote the preference system which is composed of the set Q and the two relations just
deﬁned by C, i.e., we have that

C = [Q, R1, R2].

(3)

The two relations R1 and R2 can be given the following natural interpretation:

Interpretation of R1: Assume we have D ∈ D and Ci, Cj ∈ C such that φ(Ci, D) = q and
φ(Ci, D) = p. Then (q, p) ∈ R1 means that classiﬁer Ci has at least as high quality as
classiﬁer Cj for every considered quality measure, when evaluated on data set D.

Interpretation of R2: Assume we have D ∈ D and Ci, Cj, Ck, Cl ∈ C such that φ(Ci, D) = q
and φ(Ci, D) = p and φ(Ck, D) = r and φ(Cl, D) = s. Then ((q, p), (r, s)) ∈ R2 means that,
when evaluated on data set D, the dominance of Ci over Cj is at least as strong as the the
dominance of Ck over Cl. This is due to the fact that there is component-wise dominance
in both cases and, additionally, the quality diﬀerences of Ci and Cj are at least as high
as the quality diﬀerences of Ck and Cl for those quality measures that allow for a metric
interpretation.

To take the ﬁnal step of transferring the dominance criterion from Deﬁnition 5 to the
comparison of classiﬁers, we still need to be clear about the random component in this
context. This is obviously the randomness over the data sets since we are after all concerned

1. Since D may or may not contain labels, it is not necessary to distinguish between diﬀerent types of

classiﬁcation tasks (such as, e.g., supervised or unsupervised) within the proposed framework.

8

Comparing classifiers by stochastic dominance

with the expected classiﬁcation quality. So, if now [D, σ(D), π] is a suitable probability
space, we can use (C, δ)-dominance to compare classiﬁers with respect to all quality criteria
simultaneously. To stress the crucial role of the concept, this special case deserves a separate
deﬁnition for further reference.

Deﬁnition 6 Assume C to be δ-consistent. For Ci, Cj ∈ C, with C chosen such that
{φ(C, ·) : C ∈ C} ⊆ F(C,D), we say that Ci δ-dominates Cj, abbreviated with Ci (cid:37)δ Cj,
whenever it holds that

In other words, it holds that Ci (cid:37)δ Cj, whenever

φ(Ci, ·) ≥(C,π,δ) φ(Cj, ·)

Eπ(u ◦ φ(Ci, ·)) ≥ Eπ(u ◦ φ(Cj, ·))

for all normalized representations u ∈ N δ

C respecting the threshold δ.

From a decision-theoretic point of view, the threshold parameter δ can be motivated by
the concept of just noticeable diﬀerences discussed in the seminal work of Luce (1956): It
quantiﬁes the minimal utility diﬀerence the decision maker can notice/ﬁnds relevant given
utility is measured on a [0, 1]-scale. Translated to the context of comparing classiﬁers, it
rather can be seen as a regularization device: If some of the classiﬁers remain incomparable
for a threshold of δ = 0, then increasing δ provides the opportunity to strengthen the
ordering power of the dominance relation while attenuating the inﬂuence of all quality
measures used to the same degree. This proves particularly useful for the statistical test for
δ-dominance discussed in Section 4.2: Already very small values for δ can cause a remarkable
improvement of the power of the respective test, although the basic order is only marginally
changed (see also the discussion in Remark 3 and Footnote 6).

3.3 Some Useful Properties of the δ-Dominance Relation
The following proposition lists some important properties of the binary relation (cid:37)δ just
introduced. Despite their elementary character, some of these properties will play an im-
portant role when applying the concepts in Sections 5 and 6.

Proposition 1 Consider the same situation as in Deﬁnition 6. The following holds:

i) For every ξ ∈ [0, δ], the relation (cid:37)ξ deﬁnes a pre-order on C.

ii) The relations are nested with increasing δ, i.e., we have (cid:37)ξ1⊆(cid:37)ξ2 for ξ1 ≤ ξ2 ∈ [0, δ].

iii) Let T : A → A be an automorphism w.r.t. C. Then we have that Ci (cid:37)δ Cj if and only
p represents classiﬁer Cp, but evaluated not in

if CT
i
the space A, but in the space T [A], i.e., where φ(Cp, ·) is replaced by T ◦ φ(Cp, ·).

j , where, for p ∈ {i, j}, CT

(cid:37)δ CT

i) Reﬂexivity is trivially true. To verify transitivity, assume that Ci (cid:37)ξ Cj and
C arbitrarily (this is always possible, since δ-consistency obviously

Proof
Cj (cid:37)ξ Ck. Choose u ∈ N ξ
implies ξ-consistency). Then, by assumption and deﬁnition, it holds that

Eπ(u ◦ φ(Ci, ·)) ≥ Eπ(u ◦ φ(Cj, ·)) and Eπ(u ◦ φ(Cj, ·)) ≥ Eπ(u ◦ φ(Ck, ·))

9

Jansen, Nalenz, Schollmeyer and Augustin

directly implying

Eπ(u ◦ φ(Ci, ·)) ≥ Eπ(u ◦ φ(Ck, ·)).

As u was chosen arbitrarily, this implies Ci (cid:37)ξ Ck.
ii) Assume it holds ξ1 ≤ ξ2 ∈ [0, δ]. By deﬁnition and ξ-consistency for all ξ ∈ [0, δ] (see i)),
this implies ∅ (cid:54)= N ξ2

C . Assume it holds that Ci (cid:37)ξ1 Cj. By deﬁnition, this implies

C ⊆ N ξ1

Eπ(u ◦ φ(Ci, ·)) ≥ Eπ(u ◦ φ(Cj, ·))

for all u ∈ N ξ1
iii) Because of {u | u ∈ N δ

C} = {u ◦ T | u ∈ N δ

C} we have that

C and, due to the super set relation, also for all u ∈ N ξ2

C . Thus Ci (cid:37)ξ2 Cj.

∀u ∈ N δ

C : Eπ(u ◦ φ(Ci, ·)) ≥ Eπ(u ◦ φ(Cj, ·))

is equivalent to

∀u ∈ N δ

C : Eπ(u ◦ T ◦ φ(Ci, ·)) ≥ Eπ(u ◦ T ◦ φ(Cj, ·)),

which shows the claim.

Remark 2 Benavoli et al. (2016) convincingly questioned the idea of comparing classiﬁers
by comparing their average ranks over multiple data sets, where the ranks are computed with
respect to some single quality measure φ. The main problem of such an approach is that
the comparison of two classiﬁers may depend on other classiﬁers that are irrelevant for the
problem under consideration: Speciﬁcally, if only two classiﬁers C1 and C2 are considered,
C1 may get a higher average rank than C2, but this relation is exactly reversed if a third
classiﬁer C3 is considered, although the quality values of C1 and C2 are not changed. A
simple example for such a situation is given in Table 2.

φ(Ci, Dj) D1 D2 D3 D4 D5
0.6
0.8
0.8
0.6

0.8
0.6

0.6
0.8

0.8
0.6

C1
C2

φ(Ci, Dj) D1 D2 D3 D4 D5
0.6
0.8
0.8
0.6
0.7
0.9

0.6
0.8
0.7

0.8
0.6
0.9

0.8
0.6
0.9

C1
C2
C3

Table 2: On the left, C1 receives a rank sum of 8, dominating C2 with a rank sum of 7.
However, adding a third classiﬁer C3 (right table) dominating C1 and C2 on D1, D2
and D3 and lying between C2 and C1 for D4 and D5, gives C2 and C1 rank sums
of 9 and 8, respectively. The ordering of C1 and C2 is reversed.

In the case where also multiple quality criteria φ1, . . . , φ5 are considered, a similar situa-
tion may already occur on one speciﬁc ﬁxed data set D. For a simple example, one can
reinterpret the columns in Table 2 as the quality values φk(Ci, D) of the respective classiﬁer
with respect to the respective quality criteria on the ﬁxed data set D: Comparing the average
ranks of the classiﬁers across the quality criteria gives again reversed rankings of C1 and C2
for the tables on the left and on the right. Obviously, for the case of multiple classiﬁers and

10

Comparing classifiers by stochastic dominance

multiple quality criteria both problems may occur at the same time, thereby even increasing
the problem. Note that this fact is well-known in social choice theory: the Borda rule from
voting theory (see Section 1) does not satisfy Arrow’s axiom of independence of irrelevant
alternatives (see, e.g., Brams and Fishburn (2002)).

Generally, it seems that any method that uses ranks (and also any cardinal relative crite-
rion like that used in Webb (2000) and discussed in Demˇsar (2006)) is akine to violating
independence of irrelevant alternatives. Note that in Demˇsar (2006) the rationale behind
computing ranks is to make the quality values for diﬀerent data sets commensurable. This
implicitly assumes that beforehand the quality values for diﬀerent data sets cannot be com-
pared at all. If, at the same time, one also does not want to compare on one data set at
least the values/ranks of diﬀerent classiﬁers to avoid violating independence of irrelevant
alternatives, then one eﬀectively says that one quality value of one classiﬁer for one data
set cannot be compared to any other quality value at all. This obviously will lead to an un-
solvable undertaking. In our approach to ranking classiﬁers, we do not use ranks at all and
instead demand that the quality values for diﬀerent data sets are commensurable or have
been made commensurable beforehand. We strongly think that this is doable by specifying –
in a decision-theoretic rigorous manner – an adequate loss function that provides one with
adequate corresponding commensurable quality criteria.2

Against this background, one major advantage of comparing classiﬁers with respect to our
dominance relation (cid:37)δ instead of applying rank-based approaches, is that, in fact, indepen-
dence of irrelevant alternatives is guaranteed: If it holds that Ci (cid:37)δ Cj, then this statement
is independent of how the space C \ {Ci, Cj} looks like, i.e. of how many classiﬁers are con-
sidered in the analysis besides Ci and Cj. In this way our dominance relation circumvents
one major issue which has recently been raised in the context of rank-based comparisons.

Another nice structural property of the relation (cid:37)δ is that iii) in Proposition 1 is still valid
for a random automorphism, as long as this random automrphism is applied independently
of the process that generates the data sets. We will concretize this property in the following

Proposition 2 Let I be an index set, let {Tz | z ∈ I} be a family of automorphisms w.r.t. C
and let Z : Ω → I be an indexing random variable with law P that is independent of the
process that generates the data sets D. Let, for the moment, E be a shorthand notation
for the expectation w.r.t. the product law π ⊗ P . Let now T (ω) := TZ(ω) be a random
automorphism. Furthermore, assume that for every u ∈ N δ
C the conditional expectations
E(u ◦ T ◦ φ(Ci, ·) | Z = z) and E(u ◦ T ◦ φ(Cj, ·) | Z = z) exist. Then we have

∀u ∈ N δ

C : E(u ◦ φ(Ci, ·)) ≥ E(u ◦ φ(Cj, ·))

if and only if

∀u ∈ N δ

C : E(u ◦ T ◦ φ(Ci, ·)) ≥ E(u ◦ T ◦ φ(Cj, ·)).

(4)

(5)

2. Otherwise, decision theory would be ‘science about nothing’: If a decision maker is not able to quantify
a gain/loss of a speciﬁc method in a speciﬁc situation, then she should not consult decision theory.

11

Jansen, Nalenz, Schollmeyer and Augustin

Proof Let (4) hold. Then for every ﬁxed automorphism Tz and every arbitrary u ∈ N δ
C
we have Eπ(u ◦ Tz ◦ φ(Ci, ·)) ≥ Eπ(u ◦ Tz ◦ φ(Cj, ·)) and therefore E(u ◦ Tz ◦ φ(Ci, ·)) ≥
E(u ◦ Tz ◦ φ(Cj, ·)) (compare Proposition 1). This implies

E(u ◦ T ◦ φ(Ci, ·)) =

=

(cid:90)

(cid:90)

E(u ◦ T ◦ φ(Ci, ·) | Z = z) dP (z)

E(u ◦ Tz ◦ φ(Ci, ·)) dP (z) ≥

(cid:90)

E(u ◦ Tz ◦ φ(Cj, ·)) dP (z)

= E(u ◦ T ◦ φ(Cj, ·))

and therefore (5) holds for every arbitrary u ∈ N δ
analaguously by applying the corresponding inverse (random) automorphism.

C. The implication (5) =⇒ (4) follows

Proposition 2 has a nice implication: If the quality values are observed with some additional
noise that can be described by a random automorphism, then the dominance criterion will
not change. Note that especially a random intercept or a random scaling of the cardinal
dimensions will not inﬂuence the notion of dominance. This particularly implies that in our
simulation study (see Section 5) we do not need to implement such random eﬀects.

3.4 The GSD-δ Method
As (cid:37)δ deﬁnes a pre-order on the set C of all considered classiﬁers (see Proposition 1), it
naturally induces an ordering structure on this set. The method of obtaining this ordering
structure by relying on generalized stochastic dominance as the underlying relation, will be
referred to as GSD-δ in the following (with GSD-0 abbreviated by GSD). In order to make
this method applicable in practice, two substantial questions have to be addressed. First,
the question on how to eﬃciently check for δ-dominance arises. Second, a test must be
developed for judging if in-sample diﬀerences between classiﬁers are statistically signiﬁcant.

4. Testing for Dominance

In this section, we ﬁrst establish a linear program for checking δ-dominance between two
classiﬁers if the set D of data sets is ﬁnite and the true probability law π over this set is
known. Taking into account the problem described in Level 3 from Section 1, i.e., the fact
that both π and the set D will in general be inaccessible, we then describe how to adapt this
linear program to check for δ-dominance in its empirical version, i.e., in the concrete sample
of data sets drawn from the distribution π. Afterwards, we discuss how the optimal value of
this adapted linear program can be reinterpreted as a test statistic for a statistical test for
distributional equality of the two competing classiﬁers and discuss how to extend this test
to the complete ordering structure between all considered classiﬁers. Finally, in preparation
for the comparative study carried out in Section 5, we brieﬂy review the rank-based test
proposed in Demˇsar (2006) and suggest ways to extend it to more than one quality criterion.

4.1 A Linear Program for Checking δ-Dominance

We begin by discussing a linear program for checking δ-dominance in the ﬁnite case. For
that, consider again the preference system C as deﬁned in Equation (3), however, with the

12

Comparing classifiers by stochastic dominance

additional assumption that the sets C (the classiﬁers under consideration) and D (the data
sets relevant for the comparison) are both ﬁnite. Without loss of generality, we can then
assume Q = {q1, . . . , qd} to be ﬁnite and that q1 and q2 are minimal and maximal elements
of Q with respect to R1, respectively.3 Moreover, we assume δ ∈ [0, 1) to be chosen such
that C is δ-consistent. A vector (u1, . . . , ud) ∈ [0, 1]d then contains the images of a utility
function u : Q → [0, 1] from N δ

C if it satisﬁes the system of linear (in-)equalities given by

• u1 = 0 and u2 = 1,

• ui = uj for every pair (qi, qj) ∈ IR1,
• ui − uj ≥ δ for every pair (qi, qj) ∈ PR1,
• uk − ul = ur − ut for every pair of pairs ((qk, ql), (qr, qt)) ∈ IR2 and
• uk − ul − ur + ut ≥ δ for every pair of pairs ((qk, ql), (qr, qt)) ∈ PR2.

Denote by ∇δ
We then have the following proposition on how to check δ-dominance.

C the set of all vectors (u1, . . . , ud) ∈ [0, 1]d satisfying all these (in)equalities.

Proposition 3 Consider the same situation as described above. For Ci, Cj ∈ C, we con-
sider the linear programming problem

d
(cid:88)

(cid:96)=1

u(cid:96) · [π(φ(Ci, ·)−1({q(cid:96)})) − π(φ(Cj, ·)−1({q(cid:96)}))] −→ min

(u1,...,ud)∈Rd

(6)

with constraints (u1, . . . , ud) ∈ ∇δ
problem. It then holds that Ci (cid:37)δ Cj if and only if optij ≥ 0.

C. Denote by optij the optimal value of this programming

Proof First, let optij ≥ 0. Choose u ∈ N δ
objective function of the linear program. We then have

C arbitrarily and let g : Rd → R denote the

D(u) := Eπ(u ◦ φ(Ci, ·)) − Eπ(u ◦ φ(Cj, ·)) = g(u(q1), . . . , u(qd)) ≥ 0

(7)

where the equation follows by simple manipulations of the expected values and the lower
bound of 0 follows since, by deﬁnition, (u(q1), . . . , u(qd)) ∈ ∇δ
C was chosen
arbitrarily, this implies Ci (cid:37)δ Cj.
Conversely, let optij < 0. Choose (u∗
d) ∈ ∇δ
optij and deﬁne u : Q → [0, 1] by setting u(qi) := u∗
distinguish two diﬀerent cases:
Case 1: δ > 0. One then easily veriﬁes that u ∈ N δ

C to be an optimal solution yielding
i for all i = 1, . . . , d. We then have to

C. Since u ∈ N δ

1, . . . , u∗

C and

D(u) = g(u∗

1, . . . , u∗

d) = optij < 0

(8)

Thus, u is a function from N δ

C with Eπ(u ◦ φ(Ci, ·)) < Eπ(u ◦ φ(Cj, ·)). Thus ¬(Ci (cid:37)δ Cj).

3. As the sets C and D are ﬁnite, it makes no diﬀerence in what follows, if we replace Q by the ﬁnite set
φ(C × D). If φ(C × D) does not contain minimal and maximal elements, we deﬁne new vectors q1 and q2
containing an minimal or maximal element of Qi in every dimension i, respectively. By re-indexing the
remaining vectors and considering the ﬁnite set φ(C × D) ∪ {q1, q2} we are done.

13

Jansen, Nalenz, Schollmeyer and Augustin

1, . . . , u∗

C. Then, since (u∗

Case 2: δ = 0. If u ∈ N 0
C, then the same argument as in the ﬁrst case applies. Thus, assume
that u /∈ N 0
C, we still know that u is monotone but we no
longer have strict monotonicity with respect to the relations R1 and R2 of C (meaning
that properties i) and ii) from Deﬁnition 2 are still valid but without the iﬀ condition).
Now, choose u+ ∈ N 0
C arbitrarily (this is always possible, since we assume 0-consistency).
If D(u+) < 0, then Eπ(u+ ◦ φ(Ci, ·)) < Eπ(u+ ◦ φ(Cj, ·)). This yields ¬(Ci (cid:37)δ Cj).
If
D(u+) ≥ 0, then we have

d) ∈ ∇0

0 ≤ ξ :=

D(u+)
D(u+) − D(u)

< 1

and we can choose α ∈ (ξ, 1). One then easily veriﬁes that uα := αu + (1 − α)u+ ∈ N 0
C
and that Eπ(uα ◦ φ(Ci, ·)) < Eπ(uα ◦ φ(Cj, ·)). This again yields that ¬(Ci (cid:37)δ Cj), thereby
completing the proof.

4.2 A Statistical Test for δ-Dominance

Typically, the setting discussed in Section 4.1 will be heavily idealized as actually we are
in the situation described in Level 3 from Section 1: The true probability law π on the
set D as well as the set D itself will be unknown and inaccessible and, thus, the algorithm
Instead of
for checking δ-dominance from Proposition 3 will not be directly applicable.
sample
knowing the true components, we thus usually will have to work with an i.i.d.
D1, . . . , Ds ∼ π of data sets from D in such cases. Accordingly, for deﬁning an empirical
version of the algorithm, i.e., an algorithm for checking δ-dominance in the observed sample,
we set ˆDs := {D1, . . . , Ds} and then consider the empirical law given by

ˆπ(W) :=

1
s

· |{j : j ∈ {1, . . . , s} ∧ Dj ∈ W}|

(9)

for all W ∈ 2 ˆDs. We then can simply run Proposition 3 with D replaced by ˆDs and π
replaced by ˆπ. Of course, the result of this empirical version of Proposition 3 is then
subject to statistical uncertainty: even if the optimal value indicates δ-dominance within
the observed sample of data sets, this might not generalize to the true space D. Conversely,
it might also happen that there is δ-dominance in the true space D, however, this dominance
cannot be detected in the observed sample.

In order to control the probability of an erroneous conclusion, an appropriate statistical
test should be carried out. A statistical test for the similar setup of classical stochastic
dominance between random variables with values in partially ordered sets is discussed in
Schollmeyer et al. (2017) and based on the two-sample observation-randomization test to
be found, e.g., in Pratt and Gibbons (2012, Chapter 6). We now demonstrate how such test
can be transferred to our setting: As already emphasized, for the empirical version, classiﬁer
Ci dominates classiﬁer Cj if and only if the optimal value optij of the linear program (6)
is greater than or equal to zero. Therefore, it is natural to calculate optij in the observed
sample and reject the null hypothesis

H0 : Cj (cid:37)δ Ci

14

(10)

Comparing classifiers by stochastic dominance

if this value is larger than a critical value c. Since the distribution of the statistic optij under
the null hypothesis is diﬃcult to handle, we use a permutation test that randomly swaps
the labels of the classiﬁers for every data point, i.e., for every data set in the sample. In this
way we can analyze the distribution of the test statistic under the most extreme hypothesis
in H0, i.e., the situation where the quality vectors of Ci and Cj are identically distributed.
Then one can reject the null hypothesis if the value of optij for the actually observed data
sets is larger than the (1 − α)-quantile of the values obtained under the resampling scheme.

Importantly, note that we are actually interested in a statistical test that is only sensitive
for deviations from H0 in the direction of δ-dominance in the sense of Ci (cid:31)δ Cj. Therefore
it would be desirable to take as the null hypothesis the negation of Ci (cid:31)δ Cj, however,
under this null-hypothesis, the analysis of the distribution of optij seems to be diﬃcult.
Additionally, at least for R2 = ∅ and δ = 0, which corresponds to classical ﬁrst-order
stochastic dominance, a consistent test seems to be unreachable,4 cf., Whang (2019, p.106)
and also Garcia-Gomez et al. (2019).

The concrete procedure for evaluating the distribution of optij has the following ﬁve steps:

Step 1: Use the sampled data sets to produce two separate samples (x1, . . . , xs) and
(y1, . . . , ys) from Q, one for each classiﬁer under consideration. Thereby, we used the
notations xl := φ(Ci, Dl) and yl := φ(Cj, Dl) for all l = 1, . . . , s.

Step 2: Take the pooled sample z = (x1, . . . , xs, y1, . . . , ys).

Step 3: Take all index sets I ⊆ {1, . . . , 2s} of size s and compute the optimal out-
come optI
ij of the linear program (6) that would be obtained if Ci would have pro-
duced the quality vectors (zi)i∈I and if Cj would have produced the quality vectors
(zi)i∈{1,...,2s}\I .

Step 4: Sort all optI

ij in increasing order.

Step 5: Reject H0 if optij is greater than the (cid:100)(1−α)·(cid:0)2s
ordered values optI

ij, where α is the envisaged conﬁdence level.

s

(cid:1)(cid:101)-th value of the increasingly

If (cid:0)2s
s
compute optI

(cid:1) is too large, instead of computing optI

ij for all index sets I, one can alternatively

ij only for a large enough number N of randomly drawn index sets I.

Remark 3 Three important points should be added:

i) If the statistical test described in (10) is to be used to test the entire order structure
on the set C instead of just a single pairwise comparison, it must be performed for
n · (n − 1) pairs. Then, it must be corrected for multiple testing to guarantee the
speciﬁed global signiﬁcance level.

ii) As already discussed at the end of Section 3.2, the parameter δ acts as a regularizer.
This becomes even clearer in the context of statistical testing: For δ = 0, the maximum
value of the linear program (6) is exactly zero in the dominance case and strictly less

4. A promising line of future research could be to reﬂect on whether the introduction of δ (cid:54)= 0 and/or

R2 (cid:54)= ∅ indeed leads to a consistent test for the now ‘regularized’ null- and alternative hypotheses.

15

Jansen, Nalenz, Schollmeyer and Augustin

than zero otherwise. Thus, in this case, it is impossible to compare the extent of
dominance for two diﬀerent dominance situations using our test statistic. If, on the
other hand, we choose a value δ > 0 for the test, the maximum value of the linear
program (6) in the dominance case can also assume values strictly greater than zero.
In this way, diﬀerent dominance situations can also be compared with each other in
this case: The further the maximum value is above zero, the greater the extent of
dominance. This potentially increases the power of the test, since situations can also
be distinguished in which dominance is present in the resample, but a greater degree
of dominance is present in the sample.

iii) As the parameter δ changes, of course, the hypotheses of the statistical test (10) also
change. Thus, strictly speaking, a diﬀerent statistical test is performed for each δ.
However, it is important to note here that the extreme case of distributional equality
of the two competing classiﬁers for any δ belongs to the null hypothesis. Thus, the
test from (10) for arbitrary choices of δ is suitable for detecting systematic diﬀerences
in the distributions of the classiﬁers. Furthermore, it can be argued that a very small
value of δ > 0, changes the order little to nothing compared to (cid:37)0 (and thus the
hypotheses of the associated statistical tests). However, it is shown (not least in the
simulation study from Section 5) that even such a very small value of δ > 0 can have
a clearly visible positive eﬀect on the power of the associated test.

4.3 Alternative Statistical Tests

As alternative methods to GSD-δ, we now brieﬂy review the rank-based test for compar-
ing competing classiﬁers as proposed in Demˇsar (2006) and suggest ways to extend it to
more than one quality criterion. In the original test, classiﬁers are ranked on each data set
based on their quality, typically estimated via cross-validation. Note that ranking is only
straight-forward for one criterion, with no obvious way to extend it to multiple dimensions.

Ranks are then averaged over all data sets. The rationale is that data sets vary in diﬃculty
and therefore ranking is a way to bring the diﬀerent data sets on the same scale and avoid
normality assumptions. The Friedman test can be applied to test for overall diﬀerences in
mean-ranks. If signiﬁcant diﬀerences are detected, post-hoc tests, such as the Nemenyi test
can be used to determine which pairs of classiﬁers are signiﬁcantly diﬀerent. In the second
step some form of correction for multiple testing is required to hold the overall α-level.

As the test by Demˇsar (2006) only accounts for diﬀerences between classiﬁers with respect
to one single quality criterion, it must ﬁrst be adapted to the setting with multiple quality
criteria in order to allow a meaningful comparison with our approach. To reach a decision
for multiple quality criteria we propose the following two intuitive heuristics:

all-test: Classiﬁer Ci is considered better than Cj if it performs signiﬁcantly better on each
quality criterion.

one-test: Classiﬁer Ci is considered better than classiﬁer Cj if Ci performs signiﬁcantly
better in at least one dimension and if, additionally, in any other dimension classiﬁer Cj
does not perform signiﬁcantly better than classiﬁer Ci.
It should be noted that when using the all-test or one-test heuristics the α-level of the one-

16

Comparing classifiers by stochastic dominance

dimensional tests is no longer preserved. In case of the one-test the true type 1 error will
exceed α and is therefore no longer a valid α-level test and instead becomes over sensitive
(cf. Section 5 and Appendix A3 for an example of this eﬀect on simulated data). The
all-test will often lead to a type 1 error much lower than α, as all dimensions need to be
signiﬁcant and can be therefore considered as a very conservative α-level test. We do not
adjust the α level, but instead note, that the two approaches are perhaps the most intuitive
ways to combine tests on several quality criteria.

5. A Simulation Study

In this section, we perform a simulation study to compare the proposed statistical test for
δ-dominance from (10) for two diﬀerent choices of δ with the adapted rank-based heuristics
(the all-test and the one-test) as discussed in Section 4.3. In addition, we compare how
well the relation (cid:37)δ can reproduce the order structure in the groundtruth of the simula-
tion when evaluated only in the sample, and again contrast the rank-based sample orders
comparatively. Further, we shed some light on the role of δ for the performance of our test.

5.1 Design of the Simulation Study

The simulation study is designed as follows: Seven simulated classiﬁers C1, . . . , C7 with
expected performance θi ∈ [0, 1]2 on two dimensions (i.e. on two quality criteria) are com-
pared. Both quality criteria are assumed to be interpretable on a metric scale in the sense
that both contribute to the construction of the relation R2 from Equation (2). The struc-
ture among the classiﬁers C1, . . . , C7 in the groundtruth is then induced by the recursive
graph shown in Figure 1, where a separation parameter η controls the expected diﬀerence
in performance. A speciﬁc example for such a recursive graph is given in Figure 2.

The performances xij of classiﬁer Ci on data set Dj, where j = 1, · · · , s, are i.i.d. drawn
from a normal distribution, i.e., xij ∼ N2(θi, Σ(cid:15)), where Σ(cid:15) = σ(cid:15)I and σ(cid:15) is a noise term,
which, together with η controls the diﬃculty in unravelling the underlying dominance struc-
ture. Note that the θi’s do not depend on the data set and therefore the diﬃculty is set
to be the same for each data set. Due to Proposition 2, the setting of varying diﬃculties
is also implicitly covered for many relevant situations: In particular, a random intercept
or a random scaling of the cardinal dimensions will not inﬂuence the notion of dominance,
making the simulation setup quite general (see also the discussion at the end of Section 3.3).

In the groundtruth just described, independent of the choice of η > 0, there are ten pairs
of classiﬁers between which there is component-wise dominance in expectation, whereas
all other pairwise comparisons are set to be not dominated, meaning that each classiﬁer
is preferable on one dimension. It is important to note that under the assumption of in-
dependence and constant variances, the component-wise dominance in expectation in the
groundtruth also implies each of the three types of dominance discussed earlier, namely
δ-dominance as well as the dominance w.r.t. the average rank of each classiﬁer along the
data sets under each quality criterion. Thus, in the groundtruth of the simulation there are
also ten pairs of classiﬁers which are in relation with respect to the orders underlying the
test for δ-dominance, the all-test and the one-test, respectively.

17

Jansen, Nalenz, Schollmeyer and Augustin

As the tests proposed by Demˇsar (2006) are only one-dimensional, we use the all-test and
one-test heuristic described in Section 4.3 as our best eﬀort to generalize the test to multiple
dimensions for a meaningful comparison.

θ1 =

(cid:19)

(cid:18)1
1

θ2 = θ1 −

(cid:19)

(cid:18) η
2η

θ3 = θ1 −

(cid:19)

(cid:18)2η
η

θ4 = θ2 −

(cid:19)

(cid:18)0.5η
0.5η

θ5 = θ2 −

(cid:19)

(cid:18)0.25η
η

θ6 = θ3 −

(cid:18) η

(cid:19)

0.25η

θ7 = θ3 −

(cid:19)

(cid:18)0.5η
0.5η

Figure 1: Simulation setting specifying the dominance between the simulated classiﬁers.

θ1 =

(cid:19)

(cid:18)1
1

θ2 =

(cid:19)

(cid:18)0.9
0.8

θ3 =

(cid:19)

(cid:18)0.8
0.9

θ4 =

(cid:19)

(cid:18)0.85
0.75

θ5 =

(cid:19)

(cid:18)0.875
0.7

θ6 =

(cid:19)

(cid:18) 0.7
0.875

θ7 =

(cid:19)

(cid:18)0.75
0.85

Figure 2: Example for ∆ = 0.1. Each entry in θ is the expected value on the correspond-
ing quality dimension drawn from normal distribution with ﬁxed variance and
normalized to [0, 1].

5.2 Results

In this simulation setup, we consider a total of twelve diﬀerent simulation scenarios, namely
all combinations of η ∈ {0.01, 0.05, 0.1} (i.e. varying the separation parameter) and s ∈
{7, 10, 15, 18} (i.e. varying the number of sampled data sets). Within each simulation sce-
nario, we carry out a total of 25 simulation runs. In each of the simulation runs, the number
of resamples drawn for the corresponding resample test is chosen to increase with the num-
ber d of sampled data sets. Concretely, we are interested in the following questions:

18

Comparing classifiers by stochastic dominance

How well does the order structure found in sample reproduce the groundtruth?

To answer this question, we proceed as follows: In each simulation run, we compute the
order structure of the three orders underlying the tests in the sample and compare it to
the true dominance structure in the ground truth. Speciﬁcally, we compute δ-dominance
in the sample by the empirical variant of the linear program from Proposition 3 discussed
earlier. For receiving the (coinciding) orders underlying the all-test and the one-test, we
compute the average rank of each classiﬁer along the data sets under each quality criterion,
and then deﬁne a classiﬁer to dominate another one, whenever its average rank is superior
in both quality dimensions. To measure the similarity of the orders in groundtruth and
in sample, we use the F-score, i.e., a trade-oﬀ measure between non-detected dominances
(false negatives, FN ) and falsely detected dominances (false positives, FP ).5

The results of the analyses carried out in the samples for the twelve simulation scenarios
are visualized in Figure 3. The results show a balanced picture with regard to the diﬀerent
methods: All methods reproduce the order in the ground truth about equally well. As ex-
pected, the F-score of the methods tends to improve with increasing separation parameter
η and increasing sample size d, with some random ﬂuctuations which are especially visible
for the lowly separated simulation scenarios.

How well do the statistical signiﬁcance tests reproduce the groundtruth?

To answer this question, under each simulation run in each scenario, we perform four diﬀer-
ent statistical tests: the δ-dominance test for δ = 0, the δ dominance test for δ = 10−5, the
all-test, and the one-test.6 It is important to note that the one-test was included only for
the sake of completeness: As already described in Section 4.3, this test in general will not
adhere to (and often drastically exceed) the speciﬁed α-level. Thus, the comparison with
signiﬁcance tests at this level is of course extremely problematic.

To measure the similarity of the order in the ground truth and the order given by the
signiﬁcant edges, we again use the F-score. The used global conﬁdence level is α = 0.05.
The method used for correcting for multiple testing is the (very conservative) Bonferroni-
correction for all four tests. The results of the analyses carried out at the test level for the
twelve simulation scenarios are visualized in Figure 4.

Here, some remarkable observations suggest themselves: Under each simulation scenario,
both tests for δ-dominance reveal the order structure at a global signiﬁcance level of α = 0.05
at least as well as the all-test heuristic. This dominance becomes increasingly clear as the
separation parameter δ and the number d of simulated data sets increase. As expected,
the F-score of all methods tends to improve with increasing separation parameter η and
increasing sample size d. Interestingly, both tests for δ-dominance outperform also the one-
test heuristic for separation parameters η ≥ 0.05 and at least 15 data sets, although this

5. The F-Score is deﬁned as F =

2·TP+FP+FN , where TP denotes the number of correctly detected domi-
nances. We chose this measure due to its popularity, however, an analysis under other measures (such
as the Jaccard index, see Jaccard (1912)) essentially yields the same results.

2·TP

6. The idea behind choosing an extremely small value such as 10−5 for δ in the second test, is to change
the hypotheses of the test as little as possible compared to the test for δ = 0, but still beneﬁt from the
gain in power that a strictly positive δ brings (cf. Remark 3 for further details).

19

Jansen, Nalenz, Schollmeyer and Augustin

heuristic exceeds the given ﬁrst-type error probability of 0.05 (cf. A3).7

Furthermore, the comparison of the two tests for δ-dominance for δ = 0 and δ > 0 conﬁrms
the gain in power for the latter case already theoretically indicated in Remark 3 iii): The
F-Score of the dominance test for δ > 0 exceeds the one of the dominance test for δ = 0 for
every simulation scenario. Especially remarkable is the fact that this eﬀect already occurs
for a very small value of δ = 10−5. Since for such a small δ the order (cid:37)δ is presumably
changed only very marginally compared to (cid:37)0, this suggests once more that the parameter
δ, in addition to its decision-theoretic interpretation, also has a pure regularization compo-
nent and helps to make the hypotheses more separable.

Summary of the results: We have shown that the proposed statistical test for δ-dominance
reveals the ordering structure in the groundtruth more adequately than the all-test heuristic
in each of the considered simulation scenarios. Further, we demonstrated that in the sce-
narios with at least medium separation (η ≥ 0.05) and enough data sets available (d ≥ 15),
the tests for δ-dominance also outperform the one-test heuristic, even if this heuristic does
not guarantee the global α-level. Finally, it turned out that the test for δ-dominance with
δ > 0 reproduces the order structure in all simulation scenarios at least as well as the test
with δ = 0, even for very small choices of the parameter δ.

6. Application

We now showcase on standard benchmark data sets how the relation (cid:37)δ and the resulting
GSD-δ method can be used to rank classiﬁers based on their performance on multiple data
sets with respect to multiple quality criteria. In addition, we use the statistical test proposed
in Section 4.2 to investigate which of the orderable pairs of classiﬁers found in the sample
may also be assumed to be statistically signiﬁcant. As in Section 5, we again compare our
results with those obtained under the adapted rank-based tests from Section 4.3. Finally,
we examine how our results change when the analyses are based on classical stochastic
dominance instead of the generalized stochastic dominance order (cid:37)δ.

6.1 Experimental Setup

For comparison we use 16 binary classiﬁcation benchmark data sets. All data sets are taken
from the UCI machine learning repository (Dua and Graﬀ, 2017). The data sets strongly
vary in size, dimensionality and class imbalance.

For the classiﬁer comparison, we consider the three well established metrics accuracy, area
under the curve and Brier score. On each data set 10-fold cross-validation is performed
and results are averaged for each metric and classiﬁer separately. Importantly, note that in
following analyses all three of these quality criteria are considered to be of metric scale and,

7. We note that much better trade-oﬀs (reﬂected in F-score) can be found for GSD and GSD-δ if we do not
enforce an overall but instead individual α-level of 0.05 (cf. A3 for the evaluation without Bonferroni
correction, where we can see that GSD uniformly outperforms the one-test). Presumably, this could also
be achieved using a more eﬃcient multiple-testing correction strategy.

20

Comparing classifiers by stochastic dominance

Figure 3: The ﬁgure shows the empirical distribution of the F-scores (larger is better) in
the samples (without statistical test) of the diﬀerent methods along the 25 simu-
lation runs separately for the twelve diﬀerent scenarios. The F-score is computed
by counting the number of TPs, FPs, and FNs in the respective sample order
compared with groundtruth and then evaluating the formula from Footnote 5.

accordingly, all equally contribute to the construction of the relation R2 as most generally
deﬁned in Equation (2).8 We compare two groups of algorithms:

• For decision tree based classiﬁers we included classiﬁcation and regression trees (CART)
(Breiman et al., 1983), random forests (RF) (Breiman, 2001), gradient boosted trees
(GBM) (Friedman, 2002) and boosted decision stumps (BDS) (trees with depth 2).

• As examples of more traditional models we included generalized linear models (GLM),
lasso regression (LASSO) (Tibshirani, 1996), elastic net (EN) (Zou and Hastie, 2005)
and ridge regression (RIDGE), implemented in the glmnet R-package.

8. An exception is of course given by Section 6.3, where a comparison with classical stochastic dominance

is considered, and, thus, all three quality criteria are considered to be purely ordinal for this case.

21

Jansen, Nalenz, Schollmeyer and Augustin

Figure 4: The ﬁgure shows the empirical distribution of the F-scores (larger is better) of
the signiﬁcant orders of the diﬀerent methods along the 25 runs separately for
the twelve scenarios. The F-score is computed by counting the number of TPs,
FPs and FNs in the respective signiﬁcant order compared with groundtruth. To
account for multiple testing the Bonferroni-correction is used in all cases.

Generally, we expected the ensemble methods RF and GBM to dominate other methods,
especially CART, whereas the ordering of the remaining methods is expected to be less
clear. More details on data set selection, quality metrics and algorithm implementation can
be found in Appendix A2.

6.2 Results

In the sample of data sets just described, evaluated and visualized in the three Hasse
diagrams9 in Figure 5, the following picture emerges: even though some of the classiﬁers

9. Hasse diagrams are graph representations of partial orders: Whenever two nodes can be connected by a
path leading top down in the graph, then the upper node dominates the lower node with respect to the
considered partial order. Nodes that cannot be connected by such a top down path are incomparable.

22

Comparing classifiers by stochastic dominance

remain incomparable even for a maximum threshold of δmax = 0.0077, concretely BDS and
RF as wells as EN and LASSO, a clear best classiﬁer for this sample can be identiﬁed already
for a minimum threshold of δmin = 0, namely GBM. In this case also two clear second-best,
but incomparable to each other, classiﬁers can be seen, namely RF and BDS.

While an analysis under δmin leaves the classiﬁer pairs

(GLM,RIDGE), (GLM, EN), (GLM, LASSO), (RIDGE, LASSO), (RIDGE, EN)

incomparable to each other, raising the threshold to an intermediate level of δ = 0.004
makes RIDGE dominant to both EN and LASSO. Raising the threshold even more to δmax,
the GLM classiﬁer becomes dominant over EN, LASSO and RIDGE. Note that the order un-
der threshold δmax is the most structured relation we can hope for in this concrete sample:
There exists no δ for which the relation (cid:37)δ is a linear (or a preference) order. In particular,
the classiﬁers BDS and RF as wells as EN and LASSO remain incomparable in this sample
for no matter what threshold value is chosen.

Next, similar to what we did in the simulation study in Section 5, we perform the following
three statistical tests for all pairwise comparisons: the test for δ-dominance from Equa-
tion (10) for δmin and δ = 10−5, as well as the all-test as described in Section 4.3. The
one-test is omitted since this test will in general drastically exceed the envisaged conﬁdence
level α, which was illustrated in the simulation study in Section 5. Also note that the choice
of a threshold value of 10−5 can be exactly motivated as done in Section 5: Such small value
will ensure to change the hypotheses of the test as little as possible compared to the test
for δmin, but still beneﬁt from the gain in power that a strictly positive δ brings (compare
in particular Footnote 6 and Remark 3). The results are as follows:

Dominance tests for δmin: Here we ﬁnd only one of the pairwise tests to be signiﬁcant on a
conﬁdence level of α = 0.05 (interpreted as a single test), namely the test of GBM over BDS
(even enlarging the number of resamples from N = 1000 to N = 10000 does not change the
situation). For all other pairwise comparison no signiﬁcant distributional diﬀerence can be
identiﬁed for any conﬁdence level smaller or equal than 0.05. Note that under any correction
procedure for multiple testing no signiﬁcant ordering structure among the classiﬁers can be
found using this test.

all-test: This test ﬁnds three pairwise comparison of classiﬁers to be signiﬁcant on a conﬁ-
dence level of α = 0.05: BDS over CART, GBM over CART and RF over CART. Again, for
all other pairwise comparison, no signiﬁcant distributional diﬀerence can be identiﬁed on
this level. Interestingly, the pairs of identiﬁed signiﬁcant pairwise comparisons of classiﬁers
are disjoint for this test and the resample test for δmin. Note that these three pairwise com-
parisons of classiﬁers still remain signiﬁcant at a global α-level of 0.05 under any correction
procedure for multiple testing (concretely, Bonferroni-correction was used here).
Dominance tests for δ = 10−5: The results for the resample tests for all pairwise compar-
isons of classiﬁers are given in Table 3. Concretely, for every pair (Ci, Cj) of classiﬁers, the
table gives the share of resamples with test statistic strictly smaller than the test statistic in
the original data, i.e., the value 1
ij <optij }, with IN the set of resampled index
N
sets. A line symbolizes that this share was strictly below 0.95.

I∈IN

{optI

(cid:80)

1

The table shows directly that – interpreted as one global test on the whole ordering struc-

23

Jansen, Nalenz, Schollmeyer and Augustin

Figure 5: Hasse diagrams of (cid:37)δ in the sample for the threshold values δmin = 0 (top),

δ = 0.004 (middle) and δmax = 0.0077 (bottom).

ture on the considered set C with global level α = 0.05 – a whole series of signiﬁcant pairwise
comparisons emerge: First, all classiﬁers dominate the CART method signiﬁcantly. Next,
the GBM method dominates signiﬁcantly all methods except GLM and RF. Furthermore,
it can be seen that – now interpreted as single tests at individual level α = 0.05 – BDS
dominates the methods LASSO and RIDGE, as well as RF dominates the method EN. Here
it is important to note that the latter three pairwise comparisons are no longer signiﬁcant
when corrected with any procedure for multiple testing.

Interpretation of the results: Table 3 shows that GBM, RF and GLM are not signiﬁcantly
diﬀerent from each other and are also not signiﬁcantly dominated by any other method. All
other methods are, depending on the choice of α-level and correction method, dominated
by some other model. CART is dominated by all other methods, including the linear model-
based methods, indicating that for most data sets linear models work quite well, without

24

delta = 0BDSCARTENGBMGLMLASSORFRIDGEdelta=0.004BDSCARTENGBMGLMLASSORFRIDGEdelta=0.0077BDSCARTENGBMGLMLASSORFRIDGERaising the threshold to 0.004makes RIDGE dominate bothEN and LASSO.Raising the threshold to 0.0077makes GLM dominate RIDGE,EN and LASSO.Comparing classifiers by stochastic dominance

strong non-linearities or interaction eﬀects. On the other hand, single CART models may
overﬁt which is a well-known issue of decision trees.

It is also interesting to note that the regularized regression models do not perform signiﬁ-
cantly better than GLM. The reason might be that the regularization parameter is chosen
via cross-validation, which might become unstable for smaller data sets. Also some data
sets might not include irrelevant predictors, making plain GLM a better choice. This could
also be the reason why GLM is not dominated by GBM, as the hyper-parameters in GBM
are not chosen via cross-validation and therefore might be suboptimal. GBM as the more
ﬂexible model outperforms BDS for all reasonable signiﬁcance levels, whereas GBM and RF
are considered incomparable, which is in line with our expectations.

Generally speaking, using the proposed resample test for δ-dominance may lead to con-
servative results, however, still ﬁnds more structure that the all-test heuristic, which is
the only competitor we could extract from the literature. Recall again that the one-test
heuristic is omitted here, as it does not hold the α level as discussed in Section 4.3 and,
therefore, cannot be meaningfully compared to statistical tests meeting this level. As the
results found by GSD are very trustworthy, as shown in the simulation study, we are able
to make more deﬁnite statements about the performance, such as that GBM outperforms
LASSO and most often EN and RIDGE regression.

Finally, it should be mentioned that the advantages of a small value of δ > 0 are also clearly
shown in the concrete application: The test with δ = 10−5 ﬁnds remarkably more structure
compared to the test with δmin, although due to the very small value of δ the hypotheses
of the underlying tests do hardly change.10

BDS
CART
EN

−
−
−

BDS CART
1.000
−
0.998
1.000
1.000
0.997
1.000
0.999

GBM 0.998
GLM
LASSO
RF
RIDGE

−
−
−
−

EN
0.976
−
−
0.998
−
−
0.953
−

GBM GLM LASSO RF RIDGE
0.951
0.967
−
−
−
−
0.997
0.999
−
−
−
−
−
−
−
−

−
−
−
−
−
−
−
−

−
−
−
−
−
−
−
−

−
−
−
−
−
−
−
−

Table 3: Results of the resample tests with δ = 10−5 and N = 1000 for all binary compar-
isons: For every pair (Ci, Cj) of classiﬁers, the table gives the share of resamples
with test statistic strictly smaller than the test statistic in the original data. A
line symbolizes that this share was strictly below 0.95.

10. As an indicator of how small this change of the underlying order actually is, one can name the fact that,

restricted the observed sample, the orders (cid:37)0 and (cid:37)0.00001 do actually coincide.

25

Jansen, Nalenz, Schollmeyer and Augustin

6.3 Comparison with Classical Stochastic Dominance

To complete our study, we brieﬂy compare the analysis results from Section 6.2 with the
results that would be obtained under an analysis under classical (ﬁrst-order) stochastic
dominance. As already discussed in Remark 1 in Section 3, classical stochastic dominance
arises as that special case of our dominance relation (cid:37)δ where the relation R2 from the
preference system C deﬁned in Equation (3) is empty, i.e., R2 = ∅, and the threshold
parameter δ is chosen to be 0. Importantly, note that the case R2 = ∅ corresponds to the
situation in which all the quality measures used are interpreted on a purely ordinal scale.

The detailed results of the analysis of the considered sample of data sets under classical
In summary, we observe that an
stochastic dominance can be found in Appendix A1.
analysis under our dominance relation (cid:37)δ (with non-empty R2 and maybe even δ > 0)
allows for a much more structured comparison of the competing classiﬁers than is the
case with an analysis under classical stochastic dominance. This is due to the fact that
our dominance relation also allows us to fully exploit the information of the metrically
interpretable quality criteria (here, all three), whereas stochastic dominance considers all
dimensions of the quality vectors as purely ordinal. Thus, the application example suggests
that ignoring available (partial) metric information indeed may lead to wasting relevant
information about the underlying ordering structure.

7. Summary and Concluding Remarks

In this paper, we have developed a general framework for comparing classiﬁers with re-
spect to diﬀerent quality criteria on diﬀerent data sets simultaneously. The basic idea of
this comparison is based on a generalized version of classical multidimensional stochastic
dominance, which also allows to adequately include the metric information of the quality
measures used and can be regularized while attenuating the inﬂuence of the quality mea-
sures to the same extent. We have demonstrated how this dominance relation between
classiﬁers can be detected by linear programming. Further, we showed how the optimal
value of the linear program applied to a sample of data sets can be used as a statistic for
statistically testing whether there is dominance between two competing classiﬁers. As the
distribution of our test statistic is diﬃcult to analyze, this test was performed by means
of a permutation-based adapted two-sample observation-randomization test. Finally, we
have illustrated the beneﬁts of the proposed dominance concept over existing methods in
a simulation study and applied it to real world data sets comparing eight classiﬁers with
respect to three quality criteria. There are several promising directions for future research:
Incorporating classiﬁcation diﬃculty: At this stage, the construction of our ordering (cid:37)δ
does not incorporate diﬀerences in the diﬃculty of classifying diﬀerent data sets. However,
as the heterogeneity of the considered space of data sets D increases, a co-consideration
of these diﬀerences becomes more and more relevant. In principle, we believe that there
would be two diﬀerent ways to account for this: First, the quality vectors of the classiﬁers
can be transformed before the dominance analysis with a loss function that depends on
the data set. In this way, the challenges in comparing the diﬃculties would be outsourced
to a pre-processing step. Of course, however, ﬁnding suitable loss functions is a research
topic of its own. A second possibility is to incorporate the classiﬁcation diﬃculty directly

26

Comparing classifiers by stochastic dominance

into the modeling of the underlying preference system. For this purpose, the framework
of state-dependent preference systems recently developed in Jansen and Augustin (2022)
would presumably be directly transferable.

Reducing computational complexity for special cases: In its current form, the number of con-
straints of the linear program for checking (cid:37)δ-dominance given in Proposition 3 increases
with complexity of at worst O(d4), where d denotes the number of possible quality vectors
(or the number of attained quality vectors in the observed sample, respectively). It cer-
tainly deserves further research how this worst-case complexity can be reduced if additional
constraints on the considered preference system’s metric relation R2 are imposed.
Extension to multi-criteria decision making: The concepts presented here need by no means
be limited to the comparison of classiﬁers. Thinking a bit more abstractly, any algorithms
could be statistically compared with respect to diﬀerent performance measures simultane-
ously in exactly the same way, including regression and even unsupervised learning settings,
as long as meaningful quality criteria can be formulated. In principle, our framework could
also be applied to general multi-criteria decision problems under uncertainty. An interesting
aspect is that also multi-criteria decision problems can be analyzed with respect to purely
ordinal as well as metrically scaled decision rules simultaneously.

Acknowledgments

Malte Nalenz and Georg Schollmeyer gratefully acknowledge the ﬁnancial and general sup-
port of the LMU Mentoring program.

A. Appendix

A.1 Comparison with Classical Stochastic Dominance

If we analyze the same situation as described in Section 6.1, however, this time by means
of classical stochastic dominance, we receive the results which are visualized in Figure 6.
The ﬁrst major diﬀerence from the analysis based on our dominance relation is the ordering
under a threshold of δ = 0: While a relatively structured picture emerged for the analysis
under our dominance relation (cid:37)0 already in this case (compare the top picture in Figure 5),
the analysis based on stochastic dominance yields only two pairs of comparable classiﬁers,
viz BDS over CART as well as GBM over CART. Based on stochastic dominance, no clear
best and worst classiﬁer can be identiﬁed within this concrete sample.

Considering the remaining ﬁve analyses under successively increasing threshold δ, two as-
pects in particular should be emphasized. First, as expected, the higher the threshold value,
the more comparable the classiﬁers. In comparison to the analysis based on our dominance
relation (cid:37)δ under increasing δ, however, it is noticeable that even with threshold values
that are higher by a factor of about ten, there still arise more weakly structured situations.
Second, it is striking that even when analyzed with a relatively high threshold of δ = 0.06,
no clear best classiﬁer can be identiﬁed: The methods RF and GBM remain incomparable
here. Note that the order under threshold δ = 0.06 is the best we can get: There exists no
δ for which (cid:37)δ possesses a superset of comparable pairs of classiﬁers.

27

Jansen, Nalenz, Schollmeyer and Augustin

Figure 6: Hasse diagrams of (cid:37)δ for six diﬀerent threshold values δ with all quality criteria

treated purely ordinal. The relation (cid:37)0 coincides with stochastic dominance.

A.2 Data Set Selection and Implementation in Section 6.1

Data Set Selection. The data sets used for the experiments in Section 6.1 are taken from
the UCI machine learning repository (Dua and Graﬀ, 2017). The selection criteria are:

• Here, we only consider binary classiﬁcation, but note that the method can be extended

to any learning task where performance metrics of at least ordinal scale exist.

• We chose data sets with mostly numerical features or features with low cardinality.

28

delta=0Boosted StumpsCARTElasticNetGBMglmLassorfRidgedelta=0.03Boosted StumpsCARTElasticNetGBMglmLassorfRidgedelta=0.04Boosted StumpsCARTElasticNetGBMglmLassorfRidgedelta=0.05Boosted StumpsCARTElasticNetGBMglmLassorfRidgedelta=0.055Boosted StumpsCARTElasticNetGBMglmLassorfRidgedelta=0.06Boosted StumpsCARTElasticNetGBMglmLassorfRidgeComparing classifiers by stochastic dominance

• Only data sets with low number of missing values are considered.

Generally, we selected data sets, such that the need for pre-processing is minimal.

Algorithm Settings. We brieﬂy describe the implementation of the compared methods:

• Ridge, Elastic Net and Lasso Regression are ﬁt using the R-package(R Core Team,
2021) glmnet (Friedman et al., 2010). The optimal λ is determined via cross-
validation. The mixing parameter in Elastic Net is set to 0.5.

• GBM and Gradient boosted decision stumps are ﬁt using the gbm R-package. Gra-
dient boosting uses 300 trees with a learning rate of 0.02 and a maximum depth of 3.
The stumps use 500 trees and a learning rate of 0.05.

• Random Forest is ﬁt using the randomForest R-package with default settings.

• For CART we use the rpart R-package with default settings.

Note that the results for all algorithms could likely be improved with parameter tuning,
however, we used reasonable default values for the comparison. Our aim is solely to showcase
our method, not to make any deﬁnite statements about the general performance of popular
machine learning methods.

A.3 More Detailed Results

Figure 7: The ﬁgure shows the empirical distribution of the number of false positives.

29

Jansen, Nalenz, Schollmeyer and Augustin

Figure 8: The ﬁgure shows the empirical distribution of the number of false negatives.

Figure 9: F-score for the diﬀerent tests, without Bonferroni correction. A much better

trade-oﬀ is achieved for GSD and GSD-δ.

Empirical Results. Tables 4, 5, and 6 show the raw performance values over the 16 analyzed
data sets that the classiﬁer comparison is based on.

30

Comparing classifiers by stochastic dominance

data set

Boosted Stumps

CART

ElasticNet GBM

glm

Lasso

rf

Ridge

australian
banknote
biodeg
blood transfusion
diabetes
haberman
heart
ILPD
Ionosphere
liver
parkinsons
pop failures
sonar
spambase
wbdc
wilt

0.937
1.000
0.926
0.738
0.830
0.657
0.906
0.728
0.972
0.649
0.942
0.910
0.904
0.981
0.993
0.990

0.901
0.976
0.847
0.722
0.763
0.556
0.823
0.674
0.915
0.587
0.830
0.817
0.784
0.892
0.960
0.956

0.930
1.000
0.917
0.751
0.833
0.717
0.904
0.714
0.910
0.671
0.873
0.943
0.831
0.952
0.994
0.970

0.943
1.000
0.926
0.736
0.838
0.697
0.902
0.732
0.973
0.650
0.957
0.937
0.927
0.981
0.992
0.989

0.929
1.000
0.922
0.752
0.831
0.713
0.912
0.738
0.866
0.668
0.866
0.952
0.755
0.971
0.962
0.977

0.929
1.000
0.917
0.752
0.833
0.728
0.906
0.717
0.904
0.662
0.867
0.941
0.838
0.952
0.994
0.970

0.937
1.000
0.937
0.670
0.826
0.673
0.904
0.754
0.980
0.606
0.951
0.923
0.946
0.986
0.991
0.989

0.931
1.000
0.916
0.751
0.832
0.720
0.910
0.719
0.913
0.672
0.853
0.942
0.855
0.952
0.993
0.963

Table 4: AUC for the diﬀerent methods on the 16 data sets.

data set

Boosted Stumps

CART

ElasticNet GBM

glm

Lasso

rf

Ridge

australian
banknote
biodeg
blood transfusion
diabetes
haberman
heart
ILPD
Ionosphere
liver
parkinsons
pop failures
sonar
spambase
wbdc
wilt

0.865
0.995
0.870
0.788
0.751
0.735
0.805
0.700
0.926
0.583
0.902
0.926
0.818
0.944
0.963
0.976

0.845
0.965
0.824
0.784
0.734
0.719
0.786
0.671
0.875
0.569
0.841
0.928
0.755
0.893
0.944
0.977

0.859
0.975
0.861
0.771
0.769
0.735
0.832
0.710
0.869
0.629
0.876
0.915
0.756
0.884
0.961
0.943

0.864
0.989
0.866
0.789
0.763
0.729
0.812
0.712
0.937
0.606
0.922
0.943
0.842
0.939
0.961
0.980

0.859
0.987
0.865
0.770
0.773
0.742
0.842
0.724
0.878
0.615
0.860
0.957
0.736
0.927
0.960
0.969

0.859
0.975
0.860
0.771
0.771
0.735
0.839
0.708
0.872
0.626
0.876
0.915
0.737
0.884
0.961
0.943

0.867
0.993
0.871
0.761
0.768
0.725
0.815
0.705
0.937
0.554
0.907
0.924
0.842
0.954
0.963
0.982

0.859
0.977
0.856
0.773
0.772
0.732
0.848
0.712
0.877
0.629
0.861
0.915
0.794
0.884
0.956
0.945

Table 5: Accuracy on the 16 data sets.

data set

Boosted Stumps

CART

ElasticNet GBM

glm

Lasso

rf

Ridge

australian
banknote
biodeg
blood transfusion
diabetes
haberman
heart
ILPD
Ionosphere
liver
parkinsons
pop failures
sonar
spambase
wbdc
wilt

0.095
0.011
0.099
0.156
0.163
0.191
0.134
0.182
0.058
0.248
0.070
0.052
0.120
0.046
0.025
0.017

0.119
0.032
0.141
0.157
0.195
0.203
0.168
0.224
0.099
0.278
0.126
0.058
0.204
0.094
0.050
0.019

0.106
0.034
0.123
0.159
0.163
0.184
0.132
0.187
0.114
0.234
0.112
0.061
0.181
0.112
0.059
0.044

0.091
0.011
0.100
0.156
0.158
0.187
0.133
0.179
0.051
0.242
0.061
0.043
0.110
0.048
0.028
0.015

0.101
0.009
0.101
0.155
0.158
0.183
0.128
0.176
0.105
0.228
0.111
0.033
0.264
0.059
0.040
0.024

0.106
0.034
0.123
0.159
0.163
0.183
0.132
0.187
0.117
0.235
0.114
0.061
0.176
0.112
0.059
0.044

0.096
0.006
0.093
0.183
0.161
0.194
0.131
0.172
0.050
0.253
0.072
0.055
0.124
0.039
0.030
0.013

0.106
0.041
0.126
0.159
0.163
0.184
0.130
0.187
0.111
0.233
0.117
0.061
0.169
0.111
0.062
0.044

Table 6: Brier Score on the 16 data sets.

31

Jansen, Nalenz, Schollmeyer and Augustin

References

K. Arrow. A diﬃculty in the concept of social welfare. Journal of Political Economy, 58:

328–346, 1950.

A. Benavoli, G. Corani, and F. Mangili. Should we really use post-hoc tests based on

mean-ranks? The Journal of Machine Learning Research, 17(1):152–161, 2016.

S. Brams and P. Fishburn. Voting procedures. In K. Arrow, A. Sen, and K. Suzumura,
editors, Handbook of Social Choice and Welfare, Vol. 1, pages 173–236. North-Holland,
2002.

P. Brazdil, C. Soares, and J. Da Costa. Ranking learning algorithms: Using IBL and
meta-learning on accuracy and time results. Machine Learning, 50(3):251–277, 2003.

L. Breiman. Random forests. Machine Learning, 45(1):5–32, 2001.

L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classiﬁcation and regression trees.

reprinted: 2017, Taylor & Francis, 1983.

G. Corani, A. Benavoli, J. Demˇsar, F. Mangili, and M. Zaﬀalon. Statistical comparison
of classiﬁers through Bayesian hierarchical modelling. Machine Learning, 106(11):1817–
1837, 2017.

O. Davidov and S. Peddada. Testing for the multivariate stochastic order among ordered
experimental groups with application to dose–response studies. Biometrics, 69(4):982–
990, 2013.

J. de Borda. Memoire sur les elections au scrutin. Historie de l’Academie Royale des

Sciences, 1781.

N. de Condorcet. Essai sur l’application de l’analyse a la probabilite des decisions rendues

a la pluralite des voix. Paris, 1785.

K. Deb. Multi-objective optimization. In E. Burke and G. Kendall, editors, Search Method-

ologies, pages 403–449. Springer, 2014.

J. Demˇsar. Statistical comparisons of classiﬁers over multiple data sets. The Journal of

Machine Learning Research, 7:1–30, 2006.

D. Dua and C. Graﬀ. UCI machine learning repository, 2017. URL http://archive.ics.

uci.edu/ml.

M. Eugster, T. Hothorn, and F. Leisch. Domain-based benchmark experiments: Exploratory

and inferential analysis. Austrian Journal of Statistics, 41(1):5–26, 2012.

M. Fern´andez-Delgado, E. Cernadas, S. Barro, and D. Amorim. Do we need hundreds of
classiﬁers to solve real world classiﬁcation problems? The Journal of Machine Learning
Research, 15(1):3133–3181, 2014.

32

Comparing classifiers by stochastic dominance

J. Friedman. Stochastic gradient boosting. Computational Statistics & Data Analysis, 38:

367–378, 2002.

J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear

models via coordinate descent. Journal of Statistical Software, 33:1–22, 2010.

C. Garcia-Gomez, A. Perez, and M. Prieto-Alaiz. A review of stochastic dominance methods

for poverty analysis. Journal of Economic Surveys, 33:178–191, 2019.

M. Graczyk, T. Lasota, Z. Telec, and B. Trawi´nski. Nonparametric statistical analysis of ma-
chine learning algorithms for regression problems. In R. Setchi, I. Jordanov, R. Howlett,
and L. Jain, editors, International Conference on Knowledge-Based and Intelligent Infor-
mation and Engineering Systems, pages 111–120. Springer, 2010.

H. Ismail Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller. Deep learning
for time series classiﬁcation: a review. Data Mining and Knowledge Discovery, 33(4):
917–963, 2019.

P. Jaccard. The distribution of the ﬂora in the alpine zone. New Phytologist, 11(2):37–50,

1912.

C. Jansen and T. Augustin. Decision making with state-dependent preference systems.
In D. Ciucci, I. Couso, D. Medina, J.and Slezak, D. Petturiti, B. Bouchon-Meunier, and
R. Yager, editors, Information Processing and Management of Uncertainty in Knowledge-
Based Systems, pages 729–742. Springer, 2022.

C. Jansen, G. Schollmeyer, and T. Augustin. Concepts for decision making under severe
uncertainty with partial ordinal and partial cardinal preferences. International Journal
of Approximate Reasoning, 98:112–131, 2018.

C. Jansen, H. Blocher, T. Augustin, and G. Schollmeyer. Information eﬃcient learning of
complexly structured preferences: Elicitation procedures and their application to decision
making under uncertainty. International Journal of Approximate Reasoning, 144:69–91,
2022.

D. Krantz, R. Luce, P. Suppes, and A. Tversky. Foundations of Measurement. Volume I:
Additive and Polynomial Representations. Academic Press, San Diego and London, 1971.

N. Lavesson and P. Davidsson. Evaluating learning algorithms and classiﬁers. International

Journal of Intelligent Information and Database Systems, 1:37–52, 2007.

R. Luce. Semiorders and a theory of utility discrimination. Econometrica, 24:178–191, 1956.

R. Marler and J. Arora. The weighted sum method for multi-objective optimization: new

insights. Structural and Multidisciplinary Optimization, 41(6):853–862, 2010.

O. Mersmann, M. Preuss, H. Trautmann, B. Bischl, and C. Weihs. Analyzing the BBOB
results by means of benchmarking concepts. Evolutionary Computation, 23:161–185, 2015.

33

Jansen, Nalenz, Schollmeyer and Augustin

K. Mosler and M. Scarsini. Some theory of stochastic dominance.

In K. Mosler and
M. Scarsini, editors, Stochastic Orders and Decision under Risk, pages 203–212. Insti-
tute of Mathematical Statistics, Hayward, CA, 1991.

C. M¨ussel, L. Lausser, M. Maucher, and H. Kestler. Multi-objective parameter selection

for classiﬁers. Journal of Statistical Software, 46:1–27, 2012.

J. Pratt and J. Gibbons. Concepts of Nonparametric Theory. Springer, 2012.

R Core Team. R: A Language and Environment for Statistical Computing. R Foundation
for Statistical Computing, Vienna, Austria, 2021. URL https://www.R-project.org/.

G. Schollmeyer, C. Jansen, and T. Augustin. Detecting stochastic dominance for poset-
valued random variables as an example of linear programming on closure systems, 2017.
URL https://epub.ub.uni-muenchen.de/40416/13/TR_209.pdf. Technical Report
209, Department of Statistics, LMU Munich.

R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society: Series B (Methodological), 58(1):267–288, 1996.

G. Webb. Multiboosting: A technique for combining boosting and wagging. Machine

Learning, 40:159–196, 2000.

Y-J. Whang. Econometric analysis of stochastic dominance: Concepts, methods, tools, and

applications. Cambridge University Press, 2019.

B. Yu and K. Kumbier. Veridical data science. Proceedings of the National Academy of

Science, 117(8):3920–3929, 2020.

H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of

the Royal Statistical Society: Series B (Methodological), 67(2):301–320, 2005.

34

