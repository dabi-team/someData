0
2
0
2

l
u
J

1

]

G
L
.
s
c
[

2
v
8
9
0
3
0
.
2
0
0
2
:
v
i
X
r
a

Inferential Induction: A Novel Framework for
Bayesian Reinforcement Learning

Emilio Jorge∗†

Hannes Eriksson∗†

Christos Dimitrakakis∗†‡

Debabrota Basu†

Divya Grover†

July 3, 2020

Abstract

Bayesian reinforcement learning (BRL) oﬀers a decision-theoretic
solution for reinforcement learning. While “model-based” BRL al-
gorithms have focused either on maintaining a posterior distribution
on models or value functions and combining this with approximate
dynamic programming or tree search, previous Bayesian “model-free”
value function distribution approaches implicitly make strong assump-
tions or approximations. We describe a novel Bayesian framework,
Inferential Induction, for correctly inferring value function distributions
from data, which leads to the development of a new class of BRL
algorithms. We design an algorithm, Bayesian Backwards Induction,
with this framework. We experimentally demonstrate that the proposed
algorithm is competitive with respect to the state of the art.

1

Introduction

Many Reinforcement Learning (RL) algorithms are grounded on the applica-
tion of dynamic programming to a Markov Decision Process (MDP) (Sutton
and Barto, 2018). When the underlying MDP µ is known, eﬃcient algorithms
for ﬁnding an optimal policy exist that exploit the Markov property by cal-
culating value functions. Such algorithms can be applied to RL, where the
learning agent simultaneously acts in and learns about the MDP, through e.g.
stochastic approximations, without explicitly reasoning about the underlying
MDP. Hence, these algorithms are called model-free.

In Bayesian Reinforcement Learning (BRL) (Ghavamzadeh et al., 2015),
we explicitly represent our knowledge about the underlying MDP µ through
some prior distribution β over a set M of possible MDPs. While model-based

∗Equal contribution
†Chalmers University of Technology
‡University of Oslo

1

 
 
 
 
 
 
BRL is well-understood, many works on BRL aim to become model-free by
directly calculating distributions on value functions. Unfortunately, these
methods typically make strong implicit assumptions or approximations about
the underlying MDP.

This is the ﬁrst paper to directly perform Bayesian inference over value
functions without any implicit assumption or approximation. We achieve
this through a novel BRL framework, called Inferential Induction, extending
backwards induction. This allows us to perform joint Bayesian inference
on MDPs and value function as well as to optimise the agent’s policy. We
instantiate and experimentally analyse only one of the many possible algo-
rithms, Bayesian Backwards Induction (BBI), in this family and show it is
competitive with the current state of the art.

In the rest of this section, we provide background in terms of setting and
related work. In Section 2, we explain our Inferential Induction framework
and three diﬀerent inference methods that emerge from it, before instantiating
one of them into a concrete procedure. Based on this, Section 3 describes
the BBI algorithm.
In Section 4, we experimentally compare BBI with
state-of-the art BRL algorithms.

1.1 Setting and Notation

In this paper, we generally use P and E to refer to probability (measures)
and expectations while allowing some abuse of notation for compactness.

Reinforcement Learning (RL) is a sequential learning problem faced
by agents acting in an unknown environment µ, typically modelled as a
Markov decision process (c.f. Puterman, 2005).

Deﬁnition 1.1 (Markov Decision Process (MDP)). An MDP µ with state
space S and action space A is equipped with a reward distribution Pµ(r | s)
with corresponding expectation ρµ(s) and a transition kernel Pµ(s(cid:48)|s, a) for
states s, s(cid:48) ∈ S and actions a ∈ A.

At time t, the agent observes1 the environment state st, and then selects
an action at. Then, it receives and observes a reward rt and a next state
st+1. The agent is interested in the utility Ut (cid:44) (cid:80)T
k=t γk−trk, i.e. the sum
of future rewards rt. Here, γ ∈ (0, 1] is the discount factor and T ∈ [1, ∞] is
the problem horizon. Typically, the agent wishes to maximise the expected
utility, but other objectives are possible.

The agent acts in the environment using a policy π = (π1, . . . , πt, . . .) that
takes an action at at time t with probability πt(at | st, rt−1, at−1, st−1, . . . , r1, a1, s1).
Dependence on the complete observation history is necessary, if the agent is

1In the partially-observable setting, the agent instead observes another variable depen-

dent on the state.

2

learning from experience. However, when µ is known, the policy π∗
µ maximis-
ing expected utility over ﬁnite horizon is Markovian2 of the form πt(at | st)
and is computable using dynamic programming. A useful algorithmic tool
for achieving this is the value function, i.e. the expected utility of a policy π
from diﬀerent starting states and action:

Deﬁnition 1.2 (Value Function). The state value function of policy π in
µ,t(s) (cid:44) Eπ
MDP µ is V π
µ(Ut | st = s) and the corresponding state-action (or
µ(Ut | st = s, at = a). Pπ
µ,t(s, a) (cid:44) Eπ
Q-)value function is Qπ
µ denote
probabilities and expectations under the process induced by π and µ.

µ and Eπ

Finally, the Bellman operator Bπ

s(cid:48)∈S
allows us to compute the value function recursively through V π

µV (s) (cid:44) ρµ(s) + γ (cid:80)

Pπ
µ(s(cid:48) | s)V (s(cid:48))
µ,t = Bπ
µ,t+1.3
µV π

Bayesian RL (BRL).
In BRL, our subjective belief is represented as a
probability measure β over possible MDPs. We refer to the initial belief β
as the prior distribution. By interacting with the environment until time t,
the agent obtains data D = (s1, a1, r1, . . . , st). This data is used to calculate
a posterior distribution β(µ | D) that represents agent’s current knowledge
about the MDP.4 For a given belief and an adaptive policy5 πβ(s), we deﬁne
the Bayesian value function to be:

V πβ
β,t (s) (cid:44)

(cid:90)

M

V πβ
µ,t (s) dβ(µ).

(1)

The Bayesian value function is the expected value function under the dis-
tribution β. The Bayes-optimal policy achieves the Bayes-optimal value
function V ∗
µ,t for
all µ, while V ∗
β,t typically requires exponential time. Information about the
value function distribution can be a useful tool for constructing near-optimal
policies, as well a way to compute risk-sensitive policies.

β,t involves integrating V π

β,t. Calculating V π
V π

β,t(s) = supπ

Distributions over Value Functions. Let us consider the value func-
tion V , with V = (V1, . . . , VT ) for ﬁnite-horizon problems, a prior belief β over
MDPs, and a previously collected data D = (s1, a1, r1, . . . , st−1, at−1, rt−1, st, rt)

2For inﬁnite horizon problems this policy is still Markovian, but can be non-stationary.
3In the discounted setting, the value function converges to V π
4This is expressible in closed form. When the MDP is discrete, a Dirichlet-product
prior can be used, or when the MDP is continuous and the dynamics are assumed to be
linear, a Gaussian-Wishart prior can be used (DeGroot, 1970). Gaussian process inference
can also be expressed in a closed-form but inference becomes approximate because the
computational complexity scales quadratically with time.

µ,1 as T → ∞.

µ ≡ V π

5Typically the adaptive policy’s actions depends on the complete history, but we can
equivalently write it as depending on the current belief and state instead. It is also possible
to consider the Bayesian value function of policies whose beliefs disagrees with the actual
MDP distribution, but this is beyond the scope of this paper.

3

using some policy π. Now, the posterior value function distribution is ex-
pressed in terms of the MDP posterior:

Pβ(V | D) =

(cid:90)

M

Pµ(V ) dβ(µ | D).

(2)

(2) induces an empirical measure ˆP E
Monte-Carlo estimate:

M C that corresponds to the standard

ˆP E
M C(B) (cid:44) N −1
µ

(cid:110)

v(k) ∈ B

(cid:111)

,

K
(cid:88)

1

k=1

(3)

where 1 {} is the indicator function. The practical implementation is in
Algorithm 1.6

Algorithm 1 A Monte-Carlo Estimation of Value Function Distributions

1: Select a policy π.
2: for k = 1, . . . , Nµ do
3:

Sample an MDP µ(k) ∼ β.
Calculate v(k) = V π
∼ β.

µ(k),

4:
5: end for
6: return ˆP E

M C({v(k)})

1.2 Related Work and Our Contribution

Model-free Bayesian Value Functions. Bayesian value function distribu-
tions have been considered extensively in model-free Bayesian Reinforcement
Learning (BRL). One of the ﬁrst methods was Bayesian Q-learning (Dearden
et al., 1998), which used a normal-gamma prior on the utility distribution.
However, as i.i.d. utility samples cannot be obtained by bootstrapping from
value function estimates, this idea had inherent ﬂaws. Engel et al. (2003)
developed a more sophisticated approach, the Gaussian Process Temporal
Diﬀerence (GPTD) algorithm, which has a Gaussian process (GP) prior
β(V ) on value functions. It then combines this with the likelihood function
P(D | V ) ∝ (cid:81)t
i=1 exp{−|V (si) − ri − γV (si+1)|2}. However, this makes the
implicit assumption that the deterministic empirical MDP model is true.
Engel et al. (2005) tried to relax this assumption by allowing for correlation
between sequentially visited states. Deisenroth et al. (2009) developed a
dynamic programming algorithm with a GP prior on value functions and an
explicit GP model of the MDP. Finally, Tang and Agrawal (2018) introduced

6Algorithm 1 has O(NµN 2

S NAT ) complexity for policy evaluation, while policy optimi-
sation can be performed through approximate dynamic programming (Dimitrakakis, 2011)
or Bayesian gradient ascent (Ghavamzadeh and Engel, 2006).

4

VDQN, generalising such methods to Bayesian neural networks. The assump-
tions that these model-free Bayesian methods implicitly make about the MDP
are hard to interpret, and we ﬁnd the use of an MDP model independently
of the value function distribution unsatisfactory. We argue that explicitly
reasoning about the joint value function and MDP distribution is necessary
to obtain a coherent Bayesian procedure. Unlike the above methods, we
calculate a value function posterior P(V |D) while simultaneously taking into
account uncertainty about the MDP.

Model-based Bayesian Value Functions. If a posterior over MDPs
is available, we can calculate a distribution over value functions in two steps:
a) sample from the MDP posterior and b) calculate the value function of
each MDP. Dearden et al. (1999) suggested an early version of this approach
that obtained approximate upper bounds on the Bayesian value function
and sketched a Bellman-style update for performing it online. Posterior
sampling approach was later used to obtain value function distributions
in the discrete case by Dimitrakakis (2011) and in the continuous case
by Osband et al. (2016). We instead focus on whether it is possible to
compute value function distributions exactly or approximately through a
backwards induction procedure. In particular, how can we obtain P(Vi|D)
from P(Vi+1|D)?

Utility Distributions. A similar problem is calculating utility (rather
than value) distributions through Bellman updates. Essentially, this is the
problem of estimating Pµ(Ui | Ui+1) for a given MDP µ. In this context,
Morimura et al. (2010) constructed risk-sensitive policies. More recently Belle-
mare et al. (2017) showed that modelling the full utility distribution may
also be useful for exploration. However, the utility distribution is due to the
stochasticity of the transition kernel Pµ(st+1 | st, at) rather than uncertainty
about the MDP, and hence a diﬀerent quantity from the value function
distribution, which this paper tries to estimate.

Bayes-optimal approximations. It is also possible to deﬁne the value
function with respect to the information state (st, βt). This generates a
Bayes-adaptive Markov decision process (BAMDP Duﬀ (2002)). However,
BAMDPs are exponentially-sized in the horizon due to the increasing number
of possible information states as we look further into the future. A classic
approximate algorithm in this setting is Bayesian sparse sampling (Wang
et al., 2005, BSS). BSS in particular generates a sparse BAMDP by sampling
a ﬁnite number of belief states at each step i in the tree, up to some ﬁxed
horizon T . In addition, it can also sparsely sample actions by selecting a
random action a through posterior sampling at each step i. In comparison, our
value function distributions at future steps can be thought of as marginalising
over possible future information states. This makes our space complexity
much smaller.

5

Our Contribution. We introduce Inferential Induction, a new Bayesian
Reinforcement Learning (BRL) framework, which leads to a Bayesian form
of backwards induction. Our framework allows Bayesian inference over
value functions without any implicit assumption or approximation unlike its
predecessors. The main idea is to calculate the conditional value function
distribution at step i from the value function distribution at step i + 1
analogous to backwards induction for the expectation (Eq. (4)). Following
this, we propose three possible marginalisation techniques (Methods 1, 2
and 3) and design a Monte-Carlo approximation with Method 1. We can
combine this procedure with a policy optimisation mechanism. We use
a Bayesian adaptation of dynamic programming for this and propose the
Bayesian backwards induction (BBI) algorithm. Our experimental evaluation
shows that BBI is competitive to the current state of the art. Inferential
Induction framework provides the opportunity to further design more eﬃcient
algorithms of this family.

2

Inferential Induction

The fundamental problem is calculating the value function distribution
Pπ
β(Vi | D) for a policy7 π under the belief β. The main idea is to inductively
calculate Pπ

β(Vi | D) for i ≥ t as follows:

β(Vi+1 | D) from Pπ

(cid:90)

Pπ

β(Vi | D) =

β(Vi | Vi+1, D) d Pπ
Pπ

β(Vi+1 | D).

(4)

V
Let ψi+1 be a (possibly approximate) representation of Pπ
β(Vi+1 | D). If we
can calculate the above integral, then we can also obtain ψi ≈ Pπ
β(Vi | D)
recursively, from time T up to the current time step t. Then the problem
reduces to deﬁning the term Pπ
β(Vi | Vi+1, D) appropriately. We describe
three methods for doing so, and derive and experiment on an algorithm
for one speciﬁc case, in which Bayesian inference can also be performed
through conventional priors. As all the methods that we describe involve
some marginalisation over MDPs as an intermediate step, the main practical
question is what form of sampling or other approximations suit each of the
methods.

Method 1: Integrating over Pπ

β(µ | Vi+1, D). A simple idea for
dealing with the term linking the two value functions is to directly marginalise
over the MDP as follows:

Pπ

β(Vi | Vi+1, D) =

(cid:90)

M

µ(Vi | Vi+1) d Pπ
Pπ

β(µ | Vi+1, D).

(5)

This equality holds because given µ, Vi is uniquely determined by the policy
π and Vi+1 through the Bellman operator. However, it is crucial to note that

7Here we drop the subscript β from the policy for simplicity.

6

β(µ | Vi+1, D) (cid:54)= Pβ(µ | D), as knowing the value function gives information
Pπ
about the MDP.8

Method 2: Integrating over Pπ

β(µ | Vi, Vi+1). From Bayes’ theorem,
we can write the conditional probability of Vi given Vi+1, D in terms of
the data likelihood of Vi, Vi+1 and the conditional distribution Vi|Vi+1, as
follows9:

Pπ

β(Vi ∈ B | Vi+1, D) =

(cid:82)
B
(cid:82)

V

β(D | Vi, Vi+1) d Pπ
Pπ
β(D | Vi, Vi+1) d Pπ
Pπ

β(Vi | Vi+1)
β(Vi | Vi+1)

.

The likelihood term is crucial in this formulation. One way to write it is as
follows:

Pπ

β(D | Vi, Vi+1) =

(cid:90)

M

β(D | µ) d Pπ
Pπ

β(µ | Vi, Vi+1).

This requires us to specify some appropriate distribution Pπ
β(µ | Vi, Vi+1) that
we can sample from, meaning that standard priors over MDPs cannot be
used. On the other hand, it allows us to implicitly specify MDP distributions
given a value function, which may be an advantage in some settings.

Method 3: Integrating over β(µ | D). Using the same idea as Method

2, but using Bayes’s theorem once more, we obtain:

Pπ

β(D | Vi, Vi+1) =

(cid:90)

P(µ | Vi, Vi+1, π) Pπ

β(D)

M

β(µ)

dβ(µ | D).

While there are many natural priors from which sampling from β(µ | D)
is feasible, we still need to specify Pβ(µ | Vi, Vi+1, π). This method might
be useful when the distribution that we specify is easier to evaluate than
to sample from. It is interesting to note that if we replace β(µ | D) with a
point distribution (e.g. the empirical MDP), the inference becomes similar in
form to GPTD (Engel et al., 2003) and GPDP (Deisenroth et al., 2009). In
particular, this occurs when we set P(µ | Vi, Vi+1, π) ∝ exp{−(cid:107)Vi+1 − ρµ −
µ Vi(cid:107)2}. However, this has the disadvantage of essentially ignoring our
γP π
uncertainty about the MDP.

2.1 A Monte-Carlo Approach to Method 1

We will now detail such a Monte-Carlo approach for Method 1. We ﬁrst
combine the induction step in (4) and marginalisation of Method 1 in (5).
We also substitute an approximate representation ψi+1 for the next-step
belief P(Vi | D), to obtain the following conditional probability measure on

8Assuming otherwise results in a mean-ﬁeld approximation. See Sec. 2.2.
9Here, B ⊂ V are sets of value functions in an appropriate σ-algebra.

7

value functions:

ψi(B) (cid:44) Pπ
(cid:90)

(cid:90)

β(Vi ∈ B|D)
1 (cid:8)Bπ

=

V

M

µVi+1 ∈ B(cid:9) d Pπ

β(µ|Vi+1, D) dψi+1(Vi+1)

Following Monte Carlo approach, we can estimate the outer integral as the
sample mean over the samples value functions Vi+1.

ψi(B) ≈

1
NV

NV(cid:88)

(cid:90)

1

k=1

M

(cid:110)

Bπ

µV (k)

i+1 ∈ B

(cid:111)

d Pπ

β(µ|V (k)

i+1, D).

(6)

Here, NV is the number Vi+1 samples.
Let us focus on calculating Pπ

β(µ | Vi+1, D). Expanding it, we obtain, for

any subset of MDPs A ⊆ M, the following measure:

Pπ

β(µ ∈ A | Vi+1, D) =

(cid:82)
A
(cid:82)

M

Pπ
µ(Vi+1) dβ(µ | D)
Pπ
µ(Vi+1) dβ(µ | D)

,

(7)

since Pπ

µ(Vi+1 | D) = Pπ

To compute Pπ

µ(Vi+1), as µ, π are suﬃcient for calculating Vi+1.
µ(Vi+1), we can marginalise over utility rollouts U and

states:

Pπ

µ(Vi+1) =

(cid:90)

S

dq(s)

(cid:90) ∞

−∞

µ(Vi+1 | U, s) Pπ
Pπ

µ(U |s) dU.

The details of computing rollouts are in Appendix A.1. In order to understand
the meaning of the term Pπ
µ(Vi+1 | U, s), note that Vi+1(s) = E[U | si+1 = s].
Thus, a rollout from state s gives us partial information about the value
function. Finally, the starting state distribution q is used to measure the
goodness-of-ﬁt, similarly to e.g. ﬁtted-Q iteration10.

As a design choice, we deﬁne the density of Vi+1 given a sample um from

state sm ∼ q to be a Gaussian with variance σ2 :

d
dλ

P(Vi+1 | um, sm) (cid:44) 1
√
2π

e−

|Vi+1(sm)−um|2
2σ2

.

In practice, we can generate utility samples from the sampled MDP µ and
the policy π from step t + 1 onwards and re-use those samples for all starting
times i > t.

Finally, we can write:

Pπ

µ(Vi+1) ≈

1
n

n
(cid:88)

m=1

Pπ

µ(Vi+1 | um, sm),

um ∼ Pπ

µ(U )

10As long as q has full support over the state space, any choice should be ﬁne. For
discrete MDPs, we use a uniform distribution q over states and sum over all of them, while
we sample from q in the continuous case.

8

This leads to the following approximation for (7):

Pπ

β(µ ∈ A|Vi+1, D) ≈

(cid:82)
A

(cid:82)

M

(cid:80)

m e−
m e−

(cid:80)n

|Vi+1(sm)−um|2
2σ2

|Vi+1(sm)−um|2
2σ2

dβ(µ|D)

.

dβ(µ|D)

If we generate Nµ number of MDPs µ(j) ∼ β(µ | D) and set:

wjk (cid:44)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

V

(k)
i+1

j
(sm)−u
m
2σ2

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:80)n

m=1 e−

(cid:80)Nµ

j(cid:48)=1

(cid:80)n

m=1 e−

(cid:12)
(cid:12)
V
(cid:12)
(cid:12)

(k)
i+1

j(cid:48)
m

(sm)−u
2σ2

,

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(8)

we get E wjk = Pπ
samples for step i,

β(µ ∈ M | Vi+1, D). This allows us to obtain value function

V (j,k)
i

(cid:44) Bπ

µ(j)V (k)
i+1,

(9)

each weighted by wjk, leading to the following Monte Carlo estimate of the
value function distribution at step i

ψi(B) =

1
NV Nµ

NV(cid:88)

Nµ
(cid:88)

k=1

j=1

1

(cid:110)

V (j,k)
i

(cid:111)

∈ B

wjk.

(10)

This ends the general description of the Monte-Carlo method. Detailed
design of an algorithm depends on the representation that we use for ψi and
whether the MDP is discrete or continuous.

Bayesian Backwards Induction (BBI). For instantiation, we con-
struct a policy optimisation component and two approximate representations
to use with the inferential induction based policy evaluation. For policy
optimisation, we use a dynamic programming algorithm that looks ahead
H steps, and at each step i calculates a policy maximising the Bayesian
expected utility in the next i + 1 steps. For approximate representation of
the distribution of Vi+1, we use a multivariate Gaussian and A multivariate
Gaussians for discrete and continuous state A-action MDPs respectively. We
refer to this algorithm as Bayesian Backwards Induction (BBI) (Section 3).
Speciﬁcations of results, hyperparameters and distributions are in Section 4
and Appendix A.

2.2 A Parenthesis on Mean-ﬁeld Approximation

If we ignore the value function information by assuming that Pπ
Pβ(µ | D), we obtain

β(µ | Vi+1, D) =

Pπ

β(Vi|D) =

(cid:90)

(cid:90)

V

M

µ(Vi | Vi+1) dβ(µ | D) d Pπ
Pπ

β(Vi+1 | D).

9

Unfortunately, this corresponds to a mean-ﬁeld approximation. For example,
deploying similar methodology as Section 2.1 would lead us to

ψi(B) =

1
NV Nµ

NV(cid:88)

Nµ
(cid:88)

k=1

j=1

1

(cid:110)

V (j,k)
i

(cid:111)

.

∈ B

This will eventually eliminate all the uncertainty about the correspondence
between value function and underlying MDPs because it is equivalent to
assuming the mean MDP obtained from the data D is true. For that reason,
we do not consider this approximation any further.

3 Algorithms

Algorithm 2 is a concise description of the Monte Carlo procedure that we
develop. At each time step t, the algorithm is called with the prior and data
D collected so far, and it looks ahead up to some lookahead factor H 11. We
instantiate it below for discrete and continuous state spaces.

Algorithm 2 Policy Evaluation with Method 1

1: Input: Prior β, data D, lookahead H, discount γ, policy π, Nµ, NV .
2: Initialise ψH .
3: Sample ˆM (cid:44) (cid:8)µ(j) (cid:12)
4: for i = H − 1, . . . , 1 do
5:

(cid:12) j ∈ [Nµ](cid:9) from β(µ | D).

6:

7:

Sample V (k) ∼ ψi+1(v) for k ∈ [NV ].
Generate n utility samples um
Calculate wjk from (8) and V (j,k)
Calculate ψi from (10).

i

from (9).

8:
9: end for
10: return {ψi | i = 1, . . . , H}

Discrete MDPs. When the MDPs are discrete, the algorithm is straight-
forward. Then the belief β(µ | D) admits a conjugate prior in the form of
a Dirichlet-product for the transitions. In that case, it is also possible to
use a histogram representation for ψi, so that it can be calculated by simply
adding weights to bins according to (10).

However, as a histogram representation is not convenient for a large
number of states, we model using a Gaussian ψt. In order to do this, we
use the sample mean and covariance of the weighted value function samples

11When the horizon T is small, we can set H = T − t.

10

V (j,k)
i

:

mi =

1
NV Nµ

NV(cid:88)

Nµ
(cid:88)

k=1

j=1

V (j,k)
i

wjk

Σi =

1
NV Nµ

NV(cid:88)

Nµ
(cid:88)

(V (j,k)
i

k=1

j=1

− mi)(V (j,k)

i

− mi)(cid:62)wjk.

such that ψi = N (mi, Σi) is a multivariate normal distribution.

(11)

In the continuous state case, we obtain ψ through
Continuous MDPs.
ﬁtted Q-iteration (c.f. Ernst et al., 2005). For each action a in a ﬁnite set,
i ωa + (cid:15)a, where si, ωa ∈ Rd and
we ﬁt a weighted linear model Qi(si, a) = sT
(cid:15)a ∼ N (0, σ2
a). Finding the representation ωa is equivalent to solving A
weighted linear regression problems over NV state and Q-value samples for
each action a:

ωa (cid:44) arg minω

NV(cid:88)

Nµ
(cid:88)

k=1

j=1

(cid:16)

wjk

Q(j,k)
i

(a) − (s(j,k)

i

)T ω

(cid:17)2

+ λ(cid:107)ω(cid:107)2

= arg minω (cid:107)W 1/2(Qi(a) − ST

i (ω))(cid:107)2 + λ(cid:107)ω(cid:107)2.

Here, W is the diagonal weight matrix. Si and Qi(a) are the NV Nµ × d
matrices for the states and Q-values corresponding to sampled states and
Q-values. We add an l2 regulariser λ(cid:107)ω(cid:107)2 for eﬃcient regression. We
obtain ωa = (ST
i W Qi(a). This is equivalent to estimating a
multivariate normal distribution of Q-values ψi(a) = N (ma

i W Si + λI)−1ST

i ), where

i , Σa

ma

i =

Σa

i =

1
NV Nµ

σ2
a
NV Nµ

NV(cid:88)

Nµ
(cid:88)

k=1

j=1

NV(cid:88)

Nµ
(cid:88)

k=1

j=1

(s(j,k)
i

)T ωa

(Q(j,k)
i

(a) − mi)(Q(j,k)

i

(a) − mi)(cid:62)wjk.

(12)

In practice, we often use a feature map φ : Rd → Rf for states.

3.1 Bayesian Backwards Induction

We now construct a policy optimisation component to use with the inferential
induction based policy evaluation and the aforementioned two approximation
techniques. We use a dynamic programming algorithm that looks ahead
H steps, and at each step i calculates a policy maximising the Bayesian
expected utility in the next i + 1 steps. We describe the corresponding
pseudocode in Algorithm 3.

11

Algorithm 3 Line 8: Discrete MDPs. Just as in standard backwards
induction, at each step, we can calculate πi by keeping πi+1, . . . , πH ﬁxed:

Qi(s, a) (cid:44) Eβ(U | si = s, ai = a, D)
(cid:90)

=

ρµ(s, a) +

(cid:88)

µ (s(cid:48)|s, a)V πi+1,...,πH
P(j)

µ,i

(s(cid:48))

M

≈

s(cid:48)

(cid:88)

[ρµ(j)(s, a) +

j,k

(cid:88)

s(cid:48)

µ (s(cid:48)|s, a)V (k)
P(j)

i+1(s(cid:48))]

wjk
NµNV

.

(13)

Algorithm 3 Line 8: Continuous MDPs. As we are using ﬁtted
Q-iteration, we can directly use the state-action value estimates. So we
simply set Qi(s, a) = ˆQi(s, a).

The Qi estimate is then used to select actions for every state. We set
πi(a|s) = 1 for a = arg max Qi(s, a) (Line 3.9) and calculate the value func-
tion distribution (Lines 3.10 and 3.11) for the partial policy (πi, πi+1, . . . , πH ).

Algorithm 3 Bayesian Backwards Induction (BBI) with Method 1

1: Input: Prior β, data D, lookahead H, discount γ, Nµ, NV .
2: Initialise ψi.
3: Sample ˆM (cid:44) (cid:8)µ(j) (cid:12)
4: for i = H − 1, . . . , 1 do
5:

(cid:12) j ∈ [Nµ](cid:9) from β(µ | D).

6:

7:

8:

9:

10:

Sample V (k) ∼ ψi+1(v) for k ∈ [NV ].
Generate n utility samples ui
Calculate wjk from (8).
Calculate Qi from (13) or ﬁtted Q-iteration.
Set πi(a|s) = 1 for a ∈ arg max Qi(s, a).
Calculate wjk from (8) and V (j,k)
µ(j)V (k)
i+1.
Calculate ψi from (11) or (12).

i

Bπi

11:
12: end for
13: return π = (π1, . . . , πH ).

from (9) with policy πi: V (j,k)

(cid:44)

i

4 Experimental Analysis

For performance evaluation, we compare Bayesian Backwards Induction
(BBI, Algorithm 3) with exploration by distributional reinforcement learning
(VDQN, Tang and Agrawal, 2018). We also compare BBI with posterior
sampling (PSRL, Strens, 2000; Thompson, 1933), MMBI (Dimitrakakis,
2011), BSS (Wang et al., 2005) and BQL Dearden et al. (1998) for the discrete
MDPs and with Gaussian process temporal diﬀerence (GPTD, Engel et al.,
2003) for the continuous MDPs. In Section 4.1, we describe the experimental

12

setup and the priors used for implementation. In Section 4.2, we illustrate
diﬀerent environments used for empirical evaluation. In Section 4.3, we
analyse the results obtained for diﬀerent environments in terms of average
reward obtained over time.

4.1 Experimental Setup

Parameters. We run the algorithms for the inﬁnite-horizon formulation of
value function with discount factor γ = 0.99. We evaluate their performance
in terms of the evolution of average reward to T = 106 and 105 time-steps
for discrete and continuous MDPs respectively . Each algorithm updates its
policy at steps t = 1, 3, 6, 10, . . .. We set H to 100 and 20 for discrete and
continuous MDPs respectively. More implementation details can be found in
the supplementary material.

Prior. For discrete MDPs, we use Dirichlet Dir(α) priors over each
of the transition probabilities P(s(cid:48)|s, a). The prior parameter α for each
transition is set to 0.5. We use separate NormalGamma N G(µ, κ, α, β) priors
for each of the reward distributions P(r|s, a). We set the prior parameters to
[µ0, κ0, α0, β0] = [0, 1, 1, 1]. While we use the same prior parameters for all
algorithms, we have not attempted to do an exhaustive unbiased evaluation
by tuning their hyperparameters on a small set of runs, hence, our results
should be considered preliminary.

For continuous MDPs, we use factored Bayesian Multivariate Regression
(Minka, 2001) models as priors over transition kernels and reward functions for
the continuous environments. This implies that the transition kernel P(s(cid:48)|s, a)
and reward kernel P(r|s, a) modelled as N (ATrans
s, σ2).
Σ is sampled from inverse Wishart distribution with corresponding d × d
dimensional scale matrix, while σ is sampled from inverse Gamma with
prior parameters ( 1
2 ). For transitions, we set the prior parameters to
Ψ0 = 0.001I and degrees of freedom ν0 = rank(Ψ0).

s, Σ) and N (AReward

2 , 1

a

a

For the InvertedPendulum, we use Bayesian multivariate regressor priors
on P(s(cid:48) | φ(s), a) and P(r | φ(s), a), where the feature map φ : Rd → Rf is
given by the mentioned basis functions. GPTD uses the same feature map as
BBI, while VDQN only sees the actual underlying state s. The choice of state
distribution q(s) is of utmost importance in continuous environments. In this
environment, we experimented with a few options, trading of sampling states
from our history, sampling from the starting conﬁguration of the environment
and sampling from the full support of the state space.

4.2 Description of Environments

We evaluate the algorithms on four discrete and one continuous environments.
NChain. This is a discrete stochastic MDP with 5 states, 2 actions (Strens,
2000). Taking the ﬁrst action returns a reward 2 for all states and transition-

13

ing to the ﬁrst state. Taking the second action returns 0 reward in the ﬁrst
four states (and the state increases by one) but returns 10 for the ﬁfth state
and the state remains unchained. There is a probability of slipping of 0.2
with which its action has the opposite eﬀect. This environment requires both
exploration and planning to be solved eﬀectively and thus acts as an evaluator
of posterior estimation, eﬃcient performance and eﬀective exploration.

DoubleLoop. This is a slightly more complex discrete deterministic
MDP with with two loops of states (Strens, 2000). Taking the ﬁrst action
yields traversal of the right loop and a reward 1 for every 5 state traversal.
Taking the second action yields traversal of the left loop and a reward 2 for
every 5 state traversal. This environment acts as an evaluator of eﬃcient
performance and eﬀective exploration.

LavaLake. This is a stochastic grid world (Leike et al., 2017) where
every state gives a reward of -1, unless you reach the goal, in which case you
get 50, or fall into lava, where you get -50. We tested on the 5 × 7 and a
10 × 10 versions of the environment. The agent moves in the direction of the
action (up,down,left,right) with probability 0.8 and with probability 0.2 in a
direction perpendicular to the action.

Maze. This is a grid world with four actions (ref. Fig. 3 in (Strens,
2000)). The agent must obtain 3 ﬂags and reach a goal. There are 3 ﬂags
throughout the maze and upon reaching the goal state the agent obtains a
reward of 1 for each ﬂag it has collected and the environment is reset. Similar
to LavaLake, the agent moves with probability 0.9 in the desired direction
and 0.1 in one of the perpendicular directions. The maze has 33 reachable
locations and 8 combination of obtained ﬂags for a total of 264 states.

LinearModel. This is a continuous MDP environment consisting of 4
state dimensions and 11 actions. The transitions and rewards are generated
a ∈ R4
from a linear model of the form st+1 = AS
and AS

a )T st where s, AR

a st, rt = (AR

a ∈ R4×4 for all a’s.

InvertedPendulum. To extend our results for the continuous do-
main we evaluated our algorithm in a classical environment described
in (Lagoudakis and Parr, 2003). The goal of the environment is to sta-
bilize a pendulum and to keep it from falling. If the pendulum angle θ falls
outside [ −π
2 ] then the episode is terminated and the pendulum returned
to its starting conﬁguration. The state dimensionality is a tuple of the pen-
dulum angle as well as its angular velocity, ˙θ, s = (θ, ˙θ). The environment is
considered to be completed when the pendulum has been kept within the
accepted range for 3000 steps. For further details, we refer to (Lagoudakis
and Parr, 2003).

2 , π

We use the features recommended by Lagoudakis and Parr (2003), which
are 10 basis functions that correspond to a constant term as well as 3 × 3

14

(a) NChain

(b) DoubleLoop

Figure 1: Evolution of average reward for NChain and DoubleLoop en-
vironments, averaged over 50 runs of length 106 for each algorithm. For
computational reasons BSS is only run for 104 steps. The runs are exponen-
tially smoothened with a half-life 1000 before averaging.

RBF kernels with σ2 = 1.0 and



µθ, ˙θ =



( −π
4 , −1)
(0, −1)
( π
4 , −1)

( −π
4 , 0)
(0, 0)
( π
4 , 0)

( −π
4 , 1)
(0, 1)
( π
4 , 1)



 .

We also add a regularizing term with λI, λ = 0.01 for stabilising the

ﬁtted Q-iteration.

4.3 Experimental Results

The following experiments are intended to show that the general method-
ological idea is indeed sound, and can potentially lead to high performance
algorithms.

Figures 1a, 1b, 2a, 2b and 3 illustrate the evolution of average reward
for BBI, PSRL,VDQN, MMBI, BQL and BSS on the discrete MDPs. BBI
performs similarly to to MMBI and PSRL. This is to be expected, as the
optimisation algorithm used in MMBI is close in spirit to BBI, with only
the inference being diﬀerent. In particular, this algorithm takes k MDP
samples from the posterior, and then performs backward induction in all
the MDP simultaneously to obtain a Markov policy. In turn, PSRL can
be seen as a special case of MMBI with just one sample. This indicates
that the BBI inference procedure is sound. The near-optimal Bayesian
approximation performs slightly worse in this setting, perhaps because it
was not feasible to increase the planning horizon suﬃciently.12 Finally, the

12For computational reasons we used a planning horizon of two with four next state

15

102103104105106Time steps1.01.52.02.53.03.5Average rewardBBIMMBIPSRLVDQNBQLBSS102103104105106Time steps0.150.200.250.300.350.40Average rewardBBIMMBIPSRLVDQNBQLBSS(a) LavaLake 5 × 7

(b) LavaLake 10 × 10

Figure 2: Evolution of average reward for 5 × 7 and 10 × 10 LavaLake
environments. The results are averaged over 20 and 30 runs respectively with
a length of 106 for each algorithm. The runs are exponentially smoothened
with a half-life 1000 before averaging.

less principled approximations, like VDQN and BQL do not manage to have
a satisfactory performance in these environments. In Figures 4a and 4b, we
also compare with GPTD, a classical method for Bayesian value function
estimation instead of PSRL. In Figure 4a it is evident that GPTD cannot
leverage its sophisticated Gaussian Process model to learn as well as BBI.
The same is true for VDQN, except for when the amount of data is very
small. Figure 4b shows a comparison on the InvertedPendulum environment.
Here our algorithm is competitive, and in particular performs much better
than GPTD, while it performs similarly to VDQN, which is slightly worse
initially and slightly better later in terms of average steps survived. This
performance could partially be explained by the use of a linear value function
Q(φ(s), a), in contrast to VDQN which uses a neural network. We thus feel
that further investment in our methodology is justiﬁed by our results.

5 Discussion and Future Work

We oﬀered a new perspective on Bayesian value function estimation. The
central idea is to calculate the conditional value function distribution Pπ
β(Vi |
Vi+1, D) using the data and to apply it inductively for computing the marginal
value function distribution Pπ
β(Vi | D). Following this, we propose three
possible marginalisation techniques (Methods 1, 2 and 3) and design a
Monte-Carlo approximation for Method 1. We also combined this procedure
with a suitable policy optimisation mechanism and showed that it can be

samples and two reward samples in each branching step. We hope to be able to run further
experiments with BSS at a later point.

16

102103104105106Time steps321012Average rewardBBIMMBIPSRLVDQNBQL102103104105106Time steps43210Average rewardBBIMMBIPSRLVDQNBQLFigure 3: Evolution of average reward for the Maze environment. The results
are averaged over 30 runs with a length of 106 for each algorithm. The runs
are exponentially smoothened with a half-life 1000 before averaging.

(a) LinearModel

(b) InvertedPendulum

Figure 4: Evolution of average steps survived during an episode for the
LinearModel and InvertedPendulum environment, averaged over 100 and 30
runs respectively with runs of length 105 for each algorithm. The runs are
exponentially smoothened with a half-life 1000 and 2500 respectively before
averaging.

competitive with the state of the art.

Inferential Induction diﬀers from existing Bayesian value function meth-
ods, which essentially cast the problem into regression. For example,
GPTD (Engel et al., 2003) can be written as Bayesian inference with a

17

103104105106Time steps0.000.010.020.030.040.050.060.07Average rewardBBIMMBIPSRLVDQN102103104105Time steps0.51.01.52.02.53.03.54.0Average rewardBBIGPTDVDQN102103104105Time steps101520253035Average steps survivedBBIGPTDVDQNGP prior over value functions and a data likelihood that uses a deterministic
empirical model of the MDP. While this can be relaxed by using temporal
correlations as in (Engel et al., 2005), the fundamental problem remains.
Even though such methods have practical value, we show that Bayesian
estimation of value functions requires us to explicitly think about the MDP
distribution as well.

We use speciﬁc approximations for discrete and continuous MDPs to
propose the Bayesian Backwards Induction (BBI) algorithm. Though we only
developed one algorithm, BBI, from this family, our experimental results
appear promising. We see that BBI is competitive with state-of-the art
methods like PSRL, and it signiﬁcantly outperforms the algorithms relying
on approximate inference, such as VDQN. Thus, our proposed framework of
inferential induction oﬀers a new perspective, which can provide a basis for
developing new Bayesian reinforcement learning algorithms.

Acknowledgements

Thank you to Nikolaos Tziortziotis for his useful discussions. This work was
partially supported by the Wallenberg AI, Autonomous Systems and Software
Program (WASP) funded by the Knut and Alice Wallenberg Foundation.
The experiments were partly performed on resources at Chalmers Centre for
Computational Science and Engineering (C3SE) provided by the Swedish
National Infrastructure for Computing (SNIC).

References

Bellemare, M. G., Dabney, W., and Munos, R. (2017). A distributional
perspective on reinforcement learning. In Proceedings of the 34th Interna-
tional Conference on Machine Learning-Volume 70, pages 449–458. JMLR.
org.

Dearden, R., Friedman, N., and Andre, D. (1999). Model based Bayesian
exploration. In Proceedings of the Fifteenth conference on Uncertainty in
artiﬁcial intelligence, pages 150–159.

Dearden, R., Friedman, N., and Russell, S. (1998). Bayesian Q-learning. In

Aaai/iaai, pages 761–768.

DeGroot, M. H. (1970). Optimal Statistical Decisions. John Wiley & Sons.

Deisenroth, M., Rasmussen, C., and Peters, J. (2009). Gaussian process

dynamic programming. Neurocomputing, 72(7-9):1508–1524.

18

Dimitrakakis, C. (2011). Robust Bayesian reinforcement learning through
tight lower bounds. In European Workshop on Reinforcement Learning
(EWRL 2011), pages 177–188.

Duﬀ, M. O. (2002). Optimal Learning Computational Procedures for Bayes-
adaptive Markov Decision Processes. PhD thesis, University of Mas-
sachusetts at Amherst.

Engel, Y., Mannor, S., and Meir, R. (2003). Bayes meets Bellman: The
Gaussian process approach to temporal diﬀerence learning. In Proceedings
of the 20th International Conference on Machine Learning (ICML-03),
pages 154–161.

Engel, Y., Mannor, S., and Meir, R. (2005). Reinforcement learning with
Gaussian process. In International Conference on Machine Learning, pages
201–208.

Ernst, D., Geurts, P., and Wehenkel, L. (2005). Tree-based batch mode
reinforcement learning. Journal of Machine Learning Research, 6(Apr):503–
556.

Fournier, N. and Guillin, A. (2015). On the rate of convergence in Wasserstein
distance of the empirical measure. Probability Theory and Related Fields,
162(3-4):707–738.

Ghavamzadeh, M. and Engel, Y. (2006). Bayesian policy gradient algorithms.

In NIPS 2006.

Ghavamzadeh, M., Mannor, S., Pineau, J., and Tamar, A. (2015). Bayesian
reinforcement learning: A survey. Foundations and Trends in Machine
Learning, 8(5-6):359–483.

Lagoudakis, M. and Parr, R. (2003). Least-squares policy iteration. The

Journal of Machine Learning Research, 4:1107–1149.

Leike, J., Martic, M., Krakovna, V., Ortega, P. A., Everitt, T., Lefrancq,
A., Orseau, L., and Legg, S. (2017). Ai safety gridworlds. arXiv preprint
arXiv:1711.09883.

Minka, T. P. (2001). Bayesian linear regression. Technical report, Microsoft

research.

Morimura, T., Sugiyama, M., Kashima, H., Hachiya, H., and Tanaka, T.
(2010). Nonparametric return distribution approximation for reinforcement
learning. In Proceedings of the 27th International Conference on Machine
Learning (ICML-10), pages 799–806.

Osband, I., Van Roy, B., and Wen, Z. (2016). Generalization and exploration

via randomized value functions. In ICML.

19

Puterman, M. L. (2005). Markov Decision Processes : Discrete Stochastic

Dynamic Programming. John Wiley & Sons, New Jersey, US.

Strens, M. (2000). A Bayesian framework for reinforcement learning. In

ICML 2000, pages 943–950.

Sutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduc-

tion. MIT press.

Tang, Y. and Agrawal, S. (2018). Exploration by distributional reinforcement
learning. In Proceedings of the 27th International Joint Conference on
Artiﬁcial Intelligence, pages 2710–2716. AAAI Press.

Thompson, W. (1933). On the Likelihood that One Unknown Probability
Exceeds Another in View of the Evidence of two Samples. Biometrika,
25(3-4):285–294.

Wang, T., Lizotte, D., Bowling, M., and Schuurmans, D. (2005). Bayesian
sparse sampling for on-line reward optimization. In Proceedings of the
22nd international conference on Machine learning, pages 956–963.

20

A Implementation Details

In this section we discuss some additional implementation details, in par-
ticular how exactly we performed the rollouts and the selection of some
algorithm hyperparameters, as well as some sensitivity analysis.

A.1 Computational Details of Rollouts

To speed up the computation of rollouts, we have used three possible methods
that essentially bootstrap previous rollouts or use value function samples:

uµ,πt
t

(s) = r(s, a) + γuµ,πt+1

t+1

(s(cid:48))

uµ,πt
t

(s) =

(cid:88)

r(s, a) + γP (s(cid:48)|s, a)uµ,πt+1

t+1

(s(cid:48))

uµ,πt
t

(s) =

s(cid:48)
(cid:88)

s(cid:48)

r(s, a) + γP (s(cid:48)|s, a)Vt+1(s(cid:48))

(14)

(15)

(16)

where Vt+1 ∼ ψt+1. In experiments, we have found no signiﬁcant diﬀerence
between them. All results in the paper use the formulation in (16).

A.2 Hyperparameters

For the experiments, we use the following hyperparameters.

We use 10 MDP samples, a planning horizon T of 100, γ = 0.99 and we
set the variance of the Gaussian to be σ2 = V 2
span10−4, where Vspan is the
span of possible values for each environment (obtained assuming maximum
and minimum reward). We use Eq. 16 for rollout computation with 10
samples from Vt+1 and 50 samples from Vt (20 for LavaLake 10 × 10 and
Maze). If the weights obtained in (8) are numerically unstable we attempt
to resample the value functions and then double σ until it works (but is reset
to original value when new data is obtained). This is usually only a problem
when very little data has been obtained.

In order to check the sensitivity on the choice of horizon T , we perform
In Figure 5, we can see
a sensitivity analysis with T = 10, 20, 50, 100.
that varying the horizon has a very small impact for NChain and Maze
environments.

B Additional Results

Here we present some experiments that examine the performance of inferential
induction in terms of value function estimation, inference and utility obtained.

21

(a) NChain

(b) Maze

Figure 5: Illustration of the impact of varying the horizon T in BBI. The
results are averaged over 50 and 30 runs respectively with a length of 106 for
each algorithm. The runs are exponentially smoothened with a half-life 1000
before averaging.

Figure 6: Comparisons of the achieved value functions of BBI with the
upper bound on Bayes-optimal value functions. Upper bound and BBI are
calculated from 100 MDPs and plotted for 105 time steps.

22

102103104105106Time steps1.52.02.53.03.5Average reward102050100103104105106Time steps0.000.010.020.030.040.050.060.07200400600800Time steps100200Expected utility over all statesmaxVVBBIB.1 Bayesian Value Function Estimation

In this experiment, we evaluate the Bayesian (i.e. mean) value function
of the proposed algorithm (BBI) with respect to the upper bound on
the Bayes-optimal value function. The upper bound is calculated from
(cid:82)
M maxπ V π
µ dβ(µ | D). We estimate this bound through 100 MDP samples
for NChain. We plot the time evolution of our value function and the simu-
lated Bayes bound in Figure 6 for 105 steps. We observe that this is becomes
closer to the upper bound as we obtain more data.

B.2 Value Function Distribution Estimation

Here we evaluate whether inferential induction based policy evaluation (Alg. 2)
results in a good approximation of the actual value function posterior. In
order to evaluate the eﬀectiveness of estimating the value function distribution
using inferential induction (Alg. 2), we compare it with the Monte Carlo
distribution and the mean MDP. We compare this for posteriors after 10, 100
and 1000 time steps, obtained with a ﬁxed policy in NChain that visits all
the states, in Figure 7 for 5 runs of Alg. 2. The ﬁxed policy selects the ﬁrst
action with probability 0.8 and the second action with probability 0.2. The
Monte Carlo estimate is done through 1000 samples of the value function
vector (γ = 0.99). This shows that the estimate of Alg. 2 reasonably captures
the uncertainty in the true distribution. For this data, we also compute the
Wasserstein distance (Fournier and Guillin, 2015) between the true and the
estimated distributions at the diﬀerent time steps as can be found in Table 1.
There we can see that the distance to the true distribution decreases over
time.

(a) T = 10

(b) T = 100

(c) T = 1000

Figure 7: Comparison of value function posteriors obtained by inferential
induction and Monte Carlo evaluations at diﬀerent time steps for a ﬁxed
policy. We plot for ﬁve runs of inferential induction at each time step. The
value of the mean MDP is shown by a vertical line.

23

4002000200400Value0.0000.0020.0040.0060.0080.0100.0120.014Inferential inductionInferential inductionInferential inductionInferential inductionInferential inductionMean MDPMonte Carlo050100150200250300Value0.000.010.020.030.040.05120125130135140145150155160Value0.00.10.20.30.40.5Table 1: Wasserstein distance to the true distribution of the value function,
for Alg. 2 and the mean MDP model, for NChain. For Inferential Induction,
the distances are averaged over 5 runs. The distances correspond to the plots
in Figure 7.

Time steps

Inf. Induction Mean MDP

10
100
1000

22.80
16.41
4.18

30.69
17.90
4.27

(a) BBI

(b) PSRL

(c) VDQN

(d) BQL

(e) MMBI

(f) BSS

Figure 8: Evolution of average reward for NChain environment with 50 runs
of length 106 for each algorithm. For computational reasons BSS is only run
for 104 steps. The runs are exponentially smoothened with a half-life 1000.
The mean as well as the 5th and 95th percentile performance is shown for
each algorithm and the standard error is illustrated with black lines.

B.3 Variance in Performance

In Figures 8 to 12, we illustrate the variability in performance of diﬀerent
algorithms for each environment. The black lines illustrate the standard
error and the 5th and 95th percentile performance is highlighted. The results
indicate that BSS is the most stable algorithm, followed by BBI, MMBI and
PSRL, which nevertheless have better mean performance. VDQN is quite
unstable, however.

24

102103104105106Time steps1.01.52.02.53.03.5Average reward102103104105106Time steps1.01.52.02.53.03.5Average reward102103104105106Time steps1.01.52.02.53.03.5Average reward102103104105106Time steps1.01.52.02.53.03.5Average reward102103104105106Time steps1.01.52.02.53.03.5Average reward102103104Time steps1.01.52.02.53.03.5Average reward(a) BBI

(b) PSRL

(c) VDQN

(d) BQL

(e) MMBI

(f) BSS

Figure 9: Evolution of average reward for DoubleLoop environment with 50
runs of length 106 for each algorithm. For computational reasons BSS is only
run for 104 steps. The runs are exponentially smoothened with a half-life
1000. The mean as well as the 5th and 95th percentile performance is shown
for each algorithm and the standard error is illustrated with black lines.

25

102103104105106Time steps0.150.200.250.300.350.40Average reward102103104105106Time steps0.150.200.250.300.350.40Average reward102103104105106Time steps0.150.200.250.300.350.40Average reward102103104105106Time steps0.150.200.250.300.350.40Average reward102103104105106Time steps0.150.200.250.300.350.40Average reward102103104Time steps0.150.200.250.300.350.40Average reward(a) BBI

(b) PSRL

(c) VDQN

(d) BQL

(e) MMBI

Figure 10: Evolution of average reward for LavaLake 5 × 7 environment
with 20 runs of length 106 for each algorithm. The runs are exponentially
smoothened with a half-life 1000. The mean as well as the 5th and 95th
percentile performance is shown for each algorithm and the standard error is
illustrated with black lines.

26

102103104105106Time steps321012Average reward102103104105106Time steps321012Average reward102103104105106Time steps321012Average reward102103104105106Time steps321012Average reward102103104105106Time steps321012Average reward(a) BBI

(b) PSRL

(c) VDQN

(d) BQL

(e) MMBI

Figure 11: Evolution of average reward for LavaLake 10 × 10 environment
with 30 runs of length 106 for each algorithm. The runs are exponentially
smoothened with a half-life 1000. The mean as well as the 5th and 95th
percentile performance is shown for each algorithm and the standard error is
illustrated with black lines.

27

102103104105106Time steps43210Average reward102103104105106Time steps43210Average reward102103104105106Time steps43210Average reward102103104105106Time steps43210Average reward102103104105106Time steps43210Average reward(a) BBI

(b) PSRL

(c) VDQN

(d) MMBI

Figure 12: Evolution of average reward for Maze environment with 30 runs of
length 106 for each algorithm. The runs are exponentially smoothened with
a half-life 1000. The mean as well as the 5th and 95th percentile performance
is shown for each algorithm and the standard error is illustrated with black
lines.

28

103104105106Time steps0.000.010.020.030.040.050.060.07Average reward103104105106Time steps0.000.010.020.030.040.050.060.07Average reward103104105106Time steps0.000.010.020.030.040.050.060.07Average reward103104105106Time steps0.000.010.020.030.040.050.060.07Average reward