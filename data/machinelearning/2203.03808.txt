A Fast Scale-Invariant Algorithm for
Non-negative Least Squares with Non-negative Data∗

Jelena Diakonikolas †

Chenghui Li ‡

Swati Padmanabhan §

Chaobing Song ¶

March 9, 2022

Abstract

Nonnegative (linear) least square problems are a fundamental class of problems that is well-
studied in statistical learning and for which solvers have been implemented in many of the
standard programming languages used within the machine learning community. The existing
oﬀ-the-shelf solvers view the non-negativity constraint in these problems as an obstacle and,
compared to unconstrained least squares, perform additional eﬀort to address it. However, in
many of the typical applications, the data itself is nonnegative as well, and we show that the
nonnegativity in this case makes the problem easier. In particular, while the oracle complexity
of unconstrained least squares problems necessarily scales with one of the data matrix constants
(typically the spectral norm) and these problems are solved to additive error, we show that
nonnegative least squares problems with nonnegative data are solvable to multiplicative error
and with complexity that is independent of any matrix constants. The algorithm we introduce
is accelerated and based on a primal-dual perspective. We further show how to provably ob-
tain linear convergence using adaptive restart coupled with our method and demonstrate its
eﬀectiveness on large-scale data via numerical experiments.

2
2
0
2

r
a

M
8

]

C
O
.
h
t
a
m

[

1
v
8
0
8
3
0
.
3
0
2
2
:
v
i
X
r
a

∗Authors are ordered alphabetically.
†jelena@cs.wisc.edu Department of Computer Sciences, University of Wisconsin-Madison, Madison, WI, USA
‡cli539@wisc.edu Department of Statistics, University of Wisconsin-Madison, Madison, WI, USA
§pswati@uw.edu University of Washington, Seattle, WA, USA. Part of this work was done when visiting Jelena

Diakonikolas at the University of Wisconsin-Madison in Summer 2021.

¶chaobing.song@wisc.edu Department of Computer Sciences, University of Wisconsin-Madison, Madison, WI,

USA

1

 
 
 
 
 
 
1

Introduction

Nonnegative least squares (NNLS) problems, deﬁned by

min
x≥0

1
2

(cid:107)Ax − b(cid:107)2
2,

(1.1)

where A ∈ Rm×n and b ∈ Rm, are fundamental problems and have been studied for decades in
optimization and statistical learning [LH95, BJ97, KSD13], with various oﬀ-the-shelf solvers avail-
able in standard packages of MATLAB (as lsqnonneg), Python (as optimize.nnls in the SciPy
package), and Julia (as nnls.jl). Within machine learning, NNLS problems arise whenever having
negative labels is not meaningful, for example, when labels represent quantities like prices, age, pixel
intensities, chemical concentrations, or frequency counts. NNLS is also widely used as a subroutine
in nonnegative matrix factorization to extract sparse features in applications like image processing,
computational biology, clustering, collaborative ﬁltering, and community detection [Gil14].

From a statistical perspective, NNLS problems can be shown to possess a regularization property
that enforces sparsity similar to LASSO [Tib96], while being comparatively simpler, without the
need to tune a regularization parameter or perform cross-validation [SH14, BEZ08, FK14].

From an algorithmic standpoint, the nonnegativity constraint in NNLS problems is typically
viewed as an obstacle: most NNLS algorithms need to perform additional work to handle it, and
the problem is considered harder than unconstrained least squares. However, in many applications
that use NNLS, the data is also nonnegative. This is true, for example, in problems arising in image
processing, computational genomics, functional MRI, and in applications traditionally addressed
using nonnegative matrix factorization.

We argue in this paper that when the data for NNLS is nonnegative, it is in fact possible to

obtain stronger guarantees than for traditional least squares problems.

1.1 Contributions

We study NNLS problems with (element-wise) nonnegative data matrix A, to which we refer as
the NNLS+ problems, through the lens of the (equivalent) quadratic problems:

(cid:26)

¯f (x) def=

1
2

min
x≥0

(cid:107)Ax(cid:107)2

2 − c(cid:62)x

(cid:27)

,

(P)

where c = A(cid:62)b may be assumed element-wise positive. This assumption is without loss of gen-
erality, since if there were a coordinate j of c such that cj ≤ 0, then the jth coordinate of the
gradient of ¯f would be nonnegative, implying an optimal solution x(cid:63) with x(cid:63)
j = 0. Hence, we could
ﬁx xj = 0 and optimize over only the remaining coordinates.

We further assume that the matrix A is non-degenerate: none of its rows or columns has all
elements equal to zero. This assumption is without loss of generality because (1) if such a row
existed, we could remove it without aﬀecting the objective, and (2) if the jth column had all
elements equal to zero, the optimal value of (P) would be −∞, obtained for x with xj → ∞.
Having established our assumptions and setup, we now proceed to state our contributions, which
are three-fold.

(1) A Scale-Invariant, ε-Multiplicative Algorithm. We design an algorithm based on co-
ordinate descent that, in total cost O( nnz(A)
), constructs an (cid:15)-multiplicative approximate solution
to (P). Our algorithm capitalizes on our insights on the properties of (P) that arise as a result of
the nonnegativity of A.

√

(cid:15)

2

2 (cid:107)Ax(cid:107)2

Theorem 1.1 (Informal; see Theorem 3.1). Given a matrix A ∈ Rm×n
f (x) = 1
O(n log n + n√
that E [(cid:104)∇f ((cid:101)xK), (cid:101)xK − x(cid:63)(cid:105)] ≤ ε|f (x(cid:63))|.

and ε > 0, deﬁne
2 − c(cid:62)x and x(cid:63) ∈ argminx≥0 f (x). Then, there exists an algorithm that in K =
+ such

(cid:1)(cid:1) arithmetic operations returns (cid:101)xK ∈ Rn

ε ) iterations and O(cid:0)nnz(A)(cid:0) log n + 1√

+

ε

The application of our structural observations on (P) to Theorem 4.6 of [DO19] enables the recovery
of our guarantee on the optimality gap; however, we provide a guarantee on the primal-dual gap,
and this is stronger than the one on the optimality gap stated in Theorem 1.1. What is signiﬁcant
about Theorem 1.1 is the invariance of the computational complexity to the scale of A—it does
not depend on any matrix constants. This cost stands in stark contrast to that of traditional least
squares, where the dependence of (oracle) complexity on matrix constants (speciﬁcally, the spectral
norm of A in the Euclidean case) is unavoidable [NY83], and multiplicative approximation is not
possible in general.1 On the conceptual front, our algorithm is a new acceleration technique inspired
by the variance-reduced primal-dual algorithm of [SWD21].

(2) Linear Convergence with Restart. By adopting adaptive restart in (P), we improve the
guarantee of Theorem 1.1 to one with linear convergence (with log(1/ε) complexity). Thus, we
establish the ﬁrst theoretical guarantee for NNLS+ that simultaneously satisﬁes the properties of
being scale-invariant, accelerated, and linearly-convergent.

Theorem 1.2 (Informal; see Theorem 4.2). Consider the setup of Theorem 1.1. Then, there is an
n
(cid:15) )) arithmetic operations returns (cid:101)xK ∈ Rn
algorithm that in expected O(nnz(A)(log n +
with f ((cid:101)xK) − f (x(cid:63)) ≤ ε|f (x(cid:63))|, where µ is the constant in a local error bound for (P).

µ ) log( 1

√

+

Proving this bound requires bounding the expected number of iterations between restarts in con-
junction with careful technical work in the identiﬁcation of an appropriate local error bound appli-
cable to NNLS+.

(3) Numerical Experiments. We consolidate our theoretical contributions with a demonstration
of the empirical advantage of our restarted method over state-of-the-art solvers using numerical
experiments on datasets from LibSVM with sizes up to 19996 × 1355191. Figure 1 shows that,
when combined with the restart strategy, our algorithm signiﬁcantly outperforms the compared
algorithms.

1.2 Related Work

NNLS has seen a large body of work on the empirical front. The ﬁrst method that was widely
adopted in practice (including in the lsqnonneg implementation of MATLAB) is due to the seminal
work of [LH95] (originally published in 1974). This method, based on active sets, solves NNLS via
a sequence of (unconstrained) least squares problems and has been followed up by [BJ97, VBK04,
MFLS17, DDM21] with improved empirical performance.

While these variants are generally eﬀective on small to mid-scale problem instances, they are not
suitable for extreme-scale problems ubiquitous in machine learning. For example, in the experiments
reported in [MFLS17], Fast NNLS [BJ97] took 6.63 days to solve a problem of size 45000 × 45000,
while the TNT-NN algorithm [MFLS17] took 2.45 hours. However, the latter requires computing
the Cholesky decomposition of A(cid:62)A at initialization, which can be prohibitively expensive, both

1To see why multiplicative approximation is not possible for general problems that are not nonnegative, consider
the case where the optimal objective value of the problem is equal to zero. Then any problem with a multiplicative
guarantee of the form in Theorem 1.1 would necessarily return an optimal solution.

3

in computation and in memory. Further, the experiments reported are for problems with strong
convexity, a property not satisﬁed by underdetermined systems, where NNLS is typically used.
Another prominent work on the empirical front is that of [KSD13], which performs projected
gradient descent with modiﬁed Barzilai-Borwein steps [BB88] and step sizes given by a carefully
designed sequence of diminishing scalars.

Other Related Work. To the best of our knowledge, theoretical guarantees explicitly published
for (P) have been scarce. For instance, [KSD13] and [MFLS17] mentioned in the preceding para-
graph provide only asymptotic convergence guarantees. In an orthogonal line of work, the result
on 1-fair covering by [DFO20] solves the problem dual to NNLS+, which also gives a multiplicative
guarantee for NNLS+, but with the overall complexity ˜O( nnz(A)
), where ˜O(·) hides poly-log factors.
Since our algorithm is based on the coordinate descent algorithm, we highlight some results of
other coordinate descent algorithms when specialized to the closely related problem of unconstrained
linear regression. The pioneering work of [Nes12] proposed a coordinate descent method called

(cid:15)

RCDM, which in our setting has an iteration cost O
, where (cid:107)A:j(cid:107)2 is the
Euclidean norm of the jth column of A. This was improved by [LS13], in an algorithm termed
ACDM, by combining Nesterov’s estimation technique [Nes83] and coordinate sampling, giving an
(cid:17)

for solving (P). The latest results in this line of
iteration complexity of O
work by [AZQRY16, QR16, NS17] perform non-uniform sampling atop a framework of [Nes12] and

n (cid:80)n
j=1 (cid:107)A:j (cid:107)2
2
√

(cid:107)x0 −x(cid:63)(cid:107)

(cid:16)

(cid:113)

ε

(cid:16) (cid:80)n

j=1 (cid:107)A:j (cid:107)2
2
ε

(cid:107)x0 − x(cid:63)(cid:107)2(cid:17)

(cid:113)(cid:80)n

(cid:16)

j=1 (cid:107)A:j (cid:107)2
2
√
ε

(cid:17)

achieve iteration complexity of O
, with [DO18] dropping the dependence on
max1≤j≤n (cid:107)A:j(cid:107)2. Additionally, the work of [LLX14] develops an accelerated randomized proximal
coordinate gradient (APCG) method to minimize composite convex functions.

(cid:107)x0−x(cid:63)(cid:107)

As remarked earlier, [DO19], coupled with insights on NNLS+ problems provided in this work,

can recover our guarantee for the optimality gap from Theorem 3.1.

However, our work is the ﬁrst to bring to the fore the properties of NNLS+ required to get such
a guarantee, and our choice of primal-dual perspective allows for a stronger guarantee in terms of an
upper bound on the primal-dual gap. Further, our algorithm is a novel type of acceleration, with
our primal-dual perspective transparently illustrating our use of the aforementioned properties.
We believe that these technical contributions, along with our techniques to obtain vastly improved
theoretical guarantees with the restart strategy applied to this problem, are valuable to the broader
optimization and machine learning communities.

Paper Organization. Section 2 states our notation and problem simpliﬁcation, with Section 2.3
and Section 2.4 laying out the technical groundwork for our algorithm and analysis sketch that
follow in Section 3. The primary export of our paper is in Section 3.2. Our restart strategy that
strengthens our main result is in Section 4. We conclude with numerical experiments in Section 5.
All proofs are relegated to Section A, Section B, and Section C.

2 Notation and Preliminaries

Throughout the paper, we use bold lowercase letters to denote vectors and bold uppercase letters
for matrices. For vectors and matrices, the operator (cid:48) ≥(cid:48) is applied element-wise, and R+ is the non-
negative part of the real line. We use (cid:104)a, b(cid:105) to denote the inner product of vectors a and b and ∇ for
the gradient of a function. Given a matrix A, we use A:j for its jth column vector, and for a vector
x, xj denotes its jth coordinate. We use xk for the vector in the kth iteration and, to disambiguate

4

indexing, use [xk]j to mean the jth coordinate of xk. The ith standard basis vector is denoted by
ei. For an n-dimensional vector x and A ∈ Rm×n, we deﬁne Λ = diag([(cid:107)A:1(cid:107)2
2]) and
(cid:107)x(cid:107)2

2. Finally, [n] def= {1, 2, . . . , n}.

2, . . . , (cid:107)A:n(cid:107)2

Λ = (cid:80)n

i (cid:107)A:i(cid:107)2

i=1 x2

2.1 Useful Deﬁnitions and Facts

We now review some relevant deﬁnitions and facts. A diﬀerentiable function f : Rn → R is
convex if for any x, (cid:98)x ∈ Rn, we have f ((cid:98)x) ≥ f (x) + (cid:104)∇f (x), (cid:98)x − x(cid:105). A diﬀerentiable function
f : Rn → R is said to be µ-strongly convex w.r.t. the (cid:96)2-norm if for any x, (cid:98)x ∈ Rn, we have that
f ((cid:98)x) ≥ f (x) + (cid:104)∇f (x), (cid:98)x − x(cid:105) + µ
2. We have analogous deﬁnitions for concave and strongly
concave functions, which ﬂip the inequalities noted.

2 (cid:107)x − (cid:98)x(cid:107)2

Given a convex optimization problem minx∈X f (x), where f : Rn → R is diﬀerentiable and
convex and X ⊆ Rn is closed and convex, the ﬁrst-order optimality condition of a solution x(cid:63) ∈
argminx∈X f (x) can be expressed as

(∀x ∈ X ) :

(cid:104)∇f (x(cid:63)), x − x(cid:63)(cid:105) ≥ 0.

(2.1)

2.2 Problem Setup

As discussed in the introduction, our goal is to solve
convenience, we work with the problem in the following scaled form:

(P), with A ∈ Rm×n

+ . For notational

(cid:110)
f (x) def=

1
2

min
x∈Rn
+

(cid:107)Ax(cid:107)2

2 − 1(cid:62)x

(cid:111)
,

(2.2)

This assumption is w.l.o.g. since (assuming w.lo.g. that c > 0 and A is non-degenerate; see Sec-
tion 1.1) any (P) can be brought to this form by a simple change of variable ˆxj = cjxj. This scaling
also aﬀects the matrix entries with column j being divided by cj, and they remain non-negative.
Further, the scaling need not be explicit in the algorithm since the change of variable ˆxj = cjxj is
easily reversible.

2.3 Properties of the Objective

To kick oﬀ our analysis, we highlight some properties inherent to the objective deﬁned in (2.2).
These are central to obtaining a scale-invariant algorithm for (P).

Proposition 2.1. Given f : Rn
statements all hold.

+ → R as deﬁned in (2.2) and x(cid:63) ∈ argminx∈Rn

f (x), the following

+

2 (cid:107)Ax(cid:63)(cid:107)2

a) ∇f (x(cid:63)) ≥ 0.
b) f (x(cid:63)) = − 1
2 = − 1
c) for all j ∈ [n], we have x(cid:63)
d) − 1
2

j∈[n]

(cid:80)

1
(cid:107)A:j (cid:107)2
2

≤ f (x(cid:63)) ≤ −

2 1(cid:62)x(cid:63).
(cid:104)
j ∈
0,

(cid:105)
.

1
(cid:107)A:j (cid:107)2
2
1
2 minj∈[n] (cid:107)A:j (cid:107)2
2

.

The validity of division by (cid:107)A:j(cid:107)2 in the preceding proposition is by the non-degeneracy of A

(see Section 1.1). We prove this proposition in Section A.1.

An important consequence of Proposition 2.1 (c) is that

(P) can be restricted to the hyper-
rectangle X = {x ∈ Rn : 0 ≤ xj ≤ 1
} without aﬀecting its optimal solution, but eﬀectively
reducing the search space. Thus, going forward, we replace the constraint x ≥ 0 in (P) by x ∈ X .

(cid:107)A:j (cid:107)2
2

5

2.4 Primal-Dual Gap Perspective

As alluded to earlier, our algorithm is analyzed through a primal-dual perspective. For this reason,
it is useful to consider the Lagrangian

L(x, y) = (cid:104)Ax, y(cid:105) −

1
2

(cid:107)y(cid:107)2

2 − 1(cid:62)x

(2.3)

from which we can derive our rescaled problem (2.2), which is precisely the primal problem
minx∈X P(x), where

P(x) = max
y∈Y

L(x, y) = −1(cid:62)x + max
y≥0

(cid:20)

−

1
2

(cid:21)
2 + (cid:104)Ax, y(cid:105)

(cid:107)y(cid:107)2

= −1(cid:62)x +

(cid:107)Ax(cid:107)2.

1
2

Similar to [SWD21], we use Eq. (2.3) to deﬁne the following relaxation of the primal-dual gap, for
arbitrary but ﬁxed u ∈ X , and v ∈ Rm:

Gap(u,v)
L

(x, y) def= L(x, v) − L(u, y).

(2.4)

The signiﬁcance of this relaxed gap function is that for a candidate solution (cid:101)x and an arbitrary
(cid:101)y ∈ Rm, a bound on Gap(u,v)
((cid:101)x, (cid:101)y) translates to one on the primal error, as follows. First select
u = x(cid:63), v = A(cid:101)x. Then, by observing that L((cid:101)x, A(cid:101)x) = f ((cid:101)x) and (by Lagrangian duality):

L

L(x(cid:63), (cid:101)y) ≤ max
y∈Rm

L(x(cid:63), y) = f (x(cid:63)),

we have the primal error bound:

f ((cid:101)x) − f (x(cid:63)) ≤ L((cid:101)x, A(cid:101)x) − L(x(cid:63), (cid:101)y)

= Gap(x(cid:63),A(cid:101)x)
L

((cid:101)x, (cid:101)y).

(2.5)

(2.6)

In light of this connection, our algorithm for bounding the primal error is one that generates iterates
that can be used to construct bounds on the Gap(u,v)

((cid:101)x, (cid:101)y), as we detail next.

L

3 Our Algorithm and Convergence Analysis

Our algorithm, Scale Invariant NNLS+ (SI-NNLS+), is described as follows for iterations k ≥ 1:

Conceptual Iteration of SI-NNLS+

xk = argmin

x∈X

φk(x),

yk = argmin
y∈Rm

ψk(y),

(3.1)

where φk(x) and ψk(y) are the estimate sequences derived from our analysis, as outlined in

Section 3.1.

We use our algorithm’s iterates from Eq. (3.1) to construct Gk, an upper estimate of Gap(u,v)
where (cid:101)xk, (cid:101)yk are the convex combinations of the iterates and u ∈ X , v ∈ Rm are a ﬁxed pair of
points. As stated in Inequality (2.6), our motivation for constructing Gk(u, v) ≥ Gap(u,v)
((cid:101)xk, (cid:101)yk)
is that Gap(u,v)

((cid:101)xk, (cid:101)yk) bounds our ﬁnal primal error from above.

L

L

L

((cid:101)xk, (cid:101)yk),

6

Algorithm 1 Scale Invariant Non-negative Least Squares with Non-negative Data (SI-NNLS+)
with n ≥ 4, accuracy (cid:15), initial point x0
+ such that f ((cid:101)xK) ≤ (1 + (cid:15))|f (x(cid:63))| for f deﬁned in (2.2) and x(cid:63) ∈

+

Input: Matrix A ∈ Rm×n
Output: Vector (cid:101)xK ∈ Rn
arg minx≥0 f (x).
Initialize: (cid:101)x0 = x0, y0 = y0 = Ax0, K = 5
φ0(x) = 1
2 (cid:107)x − x0(cid:107)2
Λ.
for k = 1, 2, . . . , K do

2 n log n + 6n√

ε , a1 = 1√

2n1.5 , a2 = a1

n−1 , A0 = 0, A1 = a1,

Sample jk uniformly at random from {1, 2, . . . , n}
xk ← arg min

(cid:110)
φk(x) def= a1(cid:104)A(cid:62)y0 − 1, x(cid:105) + (cid:80)k
(cid:110)
ψk(y) def= a1(cid:104)A(cid:62)y − 1, x1(cid:105) + (cid:80)k

yk ← arg max

(cid:111)
i=2 nai(cid:104)A(cid:62)yi−1 − 1, xjieji(cid:105) + φ0(x)
i=2 ai(cid:104)A(cid:62)y − 1, nxi − (n − 1)xi−1(cid:105) − Ak

.

(cid:111)

2 (cid:107)y(cid:107)2
2

(cid:2)Ak−1(cid:101)xk−1 + ak

(cid:0)nxk − (n − 1)xk−1

(cid:1)(cid:3).

(cid:101)xk = 1
Ak
yk ← yk + ak
ak+1
Ak+1 ← Ak + ak+1
ak+2 = min{ nak+1
n−1 ,

(yk − yk−1)

√

Ak+1
2n

}

end for
Return (cid:101)xK

Our analysis then bounds the upper estimate as Gk ≤ Q
Ak

, where Ak is a sequence of positive
numbers and Q is bounded. To obtain the claimed multiplicative approximation guarantee, we use
Inequality (2.6) and further argue that for u = x(cid:63), v = A(cid:101)xk, we have Q ≤ O(cid:0)|f (x∗)|(cid:1).

Our algorithm can be interpreted as a variant of the VRPDA2 algorithm from [SWD21], and our
analysis technique is inspired by the approximate duality gap technique from [DO19]; however, our
construction of Gk is novel and based on bounding the primal-dual gap (instead of the optimality
gap).

We now provide a construction of the gap estimate Gk, then analyze bounding it. A fully
speciﬁed variant of SI-NNLS+ suitable for the analysis is provided in Algorithm 1, with proofs in
Section A.2 and Section A.3. We give an implementation version of Lazy SI-NNLS+ and associated
analysis in Section C.

3.1 Gap Estimate Construction

The gap estimate Gk is constructed as the diﬀerence

Gk(u, v) = Uk(v) − Lk(u),

(3.2)

where Uk(v) ≥ L((cid:101)xk, v) and Lk(u) ≤ L(u, (cid:101)yk) are, respectively, upper and lower bounds we
construct on the Lagrangian. By the deﬁnition of Gap(u,v)
((cid:101)xk, (cid:101)yk) in Eq. (2.4), it then follows that
Gk(u, v) is a valid upper estimate of Gap(u,v)

L
((cid:101)xk, (cid:101)yk).
We ﬁrst introduce a technical component our constructions Lk and Uk crucially hinge on: we
deﬁne two positive sequences of numbers {ai}i≥1 and {ak
i }1≤i≤k, with one of their properties being
that both sum up to Ak > 0 for k ≥ 1. Elaborating further, we deﬁne A0 = 0 and {ai}i≥1 as
ai = Ai − Ai−1. The sequence {ak
1 = a1, while for

i } changes with k and for k = 1 is deﬁned by a1

L

7

k ≥ 2 :

ak
i =






a1 − (n − 1)a2,
nai − (n − 1)ai+1,
nak,

if i = 1,
if 2 ≤ i ≤ k − 1,
if i = k.

(3.3)

Summing over i ∈ [k] veriﬁes that Ak = (cid:80)k
we further require that a1 − (n − 1)a2 ≥ 0 and ∀i ≥ 2, nai − (n − 1)ai+1 ≥ 0.

i . For the sequence {ak

i=1 ak

i }1≤i≤k to be non-negative,

The signiﬁcance of these two sequences is in their role in deﬁning the algorithm’s primal-dual

output pair

(cid:101)xk =

1
Ak

(cid:88)

i∈[k]

ak
i xi

and

(cid:101)yk =

1
Ak

(cid:88)

i∈[k]

aiyi.

(3.4)

The intricate interdependence of {ai} and {ak
i } enables expressing (cid:101)xk in terms of only ai. This
expression further simpliﬁes to a recursive one, oﬀering a cheaper update for (cid:101)xk, which is in fact
the one we use in Algorithm 1.

With the sequences {ai}i≥1 and {ak

i }1≤i≤k in tow, we are now ready to show the construction

of an upper bound Uk(v) on L((cid:101)xk, v) and a lower bound Lk(u) on L(u, (cid:101)yk).

Upper Bound. To construct an upper bound, ﬁrst observe that by Eq. (2.3) and Eq. (3.4),

1
2

L((cid:101)xk, v) = (cid:104)A(cid:101)xk, v(cid:105) −
(cid:20)
(cid:104)Axi, v(cid:105) −

(cid:88)

=

ak
i

2 − 1(cid:62)
(cid:101)xk
1
2

(cid:107)v(cid:107)2

1
Ak

i∈[k]

(cid:107)v(cid:107)2

2 − 1(cid:62)xi

(cid:21)

.

Consider the primal estimate sequence deﬁned for k = 0 as ψ0 = 0 and for k ≥ 1 by

ψk(v) def=

(cid:88)

i∈[k]

(cid:20)
(cid:104)Axi, v(cid:105) −

ak
i

1
2

(cid:107)v(cid:107)2

2 − 1(cid:62)xi

(cid:21)

,

(3.5)

which ensures that L((cid:101)xk, v) = 1
ψk(v). A key upshot of constructing ψk(v) as in Eq. (3.5) is
Ak
that the quadratic term implies Ak-strong concavity of ψk for k ≥ 1, which in turn ensures that
the vector yk = arg maxy∈Rm ψk(y) from Eq. (3.1) is unique. This property, coupled with the
ﬁrst-order optimality condition in Inequality (2.1), gives that for any y ∈ Rm,

ψk(y) ≤ ψk(yk) −

Ak
2

(cid:107)y − yk(cid:107)2
2.

We are now ready to deﬁne the following upper bound by:

Uk(v) def=

1
Ak

ψk(yk) −

1
2

(cid:107)v − yk(cid:107)2
2.

(3.6)

(3.7)

The preceding discussion immediately implies that Uk is a valid upper bound for the Lagrangian,
as formalized next.

Lemma 3.1. For Uk as deﬁned in Eq. (3.7), Lagrangian deﬁned in Eq. (2.3) and (cid:101)xk ∈ Rn
Eq. (3.4), we have, for all y ∈ Rm, the upper bound

+ in

L((cid:101)xk, y) ≤ Uk(y).

8

Lower Bound. Analogous to the preceding section, we now obtain a lower bound on the La-
grangian, continuing our eﬀort to bound the gap estimate. However, the construction is more
technical than it is for the upper bound. We start with the same approach as for the upper bound.
Since L(u, (cid:101)yk) is convex in (cid:101)yk, by Jensen’s inequality:

L(u, (cid:101)yk) ≥

1
Ak

(cid:88)

(cid:16)

ai

i∈[k]

(cid:104)Au, yi(cid:105) − 1(cid:62)u −

(cid:107)yi(cid:107)2
2

(cid:17)

.

1
2

Were we to deﬁne the dual estimate sequence φk in the same way as we did for the primal estimate
sequence ψk, we would now simply deﬁne it as Ak times the right-hand side in the last inequality.
However, doing so would make φk depend on yk, which is updated after xk, which in Eq. (3.1) is
deﬁned as the minimizer of the φk.

To avoid such a circular dependency, we add and subtract a linear term (cid:80)

i∈[k](cid:104)Ayi−1, u(cid:105), where

yi−1, deﬁned later, are extrapolation points that depend only on y1, . . . yi−1. We thus have

L(u, (cid:101)yk) ≥

(cid:20)

1
Ak

(cid:88)

ai

i∈[k]

(cid:104)Au, yi−1(cid:105) − 1(cid:62)u −

(cid:21)

+

(cid:107)yi(cid:107)2
2

1
2

1
Ak

(cid:88)

i∈[k]

ai(cid:104)Au, yi − yi−1(cid:105).

If we now deﬁned φk based on the ﬁrst term in the above inequality, we run into another obstacle:
the linearity of the resulting estimate sequence is insuﬃcient for cancelling all the error terms in
the analysis. Hence, as is common, we introduce strong convexity by adding and subtracting an
appropriate strongly convex function.

Our chosen strongly convex function, motivated by the box-constrained property of the optimum
from Proposition 2.1 (c) and crucial in bounding the initial gap estimate, coincides with φ0: for
any x ∈ Rn

+, deﬁne the function

φ0(x) =

1
2

(cid:107)x − x0(cid:107)2
Λ.

(3.8)

This function is 1-strongly convex with respect to the (cid:107) · (cid:107)Λ-norm and is used in deﬁning φ1 as
follows, for some u ∈ Rn
+:

φ1(u) = a1(cid:104)A(cid:62) ¯y0 − 1, u(cid:105) + φ0(u).

(3.9)

The deﬁnition of φ1 is driven by the purpose of cancelling initial error terms. Next, we choose φk
so that for any ﬁxed u ∈ X , we have

E[φk(u)] = E

(cid:104) (cid:88)

ai(cid:104)A(cid:62)yi−1 − 1, u(cid:105) + φ0(u)

(cid:105)
,

i∈[k]

(3.10)

where the expectation is with respect to all the randomness in the algorithm. This construction
is motivated by the need to reduce the per-iteration complexity, for which we employ a randomized
single coordinate update on xk for k ≥ 2. Note that to support such updates, we relax the lower
bound to hold only in expectation.

Concretely, let ji be the coordinate sampled uniformly at random from [n] in the ith iteration
of SI-NNLS+, independent of the history. Fix yi for i = 1, 2, . . . , k − 1 and for k ≥ 2 and x ∈ X ,
deﬁne

φk(x) = φ1(x) +

nai(cid:104)A(cid:62)yi−1 − 1, xjieji(cid:105).

k
(cid:88)

For k ≥ 2, φk(u) can also be deﬁned recursively via

i=2

φk(x) = φk−1(x) + nak(cid:104)A(cid:62)yk−1 − 1, xjk ejk (cid:105).

9

(3.11)

(3.12)

The function φk inherits the strong convexity of φ0. This property, together with Eq. (3.1) and
ﬁrst-order optimality from Inequality (2.1), give

φk(x) ≥ φk(xk) +

1
2

(cid:107)x − xk(cid:107)2
Λ.

(3.13)

Along with strong convexity, our choice of φk in Eq. (3.12) leads to the following properties essential
to our analysis: (1) φk is separable in its coordinates; (2) the primal variable xk is updated only
at its jth
k coordinate; (3) Eq. (3.10) is true. These are formally stated in Proposition A.2.

With the dual estimate sequence φk deﬁned in Eq. (3.12), we now deﬁne the sequence Lk by

Lk(x) def=

1
Ak

φk(xk) +

1
2Ak

(cid:107)x − xk(cid:107)2

Λ −

1
Ak

φ0(x) +

1
Ak

(cid:88)

i∈[k]

ai(cid:104)Ax, yi − yi−1(cid:105) −

1
2Ak

(cid:88)

i∈[k]

ai(cid:107)yi(cid:107)2
2.

(3.14)

We conclude this section with the following lemma justifying our choice of Lk as a valid lower bound
on EL(x, (cid:101)yk) that holds in expectation.
Lemma 3.2. For Lk deﬁned in Eq. (3.14), for the Lagrangian in Eq. (2.3) and (cid:101)yk in Eq. (3.4), we
have, for a ﬁxed u ∈ X , the lower bound EL(u, (cid:101)yk) ≥ ELk(u), where the expectation is with respect
to all the random choices of coordinates in Algorithm 1.

3.2 Bounding the Gap Estimate

With the gap estimate Gk constructed as in the preceding section by combining Eq. (3.2), Eq. (3.7),
and Eq. (3.14), we now achieve our goal of bounding AkGk (to obtain a convergence rate of the
order 1/Ak) by bounding the change in AkGk and the initial scaled gap A1G1.

Lemma 3.3. Consider the iterates {xk} and {yk} evolving according to Algorithm 1. Let yk−1 =
yk−1 + ak−1
2n1.5 and a1 ≥ (n − 1)a2, while for
ak
k ≥ 3,

(yk−1 − yk−2). Let n ≥ 2 and assume that a1 = 1√

ak ≤ min

(cid:32)

nak−1
n − 1

,

(cid:112)Ak−1
2n

(cid:33)

.

(3.15)

Then, for ﬁxed u ∈ X , any v ∈ Rm, and all k ≥ 2, the gap estimate Gk deﬁned in Eq. (3.2) satisﬁes

E(AkGk(x, y) − Ak−1Gk−1(x, y)) ≤ −E

(cid:18) Ak
2

(cid:107)y − yk(cid:107)2

2 −

(cid:19)

(cid:107)y − yk−1(cid:107)2
2

Ak−1
2

1
2

−

E(cid:107)x − xk(cid:107)2

E(cid:107)x − xk−1(cid:107)2
Λ
− akE(cid:104)A(x − xk), yk − yk−1(cid:105) + ak−1E(cid:104)A(x − xk−1), yk−1 − yk−2(cid:105)

Λ +

1
2

−

1
4

Ak−1E(cid:107)yk − yk−1(cid:107)2

2 +

1
4

Ak−2E(cid:107)yk−1 − yk−2(cid:107)2
2.

Lemma 3.4. Consider a ﬁxed u ∈ X , any v ∈ Rm, and Gk(u, v) from Eq. (3.2). Assume ¯y0 = y0.
Then the iterates x1 and y1 of Algorithm 1 satisfy the property

A1G1(u, v) = a1(cid:104)A(cid:62)(y1 − y0), x1 − u(cid:105) + φ0(u) − φ0(x1) −

1
2

(cid:107)u − x1(cid:107)2

Λ −

A1
2

(cid:107)v − y1(cid:107)2
2.

We may combine the two preceding lemmas to bound the ﬁnal gap GK and deduce our ﬁnal

result on the primal error.

10

Theorem 3.1. [Main Result] Assume that n ≥ 4. Given a matrix A ∈ Rm×n
+ , ε > 0, an arbitrary
x0 ∈ X and ¯y0 = y0 = Ax0, let xk and Ak evolve according to SI-NNLS+ (Algorithm 1) for k ≥ 1.
For f deﬁned in (2.2), deﬁne x(cid:63) ∈ argminx≥0 f (x). Then, for all K ≥ 2, we have

E[(cid:104)∇f ((cid:101)xK), (cid:101)xK − x(cid:63)(cid:105)] ≤

2φ0(x(cid:63))
AK

=

(cid:107)x0 − x(cid:63)(cid:107)2
Λ
AK

.

The expected primal error bound is

E[f ((cid:101)xK) − f (x(cid:63))] ≤

φ0(x(cid:63))
AK

=

(cid:107)x0 − x(cid:63)(cid:107)2
Λ
2AK

.

When K ≥ 5
we have

2 n log n, we have AK ≥

(K− 5

2 n log n)2
36n2

. If φ0(x(cid:63)) ≤ |f (x(cid:63))|, then for K ≥ 5

2 n log n + 6n√
ε ,

The total cost is O(cid:0)nnz(A)(cid:0) log n + 1√
ε

(cid:1)(cid:1).

E[f ((cid:101)xK) − f (x(cid:63))] ≤ (cid:15)|f (x(cid:63))|.

The assumption φ0(x(cid:63)) ≤ |f (x(cid:63))| in the theorem is satisﬁed by x0 = 0, which can be seen by

Proposition 2.1 and Eq. (3.8).

Remark 3.1. SI-NNLS+ (Algorithm 1) and Theorem 3.1 also generalize to a mini-batch version.
Increasing the batch size grows our bounds and number of data passes by a factor of at most square-
root of the batch size m, by relating the spectral norm of the m columns of A corresponding to a
batch to the Euclidean norms of individual columns of A from the same batch. However, due to
eﬃcient available implementations of vector operations, mini-batch variants of our algorithm with
small batch sizes can have lower total runtimes on some datasets (see Section 5).

4 Adaptive Restart

We now describe how SI-NNLS+ can be combined with adaptive restart to obtain linear convergence
rate. To apply the restart strategy, we need suitable upper and lower bounds on the measure of
convergence rate. Our measure of optimality is

R(x) = x − ΠRn
+

(x − Λ−1∇f (x)) = x − (x − Λ−1∇f (x))+,

(4.1)

is the projection operator onto Rn
where ΠRn
+
is the natural map as deﬁned in, e.g., [FP07].

+ and Λ is as deﬁned in Section 2.1. For Λ = I, this

To establish local error bounds, we start with the observation that (P) is equivalent to a linear

complementarity problem.

Proposition 4.1. Problem (P) is equivalent to the following linear complementarity problem, de-
noted by LCP(M, q).

Mx + q ≥ 0, x ≥ 0, (cid:104)x, Mx + q(cid:105) = 0,

(4.2)

where Λ−1M = A(cid:62)A and q = −Λ−11.

For r(x) = (cid:107)R(x)(cid:107)Λ, a quantity termed natural residual [MR94], local error bound are obtained

as a corollary of the following theorem.

Theorem 4.1 ([MR94], Theorem 2.1). Let M ∈ Rn×n be such that LCP(M, 0) has 0 as its unique
solution. Then there exists µ > 0 such that for each x ∈ Rn, we have r(x) ≥ µ(cid:107)x − x(cid:63)(cid:107), where x(cid:63)
is a solution to LCP(M, q) that is closest to x under the norm (cid:107) · (cid:107).

11

Theorem 4.1 applies to our problem due to the nonnegativity (and nondegeneracy) of A and
choosing (cid:107) · (cid:107) = (cid:107) · (cid:107)Λ. By arguing that Theorem 3.1 provides an upper bound on r((cid:101)xK), in
expectation, we then obtain our ﬁnal result below.

Theorem 4.2. Given an error parameter (cid:15) > 0 and x0 = 0, consider the following algorithm A :

A : SI-NNLS+ with Restarts
Initialize: k = 1.
Initialize Lazy SI-NNLS+ at xk−1.
Run Lazy SI-NNLS+ until the output (cid:101)xk
K satisﬁes r((cid:101)xk
Restart Lazy SI-NNLS+ initializing at xk = (cid:101)xk
K.
Increment k.
Repeat until r((cid:101)xk

K) ≤ (cid:15).

K) ≤ 1

2 r(xk−1).

Then, the total expected number of arithmetic operations of A is at most

(cid:16)

O

(cid:16)

nnz(A)

log n +

(cid:17)

√

n
µ

log

(cid:16) r(x0)
(cid:15)

(cid:17)(cid:17)

.

As a consequence, given ¯(cid:15) > 0, the total expected number of arithmetic operations until a point with
f (x) − f (x(cid:63)) ≤ ¯(cid:15)|f (x(cid:63))| can be constructed by A is bounded by

(cid:16)

O

(cid:16)

nnz(A)

log n +

(cid:17)

√

n
µ

log

(cid:17)(cid:17)

.

(cid:16) n
µ¯(cid:15)

5 Numerical Experiments

We conclude our paper by presenting the numerical performance of SI-NNLS+ and its restart
versions (see the eﬃcient implementation version in Algorithm 2) against FISTA [BT09, Nes13], a
general-purpose large-scale optimization algorithm, and OA+DS (Ordinary Algorithm with Dimin-
ishing Scalar) designed by [KSD13] speciﬁcally for large-scale non-negative least square problems.
As an accelerated algorithm, FISTA has the optimal 1/k2 convergence rate; OA+DS, while often
eﬃcient in practice, has only an asymptotic convergence guarantee. For FISTA, we compute the
tightest Lipschitz constant (i.e., the spectral norm (cid:107)A(cid:107)); for OA+DS, we follow the best practices
laid out by [KSD13]. For our SI-NNLS+ algorithm and its restart version with batch size bs = 1,
we follow Algorithm 2 and the restart strategy in Section 4.2 For the restart version with batch size
larger than 1, we choose the best batch size in {10, 50, 300, 500} and compute the block coordinate
Lipschitz constants as the spectral norm of the corresponding block matrices.

We evaluate the performance of the algorithms on the large-scale sparse datasets real-sim,
news20, and E2006train from the LibSVM website3 for comparison. Both real-sim and news20
datasets have non-negative data matrices, but the labels may be negative. When there exist negative
labels, it is possible for the elements of A(cid:62)b to be negative. In such a case, per the discussion in
Section 1.1, we can simply remove the corresponding columns of A and solve an equivalent problem
with smaller dimension.

On the other hand, the data matrix in E2006train dataset is not non-negative, which means
that this dataset does not satisfy the assumption required for the analysis of Algorithm 1. However,

2The algorithm is implemented for the non-scaled version of the problem, (P). As discussed in Section 2.2, the

scaling in our analysis is only done for notational convenience and it has no eﬀect on the algorithm.

3The website to download LibSVM is here.

12

(a) real-sim

(b) real-sim

(c) real-sim

(d) real-sim

(e) new20

(f) new20

(g) new20

(h) new20

(i) E2006train

(j) E2006train

(k) E2006train

(l) E2006train

Figure 1: Comparison of SI-NNLS+ to FISTA and OA+DS for NNLS+ (1.1) on real-sim, news20
and E2006train datasets.

in Algorithm 1, the nonnegativity assumption for the data matrix is used only at two places: (1)
to provide additional upper bounds for variables and (2) to translate the convergence bound into a
multiplicative error guarantee. Neither of these two arguments are necessary for establishing cor-
rectness of the algorithm. As a result, for E2006train, we can run Algorithm 1 by considering only
the nonnegative constraints. We note, however, that we have not argued about linear convergence
of the algorithm with restarts for problem instances that are not non-negative, and this example
mainly serves for illustration of empirical performance.

All algorithms are implemented in the Julia programming language for high-performance com-

puting and run on a server with 16 AMD EPYC 7402P 24-Core Processors.

5.1 Results of Experiments

To compare all implemented algorithms, we plot the natural residual/objective value gap versus
number of data passes/time in Figure 1.

Figure 1(a)-(d) shows that SI+NNLS+ is better than FISTA and OA+DS in terms of number
of data passes on the real-sim dataset. Note that all of them have only sublinear convergence,
but all the SI-NNLS+ algorithms with restart strategy achieve linear convergence and are thus are
much faster in obtaining a high-accuracy solution. Speciﬁcally, with bs = 1, we have a much better
coordinate Lipschitz constant than the case of bs = 10 and thus the case of bs = 1 dominates
bs = 10. As FISTA and OA+DS take less time accessing the full dataset once, they have lower
runtimes than SI-NNLS+ but are beaten by SI-NNLS+ with restart and bs = 1.

In Figure 1(e)-(h), on the news20 dataset, in terms of number of data passes, SI-NNLS+ and

13

its restart version with bs = 1 are dominant. However, as news20 is a very sparse dataset, letting
bs = 1 signiﬁcantly increases the total time to access the full data once due to the overhead per
iteration. As a result, both the SI-NNLS+ with bs = 1 and its restart version have the worst
runtimes, while SI-NNLS+ with bs = 10 still attains the best runtime.

Figure 1(i)-(l) shows the performance comparison on the E2006train dataset. On this dataset,
both FISTA and OA+DS ran for 4 hours without visibly reducing the function value, whereas our
(block) coordinate algorithm outperforms them in both number of data passes and time. This
is because although the whole problem may be very ill-conditioned, the subproblem on certain
coordinates can be relatively well-conditioned.

6 Discussion

We introduce SI-NNLS+, a primal-dual, scale-invariant, accelerated algorithm for non-negative
least squares on non-negative data. This rate is achieved by leveraging structural properties speciﬁc
to this problem and a novel acceleration technique. Incorporating a restart strategy helps us attain
linear convergence on this problem, which we also see in our empirical results on various datasets.
It remains an open question to extend our work to other problem classes involving non-negative
data or variables.

Acknowledgements

This work was supported in part by the NSF grants 2007757 and 2023239. Part of this work was
done while SP was visiting JD at UW-Madison and while JD and CS were visiting Simons Institute
for the Theory of Computing. We thank Daniel Kane for a useful pointer to bounding one of the
expectations in the proof of Theorem 4.2.

14

References

[AZQRY16] Zeyuan Allen-Zhu, Zheng Qu, Peter Richt´arik, and Yang Yuan. Even faster acceler-
ated coordinate descent using non-uniform sampling. In International Conference on
Machine Learning, pages 1110–1119. PMLR, 2016. 1.2

[BB88]

[BEZ08]

[BJ97]

[BT09]

Jonathan Barzilai and Jonathan M Borwein. Two-point step size gradient methods.
IMA journal of numerical analysis, 8(1):141–148, 1988. 1.2

Alfred M Bruckstein, Michael Elad, and Michael Zibulevsky. On the uniqueness of
nonnegative sparse solutions to underdetermined systems of equations. IEEE Trans-
actions on Information Theory, 54(11):4813–4820, 2008. 1

Rasmus Bro and Sijmen Jong. A fast non-negativity-constrained least squares algo-
rithm. Journal of Chemometrics, 11:393–401, 09 1997. 1, 1.2

Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for
linear inverse problems. SIAM journal on imaging sciences, 2(1):183–202, 2009. 5

[DDM21] Monica Dessole, Marco Dell’Orto, and Fabio Marcuzzi. The lawson-hanson algorithm
with deviation maximization: Finite convergence and sparse recovery. arXiv preprint
arXiv:2108.05345, 2021. 1.2

[DFO20]

Jelena Diakonikolas, Maryam Fazel, and Lorenzo Orecchia. Fair packing and covering
on a relative scale. SIAM Journal on Optimization, 30(4):3284–3314, 2020. 1.2

[DO18]

[DO19]

[FK14]

[FP07]

[Gil14]

[KSD13]

[LH95]

Jelena Diakonikolas and Lorenzo Orecchia. Alternating randomized block coordinate
descent. In International Conference on Machine Learning, pages 1224–1232. PMLR,
2018. 1.2

Jelena Diakonikolas and Lorenzo Orecchia. The approximate duality gap technique: A
uniﬁed theory of ﬁrst-order methods. SIAM Journal on Optimization, 29(1):660–689,
2019. 1.1, 1.2, 3

Simon Foucart and David Koslicki. Sparse recovery by means of nonnegative least
squares. IEEE Signal Processing Letters, 21(4):498–502, 2014. 1

Francisco Facchinei and Jong-Shi Pang. Finite-dimensional variational inequalities
and complementarity problems. Springer Science & Business Media, 2007. 4

Nicolas Gillis. The why and how of nonnegative matrix factorization. Connections,
12:2–2, 2014. 1

Dongmin Kim, Suvrit Sra, and Inderjit S Dhillon. A non-monotonic method for large-
scale non-negative least squares. Optimization Methods and Software, 28(5):1012–1039,
2013. 1, 1.2, 1.2, 5

Charles L. Lawson and Richard J. Hanson. Solving least squares problems, volume 15
of Classics in Applied Mathematics. Society for Industrial and Applied Mathematics
(SIAM), Philadelphia, PA, 1995. Revised reprint of the 1974 original. 1, 1.2

[LLX14]

Qihang Lin, Zhaosong Lu, and Lin Xiao. An accelerated proximal coordinate gradient
method. Advances in Neural Information Processing Systems, 27:3059–3067, 2014. 1.2

15

[LS13]

Yin Tat Lee and Aaron Sidford. Eﬃcient accelerated coordinate descent methods and
faster algorithms for solving linear systems. In 2013 ieee 54th annual symposium on
foundations of computer science, pages 147–156. IEEE, 2013. 1.2

[MFLS17]

Joe M Myre, Erich Frahm, David J Lilja, and Martin O Saar. TNT-NN: A fast active
set method for solving large non-negative least squares problems. Procedia Computer
Science, 108:755–764, 2017. 1.2, 1.2

[MR94]

[Nes83]

[Nes12]

[Nes13]

[NS17]

[NY83]

[QR16]

Olvi L. Mangasarian and J Ren. New improved error bounds for the linear comple-
mentarity problem. Mathematical Programming, 66(1):241–255, 1994. 4, 4.1

Yurii Nesterov. A method of solving a convex programming problem with convergence
rate O(1/k2). In Doklady AN SSSR (translated as Soviet Mathematics Doklady), vol-
ume 269, pages 543–547, 1983. 1.2

Yu Nesterov. Eﬃciency of coordinate descent methods on huge-scale optimization
problems. SIAM Journal on Optimization, 22(2):341–362, 2012. 1.2

Yu Nesterov. Gradient methods for minimizing composite functions. Mathematical
programming, 140(1):125–161, 2013. 5

Yurii Nesterov and Sebastian U Stich. Eﬃciency of the accelerated coordinate de-
scent method on structured optimization problems. SIAM Journal on Optimization,
27(1):110–123, 2017. 1.2

Arkadii Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and
method eﬃciency in optimization. 1983. 1.1

Zheng Qu and Peter Richt´arik. Coordinate descent with arbitrary sampling i: Al-
gorithms and complexity. Optimization Methods and Software, 31(5):829–857, 2016.
1.2

[SH14]

Martin Slawski and Matthias Hein. Non-negative least squares for high-dimensional
linear models: consistency and sparse recovery without regularization, 2014. 1

[SWD21]

Chaobing Song, Stephen J Wright, and Jelena Diakonikolas. Variance reduction via
primal-dual accelerated dual averaging for nonsmooth convex ﬁnite-sums. In Interna-
tional Conference on Machine Learning, 2021. 1.1, 2.4, 3

[Tib96]

Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the
Royal Statistical Society: Series B (Methodological), 58(1):267–288, 1996. 1

[VBK04] Mark H Van Benthem and Michael R Keenan. Fast algorithm for the solution of large-
scale non-negativity-constrained least squares problems. Journal of Chemometrics: A
Journal of the Chemometrics Society, 18(10):441–450, 2004. 1.2

16

Appendices

A Appendix: Omitted Technical Details

In this section, we provide proofs for all the claims made in the main body of the paper, additional
supporting propositions and lemmas, and also omitted details of the implementation of our algo-
rithm. The proofs are provided in the order in which the corresponding statement appears in the
main body. In Section A.1, we prove the properties of our problem. We again emphasize that these
properties are crucial to achieving our goal of a scale-invariant algorithm for (P). Section A.2 con-
tains the proofs of all our results pertaining to the convergence analysis of Algorithm 1, with proofs
of the growth rate of the scalar sequences {ai}, {ak
i }, and Ak grouped separately in Section A.3,
owing to their more technical nature.

A.1 Omitted Proof from Section 2: Properties of Our Objective

Proposition 2.1. Given f : Rn
statements all hold.

+ → R as deﬁned in (2.2) and x(cid:63) ∈ argminx∈Rn

f (x), the following

+

2 (cid:107)Ax(cid:63)(cid:107)2

a) ∇f (x(cid:63)) ≥ 0.
b) f (x(cid:63)) = − 1
2 = − 1
c) for all j ∈ [n], we have x(cid:63)
d) − 1
2

j∈[n]

(cid:80)

1
(cid:107)A:j (cid:107)2
2

≤ f (x(cid:63)) ≤ −

2 1(cid:62)x(cid:63).
(cid:104)
0,
j ∈

(cid:105)
.

1
(cid:107)A:j (cid:107)2
2
1
2 minj∈[n] (cid:107)A:j (cid:107)2
2

.

Proof. We recall the ﬁrst-order optimality condition stated in Inequality (2.1): for all x ≥ 0, we
have (cid:104)∇f (x(cid:63)), x − x(cid:63)(cid:105) ≥ 0; we repeatedly invoke this inequality in the proof below.

1. Suppose there exists a coordinate j at which Proposition 2.1 (a) does not hold and instead, we
have ∇jf (x(cid:63)) < 0. Consider x ≥ 0 such that xi = x(cid:63)
j + (cid:15) for some
(cid:15) > 0. Then, Inequality (2.1) becomes ∇jf (x(cid:63)) · (cid:15) ≥ 0. Under the assumption ∇jf (x(cid:63)) < 0,
this is an invalid inequality, thus contradicting our assumption.

i for all i (cid:54)= j and let xj = x(cid:63)

2. From Proposition 2.1 (a), we know that ∇f (x(cid:63)) ≥ 0. If ∇if (x(cid:63)) > 0, and if x(cid:63)
i − γ for any γ ∈ (0, x(cid:63)

picking a vector x such that xj = x(cid:63)
Inequality (2.1). Therefore it must be the case that if ∇if (x(cid:63)) > 0, then x(cid:63)
have

j for j (cid:54)= i and xi = x(cid:63)

i > 0, then by
i ), we violate
i = 0. Thus we

0 = (cid:104)x(cid:63), ∇f (x(cid:63))(cid:105) = (cid:104)x(cid:63), A(cid:62)Ax(cid:63) − 1(cid:105).

Therefore, f (x(cid:63)) = 1

2 (cid:104)x(cid:63), A(cid:62)Ax(cid:63)(cid:105) − 1(cid:62)x(cid:63) = − 1

2 (cid:104)x(cid:63), A(cid:62)Ax(cid:63)(cid:105) = − 1

2 1(cid:62)x(cid:63).

3. From the proof of Proposition 2.1 (b), we have (cid:104)x(cid:63), ∇f (x(cid:63))(cid:105) = 0. We also have x(cid:63) ≥ 0 and,
from Proposition 2.1 (a), that ∇f (x(cid:63)) ≥ 0. Therefore, if x(cid:63)
i > 0 for some coordinate i then
it must be that ∇if (x(cid:63)) = 0. That is, 1 = (cid:104)A:i, Ax(cid:63)(cid:105). Combining this equality with the fact
that A and x(cid:63) are both coordinate-wise non-negative gives

which implies x(cid:63)

i ≤ 1

(cid:107)A:i(cid:107)2
2

1 = (cid:104)A:i, Ax(cid:63)(cid:105) ≥ (cid:104)A:i, A:ix(cid:63)

i (cid:105),

for all coordinates i.

17

4. The lower bound follows immediately by combining Proposition 2.1 (b) and Proposition 2.1 (c).
For the upper bound, we need to ﬁnd a feasible point (cid:98)x and compute the function value at
(cid:98)x, since f (x(cid:63)) = miny≥0 f (y) ≤ f ((cid:98)x). Let (cid:98)x = γek for some γ > 0. Then,

f ((cid:98)x) =

1
2

γ2(cid:107)A:k(cid:107)2

2 − γ.

Let γ = 1

(cid:107)A:k(cid:107)2
2
1
2 mini∈[n] (cid:107)A:i(cid:107)2
2

−

. Then, f ((cid:98)x) = − 1
as claimed.

2(cid:107)A:k(cid:107)2
2

. We pick k = arg mini∈[n] (cid:107)A:i(cid:107)2, therefore f (x(cid:63)) ≤

A.2 Omitted Proofs from Section 3: Analysis of Algorithm

A.2.1 Proofs from Section 3.1: Results on Upper and Lower Estimates

We ﬁrst show the results stating Uk and Lk are indeed valid upper and lower (respectively) estimates
of the Lagrangian.

Lemma 3.1. For Uk as deﬁned in Eq. (3.7), Lagrangian deﬁned in Eq. (2.3) and (cid:101)xk ∈ Rn
Eq. (3.4), we have, for all y ∈ Rm, the upper bound

+ in

Proof. By evaluating the Lagrangian described by Eq. (2.3) at x = (cid:101)xk, and by deﬁnition of ψk from
Eq. (3.5), we obtain the following upper bound on the Lagrangian at ((cid:101)xk, y).

L((cid:101)xk, y) ≤ Uk(y).

1
2

L((cid:101)xk, y) = (cid:104)A(cid:101)xk, y(cid:105) −
(cid:20)
(cid:104)Axi, y(cid:105) −

(cid:88)

=

ak
i

2 − 1(cid:62)
(cid:101)xk
1
2

(cid:107)y(cid:107)2

1
Ak

i∈[k]

(cid:107)y(cid:107)2 − 1(cid:62)xi

(cid:21)

=

1
Ak

ψk(y)

≤

1
Ak

ψk(yk) −

1
2

(cid:107)y − yk(cid:107)2

2 = Uk(y),

where the ﬁnal steps are by Inequality (3.6) and Eq. (3.7).

Lemma 3.2. For Lk deﬁned in Eq. (3.14), for the Lagrangian in Eq. (2.3) and (cid:101)yk in Eq. (3.4), we
have, for a ﬁxed u ∈ X , the lower bound EL(u, (cid:101)yk) ≥ ELk(u), where the expectation is with respect
to all the random choices of coordinates in Algorithm 1.

Proof. First, evaluating Eq. (2.3) at (cid:101)yk gives

L(u, (cid:101)yk) = (cid:104)Au, (cid:101)yk(cid:105) − 1(cid:62)u −

1
2

(cid:107)(cid:101)yk(cid:107)2
2.

Taking the expectation on both sides, applying the deﬁnition of (cid:101)yk, convexity of 1

2 (cid:107)·(cid:107)2, and Jensen’s

18

inequality, and adding and subtracting 1
Ak

E (cid:80)

i∈[k] ai(cid:104)Ax, yi−1(cid:105) + 1
Ak

φ0(u) gives

EL(u, (cid:101)yk) ≥

E

1
Ak





(cid:88)

(cid:20)
(cid:104)Au, yi(cid:105) − 1(cid:62)u −

ai

i∈[k]

(cid:21)





1
2

(cid:107)yi(cid:107)2
2

=

E

1
Ak

+

1
Ak





(cid:88)

(cid:20)
(cid:104)Au, yi−1(cid:105) − 1(cid:62)u −

ai

i∈[k]


(cid:88)

E



i∈[k]



ai(cid:104)Au, yi − yi−1(cid:105)

 .

1
2

(cid:107)yi(cid:107)2
2



(cid:21)

 +

1
Ak

E [φ0(u)] −

1
Ak

E [φ0(u)]

We continue the analysis as

EL(u, (cid:101)yk) ≥

≥

1
Ak

1
Ak

E [φk(u)] −

1
Ak

E [φ0(u)] +

1
Ak

(cid:88)

i∈[k]

aiE(cid:104)Au, yi − yi−1(cid:105) −

1
2Ak

E (cid:88)

i∈[k]

ai(cid:107)yi(cid:107)2
2

E [φk(xk)] + E

(cid:20) 1
2Ak

(cid:107)u − xk(cid:107)2
Λ

(cid:21)

−

1
Ak

E[φ0(u)] +

E

1
Ak





(cid:88)

i∈[k]



ai(cid:104)Au, yi − yi−1(cid:105)



−

1
2Ak

E (cid:88)

i∈[k]

ai(cid:107)yi(cid:107)2
2

= ELk(u),

the ﬁrst step comes from Eq. (3.10), the second step comes from Eq. (3.13), and the ﬁnal step
comes from Eq. (3.14).

A.2.2 Proofs from Section 3.2

We now describe three technical propositions that bound terms that show up in the proof of our
result on bounding the scaled gap estimate.

Proposition A.1. For ψk deﬁned in Eq. (3.5), with {ak
k ≥ 1,

i } deﬁned in Eq. (3.3), we have for all

ψk(yk) − ψk−1(yk−1) ≤ ak
k

(cid:26)

(cid:104)yk, Axk(cid:105) −

(cid:107)yk(cid:107)2

2 − 1(cid:62)xk

(cid:27)

1
2

k−1
(cid:88)

(ak

i − ak−1
i

(cid:20)
(cid:104)yk, Axi(cid:105) −

)

1
2

(cid:21)

(cid:107)yk(cid:107)2

2 − 1(cid:62)xi

+

−

i=1
Ak−1
2

(cid:107)yk − yk−1(cid:107)2
2.

Proof. Evaluating ψk and ψk−1 as deﬁned in Eq. (3.5) at yk and subtracting, we have

ψk(yk) − ψk−1(yk) = ak(cid:104)A(cid:62)yk − 1, nxk − (n − 1)xk−1(cid:105) −

ak
2

(cid:107)yk(cid:107)2
2.

(A.1)

Next, applying Inequality (3.6) to ψk−1 at yk and yk−1 while using the fact that yk−1 minimizes
ψk−1 gives

ψk−1(yk) − ψk−1(yk−1) ≤ −

Ak−1(cid:107)yk − yk−1(cid:107)2
2.

(A.2)

1
2

To complete the proof, it remains to add Eq. (A.1) and Inequality (A.2).

19

Proposition A.2. The random function φk : X → R, k ≥ 2, deﬁned in Eq. (3.12) satisﬁes the
following properties, with xk, yk, and yk evolving as per Algorithm 1.

a) It is separable in its coordinates: φk(x) = (cid:80)

j∈[n] φk,j(xj), where, for each j ∈ [n], we deﬁne

φ0,j(xj) = (cid:107)A:j (cid:107)2

2

2

(xj − [x0]j)2, φ1,j(xj) = a1xj(A(cid:62)y0 − 1)j + φ0,j(xj), and for k ≥ 2,

φk,j(xj) = φk−1,j(xj) + nak1j=jk (cid:104)A(cid:62)yk−1 − 1, xjk ejk (cid:105).

(A.3)

b) The primal variable xk is updated only on the jth
γejk for some γ, and [xk]j = [xk−1]j for j (cid:54)= jk.

k coordinate in each iteration: xk = xk−1 +

c) For a ﬁxed x ∈ X and for k ≥ 1, we have, over all the randomness in the algorithm,

E [φk(x)] = E [φ0(x)] +

(cid:88)

aiE

(cid:104)
(cid:104)A(cid:62)yi−1 − 1, x(cid:105)

(cid:105)

.

(A.4)

i∈[k]

Proof. In the statement of Proposition A.2 (a), the claim about separability of φ0 and φ1 can be
checked just from the deﬁnitions of φ0,j and φ1,j. We prove the claim of coordinate-wise separability
for k ≥ 2 by summing over j ∈ [n] both sides of Eq. (A.3). We can compute this sum via following
observation, which concludes the proof of Proposition A.2 (a).

ak1j=jk (cid:104)A(cid:62)yk−1 − 1, xjk ejk (cid:105) = ak(cid:104)A(cid:62)yk−1 − 1, xjk ejk (cid:105).

(cid:88)

j∈[n]

From Proposition A.2 (a), we may therefore deﬁne, for j (cid:54)= jk,

[xk]j = arg min
u∈R+

φk,j(u) = arg min
u∈R+

φk−1,j(u) = [xk−1]j.

Therefore, [xk]j = [xk−1]j for all j (cid:54)= jk, thus proving Proposition A.2 (b). To prove Proposi-
tion A.2 (c), we use induction on k. The base case holds for k = 1 by the deﬁnition of φ1(x). Let
Proposition A.2 (c) hold for k ≥ 1. Then, by the deﬁnition of φk as in Eq. (3.12), we have

φk(x) = φk−1(x) + nak(cid:104)A(cid:62)yk−1 − 1, xjk ejk (cid:105), for all k ≥ 2.

Let Fk−1 be the natural ﬁltration, containing all the randomness in the algorithm up to and
including iteration k−1. Taking expectations with respect to all the randomness until iteration k and
invoking linearity of expectation, the inductive hypothesis, and the tower rule E[ · ] = E[E[ · |Fk−1]],
we have

E[φk(x)] = E[φk−1(x)] + nakE

(cid:104)(cid:104)

= E [φ0(x)] +

(cid:88)

aiE

E(cid:104)A(cid:62)yk−1 − 1, xjk ejk (cid:105)|Fk−1
(cid:104)
(cid:104)A(cid:62)yi−1 − 1, x(cid:105)

(cid:105)

+ akE(cid:104)A(cid:62)yk−1 − 1, x(cid:105)

(cid:105)(cid:105)

= E [φ0(x)] +

i∈[k−1]
(cid:88)

aiE

i∈[k]

(cid:104)

(cid:105)
(cid:104)A(cid:62)yi−1 − 1, x(cid:105)

,

which ﬁnishes the proof of Proposition A.2 (c).

Proposition A.3. For all k ≥ 2, the random function φk : X → R, k ≥ 2, deﬁned in Eq. (3.12)
satisﬁes the following inequality, where xk and yk evolve according to Algorithm 1.

φk(xk) − φk−1(xk−1) ≥ ak

(cid:16)

n(cid:104)A(cid:62)yk−1 − 1, [xk]jk ejk (cid:105)

(cid:17)

+

1
2

(cid:107)xk − xk−1(cid:107)2
Λ.

20

Proof. We have, using x = xk in Eq. (3.12), that

φk(xk) − φk−1(xk) = nak(cid:104)A(cid:62)yk−1 − 1, [xk]jk ejk (cid:105).

Applying Eq. (3.13) to φk−1 at xk gives

φk−1(xk) − φk−1(xk−1) ≥

1
2

(cid:107)xk − xk−1(cid:107)2
Λ.

Adding these inequalities completes the proof of the claim.

We now use the preceding technical results to bound the change in scaled gap.

Lemma 3.3. Consider the iterates {xk} and {yk} evolving according to Algorithm 1. Let yk−1 =
yk−1 + ak−1
2n1.5 and a1 ≥ (n − 1)a2, while for
ak
k ≥ 3,

(yk−1 − yk−2). Let n ≥ 2 and assume that a1 = 1√

ak ≤ min

(cid:32)

nak−1
n − 1

,

(cid:112)Ak−1
2n

(cid:33)

.

(3.15)

Then, for ﬁxed u ∈ X , any v ∈ Rm, and all k ≥ 2, the gap estimate Gk deﬁned in Eq. (3.2) satisﬁes

E(AkGk(x, y) − Ak−1Gk−1(x, y)) ≤ −E

(cid:18) Ak
2

(cid:107)y − yk(cid:107)2

2 −

(cid:19)

(cid:107)y − yk−1(cid:107)2
2

Ak−1
2

1
2

−

E(cid:107)x − xk(cid:107)2

E(cid:107)x − xk−1(cid:107)2
Λ
− akE(cid:104)A(x − xk), yk − yk−1(cid:105) + ak−1E(cid:104)A(x − xk−1), yk−1 − yk−2(cid:105)

Λ +

1
2

−

1
4

Ak−1E(cid:107)yk − yk−1(cid:107)2

2 +

1
4

Ak−2E(cid:107)yk−1 − yk−2(cid:107)2
2.

Proof. Using Gk from Eq. (3.2), Uk from Eq. (3.7), and Lk from Eq. (3.14), we have

AkGk(u, v) = ψk(yk) − φk(xk) + φ0(u)
Ak
2

(cid:107)v − yk(cid:107)2

2 −

1
2

−

(cid:107)u − xk(cid:107)2

Λ −

(cid:88)

i∈[k]

ai(cid:104)Au, yi − yi−1(cid:105) +

ai
2

(cid:88)

i∈[k]

(cid:107)yi(cid:107)2
2.

Therefore, the diﬀerence in scaled gap between successive iterations is

AkGk(u, v) − Ak−1Gk−1(u, v) = [ψk(yk) − ψk−1(yk−1)] − [φk(xk) − φk−1(xk−1)]

(cid:107)v − yk−1(cid:107)2
2

−

−

(cid:107)v − yk(cid:107)2

Ak
2
1
(cid:107)u − xk(cid:107)2
2

2 +
1
2

Λ +

Ak−1
2

− ak(cid:104)Au, yk − yk−1(cid:105) +

(cid:107)u − xk−1(cid:107)2
Λ
ak
2

(cid:107)yk(cid:107)2
2.

(A.5)

Based on the above expression, to prove the lemma, it suﬃces to bound the expectation of

Tk(u) def= [ψk(yk) − ψk−1(yk−1)] − [φk(xk) − φk−1(xk−1)] − ak(cid:104)Au, yk − yk−1(cid:105) +

ak
2

(cid:107)yk(cid:107)2

2. (A.6)

First, we take expectations on both sides of the inequality in Proposition A.3 by invoking E[ · ] =
E[E[ · |Fk−1]] as before, where Fk−1 denotes the natural ﬁltration. By using the fact that xk−1 is

21

updated only at coordinate jk (as stated in Proposition A.2 (b)), we observe the following for the
term from the right hand side of Proposition A.3.

E

(cid:105)
(cid:104)
(cid:104)A(cid:62)yk−1 − 1, [xk]jk ejk (cid:105)

= E

= E

= E

(cid:104)

(cid:104)

(cid:104)

(cid:105)
(cid:104)A(cid:62)yk−1 − 1, xk − xk−1(cid:105)
(cid:105)
(cid:104)A(cid:62)yk−1 − 1, xk − xk−1(cid:105)

(cid:104)

(cid:104)
E

+ E

(cid:104)A(cid:62)yk−1 − 1, [xk−1]jk ejk (cid:105)|Fk−1

(cid:105)(cid:105)

+

E

1
n

(cid:105)
(cid:104)
(cid:104)A(cid:62)yk−1 − 1, xk−1(cid:105)
(cid:105)

(cid:104)A(cid:62)yk−1 − 1, xk − (1 − 1/n) xk−1(cid:105)

.

(A.7)

Therefore, we have from Proposition A.3 and scaling Eq. (A.7) by −nak that

−E [φk(xk) − φk−1(xk−1)] ≤ −

E (cid:2)(cid:107)xk − xk−1(cid:107)2

Λ

(cid:3)

1
2
− akE

(cid:105)
(cid:104)
(cid:104)A(cid:62)yk−1 − 1, nxk − (n − 1)xk−1(cid:105)

.

(A.8)

We now bound the expectation of the term involving diﬀerences of ψk by taking expectations of
both sides of Proposition A.1.

E [ψk(yk) − ψk−1(yk−1)] ≤ −

Ak−1
2
+ akE

E (cid:2)(cid:107)yk − yk−1(cid:107)2

2

(cid:3) −

E (cid:2)(cid:107)yk(cid:107)2

2

(cid:3)

(cid:105)
(cid:104)
(cid:104)A(cid:62)yk − 1, nxk − (n − 1)xk−1(cid:105)

ak
2

By taking expectations on both sides of Eq. (A.6), we have

E[Tk(u)] = E [ψk(yk) − ψk−1(yk−1)] − E [φk(xk) − φk−1(xk−1)]

− akE (cid:2)(cid:104)Au, yk − yk−1(cid:105)(cid:3) +

ak
2

E (cid:2)(cid:107)yk(cid:107)2

2

(cid:3) .

Combining Eq. (A.8), Eq. (A.9), and Eq. (A.10) then gives

E[Tk(u)] ≤ −

Ak−1
2
+ akE

E (cid:2)(cid:107)yk − yk−1(cid:107)2

1
2
(cid:105)
(cid:104)
(cid:104)A(cid:62)(yk − yk−1), nxk − (n − 1)xk−1 − u(cid:105)

E (cid:2)(cid:107)xk − xk−1(cid:107)2

(cid:3) −

Λ

(cid:3)

2

.

Recall that by the assumption in the statement of the lemma,

yk−1 = yk−1 +

ak−1
ak

(yk−1 − yk−2).

Plugging into Eq. (A.11) and rearranging, we have

(A.9)

.

(A.10)

(A.11)

E[Tk(u)] ≤ −

Ak−1
2

2

E (cid:2)(cid:107)yk − yk−1(cid:107)2

(cid:3) −

E (cid:2)(cid:107)xk − xk−1(cid:107)2
(cid:105)

1
2
(cid:104)A(cid:62)(yk − yk−1), xk − xk−1(cid:105)
(cid:104)
(cid:104)A(cid:62)(yk−1 − yk−2), xk−1 − u(cid:105)

− nak−1E

− ak−1E

(cid:105)
(cid:104)A(cid:62)(yk − yk−1), xk − u(cid:105)

(cid:104)

(cid:104)

Λ

(cid:3)

+ (n − 1)akE
(cid:104)

+ akE

(cid:104)A(cid:62)(yk−1 − yk−2), xk − xk−1(cid:105)

(cid:105)

(cid:105)

.

(A.12)

22

To complete the proof, we need to bound the terms from the ﬁrst two lines on the right-hand side
of Eq. (A.12). First, observe that, by the coordinate update of xk and Young’s inequality, ∀β > 0,

(cid:104)A(cid:62)(yk − yk−1), xk − xk−1(cid:105) = (cid:104)yk − yk−1, A(xk − xk−1)(cid:105)

= (cid:104)yk − yk−1, A:jk ([xk]jk − [xk−1]jk )(cid:105)

≤

=

β
2
β
2

(cid:107)yk − yk−1(cid:107)2

2 +

(cid:107)yk − yk−1(cid:107)2

2 +

1
2β
1
2β

(cid:107)A:jk (cid:107)2

2|[xk]jk − [xk−1]jk |2

(cid:107)xk − xk−1(cid:107)2
Λ.

(A.13)

By the same token, ∀γ > 0,

−(cid:104)A(cid:62)(yk−1 − yk−2), xk − xk−1(cid:105) ≤

γ
2

(cid:107)yk−1 − yk−2(cid:107)2

2 +

1
2γ

(cid:107)xk − xk−1(cid:107)2
Λ.

(A.14)

Recalling that, by the choice of step sizes, (n − 1)ak ≤ nak−1 and ak ≤
for β = 2(n − 1)ak and γ = 2nak−1, the following inequalities hold:

(n − 1)akβ − Ak−1 ≤ −

(n − 1)ak
β

+

nak−1
γ

≤ 1.

Ak−1
2

,

Combining Equations (A.12)–(A.15),

√

Ak−1
2n

, we can verify that

(A.15)

E[Tk(u)] ≤ −

Ak−1
4
+ akE

(cid:104)

E[(cid:107)yk − yk−1(cid:107)2

2] + n2ak−1
(cid:105)
(cid:104)A(cid:62)(yk − yk−1), xk − u(cid:105)

2E[(cid:107)yk−1 − yk−2(cid:107)2
2]
(cid:104)
(cid:104)A(cid:62)(yk−1 − yk−2), xk−1 − u(cid:105)

− ak−1E

(cid:105)

.

(A.16)

It remains to combine Eq. (A.5), Eq. (A.6), and Eq. (A.16).

Lemma 3.4. Consider a ﬁxed u ∈ X , any v ∈ Rm, and Gk(u, v) from Eq. (3.2). Assume ¯y0 = y0.
Then the iterates x1 and y1 of Algorithm 1 satisfy the property

A1G1(u, v) = a1(cid:104)A(cid:62)(y1 − y0), x1 − u(cid:105) + φ0(u) − φ0(x1) −

1
2

(cid:107)u − x1(cid:107)2

Λ −

A1
2

(cid:107)v − y1(cid:107)2
2.

Proof. Evaluating Eq. (3.7) and Eq. (3.14) at k = 1 gives

A1U1(v) = ψ1(y1) −

A1L1(u) = φ1(x1) +

(cid:107)v − y1(cid:107)2
2,

A1
2
1
(cid:107)u − x1(cid:107)2
2

Λ − φ0(u) + a1(cid:104)Au, y1 − y0(cid:105) −

(A.17)

(A.18)

a1
2

(cid:107)y1(cid:107)2
2.

By deﬁnition of ψ1 from Eq. (3.5), φ1 from Eq. (3.9), and the assignment a1

1 = a1, we have

ψ1(y1) − φ1(x1) = −

= −

a1
2
a1
2

(cid:107)y1(cid:107)2

2 + a1(cid:104)y1, Ax1(cid:105) − a11(cid:62)x1 − φ0(x1) − a1(cid:104)A(cid:62)y0 − 1, x1(cid:105)

(cid:107)y1(cid:107)2

2 + a1(cid:104)A(y1 − y0), x1(cid:105) − φ0(x1),

where we have used that ¯y0 = y0, which holds by assumption. To complete the proof, it remains
to subtract Eq. (A.18) from Eq. (A.17) and combine with the last equality.

23

Theorem 3.1. [Main Result] Assume that n ≥ 4. Given a matrix A ∈ Rm×n
+ , ε > 0, an arbitrary
x0 ∈ X and ¯y0 = y0 = Ax0, let xk and Ak evolve according to SI-NNLS+ (Algorithm 1) for k ≥ 1.
For f deﬁned in (2.2), deﬁne x(cid:63) ∈ argminx≥0 f (x). Then, for all K ≥ 2, we have

E[(cid:104)∇f ((cid:101)xK), (cid:101)xK − x(cid:63)(cid:105)] ≤

2φ0(x(cid:63))
AK

=

(cid:107)x0 − x(cid:63)(cid:107)2
Λ
AK

.

The expected primal error bound is

E[f ((cid:101)xK) − f (x(cid:63))] ≤

φ0(x(cid:63))
AK

=

(cid:107)x0 − x(cid:63)(cid:107)2
Λ
2AK

.

When K ≥ 5
we have

2 n log n, we have AK ≥

(K− 5

2 n log n)2
36n2

. If φ0(x(cid:63)) ≤ |f (x(cid:63))|, then for K ≥ 5

2 n log n + 6n√
ε ,

The total cost is O(cid:0)nnz(A)(cid:0) log n + 1√
ε

(cid:1)(cid:1).

E[f ((cid:101)xK) − f (x(cid:63))] ≤ (cid:15)|f (x(cid:63))|.

Proof. Observe that, by the choice of step sizes, n2a2
bound in Lemma 3.3 and combining with Lemma 3.4, we have

k−1 ≤ Ak−2

4

, ∀k ≥ 3. Thus, telescoping the

E[AKGK(u, v)] ≤φ0(u) −
AK−1
4

−

E[(cid:107)v − yK(cid:107)2

AK
2
E[(cid:107)yK − yK−1(cid:107)2

1
2
2] + n2a1

2] −

E[(cid:107)u − xK(cid:107)2

Λ] − aKE[(cid:104)A(u − xK), yK − yK−1(cid:105)]

2E[(cid:107)y1 − y0(cid:107)2

2] − E[φ0(x1)].

(A.19)
We ﬁrst show how to cancel out the inner product term with the negative quadratic terms. Observe
that, ∀β > 0,

−aK(cid:104)A(u − xK), yK − yK−1(cid:105) = − aK

n
(cid:88)

j=1

(yK − yK−1)(cid:62)A:j(uj − [xK]j)

≤aK

(cid:16) nβ
2

(cid:107)yK − yK−1(cid:107)2

2 +

(cid:107)u − xK(cid:107)2
Λ

(cid:17)

,

1
2β

where the last line is by Young’s inequality. In particular, choosing β = 2aK, we have 1
na2
1
2 (cid:107)x1 − x0(cid:107)2

2 aKnβ =
, by the choice of step sizes in SI-NNLS+. Thus, since φ0(x1) =

K, which is at most AK−1

Λ, Eq. (A.19) simpliﬁes to

4

AK
2

E[(cid:107)v − yK(cid:107)2

E[AKGK(u, v)] ≤ φ0(u) −

2] − E[φ0(x1)].
(A.20)
Since − 1
2 − φ0(x1). By
4
deﬁnition of x1 and φ1, we have x1 = x0 − a1Λ−1(A(cid:62)y0 − 1). Further, from Eq. (3.5) and Eq. (3.1),
as we have y1 = Ax1 and y0 = Ax0, we can simplify the terms to bound as follows.

Λ] ≤ 0, we can ignore it. Let us now bound n2a1

2E[(cid:107)y1 − y0(cid:107)2

E[(cid:107)u − xK(cid:107)2

E[(cid:107)u − xK(cid:107)2

2(cid:107)y1 − y0(cid:107)2

Λ] + n2a1

2] −

1
4

n2a2

1(cid:107)y1 − y0(cid:107)2

2 − φ0(x1) = a1

≤ a1

≤ a1

2(cid:16)

2(cid:16)

n2a1

n2a1

2 −

2(cid:107)AΛ−1(A(cid:62)y0 − 1)(cid:107)2

1
2
2(cid:107)Λ−1/2A(cid:62)AΛ−1/2(cid:107)2(cid:107)Λ− 1
2 (A(cid:62)y0 − 1)(cid:107)2
(cid:17)
(cid:16)

(cid:107)Λ−1(A(cid:62)y0 − 1)(cid:107)2
Λ
1
2

2 −

(cid:17)

2(cid:107)Λ− 1

2 (A(cid:62)y0 − 1)(cid:107)2
2

n3a1

2 −

1
2

,

(cid:107)Λ− 1

2 (A(cid:62)y0 − 1)(cid:107)2
2

(cid:17)

24

where the reasoning behind the ﬁrst inequality follows from the deﬁnition of spectral norm and that
(cid:107)AΛ−1/2(cid:107)2
2 = λmax(Λ−1/2A(cid:62)AΛ−1/2); the last inequality follows as the matrix Λ−1/2A(cid:62)AΛ−1/2
has all ones on the main diagonal, and thus its trace is at most n, and since it is positive semideﬁnite,
its spectral norm is at most its trace. As a1 = 1√
2−φ0(x1) ≤ 0.
Thus, Eq. (A.20) simpliﬁes to

2n1.5 , we conclude that n2a1

2(cid:107)y1−y0(cid:107)2

E[AKGK(u, v)] ≤ E[φ0(u)] −

AK
2

E[(cid:107)v − yK(cid:107)2
2].

(A.21)

By construction, Gapu,v
A(cid:101)xK, we have that f ((cid:101)xK) − f (x(cid:63)) ≤ Gapu,v
that

L ((cid:101)xK, (cid:101)yK) ≤ GK(u, v). Further, using Inequality (2.6), for u = x(cid:63), v =
L ((cid:101)xK, (cid:101)yK). Hence, we can conclude from Eq. (A.21)

E[f ((cid:101)xK) − f (x(cid:63))] ≤

φ0(x(cid:63))
AK

=

.

(cid:107)x0 − x(cid:63)(cid:107)2
Λ
2AK
L ((cid:101)xK, (cid:101)yK) ≥ 0, and, recalling from

(A.22)

On the other hand, for u = x(cid:63) and v = y(cid:63) = Ax(cid:63), Gapu,v
Eq. (3.1), Eq. (3.4), and Eq. (3.5) that yK = A(cid:101)xK, we can also conclude from Eq. (A.21) that

E

(cid:104) 1
2

(cid:107)A((cid:101)xK − x(cid:63))(cid:107)2

2

(cid:105)

≤

φ0(x(cid:63))
AK

=

(cid:107)x0 − x(cid:63)(cid:107)2
Λ
2AK

.

(A.23)

By Proposition 2.1 (b), f (x(cid:63)) = − 1
∀x,

2 1(cid:62)x(cid:63) = − 1

2 (cid:107)Ax(cid:63)(cid:107)2

2. Using this identity, one can verify that,

f (x) − f (x(cid:63)) +

1
2

(cid:107)A(x − x(cid:63))(cid:107)2

2 = (cid:104)AT Ax − 1, x − x(cid:63)(cid:105)
= (cid:104)∇f (x), x − x(cid:63)(cid:105).

Hence, summing Eq. (A.22) and Eq. (A.23), we also have

E[(cid:104)∇f ((cid:101)xK), (cid:101)xK − x(cid:63)(cid:105)] ≤

2φ0(x(cid:63))
AK

=

(cid:107)x0 − x(cid:63)(cid:107)2
Λ
AK

.

Finally, the bound on the rate of growth of Ak is provided in Appendix A.3.

A.3 Omitted Proofs from Section 3: Growth of Scalar Sequences

In this section, we use the properties of {ai} and {ak
i } to obtain our claimed rate of growth of Ak.
Note that in any iteration k ≥ 2 of Algorithm 1, there are two possible updates to ak, which we
name as follows.

Type I update: ak+1 =

Type II update: ak+1 =

nak
n − 1
√
Ak
2n

(A.24)

(A.25)

Obtaining a handle on the growth rate of Ak requires controlling the number of updates of both
types speciﬁed above. At a high level, the idea behind obtaining such a bound is that if the
algorithm had only Type II updates, we would have Ak ≥ Ω( k2
n2 ); we then go on to show that we
cannot have more than 5
2 n log n Type I updates since those make ak grow too fast. We formalize
this intuition in the following lemmas.

Lemma A.1. In Algorithm 1, we have, for k ≥ 2, that ak+1 ≥ ak and Ak+1 > Ak.

25

√

Proof. Notice that for all k, we have ak > 0, which implies that Ak+1
Ak+1 > Ak. To check the non-decreasing nature of ak, we recall that ak+1 = min

def= Ak + ak+1 satisﬁes
(cid:16) nak
(cid:17)
.
n−1 ,

Ak
2n

√

Ak
2n ≥ nak

n−1 , we have ak+1 = nak

n−1 > ak, as claimed. Consider the other case
In the case that
√
Ak
2n , and suppose, for the sake of contradiction, that ak+1 < ak. Chaining this
with ak+1 =
inequality with the assumed expression for ak+1, scaling appropriately, and squaring both sides
gives Ak < 4n2a2
k. Plugging this into Ak = Ak−1 + ak and solving for ak from this quadratic
Ak−1
.
2n

inequality (and further invoking the nonnegativity of ak), yields ak >
(cid:18)

1+16n2Ak−1

8n2

√

√

√

√

>

(cid:19)

1+

However, this contradicts ak = min

nak−1
n−1 ,

Ak−1
2n

≤

Ak−1
2n

.

2

√

c1n2 for c1 = 36.

c1n2 and ask ≥ k−1

Lemma A.2. Consider the iterations {sk} in which Algorithm 1 performs a Type II update ask+1 =
√
Ask
2n . Then we have Ask ≥ k2
Proof. We prove this claim by induction. First, notice that sk ≥ k + 1 for any k. Recall our
1√
2n1.5 . By combining this with the monotonicity property stated in
initialization a1 = A1 =
Lemma A.1, we have as1 ≥ a2 = 1√
2n2.5 ≥ 0. By using Lemma A.1 again, we have, in a similar
2n1.5 ≥ 1
2n2.5 + 1√
fashion, that As1 ≥ A2 = 1√
c1n2 , which proves the base case for induction. Assume
that for some k > 1, we have the induction hypothesis Ask ≥ k2
c1n2 . Then,
combining the monotonicity of Ak from Lemma A.1 with the fact that the algorithm performs
k
c1n2 . By again applying
a Type II update on ask , we have ask+1 =
monotonicity of Ak and the induction hypothesis about ak, we have Ask+1 = Ask+1−1 + ask+1 ≥
Ask + ask+1 ≥ 2k2+

c1n2 and ask ≥ k−1

Ask+1−1
2n

Ask
2n ≥

2c1n2 > (k+1)2
c1n2 .

c1k

√

√

≥

√

√

√

2

2

Lemma A.3. If at some kth
0

iteration of Algorithm 1, we have that

ak0 >

n − 1
c1n2 ; Ak0 ≥
√
2

1
c1

then for all k ≥ k0, we have that

ak ≥

for c1 = 36.

k − 1 − k0 + n

√

2

c1n2

; Ak ≥

(k − k0 + n)2
c1n2

(A.26)

(A.27)

Proof. We prove the claim by induction. First, the base case is true for k = k0 by our assumption
on ak0 and Ak0. Assume the induction hypothesis ak ≥ k−1−k0+n
for k ≥ k0.
c1n2
We now discuss how ak changes with the two types of updates.

and Ak ≥ (k−k0+n)2

c1n2

√

2

If the algorithm performs a Type I update on ak, then, by deﬁnition, ak+1 = nak

n−1 . Now applying

the assumed lower bound on ak, we have, when k > k0, that

ak+1 =

nak
n − 1

≥

k − 1 − k0 + n
√
c1n(n − 1)
2

≥

k − k0 + n

√

2

c1n2

.

Similarly, given that Ak ≥ (k−k0+n)2

c1n2

, we have,

Ak+1 = Ak + ak+1 ≥

(k − k0 + n)2
c1n2

+

26

k − k0 + n

√
2

c1n2 ≥

(k + 1 − k0 + n)2
c1n2

.

If, on the other hand, the algorithm performs a Type II update on ak, then we have

ak+1 =

√

Ak
2n

≥

This completes the induction.

k − k0 + n

√
2

c1n2

.

As we saw in Lemma A.3, after the kth

grows fast. We therefore need to estimate the number of Type I updates before the kth

0 iteration - starting at which Inequality (A.26) holds - Ak
0 iteration.

Lemma A.4. There are at most 3
kth
0

iteration (the ﬁrst iteration at which Inequality (A.26) holds).

2 n log n Type I updates (Equation (A.24)) performed before the

Proof. Suppose there are n1 Type I updates performed by Algorithm 1 before the kth
iteration,
0
when Inequality (A.26) starts to hold. Further, by Lemma A.1, ak is monotonically increasing
(for both types of updates). Then, when considering Type I updates (Equation (A.24)), we have
ak0 ≥
12n2 , we only need to have n1 >
. In a similar fashion, combining the monotonicity of Ak from Lemma A.1 with

In order for ak0 > n−1

(cid:16) n
n−1
(cid:16) √

a2 =
(cid:17)

1√
2n2.5 .

(cid:16) n
n−1

(cid:17)n1

(cid:17)n1

·

log n
n−1
the Type I update rule, we have

n(n−1)
√
2
6

(cid:32)

Ak0 ≥ a2

1 +

n
n − 1

+

(cid:18) n

(cid:19)2

n − 1

+ · · · +

(cid:18) n

(cid:19)n1(cid:33)

n − 1

>

( n
n−1 )n1
√
2n2.5

.

So, in order to have Ak0 > 1
).
By using the approximation 1 + x ≤ ex and combining the above two bounds, we get as soon as
n1 ≥ 3

36 per Inequality (A.26), we only need to have n1 ≥ log n

2 n log n, the inequality (A.26) holds.

n−1

( n2.5
√
2
18

Proposition A.4. [Rate of change of Ak] When k ≥ 5

2 n log n, we have Ak ≥

(k− 5

2 n log n)2
36n2

.

2 n log n. By the result of Lemma A.2, we must have Ak0 ≥ t2

Proof. Let there be t1 Type I updates and t2 Type II updates before the ﬁrst iteration at which
Inequality (A.26) holds, and let us call this iteration k0. By the result of Lemma A.4, we have
t1 ≤ 3
c1n2 . To meet
the requirement in Inequality (A.26) then, we can see that t2 ≤ n. Therefore, k0 = t1 + t2 ≤
2 n log n + n ≤ 5
3
0 iteration, the result of Lemma A.3 applies, and we
have Ak ≥ (k−k0)2
c1n2

2 n log n. Having reached the kth

c1n2 and ak0 ≥ t2−1

√

2

.

2

B Omitted Proofs from Section 4: Restart Strategy

Proposition 4.1. Problem (P) is equivalent to the following linear complementarity problem, de-
noted by LCP(M, q).

Mx + q ≥ 0, x ≥ 0, (cid:104)x, Mx + q(cid:105) = 0,

(4.2)

where Λ−1M = A(cid:62)A and q = −Λ−11.

Proof. Observe ﬁrst that, as Λ−1 is a diagonal matrix with positive elements on the diagonal, the
stated linear complementarity problem is equivalent to

∇f (x) ≥ 0, x ≥ 0, (cid:104)∇f (x), x(cid:105) = 0.

(B.1)

27

By Proposition 2.1, these conditions hold for any solution of (P). In the opposite direction, suppose
that the conditions from Eq. (B.1) hold for some x. Then applying these conditions for any u ≥ 0
gives

(cid:104)∇f (x), u − x(cid:105) = (cid:104)∇f (x), u(cid:105) ≥ 0.

But (cid:104)∇f (x), u − x(cid:105) ≥ 0 is the ﬁrst-order optimality condition for (P), and so x solves (P).

Proposition B.1. For any x ∈ Rn

+, r(x) ≤ (cid:112)2n(f (x) − f (x(cid:63))), where x(cid:63) ∈ argminu∈Rn

f (u).

+

Proof. Given x ∈ Rn
(cid:107)A:j(cid:107)2, and ˆxj = xj for j (cid:54)= j(cid:63). Then observing that

+, consider ˆx deﬁned as ˆxj(cid:63) = xj(cid:63) − Rj(cid:63)(x), where j(cid:63) = argmax1≤j≤n |Rj(x)| ·

f (ˆx) − f (x) = ∇j(cid:63)f (x)([ˆx]j(cid:63) − [x]j(cid:63)) +

|[ˆx]j(cid:63) − [x]j(cid:63)|2

≤ −

1
2

|Rj(cid:63)(x)|2(cid:107)A:j(cid:63)(cid:107)2

2 ≤ −

(cid:107)A:j(cid:63)(cid:107)2
2
2
1
(cid:107)R(x)(cid:107)2
Λ,
2n

and combining with f (ˆx) ≥ f (x(cid:63)), r(x) = (cid:107)R(x)(cid:107)Λ, the claimed bound follows after a simple
rearrangement.

Theorem 4.2. Given an error parameter (cid:15) > 0 and x0 = 0, consider the following algorithm A :

A : SI-NNLS+ with Restarts
Initialize: k = 1.
Initialize Lazy SI-NNLS+ at xk−1.
K satisﬁes r((cid:101)xk
Run Lazy SI-NNLS+ until the output (cid:101)xk
Restart Lazy SI-NNLS+ initializing at xk = (cid:101)xk
K.
Increment k.
Repeat until r((cid:101)xk

K) ≤ (cid:15).

K) ≤ 1

2 r(xk−1).

Then, the total expected number of arithmetic operations of A is at most

(cid:16)

O

(cid:16)

nnz(A)

log n +

(cid:17)

√

n
µ

log

(cid:16) r(x0)
(cid:15)

(cid:17)(cid:17)

.

As a consequence, given ¯(cid:15) > 0, the total expected number of arithmetic operations until a point with
f (x) − f (x(cid:63)) ≤ ¯(cid:15)|f (x(cid:63))| can be constructed by A is bounded by

(cid:16)

O

(cid:16)

nnz(A)

log n +

(cid:17)

√

n
µ

log

(cid:17)(cid:17)

.

(cid:16) n
µ¯(cid:15)

K) ≤ (cid:15) is bounded by log( r(x0)

Proof. Because each restart halves the natural residual r(x), it is immediate that the total number
of restarts until r((cid:101)xk
). Thus, to prove the ﬁrst (and main) part of the
theorem, we only need to bound the number of iterations (and the overall number of arithmetic
operations) of (Lazy) SI-NNLS+ in expectation. Hence, in the following, we only consider one run
of SI-NNLS+ until the natural residual is halved. To keep the notation simple, we let x0 denote the
initial point of SI-NNLS+ and (cid:101)xk denote the output of SI-NNLS+ at iteration k. If r(x0) = 0, A
halts immediately and the bound on the number of iterations holds trivially, so assume r(x0) > 0.
Using Theorem 3.1, we have that ∀k ≥ 2,

(cid:15)

E[Akr2((cid:101)xk)] ≤ n(cid:107)x0 − x(cid:63)(cid:107)2

Λ ≤

n
µ2 r2(x0).

(B.2)

28

As r2(·) is nonnegative, we can use Markov’s inequality to bound the total number of iterations
K until r((cid:101)xK) ≤ r(x0)
. In particular, using Eq. (B.2), we get by Markov’s inequality that Pr[K >
k] ≤ Pr[r2((cid:101)xk) ≥ r2(x0)
] ≤ 4n
µ2Ak

. As K is nonnegative, we can estimate its expectation using

2

4

E[K] =

∞
(cid:88)

i=0

Pr[K > i] ≤

∞
(cid:88)

i=0

(cid:110)

1,

min

(cid:111)

4n
µ2Ai

√

(cid:100)12n

≤

n+ 5

2 n log n/µ(cid:101)

(cid:88)

1 +

∞
(cid:88)

i=0

√

(cid:100)12n

n/µ+ 5

2 n log n(cid:101)+1

√

≤ 24n

n/µ +

5
2

n log n + 2,

4n
µ2Ai

where in the last inequality we use the rate of Ak from Proposition A.4.

In the lazy implementation of SI-NNLS+, as argued in Appendix C, the expected cost of an
, which leads to the claimed bound on the number of arithmetic operations until

By using that r(x0) ≤ (cid:112)2n(f (x0) − f (x(cid:63))) = (cid:112)2n|f (x(cid:63))|, f (cid:0)

(cid:1)r2((cid:101)xK1) (argued below), the bound on the number of iterations until f (cid:0)

(cid:101)xK1 − R((cid:101)xK1)(cid:1) − f (x(cid:63)) ≤ (cid:0)(n −
(cid:101)xK1 − R((cid:101)xK1)(cid:1) −

iteration is nnz(A)
r(x) ≤ (cid:15).

n

1) + n+1
µ
f (x(cid:63)) ≤ ¯(cid:15)|f (x(cid:63))| have

f (cid:0)

(cid:101)xK1 − R((cid:101)xK1)(cid:1) − f (x(cid:63)) ≤ (cid:0)(n − 1) +
≤ (cid:0)(n − 1) +

n + 1
µ
n + 1
µ
n + 1
µ

(cid:1)r2((cid:101)xK1)
(cid:1) 1
22K1
(cid:1) 1
22K1

r2(x0)

2n|f (x(cid:63))|

≤ (cid:0)(n − 1) +

(cid:1)

, we have this bound.

and by setting K1 = 1

2n(cid:0)(n−1)+ n+1
¯(cid:15)
Finally, it remains to argue that f (cid:0)

2 log2

µ

that the deﬁnition of R(x) is equivalent to x − ¯x, where

(cid:101)xK1 − R((cid:101)xK1)(cid:1) − f (x(cid:63)) ≤ (cid:0)(n − 1) + n+1

µ

(cid:1)r2((cid:101)xK1). Observe

¯x = argmin

u∈Rn
+

(cid:110)

(cid:104)∇f (x), u − x(cid:105) +

(cid:107)u − x(cid:107)2
Λ

(cid:111)
.

1
2

By the ﬁrst-order optimality of ¯x based on the equivalent deﬁnition of R(x) above, we have (cid:104)∇f (x)+
Λ(¯x − x), x(cid:63) − ¯x(cid:105) ≥ 0. Rearranging, and using the deﬁnition of convexity of f, we have

f (¯x) − f (x(cid:63)) ≤ (cid:104)∇f (¯x), ¯x − x(cid:63)(cid:105)

≤ (cid:104)∇f (x) − ∇f (¯x) + Λ(¯x − x), x(cid:63) − ¯x(cid:105)
= (cid:104)(A(cid:62)A − Λ)(x − ¯x), x(cid:63) − ¯x(cid:105)
= (cid:104)(A(cid:62)A − Λ)R(x), R(x)(cid:105) + (cid:104)(A(cid:62)A − Λ)R(x), x(cid:63) − x(cid:105)
= (cid:104)(A(cid:62)A − Λ)R(x), R(x)(cid:105) + (cid:104)A(cid:62)AR(x), x(cid:63) − x(cid:105) − (cid:104)ΛR(x), x(cid:63) − x(cid:105)
≤ (n − 1)(cid:107)R(x)(cid:107)2

Λ + (n + 1)(cid:107)R(x)(cid:107)Λ(cid:107)x − x(cid:63)(cid:107)Λ

(cid:16)

≤

(n − 1) +

(cid:17)

n + 1
µ

r2(x),

where in the last inequality we have used the error bound from Theorem 4.1.

29

C Implementation Version of SI-NNLS+

Since Algorithm 1 explicitly updates (cid:101)xk and (cid:101)yk (of lengths n and m respectively), the per iteration
cost is O(m + n), which is unnecessarily high when the matrix A is sparse. In this section, we
show that by using a lazy update strategy, we can eﬃciently implement Algorithm 1 with overall
complexity independent of the ambient dimension. To attain this result, we maintain implicit
representations for (cid:101)xk, yk, and ¯yk by introducing two auxiliary variables that are amenable to
eﬃcient updates.

Eﬃciently Updating the Primal Variable.
an implicit representation of (cid:101)xk by introducing rk.
Lemma C.1. For {(cid:101)xk} deﬁned in Eq. (3.4) (and simpliﬁed in Algorithm 1), we have, for k ≥ 1,

In Lemma C.1, we show that we can work with

(cid:101)xk = xk +

1
Ak

rk,

(C.1)

where xk evolves as per Algorithm 1, r1 = 0 and, when k ≥ 1, rk = rk−1 + ((n − 1)ak − Ak−1)(xk −
xk−1).

Proof. We prove the lemma by induction. Using the facts that x0 = 0, x1 = (cid:101)x1, s1 = 0, a1 = A1,
and A0 = 0, we have

(cid:101)x1 =

=

1
A1
1
A1

(cid:0)A0(cid:101)x0 + a1

(cid:0)nx1 − (n − 1)x0

(cid:1)(cid:1)

(a1x1 + (n − 1)a1(x1 − x0))

= x1 + ((n − 1)a1 − A0)(x1 − x0).

(C.2)

Assume for certain k ≥ 2, that Eq. (C.1) holds for k − 1. Then, using the recursion of ˜xk in

Algorithm 1, we have that for k ≥ 3,

Ak(cid:101)xk = Ak−1(cid:101)xk−1 + akxk + (n − 1)ak(xk − xk−1)

= Ak−1xk−1 + rk−1 + akxk + (n − 1)ak(xk − xk−1)
= Ak−1(xk−1 − xk + xk) + rk−1 + akxk + (n − 1)ak(xk − xk−1)
= Ak−1(xk−1 − xk) + Ak−1xk + rk−1 + akxk + (n − 1)ak(xk − xk−1)
= Akxk + rk−1 + ((n − 1)ak − Ak−1)(xk − xk−1)
= Akxk + rk,

as required.

The expression for rk in Lemma C.1 shows that it can be updated at cost O(1) as xk diﬀers
from xk−1 only at one coordinate. Therefore, by Eq. (C.1) we need not compute ˜xk in all iterations
and can instead maintain rk. Along the same lines, we give an eﬃcient implementation strategy
for yk and ¯yk in the following discussion.

Eﬃciently Updating the Dual Variable. We now show how to update the dual variable
eﬃciently.

30

Lemma C.2. Consider {yk} and {xk} evolving as per Algorithm 1. Then, for k = 1, we have
y1 = Ax1; for k ≥ 2, we have

yk =

Ak−1
Ak

yk−1 +

ak
Ak

Axk +

(n − 1)ak
Ak

A(xk − xk−1),

(C.3)

(C.4)

Proof. The proof is directly from the deﬁnition of yk in Algorithm 1.

Lemma C.3. Consider {yk} and {xk} evolving as per Algorithm 1. Then for all k ≥ 1, we have

yk = Axk +

1
Ak

sk,

(C.5)

where s1 = 0 and sk = sk−1 + ((n − 1)ak − Ak−1)A(xk − xk−1) when k ≥ 2.

Proof. We prove the lemma by induction. For the base case of k = 1, we have, by the choice of
s1 = 0, that y1 = Ax1 = Ax1 + 1
s1. Then for some k ≥ 2, assume Eq. (C.5) holds for k − 1, then
A1
we have,

Akyk = Ak−1yk−1 + akAxk + (n − 1)akA(xk − xk−1)

= Ak−1Axk−1 + sk−1 + akAxk + (n − 1)akA(xk − xk−1)
= Ak−1A(xk−1 − xk + xk) + sk−1 + akAxk + (n − 1)akA(xk − xk−1)
= AkAxk + sk−1 + ((n − 1)ak − Ak−1)A(xk − xk−1)
= AkAxk + sk,

(C.6)

where the ﬁrst step is by Lemma C.2, second step is by the induction hypothesis, third step is
by adding and subtracting Ak−1Axk, fourth step is by rearranging terms appropriately, and the
ﬁnal step uses the recursive deﬁnition of sk stated in the lemma. Dividing throughout by Ak then
ﬁnishes the proof.

Lemma C.4. Consider {xk}, {yk}, and {yk} evolving as per Algorithm 1. Then we have that

¯y1 = Ax1 +

a1
a2

A(x1 − x0).

(C.7)

and

¯yk = Axk +

(cid:16)

1
Ak

1 −

a2
k
ak+1Ak−1

(cid:17)

sk +

(n − 1)a2
k
ak+1Ak−1

A(xk − xk−1).

(C.8)

Proof. From the deﬁnition of ¯yk, the initializations for x0, y0, and y0, and Lemma C.2, we have

¯y1 = y1 +

a1
a2

(y1 − y0) = Ax1 +

a1
a2

A(x1 − x0).

For k ≥ 2, by Lemma C.2, we have

Akyk − Ak−1yk−1 = akAxk + (n − 1)akA(xk − xk−1).

As a result,

Ak−1(yk − yk−1) = akAxk + (n − 1)akA(xk − xk−1) − akyk.

(C.9)

31

Algorithm 2 SI-NNLS+ (Implementation)

+

Input: Matrix A ∈ Rm×n
Output: Vector (cid:101)xK ∈ Rn
Initialize: a1 = 1
Ax0, t0 = 0, s1 = 0, r1 = 0.
for k = 1, 2, . . . , K do

n−1 , a2 = n

with n ≥ 4, accuracy (cid:15)

+ such that f ((cid:101)xK) ≤ (1 + (cid:15))|f (x(cid:63))|.

n−1 , A1 = a1, φ0(x) = 1

2 (cid:107)x − x0(cid:107)2

Λ, y0 = y0 = Ax0, p0 = 0, q0 =

Sample jk uniformly at random from {1, 2, . . . , n}
if k = 1 then

¯y0 = q0

else if k = 2 then
¯y1 = q1 + a1
a2
else if k ≥ 3 then

t1

¯yk−1 = qk−1 + 1

Ak−1

(cid:16)

1 −

(cid:17)

a2
k−1
akAk−2

sk−1 +

(n−1)a2
akAk−2

k−1

tk−1

end if

(cid:40)

pk,i =

xk,i =

(cid:0)AT

pk−1,i,
pk−1,i + nak
xk−1,i,
max (cid:8)0, min (cid:8)x0,i − 1

:i ¯yk−1 − 1(cid:1),

(cid:40)

i (cid:54)= jk
i = jk.

(cid:107)A:i(cid:107)2 · pk,i,

1
(cid:107)A:i(cid:107)2

(cid:9)(cid:9),

i (cid:54)= jk
i = jk

tk = A(xk − xk−1)
if k ≥ 2 then

rk = rk−1 + ((n − 1)ak − Ak−1)(xk − xk−1)
sk = sk−1 + ((n − 1)ak − Ak−1)tk

end if
qk = qk−1 + tk
Ak+1 = Ak + ak+1
ak+2 = min{ nak+1
n−1 ,

end for
return xK + 1
AK

rK

√

Ak+1
2n

}

So for k ≥ 2, it follows that

(yk − yk−1)

¯yk = yk +

= yk +

(cid:16)

=

1 −

= Axk +

ak
ak+1
ak
ak+1

(cid:16) ak

Ak−1
(cid:17)

a2
k
ak+1Ak−1
(cid:16)

1 −

1
Ak

Axk +

(n − 1)ak
Ak−1
a2
k
ak+1Ak−1
(cid:17)

sk +

yk +

Axk +

A(xk − xk−1) −

ak
Ak−1

(cid:17)

yk

(n − 1)a2
k
ak+1Ak−1

A(xk − xk−1)

a2
k
ak+1Ak−1

(n − 1)a2
k
ak+1Ak−1

A(xk − xk−1),

where the ﬁrst step is by the deﬁnition of yk in Algorithm 1, the second step is by Eq. (C.9), the
third step is by rearranging, and the ﬁnal step is by Lemma C.3.

Based on the above lemmas, we give our eﬃcient lazy implementation version of Algorithm 1
in Algorithm 2. In Algorithm 2, we also introduce other auxiliary variables pk, qk and tk. Based
on Lemmas C.1-C.4, it is easy to verify the equivalence between Algorithms 1 and 2. With this

32

implementation, by updating only the dual coordinates corresponding to the nonzero coordinates of
the selected column of A, the per-iteration cost is proportional to the number of nonzero elements
of the selected row in the iteration. As a result, the overall complexity result will depend only on
the number of nonzero elements of A.

33

