Nemo: Guiding and Contextualizing Weak Supervision
for Interactive Data Programming
Technical Report

Cheng-Yu Hsieh
University of Washington
cydhsieh@cs.washington.edu

Jieyu Zhang
University of Washington
jieyuz2@cs.washington.edu

Alexander Ratner
University of Washington
Snorkel AI, Inc.
ajratner@cs.washington.edu

2
2
0
2

r
a

M
3
2

]

G
L
.
s
c
[

2
v
2
8
3
1
0
.
3
0
2
2
:
v
i
X
r
a

ABSTRACT
Weak Supervision (WS) techniques allow users to efficiently cre-
ate large training datasets by programmatically labeling data with
heuristic sources of supervision. While the success of WS relies
heavily on the provided labeling heuristics, the process of how these
heuristics are created in practice has remained under-explored. In
this work, we formalize the development process of labeling heuris-
tics as an interactive procedure, built around the existing workflow
where users draw ideas from a selected set of development data for
designing the heuristic sources. With the formalism, we study two
core problems of how to strategically select the development data
to guide users in efficiently creating informative heuristics, and how
to exploit the information within the development process to con-
textualize and better learn from the resultant heuristics (Figure 1).
Building upon two novel methodologies that effectively tackle the
respective problems considered, we present Nemo, an end-to-end
interactive system that improves the overall productivity of WS
learning pipeline by an average 20% (and up to 47% in one task)
compared to the prevailing WS approach.

1 INTRODUCTION
Manual labeling and curation of training datasets has increasingly
become one of the major bottlenecks when deploying modern ma-
chine learning models in practice. In response, recent weak su-
pervision approaches, wherein cheaper but noisier forms of labels
are used, have received increasing research and industry attention.
In one recent paradigm for weak supervision, data programming
(DP) [24], users are able to quickly create and manage large train-
ing datasets by encoding domain knowledge and labeling heuris-
tics into a set of labeling functions (LFs). Each of these functions,
serving as a weak supervision source, programmatically annotates
a subset of data points with possibly noisy weak labels. As the
LFs may have varying accuracies, different LFs could suggest con-
flicting votes on certain data points. As a result, researchers have
developed various modeling techniques to denoise and aggregate
the weak labels, provided by different weak supervision sources,
into probabilistic training labels that can be utilized to train down-
stream models [11, 24, 25]. Despite its recent emergence, the DP
paradigm has powered many industrial-scale systems to efficiently
build and maintain machine learning applications across various
domains [4, 7, 23, 26].

In leveraging the DP paradigm, the crucial—yet understudied—
first step is to develop a set of LFs as the weak supervision sources. In
practice, users typically look at a small set of data samples selected
from the unlabeled dataset, called the development data, to draw

Figure 1: Interactive Data Programming (IDP) considers the
entire data programming (DP) pipeline as an iterative cy-
cle with two core problems of (1) strategic data selection
and (2) contextualized LF modeling. The blue-colored com-
ponents/steps in IDP are typically neglected in existing DP
pipeline (the black-colored sub-procedures) focusing more
exclusively on modeling and learning from a given set of
LFs.

ideas for writing LFs [23]. As a result of this common workflow,
we especially note that the LFs developed are directly affected by
and biased with respect to the data seen by the users during the
development process. Particularly, an LF created with heuristics
extracted from certain development data is more likely to generalize,
or to provide labels, to those examples possessing similar patterns
as the development data. In addition, among the covered examples,
the LF may also be expected to perform more accurately on those
examples within closer proximity to the development data, whereas
having higher possibility of over-generalizing (to provide wrong
labels) on the examples that lie in data subspaces further away from
the development data (shown in Figure 2).

Example 1.1. Consider a sentiment classification task on product
reviews from various categories, as illustrated in Figure 3. First, by
looking at reviews from a certain category, users would more likely
create LFs that cover reviews from the same category. For instance,
by looking at “Food” product reviews, potential LFs extracted such
as “delicious → positive” are more likely to generalize to other “Food”
product reviews than to “Electronics” product reviews. Second, an LF
created from a certain category may be expected to be more accurate
for reviews in the same or similar categories. For instance, by looking at
reviews from the “Movie” category, users may find “funny → positive”
an useful LF. However, the LF appears less accurate for reviews from
“Food” category, since “funny” could indicate negative sentiment when
associated with taste/food.

 
 
 
 
 
 
While the data samples seen in the LF development process
directly impact the resultant set of LFs created and provide valu-
able contextual information about LFs, existing work on DP have
largely considered the entire LF development process as an exoge-
nous black-box to the DP learning pipeline [11, 23–25]. The lack of
attention on the LF development process thus poses three major
limitations to the current DP paradigm:

• Under-formalized LF Development Workflow: The lack
of formalism on the LF development process has obscured
systematic study to optimize the workflow, making it less
organized and more challenging for practitioners to design
LFs for DP applications [6, 12, 33, 35].

• Inefficient Development Data Selection: Current LF de-
velopment workflow selects development data with the most
straightforward approach, uniform random sampling, which
unfortunately can be time-consuming as it oftentimes re-
quires users to inspect a considerable amount of data samples
to create an informative set of LFs.

• Dropped Data-to-LF Lineage: The development context
within which the LFs were developed is neglected, i.e., from
which development data an LF was created, leaving behind
valuable information about the LFs’ expected accuracies in
different data subspaces.

In this work, we make three corresponding hypotheses to tackle
the limitations and improve the productivity of DP learning pipeline:
• Formalism of LF development process could allow sys-
tematic study and optimization of DP workflow. By
framing the LF development process formally with clear
objective, we would be able to study and improve the de-
velopment process, making DP paradigm more efficient and
effective.

• Strategic development data selection could lead to more
efficient DP pipeline. By strategically selecting the devel-
opment data to be used by the users for writing LFs, instead
of random sampling, we could ideally guide the users in effi-
ciently creating a set of LFs that provide the most informative
supervision signals for learning the subsequent models.
• Exploitation of LF lineage to development data could
enable more effective DP learning. By tracking the LF
lineages to the development data, we could ideally exploit
this information to contextualize where the LFs are expected
to perform best, and accordingly model the data-conditional
accuracies of the LFs for more effective denoising.

Accordingly, we then make the following technical contributions

that positively verify each of the above hypotheses:
Formalism of LF Development and First End-to-End System
Solution: We formalize a broader DP paradigm, Interactive Data
Programming (IDP), that explicitly considers LF development as one
central and interactive component in the entire learning workflow.
As illustrated in Figure 1, we formulate the entire DP pipeline as an
iterative human-in-the-loop cycle. In each iteration, the user creates
new LFs based on the selected development data, and subsequently
train the downstream model based on the LFs collected so far. The
goal is to train an accurate model for the target task in as few
iterations as possible. Towards the goal, we focus on studying two
core problems: (1) how to intelligently select development data so

Cheng-Yu Hsieh, Jieyu Zhang, and Alexander Ratner

Figure 2: LFs generally have higher coverage (left) and ac-
curacy (right) on the data subspace within closer proximity
to their development data. For each LF, we organize all ex-
amples into 4 subspaces based on the percentile of their dis-
tance to the development data, and compute the LF’s cover-
age/accuracy in the 4 data subspaces. The plots are averaged
results over 100 LFs on Amazon Review Dataset.

Figure 3: Left: A toy sentiment classification dataset with
4 clusters, each corresponding to product reviews from a
category. Right: Looking at development data points (stars),
users are likely to create LFs that generalize to similar ex-
amples. In addition, the LFs may be expected to be more ac-
curate around the development data. Circle/Triangle corre-
sponds to “ground truth” Positive/Negative label. Red/Blue
corresponds to “assigned” Positive/Negative label and Gray
is unlabeled.

as to guide users in developing most useful LFs efficiently, and (2)
how to leverage the LF lineage to development data for facilitating
more effective modeling and learning from the created LFs.

With the problem formalized, we then set out to design Nemo,
the first end-to-end system for IDP which is built on top of two
novel methodologies, which we shall describe shortly, that respec-
tively tackle the data selection and contextualized LF modeling
problems. Through extensive quantitative evaluations and a care-
fully conducted user study, we validate our first hypothesis where
we observe that Nemo offers significant performance lift over the
current prevailing DP system an average 20%. When compared
to other interactive learning schemes such as traditional active
learning, Nemo as well leads to an average 34% performance lift.

Intelligent Development Data Selection Strategy: In building
Nemo, we propose a novel data selection strategy, Select by Expected
Utility (SEU), to efficiently guide users in developing useful LFs. The
key idea within the proposed SEU approach is to first statistically
measure the utilities of the potentially generated LFs, and in turn
select the examples that are expected to lead to high utility (or more
informative) LFs, in which the expectation is calculated through
a proposed user model that measures the conditional probability
of user returning an LF by looking a specific development data
(illustrated in the development data selector in Figure 4). With SEU,

Nemo: Guiding and Contextualizing Weak Supervision for Interactive Data Programming

we validate our second hypothesis and improve the DP performance
an average 16% compared to random sampling baseline.

Contextualized LF Modeling with Data Lineage: In Nemo, we
propose a model-agnostic method that exploits the LF lineage to the
development data to more effectively denoise and learn from the LFs.
Particularly, based on the natural tendency for users to create LFs
that are more precise around the neighborhood of the development
data [3], we propose to refine each LF to be active only within a
certain radius from their corresponding development data points
(illustrated in LF contextualizer in Figure 4). With the method, we
validate our third hypothesis and improve the DP performance an
average 11% compared to the standard learning pipeline without
leveraging the LF development context in modeling the LFs.

2 PRELIMINARIES
We review the standard data programming (DP) setup [23, 24]. Let
each example 𝑥 ∈ X be associated with a corresponding label
𝑦 ∈ Y, where the joint density is governed by some underlying
distribution D. Given a set of 𝑛 unlabeled examples 𝑈 = {𝑥𝑖 }𝑛
𝑖=1,
drawn from distribution D, with their labels {𝑦𝑖 }𝑛
𝑖=1 unobserved,
the standard DP pipeline follows three main stages:

(1) Labeling Function Development Stage: Users encode la-
beling heuristics into a set of 𝑚 labeling functions (LFs),
{𝜆𝑗 }𝑚
𝑗=1, where 𝜆𝑗 : X → Y ∪ {0}. Each LF 𝜆𝑗 could either
provide an example 𝑥𝑖 with a possibly noisy label 𝜆𝑗 (𝑥𝑖 ) ∈ Y
or abstain with 𝜆𝑗 (𝑥𝑖 ) = 0.

(2) Label Denoising/Aggregation Stage: The provided LFs
are individually applied to the unlabeled examples, gener-
ating a label matrix 𝐿 ∈ R𝑛×𝑚 where 𝐿𝑖 𝑗 = 𝜆𝑗 (𝑥𝑖 ). Then, a
label model is learned to aggregate 𝐿, the votes on the train-
ing labels, into a set of probabilistic estimates of the ground
truth labels {𝑃 (𝑦𝑖 |𝐿)}𝑛

(3) End Model Learning Stage: Finally, these probabilistic es-
timates could serve as probabilistic soft labels that can be
used to train a final discriminative model 𝑓 : X → Y that
could generalize and make predictions on unseen examples,
with the overarching goal of minimizing the generalization
error on distribution D.

𝑖=1.

In the LF development stage, users generally refer to some devel-
opment data, sampled from the unlabeled set 𝑈 , to extract labeling
heuristics for writing LFs. As a result of this workflow, the data seen
in the development stage has a direct influence on what LFs would
be created, and on which examples these LFs may be expected to
perform best (see Example 1.1).

Despite the impact that development data have on the resultant
LFs, the problems of how to strategically select these data for guid-
ing LF development and how to potentially exploit this lineage in
modeling the LFs have remained under-explored. In fact, the entire
LF development process has been given scant attention by literature
to date, where the most straightforward but naive random sampling
has been the dominating approach for selecting the development
data, and the modeling of LFs has been blind to their development
context [11, 20, 23–25]. Unfortunately, as discussed in previous
section, this current workflow renders the entire DP pipeline less
efficient and effective.

3 INTERACTIVE DATA PROGRAMMING
To study the impact of LF development process in the DP paradigm,
we formalize a broader DP framework that considers LF develop-
ment as one central, iterative process within the entire DP learning
pipeline. We term this new formalism Interactive Data Program-
ming (IDP), as illustrated in Figure 1. We highlight two novelties in
IDP compared to the standard DP paradigm:

• First, we formalize the LF development stage as a two-step
process where (1) a subset of development data would first
be selected from the unlabeled set, and (2) the users then
develop LFs based on these selected data. While this is an
established workflow in practice, this process has not been
carefully formalized and studied in the literature.

• Second, we consider the LF development stage and the sub-
sequent label/end model learning stage as interleaved steps
in an interactive cycle, rather than a sequential and indepen-
dent procedures. In each iteration, the users develop new
LFs guided by the learning models, and the models in turn
learn from the LFs created where the models are aware of
the context within which the LFs are developed.

The overarching goal of IDP is to achieve highest end model
predictive performance, evaluated with a held-out test dataset, in
as few interactive iterations as possible. This entails that the users
could efficiently design useful weak supervision sources, the LFs,
and effectively train an accurate predictive model from the LFs.
Specifically, we study two main problems towards the goal:

• First, we consider the problem of how to intelligently select
examples to efficiently guide users in writing informative
LFs from which an accurate predicitve model can be learned.
• Second, we consider the problem of how to exploit the LF
development context to effectively model and learn from a
given set of LFs.

Connection to Active Learning. We note that the first problem
studied in IDP is analogous to active learning [29], in the sense
that the goal is to iteratively select data points from an unlabeled
dataset and solicit supervision feedbacks from the user to train an
accurate downstream model, with as few queries to the user as
possible. However, unlike active learning where in each iteration
the user provides supervision in terms of a single label annotation
to the selected data point, the supervision form provided in IDP is
at a higher functional level (i.e., LF) that noisily annotates multiple
data points at a time. The inherent noise in these higher-level weak
supervision source returned in IDP thus creates the unique second
problem, which is not considered in active learning, of exploiting
the selected data points to contextualize the expected accuracy of a
weak supervision source in different data subspaces.
A Subsuming Framework. IDP is an encompassing framework
that subsumes many existing usages of DP in practice [3, 9, 23, 24].
For example, the current widely adopted DP workflow corresponds
the vanilla instantiation of IDP that selects development data with
random sampling and the models the LFs without considering their
development context [23]; the rule-exemplar learning approach
is another instance under IDP that supports half of the IDP loop
where it models the LFs with their development context using the
ImplyLoss model proposed in [3], but does not consider strategically
selecting the development data.

Cheng-Yu Hsieh, Jieyu Zhang, and Alexander Ratner

Figure 4: Nemo system overview. In each iteration, (1) the data selector intelligently picks an example from the unlabeled set
based on current labeling information. (2) The user creates an LF based on the selected example. (3) The LF contextualizer
refines each LF based on their development context, after which the label and end model is learned from the refined LFs.

Setup. Formally, given an unlabaled dataset 𝑈 = {𝑥𝑖 }𝑛
𝑖=1, the pro-
posed IDP framework proceeds iteratively by the following steps:
(1) Development Data Selection Stage: In the 𝑡-th iteration, a
subset of examples 𝑆𝑡 ⊂ 𝑈 are strategically selected from the
unlabeled set and shown to the user to guide development
of LFs that are most informative to the models.

(2) Labeling Function Development Stage: Based on 𝑆𝑡 , the
user writes a set of 𝑘 labeling functions Λ𝑡 = {𝜆𝑡 1, . . . , 𝜆𝑡𝑘 }
which extracts meaningful labeling heuristics encoded in the
examples. The lineage of these LFs to the development data
𝑆𝑡 is tracked and represented as a tuple (Λ𝑡 , 𝑆𝑡 ).

(3) Label/End Model Learning Stage: Given the set of LFs
created so far with their data lineage, {(Λ1, 𝑆1), . . . , (Λ𝑡 , 𝑆𝑡 )},
the label and end models are learned from the LFs as in the
standard DP pipeline, but with additional access to the LFs’
development context. Finally, the current model information
would be passed back to the data selection stage to start the
next cycle; or the iteration stops and the end model is output
for the learning task of interest.

Paper Scope. In the remainder of this paper, we focus on binary
classification tasks where Y = {−1, 1} for ease of exposition. In
addition, we focus on a simplified IDP setting where in each itera-
tion, a single example is selected as the development data (|𝑆𝑡 | = 1),
and the user in return provides a single LF (|Λ𝑡 | = 1). This simpli-
fication regularizes the effort an user spends in each interactive
iteration, and allows us to more fairly study different method’s
performance with the same number of iterations. Finally, we note
the development context of Λ𝑡 includes all previous sequence of
development data the user has seen (𝑆1, . . . , 𝑆𝑡 ). In this work, we
only consider the context window to include the data the user is
currently looking at (i.e., 𝑆𝑡 ), and leave the incorporation of longer
weighted context-sequence as a future direction.

4 NEMO ARCHITECTURE
We present Nemo, the first end-to-end system designed to support
the full IDP loop by tackling the two core IDP problems. We provide
Nemo system overview in Figure 4. Nemo consists of an user-facing

frontend where users develop LFs based on selected development
data and a suite of backend engines where the system computes
the best example to select in each iteration and learns from the
created LFs along with their development context. At the frontend,
Nemo provides convenient user interface for users to easily create
LFs based on selected development data (Section 4.1). In the sys-
tem backend, the Development Data Selector strategically selects
examples from the large unlabeled set to guide users in creating
informative LFs (Section 4.2). In addition, the Labeling Function Con-
textualizer exploits the LF lineage to development data to facilitate
more effective modeling and denoising of the LFs (Section 4.3).

Nemo workflow begins by initially taking as input an unlabeled
dataset. Then, Nemo proceeds in an interactive loop where each
iteration follows three main stages (formal system pseudo-code
provided in Algorithm 1):

(1) Selecting Development Data: Based on the current model
information, the Development Data Selector takes as input
the unlabeled dataset and strategically selects a data sample
that is expected to guide the user in developing an informa-
tive LF that complements the current set of LFs.

(2) Developing Labeling Function: The frontend interface
prompts the user with the above selected data sample, and
ask the user to return an LF. The newly created LF and its
development context, i.e., the currently shown development
data, will be passed into next stage for model learning, along
with all previously created LFs.

(3) Learning with LF Lineage: Given the LFs and their devel-
opment context, the LF Contextualizer identifies data sub-
spaces where each LF is expected to be more accurate. Then,
the label model exploits this information and learns to aggre-
gates the weak labels into soft probabilistic labels. Finally,
an end discriminative model is trained with the soft labels to
make predictions on unseen examples. Nemo either contin-
ues next iteration by passing the model information back to
the first stage to inform the Development Data Selector, or
outputs the discriminative model as the final end product.

Nemo: Guiding and Contextualizing Weak Supervision for Interactive Data Programming

Algorithm 1 Nemo System Workflow.

Initial Input: An unlabeled dataset 𝑈 = {𝑥𝑖 }𝑛
𝑖=1
Final Output: A discriminative model 𝑓 : X → Y
Initialize label model 𝑔, discriminative model 𝑓 , and LF set Λ0 = ∅
for 𝑡 = 1 to 𝑇 do

Select an example 𝑥𝑡 ← 𝑆𝑒𝑙𝑒𝑐𝑡 (𝑈 , 𝑔, 𝑓 ) by Algorithm 2
Guide user develop new LF 𝜆𝑡 by showing the user with 𝑥𝑡
Expand LF set Λ𝑡 = Λ𝑡 −1 ∪ {𝜆𝑡 }
Update models (𝑔, 𝑓 ) ← 𝐿𝑒𝑎𝑟𝑛𝑖𝑛𝑔(𝑈 , Λ𝑡 , 𝑔, 𝑓 ) by Algorithm 3

end for
return Final model 𝑓 .

System Configuration and Inputs. Nemo focuses on one most
widely adopted type of LFs, the primitive-based LFs. Formally, we
consider primitive-based LFs to be any function 𝜆 : X → Y that
can be expressed by:

𝜆𝑧,𝑦 (𝑥) : return 𝑦 if 𝑥 contains 𝑧 else abstain

where 𝑧 ∈ Z is some domain-specific primitive and 𝑦 ∈ Y is
a target label. Such functional type (or family) of LFs has been
widely considered in the literature [3, 6, 32, 36], and can flexibly
capture arbitrary input pattern by defining the primitive domain Z
accordingly. One representative instantiation in text domain is the
keyword-based LFs, where Z is a set of keywords, e.g., n-grams.
Before the interactive loop starts, we ask the user to configure
Nemo by specifying : (1) the data domain X, (2) the label space Y,
and (3) the primitive domain Z. These specifications allow Nemo to
configure the user interface accordingly and later help users more
conveniently develop LFs, as we shall illustrate shortly in Figure 5.

Example 4.1. Consider the running example of sentiment classifi-
cation on product reviews. The data domain X is the text inputs. The
label space is Y = {positive, negative}. The primitive domain Z could
be specified as the set of all uni-grams contained in the unlabeled set,
which can be automatically inferred by Nemo given 𝑈 .

4.1 User Interface: Writing Labeling Functions

with Development Data

We describe how we design Nemo user interface to allow users
conveniently create LFs guided by selected development data.
Creating LFs with Development Data. In DP applications, we
observe a common practice that users would go through when
developing LFs based on development data. Specifically, by looking
at a data sample 𝑥, users generally follow three principal steps to
create an LF:

(1) Determine the corresponding label 𝑦 of the example 𝑥.
(2) Look for a primitive 𝑧 within 𝑥 that is indicative of the label
𝑦, which is expected to generalize well to other examples.

(3) Create and return the labeling function 𝜆𝑧,𝑦.

We note that such procedure has been widely adopted in practice
and in previous literature [3, 9, 23]. The Nemo user interface is
designed to support this workflow where users could easily cre-
ate LFs from development data through a few mouse-clicks. We
demonstrate the Nemo user interface using the running example
on the sentiment classification task in Example 4.2.

Figure 5: Nemo user interface. Users can easily create an LF
from a development data by selecting a target label and a
corresponding primitive. Note that in the example, the tar-
get labels—positive and negative—are configured according
to the user provided label domain Y. The shown candidate
primitives are selected according to provided primitive do-
main Z, in this case, the uni-grams in the dataset.

Example 4.2. Figure 5 shows the Nemo user interface. In each iter-
ation, the user will be shown a development data selected from the
unlabeled set. In this case, a product review: “Perfect for my work-
outs...”. To extract useful heuristic as LF from the example, the user
would first determine that the review corresponds to positive sen-
timent. Then, the user looks for a keyword primitive in the review
that supports positive sentiment, e.g., the word “perfect”. Finally, by
simply clicking on the corresponding sentiment and keyword, Nemo
automatically creates an LF 𝜆perfect, positive.

4.2 Development Data Selector: Guiding

Informative Labeling Function Creation
The Development Data Selector is the core engine in Nemo that tack-
les the problem of how to guide efficient LF development through
intelligently selected development data.
Random Selection Baseline. One straightforward selection ap-
proach is to randomly sample the development data from the unla-
beled set in each iteration. In fact, as the development data selection
problem has not been carefully considered in literature to date, the
random selection baseline has been the prevailing approach adopted
in most existing DP applications. However, a major drawback of
the random approach is that it completely ignores the information
provided by the current set of LFs on hand, potentially leading to
the development of redundant LFs that provide little extra label
information.

Example 4.3. In Figure 6, suppose that the user already developed
two LFs, 𝜆1 and 𝜆2, for an initially unlabeled dataset, and we are
already confident about the labels in two major clusters. Ideally, we
would like the user to write LFs that provide supervision over the
examples that have yet received label annotations. However, by ran-
dom sampling, the selected development data has high probability
of being an example within the two major data clusters, given the
dominating probability mass of the two major clusters. In turn, the
user will likely to create an LF that annotates the examples within
the two clusters, providing limited extra supervision information and
starving the examples in the other two smaller clusters which as well
play important roles in the classifier’s decision boundary.

Cheng-Yu Hsieh, Jieyu Zhang, and Alexander Ratner

𝑥. In SEU, we model this conditional probability for any LF 𝜆𝑧,𝑦 ∈ F
by:

,

0,

(2)

𝑃 (𝑦) ·

(cid:205)𝜆 (𝑥𝑖 ) ≠0

otherwise

1{𝜆 (𝑥𝑖 )= ˆ𝑦𝑖 }

(cid:205)𝜆 (𝑥𝑖 ) ≠0 1

𝑃 (𝜆𝑧,𝑦 |𝑥) =

𝑎𝑐𝑐 (𝜆𝑧,𝑦 )
(cid:205)𝜆∈{𝜆𝑧,𝑦 |𝑧 in 𝑥 } 𝑎𝑐𝑐 (𝜆)

if 𝑧 contained in 𝑥




where 𝑎𝑐𝑐 (𝜆) =
denotes the approximated accu-
racy of 𝜆. The user model closely reflects the procedure of how
users develop LFs in practice. Recall from Section 4.1, when given a
data sample 𝑥, the user would (1) determine the corresponding label
𝑦 for 𝑥, and (2) select a 𝑦-indicative primitive 𝑧 from the candidate
primitives contained in 𝑥 to create the LF 𝜆𝑧,𝑦, which is expected
to generalize well to other data points. By mirroring the proce-
dure in parallel, the user model leverages chain rule to decompose
𝑃 (𝜆𝑧,𝑦 |𝑥) into (1) the probability of 𝑦 being the underlying label,
and (2) the probability that 𝑧 would be picked by the user from all
candidate primitives {𝑧 ∈ Z|𝑧 contained in 𝑥 }. In the user model,
we utilize the label prior 𝑃 (𝑦) to model the probability of 𝑦 being
the ground truth label for an example 𝑥. Then, we model the prob-
ability that 𝑧 being picked to be proportional to how strongly 𝑧 is
indicative of the target label 𝑦, captured by the accuracy of 𝜆𝑧,𝑦. In
the absence of the ground truth labels, we use the current label pre-
dictions from the discriminative model ˆ𝑦 = 𝑓 (𝑥) to approximately
compute the true accuracies of the LFs. Finally, we note that the
user model simply assigns zero probability to LFs that operate on
primitives not residing in 𝑥, since these primitives would not be
selected by the user from 𝑥.
LF Utility Function. The goal of LF utility function Ψ : F → R is
to measure the informativeness of an LF, given the current collected
supervision information. In SEU, we design the LF utility function
to be:

Ψ𝑡 (𝜆) =

∑︁

𝑖 ∈𝐶

𝜓 uncertainty

𝑡

(𝑥𝑖 ) · (𝜆(𝑥𝑖 ) ˆ𝑦𝑖 ),

(3)

𝑡

𝑡

where 𝐶 = {𝑖 |𝑥𝑖 ∈ 𝑈 , 𝜆(𝑥𝑖 ) ≠ 0} is the set of indices of those
examples covered by 𝜆, and 𝜓 uncertainty
(𝑥𝑖 ) = − (cid:205)𝑦𝑖 ∈Y 𝑃 (𝑦𝑖 |Λ𝑡 ) ·
log 𝑃 (𝑦𝑖 |Λ𝑡 ) is the current label model uncertainty on an example
𝑥𝑖 based on existing LFs Λ𝑡 . The utility function Ψ(·) gives higher
scores to LFs that provide accurate label annotations to the exam-
ples with high label uncertainty. Specifically, the utility score of an
LF 𝜆 is the sum over the label uncertainty scores 𝜓 uncertainty
(𝑥𝑖 ) of
the examples to which 𝜆 provides labels, weighted by whether the
provided label 𝜆(𝑥𝑖 ) is correct or not, i.e., 𝜆(𝑥𝑖 ) ˆ𝑦𝑖 ∈ {−1, 1}, where
ˆ𝑦𝑖 is an approximate to the ground truth label. In DP, a high label
model uncertainty score 𝜓 uncertainty
(𝑥𝑖 ) corresponds to either an
example that has not been covered by any LFs, or an example on
which the LFs disagree the most. New label information on these
uncertain data points provides informative supervision signals that
reduce the label model’s uncertainty over the entire dataset, com-
plementing the existing LFs. Correct labeling to these uncertain data
points allows us to obtain a more holistic view on the entire data
space or help reduce the noise within the labeled data, both leading
to positive influences to the model performances. On the other hand,
incorrect labeling on these uncertain data points introduces influen-
tial noise into the dataset, resulting in negative impact that is likely
to undermine the subsequent model performances. As a result, we

𝑡

Figure 6: Left: Suppose we already have two LFs on hand.
Middle: We see that random sampling may fail to solicit in-
formative LFs from the user, where it has higher probability
of selecting data points within the large, labeled clusters (the
star points). Right: SEU selects the data point in the smaller
unlabeled cluster as it has higher probability of leading to
LFs that provide new, complementing label information.

Ideally, in each iteration, we seek an LF that could best comple-
ment the current set of LFs, such that newly acquired LF could, for
example, help provide coverage over an originally unlabeled data
subspace, or help resolve the conflicts between current LFs. Thus,
we call for an intelligent strategy that could more adaptively guide
users in creating useful LFs according to the information on hand.
Select by Expected Utility. In designing the Nemo Development
Data Selector, we propose a novel selection strategy, Select by Ex-
pected Utility (SEU), which adaptively picks the development data
that in expectation, could guide the user in developing the most in-
formative LF to complement the current collected label information.
Formally, in each iteration 𝑡, SEU selects the development data by:

𝑥 ∗ = arg max

𝑥 ∈𝑈

E𝑃 (𝜆 |𝑥) [Ψ𝑡 (𝜆)],

(1)

where 𝑃 (𝜆|𝑥) is the estimated conditional probability that an user
would create LF 𝜆 given the development data being 𝑥, and Ψ𝑡 (·) is
an utility function measuring the informativeness of an LF 𝜆 based
on the supervision provided by existing set of LFs Λ𝑡 . SEU is shaped
by two key properties in LF development:

• Development Tendency: By looking at different development

data, the user is likely to create different LFs.

• Varying LF Utilities: Different resultant LFs provide different

levels of useful supervision information.

Let F = {𝜆𝑧,𝑦 |𝑧 ∈ Z, 𝑦 ∈ Y} be the LF family where all possi-
ble LFs lie. Given a development example 𝑥, SEU first leverages
𝑃 (𝜆|𝑥), which we call the user model, to estimate the probability
for each LF 𝜆 ∈ F to be created by the user. Then, SEU evaluates
the informativeness of each LF 𝜆 ∈ F by the utility function Ψ(·).
Finally, an example 𝑥’s ability to lead to an informative LF 𝜆 can
be summarized by the expected value of Ψ(𝜆) taken with respect
to 𝑃 (𝜆|𝑥). We refer the readers to the Development Data Selector
in Figure 4 for an illustration of the SEU selection strategy. We as
well describe this process in Algorithm 2. By design, SEU achieves
the goal of selecting an example that is expected to lead to useful LFs
by capturing and leveraging the above two key properties in LF
development via the user model and utility function respectively.
We describe below how we design the user model 𝑃 (𝜆|𝑥) and the
utility function Ψ(·).
User Model. The goal of user model 𝑃 (𝜆|𝑥) is to provide probability
estimate for an user returning an LF 𝜆 given a development example

Nemo: Guiding and Contextualizing Weak Supervision for Interactive Data Programming

Algorithm 2 Nemo Development Data Selector Procedure

𝑝
𝑖=1, target label space Y

Initial Input: An unlabeled dataset 𝑈 = {𝑥𝑖 }𝑛
tives Z = {𝑧𝑖 }
Final Output: An example 𝑥 ∗ ∈ 𝑈
Generate potential LF set F = {𝜆𝑧,𝑦 |𝑧 ∈ Z, 𝑦 ∈ Y}
for 𝜆𝑗 ∈ F do

Measure LF utility according to Eq. 3 by Ψ(𝜆𝑗 )

𝑖=1, domain primi-

end for
for 𝑥𝑖 ∈ 𝑈 do

for 𝜆𝑗 ∈ F do

Compute conditional probability 𝑃 (𝜆𝑗 |𝑥𝑖 ) according to Eq. 2

end for
Compute SEU score 𝑆𝐸𝑈 (𝑥𝑖 ) = E𝑃 (𝜆 |𝑥𝑖 ) [Ψ(𝜆)]

end for
Select example by 𝑥 ∗ = arg max𝑥𝑖
return example 𝑥 ∗.

𝑆𝐸𝑈 (𝑥𝑖 )

design Ψ(·) to take into account of both the informativeness and
the accuracy of the supervision an LF provides for evaluating its
usefulness to the DP learning pipeline.

With the user model and the LF utility function defined, we

provide the full SEU procedure in Algorithm 2.

Example 4.4. In Figure 6, unlike random sampling, SEU aims to
select development data based on their probability of leading to useful
LFs. In this case, SEU would assign higher scores to data points in
the smaller, unlabeled clusters (green-dash circled area) as they have
higher probability of leading to LFs that provides new supervision
signals complementing current labeling information.

4.3 Labeling Function Contextualizer:

Modeling LFs with Development Context

The LF contextualizer is the main component in Nemo that tackles
the problem of exploiting LF development context for more effective
denoising and learning from the LFs.
Standard Learning Pipeline. In the DP learning pipeline without
LF contextualizer, the standard procedure to learn from a given
set of 𝑚 LFs {𝜆1, . . . , 𝜆𝑚 } is to first apply each LF to the examples
{𝑥𝑖 }𝑛
𝑖=1 to obtain a label matrix 𝐿 where 𝐿𝑖 𝑗 = 𝜆𝑗 (𝑥𝑖 ). Then, from
the label matrix 𝐿, a label model is learned to estimate the accura-
cies of these LFs [11, 23–25]. The estimated accuracies are used as
corresponding weighting terms in aggregating the votes provided
by each LF to produce the probabilistic soft labels {𝑃 (𝑦𝑖 |𝐿)}𝑛
𝑖=1. The
more accurate an LF is, the larger the weight its vote receives in
the aggregation process. Notably, in most of the prevailing label
model approaches [23–25], each LF is assumed to be uniformly
accurate over all the data points it covers, i.e., each LF is modeled to
have the same accuracy across the entire data space. Nonetheless,
this modeling assumption is often violated in practice. Specifically,
when an LF is developed by an user looking at specific development
data, we observe that the LF is likely to be more accurate on the
examples within closer proximity to the development data while
having lower accuracy on examples that lie further away.

Example 4.5. Continuing from Example 4.4, consider a case that
the newly returned LF, 𝜆3 in Figure 7, is not perfectly accurate, and

Figure 7: Left: 𝜆1 and 𝜆3 have conflicts on the yellow-colored
points. Middle: In the ideal case where we could perfectly
estimate the source accuracies, standard learning pipeline
still fails to resolve the conflicts where it assigns wrong la-
bels to the triangular points. Right: Contextualized learning
pipeline refines the LFs and resolve the conflicts perfectly.

there are conflicts (yellow-colored) between 𝜆1 and 𝜆3 . Even if we
could perfectly estimate the source accuracy of both 𝜆1 and 𝜆3, we
would still assign wrong labels to the triangular points, since 𝜆1 would
receive higher weight over 𝜆3 for 𝜆1 having a higher overall accuracy.

Contextualized Learning Pipeline. Given the observation that
LFs tend to provide noisier labels on examples that lie further away
from their development data, we would ideally exploit this lineage
to more effectively denoise and learn from the LFs. To this end, we
propose a contextualized learning pipeline in Nemo where we lever-
age the LF contextualizer to refine and denoise the LFs—according
to their development context—before feeding them as input weak
supervision sources to learning the subsequent label and end dis-
criminative model, as shown in bottom pipeline in Figure 4. In the
Nemo learning pipeline, the LF contextualizer refines each LF by
restricting it to be active only on examples within a certain prox-
imity of its corresponding development data point, dropping the
labels assigned to examples that are further away as these labels are
prone to be noisier. Formally, let 𝑥𝜆 denote the development data
point from which the LF 𝜆 is created. Given a set of 𝑚 LFs and their
), . . . , (𝜆𝑚, 𝑥𝜆𝑚 )},
corresponding development data points {(𝜆1, 𝑥𝜆1
𝑗=1 into 𝜆′
the LF contextualizer refines each LF 𝜆𝑗 ∈ {𝜆𝑗 }𝑚
(cid:40)𝜆𝑗 (𝑥),
0 (abstain),

if 𝑑𝑖𝑠𝑡 (𝑥, 𝑥𝜆 𝑗 ) ≤ 𝑟 𝑗
otherwise

𝜆′
𝑗 (𝑥) =

𝑗 by:

(4)

where 𝑑𝑖𝑠𝑡 (·, ·) : X × X → R measures the distance (dissimilarity)
between two input examples, and 𝑟 𝑗 is a given thresholding value for
the refinement radius. By discarding the assigned labels to examples
where the LFs are expected to perform more poorly, we may reduce
the noise in the generated label matrix 𝐿, and in turn produce
more accurate probabilistic soft labels {𝑃 (𝑦𝑖 |𝐿)}𝑛
𝑖=1. We provide the
Nemo contextualized learning pipeline in Algorithm 3. In practice,
the distance measurement can be selected based on feature domain.
For instance, in text domain, 𝑑𝑖𝑠𝑡 (·, ·) can be the cosine distance
or the euclidean distance. For the refinement radius, we set 𝑟 𝑗 to
be the 𝑝-th percentile value of {𝑑𝑖𝑠𝑡 (𝑥𝜆 𝑗
𝑖=1, i.e., the set of all
distances from each example 𝑥𝑖 ∈ 𝑈 to the development data point
𝑥𝜆𝑗 , where 𝑝 is a system hyperparameter that can be selected based
on the validation accuracy of the resultant estimated soft labels. We
note that the Nemo contextualized learning pipeline is compatible
with any label modeling approach, where the LF contextualizer is
essentially a pre-processing step on the weak supervision sources

, 𝑥𝑖 )}𝑛

Algorithm 3 Nemo Contextualized Learning Procedure
Initial Input: An unlabeled dataset 𝑈 = {𝑥𝑖 }𝑛
𝑖=1, current set
of LFs Λ = {𝜆1, . . . , 𝜆𝑗 }, their corresponding development data
{𝑥𝜆1
, . . . , 𝑥𝜆𝑗 }, and given threshold values {𝑟1, . . . , 𝑟 𝑗 }.
Final Output: A label model 𝑔, and a discriminative model 𝑓
for Each 𝜆𝑗 in Λ do

Contextualize LF by 𝜆𝑗 ← 𝜆′

𝑗 according to Eq. 4 with 𝑟 𝑗

end for
Learn label model 𝑔 as in standard DP pipeline
Learn discrminative model 𝑓 as in standard DP pipeline
return Label model 𝑔 and discriminative model 𝑓

and the final probabilistic soft labels can be learned using any user-
specified label model. This design allows Nemo to flexibly benefits
from the advance in weak supervision modeling approaches, where
the direction has received increasing research attention.

Example 4.6. In Figure 7, if we leverage the context that 𝜆1 and 𝜆3
were created from the red/blue-start points respectively, contextualized
learning pipeline would be able to refine the LFs, and hopefully resolve
the conflicts perfectly.

5 EVALUATION
We evaluate the end-to-end performance of Nemo, and perform a
suite of ablation studies on (1) the proposed SEU selection strategy
and (2) the contextualized learning approach. We seek to validate
the following claims in response to the key hypotheses made:

• Nemo makes the end-to-end DP learning paradigm more
efficient and productive. In Section 5.2, we see that Nemo
much improves over the current DP workflow by an average
20%, validating that IDP formalism could enable optimization
on DP learning paradigm.

• The proposed selection strategy SEU improves the effi-
ciency of LF development over existing selection meth-
ods. In Section 5.3, we see that SEU offers significant perfor-
mance lift over the random sampling baseline by an average
16%, validating that intelligent selection method can indeed
improve the efficiency of DP pipeline and the effectiveness
of SEU for this selection problem.

• The proposed contextualized learning pipeline improves
the learning performance over the standard learning
pipeline. In Section 5.4, we see that the contextualized learn-
ing pipeline leads to a considerable lift over standard learn-
ing pipeline by an average 11%, validating the importance
of exploiting LF development context in learning and the
effectiveness of the proposed approach.

5.1 Evaluation Setup
Datasets. We conduct experiments across five textual datasets
spanning two different tasks: sentiment classification and spam
classification. For sentiment classification, we include Amazon Re-
view [13], Yelp Review [38], and IMDB Review [17]. For spam
classification, we include Youtube [1] and SMS [2]. For each dataset,
we randomly partition the data into 80% training set, 10% validation
set, and 10% test set, following the common convention [36]. We
provide the dataset statistics in Table 1.

Cheng-Yu Hsieh, Jieyu Zhang, and Alexander Ratner

Table 1: Dataset statistics.

Task

Dataset

#Train #Valid #Test

Sentiment Classification

Spam Classification

Amazon
Yelp
IMDB

Youtube
SMS

14,400
20,000
20,000

1,566
4,458

1,800
2,500
2,500

195
557

1,800
2,500
2,500

195
557

𝑖=1

(cid:205)𝑛

Evaluation Protocol. For performance comparisons, similar to
active learning setting, we plot out the learning curve of end model
generalization performance on the test set over the number of iter-
ations. A more efficient method could achieve higher performance
within fewer number of iterations. We measure the generalization
performance using Accuracy Score for all datasets except for SMS,
in which we use F1-score since the dataset is highly imbalanced. For
ease of comparisons, we summarize each learning curve by calcu-
lating the average performance on the learning curve, which essen-
tially corresponds to its area under curve. Formally, given a learning
curve represented by a set of points C = {(𝑥0, 𝑦0), . . . , (𝑥𝑛, 𝑦𝑛)}
where 𝑥𝑖−1 < 𝑥𝑖 and each (𝑥𝑖, 𝑦𝑖 ) corresponds to the model perfor-
mance 𝑦𝑖 after 𝑥𝑖 iterations, we summarize the performance of the
curve by: 1
𝑦𝑖 . In the experiments, we perform a total of 50
𝑛
iterations and evaluate the model performance every 5 iterations.
We include the evaluation plots in Appendix A. All reported results
are the averaged over 5 runs with different initializations.
Simulated User. In the experiments, apart from the user study
conducted in Section 5.2, we leverage simulated user to allow more
extensive evaluations of the methods. Similar to [6], we utilize
ground truth labels to simulate real user feedbacks. Specifically,
when an example 𝑥𝑖 is selected and 𝑥𝑖 contains a set of primitives
𝐶 = {𝑧 ∈ Z|𝑧 contained in 𝑥𝑖 }, we first build a set of candidate
LFs Λ = {𝜆𝑧,𝑦𝑖 |𝑧 ∈ 𝐶}. Then, to resemble human expertise, the
candidate set Λ is refined by filtering out LFs whose accuracy lower
than some threshold 𝑡. Finally, one LF is randomly sampled from
the refined set of candidate LFs and returned. We set 𝑡 = 0.5 in the
experiments if not otherwise mentioned.
Implementation Details. In the experiments, we featurize the
input text examples with their TF-IDF representation. We fix the
end model to be logistic regression model for all methods. If not
otherwise mentioned, we adopt MeTaL [25] as the underlying label
model to aggregate the weak labels. We consider the primitive
domain Z to be the set of uni-grams in training dataset. We include
more details in Appendix B.

5.2 Nemo End-to-End System Performance
We demonstrate that Nemo outperforms existing baseline methods
across various datasets through extensive quantitative experiments
and an user study. We validate the importance of different Nemo
system components, and demonstrate Nemo’s robustness to the
change of LFs’ accuracy level.
Comparisons to baseline methods. We include two sets of base-
line methods in our experiments. First, we include existing methods
that are subsumed under IDP framework:

• Snorkel [23]: Snorkel is a vanilla instantiation of IDP that
selects development data by random sampling, and learns

Nemo: Guiding and Contextualizing Weak Supervision for Interactive Data Programming

Table 2: Performances of Nemo and existing baselines across datasets. We see Nemo consistently outperforms the baselines,
and that the proposed IDP framework offers strong performance when compared to other existing interactive schemes.

Methods

Full IDP

Vanilla IDP

Selection-only IDP

CL-only IDP

Other Interactive Schemes

Dataset

Amazon
Yelp
IMDB
Youtube
SMS

Nemo

0.7674
0.7907
0.7958
0.8722
0.7038

Snorkel

Snorkel-Abs

Snorkel-Dis

ImplyLoss-L

0.6774
0.6556
0.7107
0.8235
0.4789

0.6783
0.6664
0.7338
0.8541
0.6189

0.6733
0.6887
0.7480
0.8527
0.5485

0.6822
0.7009
0.6766
0.6811
0.5065

US

0.5970
0.6239
0.6058
0.7609
0.4234

IWS-LSE

0.6234
0.6415
0.6295
0.7904
0.6305

Table 3: Performances and median user response time for different methods in the user study on Amazon dataset. Nemo
significantly outperforms the baselines while taking slightly more time for users to respond compared to US and IWS-LSE.

Methods

Full IDP

Vanilla IDP

Selection-only IDP

CL-only IDP

Other Interactive Schemes

Metric

Performance
React Time (Median)

Nemo

0.7473
14.42s

Snorkel

Snorkel-Abs

Snorkel-Dis

ImplyLoss-L

US

IWS-LSE

0.6665
16.21s

0.6689
17.95s

0.6600
13.05s

0.6833
16.21s

0.5882
12.50s

0.5971
6.73s

from LFs without utilizing their development context. It is
the predominant approach used in practice.

• Snorkel-Abs [9]: Snorkel-Abs is a selection-only IDP method
that adaptively selects development data on which the cur-
rent LFs abstain the most. It learns from LFs without consid-
ering their development context.

• Snorkel-Dis [9]: Snorkel-Dis is as well a selection-only IDP ap-
proach that stratigically selects development data on which
the current LFs disagree the most. It does not leverage LF
development context in learning.

• ImplyLoss-L 1 [3]: ImplyLoss-L is a contextualized-learning
only IDP approach. It learns from LFs with their development
context modeled through a deliberately designed model and
loss function. However, it does not consider strategically
selecting the development data. Thus, we couple it with the
random sampling selection method.

Second, we include methods under other related interactive schemes:

• Uncertainty Sampling (US) [15]: US is a classic and competi-

tive method within active learning paradigm [29].

• IWS-LSE [6]: IWS-LSE is a representative method under the
interactive weak supervision paradigm considered in [6, 12],
where the user is iteratively queried to provide feedback on
whether a suggested labeling heuristic is useful or not.

In Table 2, we see that Nemo consistently outperforms all the
baseline methods by a significant margin, with an average 9% per-
formance lift over the second-best performing methods in each
dataset. More closely, by comparing Nemo to Snorkel, we observe
an average 20% improvement, verifying the importance and bene-
fits of the IDP framework that considers optimizing and exploiting
LF development process in the DP pipeline. While Snorkel-Abs and

1Note that we modify the discriminative part of the ImplyLoss model to be a linear
model (hence the suffix “L”) for consistency across the methods.

Snorkel-Dis improve over vanilla Snorkel by considering more adap-
tive data selection methods, the performance lift is much smaller
in these cases. This is largely due to the heuristic design of the two
approaches and their lack of further leveraging LF development
context in learning. For ImplyLoss-L, although it improves over
Snorkel (in 3 out of 5 datasets) by learning with contextualized LFs,
without strategically selecting the development data, its perfor-
mance lift appears limited especially when compared to Nemo. In
sum, while previous methods had improved over the vanilla Snorkel
baseline by either (1) designing better selection strategies or (2)
learning with LF development context, none of these methods have
considered both problems in an unifying interactive framework,
thus rendering limited performance improvement when compared
to Nemo which tackles both problems simultaneously in IDP. From
Table 2, we also observe that by soliciting user supervision at func-
tional level, i.e., LFs, IDP methods (including the vanilla Snorkel)
achieve better performances over US that queries for user feedback
at single label annotation level, by up to 66% improvement. Simi-
larly, IDP methods generally perform preferably against IWS-LSE
that queries for user feedback on the usefulness of single labeling
heuristic per iteration. Notably, the real user effort spent in an-
swering each query in different interactive schemes may not be
completely reflected by the reported performances, since varying
interactive schemes require different types of user inputs. However,
we believe that the results indeed demonstrate the importance and
the promising potential of studying the proposed IDP framework.
Case study with real users. We conduct a carefully designed
user study to validate the effectiveness of Nemo with real users
developing the LFs based on selected development data. In the
user study, we invited a total of 15 users with knowledge in basic
machine learning as participants, including graduate students and
industry engineers working in related fields. In the study, each user
is asked to perform 2 randomly assigned interactive learning tasks,

Cheng-Yu Hsieh, Jieyu Zhang, and Alexander Ratner

Figure 8: Performances under varying LF accuracy thresholds. Nemo demonstrates the strongest robustness when LF accuracy
threshold decreases from 0.7 to 0.5.

Table 4: Comparisons between Nemo performance with and
without either the data selector or the LF contextualizer. We
see that both components are critical to Nemo.

Dataset

Nemo No Data Selector No LF Contextualizer

Ablated Version

Amazon
Yelp
IMDB
Youtube
SMS

0.7674
0.7907
0.7958
0.8722
0.7038

0.7244
0.7360
0.7557
0.8407
0.6092

0.7384
0.7219
0.7932
0.8628
0.6899

so that each method receives results from 5 users 2. In each task, the
user goes through a total of 30 interactive iterations, and we record
the model performance every 3 iterations. We randomly shuffle the
order an user perform on the 2 assigned tasks to avoid potential
bias that could be introduced by the ordering of different methods.
We perform the study using the sentiment classification task on
Amazon Review dataset which most users are familiar with.

We report the user study results in Table 3. We observe an over-
all similar trend to our findings with simulated users. Specifically,
Nemo provides a significant performance lift, up to 13% and 27%,
over existing DP and interactive methods respectively. This show-
cases that Nemo, by supporting the full IDP loop, indeed leads to
an efficient and productive learning workflow in practice with real
users. In addition, we see that users generally spend a little more
time in providing LFs as feedback in IDP setting than providing la-
bel annotation as response in active learning setting. The overhead
lies in the extra time, which is roughly around 2 to 3 seconds per it-
eration, that an user needs to select a corresponding primitive from
the development data in addition to determining its ground truth
label. We also note that IWS-LSE has the shortest user response
time, as determining whether an LF is useful or not is generally
easier than determining the label for a data point.
Ablation study on Nemo. We study the importance of the two
core components in Nemo, i.e., the development data selector and
the LF contextualizer. Specifically, we evaluate Nemo’s performance
when either one of the components is removed. We see that in Ta-
ble 4, removing either components decreases Nemo’s performance,
with an average 7% and 3% drop when the data selector and the LF
contextualizer is removed respectively. This showcases the impor-
tance of both components in Nemo and the need to consider both
the data selection and contextualized learning problems in IDP.

2We compute the result for ImplyLoss based on LFs created in the Snorkel user study,
since random selection strategy is used in both approaches.

Table 5: Performances of different selection strategies when
the learning pipeline is fixed to be the standard vanilla ap-
proach without using LF development context. We see that
SEU consistently outperforms, by a large margin, the other
baselines in all datasets considered.

Selection Strategy

Dataset

SEU

Random

Abstain

Disagree

Amazon
Yelp
IMDB
Youtube
SMS

0.7384
0.7219
0.7932
0.8628
0.6899

0.6774
0.6556
0.7107
0.8235
0.4789

0.6783
0.6664
0.7338
0.8541
0.6189

0.6733
0.6887
0.7480
0.8527
0.5485

Sensitivity to LF precision. We evaluate the robustness of Nemo
with respect to the accuracy of input LFs. Recall that when simu-
lating real user response, the oracle simulator filters out LFs with
accuracy lower than a given threshold 𝑡. Here, we evaluate Nemo
under varying values of 𝑡, along with comparisons to other baseline
IDP methods. From Figure 8, we first observe that as the threshold
value increases, there is an overall trend of performance improve-
ments for all methods, suggesting that, perhaps unsurprisingly,
users could in general improve DP learning performance by provid-
ing more accurate LFs. Next, we see that regardless of the accuracy
threshold values, Nemo consistently achieves the best performance
as compared to other baselines. Finally, Nemo demonstrate stronger
robustness to the threshold value change than the baseline methods.
For example, we see that the performance of Snorkel drastically
drops when the threshold value decreases from 0.7 to 0.5, whereas
Nemo remains much stable across different threshold values. The re-
sults suggest that Nemo could be more reliably deployed in practice
where the accuracies of LFs developed could vary in range.

5.3 SEU Selection Strategy Performance
We evaluate the effectiveness of the proposed selection strategy
SEU by comparing it to different baseline selection approaches, and
ablating various aspects of its design. To focus on the comparisons
between different selection strategies alone, we fix the learning
pipeline to be the standard vanilla procedure (without the use of
LF development context) in the following sets of experiments.
Comparisons to baseline selection approaches. We compare
SEU to three baseline selection methods:

• The Random baseline that selects randomly from the unla-

beled set [23].

Nemo: Guiding and Contextualizing Weak Supervision for Interactive Data Programming

Table 6: Ablation study on the SEU’s user model. We see that
the accuracy-weighted design is critical to SEU’s success.

Dataset

Amazon
Yelp
IMDB
Youtube
SMS

User Model

SEU (Eq. 2)

Uniform

0.7384
0.7219
0.7932
0.8628
0.6899

0.6774
0.6556
0.7107
0.8235
0.4789

Table 7: Ablation study on SEU’s LF utility function. We see
that SEU achieves best performances when the utility func-
tion captures both the informativeness and the correctness
aspects of an LF.

Dataset SEU (Eq. 3)

No Informativeness

No Correctness

LF Utility Function

Amazon
Yelp
IMDB
Youtube
SMS

0.7384
0.7219
0.7932
0.8628
0.6899

0.7369
0.7211
0.7911
0.8538
0.6695

0.6683
0.6536
0.7847
0.8552
0.6517

• The Abstain baseline that selects the data point on which

the current LFs abstain the most [9].

• The Disagree baseline that selects the data point on which

the current LFs disagree the most [9].

From Table 5, we see that SEU consistently enjoys better perfor-
mance than the other baseline selection approaches often by a large
margin. When compared to Random, SEU provides performance
lift up to 44%, validating its capability of strategically selecting use-
ful development data to guide efficient development of LFs. When
compared to Abstain and Disagree, SEU as well enjoys improve-
ment up to 26%, demonstrating its advantage over these existing
straightforward strategies.
Ablation study on user model. Recall that in SEU’s user model
(Eq. 2), the conditional probability 𝑃 (𝜆𝑧,𝑦 |𝑥), if not zero, is modeled
based on the estimated accuracy of 𝜆𝑧,𝑦. SEU models that an user
would have higher probability of picking a primitive 𝑧 from 𝑥 to
create an LF 𝜆𝑧,𝑦 if 𝜆𝑧,𝑦 has higher accuracy. Here, we examine
the importance of this accuracy-weighted design in the user model
by comparing to a baseline where we modify the user model to
not consider the accuracies of LFs and to essentially assign uniform
probability to the potential LFs {𝜆𝑧,𝑦 |𝑧 ∈ Z, 𝑧 contained in 𝑥 }:

𝑃 (𝜆𝑧,𝑦 |𝑥) =

(cid:40)𝑃 (𝑦) ·

1
(cid:205)𝜆∈{𝜆𝑧,𝑦 |𝑧 in 𝑥 } 1

,

if 𝑧 contained in 𝑥

0,

otherwise

From Table 6, we see that it is indeed important to model 𝑃 (𝜆𝑧,𝑦 |𝑥)
differently based on the accuracy of 𝜆𝑧,𝑦, where the performance
of SEU much degrades when 𝑃 (𝜆𝑧,𝑦 |𝑥) is modeled uniformly for
all possible LFs.
Ablation study on LF utility function. Another core component
in SEU is the LF utility function, which is designed to assign higher
score to LFs that provide informative and accurate supervision

Table 8: Performances of different approaches to learn from
LFs. We see that contextualized pipeline improves over the
standard pipeline and the ImplyLoss model.

Learning Approach

Dataset

Contextualized

Standard

ImplyLoss

Amazon
Yelp
IMDB
Youtube
SMS

0.7244
0.7360
0.7557
0.8407
0.6092

0.6774
0.6556
0.7107
0.8235
0.4789

0.6822
0.7009
0.6766
0.6811
0.5065

signals. Specifically, recall that in Eq. 3, the first term𝜓 uncertainty
(𝑥𝑖 )
and the second term 𝜆(𝑥𝑖 ) ˆ𝑦𝑖 capture the informativeness and the
correctness of the provided label 𝜆(𝑥𝑖 ) respectively. We evaluate the
importance of both aspects by comparing to two ablated versions
of utility functions, in which we remove either term respectively:

𝑡

• No Informativeness: Ψ𝑡 (𝜆) = (cid:205)𝑖 ∈𝐶 𝜆(𝑥𝑖 ) ˆ𝑦𝑖
• No Correctness: Ψ𝑡 (𝜆) = (cid:205)𝑖 ∈𝐶 𝜓 uncertainty

𝑡

(𝑥𝑖 )

In Table 7, we see that removing either the informativeness or the
correctness term in the utility function decreases SEU performance,
suggesting the importance of considering both properties when
measuring an LF’s utility.

5.4 Contextualized Learning Performance
We evaluate the effectiveness of the proposed contextualized learn-
ing pipeline, and ablate different aspects of its design choices. To
focus on the comparisons between different approaches to learn
from the LFs, we fix the development data selection strategy to
be the vanilla random selection approach in the following sets of
experiments.
Comparisons to baseline methods. We compare the contextual-
ized learning pipeline to two baseline approaches:

• The Standard learning pipeline that learns from the LFs

without utilizing any LF contextual information.

• The Implyloss learning model that learns from the LFs and
their contextual information through a specialized model
and loss function.

In Table 8, we demonstrate that the proposed contextualized learn-
ing pipeline can effectively leverage the LF development context
to better model and learn from the weak supervision sources. It
improves over the standard learning pipeline by up to 27%. In par-
ticular, by only refining the LFs’ coverage and learn with the same
underlying label model (MeTaL), we obtain larger performance im-
provement compared to the improvement brought by designing a
more sophisticated ImplyLoss model.
Ablation study on distance function. Recall that in the contex-
tualized learning pipeline, the LF contextualizer relies on a distance
function to refine the LFs. We compare how different distance mea-
surements affect the performance. In Table 9, we see that Cosine
distance generally brings larger performance lift than Euclidean
distance. We note that regardless of the distance function used, the
contextualized pipeline improves over the standard counterpart.
Compatibility to different label models. In Section 4.3, we dis-
cussed that the proposed contextualized learning pipeline is model-
agnostic, and thus is compatible with different label models. We

Table 9: Contextualized learning with different distance
functions. Cosine distance offers larger lift than Euclidean
distance while both improves over standard pipeline.

Contextualized

Dataset Cosine Distance

Euclidean Distance

Standard

Amazon
Yelp
IMDB
Youtube
SMS

0.7244
0.7360
0.7557
0.8407
0.6092

0.6913
0.6991
0.7200
0.8181
0.6174

0.6774
0.6556
0.7107
0.8235
0.4789

Table 10: Contextualized versus standard learning with Ma-
jority Vote label model. Contextualized learning still offers
improvement over the standard baseline.

Dataset

Amazon
Yelp
IMDB
Youtube
SMS

Majority Vote as Label Model

Contextualized

Standard

0.7187
0.7399
0.7488
0.8265
0.6432

0.6869
0.6861
0.7417
0.8265
0.5615

validate its effectiveness when the underlying label model is set
to be Majority Vote, another popular choice of label aggregation
method. In Table 10, we see the contextualized pipeline is still able
to improve the performance over the standard pipeline by up to
15%, demonstrating its compatability to various label models.

6 RELATED WORK
Recent progress in DP, or more broadly weak supervision, has
largely been made in developing advanced label models [8, 11, 20,
24, 25] that denoise and aggregate the weak supervision sources
for various applications [4, 10, 14, 27, 28, 30, 34]. We defer readers
to [35] for a more comprehensive survey on weak supervision
methods, and focus this section on related work that anchors more
on the development process of weak supervision sources and the
different interactive learning schemes related to DP.
Labeling Function Development. Recent studies on DP have
pointed out that it can sometimes be challenging for users to de-
velop LFs, by needing to inspect a considerable amount of data
to look for useful labeling heuristics [33]. To reduce user effort
spent in LF development, studies have mainly taken three direc-
tions: (1) automatic LF generation, (2) interactive LF discovery, and
(3) interactively-guided LF development which our work falls into.
Automatic LF generation methods aim to create LFs automati-
cally. The methods generally require an initial set of labeled data,
or seed LFs developed by users. Snuba [33] learns weak classifiers
as heuristic models from a small labeled dataset; TALLOR [16] and
GLaRA [39] use an initial set of seed LFs to generate new ones by
compounding multiple simpler LFs and by exploiting the seman-
tic relationship of the seed LFs respectively; [31] applies program
systhesis to generate task-level LFs from a set of labeled data and
domain-level LFs.

Cheng-Yu Hsieh, Jieyu Zhang, and Alexander Ratner

Interactive LF discovery methods consider adaptively searching
for useful LFs from a large candidate set, by interactively querying
for user’s feedback on whether some suggested LFs are useful
or not. The candidate LFs are generated based on context-free
grammar [12], n-grams [6], or pretrained language models [37].
Based on the user’s feedback on the usefulness (whether an LF is
better than random) of some selected LFs, the systems adapt and
learn to identify promising LFs from the large candidate set, which
are output as the final LFs to be used in the subsequent DP pipeline.
Unlike the above two directions that require different forms of
user inputs in LF development process as compared to the existing
workflow, our work takes the third direction (interactively-guided
LF development), which is built to inherently support the existing
workflow used in practice, i.e., users write LFs by drawing ideas
from development data. However, instead of simply selecting the
development data randomly from the unlabeled set, this direction
considers strategically selecting informative development data to
guide the users in designing useful LFs efficiently. Within this
direction, [9] performed an initial exploration, but in a relatively
ad-hoc way. Our work proposes the first formalism, IDP, for the
problem that further extends the scope to exploiting the information
in LF development process to better model the resultant LFs.
Connecting Data Programming and Active Learning. Our work
formalizes IDP as a learning paradigm that bridges DP and active
learning. Related to our work, prior studies have explored the con-
nection between DP and active learning from other perspectives:
(1) applying active learning to complement DP, and (2) leveraging
DP techniques for active learning.

Within the first direction, [18] proposed to complement an ex-
isting set of LFs by asking users to annotate a selected subset of
unlabeled data; Similarly, [5] asks users to label selected data points
that would be most informative in helping denoise and aggregate
the weak supervision sources in DP; Asterisk [21] employed an
active learning process to enhance the quality, in terms of accuracy
and coverage, of the weak labels initially provided by the LFs.

On the second direction, [22] applied DP to generate an initial
set of weak labels to improve the efficiency of a later active learning
process; [19] aimed to expand an initial set of labeled data with
examples from another larger unlabeled dataset. The method first
constructs neighborhood-based LFs from the seed data, and utilizes
the LFs to identify relevant candidate examples from the larger
unlabeled set, where the candidate examples are finally annotated
by application users.
7 CONCLUSION
In this paper, we formalize IDP where LF development is considered
a central component in the DP learning pipeline. In IDP, we study
two core problems of how to strategically select development data
for guiding efficient LF development, and how to further exploit
the development context for better LF modeling. We then introduce
Nemo as the first system to support the full IDP workflow, which is
built upon the novel SEU selection strategy and the contextualized
learning pipeline for the two IDP problems respectively. We validate
that Nemo leads to more efficient and productive DP pipeline over
the existing prevailing workflow, with an average 20% (and up to
47%) improvement across various datasets and tasks.

[23] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and
Christopher Ré. 2017. Snorkel: Rapid training data creation with weak supervision.
In Proceedings of the VLDB Endowment. International Conference on Very Large
Data Bases, Vol. 11. NIH Public Access, 269.

[24] Alexander Ratner, Christopher De Sa, Sen Wu, Daniel Selsam, and Christopher
Ré. 2016. Data programming: Creating large training sets, quickly. Advances in
neural information processing systems 29 (2016), 3567.

[25] Alexander Ratner, Braden Hancock, Jared Dunnmon, Frederic Sala, Shreyash
Pandey, and Christopher Ré. 2019. Training complex models with multi-task
weak supervision. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 33. 4763–4771.

[26] Christopher Ré, Feng Niu, Pallavi Gudipati, and Charles Srisuwananukorn. 2019.
Overton: A data system for monitoring and improving machine-learned products.
arXiv preprint arXiv:1909.05372 (2019).

[27] Wendi Ren, Yinghao Li, Hanting Su, David Kartchner, Cassie Mitchell, and Chao
Zhang. 2020. Denoising Multi-Source Weak Supervision for Neural Text Classifi-
cation. In Findings of EMNLP.

[28] Esteban Safranchik, Shiying Luo, and Stephen Bach. 2020. Weakly supervised

sequence tagging from noisy rules. In AAAI, Vol. 34. 5570–5578.

[29] Burr Settles. 2009. Active learning literature survey. (2009).
[30] Changho Shin, Winfred Li, Harit Vishwakarma, Nicholas Roberts, and Frederic

Sala. 2022. Universalizing Weak Supervision. In ICLR.

[31] Albert Tseng, Jennifer J Sun, and Yisong Yue. 2021. Automatic Synthesis of Diverse
Weak Supervision Sources for Behavior Analysis. arXiv preprint arXiv:2111.15186
(2021).

[32] Paroma Varma, Bryan He, Payal Bajaj, Imon Banerjee, Nishith Khandwala,
Daniel L Rubin, and Christopher Ré. 2017. Inferring generative model structure
with static analysis. Advances in neural information processing systems 30 (2017),
239.

[33] Paroma Varma and Christopher Ré. 2018. Snuba: Automating weak supervi-
sion to label training data. In Proceedings of the VLDB Endowment. International
Conference on Very Large Data Bases, Vol. 12. NIH Public Access, 223.

[34] Paroma Varma, Frederic Sala, Shiori Sagawa, Jason Fries, Daniel Fu, Saelig
Khattar, Ashwini Ramamoorthy, Ke Xiao, Kayvon Fatahalian, James Priest,
and Christopher Ré. 2019. Multi-Resolution Weak Supervision for Sequen-
tial Data. In NeurIPS, Vol. 32. https://proceedings.neurips.cc/paper/2019/file/
93db85ed909c13838ff95ccfa94cebd9-Paper.pdf

[35] Jieyu Zhang, Cheng-Yu Hsieh, Yue Yu, Chao Zhang, and Alexander Ratner. 2022.
A Survey on Programmatic Weak Supervision. arXiv preprint arXiv:2202.05433
(2022).

[36] Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, and
Alexander Ratner. 2021. WRENCH: A Comprehensive Benchmark for Weak
Supervision. In Thirty-fifth Conference on Neural Information Processing Systems
Datasets and Benchmarks Track. https://openreview.net/forum?id=Q9SKS5k8io
[37] Rongzhi Zhang, Yue Yu, Pranav Shetty, Le Song, and Chao Zhang. 2022. PRBoost:
Prompt-Based Rule Discovery and Boosting for Interactive Weakly-Supervised
Learning. In Annual Meeting of the Association for Computational Linguistics
(ACL).

[38] Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional
networks for text classification. Advances in neural information processing systems
28 (2015), 649–657.

[39] Xinyan Zhao, Haibo Ding, and Zhe Feng. 2021. GLaRA: Graph-based Labeling
Rule Augmentation for Weakly Supervised Named Entity Recognition. In EACL.
3636–3649. https://aclanthology.org/2021.eacl-main.318

Nemo: Guiding and Contextualizing Weak Supervision for Interactive Data Programming

REFERENCES
[1] T. C. Alberto, J. V. Lochter, and T. A. Almeida. 2015. TubeSpam: Comment Spam
Filtering on YouTube. In ICMLA. 138–143. https://doi.org/10.1109/ICMLA.2015.
37

[2] Tiago A Almeida, José María G Hidalgo, and Akebo Yamakami. 2011. Contribu-
tions to the study of SMS spam filtering: new collection and results. In DocEng.
259–262.

[3] Abhijeet Awasthi, Sabyasachi Ghosh, Rasna Goyal, and Sunita Sarawagi. 2020.
Learning from Rules Generalizing Labeled Exemplars. In International Conference
on Learning Representations. https://openreview.net/forum?id=SkeuexBtDr
[4] Stephen H Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cas-
sandra Xia, Souvik Sen, Alex Ratner, Braden Hancock, Houman Alborzi, et al.
2019. Snorkel drybell: A case study in deploying weak supervision at industrial
scale. In SIGMOD (Industrial). 362–375.

[5] Samantha Biegel, Rafah El-Khatib, Luiz Otavio Vilas Boas Oliveira, Max Baak,
and Nanne Aben. 2021. Active WeaSuL: Improving Weak Supervision with Active
Learning. arXiv preprint arXiv:2104.14847 (2021).

[6] Benedikt Boecking, Willie Neiswanger, Eric Xing, and Artur Dubrawski. 2021.
Interactive Weak Supervision: Learning Useful Heuristics for Data Labeling. In
International Conference on Learning Representations. https://openreview.net/
forum?id=IDFQI9OY6K

[7] Eran Bringer, Abraham Israeli, Yoav Shoham, Alexander J. Ratner, and Christo-
pher Ré. 2019. Osprey: Weak Supervision of Imbalanced Extraction Problems
without Code. Proceedings of the 3rd International Workshop on Data Management
for End-to-End Machine Learning (2019).

[8] Salva Rühling Cachay, Benedikt Boecking, and Artur Dubrawski. 2021. End-to-
End Weak Supervision. In Advances in Neural Information Processing Systems,
A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (Eds.). https:
//openreview.net/forum?id=gbcsmD3Iznu

[9] Benjamin Cohen-Wang, Steve Mussmann, Alexander Ratner, and Christopher
Ré. 2019. Interactive Programmatic Labeling for Weak Supervision. KDD Data
Collection, Curation, and Labeling for Mining and Learning Workshop (2019).
[10] Nilaksh Das, Sanya Chaba, Renzhi Wu, Sakshi Gandhi, Duen Horng Chau, and Xu
Chu. 2020. Goggles: Automatic image labeling with affinity coding. In SIGMOD.
1717–1732.

[11] Daniel Fu, Mayee Chen, Frederic Sala, Sarah Hooper, Kayvon Fatahalian, and
Christopher Ré. 2020. Fast and three-rious: Speeding up weak supervision with
triplet methods. In International Conference on Machine Learning. PMLR, 3280–
3291.

[12] Sainyam Galhotra, Behzad Golshan, and Wang-Chiew Tan. 2021. Adaptive rule
discovery for labeling text data. In Proceedings of the 2021 International Conference
on Management of Data. 2217–2225.

[13] Ruining He and Julian McAuley. 2016. Ups and downs: Modeling the visual
evolution of fashion trends with one-class collaborative filtering. In proceedings
of the 25th international conference on world wide web. 507–517.

[14] Sarah Hooper, Michael Wornow, Ying Hang Seah, Peter Kellman, Hui Xue, Fred-
eric Sala, Curtis Langlotz, and Christopher Re. 2020. Cut out the annotator, keep
the cutout: better segmentation with weak supervision. In ICLR.

[15] David D Lewis and William A Gale. 1994. A sequential algorithm for training

text classifiers. In SIGIR’94. Springer, 3–12.

[16] Jiacheng Li, Haibo Ding, Jingbo Shang, Julian McAuley, and Zhe Feng. 2021.
Weakly Supervised Named Entity Tagging with Learnable Logical Rules. In ACL.
4568–4581. https://doi.org/10.18653/v1/2021.acl-long.352

[17] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng,
and Christopher Potts. 2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th annual meeting of the association for computational
linguistics: Human language technologies. 142–150.

[18] Ayush Maheshwari, Oishik Chatterjee, Krishnateja Killamsetty, Ganesh Ramakr-
ishnan, and Rishabh K. Iyer. 2021. Semi-Supervised Data Programming with
Subset Selection. In FINDINGS.

[19] Neil Rohit Mallinar, Abhishek Shah, T. Ho, Rajendra Ugrani, and Ayushi Gupta.
2020. Iterative Data Programming for Expanding Text Classification Corpora.
ArXiv abs/2002.01412 (2020).

[20] Alessio Mazzetto, Cyrus Cousins, Dylan Sam, Stephen H Bach, and Eli Upfal.
2021. Adversarial Multi Class Learning under Weak Supervision with Perfor-
mance Guarantees. In Proceedings of the 38th International Conference on Machine
Learning (Proceedings of Machine Learning Research), Marina Meila and Tong
Zhang (Eds.), Vol. 139. PMLR, 7534–7543. https://proceedings.mlr.press/v139/
mazzetto21a.html

[21] Mona Nashaat, Aindrila Ghosh, James Miller, and Shaikh Quader. 2020. Asterisk:
Generating Large Training Datasets with Automatic Active Supervision. ACM
Transactions on Data Science 1, 2 (2020), 1–25.

[22] Mona Nashaat, Aindrila Ghosh, James Miller, Shaikh Quader, Chad Marston,
and J. Puget. 2018. Hybridization of Active Learning and Data Programming for
Labeling Large Industrial Datasets. 2018 IEEE International Conference on Big
Data (Big Data) (2018), 46–55.

