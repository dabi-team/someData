Machine-Learned Molecular Surface and Its Application to Implicit 
Solvent Simulations 

Haixin Wei, Zekai Zhao, and Ray Luo 

Departments of Materials Science and Engineering, Molecular Biology and Biochemistry, 
Chemical and Biomolecular Engineering, and Biomedical Engineering, Graduate Program in 
Chemical and Materials Physics, University of California, Irvine, California 92697, United States 

Abstract 

Implicit solvent models, such as Poisson-Boltzmann models, play important roles in 
computational studies of biomolecules. A vital step in almost all implicit solvent models is to 
determine the solvent-solute interface, and the solvent excluded surface (SES) is the most 
widely used interface definition in these models. However, classical algorithms used for 
computing SES are geometry-based, thus neither suitable for parallel implementations nor 
convenient for obtaining surface derivatives. To address the limitations, we explored a machine 
learning strategy to obtain a level-set formulation for the SES. The training process was 
conducted in three steps, eventually leading to a model with over 95% agreement with the 
classical SES. Visualization of tested molecular surfaces shows that the machine-learned SES 
overlaps with the classical SES on almost all situations. We also implemented the machine-
learned SES into the Amber/PBSA program to study its performance on reaction field energy 
calculation. The analysis shows that the two sets of reaction field energies are highly consistent 
with 1% deviation on average. Given its level-set formulation, we expect the machine-learned 
SES to be applied in molecular simulations that require either surface derivatives or high 
efficiency on parallel computing platforms. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
1. Introduction 

Electrostatic interactions play crucial roles in biophysical processes such as protein and RNA 
folding, enzyme catalysis, and molecular recognition. Thus, accurate and efficient treatment of 
electrostatics is vital to computational studies of biomolecular structures, dynamics, and 
functions. A closely related issue is the modeling of water molecules and their electrostatic 
interactions with biomolecules that must be considered for any realistic representation of 
biomolecules at physiological conditions. Implicit solvent model has been such an attempt, in 
which, the solute molecule is treated as a low dielectric constant region with a number of point 
charges located at atomic centers, and the solvent is treated as a high dielectric constant 
region. Among all the attempts, Poisson-Boltzmann equation (PBE) based implicit solvent 
models have proven to be among the most successful ones and are widely used in 
computational studies of biomolecules. 

A crucial component of all implicit solvent models within the PBE framework is the dielectric 
model, i.e. the dielectric constant distribution of a given solution system. Typically, a solution 
system is divided into the low dielectric interior and the high dielectric exterior by a molecular 
surface. That is to say that the molecular surface is used as the dielectric interface between the 
two piece-wise dielectric constants. The solvent excluded surface (SES)1-3 is the most used 
surface definition.4, 5 Indeed, previous comparative analyses of PBE-based solvent models and 
TIP3P solvent models show that the SES definition is reasonable in calculation of reaction field 
energies and electrostatic potentials of mean force of hydrogen-bonded and salt-bridged 
dimers with respect to the TIP3P explicit solvent.6-8 Given the complexity of the SES, one 
possible approach in adapting the SES in numerical solutions is to build the molecular surface 
analytically and then to map it onto a grid,9-11 since analytical procedures can be time 
consuming. In fact, the analytical algorithms are mostly used in the visualization of SES.12-22 
Later, Rocchia et al. subsequently simplified the algorithm to facilitate the mapping of the SES 
to the grid.5 This method is much faster, though it is less accurate and without an analytical 
expression.  

The van der Waals (VDW) surface, or the hard sphere surface, represents the low-dielectric 
molecular interior as a union of atomic van der Waals spheres. This is a very efficient algorithm, 
though there exist many nonphysical high (solvent) dielectric pockets inside the solute interior 
when the VDW definition is used. Considering the limitation, the modified VDW definition was 
proposed. The basic idea of the modified VDW definition is to use the solvent accessible surface 
(SAS) definition for fully buried atoms and the VDW definition for fully exposed atoms.23 
However, the method is difficult to be optimized to reproduce the more physical SES definition.  

The density approaches have recently been developed and can be used for numerical PBE 
solutions. Either a Gaussian-like function or a smoothed step function has been explored in 
previous developments.24, 25 It has been shown that if the functional form is allowed to change, 
the density function can be explicitly optimized to reproduce the classical SES definition at least 
for certain “small” solvent probe radii, though this cannot be generalized to arbitrary probe 
radius values.26 In this type of approaches, a distance-dependent density/volume exclusion 

 
 
 
 
function is used to define each atomic volume or the dielectric constant directly.27-29 This is in 
contrast to the hard-sphere definition of atomic volume as in the VDW or the SES definition. 
Therefore, all surface cusps are removed by the use of smooth density functions.24, 25  

In this study, we intend to address the difficulty of computing SES of complex molecular 
structures. The difficulties are due to the fact that SES is formed by different patches thus its 
analytical formulation must be piecewise and localized.30 To overcome the difficulty, our 
solution is to use a machine learned analytical function. Indeed, in recent years, deep learning 
techniques have been widely adapted in handling multi-variable and highly non-linear functions 
similarly challenging such as SES. Deep learning/neural network is inspired by and resembles 
the human brain. The input variables are multiplied by the respective weights and then 
undergoes a transformation based on an activation function to obtain the outputs.31 It has been 
mathematically proven that a single hidden layer was able to solve any continuous problem.32 
With all its advantages, deep learning has been applied in various biological fields, including 
gene expression33-40, protein secondary and tertiary structures41-53, protein-protein 
interactions54-60, and others.61 

In this study, we explored an SES method via the machine learning strategy, which is explicitly 
expressed as a level set function of atomic positions and radii. The level-set formulation is very 
convenient for many applications where surface curvatures or other higher-order surface 
parameters are needed. In addition, the new algebraic formulation is naturally suitable for 
parallel computing, like on the GPU platform, which would accelerate the implementation even 
further. 

2. Method 

2.1. Finite Difference Method for Solving PBE 

As widely adopted for numerically solving partial differential equations, the finite difference 
method uses a uniform Cartesian grid to discretize the PBE. The grid points are numbered as 
(𝑖, 𝑗, 𝑘), where 𝑖 = 1, … , 𝑥+, 𝑗 = 1, … , 𝑦+, 𝑘 = 1, … , 𝑧+, and 𝑥+, 𝑦+ and 𝑧+ are the numbers of 
points along the three axes. The grid spacing between neighboring points can be uniformly set 
to h. To discretize the linearized PB equation, 

∇ ∙ 𝜀∇𝜙 − 𝜆 ∑ 𝑛6𝑞6

6

8𝜙/𝑘𝑇

= −4𝜋𝜌                                                                                                        (1) 

where the charge density 𝜌 can be expressed as 𝑞(𝑖, 𝑗, 𝑘)/ℎ?, 𝑞(𝑖, 𝑗, 𝑘) is the total charge within 
the cubic volume centered at (𝑖, 𝑗, 𝑘), 𝜆 is a masking function for the Stern layer. In the salt 
	  is the charge 
related term, 𝑛6 is the number density of the ion of type i in the bulk solution, 𝑞6
of the ion of type i, k is the Boltzmann constant, and T is the temperature. The final discretized 
PDE can be expressed as follows, if we ignore the salt related term as it is often modeled to be 
away from the molecular surface, 

 
 
 
 
 
 
 
 
{	𝜀 B𝑖 −

+𝜀 B𝑖 +

1
2
1
2

+𝜀 B𝑖, 𝑗 −

, 𝑗, 𝑘D [𝜙(𝑖 − 1, 𝑗, 𝑘) − 𝜙(𝑖, 𝑗, 𝑘)] 

, 𝑘D [𝜙(𝑖, 𝑗 − 1, 𝑘) − 𝜙(𝑖, 𝑗, 𝑘)] 

, 𝑗, 𝑘D [𝜙(𝑖 + 1, 𝑗, 𝑘) − 𝜙(𝑖, 𝑗, 𝑘)] 
1
2
1
2

+𝜀 B𝑖, 𝑗 +

, 𝑘D [𝜙(𝑖, 𝑗 + 1, 𝑘) − 𝜙(𝑖, 𝑗, 𝑘)] 
1
D [𝜙(𝑖, 𝑗, 𝑘 − 1) − 𝜙(𝑖, 𝑗, 𝑘)] 
+𝜀 B𝑖, 𝑗, 𝑘 −
2
+𝜀 H𝑖, 𝑗, 𝑘 + I
J [𝜙(𝑖, 𝑗, 𝑘 + 1) − 𝜙(𝑖, 𝑗, 𝑘)]}/ℎ8 = −4𝜋𝑞(𝑖, 𝑗, 𝑘)/ℎ?,                                              (2) 
8

where 𝜙(𝑖, 𝑗, 𝑘) is the potential at grid (𝑖, 𝑗, 𝑘), and 𝜀 H𝑖 − I
8
the mid-point of the grids (𝑖, 𝑗, 𝑘) and (𝑖 − 1, 𝑗, 𝑘). All other 𝜙 and 𝜀 are defined similarly here.  

, 𝑗, 𝑘J is the dielectric constant at 

However, the simple discretization scheme above can be applied only to situations where the 
grid point and all its six neighbors are in the same region (regular grid points), either in solvent 
or in solute. Complication arises when at least one of the six neighboring grid points belongs to 
the different region (irregular grid points), i.e., at least one of the neighboring grid edges 
intersects the solute/solvent interface. Thus, how to define and determine the interface 
parameters is a key factor for setting up the linearized PBE. Without question, SES is the most 
adapted interface definition, as it was found to reproduce both energetic and dynamic 
properties of solvated molecules in explicit solvent.6-8, 26 After choosing an interface definition, 
the discretization can then be handled with many different schemes. For example, the widely 
used harmonic average methods in a class of such strategies widely used in biomolecular 
simulations.62-64 

2.2. Level Set Functions for Interface Definition 

It is often more convenient to use a level set function to represent an interface. The level set 
function is often in the form of a second-order continuous distribution function 𝜑, satisfying,  

𝜑(𝑥, 𝑦, 𝑧) < 0, 𝑤ℎ𝑒𝑛	(𝑥, 𝑦, 𝑧) ∈ ΩS  
𝜑(𝑥, 𝑦, 𝑧) = 0, 𝑤ℎ𝑒𝑛	(𝑥, 𝑦, 𝑧) ∈ Γ  
𝜑(𝑥, 𝑦, 𝑧) > 0, 𝑤ℎ𝑒𝑛	(𝑥, 𝑦, 𝑧) ∈ ΩV                                                                                                        (3) 

Here, Γ represents the interface, ΩS and ΩV denote the inside and outside regions, 
respectively. For example, a signed distance function representing a sphere interface, centered 
at (𝑥W, 𝑦W, 𝑧W) with a radius of R can be expressed as, 

𝜑(𝑥, 𝑦, 𝑧) = X(𝑥 − 𝑥W)8+(𝑦 − 𝑦W)8 + (𝑧 − 𝑧W)8 − 𝑅.                                                                      (4) 

 
 
 
 
 
 
 
 
Not only level set functions can be used to determine the interface by setting up 𝜑 = 0, but 
also it can be used to compute various interface parameters, such as the normal and tangential 
directions on the interface, 

𝝃 = [𝜑\, 𝜑], 𝜑^_  
𝜼 = (𝜑], −𝜑\, 0)  
𝝉 = [𝜑\𝜑^, 𝜑]𝜑^, −𝜑\

8 − 𝜑]

8_                                                                                                                 (5) 

Here, 𝝃, 𝜼 and 𝝉 are the normal and two tangential directions, respectively. In addition, some 
advanced discretization schemes, like IIM,65-68 also require higher-order interface parameters, 
such as surface curvatures that can also be computed as follows, 

d𝜼𝜼
𝜉cc = −
d𝝃
𝜉ee = − d𝝉𝝉
d𝝃

𝜉ce = −

dfg
dh

                                                                                                                                                  (6) 

In summary, level set functions are convenient mathematical tools in handling interface-related 
problems: they can be used to obtain the interface itself and all its geometry parameters in a 
straightforward manner. Thus, it would greatly benefit discretization of the PBE if we can 
express SES as a level set function. However, construction steps of SES are simply too complex 
to be reproduced algebraically. What we are trying to do here is to explore how to approximate 
SES with a level set function that is learned on modern computers. 

2.3. Deep Learning and Neural Network 

Artificial neural networks (ANNs), usually simply called neural networks, are mathematical 
models that have been motivated by the brain function.31, 32 A simple example of an ANN 
structure is shown below in Figure 1. 

The basic idea of a neuron model is that an input, x, together with a bias, b, is weighted by w, 
and then summed together69, as shown schematically in Figure 1. The bias, b, is a scalar value 
whereas the input x and the weights w are vectors, i.e., x ∈ ℝn and w ∈ ℝn with n ∈ ℕ 
corresponding to the dimension of the input. The sum of these terms, i.e. z = wTx + b, forms the 
argument of an activation function, f(), resulting in the output of the neuron model, 

𝑦 = 𝑓(𝑧) = 𝑓(𝒘m𝒙	 + 	𝑏)                                                                                                                        (7) 

 
 
 
  
  
 
 
 
 
 
Figure 1. Structure of an artificial neural network with one hidden layer. Here, each circular 
node represents an artificial neuron, and an arrow represents a connection from the output of 
one artificial neuron to the input of another. 

The role of the activation function, f(), is to perform a non-linear transformation of z. There are 
many activation functions in practice, such as sigmoid, Tanh, ReLU, Leaky ReLU, and so on, and 
the ReLU activation function is usually the most popular activation function for deep neural 
networks.69 

Though its structure may seem highly complicated, a neural network can be viewed as a 
combination of simple elementary functions. Thus, a neural network is differentiable to any 
order, as all its component functions are differentiable to any order, except for a countable 
number of points. This is an important reason why we have chosen a machine learning 
approach to express the SES level set function, because it ensures that the SES level set function 
can be used to compute surface derivatives to any order as needed. 

Due to its excellent mathematical properties, ANNs have shown great promises in a range of 
applications.61 Most ANN applications fall into two categories: function 
approximation/regression analysis and classification problems. In this study, the ANN is applied 
as a classification tool because the goal is to know which region a grid point belongs to (solvent 
or solute region) in a PBE system. 

3. Computational Details 

The training data were generated from the Amber PBSA benchmark suite of 573 biomolecular 
structures.70 A modified PBSA program was used to print out the training data. The default 
atomic cavity radii were read from the topology files. The solvent probe radius was set to 1.4 Å, 
the grid spacing to 0.95 Å, and all other parameters remain as default in the PBSA module in the 
Amber 20 package.71 The training software package is Keras in TensorFlow, version 2.2.4.72 
Adam was chosen as the optimizer, and the squared hinge function as the loss function. The 
partition ratio of training and validating sets is 9:1, and an early stopping criterion of 100 
epochs was used. 

 
 
 
 
 
 
 
The training data contains two parts: labels or the target values, and the variables. The labels 
are integer values of irregular grid points derived from the Amber PBSA routine, which are 
calculated through a geometry-based SES algorithm.63 The irregular points of the outside 
(solvent) region are labeled as -1, and those of the inside (solute) region are labeled as +1. The 
variables of the training data are the tuples of coordinates and the radii of those atoms near 
the irregular grid points, with the unit of Å. An atom is considered “near” a grid point if the 
distance between the grid point and the atom is within R+2Dp, where R is the atom radius and 
Dp is the diameter of the probe. The coordinates of the atoms are expressed in the relative 
frame whose origin is the grid point of interest. The nearby atoms were sorted based on their 
distances to the grid point before training. In the training data, the maximum nearby atoms are 
48, so the maximum number of variables are 192. Overall, there are about 6 million entries of 
data generated from the Amber PBSA benchmark suite, which was separated into 6 subsets, 
each about 1 million entries. 

Figure 2. Incremental training of the model. The ANN model was set up with different numbers 
of hidden layers, each hidden layer is of 200 neurons. The model was trained with the first 
training data set. The analysis was also repeated with two additional training sets and the 
figures are shown in the Supplemental Materials Figure S1. 

The training was conducted in three steps. First, the model was trained incrementally by adding 
one atom at a time, which means in the first round of training, the model was fed with only the 
first (nearest) atom’s coordinates and radius. After the training reaches stabilization, the model 
was fed with the first two atoms’ data, and so on. After enough number of atoms were fed to 
the atom, the accuracy of the model no longer improves. We then fed it with all the nearby 
atoms. In this step, we also investigated how many hidden layers are needed to reach stable 
prediction accuracy, starting with one layer to five layers. As shown in the Figure 2, the 

 
 
 
accuracy of the model becomes stable at 16 atoms, no matter how many hidden layers were 
used in the ANN. Therefore after 16 atoms, all available nearby atoms were fed to the model. 
Figure 2 further shows that 2 hidden layers are already very good for our problem.  

The second step was to determine how many neurons are needed for the hidden layers. To 
determine the neurons needed in the first hidden layer, the number of neurons of both layers 
are varied, and the results are shown in Figure 3(a). Clearly, for the three tested training sets, all 
show that 100-neurons perform the best. Next, the first hidden layer was kept at 100 neurons, 
and the second hidden layer was varied, and the analysis is shown in Figure 3(b). Figure 3(b) 
shows that all tested conditions perform similarly, with 40 neurons slightly better than others 
so 40 neurons were selected. 

The third step was to finalize the model. Given the model structure, which contains two hidden 
layers, with 100 neurons and 40 neurons respectively, we trained the model with all the 
training sets. To minimize GPU memory usage, the model was trained sequentially with one 
training set at a time. For the first training set, the model was trained incrementally by adding 
one atom at a time as discussed above. After the first training set, all nearby atoms were fed to 
the model in the subsequent training of remaining five sets. The training order among the 
training sets is irrelevant, because all training sets perform extremely similarly (for example 
Figures 2 and 3 and Figure S1). The training script and final model can be found at https:// 
http://rayluolab.org/ml-ses/. 

 
 
 
 
 
 
 
 
(a) 

(b) 

Figure 3. Determining of the number of neurons needed in the two hidden layers. (a) First layer 
analysis in the top panel. (b) Second layer analysis in the bottom panel.  

4. Results and Discussion 

 
 
 
 
 
4.1. Model Accuracy Analysis 

To evaluate the overall performance of the machine-learned SES method, we first did an 
accuracy test on different structures. The test data sets were generated in the same fashion as 
the training data sets. Except for the original training protein monomers, we also included two 
extra sets of biomolecules for this analysis, nucleic acid structures73 and protein/protein 
complex structures from previous PBSA developments.74 The testing results are shown in Table 
1. 

Training 
Data 
data 1 
sets 
Accuracy  95.96% 

Training 
data 2 
96.01% 

Training 
data 3 
96.02% 

Training 
data 4 
96.03% 

Training 
data 5 
96.26% 

Training 
data 6 
97.78% 

Protein 
Nucleic 
complex 
acid 
97.07%  95.16% 

Table 1. Model accuracy with different structural data sets. The first six data sets are those from 
the training phase, and the last two sets are not included in the training phase. 

Table 1 shows that the machine-learned SES performs very well for all structural sets, including 
those not included in training. The classification accuracies are basically around 96% for all data 
sets, with a variance about 1%. The method performs the best for training data set 6 and the 
nucleic acid data set. This is because those two contains mostly smallest molecules among all 
tested molecules. In general, a small molecule has simpler surface geometry so it is easier to 
predict their geometries. On the contrary, the protein/protein complex structures are much 
larger and thus have more complicated surfaces, so the method performs slightly worse on the 
data set, but still obtains an over 95% accuracy. 

Another point worth mentioning is that the accuracy test clearly shows excellent transferability 
of the method, from the smaller training protein monomers to the nucleic acid structures and 
the larger protein/protein complex structures. The testing molecules overall do not resemble 
those in the training sets, such as the nucleic acids. Nevertheless, the method can successfully 
predict the SES of those unseen structures, showing that the method has indeed learned the 
fundamental rules for predicting the SES surface. In summary, the consistent accuracy confirms 
that the machine-learned SES method can be applied to typical computational analyses of 
biomolecules. 

4.2. Reproducing Classical SES Molecular Surfaces 

We chose six representative biomolecules and compared their molecular interfaces generated 
with three different methods: the newly developed machine-learned SES, the classical 
geometry-based SES, and a revised density function surface.26 The superimposed surfaces of 
machine-learned SES and the classical SES are shown in Figure 4. The superimposed surfaces of 
machine-learned SES and the density function are shown in Figure S2 in the Supplemental 
Materials. Standalone surfaces from all three methods are also included in Figure S3-S5. 

 
 
 
 
 
 
 
(a) 

(b) 

(c) 

(d) 

(e) 

(f) 

Figure 4.  Superimposed rendering of machine-learned SES surface (blue) and the classical SES 
surface (red) of representative molecules. (a) PDB ID: 1enh, all-alpha protein; (b) PDB ID: 1pgb, 
all-beta protein; (c) PDB ID: 1shg, alpha/beta protein; (d) PDB ID: 1w0u, protein/protein 
complex; (e) PDB ID: 3czw, RNA duplex; (f) PDB ID: 3fdt, protein/DNA complex. 

It is clear from Figure 4 that the machine-learned SES excellently reproduces the classical SES 
for all tested molecules, including both proteins and nucleic acids, consistent with the accuracy 
analysis shown in section 4.1. On the contrary, Figure S2 shows that the density function does 
not perform well in reproducing the classical SES for the tested systems. This is because the 
density function was found to agree well with the classical SES only with a smaller solvent 
probe, i.e., 0.7 Å. At the default solvent probe of 1.4 Å, the density surfaces are uniformly 
“fatter” than SES surfaces. Another limitation in the revised density function surface is that 
there are often deep reentry surface patches with very large curvature, which would cause 
numerical instability when it is used in numerical PB continuum solvents.68 

 
 
 
 
 
 
 
 
 
(a) 

(b) 

(c) 

(d) 

Figure 5. PB electrostatic solvation energies with different molecular surfaces. (a) Correlation 
between energies from machine-learned SES and classical SES. (b) Correlation between 
energies from density function and classical SES. (c) Energy differences in (a). (d) Energy 
differences in (b). The lines in (a) and (b) are the diagonal line “y=x” as reference.  

4.3. Application to Poisson-Boltzmann Modeling  

To illustrate the applications of the machine-learned SES for biomolecular studies, we 
implemented it into the AMBER/PBSA program. The PB reaction field energies were computed 
with the classical SES, the machine-learned SES, and the density function with otherwise 

 
 
 
 
 
 
 
identical conditions for all protein structures in the AMBER/PBSA benchmark suite.70 Figure 5 
shows the agreement between the computed energies with both machine-learned SES and the 
density function surfaces and those with the classical SES surface. 

It is clear from Figure 5 that the machine-learned SES performs uniformly well when applied to 
the AMBER/PBSA program. The deviations between the PB energies with the machine-learned 
SES and those with the classical SES are around 1%. In contrast, the average deviation is roughly 
2% but the maximum deviation is over 10% between the PB energies with the density function 
surface and those with the classical SES. Figure 5c/d further shows that the PB energies from 
the machine-learned SES are more random around the mean of zero but the PB energies from 
the density function are systematically more negative. This is because a small solvent probe (0.7 
Å) has to be used in the density function to achieve reasonable agreement with the PB energies 
with the classical SES.26 If the default probe of 1.4 Å is used, the PB energies would be 30% 
more positive than those with the classical SES. 

4.4 Can machine learning extrapolate? 

In this section, we want to discuss an interesting phenomenon observed during the training of 
the machine-learned SES. Since the ANN model was trained with irregular points near the 
solute-solvent interface (see section 3), it is obvious that the model performs well nearby the 
interface. However, it is unclear how the model performs away from the interface, for example 
at solvent-exposed grid points far away from the molecule or buried grid points deep inside the 
molecule interior, because we did not feed the model with such grid points and the model has 
never seen those situations. 

Our initial impression was that the machine-learned SES model should also yield correct 
classification of the grid points in regions far away from the interface. The reasoning is simple 
as even an untrained student can classify these grid points without any calculation. If the model 
can successfully classify the most difficult irregular points, those far from the interface should 
be no problem. In another word, our machine-learned SES model should be able to extrapolate 
a little bit, or the model has obtained a certain level of “intelligence”. The question is whether 
this is true. 

To analyze the performance of the model in the entire solvation box, not just nearby the 
interface, we used the model to classify all grid points, include those grid points that are either 
far away outside the solute or deeply buried inside the solute. The performance of the 
classification is shown in Figure 6. 

 
 
 
 
 
(a) 

(b) 

(c) 

(d) 

Figure 6. Detailed views of SES as predicted by first and second versions of the machine-learned 
SES model. The PDB id of the tested molecule is 1shg. (a) and (c) are from the first version of 
the model, outside vision and the inside vision, respectively. (b) and (d) are from a second 
version of the model, outside vision and the inside vision, respectively. 

 
 
 
 
 
Figure 6 shows that the model clearly does not work at grid points far away from the interface. 
Specifically, Figure 6(a) illustrates the presence of an “island” or a small cluster of predicted 
interface grid points, outside the interface. Even more serious failures are within the molecule 
interior as illustrated in Figure 6(c), where hundreds of small clusters of interface grid points are 
wrongly predicted. This is surprising because the model can predict with an accuracy level over 
95% in the most difficult interface region yet made hundreds of mistakes in the much easier 
regions. 

These failures are due to the lack of similar data in the training of our initial model as only 
irregular grid points were fed to the training of the model. Thus, the remedy is to assembly a 
supplementary training set to include those grid points far from the interface (both inside and 
outside the molecule). Training of the initial model on the new data set leads to the second 
version of the model. Without surprise, the second version of the model is able to successfully 
classify those grid points incorrectly predicted by the initial model as shown in Figure 6(b)/6(d). 
Of course, it is never perfect and there are still few “islands” left as in Figure 6(d), but they can 
be ignored in most molecular modeling. 

Our training experience of the SES model shows that, at least for the neural network we built, 
the machine-learned model does not possess any “intelligence”. In other words, if we want the 
model work on grid points in a particular environment, we must train it in that exact 
environment. Basically, the model cannot extrapolate, even for the tested trivial situations. 
Thus, instead of artificial intelligence, “artificial memory” seems a more suitable terminology 
for the ANN model utilized here. 

5. Conclusion 

In this study, we developed a new level-set formulation to generate the solvent excluded 
surface through a machine learning process. SES is a widely used molecular surface definition 
and there are at least two advantages of expressing SES in a level set formulism: to facilitate 
parallel computing on GPU and to make it differentiable for future developments. 

The level set was trained on the data generated from a set of heterogenous biomolecular 
structures, containing about 6 million entries. The training was conducted in three steps to 
determine the numbers of nearby atoms needed to define the level set, the number of hidden 
layers, and the number of neurons of each layer. After the final training, the machine-learned 
SES can predict the classical SES with an accuracy over 95%, on tested proteins, nucleic acids, 
and complex structures. 

Analysis of visualized molecular surfaces of tested biomolecules shows that the machined-
learned SES agrees excellently with the classical SES for the tested cases. It is also clear that a 
previous approach based on atomic density functions is clearly insufficient, generating a 
molecular surface often larger than the classical SES and causing deeper crevices in the reentry 
regions. 

 
 
 
 
 
 
 
To further test the machine-learned SES, we implemented it into the AMBER/PBSA program to 
see if it can be used to reproduce the reaction field energies computed with the classical SES. 
Our results showed that the two sets of energies differ on average only about 1%, better than 
the 2% average deviation if the energies between the density-function surface and the classical 
SES are compared. Furthermore, the deviations in energies by the machine-learned SES are 
uniformly distributed yet the deviations in energies by the density-function surface often 
exhibit large deviations as high as 10%. This shows that the machine-learned SES is much more 
stable in reproducing the classical SES. 

Finally, we highlight an interesting phenomenon in our training process: our testing shows that 
the model cannot classify grid points at locations that are not covered during the training 
process. To overcome the limitation, a supplementary training was conducted by feeding a new 
data set including the new locations. This finding shows that the ANN model cannot 
extrapolate. The observation should be kept in mind when future application of ANN models is 
used in structural analyses of biomolecules. 

6. Acknowledgements 

This work was supported by National Institute of Health/NIGMS (Grant Nos. GM093040 and 
GM130367) 

7. Supporting Information 

Supplemental materials include figures for incremental training of the ANN model for the 
second and the third data set; superimposed renderings of density function surfaces and 
classical SES of selected molecules/complexes; renderings of classical SES, machine-learned SES, 
and density function surfaces of selected molecules/complexes.  Animation movies are also 
included for superimposed renderings of machine-learned SES and classical SES of selected 
molecules/complexes and for superimposed renderings of density function surface and classical 
SES of selected molecules/complexes. This information is available free of charge via the 
Internet at http://pubs.acs.org 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
References 

Wang, J.;  Tan, C.;  Chanco, E.; Luo, R., Quantitative analysis of Poisson-Boltzmann 

Richards, F. M., Areas, Volumes, Packing, and Protein-Structure. Annual Review of 

Connolly, M. L., SOLVENT-ACCESSIBLE SURFACES OF PROTEINS AND NUCLEIC-ACIDS. 

Tan, C. H.;  Yang, L. J.; Luo, R., How well does Poisson-Boltzmann implicit solvent agree 

Connolly, M. L., ANALYTICAL MOLECULAR-SURFACE CALCULATION. J. Appl. Crystallogr. 

Swanson, J. M. J.;  Mongan, J.; McCammon, J. A., Limitations of atom-centered dielectric 

1. 
Biophysics and Bioengineering 1977, 6, 151-176. 
2. 
1983, 16 (OCT), 548-558. 
3. 
Science 1983, 221 (4612), 709-713. 
4. 
Gilson, M. K.;  Sharp, K. A.; Honig, B. H., Calculating the Electrostatic Potential of 
Molecules in Solution - Method and Error Assessment. J. Comput. Chem. 1988, 9 (4), 327-335. 
Rocchia, W.;  Sridharan, S.;  Nicholls, A.;  Alexov, E.;  Chiabrera, A.; Honig, B., Rapid grid-
5. 
based construction of the molecular surface and the use of induced surface charge to calculate 
reaction field energies: Applications to the molecular systems and geometric objects. J. Comput. 
Chem. 2002, 23 (1), 128-137. 
6. 
functions in implicit solvent models. J. Phys. Chem. B 2005, 109 (31), 14769-14772. 
7. 
with explicit solvent? A quantitative analysis. Journal of Physical Chemistry B 2006, 110 (37), 
18680-18687. 
8. 
implicit solvent in molecular dynamics simulations. Physical Chemistry Chemical Physics 2010, 
12, In press. 
9. 
Zauhar, R. J.; Morgan, R. S., Computing the Electric-Potential of Biomolecules - 
Application of a New Method of Molecular-Surface Triangulation. J. Comput. Chem. 1990, 11 
(5), 603-622. 
10. 
FOR MOLECULAR-SYSTEMS - HANDLING OF SINGULARITIES AND COMPUTATIONAL-EFFICIENCY. 
J. Comput. Chem. 1993, 14 (11), 1272-1280. 
11. 
THE SOLVENT ACCESSIBILITY OF POINTS IN A 3-DIMENSIONAL LATTICE AROUND A SOLUTE 
MOLECULE. J. Comput. Chem. 1995, 16 (6), 743-757. 
12. 
crystallography 1983, 16 (5), 548-558. 
13. 
Computer Graphics and Applications 1994, 14 (5), 19-25. 
14. 
Graphics (TOG) 1994, 13 (1), 43-72. 
15. 
molecular surface. Journal of structural biology 1996, 116 (1), 138-143. 
16. 
molecular surfaces. Biopolymers 1996, 38 (3), 305-320. 
17. 
IEEE transactions on visualization and computer graphics 2009, 15 (6), 1391-1398. 

Sanner, M. F.;  Olson, A. J.; Spehner, J. C., Reduced surface: an efficient way to compute 

You, T.; Bashford, D., AN ANALYTICAL ALGORITHM FOR THE RAPID-DETERMINATION OF 

Krone, M.;  Bidmon, K.; Ertl, T., Interactive visualization of molecular surface dynamics. 

Varshney, A.;  Brooks, F. P.; Wright, W. V., Computing smooth molecular surfaces. IEEE 

Edelsbrunner, H.; Mücke, E. P., Three-dimensional alpha shapes. ACM Transactions on 

Eisenhaber, F.; Argos, P., IMPROVED STRATEGY IN ANALYTIC SURFACE CALCULATION 

Totrov, M.; Abagyan, R., The contour-buildup algorithm to calculate the analytical 

Connolly, M. L., Analytical molecular surface calculation. Journal of applied 

 
Jurčík, A.;  Parulek, J.;  Sochor, J.; Kozlikova, B. In Accelerated visualization of transparent 

Ye, X.;  Wang, J.; Luo, R., A Revised Density Function for Molecular Surface Calculation in 

Kozlíková, B.;  Krone, M.;  Falk, M.;  Lindow, N.;  Baaden, M.;  Baum, D.;  Viola, I.;  

Grant, J. A.;  Pickup, B. T.; Nicholls, A., A smooth permittivity function for Poisson-

Lu, Q.; Luo, R., A Poisson-Boltzmann dynamics method with nonperiodic boundary 

Lindow, N.;  Baum, D.;  Prohaska, S.; Hege, H. C. In Accelerated visualization of dynamic 

Grant, J. A.; Pickup, B. T., A GAUSSIAN DESCRIPTION OF MOLECULAR SHAPE. Journal of 

18. 
molecular surfaces, Computer Graphics Forum, Wiley Online Library: 2010; pp 943-952. 
19. 
Krone, M.;  Grottel, S.; Ertl, T. In Parallel contour-buildup algorithm for the molecular 
surface, 2011 IEEE Symposium on Biological Data Visualization (BioVis). IEEE: 2011; pp 17-22. 
20. 
Parulek, J.; Viola, I. In Implicit representation of molecular surfaces, 2012 IEEE Pacific 
Visualization Symposium, IEEE: 2012; pp 217-224. 
21. 
molecular surfaces in molecular dynamics, 2016 IEEE Pacific Visualization Symposium 
(PacificVis), IEEE: 2016; pp 112-119. 
22. 
Parulek, J.; Hege, H. C. In Visualization of biomolecular structures: State of the art revisited, 
Computer Graphics Forum, Wiley Online Library: 2017; pp 178-204. 
23. 
condition. Journal of Chemical Physics 2003, 119 (21), 11035-11047. 
24. 
Physical Chemistry 1995, 99 (11), 3503-3510. 
25. 
Boltzmann solvation methods. J. Comput. Chem. 2001, 22 (6), 608-640. 
26. 
Continuum Solvent Models. Journal of Chemical Theory and Computation 2010, 6 (4), 1157-
1169. 
27. 
dielectric function for macromolecular modeling and its implementation in DelPhi. Journal of 
chemical theory and computation 2013, 9 (4), 2126-2136. 
28. 
Smooth Dielectric Function: A Surface-Free Approach for Modeling Macromolecular Binding in 
Solvents. Frontiers in molecular biosciences 2018, 5, 25. 
29. 
Petukh, M.; Li, L., DelPhi Suite: New Developments and Review of Functionalities. Journal of 
Computational Chemistry 2019. 
30. 
Hermosilla, P.;  Krone, M.;  Guallar, V.;  Vázquez, P.-P.;  Vinacua, À.; Ropinski, T., 
Interactive GPU-based generation of solvent-excluded surfaces. The Visual Computer 2017, 33 
(6-8), 869-881. 
31. 
Access 2019, 7, 53040-53065. 
32. 
networks 1991, 4 (2), 251-257. 
33. 
using deep architectures, with an application to gene clustering, 2015 IEEE International 
Conference on Bioinformatics and Biomedicine (BIBM), IEEE: 2015; pp 1328-1335. 
34. 
Chen, L.;  Cai, C.;  Chen, V.; Lu, X. In Learning a hierarchical representation of the yeast 
transcriptomic machinery using an autoencoder model, BMC bioinformatics, Springer: 2016; p 
S9. 

Shrestha, A.; Mahmood, A., Review of deep learning algorithms and architectures. IEEE 

Chakravorty, A.;  Jia, Z.;  Peng, Y.;  Tajielyato, N.;  Wang, L.; Alexov, E., Gaussian-Based 

Gupta, A.;  Wang, H.; Ganapathiraju, M. In Learning structure in gene expression data 

Li, L.;  Li, C.;  Zhang, Z.; Alexov, E., On the dielectric “constant” of proteins: smooth 

Hornik, K., Approximation capabilities of multilayer feedforward networks. Neural 

Li, C.;  Jia, Z.;  Chakravorty, A.;  Pahari, S.;  Peng, Y.;  Basu, S.;  Koirala, M.;  Panday, S. K.;  

Liang, M.;  Li, Z.;  Chen, T.; Zeng, J., Integrative data analysis of multi-platform cancer 

Singh, R.;  Lanchantin, J.;  Sekhon, A.; Qi, Y. In Attend and predict: Understanding gene 

Singh, R.;  Lanchantin, J.;  Robins, G.; Qi, Y., DeepChrome: deep-learning for predicting 

Chen, Y.;  Li, Y.;  Narayan, R.;  Subramanian, A.; Xie, X., Gene expression inference with 

Jones, D. T.;  Singh, T.;  Kosciolek, T.; Tetchner, S., MetaPSICOV: combining coevolution 

Tan, J.;  Doing, G.;  Lewis, K. A.;  Price, C. E.;  Chen, K. M.;  Cady, K. C.;  Perchuk, B.;  Laub, 

Tan, J.;  Hammond, J. H.;  Hogan, D. A.; Greene, C. S., Adage-based integration of 
35. 
publicly available pseudomonas aeruginosa gene expression data with denoising autoencoders 
illuminates microbe-host interactions. MSystems 2016, 1 (1). 
36. 
M. T.;  Hogan, D. A.; Greene, C. S., Unsupervised extraction of functional gene expression 
signatures in the bacterial pathogen Pseudomonas aeruginosa with eADAGE. bioRxiv 2016, 
078659. 
37. 
deep learning. Bioinformatics 2016, 32 (12), 1832-1839. 
38. 
gene expression from histone modifications. Bioinformatics 2016, 32 (17), i639-i648. 
39. 
regulation by selective attention on chromatin, Advances in neural information processing 
systems, 2017; pp 6785-6795. 
40. 
data with a multimodal deep learning approach. IEEE/ACM transactions on computational 
biology and bioinformatics 2014, 12 (4), 928-937. 
41.  Wang, S.;  Sun, S.; Xu, J. In AUC-Maximized deep convolutional neural fields for protein 
sequence labeling, Joint European Conference on Machine Learning and Knowledge Discovery 
in Databases, Springer: 2016; pp 1-16. 
42. 
methods for accurate prediction of contacts and long range hydrogen bonding in proteins. 
Bioinformatics 2015, 31 (7), 999-1006. 
43.  Weigt, M.;  White, R. A.;  Szurmant, H.;  Hoch, J. A.; Hwa, T., Identification of direct 
residue contacts in protein–protein interaction by message passing. Proceedings of the National 
Academy of Sciences 2009, 106 (1), 67-72. 
44.  Marks, D. S.;  Colwell, L. J.;  Sheridan, R.;  Hopf, T. A.;  Pagnani, A.;  Zecchina, R.; Sander, 
C., Protein 3D structure computed from evolutionary sequence variation. PloS one 2011, 6 (12), 
e28766. 
45. 
local protein properties. PloS one 2012, 7 (3), e32235. 
46. 
Yang, Y.; Zhou, Y., Improving prediction of secondary structure, local backbone angles and 
solvent accessible surface area of proteins by iterative deep learning. Scientific reports 2015, 5 
(1), 1-11. 
47. 
matrices. Journal of molecular biology 1999, 292 (2), 195-202. 
48. 
network for protein secondary structure prediction, International conference on machine 
learning, PMLR: 2014; pp 745-753. 
49.  Ma, J.;  Wang, S.;  Wang, Z.; Xu, J., Protein contact prediction by integrating joint 
evolutionary coupling analysis and supervised learning. Bioinformatics 2015, 31 (21), 3506-
3513. 

Qi, Y.;  Oja, M.;  Weston, J.; Noble, W. S., A unified multitask architecture for predicting 

Jones, D. T., Protein secondary structure prediction based on position-specific scoring 

Heffernan, R.;  Paliwal, K.;  Lyons, J.;  Dehzangi, A.;  Sharma, A.;  Wang, J.;  Sattar, A.;  

Zhou, J.; Troyanskaya, O. In Deep supervised and convolutional generative stochastic 

Peng, Y.; Lu, Z., Deep learning for extracting protein-protein interactions from 

Sun, T.;  Zhou, B.;  Lai, L.; Pei, J., Sequence-based prediction of protein protein 

Du, X.;  Sun, S.;  Hu, C.;  Yao, Y.;  Yan, Y.; Zhang, Y., DeepPPI: boosting prediction of 

AlQuraishi, M., End-to-end differentiable learning of protein structure. Cell systems 

Skwark, M. J.;  Raimondi, D.;  Michel, M.; Elofsson, A., Improved contact predictions 

De Las Rivas, J.; Fontanillo, C., Protein–protein interactions essentials: key concepts to 

Eickholt, J.; Cheng, J., Predicting protein residue–residue contacts using deep networks 

Zhou, D.; He, Y., Extracting interactions between proteins from the literature. Journal of 

Di Lena, P.;  Nagata, K.; Baldi, P., Deep architectures for protein contact map prediction. 

50. 
Bioinformatics 2012, 28 (19), 2449-2457. 
51. 
and boosting. Bioinformatics 2012, 28 (23), 3066-3072. 
52. 
using the recognition of protein like contact patterns. PLoS Comput Biol 2014, 10 (11), 
e1003889. 
53. 
2019, 8 (4), 292-301. e3. 
54. 
building and analyzing interactome networks. PLoS Comput Biol 2010, 6 (6), e1000807. 
55. 
biomedical informatics 2008, 41 (2), 393-407. 
56. 
biomedical literature. arXiv preprint arXiv:1706.01556 2017. 
57. 
protein–protein interactions with deep neural networks. Journal of chemical information and 
modeling 2017, 57 (6), 1499-1510. 
58. 
interaction using a deep-learning algorithm. BMC bioinformatics 2017, 18 (1), 1-8. 
59.  Wang, Y.-B.;  You, Z.-H.;  Li, X.;  Jiang, T.-H.;  Chen, X.;  Zhou, X.; Wang, L., Predicting 
protein–protein interactions from protein sequences by a stacked sparse autoencoder deep 
neural network. Molecular BioSystems 2017, 13 (7), 1336-1344. 
60. 
protein-protein interaction with Fisher score features and deep learning. Methods 2016, 110, 
97-105. 
61. 
Ferrero, E.;  Agapow, P.-M.;  Zietz, M.; Hoffman, M. M., Opportunities and obstacles for deep 
learning in biology and medicine. Journal of The Royal Society Interface 2018, 15 (141), 
20170387. 
62. 
Davis, M. E.; Mccammon, J. A., Dielectric Boundary Smoothing in Finite-Difference 
Solutions of the Poisson Equation - an Approach to Improve Accuracy and Convergence. J 
Comput. Chem. 1991, 12 (7), 909-912. 
63.  Wang, J.;  Cai, Q.;  Xiang, Y.; Luo, R., Reducing Grid Dependence in Finite-Difference 
Poisson-Boltzmann Calculations. Journal of Chemical Theory and Computation 2012, 8 (8), 2741-
2751. 
64.  Wei, H.;  Luo, A.;  Qiu, T.;  Luo, R.; Qi, R., Improved Poisson–Boltzmann Methods for 
High-Performance Computing. Journal of Chemical Theory and Computation 2019, 15 (11), 
6190-6202. 
65. 
interfaces and irregular domains. Siam: 2006; Vol. 33. 
66.  Wang, J.;  Cai, Q.;  Li, Z.-L.;  Zhao, H.-K.; Luo, R., Achieving energy conservation in 
Poisson–Boltzmann molecular dynamics: Accuracy and precision with finite-difference 
algorithms. Chemical physics letters 2009, 468 (4-6), 112-118. 

Li, Z.; Ito, K., The immersed interface method: numerical solutions of PDEs involving 

Du, T.;  Liao, L.;  Wu, C. H.; Sun, B., Prediction of residue-residue contact matrix for 

Ching, T.;  Himmelstein, D. S.;  Beaulieu-Jones, B. K.;  Kalinin, A. A.;  Do, B. T.;  Way, G. P.;  

Case, D. A.;  Belfon, K.;  Ben-Shalom, I. Y.;  Brozell, S. R.;  Cerutti, D. S.;  T.E. Cheatham, I.;  

67.  Wang, C.;  Wang, J.;  Cai, Q.;  Li, Z.;  Zhao, H.-K.; Luo, R., Exploring accurate Poisson–
Boltzmann methods for biomolecular simulations. Computational and Theoretical Chemistry 
2013, 1024, 34-44. 
68.  Wei, H.;  Luo, R.; Qi, R., An efficient second-order poisson–boltzmann method. Journal 
of computational chemistry 2019. 
69. 
Emmert-Streib, F.;  Yang, Z.;  Feng, H.;  Tripathi, S.; Dehmer, M., An introductory review 
of deep learning for prediction models with big data. Frontiers in Artificial Intelligence 2020, 3, 
4. 
70.  Wang, J.; Luo, R., Assessment of Linear Finite-Difference Poisson-Boltzmann Solvers. 
Journal of Computational Chemistry 2010, 31 (8), 1689-1698. 
71. 
Cruzeiro, V. W. D.;  Darden, T. A.;  Duke, R. E.;  Giambasu, G.;  Gilson, M. K.;  Gohlke, H.;  Goetz, 
A. W.;  Harris, R.;  Izadi, S.;  Izmailov, S. A.;  Kasavajhala, K.;  Kovalenko, A.;  Krasny, R.;  
Kurtzman, T.;  Lee, T. S.;  LeGrand, S.;  Li, P.;  Lin, C.;  Liu, J.;  Luchko, T.;  Luo, R.;  Man, V.;  Merz, 
K. M.;  Miao, Y.;  Mikhailovskii, O.;  Monard, G.;  Nguyen, H.;  Onufriev, A.;  Pan, F.;  Pantano, S.;  
Qi, R.;  Roe, D. R.;  Roitberg, A.;  Sagui, C.;  Schott-Verdugo, S.;  Shen, J.;  Simmerling, C. L.;  
Skrynnikov, N. R.;  Smith, J.;  Swails, J.;  Walker, R. C.;  Wang, J.;  Wilson, L.;  Wolf, R. M.;  Wu, X.;  
Xiong, Y.;  Xue, Y.;  York, D. M.; Kollman, P. A. Amber 2020; University of California, San 
Francisco: 2020. 
72. 
73. 
Poisson-Boltzmann Solvers. Journal of Chemical Theory and Computation 2010, 6 (1), 203-211. 
74. 
Difference Poisson-Boltzmann Methods. Journal of Chemical Theory and Computation 2011, 7 
(11), 3608-3619. 

Chollet, Keras. Github: 2015. 
Cai, Q.;  Hsieh, M.-J.;  Wang, J.; Luo, R., Performance of Nonlinear Finite-Difference 

Cai, Q.;  Ye, X.;  Wang, J.; Luo, R., On-the-Fly Numerical Surface Integration for Finite-

 
