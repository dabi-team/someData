Data-driven preference learning methods
for value-driven multiple criteria sorting with interacting criteria

Jiapeng Liua,∗, Mi losz Kadzi´nskib, Xiuwu Liaoa, Xiaoxin Maoa

aSchool of Management, Xi’an Jiaotong University, Xi’an, 710049, Shaanxi, P.R. China
bInstitute of Computing Science, Poznan University of Technology, Piotrowo 2, 60-965 Pozna´n, Poland

9
1
0
2

y
a
M
1
2

]

G
L
.
s
c
[

1
v
6
0
5
8
0
.
5
0
9
1
:
v
i
X
r
a

Abstract

The learning of predictive models for data-driven decision support has been a prevalent topic in many ﬁelds.
However, construction of models that would capture interactions among input variables is a challenging task.

In this paper, we present a new preference learning approach for multiple criteria sorting with potentially
interacting criteria. It employs an additive piecewise-linear value function as the basic preference model, which

is augmented with components for handling the interactions. To construct such a model from a given set of
assignment examples concerning reference alternatives, we develop a convex quadratic programming model.

Since its complexity does not depend on the number of training samples, the proposed approach is capable for
dealing with data-intensive tasks. To improve the generalization of the constructed model on new instances

and to overcome the problem of over-ﬁtting, we employ the regularization techniques. We also propose a few
novel methods for classifying non-reference alternatives in order to enhance the applicability of our approach

to diﬀerent datasets. The practical usefulness of the proposed method is demonstrated on a problem of
parametric evaluation of research units, whereas its predictive performance is studied on several monotone

learning datasets. The experimental results indicate that our approach compares favourably with the classical

UTADIS method and the Choquet integral-based sorting model.

Keywords: Decision analysis, Multiple criteria sorting, Preference learning, Additive value function,

Interacting criteria, Regularization

1. Introduction

The purpose of multiple criteria sorting (also called ordinal classiﬁcation) is to help a decision maker (DM) to
assign a ﬁnite set of alternatives to pre-deﬁned and preference ordered classes according to their performances

on multiple criteria. In the past decade, sorting has been among the most growing areas in Multiple Criteria
Decision Aiding (MCDA) for addressing problems in various disciplines such as credit rating, policy making

and assessment, inventory control, project management, supplier segmentation, recommendation systems, risk
assessment, or competitiveness analysis.

In the majority of recently proposed methods, dealing with sorting usually requires the DM to express
his/her preferences in form of assignment examples concerning a subset of reference alternatives. Such infor-

mation is used to construct a preference model compatible with the DM’s preferences, which is subsequently
employed for comparing the alternatives against some class proﬁles or for establishing the preference relation

∗Corresponding author
Email addresses: jiapengliu@mail.xjtu.edu.cn (Jiapeng Liu), milosz.kadzinski@cs.put.poznan.pl (Mi losz Kadzi´nski),

liaoxiuwu@mail.xjtu.edu.cn (Xiuwu Liao), maoxiaoxin29@stu.xjtu.edu.cn (Xiaoxin Mao)

Preprint submitted to Elsevier

May 22, 2019

 
 
 
 
 
 
among alternatives in a way that allows to derive the class assignments. In many such approaches, construct-
ing a preference model is usually organized as a series of interactions in which the DM provides incremental

preference information in order to calibrate the constructed model to better ﬁt his/her preferences. Meanwhile,
the DM could verify the consequences of the provided preference information on the decision outcomes, which

allows him/her to shape one’s preferences progressively and ﬁnally to be convinced about the validity of arrived
sorting recommendation.

Nowadays, data-driven decision support has been an important issue for many businesses. Speciﬁcally,
with the recent development of information technology, decision support systems are used to assist humans

in deriving insights and making decisions through the analysis of increasingly complex data. For example,
ﬁnancial institutions develop the systems for evaluating credit risks of ﬁrms and individuals according to their

transaction data and ﬁnancial indicators and for deciding whether to grant a loan. Furthermore, ﬁrms rely
on customer relationship management systems to construct proﬁles of customers from their on-line and oﬀ-

line behaviours and perform market segmentation in order to tailor diﬀerent marketing policies for targeted

segments. Although these two real-world applications can be viewed in terms of sorting, an intrinsic distinction
between such data-driven decision problems and traditional MCDA problems consists in the former requiring

the preference discovery to be performed automatically without further intervention of the DM, whereas the
latter expecting the DM’s active participation in the preference construction process.

Preference discovery (also called preference learning) has been an important ﬁeld in the Machine Learn-
Its primary focus is on constructing – in an automatic way – a model from a given

ing (ML) community.

training sample and predicting preference for a new sample of data so that the outcomes obtained with the
discovered model are “similar” to the provided training data in some sense. In contrast to MCDA, where the

preference model is co-constructed with the participation of the DM so that to ensure its interpretability and
descriptive character, preference learning in ML is mainly concerned about the ability of capturing complex

interdependencies between the input and the output as well as the predictive performance of the discovered
model. This diﬀerence is further reﬂected in the form of preference models employed in both ﬁelds. On the one

hand, additive models are widely used in MCDA due to their advantage of intuitiveness and comprehensibility.
On the other hand, some non-linear models, such as kernel methods and neural networks, are often used in

ML to capture interdependencies and other complex patterns in data. Although such non-linear models oﬀer

greater ﬂexibility in terms of ﬁtting the learning data and recognizing patterns, they are too complex to be
interpreted by users, and therefore they are often referred to as “black boxes”.

In this paper, we bridge the gap between the ﬁelds of MCDA and ML by proposing a new preference
learning approach for data-driven multiple criteria sorting problems. We aim to learn a preference model from

historical decision examples (also called training samples) so that it can be used to recommend a decision for the
non-reference alternatives. The model should not only have a high predictive performance, but also allow for

interpretable description of preferences. Speciﬁcally, the proposed approach can capture potential interactions
among criteria, which is relevant for numerous real-world applications. For example, let us consider computers

evaluated in terms of the number of CPU cores, CPU speed, and price. On the one hand, there may exist
a negative interaction between the number of CPU cores and CPU speed, because a computer that has a large

number of CPU usually has a high CPU speed. Thus, when considering such a pair of criteria jointly, its
impact on the comprehensive quality of a computer should be lesser than a simple addition of the two impacts

generated by considering each of the two criteria separately. On the other hand, there may exist a positive
interaction between CPU speed and price, because a high CPU speed usually comes along with a high price.

Thus, a computer with a high CPU speed and a low price is much appreciated, as the joint impact of such
a pair of criteria on the overall desirability of a computer should be larger than a simple summation of the two

2

impacts viewed individually.

In MCDA, several models for capturing the interactions between criteria have been developed. Firstly,

a multi-linear utility function is a more general form of a value function, which aggregates products of marginal
utilities on each criterion over all subsets of criteria. Secondly, the Choquet integral can be seen as an average

of marginal values according to a capacity that measures the relative importance of every subset of criteria.
If there is a positive (negative) interaction between two criteria, the weight assigned to such a pair is larger

(smaller) than the sum of weights assigned to each of the two criteria separately. In particular has advocated
the use of the Choquet integral as an aggregation model for preference learning, and incorporated it within

an extension of logistic regression. The main limitation of the two aforementioned models derives from the
need of expressing the performances on diﬀerent criteria on the same scale or bringing them to the joint scale

by the use of marginal value functions which need to be speciﬁed beforehand. This poses a serious burden
for the use of such preference models in real-world decision problems. The third type of a preference model

handling interactions between criteria is a general additive value function augmented by a pair components

that capture the positive and negative interactions for pairs of criteria. The latter model neither requires
speciﬁcation of all performances on the same scale nor a priori deﬁnition of marginal values. Its construction

has been traditionally based on linear programming techniques used within an interactive procedure during
which the DM could progressively discover the pairs of interacting criteria.

In the proposed preference learning approach, we consider an additive value model with piecewise-linear
marginal functions under the preferential independence hypothesis, and then extend it for capturing the inter-

actions. For this purpose, we adapt the preference model proposed in by means of two types of expressions for
quantifying the positive and negative interactions among pairs of criteria. Consequently, our approach belongs

to the family of value-based MCDA methods, which allow for establishing preference relations among alterna-
tives by comparing their numerical scores, thus preserving the advantage of intuitiveness and comprehensibility.

Our approach does not require all criteria to be expressed on the same scale, admitting an assessment of both
the relative importances of criteria and the potential interaction intensities between pairs of criteria.

We also introduce methodological advances in ML to enhance the predictive ability of the constructed
preference model and the computational eﬃciency of the preference learning procedure. We formulate the

learning problem in the regularization framework and use regularization terms for improving the generalization

ability of the constructed model on new instances. Moreover, by utilizing the properties of value functions, we
formulate a convex quadratic programming model for constructing the preference model. Since the complexity

of this technique is irrelevant from the number of training samples, it is suitable for addressing data-intensive
tasks and the respective models can be derived using popular solvers without extraordinary eﬀorts. In addition,

we propose four methods for classifying non-reference alternatives once the preference model with the optimal
ﬁtting performance is obtained. Consequently, the generalization performance can be improved by selecting

one of the four procedures that proved to be the most advantageous for a given dataset.

The various variants of the proposed approach in terms of diﬀerent interaction expressions and methods for

classifying non-reference alternatives are validated within an extensive computational study. In particular, the
practical usefulness of the proposed method is demonstrated on a problem of parametric evaluation of research

units. In this perspective, we discuss how to interpret information on the relative importance of criteria and
the interaction coeﬃcients between pairs of criteria. Moreover, we compare the proposed approach with the

UTADIS method and the Choquet integral-based sorting model in terms of their predictive performances on
nine monotone learning datasets.

The remainder of the paper is organized in the following way. In Section 2, we present the learning approach
In Section 3, we apply the proposed

for addressing sorting problems with potentially interacting criteria.

3

approach to a problem of parametric evaluation of Polish research units. We also discuss the experimental
results derived from the comparison of the introduced method with UTADIS and the Choquet integral-based

model on several public datasets. Section 4 concludes the paper and provides avenues for future research.

2. Preference learning approach for sorting problems with potentially interacting criteria

2.1. Problem description

We describe the considered sorting problems with the following notation:

• AR = {a, b, ...} – a set of reference alternatives (training sample) for which the classiﬁcation is known;

• A = {a1, a2, ...} – a set of non-reference alternatives to be classiﬁed;

• CL = {Cl1, ..., Clq} – a set of decision classes, such that Cls+1 is preferred to Cls (denoted by Cls+1 ≻

Cls), s = 1, ..., q − 1, and Cl1 and Clq are, respectively, the least and the most preferred ones;

• G = {g1, ..., gn} – a family of evaluation criteria, gj : A ∪ AR → R, and gj(a) denotes the performance of
alternative a on criterion gj; without loss of generality, we assume that all criteria are of gain type, i.e.,
the greater gj(a), the more preferred a on gj, j = 1, ..., n.

The task consists in learning a preference model from the training samples composed of reference alternatives
a ∈ AR and their assignments (denoted by Cl(a) ∈ CL) to determine the classiﬁcation for non-reference
alternatives a′ ∈ A. Let us ﬁrst deﬁne a simple additive value function under the preferential independence
hypothesis, and later extend such a preference model to consider the interactions among criteria. The additive
value model U (·) aggregates the performances of each alternative a ∈ A∪AR on all criteria into a comprehensive
score:

U (a) =

ugj (gj(a)),

(1)

n

j=1

X
where U (a) is a comprehensive value of a, and ugj (gj(a)) is a marginal value on criterion gj, j = 1, ..., n.

Since the marginal value functions ugj (·) for each criterion gj, j = 1, ..., n, are unknown, we employ
piecewise-linear marginal value functions to approximate the actual ones. Such an estimation technique
has been adopted in many ordinal regression problems. Speciﬁcally, let Xj = [αj, βj] be the performance
scale of gj, such that αj and βj are the worst and best performances, respectively. To deﬁne a piecewise-
linear marginal value function ugj (·), we divide Xj = [αj, βj] into γj > 1 equal sub-intervals, denoted by
j , x1
x0
(βj − αj), 0, 1, ..., γj. Then, the marginal value of alter-
j
native a on criterion gj can be estimated through linear interpolation:
(cid:2)

xγj −1
j
h

j = αj + k
γj

, where xk

j , x2
x1
j

, xγj
j

, ...,

i

(cid:2)

(cid:3)

(cid:3)

,

ugj (gj(a)) = ugj (xkj

j ) +

gj(a) − xkj
j
− xkj
xkj +1
j

j (cid:16)

ugj (xkj +1

j

) − ugj (xkj
j )
(cid:17)

,

for gj(a) ∈

j , xkj +1
xkj
h

j

i

.

One can observe that the piecewise-linear marginal value function ugj (·) is fully determined by the marginal
values at characteristic points, i.e., ugj (x0
j ) = ugj (βj ). Given a suﬃcient
number of characteristic points, the piecewise-linear marginal value function ugj (·) can approximate any form
of non-linear value function.
When assuming ∆ut

, t = 1, ..., γj, ugj (gj(a)) can be rewritten as:

j ) = ugj (αj), ugj (x1

..., ugj (xγj

− ugj

j ),

gj = ugj

xt−1
j

xt
j

ugj (gj(a)) =

(cid:0)

(cid:1)
kj

t=1

X

(cid:0)

∆ut

gj +

(cid:1)

gj(a) − xkj
j
− xkj
xkj +1
j
j

∆ukj +1
gj

,

for gj(a) ∈

j , xkj +1
xkj
h

j

i

.

4

Having gathered all ∆ut

gj , t = 1, ..., γj as a vector ugj =

∆u1

gj , ..., ∆uγj

gj

T

alternative a ∈ A∪AR and each criterion gj, j = 1, ..., n, we can deﬁne a vector Vgj (a) =
such that, for each t = 1, ..., γj:

(cid:16)

for criterion gj, j = 1, ..., n, for each
T

gj (a) , ..., vγj
v1

gj (a)

,

(cid:17)

(cid:16)

(cid:17)

1,
gj (a)−xt−1
j −xt−1
xt

j

j

if gj (a) > xt
j ,
if xt−1
j 6 gj (a) 6 xt
j,

,

0,

if gj (a) < xt−1

j

.

vt
gj (a) = 



Then, the marginal values ugj (·), j = 1, ..., n, can be represented as an inner product between vectors as follows:

ugj (gj (a)) = uT
gj

Vgj (a) .

Subsequently, let us denote u =
value U (·) can be expressed in the following way:

g1 , ..., uT
uT
gn

and V (a) =

(cid:0)

T

(cid:1)

Vg1 (a)T, ..., Vgn (a)T
(cid:16)

(cid:17)

T

, and the comprehensive

U (a) = uTV (a) .

2.2. Learning preference model from reference alternatives

In this section, we propose a new method for learning a preference model in form of an additive value function (1)
from a set of reference alternatives a ∈ AR and their associated precise class assignments Cl(a). Before
describing the estimation procedure, let us present the underlying consistency principle for characterizing the
preference relation between alternatives in sorting problems.

Deﬁnition 1. For any pair of alternatives a, b ∈ A ∪ AR, value function U (·) is said to be consistent with the

assignments of a and b (denoted by Cl(a) and Cl(b), respectively, and Cl(a), Cl(b) ∈ CL) iﬀ:

U (a) > U (b) ⇒ Cl (a) % Cl (b) ,

where % means “at least as good as”. Observe that implication (2) is equivalent to:

Cl (a) ≻ Cl (b) ⇒ U (a) > U (b) .

(2)

(3)

According to Deﬁnition 1, value function U (·) inferred from the analysis of assignment examples should ensure
U (a) > U (b) for pairs of reference alternatives (a, b) ∈ AR × AR such that Cl (a) ≻ Cl (b). However, there
may exist no such a value function that would guarantee perfect compatibility due to the inconsistency of some
assignment examples with an assumed preference model. In turn, we can estimate a value function that would
maximize the diﬀerence between U (a) and U (b) for pairs of reference alternatives (a, b) ∈ AR × AR such that
Cl (a) ≻ Cl (b). This can be implemented by solving the following linear programming model:

(P0) : M inimize − d,

s.t.

U (a) − U (b) > d (a, b) , a ∈ AR

s+1, b ∈ AR

s , s = 1, ..., q − 1,

1

AR

s+1

|AR
s |

(cid:12)
(cid:12)

(cid:12)
(cid:12)

d (a, b) > d, s = 1, ..., q − 1,

a∈AR

s+1, b∈AR
s

X

(4)

(5)

(6)

where AR

s denotes a set of reference alternatives assigned to class Cls, and

AR
s

is the cardinality of AR

s . For

5

(cid:12)
(cid:12)

(cid:12)
(cid:12)

s+1 × AR

any (a, b) ∈ AR
is identiﬁed by constraint (5). In constraint (6),
s+1 × AR

s , s = 1, ..., q − 1, the value diﬀerence for such a pair of alternatives, denoted by d (a, b),
d (a, b) is the average value diﬀerence

s+1, b∈AR
s
for pairs (a, b) ∈ AR
s . Then, d corresponds to the minimum of such value diﬀerences for all consecutive
classes. By maximizing the minimum value diﬀerence d (i.e., M inimize − d), model (P0) aims to ﬁnd value

1
s+1||AR
s |

a∈AR

|AR

P

function U (·) that restores the assignment consistency as accurately as possible.

Note that it would not be appropriate to maximize the minimal value diﬀerence between reference alterna-
tives from the consecutive classes (i.e., replace constraint (6) with constraint d (a, b) > d, a ∈ AR
s+1, b ∈ AR
s ,
s = 1, ..., q − 1). In case of an inconsistent reference set, such a minimal value is smaller than zero (i.e., d < 0),

and then it is meaningless to maximize it. Moreover, we do not maximize the sum of value diﬀerences between
reference alternatives from the consecutive classes (i.e., remove constraint (6) and replace objective (4) with

M inimize −

d (a, b)). The underlying reason for avoiding doing so can be illustrated

s=1,...,q−1
P

a∈AR

s+1,b∈AR
s
P

s+1, b ∈ AR
through a simple example: let us consider three reference alternatives a ∈ AR
s−1. Then,
d (a, b) = U (a) − U (b) and d (b, c) = U (b) − U (c).
If we aimed to maximize the sum of value diﬀerences
between reference alternatives from the consecutive classes, we would just maximize the diﬀerence between

s and c ∈ AR

U (a) and U (c), because d (a, b) + d (b, c) = (U (a) − U (b)) + (U (b) − U (c)) = U (a) − U (c) and then d (a, b)
and d (b, c) would be completely neglected.

When addressing a large number of reference alternatives AR, model (P0) contains a huge number of
constrains (5), exceeding the processing capacity of existing linear programming solvers. Thus, we propose

a method for transforming model (P0) to an equivalent model that is suitable for data-intensive tasks. The
key idea is to aggregate constrains (5) for all the possible pairs of reference alternatives (a, b) ∈ AR
s for
a particular s ∈ {1, ..., q − 1}, and obtain:

s+1 × AR

AR
s

(cid:12)
(cid:12)
AR
s

By dividing

AR

s+1

X
(cid:12)
(cid:12)
, constraint (7) is equivalent to:

X

(cid:12)
(cid:12)

(cid:12)
(cid:12)

U (a) −

AR

s+1

a∈AR

s+1

U (b) >

b∈AR
s

a∈AR

s+1, b∈AR
s

d (a, b).

(7)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

1
AR

s+1

X

U (a) −

a∈AR

s+1

1
|AR
s |

U (b) >

b∈AR
s

X

(cid:12)
Then, since U (a) = uTV (a), constraint (8) can be transformed to:
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

d (a, b).

(8)

a∈AR

s+1, b∈AR
s

X

X

1

AR

s+1

|AR
s |

uT

1
AR

s+1

a∈AR

s+1

− uT

V (a)

!

X

1
|AR
s |

(cid:18)

V (b)

>

(cid:19)

b∈AR
s

X

1

AR

s+1

|AR
s |

d (a, b).

(9)

a∈AR

s+1, b∈AR
s

X

For each class Cls, s = 1, ..., q, let us average V (a) for all a ∈ AR

s and derive:

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Then, constraint (10) can be written as:

µs =

1
|AR
s |

V (a).

a∈AR
s

X

uTµs+1 − uTµs >

1

AR

s+1

|AR
s |

(cid:12)
(cid:12)

(cid:12)
(cid:12)

X

a∈AR

s+1, b∈AR
s

d (a, b).

(10)

(11)

6

 
In this way, model (P0) can be reformulated as:

(P1) : M inimize − d,

s.t.

uTµs+1 − uTµs > d, s = 1, ..., q − 1.

(12)

(13)

Note that the number of constraints (13) is related only to the number of classes (i.e., q − 1) rather than the
number of pairs of reference alternatives. Thus, model (P1) can deal with a large set of reference alternatives

eﬃciently.

Proposition 1. The optimal value of the objective function of model (P0) is equal to that of model (P1).

Proof. See Appendix A.

(cid:3)

In addition to maximizing the value diﬀerence for pairs of reference alternatives from the consecutive classes,
we also propose to minimize the value diﬀerence among reference alternatives from the same class. In this

way, the distribution of comprehensive values of reference alternatives from the same class would be more
concentrated, and consecutive classes could be clearly delimited. For this purpose, we aim at minimizing the

second objective:

q

(U (a) − U (b))2 =

q

uTv (a) − uTv (b)

2

=

q

uT (v (a) − v (b)) (v (a) − v (b))Tu

i=1
X

i

Xa,b∈AR
q

= uT





i=1
X

Xa,b∈AR

i

q

where S =

i=1
X

Xa,b∈AR
i (cid:0)

(cid:1)

i=1
X

Xa,b∈AR

i

(v (a) − v (b)) (v (a) − v (b))T

u = uTSu,





(v (a) − v (b)) (v (a) − v (b))T is a γ×γ matrix and γ =

n

q

γj. Since

(U (a) − U (b))2 >

i=1
P

a,b∈AR
i
P

0 always holds, S must be positive semi-deﬁnite. Then, putting together the above two objectives, we propose
the following convex quadratic programming model:

j=1
P

i=1
P

a,b∈AR
i
P

(P2) :

s.t.

min −d + C1uTSu + C2 kuk2
2 ,
uTµk+1 > uTµk + d, k = 1, ..., q − 1,
uTV∗ = 1,

d > 0,

u > 0,

(14)

(15)

(16)

(17)

(18)

where V∗ is a γ-dimensional vector with all entries being equal to one, and constraint (16) is used to bound
U (·) to the interval [0,1]. Since class Clk+1 is preferred to class Clk, k = 1, ..., q − 1, we require that d > 0
in constraint (17). Besides the two aforementioned objectives, the regularization term kuk2
2 is added to the
objective of model (P2) to avoid the problem of over-ﬁtting. Speciﬁcally, since the performance scale on each
criterion is divided into a certain number of equal sub-intervals, the ﬁtting ability of the estimated function

improves with the increase in the number of sub-intervals, at the same time increasing the risk of over-ﬁtting.
The regularization term kuk2
2, also named Tikhonov regularization, penalizes functions that are “too wiggly”,
and derives marginal value functions that are as “smooth” as possible, which alleviates the problem of over-
ﬁtting caused by inappropriately dividing the performance scale into too many sub-intervals. The constants
C1, C2 > 0 are used to make a trade-oﬀ between the two objectives and the regularization term. Values of

7

C1 and C2 can be chosen through K-fold cross-validation in the following manner: the whole set of reference
alternatives AR is randomly partitioned into K (usually K is set to be 5 or 10) equal sized folds such that
the percentage of reference alternatives from diﬀerent decision classes in each fold are the same with that in
AR. For certain C1 and C2, K − 1 folds are used as the training data and the remaining fold is retained as
the validation data for testing the model. The cross-validation process is repeated K times, and then the K
results are averaged to evaluate the performance of the developed model (e.g., classiﬁcation accuracy). Finally,
the values of C1 and C2 corresponding to the best performance are chosen as the optimal setting for the two
parameters. For model (P2), let us remark that, since S and µk, k = 1, ..., q, can be speciﬁed in advance,
model (P2) is not related to the pairwise comparisons among reference alternatives, and thus the number of
constraints is small. Hence, model (P2) can be easily solved using popular optimization packages, such as

Lingo, Cplex, or MATLAB.

2.3. Considering interactions among criteria

Even though an additive value function model is widely used in real-world decision aiding, it is not able to
represent interactions among criteria due to the underlying preferential independence hypothesis. To handle

interactions among criteria, we incorporate and adjust the model so that to propose a new method which can
address a large set of reference alternatives eﬃciently. The underlying model is an additive value function

augmented by “bonus” and “penalty’” components for, respectively, positive and negative interactions among
criteria, which is formulated as:

n

U (a) =

ugj (gj (a)) +

syn+

gj ,gk (gj (a) , gk (a)) −

syn−

gj ,gk (gj (a) , gk (a)),

(19)

j=1
X

X{gj ,gk}∈G:j<k

X{gj ,gk}∈G:j<k

where syn+
gj and gk. The extended form (19) of value function should fulﬁl the following basic conditions:

gj ,gk (·, ·) are the bonus and penalty values for modelling the interactions between

gj,gk (·, ·) and syn−

• normalization: U (a) = 0 if gj (a) = αj , j = 1, ..., n, and U (a) = 1 if gj (a) = βj, j = 1, ..., n,

• monotonicity (a): ∀ {gj, gk} ∈ G, j < k, if gj (a) > gj (b) and gk (a) > gk (b), then syn+

gj ,gk (gj (a) , gk (a)) >

syn+

gj,gk (gj (b) , gk (b)) and syn−

gj ,gk (gj (a) , gk (a)) > syn−

gj ,gk (gj (b) , gk (b)),

• monotonicity (b): ∀H ⊆ G, if gj (a) > gj (b), ∀gj ∈ H, then:

ugj (gj (a)) +

syn+

gj ,gk (gj (a) , gk (a)) −

syn−

gj ,gk (gj (a) , gk (a))

Xgj ∈H
>

Xgj ∈H

X{gj ,gk}∈H:j<k

X{gj ,gk}∈H:j<k

ugj (gj (b)) +

syn+

gj ,gk (gj (b) , gk (b)) −

syn−

gj ,gk (gj (b) , gk (b)).

X{gj ,gk}∈H:j<k

X{gj ,gk}∈H:j<k

gj ,gk (·, ·) and syn−

The normalization conditions require U (·) to be bounded to the interval [0,1]. Monotonicity (a) ensures that
syn+
gj ,gk (·, ·) are monotone non-decreasing with respect to their arguments. Monotonicity (b)
can be interpreted as follows: when comparing any pair of alternatives a, b on a subset of criteria H ⊆ G, if
a is at least as good as b for all gj ∈ H, the comprehensive value of a derived from the analysis of H should be
not worse than that of b. Note that monotonicity (b) induces numerous constraints as G has 2n − 1 non-empty
subsets. For this reason, we assume that any criterion interacts with at most one other. This makes both

the inference of a value function more tractable and the constructed model more interpretable. Under this

8

assumption, value function (19) can be reformulated as:

U (a) =

ugi (gi (a))

gi /∈∪{gj ,gk}∈Syn{gj ,gk}
X

+

X{gj ,gk}∈Syn (cid:16)

where

ugj (gj (a)) + uk (gk (a)) + syn+

gj ,gk (gj (a) , gk (a)) − syn−

gj ,gk (gj (a) , gk (a))

(20)

,

(cid:17)

Syn =

{gj, gk} ⊆ G, j < k : syn+

gj ,gk (gj (a) , gk (a)) 6= 0 or

syn−

gj ,gk (gj (a) , gk (a)) 6= 0 for some a

n

o

denotes the set of all pairs {gj, gk} of interacting criteria. Value function (20) divides a set of criteria into two
disjoint subsets: one consisting of criteria not interacting with the remaining ones, and the other composed of
the interacting criteria. In this way, monotonicity (b) can be reduced to that, ∀ {gj, gk} ∈ Syn, if gj (a) > gj (b)
and gk (a) > gk (b), then:

ugj (gj (a)) + uk (gk (a)) + syn+

gj ,gk (gj (a) , gk (a)) − syn−

gj ,gk (gj (a) , gk (a))

> ugj (gj (b)) + uk (gk (b)) + syn+

gj ,gk (gj (b) , gk (b)) − syn−

gj ,gk (gj (b) , gk (b)) .

(21)

In this study, we propose to deﬁne the bonus and penalty components syn+
following way:

gj ,gk (·, ·) and syn−

gj ,gk (·, ·) in the

syn+

gj ,gk (gj (a) , gk (a)) =

syn−

gj ,gk (gj (a) , gk (a)) =

γj

γk

s=1
X
γj

t=1
X
γk

s=1
X

t=1
X

gj ,gk vs
η+,s,t

gj (a) vt

gk (a),

η−,s,t
gj ,gk vs

gj (a) vt

gk (a),

(22)

(23)

gj ,gk , η−,s,t

where η+,s,t
gj ,gk > 0 are the coeﬃcients for modelling the positive and negative interactions on criterion gj’s
s-th sub-interval and criterion gk’s t-th sub-interval. Since the above deﬁnition of syn+
gj ,gk (gj (a) , gk (a)) and
syn−
gk (a), it ensures that the bonus and penalty
components are monotone and strictly increasing with respect to gj (a) and gk (a) for any pair of criteria (gj, gk).
An alternative deﬁnition of the interaction components would consist in deriving the minimum from vs
gj (a)

gj ,gk (gj (a) , gk (a)) is based on the product of vs

gj (a) and vt

and vt

gk (a) as follows:

syn+

gj ,gk (gj (a) , gk (a)) =

syn−

gj ,gk (gj (a) , gk (a)) =

X

γj

s=1
γj

s=1

γk

t=1
γk

t=1

X

η+,s,t
gj ,gk min

gj (a) , vt
vs

gk (a)

,

η−,s,t
gj ,gk min

n

gj (a) , vt
vs

gk (a)

o
.

(24)

(25)

X
This makes these components monotone non-decreasing with respect to gj (a) and gk (a). It is easy to vali-
gj ,gk (·, ·) and syn−
date that such two types of deﬁnitions of syn+
gj ,gk (·, ·) satisfy monotonicity (a). To ensure
monotonicity (b), let us consider the following proposition.

X

n

o

Proposition 2. Value function (20), in which the bonus and penalty components are, respectively, deﬁned as
(22) and (23) (or (24) and (25)), satisﬁes monotonicity (b), if and only if ∀ {gj, gk} ∈ Syn:

t

∆us

gj +

gj ,gk − η−,s,q
η+,s,q
gj ,gk

> 0, s = 1, ..., γj, t = 1, ..., γk.

(26)

q=1 (cid:16)
X

(cid:17)

9

Proof. See Appendix B.

(cid:3)

Let us now introduce the method for estimating the coeﬃcients η+,s,t
gj ,gk , s = 1, ..., γj, t = 1, ..., γk,
gj, gk ∈ G, j < k, from the assignments of reference alternatives. For the convenience of the analysis, let us
deﬁne the following vectors:

gj ,gk and η−,s,t

gj (a) v1
v1
(cid:16)
gj (a) v1

V+

gj ,gk (a) =

gk (a) , ..., v1

gj (a) vγk

gk (a) , . . . , vγj

gj (a) v1

gk (a) , ..., vγj

gj (a) vγk

V−

gj ,gk (a) =

−v1

gk (a) , ..., −v1

gj (a) vγk

gk (a) , . . . , −vγj

gj (a) v1

gk (a) , ..., −vγj

gj (a) vγk

(cid:16)

η+
gj ,gk =

η−
gj ,gk =

gj ,gk , ..., η+,1,γk
η+,1,1

gj ,gk , . . . , η+,γj ,1

gj ,gk , ..., η+,γj,γk

gj ,gk

(cid:16)
gj ,gk , ..., η−,1,γk
η−,1,1

gj ,gk , . . . , η−,γj ,1

gj ,gk , ..., η−,γj,γk

gj ,gk

T

(cid:17)

T

,

.

T

,

gk (a)
(cid:17)

T

,

gk (a)
(cid:17)

Then, the bonus and penalty components syn+
an inner product between the above vectors as follows:

gj,gk (·, ·) and syn−

(cid:16)

gj ,gk (·, ·) can be reformulated in the form of

(cid:17)

syn+

gj ,gk (gj (a) , gk (a)) = η+

gj ,gk

TV+

gj ,gk (a) ,

syn−

gj ,gk (gj (a) , gk (a)) = −η−

gj ,gk

TV−

gj ,gk (a) .

By considering the interactions between criteria, we can redeﬁne V (a) and u as follows:

VINT (a) =

Vg1 (a)T, ..., Vgn (a)T,
(cid:16)

g1,g2 (a)T, V−
g2,g3 (a)T, V−

g1,g2(a)T, ..., V+
g2,g3(a)T, ..., V+

V+

V+

g1,gn (a)T, V−
g2,gn (a)T, V−

g1,gn (a)T,
g2,gn (a)T,

......,

V+

gn−1,gn (a)T, V−

gn−1,gn (a)T

T

,

(cid:17)

uINT =

g1 , ..., uT
uT
gn ,
T
, η−

T

g1,g2

η+

(cid:0)
g1,g2

, ..., η+

g1,gn

η+

g2,g3

T

, η−

g2,g3

T

, ..., η+

g2,gn

......,

η+

gn−1,gn

T

, η−

gn−1,gn

T

T

,

T

T

, η−

g1,gn

, η−

g2,gn

T

T

,

,

where VINT (a) and uINT are constructed by adding more dimensions for characterizing interactions between
criteria. In this way, according to (19), the comprehensive score of alternative a can be formulated as:

(cid:17)

U (a) = uINTT

VINT(a),

(27)

which is consistent with the form of an additive value function under the preferential independence hypothesis.

Utilizing Proposition 2, we can replace the value function in model (P2) with (19) and incorporate additional
constraints related to modelling the interactions between criteria. The optimization model adapted from (P2)

for considering the interactions among criteria can be formulated in the following way:

10

(28)

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

(37)

(P3) : M inimize − d + C1uINTT
k+1 > uINTT
µINT
µINT

uINTT

s.t.

SINTuINT + C2

uINT

k + d, k = 1, ..., q − 1,

(cid:13)
(cid:13)

2
2 ,
(cid:13)
(cid:13)

t

∆us

gj +

gj ,gk − η−,s,q
η+,s,q
gj ,gk

> 0, s = 1, ..., γj,

t = 1, ..., γk, gj, gk ∈ G, j < k,

(cid:17)
gj ,gk 6 χ−
gj ,gk , s = 1, ..., γj,

q=1 (cid:16)
X
η+,s,t
gj ,gk 6 χ+
gj ,gk , η−,s,t
gj ,gk + χ−
χ+
gj ,gk 6 1, gj, gk ∈ G, j < k,
gj ,gk + χ−
χ+

6 1, gj ∈ G,

gk∈G
(cid:16)
(cid:17)
X
χ+
gj ,gk , χ−
gj ,gk ∈ {0, 1} , gj, gk ∈ G, j < k,
uINTT

VINT (a∗) = 1,

gj ,gk

t = 1, ..., γk, gj, gk ∈ G, j < k,

d > 0,

uINT > 0,

where SINT =

q

VINT (a) − VINT (b)

VINT (a) − VINT (b)

T

, and µINT

k = 1
k |
|AR

VINT (a). χ+

gj ,gk

(cid:1)

(cid:1) (cid:0)

k=1
P

(cid:0)
gj ,gk are binary variables such that χ+

a,b∈AR
k
P
and χ−
gj ,gk = 1 if positive interaction between criteria gj and gk exists,
gj ,gk = 1 if negative interaction between criteria gj and gk exists, or otherwise χ−
or χ+
gj ,gk = 0, and χ−
gj ,gk = 0.
In constraint (31), χ+
gj ,gk and χ−
gj ,gk are used to identify whether interaction between criteria gj and gk exists,
gj ,gk greater than zero, then χ+
i.e., if there are some η+,s,t
gj ,gk = 1 or χ−
gj ,gk = 1. Constraint (32) states
gj ,gk (·, ·) and syn−
that syn+
gj,gk (·, ·) are mutually exclusive, i.e., there exists only one type of interaction (i.e.,
positive or negative) between criteria gj and gk. Constraint (33) ensures that any criterion interacts with at
most one other. VINT (a∗) is constructed according to a virtual ideal alternative a∗ such that gj (a∗) = βj,
j = 1, ..., n, and constraint (35) is used to bound U (·) to the interval [0,1].

gj ,gk or η+,s,t

a∈AR
k
P

Remark that the regularization term

2
2 not only makes the derived marginal value functions as
“smooth” as possible as in model (P2), but also smooths the variations of interaction parameters η+,s,t
gj ,gk , η−,s,t
(cid:13)
gj ,gk
(cid:13)
over the adjacent grids constituted by criteria gj and gk’s sub-intervals. Although model (P3) involves binary
variables χ+
gj ,gk for each pair of criteria gj and gk, it is a convex quadratic optimization problem
when the binary variables are ﬁxed. Thus, it can be addressed in reasonable time using popular optimization

gj ,gk and χ−

uINT

(cid:13)
(cid:13)

packages if the number of criteria is not prohibitively large.

Model (P3) discovers the interactions among criteria from the given data and identiﬁes the type of inter-

action (positive or negative) for any pair of interacting criteria in an automatic way. For some problems, it is
reasonable to consider only one type of interaction for the interacting criteria (positive or negative). This can

be implemented by adding the following constraint to Model (P3) in case only positive interaction is considered:

whereas the following one can be added when only negative interaction is admissible:

χ−

gj ,gk = 0, gj, gk ∈ G, j < k,

χ+

gj ,gk = 0, gj, gk ∈ G, j < k.

11

2.4. Classiﬁcation algorithms

Once the optimal value of u or uINT is obtained, we compute the comprehensive values for all reference (a ∈ AR)
and non-reference (a ∈ A) alternatives according to U (a) = uTV(a) or U (a) = uINTTVINT(a). To perform the
classiﬁcation of the latter ones, we will use four sorting methods denoted by MI – MIV . When determining
the assignment for a, these methods work out the coeﬃcients Mr (a → Clk), r ∈ {I, II, III, IV }, indicating
a support given to a → Clk for k = 1, . . . , q, and select class Clt for which Mr (a → Clt) is maximal. We
will explain the proposed approaches by referring to a pair of example sorting problems presented in Figures 1
and 2. Each of these problems involves 11 non-reference alternatives a1,...,11 ∈ AR with a desired class speciﬁed
in the respective ﬁgure as well as a single non-reference alternative b ∈ A whose classiﬁcation is yet to be
determined.

The ﬁrst classiﬁcation method (MI ) was proposed with the aim of measuring the support given to a hy-

pothesis a → Clk, k = 1, . . . , q, with the following consistency degree:

a∗ ∈

t=1,...,k−1 AR
t

: U (a) > U (a∗)

+

a∗ ∈

t=k+1,...,q AR
t

: U (a) < U (a∗)

MI (a → Clk) =

(cid:12)
n
(cid:12)
(cid:12)

S

(cid:12)
t=1,...,q, t6=k AR
(cid:12)
t
(cid:12)

o(cid:12)
(cid:12)
(cid:12)

n

S

.

o(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
S
(cid:12)
The measure indicates a proportion of reference alternatives assigned to a class either worse or better than
(cid:12)
Clk that attain comprehensive values, respectively, lower or greater than a according to the inferred value
function. Clearly, the greater MI(a → Clk) is, the lesser is the proportion of reference alternatives suggesting
an assignment of a to a class diﬀerent than Clk, and hence the more justiﬁed is a → Clk. For example, when
considering an assignment of alternative b presented in Figure 1 to class Cl3, MI (b → Cl3) = 5/8, because
5 (a1, a2, a3, a4, a6) out of 8 reference alternatives assigned to class Cl1 or Cl2 (i.e., diﬀerent than Cl3) admit
b → Cl3. Since MI (b → Cl1) = 3/7 and MI (b → Cl2) = 4/7, Cl3 has the greatest support and MI would
assign b to Cl3.

Method MI assumes that when conﬁrming a given assignment each reference alternative has the same voting
power, irrespective of its own consistency. In turn, method MII diﬀerentiates these powers by considering for
each a ∈ AR
k its consistency with the remaining assignment examples quantiﬁed with MI (a → Clk). For
example, when accounting for a4 and a10 from Figure 1, MI (a4 → Cl2) = 5/7 and MI (a10 → Cl3) = 8/8.
Hence, we revise the coeﬃcient used in method MI by assigning a greater voting power to the reference
alternatives which are more in line with other assignment examples:

MII (a → Clk) =

{a∗∈

t=1,...,k−1 AR

t : U(a)>U(a∗)}

S

P

t=k+1,...,q AR

{a∗∈
S
S
t=1,...,q, t6=k AR
t

t : U(a)<U(a∗)} MI (a∗ → Clt)

.

(cid:12)
(cid:12)
For example, when considering an assignment of b from Figure 1 to Cl3, MII (b → Cl3) = (7/7 + 4/7 +
(cid:12)
6/7 + 5/7 + 4/7)/8 = 26/56, because 5 (a1, a2, a3, a4, a6) out of 8 reference alternatives assigned to Cl1 or
Cl2 admit b → Cl3, but their consistency degrees do diﬀer (MI (a1 → Cl1) = 7/7, MI (a2 → Cl2) = 4/7,
MI (a3 → Cl1) = 6/7, MI (a4 → Cl2) = 5/7, MI (a6 → Cl2) = 4/7). Since MII (b → Cl1) = 20/49 and
MII (b → Cl2) = 27/49, MII would assign b to Cl2.

S

(cid:12)
(cid:12)
(cid:12)

When computing the support given to a → Clk, MI and MII consider the sets of reference alternatives
assigned to classes diﬀerent than Clk. On the contrary, method MIII accounts for the reference alternatives
contained in AR
k , hence directly supporting a → Clk. Moreover, it considers the consistencies of these reference
alternatives according to their roles in view of validating a → Clk. On one hand, for a∗ ∈ AR
k such that
U (a∗) ≤ U (a), we analyse the subset of reference alternatives with comprehensive values not lesser than U (a∗)

12

a

1 

C

1 

a

2 

C
2 

a

3 

C
1 

a

4 

C
2 

a

5 

C

3 

a

6 
C
2 

a

7 

C
1 

a

8 
C
1 

b 

a
9 

C
2 

a

10 
C

3 

a

11 
C

3 

U(a) 

0.0 

0.1 

0.15 

0.3 

0.4  0.45 

0.55 

0.6  0.65 

0.75 

0.9 

1.0 

Figure 1: Alternatives used to illustrate the use of four classiﬁcation methods in Example 1.

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

11 

3 

3 

2 

3 

2 

2 

1 

1 

1 

) 

0.3

0.0

0.55

0.15

0.6 0.65

k . On the other hand, for a∗ ∈ AR

and not greater than U (a). They all have an impact on the worst possible class of a, hence inﬂuencing the
2 
1 
strength of support given to a → Clk by a∗ ∈ AR
k such that U (a∗) > U (a), we
analyse the subset of reference alternatives with comprehensive values not greater than U (a∗) and greater than
1.0
0.4 0.45
0.1
k with either U (a∗) ≤ U (a)
U (a), as these have an impact on the best possible class of a. Overall, each a∗ ∈ AR
or U (a∗) > U (a) supports a → Clk with a degree proportional to a number of reference alternatives a
∈ AR
k
with comprehensive values U (a∗) ≤ U (a
) > U (a), respectively. For example,
with
a5 from Figure 1 supports b → Cl3 with the strength of 1/2, because out of 2 reference alternatives a
) ≤ U (b) (i.e., a5 and a6) only a5 was assigned to Cl3. In the same spirit, a11 supports b → Cl3
U (a5) ≤ U (a
with the strength of 2/5, because out of 5 reference alternatives a
) > U (b) (i.e., a7, a8,
a9, a10, and a11) only a10 and a11 were assigned to Cl3. An overall support in favor of assignment a → Clk,
k = 1, . . . , q, can be quantiﬁed as an average support given by all a∗ ∈ AR

) ≤ U (a) or U (a∗) ≥ U (a

with U (a11) ≥ U (a

0.75

0.9

′

′

′

′

′

′

′

k to a → Clk, i.e.:

MIII (a → Clk) =

P

{a∗∈AR

k : U(a)≥U(a∗)}

{a∗∈AR

k : U(a)<U(a∗)}

|AR

(a,a∗ ]
|AR

T
(a,a∗ ]|

AR
k |

,

|AR

[a∗,a]
|AR

T

AR
k |
[a∗,a]| +
AR
k

P

(cid:12)
(cid:12)

′

(cid:12)
(cid:12)

′

′

[a∗,a] = {a

(a,a∗] = {a

) ≤ U (a)} and AR

∈ AR : U (a) < U (a

∈ AR : U (a∗) ≤ U (a

where AR
) ≤ U (a∗)}). When
considering an assignment of b from Figure 1 to Cl3, MIII (b → Cl3) = (1/2 + 1/4 + 2/5)/3 = 23/60. As far as
3 reference alternatives assigned to Cl3 (a5, a10, a11) are concerned in view of b → Cl3, a5 has a comprehensive
value not greater than U (b), but its consistency degree is 1/2, because in AR
[a5,b] = {a5, a6} only 1 out of 2
reference alternatives was assigned to Cl3. Moreover, a10 and a11 have comprehensive values greater than U (b)
and their respective consistency degrees in terms of supporting b → Cl3 are 1/4 (AR
(b,a10] = {a7, a8, a9, a10})
and 2/5 (AR
(b,a11] = {a7, a8, a9, a10, a11}). Since MIII (b → Cl1) = 31/48 and MIII (b → Cl2) = 39/60, b is
assigned to Cl2 by MIII .

′

The last method (MIV ) analyses the support given to the assignment of a ∈ A to diﬀerent classes only by the
reference alternatives which are the closest to a in terms of their comprehensive values. Thus, we implement an
idea originally postulated in the K-Nearest Neighbour (K-NN) algorithm, and additionally make the power of

support given by each of K closest reference alternatives equal to the reciprocal of an absolute value diﬀerence
from U (a). In this way, the reference alternatives with very similar scores to a have a greater impact on its

recommended assignment, i.e.:

M K

IV (a → Clk) =

a∗∈AR

a,K

AR
k

1
|U(a)−U(a∗)|

P

T

a∗∈AR

a,K

1
|U(a)−U(a∗)|

,

P

where AR
a,K is a subset of K reference alternatives with comprehensive values being the closest to U (a). For
example, when considering b from Figure 1, AR
b,K=3 = {a6, a7, a8} with a6 being assigned to Cl2 and a7
as well as a8 being assigned to Cl1. Thus, the supports given to the assignment of b to diﬀerent classes
are as follows: M K=3
(b → Cl2) =
(1/0.1)/(1/0.1 + 1/0.05 + 1/0.1) = 10/40, M K=3
(b → Cl3) = 0/(1/0.1 + 1/0.05 + 1/0.1) = 0/40, and MIV

(b → Cl1) = (1/0.05 + 1/0.1)/(1/0.1 + 1/0.05 + 1/0.1) = 30/40, M K=3

IV

IV

IV

13

assigns b to Cl1. We can also use a cross-validation to determine the optimal setting for K: for each possible
value of K, classify all alternatives a in the validation set by calculating M K
IV (a → Clk), k = 1, ..., q, and
then report the classiﬁcation accuracy on the validation set. Value of K that leads to the best classiﬁcation
performance can be chosen as the optimal setting for K. Note that K should never be greater than the
cardinality of the least numerous class so that the comparison of M K
IV (a → Clk) across classes is performed
fairly.

The assignments of reference alternatives a1,...,11 ∈ AR used in Example 1 (see Figure 1) were intentionally
selected so that to illustrate the diﬀerences in the sorting recommendation suggested for b by various methods
(for MI – Cl3, for MII and MIII – Cl2, and for MIV – Cl1). On the contrary, when considering Example 2 (see
Figure 2) with the consistency being preserved for all pairs of reference alternatives, the assignments suggested
for b by the four methods are the same. The detailed conﬁrmation degrees given by MI – MIV to diﬀerent
classes are provided in Table 1. Note that in case all assignment examples are perfectly reproduced, the scores
provided by MI and MII are the same for all classes.

0.6 0.65

0.4 0.45

0.15

0.55

0.75

0.3

0.0

0.1

0.9

1.0

) 

10 

11 

1 

2 

5 

6 

7 

4 

9 

3 

8 

3 

3 

2 

3 

1 

2 

1 

2 

1 

1 

2 

a

1 

C
1 

a
2 

C

1 

a

3 

C
1 

a

4 

C

1 

a

5 

C

2 

a

6 
C

2 

a

7 

C

2 

b 

a

8 
C

2 

a

9 

C
3 

a

10 
C
3 

a

11 
C
3 

U(a) 

0.0 

0.1 

0.15 

0.3 

0.4  0.45 

0.55 

0.6  0.65 

0.75 

0.9 

1.0 

Figure 2: Alternatives used to illustrate the use of four classiﬁcation methods in Example 2.

Table 1: Conﬁrmation degrees given by methods MI – MIV to classes Cl1 − Cl2 when considering assignment of alternative b
from Example 2.

Method
MI
MII
MIII
M K=3
IV

Cl1
5/7
5/7
126/240
0/40

Cl2
7/7
7/7
4/4
40/40

Cl3
6/8
6/8
43/90
0/40

Assignment
Cl2
Cl2
Cl2
Cl2

3. Experimental analysis

3.1. Application to research unit evaluation

In this section, we apply the proposed approach to a real-world problem of parametric evaluation of research

units. The Polish Ministry of Science and Higher Education carries out an evaluation of 993 units every 3 years
and assigns each unit to one of four classes C1 − C4 (with C1 and C4 being, respectively, the least and the
most preferred ones). All research units are divided into the following ﬁve subsets: 282 units from Humanities
and Social Sciences (HS), 218 units from Life Sciences (NZ), 286 units from Exact Sciences and Engineering

(SI), 99 units from Art and Artistic Creativity (TA), and 108 units judged as Inhomogeneous (NJN). They are
evaluated in terms of four gain-type criteria: scientiﬁc activity (g1), scientiﬁc potential (g2), material eﬀects of
unit’s activities (g3), and remaining (non-material) eﬀects of unit’s activities (g4). Even though all units are
evaluated on the same family of criteria, the authorities apply diﬀerent evaluation strategies for each subset
of units to reﬂect the speciﬁcity of various ﬁelds of science. The data considered in this section is originally
available at the website1.

1http://www.bip.nauka.gov.pl/kategorie-naukowe-przyznane-jednostkom-naukowym/wyniki-kompleksowej-oceny-jakosci-

dzialalnosci-naukowej-lub-badawczo-rozwojowej-jednostek-naukowych-2017.html

14

To validate the performance of proposed approach on this problem, we use ﬁve-fold cross-validation for the
purpose of model selection and performance validation. Speciﬁcally, each subset of the dataset corresponding

to a particular ﬁeld of science (HS, NZ, SI, TA and NJN) is randomly partitioned into ﬁve folds of equal size.
Then, four folds are used as the training data to construct a model, and the remaining one is used to test

the performance of the constructed model. In the training phase, one of the four training folds serves as the
validation data to determine the optimal setting for the two parameters C1 and C2 by examining the following
values:{10−8, 5 × 10−8, 10−7, 5 × 10−7,..., 107, 5 × 107, 108, 5 × 108}. We repeat this procedure ﬁve times
using diﬀerent training and test sets. Finally, we average the results from each test and report a summary of

the averaged performance.

The classiﬁcation accuracies of the proposed approach parametrized with diﬀerent numbers of sub-intervals
on each criterion (γj = 1, 2, 3), preference models (Γ1: the variant neglecting interactions among criteria; Γ2:
the counterpart modelling the interaction eﬀects in the product form (22)-(23); Γ3: the counterpart modelling
the interaction eﬀects in the minimum form (24)-(25)) and sorting methods (MI , MII , MIII and MIV ) are
summarized in Table 2. For the sake of simplicity, we suppose that all criteria have the same number of
sub-intervals. For sorting method MIV , we determine the optimal setting for K using cross-validation by
considering the following values: {1,2,...,10}. As can be observed from Table 2, by considering the interactions
among criteria, the classiﬁcation performance of the proposed approach improves as it is more ﬂexible in ﬁtting

the training data. The variant of the proposed approach modelling ghe interaction eﬀects in the product form
(22)-(23) achieves a better performance than its counterpart incorporating the minimum form (24)-(25). The

primary reason consists in that interaction terms in the product form (22)-(23) are strictly increasing with
respect to each component, thus having a higher ability in capturing the interactions among criteria than that
in the minimum form (24)-(25). As far as the comparison between diﬀerent sorting methods (MI , MII, MIII
and MIV ) is concerned, it is evident that MII outperforms MI, since MII diﬀerentiates the voting powers of
reference alternatives by considering their consistency. Furthermore, the advantage of MII, MIII and MIV
over their competitors is unclear. Finally, the performance of proposed approach is acceptable when coupled

with diﬀerent numbers of sub-intervals on each criterion, since we use the regularization terms to avoid the
problem of over-ﬁtting. However, the classiﬁcation accuracy attained with γj = 2 is slightly higher than for
the remaining values of γj.

The proposed approach provides useful information about both the importance of individual criteria and
the interactions between pairs of criteria. Since the accounted decision problem involves few criteria, we can

analyse the constructed preference models in an intuitive way. For illustrative purpose, let us present the
models constructed for the HS subset. The models generated by Γ1, Γ2 and Γ3 with the optimal classiﬁcation
performance (denoted by U Γ1 (a), U Γ2 (a) and U Γ3 (a), respectively) are presented in the following. One can
observe that, when neglecting the interactions among criteria, the weights of the four criteria are wg1 = ∆u1
g1 +
∆u2
g4 = 0.1694,
respectively:

g3 = 0.1957 and wg4 = ∆u1

g2 = 0.2151, wg3 = ∆u1

g1 = 0.4198, wg2 = ∆u1

g3 + ∆u2

g2 + ∆u2

g4 + ∆u2

U Γ1 (a) = 0.1724 · v1

g1 (a) + 0.2474 · v2

g1 (a) + 0.0806 · v1

+0.0612 · v1

g3 (a) + 0.1345 · v2

g3 (a) + 0.0557 · v1

g2 (a) + 0.1345 · v2
g4 (a) .

g4 (a) + 0.1137 · v2

g2 (a)

In case of modelling criteria interactions in the product form, the weights are wg1 = 0.6656, wg2 = 0.3891,
wg3 = 0.1462 and wg4 = 0.3177, respectively, and there are negative interactions for the pairs {g1, g3} and
{g2, g4}:

15

Table 2: Classiﬁcation accuracy of the proposed approach for the problem of parametric evaluation of Polish research units (Γ1:
the variant neglecting the interactions among criteria; Γ2: the counterpart modelling the interaction eﬀects in the product form
(22)-(23); Γ3: the counterpart modelling the interaction eﬀects in the minimum form (24)-(25)).

Subset

Sorting method

HS

NZ

SI

TA

NJN

MI
MII
MIII
MIV

MI
MII
MIII
MIV

MI
MII
MIII
MIV

MI
MII
MIII
MIV

MI
MII
MIII
MIV

γj = 1

Γ2
0.8587
0.8870
0.8665
0.8842

0.8829
0.9041
0.8858
0.8975

0.8106
0.8153
0.8125
0.8129

0.8654
0.8827
0.8744
0.8812

0.8218
0.8543
0.8537
0.8247

Γ3
0.8557
0.8726
0.8733
0.8768

0.8937
0.9073
0.8729
0.8883

0.7976
0.8078
0.7827
0.7871

0.8456
0.8567
0.8589
0.8802

0.8150
0.8344
0.8539
0.8010

Γ1
0.8484
0.8398
0.8778
0.8638

0.8992
0.9136
0.8694
0.8874

0.7703
0.8059
0.7636
0.7844

0.8435
0.8463
0.8544
0.8715

0.7967
0.8272
0.8575
0.7913

γj = 2

Γ2
0.9216
0.9600
0.9471
0.9334

0.8904
0.9295
0.9011
0.9216

0.8324
0.8512
0.8463
0.8457

0.8891
0.8921
0.8907
0.8915

0.8545
0.8752
0.8606
0.8562

Γ3
0.9159
0.9586
0.9406
0.9241

0.8490
0.9001
0.8744
0.9074

0.8264
0.8347
0.8451
0.8255

0.8856
0.8873
0.8949
0.8637

0.7912
0.8749
0.8599
0.8327

Γ1
0.8987
0.9556
0.8961
0.8669

0.8450
0.8961
0.8394
0.8938

0.8065
0.8267
0.8313
0.7876

0.8792
0.8865
0.9026
0.8559

0.7872
0.8749
0.8539
0.8248

γj = 3

Γ2
0.8640
0.8839
0.8804
0.8689

0.8872
0.9019
0.8899
0.8906

0.8286
0.8568
0.8549
0.8526

0.8730
0.9139
0.8862
0.8886

0.8228
0.8529
0.8489
0.8410

Γ3
0.8060
0.8365
0.8707
0.8435

0.8858
0.8917
0.8829
0.8467

0.8024
0.8489
0.8176
0.8303

0.8579
0.9141
0.8933
0.8927

0.8290
0.8200
0.8530
0.7901

Γ1
0.7996
0.8241
0.8336
0.8192

0.8677
0.8420
0.8817
0.8250

0.7830
0.8100
0.7986
0.8227

0.8394
0.9141
0.8952
0.9013

0.8299
0.7997
0.8547
0.7883

U Γ2 (a) = 0.2850 · v1

g1 (a) + 0.3806 · v2

g1 (a) + 0.1526 · v1

g2 (a) + 0.2365 · v2
g4 (a)

g4 (a) + 0.2695 · v2

g2 (a)

+0.0506 · v1

−0.1298 · v1

g3 (a) + 0.0956 · v2
g1 (a) · v1

g3 (a) − 0.0 · v1

−0.0 · v2

g1 (a) · v1

g3 (a) − 0.0 · v2

−0.1526 · v1

−0.2365 · v2

g2 (a) · v1
g2 (a) · v1

g4 (a) − 0.0 · v1
g4 (a) − 0.0 · v2

g3 (a) + 0.0482 · v1
g1 (a) · v2
g3 (a)
g3 (a)
g2 (a) · v2
g2 (a) · v2

g4 (a)
g4 (a) .

g1 (a) · v2

When it comes to the case of criteria interactions modelled in the minimum form, we derive the criteria weights
as wg1 = 0.4220, wg2 = 0.2164, wg3 = 0.1979 and wg4 = 0.1707, respectively, and there exist positive interaction
between g1 and g4 and negative interaction between g2 and g3:

g2 (a) + 0.1357 · v2
g4 (a)

g4 (a) + 0.1139 · v2

g2 (a)

U Γ3 (a) = 0.1735 · v1

g1 (a) + 0.2485 · v2

g1 (a) + 0.0807 · v1

+0.0623 · v1

+0.2092 · v1

g3 (a) + 0.1356 · v2
g1 (a) · v1

g4 (a) + 0.0 · v1

+0.0 · v2

g4 (a) + 0.0 · v2
g1 (a) · v1
g2 (a) , v1
v1
g3 (a)
g2 (a) , v1
v2
g3 (a)
(cid:9)
(cid:8)

−0.0395 · min

−0.0683 · min

g3 (a) + 0.0568 · v1
g1 (a) · v2
g4 (a)
g4 (a)

g1 (a) · v2

− 0.0411 · min

− 0.0674 · min

(cid:8)

g2 (a) , v2
v1
g2 (a) , v2
v2

g3 (a)
g3 (a)

(cid:9)

.

(cid:9)

(cid:8)
3.2. Experimental evaluation on several monotone learning datasets

(cid:8)

(cid:9)

In this section, we report the results of an extensive experimental study performed to validate the practical

performance of the proposed approach on several public datasets. The goal of this study is two-fold. First, we
compare the two variants of the proposed approach which either incorporate the interactions between criteria

or neglect them with the UTADIS method and the Choquet integral-based sorting model in terms of a set of
predictive metrics. Second, we investigate the information about the relative importance of criteria and the

16

interactions among criteria discovered by the proposed approach.

The datasets used in the experiments were collected from the UCI repository2 and the WEKA ma-
chine learning datasets3. For these datasets, after removing incomplete instances and descriptive attributes
(e.g., name or gender), the assumption of monotonicity can be made on the remaining variables, so that

the input and output variables can be deemed as criteria and decision classes, respectively. A summary
of the information about these datasets is reported in Table 3. The accounted datasets can be accessed
at https://cs.uni-paderborn.de/?id=63916.

Table 3: Datasets used in the experimental study and their properties.

Dataset
Den Bosch (DBS)
CPU
Breast Cancer (BCC)
Auto MPG (MPG)
Employee Selection (ESL)
Mammographic (MMG)
Employee Rejection/Acceptance (ERA)
Lecturers Evaluation (LEV)
Car Evaluation (CEV)

#Alternatives #Criteria #Classes Distribution of classes

120
209
278
392
488
830
1000
1000
1728

8
6
7
7
4
5
4
4
6

2
4
2
3
5
2
3
4
3

60/60
50/53/53/53
196/82
107/154/131
52/100/116/135/85
427/403
415/330/255
93/280/403/224
1210/384/134

We use the same experimental setting as in Section 3.1 for model selection and performance validation. The
evaluation metrics for quantifying the predictive performance of classiﬁcation methods include accuracy, pre-

cision, recall, and F-measure for detailed deﬁnitions of these metrics). The adopted version of the UTADIS
method estimates an additive value function model by minimizing the total misclassiﬁcation errors for all ref-

erence alternatives. Then, it uses a post optimality analysis to explore other or near optimal solutions, and
ﬁnally works out a preference model by averaging all found solutions. The Choquet integral-based sorting

model – being presented in Appendix C – is implemented by replacing the preference model in the proposed
approach with the Choquet integral. UTADIS, the Choquet integral-based sorting model and the proposed
approach are implemented in Java and the optimization models are solved using the Cplex solver4. We repeat
the cross-validation procedure 100 times, and then report the average of the results.

The comprehensive experimental results of the diﬀerent variants of the proposed approach are reported in
Appendix D. In the main paper, we report the results of the variant that achieves the highest classiﬁcation

accuracy (see Table 4) for the following analysis. To save space, we average the precisions, recalls and F-

measures for all classes and report only the means of these measures. As can be observed in Table 4, the
proposed approach compares favourably with UTADIS and the Choquet integral-based model for most datasets.

In particular, the variant of the proposed approach accounting for the interactions among criteria outperforms
its counterpart without considering such interactions.

Furthermore, we perform the one-tailed paired t-test to examine a statistical signiﬁcance of the observed
diﬀerences. For each t-test, the null-hypothesis is H0 : µmethod 1 6 µmethod 2 and the alternative hypothesis
is H1 : µmethod 1 > µmethod 2, where µmethod 1 and µmethod 2 denote the means of a measure for method 1
and method 2, respectively. Note that p-values less than a reasonable signiﬁcance level 0.05 indicate that the

null-hypothesis should be rejected and the alternative hypothesis is acceptable, and thus method 1 outperforms
method 2 in terms of the underlying metric. The results of t-test are reported in Table 5. They conﬁrm both

the competitive advantage of our approach over UTADIS the Choquet integral-based model for most datasets
as well as the superiority of the variant accounting for the interactions among criteria over the method’s

2http://archive.ics.uci.edu/ml/
3http://www.cs.waikato.ac.nz/ml/weka/datasets.html
4https://www.ibm.com/analytics/cplex-optimizer

17

Table 4: Classiﬁcation performance of UTADIS, Choquet integral-based sorting model, and the proposed approach (Type I: the
proposed approach neglecting the interactions among criteria; Type II: the proposed approach considering the interactions among
criteria).

Dataset

DBS

Metrics
Accuracy
Precision Avg.
Recall Avg.
F-measure Avg.

UTADIS
0.8247
0.8258
0.8249
0.8248

Choquet integral-
based approach
0.8534
0.8527
0.8513
0.8523

Proposed approach
Type II
Type I
0.8746
0.8362
0.8724
0.8401
0.8712
0.8377
0.8724
0.8381

CPU

BCC

MPG

ESL

MMG

ERA

LEV

CEV

Accuracy
Precision Avg.
Recall Avg.
F-measure Avg.

Accuracy
Precision Avg.
Recall Avg.
F-measure Avg.

Accuracy
Precision Avg.
Recall Avg.
F-measure Avg.

Accuracy
Precision Avg.
Recall Avg.
F-measure Avg.

Accuracy
Precision Avg.
Recall Avg.
F-measure Avg.

Accuracy
Precision Avg.
Recall Avg.
F-measure Avg.

Accuracy
Precision Avg.
Recall Avg.
F-measure Avg.

Accuracy
Precision Avg.
Recall Avg.
F-measure Avg.

0.9248
0.9231
0.9234
0.9232

0.7106
0.6514
0.6469
0.6489

0.8426
0.8457
0.8511
0.8410

0.8607
0.8481
0.8610
0.8531

0.8284
0.8295
0.8307
0.8288

0.8106
0.8030
0.8098
0.8038

0.8302
0.8274
0.8362
0.8281

0.8912
0.8798
0.8954
0.8845

0.9273
0.9278
0.9252
0.9261

0.7294
0.6924
0.7096
0.6861

0.8837
0.8819
0.8853
0.8800

0.8941
0.8536
0.8460
0.8423

0.8627
0.8736
0.8660
0.8623

0.8336
0.8329
0.8347
0.8325

0.9010
0.8924
0.9105
0.9040

0.9219
0.9238
0.9174
0.9187

0.9196
0.9169
0.9250
0.9214

0.7466
0.7296
0.7739
0.7282

0.8859
0.8818
0.8905
0.8841

0.8641
0.8565
0.8684
0.8612

0.8579
0.8588
0.8592
0.8585

0.8501
0.8428
0.8496
0.8453

0.8960
0.8891
0.9019
0.8935

0.9243
0.9147
0.9298
0.9201

0.9388
0.9426
0.9359
0.9381

0.7552
0.7447
0.7953
0.7385

0.8939
0.8874
0.8889
0.8880

0.9042
0.8858
0.8967
0.8901

0.8751
0.8900
0.8782
0.8723

0.8830
0.8897
0.8842
0.8863

0.9173
0.9080
0.9278
0.9208

0.9423
0.9485
0.9352
0.9410

18

counterpart neglecting such interactions.

Table 5: Results of one-tailed paired t-test for comparing performances of diﬀerent methods – (∗) denotes signiﬁcance at 5%
level (Comparison I: the proposed approach neglecting the interactions among criteria vs. UTADIS; Comparison II: the proposed
approach considering the interactions among criteria vs. UTADIS; Comparison III: the proposed approach considering the inter-
actions among criteria vs. Choquet integral-based approach; Comparison IV: the proposed approach considering the interactions
among criteria vs. that neglecting the interactions among criteria.).

Dataset

DBS

Metrics
Accuracy
Precision Avg.
Recall Avg.
F-measure Avg.

Comparison I
0.0001*
0.0001*
0.0000*
0.0000*

Comparison II
0.0000*
0.0000*
0.0000*
0.0000*

Comparison III
0.0001*
0.0000*
0.0000*
0.0000*

Comparison IV
0.0000*
0.0000*
0.0000*
0.0000*

CPU

BCC

MPG

ESL

MMG

ERA

LEV

CEV

Accuracy
Precision Avg.
Recall Avg.
F-measure Avg.

Accuracy
Precision Avg.
Recall Avg.
F-measure Avg.

Accuracy
Precision Avg.
Recall Avg.
F-measure Avg.

Accuracy
Precision Avg.
Recall Avg.
F-measure Avg.

Accuracy
Precision Avg.
Recall Avg.
F-measure Avg.

Accuracy
Precision Avg.
Recall Avg.
F-measure Avg.

Accuracy
Precision Avg.
Recall Avg.
F-measure Avg.

Accuracy
Precision Avg.
Recall Avg.
F-measure Avg.

0.0647
0.0972
0.0738
0.1619

0.0000*
0.0000*
0.0000*
0.0000*

0.0000*
0.0000*
0.0000*
0.0000*

0.1633
0.0099*
0.0021*
0.0000*

0.0000*
0.0000*
0.0000*
0.0000*

0.0000*
0.0000*
0.0000*
0.0000*

0.0000*
0.0000*
0.0000*
0.0000*

0.0000*
0.0000*
0.0000*
0.0000*

0.0003*
0.0000*
0.0128*
0.0000*

0.0000*
0.0000*
0.0000*
0.0000*

0.0000*
0.0000*
0.0000*
0.0000*

0.0000*
0.0000*
0.0000*
0.0000*

0.0000*
0.0000*
0.0000*
0.0000*

0.0000*
0.0000*
0.0000*
0.0000*

0.0000*
0.0000*
0.0000*
0.0000*

0.0000*
0.0000*
0.0000*
0.0000*

0.0175*
0.0000*
0.0033*
0.0000*

0.0000*
0.0000*
0.0000*
0.0000*

0.0000*
0.0672
0.0049*
0.0005*

0.0110*
0.0000*
0.0000*
0.0000*

0.0000*
0.0001*
0.0000*
0.0010*

0.0000*
0.0000*
0.0000*
0.0000*

0.0000*
0.0002*
0.0000*
0.0000*

0.0000*
0.0000*
0.0000*
0.0000*

0.0000*
0.0000*
0.0011*
0.0000*

0.0001*
0.0000*
0.0000*
0.0001*

0.0145*
0.0526
0.1103
0.0783

0.0000*
0.0000*
0.0000*
0.0000*

0.0000*
0.0000*
0.0000*
0.0003*

0.0000*
0.0000*
0.0000*
0.0000*

0.0000*
0.0000*
0.0000*
0.0000*

0.0000*
0.0000*
0.0072*
0.0000*

The advantage of the proposed approach over UTADIS may be due to two reasons. First, the ﬁtting ability

of UTADIS improves with the increase in the number of sub-intervals on criteria, but it may encounter the
In the proposed approach, the
over-ﬁtting problem, leading to a poor predictive accuracy on test data.

incorporation of the regularization term alleviates the over-ﬁtting problem and enhances the generalization
ability to test data. Second, when compared with UTADIS and the counterpart neglecting the interactions

among criteria, the variant of the proposed approach accounting for the interactions is more ﬂexible and has
the ability for capturing non-linear dependencies among criteria. When it comes to the Choquet integral-

based model, although it has a higher ﬂexibility in modelling the interactions among criteria, it requires the
performance on all criteria to be deﬁned on the same scale, which is a great limitation of this preference model.

In the experimental analysis, we have brought all performances to the same scale through a scaling method
(see Appendix C), which assumes that the marginal value functions on all criteria have the same form. This

deteriorates the ﬁtting ability of the preference model on several datasets.

We present the relative importances of all criteria and the interaction coeﬃcients in Table 6, which can

be obtained by calculating the marginal values on each criterion and the bonus and penalty coeﬃcients of

19

interacting criteria for a virtual alternative a∗ with the optimal performances on each criterion (i.e., gj (a∗) = βj,
j = 1, ..., n). They correspond to the preference models worked out by the proposed approach achieving the

highest test accuracy on each dataset. For example, for the DBS dataset, when neglecting the interactions,
we can order the eight criteria according to their relative importance as follows: g4 > g2 > g8 > g5 > g1 >
g3 > g6 > g7. When considering the interactions, one can observe that the order of criteria according to their
relative importances remains the same and additionally there exist positive interactions for pairs {g1, g5} and
{g3, g7} and negative interactions for pairs {g2, g8} and {g4, g6}.

Table 6: Criteria weights and interaction coeﬃcients discovered by the proposed approach (Type I: results obtained from proposed
approach neglecting the interactions among criteria; Type II: results obtained from proposed approach considering the interactions
among criteria; w(·) indicates a weight of a particular criterion which is derived from marginal value of virtual alternative a∗ on this
criterion; Φ(·,·) indicates the interaction intensity between a pair of interacting criteria which is obtained from bonus or penalty
value of virtual alternative a∗ on the two criteria).

Dataset

Type

DBS

Type I

Criteria weights and interaction coeﬃcients
wg1 =0.1044, wg2 =0.1884, wg3 =0.0772, wg4 =0.2751,
wg5 =0.1523, wg6 =0.0258, wg7 =0.0158, wg8 =0.1610

Type II

wg1 =0.0860, wg2 =0.2405, wg3 =0.0625, wg4 =0.2885,
wg5 =0.1032, wg6 =0.0295, wg7 =0.0187, wg8 =0.1874,
Φg1 ,g5 =+0.0226, Φg2 ,g8 =–0.0418, Φg3 ,g7 =+0.0258,
Φg4 ,g6 =–0.0228

CPU

Type I

wg1 =0.1915, wg2 =0.1330, wg3 =0.3285, wg4 =0.1643,
wg5 =0.0641, wg6 =0.1186

Type II

wg1 =0.1746, wg2 =0.1519, wg3 =0.3635, wg4 =0.1487,
wg5 =0.0409, wg6 =0.1036,
Φg1 ,g5 =+0.0365, Φg2 ,g3 =–0.0480, Φg4 ,g6 =+0.0284

BCC

Type I

wg1 =0.0309, wg2 =0.1819, wg3 =0.1997, wg4 =0.1495,
wg5 =0.2988, wg6 =0.0159, wg7 =0.1231

Type II

wg1 =0.0219, wg2 =0.2004, wg3 =0.2124, wg4 =0.1423,
wg5 =0.2799, wg6 =0.0126, wg7 =0.1432,
Φg1 ,g5 =+0.0107, Φg2 ,g7 =–0.0306, Φg3 ,g6 =+0.0073

MPG

Type I

wg1 =0.1727, wg2 =0.1278, wg3 =0.0806, wg4 =0.1088,
wg5 =0.2545, wg6 =0.0372, wg7 =0.2186

Type II

Type I

Type II

wg1 =0.1875, wg2 =0.1099, wg3 =0.0927, wg4 =0.0820,
wg5 =0.2284, wg6 =0.0329, wg7 =0.1952,
Φg1 ,g3 =–0.0215, Φg2 ,g7 =0.0508, Φg4 ,g5 =+0.0421

wg1 =0.2201, wg2 =0.2223, wg3 =0.2904, wg4 =0.2672

wg1 =0.1969, wg2 =0.2184, wg3 =0.2648, wg4 =0.2761,
Φg1 ,g3 =+0.0855, Φg2 ,g4 =–0.0418

ESL

MMG

Type I

wg1 =0.1369, wg2 =0.3665, wg3 =0.1923, wg4 =0.2166,
wg5 =0.0878

Type II

Type I

Type II

Type I

Type II

wg1 =0.1430, wg2 =0.3258, wg3 =0.1745, wg4 =0.2321,
wg5 =0.1041, Φg1,g4 =–0.0877,Φg2 ,g3 =+0.1082

wg1 =0.3123, wg2 =0.4381, wg3 =0.1560, wg4 =0.0936

wg1 =0.2928, wg2 =0.3401, wg3 =0.1661, wg4 =0.1130,
Φg1 ,g2 =+0.1254, Φg3 ,g4 =–0.0376

wg1 =0.3197, wg2 =0.4361, wg3 =0.0509, wg4 =0.1933

wg1 =0.3288, wg2 =0.4895, wg3 =0.0632, wg4 =0.2161,
Φg1 ,g2 =–0.1008, Φg3 ,g4 =+0.0033

ERA

LEV

CEV

Type I

wg1 =0.1624, wg2 =0.1439, wg3 =0.0411, wg4 =0.2593,
wg5 =0.0854, wg6 =0.3077

Type II

wg1 =0.1407, wg2 =0.1206, wg3 =0.0805, wg4 =0.2010,
wg5 =0.1192, wg6 =0.2380, Φg1,g2 =+0.0467,
Φg3 ,g5 =–0.0328, Φg4 ,g6 =+0.0860

To investigate the computational eﬀorts needed by the proposed approach for addressing datasets of various

20

sizes, we report its runtime in Table 7. One can observe the results in a two-fold way. First, when neglecting
the interactions between criteria, the proposed approach addresses all sorting problems in less than 25 seconds

and there is no signiﬁcant diﬀerence among the computation time of the proposed approach for dealing with
diﬀerent datasets although they involve various numbers of alternatives, criteria and classes. This is because,
in model (P2), the reference alternatives in each class Clk, k = 1, ..., q, are aggregated into the vector µk,
and then the computation time for a speciﬁc problem is not related to the number of available reference

alternatives. Moreover, since the numbers of criteria and classes for these datasets are small (less than 10), the
convex quadratic programming model in (P2) can be easily solved in a short time. Second, when considering the

interactions among criteria, the runtime of the proposed approach increases sharply with the number of criteria.
For the datasets involving four criteria (ESL, ERA, and LEV), the runtime is less than one minute, whereas

for problems involving eight criteria (DBS), the computation time already exceeds 1000 seconds. The reason
behind this phenomenon is that model (P3) involves a combinatorial optimization problem to ﬁnd the pairs

of interacting criteria and such a problem becomes more diﬃcult to solved with greater number of criteria. In

conclusion, the proposed approach is suitable for dealing with data-intensive tasks that contain a large number
of alternatives, whereas the number of criteria – in case of accounting for the potential interactions – highly

aﬀects the computation time of the proposed approach.

Table 7: Computation time of the proposed approach for addressing diﬀerent datasets (Type I: variant neglecting the interactions
among criteria; Type II: counterpart considering the interactions among criteria).

Dataset

DBS

CPU

BCC

MPG

ESL

MMG

ERA

LEV

CEV

Type
Type I
Type II
Type I
Type II
Type I
Type II
Type I
Type II
Type I
Type II
Type I
Type II
Type I
Type II
Type I
Type II
Type I
Type II

Mean
19.7550
1012.6324
19.9319
193.1181
19.7922
493.2548
20.0035
499.1771
19.9830
48.9712
19.6550
108.8784
20.0823
49.8116
20.1161
48.5154
20.3521
190.1866

Std.
1.2227
58.3988
1.2453
10.4591
1.0719
29.3294
1.8410
20.5614
1.2722
2.4282
1.1620
5.9832
1.1856
3.2252
0.9851
2.7686
1.1549
12.9837

Min.
17.8809
910.6261
17.6707
174.4472
18.0529
444.3109
17.9479
462.1352
17.7951
44.6182
17.7482
97.9610
18.1521
44.2085
18.2588
44.3528
18.1207
167.0662

Max.
21.5419
1114.1336
22.1471
210.1466
21.6751
540.7687
21.9236
540.1671
22.1560
53.5375
21.7930
119.0889
22.1972
55.1615
21.6508
53.4136
22.1505
211.7082

4. Conclusions

In this paper, we considered multiple criteria sorting problems with potentially interacting criteria and pro-

posed a new preference learning approach for constructing a preference model from a given set of decision
examples. We perceive the introduced method as a novel data-driven decision support tool, because the pref-

erence discovery is performed automatically without participation of the DM. As such, it connects the ﬁelds
of Multiple Criteria Decision Aiding and Machine Learning. On the one hand, the proposed approach starts

with an additive piecewise-linear value function as the preference model under the preferential independence
hypothesis, and then enriches it with two types of components for modelling the positive and negative inter-

actions between criteria. Therefore, the employed preference models have the advantage of comprehensibility
and can be easily understood and accepted by the DM. On the other hand, the proposed approach utilizes

methodological advances in Machine Learning to enhance the predictive ability of the constructed preference
model and the computational eﬃciency of the preference learning procedure. Speciﬁcally, to improve the gen-

eralization ability of the constructed model on new decision instances, it uses regularization terms to avoid

21

the problem of over-ﬁtting. Therefore, it is robust with respect to the way of dividing the performance scale
of each criterion into sub-intervals. In addition, the incorporated optimization models are formulated in such

a way that the complexity is irrelevant from the number of training samples, thus being capable of addressing
data-intensive tasks.

The practical example of research unit evaluation illustrates the applicability of the proposed approach
and intuitiveness of the arrived results. Furthermore, the experimental outcomes on several public datasets

demonstrate its advantage over UTADIS and the Choquet integral-based sorting method in terms of both
predictive performance and interpretability. Such a competitive advantage derives from accounting for the

interactions between criteria without transforming all performances to the same scale, making the proposed
approach suitable for capturing complex interdependencies in data. Moreover, the joint use of piecewise-linear

marginal value functions and regularization terms improves the ﬁtting ability and avoids the problem of over-
ﬁtting, thus making the generalization performance on new alternatives more advantageous.

Actually, our approach can be deemed as a general framework for addressing data-driven multiple criteria

sorting problems. Within such a framework, we provide a set of components for implementing a preference
learning procedure, including the variants accounting or neglecting the interactions among criteria, diﬀerent

types of expressions for quantifying the positive and negative interactions among criteria and a few sorting
methods for classifying non-reference alternatives. One can select various components to equip diﬀerent variants

of the proposed approach which prove to be more advantageous for diﬀerent learning tasks. However, one
needs to be aware that a prerequisite of our approach is the monotonicity assumption on the input variables

so that they can be regarded as criteria. Therefore, the proposed approach aims to learn a predictive model
that guarantees monotonicity and considers interactions of input variables simultaneously. When such an

assumption is not valid, one can consider non-monotonicity in the input variables as an alternative way for
representing the complex patterns in the data.

Future research can investigate the following directions. We can extend the proposed approach to consider
both interacting and non-monotonic criteria simultaneously. Moreover, other types of preference models can

be extended to capture interactions between criteria and then studied in the preference learning framework
for data-driven decision support. We will also develop computationally eﬃcient algorithms for dealing with

problems involving greater number of criteria. Finally, more real-world applications are needed to validate the

performance of the proposed approach.

References

Appendix A. Proof of Proposition 1

Proof. Let d∗

0 and d∗

0 6= d∗

spectively. Suppose that d∗
d∗
0 < d∗
1.
between any pair of reference alternatives (a, b) ∈ AR
(P0). Thus, according to constraint (5), we have that U ∗ (a) − U ∗ (b) = d∗ (a, b) ,
AR

1 denote the optimal values of the objective functions of models (P0) and (P1), re-
1, or (b):
1. Then, two possible cases would occur:
In case (a), let U ∗ (·) and d∗ (a, b) be, respectively, the value function and the diﬀerence
s+1 × AR
s , s = 1, ..., q − 1, at the optimum of model
s+1, b ∈
s , s = 1, ..., q − 1. By taking the transformation (7)-(11), we will derive that u∗Tµs+1 − u∗Tµs =
d∗ (a, b), s = 1, ..., q − 1, where u∗ is the value of u at the optimum. Ac-

0 > d∗

a ∈ AR

(a): d∗

1
s+1||AR
s |

s+1, b∈AR
|AR
s
cording to constraint (6), we have d∗

a∈AR

P

0 =

min
s=1,...,q−1

1
s+1||AR
s |

|AR

a∈AR

s+1, b∈AR
s

d∗ (a, b).

In this way, we

ﬁnd a value of u (i.e., u∗) so that the minimum value of uTµs+1 − uTµs, s = 1, ..., q − 1, is equal
P
to d∗
it contradicts that the optimal value of the objective function of model (P1)

0. As d∗

0 > d∗
1,

22

1
s+1|
|AR

(cid:18)
1
s+1||AR
s |
1
n(cid:12)
s+1||AR
(cid:12)
s |

0 > d∗

1 cannot hold. On the other hand,

1. Therefore, the assumption d∗

is d∗
let u∗
be the value of u at the optimum of model (P1). Then, we use U ∗ (·) and d∗ (a, b) to denote the
value function and the diﬀerence between any pair of reference alternatives (a, b) ∈ AR
s , s =
1, ..., q − 1, determined by u∗. That is to say, U ∗ (a) = u∗TV (a), a ∈ AR, and U ∗ (a) − U ∗ (b) =
s , s = 1, ..., q − 1. Then, for s = 1, ..., q − 1, we have u∗Tµs+1 − u∗Tµs =
d∗ (a, b) , a ∈ AR
− u∗T
U ∗ (b) =

s+1, b ∈ AR

s+1 × AR

in case (b),

V (a)

V (b)

u∗T

a∈AR

s+1

U ∗ (a) − 1
|AR
s |

b∈AR
s

1
|AR
s |

b∈AR
s

a∈AR

s+1

= 1
|AR

s+1|

(cid:19)
U ∗ (a) −

(cid:16)
AR

(cid:17)
U ∗ (b)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

s+1

o

=

s+1

P

P

P

|AR

|AR

a∈AR

a∈AR

b∈AR
s

P
AR
s

s+1, b∈AR
s

|AR
=
(cid:12)
P
a∈AR
(cid:12)
such that u∗Tµs∗+1−u∗Tµs∗ =
u∗Tµs+1 −u∗Tµs =
the diﬀerence d∗ (a, b) between any pair of reference alternatives (a, b) ∈ AR
that the optimal value of the objective function of model (P0) is equal to d∗
that the optimal value of the objective function of model (P0) is d∗
cannot hold. To sum up, the assumption d∗

P
1
s+1||AR
s |
d∗ (a, b). According to constraint (13), there must be some s∗ ∈ {1, ..., q − 1}
P
s∗ d∗ (a, b) = d∗
1, while for other s 6= s∗, we have
1. In this way, we ﬁnd value function U ∗ (·) and
s , s = 1, ..., q − 1, such
1 > d∗
0, it contradicts
0 < d∗
1
(cid:3)

0. Therefore, the assumption d∗

a∈AR
s∗+1, b∈AR
d∗ (a, b) > d∗

1 cannot hold and it must be that d∗

s+1 × AR
1. As d∗

P
{U ∗ (a) − U ∗ (b)}

1
s∗+1||AR

1
s+1||AR
s |

P
s+1, b∈AR
s

0 = d∗
1.

0 6= d∗

s+1, b∈AR
s

a∈AR

|AR

|AR

s∗ |

P

Appendix B. Proof of Proposition 2

Proof. We only present the proof for the case of the bonus and penalty components being deﬁned as (22) and

(23), and another case of (24) and (25) can be analysed analogously.

On the one hand, since xs
we have:

j > xs−1
j

, s = 1, ..., γj, according to monotonicity (b), for any {gj, gk} ∈ Syn,

xs
j

+ ugk

xt
k

+

syn+

gj ,gk

j, xt
xs
k

− syn−

gj ,gk

j , xt
xs
k

(cid:0)
xs−1
j

(cid:16)
(cid:1)
+ ugk

xt
k

+

(cid:0)
syn+

(cid:1)
gj ,gk

xs−1
j

, xt
k

(cid:0)
− syn−

(cid:1)(cid:17)i
gj ,gk

xs−1
j

, xt
k

> 0,

ugj
h

(cid:0)
−

(cid:1)
ugj
h

(cid:1)
for s = 1, ..., γj, t = 1, ..., γk. Then, according the deﬁnitions of ugj (·), syn+
the above inequality can be further transformed to:

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:0)

(cid:0)

(cid:16)

(cid:1)(cid:17)i
gj ,gk (·, ·), and syn−

gj ,gk (·, ·),

ugj
h

(cid:0)
−

(cid:1)
ugj
h

"

p=1
X

xs
j

+ ugk

xt
k

+

syn+

gj ,gk

j , xt
xs
k

− syn−

gj ,gk

j , xt
xs
k

(cid:0)
xs−1
j

(cid:16)
(cid:1)
+ ugk

xt
k

+

(cid:0)
syn+

(cid:1)
gj ,gk

xs−1
j

, xt
k

(cid:0)
− syn−

(cid:1)(cid:17)i
gj,gk

xs−1
j

, xt
k

s

=

(cid:0)
∆up
gj +

t
(cid:1)

(cid:0)
gk +

∆uq

(cid:1)

s

(cid:16)

t

(cid:0)
η+,p,q
gj ,gk −

s

t
(cid:1)

η−,p,q
gj ,gk

q=1
X

t

s−1

p=1
X

q=1
X
s−1

t

p=1
X

q=1
X
s−1

t

(cid:0)

(cid:1)(cid:17)i

!#

−

"

p=1
X

∆up

gj +

∆uq

gk +

η+,p,q
gj ,gk −

q=1
X

p=1
X

q=1
X

p=1
X

q=1
X

t

η−,p,q
gj ,gk

!#

= ∆us

gj +

gj ,gk − η−,s,q
η+,s,q
gj ,gk

> 0,

q=1 (cid:16)
X

(cid:17)

for s = 1, ..., γj, t = 1, ..., γk.

On the other hand, if ∆us

gj +

t

q=1
P

(cid:17)
then, for any pair of alternatives a, b such that gj (a) > gj (b) and gk (a) > gk (b), ∀ {gj, gk} ∈ Syn, let

(cid:16)

gj ,gk − η−,s,q
η+,s,q
gj ,gk

> 0,

s = 1, ..., γj, t = 1, ..., γk, ∀ {gj, gk} ∈ Syn,

23

 
 
.
us suppose that gj (a) ∈
Obviously, p > p′ and q > q′. There are four possible cases: (a) p = p′, q = q′, (b) p > p′, q = q′, (c)
i
p = p′, q > q′, and (d) p > p′, q > q′. In case (a), ∀ {gj, gk} ∈ Syn, we can formulate

, gk(a) ∈
i

, gj (b) ∈
i

and gk(b) ∈

i

h

h

k

k

j

j

, xp
j

xp−1
j
h

xq−1
k

, xq
k

xq′−1
h

xp′−1

, xp′

, xq′

ugj (gj (a)) + ugk (gk (a)) +

syn+

gj ,gk (gj (a) , gk (a)) − syn−

gj,gk (gj (a) , gk (a))

syn+

gj ,gk (gj (b) , gk (b)) − syn−

gj ,gk (gj (b) , gk (b))

(cid:17)i

h

=

−

ugj (gj (b)) + ugk (gk (b)) +
h
p−1

(cid:16)

(cid:16)

q−1

∆us

gj + ∆up

gj vp

gj (a)

!

+

∆us

gk + ∆uq

gk vq

gk (a)

t=1
X
p−1

+

!

gj ,gk − η−,p,t
η+,p,t
gj ,gk

vp
gj (a) +

gj ,gk − η−,s,q
η+,s,q
gj ,gk

p−1

q−1

(cid:17)i
gj ,gk − η−,s,t
η+,s,t
gj ,gk

t=1 (cid:16)
s=1
X
X
gj ,gk − η−,p,q
η+,p,q
gj ,gk

(cid:16)

p−1

q−1

(cid:17)

(cid:17)

gj (a) vq
vp

gk (a)

!#

vq
gk (a) +
(cid:17)

∆us

gk + ∆uq

gk vq

gk (b)

!

+

gj ,gk − η−,s,t
η+,s,t
gj ,gk

t=1 (cid:16)
X
p−1

" 
q−1

s=1
X

(cid:17)

∆us

gj + ∆up

gj vp

gj (b)

!

s=1 (cid:16)
X
q−1

+

t=1
X

p−1

" 

s=1
X
q−1

+

−

+

gj ,gk − η−,p,t
η+,p,t
gj ,gk

vp
gj (b) +

gj ,gk − η−,s,q
η+,s,q
gj ,gk

vq
gk (b) +

t=1 (cid:16)
X

(cid:17)

s=1 (cid:16)
X
gk (a) − vq
vq

gk (b)

(cid:17)
q−1

(cid:16)

+

gj ,gk − η−,p,t
η+,p,t
gj ,gk

= ∆up
gj

gj (a) − vp
vp

+ ∆uq
gk

gj (b)
(cid:17)

(cid:16)
p−1

+

gj ,gk − η−,s,q
η+,s,q
gj ,gk

(cid:0)
gk (a) − vq
vq

gk (b)

+

t=1 (cid:16)
X
(cid:1)
gj ,gk − η−,p,q
η+,p,q
gj ,gk

t=1 (cid:16)
s=1
X
X
gj ,gk − η−,p,q
η+,p,q
gj ,gk

(cid:17)

gj (b) vq
vp

gk (b)

!#

(cid:17)

gj (a) − vp
vp

gj (b)
(cid:17)

(cid:17) (cid:16)

gj (a) vq
vp

gk (a) − vp

gj (b) vq

(cid:17) (cid:16)

p−1

gk (b)
(cid:17)

s=1 (cid:16)
X
gj (a) − vp
vp

=

gj (b)
(cid:17)
gj ,gk − η−,p,q
η+,p,q
gj ,gk

(cid:16)

+

(cid:17) (cid:0)
∆up
gj +

q−1

(cid:16)

(cid:1)
gj ,gk − η−,p,t
η+,p,t
gj ,gk

t=1 (cid:16)
X
gj (a) vq
vp

gk (a) − vp

gj (b) vq

!

(cid:17)
gk (b)
(cid:17)

+

gk (a) − vq
vq

gk (b)

∆uq

gk +

gj ,gk − η−,s,q
η+,s,q
gj ,gk

(cid:0)
.

(cid:1)

s=1 (cid:16)
X

!

(cid:17)

(cid:16)
Note that vp

(cid:17) (cid:16)

gj (a)−vp

gj (b) > 0, vq

gk (a)−vq

gk (b) > 0, and vp

gj (a) vq

gk (a)−vp

gj (b) vq

gk (b) > 0, because gj (a) >

gj (b) and gk (a) > gk (b). Moreover, ∆up

gj +

gj ,gk − η−,p,t
η+,p,t
gj ,gk

> 0 and ∆uq

gk +

Then, let us consider the following quadratic multi-variate function:

(cid:17)

q−1

t=1
P

(cid:16)

p−1

s=1
P

(cid:16)

gj ,gk − η−,s,q
η+,s,q
gj ,gk

> 0.

(cid:17)

gj (a) , vp
vp

gj (b) , vq

gk (a) , vq

=

gj (a) − vp
vp

gk (b)
(cid:17)

(cid:16)

p−1

gj (b)
(cid:17)

q−1

∆up

gj +

gj ,gk − η−,p,t
η+,p,t
gj ,gk

t=1 (cid:16)
X

!

(cid:17)

f

(cid:16)

+

gk (a) − vq
vq

gk (b)

∆uq

gk +

gj ,gk − η−,s,q
η+,s,q
gj ,gk

(cid:0)

+

gj ,gk − η−,p,q
η+,p,q
gj ,gk

(cid:1)

s=1 (cid:16)
X
gk (a) − vp

gj (b) vq

gj (a) vq
vp

!

,

(cid:17)
gk (b)
(cid:17)

(cid:17) (cid:16)
whose Hessian matrix is given as:

(cid:16)

0

0
η+,p,q
gj ,gk − η−,p,q
gj ,gk
0

(cid:16)

H =











0

0

0

(cid:17)

−

gj ,gk − η−,p,q
η+,p,q
gj ,gk

(cid:16)

(cid:17)

24

η+,p,q
gj ,gk − η−,p,q
gj ,gk
0

(cid:16)

(cid:17)

0

−

η+,p,q
gj ,gk − η−,p,q
gj ,gk

0

0

(cid:16)

0

0

(cid:17)

.











 
 
 
 
 
 
 
 
H is positive semi-deﬁnite, and thus f

gj (a) , vp
vp

gj (b) , vq

gk (a) , vq

is convex. Then, the minimum

value of f
potential optimal values reported in Table 8 are greater than zero. Thus, ∀ {gj, gk} ∈ Syn, we have

can be found in Table 8.

It can be observed that all the

gj (a) , vp
vp

gj (b) , vq

gk (a) , vq

(cid:16)
gk (b)
(cid:17)

(cid:16)

gk (b)
(cid:17)

ugj (gj (a)) + ugk (gk (a)) +

syn+

gj ,gk (gj (a) , gk (a)) − syn−

gj ,gk (gj (a) , gk (a))

ugj (gj (b)) + ugk (gk (b)) +

(cid:16)

h

>

h

(cid:16)

syn+

gj,gk (gj (b) , gk (b)) − syn−

gj ,gk (gj (b) , gk (b))

(cid:17)i

.

(cid:17)i

In case (b), utilizing the results from case (a), we can prove:

ugj (gj (a)) + ugk (gk (a)) +
h

(cid:16)

>

>

>

>

j

(cid:16)

xp−1
j
xp′

+ ugk (gk (a)) +

(cid:17)
+ ugk (gk (a)) +

ugj
h
ugj
(cid:16)
h
ugj (gj (b)) + ugk (gk (a)) +
h
ugj (gj (b)) + ugk (gk (b)) +
h

(cid:16)

(cid:17)

(cid:16)

syn+

gj ,gk (gj (a) , gk (a)) − syn−

gj ,gk (gj (a) , gk (a))

, gk (a)

− syn−

gj,gk

(cid:17)i
, gk (a)

gj ,gk

syn+
(cid:16)
syn+

gj ,gk

xp−1
j
(cid:16)
xp′
j , gk (a)

(cid:17)

− syn−

gj,gk

xp−1
j
(cid:16)
xp′
j , gk (a)
(cid:16)

(cid:16)

(cid:17)

syn+

gj,gk (gj (b) , gk (a)) − syn−

gj ,gk (gj (b) , gk (a))

(cid:17)(cid:17)i

(cid:16)
syn+

gj ,gk (gj (b) , gk (b)) − syn−

gj ,gk (gj (b) , gk (b))

In case (c), utilizing the results from case (a), we can prove:

ugj (gj (a)) + ugk (gk (a)) +
h

>

>

>

>

(cid:16)
xq−1
k
xq′

k

(cid:16)

+

(cid:17)
+

ugj (gj (a)) + ugk
h
ugj (gj (a)) + ugk
(cid:16)
h
ugj (gj (a)) + ugk (gk (b)) +
h
ugj (gj (b)) + ugk (gk (b)) +
h

(cid:17)

(cid:16)

(cid:16)

syn+

gj ,gk (gj (a) , gk (a)) − syn−

gj ,gk (gj (a) , gk (a))

syn+

gj ,gk

(cid:16)
syn+

gj ,gk

gj (a) , xq−1
(cid:16)
gj (a) , xq′

k

k

− syn−

gj ,gk

gj (a) , xq−1

(cid:17)i
k

(cid:17)

− syn−

gj ,gk

(cid:16)

gj (a) , xq′

k

(cid:16)

(cid:17)

syn+

gj ,gk (gj (a) , gk (b)) − syn−

gj ,gk (gj (a) , gk (b))

(cid:17)(cid:17)i

(cid:16)

(cid:16)
syn+

gj ,gk (gj (b) , gk (b)) − syn−

gj ,gk (gj (b) , gk (b))

(cid:17)(cid:17)i

(cid:17)i
.

(cid:17)i

(cid:17)(cid:17)i

(cid:17)i
.

(cid:17)i

In case (d), utilizing the results from case (a), we can prove:

ugj (gj (a)) + ugk (gk (a)) +

syn+

gj ,gk (gj (a) , gk (a)) − syn−

gj ,gk (gj (a) , gk (a))

+ ugk

(cid:16)
xq−1
k

+

syn+

gj,gk

ugj

xp−1
j
(cid:16)
xp′
(cid:16)

j

ugj

(cid:17)
+ ugk

(cid:16)
syn+
(cid:16)
ugj (gj (b)) + ugk (gk (b)) +

(cid:16)
xq′
k
(cid:16)

+

(cid:17)

(cid:17)

(cid:17)

gj ,gk

xp−1
j
(cid:16)
xp′
j , xq′
(cid:16)

k

(cid:17)

h

>

>

>

h

h

h

, xq−1
k

gj ,gk

− syn−
(cid:16)
xp′
j , xq′
k

(cid:17)
− syn−
gj ,gk

xp−1
j

(cid:17)i
, xq−1
k

(cid:17)(cid:17)i

syn+

gj,gk (gj (b) , gk (b)) − syn−

(cid:16)

(cid:17)(cid:17)i
gj ,gk (gj (b) , gk (b))

.

(cid:16)

(cid:17)i

η+,s,q
gj ,gk − η−,s,q
gj ,gk

> 0, s = 1, ..., γj, t = 1, ..., γk, ∀ {gj, gk} ∈ Syn, then, for

To sum up, if ∆us

gj +

t

q=1
P

(cid:16)

any pair of alternatives a, b such that gj (a) > gj (b) and gk (a) > gk (b), ∀ {gj, gk} ∈ Syn, we have

(cid:17)

ugj (gj (a)) + ugk (gk (a)) +

syn+

gj ,gk (gj (a) , gk (a)) − syn−

gj ,gk (gj (a) , gk (a))

ugj (gj (b)) + ugk (gk (b)) +

(cid:16)

h

>

h

(cid:16)

syn+

gj,gk (gj (b) , gk (b)) − syn−

gj ,gk (gj (b) , gk (b))

(cid:17)i

.

(cid:17)i

(cid:3)

25

2
6

∆uq
gk

−

No.

1

2

3

4

5

6

7

8

9

10

Table 8: Potential optimal values of f (cid:16)vp

gj (a) , vp

gj (b) , vq

gk (a) , vq

gk (b)(cid:17) and solutions at the optimum.

+

vp
gj (a)
p−1
η+,s,q
gj ,gk
Ps=1 (cid:18)
η+,p,q
−η
gj ,gk
1

−,p,q
gj ,gk

−η−,s,q

gj ,gk (cid:19)

∆uq
gk

−

+

−η−,s,q

gj ,gk (cid:19)

vp
gj (b)
p−1
η+,s,q
gj ,gk
Ps=1 (cid:18)
η+,p,q
−η
gj ,gk
1

−,p,q
gj ,gk

∆up
gj

+

−

vp
gk (a)
q−1
η+,p,t
gj ,gk
Pt=1 (cid:18)
η+,p,q
−η
gj ,gk
1

−,p,q
gj ,gk

−η−,p,t

gj ,gk (cid:19)

∆up
gj

+

−

vp
gk (b)
q−1
η+,p,t
gj ,gk
Pt=1 (cid:18)
η+,p,q
−η
gj ,gk
1

−,p,q
gj ,gk

−η−,p,t

gj ,gk (cid:19)

1

0

1

1

0

1

1

0

0

0

1

0

0

1

0

0

1

1

1

1

1

0

0

0

1

1

0

0

0

0

0

0

Value of objective function

0

∆up

gj +

∆uq

gk +

∆uq

gk +

∆up

gj +

q

Pt=1

p

0
(cid:16)η+,p,t
gj ,gk
0
(cid:16)η+,s,q
gj ,gk

Ps=1
−,p,t
gj ,gk (cid:17) + ∆uq
p−1

Ps=1

q−1

Pt=1

(cid:16)η+,s,q
gj ,gk
0
(cid:16)η+,p,t
gj ,gk
0

− η

−,p,t
gj ,gk (cid:17)

− η

gk +

− η

−,s,q
gj ,gk (cid:17)
p−1

Ps=1
−,s,q
gj ,gk (cid:17)

− η

−,p,t
gj ,gk (cid:17)

∆up

gj +

q

Pt=1

(cid:16)η+,p,t
gj ,gk

− η

(cid:16)η+,s,q
gj ,gk

− η

−,s,q
gj ,gk (cid:17)

Appendix C. Using the Choquet integral as a preference model

In this section, we use the Choquet integral to represent interactions among criteria and illustrate how to
adapt the proposed approach to such a preference model. Let 2G denote the power set of G (i.e., all subsets
of criteria). A fuzzy measure (also called capacity) µ : 2G → [0, 1] is deﬁned such that it satisﬁes the following
boundary and monotonicity conditions:

(1a) µ (∅) = 0, µ (G) = 1,

(1b) µ (T ′) 6 µ (T ) for all T ′ ⊆ T ⊆ G.

In this context, µ (T ) for each T ⊆ G can be interpreted as the importance weight assigned to the subset of

criteria T . A useful representation of such a fuzzy measure is in terms of the M¨obius transform:

for all T ⊆ G, where the M¨obius representation mµ (T ′) is given by:

µ (T ) =

mµ (T ′),

XT ′⊆T

mµ (T ′) =

(−1)|T ′−T |µ (T ).

XT ⊆T ′

A fuzzy measure is called k-additive if µ (T ) = 0 for every T ⊆ G such that |T | > k. In practice, it is suﬃcient

to consider 2-additive measures which means interactions for n-tuples (n > 2) are neglected.

Given marginal function ugj (·) on each criterion gj and fuzzy measure µ (·), for each alternative a, the

Choquet integral is deﬁned as:

n

Cµ (a) =

ug(i)

g(i) (a)

− ug(i−1)

g(i−1) (a)

µ

T(i)

,

i=1
X

(cid:2)

(cid:0)

(cid:1)

(cid:0)

(cid:1)(cid:3)

(cid:0)

(cid:1)

where (·) is a permutation of the indices of criteria such that:

ug(n)

g(n) (a)

> ug(n−1)

g(n−1) (a)

> · · · > ug(1)

g(1) (a)

,

ug(0)

g(0) (a)
(cid:0)

(cid:1)

= 0,

and T(i) =

(cid:0)

g(i), ..., g(n)
(cid:1)

.

(cid:0)

(cid:1)

The Choquet integral can also be deﬁned in terms of the M¨obius representation as:

(cid:0)

(cid:1)

(cid:8)

(cid:9)

Cµ (a) =

XT ⊆G

mµ (T ) min
gi∈T

ugi (gi (a)).

When using the Choquet integral as a preference model, the main problem is that performances on all
criteria should be deﬁned on the same scale or that the marginal values need to be given beforehand as the

input of the problem. This is a strong assumption that limits the practical applicability of such a model. In this
section, we adopt a scaling method to normalize all performances to the [0, 1] scale. A simple transformation

to this end is given by:

g′
i (a) =

gi (a) − αi
βi − αi

.

The above transformation is problematic, because outliers would make the distribution skewed. Instead, we
can use the following mapping:

g′
i (a) = F (gi (a)) ,

27

where F is the cumulative distribution function such that F : gi (a) → P (X 6 gi (a)). Function F is not known
and we can use the empirical distribution ˆF of the training data to act as F (i.e., ˆF (gi (a)) indicating the
number of relative frequency of reference alternatives a′ such that gi (a′) 6 gi (a)). One needs to be aware that
adopting such a scaling method to bring all performances to the common scale is again troublesome, because it

assumes the marginal value functions on all criteria are limited to the same form, which is not appropriate for
many problems. From the viewpoint of statistical learning, such a scaling method would make the performance

of learning model less advantageous, since it reduces the ﬂexibility of an assumed preference model. However,
since it is considered a standard way of performing such a translation, we will investigate its performance in

the experimental study. Using the above scaling method, the Choquet integral can be reformulated as follows:

With respect to a 2-additive fuzzy measure, we can rewrite Cµ (a) as:

Cµ (a) =

XT ⊆G

mµ (T ) min
gi∈T

g′
i (a) .

Cµ (a) = mT

µ Vµ (a) ,

where mµ and Vµ (a) are two vectors such that:

mµ = (mµ ({g1}) , ..., mµ ({gn}) , mµ ({g1, g2}) , ..., mµ ({gn−1, gn}))T,

Vµ (a) =

1 (a) , ...g′
g′

n (a) , min

gi∈{g1,g2}

(cid:18)

g′
i (a) , ...,

min
gi∈{gn−1,gn}

g′
i (a)

T

.

(cid:19)

Then, we can adapt the proposed approach to the Choquet integral model and derive the following optimization
problem:

(P4) : M inimize − d + C1mT

µ Sµmµ,

s.t. mT

µ Θµ,k+1 > mT

µ Θµ,k + d, k = 1, ..., q − 1,

d > 0,

mµ ({gi}) +

mµ ({gi, gj}) = 1,

Xgi∈G
Xgi,gj ∈G
mµ ({gi}) > 0, gi ∈ G,

mµ ({gi}) +

mµ ({gi, gj}) > 0, gi ∈ G, T ⊆ G\ {gi} ,

Xgj ∈T

where Sµ =

q

k=1
P

a,b∈AR
k
P

(Vµ (a) − Vµ (b)) (Vµ (a) − Vµ (b))T and Θµ,k = 1
k |
|AR

Vµ (a). The last three

a∈AR
k
P

linear constraints ensure the boundary and monotonicity properties of the Choquet integral in case of using

a 2-additive fuzzy measure.

Appendix D. Experimental results of the proposed approach in Section 3.2

In the e-Appendix (supplementary material available on-line in form of a spreadsheet), we report the detailed
results for the experimental study described in Section 3.2. Speciﬁcally, we provide the average values of

accuracy, precision, recall, and F-measure for diﬀerent variants of the proposed approach applied to the nine

28

accounted datasets. These variants are distinguished by the incorporated classiﬁcation method, the way of
modelling the interactions between criteria as well as an assumed number of linear pieces for all marginal value

functions.

29

