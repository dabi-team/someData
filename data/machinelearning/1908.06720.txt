1
2
0
2

r
p
A
5

]
h
p
-
t
n
a
u
q
[

4
v
0
2
7
6
0
.
8
0
9
1
:
v
i
X
r
a

Quantum algorithms for Second-Order Cone Pro-
gramming and Support Vector Machines

Iordanis Kerenidis1,2, Anupam Prakash1,2, and Dániel Szilágyi2

1QCWare, Palo Alto, California
2Université de Paris, CNRS, IRIF, F-75006, Paris, France

(cid:16)

√

We present a quantum interior-point method (IPM) for second-order cone
where r is the
programming (SOCP) that runs in time eO
rank and n the dimension of the SOCP, δ bounds the distance of intermediate
solutions from the cone boundary, ζ is a parameter upper bounded by
n, and
κ is an upper bound on the condition number of matrices arising in the classical
IPM for SOCP. The algorithm takes as its input a suitable quantum description
of an arbitrary SOCP and outputs a classical description of a δ-approximate
(cid:15)-optimal solution of the given problem.

r ζκ
δ2 log (1/(cid:15))

√

n

(cid:17)

Furthermore, we perform numerical simulations to determine the values of
the aforementioned parameters when solving the SOCP up to a ﬁxed precision
(cid:15). We present experimental evidence that in this case our quantum algorithm
exhibits a polynomial speedup over the best classical algorithms for solving
general SOCPs that run in time O(nω+0.5) (here, ω is the matrix multiplication
exponent, with a value of roughly 2.37 in theory, and up to 3 in practice). For
the case of random SVM (support vector machine) instances of size O(n), the
quantum algorithm scales as O(nk), where the exponent k is estimated to be
2.59 using a least-squares power law. On the same family random instances,
the estimated scaling exponent for an external SOCP solver is 3.31 while that
for a state-of-the-art SVM solver is 3.11.

1 Introduction

It is well known that many interesting and relevant optimization problems in the domain of
Machine Learning can be expressed in the framework of convex optimization [6, 10]. The
landmark result in this area was the discovery of interior-point methods (IPM) by [23], and
their subsequent generalization to all “self-scaled” (i.e. symmetric) cones by [29, 30]. Very
recently, [13] have shown that it is possible to solve linear programs (LP) in eO(nω), the
time it takes to multiply two matrices (as long as ω ≥ 2 + 1/6, which is currently the case).
This result has been further extended in [27] to a slightly more general class of cones,
however, their techniques did not yield improved complexities for second-order (SOCP)
and semideﬁnite programming (SDP). An eﬃcient algorithm for SOCP would also yield
eﬃcient algorithms for many interesting problems, such as (standard and quadratically-
constrained) convex quadratic programming, portfolio optimization, and many others [1].

Iordanis Kerenidis: jkeren@irif.fr
Anupam Prakash: anupamprakash1@gmail.com

Dániel Szilágyi: dszilagyi@irif.fr

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

1

 
 
 
 
 
 
Starting with the landmark results of [20, 35], and, more recently, [21], it has been
demonstrated that quantum computers oﬀer signiﬁcant (sometimes even exponential) asymp-
totic speedups for a number of important problems. More recently, there has been sub-
stantial work in the area of convex optimization. Quantum speedups for gradient descent
were investigated by [17], whereas [7, 9, 40, 41] presented quantum algorithms for SDP
based on the the multiplicative weights framework of [4]. However, it has been diﬃcult to
establish asymptotic speedups for this family of quantum SDP solvers as their the running
time depends on problem-speciﬁc parameters, including a 5th-power dependence on the
width of the SDP. Interestingly, the recent result of [8] suggests that such a speedup might
be obtained when applying an SDP algorithm of this type to some low-precision instances
of quadratic binary optimization.

In an orthogonal approach, [26] proposed a quantum algorithm for LPs and SDPs
by quantizing a variant of the classical interior point method and using the state of the
art quantum linear algebra tools [11, 18] – in particular, the matrix multiplication and
inversion algorithms whose running time is sub-linear in the input size. However, the
complexity of this algorithm depends on the condition number of O(n2)-sized matrices
that is diﬃcult to bound theoretically. It therefore remains an open question to ﬁnd an
end-to-end optimization problems for which quantum SDP solvers achieve an asymptotic
speedup over state of the art classical algorithms. In this paper, we propose support vector
machines (SVM) as a candidate for such an end-to-end quantum speedup using a quantum
interior point method based algorithm.

1.1 Our results and techniques

In this section, we provide a high level sketch of our results and the techniques used for the
quantum interior point method for SOCPs, we begin by discussing the diﬀerences between
classical and quantum interior point methods.

√

√

A classical interior point method solves an optimization problem over symmetric cones
by starting with a feasible solution and iteratively ﬁnding solutions with a smaller duality
gap while maintaining feasibility. A single iterative step consists of solving a system of
linear equations called the Newton linear system and updating the current iterate using
the solutions of the Newton system. The analysis of the classical IPM shows that in each
iteration, the updated solutions remain feasible and the duality gap is decreased by a
factor of (1 − α/
n) where n is the dimension of the optimization problem and α > 0 is
a constant. The algorithm therefore converges to a feasible solution with duality gap (cid:15) in
O(

n log(1/(cid:15))) iterations.
A quantum interior point method [26] uses a quantum linear system solver instead of
classical one in each iteration of the IPM. However, there is an important diﬀerence between
classical and quantum linear algebra procedures for solving the linear system Ax = b.
Unlike classical linear system solvers which return an exact description of x, quantum
tomography procedures can return an (cid:15)-accurate solution ex such that kx − xk ≤ (cid:15)kxk
with O(n/(cid:15)2) runs of the quantum linear system solver. Additionally, these linear system
solvers require A and b to be given as block-encodings [11], so this input model is used
by our algorithm as well. One of the main technical challenges in developing a quantum
interior point method (QIPM) is to establish convergence of the classical IPM which uses
(cid:15)-approximate solutions of the Newton linear system (in the ‘2 norm) instead of the exact
solutions in the classical IPM.

The quantum interior point method for second-order cone programs requires additional
ideas going beyond those used for the quantum interior point methods for SDP [26]. Sec-
ond order cone programs are optimization problems over the product of second-order or

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

2

Lorentz cones (see section 2.1 for deﬁnitions), interior point methods for SOCP can be
described using the Euclidean Jordan algebra framework [28]. The Euclidean Jordan alge-
bra framework provides analogs of concepts like eigenvalues, spectral and Frobenius norms
for matrices and positive semideﬁnite constraints for the case of SOCPs. Using these con-
ceptual ideas from the Euclidean Jordan algebra framework [28] and the analysis of the
approximate SDP interior point method [26] we provide an approximate IPM for SOCP
that converges in O(
n log(1/(cid:15))) iterations. Approximate IPMs for SOCP have not been
previously investigated in the classical or the quantum optimization literature, this analysis
is one of the main technical contributions of this paper.

√

From an algorithmic perspective, SOCPs are much closer to LPs (Linear Programs)
than to SDPs, since for cones of dimension n, the Newton linear systems arising in LP and
SOCP IPMs are of size O(n), whereas in the SDP case they are of size O(n2). Namely, a
second-order conic constraint of dimension n can be expressed as a single PSD constraint
on a (sparse) n × n matrix [1] – this allows us to embed an SOCP with n variables and
m constraints in an SDP with an n × n matrix and m constraints. The cost of solving
that SDP would have a worse dependence on the error [40] or the input size [26]. On the
other hand, the quantum representations (block encodings) of the Newton linear systems
for SOCP are also much simpler to construct than those for SDP. The smaller size of the
SOCP linear system also makes it feasible to empirically estimate the condition number
for these linear systems in a reasonable amount of time allowing us to carry out extensive
numerical experiments to validate the running time of the quantum algorithm.

The theoretical analysis of the quantum algorithm for SOCP shows that its worst-case

running time is

(cid:18)

√

(cid:19)(cid:19)

(cid:18) 1
(cid:15)

,

r

n

eO

ζκ
δ2 log
where r is the rank and n the dimension of the SOCP, δ bounds the distance of intermediate
solutions from the cone boundary, ζ is a parameter bounded by
n, κ is an upper bound on
the condition number of matrices arising in the interior-point method for SOCPs, and (cid:15) is
the target duality gap. The running time of the algorithm depends on problem dependent
parameters like κ and δ that are diﬃcult to bound in terms of the problem dimension n –
this is also the case with previous quantum SDP solvers [26, 40] and makes it important
to validate the quantum optimization speedups empirically. Interestingly, since we require
a classical solution of the Newton system, the linear system solver could also be replaced
by a classical iterative solver [34] which would yield a complexity of O(n2√

rκ log(n/(cid:15))).

(1)

√

Let us make a remark about the complexity: as it is the case with all approximation
algorithms, (1) depends on the inverse of the target duality gap (cid:15), thus the running time
of our algorithm grows to inﬁnity as (cid:15) approaches zero, as in the case of classical IPM. Our
running time also depends on κ, which in turn is empirically observed to grow inversely
with the duality gap (in particular as O(1/(cid:15))) which again makes the running time go
to inﬁnity as (cid:15) approaches zero. The quantum IPM is a low precision method, unlike
the classical IPM, and it can oﬀer speedups for settings where the desired precision (cid:15) is
moderate or low. Thus although at ﬁrst glance it seems that the (cid:15)-dependence in (1) is
logarithmic, experimental evidence suggests that the factor ζκ
δ2 depends polynomially on
1/(cid:15).

Support Vector Machines (SVM) is an important application in machine learning, where
even a modest value of (cid:15) = 0.1 yields an almost optimal classiﬁer. Since the SVM training
problem can be reduced to SOCP, the quantum IPM for SOCP can be used to obtain
an eﬃcient quantum SVM algorithm. We perform extensive numerical experiments to
evaluate our algorithm on random SVM instances and compare it against state of the art

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

3

classical SOCP and SVM solvers.

The numerical experiments on random SVM instances indicate that the running time of
the quantum algorithm scales as roughly O(n2.591), where all the parameters in the running
time are taken into account and the exponent is estimated using a least squares ﬁt. We also
benchmarked the exponent for classical SVM algorithms on the same instances and for a
comparable accuracy, the scaling exponent was found to be 3.31 for general SOCP solvers
and 3.11 for state-of-the-art SVM solvers. We note that this does not prove a worst-case
asymptotic speedup, but the experiments on unstructured SVM instances provide strong
evidence for a signiﬁcant polynomial speedup of the quantum SVM algorithm over state-
of-the-art classical SVM algorithms. We can therefore view SVMs as a candidate problem
for which quantum optimization algorithms can achieve a polynomial speedup over state
of the art classical algorithms for an end-to-end application.

1.2 Related work

Our main result is the ﬁrst specialized quantum algorithm for training support-vector
machines (SVM). While several quantum SVM algorithms have been proposed, they are
unlikely to oﬀer general speedups for the most widely used formulation of SVM – the soft-
margin (‘1-)SVM (deﬁned in eq. (10)). On one hand, papers such as [3] formulate the
SVM as a SDP, and solve that using existing quantum SDP solvers such as [7, 9, 40, 41] –
with the conclusion being that a speedup is observed only for very speciﬁc sparse instances.
On the other hand, [33] solves an easier related problem – the least-squares SVM (‘2-SVM
or LS-SVM, see eq. (11) for its formulation), thus losing the desirable sparsity properties
of ‘1-SVM [38]. It turns out that applying our algorithm to this problem also yields the
same complexity as in [33] for the ‘2-SVM. Very recently, a quantum algorithm for yet
another variant of SVM (SVM-perf, [22]) has been presented in [2].

2 Preliminaries

2.1 Second-order cone programming

For the sake of completeness, in this section we present the most important results about
classical SOCP IPMs, from [1, 28]. We start by deﬁning SOCP as the optimization problem
over the product of second-order (or Lorentz) cones L = Ln1 × · · · × Lnr , where Lk ⊆ Rk
is deﬁned as Lk =
. In this paper we consider the problem
(2) and its dual (3):

x = (x0; ex) ∈ Rk (cid:12)

(cid:12)
(cid:12) k exk ≤ x0

o

n

min c>x
s.t. Ax = b
x ∈ L,

(2)

max b>y
s.t. A>y + s = c

s ∈ L, y ∈ Rm.

(3)

We call n := Pr

i=1 ni the size of the SOCP (2), and r is its rank.

A solution (x, y, s) satisfying the constraints of both (2) and (3) is feasible, and if in
addition it satisﬁes x ∈ int L and s ∈ int L, it is strictly feasible. If at least one constraint
of (2) or (3), is violated, the solution is infeasible. The duality gap of a feasible solution
(x, y, s) is deﬁned as µ := µ(x, s) := 1
r x>s. As opposed to LP, and similarly to SDP,
strict feasibility is required for strong duality to hold [1]. From now on, we assume that
our SOCP has a strictly feasible primal-dual solution (this assumption is valid, since the
homogeneous self-dual embedding technique from [42] allows us to embed (2) and (3) in a
slightly larger SOCP where this condition is satisﬁed).

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

4

2.2 Euclidean Jordan algebras

The cone Ln has an algebraic structure similar to that of symmetric matrices under the
matrix product. Here, we consider the Jordan product of (x0, ex) ∈ Rn and (y0, ey) ∈ Rn,
deﬁned as

x ◦ y :=

"

#

xT y
x0 ey + y0 ex

, and its identity element e :=

"

1
0n−1

#

.

This product is closely related to the (linear) matrix representation Arw(x) :=

which in turn satisﬁes the following equality:

"

exT

x0
ex x0In−1

#

,

x ◦ y = Arw(x)y = Arw(x) Arw(y)e.

The key observation is that this product induces a spectral decomposition of any vector x,
that has similar properties as its matrix relative. Namely, for any vector x we deﬁne

λ1(x) := x0 + k exk, c1(x) :=

λ2(x) := x0 − k exk, c2(x) :=

1
2

1
2

#

#

,

.

" 1
ex
kexk
" 1
−ex
kexk

(4)

We use the shorthands λ1 := λ1(x), λ2 := λ2(x), c1 := c1(x) and c2 := c2(x) whenever
x is clear from the context, so we observe that x = λ1c1 + λ2c2. The set of eigenvectors
{c1, c2} is called the Jordan frame of x, and satisﬁes several properties:

Proposition 1 (Properties of Jordan frames). Let x ∈ Rn and let {c1, c2} be its Jordan
frame. Then, the following holds:

1. c1 ◦ c2 = 0 (the eigenvectors are “orthogonal”)

2. c1 ◦ c1 = c1 and c2 ◦ c2 = c2

3. c1, c2 are of the form

(cid:16) 1

(cid:17)
2 ; ±ec

with keck = 1

2

On the other hand, just like a given matrix is positive (semi)deﬁnite if and only if all
of its eigenvalues are positive (nonnegative), a similar result holds for Ln and int Ln (the
Lorentz cone and its interior):

Proposition 2. Let x ∈ Rn have eigenvalues λ1, λ2. Then, the following holds:

1. x ∈ Ln if and only if λ1 ≥ 0 and λ2 ≥ 0.

2. x ∈ int Ln if and only if λ1 > 0 and λ2 > 0.

Now, using this decomposition, we can deﬁne arbitrary real powers xp for p ∈ R as

xp := λp

1c1 + λp

2c2, and in particular the “inverse” and the “square root”

c1 +

1
λ1

1
x−1 =
c2, if λ1λ2 6= 0,
λ2
x1/2 = pλ1c1 + pλ2c2, if x ∈ Ln.

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

5

Moreover, we can also deﬁne some operator norms, namely the Frobenius and the spectral
one:

q

√

1 + λ2
λ2

2 =
kxkF =
kxk2 = max{|λ1|, |λ2|} = |x0| + k exk.

2kxk,

Finally, we deﬁne an analogue to the operation Y 7→ XY X. It turns out that for this we
need another matrix representation (quadratic representation) Qx, deﬁned as

Qx := 2 Arw2(x) − Arw(x2) =

"

kxk2
2x0 ex λ1λ2In + 2 ex exT

2x0 exT

#

.

(5)

Now, the matrix-vector product Qxy will behave as the quantity XY X. To simplify the
notation, we also deﬁne the matrix Tx := Qx1/2.

The deﬁnitions that we introduced so far are suitable for dealing with a single constraint
x ∈ Ln. For dealing with multiple constraints x1 ∈ Ln1, . . . , xr ∈ Lnr , we need to deal
with block-vectors x = (x1; x2; . . . ; xr) and y = (y1; y2; . . . ; yr). We call the number of
blocks r the rank of the vector (thus, up to now, we were only considering rank-1 vectors).
Now, we extend all our deﬁnitions to rank-r vectors.

1. x ◦ y := (x1 ◦ y1; . . . ; xr ◦ yr)

2. The matrix representations Arw(x) and Qx are the block-diagonal matrices contain-

ing the representations of the blocks:

Arw(x) := Arw(x1) ⊕ · · · ⊕ Arw(xr) and Qx := Qx1 ⊕ · · · ⊕ Qxr

3. x has 2r eigenvalues (with multiplicities) – the union of the eigenvalues of the blocks
xi. The eigenvectors of x corresponding to block i contain the eigenvectors of xi as
block i, and are zero everywhere else.

4. The identity element is e = (e1; . . . ; er), where ei’s are the identity elements for the

corresponding blocks.

Thus, all things deﬁned using eigenvalues can also be deﬁned for rank-r vectors:
F := Pr

1. The norms are extended as kxk2

i=1kxik2
2. Powers are computed blockwise as xp := (xp

F and kxk2 := maxikxik2, and
1; . . . ; xp

r) whenever the corresponding

blocks are deﬁned.

Some further matrix-algebra inspired properties of block vectors are stated in the fol-

lowing two claims:

Claim 1 (Algebraic properties). Let x, y be two arbitrary block-vectors. Then, the following
holds:

1. The spectral norm is subadditive: kx + yk2 ≤ kxk2 + kyk2.

2. The spectral norm is less than the Frobenius norm: kxk2 ≤ kxkF .

3. If A is a matrix with minimum and maximum singular values σmin and σmax respec-

tively, then the norm kAxk is bounded as σminkxk ≤ kAxk ≤ σmaxkxk.

4. The minimum eigenvalue of x + y is bounded as λmin(x + y) ≥ λmin(x) − kyk2.

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

6

5. The following submultiplicativity property holds: kx ◦ ykF ≤ kxk2 · kykF .

In general, the proofs of these statements are analogous to the matrix case, with a few
notable diﬀerences: First, the vector spectral norm k·k2 is not actually a norm, since there
exist nonzero vectors outside L which have zero norm. It is, however, still bounded by
the Frobenius norm (just like in the matrix case), which is in fact a proper norm. Sec-
ondly, the minimum eigenvalue bound also holds for matrix spectral norms, with the exact
same statement. Finally, the last property is reminiscent of the matrix submultiplicativity
property kA · BkF ≤ kAk2kBkF .

We ﬁnish with several well-known properties of the quadratic representation Qx and

Tx.

Proposition 3 (Properties of Qx, from [1]). Let x ∈ int L. Then, the following holds:

1. Qxe = x2, and thus Txe = x.

2. Qx−1 = Q−1

x , and more generally Qxp = Qp

x for all p ∈ R.

3. kQxk2 = kxk2

2, and thus kTxk2 = kxk2.

4. Qx preserves L, i.e. Qx(L) = L and Qx(int L) = int L.

2.3 Interior-point methods

Our algorithm follows the general IPM structure, i.e. it uses Newton’s method to solve a
sequence of increasingly strict relaxations of the Karush-Kuhn-Tucker (KKT) optimality
conditions:

Ax = b, A>y + s = c
x ◦ s = νe, x ∈ L, s ∈ L,

(6)

where the parameter ν > 0 is decreased by a factor σ < 1 in each iteration. Since x◦s = νe
implies that the duality gap is µ = ν, by letting ν → 0 the IPM converges towards the
optimal solution. The curve traced by (feasible) solutions (x, y, s) of (6) for ν > 0 is called
the central path, and we note that all points on it are strictly feasible.

More precisely, in each iteration we need to ﬁnd ∆x, ∆y, ∆s such that xnext := x+∆x,
ynext := y + ∆y and snext := s + ∆s satisfy (6) for ν = σµ. After linearizing the product
xnext ◦ snext, we obtain the following linear system – the Newton system:






A
0
Arw(s)

0
I

0
A>
0 Arw(x)











∆x
∆y
∆s






 =




b − Ax
c − s − A>y
σµe − x ◦ s




 .

(7)

As a ﬁnal remark, it is not guaranteed that (xnext, ynext, snext) is on the central path (6),
or even that it is still strictly feasible. Luckily, it can be shown that long as (x, y, s) starts
out in a neighborhood N of the central path, (xnext, ynext, snext) will remain both strictly
r)
feasible and in N . It can be shown that this algorithm halves the duality gap every O(
iterations, so indeed, after O(
r log(µ0/(cid:15))) it will converge to a (feasible) solution with
duality gap at most (cid:15) (given that the initial duality gap was µ0).

√

√

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

7

2.4 Quantum linear algebra

As it was touched upon in the paper, the main speedup in our algorithms comes from the
fact that we use the quantum linear algebra algorithms from [11, 18] whose complexity is
sublinear in the dimension. Of course, we need to change our computational model for this
sentence to make any sense: namely, we encode n-dimensional unit vectors as quantum
states of a dlog2(n)e-qubit system. In other words, for a vector z ∈ R2k with kzk = 1,
we use the notation |zi to refer to the k-qubit state |zi := P2k−1
i=0 zi |ii, where |ii are the
standard basis vectors of C2k , the state space of a k-qubit system [32].

Given a quantum state |zi, we have very limited ways of interacting with it: we can
either apply a unitary transformation U : C2k → C2k , or we can measure it, which means
that we discard the state and obtain a single random integer 0 ≤ i ≤ 2k − 1, with the
probability of measuring i being z2
i . In particular this means that we can neither observe
the amplitudes zi directly, nor can we create a copy of |zi for an arbitrary |zi. In addition
to this, it is a priori not clear how (and whether it is even possible) to implement the
state |zi or an arbitrary unitary U using the gates of a quantum computer. Luckily, there
exists a quantum-classical framework using QRAM data structures described in [24] that
provides a positive answer to both of these questions.

The QRAM can be thought of as the quantum analogue to RAM, i.e.

an array
[b(1), . . . , b(m)] of w-bit bitstrings, whose elements we can access in poly-logarithmic time
given their address (position in the array). More precisely, QRAM is just an eﬃcient
implementation of the unitary transformation

|ii |0i⊗w 7→ |ii

(cid:12)
(cid:12)b(i)
1 . . . b(i)
(cid:12)
w

E

, for i ∈ [m].

The usefulness of QRAM data structures becomes clear when we consider the block encoding
framework:

Deﬁnition. Let A ∈ Rn×n be a symmetric matrix. Then, the ‘-qubit unitary matrix

U ∈ C2‘×2‘ is a (ζ, ‘) block encoding of A if U =

. For an arbitrary matrix

"

A/ζ
·

#
·
·

B ∈ Rn×m, a block encoding of B is any block encoding of its symmetrized version

sym(B) :=

"

#
.

0 B
B> 0

We want U to be implemented eﬃciently, i.e. using an ‘-qubit quantum circuit of
depth (poly-)logarithmic in n. Such a circuit would allow us to eﬃciently create states
|Aii corresponding to rows (or columns) of A. Moreover, we need to be able to construct
such a data structure eﬃciently from the classical description of A. It turns out that we
are able to fulﬁll both of these requirements using a data structure built on top of QRAM.

Theorem 1 (Block encodings using QRAM [24, 25]). There exist QRAM data structures
for storing vectors vi ∈ Rn, i ∈ [m] and matrices A ∈ Rn×n such that with access to these
data structures one can do the following:

1. Given i ∈ [m], prepare the state |vii in time eO(1). In other words, the unitary

|ii |0i 7→ |ii |vii can be implemented eﬃciently.

2. A (ζ(A), 2 log n) unitary block encoding for A with ζ(A) = kAk−1

2 min(kAkF , s1(A)),
j |Ai,j| can be implemented in time eO(log n). Moreover, this
where s1(A) = maxi
block encoding can be constructed in a single pass over the matrix A, and it can be
updated in O(log2 n) time per entry.

P

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

8

From now on, we will also refer to storing vectors and matrices in QRAM, meaning
that we use the data structure from Theorem 1. Note that this is the same quantum oracle
model that has been used to solve SDPs in [26] and [40].

Once we have these block encodings, we may use them to perform linear algebra. In
(cid:12)A−1b(cid:11), corresponding to

particular, we want to construct the quantum states |Abi and (cid:12)
the matrix-vector product Ab and the solution of the linear system Ax = b:

Theorem 2 (Quantum linear algebra with block encodings [11, 18]). Let A ∈ Rn×n be a
matrix with non-zero eigenvalues in the interval [−1, −1/κ] ∪ [1/κ, 1], and let (cid:15) > 0. Given
an implementation of an (ζ, O(log n)) block encoding for A in time TU and a procedure for
preparing state |bi in time Tb,
1. A state (cid:15)-close to (cid:12)

(cid:12)A−1b(cid:11) can be generated in time O((TU κζ + Tbκ) polylog(κζ/(cid:15))).

2. A state (cid:15)-close to |Abi can be generated in time O((TU κζ + Tbκ) polylog(κζ/(cid:15))).

3. For A ∈ {A, A−1}, an estimate Λ such that Λ ∈ (1 ± (cid:15))kAbk can be generated in time

O((TU + Tb) κζ

(cid:15) polylog(κζ/(cid:15))).

Finally, in order to recover classical information from the outputs of a linear system solver,
we require an eﬃcient procedure for quantum state tomography. The tomography procedure
is linear in the dimension of the quantum state.

Theorem 3 (Eﬃcient vector state tomography, [26]). There exists an algorithm that given
a procedure for constructing |xi (i.e. a unitary mapping U : |0i 7→ |xi and its controlled
version in time TU ) and precision δ > 0 produces an estimate x ∈ Rn with kxk = 1 such
7δ with probability at least (1 − 1/n0.83). The algorithm runs in time
that kx − xk ≤
O

√

(cid:16)

(cid:17)

.

TU

n log n
δ2

Of course, repeating this algorithm eO(1) times allows us to increase the success proba-
bility to at least 1 − 1/ poly(n). Putting Theorems 1, 2 and 3 together, assuming that A
and b are already in QRAM, we obtain that the complexity of a completely self-contained
algorithm for solving the system Ax = b with error δ is eO
. For well-conditioned
matrices, this presents a signiﬁcant improvement over O(nω) (or, in practice, O(n3)) needed
for solving linear systems classically, especially when n is large and the desired precision is
not too high. This can be compared with classical iterative linear algebra algorithms [34]
that have O(nnz(A)) complexity per iteration, as well as the new quantum-inspired solvers
[19] that have high-degree polynomial dependence on the rank, error, and the condition
number.

n · κζ
δ2

(cid:16)

(cid:17)

3 A quantum interior-point method

Having introduced the classical IPM for SOCP, we can ﬁnally introduce our quantum
IPM (Algorithm 1). The main idea is to use quantum linear algebra as much as possible
(including solving the Newton system – the most expensive part of each iteration), and
falling back to classical computation when needed. Since quantum linear algebra introduces
inexactness, we need to deal with it in the analysis. The inspiration for the “quantum part”
of the analysis is [26], whereas the “classical part” is based on [28]. Nevertheless, the SOCP
analysis is unique in many aspects and a number of hurdles had to be overcome to the
make the analysis go through. In the rest of the section, we give a brief introduction of

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

9

Require: Matrix A and vectors b, c in QRAM, precision parameter (cid:15)

1. Find feasible initial point (x, y, s, µ) and store it in QRAM.

2. Repeat the following steps for O(

√

r log(µ0/(cid:15))) iterations:

(a) Compute the vector σµe − x ◦ s classically and store it in QRAM.

(b) Prepare and update the block encodings of the LHS and the RHS of the

Newton system (7)

(c) Solve the Newton system to obtain |(∆x; ∆y; ∆s)i, and obtain a classical

approximate solution (∆x; ∆y; ∆s) using tomography.

(d) Update x ← x + ∆x, s ← s + ∆s and store in QRAM.
(e) Update µ ← 1

r x>s.

3. Output (x, y, s).

Algorithm 1: A quantum IPM for SOCP

the most important quantum building blocks we use, as well as present a sketch of the
analysis.

First, we note that the algorithms from the previous section allow us to “forget” that
Algorithm 1 is quantum, and treat it as a small modiﬁcation of the classical IPM, where
the system (7) is solved up to an ‘2-error δ. Since Algorithm 1 is iterative, the main part of
the analysis is proving that a single iteration preserves closeness to the central path, strict
feasibility, and improves the duality gap. In the remainder of this section, we state our
main results informally, while the exact statements and proofs of all claims can be found
in the supplementary material.

Theorem 4 (Per-iteration correctness, informal). Let (x, y, s) be a strictly feasible primal-
dual solution that is close to the central path, with duality gap µ, and at distance at least δ
from the boundary of L. Then, the Newton system (7) has a unique solution (∆x, ∆y, ∆s).
There exist positive constants ξ, α such that the following holds: If we let ∆x, ∆s be
approximate solutions of (7) that satisfy

k∆x − ∆xkF ≤ ξδ and k∆s − ∆skF ≤ ξδ,

and let xnext := x + ∆x and snext := s + ∆s be the updated solution, then:

1. The updated solution is strictly feasible, i.e. xnext ∈ int L and snext ∈ int L.

2. The updated solution is close to the central path, and the new duality gap is less than

√

(1 − α/

r)µ.

The proof of this theorem consists of 3 main parts:

1. Rescaling x and s so that they commute in the Jordan-algebraic sense [1]. This part

can be reused from the classical analysis [28].

2. Bounding the norms of ∆x and ∆s, and proving that x + ∆x and s + ∆s are still
strictly feasible (in the sense of belonging to int L). This part of the analysis is also
inspired by the classical analysis, but it has to take into account the inexactness of
the Newton system solution.

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

10

3. Proving that the new solution (x + ∆x, y + ∆y, s + ∆s) is in the neighborhood of
the central path, and the duality gap/central path parameter have decreased by a
factor of 1 − α/
r, where α is constant. This part is the most technical, and while it
is inspired by [26], it required using many of the Jordan-algebraic tools from [1, 28].

√

Theorem 4 formalizes the fact that the Algorithm 1 has the same iteration invariant as
the classical IPM. Since the duality gap is reduced by the same factor in both algorithms,
their iteration complexity is the same, and a simple calculation shows that they need O(
r)
iterations to halve the duality gap. On the other hand, the cost of each iteration varies,
since the complexity of the quantum linear system solver depends on the precision ξδ,
the condition number κ of the Newton matrix, as well as its ζ-parameter. While exactly
bounding these quantities is the subject of future research, it is worth noting that for ζ,
we have the trivial bound ζ ≤
n, and research on iterative linear algebra methods [14]
suggests that κ = O(1/µ) = O(1/(cid:15)). The ﬁnal complexity is summarized in the following
theorem:

√

√

Theorem 5. Let (2) be a SOCP with A ∈ Rm×n, m ≤ n, and L = Ln1 × · · · × Lnr . Then,
Algorithm 1 achieves duality gap (cid:15) in time

(cid:18)√

T = eO

r log (µ0/(cid:15)) ·

nκζ
δ2 log

(cid:18) κζ
δ

(cid:19)(cid:19)

,

where the eO(·) notation hides the factors that are poly-logarithmic in n and m.

Finally, the quality of the resulting (classical) solution is characterized by the following

theorem:

Theorem 6. Let (2) be a SOCP as in Theorem 5. Then, after T iterations, the (linear)
infeasibility of the ﬁnal iterate x, y, s is bounded as

kAx − bk ≤ δkAk,
kA>y + s − ck ≤ δ (kAk + 1) .

4 Technical results

In this section, we present our main technical results – the proofs of Theorems 4, 5 and 6.

4.1 Central path

In addition to the central path deﬁned in (6), we deﬁne the distance from the central path
as d(x, s, ν) = kTxs − νekF , so the corresponding η-neighborhood is given by

Nη(ν) := {(x, y, s) | (x, y, s) strictly feasible and d(x, s, ν) ≤ ην}.

Using this neighborhood deﬁnition, we can specify what exactly do we mean when we claim
that the important properties of the central path are valid in its neighborhood as well.

Lemma 1 (Properties of the central path). Let ν > 0 be arbitrary and let x, s ∈ int L.
Then, x and s satisfy the following properties:

1. For all ν > 0, the duality gap and distance from the central path are related as

|x>s − rν| ≤

r r
2

· d(x, s, ν).

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

11

2. The distance from the central path is symmetric in its arguments i.e. d(x, s, ν) =

d(s, x, ν).

3. Let µ = 1

r x>s. If d(x, s, µ) ≤ ηµ, then (1 + η)ks−1k2 ≥ kµ−1xk2.

Proof. For part 1, let {λi}2r
Then using the properties of Tx, we have

i=1 be the eigenvalues of Txs, note that Tx is invertible as x ∈ L.

x>s = x>T −1

x Txs = (Tx−1x)>Txs = e>Txs =

1
2

2r
X

i=1

λi.

We can therefore bound the duality gap x>s as follows,

x>s =

1
2

2r
X

i=1

λi ≤ rν +

1
2

2r
X

i=1

|λi − ν| ≤ rν +

r r
2

v
u
u
t

2r
X

i=1

(λi − ν)2 = rν +

r r
2

· d(x, s, ν).

The second step used the Cauchy-Schwarz inequality while the third follows from the
deﬁnition d(x, s, ν)2 = P2r
i=1(λi − ν)2. The proof of the lower bound is similar, but starts
instead with the inequality

1
2

2r
X

i=1

λi ≥ rν −

1
2

2r
X

i=1

|ν − λi|.

For part 2, it suﬃces to prove that Txs and Tsx have the same eigenvalues. This follows
from part 2 of Theorem 10 in [1]. Finally for part 3, as d(s, x, µ) ≤ ηµ we have,

ηµ ≥ kTsx − µekF

= kTsx − µ (TsTs−1) ekF
= kTs (x − µTs−1e)kF
≥ λmin(Ts)kx − µTs−1ekF
≥ λmin(Ts)kx − µs−1k2

=

1
ks−1k2

· µ · kµ−1x − s−1k2

Therefore, ηks−1k2 ≥ kµ−1x − s−1k2. Finally, by the triangle inequality for the spectral
norm,

ηks−1k2 ≥ kµ−1xk2 − ks−1k2,

so we can conclude that ks−1k2 ≥ 1

1+η kµ−1xk2.

4.2 A single quantum IPM iteration

Recall that the essence of our quantum algorithm is repeated solution of the Newton system
(7) using quantum linear algebra. As such, our goal is to prove the following theorem:

Theorem 4 (Per-iteration correctness, formal). Let χ = η = 0.01 and ξ = 0.001 be
positive constants and let (x, y, s) be a feasible solution of (2) and (3) with µ = 1
r x>s and
√
n, the Newton system (7) has a unique solution
d(x, s, µ) ≤ ηµ. Then, for σ = 1 − χ/
(∆x, ∆y, ∆s). Let ∆x, ∆s be approximate solutions of (7) that satisfy

k∆x − ∆xkF ≤

ξ
kTx−1k

, k∆s − ∆skF ≤

ξ
2kTs−1k

,

where Tx and Tx are the square roots of the quadratic representation matrices in equation
(5). If we let xnext := x + ∆x and snext := s + ∆s, the following holds:

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

12

1. The updated solution is strictly feasible, i.e. xnext ∈ int L and snext ∈ int L.
2. The updated solution satisﬁes d(xnext, snext, µ) ≤ ηµ and 1

r x>

nextsnext = µ for µ = σµ,

σ = 1 − α√

r and a constant 0 < α ≤ χ.

Since the Newton system (7) is the same as in the classical case, we can reuse Theorem
1 from [28] for the uniqueness part of Theorem 4. Therefore, we just need to prove the
two parts about strict feasibility and improving the duality gap. Our analysis is inspired
by the general case analysis from [5], the derived SDP analysis from [26], and uses some
technical results from the SOCP analysis in [28]. The proof of Theorem 4 consists of three
main steps:

1. Rescaling x and s so that they share the same Jordan frame.

2. Bounding the norms of ∆x and ∆s, and proving that x + ∆x and s + ∆s are still

strictly feasible (in the sense of belonging to int L).

3. Proving that the new solution (x + ∆x, y + ∆y, s + ∆s) is in the η-neighborhood
of the central path, and the duality gap/central path parameter have decreased by a
factor of 1 − α/

n, where α is constant.

√

4.3 Rescaling x and s

As in the case of SDPs, the ﬁrst step of the proof uses the symmetries of the Lorentz cone
to perform a commutative scaling, that is to reduce the analysis to the case when x and
s share the same Jordan frame. Although ◦ is commutative by deﬁnition, two vectors
sharing a Jordan frame are akin to two matrices sharing a system of eigenvectors, and thus
commuting (some authors [1] say that the vectors operator commute in this case). The
easiest way to achieve this is to scale by Tx = Qx1/2 and µ−1, i.e. to change our variables
as

x 7→ x0 := T −1

x x = e and s 7→ s0 := µ−1Txs.

Note that for convenience, we have also rescaled the duality gap to 1. Recall also that
in the matrix case, the equivalent of this scaling was X 7→ X −1/2XX −1/2 = I and S 7→
µ−1X 1/2SX 1/2. We use the notation z0 to denote the appropriately-scaled vector z, so
that we have

∆x0 := T −1

x ∆x, ∆s0 := µ−1Tx∆s

For approximate quantities (e.g. the ones obtained using tomography, or any other approxi-
mate linear system solver), we use the notation ·
, so that the increments become ∆x and
0
:= µ−1Tx∆s. Finally, we
∆s, and their scaled counterparts are ∆x
denote the scaled version of the next iterate as x0
and s0
. Now,
next := „ + ∆s
0
we see that the statement of Theorem 4 implies the following bounds on k∆x0 − ∆x
kF
and k∆s0 − ∆s

x ∆x and ∆s
0
next := e + ∆x

:= T −1

0

0

0

kF :
0
k∆x0 − ∆x

k∆s0 − ∆s

0

kF = kTx−1∆x − Tx−1∆xkF

≤ kTx−1k · k∆x − ∆xkF ≤ ξ, and

kF = µ−1kTx∆s − Tx∆skF
≤ µ−1kTxkk∆s − ∆skF
= µ−1kxk2k∆s − ∆skF
≤ (1 + η)ks−1k2k∆s − ∆skF by Lemma 1
≤ 2kTs−1kk∆s − ∆skF ≤ ξ.

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

13

Throughout the analysis, we will make use of several constants: η > 0 is the distance
from the central path, i.e. we ensure that our iterates stay in the η-neighborhood Nη of
the central path. The constant σ = 1 − χ/
r is the factor by which we aim to decrease our
duality gap, for some constant χ > 0. Finally constant ξ > 0 is the approximation error for
the scaled increments ∆x
. Having this notation in mind, we can state several facts
about the relation between the duality gap and the central path distance for the original
and scaled vectors.

, ∆s

√

0

0

Claim 2. The following holds for the scaled vectors x0 and „:

1. The scaled duality gap is 1

r x0>„ = 1.

2. d(x, s, µ) ≤ ηµ is equivalent to k„ − ek ≤ η.

3. d(x, s, µσ) = µ · d(x0, „, σ), for all σ > 0.

At this point, we claim that it suﬃces to prove the two parts of Theorem 4 in the scaled

case. Namely, assuming that x0

next ∈ int L and s0

next ∈ int L, by construction we get

xnext = Txx0

next and snext = Tx−1s0

next

and thus xnext, snext ∈ int L.

next, s0

On the other hand, if µd(x0
r x0>

next, σ) ≤ ηµ, then d(xnext, snext, µ) ≤ ηµ follows by
Claim 2. Similarly, from 1
next = σ, we also get 1
nextsnext = µ. We conclude this
part with two technical results from [28], that use the auxiliary matrix Rxs deﬁned as
Rxs := Tx Arw(x)−1 Arw(s)Tx. These results are useful for the later parts of the proof of
Theorem 4.

nexts0

r x>

Claim 3 ([28], Lemma 3). Let η be the distance from the central path, and let ν > 0 be
arbitrary. Then, Rxs is bounded as

kRxs − νIk ≤ 3ην.

Claim 4 ([28], Lemma 5, proof). Let µ be the duality gap. Then, the scaled increment ∆s0
is

∆s0 = σe − „ − µ−1Rxs∆x0.

4.4 Maintaining strict feasibility

The main tool for showing that strict feasibility is conserved is the following bound on the
increments ∆x0 and ∆s0:

Lemma 2 ([28], Lemma 6). Let η be the distance from the central path and let µ be the
duality gap. Then, we have the following bounds for the scaled direction:

k∆x0kF ≤ Θ√
√
2
k∆s0kF ≤ Θ

2

, where Θ =

2pη2/2 + (1 − σ)2r
1 − 3η

Moreover, if we substitute σ with its actual value 1 − χ/
, which
we can make arbitrarily small by tuning the constants. Now, we can immediately use this
result to prove x0

r, we get Θ =

next, s0

next ∈ int L.

√

√

2η2+4χ2
1−3η

Lemma 3. Let η = χ = 0.01 and ξ = 0.001. Then, x0
i.e. x0

next ∈ int L.

next, s0

next and s0

next are strictly feasible,

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

14

0
Proof. By Lemma 2, λmin(x) ≥ 1 − k∆x
d(x, s, µ) ≤ ηµ, we have d(x0, „, 1) ≤ η, and thus

kF ≥ 1 − Θ√
2

− ξ. On the other hand, since

η2 ≥ k„ − ek2

F =

2r
X

i=1

(λi(„) − 1)2

The above equation implies that λi(„) ∈ [1 − η, 1 + η] , ∀i ∈ [2r]. Now, since kzk2 ≤ kzkF ,

λmin(s) ≥ λmin(„ + ∆s0) − k∆s

0

≥ λmin(„) − k∆s0kF − k∆s
√
≥ 1 − η − Θ

2 − ξ,

− ∆s0kF
0

− ∆s0kF

where we used Lemma 2 for the last inequality. Substituting η = χ = 0.01 and ξ = 0.001,
we get that λmin(x) ≥ 0.8 and λmin(s) ≥ 0.8.

4.5 Maintaining closeness to central path

Finally, we move on to the most technical part of the proof of Theorem 4, where we prove
that x0
next is still close to the central path, and the duality gap has decreased by a
constant factor. We split this into two lemmas.

next, s0

Lemma 4. Let η = χ = 0.01, ξ = 0.001, and let α be any value satisfying 0 < α ≤ χ. Then,
r, the distance to the central path is maintained, that is, d(x0
next, σ) <
for σ = 1 − α/
ησ.

next, s0

√

Proof. By Claim 2, the distance of the next iterate from the central path is

d(x0

next, s0

next, σ) = kTx0

next

s0
next − σekF ,

and we can bound it from above as

d(x0

next, s0

next, σ) = kTx0
= kTx0

next

next

≤ kTx0

next

s0
next − σekF
s0
next − σTx0
k · ks0

next − σ · (x0

next

Tx0−1

next

ekF
next)−1k.

So, it is enough to bound kzkF := ks0

next − σ · x0−1

nextkF from above, since

kTx0

next

k = kx0

0
nextk2 ≤ 1 + k∆x

k2 ≤ 1 + k∆x0k2 + ξ ≤ 1 +

Θ
√
2

+ ξ.

We split z as

0

„ + ∆s

z =

(cid:16)

|

− σe + ∆x
{z
z1

0(cid:17)

}

0
+ (σ − 1)∆x
}

|

{z
z2

(cid:16)

0
e − ∆x

+ σ
|

0
− (e + ∆x
{z
z3

,

)−1(cid:17)
}

and we bound kz1kF , kz2kF , and kz3kF separately.

1. By the triangle inequality, kz1kF ≤ k„ + ∆s0 − σe + ∆x0kF + 2ξ. Furthermore, after

substituting ∆s0 from Claim 4, we get

„ + ∆s0 − σe + ∆x0 = σe − µ−1Rxs∆x0 − σe + ∆x0
α − χ
√
r

e + µ−1(µI − Rxs)∆x0.

=

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

15

Using the bound for kµI − Rxsk from Claim 3 as well as the bound for k∆x0kF from
Lemma 2, we obtain

kz1kF ≤ 2ξ +

χ
√
r

+

3
√
2

ηΘ.

2. kz2kF ≤ χ√
r

(cid:16) Θ√
2

(cid:17)

+ ξ

, where we used the bound from Lemma 2 again.

3. Here, we ﬁrst need to bound k(e + ∆x0)−1 − (e + ∆x

0

)−1kF . For this, we use the

submultiplicativity of k·kF :

k(e + ∆x0)−1 − (e + ∆x

0

)−1kF = k(e + ∆x0)−1 ◦
0
+ ∆x0 − ∆x

0
≤ k(e + ∆x0)−1k2 · ke − (e + ∆x
0
= k(e + ∆x0)−1k2 · k(∆x0 − ∆x
)−1kF
0
0
≤ k(e + ∆x0)−1k2 · k∆x0 − ∆x
)−1k2
kF · k(e + ∆x
0
)−1k2.
≤ ξ · k(e + ∆x0)−1k2 · k(e + ∆x

0
) ◦ (e + ∆x

(cid:16)

0
e − (e + ∆x0) ◦ (e + ∆x

)−1(cid:17)

kF

0
) ◦ (e + ∆x

)−1kF

Now, we have the bound k(e + ∆x0)−1k2 ≤
1−k∆x0kF −ξ , so we get

1

1
1−k∆x0kF

and similarly k(e + ∆x

0

)−1k2 ≤

0
k(e + ∆x0)−1 − (e + ∆x

)−1kF ≤

ξ
(1 − k∆x0kF − ξ)2 .

Using this, we can bound kz3kF :

(cid:18)

kz3kF ≤ σ

ke − ∆x0 − (e + ∆x0)−1kF + ξ +

ξ
(1 − k∆x0kF − ξ)2

(cid:19)

.

If we let λi be the eigenvalues of ∆x0, then by Lemma 2, we have

ke − ∆x0 − (e + ∆x0)−1kF

(cid:18)

(1 − λi) −

(cid:19)2

1
1 + λi

λ4
i
(1 + λi)2 ≤

√

Θ
2 − Θ

v
u
u
t

2r
X

i=1

λ2
i

v
u
u
t

2r
X

i=1

v
u
u
t

2r
X

=

=

≤

i=1
Θ2
√

2 −

.

2Θ

Combining all bound from above, we obtain

d(x0

next, s0

next, σ) ≤

(cid:18)

1 +

Θ
√
2

(cid:19)

(cid:18)

·

+ ξ

2ξ +

+

χ
√
r

+ σ

ηΘ

3
√
2
(cid:19)

+ ξ

+

χ
√
r
(cid:18) Θ
√
2
Θ2
√

2 −

+ ξ +

2Θ

ξ
√
(1 − Θ/

2 − ξ)2

!!

.

Finally, if we plug in χ = 0.01, η = 0.01, ξ = 0.001, we get d(x, s, σ) ≤ 0.005σ ≤ ησ.

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

16

 
Now, we prove that the duality gap decreases.

Lemma 5. For the same constants, the updated solution satisﬁes 1
for α = 0.005.

r x0>

nexts0

next =

(cid:16)

1 − α√
r

(cid:17)

Proof. Since x0
counterparts is µ
we obtain the upper bound

next and s0
nexts0
r x0>

next are scaled quantities, the duality gap between their unscaled
next,

next. Applying Lemma 1 (and Claim 2) with ν = σµ and x0

next, s0

µx0>

nexts0

next ≤ rσµ +

r r
2

µd(x0

next, s0

next, σ),

which in turn implies

1
r

nexts0
x0>

next ≤

(cid:18)

1 −

0.01
√
r

(cid:19) (cid:18)

1 +

d(x0

next, s0
next, σ)
√
2r
σ

(cid:19)

.

By instantiating Lemma 1 for α = χ, from its proof, we obtain d(x0
and thus

next, s0

next, σ) ≤ 0.005σ,

1
r

nexts0
x0>

next ≤ 1 −

0.005
√
r

Therefore, the ﬁnal α for this Lemma is 0.005.

4.6 Final complexity and feasibility

In every iteration we need to solve the Newton system to a precision dependent on the
norms of Tx−1 and Ts−1. Thus, to bound the running time of the algorithm (since the
complexity of the vector state tomography procedure depends on the desired precision),
we need to bound kTx−1k and kTs−1k. Indeed, by the properties of the quadratic represen-
tation, we get

kTx−1k = kx−1k = λmin(x)−1 and
kTs−1k = ks−1k = λmin(s)−1.

If the tomography precision for iteration i is chosen to be at least (i.e. smaller than)

δi :=

ξ
4

min {λmin(xi), λmin(si)} ,

(8)

then the premises of Theorem 4 are satisﬁed. The tomography precision for the entire
algorithm can therefore be chosen to be δ := mini δi. Note that these minimum eigenvalues
are related to how close the current iterate is to the boundary of L – as long as xi, si are
not “too close” to the boundary of L, their minimal eigenvalues should not be “too small”.
There are two more parameters that impact the complexity of the quantum linear
system solver: the condition number of the Newton matrix κi and the matrix parameter
ζi of the QRAM encoding in iteration i. For both of these quantities we deﬁne their global
versions as κ = maxi κi and ζ = maxi ζi. Therefore, we arrive to the following statement
about the complexity of Algorithm 1.

Theorem 5 (restatement). Let (2) be a SOCP with A ∈ Rm×n, m ≤ n, and L =
Ln1 × · · · × Lnr . Then, our algorithm achieves duality gap (cid:15) in time

(cid:18)√

T = eO

r log (µ0/(cid:15)) ·

nκζ
δ2 log

(cid:18) κζ
δ

(cid:19)(cid:19)

.

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

17

This complexity can be easily interpreted as product of the number of iterations and
the cost of n-dimensional vector tomography with error δ. So, improving the complexity
of the tomography algorithm would improve the running time of our algorithm as well.

Note that up to now, we cared mostly about strict (conic) feasibility of x and s. Now,
we address the fact that the linear constraints Ax = b and A>y + s = c are not exactly
satisﬁed during the execution of the algorithm. Luckily, it turns out that this error is not
accumulated, but is instead determined just by the ﬁnal tomography precision:

Theorem 6 (restatement). Let (2) be a SOCP as in Theorem 5. Then, after T iterations,
the (linear) infeasibility of the ﬁnal iterate x, y, s is bounded as

kAxT − bk ≤ δkAk,
kA>yT + sT − ck ≤ δ (kAk + 1) .

Proof. Let (xT , yT , sT ) be the T -th iterate. Then, the following holds for AxT − b:

AxT − b = Ax0 + A

>
X

t=1

∆xt − b = A

>
X

t=1

∆xt.

(9)

On the other hand, the Newton system at iteration T has the constraint A∆xT = b−AxT −1,
which we can further recursively transform as,

A∆xT = b − AxT −1 = b − A

(cid:16)

xT −2 + ∆xT −1

(cid:17)

= b − Ax0 −

T −1
X

t=1

∆xt = −

T −1
X

t=1

∆xt.

Substituting this into equation (9), we get

AxT − b = A

(cid:16)

∆xT − ∆xT

(cid:17)

.

Similarly, using the constraint A>∆yT + ∆sT = c − sT −1 − A>yT −1 we obtain that

A>yT + sT − c = A> (cid:16)

∆yT − ∆yT

(cid:17)

(cid:16)

+

∆sT − ∆sT

(cid:17)

.

Finally, we can bound the norms of these two quantities,

kAxT − bk ≤ δkAk,
kA>yT + sT − ck ≤ δ (kAk + 1) .

5 Quantum Support-Vector Machines

In this section we present our quantum support vector machine (SVM) algorithm as an
application of our SOCP solver. Given a set of vectors X = {x(i) ∈ Rn | i ∈ [m]} (training
examples) and their labels y(i) ∈ {−1, 1}, the objective of the SVM training process is to
ﬁnd the “best” hyperplane that separates training examples with label 1 from those with
label −1. In this paper we focus on the (traditional) soft-margin (‘1-)SVM, which can be
expressed as the following optimization problem:

min
w,b,ξ
s.t.

kwk2 + Ckξk1
y(i)(w>x(i) + b) ≥ 1 − ξi, ∀i ∈ [m]
ξ ≥ 0.

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

(10)

18

Here, the variables w ∈ Rn and b ∈ R correspond to the hyperplane, ξ ∈ Rm corresponds
to the “linear inseparability” of each point, and the constant C > 0 is a hyperparameter
that quantiﬁes the tradeoﬀ between maximizing the margin and minimizing the constraint
violations.

As a slightly less traditional alternative, one might also consider the ‘2-SVM (or least-
squares SVM, LS-SVM) [37], where the kξk1 regularization term is replaced by kξk2. This
formulation arises from considering the least-squares regression problem with the con-
straints y(i)(w>x(i) + b) = 1, which we solve by minimizing the squared 2-norm of the
residuals:

kwk2 + Ckξk2

min
w,b,ξ
s.t.

y(i)(w>x(i) + b) = 1 − ξi, ∀i ∈ [m]

(11)

Since this is a least-squares problem, the optimal w, b and ξ can be obtained by solving
a linear system.
In [33], a quantum algorithm for LS-SVM is presented, which uses a
single quantum linear system solver. Unfortunately, replacing the ‘1-norm with ‘2 in the
objective of (11) leads to the loss of a key property of (‘1-)SVM – weight sparsity [38].

5.1 Reducing SVM to SOCP

Finally, we are going to reduce the SVM problem (10) to SOCP. In order to do that, we
deﬁne an auxiliary vector t = (t + 1; t; w), where t ∈ R – this allows us to “compute” kwk2
using the constraint t ∈ Ln+2 since

t ∈ Ln+2 ⇔ (t + 1)2 ≥ t2 + kwk2 ⇔ 2t + 1 ≥ kwk2.

Thus, minimizing kwk2 is equivalent to minimizing t. Note we can restrict our bias b to
be nonnegative without any loss in generality, since the case b < 0 can be equivalently
described by a bias −b > 0 and weights −w. Using these transformations, we can restate
(10) as the following SOCP:

min
t,b,ξ

s.t.

h

i>

0 1 0n 0 Cmi h
b ξ
t


0
1
0
... X > ... diag(y)
...








1
0
0


1 −1
0
t ∈ Ln+2, b ∈ L1, ξi ∈ L1 ∀i ∈ [m]


 =

t
b
ξ

0m

0n








"

#

y
1

(12)

Here, we use the notation X ∈ Rn×m for the matrix whose columns are the training
examples x(i), and y ∈ Rm for the vector of labels. This problem has O(n + m) variables,
and O(m) conic constraints (i.e. its rank is r = O(m)). Therefore, in the interesting case
n) iterations. More precisely, if we consider both
of m = Θ(n), it can be solved in eO(
the primal and the dual, in total they have 3m + 2n + 7 scalar variables and 2m + 4 conic
constraints.

√

In practice (as evidenced by the LIBSVM and LIBLINEAR libraries [12, 16]), a small
modiﬁcation is made to the formulations (10) and (11): instead of treating the bias sepa-
rately, all data points are extended with a constant unit coordinate. In this case, the SOCP
formulation remains almost identical, with the only diﬀerence being that the constraints
t ∈ Ln+2 and b ∈ L1 are replaced by a single conic constraint (t; b) ∈ Ln+3. This change
allows us to come up with a simple feasible initial solution in our numerical experiments,
without going through the homogeneous self-dual formalism of [42].

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

19

Figure 1: Observed complexity of Algorithm 1, its
power law ﬁt, and 95% conﬁdence interval.

Figure 2: Empirical CDF of the diﬀerence in accu-
racy between SVMs trained in diﬀerent ways.

Note also that we can solve the LS-SVM problem (11), by reducing it to a SOCP in
a similar manner. In fact, this would have resulted in just O(1) conic constraints, so an
IPM would converge to a solution in eO(1) iterations, which is comparable with the result
from [33].

5.2 Experimental results

We next present some experimental results to assess the running time parameters and the
performance of our algorithm for random instances of SVM. If an algorithm demonstrates
a speedup on unstructured instances like these, it is reasonable to extrapolate that the
speedup is generic, as it could not have used any special properties of the instance to
derive an advantage. For a given dimension n and number of training points m, we denote
our distribution of random SVMs with SVM(n, m, p), where p denotes the probability that
a datapoint is misclassiﬁed by the optimal separating hyperplane. Additionally, for every
training set sampled from SVM(n, m, p), a corresponding test set of size bm/3c was also
sampled from the same distribution. These test sets are used to evaluate the generalization
error of SVMs trained in various ways.

Our experiments consist of generating roughly 16000 instances of SVM(n, 2n, p), where
n is chosen to be uniform between 22 and 29 and p is chosen uniformly from the discrete
set {0, 0.1, . . . , 0.9, 1}. The instances are then solved using a simulation of Algorithm 1
(with the target duality gap of (cid:15) = 0.1) as well as using the ECOS SOCP solver [15] (with
the default target duality gap). We simulate the execution of Algorithm 1 by implement-
ing the classical IPM and adding noise to the solution of the Newton system (7). The
noise added to each coordinate is uniform, from an interval selected so that the noisy in-
crement (∆x, ∆y, ∆s) simulates the outputs of the tomography algorithm with precision
determined by Theorem 4. The SVM parameter C is set to be equal to 1 in all experi-
ments. Additionally, a separate, smaller experiment with roughly 1000 instances following
the same distribution is performed for comparing Algorithm 1 with LIBSVM [12] using a
linear kernel.

The experiments are performed on a Dell Precision 7820T workstation with two Intel
Xeon Silver 4110 CPUs and 64GB of RAM, and experiment logs are available at [39].
By ﬁnding the least-squares ﬁt of the power law y = axb through the observed values of
the quantity n1.5κζ
, we obtain the exponent b = 2.591, and its 95% conﬁdence interval
[2.564, 2.619] (this interval is computed in the standard way using Student’s t-distribution,

δ2

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

20

as described in [31]). These observations, the power law, and its conﬁdence interval are
show on Figure 1. Thus, we can say that for random SVM(n, 2n, p)-instances, and ﬁxed
(cid:15) = 0.1, the complexity of Algorithm 1 scales as O(n2.591). This represents a polynomial
improvement over general dense SOCP solvers with complexity O(nω+0.5).
In practice,
the polynomial speedup is conserved when compared to ECOS [15], that has a measured
running time scaling of O(n3.314), with a 95% conﬁdence interval for the exponent of
[3.297, 3.330] (this is consistent with the internal usage of a [36]-like matrix multiplication
algorithm, with complexity O(n2.807)). Neglecting constant factors, this gives us a speedup
of 104 for n = 106. The results from the LIBSVM solver indicate that the training time
with a linear kernel has a complexity of O(n3.112), with a 95% conﬁdence interval for the
exponent of [2.799, 3.425]. These results suggest Algorithm 1 retains its advantage even
when compared to state-of-the-art specialized classical algorithms.

Additionally, we use the gathered data to verify that the accuracy of our quantum (or
approximate) SVM is close to the optimum: Figure 2 shows that both at train- and at
test-time the accuracies of all three classiﬁers are most of the time within a few percent of
each other, with Algorithm 1 often outperforming the exact SOCP SVM classiﬁer.

In conclusion, the performed numerical experiments indicate that Algorithm 1 provides
a polynomial speedup for solving SOCPs with low- and medium precision requirements.
In particular, for SVM, we achieve a polynomial speedup with no detriment to the quality
of the trained classiﬁer.

Acknowledgmenets

This work was partly supported by IdEx Université de Paris ANR-18-IDEX-0001, as well
the French National Research Agency (ANR) projects QuBIC and QuDATA.

References

[1] F. Alizadeh and D. Goldfarb. Second-order cone programming. Math. Program., 95
(1, Ser. B):3–51, 2003. ISSN 0025-5610. DOI: 10.1007/s10107-002-0339-5. ISMP 2000,
Part 3 (Atlanta, GA).

[2] Jonathan Allcock and Chang-Yu Hsieh. A quantum extension of SVM-perf for training
nonlinear SVMs in almost linear time. Quantum, 4:342, October 2020. ISSN 2521-
327X. DOI: 10.22331/q-2020-10-15-342.

[3] Tomasz Arodz and Seyran Saeedi. Quantum sparse support vector machines, 2019.

URL https://arxiv.org/abs/1902.01879.

[4] Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update
method: a meta-algorithm and applications. Theory Comput., 8:121–164, 2012. DOI:
10.4086/toc.2012.v008a006.

[5] Aharon Ben-Tal and Arkadi Nemirovski. Lectures on modern convex optimization.
MPS/SIAM Series on Optimization. Society for Industrial and Applied Mathematics
(SIAM), Philadelphia, PA; Mathematical Programming Society (MPS), Philadelphia,
PA, 2001. ISBN 0-89871-491-5. DOI: 10.1137/1.9780898718829. Analysis, algorithms,
and engineering applications.

[6] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge University
Press, Cambridge, 2004. ISBN 0-521-83378-7. DOI: 10.1017/CBO9780511804441.
[7] Fernando G. S. L. Brandão, Amir Kalev, Tongyang Li, Cedric Yen-Yu Lin, Krysta M.
large speed-ups, optimality, and
In 46th International Colloquium on Automata,

Svore, and Xiaodi Wu. Quantum SDP solvers:
applications to quantum learning.

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

21

Languages, and Programming, volume 132 of LIPIcs. Leibniz Int. Proc. Inform.,
pages Art. No. 27, 14. Schloss Dagstuhl. Leibniz-Zent. Inform., Wadern, 2019. DOI:
10.4230/LIPICS.ICALP.2019.27.

[8] Fernando G. S L. Brandão, Richard Kueng, and Daniel Stilck França. Faster quantum
and classical sdp approximations for quadratic binary optimization, 2020. URL https:
//arxiv.org/abs/1909.04613.

[9] Fernando G.S.L. Brandão and Krysta M. Svore. Quantum speed-ups for solving
In 58th Annual IEEE Symposium on Foundations of Com-
semideﬁnite programs.
puter Science—FOCS 2017, pages 415–426. IEEE Computer Soc., Los Alamitos, CA,
2017. DOI: 10.1109/focs.2017.45.

[11] Shantanav Chakraborty, András Gilyén, and Stacey Jeﬀery.

[10] Sébastien Bubeck. Convex optimization: Algorithms and complexity. Foundations
and Trends® in Machine Learning, 8(3-4):231–357, 2015. DOI: 10.1561/2200000050.
The power of
improved regression techniques via faster Hamil-
block-encoded matrix powers:
In 46th International Colloquium on Automata, Languages,
tonian simulation.
and Programming, volume 132 of LIPIcs. Leibniz Int. Proc.
Inform., pages
Art. No. 33, 14. Schloss Dagstuhl. Leibniz-Zent. Inform., Wadern, 2019. DOI:
10.4230/LIPICS.ICALP.2019.33.

[12] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines.
ACM Transactions on Intelligent Systems and Technology, 2(3):1–27, April 2011. DOI:
10.1145/1961189.1961199.

[13] Michael B. Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the
In STOC’19—Proceedings of the 51st Annual
current matrix multiplication time.
ACM SIGACT Symposium on Theory of Computing, pages 938–942. ACM, New York,
2019. DOI: 10.1145/3313276.3316303.

[14] Hilary Dollar.

Iterative linear algebra for constrained optimization. PhD thesis,
University of Oxford, 2005. URL https://www.numerical.rl.ac.uk/people/hsd/
thesismain.pdf.

[15] Alexander Domahidi, Eric Chu, and Stephen Boyd. ECOS: An SOCP solver for
embedded systems. In 2013 European Control Conference (ECC). IEEE, July 2013.
DOI: 10.23919/ecc.2013.6669541.

[16] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
LIBLINEAR: A library for large linear classiﬁcation. Journal of machine learning
research, 9(Aug):1871–1874, 2008. URL https://www.jmlr.org/papers/volume9/
fan08a/fan08a.

[17] András Gilyén, Srinivasan Arunachalam, and Nathan Wiebe. Optimizing quantum
optimization algorithms via faster quantum gradient computation. In Proceedings of
the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1425–
1444. SIAM, Philadelphia, PA, 2019. DOI: 10.1137/1.9781611975482.87.

[18] András Gilyén, Yuan Su, Guang Hao Low, and Nathan Wiebe. Quantum singu-
lar value transformation and beyond: exponential improvements for quantum ma-
trix arithmetics. In STOC’19—Proceedings of the 51st Annual ACM SIGACT Sym-
posium on Theory of Computing, pages 193–204. ACM, New York, 2019. DOI:
10.1145/3313276.3316366.

[19] András Gilyén, Seth Lloyd, and Ewin Tang. Quantum-inspired low-rank stochastic
regression with logarithmic dependence on the dimension, 2018. URL https://arxiv.
org/abs/1811.04909.

[20] Lov K. Grover. A fast quantum mechanical algorithm for database search.

In
Proceedings of the Twenty-eighth Annual ACM Symposium on the Theory of Com-

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

22

puting (Philadelphia, PA, 1996), pages 212–219. ACM, New York, 1996. DOI:
10.1145/237814.237866.

[21] Aram W. Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum algorithm for lin-
ear systems of equations. Physical Review Letters, 103(15), October 2009. DOI:
10.1103/physrevlett.103.150502.

[22] Thorsten Joachims. Training linear SVMs in linear time. In Proceedings of the 12th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
KDD ’06, page 217–226, New York, NY, USA, 2006. Association for Computing Ma-
chinery. ISBN 1595933395. DOI: 10.1145/1150402.1150429.

[23] N. Karmarkar. A new polynomial-time algorithm for linear programming. In Proceed-
ings of the sixteenth annual ACM symposium on Theory of computing - STOC '84,
pages 302–311. ACM Press, 1984. DOI: 10.1145/800057.808695.

[24] Iordanis Kerenidis and Anupam Prakash. Quantum recommendation systems.
In Christos H. Papadimitriou, editor, 8th Innovations in Theoretical Computer
Science Conference (ITCS 2017), volume 67 of Leibniz International Proceed-
ings in Informatics (LIPIcs), pages 49:1–49:21, Dagstuhl, Germany, 2017. Schloss
Dagstuhl–Leibniz-Zentrum fuer
DOI:
10.4230/LIPIcs.ITCS.2017.49.

ISBN 978-3-95977-029-3.

Informatik.

[25] Iordanis Kerenidis and Anupam Prakash. Quantum gradient descent for linear sys-
tems and least squares. Phys. Rev. A, 101:022316, Feb 2020. DOI: 10.1103/Phys-
RevA.101.022316.

[26] Iordanis Kerenidis and Anupam Prakash. A quantum interior point method for LPs
and SDPs. ACM Transactions on Quantum Computing, 1(1), October 2020. ISSN
2643-6809. DOI: 10.1145/3406306.

[27] Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in
the current matrix multiplication time. In Alina Beygelzimer and Daniel Hsu, editors,
Proceedings of the Thirty-Second Conference on Learning Theory, volume 99 of Pro-
ceedings of Machine Learning Research, pages 2140–2157, Phoenix, USA, 25–28 Jun
2019. PMLR. URL http://proceedings.mlr.press/v99/lee19a.html.

[28] Renato D. C. Monteiro and Takashi Tsuchiya. Polynomial convergence of primal-
dual algorithms for the second-order cone program based on the MZ-family of di-
rections. Math. Program., 88(1, Ser. A):61–83, 2000.
ISSN 0025-5610. DOI:
10.1007/PL00011378.

[29] Yu. E. Nesterov and M. J. Todd. Self-scaled barriers and interior-point methods for
convex programming. Math. Oper. Res., 22(1):1–42, 1997. ISSN 0364-765X. DOI:
10.1287/moor.22.1.1.

[30] Yu. E. Nesterov and M. J. Todd. Primal-dual

scaled cones.
10.1137/S1052623495290209.

SIAM J. Optim., 8(2):324–364, 1998.

interior-point methods for self-
ISSN 1052-6234. DOI:

[31] John Neter, Michael H Kutner, Christopher J Nachtsheim, and William Wasserman.

Applied linear statistical models, volume 4. Irwin Chicago, 1996.

[32] Michael A. Nielsen and Isaac L. Chuang. Quantum Computation and Quantum Infor-

mation. Cambridge University Press, 2009. DOI: 10.1017/cbo9780511976667.

[33] Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. Quantum support vector ma-
chine for big data classiﬁcation. Physical Review Letters, 113(13), September 2014.
DOI: 10.1103/physrevlett.113.130503.

[34] Yousef Saad. Iterative methods for sparse linear systems. Society for Industrial and
Applied Mathematics, Philadelphia, PA, second edition, 2003. ISBN 0-89871-534-2.
DOI: 10.1137/1.9780898718003.

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

23

[35] Peter W. Shor. Algorithms for quantum computation: discrete logarithms and fac-
toring. In 35th Annual Symposium on Foundations of Computer Science (Santa Fe,
NM, 1994), pages 124–134. IEEE Comput. Soc. Press, Los Alamitos, CA, 1994. DOI:
10.1109/SFCS.1994.365700.

[36] Volker Strassen. Gaussian elimination is not optimal. Numerische Mathematik, 13(4):

354–356, August 1969. DOI: 10.1007/bf02165411.

[37] J.A.K. Suykens and J. Vandewalle. Least squares support vector machine classiﬁers.
Neural Processing Letters, 9(3):293–300, 1999. DOI: 10.1023/a:1018628609742.
[38] J.A.K. Suykens, J. De Brabanter, L. Lukas, and J. Vandewalle. Weighted least squares
support vector machines: robustness and sparse approximation. Neurocomputing, 48
(1-4):85–105, October 2002. DOI: 10.1016/s0925-2312(01)00644-0.

[39] Daniel Szilagyi, Iordanis Kerenidis, and Anupam Prakash. Quantum SVM via SOCP

experiment logs. Mar 2021. DOI: 10.6084/m9.ﬁgshare.11778189.v1.

[40] Joran van Apeldoorn and András Gilyén.

Improvements in quantum SDP-
In 46th International Colloquium on Automata, Lan-
solving with applications.
guages, and Programming, volume 132 of LIPIcs. Leibniz Int. Proc. Inform., pages
Art. No. 99, 15. Schloss Dagstuhl. Leibniz-Zent. Inform., Wadern, 2019. DOI:
10.4230/LIPICS.ICALP.2019.99.

[41] Joran van Apeldoorn, András Gilyén, Sander Gribling, and Ronald de Wolf. Quan-
tum SDP-solvers: Better upper and lower bounds. In 2017 IEEE 58th Annual Sym-
posium on Foundations of Computer Science (FOCS). IEEE, October 2017. DOI:
10.1109/focs.2017.44.

[42] Yinyu Ye, Michael J. Todd, and Shinji Mizuno. An O(

nL)-iteration homogeneous
and self-dual linear programming algorithm. Math. Oper. Res., 19(1):53–67, 1994.
ISSN 0364-765X. DOI: 10.1287/moor.19.1.53.

√

Accepted in Quantum 2021-04-05, click title to verify. Published under CC-BY 4.0.

24

