0
2
0
2

t
c
O
0
1

]

G
L
.
s
c
[

1
v
0
9
8
4
0
.
0
1
0
2
:
v
i
X
r
a

Rare-Event Simulation for Neural Network and Random Forest Predictors

YUANLU BAI, Columbia University, USA
ZHIYUAN HUANG, Carnegie Mellon University, USA
HENRY LAM, Columbia University, USA
DING ZHAO, Carnegie Mellon University, USA

We study rare-event simulation for a class of problems where the target hitting sets of interest are defined via modern machine learning

tools such as neural networks and random forests. This problem is motivated from fast emerging studies on the safety evaluation of

intelligent systems, robustness quantification of learning models, and other potential applications to large-scale simulation in which

machine learning tools can be used to approximate complex rare-event set boundaries. We investigate an importance sampling scheme

that integrates the dominating point machinery in large deviations and sequential mixed integer programming to locate the underlying

dominating points. Our approach works for a range of neural network architectures including fully connected layers, rectified linear

units, normalization, pooling and convolutional layers, and random forests built from standard decision trees. We provide efficiency

guarantees and numerical demonstration of our approach using a classification model in the UCI Machine Learning Repository.

Additional Key Words and Phrases: variance reduction, importance sampling, safety evaluation, neural network, random forest, large

deviations

1 INTRODUCTION

Due to the extensive development of artificial intelligence (AI), machine learning techniques have been embedded in

many safety-sensitive physical systems, including autonomous vehicles [68] and unmanned aircraft [66]. In autonomous

vehicles, for instance, machine learning predictors can be applied to many tasks including perception [28, 99], path

planning [42, 105], motion control [94], or end-to-end driving systems [29, 64, 75]. In these tasks, misprediction can

cause catastrophic impacts on public safety, as exemplified by the series of fatal accidents encountered by autonomous

driving systems due to the failures in detecting nearby vehicles or pedestrians (e.g. [18, 19]). To reduce the risk of such

catastrophe, machine learning models in these systems need to be carefully evaluated against safety, especially before

their mass deployment in public.

Recent research considers using probabilistic measures to quantify the risks of machine learning predictors or entire
intelligent physical systems. These measures can be defined in a variety of ways. In robustness evaluation, a prediction
model, with neural network as a dominant example, is considered more robust if it is more likely to make a consistent

prediction under small perturbations on the input [52]. When the perturbation is modeled via a random distribution,

the robustness of neural networks is measured by the probability that the prediction value persists [101â€“103]. In more

Authorsâ€™ addresses: Yuanlu Bai, yb2436@columbia.edu, Columbia University, 500 W. 120th Street, New York, USA; Zhiyuan Huang, zhuang2@andrew.
cmu.edu, Carnegie Mellon University, USA; Henry Lam, henry.lam@columbia.edu, Columbia University, 500 W. 120th Street, New York, USA; Ding Zhao,
dingzhao@cmu.edu, Carnegie Mellon University, USA.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

Â© 2020 Association for Computing Machinery.
Manuscript submitted to ACM

Manuscript submitted to ACM

1

 
 
 
 
 
 
2

Bai, Huang, Lam & Zhao

complex intelligent system evaluation, risks can be quantified by the occurrence probabilities of safety-critical events.
These events can be defined as the violation in terms of certain safety metrics (e.g., [41] listed seven potential safety

metrics for autonomous vehicles including crashes per driving hour and disengagements per scenario), and recent studies

use the probabilities of crash or injury in driving tasks as safety metrics [58, 79, 106]. For AI-equipped autonomous

vehicles, the evaluation target would implicitly involve a probabilistic measurement on the embedded machine learning

model. Moreover, in [104], neural networks are further used to approximate sophisticated safety-critical sets defined

from complex system dynamics, and the target probabilities comprise hitting sets defined via these neural network

outputs.

Our study is motivated from the estimation of probabilistic risk measures described above. Due to the complexity

of machine learning predictors, these probabilities are typically unamenable to analytical formulas, even when the

underlying stochastic distribution is fully modeled. This thus calls for the use of Monte Carlo simulation. However,

the target probabilities, which signify the risks of dangerous yet unlikely events, are tiny. The problem thus falls into

the domain of rare-event simulation, in which it is widely known that crude Monte Carlo can be extremely inefficient

and variance reduction is necessarily employed. Traditionally, rare-event simulation techniques (e.g. [25, 63]) have

been applied in broad application areas including queueing systems [11, 12, 15, 38, 69, 84, 89, 95], communication

networks [27, 65, 82], finance [43, 47, 48], insurance [3, 6, 31], reliability [56, 77, 78, 85, 97], biological processes [54, 91],

dynamical systems [39, 100], and combinatorics [9, 10]. The evaluation of machine learning models and intelligent

physical systems that we focus on here is a new application that is propelled rapidly by the growth of AI. Our goal is to

provide a first step into building rare-event simulation algorithms in these applications, which integrate tools from

both the disciplines of machine learning and rare-event simulation, and which are statistically guaranteed in terms of

the classical efficiency notions in the rare-event literature.

More specifically, we study importance sampling (IS) [93] to design efficient estimators. In rare-event estimation,

the rarity nature of hitting set dictates that crude Monte Carlo samples have a low frequency of observing the hitting

occurrence, and this inefficiency exhibits statistically as a large relative error (i.e., ratio of standard deviation to mean)

in the estimation. To mitigate this issue, IS uses an alternate distribution to generate samples that can attain a higher

frequency in hitting the target event, and reweights the outputs to maintain unbiasedness via the likelihood ratios. To

achieve a small relative error, the new generating distribution (i.e., the IS distribution) needs to be carefully selected,

often by analyzing the weights in interaction with the hitting set geometry and the underlying system dynamics [50, 90].

It is known that such analyses are important as ill-designed schemes can lead to significantly biased estimates [49].

In this paper, we follow the above analysis path in the literature and use the common theoretical notion of efficiency

called asymptotic optimality or logarithmic efficiency [7, 56, 63] that we will detail in the sequel.

In terms of our scope of study, we focus on piecewise linear machine learning predictors, which include random

forests and neural networks with common activation functions such as rectified linear units (ReLU). We also assume the

underlying distribution is Gaussian or mixtures of such. Under this setting, we design provably efficient IS schemes to

estimate rare-event probabilities that the prediction outputs hit above certain high thresholds. Our main methodology

integrates the classical notion of dominating points [35, 90] for rare-event sets with sequential mixed integer program-

ming (MIP) to attain an efficient estimator. Intuitively, a dominating point is the highest-density point in the rare-event

set, so that using an IS distribution that shifts the mean to this point (via exponential tilting) gives rise to a distribution

that hits the rare-event set more frequently and the generated likelihood ratio contributes properly to the probability of

interest, which are desirable for controlling the relative error. However, this is only a local characterization. To explain,

the simulation randomness stipulates that some generated samples may have huge likelihood ratios. Controlling these

Manuscript submitted to ACM

Rare-Event Simulation for Neural Network and Random Forest Predictors

3

ratios in turn requires a geometric property that, in the Gaussian case, implies the dominating point to be on the

boundary of the rare-event set, and that the latter lies completely inside one of the half-spaces cut by the tangential

hyperplane passing through the dominating point (e.g., these occur when the rare-event set is convex). When this

geometric property does not hold, then one needs to divide the rare-event set into union of smaller sets each bearing

its own dominating point, and an efficient IS scheme is built via a mixture of exponential tiltings targeted at all these

individual dominating points [90]. The sequential MIP in our procedure serves to locate all these dominating points. It

casts each search as a density maximization problem constrained by hitting sets induced from the considered machine

learning model. The involved feasible regions shrink sequentially as we add more â€œcutting planes" to the constraints in

order to remove the half-spaces that are already considered by earlier dominating points. Our MIPs are derived from

the reformulation techniques that appeared recently in the machine learning literature, which leverage the geometric

structures of ReLU neural networks [96] and random forests [74]. We provide a step-by-step guide in formulating

random forests and different neural network architectures as suitable MIPs to be inserted into our sequential algorithm.

We also provide theoretical results on asymptotic optimality that targets at general piecewise polyhedrals where applies

to our considered rare-event sets. Towards this, we also derive large deviations results for the associated probabilities of

interest.

The paper is organized as follows. Section 2 first provides a brief literature review. Section 3 describes our problem

setting and notations. Section 4 presents our algorithm and theoretical guarantees. Section 5 provides the MIP formu-

lations for random forests and different neural network architectures. Section 6 shows numerical results. Section 7

contains the proofs of theorems.

2 RELATED WORK

A significant line of work studies the use of large deviations to invent efficient IS procedures, which mathematically

identifies the most likely path to trigger a rare event through minimizing the so-called rate function (see, e.g., the

surveys [7, 14, 25, 45, 63, 88]). Among these studies, our approach extends the IS schemes using dominating points

[35, 90]. Similar idea of using half-spaces to split rare-event set is also considered in [2, 80] where the rare-event set is

constrained to be a union of half-spaces and the half-spaces are explicitly given in the setting. The use of sequential

MIP algorithm on an implicitly half-space separable rare-event set distinguishes our work from these studies. To prove

the efficiency of our algorithm, we need to derive the asymptotic result for the rare-event probability of interest. [55]

provides a variety of useful techniques to represent the asymptotic approximation of probability using dominating

points, but some technical modifications need to be made to fit in our settings. Similar to our derivations, [57] represents

the asymptotic of probability on convex sets using dominating points, yet focuses on a different scaling setting from

ours.

Other IS schemes include the cross-entropy method [21, 32, 83, 86, 87] that uses sequential stochastic optimization

to search for an optimal IS distribution in a parametric family. Adaptive IS [1, 22, 34, 67] updates the IS distribution

iteratively between simulated replications to approach the optimal (zero-variance) IS distribution and generates non

i.i.d samples for estimating the target expectation associated with finite-state discrete Markov chains. Another line of

studies use techniques such as Markov-chain Monte Carlo (MCMC) to sample from the rare-event set of interest, or

approximately from the conditional distribution given the occurrence of the rare event [23, 24, 26, 53]. IS schemes have

also been designed for heavy-tailed systems [13, 16, 17, 27, 37, 61, 76], in contrast to the light-tailed settings considered

in this paper. Besides IS, other competing methods for rare-event simulation include conditional Monte Carlo [4, 5] and

splitting [33, 44, 46, 72, 81].

Manuscript submitted to ACM

4

Bai, Huang, Lam & Zhao

In machine learning literature, some studies discuss using probability measure to evaluate the robustness of prediction

models. Since the measure can be extremely small, rare-event simulation techniques are considered in these studies.

[102] discusses an adaptive multilevel splitting approach to estimate the statistical robustness of machine learning

models. [98] proposes to learn a failure probability predictor to approximate the minimum variance IS distribution

in estimating agent failure probabilities. [103] proposes an approach to compute the lower and upper bounds for a

probabilistic robustness measure. The topic of these works is one of our key motivations, and our work can be viewed

as a step towards the provision of rigorous guarantees for methodologies driven by these applications.

Lastly, another related line of research studies optimization problems with machine learning models in the objective.

[74] discusses the optimization of tree ensemble models and provides treatment for large scale problems. [96] formulates

the robustness verification of neural networks as MIP problems. These studies leverage the piecewise linear property of

these machine learning models to turn optimization on the prediction output into tractable MIPs. Our MIP formulations

for finding dominating points follow from these optimization studies.

3 PROBLEM SETTING

3.1 Rare-Event Probability Estimation
We state our problem setting. Consider a prediction model ğ‘”(Â·), with input ğ‘‹ âˆˆ Rğ‘‘ and output ğ‘”(ğ‘‹ ) âˆˆ R. Suppose
that the input follows a Gaussian distribution, i.e, ğ‘‹ âˆ¼ ğ‘ (ğœ‡, Î£), where Î£ is a ğ‘‘ Ã— ğ‘‘ positive definite matrix. We want
to estimate the probability ğ‘ = ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾), where ğ›¾ âˆˆ R is a threshold that triggers a rare event. We note that the
Gaussian assumption can be relaxed without much difficulty in our framework to, for instance, mixtures of Gaussians,

which we will discuss later and can expand our scope of applicability.

This problem setting is related to risk assessments involving machine learning models, as exemplified below.

Example 3.1 (Statistical Robustness Metric [102, 103]). Consider a classification model that predicts using â€œscore
functionsâ€ ğ‘”ğ‘– (Â·) with ğ‘– = 1, .., ğ¾ where ğ¾ denotes the number of categories. The predicted output is the category that has
the maximum score, i.e. the prediction at ğ‘‹ is given by arg maxğ‘– ğ‘”ğ‘– (ğ‘‹ ). Suppose an example input ğ‘¥0 belongs to category
ğ‘. A classification model is robust if it gives correct prediction for all ğ‘¥ such that ğ‘‘ (ğ‘¥, ğ‘¥0) â‰¤ ğœ– where ğ‘‘ denotes a certain
distance and ğœ– > 0 is a small real number. A statistical robustness metric considers ğ‘ = ğ‘ƒ (maxğ‘– ğ‘”ğ‘– (ğ‘‹ ) âˆ’ ğ‘”ğ‘ (ğ‘‹ ) â‰¥ 0),
where ğ‘‹ follows a distribution concentrated around ğ‘¥0. Here ğ‘ represents the probability that the output is inconsistent
with the baseline prediction at ğ‘¥0.

Example 3.2 (Risk Evaluation of Intelligent Physical Systems [36]). Consider an intelligent physical system that embeds
a machine learning predictor ğ‘”, so that the decision of the system given an input ğ‘‹ can be expressed as â„(ğ‘”(ğ‘‹ )). The
probability ğ‘ƒ (â„(ğ‘”(ğ‘‹ )) âˆˆ ğ‘†), where ğ‘† represents a risky region, can be used to measure the risk of the system decision.
In most cases, â„ is random by itself and can have a different complexity structure than the function class ğ‘”. In this paper,
we consider a rare-event probability in the form of ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾) as a first step of study in this direction.

Example 3.3 (Probability Evaluation for Learned Rare-Event Set [104]). When the rare-event set is very complicated
(e.g., in autonomous driving contexts), one approach to retain tractability is to approximate or learn the rare-event
set via classification tools. Given historical or simulated data {ğ‘‹, ğ‘Œ }, where ğ‘Œ âˆˆ {0, 1} denotes whether a rare-event
(e.g. a crash) occurs to the system of interest under input ğ‘‹ , we train a neural network ğ‘”(Â·) to classify the rare-event
region given ğ‘‹ . The learned rare-event set is represented by {ğ‘¥ : ğ‘”(ğ‘¥) â‰¥ ğ›¾ }, where ğ›¾ is the threshold for classifying
Manuscript submitted to ACM

Rare-Event Simulation for Neural Network and Random Forest Predictors

5

rare-event (e.g. ğ›¾ = 0.5). Since {ğ‘¥ : ğ‘”(ğ‘¥) â‰¥ ğ›¾ } is an approximation of the true rare-event set, ğ‘ = ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾) provides
an approximation on the probability of the rare event.

It is known that neural networks can be vulnerable to adversarial attacks, in that a tiny perturbation in the input

can exert a large effect on the prediction output [52], and such a perturbed input is considered as an adversarial

example. Studies have discussed how to find these adversarial examples [70] and to conduct adversarial learning [71].

Among them, Example 3.1 is an example of a probabilistic measure on how likely adversarial examples appear around a

certain input. Examples 3.2 and 3.3, on the other hand, represent endeavors to tackle safety-critical problems driven by

applications involving AI systems, which can embed machine learning models or are approximated by them.

3.2 Importance Sampling

When ğ‘ is small, estimation using crude Monte Carlo is challenging since, intuitively, the samples have a low frequency

of hitting the target set. This is statistically manifested as a large relative error. To be more specific, suppose that we
use the crude Monte Carlo estimator Ë†ğ‘ğ‘ = 1
ğ‘–=1 ğ¼ (ğ‘”(ğ‘‹ğ‘– ) â‰¥ ğ›¾) to estimate ğ‘. Since the probability ğ‘ is tiny, the error
ğ‘
of the estimator should be measured relative to the size of ğ‘. In other words, we would like the probability of having a
large relative error to be small, i.e., ğ‘ƒ (| Ë†ğ‘ğ‘ âˆ’ ğ‘ | > ğœ€ğ‘) â‰¤ ğ›¿ where ğ›¿ is the confidence level and 0 < ğœ€ < 1. By Markovâ€™s
inequality, a sufficient condition for this is

(cid:205)ğ‘

ğ‘‰ ğ‘ğ‘Ÿ (ğ¼ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾))
ğ›¿ğœ€2ğ¸ [ğ¼ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾)]2 =
where ğ‘…ğ¸ = âˆšï¸ğ‘‰ ğ‘ğ‘Ÿ (ğ¼ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾))/ğ¸ [ğ¼ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾)] is the relative error. For the crude Monte Carlo estimator, the RE is
given by âˆšï¸(1 âˆ’ ğ‘)/ğ‘. That is, the simulation size ğ‘ has to be roughly proportional to 1/ğ‘ in order to achieve a given
relative error. Under the settings that ğ‘‹ has a Gaussian distribution and ğ‘” is piecewise linear (see Corollary 4.3), ğ‘ is

ğ‘ â‰¥

.

ğ‘…ğ¸2
ğ›¿ğœ–2

exponentially small in the threshold level ğ›¾, and hence the required simulation size would grow exponentially in ğ›¾.

A common approach to speed up simulation in such contexts is to use IS (see, e.g. the surveys [7, 14, 25, 45, 63, 88],
among others). Suppose ğ‘‹ has a density ğ‘“ . The basic idea of IS is to change the sampling distribution to say Ëœğ‘“ , and
output

ğ‘ = ğ¼ (ğ‘”( Ëœğ‘‹ ) â‰¥ ğ›¾)

ğ‘“ ( Ëœğ‘‹ )
Ëœğ‘“ ( Ëœğ‘‹ )

,

(1)

where Ëœğ‘‹ is sampled from Ëœğ‘“ . This output is unbiased if ğ‘“ is absolutely continuous with respect to Ëœğ‘“ over the rare-event
set {ğ‘¥ : ğ‘”(ğ‘¥) â‰¥ ğ›¾ }. By choosing Ëœğ‘“ appropriately, one can substantially reduce the simulation variance.

To measure the efficiency of an IS scheme, we introduce a rarity parameter, say ğ›¾, that parametrizes the rare-event
probability ğ‘ğ›¾ such that ğ‘ğ›¾ â†’ 0 as ğ›¾ â†’ âˆ. As discussed before, since the probability of interest is small, one should
focus on the relative error of the Monte Carlo estimator with respect to the magnitude of this probability. To this end,
we call an IS estimator ğ‘ğ›¾ for ğ‘ğ›¾ asymptotically optimal [7, 63] if

log Ëœğ¸ [ğ‘ 2
ğ›¾ ]
log Ëœğ¸ [ğ‘ğ›¾ ]

lim
ğ›¾â†’âˆ

= 2,

(2)

where Ëœğ¸ denotes the expectation with regard to Ëœğ‘“ . The notion (2) is equivalent to saying that Ëœğ¸ [ğ‘ 2
ğ›¾ ]/ Ëœğ¸ [ğ‘ğ›¾ ]2 is at most
polynomially growing in ğ›¾. This ensures that the second moment, or the variance, does not explode exponentially

relative to the probability of interest as ğ›¾ increases, thus preventing an exponentially large number of simulation

Manuscript submitted to ACM

6

Bai, Huang, Lam & Zhao

replications to achieve a given relative accuracy. We will use asymptotic optimality as our efficiency criterion in this

paper.

Another commonly used efficiency criterion is the bounded relative error, which is defined as

lim sup
ğ›¾â†’âˆ

Ëœğ¸ [ğ‘ 2
ğ›¾ ]
Ëœğ¸ [ğ‘ğ›¾ ]2

< âˆ.

This is a stronger condition than asymptotic optimality. More efficiency criteria can be found in [62, 73].

4 EFFICIENT IMPORTANCE SAMPLING VIA SEQUENTIAL MIXED INTEGER PROGRAMMING
In the case of Gaussian input distributions, finding a good Ëœğ‘“ is particularly handy and one approach to devise good
IS distributions uses the notion of so-called dominating point. As explained in the introduction, a dominating point

can be understood as the highest-density point in the rare-event set that satisfies some conditions. More precisely, the

collection of dominating points for a rare-event set with Gaussian distributed input is defined in Definition 4.1.

Definition 4.1. Suppose that a set ğ´ âŠ‚ Rğ‘‘ satisfies that ğ‘† âŠ‚ (cid:208)ğ‘ âˆˆğ´{ğ‘¥ : (ğ‘ âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğ‘) â‰¥ 0} and that ğ‘ =
arg minğ‘¥ {(ğ‘¥ âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğœ‡) : ğ‘¥ âˆˆ ğ‘† and (ğ‘ âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğ‘) â‰¥ 0} for any ğ‘ âˆˆ ğ´. Moreover, suppose that the above
conditions do not hold anymore if we remove any element from ğ´. Then the points in ğ´ are called the dominating
points of ğ‘† with input distribution ğ‘ (ğœ‡, Î£).

Note that minimizing (ğ‘¥ âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğœ‡) is equivalent to maximizing ğœ™ (ğ‘¥; ğœ‡, Î£), the Gaussian density with mean ğœ‡
and covariance Î£. The condition 2(ğ‘ âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğ‘) â‰¥ 0 is the first-order condition of optimality for the optimization
minğ‘¥ (ğ‘¥ âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğœ‡) over a convex set for ğ‘¥. Thus, intuitively, each dominating point in the collection ğ´ can be
viewed as the highest-density point in a â€œlocal" region formed by ğ‘† âˆ© {ğ‘¥ : (ğ‘ âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğ‘) â‰¥ 0}. In particular, if
{ğ‘¥ : ğ‘”(ğ‘¥) â‰¥ ğ›¾ } is a convex set, then there is only one dominating point ğ‘. In this case, a well-known IS scheme is to use
a Gaussian distribution ğ‘ (ğ‘, Î£) as the IS distribution Ëœğ‘“ .

We explain intuitively why we need more than one dominating point (the highest-density point over ğ‘†) and the

pitfall if we omit the other ones in constructing efficient IS. Suppose that the rare-event set consists of two disconnected

convex components which are nearly equi-distant with respect to the origin, and we choose the IS distribution to be

centered at the dominating point of one component. Then, if a sample from the IS distribution hits the other component,

a scenario that could be unlikely but possible, the resulting likelihood ratio, which now contributes to the output as the

rare-event set is hit, could possibly be tremendous. This ultimately leads to an explosion of the relative error in the

IS estimator. [49] presents more counterexamples which show that it is essential to find all the dominating points in

constructing an efficient IS based on mixtures.

In view of the aforementioned discussions, we consider the following IS scheme. If we can split {ğ‘¥ : ğ‘”(ğ‘¥) â‰¥ ğ›¾ } into
R1, ..., Rğ‘Ÿ , and for each Rğ‘–, ğ‘– = 1, ..., ğ‘Ÿ there exists a dominating point ğ‘ğ‘– such that ğ‘ğ‘– = arg minğ‘¥ {(ğ‘¥ âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğœ‡) :
ğ‘¥ âˆˆ Rğ‘– } and Rğ‘– âŠ† {ğ‘¥ : (ğ‘ğ‘– âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğ‘ğ‘– ) â‰¥ 0}, then we use a Gaussian mixture distribution with ğ‘Ÿ components as
the IS distribution Ëœğ‘“ , where the ğ‘–th component has mean ğ‘ğ‘– . This proposal guarantees the asymptotic optimality of the
IS (see Theorem 4.2).

In our task, because the machine learning predictor ğ‘”(ğ‘¥) is nonlinear and ğ‘¥ is high-dimensional in general, splitting
{ğ‘¥ : ğ‘”(ğ‘¥) â‰¥ ğ›¾ } into R1, ..., Rğ‘Ÿ that have dominating points is challenging even with known parameters. This challenge
motivates us to use Algorithm 1 to obtain the dominating points ğ‘1, ..., ğ‘ğ‘Ÿ that constructs an efficient IS distribution.
The procedure uses a sequential â€œcutting plane" approach to exhaustively look for all dominating points, by reducing

Manuscript submitted to ACM

Rare-Event Simulation for Neural Network and Random Forest Predictors

7

the search space at each iteration via taking away the regions covered by found dominating points. The set ğ´ in the

procedure serves to store the dominating points we have located throughout the procedure. At the end of the procedure,
we obtain a set ğ´ that contains all the dominating points ğ‘1, ..., ğ‘ğ‘Ÿ .

Algorithm 1: Procedure to find all dominating points for the set {ğ‘¥ : ğ‘”(ğ‘¥) â‰¥ ğ›¾ }.
Input: Prediction model ğ‘”(ğ‘¥), threshold ğ›¾, input distribution ğ‘ (ğœ‡, Î£).
Output: dominating point set ğ´.

1 Start with ğ´ = âˆ…;
2 While {ğ‘¥ : ğ‘”(ğ‘¥) â‰¥ ğ›¾, (ğ‘ğ‘– âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğ‘ğ‘– ) < 0, âˆ€ğ‘ğ‘– âˆˆ ğ´} â‰  âˆ… do
Find a dominating point ğ‘ by solving the optimization problem

3

ğ‘ = arg min

(ğ‘¥ âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğœ‡)

ğ‘¥
ğ‘ .ğ‘¡ . ğ‘”(ğ‘¥) â‰¥ ğ›¾

(3)

and update ğ´ â† ğ´ âˆª {ğ‘};

4 End

(ğ‘ğ‘– âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğ‘ğ‘– ) < 0, âˆ€ğ‘ğ‘– âˆˆ ğ´

Algorithm 1 gives ğ´ = {ğ‘1, . . . , ğ‘ğ‘Ÿ }. With this, we split {ğ‘¥ : ğ‘”(ğ‘¥) â‰¥ ğ›¾ } into R1, . . . , Rğ‘Ÿ where Rğ‘– = {ğ‘¥ : ğ‘”(ğ‘¥) â‰¥
ğ›¾, (ğ‘ğ‘– âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğ‘ğ‘– ) â‰¥ 0, (ğ‘ ğ‘— âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğ‘ ğ‘— ) â‰¤ 0, âˆ€ğ‘— < ğ‘–}. Clearly ğ‘ğ‘– = arg min{(ğ‘¥ âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğœ‡) : ğ‘¥ âˆˆ Rğ‘– } and
(ğ‘1 âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘1 âˆ’ ğœ‡) â‰¤ Â· Â· Â· â‰¤ (ğ‘ğ‘Ÿ âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘ğ‘Ÿ âˆ’ ğœ‡). Moreover, we note that (ğ‘1 âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘1 âˆ’ ğœ‡) = minğ‘–=1,...,ğ‘Ÿ {(ğ‘ğ‘– âˆ’
ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘ğ‘– âˆ’ ğœ‡)}.

Given the dominating point set ğ´ we use

as the IS distribution. That is, the IS estimator is

1
ğ‘Ÿ

ğ‘ (ğ‘1, Î£) + Â· Â· Â· +

1
ğ‘Ÿ

ğ‘ (ğ‘ğ‘Ÿ , Î£)

ğ‘ = ğ¼ (ğ‘”( Ëœğ‘‹ ) â‰¥ ğ›¾)ğ¿( Ëœğ‘‹ )

(4)

where Ëœğ‘‹ âˆ¼ Ëœğ‘“ and ğ¿, the likelihood ratio, is defined as

ğ‘Ÿğ‘’âˆ’ 1
2 (ğ‘¥âˆ’ğ‘1)ğ‘‡ Î£âˆ’1 (ğ‘¥âˆ’ğ‘1) + Â· Â· Â· + ğ‘’âˆ’ 1
As a summary, after computing the dominating points ğ´ = {ğ‘1, . . . , ğ‘ğ‘Ÿ } using Algorithm 1, we estimate the probability

2 (ğ‘¥âˆ’ğ‘ğ‘Ÿ )ğ‘‡ Î£âˆ’1 (ğ‘¥âˆ’ğ‘ğ‘Ÿ )

2 (ğ‘¥âˆ’ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥âˆ’ğœ‡)

ğ‘“ (ğ‘¥)
Ëœğ‘“ (ğ‘¥)

ğ¿(ğ‘¥) =

ğ‘’âˆ’ 1

=

.

of interest via Algorithm 2.

Algorithm 2: Construct the IS estimator with all the dominating points.
Input: Prediction model ğ‘”(ğ‘¥), threshold ğ›¾, dominating points ğ´ = {ğ‘1, . . . , ğ‘ğ‘Ÿ }, simulation size ğ‘ .
Output: Estimated rare-event probability Ë†ğ‘.
ğ‘Ÿ ğ‘ (ğ‘1, Î£) + Â· Â· Â· + 1
ğ‘–=1 ğ¼ (ğ‘”( Ëœğ‘‹ğ‘– ) â‰¥ ğ›¾)ğ¿( Ëœğ‘‹ğ‘– ) where
(cid:205)ğ‘

1 Generate Ëœğ‘‹1, . . . , Ëœğ‘‹ğ‘ âˆ¼ 1
2 Compute Ë†ğ‘ = 1
ğ‘

ğ‘Ÿ ğ‘ (ğ‘ğ‘Ÿ , Î£);

ğ¿(ğ‘¥) =

3 End

ğ‘Ÿğ‘’âˆ’ 1
2 (ğ‘¥âˆ’ğ‘1)ğ‘‡ Î£âˆ’1 (ğ‘¥âˆ’ğ‘1) + Â· Â· Â· + ğ‘’âˆ’ 1

2 (ğ‘¥âˆ’ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥âˆ’ğœ‡)

ğ‘’âˆ’ 1

2 (ğ‘¥âˆ’ğ‘ğ‘Ÿ )ğ‘‡ Î£âˆ’1 (ğ‘¥âˆ’ğ‘ğ‘Ÿ )

;

Manuscript submitted to ACM

8

Bai, Huang, Lam & Zhao

The efficiency guarantee of the proposed IS estimator (4) is given by:

Theorem 4.2. Suppose that the input ğ‘‹ âˆ¼ ğ‘ (ğœ‡, Î£) and the prediction model ğ‘”(Â·) is a piecewise linear function such
is at most

that ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾) > 0 for any ğ›¾ âˆˆ R. The IS estimator ğ‘ is defined in (4). Then we have that Ëœğ¸ [ğ‘ 2]/ Ëœğ¸ [ğ‘ ]2
polynomially growing in ğ›¾. That is, ğ‘ is asymptotically optimal.

Theorem 4.2 is proved by constructing an upper bound for the relative error, which in turn depends on the asymptotic

approximation of probability on polytope sets using dominating points. Our proof leverages the results in [55] on the tail
exceedance asymptotic of ğ‘ƒ (ğ‘ (0, Î£ğ‘›) â‰¥ ğ‘¡ğ‘›) where âˆ¥ğ‘¡ğ‘› âˆ¥ â†’ âˆ as ğ‘› â†’ âˆ, but requires substantial generalization. Note
that Theorem 4.2 only makes the very general assumptions that ğ‘” is piecewise linear and the probability ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾) is
nondegenerate (i.e., non-zero) for any ğ›¾ âˆˆ R. Our result applies to, for example, the probability ğ‘ƒ (ğ´ğ‘‹ â‰¥ ğ‘¡) where ğ´ is a
constant matrix and ğ‘¡ âˆ’ ğ›¾ğ‘’1 is a constant vector (here, ğ‘’1 = (1, 0, . . . , 0)ğ‘‡ ). If ğ´ğ´ğ‘‡ is not invertible, then it is not easily
reducible to the setting studied in [55]. To achieve a general result, we carefully construct a superset and a subset of the

rare-event set to derive tight enough upper and lower bounds for the probability of interest, in which we analyze the

involved asymptotic integrals instead of using the conditional probability representation in [55] that is not directly

applicable in our setting. For the detailed proof, please refer to Section 7.

A by-product in deriving Theorem 4.2 is the large deviations probability asymptotic for ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾):

Corollary 4.3. Suppose that the input ğ‘‹ âˆ¼ ğ‘ (ğœ‡, Î£) and the prediction model ğ‘”(Â·) is a piecewise linear function such
that ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾) > 0 for any ğ›¾ âˆˆ R. Denote ğ‘ = arg min{(ğ‘¥ âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğœ‡) : ğ‘”(ğ‘¥) â‰¥ ğ›¾ }. Then âˆ’ log ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾) =
(1 + ğ‘œ (1))(ğ‘ âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘ âˆ’ ğœ‡)/2 as ğ›¾ â†’ âˆ. In particular, ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾) is exponentially small in ğ›¾.

The theoretical guarantee given by Theorem 4.2 justifies the sequential MIP algorithm for searching dominating

points. The resulting mixture IS distribution is asymptotically optimal. We point out some related works that use

mixture distributions that are related to our proposed method. In [2, 80], mixture IS distributions are constructed based

on separating rare-event set with half-spaces. However, in these works, the rare-event set is restricted to be a union of

half-spaces, and these half-spaces are assumed to be known. The use of Algorithm 1 allows us to deal with more general

rare-event sets. Moreover, in relation to Corollary 4.3, we also mention the work [57] that derives an asymptotic result

for Gaussian probabilities using dominating points. However, they focus on convex hitting sets where the entire set is

scaled with a rarity parameter, which is different from our settings. First, our rare-event set is not necessarily convex.

Second, even if we separate our rare-event set into the union of convex sets, their results still cannot be applied, since

in our settings some linear constraints are allowed to be fixed instead of scaling with ğ›¾.

The proposed IS scheme can be extended to problems with Gaussian mixture inputs. Suppose the Gaussian mixture
has ğ‘š components, so that ğ‘‹ âˆ¼ (cid:205)ğ‘š
ğ‘—=1 ğœ‹ ğ‘— ğ‘ (ğœ‡ ğ‘— , Î£ ğ‘— ). For each component ğ‘—, we implement Algorithm 1 with input
distribution ğ‘ (ğœ‡ ğ‘— , Î£ ğ‘— ) to obtain dominating point set ğ´ğ‘— (with cardinality ğ‘Ÿ ğ‘— ). The proposed IS distribution is given by
Ëœğ‘“ (ğ‘¥) = (cid:205)ğ‘š
ğ‘—=1

ğ‘–=1 1/ğ‘Ÿ ğ‘— ğœ‹ ğ‘— ğ‘ (ğ‘ ğ‘—ğ‘–, Î£ ğ‘— ). We summarize the procedure as Algorithm 3.

(cid:205)ğ‘Ÿ ğ‘—

Similar to Algorithm 2, we have the efficiency guarantee for Algorithm 3:

Corollary 4.4. Suppose that the input ğ‘‹ âˆ¼ (cid:205)ğ‘š

ğ‘—=1 ğœ‹ ğ‘— ğ‘ (ğœ‡ ğ‘— , Î£ ğ‘— ) and the prediction model ğ‘”(Â·) is a piecewise linear
function such that ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾) > 0 for any ğ›¾ âˆˆ R. The IS estimator ğ‘ is defined as ğ¼ (ğ‘”( Ëœğ‘‹ ) â‰¥ ğ›¾)ğ¿( Ëœğ‘‹ ) where Ëœğ‘‹ âˆ¼
(cid:205)ğ‘š
is at most polynomially

(cid:205)ğ‘Ÿ ğ‘—
ğ‘–=1 1/ğ‘Ÿ ğ‘— ğœ‹ ğ‘— ğ‘ (ğ‘ ğ‘—ğ‘–, Î£ ğ‘— ) and ğ¿(ğ‘¥) is as defined in (5). Then we have that Ëœğ¸ [ğ‘ 2]/ Ëœğ¸ [ğ‘ ]2

ğ‘—=1

growing in ğ›¾. That is, ğ‘ is asymptotically optimal.

When we apply Algorithm 1 to find all dominating points, the key is to be able to solve the optimization problems in

(3). We will investigate this in the next section.

Manuscript submitted to ACM

Rare-Event Simulation for Neural Network and Random Forest Predictors

9

Algorithm 3: Procedure for Gaussian mixture distributed input.
Input: Prediction model ğ‘”(ğ‘¥), threshold ğ›¾, input distribution (cid:205)ğ‘š
Output: Estimated rare-event probability Ë†ğ‘.

ğ‘—=1 ğœ‹ ğ‘— ğ‘ (ğœ‡ ğ‘— , Î£ ğ‘— ), simulation size ğ‘ .

1 Implement Algorithm 1 with input distribution ğ‘ (ğœ‡ ğ‘— , Î£ ğ‘— ) to get ğ´ğ‘— = {ğ‘ ğ‘—1, . . . , ğ‘ ğ‘—ğ‘Ÿ ğ‘— };
2 Generate Ëœğ‘‹1, . . . , Ëœğ‘‹ğ‘ âˆ¼ (cid:205)ğ‘š
ğ‘–=1 1/ğ‘Ÿ ğ‘— ğœ‹ ğ‘— ğ‘ (ğ‘ ğ‘—ğ‘–, Î£ ğ‘— );
ğ‘—=1
3 Compute Ë†ğ‘ = 1
ğ‘

(cid:205)ğ‘Ÿ ğ‘—

ğ‘–=1 ğ¼ (ğ‘”( Ëœğ‘‹ğ‘– ) â‰¥ ğ›¾)ğ¿( Ëœğ‘‹ğ‘– ) where
(cid:205)ğ‘
2 ğ‘’âˆ’ 1
ğ‘—=1 ğœ‹ ğ‘— |Î£ ğ‘— |âˆ’ 1
(cid:205)ğ‘š
(cid:205)ğ‘Ÿ ğ‘—
ğ‘–=1 1/ğ‘Ÿ ğ‘— ğœ‹ ğ‘— |Î£ ğ‘— |âˆ’ 1

ğ¿(ğ‘¥) =

(cid:205)ğ‘š

ğ‘—=1

2 (ğ‘¥âˆ’ğœ‡ ğ‘— )ğ‘‡ Î£âˆ’1

ğ‘— (ğ‘¥âˆ’ğœ‡ ğ‘— )

2 ğ‘’âˆ’ 1

2 (ğ‘¥âˆ’ğ‘ ğ‘—ğ‘– )ğ‘‡ Î£âˆ’1

ğ‘— (ğ‘¥âˆ’ğ‘ ğ‘—ğ‘– )

;

(5)

4 End

5 TRACTABLE OPTIMIZATION FORMULATION FOR PREDICTION MODELS

We discuss how to formulate the optimization problems in Algorithm 1 as an MIP with quadratic objective function and

linear constraints. Sections 5.1 and 5.2 focus on random forest and neural network structures respectively.

5.1 Tractable Formulation for Random Forest

To look for dominating points in a random forest or tree ensemble, we follow the route in [74] that studies optimization

over these models. We consider a random forest as follows. The input ğ‘¥ has ğ‘‘ dimensions. Suppose the model consists
of ğ‘‡ trees ğ‘“1, ..., ğ‘“ğ‘‡ . In each tree ğ‘“ğ‘¡ , we use ğ‘ğ‘–,ğ‘— to denote the ğ‘—th unique split point for the ğ‘–th dimension of the input ğ‘¥,
such that ğ‘ğ‘–,1 < ğ‘ğ‘–,2 < ... < ğ‘ğ‘–,ğ¾ğ‘–

, where ğ¾ğ‘– is the number of unique split points for the ğ‘–th dimension of ğ‘¥.

Following the notations in [74], let leaves(ğ‘¡) be the set of leaves (terminal nodes) of tree ğ‘¡ and splits(ğ‘¡) be the
set of splits (non-terminal nodes) of tree ğ‘¡. In each split ğ‘ , we let left(ğ‘ ) be the set of leaves that are accessible from
the left branch (the query at ğ‘  is true), and right(ğ‘ ) be the set of leaves that are accessible from the right branch (the
query at ğ‘  is false). For each node ğ‘ , we use V(ğ‘ ) âˆˆ {1, ..., ğ‘‘ } to denote the dimension that participate in the node and
C(ğ‘ ) âˆˆ {1, ..., ğ¾V(ğ‘ ) } to denote the set of values of dimension ğ‘– that participate in the split query of ğ‘  (C(ğ‘ ) = { ğ‘— } and
V(ğ‘ ) = {ğ‘–} indicate the query ğ‘¥ğ‘– â‰¤ ğ‘ğ‘–,ğ‘— ). We use ğœ†ğ‘¡ to denote the weight of tree ğ‘¡ ((cid:205)ğ‘‡
ğ‘¡ =1 ğœ†ğ‘¡ = 1). For each ğ‘™ âˆˆ leaves(ğ‘¡),
ğ‘ğ‘¡,ğ‘™ denotes the output for the ğ‘™th leaf in tree ğ‘¡.

To formulate the random forest optimization as an MIP, we introduce binary decision variables ğ‘§ğ‘–,ğ‘— and ğ‘¦ğ‘¡,ğ‘™ . First, we

have

ğ‘§ğ‘–,ğ‘— = ğ¼ (ğ‘¥ğ‘– â‰¤ ğ‘ğ‘–,ğ‘— ), ğ‘– = 1, ..., ğ‘‘, ğ‘— = 1, ..., ğ¾ğ‘– .

(6)

We then use ğ‘¦ğ‘¡,ğ‘™ = 1 to denote that tree ğ‘¡ outputs the prediction value ğ‘ğ‘¡,ğ‘™ on leaf ğ‘™, and ğ‘¦ğ‘¡,ğ‘™ = 0 otherwise. We use z, y
to represent the vectors of ğ‘§ğ‘–,ğ‘— and ğ‘¦ğ‘¡,ğ‘™ respectively. For the input ğ‘¥, we assume that ğ‘¥ âˆˆ [âˆ’ğµ, ğµ]ğ‘‘ and |ğ‘ğ‘–,ğ‘— | â‰¤ ğµ. Then
(6) is represented by the following constraints

ğ‘¥ğ‘– â‰¤ ğ‘ğ‘–,ğ‘— + 2(1 âˆ’ ğ‘§ğ‘–,ğ‘— )ğµ

ğ‘¥ğ‘– > ğ‘ğ‘–,ğ‘— âˆ’ 2ğ‘§ğ‘–,ğ‘— ğµ.

Manuscript submitted to ACM

10

Bai, Huang, Lam & Zhao

Now we formulate (3) with ğ´ = âˆ… as the following MIP

min
ğ‘¥,y,z

ğ‘ .ğ‘¡ .

(ğ‘¥ âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğœ‡)

ğ‘‡
âˆ‘ï¸

ğ‘¡ =1

âˆ‘ï¸

ğœ†ğ‘¡ ğ‘ğ‘¡,ğ‘™ğ‘¦ğ‘¡,ğ‘™ â‰¥ ğ›¾

ğ‘™ âˆˆleaves(ğ‘¡ )
âˆ‘ï¸

ğ‘¦ğ‘¡,ğ‘™ = 1, âˆ€ğ‘¡ âˆˆ {1, ...,ğ‘‡ }

(7)

ğ‘™ âˆˆleaves(ğ‘¡ )
âˆ‘ï¸

ğ‘¦ğ‘¡,ğ‘™ â‰¤

âˆ‘ï¸

ğ‘§V(ğ‘ ),ğ‘— , âˆ€ğ‘¡ âˆˆ {1, ...,ğ‘‡ }, ğ‘  âˆˆ splits(ğ‘¡)

ğ‘™ âˆˆleft(ğ‘ )
âˆ‘ï¸

ğ‘— âˆˆC(ğ‘ )
ğ‘¦ğ‘¡,ğ‘™ â‰¤ 1 âˆ’

âˆ‘ï¸

ğ‘§V(ğ‘ ),ğ‘— , âˆ€ğ‘¡ âˆˆ {1, ...,ğ‘‡ }, ğ‘  âˆˆ splits(ğ‘¡)

ğ‘— âˆˆC(ğ‘ )
ğ‘™ âˆˆright(ğ‘ )
ğ‘§ğ‘–,ğ‘— â‰¤ ğ‘§ğ‘–,ğ‘—+1, âˆ€ğ‘– âˆˆ {1, ..., ğ‘‘ }, ğ‘— âˆˆ {1, ..., ğ¾ğ‘– âˆ’ 1}

ğ‘§ğ‘–,ğ‘— âˆˆ {0, 1}, âˆ€ğ‘– âˆˆ {1, ..., ğ‘‘ }, ğ‘— âˆˆ {1, ..., ğ¾ğ‘– }

ğ‘¦ğ‘¡,ğ‘™ â‰¥ 0, âˆ€ğ‘¡ âˆˆ {1, ...,ğ‘‡ }, ğ‘™ âˆˆ leaves(ğ‘¡)

ğ‘¥ğ‘– â‰¤ ğ‘ğ‘–,ğ‘— + 2(1 âˆ’ ğ‘§ğ‘–,ğ‘— )ğµ, âˆ€ğ‘– âˆˆ {1, ..., ğ‘‘ }, ğ‘— âˆˆ {1, ..., ğ¾ğ‘– }

ğ‘¥ğ‘– > ğ‘ğ‘–,ğ‘— âˆ’ 2ğ‘§ğ‘–,ğ‘— ğµ, âˆ€ğ‘– âˆˆ {1, ..., ğ‘‘ }, ğ‘— âˆˆ {1, ..., ğ¾ğ‘– }.

This formulation has a quadratic objective function and linear constraints. Similarly, we can formulate (3) with ğ´ â‰  âˆ…
by adding linear constraints (ğ‘ğ‘– âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğ‘ğ‘– ) < 0, âˆ€ğ‘ğ‘– âˆˆ ğ´ to (7). Note that both the number of decision variables
and the number of constraints are linearly dependent on the total number of nodes in the random forest.

5.2 Tractable Formulation for Neural Network

A neural network ğ‘”(Â·) is a network that connects a large number of computational units (known as neurons) [30, 51].
According to its task, a network has a specific architecture that usually involves multiple layers of neurons and different

operations over the neurons. For simplification, here we consider layers with consecutive architecture and each layer of

the neural network only contains one specific structure.

The key part of the reformulation is to deal with the non-linearity brought by the maximum function. Our treatment

of the maximum function follows from [96], which rewrites neural network structures into linear equations with binary

variables.

In order to obtain tractable formulation for the constraint ğ‘”(ğ‘¥) â‰¥ ğ›¾, we independently handle each single layer in
ğ‘”(Â·). Assume we have ğ‘™ layers in ğ‘”(Â·), where ğ‘”ğ‘– (Â·) denotes the ğ‘–th layer. Given input ğ‘¥, the output of the neural network
can be represented as ğ‘”(ğ‘¥) = ğ‘”ğ‘™ (ğ‘”ğ‘™âˆ’1 (...ğ‘”1 (ğ‘¥))). For convenience, we introduce ğ‘¥ğ‘– to denote the output of the ğ‘–th layer
(note that it is also the input for the ğ‘– + 1th layer). In other words, for the ğ‘–th layer we have ğ‘¥ğ‘– = ğ‘”ğ‘– (ğ‘¥ğ‘˜âˆ’1). Using these

Manuscript submitted to ACM

Rare-Event Simulation for Neural Network and Random Forest Predictors

11

notations, we can transform the constraint ğ‘”(ğ‘¥) â‰¥ ğ›¾ into a sequence of constraints:

ğ‘¥ğ‘™ â‰¥ ğ›¾,
ğ‘¥ğ‘™ = ğ‘”ğ‘™ (ğ‘¥ğ‘™âˆ’1),
ğ‘¥ğ‘™âˆ’1 = ğ‘”ğ‘™âˆ’1 (ğ‘¥ğ‘™âˆ’2),
...,
ğ‘¥1 = ğ‘”1 (ğ‘¥).

This transformation makes clear that the constraints altogether are tractable if the constraint for each layer (i.e.
ğ‘¥ğ‘– = ğ‘”ğ‘– (ğ‘¥ğ‘˜âˆ’1)) is tractable . Note that both the number of decision variables and the number of constraints are linearly
dependent on the total number of neurons in the neural network. In the rest of this section, we discuss the reformulation

of neural network layers concerning different structures.

Fully Connected Layer. In a fully connected layer, each neuron performs a linear transformation on the input. We
5.2.1
consider a layer with ğ‘› neurons and the input for this layer is a vector ğ‘¥ âˆˆ Rğ‘š. We use ğ‘¤ğ‘– âˆˆ Rğ‘š and ğ‘ğ‘– âˆˆ R to denote
the weight and bias respectively for the linear transformation in the ğ‘–th neuron. Then the output of the ğ‘–th neuron can
be represented by ğ‘¦ğ‘– = ğ‘¤ğ‘‡

ğ‘– ğ‘¥ + ğ‘ğ‘– . To summarize, the output of the layer, ğ‘¦ = [ğ‘¦1; ğ‘¦2; ...; ğ‘¦ğ‘›] âˆˆ Rğ‘›, is given by

where ğ‘Š = [ğ‘¤1, ğ‘¤2, ..., ğ‘¤ğ‘›] and ğ‘ = [ğ‘1; ğ‘2; ...; ğ‘ğ‘›].

ğ‘¦ = ğ‘Š ğ‘‡ ğ‘¥ + ğ‘,

5.2.2 ReLU Layer. In a rectified linear unit (ReLU) layer, negative elements in the input are replaced by 0â€™s. For the ğ‘–th
input, the output is given by ğ‘¦ğ‘– = ğ‘šğ‘ğ‘¥ {ğ‘¥ğ‘–, 0}. This can be represented by

ğ‘¦ğ‘– â‰¤ ğ‘¥ğ‘– âˆ’ ğ‘™ (1 âˆ’ ğ‘§ğ‘– ),

ğ‘¦ğ‘– â‰¥ ğ‘¥ğ‘–,

ğ‘¦ğ‘– â‰¤ ğ‘¢ğ‘§ğ‘–,

ğ‘¦ğ‘– â‰¥ 0,

ğ‘§ğ‘– âˆˆ {0, 1} ,

where ğ‘§ğ‘– âˆˆ {0, 1} is a binary variable, ğ‘¢ and ğ‘™ are the upper and lower bounds of the input respectively.

5.2.3 Normalization Layer. In a normalization layer, the input is normalized and linearly transformed to make the
gradient decent algorithm more efficient. Again we assume the input is ğ‘¥ âˆˆ Rğ‘š with a given normalization parameter
ğœ‡ âˆˆ Rğ‘š and Î£ âˆˆ Rğ‘šÃ—ğ‘š. Moreover, we have the transformation matrix ğ›¾ âˆˆ Rğ‘šÃ—ğ‘š and bias vector ğ›½ âˆˆ Rğ‘š. The output
is given by

ğ‘¦ = ğ›¾

(cid:16)Î£âˆ’1/2 (ğ‘¥ âˆ’ ğœ‡)

(cid:17)

+ ğ›½.

5.2.4 Pooling Layer. In a pooling layer, a â€œfilterâ€ that can be applied to adjacent elements in a vector or matrix goes
through the input with a certain stride. Such type of layer is used to summarize â€œlocalâ€ information and reduce the

dimension of the input. Max pooling and average pooling are two types of commonly used filters.

Manuscript submitted to ACM

12

Bai, Huang, Lam & Zhao

Suppose the input is represented by matrix ğ‘¥ âˆˆ Rğ‘š1Ã—ğ‘š2 , where ğ‘¥ğ‘– ğ‘— denotes the element in the ğ‘–th row ğ‘—th column.
The size of the filter is ğ‘ 1 Ã— ğ‘ 2 with stride (ğ‘ 1, ğ‘ 2). The output have size ğ‘¦ âˆˆ Rğ‘›1,ğ‘›2 , where ğ‘›1 = ğ‘š1/ğ‘ 1 and ğ‘›2 = ğ‘š2/ğ‘ 2.
We assume that the value of ğ‘ 1, ğ‘ 2 are carefully chosen so that ğ‘›1 and ğ‘›2 are integers.

For average pooling layer, we have

ğ‘¦ğ‘– ğ‘— =

(cid:205)ğ‘–ğ‘ 1

ğ‘Ÿ =(ğ‘–âˆ’1)ğ‘ 1+1

(cid:205)ğ‘—ğ‘ 2

ğ‘=( ğ‘—âˆ’1)ğ‘ 2+1

ğ‘¥ğ‘Ÿğ‘

ğ‘ 1ğ‘ 2

for ğ‘– = 1, ..., ğ‘›1, ğ‘— = 1, ..., ğ‘›2.

For max pooling layer, we have ğ‘¦ğ‘– ğ‘— = max(ğ‘Ÿ,ğ‘) âˆˆğ‘† ğ‘¥ğ‘Ÿğ‘ for ğ‘– = 1, ..., ğ‘›1, ğ‘— = 1, ..., ğ‘›2, where ğ‘† = {(ğ‘Ÿ, ğ‘)|ğ‘Ÿ = (ğ‘– âˆ’ 1)ğ‘ 1 +

1, ..., ğ‘–ğ‘ 1, ğ‘ = ( ğ‘— âˆ’ 1)ğ‘ 2 + 1, ..., ğ‘—ğ‘ 2}. The tractable formulation is given by

ğ‘¦ğ‘– ğ‘— â‰¤ ğ‘¥ğ‘Ÿğ‘ âˆ’ (ğ‘¢ âˆ’ ğ‘™)(1 âˆ’ ğ‘§ğ‘Ÿğ‘ ),

ğ‘¦ğ‘– ğ‘— â‰¥ ğ‘¥ğ‘Ÿğ‘,
âˆ‘ï¸

ğ‘§ğ‘Ÿğ‘ = 1

(ğ‘Ÿ,ğ‘) âˆˆğ‘†
ğ‘§ğ‘Ÿğ‘ âˆˆ {0, 1},

(ğ‘Ÿ, ğ‘) âˆˆ ğ‘†

(ğ‘Ÿ, ğ‘) âˆˆ ğ‘†

(ğ‘Ÿ, ğ‘) âˆˆ ğ‘†.

5.2.5 Convolutional Layer. In a convolutional layer, several filters are used to extract features from the input. The
input of the layer is ğ‘¥ âˆˆ Rğ‘š1,ğ‘š2 . Suppose we have ğ‘Ÿ filters and assume the filters have size ğ‘ 1 Ã— ğ‘ 2 with stride (ğ‘¡1, ğ‘¡2).
We use ğ‘¤ğ‘– âˆˆ Rğ‘¡1ğ‘¡2 and ğ‘ğ‘– âˆˆ Rğ‘¡1ğ‘¡2 to denote the weight and bias for the ğ‘–th filter. The output is ğ‘¦ âˆˆ Rğ‘›1Ã—ğ‘›2Ã—ğ‘Ÿ , where
ğ‘›1 = (ğ‘š1 âˆ’ ğ‘ 1)/ğ‘¡1 and ğ‘›2 = (ğ‘š2 âˆ’ ğ‘ 2)/ğ‘¡2. Again we assume the numbers are carefully chosen so that ğ‘›1, ğ‘›2 are integers.

Then we have

ğ‘˜ ( Ëœğ‘¥ğ‘– ğ‘— ) + ğ‘ğ‘˜,

ğ‘¦ğ‘– ğ‘—ğ‘˜ = ğ‘¤ğ‘‡
Ëœğ‘¥ğ‘– ğ‘— = [ğ‘¥ (ğ‘–âˆ’1)ğ‘¡1+1,( ğ‘—âˆ’1)ğ‘¡2+1; ğ‘¥ (ğ‘–âˆ’1)ğ‘¡1+2,( ğ‘—âˆ’1)ğ‘¡2+1; ...; ğ‘¥ (ğ‘–âˆ’1)ğ‘¡1+1,( ğ‘—âˆ’1)ğ‘¡2+2, ...; ğ‘¥ (ğ‘–âˆ’1)ğ‘¡1+ğ‘ 1,( ğ‘—âˆ’1)ğ‘¡2+ğ‘ 2 ].

for integers 1 â‰¤ ğ‘– â‰¤ ğ‘›1, 1 â‰¤ ğ‘— â‰¤ ğ‘›2 and 1 â‰¤ ğ‘˜ â‰¤ ğ‘Ÿ .

5.2.6 Reformulation in the Output Layer. Here we discuss the reformulation of the output layer, which also provides
us clues on how other more general problems in classification tasks are potentially transformable into the constraint
ğ‘”(ğ‘¥) â‰¥ ğ›¾. Although the output layer is usually highly nonlinear, we show how to formulate it as linear mixed-integer
constraints.

In classification tasks, the neural network usually uses a softmax layer as the output layer for training purposes.
Suppose the classification problem has ğ‘› categories in total, the last layer inputs ğ‘¥ âˆˆ Rğ‘› and outputs ğ‘¦ âˆˆ Rğ‘› with
ğ‘’ğ‘¥ğ‘–
ğ‘— =1 ğ‘’ğ‘¥ ğ‘— . The prediction for classification is determined by the maximum value of ğ‘¦ğ‘– . Indeed, the result is equivalent
ğ‘¦ğ‘– =
if we determine the categories by the maximum value of ğ‘¥ğ‘– .

(cid:205)ğ‘›

When the constraint is ğ‘”(ğ‘‹ ) = ğ‘– or ğ‘”(ğ‘‹ ) â‰  ğ‘–, we can use this equivalence to reformulate the last layer (and therefore
complete the formulation for the whole network). Specifically, ğ‘”(ğ‘‹ ) = ğ‘– can be formulated as ğ‘¥ğ‘– â‰¥ ğ‘¥ ğ‘— , ğ‘“ ğ‘œğ‘Ÿ ğ‘— â‰  ğ‘– and
ğ‘”(ğ‘‹ ) â‰  ğ‘– can be formulated as ğ‘¥ğ‘– â‰¤ maxğ‘—â‰ ğ‘– ğ‘¥ ğ‘— , where ğ‘— â‰  ğ‘– denotes ğ‘— is an element for the set that contains all possible

Manuscript submitted to ACM

Rare-Event Simulation for Neural Network and Random Forest Predictors

13

indexes except ğ‘–. For tractable form, the latter formula can be further rewritten as:

ğ‘¥ğ‘– â‰¤ ğ‘¥ ğ‘— + (1 âˆ’ ğ‘§ ğ‘— )(ğ‘¢ âˆ’ ğ‘™), ğ‘— â‰  ğ‘–.
âˆ‘ï¸

ğ‘§ ğ‘— â‰¥ 1,

ğ‘—â‰ ğ‘–
ğ‘§ ğ‘— âˆˆ {0, 1}, ğ‘– â‰  ğ‘.

6 EXPERIMENTS

This section presents several experimental results using our Algorithm 1 for neural network and random forest predictors.

In Section 6.1, we consider two simple toy examples. The first problem has one dominating point and the second

problem has multiple dominating points. To illustrate the efficiency of the IS scheme, we compare it with the naive use

of a uniform IS estimator. In Section 6.2, we consider a realistic problem generated from a classification data set with a

high dimensional feature space.

6.1 Toy Problems
Consider a problem where ğ‘‹ follows a distribution ğ‘“ (ğ‘¥), and the set {ğ‘¥ : ğ‘”(ğ‘¥) â‰¥ ğ›¾ } is known to lie inside [ğ‘™, ğ‘¢]ğ‘‘ where
ğ‘‘ is the dimension of the input variable ğ‘‹ . The uniform IS estimator is given by

ğ‘ğ‘¢ğ‘›ğ‘– ğ‘“ ğ‘œğ‘Ÿğ‘š = ğ¼ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾) ğ‘“ (ğ‘‹ )(ğ‘¢ âˆ’ ğ‘™)ğ‘‘,

where ğ‘‹ is generated from a uniform distribution on [ğ‘™, ğ‘¢]ğ‘‘ . This estimator has a polynomially growing relative
efficiency as the magnitude of the dominating points grows [60], but the efficiency also depends significantly on the

size of the bounded set, i.e., ğ‘™, ğ‘¢, ğ‘‘.

The first problem has input ğ‘¥ = [ğ‘¥1, ğ‘¥2] over the bounded space [0, 5]2. We generate 2,601 samples using a uniform

grid over the space with a mesh of 0.1 on each coordinate and use the function

ğ‘¦ (ğ‘¥) = (ğ‘¥1 âˆ’ 5)3 + (ğ‘¥2 âˆ’ 4.5)3 + (ğ‘¥1 âˆ’ 1)2 + ğ‘¥ 2

2 + 500

(8)

to label these samples. The dataset we obtained is denoted as ğ· = {(ğ‘‹ğ‘›, ğ‘Œğ‘›)}. ğ‘”(ğ‘¥) is trained using ğ·. We consider only
ğ‘‹ in the region [0, 5]2, so that ğ‘”(ğ‘¥) can be thought of as being set to 0 outside this box. We use ğ›¾ = 500 in this example
and the shape of the rare-event set {ğ‘¥ : ğ‘”(ğ‘¥) â‰¥ ğ›¾ }.

We first train a random forest ğ‘”(ğ‘¥), which ensembles three regression trees. The three regression trees are averaged
and each of them has around 600 nodes. The rare-event set is presented in Figure 1. The dominating point is obtained
by implementing Algorithm 1, which is located at (3.05, 2.65). We recall the problem setting that the input ğ‘‹ follows a
Gaussian distribution. In particular, we use Gaussian distributions ğ‘ (0, ğ¼ğœ2), where ğ¼ denotes the identity matrix and
ğœ2 âˆˆ R+. In our experiment, we vary the value of ğœ2 to create problems with different rarity, where a smaller ğœ2 gives a
rarer probability.

Figures 3 and 4 present the experimental results based on 50,000 samples. In Figure 3, we observe that the estimates

for the two IS schemes are similar in all considered cases. On the other hand, Figure 4 shows the relative error for
the proposed IS is smaller in all ğœ2 considered. Moreover, as the rarity increases, the relative error of the proposed
IS increases from roughly 2.5 to 5, whereas the relative error of the uniform IS increases from 5 to 40. The slower

increasing rate indicates that the proposed IS scheme is more efficient and the outperformance is stronger for rarer

problems.

Manuscript submitted to ACM

14

Bai, Huang, Lam & Zhao

Fig. 1. Rare-event set and dominating points
for the random forest (case 1).

Fig. 2. Rare-event set and dominating points
for the neural network (case 1).

Fig. 3. Probability estimation with different
numbers of samples. Random forest, case 1.

Fig. 4. 95% confidence interval half-width
with different numbers of samples. Random
forest, case 1.

Next, we train a neural network predictor as ğ‘”(ğ‘¥). The neural network has 3 layers with 100 neurons in each of
the 2 hidden layers, and all neurons are ReLU. The defined rare-event set is presented in Figure 2. We observe that

the set is roughly convex and should have a single dominating point. We obtain the dominating point for the set at
(3.3676, 2.6051). Figures 5 and 6 shows our results. Again we observe the proposed IS scheme provides smaller relative
errors in all cases and the advantage increases with the rarity level (the relative error increases from 2.5 to 10 for the

proposed IS and 5 to 55 for the uniform IS).

Next, we consider true output values generated according to the function

ğ‘¦ (ğ‘¥) = 10 Ã— ğ‘’âˆ’

(cid:16) ğ‘¥1âˆ’5
3

(cid:17) 2

(cid:16) ğ‘¥2âˆ’5
4

âˆ’

(cid:17) 2

+ 10 Ã— ğ‘’âˆ’ğ‘¥1

2âˆ’(ğ‘¥2âˆ’4.5) 2

.

(9)

Again we use a uniform grid over [0, 5]2 with a mesh of 0.1 on each coordinate to train the predictors. The random
forest ensembles three regression trees with around 600 nodes and the neural network with 2 hidden layers, 100 neurons

in the first hidden layer and 50 neurons in the second hidden layer. All neurons in the neural network are ReLU. We
set ğ›¾ = 8. The shapes of the rare-event sets are shown in Figures 7 and 8. We observe that the set now consists of
Manuscript submitted to ACM

Rare-Event Simulation for Neural Network and Random Forest Predictors

15

Fig. 5. Probability estimation with different
numbers of samples. Neural network, case 1.

Fig. 6. 95% confidence interval half-width
with different numbers of samples. Neural
network, case 1.

Fig. 7. Rare-event set and dominating point
for the random forest (case 2).

Fig. 8. Rare-event set and dominating point
for the neural network (case 2).

two disjoint regions and therefore we expect to obtain multiple dominating points. Using Algorithm 1, we obtain two
dominating points in each case: (0, 4.15) and (3.75, 3.55) for the random forest model; (0.113, 4.162) and (4.187, 3.587)
for the neural network model. We use these dominating points to construct a mixture distribution, as discussed in
Section 3, as the IS distribution. Again we vary ğœ2 to obtain problems with different rarities and use 50,000 samples for
each case.

The experimental results for the random forest predictor are shown in Figures 9 and 10, and the results for the

neural network predictor are shown in Figures 11 and 12. Similar to the previous problem, both IS schemes give similar

estimates in all the cases, as observed in Figures 9 and 11. The relative errors shown in Figures 10 and 12 illustrate that,

as the probability of interest decreases, the relative error ratio between the uniform IS and the proposed IS increases

from 2 to around 5-6. We can conclude that the proposed IS scheme again outperforms the uniform IS and is more

preferable as the rarity increases.

Manuscript submitted to ACM

16

Bai, Huang, Lam & Zhao

Fig. 9. Probability estimation with different
numbers of samples. Random forest, case 2.

Fig. 10. 95% confidence interval half-width
with different numbers of samples. Random
forest, case 2.

Fig. 11. Probability estimation with differ-
ent numbers of samples. Neural network,
case 2.

Fig. 12. 95% confidence interval half-width
with different numbers of samples. Neural
network, case 2.

6.2 MAGIC Gamma Telescope Data Set

We study a rare-event probability estimation problem from a realistic classification task. The classification problem

uses the MAGIC Gamma Telescope data set in the UCI Machine Learning Repository [8]. The problem is to classify

images of electromagnetic showers collected by a ground-based atmospheric Cherenkov gamma telescope. The features

of the data are 10-dimensional characteristic parameters of the images and the data set contains 19020 data points in

total. Studies [20, 40, 92] use machine learning predictors to discriminate images caused by a â€œsignalâ€ (primary gammas)

from those initiated by the â€œbackgroundâ€ (cosmic rays in the upper atmosphere).

To train the predictors, we allocate 15,000 data points as the training set and use the remaining 4,020 data points as

the testing set. We train a random forest that ensembles 10 random trees to achieve 85.6% testing set accuracy. For

neural network, we use 2 hidden layers with 20 neurons and achieved 87% testing set accuracy.

The rare-event probability of interest is the statistical robustness metric (Example 3.1) of the two trained predictors.

Specifically, we consider a testing data point, say with input ğ‘¥ and true label ğ‘¦, that is correctly predicted in both
predictors (the predicted value ğ‘”(ğ‘¥) is consistent with ğ‘¦). Then we perturb the input ğ‘¥ with a Gaussian noise ğœ– âˆ¼ ğ‘ (0, ğ¼ğœ2)
Manuscript submitted to ACM

Rare-Event Simulation for Neural Network and Random Forest Predictors

17

Fig. 13. Probability estimation with differ-
ent numbers of samples. Random forest,
MAGIC.

Fig. 14. 95% confidence interval half-width
with different numbers of samples. Random
forest, MAGIC.

and estimate the probability of ğ‘ƒ (ğ‘”(ğ‘¥ + ğœ–) â‰  ğ‘¦), where we vary the value of ğœ2 to construct rare-event with different
rarities. Note that, as discussed in Example 3.1, ğ‘ƒ (ğ‘”(ğ‘¥ + ğœ–) â‰  ğ‘¦) can be transformed into the format considered in this
paper, i.e. ğ‘ƒ (ğ‘”(ğ‘‹ ) > ğ›¾).

First, we implement Algorithm 1 to obtain dominating points for the rare-event sets {ğ‘”(ğ‘¥ + ğœ–) â‰  ğ‘¦} with random
forest and neural network as ğ‘”(Â·) respectively. We obtain 53 dominating points for the rare-event sets associated with
the random forest predictor and 217 dominating points in the neural network case. The IS distributions are constructed
using these dominating points. In both problems, ğœ2 ranges from 0.03 to 0.1 and we use 50,000 samples to estimate each
target rare-event probabilities.

The experimental results for the random forest and neural network are presented in Figures 13 and 14 respectively.

We observe that the estimates are very accurate in all experiments (with different rarities), which are indicated by the

tight 95% confidence intervals. These results show that our proposed IS scheme performs well with large numbers of

dominating points and in relatively high-dimensional problems.

7 PROOF OF THEOREMS

Throughout this section, we write ğ‘“1 (ğ›¾) âˆ¼ ğ‘“2 (ğ›¾) if limğ›¾â†’âˆ ğ‘“1 (ğ›¾)/ğ‘“2 (ğ›¾) = 1. First of all, we adapt Theorem 4.1 in [55] to
obtain the following lemma.

Lemma 7.1. Let ğ‘Œ be a ğ‘‘-dimensional Gaussian random vector with zero mean and positie definite covariance matrix ËœÎ£.
Suppose that Ëœğ‘  = Ëœğ‘  (ğ›¾) is a ğ‘‘-dimensional vector such that as ğ›¾ â†’ âˆ, at least one of its components goes to âˆ. Use ğ‘¦âˆ— to
denote arg minğ‘¦ â‰¥ Ëœğ‘  ğ‘¦ğ‘‡ ËœÎ£âˆ’1ğ‘¦. Then by Proposition 2.1 in [55], we know that there exists a unique set ğ¼ âŠ‚ {1, Â· Â· Â· , ğ‘‘ } such that

1 â‰¤ |ğ¼ | â‰¤ ğ‘‘;

ğ‘¦âˆ—
ğ¼ = Ëœğ‘ ğ¼ â‰  0ğ¼ ;
If ğ½ := {1, . . . , ğ‘‘ } \ ğ¼ â‰  âˆ…, then ğ‘¦âˆ—
ğ‘– ( ËœÎ£ğ¼ ğ¼ )âˆ’1 Ëœğ‘ ğ¼ > 0;
âˆ€ğ‘– âˆˆ ğ¼, ğ‘’ğ‘‡

ğ½ = âˆ’( ËœÎ£âˆ’1)âˆ’1

ğ½ ğ½ ( ËœÎ£âˆ’1)ğ½ ğ¼ Ëœğ‘ ğ¼ â‰¥ Ëœğ‘ ğ½

ğ‘¦ğ‘‡ ËœÎ£âˆ’1ğ‘¦ = (ğ‘¦âˆ—)ğ‘‡ ËœÎ£âˆ’1ğ‘¦âˆ— > 0.

min
ğ‘¦ â‰¥ğ‘¡

(10a)

(10b)

(10c)

(10d)

(10e)

Manuscript submitted to ACM

18

Bai, Huang, Lam & Zhao

We suppose that for sufficiently large ğ›¾, the set ğ¼ does not change with ğ›¾ and limğ›¾â†’âˆ ( Ëœğ‘  âˆ’ ğ‘¦âˆ—)ğ½ = Ëœğ‘ âˆ—
ğ‘– ( ËœÎ£ğ¼ ğ¼ )âˆ’1 Ëœğ‘ ğ¼ either goes to âˆ or is a positive constant. Then as ğ›¾ â†’ âˆ, we have that
âˆ€ğ‘– âˆˆ ğ¼ , ğ‘’ğ‘‡

ğ½ . Suppose further that

where ğ¶ = ğ¶ (ğ›¾) is a positive constant.

ğ‘ƒ (ğ‘Œ â‰¥ Ëœğ‘ ) âˆ¼ ğ¶

exp{âˆ’(ğ‘¦âˆ—)ğ‘‡ ËœÎ£âˆ’1ğ‘¦âˆ—/2}

(cid:206)ğ‘– âˆˆğ¼ ğ‘’ğ‘‡

ğ‘– ( ËœÎ£ğ¼ ğ¼ )âˆ’1 Ëœğ‘ ğ¼

Proof. Given ğ‘¥ âˆˆ Rğ‘‘ , we define Ëœğ‘¥ in the following way: ( Ëœğ‘¥)ğ‘– = (ğ‘’ğ‘‡

ğ‘– ( ËœÎ£ğ¼ ğ¼ )âˆ’1 Ëœğ‘ ğ¼ )âˆ’1ğ‘¥ğ‘–, âˆ€ğ‘– âˆˆ ğ¼ ; ( Ëœğ‘¥)ğ½ = ğ‘¥ ğ½ . Using (3.4) in

[55], we know that

and thus

(ğ‘¥ + ğ‘¦âˆ—)ğ‘‡ ËœÎ£âˆ’1 (ğ‘¥ + ğ‘¦âˆ—) = ğ‘¥ğ‘‡ ËœÎ£âˆ’1ğ‘¥ + 2(ğ‘¥ğ¼ )ğ‘‡ ( ËœÎ£ğ¼ ğ¼ )âˆ’1 Ëœğ‘ ğ¼ + (ğ‘¦âˆ—)ğ‘‡ ËœÎ£âˆ’1ğ‘¦âˆ—,

ğœ™ ( Ëœğ‘¥ + ğ‘¦âˆ—) = (2ğœ‹)âˆ’ ğ‘‘

2 | ËœÎ£|âˆ’ 1

2 exp{âˆ’

= (2ğœ‹)âˆ’ ğ‘‘

2 | ËœÎ£|âˆ’ 1

2 exp{âˆ’

(cid:104)

(cid:104)

1
2
1
2

( Ëœğ‘¥)ğ‘‡ ËœÎ£âˆ’1 Ëœğ‘¥ + 2( Ëœğ‘¥ğ¼ )ğ‘‡ ( ËœÎ£ğ¼ ğ¼ )âˆ’1 Ëœğ‘ ğ¼ + (ğ‘¦âˆ—)ğ‘‡ ËœÎ£âˆ’1ğ‘¦âˆ—(cid:105)
ğ¼ 1ğ¼ + (ğ‘¦âˆ—)ğ‘‡ ËœÎ£âˆ’1ğ‘¦âˆ—(cid:105)

( Ëœğ‘¥)ğ‘‡ ËœÎ£âˆ’1 Ëœğ‘¥ + 2ğ‘¥ğ‘‡

}

}

where ğœ™ is the density function of ğ‘ (0, ËœÎ£). Then we get that

ğ‘ƒ (ğ‘Œ â‰¥ Ëœğ‘ )
âˆ«

=

ğœ™ (ğ‘¦)dğ‘¦

ğ‘¦ â‰¥ Ëœğ‘ 

=

(cid:32)

(cid:214)

ğ‘– âˆˆğ¼

ğ‘– ( ËœÎ£ğ¼ ğ¼ )âˆ’1 Ëœğ‘ ğ¼
ğ‘’ğ‘‡

(cid:33)âˆ’1

âˆ«

ğ‘¥ â‰¥ Ëœğ‘ âˆ’ğ‘¦âˆ—

ğœ™ ( Ëœğ‘¥ + ğ‘¦âˆ—)dğ‘¥

=(2ğœ‹)âˆ’ ğ‘‘

2 | ËœÎ£|âˆ’ 1

2

(cid:33)âˆ’1

ğ‘– ( ËœÎ£ğ¼ ğ¼ )âˆ’1 Ëœğ‘ ğ¼
ğ‘’ğ‘‡

(cid:32)

(cid:214)

ğ‘– âˆˆğ¼

exp{âˆ’(ğ‘¦âˆ—)ğ‘‡ ËœÎ£âˆ’1ğ‘¦âˆ—/2}

âˆ«

ğ‘¥ â‰¥ Ëœğ‘ âˆ’ğ‘¦âˆ—

exp{âˆ’( Ëœğ‘¥)ğ‘‡ ËœÎ£âˆ’1 Ëœğ‘¥/2 âˆ’ ğ‘¥ğ‘‡

ğ¼ 1ğ¼ }dğ‘¥ .

Apparent from the above, it suffices to show that âˆ«
as ğ›¾ â†’ âˆ. Indeed, using (3.6) in [55] we know that ( Ëœğ‘¥)ğ‘‡ ËœÎ£âˆ’1 Ëœğ‘¥ â‰¥ ğ‘¥ğ‘‡

ğ‘¥ â‰¥ğ‘¡ âˆ’ğ‘¦âˆ— exp{âˆ’( Ëœğ‘¥)ğ‘‡ ËœÎ£âˆ’1 Ëœğ‘¥/2 âˆ’ ğ‘¥ğ‘‡

ğ¼ 1ğ¼ }dğ‘¥ converges to a positive constant

exp{âˆ’( Ëœğ‘¥)ğ‘‡ ËœÎ£âˆ’1 Ëœğ‘¥/2 âˆ’ ğ‘¥ğ‘‡

ğ¼ 1ğ¼ } â‰¤ exp{âˆ’ğ‘¥ğ‘‡

ğ½ ( ËœÎ£ğ½ ğ½ )âˆ’1ğ‘¥ ğ½ and thus
ğ½ ( ËœÎ£ğ½ ğ½ )âˆ’1ğ‘¥ ğ½ /2 âˆ’ ğ‘¥ğ‘‡

ğ¼ 1ğ¼ }.

Moreover, we have that

âˆ«

ğ‘¥ğ¼ â‰¥0ğ¼

exp{âˆ’ğ‘¥ğ‘‡

ğ½ ( ËœÎ£ğ½ ğ½ )âˆ’1ğ‘¥ ğ½ /2 âˆ’ ğ‘¥ğ‘‡

ğ¼ 1ğ¼ }dğ‘¥ =

âˆ«

R|ğ½ |

exp{âˆ’ğ‘¥ğ‘‡

ğ½ ( ËœÎ£ğ½ ğ½ )âˆ’1ğ‘¥ ğ½ /2}dğ‘¥ ğ½ < âˆ.

ğ‘– ( ËœÎ£ğ¼ ğ¼ )âˆ’1 Ëœğ‘ ğ¼ â†’ âˆ} and ğ¼2 = {ğ‘– âˆˆ ğ¼ : ğ‘’ğ‘‡

ğ‘– ( ËœÎ£ğ¼ ğ¼ )âˆ’1 Ëœğ‘ ğ¼ is a positive constant}. Then we get

We partition ğ¼ into ğ¼1 = {ğ‘– âˆˆ ğ¼ : ğ‘’ğ‘‡
that

exp{âˆ’( Ëœğ‘¥)ğ‘‡ ËœÎ£âˆ’1 Ëœğ‘¥/2 âˆ’ ğ‘¥ğ‘‡

ğ¼ 1ğ¼ }

lim
ğ›¾â†’âˆ
(cid:26)

= exp

Manuscript submitted to ACM

(cid:104)

1
2

âˆ’

( Ëœğ‘¥ğ¼2 )ğ‘‡ ( ËœÎ£âˆ’1)ğ¼2ğ¼2

Ëœğ‘¥ğ¼2 + ( Ëœğ‘¥ğ¼2 )ğ‘‡ ( ËœÎ£âˆ’1)ğ¼2 ğ½ ğ‘¥ ğ½ + ğ‘¥ğ‘‡

ğ½ ( ËœÎ£âˆ’1)ğ½ ğ¼2

Ëœğ‘¥ğ¼2 + ğ‘¥ğ‘‡

ğ½ ( ËœÎ£âˆ’1)ğ½ ğ½ ğ‘¥ ğ½

(cid:105)

âˆ’ ğ‘¥ğ‘‡

ğ¼ 1ğ¼

(cid:27)

.

Rare-Event Simulation for Neural Network and Random Forest Predictors

19

We know that the above limit does not depend on ğ›¾. By applying the dominated convergence theorem, we get that

âˆ«

ğ‘¥ â‰¥ Ëœğ‘ âˆ’ğ‘¦âˆ—

lim
ğ›¾â†’âˆ
âˆ« âˆ«

exp{âˆ’( Ëœğ‘¥)ğ‘‡ ËœÎ£âˆ’1 Ëœğ‘¥/2 âˆ’ ğ‘¥ğ‘‡

ğ¼ 1ğ¼ }dğ‘¥

ğ‘¥ğ¼ â‰¥0ğ¼ ,ğ‘¥ ğ½ â‰¥ Ëœğ‘ âˆ—
ğ½
(cid:26)

(cid:104)

1
2

exp

âˆ’

( Ëœğ‘¥ğ¼2 )ğ‘‡ ( ËœÎ£âˆ’1)ğ¼2ğ¼2

âˆ« âˆ«

Ëœğ‘¥ğ¼2 + ( Ëœğ‘¥ğ¼2 )ğ‘‡ ( ËœÎ£âˆ’1)ğ¼2 ğ½ ğ‘¥ ğ½ + ğ‘¥ğ‘‡

ğ½ ( ËœÎ£âˆ’1)ğ½ ğ¼2

Ëœğ‘¥ğ¼2 + ğ‘¥ğ‘‡

ğ½ ( ËœÎ£âˆ’1)ğ½ ğ½ ğ‘¥ ğ½

(cid:27)

(cid:105)

âˆ’ ğ‘¥ğ‘‡

ğ¼ 1ğ¼

dğ‘¥ğ¼ dğ‘¥ ğ½

=

=

ğ‘¥ğ¼2 â‰¥0ğ¼2 ,ğ‘¥ ğ½ â‰¥ Ëœğ‘ âˆ—
ğ½
(cid:26)

exp

âˆ’

(cid:104)

( Ëœğ‘¥ğ¼2 )ğ‘‡ ( ËœÎ£âˆ’1)ğ¼2ğ¼2

1
2
ğ‘¥ â‰¥ğ‘¡ âˆ’ğ‘¦âˆ— exp{âˆ’( Ëœğ‘¥)ğ‘‡ ËœÎ£âˆ’1 Ëœğ‘¥/2 âˆ’ ğ‘¥ğ‘‡

This shows that âˆ«
proved the theorem.

Ëœğ‘¥ğ¼2 + ( Ëœğ‘¥ğ¼2 )ğ‘‡ ( ËœÎ£âˆ’1)ğ¼2 ğ½ ğ‘¥ ğ½ + ğ‘¥ğ‘‡

ğ½ ( ËœÎ£âˆ’1)ğ½ ğ¼2

Ëœğ‘¥ğ¼2 + ğ‘¥ğ‘‡

ğ½ ( ËœÎ£âˆ’1)ğ½ ğ½ ğ‘¥ ğ½

(cid:27)

(cid:105)

âˆ’ ğ‘¥ğ‘‡

ğ¼2 1ğ¼2

dğ‘¥ğ¼2 dğ‘¥ ğ½ .

ğ¼ 1ğ¼ }dğ‘¥ converges to a positive constant as ğ›¾ â†’ âˆ, and hence we have
â–¡

Proof of Theorem 4.2. Suppose that ğ‘”(ğ‘¥) = ğ‘”ğ‘– (ğ‘¥) for â„ğ‘– ğ‘— (ğ‘¥) â‰¥ 0, ğ‘— = 1, . . . , ğ‘šğ‘– , ğ‘– = 1, . . . , ğ‘Ÿ â€² where ğ‘”ğ‘– â€™s and â„ğ‘– ğ‘— â€™s
are all affine functions. Then we can split {ğ‘¥ : ğ‘”(ğ‘¥) â‰¥ ğ›¾ } into ËœR1, . . . , ËœRğ‘Ÿ â€² where ËœRğ‘– = {ğ‘¥ : ğ‘”ğ‘– (ğ‘¥) â‰¥ ğ›¾, â„ğ‘– ğ‘— (ğ‘¥) â‰¥ 0, ğ‘— =
1, . . . , ğ‘šğ‘– }. We denote Ëœğ‘ğ‘– = arg minğ‘¥ {(ğ‘¥ âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğœ‡) : ğ‘¥ âˆˆ ËœRğ‘– }.

To justify the asymptotic optimality of the proposed IS estimator (4), we need to show that

Ëœğ¸ [ğ‘ 2]
Ëœğ¸ [ğ‘ ]2 =

ğ¸ [ğ¼ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾)ğ¿(ğ‘‹ )]
ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾)2

=

(cid:205)ğ‘Ÿ

ğ‘–=1 ğ¸ [ğ¼ (ğ‘‹ âˆˆ Rğ‘– )ğ¿(ğ‘‹ )]
(cid:16)(cid:205)ğ‘Ÿ â€²
ğ‘–=1 ğ‘ƒ (ğ‘‹ âˆˆ ËœRğ‘– )

(cid:17)2

is at most polynomially growing in ğ›¾.

To simplify the notations, we consider the polyhedron ğ‘ƒ1 := {ğ‘¥ âˆˆ Rğ‘‘ : ğ´ğ‘¥ â‰¥ ğ‘¡ } where ğ´ âˆˆ Rğ‘šÃ—ğ‘‘, ğ‘¡ âˆˆ Rğ‘š and
in particular, ğ‘¡1 = ğ›¾ + ğ‘ for some constant ğ‘ âˆˆ R and ğ‘¡2, . . . , ğ‘¡ğ‘š are all constants in R. Naturally, we assume that
ğ‘ƒ (ğ‘‹ âˆˆ ğ‘ƒ1) > 0 where ğ‘‹ âˆ¼ ğ‘ (ğœ‡, Î£) for any ğ›¾ âˆˆ R. We define ğ‘¥ âˆ— = arg min{(ğ‘¥ âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğœ‡) : ğ‘¥ âˆˆ ğ‘ƒ1}. Note that for
sufficiently large ğ›¾, each component of ğ‘¥ âˆ— is an affine function of ğ›¾, so (ğ‘¥ âˆ— âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ— âˆ’ ğœ‡) is a quadratic polynomial
of ğ›¾. We will prove that âˆ’ log ğ‘ƒ (ğ‘‹ âˆˆ ğ‘ƒ1) âˆ¼ (ğ‘¥ âˆ— âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ— âˆ’ ğœ‡)/2 as ğ›¾ â†’ âˆ.

We use ğ´ğ‘– to denote the ğ‘–-th row vector of ğ´. Suppose that ğ´ğ‘‡
ğ‘– ğ‘—

ğ‘¥ â‰¥ ğ‘¡ğ‘– ğ‘— , ğ‘— = 1, . . . , ğ‘šâ€² are all the linearly independent
active constraints at ğ‘¥ âˆ—. If ğ‘šâ€² < ğ‘‘, then we can add redundant constraints in the form of ğ‘¥ğ‘˜ğ‘™ â‰¥ âˆ’âˆ, ğ‘™ = 1, Â· Â· Â· , ğ‘‘ âˆ’ ğ‘šâ€²
such that we get ğ‘‘ linearly independent constraints now. More specifically, let

ğ´ğ‘‡
ğ‘–1
...
ğ´ğ‘‡
ğ‘–ğ‘šâ€²
ğ‘’ğ‘‡
ğ‘˜1
...

ğµ =

(cid:170)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:172)
By the definition, we get that ğµ is invertible. We know that for sufficiently large ğ›¾, the active constraints at ğ‘¥ âˆ— do not
change as ğ›¾ increases. Thus, in our following discussions, we assume that ğµ and ğ‘  does not change with ğ›¾. Also, it
is clear that the constraint ğ´ğ‘‡
1 â‰¥ ğ‘¡1 = ğ›¾ + ğ‘ must be active at ğ‘¥ âˆ—, i.e. ğ‘–1 = 1. Since ğ‘ƒ2 := {ğ‘¥ : ğµğ‘¥ â‰¥ ğ‘ } is obtained by
Manuscript submitted to ACM

(cid:169)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
ğ‘’ğ‘‡
ğ‘˜ğ‘‘âˆ’ğ‘šâ€²
(cid:171)

(cid:170)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:172)

, ğ‘  =

.

ğ‘¡ğ‘–1
(cid:169)
...
(cid:173)
(cid:173)
(cid:173)
(cid:173)
ğ‘¡ğ‘–ğ‘šâ€²
(cid:173)
(cid:173)
(cid:173)
âˆ’âˆ
(cid:173)
...
(cid:173)
(cid:173)
(cid:173)
(cid:173)
âˆ’âˆ
(cid:171)

20

Bai, Huang, Lam & Zhao

removing constraints from ğ‘ƒ1, we have that ğ‘ƒ1 âŠ‚ ğ‘ƒ2. Our first step is to develop the asymptotic result of ğ‘ƒ (ğ‘‹ âˆˆ ğ‘ƒ2),
where we directly apply Lemma 7.1.

We know that ğ‘Œ := ğµ(ğ‘‹ âˆ’ğœ‡) âˆ¼ ğ‘ (0, ËœÎ£) where ËœÎ£ = ğµÎ£ğµğ‘‡ is positive definite. We denote ğ‘¦âˆ— = arg min{ğ‘¦ğ‘‡ ËœÎ£âˆ’1ğ‘¦ : ğ‘¦ â‰¥ Ëœğ‘ }
where Ëœğ‘  = ğ‘  âˆ’ ğµğœ‡. Recall that under our settings, ğ‘ 1 = ğ›¾ + ğ‘ for some constant ğ‘ âˆˆ R so Ëœğ‘ 1 â†’ âˆ as ğ›¾ â†’ âˆ. We still use
the symbol ğ¼ to denote the set that satisfies (10). Similar to our previous argument, ğ¼ does not change for sufficiently
ğ‘– ( ËœÎ£ğ¼ ğ¼ )âˆ’1 Ëœğ‘ ğ¼ > 0 and it is an affine function
large ğ›¾. Also the limit limğ›¾â†’âˆ ( Ëœğ‘  âˆ’ ğ‘¦âˆ—)ğ½ exists. For any ğ‘– âˆˆ ğ¼ , we know that ğ‘’ğ‘‡
of ğ›¾, and thus either it goes to âˆ or it is a positive constant as ğ›¾ â†’ âˆ. In conclusion, all the assumptions of Lemma 7.1
hold in this case. Therefore, we get that

ğ‘ƒ (ğ‘‹ âˆˆ ğ‘ƒ2) âˆ¼ ğ¶

exp{âˆ’(ğ‘¦âˆ—)ğ‘‡ ËœÎ£âˆ’1ğ‘¦âˆ—/2}

(cid:206)ğ‘– âˆˆğ¼ ğ‘’ğ‘‡

ğ‘– ( ËœÎ£ğ¼ ğ¼ )âˆ’1 Ëœğ‘ ğ¼

(11)

for some constant ğ¶. It is easy to verify that (ğ‘¦âˆ—)ğ‘‡ ËœÎ£âˆ’1ğ‘¦âˆ— = (ğ‘¥ âˆ— âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ— âˆ’ ğœ‡), and hence âˆ’ log ğ‘ƒ (ğ‘‹ âˆˆ ğ‘ƒ2) âˆ¼
(ğ‘¥ âˆ— âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ— âˆ’ ğœ‡)/2.

Clearly ğ‘ƒ (ğ‘‹ âˆˆ ğ‘ƒ2) gives an upper bound for ğ‘ƒ (ğ‘‹ âˆˆ ğ‘ƒ1). Now we develop a lower bound using similar techniques.
We denote ğ‘¥ âˆ—âˆ— = arg minğ‘¥ {(ğ‘¥ âˆ’ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ğœ‡) : ğ‘¥ âˆˆ ğ‘ƒ2 \ ğ‘ƒ1} and ğ‘¥ âˆ—âˆ—âˆ— = arg minğ‘¥ {(ğ‘¥ âˆ’ğ‘¥ âˆ—)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ğ‘¥ âˆ—) : ğ‘¥ âˆˆ ğ‘ƒ2 \ ğ‘ƒ1}.
Clearly each component of ğ‘¥ âˆ— and ğ‘¥ âˆ—âˆ—âˆ— is affine in ğ›¾ when ğ›¾ is sufficiently large, and hence (ğ‘¥ âˆ—âˆ—âˆ—âˆ’ğ‘¥ âˆ—)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ—âˆ—âˆ—âˆ’ğ‘¥ âˆ—) â‰¥ 0
is polynomial in ğ›¾. Thus we know that (ğ‘¥ âˆ—âˆ—âˆ— âˆ’ğ‘¥ âˆ—)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ—âˆ—âˆ— âˆ’ğ‘¥ âˆ—) either goes to infinity or stays a nonnegative constant
as ğ›¾ â†’ âˆ. However, if (ğ‘¥ âˆ—âˆ—âˆ— âˆ’ ğ‘¥ âˆ—)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ—âˆ—âˆ— âˆ’ ğ‘¥ âˆ—) = 0 for sufficiently large ğ›¾, then we have that ğ‘¥ âˆ—âˆ—âˆ— = ğ‘¥ âˆ—, and hence
ğ‘¥ âˆ— âˆˆ ğ‘ƒ2 \ ğ‘ƒ1, which contradicts the easily verified fact that (ğ‘¥ âˆ—âˆ— âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ—âˆ— âˆ’ ğœ‡) > (ğ‘¥ âˆ— âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ— âˆ’ ğœ‡). Therefore,
there exists a constant 0 < ğœ€ < 1 such that {ğ‘¥ : (ğ‘¥ âˆ’ ğ‘¥ âˆ—)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğ‘¥ âˆ—) â‰¤ ğœ€2} âˆ© ğ‘ƒ1 = {ğ‘¥ : (ğ‘¥ âˆ’ ğ‘¥ âˆ—)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğ‘¥ âˆ—) â‰¤ ğœ€2} âˆ© ğ‘ƒ2
for sufficiently large ğ›¾. Correspondingly, there exists ğœ€ â€² > 0 such that {ğ‘¥ : âˆ¥ğ‘¥ âˆ¥âˆ â‰¤ ğœ€ â€²} âŠ† {ğ‘¥ : ğ‘¥ğ‘‡ Î£âˆ’1ğ‘¥ â‰¤ ğœ€2}.

Still we define ğ‘Œ = ğµ(ğ‘‹ âˆ’ ğœ‡) âˆ¼ ğ‘ (0, ËœÎ£). Then we get that

ğ‘ƒ (ğ‘‹ âˆˆ ğ‘ƒ1) â‰¥ ğ‘ƒ ((ğ‘‹ âˆ’ ğ‘¥ âˆ—)ğ‘‡ Î£âˆ’1 (ğ‘‹ âˆ’ ğ‘¥ âˆ—) â‰¤ ğœ€2, ğ‘‹ âˆˆ ğ‘ƒ1)
= ğ‘ƒ ((ğ‘‹ âˆ’ ğ‘¥ âˆ—)ğ‘‡ Î£âˆ’1 (ğ‘‹ âˆ’ ğ‘¥ âˆ—) â‰¤ ğœ€2, ğ‘‹ âˆˆ ğ‘ƒ2)
= ğ‘ƒ ((ğ‘Œ + ğµğœ‡ âˆ’ ğµğ‘¥ âˆ—)ğ‘‡ ËœÎ£âˆ’1 (ğ‘Œ + ğµğœ‡ âˆ’ ğµğ‘¥ âˆ—) â‰¤ ğœ€2, ğ‘Œ â‰¥ Ëœğ‘ ).

Similar to the proof of Lemma 7.1, we have that

ğ‘ƒ (ğ‘‹ âˆˆ ğ‘ƒ1)
âˆ«

â‰¥

(ğ‘¦+ğµğœ‡âˆ’ğµğ‘¥ âˆ—)ğ‘‡ ËœÎ£âˆ’1 (ğ‘¦+ğµğœ‡âˆ’ğµğ‘¥ âˆ—) â‰¤ğœ€2,ğ‘¦ â‰¥ Ëœğ‘ 
(cid:33)âˆ’1

âˆ«

(cid:32)

=

(cid:214)

ğ‘– ( ËœÎ£ğ¼ ğ¼ )âˆ’1 Ëœğ‘ ğ¼
ğ‘’ğ‘‡

ğœ™ (ğ‘¦)dğ‘¦

ğ‘– âˆˆğ¼

â‰¥(2ğœ‹)âˆ’ ğ‘‘

2 | ËœÎ£|âˆ’ 1

2

(cid:32)

(cid:214)

ğ‘– âˆˆğ¼

ğœ™ ( Ëœğ‘¥ + ğ‘¦âˆ—)dğ‘¥

Ëœğ‘¥ğ‘‡ ËœÎ£âˆ’1 Ëœğ‘¥ â‰¤ğœ€2, Ëœğ‘¥ â‰¥ Ëœğ‘ âˆ’ğ‘¦âˆ—
(cid:33)âˆ’1

ğ‘– ( ËœÎ£ğ¼ ğ¼ )âˆ’1 Ëœğ‘ ğ¼
ğ‘’ğ‘‡

exp{âˆ’(ğ‘¦âˆ—)ğ‘‡ ËœÎ£âˆ’1ğ‘¦âˆ—/2}(1 âˆ’ ğœ€2/2)

âˆ«

0â‰¤ Ëœğ‘¥ â‰¤ğœ€â€²1

exp{âˆ’ğ‘¥ğ‘‡

ğ¼ 1ğ¼ }dğ‘¥

(cid:32)

2 | ËœÎ£|âˆ’ 1

=(2ğœ‹)âˆ’ ğ‘‘

2 (1 âˆ’ ğœ€2/2)ğœ€ â€²|ğ½ |

1 âˆ’ exp{âˆ’ğ‘’ğ‘‡
ğ‘– ( ËœÎ£ğ¼ ğ¼ )âˆ’1 Ëœğ‘ ğ¼
ğ‘’ğ‘‡
Combining the upper and lower bound for ğ‘ƒ (ğ‘‹ âˆˆ ğ‘ƒ1), we finally get that âˆ’ log ğ‘ƒ (ğ‘‹ âˆˆ ğ‘ƒ1) âˆ¼ (ğ‘¥ âˆ— âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ— âˆ’ ğœ‡)/2
as ğ›¾ â†’ âˆ. We apply this result to ËœRğ‘–, ğ‘– = 1, . . . , ğ‘  to get that âˆ’ log ğ‘ƒ (ğ‘‹ âˆˆ ËœRğ‘– ) âˆ¼ ( Ëœğ‘ğ‘– âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 ( Ëœğ‘ğ‘– âˆ’ ğœ‡)/2, which implies

exp{âˆ’(ğ‘¦âˆ—)ğ‘‡ ËœÎ£âˆ’1ğ‘¦âˆ—/2}.

ğ‘– ( ËœÎ£ğ¼ ğ¼ )âˆ’1 Ëœğ‘ ğ¼ ğœ€ â€²}

(cid:214)

ğ‘– âˆˆğ¼

(cid:33)

Manuscript submitted to ACM

Rare-Event Simulation for Neural Network and Random Forest Predictors

21

that

âˆ’ log ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾) = âˆ’ log

Moreover, since

(cid:33)

ğ‘ƒ (ğ‘‹ âˆˆ ËœRğ‘– )

(cid:32) ğ‘ 
âˆ‘ï¸

ğ‘–=1

âˆ¼ min
ğ‘–=1,...,ğ‘ 

{( Ëœğ‘ğ‘– âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 ( Ëœğ‘ğ‘– âˆ’ ğœ‡)}/2 = (ğ‘1 âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘1 âˆ’ ğœ‡)/2.

(12)

ğ¿(ğ‘¥) â‰¤

ğ‘Ÿğ‘’âˆ’(ğ‘¥âˆ’ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥âˆ’ğœ‡)/2
ğ‘’âˆ’(ğ‘¥âˆ’ğ‘ğ‘– )ğ‘‡ Î£âˆ’1 (ğ‘¥âˆ’ğ‘ğ‘– )/2 = ğ‘Ÿğ‘’âˆ’(ğ‘ğ‘– âˆ’ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘ğ‘– âˆ’ğœ‡)/2âˆ’(ğ‘ğ‘– âˆ’ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥âˆ’ğ‘ğ‘– )

and (ğ‘ğ‘– âˆ’ ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘¥ âˆ’ ğ‘ğ‘– ) â‰¥ 0 on Rğ‘– , we get that

ğ¸ [ğ¼ (ğ‘‹ âˆˆ Rğ‘– )ğ¿(ğ‘‹ )] â‰¤ ğ‘Ÿğ‘’âˆ’(ğ‘ğ‘– âˆ’ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘ğ‘– âˆ’ğœ‡)/2ğ‘ƒ (ğ‘‹ âˆˆ Rğ‘– ) â‰¤ ğ‘Ÿğ‘’âˆ’(ğ‘1âˆ’ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘1âˆ’ğœ‡)/2ğ‘ƒ (ğ‘‹ âˆˆ Rğ‘– ),

and hence Ëœğ¸ [ğ‘ 2] â‰¤ ğ‘Ÿğ‘’âˆ’(ğ‘1âˆ’ğœ‡)ğ‘‡ Î£âˆ’1 (ğ‘1âˆ’ğœ‡)/2ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾). Combining the inequality with the asymptotic result for
â–¡
ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾), we can easily get that the IS estimator ğ‘ is asymptotically optimal.

Proof of Corollary 4.3. See (12) in the proof of Theorem 4.2.

Proof of Corollary 4.4. Now we suppose that ğ‘‹ âˆ¼ (cid:205)ğ‘š

ğ‘—=1 ğœ‹ ğ‘— ğ‘ (ğœ‡ ğ‘— , Î£ ğ‘— ). We know that

ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾) =

ğ‘š
âˆ‘ï¸

ğ‘—=1

ğœ‹ ğ‘— ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾ |ğ‘‹ âˆ¼ ğ‘ (ğœ‡ ğ‘— , Î£ ğ‘— ))

âˆ’ log ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾) âˆ¼ min

ğ‘—=1,...,ğ‘š

{(ğ‘ ğ‘—1 âˆ’ ğœ‡ ğ‘— )ğ‘‡ Î£âˆ’1
ğ‘—

(ğ‘ ğ‘—1 âˆ’ ğœ‡ ğ‘— )/2}.

and thus

Moreover, we have that

ğ‘— (ğ‘¥âˆ’ğœ‡ ğ‘— )/2

ğ‘’âˆ’(ğ‘¥âˆ’ğœ‡ ğ‘— )ğ‘‡ Î£âˆ’1
ğ‘–=1 1/ğ‘Ÿ ğ‘— ğ‘’âˆ’(ğ‘¥âˆ’ğ‘ ğ‘—ğ‘– )ğ‘‡ Î£âˆ’1
(cid:205)ğ‘Ÿ ğ‘—

ğ‘— (ğ‘¥âˆ’ğ‘ ğ‘—ğ‘– )/2

â‰¤ ğ‘Ÿ ğ‘— ğ‘’âˆ’(ğ‘ ğ‘— 1âˆ’ğœ‡ ğ‘— )ğ‘‡ Î£âˆ’1

ğ‘— (ğ‘ ğ‘— 1âˆ’ğœ‡ ğ‘— )/2

â‰¤ max
ğ‘—

{ğ‘Ÿ ğ‘— }ğ‘’âˆ’ minğ‘— { (ğ‘ ğ‘— 1âˆ’ğœ‡ ğ‘— )ğ‘‡ Î£âˆ’1

ğ‘— (ğ‘ ğ‘— 1âˆ’ğœ‡ ğ‘— )/2}

and hence

Therefore, we get that

ğ¿(ğ‘¥) â‰¤ max

ğ‘—

{ğ‘Ÿ ğ‘— }ğ‘’âˆ’ minğ‘— { (ğ‘ ğ‘— 1âˆ’ğœ‡ ğ‘— )ğ‘‡ Î£âˆ’1

ğ‘— (ğ‘ ğ‘— 1âˆ’ğœ‡ ğ‘— )/2}.

Ëœğ¸ [ğ‘ 2]
Ëœğ¸ [ğ‘ ]2 =

ğ¸ [ğ¼ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾)ğ¿(ğ‘‹ )]
ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾)2

â‰¤

maxğ‘— {ğ‘Ÿ ğ‘— }ğ‘’âˆ’ minğ‘— { (ğ‘ ğ‘— 1âˆ’ğœ‡ ğ‘— )ğ‘‡ Î£âˆ’1

ğ‘— (ğ‘ ğ‘— 1âˆ’ğœ‡ ğ‘— )/2}

ğ‘ƒ (ğ‘”(ğ‘‹ ) â‰¥ ğ›¾)

grows polynomially in ğ›¾ and the IS estimator ğ‘ is asymptotically optimal.

â–¡

â–¡

ACKNOWLEDGEMENTS

We gratefully acknowledge support from the National Science Foundation under grants CAREER CMMI-1653339/1834710,

IIS-1849280, IIS-1849304, and the Manufacturing Futures Initiative at Carnegie Mellon University. A preliminary confer-

ence version of this work has appeared in [59].

REFERENCES

[1] T. P. I. Ahamed, V. S. Borkar, and S. Juneja. Adaptive importance sampling technique for Markov chains using stochastic approximation. Operations

Research, 54:489â€“504, 2006.

[2] Dohyun Ahn and Kyoung-Kuk Kim. Efficient simulation for expectations over the union of half-spaces. ACM Transactions on Modeling and

Computer Simulation (TOMACS), 28(3):1â€“20, 2018.

Manuscript submitted to ACM

22

Bai, Huang, Lam & Zhao

[3] S. Asmussen. Conjugate processes and the simulation of ruin problems. Stochastic Processes and their Applications, 20:213â€“229, 1985.
[4] S. Asmussen and K. Binswanger. Simulation of ruin probabilities for subexponential claims. Astin Bulletin, 27:297â€“318, 1997.
[5] S. Asmussen and D. Kroese. Improved algorithms for rare event simulation with heavy tails. Advances in Applied Probability, 38:545â€“558, 2006.
[6] SÃ¸ren Asmussen and HansjÃ¶rg Albrecher. Ruin probabilities, volume 14. World scientific, 2010.
[7] SÃ¸ren Asmussen and Peter W Glynn. Stochastic Simulation: Algorithms and Analysis, volume 57. Springer Science & Business Media, New York,

2007.

[8] Arthur Asuncion and David Newman. Uci machine learning repository, 2007.
[9] M. Bayati, J. Kim, and A. Saberi. A sequential algorithm for generating random graphs. Approximation, Randomization and combinatorial

Optimization. Algorithms and Techniques. Lecture Notes in Computer Science, 4627:326â€“340, 2007.

[10] J. Blanchet. Efficient importance sampling for binary contingency tables. Annals of Applied Probability, 19:949â€“982, 2009.
[11] J. Blanchet, P. Glynn, and H. Lam. Rare event simulation for a slotted time ğ‘€/ğº/ğ‘  model. Queueing Systems, 63:33â€“57, 2009.
[12] J. Blanchet and M. Mandjes. Rare event simulation for queues. In Rare Event Simulation Using Monte Carlo Methods, pages 87â€“124. 2009. Chapter 5.
[13] Jose Blanchet and Peter Glynn. Efficient rare-event simulation for the maximum of heavy-tailed random walks. The Annals of Applied Probability,

pages 1351â€“1378, 2008.

[14] Jose Blanchet and Henry Lam. State-dependent importance sampling for rare-event simulation: An overview and recent advances. Surveys in

Operations Research and Management Science, 17(1):38â€“59, 2012.

[15] Jose Blanchet and Henry Lam. Rare-event simulation for many-server queues. Mathematics of Operations Research, 39(4):1142â€“1178, 2014.
[16] Jose Blanchet, Henry Lam, and Bert Zwart. Efficient rare-event simulation for perpetuities. Stochastic Processes and their Applications, 122(10):3361â€“

3392, 2012.

[17] Jose H Blanchet and Jingchen Liu. State-dependent importance sampling for regularly varying random walks. Advances in Applied Probability,

40(4):1104â€“1128, 2008.

[18] National Transpotation Safety Board. Preliminary report, highway, hwy18mh010, 2018.
[19] National Transpotation Safety Board. Collision between car operating with partial driving automation and truck-tractor semitrailer delray beach,

florida, march 1, 2019, 2019.

[20] RK Bock, A Chilingarian, M Gaug, F Hakl, Th Hengstebeck, M JiÅ™ina, J Klaschka, E KotrÄ, P Savick`y, S Towers, et al. Methods for multidimensional
event classification: a case study using images from a cherenkov gamma-ray telescope. Nuclear Instruments and Methods in Physics Research Section
A: Accelerators, Spectrometers, Detectors and Associated Equipment, 516(2-3):511â€“528, 2004.

[21] P. T. De Boer, V.F. Nicola, and R.Y. Rubinstein. Adaptive importance sampling simulation of queueing networks. In Proceedings of 2000 Winter

Simulation Conference, pages 646â€“655. IEEE Press, 2000.

[22] V. S. Borkar, S. Juneja, and A. A. Kherani. Performance analysis conditioned on rare events: An adaptive simulation scheme. Communications in

Information, 3:259â€“278, 2004.

[23] Zdravko I Botev and Pierre Lâ€™Ecuyer. Sampling conditionally on a rare event via generalized splitting. INFORMS Journal on Computing, 2020.
[24] Zdravko I Botev, Pierre Lâ€™Ecuyer, and Bruno Tuffin. Markov chain importance sampling with applications to rare event probability estimation.

Statistics and Computing, 23(2):271â€“285, 2013.

[25] James Bucklew. Introduction to Rare Event Simulation. Springer Science & Business Media, New York, 2013.
[26] Joshua CC Chan and Dirk P Kroese. Improved cross-entropy method for estimation. Statistics and Computing, 22(5):1031â€“1040, 2012.
[27] Bohan Chen, Jose Blanchet, Chang-Han Rhee, and Bert Zwart. Efficient rare-event simulation for multiple jump events in regularly varying

random walks and compound poisson processes. Mathematics of Operations Research, 44(3):919â€“942, 2019.

[28] Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. Deepdriving: Learning affordance for direct perception in autonomous driving. In

Proceedings of the IEEE International Conference on Computer Vision, pages 2722â€“2730, 2015.

[29] Zhilu Chen and Xinming Huang. End-to-end learning for lane keeping of self-driving cars. In 2017 IEEE Intelligent Vehicles Symposium (IV), pages

1856â€“1860. IEEE, 2017.

[30] Leon O Chua and Lin Yang. Cellular neural networks: Theory. IEEE Transactions on circuits and systems, 35(10):1257â€“1272, 1988.
[31] Jeffrey F Collamore. Importance sampling techniques for the multidimensional ruin problem for general markov additive sequences of random

vectors. The Annals of Applied Probability, 12(1):382â€“421, 2002.

[32] P. T. de Boer, D. Kroese, S. Mannor, and R. Rubinstein. A tutorial on the cross-entropy method. Annals of Operations Research, 134:19â€“67, 2005.
[33] Thomas Dean and Paul Dupuis. Splitting for rare event simulation: A large deviation approach to design and analysis. Stochastic Processes and their

Applications, 119(2):562 â€“ 587, 2009.

[34] P. Y. Desai and P. W. Glynn. A Markov chain perspective on adaptive Monte Carlo algorithms. Proceedings of 2001 Winter Simulation Conference,

9:391â€“412, 2001.

[35] AB Dieker and Michel Mandjes. On asymptotically efficient simulation of large deviation probabilities. Advances in applied probability, 37(2):539â€“552,

2005.

[36] Tommaso Dreossi, Alexandre DonzÃ©, and Sanjit A Seshia. Compositional falsification of cyber-physical systems with machine learning components.

Journal of Automated Reasoning, 63(4):1031â€“1053, 2019.

[37] Paul Dupuis, Kevin Leder, and Hui Wang. Importance sampling for sums of random variables with regularly varying tails. ACM Transactions on

Modeling and Computer Simulation (TOMACS), 17(3):14â€“es, 2007.

Manuscript submitted to ACM

Rare-Event Simulation for Neural Network and Random Forest Predictors

23

[38] Paul Dupuis, Kevin Leder, and Hui Wang. Importance sampling for weighted-serve-the-longest-queue. Mathematics of Operations Research,

34(3):642â€“660, 2009.

[39] Paul Dupuis, Konstantinos Spiliopoulos, and Hui Wang. Importance sampling for multiscale diffusions. Multiscale Modeling & Simulation, 10(1):1â€“27,

2012.

[40] Jakub DvoÅ™Ã¡k and Petr Savick`y. Softening splits in decision trees using simulated annealing. In International Conference on Adaptive and Natural

Computing Algorithms, pages 721â€“729. Springer, 2007.

[41] Laura Fraade-Blanar, Marjory S Blumenthal, James M Anderson, and Nidhi Kalra. Measuring automated vehicle safety: forging a framework. RAND

Corporation, 2018.

[42] Roy Glasius, Andrzej Komoda, and Stan CAM Gielen. Neural network dynamics for path planning and obstacle avoidance. Neural Networks,

8(1):125â€“133, 1995.

[43] P. Glasserman. Monte Carlo Methods in Financial Engineering. Springer, 2004.
[44] P. Glasserman, P. Heidelberger, P. Shahabuddin, and T. Zajic. Multilevel splitting for estimating rare event probabilities. Operations Research,

47:585â€“600, 1999.

[45] Paul Glasserman. Monte Carlo Methods in Financial Engineering, volume 53. Springer Science & Business Media, New York, 2013.
[46] Paul Glasserman, Philip Heidelberger, Perwez Shahabuddin, and Tim Zajic. A large deviations perspective on the efficiency of multilevel splitting.

IEEE Transactions on Automatic Control, 43(12):1666â€“1679, 1998.

[47] Paul Glasserman, Wanmo Kang, and Perwez Shahabuddin. Fast simulation of multifactor portfolio credit risk. Operations Research, 56(5):1200â€“1217,

2008.

[48] Paul Glasserman and Jingyi Li. Importance sampling for portfolio credit risk. Management Science, 51(11):1643â€“1656, 2005.
[49] Paul Glasserman and Yashan Wang. Counterexamples in importance sampling for large deviations probabilities. The Annals of Applied Probability,

7(3):731â€“746, 1997.

[50] Peter W Glynn and Donald L Iglehart. Importance sampling for stochastic simulations. Management Science, 35(11):1367â€“1392, 1989.
[51] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep Learning, volume 1. MIT press Cambridge, Massachusetts, 2016.
[52] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.
[53] Adam W Grace, Dirk P Kroese, and Werner Sandmann. Automated state-dependent importance sampling for markov jump processes via sampling

from the zero-variance distribution. Journal of Applied Probability, 51(3):741â€“755, 2014.

[54] P. Grassberger. Go with the winners: A general Monte Carlo strategy. Computer Physics Communications, 147:64â€“70, 2002.
[55] Enkelejd Hashorva and Juerg Huesler. On multivariate gaussian tails. Annals of the Institute of Statistical Mathematics, 55:507â€“522, 02 2003.
[56] P. Heidelberger. Fast simulation of rare events in queueing and reliability models. ACM Transactions on Modeling and Computer Simulation

(TOMACS), 5:43â€“85, 1995.

[57] Harsha Honnappa, Raghu Pasupathy, and Prateek Jaiswal. Dominating points of gaussian extremes, 2018.
[58] Z. Huang, H. Lam, D. J. LeBlanc, and D. Zhao. Accelerated evaluation of automated vehicles using piecewise mixture models. IEEE Transactions on

Intelligent Transportation Systems, pages 1â€“11, 2017.

[59] Zhiyuan Huang, Henry Lam, and Ding Zhao. Designing importance samplers to simulate machine learning predictors via optimization. In 2018

Winter Simulation Conference (WSC), pages 1730â€“1741. IEEE, 2018.

[60] Zhiyuan Huang, Henry Lam, and Ding Zhao. Rare-event simulation without structural information: a learning-based approach. In 2018 Winter

Simulation Conference (WSC), pages 1826â€“1837. IEEE, 2018.

[61] Henrik Hult and Jens Svensson. On importance sampling with mixtures for random walks with heavy tails. ACM Transactions on Modeling and

Computer Simulation (TOMACS), 22(2):1â€“21, 2012.

[62] S. Juneja and P. Shahabuddin. Chapter 11 rare-event simulation techniques: An introduction and recent advances. In Shane G. Henderson and
Barry L. Nelson, editors, Simulation, volume 13 of Handbooks in Operations Research and Management Science, pages 291 â€“ 350. Elsevier, 2006.
[63] Sandeep Juneja and Perwez Shahabuddin. Rare-event simulation techniques: An introduction and recent advances. Handbooks in Operations

Research and Management Science, 13:291â€“350, 2006.

[64] Nidhi Kalra and Susan M Paddock. Driving to safety: How many miles of driving would it take to demonstrate autonomous vehicle reliability?

Transportation Research Part A: Policy and Practice, 94:182â€“193, 2016.

[65] G. Kesidis, J. Walrand, and C.-S. Chang. Effective bandwidths for multiclass Markov fluids and other ATM sources. IEEE/ACM Transactions on

Networks., 1:424â€“428, 1993.

[66] Mykel J Kochenderfer, Jessica E Holland, and James P Chryssanthacopoulos. Next-generation airborne collision avoidance system. Technical

report, Massachusetts Institute of Technology-Lincoln Laboratory Lexington United States, 2012.

[67] C. Kollman, K. Baggerly, D. Cox, and R. Picard. Adaptive importance sampling on discrete Markov chains. Annals of Applied Probability, 9:391â€“412,

1999.

[68] Philip Koopman and Michael Wagner. Autonomous vehicle safety: An interdisciplinary challenge. IEEE Intelligent Transportation Systems Magazine,

9(1):90â€“96, 2017.

[69] D. P. Kroese and V. F. Nicola. Efficient estimation of overflow probabilities in queues with breakdowns. Performance Evaluation, 36-37:471â€“484,

1999.

[70] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.

Manuscript submitted to ACM

24

Bai, Huang, Lam & Zhao

[71] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236, 2016.
[72] P. Lâ€™Ecuyer, F. Le Gland, P. Lezaud, and B. Tuffin. Splitting techniques. In Rare Event Simulation Using Monte Carlo Methods, pages 39â€“62. 2009.

Chapter 3.

[73] Pierre Lâ€™Ecuyer, Jose H. Blanchet, Bruno Tuffin, and Peter W. Glynn. Asymptotic robustness of estimators in rare-event simulation. ACM Trans.

Model. Comput. Simul., 20(1), February 2010.

[74] Velibor V MiÅ¡ic. Optimization of tree ensembles. Working Paper: arXiv preprint arXiv:1705.10883, 2017.
[75] Urs Muller, Jan Ben, Eric Cosatto, Beat Flepp, and Yann L Cun. Off-road obstacle avoidance through end-to-end learning. In Advances in neural

information processing systems, pages 739â€“746, 2006.

[76] Karthyek RA Murthy, Sandeep Juneja, and Jose Blanchet. State-independent importance sampling for random walks with regularly varying

increments. Stochastic Systems, 4(2):321â€“374, 2015.

[77] Victor F Nicola, Marvin K Nakayama, Philip Heidelberger, and Ambuj Goyal. Fast simulation of highly dependable systems with general failure

and repair processes. IEEE Transactions on Computers, 42(12):1440â€“1452, 1993.

[78] Victor F Nicola, Perwez Shahabuddin, and Marvin K Nakayama. Techniques for fast simulation of models of highly dependable systems. IEEE

Transactions on Reliability, 50(3):246â€“264, 2001.

[79] Matthew Oâ€™Kelly, Aman Sinha, Hongseok Namkoong, Russ Tedrake, and John C Duchi. Scalable end-to-end autonomous vehicle testing via

rare-event simulation. In Advances in Neural Information Processing Systems, pages 9827â€“9838, 2018.

[80] Art B Owen, Yury Maximov, Michael Chertkov, et al. Importance sampling the union of rare events with an application to power systems analysis.

Electronic Journal of Statistics, 13(1):231â€“254, 2019.

[81] P. Glasserman, P. Heidelberger and P. Shahabuddin and T. Zajic. A large deviations perspective on the effiency of multilevel splitting. IEEE

Transactions on Automated Control, pages 1666â€“1679, 1998.

[82] S. Parekh and J. Walrand. Quick simulation of rare events in networks. IEEE Transactions on Automatic Control, 34:54â€“66, 1989.
[83] R. Y. Rubinstein. Rare-event simulation via cross-entropy and importance sampling. Second Workshop on Rare Event Simulation, RESIMâ€™99, pages

1â€“17, 1999.

[84] A. Ridder. Importance sampling algorithms for first passage time probabilities in the infinite server queue. European Journal of Operational Research,

199:176â€“186, 2009.

[85] G. Rubino and B. Tuffin. Markovian models for dependability analysis. In Rare Event Simulation Using Monte Carlo Methods, pages 125â€“144. 2009.

Chapter 6.

[86] R. Rubinstein and D. Kroese. The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine

Learning. Springer-Verlag, 2004.

[87] R. Y. Rubinstein. Optimization of computer simulation models with rare events. European Journal of Operations Research, 99:89â€“112, 1997.
[88] Reuven Y Rubinstein and Dirk P Kroese. Simulation and the Monte Carlo Method, volume 10. John Wiley & Sons, New Jersey, 2016.
[89] J. S. Sadowsky. Large deviations theory and efficient simulation of excessive backlogs in a ğºğ¼ /ğºğ¼ /ğ‘š queue. IEEE Transactions on Automatic

Control, 36:1383â€“1394, 1991.

[90] John S Sadowsky and James A Bucklew. On large deviations theory and asymptotically efficient monte carlo estimation. IEEE transactions on

Information Theory, 36(3):579â€“588, 1990.

[91] W. Sandmann. Rare event simulation methodologies in systems biology. In Rare Event Simulation Using Monte Carlo Methods, pages 243â€“266. 2009.

Chapter 11.

[92] Petr Savick`y and Emil Kotrc. Experimental study of leaf confidences for random forest. In Proceedings of the 16th Symposium on Computational

Statistics, pages 1767â€“1774. Prague, Czech Republic, 2004.

[93] David Siegmund. Importance sampling in the monte carlo study of sequential tests. The Annals of Statistics, pages 673â€“684, 1976.
[94] Nathan A Spielberg, Matthew Brown, Nitin R Kapania, John C Kegelman, and J Christian Gerdes. Neural network vehicle models for high-

performance automated driving. Science Robotics, 4(28), 2019.

[95] R. Szechtman and P. Glynn. Rare event simulation for infinite server queues. In Proceedings of the 2002 Winter Simulation Conference, pages

416â€“423, 2002.

[96] Vincent Tjeng and Russ Tedrake. Verifying neural networks with mixed integer programming. Working Paper: arXiv preprint arXiv:1711.07356,

2017.

[97] B. Tuffin. On numerical problems in simulation of highly reliable Markovian systems. In Proceedings of the 1st International Conference on

Quantitative Evaluation of SysTems (QEST), pages 156â€“164. IEEE Computer Society Press, 2004.

[98] Jonathan Uesato, Ananya Kumar, Csaba Szepesvari, Tom Erez, Avraham Ruderman, Keith Anderson, Nicolas Heess, Pushmeet Kohli, et al. Rigorous

agent evaluation: An adversarial approach to uncover catastrophic failures. arXiv preprint arXiv:1812.01647, 2018.

[99] Jessica Van Brummelen, Marie Oâ€™Brien, Dominique Gruyer, and Homayoun Najjaran. Autonomous vehicle perception: The technology of today

and tomorrow. Transportation research part C: emerging technologies, 89:384â€“406, 2018.

[100] Eric Vanden-Eijnden and Jonathan Weare. Rare event simulation of small noise diffusions. Communications on Pure and Applied Mathematics,

65(12):1770â€“1803, 2012.

[101] Benjie Wang, Stefan Webb, and Tom Rainforth. Statistically robust neural network classification. arXiv preprint arXiv:1912.04884, 2019.

Manuscript submitted to ACM

Rare-Event Simulation for Neural Network and Random Forest Predictors

25

[102] Stefan Webb, Tom Rainforth, Yee Whye Teh, and M Pawan Kumar. A statistical approach to assessing neural network robustness. arXiv preprint

arXiv:1811.07209, 2018.

[103] Tsui-Wei Weng, Pin-Yu Chen, Lam M Nguyen, Mark S Squillante, Ivan Oseledets, and Luca Daniel. Proven: Certifying robustness of neural networks

with a probabilistic approach. arXiv preprint arXiv:1812.08329, 2018.

[104] Jianxin Wu, James M Rehg, and Matthew D Mullin. Learning a rare event detection cascade by direct feature selection. In Advances in Neural

Information Processing Systems, pages 1523â€“1530, 2004.

[105] Simon X Yang and Chaomin Luo. A neural network approach to complete coverage path planning. IEEE Transactions on Systems, Man, and

Cybernetics, Part B (Cybernetics), 34(1):718â€“724, 2004.

[106] Ding Zhao, Henry Lam, Huei Peng, Shan Bao, David J LeBlanc, Kazutoshi Nobukawa, and Christopher S Pan. Accelerated evaluation of automated
IEEE transactions on intelligent transportation systems,

vehicles safety in lane-change scenarios based on importance sampling techniques.
18(3):595â€“607, 2016.

Manuscript submitted to ACM

