1
2
0
2

v
o
N
9
2

]

Y
C
.
s
c
[

1
v
5
7
2
1
0
.
2
1
1
2
:
v
i
X
r
a

Advancing Artiﬁcial Intelligence and Machine Learning in the U.S. Government
Through Improved Public Competitions

Ezekiel J. Maier
Booz Allen Hamilton
4747 Bethesda Ave.
Bethesda, MD 20814

Abstract

In the last two years, the U.S. government has emphasized
the importance of accelerating artiﬁcial intelligence (AI) and
machine learning (ML) within the government and across the
nation. In particular, the National Artiﬁcial Intelligence Ini-
tiative Act of 2020, which became law on January 1, 2021,
provides for a coordinated program across the entire fed-
eral government to accelerate AI research and application.
The U.S. government can beneﬁt from public artiﬁcial intel-
ligence and machine learning challenges through the devel-
opment of novel algorithms and participation in experiential
training. Although the public, private, and non-proﬁt sectors
have a history of leveraging crowdsourcing initiatives to gen-
erate novel solutions to difﬁcult problems and engage stake-
holders, interest in public competitions has waned in recent
years as a result of at least three major factors: (1) a lack
of high-quality, high-impact data; (2) a narrow engagement
focus on specialized groups; and (3) insufﬁcient operational-
ization of challenge results. Herein we identify common is-
sues and recommend approaches to increase the effectiveness
of challenges. To address these barriers, enabling the use of
public competitions for accelerating AI and ML practice, the
U.S. government must leverage methods that protect sensi-
tive data while enabling modelling, enable easier participa-
tion, empower deployment of validated models, and incen-
tivize engagement from broad sections of the population.

Introduction
The White House, Congress, and Federal agencies recognize
the beneﬁts of artiﬁcial intelligence (AI) and machine learn-
ing (ML) and are accelerating AI and ML adoption. In addi-
tion to serving citizens more effectively, acceleration of AI
and ML research and application is critical for the economic
prosperity and national security of the U.S. (Schmidt et al.
2021). In particular, federal agencies are (1) modernizing in-
frastructure to support AI and ML development and opera-
tions; (2) adopting AI and ML solutions for improving and
automating business processes; and (3) offering training to
increase staff AI and ML awareness. For example, the U.S.
Food and Drug Administration (FDA), through its Technol-
ogy Modernization Action Plan (TMAP) and Data Modern-
ization Action Plan (DMAP), is upgrading FDA’s technical
infrastructure; building processes for innovative product de-
velopment (such as AI and ML models); developing con-
sistent, repeatable, and modern data management practices;

and developing data science talent within the workforce via
recruitment and retention activities, training, and knowledge
sharing (FDA 2019) (FDA 2021). While these actions are
advancing the use of AI and ML within the federal govern-
ment, these steps alone are not sufﬁcient to ensure that the
most beneﬁcial applications of AI and ML are prioritized
and staff are AI-ready.

Federal agencies often offer training on informatics, data
science, and data management via virtual self-paced on-
demand courses, webinars, tutorials and documentation, and
classroom-based learning (National Library of Medicine
2021). While this variety of approaches provides ﬂexibility
to learners, the lack of time constraints and instructor feed-
back may hinder course completion and reduce the ability of
learners to convert gained knowledge to action. Augment-
ing current training options with experiential learning tech-
niques will boost engagement and empower staff to seek and
utilize AI and ML solutions. Experiential learning is em-
ployed in a variety of learning settings (e.g., medical and
business school) to better engage students, enable collabo-
ration and creativity, and achieve a better real-world under-
standing of a topic. AI-focused public challenges can pro-
vide similar experiential learning beneﬁts, bridging the gap
between self-paced virtual training and utilizing AI and ML.
The public, private, and nonproﬁt sectors have all utilized
crowdsourcing to increase engagement, spur innovation, and
solve real-world problems. In fact, the U.S. government has
made a signiﬁcant commitment to engaging citizens in vol-
untary crowdsourcing activities through its Challenge.gov
platform which serves as a central source of government-
wide challenges and prize competitions. Challenge.gov links
to competitions on a variety of other federal government
platforms including the National Aeronautics and Space
Administration’s (NASA) Tournament Lab (Gustetic et al.
2015) and the FDA’s precisionFDA platform (Altman et al.
2016). Currently, Challenge.gov is hosting 29 active federal
government challenges. In addition, 849 challenges were
completed on Challenge.gov between 2010-2020. The De-
partment of Health and Human Services (HHS), NASA, and
Department of Defense have been crowdsourcing leaders,
each running more than 60 challenges since 2010. More-
over, more than 25 Challenge.gov hosted challenges have fo-
cused on AI and ML model development since 2018. In the
private and non-proﬁt sectors, notable crowdsourcing chal-

 
 
 
 
 
 
lenge platforms include Kaggle, InnoCentive, TopCoder,
and the DREAM Challenges. Technically complex compe-
titions, including AI, ML, and bioinformatics, are targeted
toward industry, research communities, and educational in-
stitutions where they strengthen collaboration, engage new
organizations and individuals, encourage innovation, supply
opportunities for hands-on training, increase openness and
availability of high-quality data sets and tools, and provide
independent evaluations of tools and techniques.

Despite this commitment from public, private, and non-
proﬁt sectors, and the signiﬁcant gains in educational attain-
ment and internet access, the interest and perceived effec-
tiveness of crowdsourcing competitions is decreasing. No-
tably, the global gross enrollment ratio in tertiary education
increased to 38% in 2017 (UNESCO 2019), and there has
been a 10% annual increase in worldwide Internet users,
topping out at more than 4.1 billion in 2019 (International
Telecommunication Union 2019). Within the United States,
83.7 million adults, aged 25 and over, have achieved a bach-
elor’s degree or higher as of 2020 (United States Census
Bureau 2021). In addition, Google Trends shows that world-
wide web search interest in the term ”machine learning” has
been at or near all-time highs since early 2019. Yet interest
in “crowdsourcing” has decreased. Google Trends shows a
decrease by more than 50% for the term “crowdsourcing”
since peaking in late 2013. Moreover, Citizen Data Science
is rated as entering the “Trough of Disillusionment” in the
2021 Gartner Hype Cycle for Machine Learning and Data
Science. This practice paper reports on a novel initial ex-
ploratory analysis of U.S. government hosted public chal-
lenges, and describes opportunities to reinvigorate competi-
tions by leveraging under utilized and unused approaches in
the crowdsourcing community.

Barriers to the Long-Term Success of
Crowdsourcing Competitions

Since 2010, the U.S. government has invested signiﬁcantly
in crowdsourcing efforts. In addition to the innumerable
person-hours spent organizing and running challenges, the
government has allocated more than $204 million dollars in
prizes for the 878 completed and active challenges hosted
on Challenge.gov. To increase the effectiveness of crowd-
sourcing competitions for advancing AI and ML literacy and
applications, U.S. government organized challenges must
better align with the expectations placed on them (Simula
2013). There are three major barriers that decrease the ef-
fectiveness of U.S. government AI and ML crowdsourcing
challenges: (1) a lack of high-quality, high-impact data, (2)
a narrow engagement focus on specialized groups, and (3)
insufﬁcient operationalization of challenge results.

Insufﬁcient high-quality publicly available high-impact
data.
In order to protect personally identiﬁable informa-
tion, deidentiﬁed or synthetic data often is used in place of
sensitive data. By using synthetic data instead of, for exam-
ple, electronic health records or human genomes, the result-
ing models may be less applicable to real-world problems,

and as such, may dissuade public engagement and discour-
age operationalization of developed AI and ML models.

Narrow engagement focus on specialized groups.
Ide-
ally, crowdsourcing competitions would leverage the ”wis-
dom of the crowd”. However, public challenges often are or-
ganized for, and advertised to, relatively small, specialized
groups, such as academic data scientists and bioinformati-
cians. While specialized knowledge is important, the diverse
thinking that results from engagement of a wider audience
can lead to new innovations and improved understanding of
AI and ML through experiential learning.

Insufﬁcient operationalization of challenge results. The
post-challenge phase is crucial for extracting knowledge
from the challenge results and validating, improving, and
operationalizing models. However, challenge sponsors, or-
ganizers, and participants often spend the majority of their
focus on the modeling phase. Without additional focus on
the post-challenge collaborative phase, challenges will not
provide the beneﬁts or have the impact that they are capable
of.

Approaches to Reinvigorate Crowdsourcing
Competitions
There are several approaches to evolve and increase the ef-
fectiveness of crowdsourcing challenges for advancing AI
and ML use in the U.S. Government. These approaches in-
clude: model-to-data, autoML, design-a-thons, MLOps, and
the introduction of novel incentives.

Model-to-data. Popularized by the DREAM Challenges,
model-to-data approaches can address data privacy concerns
by evaluating models in a secure private computational en-
vironment that holds the underlying sensitive data. In this
approach, participants develop and train their model on non-
sensitive data, then containerize and submit their model for
evaluation in the private computational environment (Ellrott
et al. 2019). A distributed model-to-data framework is being
used in the current COVID-19 EHR DREAM Challenge to
enable development and evaluation of models that use elec-
tronic health records (EHRs) to predict patient speciﬁc risk
for COVID-19 associated health outcomes. The U.S. gov-
ernment AI and ML challenge community should continue
to adopt model-to-data approaches, enabling challenges that
use high-quality, real, high-impact data and produce gener-
alizable models.

AutoML. Automated machine learning (AutoML) tools
automate many of the steps in the machine learning pipeline,
including feature engineering, model selection, model train-
ing, and model validation (Waring, Lindvall, and Ume-
ton 2020). There are a number of vendors (e.g., Ama-
zon Web Services (AWS), Google Cloud, Microsoft Azure,
DataRobot) and open source tools (e.g., H2O, R, Python)
that provide autoML tools. The U.S. government AI and

ML challenge community should leverage autoML to ex-
pand access to challenges and increase efﬁciency. For ex-
ample, beginner tracks of AI and ML challenges can be
hosted on user-friendly point-and-click interfaces that sim-
plify modeling-based decision making.

Design-a-thons. Similar to hack-a-thons, design-a-thons
engage a broad array of stakeholders to ideate possible so-
lutions to real-world problems. Importantly, design-a-thons
are welcoming to a broader audience by not requiring spe-
cialized subject matter or programming knowledge for par-
ticipation. For example, the precisionFDA platform recently
hosted the FDA New Era of Smarter Food Safety Low- or
No-Cost Tech-Enabled Traceability Challenge to promote
ideation and innovation of hardware, software, and advanced
analytics solutions for enabling digital traceability along the
entire food system. By requiring PowerPoint and video pre-
sentations, rather than an implemented solution, this chal-
lenge enabled broader participation, leading to more than
90 submissions. The U.S. government AI and ML challenge
community should leverage design-a-thons to engage em-
ployees and the public in the prioritization of AI and ML
use cases.

MLOps. Machine learning operations (MLOps) is a set
of machine learning and DevOps practices for develop-
ing, deploying, and maintaining machine learning solutions,
which includes model and data versioning, pipeline automa-
tion, testing, continuous integration and continuous deliv-
ery, and monitoring. The U.S. government AI and ML chal-
lenge community should adopt MLOps in the post-challenge
phase to ensure that community developed models can be
operationalized to beneﬁt the government and public by be-
ing testable, transparent, scalable, secure, and reproducible.
For example, the three top performing teams from the pre-
cisionFDA NCI-CPTAC Multi-omics Enabled Sample Mis-
labeling Correction Challenge participated in a collabora-
tive post-challenge phase with the challenge organizers to
(1) validate their computational methods for identifying and
correcting sample mislabeling on independent datasets and
(2) generate a single-best consensus pipeline. This consen-
sus approach, named COrrection of Sample Mislabeling by
Omics (COSMO), was developed and validated following
MLOps considerations including scalability, reproducibility,
and deployability via the use of Docker containerization and
Nextﬂow (Yoo et al. 2021).

Novel incentives. Novel incentives for top performance,
such as fast-tracking pilots, partnerships, and contracts, will
boost engagement while ensuring that clear steps are in place
to reward winning models. For example, Artiﬁcial Intelli-
gence Tech Sprints, organized by the National Artiﬁcial In-
telligence Institute (NAII), award both monetary prizes and
opportunities for partnership and piloting of selected proto-
types (National Artiﬁcial Intelligence Institute 2020). More-
over, while only 3.3% of the 432 challenges completed on
the Kaggle platform from 2010-2020 utilized jobs as an in-
centive, there is an 80% increase in the median number of

participating teams as compared to the 70% of Kaggle chal-
lenges that utilized monetary incentives.

Conclusions and Discussion
The U.S. government is leading a national initiative to ac-
celerate AI/ML research and development, and upskill the
workforce to enable AI/ML integrattion. Public crowdsourc-
ing challenges have long been used as a tool for innova-
tion and stakeholder engagement. For example, from 2006-
2009 Netﬂix ran the Netﬂix Prize public competition, which
offered a grand prize of $1,000,000 to the top perform-
ing team that could improve the prediction of user ratings
of ﬁlms by 10%. Through this public competition, Net-
ﬂix was able to directly engage with over 40,000 regis-
tered teams that participated in the challenge, encourage ad-
vancements in the ﬁeld of collaborative ﬁltering (Koren and
Bell 2015), and increase the public’s awareness of Netﬂix
and recommendation systems. Improved utilization of pub-
lic crowdsourcing competitions can provide the U.S. gov-
ernment with similar beneﬁts, including AI/ML innovation
and improved workforce AI-readiness. To achieve these ben-
eﬁts, U.S. government sponsored public challenges must
democratize challenge participation (e.g., through autoML,
design-a-thons, and novel incentives), enable validation on
real-world data (e.g., via model-to-data), and focus on op-
erationalizing high-performing validated models (e.g., using
MLOps).

In addition to the crowdsourcing barriers and improve-
ments discussed in this paper, more study is needed to
identify and quantify the factors that inﬂuence the suc-
cess of public competitions. More analysis of public chal-
lenges, such as those hosted by Challenge.gov and Kag-
gle, is needed to understand all predictive factors. To em-
power this analysis, the crowdsourcing community, includ-
ing the Federal Community of Practice on Crowdsourcinig
and Citizen Scienc (FedCCS), must improve the measure-
ment and documentation of challenge outcomes. Measure-
ment and documentation of outcomes, such as deployed
models, scientiﬁc publications, and educational attainment
will strengthen subsequent recommendations and ultimately
increase the impact of federal AI/ML public challenges.

References

Altman, R. B., Prabhu, S., Sidow, A., Zook, J. M.,
Goldfeder, R., Litwack, D., ... and Giacomini, K. M. 2016.
A research roadmap for next-generation sequencing infor-
matics. Science translational medicine 8(335): 335ps10-
335ps10.

Ellrott, K., Buchanan, A., Creason, A., Mason, M.,
Schaffter, T., Hoff, B., ... and Saez-Rodriguez, J. 2019. Re-
producible biomedical benchmarking in the cloud: lessons
from crowd-sourced data challenges. Genome biology 20(1):
1-9.

Food and Drug Administration. 2019. FDA’s Tech-
(TMAP). URL
nology Modernization Action Plan
https://www.fda.gov/media/130883/download
(accessed
10.31.21).

and

Drug

Food
Modernization
Action
https://www.fda.gov/media/143627/download
10.31.21).

Administration.
Plan

2021.
(DMAP).

Data
URL
(accessed

Gustetic, J. L., Crusan, J., Rader, S., and Ortega, S. 2015.
Outcome-driven open innovation at NASA. Space Policy,
34: 11-17.

International Telecommunication Union. 2019. Mea-
suring digital development: Facts and ﬁgures 2019.
ITU Publications. URL
https://www.itu.int/en/ITU-
(ac-
D/Statistics/Documents/facts/FactsFigures2019.pdf
cessed 9.25.20).

Koren, Y., Bell, R. 2015. Advances in collaborative ﬁltering.
Recommender systems handbook, 77-118.

National Artiﬁcial Intelligence Institute. 2020. Industry, aca-
demics invited to design AI tool to help Veterans. VAntage
Point. URL https://blogs.va.gov/VAntage/77660/industry-
academics-invited-design-ai-tool-help-veterans/
(accessed
9.25.20).

National Library of Medicine. 2021. Training on Biomed-
ical Informatics, Data Science, and Data Management.
URL
https://learn.nlm.nih.gov/documentation/training-
packets/T000181112/ (accessed 10.31.21).

Schmidt, E., Work, B., Catz, S., Chien, S., Darby, C., Ford,
K., Grifﬁths, J.M., Horvitz, E., Jassy, A., Mark, W. and Ma-
theny, J. 2021. National Security Commission on Artiﬁcial
Intelligence Final Report. National Security Commission on
Artiﬁcial Intelligence.

Simula, H. 2013. The rise and fall of crowdsourcing?. In
2013 46th Hawaii International Conference on System Sci-
ences, 2783-2791. IEEE.

URL

CPS
URL

2019.

UNESCO.
https://unesdoc.unesco.org/ark:/48223/pf0000370738
(accessed 9.25.20).

#CommitToEducation.

Time

States

Census

United
Historical
https://www.census.gov/library/visualizations/time-
series/demo/cps-historical-time-series.html
8.13.21).

2020.
Visualizations.

Bureau.

Series

(accessed

Waring, J., Lindvall, C., and Umeton, R. 2020. Automated
machine learning: Review of the state-of-the-art and oppor-
tunities for healthcare. Artiﬁcial Intelligence in Medicine
101822.

Yoo, S., Shi, Z., Wen, B., Kho, S., Pan, R., Feng, H., ... and
Zhang, B. 2021. A community effort to identify and correct
mislabeled samples in proteogenomic studies. Patterns 2(5):
100245.

