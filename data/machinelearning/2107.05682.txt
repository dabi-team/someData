2
2
0
2

p
e
S
6

]

G
L
.
s
c
[

2
v
2
8
6
5
0
.
7
0
1
2
:
v
i
X
r
a

Least-Squares Linear Dilation-Erosion Regressor
Trained using a Convex–Concave Procedure(cid:63)

Angelica Louren¸co Oliveira[0000−0002−8689−8522] and
Marcos Eduardo Valle[0000−0003−4026−5110]

University of Campinas, Campinas – S˜ao Paulo, Brazil.
ra211686@ime.unicamp.br and valle@ime.unicamp.br

Abstract. This paper presents a hybrid morphological neural network
for regression tasks called linear dilation-erosion regressor ((cid:96)-DER). An (cid:96)-
DER is given by a convex combination of the composition of linear and
morphological operators. They yield continuous piecewise linear func-
tions and, thus, are universal approximators. Besides introducing the
(cid:96)-DER model, we formulate their training as a diﬀerence of convex (DC)
programming problem. Precisely, an (cid:96)-DER is trained by minimizing the
least-squares using the convex-concave procedure (CCP). Computational
experiments using several regression tasks conﬁrm the eﬃcacy of the pro-
posed regressor, outperforming other hybrid morphological models and
state-of-the-art approaches such as the multilayer perceptron network
and the radial-basis support vector regressor.

Keywords: Morphological neural network · continuous piecewise linear
function · regression · DC optimization.

1

Introduction

Dilations and erosions are the elementary operations of mathematical morphol-
ogy, a non-linear theory widely used for image processing and analysis [11, 27].
In the middle 1990s, Ritter et al. proposed the ﬁrst morphological neural net-
works whose processing units, the morphological neurons, perform dilations and
erosions [22, 23]. Morphological neurons are obtained by replacing the usual dot
product with either the maximum or minimum of sums. Because of the maximum
and minimum operations, morphological neural networks are usually cheaper
than traditional models. However, training morphological neural networks are
often a big challenge because of the non-diﬀerentiability of the maximum and
minimum operations [21]. This paper addresses this issue by proposing a diﬀerent
method for training a hybrid morphological neural network for regression tasks.
Precisely, we focus on training the so-called linear dilation-erosion perceptron
using a diﬀerence of convex (DC) optimization method.

(cid:63) This work was supported in part by CNPq under grant no. 315820/2021-7, FAPESP
under grant no. 2022/01831-2, and Coordena¸c˜ao de Aperfei¸coamento de Pessoal de
N´ıvel Superior - Brasil (CAPES) - Finance Code 001.

 
 
 
 
 
 
2

A.L. Oliveira and M.E. Valle

A dilation-erosion perceptron (DEP) is a hybrid morphological neural net-
work obtained by a convex combination of dilations and erosions [3]. Despite
its application in regression tasks such as time-series prediction and software
development cost estimation [2,3], the DEP model has an inherent drawback: as
an increasing operator, it implicitly assumes an ordering relationship between
inputs and outputs [32]. Fortunately, one can circumvent this problem by adding
neurons that perform anti-dilations or anti-erosions [30]. For example, consider-
ing the importance of dendritic structures, Ritter and Urcid presented a single
morphological neuron that circumvents the limitations of the DEP model [24].

Alternatively, Valle proposed the reduced dilation-erosion perceptron (r-
DEP) using concepts from multi-valued mathematical morphology [32]. In a
few words, an r-DEP is obtained by composing an appropriate transformation
with the DEP model, i.e., the inputs are transformed before they are fed to the
DEP model. However, choosing the proper transformation is challenging in de-
signing an eﬃcient r-DEP model. As a solution, Oliveira and Valle proposed the
so-called linear dilation-erosion perceptron ((cid:96)-DEP) by considering linear map-
pings instead of arbitrary transformations [19]. Interestingly, the linear dilation-
erosion perceptron is equivalent to a maxout network with two hidden units [8].
The (cid:96)-DEP model is also closely related to one of two hybrid morphological
neural networks investigated by Hern´andez et al. for big data classiﬁcation [12].

From a mathematical point of view, the (cid:96)-DEP yields a continuous piecewise
linear function. Thus, like many traditional neural networks, they are universal
approximators; that is, an (cid:96)-DEP model can approximate a continuous func-
tion within any desired accuracy in a compact region in a Euclidean space [33].
Besides the models mentioned above, the morphological/linear perceptron [29]
and the hybrid multilayer morphological network [18] also exhibit universal ap-
proximation capability. However, all these hybrid morphological networks diﬀer
signiﬁcantly in the training rule.

Like traditional neural networks, maxout and hybrid multilayer morphologi-
cal networks can be trained using the stochastic gradient descent (SGD) method
[8, 18]. Despite the non-diﬀerentiability of morphological operators, Hen´andez
et al. also used the SDG method for training their hybrid morphological neu-
ral networks [12]. In contrast, Sussner and Campiotti circumvented the non-
diﬀerentiability of the morphological operators using an extreme learning ma-
chine approach to train the morphological/linear perceptron [29]. Apart from
the hybrid morphological/linear network literature, Ho et al. formulated the
learning of a continuous piecewise linear function as a diﬀerence of convex func-
tions (DC) programming problem [14]. Because the (cid:96)-DEP model can be identi-
ﬁed with continuous piecewise linear functions, it can also be trained using DC
programming. Indeed, this paper uses the concave-convex procedure (CCP) for
training the (cid:96)-DEP model for regression tasks. Interestingly, the CCP results
in more straightforward optimization problems than the approach of Ho et al..
Computational experiments with many regression tasks conﬁrm the advantage
of the proposed (cid:96)-DEP model against the hybrid morphological/linear methods
from the literature and the approach based on the DC optimization of Ho et al.

Least-Squares Linear Dilation-Erosion Regression

3

The paper is organized as follows. The following section reviews some basic
concepts regarding DC optimization, including deﬁnitions and the CCP opti-
mization technique. Section 3 presents the (cid:96)-DEP model for regression tasks
while the training based on CCP is addressed in Section 4. Computational ex-
periments using several regression problems from the literature are detailed in
Section 5. The paper ﬁnishes with some remarks in Section 6.

2 Diﬀerence of Convex Optimization and the

Convex-Concave Procedure

The diﬀerence of convex (DC) functions results non-convex functions that enjoy
interesting and useful properties [9, 31]. DC optimization aims to optimize such
kinds of functions. Applications of DC optimization programs include signal
processing, machine learning, computer vision, and statistics [16,26]. This section
presents some basic concepts of DC optimization. This section also addresses the
convex-concave procedure proposed by Yuille and Rangarajan for constrained
DC optimization problems [35]. The convex-concave procedure will be used in
Section 4 for training the linear dilation-erosion perceptron for regression tasks.

2.1 Basic Concepts of DC Optimization

Consider a real-valued function f : C → R deﬁned in a convex set C ⊆ Rn. We
say that f is a DC function if there are convex functions g, h : C → R such that

f (α) = g(α) − h(α),

∀α ∈ C.

(1)

Recall that a function is convex if the line joining two points on its graph is
below the graph [17]. The functions g and h are called the DC components of f
and the identity in (1) is is the DC decomposition of f .

In a DC optimization problem, the objective and constraints are DC func-
tions. In this paper, we focus on the following constrained DC optimization
problem:

(cid:40)minimize
α∈Rn
subject to

f0(α) = g0(α) − h0(α)

fi(α) = gi(α) − hi(α) ≤ 0,

i = 1, . . . , m,

(2)

where gi : Rn → R and hi : Rn → R are convex functions for all i = 0, 1, . . . , m.
In general terms, DC optimization methods take advantage of the convexity
of the DC components gi and hi of fi. For example, many methods approximate
the convex terms hi of the DC decomposition of fi an aﬃne function, resulting
in convex optimization subproblems that are solved more eﬀectively than the
original DC problem [14, 16, 31]. This paper uses the convex-concave procedure
(CCP) proposed by Yuille and Rangarajan [35] for solving DC optimization
problems given by (2).

4

A.L. Oliveira and M.E. Valle

Algorithm 1: CCP

: Convex functions: g0, . . . , gm and f0, . . . , fm.

Input
Output : α∗ (Solution)
Initialize: a feasible α0 ∈ Rn and k = 0.
repeat

Compute a subgradient βi of hi at αk, for all i = 0, . . . , m.
Solve the convex problem:



g0(α) − h0(αk) − (cid:104)β0, α − αk(cid:105)

gi(α) ≤ hi(αk) + (cid:104)βi, α − αk(cid:105),

i = 1, . . . , m.

αk+1 = argmin
α∈Rn
s.t.


k = k + 1
until converge;
return α∗ = αk

2.2 Convex-Concave Procedure

The convex-concave procedure (CCP), also called concave-convex procedure [35],
is a majorization-minimization methodology that uses convex optimization tools
to ﬁnd a local optimum for DC problems through of a sequence of convex sub-
problems. Brieﬂy, the CCP method solves a constrained DC problem sequentially
as follows: Given a feasible approximation αk ∈ Rn, the convex component
hi : Rn → R is approximated by

˜hi(α) = hi(αk) + (cid:104)βi, α − αk(cid:105),

∀i = 0, 1, . . . , m,

(3)

where βi is a subgradient of hi at αk. In particular, βi = ∇hi(αk) if the function
hi is diﬀerentiable at αk. Replacing hi by ˜hi results the convex optimization
problem

(cid:40)minimize
α∈Rn
subject to

g0(α) − h0(αk) − (cid:104)β0, α − αk(cid:105)

gi(α) − hi(αk) − (cid:104)βi, α − αk(cid:105) ≤ 0, i = 1, . . . .

(4)

The solution of (4) is a new feasible approximation αk+1, and the process is
repeated. The sequence {αk} obtained solving (4) recursively is convergent and
{f (αk)} is non-increasing. Further details on the convergence of the sequence
{αk}k≥0 can found at [16]. Algorithm 2 summarizes the CCP method.

3 Linear Dilation-Erosion Regressor

Predictive classiﬁcation models categorize information based on a set of historical
data. Predictive regression models are used to solve curve-ﬁtting problems whose
goal is to ﬁnd a function that best ﬁts a given data set. The adjusted mapping
can be used for forecasting or predictions outside the data set.

Recently, [19] introduced the linear dilation-erosion perceptron ((cid:96)-DEP) for
classiﬁcation tasks. A linear dilation-erosion perceptron is given by a convex com-
bination of the composition of linear transformations and two elementary opera-
tors from mathematical morphology [3,22]. Let us review the main concepts from

Least-Squares Linear Dilation-Erosion Regression

5

mathematical morphology and the (cid:96)-DEP model. We will subsequently present
the linear dilation-erosion regressor, the predictive model of the regression type
corresponding to the (cid:96)-DEP regressor.

Mathematical morphology is mainly concerned with non-linear operators de-
ﬁned on complete lattices [11, 27]. Complete lattices are partially ordered sets
with well-deﬁned supremum and inﬁmum operations [4]. Dilations and erosions
are the elementary operators from mathematical morphology. Given complete
lattices L and M, a dilation δ : L → M and an erosion ε : L → M are operators
such that

δ (sup X) = sup{δ(x) : x ∈ X}

and ε (inf X) = inf{ε(x) : x ∈ X},

(5)

for all X ⊆ L [11]. For example, consider the complete lattices L = ¯Rn and
M = ¯R, where ¯R = R ∪ {−∞, +∞}. Given vectors a, b ∈ Rn, the operators
δa, εb : ¯Rn → ¯R deﬁned by

δa(x) = max
j=1:n

{xj + aj}

and εb(x) = min
j=1:n

{xj − bj},

∀x ∈ ¯Rn,

(6)

are respectively a dilation and an erosion [30]. Note that dilations and erosions
given by (6) satisfy the duality identity εb(−x) = −δb(x), for all x ∈ ¯Rn.

A dilation-erosion perceptron (DEP) is given by a convex combination of a
dilation and an erosion deﬁned by (6). The reduced dilation-erosion perceptron
(r-DEP) proposed by Valle is an improved version of the DEP model obtained
using concepts from vector-valued mathematical morphology [32]. The (cid:96)-DEP
model is a particular but powerful r-DEP classiﬁer [19]. Formally, given a one-
to-one mapping σ from the set of binary class labels C to {+1, −1}, an (cid:96)-DEP
classiﬁer is deﬁned by the equation y = σ−1f τ (cid:96)(x), where f : R → {−1, +1} is
a threshold function and τ (cid:96) : Rn → R is the decision function given by

τ (cid:96)(x) = δa(W x) − δb(M x),

(7)

where W ∈ Rr1×n, M ∈ Rr2×n, a ∈ Rr1 , and b ∈ Rr2 . We would like to point
out that the erosion has been replaced by minus a dilation in (7) using the
duality identity. Equivalently, the decision function τ (cid:96) satisﬁes

τ (cid:96)(x) = max
i=1:r1

{(cid:104)wi, x(cid:105) + ai} − max
j=1:r2

{(cid:104)mj, x(cid:105) + bj},

(8)

where a = (a1, . . . , ar1) ∈ Rr1 and b = (b1, . . . , br2) ∈ Rr2 , and wi ∈ Rn and
mi ∈ Rn are rows of W ∈ Rr1×n and M ∈ Rr2×n, respectively. From the last
identity, we can identify τ (cid:96) with a piece-wise linear function [33]. Moreover, from
Theorem 4.3 in [8], the decision function τ (cid:96) is an universal approximator, i.e., it
is able to approximate any continuous-valued function from a compact set on Rn
to R [8, 28]. Consequently, an (cid:96)-DEP model can theoretically solve any binary
classiﬁcation problem. In addition, the decision function τ (cid:96) can be identiﬁed with
a maxout network with two hidden units [8].

Because the decision function of an (cid:96)-DEP model is a universal approximator,
τ (cid:96) given by (7) can also be used as a predictive model for regression tasks. In

6

A.L. Oliveira and M.E. Valle

other words, it is possible to use the mapping τ (cid:96) as the prediction function
that maps a set of independent variables in Rn to a dependent variable in R.
In this case, we refer to the operator τ (cid:96) : Rn → R given by (7) as a linear
dilation-erosion regressor ((cid:96)-DER). In this paper, the parameters (wT
i , ai) ∈
Rn+1 and (mT
j , bj) ∈ Rn+1, for i = 1, . . . , r1 and j = 1, . . . , r2, are determined
by minimizing the squares of the diﬀerence between the predicted and desired
values. The following section addresses an approach for training an (cid:96)-DER model.

4 Training the (cid:96)-DER using CCP

In this section, we present approaches for training an (cid:96)-DER model using a set
T = {(xi, yi) : i = 1 : m} ⊂ Rn × R, called training set. The goal is to ﬁnd the
parameters of an (cid:96)-DER model such that the estimate τ (cid:96)(xi) approaches the
desired output yi according to some loss function. Recall that the parameters
of an (cid:96)-DEP regressor are the matrices W ∈ Rr1×n and M ∈ Rr2×n as well as
the vectors a = (a1, . . . , ar1) ∈ Rr1 and b = (b1, . . . , br2 ) ∈ Rr2 . To simplify the
exposition, the parameters of an (cid:96)-DER are arranged in a vector

α = (w1, a1, ..., wr1, ar1 , m1, b1, ..., mr2, br2) ∈ R(r1+r2)(n+1),

(9)

where wi and mi denote rows of W ∈ Rr1×n and M ∈ Rr2×n, respectively.
During the training, an (cid:96)-DER is interpreted as a function of its parameters,
that is, τ (cid:96)(x) ≡ τ (cid:96)(x; α).

In this paper, the mean squared error (MSE) deﬁned as follows using the
training set T = {(xi, yi) : i = 1, . . . , m} ⊂ Rn × R is considered as the loss
function:

MSE(T , α) =

1
m

m
(cid:88)

i=1

(yi − τ (cid:96)(xi; α))2,

(10)

As a consequence, the parameters of the (cid:96)-DER are determined by solving the
optimization problem

minimize
α

1
m

m
(cid:88)

i=1

(yi − τ (cid:96)(xi; α))2.

(11)

4.1 Training Based on the Convex-Concave Programming

Inspired by the methodology developed by Charisopoulos and Maragos for train-
ing morphological perceptrons [6], we reformulate the unrestricted optimization
problem (11) as a constrained DC problem. Precisely, by setting ξi = yi − τ (cid:96)(xi),
the unrestricted problem (11) corresponds to






m
(cid:88)

1
minimize
m
W,a,M,b,ξ
subject to τ (cid:96)(xi) = yi − ξi,

ξ2
i

i=1

i = 1, ..., m.

(12)

Least-Squares Linear Dilation-Erosion Regression

7

In other words, the (cid:96)-DER can be trained by solving the following problem






minimize
W,a,M,b,ξ

1
m

m
(cid:88)

i=1

ξ2
i

subject to δa(W xi) + ξi = δb(M xi) + yi,

i = 1, ..., m.

(13)

Note that the objective function in (13) is a convex quadratic function. Moreover,
the functions at both sides of the equality constraints are convex. Thus, we can
view the constraints as DC functions, and the optimization problem (13) can be
solved using Algorithm 1 by dealing appropriately with the equality constraints.
We ﬁrst transform each of the equality constraints into two inequality con-

straints:

δa(W xi) + ξi ≤ δb(M xi) + yi

and δb(M xi) + yi ≤ δa(W xi) + ξi,

(14)

for all i = 1, . . . , m. Because a majorant of a set is also a majorant of each of its
elements and recalling that

δa(W xi) = max
j=1:r1

{(cid:104)wj, xi(cid:105) + aj}

and δb(M xi) = max
j=1:r2

{(cid:104)mj, xi(cid:105) + bj}, (15)

the two inequalities in (14) yield

(cid:104)wl, xi(cid:105) + al + ξi ≤ δb(M xi) + yi,
(cid:104)ml, xi(cid:105) + bl + yi ≤ δa(W xi) + ξi,

i ∈ I, l ∈ L1,
i ∈ I, l ∈ L2,

(16)

(17)

where I = {1, . . . , m}, L1 = {1, . . . , r1}, and L2 = {1, . . . , r2}. Thus, (13) can
be equivalently written as





minimize
W,a,M,b,ξ

1
m

m
(cid:88)

ξ2
i

i=1

subject to (cid:0)(cid:104)wl, xi(cid:105) + al + ξi − yi
(cid:0)(cid:104)ml, xi(cid:105) + bl − ξi + yi

(cid:1) − δb(M xi) ≤ 0, i ∈ I, l ∈ L1,
(cid:1) − δa(W xi) ≤ 0, i ∈ I, l ∈ L2.

(18)

Note that (18) can be identiﬁed with a DC optimization problem (2). Further-
more, the convex functions

hi1(α) = δb(M xi)

and hi2(α) = δa(M xi),

(19)

can be approximated by the aﬃne functions

˜hi1(α) = (cid:104)mji1 , xi(cid:105) + bj

and ˜hi2(α) = (cid:104)wji2 , xi(cid:105) + aj,

(20)

where

ji1 = arg max
j=1,...,r2

{(cid:104)mj, xi(cid:105) + bj}

and ji2 = arg max
j=1,...,r1

{(cid:104)wj, xi(cid:105) + aj},

(21)

for all i ∈ I.

8

A.L. Oliveira and M.E. Valle

Using the aﬃne approximations of the convex components hi1 and hi2 of the
constraints, we obtain the following quadratic problem which is solved at each
iteration of the CCP method used for training an (cid:96)-DER model:





minimize
W,a,M,b,ξ,p,q

subject to

1
m

m
(cid:88)

i=1

ξ2
i

(cid:104)wl − mji1, xi(cid:105) + al − bji1 ≤ yi − ξi, i ∈ I, l ∈ L1
(cid:104)ml − wji2, xi(cid:105) + bl − aji2 ≤ ξi − yi, i ∈ I, l ∈ L2

(22)

The iterations are performed until a maximum number of iterations is reached
or when the diﬀerence of the objective function at two consecutive iterations is
less than a tolerance (cid:15). Algorithm 2 summarizes the training of the (cid:96)-DER model
with the CCP method.

For our models, the starting point for the optimization problems was pro-
duced using a deterministic strategy presented in [13]. Roughly speaking, we
ﬁrst use the KKZ method to ﬁnd a subset of centroids of the training data [15].
The training data is then grouped using Voronoi partitions. Finally, the starting
point of the DC optimization method is obtained by (traditional) linear least
squares data-ﬁt on the Voronoi regions.

Algorithm 2: (cid:96)-CCP

: Training set T = {(xi, yi) : i ∈ I} and the parameters kmax and (cid:15).

Input
Output : {W, a, M, b}
Initialize: W , a, M , b, ξ and k = 0
repeat

Determine the indexes ji1 and ji2 using (21)
Compute W , a, M , b, ξ for i = 1, . . . , m solving (22)
k = k + 1

until k ≥ kmax or the diﬀerence in the objective function is less than an (cid:15);
return W, a, M, b

5 Computational Experiments

This section presents some computational experiments to evaluate the perfor-
mance of the proposed (cid:96)-DER model trained using the CCP method for re-
gression tasks. Furthermore, we compare the (cid:96)-DER model with other models
from the literature. Namely, we consider the multilayer perceptron (MLP) [10],
the support vector regressor (SVR) [25], the hybrid linear/morphological ex-
treme learning machine (HLM-ELM) [29], the morphological dense network
(MDN) [18], and the maxout network [8]. Because the (cid:96)-DER yields a continuous
piecewise linear function, we also compare it with the regressor trained by the
diﬀerence of convex algorithm (DCA) proposed by Ho et al. [14].

Least-Squares Linear Dilation-Erosion Regression

9

Table 1. Hyper-parameters for the datasets.

Model
MLP
SVR
HLM-ELM

MDN

MAXOUT
DCA
(cid:96)-DER

Parameters
Default Sklearn
Default Sklearn
linear neurons=132
morfological neurons=141
1 , r1
(r1
1 , r2
(r2

2 , out1) = (100, 100, 100)
2 , out2) = (100, 100, 1)
(r1, r2) = (3, 3)
(r1, r2) = (3, 2)
(r1, r2) = (3, 2)

We evaluated the performance of the proposed (cid:96)-DER and the other models
from the literature using regression datasets from the Penn Machine Learning
Benchmarks (PMLB), a signiﬁcant benchmark suite for machine learning evalu-
ation and comparison [20]. The chosen datasets are listed in Table 2. Note that
we considered small and medium-sized datasets. We would like to point out that
we handled possible missing data using sklearn’s SimpleImputer command.
Furthermore, we partitioned the data set into training and test sets using the
sklearn’s KFold, with the default number of splits (n splits=5).

We trained the (cid:96)-DER model using the CVXPY package [7] with the MOSEK
solver [1]. Because the (cid:96)-DER is equivalent to a continuous piecewise linear
function, we adopt the same hyperparameters r1 = 3 and r2 = 2 as Ho et al. for
all datasets [14]. For a fair comparision between the continuous piecewise linear
models, we consider r1 = r2 = 3 for the maxout network [8]. Finally, the other
models’ hyperparameters are set to the default value of the sklearn or as far as
possible to values reported in the literature [5, 18, 29]. Table 1 summarizes the
hyperparameters used in our computational experiment.

We evaluated the performance of the regression models quantitatively using
the well-known mean squared error (MSE) and the fraction of variance unex-
plained (FVU) given by the following equations:

MSE =

1
m

m
(cid:88)

i=1

(yi − f (xi))2

and FVU =

(cid:80)m

i=1(yi − f (xi))2
(cid:80)m
i=1(yi − ¯y)2

,

(23)

where f : Rn → R denotes the regressor and ¯y =

1
m

m
(cid:88)

yi is the average of the

i=1

output values. Note that the FVU is the ratio between the MSEs produced by the
regressor and a dummy model, which consistently predicts the average output
values. The less the MSE and the FVU scores, the better is the regressor f . Ta-
ble 3 contains the average and the standard deviation of the FVU score obtained
from the regressors using 5-fold cross-validation on the considered datasets. The
best outcome for each dataset has been typed using boldface numbers. The box-
plots shown in Figure 1 summarize the MSE and FVU scores and the execution
time (in seconds) taken for training the regressors in the sixteen datasets.

10

A.L. Oliveira and M.E. Valle

Table 2. Information about the datasets of PMLB for regression tasks.

Dataset Name

Instances Feature

PM10
PWLINEAR

ANALCATDATA VEHICLE
D1
BODYFAT
D2
CHSCASE GEYSER1
D3
CLOUD
D4
CPU
D5
D6
ELUSAGE
D7 MACHINE CPU
D8
D9
D10 RABE 266
D11 RMFTSA LADATA
SLEUTH CASE1202
D12
D13
SLEUTH EX1605
D14 VINEYARD
D15 VINNIE
D16 VISUALIZING ENVIRONMENTAL
D17 VISUALIZING GALAXY

48
252
222
108
209
55
209
500
200
120
508
93
62
52
380
111
323

4
14
2
5
7
2
6
7
10
2
10
6
5
2
2
3
4

PMLB Code
485 analcatdata vehicle
560 bodyfat
712 chscase geyser1
210 cloud
561 cpu
228 elusage
230 machine cpu
522 pm10
229 pwLinear
663 rabe 266
666 rmftsa ladata
706 sleuth case1202
687 sleuth ex1605
192 vineyard
519 vinnie
678 visualizing environmental
690 visualizing galaxy

From the boxplot shown in Figure 1, the (cid:96)-DER trained with the CCP proce-
dure achieved the best performance (concerning both MSE and FVU) together
with the maxout network and the continuous piecewise linear regressor trained
using the DCA algorithm. The HLM-ELM and MLP networks presented the
worst performance according to both MSE and FVU scores.

The Hasse diagrams depicted in Figure 2 illustrate the outcome of Wilcoxon’s
nonparametric statistical test with a conﬁdence level of 95% for the MSE and
FVU scores [34]. In the Hasse diagram depicted in Figure 2, an edge means that
the model on top statistically outperformed the one below. Thus, the topmost
models are the best-performing models. Moreover, transitivity is valid in the
Hasse diagram. Note from Figure 2 that the (cid:96)-DER achieved the best perfor-
mance in terms of FVU score. The maxout network and the model trained using
DCA are competitive but superior to the remaining models for the FVU. In
terms of the MSE score, the (cid:96)-DER, maxout network, and the model trained us-
ing DCA are all competitive and superior to the other models in these regression
tasks.

Finally, note that training the (cid:96)-DER model using the CCP method is, on
average, faster than the DCA model. Indeed, the (cid:96)-DER model’s processing
time is approximately 21% of the time the DCA model spends on training.
The slowness of DCA probably follows because it requires solving more complex
optimization problems than the ones obtained using CCP.

6 Concluding Remarks

This paper introduced the linear dilation-erosion regressor ((cid:96)-DER), which is
given by a convex combination of the composition of linear transformations and
elementary morphological operators. Precisely, an (cid:96)-DER is deﬁned by the DC

Least-Squares Linear Dilation-Erosion Regression

11

Fig. 1. Boxplots of the MSE and FVU scores and average time required to train the
machine learning regressors.

MLPSVRHLM-ELMMDNMAXOUTDCA-DER100102104106108Average MSEMLPSVRHLM-ELMMDNMAXOUTDCA-DER102100102104Average FVUMLPSVRHLM-ELMMDNMAXOUTDCA-DER102100102Average TIME12

A.L. Oliveira and M.E. Valle

a) Mean square error (MSE)

b) Fraction of variance unexplained (FVU)

Fig. 2. Hasse diagram of Wilcoxon hypothesis test using the MSE and FVU scores.

function τ (cid:96) given by (7). Because τ (cid:96) is a continuous piecewise linear function,
an (cid:96)-DER is a universal approximator [33].

An (cid:96)-DER model can be trained by minimizing the mean square error given
by (11) using a training set T = {(xi, yi) : i = 1 : m} ⊂ Rn × R. This paper
proposed to train an (cid:96)-DER τ (cid:96) using DC optimization. The approach solves a
constrained DC problem using the CCP method [16].

We compared the (cid:96)-DER trained using CCP with other approaches from the
literature using several regression tasks. According to the preliminary computa-
tional experiments, the (cid:96)-DER trained using CCP is competitive with state-of-
the-art methods like the SVR. Furthermore, regarding the computational com-
plexity of training the (cid:96)-DER, solving the quadratic optimization subproblems
incurs the highest cost.

In the future, we plan to investigate further the performance of the (cid:96)-DER
model trained by the CCP method. In particular, we intend to study the eﬀects
of the hyper-parameters – such as the shape of the matrices W and M – on the
curve ﬁtting capability of the (cid:96)-DER.

References

1. ApS, M.: MOSEK Optimizer API for Python (2020), release 9.3.10
2. Ara´ujo, R.d.A., Oliveira, A.L., Soares, S., Meira, S.: An evolutionary morphological
approach for software development cost estimation. Neural Networks 32, 285–291
(8 2012). https://doi.org/10.1016/j.neunet.2012.02.040

3. Ara´ujo, R.d.A.: A class of hybrid morphological perceptrons with application
in time series forecasting. Knowledge-Based Systems 24(4), 513–529 (2011).
https://doi.org/10.1016/j.knosys.2011.01.001

4. Birkhoﬀ, G.: Lattice Theory. American Mathematical Society, Providence, 3 edn.

(1993)

MLPSVRHLM-ELMMDNMAXOUTDCAl-DERMLPSVRMDNHLM-ELMMAXOUTDCAl-DERLeast-Squares Linear Dilation-Erosion Regression

13

Table 3. Average and standard deviation of the FVU scores on datasets in Table 2.

3
8
1
.
0
±
9
7
3
.
0

5
8
1
.
0
±
5
1
3
.
0

9
9
1
.
0
±
9
9
2
.
0

2
4
2
.
0
±
4
5
1
.
1

4
9
3
.
0
±
6
5
6
.
0

7
8
0
.
0
±
1
9
0
.
1

3
3
2
.
0
±
0
9
4
.
2

7
3
0
.
0
±
7
2
0
.
0

9
2
0
.
0
±
8
2
0
.
0

0
3
0
.
0
±
3
4
0
.
0

0
1
0
.
1
±
4
0
9
.
0

3
0
1
.
0
±
5
2
2
.
0

8
5
0
.
0
±
9
0
2
.
0

4
1
4
.
0
±
7
7
2
.
1

4
3
0
.
0
±
4
3
2
.
0

2
3
0
.
0
±
6
3
2
.
0

0
5
0
.
0
±
1
5
2
.
0

1
4
5
.
0
±
8
2
2
.
1

1
e
4
.
2
±
1
e
3
.
2

6
1
0
.
0
±
5
6
2
.
0

9
3
5
.
4
±
1
e
6
.
2

8
0
1
.
0
±
2
0
2
.
0

2
3
5
.
0
±
9
2
5
.
0

3
3
1
.
0
±
7
4
2
.
0

4
2
1
.
0
±
7
0
3
.
0

5
0
0
.
1
±
4
2
7
.
1

4
7
1
.
0
±
1
6
3
.
0

2
8
1
.
0
±
5
3
2
.
0

2
6
0
.
0
±
9
9
0
.
0

0
0
3
.
0
±
6
6
1
.
0

2
2
0
.
0
±
7
3
0
.
0

7
2
2
.
0
±
6
5
5
.
0

2
e
1
.
1
±
1
e
5
.
7

0
6
0
.
0
±
3
1
0
.
1

0
0
2
.
0
±
5
1
2
.
1

3
3
1
.
0
±
4
4
3
.
0

1
3
1
.
0
±
7
6
3
.
0

9
5
2
.
0
±
9
0
4
.
0

5
7
1
.
0
±
0
4
3
.
0

4
5
7
.
0
±
6
9
8
.
1

3
3
2
.
0
±
3
0
7
.
0

3
8
8
.
0
±
6
0
5
.
3

0
9
0
.
0
±
8
4
1
.
0

9
2
1
.
0
±
9
2
2
.
0

4
6
0
.
0
±
9
2
1
.
0

1
9
1
.
0
±
1
1
5
.
0

4
e
7
.
1
±
4
e
1
.
1

3
6
0
.
0
±
7
1
0
.
1

4
7
1
.
0
±
5
5
2
.
1

0
4
0
.
0
±
4
5
8
.
0

0
6
1
.
0
±
5
9
8
.
0

6
5
0
.
0
±
6
8
8
.
0

2
1
1
.
0
±
7
7
9
.
0

2
7
5
.
0
±
4
4
5
.
2

3
3
0
.
0
±
4
2
7
.
0

3
5
0
.
0
±
8
6
8
.
0

5
5
0
.
0
±
7
3
2
.
0

7
3
1
.
0
±
9
9
2
.
0

4
1
0
.
0
±
1
8
1
.
0

0
5
1
.
0
±
6
7
7
.
0

3
9
0
.
0
±
1
2
4
.
0

9
6
0
.
0
±
3
5
3
.
0

4
5
0
.
0
±
1
2
2
.
0

0
1
0
.
0
±
0
1
0
.
0

1
0
0
.
0
±
2
0
0
.
0

1
0
0
.
0
±
4
0
0
.
0

9
3
3
.
0
±
4
2
5
.
0

2
0
0
.
0
±
5
0
0
.
0

1
0
1
.
0
±
3
3
6
.
0

9
7
2
.
0
±
7
7
4
.
1

0
7
1
.
0
±
3
8
4
.
0

4
3
1
.
0
±
3
2
4
.
0

6
4
1
.
0
±
7
6
4
.
0

6
3
2
.
0
±
7
0
6
.
0

4
0
7
.
0
±
9
5
4
.
1

6
9
0
.
0
±
1
1
5
.
0

8
3
2
.
0
±
9
0
6
.
0

4
9
0
.
0
±
6
7
3
.
0

9
4
1
.
0
±
6
4
4
.
0

3
9
1
.
0
±
2
8
4
.
0

6
6
3
.
0
±
6
2
1
.
1

8
6
9
.
0
±
6
4
7
.
1

6
7
1
.
0
±
4
7
0
.
1

8
3
2
.
0
±
5
4
9
.
1

3
1
3
.
0
±
6
7
5
.
0

4
4
5
.
1
±
4
0
5
.
1

7
3
6
.
0
±
0
1
0
.
1

6
6
6
.
7
±
0
3
2
.
8

0
1
5
.
0
±
5
9
2
.
1

8
1
1
.
0
±
5
2
0
.
1

1
e
0
.
3
±
1
e
3
.
5

4
9
3
.
0
±
9
2
5
.
0

8
3
4
.
0
±
8
8
5
.
0

5
1
0
.
3
±
1
6
9
.
1

0
4
8
.
3
±
2
0
7
.
5

8
4
4
.
1
±
4
3
7
.
1

8
6
1
.
0
±
8
2
6
.
0

9
8
1
.
8
±
1
e
0
.
1

2
2
0
.
0
±
4
5
2
.
0

6
2
0
.
0
±
4
6
2
.
0

5
1
0
.
0
±
7
5
2
.
0

4
6
0
.
0
±
4
1
3
.
0

3
4
0
.
0
±
2
5
3
.
0

2
4
0
.
0
±
9
7
2
.
0

7
2
0
.
0
±
8
7
2
.
0

2
9
0
.
0
±
2
5
6
.
0

3
3
0
.
0
±
0
9
6
.
0

5
6
0
.
0
±
6
1
7
.
0

1
0
2
.
0
±
0
5
2
.
1

8
0
6
.
1
±
2
3
7
.
4

1
1
1
.
0
±
8
2
7
.
0

7
0
4
.
0
±
6
7
5
.
1

0
1
0
.
0
±
7
9
0
.
0

4
1
0
.
0
±
2
5
0
.
0

9
7
0
.
0
±
7
5
1
.
0

1
4
1
.
6
±
1
e
5
.
2

3
e
7
.
3
±
3
e
6
.
2

5
1
0
.
0
±
3
1
5
.
0

1
e
8
.
2
±
2
e
5
.
2

1
D

2
D

3
D

4
D

5
D

6
D

7
D

8
D

9
D

0
1
D

1
1
D

2
1
D

3
1
D

4
1
D

5
1
D

6
1
D

7
1
D

7
2
2
.
0
±
4
2
3
.
0

7
5
3
.
0
±
4
1
4
.
0

3
7
4
.
0
±
3
4
4
.
0

1
1
9
.
5
±
7
1
9
.
2

3
e
5
.
2
±
2
e
8
.
7

6
9
2
.
0
±
5
5
6
.
0

1
e
5
.
6
±
1
e
2
.
2

e
g
a
r
e
v
A

R
E
D

-
(cid:96)

A
C
D

T
U
O
X
A
M

N
D
M

M
L
E
-
M
L
H

R
V
S

P
L
M

t
e
s
a
t
a
D

14

A.L. Oliveira and M.E. Valle

5. Buitinck, L., Louppe, G., Blondel, M., Pedregosa, F., Mueller, A., Grisel, O., Nic-
ulae, V., Prettenhofer, P., Gramfort, A., Grobler, J., Layton, R., VanderPlas, J.,
Joly, A., Holt, B., Varoquaux, G.: {API} design for machine learning software:
experiences from the scikit-learn project. In: ECML PKDD Workshop: Languages
for Data Mining and Machine Learning. pp. 108–122 (2013)

6. Charisopoulos, V., Maragos, P.: Morphological Perceptrons: Geometry and Train-
ing Algorithms. In: Math. Morph. and Its Appli. to Signal and Image Proc. pp.
3–15. Springer (2017). https://doi.org/0.1007/978-3-319-57240-6 1

7. Diamond, S., Boyd, S.: CVXPY: A Python-Embedded Modeling Language for

Convex Optimization (Jun 2016)

8. Goodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., Bengio, Y.: Maxout
Networks. In: International Conference on Machine Learning. pp. 1319–1327 (2013)
9. Hartman, P.: On functions representable as a diﬀerence of convex functions. Pa-
ciﬁc Journal of Mathematics 9(3), 707–713 (Sep 1959), publisher: Mathematical
Sciences Publishers

10. Haykin, S.: Neural Networks and Learning Machines. Prentice Hall, New York, 3ª

edi¸c˜ao edn. (Jun 2008)

11. Heijmans, H.J.A.M.: Mathematical Morphology: A Modern Approach in Image
Processing Based on Algebra and Geometry. SIAM Review 37(1), 1–36 (1995)
12. Hern´andez, G., Zamora, E., Sossa, H., T´ellez, G., Furl´an, F.: Hybrid neural
networks for big data classiﬁcation. Neurocomputing 390, 327–340 (5 2020).
https://doi.org/10.1016/j.neucom.2019.08.095

13. Ho, V.T., Le Thi, H.A., Pham Dinh, T.: DCA with Successive DC Decomposi-
tion for Convex Piecewise-Linear Fitting. In: Le Thi, H.A., Le, H.M., Pham Dinh,
T., Nguyen, N.T. (eds.) Advanced Computational Methods for Knowledge En-
gineering. pp. 39–51. Advances in Intelligent Systems and Computing, Springer
International Publishing, Cham (2020). https://doi.org/10.1007/978-3-030-38364-
0 4

14. Ho, V.T., Le Thi, H.A., Pham Dinh, T.: DCA-based algorithms for DC ﬁtting.
Journal of Computational and Applied Mathematics 389, 113353 (Jun 2021).
https://doi.org/10.1016/j.cam.2020.113353

15. Katsavounidis, I., Jay Kuo, C.C., Zhang, Z.: A new initialization technique for
generalized Lloyd iteration. IEEE Signal Processing Letters 1(10), 144–146 (Oct
1994). https://doi.org/10.1109/97.329844, conference Name: IEEE Signal Process-
ing Letters

16. Lipp, T., Boyd, S.: Variations

and extension of

procedure. Optimization and Engineering 17(2),
https://doi.org/10.1007/s11081-015-9294-x

the
263–287

convex–concave
2015).

(Nov

17. Luenberger, D.G.: Linear and Nonlinear Programming. Addison-Wesley, 2 edn.

(1984)

18. Mondal, R., Mukherjee, S.S., Santra, S., Chanda, B.: Morphological Network: How
Far Can We Go with Morphological Neurons? Tech. Rep. arXiv:1901.00109, arXiv
(Dec 2020). https://doi.org/10.48550/arXiv.1901.00109, http://arxiv.org/abs/
1901.00109, arXiv:1901.00109 [cs, stat] type: article

19. Oliveira, A.L., Valle, M.E.: Linear Dilation-Erosion Perceptron Trained Using a
Convex-Concave Procedure. In: Abraham, A., Ohsawa, Y., Gandhi, N., Jabbar,
M., Haqiq, A., McLoone, S., Issac, B. (eds.) Proceedings of SoCPaR 2020. pp.
245–255. Advances in Intelligent Systems and Computing, Springer International
Publishing, Cham (2021). https://doi.org/10.1007/978-3-030-73689-7 24

Least-Squares Linear Dilation-Erosion Regression

15

20. Olson, R.S., La Cava, W., Orzechowski, P., Urbanowicz, R.J., Moore, J.H.: PMLB:
a large benchmark suite for machine learning evaluation and comparison. BioData
Mining 10(1), 36 (Dec 2017). https://doi.org/10.1186/s13040-017-0154-4

21. Pessoa, L.F.C., Maragos, P.: Neural networks with hybrid morphologi-
cal/rank/linear nodes: a unifying framework with applications to handwritten char-
acter recognition. Pattern Recognition 33, 945–960 (2000)

22. Ritter, G.X., Sussner, P.: An Introduction to Morphological Neural Networks. In:
Proceedings of the 13th International Conference on Pattern Recognition. pp. 709–
717. Vienna, Austria (1996)

23. Ritter, G.X., Sussner, P., Diaz-De-Leon, J.L.: Morphological associative
IEEE Transactions on Neural Networks 9(2), 281–293 (1998).

memories.
https://doi.org/10.1109/72.661123

24. Ritter, G.X., Urcid, G.: Lattice Algebra Approach to Single-Neuron Computation.

IEEE Transactions on Neural Networks 14(2), 282–295 (2003)

25. Sch¨olkopf, B., Smola, A.: Learning with Kernels: Support Vector Machines, Regu-

larization, Optimization, and Beyond. MIT Press, Cambridge, MA (2002)

26. Shen, X., Diamond, S., Gu, Y., Boyd, S.: Disciplined convex-concave programming.
In: 2016 IEEE 55th Conference on Decision and Control (CDC). pp. 1009–1014
(2016). https://doi.org/10.1109/CDC.2016.7798400

27. Soille, P.: Morphological Image Analysis. Springer Verlag, Berlin (1999)
28. Stone, M.H.: The Generalized Weierstrass Approximation Theorem. Mathemat-
ics Magazine 21(4), 167–184 (1948). https://doi.org/10.2307/3029750, publisher:
Mathematical Association of America

29. Sussner, P., Campiotti,

I.: Extreme learning machine for a new hybrid
morphological/linear perceptron. Neural Networks 123, 288–298 (Mar 2020).
https://doi.org/10.1016/j.neunet.2019.12.003

30. Sussner, P., Esmi, E.L.: Morphological Perceptrons with Competitive Learning:
Lattice-Theoretical Framework and Constructive Learning Algorithm. Information
Sciences 181(10), 1929–1950 (2011). https://doi.org/10.1016/j.ins.2010.03.016
31. Tuy, H.: DC Functions and DC Sets. In: Tuy, H. (ed.) Convex Analysis and Global
Optimization, pp. 103–123. Springer Optimization and Its Applications, Springer
International Publishing, Cham (2016), 10.1007/978-3-319-31484-6\ 4

32. Valle, M.E.: Reduced Dilation-Erosion Perceptron for Binary Classiﬁcation. Math-

ematics 8(4), 512 (2020). https://doi.org/10.3390/math8040512

33. Wang, S.: General constructive representations for continuous piecewise-linear
functions. IEEE Transactions on Circuits and Systems I: Regular Papers 51(9),
1889–1896 (2004). https://doi.org/10.1109/TCSI.2004.834521

34. Weise, T., Chiong, R.: An alternative way of presenting statistical test results
when evaluating the performance of stochastic approaches. Neurocomputing 147,
235–238 (2015). https://doi.org/10.1016/j.neucom.2014.06.071

35. Yuille, A.L., Rangarajan, A.: The concave-convex procedure. Neural Computation

15(4), 915–936 (2003). https://doi.org/10.1162/08997660360581958

