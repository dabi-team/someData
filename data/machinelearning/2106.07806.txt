Highdicom:APythonlibraryforstandardizedencodingofimageannotationsandmachinelearningmodeloutputsinpathologyandradiologyChristopherP.Bridge1,2,ChrisGorman3,StevenPieper4,SeanW.Doyle2,JochenK.Lennerz5,6,JayashreeKalpathy-Cramer1,2,7,DavidA.Clunie8,AndriyY.Fedorov7,9,andMarkusD.Herrmann3,6,*1MartinosCenterforBiomedicalImaging,MassachusettsGeneralHospital,Boston,MA,USA2MGH&BWHCenterforClinicalDataScience,MassGeneralBrigham,Boston,MA,USA3ComputationalPathology,DepartmentofPathology,MassachusettsGeneralHospital,Boston,MA,USA4Isomics,Inc.,Cambridge,MA,USA5CenterforIntegratedDiagnostics,DepartmentofPathology,MassachusettsGeneralHospital,Boston,MA,USA6DepartmentofPathology,HarvardMedicalSchool,Boston,MA,USA7DepartmentofRadiology,HarvardMedicalSchool,Boston,MA,USA8PixelMedPublishing,LLC.,Bangor,PA,USA9SurgicalPlanningLaboratory,DepartmentofRadiology,BrighamandWomen’sHospital,Boston,MA,USA*correspondingauthor:mdherrmann@mgh.harvard.eduAbstractMachinelearning(ML)isrevolutionizingimage-baseddiagnosticsinpathologyandradiology.MLmodelshaveshownpromisingresultsinresearchsettings,butthelackofinteroperabilitybetweenMLsystemsandenterprisemedicalimagingsystemshasbeenamajorbarrierforclinicalintegrationandevaluation.TheDICOM®standardspeciﬁesInformationObjectDeﬁnitions(IODs)andServicesfortherepresentationandcommunicationofdigitalimagesandrelatedinformation,includingimage-derivedannotationsandanalysisresults.However,thecomplexityofthestandardrepresentsanobstacleforitsadoptionintheMLcommunityandcreatesaneedforsoftwarelibrariesandtoolsthatsimplifyworkingwithdatasetsinDICOMformat.Herewepresentthehighdicomlibrary,whichprovidesahigh-levelapplicationprogramminginterface(API)forthePythonprogramminglanguagethatabstractslow-leveldetailsofthestandardandenablesencodinganddecodingofimage-derivedinformationinDICOMformatinafewlinesofPythoncode.ThehighdicomlibraryleveragesNumPyarraysforeﬃcientdatarepresentationandtiesintotheextensivePythonecosystemforimageprocessingandma-chinelearning.Simultaneously,bysimplifyingcreationandparsingofDICOM-compliantﬁles,highdicomachievesinteroperabilitywiththemedicalimagingsystemsthatholdthedatausedtotrainandrunMLmodels,andultimatelycommunicateandstoremodeloutputsforclinicaluse.Wedemonstratethroughexperimentswithslidemicroscopyandcomputedtomographyimaging,that,bybridgingthesetwoecosystems,highdicomenablesdevelopersandresearcherstotrainandevaluatestate-of-the-artMLmodelsinpathologyandradiologywhileremainingcompliantwiththeDICOMstandardandinteroperablewithclinicalsystemsatallstages.TopromotestandardizationofMLresearchandstreamlinetheMLmodeldevelopmentanddeploymentprocess,wemadethelibraryavailablefreeandopen-sourceathttps://github.com/herrmannlab/highdicom.1arXiv:2106.07806v3  [eess.IV]  8 May 2022BackgroundRecentbreakthroughsinmachinelearning(ML)andcomputationalprocessingcapabilitieshaveledtothedevelopmentofMLmodelsthatdemonstrateunprecedentedperformanceonavarietyofhighlycomplexcomputervisiontasks[1].State-of-the-artconvolutionalneuralnetworkmodelsnowregularlyachievenear-humanorevensuperhumanperformanceonavarietyofchallengingvisiontasksandacrossdiﬀerentimagingmodalities,includingsegmentationandclassiﬁcationofslidemicroscopyimagesinpathology[2–4]aswellascomputedtomographyormagneticresonanceimagesinradiology[5–7].Overthelastcoupleofyears,MLmodelshaveevolvedtechnicallywithintheresearchdomain[8]andthevisionisthatthesemodelswillsoonbeappliedwidelyinclinicalpracticetosupportpathologistsandradiologistsininterpretationofimagesandultimatelyimprovediagnosticaccuracyandeﬃciency[6,9–11].Torealizethisvision,healthcareenterprisesarenowtaskedwithevaluatingmodelperformanceinclinicalcontextandintegratingtheoutputsofMLmodelsintoclinicalworkﬂows.Similarly,developmentofmodelsmaybeexpeditedifimageannotationsgeneratedbyclinicalexpertsandstoredwithinclinicalinformationsystemscouldbedirectlyconsumedbyMLtrainingandvalidationpipelines.Unfortunately,thisiscurrentlyimpededbythelackofstandardinterfacesforexchangeofimageannotationsandMLmodeloutputsbetweenimageanalysis,imagedisplay,andimagemanagementsystems.DigitalImagingandCommunicationinMedicine(DICOM)istheinternationallyacceptedstandardforcommunicationofmedicalimagesandrelatedinformationacrossawiderangeofmedicalimag-ingmodalitiesanddisciplines.Hospitalsaroundtheworldhaveestablishedanextensiveenterpriseimaginginfrastructure,workﬂows,andsoftwareapplicationsbasedonDICOM[12]andpathologyandradiologyareconvergingtowardsusingDICOMforcommunicationofdigitalimages[13–16].However,existingpathologyaswellasradiologysystemsprimarilyrelyonnon-standardformatsandinterfacesforthestorageandexchangeofimageannotationsandcomputationalimageanalysisresults,towhichwehereaftercollectivelyreferasannotations.Similarly,MLmodelsdevelopedbyresearchersgenerallyreceiveandreturnannotationsinavarietyofcustomizedformatsthatareincompatiblewithclinicallyavailableimagemanagementanddisplaysystemsandthatlackmeta-datarequiredforinterpretationanduseoftheinformationinclinicalcontext.Instead,itwouldbedesirableifMLmodelsweredevelopedaccordingtotheFAIRguidingprinciples[17]usingstan-dardizedmetadatatoallowforannotationstobeﬁndable,accessible,interoperable,andreusable.TheDICOMstandardprovidesinformationobjectdeﬁnitions(IODs),suchasSegmentationsandStructuredReports,forannotations[18,19],andimplementationoftheseIODstoenableinterop-erablestorageandcommunicationofMLmodeloutputshasbeenproposedbytheIntegratingtheHealthcareEnterprise(IHE)RadiologyTechnicalCommittee[20].Pythonisthedefactostandardprogramminglanguageofdatascienceandprovidesarichecosystemforscientiﬁccomputing,imageprocessing,andmachinelearning[21–24].ThemajorityofMLmodelsaredevelopedanddeployedintheformofPythonprograms.Thepydicomlibrary[25]providesdatastructuresandroutinesforstoringandaccessingdataofDICOMdatasets(parts5and6oftheDICOMstandard)aswellasreadingandwritingDICOMﬁles(part10oftheDICOMstandard).However,pydicomhasnoconceptofIODs(parts3and16oftheDICOMstandard)andassuchleavesittoeachdevelopertosetallattributesrequiredbyanIODmanuallyandensurethattheyfollowallrelevantconstraintswhencreatingnewDICOMobjectscontainingannotations.Similarly,parsingtheannotationIODsfortheinformationrelevanttoaparticularMLtaskusingthepydicomAPIischallengingduetotheirhighlynestedandinterdependentstructure.Consequently,bothtasksareslow,complex,anderror-proneandrequireconsiderableknowledgeoftheDICOMstandard.Wethereforeidentiﬁedaneedforahigher-levelabstractionlayerbetweentheMLmodeldeveloperandthelow-levelencodingrulesoftheDICOMstandard.Thismotivated2ustocreatetheopen-sourcehighdicomlibrary,whichprovidesahigh-levelapplicationprogramminginterfaceforcreatingandreadingannotationsinDICOMformatusingthePythonprogramminglanguage.OurgoalinreleasingthislibraryistoenableMLprocessesthatachieveinteroperabilitybetweenMLmodelsandclinicalinformationsystemsthroughouttheentiremodeldevelopmentanddeploymentlifecyclewhileavoidingthecomplexitythatthiscurrentlyentails.Further,weaimedtocreatealibrarythatisapplicableacrossarangeofcommonMLtasksandimagingdomains.Inthisarticle,weﬁrstdescribethedesignandimplementationofthehighdicomlibrarytomeetthisunmetneedandthenassessthelibrary’scapabilitiesinencodinganddecodingannotations(eithergeneratedbyhumanreadersorMLmodels)inDICOMformat.WeperformexperimentsthatdemonstratetheuseofthelibraryduringMLmodeltrainingandinferenceandshowhowthelibraryenablesthedevelopmentofMLmodelsthatareinteroperablewithestablishedimagemanagementanddisplaysystemsandthuscanbereadilyintegratedintoanenterprisemedicalimagingenvironment.Tothisend,weconsideravarietyofclinically-relevantcomputervisionproblemsandmultipleimagingmodalitiesacrossdiﬀerentmedicaldisciplines,placingafocusonlungtumordetectioninslidemicroscopyimagesinpathologyandcomputedtomographyimagesinradiologyasanillustrativeusecase.MethodsDesignOverviewandApplicationProgrammingInterface(API)ThesoftwarecomponentsresponsiblefortransformingthedatainputandoutputfromMLmodels,andtherebyensuringinteroperabilitywithadjacentsystems,arecommonlyreferredtoasdatapipelines[26,27].Duringinference,pipelinesareresponsibleforretrievingandpreprocessinginputimagesintoanin-memoryformatthatcanbeconsumedbythemodelandencodingthemodel’sin-memoryoutputsintoaformsuitableforcommunicationandstorage.Duringtraining,theyretrieveandpreprocessinputimagesandadditionally,ifrequired,decodeannotationsintoanin-memoryrepresentationofthetargetformodeltraining.ThehighdicomlibraryisintendedtooperatewithindatapipelinesthatconnectclinicalinfrastructureusingtheDICOMstandardtopopularPythonMLframeworkssuchasPyTorch[28]andTensorﬂow[29],andisfocusedonannotationsratherthantheinputimagesthemselves.Thelibrary’scorefunctionalityistwofold:First,encodingmodeloutputsinformofNumPyarraystogetherwithrelevantmetadataintoannotationsintheformofpydicomobjects(ﬁgure1A).Second,decodingannotationsprovidedaspydicomobtainstoobtaintargetsinformofNumPyarrays(ﬁgure1B)byreadingandinterpretingtheincludedmetadata.Wechosethen-dimensionalNumPyarraydatastructure[22]asanin-memoryrepresentationofmodeloutputsandtargetsbecauseitisinteroperablewithpydicomaswellasPyTorchandTensorﬂowandmanyotherwell-establishedPythonimageprocessinglibraries(e.g.,OpenCV[30]andITK[31]).APIOverviewWedesignedhighdicomfollowingtheobject-orientedprogrammingparadigmandmodelledtheAPIaccordingtotheDICOMInformationModel,whichspeciﬁesdiﬀerentabstractdatatypesthatarereferredtoasInformationObjectDeﬁnitions(IODs)(ﬁgure2).AnIODdeﬁnesthesetofrequiredandoptionalDICOMattributesthatmaybeincludedintoDICOMobjects.WeselectedvariousIODsforstorageofannotationsandimplementedeachinhighdicomasaPythonclass.Strictlyspeaking,eachPythonclassimplementsaDICOMStorageService-ObjectPair(SOP)Class,whichisthedatastructurewithintheDICOMstandardthatstorestheattributesdeﬁnedbyanIOD.AninstanceofsuchaPythonclassthusrepresentsaDICOMSOPInstanceandserves3BAModelModelInputnumpy.ndarrayImagepydicom.DatasetAnnotationspydicom.DatasetOutputnumpy.ndarrayhighdicom encodinghighdicom decodingInputnumpy.ndarrayImagepydicom.DatasetOutputnumpy.ndarrayTargetnumpy.ndarrayAnnotationspydicom.DatasetMetadataMetadatacompute lossFigure1:Intendeduseofhighdicomindatapipelinesduringmachinelearningmodeltrainingandinferenceworkﬂows.A.Encodingofmodeloutputsuponinferenceinthepostprocessingpipeline.B.Decodingofimageannotationsformodeltraininginthepreprocessingpipeline.asacontainerforaDICOMDataSet,whereeachinstanceattributeholdsthevalueofaDICOMDataElement.ThePythonclassesareultimatelyderivedfromthepydicom.Datasetclassfromtheexistingpydicompackageandthereforeinheritlow-levelbehaviors,suchasaccessing,setting,iteratingoverdataelementsandreading/writingto/fromﬁlesthatmanydevelopersarealreadyfamiliarwith.Itfurtherallowsdeveloperstoretainlow-levelcontroloveralldataelementsinordertoaddtooralterinformationinobjectsconstructedbyhighdicom.Belowpydicom.Datasetintheclasshierarchy,thereisacommonabstractbaseclasscalledhighdicom.SOPClass(ﬁgure2A),whichabstractstheattributesthatarerequiredbyallSOPclasses.SpeciﬁcSOPclassesarethenim-plementedbydedicatedPythonclassesthatarederivedfromtheabstractbaseclass(ﬁgure2B).Inthisway,weaimtoprovideanidiomaticPythoninterfacethatabstractsasmuchofthelow-levelDICOMencodinganddecodingrulesaspossiblewhilestayingclosetothestandardDICOMterminologytoavoidpotentialambiguities.EncodingofDICOMSOPInstancesTheprocessofencodinginformationinderivedob-jectsisimplementedintheconstructormethodsofthecorrespondingSOPclasses(eitherinthehighdicom.SOPClassabstractbaseclassorinderivedIOD-speciﬁcclasses).ForconstructionofanSOPinstance,thedeveloperprovidestheimage-derivedinformationthatisoutputtedbyamodel(e.g.,pixeldataorgraphicdata)togetherwithdescriptivecontextualinformationthatthestandardrequiresforthecorrespondingIOD.Attributevaluesthatarestaticorcanbederivedfromprovidedargumentsareautomaticallysetuponobjectconstruction.Forexample,relevantmetadataaboutthepatient,thestudy,orthespecimenareautomaticallycopiedfromthemetadataofprovidedsourceimagesandreferencestothesourceimagesareincludedinthederivedobjects(ﬁgure3A-B).Furthermore,theconstructorautomaticallyvalidatesthecontentofcreatedSOPinstancesthroughruntimecheckstoensurethatconstructedobjectsarefullycompliantwiththe4relevantIODinthestandard.Bydesign,allrequiredinformationmustbepassedtotheSOPclassconstructorwhencreatingtheobject,andthereaftertheobjectremainsimmutablethroughthehighdicomAPI(thoughanexperienceddevelopermayusethelower-levelinterfaceprovidedbythepydicomAPItomodifytheobjectifrequired).Thismeansthattheconstructorcanvalidateallinputparametersatonceaccountingforallinterdependenciesandconditionallogicbetweenattributes.ItalsoreﬂectstheintentofthestandardinthatDICOMobjectsareimmutablefollowingcreation.DecodingofDICOMSOPInstancesThepydicomlibraryprovidesapowerfullow-levelPythoninterfacetodeveloperstoaccessDICOMdataelementsofadatasetdirectly,withlittleab-stractionfromthedetailsofthedataformat.Whilethisisappropriateformanyimageobjects,thecomplexityofthederivedobjectsusedforannotationsmeansthataccessingthedesiredinformationusingthepydicomAPIrequiresadetailedknowledgeoftheunderlyingdatastructuresandinourexperienceresultsinaverbose,cumbersome,anderror-proneprocess.Therefore,wehaveendowedhighdicomSOPclasseswithadditionalmethods(notinthestandard)thatprovideameansfordeveloperstoaccess,ﬁlter,andinterpretthecontentofaDICOMobjectwhenpreparingimageannotationstobeusedastargetsforatrainingalgorithm.Inaddition,highdicomSOPclassesimplementalternativeconstructormethodsthatallowforthecreationofhighdicomSOPinstancesfromexistingpydicom.Datasetobjects,whichwerereadfromaﬁleorretrievedovernetwork,andtherebyenhancetheobjectswithadditional,modality-speciﬁcmethodsandpropertiesfordataaccess.DatatypesandstructuresThemajorityofDICOMmetadataattributevaluesthatarepassedtoandreturnedfromthehighdicomAPIuponencodinganddecodingofSOPinstanceshaveprimitive,built-inPythontypessuchasstrings(str),integers(int),andﬂoats(float).TofurtherencapsulatecloselyrelatedmetadataofcompositeDICOMdatatypes(DICOMSequencesorSequenceItems)andtoimprovecodereadabilityandreusability,thehighdicomAPIfurtherprovidescustomPythontypes,whichareimplementedintheformofPythonclassesandaregenerallyderivedfromeitherpydicom.Datasetorpydicom.Sequence.DICOMbulkdatavaluessuchaspixeldataorvectorgraphicdataarepassedtoandreturnedfromhighdicomPythonclassesasNumPyobjects(numpy.ndarray).StorageofannotationsinDICOMformatHavingdescribedthegeneralapproachtakenbyourlibrary,wenowbegintodiscusstheindividualIODsthatweselectedforimplementation.TheDICOMstandardspeciﬁesawiderangeofIODsfordiﬀerenttypesofDICOMobjects,includingimagesacquiredbyvariousmodalities(e.g.,computedtomographyorwholeslidemicroscopy)aswellasimage-derivedinformationgeneratedbyimagedisplay,processing,oranalysissystems[32].Forimplementationinthehighdicomlibrary,weconsideredstandardIODsthatprovidemechanismstostoreimageannotationsforcommonMLtasksacrosspathologyandradiologyusecases.Wetherebyfocusedonthefollowingdecisionproblemsandtheircorrespondingannotations(ﬁgure3A)[1]:1.Imageclassiﬁcation—classlabelsintheformofdiscretebinaryorcategoricalvaluesandoptionallyclassscoresintheformofcontinuousprobabilisticvalues(ﬁgure3Aupperpanel)2.Imagesegmentation—classlabelsatpixelresolutionthatidentifysemanticallydistinctregionsofinterest(ROIs)withinanimageintheformofrastergraphics(ﬁgure3Amiddlepanel)5class SOPClass(pydicom.Dataset):    def __init__(        self,        study_instance_uid: str,        series_instance_uid: str,        modality: str,        sop_instance_uid: str,        sop_class_uid: str,        patient_id: Optional[str] = None,         study_id: Optional[str] = None,        **kwargs    ):        super().__init__()        # Patient Module Attributes        self.PatientID = patient_id        # General Study Module Attributes        self.StudyInstanceUID = study_instance_uid        self.StudyID = study_id        # General Series Module Attributes        self.SeriesInstanceUID = series_instance_uid        self.Modality = modality        # SOP Common Module Attributes        self.SOPInstanceUID = sop_instance_uid        self.SOPClassUID = sop_class_uid        ...class Segmentation(SOPClass):    def __init__(        self,        source_images: Sequence[pydicom.Dataset],        pixel_array: numpy.ndarray,        study_instance_uid: str,        series_instance_uid: str,        sop_instance_uid: str,        **kwargs    ):        ref_image = source_images[0]        super().__init__(            patient_id=ref_image.PatientID,            study_id=ref_image.StudyID,            study_instance_uid=study_instance_uid,            series_instance_uid=series_instance_uid,            modality='SEG',            sop_instance_uid=sop_instance_uid,            sop_class_uid='1.2.840.10008.5.1.4.1.1.66.4',            **kwargs        )        # Image Pixel Module Attributes        self.Rows = pixel_array.shape[1]        self.Columns = pixel_array.shape[2]        ...    @classmethod    def from_dataset(        cls,        dataset: pydicom.Dataset    ) -> 'Segmentation':        ...ABimplementsDICOM Information ModelSegmentation StorageStorage Service-Object Pair (SOP) ClassSegmentationPatient Module AttributesPatient ID...General Study Module AttributesStudy Instance UIDStudy ID...General Series Module AttributesSeries Instance UIDModality...SOP Common Module AttributesSOP Instance UIDSOP Class UID...Image Pixel Module AttributesRowsColumns...definesderivesderivesPython Application Programming Interfacehighdicom.SOPClassAbstract Base Classpydicom.DatasetClasshighdicom.seg.SegmentationClassalternative constructorconstructorMetadata about Patient,Study, and Specimensare copied frompydicom.Dataset objectMetadataMetadata about Imageare derived fromnumpy.ndarray objectAttributes are set onpydicom.Dataset object Type 1 attributes:must be included in datasetmust have non-empty valueType 2 attributes:must be included in datasetmay have empty value NoneMetadata and Pixel Dataare copied from existingpydicom.Dataset objectPixel Data (model output)...Information Object Definition (IOD)Figure2:ImplementationoftheDICOMInformationModelinPython.A.ThehighdicomPythonabstractbaseclasshighdicom.SOPClassanditsrelationshiptoanDICOMInformationObjectDeﬁnition(IOD)andDICOMStorageService-ObjectPair(SOP)Class.B.AhighdicomPythonclassforaspeciﬁcDICOMIODandSOPClass(exempliﬁedbyhighdicom.seg.SegmentationthatimplementstheDICOMSegmentationStorageSOPClassdeﬁnedbytheDICOMSegmentationIOD).3.Objectdetection—spatialcoordinatesforindividualROIsintheformofvectorgraphics(commonlyboundingboxes),combinedwithclasslabelsanddetectionscores(ﬁgure3Alowerpanel)WeidentiﬁedthreeIODsthattogetherallowfortheencodingofannotationsforthesecommonusecases:theSegmentationIODandtwoStructuredReport(SR)IODs.TheSegmentationIODwasselectedtoencodeROIsreturnedbyimagesegmentationmodelsasrastergraphics.TheComprehensiveSRandComprehensive3DSRIODswerechosentoencodevectorgraphicROIsreturnedbyobjectdetectionmodelsaswellasclasslabels,scores,andmeasurementsreturnedby6imageclassiﬁcationandregressionmodels(ﬁgure3A).AllthreeIODsaredesignedtobeagnosticoftheimagingmodalityandabletosupportusecasesacrossmedicaldisciplinesincludingpathologyandradiology.Ref.Ref.SegmentationImagePatientStudySeriesEquipmentFrameFrameFrameFrameFrame of ReferenceVL Whole Slide Microscopy ImageImagePatientStudySeriesEquipmentFrameFrameFrameFrameFrame of ReferenceSpecimenSpecimenCopyCopyComprehensive 3D SRPatientStudySeriesEquipmentDocumentSCOORD3DCODEValueNameValueNameValueNameValueNameNUMIMAGECopyCopyCopyCopyBAhighdicom.sr.ComprehensiveSRDocumentImage ClassificationPixel Datanumpy.ndarrayImagepydicom.DatasetClass Labelsnumpy.ndarrayMetadatapydicom.Datasethighdicom.sr.Comprehensive3DSRDocumentPixel Datanumpy.ndarrayImagepydicom.DatasetGraphic Datanumpy.ndarrayClass Labelsnumpy.ndarrayObject DetectionMetadatapydicom.DatasetImagehighdicom.seg.SegmentationPixel Datanumpy.ndarrayImagepydicom.DatasetImage SegmentationPixel Datanumpy.ndarrayMetadatapydicom.DatasetFigure3:EncodingofmachinelearningmodeloutputsinDICOM.A.InformationentitiesandthePythontypesusedtorepresentmachinelearningmodelinputs(images)andoutputs(image-derivedinformation)forthreecommondecisionproblems.B.Schematicoverviewofthecontentofsourceimageobjects(exempliﬁedbyaDICOMVLWholeSlideMicroscopyImage)andderivedobjects(DICOMComprehensive3DSRandDICOMSegmentation).Notethatdescriptivemetadataiscopiedfromsourcetoderivedobjectsandderivedobjectsmayreferenceinformationcontainedinsourceimagesorotherderivedobjects.7DICOMSegmentationimagesTheSegmentationIODisimplementedinhighdicomasthehighdicom.seg.SegmentationPythonclassandallowsfortheencodingofoneormorecompo-nents,whichinDICOMarereferredtoassegments.Eachsegmentmayrepresentapixelclass(category)oranindividualinstanceofagivenclassasgeneratedbysemanticsegmentationorinstancesegmentationmodels[33],respectively.Segmentsmayfurtherhavebinaryorfractionaltype,eitherrepresentingamaskofBooleanvalueswherenon-zeropixelsencodeclassmembershiporamaskofdecimalnumberswherepixelsencodeclassprobability.InordertoencodeaDICOMSegmentationimage,thedeveloperpassestotheconstructoramaskasanumpy.ndarray(ofeitherBoolean,integer,orﬂoatingpointdatatype)alongwithadditionalmetadatathatdescribethemeaningofeachsegmentwithinthesegmentation(highdicom.seg.SegmentDescription)andthealgorithmresponsibleforproducingthesegmentation(highdicom.AlgorithmIdentificationSequence).TofacilitatedecodingofDICOMSegmentationimages,thehighdicom.seg.Segmentationclassprovidesmethodsthatallowdeveloperstoﬁltersegmentsbytheirlabel,segmentedpropertycate-goryortype,ortrackingidentiﬁers.Itfurtherprovidesmethodstoobtainasegmentationmaskasanumpy.ndarrayforagivensetofsegmentsandsourceimageframes.Whileconceptuallystraight-forward,inpracticeseveralstepsarenecessarytoachievethiscorrectly:(i)determiningwhichframesstoredintheSegmentationimagearerelevanttoagivensetofsegmentsandsourceimageframesbasedonthemulti-framedimensionindexinginformation,(ii)sortingtheSegmentationimageframesaccordingthequery,(iii)addinginmissingpixelvaluesincaseofsparseSegmenta-tionimageswherebackgroundimageframeswereomittedduringencodingtosavestoragespace(iv)(optionally)combiningmultibinarysegmentsintoamulti-classlabelmap.DICOMStructuredReportdocumentsTherearevariousIODsdeﬁnedbythestandardthatutilizestructuredreporting,butweselectedtheComprehensiveSR(highdicom.sr.ComprehensiveSR)andComprehensive3DSR(highdicom.sr.Comprehensive3DSR)IODsforimplementationinhighdicombecausetheyprovidethemostﬂexiblemechanismsforstoringannotations.InadditiontotheIODdeﬁnitions,thestandardprovidesSRtemplates,whichserveasschemasthatdeﬁnehowthecon-tentofanSRdocumentshallbestructuredandhowtheinformationshallbeencoded.Atemplateconsistsofasequenceofcontentitems,eachdeﬁninganame-valuepair(orquestion-answerpair)thatencodesadomain-speciﬁcpropertyorconcept(ﬁgure4A).Notably,bothconceptnamesandvalueshaveacompositedatatypeandareeachencodedbyoneormoreDICOMattributes.Con-ceptnamesarecodedusingstandardmedicalterminologiesandontologiessuchastheDICOMControlledTerminologyortheSystematizedNomenclatureofMedicineClinicalTerms(SNOMEDCT)andtherebygetendowedwithanexplicit,domain-speciﬁcmeaning[34].Thestructureofthecorrespondingvaluedependsonthevaluetype,whichdeﬁnesasetofDICOMattributesthatareincludedintheSRdocumenttorepresenttheassignedvalue.Withinhighdicom,thehighdicom.sr.CodedConceptclassisanimportantdatatypethatencap-sulatestheDICOMattributesrequiredtocodeaconceptusingastandardcodingschemewithinasinglePythonobject.Wefurthercontributedlower-leveldatatypestotheunderlyingpydicomlibrarythatprovideprogrammaticaccesstocodesincludedintheDICOMstandard,speciﬁcallytheDICOMControlledTerminology(DCM),SNOMED-CT(SCT),andUniﬁedCodeforUnitsofMeasure(UCUM)codingschemes.Thesecodesthatareincludedinthepydicomlibraryarefullycompatiblewiththecodedconceptsofthehighdicomlibraryandcangenerallybeusedin-terchangeablythroughouttheAPI.Furthermore,foreachofthediﬀerentDICOMcontentitemvaluetypeswehaveimplementedaseparatePythonclassthatisderivedfrompydicom.Datasetandencapsulatesboththecodedconceptnameandthecorrespondingvalueofthegiventype(ﬁgure4B).8Notablecontentitemclassesincludehighdicom.sr.CodeContentItem,whichmaybeusedtostoreclasslabelsascodedvalues,andhighdicom.sr.NumContentItem,whichmaybeusedtostoreameasurementalongwithitsunit.ROIsmaybeeitherencodedbyvalueorbyreferenceandstoredwithinoroutsideoftheSRdocumentcontent,respectively.Inthecaseofvectorgraphics(includingbutnotlimitedtoboundingboxes),thegraphicdatamaybestoredwithintheSRdocumentandencodedviaDICOMcontentitemsofvaluetypeSCOORD3D,whichencodes3Dspatialcoordinatesofgeometricobjectsintheframeofreference(patientorslidecoordinatesystem).Thisvaluetypeisimplementedinhighdicombythehighdicom.sr.Scoord3DContentItemPythonclass(ﬁgure4B).Inthecaseofrastergraphics,thepixeldataofSegmentationimagesarestoredoutsideoftheSRdocument,butspeciﬁcsegmentscanbereferencedfromwithintheSRdocumentviacontentitemsofvaluetypeIMAGE.(implementedbythehighdicom.sr.ImageContentItemPythonclass),whichincludesDICOMidentiﬁersforthereferencedimageobjectandsegmentscontainedtherein.ThestandardprovidesdiﬀerentSRtemplatesforavarietyofcommonclinicalusecasesanddi-agnosticstasks,suchasrecordingX-Raydoseexposureorreportingechocardiographyﬁndings.WechosetoimplementthemoregenerictemplateTID1500“MeasurementReport”inhighdicomforencodingannotations,becausethetemplateprovidesstandardcontentitemstodescribemea-surementsandqualitativeevaluationsofimagesaswellasindividualimageROIs(ﬁgure4C)andbecauseithasalreadybeensuccessfullyusedforstandardizedcommunicationofquantitativeimageanalysisresults[18,19].Importantly,sub-templatesthatcanbeincludedinTID1500toallowforencodingofannotations(ﬁgure4C).Withinthelibrary’sAPI,theseselectedtemplatesareimple-mentedbyPythonclasses,whicharederivedfromanabstractbaseclasshighdicom.sr.Template,whichisinturnderivedfrompydicom.Sequence(ﬁgure4D).TheconstructorsofthesePythonclassesrequirethedevelopertopasstherelevantdatavianamedparametersbutthenhandleitsinclusioninthetemplatewiththecorrectconceptnamesandaswellasensuringallconstraintsaresatisﬁed.WhendecodingSRdocuments,thehighdegreeofnestinginthedocumenttreeandthevari-ableorderofcontentitemsateachlevel,meansthatﬁndingaparticularcontentitemofinterestinthetreepotentiallyrequiresmultiplenestedloops.Further,asdescribedabove,eachcontentitemisacollectionofdataelementsthatmustﬁrstbeparsedandinterpretedasaunit.ThePythonclassesthatimplementSRtemplatesandindividualSRcontentitemsprovidemethodsandpropertiestofacilitatedataaccess.Usingtheprovidedmethods,measurementgroupswithinahighdicom.sr.ComprehensiveSRorhighdicom.sr.Comprehensive3DSRobjectcanbeﬁlteredbytheirﬁndingtype,ﬁndingsite,ortrackingidentiﬁers.Individualmeasurementsandqualitativeevaluationscontainedwithinthesegroupscansimilarlybeﬁlteredbytheirconceptname.Fur-thermore,highdicomclassesrepresentingSRtemplatesandcontentitemsprovideaccesstotheircontentitemsorvalues,respectively,throughPythonpropertiesthatreturnthedataeitherasabuilt-inPythontypeoracustomhighdicomtype(whichwilltypicallymatchthetypeoftheargumentpassedtotheconstructor).ResultsHavinglaidthefoundationthroughthedescriptionofthelibrary’sdesignandimplementation,wenowproceedtodemonstratingthecapabilitiesofthelibrary.Weconsideraconcreteusecaseofdevelopingmachinelearningmodelsforlungtumordetectioninbothpathologyandradiologyanddeployingthemodelsclinicallyusingacommonplatformandframeworkthatisapplicableindependentofthemedicaldisciplineorimagingmodality.Inthissection,weﬁrstdescribethestepsnecessarytoencodetheannotationsinDICOMusinghighdicom,includingthedescription9CDABTID 1500“Measurement Report”TID 1001“Observation Context”11-20-1TID 1002“Observer Context”TID 1006“Subject Context”TID 1003 “Person Observer Identifying Attributes”0-1TID 1004 “Device Observer Identifying Attributes”0-11TID 1009 “Subject Context, Specimen”0-nTID 1501“Measurements and Qualitative Evaluation Group”TID 300 “Measurement”1-nTID 1410“Planar ROI Measurements and Qualitative Evaluations”TID 1419 “ROI Measurement”1TID 1411“Volumetric ROI Measurements and Qualitative Evaluations”TID 1419 “ROI Measurement”1class Template(pydicom.Sequence): ...class ObserverContext(Template): ...class SubjectContext(Template): ...class ObservationContext(Template):    def __init__(        self,        person_observer_context: Optional[ObserverContext] = None,        device_observer_context: Optional[ObserverContext] = None,        subject_context: Optional[SubjectContext] = None    ):        ...class MeasurementsAndQualitativeEvaluations(Template): ...class PlanarROIMeasurementsAndQualitativeEvaluations(    MeasurementsAndQualitativeEvaluations):    ...class VolumetricROIMeasurementsAndQualitativeEvaluations(    MeasurementsAndQualitativeEvaluations):    ...class MeasurementReport(Template):    def __init__(        self,        observation_context: ObservationContext,        imaging_measurements: Sequence[            Union[                MeasurementsAndQualitativeEvaluations,                PlanarROIMeasurementsAndQualitativeEvaluations,                VolumetricROIMeasurementsAndQualitativeEvaluations,            ]        ],        **kwargs    ):        ...0-n0-nNameValueConcept Name Code Sequence> Code Value> Coding Scheme Designator> Code MeaningGraphic TypeGraphic DataFrame of Reference UIDSCOORD3DNameValueConcept Name Code Sequence> Code Value> Coding Scheme Designator> Code MeaningMeasured Value Sequence> Numeric Value> Measurement Units Code Sequence>> Code Value>> Coding Scheme Designator>> Code MeaningNUMNameValueConcept Code Sequence> Code Value> Coding Scheme Designator> Code MeaningCODEConcept Name Code Sequence> Code Value> Coding Scheme Designator> Code MeaningNameValueConcept Name Code Sequence> Code Value> Coding Scheme Designator> Code MeaningReferenced SOP Sequence> Referenced SOP Class UID> Referenced SOP Instance UID> Referenced Frame Number> Referenced Segment NumberIMAGEclass CodedConcept(pydicom.Dataset):    def __init__(        self, value: str, scheme_designator: str, meaning: str    ):         self.CodeValue = value        self.CodingSchemeDesignator = scheme_designator        self.CodeMeaning = meaningclass ContentItem(pydicom.Dataset):    def __init__(        self, value_type: str, name: CodedConcept, **kwargs    ):        self.ValueType = value_type        self.ConceptNameCodeSequence = [name]        ...class Scoord3dContentItem(ContentItem):    def __init__(        self,        name: CodedConcept,        graphic_type: str,        graphic_data: numpy.ndarray,        frame_of_reference_uid: str,         **kwargs    ):        super().__init__(value_type='SCOORD3D', name=name, **kwargs)        self.GraphicType = graphic_type        self.GraphicData = graphic_data.flatten().tolist()        self.FrameOfReferenceUID = frame_of_reference_uid        ...Figure4:EncodingofannotationsasDICOMStructuredReporting(SR)contentitemsandtem-platesforinclusionintoanSRdocument.A.SRcontentitemsofdiﬀerentvaluestypes.B.ImplementationofSRcontentitemsinhighdicombyclassesthatinheritfrompydicom.Dataset.C.SRtemplateTID1500“MeasurementReport”andincludedsub-templates.D.ImplementationofSRtemplatesinhighdicombyclassesthatinheritfrompydicom.Sequence.10ofthedetectedregionofinterest,theidentiﬁedﬁnding,andrelatedmeasurementsandqualitativeevaluations.WethenshowthroughaseriesofexperimentshowhighdicomcanstreamlineMLmodeltrainingandinferenceforthisusecase.HighdicomfacilitatesencodingofimageannotationsinDICOMformatStructuredreportingusingstandardmedicalterminologiesWhiletheapproachofusingstandardizedvocabulariesispowerfulandimportantforinteroperability,itcomplicatesworkingwiththedata.Forexample,comparingtwoconceptsforequalityrequirescomparisonoftheircodevalues,codingschemedesignators,andcodingschemeversions.Thehighdicom.sr.CodedConceptandthelower-levelpydicomtypesfacilitatetheuseofcodedconceptsforstructuredreportingofannotationsinPythonatahighlevelofabstraction(codesnippet1).importhighdicomashdfrompydicom.sr.codedictimportcodestumor=hd.sr.CodedConcept("108369006","SCT","Tumor")neoplasm=codes.SCT.Neoplasmassert(tumor==neoplasm)#Trueassert(tumor.value==neoplasm.value)#Trueassert(tumor.scheme_designator==neoplasm.scheme_designator)#TrueCodesnippet1:Codingofconceptsusingdomain-speciﬁcterminologies.“Neoplasm”and“Tumor”aresynonymsthatmaptothesamecodewithintheSNOMED-CTcodingscheme.DescribingROIevaluationsandmeasurementsTheCodedConcepttypeformsthebasisforadditionalhigher-levelcompositedatatypesforDICOMstructuredreportingsuchasSRcontentitems.Codesnippet2demonstratesexamplecontentitemsfortheencodingofatumorimageregionofinterest,thetumorﬁnding,andanassociatedtumormeasurement.importnumpyasnpimporthighdicomashdfrompydicom.sr.codedictimportcodes#Encodeboundingboxenclosingthetumorregionusing3Dspatialcoordinatesregion_item=hd.sr.ImageRegion3D(graphic_type=hd.sr.GraphicTypeValues3D.POLYGON,graphic_data=np.array([[45.,34.,0.],[49.,34.,0.],[49.,42.,0.],[45.,42.,0.],[45.,34.,0.]]),frame_of_reference_uid="1.2.3.4")#Encodeaqualitativeevaluationdescribingthe"morphology"as"adenocarcinoma"morphology_item=hd.sr.QualitativeEvaluation(name=codes.SCT.AssociatedMorphology,value=hd.sr.CodedConcept(value="8551/3",scheme_designator="ICDO3",meaning="Acinaradenocarcinoma"))#Encodeameasurementwithname"area"andofunit"squaremillimeter"area_item=hd.sr.Measurement(name=codes.SCT.Area,11value=12.07,unit=codes.UCUM.SquareMillimeter)Codesnippet2:Encodingofregionsofinterest,qualitativeevaluationsandmeasurements.ThisdemonstratesusingtheSCTvocabularybuiltintopydicomtoencodeaconceptnameas‘Morphology’,andadomain-speciﬁccodingscheme,theInternationalClassiﬁcationofDiseasesforOncology(ICD-O),tospecifytheexacttypeoftumorastheconceptvalue.Ofnote,theareameasurementinourexampleisencodedinawell-deﬁnedphysicalunit,aswouldbeexpectedforclinicaldecisionmaking.Thecorrespondingimageregionisdeﬁnedinthesamephysicalspace.InDICOM,imageregionsmaybedeﬁnedbyspatialcoordinateswithineitherthepixelmatrixofanindividualimage,or,asinthisexample,theframeofreference(the3Dpatient-orslide-basedphysicalcoordinatesystem).Whiletheformerappearsmorestraightforward,thelatterismoregeneralandallowsforannotationsderivedfromtransformedversionsoftheoriginalimageswitharbitraryaﬃnetransformations(rotations,scaling,etc.)aswellascrops.CreationofDICOMannotationobjectsThecomputervisionproblemoftumordetectioncouldbesolvedusingeitheranobjectdetectionorimagesegmentationmodel.Accordingly,theoutputofthesemodelsandtheannotationsusedtotrainthemcanbeencodedusingthehighdicom.sr.Comprehensive3DSR(codesnippet3)andhighdicom.seg.Segmentation(codesnippet4)classesrespectively.Ineithercase,thisinvolvesdescribingtheﬁndingandtheanatomi-calsiteoftheﬁndingaswellassupplyingrelevantcontextualmetadatasuchasthedeviceorpersonreportingtheobservation.However,notethatitisnotnecessarytospecifypatient,study,orspec-imeninformationsincehighdicomcopiesthismetadatadirectlyfromthesourceimagesprovidedasevidencetotheconstructor.importhighdicomashdfrompydicom.datasetimportDatasetfrompydicom.sr.codedictimportcodesdefload_image()->Dataset:...source_image=load_image()#Describethedevicethatisreportingtheobservationobservation_context=hd.sr.ObservationContext(observer_device_context=hd.sr.ObserverContext(observer_type=codes.DCM.Device,observer_identifying_attributes=hd.sr.DeviceObserverIdentifyingAttributes(uid=hd.UID(),name="GPU01")))#Describetheregionofinterestfindingandassociatedmeasurementsandevaluationsroi_description=hd.sr.PlanarROIMeasurementsAndQualitativeEvaluations(finding_type=codes.SCT.Neoplasm,finding_sites=[hd.sr.FindingSite(anatomic_location=codes.SCT.Lung)],referenced_region=region_item,measurements=[area_item],qualitative_evaluations=[morphology_item])#ConstructameasurementreportfromtheROI(s)measurement_report=hd.sr.MeasurementReport(observation_context=observation_context,procedure_reported=codes.SCT.ImagingProcedure,imaging_measurements=[roi_description]12)#ConstructanSRdocumentcontainingthemeasurementreportreport_document=hd.sr.Comprehensive3DSR(evidence=[source_image],content=measurement_report[0],series_number=1,series_instance_uid=hd.UID(),sop_instance_uid=hd.UID(),instance_number=1,manufacturer="MGHComputationalPathology",manufacturer_model_name="ObjectDetectionModel",software_version="v1",device_serial_number="XYZ")Codesnippet3:CreationofaDICOMComprehensive3DSRdocumentinstancetoencodepredictionsofanobjectdetectionmodelusingcontentitemsconstructedinsnippet2.fromtypingimportTupleimporthighdicomashdimportnumpyasnpfrompydicom.sr.codedictimportcodesfrompydicom.datasetimportDatasetdefload_image()->Tuple[Dataset,np.ndarray]:...defsegment_image(image:np.ndarray)->np.ndarray:...metadata,frames=load_image()probabilities=segment_image(frames)segment_description=hd.seg.SegmentDescription(segment_number=1,segment_label="Tumor",segmented_property_category=codes.cid7150.MorphologicallyAbnormalStructure,segmented_property_type=codes.cid7159.Neoplasm,algorithm_type=hd.seg.SegmentAlgorithmTypeValues.AUTOMATIC,algorithm_identification=hd.AlgorithmIdentificationSequence(name="highdicom-experiments",version="v1.0",family=codes.cid7162.ArtificialIntelligence))segmentation_image=hd.seg.Segmentation(source_images=[metadata],pixel_array=probabilities,segmentation_type=hd.seg.SegmentationTypeValues.FRACTIONAL,segment_descriptions=[segment_description],series_instance_uid=hd.UID(),series_number=1,sop_instance_uid=hd.UID(),instance_number=1,manufacturer="MGHComputationalPathology",manufacturer_model_name="SegmentationModel",software_versions="v1",device_serial_number="ServerXYZ")Codesnippet4:CreationofaDICOMSegmentationimageinstancetoencodepredictionsofasemanticimagesegmentationmodel.13HighdicomfacilitateseﬃcientloadinganddecodingofimagesandcorrespondingannotationsWhenitcomestotrainingamodelfortumordetection,annotationsmaybeprovidedintheformofeitherrastergraphicswithinaSegmentationimage,orvectorgraphicswithinanSRdocument.Inbothcases,highdicomprovidesmethodsthatsimplifyaccessto,andinterpretationof,therelevantcontentintheannotationSOPinstances.IfannotationsareprovidedasrastergraphicswithinaSegmentationimage,modeltrainingmayrequirecombiningbinarybitplanesfrommultiplesegmentsintheSegmentationimagetocreateasinglelabelmap,representedasaNumPyarray,inwhichpixelsencodetumoridentities.IfinsteadannotationsareprovidedasvectorgraphicswithinanSRdocument,thespatialcoordinatesofimageregionswillneedtobecollectedfromwithinthedocumentcontenttreeandpassedasNumPyarraystotrainingprocesses.Snippets5and6showexampleusageofthemethodsthathighdicomprovidesforthesepurposes.fromtypingimportListimporthighdicomashdimportnumpyasnpfrompydicomimportdcmreadfrompydicom.sr.codedictimportcodes#CreatehighdicomSRdocumentobjectfromadatasetreadfromfiledataset=dcmread(’annotation.dcm’)sr_document=hd.sr.Comprehensive3DSR.from_dataset(dataset)#EnsurethatthedocumentcontentisaTID1500"MeasurementReport"assertisinstance(sr_document.content,hd.sr.MeasurementReport)#Findboundingpolygonsforlungtumorregionsofinteresttumor_regions:List[np.ndarray]=[group.roi.valueforgroupinsr_document.content.get_planar_roi_measurement_groups(finding_type=codes.SCT.Neoplasm,finding_site=codes.SCT.Lung,graphic_type=hd.sr.GraphicTypeValues3D.POLYGON)]Codesnippet5:ExampleofparsingaDICOMSRdocumenttoﬁndlungtumorregions.fromtypingimportListimporthighdicomashdimportnumpyasnpfrompydicomimportdcmreadfrompydicom.sr.codedictimportcodes#Createhighdicomsegmentationobjectfromadatasetreadfromfiledataset=dcmread(’annotation.dcm’)seg_image=hd.seg.Segmentation.from_dataset(dataset)#Findnumbersofsegmentsforwhichthesegmentedpropertyis"tumor"tumor_segment_numbers:List[int]=seg_image.get_segment_numbers(segmented_property_type=codes.SCT.Neoplasm)#GetlabelmapwiththesesegmentsforagivenSOPinstancelabel_map:np.ndarray=seg_image.get_pixels_by_source_instance(source_sop_instance_uids=[’1.2.3.4’],segment_numbers=tumor_segment_numbers,combine_segments=True,relabel=True14)Codesnippet6:ExampleofparsingaDICOMSegmentationimagetocreatealabelmapoflungtumorregionsforagivensourceframe.Highdicomfacilitatesdecodingandencodingofannotationsduringmodeltrain-ingandinference,respectivelyToestablishaproof-of-conceptstandard-basedMLworkﬂowandtodemonstratetheutilityofthehighdicomlibraryforML,weperformedasetofexperimentsonthetrainingandevaluationofdeepconvolutionalneuralnetwork(CNN)modelsusingpublicly-availableslidemicroscopy(SM)andcomputedtomography(CT)imagedatasets.WeemphasizethatourintentistodemonstrateacompleteMLworkﬂowforpathologyandradiologyfullybasedonDICOM,ratherthancreatemodelswithoptimalperformanceorreachstate-of-the-artforaparticulartask.Forpathology,wetrainedandevaluatedmodelsusinglungcancercollectionsofslidemicroscopy(SM)imagesfromTheCancerImagingArchive(TCIA)[35]thatwereacquiredaspartofTheCancerGenomeAtlas(TCGA)LungAdenocarcima(LUAD)orLungSquamousCellCarinoma(LUSC)projectsandwhichweconvertedintoDICOMformataspreviouslydescribed[14,36].Forradiology,weusedthecollectionofCTimagesoftheLungImageDatabaseConsortium(LIDC)andImageDatabaseResourceInitiative(IDRI)(LIDC-IDRI)[35,37,38],whichwerealreadyavailableinDICOMformat.WeusedavailablemeasurementsandqualitativeevaluationsfortheseSMandCTﬁlesprovidedbyTCIAasimageannotations,whichweencodedinDICOMSRdocumentsorDICOMSegmentationimagesusinghighdicom(seesupplementarymethods),resultingintrainingsetsforCTlungnoduledetectionandSMimageclassiﬁcationencodedentirelywithinDICOMformat.Wedevelopedproof-of-conceptMLmodelsbasedonpublishedalgorithmsandimplemeteddatapre-andpostprocessingpipelinesforeachmodeltoloadmodelinputsfromDICOMSMorCTimageinstances,annotationsfromDICOMSRdocumentsorDICOMSegmentationimagesrespectively,andstoreoutputstoDICOMSRinstances.Forpathology,weimplementedaweakly-supervisedimageclassiﬁcationmodelusingmultipleinstancelearningwiththeobjectivetoclassifyindividualSMimageframesoflungtissuesectionsintoslidebackground,normallungtissue,lungadenocar-cinoma,orlungsquamouscellcarcinomasimilartopriorworkdescribedbyCoudrayetal[39].Tothisend,weusedamodiﬁedversionofaResNet-101model[40],whichweinitializedwithparam-etersfrompre-trainingonImageNet[41]andfurtheroptimizedusingSMimageframesandimageannotationsfromtheTCGAcollectionssimilartothealgorithmsdescribedbyLerousseauetal[42]andLuetal[43].Duringtraining,eachtrainingsamplewascreatedbyselectingoneormoreframesofanSMimagefromagivenseries(i.e.,digitalslide)togetherwiththecorrespondingimage-levelannotationsobtainedfromtheSRdocumentusinghighdicom.Duringinference,thedatapostpro-cessingpipelinecollectspredictedclassprobabilitiesforeachimageframe,constructslow-resolutionprobabilisticsegmentationmaskforeachclass(withpixelsrepresentingclassprobabilitistiesforin-dividualframes),andﬁnallyencodestheconstructedmasksinaDICOMSegmentationimagewithFRACTIONALSegmentationTypeandPROBABILITYFractionalSegmentationType(ﬁgure5upperpanel).Thepostprocessingpipelinefurtherthresholdstheindividualclassprobabilitypredictionstogenerateabinarysegmentationmaskforeachclass(normallungtissue,lungadenocarcinoma,orlungsquamouscarcinoma),performsaconnectedcomponentanalysisandborderfollowingtoﬁndthecontoursofROIsrepresentingclassinstances,andencodeseachdetectedROItogetherwithadditionalmeasurementsandqualitativeevaluationsinaDICOMComprehensive3DSRdocument(ﬁgure5lowerpanel).15SR DocumentSeries ofSM ImagesSM ImageObservation ContextTID 1500 Measurement ReportImaging MeasurementsAlgorithm IdentificationSEG ImageFRACTIONALSEG ImageBINARYSlideCoordinate SystemPixel MatrixCoordinate SystemSegment #1 - TissueSegment #2 - LUADSegment #3 - LUSCTID 1410 Planar ROI Measurements and Qualitative Evaluations(121071, DCM, “Finding”) (42798000, SCT, “Area”) (111030, DCM, “Image Region”) (108369006, SCT, “Neoplasm”)POLYGON   x1 y1 z1  x2 y2 z2  ...  x5 y5 z5 12.07 (mm2, UCUM, “Square millimeter”) (39607008, SCT, “Lung”)(8140/3, ICD03, “Adenocarcinoma, NOS”)(C34, I10C, “Bronchus and lung”)(363698007, SCT, “Finding Site”)(116676008, SCT, “Morphology”) (116677004, SCT, “Topography”)0.4160.2820.997Frame #55Model0.0430.8631.000Frame #43Model0.0000.9961.000Frame #33Model0.1531.0000.671Frame #27Model0.0080.000Frame #120.000Model4x10x20x0.01.0Class scoreFigure5:Schematicoverviewofoutputpost-processingpipelinesofthepathologymodel,whichclassiﬁesindividualimageframesofamulti-frameSMimageofalungtissuesectionspecimen.Outputtedscoresgettransformedintoasegmentationmaskfromwhichboundingboxesofthetumorregionsarederived.Thecoordinatesoftheboundingboxverticesarestoredas3Dspatialcoordinatesinthereferenceslidecoordinatesystem.Forradiology,weimplementedanobjectdetectionmodeltodetectlungnodulesinindividualCTslicesofthechest.Weusedanoﬀ-the-shelfimplementationofthewidely-usedRetinaNetconvolutionalneuralnetwork[44]availablewiththetorchvisionpackage1.Speciﬁcally,weusedaRetinaNetmodelwiththeResNet-50backbone[40]andinitializedthemodelwithweightsfrompre-trainingontheImageNetdataset[41].Duringtraining,eachtrainingsamplewascreatedbyselectingarandomCTimageframe(2Daxialslice)fromagivenseries.TheannotationsencodedinDICOMSegmentationimageswerereadusinghighdicomandtheboundingboxcontainingeachnoduleintheslicewascalculatedon-the-ﬂyfromthecontainedsegmentsandusedasagroundtruthlabelforsupervisedtrainingoftheRetinaNetmodel.Thepost-processingpipelineforthechestCTmodelcollectedpredictedboundingboxesandtheirdetectionscoresoutputtedbytheRetinaNetmodelforeveryframeintheCTseriesandencodedtheminaDICOMComprehensive3DSRdocument,withvectorgraphicsusedtorepresentboundingboxcoordinatesanddetectionscoresencodedasameasurementoftheregionrepresentedbytheboundingbox(ﬁgure6).AnnotationsgeneratedbyhighdicomcanbestoredinimagemanagementsystemsusingDICOMwebservicesandvisualizedusingDICOM-compliantdisplaysystems1https://pytorch.org/vision/0.8/index.html16Series ofCT ImagesPatientCoordinate SystemPixel MatrixCoordinate SystemCT Image #39CT Image #74CT Image #77Model[ [412, 432, 231, 242] ][ ][ [110, 127, 191, 217] ]ModelModelSR DocumentObservation ContextTID 1500 Measurement ReportImaging MeasurementsAlgorithm IdentificationTID 1410 Planar ROI Measurements and Qualitative Evaluations(121071, DCM, “Finding”) (42798000, SCT, “Area”) (111030, DCM, “Image Region”) 2.14 (cm2, UCUM, “Square centimeter”)(39607008, SCT, “Lung”)0.76 ({0:1}, UCUM, “Range: 0:1”)(363698007, SCT, “Finding Site”)(246262008, SCT, “Score”)(27925004, SCT, “Nodule”)POLYGON   x1 y1 z1  x2 y2 z2  ...  x5 y5 z5 412110110191191412231231Figure6:Schematicoverviewofoutputpost-processingpipelineoftheradiologymodel,whichdetectslungnodulesinimageframesofsingle-frameCTimagesofthethoraxandoutputsboundingboxesoflungnoduleregions.Thecoordinatesoftheboundingboxverticesarestoredas3Dspatialcoordinatesinthereferencepatientcoordinatesystem.Aftermodeltraining,weselectedonepathologyandradiologymodelforfurtherclinicalevaluationanddeployeditintoaproduction-likeenvironment,consistingofanimagemanagementsystem(IMS)withaDICOMwebinterface[45]andDICOM-compliantimagedisplaysystems.Speciﬁcally,adcm4chee-arc-lightarchive2servedastheIMSandwestoredSMandCTimagesintheIMSviaDICOMwebRESTfulservicesusingthedicomweb-clientPythonlibrary[14]3.Uponinference,thedatapreprocessingpipelinesretrievedDICOMSMorCTimagesfromtheIMSovernetworkusingthedicomweb-client,readandinterpretedtheimagemetadataandpixeldatausingpydicom,andpassedthepixeldataasinputstothemodelasNumPyarrays.ModeloutputsreceivedasNumPyarrayswereencodedasDICOMSRdocumentsinthedatapostprocessingpipelineusinghighdicomandstoredbackintheIMSovernetworkusingthedicomweb-client.Forradiology,wevisualizedthegroundtruthlungnodulesusingtheOHIF4viewer,whichretrievedtheDICOMSegmentationimagesovernetworkusingthedicomweb-clientlibraryanddisplayedeachsegmentasarastergraphicontopofthecorrespondingCTimages(supplementaryﬁgureS3B).WeadditionallyvisualizeddetectedROIsusingtheopen-source3DSlicer5software(supplementary2https://github.com/dcm4che/dcm4chee-arc-light3https://github.com/mghcomputationalpathology/dicomweb-client4https://github.com/ohif/viewers5https://slicer.org17ﬁgureS3B).Forpathology,wevisualizeddetectedlungtumorregionsusingtheSlim6viewer,whichretrievedtheDICOMSRdocumentsovernetworkusingthedicomweb-clientJavaScriptlibrary7anddisplayedthespatialcoordinatesofeachROIcontainedintheSRdocumentsasavectorgraphicontopofthecorrespondingSMimages(supplementaryﬁgureS4).DiscussionThemaincontributionsofthispaperare:(i)ThedemonstrationthatimageannotationscanbeencodedandexchangedinDICOMformatusingexistingDICOMIODsandservices,respectively.(ii)Thedevelopmentofasoftwarelibrarythatprovidesahigh-levelapplicationprogramminginterface(API)forthePythonprogramminglanguagetofacilitatecreationofDICOMobjectsforstorageofimage-derivedinformation,includingimageannotations,aswellasaccessingandinterpretinginformationstoredinDICOMobjects.(iii)Theestablishmentofastandard-basedworkﬂowforMLmodeltrainingandinferencethatisgenerallyapplicableacrossdiﬀerentimagingmodalities,computervisionproblems,andmedicaldisciplines.IndevelopingthehighdicomlibraryandestablishinganMLworkﬂowbasedonDICOM,wemadeseveralobservationsthatmeritfurtherdiscussion.Clinicaluseofmachinelearningmodeloutputsinpathologyandradiologyre-quiresdomain-speciﬁcmetadataMedicalimagesandimageannotationsmustnotonlycontaintheactualdata,suchasthepixeldataincaseofanimage,butrequireadditionalmetadatathatenableinterpretationanduseofthedata.Suchmetadatacanbegroupedintoinformationrelatedtodatarepresentation,informationaboutthedataacquisitionprocessandequipment,andinformationrelatedtotheclinicalcontextinwhichthedatawasacquired,includingidentifyinganddescriptiveinformationaboutthepatient,study,andspecimens.Thiscontextualinformationthatdescribeshowthedatarelatestotherealworldiscrucialforunambiguousinterpretationofmedicalimagesaswellasanyregionsofinterest,measurements,orqualitativeevaluationsderivedfromthem.Toensurethatclinicaldecisionsbasedonthisinformationaremadefortherightpatientandspecimenandinthecorrectclinicalsetting,real-worldentitiesneedtobeuniquelyidentiﬁablethroughoutthedigitalworkﬂow.Assuchitisdesirabletoestablishanunambiguousassociationbetweenthedigitalinformation(imagesandimageannotations)ontheonehandandclinicallyrele-vantreal-worldentities(patients,specimens,etc.)ontheotherhandbyincludingclinicalidentiﬁersintodigitalobjects.Thisfurthermorefacilitatesexchangeofinformationbetweendepartmentsandinstitutionsupontransferandreferralofpatients.DICOMspeciﬁesstandardinformationobjectdeﬁnitionsandattributestostoreandexchangedigitalimagesandimage-derivedinformationto-getherwiththerelevantclinicalidentiﬁersascompositeobjects.ThehighdicomAPIfacilitatesaccesstoandcreationofsuchstandardDICOMobjectsusingthePythonprogramminglanguageandtherebyenablesdataengineersandscientiststodevelopMLmodelsandsystemsthatcanreceiveinputsandreturnoutputsthatincluderelevantidentiﬁersforclinicalapplication.Addi-tionally,inmanycasesincludingpatientinformationandreferencestothesourceimages,highdicomwillﬁndandcopytherelevantmetadatafromthedatasetofthesourceimagetoreducetheroomforhumanerrorasfaraspossible.6https://github.com/herrmannlab/slim7https://github.com/dcmjs-org/dicomweb-client18Inadditiontoidentiﬁers,DICOMobjectscontaindescriptivemetadataabouttheimagingtarget(patientorspecimen),theimagingmodalityandprocedure,theanatomicallocationoftheimagingorsurgicalprocedure,andincaseofpathologythepreparationofthespecimen.Thisinforma-tioncanbecriticalfortheinterpretationofimagesorimageannotationsbyMLsystemsduringmodeltrainingorinferenceaswellasbyothersystemsthatuseorinterpretmodeloutputs.Mostimportantlythisdescriptivemetadataallowsautomatedsystemstodecidewhetherornotagiveninformationobjectmaybeappropriatetouseinthecontextoftheintendeduseorselectoneofseveralavailableobjectsforanalysisordisplay[46].Descriptivemetadataisalsousefulforper-formingmodelvalidationanderroranalysistodeterminegroupsofinputs,accordingtopatientdemographicinformation,pre-analyticspecimenpreparationvariables,orimageacquisitionpa-rameters,uponwhichmodelsareunder-performing.Furthermore,theDICOMstandardprovidesmechanismsfordescribingtheimageanalysisalgorithm(name,version,etc.)aswellasthecom-pletenessorvalidityofanalysisresultsatvariousstagesoftheclinicaldecisionmakingprocess.Forexample,theDICOMSRIODsincludeattributesthatallowclinicaluserstoverifyor,ifnecessary,completeorcorrectMLmodeloutputs,torecordtheveriﬁcationormodiﬁcationactivity,andtocreateanaudittrailthatestablishestherelationshipbetweenthedocumentcontainingtheveriﬁedormodiﬁedcontentandthepredecessordocumentcontainingtheunveriﬁedmodeloutputs.ThesemechanismsarecriticalforsafeclinicalapplicationofMLmodels,sincetheiroutputsaregenerallyintendedforclinicaldecisionsupportratherthanindependentdecisionmaking[47]andthusrequirereviewbyaclinicalexpertbeforeinclusionintothemedicalrecord.ThehighdicomlibraryenablesdeveloperstoaccessrelevantdescriptiveinformationinreceivedDICOMobjectsuponpreprocessingorincludesuchinformationintogeneratedDICOMobjectuponpost-processingandtherebymakeitavailabletodownstreamclinicalsystems.Thehigh-levelandwell-testedabstractionsprovidedbyhighdicomallowdeveloperstoachievethisgoalwithonlyafewlinesofPythoncode.Standardcodingschemesenableunambiguousinterpretationofimageannota-tionsSubtlediﬀerencesinthedescriptionofimagingﬁndingscanleadtodrasticallydiﬀerenttreatmentdecisions.Toensurethatimageannotationscanbeinterpretedunambiguouslybybothcliniciansanddevicesorautomatedsystemsthatmayactupontheinformation,thetermsusedtodescribeandreportannotationsneedtobewell-deﬁned.DICOMstructuredreportingusescodesofestab-lishedclinicalterminologiesandontologiestodescribeimage-derivedinformationratherthanusingfreetext.Forexample,whilemanywordsinEnglishandotherlanguagesmaybeusedtorefertoa“tumor”astheﬁndingtypeoftheROI,theconceptcanbeunambiguouslyrepresentedacrosslanguagesanddomainsbytheSNOMED-CTcode“108369006”.TheuseofstructuredreportsandstandardizedcodesfacilitatesinterpretationofimageannotationsbybothhumansandmachinesandisthereforecriticalforenablingstructuralandsemanticinteroperabilitybetweenMLmodelsandclinicalsystems.Thestandard-basedapproachfurtherfacilitatesthere-useofdatabeyondthescopeoftheprojectorusecaseforwhichtheywereinitiallycreated.Whilethereareseveraladvantagestousingcodes,theyarecumbersometoworkwithandincreasethecomplexityofMLprogramsandarethusinourexperienceoftenfrownedonbydevelopers.Thehighdicomlibraryprovidesdatastructuresandmethodsthatabstractthecodesandsigniﬁcantlysimplifyusingandoperatingoncodedconcepts.Whilecodeschosenfromwell-establishedcodingschemescansigniﬁcantlyimproveinteroperability,thechoiceoftheappropriatecodecanstillposeasigniﬁcantchallengetobothdevelopersandclinicalexperts.Thehighdicomlibrarydoesnot(andcannot)fullysolvethisproblem.Indeed,in19practiceitmaybethecasethatnostandardcodedconceptaccuratelydescribestheannotationandacustomcodingschemeisrequired.DICOMallows,andpydicomandhighdicomsupport,thedeﬁnititionofsuchcustomcodingschemeswiththeconventionofapreﬁxof“99“followedbyanidentifyingtextstring.Consumersofcustomcodedconceptsshoulddetectthisconditionandseekout-of-bandinformationforcorrectinterpretationoftheannotations.However,foralargerangeofcommonclinicalusecasesthelibrary(togetherwiththeunderlyingpydicomlibrary)exposesvaluesetsdeﬁnedintheDICOMstandardviaabstractions,andbydependingontheseabstractionsthroughoutitsAPI,encouragesdeveloperstochoosecodesfromthesepredeﬁnedsets.Encodingimageregionsinawell-deﬁnedcoordinatesysteminthree-dimensionalphysicalspaceallowsforclinically-actionablemeasurementsEstablishinganunambiguousspatialrelationshipbetweenROIsandtheircorrespondingsourceimagesfordisplayorcomputationalanalysisrequiresacommonframeofreference,whichdeﬁnesthecoordinatesystemtouniquelylocalizebothimagesorimageregionswithrespecttotheimagingtarget(thespecimeninpathologyorthepatientinradiology)withbothpositionandorientation.ManyapplicationssimplyspecifyROIsrelativetothepixelmatrixofanimageinpixelunits.However,thissimpleapproachisproblematicforinteroperability,becausetheimagepixelgridformsanill-deﬁnedcoordinatesystemandthelocation(oﬀset,rotation,andscale)ofanimagewithrespecttotheimagingtargetchangesuponspatialtransformationoftheimage.DICOMspeciﬁesaframeofreferenceforbothslide-basedandpatient-basedcoordinatesystems,whichenablesaccurateandpreciselocalizationofaROIwithrespecttothepatientorthespecimenontheslideindependentofwhetheraﬃnetransformationshavebeenappliedtoimages.DeﬁningROIsinphysicalspaceinmillimeterunitsfurtherhastheadvantagethatspatialROImeasurementssuchasdiameterorareacanbereadilytakeninthisframeofreferencewithouttheneedtotransformcoordinates,aprocessthatcanbeerrorproneandresultinincorrectmeasurementswithpotentiallyseriousclinicalimplications.Thehighdicomlibraryenablesdeveloperstoworkwithboth2Dpixelmatrixand3Dframeofreferencecoordinatesandprovidesdevelopersmethodstoreadilyconvertcoordinatesbetweenthediﬀerentcoordinatesystems.ScalingtolargenumbersofimageannotationsinthecontextofslidemicroscopyimaginginpathologyAsdemonstratedinthispaper,encodingofROIsinSRdocumentsworksforbothpathologyandradiology.However,thedeeplynestedstructureofSRdocumentsdoesnotscalewelltoobjectdetec-tionproblemsinpathology,wheremillionsofcellsornucleimaybedetectedperwholeslideimage.Toaddressthischallenge,DICOMWorkingGroup26Pathology(WG-26)hasdevelopedasupple-mentfortheDICOMstandardthatproposestheintroductionofaMicroscopyBulkSimpleAnnota-tionsIODandAnnotation(ANN)modalityspeciﬁcallydesignedforthestorageandexchangeofalargenumberofimageannotationsinformofspatialcoordinates[48].ThegraphictypesusedintheANNobjectshavebeenharmonizedwiththoseinSRs,andtheirstructureissimilartothatofSEGimages.ThissupplementwasrecentlyapprovedandincorporatedintotheDICOMstandardandisnowimplementedinhighdicomasahighdicom.ann.MicroscopyBulkSimpleAnnotationsSOPclass,reusingtheexistingbuildingblocksofthelibraryforcodedconceptsandspatialcoordinates.20AbstractingthecomplexityofthestandardwithoutoversimplifyingmedicalimagingusecasesDICOMistheubiquitousstandardforrepresentationandcommunicationofmedicalimagedataandstandardizesmanyaspectsoftheimagingworkﬂowtoenableinteroperabilityintheclinicalsetting.However,DICOMisoftencriticizedbythebiomedicalimagingresearchcommunityforitselaboratenessandalternativedataformatshaveemergedintheresearchsettingthatareintendedtosimplifyaccesstoandstorageofdatabyresearchersthatdonotwanttocopewithintricaciesofthestandard[49].TheﬁrststepinanimageanalysispipelineisthusoftentheconversionofDICOMobjectsintoanalternativeformatthatisconsideredmoresuitableforresearchuse[50].WhileconversionofclinicallyacquiredDICOMobjectsintoanotherformatmayworkwellwithinthelimitedscopeofaresearchproject,thereverse,i.e.,theconversionofagivenresearchoutputintostandardDICOMrepresentation,isgenerallynotpossible,sinceimportantcontextualinformationislostalongtheway[51].ManyoftheattributesofDICOMobjectsthatareregardedsuperﬂuousbyresearchersandarereadilyremovedforeaseofuse,arecrucialforinteroperabilitywithclinicalsystemsandforcorrectrepresentationandinterpretationofthedatainclinicalpractice.WearguethatthediscussionregardingtheestablishmentandadoptionofstandardsforclinicaldeploymentofMLmodelsandintegrationoftheiroutputsintoclinicalworkﬂowsshouldbeguidedprimarilybytherequirementsofclinicalsystemsandcliniciansforinterpretabilityandclinicalde-cisionmaking,ratherthancurrentpracticeswithinresearchcommunities.TheDICOMstandardhasbeenevolvingovermanyyearsthroughcontinuouscollaborationofaninternationalgroupofexpertsandadiversesetofstakeholdersbasedonaconsiderateandcontrolledprocessthattakesavarietyofusecasesaswellaslegalandregulatoryaspectsintoaccount.Whilethecomprehensive-nessandinclusivenessofthestandardhasadvantages,ithasalsoresultedinsigniﬁcantcomplexityanddemandsanimplementationthatexposestheusefulpartsofthestandardsthroughalayerofabstraction.Thehighdicomlibrarystrikesaﬁnebalance,byprovidinganAPIthathidesasmanydetailsoftheDICOMstandardaspossiblefrommodeldevelopers,whileacknowledgingthatmedicalimagingiscomplexandthateﬀortsaimingforDICOMabstractionshouldinvolvetechnicalanddomainexpertstoavoidoversimpliﬁcationwithdetrimentaleﬀectsoninteroperabilityandul-timatelypatientsafety.TheresultisanAPIthatabstractstheintricatestructureofDICOMdatasets,butretainsfullanddirectaccesstoallDICOMattributesandstaysclosetotheterminologyoftheDICOMdatamodelstoavoidanyambiguities.BridgingthegapbetweenmodeldevelopmentinresearchandmodeldeploymentinclinicalpracticeResearchers,medicaldevicemanufacturers,andhealthcareprovidersaregenerallyinterestedinacceleratingthetranslationofresearchﬁndingsintoclinicalpracticeandenablepatientstogetaccesstoandbeneﬁtfromdiagnosticandtherapeuticinnovations.However,theincentivesforthediﬀerentstakeholderswhoparticipateinthetranslationprocessatdiﬀerenttimepointsfrommodeldevelopmenttodeploymentarenotnecessarywellaligned.Currently,theproductiondeploymentofanMLmodelisgenerallynotamajorconcerntomodeldevelopers,whoprimarilyoperateinaresearchenvironment.Thedeveloperoftendoesnotreceiveatechnicalspeciﬁcationagainstwhichthemodelshouldbedevelopedandisunawareoftheenvironmentintowhichthemodelshouldultimatelybedeployedforclinicalvalidation.Asaconsequence,thestructureofdataoutputtedbyMLmodelsdevelopedinresearchsettingsaregenerallyhighlycustomizedtowardsaparticularresearchprojectandspeciﬁcusecaseandlackidentifyingordescriptivemetadatarelevantforclinicalapplication(seeabove).Further,currentMLmodelsstoredatainavarietyofproprietary21formatsthatareincompatiblewithclinicalsystems,whichgenerallyrelyonaDICOMinterfacefordataexchange.Together,thesefactorsimpedethedeploymentofanMLmodelanditsintegrationintoexistingclinicalworkﬂowsforvalidationorapplication.OneopportunityforstreamliningthisprocessistorelyonDICOMasacommonformatandinterfacefordataexchangeduringbothmodeldevelopmentanddeployment.Inourexperiments,wedemonstratedthathighdicommakesfeasibleafullyDICOM-basedworkﬂowinwhichallﬁlesstoredonstoragedevicesareinDICOMformatwithminimalincreaseincomplexityforthedeveloper.Adaptingamodeldevelopedinsuchaworkﬂowforclinicaldeploymentbecomesastraightforwardtask.Acommonusefornon-DICOMformatsisforstorageofintermediateresultswithintheinputim-agepreprocessingpipeline,suchastheresultsofimageregistrationoperations.AlimitationofourproposedDICOM-onlyworkﬂowisthatitassumesthatmodeltrainingandinferencepre-processingpipelinesoperatedirectlyonthesourceimages.However,wearguethatmodelsdevelopedforeven-tualclinicaldeploymentmusthaveinputpreprocessingpipelinesthatareabletooperateeﬃcientlyfromtherawsourcedataandassuchhavingthisconstraintinplacethroughmodeldevelopmentprocesssimpliﬁesdeployment.Furthermore,intermediateresultscouldalsoberepresentedinDI-COMformat(e.g.,usingtheSpatialRegistrationIODforimageregistrationresults)andfutureversionsofhighdicommayprovidetoolstohelpwiththecreationandaccessofintermediateresultsinDICOMformat.Commonplatforms,services,andtoolswillfacilitateenterprisemedicalimaging,interdisciplinaryresearch,andintegrateddiagnosticsStandardizationofimages,imageannotations,andmodelpredictionsbetweenpathologyandradi-ologyopensnewavenuesforenterprisemedicalimaging,interdisciplinaryquantitativebiomedicalimagingresearch,andintegratedimage-baseddiagnostics.Despiteuniquechallengesandusecasesforimagemanagementindigitalpathologyandradiology,thereareopportunitiesforstreamliningtheinvestmentintoanduseofITinfrastructureandplatformsacrossmedicaldisciplineswithintheenterprise.GiventhatmosthospitalsalreadyhaveanexistingmedicalimaginginfrastructurebasedonDICOM,encodingimageannotationsinDICOMformatmaylowerthebarrierforintegrationofMLsystemsintoclinicalworkﬂows.RelyingontheDICOMstandardmayfurtherpromoteinterdisciplinarybiomedicalimagingre-searchby,forexample,clearingthewayfortheuseofannotationsofslidemicroscopyimagesinpathologyasgroundtruthfortrainingMLmodelsforanalysisofCTimagesinradiologyorviceversa.Furthermore,leveragingastandarddataformatandcommunicationinterfaceprovidesanopportunitytosynthesizediﬀerentimagingmodalitiesandinterpretpathologyandradiologyMLmodeloutputsside-by-side.Inthispaper,wedemonstratethathighdicomfacilitatesthecreationandinterpretationofimageannotationsindependentofaspeciﬁcmedicalimagingmodality,disci-pline,department,orinstitution.WefurthershowthatdatacanbeexchangedandstoredusingDICOM-compatibleimagemanagementsystems,whichalreadyexistinhospitalsworldwideandareincreasinglybeingadoptedbybiomedicalimagingresearchinitiativesaroundtheworld.Forexample,theNationalCancerInstitute’sImagingDataCommons(IDC)intheUnitedStateswillmakelargepubliccollectionsofpathologyandradiologyimages,imageannotations,andimageanalysisresultsavailableinDICOMformat[52].Thehighdicomlibrarywillallowresearcherstoleveragetheseresourcesandenablethemtoreadilysharetheirresultsandmakethemusablebyotherresearchers.WethereforeseethepotentialforhighdicomtostreamlinethedevelopmentanddeploymentofMLmodelsacrossdepartmentalboundaries,acceleratethetranslationoftechno-logicalinnovationsfromresearchintoclinicalpractice,andtoassistintherealizationofAIin22healthcare.ConclusionThehighdicomlibraryabstractsthecomplexityoftheDICOMstandard,andexposesmedicalimagingdatatoMLmodeldevelopersviaapythonicinterfacethattiesintothescientiﬁcPythonecosystemformachinelearningandimageprocessingandallowsdatascientiststothinkofimagingdataatahighlevelofabstractionwithouthavingtoworryaboutthelow-leveldetailsandrulesoftheDICOMstandard.Focusingontheusecaseofdetectinglungtumorsinslidemicroscopyimagesofsurgicaltissuesectionspecimensaswellasincomputedtomographyimagesofthechest,weexaminedexamplesfortheinterpretationofDICOM-encodedimageannotationsduringmodeltrainingandencodingofmodeloutputsduringmodelinference.Throughaseriesofexperiments,wehavedemonstratedtheutilityofthelibraryforthedevelopmentofMLmodelsandshownthat,byrelyingontheDICOMstandard,thelibraryenablesinteroperabilityofthedevelopedMLmodelswithcommerciallyavailableDICOM-compliantinformationsystemsandallowsforunambiguousinterpretationofmodeloutputsinclinicalcontextindependentofthespeciﬁcmedicalimagingmodalityordiscipline.ByfacilitatingtheuseofDICOMthroughoutthemodeldevelopmentanddeploymentprocess,highdicomhasthepotentialtobridgethegapbetweenresearchandclinicalapplicationandtherebystreamlineclinicalintegrationandvalidationofMLmodels.References[1]LeCun,Y.,Bengio,Y.,andHinton,G.“Deeplearning”.Nature521.7553(2015),pp.436–444.[2]Campanella,G.,Hanna,M.G.,Geneslaw,L.,Miraﬂor,A.,WerneckKraussSilva,V.,Busam,K.J.,Brogi,E.,Reuter,V.E.,Klimstra,D.S.,andFuchs,T.J.“Clinical-gradecomputationalpathologyusingweaklysuperviseddeeplearningonwholeslideimages”.NatureMedicine25.8(July2019),pp.1301–1309.[3]Lu,M.Y.,Chen,T.Y.,Williamson,D.F.K.,Zhao,M.,Shady,M.,Lipkova,J.,andMah-mood,F.“AI-basedpathologypredictsoriginsforcancersofunknownprimary”.Nature594.7861(June2021),pp.106–110.[4]Laak,J.vander,Litjens,G.,andCiompi,F.“Deeplearninginhistopathology:thepathtotheclinic”.NatMed27.5(May2021),pp.775–784.[5]McKinney,S.M.,Sieniek,M.,Godbole,V.,Godwin,J.,Antropova,N.,Ashraﬁan,H.,Back,T.,Chesus,M.,Corrado,G.S.,Darzi,A.,Etemadi,M.,Garcia-Vicente,F.,Gilbert,F.J.,Halling-Brown,M.,Hassabis,D.,Jansen,S.,Karthikesalingam,A.,Kelly,C.J.,King,D.,Ledsam,J.R.,Melnick,D.,Mostoﬁ,H.,Peng,L.,Reicher,J.J.,Romera-Paredes,B.,Side-bottom,R.,Suleyman,M.,Tse,D.,Young,K.C.,DeFauw,J.,andShetty,S.“InternationalevaluationofanAIsystemforbreastcancerscreening”.Nature577.7788(Jan.2020),pp.89–94.[6]Choy,G.,Khalilzadeh,O.,Michalski,M.,Do,S.,Samir,A.E.,Pianykh,O.S.,Geis,J.R.,Pandharipande,P.V.,Brink,J.A.,andDreyer,K.J.“CurrentApplicationsandFutureImpactofMachineLearninginRadiology”.Radiology288.2(Aug.2018),pp.318–328.23[7]Ardila,D.,Kiraly,A.P.,Bharadwaj,S.,Choi,B.,Reicher,J.J.,Peng,L.,Tse,D.,Etemadi,M.,Ye,W.,Corrado,G.,Naidich,D.P.,andShetty,S.“End-to-endlungcancerscreeningwiththree-dimensionaldeeplearningonlow-dosechestcomputedtomography”.NatMed25.6(June2019),pp.954–961.[8]Hosny,A.andAerts,H.J.W.L.“Artiﬁcialintelligenceforglobalhealth”.Science366.6468(Nov.2019),pp.955–956.[9]Allen,B.,Seltzer,S.E.,Langlotz,C.P.,Dreyer,K.P.,Summers,R.M.,Petrick,N.,Marinac-Dabic,D.,Cruz,M.,Alkasab,T.K.,Hanisch,R.J.,Nilsen,W.J.,Burleson,J.,Lyman,K.,andKandarpa,K.“ARoadMapforTranslationalResearchonArtiﬁcialIntelligenceinMedicalImaging:Fromthe2018NationalInstitutesofHealth/RSNA/ACR/TheAcademyWorkshop”.JAmCollRadiol16.9PtA(Sept.2019),pp.1179–1189.[10]Granter,S.R.,Beck,A.H.,andPapke,D.J.“AlphaGo,DeepLearning,andtheFutureoftheHumanMicroscopist”.ArchivesofPathology&LaboratoryMedicine141.5(2017),pp.619–621.[11]Abels,E.,Pantanowitz,L.,Aeﬀner,F.,Zarella,M.D.,Laak,J.vander,Bui,M.M.,Ve-muri,V.N.,Parwani,A.V.,Gibbs,J.,Agosto-Arroyo,E.,Beck,A.H.,andKozlowski,C.“Computationalpathologydeﬁnitions,bestpractices,andrecommendationsforregulatoryguidance:awhitepaperfromtheDigitalPathologyAssociation”.J.Pathol.(July2019).[12]Roth,C.J.,Lannum,L.M.,andPersons,K.R.“AFoundationforEnterpriseImaging:HIMSS-SIIMCollaborativeWhitePaper”.29.5(Oct.2016),pp.530–538.[13]Clunie,D.,Hosseinzadeh,D.,Wintell,M.,DeMena,D.,Lajara,N.,Garcia-Rojo,M.,Bueno,G.,Saligrama,K.,Stearrett,A.,Toomey,D.,Abels,E.,Apeldoorn,F.V.,Langevin,S.,Nichols,S.,Schmid,J.,Horchner,U.,Beckwith,B.,Parwani,A.,andPantanowitz,L.“DigitalImagingandCommunicationsinMedicineWholeSlideImagingConnectathonatDigitalPathologyAssociationPathologyVisions2017”.JournalofPathologyInformatics9(2018),p.6.[14]Herrmann,M.D.,Clunie,D.A.,Fedorov,A.,Doyle,S.W.,Pieper,S.,Klepeis,V.,Le,L.P.,Mutter,G.L.,Milstone,D.S.,Schultz,T.J.,Kikinis,R.,Kotecha,G.K.,Hwang,D.H.,Andriole,K.P.,Iafrate,A.J.,Brink,J.A.,Boland,G.W.,Dreyer,K.J.,Michalski,M.,Golden,J.A.,Louis,D.N.,andLennerz,J.K.“ImplementingtheDICOMStandardforDigitalPathology”.JournalofPathologyInformatics9(2018),p.37.[15]Dash,R.,Jones,C.,Merrick,R.,Haroske,G.,Harrison,J.,Sayers,C.,Haarselhorst,N.,Wintell,M.,Herrmann,M.,andMacary,F.“Integratingthehealth-careenterprisepathologyandlaboratorymedicineguidelinefordigitalpathologyinteroperability”.JPatholInform12.16(Mar.2021).[16]IHEPaLMTechnicalCommitteeincollaborationwithDICOMWG26.IHEPathologyandLaboratoryMedicineTechnicalFrameworkSupplementDigitalPathologyWorkﬂow–ImageAcquisition(DPIA).https://www.ihe.net/uploadedFiles/Documents/PaLM/IHE_PaLM_Suppl_DPIA.pdf.Aug.2020.[17]Wilkinson,M.D.,Dumontier,M.,Aalbersberg,I.J.,Appleton,G.,Axton,M.,Baak,A.,Blomberg,N.,Boiten,J.W.,SilvaSantos,L.B.da,Bourne,P.E.,Bouwman,J.,Brookes,A.J.,Clark,T.,Crosas,M.,Dillo,I.,Dumon,O.,Edmunds,S.,Evelo,C.T.,Finkers,R.,Gonzalez-Beltran,A.,Gray,A.J.,Groth,P.,Goble,C.,Grethe,J.S.,Heringa,J.,Hoen,P.A.,Hooft,R.,Kuhn,T.,Kok,R.,Kok,J.,Lusher,S.J.,Martone,M.E.,Mons,A.,Packer,A.L.,Persson,B.,Rocca-Serra,P.,Roos,M.,Schaik,R.van,Sansone,S.A.,Schultes,E.,Sengstag,24T.,Slater,T.,Strawn,G.,Swertz,M.A.,Thompson,M.,Lei,J.vander,Mulligen,E.van,Velterop,J.,Waagmeester,A.,Wittenburg,P.,Wolstencroft,K.,Zhao,J.,andMons,B.“TheFAIRGuidingPrinciplesforscientiﬁcdatamanagementandstewardship”.SciData3(Mar.2016),p.160018.[18]Fedorov,A.,Clunie,D.,Ulrich,E.,Bauer,C.,Wahle,A.,Brown,B.,Onken,M.,Riesmeier,J.,Pieper,S.,Kikinis,R.,Buatti,J.,andBeichel,R.R.“DICOMforquantitativeimagingbiomarkerdevelopment:astandardsbasedapproachtosharingclinicaldataandstructuredPET/CTanalysisresultsinheadandneckcancerresearch”.PeerJ4(2016),e2057.[19]Herz,C.,Fillion-Robin,J.C.,Onken,M.,Riesmeier,J.,Lasso,A.,Pinter,C.,Fichtinger,G.,Pieper,S.,Clunie,D.,Kikinis,R.,andFedorov,A.“dcmqi:AnOpenSourceLibraryforStandardizedCommunicationofQuantitativeImageAnalysisResultsUsingDICOM”.CancerResearch77.21(2017),e87–e90.[20]IHERadiologyTechnicalCommittee.IHERadiologyTechnicalFrameworkSupplementAIResults(AIR).https://www.ihe.net/uploadedFiles/Documents/Radiology/IHE_RAD_Suppl_AIR.pdf.June2020.[21]Virtanen,P.etal.“SciPy1.0:fundamentalalgorithmsforscientiﬁccomputinginPython”.NatMethods17.3(Mar.2020),pp.261–272.[22]Harris,C.R.,Millman,K.J.,Walt,S.J.vander,Gommers,R.,Virtanen,P.,Cournapeau,D.,Wieser,E.,Taylor,J.,Berg,S.,Smith,N.J.,Kern,R.,Picus,M.,Hoyer,S.,Kerkwijk,M.H.van,Brett,M.,Haldane,A.,DelR´o,J.F.,Wiebe,M.,Peterson,P.,G´erard-Marchant,P.,Sheppard,K.,Reddy,T.,Weckesser,W.,Abbasi,H.,Gohlke,C.,andOliphant,T.E.“ArrayprogrammingwithNumPy”.Nature585.7825(Sept.2020),pp.357–362.[23]Pedregosa,F.,Varoquaux,G.,Gramfort,A.,Michel,V.,Thirion,B.,Grisel,O.,Blondel,M.,Prettenhofer,P.,Weiss,R.,Dubourg,V.,Vanderplas,J.,Passos,A.,Cournapeau,D.,Brucher,M.,Perrot,M.,andDuchesnay,´E.“Scikit-Learn:MachineLearninginPython”.J.Mach.Learn.Res.12(Nov.2011),pp.2825–2830.[24]Walt,S.vander,Sch¨onberger,J.,Nunez-Iglesias,J.,Boulogne,F.,Warner,J.,Yager,N.,Gouillart,E.,Yu,T.,andcontributors,thescikit-image.“scikit-image:imageprocessinginPython”.PeerJ2(2014),e453.[25]Mason,D.“SU-E-T-33:pydicom:anopensourceDICOMlibrary”.MedicalPhysics38.6Part10(2011),pp.3493–3493.[26]Hapke,H.andNelson,C.Buildingmachinelearningpipelines.O’ReillyMedia,2020.[27]Sambasivan,N.,Kapania,S.,Highﬁll,H.,Akrong,D.,Paritosh,P.,andAroyo,L.M.““Ev-eryoneWantstoDotheModelWork,NottheDataWork”:DataCascadesinHigh-StakesAI”.Proceedingsofthe2021CHIConferenceonHumanFactorsinComputingSystems.CHI’21.Yokohama,Japan:AssociationforComputingMachinery,2021.[28]Paszke,A.,Gross,S.,Massa,F.,Lerer,A.,Bradbury,J.,Chanan,G.,Killeen,T.,Lin,Z.,Gimelshein,N.,Antiga,L.,Desmaison,A.,Kopf,A.,Yang,E.,DeVito,Z.,Raison,M.,Tejani,A.,Chilamkurthy,S.,Steiner,B.,FangL.Bai,J.,andChintala,S.“PyTorch:AnImperativeStyle,High-PerformanceDeepLearningLibrary”.AdvancesinNeuralInformationProcessingSystems32.CurranAssociates,Inc.,2019,pp.8026–8037.25[29]Abadi,M.,Barham,P.,Chen,J.,Chen,Z.,Davis,A.,Dean,J.,Devin,M.,Ghemawat,S.,Irving,G.,Isard,M.,etal.“Tensorﬂow:Asystemforlarge-scalemachinelearning”.12thUSENIXSymposiumonOperatingSystemsDesignandImplementation(OSDI16).2016,pp.265–283.[30]Bradski,G.“TheOpenCVLibrary”.Dr.Dobb’sJournalofSoftwareTools(2000).[31]Yoo,T.S.,Ackerman,M.J.,Lorensen,W.E.,Schroeder,W.,Chalana,V.,Aylward,S.,Metaxas,D.,andWhitaker,R.“EngineeringandalgorithmdesignforanimageprocessingAPI:atechnicalreportonITK-theinsighttoolkit”.MedicineMeetsVirtualReality02/10.IOSpress,2002,pp.586–592.[32]Pianykh,O.S.Digitalimagingandcommunicationsinmedicine(DICOM):apracticalin-troductionandsurvivalguide.Vol.6.Springer,2008.[33]Haﬁz,A.M.andBhat,G.M.“Asurveyoninstancesegmentation:stateoftheart”.Inter-nationalJournalofMultimediaInformationRetrieval9.3(2020),pp.171–189.[34]Bidgood,W.D.“TheSNOMEDDICOMmicroglossary:controlledterminologyresourcefordatainterchangeinbiomedicalimaging”.MethodsInfMed37.4-5(Nov.1998),pp.404–414.[35]Clark,K.,Vendt,B.,Smith,K.,Freymann,J.,Kirby,J.,Koppel,P.,Moore,S.,Phillips,S.,Maﬃtt,D.,Pringle,M.,Tarbox,L.,andPrior,F.“TheCancerImagingArchive(TCIA):MaintainingandOperatingaPublicInformationRepository”.JournalofDigitalImaging26.6(Dec.2013),pp.1045–1057.[36]Clunie,D.A.“Dual-PersonalityDICOM-TIFFforWholeSlideImages:AMigrationTech-niqueforLegacySoftware”.JournalofPathologyInformatics10(2019),p.12.[37]ArmatoIII,S.G.,McLennan,G.,Bidaut,L.,McNitt-Gray,M.F.,Meyer,C.R.,Reeves,A.P.,Zhao,B.,Aberle,D.R.,Henschke,C.I.,Hoﬀman,E.A.,Kazerooni,E.A.,MacMahon,H.,Beek,E.J.R.van,Yankelevitz,D.,Biancardi,A.M.,Bland,P.H.,Brown,M.S.,Engelmann,R.M.,Laderach,G.E.,Max,D.,Pais,R.C.,Qing,D.P.-Y.,Roberts,R.Y.,Smith,A.R.,Starkey,A.,Batra,P.,Caligiuri,P.,Farooqi,A.,Gladish,G.W.,Jude,C.M.,Munden,R.F.,Petkovska,I.,Quint,L.E.,Schwartz,L.H.,Sundaram,B.,Dodd,L.E.,Fenimore,C.,Gur,D.,Petrick,N.,Freymann,J.,Kirby,J.,Hughes,B.,VandeCasteele,A.,Gupte,S.,Sallam,M.,Heath,M.D.,Kuhn,M.H.,Dharaiya,E.,Burns,R.,Fryd,D.S.,Salganicoﬀ,M.,Anand,V.,Shreter,U.,Vastagh,S.,Croft,B.Y.,andClarke,L.P.“TheLungImageDatabaseConsortium(LIDC)andImageDatabaseResourceInitiative(IDRI):ACompletedReferenceDatabaseofLungNodulesonCTScans”.MedicalPhysics38.2(2011),pp.915–931.[38]ArmatoIII,S.,McLennan,G.,Bidaut,L.,McNitt-Gray,M.,Meyer,C.,Reeves,A.,Zhao,B.,Aberle,D.,Henschke,C.,Hoﬀman,E.,Kazerooni,E.,MacMahon,H.,Beek,E.van,Yankelevitz,D.,Biancardi,A.,Bland,P.,Brown,M.,Engelmann,R.,Laderach,G.,Max,D.,Pais,R.,Qing,D.,Roberts,R.,Smith,A.,Starkey,A.,Batra,P.,Caligiuri,P.,Farooqi,A.,Gladish,G.,Jude,C.,Munden,R.,Petkovska,I.,Quint,L.,Schwartz,L.,Sundaram,B.,Dodd,L.,Fenimore,C.,Gur,D.,Petrick,N.,Freymann,J.,Kirby,J.,Hughes,B.,Casteele,A.,Gupte,S.,Sallam,M.,Heath,M.,Kuhn,M.,Dharaiya,E.,Burns,R.,Fryd,D.,Salganicoﬀ,M.,Anand,V.,Shreter,U.,Vastagh,S.,Croft,B.,andClarke,L.DataFromLIDC-IDRI.Tech.rep.TheCancerImagingArchive,2015.26[39]Coudray,N.,Ocampo,P.S.,Sakellaropoulos,T.,Narula,N.,Snuderl,M.,Fenyo,D.,Moreira,A.L.,Razavian,N.,andTsirigos,A.“Classiﬁcationandmutationpredictionfromnon-smallcelllungcancerhistopathologyimagesusingdeeplearning”.NatureMedicine24.10(2018),pp.1559–1567.[40]He,K.,Zhang,X.,Ren,S.,andSun,J.“DeepResidualLearningforImageRecognition”.2016IEEEConferenceonComputerVisionandPatternRecognition(CVPR).2016.[41]Deng,J.,Dong,W.,Socher,R.,Li,L.-J.,Li,K.,andFei-Fei,L.“ImageNet:ALarge-ScaleHierarchicalImageDatabase”.CVPR09.2009.[42]Lerousseau,M.,Vakalopoulou,M.,Classe,M.,Adam,J.,Battistella,E.,Carr´e,A.,Estienne,T.,Henry,T.,Deutsch,E.,andParagios,N.“WeaklySupervisedMultipleInstanceLearningHistopathologicalTumorSegmentation”.MedicalImageComputingandComputerAssistedIntervention–MICCAI2020.SpringerInternationalPublishing,2020,pp.470–479.[43]Lu,M.Y.,Williamson,D.F.K.,Chen,T.Y.,Chen,R.J.,Barbieri,M.,andMahmood,F.“Data-eﬃcientandweaklysupervisedcomputationalpathologyonwhole-slideimages”.NatBiomedEng(Mar.2021).[44]Lin,T.-Y.,Goyal,P.,Girshick,R.,He,K.,andDoll´ar,P.“Focallossfordenseobjectdetec-tion”.ProceedingsoftheIEEEinternationalconferenceoncomputervision.2017,pp.2980–2988.[45]DICOMStandardsCommittee.DICOMPS3.18–WebServices.http://dicom.nema.org/medical/dicom/current/output/chtml/part18/PS3.18.html.2021.[46]Gauriau,R.,Bridge,C.,Chen,L.,Kitamura,F.,Tenenholtz,N.A.,Kirsch,J.E.,Andriole,K.P.,Michalski,M.H.,andBizzo,B.C.“UsingDICOMMetadataforRadiologicalImageSeriesCategorization:aFeasibilityStudyonLargeClinicalBrainMRIDatasets”.JDigitImaging33.3(June2020),pp.747–762.[47]Magrabi,F.,Ammenwerth,E.,McNair,J.B.,DeKeizer,N.F.,Hypponen,H.,Nyk¨anen,P.,Rigby,M.,Scott,P.J.,Vehko,T.,Wong,Z.S.-Y.,andGeorgiou,A.“ArtiﬁcialIntelligenceinClinicalDecisionSupport:ChallengesforEvaluatingAIandPracticalImplications”.YearbMedInform28(012019),pp.128–134.[48]DICOMStandardsCommittee,WorkingGroup26(Pathology).Supplement222:MicroscopyBulkSimpleAnnotationsStorageSOPClass.ftp://medical.nema.org/medical/dicom/supps/LB/sup222_lb_WSIAnnotations.pdf.2021.[49]Larobina,M.andMurino,L.“Medicalimageﬁleformats”.JDigitImaging27.2(Apr.2014),pp.200–206.[50]Li,X.,Morgan,P.S.,Ashburner,J.,Smith,J.,andRorden,C.“Theﬁrststepforneuroimag-ingdataanalysis:DICOMtoNIfTIconversion”.J.Neurosci.Methods264(Apr.2016),pp.47–56.[51]Roberts,M.,Driggs,D.,Thorpe,M.,Gilbey,J.,Yeung,M.,Ursprung,S.,Aviles-Rivero,A.I.,Etmann,C.,McCague,C.,Beer,L.,etal.“CommonpitfallsandrecommendationsforusingmachinelearningtodetectandprognosticateforCOVID-19usingchestradiographsandCTscans”.NatureMachineIntelligence3.3(2021),pp.199–217.27[52]Fedorov,A.,Longabaugh,W.,Pot,W.,Clunie,D.,Pieper,S.,AertsHugo,J.,Homeyer,A.,Lewis,R.,Akbarzadeh,A.,Bontempi,D.,Cliﬀord,D.,Herrmann,M.,H¨ofener,H.,Octaviano,I.,Osborne,C.,Paquette,S.,Petts,J.,Punzo,D.,Reyes,M.,Schacherer,D.,Tian,M.,White,G.,Ziegler,E.,Shmulevich,I.,Pihl,T.,Wagner,U.,Farahani,K.,andR,K.“NCIImagingDataCommons”.CancerResearch(2021).28Supplementarymaterialfor“Highdicom:APythonlibraryforstandardizedencodingofimageannotationsandmachinelearningmodeloutputsinpathologyandradiology”May10,2022ExperimentalimagedatasetsPreparationofslidemicroscopyimagesandannotationsinDICOMformatWedownloadedSMimagesinSVSformatfromeithertheGenomicsDataCommons(GDC)DataPortalusingtheGDCDataTransferToolsoftwareorfromTCIAviatheCPTAChistopathologyinterfaceincaseofTCGAandCPTACcollections,respectively.Inaddition,wedownloadedthecorrespondingbiospecimenandclinicalmetadatainJSONformatviatheGDCDataPortalforbothTCGAandCPTACcollectionsandadditionalmetadatainJSONformatviatheCPTACClinicalDataAPIfortheCPTACcollections.ForeachimagecontainedinagivenSVSﬁle,wecreatedaDICOMVLWholeSlideMicroscopyImageinstanceandstoreditinaDICOMPart10ﬁle.Brieﬂy,wecopiedimagepixeldatacontainedinTIFFtilesaswellasrelevantpixel-relatedmetadatacontainedinTIFFtags(e.g.,ImageLength,ImageWidth,ImageDescription,Compression,andPhotometricInterpretation)intothecorrespondingDICOMdataelements.WefurtherenrichedDICOMdatasetswithinformationextractedfromTCIAbiospecimenmetadataJSONdocuments,e.g.ﬁxativesandembeddingmediausedforspecimenpreparationaswellasrelevantpatient,study,andspecimenidentiﬁers.Foridentiﬁers,wefollowedthesamepatternsusedfortheradiologydatasetstofacilitatematchingofpathologyandradiologyDICOMdatasets.AsdescribedintheMethodssection,westructuredthecontentoftheSRdocumentsbasedontemplateTID1500“MeasurementReport”andencodedimageannotationsusingcontentitemsdeﬁnedeitherintemplateTID1410“PlanarROIMeasurementsandQualitativeEvaluations”incaseofgraphicalROIannotationsorTID1501“MeasurementandQualitativeEvaluationGroup”incasetheannotationsappliedtoanentireimageorseries.Ineithercase,weincludedcontentitem“Finding”(DCM121071)toencodetheconceptthatthegivenimageorimageregionrepre-sents,e.g.,“Neoplasm”(SCT108369006).WefurtherextendedtemplateTID1501andincludedadditionalcontentitemstoencodemeasurementsandqualitativeevaluationsoftheimageorimageregionthatarespeciﬁctocancerdiagnosisinpathologyorradiology.Weincludedcontentitems“Morphology”(SCT116676008)and“Topography”(SCT116677004)withvaluetypeCODEtoen-codethetumorhistomorphology(squamouscellcarcinoma,adenocarcinoma,etc.)andtissueoforigin(bronchus,upperlobe,etc.)usingtheInternationalClassiﬁcationofDiseasesforOncology(ICD-O-3)andtheClinicalModiﬁcationoftheInternationalClassiﬁcationofDiseases(ICD-10-CM)codingsystems,respectively.Inaddition,weincludedcontentitems“Percenttumorcells”(caDSR5432686),“Percenttumornuclei”(caDSR5455534),and“Specimennecrosis”(caDSR5455511)withvaluetypeNUMtoprovidemeasurementsofthepercentageofviableordeadtumortissue.Weencodedgraphicalannotationsofimageregionsofinterest(ROIs)asvectorgraphicsin1arXiv:2106.07806v3  [eess.IV]  8 May 2022DICOMComprehensiveSRorComprehensive3DSRdocumentsasdescribedabove.ForSMim-agesusedinthepathologyexperiments,graphicalannotationsoftumorregionswerenotavailableinTCIA,andwethereforeencodedimage-levelannotationsinDICOMComprehensiveSRdocu-mentsusingTID1501“MeasurementandQualitativeEvaluationGroup”.However,weannotatedasubsetofSMimagesusingtheSliMviewer1andencodedtheresultingROIsascontentitemswithtypeSCOORD3DinDICOMComprehensive3DSRdocumentsusingTID1410“PlanarROIMeasurementsandQualitativeEvaluations”.PreparationofcomputedtomographyimagesandannotationsinDICOMformatFortheradiologyexperiments,weusedtheLIDC-IDRIdatasetof1018diagnosticandscreeningCTstudiesfrommultipleinstitutions.WedownloadedCTimages,consistingofasingleaxialseriesperstudy,inDICOMformatfromTCIAusingtheNBIADataRetrieversoftware.EachscanintheLIDC-IDRIdatasetisannotatedintheformofsegmentationmasksforlungnodules.Eachscancontainsoneormoreannotatednodules,andeachnodulemaybeannotatedbymultiplereaders.TheannotationsforthisdatasetwerealreadyavailableintheoriginalcustomXML-basedformat,andalsoasDICOMSegmentationimagesandDICOMSRdocumentsthatwerecreatedinpriorworkusingtheC++-basedDCMQItoolkit[1,2].ForthepurposeofdemonstratingafullworkﬂowwithinthePythonprogramminglanguage,were-createdannotationsinbothDICOMSegmenta-tionandSRformatfromtheoriginalXMLannotations,largelyfollowingFedorovetal.[1,2],usinghighdicomandusedtheresultingdatasets(ratherthanthosecreatedwithDCMQI)forthesubse-quentexperiments.TheSEGimagesareherebyusedtoencodetheannotationofasinglenodulebyasinglereaderasasinglesegment,withaSegmentedPropertyCategoryof“MorphologicallyAbnormalStructure”(SCT49755003)andSegmentedPropertyType“Nodule”(SCT27925004).TheSRdocumentsusedtheComprehensive3DSRIODandincludedmeasurementsandqualita-tiveevaluationsofeachoftheregionsintheSEGimages,referencedviaacontentitemoftypeIMAGE.MeasurementsofeachnoduleROIincludedthe“Volume”(SCT118565006)and“Diameter”(SCT81827009)asNUMcontentitems,andqualitativeevaluationsincludedthesubtlety,internalstructure,calciﬁcation,sphericity,margin,lobulation,spiculation,texture,andmalignancyofthenoduleusingarangeofcodedconceptsassuggestedby[1,2].Notethatthesemeasurementsandqualitativeevaluationswerenotusedduringmodeltraining,butwereincludedintheannotationSRforthesakeofcompleteness.Thescriptusedforconversionispubliclyavailable2.ValidationofDICOMdatasetsWeevaluatedthecomplianceofpreparateddatasetswiththeDICOMstandardusingbothauto-matedvalidationtoolsandmanualexpertreview.Speciﬁcally,weusedthedciodvfycommandlinetoolofthedicom3tools3packagetoassertthatindividualDICOMﬁlesarestructuredaccordingthecorrespondingInformationObjectDeﬁnition(IOD)andtheDicomSRValidatorprogramofthePixelMedJavaDICOMToolkit4.Wefurtherusedthedcdumpanddcsrdumpcommandlinetoolsofthedicom3toolspackageandthedcmdumpcommandlinetooloftheDICOMtoolkit(DCMTK)5packagetomanuallyreviewandvalidatethecontentofDICOMﬁles.1https://github.com/mghcomputationalpathology/slim2https://github.com/QIICR/lidc2dicom3http://www.dclunie.com/dicom3tools.html4http://www.pixelmed.com/dicomtoolkit.html5https://dcmtk.org/2FurthermodeltrainingdetailsWeimplementedthedeepconvolutionalneuralnetworkmodelsusingthePyTorchPythonli-brary[3]andtrainedthemonaLinuxsupercomputerwithNVIDIAV-100graphicalprocessingunits(GPUs)usingtheCUDAandcuDNNC++libraries.WeoptimizedmodelparametersusingAdamoptimizerwithmomentumandoptimizedthelearningrateaswellasotherhyperparametersusingrandomsearch.Thedatapreprocessingpipelineswereimplementedinformofclassesderivedfromtorch.data.Dataset.whichloadtheimagesandcorrespondingimageannotationsfromDICOMﬁlesintoNumPyarrays,transformthedatainmemoryintotherepresentationexpectedbytherespectivemodel,andreturnapairofimageframesandlabelsasPyTorchtensors.InstancesoftheseclassesareinstantiatedgiventhelocationofDICOMﬁlesondiskaswellastheSOPInstanceUIDsofDICOMSRdocu-ments,whichcontainannotationsthatareconsideredthegroundtruthformodeltrainingaswellasreferencestothesourceimagesfromwhichtheannotationswerederived.Foreachexample,thegetitemmethodloadsimageframesandannotationsfromDICOMﬁlesondiskintomemoryasnumpy.ndarrayobjects,transformsthedataintotherepresentationexpectedbytheﬁrstlayeroftheneuralnetwork(optionallyperformingdataaugmentation),andreturnsthepairoftransformedpixeldataandassociatedlabelsasatupleoftorch.tensorobjects.ModelevaluationWeselectedonepathologyandradiologymodelonavalidationsetandevaluatedtheperformanceofselectedmodelsonahold-outtestset.Forpathology,wedevelopedclassiﬁerstocategorizewholeslideimagesintoeithernormallungtissue,lungadenocarcinoma,orlungsquamouscellcarcinoma.Tothisend,wethresholdedtheprobabilisticfractionalsegmentationimagesoutputtedbytheneuralnetworkmodel(usingathresholdvalueof0.5probability)foreachclassandusedtherebygeneratedbinarysegmentationmaskstocomputetherelativetumorarea,i.e.,theratioofthenumberofimageframesclassiﬁedastumorandthenumberofimageframesclassiﬁedastissue.Basedontheseaggregatedmeasure-ments,wecreatedbinarywholeslideimageclassiﬁerstodistinguishnormallungfromnon-smallcelllungcancer(NSCLC,lungadenocarcinomaorlungsquamouscellcarcinoma),lungadeno-carcinoma(LUAD),orlungsquamouscellcarcinoma(LUSC).TheoptiomalthresholdforeachclassiﬁerwasdeterminedviaReceiverOperatorCharacteristic(ROC)analysis(SupplementaryFigureS1A).Evaluatingtheclassiﬁersattheselectedthresholdvalues,weachievedanaccuracyof0.98forseparatingnormallungfromNSCLC(SupplementaryFigureS1B),ahighcorrelationbetweenthepredictedrelativetumorareaandtheannotatedtumorcellpercentageforNSCLCexamples(SupplementaryFigureS1C),andanaccuracyof0.85fordistinguishingbetweenLUADandLUSC(SupplementaryFigureS1D).Forradiology,weevaluatedtheCTlungnoduledetectionmodelintermsofstudy-levelresults.Nodulesensitivity(recall)wascalculatedasthefractionofannotatednodulesforwhichanypre-dictedboxoverlappedanyreader’sannotatedwithintersection-over-union(IoU)of0.5orgreateronanyframe.FalsepositivesinneighboringframeswereclusteredtogetheriftheirIoU(inthexandydirectionsonly)wasgreaterthan0.5,andassignedthescoreofthehighestscoreinthecluster.Thisgaveanaverageprecisionmetricof0.582fornoduledetection.Thefree-responsereceiveroperatingcharacteristic(FROC),whichplotsnodulesensitivity(recall)againsttheaver-agenumberoffalsepositivesperstudyasthedetectionscorethresholdisvaried(SupplementaryFigureS2).3VisualizationofOutputsFiguresS3andS4containexamplesofimageannotationsandmodeloutputsvisualizedwithinopensourcetools.References[1]A.Fedorov,M.Hancock,D.Clunie,M.Brockhhausen,J.Bona,J.Kirby,J.Freymann,H.Aerts,R.Kikinis,andF.Prior.StandardizedrepresentationoftheTCIALIDC-IDRIannota-tionsusingDICOM.Tech.rep.TheCancerImagingArchive,2018.[2]A.Fedorov,M.Hancock,D.Clunie,M.Brochhausen,J.Bona,J.Kirby,J.Freymann,S.Pieper,H.J.W.L.Aerts,R.Kikinis,andF.Prior.“DICOMre-encodingofvolumetricallyannotatedLungImagingDatabaseConsortium(LIDC)nodules”.In:MedicalPhysics47.11(2020),pp.5953–5965.[3]A.Paszke,S.Gross,F.Massa,A.Lerer,J.Bradbury,G.Chanan,T.Killeen,Z.Lin,N.Gimelshein,L.Antiga,A.Desmaison,A.Kopf,E.Yang,Z.DeVito,M.Raison,A.Tejani,S.Chilamkurthy,B.Steiner,J.FangL.Bai,andS.Chintala.“PyTorch:AnImperativeStyle,High-PerformanceDeepLearningLibrary”.In:AdvancesinNeuralInformationProcessingSystems32.CurranAssociates,Inc.,2019,pp.8026–8037.[4]A.Fedorov,R.Beichel,J.Kalpathy-Cramer,J.Finet,J.-C.Fillion-Robin,S.Pujol,C.Bauer,D.Jennings,F.Fennessy,M.Sonka,J.Buatti,S.Aylward,J.V.Miller,S.Pieper,andR.Kikinis.“3DSlicerasanimagecomputingplatformfortheQuantitativeImagingNetwork”.In:MagneticResonanceImaging30.9(2012),pp.1323–1341.4ABCDFalse Positive Rate (1 - Specificity)PredictionNormalNormalNSCLCNSCLCGround truthTrue Positive Rate (Sensitivity)Prediction [percent tumor area]Ground truth [percent tumor cells]r 2 = 1.10NSCLC (AUC=0.99)LUAD (AUC=0.93)LUSC (AUC=0.95)PredictionLUADNSCLCLUADLUSCGround truthFigureS1:PerformanceofSMimageclassiﬁers(NSCLC—non-smallcelllungcancer,LUAD—lungadenocarcinoma,LUSC—lungsquamouscellcarcinoma).AReceiveroperatingcharac-teristiccurveforeachbinaryclassiﬁcationproblem.BConfusionmatrixcomparingthegroundtruthagainstpredictedclasslabelsfortheclassiﬁcationofnormallungversusnon-smallcelllungcancerusingthethresholdselectedinA.CCorrelationbetweenannotatedpercenttumorcellsandpredictedpercenttumorareaconsideringexamplesthatwereclassiﬁedasnon-smallcelllungcancer.DConfusionmatrixcomparingthegroundtruthagainstpredictedclasslabelsfortheclassiﬁcationoflungadenocarcinomaversuslungsquamouscellcarcinomaconsideringexamplesthatwerecorrectlyclassiﬁedasnon-smallcelllungcancer.50246810False Positives Per Study0.20.30.40.50.60.70.8SensitivityFigureS2:Free-responsereceiveroperatingcharacteristicfortheCTlungnoduledetectionmodel.FigureS3:ALungnoduleannotationinaCTimageoftheLIDC-IDRIcollectionencodedasasegmentinaDICOMSegmentationimageusinghighdicomanddisplayedintheopen-sourceOHIFviewer.BBoundingboxdetectionoutputfromtheCTlungnoduledetectionmodelencodedasaDICOMSRdocumentandvisualizedusingtheopen-source3DSlicersoftwareandtheQuantitativeReportingextension[4].Detectedboundingboxesareshadedbytheirdetectionscoresfrompurple(verylowscore)toyellow(highscore).6FigureS4:LungadenocarcinomaregionsdetectedinaslidemicroscopyimageoftheTCGA-LUADcollectionencodedasSCOORD3DcontentitemsinaDICOMComprehensive3DSRdocumentusinghighdicomanddisplayedintheopen-sourceSliMviewer.7