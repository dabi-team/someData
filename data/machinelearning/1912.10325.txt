Online Reinforcement Learning of Optimal
Threshold Policies for Markov Decision Processes

Arghyadip Roy, Member, IEEE, Vivek Borkar, Fellow, IEEE, Abhay Karandikar, Member, IEEE and
Prasanna Chaporkar, Member, IEEE

1

1
2
0
2

g
u
A
2
2

]

G
L
.
s
c
[

3
v
5
2
3
0
1
.
2
1
9
1
:
v
i
X
r
a

Abstract—To overcome the curses of dimensionality and mod-
eling of Dynamic Programming (DP) methods to solve Markov
Decision Process (MDP) problems, Reinforcement Learning (RL)
methods are adopted in practice. Contrary to traditional RL
algorithms which do not consider the structural properties of the
optimal policy, we propose a structure-aware learning algorithm
to exploit the ordered multi-threshold structure of the optimal
policy,
if any. We prove the asymptotic convergence of the
proposed algorithm to the optimal policy. Due to the reduction
in the policy space, the proposed algorithm provides remarkable
improvements in storage and computational complexities over
classical RL algorithms. Simulation results establish that the
proposed algorithm converges faster than other RL algorithms.

Index Terms—Markov Decision Process, Stochastic Approxi-
mation Algorithms, Reinforcement Learning, Stochastic Control,
Online Learning of Threshold Policies.

I. INTRODUCTION

Markov Decision Process (MDP) [2] is a framework which
is widely used for the optimization of stochastic systems
(e.g., queue [3], inventory and production management) to
temporal decisions. Dynamic Programming
make optimal
(DP) methods [2] to compute the optimal policy suffer from
the curse of dimensionality [4, Chapter 4.1], [5] in many
practical applications as they are computationally inconvenient
due to extremely high dimension of the iterates. Furthermore,
they suffer from the curse of modeling since the knowledge of
the underlying transition probabilities (which often depend on
the statistics of unknown system parameters) required by DP
methods, may not be available beforehand. RL techniques [6]
address the curse of modeling by learning the optimal policy
iteratively. RL algorithms do not require any prior knowledge
regarding the transition probabilities of the underlying model.
RL being sampling based, updates only one component at
a time, reducing per iterate computation at the expense of
speed. Examples are: Q-learning [7] iteratively evaluates the
Q-function of every state-action pair using a combination of
exploration and exploitation. In [8]–[10], upper conﬁdence
bound based exploration improves the convergence speed over

This paper is a substantially expanded and revised version of the work in

[1].

Arghyadip Roy was with Department of Electrical Engineering, Indian
Institute of Technology Bombay, Mumbai, 400076, India when the work was
done and is currently with Coordinated Science Laboratory, University of
Illinois at Urbana-Champaign, 61820, USA. e-mail: arghyad4@illionois.edu.
Vivek Borkar, Abhay Karandikar and Prasanna Chaporkar are with the
Department of Electrical Engineering, Indian Institute of Technology Bom-
bay. e-mail: {borkar,karandi,chaporkar}@ee.iitb.ac.in. Abhay Karandikar is
currently Director, Indian Institute of Technology Kanpur (on leave from IIT
Bombay), Kanpur, 208016, India. e-mail:karandi@iitk.ac.in.

classical (cid:15)-greedy exploration. PDS learning algorithm [4],
[11] removes the requirement of action exploration and results
in faster convergence than Q-learning. Virtual Experience
(VE) learning algorithm in [12] updates multiple PDSs at a
time. Faster convergence is achieved at the cost of increased
computational complexity.

However, popular RL techniques [4], [6], [7], [11], [13],
[14] do not exploit the known structural properties of the
optimal policy if any, and consider the set of all policies as
the policy search space. However, in operations research and
communications literature, various structural properties of the
optimal policy including threshold structure, transience of cer-
tain states and index rules [15], [16] are often established using
monotonicity, convexity/concavity and sub-modularity/super-
modularity properties of value functions of states. If one can
exploit these structural properties to reduce the search space
while learning, then faster convergence can be achieved with
a reduction in the computational complexity. A few works
in the literature [17]–[20] focus on the exploitation of the
structural properties [16] while learning the optimal policy.
Q-learning based approaches in [17], [19], in every iteration,
project
the value functions to guarantee the monotonicity
in system state. Although improved convergence speed is
obtained, the per-iteration computational complexity does not
improve over Q-learning. In [18], a scheme based on piece-
wise linear approximation of the value function, is proposed.
However, as the approximation becomes better, the complexity
increases. In [21], [22], a Stochastic Approximation (SA) [23]
approach based on simultaneous perturbation is proposed to
compute the optimal thresholds. It uses a combination of ideas
from renewal theory and Monte Carlo simulation. The low
complexity Q-learning algorithm proposed in [24] exploits the
threshold nature of the optimal policy. The proposed algorithm
estimates the optimal policy for a subset of sets, and then
policy interpolation is performed for the unvisited states. The
performance of policy interpolation is further improved by a
policy reﬁnement. However, the proposed algorithm provides
a near-optimal policy after a ﬁnite number of visits to a set of
state-action pairs.

We consider a scenario where the optimal policy has
a multi-threshold structure, and the thresholds for different
events are ordered. Therefore, learning the optimal policy is
equivalent to learning the value of threshold for each event.
Motivated by this, we propose a Structure-Aware Learning for
MUltiple Thresholds (SALMUT) algorithm which considers
only the set of ordered threshold policies. We consider a
two timescale approach where the value functions of the

 
 
 
 
 
 
states and threshold parameters are updated in faster and
slower timescale, respectively. Threshold parameters are up-
dated based on the gradients of the average reward w.r.t
the threshold. The proposed scheme results in reductions
in storage and computational complexities (by amortizing
the computation, and hence the complexity of the original
problem, over several iterations) compared to traditional RL
schemes. We prove that the proposed algorithm converges to
the optimal policy asymptotically. Simulation results exhibit
that SALMUT converges faster than classical RL schemes
due to reduction in policy search space. The techniques in this
paper can be employed to learn the optimal policy in problems
involving optimality of threshold policies [15], [25]–[28]. We
illustrate this using an example of a ﬁnite buffer multi-server
queue with multiple customer classes (similar to [3], [29]).

The proposed SALMUT algorithm can be adopted for single
threshold case in [1], without any modiﬁcation. In [30], a
structure-aware learning algorithm learns a single parameter-
ized threshold where the thresholds for different parameter
values are independent of each other. However, in this paper,
the thresholds have to satisfy certain ordering constraints and
hence, are not independent. Therefore, the threshold update
scheme in the slower timescale and corresponding convergence
behavior differ signiﬁcantly from [30]. To the best of our
knowledge, contrary to other works [17]–[20], we for the ﬁrst
time consider the threshold vector as a parameter while learn-
ing, to reduce a non-linear iteration (involving maximization
over a set of actions) into a linear iteration for a quasi-static
value of threshold and thereby, achieve a signiﬁcant reduction
in the per-iteration computational complexity.

II. SYSTEM MODEL & PROBLEM FORMULATION

}

∈ {

∈ {

S × I
0, 1, . . . , N

Consider a continuous time MDP problem where we aim
to obtain the optimal control policy in a multi-event scenario.
, say) be (s, i)
Let the system state in the state space (
where s
denotes
, and i
0, 1, . . . , W
}
the event type. Since we have a ﬁnite-state regular Markov
chain [31], it is sufﬁcient to observe the system state only at
consist of
the decision epochs [32]. Let the action space
two actions, viz., A1, and A2. The transition probability from
state (s, i) to (s(cid:48), i(cid:48)) under action a (p((s, i), (s(cid:48), i(cid:48)), a), say)
can be factored into two parts, viz., the deterministic and the
probabilistic transitions due to the chosen action (p(s, s(cid:48), a),
say) and the next event, respectively. Let the mean transition
rate from state (s, i) be denoted by v(s). Let the arrival times
of events in state s be independent exponentially distributed
with means λi(s) respectively, so that the next event is i with
probability

A

(cid:80)N

.

λi(s)
j=0 λj (s)
N
i=0 λi(s). Hence,

Now, v(s) =

(cid:80)

p((s, i), (s(cid:48), i(cid:48)), a) = p(s, s(cid:48), a)

λi(cid:48)(s(cid:48))
v(s(cid:48))

.

Let the non-negative reward rate (r((s, i), a), say) obtained
by choosing action A2 for event i be Ri, where Ri > Rj
for i < j. Therefore, r((s, i), a) = Ri1{a=A2}. Let the non-
negative cost rate in state (s, i) be h(s) (independent of i).

2

Q

Let

be the set of stationary policies, Since the zero
state is reachable from any state with positive probability,
the underlying Markov chain is unichain and hence, a unique
stationary distribution exists. Let the inﬁnite horizon average
reward (independent of the initial state) under policy Q
∈ Q
EQ[R(t)],
be ρQ. We aim to maximize ρQ = limt→∞
where R(t) is the total reward till time t. The DP equation
describing the necessary condition for optimality in a semi-
Markov decision process (

(s, i), (s(cid:48), i(cid:48))

) is

1
t

∀{
[r((s, i), a) +

} ∈ S × I
p((s, i), (s(cid:48), i(cid:48)), a) ¯V (s(cid:48), i(cid:48))

¯V (s, i) = max
a∈A

−

−

ρ ¯β((s, i), a)]

(cid:88)s(cid:48),i(cid:48)
h(s),
where ¯V (s, i), ρ and ¯β((s, i), a) denote the value function of
state (s, i), the optimal average reward and the mean transition
time from state (s, i) upon choosing action a, respectively.
We rewrite the DP equation after substituting the values of
r((s, i), a) and transition probabilities as

¯V (s, i) = max
a∈A

Ri1{a=A2} +

p(s, s(cid:48), a)

λi(cid:48)(s(cid:48))
v(s(cid:48))

¯V (s(cid:48), i(cid:48))

¯V (s, i). Therefore, the follow-

(cid:88)s(cid:48),i(cid:48)
h(s).

(cid:2)

ρ ¯β((s, i), a)

−

−
λi(s)
(cid:3)
v(s)

N
i=0

We deﬁne V (s) :=
ing relations hold.

(cid:80)

¯V (s, i) = max
a∈A

Ri1{a=A2} +

p(s, s(cid:48), a)V (s(cid:48))

(cid:2)

ρ ¯β((s, i), a)

−
is independent of a)

−

(cid:3)

(cid:88)s(cid:48)
h(s),

and (since λi(s)
v(s)

V (s) = max
a∈A

λi(s)
v(s)

i
(cid:2) (cid:88)
ρ ¯β(s, a)

−

h(s),

−

Ri1{a=A2} +

p(s, s(cid:48), a)V (s(cid:48))

(cid:88)s(cid:48)

(1)

where ¯β(s, a) =

(cid:3)

λi(s)
v(s)

¯β((s, i), a). Using (1), instead of

i
, we can consider the system
considering (s, i)
(cid:80)
∈ S × I
state as s
with value function V (s) and transition
probability p(s, s(cid:48), a), and the analysis remains unaffected.
However, in this model, the reward rate is the weighted average
of original reward rates. The sojourn times being exponentially
distributed, following [2], we obtain

∈ S

V (s) = max
a∈A

[

λi(s)
v(s)

i
(cid:88)
h(s)
−

−

ρ,

Ri1{a=A2} +

p(s, s(cid:48), a)V (s(cid:48))]

(cid:88)s(cid:48)

(2)
s(cid:48)(cid:54)=s p(s, s(cid:48), a). (2) is the DP equation
where p(s, s, a) = 1
for an equivalent discrete time MDP having controlled transi-
tion probabilities p(s, s(cid:48), a) which is used throughout the rest
of the paper. This problem can be solved using Relative Value
Iteration Algorithm (RVIA) as follows.

(cid:80)

−

Vn+1(s) = max
a∈A

[

λi(s)
v(s)

i
(cid:88)
Vn(s∗)

−

h(s),

−

Ri1{a=A2} +

p(s, s(cid:48), a)Vn(s(cid:48))]

(cid:88)s(cid:48)

(3)

where s∗
value function of state s at nth iteration.

∈ S

is a ﬁxed state and Vn(s) is the estimate of

III. EXPLOITATION OF STRUCTURAL PROPERTIES IN RL

We assume that the optimal policy is of threshold-type
where it is optimal to choose action A2 only upto a thresh-
old τ (i)
which is a non-increasing function of i. In
this section, we propose an RL algorithm which exploits
the knowledge regarding the existence of a threshold-based
optimal policy.

∈ S

Given that the optimal policy is threshold in nature where
the optimal action changes from A2 to A1 at τ (i) for ith
event, the knowledge of τ (0), . . . , τ (N ) uniquely characterizes
the optimal policy. However, computation of these threshold
parameters requires the knowledge of the event probabilities
in state s (governed by λi(s)). When the λi(.)s are unknown,
then we can learn these ordered thresholds instead of learning
the optimal policy from the set of all policies including the
non-threshold policies. We devise an iterative update rule for a
threshold vector of dimensionality (N +1) so that the threshold
vector iterate converges to the optimal threshold vector.

≥

≥

≥

τ (1)

We consider the set of threshold policies where the thresh-
olds for different events are ordered (τ (i)
τ (j) for i < j)
and represent them as policies parameterized by the threshold
vector τ = [τ (0), τ (1), . . . , τ (N )]T where τ (0)
≥
τ (N ). In this context, we redeﬁne the notations
. . .
associated with the MDP to reﬂect their dependence on τ .
We intend to compute the gradient of the average reward w.r.t
τ and improve the policy by updating τ in the direction of
the gradient. Let the transition probability from state s to state
s(cid:48) corresponding to the threshold vector τ be Pss(cid:48)(τ ). Hence,
Pss(cid:48)(τ ) = P (Xn+1 = s(cid:48)
Xn = s, τ ). Let the value function
of state s, the average reward of the Markov chain and the
stationary probability of state s parameterized by τ be denoted
by V (s, τ ), σ(τ ) and π(s, τ ), respectively.

|

The optimal policy can be computed using (3) if we know
the state transition probabilities and λi(s)s. When these param-
eters are unknown, theory of SA [23] enables us to replace the
expectation operation in (3) by averaging over time and still
converge to the optimal policy. Let a(n) be a positive step-size
∞
sequence satisfying
.
n=1 a(n) =
∞
Let b(n) be another step-size sequence which apart from the
b(n)
a(n) = 0. We update the value
above properties satisﬁes lim
n→∞
function of the system state (based on the type of event) at
any given iteration and keep the value functions of other states
unchanged. Let Sn be the state of the system at nth iteration.
1{Sx=s}. For a ﬁxed threshold vector τ
Let γ(s, n) =
(i.e., a ﬁxed policy), max operator in (2) goes away, and the
resulting system becomes a linear system. Therefore we have

∞
n=1(a(n))2 <

n
x=1

(cid:80)

(cid:80)

(cid:80)

∞

;

V (s) =

Ri1{l(s,τ (i))=A2} +

p(s, s(cid:48), a)V (s(cid:48))

(cid:88)s(cid:48)

λi(s)
v(s)

i
(cid:88)
−

h(s)

ρ,

−

where l(s, τ (i)) = A2 if s < τ (i), l(s, τ (i)) = A1 else.

The update of value function of state s (corresponding to

3

the ith event) is done using the following scheme:
Vn+1(s, τ ) =(1

a(γ(s, n)))Vn(s, τ ) + a(γ(s, n))[

−

Ri1{l(s,τ (i))=A2} + Vn(s(cid:48), τ )
˜s

= s,

−

Vn+1(˜s, τ ) = Vn(˜s, τ ),

h(s)+

−

Vn(s∗, τ )],

∀

(4)

where Vn(s, τ ) denotes the value function of state s at nth
iteration provided the threshold vector is τ . This is known as
the primal RVIA which is performed in the faster timescale.
The scheme (4) works for a ﬁxed value of threshold vector.
To obtain the optimal value of τ , the threshold vector needs
to be iterated in a slower timescale b(.). The idea is to learn
σ(τ ) and update
the optimal threshold vector by computing
the value of threshold in the direction of the gradient. This
scheme is similar to stochastic gradient routine:

∇

τ n+1 = τ n + b(n)

σ(τ n),

(5)

∇

where τ n is the threshold vector at nth iteration. Conditions on
step sizes ensure that the value function and threshold vector
iterates are updated in different timescales. From the slower
timescale, the value functions seem to be quasi-equilibrated,
whereas form the faster timescale, the threshold vector appears
to be quasi-static (known as the “leader-follower” behavior).
Given a threshold vector τ , it is assumed that the transition
from state s for ith event is driven by the rule P1(s(cid:48)
s) if s <
|
τ (i) and by the rule P0(s(cid:48)
s),
|
the system moves from state s to state s(cid:48) = s + 1 following
action A2 if s < τ (i). On the other hand, rule P0(s(cid:48)
s) dictates
that the system remains in state s following action A1. For a
ﬁxed τ , (4) is updated using the above rule. Let gs(τ (i)) =
Ri1{l(s,τ (i))=A2} −
h(s). The following assumption is made
on Pss(cid:48)(τ ) and gs(τ ) to embed the discrete parameter τ into
a continuous domain later.

s) otherwise. Under rule P1(s(cid:48)

|

|

Assumption 1. Pss(cid:48)(τ ) and gs(τ ) are bounded and twice
differentiable functions of τ . It has bounded ﬁrst and second
derivatives.

For a given event, the threshold policy chooses the rule
.), thereafter.
.) upto a threshold and follows the rule P0(.
P1(.
|
|
Therefore the threshold policy is deﬁned at discrete points
and does not satisfy Assumption 1 as the derivative is un-
deﬁned. To address this issue, we propose an approximation
interpolation to continuous domain) of the threshold policy,
(
≈
which resembles a step function, so that the derivative exists at
every point. This results in a randomized policy which in state
s, chooses policies P0(s(cid:48)
s) with probabilities
|
f (s, τ ) and 1

f (s, τ ), respectively. In other words,

s) and P1(s(cid:48)

|

|

−

≈

s)(1

Pss(cid:48)(τ )

f (s, τ )).

s)f (s, τ ) + P1(s(cid:48)
|
Intuitively, f (s, τ ) should allocate similar probabilities to
P0(s(cid:48)
s) near the threshold. As we move away
|
towards the left (right) direction, the probability of choosing
P0(s(cid:48)
s)) should decrease. The following function
is chosen as a convenient approximation as it is continuously
differentiable and the derivative is non-zero at every point.

s) and P1(s(cid:48)
|
s) (P1(s(cid:48)
|

(6)

|

−
P0(s(cid:48)

f (s, τ (i)) =

e(s−τ (i)−0.5)
1 + e(s−τ (i)−0.5) .

(7)

(cid:54)
Similar to (6), we approximate gs(τ ) as

gs(τ (i))

f (s, τ (i))h(s) + (1

≈ −

f (s, τ (i)))(Ri −

−

h(s)).

Remark 1. Note that although the state space is discrete, indi-
vidual threshold vector component iterates may take values in
the continuous domain. However, only an ordinal comparison
dictates which action needs to be chosen in the current state.

−

Remark 2. Instead of the sigmoid function in (7), the function
f (s, τ (i)) = 1.1{s≥τ (i)+1} + (s
τ (i))1{s<τ (i)<s+1} which
uses approximation only when s < τ (i) < s + 1, could have
been chosen. This function suffers less approximation error
than that of (7). However, it may lead to slow convergence
since the derivative of the function and hence the gradient
becomes zero outside s < τ (i) < s+1. Although the derivative
of sigmoid function decays exponentially fast too, we observe
in simulations that the convergence behavior with (7) is better.

Under Assumption 1, the following proposition [33, Propo-
sition 1] provides a closed form expression for the gradient
of σ(τ ). The proposition stated next is extended to policy
gradient theorem in [34].

Proposition 1.

∇

σ(τ ) =

π(s, τ )(

(cid:88)s∈S

gs(τ ) +

∇

∇

(cid:88)s(cid:48)∈S

Pss(cid:48)(τ )V (s(cid:48), τ )).

Remark 3. The ﬁrst term on the right of (8) was dropped
earlier, the reason given being that ‘the reward is independent
of the parameter’. This is incorrect because the indicator
function multiplying the reward is not so. Nevertheless, it turns
out (see Fig. 1a and 1b) that the performance is hardly affected
by the omission of this term. This is presumably because
this term is signiﬁcant only in a small neighborhood of the
threshold. This mistake is present in our earlier works [1],
[30] and similar remarks apply there. 1

Based on the proposed approximation (7), we set to devise
an online update rule for the threshold vector in the slower
Pss(cid:48)(τ ) as a
timescale b(.), following (5). We evaluate
σ(τ ) since the stationary probabilities in
representative of
(8) can be replaced by averaging over time. Using (6), we get

∇

∇

−

∇

∇

(9)

s))

s)
|

f (s, τ ).

P1(s(cid:48)
Pss(cid:48)(τ ) = (P0(s(cid:48)
|
We incorporate a multiplying factor of 1
2 in the right hand side
of (9) since multiplication by a constant term does not alter the
scheme. The physical signiﬁcance of this operation is that at
every iteration, transitions following rules P1(.
.)
f (s, τ ) depends on the
are adopted with equal probabilities.
system state and threshold vector at any given iteration. Similar
gs(τ ) using an identical
online rule can be devised for
procedure.

.) and P0(.
|
|

∇

∇

Based on this, when ith event occur, the online update rule

for the ith component of the threshold vector is as follows.
1)βn ˆhβn (s, i)+

τn+1(i) =Ωi[τn(i) + b(n)

f (s, τn(i))((

1)αn Vn(ˆs, τn(i)))],

∇

(
−

−

4

αn)P0(ˆs
|

where αn and βn are i.i.d. random variables, each of which
can take values 0 and 1 with equal probabilities. If αn = 1,
.), else by
then the transition is governed by the rule P1(.
|
.). In other words, the next state is ˆs with probability
P0(.
|
s). Similarly, ˆhβ(s, i) =
αnP1(ˆs
βh(s) +
s) + (1
−
−
|
h(s) and
h(s)) where the obtained reward is
(1
β)(Ri −
−
−
h(s) with equal probabilities. The projection operator
Ri −
Ωi ensures that τ (i) iterates remain bounded in a speciﬁc
interval, as speciﬁed later. Recall
that, we have assumed
that the threshold for ith event is a non-increasing function
of i. Therefore, the ith component of the threshold vector
1)th
iterates should always be less than or equal to the (i
component. The ﬁrst component of τ is considered to be a free
variable which can choose any value in [0, W ]. The projection
ensures that τn(i) remains bounded in
operator Ωi,
[0, τn(i

1)]. To be precise,

i > 0
}
{

−

−
Ω0 : x
Ωi : x

(x

(x

0

0

∨

∨

∧

∧

(cid:55)→

(cid:55)→

W )

τ (i

∈

−

[0, W ],

1))

[0, τ (i

1)].

−

i > 0.

∀

∈

The framework of SA enables us to obtain the effective drift in
(9) by performing averaging. Therefore the online RL scheme
where the value functions and the threshold vector are updated
is as
in the faster and the slower timescale, respectively,
follows. We suppress the parametric dependence of V on τ .

(8)

Vn+1(s) =(1

a(γ(s, n)))Vn(s) + a(γ(s, n))[

h(s)+

−

Ri1{l(s,τ (i))=A2} + Vn(s(cid:48))
= s,

˜s

−

−
Vn(s∗)],

Vn+1(˜s) =Vn(˜s),

∀

(10)

f (s, τn(i))((

1)βn ˆhβn (s, i)+

−

∇

and

(

τn+1(i) =Ωi[τn(i) + b(n)
1)αnVn(ˆs))],
τn+1(i(cid:48)) =τn(i(cid:48)),
i(cid:48) < i,
∀
τn+1(i(cid:48)) =Ωi[τn(i(cid:48))],

−

i(cid:48) > i,
∀

(11)

−

−

αn)P0(ˆs
|
βnh(s) + (1
−

where for current state s, transition to next state s(cid:48) in (10)
refers to a single run of a simulated chain as commonly seen in
RL, and ˆs in (11) is determined separately following the proba-
s). The immediate
bility distribution αnP1(ˆs
s)+(1
|
reward is determined using
h(s)).
βn)(Ri −
The physical signiﬁcance of (11) is that when ith type of event
occurs, then the ith component of τ is updated. However,
since the components are provably ordered, we need to update
the i(cid:48)th components too where i(cid:48) > i. We no longer need to
update the components for i(cid:48) < i since the order is already
preserved while updating the ith component. Also, in (10),
the reward function is taken to be Ri1{l(s,τ (i))=A2} when ith
event occurs. The expectation operation in (3) is mimicked by
the averaging over time implicit in an SA scheme. Note that
contrary to [30] where due to the independence among the
threshold parameters, only one threshold is updated at a time,
in this paper, multiple threshold parameters may need to get
updated to capture the ordering constraints.

1We thank Prof. Aditya Mahajan, McGill University for pointing out this

error.

Remark 4. Instead of the two-timescale approach adopted in
this paper, a multi-timescale approach where each individual

(cid:54)
threshold is updated in a separate timescale, may be chosen.
However, since the updates of thresholds are coupled only
they can be updated in
through the ordering constraints,
the same timescale. Moreover, in practice, a multi-timescale
scheme may not work well since the fastest (slowest) timescale
may be too fast (slow), leading to higher ﬂuctuations and/or
very slow speed.

Theorem 1. Update rules (10) and (11) converge to the
optimal policy almost surely (a.s.).

Proof. Proof is given in Appendix B.

Remark 5. Unlike the value function iterates, the thresh-
old vector iterates do not require local clocks of individual
elements for convergence. However, all components of the
threshold vector are required to get updated comparably often.
In other words, their frequencies of update should be bounded
away from zero which generally holds true for stochastic
gradient approaches [23, Chapter 7].

Based on the foregoing analysis, we describe the resulting
SALMUT algorithm in Algorithm 1. On a decision epoch,
action a is chosen based on the current value of threshold
vector. Based on the event, value function of current state s is
updated then using (10) in the faster timescale. The threshold
vector is also updated following (11) in the slower timescale.
Note that the value function is updated one component at a
time. However, multiple components of the threshold vector
may need to be updated in a single iteration. The scheme
resembles actor-critic method [13] with policy gradient for
the actor part (see (11)) and post-decision framework for the
critic (see (10)).

if ith event occurs then

Algorithm 1 Two-timescale SALMUT algorithm
1: Initialize n ← 1, V (s) ← 0, ∀s ∈ S and τ ← (cid:126)0.
2: while TRUE do
3:
4:
5:
6:
7:
8:
9: end while

end if
Update value function of state s using (10).
Update threshold τ using (11).
Update s ← s(cid:48) and n ← n + 1.

Choose action a based on current value of τ (i).

Remark 6. Even if there does not exist an optimal threshold
policy for a given MDP problem, the techniques in this paper
can be applied to learn the best threshold policy (locally at
least) asymptotically. Threshold policies are easy to implement
and often provide comparable performances to that of the
optimal policy, with a signiﬁcantly lower storage complexity.

IV. COMPLEXITY ANALYSIS

In this section, we compare the storage and computational
complexities of SALMUT algorithm with those of existing
learning schemes and summarize in Table I. Q-learning and
PDS learning need to store the value function of every state-
action pair and every PDS, respectively. While updating the
value function, both choose the best one after evaluating
|A|
functions. Since only one state-action pair is updated at a time,

5

Table I: Complexities of RL algorithms.

Algorithm

Q-learning [6], [7]
Monotone Q-learning [17], [19]
MH-Q-learning
OQL [9]
EEQL [10]
PDS learning [4], [11]
MH-PDS learning
VE learning [12]
Grid learning [20]
Adaptive appx. learning [18]
SALMUT

Computational
complexity
O(|A|)
O(|A|)
O(log |A|)
O(log |A|)
O(log |A|)
O(|A|)
O(log |A|)
O(|V| × |A|)
O(|W| × |A|)
O(q(δ)|A|)
O(1)

Storage
complexity
O(|S| × |A|)
O(|S| × |A|)
O(|S| × |A|)
O(|S| × |A|)
O(|S| × |A|)
O(|S|)
O(|S|)
O(|S|)
O(|S|)
O(|S|)
O(|S|)

|S|

|S|

|V|

|W|

|W|

) and O(

and q(δ), respectively.

the computational complexity associated with remaining oper-
ations is constant. Thus, the storage complexities of Q-learning
and PDS learning are O(
), respectively.
|S| × |A|
The per-iteration computational complexity of Q-leanring and
PDS learning is O(
). Since at each iteration, the monotone
Q-learning algorithm [17], [19] projects the policy obtained
using Q-learning within the set of monotone policies, the com-
plexities are identical to those of Q-learning. VE learning [12]
updates multiple PDSs at a time. Therefore, the computational
which signiﬁes
complexity contains an additional term
the cardinality of the VE tuple. Similarly, grid learning [20]
and adaptive approximation learning [18] are associated with
additional factors
and q(δ)
depend on the depth of a quadtree used for value function
approximation and the approximation error threshold δ, re-
spectively. Note that computational complexities of Q-learning
and PDS learning can be reduced to O(log
) using a max-
heap implementation (MH-Q-learning and MH-PDS learning
in Table I) where the complexities of obtaining the best action
and updating a value are O(1) and O(log
), respectively.
The computational complexity of a max-heap implementation
of Optimistic Q-learning (OQL) [9] and Exploration Enhanced
). The storage complexity
Q-learning (EEQL) [10] is O(log
|A|
of OQL and EEQL is O(
). It is not clear whether
|S| × |A|
the knowledge of structural properties can be encoded in Q-
learning easily since the range of the threshold is large, viz.,
the entire state space. The storage complexity of SALMUT
algorithm is O(
) as we need to store the value functions of
states. SALMUT may require to update all components of the
threshold vector at a time (See (11)). Furthermore, the update
of value function involves the computation of a single function
based on the current threshold (See (10)). Therefore, the per-
iteration computational complexity is O(1). Thus SALMUT
provides signiﬁcant improvements in storage and per-iteration
computational complexities compared to other schemes. Note
that
the computational complexity of SALMUT does not
depend on

and depends only on the number of events.

|A|

|A|

|S|

|A|

V. ILLUSTRATIVE EXAMPLE

We consider a queuing system with m identical servers, N
customer classes and a ﬁnite buffer of size B. We investigate
an optimal admission control problem. It is assumed that the
arrival of class-i customers is a Poisson process with mean λi.
Let the service time be exponentially distributed with mean

}

∈ {

∈ {

0, 1, . . . , N

0, 1, . . . , m + B

1
µ , irrespective of the customer class. Note that presence of
m identical servers does not change the nature of the model.
Similar analysis holds for the single server case also. The
state of the system is (s, i) where s
}
denotes the total number of customers in the system and
denotes the class type. Arrivals and de-
i
partures of customers are taken as decision epochs. We take
i = 0 as the departure event and i = 1, . . . , N as an arrival
of a 1, . . . , N th class of customer. For example, states (2, 0)
and (2, 1) correspond to a departure and an arrival of class 1
customer while there are 2 users in the system, respectively.
consists of three actions, viz., blocking of an arriving user
A
(A1), admission of an arriving user (A2) and continue/do
nothing (A0, say) for departures. When s = m + B, then
the only feasible action for an arrival (i.e., i > 0) is A1.
We have, p(s, s(cid:48), A2) = 1{s(cid:48)=s+1}, p(s, s(cid:48), A1) = 1{s(cid:48)=s}
and p(s, s(cid:48), A0) = 1{s(cid:48)=(s−1)+}. where x+ = max
and
x, 0
}
{
N
µ and
µ. Let λ0(s) = min
v(s) =
s, m
s, m
i=1 λi +min
}
{
}
{
= 0. Note that λ1(s), . . . , λN (s) do not depend
λi(s) = λi,
(cid:80)
on s. Furthermore, r((s, i), a) = Ri1{a=A2,i>0}. R0 = 0
corresponds to a departure event. In state (s, i), a non-negative
cost rate of h(s) is incurred. h(s) and h(s + 1)
h(s) are
increasing in s (convex increasing in the discrete domain).

i
∀

−

One application of this model is a make-to-stock production
system that produces m items with N demand classes [35] and
buffer size B. Satisfaction of a demand (requesting a single
unit of the product) from class-i gives rise to reward rate Ri.
The production time is exponentially distributed with mean 1
µ .
The inventory holding cost rate is h(s).

Now, we derive that there exists a threshold based optimal
policy which admits a class-i customer only upto a threshold
τ (i) which is a non-increasing function of i. We prove these
properties using the following lemma. The proof follows from
[36, Theorem 3.1]. Detailed proof is given in Appendix A.

Lemma 1. V (s + 1)

−

V (s) is decreasing in s.

Theorem 2. The optimal policy is of threshold-type where it
is optimal to admit class-i customers only upto a threshold
which is a non-increasing function of i.
τ (i)

∈ S

Proof. For class-i customers, if A1 is optimal in state s, then
V (s) (Using (2)). From Lemma 1, V (s +
Ri + V (s + 1)
V (s) is decreasing in s. This proves the existence of
1)
a threshold τ (i) for class-i customers. Since Ri > Rj for
i < j, Ri + V (s + 1)
V (s).
Therefore, τ (i) is a non-increasing function of i.

V (s) implies Rj + V (s + 1)

≤

≤

−

≤

The subsequent lemmas establish the unimodality of the
average reward with respect to τ . Hence, Theorem 1 holds.
Proofs are presented in Appendix C.

Lemma 2. vn(s + 1)

−

vn(s) is decreasing in n.

Lemma 3. σ(τ ) is unimodal in τ .

For this particular problem, the improvement in computa-
tional complexity offered by SALMUT may not be signiﬁcant
, the
= 3. However, in general, for a large
since
improvement in computational complexity may be remarkable.

|A|

|A|

6

VI. SIMULATION RESULTS

In this section, we compare the convergence speed of
SALMUT algorithm with traditional RL algorithms. We sim-
ulate the ﬁnite buffer multi-server system with two customer
classes. We take λ1 = λ2 = 1 s−1, m = B = 5, R1 = 20,
100 (cid:99)+2)0.6 and b(n) = 10
R2 = 10, h(s) = 0.1s2, a(n) =
n .
((cid:98) n
We exclude initial 10 burn-in period values of the iterates.

1

(cid:80)

In practical cases, when the average reward of the system
does not change much over a suitable window, we may
conclude that stopping condition is met as the obtained policy
is close to the optimal policy with a high probability. The
p+n
choice of window size
k=p a(k) over n is to eliminate
the effect of diminishing step size affecting the convergence
behavior. We choose a window size of 50 and stop when
the ratio of maximum and minimum average rewards exceeds
0.95. Fig. 1a (1b) reveals that practical convergences for Q-
learning, PDS learning and SALMUT algorithms are achieved
in 1180 (1180), 580 (1180) and 426 (580) iterations, respec-
tively. Since unlike PDS learning, Q-learning is associated
with exploration mechanism, PDS learning converges faster
than Q-learning. However, SALMUT converges faster than
both Q-learning and PDS learning algorithms since it operates
on a smaller policy space (threshold policies only). Due to
efﬁcient exploration, the performances of OQL [9] and EEQL
[10] are better than that of Q-learning and slightly worse
than that of SALMUT. Since for a given sample path, the
sequence of actions are same, the performance of MH-Q-
learning (MH-PDS learning) is identical to that of Q-learning
(PDS learning). The performance of adaptive approximation
learning algorithm [18] is identical to that of PDS learning as
batch update of PDSs is not possible. Therefore, we do not
show their performances in the plot. The convergence behavior
of Renewal Monte Carlo (RMC) [21], [22] algorithm (with
discount factor=0.999) is slightly worse than that of SALMUT.
Since the sampled Q-learning algorithm [24] provides a near-
optimal policy after a ﬁnite number of visits to a set of state-
action pairs, the convergence behavior may not be comparable.

VII. CONCLUSIONS & FUTURE DIRECTIONS

In this paper, we have proposed an RL algorithm which
exploits the ordered multi-threshold nature of the optimal
policy. The convergence of the proposed algorithm to the
globally optimal threshold vector is established. The proposed
scheme provides improvements in storage and computational
complexities over traditional RL algorithms. Simulation re-
sults establish the improvement in convergence behavior with
respect to state-of-the-art RL schemes. In future, this can
be extended to develop RL algorithms for constrained MDP
problems by updating the Lagrange Multiplier (LM) in a
slower timescale [23] than that of value functions. The LM
and the threshold parameter can be updated in the same
slower timescale without requiring a third timescale as they are
independent of each other. Another possible future direction is
to develop RL algorithms for restless bandits such as [37] since
threshold policies often translate into index-based policies.

(cid:54)
7

Figure 1: Plot of average reward vs. (

n

k=1 a(k)) for different algorithms ((a) µ = 4s−1, (b) µ = 2s−1).

(a)

(b)

(cid:80)

APPENDIX A
PROOF OF LEMMA 1

Proof techniques are similar to those of our earlier work

[1]. The optimality equation for value function is
[Ri1{a=A2} +

p(s, s(cid:48), a)V (s(cid:48))]

¯V (s, i) = max
a∈A

(cid:88)s(cid:48)
In Value Iteration Algorithm (VIA), let the value function
of state s at nth iteration be denoted by vn(s). We start with
v0(s) = 0. Since
. Therefore, v0(s + 1)
v0(s) = 0,
s
∀
(using deﬁnition of V (s))

∈ S

−

h(s).

−

N

vn+1(s) =

+ min

i=1
(cid:88)
µvn((s
s, m
}
{

λi max
{

Ri + vn(s + 1), vn(s)
}

1)+) + (1

−

v(s))vn(s)

−

−

h(s),

(12)

−

h(s) is increasing in s, v1(s + 1)

v1(s) is
and h(s + 1)
−
decreasing in s. Let us assume that vn(s + 1)
vn(s) is a
decreasing function of s. We require to prove that vn+1(s +
vn(s) =
1)
V (s), this implies the lemma.

vn+1(s) is a decreasing function of s. Since lim
n→∞

−

−

We deﬁne ˆvi,n+1(s, a) and ˆvi,n+1(s),

i
∀

∈ {

1, 2, . . . , N

}

as

ˆvi,n+1(s, a) =

a = A1,
vn(s),
Ri + vn(s + 1), a = A2.

(cid:40)

and ˆvi,n+1(s) = max
a∈A
Dvn(s) = vn(s+1)
ˆvi,n(s, a). Hence, we have,

−

ˆvi,n+1(s, a). Also, we deﬁne
vn(s) and Dˆvi,n(s, a) = ˆvi,n(s+1, a)

−

Dˆvi,n+1(s, a) =

and

D2ˆvi,n+1(s, a) =

Dvn(s),
a = A1,
Dvn(s + 1), a = A2,

D2vn(s),
a = A1,
D2vn(s + 1), a = A2.

(cid:40)

(cid:40)

Since vn(s+1)
1, a)

vn(s) is a decreasing function of s, ˆvi,n+1(s+
and

ˆvi,n+1(s, a) is decreasing in s,

1, 2, . . . N

−

i
∀

∈ {

}

−

. Let the maximizing actions for the admission of a
a
∀
∈ A
class-i customer in states (s+2) and s be denoted by ai,1 ∈ A
and ai,2 ∈ A
ˆvi,n+1(s + 1, ai,1) + ˆvi,n+1(s + 1, ai,2)
2ˆvi,n+1(s + 1)
= ˆvi,n+1(s + 2, ai,1) + ˆvi,n+1(s, ai,2) + Dˆvi,n+1(s, ai,2)

, respectively. Therefore,

≥

Dˆvi,n+1(s + 1, ai,1).

−

Let us denote Z = Dˆvi,n+1(s, ai,2)
prove that ˆvi,n+1(s + 1)
need to prove that Z
• ai,1 = ai,2 = A1
Z = Dvn(s)

Dvn(s + 1) =

≥

Dˆvi,n+1(s + 1, ai,1). To
ˆvi,n+1(s) is non-increasing in s, we

−
0. We consider the following cases.

−

D2vn(s)

−

0.

≥

−

• ai,1 = A1, ai,2 = A2
Z = Dvn(s + 1)
• ai,1 = ai,2 = A2
Z = Dvn(s + 1)
−
• ai,1 = A2, ai,2 = A1

−

Dvn(s + 1) = 0.

Z = Dvn(s)

Dvn(s + 2) =

−

−

Dvn(s + 2) =

D2vn(s + 1)

0.

−
D2vn(s)

≥
D2vn(s + 1)

−

0.

≥

To analyze the difference of second and third terms in Equation
(12) corresponding to states (s + 1) and s, we consider two
cases.

• s

≥

m: Both mµ(vn(s)+

vn(s

1)+) and (1

−

−

mµ)(vn(s + 1)

vn(s)) are decreasing in s.

−
• s < m: The difference is equal to

N

−

i=1
(cid:80)

λi −

sµ(vn(s)+

vn(s

−
which is decreasing in s.

−

1)+)+(1

v(s+1))(vn(s+1)

vn(s)),

−

−

vn(s) is decreasing in s. Hence, V (s + 1)

h(s) is increasing in s, this proves that vn(s+
V (s) is

−

Since h(s+1)
1)
decreasing in s.

−

−

APPENDIX B
PROOF OF THEOREM 1

We adopt the approach of viewing SA algorithms as a noisy
discretization of a limiting Ordinary Differential Equation

0200400600800102030405060nPk=1a(k)AveragerewardSALMUTSALMUT(w/o∇gs(τ))PDSlearningQlearningOQLEEQLRMC0200400600800102030405060nPk=1a(k)AveragerewardSALMUTSALMUT(w/o∇gs(τ))PDSlearningQlearningOQLEEQLRMC(ODE), similar to [1], [30]. Step size parameters are viewed
as discrete time steps. Standard assumptions on step sizes
the errors due to noise and discretization are
ensure that
negligible asymptotically. Therefore, the iterates closely follow
the trajectory of the ODE and ensure a.s. convergence to
the globally asymptotically stable equilibrium. Using the two
timescale approach [23], we consider (10) for a ﬁxed τ . Let
M1 :

|S| be the following map

|S|

R

→ R

8

Proof. The limiting ODE for (11) is the gradient ascent
scheme ˙τ =
σ(τ ). Note that the gradient points inwards
at τ (.) = 0 and τ (.) = W . Since σ(τ ) is unimodal in τ , there
does not exist any local maximum except τ ∗ which is the
global maximum. This concludes the proof of the lemma.

∇

Remark 7. If the unimodality of the average reward with
respect to the threshold vector does not hold, then convergence
to only a local maximum can be guaranteed.

M1(x) =

Pss(cid:48)(τ )[

N

λi(s)
v(s)

Ri1{a=A2} −
|S|.

(cid:88)s(cid:48)
+ x(s(cid:48))]

−

i=0
(cid:88)
x(s∗), x

∈ R

h(s)

(13)

APPENDIX C

A. Proof of Lemma 2:
Proof. Proof methodologies are similar to [1].

Note that

the knowledge of Pss(cid:48)(τ ) and λi(s) are not
required for the algorithm and is only required for analysis. For
a ﬁxed τ , (10) tracks the limiting ODE ˙V (t) = M1(V (t))
−
V (t). V (t) converges to the ﬁxed point of M1(.) (determined
using M1(V ) = V ) [38] which is the asymptotically stable
. Analogous methodolo-
equilibrium of the ODE, as t
gies are adopted in [38], [39]. Note that the approximation
described in (7) does not impact the convergence argument
since the ﬁxed point of M1(.) remains the same. Next we
establish that the value function and threshold vector iterates
are bounded.

→ ∞

Lemma 4. The threshold vector and value function iterates
are bounded a.s.

Proof. Let M0 :

|S|

|S| be the following map

R

→ R
Pss(cid:48)(τ )x(s(cid:48))

M0(x) =

(cid:88)s(cid:48)

x(s∗), x

|S|.

∈ R

−

(14)

Clearly, if the reward and cost functions are zero, (13)
= M0(V ). The
is same as (14). Also,
limb→∞
globally asymptotically stable equilibrium of the ODE ˙V (t) =
V (t) which is a scaled limit of ODE ˙V (t) =
M0(V (t))
V (t), is the origin. Boundedness of value func-
M1(V (t))
tions and threshold vector iterates follow from [40] and (11),
respectively.

M1(bV )
b

−
−

Lemma 5. Vn −
→
of states for τ = τ n a.s.

V τ n

0, where V τ n is the value function

Proof. Since the threshold vector iterates are updated in a
slower timescale, value function iterates in the faster timescale
treat the threshold vector iterates as ﬁxed. Therefore, iterations
for τ are τ n+1 = τ n + γ(n), where γ(n) = O(b(n)) =
the limiting ODEs for value function
o(a(n)). Therefore,
and threshold vector iterates are ˙V (t) = M1(V (t))
V (t)
and ˙τ (t) = 0, respectively. It is sufﬁcient to consider the
ODE ˙V (t) = M1(V (t))
V (t) alone for a ﬁxed τ because
˙τ (t) = 0. The rest of the proof is analogous to that of [13].

−

−

For the time being assume that σ(τ ) is unimodal in τ .
This is proved later. The lemmas presented next establish that
threshold vector iterates τ n converge to the optimal threshold
vector τ ∗ and hence, (Vn, τ n) converges to the optimal pair
(V, τ ∗).
Lemma 6. The threshold vector iterates τ n →

τ ∗ a.s.

N

vn+1(s) =

i=1
(cid:88)
min

λi max
{

+
Ri + vn(s + 1), vn(s)
}

µvn((s
s, m
}
{

−

1)+) + (1

v(s))vn(s)

−

h(s),
(15)

−

N

N

−

−

−

≥

≤

−

−

(cid:80)
−
(cid:80)

1)+) + (1

1)+) + (1

1)+) + (1

A1, A2}

−
1)+)+(1

v(s + 1))Dvn(s).

i=1 λi −
i=1 λi−
v(s + 1))Dvn+1(s) is

. We know,
and ˆvi,n+1(s) = max
vn(s), Ri + vn(s + 1)
{
}
vn(s). We use induction to prove
Dvn(s) = vn(s + 1)
−
that Dvn(s) is decreasing in n. If n = 0, v0(s) = 0 and
Dv0(s) = 0. Using (15), it is easy to see that Dv1(s) <
Dv0(s) as h(s + 1) > h(s). Assuming that the claim holds
for any n, i.e., Dvn+1(s) < Dvn(s), we need to prove that
Dvn+2(s) < Dvn+1(s). To analyze the second and third terms
in (15), we consider two cases.
m: mµDvn+1((s
(a) s
mµ)Dvn+1(s) is less than mµDvn((s
mµ)Dvn(s).
(b) s < m: sµDvn+1((s
less than sµDvn((s
−
We proceed to prove that Dˆvi,n+2(s)
Dˆvi,n+1(s). Let
at (n + 2)th iteration, maximizing actions for the admission
of class-i customers in states s and (s + 1) be denoted by
, respectively. Let
ai,1 ∈ {
be the maximizing actions in states s
bi,1, bi,2 ∈ {
and (s + 1), respectively, at (n + 1)th iteration. It is not
possible to have ai,2 = A2 and bi,1 = A1. If bi,1 = A1, then
Dvn(s)
Ri
(Using Lemma 1). If ai,2 = A2, then Dvn+1(s + 1)
Ri
which contradicts the inductive assumption. Therefore, we
consider the remaining cases. Note that
if the inequality
holds for any ai,1 and bi,2 for given ai,2 and bi,1, then the
maximizing actions will satisfy the inequality too.
(a) ai,2 = bi,1 = A1: We choose ai,1 = bi,2 = A1 to get
Dˆvi,n+2(s)
(b) ai,2 = bi,1 = A2: Proof is similar to the preceding case
by choosing ai,1 = bi,2 = A2.
(c) ai,2 = A1, bi,1 = A2: Choose ai,1 = A2 and bi,2 = A1.
Dˆvi,n+2(s)
vn(s + 1) + Ri + vn(s + 1) = 0.
Thus, Dˆvi,n+2(s)
and h(s) is independent of n, this concludes the proof.

Ri. Therefore, we must have Dvn(s + 1) <

Dˆvi,n+1(s). Since this holds for every i

Dˆvi,n+1(s) = vn+1(s + 1)

Dˆvi,n+1(s) = Dˆvi,n+1(s)

and ai,2 ∈ {

A1, A2}

A1, A2}

vn+1(s + 1)

−
≥ −

Dˆvi,n(s)

Ri −

≤ −

−

≤

−

−

−

−

≤

0.

B. Proof of Lemma 3:
Proof. Proof idea is similar to that of [30]. We prove this
lemma for ith component of the threshold vector (viz., τ (i)).

≤

−

−

−

Ri,

Ri,

∀
−

≤ −

≤ −

V (s)

τ (cid:48)(i)

vn(s)

vn(s)

n
≥
vn(s)

N0, vn(s + 1)

s
∀
s < τ ∗(i). Let Ui,n, n
∀
N0 : vn(s + 1)

If the optimal action for the admission of class-i customers
in state s is A1, then V (s + 1)
Ri. Since VIA
converges to the optimal threshold vector τ ∗ in ﬁnite time,
N0 > 0 such that
∃
≥
τ ∗(i) and vn(s+1)
≥
≥ −
1 be the optimal threshold for class-i customers at nth iteration
of VIA. Hence, Ui,n = min
s
{
Ri}
−
m+B. Since vn(s+1)
−
Ui,n is monotonically decreasing in n, and lim
n→∞

≤
∈
. If no values of s satisﬁes the inequality, then Ui,n =
vn(s) is decreasing in n (Lemma 2),
Ui,n = τ ∗(i).
Consider a re-designed problem where for a given threshold
vector τ (cid:48) such that τ ∗(i)
m + B, action A1
≤
is not allowed in any state s < τ (cid:48)(i). Note that Lemma 2
holds for this re-designed problem also. Let nτ (cid:48)(i) be the
ﬁrst iteration of VIA when the threshold reduces to τ (cid:48)(i).
The value function iterates for the original and re-designed
problem are same for n
nτ (cid:48)(i) because in the original
problem also A1 is never chosen as the optimal action in states
s < τ (cid:48)(i) at these iterations. Hence, nτ (cid:48)(i) must be ﬁnite and
the inequality vn(τ (cid:48)(i) + 1)
Ri is true for
−
both the problems after nτ (cid:48)(i) iterations. Using Lemma 2, this
τ (cid:48)(i). Therefore, in the re-designed
inequality holds
problem, Ui,n converges to τ (cid:48)(i). Thus, the threshold policy
with τ (cid:48)(i) is superior than that with τ (cid:48)(i) + 1. Since this holds
for arbitrary choice of τ (cid:48)(i), average reward monotonically
decreases with τ (cid:48)(i),
τ (cid:48)(i) > τ ∗(i).
∀
σ(τ + ei) (where ei ∈
If we have σ(τ )
≥

RN is a vector
with all zeros except the ith element being ‘1’), then we must
σ(τ + 2ei). Hence,
have τ (i)
the average reward is unimodal in τ (i). Since the proof holds
for any i, this concludes the proof of the lemma.

τ ∗(i). Therefore, σ(τ + ei)

vn(τ (cid:48)(i))

≤ −

≥

≥

≥

≤

n

∀

ACKNOWLEDGMENT

Works of Arghyadip Roy, Abhay Karandikar and Prasanna
Chaporkar are supported by the Ministry of Electronics and
Information Technology (MeitY), Government of India as
part of “5G Research and Building Next Gen Solutions for
Indian Market” project. Work of Vivek Borkar is supported
by the CEFIPRA grant for “Machine Learning for Network
Analytics” and a J. C. Bose Fellowship.

REFERENCES

[1] A. Roy, V. Borkar, A. Karandikar, and P. Chaporkar, “A structure-aware
online learning algorithm for Markov decision processes,” in ACM EAI
VALUETOOLS, 2019, pp. 71–78.

[2] M. L. Puterman, Markov decision processes: discrete stochastic dynamic

programming.

John Wiley & Sons, 2014.

[3] E. B. C¸ il, E. L. ¨Ormeci, and F. Karaesmen, “Effects of system parameters
on the optimal policy structure in a class of queueing control problems,”
Queueing Systems, vol. 61, no. 4, pp. 273–304, 2009.

of dimensionality.

[4] W. B. Powell, Approximate Dynamic Programming: Solving the curses
John Wiley & Sons, 2007.
[5] R. Bellman, “Dynamic programming,” Princeton University Press, 1957.
[6] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.

MIT press Cambridge, 1998.

[7] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8, no.

3-4, pp. 279–292, 1992.

[8] C. Jin, Z. Allen-Zhu, S. Bubeck, and M. I. Jordan, “Is Q-learning
provably efﬁcient?” in Advances in Neural Information Processing
Systems, 2018, pp. 4863–4873.

9

[9] C.-Y. Wei, M. Jafarnia-Jahromi, H. Luo, H. Sharma, and R. Jain,
“Model-free reinforcement learning in inﬁnite-horizon average-reward
Markov decision processes,” arXiv preprint arXiv:1910.07072, 2019.

[10] M. Jafarnia-Jahromi, C.-Y. Wei, R. Jain, and H. Luo, “A model-free
learning algorithm for inﬁnite-horizon average-reward MDPs with near-
optimal regret,” arXiv preprint arXiv:2006.04354, 2020.

[11] N. Salodkar, A. Bhorkar, A. Karandikar, and V. S. Borkar, “An on-line
learning algorithm for energy efﬁcient delay constrained scheduling over
a fading channel,” IEEE Journal on Selected Areas in Communications,
vol. 26, no. 4, pp. 732–742, 2008.

[12] N. Mastronarde and M. van der Schaar, “Joint physical-layer and system-
level power management for delay-sensitive wireless communications,”
IEEE Transactions on Mobile Computing, vol. 12, no. 4, pp. 694–709,
2012.

[13] V. S. Borkar, “An actor-critic algorithm for constrained Markov decision
processes,” Systems & control letters, vol. 54, no. 3, pp. 207–213, 2005.
[14] D. P. Bertsekas, Dynamic programming and optimal control. Athena

scientiﬁc Belmont, MA, 1995, vol. 1, no. 2.

[15] M. Agarwal, V. S. Borkar, and A. Karandikar, “Structural properties of
optimal transmission policies over a randomly varying channel,” IEEE
Transactions on Automatic Control, vol. 53, no. 6, pp. 1476–1491, 2008.
[16] J. E. Smith and K. F. McCardle, “Structural properties of stochastic
dynamic programs,” Operations Research, vol. 50, no. 5, pp. 796–809,
2002.

[17] S. Kunnumkal and H. Topaloglu, “Exploiting the structural properties of
the underlying Markov decision problem in the Q-learning algorithm,”
INFORMS Journal on Computing, vol. 20, no. 2, pp. 288–301, 2008.

[18] F. Fu and M. van der Schaar, “Structure-aware stochastic control for
transmission scheduling,” IEEE Transactions on Vehicular Technology,
vol. 61, no. 9, pp. 3931–3945, 2012.

[19] M. H. Ngo and V. Krishnamurthy, “Monotonicity of constrained optimal
transmission policies in correlated fading channels with ARQ.” IEEE
Transactions on Signal Processing, vol. 58, no. 1, pp. 438–451, 2010.
[20] N. Sharma, N. Mastronarde, and J. Chakareski, “Accelerated structure-
aware reinforcement learning for delay-sensitive energy harvesting wire-
less sensors,” arXiv preprint arXiv:1807.08315, 2018.

[21] J. Chakravorty and A. Mahajan, “Remote estimation over a packet-drop
channel with markovian state,” IEEE Transactions on Automatic Control,
vol. 65, no. 5, pp. 2016–2031, 2019.

[22] J. Subramanian and A. Mahajan, “Renewal monte carlo: Renewal theory-
based reinforcement learning,” IEEE Transactions on Automatic Control,
vol. 65, no. 8, pp. 3663–3670, 2019.

[23] V. S. Borkar, Stochastic approximation: A dynamical systems viewpoint.

Cambridge University Press, 2008.

[24] L. Liu and U. Mitra, “On sampled reinforcement learning in wireless
networks: Exploitation of policy structures,” IEEE Transactions on
Communications, vol. 68, no. 5, pp. 2823–2837, 2020.

[25] A. Sinha and P. Chaporkar, “Optimal power allocation for a renewable

energy source,” in IEEE NCC, 2012, pp. 1–5.

[26] G. Koole, “Structural results for the control of queueing systems using
event-based dynamic programming,” Queueing systems, vol. 30, no. 3-4,
pp. 323–339, 1998.

[27] G. A. Brouns and J. Van Der Wal, “Optimal threshold policies in a two-
class preemptive priority queue with admission and termination control,”
Queueing Systems, vol. 54, no. 1, pp. 21–33, 2006.

[28] M. H. Ngo and V. Krishnamurthy, “Optimality of threshold policies for
transmission scheduling in correlated fading channels,” IEEE Transac-
tions on Communications, vol. 57, no. 8, 2009.

[29] E. B. C¸ il, E. L. ¨Ormeci, and F. Karaesmen, “Structural results on a batch
acceptance problem for capacitated queues,” Mathematical Methods of
Operations Research, vol. 66, no. 2, pp. 263–274, 2007.

[30] A. Roy, V. Borkar, P. Chaporkar, and A. Karandikar, “Low complexity
online radio access technology selection algorithm in LTE-WiFi hetnet,”
IEEE Transactions on Mobile Computing, vol. 19, no. 2, pp. 376–389,
2019.

[31] R. W. Wolff, Stochastic modeling and the theory of queues.

Pearson

College Division, 1989.

[32] A. Kumar, “Discrete event stochastic processes,” Lecture Notes for
Engineering Curriculum, 2012. [Online]. Available: https://ece.iisc.ac.
in/∼anurag/books/anurag/spqt.pdf

[33] P. Marbach and J. N. Tsitsiklis, “Simulation-based optimization of
Markov reward processes,” IEEE Transactions on Automatic Control,
vol. 46, no. 2, pp. 191–209, 2001.

[34] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour, “Policy
gradient methods for reinforcement learning with function approxima-
tion,” in Advances in neural information processing systems, 2000, pp.
1057–1063.

10

[35] A. Y. Ha, “Inventory rationing in a make-to-stock production system
with several demand classes and lost sales,” Management Science,
vol. 43, no. 8, pp. 1093–1103, 1997.

[36] G. Koole, Monotonicity in Markov reward and decision chains: Theory

and applications. Now Publishers Inc, 2007, vol. 1.

[37] V. S. Borkar and K. Chadha, “A reinforcement learning algorithm for

restless bandits,” in IEEE ICC, 2018, pp. 89–94.

[38] V. R. Konda and V. S. Borkar, “Actor-critic-type learning algorithms for
Markov decision processes,” SIAM Journal on control and Optimization,
vol. 38, no. 1, pp. 94–123, 1999.

[39] J. Abounadi, D. Bertsekas, and V. S. Borkar, “Learning algorithms for
Markov decision processes with average cost,” SIAM Journal on Control
and Optimization, vol. 40, no. 3, pp. 681–698, 2001.

[40] V. S. Borkar and S. P. Meyn, “The ODE method for convergence of
stochastic approximation and reinforcement learning,” SIAM Journal on
Control and Optimization, vol. 38, no. 2, pp. 447–469, 2000.

