2
2
0
2

t
c
O
8
1

]
L
P
.
s
c
[

3
v
8
7
5
3
0
.
7
0
2
2
:
v
i
X
r
a

CODE TRANSLATION WITH COMPILER
REPRESENTATIONS

Marc Szafraniec∗ Baptiste Rozière* Hugh Leather François Charton
Patrick Labatut Gabriel Synnaeve
Meta AI
{mszafraniec,broz}@fb.com

ABSTRACT

In this paper, we leverage low-level compiler intermediate representations (IR)
to improve code translation. Traditional transpilers rely on syntactic information
and handcrafted rules, which limits their applicability and produces unnatural-
looking code. Applying neural machine translation (NMT) approaches to code
has successfully broadened the set of programs on which one can get a natural-
looking translation. However, they treat the code as sequences of text tokens,
and still do not differentiate well enough between similar pieces of code which
have different semantics in different languages. The consequence is low quality
translation, reducing the practicality of NMT, and stressing the need for an approach
signiﬁcantly increasing its accuracy. Here we propose to augment code translation
with IRs, speciﬁcally LLVM IR, with results on the C++, Java, Rust, and Go
languages. Our method improves upon the state of the art for unsupervised code
translation, increasing the number of correct translations by 11% on average,
and up to 79% for the Java → Rust pair with greedy decoding. With beam
search, it increases the number of correct translations by 5.5% in average. We
extend previous test sets for code translation, by adding hundreds of Go and Rust
functions. Additionally, we train models with high performance on the problem of
IR decompilation, generating programming source code from IR, and study using
IRs as intermediary pivot for translation.

1

INTRODUCTION

Automatic code translation allows to port old codebases to new frameworks, or high-level (but
slow) languages to low-level (and fast) ones. Current industry solutions, known as transpilers or
transcompilers1, rely on handcrafted rules that are applied systematically. They produce unidiomatic
translations that prove hard to read for human programmers. This is a serious limitation: the translated
code should be easy to read and understand, as it will eventually be maintained by human developers.

In recent years, Neural Machine Translation (NMT) was proposed as an alternative to rule-based
code translation (Roziere et al., 2020; Weisz et al., 2021; 2022). These models, trained from existing
human-readable code, produce idiomatic, easy to understand, translations. Unfortunately, neural
transpilers are unreliable, and often fail to translate the semantics of the input program accurately.
This is a serious limitation, as some of the human work saved by the transpiler has to be reinvested
debugging its output.

We propose to improve the reliability of NMT by leveraging information from compiler toolchains.
When processing source code, compilers create Intermediary Representations (IR): language-agnostic
pseudocode that describes the semantics of the program. Augmenting training data with the corre-
sponding IR can beneﬁt a Neural Transpiler in two ways: it helps align embeddings for different
languages and improves the semantic understanding of the code. As shown in Figure 1, this can
greatly improve the semantic quality of neural translations.

In this work, we leverage LLVM (Lattner and Adve, 2004) to augment source code with corresponding
Intermediate Representation and train models for code translation and decompilation. We compare

∗Equal contribution
1https://en.wikipedia.org/wiki/Source-to-source_compiler

1

 
 
 
 
 
 
Input function

TransCoder

TransCoder-IR

// C++
int nthTerm ( int n ) {

return ( n*n ) + ( n*n*n ) ;

}

// Rust
fn nth_term (n: usize )-> usize {

// Rust
fn nth_term ( n : i32 ) -> i32 {

( n*n ) + ( n*n*n )

}

( n*n ) + ( n*n*n )

}

// Java
static int divisorSum(int n){

int sum = 0 ;
for(int i=1; i<=n; ++i )
sum += ( n / i ) * i;

return sum ;

}

// Go
func divisorSum(n int) (int, int) {

// Go
func divisorSum ( n int ) int {

sum := 0
for i := 1 ; i <= n ; i ++ {

sum += ( n / i ) * i

}
return sum , n
}

sum := 0
for i := 1 ; i <= n ; i ++ {

sum += ( n / i ) * i

}
return sum

}

// Java
static boolean isDivBy9(int n){

// Go
func IsDivBy9 ( n int ) bool {

// Go
func IsDivBy9 ( n int ) bool {

if ( n == 0

n == 9 )

return true;

if ( n < 9 )

return false;
return isDivBy9(

if n == 0

n == 9 {

return true

}
if n < 9 {

return false

(int)(n >> 3) - (int)(n & 7)
}
);
return IsDivBy9 (int(n) > 3 )

- int(n & 7)

}

}

if n == 0

n == 9 {

return true

}
if n < 9 {

return false

}
return IsDivBy9 (

int(n >> 3) - int(n & 7)}

)

}

Figure 1: Improvements over TransCoder. The ﬁrst example shows a translation from C++ to rust, where
TransCoder generates code using unsigned instead of signed integers. In the second example, a translation from
Java to Go, it generates a function with the wrong return type. In the third example, which is also a translation
from Java to Go, the model outputs a function that looks similar to the correct solution but it confuses > with
» and closes an expression with a parenthesis too early. In these cases and many others, TransCoder makes
mistakes that are small in terms of edit distance, but have a large impact on the semantics of the code. Using the
IR to ground the representations to the semantics often helps solving these issues.

it to TransCoder, which uses only code and no IR. We also design an IR-only baseline, dubbed the
pivot method, which generates a translation solely by decompiling an IR generated from the source
language to a different target language. We experiment with four languages: C++ Java, Rust and Go,
and show that utilizing both the code and the IR allows for an average relative improvement of 5.5%.
Moreover, our method only uses the IR at training time and does not require extra computations at
inference time.

Our main contributions are:

• We implement a new IR-augmented translation method, which leverages LLVM IRs to
improve code representations. It allows us to increase the number of correct translations
generated by TransCoder for C++, Java, Go and Rust by 5.5%. Compared to our IR-only
pivot method, the improvement reaches 170%

• Our method is especially useful in the low data regime: with relative improvements reaching

29.7% when translating to Rust and 25.6% when translating from it.

• We extend the parallel evaluation dataset of 852 functions in C++, Java and Python from
Roziere et al. (2020) with 343 more functions in Go and 280 more in Rust, along with
corresponding test cases

• In addition, we achieve 78% accuracy when decompiling LLVM IRs to C++

2 RELATED WORKS

Source-to-Source Translation. Many rule-based methods are available for transpilation, an inven-
tory of which can be found online1. In particular, C2Rust2 and CxGo3, along with manual corrections,
were central for us in translating evaluation tests to Go and Rust (See Section 4.3). Similarly, 2to34,

2https://github.com/immunant/c2rust
3https://github.com/gotranspile/cxgo
4https://docs.python.org/2/library/2to3.html

2

a Python library porting Python 2 code to Python 3, was used in Aggarwal et al. (2015) to create a
parallel dataset and train a machine learning model.

Neural Machine Translation for code is hampered by the lack of parallel data between programming
languages. Indeed, apart from a few language pairs, such as Java-C# (Nguyen et al., 2013; Chen
et al., 2018), and speciﬁc domains (e.g. competitive programming code), it is difﬁcult to collect large
datasets of semantically equivalent code in different languages. TransCoder (Roziere et al., 2020)
bridges this gap by introducing unsupervised machine translation to programming languages. They
take advantage of large monolingual code bases to learn to translate between C++, Python and Java
with high performance. Later, DOBF (Lachaux et al., 2021) improved the model pre-training method
used in TransCoder, and Roziere et al. (2022) used automatically generated unit tests to improve
translation performance between Java, C++ and Python. Recently, large language models trained on
code, such as Codex (Chen et al., 2021) and PALM (Chowdhery et al., 2022), have been used for
unsupervised code translation.

Using the Transcoder model, Weisz et al. (2021) and Weisz et al. (2022) survey the links between
humans and NMT methods for code translation. They view neural translation methods as aids to
programmers. In this context, they demonstrate that even imperfect models can improve the quality of
an engineer’s work for code translation, and plead for the improvement of human-machine interfaces.

Decompilation. Like transpilation, decompilation is usually performed using rule-based methods
that rely on pattern matching to parse the control ﬂow structure of the program. RetDec, an open
source decompiler created by Avast (Kˇroustek et al., 2017), can decompile an executable to C and
a Python-like language via LLVM IR. Other tools exist, such as the Hex-Rays Decompiler5 and
Brumley et al. (2013). A thorough review of rule-based methods can be found in papers such as
Liang et al. (2021a) and Katz et al. (2019). With these methods, decompilation can fail if the code is
too convoluted, or if it contains language features that were not explicitly translated. Most methods
also produce unstructured programs, relying on a large number of goto statements to simulate the
control ﬂow of the lower level programming languages. This is semantically correct, but very rarely
found in human-written code.

A few works have studied the use of sequence-to-sequence neural networks for neural decompilation.
Katz et al. (2019) uses LSTM networks to decompile LLVM IRs and assembly code to C. Their
approach generates code templates based on the IR, that determine the structure of the output. Then,
they ﬁll them with correct variable assignments and numerical values. In the same vein, Fu et al.
(2019) tries to address limitations of neural decompilation with two sequential phases: code sketch
generation and iterative error correction. Finally, Liang et al. (2021b) use a method close to ours, and
train Transformer models to translate between binary code and C.

Intermediate representations are almost as old as compiler design. The ﬁrst IR, UNCOL (Strong
et al., 1958) was introduced in the mid-1950s, together with the idea of reusing the same compiler for
several languages and machines. In 1960, NELIAC (a variant of ALGOL) (Huskey et al., 1960) was
the ﬁrst retargetable compiler, portable to different architectures. Feldman (1979) describes how a
compiler for Fortran 77 can be added to the C compilers of Johnson (1979) and Ritchie (1979). GCC
(Stallman, 2001) introduces Register Transfer Language (RTL) a low-level IR inspired by Davidson
and Fraser (1980), and then GENERIC and GIMPLE (Merrill, 2003), precursors of the IR used in
LLVM (Lattner and Adve, 2004).

3

INTERMEDIATE REPRESENTATIONS IN COMPILERS

Compilers translate programs written in a computer language into executable code for a speciﬁc
machine. Most compilers consist of a front-end taking source code as input, and a back-end which
produces machine binary code. The front-end lexes (tokenizes) and parses the program. Then, it
produces an abstract syntax tree (AST), and translates it into some Intermediate Representation (IR).
The back-end converts the IR into machine-speciﬁc executable code.

In modern compilers such as LLVM (Lattner and Adve, 2004), the IR is generic across different
input languages (and thus different front-ends). It allows the application of transformations and target
agnostic optimizations to the IR, in a middle-end module independent from the source language and

5https://hex-rays.com/decompiler/

3

Figure 2: A bird’s eye view of a compiler toolchain, exempliﬁed with LLVM. The unoptimized
version (-O0) is shown here for illustration. In practice we used the size-optimized version (-Oz) of
the IR as boxed, which does the compile time optimization of computing the addition of 26 and 16.

target machine. This results in an efﬁcient compiler structure: new languages can be implemented by
rewriting the front-end, and new target machines by rewriting the back-end.

Several IRs usually co-exist in a compiler: each stage in the toolchain (Figure 2) introduces a new
representation. Early stage IRs are language-dependent (e.g. ASTs mirror the syntax of the source
language). Late stage IRs replace named variables by registers and reﬂect the speciﬁcs of the target
architecture. In this work, we are interested in middle-end IRs, which are independent from the target
machine, and similar for all source languages (like dialects in natural languages).

4 DATA

4.1 TRAINING DATA

Our training data was extracted with Google BigQuery, which indexes over 2.8 million open source
repositories from GitHub6. We selected projects whose license explicitly permits re-distribution of
parts, and extracted all individual C++, Java, Rust and Go functions. To learn to decompile IRs, we
also used the CodeNet dataset (Puri et al., 2021), a repository of 14 million competitive programming
solutions in 55 languages. Our models work at function level: this reduces compilation failures over
missing dependencies, while keeping sequence lengths short.

Table 1: Dataset coverage across languages, in number of standalone functions.
C++

Rust

Java

Go

Monolingual data
Code / IR Parallel Data
Successful IR Compilation

6.6 M

9.4 M 7.8 M 576.3 K
344.4 K 384.4 K 2.2 M 19.2 K
3.3%

4.1% 28.2%

5.2%

4.2 GENERATING INTERMEDIATE REPRESENTATIONS

While the LLVM ecosystem is large, not every language has an LLVM front-end, and not every
front-end can produce LLVM IR out-of-the-box. We use clang++7 Lattner and Adve (2004) from
the established LLVM C++ compilation toolchain, JLang8 for Java, Gollvm9 for Go and rustc
Matsakis and Klock II (2014) for Rust. For the same program, written in different languages, different
front-ends may produce different IR. To minimize these variations, we process the source code as
follows. First, we generate the most size-optimized IR (-Oz ﬂag), which makes the IR more uniform
across languages. Second, we strip all unnecessary information (e.g. header and footer with attributes,
debug information, comments). Finally, block names are canonicalized and symbol names demangled

6https://console.cloud.google.com/marketplace/details/github/

github-repos

7https://clang.llvm.org/
8https://polyglot-compiler.github.io/JLang/
9https://go.googlesource.com/gollvm/

4

Lexing and ParsingSemantic analysis (AST)LLVM IRgenerationGeneric optimizationsTarget arch. optimization.c.ll.ll.sfoo.c:int main(int argc, char* argv[]){    int a = 26;    return 16 + a;}-O0 version of foo.ll:define i32 @main(i32 %0, i8** %1) #0 {[…]  store i32 26, i32* %6, align 4  %7 = load i32, i32* %6, align 4  %8 = add nsw i32 16, %7  ret i32 %8}-Oz version of foo.s:    pushq   %rbp    .cfi_def_cfa_offset 16    .cfi_offset %rbp, -16    movq    %rsp, %rbp    .cfi_def_cfa_register %rbp    movl    $42, %eax    popq    %rbp    retq-Oz version of foo.ll:define i32 @main(i32 %0, i8** nocapture readnone %1) local_unnamed_addr #0 {  ret i32 42}.o or exeBinary generationlanguage front-endlanguage agnostic middle-endtarget back-endto facilitate their recovery. The functions that fail to compile at this point (e.g. because of missing
dependencies) are not included in the parallel dataset, as seen in the last row of Table 1.

4.3 EVALUATION

Traditional NMT evaluation relies on metrics such as BLEU, that are based on n-gram overlaps.
However, when dealing with programming languages, syntax and in particular compilation and
computation outputs can differ widely despite minor changes in the code. Conversely, semantically
equivalent code, that differ only in variable names or order of operations can have a low BLEU score.
To take this into account, we use and enhance the computational accuracy test suite from Roziere
et al. (2020), that contains 852 parallel competitive programming solutions in C++, Java and Python.
Using C2Rust, CxGo and some manual code cleaning, we translated 280 functions and test suites
in Rust and 343 in Go to measure the performance of our models in these languages. We measure
our performance using the computational accuracy (CA@1) metric (Kulal et al., 2019; Roziere et al.,
2020), which considers that a translation is correct if it passes a series of unit tests.

5 TRAINING OBJECTIVES

Unsupervised machine translation consists of two tasks: training language embeddings (one embed-
ding per language) and aligning them (Lample et al., 2018a). We now present the objective functions
for these tasks. In section 5.1, we review the three basic objectives used by TransCoder, our baseline
NMT system. In section 5.2, we introduce three new functions that leverage LLVM IRs to improve
the multilingual representation of source code, and the performance of our translation models. During
training, we alternate between all six objectives, running each for the same number of optimisation
steps. At inference, the model is only provided with the source code, i.e. the IR is not needed.
. . . z(x)
Formally, let x = x1 . . . xNso be the source sentence, z(x) = z(x)
the corresponding IR, and
Nir
y = y1 . . . yNta the target sentence. We write LCE(ˆy, y) = (cid:80)
i (cid:96)CE( ˆyi, yi), with (cid:96)CE( ˆyi, yi) the
pairwise cross-entropy loss between ˆyi and yi. We deﬁne the machine translation loss (or seq2seq
loss) from x to y, LM T as the sum of the negative log-likelihood of each token yi, given x and
previous tokens y0 . . . yi−1 (note that x and y can have different lengths) :

1

LM T (x, y) = −

(cid:88)

i

log (P (yi|x, y1 . . . yi−1))

5.1 COMMON OBJECTIVE FUNCTIONS

TransCoder (Roziere et al., 2020) learns to translate between programming languages by leveraging
three unsupervised objectives developed for natural language (Lample et al., 2018b):

Masked Language Modeling (MLM) trains an encoder to predict randomly masked inputs. It
is commonly used to pre-train embeddings for natural (Devlin et al., 2018; Liu et al., 2019) and
programming languages (Kanade et al., 2020; Feng et al., 2020). MLM allows the model to learn
the syntax and semantics of programs. Alternative objectives, have been proposed for programming
languages (Guo et al., 2020; Lachaux et al., 2021; Ahmad et al., 2021; Wang et al., 2021). We do not
use them here, as MLM remains effective and easy to use on a wide range of programming languages.

Denoting mask(x) the masked version of the code sentence x, and enc(t) the encoder output, MLM
uses the following loss:

LM LM = LCE (enc(mask(x)), x) .

(1)

Denoising Auto Encoding (AE) trains a sequence to sequence (seq2seq) model to retrieve an original
sequence from a corrupted version. Corruption is done by masking spans of tokens randomly sampled
from a Poisson distribution, as well as removing and shufﬂing tokens. It uses the following loss
(noise(x) denotes the corrupted version of x):

LAE = LM T (noise(x), x) .

(2)

Back-Translation (BT). Back-Translation (Sennrich et al., 2015) uses the model to generate a noisy
translation of the input sentence, and then trains the model to recover the original input from the

5

Figure 3: IR for code representation objectives. We show examples of masking (used in TLM and TAE) and
IR generation used to improve code representations with IRs. The masking objective in TLM or TAE makes the
model understand the relationship between code and IR. The IR generation objective helps the model to build
semantic representations of the code. For instance, another C++ function computing 39 + 3 would result in
the same IR. A Go function that returns 42 would also have a similar LLVM IR. Therefore, the IR Generation
objective encourages the model to build similar representations for these three semantically equivalent functions.

translation. It is a simple yet powerful objective for unsupervised machine translation (Lample et al.,
2018a; Artetxe et al., 2018). In practice, it is a required loss to get competitive performance, so it is a
staple of all our experiments. Formally, we use the model to translate sequence x into ˆy and train the
model to reverse the translation process, using the loss:

LBT = LM T (ˆy, x)

(3)

5.2

IR FOR CODE REPRESENTATIONS

Intermediate representations (IR) provide additional information about the code to be translated. We
add them to the training dataset, as described in section 4.2, and leverage them by adding three new
objective functions to those described in section 5.1.

Translation Language Modeling (TLM), ﬁrst introduced in Lample and Conneau (2019), strives
at generating common representations for parallel sentences in different languages. Like the masked
language modeling (MLM) objective, it trains an encoder to predict random masked inputs. However,
TLM is trained on pairs of parallel sentences, concatenated together and separated by a special token.
Here, we concatenate functions in their source language and their corresponding IR, using the source
code and IR language embeddings, and train the encoder to predict randomly masked tokens. This
allows the model to learn correspondences between the source and the IR. The corresponding loss is
(⊕ denotes concatenation):

LT LM = LCE

(cid:16)

mask(x ⊕ z(x)), x ⊕ z(x)(cid:17)

(4)

Translation Auto-Encoding (TAE)
amounts to transposing the TLM objective into a denoising
auto-encoder. The source code and corresponding IR are corrupted and masked, and then concatenated
into one sequence (using the language embeddings for code and IR, as previously). TAE is then
tasked to recover the original, using the following loss:

LT AE = LM T

(cid:16)

noise(x) ⊕ noise(z(x)), x ⊕ z(x)(cid:17)

(5)

IR Generation (MT)
allows the encoder to learn source code representations from the semantics of the IR. The loss is:

trains the model to translate the source code into the corresponding IR. This

6

Figure 4: IR Decompilation objective. Here, we generate the IR corresponding to each function and train a
model to decompile it. The IR pivot model uses this objective, as well as back-translation objectives, allowing it
generalize to IRs generated from any language.

LIRGen = LM T

(cid:16)

x, z(x)(cid:17)

(6)

These three objectives need both the source code and the corresponding IR. However, only a fraction
of the functions and ﬁles in our dataset could be compiled. To mitigate this, we also train the models
on the full monolingual data using the MLM and AE objectives described above. In this setup, the
back-translation (BT) objective is the same as in Roziere et al. (2020), and allows our model to
translate directly from source code only at inference time.

5.3 ADDITIONAL LOSSES: IR DECOMPILATION AND PIVOT

We study two alternative uses of intermediary representations: IR decompilation, and IR pivot
translation. IR decompilation consists in recovering source code corresponding to a given IR. In
practice, it reverses the computations performed by the compiler. IR Pivot is a translation method built
upon IR decompilation. Since LLVM can compile many languages (C++, Java, Rust, Go) into the
same IR, an obvious approach to code translation consists in decompiling the IR generated from the
source language into code in the target language. We call this method “IR pivot”. Note that, whereas
the IR for code representation techniques only used IR during training, both the decompilation and
pivot method also need the IR for inference.

Decompilation.
In this supervised task, we use LLVM to generate IR from source code, and train
a language model to reverse the process, i.e. learn to predict the source code from the IR. Models
are pre-trained using the MLM and AE objectives, and decompilation is learned using the machine
translation loss:

LDecomp = LM T

(cid:16)

(cid:17)

z(x), x

(7)

IR Pivot. This task leverages the IR as a pivot for code translation. For instance, to translate from
Rust to C++, we ﬁrst use LLVM to compile a Rust program into IR and then decompile the IR to C++
using a neural decompiler. In practice, slight variations exists between the IR generated for different
languages: the Rust-IR and C++-IR behave like dialects of the LLVM-IR. This often leads to poor
performance of the IR Pivot method. We mitigate these issues using a variety of techniques, which
we describe in section C of the appendix.

6 RESULTS

6.1 EXPERIMENTAL DETAILS

For TransCoder, we consider a sequence-to-sequence (seq2seq) transformer model (Vaswani et al.,
2017) with attention (Bahdanau et al., 2015; Sutskever et al., 2014) and the same architecture as
Roziere et al. (2020). Our model has 12 layers (6 in the encoder and 6 in the decoder), 8 attention
heads, and a dimension of 1024. For the objectives that add noise and masks to the input sentence,
such as MLM, TLM, AE, and TAE, we choose the masked tokens and noise randomly on the ﬂy at
each epoch. We mask 15% of the tokens in MLM and TLM. In AE and TAE, we mask 20% of the

7

from C++ to C++ from Go to Go from Java to Java from Rust

to Rust AVG

Greedy decoding
IR Pivot
TransCoder (baseline)
TLM
MLM + TAE
TLM + TAE
MLM + MT
TLM + MT
TLM + TAE + MT

Beam size 5
TransCoder (baseline)
TLM + TAE + MT

17.4
46.4
47.5
47.3
46.9
45.5
45.6
47.8

53.8
52.9

24.0
52.1
54.8
53.3
55.9
51.0
51.5
54.3

19.9
42.1
45.4
47.2
45.0
44.0
45.1
46.6

11.5
45.6
41.2
44.8
37.9
48.9
47.1
51.6

11.9
41.2
39.8
41.8
38.5
46.6
46.9
47.1

22.2
44.5
52.1
45.9
54.5
45.2
45.5
49.6

53.4
53.5

45.2
48.8

54.4
57.1

46.1
51.5

51.5
53.4

16.3
29.6
31.1
25.1
34.9
25.7
24.4
35.3

35.9
37.9

7.8
16.4
17.0
39.8
15.7
40.9
17.4
40.4
16.8
41.3
16.6
40.5
40.5
17.9
21.4 44.2

20.9
45.3
27.1 47.8

Table 2: Translation performance (CA@1), for greedy decoding and beam size 5. “To X”: average
performance when translating to language X. “From X”: average performance when translating from language
X. See Table 3 in the appendix for more detailed results. All combinations of the TLM, MT and TAE objectives
improve the performance compared to TransCoder. The best results are obtained when all three are used at the
same time. Beam search results in better performance. The IR Pivot method generates a translation in the target
language from an IR generated from the source, and performs poorly in our setting.

tokens. MLM is trained on streams of data, while the other objectives are trained at function level. We
use the Adam optimizer (Kingma and Ba, 2015) and an inverse squared-root learning rate scheduler,
with an initial learning rate of 10−5 in most of our experiments. Our models are implemented in
PyTorch using mixed-precision ﬂoats. The pre-trained models were trained until convergence. The
translation models presented in Tables 2 and 3 were trained for a week on 32 NVIDIA V100 GPUs.

6.2

IR-AUGMENTED CODE REPRESENTATIONS FOR TRANSLATION

Models using combinations of the three objectives—TAE, TLM and MT—introduced to leverage
IR, were trained to translate between pairs of four languages (C++, Java, Rust, Go). Their average
performance when translating to and from every language are presented in table 2 (additional details
can be found in Table 3) in the appendix. As a baseline, we use a TransCoder (Roziere et al., 2020)
model, trained with MLM on the same dataset.

Using greedy decoding, the new TLM, TAE and MT objectives, which leverage the IR, improve
performance for every language. The best average results are obtained when combining all of them.
Compared to TransCoder, they improve performance by an average 4.4% point (11% relative). The
largest impacts are observed in the low data regime: translations from and into Rust (a language less
represented in our training set) are improved by 25.6% and 19.3% (relative). Beam search improves
the results of both TransCoder and our models, using IR-augmented representation still results in
better performance. Qualitatively, we observe that IRs help our model translate types when the source
and target types are represented by different tokens. For instance, in the ﬁrst example of Table 1, it
translates the semantics of int correctly using i32 instead of an unsigned integer type (usize).

Compared to IR-augmented translation models, the “obvious” IR Pivot method proves disappointing,
even though it achieves non trivial performances. It is heavily dependent on the size of the training set:
the IR pivot performs relatively well when translating from low-resource to high-resource languages
(e.g. from Rust), and badly when translating to low-resource languages (e.g. to Rust).

6.3 DECOMPILATION RESULTS

To compute the IR pivot, we trained a neural decompiler to retrieve source code from IRs. We tried
two separate conﬁgurations for decompilation: a shared decoder with 6 layers for all language / IR
pairs, or four separate decoders of with two layers each (one per language). Using a shared decoder
improves the performance for all languages, and particularly when the data is scarce (e.g. Rust). See
Table 5 in the appendix for more information.

8

We compare the performance of our model to RetDec (Kˇroustek et al., 2017), a rule-based decompiler.
It obtains a computational accuracy of 68.75 on our C++ dataset and a BLEU score of 8.54. In
comparison, our model obtains a computational accuracy of 77.9 and a BLEU score of 63.6 in the
same setting. In particular, RetDec fails to decompile LLVM ﬁles generated from C++ code, especially
snippets leveraging the standard library structures such as unordered_map or std :: allocator. The
limitations of RetDec, which was implemented by a team of 24 developers in 7 years 10, shows how
difﬁcult it is to build exhaustive rule-based decompilers, especially when the IR comes from different
languages or tools.

7 DISCUSSION

Different IR and interpreted languages The four languages considered in this work have front-
ends that can output LLVM Intermediary Representation. LLVM presently covers more than 30
computer languages. Using IR as pivot requires that the source and destination language have
front-ends that use the same IR. This rules out some widely-used languages (e.g. Python). Using the
IR to improve embeddings is less restrictive: the source and destination language can be trained on
different IR, and aligned with back-translation. In this paper, we focus on compiled languages, but it
is important to note that Intermediary Representations are usually available for interpreted languages
as well: modern interpreters translate the source code into byte-code, that can serve as an IR.

Pivot vs Embedding TransCoder is an unsupervised model that learns to align code representations
and translate code from one language to another. It is based solely on source code and does not
use IRs. The pivot method uses automatically generated parallel sentences to learn to decompile
IRs, and back-translation to adapt to different IR dialects. This method learns to translate using only
IR-level similarities, and does not use the source code itself except to compute the IR. Although it
underperforms other methods, it performs relatively well when little data is available for the source
language, because the IR can be computed using a rule-based compiler. However, it requires to
compute IRs at test time, which can be cumbersome. Instead, adding the TLM, TAE, and MT
objectives to the objectives generally used for unsupervised code translation allows the model to get
the best of both worlds. It can learn multilingual representations of source code from similarities in
the IR and in the source code itself. As shown in Table 2, it outperforms both TransCoder and the
pivot method. At the same time, this model does not require to compute IRs at test time, and is as
easy to use as TransCoder.

Using our model at inference time. Our self-supervised IR-augmented TLM, TAE and MT ob-
jectives are designed to improve the multilingual code representations used in translation models.
However, the translation task does not require to compute these objectives. Therefore, they lead to
models that are just as simple to use as TransCoder: computing the IR is not required at test time and
the model generates the translation directly from the source function.

8 CONCLUSION

In this paper, we leverage LLVM IRs to improve neural machine translation for source code. The
IR provides a common semantically-rich language, into which C++, Go, Java and Rust code can
all be compiled. We develop three objectives, designed to leverage IRs for better multilingual
representations of source code, which lead to a 5.5% relative average improvement for code translation.
We also show that sequence-to-sequence transformers perform well for neural decompilation, and
use this for pivot translation.

We only worked with the LLVM IR, but our approach is broadly applicable to any pair of languages
that share a common Intermediate Representation. More generally any IR can help improve the code
representations by tying them to the semantics. Another limitation is the scale of our current source
and target sequences. As future work, LLVM IRs could be generated at a larger scale by compiling
entire projects, which would greatly improve the percentage of successful IR compilations in Table 1.
More languages and IRs could be used, and those extensions could be powered by larger models.

10https://blog.fpmurphy.com/2017/12/avast-retargetable-decompiler-ida-plugin.

html

9

REFERENCES

Karan Aggarwal, Mohammad Salameh, and Abram Hindle. Using machine translation for converting

Python 2 to Python 3 code. Technical report, PeerJ PrePrints, 2015.

Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Uniﬁed pre-training for pro-
gram understanding and generation. In Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, pages
2655–2668, 2021.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. Unsupervised statistical machine translation.
In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,
2018.

Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In 3rd International Conference on Learning Representations, 2015.

David Brumley, JongHyup Lee, Edward J Schwartz, and Maverick Woo. Native x86 decompilation
using semantics-preserving structural analysis and iterative control-ﬂow structuring. In 22nd
USENIX Security Symposium (USENIX Security 13), pages 353–368, 2013.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards,
Yura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on
code. arXiv preprint arXiv:2107.03374, 2021.

Xinyun Chen, Chang Liu, and Dawn Song. Tree-to-tree neural networks for program translation. In

Advances in neural information processing systems, pages 2547–2557, 2018.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

Jack W. Davidson and Christopher W. Fraser. The design and application of a retargetable peephole

optimizer. ACM Trans. Program. Lang. Syst., 2(2):191–202, apr 1980. ISSN 0164-0925.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep

bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.

Stuart I. Feldman. Implementation of a portable Fortran 77 compiler using modern tools. SIGPLAN
Not., 14(8):98–106, aug 1979. ISSN 0362-1340. doi: 10.1145/872732.806959. URL https:
//doi.org/10.1145/872732.806959.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing
Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural
languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing: Findings, pages 1536–1547, 2020.

Cheng Fu, Huili Chen, Haolan Liu, Xinyun Chen, Yuandong Tian, Farinaz Koushanfar, and Jishen
Zhao. Coda: An end-to-end neural program decompiler. In Advances in Neural Information
Processing Systems, pages 3703–3714, 2019.

Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, LIU Shujie, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, et al. Graphcodebert: Pre-training code representations with
data ﬂow. In International Conference on Learning Representations, 2020.

Harry D. Huskey, M. H. Halstead, and R. McArthur. NELIAC — dialect of ALGOL. Commun.
ACM, 3(8):463–468, aug 1960. ISSN 0001-0782. doi: 10.1145/367368.367373. URL https:
//doi.org/10.1145/367368.367373.

S. C. Johnson. A tour through the portable C compiler. In Unix Programmer’s Manual, 7th Edition,

2B, Section 33, 1979.

Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating
contextual embedding of source code. In International Conference on Machine Learning, pages
5110–5121. PMLR, 2020.

10

Omer Katz, Yuval Olshaker, Yoav Goldberg, and Eran Yahav. Towards neural decompilation. arXiv

preprint arXiv:1905.08325, 2019.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.

Jakub Kˇroustek, Peter Matula, and P Zemek. RetDec: An open-source machine-code decompiler,

2017. URL https://github.com/avast/retdec.

Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy S
Liang. Spoc: Search-based pseudocode to code. Advances in Neural Information Processing
Systems, 32:11906–11917, 2019.

Marie-Anne Lachaux, Baptiste Roziere, Marc Szafraniec, and Guillaume Lample. DOBF: A deob-
fuscation pre-training objective for programming languages. arXiv preprint arXiv:2102.07492,
2021.

Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. Advances in

Neural Information Processing Systems, 32:7059–7069, 2019.

Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. Unsupervised

machine translation using monolingual corpora only. ICLR, 2018a.

Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. Phrase-

based & neural unsupervised machine translation. In EMNLP, 2018b.

Chris Lattner and Vikram Adve. Llvm: A compilation framework for lifelong program analysis &
transformation. In International Symposium on Code Generation and Optimization, 2004. CGO
2004., pages 75–86. IEEE, 2004.

Ruigang Liang, Ying Cao, Peiwei Hu, and Kai Chen. Neutron: an attention-based neural decompiler.

Cybersecurity, 4(1):1–13, 2021a.

Ruigang Liang, Ying Cao, Peiwei Hu, Jinwen He, and Kai Chen. Semantics-recovering decompilation

through neural machine translation. arXiv preprint arXiv:2112.15491, 2021b.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.

Nicholas D Matsakis and Felix S Klock II. The Rust language. In ACM SIGAda Ada Letters,

volume 34, pages 103–104. ACM, 2014.

Jason Merrill. GENERIC and GIMPLE: A new tree representation for entire functions. In in Proc.

GCC Developers Summit, 2003, pages 171–180, 2003.

Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N Nguyen. Lexical statistical machine translation
for language migration. In Proceedings of the 2013 9th Joint Meeting on Foundations of Software
Engineering, pages 651–654, 2013.

Ruchir Puri, David S. Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov,
Julian Dolby, Jie Chen, Mihir R. Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti,
Saurabh Pujar, and Ulrich Finkler. Project CodeNet: A large-scale AI for code dataset for learning
a diversity of coding tasks. CoRR, abs/2105.12655, 2021. URL https://arxiv.org/abs/
2105.12655.

D.M. Ritchie. A Tour Through the UNIX C Compiler. 1979. URL https://books.google.

com/books?id=5UvEGwAACAAJ.

Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. Unsupervised
translation of programming languages. Advances in Neural Information Processing Systems, 33,
2020.

Baptiste Roziere, Jie M Zhang, Francois Charton, Mark Harman, Gabriel Synnaeve, and Guillaume

Lample. Leveraging automated unit tests for unsupervised code translation. ICLR, 2022.

11

Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation mod-
els with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics, pages 86–96, 2015.

Richard Stallman. Using and porting the GNU Compiler Collection. M.I.T. Artiﬁcial Intelligence

Laboratory, 2001.

J. Strong, J. Wegstein, A. Tritter, J. Olsztyn, O. Mock, and T. Steel. The problem of programming
communication with changing machines: A proposed solution. Commun. ACM, 1(8):12–18, aug
1958. ISSN 0001-0782. doi: 10.1145/368892.368915. URL https://doi.org/10.1145/
368892.368915.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.

In Advances in neural information processing systems, pages 3104–3112, 2014.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998–6008, 2017.

Yue Wang, Weishi Wang, Shaﬁq Joty, and Steven CH Hoi. Codet5: Identiﬁer-aware uniﬁed pre-
trained encoder-decoder models for code understanding and generation. In Proceedings of the 2021
Conference on Empirical Methods in Natural Language Processing, pages 8696–8708, 2021.

Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross, Fernando Martinez,
Mayank Agarwal, and Kartik Talamadupula. Perfection not required? human-ai partnerships in
code translation. In 26th International Conference on Intelligent User Interfaces, pages 402–412,
2021.

Justin D Weisz, Michael Muller, Steven I Ross, Fernando Martinez, Stephanie Houde, Mayank
Agarwal, Kartik Talamadupula, and John T Richards. Better together? an evaluation of AI-
supported code translation. In 27th International Conference on Intelligent User Interfaces, pages
369–391, 2022.

12

A FULL SCORES TABLE

Table 3: Results on unsupervised code translation. The metric shown is the computational accuracy for
a single generation (CA@1), measuring the translation correctness using unit tests. It is the full version of
Table 2. The models were all trained with the same budget. Although it is not the case for every language
pair, TransCoder-IR, which uses the TLM, TAE, and MT objectives outperforms other methods on average.
TransCoder-ST (Roziere et al., 2022) uses a parallel dataset generated with automated unit tests and outperforms
other methods for C++ ↔ Java. Their method is orthogonal to ours, and we could also improve our performance
with similar methods.

C++ → Go C++ → Java C++ → Rust Go → C++ Go → Java Go → Rust

Baseline MLM
TransCoder-ST
Pivot
TLM
MLM + TAE
TLM + TAE
MLM + MT
TLM + MT
TLM + TAE + MT

Baseline (MLM)
TransCoder-ST
Pivot
TLM
MLM + TAE
TLM + TAE
MLM + MT
TLM + MT
TLM + TAE + MT

57.7
-
16.1
61.8
57.7
58.2
56.8
58.6
55.9

63.3
68.0
22.0
62.5
62.5
63.3
60.6
58.5
62.9

18.2
-
14.0
18.2
21.7
19.2
19.2
19.7
24.8

56.1
-
30.5
57.6
63.0
55.2
60.3
57.3
61.8

46.9
-
26.5
56.4
54.7
57.0
53.8
54.1
55.7

23.3
-
2.7
22.2
23.8
22.8
18.0
23.8
22.2

Java → C++ Java → Go

Java → Rust Rust → C++ Rust → Go Rust → Java

77.9
84.6
19.5
80.9
80.3
82.2
76.2
77.9
74.5

35.9
-
9.4
31.8
38.6
24.6
50.9
52.7
49.6

9.6
-
6.7
6.6
6.6
8.6
12.6
10.1
17.2

22.4
-
22.0
25.9
16.6
30.4
16.6
19.2
26.5

43.2
-
8.9
30.0
38.1
31.0
39.1
30.0
49.2

23.4
-
18.1
37.5
20.6
43.3
21.3
24.1
30.2

13

B BEAM SIZE EVALUATION

Table 4: Results on unsupervised code translation with different beam sizes. The metric shown is still the
computational accuracy for a single generation (CA@1). Using beam search improves the average performance
of every model. BS N means that the model is evaluated with beam size N. When the beam size is not given,
we use greedy decoding. Surprisingly, beam size 5 outperforms beam size 10. Our method using intermediate
representations still outperforms the baseline with beam size 5 and 10 in average. With the baseline, we obtain
average CA@1 scores of 45.3 with beam size 5 and 44.0 with beam size 10. Our method yields CA@1 scores
of 47.8 with beam size 5 and 46.8 with beam size 10.

C++ → Go C++ → Java C++ → Rust Go → C++ Go → Java Go → Rust

Baseline MLM
footnotesizeBaseline MLM (BS 5)
Baseline MLM (BS 10)
TLM + TAE + MT
TLM + TAE + MT (BS 5)
TLM + TAE + MT (BS 10)

57.7
65.5
65
55.9
61.4
61.4

63.3
67.6
68.7
62.9
66.6
67.4

18.2
28.3
27.8
24.8
30.8
29.3

56.1
52.1
52.1
61.8
57.3
56.4

46.9
59.3
57.0
55.7
59.0
59.0

23.3
24.3
23.8
22.2
30.2
29.1

Baseline (MLM)
Baseline MLM (BS 5)
Baseline MLM (BS 10)
TLM + TAE + MT
TLM + TAE + MT (BS 5)
TLM + TAE + MT (BS 10)

Java → C++ Java → Go

Java → Rust Rust → C++ Rust → Go Rust → Java

77.9
82.9
80.9
74.5
76.4
77.7

35.9
45.5
46.4
49.6
57.7
57.3

9.6
10.1
7.6
17.2
20.2
18.2

22.4
25.2
23.6
26.5
26.8
26.2

43.2
54.8
51.8
49.2
52.3
51.8

23.4
27.5
23.0
30.2
34.7
28.2

C PIVOT METHOD DETAILS

As mentioned in Section 5.3, IR generated from different languages contain slight variations and
can be seen as dialects of the same language. In practice, these variations prevent us from simply
using our best decompilation model to generate source code in another language than the one used
to generate the IR. Although we prompt the model to generate code in the target language with
language embeddings, it learns to focus on the particularities of each dialect and ignores the language
embeddings. Therefore, it generates code in the source language, which results in a computational
accuracy score of 0 for translation.

One way to solve this issue is to use one decoder per target language. Then, the model is able to
generate code in the target language. However, this method still performs poorly due to the small
differences between the IR dialects. The method we tested that performed the best, and which is
reported in Table 2, uses back-translation to make the model to translate from any IR dialect to any
language. This model is also grounded by supervised translation steps making it generate IR from
code and code from IR. In practice, we create new language embeddings for every IR dialect (i.e.
IR-C++, IR-Go, IR-Java, IR-Rust) for depending on the source language. At training time, we make
the model generate noisy translations in the IR-Go, IR-Java and IR-Rust “languages” for every C++
sequence, and train it to re-generate the C++ sequence from the noisy translation. To allow the model
to generate good training data for IR-X → C++, we also generate noisy translations in Go, Java, and
Rust for every IR generated from C++ in our dataset and train the model to retrieve the IR. Using our
parallel code//IR dataset, we also train the model to translate between C++ and IR-C++ sequences.
We do the same for every language and alternate between them.

14

D IR DECOMPILATION

Table 5: Performance of LLVM IRs Decompilation. This table shows the computational accuracy (CA@1)
of our neural decompiler and the RetDec C++ rule-based decompiler. Our neural decompiler outperforms
RedDec on C++ and is more broadly applicable.

C++

Go

Java

Rust

Baseline - RetDec
Separate Decoders
Shared Decoder

68.8 —
42.2
52.7
70.1
77.9

—
60.1
82.2

—
19.5
61.0

E TRANSLATION EXAMPLES

Input function (C++)

LLVM IR

Decompilation code (C++)

int max(int a, int b){

for (int i = 0; i < 10; ++i)

a = 2 * a - a;

c = b
return a - b + c > b ? a : b;

}

define i32 @"max"(i32 %a, i32 %b)
local_unnamed_addr #0 {
bb1:

%cmp = icmp sgt i32 %a, %b
%cond = select i1 %cmp, i32 %a, i32 %b
ret i32 %cond

}

bool is_even(int n){
return n & 1 == 0;

}

long multiplyBySeven(long n){

return ((n << 3) - n);

}

define dso_local zeroext
i1 @"is_even"(i32 %0)
local_unnamed_addr #0 {

ret i1 false

}

define dso_local
i64 @"multiplyBySeven"(i64 %0)
local_unnamed_addr #0 {

%2 = mul i64 %0, 7
ret i64 %2

}

int max(int a, int b){

return a > b ? a : b;

}

bool is_even(int n){

return false

}

long multiplyBySeven(long n){

return n * 7;

}

Figure 5: Code simpliﬁcation examples with Decompilation / Pivot. Since the LLVM IR is optimized,
functions that are semantically equivalent after optimization map to the same IR. In the ﬁrst example, it allows to
remove useless code by decompiling the generated LLVM IR. In the second example, the simpliﬁcation allows
to ﬁnd a bug: the & operator has precedence over == in C++, causing this function to always evaluate to false.
It is not obvious when looking at the input code, but becomes clear with the IR and simpliﬁed C++ code. In the
third example, it replaces a bitwise operation by a more straightforward multiplication. In all examples, we can
run the compiler again to check that the IR of the decompiled code is exactly the same as that of the input. It
guarantees that the input and simpliﬁed code have the same semantics.

15

Input function

TransCoder

TransCoder-IR

// Java
static int addOne ( int x ) {

return ( - ( ~ x ) ) ;

}

// Rust
fn add_one (x : int ) -> int {

(- ( ~ x) as int)

}

// Rust
fn add_one (x : i32 ) -> i32 {

(- ( ! x))

}

// Java
static boolean isEven (int n){

return ( n % 2 == 0 );

}

// Rust
fn is_even ( n : u32 ) -> bool {

// Rust
fn is_even ( n : i32 ) -> bool {

( n % 2 == 0 ) as bool

( n % 2 == 0 )

}

}

Figure 6: Java to Rust translation examples. In the ﬁrst example, the IR allows the model to understand that
the Java bitwise complement operator ~ should be replaced by ! in Rust. Also, it allows the model to translate
the type correctly in both examples and avoids unnecessary casts. The IR allows the model to generate the right
types (e.g. i32 instead of u32 when translating int) and operator (e.g. ! instead of ~ in Rust).

Input function

TransCoder-IR

// Rust
pub fn binary_search(nums: Vec<i32>,

// Go
func binarySearch(nums[] int, target int) int {

target: i32,

) -> i32 {

if nums.is_empty() {

return 0

}
if target < nums[0] {

return 0

}
let mut left = 0;
let mut right = nums.len() - 1;
while left <= right {

let mid = left + (right - left) / 2;
if nums[mid] == target {

return mid as i32;

if len(nums) == 0 {

return 0

}
if target < nums[0] {

return 0

}
left: = 0
right: = len(nums) - 1
for left <= right {

mid: = left + (right - left) / 2
if nums[mid] == target {

return mid

} else if nums[mid] < target {

left = mid + 1

} else if nums[mid] < target {

} else {

left = mid + 1;

} else {

right = mid - 1;

}

}
return left as i32;

}

right = mid - 1

}

}
return left

}

Figure 7: Rust to Go translation example. This function performs binary search to ﬁnd the insertion index
for an element in an ordered vector. The model translates types, function deﬁnitions, variable deﬁnitions, and
while loops correctly.

16

Input function

TransCoder-IR

// Go
func pairsInSortedRotated(arr[] int,

// C++
int pairsInSortedRotated(int arr[],

n int,
x int)

int {

var i int
for i = 0; i < n - 1; i++{
if arr[i] > arr[i + 1] {

break

}

}
var l int = (i + 1) % n
var r int = i
var cnt int = 0
for l != r {

if arr[l] + arr[r] == x {

cnt++
if l == (r - 1 + n) % n {

return cnt

}
l = (l + 1) % n
r = (r - 1 + n) % n

} else if arr[l] + arr[r] < x {

l = (l + 1) % n

} else {

r = (n + r - 1) % n

}

}
return cnt

}

int n,
int x)

{

int i;
for (i = 0; i < n - 1; i ++) {

if (arr [i] > arr [i + 1]) break;

}
int l = (i + 1) % n;
int r = i;
int cnt = 0;
while (l != r) {

if (arr [l] + arr [r] == x) {

cnt ++;
if (l == (r - 1 + n) % n) return cnt;
l = (l + 1) % n;
r = (r - 1 + n) % n;

}
else if (arr[l] + arr[r] < x) l = (l + 1) % n;
else r = (n + r - 1) % n;

}
return cnt;

Figure 8: Go to C++ translation example. This function computes the number of pairs of elements that sum
to a given target in a sorted and rotated array. Our TransCoder-IR model translates it correctly.

17

Input function

TransCoder-IR

// Rust
pub fn get_max_repetitions(s1: String,

// Go
func getMaxRepetitions(s1 string,

n1: i32,
s2: String,
n2: i32,

n1 int,
s2 string,
n2 int,

) -> i32 {

if n1 == 0 {

return 0;

) int {

if n1 == 0 {
return 0

};
let mut indices = vec![0;n1 as usize + 1];
let mut counts = vec![0;n1 as usize + 1];
let mut index = 0;
let mut count = 0;
for i in 1.. = n1 as usize {

for j in 0..s1.len() {

if (

};
indices := make([] int, n1 + 1)
counts := make([] int, n1 + 1)
index := 0
count := 0
for i := 1;
i <= n1;
i++{

s1.as_bytes()[j]
== s2.as_bytes()[index]

) {

index += 1;

}
if index == s2.len() {

index = 0;
count += 1;

}

}
counts[i] = count;
indices[i] = index;
for k in 0..i {

if indices[k] == index {

let pre_count = counts[k];
let pattern_count = (n1 - k as i32)

/ (i - k) as i32
* (counts[i] - pre_count);

let remain_count = counts

[

k
+ (n1 as usize - k)
% (i - k)

] - pre_count;

return (

pre_count
+ pattern_count
+ remain_count

) / n2;

for j: = 0;
j < len(s1);
j++{

if s1[j] == s2[index] {

index++

}
if index == len(s2) {

index = 0
count++

}

}
counts[i] = count
indices[i] = index
for k := 0;
k < i;
k++{

if indices[k] == index {

preCount := counts[k]
patternCount := int(n1 - k)

/ (i - k)
* (counts[i] - preCount)

remainCount := counts[

k + (n1 - k) % (i - k)

] - preCount
return (

preCount
+ patternCount
+ remainCount

) / n2

}

}

}
counts[n1 as usize] / n2

}

}

}

}
return counts[n1] / n2

}

Figure 9: Rust to Go translation example. We call S1 the string n1 repeated s1 times and S2 the string n2
repeated s2 times. This function ﬁnds the largest number of repetitions of S2 appearing in any subset of S1.
The model translates the types correctly, understands that casting vector indices to unsigned int (i.e. with as
usize) is not required in Go, and correctly translates other Rust constructs to Go.

18

