Coordinate Methods for Matrix Games

Yair Carmon

Yujia Jin

Aaron Sidford

Kevin Tian

{yairc,yujiajin,sidford,kjtian}@stanford.edu

Abstract

We develop primal-dual coordinate methods for solving bilinear saddle-point problems of the
form minx∈X maxy∈Y y(cid:62)Ax which contain linear programming, classiﬁcation, and regression as
special cases. Our methods push existing fully stochastic sublinear methods and variance-
reduced methods towards their limits in terms of per-iteration complexity and sample complex-
ity. We obtain nearly-constant per-iteration complexity by designing eﬃcient data structures
leveraging Taylor approximations to the exponential and a binomial heap. We improve sam-
ple complexity via low-variance gradient estimators using dynamic sampling distributions that
depend on both the iterates and the magnitude of the matrix entries.

Our runtime bounds improve upon those of existing primal-dual methods by a factor depend-
ing on sparsity measures of the m by n matrix A. For example, when rows and columns have
constant (cid:96)1/(cid:96)2 norm ratios, we oﬀer improvements by a factor of m + n in the fully stochastic
m + n in the variance-reduced setting. We apply our methods to computational
setting and
geometry problems, i.e. minimum enclosing ball, maximum inscribed ball, and linear regression,
and obtain improved complexity bounds. For linear regression with an elementwise nonnegative
matrix, our guarantees improve on exact gradient methods by a factor of (cid:112)nnz(A)/(m + n).

√

0
2
0
2

p
e
S
7
1

]
S
D
.
s
c
[

1
v
7
4
4
8
0
.
9
0
0
2
:
v
i
X
r
a

 
 
 
 
 
 
Contents

1 Introduction

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1 Our results
1.2 Our approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4 Paper organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Preliminaries

2.1 Local norm setups
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 The problem and optimality criterion . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Matrix access models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Data structure interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Framework

3.1 Sublinear coordinate methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Variance-reduced coordinate methods . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Matrix games

4.1
4.2

(cid:96)1-(cid:96)1 sublinear coordinate method . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
(cid:96)1-(cid:96)1 variance-reduced coordinate method . . . . . . . . . . . . . . . . . . . . . . . .

5 Data structure implementation

5.1 IterateMaintainerp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 ApproxExpMaintainer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 ScaleMaintainer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Applications

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.1 Maximum inscribed ball
6.2 Minimum enclosing ball
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.3 Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A Deferred proofs from Section 2

B Deferred proofs from Section 3

C Deferred proofs for sublinear methods

D Deferred proofs for variance-reduced methods

E Additional results on variance-reduced methods

F Deferred proofs from Section 6

G IterateMaintainer2: numerical stability and variations

1
1
4
11
12

13
13
14
15
15

17
17
19

22
22
24

30
30
33
38

44
45
47
48

54

54

60

67

76

80

84

1

Introduction

Bilinear minimax problems of the form

min
x∈X

max
y∈Y

y(cid:62)Ax where A ∈ Rm×n,

(1)

are fundamental to machine learning, economics and theoretical computer science [26, 46, 13]. We
focus on three important settings characterized by diﬀerent domain geometries. When X and Y are
probability simplices—which we call the (cid:96)1-(cid:96)1 setting—the problem (1) corresponds to a zero-sum
matrix game and also to a linear program in canonical feasibility form. The (cid:96)2-(cid:96)1 setting, where X
is a Euclidean ball and Y is a simplex, is useful for linear classiﬁcation (hard-margin support vector
machines) as well as problems in computational geometry [10]. Further, the (cid:96)2-(cid:96)2 setting, where
both X and Y are Euclidean balls (with general center), includes linear regression.

Many problems of practical interest are sparse, i.e., the number of nonzero elements in A, which
we denote by nnz, satisﬁes nnz (cid:28) mn. Examples include: linear programs with constraints involving
few variables, linear classiﬁcation with 1-hot-encoded features, and linear systems that arise from
physical models with local interactions. The problem description size nnz plays a central role in
several runtime analyses of algorithms for solving the problem (1).

However, sparsity is not an entirely satisfactory measure of instance complexity: it is not con-
tinuous in the elements of A and consequently it cannot accurately reﬂect the simplicity of “nearly
sparse” instances with many small (but nonzero) elements. Measures of numerical sparsity, such as
the (cid:96)1 to (cid:96)2 norm ratio, can ﬁll this gap [17]. Indeed, many problems encountered in practice are
numerically sparse. Examples include: linear programming constraints of the form x1 ≥ 1
i xi,
n
linear classiﬁcation with neural network activations as features, and linear systems arising from
physical models with interaction whose strength decays with distance.

(cid:80)

Existing bilinear minimax solvers do not exploit the numerical sparsity of A and their runtime
guarantees do not depend on it—the basic limitation of these methods is that they do not directly
access the large matrix entries, and instead sample the full columns and rows in which they occur.
To overcome this limitation, we propose methods that access A a single entry at a time, leverage
numerical sparsity by accessing larger coordinates more frequently, and enjoy runtime guarantees
that depend explicitly on numerical sparsity measures. For numerically sparse large-scale instances
our runtimes are substantially better than the previous state-of-the-art. Moreover, our runtimes
subsume the previous state-of-the-art dependence on nnz and rcs, the maximum number of nonzeros
in any row or column.

In addition to proposing algorithms with improved runtimes, we develop two techniques that may
be of broader interest. First, we design non-uniform sampling schemes that minimize regret bounds;
we use a general framework that uniﬁes the Euclidean and (local norms) simplex geometries, possibly
facilitating future extension. Second, we build a data structure capable of eﬃciently maintaining
and sampling from multiplicative weights iterations (i.e. entropic projection) with a ﬁxed dense
component. This data structure overcomes limitations of existing techniques for maintaining entropic
projections and we believe it may prove eﬀective in other settings where such projections appear.

1.1 Our results

Table 2 summarizes our runtime guarantees and puts them in the context of the best existing results.
We consider methods that output (expected) (cid:15)-accurate solutions of the saddle-point problem (1),
namely a pair x, y satisfying

(cid:20)

E

max
v∈Y

v(cid:62)Ax − min
u∈X

1

(cid:21)
y(cid:62)Au

≤ (cid:15).

The algorithms in Table 2 are all iterative solvers for the general problem minx∈X maxy∈Y f (x, y),
specialized to f (x, y) = y(cid:62)Ax. Each algorithm presents a diﬀerent tradeoﬀ between per-iteration
complexity and the required iteration count, corresponding to the matrix access modality: exact
gradient methods compute matrix-vector products in each iteration, row-column stochastic gradient
methods sample a row and a column in each iteration, and our proposed coordinate methods take
this tradeoﬀ to an extreme by sampling a single coordinate of the matrix per iteration.1 In addition,
variance reduction (VR) schemes combine both fast stochastic gradient computations and infrequent
exact gradient computations, maintaining the amortized per-iteration cost of the stochastic scheme
and reducing the total iteration count for suﬃciently small (cid:15).

The runtimes in Table 2 depend on the numerical range of A through a matrix norm L that
changes with both the problem geometry and the type of matrix access; we use Lmv, Lrc and Lco
to denote the constants corresponding to matrix-vector products, row-column queries and coordi-
nated queries, respectively. Below, we describe these runtimes in detail. In the settings we study,
our results are the ﬁrst theoretical demonstration of runtime gains arising from sampling a single
coordinate of A at a time, as opposed to entire rows and columns.

Coordinate stochastic gradient methods. We develop coordinate stochastic gradient estima-
tors which allow per-iteration cost (cid:101)O (1) and iteration count (cid:101)O (cid:0)n + m + ( Lco
(cid:15) )2(cid:1). We deﬁne Lco in
is a measure of the numerical sparsity of A,
Table 1; for each domain geometry, the quantity Lco
Lrc
satisfying

1 ≤

≤ rcs.

L2
co
L2
rc
Every iteration of our method requires sampling an element in a row or a column with probability
proportional to its entries. Assuming a matrix access model that allows such sampling in time (cid:101)O (1)
(similarly to [5, 41, 15]), the total runtime of our method is (cid:101)O (cid:0)n + m + ( Lco
(cid:15) )2(cid:1). In this case, for
numerically sparse problems such that Lco = O(Lrc), the proposed coordinate methods outperform
row-column sampling by a factor of m + n. Moreover, the bound L2
rc(m + n) implies that
our runtime is never worse than that of row-column methods. When only coordinate access to the
matrix A is initially available, we may implement the required sampling access via preprocessing
in time O(nnz). This changes the runtime to (cid:101)O (cid:0)nnz + ( Lco
(cid:15) )2(cid:1), so that the comparison above holds
(cid:15) )2 = ˜Ω(nnz). In that regime, the variance reduction technique we describe below
only when ( Lco
provides even stronger guarantees.

co ≤ L2

nnz · Lco
(cid:15)

Coordinate methods with variance reduction. Using our recently proposed framework [8]
we design a variance reduction algorithm with amortized per-iteration cost (cid:101)O (1), required iteration
count of (cid:101)O (cid:0)√
In the numerically sparse
regime Lco = O(Lrc), our runtime improves on row-column VR by a factor of (cid:112)nnz/(m + n), and
in general the bound Lco ≤ Lrc
m + n guarantees it is never worse. Since variance reduction
methods always require a single pass over the data to compute an exact gradient, this comparison
In the (cid:96)2-(cid:96)2 setting we note that for elementwise
holds regardless of the matrix access model.

(cid:1) and total running time (cid:101)O (cid:0)nnz +

nnz · Lco
(cid:15)

(cid:1).

√

√

1 Interior point methods oﬀer an alternative tradeoﬀ between iteration cost and iteration count: the number
of required iterations depends on 1/(cid:15) only logarithmically, but every iteration is costly, requiring a linear system
solution which at present takes time Ω(min{m, n}2). In the (cid:96)1-(cid:96)1 geometry, the best known runtimes for interior
point methods are (cid:101)O((nnz + min{m, n}2)(cid:112)min{m, n}) [23], (cid:101)O(max{m, n}ω) [12], and (cid:101)O(mn + min{m, n}3) [45]. In
this paper we are mainly interested in the large-scale low-accuracy regime with L/(cid:15) < min(m, n) where the runtimes
described in Table 2 are favorable (with the exception of [45] in certain cases). Our methods take only few passes
over the data, which are not the case for many interior-point methods [23, 12]. Also, our methods do not rely on a
general (ill-conditioned) linear system solver, which is a key ingredient in interior point methods.

2

Lmv (matrix-vector)

Lrc (row-column)

Lco (coordinate)

(cid:96)1-(cid:96)1

(cid:96)2-(cid:96)1

(cid:96)2-(cid:96)2

maxi,j |Aij|

maxi (cid:107)Ai:(cid:107)2

(cid:107)A(cid:107)op

maxi,j |Aij|

maxi (cid:107)Ai:(cid:107)2

(cid:107)A(cid:107)F

max

(cid:110)

max

max

maxi (cid:107)Ai:(cid:107)2 , maxj (cid:107)A:j(cid:107)2
(cid:110)
(cid:111)†

maxi (cid:107)Ai:(cid:107)1 , (cid:107)A(cid:107)F
i (cid:107)Ai:(cid:107)2
1,

(cid:113)(cid:80)

j (cid:107)A:j(cid:107)2

1

(cid:110)(cid:113)(cid:80)

(cid:111)

(cid:111)

Table 1: Dependence on A for diﬀerent methods in diﬀerent geometries. Comments: Ai: and A:j
denote the ith row and jth column of A, respectively. Numerically sparse instances satisfy Lco = O(Lrc).
† In the (cid:96)2-(cid:96)1 setting we can also achieve, via alternative sampling schemes, Lco = Lrc
rcs and Lco =
max{maxi (cid:107)Ai:(cid:107)1 , (cid:112)maxi (cid:107)Ai:(cid:107)1 maxj (cid:107)A:j(cid:107)1}.

√

Method

Iteration cost

Exact gradient [28, 31]

Row-column [16, 10, 7]

Row-column VR [7, 8]

O(nnz)

O(n + m)

O(n + m)

(cid:16)

(cid:101)O

(cid:16)

Total runtime
nnz · Lmv · (cid:15)−1(cid:17)
(cid:101)O
rc · (cid:15)−2(cid:17)
(cid:16)
(m + n) · L2

(cid:101)O

nnz + (cid:112)nnz · (m + n) · Lrc · (cid:15)−1(cid:17)

Sparse row-col (folklore)

Sparse row-col VR (Appendix E)

Coordinate (Section 3.1)

Coordinate VR (Section 3.2)

(cid:101)O (rcs)

(cid:101)O (rcs)

(cid:101)O (1)

(cid:101)O (1)

(cid:101)O

(cid:16)

(cid:101)O

nnz +

(cid:16)

rc · (cid:15)−2(cid:17)
rcs · L2
√
nnz · rcs · Lrc · (cid:15)−1(cid:17)
co · (cid:15)−2(cid:17)
nnz · Lco · (cid:15)−1(cid:17)

nnz + L2
√

(cid:16)

(cid:101)O

(cid:16)

(cid:101)O

nnz +

Table 2: Comparison of iterative methods for bilinear problems. Comments: nnz denotes the
number of nonzeros in A ∈ Rm×n and rcs ≤ max{m, n} denotes the maximum number of nonzeros in any
row and column of A. The quantities Lmv, Lco and Lrc depend on problem geometry (see Table 1).

Method

Runtime

Task

MaxIB

Allen-Zhu et al. [2]

Our method (Theorem 3)

MinEB
(when m ≥ n)

Allen-Zhu et al. [2]

Our method (Theorem 4)

AGD [30]

Regression
(A(cid:62)A (cid:23) µI)

Gupta and Sidford [17]

(cid:101)O

(cid:16)

Our method (Theorem 5)

(cid:101)O

(cid:101)O (cid:0)mn + ρm

(cid:101)O (cid:0)nnz + ρ

(cid:101)O (cid:0)mn + m

√

√

√

n · (cid:15)−1(cid:1)
nnz · rcs · (cid:15)−1(cid:1)†
√
n · (cid:15)−1/2(cid:1)
nnz · rcs · (cid:15)−1/2(cid:1)†

nnz · (cid:107)A(cid:107)op

(cid:17)

1√
µ

(cid:101)O (cid:0)nnz +
(cid:16)

(cid:101)O
(cid:16) (cid:80)

nnz + nnz2/3 ·
√
(cid:16)

nnz +

nnz · max

i∈[n] (cid:107)A(cid:107)F · (cid:107)Ai:(cid:107)1 · (cid:107)Ai:(cid:107)2
(cid:110)(cid:113)(cid:80)

(cid:113)(cid:80)

i (cid:107)Ai:(cid:107)2
1,

j (cid:107)A:j(cid:107)2
1

(cid:17)1/3

(cid:17)

1√
µ
(cid:17)
(cid:111) 1√

µ

Table 3: Comparison of complexity for diﬀerent applications. Comments: ρ denotes the radii ratio
of the minimum ball enclosing the rows of A and maximum ball inscribed in them. † For MaxIB and MinEB,
we refer the reader to Section 6.2 for a more ﬁne-grained runtime bound.

3

m + n, and consequently our method
non-negative matrices, Lco = max{(cid:107)A1(cid:107)2, (cid:107)A(cid:62)1(cid:107)2} ≤ Lmv
outperforms exact gradient methods by a factor of (cid:112)nnz/(m + n), even without any numerical or
spectral sparsity in A. Notably, this is the same factor of improvement that row-column VR achieves
over exact gradient methods in the (cid:96)1-(cid:96)1 and (cid:96)2-(cid:96)1 regimes.

√

Optimality of the constant Lco. For the (cid:96)1-(cid:96)1 and (cid:96)2-(cid:96)2 settings, we argue that the constant Lco
in Table 1 is optimal in the restricted sense that no alternative sampling distribution for coordinate
gradient estimation can have a better variance bound than Lco (a similar sense of optimality also
holds for Lrc in each geometry). In the (cid:96)2-(cid:96)1 setting, a diﬀerent sampling distribution produces an
improved (and optimal) constant max{maxi (cid:107)Ai:(cid:107)1 , (cid:107)|A|(cid:107)op}, where Ai: is the ith row of A, and
|A|ij = |Aij| is the elementwise absolute value of A. However, it is unclear how to eﬃciently sample
from this distribution.

Row-column sparse instances. Some problem instances admit a structured form of sparsity
where every row and column has at most rcs nonzero elements. In all settings we have Lco ≤ Lrc
rcs
and so our coordinate methods naturally improve when rcs is small. Speciﬁcally, the sampling
distributions and data structures we develop in this paper allow us to modify previous methods for
row-column VR [8] to leverage row-column sparsity, reducing the amortized per-iteration cost from
O (m + n) to (cid:101)O (rcs).

√

Applications. We illustrate the implications of our results for two problems in computational
geometry, minimum enclosing ball (Min-EB) and maximum inscribed ball (Max-IB), as well as
linear regression. For Min-EB and Max-IB in the non-degenerate case m ≥ n, we apply our (cid:96)2-(cid:96)1
results to obtain algorithms whose runtime bounds coincide with the state-of-the-art [2] for dense
problems, but can be signiﬁcantly better for sparse or row-column sparse instances. For linear
regression we focus on accelerated linearly converging algorithms, i.e., those that ﬁnd x such that
(cid:107)Ax − b(cid:107)2 ≤ (cid:15) in time proportional to µ− 1
where µ is the smallest eigenvalue of A(cid:62)A. Within
this class and in a number of settings, our reduced variance coordinate method oﬀers improvement
over the state-of-the-art: for instances where (cid:107)Ai:(cid:107)1 = O((cid:107)Ai:(cid:107)2) and (cid:107)A:j(cid:107)1 = O((cid:107)A:j(cid:107)2) for all i, j
it outperforms [17] by a factor of nnz1/6, and for elementwise nonnegative instances it outperforms
accelerated gradient descent by a factor of (cid:112)nnz/(m + n). See Table 3 for a detailed runtime
comparison.

2 log 1
(cid:15)

1.2 Our approach

We now provide a detailed overview of our algorithm design and analysis techniques, highlighting our
main technical insights. We focus on the (cid:96)1-(cid:96)1 geometry, since it showcases all of our developments.
Our technical contributions have two central themes:

1. Sampling schemes design. The key to obtaining eﬃcient coordinate methods is carefully
choosing the sampling distribution. Here, local norms analysis of stochastic mirror descent [39]
on the one hand enables tight regret bounds, and on the other hand imposes an additional design
constraint since the stochastic estimators must be bounded for the analysis to apply. We achieve
estimators with improved variance bounds meeting this boundedness constraint by leveraging
a “clipping” operation introduced by Clarkson et al. [10]. Speciﬁcally, in the simplex geometry,
we truncate large coordinates of our estimators, and show that our method is robust to the
resulting distortion.

4

2. Data structure design. Our goal is to perform iterations in (cid:101)O (1) time, but our mirror descent
procedures call for updates that change m + n variables in each step. We resolve this tension via
data structures that implicitly maintain the iterates. Variance reduction poses a considerable
challenge here, because every reduced-variance stochastic gradient contains a dense component
that changes all coordinates in a complicated way. In particular, existing data structures cannot
eﬃciently compute the normalization factor necessary for projection to the simplex. We design
a data structure that overcomes this hurdle via Taylor expansions, coordinate binning, and a
binomial heap-like construction. The data structure computes approximate mirror projections,
and we modify the standard mirror descent analysis to show it is stable under the particular
structure of the resulting approximation errors.

At the intersection of these two themes is a novel sampling technique we call “sampling from the
sum,” which addresses the same variance challenges as the “sampling from the diﬀerence” technique
of [8], but is more amenable to eﬃcient implementation with a data structure.

1.2.1 Coordinate stochastic gradient method

Our algorithm is an instance of stochastic mirror descent [29], which in the (cid:96)1-(cid:96)1 setting produces
a sequence of iterates (x1, y1), (x2, y2), . . . according to

xt+1 = Π∆ (xt ◦ exp{−η˜gx(xt, yt)}) and yt+1 = Π∆ (yt ◦ exp{−η˜gy(xt, yt)}) ,

(2)

where Π∆(v) = v
is the projection onto the simplex (exp and log are applied to vectors elemen-
(cid:107)v(cid:107)1
twise, and elementwise multiplication is denoted by ◦), η is a step size, and ˜gx, ˜gy are stochastic
gradient estimators for f (x, y) = y(cid:62)Ax satisfying

E ˜gx(x, y) = ∇xf (x, y) = A(cid:62)y and E ˜gy(x, y) = −∇yf (x, y) = −Ax.

We describe the computation and analysis of ˜gx; the treatment of ˜gy is analogous. To compute

˜gx(x, y), we sample i, j from a distribution p(x, y) on [m] × [n] and let

˜gx(x, y) =

yiAij
pij(x, y)

ej,

(3)

where pij(x, y) denotes the probability of drawing i, j from p(x, y) and ej is the jth standard basis
vector—a simple calculation shows that E ˜gx = A(cid:62)y for any p. We ﬁrst design p(x, y) to guarantee
an (cid:101)O (cid:0)( Lco
(cid:15) )2(cid:1) iteration complexity for ﬁnding an (cid:15)-accurate solution, and then brieﬂy touch on how
to compute the resulting iterations in (cid:101)O (1) time.

∞ ≤ L2 for all x, y (and similarly for ˜gy), taking η = (cid:15)

Local norms-informed distribution design. The standard stochastic mirror descent analy-
sis [29] shows that if E (cid:107)˜gx(x, y)(cid:107)2
L2 and a
choice of T = (cid:101)O (cid:0)( L
t=1(xt, yt) is an (cid:15)-accurate
solution in expectation. Unfortunately, this analysis demonstrably fails to yield suﬃciently tight
bounds for our coordinate estimator: there exist instances for which any distribution p produces
L ≥ nLrc. We tighten the analysis using a local norms argument [cf. 39, Section 2.8], showing that
(cid:101)O (cid:0)( L

(cid:15) )2(cid:1) iterations suﬃce whenever (cid:107)η˜gx(cid:107)∞ ≤ 1 with probability 1 and for all x, y

(cid:15) )2(cid:1) suﬃces to ensure that the iterate average 1

(cid:80)T

T

E (cid:107)˜gx(x, y)(cid:107)2

x ≤ L2, where (cid:107)γ(cid:107)2

x =

(cid:88)

xjγ2
j

j

5

is the local norm at x ∈ X . We take

pij = yi

A2
ij
(cid:107)Ai:(cid:107)2
2

(4)

(recalling that x, y are both probability vectors). Substituting into (3) gives

E (cid:107)˜gx(x, y)(cid:107)2

x =

ijxj

i A2
y2
pij

(cid:88)

i,j

(cid:88)

=

i,j

yi (cid:107)Ai:(cid:107)2

2 xj =

(cid:88)

i

yi (cid:107)Ai:(cid:107)2

2 ≤ max

i

(cid:107)Ai:(cid:107)2

2 ≤ L2
co,

with Lco = max{maxi (cid:107)Ai:(cid:107)2 , maxj (cid:107)A:j(cid:107)2} as in Table 1.
While this is the desired bound on E (cid:107)˜gx(x, y)(cid:107)2
x

, the requirement (cid:107)η˜gx(cid:107)∞ ≤ 1 does not hold
when A has suﬃciently small elements. We address this by clipping ˜g: we replace η˜gx with clip(η˜xx),
where

[clip(v)]i := min{|vi|, 1} sign(vi),
the Euclidean projection to the unit box. The clipped gradient estimator clearly satisﬁes the de-
sired bounds on inﬁnity norm and local norm second moment, but is biased for the true gradient.
Following the analysis of Clarkson et al. [10], we account for the bias by relating it to the second
moment via

| (cid:104)γ − clip(γ), x(cid:105) | ≤ (cid:107)γ(cid:107)2
x ,

which allows to absorb the eﬀect of the bias into existing terms in our error bounds. Putting together
these pieces yields the desired bound on the iteration count.

Eﬃcient implementation. Data structures for performing the update (2) and sampling from
the resulting iterates in (cid:101)O (1) time are standard in the literature [e.g., 37]. We add to these the
somewhat non-standard ability to also eﬃciently track the running sum of the iterates. To eﬃciently
sample i, j ∼ p according to (4) we ﬁrst use the data structure to sample i ∼ y in (cid:101)O (1) time and then
draw j ∈ [n] with probability proportional to A2
in time O(1), via either O(nnz) preprocessing or
ij
an appropriate assumption about the matrix access model. The “heavy lifting” of our data structure
design is dedicated for supporting variance reduction, which we describe in the next section.

Sampling distributions beyond (cid:96)1-(cid:96)1. Table 4 lists the sampling distributions we develop for
the various problem geometries. Note that for the (cid:96)2-(cid:96)1 setting we give three diﬀerent distributions
for sampling the simplex block of the gradient (i.e., ˜gy); each distribution corresponds to a diﬀerent
parameter Lco (see comments following Table 1). The distribution qij ∝
yi |Aijxj| yields a stronger
bound L in the (cid:96)2-(cid:96)1 setting, but we do not know how to eﬃciently sample from it.

√

1.2.2 Coordinate variance reduction

To accelerate the stochastic coordinate method we apply our recently proposed variance reduction
framework [8]. This framework operates in α
epochs, where α is a design parameter that trades
(cid:15)
between full and stochastic gradient computations. Each epoch consists of three parts: (i) computing
the exact gradient at a reference point (x0, y0), (ii) performing T iterations of regularized stochastic
mirror descent to produce the sequence (x1, y1), . . . , (xT , yT ) and (iii) taking an extra-gradient step
from the average of the iterates in (ii). Setting κ = 1/(1+ηα/2), the iterates xt follow the recursion

xt+1 = Π∆

(cid:0)xκ

t ◦ x1−κ
0

◦ exp{−ηκ[gx

0 + ˜δx(xt, yt)]}(cid:1), where Π∆(v) =

v
(cid:107)v(cid:107)1

,

(5)

6

0 = A(cid:62)y0 is the exact gradient at the reference point, and ˜δx is a stochastic gradient diﬀerence

and gx
estimator satisfying

E ˜δx(x, y) = ∇xf (x, y) − ∇xf (x0, y0) = A(cid:62)(y − y0).

The iteration for yt is similar. In [8] we show that if ˜δx satisﬁes

E (cid:107)˜δx(x, y)(cid:107)2

∞ ≤ L2 (cid:16)

(cid:107)x − x0(cid:107)2

1 + (cid:107)y − y0(cid:107)2

1

(cid:17)

∀x, y

(6)

, then T = O( L2

α2 ) iterations per epoch with step size

and a similar bound holds on E (cid:107)˜δy(x, y)(cid:107)2
∞
η = α

L2 suﬃce for the overall algorithm to return a point with expected error below (cid:15).
We would like to design a coordinate-based estimator ˜δ such that the bound (6) holds for
L = Lco as in Table 1 and each iteration (5) takes (cid:101)O (1) time. Since every epoch also requires
O(nnz) time for matrix-vector product (exact gradient) computations, the overall runtime would be
(cid:101)O((nnz + L2

nnz then gives the desired runtime (cid:101)O(nnz +

(cid:15) ). Choosing α = Lco/

nnz · Lco

α2 ) · α

(cid:15) ).

√

√

co

Distribution design (sampling from the diﬀerence). We start with a straightforward adap-
tation of the general estimator form (3). To compute ˜δx(x, y), we sample i, j ∼ p, where p may
depend on x, x0, y and y0, and let

˜δx(x, y) =

(yi − [y0]i)Aij
pij

ej,

(7)

where ej is the jth standard basis vector. As in the previous section, we ﬁnd that the requirement (6)
is too stringent for coordinate-based estimators. Here too, we address this challenge with a local
norms argument and clipping of the diﬀerence estimate. Using the “sampling from the diﬀerence”
technique from [8], we arrive at

pij =

|yi − [y0]i|
(cid:107)y − y0(cid:107)1

·

A2
ij
(cid:107)Ai:(cid:107)2
2

.

(8)

This distribution satisﬁes the local norm relaxation of (6) with L2 = L2
co

.

Data structure design. Eﬃciently computing (5) is signiﬁcantly more challenging than its coun-
terpart (2). To clarify the diﬃculty and describe our solution, we write

and break the recursion for the unnormalized iterates ˆxt into two steps

xt = Π∆(ˆxt) = ˆxt/ (cid:107)ˆxt(cid:107)1

t = ˆxκ
ˆx(cid:48)
ˆxt+1 = ˆx(cid:48)

t ◦ exp{v}, and
t ◦ exp{st},

(9)

(10)

is a ﬁxed dense vector, and st = −η˜δx(xt, yt) is a varying 1-sparse
where v = (1 − κ) log x0 − ηκgx
0
vector. The key task of the data structures is maintaining the normalization factor (cid:107)ˆxt(cid:107)1
in near-
constant time. Standard data structures do not suﬃce because they lack support for the dense
step (9).

Our high-level strategy is to handle the two steps (9) and (10) separately. To handle the dense
step (9), we propose the data structure ScaleMaintainer that eﬃciently approximates (cid:107)ˆxt(cid:107)1
in the
“homogeneous” case of no sparse updates (i.e. st = 0 for all t). We then add support for the sparse
step (10) using a binomial heap-like construction involving O(log n) instances of ScaleMaintainer.

7

The ScaleMaintainer data structure. When st = 0 for all t the iterates ˆxt admit closed forms

ˆxt+τ = ˆxκτ

t ◦ exp

(cid:41)

κt(cid:48)

(cid:40)
v

τ −1
(cid:88)

t(cid:48)=0

= ˆxκτ

t ◦ exp

(cid:110) 1 − κτ
1 − κ

(cid:111)

v

= ˆxt ◦ exp {[1 − κτ ]¯v} ,

where ¯v = v
dimensional vectors ¯x ∈ R¯n
≥0

1−κ − log xt. Consequently, we design ScaleMaintainer to take as initialization ¯n-
, and ¯v ∈ R¯n and provide approximations of the normalization factor

Zτ (¯x, ¯v) = (cid:107)¯x ◦ exp{(1 − κτ )¯v})(cid:107)1

(11)

for arbitrary values of τ ≥ 1. We show how to implement each query of Zτ (¯x, ¯v) in amortized
time (cid:101)O (1). The data structure also supports initialization in time (cid:101)O (¯n) and deletions (i.e., setting
elements of ¯x to zero) in amortized time (cid:101)O (1).

To eﬃciently approximate the quantity Zτ (¯x, ¯v) we replace the exponential with its order p =

O(log n) Taylor series. That is, we would like to write

Zτ (¯x, ¯v) =

(cid:88)

i∈[¯n]

[¯x]ie(1−κτ )[¯v]i ≈

(cid:88)

[¯x]i

i∈[¯n]

p
(cid:88)

q=0

1
q!

(1 − κτ )q[¯v]q

i =

p
(cid:88)

q=0

(1 − κτ )q
q!

(cid:104)¯x, ¯vq(cid:105) .

q=0

(1−κτ )q
q!

The approximation (cid:80)p
(cid:104)¯x, ¯vq(cid:105) is cheap to compute, since for every τ it is a linear combina-
tion of the p = (cid:101)O (1) numbers {(cid:104)¯x, ¯vq(cid:105)}q∈[p] which we can compute once at initialization. However,
the Taylor series approximation has low multiplicative error only when |(1 − κτ )[¯v]i| = O(p), which
may fail to hold, as we may have (cid:107)¯v(cid:107)∞ = poly(n) in general. To handle this, suppose that for a
ﬁxed τ we have an oﬀset µ ∈ R and “active set” A ⊆ [¯n] such that the following conditions hold for a
threshold R = O(p): (a) the Taylor approximation is valid in A, e.g. we have |(1 − κτ )(¯vi − µ)| ≤ 2R
for all i ∈ A, (b) entries outside A are small; (1 − κτ )[¯vi − µ] ≤ −R for all i /∈ A, and (c) at least
one entry in the active set is large; (1 − κτ )[¯vi − µ] ≥ 0 for some i ∈ A. Under these conditions, the
entries in Ac are negligibly small and we can truncate them, resulting in the approximation

e(1−κτ )µ





p
(cid:88)

q=0

(1 − κτ )q
q!

(cid:104)¯x, (¯v − µ)q(cid:105)A + e−R (cid:104)¯x, 1(cid:105)Ac

 ,



which we show approximates Zτ (¯x, ¯v) to within eO(R+log n)−Ω(p) multiplicative error, where we used
(cid:104)a, b(cid:105)S := (cid:80)
= O(R), which we guarantee when
choosing the initial ¯x.

i∈S aibi; here, we also require that log maxi ¯xi
mini ¯xi

The challenge then becomes eﬃciently mapping any τ to {(cid:104)¯x, (¯v − µ)q(cid:105)A}q∈[p] for suitable µ
and A. We address this by jointly bucketing τ and ¯v. Speciﬁcally, we map τ into a bucket index
1−κτ
1−κ (cid:99), pick µ to be the largest integer multiple of R/((1 − κ)2k) such that µ ≤ maxi ¯vi,
k = (cid:98)log2
1−κ (cid:99) = O(log n), we argue that
and set A = {i | |(1 − κ)2k(¯vi − µ)| ≤ R}. Since k ≤ kmax = (cid:98)log2
computing (cid:104)¯x, (¯v − µ)q(cid:105)A
1−κ ) = (cid:101)O (¯n)
time, which we can charge to initialization. We further show how to support deletions in (cid:101)O (1) time
by carefully manipulating the computed quantities.

for every possible resulting µ and A takes at most O(¯np log 1

1

Supporting sparse updates. Building on ScaleMaintainer, we design the data structure
ApproxExpMaintainer that (approximately) implements the entire mirror descent step (5) in time

8

(cid:101)O (1).2 The data structure maintains vectors ¯x ∈ ∆n and ¯v ∈ Rn and K = (cid:100)log2(n + 1)(cid:101) instances
of ScaleMaintainer denoted {ScaleMaintainerk}k∈[K]. The kth instance tracks a coordinate sub-
. We let τk ≥ 0
set Sk ⊆ [n] such that {Sk}k∈[K] partitions [n], and has initial data [¯x]Sk
denote the “time index” parameter of the kth instance. The data structure satisﬁes two invariants;
ﬁrst, the unnormalized iterate ˆx satisﬁes

and [¯v]Sk

[ˆx]Sk = [¯x ◦ exp{(1 − κτk )¯v}]Sk

, for all k ∈ [K].

Second, the partition satisﬁes

|Sk| ≤ 2k − 1 for all k ∈ [K],

(12)

(13)

where at initialization we let SK = [n] and Sk = ∅ for k < K, ¯x = x0, ¯v = v

, since ScaleMaintainer allows us to approximate (cid:107)ˆxt(cid:107)1 = (cid:80)

1−κ − log x0 and τK = 0.
The invariant (12) allows us to eﬃciently (in time (cid:101)O (K) = (cid:101)O (1)) query coordinates of xt =
k∈[K] Zτk ([¯x]Sk , [¯v]Sk ) with
ˆxt/ (cid:107)ˆxt(cid:107)1
Z as deﬁned in (11). To implement the dense step (9), we simply increment τk ← τk + 1 for
every k. Let j be the nonzero coordinate of st in the sparse step (10), and let k ∈ [K] be such
that j ∈ Sk. To implement (10), we delete coordinate j from ScaleMaintainerk, and create a
singleton instance ScaleMaintainer0 maintaining S0 = {j} with initial data [¯x]S0 = est ˆxj, [¯v]S0 =
vj/(1 − κ) − log(est ˆxj) and τ0 = 0. Going from k = 1 to k = K, we merge ScaleMaintainerk−1
into ScaleMaintainerk until the invariant (13) holds again. For example, if before the sparse step
we have |S1| = 1, |S2| = 3 and |S3| = 2, we will perform 3 consecutive merges, so that afterwards
we have |S1| = |S2| = 0 and |S3| = 7.

To merge two ScaleMaintainerk−1 into ScaleMaintainerk, we let S(cid:48)

k = Sk−1 ∪Sk and initialize
a new ScaleMaintainer instance with [¯x]S(cid:48)
,3 [¯v]S(cid:48)
and τk = 0;
k|) = (cid:101)O (cid:0)2k(cid:1) time due to the invariant (13). Noting that a merge at level k can only
this takes (cid:101)O (|S(cid:48)
happen once in every Ω(2k) updates, we conclude that the amortized cost of merges at each level is
(cid:101)O (1), and (since K = (cid:101)O (1)), so is the cost of the sparse update.

/(1 − κ) − log[ˆx]S(cid:48)

= [ˆx]S(cid:48)

= [v]S(cid:48)

k

k

k

k

k

Back to distribution design (sampling from the sum). Our data structure enables us to
compute the iteration (5) and query coordinates of the iterates xt and yt in (cid:101)O (1) amortized time.
However, we cannot compute ˜δx using the distribution (8) because we do not have an eﬃcient way
of sampling from |yt − y0|; Taylor approximation techniques are not eﬀective for approximating the
absolute value because it is not smooth. To overcome this ﬁnal barrier, we introduce a new design
which we call “sampling from the sum,”

pij(x, y) =

(cid:18) 1
3

yi +

(cid:19)

[y0]i

·

2
3

A2
ij
(cid:107)Ai:(cid:107)2
2

.

(14)

Sampling from the modiﬁed distribution is simple, as our data structure allows us to sample from
yt. Moreover, we show that the distribution (14) satisﬁes a relaxed version of (6) where the LHS is
replaced by a local norm as before, and the RHS is replaced by L2(Vx0(xt) + Vy0(yt)), where Vx(x(cid:48))
is the KL divergence between x and x(cid:48). In Table 5 we list the sampling distributions we design for
variance reduction in the diﬀerent domain geometries.

2The data structures ApproxExpMaintainer and ScaleMaintainer structure support two additional operations
necessary for our algorithm: eﬃcient approximate sampling from xt and maintenance of a running sum of ˆxτ . Given
the normalization constant approximation, the implementation of these operations is fairly straightforward, so we do
not discuss them in the introduction.
3More precisely, for every j ∈ S(cid:48)

ˆxi, where ε is a small padding constant that ensures

k we set ¯xj = ˆxj + ε maxi∈S(cid:48)

the bounded multiplicative range necessary for correct operation of ScaleMaintainer.

k

9

Setting

(cid:96)1-(cid:96)1

(cid:96)2-(cid:96)1

(cid:96)2-(cid:96)1

(cid:96)2-(cid:96)1

(cid:96)2-(cid:96)2

(cid:96)2-(cid:96)2

pij

A2
ij
(cid:107)Ai:(cid:107)2
2

|Aij|
(cid:107)Ai:(cid:107)1

yi ·

yi ·

yi ·

|Aij|
(cid:107)Ai:(cid:107)1

yi ·

|Aij|
(cid:107)Ai:(cid:107)1

qij

xj ·

A2
ij
(cid:107)A:j(cid:107)2
2

A2
ij
(cid:107)A(cid:107)2
F

∝ x2

j · 1Aij (cid:54)=0

Aij · x2
j
k∈[n] (cid:107)A:k(cid:107)1 · x2
k

(cid:80)

(cid:107)Ai:(cid:107)2
1
k∈[m] (cid:107)Ak:(cid:107)2
1

(cid:80)

·

|Aij|
(cid:107)Ai:(cid:107)1

(cid:107)A:j(cid:107)2
1
k∈[n] (cid:107)A:k(cid:107)2
1

(cid:80)

·

|Aij|
(cid:107)A:j(cid:107)1

2

yi
(cid:107)y(cid:107)2
2

·

|Aij|
(cid:107)Ai:(cid:107)1

2

xj
(cid:107)x(cid:107)2
2

·

|Aij|
(cid:107)A:j(cid:107)1

Table 4: The distributions p, q used in our coordinate gradient estimator. Comments: The
estimator is of the form ˜g(x, y) = (cid:0) 1
pij

(cid:1) where i, j ∼ p and l, k ∼ q.

yiAij · ej, − 1
qlk

Alkxk · el

Setting

pij

qij

(cid:96)1-(cid:96)1

(cid:96)2-(cid:96)1

(cid:96)2-(cid:96)1

(cid:96)2-(cid:96)1

(cid:96)2-(cid:96)2

(cid:96)2-(cid:96)2

yi + 2[y0]i
3

yi + 2[y0]i
3

yi + 2[y0]i
3

yi + 2[y0]i
3

·

·

·

·

A2
ij
(cid:107)Ai:(cid:107)2
2

|Aij|
(cid:107)Ai:(cid:107)1

|Aij|
(cid:107)Ai:(cid:107)1

|Aij|
(cid:107)Ai:(cid:107)1

xj + 2[x0]j
3

·

A2
ij
(cid:107)A:j(cid:107)2
2

A2
ij
(cid:107)A(cid:107)2
F

∝ [x − x0]2

j · 1Aij (cid:54)=0

|Aij| · [x − x0]2
j
k∈[n] (cid:107)A:k(cid:107)1 · [x − x0]2
k

(cid:80)

(cid:107)Ai:(cid:107)2
1
k∈[m] (cid:107)Ak:(cid:107)2
1

(cid:80)

·

|Aij|
(cid:107)Ai:(cid:107)1

[y − y0]2
i
(cid:107)y − y0(cid:107)2
2

·

|Aij|
(cid:107)Ai:(cid:107)1

(cid:80)

(cid:107)A:j(cid:107)2
1
k∈[n] (cid:107)A:k(cid:107)2
1
[x − x0]2
j
(cid:107)x − x0(cid:107)2
2

·

·

|Aij|
(cid:107)A:j(cid:107)1

|Aij|
(cid:107)A:j(cid:107)1

Table 5: The distributions p, q used for our reduced variance coordinate gradient estimator.
(cid:1)
Comments: The estimator is of the form ˜g(x, y) = (cid:0)A(cid:62)y +
Alk(xk − x0,k) · el

(yi − y0,i)Aij · ej, −Ax −

1
pij

1
qlk

where i, j ∼ p and l, k ∼ q and x0, y0 is a reference point.

10

1.3 Related work

Coordinate methods. Updating a single coordinate at a time—or more broadly computing only
a single coordinate of the gradient at every iteration—is a well-studied and successful technique in
optimization [50]. Selecting coordinates at random is key to obtaining strong performance guaran-
tees: Strohmer and Vershynin [42] show this for linear regression, Shalev-Shwartz and Tewari [36]
show this for (cid:96)1 regularized linear models, and Nesterov [32] shows this for general smooth mini-
mization. Later works [22, 3, 33] propose accelerated coordinate methods. These works share two
common themes: selecting the gradient coordinate from a non-uniform distribution (see also [34]),
and augmenting the 1-sparse stochastic gradient with a dense momentum term. These techniques
play important roles in our development as well.

To reap the full beneﬁts of coordinate methods, iterations must be very cheap, ideally taking
near-constant time. However, most coordinate methods require super-constant time, typically in the
form of a vector-vector computation. Even works that consider coordinate methods in a primal-dual
context [38, 2, 52, 27, 37] perform the coordinate updates only on the dual variable and require a
vector-vector product (or more generally a component gradient computation) at every iteration.

A notable exception is the work of Wang [49, 48] which develops a primal-dual stochastic co-
ordinate method for solving Markov decision processes, essentially viewing them as (cid:96)∞-(cid:96)1 bilinear
saddle-point problems. Using a tree-based (cid:96)1 sampler data structure similar to the (cid:96)1 sampler we use
for simplex domains for the sublinear case, the method allows for (cid:101)O (1) iterations and a potentially
sublinear runtime scaling as (cid:15)−2. Tan et al. [43] also consider bilinear saddle-point problems and
variance reduction. Unlike our work, they assume a separable domain, use uniform sampling, and
do not accelerate their variance reduction scheme with extra-gradient steps. The separable domain
makes attaining constant iteration cost time much simpler, since there is no longer a normalization
factor to track, but it also rules out applications to the simplex domain. While Tan et al. [43] report
promising empirical results, their theoretical guarantees do not improve upon prior work.

Our work develops coordinate methods with (cid:101)O (1) iteration cost for new types of problems.
Furthermore, it maintains the iteration eﬃciency even in the presence of dense components arising
from the update, thus allowing for acceleration via an extra-gradient scheme.

Data structures for optimization. Performing iterations in time that is asymptotically smaller
than the number of variables updated at every iteration forces us to carry out the updates implicitly
using data structures; several prior works employ data structures for exactly the same reason. One
of the most similar examples comes from Lee and Sidford [22], who design a data structure for
an accelerated coordinate method in Euclidean geometry. In our terminology, their data structure
allows performing each iteration in time O(rcs) while implicitly updating variables of size O(n).
Duchi et al. [14] design a data structure based on balanced search trees that supports eﬃcient
Euclidean projection to the (cid:96)1 ball of vector of the form u + s where u is in the (cid:96)1 ball and s is
sparse. They apply it in a stochastic gradient method for learning (cid:96)1 regularized linear classiﬁer
with sparse features. Among the many applications of this data structure, Namkoong and Duchi
[27] adapt it to eﬃciently compute Euclidean projections into the intersection of the simplex and a
χ2 ball for 1-sparse updates. Shalev-Shwartz and Wexler [37] and Wang [49, 48], among others, use
binary tree data structures to perform multiplicative weights projection to the simplex and sampling
from the iterates.

A recent work of Sidford and Tian [40] develops a data structure which is somewhat similar to our
ApproxExpMaintainer data structure, for updates arising from a primal-dual method to eﬃciently
solve (cid:96)∞ regression. Their data structure was also designed to handle updates to a simplex variable
which summed a structured dense component and a sparse component. However, the data structure

11

design of that work speciﬁcally exploited the structure of the maximum ﬂow problem in a number
of ways, such as bounding the sizes of the update components and relating these bounds to how
often the entire data structure should be restarted. Our data structure can handle a broader range
of structured updates to simplex variables and has a much more ﬂexible interface, which is crucial
to the development of our variance-reduced methods as well as our applications.

Another notable use of data structures in optimization appears in second order methods, where
a long line of work uses them to eﬃciently solve sequences of linear systems and approximately
compute iterates [20, 4, 23, 12, 25, 44, 45]. Finally, several works on low rank optimization make
use of sketches to eﬃciently represent their iterates and solutions [9, 51].

Numerical sparsity. Measures of numerical sparsity, such as the (cid:96)2/(cid:96)∞ or (cid:96)1/(cid:96)2 ratios, are
continuous and dimensionless relaxations of the (cid:96)0 norm. The stable rank of a matrix A measures
the numerical sparsity of its singular values (speciﬁcally, their squared (cid:96)2/(cid:96)∞ ratio) [11].

For linear regression, stochastic methods generally outperform exact gradient methods only when
A is has low stable rank, cf. discussion in [8, Section 4.3], i.e., numerically sparse singular values. In
recent work, Gupta and Sidford [17] develop algorithms for linear regression and eigenvector prob-
lems for matrices with numerically sparse entries (as opposed to singular values). Our paper further
broadens the scope of matrix problems for which we can beneﬁt from numerical sparsity. Moreover,
our results have implications for regression as well, improving on [17] in certain numerically sparse
regimes.

In recent work by Babichev et al. [6], the authors develop primal-dual sublinear methods for
(cid:96)1-regularized linear multi-class classiﬁcation (bilinear games in (cid:96)1-(cid:96)∞ geometry), and obtain com-
plexity improvements depending on the numerical sparsity of the problem. Similarly to our work,
careful design of the sampling distribution plays a central role in [6]. They also develop a data
structure that allows iteration cost independent of the number of classes. However, unlike our work,
Babichev et al. [6] rely on sampling entire rows and columns, have iteration costs linear in n + m,
and do not utilize variance reduction. We believe that our techniques can yield improvements in
their setting.

1.4 Paper organization

In Section 2, we set up our terminology, notation, the interfaces of our data structures, and the
diﬀerent matrix access models we consider. In Section 3 we develop our algorithmic framework: we
present coordinate stochastic gradient methods in Section 3.1 and their reduced variance counter-
parts in Section 3.2. In Section 4 we apply both methods to solving (cid:96)1-(cid:96)1 matrix games; we show
how to implement the method using our data structures and analyze the runtime. In Section 5, we
discuss in detail the implementation and analysis of our data structures. Finally, in Section 6 we
specialize our results to obtain algorithms for minimum enclosing ball and maximum inscribed ball
problems as well as linear regression. Many proof details as well as our algorithms for other domain
setups, i.e. (cid:96)2-(cid:96)1 and (cid:96)2-(cid:96)2 are deferred to the appendix.

12

2 Preliminaries

In Section 2.1, we abstract the properties of the diﬀerent domains we handle into a general notion of
a “local norm” setup under which we develop our results. In Section 2.2, we give the deﬁnition and
optimality criterion of the bilinear saddle-point problem we study. In Section 2.3, we give the matrix
access models used in the algorithms we design. In Section 2.4, we summarize the interfaces and
complexity of the data structures we design, deferring their detailed implementations to Section 5.

2.1 Local norm setups

The analyses of our algorithms cater to the geometric of each speciﬁc domain. To express our
results generically, for each pair of domains Z = X × Y, we deﬁne an associated “local norm setup”,
which contains various data tailored for our analyses. While initially this notation may appear
complicated or cumbersome, later it helps avoid redundancy in the paper. Further, it clariﬁes the
structure necessary to generalize our methods to additional domain geometries.

Deﬁnition 1. A local norm setup is the quintuplet (Z, (cid:107)·(cid:107)·, r, Θ, clip) , where

1. Z is a compact and convex subset of Z ∗ := Rn × Rm.

2. (cid:107)·(cid:107)· is a local norm: for every z ∈ Z, the function (cid:107)·(cid:107)z : Z ∗ → R≥0 is a norm on Z ∗.
3. r : Z → R is a convex distance generating function: its induced Bregman divergence

Vz(z(cid:48)) := r(z(cid:48)) − r(z) − (cid:10)∇r(z), z(cid:48) − z(cid:11) .

satisﬁes

(cid:10)γ, z − z(cid:48)(cid:11) − Vz(z(cid:48)) ≤

1
2

(cid:107)γ(cid:107)2

∗ :=

1
2

max
s∈Z

(cid:107)γ(cid:107)2
s

for all z, z(cid:48) ∈ Z and γ ∈ Z ∗.

(15)

4. Θ = maxz,z(cid:48)∈Z {r(z) − r(z(cid:48))} is the range of r. For z∗ ∈ arg minz∈Z r(z) we have Θ is an

upper bound on the range of Vz∗(z) ≤ Θ for all z ∈ Z.

5. clip : Z ∗ → Z ∗ is a mapping that enforces a local version of (15):

| (cid:10)clip(γ), z − z(cid:48)(cid:11) | − Vz(z(cid:48)) ≤ (cid:107)γ(cid:107)2

z

for all z, z(cid:48) ∈ Z and γ ∈ Z ∗,

(16)

and satisﬁes the distortion guarantee

| (cid:104)γ − clip(γ), z(cid:105) | ≤ (cid:107)γ(cid:107)2
z

for all z ∈ Z and γ ∈ Z ∗.

(17)

Table 6 summarizes the three local norm setups we consider. Throughout the paper,

for a vector z ∈ X × Y, we denote its X and Y blocks by zx and zy.

In addition, we write coordinate i of any vector v as [v]i.

Proposition 1. The quintuplets (Z, (cid:107)·(cid:107)·, r, Θ, clip) in Table 6 satisfy the local norm setup re-
quirements in Deﬁnition 1.

13

(cid:96)1-(cid:96)1

∆n

∆m

(cid:113)(cid:80)

k∈[n+m][z]k[δ]2
k

(cid:80)

k∈[n+m][z]k log[z]k
log(mn)

X

Y

(cid:107)δ(cid:107)z

r

Θ

clip(δ)

sign(δ) ◦ min{1, |δ|}

(cid:96)2-(cid:96)1

Bn

∆m
2 + (cid:80)
2 + (cid:80)

(cid:113)

(cid:107)δx(cid:107)2

1

2 (cid:107)zx(cid:107)2

i∈[m][zy]i[δy]2
i
i∈[m][zy]i log[zy]i

1
2 + log(m)
(δx, sign(δy) ◦ min{1, |δy|})

(cid:96)2-(cid:96)2

Bn

Bm

(cid:107)δ(cid:107)2
2 (cid:107)z(cid:107)2
1
1

2

δ

Table 6: Local norm setups. Comments: In each case, Z = X × Y, ∆n is the probability simplex
{x | x ∈ Rn
n x = 1}, Bn is the Euclidean ball {x | x ∈ Rn, (cid:107)x(cid:107)2 ≤ 1}, the operations sign,
min, and | · | are performed entrywise on a vector, and ◦ stands for the entrywise product between
vectors.

≥0, 1(cid:62)

While Proposition 1 is not new, for completeness and compatibility with our notation we prove it
in Appendix A.

In each local norm setup, we slightly overload notation and use (cid:107)·(cid:107) (without a subscript) to
, i.e., (cid:107)η(cid:107) := maxδ:(cid:107)δ(cid:107)∗≤1 δ(cid:62)η. In each domain geometry (cid:107)·(cid:107) and (cid:107)·(cid:107)∗

denote the dual norm of (cid:107)·(cid:107)∗
are as follows:

(cid:107)η(cid:107) =

(cid:107)η(cid:107) =

(cid:113)

(cid:113)

(cid:107)ηx(cid:107)2

(cid:107)ηx(cid:107)2

1

1 + (cid:107)ηy(cid:107)2
2 + (cid:107)ηy(cid:107)2

1

(cid:107)η(cid:107) = (cid:107)η(cid:107)2

(cid:113)

(cid:113)

(cid:107)δx(cid:107)2

(cid:107)δx(cid:107)2

∞

∞ + (cid:107)δy(cid:107)2
2 + (cid:107)δy(cid:107)2

∞

(cid:107)δ(cid:107)∗ =

(cid:107)δ(cid:107)∗ =
(cid:107)δ(cid:107)∗ = (cid:107)δ(cid:107)2

for (cid:96)1-(cid:96)1

for (cid:96)2-(cid:96)1
for (cid:96)2-(cid:96)2 .

(18)

2.2 The problem and optimality criterion

Throughout, we consider the bilinear saddle point problem

min
x∈X

max
y∈Y

f (x, y), where f (x, y) := y(cid:62)Ax + b(cid:62)x − c(cid:62)y, for A ∈ Rm×n, b ∈ Rn and c ∈ Rm.

(19)

We will always assume that every row and column of A has at least one nonzero entry (else removing
said row or column does not aﬀect the problem value), so that the number of nonzeros nnz is at
least m + n − 1. To simplify the exposition of the (cid:96)1-(cid:96)1 and (cid:96)2-(cid:96)1 setups we will assume b = 0n and
c = 0m as is standard in the literature. Adding linear terms to these setups is fairly straightforward
and does not aﬀect the complexity (up to logarithmic factors) of our designed algorithms using data
structures designed in this paper (speciﬁcally ApproxExpMaintainer in Section 2.4); see Section 6
for an example. The gradient mapping associated with (19) for z = (zx, zy) ∈ Z = X × Y is

g(z) := (∇xf (z), −∇yf (z)) = (A(cid:62)zy + b, −Azx + c).

(20)

The mapping g is continuous and monotone, where we call g monotone if and only if

(cid:10)g(z(cid:48)) − g(z), z(cid:48) − z(cid:11) ≥ 0, ∀z, z(cid:48) ∈ Z.

14

This holds due to the convexity-concavity (indeed, bilinearity) of function f . Our goal is to de-
sign randomized algorithms for ﬁnding an (expected) (cid:15)-accurate saddle point z ∈ Z such that, in
expectation,

(cid:20)

(cid:21)

E Gap(z) := E

max
y(cid:48)∈Y

f (zx, y(cid:48)) − min
x(cid:48)∈X

f (x(cid:48), zy)

≤ (cid:15).

(21)

In order to do so, we aim to ﬁnd a sequence z1, z2, . . . , zK with (expected) low average regret,
(cid:111)

i.e., such that E maxu∈Z

(cid:110) 1
K

(cid:80)K

k=1 (cid:104)g(zk), zk − u(cid:105)

≤ (cid:15). Due to bilinearity of f we have

E Gap

(cid:32)

(cid:33)

1
K

K
(cid:88)

k=1

zk

= E max
u∈Z

(cid:40)

1
K

K
(cid:88)

k=1

(cid:41)

(cid:104)g(zk), zk − u(cid:105)

≤ (cid:15).

Finally, we make the explicit assumption that whenever we are discussing an algorithm in this
paper with a simplex domain (e.g. in (cid:96)1-(cid:96)1 or (cid:96)2-(cid:96)1 case), the quantity Lco/(cid:15) is bounded by (m + n)3,
as otherwise we are in the high-accuracy regime where the runtimes of interior point methods or
cutting-plane methods [24, 18] are favorable. Speciﬁcally for (cid:96)1-(cid:96)1 matrix games in this regime,
interior-point methods [23, 12, 45] are always faster, see footnote in Section 1.1. We make this
assumption for notational convenience when discussing logarithmic factors depending on multiple
quantities, such as m, n, Lco, and (cid:15)−1.

2.3 Matrix access models

We design randomized algorithms which require accessing and sampling from the matrix A ∈ Rn×m
in a variety of ways. Here, we list these operations, where we assume each takes constant time.
Speciﬁc algorithms only require access to a subset of this list; we make a note of each algorithm’s
requirements when presenting it.

A1. For i, j ∈ [m] × [n], return Aij.

A2. For i ∈ [m] and p ∈ {1, 2}, draw j ∈ [n] with probability |Aij|p/ (cid:107)Ai:(cid:107)p
p

.

A3. For j ∈ [n] and p ∈ {0, 1, 2}, draw i ∈ [m] with probability |Aij|p/ (cid:107)A:j(cid:107)p
p

.

A4. For i ∈ [m] (j ∈ [n]) and p ∈ {1, 2}, return (cid:107)Ai:(cid:107)p
A5. For p ∈ {1, 2}, return maxi∈[m] (cid:107)Ai:(cid:107)p

, maxj∈[n] (cid:107)A:j(cid:107)p

((cid:107)A:j(cid:107)p

).

, nnz, rcs, and (cid:107)A(cid:107)F

.

Given any representation of the matrix as a list of nonzero entries and their indices, we can always
implement the access modes above (in the assumed constant time) with O(nnz) time preprocessing;
see e.g. Vose [47] for an implementation of the sampling (in a unit cost RAM model). Our variance-
reduced algorithms have an additive O(nnz) term appearing in their runtimes due to the need to
compute at least one matrix-vector product to implement gradient estimators. Thus, their stated
runtime bounds hold independently of matrix access assumptions.

2.4 Data structure interfaces

We rely on data structures to maintain and sample from the iterates of our algorithms. Below, we
give a summary of the operations supported by our data structures and their runtime guarantees.
We show how to implement these data structures in Section 5.

15

2.4.1

IterateMaintainerp

Given p ∈ {1, 2}, we design a data structure IterateMaintainerp which maintains an implicit
representation of the current iterate x ∈ Rn and a running sum s of all iterates. At initialization,
this data structure takes as input the initial iterate x0 to be maintained and for p = 2, the data
structure also takes as input a ﬁxed vector v. It then supports the following operations.

Runtime

Category Function
initialize

O(n)

Init(x0, v): x ← x0, s ← 0
Scale(c): x ← cx
AddSparse(j, c): [x]j ← [x]j + c (if p = 1, we require c ≥ −[x]j) O(log n)†
AddDense(c): x ← x + cv (supported if p = 2)
UpdateSum(): s ← s + x
Get(j): Return [x]j
GetSum(j): Return [s]j
GetNorm(): Return (cid:107)x(cid:107)p
Sample(): Return j with probability [x]p

O(1)
O(1)
O(1)

O(1)
O(1)

O(log n)

O(1)

j / (cid:107)x(cid:107)p

p

update

query

sample†

† An alternative implementation does not support Sample, but performs AddSparse in time O(1).

The implementation of IterateMaintainerp is given in Section 5.1. In Sections C.2 and D.3 we

use variants of this data structure WeightedIterateMaintainer2
and defer the detailed discussions of their implementations to Appendix G.

and CenteredIterateMaintainer2,

2.4.2

ApproxExpMaintainer

To maintain multiplicative weights updates with a ﬁxed dense component, we design a data structure
ApproxExpMaintainer initialized with an arbitrary point x0 ∈ ∆n, a direction v ∈ Rn, a decay
constant κ ∈ [0, 1] and an approximation error parameter ε. In order to specify the implementation
of our data structure, we require the following deﬁnition.

Deﬁnition 2 (β-padding). For x, x(cid:48) ∈ ∆n, we say x(cid:48) is a β-padding of x if x(cid:48) = ˜x/ (cid:107)˜x(cid:107)1, for a
point ˜x ∈ Rn

≥0 with ˜x ≥ x entrywise and (cid:107)˜x − x(cid:107)1 ≤ β.

Notions similar to β-padding appear in previous literature [e.g., 21]. A key technical property of
β-paddings is that they do not increase entropy signiﬁcantly (see Lemma 5).

ApproxExpMaintainer has maintains two vectors x, ˆx ∈ ∆n that, for an error tolerance param-

eter ε, satisfy the invariant

ˆx is a ε-padding of x.

(22)

an error tolerance parameter ε We now specify the interface, where ◦ denotes elementwise product,
to lie in the simplex,
[xκ]j = [x]κ
j
denotes the number of nonzeroes in vector s. To state our runtimes, we deﬁne
and (cid:107)s(cid:107)0

denotes elementwise power, Π∆(z) = z/ (cid:107)z(cid:107)1

normalizes z ∈ Rn
≥0

ω := max

(cid:18) 1

1 − κ

(cid:19)

.

n
λε

,

For most of our applications of ApproxExpMaintainer, ω is a polynomial in m and n (our iterate
dimensions), so log(ω) = O(log(mn)) (with the exception of our maximum inscribed ball application,

16

Runtime

Category Function
initialize

Init(x0, v, κ, ε, λ) : κ ∈ [0, 1), ε > 0, minj[x0]j ≥ λ O(n log n log2 ω)
MultSparse(g): x ← ε-padding of Π∆(x ◦ exp(g)) O((cid:107)g(cid:107)0 log2 n log2 ω)
DenseStep(): x ← Π∆(xκ ◦ exp(v))
UpdateSum(): s ← s + ˆx (recall invariant (22))
Get(j): Return [ˆx]j
GetSum(j): Return [s]j
Sample(): Return j with probability [ˆx]j

O(log n log ω)
O(log2 ω)
O(log n log ω)

O(log n log ω)

O(log n)

update

query

sample

where our runtimes additionally depend polylogarithmically on the size of the hyperplane shifts b;
see Remark 2). We defer a more ﬁne-grained runtime discussion to Section 5.3.

The role of ApproxExpMaintainer is in to eﬃciently implement the regularized and reduced-
variance stochastic mirror descent steps of the form (5). To do this, we initialize the data structure
with v = (1 − κ) log x0 − ηκgx
. Then, the iteration (5) consists of calling DenseStep() followed by
0
MultSparse(−ηκ˜δx).

3 Framework

In this section, we develop our algorithmic frameworks. The resulting algorithms have either sub-
linear or variance-reduced complexities. We develop our sublinear coordinate method framework in
Section 3.1, and its variance-reduced counterpart in Section 3.2.

3.1 Sublinear coordinate methods

In Section 3.1.1 we introduce the concept of a local gradient estimator, which allow stronger guar-
antees for stochastic mirror descent with clipping (Algorithm 1) via local norms analysis. Then, in
Section 3.1.2 we state the form of the speciﬁc local gradient estimators we use in our coordinate
methods, and motivate the values of Lco in Table 1.

3.1.1 Convergence analysis

Deﬁnition 3. For local norm setup (Z, (cid:107)·(cid:107)·, r, Θ, clip) , we call a stochastic gradient estimator
˜g : Z → Z ∗ an L-local estimator if it satisﬁes the following properties for all z ∈ Z:

1. Unbiasedness: E [˜g(z)] = g(z).

2. Second moment bound: for all w ∈ Z, E [(cid:107)˜g(z)(cid:107)2

w] ≤ L2.

The following lemma shows that L-local estimators are unbiased for L-bounded operators.

Lemma 1. A gradient mapping that admits an L-local estimator satisﬁes (cid:107)g(z)(cid:107)∗ ≤ L for all z ∈ Z.
Proof. For every z ∈ Z, the function (cid:107)·(cid:107)2
z

is convex. Thus by Jensen’s inequality,

(cid:107)g(z)(cid:107)2

z = (cid:107)E ˜g(z)(cid:107)2

z ≤ E (cid:107)˜g(z)(cid:107)2

z ≤ L2.

Taking supremum over z ∈ Z gives (cid:107)g(z)(cid:107)2

∗ ≤ L2.

17

We note that the same result does not hold for ˜g because maximum and expectation do not commute.
That is, E (cid:107)˜g(cid:107)2
∗

is not bounded by L2. This fact motivates our use of local norms analysis.

Below, we state Algorithm 1, stochastic mirror descent with clipping, and a guarantee on its rate
of convergence using local gradient estimators. We defer the proof to Appendix B and note here
that it uses the “ghost iterates” technique due to Nemirovski et al. [29] in order to rigorously bound
the expected regret with respect to the best response to our iterates, rather than a pre-speciﬁed
point. This technique is purely analytical and does not aﬀect the algorithm. We also note that
the second inequality in Proposition 2 holds with any convex-concave function f , similarly to [8,
Corollary 1]; the ﬁrst uses bilinearity of our problem structure.

Algorithm 1: Stochastic mirror descent

Input: Matrix A ∈ Rm×n, L-local gradient estimator ˜g, clipping function clip(·)
Output: A point with O( Θ
Parameters: Step-size η, number of iterations T

ηT + ηL2) expected duality gap

1 z0 ← arg minz∈Z r(z)
2 for t = 1, . . . , T do
zt ← arg minz∈Z
3
t=0 zt

4 return 1
T +1

(cid:80)T

(cid:8)(cid:104)clip(η˜g(zt−1)), z(cid:105) + Vzt−1(z)(cid:9)

Proposition 2. Let (Z, (cid:107)·(cid:107)·, r, Θ, clip) be a local norm setup, let L, (cid:15) > 0, and let ˜g be an L-local
estimator. Then, for η ≤ (cid:15)

, Algorithm 1 outputs a point ¯z such that

9L2 and T ≥ 6Θ

η(cid:15) ≥ 54L2Θ

(cid:15)2

E Gap(¯z) ≤ E

(cid:34)

1
T + 1

sup
u∈Z

T
(cid:88)

t=0

(cid:35)

(cid:104)g(zt), zt − u(cid:105)

≤ (cid:15).

3.1.2 Coordinate gradient estimators

We now state the general form which our local gradient estimators ˜g take. At a point z ∈ Z, for
speciﬁed sampling distributions p(z), q(z), sample ix, jx ∼ p(z) and iy, jy ∼ q(z). Then, deﬁne

˜g(z) :=

(cid:18) Aixjx[zy]ix
pixjx(z)

ejx,

−Aiyjy[zx]jy
qiyjy(z)

(cid:19)

eiy

+ g(0) where g(0) = (b, c).

(23)

It is clear that regardless of the distributions p(z), q(z), for the gradient operator in (20), ˜g(z) is an
unbiased gradient estimator (E [˜g(z)] = g(z)) and ˜g(z) − g(0) is 2-sparse.

In the remainder of this section we assume for simplicity the g(0) = 0
Optimal values of Lco.
(i.e. the objective f in (19) has not linear terms). Here we compute the optimal values of L for
local gradient estimators (see Deﬁnition 3) of the form (23) for each of the local norm setups we
consider. This motivates the values of Lco we derive in the following sections. First, in the (cid:96)1-(cid:96)1
case, the second moment of (cid:107)˜gx(z)(cid:107)2

wx (the local norm of the X block of ˜g(z) at point w) is

(cid:34)
[wx]jx

E

(cid:18) Aixjx[zy]ix
pixjx(z)

(cid:19)2(cid:35)

=

(cid:88)

i∈[m],j∈[n]

A2

i [wx]j

ij[zy]2
pij(z)



≥



(cid:88)

|Aij|[zy]i

(cid:113)

[wx]j


2



.

i∈[m],j∈[n]

18

√

√

wx ∈ Bn with (cid:13)
(cid:13)

wx(cid:13)
. Similarly, the best possible bound on the Y is maxj (cid:107)A:j(cid:107)2
2

Since zy ∈ ∆m and
maxi (cid:107)Ai:(cid:107)2
2
setup, no local estimator has parameter L smaller than Lco in Table 1.
Next, in the (cid:96)2-(cid:96)2 case, the ((cid:96)2) second moment of the X block is

(cid:13)2 = 1, the above lower bound is in the worst case
. Therefore, in the (cid:96)1-(cid:96)1

E

(cid:34)(cid:18) Aixjx[zy]ix
pixjx(z)

(cid:19)2(cid:35)

=

A2
ij[zy]2
i
pij(z)



≥



(cid:88)

i∈[m],j∈[n]


2





2

(cid:88)

|Aij|[zy]i



=



i∈[m],j∈[n]

(cid:88)

i∈[m]

(cid:107)Ai:(cid:107)1 [zy]i



.

In the worst case, this is at least ((cid:80)
Y block is ((cid:80)

j∈[n] (cid:107)A:j(cid:107)1)2, which means that Lco is similarly unimprovable in the (cid:96)2-(cid:96)2 setup.

Finally, in the (cid:96)2-(cid:96)1 case, where X = Bn and Y = ∆m, we again have that the (cid:96)2 second moment

i∈[m] (cid:107)Ai:(cid:107)1)2; similarly, the best second moment bound for the

of the X (ball) block is at least

E

(cid:34)(cid:18) Aixjx[zy]ix
pixjx(z)

(cid:19)2(cid:35)



≥





2

(cid:88)

|Aij|[zy]i



.

i∈[m],j∈[n]

Here, since zy ∈ ∆m, the worst-case lower bound of the variance is maxi (cid:107)Ai:(cid:107)2
1
norm (at w) second moment of the Y (simplex) block is at least

. Further, the local

(cid:34)
[wy]iy

E

(cid:18) Aiyjy[zx]jy
qiyjy(z)

(cid:19)2(cid:35)



≥



(cid:88)

|Aij|[zx]j

(cid:112)[wy]i



.


2

i∈[m],j∈[n]

√

wy ∈ Bm, in the worst case this second moment can be as high as (cid:107)|A|(cid:107)op

Since zx ∈ Bn and
, where
we use |A| to denote the elementwise absolute value of A. This is better than the Lco in Table 1,
suggesting there is room for improvement here. However, the sampling probabilities inducing this
optimal variance bound are of the form

qij(z; w) ∝ |Aij|(cid:112)[wy]i · [zx]j,

and it unclear how to eﬃciently sample from this distribution. Improving our (cid:96)2-(cid:96)1 gradient esti-
mator (or proving that no improvement is possible) remains an open problem.

3.2 Variance-reduced coordinate methods

In this section, we develop the algorithmic framework we use in our variance-reduced methods. We
ﬁrst deﬁne a type of “centered-local” gradient estimator, modifying the local gradient estimators of
the previous section. We then give the general form of a variance-reduced method and analyze it in
the context of our gradient estimators and the error incurred by our data structure maintenance.

3.2.1 General convergence result

Deﬁnition 4. For local norm setup (Z, (cid:107)·(cid:107)·, r, Θ, clip), and given a reference point w0 = (wx
0),
we call a stochastic gradient estimator ˜gw0 : Z → Z ∗ an L-centered-local estimator if it satisﬁes the
following properties:

0, wy

1. Unbiasedness: E [˜gw0(z)] = g(z).
2. Relative variance bound: for all w ∈ Z, E [(cid:107)˜gw0(z) − g(w0)(cid:107)2

w] ≤ L2Vw0(z).

19

Remark 1. Similarly to Lemma 1, a gradient mapping that admits an L-centered-local estimator
also satisﬁes (cid:107)g(z) − g(w0)(cid:107)2

∗ ≤ L2Vw0(z), by Jensen’s inequality.

Algorithm 2 below is an approximate variant of the variance reduction algorithm in our earlier
work [8] which closely builds upon the “conceptual prox-method” of Nemirovski [28]. The algorithm
repeatedly calls a stochastic oracle O : Z → Z to produce intermediate iterates, and then performs
an extragradient (linearized) proximal step using the intermediate iterate. The main modiﬁcation
compared to [8] is Line 5, which accommodates slight perturbations to the extra-gradient step
results. These perturbations arise due to input requirements of our data structures: we slightly pad
coordinates in simplex blocks to ensure they are bounded away from zero.

Algorithm 2: OuterLoop(O) (conceptual prox-method [28])

Input: Target approximation quality εouter, (α, εinner)-relaxed proximal oracle O(z) for

gradient mapping g and some εinner < εouter, distance-generating r

Parameters: Number of iterations K.
Output: Point ¯zK with E Gap(¯z) ≤ αΘ

K + εouter

1 z0 ← arg minz∈Z r(z)
2 for k = 1, . . . , K do
zk−1/2 ← O(zk−1)
3
k := Proxα
z(cid:63)
(g(zk−1/2)) = arg minz∈Z
zk ← any point satisfying Vzk (u) − Vz(cid:63)
k=1 zk−1/2

5
6 return ¯zK = 1
K

(cid:80)K

zk−1

4

k

(cid:46) We implement O(zk−1) by calling InnerLoop(zk−1, ˜gzk−1, α)

(cid:8)(cid:10)g (cid:0)zk−1/2
(u) ≤ εouter−εinner

(cid:1) , z(cid:11) + αVzk−1(z)(cid:9)
, for all u ∈ Z

α

The following deﬁnition summarizes the key property of the oracle O.

Deﬁnition 5 ([8, Deﬁnition 1]). Let operator g be monotone and α, εinner > 0. An (α, εinner)-relaxed
proximal oracle for g is a (possibly randomized) map O : Z → Z such that z(cid:48) = O(z) satisﬁes

(cid:20)

E

max
u∈Z

(cid:8) (cid:10)g(z(cid:48)), z(cid:48) − u(cid:11) − αVz(u)(cid:9)

(cid:21)

≤ εinner.

The following proposition, a variant of [8, Proposition 1], shows that despite the error permitted
tolerated in Line 5, the algorithm still converges with rate 1/K. We defer its proof to Appendix B.

Proposition 3. Let O be an (α, εinner)-relaxed proximal oracle with respect to gradient mapping g,
distance-generating function r with range at most Θ and some εinner ≤ εouter. Let z1/2, z3/2, . . . , zK−1/2
be iterates of Algorithm 2 and let ¯zK be its output. Then

E Gap(¯zK) ≤ E max
u∈Z

1
K

K
(cid:88)

k=1

(cid:10)g(zk−1/2), zk−1/2 − u(cid:11) ≤

αΘ
K

+ εouter.

Algorithm 3 is a variant of the variance-reduced inner loop of [8], adapted for local norms and
inexact iterates (again, due to approximations made by the data structure). It tolerates error in
three places:

1. Instead of estimating the gradient at the previous iterate wt−1, we estimate it at a point ˆwt−1
such that wt−1 − ˆwt−1 has small norm and similar divergence from the reference point w0 (Line 2).

2. Instead of letting the next iterate be the exact mirror descent step w(cid:63)
t

, we let be a point wT that
in norm and has similar divergences to from w0 and to any any point in Z (Line 4).

is close to w(cid:63)
t

20

3. The output ˜w can be an approximation of the average of the iterates, as long as its diﬀerence to

the true average has bounded norm (Line 5).

We quantify the eﬀect of these approximations in Proposition 4, which gives a runtime guarantee
, see (18)). The proof

for Algorithm 3 (where we recall the deﬁnition of (cid:107)·(cid:107) as the dual norm of (cid:107)·(cid:107)∗
is deferred to Appendix B.

Algorithm 3: InnerLoop(w0, ˜gw0, ϕ)

Input: Initial w0 ∈ Z, L-centered-local gradient estimator ˜gw0
Parameters: Step size η, number of iterations T , approximation tolerance ϕ
Output: Point ˜w satisfying Deﬁnition 5

, oracle quality α > 0

1 for t = 1, . . . , T do

2

3

4

(cid:8)(cid:104)w, clip(η˜gw0( ˆwt−1) − ηg(w0)) + ηg(w0)(cid:105) + αη

t ← arg minw∈Z
satisfying

ˆwt−1 ≈ wt−1 satisfying (a) Vw0( ˆwt−1) − Vw0(wt−1) ≤ ϕ
α
w(cid:63)
wt ≈ w(cid:63)
t
(a) maxu
(b) Vw0(wt) − Vw0(w(cid:63)
(c) (cid:107)wt − w(cid:63)
(cid:80)T

(u)(cid:3) ≤ ηϕ,
, and

t (cid:107) ≤ ϕ
2LD
t=1 wt satisfying

(cid:2)Vwt(u) − Vw(cid:63)

t ) ≤ ϕ
α

t=1 wt

(cid:13)
(cid:13) ˜w − 1
(cid:13)

(cid:13)
(cid:13) ≤ ϕ
(cid:13)

(cid:80)T

LD

T

t

.

5 return ˜w ≈ 1
T

and (b) (cid:107) ˆwt−1 − wt−1(cid:107) ≤ ϕ
LD
2 Vw0(w) + Vwt−1(w)(cid:9)

Proposition 4. Let (Z, (cid:107)·(cid:107)·, r, Θ, clip) be any local norm setup. Let w0 ∈ Z, α ≥ εinner > 0,
and ˜gw0 be an L-centered-local estimator for some L ≥ α. Assume the domain is bounded by
maxz∈Z (cid:107)z(cid:107) ≤ D, that g is L-Lipschitz, i.e. (cid:107)g(z) − g(z(cid:48))(cid:107)∗ ≤ L (cid:107)z − z(cid:48)(cid:107), that g is LD-bounded,
α2 , and ϕ = εinner
i.e. maxz∈Z (cid:107)g(z)(cid:107)∗ ≤ LD, and that ˆw0 = w0. Then, for η = α
,
Algorithm 3 outputs a point ˆw ∈ Z such that

10L2 , T ≥ 6

ηα ≥ 60L2

6

E max
u∈Z

[(cid:104)g( ˜w), ˜w − u(cid:105) − αVw0(u)] ≤ εinner,

(24)

i.e. Algorithm 3 is an (α, εinner)-relaxed proximal oracle.
Remark 2 (Assumption of boundedness on g). The assumption that g is LD-bounded in the dual
norm is immediate from other assumptions used in Proposition 4 in the case of the applications
in Section 4, where we develop methods for solving (cid:96)1-(cid:96)1 matrix games and assume that g(0) = 0.
In applications in Section 6, due to the existence of extra linear terms b, c (cid:54)= 0, all complexity
bounds will have an additional dependence on log((cid:107)[b; c](cid:107)∗) which we pay in the implementation of
data structure ApproxExpMaintainer (i.e. the parameter L in the bound on g is larger if (cid:107)[b; c](cid:107)∗
is large). We hide this extra polylogarithmic factor in the (cid:101)O notation.

We also remark that (up to constants) the bounds on the range of εinner ≤ α ≤ L in the statement
of Proposition 4 correspond to the cases where the inner and outer loop consist of a single iteration.

3.2.2 Variance-reduced coordinate gradient estimators

We now state the general form which our centered-local estimators ˜gw0
take, given a reference
point w0 ∈ Z. At a point z, for sampling distributions p(z; w0), q(z; w0) to be speciﬁed, sample
ix, jx ∼ p(z; w0) and iy, jy ∼ q(z; w0). Then, deﬁne

˜gw0(z) =

(cid:18) Aixjx[zy − wy
pixjx(z; w0)

0]ix

ejx,

−Aiyjy[zx − wx
qiyjy(z; w0)

0]jy

(cid:19)

eiy

+ g(w0).

(25)

21

It is clear that regardless of the distributions p(z; w0), q(z; w0), this is an unbiased gradient

estimator (E [˜gw0(z)] = g(z)). Furthermore, ˜gw0(z) − g(w0) is always 2-sparse.

4 Matrix games

In this section we instantiate the algorithmic framework of Section 3 in (cid:96)1-(cid:96)1 setup without linear
terms, i.e. b = c = 0 in the objective (19). This is the fundamental “matrix game” problem

min
x∈∆m

max
y∈∆n

y(cid:62)Ax.

We give two algorithms for approximately solving matrix games. In Section 4.1 we develop a
co /(cid:15))2(cid:17)
stochastic coordinate method based on Algorithm 1 with potentially sublinear runtime (cid:101)O
.
In Section C we develop a coordinate variance-reduction based on Algorithm 2 with runtime
that improves on the former runtime whenever it is Ω(nnz). In both cases

nnz · L1,1

(L1,1

nnz +

√

(cid:16)

(cid:17)

(cid:16)

co /(cid:15)

(cid:101)O
we have

(cid:26)

L1,1

co := max

max
i

(cid:107)Ai:(cid:107)2 , max

j

(cid:107)A:j(cid:107)2

(cid:27)

(26)

as in Table 1.

Instantiations for the (cid:96)2-(cid:96)1 and (cid:96)2-(cid:96)2 setups follow similarly. We carry them out in Appendices C

(for stochastic coordinate methods) and D (for variance reduction methods).

Remark 3. For simplicity in this section (and the remaining implementations in Appendices C, D),
we will set g(0) = 0 whenever the setup is not (cid:96)2-(cid:96)2, as is standard in the literature. We defer a
discussion of how to incorporate arbitrary linear terms in simplex domains to Section 6; up to
additional logarithmic terms in the runtime, this extension is supported by ApproxExpMaintainer.

Assumptions. Throughout (for both Sections 4.1 and 4.2), we assume access to entry queries, (cid:96)2
norms of rows and columns, and (cid:96)2 sampling distributions for all rows and columns. We use the
(cid:96)1-(cid:96)1 local norm setup (Table 6). We also deﬁne Lmax := (cid:107)A(cid:107)max = maxi∈[m],j∈[n] |Aij|.

4.1 (cid:96)1-(cid:96)1 sublinear coordinate method

4.1.1 Gradient estimator

For z ∈ ∆n × ∆m and desired accuracy (cid:15) > 0, we specify the sampling distributions p(z), q(z):

pij(z) := [zy]i

A2
ij
(cid:107)Ai:(cid:107)2
2

and qij(z) := [zx]j

A2
ij
(cid:107)A:j(cid:107)2
2

.

(27)

We ﬁrst state and prove the local properties of this estimator.

Lemma 2. In the (cid:96)1-(cid:96)1 setup, estimator (23) using the sampling distribution in (27) is a
√

2L1,1

co -local estimator.

Proof. Unbiasedness holds by deﬁnition. For arbitrary wx, we have the variance bound:
(cid:32)

(cid:104)

E

(cid:105)

(cid:107)˜gx(z)(cid:107)2
wx

(cid:88)

pij(z) ·

[wx]j ·

(cid:19)2(cid:33)

(cid:18) Aij[zy]i
pij(z)

(cid:88)

=

[wx]j

A2
ij[zy]2
i
pij(z)

(cid:107)Ai:(cid:107)2

2 ≤ (Lco

i∈[m],j∈[n]
1,1)2.

≤

≤

i∈[m],j∈[n]
(cid:88)

i∈[m],j∈[n]

[wx]j[zy]i (cid:107)Ai:(cid:107)2

2 ≤ max
i∈[m]

22

Similarly, we have

(cid:104)

E

(cid:105)

(cid:107)˜gy(z)(cid:107)2
wy

≤ (Lco

1,1)2.

The deﬁnition (cid:107)˜g(z)(cid:107)2

w = (cid:107)˜gx(z)(cid:107)2

wx + (cid:107)˜gy(z)(cid:107)2

wy yields the claimed variance bound.

4.1.2

Implementation details

In this section, we discuss the details of how to leverage the IterateMaintainer1 data structure
to implement the iterations of our algorithm. The algorithm we analyze is Algorithm 1, using the
local estimator deﬁned in (23), and the distribution (27). We choose

η =

(cid:15)
L1,1
co

(cid:16)

(cid:17)2

18

and T =

(cid:25)

(cid:24) 6Θ
η(cid:15)

≥

(cid:17)2

(cid:16)

L1,1
co

108

log(mn)

(cid:15)2

.

Lemma 2 implies that our estimator satisﬁes the remaining requirements for Proposition 2, giving
the duality gap guarantee in T iterations. In order to give a runtime bound, we claim that each
iteration can be implemented in log(mn) time, with O(m + n) additional runtime.

Data structure initializations and invariants. At the start of the algorithm, we spend O(m+
1, IMy
n) time initializing data structures via IMx
1
are appropriate instantiations of IterateMaintainer1 data structures. Throughout, we preserve
1, IMy
correspond to the x and y blocks of the current
the invariant that the points maintained by IMx
iterate zt at iteration t of the algorithm.

m 1m, 0m), where IMx

n 1n, 0n) and IMy

1.Init( 1

1.Init( 1

1

Iterations. For simplicity, we only discuss the runtime of updating the x block as the y block
follows symmetrically. We divide each iteration into the following substeps, each of which we show
runs in time O(log mn). We refer to the current iterate by z = (zx, zy), and the next iterate by
w = (wx, wy).

Sampling. Recall that

pij(z) := [zy]i

A2
ij
(cid:107)Ai:(cid:107)2
2

.

We ﬁrst sample coordinate i via IMy
ity proportional to A2
ij
matrix access model.

1.Sample() in O(log m). Next, we sample j ∈ [n] with probabil-
using the data structure corresponding to Ai: in O(1) by assumption of the

Computing the gradient estimator. To compute c := clip(Aij[zy]i/pij), it suﬃces to compute Aij,
[zy]i, and pij. Using an entry oracle for A we obtain Aij, and we get [zy]i by calling IMy
1.Get(i).
and the values of Aij, [zy]i therefore takes O(1) time.
Computing pij using the precomputed (cid:107)Ai:(cid:107)2

Performing the update. For the update corresponding to a proximal step, we have

wx ← ΠX (zx ◦ exp(−η˜gx(z))) =

zx ◦ exp(−η˜gx(z))
(cid:107)zx ◦ exp(−η˜gx(z))(cid:107)1

.

23

We have computed ˜gx(z), so to perform this update, we call

1.Get(j);

ξ ← IMx
IMx
IMx
IMx

1.AddSparse(j, (exp(−ηc) − 1)ξ);
1.Scale(IterateMaintainerx.GetNorm()−1);
1.UpdateSum().

By assumption, each operation takes time O(log n), giving the desired iteration complexity. It is
clear that at the end of performing these operations, the invariant that IMx
maintains the x block
1
of the iterate is preserved.

Averaging. After T iterations, we compute the average point ¯zx:

[¯zx]j ←

1
T

· IMx

1.GetSum(j), ∀j ∈ [n].

By assumption, this takes O(n) time.

4.1.3 Algorithm guarantee

Theorem 1. In the (cid:96)1-(cid:96)1 setup, the implementation in Section 4.1.2 has runtime



(cid:16)

O




(cid:17)2

L1,1
co

log2(mn)

(cid:15)2




 ,

+ m + n

and outputs a point ¯z ∈ Z such that E Gap(¯z) ≤ (cid:15)..

Proof. The runtime follows from the discussion in Section 4.1.2. The correctness follows from
Proposition 2.

Remark 4. Using our IterateMaintainer1 data structure, the (cid:96)1-(cid:96)1 algorithm of Grigoriadis
and Khachiyan [16] runs in time O(rcs (cid:107)A(cid:107)2
max log2(mn)/(cid:15)2), where rcs is the maximum number of
.
nonzeros in any row or column. Our runtime universally improves upon it since (Lco
max

1,1)2 ≤ rcs(cid:107)A(cid:107)2

4.2 (cid:96)1-(cid:96)1 variance-reduced coordinate method

4.2.1 Gradient estimator

Given reference point w0 ∈ ∆n × ∆m, for z ∈ ∆n × ∆m and a parameter α > 0, we specify the
sampling distributions p(z; w0), q(z; w0):

pij(z; w0) :=

[zy]i + 2[wy
0]i
3

·

A2
ij
(cid:107)Ai:(cid:107)2
2

and qij(z; w0)

:=

[zx]j + 2[wx
3

0]j

·

A2
ij
(cid:107)A:j(cid:107)2
2

.

(28)

We remark that this choice of sampling distribution, which we term “sampling from the sum” (of
the current iterate and reference point), may be viewed as a computationally-eﬃcient alternative to
the distribution speciﬁed in [8], which was based on “sampling from the diﬀerence”. In particular,
sampling from the diﬀerence is an operation which to the best of our knowledge is diﬃcult to
implement in sublinear time, so we believe that demonstrating that this alternative distribution
suﬃces may be of independent interest.
In order to show its correctness, we need the following
claim, whose proof we defer to Appendix D.1.

24

Lemma 3. For y, y(cid:48) ∈ ∆m, divergence Vy(y(cid:48)) generated by r(y) = (cid:80)

i∈[m][y]i log[y]i − [y]i satisﬁes

Vy(y(cid:48)) ≥

1
2

(cid:13)y(cid:48) − y(cid:13)
(cid:13)
2
(cid:13)

3
2y+y(cid:48)

=

1
2

(cid:88)

i∈[m]

([y]i − [y(cid:48)]i)2
2
3 [y]i + 1
3 [y(cid:48)]i

.

We now show the local properties of this estimator.

Lemma 4. In the (cid:96)1-(cid:96)1 setup, estimator (25) using the sampling distribution in (28) is a
√

2L1,1

co -centered-local estimator.

Proof. Unbiasedness holds by deﬁnition. For arbitrary wx, we have the variance bound:

E

(cid:104)(cid:13)
(cid:13)˜gx

w0(z) − gx(w0)(cid:13)
2
(cid:13)
wx

(cid:105)

=

(cid:88)

pij(z; w0) ·


[wx]j ·

(cid:32)

Aij

(cid:1)

(cid:0)[zy]i − [wy
0]i
pij(z; w0)

(cid:33)2


i∈[m],j∈[n]

(cid:88)

A2
ij

[wx]j

=

≤

i∈[m],j∈[n]

(cid:88)

[wx]j

i∈[m],j∈[n]

(cid:18)

≤ 2

max
i

(cid:107)Ai:(cid:107)2
2

(cid:1)2

(cid:0)[zy]i − [wy
0]i
pij(z; w0)

(cid:107)Ai:(cid:107)2
2

([zy]i − [wy
3 [zy]i + 2
1
(cid:19)

0]i)2
3 [wy
0]i

(zy),

Vwy

0

where in the last inequality we used Lemma 3. Similarly, we have for arbitrary wy,

E

(cid:104)(cid:13)
(cid:13)˜gy

w0(z) − gy(w0)(cid:13)
2
(cid:13)
wy

(cid:105)

≤ 2

(cid:18)

max
j

(cid:19)

(cid:107)A:j(cid:107)2
2

Vwx

0

(zx).

Combining these and using

(cid:107)˜gw0(z) − g(w0)(cid:107)2

w := (cid:13)

(cid:13)˜gx

wx + (cid:13)
w0(z) − gx(w0)(cid:13)
2
(cid:13)˜gy
(cid:13)

w0(z) − gy(w0)(cid:13)
2
(cid:13)
wy

yields the desired variance bound.

4.2.2

Implementation details

In this section, we discuss the details of how to leverage the ApproxExpMaintainer data structure
to implement the iterations of our algorithm. We ﬁrst state one technical lemma on the eﬀect
of β-padding (Deﬁnition 2) on increasing entropy, used in conjunction with the requirements of
Proposition 4 to bound the error tolerance required by our ApproxExpMaintainer data structure.
The proof is deferred to Appendix D.1.

Lemma 5. Let x(cid:48) ∈ ∆n be a β-padding of x ∈ ∆n. Then,

j log x(cid:48)
x(cid:48)

j −

(cid:88)

j∈[n]

(cid:88)

j∈[n]

xj log xj ≤

βn
e

+ β(1 + β).

This leads to the following divergence bounds which will be used in this section.

25

Lemma 6. Let x(cid:48) ∈ ∆n be a β-padding of x ∈ ∆n. Then

Vx(cid:48)(u) − Vx(u) ≤ β, ∀u ∈ Z

and if (cid:107)log(x0)(cid:107)∞ ≤ M , then

Vx0(x(cid:48)) − Vx0(x) ≤ β

(cid:16)

2M +

n
e

(cid:17)

+ 1 + β

Proof. Throughout this proof, let ˜x be the point in Deﬁnition 2 such that (cid:107)˜x − x(cid:107)1 ≤ β and
x(cid:48) = ˜x/ (cid:107)˜x(cid:107)1

.

The ﬁrst claim follows from expanding

Vx(cid:48)(u) − Vx(u) =

(cid:88)

j∈[n]

uj log

xj
x(cid:48)
j

(cid:88)

=

uj log

j∈[n]

(cid:18) xj
˜xj

(cid:19)

· (cid:107)˜x(cid:107)1

≤ log((cid:107)˜x(cid:107)1) ≤ β.

The ﬁrst inequality used u ∈ ∆n and ˜x ≥ x entrywise, and the last inequality used log(1 + β) ≤ β.

For the second claim, we have by the triangle inequality

(cid:13)x − x(cid:48)(cid:13)
(cid:13)

(cid:13)1 ≤ (cid:107)x − ˜x(cid:107)1 + (cid:13)

(cid:13)˜x − x(cid:48)(cid:13)

(cid:13)1 ≤ β + ((cid:107)˜x(cid:107)1 − 1) (cid:13)

(cid:13)x(cid:48)(cid:13)

(cid:13)1 ≤ 2β.

The claim then follows from expanding

Vx0(x(cid:48)) − Vx0(x) =

j log x(cid:48)
x(cid:48)

j −

(cid:88)

j∈[n]

(cid:88)

j∈[n]

xj log xj + (cid:10)log x0, x − x(cid:48)(cid:11) ,

and applying Lemma 5.

The algorithm we analyze is Algorithm 2 with K = 3αΘ/(cid:15), εouter = 2(cid:15)/3, εinner = (cid:15)/3 using
Algorithm 3 as an (α, εinner)-relaxed proximal oracle. The speciﬁc modiﬁcation we perform to
k} uses the following
deﬁne the iterates {zk} of Algorithm 2 as modiﬁcations of the ideal iterates {z(cid:63)
deﬁnition.

Deﬁnition 6. For a simplex variable x(cid:48) ∈ ∆n, we deﬁne truncate(x(cid:48), δ) to be the point x ∈ ∆n with
xj ∝ max(x(cid:48)
j, δ) for all j ∈ [n]. For a variable z on two blocks, we overload notation and deﬁne
truncate(z, δ) to be the result of applying truncate(·, δ) to each simplex block of z.

For our implementation, in each step of Algorithm 2, we will compute the point z(cid:63)
k

exactly,
. We now quantify the eﬀect of

k, δ), for δ = εouter−εinner
and apply the operation zk ← truncate(z(cid:63)
α(m+n)
truncation in terms of Bregman divergence to an arbitrary point.

Lemma 7 (Eﬀect of truncation). Let x(cid:48) ∈ ∆n, and let x = truncate(x(cid:48), δ). Then, for any u ∈ ∆n,
and where divergences are with respect to entropy,

Vx(u) − Vx(cid:48)(u) ≤ δn.

Proof. Note that x is a δn-padding of x(cid:48), as it is the result of adding at most δ to each coordinate
and renormalizing to lie in the simplex. Consequently, the result follows from Lemma 6.

Lemma 7 thus implies our iterates satisfy the requirements of Algorithm 2. Our implementation

of Algorithm 3 will use approximation tolerance ϕ = εinner/6 = (cid:15)/18, where we always set

L1,1

co ≥ α ≥ εinner.

26

(29)

This matches the requirements of Proposition 4. In the implementation of Algorithm 3, we use the
centered-local gradient estimator deﬁned in (25), using the sampling distribution (28). For each use
of Algorithm 3, we choose

η =

α
L1,1
co

(cid:16)

(cid:17)2

20

and T =

(cid:25)

(cid:24) 6
ηα

≥

(cid:17)2

(cid:16)

L1,1
co

120

α2

.

Our discussion will follow in four steps: ﬁrst, we discuss the complexity of all executions in
Algorithm 2 other than calls to the oracles. Next, we discuss the complexity of all initializations of
ApproxExpMaintainer data structures. Then, we discuss the complexity of all other iterations of
Algorithm 3. For simplicity, when discussing Algorithm 3, we will only discuss implementation of
the x-block, and the y-block will follow symmetrically, while most runtimes are given considering
both blocks. Lastly, we discuss complexity of computing the average iterate in the end of the inner
loop. Altogether, the guarantees of Proposition 3 and Proposition 4 imply that if the guarantees
required by the algorithm hold, the expected gap of the output is bounded by (cid:15).

Outer loop extragradient steps. Overall, we execute K = 3αΘ/(cid:15) iterations of Algorithm 2,
with εouter = 2(cid:15)/3, εinner = (cid:15)/3 to obtain the desired gap, where Θ = log(mn) in the (cid:96)1-(cid:96)1 setup. We
,
spend O(nnz) time executing each extragradient step in Algorithm 2 exactly to compute iterates z(cid:63)
k
where the dominant term in the runtime is computing each g(zk−1/2), for k ∈ [K]. We can maintain
the average point ¯z throughout the duration of the algorithm, in O(m + n) time per iteration.
Finally, we spend an additional O(m + n) time per iteration applying truncate to each iterate z(cid:63)
k

.

Data structure initializations and invariants. We consider the initialization of data structures
for implementing an (α, εinner = (cid:15)/3)-relaxed proximal oracle with error tolerance ϕ = (cid:15)/18. First,
used in the initialization of every inner loop, by the guarantees of truncate
note that the point wx
0
operation, has no two coordinates with multiplicative ratio larger than δ = (cid:15)/(3α(m + n)) ≥
(m + n)−4, by our choice α ≤ L1,1
co /(cid:15) (cf. Section 2.2). Since
clearly a simplex variable in ∆n has a coordinate at least 1/n, the entries of w0 are lower bounded
by λ = (m + n)−5.

co (29) and our assumptions on L1,1

Next, we discuss the initial parameters given to AEMx, an instance of ApproxExpMaintainer
which will support necessary operations for maintaining the x variable (we will similarly initialize
an instance AEMy). Speciﬁcally, the invariant that we maintain throughout the inner loop is that in
iteration t, the“exact vector” x maintained by AEMx corresponds to the x block of the current iterate
wt, and the “approximate vector” ˆx maintained corresponds to the x block of the approximate iterate
ˆwt, as deﬁned in Algorithm 3.

We will now choose ˜ε so that if AEMx is initialized with error tolerance ˜ε, all requirements of
Proposition 4 (e.g. the bounds stipulated in Algorithm 3) are met. We ﬁrst handle all divergence
, xt and ˆxt respectively,
requirements. In a given iteration, denote the x blocks of w(cid:63)
t
. The former of these
, and ˆxt is a ˜ε-padding of x(cid:63)
and recall AEMx guarantees xt is a ˜ε-padding of x(cid:63)
t
t
guarantees is true by the speciﬁcation of MultSparse (which will be used in the implementation of
the step, see “Performing the update” below), and the latter is true by the invariant on the points
supported by AEMx. Lines 2 and 4 of Algorithm 3 stipulate the divergence requirements, where

, wt and ˆwt by x(cid:63)
t

27

x0 := wx
0

,

max {Vx0(xt) − Vx0 (x(cid:63)

t ) , Vx0 (ˆxt) − Vx0 (xt)} ≤

and

max
u

(cid:2)Vxt(u) − Vx(cid:63)

t

(u)(cid:3) ≤

ϕ
2α

ηϕ
2

=

(cid:15)
36α

=

η(cid:15)
36

.

(30)

(31)

Clearly, combining this guarantee with a similar guarantee on the y blocks yields the desired bound.
Since we derived (cid:107)log w0(cid:107)∞ ≤ 5 log(mn), we claim that choosing

˜ε ≤

(cid:15)
36α(m + n)

suﬃces for the guarantees in (30). By the ﬁrst part of Lemma 6, for all suﬃciently large m + n,

max {Vx0(xt) − Vx0 (x(cid:63)

t ) , Vx0 (ˆxt) − Vx0 (xt)} ≤ ˜ε

(cid:16)

10 log(mn) +

(cid:17)
+ 1 + ˜ε

≤

n
e

(cid:15)
36α

.

Similarly for guarantees in (31), by the second part of Lemma 6 we know it suﬃces to choose

˜ε ≤

(cid:15)2
(cid:16)
L1,1
co

(cid:17)2 ≤

(cid:15)α
(cid:16)
L1,1
co

(cid:17)2 =

η(cid:15)
36

.

720

720

Here, we used the restriction εinner ≤ α ≤ L1,1
guarantees in Lines 2, 4, and 5) imply we require

co . Next, the norm requirements of Algorithm 3 (the

where we used that g is (cid:107)A(cid:107)max ≤ L1,1
our assumptions on the size of parameters in Section 2.2, it suﬃces to set the error tolerance

co -Lipschitz and the diameter of Z is bounded by

2. Using

√

˜ε ≤

(cid:15)
√
2L1,1
co

18

,

˜ε = (m + n)−8.

To give the remainder of speciﬁed parameters, AEMx is initialized via Init(wx

0, v, κ, ˜ε) for

κ :=

1
1 + ηα/2

, v := (1 − κ) log wx

0 − ηκgx(w0).

To motivate this form of updates, note that each iteration of Algorithm 3 requires us to compute

(cid:26)

arg min

α
2
We can see that the solution to this update is given by
(cid:3)x ← Π∆ ([wx

t ]κ ◦ exp ((1 − κ) log wx

(cid:104)ctej + gx(w0), x(cid:105) +

(cid:2)w(cid:63)

t+1

(x) +

(cid:27)

Vwx

t

(x)

.

1
η

Vwx

0

0 − ηκgx(w0)) ◦ exp(−ηκctej)) .

(32)

This form of update is precisely supported by our choice of κ and v, as well as the DenseStep and
MultSparse operations. By the choice of parameters, we note that 1 − κ ≥ (m + n)−8.

Finally, in order to support our sampling distributions and gradient computations, we compute
and store the vectors w0 and g(w0) in full using O(nnz(A)) time at the beginning of the inner loop.
In O(m + n) time, we also build two data structures which allow us to sample from entries of the
given ﬁxed vectors wx
0

, in constant time respectively.

, and wy
0

Following Section 2.4.2, we deﬁned the parameter
(cid:18) 1

(cid:19)

ω := max

,

n
λ˜(cid:15)

1 − κ

≤ (m + n)13, so that log(ω) = O(log(mn)).

Altogether, these initializations take time O(nnz + (m + n) log3(mn)), following Section 2.4.2.

28

Inner loop iterations. We discuss how to make appropriate modiﬁcations to the x-block. For
simplicity we denote our current iterate as z, and the next iterate as w. Also, we denote ˆz as
the concatenation of implicit iterates that the two ApproxExpMaintainer copies maintain (see
Section 2.4.2 for more details), which is ˜ε close in (cid:96)1 distance to z, the prior iterate, so that we can
query or sample entries from ˆz using AEMx and AEMy. Each inner loop iteration consists of using
a gradient estimator at ˆz satisfying (cid:107)ˆz − z(cid:107)1 ≤ ˜ε, sampling indices for the computation of ˜gw0(ˆz),
computing the sparse part of ˜gw0(ˆz), and performing the approximate update to the iterate. We
show that we can run each substep using data structure AEMx in time O(log4(mn)), within the
error tolerance of Proposition 4 due to the deﬁnition of ˜ε. Combining with our discussion of the
complexity of initialization, this implies that the total complexity of the inner loop, other than
outputting the average iterate, is

O(T log4(mn) + nnz + (m + n) log3(mn)) = O



(cid:16)




(cid:17)2

L1,1
co

· log4(mn)

α2



+ nnz + (m + n) log3(mn)


 .

Sampling. Recall that the distribution we sample from is given by

pij(ˆz; w0) :=

[ˆzy]i + 2[wy
0]i
3

·

A2
ij
(cid:107)Ai:(cid:107)2
2

.

First, with probability 2/3, we sample a coordinate i from the precomputed data structure for
sampling from wy
in constant time; otherwise, we sample i via AEMy.Sample(). Then, we sample
0
an entry of Ai: proportional to its square via the precomputed data structure (cf. Section 2.3) in
constant time. This takes in total O(log mn) time.

Computing the gradient estimator. Proposition 4 requires us to compute the sparse component
0]j and
of the gradient estimator (25) at point ˆz. To do this for the x block, we ﬁrst query [wx
[ˆzy]i ← AEMy.Get(i), and then access the precomputed norm (cid:107)Ai:(cid:107)2
and entry Aij. We then compute
(cid:32)

(cid:33)

c = clip

Aij

(cid:2)ˆzy − wy
0

(cid:3)
i ·

3
[ˆzy]i + 2[wy
0]i

·

(cid:107)Ai:(cid:107)2
2
A2
ij

.

By the guarantees of ApproxExpMaintainer, this takes total time bounded by O(log(mn)).

Performing the update. To perform the update, by observing the form of steps in Algorithm 3 with
our choice of entropy regularizer, the update form given by the regularized mirror-descent step is
(as derived in the discussion of the initialization of AEMx, see (32))

[w(cid:63)x ← Π∆((wx)κ ◦ exp((1 − κ) log wx

0 − ηκgx(w0) − ηκcej)).

To implement this, recalling our choice of the vector v in the initialization of AEMx, it suﬃces to call

AEMx.DenseStep();
AEMx.MultSparse(−ηκcej);
AEMx.UpdateSum().

By assumption, each operation takes time bounded by O(log4(mn)), where we note the vector used
in the MultSparse operation is 1-sparse. The implementation of this update is correct up to a
˜ε-padding, whose error we handled previously. By the discussion in the data structure initialization
section, this preserves the invariant that the x block of the current iterate is maintained by AEMx.

29

Average iterate computation. At the end of each run of Algorithm 3, we compute and return
the average iterate via calls AEMx.GetSum(j) for each j ∈ [n], and scaling by 1/T , and similarly query
AEMy. The overall complexity of this step is O((m + n) log2(mn)). The correctness guarantee, i.e.
that the output approximates the average in (cid:96)1 norm up to ϕ/LD, is given by the choice of ˜ε and
the guarantees of ApproxExpMaintainer, where in this case the domain size D is bounded by
2.
This is never the dominant factor in the runtime, as it is dominated by the cost of initializations.

√

4.2.3 Algorithm guarantee

Theorem 2. In the (cid:96)1-(cid:96)1 setup, let nnz(cid:48)
Section 4.2.2 with the optimal choice of α = max

(cid:16)

:= nnz + (m + n) log3(mn). The implementation in
√

(cid:15)/3, L1,1

co log2 (mn) /

nnz(cid:48)(cid:17)

has runtime







nnz(cid:48) +


(cid:17)2

(cid:16)

L1,1
co

log4(mn)

α2








α log(mn)
(cid:15)


 = O

√

(cid:32)

nnz(cid:48) +

co log3(mn)

nnz(cid:48)L1,1
(cid:15)

(cid:33)

O

and outputs a point ¯z ∈ Z such that

E Gap(¯z) ≤ (cid:15).

Proof. The correctness of the algorithm is given by the discussion in Section 4.2.2 and the guarantees
of Proposition 3 with K = 3αΘ/(cid:15), εouter = 2(cid:15)/3, εinner = (cid:15)/3, Proposition 4 with ϕ = (cid:15)/15, and the
data structure ApproxExpMaintainer with our choice of

˜ε := (m + n)−8,

to meet the approximation conditions in Line 2, 4 and 5 of Algorithm 3. The runtime bound is
given by the discussion in Section 4.2.2, and the optimal choice of α is clear.

5 Data structure implementation

In this section, we give implementations of our data structures, fulﬁlling the interface and runtime
guarantees of Section 2.4. In Section 5.1 we provide the implementation of IterateMaintainerp for
p ∈ {1, 2} used for sublinear coordinate methods. In Section 5.2, we provide an implementation of
ApproxExpMaintainer used in variance-reduced coordinate methods for simplex domains, provided
we have an implementation of a simpler data structure, ScaleMaintainer, which we then provide
in Section 5.3.

5.1 IterateMaintainerp
The IterateMaintainerp, p ∈ {1, 2} data structure is described in Section 2.4.1 and used for
tracking the iterates in our fully stochastic methods and the Euclidean part of our the iterates in our
variance-reduced methods. The data structure maintains an internal representation of x, the current
iterate, and s, a running sum of all iterates. The main idea behind the eﬃcient implementation of
the data structure is to maintain x and s as a linear combination of sparsely-updated vectors. In
particular, the data structure has the following state: scalars ξu, ξv, σu, σv, ι, ν; vectors u, u(cid:48), v,
and the scalar (cid:107)v(cid:107)2
; the vector v is only relevant for variance reduction and is therefore set 0 for
2
the non-Euclidean case p = 1.

We maintain the following invariants on the data structure state at the end of every operation:

30

• x = ξuu + ξvv, the internal representation of x

• s = u(cid:48) + σuu + σvv, the internal representation of running sum s

• ι = (cid:104)x, v(cid:105), the inner product of the iterate with ﬁxed vector v

, the appropriate norm of the iterate

• ν = (cid:107)x(cid:107)p
In addition, to support sampling, our data structure also maintains a binary tree distx of depth
O(log n). Each leaf node is associated with a coordinate j ∈ [n], and each internal node is associated
with a subset of coordinates corresponding to leaves in its subtree. For the node corresponding to
S ⊆ [n] (where S may be a singleton), we maintain the sums (cid:80)
j∈S[v]p
.
j
We now give the implementation of each operation supported by IterateMaintainerd, followed

j∈S[u]j[v]j, and (cid:80)

j∈S[u]p

, (cid:80)

j

by proofs of correctness and of the runtime bounds when applicable.

5.1.1

Initialization

• Init(x0, v). Runs in time O(n).

If p = 1 set v ← 0n; otherwise we compute and store (cid:107)v(cid:107)2
. Initialize the remaining data struc-
2
ture state as follows: (ξu, ξv, u) ← (1, 0, x0), (σu, σv, u(cid:48)) ← (0, 0, 0n), (ι, ν) ← ((cid:104)x0, v(cid:105) , (cid:107)x0(cid:107)p).
Initialize distx, storing the relevant sums in each internal node.

It is clear that x = ξuu + ξvv, s = u(cid:48) + σuu + σvv, and that the invariants of ι, ν hold. Each
step takes O(n) time; for the ﬁrst 4 steps this is immediate, and the ﬁnal recursing upwards from
the leaves spends constant time for each internal node, where there are O(n) nodes.

5.1.2 Updates

• Scale(c): x ← cx. Runs in time O(1).

Multiply each of ξu, ξv, ν, ι by c.

• AddSparse(j, c): [x]j ← [x]j + c, with the guarantee c ≥ −[x]j if p = 1. Runs in time O(log n).

ej.

1. u ← u + c
ej.
ξu
2. u(cid:48) ← u(cid:48) − cσu
ξu
3. If p = 1, ν ← ν + c. If p = 2, ν ← (cid:112)ν2 + 2c[ξuu + ξvv]j + c2.
4. ι ← ι + c[v]j.
5. For internal nodes of distx on the path from leaf j to the root, update (cid:80)

j∈S[u]p

j

,

(cid:80)

j∈S[u]j[v]j appropriately.

• AddDense(c): x ← x + cv. Runs in time O(1). (Supported only for p = 2).

Set ξv ← ξv + c, ν ←

(cid:113)

ν2 + 2cι + c2 (cid:107)v(cid:107)2
2

, and ι ← ι + c (cid:107)v(cid:107)2
2

.

• UpdateSum(): s ← s + x. Runs in time O(1).

Set σu ← σu + ξu and σv ← σv + ξv.

31

Each of the runtime bounds clearly hold; we now demonstrate that the necessary invariants are
preserved. Correctness of Scale and UpdateSum are clear. Regarding correctness of AddSparse,
note that (ignoring the v terms when p = 1)

(cid:18)

ξu

u +

(cid:18)

u(cid:48) −

cσu
ξu

(cid:19)

(cid:18)

ej

+ σu

u +

(cid:19)

ej

+ ξvv = ξuu + ξvv + cej,

(cid:19)

ej

+ σvv = u(cid:48) + σuu + σvv.

c
ξu
c
ξu

When p = 1, the update to ν is clearly correct. When p = 2, because only [x]j changes,

[ξuu + ξvv + cej]2

j = [ξuu + ξvv]2

j + 2c[ξuu + ξvv]j + c2,

([ξuu + ξvv + cej]j) · [v]j = ([ξuu + ξvv]j) · [v]j + c[v]j.

Thus, the updates to the norm and inner product are correct. Regarding correctness of AddDense
when p = 2, we have

ξuu + (ξv + c)v = ξuu + ξvv + cv,

2 = ν2 + 2cι + c2 (cid:107)v(cid:107)2
2 ,

(cid:107)x + cv(cid:107)2
(cid:104)x + cv, v(cid:105) = ι + c (cid:107)v(cid:107)2
2 .

Here, we use the invariants that ν = (cid:107)x(cid:107)2

and ι = (cid:104)x, v(cid:105).

5.1.3 Queries

• Get(j): Return [x]j. Runs in time O(1).

Return ξu[u]j + ξv[v]j.

• GetSum(j): Return [s]j. Runs in time O(1).

Return [u(cid:48)]j + σu[u]j + σv[v]j.

• Norm(): Return (cid:107)x(cid:107)p

. Runs in time O(1).

Return ν.

By our invariants, each of these operations is correct.

5.1.4 Sampling
The method Sample returns a coordinate j with probability proportional to [x]p
in time O(log n).
j
To implement it, we recursively perform the following procedure, where the recursion depth is at
most O(log n), starting at the root node and setting S = [n]:

1. Let S1, S2 be the subsets of coordinates corresponding to the children of the current node.
2. Using scalars ξu, ξv, and the maintained (cid:80)
[ξuu + ξvv]p
j

[u]p
j∈Si
j
for i ∈ {1, 2}.

[u]j[v]j, (cid:80)

compute (cid:80)

j = (cid:80)

when appropriate,

[v]p
j

[x]p

, (cid:80)

j∈Si

j∈Si

j∈Si

j∈Si

3. Sample a child i ∈ {1, 2} of the current node proportional to (cid:80)

[x]p
j

by ﬂipping an appro-

j∈Si

priately biased coin. Set S ← Si.

It is clear that this procedure samples according to the correct probabilities. Furthermore, step
2 can be implemented in O(1) time using precomputed values, so the overall complexity is O(log n).

32

5.2 ApproxExpMaintainer

In this section, we give the implementation of ApproxExpMaintainer which supports dense update
to simplex mirror descent iterates. For convenience, we restate its interface, where we recall the
notation (cid:107)g(cid:107)0

for the number of nonzero entries in g, Deﬁnition 2 of an ε-padding, the invariant

and the notation

ˆx is a ε-padding of x,

ω := max

(cid:18) 1

1 − κ

(cid:19)

.

n
λ(cid:15)

,

(33)

Runtime

Category Function
initialize

Init(x0, v, κ, ε, λ) : κ ∈ [0, 1), ε > 0, minj[x0]j ≥ λ O(n log n log2 ω)
MultSparse(g): x ← ε-padding of Π∆(x ◦ exp(g)) O((cid:107)g(cid:107)0 log2 n log2 ω)
DenseStep(): x ← Π∆(xκ ◦ exp(v))
UpdateSum(): s ← s + ˆx (recall invariant (22))
Get(j): Return [ˆx]j
GetSum(j): Return [s]j
Sample(): Return j with probability [ˆx]j

O(log n log ω)
O(log2 ω)
O(log n log ω)

O(log n log ω)

O(log n)

update

query

sample

We build ApproxExpMaintainer out of a simpler data structure called ScaleMaintainer, which
maintains the simplex projection of ﬁxed vectors raised elementwise to arbitrary powers; this suﬃces
to support consecutive DenseStep calls without MultSparse calls between them. To add support
for MultSparse, we combine O(log n) instances of ScaleMaintainer in a formation resembling
a binomial heap: for every entry updated by MultSparse we delete it from the ScaleMaintainer
instance currently holding it, put it in a new singleton ScaleMaintainer instance (after appropriate
scaling due to MultSparse), and merge this singleton into existing instances. We now give a brief
description of the ScaleMaintainer interface, and based on it, describe the implementation of
ApproxExpMaintainer. We will provide the implementation of ScaleMaintainer in Section 5.3.

ScaleMaintainer is initialized with vectors ¯x ∈ Rn(cid:48)
≥0

eﬃcient approximate queries on vectors of the form

and ¯δ ∈ Rn(cid:48) (with n(cid:48) ≤ n) and supports

x[σ] := ¯x ◦ exp (cid:0)σ¯δ(cid:1) ,
for any scalar σ ∈ [σmin, 1]. More speciﬁcally, the data structure allows eﬃcient computation of
(to within small multiplicative error εscm), as well as entry queries, sampling and running
(cid:107)x[σ](cid:107)1
sum accumulation from a vector ˆx[σ] satisfying

x[σ]
(cid:107)x[σ](cid:107)1
We make the following assumptions on the input to the data structure:

ˆx[σ] is a εscm-padding of Π∆

(cid:0)¯x ◦ exp (cid:0)σ¯δ(cid:1)(cid:1) =

.

(34)

λscm ≤ [¯x]i ≤ 1 for all i ∈ [n(cid:48)] and σ ∈ (σmin, 1).
The upper bounds on ¯x and σ are arbitrary, and we may choose λscm and σmin to be very small since
the data structure runtime depends on them only logarithmically. To summarize this dependence,
we deﬁne

ωscm := max

(cid:26) 1
σmin

,

n
λscmεscm

(cid:27)

.

With these assumptions and notation, we deﬁne the formal interface of ScaleMaintainer.

33

Category Function
initialize

Runtime
O(n(cid:48) log n log2 ωscm)
O(1)

Init(¯x, ¯δ, σmin, εscm, λscm)
Del(j): Remove coordinate j from ¯x, ¯δ
UpdateSum(γ, σ): s ← s + γ ˆx[σ], with ˆx[σ] deﬁned in (34) O(log ωscm)
Get(j): Return [ˆx[σ]]j
O(log ωscm)
GetSum(j): Return [s]j.
O(log2 ωscm)
GetNorm(σ): Return 1 ± ε approx. of (cid:13)
O(log ωscm)
Sample(σ): Return j with probability [ˆx[σ]]j
O(log n log ωscm)

(cid:13)¯x ◦ exp(σ¯δ)(cid:13)
(cid:13)1

update

query

sample

5.2.1

ApproxExpMaintainer state

Throughout this section, we denote K := (cid:100)log n(cid:101). ApproxExpMaintainer maintains a partition of
[n] into K sets S1 . . . , SK (some of them possibly empty) that satisfy the invariant

|Sk| ≤ 2k for all k ∈ [K].

(35)

We refer to the index k as “rank” and associate with each rank k ∈ [K] the following data

1. Scalar γk ≥ 0 and nonnegative integer τk.

2. Vectors ¯xk, ¯δk ∈ R|Sk| such that λscm ≤ [¯xk]i ≤ 1 for all i ∈ [|Sk|], where λscm = min(ε/n, λ).

3. A ScaleMaintainer instance, denoted ScaleMaintainerk, initialized with ¯xk, ¯δk and λscm

deﬁned above, σmin = 1 − κ and εscm = ε/10, so that log ωscm = O(log ω).

ApproxExpMaintainer also maintains a vector u ∈ Rn for auxiliary running sum storage.

Deﬁne the vector δ ∈ Rn by

[δ]Sk

= log (cid:0)γk

(cid:2)¯xk ◦ exp (cid:0)(1 − κτk )¯δk

(cid:1)(cid:3)(cid:1) , k ∈ {1, . . . , K},

(36)

where [δ]Sk
denotes the coordinates of δ in Sk. Recall that x denotes the point in ∆n maintained
throughout the operations of ApproxExpMaintainer; we maintain the key invariant that the point
x is proportional to exp(δ), i.e.,

x =

exp(δ)
(cid:107)exp(δ)(cid:107)1

.

(37)

Speciﬁcally, we show in Section 5.2.3 that our implementation of DenseStep modiﬁes ¯x, ¯δ, {τk}K
{γk}K

so that the resulting eﬀect on δ, per deﬁnition (37), is

k=0

k=0

,

δ ← κδ + v.

(38)

Similarly, our implementation of MultSparse modiﬁes the state so that the resulting eﬀect on δ is

exp(δ)
(cid:107)exp(δ)(cid:107)1

← ε-padding of

exp(δ + v)
(cid:107)exp(δ + v)(cid:107)1

.

(39)

We remark that the role of γk is to scale ¯xk so that it lies coordinatewise in the range [λscm, 1],
conforming to the ScaleMaintainer input requirement. This is also the reason we require the
ε-padding operation in the deﬁnition of MultSparse.

34

5.2.2

ε-padding point ˆx

We now concretely deﬁne the point ˆx, which is the ε-padding of x that ApproxExpMaintainer
maintains. Let

Γ :=

K
(cid:88)

k=1

γkScaleMaintainerk.GetNorm(1 − κτk ),

(40)

be the approximation of exp(δ) derived from the ScaleMaintainer instances. For any j ∈ [n], let
kj be such that j ∈ Skj

, and let ij be the index of j in Skj

. The jth coordinate of ˆx is

[ˆx]j :=

γkj ScaleMaintainerkj .GetNorm(1 − κτkj )
Γ

· ScaleMaintainerkj .Get(ij, 1 − κτkj ).

(41)

Since for each k, (cid:80)
ScaleMaintainerk.Get(j, 1 − κτk ) = ScaleMaintainerk.GetNorm(1 − κτk )
we have that ˆx ∈ ∆n. We now prove that ˆx is a ε-padding of x. To do so, we prove the following
lemma.

j∈Sk

Lemma 8. Let εscm ≤ 1
an εscm-padding of xk ∈ ∆|Sk|. Further, suppose we have positive scalars {νk}K

k=1 be a partition of [n]. Suppose for each k ∈ [K], ˆxk ∈ ∆|Sk| is
k=1 satisfying

10 and {Sk}K

k=1, {ˆνk}K

(1 − εscm)νk ≤ ˆνk ≤ (1 + εscm)νk, for all 1 ≤ k ≤ K.

Then, for N = (cid:80)K
x := (cid:80)K

νk
N xk.

k=1

k=1 νk and ˆN = (cid:80)K

k=1 ˆνk, we have that ˆx := (cid:80)K

k=1

ˆνk
ˆN

ˆxk is a 10εscm-padding of

Proof. For every k ∈ [K], let ˜xk to be such that ˜xk ≥ xk elementwise, ˆxk = ˜xk/ (cid:107)˜xk(cid:107)1
(cid:107)˜xk − xk(cid:107)1 ≤ εscm. Consider the point

, and

˜x :=

K
(cid:88)

k=1

˜νk
ˆN

˜xk, where ˜νk := ˆνk

maxk∈[K] (cid:107)˜xk(cid:107)1
(cid:107)˜xk(cid:107)1

·

1 + εscm
1 − εscm

,

. Since ˜xk ≥ xk elementwise, ˆνk ≥ (1 − εscm)νk and ˆN ≤ (1 + εscm)N , we have
so that ˆx = ˜x/ (cid:107)˜x(cid:107)1
that ˜x ≥ x elementwise. Furthermore, we have ˆνk ≤ (1 + εscm)νk and ˆN ≥ (1 − εscm)N , and the
properties ˜xk ≥ xk and (cid:107)˜xk − xk(cid:107)1
≤ 1 + εscm.
Therefore

imply 1 ≤ (cid:107)˜xk(cid:107)1 ≤ 1 + εscm as well as maxk∈[K](cid:107)˜xk(cid:107)1

(cid:107)˜xk(cid:107)1

(cid:107)˜x − x(cid:107)1 ≤

K
(cid:88)

k=1

νk
N

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(1 + εscm)3
(1 − εscm)2 ˜xk − xk

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

≤

(cid:18) (1 + εscm)3
(1 − εscm)2 − 1

(cid:19)

(1 + εscm) + εscm ≤ 10εscm,

where the ﬁnal bound is veriﬁed numerically for εscm ≤ 1/10.

The ScaleMaintainer interface guarantees that calls to ScaleMaintainerk.GetNorm return
(cid:13)¯xk ◦ exp((1 − κτk )¯δk)(cid:13)
(cid:13)
to within a 1 ± εscm multiplicative factor, and moreover that Get returns
(cid:13)1
entries from an εscm-padding of Π∆(¯xk ◦ exp((1 − κτk )¯δk)). Thus, applying Lemma 8 with our
deﬁnition of ˆx in (41) yields that ˆx is a 10εscm = ε-padding of x.

5.2.3

ApproxExpMaintainer initialization and updates

We give the implementation and prove runtimes of Init, MultSparse, DenseStep, and UpdateSum.

35

Init. Upon initialization of ApproxExpMaintainer, we set γK = maxj∈[n][x0]j and τk = 0 for
all k. We let SK = [n] (so that Sk = ∅ for all k < K) and instantiate a single instance of
ScaleMaintainer of rank K with parameters

¯xK =

x0
γK

, ¯δK =

v
1 − κ

− log x0, εscm =

ε
10

, λscm = min

(cid:17)

.

, λ

(cid:16) ε
n

It is clear that the invariant (37) holds at initialization, and that the coordinates of ¯xK lie in the
appropriate range, since we assume that x0 ∈ [λ, 1]n. We will use the same choices of εscm, λscm for
every ScaleMaintainer instance. The overall complexity of this operation is O(n log n log2 ω).

MultSparse. We state the implementation of MultSparse, prove that the resulting update is
(39), and ﬁnally give its runtime analysis. We perform MultSparse(g) in sequence for each nonzero
coordinate of g. Let j denote such nonzero coordinate and let kj be such that j ∈ Skj
; the operation
consists of the following steps.

1. Remove j from Skj

call to Del).

and delete the corresponding coordinate from ScaleMaintainerkj

(via a

2. Let S0 = j and initialize ScaleMaintainer0 with initial data ¯x and ¯δ described below.

3. For k going from 1 to K, set Sk ← Sk ∪ Sk−1 and Sk−1 = ∅, merging ScaleMaintainerk and
ScaleMaintainerk−1 as described below. If the new set Sk satisﬁes |Sk| ≤ 2k, break the loop;
else, proceed to the next k.

We now state the initial data given to each ScaleMaintainer upon initialization in the steps
above. Whenever a ScaleMaintainerk is created supporting Sk ⊆ [n], we ﬁrst compute δi for each
i ∈ Sk according to (37). When creating the singleton instance ScaleMaintainer0 we perform the
update

δj ← δj + gj;
this implements multiplication of the jth coordinate by exp(gj). To instantiate ScaleMaintainerk,
we set τk = 0, γk ← maxi∈Sk exp([δ]i) and modify δ according to

(42)

[δ]Sk ← max {[δ]Sk , log (λscm · γk)} .

(43)

In other words, we raise very small entries of [δ]Sk
of [exp(δ)]Sk

is in the range [λ−1

scm, λscm]. We then give ScaleMaintainerk the initial data

to ensure that the ratio between any two entries

¯xk =

1
γk

[exp (δ)]Sk

, ¯δk =

(cid:20) v
1 − κ

(cid:21)

− δ

.

Sk

(44)

It is clear that entries of ¯xk are in the range [λscm, 1], and invariant (37) holds at initialization, as
τk = 0. Therefore, the operation (42) implements x ← Π∆(x ◦ exp(g)) exactly; it remains to show
that the operation (43) amounts to an ε-padding of exp(δ)/ (cid:107)exp(δ)(cid:107)1
. We do so by invoking the
following lemma, substituting the values of δ before (42) for δ− and δ+ respectively, and λscm ≤ ε/n
for ρ.

Lemma 9. Let δ−, δ+ ∈ Rn satisfy

[δ+]i = max

(cid:26)

[δ−]i, max

j

(cid:27)

[δ−]j + log ρ

for all j ∈ [n] and ρ ≤ 1.

Then, exp(δ+)/ (cid:107)exp(δ+)(cid:107)1 is a ρn-padding of exp(δ−)/ (cid:107)exp(δ−)(cid:107)1.

36

, x(cid:48) = exp(δ+)/ (cid:107)exp(δ+)(cid:107)1

, and ˜x = exp(δ+)/ (cid:107)exp(δ−)(cid:107)1

.

Proof. Let x = exp(δ−)/ (cid:107)exp(δ−)(cid:107)1
Clearly x(cid:48) = ˜x/ (cid:107)˜x(cid:107)1

and ˜x ≥ x element-wise. Moreover, letting M = maxj exp([δ−]j), we have
(cid:107)exp(δ+) − exp(δ−)(cid:107)1
(cid:107)exp(δ−)(cid:107)1
establishing the ρn-padding property.

ρM |{i | [δ+]i (cid:54)= [δ−]i}|
(cid:107)exp(δ−)(cid:107)1

ρM · n
(cid:107)exp(δ−)(cid:107)1

(cid:107)˜x − x(cid:107)1 =

≤ ρn,

≤

≤

Finally, we discuss runtime. Recall that the cost of initializing a ScaleMaintainer with |S|
elements is O(|S| log n log2 ω). So, step 1 of our implementation of MultSparse, i.e., calling Del
once and initializing a rank-1 ScaleMaintainer per nonzero element, costs O((cid:107)g(cid:107)0 log n log2 ω). We
now discuss costs of merging in step 2. We show these merges cost an amoritized O(log2 n log2 ω)
per nonzero coordinate of g, leading to the claimed bound. Speciﬁcally, we show the cost of T
deletions and initializations due to nonzero entries of MultSparse arguments is O(T log2 n log2 ω).
Consider the number of times a rank-k set can be created through merges: we claim it is upper
bounded by O (cid:0)T /2k(cid:1). It follows that the overall complexity of step 2 is

(cid:32) K
(cid:88)

O

k=0

(cid:33)

2k T

2k log n log2 ω

= O(T log2 n log2 ω).

The claimed bound on the number of rank-k merges holds because at least 2k−1 deletions (and
hence that many MultSparse calls) must occur between consecutive rank-k merges. To see this,
for each k, maintain a potential Φk for the sum of cardinalities of all rank (cid:96) sets for (cid:96) < k. Each
deletion increases Φk by at most 1. For an insertion merge to create a rank-k set, Φk must have
been at least 2k−1 + 2; after the merge, it is 0, as in its creation, all rank-(cid:96) sets for (cid:96) < k must have
been merged. So, there must have been at least 2k−1 deletions in between merges.

DenseStep. To implement DenseStep we simply increment τk ← τk + 1 for all k; clearly, this
takes time O(log n). We now show that (37) is maintained, i.e., that the resulting update to the
variable δ under DenseStep is (38). Recall that ScaleMaintainerk is initialized according (44).
Clearly, (37) holds at initialization for the set Sk, as 1 − κ0 = 0. We now show that it continues to
when ScaleMaintainerk is
hold after any number of DenseStep calls. Let δ0 be the value of [δ]Sk
initialized, and let δτ be the value of [δ]Sk
after τ calls to DenseStep, each performing the update
x ← Π∆(xκ ◦ exp(v)). This is consistent with the update δτ +1 = κδτ + [v]Sk

, which requires

δτ = κτ δ0 +

τ −1
(cid:88)

τ (cid:48)=0

κτ (cid:48)

[v]Sk = κτ δ0 +

1 − κτ
1 − κ

[v]Sk = log(γk ¯xk) + (1 − κτ )¯δk,

where in the ﬁnal transition we substituted δ0 = log(γk ¯xk) and [v]Sk = (1 − κ)[¯δk + δ0] according
to (44). We see that the required of form of δτ is identical to its deﬁnition (36) and consequently
that (37) holds.

UpdateSum. We maintain the running sum s via the invariant

[s]Sk = [u]Sk + ScaleMaintainerk.GetSum(), ∀k ∈ [K],
which we preserve in two separate procedures. First, whenever UpdateSum() is called, we compute
the quantity Γ deﬁned in (40), and for each k ∈ [K] call

(45)

ScaleMaintainerk.UpdateSum

(cid:18) γkScaleMaintainerk.GetNorm(1 − κτk )
Γ

(cid:19)

.

37

It is straightforward to see that this indeed preserves the invariant (45) for our deﬁnition of ˆx in (41),
and takes time O(log n log ω). Next, whenever a coordinate is deleted from a ScaleMaintainerk
instance, or an entire ScaleMaintainerk instance is deleted due to a merge operation, we update

uj ← uj + ScaleMaintainerk.GetSum(j)

for every deleted coordinate j, or j involved in the merge, respectively. We charge the cost of this
operations to that of new ScaleMaintainer instance, which we accounted for in the analysis of
MultSparse.

5.2.4 Queries

Get(j). Recalling our deﬁnition of ˆx (41), we compute Γ in time O(log n log ω) by obtaining
GetNorm(1 − κτk ) for each k, and then call Get(j, 1 − κτk ) in time O(log ω) for the relevant k.

GetSum(j). Recalling our deﬁnition of s (45), we implement GetSum(j) in O(log ω) time via a single
call to GetSum on the relevant ScaleMaintainer instance, and querying a coordinate of u.

Sample. Recalling (41), we ﬁrst compute Γ, as well as all γkScaleMaintainerk.GetNorm(1−κτk ), in
O(log n log ω) time. We then sample an instance ScaleMaintainerk, for 0 ≤ k ≤ K, proportional
to the value γkScaleMaintainerk.GetNorm(1 − κτk ), in O(log n) time. Finally, for the sampled
instance, we call ScaleMaintainerk.Sample(1 − κτk ) to output a coordinate in O(log n log ω) time.
By the deﬁnition of ˆx used by ApproxExpMaintainer, as well as the deﬁnitions used by each
ScaleMaintainerk instance, it is clear this preserves the correct sampling probabilities.

5.3 ScaleMaintainer

Finally, we provide a self-contained treatment of ScaleMaintainer, the main building block in the
implementation of ApproxExpMaintainer described above.

5.3.1

Interface

For ease of reference we restate the interface of ScaleMaintainer, where for the sake of brevity
we drop the subscript scm from ε and λ, and use n rather than n(cid:48) to denote the input dimension.
Recall that for the vectors ¯x and ¯δ given at initialization, the data structure keeps track of vectors
of the form

ˆx[σ] := a ε-padding of Π∆

(cid:0)¯x ◦ exp (cid:0)σ¯δ(cid:1)(cid:1) ,

(46)

where σ is any scalar in the range {0} ∪ [σmin, 1].

The implementation of the data structure relies on three internal parameters: polynomial ap-
proximation order p ∈ N, truncation threshold R ≥ 0, and σ discretization level K ∈ N. To satisfy
the accuracy requirements we set these as

R = Θ(1) log

1
ελ

, p = Θ(1) log

1
ελ

, and K =

(cid:24)

log

(cid:25)

;

1
σmin

we give the runtime analysis in terms of these parameters.

5.3.2 Overview

We now outline our design of ScaleMaintainer, where the main challenge is supporting eﬃcient
GetNorm operations under no assumptions on the numerical range of the input ¯δ.

38

Category Function
initialize

Init(¯x, ¯δ, σmin, ε, λ): require ¯x ∈ [λ, 1]n
Del(j): Remove coordinate j from ¯x, ¯δ
UpdateSum(γ, σ): s ← s + γ ˆx[σ]
Get(j, σ): Return [ˆx[σ]]j
GetSum(j): Return [s]j.
GetNorm(σ): Return 1 ± ε approx. of (cid:13)
Sample(σ): Return j with probability [ˆx[σ]]j

update

query

sample

Runtime

O(npK log n)

O(1)

O(p)

O(p)

O(pK)

O(p log n)

(cid:13)¯x ◦ exp(σ¯δ)(cid:13)

(cid:13)1 O(p)

Exponential approximation via Taylor expansion. Our main strategy is to replace the ex-
ponential in the deﬁnition of ˆx[σ] with its Taylor expansion of order p = O(log n
ελ ), giving the
following approximation to the GetNorm(σ)

(cid:13)¯x ◦ exp (cid:0)σ¯δ(cid:1)(cid:13)
(cid:13)

(cid:13)1 ≈

(cid:42)

¯x,

p
(cid:88)

q=0

1
q!

(cid:43)

(σ¯δ)q

=

p
(cid:88)

q=0

σq
q!

(cid:10)¯x, ¯δq(cid:11) ,

where qth powers are applied to ¯δ elementwise. By pre-computing all the inner products {(cid:10)¯x, ¯δq(cid:11)}p
q=0
at initialization, we may evaluate this Taylor approximation of GetNorm in time O(p). The validity
of the approximation relies on the following well-known fact.

Fact 1 (Theorem 4.1 in [35]). Let ε(cid:48), R ≥ 0. A Taylor series fp(t) = (cid:80)p
O(R + log 1

ε(cid:48) ) satisﬁes

tq
q! of degree p =

q=0

| exp(t) − fp(t)| ≤ exp(t)ε(cid:48)

for all t ∈ [−R, 0].

Truncating small coordinates and σ discretization. For Fact 1 to directly imply the desired
approximation guarantee for GetNorm, the entries of σ¯δ must all lie in [−R, 0] for some R = (cid:101)O (1).
However, this will not hold in general, as our data structure must support any value of ¯δ. For a
ﬁxed value of σ, we can work instead with a shifted and truncated version of ¯δ, i.e.,

˜δ[σ, µ] := max{¯δ − µ, −R/σ},

where the oﬀset µ is roughly the maximum element of ¯δ. Fact 1 allows us to approximate the
exponential of σ˜δ[σ, µ], and for R = Θ(log( n
ελ )) we argue that the truncation of the smallest entries
of δ results in small multiplicative error. Unfortunately, the dependence of ˜δ[σ, µ] on σ would defeat
the purpose of eﬃcient computation, because it is impossible to precompute {(cid:10)¯x, ˜δ[σ, µ]q(cid:11)}p
for
every σ ∈ [σmin, 1]. To address this, we argue that truncation of the form ˜δ[ˆσ, µ] is accurate enough
for any σ ∈ [ˆσ/2, ˆσ]. Therefore, it suﬃces to to discretize [σmin, 1] into K = (cid:100)log 1
σmin

(cid:101) levels

q=0

ˆσk := 2k−1σmin
and precompute (cid:10)¯x, ˜δ[ˆσk, µ]q(cid:11) for every k ∈ [K] in q ≤ p. This allows us to compute GetNorm(σ) in
O(p) = (cid:101)O (1) time, with O(npK) = (cid:101)O (n) preprocessing time.

39

Supporting deletion via lazy oﬀset selection. Had the dataset not supported deletions, we
could have simply set µ to be the largest entry of ¯δ (independent of k). However, with deletions
the largest entry of ¯δ could change, potentially invalidating the truncation. To address this, we
maintain a diﬀerent threshold µk for every k ∈ [K], and argue that the approximation remains valid
if the invariant

¯δmax ≤ µk ≤ ¯δmax +

holds, where ¯δmax := maxj ¯δj. Writing

R
2ˆσk

for every k ∈ [K]

˜δ[k] := ˜δ[ˆσk, µk] = max

(cid:26)

¯δ − µk, −

(cid:27)

R
ˆσk

for every k ∈ [K],

(47)

(48)

the data structure only needs to maintain µk and (cid:10)¯x, ˜δ[k]q(cid:11) for every k ∈ [K] in q ≤ p.

from (cid:10)¯x, ˜δ[k]q(cid:11) for every q ≤ p.

When deleting coordinate j, for every k we test whether the invariant (47) remains valid.4 If
it does, we keep µk the same and implement deletion (for this value of k) in time O(p) = (cid:101)O (1)
by subtracting [¯x]j[˜δ[k]]q
If the invariant is no longer valid, we
j
reset µk to the new value of ¯δmax and recompute (cid:10)¯x, ˜δ[k]q(cid:11) for every q ≤ p. Note that the re-
computation time is proportional to the number of un-truncated coordinates in the newly deﬁned
˜δ[k]. The key observation here is that every re-computation decreases µk by at least R/(2ˆσk) and
so no element of ¯δ can remain un-truncated for more than two re-computation. Therefore, the cost
of recomputing inner products due to deletions, for the entire lifetime of the data structure, is at
most O(npK) = (cid:101)O (n), which we charge to the cost of initialization.

Explicit expression for ˆx[σ]. Following the preceding discussion, for any σ ≥ σmin we set

k(cid:63) =

(cid:24)

log2

(cid:25)

σ
σmin

,

so that σ ∈

(cid:20) ˆσk(cid:63)
2

(cid:21)

, ˆσk(cid:63)

,

and deﬁne

Z[σ] := eσµk(cid:63)

ˆx[σ] :=

eσµk(cid:63)
Z[σ]

p
(cid:88)

q=0
p
(cid:88)

q=0

σq
q!

σq
q!

¯x, ˜δ[k(cid:63)]q(cid:69)
(cid:68)

≈ (cid:13)

(cid:13)¯x ◦ exp (cid:0)σ¯δ(cid:1)(cid:13)
(cid:13)1

¯x ◦ ˜δ[k(cid:63)]q ≈ Π∆

(cid:0)¯x ◦ exp (cid:0)σ¯δ(cid:1)(cid:1) ,

(49)

(50)

with ˜δ as deﬁned in (48).

5.3.3 Correctness

We now prove that the approximation guarantees of ScaleMaintainer hold.

Proposition 5. There exist R = O(1) · log n
the invariant (47) holds we have that Z[σ] is an ε multiplicative approximation of (cid:13)
and ˆx[σ] is a ε-padding of Π∆
respectively.

ελ such that for all σ ∈ [σmin, 1], if
(cid:13)¯x ◦ exp (cid:0)σ¯δ(cid:1)(cid:13)
(cid:13)1
(cid:0)¯x ◦ exp (cid:0)σ¯δ(cid:1)(cid:1), with Z[σ] and ˆx[σ] deﬁned in Eq.s (49) and (50)

ελ and p = O(1) · log n

4 We can query the maximum entry of ¯δ under deletions in O(1) time via a standard data structure, e.g. a

doubly-linked list of the sorted entries of ¯δ.

40

Proof. To simplify notation, we write µ = µk(cid:63) and ˆσ = ˆσk(cid:63). We begin by noting that the inequali-
ties (47) and σ ≤ ˆσ imply that σ˜δi[k(cid:63)] ∈ [−R, 0] for every i ∈ [n] and we may therefore apply Fact 1
to obtain

¯xj ˜δi[k(cid:63)]q ≥ (1 − ε(cid:48))¯xi exp(σ˜δi[k]) ≥ (1 − ε(cid:48))e−σµ ¯xi exp(σ¯δi[k])

(51)

p
(cid:88)

σq
q!

q=0
for every i ∈ [n]. Therefore, we have

Z[σ] ≥ (1 − ε(cid:48)) (cid:13)

(cid:13)¯x ◦ exp (cid:0)σ¯δ(cid:1)(cid:13)

(cid:13)1 .

(52)

Similarly, we have

p
(cid:88)

q=0

σq
q!

¯xj ˜δi[k(cid:63)]q ≤ (1 + ε(cid:48))¯xi exp(σ˜δi[k]) ≤ (1 + ε(cid:48))e−σµ ¯xi(exp(σ¯δi[k]) + exp(−σR/ˆσ))

Note that the condition (47) also implies that ˜δj[k(cid:63)] ≥ −R/(2ˆσ) for some j ∈ [n] (namely the
maximal element of ¯δ). Using also ¯xj ≥ λ, we have

eσµ exp(−σR/ˆσ) ≤ exp(−σR/(2ˆσ))

¯xj
λ

exp(σ¯δj).

Taking R ≥ 2 log 2n
quently

λε(cid:48) and recalling that σ ≥ ˆσ/2, we have exp(−σR/(2ˆσ)) ≤ λε(cid:48)/(2n) and conse-

eσµ exp(−σR/ˆσ) ≤ ε(cid:48) ¯xj exp(σ¯δj) ≤

ε(cid:48)
n

(cid:13)¯x ◦ exp (cid:0)σ¯δ(cid:1)(cid:13)
(cid:13)

(cid:13)1 .

Substituting back and using ¯xi ≤ 1 and ε(cid:48) < 1 gives

eσµ

p
(cid:88)

q=0

σq
q!

¯xj ˜δi[k(cid:63)]q ≤ (1 + ε(cid:48))¯xi exp(σ¯δi) +

ε(cid:48)
n

(cid:13)
(cid:13)¯x ◦ exp (cid:0)σ¯δ(cid:1)(cid:13)

(cid:13)1 .

Summing over i ∈ [n], we obtain

Z[σ] ≤ (1 + 2ε(cid:48)) (cid:13)

(cid:13)¯x ◦ exp (cid:0)σ¯δ(cid:1)(cid:13)
(cid:13)1
(cid:13)¯x ◦ exp (cid:0)σ¯δ(cid:1)(cid:13)
(cid:13)1

.

Therefore, Z[σ] is a 2ε(cid:48)-multiplicative approximation of (cid:13)

(cid:0)¯x ◦ exp (cid:0)σ¯δ(cid:1)(cid:1). First, if we deﬁne
It remains to show that ˆx[σ] is a ε-padding of x[σ] := Π∆
1−ε(cid:48) ˆx[σ] then the bounds (51) and (54) imply that ˜x ≥ x[σ] elementwise. Also, the bounds (52)

˜x = 1+2ε(cid:48)
and (53) imply that

(53)

(54)

for every i ∈ [n]. Therefore, for ε(cid:48) < 1/10,

ˆxi[σ] − xi[σ] ≤

(1 + ε(cid:48))xi[σ] + ε(cid:48)/n
1 − ε(cid:48)

(cid:107)˜x − x[σ](cid:107)1 ≤

(cid:19)2

(cid:18) 1 + 2ε(cid:48)
1 − ε(cid:48)

− 1 ≤ 10ε(cid:48),

so that ˆx[σ] is a 10ε(cid:48) padding of x[σ]. Taking ε(cid:48) = ε/10 concludes the proof.

41

Implementation: data structure state and initialization

5.3.4
Besides storing ¯x and ¯δ, the data structure maintains the following ﬁelds.

1. An oﬀset µk ∈ R for every k ≤ K =

(cid:108)

log 1
σmin

(cid:109)

, initialized as µk = maxj[¯δ]j for all k.

2. A balanced binary tree with n leaves. For node v in the tree, k ∈ [K] and q ∈ {0, . . . , p}, we

store

Av[k, q] :=

(cid:68)

¯x, ˜δ[k]q(cid:69)

Sv

,

where ˜δ[k] = max{¯δ − µk, −R/ˆσk} as before, the set Sv contains the leaves in the subtree
rooted in v, and (cid:104)a, b(cid:105)S := (cid:80)
i∈S aibi. When referring to the root of the tree we omit the
subscript, i.e., we write

A[k, q] :=

¯x, ˜δ[k]q(cid:69)
(cid:68)

.

3. A vector u ∈ Rn and coeﬃcients ck,q ∈ R for every k ∈ [K] and q ∈ {0, . . . , p}, for maintaining
the running sum. We initialize them all to be 0. The running sum obeys the following
invariant:

s = u +

K
(cid:88)

p
(cid:88)

k=1

q=0

ck,q
q!

¯x ◦ ˜δ[k]q.

(55)

4. A doubly linked list of the sorted entries of ¯δ, with a pointer to the maximal element of ¯δ as

well as pointers to the largest element smaller than µk − R/ˆσk for every k ∈ [K].

Initializing the data structure for maintaining the maximum element takes time O(n log n) due
to the need to sort ¯δ. With it, initializing µk is trivial and so is the initialization of u and cq,k.
Initializing the data stored in the binary tree takes time O(npK), since for every value k and q and
internal node v with children v(cid:48), v(cid:48)(cid:48) we can recursively compute Av[k, q] as Av(cid:48)[k, q] + Av(cid:48)(cid:48)[k, q]. We
will also charge some additional deletion costs to the initialization runtime, resulting in the overall
complexity O(npK log n).

5.3.5

Implementation: queries and sampling

GetNorm(σ). We compute k(cid:63) =
σ
σmin
takes O(p) time and Proposition 5 provides the claimed approximation guarantee.

and return Z[σ] = eσk(cid:63) (cid:80)p

q=0

σq
q! A[k(cid:63), q]. Clearly, this

(cid:108)
log2

(cid:109)

Get(j, σ). We compute Z[σ] and k(cid:63) as described above and return eσµk(cid:63)
σq
q! ¯xj ˜δj[k(cid:63)]q in ac-
Z[σ]
cordance with the form (50) of ˆx[σ]. Again, this takes O(p) time and Proposition 5 provides the
claimed approximation guarantee.

q=0

(cid:80)p

GetSum(j). Recalling the invariant (55), we return uj + (cid:80)K

k=1

(cid:80)p

q=0

ck,q
q! ¯xj ˜δj[k]q in time O(pK).

Sample(σ). We perform a random walk from the root of our binary tree data structure to a leaf.
A each internal node v with children v(cid:48) and v(cid:48)(cid:48), we select node v(cid:48) with probability

(cid:104)1, ˆx[σ](cid:105)Sv(cid:48)
(cid:104)1, ˆx[σ](cid:105)Sv

=

(cid:80)p

q=0

(cid:80)p

q=0

σq
q! Av(cid:48)[k(cid:63), q]
σq
q! Av[k(cid:63), q]

,

42

. We return the index associated with the leaf in
and otherwise select v(cid:48)(cid:48), where k(cid:63) =
which we end the walk; the probability of returning index j is exactly [ˆx[σ]j. Each step in the walk
takes time O(p) and there are O(log n) steps, so the total time is O(p log n).

σ
σmin

log2

(cid:108)

(cid:109)

5.3.6

Implementation: updates

UpdateSum(σ). Recalling the invariant (55) and the form (50) of ˆx[σ], we compute k(cid:63) and Z[σ] as
in the GetNorm implementation, and update update

ck(cid:63),q ← ck(cid:63),q +

eσµk(cid:63) σq
Z[σ]

for every q ∈ {0, . . . , p}. This takes time O(p).

Del(j). We set [¯δj] ← −∞, remove the element corresponding to index j from the doubly linked
list, and perform the following operations for each k ∈ [K] separately. First, we check if the new
maximum element of ¯δ is a least µk − R/(2ˆσk). If it is, we leave µk unchanged and we simply update

Av[k, q] ← Av[k, q] − ¯xj ˜δj[k]q

for every q ≤ p and node v on the path from the root to the leaf corresponding to index j. Since
the length of the path is O(log n), this update takes time O(p log n).

Otherwise, the new maximum element is less than µk − R/(2ˆσk), and we must change µk in

order to maintain the invariant (47). Let µnew

be the new maximum element of ¯δ, and let

k

Uk =

(cid:26)

i

(cid:12)
(cid:12)
(cid:12)
(cid:12)

[¯δ]i ≥ µnew

k +

(cid:27)

R
ˆσk

be the new set of un-truncated indices. (We ﬁnd the elements in this set when we update the pointer
to the ﬁrst element smaller than µk − R/ˆσk). We recompute Av[k, q] =
for every q ≤ p
and every node v with a child in Uk. Performing the computation recursively from leaf to root, this
take at most O(|Uk|p log n) time. To maintain the invariant (55) as the deﬁnition of ˜δ[k] changes,
we update

¯x, ˜δ[k]q(cid:69)

Sv

(cid:68)

uj ← uj +

p
(cid:88)

q=0

ck,q
q!

¯xj

(cid:0)[¯δj − µnew

k

]q − [max (cid:8)¯δj − µk, −R/ˆσk

(cid:9)]q(cid:1) for every j ∈ Uk;

this update takes O(|Uk|p) time. Finally, we update µk ← µnew

.

k

Summing over k ∈ [K], deletion operations of the ﬁrst kind (with µk unchanged) take at most
O(Kp log n) time per call to Del. Operations of the second kind (with µk decreased) take time
O(N p log n) throughout the data structure lifetime, where N = (cid:80)
k | and for each
k ∈ [K] we write U (1)
k , . . . to denote the diﬀerent sets Uk generated by all calls to Del. For
each k, if µk is decreased at all then it must decrease by at least R/(2ˆσk). Therefore, by deﬁnition
of Uk, an index j can belong to U (t)
for at most 2 values of t. Consequently, we have N = O(nK).
k
Therefore, deletion operations of the second kind contribute at most O(nKp log n) to the total
runtime, which we charge to initialization.

k=1 |U (t)

k , U (2)

(cid:80)K

t≥1

43

6 Applications

In this section, we leverage the techniques of this paper to obtain improved runtimes for solving
certain structured optimization problems.

In Sections 6.1 and 6.2, we use a variant of our variance-reduced coordinate method in the
(cid:96)2-(cid:96)1 setup to obtain algorithms for solving the maximum inscribed ball (Max-IB) and minimum
enclosing ball (Min-EB) problems. Our algorithms improve upon the runtimes of those in Allen-
Zhu et al. [2] by a factor depending on the sparsity of the matrix. This improvement stems from a
preprocessing step in [2] where the input is randomly rotated to improve a norm dependence of the
algorithm. Our methods avoid this preprocessing and obtain runtimes dependent on the both the
sparsity and numerical sparsity of the data, providing universal improvements in the sparse regime,
in the non-degenerate case where the span of the points is full-rank.

In Section 6.3, we use the results of our variance-reduced algorithm in the (cid:96)2-(cid:96)2 setup (cf.
Section D.2) to obtain improved regression algorithms for a variety of data matrices, including
when the matrix is numerically sparse or entrywise nonnegative.

Our methods in this section rely on an extension of the outer loop of this paper (Algorithm 2)
for strongly monotone minimax problems, developed in our previous work [8]. Speciﬁcally, for a
separable regularizer r(x, y) = rx(x) + ry(y) on a joint space for any of our setups, consider the
following composite bilinear minimax problem:

min
x∈X

max
y∈Y

f (x, y) := y(cid:62)Ax + µxφ(x) − µyψ(y), where φ = V x

x(cid:48), ψ = V y
y(cid:48).

(56)

We call such problem a (µx, µy)-strongly monotone problem; this is a special case of a gen-
eralization of the notion of strong convexity, in the case of convex minimization. For general
strongly-monotone problems, Carmon et al. [8] provided a variant of Algorithm 2 with the following
guarantee.

Proposition 6 (Proposition 5, Carmon et al. [8]). For problem (56), denote µ :=
µxµy and ρ :=
(cid:112)µx/µy. Let O be an (α,ε)-relaxed proximal oracle for operator g(x, y) := (∇xf (x, y), −∇yf (x, y)),
let Θ be the range of r, and let (cid:107)(∇xf (z), −∇yf (z(cid:48)))(cid:107)∗ ≤ G, for all z, z(cid:48) ∈ Z. Let zK be the output
of K iterations of OuterLoopStronglyMonotone, Algorithm 7 of Carmon et al. [8]. Then

√

E Gap(zK) ≤

√

2G

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:32)(cid:18) α

µ + α

(cid:19)K (cid:18)

ρ +

(cid:19)

1
ρ

Θ +

(cid:33)

.

ε
µ

Each iteration k ∈ [K] consists of one call to O, producing a point zk−1/2, and one step of the form
(cid:111)
(cid:110)(cid:10)g(zk−1/2), z(cid:11) + α ˆVzk−1(z) + µ ˆVzk−1/2(z)

zk ←

(57)

,

where ˆV := ρV x + ρ−1V y. In particular, by setting

ε =

µ(cid:15)2
4G2 ,

using K = (cid:101)O (α/µ) iterations, we have the guarantee E Gap(zK) ≤ (cid:15).

The (α,ε)-relaxed proximal oracle works similarly as in Algorithm 3 except for the additional
composite terms. For completeness we include the algorithm with its theoretical guarantees and
implementation in Section E.2 (see Algorithm 4, Proposition 1 and Section E.2.2).

44

In all of our applications discussed in this section, the cost of each step (57) is O(nnz), stemming

from the computation of g(x, y). The resulting algorithms therefore have runtime

(cid:18)

˜O

(nnz + (cost of implementing O)) ·

(cid:19)

.

α
µ

6.1 Maximum inscribed ball

In the maximum inscribed ball (Max-IB) problem, we are given a polyhedron P ⊂ Rn deﬁned by
m halfspaces {Hi}i∈[m], each characterized by a linear constraint Hi = {x ∈ Rn : (cid:104)ai, x(cid:105) + bi ≥ 0},
i.e. P = ∩i∈[n]Hi. The goal is to (approximately) ﬁnd a point x∗ ∈ P that maximizes the smallest
distance to any of the bounding hyperplanes Hi, i.e.

x∗ ∈ arg max

x∈P

min
i∈[n]

(cid:104)ai, x(cid:105) + bi
(cid:107)ai(cid:107)2

.

More formally, if the optimal radius of the maximum inscribed ball is r∗, the goal is to ﬁnd an
(cid:15)-accurate solution, i.e. a point in P which has minimum distance to all bounding hyperplanes at
least (1 − (cid:15))r∗.

Given halfspace information A, b where Ai: = ai for all i ∈ [m], the polytope is deﬁned by
, r∗ is the value
P = {x | Ax + b ≥ 0}. We use the following notation in this section: B := (cid:107)b(cid:107)∞
of the maximum inscribed ball problem, R is the radius of the minimum enclosing ball, which is
deﬁned as the Euclidean ball containing P with smallest radius possible, x∗ is the center of the
maximum inscribed ball, and ρ is an upper bound on the aspect ratio R/r∗. As in Allen-Zhu et al.
[2], we will make the following assumptions:

1. The polytope is bounded, and thus m ≥ n. This is without loss of generality since when the

polytope is unbounded, the aspect ratio ρ = ∞, and our runtime result holds trivially.

2. (cid:107)Ai:(cid:107)2

2 = 1 for all i ∈ [m], so (cid:107)A(cid:107)2→∞ = 1, by properly scaling A (one can consider the trivial

case when for some i, ai = 0 separately).

3. The origin is inside polytope P , i.e. O ∈ P , by properly shifting P .

We also deﬁne the following constant (see Appendix D.3) in this section with respect to the

rescaled matrix A,

L2,1

co := min

(cid:110)

L2,1,(1)
co

, L2,1,(2)
co

, L2,1,(3)
co

(cid:111)

≤

√

rcs · L2,1

rc ≤

√

rcs,

given the deﬁnitions of L2,1,(1)
above (namely, that L2,1

co

, L2,1,(2)
co

, L2,1,(3)
co

rc = maxi∈[m] (cid:107)Ai:(cid:107)2 = 1).

as in (94), (95), and (96), and the second assumption

Allen-Zhu et al. [2] show that solving Max-IB is equivalent to the following minimax problem:

r∗ := max
x∈Rn

min
y∈∆m

f (x, y) := y(cid:62)Ax + y(cid:62)b,

(58)

and moreover, to solve the problem to (cid:15)-multiplicative accuracy, it suﬃces to ﬁnd x∗
(cid:15)
minimax problem to (cid:15)-multiplicative accuracy in terms of the one-sided gap of the x block, i.e.

that solves the

min
y∈∆m

f (x∗

(cid:15) , y) ≥ (1 − (cid:15))f (x∗, y∗),

where (x∗, y∗) is the optimal saddle point of problem (58). We ﬁrst state several bounds on the
parameters of the problem from Allen-Zhu et al. [2].

45

Fact 2 (Geometric properties of Max-IB). We have (cid:107)x∗(cid:107)2 ≤ 2R, and

r∗ = max
x∈Rn

min
y∈∆m

f (x, y) := y(cid:62)Ax + y(cid:62)b ≤ B ≤ 2R.

These facts imply that we can instead consider the constrained minimax problem (where we

overload our deﬁnition of f for the rest of the section):

r∗ := max
x∈Bn

min
y∈∆m

f (x, y) = y(cid:62) ˜Ax + y(cid:62)b, where ˜A = 2R · A.

(59)

We ﬁrst use a “warm start” procedure to ﬁnd a constant multiplicative estimate of r∗, which uses
the strongly monotone algorithm OuterLoopStronglyMonotone of Carmon et al. [8] together with
Algorithm 4 of Section E.2 as a relaxed proximal oracle on the (µ, µ)-strongly monotone problem

max
x∈Bn

min
y∈∆m

fµ(x, y) := y(cid:62) ˜Ax + y(cid:62)b + µ

[y]i log[y]i −

µ
2

(cid:107)x(cid:107)2
2 ,

(cid:88)

i∈[m]

and a line search over parameter µ. The following lemma is an immediate consequence of Proposi-
tion 6 and Corollary 1, whose proof we defer to Appendix F.1.

Lemma 10. We can spend (cid:101)O
approximation ˆr of r∗, i.e.

(cid:16)

√

nnz + ρ

nnz · L2,1
co

(cid:17)

time preprocessing to obtain a 8-multiplicative

ˆr
8

≤ r∗ ≤ ˆr.

Finally, we use our variance-reduced coordinate algorithm, namely Algorithm 4 as a relaxed prox-
imal oracle in OuterLoopStronglyMonotone together with Proposition 6 once more to solve (59)
to the desired accuracy. The implementation in Section E.2.2 and complexity results in Section D.3
yield the runtime. This implementation crucially uses our development of the ApproxExpMaintainer
data structure in order to obtain a runtime depending directly on rcs rather than dimensions of the
matrix, as well as independence on B. For completeness, a proof can be found in Appendix F.1.

Theorem 3. The algorithm of Section D.3 can be used to ﬁnd an (cid:15)-accurate solution x∗
satisfying miny∈∆m f (x∗

(cid:15) , y) ≥ (1 − (cid:15))r∗ with high probability in time 5
(cid:33)

√

√

(cid:32)

(cid:15) to Max-IB

(cid:101)O

nnz +

ρ

nnz · L2,1
co

(cid:15)

(cid:18)

= (cid:101)O

nnz +

ρ

nnz · rcs

(cid:19)

.

(cid:15)

Remark 5. Because we assumed m ≥ n, in the case A is dense, up to logarithmic terms our runtime
improves upon the runtime of (cid:101)O (ρm

n/(cid:15)) in Allen-Zhu et al. [2] by a factor of at least

√

(cid:114) mn
nnz

·

m
rcs

generically. This is an improvement when A is sparse or column-sparse, i.e. nnz (cid:28) mn, or rcs (cid:28) m.
Such a saving is larger when A has numerical sparsity so that e.g.
(cid:0)maxi∈[m] (cid:107)Ai:(cid:107)1

(cid:1) < rcs · maxi∈[m] (cid:107)Ai(cid:107)2
factor of polylog((cid:107)b(cid:107)∞) due to the additional cost in the runtime of

(cid:1) (cid:0)maxj∈[n] (cid:107)A:j(cid:107)1
5Here (cid:101)O is hiding an additional

≤ maxi∈[m] (cid:107)Ai:(cid:107)2

L2,1
co

1 +

(cid:17)2

(cid:16)

2

.

ApproxExpMaintainer, caused by the linear term b (see Remark 2).

46

6.2 Minimum enclosing ball

In the minimum enclosing ball (Min-EB) problem, we are given a set of data points {a1, . . . , am}
with a1 = 0, maxi∈[m] (cid:107)ai(cid:107) = 1.6 The goal is to ﬁnd the minimum radius R∗ such that there exists
a point x with distance at most R∗ to all points. Following the presentation of Allen-Zhu et al.
[2], we consider Min-EB in an equivalent form. Deﬁne the vector b to have bi = 1
entrywise.
Then, Min-EB is equivalent to the minimax problem

2 (cid:107)ai(cid:107)2

2

R∗ := min
x∈Rn

max
y∈∆m

1
2

(cid:88)

i

yi(cid:107)x − ai(cid:107)2

2 = min
x∈Rn

max
y∈∆m

f (x, y), where f (x, y) := y(cid:62)Ax + y(cid:62)b +

1
2

(cid:107)x(cid:107)2
2 .

(60)

By assumption, (cid:107)A(cid:107)2→∞ = 1. We let (x∗, y∗) be the optimal solution to the saddle point problem.
We ﬁrst state several bounds on the quantities of the problem. These bounds were derived in
Allen-Zhu et al. [2] and obtained by examining the geometric properties of the problem.

Fact 3. The following bounds hold: (cid:107)x∗(cid:107)2 ≤ 1, and R∗ ≥ 1/8.

(cid:15) ) achieving maxy f (x∗

To achieve a multiplicative approximation, since R∗ ≥ 1/8 by Fact 3, it suﬃces to obtain a pair
(cid:15) ) ≤ (cid:15)/8. In light of minimax optimality, Lemma 11
(x∗
(cid:15) , y∗
(proved in Section F.2) shows that it suﬃces to consider, for (cid:15)(cid:48) = Θ((cid:15)/ log m), solving the following
(1, (cid:15)(cid:48))-strongly monotone problem to suﬃcient accuracy:

(cid:15) , y) − minx f (x, y∗

min
x∈Rn

max
y∈∆m

f(cid:15)(cid:48)(x, y) := y(cid:62)Ax + y(cid:62)b − (cid:15)(cid:48) (cid:88)

[y]i log[y]i +

i∈[m]

1
2

(cid:107)x(cid:107)2
2 .

(61)

Lemma 11. Setting (cid:15)(cid:48) = (cid:15)/(32 log m), an (cid:15)/16-accurate solution or (61) is an (cid:15)/8-accurate solution
to the original problem (60).

As an immediate result of the above lemma, the runtime in Section D.3 and the correctness

proofs of Proposition 6 and Corollary 1, we obtain the following guarantee.

Theorem 4. The strongly monotone algorithm OuterLoopStronglyMonotone of Carmon et al. [8],
using Algorithm 4 of Section E.2 and the estimator of Section D.3 as a relaxed proximal oracle, ﬁnds
(cid:15) to Min-EB satisfying R∗ ≤ maxy f (x∗
an (cid:15)-accurate solution x∗
(cid:15) , y) ≤ (1 + (cid:15))R∗ with high probability
in time
√

√

(cid:32)

(cid:101)O

nnz +

(cid:33)

nnz · L2,1
co√
(cid:15)

(cid:18)

= (cid:101)O

nnz +

nnz · rcs
(cid:15)

√

(cid:19)

.

Remark 6. When m ≥ n,7 up to logarithmic terms our runtime improves the (cid:101)O (m
of Allen-Zhu et al. [2] by a factor of

√

√

n/

(cid:15)) runtime

(cid:114) mn
nnz
generically. This is an improvement when A is sparse or column-sparse, i.e. nnz (cid:28) mn, or rcs (cid:28) m.
As in Section 6.1, the improvement is larger when A is numerically sparse, i.e. when
(cid:1) (cid:0)maxj∈[n] (cid:107)A:j(cid:107)1
maxi∈[m] (cid:107)Ai:(cid:107)2

(cid:1) < rcs · maxi∈[m] (cid:107)Ai(cid:107)2

1 + (cid:0)maxi∈[m] (cid:107)Ai:(cid:107)1

m
rcs

L2,1
co

(cid:17)2

≤

(cid:16)

2

·

.

6This can be assumed without loss of generality by shifting and rescaling as in Allen-Zhu et al. [2] and considering

the trivial case when all ai, i ∈ [m] are equal.

7When m < n, the runtime of the algorithm in Allen-Zhu et al. [2] still holds and is sometimes faster than ours.

47

6.3 Regression

We consider the standard (cid:96)2 linear regression problem in a data matrix A ∈ Rm×n and vector
b ∈ Rm, i.e. minx∈Rn (cid:107)Ax − b(cid:107)2

. In particular, we consider the equivalent primal-dual form,

min
x∈Rn

max
y∈Bm

f (x, y) := y(cid:62)(Ax − b).

(62)

Throughout, we assume the smallest eigenvalue of A(cid:62)A is µ > 0 and denote an optimal solution
to (62) by z∗ = (x∗, y∗) (where x∗ is the unique solution to the regression problem). Our strategy
is to consider a sequence of modiﬁed problems, parameterized by β > 0, x(cid:48) ∈ Rn:

min
x∈Rn

max
y∈Bm

f β
x(cid:48)(x, y) := y(cid:62)(Ax − b) +

β
2

(cid:13)x − x(cid:48)(cid:13)
(cid:13)
2
2 −
(cid:13)

β
2

(cid:107)y(cid:107)2
2 .

(63)

We denote the optimal solution to (63) by z∗
(β,x(cid:48)) = (x∗
(β,x(cid:48)), y∗
√
simplicity we drop β and write z∗
x(cid:48)) (as β =
x(cid:48), y∗
(proved in Section F.3) states a known relation between the optimal solutions for (62) and (63).

(β,x(cid:48))); when clear from context, for
µ throughout our algorithm). Lemma 12

x(cid:48) = (x∗

Lemma 12. Letting (x∗, y∗) be the optimal solution for (62) and (x∗
for (63), the following relation holds:

x(cid:48), y∗

y(cid:48)) be the optimal solution

(cid:107)x∗

x(cid:48) − x∗(cid:107)2 ≤

1
1 + µ
β2

(cid:13)x(cid:48) − x∗(cid:13)
(cid:13)

(cid:13)2 .

We give a full implementation of the regression algorithm in Algorithm 5 (see Section F.3), and
state its correctness and runtime in Theorem 5. The algorithm repeatedly solves problems of the
form (63) in phases, each time using Lemma 12 to ensure progress towards x∗. Observing that each
subproblem is (β, β)-strongly monotone, each phase is conducted via OuterLoopStronglyMonotone,
an algorithm of Carmon et al. [8], using the (cid:96)2-(cid:96)2 algorithms of Section D.2 as a proximal oracle. Due
to the existence of composite terms, our inner loop steps are slightly diﬀerent than in Section D.2; we
give a more formal algorithm for the relaxed proximal oracle and its implementation in Algorithm 4
and Appendix E.2. We remark that by a logarithmic number of restarts per phase, a standard
argument boosts Theorem 5 to a high-probability claim.

Theorem 5. Given data matrix A ∈ Rm×n, vector b ∈ Rm, and desired accuracy (cid:15) ∈ (0, 1),
assuming A(cid:62)A (cid:23) µI for µ > 0, Algorithm 5 outputs an expected (cid:15)-accurate solution ˜x, i.e.

E [(cid:107)˜x − x∗(cid:107)2] ≤ (cid:15),

and runs in time



(cid:101)O


nnz +

max

√

nnz ·

(cid:110)(cid:113)(cid:80)

i (cid:107)Ai:(cid:107)2
1,
√
µ

(cid:113)(cid:80)

j (cid:107)A:j(cid:107)2
1



(cid:111)


 .

We give two settings where the runtime of Algorithm 5 improves upon the state of the art.

In the particular setting when all entries of A are nonnegative,8 by
Entrywise nonnegative A.
Proposition 7 our complexity as stated in Theorem 5 improves by a factor of (cid:112)nnz/(m + n) the
runtime of accelerated gradient descent [30], which is the previous state-of-the-art in certain regimes
with runtime O (cid:0)nnz · (cid:107)A(cid:107)op/

µ(cid:1). This speedup is most beneﬁcial when A is dense.

√

8More generally, this holds for arbitrary A ∈ Rm×n satisfying (cid:107)|A|(cid:107)op ≤ (cid:107)A(cid:107)op.

48

Numerically sparse A. For numerically sparse A with (cid:107)Ai:(cid:107)1/ (cid:107)Ai:(cid:107)2 = O(1), (cid:107)A:j(cid:107)1/ (cid:107)A:j(cid:107)2 =
√
O(1) for all i ∈ [m], j ∈ [n], we can choose α =

µ in Algorithm 5 and obtain the runtime

(cid:32)

O

nnz +

max(cid:8)(cid:80)

1, (cid:80)
i (cid:107)Ai:(cid:107)2
µ

j (cid:107)A:j(cid:107)2
1

(cid:9)

(cid:33)

(cid:32)

= O

nnz +

(cid:33)

(cid:107)A(cid:107)2
F
µ

using the argument in Theorem 5. Under a (similar, but weaker) numerically sparse condition
(cid:107)Ai:(cid:107)1/ (cid:107)Ai:(cid:107)2 = O(1), the prior state-of-the-art stochastic algorithm [19] obtains a runtime of
O(nnz + rcs · (cid:107)A(cid:107)2
F /µ), and the recent state-of-the-art result in the numerically sparse regime [17]
improves this to O(nnz + ((cid:107)A(cid:107)2
µ). Improving universally over both,
our method gives O(nnz + (cid:107)A(cid:107)2

F /µ)1.5) when rcs = Ω((cid:107)A(cid:107)F /
F /µ) in this setting.

√

Acknowledgements

This research was supported in part by Stanford Graduate Fellowships, NSF CAREER Award
CCF-1844855, NSF Graduate Fellowship DGE-1656518 and a PayPal research gift. We thank
the anonymous reviewers who helped improve the completeness and readability of this paper by
providing many helpful comments.

49

References

[1] Z. Allen-Zhu, Y. T. Lee, and L. Orecchia. Using optimization to obtain a width-independent,
parallel, simpler, and faster positive sdp solver. In Proceedings of the twenty-seventh annual
ACM-SIAM symposium on Discrete algorithms, pages 1824–1831. Society for Industrial and
Applied Mathematics, 2016.

[2] Z. Allen-Zhu, Z. Liao, and Y. Yuan. Optimization algorithms for faster computational geometry.
In 43rd International Colloquium on Automata, Languages, and Programming, pages 53:1–53:6,
2016.

[3] Z. Allen-Zhu, Z. Qu, P. Richtárik, and Y. Yuan. Even faster accelerated coordinate descent
using non-uniform sampling. In International Conference on Machine Learning, pages 1110–
1119, 2016.

[4] E. Andersen, C. Roos, T. Terlaky, T. Trafalis, and J. Warners. The use of low-rank updates in

interior-point methods. Numerical Linear Algebra and Optimization, pages 1–12, 1996.

[5] M. G. Azar, R. Munos, and H. J. Kappen. Minimax pac bounds on the sample complexity of
reinforcement learning with a generative model. Machine learning, 91(3):325–349, 2013.
[6] D. Babichev, D. Ostrovskii, and F. Bach. Eﬃcient primal-dual algorithms for large-scale mul-

ticlass classiﬁcation. arXiv preprint arXiv:1902.03755, 2019.

[7] P. Balamurugan and F. R. Bach. Stochastic variance reduction methods for saddle-point prob-

lems. In Advances in Neural Information Processing Systems, 2016.

[8] Y. Carmon, Y. Jin, A. Sidford, and K. Tian. Variance reduction for matrix games. In Advances

in Neural Information Processing Systems, 2019.

[9] K. L. Clarkson and D. P. Woodruﬀ. Low rank approximation and regression in input sparsity
time. In Proceedings of the 45th annual ACM symposium on Symposium on theory of computing,
pages 81–90. ACM, 2013.

[10] K. L. Clarkson, E. Hazan, and D. P. Woodruﬀ. Sublinear optimization for machine learning.
In 51th Annual IEEE Symposium on Foundations of Computer Science, pages 449–457, 2010.
[11] M. B. Cohen, J. Nelson, and D. P. Woodruﬀ. Optimal approximate matrix product in terms
of stable rank. In 43rd International Colloquium on Automata, Languages, and Programming
(ICALP 2016). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2016.

[12] M. B. Cohen, Y. T. Lee, and Z. Song. Solving linear programs in the current matrix multipli-

cation time. arXiv preprint arXiv:1810.07896, 2018.

[13] G. B. Dantzig. Linear Programming and Extensions. Princeton University Press, Princeton,

NJ, 1953.

[14] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Eﬃcient projections onto the l 1-ball
for learning in high dimensions. In Proceedings of the 25th international conference on Machine
learning, pages 272–279. ACM, 2008.

[15] A. Gilyén, S. Lloyd, and E. Tang. Quantum-inspired low-rank stochastic regression with loga-

rithmic dependence on the dimension. arXiv preprint arXiv:1811.04909, 2018.

[16] M. D. Grigoriadis and L. G. Khachiyan. A sublinear-time randomized approximation algorithm

for matrix games. Operation Research Letters, 18(2):53–58, 1995.

50

[17] N. Gupta and A. Sidford. Exploiting numerical sparsity for eﬃcient learning: faster eigenvector
In Advances in Neural Information Processing Systems, pages

computation and regression.
5269–5278, 2018.

[18] H. Jiang, Y. T. Lee, Z. Song, and S. C.-w. Wong. An improved cutting plane method for convex
optimization, convex-concave games, and its applications. In Proceedings of the 52nd Annual
ACM SIGACT Symposium on Theory of Computing, pages 944–953, 2020.

[19] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems, 2013.

[20] N. Karmarkar. A new polynomial-time algorithm for linear programming. In Proceedings of
the sixteenth annual ACM symposium on Theory of computing, pages 302–311. ACM, 1984.
[21] I. Koutis, G. L. Miller, and R. Peng. Approaching optimality for solving SDD linear systems.
In 51th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2010, October
23-26, 2010, Las Vegas, Nevada, USA, pages 235–244, 2010.

[22] Y. T. Lee and A. Sidford. Eﬃcient accelerated coordinate descent methods and faster algo-
rithms for solving linear systems. In 2013 IEEE 54th Annual Symposium on Foundations of
Computer Science, 2013.

[23] Y. T. Lee and A. Sidford. Eﬃcient inverse maintenance and faster algorithms for linear pro-
In IEEE 56th Annual Symposium on Foundations of Computer Science, pages

gramming.
230–249, 2015.

[24] Y. T. Lee, A. Sidford, and S. C.-w. Wong. A faster cutting plane method and its implications for
combinatorial and convex optimization. In 2015 IEEE 56th Annual Symposium on Foundations
of Computer Science, pages 1049–1065. IEEE, 2015.

[25] Y. T. Lee, Z. Song, and Q. Zhang. Solving empirical risk minimization in the current matrix
multiplication time. In Conference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix,
AZ, USA, pages 2140–2157, 2019.

[26] M. Minsky and S. Papert. Perceptrons—an introduction to computational geometry. MIT Press,

1987.

[27] H. Namkoong and J. C. Duchi. Stochastic gradient methods for distributionally robust op-
timization with f-divergences. In Advances in Neural Information Processing Systems, pages
2208–2216, 2016.

[28] A. Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with
lipschitz continuous monotone operators and smooth convex-concave saddle point problems.
SIAM Journal on Optimization, 15(1):229–251, 2004.

[29] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach

to stochastic programming. SIAM Journal on optimization, 19(4):1574–1609, 2009.

[30] Y. Nesterov. A method for solving a convex programming problem with convergence rate

o(1/k2). Doklady AN SSSR, 269:543–547, 1983.

[31] Y. Nesterov. Dual extrapolation and its applications to solving variational inequalities and

related problems. Mathematical Programing, 109(2-3):319–344, 2007.

[32] Y. Nesterov. Eﬃciency of coordinate descent methods on huge-scale optimization problems.

SIAM Journal on Optimization, 22(2):341–362, 2012.

51

[33] Y. Nesterov and S. U. Stich. Eﬃciency of the accelerated coordinate descent method on
structured optimization problems. SIAM Journal on Optimization, 27(1):110–123, 2017.
[34] P. Richtárik and M. Takáč. On optimal probabilities in stochastic coordinate descent methods.

Optimization Letters, 10(6):1233–1243, 2016.

[35] S. Sachdeva and N. K. Vishnoi. Faster algorithms via approximation theory. Foundations and

Trends in Theoretical Computer Science, 9(2):125–210, 2014.

[36] S. Shalev-Shwartz and A. Tewari. Stochastic methods for (cid:96)1-regularized loss minimization.

Journal of Machine Learning Research, 12:1865–1892, 2011.

[37] S. Shalev-Shwartz and Y. Wexler. Minimizing the maximal loss: How and why.

In ICML,

pages 793–801, 2016.

[38] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized

loss minimization. Journal of Machine Learning Research, 14:567–599, 2013.

[39] S. Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and

Trends in Machine Learning, 4(2):107–194, 2012.

[40] A. Sidford and K. Tian. Coordinate methods for accelerating (cid:96)∞ regression and faster approx-
imate maximum ﬂow. In 59th IEEE Annual Symposium on Foundations of Computer Science,
FOCS 2018, Paris, France, October 7-9, 2018, pages 922–933, 2018.

[41] A. Sidford, M. Wang, X. Wu, and Y. Ye. Variance reduced value iteration and faster algorithms
for solving markov decision processes. In Proceedings of the Twenty-Ninth Annual ACM-SIAM
Symposium on Discrete Algorithms, pages 770–787. Society for Industrial and Applied Mathe-
matics, 2018.

[42] T. Strohmer and R. Vershynin. A randomized Kaczmarz algorithm with exponential conver-

gence. Journal of Fourier Analysis and Applications, 15(2):262, 2009.

[43] C. Tan, T. Zhang, S. Ma, and J. Liu. Stochastic primal-dual method for empirical risk min-
imization with o(1) per-iteration complexity. In Advances in Neural Information Processing
Systems, 2018.

[44] J. van den Brand. A deterministic linear program solver in current matrix multiplication time.

arXiv preprint arXiv:1910.11957, 2019.

[45] J. van den Brand, Y. T. Lee, A. Sidford, and Z. Song. Solving tall dense linear programs in
nearly linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory
of Computing, 2020. To appear.

[46] J. Von Neumann and O. Morgenstern. Theory of games and economic behavior (commemorative

edition). Princeton university press, 1944.

[47] M. D. Vose. A linear algorithm for generating random numbers with a given distribution. IEEE

Transactions on software engineering, 17(9):972–975, 1991.

[48] M. Wang. Primal-dual π learning: Sample complexity and sublinear run time for ergodic

Markov decision problems. arXiv preprint arXiv:1710.06100, 2017.

[49] M. Wang. Randomized linear programming solves the discounted Markov decision problem in
nearly-linear (sometimes sublinear) running time. arXiv preprint arXiv:1704.01869, 2017.
[50] S. J. Wright. Coordinate descent algorithms. Mathematical Programming, 151(1):3–34, 2015.

52

[51] A. Yurtsever, M. Udell, J. A. Tropp, and V. Cevher. Sketchy decisions: Convex low-rank matrix
optimization with optimal storage. In International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS), pages 1188–1196, 2017.

[52] Y. Zhang and L. Xiao. Stochastic primal-dual coordinate method for regularized empirical risk

minimization. The Journal of Machine Learning Research, 18(1):2939–2980, 2017.

53

Appendix

A Deferred proofs from Section 2

Proof of Proposition 1. It is clear that our choices of X , Y are compact and convex, and that the
local norms we deﬁned are indeed norms (in all cases, they are quadratic norms). Validity of our
choices of Θ follow from the well-known facts that for x ∈ ∆n, the entropy function (cid:80)
j∈[n] xj log xj
is convex with range log n, and that for x ∈ Bn, 1

is convex with range 1
2

2 (cid:107)x(cid:107)2

2

.

In the Euclidean case we have that Vx(x(cid:48)) = 1

2 (cid:107)x − x(cid:48)(cid:107)2

2

Schwarz and Young inequalities:

and (15) follows from the Cauchy-

(cid:10)γ, x(cid:48) − x(cid:11) ≤

1
2

(cid:107)γ(cid:107)2

2 +

1
2

(cid:13)x − x(cid:48)(cid:13)
(cid:13)
2
2 .
(cid:13)

Similarly, for the simplex we have that entropy is 1-strongly-convex with respect to (cid:107)·(cid:107)1
Vy(y(cid:48)) ≥ 1

, and we obtain (15) from the Hölder and Young inequalities,

2 (cid:107)y − y(cid:48)(cid:107)2

1

and therefore

(cid:10)γ, y(cid:48) − y(cid:11) ≤

1
2

(cid:107)γ(cid:107)2

∞ +

1
2

(cid:13)y − y(cid:48)(cid:13)
(cid:13)
2
1 ,
(cid:13)

where we note that (cid:107)γ(cid:107)∞ = (cid:107)γ(cid:107)∗

in this case.

Finally, clip(·) is not the identity only when the corresponding domain is the simplex, in which

case entropy satisﬁes the local norms bound [cf. 8, Lemma 13],

(cid:10)γ, y − y(cid:48)(cid:11) − Vy(y(cid:48)) ≤

(cid:88)

i∈[m]

γ2
i yi

for all y, y(cid:48) ∈ ∆m and γ ∈ Rm such that (cid:107)γ(cid:107)∞ ≤ 1.

Noting that (cid:107)clip(γ)(cid:107)∞ ≤ 1 and that (cid:107)clip(γ)(cid:107)y ≤ (cid:107)γ(cid:107)y
bound (16). Finally, for every coordinate i ∈ [m] we have

for all y ∈ ∆m, we have the desired local

|γi − [clip(γ)]i| = ||γi| − 1| I{|γi|>1} ≤ |γi| I{|γi|>1} ≤ |γi|2.

Consequently, | (cid:104)γ − clip(γ), z(cid:105) | ≤ (cid:80)

i∈[m] γ2

i zi, giving the distortion bound (17).

B Deferred proofs from Section 3

B.1 Proof of Proposition 2

In this section, we provide a convergence result for mirror descent under local norms. We require
the following well-known regret bound for mirror descent.

Lemma 13 ([8, Lemma 12]). Let Q : Z → R be convex, let T ∈ N, z0 ∈ Z and γ0, γ1, . . . , γT ∈ Z ∗.
The sequence z1, . . . , zT deﬁned by

zt = arg min

z∈Z

(cid:8)(cid:104)γt−1, z(cid:105) + Q(z) + Vzt−1(z)(cid:9)

satisﬁes for all u ∈ Z (denoting zT +1 := u),

T
(cid:88)

t=0

(cid:104)γt, zt − u(cid:105) +

T
(cid:88)

t=1

(cid:104)∇Q(zt), zt − u(cid:105) ≤ Vz0(u) +

T
(cid:88)

{(cid:104)γt, zt − zt+1(cid:105) − Vzt(zt+1)}.

(64)

t=0

54

The proposition follows from this regret bound, the properties of the local norm setup, and the

“ghost iterate” argument due to [29].

Proposition 2. Let (Z, (cid:107)·(cid:107)·, r, Θ, clip) be a local norm setup, let L, (cid:15) > 0, and let ˜g be an L-local
estimator. Then, for η ≤ (cid:15)

, Algorithm 1 outputs a point ¯z such that

η(cid:15) ≥ 54L2Θ
9L2 and T ≥ 6Θ
(cid:34)

(cid:15)2

E Gap(¯z) ≤ E

sup
u∈Z

1
T + 1

(cid:35)

(cid:104)g(zt), zt − u(cid:105)

≤ (cid:15).

T
(cid:88)

t=0

Proof. Deﬁning

and the ghost iterates

˜∆t := g(zt) −

1
η

clip(η˜g(zt))

st = arg min

s∈Z

(cid:26)(cid:28) 1
2

(cid:29)

η ˜∆t−1, s

(cid:27)

+ Vst−1(s)

with s0 = w0,

we rearrange the regret as

η

T
(cid:88)

t=0

(cid:104)g(zt), zt − u(cid:105) ≤

T
(cid:88)

t=0

(cid:104)clip(η˜g(zt)), zt − u(cid:105) +

(cid:69)
(cid:68)
η ˜∆t, st − u

+

T
(cid:88)

t=0

T
(cid:88)

t=0

(cid:68)
η ˜∆t, zt − st

(cid:69)

,

(65)

and bound each term in turn.

We ﬁrst apply Lemma 13 with Q = 0 and γt = clip(η˜g(zt)), using (16) to conclude that

T
(cid:88)

t=0

(cid:104)clip(η˜g(zt)), zt − u(cid:105) ≤ Vz0(u) + η2

T
(cid:88)

t=0

(cid:107)˜g(zt)(cid:107)2
zt

, for all u ∈ Z.

(66)

Next, we apply Lemma 13 again, this time with γt = 1

2 η ˜∆t, to obtain the regret bound

T
(cid:88)

t=0

(cid:10)η ˜∆t, st − u(cid:11) ≤ 2Vz0(u) +

T
(cid:88)

t=0

(cid:110)(cid:68)

η ˜∆t, st − st+1

(cid:69)

− 2Vst(st+1)

(cid:111)

≤ 2Vz0(u) + η2

T
(cid:88)

(cid:26)

t=0

(cid:107)˜g(zt)(cid:107)2
st

+

(cid:107)g(zt)(cid:107)2
∗

(cid:27)

,

1
2

(67)

for all u ∈ Z, where we used

(cid:68)
η ˜∆t, st − st+1

(cid:69)

− 2Vst(st+1) ≤ (cid:104)ηg(zt), st − st+1(cid:105) − Vst(st+1)

+ | (cid:104)clip(η˜g(zt)), st − st+1(cid:105) | − Vst(st+1),

and then appealed to the bounds (15) and (16) in the deﬁnition of the local norm setup. Now,
substituting (66) and (67) into (65), maximizing over u, and taking an expectation, we obtain

E sup
u∈Z

T
(cid:88)

t=0

(cid:104)ηg(zt), zt − u(cid:105) ≤ 3Θ + η2E

T
(cid:88)

t=0

(cid:110)
(cid:107)˜g(zt)(cid:107)2
zt

+ (cid:107)˜g(zt)(cid:107)2
st

+ E

T
(cid:88)

t=0

(cid:68)
η ˜∆t, zt − st

(cid:69)

.

55

+ 1

2 (cid:107)g(zt)(cid:107)2

∗

(cid:111)

(68)

To bound the last term we use the fact that g(zt) = E [˜g(zt) | zt, st] (which follows from the ﬁrst
part of Deﬁnition 3). We then write

(cid:68)

(cid:12)
E
(cid:12)
(cid:12)

η ˜∆t, zt − st

(cid:69)(cid:12)
(cid:12) ≤ E |(cid:104)η˜g(zt) − clip(η˜g(zt)), zt − st(cid:105)| ≤ η2 (cid:107)˜g(zt)(cid:107)2
(cid:12)
where the ﬁrst inequality is by Jensen’s inequality, and the last is due to the property (17) of the
local norm setup. Substituting (69) into (68), we obtain

+ η2 (cid:107)˜g(zt)(cid:107)2
st

(69)

zt

,

E sup
u∈Z

T
(cid:88)

t=0

(cid:104)ηg(zt), zt − u(cid:105) ≤ 3Θ + η2E

(cid:26)

2 (cid:107)˜g(zt)(cid:107)2
zt

T
(cid:88)

t=0

+ 2 (cid:107)˜g(zt)(cid:107)2
st

+

(cid:107)g(zt)(cid:107)2
∗

(cid:27)

.

1
2

Finally, using the second moment bound of local gradient estimator (Deﬁnition 3) and its conse-
quence Lemma 1, we may bound each of the expected squared norm terms by L2. Dividing through
by η(T + 1) gives

E sup
u∈Z

(cid:34)

1
T + 1

T
(cid:88)

t=0

(cid:104)g(zt), zt − u(cid:105)

≤

(cid:35)

3Θ
η(T + 1)

+

9ηL2
2

.

Our choices η = (cid:15)

9L2 and T ≥ 6Θ

η(cid:15)

imply that the right hand side is at most (cid:15), as required.

B.2 Proof of Proposition 3

Proposition 3. Let O be an (α, εinner)-relaxed proximal oracle with respect to gradient mapping g,
distance-generating function r with range at most Θ and some εinner ≤ εouter. Let z1/2, z3/2, . . . , zK−1/2
be iterates of Algorithm 2 and let ¯zK be its output. Then

E Gap(¯zK) ≤ E max
u∈Z

1
K

K
(cid:88)

k=1

(cid:10)g(zk−1/2), zk−1/2 − u(cid:11) ≤

αΘ
K

+ εouter.

Proof. For some iteration k, we have by the optimality conditions on z(cid:63)
k

that

(cid:10)g(zk−1/2), z(cid:63)

k − u(cid:11) ≤ α

(cid:16)

Vzk−1(u) − Vz(cid:63)

k

(cid:17)
(u) − Vzk−1 (z(cid:63)
k)

∀u ∈ Z.

Summing over k, writing (cid:10)g(zk−1/2), z(cid:63)
and rearranging yields

k − u(cid:11) = (cid:10)g(zk−1/2), zk−1/2 − u(cid:11) − (cid:10)g(zk−1/2), zk−1/2 − z(cid:63)

k

(cid:11),

K
(cid:88)

k=1

(cid:10)g(zk−1/2), zk−1/2 − u(cid:11) ≤αVz0(u) +

(cid:16)

α

K
(cid:88)

k=1

Vzk (u) − Vz(cid:63)

k

(cid:17)

(u)

+

K
(cid:88)

k=1

(cid:0)(cid:10)g(zk−1/2), zk−1/2 − z(cid:63)

k

(cid:11) − αVzk−1 (z(cid:63)

k)(cid:1) ,

(70)

for all u ∈ Z. Since z0 minimizes r, the ﬁrst term is bounded by Vz0(u) ≤ r(u) − r(z0) ≤ Θ. The
second term is bounded by the deﬁnition of zk in Algorithm 2:

(cid:16)

α

K
(cid:88)

k=1

Vzk (u) − Vz(cid:63)

k

(cid:17)

(u)

≤ K(εouter − εinner).

56

Thus, maximizing (70) over u and then taking an expectation yields

E max
u∈Z

K
(cid:88)

k=1

(cid:10)g(zk−1/2), zk−1/2 − u(cid:11) ≤ αΘ + K(εouter − εinner)

+

K
(cid:88)

k=1

E (cid:2)(cid:10)g(zk−1/2), zk−1/2 − z(cid:63)

k

(cid:11) − αVzk−1 (z(cid:63)

k)(cid:3) .

Finally, by Deﬁnition 5, E (cid:2)(cid:10)g(zk−1/2), zk−1/2 − z(cid:63)
result follows by dividing by K.

k

(cid:11) − αVzk−1(z(cid:63)

k)(cid:3) ≤ εinner for every k, and the

B.3 Proof of Proposition 4

We provide a convergence result for the variance-reduced stochastic mirror descent scheme in Al-
gorithm 3. We ﬁrst state the following helper bound which is an application of Lemma 15. It is
immediate from the variance bound of local-centered estimators (Property 2 of Deﬁnition 4) and
the fact that all local norms (whether the domains are balls or simplices) are quadratic.

Lemma 14. For any w ∈ Z, (L, (cid:15))-centered-local estimator ˜gw0 satisﬁes

E (cid:107)˜gw0(z) − g(z)(cid:107)2

w ≤ L2Vw0(z).

Lemma 15. Let (cid:107)·(cid:107)D be a quadratic norm in a diagonal matrix, e.g. for some D = diag(d) and
d ≥ 0 entrywise, let (cid:107)x(cid:107)2

i . Then, if X is a random vector, we have

D = (cid:80) dix2

Proof. This follows from the deﬁnition of variance:

E (cid:107)X − E [X](cid:107)2

D ≤ E (cid:107)X(cid:107)2

D .

E (cid:107)X − E [X](cid:107)2

D = E (cid:107)X(cid:107)2

D − (cid:107)E [X](cid:107)2

D ≤ E (cid:107)X(cid:107)2

D .

Proposition 4. Let (Z, (cid:107)·(cid:107)·, r, Θ, clip) be any local norm setup. Let w0 ∈ Z, α ≥ εinner > 0,
and ˜gw0 be an L-centered-local estimator for some L ≥ α. Assume the domain is bounded by
maxz∈Z (cid:107)z(cid:107) ≤ D, that g is L-Lipschitz, i.e. (cid:107)g(z) − g(z(cid:48))(cid:107)∗ ≤ L (cid:107)z − z(cid:48)(cid:107), that g is LD-bounded,
α2 , and ϕ = εinner
i.e. maxz∈Z (cid:107)g(z)(cid:107)∗ ≤ LD, and that ˆw0 = w0. Then, for η = α
,
Algorithm 3 outputs a point ˆw ∈ Z such that

10L2 , T ≥ 6

ηα ≥ 60L2

6

E max
u∈Z

[(cid:104)g( ˜w), ˜w − u(cid:105) − αVw0(u)] ≤ εinner,

i.e. Algorithm 3 is an (α, εinner)-relaxed proximal oracle.

Proof. For any u ∈ Z, and deﬁning ˜∆t := ˜g( ˆwt) − g(w0) and ∆t := g( ˆwt) − g(w0), we have

(cid:88)

t∈[T ]

(cid:104)ηg(wt), wt − u(cid:105) =

(cid:88)

(cid:69)
(cid:68)
clip(η ˜∆t) + ηg(w0), wt − u

+

(cid:88)

(cid:68)

(cid:69)
η∆t − clip(η ˜∆t), wt − u

(cid:104)ηg(wt) − ηg( ˆwt), wt − u(cid:105) .

t∈[T ]

+

t∈[T ]
(cid:88)

t∈[T ]

57

(24)

(71)

We proceed to bound the three terms on the right hand side of (71) in turn. For the ﬁrst term,

recall the guarantees for the “ideal” iterates of Algorithm 3,

w(cid:63)

t = arg min

w∈Z

(cid:110)(cid:68)

clip(η ˜∆t) + ηg(w0), w

(cid:69)

+

αη
2

Vw0(w) + Vwt−1(w)

(cid:111)

.

By using the optimality conditions of these iterates, deﬁning Q(z) := (cid:104)ηg(w0), z(cid:105) + αη
clip(η ˜∆t), and deﬁning for notational convenience w(cid:63)

T +1 := u,

2 Vw0(z), γt :=

(cid:104)γt−1 + ∇Q(w(cid:63)

t ), w(cid:63)

t − u(cid:105) ≤

(cid:88)

t∈[T ]

=

(cid:88)

t∈[T ]
(cid:88)

t∈[T ]

(cid:10)−∇Vwt−1(w(cid:63)

t ), w(cid:63)

t − u(cid:11)

(cid:0)Vwt−1(u) − Vw(cid:63)

t

(u) − Vwt−1(w(cid:63)

t )(cid:1)

(72)

= Vw0(u) +

(cid:88)

t∈[T ]

(cid:0)Vwt(u) − Vw(cid:63)

t

(u)(cid:1) −

T
(cid:88)

t=0

Vwt(w(cid:63)

t+1).

We thus have the chain of inequalities, recalling γ0 = 0,

(cid:88)

(cid:69)
(cid:68)
clip(η ˜∆t) + ηg(w0), wt − u

+

αη
2

(cid:88)

t∈[T ]

(cid:104)∇Vw0(w(cid:63)

t ), w(cid:63)

t − u(cid:105)

t∈[T ]

=

T
(cid:88)

t=0

(cid:104)γt, wt − u(cid:105) +

(cid:88)

t∈[T ]

(cid:104)∇Q(w(cid:63)

t ), w(cid:63)

t − u(cid:105) +

(cid:104)ηg(w0), wt − w(cid:63)
t (cid:105)

(cid:88)

t∈[T ]

(i)
≤Vw0(u) +

(cid:88)

(cid:0)Vwt(u) − Vw(cid:63)

t

(u)(cid:1) +

T
(cid:88)

t=0

(cid:0)(cid:104)γt, wt − w(cid:63)

t+1(cid:105) − Vwt(w(cid:63)

t+1)(cid:1) +

(cid:104)ηg(w0), wt − w(cid:63)
t (cid:105)

(cid:88)

t∈[T ]

t∈[T ]

(ii)
≤ Vw0(u) + 2ηϕT +

T
(cid:88)

(cid:16)

t=0

(cid:104)clip(η ˜∆t), wt − w(cid:63)

t+1(cid:105) − Vwt(w(cid:63)

t+1)

≤ Vw0(u) + 2ηϕT +

(cid:17) (iii)

T
(cid:88)

t=0

η2 (cid:13)
˜∆t
(cid:13)
(cid:13)

.

(cid:13)
2
(cid:13)
(cid:13)
wt
(73)

Here, (i) was by rearranging (72) via the equality

(cid:104)γt−1, w(cid:63)

t − u(cid:105) =

(cid:88)

t∈[T ]

T
(cid:88)

t=0

(cid:104)γt, wt − u(cid:105) −

T
(cid:88)

t=0

(cid:10)γt, wt − w(cid:63)

t+1

(cid:11) ,

satisﬁed by the
(ii) was by the conditions maxu
iterates, and (iii) was by the property of clipping (15), as deﬁned in the problem setup. Now by
rearranging and using the three-point property of Bregman divergence (i.e. (cid:104)−∇Vw(cid:48)(w), w − u(cid:105) =
Vw(cid:48)(u) − Vw(u) − Vw(cid:48)(w)), it holds that

(u)(cid:3) ≤ ηϕ and (cid:107)wt − w(cid:63)

(cid:2)Vwt(u) − Vw(cid:63)

t (cid:107) ≤ ϕ
LD

t

(cid:88)

(cid:68)

(cid:69)
clip(η ˜∆t) + ηg(w0), wt − u

≤Vw0(u) + 2ηϕT + η2

t∈[T ]

≤Vw0(u) + 3ηϕT + η2

T
(cid:88)

t=0

T
(cid:88)

t=0

(cid:13)
˜∆t
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

wt

+

(cid:13)
˜∆t
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

wt

+

αη
2

αη
2

(cid:88)

t∈[T ]

(cid:88)

t∈[T ]

(Vw0(u) − Vw0(w(cid:63)

t ))

(Vw0(u) − Vw0( ˆwt)) ,

(74)

58

where the second inequality follows from the condition Vw0( ˆwt) − Vw0(w(cid:63)
of Algorithm 3. To bound the second term of (71), we deﬁne the ghost iterate sequnce {st} by

satisﬁed by iterates

t ) ≤ 2ϕ
α

st = arg min

s∈Z

(cid:26) 1
2

(cid:10)η∆t−1 − clip(η ˜∆t−1), s(cid:11) + Vst−1(s)

(cid:27)

with s0 = w0.

Applying Lemma 13 with Q = 0 and γt = 1

2 (η∆t − clip(η ˜∆t)), and observing that again γ0 = 0,

(cid:10)η∆t − clip(η ˜∆t), st − u(cid:11)

(cid:88)

t∈[T ]

≤2Vw0(u) +

T
(cid:88)

t=0

(cid:111)
(cid:110)
(cid:104)η∆t − clip(η ˜∆t), st − st+1(cid:105) − 2Vst(st+1)

≤2Vw0(u) + η2

T
(cid:88)

t=0

(cid:18)

(cid:107)∆t(cid:107)2

∗ +

(cid:13)
˜∆t
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

st

(cid:19)

.

Here, we used properties (15) and (16). Consequently,

(cid:88)

(cid:69)
(cid:68)
η∆t − clip(η ˜∆t), wt − u

t∈[T ]
(cid:88)

t∈[T ]

=

(cid:68)
η∆t − clip(η ˜∆t), wt − st

(cid:69)

+

(cid:88)

(cid:68)

(cid:69)
η∆t − clip(η ˜∆t), st − u

t∈[T ]

(75)

≤ 2Vw0(u) + η2

T
(cid:88)

(cid:18)

t=0

(cid:107)∆t(cid:107)2

∗ +

(cid:19)

(cid:13)
˜∆t
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

st

(cid:88)

(cid:68)
η∆t − clip(η ˜∆t), wt − st

(cid:69)

.

+

t∈[T ]

To bound the third term of (71), we use the condition (cid:107)wt − ˆwt(cid:107) ≤ ϕ
LD

which implies

(cid:88)

t∈[T ]

(cid:104)ηg(wt) − ηg( ˆwt), wt − u(cid:105) ≤

(cid:88)

t∈[T ]

(cid:107)ηg(wt) − ηg( ˆwt)(cid:107)∗ (cid:107)wt − u(cid:107) ≤ 2ηϕT.

(76)

Combining our three bounds (74), (75), and (76) in the context of (71), using ˆw0 = w0 and ˜g(w0) =
g(w0), and ﬁnally dividing through by ηT , we obtain

1
T

(cid:88)

t∈[T ]

(cid:104)g(wt), wt − u(cid:105) −

(cid:18) 3
ηT

+

(cid:19)

α
2

Vw0(u)

≤ 5ϕ +

1
T

(cid:18)

(cid:88)

t∈[T ]

η

(cid:13)
˜∆t
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

wt

+ η

(cid:13)
˜∆t
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

st

+ η (cid:107)∆t(cid:107)2

∗ +

(cid:28)

∆t −

1
η

clip(η ˜∆t), wt − st

(cid:29)

−

α
2

(cid:19)

Vw0( ˆwt)

.

(77)

Since T ≥ 6
αη

, taking a supremum over u ∈ Z in (77) and then an expectation yields

E sup
u∈Z





+

1
T

E





1
T

(cid:88)

t∈[T ]

(cid:104)g(wt), wt − u(cid:105) − αVw0(u)

 ≤ 5ϕ



(cid:88)

t∈[T ]

η

(cid:13)
˜∆t
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

wt

+ η

(cid:13)
˜∆t
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

st

+ η (cid:107)∆t(cid:107)2

∗ +

(cid:28)

∆t −

1
η

clip(η ˜∆t), wt − st

(cid:29)

−

α
2

(78)



Vw0( ˆwt)

 .

59

We will show the second line of (78) is nonpositive. To do so, observe for each t ∈ [T ], by the
property (17) of clip(·), since conditional on wt, st, ˜∆t is unbiased for deterministic ∆t,
(cid:13)
˜∆t
(cid:13)
(cid:13)

clip(η ˜∆t), wt − st

clip(η ˜∆t), wt − st

(cid:13)
˜∆t
(cid:13)
(cid:13)

. (79)

˜∆t −

∆t −

(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

≤ η

+ η

=

(cid:28)

(cid:28)

E

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
E
(cid:12)
(cid:12)

wt

st

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
η

1
η

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Finally, by using property 2 of the centered-local estimator ˜∆t, as well as Remark 1, we have for
each t ∈ [T ],
(cid:20)
η

≤ ηL2Vw0( ˆwt), and η (cid:107)∆t(cid:107)2

≤ ηL2Vw0( ˆwt), E

∗ ≤ ηL2Vw0( ˆwt).

(cid:20)
η

(80)

E

(cid:21)

(cid:21)

(cid:13)
˜∆t
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

wt

(cid:13)
˜∆t
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

st

Using bounds (79) and (80) in (78), as well as η ≤ α

10L2 ,





1
T

(cid:88)

t∈[T ]

E sup
u∈Z

(cid:104)g(wt), wt − u(cid:105) − αVw0(u)

 ≤ 5ϕ.

(81)



For the ﬁnal claim, denote the true average iterate by ¯w := 1
T

(cid:80)

t∈[T ] wt. We have ∀u ∈ Z,

(cid:104)g( ˜w), ˜w − u(cid:105)

(i)
= − (cid:104)g( ˜w), u(cid:105) = (cid:104)g( ¯w) − g( ˜w), u(cid:105) + (cid:104)g( ¯w), ¯w − u(cid:105)

(ii)
≤ ϕ + (cid:104)g( ¯w), ¯w − u(cid:105)

=

1
T

(cid:88)

t∈[T ]

(cid:104)g(wt), wt − u(cid:105) + ϕ.

Here, (i) used the fact that linearity of g gives (cid:104)g(z), z(cid:105) = 0, ∀z ∈ Z, and (ii) used Hölder’s inequality
(cid:104)g( ¯w) − g( ˜w), u(cid:105) ≤ (cid:107)g( ¯w) − g( ˜w)(cid:107)∗ (cid:107)u(cid:107) ≤ 2LD (cid:107) ˜w − ¯w(cid:107) ≤ ϕ following from the approximation
guarantee (cid:107) ˜w − ¯w(cid:107) ≤ ϕ
2LD

. Combining with (81) yields the conclusion, as 6ϕ = εinner.

C Deferred proofs for sublinear methods

C.1 (cid:96)2-(cid:96)2 sublinear coordinate method
Assumptions. The algorithm in this section will assume access to entry queries, (cid:96)1 norms of rows
and columns, and (cid:96)1 sampling distributions for rows and columns. Further, it assumes the ability
to sample a row or column proportional to its squared (cid:96)1 norm; given access to all (cid:96)1 norms, the
algorithm may spend O(m + n) constructing these sampling oracles in O(m + n) time, which does
not aﬀect its asymptotic runtime. We use the (cid:96)2-(cid:96)2 local norm setup (Table 6). We deﬁne

L2,2

co :=

(cid:115) (cid:88)

i∈[m]

(cid:107)Ai:(cid:107)2

1 +

(cid:88)

j∈[n]

(cid:107)A:j(cid:107)2
1.

(82)

C.1.1 Gradient estimator

For z ∈ Bn × Bm, we specify two distinct choices of sampling distributions p(z), q(z) which obtain
the optimal Lipschitz constant. The ﬁrst one is an oblivious distribution:

pij(z) :=

(cid:107)Ai:(cid:107)2
1
k∈[m] (cid:107)Ak:(cid:107)2
1

(cid:80)

·

|Aij|
(cid:107)Ai:(cid:107)1

and qij(z) :=

(cid:107)A:j(cid:107)2
1
k∈[n] (cid:107)A:k(cid:107)2
1

(cid:80)

·

|Aij|
(cid:107)A:j(cid:107)1

.

(83)

60

The second one is a dynamic distribution:

pij(z) :=

2

[zy]i
(cid:107)zy(cid:107)2
2

·

|Aij|
(cid:107)Ai:(cid:107)1

and qij(z) :=

2

[zx]j
(cid:107)zx(cid:107)2
2

·

|Aij|
(cid:107)A:j(cid:107)1

.

(84)

We now state the local properties of each estimator.

Lemma 16. In the (cid:96)2-(cid:96)2 setup, estimator (23) using the sampling distribution in (83) or (84) is
an L2,2

co -local estimator.

Proof. For convenience, we restate the distributions here: they are respectively

pij(z) :=

(cid:107)Ai:(cid:107)2
1
k∈[m] (cid:107)Ak:(cid:107)2
1

(cid:80)

·

|Aij|
(cid:107)Ai:(cid:107)1

and qij(z) :=

(cid:107)A:j(cid:107)2
1
k∈[n] (cid:107)A:k(cid:107)2
1

(cid:80)

·

|Aij|
(cid:107)A:j(cid:107)1

and

pij(z) :=

2

[zy]i
(cid:107)zy(cid:107)2
2

·

|Aij|
(cid:107)Ai:(cid:107)1

and qij(z) :=

2

[zx]j
(cid:107)zx(cid:107)2
2

·

|Aij|
(cid:107)A:j(cid:107)1

.

Unbiasedness holds by deﬁnition. We ﬁrst show the variance bound on the x block for distribu-

tion (83):

(cid:104)

E

(cid:107)˜gx(z)(cid:107)2
2

(cid:105)

=

(cid:88)

pij(z) ·

i∈[m],j∈[n]

(cid:19)2

(cid:18) Aij[zy]i
pij(z)

=

A2
ij[zy]2
i
pij(z)

(cid:88)

i∈[m],j∈[n]


(cid:88)

=

i∈[m],j∈[n]

|Aij|
(cid:107)Ai:(cid:107)1



[zy]2
i ·



(cid:88)

i∈[m]

(cid:107)Ai:(cid:107)2
1

 =

(cid:88)

i∈[m]

(cid:107)Ai:(cid:107)2
1.

Similarly, we have

(cid:104)

E

(cid:107)˜gy(z)(cid:107)2
2

(cid:105)

≤

(cid:88)

(cid:107)A:j(cid:107)2
1.

j∈[n]

Now, we show the variance bound on the x block for distribution (84):

E

(cid:104)
(cid:107)˜gx(z)(cid:107)2
2

(cid:105)

=

(cid:88)

pij(z) ·

(cid:19)2

(cid:18) Aij[zy]i
pij(z)

i∈[m],j∈[n]
(cid:88)

=

i∈[m],j∈[n]

|Aij|(cid:107)Ai:(cid:107)1 (cid:107)zy(cid:107)2

2 ≤

A2
ij[zy]2
i
pij(z)

(cid:88)

=

(cid:88)

i∈[m],j∈[n]
(cid:107)Ai:(cid:107)2
1,

i∈[m]

and a similar bound holds on the y block.

We remark that using the oblivious distribution (83) saves a logarithmic factor in the runtime
compared to the dynamic distribution, so for the implementation of all of our (cid:96)2-(cid:96)2 algorithms we
will use the oblivious distribution.

61

C.1.2

Implementation details

In this section, we discuss the details of how to leverage the IterateMaintainer2 data structure
to implement the iterations of our algorithm. The algorithm we analyze is Algorithm 1, using the
local estimator deﬁned in (23), and the distribution (83). We choose

η =

(cid:15)
L2,2
co

(cid:16)

9

(cid:17)2

and T =

(cid:25)

(cid:24) 6Θ
η(cid:15)

≥

(cid:17)2

(cid:16)

L2,2
co

54

(cid:15)2

.

Lemma 16 implies that our estimator satisﬁes the remaining requirements for Proposition 2, giving
the duality gap guarantee in T iterations. In order to give a runtime bound, we claim that each
iteration can be implemented in constant time, with O(m + n) additional runtime.

Data structure initializations and invariants. At the start of the algorithm, we spend O(m+
2.Init(0n, b), IMy
are instan-
n) time initializing data structures via IMx
tiations of IterateMaintainer2 data structures. Throughout, we preserve the invariant that the
correspond to the x and y blocks of the current iterate zt at iteration
points maintained by IMx
t of the algorithm. We note that we instantiate data structures which do not support Sample().

2.Init(0m, c), where IMx

2, IMy

2, IMy

2

2

Iterations. For simplicity, we only discuss the runtime of updating the x block as the y block
follows symmetrically. We divide each iteration into the following substeps, each of which we show
run in constant time. We refer to the current iterate by z = (zx, zy), and the next iterate by
w = (wx, wy).

Sampling. Because the distribution is oblivious, sampling both i and j | i using precomputed data
structures takes constant time.

Computing the gradient estimator. To compute c := Aij[zy]i/pij, it suﬃces to compute Aij, [zy]i,
and pij. Using an entry oracle for A obtains Aij in constant time, and calling IMy
2.Get(i) takes
constant time. Computing pij using the precomputed row norms and the values of Aij, [zy]i takes
constant time.

Performing the update. For the update corresponding to a proximal step, we have

wx ← ΠX (zx − η˜gx(z)) =

zx − η˜gx(z)
max{(cid:107)zx − η˜gx(z)(cid:107)2 , 1}

.

We have computed ˜gx(z), so to perform this update, we call

IMx
IMx
IMx
IMx

2.AddSparse(j, −ηc);
2.AddDense(−η);
2.Scale(max{IMx.GetNorm(), 1}−1);
2.UpdateSum().

By assumption, each operation takes constant time because we do not support Sample in our
instances of IM2, giving the desired iteration complexity. It is clear that at the end of performing
these operations, the invariant that IMx
2

maintains the x block of the iterate is preserved.

62

Averaging. After T iterations, we compute the average point ¯zx:

[¯zx]j ←

1
T

· IMx

2.GetSum(j), ∀j ∈ [n].

By assumption, this takes O(n) time.

C.1.3 Algorithm guarantee

Theorem 6. In the (cid:96)2-(cid:96)2 setup, the implementation in Section C.1.2 has runtime




(cid:17)2

(cid:16)

L2,2
co

O




(cid:15)2

+ m + n




and outputs a point ¯z ∈ Z such that

E Gap(¯z) ≤ (cid:15).

Proof. The runtime bound follows from the discussion in Section C.1.2. The correctness follows
from Proposition 2.

Remark 7. Using our IterateMaintainer2 data structure, the (cid:96)2-(cid:96)2 algorithm of Balamurugan
and Bach [7] runs in time O (cid:0)rcs(cid:107)A(cid:107)2

F /(cid:15)2(cid:1). Our runtime universally improves upon it since

(cid:88)

i∈[m]

(cid:107)Ai:(cid:107)2

1 +

(cid:88)

j∈[n]

(cid:107)A:j(cid:107)2

1 ≤ 2rcs (cid:107)A(cid:107)2
F .

C.2 (cid:96)2-(cid:96)1 sublinear coordinate method
Assumptions The algorithm in this section will assume access to every oracle listed in Section 2.3.
However, for a speciﬁc matrix A, only one of three sampling distributions will be used in the
algorithm; we describe the speciﬁc oracle requirements of each distribution following their deﬁnition.
We use the (cid:96)2-(cid:96)1 local norm setup (Table 6). Throughout this section, we will assume that the linear
term in (23) is g(0) = 0 uniformly.

Finally, in this section we assume access to a weighted variant of IterateMaintainer2, which
takes a nonnegative weight vector w as a static parameter. WeightedIterateMaintainer2
supports
two modiﬁed operations compared to the data structure IterateMaintainer2: its GetNorm() op-
eration returns (cid:113)(cid:80)
, and its Sample() returns coordinate j with probability proportional
to [w]j[x]2
j

(cf. Section 2.4.1). We give the implementation of this extension in Appendix G.

j[w]j[x]2
j

C.2.1 Gradient estimator

For z ∈ Bn × ∆m and desired accuracy (cid:15) > 0, we specify three distinct choices of sampling distri-
butions p(z), q(z). Each of our distributions induces an estimator with diﬀerent properties.

The ﬁrst one is

The second one is

pij(z) :=

|Aij|
(cid:107)Ai:(cid:107)1

· [zy]i and qij(z) :=

A2
ij
(cid:107)A(cid:107)2
F

.

pij(z) :=

|Aij|
(cid:107)Ai:(cid:107)1

· [zy]i and qij(z) :=

[zx]2
(cid:80)

j · 1{Aij (cid:54)=0}
l∈[n] csl · [zx]2
l

.

63

(85)

(86)

Here, we let csj ≤ rcs denote the number of nonzero elements in column A:j. The third one is

pij(z) :=

|Aij|
(cid:107)Ai:(cid:107)1

· [zy]i and qij(z) :=

|Aij| · [zx]2
j
l∈[n] (cid:107)A:l(cid:107)1 · [zx]2
l

(cid:80)

.

(87)

, L2,1,(2)
co

For L2,1,(1)
co

, and L2,1,(3)

to be deﬁned, the estimators induced by these distributions
are local estimators whose guarantees depend on these constants respectively. Furthermore, these
Lipschitz constants are in general incomparable and depend on speciﬁc properties of the matrix.
Therefore, we may choose our deﬁnition of L2,1
co to be the minimum of these constants, by choosing
an appropriate estimator. We now state the local properties of each estimator.

co

Lemma 17. In the (cid:96)2-(cid:96)1 setup, estimator (23) using the sampling distributions in (85), (86), or (87)
is respectively a L2,1,(k)

-local estimator, for k ∈ {1, 2, 3}, and

co

L2,1,(1)
co

:=

(cid:114)

max
i∈[m]

(cid:107)Ai:(cid:107)2

1 + (cid:107)A(cid:107)2
F,

L2,1,(2)
co

:=

(cid:114)

2rcs max
i∈[m]

(cid:107)Ai:(cid:107)2
2,

(cid:115)

L2,1,(3)
co

:=

max
i∈[m]

(cid:107)Ai:(cid:107)2

1 +

(cid:18)

(cid:19) (cid:18)

max
i∈[m]

(cid:107)Ai:(cid:107)1

max
j∈[n]

(cid:107)A:j(cid:107)1

(cid:19)
.

Proof. First, we give the proof for the sampling distribution (85). Unbiasedness holds by deﬁnition.
For the x block, we have the variance bound:

E

(cid:104)
(cid:107)˜gx(z)(cid:107)2
2

(cid:105)

=

(cid:88)

pij(z) ·

(cid:19)2

(cid:18) Aij[zy]i
pij(z)

=

(cid:88)

|Aij|(cid:107)Ai:(cid:107)1[zy]i

i∈[m],j∈[n]

i∈[m],j∈[n]
(cid:88)

=

i∈[m]

(cid:107)Ai:(cid:107)2

1[zy]i ≤ max
i∈[m]

(cid:107)Ai:(cid:107)2
1.

For arbitrary wy, we have the variance bound on the y block:
(cid:19)2(cid:33)

(cid:32)

(cid:104)

E

(cid:105)

(cid:107)˜gy(z)(cid:107)2
wy

(cid:88)

qij(z) ·

[wy]i ·

(cid:18) Aij[zx]j
qij(z)

(cid:88)

=

i∈[m],j∈[n]

[wy]i

A2
ij[zx]2
j
qij(z)

=

=

i∈[m],j∈[n]
(cid:88)

i∈[m],j∈[n]

[wy]i[zx]2

j (cid:107)A(cid:107)2

F ≤ (cid:107)A(cid:107)2
F .

Next, we give the proof for the sampling distribution (86). Unbiasedness holds by deﬁnition. By
Cauchy-Schwarz and our earlier proof, we have the variance bound for the x block:

E

(cid:104)
(cid:107)˜gx(z)(cid:107)2
2

(cid:105)

≤ max
i∈[m]

(cid:107)Ai:(cid:107)2

1 ≤ rcs max
i∈[m]

(cid:107)Ai:(cid:107)2
2 .

For arbitrary wy, we have the variance bound on the y block, where Si := (cid:8)j | 1Aij (cid:54)=0 = 1(cid:9):

(cid:104)

E

(cid:107)˜gy(z)(cid:107)2
wy

(cid:105)

=

(cid:88)

≤

i∈[m],j∈Si
(cid:88)

i∈[m],j∈Si

(cid:32)

qij(z) ·

[wy]i ·

(cid:19)2(cid:33)

(cid:18) Aij[zx]j
qij(z)

(cid:88)

=

i∈[m],j∈Si

[wy]i

A2
ij[zx]2
j
qij(z)

[wy]iA2

ijrcs ≤ rcs max
k∈[m]

(cid:107)Ak:(cid:107)2
2 .

64

Finally, we give the proof for the sampling distribution (87). Unbiasedness and the variance bound
for the x block again hold. For arbitrary wy, we have the variance bound on the y block:

(cid:104)

E

(cid:107)˜gy(z)(cid:107)2
wy

(cid:105)

=

(cid:88)

(cid:32)

qij(z) ·

[wy]i ·

(cid:19)2(cid:33)

(cid:18) Aij[zx]j
qij(z)

i∈[m],j∈[n]


(cid:88)

≤



i∈[m],j∈[n]





[wy]i|Aij|





(cid:88)

l∈[n]

(cid:107)A:l(cid:107)1 [zx]2
l



(cid:88)

i∈[m],j∈[n]

[wy]i

A2
ij[zx]2
j
qij(z)

=



(cid:18)

(cid:19) (cid:18)

(cid:19)

≤

max
k∈[m]

(cid:107)Ak:(cid:107)1

max
l∈[n]

(cid:107)A:l(cid:107)1

.

By using the deﬁnitions of L2,1,(1)

co

, L2,1,(2)
co

, and L2,1,(3)

co

, we deﬁne the constant

(cid:115)

L2,1

co :=

max
i∈[m]

(cid:107)Ai:(cid:107)2

1 + min

(cid:18)

(cid:107)A(cid:107)2

F , rcs max
i∈[m]

(cid:18)

(cid:107)Ai:(cid:107)2
2 ,

(cid:19) (cid:18)

max
i∈[m]

(cid:107)Ai:(cid:107)1

max
j∈[n]

(cid:107)A:j(cid:107)1

(cid:19)(cid:19)

.

(88)

In particular, by choosing whichever of the distributions (85), (86), or (87) yields the minimial
Lipschitz constant, we may always ensure we have a L2,1
co -local estimator. We now discuss the speciﬁc
precomputed quantities each estimator requires, among those listed in Section 2.3. All distributions
require access to entry queries, (cid:96)1 norms of rows, and (cid:96)1 sampling distributions for rows.

• Using the sampling distribution (85) requires additional access to (cid:96)2 sampling distributions

for rows and columns and the Frobenius norm of A.

• Using the sampling distribution (86) requires additional access to uniform sampling nonzero

entries of columns.

• Using the sampling distribution (87) requires additional access to (cid:96)1 norms of columns and (cid:96)1

sampling distributions for columns.

C.2.2

Implementation details

In this section, we discuss the details of how to leverage the appropriate IterateMaintainer1
and IterateMaintainer2 data structures to implement the iterations of our algorithm. The algo-
rithm we analyze is Algorithm 1, using the local estimator deﬁned in (23), and the best choice of
distribution among (85), (86), (87). We choose

η =

(cid:15)
L2,1
co

(cid:16)

9

(cid:17)2

and T =

(cid:25)

(cid:24) 6Θ
η(cid:15)

≥

(cid:17)2

(cid:16)

L2,1
co

54

log(2m)

(cid:15)2

.

Lemma 17 implies that our estimator satisﬁes the remaining requirements for Proposition 2, giving
the duality gap guarantee in T iterations. In order to give a runtime bound, we claim that each
iteration can be implemented in O(log mn) time, with O(m + n) additional runtime. For simplicity,
because most of the algorithm implementation details are exactly same as the discussion of Sec-
tion 4.1.2 for the simplex block y ∈ Y, and exactly the same as the discussion of Section C.1.2 for

65

the ball block x ∈ X , we discuss the diﬀerences here, namely the implementations of sampling and
gradient computation.

We assume that we have initialized IMy
1

, an
instantiation of IterateMaintainer2. When the choice of distribution is (86), we also assume
initialized with the weight vec-
access to WIMx
2
tor of nonzero counts of columns of the matrix; similarly, for distribution (87) we instantiate a
WeightedIterateMaintainer2

, an instantiation of WeightedIterateMaintainer2

, an instantiation of IterateMaintainer1, and IMx
2

with the weight vector of (cid:96)1 norms of each column.

Sampling. Recall that

pij(z) :=

|Aij|
(cid:107)Ai:(cid:107)1

· [zy]i.

We ﬁrst sample coordinate i via IMy
structure corresponding to Ai: in O(1). Next, to sample from the distribution

1.Sample() in O(log m), and then sample j using the data

qij(z) :=

A2
ij
(cid:107)A(cid:107)2
F

required by (85), we can sample a coordinate of the matrix proportional to its square in constant
time using our matrix access. To sample from the distribution

qij(z) :=

[zx]2
(cid:80)

j · 1{Aij (cid:54)=0}
l∈[n] csl · [zx]2
l

required by (86), we ﬁrst sample coordinate j via WIMx
2.Sample() in O(log n), and then uniformly
sample a coordinate i amongst the entries of A:j for which the indicator labels as nonzero. Finally,
to sample from the distribution

qij(z) :=

(cid:80)

|Aij| · [zx]2
j
l∈[n] (cid:107)A:l(cid:107)1 · [zx]2
l

required by (87), we sample coordinate j via WIMx
portional to its absolute value using a column sampling oracle.

2.Sample(), and then sample a coordinate i pro-

Computing the gradient estimator. By the proofs of Theorem 1 and Theorem 6, it suﬃces to compute
pixjx, qiyjy
2.GetNorm()
when appropriate, and using access to precomputation allows us to obtain all relevant quantities
for the computations in O(1).

in constant time. Calling IMx

2.GetNorm(), and WIMx

2.Get(j), IMy

1.Get(i), IMx

C.2.3 Algorithm guarantee

Theorem 7. In the (cid:96)2-(cid:96)1 setup, the implementation in Section C.2.2 has runtime

(cid:17)2

L2,1
co



(cid:16)

O




log m log(mn)

(cid:15)2






+ m + n

and outputs a point ¯z ∈ Z such that

E Gap(¯z) ≤ (cid:15).

66

Proof. The runtime bound follows from the discussion in Section C.2.2. The correctness follows
from Proposition 2.

Remark 8. Using our IterateMaintainer1 and IterateMaintainer2 data structures, the (cid:96)2-(cid:96)1
2 log2(mn)/(cid:15)2). By noting the
[10] runs in time O(rcs maxi∈[m] (cid:107)Ai:(cid:107)2
algorithm of Clarkson et al.
(cid:16)
L2,1
.
≤ 2rcs maxi∈[m] (cid:107)Ai:(cid:107)2
co
2

, our runtime universally improves upon it since

deﬁnition of L2,1,(2)

(cid:17)2

co

D Deferred proofs for variance-reduced methods

D.1 Helper proofs
Lemma 3. For y, y(cid:48) ∈ ∆m, divergence Vy(y(cid:48)) generated by r(y) = (cid:80)

i∈[m][y]i log[y]i − [y]i satisﬁes

Vy(y(cid:48)) ≥

1
2

(cid:13)y(cid:48) − y(cid:13)
(cid:13)
2
(cid:13)

3
2y+y(cid:48)

=

1
2

(cid:88)

i∈[m]

([y]i − [y(cid:48)]i)2
2
3 [y]i + 1
3 [y(cid:48)]i

.

Proof. Let γ ∈ Rm. Note that for every τ ∈ [0, 1] (with elementwise multiplication, division

and square root), (cid:104)γ, y − y(cid:48)(cid:105) =

(cid:28)

γ(cid:112)(1 − τ )y + τ y(cid:48),

√

y−y(cid:48)
(1−τ )y+τ y(cid:48)

(cid:29)

. Therefore, using 2 (cid:104)u, w(cid:105) ≤

(cid:107)u(cid:107)2

2 + (cid:107)w(cid:107)2

2

, we have for every τ ∈ [0, 1],

2 (cid:10)γ, y − y(cid:48)(cid:11) ≤

(cid:88)

i∈[m]

(cid:0)(1 − τ )[y]i + τ [y(cid:48)]i

(cid:1) [γ]2

i +

(cid:88)

i∈[m]

([y]i − [y(cid:48)]i)2
(1 − τ )[y]i + τ [y(cid:48)]i

.

Applying the double integral (cid:82) 1
and (cid:82) 1
0 τ · dτ = 1

0 dt (cid:82) t

gives

6

0 dt (cid:82) t

0 dτ to both sides of the inequality, and using (cid:82) 1

0 dt (cid:82) t

0 1 · dτ = 1

2

(cid:10)γ, y − y(cid:48)(cid:11) ≤

(cid:88)

i∈[m]

(cid:18) 1
3

[y]i +

(cid:19)

1
6

[y(cid:48)]i

[γ]2

i +

(cid:90) 1

0

dt

(cid:90) t

(cid:88)

0

i∈[m]

([y]i − [y(cid:48)]i)2
(1 − τ )[y]i + τ [y(cid:48)]i

dτ.

Identifying the double integral with the expression

Vy(y(cid:48)) =

(cid:18)

y(cid:48)
i log

y(cid:48)
i
yi

(cid:88)

i∈[m]

+ yi − y(cid:48)
i

(cid:90) 1

(cid:19)

=

dt

(cid:90) t

(cid:88)

0

0

i∈[m]

i)2

(yi − y(cid:48)
(1 − τ )yi + τ y(cid:48)
i

dτ.

(89)

for the divergence induced by entropy, the result follows by choosing [γ]i = [y]i−[y(cid:48)]i
3 [y(cid:48)]i

3 [y]i+ 1

2

.

Lemma 5. Let x(cid:48) ∈ ∆n be a β-padding of x ∈ ∆n. Then,

j log x(cid:48)
x(cid:48)

j −

(cid:88)

j∈[n]

(cid:88)

j∈[n]

xj log xj ≤

βn
e

+ β(1 + β).

Proof. Letting ˜x be the point inducing x(cid:48) in Deﬁnition 2, we have

j log x(cid:48)
x(cid:48)

j −

(cid:88)

j∈[n]

(cid:88)

j∈[n]

xj log xj =







(cid:88)

j∈[n]

j log x(cid:48)
x(cid:48)

j −

(cid:88)

+



j∈[n]

˜xj log ˜xj −

67



˜xj log ˜xj





xj log xj

 .

(cid:88)

j∈[n]

(cid:88)

j∈[n]

We bound these two terms separately. For the ﬁrst term, let (cid:107)˜x(cid:107)1 = 1 + b, for some b ≤ β; we

see that entrywise, (1 + b)x(cid:48)

j = ˜xj. For each j ∈ [n],

j log x(cid:48)
x(cid:48)

j − ˜xj log ˜xj = x(cid:48)
= bx(cid:48)

j log (cid:0)(1 + b)x(cid:48)

j

(cid:1)

j log(1 + b)

j log

j log x(cid:48)
j − (1 + b)x(cid:48)
1
− (1 + b)x(cid:48)
x(cid:48)
j
1
x(cid:48)
j

j log

β
e

≤

.

≤ bx(cid:48)

The ﬁrst inequality was due to nonnegativity of (1 + b) log(1 + b) and x(cid:48)
j
to the maximum value of the scalar function z log 1
z
over all coordinates yields that the ﬁrst term is bounded by βn/e.
For the second term, we have by integration that entrywise

, and the second was due
over the nonnegative reals being 1/e. Summing

˜xj log ˜xj − xj log xj =

≤

≤

(cid:90) 1

α=0

(cid:90) 1

α=0

(cid:90) 1

α=0

(1 + log(xj + α(˜xj − xj)))(˜xj − xj)dα

(1 + log(˜xj))(˜xj − xj)dα

˜xj(˜xj − xj)dα ≤ (1 + β)|˜xj − xj|.

The ﬁrst inequality is by ˜xj ≥ xj for all j ∈ [n] and log(x) is monotone in x > 0; the second is by
log(x) ≤ x − 1 for all x > 0; the third again uses ˜xj ≥ xj and that ˜xj ≤ (cid:107)˜x(cid:107)1 ≤ 1 + β, and the
second condition in Deﬁnition 2. Finally, combining yields the desired

j log x(cid:48)
x(cid:48)

j −

(cid:88)

j∈[n]

(cid:88)

j∈[n]

xj log xj ≤

βn
e

+ β(1 + β).

D.2 (cid:96)2-(cid:96)2 variance-reduced coordinate method
Assumptions. As in Section C.1, the algorithm in this section will assume access to entry queries,
(cid:96)1 norms of rows and columns, and (cid:96)1 sampling distributions for rows and columns, and the ability
to sample a row or column proportional to its squared (cid:96)1 norm. We use the (cid:96)2-(cid:96)2 local norm setup
(cf. Table 6). Again, we deﬁne

L2,2

co :=

(cid:115) (cid:88)

i∈[m]

(cid:107)Ai:(cid:107)2

1 +

(cid:88)

j∈[n]

(cid:107)A:j(cid:107)2
1.

D.2.1 Gradient estimator

Given reference point w0 ∈ Bn × Bm, for z ∈ Bn × Bm, we specify two distinct sampling distribu-
tions p(z; w0), q(z; w0) which obtain the optimal Lipschitz constant. The ﬁrst one is an oblivious
distribution:

pij(z; w0) :=

(cid:107)Ai:(cid:107)2
1
k∈[m] (cid:107)Ak:(cid:107)2
1

(cid:80)

·

|Aij|
(cid:107)Ai:(cid:107)1

and qij(z; w0) :=

(cid:107)A:j(cid:107)2
1
k∈[n] (cid:107)A:k(cid:107)2
1

(cid:80)

·

|Aij|
(cid:107)A:j(cid:107)1

.

(90)

68

The second one is a dynamic distribution:

pij(z; w0) :=

[wy
(cid:13)
(cid:13)wy

0 − zy]2
i
0 − zy(cid:13)
2
(cid:13)
2

·

|Aij|
(cid:107)Ai:(cid:107)1

and qij(z; w0) :=

We now state the local properties of each estimator.

[wx
(cid:107)wx

0 − zx]2
j
0 − zx(cid:107)2

2

·

|Aij|
(cid:107)A:j(cid:107)1

.

(91)

Lemma 18. In the (cid:96)2-(cid:96)2 setup, estimator (25) using the sampling distribution in (90) or (91) is a
√

2L2,2

co -centered-local estimator.

Proof. Unbiasedness holds by deﬁnition in both cases. We ﬁrst show the variance bound on the x
block for distribution (90):

E

(cid:104)(cid:13)
(cid:13)˜gx

w0(z) − gx(w0)(cid:13)
2
(cid:13)
2

(cid:105)

=

(cid:88)

pij(z; w0) ·

(cid:18) Aij[zy − wy
0]i
pij(z; w0)

(cid:19)2

=

[zy − wy

0]2
i ·





(cid:88)

(cid:107)Ak:(cid:107)2
1



k∈[m]

A2

ij[zy − wy
0]2
i
pij(z; w0)

(cid:88)

i∈[m],j∈[n]


i∈[m],j∈[n]

(cid:88)

=

i∈[m],j∈[n]


|Aij|
(cid:107)Ai:(cid:107)1



=



(cid:88)

i∈[m]

(cid:107)Ai:(cid:107)2
1



(cid:13)
(cid:13)zy − wy
0

(cid:13)
2
2 .
(cid:13)

Similarly, we have

E

(cid:104)(cid:13)
(cid:13)˜gy

w0(z) − gy(w0)(cid:13)
2
(cid:13)
2

(cid:105)



≤



(cid:88)

(cid:107)A:j(cid:107)2
1


 (cid:107)zx − wx

0(cid:107)2
2 .

j∈[n]

Combining these and using (cid:107)zx − wx
Now, we show the variance bound on the x block for distribution (91):

(cid:13)zy − wy
0

0(cid:107)2

(cid:13)
2
2 = 2Vw0(z) yields the desired variance bound.
(cid:13)

2 + (cid:13)

E

(cid:104)(cid:13)
(cid:13)˜gx

w0(z) − gx(w0)(cid:13)
2
(cid:13)
2

(cid:105)

=

(cid:88)

pij(z; w0) ·

(cid:18) Aij[zy − wy
0]i
pij(z; w0)

(cid:19)2

=

(cid:88)

i∈[m],j∈[n]

A2

ij[zy − wy
0]2
i
pij(z; w0)

i∈[m],j∈[n]
(cid:88)

=

|Aij|(cid:107)Ai:(cid:107)1

(cid:13)
(cid:13)zy − wy
0

(cid:13)
2
(cid:13)
2

i∈[m],j∈[n]




=



(cid:88)

i∈[m]

(cid:107)Ai:(cid:107)2
1



(cid:13)
(cid:13)zy − wy
0

(cid:13)
2
2 .
(cid:13)

and a similar bound holds on the y block.

Again, for algorithmic considerations (i.e. an additional logarithmic factor in the complexity of

sampling from (91)), we will only discuss using the oblivious distribution (90) in our algorithm.

D.2.2

Implementation details

In this section, we discuss the details of how to leverage the IterateMaintainer2 data structure to
implement the iterations of our algorithm. The algorithm we analyze is Algorithm 2 with K = αΘ/(cid:15),

69

using Algorithm 3 as an (α, 0)-relaxed proximal oracle. In the implementation of Algorithm 3, we
use the centered-local gradient estimator deﬁned in (90). For each use of Algorithm 3, we choose

η =

α
L2,2
co

(cid:16)

(cid:17)2

20

and T =

(cid:25)

(cid:24) 6
ηα

≥

(cid:17)2

(cid:16)

L2,2
co

120

α2

.

(92)

Our discussion will follow in three steps: ﬁrst, we discuss the complexity of all executions in Al-
gorithm 2 other than the calls to the oracles, as well as the initialization procedure for each inner
loop. Next, we discuss the complexity of each iteration of Algorithm 3. Finally, we discuss the
complexity of computing the average iterate in each run of Algorithm 3. For simplicity, when dis-
cussing Algorithm 3, we will only discuss implementation of the x-block, and the y-block will follow
symmetrically. Altogether, the guarantees of Proposition 3 and Proposition 4 imply that if the
guarantees required by the algorithm hold, the expected gap of the output is bounded by (cid:15).

Outer loop extragradient steps and inner loop data structures. Overall, we execute K =
αΘ/(cid:15) iterations of Algorithm 2, and let εouter = εinner = 0 to obtain the desired gap, where Θ = 1
in the (cid:96)2-(cid:96)2 setup. We spend O(nnz) time executing each extragradient step in Algorithm 2 exactly,
where the dominant term in the runtime is the computation of each g(zk−1/2), for k ∈ [K]. Also,
we can maintain the average point ¯z throughout the duration of the algorithm, in O(m + n) time
per iteration. At the beginning of each inner loop, we initialize a data structure IMx
which does not
2
support sampling, an instance of IterateMaintainer2, with IMx

2.Init(wx

0, v), for

0 − ηκgx(w0),
. The inner loop will preserve the invariant that the point maintained by IMx
2

where κ :=
is
the x block of the current inner loop iterate wt in each iteration t. To motivate this initialization,
we recall the form of the updates,

v = (1 − κ)wx

1
1+ηα/2

wx

t+1 ← ΠX

(cid:18)

κ

(cid:18)

wx

t +

(cid:19)

− 1

(cid:18) 1
κ

(cid:19)(cid:19)

wx

0 − η˜gx

w0(wt)

,

(93)

where ΠX (w) =
w0(wt) is gx(w0). Therefore, in the
following discussion we will be able to maintain this diﬀerence via a scaling by κ, an appropriate
addition of the scaled dense vector, and a sparse update.

, and the ﬁxed dense part of ˜gx

w
max{1,(cid:107)w(cid:107)2}

Finally, we also store the vector w0 in full, supporting entry queries.

, computing the sparse part of ˜gw0

Inner loop iterations. Each inner loop iteration consists of sampling indices for the computation
of ˜gw0
, and performing the update to the iterate. We show that
we can run each substep in constant time. Then, this implies that the total complexity of the inner
loop, other than initializing the data structures and outputting the average iterate, is

O(T ) = O



(cid:16)




L2,2
co

α2

(cid:17)2




 .

We discuss how to make appropriate modiﬁcations to the x-block. For simplicity we denote our
current iterate as z, and the next iterate as w. Recall that the distribution is given by

pij(z; w0) :=

(cid:107)Ai:(cid:107)2
1
k∈[m] (cid:107)Ak:(cid:107)2
1

(cid:80)

·

|Aij|
(cid:107)Ai:(cid:107)1

.

70

Sampling. By using precomputed distributions, we can sample i ∝ (cid:107)Ai:(cid:107)2
1
constant time.

and then j | i ∝ |Aij| in

Computing the gradient estimator. Computing the sparse component of the gradient estimator 25
requires computing Aij, [zy − wy
0]i, and pij(z; w0). Using appropriate use of precomputed access to
entries and row norms (it is clear we may pay O(m + n) at the beginning of the algorithm to store
the sum (cid:80)
k∈[m] (cid:107)Ak:(cid:107)2
2.Get(i) allows us to perform the required computation
of the sparse component

0]i, and IMy

), entry [wy

1

in constant time, by assumption.

c := [˜gx

w0(z) − g(w0)]j

Performing the update. In order to perform the update, we recall the form of the update given by
(93). Thus, it suﬃces to call

IMx
IMx
IMx
IMx
IMx

2.Scale(κ);
2.AddDense(1);
2.AddSparse(j, −κηc);
2.Scale(max{IMx
2.UpdateSum()

2.GetNorm(), 1}−1);

By assumption, each operation takes constant time. By the discussion in the data structure initial-
is the x
ization section, it is clear that we preserve the invariant that the point maintained by IMx
2
block of the current iterate.

Average iterate computation. At the end of each run of Algorithm 3, we spend O(n) time
2.GetSum(j) for each j ∈ [n],
computing and returning the average iterate via appropriate calls to IMx
and scaling by 1/T . This operation is asymptotically dominated by the O(nnz(A)) cost of the
extragradient step.

D.2.3 Algorithm guarantee

Theorem 8. In the (cid:96)2-(cid:96)2 setup, the implementation in Section D.2.2 with the optimal choice of
α = max{(cid:15), L2,2
co

(cid:112)1/nnz} has runtime





O





nnz +

(cid:16)

L2,2
co

α2

(cid:17)2








α
(cid:15)


 = O

(cid:32)

nnz +

√

(cid:33)

nnzL2,2
co
(cid:15)

and outputs a point ¯z ∈ Z such that

E Gap(¯z) ≤ (cid:15).

Proof. The correctness of the algorithm is given by the discussion in Section D.2.2 and the guarantees
of Proposition 3 and Proposition 4. The runtime bound is given by the discussion in Section D.2.2,
and the optimal choice of α is clear.

To better understand the strengths of our runtime guarantee, Proposition 7 shows that Theo-
rem 8 implies a universal improvement for (cid:96)2-(cid:96)2 games compared to accelerated gradient descent for
matrices A with nonnegative entries (or more generally, for A with (cid:107)|A|(cid:107)op = O((cid:107)A(cid:107)op)).

71

Proposition 7. For any A ∈ Rm×n, we have

L2,2

co := max




(cid:115)(cid:88)



i

(cid:107)Ai:(cid:107)2
1,

(cid:115)(cid:88)

j

(cid:107)A:j(cid:107)2
1






√

≤

m + n · (cid:107)|A|(cid:107)op .

Proof. Denote 1k as the all 1 vector in Rk. We have the following sequence of inequalities:

(cid:115) (cid:88)

i∈[m]

(cid:107)Ai:(cid:107)2

1 =

(cid:13)
(cid:13)|A|(cid:62)1m
(cid:13)

(cid:13)
(cid:13)
(cid:13)2

= max
x∈Bn

1(cid:62)
m|A|x ≤ (cid:107)1m(cid:107)2 max
x∈Bn

(cid:107)|A|x(cid:107)2 ≤

√

m (cid:107)|A|(cid:107)op .

Similarly, bounding maxy∈Bn y(cid:62)|A|1n implies (cid:113)(cid:80)
√
n} ≤

mum and using max{

m + n implies the result.

m,

√

√

j∈[n] (cid:107)A:j(cid:107)2

1 ≤

√

n (cid:107)|A|(cid:107)op

. Taking a maxi-

Remark 9. For matrix A ∈ Rm×n, combining the guarantees of Theorem 8 with the bound from
Proposition 7 implies a runtime bounded by

(cid:32)

O

nnz +

(cid:112)nnz · (m + n) (cid:107)|A|(cid:107)op
(cid:15)

(cid:33)

.

, this is an improvement by a factor of (cid:112)nnz/(m + n) compared to the
Whenever (cid:107)A(cid:107)op ≥ (cid:107)|A|(cid:107)op
accelerated full-gradient method (c.f. Table 2), which obtains a runtime of O(nnz · (cid:107)A(cid:107)op /(cid:15)). This
applies without any sparsity or numerical sparsity assumptions, and is the same speedup factor as
we obtained for (cid:96)1-(cid:96)1 and (cid:96)2-(cid:96)1 games using a variance reduction framework with row and column
based gradient estimators in Carmon et al. [8]. The (cid:96)2-(cid:96)2 variance reduction algorithms of Carmon
et al. [8] and Balamurugan and Bach [7] do not oﬀer such improvements, and our improvement
stems from our coordinate-based gradient estimators and our data structure design.

D.3 (cid:96)2-(cid:96)1 variance-reduced coordinate method
Assumptions. The algorithm in this section will assume access to entry queries, (cid:96)1 norms of rows,
(cid:96)2 sampling distributions for rows and columns, and the Frobenius norm of A. We use the (cid:96)2-(cid:96)1
local norm setup (cf. Table 6). Again, we deﬁne

L2,1,(1)
co

:=

(cid:114)

max
i∈[m]

(cid:107)Ai:(cid:107)2

1 + (cid:107)A(cid:107)2
F,

L2,1,(2)
co

:=

(cid:114)

2rcs max
i∈[m]

(cid:107)Ai:(cid:107)2
2,

(cid:115)

L2,1,(3)
co

:=

max
i∈[m]

(cid:107)Ai:(cid:107)2

1 +

(cid:18)

(cid:19) (cid:18)

max
i∈[m]

(cid:107)Ai:(cid:107)1

max
j∈[n]

(cid:107)A:j(cid:107)1

(cid:19)
.

(94)

(95)

(96)

Finally, in this section we assume access to a centered variant of WeightedIterateMaintainer2
,
which takes a point x0 as a static parameter, where x0 is in the space as the iterates x maintained.
CenteredIterateMaintainer2 supports two additional operations compared to the data structure
: Sample() returns coordinate j with probability proportional to
WeightedIterateMaintainer2
in constant time,
[w]j[x − x0]2
j
where w is a speciﬁed weight vector. We give the implementation of this extension in Appendix G.

(cf. Section 2.4.1) in O(log n) time, and we may query (cid:107)x − x0(cid:107)2
w

72

D.3.1 Gradient estimator

Given reference point w0 ∈ Bn × ∆m, for z ∈ Bn × ∆m and a parameter α > 0, as in Section C.2,
we specify three distinct choices of sampling distributions p(z; w0), q(z; w0).

The ﬁrst one is

pij(z; w0) :=

[zy]i + 2[wy
0]i
3

·

|Aij|
(cid:107)Ai:(cid:107)1

and qij(z; w0) :=

A2
ij
(cid:107)A(cid:107)2
F

.

(97)

The second one is

pij(z; w0) :=

[zy]i + 2[wy
0]i
3

·

|Aij|
(cid:107)Ai:(cid:107)1

and qij(z) :=

0]2

[zx − wx
(cid:80)

j · 1{Aij (cid:54)=0}
0]2
l∈[n] csl · [zx − wx
l

.

(98)

As in Section C.2, csj is the number of nonzeros of A:j. The third one is

pij(z; w0) :=

[zy]i + 2[wy
0]i
3

·

|Aij|
(cid:107)Ai:(cid:107)1

and qij(z) :=

We now state the local properties of each estimator.

|Aij| · [zx − wx
l∈[n] (cid:107)A:l(cid:107)1 · [zx − wx

0]2
j

0]2
l

(cid:80)

.

(99)

√

Lemma 19. In the (cid:96)2-(cid:96)1 setup, estimator (25) using the sampling distributions in (97), (98), or (99)
is respectively a
Proof. First, we give the proof for the sampling distribution (97). Unbiasedness holds by deﬁnition.
For the x block, we have the variance bound:

-centered-local estimator, for k ∈ {1, 2, 3}.

2L2,1,(k)
co

E

(cid:104)(cid:13)
(cid:13)˜gx

w0(z) − gx(w0)(cid:13)
2
(cid:13)
2

(cid:105)

=

(cid:88)

pij(z; w0)

i∈[m],j∈[n]

(cid:18) Aij[zy − wy
0]i
pij(z; w0)

(cid:19)2

=

(cid:88)

i∈[m],j∈[n]

A2

ij[zy − wy
0]2
i
pij(z; w0)

where in the last inequality we used Lemma 3.

≤ 2 max
i∈[m]

(cid:107)Ai:(cid:107)2

1 Vwy

0

(zy),

For arbitrary wy, we have the variance bound on the y block:
ij[zx − wx
0]2
j
qij(z; w0)

w0(z) − gy(w0)(cid:13)
2
(cid:13)
wy

(cid:104)(cid:13)
(cid:13)˜gy

[wy]i

(cid:88)

A2

=

E

(cid:105)

i∈[m],j∈[n]
(cid:88)

=

[wy]i[zx − wx

j (cid:107)A(cid:107)2
0]2

F ≤ 2 (cid:107)A(cid:107)2

F Vwx

0

(zx).

Combining these and using

i∈[m],j∈[n]

(cid:107)˜gw0(z) − g(w0)(cid:107)2

w := (cid:107)˜gw0(z)x − g(w0)x(cid:107)2

2 + (cid:107)˜gw0(z)y − g(w0)y(cid:107)2
wy

yields the desired variance bound. For the remaining two distributions, the same argument demon-
strates unbiasedness and the variance bound for the x block. For sampling distribution (98) and
arbitrary wy, we have the variance bound on the y block:
A2

E

(cid:104)(cid:13)
(cid:13)˜gy

w0(z) − gy(w0)(cid:13)
2
(cid:13)
wy

(cid:105)

=

(cid:88)

[wy]i

0]2
j

ij[zx − wx
qij(z; w0)

i∈[m],j∈[n]


(cid:88)

≤



i∈[m],j∈[n]





[wy]iA2
ij



rcs



[zx − wx

0]2
j



(cid:88)

j∈[n]

(cid:107)Ai:(cid:107)2

2 Vwx

0

(zx).

≤ 2rcs max
i∈[m]

73

Finally, for sampling distribution (99), we have the variance bound on the y block:

E

(cid:104)(cid:13)
(cid:13)˜gy

w0(z) − gy(w0)(cid:13)
2
(cid:13)
wy

(cid:105)

=

(cid:88)

[wy]i

i∈[m],j∈[n]


A2

0]2
j

ij[zx − wx
qij(z; w0)







≤



(cid:88)

[wy]i|Aij|





(cid:88)

0]2
(cid:107)A:l(cid:107)1 · [zx − wx
l



i∈[m],j∈[n]
(cid:18)

(cid:19) (cid:18)

l∈[n]

(cid:19)

≤ 2

max
i∈[m]

(cid:107)Ai:(cid:107)1

max
j∈[n]

(cid:107)A:j(cid:107)1

Vwx

0

(zx).

Finally, as in Section C.2, we deﬁne the constant

(cid:115)

L2,1

co :=

max
i∈[m]

(cid:107)Ai:(cid:107)2

1 + min

(cid:18)

(cid:107)A(cid:107)2

F , rcs max
i∈[m]

(cid:18)

(cid:107)Ai:(cid:107)2
2 ,

(cid:19) (cid:18)

max
i∈[m]

(cid:107)Ai:(cid:107)1

max
j∈[n]

(cid:107)A:j(cid:107)1

(cid:19)(cid:19)

,

and note that Lemma 19 implies that we can obtain a
priately choosing a sampling distribution depending on the minimizing parameter.

co -centered-local estimator by appro-

2L2,1

√

D.3.2

Implementation details

The algorithm we analyze is Algorithm 2 with K = 3αΘ/(cid:15), εouter = 2(cid:15)/3 using Algorithm 3 as an
(α, εinner = (cid:15)/3)-relaxed proximal oracle with ϕ = (cid:15)/18. In the implementation of Algorithm 2,
, where the truncate operation only
we again apply the truncate(·, δ) operation to each iterate z(cid:63)
k
aﬀects the y block; choosing δ = εouter−εinner
suﬃces for its guarantees (see Section 4.2.2 for the
In the implementation of Algorithm 3, we use the centered-local gradient
relevant discussion).
estimator deﬁned in (25), using the sampling distribution amongst (97), (98), or (99) which attains
the variance bound L2,1

co . For each use of Algorithm 3, we choose

αm

η =

α
L2,1
co

(cid:16)

(cid:17)2

20

and T =

(cid:25)

(cid:24) 6
ηα

=

(cid:17)2

(cid:16)

L2,1
co

120

α2

.

For simplicity, because most of the algorithm implementation details are exactly the same as the
discussion of Section 4.2.2 for the simplex block y ∈ Y, and exactly the same as the discussion of
Section D.2.2 for the ball block x ∈ X , we discuss the diﬀerences here.

Outer loop extragradient steps. We execute 3α log(2m)/(cid:15) iterations of Algorithm 2 to obtain
the desired gap. We spend O(nnz) time executing each extragradient step exactly, and then O(m+n)
time applying the truncate operation and maintaining the average point ¯z. When we initialize the
inner loop, we also create a data structure supporting sampling from wy
0

in constant time.

Data structure initializations and invariants. On the simplex block, we follow the strategy
outlined in Section 4.2.2. We initialize our simplex maintenance data structure AEMy(wy
0, v, κ, ˜ε)
with parameters

κ :=

1
1 + ηα/2

, v := (1 − κ) log wy

0 − ηκgy(w0), ˜ε := (m + n)−8.

74

We will again maintain the invariant that the data structures maintain “exact” and “approximate”
points corresponding to the iterates of our algorithm. The correctness of this setting with respect to
the requirements of Proposition 4, i.e. the approximation conditions in Line 2, 4 and 5 in Algorithm 3,
follows from the discussion of Section 4.2.2; we note that the condition minj[wx
0]j ≥ (m + n)−5 = λ
again holds, and that 1 − κ ≥ (m + n)−8. Thus, for the parameter ω used in the interface of
ApproxExpMaintainer, we have

(cid:18)

log(ω) = log

max

(cid:18) 1

1 − κ

(cid:19)(cid:19)

,

m
λ˜ε

= O(log(mn)).

On the ball block, we follow the strategy outlined in Section D.2.2, but instead of using an
IterateMaintainer2 on the x-block, we use CIMx
, an instance of CenteredIterateMaintainer2
2
, supporting the required sampling operation. For the
data structure initialized with the point wx
0
sampling distribution (98), we use the weight vector of column nonzero counts, and for (99) we use
the weight vector of column (cid:96)1 norms. Overall, the complexity of the initializations on both blocks
is bounded by O(n + m log2(m) log2(mn)).

Inner loop iterations. We discuss how to sample from each of the distributions (97), (98),
and (99) in O(log(m) log(mn)). Combining with the discussions of implementing the inner loop in
Sections 4.2.2 and D.2.2, the total complexity of the inner loop, other than outputting the average
iterate, is

O (cid:0)T log2(m) log2(mn) + nnz + m log(m) log2(mn)(cid:1)

(cid:17)2

L2,1,(1)
co

log2(m) log2(mn)

+ nnz + m log(m) log2(mn)


 .

α2



(cid:16)

= O




As in the variance-reduced (cid:96)1-(cid:96)1 setting, the dominant term in the runtime is the complexity of
calling AEMy.AddSparse in each iteration. Recall that the distribution p in every case is given by

pij(z; w0) :=

[zy]i + 2[wy
0]i
3

·

|Aij|
(cid:107)Ai:(cid:107)1

With probability 2/3 we sample a coordinate i from the precomputed data structure for sampling
from wy
, and otherwise we sample i via AEMy.Sample(). Then, we sample an entry j proportional
0
to its magnitude from the (cid:96)1 sampling oracle for Ai: in constant time. The runtime is dominated
by O(log(m) log(mn)).

To sample from the distribution q in (97), we follow the outline in Section D.3. Similarly, for
sampling from distributions (98) and (99), we follow the outline in Section D.3 but replace all calls
to an IterateMaintainer instance with a call to CIMx
initialized with an appropriate weight vector.
2
In all cases, the runtime is O(log m) which does not dominate the iteration complexity.

Finally, it is clear from discussions in previous sections that the iterate maintenance invariants

of our data structures are preserved by the updates used in this implementation.

75

D.3.3 Algorithm guarantee

Theorem 9. In the (cid:96)2-(cid:96)1 setup, let nnz(cid:48)
Section D.3.2 with the optimal choice of α = max

(cid:16)

:= nnz + m log(m) log2(mn). The implementation in

(cid:15)/3, L2,1

co log(m) log (mn) /

√

nnz(cid:48)(cid:17)

has runtime







nnz(cid:48) +


O

(cid:17)2

(cid:16)

L2,1
co

log2(m) log2(mn)

α2








α log(m)
(cid:15)


 = O

√

(cid:32)

nnz(cid:48) +

nnz(cid:48)L2,1

co log(mn) log2(m)

(cid:33)

(cid:15)

and outputs a point ¯z ∈ Z such that

E [Gap(z)] ≤ (cid:15).

Proof. The correctness of the algorithm is given by the discussion in Section D.3.2 and the guarantees
of Proposition 3 with K = 3αΘ/(cid:15), εouter = 2(cid:15)/3, εinner = (cid:15)/3, Proposition 4 with ϕ = (cid:15)/18 and
data structure ApproxExpMaintainer with our choice of

˜ε := (m + n)−8

to meet the approximation conditions in Line 2, 4 and 5 in Algorithm 3. The runtime bound is
given by the discussion in Section D.3.2, and the optimal choice of α is clear.

E Additional results on variance-reduced methods

E.1 Row-column sparsity variance-reduced methods

By instantiating relaxed proximal oracles with row-column based gradient estimators developed
in [8], implemented with the data structures we develop in Section 5, we obtain the improved
complexities as stated in Table 2. Namely, up to logarithmic factors, we generically replace a
dependence on O(m + n) with O(rcs), where rcs is deﬁned as the maximum number of nonzero
entries for any row or column. In this section, we give implementation details.

The estimators ˜gw0

of [8], parameterized by reference point w0, sample a full column or row
of the matrix (rather than a coordinate). To compute ˜gw0(z) we sample i ∼ p(z) and j ∼ q(z)
independently according to a speciﬁed distribution depending on the setup, and use the estimator

˜gw0(z) :=

(cid:18)

A(cid:62)wy

0 + Ai:

[zy]i − [wy
0]i
pi(w)

, −Awx

0 − A:j

[zx]j − [wx
qj(w)

0]j

(cid:19)

,

(100)

The key diﬀerence between this estimator with that of Section 3.2.2 is that its diﬀerence with
g(w0) is O(rcs)-sparse rather than O(1)-sparse, requiring MultSparse steps with O(rcs)-sparse vec-
tors. In all other respects, the implementation details are exactly the same as those in Section 4.2
and Appendix D, so we omit them for brevity. We now state our sampling distributions used with
the estimator form (100), and the corresponding centered local variance bounds.

In the (cid:96)1-(cid:96)1 setup, we use the sampling distribution (from reference point w0 ∈ ∆m × ∆n)

pi(z) :=

[zy]i + 2[wy
0]i
3

and qj(z) :=

[zx]j + 2[wx
3

0]j

.

(101)

Lemma 20. In the (cid:96)1-(cid:96)1 setup, gradient estimator (100) using the sampling distribution in (101)
is a

2 (cid:107)A(cid:107)max-centered-local estimator.

√

76

Proof. Unbiasedness holds by deﬁnition. For the variance bound, it suﬃces to show that

E (cid:107)˜gw0(z) − g(w0)(cid:107)2

∞ ≤ 2 (cid:107)A(cid:107)2

max Vwx

0

(zx);

clearly this implies the weaker relative variance bound statement (along with an analogous bound
on the y block). To this end, we have

E (cid:107)˜gw0(z) − g(w0)(cid:107)2

∞ ≤

(cid:107)Ai:(cid:107)2

∞ [zy − wy
0]2
i
pi(z)

(cid:88)

i∈[m]

where the last inequality used Lemma 3.

≤ 2 (cid:107)A(cid:107)2

max Vwx

0

In the (cid:96)2-(cid:96)2 setup, we use the oblivious sampling distribution
(cid:107)A:j(cid:107)2
2
(cid:107)A(cid:107)2
F

(cid:107)Ai:(cid:107)2
2
(cid:107)A(cid:107)2
F

and qj =

pi =

.

(zx),

(102)

-
We proved that gradient estimator (100) using the sampling distribution in (102) admits a (cid:107)A(cid:107)F
centered estimator in [8], which is an equivalent deﬁnition to Deﬁnition 4 in the (cid:96)2-(cid:96)2 setup. In the
(cid:96)2-(cid:96)1 setup, we use the sampling distribution (from reference point w0 ∈ Bn × ∆m)

pi(z) =

[zy]i + 2[wy
0]i
3

and qj(z) =

0]j)2
([zx]j − [wx
0(cid:107)2
(cid:107)zx − wx

2

.

(103)

√

Lemma 21. In the (cid:96)2-(cid:96)1 setup, gradient estimator (100) using the sampling distribution in (103)
is a

2L-centered-local estimator with L = maxi∈[m] (cid:107)Ai:(cid:107)2 = (cid:107)A(cid:107)2→∞.
Proof. Unbiasedness holds by deﬁnition. For the variance bound, we ﬁrst note

E

(cid:104)(cid:13)
(cid:13)˜gx

w0(z) − gx(w0)(cid:13)
2
(cid:13)
2

(cid:105)

≤

(cid:88)

i∈[m]

(cid:107)Ai:(cid:107)2
2

1

(cid:0)[zy]i − [wy
(cid:1)2
0]i
3 [wy
3 [zy]i + 2
0]i
(zy),

2 Vwy

0

≤ 2 max
i∈[m]

(cid:107)Ai:(cid:107)2

≤ max
i∈[m]

(cid:107)Ai:(cid:107)2
2





(cid:88)

i∈[m]

(cid:0)[zy]i − [wy
(cid:1)2
0]i
3 [wy
3 [zy]i + 2
0]i

1





where for the last inequality we use Lemma 3. On the other block, we have

max
i∈[m]

E (cid:2)˜gy

w0(w) − gy(w0)(cid:3)2

i ≤ max
i∈[m]

(cid:88)

j∈[n]

A2

0]2
j

ij[wx − wx
qj(w)

= 2 max
i∈[m]

(cid:107)Ai:(cid:107)2

2 Vwx

0

(wx).

Summing these two bounds concludes the proof.

E.2 Extensions with composite terms

In this section, we give a brief discussion of how to change Proposition 4 and implementations of the
procedures in Sections 3.1 and 3.2 to handle modiﬁed regularization in the context of Proposition 6,
and composite regularization terms in the objective in the methods of Section 6. Speciﬁcally we
consider a composite optimization problem of the form:

min
x∈X

max
y∈Y

y(cid:62)Ax + µxφ(x) − µyψ(y) where φ = V x

x(cid:48) and ψ = V y
y(cid:48).

For simplicity of notation we deﬁne Υ(x, y) := µxφ(x) + µyψ(y). We remark that x(cid:48) = 0 recovers
n 1 recovers the case of φ = rx when X = ∆n (similarly

the case of φ = rx when X = Bn, and x(cid:48) = 1
setting y(cid:48) allows us to recover this for the y block).

77

E.2.1 Changes to inner loop

In this section, we ﬁrst discuss the necessary changes to Algorithm 3 and Proposition 4. For
simplicity of notation, we denote ρ := (cid:112)µx/µy, ˆV x := ρV x, ˆV y := 1

ρ V y, ˆV := ˆV x + ˆV y.

Algorithm 4: InnerLoop(w0, ˜gw0, ϕ)

Input: Initial w0 ∈ Z, (L, α)-centered-local gradient estimator ˜gw0
Parameters: Step size η, number of iterations T , approximation tolerance ϕ
Output: Point ˜w satisfying Deﬁnition 5

, oracle quality α > 0

1 for t = 1, . . . , T do

2

3

4

ˆwt−1 ≈ wt−1 satisfying ˆVw0( ˆwt−1) − ˆVw0(wt−1) ≤ ϕ
α
w(cid:63)

(cid:110)
(cid:104)clip(η˜gw0( ˆwt−1) − ηg(w0)), w(cid:105) + ηΥ(w) + ηα
2

t ← arg min

(cid:104) ˆVwt(u) − ˆVw(cid:63)

(cid:105)
(u)

t

≤

ϕ
√
µxµy

1+

wt ≈ w(cid:63)
t
ˆVz(cid:48)(wt) − ˆVz(cid:48)(w(cid:63)

satisfying maxu
t ) ≤ ϕ√
t=1 wt satisfying

(cid:80)T

µxµy

5 return ˜w ≈ 1
T
ˆVw(cid:48)( ˜w) − ˆVw(cid:48)( ¯w) ≤ ϕ√

µxµy

(cid:13)
(cid:13) ˜w − 1
(cid:13)
T
t (cid:107) ≤ ϕ
, and (cid:107)wt − w(cid:63)
2LD

(cid:80)T

t=1 wt

and (cid:107) ˆwt−1 − wt−1(cid:107) ≤ ϕ
LD

ˆVw0(w) + ˆVwt−1(w)
t ) ≤ ϕ
α

, ˆVw0(wt) − ˆVw0(w(cid:63)

, and

(cid:111)

(cid:13)
(cid:13) ≤ ϕ
(cid:13)

LD

, maxu

(cid:104) ˆV ˜w(u) − ˆV ¯w(u)

(cid:105)

≤ ϕ√

µxµy

,

Corollary 1. Let (Z, (cid:107)·(cid:107)·, r, Θ, clip) be any local norm setup. Let w0 ∈ Z, εinner > 0, and ˜gw0
be an L-centered-local estimator for some L ≥ α ≥ εinner. Assume the problem has bounded domain
size maxz∈Z (cid:107)z(cid:107) ≤ D, g is L-Lipschitz, i.e. (cid:107)g(z) − g(z(cid:48))(cid:107)∗ ≤ L (cid:107)z − z(cid:48)(cid:107), that g is LD-bounded, i.e.
maxz∈Z (cid:107)g(z)(cid:107)∗ ≤ LD, and ˆw0 = w0. Then, for η = α
10 , Algorithm 4
outputs a point ˆw ∈ Z such that

α2 , ϕ = εinner

10L2 , T ≥ 8

ηα ≥ 60L2

E max
u∈Z

[(cid:104)g( ˜w) + ∇Υ( ˜w), ˜w − u(cid:105) − αVw0(u)] ≤ εinner,

(104)

i.e. Algorithm 4 is an (α, εinner)-relaxed proximal oracle.
Proof sketch. Note that the only change is in the deﬁnition of the regularized mirror descent step
with extra composite terms

w(cid:63)

t ← arg min

(cid:110)

(cid:104)clip(η˜gw0( ˆwt−1) − ηg(w0)), w(cid:105) + ηΥ(w) +

(cid:111)
ˆVw0(w) + ˆVwt−1(w)

.

ηα
2

Denote ∇Υ(w) = (µx∇φ(wx), µy∇ψ(wy)), so that for the ﬁnal regret bound there are two additional
error terms. The ﬁrst term comes from the error in regularized mirror descent steps via (denoting
z(cid:48) = (x(cid:48), y(cid:48)))

1
T
√

(cid:88)

t∈[T ]
µxµy
T

≤

[−(cid:104)∇Υ(w(cid:63)

t ), w(cid:63)

t − u(cid:105) + (cid:104)∇Υ(wt), wt − u(cid:105)]

(cid:16) ˆVz(cid:48)(wt) − ˆVz(cid:48)(w(cid:63)

t ) + ˆVwt(u) − ˆVw(cid:63)

t

(cid:17)

(u)

≤ 2ϕ

(cid:88)

t∈[T ]

following the approximation guarantee in Line 4. The other term comes from averaging error.
Denote the true average iterate by ¯w := 1
T

t∈[T ] wt. We have ∀u ∈ Z,

(cid:80)

(cid:104)g( ˜w), ˜w − u(cid:105) −

1
T

(cid:88)

t∈[T ]

(cid:104)g(wt), wt − u(cid:105) = − (cid:104)g( ˜w), u(cid:105) − (cid:104)g( ¯w), ¯w − u(cid:105)

= (cid:104)g( ¯w) − g( ˜w), u(cid:105) ≤ ϕ,

78

and also

(cid:104)∇Υ( ˜w), ˜w − u(cid:105) = (cid:104)∇Υ( ˜w) − ∇Υ( ¯w), ˜w − u(cid:105) + (cid:104)∇Υ( ¯w), ˜w − ¯w(cid:105) + (cid:104)∇Υ( ¯w), ¯w − u(cid:105)

(i)
=

(ii)
=

√

µxµy (cid:16)
√
µxµy (cid:16)

(cid:17)
− ˆV ¯w(u) + ˆV ˜w(u) + ˆV ¯w( ˜w)

+ (cid:104)∇Υ( ¯w), ˜w − ¯w(cid:105) + (cid:104)∇Υ( ¯w), ¯w − u(cid:105)

− ˆV ¯w(u) + ˆV ˜w(u) + ˆVw(cid:48)( ˜w) − ˆVw(cid:48)( ¯w)

(cid:17)

+ (cid:104)∇Υ( ¯w), ¯w − u(cid:105),

(iii)
≤ 2ϕ + (cid:104)∇Υ( ¯w), ¯w − u(cid:105)

(iv)
≤ 2ϕ +

1
T

(cid:88)

t∈[T ]

(cid:104)∇Υ(wt), wt − u(cid:105).

where we use (i) the three-point property of Bregman divergence, (ii) the fact that ˆV ¯w( ˜w) +
(cid:104)∇Υ( ¯w), ˜w − ¯w(cid:105) = ˆVw(cid:48)( ˜w) − ˆVw(cid:48)( ¯w) again by the three-point property, (iii) the approximation
guarantee of Line 5, and (iv) the fact that (cid:104)∇Υ(w), w − u(cid:105) is convex in w for our choices of Υ.
Hence incorporating the above extra error terms into the regret bound yields the conclusion, as
10ϕ = εinner by our choice of ϕ.

E.2.2 Changes to implementation

Broadly speaking, all of these modiﬁcations can easily be handled via appropriate changes to the
initial data given to our data structures CenteredIterateMaintainer2 and ApproxExpMaintainer.
We discuss general formulations of iterations with these modiﬁcations in both simplices and Eu-
clidean balls, and provide appropriate modiﬁcations to the inital data given to our data structures.
Finally, it is simple to check that all relevant parameters are still bounded by a polynomial in the di-
mensions of variables, so no additional cost due to the data structure is incurred. For simplicity here
we only considerfor the x-block when φx(x) = µr(x) and remark that the case when φx(x) = µVx(cid:48)(x)
for some x(cid:48) follows similarly.

(cid:96)1 domains. For this section, deﬁne a domain X = ∆n, let r(x) = (cid:80)
and let µ, α, η, ρ be nonnegative scalar parameters. Consider a sequence of iterates of the form

j∈[n] xj log xj be entropy,

xt+1 ← arg min

x∈X

(cid:104)˜gx0(xt), x(cid:105) + µr(x) +

αρ
2

Vx0(x) +

ρ
η

Vxt(x).

This update sequence, for the form of gradient estimator

˜gx0(x) = g(x0) + b + g(cid:48)(x),
where g(cid:48)(x) is a vector with suitable sparsity assumptions depending on the point x, and b is some
ﬁxed vector, generalizes all of the settings described above used in our various relaxed proximal
oracle implementations. Optimality conditions imply that the update may be rewritten as

(cid:32)

xt+1 ← Π∆

exp

(cid:32) ρ

η log xt + αρ

2 log x0 − g(x0) − b − g(cid:48)(xt)

µ + αρ

2 + ρ

η

(cid:33)(cid:33)

.

Thus, initializing an ApproxExpMaintainer instance with

κ =

1
ρ + αη
2 + 1

µη

, v =

αρ
2 log x0 − g(x0) − b

µ + αρ

2 + ρ

η

enables DenseStep to propagate the necessary changes to the iterate; we propagate changes due to
g(cid:48)(xt) via AddSparse and the appropriate sparsity assumptions.

79

(cid:96)2 domains. For this section, deﬁne a domain X = Bn, let r(x) = 1
α, η, ρ be nonnegative scalar parameters. Consider a sequence of iterates of the form

2 (cid:107)x(cid:107)2

2

be entropy, and let µ,

xt+1 ← arg min

x∈X

(cid:104)˜gx0(xt), x(cid:105) + µr(x) +

αρ
2

Vx0(x) +

ρ
η

Vxt(x).

This update sequence, for the form of gradient estimator

˜gx0(x) = g(x0) + b + g(cid:48)(x),

where g(cid:48)(x) is a vector with suitable sparsity assumptions depending on the point x, and b is some
ﬁxed vector, generalizes all of the settings described above used in our various relaxed proximal
oracle implementations. Optimality conditions imply that the update may be rewritten as

xt+1 ← ΠBn

(cid:32) ρ

η xt + αρ

2 x0 − g(x0) − b − g(cid:48)(xt)
2 + ρ

µ + αρ

η

(cid:33)

.

Thus, initializing an CenteredIterateMaintainer instance with

v =

αρ
2 x0 − g(x0) − b

µ + αρ

2 + ρ

η

enables AddDense, Scale, and GetNorm to propagate the necessary changes to the iterate; we
propagate changes due to g(cid:48)(xt) via AddSparse and the appropriate sparsity assumptions.

F Deferred proofs from Section 6

F.1 Proofs from Section 6.1

Proof of Lemma 10. We consider the following (µ, µ)-strongly monotone problem, for various levels
of µ:

max
x∈Bn

min
y∈∆m

fµ(x, y) := y(cid:62) ˜Ax + y(cid:62)b + µ

[y]i log[y]i −

µ
2

(cid:107)x(cid:107)2
2 .

(cid:88)

i∈[m]

We claim we can implement an (α, ε)-relaxed proximal oracle for this problem in time



(cid:16)

(cid:101)O




L2,1
co

α2

(cid:17)2




 .

The oracle is a composite implementation of Algorithm 3 as in Algorithm 4, using the estimator of
Appendix D.3. By an application of Proposition 6, the overall complexity of solving this problem
is (by choosing the optimal α, and overloading the constant L2,1

co to be with respect to ˜A):





(cid:101)O





nnz +

(cid:16)

L2,1
co

α2

(cid:17)2








α
µ


 = (cid:101)O

(cid:32)

nnz +

√

nnz · L2,1
co
µ

(cid:33)

.

By conducting a line search over the parameter µ via repeatedly halving, the total cost of solving
each of these problems is dominated by the last setting, wherein µ = Θ(r∗/ log m), and R/µ = (cid:101)O (ρ);
here, we recall that we rescaled ˜A so that L2,1
co = O(R). We defer details of the line search procedure
to Lemma C.3 of Allen-Zhu et al. [1].

80

Proof of Theorem 3. We solve the problem (58) to duality gap (cid:15)ˆr/8 ≤ (cid:15)r∗, using the algorithm of
Appendix D.3 for (cid:96)2-(cid:96)1 games. The complexity of this algorithm is (choosing α optimally)







nnz( ˜A) +


(cid:16)

L2,1
co

α2

(cid:17)2






 ·

α
(cid:15)ˆr


 = (cid:101)O

(cid:32)

nnz +

√

ρ

nnz · L2,1
co

(cid:15)

(cid:33)

,

(cid:101)O

as claimed. Here, we used that ˜A is a rescaling of A by 2R, and ˆr is a constant multiplicative
approximation of r. The approximate solution (x∗
(cid:15)(cid:48)) obtains the requisite duality gap in expec-
tation; Markov’s inequality implies that with logarithmic overhead in the runtime, we can obtain a
pair of points satisfying with high probability

(cid:15)(cid:48), y∗

max
x

f (x, y∗

(cid:15)(cid:48)) − min

y

f (x∗

(cid:15)(cid:48), y) = max

x

f (x, y∗

(cid:15)(cid:48)) − f (x∗, y∗) + f (x∗, y∗) − min
y

f (x∗

(cid:15)(cid:48), y) ≤ (cid:15)(cid:48).

Because y∗ is the best response to x∗, we have f (x∗, y∗

(cid:15)(cid:48)) ≥ f (x∗, y∗), which implies

max
x

f (x, y∗

(cid:15)(cid:48)) − f (x∗, y∗) = max

x

f (x, y∗

(cid:15)(cid:48)) − f (x∗, y∗

(cid:15)(cid:48)) + f (x∗, y∗

(cid:15)(cid:48)) − f (x∗, y∗) ≥ 0.

Combining yields f (x∗, y∗) − miny f (x∗
miny f (x∗

(cid:15)(cid:48), y) ≥ r∗ − (cid:15)(cid:48) ≥ (1 − (cid:15))r∗. Thus, x∗

(cid:15)(cid:48), y) ≤ (cid:15)(cid:48) ≤ (cid:15)r∗, so since f (x∗, y∗) = r∗, rearranging implies

(cid:15)(cid:48) is an (cid:15)-approximate solution for Max-IB.

F.2 Proofs from Section 6.2

Proof of Lemma 11. If (x(cid:48), y(cid:48)) is an approximately optimal solution with duality gap (cid:15)/16 for (61),
by deﬁnition

Therefore, the following sequence of inequalities hold:

max
y∈∆m

f(cid:15)(cid:48)(x(cid:48), y) − min
x∈Rn

f(cid:15)(cid:48)(x, y(cid:48)) ≤

(cid:15)
16

.

f (x(cid:48), y) − min
x∈Rn

f (x, y(cid:48)) =

max
y∈∆m
(cid:18)

+

(i)
≤

max
y∈∆m

(cid:18)

max
y∈∆m

f(cid:15)(cid:48)(x(cid:48), y) − min
x∈Rn

f(cid:15)(cid:48)(x, y(cid:48))

f (x(cid:48), y) − max
y∈∆m

(cid:19)

f(cid:15)(cid:48)(x(cid:48), y)

min
x∈Rn
(cid:18)

+

+

(cid:15)
16

+

min
x∈Rn

(cid:18)

max
y∈∆m
(cid:18)
(cid:19)

f (x(cid:48), y) − max
y∈∆m

(cid:19)

f(cid:15)(cid:48)(x(cid:48), y)

f(cid:15)(cid:48)(x, y(cid:48)) − min
x∈Rn

(cid:19)

f (x, y(cid:48))

f(cid:15)(cid:48)(x, y(cid:48)) − min
x∈Rn

(cid:19)

f (x, y(cid:48))

(ii)
≤

(cid:15)
32

+

(cid:15)
16

+

(cid:15)
32

=

(cid:15)
8

.

In (i), we used the fact that the pair (x(cid:48), y(cid:48)) has good duality gap with respect to f(cid:15)(cid:48), and in (ii)
we used that for the ﬁrst summand, f(cid:15)(cid:48)(x(cid:48), ·) approximates f (x(cid:48), ·) to an additive (cid:15)/32, and for the
third summand, −(cid:15)(cid:48) (cid:80)

i∈[m][y(cid:48)]i log[y(cid:48)]i is bounded by (cid:15)/32, and all other terms cancel.

F.3 Proofs from Section 6.3

Proof of Lemma 12. At optimality for (62), it holds that

(cid:40)

x(cid:48) = 1
β (Ax∗
y∗
x(cid:48) = x(cid:48) − 1
x∗

x(cid:48) − b)
β A(cid:62)y∗
x(cid:48)

.

81

By substituting y∗

x(cid:48) and rearranging terms we get
(cid:18)

which in turn gives

I +

(cid:19)
1
β2 A(cid:62)A

(x∗

x(cid:48) − x∗) = x(cid:48) − x∗,

(cid:107)x∗

x(cid:48) − x∗(cid:107)2 =

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:19)−1

I +

1
β2 A(cid:62)A

(x(cid:48) − x∗)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≤

1
1 + µ/β2

(cid:13)x(cid:48) − x∗(cid:13)
(cid:13)

(cid:13)2 .

For the last inequality we use the fact that

(cid:13)
(cid:13)
(cid:13)
(cid:13)

I +

1
β2 A(cid:62)A

−1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

(cid:18)

= λmin

I +

(cid:19)−1

1
β2 A(cid:62)A

=

1
1 + µ/β2 ,

by the deﬁnition of µ and since I and A(cid:62)A commute.

Theorem 5. Given data matrix A ∈ Rm×n, vector b ∈ Rm, and desired accuracy (cid:15) ∈ (0, 1),
assuming A(cid:62)A (cid:23) µI for µ > 0, Algorithm 5 outputs an expected (cid:15)-accurate solution ˜x, i.e.

E [(cid:107)˜x − x∗(cid:107)2] ≤ (cid:15),

and runs in time



(cid:101)O


nnz +

max

√

nnz ·

(cid:110)(cid:113)(cid:80)

i (cid:107)Ai:(cid:107)2
1,
√
µ

(cid:113)(cid:80)

j (cid:107)A:j(cid:107)2
1



(cid:111)


 .

Proof. We ﬁrst prove correctness. We bound the progress from x(h) to x(h+1), for some h ∈ [H], by

1
2

(cid:13)
(cid:13)

(cid:13)x(h+1) − x∗(cid:13)

2
(cid:13)
(cid:13)
2

≤

(cid:13)
(cid:13)x(h+1) − x∗
(cid:13)

x(h)

(cid:13)
2
(cid:13)
(cid:13)
2

+ (cid:13)

(cid:13)x∗

x(h) − x∗(cid:13)
2
(cid:13)
2

≤ 2Vz(h+1)(z∗

x(h)) + (cid:13)

(cid:13)x∗

x(h) − x∗(cid:13)
2
(cid:13)
2

.

(105)

2 + 2 (cid:107)b(cid:107)2
The ﬁrst inequality used (cid:107)a + b(cid:107)2
, and the second used the deﬁnition of the
divergence in the (cid:96)2-(cid:96)2 setup. Next, choosing a suﬃciently large value of K = (cid:101)O (β/µ), we use
Proposition 6 to obtain a point z(h+1) satisfying

2 ≤ 2 (cid:107)a(cid:107)2

2

Vz(h+1)(z∗

x(h)) ≤

(cid:15)2
80

Vz(h)(z∗

(cid:15)2
40

Vz(h)(z∗) +

(cid:15)2
40

Vz∗(z∗

x(h)).

x(h)) ≤
√

Further, using Lemma 12 with x(cid:48) = x(h), β =

µ yields

(cid:13)
(cid:13)x∗

x(h) − x∗(cid:13)
(cid:13)2

≤

1
2

(cid:13)
(cid:13)

(cid:13)x(h) − x∗(cid:13)
(cid:13)
(cid:13)2

.

(106)

(107)

Plugging these two bounds into (105), and using the form of the divergence in the (cid:96)2-(cid:96)2 setup,

1
2

(cid:13)
(cid:13)

(cid:13)x(h+1) − x∗(cid:13)

2
(cid:13)
(cid:13)
2

(106)
≤

(107)
≤

≤

3
4

(cid:15)2
20
1
2
1
2

Vz(h)(z∗) +
(cid:18) (cid:15)2
20
(cid:13)
(cid:13)

(cid:13)x(h) − x∗(cid:13)

(cid:15)2
20

+

+

(cid:15)2
20
1
2
2
(cid:13)
(cid:13)
2

x(h)) + (cid:13)
Vz∗(z∗
(cid:13)x∗
(cid:19) (cid:13)
(cid:13)x(h) − x∗(cid:13)
(cid:13)
(cid:15)2
5

2
(cid:13)
(cid:13)
2

.

+

·

x(h) − x∗(cid:13)
2
(cid:13)
2
(cid:18)(cid:13)
(cid:15)2
(cid:13)y(h) − y∗(cid:13)
(cid:13)
40

+

2
(cid:13)
(cid:13)
2

+ (cid:13)

(cid:13)y∗

x(h) − y∗(cid:13)
2
(cid:13)
2

(cid:19)

(108)

82

Algorithm 5: Coordinate variance reduced method for linear regression

Input: Matrix A ∈ Rm×n with ith row Ai: and jth column A:j, vector b ∈ Rm, accuracy (cid:15)
Output: A point ˜x with (cid:107)˜x − x∗(cid:107)2 ≤ (cid:15)
(cid:111)
i (cid:107)Ai:(cid:107)2
1,

µ, η ← α
4L2
, K ← (cid:101)O (α/β), H = (cid:101)O (1), z(0) = (x(0), y(0)) ← (0n, 0m), (zx

(cid:110)(cid:113)(cid:80)
1 L ← max
(cid:108) 4
(cid:109)
ηα

0) ← (0n, 0m)

, α ← L/

j (cid:107)A:j(cid:107)2
1

nnz, β =

2 T ←

(cid:113)(cid:80)

0, zy

√

√

3 for h = 1, 2, · · · , H do
for k = 1, . . . , K do
4

5

6

7

8

9

10

11

12

13

14

(cid:46) Relaxed oracle query:
k−1, zy
(x0, y0) ← (zx
for t = 1, . . . , T do

k−1), (gx

0, gy

0) ← (A(cid:62)y0 + β(x0 − x(h−1)), −Ax0 + βy0)

(cid:46) Gradient estimation:
Sample i ∼ p where pi =

Sample j ∼ q where qj =

Set ˜gt−1 = g0 +

(cid:18)

Ai:

([yt−1]i − [y0]i)2
(cid:107)yt−1 − y0(cid:107)2
2
([xt−1]j − [x0]j)2
(cid:107)xt−1 − x0(cid:107)2
2
[xt−1]j − [x0]j
qj

, −A:j

[yt−1]i − [y0]i
pi

(cid:19)

(cid:16)

xt ←

(cid:46) Mirror descent step:
1
1 + ηα/2
(cid:18)
1
1 + ηα/2

yt ← ΠY

(cid:16)

xt−1 +

x0 − η˜gx

t−1

(cid:17)

ηα
2

yt−1 +

ηα
2

y0 − η˜gy

t−1

(cid:17)(cid:19)

(cid:46) ΠY (v) =

v
max{1,(cid:107)v(cid:107)2}

zk−1/2 ←

1
T

T
(cid:88)

(xt, yt)

t=1

(cid:46) Extragradient step:
α
α + 2β

zx
k−1 +
(cid:18) α

zx
k ←

zy
k ← ΠY

α + 2β

2β
α + 2β

zy
k−1 +

zx
k−1/2 −
2β
α + 2β

(cid:16)

1
α + 2β

A(cid:62)zy

zy
k−1/2 +

1
α + 2β

k−1/2 − x(h−1))

(cid:17)

k−1/2 + β(zx
(cid:16)

Azx

k−1/2 − βzy

(cid:17)(cid:19)

k−1/2

(cid:46) Reshifting the oracle:
z(h) = (x(h), y(h)) ← zK = (zx

K, zy
K)

15
16 return ˜x ← x(H)

83

In the last inequality we use the conditions that (cid:15) ∈ (0, 1) and Y = Bm. Recursively applying this
bound for h ∈ [H], and for a suﬃciently large value of H = (cid:101)O (1), we have the desired

(cid:13)
(cid:13)

(cid:13)x(H) − x∗(cid:13)

2
(cid:13)
(cid:13)
2

≤

(cid:18) 3
4

(cid:19)H (cid:13)
(cid:13)

(cid:13)x(0) − x∗(cid:13)

2
(cid:13)
(cid:13)
2

+

4(cid:15)2
5

≤ (cid:15)2.

To bound the runtime, recall the inner loop runs for T = O((L2,2

co )2/α2) iterations, each costing
constant time, and the outer loop runs for K = (cid:101)O (α/β) iterations, each costing O(T +nnz). Finally,
since H = (cid:101)O (1), the overall complexity of the algorithm is








(cid:101)O





nnz +

(cid:17)2

(cid:16)

L2,2
co

α2




α
β


 .

Choosing α = max{L2,2
co /

√

nnz, β} optimally and substituting

√

β =

µ, L2,2

co = max




(cid:115)(cid:88)



i

(cid:107)Ai:(cid:107)2
1,

(cid:115)(cid:88)

j

(cid:107)A:j(cid:107)2
1






,

we have the desired runtime bound on Algorithm 5.

G IterateMaintainer2: numerical stability and variations

G.1 Numerical stability of IterateMaintainer1.
We discuss the implementation of a numerically stable version of IterateMaintainer1, and the
complexity of its operations, for use in our sublinear algorithms in Section 4.1 and Section C.2. We
discuss this implementation for a simplex block, e.g. a simplex variable of dimension n, as for an (cid:96)2
geometry numerical stability is clear. The main modiﬁcations we make are as follow.

• We reinitialize the data structure whenever the ﬁeld ν grows larger than some ﬁxed polynomial

in n, or if n/2 iterations have passed.

• We track the coordinates modiﬁed between restarts.

• Every time we reinitialize, we maintain the invariant that the multiplicative range of coordi-
nates of x is bounded by a polynomial in n, i.e. maxj xj/ minj xj is bounded by some ﬁxed
polynomial in n. We will implement this via an explicit truncation, and argue that such an
operation gives negligible additive error compared to the accuracy of the algorithm.

• We implicitly track the set of truncated coordinates at each data structure restart. We do so
by explicitly tracking the set of non-truncated coordinates whenever a truncation operation
happens (see the discussion below), in constant amortized time.

We now discuss the complexity and implementation of these restarts. First, note that ν can never
decrease by more than a multiplicative polynomial in n between restarts, because of nonnegativity
of the exponential, the fact that the original range at the time of the last restart is multiplicatively
bounded, and we restart every time half the coordinates have been touched. Thus, the only source
of numerical instability comes from when ν grows by more than a multiplicative polynomial in n.
Suppose this happens in τ iterations after the restart. Then,

84

• If τ < n/2, we claim we can implement the restart in O(τ ), so the amortized cost per iteration
is O(1). To see this, for every coordinate touched in these τ iterations, we either keep or
explicitly truncate if the coordinate is too small. For every coordinate not touched in these τ
iterations, the relative contribution is at most inverse polynomial in n; we truncate all such
coordinates. Then, we compute the normalization constant according to all non-truncated
coordinates, such that the value of all truncated coordinates is set to a ﬁxed inverse polynomial
in n. We can implement this by implicitly keeping track of the set of truncated coordinates
as well as their contribution to the normalization factor, and explicitly setting their value in
the data structure when they are updated by AddSparse. Overall, this does not aﬀect the
value of the problem by more than a small multiple of (cid:15), by our assumptions on Lrc/(cid:15). To see
that we can track the non-truncated coordinates explicitly, we note that it is a subset of the
at most τ coordinates that were touched, so this can be done in constant amortized time.

• If τ = n/2, we claim we can implement the restart in O(n), so the amortized cost per iteration
is O(1). This is clear: we can do so by explicitly recomputing all coordinates, and truncating
any coordinates which have become too small.

We describe how the data structure implements this through its maintained ﬁelds: for non-truncated
coordinates, we do not do anything other than change the scaling factor ν, and for truncated
coordinates, we reset the values of u, u(cid:48) in that coordinate appropriately once they have been
sparsely updated. Overall, this does not aﬀect the amortized runtime of our algorithm.

G.2 WeightedIterateMaintainer2
In this section, we give implementation details for a weighted generalization of IterateMaintainer2,
. It is used in Section C.2, when using the sam-
which we will call WeightedIterateMaintainer2
pling distribution (87). At initialization, WeightedIterateMaintainer2
is passed an additional
, a nonnegative weight vector. We let
parameter w ∈ Rn
≥0

(cid:104)u, v(cid:105)w :=

(cid:88)

j∈[n]

[w]j[u]j[v]j, (cid:107)v(cid:107)w :=

(cid:113)

(cid:104)v, v(cid:105)w.

WeightedIterateMaintainer2
diﬀerences:

supports all the same operations as IterateMaintainer2, with two

• For the current iterate x, WeightedIterateMaintainer2.Norm() returns weighted norm (cid:107)x(cid:107)w
• For the current iterate x, WeightedIterateMaintainer2.Sample() returns a coordinate j with

.

probability proportional to [w]j[x]2
j

.

Similarly to IterateMaintainer2, WeightedIterateMaintainer2
• Scalars ξu, ξv, σu, σv, ι, ν

maintains the following ﬁelds.

• Vectors u, u(cid:48), v, w

• Precomputed value (cid:107)v(cid:107)2
w

.

We maintain the following invariants on the data structure ﬁelds at the end of every operation:

• x = ξuu + ξvv, the internal representation of x

85

• s = v + σuu + σvv, the internal representation of running sum s
• ι = (cid:104)x, v(cid:105)w
• ν = (cid:107)x(cid:107)w
To support sampling, our data structure also maintains a binary tree distx of depth O(log n).

, the weighted inner product of the iterate with ﬁxed vector v

, the weighted norm of the iterate

For the node corresponding to S ⊆ [n] (where S may be a singleton), we maintain

j∈S[w]j[u]2
j

• (cid:80)
j∈S[w]j[v]2
j
We now give the implementation of the necessary operations for WeightedIterateMaintainer2

j∈S[w]j[u]j[v]j, (cid:80)

, (cid:80)

giving additional proofs of correctness when applicable.

,

Initialization.

• Init(x0, v, w). Runs in time O(n).

1. (ξu, ξv, u) ← (1, 0, x0).
2. (σu, σv, u(cid:48)) ← (0, 0, 0n).
3. (ι, ν) ← ((cid:104)x0, v(cid:105)w , (cid:107)x0(cid:107)w).
4. Compute and store (cid:107)v(cid:107)2
.
w
5. Initialize distx, storing the relevant sums in each internal node.

Updates. Scale(c) and UpdateSum() follow identically to the analysis of IterateMaintainer2.

• AddSparse(j, c): [x]j ← [x]j + c. Runs in time O(log n).

ej.

1. u ← u + c
ej.
ξu
2. u(cid:48) ← u(cid:48) − cσu
ξu
3. ν ← (cid:112)ν2 + 2c[w]j[ξuu + ξvv]j + c2[w]j.
4. ι ← ι + c[w]j[v]j.
5. For internal nodes of distx on the path from leaf j to the root, update (cid:80)

(cid:80)

j∈S[w]j[u]j[v]j appropriately.
• AddDense(c): x ← x + cv. Runs in time O(1).

j∈S[w]j[u]2
j

,

1. ξv ← ξv + c.
(cid:113)
2. ν ←
3. ι ← ι + c (cid:107)v(cid:107)2
w

ν2 + 2cι + c2 (cid:107)v(cid:107)2
w
.

.

We demonstrate that the necessary invariants on ι, ν are preserved. Regarding correctness of
AddSparse, the updates to u and u(cid:48) are identical to in the analysis of IterateMaintainer2. Next,
because only [x]j changes, the updates to ν, ι are correct respectively by

[w]j · [ξuu + ξvv + c]2

j = [w]j · (cid:0)[ξuu + ξvv]2

j + 2c[ξuu + ξvv]j + c2(cid:1) ,

[w]j · ([ξuu + ξvv + c]j) · [v]j = [w]j · ([ξuu + ξvv]j · [v]j + c[v]j) .

Regarding correctness of AddDense,

Here, we used that the invariants ν = (cid:107)x(cid:107)w

w = ν2 + 2cι + c2 (cid:107)v(cid:107)2
w ,

(cid:107)x + cv(cid:107)2
(cid:104)x + cv, v(cid:105)w = ι + c (cid:107)v(cid:107)2
w .
and ι = (cid:104)x, v(cid:105)w

held.

86

Queries. Get(j) and GetSum(j) follow identically to the analysis of IterateMaintainer2.

• Norm(): Return (cid:107)x(cid:107)w
1. Return ν.

. Runs in time O(1).

Sampling. To support Sample, we must produce a coordinate j with probability proportional to
. To do so, we recursively perform the following procedure, where the recursion depth is at
[w]j[x]2
j
most O(log n), starting at the root node and setting S = [n]: the proof of correctness is identical
to the proof in the analysis of IterateMaintainer2.Sample().

1. Let S1, S2 be the subsets of coordinates corresponding to the children of the current node.
2. Using scalars ξu, ξv, and the maintained (cid:80)
j = (cid:80)

, (cid:80)
[w]j[u]2
j
for i ∈ {1, 2}.

[w]j[ξuu + ξvv]2
j

[w]j[u]j[v]j, (cid:80)

[w]j[v]2
j

[w]j[x]2

pute (cid:80)

j∈Si

j∈Si

j∈Si

j∈Si

j∈Si

, com-

3. Sample a child i ∈ {1, 2} of the current node proportional to (cid:80)

[w]j[x]2
j

by ﬂipping an

j∈Si

appropriately biased coin. Set S ← Si.

G.3 CenteredIterateMaintainer2
,
In this section, we give implementation details for a generalization of WeightedIterateMaintainer2
which we call CenteredIterateMaintainer2. It is used in Section D.3, when using the sampling
distributions (98) and (99). At initialization, CenteredIterateMaintainer2 is passed an addi-
tional parameter x0 ∈ Rn, a reference point. CenteredIterateMaintainer2 supports all the same
operations as WeightedIterateMaintainer2

, with two diﬀerences:

• For the current iterate x, CenteredIterateMaintainer2.Sample() returns a coordinate j with

probability proportional to [w]j[x − x0]2
j

.

• CenteredIterateMaintainer2 supports querying (cid:107)x − x0(cid:107)2
w

in constant time.

Because all the other operations, ﬁelds, and invariants supported and maintained by the data
structure are exactly the same as IterateMaintainer2, we only discuss the changes made to the
binary tree distx in this section for brevity. In particular, to support sampling, our data structure
also maintains a binary tree distx of depth O(log n). For the node corresponding to S ⊆ [n] (where
S may be a singleton), we maintain

• (cid:80)

• (cid:80)

j∈S[w]j[u]2
j
j∈S[w]j[x0]2
j

, (cid:80)

j∈S[w]j[u]j[v]j, (cid:80)
j∈S[w]j[u]j[x0]j, (cid:80)

, (cid:80)

j∈S[w]j[v]2
j

j∈S[w]j[v]j[x0]j
At initialization, CenteredIterateMaintainer2 creates this data structure and stores the relevant
sums in each internal node. Upon modiﬁcations to u due to updates of the form AddSparse(j, c),
CenteredIterateMaintainer2 propagates the changes along internal nodes of distx on the path
from leaf j to the root. Thus, using these maintained values and the stored values ξu, ξv, it is clear
that for any appropriate subset S, we are able to compute the quantity
(cid:88)

(cid:88)

u[u]2

j + ξ2

v [v]2

j + 2ξuξv[u]j[v]j + [x0]2

j + 2ξu[u]j[x0]j + 2ξv[v]j[x0]j

[w]j[ξu+ξvv−x0]2

j =

[w]j

(cid:0)ξ2

(cid:1)

j∈S

j∈S

in constant time, admitting the sampling oracle in time O(log n) by propagating down the tree main-
tained by distx. This proves the desired sampling complexity. Finally, by appropriately querying
the stored values in the root node, we can return (cid:107)x − x0(cid:107)2
w

in constant time.

87

