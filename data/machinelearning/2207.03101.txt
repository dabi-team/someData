A conditional gradient homotopy method with applications
to Semideﬁnite Programming

Pavel Dvurechensky1, Shimrit Shtern3, and Mathias Staudigl*4

1Weierstrass Institute for Applied Analysis and Stochastics, Mohrenstr. 39, 10117 Berlin, Germany,
(Pavel.Dvurechensky@wias-berlin.de)
3Faculty of Industrial Engineering and Management, Technion - Israel Institute of Technology, Haifa, Israel,
(shimrits@technion.ac.il)
4Department of Data Science and Knowledge Engineering, Maastricht University, P.O. Box 616, NL–6200 MD
Maastricht, The Netherlands, (m.staudigl@maastrichtuniversity.nl)

July 8, 2022

Abstract

We propose a new homotopy-based conditional gradient method for solving convex opti-
mization problems with a large number of simple conic constraints. Instances of this template
naturally appear in semideﬁnite programming problems arising as convex relaxations of com-
binatorial optimization problems. Our method is a double-loop algorithm in which the conic
constraint is treated via a self-concordant barrier, and the inner loop employs a conditional
gradient algorithm to approximate the analytic central path, while the outer loop updates the
accuracy imposed on the temporal solution and the homotopy parameter. Our theoretical it-
eration complexity is competitive when confronted to state-of-the-art SDP solvers, with the
decisive advantage of cheap projection-free subroutines. Preliminary numerical experiments
are provided for illustrating the practical performance of the method.

1 Introduction

In this paper we investigate a new algorithm for solving a large class of convex optimization
problems with conic constraints of the following form:

min
x

g(x)

s.t. x ∈ X, P(x) ∈ K ⊆ H,

(P)

where g : E → (−∞, ∞] is a closed convex and lower semi-continuous function, X ⊆ E is a bounded
closed and convex set, P : E → H is an aﬃne mapping between two ﬁnite-dimensional Euclidean
vector spaces E and H, and K is a closed convex pointed cone. Problem (P) is suﬃciently generic to
cover many optimization settings considered in the literature.

2
2
0
2

l
u
J

7

]

C
O
.
h
t
a
m

[

1
v
1
0
1
3
0
.
7
0
2
2
:
v
i
X
r
a

Example 1.1 (Packing SDP). The model template (P) covers the large class of packing semideﬁnite
programs (SDPs) [10, 15]. In this problem class we have E = Sn – the space of symmetric n × n
matrices, a collection of positive semideﬁnite input matrices A1
+, with constraints
(cid:62)
given by P(x) = [tr(A1x) − 1; . . . ; tr(Amx) − 1]
+ , coupled with the objective function
g(x) = (cid:104)c, x(cid:105) and the set X = {x ∈ Sn
+| tr(x) ≤ ρ}.

, . . . , Am ∈ Sn

and K = Rm

*Corresponding Author

1

 
 
 
 
 
 
Example 1.2 (Sparse Recovery). Recovering a signal from sparse measurements is a classical task in
signal processing. A popular convex relaxation of this problem is the norm minimization problem

(cid:107)x(cid:107)

min
x

s.t.: x ∈ Sn

+, (cid:107)Ax − b(cid:107)2 ≤ δ.

Here (cid:107)·(cid:107) is a suitable matrix norm (usually the nuclear norm to obtain a low-rank solution), Ax is
the predicted output of the linear model b = Ax∗ + ξ, where x∗
is the ground truth and ξ is the
+, as well as X = {x ∈ Sn| 1
observation error. Let E = Sn = H, and set P(x) = x, K = Sn
(cid:107)Ax−b(cid:107)2 −δ ≤ 0},
2
where δ > 0 is an a-priori bound on 1
(cid:107)ξ(cid:107)2. See [24] for a survey and [14] for an application of
2
Conditional Gradient (CndG) methods to this norm minimization problem.

Example 1.3 (Covering SDP). A dual formulation of the packing SDP is the covering problem [10].
In its normalized version, the problem reads as

m(cid:88)

m(cid:88)

xi

s.t.:

xiAi − I (cid:23) 0, x ∈ Rm
+ ,

min
x

i=1

i=1
, . . . , Am ∈ Sn

+ | (cid:80)m

i=1 xiAi − I, K = Sn

i=1 xi, P(x) = (cid:80)m

where A1
(P) with g(x) = (cid:80)m
X = {x ∈ Rm

+ are given input matrices. This can be seen as a special case of problem
+, H = Sn, E = Rm, and the constraint set
i=1 xi ≤ ρ} for some large ρ added to the problem for the compactiﬁcation purposes.
The model template (P) can be solved with moderate accuracy using fast proximal gradient
methods, like FISTA and related acceleration tricks [8]. In this formalism, the constraints imposed
on the optimization problem have to be included in proximal operators that essentially means
that one can calculate a projection onto the feasible set. Surprisingly, the computation of proximal
operators can impose a signiﬁcant burden and even lead to intractability of gradient-based methods.
This is in particular the case in SDPs with a large number of constraints and decision variables, such
as in convex relaxations of NP-hard combinatorial optimization problems [19]. Other prominent
examples in machine learning are k-means clustering [23], k-nearest neighbor classiﬁcation [27],
sparse PCA [5], kernel learning [18], among many others. In many of these mentioned applications,
it is not only the size of the problem that challenges the application of oﬀ-the-shelve solvers, but also
the desire to obtain sparse solutions. As a result, the CndG method [11] has seen renewed interest in
large-scale convex programming, since it requires only a Linear minimization oracle (LMO) in each
iteration. Compared to proximal methods, CndG features signiﬁcantly reduced computational
costs (e.g. when X is a spectrahedron), tractability and interpretability (e.g. they generate solutions
as a combination of atoms of X). Modern references to this classical methodology are (among many
others) [16, 17].

In this paper we propose a CndG framework for solving (P) with rigorous iteration complexity
guarantees. Our approach retains the simplicity of CndG, but allows to disentangle the sources of
complexity describing the problem. Speciﬁcally, we study the performance of a new homotopy/path-
following method, which iteratively approaches a solution of problem (P). To develop the path-
following argument we start by decomposing the feasible set into two components. We assume
that X is a compact convex set which admits eﬃcient applications of CndG. The challenging
constraints are embodied by the membership condition P(x) ∈ K. These are conic restrictions on
the solution, and in typical applications of our method we have to manage a lot of them. We
propose to do so via a logarithmically homogeneous barrier for the cone K. [22] introduced this class of
functions in connection with polynomial-time interior point methods. It is a well-known fact that
any closed convex cone admits a logarithmically homogeneous barrier (the universal barrier) [22,
Thm. 2.5.1]. A central assumption of our approach is that we have a practical representation of a
logarithmically homogenous barrier for the cone K. As many restrictions appearing in applications

2

are linear or PSD inequalities, this assumption is rather mild. Section 5 presents a wealth of
relevant SDPs which can be eﬃciently handled with our framework. Exploiting the announced
decomposition, we approximate the target problem (P) by a family of parametric penalty-based
composite convex optimization problems

{Vt(x) := 1
t

min
x∈X

F(x) + g(x)},

(1.1)

where t > 0 is a path-following/homotopy parameter, and F is a barrier function over the set P−1(K).
By minimizing the function Vt over the set X for a sequence of increasing values of t, we can trace the
analytic central path z∗
(t) of (1.1) as it converges to a solution of (P). Importantly, our approach gen-
erates a sequence of feasible points for the original problem (P). Exact homotopy/path-following
methods were developed in statistics and signal processing in the context of the (cid:96)
1-regularized least
squares problem (LASSO) [26] to compute the entire regularization path. Relatedly, approximate
homotopy methods have been studied in this context as well [13, 28], and superior empirical perfor-
mance has been reported for seeking sparse solutions. [30] is the ﬁrst reference which investigates
the complexity of a proximal gradient homotopy method for the LASSO problem. Our paper
provides the ﬁrst complexity analysis of an approximate homotopy CndG algorithm to compute
an approximate solution close to the analytic central path for the general problem class (P), build-
ing on and extending the recent work [33]. Speciﬁcally, [33] study minimization of a regularized
self-concordant barrier by a CndG algorithm. Unlike [33], we analyze a more complicated dynamic
problem (1.1), where the objective changes as we update the penalty parameter, and we propose
also a line-search and inexact-LMO versions of the CndG algorithm for (1.1). Furthermore, we
analyze the complexity of the whole path-following process in order to approximate a solution to
the initial non-penalized problem (P). One of the challenging questions here is choosing a penalty
parameter updates schedule, and we develop a practical answer to this question. To the best of
our knowledge, this is the ﬁrst path-following CndG method with explicit complexity analysis.
Speciﬁcally, our contributions can be summarized as follows:

• We introduce a simple CndG framework for solving problem (P) and prove that it achieves a
, C2 are constants depending on the problem

˜O(C1
and parameters of the algorithm, and ˜O(·) hides polylogarithmic factors.

ε−2 + C2ε−1) iteration complexity, where C1

• We provide two extensions for the inner-loop CndG algorithm solving (1.1): a line-search
and an inexact-LMO versions. For both, we show that the complexity of the inner and the
outer loop are the same as in the basic variant up to constant factors.

• We present key instances of our framework, including instances of SDPs arising from convex
relaxations of NP-hard combinatorial optimization problems, and provide promising results
of the numerical experiments.

Specializing our setup to speciﬁc instances of SDPs, we remark that the theoretical complex-
ity achieved by our method matches or even improves the state-of-the-art iteration complexity
results reported in the literature. For packing and covering types of SDPs, the state-of-the-art
upper complexity bound reported in [10] is worse than ours since their algorithm has complexity
, C2 are constants depending on the problem data. For SDPs with
˜O(C1
linear equality constraints and spectrahedron constraints, the primal-dual method CGAL of [32]
provides an approximately optimal and feasible point in O(ε−2) iterations. In contrast, our method
is purely primal, generating feasible points anytime, at the same complexity.
This paper is organized as follows: Section 2 ﬁxes the notation and terminology we use throughout

ε−2.5 + C2ε−2), where C1

3

this paper. Section 3 presents the basic algorithm under consideration in this paper. In section
4 extensions and modiﬁcations of the basic scheme are derived and discussed. Section 5 shows
how many relevant conic optimization problems, which are considered to be hard instances for
perceived methods, can be formulated into our framework. Preliminary numerical results on the
MAXCUT and the Markov Chain mixing problem are also reported there. Section 6 contains all
the technical proofs of the main theorems presented in this paper.

2 Notation & Preliminaries

Let E be a ﬁnite-dimensional Euclidean vector space, E∗
linear functions on E. The value of function s ∈ E∗
positive deﬁnite self-adjoint operator. Deﬁne the norms

at x ∈ E is denoted by (cid:104)s, x(cid:105). Let B : E → E∗

its dual space, which is formed by all
be a

(cid:107)h(cid:107)B = (cid:104)Bh, h(cid:105)1/2
−1s(cid:105)1/2
(cid:107)s(cid:107)∗
= (cid:104)s, B
B
(cid:107)A(cid:107) = max
(cid:107)Ah(cid:107)∗
(cid:107)h(cid:107)≤1

h ∈ E,
s ∈ E∗,
A : E → E∗.

For a diﬀerentiable function f (x) with dom( f ) ⊆ E, we denote by f (cid:48)
f (cid:48)(cid:48)
its Hessian, and by f (cid:48)(cid:48)(cid:48)
of f in direction h ∈ E is denoted as f (cid:48)

its gradient, by
(x) its third derivative. Accordingly, the directional derivative
(x)[h].

(x) : E → E∗

(x) ∈ E∗

2.1 Self-Concordant Functions

Let Q be an open and convex subset of E. A function f : E → (−∞, ∞] is self-concordant (SC) on Q
(x)[h, h])3/2. In case where cl(Q)
if f ∈ C3(Q) and for all x ∈ Q, h ∈ E, we have | f (cid:48)(cid:48)(cid:48)
is a closed convex cone, more structure is available to us. We call f a ν-canonical barrier for Q,
denoted by f ∈ Bν(Q), if it is SC and

(x)[h, h, h]| ≤ 2( f (cid:48)(cid:48)

∀(x, t) ∈ Q × (0, ∞) :

f (tx) = f (x) − ν log(t).

From [22, Prop. 2.3.4], the following properties are satisﬁed by a ν-canonical barrier for each
x ∈ Q, h ∈ E, t > 0:

(x),
(cid:48)(cid:48)
(x)[x, h] ∀h ∈ E,

(cid:48)

(cid:48)

(cid:48)

−1 f
(tx) = t
(x)[h] = − f
(x)[x] = −ν
(cid:48)(cid:48)
(x)[x, x] = ν.

(cid:48)

f

f

f

f

(2.1)

(2.2)

(2.3)

(2.4)

(x)[h, h])1/2 for all x ∈ dom( f ) and h ∈ E. SC functions are in
We deﬁne the local norm (cid:107)h(cid:107)
general not Lipschitz smooth. Still, we have access to a version of a descent lemma of the following
form [21, Thm. 5.1.9]:

f (cid:48)(cid:48)(x) := ( f (cid:48)(cid:48)

f (x + h) ≤ f (x) + f

(cid:48)

(x)[h] + ω∗((cid:107)h(cid:107)

f (cid:48)(cid:48)(x))

for all x ∈ dom( f ) and h ∈ E such that (cid:107)h(cid:107)
that [21, Thm. 5.1.8]

f (cid:48)(cid:48)(x)

f (x + h) ≥ f (x) + f

(cid:48)

(x)[h] + ω((cid:107)h(cid:107)

f (cid:48)(cid:48)(x))

< 1, where ω∗(t) = −t − log(1 − t). It also holds true

(2.5)

(2.6)

4

for all x ∈ dom( f ) and h ∈ E such that x + h ∈ dom( f ), where ω(t) = t − log(1 + t) for t ≥ 0.

A classical and useful bound is [21, Lemma 5.1.5]:

t2
2 − t

≤ ω∗(t) ≤

t2
2(1 − t)

∀t ∈ [0, 1).

2.2 The Optimization Problem

(2.7)

The following assumptions are made for the rest of this paper.
Assumption 1. K ⊂ H is a closed convex cone with int(K) (cid:44) ∅, admitting a ν-canonical barrier
f ∈ Bν(K).

The next assumption transports the barrier setup from the codomain K to the domain P−1(K).
This is a common operation in the framework of the "barrier calculus" developed in [22, Section
5.1].
Assumption 2. The map P : E → H is linear, and F(x) := f (P(x)) is a ν-canonical barrier on the cone
P−1(K), i.e. F ∈ Bν(P−1(K)).

Note that dom(F) = int P−1(K). At this stage a simple example might be useful to illustrate the

working of this transportation technique.

Example 2.1. Considering the normalized covering problem presented in Example 1.3. [10] solve
this problem via a logarithmic potential function method [22], involving the logarithmically homo-
geneous barrier f (X) = log det(X) for X ∈ Sn
+. This is a typical choice in Newton-type methods to
impose the semideﬁniteness constraint. However, in our projection-free environment, we use the
power of the linear minimization oracle to obtain search directions which leaves the cone of positive
semideﬁnite matrices invariant. Instead, we employ barrier functions to incorporate the additional
linear constraints in (P). Hence, we set F(x) = f (P(x)) = log det(
i=1 xiAi −I) to absorb the constraint
P(x) ∈ Sn
+. In particular, this frees us from matrix inversions, and related computationally intensive
steps coming with Newton and interior-point methods.
Assumption 3. X is a nonempty compact convex set in E, and C := X ∩ dom(F) ∩ dom(g) (cid:44) ∅.

(cid:80)m

Let Opt := min{g(x)|x ∈ X, P(x) ∈ K}. Thanks to assumptions 1 and 3, Opt is attained. Our goal

is to ﬁnd an ε-solution of problem (P), deﬁned as follows.
Deﬁnition 2.1. Given a tolerance ε > 0, we say that z∗
ε ∈ P−1(K) ∩ X and g(z
∗
z

ε is an ε-solution for (P) if
∗
ε) − Opt ≤ ε.

We underline that we seek for a feasible ε-solution of problem (P).
Given t > 0, deﬁne

Opt(t) := min
x∈X

{Vt(x) = 1
t

∗
F(x) + g(x)}, and z

(t) ∈ {x ∈ X|Vt(x) = Opt(t)}.

(2.8)

The following Lemma shows that the path t (cid:55)→ z∗
(t) traces a trajectory in the interior of the feasible
set which can be used to approximate a solution of the original problem (P), provided the penalty
parameter t is chosen large enough.
Lemma 2.2. For all t > 0, it holds that z∗

(t) ∈ C. In particular,

∗
g(z

(t)) − Opt ≤

.

ν

t

5

(2.9)

Proof. Since F ∈ Bν(P−1(K)), it follows immediately that z∗
we have

(t) ∈ dom(F) ∩ X. By Fermat’s principle

0 ∈ 1
t

(cid:48)

∗
(z

∗
(t)) + ∂g(z

F

∗
(t)) + NCX(z

(t)).

Convexity and [21, Thm. 5.3.7] implies that for all z ∈ dom(F) ∩ X, we have

∗
g(z) ≥ g(z

(t)) − 1
t

(cid:48)
F

∗

∗
(t))[z − z

∗
(t)] ≥ g(z

(z

(t)) −

.

ν

t

Let z∗
with z j → z∗

be a feasible point point satisfying g(z∗
, the continuity on dom(g) gives

) = Opt. Choosing a sequence {zj} j∈N ⊂ dom(F) ∩ X

Opt ≥ g(z

∗

(t)) −

.

ν

t

(cid:4)

Lemma 2.2 seems to indicate that our aim of ﬁnding an ε-solution is already achieved: It seems
to suﬃce to pick t = ν
ε , in order to meet our target. Unfortunately this naïve choice is usually not
working well in practice. Choosing t large (i.e. ε small) from the outset means that the subproblems
involved in any iterative solver are nearly ill-conditioned. In practice this typically manifests in
numerical stability issues and slow convergence. Instead, path-following ideas and continuation
methods are usually designed in which the homotopy parameter t is increasing.

3 Algorithm & Convergence

In this section we ﬁrst describe a new CndG method for solving general conic constrained convex
optimization problems of the form (2.8), for a ﬁxed t > 0. This procedure serves as the inner loop of
our homotopy method. The path-following strategy in the outer loop is then explained in Section
3.2.

3.1 The Proposed CndG Solver

To make our approach eﬃcient, we rely on the following Assumption:

Assumption 4. For any c ∈ E∗

, the auxiliary problem

Lg(c) := argmin

{(cid:104)c, s(cid:105) + g(s)}

s∈X

(3.1)

is easily solvable.

We can compute the gradient and Hessian of F as

(cid:48)
F

(x) = f

(cid:48)

(P(x))P(cid:48)

(x),

(cid:48)(cid:48)

F

(x)[s, t] = f

(cid:48)(cid:48)

(P(x))[P(s), P(t)] ∀x ∈ int P−1(K).

This means that we obtain a local norm on H given by

(cid:107)w(cid:107)x = F

(cid:48)(cid:48)

(x)[w, w]1/2

∀(x, w) ∈ int P−1(K) × H.

Note that in order to evaluate the local norm we do not need to compute the full Hessian F(cid:48)(cid:48)
(x).
It only requires a directional derivative, which is potentially easy to do numerically. Deﬁne the

6

(x0, t) ∈ C × (0, ∞) initial state; ε > 0 accuracy level

Algorithm 1: CG(x0, ε, t)

Input:
for k = 0, 1, . . . do

if Gapt(xk) > ε then

Obtain sk = st(xk) deﬁned in (3.2).
α
k
Set xk+1 = xk + α

= αt(xk) deﬁned in (3.6).
k(sk − xk).

else

Return xk.

end if
end for

vector ﬁeld

st(x) ∈ Lg(t

(cid:48)
−1F

(x)).

(3.2)

Note that our analysis does not rely on a speciﬁc tie-breaking rule, so any proposal of the oracle (3.1)
will be acceptable. To measure solution accuracy and overall algorithmic progress, we introduce
two merit functions:
(cid:48)
−1F
∆t(x) := Vt(x) − Vt(z

(x)[x − st(x)] + g(x) − g(st(x)), and

Gapt(x) := t

(t)).

(3.4)

(3.3)

∗

Note that Gapt(x) ≥ 0 and ∆t(x) ≥ 0 for all x ∈ dom(F). Convexity together with the deﬁnition of
the point z∗

(t), gives

0 ≤ ∆t(x) = t
−1F
(cid:48)
(cid:48)
−1F

∗
−1[F(x) − F(z
∗

∗
(t))] + g(x) − g(z
∗
(t)] + g(x) − g(z

(x)[x − z
(x)[x − st(x)] + g(x) − g(st(x)) = Gapt(x).

≤ t
≤ t

(t))

(t))

Deﬁne

et(x) := (cid:107)st(x) − x(cid:107)x.

Then, for α ∈ (0, min{1, 1/et(x)}), we get from eq. (2.5)

(cid:48)
F(x + α(st(x) − x)) ≤ F(x) + αF

(x)[st(x) − x] + ω∗(αet(x)).

Together with the convexity of g, this implies

Vt(x + α(st(x) − x)) ≤ Vt(x) − α Gapt(x) + t
We optimize the r.h.s. in α to obtain the analytic step-size policy

−1ω∗(αet(x)).

(cid:40)

αt(x) := min

1,

t Gapt(x)
et(x)(et(x) + t Gapt(x))

(cid:41)

∈ [0, 1/et(x)].

(3.5)

(3.6)

Equipped with this step strategy, procedure CG(x0, ε, t), described in Algorithm 1, constructs a
sequence {xk
k≥0 which produces an approximately-optimal solution in terms of the merit function
t
Gapt(·) and the potential function gap ∆t. Speciﬁcally, the following iteration complexity results
can be established.

}

7

Algorithm 2: Method Homotopy(x0, ε, f )

= x0, I = (cid:100) log(2η0/ε)
log(1/σ)

Input: x0 ∈ C, f ∈ Bν(K).
Parameters: ε, t0, η0 > 0, σ ∈ (0, 1).
(cid:101).
Initialize: x0
t0
for i = 0, 1, . . . , I do
Set Ri = R(x0
, ηi, ti).
ti
Set ˆxi = xRi
, ηi, ti).
the last iterate of CG(x0
ti
ti
= ˆxi.
= ti/σ, ηi+1
Update ti+1

= σηi, x0
ti+1

end for
ˆz = ˆxI an ε-solution of (P).

Proposition 3.1. Given η, t > 0. Let R(x0
t
Gapt(xk

t ) ≤ η. Then

, η, t) be the ﬁrst iterate k of Algorithm CG(x0
t

, η, t) satisfying

R(x0
t

, η, t) ≤ (cid:100)5.3(ν + t∆t(x0

t ) + tΩg) log(10.6t∆t(x0

t ))(cid:101) + (cid:100) 24

tη (ν + tΩg)2(cid:101),

where Ωg := maxx,y∈dom(g)∩X|g(x) − g(y)|.

Proposition 3.2. Algorithm CG(x0
t

, η, t) requires at most

N(x0
t

, η, t) := (cid:100)5.3(ν + t∆t(x0

t ) + tΩg) log(10.6t∆t(x0

t ))(cid:101) + (cid:100)12(ν + tΩg)2(

−

1
tη

1
t∆t(x0
t )

+(cid:101)
)

(3.7)

iterations, in order to reach a point satisfying ∆t(xk

t ) ≤ η.

We prove these results in Section 6.1.

3.2 Updating the Homotopy Parameter
Our analysis so far focuses on minimization of the potential function Vt(x) for a ﬁxed t > 0.
However, in order to solve the initial problem (P), one must trace the sequence of approximate
solutions as ti ↑ ∞. The construction of such an increasing sequence of homotopy parameters is
the purpose of this section.

Our aim is to reach an ε-solution (Deﬁnition 2.1). Let {ηi}i≥0

, {ti}i≥0 be a sequence of approxi-
, ηi, ti) with
mation errors and homotopy parameters. For each run i, we activate procedure CG(x0
ti
the given conﬁguration (ηi, ti). For i = 0, we assume to have an admissible initial point x0
= x0
t0
available. For i ≥ 1, we restart Algorithm 1 using a kind of warm-start strategy by choosing
= xRi−1
, ti−1) satisfy-
, ti−1) is the ﬁrst iterate k of Algorithm CG(x0
x0
ti−1
ti
ti−1
) ≤ ηi−1. Note that Ri−1 is upper bounded in Proposition 3.1. After the i-th restart,
ing Gapti−1(xk
ti−1
let us call the obtained iterate ˆxi := xRi
In this way we obtain a sequence { ˆxi}i≥0, consisting of
ti
candidates for approximately following the central path, as they are ηi-close in terms of the gap
Gapti (and, hence, in terms of the function gap) to the stage ti’s optimal point z∗
(ti). We update the
parameters (ti, ηi) as follows:

, where Ri−1

≡ R(x0

, ηi−1

, ηi−1

ti−1

.

• The sequence of homotopy parameters is determined as ti = t0σ−i for σ ∈ (0, 1) until the last

round of Homotopy(x0, ε, f ) is reached.

8

Algorithm 3: CndG with line search: LCG(x0, ε, t)

(x0, t) ∈ C × (0, ∞) initial state; ε > 0 accuracy level

Input:
for k = 0, 1, . . . do

if Gapt(xk) > ε then

Same as Algorithm 1, but with step size strategy (4.1).

else

Return xk.

end if
end for

• The sequence of accuracies requested in the algorithm CG(x0
ti

, ηi, ti) is updated by ηi = η0σi.

• The Algorithm stops after I ≡ I(σ, η0, ε) = (cid:100) log(2η0/ε)
log(1/σ)

parameters and yields an ε-approximate solution of problem (P).

(cid:101) updates of the accuracy and homotopy

Our updating strategy for the parameters ensures that tiηi = t0η0. This equilibrating choice
between the increasing homotopy parameter and the decreasing accuracy parameter is sensible,
because the iteration complexity of the CndG solver is inversely proportional to tiηi (cf. Proposition
3.1). Making the judicious choice η0t0 = 2ν, yields a compact assessment of the total complexity of
method Homotopy(x0, ε, f ).
Theorem 3.3. Choose t0 = ν
Ωg
to ﬁnd an ε-solution of (P) is Compl(x0, ε, f ) = ˜O
˜O hides polylogarithmic factors in ε−1.

and η0 = 2Ωg. The total iteration complexity of method Homotopy(x0, ε, f )
, where

+ Ωg log(21.2ν(1 + (2/ε)Ωg)) 21.2ν

(cid:18) 384Ω2
ν
ε2(1−σ2)

2−σ
1−σ

(cid:19)

ε

g

The theorem leaves open the choice of the parameter σ. This is a tunable hyperparameter that

can be chosen in a tailor-made fashion. The proof of Theorem 3.3 can be found in Section 6.2.

4 Extensions

4.1 Line Search

The step size policy employed in Algorithm 1 is inversely proportional to the local norm et(x). In
particular, its derivation is based on the a-priori restriction that we force our iterates to stay within
a trust-region deﬁned by the Dikin ellipsoid. This restriction may force the method to take very
small steps, and as a result display bad performance in practice. A simple remedy to this is to use
the line search. Given (x, t) ∈ dom(F) × (0, +∞), let

γt(x) =

argmin
γ∈[0,1] s.t. x+γ(st(x)−x)∈dom(F)

Vt(x + γ(st(x) − x)).

(4.1)

Thanks to the barrier structure of the potential function Vt, some useful consequences can be
drawn from the deﬁnition of γt(x). First, γt(x) ∈ {γ ≥ 0|x + γ(st(x) − x) ∈ dom(F)}. This implies
x + γt(x)(st(x) − x) ∈ X ∩ dom(F) ∩ dom(g). Second, since αt(x) is also contained in the latter set, we
have

Vt(x + γt(x)(st(x) − x)) ≤ Vt(x + αt(x)(st(x) − x))

∀(x, t) ∈ dom(F) × (0, +∞).

9

Algorithm 4: CndG with inexact oracle: ICG(x0, ε, t)

(x0, t) ∈ C × (0, ∞) initial state; ε > 0 accuracy level, γ ∈ [0, 1) inexactness parameter

Input:
for k = 0, 1, . . . do

if (cid:103)Gapt(x) > (1 − γ)ε then

Find ˜sk = ˜st(xk) ∈ X such that (4.2) holds at xk.
Set α
= ˜αt(xk) by evaluating eq. (4.4).
Update xk+1 = xk + α

k(˜sk − xk).

k

else

Return xk.

end if
end for

Via a comparison principle, this allows us to deduce the analysis of the sequence produced by
LCG(x0, ε, t) from the analysis of the sequence induced by CG(x0, ε, t). Indeed, if {ξk
k≥0 is the sequence
t
constructed by the line search procedure LCG(x0, η, t), then we have Vt(ξk+1
) ≤ Vt(ξk
t ) −
t
xk
t )) for all k. Hence, we can perform the complexity analysis on the majorizing function as in the
analysis of procedure CG(x0, ε, t). Consequently, all the complexity-related estimates for the method
CG(x0, η, t) apply verbatim to the sequence {ξk
t

k≥0 induced by the method LCG(x0, η, t).

t )(st(ξk

+ αt(ξk

}

}

t

4.2

Inexact Implementation

Algorithm 1 assumes that the LMO performs exact computation of the search direction st(x). This
may not be directly available in practice.
Just like in [16], we show that our analysis carries
essentially through even in the relaxed LMO model with inexact implementation. Let γ ∈ [0, 1) be
a given parameter, measuring the approximation quality of the oracle (3.1) relative to the target
accuracy η. Instead of the search direction st(x), suppose the LMO returns us a point ˜st = ˜st(x)
satisfying

(cid:48)
−1F

t

(x)[˜st(x)] + g(˜st(x)) ≤ min
s∈X

(cid:110)
−1F
t

(cid:48)

(cid:111)
(x)[s] + g(s)

+ γη.

(4.2)

Following the derivations performed in the exact computational model, we are deﬁning the inexact
gap function

(cid:103)Gapt(x) := t

(cid:48)
−1F

(x)[x − ˜st(x)] + g(x) − g(˜st(x)),

and the associated analytic step size policy, using ˜et(x) := (cid:107)˜st(x) − x(cid:107)x,

˜αt(x) = min



1,


t (cid:103)Gapt(x)
˜et(x)(˜et(x) + t (cid:103)Gapt(x))





.

(4.3)

(4.4)

Remark 4.1. Note that as η → 0, the LMO’s accuracy as deﬁned in (4.2) improves. [16] achieves
such an improving oracle by coupling the accuracy imposed on the subproblem solver with the
1/k-step size policy employed in the vanilla CndG.

The next propositions are the resulting iteration complexity statements of Algorithm ICG(x0, η, t),
mimicking the statements in Proposition 3.2 and 3.1. Since the analysis of the inexact implementa-
tion regime is analogous to the exact LMO model, we provide sketches of proofs of these statements
in Section 6.3.

10

Proposition 4.1. Algorithm ICG(x0
t
satisfying ∆t(xk

t ) ≤ η, where

, η, t) requries at most ˜Nγ(x0
t

, η, t) iterations, in order to reach a point

˜Nγ(x0
t

, η, t) := Kt,γ(x0

t ) + (cid:100)12(ν + tΩg)2





1
tη

−

1
t ) − γη)
t(∆t(x0

+





(cid:101),

and

Kt,γ(x0

t ) :=





0
(cid:100)5.3(ν + t(Ωg − γη) + t∆t(x0

t ))(cid:101) log

(cid:18) 10.6t∆t(x0
t )
1−10.6tγη

(cid:19)

if γη ≥ 1
10.6t
.
if γη < 1
10.6t

(4.5)

(4.6)

Proposition 4.2. Given η, t > 0 and γ ∈ (0, 1/3). Let ˜R(x0
t
for Algorithm ICG(x0, η, t). Then

, η, t) be the ﬁrst iterate k satisfying Gapt(xk

t ) ≤ η

˜Rγ(x0
t

, η, t) ≤ Kt,γ(x0

t ) + (cid:100)

24

t(1 − γ)η (ν + tΩg)2(cid:101),

where Kt,γ(x0

t ) is deﬁned in (4.6).

Remark 4.2. Observe that all the complexity estimates reduce to the ones proved for the exact oracle
model by setting γ = 0.

Based on the above estimates, it becomes clear that the worst-case bounds on the overall iteration
complexity of Homotopy(x0, ε, f ) with ICG(x0, η, t) as the subroutine is the same as in Theorem 3.3,
subject to the adjustment of constant factors.

5 Examples and Numerical Experiments

In this section we give several examples covered by the problem template (P) and report some
preliminary numerical experiments illustrating the practical performance of our method.

5.1 A Generic Model Problem

The following class of SDPs is studied in [2]. This SDP arises in many algorithms such as approxi-
mating MAXCUT, approximating the CUTNORM of a matrix, and approximating solutions to the
little Grothendieck problem [3, 4]. The program is given by

max(cid:104)C, X(cid:105)

s.t. Xii ≤ 1 i = 1, . . . , n

(MAXQP)

X (cid:23) 0
Let Q := {y ∈ Rn|yi ≤ 1,
Rn × R| 1

t y ∈ Q, t > 0} ⊂ Rn+1. This set admits the ν = n logarithmically homogenous barrier

i = 1, . . . , n}, and consider the conic hull of Q, deﬁned as K := {(y, t) ∈

f (y, t) = −

n(cid:88)

i=1

log(t − yi) = −

n(cid:88)

i=1

log(1 − 1
t

yi) − n log(t).

We can then reformulate (MAXQP) as

{g(X, t) := (cid:104)C, X(cid:105)}

max
X,t

s.t. (X, t) ∈ X := {X ∈ Sn × R|X (cid:23) 0, tr(X) ≤ n, t = 1},

(5.1)

P(X, t) ∈ K

11

where P(X, t) := [X11; . . . ; Xnn; t]
F(X, t) = f (P(X, t)) for (X, t) ∈ Sn × (0, ∞).

(cid:62)

is a linear homogenous mapping from Sn × R → Rn+1. Set

Data set

# Nodes

# Vertices Value λ

SDPT3

G1
G2
G3
G4
G5
G6
G7
G8
G9
G10
G11
G12
G13
G14
G15
G16
G17
G18
G19
G20
G21
G22
G23
G24
G25
G26
G27
G28
G29
G30

800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
2000
2000
2000
2000
2000
2000
2000
2000
2000

19176
19176
19176
19176
19176
19176
19176
19176
19176
19176
19176
19176
19176
19176
19176
4672
4667
4694
4661
4672
4667
19990
19990
19990
19990
19990
19990
19990
19990
19990

48333
48357
48337
48446
48400
10668
9977
10141
48446
48446
48446
48446
48446
48446
48446
12700
12685
4695
4364
4476
4448
73108
73157
73147
73174
73083
33218
33014
33511
33524

min

Feas.
value
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
48262
39951
39991
39984
39982
16567
16403
16836
16862

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
-1
-1
-1
-1
-1
-1
-1
-1
-1

Table 1: Max-Cut datasets

We apply this formulation to the classical MAXCUT problem in which C = L, i.e. the combina-
torial Laplace matrix of an undirected graph G = (V, E) with vertex set V = {1, . . . , n} and edge set E.
To evaluate the performance, we consider the random graphs G1-G30, published online in [1]. We
implement Algorithm 2 using procedure CG and LCG as solvers for the inner loops. The Max-Cut
problem was run using Matlab R2019b on an Intel(R) Xeon(R) Gold 6254 CPU @ 3.10GHz server
limited to 4 threads per run and 512G total RAM. We used CVX version 2.2 with SDPT3 version 4.0
to obtain the interior point solution which we take as a reference value in order to benchmark our
results. The benchmark solutions are displayed in Table 1. The relative gap we compute to assess

the relative performance of our method is then computed as

g(x)−g(xref)
|g(xref)|

. Table 1 also displays the

12

Figure 1: Max-Cut datasets G1-G21 relative gap from optimal solution vs. iteration.

13

G19G20G21G16G17G18G13G14G15G10G11G12G7G8G9G4G5G6G1G2G31101001000100001101001000100001101001000100000.010.101.000.010.101.000.010.101.000.010.101.000.010.101.000.010.101.000.010.101.00IterationOptimality GapAlgorithmCGLCGs0.50.90.99Figure 2: Max-Cut datasets G1-G21 relative gap from optimal solution vs. time.

14

G19G20G21G16G17G18G13G14G15G10G11G12G7G8G9G4G5G6G1G2G31e−011e+011e+031e−011e+011e+031e−011e+011e+030.010.101.000.010.101.000.010.101.000.010.101.000.010.101.000.010.101.000.010.101.00Time (sec)Optimality GapAlgorithmCGLCGs0.50.90.99size of each dataset, the value obtained by solving the Max-Cut SDP relaxation using CVX with
the SDPT3 solver and the minimum eigenvalue of the solution λ
min(X). We observe that for larger
graphs SDPT3 returns infeasible solutions, featuring negative eigenvalues. If this occurs, the value
obtained by a corrected solution is used as a reference value instead. The corrected solution ˜X is
min(X)I)/α where I is the identity matrix, and α is the minimal number for
obtained as ˜X = (X − λ
which ˜Xii ≤ 1 for all i = 1, . . . , n. Figures 1-6 illustrate the performance of our methods for various
parameter values, and Table 2 collects the numerical values obtained for the schemes CG and LCG,
in dependence of the scaling factor σ.

15

Figure 3: Max-Cut datasets G1-G21 objective function value vs. iteration.

16

G19G20G21G16G17G18G13G14G15G10G11G12G7G8G9G4G5G6G1G2G311010010001000011010010001000011010010001000001000020000300004000050000025005000750010000025005000750010000050010001500200025000250050007500100001250001000200030004000010002000300040000100002000030000400005000001000020000300004000050000025005000750010000050010001500200025000250050007500100001250002500500075001000012500010002000300040000100002000030000400005000001000020000300004000050000025005000750010000025005000750010000050010001500200025000250050007500100001250001000200030004000IterationObjectiveAlgorithmCGLCGs0.50.90.99Figure 4: Max-Cut datasets G1-G21 objective function value vs. time.

17

G19G20G21G16G17G18G13G14G15G10G11G12G7G8G9G4G5G6G1G2G31e−011e+011e+031e−011e+011e+031e−011e+011e+0301000020000300004000050000025005000750010000025005000750010000050010001500200025000250050007500100001250001000200030004000010002000300040000100002000030000400005000001000020000300004000050000025005000750010000050010001500200025000250050007500100001250002500500075001000012500010002000300040000100002000030000400005000001000020000300004000050000025005000750010000025005000750010000050010001500200025000250050007500100001250001000200030004000Time (sec)ObjectiveAlgorithmCGLCGs0.50.90.99Figure 5: Max-Cut datasets G22-G30 objective function value vs. iteration.

Table 2: Results for datasets G1-G30 after a number of iterations, including objective function value,
relative gap from SDPT3 corrected solution, and time (in seconds).

Dataset

Iter.

Alg.
σ

0.99

CG

0.9

0.5

0.99

LCG

0.9

0.5

Value
g(z)
Gap
Time

G1

100

1000

10000

G2

100

29141
3.97e-01
14
45292
6.29e-02
171
47785
1.13e-02
2013
29127
3.98e-01

29500
3.90e-01
15
45290
6.29e-02
169
47788
1.13e-02
2033
29371
3.93e-01

28611
4.08e-01
16
45175
6.53e-02
188
47776
1.15e-02
2126
28544
4.10e-01

33151
3.14e-01
34
46034
4.76e-02
278
47813
1.08e-02
2619
33314
3.11e-01

35604
2.63e-01
50
46139
4.54e-02
305
47828
1.04e-02
2549
35423
2.67e-01

35605
2.63e-01
45
46204
4.40e-02
298
47861
9.77e-03
2490
35324
2.70e-01

18

G28G29G30G25G26G27G22G23G241101001000100001101001000100001101001000100000200004000005000100001500005000100001500002000040000020000400000500010000150000200004000002000040000050001000015000IterationObjectiveAlgorithmCGLCGs0.50.90.991000

10000

G3

100

1000

10000

G4

100

1000

10000

G5

100

1000

10000

G6

100

1000

10000

G7

100

1000

15
45268
6.39e-02
165
47809
1.13e-02
1947
28622
4.08e-01
16
45258
6.37e-02
175
47796
1.12e-02
2066
28967
4.02e-01
15
45402
6.28e-02
162
47911
1.10e-02
1837
28920
4.02e-01
17
45355
6.29e-02
181
47866
1.10e-02
2125
3119
7.08e-01
15
9819
7.96e-02
202
10497
1.60e-02
3188
3569
6.42e-01
18
9171
8.08e-02

16
45268
6.39e-02
163
47812
1.13e-02
1957
28917
4.02e-01
16
45241
6.41e-02
172
47800
1.11e-02
2090
29132
3.99e-01
15
45403
6.28e-02
159
47914
1.10e-02
1860
29414
3.92e-01
15
45365
6.27e-02
178
47870
1.09e-02
2113
6048
4.33e-01
17
9788
8.25e-02
208
10486
1.71e-02
3029
5641
4.35e-01
19
9137
8.42e-02

16
45134
6.67e-02
181
47797
1.16e-02
1990
28167
4.17e-01
16
45082
6.73e-02
193
47784
1.14e-02
2172
28376
4.14e-01
14
45235
6.63e-02
173
47922
1.08e-02
1911
28296
4.15e-01
17
45184
6.64e-02
195
47868
1.10e-02
2106
5803
4.56e-01
21
9652
9.52e-02
246
10450
2.04e-02
3086
5359
4.63e-01
18
9020
9.59e-02

19

34
46048
4.78e-02
270
47835
1.08e-02
2689
33150
3.14e-01
33
46015
4.80e-02
286
47821
1.07e-02
2664
33013
3.19e-01
29
46183
4.67e-02
260
47939
1.05e-02
2624
32808
3.22e-01
37
46148
4.65e-02
306
47891
1.05e-02
2822
3280
6.93e-01
26
10088
5.43e-02
280
10528
1.31e-02
3941
3771
6.22e-01
28
9438
5.40e-02

54
46148
4.57e-02
318
47850
1.05e-02
2753
35287
2.70e-01
50
46111
4.61e-02
310
47833
1.04e-02
2574
35264
2.72e-01
51
46283
4.46e-02
311
47952
1.02e-02
2712
35454
2.67e-01
54
46234
4.48e-02
319
47905
1.02e-02
2593
7529
2.94e-01
16
10062
5.68e-02
134
10505
1.53e-02
1649
7098
2.89e-01
35
9415
5.63e-02

54
46203
4.46e-02
343
47884
9.79e-03
2742
34566
2.85e-01
48
46141
4.54e-02
316
47874
9.59e-03
2542
35484
2.68e-01
52
46355
4.32e-02
323
47995
9.30e-03
2661
35118
2.74e-01
48
46247
4.45e-02
313
47945
9.40e-03
2581
7565
2.91e-01
36
10024
6.03e-02
281
10471
1.85e-02
2828
6913
3.07e-01
39
9335
6.43e-02

10000

G8

100

1000

10000

G9

100

1000

10000

G10

100

1000

10000

G11

100

1000

10000

G12

100

1000

10000

235
9817
1.60e-02
3315
2788
7.23e-01
14
9282
7.79e-02
208
9910
1.56e-02
3272
2901
7.14e-01
18
9322
8.07e-02
247
9980
1.58e-02
3565
3378
6.61e-01
17
9156
8.23e-02
224
9816
1.62e-02
3244
350
8.62e-01
14
1922
2.43e-01
211
2396
5.65e-02
2792
369
8.53e-01
15
1961
2.20e-01
198
2393
4.80e-02

234
9803
1.74e-02
3224
5812
4.23e-01
17
9263
7.98e-02
220
9896
1.69e-02
3186
5539
4.54e-01
17
9300
8.29e-02
236
9968
1.71e-02
3381
5716
4.27e-01
17
9126
8.54e-02
224
9804
1.73e-02
3087
635
7.50e-01
15
1932
2.39e-01
212
2392
5.79e-02
2826
655
7.39e-01
16
1969
2.17e-01
200
2390
4.90e-02

232
9774
2.03e-02
2878
5614
4.42e-01
18
9141
9.19e-02
229
9859
2.05e-02
2561
5214
4.86e-01
18
9162
9.66e-02
244
9934
2.04e-02
3101
5486
4.50e-01
16
9007
9.73e-02
203
9775
2.02e-02
2545
665
7.38e-01
16
1911
2.47e-01
221
2379
6.31e-02
2650
652
7.41e-01
17
1938
2.29e-01
232
2380
5.32e-02

20

261
9839
1.38e-02
3509
2931
7.09e-01
20
9544
5.19e-02
215
9931
1.34e-02
2810
3073
6.97e-01
26
9588
5.46e-02
274

3557
6.43e-01
24
9424
5.54e-02
235
9844
1.34e-02
3019
419
8.35e-01
23
2027
2.02e-01
264
2409
5.11e-02
3060
422
8.32e-01
18
2047
1.86e-01
217
2402
4.43e-02

273
9817
1.60e-02
3352
7333
2.72e-01
24
9520
5.43e-02
200
9909
1.57e-02
2359
6966
3.13e-01
35
9570
5.63e-02
289
9985
1.54e-02
3520
7146
2.84e-01
28
9405
5.74e-02
222
9825
1.52e-02
2664
853
6.64e-01
22
2035
1.98e-01
270
2407
5.22e-02
3053
811
6.77e-01
19
2051
1.84e-01
229
2400
4.54e-02

289
9792
1.85e-02
3095
7095
2.95e-01
24
9486
5.76e-02
187
9876
1.89e-02
2045
6708
3.39e-01
36
9498
6.35e-02
308
9953
1.86e-02
3245
6910
3.07e-01
29
9325
6.54e-02
229
9796
1.81e-02
2414
864
6.60e-01
27
2021
2.04e-01
292
2402
5.42e-02
2891
869
6.54e-01
21
2024
1.95e-01
233
2389
4.95e-02

G13

100

1000

10000

G14

100

1000

10000

G15

100

1000

10000

G16

100

1000

10000

G17

100

1000

10000

G18

100

2751
351
8.65e-01
8
2034
2.20e-01
111
2490
4.45e-02
3046
1313
8.97e-01
4
9101
2.87e-01
302
12071
5.44e-02
4359
1092
9.14e-01
3
8824
3.04e-01
370
11961
5.72e-02
4233
1474
8.84e-01
8
9014
2.90e-01
474
11981
5.66e-02
5148
1175
9.07e-01
3
8869
3.01e-01
168
11942
5.86e-02
3842
435
9.07e-01

2928
665
7.45e-01
20
2050
2.14e-01
251
2487
4.57e-02
3126
2909
7.72e-01
9
9127
2.85e-01
416
12056
5.56e-02
4129
2786
7.80e-01
8
8913
2.97e-01
388
11947
5.83e-02
3815
2979
7.65e-01
8
9014
2.90e-01
420
11965
5.79e-02
4298
2937
7.68e-01
9
8919
2.97e-01
391
11924
6.00e-02
3932
2134
5.45e-01

2866
662
7.46e-01
17
2003
2.31e-01
231
2476
4.98e-02
2741
2794
7.81e-01
7
8753
3.14e-01
350
11929
6.56e-02
3151
2750
7.83e-01
6
8456
3.33e-01
325
11787
7.09e-02
3188
2825
7.78e-01
4
8653
3.19e-01
273
11851
6.69e-02
2691
2948
7.68e-01
6
8494
3.30e-01
330
11790
7.06e-02
3807
2038
5.66e-01

21

2619
418
8.40e-01
13
2124
1.85e-01
160
2499
4.13e-02
3158
1335
8.95e-01
8
9381
2.65e-01
256
12141
4.90e-02
2756
1103
9.13e-01
9
9133
2.80e-01
196
12022
5.24e-02
4010
1451
8.86e-01
9
9314
2.67e-01
213
12052
5.10e-02
4131
1179
9.07e-01
13
9110
2.82e-01
331
12006
5.35e-02
3511
451
9.04e-01

2559
829
6.82e-01
25
2135
1.81e-01
299
2496
4.21e-02
3249
2978
7.67e-01
10
9379
2.65e-01
260
12129
4.99e-02
2776
3182
7.49e-01
20
9187
2.76e-01
432
12014
5.30e-02
4101
3070
7.58e-01
19
9270
2.70e-01
457
12038
5.21e-02
4316
3129
7.53e-01
15
9170
2.77e-01
351
11999
5.41e-02
3477
2643
4.37e-01

2241
859
6.70e-01
25
2112
1.90e-01
283
2486
4.60e-02
2849
3092
7.58e-01
8
9060
2.90e-01
244
12010
5.93e-02
2488
3163
7.51e-01
16
8766
3.09e-01
365
11865
6.47e-02
3329
2970
7.66e-01
16
8952
2.95e-01
423
11931
6.06e-02
3339
3313
7.39e-01
16
8800
3.06e-01
336
11870
6.43e-02
3448
2548
4.57e-01

1000

G19

100

1000

G20

100

1000

G21

100

1000

G22

100

1000

10000

G23

100

1000

10000

G24

100

1000

10000

4
4075
1.32e-01
186
210
9.52e-01
3
3816
1.26e-01
254
691
8.46e-01
6
3931
1.22e-01
340
396
9.11e-01
9
3890
1.25e-01
491
25464
4.72e-01
23
50906
-5.48e-02
257
55618
-1.52e-01
3021
20667
4.83e-01
31
51116
-2.79e-01
394
55639
-3.93e-01
4605
19364
5.16e-01
28
51035
-2.76e-01
362
55615
-3.91e-01

26
4062
1.35e-01
430
2062
5.27e-01
25
3809
1.27e-01
447
2141
5.22e-01
26
3908
1.27e-01
428
2161
5.14e-01
27
3879
1.28e-01
535
24946
4.83e-01
20
50819
-5.30e-02
257
55605
-1.52e-01
3026
25646
3.58e-01
34
51217
-2.82e-01
409
55641
-3.93e-01
4604
25066
3.73e-01
33
51119
-2.78e-01
390
55623
-3.91e-01

22
3952
1.58e-01
384
1921
5.60e-01
24
3731
1.45e-01
451
2028
5.47e-01
25
3809
1.49e-01
444
2065
5.36e-01
32
3775
1.51e-01
557
23293
5.17e-01
21
50279
-4.18e-02
259
55516
-1.50e-01
2870
24530
3.86e-01
32
50980
-2.76e-01
394
55670
-3.93e-01
4074
23552
4.11e-01
32
50828
-2.71e-01
389
55646
-3.91e-01

22

16
4230
9.91e-02
341
209
9.52e-01
12
3944
9.62e-02
336
702
8.43e-01
18
4058
9.32e-02
350
403
9.09e-01
14
4025
9.51e-02
334
34937
2.76e-01
162
53128
-1.01e-01
1392
55827
-1.57e-01
10531
25985
3.50e-01
99
52939
-3.25e-01
1052
55764
-3.96e-01
10485
24082
3.98e-01
99
52847
-3.21e-01
1065
55750
-3.94e-01

30
4222
1.01e-01
358
2460
4.36e-01
28
3943
9.67e-02
344
2534
4.34e-01
32
4037
9.79e-02
361
2638
4.07e-01
28
4019
9.65e-02
353
35727
2.60e-01
167
53106
-1.00e-01
1447
55828
-1.57e-01
10514
36225
9.33e-02
140
53089
-3.29e-01
1088
55769
-3.96e-01
9659
34833
1.29e-01
123
52957
-3.24e-01
1057
55762
-3.94e-01

31
4157
1.15e-01
373
2376
4.56e-01
28
3881
1.11e-01
337
2548
4.31e-01
33
3961
1.15e-01
376
2485
4.41e-01
26
3965
1.09e-01
337
33583
3.04e-01
76
52908
-9.63e-02
692
55790
-1.56e-01
8783
34699
1.31e-01
126
53198
-3.32e-01
1081
55843
-3.98e-01
9384
33722
1.57e-01
131
53104
-3.28e-01
1123
55826
-3.96e-01

G25

100

1000

10000

G26

100

1000

10000

G27

100

1000

10000

G28

100

1000

10000

G29

100

1000

10000

G30

100

4371
18750
5.31e-01
28
51044
-2.77e-01
380
55632
-3.91e-01
4386
18204
5.45e-01
29
50949
-2.74e-01
399
55579
-3.90e-01
4634
61
9.96e-01
19
14788
1.07e-01
484
16302
1.60e-02
6601
48
9.97e-01
20
14634
1.08e-01
480
16156
1.51e-02
6620
179
9.89e-01
23
14989
1.10e-01
484
16539
1.76e-02
6608
87
9.95e-01

4381
25136
3.71e-01
33
51159
-2.79e-01
384
55640
-3.92e-01
4384
24203
3.95e-01
34
51053
-2.77e-01
419
55585
-3.90e-01
4644
6809
5.89e-01
34
14900
1.01e-01
449
16294
1.65e-02
4931
6691
5.92e-01
33
14743
1.01e-01
409
16145
1.57e-02
4461
6951
5.87e-01
30
15085
1.04e-01
411
16531
1.81e-02
4505
6814
5.96e-01

4323
23712
4.07e-01
32
50885
-2.73e-01
392
55661
-3.92e-01
4311
22165
4.46e-01
33
50717
-2.69e-01
410
55608
-3.91e-01
4127
6827
5.88e-01
28
14719
1.12e-01
368
16274
1.77e-02
4085
6615
5.97e-01
26
14534
1.14e-01
349
16120
1.73e-02
3817
6923
5.89e-01
27
14864
1.17e-01
337
16503
1.97e-02
3819
6671
6.04e-01

23

9844
23942
4.01e-01
102
52832
-3.21e-01
1076
55768
-3.95e-01
9864
22893
4.27e-01
94
52774
-3.20e-01
1040
55719
-3.94e-01
10429
61
7.32e-01
97
15532
6.25e-02
856
16367
1.20e-02
7575
49
7.39e-01
89
15350
6.42e-02
677
16204
1.21e-02
7461
5356
6.82e-01
64
15732
6.55e-02
644
16606
1.36e-02
6538
89
6.94e-01

9736
34972
1.25e-01
129
52984
-3.25e-01
1066
55778
-3.95e-01
9732
32934
1.76e-01
142
52909
-3.23e-01
1130
55725
-3.94e-01
9631
10089
3.93e-01
96
15474
6.59e-02
680
16331
1.42e-02
6637
9725
4.25e-01
95
15296
6.75e-02
742
16165
1.45e-02
7249
10267
3.90e-01
70
15662
6.97e-02
669
16572
1.56e-02
6566
10271
4.04e-01

9322
34383
1.40e-01
132
53120
-3.29e-01
1140
55842
-3.97e-01
9311
34850
1.28e-01
126
53029
-3.26e-01
1083
55789
-3.95e-01
9361
9704
4.14e-01
112
15450
6.74e-02
1058
16329
1.44e-02
10163
9610
4.14e-01
100
15285
6.81e-02
1043
16164
1.46e-02
10230
9717
4.23e-01
65
15482
8.04e-02
642
16527
1.83e-02
6082
10304
3.89e-01

1000

10000

24
15049
1.08e-01
466
16591
1.61e-02
6290

38
15144
1.02e-01
441
16586
1.64e-02
4078

23
14925
1.15e-01
291
16566
1.76e-02
3374

102
15802
6.29e-02
702
16658
1.21e-02
7146

97
15734
6.69e-02
709
16627
1.40e-02
6910

109
15750
6.59e-02
1022
16619
1.44e-02
10328

5.2 Estimating Probability Distributions Arising in Genetics

The following SDP arises in the context of estimating haploid frequencies in a population; see [2]
for a discussion. The optimization problem reads as

max(cid:104)C, X(cid:105)

s.t. tr(X) = 1

X (cid:23) 0, X ≥ 0

(5.2)

This deﬁnes SDP over the double nonnegative cone. To bring this problem into the form of our
+| tr(X) ≤ 1}, g(X) = −(cid:104)C, X(cid:105), and K = {x ∈
convex programming template (P), we set X = {X ∈ Sn
Sn|Xij ≥ 0, i, j = 1, . . . , n} the cone of positive matrices. We take P(X) = X, together with the ν = n(n+1)
logarithmically homogeneous barrier

2

F(X) = −

(cid:88)

i, j

log(Xij),

to arrive at a problem instance where our algorithm can be applied directly.

5.3 The Maximum Stable Set Problem

Let G = (V, E) be a given undirected graph. A set W ⊂ V is an independent set of G if no two vertices
in W are adjacent. The maximum cardinality of an independent set is denoted by α(G), known as
the stability number of G. The maximum independent set problem is to compute α(G). This problem
is known to be NP-hard. A classical semideﬁnite programming upper bound has been derived in
[20], and is based on the following observation. Let S be an independent set in a graph G and let
x ∈ {0, 1}n be its incidence vector. Deﬁne the matrix X = 1
. This matrix satisﬁes the following
) = (cid:104)X1, 1(cid:105) = |S|. It is
conditions: X ∈ Sn
therefore natural to consider the following semideﬁnite program

+, Xij = 0 for all (i, j) ∈ E and tr(X) = 1. Furthermore, tr(X11(cid:62)

|S| xx(cid:62)

(cid:62)

)

max tr(X11
+,
s.t.: tr(X) = 1, X ∈ Sn
Xij = 0 ∀(i, j) ∈ E

(5.3)

+| tr(X) =
Clearly, this is an example of our convex programming template (P), with X = {X ∈ Sn
1, Xij = 0 for (i, j) ∈ E} and P being trivial. A tighter relaxation can be obtained by adding non-
negativity constraints to the entries of the matrix. This leads to the doubly nonnegative relaxation

24

Figure 6: Max-Cut datasets G22-G30 objective function value vs. time.

of the Max-Stable-Set problem [6, 31]:

(cid:62)

tr(X11

max
X
s.t.: tr(X) = 1

)

Xij = 0 ∀(i, j) ∈ E
X (cid:23) 0, X ≥ 0

(5.4)

In this formulation, we have the added entry-wise restriction P(X) = X ≥ 0, which will be absorbed
within the log barrier F(X) = − (cid:80)

(i, j)(cid:60)E log(Xij).

5.4 Finding the Fastest Mixing Markov Chain

We next consider the problem of ﬁnding the fastest mixing rate of a Markov chain on a graph [25].
In this problem, a symmetric Markov chain is deﬁned on an undirected graph G = ({1, . . . , n}, E).
Given G and weights dij for {i, j} ∈ E, we are tasked with ﬁnding the transition rates wij ≥ 0 for each
{i, j} ∈ E with weighted sum smaller than 1, that result in the fastest mixing rate. We assume that
(cid:80)
= n2. The mixing rate is given by the second smallest eigenvalue of the graph’s Laplacian

ij d2
ij

matrix, described as

L(w)ij =






(cid:80)

j:{i, j}∈E wij

−wij
0

j = i
{i, j} ∈ E
otherwise.

.

25

G28G29G30G25G26G27G22G23G241e−011e+011e+031e−011e+011e+031e−011e+011e+030200004000005000100001500005000100001500002000040000020000400000500010000150000200004000002000040000050001000015000Time (sec)ObjectiveAlgorithmCGLCGs0.50.90.99From L1 = 0 it follows that 1
n 1 is a stationary distribution of the process The mixing time is deﬁned
as Tmix := supπ(cid:107)πP(t) − 1
n 1(cid:107)TV, where the supremum is taken with respect to all distrbutions over
ne−λ2t,
the set of nodes, and the norm is the total variation distance. It can be shown that Tmix
where λ2 is the second-largest eigenvalue of the Laplacian L [29]. To bound this eigenvalue, we
follow the strategy laid out in [25]. Thus, the problem can be written as

≤ 1
2

√

max
w

s.t.

λ2(L(w))
(cid:88)

{i,j}∈E
w ≥ 0.

ijwij ≤ 1
d2

This problem can also alternatively be formulated as

(cid:88)

d2
ijwij

min
w

{i,j}∈E
s.t. λ2(L(w)) ≥ 1
w ≥ 0.

Due to the properties of the Laplacian, the ﬁrst constraint can be reformulated as

min
w

(cid:88)

{i,j}∈E

d2
ijwij

s.t. L(w) (cid:23) In×n − 1
n
w ≥ 0.

(cid:62)

11

The dual problem is then given by

(cid:104)In×n − 1
min
n
X∈Sn
+
s.t. Xii + X jj − 2Xij ≤ d2
ij

(cid:62), X(cid:105)

11

{i, j} ∈ E

(5.5)

(5.6)

(5.7)

(5.8)

To obtain an SDP within our convex programming model, we combine arguments from [25] and
[2]. Let X be a feasible point for (5.10). Then, there exists a n × n matrix V such that X = VV(cid:62)
. It is
easy to see that multiplying each row of the matrix V(cid:62) = [v1
, . . . , vn] with an orthonormal matrix
does not change the feasibility of the candidate solution. Moreover Xij = v(cid:62)
i vj for all 1 ≤ i, j ≤ n, so
, . . . , vn
that Xii + Xj j − 2Xij = (cid:107)vi − vj(cid:107)2
(cid:107)vi(cid:107)2. This gives the equivalent
so that
optimization problem

2. Without loss of generality, we can normalize the vectors v1

i=1 vi = 0. This implies (cid:104)In×n − 1

n 11(cid:62), X(cid:105) = tr(X) = (cid:80)n
i=1

(cid:80)n

max
,...,vn
v1

s.t.

n(cid:88)

(cid:107)vi(cid:107)2

i=1
(cid:107)vi − v j(cid:107)2 ≤ d2
ij

vi ∈ Rn,

n(cid:88)

i=1

vi = 0.

{i, j} ∈ E,

26

(5.9)

Dataset
1
2
3
4
5
6
7
8

# Nodes
100
100
200
200
400
400
800
800

# Edges
1000
2000
1000
4000
1000
8000
4000
16000

SDPT3 Value
15.62
7.93
72.32
17.84
388.52
38.37
333.25
80.03

Table 3: Mixing datasets characteristics.

This is the geometric dual derived in [25], which is strongly connected to the geometric embedding
of a graph in the plane [12]. Set X = {X ∈ Sn|X (cid:23) 0, tr(X) ≤ Dn
ij. Then, we
2
can add this trace constraint to the problem formulation, and obtain the equivalent formulation:
given by D(X){i,j} = Xii + Xjj − 2Xij. Let Q = {y ∈ R|E||yi,j ≤ d2
Deﬁne D : Sn → R|E|
, {i, j} ∈ E} and
i,j
K = {(y, t) ∈ R|E| × R| 1
t y ∈ Q, t > 0}. This is a closed convex cone with logarithmically homogeneous
barrier

}, where D = maxij d2

f (y, t) = −

(cid:88)

{i,j}∈E

log(td2
i, j

− yi,j) = −

(cid:88)

e∈E

log(d2
i, j

− 1
t

yi, j) − |E| log(t).

This gives a logarithmically homogeneous barrier F(X, t) = f (P(X, t)), where P(X, t) = [D(X); t] ∈
R|E| × R.

min

X∈Sn

(cid:104)In×n − 1
n

(cid:62), X(cid:105)

11

+,t>0
s.t. P(X, t) ∈ K, t = 1

(5.10)

X ∈ X.

The Mixing problem was run using Matlab R2021b on an Intel(R) Xeon(R) Gold 6354 CPU @
3.00GHz server limited to 4 threads per run and 383G total RAM. We generated random connected
undirected graphs of various sizes, and for each edge {i, j} in the graph we generated a random
ij uniformly in [0, 1]. Table 3 provides the size of each dataset, and the value obtained by solving
d2
the Mixing problem SDP using CVX with SDPT3 solver. All datasets were run for both CG and
LCG options with the following choice of parameters η0 ∈ {0.01Ωg, 0.1Ωg, 0.5Ωg, 1Ωg, 2Ωg} and
σ ∈ {0.99, 0.9, 0.5, 0.25}. Each of the runs was terminated after it reached at least 10000 iterations at
least 3600 seconds running time. Figures 7-8 illustrate the results for some of the parameter values.
Table 4 displays the numerical values obtained from our experiments with the best conﬁguration
of parameters η0 and σ.

In the experiments we observe that the line search version LCG outperforms CG by orders of
magnitude. One explanation of this is that the step size policy employed in CG is based on global
optimization ideas which do not take into account the local structure of the problem. Line search
captures these local features of the problem much better, leading to better numerical performance.

27

6 Complexity Analysis

6.1 Analysis of procedure CG(x0, ε, t)
Recall Ωg := maxx,y∈dom(g)∩X|g(x) − g(y)| and C = dom(F) ∩ X ∩ dom(g). The following estimate can
be established as in [33, Prop. 2.3]. Therefor we omit a proof.

Lemma 6.1. For all (x, t) ∈ C × (0, ∞), we have

−1et(x) ≤ ν/t + Gapt(x) + Ωg.
t

Observe that

(cid:48)(cid:48)
et(x) = (cid:107)st(x) − x(cid:107)x = F

(x)[st(x) − x, st(x) − x]1/2 ≥ ν−1/2|F

(cid:48)

(x)[st(x) − x]|

= t√
ν
≥ t√
ν

|Gapt(x) − g(x) + g(st(x))|

(Gapt(x) − Ωg)

(6.1)

6.1.1 Proof of Proposition 3.2
Suppose that Gapt(x) > ν
√
ν ≥ 1. This in turn implies

t

deﬁned by the threshold at := ν
CG(x0
t
deﬁne KI(t) := {k ∈ N| Gapt(xk

t

+ Ωg. Then, it readily follows from the previous display that et(x) ≥
t Gapt(x)
< 1. This suggests a two-phase analysis
et(x)+t Gapt(x)
}
0≤k≤Kt(x0) be the sequence obtained when running
, ε, t) until reaching a point satisfying the stopping criterion Gapt(xKt(x0)) ≤ ε. To that end,

<
+ Ωg. Let {xk
t

t Gapt(x)
et(x)(et(x)+t Gapt(x))

t ) > at} and KII(t) := {k ∈ N| Gapt(xk

t ) ≤ at}.

We start with the analysis of the trajectory on KI(t). By the previous estimates, we know that on
=
. To reduce notational clutter,

t Gapt(xk
t )
t )+t Gapt(xk
t ))

t )(et(xk

et(xk

this phase the algorithm chooses the step sizes α
we set ek
t ) and Gk
t
t
potential function can be estimated as follows:

≡ Gapt(xk

≡ et(xk

k

t ). In terms of these quantities, the per-iteration reduction of the

Vt(xk+1

t

) ≤ Vt(xk

t ) −

Gk
t
ek
t
Gk
t
ek
t
t ) − 1
t

t ) −

ω

tGk
t
+ tGk
t

log

ek
t
+ 1
t





tGk
t
ek
t





,

= Vt(xk

= Vt(xk

+ 1
t



1 +





tGk
t
+ tGk
t

ek
t





ω∗


tGk
t
ek
t

where ω(t) = t − log(1 + t). This readily yields ∆t(xk

t
is monotonically decreasing. For k ∈ KI(t), we also see
implies

t ) − ∆t(xk+1

ω

) ≥ 1
t
tGk
t
+tΩg
ν+tGk
t

tGk
t
ek
t

≥

Gk
t
ν/t + Gk
t

+ Ωg

=

tGk
t
+ tΩg
ν + tGk
t

.

> 1
2

28

(6.2)

. This implies that {∆t(xk
t )}

k

2 . Together with (6.1), this

(cid:19)

(cid:18) tGk
t
ek
t
> 1

Using (6.1), the monotonicity of ω(·), the fact that the function x (cid:55)→ x
t, a > 0, and [33, Prop. 2.1], we arrive at

a+tx is strictly increasing for

∆t(xk

t ) − ∆t(xk+1

t

) ≥ 1
5.3

Gk
t
+ tΩg
ν + tGk
t

≥

∆t(xk
t )
5.3(ν + tΩg + t∆t(xk

t ))

.

Coupled with ∆t(xk

t ) ≤ ∆t(x0

t ), we conclude

1 −

∆t(xk
t )

1
5.3(ν + tΩg + t∆t(x0

t ))





≥ ∆t(xk+1

t

).

Deﬁne qt :=

1
5.3(ν+tΩg+t∆t(x0

t ))

∈ (0, 1) (since ν ≥ 1), to arrive at the recursion

∆t(xk

t ) ≤ (1 − qt)k∆t(x0

t ).
t ) ≤ exp(−qtk)∆t(x0

Furthermore,
∆t(xk

t ) ≥ ∆t(xk

)

t

t ) − ∆t(xk+1
Gk
t
5.3(ν + tGk
t

+ tΩg)

≥

≥ 1

10.6t

.

Hence,
(cid:100)5.3(ν + t∆t(x0

1
10.6t∆t(x0
t )
t ) + tΩg) log(10.6t∆t(x0

≤ exp(−qtk), so that process CG(x0
t

, ε, t) must exit phase I in at most Kt(x0

t ) :=

t ))(cid:101) iterations.
We now upper bound the time the process spends in phase II, so that Gk
t

+ Ωg. When the
= 1), from
< 1). Both regimes lead to a universal lower bound on the per-iteration

algorithm enters this phase, we have to distinguish the iterates using large step sizes (α
those using short steps (α
k
potential reduction achieved, as we illustrate next.
= 1. Then, tGk
t

t ), which implies ek
t

∈ (0, 1), and

t (tGk
t

≤ ν
t

≥ ek

+ ek

k

k

To start with, let us assume α
≥ (ek
t )2
t(1−ek
t )

Gk
t

. Moreover, using (2.7), we readily obtain

Vt(xk+1

t

) ≤ Vt(xk

t ) − Gk

t

≤ Vt(xk

t ) − Gk

t

+ 1
t
+ 1
t

ω∗(ek
t )
(ek
t )2
2(1 − ek
t )

≤ Vt(xk

t ) −

Gk
t
2

.

Therefore,

∆t(xk

t ) − ∆t(xk+1

t

= 1
2

t(Gk
t )2
ν + tΩg

) ≥

Gk
t
2

≥ 1
12

(Gk
t )2
≥ 1
ν/t + Ωg
2
t(Gk
t )2
(ν + tΩg)2

.

On the other hand, in the regime where α
by following the same reasoning as in [33]. We conclude

k

< 1, we obtain the estimate ∆t(xk

t ) − ∆t(xk+1

t

1
∆t(xk+1
t

)

−

1
∆t(xk
t )

≥

t
12(ν + tΩg)2

∀k ∈ KII(t).

29

) ≥ 1
12

t(Gk
t )2
(ν+tΩg)2 ,

(6.3)

, η, t) an upper bound on the total number of
Given the pair (t, η) ∈ (0, ∞)2, denote by N ≡ N(x0
t
iterations of CG(x0
t ) of these iterations are in phase I. To
t
estimate the remaining iterations in phase II, i.e. M = N − K, we ﬁrst telescope the inequality (6.3),
to get

, η, t). We know that at most K ≡ Kt(x0

1
∆t(xN
t )

≥

+

1
∆t(xK
t )
tη − 1
t∆t(x0
t )
, η, t).

Choosing M = (cid:100)12(ν + tΩg)2( 1
complexity of algorithm CG(x0
t

≥

Mt
12(ν + tΩg)2
+(cid:101) ensures that ∆t(xN
)

1
∆t(x0
t )

+

Mt
12(ν + tΩg)2

.

t ) ≤ η. This bounds the iteration

6.1.2 Proof of Proposition 3.1

Let {kj(t)} j denote the increasing set of indices enumerating KII(t). From the analysis of the trajectory
given in the previous section, we know that

∆t(x

kj+1(t)
t

) − ∆t(x

kj(t)
t

) ≤ − 1
12

k j(t)
)2
t(G
t
(ν + tΩg)2

k j(t)
, as well as G
t

≥ ∆t(x

kj(t)
t

).

Set d j ≡ ∆t(x

kj(t)
t

) and 1
M

≡

t
12(ν+tΩg)2

kj(t)
, Γj ≡ G
t

. We thus obtain the recursion

dj+1

≤ dj −

Γ2
j
M

,

Γj ≥ dj.

We can apply [33, Prop. 2.4] directly to the above recursion to obtain the estimates

dj ≤ M
j

,

and min{Γ0, . . . , Γj} ≤ 2M
j

.

t ) ≤ η, we need to run the process until the label
Therefore, in order to reach an iterate with Gapt(xk
≤ η. Solving this for j yields j = (cid:100) 24(ν+tΩg)2
(cid:101). Combined with the upper
j is reached satisfying 2M
j
bound obtained for the time the process spends in phase I, we obtain the total complexity estimate
postulated in Proposition 3.1.

tη

6.2 Analysis of the outer loop
Let I ≡ I(η0, σ, ε) = (cid:100) log(2η0/ε)
log(1/σ)
homotopy parameter. We set ˆxi ≡ xRi
ti
Gapti( ˆxi) ≤ ηi. From Proposition 3.1, we deduce that
(ti)) ≤ Gapti( ˆxi) + 1
ti

∗
g( ˆxi) − g(z

∗
( ˆxi)[z

(cid:48)
F

(ti) − ˆxi] ≤ ηi +

.

ν

ti

(cid:101) denote the a-priori ﬁxed number of updates of the accuracy and
, ηi, ti) satisfying ∆ti( ˆxi) ≤

, the last iterate of procedure CG(x0
ti

Hence, using Lemma 2.2, we observe

∗
g( ˆxi) − Opt = g( ˆxi) − g(z

(ti)) + g(z

∗

(ti)) − Opt ≤ ηi + 2ν
ti

.

(6.4)

30

Since ti = 2ν
ηi
incurred by our warm-start strategy. Observe that

, we obtain g( ˆxi) − Opt ≤ 2ηi. We next estimate the initial gap ∆ti+1(x0
ti+1

) = ∆ti+1( ˆxi)

Vti+1(x0

i+1) − Vti+1(z

∗

(ti+1)) = Vti(x0

i ) − Vti(z

∗

− 1
ti
(cid:16)

∗

1
ti+1

(ti+1)) + (
(ti)) + (1 − ti+1
ti
)).

(ti+1)) − g(x0
ti+1

)

≤ Vti(xRi
ti
+ (1 − ti+1
ti

) − Vti(z

∗

)(g(z

)(F(x0

ti+1

∗
) − F(z

(ti+1)))

Vti+1(x0
ti+1

) − Vti+1(z

∗

(ti+1))

(cid:17)

Whence,

ti+1
ti

(Vti+1(x0
ti+1

∗
) − Vti+1(z

(ti+1))) ≤ Vti(xRi
ti

) − Vti(z

∗

(ti)) + (1 − ti+1
ti

∗
)(g(z

(ti+1)) − g(x0
ti+1

))

Since ti+1

> ti and the deﬁnition of the stopping time Ri ≡ R(x0
ti

, ηi, ti), this implies

ti+1

∆ti+1(x0
ti+1

) ≤ ti∆Ri
ti

− (ti+1

− ti)Ωg ≤ tiηi + (ti+1

− ti)Ωg = 2ν + (ti+1

− ti)Ωg.

(6.5)

We are now in the position to estimate the total iteration complexity Compl(x0, ε, f ) := (cid:80)I
, ηi, ti),
and thereby prove Theorem 3.3. We do so by using the updating regime of the sequences {ti} and
{ηi} explained in Section 3. These updating mechanisms imply

i=0 R(x0
ti

I(cid:88)

i=1

Ri ≤

I(cid:88)

i=1

5.3(3ν + (ti − ti−1)Ωg + tiΩg) log(10.6(2ν + (ti − ti−1)Ωg)) +

I(cid:88)

i=1

12
ν (ν + tiΩg)2

≤ log(10.6(2ν + tIΩg))

I(cid:88)

i=1

5.3(3ν + tiΩg + (ti − ti−1)Ωg) +

I(cid:88)

24
ν (ν2 + t2

i

Ω2
g)

≤ 15.9ν log(10.6(2ν + tIΩg))I + 5.3Ωg log(10.6(2ν + tIΩg))

ti

+ 5.3Ωg log(10.6(2ν + tIΩg))

I(cid:88)

(ti − ti−1) + 24νI +

(cid:104)
24ν + 15.9ν log(10.6(2ν + tIΩg))

i=1

(cid:105)

≤ I

+ 5.3Ωg log(10.6(2ν + tIΩg))tI

i=1
I(cid:88)

24Ω2
g
ν

i=1
I(cid:88)

i=1

t2
i

+

24Ω2
g
ν

I(cid:88)

i=1

t2
i

+ 5.3Ωg log(10.6(2ν + tIΩg))

I(cid:88)

i=1

ti.

Let us estimate the terms appearing in this expression. First,

tI ≤ t0σ−I ≤ 4ν

ε

.

Second,

I(cid:88)

i=1

ti = t0σ−1

I−1(cid:88)

i=0

(1/σ)i ≤ t0

σ−I
1 − σ

≤

4ν
ε(1 − σ)

.

31

Third,

I(cid:88)

i=1

t2
i

= t2
0

I(cid:88)

(1/σ)2i =

i=1

t2
0
σ2

I−1(cid:88)

i=0

(1/σ2)i ≤

16ν2
ε2(1 − σ2)

.

This delivers the bound

(cid:105)
(cid:104)
24ν + 15.9ν log(21.2ν(1 + (2/ε)Ωg))

+ 5.3Ωg log(21.2ν(1 + (2/ε)Ωg))

4ν
ε

I(cid:88)

i=1

Ri ≤ log(2η0/ε)
log(1/σ)
24Ω2
g
ν

+

16ν2
ε2(1 − σ2)

+ 5.3Ωg log(21.2ν(1 + (2/ε)Ωg))

=

ν log(2η0/ε)
log(1/σ)

(cid:104)
24 + 15.9 log(21.2ν(1 + (2/ε)Ωg))

(cid:105)

+ Ωg log(21.2ν(1 + (2/ε)Ωg))

21.2ν
ε

2 − σ
1 − σ

.

It remains to bound the complexity at i = 0. We have

4ν
ε(1 − σ)
384Ω2
g
ε2(1 − σ2)

ν

+

R0 ≤ 5.3(ν + t0∆t0(x0
t0

) + t0Ωg) log(10.6t0∆t0(x0)) + 12

ν (ν + t0Ωg)2

≤ 5.3(ν + F(x0) − F(z

∗

(t0)) + 2t0Ωg) log

(cid:16)

∗
10.6(F(x0) − F(z

(cid:17)
(t0)) + t0Ωg)

+ 24

ν (ν2 + t2

0

g).
Ω2

Since t0 = ν
Ωg

and η0 = 2Ωg, we have

R0 ≤ 5.3(3ν + F(x0) − F(z

∗

(t0))) log

(cid:16)

10.6(ν + F(x0) − F(z

∗

(cid:17)
(t0)))

+ 48ν.

Adding the two gives the total complexity bound

Compl(x0, ε, f ) ≤ 5.3(3ν + F(x0) − F(z

∗

(t0))) log

(cid:16)

10.6(ν + F(x0) − F(z

∗

(cid:17)
(t0)))

+ 48ν

+

+

ν log(2η0/ε)
log(1/σ)
ν
384Ω2
g
ε2(1 − σ2)
384Ω2

g
ε2(1 − σ2)

ν



= ˜O

(cid:105)
(cid:104)
24 + 15.9 log(21.2ν(1 + (2/ε)Ωg))

+ Ωg log(21.2ν(1 + (2/ε)Ωg))

21.2ν
ε

2 − σ
1 − σ

+ Ωg log(21.2ν(1 + (2/ε)Ωg))

21.2ν
ε

2 − σ
1 − σ





.

6.3 Modiﬁcations Due to Inexact Oracles

In this section we outline the changes that need to be made in evaluating the iteration complexity
under inexact oracle information. Consider algorithm ICG(x0, η, t) with accuracy parameter γ ∈
[0, 1). Fix t > 0 and deﬁne ˜ek ≡ et(xk) as well as ˜Gk ≡ (cid:103)Gapt(xk
t ). The following modiﬁcations have
to be made to replicate the results from the exact to the inexact oracle model.

1. Lemma 6.1 is replaced by

−1 ˜et(x) ≤ ν/t + (cid:103)Gapt(x) + Ωg.
t

32

2. We can show that the sequence of function gaps {∆t(xk
t )}

the sequence {xk
t

}

k generated by ICG(x0, η, t).

k is monotonically decreasing under

3. The phases of the CndG algorithm are characterized by KI(t) := {k ∈ N| (cid:103)Gapt(xk

t ) ≥ at}, and

KII(t) := {k ∈ N| (cid:103)Gapt(xk

t ) ≤ at}.

4. In phase I, the linear convergence is contaminated by a perturbed linear convergence rate in

terms of the potential function gap:

∆t(xk

t ) ≤ (1 − ˜qt)k∆t(x0

t ) + γη,

where ˜qt :=
becomes

1
5.3(ν+t(Ωg−γη)+∆t(x0

t ))

. Hence, the bound on the time the process spends in phase I

Kt,γ(x0

t ) :=





0
(cid:100)5.3(ν + t(Ωg − γη) + t∆t(x0

t ))(cid:101) log

(cid:18) 10.6t∆t(x0
t )
1−10.6tγη

(cid:19)

if γη ≥ 1
10.6t
.
if γη < 1
10.6t

5. We continue with the analysis on phase II. Following exactly the same arguments used in the

model with exact oracles we arrive at the lower bound

∆t(xk

t ) − ∆t(xk+1

t

) ≥ 1
12

t( ˜Gk)2
(ν + tΩg)2

.

This shows that even under inexact oracle information, the sequence {∆t(xk
t )}
decreasing. If ∆t(xk+1
t
Setting Gk ≡ Gapt(xk
follows:

) < η, then we are done. Hence, let us assume that ∆t(xk+1
t ), and using the relation ˜Gk ≥ Gapt(xk

k is monotonically
) ≥ η > γη.
t ) − γη for all k, we can proceed as

t

∆t(xk

t ) − ∆t(xk+1

t

) ≥ 1
12

≥ t
12

Hence,

≥ 1
12

t( ˜Gk)2
(ν + tΩg)2
(∆t(xk

t ) − γη)2
(ν + tΩg)2

= t
12

t(Gk − γη)2
(ν + tΩg)2
(∆t(xk

t ) − γη)(∆t(xk+1
t
(ν + tΩg)2

) − γη)

.

(∆t(xk

t ) − γη) − (∆t(xk+1

t

) − γη) ≥ t
12

(∆t(xk

t ) − γη)(∆t(xk+1
t
(ν + tΩg)2

) − γη)

.

Multiplying both sides by

(∆t(xk

1
t )−γη)(∆t(xk+1

t

gives

)−γη)

1
∆t(xk+1
) − γη
t

−

1
t ) − γη
∆t(xk

≥

t
12(ν + tΩg)2

.

Telescoping this expression just as in the model with exact oracle feedback, we arrive at the
iteration bound for phase II as (cid:100)12(ν + tΩg)2( 1
tη −

+(cid:101). This proves Proposition 4.1.
)

1
t )−γη)
t(∆t(x0

6. To prove Proposition 4.2, we argue as follows: Let {k j(t)} j denote the increasing set of indices

enumerating KII(t). From the analysis of the trajectory above, we know that

k j+1(t)
∆t(x
t

) − ∆t(x

kj(t)
t

) ≤ − 1
12

t( ˜Gkj(t))2
(ν + tΩg)2

≤ − 1
12

k j(t)
t(Gapt(x
t
(ν + tΩg)2

) − γη)2

.

33

kj(t)
t

Set dj ≡ ∆t(x
) − γη and 1
M
J is ﬁnite, since {∆t(xk
t )}
Γj ≥ dj. For j = 1, . . . , J − 1, it thus holds true that

, Γ j ≡ ˜Gkj(t). Let J = sup{ j|d j ≥ 0} ∈ N. We know that
k is monotonically decreasing. For all j ∈ {1, . . . , J} we have d j ≥ 0 and

t
12(ν+tΩg)2

≡

d j+1

≤ d j −

Γ2
j
M

,

Γ j ≥ d j.

By [33, Prop. 2.4], we have for j = 1, . . . , J,

d j ≤ M
j

,

and min{Γ0, . . . , Γj} ≤ 2M
j

.

This shows J = J(η, γ) := (cid:98) 12(ν+tΩg)2
tγη
to run the process until the label j is reached satisfying 2M
j
to observe

(cid:99). In order to reach an iterate with Gapt(xk

t ) ≤ η, we need
≤ (1 − γ)η. To see this, it suﬃces

min{Gapt(xk0(t)

t

), . . . , Gapt(xk0(t)

t

)} − γη ≤ min{ ˜Gk0(t), . . . , ˜Gkj(t)} ≤ 2M
j

≤ (1 − γ)η

⇒ min{Gapt(xk0(t)

t

), . . . , Gapt(xk0(t)

t

)} ≤ η.

Solving this for j yields j = (cid:100) 24(ν+tΩg)2
t(1−γ)η (cid:101). Since γ < 1/3, we see that this bound is smaller
than J(η, γ). Hence, once combined with the upper bound obtained for the time the process
spends in phase I, we obtain the total complexity estimate postulated in Proposition 4.2.

7 Conclusion

Solving large scale conic-constrained convex programming problems is still a key challenge in
mathematical optimization and machine learning. In particular, SDPs with a massive amount of
linear constraints appear naturally as convex relaxations of combinatorial optimization problems.
For such problems, the development of scalable algorithms is a very active ﬁeld of research [10, 32].
In this paper, we introduce a new path-following/homotopy strategy to solve such massive conic-
constrained problems, building on recent advances in projection-free methods for self-concordant
minimization [7, 9, 33]. Our scheme is competitive with state-of-the-art solvers in terms of its
theoretical iteration complexity and gives promising results in practice. Future work will focus on
improvements of our deployed CndG solver to accelerate the subroutines.

References

[1] Y. Ye, GSet random graphs, https://www.cise.u.edu/research/sparse/matrices/Gset/ (accessed May, 2022).
[2] S. Arora, E. Hazan, and S. Kale. Fast algorithms for approximate semideﬁnite programming using the multiplicative
weights update method. In 46th Annual IEEE Symposium on Foundations of Computer Science (FOCS’05), pages 339–
348, 2005. doi: 10.1109/SFCS.2005.35.

[3] Afonso S. Bandeira, Nicolas Boumal, and Vladislav Voroninski. On the low-rank approach for semideﬁnite
programs arising in synchronization and community detection.
In Vitaly Feldman, Alexander Rakhlin, and
Ohad Shamir, editors, 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learn-
ing Research, pages 361–382, Columbia University, New York, New York, USA, 23–26 Jun 2016. PMLR. URL
https://proceedings.mlr.press/v49/bandeira16.html.

34

[4] M. Charikar and A. Wirth. Maximizing quadratic programs: extending grothendieck’s inequality. In 45th Annual

IEEE Symposium on Foundations of Computer Science, pages 54–60, 2004. doi: 10.1109/FOCS.2004.39.

[5] Alexandre d’Aspremont, Francis Bach, and Laurent El Ghaoui. Optimal solutions for sparse principal component

analysis. Journal of Machine Learning Research, 9(7), 2008.

[6] E. de Klerk and D. V. Pasechnik. Approximation of the stability number of a graph via copositive programming.
SIAM Journal on Optimization, 12(4):875–892, 2002. doi: 10.1137/S1052623401383248. URL https://doi.org/10.
1137/S1052623401383248.

[7] Pavel Dvurechensky, Petr Ostroukhov, Kamil Saﬁn, Shimrit Shtern, and Mathias Staudigl. Self-concordant analysis
of Frank-Wolfe algorithms. In Hal Daume III and Aarti Singh, editors, Proceedings of the 37th International Conference
on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 2814–2824, Virtual, 13–18 Jul
2020. PMLR. URL http://proceedings.mlr.press/v119/dvurechensky20a.html. arXiv:2002.04320.

[8] Pavel Dvurechensky, Shimrit Shtern, and Mathias Staudigl. First-order methods for convex optimization. EURO
Journal on Computational Optimization, 9:100015, 2021. ISSN 2192-4406. doi: https://doi.org/10.1016/j.ejco.2021.100015.
URL https://www.sciencedirect.com/science/article/pii/S2192440621001428. arXiv:2101.00935.

[9] Pavel Dvurechensky, Kamil Saﬁn, Shimrit Shtern, and Mathias Staudigl. Generalized self-concordant analysis of
frank–wolfe algorithms. Mathematical Programming, 2022. doi: 10.1007/s10107-022-01771-1. URL https://doi.
org/10.1007/s10107-022-01771-1.

[10] Khaled Elbassioni, Kazuhisa Makino, and Waleed Najy. Finding sparse solutions for packing and covering
semideﬁnite programs. SIAM Journal on Optimization, 32(2):321–353, 2022. doi: 10.1137/20M137570X. URL
https://doi.org/10.1137/20M137570X.

[11] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval Research Logistics Quarterly,
3(1-2):95–110, 2019/09/05 1956. doi: 10.1002/nav.3800030109. URL https://doi.org/10.1002/nav.3800030109.
[12] Frank Göring, Christoph Helmberg, and Markus Wappler. Embedded in the shadow of the separator. SIAM Journal
on Optimization, 19(1):472–501, 2008. doi: 10.1137/050639430. URL https://doi.org/10.1137/050639430.

[13] Elaine T. Hale, Wotao Yin, and Yin Zhang. Fixed-point continuation for (cid:96)

1-minimization: Methodology and
convergence. SIAM Journal on Optimization, 19(3):1107–1130, 2008. doi: 10.1137/070698920. URL https://doi.
org/10.1137/070698920.

[14] Zaid Harchaoui, Anatoli Juditsky, and Arkadi Nemirovski. Conditional gradient algorithms for norm-regularized
smooth convex optimization. Mathematical Programming, 152(1):75–112, 2015. doi: 10.1007/s10107-014-0778-9. URL
https://doi.org/10.1007/s10107-014-0778-9.

[15] Garud Iyengar, David J Phillips, and Cliﬀ Stein. Feasible and accurate algorithms for covering semideﬁnite

programs. Scandinavian Workshop on Algorithm Theory, pages 150–162, 2010.

[16] Martin Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In International Conference on

Machine Learning, pages 427–435, 2013.

[17] Guanghui Lan. First-order and stochastic optimization methods for machine learning. Springer, 2020.

[18] Gert RG Lanckriet, Nello Cristianini, Peter Bartlett, Laurent El Ghaoui, and Michael I Jordan. Learning the kernel

matrix with semideﬁnite programming. Journal of Machine learning research, 5(Jan):27–72, 2004.

[19] Monique Laurent and Franz Rendl. Semideﬁnite programming and integer programming. Handbooks in Operations

Research and Management Science, 12:393–514, 2005.

[20] L. Lovasz. On the shannon capacity of a graph.

10.1109/TIT.1979.1055985.

IEEE Transactions on Information Theory, 25(1):1–7, 1979. doi:

[21] Yurii Nesterov. Lectures on Convex Optimization, volume 137 of Springer Optimization and Its Applications. Springer

International Publishing, 2018.

[22] Yurii Nesterov and Arkadi Nemirovski. Interior Point Polynomial methods in Convex programming. SIAM Publications,

1994.

[23] Jiming Peng and Yu Wei. Approximating k-means-type clustering via semideﬁnite programming. SIAM Journal on

Optimization, 18(1):186–205, 2007. doi: 10.1137/050641983. URL https://doi.org/10.1137/050641983.

[24] Benjamin Recht, Maryam Fazel, and Pablo A. Parrilo. Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization. SIAM Review, 52(3):471–501, 2010. doi: 10.1137/070697835. URL
https://doi.org/10.1137/070697835.

[25] Jun Sun, Stephen Boyd, Lin Xiao, and Persi Diaconis. The fastest mixing markov process on a graph and a
connection to a maximum variance unfolding problem. SIAM Review, 48(4):681–699, 2022/01/12/ 2006. URL
http://www.jstor.org/stable/20453871.

35

[26] Ryan J. Tibshirani and Jonathan Taylor. The solution path of the generalized lasso. Ann. Statist., 39(3):1335–1371,

2011. doi: 10.1214/11-AOS878. URL https://projecteuclid.org:443/euclid.aos/1304514656.

[27] Kilian Q Weinberger and Lawrence K Saul. Distance metric learning for large margin nearest neighbor classiﬁcation.

Journal of machine learning research, 10(2), 2009.

[28] Zaiwen Wen, Wotao Yin, Donald Goldfarb, and Yin Zhang. A fast algorithm for sparse reconstruction based on
shrinkage, subspace optimization, and continuation. SIAM Journal on Scientiﬁc Computing, 32(4):1832–1857, 2010.
doi: 10.1137/090747695. URL https://doi.org/10.1137/090747695.

[29] EL Wilmer, David A Levin, and Yuval Peres. Markov chains and mixing times. American Mathematical Soc.,

Providence, 2009.

[30] Lin Xiao and Tong Zhang. A proximal-gradient homotopy method for the sparse least-squares problem. SIAM Jour-

nal on Optimization, 23(2):1062–1091, 2013. doi: 10.1137/120869997. URL https://doi.org/10.1137/120869997.

[31] A. Yoshise and Y. Matsukawa. On optimization over the doubly nonnegative cone. In 2010 IEEE International
Symposium on Computer-Aided Control System Design, pages 13–18, 2010. ISBN 2165-302X. doi: 10.1109/CACSD.
2010.5612811.

[32] Alp Yurtsever, Joel A. Tropp, Olivier Fercoq, Madeleine Udell, and Volkan Cevher. Scalable semideﬁnite pro-
gramming. SIAM Journal on Mathematics of Data Science, 3(1):171–200, 2021. doi: 10.1137/19M1305045. URL
https://doi.org/10.1137/19M1305045.

[33] Renbo Zhao and Robert M. Freund. Analysis of the Frank–Wolfe method for convex composite optimization in-
volving a logarithmically-homogeneous barrier. Mathematical Programming, 2022. doi: 10.1007/s10107-022-01820-9.
URL https://doi.org/10.1007/s10107-022-01820-9.

36

Dataset
1

Iter.
1000

5000

Alg.
CG
LCG
CG
LCG

2

3

4

5

6

7

8

10000 CG

LCG

50000 CG

1000

5000

LCG
CG
LCG
CG
LCG

10000 CG

LCG

50000 CG

1000

5000

LCG
CG
LCG
CG
LCG

10000 CG

1000

5000

LCG
CG
LCG
CG
LCG

10000 CG

1000

5000

LCG
CG
LCG
CG
LCG

10000 CG

1000

5000

LCG
CG
LCG
CG
LCG

10000 CG

1000

5000

LCG
CG
LCG
CG
LCG

10000 CG

1000

5000

LCG
CG
LCG
CG
LCG

10000 CG

LCG

η0 (x · Ωg)
0.01
2
1
2
2
2
2
2
0.01
2
1
2
1
2
2
2
0.01
2
0.01
2
0.01
2
0.01
2
0.01
2
0.01
2
0.01
2
0.01
2
0.01
2
0.01
1
0.01
1
0.5
1
0.01
2
0.01
2
0.01
2
0.01
1
0.01
2
0.01
2

σ Obj. Value
14.18
14.67
14.34
14.99
14.36
15.12
14.56
15.36
7.24
7.40
7.27
7.54
7.27
7.60
7.33
7.75
8.78
67.95
67.31
69.68
67.40
70.34
9.70
16.37
15.98
16.82
15.99
17.02
130.86
362.16
356.87
373.61
367.95
377.77
19.71
35.29
34.91
36.17
35.03
36.57
89.68
311.85
263.38
320.39
312.09
323.44
10.91
72.62
49.59
75.02
71.97
76.04

0.99
0.99
0.25
0.99
0.9
0.99
0.99
0.99
0.99
0.99
0.25
0.99
0.25
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.25
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99

37

Gap
9.23e-02
6.07e-02
8.23e-02
4.08e-02
8.06e-02
3.22e-02
6.80e-02
1.68e-02
8.65e-02
6.65e-02
8.33e-02
4.89e-02
8.26e-02
4.12e-02
7.47e-02
2.29e-02
8.79e-01
6.04e-02
6.93e-02
3.65e-02
6.80e-02
2.74e-02
4.56e-01
8.26e-02
1.04e-01
5.72e-02
1.04e-01
4.59e-02
6.63e-01
6.78e-02
8.15e-02
3.84e-02
5.29e-02
2.77e-02
4.86e-01
8.03e-02
9.01e-02
5.72e-02
8.69e-02
4.67e-02
7.31e-01
6.42e-02
2.10e-01
3.86e-02
6.35e-02
2.95e-02
8.64e-01
9.25e-02
3.80e-01
6.26e-02
1.01e-01
4.98e-02

Time
57.33
61.07
299.84
315.06
590.61
630.42
2996.00
3172.51
59.50
47.08
297.11
264.89
589.45
561.60
2943.62
3037.44
125.78
91.18
717.50
485.01
1512.97
1020.81
170.83
84.17
822.57
454.63
1588.48
940.77
229.34
160.07
1020.93
819.19
2216.81
1668.29
211.59
172.43
1143.19
942.42
2469.83
1904.46
669.59
441.59
3589.64
2231.72
6995.37
4661.37
667.97
427.37
3859.94
2262.00
7479.36
4792.46

Table 4: Mixing datasets results. For each dataset, number of iterations, and algorithm provides the

best parameter choice for η0 and σ, the function value obtained, the relative gap from the SDPT3

solution, and the time.

Figure 7: Mixing datasets relative gap from SDPT3 solution vs.
choices.

iteration for various parameter

38

h0=0.01Wgh0=0.5Wgh0=2WgDataset 1Dataset 2Dataset 3Dataset 4Dataset 5Dataset 6Dataset 7Dataset 81e+011e+031e+051e+011e+031e+051e+011e+031e+050.11.00.11.00.11.00.11.00.11.00.11.00.11.00.11.0IterationOptimality GapAlgorithmCGLCGs0.250.50.99Figure 8: Mixing datasets relative gap from SDPT3 solution vs. time for various parameter choices.

39

h0=0.01Wgh0=0.5Wgh0=2WgDataset 1Dataset 2Dataset 3Dataset 4Dataset 5Dataset 6Dataset 7Dataset 81100100001100100001100100000.11.00.11.00.11.00.11.00.11.00.11.00.11.00.11.0Time (sec)Optimality GapAlgorithmCGLCGs0.250.50.99