Aspect-Based API Review Classiﬁcation: How Far
Can Pre-Trained Transformer Model Go?

Chengran Yang1, Bowen Xu1∗, Junaed Younus Khan2, Gias Uddin2, Donggyun Han1, Zhou Yang1, David Lo1
1 School of Computing and Information System, Singapore Management University
2 Department of Electrical and Computer Engineering, University of Calgary, Alberta, Canada
{cryang, bowenxu.2017, dhan, zyang, davidlo}@smu.edu.sg, {junaedyounus.khan, gias.uddin}@ucalgary.ca

2
2
0
2

n
a
J

7
2

]
E
S
.
s
c
[

1
v
7
2
3
1
1
.
1
0
2
2
:
v
i
X
r
a

Abstract—APIs (Application Programming Interfaces) are
reusable software libraries and are building blocks for modern
rapid software development. Previous research shows that pro-
grammers frequently share and search for reviews of APIs on
the mainstream software question and answer (Q&A) platforms
like Stack Overﬂow, which motivates researchers to design tasks
and approaches related to process API reviews automatically.
Among these tasks, classifying API reviews into different aspects
(e.g., performance or security), which is called the aspect-based
API review classiﬁcation, is of great importance. The current
state-of-the-art (SOTA) solution to this task is based on the
traditional machine learning algorithm. Inspired by the great
success achieved by pre-trained models on many software engi-
neering tasks, this study ﬁne-tunes six pre-trained models for the
aspect-based API review classiﬁcation task and compares them
with the current SOTA solution on an API review benchmark
collected by Uddin et al. The investigated models include four
models (BERT, RoBERTa, ALBERT and XLNet) that are pre-
trained on natural languages, BERTOverﬂow that is pre-trained
on text corpus extracted from posts on Stack Overﬂow, and
CosSensBERT that is designed for handling imbalanced data.
The results show that all the six ﬁne-tuned models outperform
the traditional machine learning-based tool. More speciﬁcally, the
improvement on the F1-score ranges from 21.0% to 30.2%. We
also ﬁnd that BERTOverﬂow, a model pre-trained on the corpus
from Stack Overﬂow, does not show better performance than
BERT. The result also suggests that CosSensBERT also does not
exhibit better performance than BERT in terms of F1, but it is
still worthy of being considered as it achieves better performance
on MCC and AUC.

Index Terms—software mining, natural language processing,

multi-label classiﬁcation, pre-trained models

I. INTRODUCTION

Application programming interfaces (APIs) allow data and
functionality to be easily shared across different projects. APIs
have signiﬁcantly facilitated the modern software development
process. For example, the Java Software Development Kit
comes with thousands of components that developers can
conveniently reuse by calling APIs, which reduces both de-
velopment time and effort [1]. To ﬁnd the API information
that is relevant to their requirements, developers usually use
mainstream software question and answer (Q&A) platforms
like Stack Overﬂow1, where developers share their reviews on
APIs. Such reviews can involve multiple aspects of APIs (e.g.,
usability and documentation), and understanding the aspects

∗ Corresponding author.
1https://stackoverﬂow.com

that an API review is about can further aid developers in
ﬁnding more relevant API information.

Stack Overﬂow is a popular software question and answer
platform with extensive discussion on APIs as well as a rich
amount of API reviews [2]. It becomes a valuable resource
and attracts many researchers to further leverage the API
information for developers [1], [3]. Previous research found
that API documentation often suffers from quality problems,
i.e., incomplete, obsolete, and/or incorrect API documenta-
tion [4]. Thus, many developers seek help and insights about
API from other developers in online developer forums (i.e.,
Stack Overﬂow) [2]. Uddin et al. [2] surveyed 178 software
developers to understand their requirements when searching
for API information on Q&A platforms. The results show that
although there are plenty of reviews on the same APIs, devel-
opers usually value more the API reviews that discuss certain
aspects (e.g., is the feature offered by this API scalable?). Such
facts motivate researchers to design automated approaches that
can accurately classify the aspects of API reviews [5]–[8].

The task of aspect-based API review classiﬁcation is origi-
nally introduced by Uddin et al. in [5]. The aim of the task is
to assign API reviews with pre-deﬁned aspects (e.g., Usability
and Security). To tackle the task, many approaches have been
proposed [6]–[8]. Uddin et al. propose a machine learning-
based approach [6] to assign Stack Overﬂow sentences with
pre-deﬁned aspect labels. Lin et al. propose a pattern-based
approach to classify API opinions from Q&A platforms into
aspects [7]. Furthermore, Uddin et al. propose a state-of-the-art
machine learning-based aspect-based API review classiﬁcation
Opiner [8].

In recent years, pre-trained transformer-based models have
achieved exceptional performance in many tasks and areas
including the software engineering domain [9]–[12]. For ex-
ample, Zhang et al. conduct an empirical study on benchmark-
ing four pre-trained transformer-based models (BERT [13],
RoBERTa [14], ALBERT [15], and XLNet [16]) for sentiment
analysis on six software repositories (e.g., code reviews) [9].
They ﬁnd that the pre-trained transformer-based models out-
perform existing sentiment analysis tools by a big margin. In
the rest of the paper, we refer to the pre-trained transformer-
based models collectively as PTMs.

In this work, we focus on the task of aspect-based API
review classiﬁcation as well as the corresponding state-of-the-
art approach Opiner [8]. Speciﬁcally, we would like to inves-

 
 
 
 
 
 
tigate the effectiveness of PTMs on this task. This task has
the following characteristics: ﬁrst, the corresponding state-of-
the-art (SOTA) approach is still based on traditional machine
learning approaches; second, the dataset released in [8] is
based on Stack Overﬂow, which can be considered as SE-
speciﬁc data, different from the data that
is used to pre-
train many of the transformer-based models; third, the data is
imbalanced as the numbers of instances for different aspects of
API review vary. Considering the above, we extend the SOTA
work on aspect-based API review classiﬁcation [8] from the
following three perspectives:

1) Standard PTMs vs. SOTA Approach We compare the
performance of the SOTA approach against four well-
known PTMs (i.e., BERT, RoBERTa, ALBERT, XLNet)
for the task. By comparing their performance, we inves-
tigate whether PTMs outperform the SOTA approach and
by how much margin.

2) Standard BERT vs. Domain-speciﬁc BERT Previous
works [17], [18] show that PTMs pre-trained on domain-
speciﬁc data are expected to outperform a standard PTM
that is trained on general data. On the other hand, standard
PTMs are trained on the huge amount of data which is
usually signiﬁcantly larger than domain-speciﬁc data. In
this study, we investigate the effectiveness of a domain-
speciﬁc variant of BERT named BERTOverﬂow, which
is a BERT-based PTM trained on the data from Stack
Overﬂow [18].

3) Standard BERT vs. BERT for imbalanced data Simi-
larly, we apply a variant of BERT named CostSensBERT
which is designed to handle imbalanced data [19]. To the
best of our knowledge, our work is the ﬁrst to investigate
whether CostSensBERT can contribute to the downstream
task in the software engineering domain.

Overall, we replicate the SOTA approach and adapt six

different PTMs to answer the above questions, respectively.

The contributions of this work are as follows:

1) We adapt four well-known PTMs for aspect-based API
review classiﬁcation task. Our experiment results show
that all the PTMs outperform the SOTA approach by a
large margin (ranges from 28.9% to 30.2%) in terms of
F1. In addition, XLNet, RoBERTa, and BERT outperform
Opiner in terms of MCC (ranges from 8.8% to 27.0%)
and AUC (ranges from 1.5% to 6.9%). Exceptionally,
ALBERT performs better than Opiner in terms of MCC
(by 0.8%) but worse in terms of AUC (by 4.1%). Overall,
our experimental results suggest that the PTMs should be
considered as baselines for the following works.

2) By comparing the performance of the standard BERT
with BERTOverﬂow, we ﬁnd that the standard BERT
slightly outperforms BERTOverﬂow in terms of F1 (by
3.9%). However, BERTOverﬂow performs worth than
BERT in terms of MCC and AUC by 77.9% and 28.8%,
respectively. We suggest that BERT may be better to
be used for many Stack Overﬂow related tasks instead
of BERTOverﬂow. The original BERTOverﬂow paper

has only investigated its effectiveness on the SE-speciﬁc
name entity recognition task. This being said, BERTOver-
ﬂow still outperforms Opiner by a large margin (by
25.3%) in terms of F1. However, BERTOverﬂow perform
worse than Opiner in terms of MCC and AUC by 72.0%
and 23.8%, respectively.

3) By comparing BERT and CostSensBERT, we ﬁnd that
BERT performs better than CostSensBERT in terms of
F1 (by 7.5%) but worse on MCC and AUC (by 8.9%
and 3.7%, respectively). Considering that CostSensBERT
achieves better performance in terms of MCC and AUC
than BERT, it indicates that CostSensBERT may still
be worthy of being considered as baselines in future
works. Also, CostSensBERT outperforms Opiner on F1,
MCC, and AUC by a large margin (21.0%, 38.3%, 10.9%,
respectively).

II. BACKGROUND

In this section, we introduce the problem formulation of
aspect-based API review classiﬁcation. In addition, we de-
scribe the state-of-the-art approach Opiner. Finally, we intro-
duce the four popular PTMs (BERT, RoBERTa, XLNet, and
ALBERT) and two variants (BERTOverﬂow and CostSens-
BERT).

A. Problem Formulation

In [5], Uddin et al. surveyed 178 software developers and
summarized 11 API review aspects developers prefer to see in
the Stack Overﬂow. The deﬁnition of each aspect is shown
in Table I as well as the corresponding example instances
extracted from Stack Overﬂow.

Given API reviews in the form of sentences, the goal of
aspect-based API review classiﬁcation is to assign each sen-
tence with one or multiple pre-deﬁned aspects. By following
the SOTA approach Opiner [8], this task is formulated as
multiple binary classiﬁcation problems while each problem
corresponds to predicting if a sentence (in an API review)
refers to a particular aspect (e.g., Performance, Usability,
Legal). In other words, each aspect corresponds to a binary
classiﬁcation problem. The input is a given API review while
the output is the prediction if the review is related to the aspect.

B. Opiner

Opiner is a tool developed by Uddin et al. which aims
to leverage the API-related information from Stack Overﬂow
for helping developers [5], [8]. One of the key functionalities
of Opiner is API review classiﬁcation. For each API review
aspect, Opiner performs two main steps: feature extraction and
classiﬁcation. In the feature extraction step, Opiner tokenizes
and vectorizes the data instances (i.e., sentences) into n-grams.
In particular, Opiner uses n = 1,2,3 for n-grams, i.e., unigrams
(one single word) to trigrams (a sequence of three words).
Then, Opiner normalizes the n-grams by applying a standard
TF-IDF algorithm that converts the collection of instances to
the set of matrix of TF-IDF features. In the classiﬁcation
step, two traditional machine learning models are selected

Aspect

Deﬁnition

Examples

TABLE I
THE DEFINITION OF API REVIEW ASPECTS

Performance

Usability

Security

Bug

Community

Compatibility

Documentation

Legal

Portability

OnlySentiment

Others

How the software performs in terms of speed or other
performance issues?
Is the software easy to use? How well is the software
designed to meet speciﬁc development requirements?
Does the usage of the software pose any security
threat?
The opinion is about a bug related to the software.
How supportive/active the community
(e.g., mailing list) related to the software?
Whether the usage of the software require of another
software or the underlying development/deployment
environment.
Documentation about the software is available and is
of good/bad quality.
The usage of the software does/does not require any
legal considerations.
The opinion about the usage of the software across
different platforms.
Opinions about the software without specifying any
particular aspect/feature of the software.
The opinion about the aspect cannot be labelled using
any of the above categories.

The object conversion in GSON is fast

GSON is easy to use

The network communication using the HTTPClient API is
not secure
GSON crashed when converting large JSON objects

The GSON mailing list is active

Spring uses Jackson to provide JSON parsing

GSON has good documentation

GSON has an open-source license

GSON can be used in windows, linux and mobile

I like GSON

What is the difference between swing and awt

as candidate classiﬁers: SVM and Logistic Regression. To
evaluate Opiner, Uddin et al. apply 10-fold cross-validation
and pick the best performing classiﬁer in every aspect.

C. Pre-trained Transformer-based Models

• BERT (Bidirectional Encoder Representations

from
Transformers) is a language representation model which
proposed by Google [13]. BERT is conceptually simple
and empirically powerful [9]. Its key technical novelty
is applying the bidirectional training of Transformer, a
popular attention model for language modeling. BERT is
pre-trained by two unsupervised tasks, Masked Language
Modeling (MLM) and Next Sentence Prediction (NSP).
In MLM, some words from input sentences are randomly
masked and BERT attempts to predict the masked words,
based on the context provided by the other, non-masked,
words in the sequence. In NSP, BERT predicts if one
sentence follows another. BERT has achieved outstand-
ing performance for many software tasks(e.g., sentiment
classiﬁcation for software artifacts [9]).

• RoBERTa (Robustly Optimized BERT Approach) is a
modiﬁed version of BERT on the training procedure [14].
The modiﬁcation includes: (1) training the model longer
time with bigger batches, over more data; (2) removing
the NSP task; (3) training on longer sequences; and (4)
dynamically changing the masking pattern applied to the
training data in the MLM task. Yang et al. [20] ﬁne-tuned
RoBERTa to classify the programming language type of
the source code.

• ALBERT (A Lite BERT) is a lite version of BERT
which scales much better compare to the original
BERT [15]. The technical design is motivated when

model increases become harder due to GPU/TPU memory
limitations and longer training times. To address these
problems, ALBERT utilizes two parameter reduction
techniques, factorized embedding parameterization and
cross-layer parameter sharing,
to lower memory con-
sumption and increase the training speed of BERT.

• XLNet is a pre-trained model that uses a permutation
language modeling objective to leverage the strengths
of autoregressive (AR) and autoencoding (AE) methods
while avoiding their limitations [16]. XLNet is capable
to learning bidirectional contexts by maximizing the
expected likelihood over all permutations of the factoriza-
tion order. Aside from using permutation language model-
ing, XLNet utilizes Transformer XL [21], which improves
its performance further. The key technical novelties of
Transformer XL are segment recurrence mechanism and
relative encoding scheme.

• BERTOverﬂow is a domain-speciﬁc version of BERT
which is trained based on 152 million sentences from
Stack Overﬂow [18]. It is originally applied for the task
of software-speciﬁc name entity recognition based on
Stack Overﬂow data. Similarly, we also focus on Stack
Overﬂow data in this study. Thus, BERTOverﬂow could
potentially perform well for our target task, i.e., aspect-
based API review classiﬁcation.

• CostSensBERT is also a modiﬁed version of BERT
which is designed to handle imbalanced data by incor-
porating cost-sensitivity [19]. The main idea of Cost-
SensBERT is that increasing the cost of predicting the
classiﬁcation of an “important” class wrong and corre-
sponding decrease the cost of predicting a less important

TABLE II
DISTRIBUTION OF ASPECTS IN THE DATASET

Aspect

# (%) of Instances

Performance
Usability
Security
Bug
Community
Compatibility
Documentation
Legal
Portability
OnlySentiment
Others
Total

348 (7.7)
1,437 (31.8)
163 (3.6)
189 (4.2)
93 (2.1)
93 (2.1)
256 (5.6)
50 (1.1)
70 (1.5)
348 (7.7)
1,699 (37.6)
4,522

class wrong. Considering the used data in this study
is imbalanced,
intuitively, CostSensBERT could be a
suitable variant for our task.

III. METHODOLOGY

This section ﬁrst describes the dataset used in this work.
Then, we elaborate on the implementation details of all
approaches. Lastly, we describe the evaluation metrics and
experimental settings.

A. Dataset

To facilitate the comparison, we use the same dataset
constructed by Uddin et al. [8]. The dataset consists of 4,522
sentences extracted from 1,338 posts that discussed APIs in
Stack Overﬂow. Each sentence is manually labeled as one or
multiple API review aspects [22]. Among all the sentences in
the dataset, 4,307 of them are labeled by only one API aspect,
209 sentences are labeled with two aspects, and 6 sentences
are assigned with three or more aspects.

The data distribution shown in Table II indicates that the
dataset is imbalanced, i.e., the numbers of instances differ
across the aspects. Among all the instances, the ratio of every
API review aspect varies from 1.1% to 37.6% in this dataset.
Two API review aspects contain more than 30% instances. In
comparison, 6 out of 11 aspects correspond to less than 5%
samples.

B. Sample Strategy

As mentioned in Section III-A, the dataset is imbalanced.
In [8], Uddin et al. utilized two different sampling strategies
to mitigate the data imbalance issue: balanced sample strat-
egy with undersampling; imbalanced sample strategy without
undersampling. We apply both sampling strategies on all
approaches.
With Undersampling For each given API review aspect, we
regard all the corresponding instances as positive samples and
randomly select the same number of instances from remaining
aspects (labeled as other aspects) as negative samples.
Without Undersampling For each given API review aspect,
we regard all the corresponding instances as positive samples,
and all the remaining instances as negative samples.

C. Experimental Setting

In this study, we follow the same experimental setting in
Opiner [8] and apply the setting to all the approaches for a
fair comparison. We employ a standard stratiﬁed 10-fold cross-
validation to evaluate the approaches. Moreover, for the pre-
trained transformer-based models, we split the data into ten
folds, while eight for ﬁne-tuning, one for validation, and one
for test. In adition, we used AdamW as the optimizer for all the
PTMs which is widely-used in many works, e.g., [14], [15],
[23]. In adition, we utilize AdamW which is widely-used in
many works (e.g., [14], [15], [23]) as the optimizer for all the
PTMs.

D. Implementations

The SOTA approach: Opiner Since the replication pack-
age of Opiner has not been released online yet and the main
algorithms used in Opiner are simple, we re-implemented it
based on the description from the original paper and consulted
with the corresponding authors to conﬁrm a few unclear
details (e.g., the selection of optimal hyper-parameters). We
managed to replicate Opiner and observed even slightly better
performance than the results reported in the original paper by
the above means. We further discuss the potential threat in
Section V-C.

Pre-trained Transformer-based Approaches In this study,
we consider
four popular and state-of-the-art pre-trained
transformer-based models which have been utilized in
many other software tasks [9], [24], [25], including BERT,
RoBERTa, ALBERT, XLNet. We also apply two PTM vari-
ants: BERTOverﬂow [18] that is pre-trained with software
engineer in-domain data; CostSensBERT [19] that designed to
handle imbalanced data. To adapt the four well-known PTMs
for our task, we follow a standard way by adding a dropout
layer and a linear layer [13] on the top of the PTMs. We
ﬁne-tune the PTMs by feeding the training data mentioned in
Section III-A.

Inspired by the hyper-parameter setting of BERT, we tune
three key hyper-parameters with the same scope mentioned
in the original paper [13], they are (1). learning rate; (2)
batch size; (3) number of epochs. We tune the learning rate
by varying three different values, 5e-5, 3e-5, and 1e-5 while
varying 16 and 32 for Batch size. We set the maximum number
of the epoch as 5.

In addition, to adapt CostSensBERT, one additional hyper-
parameter named Class Weight needs to be set, which refers
to the cost increasing for obtaining the low-frequency class
label wrong. We set its value as (1,20), which is suggested in
the paper [19].

We implement all PTMs except CostSensBERT by using
a popular deep learning library Hugging Face Transformer2.
For CostSensBERT, we reuse its replication package3. BERT,
RoBERTA, XLNet, and ALBERT released several versions

2https://huggingface.com
3https://github.com/H-TayyarMadabushi/Cost-Sensitive Bert and

Transformers

with different parameter sizes. We use the same model version
by following their original papers for all PTM approaches.
The versions of the PTMs presented in Table III are the
corresponding names in the Hugging Face Transformer library.

TABLE III
MODEL VERSION OF CONSIDERED PTMS

Model Architecture

Model Version

BERT
ALBERT
RoBERTa
XLNet
BERTOverﬂow
CostSensBERT

bert-base-uncased
albert-base-v2
roberta-base
xlnet-base-cased
jeniya/BERTOverﬂow
bert-base-uncased

E. Evaluation Metrics

By following the latest work [8], we use the same ﬁve
evaluation metrics to measure all the approaches. They are
weighted precision (denoted as P), weighted recall (denoted
as R), weighted F1 (denoted as F1), the Matthews Correlation
Coefﬁcient score (MCC), and the score of Weighted Area
Under ROC Curve (AUC). Same as [8], we also regard F1 as
the main evaluation metric for a fair comparison. The formulas
of P, R, and F1 are calculated as follows:

P recategory =

Reccategory =

#T Pcategory
#T Pcategory + #F Pcategory

#T Pcategory
#T Pcategory + #F Ncategory

F 1category = 2 ×

P recategory × Rcategory
P recategory + Rcategory

(1)

(2)

(3)

ncategory × P recategory

(4)

P =

R =

1
2

1
2

×

×

#category
(cid:88)

category=1

#category
(cid:88)

category=1

M CC =

√

T P ×T N −F P ×F N

(T P +F P )(T P +F N )(T N +F P )(T N +F N )

(7)

AUC stands for the area under the Receiver Operating Char-
acteristic Curve (ROC-AUC). It is threshold independent mea-
sure, which give us the insight from ranking prediction per-
spective. It represents to the probability that a randomly chosen
negative example will have a smaller estimated probability of
belonging to the positive class than a randomly chosen positive
example [26], [27]. The formula of AUC score is calculated
as follow:

AU C =

S0 − n0(n0 + 1)/2
n0n1

(8)

Where n0 and n1 are the numbers of positive and negative
samples, respectively, and S0 = (cid:80) ri, where ri is the rank of
the ith positive example in the descending list of output score
produced by every model.

IV. EVALUATION

In this section, we answer three research questions:

RQ1. Can pre-trained transformer-based models achieve better
performance than the state-of-the-art approach which is based
on traditional machine learning models?
RQ2. Can domain-speciﬁc BERT (i.e., BERTOverﬂow)
achieve better performance than the standard BERT? If not,
can it still outperform Opiner?
RQ3. Can the BERT for imbalanced data (i.e., CostSens-
BERT) achieve better performance than the standard BERT?
If not, can it still outperform Opiner?

For each research question, we ﬁrst describe the correspond-
ing motivation and then report and analyze the performance
of seven approaches on aspect-based API review classiﬁcation
task. For each API review aspect, we evaluate the performance
in terms of ﬁve evaluation metrics (i.e., P, R, F 1, M CC, and
AU C) as introduced in Section III-E.

ncategory × Reccategory

(5)

TABLE IV
AVERAGE PERFORMANCE OF APPROACHES ON ALL THE ASPECTS

F 1 =

1
2

×

#category
(cid:88)

category=1

ncategory × F 1category

(6)

As the problem is formulated as binary classiﬁcation prob-
lem, the number of category is set to 2. For every aspect,
T Pcategory refers to the number of true-positive samples of a
particular category (i.e., sentences are correctly classiﬁed with
the aspect); F Pcategory refers to the number of false-positive
samples of a particular category (i.e., sentences are mistakenly
classiﬁed with the aspect); F Ncategory refers to the numbers of
false-negative samples of a particular category (i.e., sentences
mistakenly classiﬁed as other aspects). ncategory is the number
of instances of a particular category.

MCC is a measure of the quality of binary classiﬁcation,
which takes into account true and false positives and negatives.
The formula of MCC is calculated as follow:

Model Name

Avg F1

Avg MCC

Avg AUC

Opiner
ALBERT
BERT
RoBERTa
XLNet
BERTOverﬂow
CostSensBERT
The numbers in bold show the highest scores among all approaches.

0.724
0.933
0.942
0.943
0.938
0.907
0.876

0.734
0.704
0.785
0.775
0.745
0.559
0.814

0.478
0.482
0.607
0.585
0.520
0.134
0.661

RQ1 Can pre-trained transformer-based models achieve better
performance than the state-of-the-art approach which is based
on traditional machine learning models?
Motivation Previous studies have shown the great potential
of pre-trained transformer-based models on many software
engineering tasks, e.g., sentiment analysis for software data [9]
and code summarization [12]. However, the efﬁcacy of the
pre-trained transformer-based models for various types of

TABLE V
DETAILED PERFORMANCE OF PTMS AND OPINER. (-: THE BEST PERFORMER IN TERMS OF F1. THE HIGHEST F1 SCORE IS IN BOLD)

Aspect

Approach

Precision

Recall

Performance

Usability

Security

Community

Compatibility

Portability

Documentation

Bug

Legal

OnlySentiment

General Features

Opiner
ALBERT
BERT
RoBERTa
XLNet -
BERTOverﬂow
CostSensBERT
Opiner
ALBERT
BERT
RoBERTa -
XLNet
BERTOverﬂow
CostSensBERT
Opiner
ALBERT
BERT
RoBERTa -
XLNet
BERTOverﬂow
CostSensBERT
Opiner
ALBERT
BERT
RoBERTa -
XLNet
BERTOverﬂow
CostSensBERT
Opiner
ALBERT
BERT -
RoBERTa
XLNet
BERTOverﬂow
CostSensBERT
Opiner
ALBERT
BERT
RoBERTa -
XLNet
BERTOverﬂow
CostSensBERT
Opiner
ALBERT
BERT -
RoBERTa
XLNet
BERTOverﬂow
CostSensBERT
Opiner
ALBERT
BERT
RoBERTa
XLNet -
BERTOverﬂow
CostSensBERT
Opiner
ALBERT
BERT -
RoBERTa
XLNet
BERTOverﬂow
CostSensBERT
Opiner
ALBERT
BERT
RoBERTa -
XLNet
BERTOverﬂow
CostSensBERT
Opiner
ALBERT
BERT
RoBERTa -
XLNet
BERTOverﬂow
CostSensBERT

0.680
0.952
0.963
0.966
0.966
0.882
0.913
0.649
0.783
0.799
0.804
0.794
0.747
0.777
0.876
0.981
0.986
0.983
0.986
0.930
0.935
0.641
0.962
0.976
0.976
0.972
0.959
0.933
0.619
0.959
0.973
0.964
0.966
0.960
0.897
0.843
0.981
0.991
0.992
0.962
0.969
0.959
0.729
0.950
0.964
0.960
0.962
0.908
0.919
0.849
0.966
0.974
0.962
0.974
0.919
0.905
0.877
0.983
0.996
0.996
0.995
0.978
0.930
0.814
0.947
0.946
0.951
0.948
0.917
0.912
0.694
0.798
0.812
0.814
0.798
0.718
0.797

0.796
0.951
0.962
0.965
0.966
0.923
0.878
0.780
0.779
0.795
0.795
0.784
0.742
0.769
0.786
0.975
0.974
0.985
0.976
0.964
0.921
0.534
0.980
0.981
0.981
0.980
0.979
0.804
0.664
0.979
0.981
0.980
0.980
0.980
0.847
0.757
0.985
0.990
0.992
0.980
0.985
0.930
0.826
0.955
0.964
0.961
0.961
0.944
0.905
0.742
0.965
0.974
0.980
0.974
0.959
0.876
0.820
0.990
0.995
0.995
0.995
0.989
0.911
0.702
0.951
0.948
0.952
0.951
0.927
0.908
0.619
0.789
0.808
0.812
0.797
0.716
0.794

F1

0.732
0.950
0.962
0.965
0.966
0.898
0.887
0.707
0.779
0.795
0.797
0.785
0.742
0.770
0.823
0.975
0.978
0.983
0.979
0.947
0.918
0.572
0.970
0.976
0.976
0.974
0.969
0.847
0.637
0.969
0.975
0.971
0.972
0.970
0.869
0.762
0.983
0.990
0.992
0.970
0.977
0.933
0.772
0.948
0.964
0.960
0.961
0.924
0.913
0.786
0.964
0.974
0.970
0.974
0.938
0.889
0.835
0.986
0.995
0.995
0.995
0.983
0.916
0.750
0.948
0.947
0.951
0.948
0.920
0.910
0.653
0.789
0.808
0.812
0.795
0.716
0.793

MCC

0.433
0.652
0.735
0.761
0.759
0.149
0.773
0.371
0.496
0.532
0.544
0.518
0.412
0.547
0.678
0.681
0.773
0.757
0.774
0.000
0.825
0.242
0.049
0.331
0.352
0.269
0.000
0.423
0.249
0.000
0.299
0.074
0.107
0.000
0.306
0.623
0.375
0.693
0.746
0.049
0.000
0.786
0.527
0.482
0.654
0.616
0.632
0.120
0.764
0.598
0.557
0.664
0.549
0.667
0.000
0.739
0.640
0.526
0.788
0.784
0.753
0.000
0.789
0.546
0.620
0.612
0.647
0.624
0.402
0.679
0.351
0.563
0.594
0.601
0.565
0.388
0.611

AUC

0.711
0.810
0.863
0.875
0.868
0.545
0.878
0.680
0.748
0.766
0.778
0.764
0.707
0.771
0.835
0.821
0.894
0.866
0.893
0.500
0.921
0.615
0.512
0.592
0.624
0.591
0.500
0.664
0.622
0.500
0.587
0.523
0.538
0.500
0.596
0.800
0.667
0.840
0.883
0.512
0.500
0.880
0.759
0.685
0.818
0.794
0.813
0.540
0.875
0.794
0.739
0.810
0.712
0.814
0.500
0.856
0.810
0.709
0.889
0.869
0.849
0.500
0.881
0.769
0.774
0.784
0.801
0.780
0.670
0.828
0.674
0.782
0.792
0.798
0.775
0.684
0.804

software data still remains unclear. Hence, we investigate the
effectiveness of four popular state-of-the-art PTMs (i.e., BERT,
RoBERTa, ALBERT, XLNet) for the task of aspect-based API
review classiﬁcation.
Results & Analysis To answer RQ1, we compare the per-
formance of four popular PTMs (i.e., BERT, RoBERTa, AL-
BERT, XLNet) with the state-of-the-art approach (i.e., Opiner).
We present
the summative and the detailed experimental
results of the approaches in Table IV and Table V, respectively.
For the summative result in Table IV, we calculate the arith-
metic average of the used evaluation metrics of each approach
across all the aspects as avg. F1, avg. MCC, and avg. AUC,
respectively. From Table IV, we found that all the four PTMs
outperform Opiner in terms of F1 while RoBERTa is the best
performer. The improvement ranges from 28.9% to 30.2%.

We also found that the best performer for all the aspects is
not always the same. In particular, among all the 11 aspects,
RoBERTa achieves the best performance for six aspects (i.e.,
Usability, Security, Community, Portability, OnlySentiment,
General Features), BERT is the best for three aspects (i.e.,
Compatibility, Documentation, Legal), and XLNet is the best
for two aspects, (i.e., Performance, Bug).

Moreover, comparing the best PTM performer

In addition, we observed that most of the PTMs outperform
Opiner on all the evaluation metrics but there is an exception.
ALBERT performs worse than Opiner in terms of average
AUC score by 4.08%. More speciﬁcally, the MCC and AUC
of ALBERT are worse than Opiner in the three aspects (i.e.,
Community, Compatibility, Legal) by at least 60%. And all
three aspects have less than 3% instances in the whole dataset.
ALBERT even has an MCC score of 0 on the Compatibility
aspect. Conversely, the other three PTMs outperform Opiner
on two or all aspects in terms of MCC score. One potential
reason is that: Considering that ALBERT is a lightweight
version of BERT, the parameter reduction techniques utilized
in ALBERT may result in potential performance degradation.
(i.e.,
RoBERTa) with the state-of-the-art approach Opiner, we found
the improvement of RoBERTa varies from different aspects.
For instance, in terms of the F1 score, RoBERTa achieves
the greatest improvement on the aspect of Community by
70.6%. However, for the aspect Usability, RoBERTa reaches
the slightest improvement of 12.7%. In addition, we found
that the improvements are signiﬁcant, especially for the aspects
with a small number of data instances. For example, RoBERTa
outperforms Opiner the most on the aspect Community, which
only corresponds to 2.1% instances of the whole dataset (see
Table I). The aspect Usability contains 15 times more labeled
instances than the aspect Community, while RoBERTa only
achieves 12.7% improvement over Opiner. It indicates that
PTM approaches are more capable of extracting semantic
information than Opiner, especially when only a small number
of data instances are available in the training set. We discuss
more about the rationale by conducting an error analysis in
Section V-A. We also observed that the best and the worst
PTM approach varies in different API review aspects. For
example, XLNet achieves the best performance on the aspect

Performance while it is the worst on the aspect Portability.

Compared with the prior work [9] which utilizes PTMs for
sentiment analysis on software data, we have the same ﬁnding
that PTMs outperform existing SOTA approaches by a large
margin (ranges from 6.5% to 35.6% in [9] and from 21%
to 30.2% in this work). It indicates that PTMs have a great
potential for boosting various types of SE tasks. Furthermore,
RoBERTa is also the overall winner in both [9] and our work,
which demonstrates that RoBERTa equips better generalization
ability in SE domain tasks than other considered PTMs. Thus,
it suggests that considering RoBERTa as the ﬁrst attempt for
the following works.

The four well-known pre-trained transformer-based
models consistently outperform the state-of-the-art
the 11 aspects. The best
tools, Opiner, on all
transformer-based approach varies in different aspects
and the improvement (over Opiner) of every PTM
model varies in different aspects. The performance
improvements achieved by the PTM model ranges
from 28.9% to 30.2% in terms of average F1 score
for all API review aspects.

RQ2: Can domain-speciﬁc BERT (i.e., BERTOverﬂow)
achieve better performance than the standard BERT? If not,
can it still outperform Opiner?
Motivation Recent studies [15], [17], [18] have proven that,
for many tasks, there are two typical ways to boost the model
performance: (1) train on a larger dataset and (2) train on
a domain-speciﬁc dataset. However, these two means usu-
ally cannot be applied simultaneously as large-scale domain-
speciﬁc data is often unavailable. Thus, in this work, we are
interested in investigating which one plays a more important
role for aspect-based API review classiﬁcation. To achieve
that, we compare the standard BERT with its variant named
BERTOverﬂow [18]. BERTOverﬂow follows the same archi-
tecture of BERT but is trained on the domain-speciﬁc data (i.e.,
Stack Overﬂow).
Results & Analysis To answer this research question, we
report the performance of BERTOverﬂow for aspect-based
API review classiﬁcation. As BERTOverﬂow is trained with
BERT structure, we compare its performance with base version
BERT on the API review dataset.

In Table IV, we observed that for all the aspects, the stan-
dard BERT consistently outperforms BERTOverﬂow for all
the evaluation metrics. Moreover, we found the performance
gap between the standard BERT and BERTOverﬂow differs
for different API review aspects. For the aspect Compatibility,
BERT slightly outperforms BERTOverﬂow by 0.5% in terms
of F1 score. However, for the aspect General Features, the
standard BERT outperforms BERTOverﬂow by 13.8% in terms
of F1. The potential reason could be because of the size of
the training data varies. The size of training data of BERT
is 3.3 billion words (from Wikipedia dataset and BookCorpus
dataset) while which of BERTOverﬂow is 152M sentences.
Considering an average sentence length of BookCorpus is 11

words, the sentences used to pre-train the standard BERT is 2
times more than BERTOverﬂow.

In addition, we found that BERTOverﬂow still outperforms
Opiner on all the aspects in terms of F1 score. However,
we found that BERTOverﬂow performs worse than Opiner in
terms of MCC and AUC by 72% and 23.8%, respectively.

Standard BERT outperforms BERTOverﬂow by 3.9%,
77.9%, 28.8% in terms of average F1, MCC, and AUC.
Compared with Opiner, BERTOverﬂow performs bet-
ter in terms of average F1 by 25.3% but worse on
MCC and AUC by 72.0% and 23.8%.

RQ3: Can the BERT for imbalanced data (i.e., CostSens-
BERT) achieve better performance than the standard BERT?
If not, can it still outperform Opiner?
Motivation As the transformer-based models evolved rapidly,
variants of PTMs emerged to solve speciﬁc tasks. CostSens-
BERT is one of them which is designed to deal with the
data imbalance problem. In the original paper of CostSens-
BERT [19], CostSensBERT achieves a better performance than
BERT on the task of SE-speciﬁc named entity recognition
on Stack Overﬂow data. In this study, we are interested in
exploring the applicability of CostSensBERT to aspect-based
API review classiﬁcation.
Results & Analysis As shown in Table V, we observed
that the performance of CostSensBERT is worse than BERT
in terms of F1. In particular, BERT outperforms CostSens-
BERT by 7.5% in terms of F1. However, CostSensBERT
achieves better performance than BERT in terms of MCC
by 8.89%. According to the deﬁnition of MCC mentioned in
Section III-E, a high MCC can only be achieved when the pre-
dicted results are promising on all of the four confusion matrix
categories (true positives, false negatives, true negatives, and
false positives). Therefore, compared with F1, MCC can shed
a different light on the prediction ability of the minority class.
Our experimental results demonstrate that CostSensBERT im-
proves the prediction ability of the minority class but hurts
the prediction ability of majority samples to some extent. In
Table IV, compared with Opiner, CostSensBERT outperforms
Opiner by 21.0% in terms of average F1 score. From Table
V, we can ﬁnd that CostSensBERT outperforms Opiner on all
API review aspects.

Furthermore, the experimental results demonstrate the ad-
vantage of CostSensBERT in handling the imbalanced dataset.
For instance, Usability and General Features are the two
aspects with the most number of instances. CostSensBERT
outperforms BERT on aspect Usability by 2.7% and 0.6%
in terms of AUC and MCC, respectively. However, standard
Bert outperforms CostSensBERT on the Usability aspect in
terms of F1 by 3.2%. The same observation applies to the
other aspects with a large number of instances, like General
Features. On the other hand, for the aspects with a small
number of instances like Portability, CostSensBERT achieves
better performance than BERT by 11.8% and 4.5% on AUC
and MCC, respectively. This conﬁrms our ﬁnding that the

Fig. 1. Venn Diagram of Aspect Usability

CostSensBERT does help to handle imbalanced datasets by
enhancing the ability to identify minority class samples, but
at the same time, more majority class instances are mistakenly
identiﬁed. Thus, our results indicate that CostSensBERT may
still be worthy of being considered. Also, a model which lever-
ages the advantages of standard BERT and CostSensBERT and
avoids their disadvantages could be promising. And we leave
it as our future work.

Standard BERT outperforms CostSensBERT in terms
of average F1 by 7.5%, but CostSensBERT performs
better on average MCC and AUC by 8.9% and 3.7%,
respectively. Moreover, CostSensBERT still outper-
forms Opiner by 21.0%, 38.3%, 10.9% in terms of
average F1, MCC and AUC.

A. Error Analysis

V. DISCUSSION

We conduct an error analysis by (1) randomly sampling
the predicted result produced by the considered approaches,
(2) drawing Venn diagrams to interpret the relationship of
the correctly predicted instances between the best performing
PTM and Opiner.

Compared with Opiner, we observed that unseen features is
one primary reason that all the considered PTMs achieve better
performance. During the prediction stage, Opiner is unable to
capture the semantic meaning of the words when they do not
exist in the vocabulary build on the training data. However,
beneﬁting from the pre-training on the huge amount of training
data, PTMs can still capture partial semantic meaning. Take
an instance in our dataset as an example, the sentence “The
javabean getters/setters have ... and does not slowdown...”
is labeled as Performance. Opiner misclassiﬁes this sentence
while all PTMs predict the label correctly. We further found
that the keyword “slowdown” does not exist in the training
dataset. The example further proves that Opiner suffers from
out-of-vocabulary issue [28], [29]. At the same time, it also
demonstrates the key advantage of pre-trained models.

Furthermore, we compare the best performing pre-trained
transformer-based model, RoBERTA and Opiner on two se-
lected API review aspects, Usability and Portability. Usability
and Portability are one of the aspects with the largest and

2925434RoBERTaCorrect PredictionsOpinerCorrect Predictionsobserved that in all the best experimental settings of Opiner,
applying undersampling strategy (i.e., with undersampling in
Section III-B) consistently achieves the best performance. For
all the PTMs, we observed the opposite that undersampling
does not boost and even slightly hurts the performance.
2) Set the batch size to 32 produces better results than
set it to 16 for most of the aspects. We found that for
all
there is no golden combination
of parameters that can achieve the best performance on all
the aspects. One potential reason could be the diversity of
the data in terms of the distribution and semantics across
different aspects. However, we found that the batch size of
32 is preferable in most of the aspects thus we recommended
future works to use the batch size as their ﬁrst attempts.

the considered PTMs,

C. Threats to Validity

Threat to internal validity relates to errors and biases in our
experiments. To mitigate the threat, we employ a standard 10-
fold cross-validation to avoid the bias that might be introduced
test dataset. Also, we
to the results by the leave-one-out
follow a standard hyper-parameter searching procedure on all
PTMs [13]. As mentioned in Section III-D, we re-implemented
Opiner as its replication package is not online available.
We followed three steps to ensure the correctness of our
replication: (1) we carefully followed all the technical details
described in the original paper, (2) double check with the
original authors about unclear information. Due to possible
differences in the random data shufﬂing in the data pre-
processing stage, our performance in all API review aspects
is slightly higher than the performance of Opiner reported in
the original paper [8]. To replicate two speciﬁc variants of
BERT (i.e., BERTOverﬂow and CostSensBERT), we reused
the replication package released by the original authors. To
implement the four popular PTMs, we utilized a widely-used
deep learning library Hugging Face Transformer. Considering
above, we believe the threat is minimal. Moreover, we will
also release a replication package which includes both data
and code to facilitate validation and extension. In addition,
one of the threats to internal validity refers to the quality of
datasets labels. We inherit this threat as we use a publicly
accessible dataset released from previous work. Threats to ex-
ternal validity related to the generalizability of our results. In
contrast with the prior works which only consider mainstream
PTMs [9], we also consider variants of PTMs which might
be potentially suitable considering the characteristics of the
task. For fair comparison, we reuse the same data that was
released by Uddin et al. and used to evaluate Opiner [8]. The
ﬁndings that we have here may be different when additional
PTMs and data are considered. We plan to consider more
PTMs and a larger dataset than the one collected by Uddin
et al. [8] in the future. The threat to the construct validity
of our work related to propriety of our evaluation metrics. We
follow the same evaluation metrics used in the prior works on
the same tasks [5], [8]. Moreover, the used evaluation metrics
(i.e., Precision, Recall, F1 score, MCC, and AUC) are widely
used in the SE ﬁelds [30]–[32].

Fig. 2. Venn Diagram of Aspect Portability

smallest number of data instances, respectively. The number
of data instances labeled as Usability (1,437) is 15 times bigger
than which of Portability (70). In total, we have 448 instances
in the test dataset.

We observed that some instances where only Opiner can
produce the correct labels. There are also cases where only
RoBERTa can produce correct labels. Figures 1 and 2 demon-
strate the number of correct predictions produced by RoBERTa
and Opiner in the two aspects Usability and Portability. For the
aspect of Usability, RoBERTa and Opiner correctly predict 346
(77.2%) and 326 (67.9%) instances, respectively. In addition,
for 336 instances, both methods make the correct predictions.
For the API review aspect of portability, RoBERTa and Opiner
correctly predict 445 (99.3%) and 339 (75.6%) instances,
respectively. In addition, for 292 instances, both methods make
the correct predictions. Thus we found that although Opiner
performs worse than RoBERTa overall, it still performs better
in some instances (i.e., 34 instances in Usability aspect, 3
instances in Portability aspect). There is a potential to combine
the two approaches to boost performance further. We leave this
as future work.

B. Parameter Settings

Parameter settings play a critical role in determining the
performance of deep learning models including PTMs. In this
section, we describe our observations on the best parameter
setting of considered PTMs. By this means, we aim to shed
light on how to effectively adapt these PTMs for future re-
search on aspect-based API review classiﬁcation. Our ﬁndings
may also possibly translate to other tasks.

In our experiments, we ran 10-fold cross-validation for
every approach on all eleven aspects. In each fold, we ﬁne-
tune six PTMs twelve times based on different combina-
tions of hyper-parameters described in Table III. Overall,
we implement
the ﬁne-tuning process about 7,920 times
(10 × 11 × 6 × 12). Moreover, as the dataset is imbalanced,
our experiment covers different scales of classiﬁcation tasks
for the aspects (e.g., the number of instances in the Usability
aspect is about 29 times of the Legal aspect), two sampling
strategies, and 11 semantic labels (i.e., aspects). We summarize
our observations and ﬁndings on parameter settings as below:
1) Undersampling boosts the performance of traditional
machine learning models but not for PTMs in this task. We

3361093RoBERTaCorrect PredictionsOpinerCorrect PredictionsVI. RELATED WORK

A. Pre-trained transformer models for SE tasks

Recently, pre-trained transformer models are receiving great
attention for several domains including software engineering.
Zhang et al. ﬁne-tuned BERT, RoBERTa, XLNet, ALBERT
for sentiment analysis for software engineering (SA4SE) and
compared them with ﬁve existing SA4SE tools on six datasets
[9]. They found that the PTMs outperform the traditional
ones ranges from 6.5 to 35.6% in terms of F1. Biswas et al.
used BERT for sentiment analysis of software artifacts with
signiﬁcant improvement over the state-of-the-art [24]. Khan et
al. used BERT to detect different API documentation issues
that outperformed all other models [25]. Moreover, Lin et
al. utilize BERT and transfer learning to generate trace links
between software text and source code [33].

Several transformer models can further be trained to learn
code as well as contextual word representations and can
be ﬁne-tuned for different downstream SE tasks such as
natural language code search, automated program repair, auto-
mated testing, code summarization. Tabassum et al. developed
BERTOverﬂow, ELMoVerﬂow, GloVerﬂow for SE tasks by
training the corresponding original versions (i.e. BERT, ELMo,
GloVe) on 152 million Stack Overﬂow sentences [18]. Based
on the proposed models, they presented SoftNER for code
and named entity recognition on Stack Overﬂow data. Mosel
et al. developed seBERT by pre-training it with 204.4 GB
data coming from Stack Overﬂow, GitHub, and Jira [34]. They
found that seBERT can achieve promising results on multiple
tasks, e.g., quality improving commit identiﬁcation and issue
type prediction. Feng et al. presented CodeBERT, a bimodal
model for both natural and programming language which is
pre-trained on a large dataset containing 2.1M bimodal data
(i.e. code and corresponding documentation) and 6.4M uni-
modal data (i.e. only code) across six programming languages
(Python, Java, JavaScript, PHP, Ruby, and Go) [35].

B. API Review Analysis

With rapid growth of APIs and their impact on software
development process, developers often face difﬁculties to
choose the right API. Hence, developers seek advice and
reviews about different APIs from other developers in various
online forums. However, the vast number of API reviews is
hard to follow which poses a new challenge for the developers.
Hence, several studies have focused on the analysis of such
online developers discussion. Hou et al. [36] identiﬁed several
categories of obstacles in using APIs and discussed about how
to overcome those obstacles. Rosen et al. analyzed over 13M
Stack Overﬂow posts by mobile developers which revealed
various concerns such as various app distribution, mobile
APIs, data management [37]. The automatic mining of API
insights from such review discussion has also gotten a lot of at-
tention. Treude et al. detected insightful API related sentences
from Stack Overﬂow discussion using ML techniques and used
them to enhance API documentation [38]. Besides, Zhang et
al. [39] extracted the problematic API features from online

discussion by using sentiment analysis. Being motivated by the
presence of various API aspects (e.g., performance, usability)
in their Stack Overﬂow case study, Uddin et al. developed
a set of ML classiﬁers to detect them [5], [8]. Their survey
involving 178 developers shows that developers need API
reviews and seek for automated tool support to assess them
[2]. Ahasanuzzaman et al. focus on identifying API issue-
related posts in Stack Overﬂow. To address the problem, they
develop a supervised learning approach CAPS, which based on
ﬁve different dimensions and conditional random ﬁeld (CRF)
technique [1]. In the following work, Ahasanuzzaman et al.
further extend their approach by proposing more features [3].

VII. CONCLUSION AND FUTURE WORK

In this study, we conducted an empirical evaluation on the
performance of PTMs (pre-trained transformer-based models)
and the state-of-the-art machine learning-based approach for
API review classiﬁcation. We are the ﬁrst to shed a light
of the effectiveness among variants of PTMs (i.e., domain-
speciﬁc, designed for imbalanced data). Our empirical study
includes six PTMs, i.e., BERT, RoBERTa, ALBERT, XLNET,
BERTOverﬂow, and CostSensBERT. They cover not only
the state-of-the-art PTMs but also PTMs that are designed
considering the characteristics of the data that we have (e.g.,
SE domain-speciﬁc data, imbalanced label distribution). Our
experimental result shows that all
the PTMs consistently
outperform the state-of-the-art approaches. More importantly,
we found that two potential suitable variants of BERT do not
perform better than the standard BERT in our task. However,
particularly, CostSensBERT sheds a light on its ability on
identifying the minority class that it achieves higher MCC than
the standard BERT. Hence, we suggest that CostSensBERT is
still worth trying for SE tasks with imbalanced data.

Overall, PTMs are more applicable than the existing state-
of-the-art approach in API review classiﬁcation task. We
require a more in-depth look at the relationship among PTMs
different from data domain and design purposes. We release
the replication package 4. In the future, we plan to explore
the following directions for further boosting aspect-based API
review classiﬁcation: (1) investigate more potentially suitable
PTM variants, and (2) design an ensemble PTM-based model
to leverage the advantage of RoBERTa and Opiner and avoid
their shortcomings at the same time.

ACKNOWLEDGMENT

This research / project is supported by the Ministry of
Education, Singapore, under its Academic Research Fund Tier
2 (Award No.: MOE2019-T2-1-193). Any opinions, ﬁndings
and conclusions or recommendations expressed in this material
are those of the author(s) and do not reﬂect the views of the
Ministry of Education, Singapore. This research / project is
also supported by Natural Sciences and Engineering Research
Council of Canada (NSERC), University of Calgary, and
Alberta Innovates.

4https://github.com/soarsmu/PTM4SE

REFERENCES

[1] M. Ahasanuzzaman, M. Asaduzzaman, C. K. Roy, and K. A. Schneider,
“Classifying stack overﬂow posts on api issues,” in 2018 IEEE 25th in-
ternational conference on software analysis, evolution and reengineering
(SANER).

IEEE, 2018, pp. 244–254.

[2] G. Uddin, O. Baysal, L. Guerrouj, and F. Khomh, “Understanding
how and why developers seek and analyze api-related opinions,” IEEE
Transactions on Software Engineering, vol. 47, no. 04, pp. 694–735,
2021.

[3] M. Ahasanuzzaman, M. Asaduzzaman, C. K. Roy, and K. A. Schneider,
“Caps: a supervised technique for classifying stack overﬂow posts
concerning api issues,” Empirical Software Engineering, vol. 25, no. 2,
pp. 1493–1532, 2020.

[4] G. Uddin and M. P. Robillard, “How api documentation fails,” Ieee

software, vol. 32, no. 4, pp. 68–75, 2015.

[5] G. Uddin and F. Khomh, “Mining api aspects in api reviews,” in

Technical Report, 2017.

[6] G. Uddin and F. Khomh, “Automatic summarization of api reviews,” in
2017 32nd IEEE/ACM International Conference on Automated Software
Engineering (ASE).
IEEE, 2017, pp. 159–170.

[7] B. Lin, F. Zampetti, G. Bavota, M. Di Penta, and M. Lanza, “Pattern-
based mining of opinions in q&a websites,” in 2019 IEEE/ACM 41st
International Conference on Software Engineering (ICSE).
IEEE, 2019,
pp. 548–559.

[8] G. Uddin and F. Khomh,

“Automatic mining of opinions expressed
about apis in stack overﬂow,” IEEE Transactions on Software Engineer-
ing, vol. 47, no. 3, pp. 522–559, 2021.

[9] T. Zhang, B. Xu, F. Thung, S. A. Haryono, D. Lo, and L. Jiang,
“Sentiment analysis for software engineering: How far can pre-trained
transformer models go?” in 2020 IEEE International Conference on
Software Maintenance and Evolution (ICSME).
IEEE, 2020, pp. 70–
80.

[10] S. Zhou, B. Shen, and H. Zhong, “Lancer: Your code tell me what you
need,” in 2019 34th IEEE/ACM International Conference on Automated
Software Engineering (ASE).

IEEE, 2019, pp. 1202–1205.

[11] Z. Yu, R. Cao, Q. Tang, S. Nie, J. Huang, and S. Wu, “Order matters:
Semantic-aware neural networks for binary code similarity detection,” in
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 34,
no. 01, 2020, pp. 1145–1152.

[12] R. Wang, H. Zhang, G. Lu, L. Lyu, and C. Lyu, “Fret: Functional
reinforced transformer with bert for code summarization,” IEEE Access,
vol. 8, pp. 135 591–135 604, 2020.

[13] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional
transformers for language understanding,” in
Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), 2019, pp. 4171–4186.
[14] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert
pretraining approach,” arXiv preprint arXiv:1907.11692, 2019.

[15] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,
“Albert: A lite bert for self-supervised learning of language representa-
tions,” arXiv preprint arXiv:1909.11942, 2019.

[16] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and
Q. V. Le, “Xlnet: Generalized autoregressive pretraining for language
understanding,” Advances in neural
information processing systems,
vol. 32, 2019.

[17] S. Gururangan, A. Marasovi´c, S. Swayamdipta, K. Lo, I. Beltagy,
D. Downey, and N. A. Smith, “Don’t stop pretraining: adapt language
models to domains and tasks,” arXiv preprint arXiv:2004.10964, 2020.
[18] J. Tabassum, M. Maddela, W. Xu, and A. Ritter, “Code and named entity
recognition in stackoverﬂow,” in Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics, 2020, pp. 4913–4926.
[19] H. T. Madabushi, E. Kochkina, and M. Castelle, “Cost-sensitive bert for
generalisable sentence classiﬁcation with imbalanced data,” EMNLP-
IJCNLP 2019, p. 125, 2019.

[21] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdi-
nov, “Transformer-xl: Attentive language models beyond a ﬁxed-length
context,” arXiv preprint arXiv:1901.02860, 2019.

[20] G. Yang, Y. Zhou, C. Yu, and X. Chen, “Deepscc: Source code

classiﬁcation based on ﬁne-tuned roberta,” 2021.

[22] G. Uddin, O. Baysal, L. Guerrouj, and F. Khomh, “Understanding
how and why developers seek and analyze api-related opinions,” IEEE
Transactions on Software Engineering, 2019.

[23] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”

arXiv preprint arXiv:1711.05101, 2017.

[24] E. Biswas, M. E. Karabulut, L. Pollock, and K. Vijay-Shanker, “Achiev-
ing reliable sentiment analysis in the software engineering domain using
bert,” in 2020 IEEE International Conference on Software Maintenance
and Evolution (ICSME).

IEEE, 2020, pp. 162–173.

[25] J. Y. Khan, M. T. I. Khondaker, G. Uddin, and A. Iqbal, “Automatic
detection of ﬁve api documentation smells: Practitioners’ perspectives,”
in 2021 IEEE International Conference on Software Analysis, Evolution
and Reengineering (SANER).

IEEE, 2021, pp. 318–329.

[26] D. J. Hand and R. J. Till, “A simple generalisation of the area under the
roc curve for multiple class classiﬁcation problems,” Machine learning,
vol. 45, no. 2, pp. 171–186, 2001.

[27] J. Huang and C. X. Ling, “Using auc and accuracy in evaluating learning
algorithms,” IEEE Transactions on knowledge and Data Engineering,
vol. 17, no. 3, pp. 299–310, 2005.

[28] S. Moon and N. Okazaki, “Patchbert: Just-in-time, out-of-vocabulary
patching,” in Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), 2020, pp. 7846–7852.
[29] N. Hartmann, L. Avanc¸o, P. Balage Filho, M. S. Duran, M. D. G. V.
Nunes, T. Pardo, and S. Alu´ısio, “A large corpus of product reviews in
portuguese: Tackling out-of-vocabulary words,” in Proceedings of the
Ninth International Conference on Language Resources and Evaluation
(LREC’14), 2014, pp. 3865–3871.

[30] B. Xu, A. Shirani, D. Lo, and M. A. Alipour, “Prediction of relatedness
in stack overﬂow: deep learning vs. svm: a reproducibility study,”
in Proceedings of the 12th ACM/IEEE International Symposium on
Empirical Software Engineering and Measurement, 2018, pp. 1–10.
[31] B. Xu, D. Ye, Z. Xing, X. Xia, G. Chen, and S. Li, “Predicting seman-
tically linkable knowledge in developer online forums via convolutional
neural network,” in Proceedings of the 31st IEEE/ACM International
Conference on Automated Software Engineering. ACM, 2016, pp. 51–
62.

[32] B. Xu, T. Hoang, A. Sharma, C. Yang, X. Xia, and D. Lo, “Post2vec:
Learning distributed representations of stack overﬂow posts,” IEEE
Transactions on Software Engineering, 2021.

[33] J. Lin, Y. Liu, Q. Zeng, M. Jiang, and J. Cleland-Huang, “Traceability
transformed: Generating more accurate links with pre-trained bert mod-
els,” in 2021 IEEE/ACM 43rd International Conference on Software
Engineering (ICSE).
IEEE, 2021, pp. 324–335.

[34] J. von der Mosel, A. Trautsch, and S. Herbold, “On the validity of
pre-trained transformers for natural language processing in the software
engineering domain,” arXiv preprint arXiv:2109.04738, 2021.

[35] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
T. Liu, D. Jiang et al., “Codebert: A pre-trained model for programming
and natural languages,” arXiv preprint arXiv:2002.08155, 2020.
[36] D. Hou and L. Li, “Obstacles in using frameworks and apis: An ex-
ploratory study of programmers’ newsgroup discussions,” in 2011 IEEE
19th International Conference on Program Comprehension.
IEEE,
2011, pp. 91–100.

[37] C. Rosen and E. Shihab, “What are mobile developers asking about? a
large scale study using stack overﬂow,” Empirical Software Engineering,
vol. 21, no. 3, pp. 1192–1223, 2016.

[38] C. Treude and M. P. Robillard, “Augmenting api documentation with
insights from stack overﬂow,” in 2016 IEEE/ACM 38th International
Conference on Software Engineering (ICSE).
IEEE, 2016, pp. 392–
403.

[39] Y. Zhang and D. Hou, “Extracting problematic api features from
forum discussions,” in 2013 21st International Conference on Program
Comprehension (ICPC).

IEEE, 2013, pp. 142–151.

