AWAC: Accelerating Online Reinforcement
Learning with Ofﬂine Datasets

Ashvin Nair∗, Abhishek Gupta∗, Murtaza Dalal, Sergey Levine
Department of Electrical Engineering and Computer Science, UC Berkeley

1
2
0
2

r
p
A
4
2

]

G
L
.
s
c
[

6
v
9
5
3
9
0
.
6
0
0
2
:
v
i
X
r
a

Abstract—Reinforcement learning (RL) provides an appealing
formalism for learning control policies from experience. However,
the classic active formulation of RL necessitates a lengthy active
exploration process for each behavior, making it difﬁcult to apply
in real-world settings such as robotic control. If we can instead
allow RL algorithms to effectively use previously collected data
to aid the online learning process, such applications could be
made substantially more practical: the prior data would provide
a starting point that mitigates challenges due to exploration and
sample complexity, while the online training enables the agent to
perfect the desired skill. Such prior data could either constitute
expert demonstrations or, more generally, sub-optimal prior data
that illustrates potentially useful transitions. While a number
of prior methods have either used optimal demonstrations to
learning, or have used sub-optimal
bootstrap reinforcement
it remains exceptionally difﬁcult
data to train purely ofﬂine,
to train a policy with potentially sub-optimal ofﬂine data and
actually continue to improve it further with online RL. In
this paper we systematically analyze why this problem is so
challenging, and propose an algorithm that combines sample-
efﬁcient dynamic programming with maximum likelihood policy
updates, providing a simple and effective framework that is
able to leverage large amounts of ofﬂine data and then quickly
perform online ﬁne-tuning of RL policies. We show that our
method, advantage weighted actor critic (AWAC), enables rapid
learning of skills with a combination of prior demonstration
data and online experience. We demonstrate these beneﬁts on a
variety of simulated and real-world robotics domains, including
dexterous manipulation with a real multi-ﬁngered hand, drawer
opening with a robotic arm, and rotating a valve. Our results
show that incorporating prior data can reduce the time required
to learn a range of robotic skills to practical time-scales.

I. INTRODUCTION

Learning models that generalize effectively to complex open-
world settings, from image recognition [29] to natural language
processing [10], relies on large, high-capacity models as well
as large, diverse, and representative datasets. Leveraging this
recipe of pre-training from large-scale ofﬂine datasets has
the potential to provide signiﬁcant beneﬁts for reinforcement
learning (RL) as well, both in terms of generalization and
sample complexity. But most existing RL algorithms collect
data online from scratch every time a new policy is learned,
which can quickly become impractical in domains like robotics
where physical data collection has a non-trivial cost. In the
same way that powerful models in computer vision and NLP
are often pre-trained on large, general-purpose datasets and
then ﬁne-tuned on task-speciﬁc data, practical instantiations
of reinforcement learning for real world robotics problems
will need to be able to incorporate large amounts of prior

∗Equal contribution. Correspondence to nair@eecs.berkeley.edu.

Figure 1: Utilizing prior data for online learning allows us
to solve challenging real-world robotics tasks, such as this
dexterous manipulation task where the learned policy must
control a 4-ﬁngered hand to reposition an object.

data effectively into the learning process, while still collecting
additional data online for the task at hand. Doing so effectively
will make the online data collection process much more
practical while still allowing robots operating in the real world
to continue improving their behavior.
For data-driven reinforcement

learning, ofﬂine datasets
consist of trajectories of states, actions and associated rewards.
This data can potentially come from demonstrations for the
desired task [48, 5], suboptimal policies [15], demonstrations
for related tasks [63], or even just random exploration in
the environment. Depending on the quality of the data that
is provided, useful knowledge can be extracted about the
dynamics of the world, about the task being solved, or both.
Effective data-driven methods for deep reinforcement learning
should be able to use this data to pre-train ofﬂine while
improving with online ﬁne-tuning.

Since this prior data can come from a variety of sources,
we would like to design an algorithm that does not utilize
different types of data in any privileged way. For example,
prior methods that incorporate demonstrations into RL directly
aim to mimic these demonstrations [39], which is desirable
when the demonstrations are known to be optimal, but imposes
strict requirements on the type of ofﬂine data, and can cause
undesirable bias when the prior data is not optimal. While prior
methods for fully ofﬂine RL provide a mechanism for utilizing
ofﬂine data [14, 30], as we will show in our experiments, such
methods generally are not effective for ﬁne-tuning with online
data as they are often too conservative. In effect, prior methods
require us to choose: Do we assume prior data is optimal or
not? Do we use only ofﬂine data, or only online data? To make
it feasible to learn policies for open-world settings, we need
algorithms that learn successfully in any of these cases.

In this work, we study how to build RL algorithms that
are effective for pre-training from off-policy datasets, but

 
 
 
 
 
 
Figure 2: We study learning policies by ofﬂine learning on a prior dataset D and then ﬁne-tuning with online interaction.
The prior data could be obtained via prior runs of RL, expert demonstrations, or any other source of transitions. Our method,
advantage weighted actor critic (AWAC) is able to learn effectively from ofﬂine data and ﬁne-tune in order to reach expert-level
performance after collecting a limited amount of interaction data. Videos and data are available at awacrl.github.io

also well suited to continuous improvement with online data
collection. We systematically analyze the challenges with
using standard off-policy RL algorithms [18, 30, 2] for this
problem, and introduce a simple actor critic algorithm that
elegantly bridges data-driven pre-training from ofﬂine data and
improvement with online data collection. Our method, which
uses dynamic programming to train a critic but a supervised
learning style update to train a constrained actor, combines
the best of supervised learning and actor-critic algorithms.
Dynamic programming can leverage off-policy data and enable
sample-efﬁcient learning. The simple supervised actor update
implicitly enforces a constraint that mitigates the effects of
distribution shift when learning from ofﬂine data [14, 30],
while avoiding overly conservative updates.

We evaluate our algorithm on a wide variety of robotic
control tasks, using a set of simulated dexterous manipulation
problems as well as three separate real-world robots: drawer
opening with a 7-DoF robotic arm, picking up an object
with a multi-ﬁngered hand, and rotating a valve with a 3-
ﬁngered claw. Our algorithm, Advantage Weighted Actor Critic
(AWAC), is able to quickly learn successful policies for these
challenging tasks, in spite of high dimensional action spaces
and uninformative, sparse reward signals. We show that AWAC
ﬁnetunes much more efﬁciently after ofﬂine pretraining as
compared to prior methods and, given a ﬁxed time budget,
attains signiﬁcantly better performance on the real-world tasks.
Moreover, AWAC can utilize different types of prior data
without any algorithmic changes: demonstrations, suboptimal
data, or random exploration data. The contribution of this
work is not just another RL algorithm, but a systematic study
of what makes ofﬂine pre-training with online ﬁne-tuning
unique compared to the standard RL paradigm, which then
directly motivates a simple algorithm, AWAC, to address
these challenges. We additionally discuss the design decisions
required for applying AWAC as a practical tool for real-world
robotic skill learning.

II. PRELIMINARIES

We use the standard reinforcement learning notation, with
states s, actions a, policy π(a|s), rewards r(s, a), and dy-
namics p(s(cid:48)|s, a). The discounted return is deﬁned as Rt =
(cid:80)T
i=t γir(si, ai), for a discount factor γ and horizon T which
may be inﬁnite. The objective of an RL agent is to maximize
the expected discounted return J(π) = Epπ(τ )[R0] under the
distribution induced by the policy. The optimal policy can

be learned directly (e.g., using policy gradient to estimate
∇J(π) [58]), but this is often ineffective due to high variance of
the estimator. Many algorithms attempt to reduce this variance
by making use of the value function V π(s) = Epπ(τ )[Rt|s],
action-value function Qπ(s, a) = Epπ(τ )[Rt|s, a], or advantage
Aπ(s, a) = Qπ(s, a) − V π(s). The action-value function for
a policy can be written recursively via the Bellman equation:

Qπ(s, a) = r(s, a) + γEp(s(cid:48)|s,a)[V π(s(cid:48))]

= r(s, a) + γEp(s(cid:48)|s,a)[Eπ(a(cid:48)|s(cid:48))[Qπ(s(cid:48), a(cid:48))]].

(1)
(2)

Instead of estimating policy gradients directly, actor-critic
algorithms maximize returns by alternating between two
phases [27]: policy evaluation and policy improvement. During
the policy evaluation phase, the critic Qπ(s, a) is estimated for
the current policy π. This can be accomplished by repeatedly
applying the Bellman operator B, corresponding to the right-
hand side of Equation 2, as deﬁned below:

BπQ(s, a) = r(s, a) + γEp(s(cid:48)|s,a)[Eπ(a(cid:48)|s(cid:48))[Qπ(s(cid:48), a(cid:48))]].

(3)

By iterating according to Qk+1 = BπQk, Qk converges to
Qπ [50]. With function approximation, we cannot apply the
Bellman operator exactly, and instead minimize the Bellman
error with respect to Q-function parameters φk:

φk = arg min

φ

ED[(Qφ(s, a) − y)2],

y = r(s, a) + γEs(cid:48),a(cid:48)[Qφk−1(s(cid:48), a(cid:48))].

(4)

(5)

During policy improvement, the actor π is typically updated
based on the current estimate of Qπ. A commonly used
technique [34, 13, 18] is to update the actor πθk (a|s) via
likelihood ratio or pathwise derivatives to optimize the follow-
ing objective, such that the expected value of the Q-function
Qπ is maximized:

θk = arg max

θ

Es∼D[Eπθ(a|s)[Qφk (s, a)]]

(6)

Actor-critic algorithms are widely used in deep RL [35, 34, 18,
13]. With a Q-function estimator, they can in principle utilize
off-policy data when used with a replay buffer for storing prior
transition tuples, which we will denote β, to sample previous
transitions, although we show that this by itself is insufﬁcient
for our problem setting.

D={(s,a,s0,r)j}⇡✓(a|s)Update⇡✓p(s0|s,a)(s,a,s0,r)Q Update⇡✓Q  -off-policy data -expert demos -prior runs of RL1.O✏ineLearning2.OnlineFine-tuning III. CHALLENGES IN OFFLINE RL WITH ONLINE
FINE-TUNING
In this section, we study the unique challenges that exist
when pre-training using ofﬂine data, followed by ﬁne-tuning
with online data collection. We ﬁrst describe the problem,
and then analyze what makes this problem difﬁcult for prior
methods.

A. Problem Deﬁnition

A static dataset of transitions, D = {(s, a, s(cid:48), r)j}, is pro-
vided to the algorithm at the beginning of training. This dataset
can be sampled from an arbitrary policy or mixture of policies,
and may even be collected by a human expert. This deﬁnition
is general and encompasses many scenarios: learning from
demonstrations, random data, prior RL experiments, or even
from multi-task data. Given the dataset D, our goal is to
leverage D for pre-training and use a small amount of online
interaction to learn the optimal policy π∗(a|s), as depicted in
Fig 2. This setting is representative of many real-world RL
settings, where prior data is available and the aim is to learn new
skills efﬁciently. We ﬁrst study existing algorithms empirically
in this setting on the HalfCheetah-v2 Gym environment1.
The prior dataset consists of 15 demonstrations from an
expert policy and 100 suboptimal trajectories sampled from a
behavioral clone of these demonstrations. All methods for the
remainder of this paper incorporate the prior dataset, unless
explicitly labeled “scratch”.

B. Data Efﬁciency

One of the simplest ways to utilize prior data such as
demonstrations for RL is to pre-train a policy with imitation
learning, and ﬁne-tune with on-policy RL [16, 46]. This
approach has two drawbacks: (1) prior data may not be optimal;
(2) on-policy ﬁne-tuning is data inefﬁcient as it does not reuse
the prior data in the RL stage. In our setting, data efﬁciency is
vital. To this end, we require algorithms that are able to reuse
arbitrary off-policy data during online RL for data-efﬁcient
ﬁne-tuning. We ﬁnd that algorithms that use on-policy ﬁne-
tuning [46, 16], or Monte-Carlo return estimation [42, 55, 41]
are generally much less efﬁcient than off-policy actor-critic
algorithms, which iterate between improving π and estimating
Qπ via Bellman backups. This can be seen from the results in
Figure 3 plot 1, where on-policy methods like DAPG [46] and
Monte-Carlo return methods like AWR [41] and MARWIL [55]
are an order of magnitude slower than off-policy actor-critic
methods. Actor-critic methods, shown in Figure 3 plot 2, can
in principle use off-policy data. However, as we will discuss
next, naïvely applying these algorithms to our problem suffers
from a different set of challenges.

C. Bootstrap Error in Ofﬂine Learning with Actor-Critic
Methods

When standard off-policy actor-critic methods are applied
to this problem setting, they perform poorly, as shown in the

1We use this environment for analysis because it helps understand and
accentuate the differences between different algorithms. More challenging
environments like the ones shown in Fig 4 are too hard to solve to analyze
variants of different methods.

second plot in Figure 3: despite having a prior dataset in the
replay buffer, these algorithms do not beneﬁt signiﬁcantly from
ofﬂine training. We evaluate soft actor critic [18], a state-of-
the-art actor-critic algorithm for continuous control. Note that
“SAC-scratch,” which does not receive the prior data, performs
similarly to “SACfD-prior,” which does have access to the
prior data, indicating that the off-policy RL algorithm is not
actually able to make use of the off-policy data for pre-training.
Moreover, even if the SAC is policy is pre-trained by behavior
cloning, labeled “SACfD-pretrain”, we still observe an initial
decrease in performance, and performance similar to learning
from scratch.

This challenge can be attributed to off-policy bootstrapping
error accumulation, as observed in several prior works [50, 30,
59, 33, 14]. In actor-critic algorithms, the target value Q(s(cid:48), a(cid:48)),
with a(cid:48) ∼ π, is used to update Q(s, a). When a(cid:48) is outside of
the data distribution, Q(s(cid:48), a(cid:48)) will be inaccurate, leading to
accumulation of error on static datasets.

Ofﬂine RL algorithms [14, 30, 59] propose to address
this issue by explicitly adding constraints on the policy
improvement update (Equation 6) to avoid bootstrapping on
out-of-distribution actions, leading to a policy update of this
form:

arg max
θ

Es∼D[Eπθ(a|s)[Qφk (s, a)]] s.t. D(πθ, πβ) ≤ (cid:15).

(7)

Here, πθ is the actor being updated, and πβ(a|s) represents the
(potentially unknown) distribution from which all of the data
seen so far (both ofﬂine data and online data) was generated.
In the case of a replay buffer, πβ corresponds to a mixture
distribution over all past policies. Typically, πβ is not known,
especially for ofﬂine data, and must be estimated from the
data itself. Many ofﬂine RL algorithms [30, 14, 49] explicitly
ﬁt a parametric model to samples for the distribution πβ via
maximum likelihood estimation, where samples from πβ are
obtained simply by sampling uniformly from the data seen thus
Es,a∼πβ [log ˆπβ(a|s)]. After estimating ˆπβ,
far: ˆπβ = maxˆπβ
prior methods implement the constraint given in Equation 7
in various ways, including penalties on the policy update [30,
59] or architecture choices for sampling actions for policy
training [14, 49]. As we will see next, the requirement for
accurate estimation of ˆπβ makes these methods difﬁcult to use
with online ﬁne-tuning.

D. Excessively Conservative Online Learning

While ofﬂine RL algorithms with constraints [30, 14, 59]
perform well ofﬂine, they struggle to improve with ﬁne-tuning,
as shown in the third plot in Figure 3. We see that the purely
ofﬂine RL performance (at “0K” in Fig. 3) is much better
than the standard off-policy methods shown in Section III-C.
However, with additional iterations of online ﬁne-tuning, the
performance increases very slowly (as seen from the slope of
the BEAR curve in Fig 3). What causes this phenomenon?

This can be attributed to challenges in ﬁtting an accurate
behavior model as data is collected online during ﬁne-tuning.
In the ofﬂine setting, behavior models must only be trained
once via maximum likelihood, but in the online setting, the

s
n
r
u
t
e
R

e
g
a
r
e
v
A

Figure 3: Analysis of prior methods on HalfCheetah-v2 using ofﬂine RL with online ﬁne-tuning. (1) On-policy methods (DAPG,
AWR, MARWIL) learn relatively slowly, even with access to prior data. We present our method, AWAC, as an example of how
off-policy RL methods can learn much faster. (2) Variants of soft actor-critic (SAC) with ofﬂine training (performed before
timestep 0) and ﬁne-tuning. We see a “dip” in the initial performance, even if the policy is pretrained with behavioral cloning.
(3) Ofﬂine RL method BEAR [30] on ofﬂine training and ﬁne-tuning, including a “loose” variant of BEAR with a weakened
constraint. Standard ofﬂine RL methods ﬁne-tune slowly, while the “loose” BEAR variant experiences a similar dip as SAC. (4)
We show that the ﬁt of the behavior models ˆπβ used by these ofﬂine methods degrades as new data is added to the buffer
during ﬁne-tuning, potentially explaining their poor ﬁne-tuning performance.

behavior model must be updated online to track incoming data.
Training density models online (in the “streaming” setting) is
a challenging research problem [47], made more difﬁcult by a
potentially complex multi-modal behavior distribution induced
by the mixture of online and ofﬂine data. To understand this,
we plot the log likelihood of learned behavior models on the
dataset during online and ofﬂine training for the HalfCheetah
task. As we can see in the plot, the accuracy of the behavior
models (log πβ on the y-axis) reduces during online ﬁne-tuning,
indicating that it is not ﬁtting the new data well during online
training. When the behavior models are inaccurate or unable
to model new data well, constrained optimization becomes too
conservative, resulting in limited improvement with ﬁne-tuning.
This analysis suggests that, in order to address our problem
setting, we require an off-policy RL algorithm that constrains
the policy to prevent ofﬂine instability and error accumulation,
but not so conservatively that it prevents online ﬁne-tuning
due to imperfect behavior modeling. Our proposed algorithm,
which we discuss in the next section, accomplishes this by
employing an implicit constraint, which does not require any
explicit modeling of the behavior policy.

IV. ADVANTAGE WEIGHTED ACTOR CRITIC: A SIMPLE
ALGORITHM FOR FINE-TUNING FROM OFFLINE DATASETS

In this section, we will describe the advantage weighted
actor-critic (AWAC) algorithm, which trains an off-policy critic
and an actor with an implicit policy constraint. We will show
AWAC mitigates the challenges outlined in Section III. AWAC
follows the design for actor-critic algorithms as described in
Section II, with a policy evaluation step to learn Qπ and a policy
improvement step to update π. AWAC uses off-policy temporal-
difference learning to estimate Qπ in the policy evaluation step,
and a policy improvement update that is able to obtain the
beneﬁts of ofﬂine RL algorithms at training from prior datasets,
while avoiding the overly conservative behavior described in
Section III-D. We describe the policy improvement step in
AWAC below, and then summarize the entire algorithm.

Policy improvement for AWAC proceeds by learning a policy
that maximizes the value of the critic learned in the policy
evaluation step via TD bootstrapping. If done naively, this can
lead to the issues described in Section III-D, but we can avoid

the challenges of bootstrap error accumulation by restricting
the policy distribution to stay close to the data observed thus
far during the actor update, while maximizing the value of the
critic. At iteration k, AWAC therefore optimizes the policy to
maximize the estimated Q-function Qπk (s, a) at every state,
while constraining it to stay close to the actions observed
in the data, similar to prior ofﬂine RL methods, though this
constraint will be enforced differently. Note from the deﬁnition
of the advantage in Section II that optimizing Qπk (s, a) is
equivalent to optimizing Aπk (s, a). We can therefore write this
optimization as:

πk+1 = arg max

π∈Π

Ea∼π(·|s)[Aπk (s, a)]

s.t. DKL(π(·|s)||πβ(·|s)) ≤ (cid:15).

(8)

(9)

As we saw in Section III-C, enforcing the constraint by incor-
porating an explicit learned behavior model [30, 14, 59, 49]
leads to poor ﬁne-tuning performance. Instead, we enforce
the constraint implicitly, without learning a behavior model.
We ﬁrst derive the solution to the constrained optimization
in Equation 8 to obtain a non-parametric closed form for
the actor. This solution is then projected onto the parametric
policy class without any explicit behavior model. The analytic
solution to Equation 8 can be obtained by enforcing the KKT
conditions [42, 45, 41]. The Lagrangian is:

L(π, λ) = Ea∼π(·|s)[Aπk (s, a)] + λ((cid:15) − DKL(π(·|s)||πβ(·|s))),
(10)

and the closed form solution to this problem is π∗(a|s) ∝
λ Aπk (s, a)(cid:1). When using function approxima-
πβ(a|s) exp (cid:0) 1
tors, such as deep neural networks as we do, we need to
project the non-parametric solution into our policy space. For
a policy πθ with parameters θ, this can be done by minimizing
the KL divergence of πθ from the optimal non-parametric
solution π∗ under the data distribution ρπβ (s):

arg min
θ

E
ρπβ (s)

= arg min
θ

E
ρπβ (s)

[DKL(π∗(·|s)||πθ(·|s))]

(cid:20)

E
π∗(·|s)

(cid:21)
[− log πθ(·|s)]

(11)

(12)

0K50K100K150K200KTimesteps02500500075001. Data Efficiency from Prior DataAWAC (Ours)AWR [41]DAPG [46]MARWIL [55]0K50K100K150K200KTimesteps2. Actor-Critic MethodsAWAC (Ours)SACfD-pretrainSACfD-prior [54]SAC-scratch [18]0K50K100K150K200KTimesteps3. Policy Constraint MethodsBEAR [30]BEAR-looseOffline TrainingOffline dataOnline dataNote that the parametric policy could be projected with either
direction of KL divergence. Choosing the reverse KL results
in explicit penalty methods [59] that rely on evaluating the
density of a learned behavior model. Instead, by using forward
KL, we can compute the policy update by sampling directly
from β:

θk+1 = arg max

θ

E
s,a∼β

(cid:20)
log πθ(a|s) exp

Aπk (s, a)

(cid:19)(cid:21)

.

(cid:18) 1
λ

(13)

This actor update amounts to weighted maximum likelihood
(i.e., supervised learning), where the targets are obtained by re-
weighting the state-action pairs observed in the current dataset
by the predicted advantages from the learned critic, without
explicitly learning any parametric behavior model, simply
sampling (s, a) from the replay buffer β. See Appendix A
for a more detailed derivation and Appendix B for speciﬁc
implementation details.
Avoiding explicit behavior modeling. Note that the update in
Equation 13 completely avoids any modeling of the previously
observed data β with a parametric model. By avoiding any
explicit learning of the behavior model AWAC is far less
conservative than methods which ﬁt a model ˆπβ explicitly, and
better incorporates new data during online ﬁne-tuning, as seen
from our results in Section VI. This derivation is related to
AWR [41], with the main difference that AWAC uses an off-
policy Q-function Qπ to estimate the advantage, which greatly
improves efﬁciency and even ﬁnal performance (see results in
Section VI-A1). The update also resembles ABM-MPO, but
ABM-MPO does require modeling the behavior policy which,
as discussed in Section III-D, can lead to poor ﬁne-tuning. In
Section VI-A1, AWAC outperforms ABM-MPO on a range of
challenging tasks.
Policy evaluation. During policy evaluation, we estimate the
action-value Qπ(s, a) for the current policy π, as described in
Section II. We utilize a temporal difference learning scheme
for policy evaluation [18, 13], minimizing the Bellman error
as described in Equation 3. This enables us to learn very
efﬁciently from off-policy data. This is particularly important
in our problem setting to effectively use the ofﬂine dataset,
and allows us to signiﬁcantly outperform alternatives using
Monte-Carlo evaluation or TD(λ) to estimate returns [41].

Algorithm 1 Advantage Weighted Actor Critic (AWAC)

1: Dataset D = {(s, a, s(cid:48), r)j}
2: Initialize buffer β = D
3: Initialize πθ, Qφ
4: for iteration i = 1, 2, ... do
5:
6:
7:
8:
9:
10:
11:
end if
12:
13: end for

τ1, . . . , τK ∼ pπθ (τ )
β ← β ∪ {τ1, . . . , τK}

Sample batch (s, a, s(cid:48), r) ∼ β
y = r(s, a) + γEs(cid:48),a(cid:48)[Qφk−1(s(cid:48), a(cid:48))]
φ ← arg minφ ED[(Qφ(s, a) − y)2]
Es,a∼β
θ ← arg maxθ
if i > num_ofﬂine_steps then

(cid:2)log πθ(a|s) exp (cid:0) 1

λ Aπk (s, a)(cid:1)(cid:3)

Algorithm summary. The full AWAC algorithm for ofﬂine
RL with online ﬁne-tuning is summarized in Algorithm 1. In
a practical implementation, we can parameterize the actor and
the critic by neural networks and perform SGD updates from
Eqn. 13 and Eqn. 4. Speciﬁc details are provided in Appendix B.
AWAC ensures data efﬁciency with off-policy critic estimation
via bootstrapping, and avoids ofﬂine bootstrap error with a
constrained actor update. By avoiding explicit modeling of the
behavior policy, AWAC avoids overly conservative updates.

While AWAC is related to several prior RL algorithms, we
note that there are key differences that make it particularly
amenable to the problem setting we are considering – ofﬂine
RL with online ﬁne-tuning – that other methods are unable
to tackle. As we show in our experimental analysis with
direct comparisons to prior work, every one of the design
decisions being made in this work are important for algo-
rithm performance. As compared to AWR [41], AWAC uses
TD bootstrapping for signiﬁcantly more efﬁcient and even
asymptotically better performance. As compared to ofﬂine RL
techniques like ABM [49], MPO [2], BEAR [30] or BCQ [14]
this work is able to avoid the need for any behavior modeling,
thereby enabling the online ﬁne-tuning part of the problem
much better. As shown in Fig 4, when these seemingly ablations
are made to AWAC, the algorithm performs signiﬁcantly worse.

V. RELATED WORK

Off-policy RL algorithms are designed to reuse off-policy
data during training, and have been studied extensively [27, 9,
35, 18, 13, 8, 43, 62, 57, 6]. While standard off-policy methods
are able to beneﬁt from including data seen during a training
run, as we show in Section III-C they struggle when training
from previously collected ofﬂine data from other policies, due
to error accumulation with distribution shift [14, 30]. Ofﬂine
RL methods aim to address this issue, often by constraining
the actor updates to avoid excessive deviation from the data
distribution [32, 52, 20, 21, 19, 3, 30, 14, 11, 37, 49, 33, 61].
One class of these methods utilize importance sampling [52,
61, 37, 9, 24, 19]. Another class of methods perform ofﬂine
reinforcement learning via dynamic programming, with an
explicit constraint to prevent deviation from the data distribution
[32, 30, 14, 59, 23]. While these algorithms perform well
in the purely ofﬂine settings, we show in Section III-D that
such methods tend to be overly conservative, and therefore
may not learn efﬁciently when ﬁne-tuning with online data
collection. In contrast, our algorithm AWAC is comparable to
these algorithms for ofﬂine pre-training, but learns much more
efﬁciently during subsequent ﬁne-tuning.

Prior work has also considered the special case of learning
from demonstration data. One class of algorithms initializes
the policy via behavioral cloning from demonstrations, and
then ﬁne-tunes with reinforcement learning [44, 22, 51, 25,
46, 16, 64]. Most such methods use on-policy ﬁne-tuning,
which is less sample-efﬁcient than off-policy methods that
perform value function estimation. Other prior works have
incorporated demonstration data into the replay buffer using
off-policy RL methods [54, 38]. We show in Section III-C that
these strategies can result in a large dip in performance during

e
t
a
R

s
s
e
c
c
u
S

Figure 4: Comparative evaluation on the dexterous manipulation tasks. These tasks are difﬁcult due to their high action
dimensionality and reward sparsity. We see that AWAC is able to learn these tasks with little online data collection required
(100K samples ≈ 16 minutes of equivalent real-world interaction time). Meanwhile, most prior methods are not able to solve
the harder two tasks: door opening and object relocation.

online ﬁne-tuning, due to the inability to pre-train an effective
value function from ofﬂine data. In contrast, our work shows
that using supervised learning style policy updates can allow
for better bootstrapping from demonstrations as compared to
Veˇcerík et al. [54] and Nair et al. [38].

Our method builds on algorithms that implement a maximum
likelihood objective for the actor, based on an expectation-
maximization formulation of RL [42, 40, 51, 45, 41, 2, 55].
Most closely related to our method in this respect are the
algorithms proposed by Peng et al. [41] (AWR) and Siegel et al.
[49] (ABM). Unlike AWR, which estimates the value function
of the behavior policy, V πβ via Monte-Carlo estimation
or TD−λ, our algorithm estimates the Q-function of the
current policy Qπ via bootstrapping, enabling much more
efﬁcient learning, as shown in our experiments. Unlike ABM,
our method does not require learning a separate function
approximator to model the behavior policy πβ, and instead
directly samples the dataset. As we discussed in Section III-D,
modeling πβ can be a major challenge for online ﬁne-tuning.
While these distinctions may seem somewhat subtle, they are
important and we show in our experiments that they result
in a large difference in algorithm performance. Finally, our
work goes beyond the analysis in prior work, by studying
the issues associated with pre-training and ﬁne-tuning in
Section III. Closely to our work, Wang et al. [56] proposed
critic regularized regression for ofﬂine RL, which uses off-
policy Q-learning and an equivalent policy update. In contrast
to this concurrent work, we speciﬁcally study the ofﬂine
pretraining online ﬁne-tuning problem, which this prior work
does not address, analyze why other methods are ineffective
in this setting, and show that our approach enables strong
ﬁne-tuning results on challenging dextrous manipulation tasks
and real-world robotic systems.

The idea of bootstrapping learning from prior data for real-
world robotic learning is not a new one; in fact, it has been
extensively explored in the context of providing initial rollouts
to bootstrap policy search [26, 44, 28], initializing dynamic
motion primitives [7, 28, 36] in the context of on-policy
reinforcement learning algorithms [46, 64], inferring reward
shaping [60] and even for inferring reward functions [65, 1].
Our work shows how we can generalize the idea of boot-

Figure 5: Illustration of dexterous manipulation tasks in simu-
lation. These tasks exhibit sparse rewards, high-dimensional
control, and complex contact physics.

strapping robotic learning from prior data to include arbitrary
sub-optimal data rather than just demonstration data and shows
the ability to continue improving beyond this data as well.

VI. EXPERIMENTAL EVALUATION

In our experimental evaluation we aim to answer the

following question:

1) Does AWAC effectively combine prior data with online
experience to learn complex robotic control tasks more
efﬁciently than prior methods?

2) Is AWAC able to learn from sub-optimal or random data?
3) Does AWAC provide a practical way to bootstrap real-

world robotic reinforcement learning?

In the following sections, we study these questions using several
challenging and high-dimensional simulated robotic tasks, as
well as three separate real-world robotic platforms. Videos of
all experiments are available at awacrl.github.io

A. Simulated Experiments

We study the ﬁrst two questions in challenging simulation

environments.

1) Comparative Evaluation When Bootstrapping From Prior
Data: We study tasks in simulation that have signiﬁcant
exploration challenges, where ofﬂine learning and online ﬁne-
tuning are likely to be effective. We begin our analysis with
a set of challenging sparse reward dexterous manipulation
tasks proposed by Rajeswaran et al. [46] in simulation. These
tasks involve complex manipulation skills using a 28-DoF ﬁve-
ﬁngered hand in the MuJoCo simulator [53] shown in Figure 4:
in-hand rotation of a pen, opening a door by unlatching the
handle, and picking up a sphere and relocating it to a target

0K200K400K600K800KTimesteps0.00.20.40.60.81.0pen-binary-v00K200K400K600K800KTimesteps0.00.20.40.60.81.0door-binary-v00M1M2M3M4MTimesteps0.00.20.40.60.81.0relocate-binary-v0AWAC (Ours)ABM [49]AWR [41]MARWIL [55]BEAR [30]BRAC [59]DAPG [46]SACfD [54]SAC+BC [39]location. The reward functions in these environments are binary
0-1 rewards for task completion. 2 Rajeswaran et al. [46]
provide 25 human demonstrations for each task, which are
not fully optimal but do solve the task. Since this dataset is
small, we generated another 500 trajectories of interaction data
by constructing a behavioral cloned policy, and then sampling
from this policy.

First, we compare our method on these dexterous manipula-
tion tasks against prior methods for off-policy learning, ofﬂine
learning, and bootstrapping from demonstrations. Speciﬁc
implementation details are discussed in Appendix D. The results
are shown in Fig. 4. Our method is able to leverage the prior
data to quickly attain good performance, and the efﬁcient off-
policy actor-critic component of our approach ﬁne-tunes much
more quickly than demonstration augmented policy gradient
(DAPG), the method proposed by Rajeswaran et al. [46]. For
example, our method solves the pen task in 120K timesteps,
the equivalent of just 20 minutes of online interaction. While
the baseline comparisons and ablations make some amount of
progress on the pen task, alternative off-policy RL and ofﬂine
RL algorithms are largely unable to solve the door and relocate
task in the time-frame considered. We ﬁnd that the design
decisions to use off-policy critic estimation allow AWAC to
outperform AWR [41] while the implicit behavior modeling
allows AWAC to signiﬁcantly outperform ABM [49], although
ABM does make some progress. Rajeswaran et al. [46] show
that DAPG can solve variants of these tasks with more well-
shaped rewards, but requires considerably more samples.

Additionally, we evaluated all methods on the Gym MuJoCo
locomotion benchmarks, similarly providing demonstrations as
ofﬂine data. The results plots for these experiments are included
in Appendix E in the supplementary materials. These tasks are
substantially easier than the sparse reward manipulation tasks
described above, and a number of prior methods also perform
well. SAC+BC and BRAC perform on par with our method
on the HalfCheetah task, and ABM performs on par with our
method on the Ant task, while our method outperforms all
others on the Walker2D task. However, our method matches or
exceeds the best prior method in all cases, whereas no other
single prior method attains good performance on all tasks.

Figure 7: Comparison of ﬁne-tuning from an initial dataset of
suboptimal data on a Sawyer robot pushing task.

2Rajeswaran et al. [46] use a combination of task completion factors as the
sparse reward. For instance, in the door task, the sparse reward as a function
of the door position d was r = 101d>1.35 + 81d>1.0 + 21d>1.2 − 0.1||d −
1.57||2. We only use the fully sparse success measure r = 1d>1.4, which is
substantially more difﬁcult.

2) Fine-Tuning from Random Policy Data: An advantage of
using off-policy RL for reinforcement learning is that we can
also incorporate suboptimal data, rather than demonstrations.
In this experiment, we evaluate on a simulated tabletop pushing
environment with a Sawyer robot pictured in Fig 4 and
described further in Appendix C. To study the potential to
learn from suboptimal data, we use an off-policy dataset of
500 trajectories generated by a random process. The task is
to push an object to a target location in a 40cm x 20cm goal
space. The results are shown in Figure 7. We see that while
many methods begin at the same initial performance, AWAC
learns the fastest online and is actually able to make use of
the ofﬂine dataset effectively.

B. Real-World Robot Learning with Prior Data

We next evaluate AWAC and several baselines on a range of
real-world robotic systems, shown in the top row of Fig 6. We
study the following tasks: rotating a valve with a 3-ﬁngered
claw, repositioning an object with a 4-ﬁngered hand, and
opening a drawer with a Sawyer robotic arm. The dexterous
manipulation tasks involve ﬁne ﬁnger coordination to properly
reorient and reposition objects, as well as high dimensional state
and action spaces. The Sawyer drawer opening task requires
accurate arm movements to properly hook the end-effector into
the handle of the drawer. To ensure continuous operation, all
environments are ﬁtted with an automated reset mechanism
that executes before each trajectory is collected, allowing us to
run real-world experiments without human supervision. Since
real-world experiments are signiﬁcantly more time-consuming,
we could not compare to the full range of prior methods in
the real world, but we include comparisons with the following
methods: direct behavioral cloning (BC) of the provided data
(which is reasonable in these settings, since the prior data
includes demonstrations), off-policy RL with soft actor-critic
(SAC) [18], where the prior data is included in the replay buffer
and used to pretrain the policy (which refer to as SACfD), and
a modiﬁed version of SAC that includes an added behavioral
cloning loss (SAC+BC), which is analogous to Nair et al. [39]
or an off-policy version of Rajeswaran et al. [46]. Further
implementation details of these algorithms are provided in
Appendix D in the supplementary materials.

Next, we describe the experimental setup for hardware
experiments. Precise details of the hardware setup can be
found in Appendix H in the supplementary materials.
Dexterous Manipulation with a 3-Fingered Claw. This
task requires controlling a 3-ﬁngered, 9 DoF robotic hand,
introduced by Ahn et al. [4], to rotate a 4-pronged valve object
by 180 degrees. To properly perform this task, multiple ﬁngers
need to coordinate to stably and efﬁciently rotate the valve
into the desired orientation. The state space of the system
consists of the joint angles of all the 9 joints in the claw,
and the action space consists of the joint positions of the
ﬁngers, which are followed by the robot using a low-level PID
controller. The reward for this task is sparse: −1 if the valve
is rotated within 0.25 radians of the target, and 0 otherwise.
Note that this reward function is signiﬁcantly more difﬁcult
than the dense, well-shaped reward function typically used

0K20K40K60K80K100KTimesteps0.00.20.40.60.81.0Success RateLearning From Random DataAWACBEARBRACABMSACSAC+BCn
r
u
t
e
R

e
g
a
r
e
v
A

Figure 6: Algorithm comparison on three real-world robotic environments. Images of real world robotic tasks are pictured
above. Left: a three ﬁngered D’claw must rotate a valve 180◦. Middle: a Sawyer robot must slide open a drawer using a hook
attachment. Right: a dexterous hand attached to a Sawyer robot must re-position an object to to the center of the table. On each
task, AWAC trained ofﬂine achieves reasonable performance (shown at timestep 0) and then steadily improves from online
interaction. Other methods, which also all have access to prior data, fail to utilize the prior data effectively ofﬂine and therefore
exhibit slow or no online improvement. Videos of all experiments are available at awacrl.github.io

in prior work [4]. The prior data consists of 10 trajectories
collected using kinesthetic teaching, combined this with 200
trajectories obtained through executing a policy trained via
imitation learning in the environment.
Drawer Opening with a Sawyer Arm. This task requires
controlling a Sawyer arm to slide open a drawer. The robot
uses 3-dimensional end-effector control, and is equipped with
a hook attachment to make the drawer opening possible. The
state space is 4-dimensional, consisting of the position of
the robot end-effector and the linear position of the drawer,
measured using an encoder. The reward is sparse: −1 if the
drawer is open beyond a threshold and 0 otherwise. For this
task, the prior data consists of 10 demonstration trajectories
collected using via teleoperation with a 3D mouse, as well as
500 trajectories obtained through executing a policy trained via
imitation learning in the environment. This task is challenging
because it requires very precise insertion of the hook attachment
into the opening, as pictured in Fig 6, before the robot can
open the drawer. Due to the sparse reward, making learning
progress on this task requires utilizing prior data to construct
an initial policy that at least sometimes succeeds.
Dexterous Manipulation with a Robotic Hand. This task
requires controlling a 4-ﬁngered robotic hand mounted on a
Sawyer robotic arm to reposition an object [17]. The task
requires careful coordination between the hand and the arm
to manipulate the object accurately. The reward for this task
is a combination of the negative distance between the hand
and the object and the negative distance between the object
and the target. The actions are 19-dimensional, consisting of
16-dimensional ﬁnger control and 3-dimensional end effector
control of the arm. For this task, the prior data of 19 trajectories
were collected using kinesthetic teaching and combined with
50 trajectories obtained by executing a policy trained with
imitation learning on this data.

The results on these tasks are shown in Figure 6. We ﬁrst
see that AWAC attains performance that is comparable to the
best prior method from ofﬂine training alone, as indicated by
the value at time step 0 (which corresponds to the beginning of

online ﬁnetuning). This means that, during online interaction,
AWAC collects data that is of higher quality, and improves
more quickly. The prior methods struggle to improve from
online training on these tasks, likely because the sparse reward
function and challenging dynamics make progress very difﬁcult
from a bad initialization. These results suggest that AWAC
is effectively able to leverage prior data to bootstrap online
reinforcement learning in the real world, even on tasks with
difﬁcult and uninformative reward functions.

VII. DISCUSSION AND FUTURE WORK

We have discussed the challenges existing RL methods
face when ﬁne-tuning from prior datasets, and proposed an
algorithm, AWAC, for this setting. The key insight in AWAC is
that an implicitly constrained actor-critic algorithm is able to
both train ofﬂine and continue to improve with more experience.
We provide detailed empirical analysis of the design decisions
behind AWAC, showing the importance of off-policy learning,
bootstrapping and the particular form of implicit constraint
enforcement. To validate these ideas, we evaluate on a variety
of simulated and real world robotic problems.

While AWAC is able to effectively leverage prior data
for signiﬁcantly accelerating learning, it does run into some
limitations. Firstly, it can be challenging to choose the appro-
priate threshold for constrained optimization. Resolving this
would involve exploring adaptive threshold tuning schemes.
Secondly, while AWAC is able to avoid over-conservative
behavior empirically, in future work, we hope to analyze
theoretical factors that go into building a good ﬁnetuning
algorithm. And lastly, in the future we plan on applying AWAC
to more broadly incorporate data across different robots, labs
and tasks rather than just on isolated setups. By doing so, we
hope to enable an even wider array of robotic applications.

VIII. ACKNOWLEDGEMENTS

This research was supported by the Ofﬁce of Naval Research,
the National Science Foundation through IIS-1700696 and IIS-
1651843, and ARL DCIST CRA W911NF-17-2-0181. We

5000100001500020000Timesteps100806040RobelTurnFixed-v010000200003000040000Timesteps100959085SawyerDrawerOpening-v001000020000300004000050000Timesteps225200175150125100SawyerDHandReposition-v0AWAC (Ours)SACfD [39]SAC+BC [54]BCwould like to thank Aviral Kumar, Ignasi Clavera, Karol
Hausman, Oleh Rybkin, Michael Chang, Corey Lynch, Kamyar
Ghasemipour, Alex Irpan, Vitchyr Pong, Graham Hughes,
Zihao Zhao, Vikash Kumar, Saurabh Gupta, Shubham Tulsiani,
Abhinav Gupta and many others at UC Berkeley RAIL Lab
and Robotics at Google for their valuable feedback on the
paper and insightful discussions.

REFERENCES
[1] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning
In International
via inverse reinforcement learning.
Conference on Machine Learning (ICML), pp. 1, 2004.
[2] Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval
Tassa, Remi Munos, Nicolas Heess, and Martin Ried-
miller. Maximum a Posteriori Policy Optimisation. In
International Conference on Learning Representations
(ICLR), pp. 1–19, 2018.

[3] Rishabh Agarwal, Dale Schuurmans, and Mohammad
Norouzi. An Optimistic Perspective on Ofﬂine Reinforce-
ment Learning. In International Conference on Machine
Learning (ICML), 2019.

[4] Michael Ahn, Henry Zhu, Kristian Hartikainen, Hugo
Ponte, Abhishek Gupta, Sergey Levine, and Vikash Kumar.
ROBEL: Robotics Benchmarks for Learning with Low-
Cost Robots. In Conference on Robot Learning (CoRL).
arXiv, 2019.

[5] Christopher G Atkeson and Stefan Schaal. Robot Learning
In International Conference on

From Demonstration.
Machine Learning (ICML), 1997.

[6] David Balduzzi and Muhammad Ghifary. Compatible
value gradients for reinforcement learning of continuous
deep policies. CoRR, abs/1509.03005, 2015.

[7] Darrin C. Bentivegna, Gordon Cheng, and Christopher G.
Atkeson. Learning from observation and from practice
using behavioral primitives.
In Paolo Dario and Raja
Chatila (eds.), Robotics Research, The Eleventh Inter-
national Symposium, ISRR, October 19-22, 2003, Siena,
Italy, volume 15 of Springer Tracts in Advanced Robotics,
pp. 551–560. Springer, 2003. doi: 10.1007/11008941\_59.
[8] Shalabh Bhatnagar, Richard S. Sutton, Mohammad
Ghavamzadeh, and Mark Lee. Natural actor-critic
algorithms. Autom., 45(11):2471–2482, 2009.
doi:
10.1016/j.automatica.2009.07.008.

[9] Thomas Degris, Martha White, and Richard S. Sutton.
Off-Policy Actor-Critic. In International Conference on
Machine Learning (ICML), 2012.

[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding. In Association
for Compuational Linguistics (ACL), 2019.

[11] Rasool Fakoor, Pratik Chaudhari, and Alexander J Smola.
P3O: Policy-on Policy-off Policy Optimization.
In
Conference on Uncertainty in Artiﬁcial Intelligence (UAI),
2019.

[12] Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker,
and Sergey Levine. D4RL: Datasets for Deep Data-Driven
Reinforcement Learning. 2020.

[13] Scott Fujimoto, Herke van Hoof, and David Meger.
Addressing Function Approximation Error in Actor-Critic
Methods. International Conference on Machine Learning
(ICML), 2018.

[14] Scott Fujimoto, David Meger, and Doina Precup. Off-
Policy Deep Reinforcement Learning without Exploration.
In International Conference on Machine Learning (ICML),
2019.

[15] Yang Gao, Huazhe Xu, Ji Lin, Fisher Yu, Sergey Levine,
and Trevor Darrell. Reinforcement learning from imper-
fect demonstrations. CoRR, abs/1802.05313, 2018.
[16] Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey
Levine, and Karol Hausman. Relay Policy Learning: Solv-
ing Long-Horizon Tasks via Imitation and Reinforcement
In Conference on Robot Learning (CoRL),
Learning.
2019.

[17] Abhishek Gupta, Justin Yu, Tony Zhao, Vikash Kumar,
Kelvin Xu, Thomas Devlin, Aaron Rovinsky, and Sergey
Levine. Reset-Free Reinforcement Learning via Multi-
Task Learning: Learning Dexterous Manipulation Be-
In International
haviors without Human Intervention.
Conference on Robotics and Automation (ICRA), 2021.
[18] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey
Levine. Soft Actor-Critic: Off-Policy Maximum Entropy
Deep Reinforcement Learning with a Stochastic Actor.
In International Conference on Machine Learning, 2018.
[19] Assaf Hallak and Shie Mannor. Consistent On-Line
Off-Policy Evaluation. In International Conference on
Machine Learning (ICML), 2017.

[20] Assaf Hallak, Francois Schnitzler, Timothy Mann, and
Shie Mannor. Off-policy Model-based Learning under Un-
known Factored Dynamics. In International Conference
on Machine Learning (ICML), 2015.

[21] Assaf Hallak, Aviv Tamar, Rémi Munos, and Shie
Mannor. Generalized Emphatic Temporal Difference
In Association for
Learning: Bias-Variance Analysis.
the Advancement of Artiﬁcial Intelligence (AAAI), 2016.
[22] Auke Jan Ijspeert, Jun Nakanishi, and Stefan Schaal.
Learning Attractor Landscapes for Learning Motor Prim-
In Advances in Neural Information Processing
itives.
Systems (NIPS), pp. 1547–1554, 2002. ISBN 1049-5258.
[23] Natasha Jaques, Asma Ghandeharioun, Judy Hanwen
Shen, Craig Ferguson, Àgata Lapedriza, Noah Jones,
Shixiang Gu, and Rosalind W. Picard. Way off-policy
batch deep reinforcement learning of implicit human
preferences in dialog. CoRR, abs/1907.00456, 2019.
[24] Nan Jiang and Lihong Li. Doubly Robust Off-policy Value
Evaluation for Reinforcement Learning. In International
Conference on Machine Learning (ICML), 2016.

[25] Beomjoon Kim, Amir-Massoud Farahmand, Joelle Pineau,
and Doina Precup. Learning from Limited Demonstrations.
In Advances in Neural Information Processing Systems
(NIPS), 2013.

[26] Jens Kober and J. Peter. Policy search for motor primitives
in robotics. In Advances in Neural Information Processing
Systems (NIPS), volume 97, 2008.

[27] Vijay R Konda and John N Tsitsiklis. Actor-Critic Algo-

rithms. In Advances in Neural Information Processing
Systems (NeurIPS), 2000.

[28] Petar Kormushev, Sylvain Calinon, and Darwin G. Cald-
well. Robot motor skill coordination with em-based
reinforcement learning. In 2010 IEEE/RSJ International
Conference on Intelligent Robots and Systems, October
18-22, 2010, Taipei, Taiwan, pp. 3232–3237. IEEE, 2010.
doi: 10.1109/IROS.2010.5649089.

[29] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural
networks. In Advances in Neural Information Processing
Systems (NIPS), pp. 1097–1105, 2012.

[30] Aviral Kumar, Justin Fu, George Tucker, and Sergey
Levine. Stabilizing Off-Policy Q-Learning via Bootstrap-
ping Error Reduction. In Neural Information Processing
Systems (NeurIPS), 2019.

[31] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey
Levine. Conservative Q-Learning for Ofﬂine Reinforce-
In Advances in Neural Information
ment Learning.
Processing Systems (NeurIPS), 2020.

[32] Sascha Lange, Thomas Gabel, and Martin A. Riedmiller.
Batch reinforcement learning. In Marco Wiering and Mar-
tijn van Otterlo (eds.), Reinforcement Learning, volume 12
of Adaptation, Learning, and Optimization, pp. 45–73.
Springer, 2012. doi: 10.1007/978-3-642-27645-3\_2.
[33] Sergey Levine, Aviral Kumar, George Tucker, and Justin
Fu. Ofﬂine Reinforcement Learning: Tutorial, Review,
and Perspectives on Open Problems. Technical report,
2020.

[34] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel,
Nicolas Heess, Tom Erez, Yuval Tassa, David Silver,
and Daan Wierstra. Continuous control with deep
reinforcement learning. In International Conference on
Learning Representations (ICLR), 2016. ISBN 0-7803-
3213-X. doi: 10.1613/jair.301.

[35] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi
Mirza, Alex Graves, Tim Harley, Timothy P Lillicrap,
David Silver, and Koray Kavukcuoglu. Asynchronous
Methods for Deep Reinforcement Learning. In Interna-
tional Conference on Machine Learning (ICML), 2016.
[36] Katharina Mülling, Jens Kober, Oliver Kroemer, and
Jan Peters. Learning to select and generalize striking
movements in robot table tennis. Int. J. Robotics Res.,
32(3):263–279, 2013. doi: 10.1177/0278364912472380.
[37] Oﬁr Nachum, Yinlam Chow, Bo Dai, and Lihong Li.
DualDICE: Behavior-Agnostic Estimation of Discounted
Stationary Distribution Corrections. In Advances in Neural
Information Processing Systems (NeurIPS), 2019.
[38] Ashvin Nair, Dian Chen, Pulkit Agrawal, Phillip Isola,
Pieter Abbeel, Jitendra Malik, Sergey Levine, Dian
Chen, Phillip Isola, Pieter Abbeel, Jitendra Malik, and
Sergey Levine. Combining Self-Supervised Learning
and Imitation for Vision-Based Rope Manipulation. In
IEEE International Conference on Robotics and Au-
tomation (ICRA), 2017.
ISBN 9781509046331. doi:
10.1109/ICRA.2017.7989247.

[39] Ashvin Nair, Bob Mcgrew, Marcin Andrychowicz, Woj-

ciech Zaremba, and Pieter Abbeel. Overcoming Explo-
ration in Reinforcement Learning with Demonstrations.
In IEEE International Conference on Robotics and
Automation (ICRA), 2018.

[40] Gerhard Neumann and Jan Peters. Fitted Q-iteration by
Advantage Weighted Regression. In Advances in Neural
Information Processing Systems (NeurIPS), 2008.
[41] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey
Levine. Advantage-Weighted Regression: Simple and
Scalable Off-Policy Reinforcement Learning. 2019.
[42] Jan Peters and Stefan Schaal. Reinforcement Learning by
Reward-weighted Regression for Operational Space Con-
trol. In International Conference on Machine Learning,
2007.

[43] Jan Peters and Stefan Schaal. Natural actor-critic.
Neurocomputing, 71(7-9):1180–1190, 2008. doi: 10.1016/
j.neucom.2007.11.026.

[44] Jan Peters and Stefan Schaal. Reinforcement learning
of motor skills with policy gradients. Neural Networks,
21(4):682–697, 2008. ISSN 08936080. doi: 10.1016/j.
neunet.2008.02.003.

[45] Jan Peters, Katharina Mülling, and Yasemin Altün. Rel-
In AAAI Conference on

ative Entropy Policy Search.
Artiﬁcial Intelligence, pp. 1607–1612, 2010.

[46] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta,
John Schulman, Emanuel Todorov, and Sergey Levine.
Learning Complex Dexterous Manipulation with Deep
Reinforcement Learning and Demonstrations. In Robotics:
Science and Systems, 2018.

[47] Jason Ramapuram, Magda Gregorova, and Alexandros
Kalousis. Lifelong Generative Modeling. Neurocomputing,
2017.

[48] Stefan Schaal. Learning from demonstration. In Advances
in Neural Information Processing Systems (NeurIPS),
number 9, pp. 1040–1046, 1997.
ISBN 1558604863.
doi: 10.1016/j.robot.2004.03.001.

[49] Noah Y. Siegel,

Jost Tobias Springenberg, Felix
Berkenkamp, Abbas Abdolmaleki, Michael Neunert,
Thomas Lampe, Roland Hafner, Nicolas Heess, and
Martin Riedmiller. Keep doing what worked: Behavioral
modelling priors for ofﬂine reinforcement learning, 2020.
[50] Richard S Sutton and Andrew G Barto. Reinforcement

Learning: An Introduction. 1998.

[51] Evangelos A Theodorou, Jonas Buchli, and Stefan Schaal.
A Generalized Path Integral Control Approach to Re-
Journal of Machine Learning
inforcement Learning.
Research (JMLR), 11:3137–3181, 2010.

[52] Philip S. Thomas and Emma Brunskill. Data-efﬁcient
off-policy policy evaluation for reinforcement learning.
In Maria-Florina Balcan and Kilian Q. Weinberger (eds.),
Proceedings of the 33nd International Conference on
Machine Learning, ICML 2016, New York City, NY,
USA, June 19-24, 2016, volume 48 of JMLR Workshop
and Conference Proceedings, pp. 2139–2148. JMLR.org,
2016.

[53] Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo:
A physics engine for model-based control. In IEEE/RSJ

International Conference on Intelligent Robots and Sys-
tems (IROS), pp. 5026–5033, 2012. ISBN 9781467317375.
doi: 10.1109/IROS.2012.6386109.

[54] Matej Veˇcerík, Todd Hester, Jonathan Scholz, Fumin
Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas
Rothörl, Thomas Lampe, and Martin Riedmiller. Leverag-
ing Demonstrations for Deep Reinforcement Learning
on Robotics Problems with Sparse Rewards. CoRR,
abs/1707.0, 2017.

[55] Qing Wang, Jiechao Xiong, Lei Han, Peng Sun, Han Liu,
and Tong Zhang. Exponentially Weighted Imitation Learn-
ing for Batched Historical Data. In Neural Information
Processing Systems (NeurIPS), 2018.

[56] Ziyu Wang, Alexander Novikov, Konrad Zołna, Jost To-
bias Springenberg, Scott Reed, Bobak Shahriari, Noah
Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, and
Nando De Freitas. Critic Regularized Regression. 2020.
[57] Pawel Wawrzynski. Real-time reinforcement learning
by sequential actor-critics and experience replay. Neural
Networks, 22(10):1484–1497, 2009. doi: 10.1016/j.neunet.
2009.05.011.

[58] Ronald J Williams. Simple Statistical Gradient-Following
Algorithms for Connectionist Reinforcement Learning.
Machine Learning, pp. 229–256, 1992.

[59] Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior
Regularized Ofﬂine Reinforcement Learning. 2020.
[60] Yuchen Wu, Melissa Moziﬁan, and Florian Shkurti.
Shaping rewards for reinforcement learning with imperfect
demonstrations using generative models, 2020.

[61] Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans.
GenDICE: Generalized Ofﬂine Estimation of Stationary
Values. In International Conference on Learning Repre-
sentations (ICLR), 2020.

[62] Shangtong Zhang, Wendelin Boehmer, and Shimon White-
son. Generalized off-policy actor-critic. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. dÁlché-Buc, E. Fox,
and R. Garnett (eds.), Advances in Neural Information
Processing Systems 32, pp. 2001–2011. Curran Associates,
Inc., 2019.

[63] Allan Zhou, Eric Jang, Daniel Kappler, Alexander Her-
zog, Mohi Khansari, Paul Wohlhart, Yunfei Bai, Mrinal
Kalakrishnan, Sergey Levine, and Chelsea Finn. Watch,
try, learn: Meta-learning from demonstrations and reward.
CoRR, abs/1906.03352, 2019.

[64] Henry Zhu, Abhishek Gupta, Aravind Rajeswaran, Sergey
Levine, and Vikash Kumar. Dexterous Manipulation with
Deep Reinforcement Learning: Efﬁcient, General, and
In Proceedings - IEEE International Con-
Low-Cost.
ference on Robotics and Automation, volume 2019-May,
pp. 3651–3657. Institute of Electrical and Electronics
Engineers Inc., 2019.

[65] Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and
Anind K Dey. Maximum Entropy Inverse Reinforcement
Learning. In AAAI Conference on Artiﬁcial Intelligence,
pp. 1433–1438, 2008. ISBN 9781577353683 (ISBN).

A. Algorithm Derivation Details

APPENDIX

The full optimization problem we solve, given the previous
off-policy advantage estimate Aπk and buffer distribution πβ,
is given below:

πk+1 = arg max

π∈Π

Ea∼π(·|s)[Aπk (s, a)]

s.t. DKL(π(·|s)||πβ(·|s)) ≤ (cid:15)

(cid:90)

π(a|s)da = 1.

(14)

(15)

(16)

a

Our derivation follows Peters et al. [45] and Peng et al. [41].
The analytic solution for the constrained optimization problem
above can be obtained by enforcing the KKT conditions. The
Lagrangian is:

L(π, λ, α) =Ea∼π(·|s)[Aπk (s, a)]

+ λ((cid:15) − DKL(π(·|s)||πβ(·|s)))

(cid:90)

+ α(1 −

π(a|s)da).

(17)
(18)

(19)

Differentiating with respect to π gives:

a

∂L
∂π

= Aπk (s, a) − λ log πβ(a|s) + λ log π(a|s) + λ − α.

(20)

Setting ∂L
solution to this problem:

∂π to zero and solving for π gives the closed form

π∗(a|s) =

1
Z(s)

πβ(a|s) exp

Aπk (s, a)

(cid:19)

,

(cid:18) 1
λ

(21)

Next, we project the solution into the space of parametric
policies. For a policy πθ with parameters θ, this can be done
by minimizing the KL divergence of πθ from the optimal
non-parametric solution π∗ under the data distribution ρπβ (s):

arg min
θ

E
ρπβ (s)

= arg min
θ

E
ρπβ (s)

[DKL(π∗(·|s)||πθ(·|s))]

(cid:20)

E
π∗(·|s)

(cid:21)
[− log πθ(·|s)]

(22)

(23)

Note that in the projection step, the parametric policy could
be projected with either direction of KL divergence. However,
choosing the reverse KL direction has a key advantage: it
allows us to optimize θ as a maximum likelihood problem
with an expectation over data s, a ∼ β, rather than sampling
actions from the policy that may be out of distribution for the
Q function. In our experiments we show that this decision is
vital for stable off-policy learning.

Furthermore, assume discrete policies with a minimum

probably density of πθ ≥ αθ. Then the upper bound:

DTV(π∗, πθ)2

DKL(π∗||πθ) ≤

2
αθ
1
αθ
holds by the Pinsker’s inequality, where DTV denotes the
total variation distance between distributions. Thus minimizing

DKL(πθ||π∗)

(25)

(24)

≤

the reverse KL also bounds the forward KL. Note that we
can control the minimum α if desired by applying Laplace
smoothing to the policy.

B. Implementation Details

We implement

the algorithm building on top of twin
soft actor-critic [18], which incorporates the twin Q-function
architecture from twin delayed deep deterministic policy
gradient (TD3) from Fujimoto et al. [13]. All off-policy
algorithm comparisons (SAC, BRAC, MPO, ABM, BEAR) are
implemented from the same skeleton. The base hyperparameters
are given in Table II. The policy update is replaced with:

θk+1 = arg max

θ

E
s,a∼β

(cid:20)
log πθ(a|s)

1
Z(s)

exp

(cid:18) 1
λ

(cid:19)(cid:21)

Aπk (s, a)

.

(26)

Env

pen

door

relocate

Use
Z(s)

84%

0%

0%

Omit
Z(s)

98%

95%

54%

Table I: Success rates after online ﬁne-tuning (after 800K steps
for pen, door and 4M steps for relocate) using AWAC with
and without Z(s) weight. These results show that although
we can estimate Z(s), weighting by Z(s) actually results in
worse performance.

a πθ(a|s) exp (cid:0) 1

Similar to advantage weight regression [41] and other
prior work [40, 55, 49], we disregard the per-state normal-
λ Aπk (s, a)(cid:1) da =
izing constant Z(s) = (cid:82)
Ea∼πθ(·|s)[Aπk (s, a)]. We did experiment with estimating this
expectation per batch element with K = 10 samples, but found
that this generally made performance worse, perhaps because
errors in the estimation of Z(s) caused more harm than the
beneﬁt the method derived from estimating this value. We
report success rate results for variants of our method with and
without Z(s) estimation in Table I.

it

While prior work [40, 55, 41] has generally ignored the
omission of Z(s) without any speciﬁc justiﬁcation,
is
possible to bound this value both above and below using
the Cauchy-Schwarz and reverse Cauchy-Schwarz (Polya-
Szego) inequalities, as follows. Let f (a) = π(a|s) and
g(a) = exp(A(s, a)/λ). Note f (a) > 0 for stochastic policies
and g(a) > 0. By Cauchy-Schwarz, Z(s) = (cid:82)
a f (a)g(a)da ≤
(cid:113)(cid:82)
a g(a)2da = C1. To apply Polya-Szego, let mf
and mg be the minimum of f and g respectively and Mf , Mg be
)−1C1 = C2.
the maximum. Then Z(s) ≥ 2(
We therefore have C1 ≤ Z(s) ≤ C2, though the bounds are
generally not tight.

a f (a)2da (cid:82)

(cid:113) Mf Mg
mf mg

+ mf mg
Mf Mg

A further, more intuitive argument for why omitting Z(s)
may be harmless in practice comes from observing that this
normalizing factor only affects the relative weight of different
states in the training objective, not different actions. The state
distribution in β already differs from the distribution over

orientation be denoted by dp and do respectively. The reward
function is r = 1|xp−dp|≤0.0751|xo·do|≤0.95 - 1. In Rajeswaran
et al. [46], the episode was terminated when the pen fell out of
the hand; we did not include this early termination condition.
b) door-binary-v0.: The task is to open a door, which
requires ﬁrst twisting a latch. The action dimension is 28 and
the observation dimension is 39. Let d denote the angle of the
door. The reward function is r = 1d>1.4 - 1.

c) relocate-binary-v0.: The task is to relocate an object to
a goal location. The action dimension is 30 and the observation
dimension is 39. Let xp denote the object position and dp
denote the desired position. The reward is r = 1|xp−dp|≤0.1 -
1.

2) Sawyer Manipulation Environment:

a) SawyerPush-v0.: This environment is included in the
Multiworld library. The task is to push a puck to a goal position
in a 40cm x 20cm, and the reward function is the negative
distance between the puck and goal position. When using
this environment, we use hindsight experience replay for goal-
conditioned reinforcement learning. The random dataset for
prior data was collected by rolling out an Ornstein-Uhlenbeck
process with θ = 0.15 and σ = 0.3.

3) Off-Policy Data Performance: The performances of the
expert data, behavior cloning (BC) on the expert data (1),
and BC on the combined expert+BC data (2) are included in
Table III. For Gym benchmarks we report average return, and
expert data is collected by a trained SAC policy. For dextrous
manipulation tasks we report the success rate, and the expert
data consists of human demonstrations provided by Rajeswaran
et al. [46].

Env

Expert

BC (1)

BC (2)

cheetah

walker

ant

pen

door

relocate

9962

5062

5207

1

1

1

2507

2040

687

0.73

0.10

0.02

4524

1701

1704

0.76

0.00

0.01

Table III: Performance of the off-policy data for each envi-
ronment. BC (1) indicates BC on the expert data, while BC
(2) indicates BC on the combined expert+BC data used as
off-policy data for pretraining.

Hyper-parameter

Training Batches Per Timestep

Value

1

Exploration Noise

None (stochastic policy)

RL Batch Size

Discount Factor

Reward Scaling

Replay Buffer Size

Number of pretraining steps

1024

0.99

1

1000000

25000

Policy Hidden Sizes

[256, 256, 256, 256]

Policy Hidden Activation

Policy Weight Decay

Policy Learning Rate

ReLU

10−4

3 × 10−4

Q Hidden Sizes

[256, 256, 256, 256]

Q Hidden Activation

Q Weight Decay

Q Learning Rate

Target Network τ

ReLU

0

3 × 10−4

5 × 10−3

Table II: Hyper-parameters used for RL experiments.

states that will be visited by πθ, and therefore preserving
this state distribution is likely to be of limited utility to
downstream policy performance. Indeed, we would expect
that sufﬁciently expressive policies would be less affected
by small to moderate variability in the state weights. On the
other hand, inaccurate estimates of Z(s) may throw off the
training objective by increasing variance, similar to the effect
of degenerate importance weights.

The Lagrange multiplier λ is treated as a hyperparameter in
our method. In this work we use λ = 0.3 for the manipulation
environments and λ = 1.0 for the MuJoCo benchmark
environments. One could adaptively learn λ with a dual gradient
descent procedure, but this would require access to πβ.

As rewards for the dextrous manipulation environments are
non-positive, we clamp the Q value for these experiments to
be at most zero. We ﬁnd this stabilizes training slightly.

C. Environment-Speciﬁc Details

We evaluate our method on three domains: dexterous manip-
ulation environments, Sawyer manipulation environments, and
MuJoCo benchmark environments. In the following sections
we describe speciﬁc details.

1) Dexterous Manipulation Environments: These environ-
ments are modiﬁed from those proposed by Rajeswaran et al.
[46].

a) pen-binary-v0.: The task is to spin a pen into a given
orientation. The action dimension is 24 and the observation
dimension is 45. Let the position and orientation of the pen be
denoted by xp and xo respectively, and the desired position and

Name

SAC

SAC + BC

BCQ

BEAR

AWR

MPO

ABM-MPO

DAPG

BRAC

AWAC (Ours)

Policy Objective

ˆQ
Qπ DKL(πθ|| ¯Q)
Qπ Mixed
Qπ DKL(πθ|| ¯Q)
Qπ DKL(πθ|| ¯Q)
Qβ DKL( ¯Q||πθ)
Qπ DKL( ¯Q||πθ)
Qπ DKL( ¯Q||πθ)
J(πθ)
-
Qπ DKL(πθ|| ¯Q)
Qπ DKL( ¯Q||πθ)

ˆπβ ?

No

No

Yes

Yes

No
Yes∗

Yes

No

Yes

No

Constraint

None

None
Support ((cid:96)∞)

Support (MMD)

Implicit

Prior

Learned Prior

None

Explicit KL penalty

Implicit

Figure 8: Comparison of prior algorithms that can incorporate
prior datasets. See section D for speciﬁc implementation details.
We argue that avoiding estimating ˆπβ (i.e., ˆπβ is “No”) is
important when learning with complex datasets that include
experience from multiple policies, as in the case of online ﬁne-
tuning, and maintaining a constraint of some sort is essential
for ofﬂine training. At the same time, sample-efﬁcient learning
requires using Qπ for the critic. Our algorithm is the only one
that fulﬁlls all of these requirements.

D. Baseline Implementation Details

We used public implementations of prior methods (DAPG,
AWR) when available. We implemented the remaining algo-
rithms in our framework, which also allows us to understand
the effects of changing individual components of the method.
In the section, we describe the implementation details. The
full overview of algorithms is given in Figure 8.

Behavior Cloning (BC). This method learns a policy with

supervised learning on demonstration data.

Soft Actor Critic (SAC). Using the soft actor critic
algorithm from [18], we follow the exact same procedure
as our method in order to incorporate prior data, initializing
the policy with behavior cloning on demonstrations and adding
all prior data to the replay buffer.

Behavior Regularized Actor Critic (BRAC). We imple-
ment BRAC as described in [59] by adding policy regularization
log(πβ(a|s)) where πβ is a behavior policy trained with
supervised learning on the replay buffer. We add all prior
data to the replay buffer before online training.

Advantage Weighted Regression (AWR). Using the ad-
vantage weighted regression algorithm from [41], we add all
prior data to the replay buffer before online training. We use
the implementation provided by Peng et al. [41], with the key
difference from our method being that AWR uses TD(λ) on
the replay buffer for policy evaluation.

Monotonic Advantage Re-Weighted Imitation Learning
(MARWIL). Monotonic advantage re-weighted imitation learn-
ing was proposed by Wang et al. [55] for ofﬂine imitation
learning. MARWIL was not demonstrated in online RL settings,
but we evaluate it for ofﬂine pretraining followed by online
ﬁne-tuning as we do other ofﬂine algorithms. Although derived
differently, MARWIL and AWR are similar algorithms and
only differ in value estimation: MARWIL uses the on-policy

single-path advantage estimate A(s, a) = Qπβ (s, a) − V πβ (s)
instead of TD(λ) as in AWR. Thus, we implement MARWIL
by modifying the implementation of AWR.

Maximum a Posteriori Policy Optimization (MPO). We
evaluate the MPO algorithm presented by Abdolmaleki et al.
[2]. Due to a public implementation being unavailable, we
modify our algorithm to be as close to MPO as possible. In
particular, we change the policy update in Advantage Weighted
Actor Critic to be:

θi ←− arg max

Es∼D,a∼π(a|s)

θi

(cid:20)
log πθi(a|s) exp(

Qπβ (s, a))

(cid:21)

.

1
β

(27)

Note that in MPO, actions for the update are sampled from
the policy and the Q-function is used instead of advantage for
weights. We failed to see ofﬂine or online improvement with
this implementation in most environments, so we omit this
comparison in favor of ABM.

Advantage-Weighted Behavior Model (ABM). We evalu-
ate ABM, the method developed in Siegel et al. [49]. As with
MPO, we modify our method to implement ABM, as there is
no public implementation of the method. ABM ﬁrst trains an
advantage model πθabm(a|s):
Eτ ∼D

θabm = arg max

θi

|τ |
(cid:88)






log πθabm(at|st)f (R(τt:N ) − ˆV (s))

 .

(28)

t=1

where f is an increasing non-negative function, chosen to
be f = 1+. In place of an advantage computed by empirical
returns R(τt:N )− ˆV (s) we use the advantage estimate computed
per transition by the Q value Q(s, a) − V (s). This is favorable
for running ABM online, as computing R(τt:N ) − ˆV (s) is
similar to AWR, which shows slow online improvement. We
then use the policy update:

θi ←− arg max

Es∼D,a∼πabm(a|s)

θi

(cid:20)

log πθi(a|s) exp

(Qπi(s, a) − V πi(s))

(cid:19)(cid:21)

.

(29)

(cid:18) 1
λ

Additionally, for this method, actions for the update are sampled
from a behavior policy trained to match the replay buffer and
the value function is computed as V π(s) = Qπ(s, a) s.t. a ∼ π.
Demonstration Augmented Policy Gradient (DAPG). We
directly utilize the code provided in [46] to compare against
our method. Since DAPG is an on-policy method, we only
provide the demonstration data to the DAPG code to bootstrap
the initial policy from.

Bootstrapping Error Accumulation Reduction (BEAR).
We utilize the implementation of BEAR provided in rlkit. We
provide the demonstration and off-policy data to the method
together. Since the original method only involved training
ofﬂine, we modify the algorithm to include an online training
phase. In general we found that the MMD constraint in the
method was too conservative. As a result, in order to obtain the

n
r
u
t
e
R

e
g
a
r
e
v
A

Figure 9: Comparison of our method and prior methods on standard MuJoCo benchmark tasks. These tasks are much easier
than the dexterous manipulation tasks, and allow us to better inspect the performance of methods in the setting of ofﬂine
pretraining followed by online ﬁne-tuning. SAC+BC and BRAC perform on par with our method on the HalfCheetah task, and
ABM performs on par with our method on the Ant task, while our method outperforms all others on the Walker2D task. Our
method matches or exceeds the best prior method in all cases, whereas no other single prior method attains good performance
on all of the tasks.

algorithm. We ﬁnd that these algorithms exhibit a characteristic
dip at the start of learning. Although this dip is only present
in the early part of the learning curve, a poor initial policy and
lack of steady policy improvement can be a safety concern and
a signiﬁcant hindrance in real-world applications. Moreover,
recall that in the more difﬁcult dextrous manipulation tasks,
these algorithms do not show any signiﬁcant learning.

Conservative online learning. Finally, we consider con-
servative ofﬂine algorithms: ABM [49], BEAR [30], and
BRAC [59]. We found that BRAC performs similarly to SAC
for working hyperparameters. BEAR trains well ofﬂine – on Ant
and Walker2d, BEAR signiﬁcantly outperforms prior methods
before online experience. However, online improvement is slow
for BEAR and the ﬁnal performance across all three tasks is
much lower than AWAC. The closest in performance to our
method is ABM, which is comparable on Ant-v2, but much
slower on other domains.

results displayed in our paper, we swept the MMD threshold
value and chose the one with the best ﬁnal performance after
ofﬂine training with ofﬂine ﬁne-tuning.

E. Gym Benchmark Results From Prior Data

In this section, we provide a comparative evaluation on
MuJoCo benchmark tasks for analysis. These tasks are simpler,
with dense rewards and relatively lower action and observation
dimensionality. Thus, many prior methods can make good
progress on these tasks. These experiments allow us to
understand more precisely which design decisions are crucial.
For each task, we collect 15 demonstration trajectories using
a pre-trained expert on each task, and 100 trajectories of off-
policy data by rolling out a behavioral cloned policy trained
on the demonstrations. The same data is made available to
all methods. The results are presented in Figure 9. AWAC is
consistently the best or on par with the best-performing method.
No other single method consistently attains the best results –
on HalfCheetah, SAC + BC and BRAC are competitive, while
on Ant-v2 ABM is competitive with AWAC. We summarize
the results according to the challenges in Section III.

Data efﬁciency. The three methods that do not estimate Qπ
are DAPG [46], AWR [41], and MARWIL [55]. Across all three
tasks, we see that these methods are somewhat worse ofﬂine
than the best performing ofﬂine methods, and exhibit steady
but very slow improvement during ﬁne-tuning. In robotics, data
efﬁciency is vital, so these algorithms are not good candidates
for practical real-world applications.

Bootstrap error in ofﬂine learning. For SAC [18], across
all three tasks, we see that the ofﬂine performance at epoch 0
is generally poor. Due to the data in the replay buffer, SAC
with prior data does learn faster than from scratch, but AWAC
is faster to solve the tasks in general. SAC with additional
data in the replay buffer is similar to the approach proposed
by Veˇcerík et al. [54]. SAC+BC reproduces Nair et al. [39]
but uses SAC instead of DDPG [34] as the underlying RL

0K100K200K300K400K500KTimesteps0200040006000800010000HalfCheetah-v20K100K200K300K400K500KTimesteps20000200040006000Ant-v20K100K200K300K400K500KTimesteps1000010002000300040005000Walker2d-v2AWAC (Ours)ABM [49]AWR [41]MARWIL [55]BEAR [30]BRAC [59]DAPG [46]SACfD [54]SAC+BC [39]methods often start at high performance when initialized from
higher-quality data.

AWAC

AWAC

(ofﬂine)

(online)

BEAR

HalfCheetah

random

medium

2.2

37.4

medium-expert

36.8

Hopper

expert

random

medium

78.5

9.6

72.0

medium-expert

80.9

expert

Walker2D

random

medium

85.2

5.1

30.1

medium-expert

42.7

52.9

41.1

41.0

25.5

38.6

51.7

105.6

108.2

62.8

91.0

111.9

111.8

11.7

79.1

78.3

9.5

47.6

4.0

110.3

6.7

33.2

10.8

expert

57.0

103.0

106.1

Figure 11: Comparison of our method (AWAC) ﬁne-tuning on
varying data quality datasets in D4RL [12]. AWAC is able to
improve its ofﬂine performance by further ﬁne-tuning online.

F. Extra Baseline Comparisons (CQL, AlgaeDICE)

In this section, we add comparisons to constrained Q-learning
(CQL) [31] and AlgaeDICE [37]. For CQL, we use the authors’
implementation, modiﬁed for additionally online-ﬁnetuning
instead of only ofﬂine training. For AlgaeDICE, we use the
publicly available implementation, modiﬁed to load prior data
and perform 25K pretraining steps before online RL. The
results are presented in Figure 10.

Figure 10: Comparison of our method (AWAC) with CQL and
AlgaeDICE. CQL and AWAC perform similarly ofﬂine, but
CQL does not improve when ﬁne-tuning online. AlgaeDICE
does not perform well for ofﬂine pretraining.

G. Online Fine-Tuning From D4RL

In this experiment, we evaluate the performance of varied
data quality (random, medium, medium-expert, and expert)
datasets included in D4RL [12], a dataset intended for ofﬂine
RL. The results are obtained by ﬁrst by training ofﬂine and
then ﬁne-tuning online on each setting for 500,000 additional
steps. The performance of BEAR [30] is attached as reference.
We attempted to ﬁne-tune BEAR online using the same
protocol as AWAC but the performance did not improve and
often decreased; thus we report the ofﬂine performance. All
performances are scaled to 0 to 100, where 0 is the average
returns of a random policy and 100 is the average returns of
an expert policy (obtained by training online with SAC), as is
standard for D4RL.

The results are presented in Figure 11. First, we observe that
AWAC (ofﬂine) is competitive with BEAR, a commonly used
ofﬂine RL algorithm. Then, AWAC is able to make progress in
solving the tasks with online ﬁne-tuning, even when initialized
from random data or “medium” quality data, as shown by the
performance of AWAC (online). In almost all settings, AWAC
(online) is the best performing or tied with BEAR. In four of
the six lower quality (random or medium) data settings, AWAC
(online) is signiﬁcantly better than BEAR; it is reasonable that
AWAC excels in the lower-quality data regime because there
is more room for online improvement, while both ofﬂine RL

Figure 12: Full views of the robot hardware setups. Videos are available at awacrl.github.io

H. Hardware Experimental Setup

Here, we provide further details of the hardware experimental

setups, which are pictured in Fig 12.
Dexterous Manipulation with a 3 Fingered Claw.

• State space: 22 dimensions, consisting of joint angles of

the robot and rotational position of the object.

• Action space: 9 dimensions, consisting of desired joint

angles of the robot.

• Reward: −1 if the valve is rotated within 0.25 radians of

the target, and 0 otherwise.

• Prior data: 10 demonstrations collected by kinesthetic
teaching and 200 trajectories of behavior cloned data.

Drawer Opening with a Sawyer Arm.

• State space: 4 dimensions, end effector position of the
robot and rotational position of the motor attached to the
drawer.

• Action space: 3 dimensions, for velocity control of end-

effector position.

• Reward: −1 if the motor is rotated more than 15 radians

of the reset position, and 0 otherwise.

• Prior data: 10 demonstrations collected using a 3DConnex-
ion Spacemouse device and 500 trajectories of behavior
cloning data.

Dexterous Manipulation with a Robotic Hand.

• State space: 25 dimensions, consisting of joint angles of
the hand, end effector positions of the arm, object position
and target position.

• Action space: 19 dimensions, consisting of desired 16
joint angles of the hand and 3 dimensions for end-effector
control of the arm.

• Reward: let o be the position of the object, h be the
position of the hand, and g be the target location of the
object. Then r = −||o − h|| − 3||o − g||.

• Prior data: 19 demonstrations obtained via kinesthetic
teaching and 50 trajectories of behavior cloned data.

