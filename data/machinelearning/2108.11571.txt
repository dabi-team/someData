2
2
0
2

n
u
J

4
2

]

G
L
.
s
c
[

2
v
1
7
5
1
1
.
8
0
1
2
:
v
i
X
r
a

GNNSampler: Bridging the Gap between
Sampling Algorithms of GNN and Hardware

Xin Liu1,2, Mingyu Yan ((cid:0))1, Shuhan Song1,2, Zhengyang Lv1,2,
Wenming Li1,2, Guangyu Sun3, Xiaochun Ye1,2, and Dongrui Fan1,2

1 SKLP, Institute of Computing Technology, CAS
2 University of Chinese Academy of Sciences
3 School of Integrated Circuits, Peking University
{liuxin19g,yanmingyu,songshuhan19s,lvzhengyang19b,
liwenming,yexiaochun,fandr}@ict.ac.cn
gsun@pku.edu.cn

Abstract. Sampling is a critical operation in Graph Neural Network
(GNN) training that helps reduce the cost. Previous literature has ex-
plored improving sampling algorithms via mathematical and statistical
methods. However, there is a gap between sampling algorithms and hard-
ware. Without consideration of hardware, algorithm designers merely op-
timize sampling at the algorithm level, missing the great potential of pro-
moting the eﬃciency of existing sampling algorithms by leveraging hard-
ware features. In this paper, we pioneer to propose a uniﬁed programming
model for mainstream sampling algorithms, termed GNNSampler, cov-
ering the critical processes of sampling algorithms in various categories.
Second, to leverage the hardware feature, we choose the data locality
as a case study, and explore the data locality among nodes and their
neighbors in a graph to alleviate irregular memory access in sampling.
Third, we implement locality-aware optimizations in GNNSampler for
various sampling algorithms to optimize the general sampling process.
Finally, we emphatically conduct experiments on large graph datasets
to analyze the relevance among training time, accuracy, and hardware-
level metrics. Extensive experiments show that our method is universal
to mainstream sampling algorithms and helps signiﬁcantly reduce the
training time, especially in large-scale graphs.

Keywords: Graph neural network · Sampling algorithms · Acceleration
· Hardware feature · Data locality

1

Introduction

Motivated by conventional deep learning methods, graph neural networks (GNNs)
[1] are proposed and have shown remarkable performance in graph learning,
bringing about signiﬁcant improvements in tackling graph-based tasks [2,3,4].
Whereas, a crucial issue is that real-world graphs are extremely large. Learning
large-scale graphs generally requires massive computation and storage resources

 
 
 
 
 
 
2

X. Liu et al.

Fig. 1. Overview of eﬃcient sampling in GNN training based on GNNSampler, where
GNNSampler consists of multiple steps in the INIT and EXECUTE stage, making it
possible that some substeps of sampling can be optimized in a ﬁne-grained manner by
leveraging hardware features.

in practice, leading to high cost in training GNN [5]. To this end, sampling algo-
rithms are proposed for eﬃcient GNN training, by conditionally selecting nodes
to reduce the computation and storage costs in GNN training.

However, abundant irregular memory accesses to neighbors of each node are
generally required in all sampling algorithms, introducing signiﬁcant overhead
due to the irregular connection pattern in graph [6]. Previous sampling-based
models [4,7,8,9,10,11,12,13] leverage mathematical and statistical approaches for
improvement, but they do not alleviate the high cost caused by irregular memory
access. Thereby, algorithm designers merely optimize sampling at the algorithm
level without considering hardware features. The eﬃcient execution of sampling,
and even the eﬃciency of GNN training, are limited by the gap between algo-
rithm and hardware.

To this end, we target to bridge the gap between sampling algorithms of
GNN and hardware. In this paper, as illustrated in Figure 1, we pioneer to build
GNNSampler, a uniﬁed programming model for sampling, by abstracting diverse
sampling algorithms. Our contributions can be summarized as follows:

• We propose a uniﬁed programming model, termed GNNSampler, for main-
stream sampling algorithms, which covers key procedures in the general sam-
pling process.

• We choose data locality in graph datasets as a case study to leverage hard-
ware features. Moreover, we explore the data locality among nodes and their
neighbors in a graph to alleviate irregular memory access in sampling.

• We implement locality-aware optimizations in GNNSampler to improve the
general sampling process, helping reduce considerable cost in terms of time.
Notably, the optimization is adjustable and is performed once and for all,
providing vast space for trading oﬀ the training time and accuracy.

• We conduct extensive experiments on large graph datasets, including time-
accuracy comparison and memory access quantiﬁcation, to analyze the rel-
evance among the training time, accuracy, and hardware-level metrics.

VbVIN1VIN2VoutVDDVDDVbVIN1VIN2VoutVDDVDDDiverseSamplingAlgorithmsHardwareFeaturesDiverseSamplingAlgorithmsHardwareFeaturesGEEBBGEBSamplingSamplingHHDKKAGFGEBSamplingHDKAGFOptimizeGNNSamplerEXEINITStageStageOptimizeGNNSamplerEXEINITStageStageAbstractAbstractDiverseSamplingAlgorithmsHardwareFeaturesDiverseSamplingAlgorithmsHardwareFeaturesGEEBBGEBSamplingSamplingHHDKKAGFGEBSamplingHDKAGFOptimizeGNNSamplerEXEINITStageStageOptimizeGNNSamplerEXEINITStageStageAbstractAbstractGNNSampler: Bridging GNN Sampling and Hardware

3

2 Background and Motivation

In this section, we ﬁrst introduce the background of GNN and mainstream sam-
pling algorithms. Then, we highlight the gap between the algorithms and hard-
ware, and put forward our motivation.

2.1 Background of GNN

GNN [1] was ﬁrst proposed to apply neural networks to graph learning. It learns
a state embedding hv to represent neighborhood information of each node v in
a graph. Generally, the hv can be represented in the following form:

hv = fw(xv, xe[v], hn[v], xn[v]),

(1)

ov = gw(hv, xv),
where xv, xe[v], and xn[v] denote the features of v, v’s edges, and v’s neighbors,
respectively. fw(·) and gw(·) are the functions deﬁned for local transition and
local output. And ov is the output generated by the embedding and feature
of node v. In this way, the hidden information of a graph is extracted by the
following approach:

(2)

hl+1 = fw(hl, x),
where l denotes the l-th iteration of the embedding computation. Many variants
take the idea from the original GNN and add some particular mechanisms to
modify the models for handling various graph-based tasks. Herein, we highlight
the form of graph convolutional networks (GCNs) [2] since most sampling algo-
rithms are applied to GCNs for eﬃcient model training. Generally, GCNs use a
layer-wise propagation rule to calculate the embedding in the following form:

(3)

Hl+1 = σ

D

−1/2

−1/2

D

A

HlWl

,

(cid:18)

(cid:19)

(4)

(cid:101)

(cid:101)
where Hl, Wl are the hidden representation matrix and trainable weight matrix
in the l-th layer of the model. And σ(·) is the nonlinear activation function,
such as ReLU and Softmax. GCNs represent neighborhood information with a
renormalized adjacency matrix and extract the hidden information in such an
iterative manner.

(cid:101)

2.2 Sampling Algorithms in Training

Training GNNs, especially GCNs, generally requires full graph Laplacian and all
intermediate embeddings, which brings about extensive storage cost and makes
it hard to scale the training on large-scale graphs. Moreover, the conventional
training approach uses a full-batch scheme to update the model, leading to a
slow convergence rate.

4

X. Liu et al.

Fig. 2. Illustration on typical processes of various sampling algorithms.

To overcome these drawbacks, sampling algorithms are proposed to modify
the conventional training through a mini-batch scheme and conditionally se-
lect partial neighbors, reducing the cost in terms of storage and computation.
Speciﬁcally, mainstream sampling algorithms can be divided into multi-
ple categories according to the granularity of the sampling operation in one
sampling batch [14]. As illustrated in Figure 2, we respectively show typical
sampling processes of multiple categories, that is, node-wise, layer-wise, and
subgraph-based sampling algorithms.

As typical of node-wise sampling algorithms, GraphSAGE [4] randomly
samples the neighbors of each node in multiple hops recursively; VR-GCN [7] im-
proves the strategy of random neighbor sampling by restricting sampling size to
an arbitrarily small value, which guarantees a fast training convergence. Layer-
wise sampling algorithms, e.g., FastGCN [8] and AS-GCN [9], generally conduct
sampling on a multi-layer model in a top-down manner. FastGCN presets the
number of nodes to be sampled per layer without paying attention to a single
node’s neighbors. AS-GCN executes the sampling process conditionally based
on the parent nodes sampled in the upper layer, where the layer sampling is
probability-based and dependent among layers. As for subgraph-based sam-
pling algorithms, multiple subgraphs, which are generated by partitioning the
entire graph or inducing nodes (edges), are sampled for each mini-batch for
training. Cluster-GCN [10] partitions the original graph with a clustering al-
gorithm and then randomly selects multiple clusters to construct subgraphs.
GraphSAINT [11] induces subgraphs from probabilistic sampled nodes (edges)
by leveraging multiple samplers.

Unfortunately, the cost of sampling is gradually becoming non-negligible in
the training of some sampling-based models, especially on large datasets. As
proof, we conduct experiments on datasets with a growing graph scale (amount
of nodes & edges) using GraphSAGE [4], FastGCN [8] and GraphSAINT (node
sampler) [11], and quantify the proportion of sampling time to the training time,
based on the oﬃcial setting of all parameters in their works. The sam-

DLayer-wiseSamplingSubgraph-basedSamplingConstruct  Subgraph  HNode-wiseSamplingInitialize with the Root NodesGABDGABDGABDDGABDGABDABABDDABDCEDFGBAABCDEFGABCDEFGCEDFGBAABCDEFGABCDEFGCEDFGBAABCDEFGABCDEFGCEDFGBAABCDEFGABCDEFGGABDGABDGABDGABDGGGraph forTraining1-hop Neighbor Sampling2-hop Neighbor SamplingGNNSampler: Bridging GNN Sampling and Hardware

5

Fig. 3. The proportion of the execution time in diﬀerent parts of training across dif-
ferent models and datasets.

pling part includes selecting nodes, constructing an adjacency matrix, and some
subsequent processes. The other part denotes the rest of processes in training,
e.g., feature aggregation and update. For each sampling algorithms, we consider
the proportion of sampling time in the smallest dataset (e.g., Cora [15]) as the
baseline and plot the growth trend of sampling time on each dataset. Detailed
information about datasets is shown in Table 1. Distinctly, for all these sampling-
based models, as the number of nodes and edges in a graph dataset grows, the
cost of sampling becomes increasingly large and intolerable. As illustrated in
Figure 3, the proportion of sampling time becomes larger as the graph scale of
a dataset increases, even up to 62.7%.

2.3 The Gap between Algorithm and Hardware

As a critical process in training, sampling is becoming non-trivial, and its cost
mainly derives from the gap between sampling algorithms and hardware. Algo-
rithm designers do not consider hardware features and merely improve sampling
at the algorithm level. On the other hand, hardware designers have not im-
proved sampling algorithms since they do not know speciﬁc implementations of
sampling algorithms. Therefore, mining of the improvement space for sampling
algorithms is restricted by the gap. Thereby, recent literature proposes to lever-
age the hardware feature for improving the eﬃciency regarding training and
inference of GNNs. For example, NeuGraph [16] makes the best of the hard-
ware features of platforms (CPU and GPU), achieving excellent improvements
in GNN training. For another example, HyGCN [17] tailors its hardware features
to the execution semantic of GCNs based on the execution characterization of
GCNs [18], greatly improving the performance of GCN inference.

We argue that sampling is also a process of algorithm and hardware coordi-
nation. We observe that existing sampling algorithms vary in their mechanisms.
And an eﬃcient improvement should be universal to most sampling algorithms of
diﬀerent mechanisms, urging the demand to put forward a general sampling pro-
gramming model. To support our argument, we propose GNNSampler, a uniﬁed
programming model for mainstream sampling algorithms. Moreover, we choose

14.2%34.1%14.3%15.4%32.4%47.6%62.7%00.511.522.530%20%40%60%80%100%120%GraphSAGEFastGCNGraphSAINTGrowth Ratio of SamplingProportion of Time CostSamplingOthersSampling Growth6

X. Liu et al.

Fig. 4. (a) Abstraction for node-wise sampling algorithms. (b) Abstraction for layer-
wise sampling algorithms. To be as uniﬁed as possible, we use the same name for steps
(functions) in the abstractions, despite some slight distinctions of required parameters.

data locality as a case study and implement locality-aware optimizations to im-
prove the general sampling process. By improving the process of sampling, we
eventually reduce the time consumption of GNN training.

3 Uniﬁed Programming Model

In this section, we abstract sampling algorithms in diﬀerent categories and pro-
pose the uniﬁed programming model.

3.1 Abstractions for Sampling Algorithms

• Abstraction for Node-wise Sampling Algorithms.

In Figure 4 (a), lines 3-4 of Algorithm 1 correspond to the initialization
phase of sampling. In this phase, nodes needed for training are ﬁrst divided into
multiple mini-batches. Lines 5-13 correspond to the execution phase of sampling.
First, nodes in a mini-batch are ﬁrst obtained in the form of an indices list. Next
up, the sampling size for each node is obtained before sampling. Then, sampling
is executed on each node in a batch to get indices of neighbors by leveraging
some pre-calculated parameters. Finally, the adjacency matrix is constructed
based on the sampled nodes and updated periodically per batch.
• Abstraction for Layer-wise Sampling Algorithms.

In Figure 4 (b), layer-wise samplings do not need to focus on a single node
since they sample a ﬁxed number of nodes together in each layer based on the
pre-calculated probability distribution. In Algorithm 2, line 9 corresponds to
the calculation of probability distribution. Line 10 represents that nl nodes are
sampled together in the l -th layer by leveraging some pre-calculated parameters.
Generally, the indices of sampled nodes are used to update the adjacency matrix.
For some layer-wise sampling algorithms (e.g., AS-GCN [9]) in which nodes are

matricesconstructionandmini-batchesgeneration.Thepro-portionoftrainingonlycontainsnecessarystepsfortrainingmodel,whichexcludesmodelevaluation.Foreachsamplingmethod,weconsidertheproportionofsamplingtimeinthesmallestdataset(e.g.,CoraandPPI)asthebaselineandplotthegrowthtrendofsamplingproportionineachdataset.Explicitly,theproportionofsamplingtimetototaltrainingtimebecomeslargerasthedataset’ssizeincreases.Layer-1Layer-2Layer-3TotalHyGCN0.0330580.0130310.0079160.054006100CoraC2A-only0.0272260.0173470.0067230.05129794.98392PubmedBifusion0.0272260.0173470.0067230.05129794.98392RedditHyGCN0.0329440.0107350.0078880.05156795.48427CoraC2A-only0.0272260.0129860.0096720.04988592.36937PubmedBifusion0.0272260.0107350.0096720.04763388.2003RedditPPIRedditGraphSAINTTime statisticsGCN-CDGCN-FLGraphSAGEFastGCN01020304050020406080100120GraphSAGEFastGCNGraphSAINTSampling Growth RatioProportion of Time Cost in Different Part of Training (%) SamplingTrainingOthersSampling GrowthFig.3:Quantiﬁcationoftheproportionofdifferentparts’executiontimeinmodeltrainingAlthoughprevioussamplingmethodsprovideagreatleaptospeedupthetrainingofGNN,theyarestilllimitedbythegapbetweensamplingalgorithmsandhardware.Duetodifferentresearchbackgrounds,algorithmdesignersdonotconsiderthehardwarecharacteristicsandonlyimprovesam-plingalgorithmsatthealgorithmlevel.Hardwaredesignershavenotprovidedhardwaresupportforspeciﬁcalgorithmssincetheydonotknowspeciﬁcalgorithmimplementations.Therefore,miningoftheimprovementspaceforsamplingal-gorithmsisrestrictedbythegap.Ontheotherhand,leveraginghardwarecharacteristicshelpsdealwithgraph-relevanttasks.Forinstance,NeuGraph[11]designsastreamingschedulerthatmakesthebestofthecharacteristicsofCPUandGPUtoacceleratetheprocessingoflarge-scalegraphdata.GraphACT[26]designsanacceleratoronheterogeneousplatformsthatleveragesthecharacteristicsofCPUandFPGAtospeedupGCNtraining.Wearguethatsamplingisalsoaprocessofalgorithmandhardwarecoordination.Byleveraginghardwarecharacteristics,somesubstepsinsamplingcanbemoreefﬁcientafteropti-mizing,suchassearchingneighborsintheadjacencymatrixandaccessingnodes’embedding.Wealsoobservethattheexistingsamplingalgorithmsareusuallybasedondifferentmechanisms.Anefﬁcientimprovementshouldbeuniversaltomostsamplingalgorithms,urgingthedemandtoputforwardageneralsamplingﬂow.Tosupportourargument,weproposeGNNSampler,auniﬁedprogrammingmodelforsamplingalgorithms.Moreover,weoptimizethesamplingprocesslever-aginghardwarecharacteristicsandintegrateGNNSamplerintoageneralframeworkforefﬁcientGNNtraining.AdetaileddescriptionofGNNSamplerisgiveninSectionIII-B.Algorithm1:Abstractionfornode-wisesampling1INPUTGraphG(V,E,X)2OUTPUTadjacencymatrix3getminibatch(G)4S←getsamplestandard()5foreachbatchBkdo6inputs←getbatchinput(Bk)7foreachlayerldo8nl←getsamplenum(l);9foreachnodevinlayerldo10neiidx←getneighbor(inputs)11sampledidx←samplenode(neiidx,nl,S)12updateadjacencymatrix(sampledidx)13end14end15endAlgorithm2:Abstractionforlayer-wisesampling1INPUTGraphG(V,E,X)2OUTPUTadjacencymatrix3getminibatch(G)4S←getsamplestandard()5foreachbatchBkdo6inputs←getbatchinput(Bk)7foreachlayerldo8nl←getsamplenum(l)9P←getprobdistribution()10sampledidx←samplenode(inputs,nl,S,P)11updateadjacencymatrix(sampledidx)12inputs←updateinput(sampledidx)13end14endAlgorithm3:Abstractionforsubgraph-basedsam-pling1INPUTGraphG(V,E,X)2OUTPUTadjacencymatrix3getminibatch(G)4S←getsamplestandard()5foreachbatchBkdo6inputs←getbatchinput(Bk)7P←getprobdistribution()8whilenodenum≤subgraphsizedo9sampledidx←samplenode(inputs,S,P)10nodepool←appendnode(sampledidx)11end12adjacencymatrix←processing(nodepool)13inducesubgraph(adjacencymatrix)14end(a)Abstractionfornode-wisesamplingmatricesconstructionandmini-batchesgeneration.Thepro-portionoftrainingonlycontainsnecessarystepsfortrainingmodel,whichexcludesmodelevaluation.Foreachsamplingmethod,weconsidertheproportionofsamplingtimeinthesmallestdataset(e.g.,CoraandPPI)asthebaselineandplotthegrowthtrendofsamplingproportionineachdataset.Explicitly,theproportionofsamplingtimetototaltrainingtimebecomeslargerasthedataset’ssizeincreases.Layer-1Layer-2Layer-3TotalHyGCN0.0330580.0130310.0079160.054006100CoraC2A-only0.0272260.0173470.0067230.05129794.98392PubmedBifusion0.0272260.0173470.0067230.05129794.98392RedditHyGCN0.0329440.0107350.0078880.05156795.48427CoraC2A-only0.0272260.0129860.0096720.04988592.36937PubmedBifusion0.0272260.0107350.0096720.04763388.2003RedditPPIRedditGraphSAINTTime statisticsGCN-CDGCN-FLGraphSAGEFastGCN01020304050020406080100120GraphSAGEFastGCNGraphSAINTSampling Growth RatioProportion of Time Cost in Different Part of Training (%) SamplingTrainingOthersSampling GrowthFig.3:Quantiﬁcationoftheproportionofdifferentparts’executiontimeinmodeltrainingAlthoughprevioussamplingmethodsprovideagreatleaptospeedupthetrainingofGNN,theyarestilllimitedbythegapbetweensamplingalgorithmsandhardware.Duetodifferentresearchbackgrounds,algorithmdesignersdonotconsiderthehardwarecharacteristicsandonlyimprovesam-plingalgorithmsatthealgorithmlevel.Hardwaredesignershavenotprovidedhardwaresupportforspeciﬁcalgorithmssincetheydonotknowspeciﬁcalgorithmimplementations.Therefore,miningoftheimprovementspaceforsamplingal-gorithmsisrestrictedbythegap.Ontheotherhand,leveraginghardwarecharacteristicshelpsdealwithgraph-relevanttasks.Forinstance,NeuGraph[11]designsastreamingschedulerthatmakesthebestofthecharacteristicsofCPUandGPUtoacceleratetheprocessingoflarge-scalegraphdata.GraphACT[26]designsanacceleratoronheterogeneousplatformsthatleveragesthecharacteristicsofCPUandFPGAtospeedupGCNtraining.Wearguethatsamplingisalsoaprocessofalgorithmandhardwarecoordination.Byleveraginghardwarecharacteristics,somesubstepsinsamplingcanbemoreefﬁcientafteropti-mizing,suchassearchingneighborsintheadjacencymatrixandaccessingnodes’embedding.Wealsoobservethattheexistingsamplingalgorithmsareusuallybasedondifferentmechanisms.Anefﬁcientimprovementshouldbeuniversaltomostsamplingalgorithms,urgingthedemandtoputforwardageneralsamplingﬂow.Tosupportourargument,weproposeGNNSampler,auniﬁedprogrammingmodelforsamplingalgorithms.Moreover,weoptimizethesamplingprocesslever-aginghardwarecharacteristicsandintegrateGNNSamplerintoageneralframeworkforefﬁcientGNNtraining.AdetaileddescriptionofGNNSamplerisgiveninSectionIII-B.Algorithm1:Abstractionfornode-wisesampling1INPUTGraphG(V,E,X)2OUTPUTadjacencymatrix3getminibatch(G)4S←getsamplestandard()5foreachbatchBkdo6inputs←getbatchinput(Bk)7foreachlayerldo8nl←getsamplenum(l);9foreachnodevinlayerldo10neiidx←getneighbor(inputs)11sampledidx←samplenode(neiidx,nl,S)12updateadjacencymatrix(sampledidx)13end14end15endAlgorithm2:Abstractionforlayer-wisesampling1INPUTGraphG(V,E,X)2OUTPUTadjacencymatrix3getminibatch(G)4S←getsamplestandard()5foreachbatchBkdo6inputs←getbatchinput(Bk)7foreachlayerldo8nl←getsamplenum(l)9P←getprobdistribution()10sampledidx←samplenode(inputs,nl,S,P)11updateadjacencymatrix(sampledidx)12inputs←updateinput(sampledidx)13end14endAlgorithm3:Abstractionforsubgraph-basedsam-pling1INPUTGraphG(V,E,X)2OUTPUTadjacencymatrix3getminibatch(G)4S←getsamplestandard()5foreachbatchBkdo6inputs←getbatchinput(Bk)7P←getprobdistribution()8whilenodenum≤subgraphsizedo9sampledidx←samplenode(inputs,S,P)10nodepool←appendnode(sampledidx)11end12adjacencymatrix←processing(nodepool)13inducesubgraph(adjacencymatrix)14end(b)Abstractionforlayer-wisesamplingmatricesconstructionandmini-batchesgeneration.Thepro-portionoftrainingonlycontainsnecessarystepsfortrainingmodel,whichexcludesmodelevaluation.Foreachsamplingmethod,weconsidertheproportionofsamplingtimeinthesmallestdataset(e.g.,CoraandPPI)asthebaselineandplotthegrowthtrendofsamplingproportionineachdataset.Explicitly,theproportionofsamplingtimetototaltrainingtimebecomeslargerasthedataset’ssizeincreases.Layer-1Layer-2Layer-3TotalHyGCN0.0330580.0130310.0079160.054006100CoraC2A-only0.0272260.0173470.0067230.05129794.98392PubmedBifusion0.0272260.0173470.0067230.05129794.98392RedditHyGCN0.0329440.0107350.0078880.05156795.48427CoraC2A-only0.0272260.0129860.0096720.04988592.36937PubmedBifusion0.0272260.0107350.0096720.04763388.2003RedditPPIRedditGraphSAINTTime statisticsGCN-CDGCN-FLGraphSAGEFastGCN01020304050020406080100120GraphSAGEFastGCNGraphSAINTSampling Growth RatioProportion of Time Cost in Different Part of Training (%) SamplingTrainingOthersSampling GrowthFig.3:Quantiﬁcationoftheproportionofdifferentparts’executiontimeinmodeltrainingAlthoughprevioussamplingmethodsprovideagreatleaptospeedupthetrainingofGNN,theyarestilllimitedbythegapbetweensamplingalgorithmsandhardware.Duetodifferentresearchbackgrounds,algorithmdesignersdonotconsiderthehardwarecharacteristicsandonlyimprovesam-plingalgorithmsatthealgorithmlevel.Hardwaredesignershavenotprovidedhardwaresupportforspeciﬁcalgorithmssincetheydonotknowspeciﬁcalgorithmimplementations.Therefore,miningoftheimprovementspaceforsamplingal-gorithmsisrestrictedbythegap.Ontheotherhand,leveraginghardwarecharacteristicshelpsdealwithgraph-relevanttasks.Forinstance,NeuGraph[11]designsastreamingschedulerthatmakesthebestofthecharacteristicsofCPUandGPUtoacceleratetheprocessingoflarge-scalegraphdata.GraphACT[26]designsanacceleratoronheterogeneousplatformsthatleveragesthecharacteristicsofCPUandFPGAtospeedupGCNtraining.Wearguethatsamplingisalsoaprocessofalgorithmandhardwarecoordination.Byleveraginghardwarecharacteristics,somesubstepsinsamplingcanbemoreefﬁcientafteropti-mizing,suchassearchingneighborsintheadjacencymatrixandaccessingnodes’embedding.Wealsoobservethattheexistingsamplingalgorithmsareusuallybasedondifferentmechanisms.Anefﬁcientimprovementshouldbeuniversaltomostsamplingalgorithms,urgingthedemandtoputforwardageneralsamplingﬂow.Tosupportourargument,weproposeGNNSampler,auniﬁedprogrammingmodelforsamplingalgorithms.Moreover,weoptimizethesamplingprocesslever-aginghardwarecharacteristicsandintegrateGNNSamplerintoageneralframeworkforefﬁcientGNNtraining.AdetaileddescriptionofGNNSamplerisgiveninSectionIII-B.Algorithm1:Abstractionfornode-wisesampling1INPUTGraphG(V,E,X)2OUTPUTadjacencymatrix3getminibatch(G)4S←getsamplestandard()5foreachbatchBkdo6inputs←getbatchinput(Bk)7foreachlayerldo8nl←getsamplenum(l);9foreachnodevinlayerldo10neiidx←getneighbor(inputs)11sampledidx←samplenode(neiidx,nl,S)12updateadjacencymatrix(sampledidx)13end14end15endAlgorithm2:Abstractionforlayer-wisesampling1INPUTGraphG(V,E,X)2OUTPUTadjacencymatrix3getminibatch(G)4S←getsamplestandard()5foreachbatchBkdo6inputs←getbatchinput(Bk)7foreachlayerldo8nl←getsamplenum(l)9P←getprobdistribution()10sampledidx←samplenode(inputs,nl,S,P)11updateadjacencymatrix(sampledidx)12inputs←updateinput(sampledidx)13end14endAlgorithm3:Abstractionforsubgraphsampling1INPUTGraphG(V,E,X)2OUTPUTadjacencymatrix3getminibatch(G)4S←getsamplestandard()5foreachbatchBkdo6inputs←getbatchinput(Bk)7P←getprobdistribution()8whilenodenum≤subgraphsizedo9sampledidx←samplenode(inputs,S,P)10nodepool←appendnode(sampledidx)11end12adjacencymatrix←processing(nodepool)13inducesubgraph(adjacencymatrix)14endIII.PROGRAMMINGMODELInthissection,weabstractsamplingalgorithmsindifferentcategoriesanddemonstratesthedesignofGNNSampler.A.SamplingalgorithmsabstractionTomaketheprogrammingmodelcompatiblewithdifferentsamplingalgorithms,weextractthekeyproceduresofsam-plingalgorithmsineachcategorybasedontheircodeimple-mentations.ThepseudocodesaredemonstratedinFig.??.(c)Abstractionforsubgraph-basedsamplingFig.4:Abstractionforsamplingmethodsineachcategorygeneralsamplingprocessleveraginghardwarefeatures,i.e.,localityamongneighbors,andintegrateGNNSamplerintoageneralframeworkforefﬁcientGNNtraining.Algorithm1:Abstractionfornode-wisesampling1INPUTGraphG(V,E,X)2OUTPUTadjacencymatrix3getminibatch(G)4S←getsamplestandard()5foreachbatchBkdo6inputs←getbatchinput(Bk)7n←getsamplenum();8foreachnodevinbatchinputsdo9neiidx←getneighbor(v)10sampledidx←sampleneighbor(neiidx,n,S)11updateadjacencymatrix(sampledidx)12end13endAlgorithm2:Abstractionforlayer-wisesampling1INPUTGraphG(V,E,X)2OUTPUTadjacencymatrix3getminibatch(G)4S←getsamplestandard()5foreachbatchBkdo6inputs←getbatchinput(Bk)7foreachlayerldo8nl←getsamplenum(l)9P←getprobdistribution()10sampledidx←samplenode(inputs,nl,S,P)11updateadjacencymatrix(sampledidx)12inputs←updateinput(sampledidx)13end14endAlgorithm3:Abstractionforsubgraphsampling1INPUTGraphG(V,E,X)2OUTPUTadjacencymatrix3getminibatch(G)4S←getsamplestandard()5foreachbatchBkdo6inputs←getbatchinput(Bk)7P←getprobdistribution()8whilenodenum≤subgraphsizedo9sampledidx←samplenode(inputs,S,P)10nodepool←appendnode(sampledidx)11end12adjacencymatrix←processing(nodepool)13inducesubgraph(adjacencymatrix)14endIII.PROGRAMMINGMODELInthissection,weabstractsamplingalgorithmsindifferentcategoriesanddemonstratethedesignofGNNSampler.A.SamplingAlgorithmsAbstractionTomaketheprogrammingmodelcompatiblewithdiffer-entsamplingalgorithms,weextractthekeyproceduresofsamplingalgorithmsineachcategorybasedontheircodeimplementations.ThepseudocodesaredemonstratedinFig.4.AbstractionforNode-wiseSamplingAlgorithms.Algorithm1describesthepseudocodeabstractedfromnode-wisesamplingalgorithmswheretheentiregraphGisprovidedasinput.Theoutputisanadjacencymatrixforrepresentingconnectionsbetweensamplednodes.Lines3-4correspondtotheinitializationphaseofsampling.Inthisphase,nodesneededfortrainingareﬁrstdividedintomultiplemini-batches.Somekeyfactors,suchasneighborreplacementstrategyandsamplingrate,arethenobtainedasthesamplingstandard.Lines5-15correspondtotheexecutionphaseofsam-pling,whichcontainsathree-layerloopfordenotingcorestepsofnode-wisesamplingmethods.Speciﬁcally,line6transformsnodesinamini-batchintoindexlist.Nextup,thesamplingsizeforeachnodeinthecurrentlayerisobtainedbeforeperformingneighborsampling.Then,neighborsamplingisexecutedoneachnodeinthecurrentlayertogetindicesofneighborsleveragingsomepre-calculatedparameters,and(a)Abstractionfornode-wisesamplingmatricesconstructionandmini-batchesgeneration.Thepro-portionoftrainingonlycontainsnecessarystepsfortrainingmodel,whichexcludesmodelevaluation.Foreachsamplingmethod,weconsidertheproportionofsamplingtimeinthesmallestdataset(e.g.,CoraandPPI)asthebaselineandplotthegrowthtrendofsamplingproportionineachdataset.Explicitly,theproportionofsamplingtimetototaltrainingtimebecomeslargerasthedataset’ssizeincreases.Layer-1Layer-2Layer-3TotalHyGCN0.0330580.0130310.0079160.054006100CoraC2A-only0.0272260.0173470.0067230.05129794.98392PubmedBifusion0.0272260.0173470.0067230.05129794.98392RedditHyGCN0.0329440.0107350.0078880.05156795.48427CoraC2A-only0.0272260.0129860.0096720.04988592.36937PubmedBifusion0.0272260.0107350.0096720.04763388.2003RedditPPIRedditGraphSAINTTime statisticsGCN-CDGCN-FLGraphSAGEFastGCN01020304050020406080100120GraphSAGEFastGCNGraphSAINTSampling Growth RatioProportion of Time Cost in Different Part of Training (%) SamplingTrainingOthersSampling GrowthFig.3:Quantiﬁcationoftheproportionofdifferentparts’executiontimeinmodeltrainingAlthoughprevioussamplingmethodsprovideagreatleaptospeedupthetrainingofGNN,theyarestilllimitedbythegapbetweensamplingalgorithmsandhardware.Duetodifferentresearchbackgrounds,algorithmdesignersdonotconsiderthehardwarecharacteristicsandonlyimprovesam-plingalgorithmsatthealgorithmlevel.Hardwaredesignershavenotprovidedhardwaresupportforspeciﬁcalgorithmssincetheydonotknowspeciﬁcalgorithmimplementations.Therefore,miningoftheimprovementspaceforsamplingal-gorithmsisrestrictedbythegap.Ontheotherhand,leveraginghardwarecharacteristicshelpsdealwithgraph-relevanttasks.Forinstance,NeuGraph[11]designsastreamingschedulerthatmakesthebestofthecharacteristicsofCPUandGPUtoacceleratetheprocessingoflarge-scalegraphdata.GraphACT[26]designsanacceleratoronheterogeneousplatformsthatleveragesthecharacteristicsofCPUandFPGAtospeedupGCNtraining.Wearguethatsamplingisalsoaprocessofalgorithmandhardwarecoordination.Byleveraginghardwarecharacteristics,somesubstepsinsamplingcanbemoreefﬁcientafteropti-mizing,suchassearchingneighborsintheadjacencymatrixandaccessingnodes’embedding.Wealsoobservethattheexistingsamplingalgorithmsareusuallybasedondifferentmechanisms.Anefﬁcientimprovementshouldbeuniversaltomostsamplingalgorithms,urgingthedemandtoputforwardageneralsamplingﬂow.Tosupportourargument,weproposeGNNSampler,auniﬁedprogrammingmodelforsamplingalgorithms.Moreover,weoptimizethesamplingprocesslever-aginghardwarecharacteristicsandintegrateGNNSamplerintoageneralframeworkforefﬁcientGNNtraining.AdetaileddescriptionofGNNSamplerisgiveninSectionIII-B.Algorithm1:Abstractionfornode-wisesampling1INPUTGraphG(V,E,X)2OUTPUTadjacencymatrix3getminibatch(G)4S←getsamplestandard()5foreachbatchBkdo6inputs←getbatchinput(Bk)7foreachlayerldo8nl←getsamplenum(l);9foreachnodevinlayerldo10neiidx←getneighbor(inputs)11sampledidx←samplenode(neiidx,nl,S)12updateadjacencymatrix(sampledidx)13end14end15endAlgorithm2:Abstractionforlayer-wisesampling1INPUTGraphG(V,E,X)2OUTPUTadjacencymatrix3getminibatch(G)4S←getsamplestandard()5foreachbatchBkdo6inputs←getbatchinput(Bk)7foreachlayerldo8nl←getsamplenum(l)9P←getprobdistribution()10sampledidx←samplenode(inputs,nl,S,P)11updateadjacencymatrix(sampledidx)12inputs←updateinput(sampledidx)13end14endAlgorithm3:Abstractionforsubgraph-basedsam-pling1INPUTGraphG(V,E,X)2OUTPUTadjacencymatrix3getminibatch(G)4S←getsamplestandard()5foreachbatchBkdo6inputs←getbatchinput(Bk)7P←getprobdistribution()8whilenodenum≤subgraphsizedo9sampledidx←samplenode(inputs,S,P)10nodepool←appendnode(sampledidx)11end12adjacencymatrix←processing(nodepool)13inducesubgraph(adjacencymatrix)14end(b)Abstractionforlayer-wisesamplingmatricesconstructionandmini-batchesgeneration.Thepro-portionoftrainingonlycontainsnecessarystepsfortrainingmodel,whichexcludesmodelevaluation.Foreachsamplingmethod,weconsidertheproportionofsamplingtimeinthesmallestdataset(e.g.,CoraandPPI)asthebaselineandplotthegrowthtrendofsamplingproportionineachdataset.Explicitly,theproportionofsamplingtimetototaltrainingtimebecomeslargerasthedataset’ssizeincreases.Layer-1Layer-2Layer-3TotalHyGCN0.0330580.0130310.0079160.054006100CoraC2A-only0.0272260.0173470.0067230.05129794.98392PubmedBifusion0.0272260.0173470.0067230.05129794.98392RedditHyGCN0.0329440.0107350.0078880.05156795.48427CoraC2A-only0.0272260.0129860.0096720.04988592.36937PubmedBifusion0.0272260.0107350.0096720.04763388.2003RedditPPIRedditGraphSAINTTime statisticsGCN-CDGCN-FLGraphSAGEFastGCN01020304050020406080100120GraphSAGEFastGCNGraphSAINTSampling Growth RatioProportion of Time Cost in Different Part of Training (%) SamplingTrainingOthersSampling GrowthFig.3:Quantiﬁcationoftheproportionofdifferentparts’executiontimeinmodeltrainingAlthoughprevioussamplingmethodsprovideagreatleaptospeedupthetrainingofGNN,theyarestilllimitedbythegapbetweensamplingalgorithmsandhardware.Duetodifferentresearchbackgrounds,algorithmdesignersdonotconsiderthehardwarecharacteristicsandonlyimprovesam-plingalgorithmsatthealgorithmlevel.Hardwaredesignershavenotprovidedhardwaresupportforspeciﬁcalgorithmssincetheydonotknowspeciﬁcalgorithmimplementations.Therefore,miningoftheimprovementspaceforsamplingal-gorithmsisrestrictedbythegap.Ontheotherhand,leveraginghardwarecharacteristicshelpsdealwithgraph-relevanttasks.Forinstance,NeuGraph[11]designsastreamingschedulerthatmakesthebestofthecharacteristicsofCPUandGPUtoacceleratetheprocessingoflarge-scalegraphdata.GraphACT[26]designsanacceleratoronheterogeneousplatformsthatleveragesthecharacteristicsofCPUandFPGAtospeedupGCNtraining.Wearguethatsamplingisalsoaprocessofalgorithmandhardwarecoordination.Byleveraginghardwarecharacteristics,somesubstepsinsamplingcanbemoreefﬁcientafteropti-mizing,suchassearchingneighborsintheadjacencymatrixandaccessingnodes’embedding.Wealsoobservethattheexistingsamplingalgorithmsareusuallybasedondifferentmechanisms.Anefﬁcientimprovementshouldbeuniversaltomostsamplingalgorithms,urgingthedemandtoputforwardageneralsamplingﬂow.Tosupportourargument,weproposeGNNSampler,auniﬁedprogrammingmodelforsamplingalgorithms.Moreover,weoptimizethesamplingprocesslever-aginghardwarecharacteristicsandintegrateGNNSamplerintoageneralframeworkforefﬁcientGNNtraining.AdetaileddescriptionofGNNSamplerisgiveninSectionIII-B.Algorithm1:Abstractionfornode-wisesampling1INPUTGraphG(V,E,X)2OUTPUTadjacencymatrix3getminibatch(G)4S←getsamplestandard()5foreachbatchBkdo6inputs←getbatchinput(Bk)7foreachlayerldo8nl←getsamplenum(l);9foreachnodevinlayerldo10neiidx←getneighbor(inputs)11sampledidx←samplenode(neiidx,nl,S)12updateadjacencymatrix(sampledidx)13end14end15endAlgorithm2:Abstractionforlayer-wisesampling1INPUTGraphG(V,E,X)2OUTPUTadjacencymatrix3getminibatch(G)4S←getsamplestandard()5foreachbatchBkdo6inputs←getbatchinput(Bk)7foreachlayerldo8nl←getsamplenum(l)9P←getprobdistribution()10sampledidx←samplenode(inputs,nl,S,P)11updateadjacencymatrix(sampledidx)12inputs←updateinput(sampledidx)13end14endAlgorithm3:Abstractionforsubgraphsampling1INPUTGraphG(V,E,X)2OUTPUTadjacencymatrix3getminibatch(G)4S←getsamplestandard()5foreachbatchBkdo6inputs←getbatchinput(Bk)7P←getprobdistribution()8whilenodenum≤subgraphsizedo9sampledidx←samplenode(inputs,S,P)10nodepool←appendnode(sampledidx)11end12adjacencymatrix←processing(nodepool)13inducesubgraph(adjacencymatrix)14endIII.PROGRAMMINGMODELInthissection,weabstractsamplingalgorithmsindifferentcategoriesanddemonstratesthedesignofGNNSampler.A.SamplingalgorithmsabstractionTomaketheprogrammingmodelcompatiblewithdifferentsamplingalgorithms,weextractthekeyproceduresofsam-plingalgorithmsineachcategorybasedontheircodeimple-mentations.ThepseudocodesaredemonstratedinFig.??.(c)Abstractionforsubgraph-basedsamplingFig.4:Abstractionforsamplingmethodsineachcategorygeneralsamplingprocessleveraginghardwarefeatures,i.e.,localityamongneighbors,andintegrateGNNSamplerintoageneralframeworkforefﬁcientGNNtraining.III.PROGRAMMINGMODELInthissection,weabstractsamplingalgorithmsindifferentcategoriesanddemonstratethedesignofGNNSampler.A.SamplingAlgorithmsAbstractionTomaketheprogrammingmodelcompatiblewithdiffer-entsamplingalgorithms,weextractthekeyproceduresofsamplingalgorithmsineachcategorybasedontheircodeimplementations.ThepseudocodesaredemonstratedinFig.4.AbstractionforNode-wiseSamplingAlgorithms.Algorithm1describesthepseudocodeabstractedfromnode-wisesamplingalgorithmswheretheentiregraphGisprovidedasinput.Theoutputisanadjacencymatrixforrepresentingconnectionsbetweensamplednodes.Lines3-4correspondtotheinitializationphaseofsampling.Inthisphase,nodesneededfortrainingareﬁrstdividedintomultiplemini-batches.Somekeyfactors,suchasneighborreplacementstrategyandsamplingrate,arethenobtainedasthesamplingstandard.Lines5-15correspondtotheexecutionphaseofsam-pling,whichcontainsathree-layerloopfordenotingcorestepsofnode-wisesamplingmethods.Speciﬁcally,line6transformsnodesinamini-batchintoindexlist.Nextup,thesamplingsizeforeachnodeinthecurrentlayerisobtainedbeforeperformingneighborsampling.Then,neighborsamplingisexecutedoneachnodeinthecurrentlayertogetindicesofneighborsleveragingsomepre-calculatedparameters,andtheadjacencymatrixissubsequentlyconstructedaccordingtothesampledneighbors’indices.Finally,theadjacencymatrixisupdatedperiodicallyafterneighborsamplingbasedontheconnectionsbetweenthesamplednodes.AbstractionforLayer-wiseSamplingAlgorithms.Algorithm2describesthepseudocodeabstractedfromlayer-wisesamplingalgorithmswheretheinputandoutputareconsistentwithAlgorithm1.Inordertobeasuniﬁedaspossible,weusethesamenameforsubstepsintheabstractionoflayer-wisesamplingalgorithmsasAlgorithm1.Lines5-15containatwo-layerloop.Indicesofinputnodesandthesam-plingsizeinthecurrentlayerareobtainedaheadofperformingsampling.Distinctively,layer-wisesamplingsdonotneedtofocusoneverynodesincetheysampleaﬁxednumberofnodestogetherineachlayerbasedonthepre-calculatedprobabilitydistribution.Speciﬁcally,line9correspondstothecalculationofprobabilitydistribution.Line10representsthatnlnodesaresampledtogetherleveragingsomepre-calculatedparameters.Generally,samplednodes’indicesareusedtoupdatetheadjacencymatrix.Forsomelayer-wisesamplingalgorithms(e.g.,AS-GCN[22])wherenodesaresampledaccordingtoparentnodes(alreadysamplednodes)intheupperlayer,thesamplednodes’indicesarefurtherusedtoupdatetheinputofthesamplingloopinthenextlayer.AbstractionforSubgraph-basedSamplingAlgorithms.Algorithm3describesthepseudocodeabstractedfromsubgraph-basedsamplingalgorithmswheretheentiregraphG,especiallynodesandcorrespondingindicesfortraining,areprovidedasinput.Theoutputisanadjacencymatrixgeneratedfromsamplednodes(edges)forinducingasubgraph.Lines3-4correspondtotheinitializationphaseofsampling.Notably,mini-batchesobtainedinline3correspondtothetrainingnodespartitionedrandomly,andthenumberofmini-batchesdependsonthenumberofsubgraphsbeingsampled.Distin-guishedfromAlgorithm1andAlgorithm2,theprobabilitydistributionforsamplingisonlycalculatedonceperbatch.Lines8-11correspondtothemainloopofsampling,anditdoesnotstopuntilthenumberofsamplednodesequalsthenumberneededtoformasubgraph.Thesamplednodesareaddedtoalist(vector)fortransitorystorage.Thefunctionnamed“processing”inline12denotesfurtherprocessaftersampling,e.g.,nodes’indicessortandremovingtheduplicatenodes.Finally,asubgraphisinducedfromtheadjacencymatrixgeneratedbysamplednodes.B.GNNSamplerDesignObservations:Todesignageneralprogrammingmodelforsampling,weﬁrstextractthekeystepsofsamplingalgorithmsinallcategoriesandputforwardtheabstractions.Andwehavethefollowingobservations.OB-1):Allthesemethodshaveapreprocessingphasetoprovidethenecessaryinformationforsampling.OB-2):Allthesemethodssplitnodesfortrainingusingamini-batchscheme.OB-3):AllthesemethodsrequireGNNSampler: Bridging GNN Sampling and Hardware

7

Fig. 5. (a) Abstraction for subgraph-based sampling algorithms. (b) Pseudocode of
GNNSampler. Please note that, in subplot (b), we add a particular step, termed “con-
struct_locality”, to the INIT stage to exploit the data locality among nodes and their
neighbors in a graph. In the EXECUTE stage, sampling can be executed with less
irregular memory access since the sampling weight is computed according to the data
locality among nodes.

sampled according to parent nodes (already sampled nodes) in the upper layer,
the indices are further used to update the input of sampling for the next layer.
• Abstraction for Subgraph-based Sampling Algorithms.

In Figure 5 (a), mini-batches obtained in line 3 of Algorithm 3 correspond
to the training nodes partitioned artiﬁcially or with a clustering algorithm. Lines
8-11 correspond to the main loop of sampling, and it does not stop until the
number of sampled nodes equals the number needed to form a subgraph. The
sampled nodes are temporarily stored in a “node_pool”. The function named
“processing” in line 12 denotes a further process after sampling, e.g., sorting
nodes’ indices and removing the duplicate nodes. Finally, a subgraph is induced
from the adjacency matrix generated by sampled nodes. Multiple subgraphs are
generated by repeating the above process.

3.2 GNNSampler and Workﬂow

Design: Based on the above abstractions, we propose the uniﬁed programming
model, i.e. GNNSampler, in Figure 5 (b). We divide the sampling process into
two stages, namely INIT and EXECUTE. The target of the INIT stage is to
obtain necessary data, e.g., batched nodes, in advance for the EXECUTE stage.
In the EXECUTE stage, line 7 denotes that information of model structure is
obtained to help conﬁgure the sampling. In line 8, the obtained metrics denote
critical factors for sampling, e.g., sampling size and probability. We also add the
inﬂuence of data locality in the calculation of the sampling probability (i.e., L
that computed by function construct_locality). Lines 9-12 denote an iterative
sampling process requiring signiﬁcant computation and storage resources. Fi-
nally, the batched adjacency matrix is updated after the batched sampling. And

(a)Abstractionforsubgraph-basedsampling(b)PseudocodeofGNNSampler(c)Abstractionforsubgraph-basedsamplingFig.4:Abstractionforsamplingmethodsineachcategorygeneralsamplingprocessleveraginghardwarefeatures,i.e.,localityamongneighbors,andintegrateGNNSamplerintoageneralframeworkforefﬁcientGNNtraining.III.PROGRAMMINGMODELInthissection,weabstractsamplingalgorithmsindifferentcategoriesanddemonstratethedesignofGNNSampler.A.SamplingAlgorithmsAbstractionTomaketheprogrammingmodelcompatiblewithdiffer-entsamplingalgorithms,weextractthekeyproceduresofsamplingalgorithmsineachcategorybasedontheircodeimplementations.ThepseudocodesaredemonstratedinFig.4.AbstractionforNode-wiseSamplingAlgorithms.Algorithm1describesthepseudocodeabstractedfromnode-wisesamplingalgorithmswheretheentiregraphGisprovidedasinput.Theoutputisanadjacencymatrixforrepresentingconnectionsbetweensamplednodes.Lines3-4correspondtotheinitializationphaseofsampling.Inthisphase,nodesneededfortrainingareﬁrstdividedintomultiplemini-batches.Somekeyfactors,suchasneighborreplacementstrategyandsamplingrate,arethenobtainedasthesamplingstandard.Lines5-15correspondtotheexecutionphaseofsam-pling,whichcontainsathree-layerloopfordenotingcorestepsofnode-wisesamplingmethods.Speciﬁcally,line6transformsnodesinamini-batchintoindexlist.Nextup,thesamplingsizeforeachnodeinthecurrentlayerisobtainedbeforeperformingneighborsampling.Then,neighborsamplingisexecutedoneachnodeinthecurrentlayertogetindicesofneighborsleveragingsomepre-calculatedparameters,andtheadjacencymatrixissubsequentlyconstructedaccordingtothesampledneighbors’indices.Finally,theadjacencymatrixisupdatedperiodicallyafterneighborsamplingbasedontheconnectionsbetweenthesamplednodes.AbstractionforLayer-wiseSamplingAlgorithms.Algorithm2describesthepseudocodeabstractedfromlayer-wisesamplingalgorithmswheretheinputandoutputareconsistentwithAlgorithm1.Inordertobeasuniﬁedaspossible,weusethesamenameforsubstepsintheabstractionoflayer-wisesamplingalgorithmsasAlgorithm1.Lines5-15containatwo-layerloop.Indicesofinputnodesandthesam-plingsizeinthecurrentlayerareobtainedaheadofperformingsampling.Distinctively,layer-wisesamplingsdonotneedtofocusoneverynodesincetheysampleaﬁxednumberofnodestogetherineachlayerbasedonthepre-calculatedprobabilitydistribution.Speciﬁcally,line9correspondstothecalculationofprobabilitydistribution.Line10representsthatnlnodesaresampledtogetherleveragingsomepre-calculatedparameters.Generally,samplednodes’indicesareusedtoupdatetheadjacencymatrix.Forsomelayer-wisesamplingalgorithms(e.g.,AS-GCN[22])wherenodesaresampledaccordingtoparentnodes(alreadysamplednodes)intheupperlayer,thesamplednodes’indicesarefurtherusedtoupdatetheinputofthesamplingloopinthenextlayer.AbstractionforSubgraph-basedSamplingAlgorithms.Algorithm3describesthepseudocodeabstractedfromsubgraph-basedsamplingalgorithmswheretheentiregraphG,especiallynodesandcorrespondingindicesfortraining,areprovidedasinput.Theoutputisanadjacencymatrixgeneratedfromsamplednodes(edges)forinducingasubgraph.Lines3-4correspondtotheinitializationphaseofsampling.Notably,mini-batchesobtainedinline3correspondtothetrainingnodespartitionedrandomly,andthenumberofmini-batchesdependsonthenumberofsubgraphsbeingsampled.Distin-guishedfromAlgorithm1andAlgorithm2,theprobabilitydistributionforsamplingisonlycalculatedonceperbatch.Lines8-11correspondtothemainloopofsampling,anditdoesnotstopuntilthenumberofsamplednodesequalsthenumberneededtoformasubgraph.Thesamplednodesareaddedtoalist(vector)fortransitorystorage.Thefunctionnamed“processing”inline12denotesfurtherprocessaftersampling,e.g.,nodes’indicessortandremovingtheduplicatenodes.Finally,asubgraphisinducedfromtheadjacencymatrixgeneratedbysamplednodes.B.GNNSamplerDesignObservations:Todesignageneralprogrammingmodelforsampling,weﬁrstextractthekeystepsofsamplingalgorithmsinallcategoriesandputforwardtheabstractions.Andwehavethefollowingobservations.OB-1):Allthesemethodshaveapreprocessingphasetoprovidethenecessaryinformationforsampling.OB-2):Allthesemethodssplitnodesfortrainingusingamini-batchscheme.OB-3):Allthesemethodsrequire8

X. Liu et al.

Fig. 6. (a) Workﬂow of
learning large-scale graphs with GNN embedded with
GNNSampler (some general processes, e.g., loss compute and model update, are not
specially shown). (b) An exemplar for sampling nodes leveraging hardware feature.

subsequent steps, e.g., subgraphs induction and model training, can directly use
the generated adjacency matrix.
Workﬂow: To embed GNNSamlper to GNN training, we ﬁrst introduce the
steps of pre-processing, sampling, aggregation, and update in GNN training,
where sampling is a tight connection between other steps. Figure 6 (a) illus-
trates the workﬂow of learning large-scale graphs with GNN, where GNNSam-
pler is embedded for optimizing sampling. To begin with, the graph data, such as
social network, is transformed into a topology graph and is further converted into
elements directly used during sampling, in the pre-processing step. 1 In the sam-
pling step, GNNSampler decomposes sampling into INIT and EXECUTE stages.
The processed elements are fed into the INIT stage to compute critical metrics
for sampling. In the EXECUTE stage, sampling is performed according to the
critical metrics and acquires an adjacency matrix. 2 In the aggregation step,
the aggregation feature (abbreviated as AGG. Feats.) of one node is aggregated
from features of the sampled nodes, after which a concatenation operation is ap-
plied to the AGG. Feats. and the representation feature (abbreviated as Repst.
Feats.) of the node in the upper layer. 3 In the update step, the Repst. Feats.
of one node is updated by transforming the weighted concatenate feature with
a nonlinear function [4]. The most critical one, i.e., the sampling process with
GNNSampler embedded, is designed to be universal for all categories of sampling
algorithms. Based on this universal design, we can propose a generic and highly
compatible optimization to beneﬁt all categories of sampling algorithms.

4 Case Study: Leveraging Locality

In this section, to leverage the hardware feature, we choose the data locality
as a case study and implement locality-aware optimizations in GNNSampler to
improve sampling. Please note that we refer to data locality as locality in brief
in the rest of the paper.

(a)Aggregation++++u vSocial NetworkTopology GraphLabels; Adjacent MatrixIndices; Input FeaturesLabels; Adjacent MatrixIndices; Input FeaturesGNNSamplerSamplingSamplingSubgraphBasedSubgraphBasedLayerWiseLayerWiseNodeWiseNodeWiseEXECUTE   EXECUTE   Update Adj.Sample NodeGet MetricsModel-Info.INIT   INIT   Node PartitionSample StandardGraph LocalityUpdatew Data FlowData FlowMappingMappingStepStepSampled NodesSampled NodesAGG. Feats.AGG. Feats.Repst. Feats.Repst. Feats.Data FlowMappingStepSampled NodesAGG. Feats.Repst. Feats.WeightWeightAggregation++++u vSocial NetworkTopology GraphLabels; Adjacent MatrixIndices; Input FeaturesGNNSamplerSamplingSubgraphBasedLayerWiseNodeWiseEXECUTE   Update Adj.Sample NodeGet MetricsModel-Info.INIT   Node PartitionSample StandardGraph LocalityUpdatew Data FlowMappingStepSampled NodesAGG. Feats.Repst. Feats.Weight(b)Locality-awareOptimizationA well-connected region of nodes is circled for analysis.8369411Randomly Distributed24252081094324411693222519197777242528249606015158888311320222569109111.........409420......109“Bad Neighbors”“Good Neighbors”4320222425Good Localityin Storage4320222425Good Localityin StoragePart of a Graph for TrainingVbVIN1VIN2VoutVDDVDDVbVIN1VIN2VoutVDDVDDGNNSampler: Bridging GNN Sampling and Hardware

9

Fig. 7. Illustration of two critical steps of the locality-aware optimization: generat-
ing good neighbors and calculating the similarity between real and generated good
neighbors. Please note that, this process is oﬄine performed for each training node.

4.1 Exploring Locality in Graph

Locality, i.e., principle of locality [19], is a particular tendency that processors
have more opportunity to access data in the same set of memory locations re-
peatedly during a shorter interval of time. Speciﬁcally, locality can be embodied
in two basic types: temporal locality for denoting the repeated access to the
same memory location within a short time, and spatial locality for denoting the
access to particular data within close memory locations.

Inspired by the success of locality exploitation in graph processing [20], we
exploit locality to alleviate the irregular memory access in the sampling which
helps reduce the execution time. Figure 6 (b) gives an exemplar for analyzing
neighbors of two nodes (i.e., “No.8” and “No.43”) in a well-connected region.
Neighbors of node “No.8” are randomly distributed, whilst neighbors of node
“No.43” are almost contiguous. Notably, neighbors of “No.43” have a greater
probability of being accessed within a short time according to the feature of
locality since they are stored in adjacent location in memory. We would sample
node “No.43” rather than “No.8”.

Moreover, a new graph is constructed before sampling ends based on the
sampled nodes in some cases. In this process, all nodes for training are required
to search whether their neighbors are stored in the pool of the sampled nodes. For
a node like “No.43”, it is immediate to search the adjacent locations in the pool
to verify their connections and add connections in storage (e.g., CSR format).
When the graph is large, locality-aware optimizations can reduce considerable
time cost in the above process since irregular memory accesses in searching
neighbors are avoided as possible.

4.2

Implementations of Locality-aware Optimization

Design ﬂow: The distribution of “No.43” and its neighbors is an exactly ideal
case in a graph. In most situations, nodes are randomly distributed in gen-
eral. Therefore, to estimate which node is suitable for sampling, we design two
modules, i.e., a generator to yield (virtual) good neighbors and a calculator to

24252081094324411693222519197777242528249606015158888311“Good Neighbors”Part of a Graph for TrainingNeighbor GeneratorSimilarity CalculatorOffline ExecutionLoop for Training Nodes10

X. Liu et al.

Fig. 8. Pseudocode of locality-aware optimizations.

compute the similarity between real neighbors and the generated good neigh-
bors. As illustrated in Figure 7, for one node v used in training, based on our
prescript of good neighbors, i.e., neighbors are contiguous and adjacently located
in memory, we use a generator to yield good neighbors for the given v. However,
there may exist a gap between real neighbors and the generated good neighbors.
We then utilize a calculator to quantify the gap. Speciﬁcally, we calculate the
similarity between two neighbor sequences via a dot product ratio scheme:

Similarityv =

n−1
i=0 nv[i] · gnv[i]
n−1
i=0 n2

v[i]

(5)

(cid:80)

(cid:80)
where nv and gnv denote sequences of real and the generated good neighbors of v.
Through evaluation, we discover that the calculated similarity is generally larger
as the resemblance of sequences’ distributions increases. By setting a suitable
threshold for the similarity, nodes whose real neighbors meet our standard (i.e.,
exceed the threshold) are chosen for sampling. The sampling process is uniformly
performed (i.e., the sampling probability is uniform) on these nodes.
Implementation: We implement locality-aware optimizations for all categories
of sampling algorithms. For node-wise and subgraph-based sampling algorithms,
we use the function “construct_locality” given in Figure 8 to construct locality

TitleSuppressedDuetoExcessiveLength11Algorithm5:Locality-awareOptimizations1INPUTAdjacencymatrixfortraining:adj_train;Similaritythreshold:s;Minimumnumberofneighbors:n2OUTPUTLocality-basedsamplingweight:L3Functiongood_neighbor_generation(neighborsofv:neiv)4avgv←get_average_neighbor_idx(neiv)5num_neiv←get_neighbor_number(neiv)6good_neiv←[···,avgv−k×step,avgv,avgv+k×step,···](wherek=1,2,···,num_neiv/2,andstepcanbespecified)7Returngood_neiv8Functionconstruct_locality(adj_train,s,n)9L←Initialize(adj_train)10foreachnodevintrainingnodesetdo11neiv←get_neighbor(adj_train,v)12ifnumberofneiv<nthen13L←update_weight(0,v)14end15else16good_neiv←good_neighbor_generation(neiv)17similarityv←Pn−1i=0(neiv[i]×good_neiv[i])/Pn−1i=0neiv[i]218ifsimilarityv>sthen19L←update_weight(1,v)20end21else22L←update_weight(0,v)23end24end25end26ReturnLGNNSampler: Bridging GNN Sampling and Hardware

11

among nodes and generate sampling weight. The input are the adjacency matrix,
the minimum number of neighbors per node (abbreviated as n), and the similar-
ity threshold (abbreviated as s). n is used to ﬁlter nodes with sparse connections,
and s is used to measure the quality of locality among neighbors of each node by
comparing the similarity between real neighbors and the generated good neigh-
bors. The function “good_neighbor_generation” is used to generate neighbor.
The output of the function “construct_locality” is the locality-based sampling
weight L ﬁlled with “0” or “1” value which is used to represent whether a node v
is suitable to be sampled. The subsequent sampling is performed based on L. By
this means, nodes whose neighbors are stored closely in memory (i.e., with good
locality) are more likely to be sampled, helping alleviate irregular data access.
For layer-wise sampling algorithms, number of nodes to be sampled in each layer
is relatively small and ﬁxed for all datasets, which tends to form sparse connec-
tions between layers, especially in large datasets. We thereby explore leveraging
the number of neighbors and previously sampled nodes to construct locality in
two continuous layers. We ﬁrst initialize a weight vector L for training nodes
and set lv as the weight in the corresponding position in L, where lv is directly
proportional to the number of v’s neighbors. Moreover, nodes sampled in the
upper layer are partly added to the candidate set to be sampled in the current
layer to increase the sampling probability of the frequently accessed nodes.
Once-for-all: The proposed optimization is ﬂexible to be embedded in the pre-
processing step of mainstream sampling-based models. Moreover, two parame-
ters, i.e., n and s, can be adaptively adjusted to achieve the desired trade-oﬀ.
Notably, the computation of L merely requires the connections among training
nodes and their neighbors (i.e., an adjacency matrix for training: adj_train) and
can be performed oﬄine. The pre-computed L can be reused in each batch
of sampling, making the computation of L a once-for-all process for each
dataset. Please refer to our code 1 for more details.

5 Experiment

To analyze the advance of our method, we conduct experiments on all categories
of sampling algorithms to compare vanilla training methods (i.e., original mod-
els) and our improved approaches with locality-aware optimizations.

5.1 Experimental Setup

Since the GNNSamper is general and compatible with mainstream sampling
algorithms, we choose sampling algorithms in all categories as representatives,
including GraphSAGE [4] (node-wise sampling), FastGCN [8] (layer-wise sam-
pling), and GraphSAINT [11] (subgraph-based sampling). For all sampling-based
models, we use their oﬃcial conﬁgurations in both sampling and train-
ing, especially batch size, sampling size, and learning rate, to guarantee simi-
lar performance compared to their reported values. The basic GCN used in all

1 https://github.com/TeMp-gimlab/GNNSampler

12

X. Liu et al.

(a) Comparison of training time

(b) Comparison of validation accuracy

Fig. 9. Comparisons between vanilla and our optimized approaches (normalized to
vanilla) among three models using various datasets.

Table 1. Comprehensive analysis on the datasets.

Dataset
Pubmed [15]
PPI [21]
Flickr [11]
Reddit [4]
Amazon [11]

#Node
19717
14755
89250
232965
1598960

#Edge
44338
225270
899756
11606919
132169734

ANN MNN

4
15
5
50
85

12
39
9
113
101

NRR
77.76%
93.54%
80.16%
97.99%
98.82%

cases is a two-layer model. For GraphSAINT, we choose the serial node sam-
pler implemented via Python to randomly sample nodes for inducing subgraphs.
These sampling-based models are regarded as comparison baselines, and we ap-
ply locality-aware optimizations to these models for improvement. By referring
to the practice of previous works, we mainly focus on ﬁve benchmark datasets
distinguishing in graph size and connection density as shown in Table 1. All
experiments are conducted on a Linux server equipped with dual 14-core Intel
Xeon E5-2683 v3 CPUs and an NVIDIA Tesla V100 GPU (16 GB memory).

5.2 Experimental Result and Analysis

Preliminary: We ﬁrst analyze the datasets in multiple aspects. As given in
Table 1, we make statistics on the (round-oﬀ) average number of neighbors per
node (ANN) and the maximum number of neighbors of 90% nodes (MNN)
in datasets to reﬂect density of connections among nodes. We count neighbor
reusing rate (NRR) by calculating the number of reused neighbors as a propor-
tion of the total number of neighbors of all nodes. The collected statistics will
help establish a relationship among attributes (e.g., size, density of connection)
of graph datasets and experimental results.
Result: As illustrated in Figure 9 (a), we compare the converged training time
and validation accuracy on diverse sampling-based models and datasets. The
overall average time reduction is 13.29% with a 2.74% average accuracy loss.
Speciﬁcally, the average time reduction on GraphSAGE and FastGCN is 7.06%
and 7.14%. Notability, the optimization achieves an average 24.67% time reduc-
tion in GraphSAINT, while the peak of time reduction is 44.62% on Amazon

0%10%20%30%40%50%00.20.40.60.811.2GraphSAGEFastGCNGraphSAINTTime Reduction RatioTraining Time VanillaOursTime Reduction Ratio0%1%2%3%4%5%6%00.20.40.60.81GraphSAGEFastGCNGraphSAINTAccuracy Loss Ratio Validation AccuracyVanillaOursAccuracy Loss RatioGNNSampler: Bridging GNN Sampling and Hardware

13

Table 2. Training time and accuracy comparisons on diﬀerent sampling algorithms
between vanilla and optimized approaches. Please note that, parameters n and s denote
the minimum number of neighbors per node and the similarity threshold, respectively.
Related content has been detailedly discussed in Section 4.2.

Model

Dataset

GraphSAGE

FastGCN

GraphSAINT

PPI
Reddit
Pubmed
Flickr
Reddit
Flickr
Reddit
Amazon

Sampling
Size
25 & 10
25 & 10
100
100
100
8000
4000
4500

Training Time
(Vanilla / Ours)
34.97s / 31.89s
299.21s / 283.40s
35.84s / 33.30s
110.29s / 103.34s
671.60s / 639.16s
16.04s / 14.39s
229.47s / 185.67s
2263.92s / 1253.33s

Time
Reduction
8.81%
5.28%
7.09%
6.30%
4.83%
10.29%
19.09%
44.64%

Param.
(n / s)
4/0.95
4/0.87
-
-
-
3/0.865
3/0.77
4/0.775

Accuracy
Loss
2.41%
2.48%
1.93%
0.06%
0.48%
5.25%
4.51%
4.83%

dataset. We also observe a trivial decline in accuracy in Figure 9 (b), which
varies by model. Considering characteristics of locality, nodes whose neighbors
are adjacently distributed have a higher probability of being sampled, which
yields a non-uniform sampling distribution. Therefore, biased sampling can re-
sult in a sacriﬁce in accuracy despite the considerable time reduction. Detailed
performance and parameters (n and s) are given in Table 2.
Analysis: Based on the result, we analyze the relevance among the training time,
accuracy, and hardware-level metrics, and summarize our ﬁndings as follows:
• Time reduction introduced by the optimization varies by dataset
because of the distinct sampling size and graph scale. Distinctly, the
percentage of time reduction in GraphSAINT is larger than in GraphSAGE and
FastGCN, since the sampling size used in GraphSAINT is quite large. Moreover,
for GraphSAINT, the percentage of time reduction on Amazon dataset is larger
than on Flickr and Reddit since Amazon has an enormous graph scale (amount
of nodes & edges). We also argue that higher NRR is another reason for achiev-
ing signiﬁcant time reduction on Amazon. As shown in Table 1, NRR is a
metric to reﬂect neighbor reusing rate. Generally, a dataset with higher NRR
includes node regions in which multiple nodes have many common neighbors. If
neighbors of one node are frequently accessed, it is likely that for other nodes in
such regions, their neighbors are also frequently accessed since they share many
common neighbors. Thus, locality among nodes is more easier to explore in this
case. Consequently, models using large sampling sizes and large-scale graphs are
more likely to beneﬁt from locality-aware optimizations.
• A good trade-oﬀ between training time and accuracy can be achieved
by adjusting parameters n and s. Since we merely retain the “good nodes” for
sampling, our target is to ﬁnd a trade-oﬀ point with considerable time reduction
and tolerable accuracy loss. As illustrated in the left subplot of Figure 10(a), as
s increases (with n ﬁxed), we have a tighter restriction on the quality of locality
among neighbors per node, causing the nodes to be sampled are a minor part of
total. This leads to accuracy loss since reducing nodes implies losing connections
in a graph. Moreover, we sample nodes with good locality to reduce irregular
memory access, indirectly saving training time. In the right subplot of Figure

14

X. Liu et al.

(a)

(b)

Fig. 10. (a) Exploration of the trade-oﬀ between training time and accuracy in Graph-
SAINT on Reddit by adjusting n & s. (b) Comparison of the number of data access.

10(a), as n increases (with s ﬁxed), the training time is generally increasing
before reaching a peak. When n is set to 5, we can obtain a competitive accuracy
with an undesirable training time, implying a compromise of choosing a smaller
n is acceptable under variations in the accuracy are trivial. Thus, there is a
correlation between training time and accuracy. By adjusting the parameters,
one can derive a comparable accuracy with an acceptable training time.

• Alleviating irregular memory accesses to neighbors helps reduce the
training time. As shown in Figure 10 (b), we quantify the number of data access
from L2 cache to L3 cache (L2-L3) and L3 cache to DRAM (L3-DRAM) with
Intel PCM Tools [22] to analyze the sampling process. Deﬁnitely, locality-aware
optimizations can signiﬁcantly reduce the number of data access in L3-DRAM
under the condition that L2-L3 is almost similar. By reducing data access to
DRAM, time of sampling is saved, which eventually accelerates the training.

• Locality can be empirically reﬂected by the topology of sampled
subgraphs. With locality-aware optimizations, we argue that subgraphs sam-
pled are more concentrated in the local structure, avoiding irregular or highly
stochastic pattern in the graph topology. To reﬂect such properties, we conduct
analysis via Stanford Network Analysis Platform (SNAP) [23] tools. Speciﬁ-
cally, by quantifying two metrics, i.e., the clustering coeﬃcient (CC) and the
number of closed triads (NCT), we analyze sampled subgraphs on Reddit using
GraphSAINT model. CC is a local property that measures the cliquishness of a
typical neighbourhood in a graph [24]. In Figure 11 (a), as the number of sam-
pled subgraphs increases, the average CC of Ours is 1.42X of Vanilla, implying
our sampled subgraphs are more concentrated and have a higher tendency of
clustering. NCT is a typical structure among three nodes with any two of them
connected, which is used to reﬂect to a balanced group pattern in social net-
works [25]. In Figure 11 (b), NCT of Ours is 1.18X of Vanilla, indicating highly
interconnected structures are sampled under locality-aware optimizations.

                 V                7 U D L Q L Q J  7 L P H   V H F   Q    7 U D L Q L Q J  7 L P H  R Q  5 H G G L W $ F F X U D F \  R Q  5 H G G L W                      $ F F X U D F \     Q                   7 U D L Q L Q J  7 L P H   V H F   V                            $ F F X U D F \0.960.5600.511.5L2-L3L3-DRAMRedditNumber of Data AccessVanillaOursGNNSampler: Bridging GNN Sampling and Hardware

15

Fig. 11. Comparisons between vanilla and our (use GNNSampler) cases on Reddit
among various metrics: (a) Comparisons on clustering coeﬃcient; (b) Comparisons on
the number of closed triads. Curves in subplots denote value variations of metrics.

6 Conclusion

In this paper, we propose a uniﬁed programming model for mainstream GNN
sampling algorithms, termed GNNSampler, to bridge the gap between sampling
algorithms and hardware. Then, to leverage hardware features, we choose locality
as a case study and implement locality-aware optimizations in algorithm level
to alleviate irregular memory access in sampling. Our work target to open up a
new view for optimizing sampling in the future works, where hardware should
be well considered for algorithm improvement.

Acknowledgment

This work was partly supported by the Strategic Priority Research Program of
Chinese Academy of Sciences (Grant No. XDA18000000), National Natural Sci-
ence Foundation of China (Grant No.61732018 and 61872335), Austrian-Chinese
Cooperative R&D Project (FFG and CAS) (Grant No. 171111KYSB20200002),
CAS Project for Young Scientists in Basic Research (Grant No. YSBR-029), and
CAS Project for Youth Innovation Promotion Association.

References

1. Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., & Monfardini, G.: The graph
neural network model. IEEE transactions on neural networks 20(1), 61–80 (2008).
2. Kipf, T. N., & Welling, M.: Semi-supervised classiﬁcation with graph convolutional

networks. In: ICLR (2017)

3. Schlichtkrull, M., Kipf, T. N., Bloem, P., Berg, R. V. D., Titov, I., & Welling,
M.: Modeling relational data with graph convolutional networks. In: European
semantic web conference (2018)

           1 X P E H U  R I  V D P S O H G  V X E J U D S K V                         & O X V W H U L Q J  & R H I I L F L H Q W   U D W L R  9 D Q L O O D  & D V H 8 V H  * 1 1 6 D P S O H U                                                 & O X V W H U L Q J  & R H I I L F L H Q W   Y D O X H            1 X P E H U  R I  V D P S O H G  V X E J U D S K V                      1 X P E H U  R I  & O R V H G  7 U L D G V   U D W L R                    1 X P E H U  R I  & O R V H G  7 U L D G V   Y D O X H   D                                                                E 16

X. Liu et al.

4. Hamilton, W., Ying Z., & Leskovec, J.: Inductive representation learning on large

graphs. Advances in neural information processing systems 30 (2017).

5. Liu, X., Yan, M., Deng, L., Li, G., Ye, X., et al.: Survey on graph neural network
acceleration: An algorithmic perspective. arXiv preprint arXiv:2202.04822 (2022)
6. Yan, M., Hu, X., Li, S., et al.: Alleviating irregularity in graph analytics accelera-

tion: A hardware/software co-design approach. In: MICRO (2019)

7. Chen, J., Zhu, J., & Song, L.: Stochastic training of graph convolutional networks

with variance reduction. In: ICML (2018).

8. Chen, J., Ma, T., & Xiao, C.: Fastgcn: fast learning with graph convolutional

networks via importance sampling. In: ICLR (2018)

9. Huang, W., Zhang, T., Rong, Y., & Huang, J.: Adaptive sampling towards fast
graph representation learning. Advances in neural information processing system
31, 4563–4572 (2018)

10. Chiang, W. L., Liu, X., Si, S., et al.: Cluster-gcn: An eﬃcient algorithm for training

deep and large graph convolutional networks. In: SIGKDD (2019)

11. Zeng, H., Zhou, H., Srivastava, A., Kannan, R., & Prasanna, V.: Graphsaint: Graph

sampling based inductive learning method. In: ICLR (2020)

12. Zeng, H., Zhou, H., Srivastava, A., Kannan, R., & Prasanna, V.: Accurate, eﬃcient

and scalable graph embedding. In: IPDPS (2019)

13. Zeng, H., Zhang, et al.: Decoupling the depth and scope of graph neural networks.
Advances in Neural Information Processing Systems 34, 19665-19679 (2021)
14. Liu, X., Yan, M., Deng, L., Li, G., Ye, X., & Fan, D.: Sampling methods for
eﬃcient training of graph convolutional networks: A survey. IEEE/CAA Journal
of Automatica Sinica 9(2), 205-234 (2021)

15. Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., & Eliassi-Rad, T.: Col-

lective classiﬁcation in network data. AI magazine 29(3), 93-93 (2008)

16. Ma, L., Yang, Z., Miao, Y., et al.: NeuGraph: Parallel Deep Neural Network Com-

putation on Large Graphs. In: USENIX ATC 19 (2019).

17. Yan, M., Deng, L., Hu, X., Liang, L., Feng, Y., Ye, X., Zhang, Z., Fan, D., Xie,

Y.: Hygcn: A gcn accelerator with hybrid architecture. In: HPCA (2020)

18. Yan, M., Chen, Z., Deng, L., et al.: Characterizing and understanding gcns on gpu.

IEEE Computer Architecture Letters 19(1), 22–25 (2020)

19. Denning, P.J.: The locality principle. In: Communication Networks And Computer

Systems: A Tribute to Professor Erol Gelenbe (2006)

20. Mukkara, A., Beckmann, N., Abeydeera, M., et al.: Exploiting locality in graph

analytics through hardware-accelerated traversal scheduling. In: MICRO (2018)

21. Zitnik, M., & Leskovec, J.: Predicting multicellular function through multi-layer

tissue networks. Bioinformatics 33(14), i190-i198 (2017)

22. Thomas, W., Roman, D.: Intel performance counter monitor - a better way to

measure cpu utilization. https://github.com/opcm/pcm (2018)

23. Leskovec, J., & Sosič, R.: Snap: A general-purpose network analysis and graph-
mining library. ACM Transactions on Intelligent Systems and Technology (TIST)
8(1), 1-20 (2016)

24. Watts, D.J., Strogatz, S.H.: Collective dynamics of ‘small-world’networks. nature

393(6684), 440–442 (1998)

25. Easley, D., & Kleinberg, J.: Networks, crowds, and markets: Reasoning about a

highly connected world. Cambridge university press (2010)

