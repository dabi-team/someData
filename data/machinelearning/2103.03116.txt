1
2
0
2

r
a

M
4

]

G
L
.
s
c
[

1
v
6
1
1
3
0
.
3
0
1
2
:
v
i
X
r
a

Universal Representation for Code

Linfeng Liu1(cid:63)((cid:66)), Hoan Nguyen2, George Karypis2, and Srinivasan Sengamedu2

1 Tufts University, Medford MA 02155, USA
linfeng.liu@tufts.edu
2 Amazon Web Services, Seattle WA 98109, USA
{hoanamzn, gkarypis, sengamed}@amazon.com

Abstract. Learning from source code usually requires a large amount
of labeled data. Despite the possible scarcity of labeled data, the trained
model is highly task-speciÔ¨Åc and lacks transferability to diÔ¨Äerent tasks.
In this work, we present eÔ¨Äective pre-training strategies on top of a novel
graph-based code representation, to produce universal representations
for code. SpeciÔ¨Åcally, our graph-based representation captures important
semantics between code elements (e.g., control Ô¨Çow and data Ô¨Çow). We
pre-train graph neural networks on the representation to extract universal
code properties. The pre-trained model then enables the possibility of
Ô¨Åne-tuning to support various downstream applications. We evaluate our
model on two real-world datasets ‚Äì spanning over 30M Java methods and
770K Python methods. Through visualization, we reveal discriminative
properties in our universal code representation. By comparing multi-
ple benchmarks, we demonstrate that the proposed framework achieves
state-of-the-art results on method name prediction and code graph link
prediction.

Keywords: Code representation ¬∑ Graph neural network ¬∑ Pre-training.

1

Introduction

Analysis of software using machine learning approaches has several important
applications such as identifying code defects [1], improving code search [2],
and improving developer productivity [3]. One common aspect of any code-
related application is that they learn code representations by following a two-
step process. The Ô¨Årst step takes code snippets and produces a symbolic code
representation using program analysis techniques. The second step uses the
symbolic code representation to generate neural code representations using deep
learning techniques.

Symbolic representations need to capture both syntactic and semantic struc-
tures in code. Approaches to generating symbolic representations can be catego-
rized as sequence-, tree-, and graph-based. Sequence-based approaches represent
code as a sequence of tokens and only capture the shallow and textual structures
of the code [4]. Tree-based approaches represent the code via abstract syntax

(cid:63) To appear on PAKDD 2021. Work done while the author was an intern at AWS.

 
 
 
 
 
 
2

L. Liu et al.

Fig. 1. Model pipeline. The œÉ0/œÉ1 graph is deÔ¨Åned in Section 3, and the pre-training
signals are deÔ¨Åned in Section 4.

trees (ASTs) [5] that highlight structural and content-related details in code.
However, some critical relations (e.g., control Ô¨Çow and data Ô¨Çow), which often
impact machine learning models‚Äô success in abstracting code information, are not
available in trees. Graph-based approaches augment ASTs with extra edges to
partially represent the control Ô¨Çow and the data Ô¨Çow [1,4,6]. Depending on the
type of symbolic representation used, the approaches for generating the neural
code representations are either sequence-based [3,7] or graph-based [4,6] neural
network models. However, these works are generally task-speciÔ¨Åc, making it hard
to transfer the learned representations to other tasks. In addition, the scarcity of
labeled data may cause insuÔ¨Écient training in deep learning models.

In this work, we touch upon all the three aspects of ML-based code analysis:
symbolic code representation, task-independent neural code representation, and
task-speciÔ¨Åc learning. Fig. 1 gives an overview of our approach. For symbolic
code representation, we explore two alternatives and show that symbolic code
representation (called œÉ0/œÉ1 graph) which captures richer relations leads to better
performance in downstream tasks. For neural code representation, we specialize
a recently proposed universal representation for graphs, PanRep [8], to code
graphs. And, Ô¨Ånally, we explore two tasks to demonstrate the eÔ¨Äectiveness of the
learned representations: method name prediction (for Python and Java) and link
prediction (for Java). Our proposed method consistently improves the prediction
accuracy across all experiments.

To summarize, the contributions of this work are as follows:

‚Äì We introduce a Ô¨Åne-grained symbolic graph representation for source code,

and adapt to 29M Java methods collected from GitHub.

‚Äì We present a pre-training framework that leverages the graph-based code
representations to produce universal code representations, supporting various
downstream tasks via Ô¨Åne-tuning.

‚Äì We combine the graph-based representation and the pre-training strategies to
go beyond code pre-training with sequence- and tree-based representations.

Universal Representation for Code

3

2 Preliminary

Notation Let G = {V, E} denote a heterogeneous graph with |T | node types
and |R| edge types. V = {{V t}t‚ààT } represents the node set, and E = {{E r}r‚ààR}
represents the edge set. Each node vt
i ‚àà V t is associated with a feature vector.
Throughout the paper, we often use ‚Äúrepresentation‚Äù and ‚Äúembedding‚Äù inter-
changeably unless there is any ambiguity.

2.1 Graph Neural Networks

Graph neural networks (GNNs) learn representations of graphs [9]. A GNN
typically consists of a sequence of L graph convolutional layers. Each layer
updates nodes‚Äô representation from their direct neighbors. By stacking multiple
layers, each node receives messages from higher-order neighbors. In this work,
we utilize the relational graph convolutional network (RGCN) [10] to model our
heterogeneous code graphs. RGCN‚Äôs update rule is given by

Ô£´

h(l+1)
i

= œÜ

Ô£≠

(cid:88)

(cid:88)

r‚ààR

n‚ààN r
i

Ô£∂

h(l)
n W(l)
r

Ô£∏ ,

1
ci,r

where N r
i
use ci,r = |N r
at layer l, W(l)
function. Usually, h(0)
at the last layer) is used as the Ô¨Ånal representations.

is the neighbor set of node i under edge type r, ci,r is a normalizer (we
i | as suggested in [10]), h(l)
is the hidden representation of node i
r are learnable parameters, and œÜ(¬∑) is any nonlinear activation
(the representation

is initialized as node features, and h(L)

i

i

i

2.2 Pre-training for GNNs and for Source Code

Recently, there is a rising interest in pre-training GNNs to model graph data [11,12].
To pre-train GNNs, most works encourage GNNs to capture graph structure
information (e.g., graph motif) and graph node information (e.g., node feature).
PanRep [8] further extends GNN pre-training to heterogeneous graphs.

Pre-training on source code has been studied in [13,14,15]. However, these
models build upon sequence-based code representations and fail to encode code‚Äôs
structural information explicitly. We diÔ¨Äer from these works, by pre-training on a
novel graph-based code representation to capture code‚Äôs structural information.

3 Code Graph

Previous Machine Learning (ML) models [4,6,16] are largely based on ASTs to
reÔ¨Çect structural code information. Though ASTs are simple to create and use,
they have tree-based structures and do not capture control Ô¨Çow and data Ô¨Çow
relations. Here, the control Ô¨Çow represents the order of the execution and the data
Ô¨Çow represents the Ô¨Çow of data along the computation. For example, to represent

4

L. Liu et al.

a loop snippet, ASTs cannot naturally use an edge pointing from the end of the
program statement to the beginning of the loop. In addition, the relation between
the deÔ¨Ånition and uses of a variable is not captured in ASTs. In this work, we
represent code as graphs to eÔ¨Éciently capture both control Ô¨Çow and data Ô¨Çow be-
tween program elements. We call our code graphs as œÉ0 graphs and œÉ1 graphs. The
œÉ0 graphs are related to classical Program Dependence Graphs (PDGs) [17]. The
œÉ1 graphs build upon the œÉ0 graphs and include additional syntactic and semantic
information (detailed in section 3.2). Our experiments show that ML models
using the œÉ1 graphs achieve better prediction accuracy than using the œÉ0 graphs.

3.1 The œÉ0 Graph

The œÉ0 graphs, which relate to PDGs, are used
for tasks such as detection of security vulnera-
bilities and identiÔ¨Åcation of concurrency issues.
In œÉ0 graphs, nodes represent diÔ¨Äerent kinds of
program elements including data and operations;
edges represent diÔ¨Äerent kinds of control Ô¨Çow
and data Ô¨Çow between program elements. We
showcase a œÉ0 graph in Fig. 2.

Both nodes and edges in the œÉ0 graph are
typed. SpeciÔ¨Åcally, we have Ô¨Åve node types: en-
try, exit, data, action, and control nodes. Entry
and exit nodes indicate the control Ô¨Çow entering
and exiting the graph. Data nodes represent the
data in programs such as variables, constants
and literals. Action nodes represent the oper-
ations on the data such as method calls, con-
structor calls and arithmetic/logical operations,
etc. Control nodes represent control points in
the program such as branching, looping, or some
special code blocks such as catch clauses and
Ô¨Ånally blocks. We have two edge types: control
and data edges. Control edges represent the or-
der of execution through the programs and data
edges represent how data is created and used in the programs. Examples of edge
types include parameter edges which indicate the data Ô¨Çow into operations and
throw edges which represent the control Ô¨Çows when exceptions are thrown.

Fig. 2. An example of œÉ0 graph.

3.2 The œÉ1 Graph

One limitation of œÉ0 graph is that the downstream modules perform additional
analysis such as control dependence and aliasing is not reÔ¨Çected in the œÉ0 graph
explicitly and requires machine learning models to infer it. We propose œÉ1 graph
as an augmentation of the œÉ0 graph, which is enhanced by additional information.

Universal Representation for Code

5

SpeciÔ¨Åcally, œÉ1 graph attaches AST node types to nodes in the graph. AST node
types capture syntactic information (e.g., InÔ¨ÅxOperator) provided by the parser.
Higher-order semantic relations such as variable usage (e.g., FirstUse/LastUse),
node aliasing, and control dependence are also included as graph edges.

3.3 The Heterogeneous Code Graph

The proposed œÉ0 and œÉ1 graphs are heterogeneous graphs, i.e. nodes and edges
have types and features. Nodes are categorized into Ô¨Åve types: entry, exit, data,
action, and control. Node features are attributed by their names. Edge types
are the same as edge features, which are deÔ¨Åned by their functionalities. Below
we Ô¨Årst describe node features, followed by edge features.

Entry nodes have feature ENTRY and exit nodes have feature EXIT. Fea-
tures of control nodes are their corresponding keywords such as if, while,
and finally. Features of variables are types, and the variable names are ig-
nored. For example, int.x ‚Üí int; String.fileName ‚Üí String. Method names
contain the method class, method name, and parameter class. For example,
Request.setConnectionKeepAlive#boolean#. Here, Request is the method
class, setConnectionKeepAlive is the method name, and boolean is the param-
eter class.

Features of control edges and data edges are deÔ¨Åned separately. There are
two kinds of control edges: normal control edges and exception control edges. A
normal control edge is a directed edge that connects two control or action nodes;
deÔ¨Åned as dep. See Fig. 2. An exception control edge connects an action node to
a control node to handle an exception that could be thrown by the action; deÔ¨Åned
as throw. Data edges have Ô¨Åve features: receiver, parameter, definition,
condition, and qualifier. Examples include receiver.call() as a receiver
edge; call(param) as a parameter edge, and foo=bar() as a deÔ¨Ånition edge.

3.4 Corpus-level Graphs

In a typical ML application, the corpus consists of a collection of packages or
repositories. Repositories contain multiple Ô¨Åles or classes. Classes contain methods.
The œÉ0 and œÉ1 graphs are at method-level. Corpus-level graphs are a collection of
method-level œÉ0/œÉ1 graphs. Let œÉ refer to either œÉ0 or œÉ1 for notational ease.

4 Method

We propose a new model, Universal Code Representation via GNNs (UniCoRN),
to produce universal code representations based on œÉ graphs. UniCoRN has
two components. First, a GNN encoder that takes in œÉ graphs and generates
node embeddings. Second, pre-training signals that train the GNN encoder in an
unsupervised manner. By sharing the same GNN encoder across all œÉ graphs,
the learned embeddings reveal universal code properties. Below, we show our
design of pre-training signals to help UniCoRN eÔ¨Éciently distill universal code
semantics. The instantiation of the GNN encoder is given in the experiment.

6

L. Liu et al.

4.1 Pre-training Signals

Metapath Random Walk (MRW) signal. A MRW is a random path
that follows a sequence of edge types. We assume node pairs in the same
MRW are proximal to each other; accordingly, they should have similar embed-
dings. For example, for a MRW with nodes [Collection.iterator(), Iterator,
Iterator.hasNext()] and edges [definition, receiver], nodes Iterator and
Iterator.hasNext() are expected to have similar embeddings. Formally, the
signal is deÔ¨Åned as

LM RW =

(cid:88)

(cid:16)

log

1 + exp

(cid:16)

‚àíy √ó ht

vWt,t(cid:48)

ht(cid:48)
v(cid:48)

(cid:17)(cid:17)

,

(1)

v,v(cid:48)‚ààV

v and ht(cid:48)

where ht
v(cid:48) are embeddings for nodes v and v(cid:48) with node types t and t(cid:48).
Wt,t(cid:48)
is a diagonal matrix weighing the similarity between diÔ¨Äerent node types. y
equals 1 as positive pairs if v and v(cid:48) are in the same MRW, otherwise y equals -1
as negative pairs. During training, we sample 5 negative pairs per positive pair.

Heterogeneous Information Maximization (HIM) signal. Nodes of the
same type should reside in some shared embedding space, encoding their similarity.
On the other hand, nodes of diÔ¨Äerent types, such as control nodes and data nodes,
ought to have discriminative embedding space as they are semantically diÔ¨Äerent.
However, standard GNNs fail to do so with only local message propagation.
Following [8], we use a HIM signal to encode these properties:

LHIM =

(cid:88)

(cid:88)

(cid:16)

t‚ààT

v‚ààV t

log (cid:0)œÜ(h(cid:62)

v Wst)(cid:1) + log (cid:0)1 ‚àí œÜ(Àúh(cid:62)

v Wst)(cid:1)(cid:17)

.

(2)

(cid:80)

Here, st = 1
v‚ààV t hv is a global summary of nodes typed t, œÜ(¬∑) is a sigmoid
|V t|
v Wst) quantiÔ¨Åes the closeness between a node embedding hv
function, and œÜ(h(cid:62)
and a global summary st. Negative samples Àúhv are obtained by Ô¨Årst row-wise
shuÔ¨Ñing input node features then propagating through the GNN encoder [8].

Motif (MT) signal. Code graph has connectivity patterns. For example, node
ENTRY has only one outgoing edge; node IfStatement has True and False branch.
Such structural patterns can be captured in graph
motifs, see Fig. 3. With this observation, we pre-
train GNNs with MT signal to generate structure-
aware node embeddings for code graphs. Formally,
we aim to recover the ground truth motif around each node, mv, from the node
embedding hv using an approximator fM T (¬∑) (we use a two-layer MLP),

Fig. 3. Motifs sized 3 and 4.

LM T =

(cid:88)

v‚ààV

||mv ‚àí fM T (hv)||2
2.

(3)

The ground truth mv is obtained using a fast motif extraction method [18].

Node Tying (NT) signal. The corpus-level graph (Cf. section 3.4) contains
many duplicate nodes that have the same feature (e.g., two ENTRY nodes will be

Universal Representation for Code

7

induced from two methods). These duplicate nodes serve as anchors to imply
underlying relations among diÔ¨Äerent graphs. We divide duplicate nodes into two
categories: strict equality and weak equality. Strict equality refers to duplicate
nodes whose semantic meaning should be invariant to their context, including
keywords (if, while, do ...), operators (=, *, << ...), entry and exit nodes.
Duplicate nodes of strict equality will have the same embedding in all œÉ graphs.
We keep a global embedding matrix to maintain their embeddings. Weak equality
refers to other duplicate nodes whose semantic meaning can be aÔ¨Äected by their
context. For example, two foo() nodes in two methods, or two tied nodes due to
the simple qualiÔ¨Åed types of one or two nodes1. We use NT signal to encourage
duplicate node of weak equality to have similar embeddings:

LN T =

(cid:88)

k‚ààK

ave({||hv ‚àí gk||2

2}, v has feature k),

(4)

where K is the set of distinct node features (exclude strict equality nodes), ave(¬∑)
is an average function, and gk = ave({hv}, v has label k) is a global summary of
nodes featured k. In (4), we Ô¨Årst group nodes featured k, followed by computing
the group centers gk, then minimize the distance between nodes to their group
centers.

Pre-training objective. We combine the four pre-training signals to yield a
Ô¨Ånal objective:

L = œâ1LM RW + œâ2LHIM + œâ3LM T + œâ4LN T ,

(5)

where œâ1, . . . , œâ4 balance the importance of diÔ¨Äerent signals. The objective
resembles the objective in multi-task learning [19].

4.2 Data Pre-processing and Fine-tuning

Numeric node features. The initial node features are strings (Cf. section
3.3), which need to be cast into numeric forms before feeding into the GNN
encoder. To this end, we Ô¨Årst split each node‚Äôs feature into subtokens based on
the delimiter ‚Äú.‚Äù. Then, language models are used to get subtoken embeddings, in
which we use FastText [20]. Finally, we use average subtoken embeddings as the
node‚Äôs numeric feature.

Inverse edges. We enrich our œÉ graphs with inverse edges. Recent work has
proven improved performance by adding inverse edges to ASTs [6].

Fine-tuning. After pre-training, we can Ô¨Åne-tune on downstream tasks. Fine-
tuning involves adding downstream classiÔ¨Åers on top of the pre-trained node
embeddings, and predicting downstream labels. A graph pooling layer [9] might
be needed if the downstream tasks are deÔ¨Åned on the graph/method level.

1 We use simple types instead of fully qualiÔ¨Åed types since we create graphs from

source Ô¨Åles and not builds. In this case, types are not fully resolvable.

8

L. Liu et al.

Table 1. Dataset statistics. The œÉ1 graph dou-
bled the number of edges as the œÉ0 graph, pro-
viding extra information for code graphs.

Table 2. Result for method
name prediction on Java dataset.
Higher value indicates better per-
formance.

Dataset Repository Method Node Edge
29M 621M 1,887M
Java (œÉ0)
29M 529M 3,782M
Java (œÉ1)
450K 57M 156M
Python

28K
28K
14K

F1 Precision Recall

œÉ0 21.7
œÉ1 23.6

26.1
27.5

19.9
22.0

5 Experiment

5.1 Dataset

We tested on two real-world datasets, spanning over two programming languages
Java and Python. Summary of the datasets is listed in Table 1.
Java The Java dataset is extracted from 27,581 GitHub packages. In total, these
packages contain 29,024,142 Java methods. We convert each Java method into a
œÉ0 graph and a œÉ1 graph. The data split is on package-level, with training (80%),
validation (10%), and testing (10%).
Python The Python dataset is collected from Stanford Open Graph Benchmark
(ogbg-code) [6]. The total number of Python methods is 452,741, with each
method is represented as an AST. These ASTs are further augmented with
next-token edges and inverse edges. The data split keeps in line with ogbg-code.

5.2 Experimental Setup

We tune hyperparameters on all models based on their validation performances.
For Java dataset, we consider a two-layer RGCN with 300 hidden units. For
Python dataset, we follow ogbg-code and use a Ô¨Åve-layer RGCN with 300 hidden
units. We use Adam [21] as optimizer, with learning rate ranges from 0.01 to
0.0001. Mini-batch training is adopted to enable training on very large graphs2.
We apply dropout at rate 0.2, L2 regularization with parameter 0.0001. The
model is Ô¨Årst pre-trained on a maximum of 10 epochs, then Ô¨Åne-tuned up to 100
epochs on downstream tasks until convergence. The model is implemented using
Deep Graph Library (DGL) [22]. Models have access to 4 Tesla V100 GPUs, 32
CPUs, and 244GB memory.

5.3 Analysis of Embeddings

We begin by analyzing code embeddings via t-SNE [23] visualization. We study
two levels of embeddings: node-level embeddings and method-level embeddings.
Here, a method-level embedding summarizes a method, computed by averaging
node embeddings in its œÉ graph. For better visualization, we show results on 10
random Java packages (involving 4,107 Java methods) using œÉ1 graphs.

2 https://github.com/dmlc/dgl/tree/master/examples/pytorch/rgcn-hetero

Universal Representation for Code

9

Fig. 4. Visualization of node-level em-
beddings with t-SNE.

Fig. 5. Visualization of method-level embed-
dings with t-SNE. Colored with K-means.

In Fig. 4, we see that data nodes and action nodes are forming separate clusters,
indicating our code embeddings preserve important node type information. Fig. 5
suggests method embeddings are forming discriminative clusters. By manually
annotating each cluster, we discovered that cluster 4 contains 91% (out of all)
set functions, cluster 3 contains 78% find functions, and cluster 1 contains 69%
functions which end with Exception. This result suggests that our model has
the potential to distinguish methods in terms of method functionalities.

5.4 Method Name Prediction

We use pre-trained UniCoRN model to initialize code embeddings. Then following
[6,16], we predict method names as downstream tasks. The method name is treated
as a sequence of subtokens (e.g. getItemId ‚Üí [get, item, id]). As in [6], we
use independent linear classiÔ¨Åers to predict each subtoken. The task is deÔ¨Åned
on the method-level: predict one name for one method (code graph). We use
attention pooling [24] to generate a single embedding per method. We follow
[6,16] to report F1, precision, recall for evaluation. Below we show results on
Java and Python separately, as they are used for diÔ¨Äerent testing purposes.
Java. We evaluate the performance gain achieved by switching from the œÉ0
graph to the œÉ1 graph. We truncate subtoken sequences to a maximal length of 5
to cover 95% of the method names. Vocabulary size
is set to 1,000, covering 95% of tokens. Tokens not
in the vocabulary are replaced by a special unknown
token. Similar techniques have been adopted in [6].
We experiment on approximately 774,000 methods
from randomly selected 1,000 packages.

The result is summarized in Table 2. We see that
the œÉ1 graph outperforms the œÉ0 graph, indicating
that the extra information provided by the œÉ1 graph is
beneÔ¨Åcial for abstracting code snippets. Fig. 6 further
supports this observation. The œÉ1 graph consistently
outperforms the œÉ0 graph w.r.t. the F1 score for diÔ¨Äerent method name lengths.

Fig. 6. F1 at name length.

      !  6 X E W R N H Q  O H Q J W K  R I  P H W K R G  Q D P H           ) 0110

L. Liu et al.

Table 3. Method name prediction for Python.
Pooling: average‚Ä†, virtual node¬ß, and attention‚Ä°.
GCN‚Ä†,¬ß and GIN‚Ä†,¬ß: Reported in [6].

GCN-NextTokenOnly‚Ä† 29.77
29.00
GIN-NextTokenOnly‚Ä†
31.63
GCN‚Ä†
31.63
GIN‚Ä†
UniCoRN w/o pretrain‚Ä† 32.81
33.28
32.63
32.04
33.80
32.80
32.60
33.94

UniCoRN‚Ä†
GCN¬ß
GIN¬ß
UniCoRN¬ß
GCN‚Ä°
GIN‚Ä°
UniCoRN‚Ä°

F1 Precision Recall
29.18
28.13
-
-
31.71
32.36
-
-
32.89
31.88
31.77
32.99

31.09
30.98
-
-
35.25
35.28
-
-
35.81
34.72
34.42
36.02

Table 4. MRR and Hit@K(%) results
for link prediction. Higher values are
better. Superscripts D and M denote
DistMult and MLP link predictors.
Hit@K for random is computed as
K/(1 + 200), where 200 is the number
of negative edges per testing edge.

MRR Hit@1 Hit@3 Hit@10

-

œÉD
0
œÉD
1

Random
FastTextD 0.01
0.26
0.32
FastTextM 0.05
0.51
0.53

œÉM
0
œÉM
1

0.5
0.4
15.4
18.1
1.9
46.1
46.2

1.5
1.0
28.5
38.4
4.0
49.8
55.0

5.0
2.3
41.3
61.4
8.0
58.4
65.2

Ground truth
get_config

Prediction
get_config

create_collection create_collection

get_aws_credentials
wait_for_task_ended
add_role
load_bytes

get_ec2
wait_job
create_permission
upload_file

Fig. 7. Examples of method name
prediction on Python in diÔ¨Äerent
degree of consensus. Each pair of
results is demonstrated as ground
truth name and predicted name.

def wait_for_task_ended(self):

try:

waiter = self.client.\

get_waiter('job_execution_complete')

# timeout is managed by airflow
waiter.config.max_attempts = sys.maxsize
waiter.wait(jobs=[self.jobId])

except ValueError:

# If waiter not available use expo
retry ...

Prediction: wait_job.

Fig. 8. Reasonable prediction based on the code
context is observed, though it is inexact match.

Note that the F1 score at method names of length 1 is low. We suspect that some
names at this length are not semantically meaningful, such as a or xyz. Thus,
these method names are hard to predict correctly.
Python. We compare the performances of UniCoRN with various baselines on
Python. Our experiment setup closely follows ogbg-code [6]. For the baseline,
ogbg-code considers GCN and GIN. Additionally, we introduce two baselines
that run GCN and GIN with next-token edges only. We expect these two new
baselines to mimic the performance of sequence-based models. In this task, we
test three pooling methods: average, virtual node [6], and attention [24].

Results are given in Table 3. We list three observations. First, UniCoRN with
attention pooling (UniCoRN‚Ä°) performs the best, endorsing UniCoRN‚Äôs superior
modeling capacity. Second, UniCoRN with pre-training shows performance gain
over UniCoRN without pre-training, verifying the usefulness of our pre-training
strategies. Third, GCN(GIN) improves GCN(GIN)-NextTokenOnly, conÔ¨Årming
the importance of using structural information.

Example pairs of ground truth and prediction are shown in Fig. 7. The
examples of prediction encompass exact matches, such as get_config pair,

Universal Representation for Code

11

context matches, such as get_aws_credentials pair, and mismatches, such as
load_bytes pair. We showcase a prediction example in Fig. 8.

5.5 Link Prediction

In this task, we examine how UniCoRN recovers links in code graphs. We follow
the same experimental setup as in [8]. We feed two node embeddings of a link
to a predictor, a DistMult [25] or a two-layer MLP, to predict the existence of
the link. Here, node embeddings are obtained by applying UniCoRN to œÉ0/œÉ1
graphs, or simply obtained from FastText embeddings. Note that the FastText
is the initial embedding of UniCoRN. We freeze UniCoRN after pre-training.
When Ô¨Åne-tuning link predictors, we ensure œÉ0 and œÉ1 have the same set of
training edges. During inference, we sample 200 negative edges per testing edge
(positive) and evaluate the rank of the testing edge. Evaluations are based on
Mean Reciprocal Rank (MRR) and Hit@K (K=1, 3, 10).

Table 4 shows the results. UniCoRN outper-
forms FastText. The results suggest that node em-
beddings from UniCoRN capture neighborhood
correlations. We see œÉ1 graph again improves
œÉ0 graph. Fig. 9 demonstrates the histogram of
scores for 1,000 positive and 1,000 negative edges.
The score, which ranges from 0 to 1, indicates
the plausibility of the link existence. Positive
edges (0.58¬±0.33) score higher than negative edges
(0.13¬±0.15), with p-value less than 0.00001 using
t-test, suggesting that UniCoRN is capable to dis-
tinguish positive and negative links in code graphs.

6 Conclusion

Fig. 9. Scores for positive and
negative edges.

This paper presents a new model, UniCoRN, to provide a universal representation
for code. Building blocks of UniCoRN include a novel œÉ graph to represent code as
graphs, and four eÔ¨Äective signals to pre-train GNNs. Our pre-training framework
enables Ô¨Åne-tuning on various downstream tasks. Empirically, we show UniCoRN‚Äôs
superior ability to oÔ¨Äer high-quality code representations.

There are several possibilities for future works. First, we are looking to enhance
UniCoRN with additional code-speciÔ¨Åc signals. Second, the explainability of the
learned code representation deserves further study. The explainability can in
turn motivate additional signals to embed desired code properties. Third, more
downstream applications are left to be explored, such as bug detection and
duplicate code detection upon the availability of labeled data.

References

1. E. Dinella, H. Dai, Z. Li, M. Naik, L. Song, and K. Wang, ‚ÄúHoppity: Learning graph

transformations to detect and Ô¨Åx bugs in programs,‚Äù in ICLR, 2019.

                   6 F R U H                    & R X Q W 3 R V L W L Y H 1 H J D W L Y H12

L. Liu et al.

2. J. Cambronero, H. Li, S. Kim, K. Sen, and S. Chandra, ‚ÄúWhen deep learning met

code search,‚Äù in ESEC/FSE, pp. 964‚Äì974, 2019.

3. V. Raychev, M. Vechev, and E. Yahav, ‚ÄúCode completion with statistical language

models,‚Äù in PLDI, pp. 419‚Äì428, 2014.

4. M. Allamanis, M. Brockschmidt, and M. Khademi, ‚ÄúLearning to represent programs

with graphs,‚Äù in ICLR, 2018.

5. L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin, ‚ÄúConvolutional neural networks over

tree structures for programming language processing,‚Äù in AAAI, 2016.

6. W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec,
‚ÄúOpen graph benchmark: Datasets for machine learning on graphs,‚Äù NeurIPS, 2020.
7. A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu, ‚ÄúOn the naturalness of

software,‚Äù in ICSE, pp. 837‚Äì847, IEEE, 2012.

8. V. N. Ioannidis, D. Zheng, and G. Karypis, ‚ÄúPanrep: Universal node embeddings

for heterogeneous graphs,‚Äù DLG-KDD, 2020.

9. Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, ‚ÄúA comprehensive

survey on graph neural networks,‚Äù TNNLS, 2020.

10. M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and M. Welling,
‚ÄúModeling relational data with graph convolutional networks,‚Äù in ESWC, pp. 593‚Äì
607, Springer, 2018.

11. W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V. Pande, and J. Leskovec, ‚ÄúStrategies

for pre-training graph neural networks,‚Äù ICLR, 2020.

12. W. Jin, T. Derr, H. Liu, Y. Wang, S. Wang, Z. Liu, and J. Tang, ‚ÄúSelf-supervised
learning on graphs: Deep insights and new direction,‚Äù arXiv preprint, 2020.
13. A. Kanade, P. Maniatis, G. Balakrishnan, and K. Shi, ‚ÄúLearning and evaluating

contextual embedding of source code,‚Äù ICML, 2020.

14. Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu,
D. Jiang, et al., ‚ÄúCodebert: A pre-trained model for programming and natural
languages,‚Äù EMNLP, 2020.

15. A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan, ‚ÄúIntellicode compose: Code

generation using transformer,‚Äù ESEC/FSE, 2020.

16. U. Alon, S. Brody, O. Levy, and E. Yahav, ‚Äúcode2seq: Generating sequences from

structured representations of code,‚Äù ICLR, 2019.

17. J. Ferrante, K. J. Ottenstein, and J. D. Warren, ‚ÄúThe program dependence graph

and its use in optimization,‚Äù TOPLAS, 1987.

18. N. K. Ahmed, J. Neville, R. A. Rossi, N. G. DuÔ¨Éeld, and T. L. Willke, ‚ÄúGraphlet
decomposition: Framework, algorithms, and applications,‚Äù KAIS, vol. 50, no. 3,
pp. 689‚Äì722, 2017.

19. Y. Zhang and Q. Yang, ‚ÄúA survey on multi-task learning,‚Äù CoRR, 2017.
20. P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, ‚ÄúEnriching word vectors with

subword information,‚Äù TACL, vol. 5, pp. 135‚Äì146, 2017.

21. D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù ICLR,

2015.

22. M. Wang, D. Zheng, Z. Ye, Q. Gan, M. Li, X. Song, J. Zhou, C. Ma, L. Yu,
Y. Gai, T. Xiao, T. He, G. Karypis, J. Li, and Z. Zhang, ‚ÄúDeep graph library: A
graph-centric, highly-performant package for graph neural networks,‚Äù arXiv, 2019.
23. L. v. d. Maaten and G. Hinton, ‚ÄúVisualizing data using t-sne,‚Äù JMLR, vol. 9,

no. Nov, pp. 2579‚Äì2605, 2008.

24. Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel, ‚ÄúGated graph sequence neural

networks,‚Äù ICLR, 2016.

25. B. Yang, W.-t. Yih, X. He, J. Gao, and L. Deng, ‚ÄúEmbedding entities and relations

for learning and inference in knowledge bases,‚Äù ICLR, 2015.

