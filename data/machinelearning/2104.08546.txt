IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. X, NO. X,XXXXXX

1

Fuzzy Discriminant Clustering with Fuzzy Pairwise
Constraints

Zhen Wang, Shan-Shan Wang, Lan Bai, Wen-Si Wang, Yuan-Hai Shao

1
2
0
2

r
p
A
7
1

]

G
L
.
s
c
[

1
v
6
4
5
8
0
.
4
0
1
2
:
v
i
X
r
a

Abstract—In semi-supervised fuzzy clustering, this paper ex-
tends the traditional pairwise constraint
(i.e., must-link or
cannot-link) to fuzzy pairwise constraint. The fuzzy pairwise
constraint allows a supervisor to provide the grade of similarity
or dissimilarity between the implicit fuzzy vectors of a pair of
samples. This constraint can present more complicated relation-
ship between the pair of samples and avoid eliminating the fuzzy
characteristics. We propose a fuzzy discriminant clustering model
(FDC) to fuse the fuzzy pairwise constraints. The nonconvex
optimization problem in our FDC is solved by a modiﬁed
expectation-maximization algorithm, involving to solve several
indeﬁnite quadratic programming problems (IQPPs). Further, a
diagonal block coordinate decent (DBCD) algorithm is proposed
for these IQPPs, whose stationary points are guaranteed, and the
global solutions can be obtained under certain conditions. To suit
for different applications, the FDC is extended into various metric
spaces, e.g., the Reproducing Kernel Hilbert Space. Experimental
results on several benchmark datasets and facial expression
database demonstrate the outperformance of our FDC compared
with some state-of-the-art clustering models.

Index Terms—Fuzzy clustering,

ing, pairwise constraint, fuzzy pairwise constraint,
quadratic programming.

semi-supervised cluster-
indeﬁnite

I. INTRODUCTION

Clustering [1], assigning the given samples into several
clusters, has been employed in many real world applications
[2], [3], [4], [5]. Different from traditional clustering that a
sample can belong in only one cluster, fuzzy clustering [6], [7]
allows it to belong in more clusters with fuzzy memberships.
In some applications, e.g., facial expression recognition [8],
[9], fuzzy clustering is more suitable to present the ground
truth than traditional clustering [10], [11], [12], [13], [14]. To
guide the assignment in clustering, external information was
imported given by supervisors. An simple external information
is the pairwise constraints [15], [16], which assign several pairs
of samples in either a cluster or two different clusters, called
must-link or cannot-link respectively. In the literature, there

Zhen Wang is with School of Mathematical Sciences, Inner Mongolia
University, Hohhot, 010021, P.R.China, and Key Laboratory of Symbolic
Computation and Knowledge Engineering of Ministry of Education, Jilin
University, Changchun, 130012, P.R.China e-mail: wangzhen@imu.edu.cn.
Shan-Shan Wang is with School of Mathematical Sciences,

In-
ner Mongolia University, Hohhot, 010021, P.R.China e-mail: wangshan-
shan202103@163.com.

Lan Bai is with School of Mathematical Sciences, Inner Mongolia Univer-

sity, Hohhot, 010021, P.R.China e-mail: imubailan@163.com.

Wen-Si Wang is with Engineering Research Center of Intelligent Perception
and Autonomous Control (Ministry of Education), and Faculty of Information
Technology, Beijing University of Technology, Beijing 100124, P.R.China, e-
mail: wensi.wang@bjut.edu.cn.

Yuan-Hai Shao (*Corresponding author)

is with School of Manage-
ment, Hainan University, Haikou, 570228, P.R.China e-mail: shaoyuan-
hai21@163.com.

have been many clustering and fuzzy clustering models guided
by pairwise constraints [15], [16], [17], [18], [19], [20], [21],
[22].

7

85

Ground truth (u7,u85):

Pairwise constraint

Fuzzy pairwise constraint

must-link

Happy

7

85

0.7 degree

Surprise

7
0.7
Happy

85

Sad

Illustration of the traditional pairwise constraint and the proposed
Fig. 1.
fuzzy pairwise constraint in facial expression fuzzy clustering. The images
are derived from the JAFFE database [23], and this database also provides a
labeled fuzzy expressions (i.e., the ground truth). Its details can be found in
Section IV.

Note that the fuzzy vectors are formed by the fuzzy mem-
berships. However, the above alternative choice in pairwise
constraint conceals the complicated relationship between the
pair of samples in fuzzy clustering, where the must-link con-
straints require the largest memberships of the fuzzy vectors
to be in the same cluster, and the cannot-link constraints
require the largest memberships of the fuzzy vectors to be
in different clusters. For a supervisor, it is usually not easy
to make sure whether they are must-link or cannot-link when
the fuzzy vectors have two or more dominant memberships.
Furthermore, for the sample under such a pairwise constraint,
its fuzzy memberships assemble in a deﬁnite cluster, losing
its fuzzy characteristics. Fig. 1 illustrates this restriction of
pairwise constraint encountered in facial expression fuzzy
clustering. Given a pair of images, a supervisor needs to decide
whether they are must-link or cannot-link, while a personal
image often contains more facial expressions that an adult
can understand. As shown in Fig. 1, the 7th image expresses

 
 
 
 
 
 
IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. X, NO. X,XXXXXX

2

happy with a bit surprise, while the 85th image also shows
happy but with a bit sad. Thus, the supervisor is hard to
decide the constraint between the 7th and 85th images. Even
though the supervisor gives a decision (e.g., must-link) by her
expertise, the pairwise constraint would obliterate the fuzzy
characteristics, resulting in a pure cluster. Similar phenomenon
may appear on the cannot-link decision. In fact, if a supervisor
decides a pair of samples to be must-link or cannot-link, she/he
offers two viewpoints: the ﬁrst one implies that each of the
two samples belongs to a cluster without any hesitation, and
the second one manifests that the two samples are in the
same cluster or in different clusters. Therefore, the pairwise
constraints does not quite match with fuzzy clustering.

The pairwise constraints have been softened to allow su-
pervisors give the conﬁdence level for their decisions, e.g.,
probabilistic constraints in model-based clustering [24], soft
constraints in clustering applications [25], [26], [27], and fuzzy
constraints in hierarchical clustering [28], [29]. However, these
pairwise constraints still fuse the hypothesis of must-link
and cannot-link, and thus they cannot reveal the complicated
relationship between the pair of samples in fuzzy clustering.
In this paper, completely ruled must-link and cannot-link
out, we propose a new instance-level pairwise constraint in
fuzzy clustering, called fuzzy pairwise constraint. For a pair
of samples, it is concerned with the similarity or dissimilarity
between their implicit fuzzy vectors. As shown in Fig. 1, the
supervisor gives his decision breezily, and the fuzzy character-
istics of fuzzy vectors are retained. To fuse our fuzzy pairwise
constraints in fuzzy clustering, a fuzzy discriminant clustering
model (FDC) is proposed subsequently. Mathematically, the
nonconvex optimization problem in our model is solved by
a modiﬁed expectation-maximization (MEM) algorithm, in-
volving to solve indeﬁnite quadratic programming problems
(IQPPs). It is well known that the general IQPP are still
intrinsically hard problem [30], [31]. Noticing the speciﬁc
properties of our IQPP, a diagonal block coordinate decent
(DBCD) algorithm is constructed, where a stationary point
is guaranteed, and its global solution can be obtained under
certain conditions. To promote the performance of our FDC, it
is extended to various metric spaces to suit for different data
types and distributions [32], [33], [34], [35]. As an example,
the kernel FDC is proposed via kernel tricks [13], [7], [36].

The main contributions of this paper includes:

(i) To match fuzzy clustering, a new fuzzy pairwise constraint
is proposed, resulting in a fuzzy discriminant clustering model
(FDC).
(ii) The optimization problem in our FDC is solved by
a modiﬁed expectation-maximization (MEM) algorithm. To
solve nonconvex subproblems involved, an efﬁcient diagonal
block coordinate decent (DBCD) algorithm is designed.
(iii) Our model is extended to various metric spaces.
(iv) Experimental results on the benchmark datasets and facial
expression database conﬁrm its competitive performance.

This paper is organized as follows. Section II brieﬂy reviews
the pairwise constraints and the related fuzzy clustering. In
Section III, the fuzzy pairwise constraint is deﬁned ﬁrst, and
then, we elaborate the FDC, including its formation, the MEM
algorithm to FDC, the DBCD algorithm to IQPP, and the

extension of FDC in sequence. Experiments are arranged in
Section IV and conclusions are given in Section V.

II. BACKGROUND

Remind the clustering problem with m samples in the n-
dimensional real space denoted by X = (x1, x2, . . . , xm) ∈
Rn×m. In non-fuzzy clustering,
it aims at assigning the
samples into k clusters with their corresponding labels Y =
(y1, y2, . . . , ym)⊤ ∈ {1, 2, . . . , k}m. In fuzzy clustering, it
aims at assigning each sample into all the k clusters with
k fuzzy memberships. These fuzzy memberships can be or-
ganized by m fuzzy vectors {ui ∈ Rk|i = 1, . . . , m}. The
i-th (i = 1, . . . , m) fuzzy vector ui = (ui(1), . . . , ui(k))⊤
indicates the membership degrees of sample xi
to the k
clusters. Apparently, the fuzzy memberships can be converted
to labels easily by

yi = arg max

j=1,...,k

ui(j).

(1)

A. Hard/Soft Pairwise Constraints

The traditional pairwise constraints (called hard pairwise
constraints) are deﬁned by the must-link set M and cannot-
link set C, where a pair of samples in M or C indicates they
are in the same cluster or different clusters, respectively. In
the literature, hard pairwise constraints require that clustering
models must comply with these constrains [27]. However, the
constraints supplied by supervisors are not exactly correct and
often contradict from different supervisors [2], [3]. Therefore,
hard pairwise constraints were always slacked in clustering
models [15], [18], [37], [20].

In model-based clustering, the probabilistic pairwise con-
straints [24] were proposed by setting the conﬁdence of
constraints in M with probability p and ignoring the cannot-
link constraints. The probability p of a must-link pair is
given by the supervisor according to her/his expertise. It
indicates that the two samples are with the same label with
probability p, and with different labels with probability 1 − p.
Then, the probabilistic constraints were extended to soft/fuzzy
constraints both on M and C with probabilities in hierarchical
clustering [29] and other clustering problems [25], [26]. The
conﬁdence level of a pair of samples is in [0, 1] corresponding
to M or C by the supervisor, where 1 indicates the deﬁnite
decision and 0 denotes unknown. For consistency, the above
probabilistic, soft or fuzzy constraints based on hard pairwise
constrains are called soft pairwise constraints uniformly.

Neither hard nor soft pairwise constraints can reﬂect the
fuzzy characteristics in fuzzy clustering, because these con-
straints indicate that the decisions are made from must-link or
cannot-link alternatively.

B. Fuzzy Clustering with Pairwise Constraints

In fuzzy clustering, the fuzzy vectors are normalized as
ui(j) = 1, i = 1, . . . , m} generally. The

{ui ∈ [0, 1]k|

k

Pj=1

classical fuzzy c-means (FCM) [6] is a fuzzy clustering model
without external information. It requires that the samples are
close to the cluster prototypes {cj ∈ Rn|j = 1, . . . , k} by the

IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. X, NO. X,XXXXXX

3

extent of their fuzzy memberships, resulting in a nonconvex
problem as

min
{ui},{cj }

m

k

Pi=1

Pj=1

ui(j)γ||xi − cj||2,

(2)

where γ > 0 is a fuzzy parameter to control the level of
fuzzy memberships, and || · || denotes the L2 norm. The above
problem can be solved by the expectation-maximization (EM)
algorithm [38].

Some fuzzy clustering models extends FCM with pairwise
constraints [19], [20], [39]. For pairwise constraints, the typ-
ical way to utilize them in fuzzy clustering is to utilize the
outer product. For a pair of samples (e.g., xp and xq), the
outer product of their fuzzy vectors is deﬁned as

up × uq = 



up(1)uq(1)
...
up(k)uq(1)

· · · up(1)uq(k)
. . .
· · · up(k)uq(k)

...

(3)



.




If the above two samples are from pairwise constraints, they
should be sheer samples, i.e., their fuzzy vectors up and uq
should tend to the vectors consist of 1 and 0. Furthermore,
if they are from M, the only 1 of these vectors should be
at the same index. Note the restrictions on up and uq. The
two targets can be realized simultaneously by maximizing
the trace of (3) (i.e., inner product) or minimizing the sum
of elements in (3) without its diagonal (called non-diagonal
product) [15]. On the contrary, if two samples are from C, one
can minimize the corresponding inner product or maximize
the non-diagonal product. For instance, a semi-supervised
fuzzy clustering model (PCCA) [15], [18] minimizes the non-
diagonal product on M and the inner product on C as

clustering to select the cluster number [15], [22]. Other FCM-
based semi-supervised clustering models refer to the review
articles [19], [20].

III. FUZZY DISCRIMINANT CLUSTERING (FDC)

A. Fuzzy pairwise constraint
Deﬁnition III.1. Given a pair of samples {xp, xq ∈ X},
its fuzzy pairwise constraint is deﬁned as spq ∈ [−1, 1] to
measure the similarity or dissimilarity degree between their
implicit fuzzy vectors up and uq, where (0, 1] is used for
similarity degree, [−1, 0) is used for dissimilarity degree, and
0 denotes unknown.

Given a pair of samples, supervisors should decide whether
they are similar or dissimilar according to their implicit cluster
vectors ﬁrstly, and then consider the similarity or dissimilarity
degree. As shown in Fig. 1, the supervisor decides the similar-
ity degree to be 0.7 between the unknown facial expressions of
the 7th and 85th images, on account of the two persons smile
similar with a little different expressions. Mathematically, our
fuzzy pairwise constraint refers to consider two implicit fuzzy
vectors. Generally, they are apt to similar if one of the two
vectors has large values on some components and meanwhile
the other has as many large values as possible on the corre-
sponding components. Conversely, they are apt to dissimilar
if one has large values on some components and the other
has as many small values as possible on the corresponding
components. The similarity or dissimilarity degree measures
the difference between the pairs of components in the fuzzy
vectors. Therefore, considering the discrepancy between the
pair of the largest components results in the hard pairwise
constraint, which is a degeneration of our fuzzy pairwise
constraint.

min
{ui},{cj }
k

m

k

Pi=1

Pj=1

up(j)uq(l) +

ui(j)2||xi − cj||2 + β(

k

k

P(p,q)∈M

Pj=1

up(j)uq(j))

Pl=1,l6=j
s.t.

k

P(p,q)∈C
ui(j) = 1, ∀i = 1, . . . , m,

Pj=1

Pj=1
ui(j) ≥ 0,

∀i = 1, . . . , m, ∀j = 1, . . . , k,

where β > 0 is a tradeoff parameter. Correspondingly, the
outer product is optimized on both M and C in ref. [14].
In ref. [21], a weighted fuzzy clustering model with pairwise
constraints was proposed to furnish dissimilarity among the
samples. However, the nonconvex optimization problems in
the above models were solved by some greedy methods instead
of EM type algorithm, because the convergence of EM for
these problems cannot be guaranteed due to the non-convex
subproblems in the maximization step. Thus, to avoid the
nonconvex subproblems, several researchers hired the pairwise
constraints in a pre-step beyond the optimization problem, e.g.,
introducing the entropy regularization [40] or dissimilarity
measurement [37] to keep it consistent with the pairwise
constraints. Additionally, the cluster number k should be given
before implementing these FCM-based clustering models. A
clustering regularization was introduced in semi-supervised

B. Formation of the model

(4)

Given a fuzzy pairwise constraint set S = {spq} with its

index set NS = {(p, q)}, the FDC is formulated as

min
{ui},{cj }

k

s.t.

m

k

(ui(j)γ − α)||xi − cj||2

Pi=1
+ β

Pj=1
P(p,q)∈NS

C(up, uq)

ui(j) = 1, ∀i = 1, . . . , m,

(5)

Pj=1
ui(j) ≥ 0,

∀i = 1, . . . , m, ∀j = 1, . . . , k,

where 0 ≤ α < 1 and β > 0 are parameters, γ > 0 is the
fuzzy parameter, and the cost of each fuzzy pairwise constraint
is deﬁned as

C(up, uq) =

2 spq||up − uq||2
1
uq
−spqu⊤
p

(cid:26)

if spq ≥ 0,
otherwise.

(6)

Our FDC consists of the prototype aggregation and con-
straint guidance. In the prototype aggregation (i.e., the ﬁrst
term in the objective of problem (5)), a discriminative structure
that each sample is close to its cluster prototype and far away
from the other cluster prototypes is proposed. Due to each
sample contributes on each cluster by its fuzzy memberships,

IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. X, NO. X,XXXXXX

4

we consider that a sample can affect each cluster prototype
from positive (i.e., close to it) or negative (i.e, far away
from it) aspect by the corresponding fuzzy membership. Thus,
parameter α can be regarded as a threshold to control the effect
of samples on the cluster prototypes. In addition, the metering
of α should be consistent with γ. For example, for γ = 2, set
α = 0.25 if the user deems that the samples affects positively
with ui(j) ≥ 0.5.

In the constraint guidance (i.e., the last term in the objective
of problem (5)), the L2 norm or inner product are fused in (6)
for different fuzzy pairwise constraints. It is interesting that the
measurements of similarity and dissimilarity are asymmetric
to preserve the fuzzy characteristics. Suppose there are two
fuzzy vectors up, uq ∈ R3. Their similarity may be estimated
in some manner, e.g., inner product or non-diagonal product
stated in Section II.B. However, it is infeasible to use their
inner product and/or non-diagonal product for similarity. Note
that maximizing the inner product or minimizing the non-
diagonal product leads to sheer samples. For example, spq = 1
uq
implies up = uq, while maximizing inner product u⊤
p
uq
(up × uq) − u⊤
or minimizing non-diagonal product
p
leads to one of the three equations: up = uq = (1, 0, 0)⊤,
up = uq = (0, 1, 0)⊤, and up = uq = (0, 0, 1)⊤. Thus, we
should minimize ||up − uq|| for similar up and uq to preserve
their fuzzy characteristics. Correspondingly, the L2 norm is
not a good manner for dissimilarity. For example, spq = −1
implies that up and uq are totally different, i.e., the nonzero
elements in up correspond to zeros in uq, and vice versa.
However, maximizing ||up − uq|| leads to sheer samples, e.g.,
up = (1, 0, 0)⊤ and uq = (0, 0, 1)⊤. To preserve the fuzzy
characteristics of dissimilarity, the inner product is our choice.
In summary, to preserve the fuzzy characteristics, we should
minimize the L2 norm of two fuzzy vectors for similarity and
maximize their inner product for dissimilarity, as the formation
of (6).

P

C. Solving the main problem

For the fuzzy parameter r in fuzzy clustering, researchers
suggested 1.5 ≤ γ ≤ 2.5 for its better performance with the
suitable fuzzy level [6], [22], [18], [21], [14], [40], [37], [19],
[20]. Thus, we set γ = 2 in the main problem (5) for its
computational simplicity.

C.a. Framework of MEM algorithm

Problem (5) is a nonconvex optimization problem, and we
propose a modiﬁed expectation-maximization (MEM) algo-
rithm to solve it. Starting from an initial {u(0)
i }, the cluster
prototypes {c(t)
j } in the expectation step and the fuzzy vectors
{u(t)
i } in the maximization step are updated alternately with
t = 1, 2, . . ., by solving (5) with the ﬁxed counterparts, until
meet some terminate conditions. The framework of MEM
algorithm is summarized in Algorithm 1.

Algorithm 1 Framework of MEM algorithm to solve problem
(5)
Input: Dataset X, fuzzy pairwise constraints S, cluster num-
ber upper bound k, parameter α ∈ [0, 1) and β > 0.
Output: Membership vectors {ui}.
Initialize {u(0)
while true

i } and set t = 0.

(a) Expectation step:
Fix {u(t)

i } and update {c(t)

j } by solving

min
{cj }

k

m

Pj=1

Pi=1

(u(t)

i (j)2 − α)||xi − cj ||2,

(7)

which can be decomposed into k subproblems with j =
1, . . . , k as

mincj

m

Pi=1

(u(t)

i (j)2 − α)||xi − cj||2.

(8)

(b) Maximization step:
Fix {c(t)

j } and update {u(t+1)

i

} by solving

m

k

min
{ui}

ui(j)2||xi − c(t)

j ||2 + β

Pj=1

Pi=1
spq||up − uq||2 − β

2
P0<spq∈S
spqup⊤uq

k

s.t.

P0>spq∈S

ui(j) = 1, ∀i = 1, . . . , m,

Pj=1
ui(j) ≥ 0,

∀i = 1, . . . , m, ∀j = 1, . . . , k.

(9)

According to the index set NS of S, the above problem can
be decomposed into two subproblems:

min
{ui|i /∈NS }
k

s.t.

k

ui(j)2||xi − c(t)

j ||2

Pj=1
ui(j) = 1,

Pj=1
ui(j) ≥ 0, ∀j = 1, . . . , k,

and

m

k

min
{ui|i∈NS }

Pi∈NS

Pj=1
spq||up − uq||2 − β

ui(j)2||xi − c(t)

j ||2 + β
spqup⊤uq

2

P0<spq∈S

k

s.t.

P0>spq∈S

ui(j) = 1, ∀i ∈ NS,

(10)

(11)

∀i ∈ NS, ∀j = 1, . . . , k.

Pj=1
ui(j) ≥ 0,
(c) Termination check:
If {u(t)

t + 1.

i } is unchanged, break the loop; Otherwise, set t =

Remark. Compared with the EM algorithm, several modi-
ﬁcations are added in the MEM algorithm. Firstly, the input
includes only an upper bound k as the cluster number instead
of the cluster number itself. This is realized in the expectation
step, where the solution to subproblem (8) may not exist, re-
sulting in reducing cluster number. Then, in the maximization
step, if an iterative algorithm is employed to solve problem
(11), its initial point is set to be the previous one, resulting in
the convergence of our algorithm.

IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. X, NO. X,XXXXXX

5

Theorem III.1. The series of the objectives of problem (5)
obtained by MEM (Algorithm 1) converges, whatever the
global, local or saddle points to subproblems (11) are obtained
in iteration.

· · · ; 0(r−1)k⊤, 1k⊤) ∈ Rr×rk, r = |NSl|, and the vectors
1r and 0rk consist of r ones and rk zeros, respectively.
D ∈ Rrk×rk is a symmetric matrix and can be partitioned
by k rows and k columns as

Its proof is given in Appendix A.

In Algorithm 1, subproblems (8), (10) and (11) need to be
solved in the loop, and we will elaborate their solutions in
the following. For simplicity, the superscripts that denote the
iterative step are ignored in these subproblems.

C.b. Solutions to subproblems (8) and (10)

D11
...
Dr1

· · · D1r
...
. . .
· · · Drr




D = 





,

(17)

where the diagonal blocks (i.e., Dii with i = 1, . . . , r) are
the diagonal matrices whose diagonal elements are larger than
zero, and Dij = D⊤
ji with i, j = 1, . . . , r. Thereinto, for i =
1, . . . , r,

In the expectation step, for the j-th (j = 1, . . . , k) subprob-

lem (8), if

Dii = diag(||xi − c1||2, . . . , ||xi − ck||2) + β
2

sijI,

(18)

Psij >0

m

(ui(j)2 − α) ≤ 0,

(12)

where I is the identity matrix, and for i, j = 1, . . . , r (i 6= j),

Pi=1

its solution does not exist, which implies that this cluster
prototype cj is inﬁnite. Thus, any sample does not belong
in this cluster, and we delete this cluster (i.e., delete prototype
cj and the j-th dimension in fuzzy vectors {uj}). Otherwise,
the closed-form solution to subproblem (8) is

cj =

m
P
i=1
m
P
i=1

(ui(j)

2

−α)xi

(ui(j)2−α)

.

(13)

In the maximization step, the closed-form solution to sub-

problem (10) with i /∈ NS is
1

1

ui =

k
P
j=1

1
i −c

2

j ||

||x

(

||xi−c1||2 , . . . ,

1

||xi−ck||2 )⊤.

(14)

C.c. Solution to subproblem (11)

Note that subproblem (11) is separable. We partition S into
several mutually disjoint subsets S1, . . . , Sh w.r.t. the sample
index. Let NS1, . . . , NSh be the index sets of S1, . . . , Sh
respectively, where the samples in each index set are associated
with each other directly or indirectly. Then, subproblem (11)
is decomposed by {Sl, NSl|l = 1, . . . , h} into h subproblems
as

k

min
{ui|i∈NSl }
k

spq(

Pi∈NSl
up(j)2 +

Pj=1
k

ui(j)2||xi − cj||2 + β
2

uq(j)2) − β

P0<spq∈Sl
p uq

spqu(t)⊤

Pj=1

k

s.t.

Pj=1

Pspq∈Sl

ui(j) = 1, ∀i ∈ NSl,

Pj=1
ui(j) ≥ 0,

∀i ∈ NSl, ∀j = 1, . . . , k.

For simplicity, subproblem (15) is reformulated as

(15)

B =






Dij = −diag( βsij

2 , . . . , βsij

2 ).

(19)

Note tr(D) > 0. D has at least a positive eigenvalue, which

supports the following lemma.
Lemma III.1. D is positive semi-deﬁnite or indeﬁnite alter-
natively.

When D is positive semi-deﬁnite, problem (16) is a convex
quadratic programming problem (CQPP) and can be solved
by some CQPP solvers [41] to obtain its global solution.
Otherwise, problem (16) is an IQPP. Though there have been
some algorithms to solve an IQPP [42], [43], [44], [45], their
speciﬁc formations or large amount of computation impedes
the application to our problem. However, we still have an
opportunity to obtain the global solution to problem (16) with
an indeﬁnite D.

Consider the following quadratic programming problem

v⊤B⊤DBv + ˆu⊤DBv

1
2

min
v
s.t. Gv ≤ 1r,

(20)

v ≥ 0r(k−1),

1 , . . . , v⊤

where v = (v⊤
r )⊤ = (v11, . . . , v1(k−1), v21,
. . . , v2(k−1), . . . , vr(k−1)) ∈ Rr(k−1), G = (1k−1⊤,
0(r−1)(k−1)⊤; 0k−1⊤, 1k−1⊤, 0(r−2)(k−1)⊤; · · · ; 0(r−1)(k−1)⊤,
1k−1⊤) ∈ Rr×r(k−1),

−1k−1

I



−1k−1

I

⊤



. (21)

. . .

−1k−1

I






Theorem III.2. Suppose v∗ is the solution to problem (20).
Then

1
2

u⊤Du

min
u
s.t. Au = 1r,
u ≥ 0rk,

(16)

where u = (u⊤
r )⊤ = (u11, . . . , u1k, u21, . . . , u2k,
. . . , urk)⊤ ∈ Rrk, A = (1k⊤, 0(r−1)k⊤; 0k⊤, 1k⊤, 0(r−2)k⊤;

1 , . . . , u⊤

u∗ = ˆu + Bv∗

the

is
solution
(1, 0k−1⊤, . . . , 1, 0k−1⊤)⊤.

to

problem (16), where

Its proof is given in Appendix B.

(22)

ˆu

=

IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. X, NO. X,XXXXXX

6

Remark. Apparently, B⊤DB is a symmetric matrix and can
be partitioned by k − 1 rows and k − 1 columns as

B⊤DB = 



11

D′
...
D′

r1

· · · D′
1r
...
. . .
· · · D′
rr



.




(23)

11, . . . , D′

11, . . . , D′

After some algebra, it is easy to deduce that the formation of
rr} resembles the formation of {D11, . . . , Drr},
{D′
i.e., the diagonal blocks {D′
rr} are the diagonal
matrices whose diagonal elements are larger than zero. From
Lemma III.1, we conclude that B⊤DB is positive semi-
deﬁnite or indeﬁnite alternatively. In fact, B⊤DB may be
positive semi-deﬁnite even though D is indeﬁnite. For exam-
ple, D = (1, 0, 2, 0; 0, 5, 0, 2; 2, 0, 1, 0; 0, 2, 0, 5) is indeﬁnite,
because −1 is one of its eigenvalues. However, B⊤DB =
(6, 4; 4, 6) is positive deﬁnite obviously. Thus, if B⊤DB is
positive semi-deﬁnite, the global solution to problem (16) can
be obtained by solving problem (20) by some CQPP solvers
[41].
Lemma III.2. If B⊤DB is indeﬁnite, then D is indeﬁnite.

Its proof is given in Appendix C.
If B⊤DB is indeﬁnite, we propose a Diagonal Block Coor-
dinate Decent (DBCD) algorithm to solve IQPP (16). Starting
from a feasible point, problem (16) w.r.t. ui (i = 1, . . . , r),
i.e.,

i |i ∈ NSl}.

Algorithm 2 Solving problem (15)
Input: Mutually disjoint fuzzy pairwise constraints Sl with
NSl, {xi|i ∈ NSl}, {cj|j = 1, . . . , k}, parameter β > 0,
CQPP solver, and a small tolerance (typically tol = 1e − 3).
Output: Solution {u∗
1. Build problem (16).
2. If D is positive semi-deﬁnite, employ the CQPP solver to
solve problem (16). Then, return the solution and terminate
the algorithm.
3. If B⊤DB is positive semi-deﬁnite, employ the CQPP solver
to solve problem (20) and substitute its solution into (22) to
obtain u∗. Then, return u∗ and terminate the algorithm.
4. If k = r = 2, set u(0) be each vertex and implement the
following loop exhaustively to obtain the smallest objective of
(16); Otherwise, initialize u(0) and implement the following
loop once. Set t = 1.
5. while true

for i = 1, . . . , r

, then
i be the solution to the CQPP (24) by the CQPP

for all j = 1, . . . , r and j 6= i, set uj = u(t−1)
set u(t)
solver;

j

end for
if ||u(t) − u(t−1)|| < tol

set u∗ = u(t);
break;

else set t = t + 1;
end if
end while

min
ui

s.t.

1
2

i−1, u⊤

1 , . . . , u⊤

Diiui + (u⊤

u⊤
i
. . . , Di(i−1), Di(i+1), . . . , Dir)⊤ui
1k⊤ui = 1,
ui ≥ 0k,

i+1, . . . , u⊤

r )(Di1,

is solved in sequence to update ui. The above loop continues
until some terminate conditions are satisﬁed. The ﬁnal u is set
to be the solution to problem (16).

In the DBCD algorithm, the Hessian matrix Dii of problem
(24) is positive deﬁnite obviously from the formation of D.
Thus, the global solution to CQPP (24) can be obtained by
some CQPP solvers. The convergence of the DBCD algorithm
is given as follows, and its proof can be found in Appendix
D.

Theorem III.3. DBCD algorithm converges to a stationary
point to problem (16).

Speciﬁcally, for a small size problem, we can get a global
solution to IQPP (16) by DBCD with exhaustive initial points.
Corollary III.1. There is a vertex u(0) to problem (16) with
k = r = 2 such that DBCD with this initial u(0) converges to
a global solution to problem (16), where the vertex is a such
point that one of its elements is 1 and the rest elements are 0.

Its proof is given in Appendix E.

(24)

D. FDC for Various Metric Spaces

To suit for different data types and distributions, our FDC
is extended into various metric spaces in this subsection. Note
that the Euclidean distances between samples and prototypes
are fused in (5) and (6). The FDC in a metric space can be
formulated by replacing the Euclidean distance with the new
distance d(xi, cj), and it can also be solved by Algorithm
1. It is worth to notice that the formula for updating the
prototypes would be different from (13) in different metric
spaces. However, we can skip updating the prototypes and
update the fuzzy vectors immediately if necessary. As an
example, we extend the FDC for nonlinear clustering via
kernel tricks [36].

Suppose φ(·) : Rn → Rd is a nonlinear mapping. Our

kernel FDC considers

min
{ui},{cj }

k

s.t.

m

k

(ui(j)r − α)||φ(xi) − cj||2

Pi=1
+ β

Pj=1
P(p,q)∈NS

C(up, uq)

ui(j) = 1, ∀i = 1, . . . , m,

Pj=1
ui(j) ≥ 0,

∀i = 1, . . . , m, ∀j = 1, . . . , k,

(25)

The pseudocode to solve problem (15) is summarized in

Algorithm 2.

Problem (25) can be solved by MEM apparently. Corre-
spondingly, for ﬁxed {ui} (i = 1, . . . , m), the j-th (j =

IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. X, NO. X,XXXXXX

7

1, . . . , k) cluster is deleted if (12) holds. Otherwise, we have

cj =

m
P
i=1

(ui(j)

2

−α)φ(xi)

m
P
i=1

(ui(j)2−α)

,

j = 1, . . . , k.

(26)

If the nonlinear mapping φ(·) is given, the rest part is similar
to linear FDC by replacing xi with φ(xi) for i = 1, . . . , m.
Otherwise, we can obtain the fuzzy vectors by skipping over
the computation of the prototypes and solving the subproblem
in the maximization step via kernel tricks. For ﬁxed implicit
{cj} (j = 1, . . . , k), the corresponding subproblem relates to
||φ(xi) − cj || with i = 1, . . . , m and j = 1, . . . , k. According
to (26), we deﬁne

d(x, cj ) = ||φ(x) − cj||2
(ui(j)

2

= φ(x)⊤φ(x) −

m
P
i=1

2

−α)φ(xi)⊤φ(x)

(ui(j)2−α)

m
P
i=1
m
P
i=1

m
P
i=1

(ui(j)

2

(

+

−α)φ(xi))⊤(

(ui(j)

(

2

m
P
i=1
m
P
i=1

(ui(j)2−α))2

(ui(j)

2

−α)K(xi,x)

2

−α)φ(xi))

(27)

= K(x, x) −

m
P
i1 =1

m
P
i2=1

(ui1 (j)

+

m
P
i=1

(ui(j)2−α)

2

−α)(ui2 (j)

2

−α)K(xi1 ,xi2 )

,

(

m
P
i=1

(ui(j)2−α))2

where K(·, ·) is a predeﬁned kernel function refers to the inner
product in the Reproducing Kernel Hilbert Space. Thus, by
substituting (27) into (14), the closed-form solution for i /∈ NS
is

ui =

1

k
P
j=1

d(xi,cj )

(

1

d(xi,c1) , . . . ,

1

d(xi−ck) )⊤,

(28)

and the subproblem (11) for each subset Sl with NSl becomes
to

m

k

min
{ui|i∈NS }

Pi∈NS

Pj=1
spq||up − uq||2 − β

ui(j)2d(xi, cj) + β
2
spqu(t)⊤

P0<spq∈S
p uq

k

s.t.

P0>spq∈S

ui(j) = 1, ∀i ∈ NS,

TABLE I
DETAILS OF BENCHMARK DATASETS

Data
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
(j)
(k)
(l)
(m)
(n)
(o)
(p)
(q)
(r)
(s)

Name
Soybean
Zoo
Echocardiogram
Hepatitis
Wine
Seeds
Heartc
Ecoli
Dermatology
Australia
Creadit
Phishing
Car
Segment
Wave
Satimage
Two
Letter
Shuttle

Samples (m)
47
101
131
155
178
210
303
336
366
690
690
1,353
1,728
2,310
5,000
6,435
7,400
20,000
58,000

Dimension (n)
35
16
10
19
13
7
14
7
34
14
15
9
6
18
21
36
20
16
10

Classes (k)
2
7
2
2
3
3
2
8
6
2
2
3
4
7
3
6
2
26
7

and pairwise-constrained competitive agglomeration (PCCA)
[15]. The classical FCM [19] represented the baseline. All
these models were implemented by MATLAB2017, on a PC
with an Intel Core Duo Processor (4.2 GHz) with 16GB
RAM. In the experiments, the normalized adjusted rand index
(ARI∈ [0%, 100%]) [47] and normalized mutual information
(NMI∈ [0%, 100%]) [48] were used to measure the clustering
performance. Their parameters were optimized to maximize
the ARI and/or NMI by grid searching, and the cluster
number was set to the real one. For practical convenience, the
corresponding FDC Matlab codes have been uploaded upon
the github3. The implementation details of these models are
as follows.

FCM[19]

SSDC[17]
DSC[16]

FHSS[29]

(29)

PCCA[15]

Without any constraints, it was implemented 20 times by
the fcm function with random initialization provided by
MATLAB. It output fuzzy vectors by parameter k.
It accepted pairwise constraints and output cluster labels.
It accepted pairwise constraints and output cluster labels by
parameter k.
It accepted soft pairwise constraints and output cluster labels
with a cutoff parameter α = 0.05.
It accepted pairwise constraints and was implemented 20
times with random initialization. It output fuzzy vectors by
parameter k and a tradeoff parameter selected from Ω :=
{2i|i = −8, −7, . . . , 7}.
It accepted fuzzy pairwise constraints and was implemented
20 times with random initialization. It output fuzzy vectors
by parameter k and two tradeoff parameters, where param-
eter α was selected from Ω and parameter β was selected
from {0, 0.02, 0.04, . . . , 0.3}.

Pj=1
ui(j) ≥ 0,

∀i ∈ NS, ∀j = 1, . . . , k,

FDC

which can be solved similar to (11) by Algorithm 2. So the
details are omitted.

IV. EXPERIMENTS

In this section, we analyze the clustering performance of our
FDC on some benchmark datasets [46] and a facial expression
database [23] compared with several state-of-the-art semi-
supervised clustering models, including semi-supervised den-
peak clustering (SSDC1) [17], dominant set clustering (DSC2)
[16], fuzzy hierarchical semisupervised clustering (FHSS) [29]

A. Benchmark Datasets

In this subsection, we analyze the hard clustering perfor-
mance of these models on 19 benchmark datasets4. The details
of these datasets are shown in Table I. For each dataset,
we offered four groups of fuzzy pairwise constraints: (i)

1https://github.com/Huxhh/SSDC
2https://github.com/erogol/DominantSetClustering

3https://github.com/gamer1882/FDC
4http://archive.ics.uci.edu/ml/datasets.php

IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. X, NO. X,XXXXXX

8

TABLE II
PERFORMANCE OF THE STATE-OF-THE-ART CLUSTERING MODELS ON THE BENCHMARK DATASETS

Data

Group

Baseline
ARI(%)/NMI(%)

(i)
(ii)
(iii)
(iv)
(i)
(ii)
(iii)
(iv)
(i)
(ii)
(iii)
(iv)
(i)
(ii)
(iii)
(iv)
(i)
(ii)
(iii)
(iv)
(i)
(ii)
(iii)
(iv)
(i)
(ii)
(iii)
(iv)
(i)
(ii)
(iii)
(iv)
(i)
(ii)
(iii)
(iv)
(i)
(ii)
(iii)
(iv)
(i)
(ii)
(iii)
(iv)
(i)
(ii)
(iii)
(iv)
(i)
(ii)
(iii)
(iv)
(i)
(ii)
(iii)
(iv)
(i)
(ii)
(iii)
(iv)
(i)
(ii)
(iii)
(iv)
(i)
(ii)
(iii)
(iv)
(i)
(ii)
(iii)
(iv)
(i)
(ii)
(iii)
(iv)

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(j)

(k)

(l)

(m)

(n)

(o)

(p)

(q)

(r)

(s)

•
◦

•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•

66.61±19.07/32.55±37.61

•
•
•

81.68±6.71/69.18±7.44

65.03±5.89/22.66±9.15

56.88±3.81/15.49±6.84

87.05±11.50/75.87±17.01

87.96±0.70/71.62±1.25

70.73±6.49/32.86±10.83

68.69±0.49/55.55±0.88

79.10±4.54/68.48±6.97

65.92±7.80/24.78±12.37

62.91±6.48/20.05±9.73

66.23±3.58/24.37±4.96

52.18±2.00/9.22±4.63

75.75±0.56/61.32±1.08

62.38±0.01/32.83±0.01

76.67±0.02/60.90±0.04

•
•
•

•
•
•

•

•
•

•
•
•

•
•

87.95±17.42/69.92±33.47

50.41±0.16/6.66±1.46

60.20±2.37/47.07±6.51

•
•
•
•
•
•
•
•
•
•
61/76
1/76

SSDC
ARI(%)/NMI(%)
55.05/32.80•
55.05/32.80•
70.78/64.62•
70.78/64.62•
75.89/74.99•
78.37/71.93•
75.96/74.01•
78.50/74.58•
47.17/10.09•
47.38/7.35•
48.17/3.41•
58.75/18.88•
45.90/5.51•
49.48/9.26•
52.38/9.86•
53.69/12.64•
68.63/45.64•
78.74/59.19•
78.96/60.16•
80.88/63.95•
54.55/15.05•
54.58/15.59•
73.71/58.55•
83.64/67.27•
52.95/15.93•
55.68/20.16•
61.84/27.32•
63.14/29.32•
54.57/31.15•
71.12/51.30•
75.52/54.36•
76.17/58.75•
54.30/36.43•
70.57/60.47•
75.84/67.81•
83.82/73.26•
53.13/20.82•
53.62/20.28•
53.78/21.18•
54.45/12.94•
52.31/14.94•
53.11/18.49•
53.19/18.00•
53.46/18.49•
51.17/10.49
51.51/18.44•
52.57/19.28•
53.14/19.88•
51.17/12.36
51.94/18.97
52.23/19.09
52.50/19.02•
64.58/55.45•
67.71/56.38•
77.28/65.58
78.45/67.16
55.00/25.62•
56.06/23.29•
56.00/25.52•
57.47/27.83•
58.73/42.39•
59.02/50.05•
66.34/53.32•
68.81/53.76•
52.30/15.10•
54.05/21.89•
60.64/29.63•
66.13/37.07•

–
–
–
–
–
–
–
–
62/68
0/76

DSC
ARI(%)/NMI(%)
50.00/0.00•
50.00/0.00•
50.00/0.00•
50.00/0.00•
50.80/8.78•
50.80/8.78•
50.80/8.78•
50.80/8.78•
60.27/12.65•
60.27/12.65•
61.16/15.69•
61.34/15.33•
60.12/9.81•
60.12/9.81•
60.12/9.81•
60.12/9.81•
50.05/2.82•
50.05/2.82•
50.05/2.82•
50.15/3.05•
59.65/37.95•
59.65/37.95•
60.15/38.74•
60.15/38.74•
53.88/13.14•
53.88/13.14•
53.95/14.60•
54.27/13.10•
65.91/34.44•
65.91/34.44•
65.91/34.44•
65.91/34.44•
50.58/9.99•
50.65/10.81•
50.65/10.81•
50.65/10.81•
53.30/6.37•
53.30/6.37•
53.30/6.37•
53.30/6.37•
56.42/13.03•
56.71/13.50
56.90/13.83•
58.24/15.45•
51.06/5.44
51.06/5.44•
51.06/5.44•
51.06/5.44•
50.00/0.00•
50.00/0.00•
50.00/0.00•
50.00/0.00•
66.09/51.88•
66.09/51.99•
66.12/51.91•
66.27/51.94•
51.74/12.69•
51.74/12.71•
51.74/12.70•
51.74/12.70•
65.52/47.63•
65.58/47.80•
65.58/47.80•
65.58/47.83•
50.00/0.02•
50.00/0.02•
50.00/0.02•
50.00/0.02•
–
–
–
–
–
–
–
–
66/68
0/76

FHSS
ARI(%)/NMI(%)
50.18/42.41•
60.71/55.76•
90.75/83.55
90.75/83.55•
56.77/66.19•
57.24/66.61•
80.05/80.42
85.88/76.13•
50.00/35.51•
50.80/5.08•
53.87/32.92•
60.90/28.16•
50.01/31.83•
58.41/24.51•
61.21/21.22•
68.07/22.26•
49.46/6.57•
50.45/46.95•
55.07/53.33•
68.86/61.35•
50.00/0.00•
54.49/50.27•
72.27/56.02•
74.67/57.52•
50.00/34.77•
50.01/34.80•
50.22/34.23•
56.63/29.19•
50.01/51.11•
50.01/51.11•
50.01/51.11•
72.54/57.56•
50.04/53.57•
50.20/53.65•
50.42/53.89•
52.79/35.46•
50.00/32.34•
50.08/2.53•
50.51/5.21•
52.20/23.06•
50.00/0.00•
50.00/32.17•
50.78/7.30•
53.26/24.23•
50.14/5.96
51.60/29.57•
52.03/29.68•
60.32/22.67•
50.00/0.00•
50.08/30.26•
50.08/30.26•
53.12/2.08•
50.00/3.89•
50.05/50.57•
50.05/50.57•
50.05/50.57•
50.00/0.00•
50.08/30.89•
50.00/35.79•
50.00/35.90•
50.00/0.00•
50.05/44.82•
65.84/44.89•
75.11/59.13•
50.00/27.89•
50.04/27.71•
61.99/35.33•
81.81/49.90•

–
–
–
–
–
–
–
–
65/68
0/76

PCCA
ARI(%)/NMI(%)
68.35±0.00/41.25±0.00
68.35±0.00/41.25±0.00•
68.35±0.00/41.25±0.00•
87.79±0.00/71.03±0.00•
80.88±0.15/78.98±0.76
80.89±0.17/79.01±0.91•
81.64±0.15/80.82±0.99•
81.63±0.00/80.43±0.00•
72.68±0.00/33.81±0.00•
72.68±0.00/33.81±0.00•
74.78±0.00/37.06±0.00•
75.96±0.00/40.33±0.00•
57.29±2.44/19.60±5.23•
57.95±0.00/17.09±0.00•
59.04±0.00/20.12±0.00•
59.48±0.00/18.72±0.00•
94.87±0.00/87.59±0.00•
94.87±0.00/87.59±0.00•
94.87±0.00/87.59±0.00•
94.87±0.00/87.59±0.00•
66.14±20.28/30.43±38.24•
88.62±0.00/72.75±0.00•
88.62±0.00/72.75±0.00•
89.25±0.00/73.84±0.00•
77.01±0.00/43.96±0.00
77.49±0.00/44.80±0.00◦
77.49±0.00/44.80±0.00•
80.53±0.00/50.22±0.00•
68.88±0.65/56.51±0.96•
68.75±0.71/56.33±1.00•
68.68±0.68/56.23±0.96•
68.77±0.65/56.33±0.91•
60.50±9.74/34.76±32.26•
67.89±3.94/54.38±6.37•
66.13±0.00/51.21±0.00•
66.13±0.00/51.21±0.00•
72.37±0.00/34.91±0.00•
73.56±0.00/37.01±0.00•
73.95±0.00/37.71±0.00•
74.56±0.00/38.81±0.00•
66.36±0.51/25.15±0.76
66.58±0.76/25.45±1.18
69.75±2.50/30.76±3.71•
71.39±0.00/33.27±0.15•
59.81±0.00/29.61±0.00
67.86±0.00/25.47±0.00
68.07±0.00/26.34±0.00•
69.09±0.00/27.86±0.00•
52.32±2.26/8.60±4.61
52.56±1.61/9.50±4.37
52.95 ±2.21/9.99±4.84
55.20±3.94/14.87±4.74•
50.08±0.18/0.61±1.36•
53.37±4.85/12.50±17.68•
57.31±4.30/25.04±14.44•
59.68±1.05/30.49±2.41•
53.68±5.03/9.68±13.26•
54.46±6.14/8.52±11.73•
62.68±7.09/24.69±13.80
61.32±0.19/29.62±1.84•
56.00±5.85/17.27±14.34•
50.00±0.00/0.00±0.00•
51.97±4.42/7.06±13.42•
52.52±5.63/6.47±14.48•
53.12±0.00/14.28±0.00•
59.56±5.34/21.19±11.84•
58.55±0.00/25.34±0.00•
87.23±0.00/64.00±0.00•
50.00±0.00/0.00±0.00•
50.00±0.00/0.00±0.00•
50.00±0.00/0.00±0.00•
50.00±0.00/0.00±0.00•
49.01±1.19/5.28±4.94•
49.97±0.01/0.21±0.48•
50.00±0.00/0.00±0.00•
50.14±0.32/1.78±3.98•
65/76
1/76

FDC
ARI(%)/NMI(%)
68.94±1.88/42.09±2.73
82.65±4.90/61.77±7.72
94.26±9.69/88.49±18.86
100.0±0.00/100.0±0.00
80.91±0.83/77.46±2.38
81.01±0.00/79.89±0.00
86.51±6.53/74.70±8.75
94.62±2.07/87.56±3.66
76.93±0.00/40.36±0.00
78.75±5.58/45.54±9.24
78.07±0.00/42.50±0.00
78.12±0.00/42.75±0.00
65.11±2.40/23.29±2.72
66.68±3.42/22.31±3.80
67.46±5.54/25.95±6.21
70.20±0.42/29.18±0.85
96.21±0.74/90.17±1.82
96.54±0.00/90.88±0.00
96.54±0.00/90.88±0.00
96.54±0.00/90.88±0.00
88.79±0.28/73.09±0.53
89.25±0.00/73.84±0.00
90.50±0.00/76.16±0.00
91.22±0.23/77.97±0.46
74.89±5.84/40.14±9.45
75.70±3.67/41.75±6.22
77.89±0.57/45.53±1.17
82.10±0.42/53.30±0.81
75.85±3.53/52.34±5.61
78.37±4.48/53.72±6.08
81.32±6.08/60.18±11.01
80.37±5.67/62.85±4.35
81.52±3.81/77.29±1.78
85.46±4.23/78.87±5.42
86.37±1.17/79.16±2.14
89.73±1.21/80.74±1.81
74.76±0.00/39.62±0.00
74.76±0.00/39.62±0.00
76.00±0.00/41.72±0.00
76.66±0.20/42.97±0.57
62.51±6.27/19.49±9.21
67.87±5.11/27.68±7.99
71.13±0.10/32.79±0.17
71.98±0.45/34.62±0.77
63.12±10.99/19.63±16.45
68.02±1.44/26.32±1.55
70.45±0.30/30.43±0.55
73.34±5.53/35.66±8.46
51.73±2.12/4.91±5.11
52.10±1.67/7.85±2.87
53.11±2.29/7.70±5.05
59.33±4.77/18.22±8.68
75.43±0.44/58.24±1.22
75.81±0.55/61.49±1.23
78.12±1.70/66.24±2.92
78.83±0.95/68.08±1.33
62.35±0.06/32.31±0.42
63.29±0.03/31.78±0.14
64.49±0.01/37.24±0.11
66.57±0.25/38.91±6.46
75.48±1.28/57.17±1.67
77.72±0.83/61.65±0.54
78.70±0.68/63.39±1.45
79.64±0.79/67.49±0.37
87.20±0.05/63.91±0.12
88.08±0.06/65.84±0.15
95.88±0.05/85.28±0.16
95.95±0.42/85.51±1.21
56.52±0.41/33.41±1.01
56.86±0.51/34.71±1.10
58.30±0.59/38.87±0.82
58.51±0.48/39.93±1.23
78.35±1.77/57.69±2.61
78.59±1.47/57.98±3.01
79.74±1.95/61.26±5.96
79.73±1.29/61.78±3.39

‘–’ denotes out of memory; •/◦ indicates FDC is signiﬁcantly better/worse than compared model (paired t-tests at 95% signiﬁcance level).

IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. X, NO. X,XXXXXX

9

)

%

(

I

M
N

)

%

(

I

M
N

)

%

(

I

M
N

100
80
60
40
20
100\0
80
60
40
20
100\0
80
60
40
20
100\0
80
60
40
20
0

100
80
60
40
20
100\0
80
60
40
20
100\0
80
60
40
20
100\0
80
60
40
20
0

100
80
60
40
20
100\0
80
60
40
20
100\0
80
60
40
20
100\0
80
60
40
20
0

NMI(%)

Cluster number

Group (iv)

Group (iii)

Group (ii)

Group (i)

0

0.005

0.01

0.015

0.02

0.025

0.03

0.035

0.04

0.045

(b) Zoo

NMI(%)

Cluster number

Group (iv)

Group (iii)

Group (ii)

Group (i)

0

0.005

0.01

0.015

0.02

0.025

0.03

0.035

0.04

0.045

(i) Dermatology

NMI(%)

Cluster number

Group (iv)

Group (iii)

Group (ii)

Group (i)

0

0.005

0.01

0.015

0.02

0.025

0.03

0.035

0.04

0.045

*

k

)

%

(

I

M
N

*

k

)

%

(

I

M
N

100
80
60
40
20
100\0
80
60
40
20
100\0
80
60
40
20
100\0
80
60
40
20
0

100
80
60
40
20
100\0
80
60
40
20
100\0
80
60
40
20
100\0
80
60
40
20
0

*

k

)

%

(

I

M
N

40
30
20
10
40\0
30
20
10
40\0
30
20
10
40\0
30
20
10
0

6
4
2
0
6
4
2
0
6
4
2
0
6
4
2
0
0.05

6

4

2

0
6

4

2

0
6

4

2

0
6

4

2

0
0.05

6
4
2
0
6
4
2
0
6
4
2
0
6
4
2
0
0.05

NMI(%)

Cluster number

Group (iv)

Group (iii)

Group (ii)

Group (i)

0

0.005

0.01

0.015

0.02

0.025

0.03

0.035

0.04

0.045

(h) Ecoli

NMI(%)

Cluster number

Group (iv)

Group (iii)

Group (ii)

Group (i)

0

0.005

0.01

0.015

0.02

0.025

0.03

0.035

0.04

0.045

(n) Segment

NMI(%)

Cluster number

Group (iv)

Group (iii)

Group (ii)

Group (i)

0

0.002

0.004

0.006

0.008

0.01

0.012

0.014

0.016

0.018

*

k

*

k

8
6
4
2
0
8
6
4
2
0
8
6
4
2
0
8
6
4
2
0
0.05

6
4
2
0
6
4
2
0
6
4
2
0
6
4
2
0
0.05

*

k

25
20
15
10
5
0
25
20
15
10
5
0
25
20
15
10
5
0
25
20
15
10
5
0
0.02

(p) Satimage

(r) Letter

Fig. 2. Parameter inﬂuence of FDC on the four groups of six benchmark datasets. The parameter α increases along with the horizontal axis, and the left and
right vertical axes shows the corresponding NMI (blue solid line) and cluster number k∗ (red dash line), respectively.

)

%

(

I

R
A

100

95

90

85

80

75

70

65

60

55

FDC
Kernel FDC

(a)

(b)

(c)

(d)

(e)

(f)

(g)
Dataset

(h)

(i)

(j)

(k)

(l)

(m)

Fig. 3. Performance of kernel FDC on the benchmark datasets with group
(iv) compared with linear FDC.

This group contained 0.05m pairs, where the fuzzy pairwise
constraints were opposite to the ground truth; (ii) This group
contained 0.1m pairs, where half of them accorded with the
ground truth and the rest were opposite; (iii) This group
contained 0.05m pairs accorded with the ground truth; (iv)
This group contained 0.1m pairs accorded with the ground
truth. Thereinto, for data (s),
the ratio of fuzzy pairwise
constraints is a ﬁvefold reduction. Since there are at most
m2−m pairs for a dataset, the number of pairs in each group is
much fewer than the maximum. For the models with traditional
pairwise constraints (i.e., SSDC, DSC and PCCA), the pairs in
these groups were moved into the must-link M and cannot-
link C. For the models with soft/fuzzy pairwise constraints
(i.e., FHSS and our FDC), when the pair of samples was from
the same class, the fuzzy value of a pair was set to 0.5 plus a

 
 
 
 
 
 
 
IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. X, NO. X,XXXXXX

10

random number with uniform distribution U[0, 0.5] if a sample
in the pair is the 10-nearest neighbor of the other; Otherwise,
it was set to a random number with U[0, 1]. When the pair of
samples was from difference classes, the fuzzy value of a pair
was set to −0.5 minus a random number with U[0, 0.5] if each
sample in the pair is not the 10-nearest neighbor of the other;
Otherwise, it was set to a random number with U[−1, 0]. The
random values and opposite pair settings in these groups were
used to simulate the practical application. The fuzzy vectors
obtained by the fuzzy models, including FCM, PCCA and our
FDC, were transformed to labels by (1). The average ARI
and NMI with the standard deviation for FCM, PCCA and
our FDC, and the one-run ARI and NMI for SSDC, DSC
and FHSS on these datasets were reported in Table II. The
highest ARI and NMI for each dataset were bold compared
with the baseline. From Table II, we observe that the SSDC,
DSC and FHSS perform under the baseline even though some
correct pairwise constraints are given in groups (iii) and (iv).
There are a few datasets on which SSDC and FHSS exceed
the baseline with correct pairwise constraints, e.g., data (a).
However, PCCA and our FDC performs much better than
the baselines on many datasets even though some opposite
pairwise constraints are given in groups (i) and (ii), e.g., data
(h) and (j). On the other datasets, the performance of PCCA
and FDC relies on the pairwise constraints. Generally, they
works better with correct pairwise constraints than with wrong
pairwise constraints. However, the clustering ability of PCCA
becomes poor on some large scale datasets, e.g., on data (r) and
(s). Correspondingly, our FDC keeps its better performance
than PCCA on most of these datasets. In conclusion, the
SSDC, DSC and FHSS are not competitive with the other
models, and they underuse the (soft) pairwise constraints,
especially DSC. Besides, FCM, PCCA and our FDC are a
series of fuzzy clustering models, and our FDC can utilize the
fuzzy pairwise constraints more sufﬁciently than the pairwise
constrains in PCCA. The wrong pairwise constraints mislead
all of these clustering models, and PCCA and FDC are less
affected because of their trade-off parameters. Moreover, the
fuzzy characteristics of fuzzy pairwise constraint may further
reduce the inﬂuence from the wrong pairwise constraint, which
lead our FDC perform better than the PCCA on groups (i) and
(ii) of many datasets.

Statistically, the paired t-test was adopted to compare the
difference of our FDC and the other models on the benchmark
datasets. For each group of the datasets in Table II, the •/◦
indicates FDC is signiﬁcantly better/worse than compared
model at 95% signiﬁcance level, and the overall numbers of
•/◦ are calculated in the last row. On most of the datasets, our
FDC is signiﬁcantly better than the other models with a large
number of • and a low number of ◦. Therefore, it is statistical
signiﬁcant that our FDC is better than the other models on the
benchmark datasets, which supports the previous observation.

Subsequently, we analyze the inﬂuence of the parameters α
and β in the FDC. The purpose of β is clear: larger β indicates
that we count on the fuzzy pairwise constraints more, and
vice versa. Thus, the precision of fuzzy pairwise constraint
directs the performance of FDC. The other parameter α can

adjust the cluster number in theory. To verify the inﬂuence of
α, we augmented α from 0 on the above four groups of six
benchmark datasets with larger cluster numbers. The clustering
results and the cluster numbers were reported in Fig. 2, where
the cluster number was decided by (1). It is obvious that the
cluster number decreases with the increasing α generally, and
there is a threshold smaller than 1 such that the cluster number
would be ﬁxed to 1 when α is larger than this threshold. Notice
that the cluster number may increase with a larger α, e.g., in
group (iii), Fig. 2 (i), for the cluster prototypes and cluster
numbers may be inconsistent. Though we set k to be the truth
from Table (I), smaller clusters would be obtained with α =
0, e.g., on data (i). However, more clusters were formulated
with nonzero α, and then better performance was obtained. In
practice, we should adjust α carefully for different datasets,
and its upper bound would be much smaller than 1, especially
for large scale datasets.

Finally, we tested the FDC for various metric spaces. Gen-
erally, an appropriate metric can promote the performance of
FDC, and the metric changes with the data space, which leads
to the metric learning problems [49], [50]. For fairness, we
implemented the kernel FDC on these benchmark datasets with
group (iv) compared with the linear FDC, where the Gaussian
kernel [51] K(x1, x2) = exp{−µ||x1 − x2||2} was used and
its parameter µ was selected from {2i|i = −10, −9, . . . , 5}.
The comparisons were depicted in Fig. 3. Apparently, the
Gaussian kernel can further improve the performance of FDC
on most of the datasets. Once an appropriate metric is decided
for a data space, we can apply it into FDC without any
difﬁculty.

B. Facial Expression Clustering

This subsection experiments the facial expression clustering
on the Japanese Female Facial Expression (JAFFE) Database
[23]. Data JAFFE includes two datasets of images from ten
Japanese female expressers, whose averaged semantic ratings
on six facial expressions are offered by 60 Japanese viewers.
The six basic facial expressions are “Happy”, “Surprise”,
“Sad”, “Angry”, “Disgust” and “Fear”. In this database, the
ﬁrst dataset consists of 213 images and the second dataset
consists of 181 images by excluding the “fear” attributes
and images. We hired the second dataset and offered ﬁve
groups of fuzzy pairwise constraints, where these groups in
sequence contains 0.2m, 0.4m, 0.6m, 0.8m and m fuzzy
pairwise constraints. These fuzzy pairwise constraints were
offered following the equation (6). Then, we implemented
these clustering models on the dataset with these groups of
fuzzy pairwise constraints and reported the results in Table
III, where the cluster number is set to 5. Another criterion
Accuracy (Acc.) [52] and the classical kmeans [53] were
added in the experiment to reveal the overall clustering per-
formance. Table III shows that the clustering performance
is improved with the increasing number of fuzzy pairwise
constraints for all these semi-supervised models, which implies
that the pairwise constraints are very useful to facial expression
clustering problems. Thereinto, the largest improvement arises

IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. X, NO. X,XXXXXX

11

TABLE III
CLUSTERING PERFORMANCE ON THE JAFFE DATABASE WITH FIVE GROUPS OF FUZZY PAIRWISE CONSTRAINTS

Group

(i)

(ii)

(iii)

(iv)

(v)

Criterion
ARI(%)
NMI(%)
Acc.(%)
ARI(%)
NMI(%)
Acc.(%)
ARI(%)
NMI(%)
Acc.(%)
ARI(%)
NMI(%)
Acc.(%)
ARI(%)
NMI(%)
Acc.(%)

kmeans
50.45±0.48
7.32±1.70
50.96±10.21•
50.45±0.48•
7.32±1.70
50.96±10.21•
50.45±0.48•
7.32±1.70•
50.96±10.21•
50.45±0.48•
7.32±1.70•
50.96±10.21•
50.45±0.48•
7.32±1.70•
50.96±10.21•

FCM
50.86±0.38
5.17±1.17
67.83±0.62
50.86±0.38•
5.17±1.17•
67.83±0.62•
50.86±0.38•
5.17±1.17•
67.83±0.62•
50.86±0.38•
5.17±1.17•
67.83±0.62•
50.86±0.38•
5.17±1.17•
67.83±0.62•

SSDC
50.46•
5.97
64.25•
51.07•
6.62•
58.50•
51.58•
6.89•
59.94•
51.58•
6.89•
59.94•
51.64•
10.53•
54.06•

DSC
50.45•
5.52
26.62•
50.63•
6.74•
28.05•
50.63•
6.74•
28.05•
50.63•
6.74•
28.05•
50.77•
7.81•
28.82•

FHSS
49.60•
6.73
22.57•
49.78•
7.59
22.92•
49.92•
8.23•
23.17•
50.00•
8.41•
23.44•
50.17•
10.51•
27.91•

PCCA
49.97±0.18•
3.25±1.07•
52.54±3.11•
50.86±0.01•
5.65±0.63•
53.19±0.50•
51.43±0.12•
6.46±1.13•
53.26±0.58•
51.96±0.61•
7.67±2.22•
54.68±3.71•
52.60±0.39•
9.25±1.36•
56.04±3.57•

FDC
51.72±0.50
8.27±2.64
68.16±1.10
54.04±1.11
10.63±2.80
70.36±0.77
54.05±0.85
11.76±1.30
70.57±0.76
55.74±1.14
12.26±2.39
71.96±1.20
56.61±1.07
13.67±2.77
73.37±0.67

•/◦ indicates FDC is signiﬁcantly better/worse than compared model (paired t-tests at 95% signiﬁcance level).

)

%

(

.
c
c
A

75

70

65

60

55

50

45

FCM

PCCA

FDC

1

2

3
4
Largest index assignment

5

)

%

(

.
c
c
A

75

70

65

60

55

50

45

FCM

PCCA

FDC

1

2

4
3
Largest index assignment

5

)

%

(

.
c
c
A

75

70

65

60

55

50

45

FCM

PCCA

FDC

1

2

3
4
Largest index assignment

5

Group (i)

Group (ii)

Group (iii)

)

%

(

.
c
c
A

75

70

65

60

55

50

45

FCM

PCCA

FDC

1

2

4
3
Largest index assignment

5

)

%

(

.
c
c
A

75

70

65

60

55

50

45

FCM

PCCA

FDC

1

2

3
4
Largest index assignment

5

Fig. 4. LIA-based Acc. of the fuzzy clustering models on JAFFE with the ﬁve groups of fuzzy pairwise constraints.

Group (iv)

Group (v)

in our FDC indicates its outperformance with fuzzy pairwise
constraints. Moreover, the highest criteria of FDC reveals that
it discovers more facial expressions than other models.

For facial expression clustering problem, the challenge is to
discover more expressions in an image if it contains more
than one expressions as shown in Fig. 1. Due to fuzzy
clustering obtains fuzzy vectors regarded as ratings on the
clusters, we focus on FCM, PCCA and FDC to evaluate
the ability of discovering more expressions in the following.
We propose two vector-level criteria based on ranking: (i)
Minimal Average Hamming Distance (MAHD); (ii) Largest
Index Assignment (LIA). After ranking two fuzzy vectors,
their Hamming distance [54] can be calculated easily if their
cluster indices are aligned. The ground truth cluster indices
of JAFFE have been given, but the cluster indices of fuzzy
matrix are not known by fuzzy clustering. Thus, we deﬁne
the MAHD as the minimum of the average Hamming distance

between the ground truth matrix and a fuzzy matrix for all
possible cluster indices. Table V reported the MAHDs of the
three fuzzy clustering models on the JAFFE with ﬁve groups
of fuzzy pairwise constraints, where the smallest values were
bold. Compared with FCM and PCCA, the MAHDs of our
FDC are the smallest ones on the ﬁve groups, which implies
that the fuzzy matrix by FDC is more similar to the ground
truth. Though more fuzzy pairwise constraints greatly improve
the hard clustering results in Table III, the improvements in
Table V are inapparent. Therefore, we infer from Table V
that our FDC may discover more expressions than FCM and
PCCA, but the number of discovered expressions would be
limited.

The criterion MAHD has several shortcomings, e.g.,
it
cannot detail each cluster, the calculation only suits for the
same size of fuzzy matrices, and the permutation number is
going to be huge for a slightly larger k. Hence, we hire another

 
 
 
 
 
IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. X, NO. X,XXXXXX

12

TABLE IV
EXAMPLES OF THE FUZZY VECTORS BY THE FUZZY CLUSTERING MODELS ON JAFFE

ID

Image

Ground truth
Happy (0.91)

FCM
Happy (0.20)

PCCA
Happy (0.32)

FDC
Happy (0.97)

Surprise

Disgust

Surprise

Disgust

Surprise

Disgust

Surprise

Disgust

45

56

75

180

53

1

150

Sad

Angry

Sad

Angry

Sad

Angry

Sad

Angry

Happy

Happy

Happy

Happy

Surprise
(0.53)

Disgust

Surprise
(0.20)

Disgust

Surprise
(0.25)

Disgust

Surprise
(1.00)

Disgust

Sad

Angry

Sad

Angry

Sad

Angry

Sad

Angry

Happy

Happy

Happy

Happy

Surprise

Disgust
(0.42)

Surprise

Disgust
(0.20)

Surprise

Disgust
(0.28)

Surprise

Disgust
(0.86)

Sad

Angry

Sad

Angry

Sad

Angry

Sad

Angry

Happy

Happy

Happy

Happy

Surprise
(0.48)

Disgust

Surprise
(0.20)

Disgust

Surprise
(0.31)

Disgust

Surprise
(0.44)

Disgust

Sad

Angry

Sad

Angry

Sad

Angry

Sad

Angry

Happy

Happy

Happy

Happy

Surprise

Disgust

Surprise

Disgust

Surprise

Disgust

Surprise

Disgust

Sad (0.39)

Angry

Sad (0.20)

Angry

Sad (0.31)

Angry

Sad (0.36)

Angry

Happy

Happy

Happy

Happy

Surprise

Disgust

Surprise

Disgust

Surprise

Disgust

Surprise

Disgust

Sad

Angry (0.40)

Sad

Angry (0.20)

Sad

Angry (0.29)

Sad

Angry (0.51)

Happy

Happy

Happy

Happy

Surprise

Disgust
(0.42)

Surprise

Disgust
(0.20)

Surprise

Disgust
(0.29)

Surprise

Disgust
(0.34)

Sad

Angry

Sad

Angry

Sad

Angry

Sad

Angry

TABLE V
MINIMAL AVERAGE HAMMING DISTANCE (MAHD) ON JAFFE

Group
(i)
(ii)
(iii)
(iv)
(v)

FCM
0.7614±0.0087
0.7614±0.0087
0.7614±0.0087
0.7614±0.0087
0.7614±0.0087

PCCA
0.7603±0.0209
0.7606±0.0257
0.7554±0.0131
0.7599±0.0144
0.7504±0.0204

FDC
0.7233±0.0126
0.7162±0.0092
0.7176±0.0106
0.7179±0.0090
0.7117±0.0095

LIA to evaluate the contributions of fuzzy memberships based
on the hard clustering criterion. Given a hard clustering
criterion such as Acc., LIA obtains k∗ results by labeling
the samples with the 1st, 2nd, . . ., or k∗-th largest indices
for all the fuzzy vectors in the ground truth and prediction,
respectively. For instance, the criteria based on (1) are actually
based on the 1st LIA. The coherence reﬂexes the ability of
discovering more expressions. Fig. 4 reported the LIA-based
Acc. of FCM, PCCA and our FDC on the JAFFE database

with the ﬁve groups of fuzzy pairwise constraints. From Fig.
4, we observe that the 1st LIA-based Acc. of PCCA is lower
than FCM and FDC, which is consistent with the results in
Table III. Moreover, the lower overall LIA-based Acc. of
PCCA implies that the recognition ability of PCCA on the
facial expressions is lower than FCM and our FDC. Compared
with FCM, the overall LIA-based Acc. of our FDC is more
higher, which is consistent with MAHD. By comparing the
coherence among the LIA-based Acc. in Fig. 4, we observe
that: (i) The coherence of PCCA is disordered; (iii) The 1st
and 2nd LIA-based Acc.’s of FCM are coherent, with the
3rd slightly lower LIA-based Acc.; (iii) The phenomenon of
FCM appears in FDC. Consequently, it is inferred that for the
ability of discovering more expressions in an image, FCM and
our FDC can discover the 2nd expression with the ability the
same as discovering the 1st one, and they may discover the
3rd expression in an image but the ability is not as strong as
previous. For PCCA, its ability is obviously weaker than FCM

IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. X, NO. X,XXXXXX

13

and FDC.

To further reveal the differences among these fuzzy clus-
tering models, Table IV illustrated some examples of the
results by the three fuzzy clustering models on JAFFE with
the fuzzy pairwise constraints of group (v). Since the facial
expression labels are unknown in these models, we selected
several representatives and mixtures of the clusters in Table
IV, where the number along with the facial expression is the
largest fuzzy membership in the fuzzy vector. Apparently, the
differences of fuzzy memberships in FCM are tiny, and these
in PCCA are disordered, supporting the conclusion from Fig.
4. From the images in Table IV, we deem that the represen-
tatives of our FDC on “Happy”, “Surprise” and “Disgust”
are much accurate. For the images with more expressions,
the distinctiveness of FDC is superior to FCM and PCCA
obviously. Therefore, our FDC outperforms FCM and PCCA
on JAFFE from various perspectives.

V. CONCLUSION

The fuzzy pairwise constraint has been proposed in fuzzy
clustering, and a fuzzy discriminant clustering (FDC) model
has also been proposed to utilize the fuzzy pairwise con-
straints. The discriminant structure of cluster prototypes and
piecewise cost function of fuzzy pairwise constraint allow our
FDC to present the fuzzy characteristics precisely. The non-
convex optimization problem in FDC has been decomposed
into several CQPPs and IQPPs by the MEM algorithm, where
the global solutions to these CQPPs have been given explicitly
or solved by some CQPP solvers, and the stationary points
to these IQPPs have been obtained by a proposed DBCD
algorithm efﬁciently. Under certain conditions, e.g., binary
clustering problem with disjoint fuzzy pairwise constraints, it
has been proved that the global solutions to these IQPPs can
be obtained by the DBCD algorithm. Moreover, FDC has been
extended into various metric spaces to suit for different appli-
cations. Experimental results on the benchmark datasets and
facial expression clustering problem have indicated that our
FDC outperforms many other state-of-the-art clustering mod-
els. For practical convenience, the corresponding FDC codes
have been uploaded upon https://github.com/gamer1882/FDC.
Future work includes applying fuzzy pairwise constraint for
other fuzzy models and designing speciﬁc discriminant struc-
tures [55], [56] to suit for various applications.

ACKNOWLEDGMENT

This work is supported in part by National Natural
Science Foundation of China (Nos. 61966024, 61866010
and 11871183), in part by Natural Science Foundation of
Inner Mongolia Autonomous Region (Nos. 2019BS01009,
2019MS06008), and in part by the Fundamental Research
Funds for the Central Universities, JLU.

REFERENCES

[1] M. Anderberg, Cluster Analysis for Applications. New York: Academic

Press, 1973.

[2] H. Liu, Z. Tao, and Y. Fu, “Partition level constrained clustering,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 40,
no. 10, pp. 2469–2483, 2017.

[3] J. Mei, “Semisupervised fuzzy clustering with partition information of
subsets,” IEEE Transactions on Fuzzy Systems, vol. 27, no. 9, pp. 1726–
1737, 2018.

[4] P. Mishro, S. Agrawal, R. Panda, and A. Abraham, “A novel type-2 fuzzy
c-means clustering for brain mr image segmentation,” IEEE Transactions
on Cybernetics, vol. doi: 10.1109/TCYB.2020.2994235, 2020.

[5] J. Shen, X. Dong, J. Peng, X. Jin, L. Shao, and F. Porikli, “Submodular
function optimization for motion clustering and image segmentation,”
IEEE Transactions on Neural Networks and Learning Systems, vol. 30,
no. 9, pp. 2637–2649, 2019.

[6] J. Bezdek, R. Ehrlich, and W. Full, “Fcm: The fuzzy c-means clustering
algorithm,” Computers & Geosciences, vol. 10, no. 2-3, pp. 191–203,
1984.

[7] H. Huang, Y. Chuang, and C. Chen, “Multiple kernel fuzzy clustering,”
IEEE Transactions on Fuzzy Systems, vol. 20, no. 1, pp. 120–134, 2011.
[8] A. Uc¸ar, Y. Demir, and C. G¨uzelis¸, “A new facial expression recognition
transform and online sequential extreme learning
based on curvelet
machine initialized with spherical clustering,” Neural Computing and
Applications, vol. 27, no. 1, pp. 131–142, 2016.

[9] T. Vandal, D. McDuff, and R. E.K., “Event detection: Ultra large-scale
clustering of facial expressions,” in 11th IEEE International Conference
and Workshops on Automatic Face and Gesture Recognition (FG), vol. 1.
IEEE, 2015, pp. 1–8.

[10] A. Pour, A. Taheri, M. Alemi, and A. Meghdari, “Human-robot facial
expression reciprocal interaction platform: case studies on children with
autism,” International Journal of Social Robotics, vol. 10, no. 2, pp.
179–198, 2018.

[11] Z. Hu, Y. Bodyanskiy, N. Kulishova, and O. Tyshchenko, “A multidi-
mensional extended neo-fuzzy neuron for facial expression recognition,”
International Journal of Intelligent Systems and Applications, vol. 9,
no. 9, pp. 29–36, 2017.

[12] H. Andres, A. Bonarini, E. Enrique, M. N.M., and P. Hector, “Facial
expression recognition with automatic segmentation of face regions
using a fuzzy based classiﬁcation approach,” Knowledge-Based Systems,
vol. 110, pp. 1–14, 2016.

[13] L. Chen, C. Chen, and M. Lu, “A multiple-kernel

fuzzy c-means
algorithm for image segmentation,” IEEE Transactions on Systems, Man,
and Cybernetics, Part B (Cybernetics), vol. 41, no. 5, pp. 1263–1274,
2011.

[14] Y. Yan, L. Chen, and W. Tjhi, “Fuzzy semi-supervised co-clustering for

text documents,” Fuzzy Sets and Systems, vol. 215, pp. 74–89, 2013.

[15] N. Grira, M. Crucianu, and N. Boujemaa, “Semi-supervised fuzzy
clustering with pairwise-constrained competitive agglomeration,” in The
14th IEEE International Conference on Fuzzy Systems.
IEEE, 2005,
pp. 867–872.

[16] P. Massimiliano and P. Marcello, “Dominant sets and pairwise cluster-
ing,” IEEE Transactions on Pattern Analysis and Machine Intelligence,
vol. 29, no. 1, pp. 167–172, 2006.

[17] Y. Ren, X. Hu, K. Shi, G. Yu, D. Yao, and Z. Xu, “Semi-supervised den-
peak clustering with pairwise constraints,” in Paciﬁc Rim International
Conference on Artiﬁcial Intelligence, vol. 11012. Springer, 2018, pp.
837–850.

[18] H. Frigui and C. Hwang, “Fuzzy clustering and aggregation of rela-
tional data with instance-level constraints,” IEEE Transactions on Fuzzy
Systems, vol. 16, no. 6, pp. 1565–1581, 2008.

[19] J. Nayak, B. Naik, and H. Behera, “Fuzzy c-means (fcm) clustering
algorithm: a decade review from 2000 to 2014,” in Computational
Intelligence in Data Mining.
Springer, New Delhi, 2010, vol. 2, pp.
133–149.

[20] J. Mei, H. Lv, J. Cao, and W. Gong, “Pairwise constrained fuzzy clus-
tering: Relation, comparison and parallelization,” International Journal
of Fuzzy Systems, vol. 21, no. 6, pp. 1938–1949, 2019.

[21] F. de Melo and F. de Carvalho, “Semi-supervised fuzzy c-medoids
clustering algorithm with multiple prototype representation,” IEEE In-
ternational Conference on Fuzzy Systems, pp. 1–7, 2013.

[22] N. Grira, M. Crucianu, and N. Boujemaa, “Active semi-supervised fuzzy
clustering,” Pattern Recognition, vol. 41, no. 5, pp. 1834–1844, 2008.
[23] M. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba, “Coding facial
expressions with gabor wavelets,” in Proceedings 3rd IEEE International
Conference on Automatic Face and Gesture Recognition.
IEEE, 1998,
pp. 200–205.

[24] H. Martin, T. Alexander, and K. Anil, “Model-based clustering with
probabilistic constraints,” in Proceedings of the 2005 SIAM International
Conference on Data Mining. SIAM, 2005, pp. 641–645.

IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. X, NO. X,XXXXXX

14

[50] C. Hsieh, L. Yang, Y. Cui, T. Lin, S. Belongie, and D. Estrin, “Col-
laborative metric learning,” in Proceedings of the 26th International
Conference on World Wide Web, 2017, pp. 193–201.

[51] R. Khemchandani, Jayadeva, and S. Chandra, “Optimal kernel selection
in twin support vector machines,” Optimization Letters, vol. 3, pp. 77–
88, 2009.

[52] Z. Wang, Y. Shao, L. Bai, and N. Deng, “Twin support vector machine
for clustering,” IEEE Transactions on Neural Networks and Learning
Systems, vol. 26, no. 10, pp. 2583–2588, 2015.

[53] X. Huang, Y. Ye, and H. Zhang, “Extensions of kmeans-type algorithms:
a new clustering framework by integrating intracluster compactness and
intercluster separation.” IEEE Transactions on Neural Networks and
Learning Systems, vol. 25, no. 8, pp. 1433–1446, 2014.

[54] K. Pang and A. El Gamal, “Communication complexity of computing
the hamming distance,” SIAM Journal on Computing, vol. 15, no. 4, pp.
932–947, 1986.

[55] Z. Wang, Y. Shao, L. Bai, C. Li, and L. Liu, “General plane-
based clustering with distribution loss,” IEEE Transactions on
Neural Networks and Learning Systems, vol. early access, doi:
10.1109/TNNLS.2020.3016078, 2020.

[56] L. Bai, Y. Shao, Z. Wang, W. Chen, and N. Deng, “Multiple ﬂat projec-
tions for cross-manifold clustering,” IEEE Transactions on Cybernetics,
vol. early access, doi: 10.1109/TCYB.2021.3050487, 2021.

[57] E. Song, Q. Shi, and Y. Zhu, “Acceleration of block coordinate descent
method achieves the o(1/k˜2) rate of convergence for a block coordinate
strong convexity function (in chinese),” Scientia Sinica Mathematica,
vol. 46, no. 10, pp. 1499–1506, 2016.

[58] D. Bertsekas, “Nonlinear programming, second printing,” Athena Scien-

tiﬁc, Belmont, Massachusets, 2003.

[25] M. H.C.L., T. Alexander, and K. Anil, “Clustering with soft and
group constraints,” in Joint IAPR International Workshops on Statistical
Techniques in Pattern Recognition (SPR) and Structural and Syntactic
Pattern Recognition (SSPR). Springer, 2004, pp. 662–670.

[26] L. Michele and W. Martin, “Clustering by soft-constraint afﬁnity prop-
agation: applications to gene-expression data,” Bioinformatics, vol. 23,
no. 20, pp. 2708–2715, 2007.

[27] G. Valerio, R. Andrea, and T. Franco, “Survey on using constraints in
data mining,” Data Mining and Knowledge Discovery, vol. 31, no. 2,
pp. 424–464, 2017.

[28] I. Diaz-Valenzuela, M. Martin-Bautista, and V. Maria-Amparo, “A fuzzy
semisupervised clustering method: application to the classiﬁcation of
scientiﬁc publications,” in International Conference on Information Pro-
cessing and Management of Uncertainty in Knowledge-Based Systems.
Springer, 2014, pp. 179–188.

[29] ——, “On the use of fuzzy constraints in semisupervised clustering,”
IEEE Transactions on Fuzzy Systems, vol. 24, no. 4, pp. 992–999, 2015.
[30] P. Pardalos, “Global optimization algorithms for linearly constrained
indeﬁnite quadratic problems,” Computers & Mathematics with Appli-
cations, vol. 21, no. 6-7, pp. 87–97, 1991.

[31] W. Huyer and A. Neumaier, “Minq8: general deﬁnite and bound con-
strained indeﬁnite quadratic programming,” Computational Optimization
and Applications, vol. 69, no. 2, pp. 351–381, 2018.

[32] N. Mammone, C. Ieracitano, H. Adeli, A. Bramanti, and F. Morabito,
“Permutation jaccard distance-based hierarchical clustering to estimate
eeg network density modiﬁcations in mci subjects,” IEEE Transactions
on Neural Networks and Learning Systems, vol. 29, no. 10, pp. 5122–
5135, 2018.

[33] Z. Tao, H. Liu, H. Fu, and Y. Fu, “Image cosegmentation via saliency-
guided constrained clustering with cosine similarity,” in Proceedings of
the AAAI Conference on Artiﬁcial Intelligence, vol. 31, no. 1, 2017, pp.
4285–4291.

[34] W. Cheng, X. Zhu, X. Chen, M. Li, J. Lu, and P. Li, “Manhattan
distance-based adaptive 3d transform-domain collaborative ﬁltering for
laser speckle imaging of blood ﬂow,” IEEE Transactions on Medical
Imaging, vol. 38, no. 7, pp. 1726–1735, 2019.

[35] R. Ramya and T. Sasikala, “An efﬁcient minkowski distance-based
matching with merkle hash tree authentication for biometric recognition
in cloud computing,” Soft Computing, vol. 23, no. 24, pp. 13 423–13 431,
2019.

[36] B. Sch¨olkopf and A. Smola, Learning with kernels.

Cambridge:

MA:MIT Press, 2002.

[37] D. Lai, J. Garibaldi, and J. Reps, “Investigating distance metric learn-
ing in semi-supervised fuzzy c-means clustering,” IEEE International
Conference on Fuzzy Systems, pp. 1817–1824, 2014.

[38] T. Moon, “The expectation-maximization algorithm,” IEEE Signal Pro-

cessing Magazine, vol. 13, no. 6, pp. 47–60, 1996.

[39] Y. Kanzawa, Y. Endo, and S. Miyamoto, “Semi-supervised fuzzy c-
means algorithm by revising dissimilarity between data,” Journal of Ad-
vanced Computational Intelligence and Intelligent Informatics, vol. 15,
no. 1, pp. 95–101, 2011.

[40] X. Yin, T. Shu, and Q. Huang, “Semi-supervised fuzzy clustering with
metric learning and entropy regularization,” Knowledge-Based Systems,
vol. 35, pp. 304–311, 2012.

[41] S. Boyd, S. Boyd, and L. Vandenberghe, “Convex optimization,” Cam-

bridge university press, 2004.

[42] R. Mueller, “A method for solving the indeﬁnite quadratic programming
problem,” Management Science, vol. 16, no. 5, pp. 333–339, 1970.
[43] S. Vavasis, “Approximation algorithms for indeﬁnite quadratic program-
ming,” Mathematical Programming, vol. 57, no. 1, pp. 279–311, 1992.
[44] P. Absil and A. Tits, “Newton-kkt interior-point methods for indeﬁnite
quadratic programming,” Computational Optimization and Applications,
vol. 36, no. 1, pp. 5–41, 2007.

[45] C. Wang, Y. Deng, and P. Shen, “A global optimization algorithm for
solving indeﬁnite quadratic programming.” Engineering Letters, vol. 28,
no. 4, pp. 1058–1062, 2020.

[46] C. Blake and C. Merz, UCI Repository for Machine Learning Databases,

http://www.ics.uci.edu/∼mlearn/MLRepository.html, 1998.

[47] L. Hubert and P. Arabie, “Comparing partitions,” Journal of Classiﬁca-

tion, vol. 2, no. 1, pp. 193–218, 1985.

[48] P. Estevez, M. Tesmer, C. Perez, and et al., “Normalized mutual
information feature selection,” IEEE Transactions on Neural Networks,
vol. 20, no. 2, pp. 189–201, 2009.

[49] B. Kulis, “Metric learning: A survey,” Foundations and Trends in

Machine Learning, vol. 5, no. 4, pp. 287–364, 2012.

IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. X, NO. X,XXXXXX

15

1, v∗

1 , v∗

Suppose v∗ = (v∗

2 ∈ R. Then, we will show that v∗

2)⊤ is the global solution to problem
(30), where v∗
1 is 0 or
2 is 0 or 1 alternatively. Suppose 0 < v∗
1, or v∗
1 < 1 and
2 < 1. Since B⊤DB is indeﬁnite, there is always a
0 < v∗
feasible descent direction for all feasible interior points, which
is contradict with the fact that v∗ is the global solution. Thus,
0 < v∗
2 < 1 cannot holds simultaneously.
Without loss of generality, we suppose v∗

1 < 1 and 0 < v∗

1 is 0 or 1.

2 = 0 or v∗

2)⊤, where 0 < v∗

Note that the equivalence of DBCD becomes to the classical
coordinate descent algorithm. If v∗
2 = 1, the
conclusion of corollary holds obviously. Without loss of gen-
erality, suppose v∗ = (0, v∗
2 < 1. Then, we
will show that the classical coordinate descent algorithm with
initial vertex v(0) which equals (0, 0)⊤ or (0, 1)⊤ converges
to v∗. For v∗, there is always an infeasible descent direction
(d1, d2)⊤, which is the eigenvector of B⊤DB corresponding
to the negative eigenvalue, towards the negative inﬁnity. We
have d1 < 0 obviously. Moreover, d2 6= 0 holds; Otherwise,
the direction (d1, 0)⊤ (i.e., the coordinate direction) is towards
the negative inﬁnity, which is contradict with the fact that
for any ﬁxed v2 problem (30) w.r.t. v1 is a CQPP. Therefore,
1, 1)⊤ with
there is an infeasible point v′ equals (v′
v′
1 < 0, which decreases the objective of (30) from v∗. Then,
the objective at v′ is less than it at corresponding (0, 0)⊤ or
(0, 1)⊤, which implies that 0 is global solution to problem (30)
w.r.t. v1 in the coordinate descent algorithm. In the next step,
the coordinate descent algorithm obtain the global solution
(0, v∗

2) from CQPP (30) w.r.t. v2.

1, 0)⊤ or (v′

APPENDICES

A. Proof of Theorem III.1

Proof: From the procedure of MEM, it is obvious that
the cluster number may reduce to 1 at most. Suppose the ﬁnal
cluster number is k∗. Then, once the cluster number reduce
to k∗, the objective of (5) in neither the expectation step nor
maximization step increases in iteration. Since the objective
in (5) has a lower bound based on its constraints, the series
of the objectives of problem (5) obtained by MEM converges.

B. Proof of Theorem III.2

Proof: From the constraint Au = 1r in problem (16),
the equation u = ˆu + Bv always holds, where the columns
of B build the fundamental system of solutions to the system
of homogeneous linear equations Au = 0r. Substituting the
above equation into problem (16), it is convert to problem
(20). Therefore, the solutions to problems (16) and (20) satisfy
equation (22).

C. Proof of Lemma III.2

Proof: Suppose B⊤DBv = λv, where λ < 0 is an
eigenvalue and v is the corresponding eigenvector. We have
v⊤B⊤DBv = λ,
i.e, (Bv)⊤D(Bv) < 0. Thus, D is
indeﬁnite or negative semi-deﬁnite alternatively. From Lemma
III.1, D is indeﬁnite.

D. Proof of Theorem III.3

Proof: From the constraints of problem (16), 0rk ≤ u ≤
u⊤Du has a lower bound.
1rk holds. Thus, the objective 1
2
The CQPPs (24) are always solved in the steps of DBCD,
which guarantees that the objective does not increase in each
step. Combining the above facts, the series of the objective
u(t)Du(t)|t = 1, 2, . . .} converges. Since the block
values { 1
2
subproblem (24) is strictly convex, its unique global solution
can always be obtained. Thus, any accumulation points of
the block coordinate descent algorithm (i.e., DBCD) are the
stationary points [57], [58].

E. Proof of Corollary III.1

Proof: Note that for k = r = 2, problem (20) becomes

to

min
v
s.t.

v⊤B⊤DBv + ˆu⊤DBv
1
2
02 ≤ v ≤ 12.

(30)

From the proof of Theorem III.2, we just need to prove that
there is a vertex v(0) to problem (30) such that the equivalence
of DBCD with v(0) converges to the global solution to
problem (30).

