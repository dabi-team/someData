9
1
0
2

y
a
M
1
1

]
S
D
.
s
c
[

1
v
7
4
4
4
0
.
5
0
9
1
:
v
i
X
r
a

Solving Empirical Risk Minimization in
the Current Matrix Multiplication Time

Yin Tat Lee ∗

Zhao Song †

Qiuyi Zhang ‡

Many convex problems in machine learning and computer science share the same form:

Abstract

fi(Aix + bi),

min
x

i
X

where fi are convex functions on Rni with constant ni, Ai ∈
i ni = n.
This problem generalizes linear programming and includes many problems in empirical risk
minimization.

Rni×d, bi ∈

Rni and

P

In this paper, we give an algorithm that runs in time

O∗((nω + n2.5−α/2 + n2+1/6) log(n/δ))

where ω is the exponent of matrix multiplication, α is the dual exponent of matrix multiplication,
and δ is the relative accuracy. Note that the runtime has only a log dependence on the condition
numbers or other data dependent parameters and these are captured in δ. For the current bound
0.31 [Le Gall, Urrutia’18], our runtime
ω
O∗(nω log(n/δ)) matches the current best for solving a dense least squares regression problem,
a special case of the problem we consider. Very recently, [Alman’18] proved that all the current
known techniques can not give a better ω below 2.168 which is larger than our 2 + 1/6.

2.38 [Vassilevska Williams’12, Le Gall’14] and α

∼

∼

Our result generalizes the very recent result of solving linear programs in the current matrix
multiplication time [Cohen, Lee, Song’19] to a more broad class of problems. Our algorithm
proposes two concepts which are diﬀerent from [Cohen, Lee, Song’19] :

We give a robust deterministic central path method, whereas the previous one is a stochastic

•
central path which updates weights by a random sparse vector.

We propose an eﬃcient data-structure to maintain the central path of interior point methods

•
even when the weights update vector is dense.

∗yintat@uw.edu University of Washington & Microsoft Research Redmond. Part of the work done while visiting

Simons Institute for the Theory of Computing at Berkeley.

†zhaosong@uw.edu University of Washington & UT-Austin. Part of the work done while visiting University of
Washington and hosted by Yin Tat Lee. Part of the work done while visiting Simons Institute for the Theory of
Computing at Berkeley and hosted by Peter L. Bartlett, Nikhil Srivastava, Santosh Vempala, and David P. Woodruﬀ.

‡qiuyi@math.berkeley.edu University of California, Berkeley.

 
 
 
 
 
 
1 Introduction

Empirical Risk Minimization (ERM) problem is a fundamental question in statistical machine learn-
ing. There are a huge number of papers that have considered this topic [Nes83, Vap92, PJ92, Nes04,
BBM05, BB08, NJLS09, MB11, FGRW12, LRSB12, JZ13, Vap13, SSZ13, DB14, DBLJ14, FGKS15,
DB16, SLC+17, ZYJ17, ZX17, ZWX+17, GSS17, MS17, NS17, AKK+17, Csi18, JLGJ18] as almost
all convex optimization machine learning can be phrased in the ERM framework [SSBD14, Vap92].
While the statistical convergence properties and generalization bounds for ERM are well-understood,
a general runtime bound for general ERM is not known although fast runtime bounds do exist for
speciﬁc instances [AKPS19].

Examples of applications of ERM include linear regression, LASSO [Tib96], elastic net [ZH05],
logistic regression [Cox58, HJLS13], support vector machines [CV95], ℓp regression [Cla05, DDH+09,
BCLL18, AKPS19], quantile regression [Koe00, KH01, Koe05], AdaBoost [FS97], kernel regression
[Nad64, Wat64], and mean-ﬁeld variational inference [XJR02].

The classical Empirical Risk Minimization problem is deﬁned as

m

fi(a⊤i x + bi)

min
x

Xi=1
R,
Rd, and bi ∈
R is a convex function, ai ∈
where fi : R
also captures most standard forms of regularization as well.
Letting yi = a⊤i x + bi, and zi = fi(a⊤i x + bi) allows us to rewrite the original problem in the

[m]. Note that this formulation

→

∈

∀

i

following sense,

m

zi

(1)

Ki =

(yi, zi) : fi(yi)
{

,
zi}

i

∀

∈

≤

[m]

∈

We can consider a more general version where dimension of Ki can be arbitrary, e.g. ni. Therefore,
we come to study the general n-variable form

∈
m
i=1 ni = n. We state our main result for solving the general model.

min

c⊤x

x

Qm

i=1 Ki,Ax=b

where

P

Rd, c

Theorem 1.1 (Main result, informal version of Theorem C.3). Given a matrix A
tors b
constraints and ni = O(1),
solves

n, two vec-
, Km. Assume that there is no redundant
[m]. There is an algorithm (procedure Main in Algorithm 6) that

Rn, and m compact convex sets K1, K2,

· · ·

∈

∈

∈

∈

∀

×

i

Rd

∈
up to δ precision and runs in expected time

min

c⊤x

x

Qm

i=1 Ki,Ax=b

(nω+o(1) + n2.5
−
(cid:16)

O

e

α/2+o(1) + n2+1/6+o(1))

log(

·

n
δ

)

(cid:17)

where ω is the exponent of matrix multiplication, α is the dual exponent of matrix multiplication.

For the current value of ω

simply nω+o(1)

O(log( n

δ )).

∼

2.38 [Wil12, LG14] and α

∼

0.31 [LGU18], the expected time is

e

1

min
x,y,z

Xi=1
s.t. Ax + b = y
(yi, zi)

Remark 1.2. More precisely, when ni is super constant, our running time depends polynomially
on maxi

[m] ni (but not exponential dependence).
∈

Also note that our runtime depends on diameter, but logarithmically to the diameter. So, it can

be applied to linear program by imposing an artiﬁcial bound on the solution.

1.1 Related Work

First-order algorithms for ERM are well-studied and a long series of accelerated stochastic gradi-
ent descent algorithms have been developed and optimized [Nes98, JZ13, XZ14, SSZ14, FGKS15,
LMH15, MLF15, AY16, RHS+16, SS16, AH16, SLRB17, MS17, LMH17, LJCJ17, All17b, All17a,
All18b, All18a]. However, these rates depend polynomially on the Lipschitz constant of
fi and in
order to achieve a log(1/ǫ) dependence, the runtime will also have to depend on the strong convexity
of the
i fi. In this paper, we want to focus on algorithms that depend logarithmically on diam-
eter/smoothness/strong convexity constants, as well as the error parameter ǫ. Note that gradient
descent and a direct application of Newton’s method do not belong to these class of algorithms, but
for example, interior point method and ellipsoid method does.

P

∇

Therefore, in order to achieve high-accuracy solutions for non-smooth and non strongly convex
case, most convex optimization problems will rely on second-order methods, often under the general
interior point method (IPM) or some sort of iterative reﬁnement framework. So, we note that our
algorithm is thus optimal in this general setting since second-order methods require at least nω
runtime for general matrix inversion.

Our algorithm applies the interior point method framework to solve ERM. The most general
interior point methods require O(√n)-iterations of linear system solves [Nes98], requiring a naive
runtime bound of O(nω+1/2). Using the inverse maintenance technique [Vai89, CLS19], one can
improve the running time for LP to O(nω). This essentially implies that almost all convex opti-
mization problems can be solved, up to subpolynomial factors, as fast as linear regression or matrix
inversion!

−

1/2

1/p

Op(n|

The speciﬁc case of ℓ2 regression can be solved in O(nω) time since the solution is explicitly
In the more general case of ℓp regression, [BCLL18] proposed
given by solving a linear system.
|)-iteration iterative solver with a naive O(nω) system solve at each step. Recently,
a
Op(nmax (ω,7/3)), which is current matrix multiplication time as
[AKPS19] improved the runtime to
ω > 7/3. However, both these results depend exponentially on p and fail to be impressive for large
e
p. Otherwise, we are unaware of other ERM formulations that have have general runtime bounds
for obtaining high-accuracy solutions.

e

Recently several works [AW18a, AW18b, Alm18] try to show the limitation of current known
techniques for improving matrix multiplication time. Alman and Vassilevska Williams [AW18b]
proved limitations of using the Galactic method applied to many tensors of interest (including
Coppersmith-Winograd tensors [CW87]). More recently, Alman [Alm18] proved that by applying
the Universal method on those tensors, we cannot hope to achieve any running time better than
n2.168 which is already above our n2+1/6.

2 Overview of Techniques

In this section, we discuss the key ideas in this paper. Generalizing the stochastic sparse update
approach of [CLS19] to our setting is a natural ﬁrst step to speeding up the matrix-vector multipli-
cation that is needed in each iteration of the interior point method. In linear programs, maintaining
approximate complementary slackness means that we maintain x, s to be close multiplicatively to

2

∇

the central path under some notion of distance. However, the generalized notion of complementary
slackness requires a barrier-dependent notion of distance. Speciﬁcally, if φ(x) is a barrier func-
tion, then our distance is now deﬁned as our function gradient being small in a norm depending
2φ(x). One key fact of the stochastic sparse update is that the variance introduced does not
on
perturb the approximation too much, which requires understanding the second derivative of the dis-
tance function. For our setting, this would require bounding the 4th derivative of φ(x), which may
not exist for self-concordant functions. So, the stochastic approach may not work algorithmically
(not just in the analysis) if φ(x) is assumed to be simply self-concordant. Even when assumptions
on the 4th derivative of φ(x) are made, the analysis will become signiﬁcantly more complicated due
to the 4th derivative terms. To avoid these problems, the main contributions of this paper is to 1)
introduce a robust version of the central path and 2) exploit the robustness via sketching to apply
the desired matrix-vector multiplication fast.

More generally, our main observation is that one can generally speed up an iterative method
using sketching if the method is robust in a certain sense. To speed up interior point methods, in
Section 4 and A, we give a robust version of the interior point method; and in Section B, we give a
data structure to maintain the sketch; and in Section C, we show how to combine them together. We
provide several basic notations and deﬁnitions for numerical linear algebra in Section 3. In Section D,
we provide some classical lemmas from the literature of interior point methods. In Section E, we
prove some basic properties of the sketching matrix. Now, we ﬁrst begin with an overview of our
robust central path and then proceed with an overview of sketching iterative methods.

2.1 Central Path Method

We consider the following optimization problem

min

Qm

i=1 Ki,Ax=b

x

∈

c⊤x

(2)

m
where
i=1 Ki is the direct product of m low-dimensional convex sets Ki. We let xi be the i-th block
of x corresponding to Ki. Interior point methods consider the path of solutions to the following
optimization problem:

Q

m

x(t) = arg min
Ax=b

c⊤x + t

φi(xi)

(3)

Xi=1
R are self-concordant barrier functions. This parameterized path is commonly
where φi : Ki →
known as the central path. Many algorithms solve the original problem (2) by following the central
path as the path parameter is decreased t
0. The rate at which we decrease t and subsequently the
runtimes of these path-following algorithms are usually governed by the self-concordance properties
of the barrier functions we use.

→

Deﬁnition 2.1. We call a function φ a ν self-concordant barrier for K if domφ = K and for any
x

domφ and for any u

Rn

∈

∈
D3φ(x)[u, u, u]
|
2φ(x) and

k∗x :=

v
k

| ≤

k∇

and

3/2
u
2
x
k
k

φ(x)
∗x ≤
k
2φ(x)−1 , for any vector v.

k∇

√ν

1 for any self-concordant barrier function.

≥

v
k

where

kx :=

v
k
Remark 2.2. It is known that ν

v
k

k∇

Nesterov and Nemirovsky showed that for any open convex set K

Rn, there is a O(n) self-
In this paper, the convex set Ki we considered has O(1)

⊂

concordant barrier function [Nes98].

3

, we use
x : x > 0
}
{

dimension. While Nesterov and Nemirovsky gave formulas for the universal barrier; in practice,
most ERM problems lend themselves to explicit O(1) self-concordant barriers for majority of the
2); for the
convex functions people use. For example, for the set
k
set
log(x), and so on. That is the reason why we assume the gradient and
hessian can be computed in O(1) time. Therefore, in this paper, we assume a νi self-concordant
2φi in O(1) time. The main result we
barrier φi is provided and that we can compute
will use about self-concordance is that the norm

∇
k · kx is stable when we change x.
Theorem 2.3 (Theorem 4.1.6 in [Nes98]). If φ is a self-concordant barrier and if
then we have :

y
k

x

kx < 1,

−

, we use
< 1
}

φi and

x :
{

log(1

x
k

− k

∇

−

−

x

k

(1

y
− k

−

x

kx)2

∇

2φ(x)

(cid:22) ∇

2φ(y)

1

(cid:22)

(1

y
− k

−

x

kx)2 ∇

2φ(x).

In general, we can simply think of φi as a function penalizing any point xi /
∈

Ki. It is known
how to transform the original problem (2) by adding O(n) many variables and constraints so that

•

•

The minimizer x(t) at t = 1 is explicitly given.

One can obtain an approximate solution of the original problem using the minimizer at small
t in linear time.

For completeness, we show how to do it in Lemma D.2. Therefore, it suﬃces to study how we can
move eﬃciently from x(1) to x(ǫ) for some tiny ǫ where x(t) is again the minimizer of the problem
(3).

2.2 Robust Central Path

In the standard interior point method, we use a tight ℓ2-bound to control how far we can deviate
from x(t) during the entirety of the algorithm. Speciﬁcally, if we denote γt
i (xi) as the appropriate
measure of error (this will be speciﬁed later and is often called the Newton Decrement) in each block
coordinate xi at path parameter t, then as we let t
0, the old invariant that we are maintaining
is,

→

m

Φt

old(x) =

i (xi)2
γt

O(1).

≤

Xi=1
It can be shown that a Newton step in the standard direction will allow for us to maintain Φt
old to be
1/2) in each iteration, thereby giving
small even as we decrease t by a multiplicative factor of O(m−
a standard O(√m) iteration analysis. Therefore, the standard approach can be seen as trying to
remain within a small ℓ2 neighborhood of the central path by centering with Newton steps after
making small decreases in the path parameter t. Note however that if each γi can be perturbed by
old(x) can easily become too large for the potential argument to work.
an error that is Ω(m−
To make our analysis more robust, we introduce a robust version that maintains the soft-max

1/2), Φt

potential:

m

Φt

new(x) =

exp(λγt

i (xi))

Xi=1

O(m)

≤

for some λ = Θ(log m). The robust central path is simply the region of all x that satisﬁes our poten-
tial inequality. We will specify the right constants later but we always make λ large enough to ensure

4

that γi ≤
into a small multiplicative change in Φt, tolerating errors on each γi of up to O(1/poly log(n)).

1 for all x in the robust central path. Now note that a ℓ

perturbation of γ translates

∞

new(x)

However, maintaining Φt

O(m) is not obvious because the robust central path is a much
wider region of x than the typical ℓ2-neighborhood around the central path. We will show later how
to modify the standard Newton direction to maintain Φt
O(m) as we decrease t. Speciﬁcally,
we will show that a variant of gradient descent of Φt
new in the Hessian norm suﬃces to provide the
correct guarantees.

new(x)

≤

≤

2.3 Speeding up via Sketching

To motivate our sketching algorithm, we consider an imaginary iterative method

z(k+1)

z(k) + P

F (z(k))

·

←

where P is some dense matrix and F (z) is some simple formula that can be computed eﬃciently in
linear time. Note that the cost per iteration is dominated by multiplying P with a vector, which
takes O(n2) time. To avoid the cost of multiplication, instead of storing the solution explicitly, we
u(k). Now, the algorithm becomes
store it implicitly by z(k) = P

·

u(k+1)

←

u(k) + F (P

u(k)).

·

This algorithm is as expensive as the previous one except that we switch the location of P . However,
if we know the algorithm is robust under perturbation of the z(k) term in F (z(k)), we can instead
do

u(k) + F (R⊤RP
u(k+1)
←
for some random Gaussian matrix R : Rb
n. Note that the matrix RP is ﬁxed throughout the whole
×
algorithm and can be precomputed. Therefore, the cost of per iteration decreases from O(n2) to
O(nb).

u(k))

·

For our problem, we need to make two adjustments. First, we need to sketch the change of z,
u(k)), instead of z(k) directly because the change of z is smaller and this creates a
that is F (P
smaller error. Second, we need to use a fresh random R every iteration to avoid the randomness
dependence issue in the proof. For the imaginary iterative process, it becomes

·

z(k+1)
u(k+1)

←

←

⊤R(k)P

z(k) + R(k)
u(k) + F (z(k)).

·

F (z(k)),

After some iterations, z(k) becomes too far from z(k) and hence we need to correct the error by
setting z(k) = P

u(k), which zeros the error.

Note that the algorithm explicitly maintains the approximate vector z while implicitly main-
taining the exact vector z by P u(k). This is diﬀerent from the classical way to sketch Newton
method [PW16, PW17], which is to simply run z(k+1)
F (z(k)) or use another way
to subsample and approximate P . Such a scheme relies on the iteration method to ﬁx the error
accumulated in the sketch, while we are actively ﬁxing the error by having both the approximate
explicit vector z and the exact implicit vector z.

z(k) + R⊤RP

←

·

Without precomputation, the cost of computing R(k)P is in fact higher than that of P

F (z(k)).
The ﬁrst one involves multiplying multiple vectors with P and the second one involves multiplying
; R(T )
1 vector with P . However, we can precompute [R(1)
P by fast matrix multi-
⊤]⊤ ·
plication. This decreases the cost of multiplying 1 vector with P to nω
1 per vector. This is a huge
saving from n2. In our algorithm, we end up using only
O(n) random vectors in total and hence
the total cost is still roughly nω.

⊤; R(2)

· · ·

⊤;

−

·

e

5

·

2.4 Maintaining the Sketch

The matrix P we use in interior point methods is of the form

P = √W A⊤(AW A⊤)−

1A√W

[CLS19] showed one can approximately maintain the
where W is some block diagonal matrix.
O(nω) across all iterations of interior point method. However, the cost of
matrix P with total cost
k0) which is O(n2) for dense vectors.
applying the dense matrix P with a vector z is roughly O(n
z
k
Since interior point methods takes at least √n iterations in general, this gives a total runtime of
O(n2.5). The key idea in [CLS19] is that one can design a stochastic interior point method such
O(√n). This bypasses the n2.5
that each step only need to multiply P with a vector of density
bottleneck.

e

e

P z
k

n matrix R with values

1
√n by P z, we have
O(√n) iterations in interior point method, the total error is roughly

In this paper, we do not have this issue because we only need to compute RP z which is much
cheaper than P z. We summarize why it suﬃces to maintain RP throughout the algorithm.
In
general, for interior point method, the vector z is roughly an unit vector and since P is an orthogonal
k2 = O(1). One simple insight we have is that if we multiply a random
projection, we have
√n
√n ) (Lemma E.5). Since
k∞
×
O(1) in a correctly
there are
norm. In Section A, we showed that this is exactly what interior point method needs
reweighed ℓ
e
for convergence. Furthermore, we note that though each step needs to use a fresh random matrix
Rl of size √n
n budget.
O(n)
Therefore, throughout the algorithm, we simply need to maintain the matrix [R⊤1 ; R⊤2 ;
; R⊤T ]⊤P
which can be done with total cost

e
; R⊤T ]⊤ we need can all ﬁt into

O(nω) across all iterations using idea similar to [CLS19].

n, the random matrices [R⊤1 ; R⊤2 ;

RP z
k

×
· · ·

O( 1

· · ·

=

×

±

∞

e

e

The only reason the data structure looks complicated is that when the block matrix W changes
in diﬀerent location in √W A⊤(AW A⊤)−
; RT ]P
appropriately. This gives us few simple cases to handle in the algorithm and in the proof. For the
intuition on how to maintain P under W change, see [CLS19, Section 2.2 and 5.1].

1A√W , we need to update the matrix [R1; R2;

· · ·

e

2.5 Fast rectangular matrix multiplication

×

n matrices, the time of multiplying them is n2.81 < n3 by applying Strassen’s
Given two size n
original algorithm [Str69]. The current best running time takes nω time where ω < 2.373 [Wil12,
LG14]. One natural extension of multiplying two square matrices is multiplying two rectangular
n matrix?
matrices. What is the running time of multiplying one n
Let α denote the largest upper bound of a such that multiplying two rectangular matrices takes
n2+o(1) time. The α is called the dual exponent of matrix multiplication, and the state-of-the-art
result is α = 0.31 [LGU18]. We use the similar idea as [CLS19] to delay the low-rank update when
the rank is small.

na matrix with another na

×

×

3 Preliminaries

Given a vector x

Rn and m compact convex sets K1 ⊂
m
i=1 ni = n. We use xi to denote the i-th block of x, then x

∈

Rn1, K2 ⊂
∈

Rn2,
· · ·
i=1 Ki if xi ∈

m

, Km ⊂
i
Ki,
∈
∀

Rnm with
[m].

P

Q

6

We say a block diagonal matrix A

m
i=1

Rni×

ni if A can be written as

∈ ⊕

A1

A2



. . .

A = 




n2, and Am ∈
×
to denote its operator norm. There are some trivial facts
k2.
B
kF · k
constraints. In particular, this implies that the constraint matrix A is full rank.

Rn2
n1, A2 ∈
where A1 ∈
Frobenius norm and use
A
k
k
A
AB
k2 and
A
kF ≤ k
k
k

k2 · k
For notation convenience, we assume the number of variables n

nm. For a matrix A, we use






Rnm

Am

Rn1

≥

B

×

×

kF to denote its
A
k
AB
k2 ≤
k

10 and there are no redundant

For a positive integer n, let [n] denote the set
For any function f , we deﬁne

O(f ) to be f

, n

· · ·

.
}

1, 2,
{
logO(1)(f ). In addition to O(
) notation, for two
·
) for some
Cg (resp.

·

For a vector v, We denote

functions f, g, we use the shorthand f . g (resp. &) to indicate that f
e
absolute constant C. For any function f , we use domf to denote the domain of function f .
as the standard Euclidean norm of v and for a symmetric PSD
kA = (v⊤Av)1/2. For a convex function f (x) that is clear from context, we
v
k
2f (x) and
k∇

matrix A, we let
v
v
denote
k
k

k
k∗x =

2f (x)−1 .

kx =

v
k

v
k

v
k

k∇

≥

≤

4 Robust Central Path

In this section we show how to move move eﬃciently from x(1) to x(ǫ) for some tiny ǫ by staying on a
robust version of the central path. Because we are maintaining values that are slightly oﬀ-center, we
perturbations on the order of O(1/poly log(n)).
show that our analysis still goes through despite ℓ

∞

4.1 Newton Step

To follow the path x(t), we consider the optimality condition of (3):

where
· · ·
we consider the perturbed central path

φ(x) = (

φ1(x1),

φ2(x2),

∇

∇

∇

s/t +

∇

φ(x) = 0,

Ax = b,

A⊤y + s = c

φm(xm)). To handle the error incurred in the progress,

,

∇

s/t +

∇

φ(x) = µ,

Ax = b,

A⊤y + s = c

where µ represent the error between the original central path and our central path. Each iteration,
we decrease t by a certain factor. It may increase the error term µ. Therefore, we need a step to
decrease the norm of µ. The Newton method to move µ to µ + h is given by

1
t ·

δideal
s +

∇

2φ(x)

δideal
x = h,

·
Aδideal
y + δideal

x = 0,
= 0

s

A⊤δideal

7

where
(
∇

∇
2φ(x))−

2φ(x) is a block diagonal matrix with the i-th block is given by

1, we can solve this:

δideal
y

=

−

δideal
s

= t

·

t

·

AW A⊤

1

−

AW h,

(cid:16)
A⊤

(cid:17)
AW A⊤

1

−

AW h,

δideal
x = W h

(cid:16)
W A⊤

(cid:17)
AW A⊤

−
(cid:16)
n as follows

1

−

AW h.

(cid:17)

We deﬁne projection matrix P

Rn

×

∈

and then we rewrite them

P = W 1/2A⊤

AW A⊤

1

−

AW 1/2

(cid:16)

(cid:17)

x = W 1/2(I
δideal
δideal
= tW −
s

−

1/2P W 1/2δµ.

P )W 1/2δµ,

2φi(xi). Letting W =

∇

(4)

(5)

2φ(x)−1 and uses
One standard way to analyze the central path is to measure the error by
2φ(x)−1 < 1
10 , one step of Newton
the step induced by h =
step decreases the norm by a constant factor. Therefore, one can alternatively decrease t and do a
Newton step to follow the path.

µ. One can easily prove that if

µ
k

µ
k

k∇

k∇

−

4.2 Robust Central Path Method

In this section, we develop a central path method that is robust under certain ℓ
Due to the ℓ

perturbations.
perturbation, we measure the error µ by a soft max instead of the ℓ2 type potential:

∞

∞
Deﬁnition 4.1. For each i

[m], let µt

i(x, s)

∈

∈

Rni and γt

i (x, s)

∈

R be deﬁned as follows:

∇
µt
i(x, s)
k
and we deﬁne potential function Φ as follows:

µt
i(x, s) = si/t +
γt
i (x, s) =

φi(xi),

2φi(xi)−1,

k∇

(6)

(7)

Φt(x, s) =

m

Xi=1

exp(λγt

i (x, s))

where λ = O(log m).

The robust central path is the region (x, s) that satisﬁes Φt(x, s)

O(m). To run our convergence
argument, we will be setting λ appropriately so that staying on the robust central path will guarantee
bound on γ. Then, we will show how to maintain Φt(x, s) to be small throughout the algorithm
a ℓ
while decreasing t, always staying on the robust central path. This is broken into a two step analysis:
the progress step (decreasing t) and the centering step (moving x, s to decrease γ).

≤

∞

It is important to note that to follow the robust central path, we no longer pick the standard
µ. To explain how we pick our centering step, suppose we can
2φ(x)−1 = α. Then, the

µ + h arbitrarily with the only restriction on the distance

−

Newton direction by setting h =
move µ
natural step would be

→

h
k

k∇

h = arg

min

k∇2φ(x)−1 =α h∇

h

k

f (µ(x, s)), h
i

8

where f (µ) =

m
i=1 exp(λ

µ
k

k∇

2φi(xi)−1). Note that

P

∇

f (µt(x, s))i = λ exp(λγt

i (x, s))/γt

i (x, s)

2φi(xi)−

1µt

i(x, s).

· ∇

Therefore, the solution for the minimization problem is

where µt

i(x, s)

∈

Rni is deﬁned as Eq. (6) and ct

i(x, s)

∈

hideal
i

=

α

−

·

i(x, s)idealµt
ct

i(x, s)

Rni,

∈
R is deﬁned as

i(x, s)ideal =
ct

(

i (x, s))/γt

exp(λγt
m
i=1 exp(2λγt

i (x, s)
i (x, s)))1/2

.

Eq. (4) and Eq. (5) gives the corresponding ideal step on x and s.

P

Now, we discuss the perturbed version of this algorithm. Instead of using the exact x and s in
the formula of h, we use a x which is approximately close to x and a s which is close to s. Precisely,
we have

hi =

α

−

·

i(x, s)µt
ct

i(x, s)

where

ct
i(x, s) =

i (x,s))/γt

exp(λγt
i=1 exp(2λγt

i (x,s)
i (x,s)))1/2

(Pm
0
(

if γt

i (x, s)
otherwise

≥

96√α

.

(8)

(9)

i ensures that ct

Note that our deﬁnition of ct
makes sure we do not move too much in any coordinates and indeed when γt
to set ct
i = 0. Furthermore, for the formula on δx and δs, we use some matrix
(
∇

1. Precisely, we have

2φ(x))−

i(x, s)

96√α regardless of the value of γt

i (x, s). This
i is small, it is ﬁne
V that is close to

≤

1

δx =

V 1/2(I

δs = t
·
e

V −

−
1/2
P

P )

V 1/2h,
V 1/2h.
e
e

e

(10)

(11)

where

P =

V 1/2A⊤(A

e

e

e
V A⊤)−

1A

V 1/2.

Here we give a quick summary of our algorithm. (The more detailed of our algorithm can be
e

e

e

e

found in Algorithm 5 and 6 in Section C.)

RobustIPM(A, b, c, φ, δ)

•

– λ = 216 log(m), α = 2−
– δ = min( 1
λ , δ).
– ν =

20λ−

2, κ = 2−

10α.

m
i=1 νi where νi are the self-concordant parameters of φi.

– Modify the convex problem and obtain an initial x and s according to Lemma D.2.

P
– t = 1.
– While t > δ2
4ν

Find x and s such that

Find

Vi such that (1

−

∗

∗

e

xi −
k
α)(
∇

xikxi < α and
2φi(xi))−

1

(cid:22)

si −
k
(1 + α)(
Vi (cid:22)
e

∇

sik∗xi < tα for all i.
2φi(xi))−

1 for all i.

9

Compute h =

α

−

·

∗

i(x, s)µt
ct

i(x, s) where
i (x,s))/γt
exp(λγt
i=1 exp(2λγt

i (x,s)
i (x,s)))1/2

ct
i(x, s) =

(

(Pm
0

if γt

i (x, s)
otherwise

≥

96√α

.

i(x, s) = si/t +
V 1/2A⊤(A

∇
V A⊤)−

φi(xi) and γt

i (x, s) =

µt
k

i(x, s)

k∇

2φi(xi)−1

V 1/2.

1A
V 1/2h and δs = t

and µt
Let
P =
Compute δx =
e
e
Move x
←
tnew = (1
−

∗
∗
∗
∗

V 1/2(I
e
←

x + δx, s
e
κ
√ν )t.

P )
−
s + δs.
e
e

e

V −

1/2

P

V 1/2h.

·

e

e

e

– Return an approximation solution of the convex problem according to Lemma D.2.

Theorem 4.2 (Robust Interior Point Method). Consider a convex problem minAx=b,x
where Ki are compact convex sets. For each i
function φi for Ki. Let ν =

i=1 Ki c⊤x
[m], we are given a νi-self concordant barrier
m
i=1 φi(xi). Assume that

Qm

∈

∈
m
i=1 νi. Also, we are given x(0) = arg minx
m
R.
i=1 Ki, we have that

P

x
k

k2 ≤

1. Diameter of the set: For any x

∈
2. Lipschitz constant of the program:

P

c
Q
k2 ≤
k

L.

Then, the algorithm RobustIPM ﬁnds a vector x such that

c⊤x

≤

Ax=b,x

min
∈

Qm

i=1 Ki

c⊤x + LR

δ,

·

Ax
k

−

b

k1 ≤

3δ

· 

R

in O(√ν log2 m log( ν

δ )) iterations.

x

∈

m


Ki.

Yi=1

Ai,j|
|

+

b
k

k1

,



Xi,j

Proof. Lemma D.2 shows that the initial x and s satisﬁes

where the last inequality is due to our step δ

φi(xi)

k∗xi ≤

∇
repetitively, we have that Φt(x, s)
at the end of the algorithm. This implies that

1
λ and hence Φ1(x, s)
≤

m

1
λ

δ

∇

s +
k

φ(x)
∗x ≤
≤
k
min( 1
λ , δ). This implies that γ1
si +
k
α for the initial x and s. Apply Lemma A.8
e
≤
80 m
α during the whole algorithm. In particular, we have this

i (x, s) =

←
80 m

≤

·

si +
k

φi(xi)
∗xi ≤
k

∇

log(80 m
α )
λ

1

≤

at the end. Therefore, we can apply Lemma D.3 to show that

c, x

h

c, x∗

i ≤ h

+ 4tν

c, x∗

i

≤ h

i

+ δ2

where we used the stop condition for t at the end. Note that this guarantee holds for the modiﬁed
convex program. Since the error is δ2, Lemma D.2 shows how to get an approximate solution for
the original convex program with error LR

δ.

The number of steps follows from the fact we decrease t by 1

·

1
√ν log2 m

−

factor every iteration.

10

References

[AH16]

Zeyuan Allen-Zhu and Elad Hazan. Variance Reduction for Faster Non-Convex Opti-
mization.
In Proceedings of the 33rd International Conference on Machine Learning,
ICML ’16, 2016. Full version available at http://arxiv.org/abs/1603.05643.

[AKK+17] Naman Agarwal, Sham Kakade, Rahul Kidambi, Yin Tat Lee, Praneeth Netrapalli, and
Aaron Sidford. Leverage score sampling for faster accelerated regression and erm. arXiv
preprint arXiv:1711.08426, 2017.

[AKPS19] Deeksha Adil, Rasmus Kyng, Richard Peng, and Sushant Sachdeva. Iterative reﬁnement
for ℓp-norm regression. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium
on Discrete Algorithms (SODA), pages 1405–1424. SIAM, 2019.

[All17a]

[All17b]

[All18a]

Zeyuan Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient meth-
ods. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Com-
puting, pages 1200–1205. ACM, 2017.

Zeyuan Allen-Zhu.
Strongly Non-Convex Parameter.
ference on Machine Learning,
http://arxiv.org/abs/1702.00763.

Natasha: Faster Non-Convex Stochastic Optimization via
In Proceedings of the 34th International Con-
Full version available at

ICML ’17,

2017.

Zeyuan Allen-Zhu.
tic Sum-of-Nonconvex Optimization.
Conference on Machine Learning,
http://arxiv.org/abs/1802.03866.

Katyusha X: Practical Momentum Method for Stochas-
the 35th International
Full version available at

In Proceedings of

ICML ’18, 2018.

[All18b]

Zeyuan Allen-Zhu. Natasha 2: Faster Non-Convex Optimization Than SGD. In Pro-
ceedings of the 32nd Conference on Neural Information Processing Systems, NIPS ’18,
2018. Full version available at http://arxiv.org/abs/1708.08694.

[Alm18]

Josh Alman. Limits on the universal method for matrix multiplication. arXiv preprint
arXiv:1812.08731, 2018.

[AW18a]

Josh Alman and Virginia Vassilevska Williams. Further limitations of the known ap-
proaches for matrix multiplication. In ITCS. arXiv preprint arXiv:1712.07246, 2018.

[AW18b]

Josh Alman and Virginia Vassilevska Williams. Limits on all known (and some un-
known) approaches to matrix multiplication. In 2018 IEEE 59th Annual Symposium on
Foundations of Computer Science (FOCS). IEEE, 2018.

[AY16]

Zeyuan Allen-Zhu and Yang Yuan.
or Sum-of-Non-Convex Objectives.
Conference on Machine Learning,
http://arxiv.org/abs/1506.01972.

Improved SVRG for Non-Strongly-Convex
the 33rd International
In Proceedings of
Full version available at

ICML ’16, 2016.

[BB08]

Léon Bottou and Olivier Bousquet. The tradeoﬀs of large scale learning. In Advances
in neural information processing systems, pages 161–168, 2008.

[BBM05] Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complex-

ities. The Annals of Statistics, 33(4):1497–1537, 2005.

11

[BCLL18] Sébastien Bubeck, Michael B Cohen, Yin Tat Lee, and Yuanzhi Li. An homotopy
method for ℓp regression provably beyond self-concordance and in input-sparsity time.
In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing
(STOC), pages 1130–1137. ACM, 2018.

[CKPS16] Xue Chen, Daniel M Kane, Eric Price, and Zhao Song. Fourier-sparse interpolation
In Foundations of Computer Science (FOCS), 2016 IEEE

without a frequency gap.
57th Annual Symposium on, pages 741–750. IEEE, 2016.

[Cla05]

Kenneth L Clarkson. Subgradient and sampling algorithms for ℓ1 regression. In Pro-
ceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms (SODA),
pages 257–266, 2005.

[CLS19] Michael B Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current
matrix multiplication time. In STOC. https://arxiv.org/pdf/1810.07896.pdf, 2019.

[Cox58]

David R Cox. The regression analysis of binary sequences. Journal of the Royal Statis-
tical Society. Series B (Methodological), pages 215–242, 1958.

[Csi18]

[CT65]

[CV95]

[CW87]

[DB14]

Dominik Csiba. Data sampling strategies in stochastic algorithms for empirical risk
minimization. arXiv preprint arXiv:1804.00437, 2018.

James W Cooley and John W Tukey. An algorithm for the machine calculation of
complex Fourier series. Mathematics of computation, 19(90):297–301, 1965.

Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning,
20(3):273–297, 1995.

Don Coppersmith and Shmuel Winograd. Matrix multiplication via arithmetic progres-
sions. In Proceedings of the nineteenth annual ACM symposium on Theory of comput-
ing(STOC), pages 1–6. ACM, 1987.

Alexandre Défossez and Francis Bach. Constant step size least-mean-square: Bias-
variance trade-oﬀs and optimal sampling distributions. arXiv preprint arXiv:1412.0156,
2014.

[DB16]

Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with
large step-sizes. The Annals of Statistics, 44(4):1363–1399, 2016.

[DBLJ14] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental
gradient method with support for non-strongly convex composite objectives. In Advances
in neural information processing systems, pages 1646–1654, 2014.

[DDH+09] Anirban Dasgupta, Petros Drineas, Boulos Harb, Ravi Kumar, and Michael W Ma-
honey. Sampling algorithms and coresets for ℓp regression. SIAM Journal on Computing,
38(5):2060–2078, 2009.

[FGKS15] Roy Frostig, Rong Ge, Sham M Kakade, and Aaron Sidford. Competing with the
empirical risk minimizer in a single pass. In Conference on learning theory (COLT),
pages 728–763, 2015.

[FGRW12] Vitaly Feldman, Venkatesan Guruswami, Prasad Raghavendra, and Yi Wu. Agnostic
learning of monomials by halfspaces is hard. SIAM Journal on Computing, 41(6):1558–
1590, 2012.

12

[FS97]

Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line
learning and an application to boosting. Journal of computer and system sciences,
55(1):119–139, 1997.

[GSS17]

Alon Gonen and Shai Shalev-Shwartz. Fast rates for empirical risk minimization of
strict saddle problems. arXiv preprint arXiv:1701.04271, 2017.

[HIKP12a] Haitham Hassanieh, Piotr Indyk, Dina Katabi, and Eric Price. Nearly optimal sparse
fourier transform. In Proceedings of the forty-fourth annual ACM symposium on Theory
of computing, pages 563–578. ACM, 2012.

[HIKP12b] Haitham Hassanieh, Piotr Indyk, Dina Katabi, and Eric Price. Simple and practical
algorithm for sparse Fourier transform. In Proceedings of the twenty-third annual ACM-
SIAM symposium on Discrete Algorithms, pages 1183–1194. SIAM, 2012.

[HJLS13] David W Hosmer Jr, Stanley Lemeshow, and Rodney X Sturdivant. Applied logistic

regression, volume 398. John Wiley & Sons, 2013.

[IK14]

[IKP14]

Piotr Indyk and Michael Kapralov. Sample-optimal fourier sampling in any constant
dimension. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Sym-
posium on, pages 514–523. IEEE, 2014.

Piotr Indyk, Michael Kapralov, and Eric Price. (Nearly) Sample-optimal sparse Fourier
transform. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Dis-
crete Algorithms, pages 480–499. SIAM, 2014.

[JLGJ18] Chi Jin, Lydia T Liu, Rong Ge, and Michael I Jordan. On the local minima of the
empirical risk. In Advances in Neural Information Processing Systems (NeurIPS), pages
4901–4910, 2018.

[JZ13]

Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive
variance reduction. In Advances in neural information processing systems, pages 315–
323, 2013.

[Kap16] Michael Kapralov. Sparse Fourier transform in any constant dimension with nearly-
optimal sample complexity in sublinear time. In Symposium on Theory of Computing
Conference, STOC’16, Cambridge, MA, USA, June 19-21, 2016, 2016.

[Kap17] Michael Kapralov. Sample eﬃcient estimation and recovery in sparse ﬀt via isolation
on average. In Foundations of Computer Science, 2017. FOCS’17. IEEE 58th Annual
IEEE Symposium on. https://arxiv.org/pdf/1708.04544, 2017.

[KH01]

[Koe00]

Roger Koenker and Kevin F Hallock. Quantile regression. Journal of economic perspec-
tives, 15(4):143–156, 2001.

Roger Koenker. Galton, edgeworth, frisch, and prospects for quantile regression in
econometrics. Journal of Econometrics, 95(2):347–374, 2000.

[Koe05]

Roger Koenker. Quantile Regression. Cambridge University Press, 2005.

[LDFU13] Yichao Lu, Paramveer Dhillon, Dean P Foster, and Lyle Ungar. Faster ridge regression
via the subsampled randomized hadamard transform. In Advances in neural information
processing systems, pages 369–377, 2013.

13

[Lee17]

[LG14]

[LGU18]

Tat
Talk

Lee.
at Michael

Yin
In
https://simons.berkeley.edu/talks/welcome-andbirds-eye-view-michaels-work.,
2017.

sampling
Cohen Memorial

maintenance.
at:

and
Symposium.

Available

Uniform

inverse

François Le Gall. Powers of tensors and fast matrix multiplication. In Proceedings of
the 39th international symposium on symbolic and algebraic computation(ISSAC), pages
296–303. ACM, 2014.

Francois Le Gall and Florent Urrutia. Improved rectangular matrix multiplication using
powers of the coppersmith-winograd tensor. In Proceedings of the Twenty-Ninth Annual
ACM-SIAM Symposium on Discrete Algorithms(SODA), pages 1029–1046. SIAM, 2018.

[LJCJ17] Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex ﬁnite-sum op-
timization via scsg methods. In Advances in Neural Information Processing Systems,
pages 2348–2358, 2017.

[LM00]

Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by
model selection. Annals of Statistics, pages 1302–1338, 2000.

[LMH15] Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. A universal catalyst for ﬁrst-order
optimization. In Advances in Neural Information Processing Systems, pages 3384–3392,
2015.

[LMH17] Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. Catalyst acceleration for ﬁrst-order
convex optimization: from theory to practice. arXiv preprint arXiv:1712.05654, 2017.

[LRSB12] Nicolas Le Roux, Mark W Schmidt, and Francis R Bach. A stochastic gradient method
with an exponential convergence rate for ﬁnite training sets. In NIPS, pages 2672–2680,
2012.

[MB11]

[MLF15]

[MS17]

Eric Moulines and Francis R Bach. Non-asymptotic analysis of stochastic approximation
algorithms for machine learning. In Advances in Neural Information Processing Systems,
pages 451–459, 2011.

Zhuang Ma, Yichao Lu, and Dean Foster. Finding linear structure in large datasets
with scalable canonical correlation analysis. In International Conference on Machine
Learning, pages 169–178, 2015.

Tomoya Murata and Taiji Suzuki. Doubly accelerated stochastic variance reduced dual
averaging method for regularized empirical risk minimization. In Advances in Neural
Information Processing Systems, pages 608–617, 2017.

[Nad64]

Elizbar A Nadaraya. On estimating regression. Theory of Probability & Its Applications,
9(1):141–142, 1964.

[Nes83]

[Nes98]

Yurii E Nesterov. A method for solving the convex programming problem with conver-
gence rate O(1/k2). In Dokl. Akad. Nauk SSSR, volume 269, pages 543–547, 1983.

Yurii Nesterov. Introductory lectures on convex programming volume i: Basic course.
Lecture notes, 1998.

14

[Nes04]

Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87.
Springer Science & Business Media, 2004.

[NJLS09] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust
stochastic approximation approach to stochastic programming. SIAM Journal on opti-
mization, 19(4):1574–1609, 2009.

[NS17]

Yurii Nesterov and Sebastian U Stich. Eﬃciency of the accelerated coordinate de-
scent method on structured optimization problems. SIAM Journal on Optimization,
27(1):110–123, 2017.

[NSW19] Vasileios Nakos, Zhao Song, and Zhengyu Wang. (Nearly) sample-optimal sparse Fourier

transform in any dimension; RIPless and Filterless. In manuscript, 2019.

[PJ92]

[Pri13]

[PS15]

Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by
averaging. SIAM Journal on Control and Optimization, 30(4):838–855, 1992.

Eric C. Price. Sparse recovery and Fourier sampling. PhD thesis, Massachusetts Institute
of Technology, 2013.

Eric Price and Zhao Song. A robust sparse Fourier transform in the continuous setting.
In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on,
pages 583–600. IEEE, 2015.

[PSW17] Eric Price, Zhao Song, and David P. Woodruﬀ. Fast regression with an ℓ

guarantee. In
International Colloquium on Automata, Languages, and Programming (ICALP), 2017.

∞

[PW16]

[PW17]

Mert Pilanci and Martin J Wainwright.
Iterative hessian sketch: Fast and accurate
solution approximation for constrained least-squares. The Journal of Machine Learning
Research, 17(1):1842–1879, 2016.

Mert Pilanci and Martin J Wainwright. Newton sketch: A near linear-time optimization
algorithm with linear-quadratic convergence. SIAM Journal on Optimization, 27(1):205–
245, 2017.

[RHS+16] Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochas-
tic variance reduction for nonconvex optimization. In International conference on ma-
chine learning, pages 314–323, 2016.

[SLC+17] Fanhua Shang, Yuanyuan Liu, James Cheng, KW Ng, and Yuichi Yoshida. Vari-
arXiv preprint

ance reduced stochastic gradient descent with suﬃcient decrease.
arXiv:1703.06807, 2017.

[SLRB17] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing ﬁnite sums with the

stochastic average gradient. Mathematical Programming, 162(1-2):83–112, 2017.

[SS16]

Shai Shalev-Shwartz. SDCA without duality, regularization, and individual convexity.
In International Conference on Machine Learning, pages 747–754, 2016.

[SSBD14] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory

to algorithms. Cambridge university press, 2014.

15

[SSZ13]

[SSZ14]

[Str69]

[Tib96]

[Vai89]

[Vap92]

[Vap13]

[Wat64]

[Wil12]

[XJR02]

[XZ14]

[ZH05]

Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for
regularized loss minimization. Journal of Machine Learning Research, 14(Feb):567–599,
2013.

Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordi-
nate ascent for regularized loss minimization. In International Conference on Machine
Learning, pages 64–72, 2014.

Volker Strassen. Gaussian elimination is not optimal. Numerische Mathematik,
13(4):354–356, 1969.

Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the
Royal Statistical Society. Series B (Methodological), pages 267–288, 1996.

Pravin M Vaidya. Speeding-up linear programming using fast matrix multiplication. In
FOCS. IEEE, 1989.

Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in
neural information processing systems, pages 831–838, 1992.

Vladimir Vapnik. The nature of statistical learning theory. Springer science & business
media, 2013.

Geoﬀrey S Watson. Smooth regression analysis. Sankhy¯a: The Indian Journal of Statis-
tics, Series A, pages 359–372, 1964.

Virginia Vassilevska Williams. Multiplying matrices faster than coppersmith-winograd.
In Proceedings of the forty-fourth annual ACM symposium on Theory of computing
(STOC), pages 887–898. ACM, 2012.

Eric P Xing, Michael I Jordan, and Stuart Russell. A generalized mean ﬁeld algorithm for
variational inference in exponential families. In Proceedings of the Nineteenth conference
on Uncertainty in Artiﬁcial Intelligence, pages 583–591. Morgan Kaufmann Publishers
Inc., 2002.

Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive
variance reduction. SIAM Journal on Optimization, 24(4):2057–2075, 2014.

Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301–
320, 2005.

[ZWX+17] Shun Zheng, Jialei Wang, Fen Xia, Wei Xu, and Tong Zhang. A general distributed
dual coordinate optimization framework for regularized loss minimization. The Journal
of Machine Learning Research, 18(1):4096–4117, 2017.

[ZX17]

[ZYJ17]

Yuchen Zhang and Lin Xiao. Stochastic primal-dual coordinate method for regularized
empirical risk minimization. The Journal of Machine Learning Research, 18(1):2939–
2980, 2017.

Lijun Zhang, Tianbao Yang, and Rong Jin. Empirical risk minimization for stochas-
tic convex optimization: O(1/n)-and O(1/n2)-type of risk bounds. arXiv preprint
arXiv:1702.02030, 2017.

16

Section

Statement
Parameters
Lemma A.2 Section A.2 µt
i(x, s)
→
γt
Lemma A.4 Section A.2
i (x, x, s)
Lemma A.5 Section A.3 Φ(x, x, s)
Lemma A.6 Section A.4 Φ(xnew, x, snew)
Lemma A.7 Section A.5 Φt
Lemma A.8 Section A.6 Φt(x, s)

→
→
Φtnew

→

Φtnew

→

i(xnew, snew)
µt
γt
i (xnew, x, snew)
Φ(xnew, x, snew)

Φ(xnew, xnew, snew)

→

(xnew, snew)

Table 1: Bounding the changes of diﬀerent variables

Appendix

A Robust Central Path

The goal of this section is to analyze robust central path. We provide an outline in Section A.1. In
Section A.2, we bound the changes in µ and γ. In Section A.3, we analyze the changes from (x, x, s)
to (xnew, x, snew). In Section A.4, we analyze the changes from (xnew, x, snew) to (xnew, xnew, snew).
We bound the changes in t in Section A.5. Finally, we analyze entire changes of potential function
in Section A.6.

A.1 Outline of Analysis

Basically, the main proof is just a simple calculation on how Φt(x, s) changes during 1 iteration.
It could be compared to the proof of ℓ
potential reduction arguments for the convergence of
long-step interior point methods, although the main diﬃculty arises from the perturbations from
stepping using x, s instead of x, s.

∞

To organize the calculations, we note that the term γt

2φi(xi)−1 has two terms
involving x, one in the µ term and one in the Hessian. Hence, we separate how diﬀerent x aﬀect
the potential by deﬁning

i (x, s) =

i(x, s)

µt
k

k∇

γt
i (x, z, s) =

Φt(x, z, s) =

µt
i(x, s)
k
m

k∇
exp(λγt

2φi(zi)−1,

i (x, z, s)).

Xi=1

One diﬀerence between our proof and standard ℓ2 proofs of interior point is that we assume the
barrier function is decomposable. We deﬁne αi =
δx,ikxi is the “step” size of the coordinate i. One
k
crucial fact we are using is that sum of squares of the step sizes is small.

Lemma A.1. Let α denote the parameter in RobustIPM. For all i

[m], let αi =

δx,ikxi. Then,
k

∈

m

Xi=1

α2

i ≤

4α2.

Proof. Note that

m

Xi=1

α2

i =

2
x = h⊤

δxk
k

V 1/2(I

−

P )

V 1/2

∇

2φ(x)

V 1/2(I

−

P )

V 1/2h.

e

e

e

17

e

e

e

Since (1

Using α

−

≤

1

α)(

∇

2φi(xi))−

Vi (cid:22)
α)(
e
−
10000 , we have that

(cid:22)
(1

1

(1 + α)(

∇
1
2φ(x))−

∇

2φi(xi))−

1, we have that

(1 + α)(

∇

(cid:22)

2φ(x))−

1.

(cid:22)

V

e

m

α2

i ≤

2h⊤

V 1/2(I

P )(I

P )

V 1/2h

2h⊤

V h

−

−

≤

Xi=1
e
P is an orthogonal projection at the end. Finally, we note that

e

e

e

e

where we used that I

−

e
V h
h⊤

e

m

2

≤

Xi=1
= 2α2

2
∗
xi

hik
k

m

ct
i(x, s)2

2
µt
i(x, s)
∗
xi
k
k

i (x, s))/γt

exp(2λγt
m
i=1 exp(2λγt

i (x, s)2
i (x, s))1/2 k

µt

2
i(x, s)
∗
xi
k

Xi=1
m
i=1 exp(2λγt
m
i=1 exp(2λγt

P

i (x, s))
i (x, s))

Xi=1
m

2α2

≤

= 2α2

= 2α2

P
P

where the second step follows from deﬁnition of hi (8), the third step follows from deﬁnition ct
the fourth step follows from deﬁnition of γt

i (9),

i (7).

Therefore, putting it all together, we can show

m

Xi=1

α2

i ≤

4α2.

A.2 Changes in µ and γ

We provide basic lemmas that bound changes in µ, γ due to the centering steps.

Lemma A.2 (Changes in µ). For all i

[m], let

∈
i(xnew, snew) = µt
µt

i(x, s) + hi + ǫ(µ)

i

.

Then,

ǫ(µ)
i k∗xi ≤
k

10α

αi.

·

Proof. Let x(u) = uxnew + (1

−

u)x and µnew

i = µt

i(xnew, snew). The deﬁnition of µ (6) shows that

µnew
i = µi +

= µi +

= µi +

1
t
1
t
1
t

δs,i +

∇

φi(xnew
i
1

δs,i +

0 ∇

Z

)

− ∇

φi(xi)

2φi(x(u)

i

)δx,i du

δs,i +

∇

2φi(xi)δx,i +

1

0
Z

∇

(cid:16)

2φi(x(u)

i

)

− ∇

2φi(xi)
(cid:17)

δx,i du.

18

By the deﬁnition of δx and δs (10) and (11), we have that 1

t δs,i +

i = µi + hi + ǫ(µ)
µnew

i

δx,i = hi. Hence, we have

1

V −
i

e

where

1

ǫ(µ)
i =

∇

0
Z
, we note that

(cid:16)

2φi(x(u)

i

To bound ǫ(µ)

i

)

− ∇

2φi(xi)
(cid:17)

δx,i du + (

∇

2φi(xi)

−

1

V −
i

)δx,i.

(12)

e

x(t)
i −
k

xikxi ≤ k

xi −
k
where the ﬁrst step follows from triangle inequality, the third step follows from deﬁnition of αi
(Lemma A.1), and the last step follows from αi ≤

δx,ikxi + α = αi + α

2α (Lemma A.1).

xikxi ≤ k

xikxi +

3α

≤

x(t)
i −

Using α

≤

1
100 , Theorem 2.3 shows that

7α

−

· ∇

2φi(xi)

(cid:22) ∇

2φi(x(u)

i

)

− ∇

2φi(xi)

7α

(cid:22)

· ∇

2φi(xi).

Equivalently, we have

2φi(x(u)

i

)

(
∇

− ∇

2φi(xi))

(
∇

·

2φi(xi))−

1

2φi(x(u)

i

)

(
∇

·

− ∇

2φi(xi))

(cid:22)

(7α)2

· ∇

2φi(xi).

Using this, we have

1

∇

(cid:16)

0
Z

(cid:13)
(cid:13)
(cid:13)
(cid:13)

2φi(x(u)

i

)

− ∇

2φi(xi)
(cid:17)

∗

δx,i du

xi

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

≤

1

2φi(x(u)

i

∇

0
Z
(cid:13)
(cid:16)
(cid:13)
δx,ikxi = 7α
7α
(cid:13)
k

)

·

− ∇

αi,

δx,i

2φi(xi)
(cid:17)

du

∗
xi

(cid:13)
(cid:13)
(cid:13)

(13)

where the last step follows from deﬁnition of αi (Lemma A.1).

For the other term in ǫ(µ)

i

, we note that

(1

2α)

(
∇

·

−

2φi(xi))

1
V −
i (cid:22)

(cid:22)

(1 + 2α)

2φi(xi)).

(
∇

·

Hence, we have

Combining (12), (13) and (14), we have
e

(
∇

2φi(xi)

1

V −
i

−

(cid:13)
(cid:13)
(cid:13)

∗
xi ≤

δx,ikxi = 2α
2α
k

·

αi.

(14)

e
)δx,i

(cid:13)
(cid:13)
(cid:13)

ǫ(µ)
i k
k

∗xi ≤

9α

αi.

·

Finally, we use the fact that xi and xi are α close and hence again by self-concordance,
10α

αi.

·

ǫ(µ)
i k∗xi ≤
k

Before bounding the change of γ, we ﬁrst prove a helper lemma:

Lemma A.3. For all i

[m], we have

∈

µt
i(x, s)
k

µt
i(x, s)
∗xi ≤
k

−

4α.

19

Proof. Note that

For the ﬁrst term, we have

µt
i(x, s)
k

µt
i(x, s)
∗xi =
k
−
sik∗xi ≤
tα.
si −
k
For the second term, let x(u)
i = uxi + (1
2φi(x(u)
2φi(xi). Hence, we have

si −

sik

shows that

−

2

)

∇

i

(cid:22)

· ∇

1
t k

∗xi +

φi(xi)

φi(xi)
∗xi.
k

− ∇

k∇

u)xi. Since xi is close enough to xi, Theorem 2.3

φi(xi)

k∇

− ∇

φi(xi)
∗xi =
k

2φi(x(u)

i

)

(xi −

·

xi)du

∗

xi

xi −
2
k

xikxi = 2α.

≤

1

0 ∇

Z

(cid:13)
(cid:13)
(cid:13)
(cid:13)
k∗xi ≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Hence, we have
result.

µt
i(x, s)
k

−

µt
i(x, s)

3α and using again xi is close enough to xi to get the ﬁnal

Lemma A.4 (Changes in γ). For all i

i (xnew, x, snew)
γt

[m], let

∈

(1

α

·

−

≤

ct
i(x, s))γt

i (x, x, s) + ǫ(γ)

i

.

then ǫ(γ)

i ≤

10α

(αct

i(x, s) + αi). Furthermore, we have

| ≤
Proof. For the ﬁrst claim, Lemma A.2, the deﬁnition of γ (7), h (8) and c (9) shows that

−

·

γt
i (xnew, x, snew)
|

γt
i (x, x, s)

3α.

i (xnew, x, snew) =
γt
=

i(x, s) + hi + ǫ(µ)
µt
i k
k
i(x, s))µt
ct
i(x, s) + ǫik
(1
∗xi
k

∗xi

α

·

where ǫi = α

ct
i(x, s)(µt
i(x, s)
From the deﬁnition of ct

·

−
i(x, s)) + ǫ(µ)
µt
i, we have that ct

−

.

i

i ≤

1

96√α ≤

1
α and hence 0

1

α

·

−

≤

ct
i(x, s)

1.

≤

Therefore, we have

Now, we bound

ǫik∗xi:
k

i (xnew, x, snew)
γt

(1

≤

α

·

−

i(x, s))γt
ct

i (x, x, s) +

ǫik
k

∗xi.

ǫik
k

∗xi ≤
≤

i(x, s)

αct
4α2ct

· k
i(x, s) + 10α

µt

i(x, s)

·

µt

∗xi +
i(x, s)
k

ǫ(µ)
i k
k

∗xi

−
αi

where we used Lemma A.3 and Lemma A.2 at the end.

For the second claim, we have

i (xnew, x, snew)
γt

γt
i (x, x, s)

−
hik∗xi ≤
k

h
2
k∗x ≤
k

hi + ǫ(µ)
i k

≤ k

∗xi ≤
(cid:12)
2α. From Lemma A.1 and that α
(cid:12)

·

2α + 10α

αi

where we used (16) and that
20α2
10α

(cid:12)
(cid:12)
α.

αi ≤

·

≤

20

(15)

(16)

1

10000 , we have

≤

A.3 Movement from (x, x, s) to (xnew, x, snew)

ct
In the previous section, we see that γi will be expected to decrease by a factor of α
i up to some
small perturbations. We show that our potential Φt will therefore decrease signiﬁcantly.

·

Lemma A.5 (Movement along the ﬁrst and third parameters). Assume that γt
i. We have

i (x, x, s)

1 for all

≤

Φt(xnew, x, snew)

Φt(x, x, s)

≤

αλ
5  

−

m

Xi=1

exp(2λγt

i (x, s))

!

1/2

+ √mλ

·

exp(192λ√α).

Note that γ is a function that has three inputs. We use γ(x, s) to denote γ(x, x, s) for simplicity.

Proof. Let Φnew = Φt(xnew, x, snew), Φ = Φt(x, x, s),

γ(u) = uγt

i (xnew, x, snew) + (1

u)γt

i (x, x, s).

−

Then, we have that

Φnew

Φ =

−

m

(eλγ(1)

i

Xi=1

eλγ(0)

i ) = λ

−

m

Xi=1

eλγ(ζ)

i

(γ(1)

i −

γ(0)
i

)

for some 0

ζ

≤

≤

1. Let vi = γ(1)

i −

γ(0)
i

. Lemma A.4 shows that

vi ≤ −

α

·

ct
i(x, s)

·

i (x, x, s) + ǫ(γ)
γt

i =

α

−

·

ct
i(x, s)

·

i + ǫ(γ)
γ(0)

i

and hence

Φ

Φnew
λ

−

α

≤ −

m

Xi=1

ct
i(x, s)

·

γ(0)
i

exp(λγ(ζ)

i

) +

m

ǫ(γ)
i

exp(λγ(ζ)

i

).

(17)

Xi=1
and γt

i (x, s) . Lemma A.4 shows that

To bound the ﬁrst term in (17), we ﬁrst relate γ(0)

i

, γ(ζ)
i

Finally, we have

γt
i (x, s)
(cid:12)
(cid:12)
(cid:12)

γ(0)
i

−

(cid:12)
(cid:12)
(cid:12)

γ(0)
i −
|

γ(ζ)
i

γ(0)
i −

| ≤ |

γ(1)
i

| ≤

3α.

(18)

≤

=

+

−

γt
γt
i (x, x, s)
i (x, x, s)
γt
γt
γt
(cid:12)
(cid:12)
i (x, x, s)
i (x, x, s)
i (x, x, s)
(cid:12)
(cid:12)
−
−
µt
µt
µt
i(x, s)
i(x, s)
i(x, s)
∗xi +
∗xi − k
(cid:12)
(cid:12)
(cid:12)
k
−
≤ k
k
k
(cid:12)
(cid:12)
(cid:12)
µt
µt
µt
i(x, s)
i(x, s)
2
i(x, s)
∗xi +
∗xi − k
(cid:12)
k
k
k
k
(cid:12)
µt
µt
µt
i(x, s)
∗xi + 2α
i(x, s)
i(x, s)
2
∗xi
(cid:12)
k
k
k
k
(cid:12)
8α + 2α = 10α

−

−

≤

≤

≤

γt
i (x, x, s)
µt
i(x, s)
∗xi
(cid:12)
k
(cid:12)
µt
i(x, s)
∗xi
(cid:12)
k
(cid:12)

(cid:12)
(cid:12)

(19)

where the ﬁrst step follows from deﬁnition, the second and third step follows from triangle inequality,
µt
µt
k∗xi, the ﬁfth step follows
i(x, s)
i(x, s)
the fourth step follows from
k∗xi ≤
k
µt
1
from self-concordance, the sixth step follows from Lemma A.3 and that
i(x, s)
≤
k
for all i

µt
i(x, s)
2
k

k∗xi = γt

i (x, x, s)

µt
i(x, s)

−

−

21

γ(0)
i

exp(λγ(ζ)

i

)

γ(0)
i

exp(λγt

i (x, s)

γ(0)
i

exp(λγt

i (x, s)

−

−

·

·

·

λγt

i (x, s) + λγ(0)

i −

λγ(0)

i + λγ(ζ)

i

)

13λα)

Using (18) and (19), we have

m

Xi=1
m

Xi=1
m

ct
i(x, s)

ct
i(x, s)

ct
i(x, s)

=

≥

≥

≥

Xi=1
m
1
2

Xi=1
m

1
2

ct
i(x, s)

·

γ(0)
i

exp(λγt

i (x, s))

ct
i(x, s)

i (x, s) exp(λγt
γt

i (x, s))

·

3α

−

m

Xi=1

ct
i(x, s) exp(λγt

i (x, s)).

(20)

13λα)

≥

1/2, and the last step follows from (18).

−

Xi=1
where the third step follows from exp(
For the ﬁrst term in (20), we have

≥  
Xi=1
m
i=1 exp(2λγt

i (x, s))

So, if

P
m

Xi=1

ct
i(x, s)

·

P

m

Xi=1

m

Xi=1

(

Xi=1
m

=

=

ct
i(x, s)

·

i (x, s) exp(λγt
γt

i (x, s))

γt
i (x, s))
exp(2λ
m
i=1 exp(2λγt
i (x, s)))1/2

·

(

96√α

Xγt
i (x,s)
≥
m

P
γt
i (x, s))
exp(2λ
m
i=1 exp(2λγt
i (x, s)))1/2 −

·

P
exp(2λγt

i (x, s))

1/2

−

(

!

γt
i (x, s))
exp(2λ
m
i=1 exp(2λγt
i (x, s)))1/2

·

(

Xγt
i (x,s)<96√α
m
exp(192λ
m
i=1 exp(2λγt

·

P
√α)
·
i (x, s)))1/2

.

√m

·

−

exp(192λ√α).

m

·

≥

exp(192λ

·

P
√α), we have

m

1/2

i (x, s) exp(λγt
γt

i (x, s))

exp(2λγt

i (x, s))

!

≥  

Xi=1
exp(192λ

i(x, s) exp(λγt
ct

i (x, s)) =

≤

≤

=

i (x, s))/γt
γt
exp(λ
m
i=1 exp(2λγt
(

i (x, s)
i (x, s)))1/2

·

exp(λγt

i (x, s))

P

γt
i (x, s))
exp(2λ
m
i=1 exp(2λγt
i (x, s)))1/2

·

(

96√α

Xγt
i (x,s)
≥
m

96√α

Xγt
i (x,s)
≥
1
96√α

1
96√α

1
96√α  

(

Xi=1
m

Xi=1

22

P
γt
i (x, s))
exp(2λ
m
i=1 exp(2λγt
i (x, s)))1/2
1/2

·

P
exp(2λγt

i (x, s))

.

!

Note that if
i (x, s))
lower bounded by 0. For the second term in (20), we have

m
i=1 exp(2λγt

m

≤

·

·

√α), this is still true because left hand side is

where the second step follows
summation is non-negative.

1

γt
i (x,s) ≤

1

96√α , and the third step follows from each term in the

Combining the bounds for both ﬁrst and second term in (20), we have

m

Xi=1

ct
i(x, s)

·

γ(0)
i

exp(λγ(ζ)

i

)

≥

1
2 

m

1/2

exp(2λγt

i (x, s))

!

√m

·

−

1/2

exp(192λ√α)







Xi=1
3α
96√α  

−

m

Xi=1

exp(2λγt

i (x, s))

!

2
5  

≥

m

Xi=1

where the last step follows from 1

3α
96√α ≥
For the second term in (17), we note that

2 −

m

ǫ(γ)
i

exp(λγ(ζ)

i

)

exp(2λγt

i (x, s))

!

2
5 .

96 = 45
96 ≥
γt
i (x, s)

3

1
2 −
γ(ζ)
i −
|
m

Now, we use ǫ(γ)

i ≤

10α

·

Xi=1
(αct
i(x, s) + αi) (Lemma A.4) to get

Xi=1

2

≤

ǫ(γ)
i

exp(λγt

i (x, s)).

1/2

√m

·

−

exp(192λ√α).

(21)

13α

≤

1
2λ by (18) and (19). Hence,

| ≤

m

Xi=1

ǫ(γ)
i

exp(λγ(ζ)

i

)

≤

≤

m

20α

(αct

i(x, s) + αi)

Xi=1
m

exp(λγt

i (x, s))

·

1/2

m

20α

(αct

i(x, s) + αi)2

Xi=1

!

exp(2λγt

i (x, s))

!

Xi=1

where the last step follows from Cauchy-Schwarz inequality.

Note that by using Cauchy-Schwarz,

m

(αct

i(x, s) + αi)2

Xi=1

1/2

m

1/2

m

1/2

!

α

≤

α

·

≤

ct
i(x, s)2

Xi=1
1
96√α

+ 2α

+

√α
90

.

!

≤

α2
i

!

Xi=1

where we used the deﬁnition of ct

i, Lemma A.1 and α

1
224 . Together, we conclude

≤

ǫ(γ)
i

exp(λγ(ζ)

i

)

1
5

α

≤

m

Xi=1

m

Xi=1

Combining (21) and (22) to (17) gives

exp(2λγt

i (x, s))

!

1/2

.

Φ

Φnew
λ

−

2
5

1
5

α

α

≤ −

=

−

m

Xi=1
m

exp(2λγt

i (x, s))

!

exp(2λγt

i (x, s))

!

Xi=1

1/2

1/2

+ √m

+ √m

·

·

exp(192λ√α) +

exp(192λ√α).

1
5

α

m

Xi=1

where the last step follows from merging the ﬁrst term with the third term.

23

1/2

.

(22)

1/2

exp(2λγt

i (x, s))

!

 
 
 
 
 
 
 
 
 
 
A.4 Movement from (xnew, x, snew) to (xnew, xnew, snew)

Next, we must analyze the potential change when we change the second term.

Lemma A.6 (Movement along the second parameter). Assume that
have

γt(x, x, s)
k

k∞ ≤

1. Then we

Φt(xnew, xnew, snew)

γt(x, x, s)
Φt(xnew, x, snew) + 12α(
k

k∞

≤

+ 3α)λ

Proof. We can upper bound Φt(xnew, xnew, snew) as follows

m

Xi=1

exp(2λγt

i (x, x, s))

!

1/2

.

Φt(xnew, xnew, snew) =

m

Xi=1
m

exp(λγt

i (xnew, xnew, snew))

exp(λγt

i (xnew, x, snew)(1 + 2αi)).

≤

Xi=1

where the second step follows from γt
concordance (Theorem 2.3) and

xnew
i −
k
Now, by Lemma A.4, we note that γt
Hence, by a simple taylor expansion, we have

i (xnew, xnew, snew)
γt
i (xnew, x, snew)
≤
xnew
2αi.
xikxi ≤
2
xikxi ≤
i −
k
γt
i (xnew, x, snew)
i (x, x, s)+3α
≤

≤

(1 + 2αi) by self-

·

1+3α and that α

1
100λ .

≤

Φt(xnew, xnew, snew)
m

exp(λγt

i (xnew, x, snew)) + 3

≤

Xi=1

m

Xi=1

Finally, we bound the last term by

αi exp(λγt

i (xnew, x, snew))γt

i (xnew, x, snew).

exp(λγt

i (xnew, x, snew))γt

i (xnew, x, snew)αi

m

Xi=1
m

≤

≤

exp(λγt

i (x, x, s) + 3λα)(γt

i (x, x, s) + 3α)αi

Xi=1
γt(x, x, s)
2(
k

k∞

m

+ 3α)

exp(λγt

i (x, x, s))αi

Xi=1
m

γt(x, x, s)
2(
k

k∞

≤

+ 3α)

exp(2λγt

i (x, x, s))

γt(x, x, s)
4α(
k

k∞

≤

+ 3α)

exp(2λγt

i (x, x, s))

Xi=1
m

Xi=1
i (xnew, x, snew)

!

1/2

m

1/2

α2
i

!

Xi=1

1/2

,

!

exp(λγt
i (x, x, s)+3λα), the second step follows
2, the third step follows from Cauchy-Schwarz inequality, the last step follows from

≤

where the ﬁrst step follows from λγt
exp(3λα)
m
i=1 α2

≤
4α2.
i ≤

P

24

 
 
 
 
A.5 Movement of t

Lastly, we analyze the eﬀect of setting t

tnew.

→

Lemma A.7 (Movement in t). For any x, s such that γt
where ν =

m
i=1 νi, we have

i (x, s)

≤

1 for all i, let tnew =

1

(cid:16)

−

κ
√ν

t

(cid:17)

P

Φtnew

(x, s)

≤

Φt(x, s) + 10κλ

m

Xi=1

exp(2λγt

i (x, s))

!

1/2

.

Proof. Note that

γtnew
i

(x, s) =

=

≤

≤

≤

+

∇

t(1

∗
xi

φi(xi)

(cid:13)
(cid:13)
(cid:13)
∇

s
tnew +
(cid:13)
s
(cid:13)
(cid:13)
κ/√ν)
(cid:13)
−
(cid:13)
(1 + 2κ/√ν)γt
(cid:13)
(cid:13)
(1 + 2κ/√ν)γt
γt
i (x, s) + 5κ√νi/√ν

∗

φi(xi)

xi

(cid:13)
(cid:13)
(κ/√ν)
i (x, s) + 2
(cid:13)
k
∇
(cid:13)
i (x, s) + 3κ√νi/√ν

φi(xi)
∗xi
k

where the ﬁrst step follows from deﬁnition, the second step follows from tnew = t(1
κ/√ν), the
second last step follows from the fact that our barriers are νi-self-concordant and the last step used
γt
1, we have by simple taylor expansion,
i (x, s)

1. Using that 5κ

1
10λ and γt

i (x, s)

−

1 and νi ≥

≤

≤

≤

Φtnew

(x, s)

≤

=

≤

=

m

Xi=1
m

Xi=1
m

Xi=1
m

Xi=1

m

exp(λγt

i (x, s)) + 2λ

exp(λγt

i (x, s))

5κ

νi/ν

Xi=1
i (x, s)) + 10κλ

exp(λγt

m

(cid:16)

p

exp(λγt

i (x, s))

νi/ν

exp(λγt

i (x, s)) + 10κλ

exp(λγt

i (x, s)) + 10κλ

Xi=1
m

Xi=1
m

Xi=1

(cid:16)p

1/2

exp(2λγt

i (x, s))

!

exp(2λγt

i (x, s))

!

1/2

,

(cid:17)

(cid:17)

1/2

m

Xi=1

νi
ν !

where the third step follows from Cauchy-Schwarz, and the last step follows from

A.6 Potential Maintenance

m
i=1 νi = ν.

P

Putting it all together, we can show that our potential Φt can be maintained to be small throughout
our algorithm.

Lemma A.8 (Potential Maintenance). If Φt(x, s)

80 m

α , then

≤

Φtnew

(xnew, snew)

1

−

≤

(cid:18)

Φt(x, s) + √mλ

exp(192λ√α).

·

In particularly, we have Φtnew

(xnew, snew)

≤

αλ
40√m

(cid:19)
80 m
α .

25

 
 
 
 
Proof. Let

ζ(x, s) =

By combining our previous lemmas,

m

Xi=1

exp(2λγt

i (x, s))

!

1/2

.

Φtnew
(xnew, snew)
Φt(xnew, snew) + 10κλ
ζ(xnew, snew)
γt(x, s)
Φt(xnew, x, snew) + 12αλ(
k

k∞

·

≤

≤

≤

Φt(x, x, s)

αλ
5
−
γt(x, s)
+ 12αλ(
k

ζ(x, s) + √mλ

exp(192λ√α)

·

+ 3α)

·

ζ(x, s) + 10κλ

ζ(xnew, snew)

·

+ 3α)

·

ζ(x, s) + 10κλ

k∞

ζ(xnew, snew)

·

(23)

where the ﬁrst step follows from Lemma A.7, the second step follows from Lemma A.6, and the last
step follows from Lemma A.5. We note that in all lemma above, we used that fact that
1
(for diﬀerent combination of x, x, xnew, s, s, snew) which we will show later.

γt
k

k∞ ≤

We can upper bound γt

i (xnew, snew) in the following sense,

i (xnew, snew)
γt
where the ﬁrst step follows from self-concordance and γi ≤
µt
i(x, s)
k

Hence, since ζ changes multiplicatively when γ changes additively, ζ(xnew, snew)
µt
i(x, s)
Lemma A.3 shows that

i (xnew, x, snew) + 2α
γt

4α and hence

k∗xi ≤

≤

−

γt
i (x, x, s) + 5α.

≤

(24)

1, the second step follows from Lemma A.4.

1/2

exp(2λγt

i (x, x, s))

!

exp(2λγt

i (x, x, s)

1/2

8αλ)

!

−

m

Xi=1
m

ζ(x, s)

≥

≥

≥

2
3  

2
3  
1
2

Xi=1
ζ(x, s).

2ζ(x, s).

≤

(25)

Combining (24) and (25) into (23) gives

Φtnew

(xnew, snew)

Φt(x, s) +

Φt(x, s) +

≥

≥

(cid:18)

(cid:18)

γt(x, s)
12αλ(
k

k∞

+ 3α) + 20κλ

γt(x, s)
12αλ
k

k∞ −

αλ
20

·

(cid:19)

αλ
10

−

·

(cid:19)
ζ(x, s) + √mλ

where the last step follows from κ
Finally, we need to bound

α
1000 and α

1
10000 .

≤

. The bound for other

≤
γt(x, s)
k

k∞

of x, x, xnew, s, s, snew, are similar. We note that

ζ(x, s) + √mλ

exp(192λ√α)

·

exp(192λ√α)

·

γt
k

k∞

, i.e. for diﬀerent combination

implies that
and hence

γt(x, s)
k

k∞ ≤

log(80 m
α )
λ

Φt(x, s)

80

m
α

≤

. Hence, by our choice of λ and α, we have that λ

480 log(80 m
α )

≥

γt(x, s)
12αλ
k

k∞ ≤

αλ
40

.

26

 
Name
Initialize
Update
FullUpdate
PartialUpdate
Query
MultiplyMove
Multiply
Move

Algorithm Input
Statement
Type
Alg. 1
Lemma B.4
public
Alg. 2
public
Lemma B.5
Alg. 3
private Lemma B.7
Alg. 2
private Lemma B.6
Lemma B.8
public
Alg. 1
public
Lemma B.11 Alg. 4
private Lemma B.10 Alg. 4
Alg. 4
private Lemma B.9

∅
h, t
h, t

∅

A, x, s, W , ǫmp, a, b
W
W
W

Output

∅
∅
∅
∅
x, s

∅
∅
∅

Table 2: Summary of data structure CentralPathMaintenance

Finally, using Φt(x, s)

√m

·

≤

ζ(x, s), we have

Φtnew

(xnew, snew)

Since λ

≤

1

400√α , we have Φt(x, s)

≥

≥

≤

Φt(x, s)

αλ
40

−
αλ
40√m

1

(cid:18)
80 m

−
α implies Φtnew

(cid:19)

ζ(x, s) + √mλ

·
Φt(x, s) + √mλ

exp(192λ√α)

exp(192λ√α).

·

(xnew, snew)

80 m
α .

≤

B Central Path Maintenance

1/2)
The goal of this section is to present a data-structure to perform our centering steps in
amortized time and prove a theoretical guarantee of it. The original idea of inverse maintenance is
from Michael B. Cohen [Lee17], then [CLS19] used it to get faster running time for solving Linear
Programs. Because a simple matrix vector product would require O(n2) time, our speedup comes
guarantees, which is unlike the sparse vector approach
via a low-rank embedding that provides ℓ
of [CLS19]. In fact, we are unsure if moving in a sparse direction h can have suﬃciently controlled
noise to show convergence. Here, we give a stochastic version that is faster for dense direction h.

O(nω

∞

e

−

n with n
Theorem B.1 (Central path maintenance). Given a full rank matrix A
d, a
m
i=1 ni. Given any positive
tolerance parameter 0 < ǫmp < 1/4 and a block diagonal structure n =
number a such a
α where α is the dual exponent of matrix multiplication. Given any linear sketch
of size b, there is a randomized data structure CentralPathMaintenance (in Algorithm 1, 2,
4) that approximately maintains the projection matrices

P

≥

≤

∈

×

Rd

ni; exactly implicitly maintains central path pa-
for positive block diagonal psd matrix W
rameters (x, s) and approximately explicitly maintains path parameters through the following ﬁve
operations:

1A√W

√W A⊤(AW A⊤)−
⊕i Rni×

(0)

,

· · ·

) : Assume W

(0)

∈ ⊗iRni×

ni. Initialize all the parameters in O(nω)

1. Initialize(W

time.

2. Update(W ) : Assume W

ni. Output a block diagonal matrix

∈ ⊕iRni×
ǫmp)
(1

−

vi (cid:22)

wi (cid:22)

(1 + ǫmp)

vi.

e

27

e

⊕i Rni×

ni such that

V

e

3. Query() : Output (x, s) such that

k eV −1 ≤
used in MultiplyMove, where ǫmp = α log2(nT ) n1/4
√b
This step takes O(n) time.

x
k

−

x

ǫmp and
and the success probability is 1

tǫmp where t is the last t
1/ poly(nT ).

k eV ≤

s
k

−

s

−

4. MultiplyMove(h, t) : It outputs nothing. It implicitly maintains:

x = x +

V 1/2(I

−

P )

V 1/2h, s = s + t

V −

1/2

P

V 1/2h.

P =

V 1/2A⊤(A
where
each call takes O(nb + naω+o(1) + na

V A⊤)−

1A

V 1/2. It also explicitly maintains x, s. Assuming t is decreasing,
e

e

e

e

e

e

(0)

e
Let W

e
, W
be the initial matrix and W
assumption that there is a sequence of matrix W (0),

· · ·

e

e

,

be the (random) update sequence. Under the
, W (T )

ni satisﬁes for all k

Rni×

m
i=1

k0 + n1.5) amortized time.
h
k

(T )

(1)

m

(w(k)
i

)−

1/2(E[w(k+1)

]

(cid:13)
(cid:13)
(cid:13)

i

w−
i

· · ·
1/2
(wi −
w(k)
i

−

∈ ⊕
wi)w−
i

1/2

)(w(k)
i

1/2

)−

)−

1/2(w(k+1)
i

w(k)
i

)(w(k)
i

)−

−

m

Xi=1 (cid:13)
(cid:13)
(cid:13)
(w(k)
E
i

Xi=1 (cid:18)

(cid:20)(cid:13)
(cid:13)
(cid:13)

ǫmp,

C 2
1 ,

F ≤
(cid:13)
2
(cid:13)
(cid:13)

F ≤

(cid:13)
(cid:13)
(cid:13)

2

C 2
2 ,

≤

1
4

.

F ≤

(cid:13)
(cid:13)
(cid:13)

2

1/2

(cid:21)(cid:19)

F
(cid:13)
(cid:13)
(cid:13)
1/2
)−

(w(k)
i
(cid:13)
(cid:13)
(cid:13)
Then, the amortized expected time per call of Update(w) is

is the i-th block of W (k),

1/2(w(k+1)
i

)(w(k)
i

w(k)
i

[m].

)−

−

∈

∀

i

i

where w(k)

(C1/ǫmp + C2/ǫ2

mp)

·

(nω

1/2+o(1) + n2
−

a/2+o(1)).

−

Remark B.2. For our algorithm, we have C1 = O(1/ log2 n), C2 = O(1/ log4 n) and ǫmp =
O(1/ log2 n). Note that the input of Update W can move a lot. It is working as long as W is
close to some W that is slowly moving. In our application, our W satisﬁes C1, C2 deterministically.
We keep it for possible future applications.

B.1 Proof of Theorem B.1

We follow the proof-sketch as [CLS19]. The proof contains four parts : 1) Deﬁnition of X and Y ,
2) We need to assume sorting, 3) We provide the deﬁnition of potential function, 4) We write the
potential function.

Deﬁnition of matrices X and Y . Let us consider the k-th round of the algorithm. For all
i

ni is constructed based on procedure Update (Algorithm 2) :

[m], matrix y(k)

Rni×

∈

i ∈

and π is a permutation such that

For the purpose of analysis : for all i

y(k)
i =

−

I.

w(k+1)
i
v(k)
i
y(k)
π(i+1)kF .
[m], we deﬁne x(k)

i

∈

y(k)
π(i)kF ≥ k
k

, x(k)
i

and y(k)

i ∈

Rni×

ni as follows:

x(k)
i =

w(k)
i
v(k)
i

I,

−

y(k)
i =

w(k+1)
i
v(k)
i

I,

−

x(k+1)
i

=

w(k+1)
i
v(k+1)
i

I,

−

28

Algorithm 1 Central Path Maintenance Data Structure - Initial, Query, Move

⊲ Theorem B.1

n

∈

×

5:

9:

ni

[m]

[m]

10:

Rni×

∈ ⊗i
V

∈
∈ ⊗i
Rd
n
×
Rn
(0, 1/4)

1: datastructure CentralPathMaintenance
2:
3: private : members
ni
4: W
Rni×
V,
A
6:
7: M
8:

∈
e
∈
ǫmp ∈
a
(0, α]
∈
Z+
b
∈
Rn1+o(1)
R
∈
Rb
n
Q
×
∈
Rn, F
u1 ∈
Rn, G
u3 ∈
Rn
x, s
∈
Z+
l
∈
tpre
17:
18: end members
19:
20: public : procedure Initialize(A, x, s, W, ǫmp, a, b)
21:

n, u2 ∈
n, u4 ∈

Rn
Rn

Rn
Rn

11:
12:

R+

∈
∈

15:

14:

13:

16:

∈

×

×

×

n

⊲ Target vector, W is ǫw-close to W
⊲ Approximate vector
⊲ Constraints matrix
⊲ Approximate Projection Matrix
⊲ Tolerance
⊲ Batch Size for Update (na)
⊲ Sketch size of one sketching matrix
⊲ A list of sketching matrices
⊲ Sketched matrices
u2
⊲ Implicit representation of x, x = u1 + F
u4
⊲ Implicit representation of s, s = u3 + G
⊲ Central path parameters, maintain explicitly
n

·
·

⊲ Randomness counter, Rl ∈

⊲ Tracking the changes of t

×

Rb

⊲ Lemma B.4
⊲ parameters will never change after initialization

22:

A

23:
24: W
25:

A, a

a, b

←

←

←

ǫmp

b, ǫmp ←
V
V

×

29:

26:

←

←

←

· · ·

W ,

0
p

←
Rb

V M

0, u3 ←

27: M
28:

R
←
s, u4 ←

W , V
n to be sketching matrix,
Choose Rl ∈
e
[R⊤1 , R⊤2 ,
]⊤
R
1A, Q
A⊤(AV A⊤)−
←
u1 ←
x, u2 ←
s
x, s
x
←
←
1
l
30:
←
31: end procedure
32:
33: public : procedure Query()
return (x, s)
34:
35: end procedure
36:
37: end datastructure

e

⊲ parameters will still change after initialization

[√n]

l

∀

∈

⊲ Lemma E.5
⊲ Batch them into one matrix R

⊲ Initialize projection matrices
⊲ Initialize x and s

⊲ Lemma B.8

where w(k)
i
v(k)
i

denotes (v(k)

i

)−

1/2w(k)
i

(v(k)
i

)−

1/2.

It is not hard to observe the diﬀerence between x(k)
and x(k+1)
i

move”. Similarly, the diﬀerence between y(k)

i

i

and y(k)
is that w is changing. We call it “w
is that v is changing. We call it “v move”.

i

For each i, we deﬁne βi as follows

βi =

(w(k)
i
k

)−

1/2(E[w(k+1)

i

]

−

w(k)
i

)(w(k)
i

)−

1/2

kF ,

29

Algorithm 2 Central Path Maintenance Data Structure - Update and PartialUpdate

1: datastructure CentralPathMaintenance
2:
3: public : procedure Update(W
wnew
4:
i
5:

v−
i
−
the number of indices i such that

)
[m]

v−
i

new

1/2

1/2

1,

∈

∀

i

yi ←
r
←
if r < na then

yikF ≥
k

PartialUpdate(W

new

)

⊲ Theorem B.1

⊲ Lemma B.5, W

new

is close to W new

ǫmp

else

FullUpdate(W

new

)

⊲ Algorithm 3

6:

7:

8:

9:

new

)

⊲ Lemma B.6

end if

10:
11: procedure
12:
13: private : procedure PartialUpdate(W
14: W

new

W

15:

16:

17:
18:

19:

20:

21:

wi (cid:22)

ǫmp)vi (cid:22)

←
vi
vnew
i ← (
wi
F new
F + ((
e
Gnew
G + ((
u1 ←
F
←
Let
x
and s

if (1
−
otherwise
V new)1/2
V )1/2)M
(
←
−
V new)−
1/2
V )−
(
←
F new)u2, u3 ←
e
e
u1 + (F
−
Gnew
F new, G
e
e
←
S denote the blocks where
bS + (F u2)

(u3)
e

bS ←
b

bS , s

bS ←

(u1)

−

(1 + ǫmp)vi

1/2)M
u3 + (G

⊲ only takes n1+a time, instead of n2

Gnew)u4

−

V new are diﬀerent

V and
bS + (Gu2)
bS
e

⊲ make sure x and x are close, similarly for s

22: end procedure
23:
24: end datastructure

then one of assumption becomes

m

Xi=1

β2
i ≤

C 2
1 .

Assume sorting for diagonal blocks. Without loss of generality, we can assume the diagonal
blocks of matrix x(k)
is a
scalar. They sorted the sequence based on absolute value. In our situation, x(k)
i
x(k+1)
sort the sequence based on Frobenius norm. Let τ permutation such that
τ (i) kF ≥ k
k

x(k)
i+1kF . In [CLS19], x(k)
is a matrix. We
x(k+1)
τ (i+1)kF .

ni are sorted such that

x(k)
i kF ≥ k
k

Rni×

∈ ⊕

m
i=1

i

Let π denote the permutation such that

y(k)
π(i)kF ≥ k
k

y(k)
π(i+1)kF .

Deﬁnition of Potential function. We deﬁne three functions g, ψ and Φk here. The deﬁnition
of ψ is diﬀerent from [CLS19], since we need to handle matrix. The deﬁnitions of g and Φk are the
same as [CLS19].

30

Algorithm 3 Central Path Maintenance Data Structure - Full Update

1: datastructure CentralPathMaintenance
2:
3: private : procedure FullUpdate(W
4:
5:

v−
i
−
the number of indices i such that

wnew
i

[m]

v−
i

new

1/2

1/2

1,

∈

∀

)

i

yi ←
r
←
Let π : [m]
while 1.5

r

·
1.5
min(
⌈
←
end while
wnew
π(i)
vπ(i)

vnew
π(i) ← (

6:

7:

8:

9:

10:

11:

i

i

∈ {

∈ {

1, 2,

, r

· · ·

r + 1,

}
, m

· · ·

}

[m] be a sorting permutation such that

→
r < m and

yπ(1.5r)kF ≥
k
, m)
⌉

r

·

(1

ǫmp

yikF ≥
k
yπ(i)kF ≥ k
k
yπ(r)kF
1/ log m)
k

−

yπ(i+1)kF

⊲ Theorem B.1

⊲ Lemma B.7

⊲ Compute M new = A⊤(AV newA⊤)−

1A via Matrix Woodbury
n and
k0 = r
⊲ ∆

∆
k

Rn

∈

×

π([r]) be the ﬁrst r indices in the permutation
O(r) be the r column-blocks from S of M

O(r) be the r row-blocks and column-blocks from S of M , ∆
×
1
S,S + MS,S)−
(∆−
M new) + R
√V
·
M new, Q

⊲ Update M
⊲ Update Q
⊲ Update in memory

,S)⊤
(M
·
∗
(M new

−
Qnew

M )

1

·

←
wi (cid:22)

←

(1 + ǫmp)vi

×

V

−

13:

12:

14:

15:

Rn

√V

RO(r)

V new
−
√V new

∆
←
Γ
←
Let S
←
Let M
,S ∈
∗
Let MS,S, ∆S,S ∈
16:
17: M new
M
M
,S ·
←
−
∗
Qnew
Q + R
(Γ
18:
·
←
·
new
V new, M
, V
W
19: W
←
←
ǫmp)vi (cid:22)
if (1
vi
vi ← (
−
otherwise
wi
V M , Gnew
F new
e
u1 ←
F
←
Let
x
and s
tpre

M
←
←
F new)u2, u3 ←
p
u1 + (F
e
Gnew
F new, G
S denote the blocks where

−
←
bS + (F u2)

bS ←
b

1
√
eV

bS , s

bS ←

(u1)

21:

20:

22:

25:

24:

23:

t

←

26:
27: end procedure
28:
29: end datastructure

u3 + (G

−

Gnew)u4

V new are diﬀerent

V and
bS + (Gu2)
bS
e

(u3)
e

⊲ make sure x and x are close, similarly for s

For the completeness, we still provide a deﬁnition of g. Let g be deﬁned as

gi =

a,

n−
ω−2
1−a n−

i
(

if i < na;

otherwise.

a(ω−2)
1−a ,

In [CLS19], the input of function ψ : R

: square matrix

→

R be deﬁned by

R has to be a number. We allow matrix here. Let ψ

→

2
x
F
k
k
2ǫmp

,
ǫmp −
ǫmp,

ψ(x) = 



(4ǫ2

x
mp−k
18ǫ3

F )2
2
k
mp

,

x
k
x
k
x
k

kF ∈
kF ∈
kF ∈

[0, ǫmp];

(ǫmp, 2ǫmp];

(2ǫmp, +

).

∞

(26)

31

Algorithm 4 Central Path Maintenance Data Structure - Multiply and Move

1: datastructure CentralPathMaintenance
2:
3: public : procedure MultiplyAndMove(h, t)
4: Multiply(h, t)
5: Move()
6: end procedure
7:
8: private : procedure Multiply (h, t)
9:

S be the indices i such that (1

ǫmp)vi (cid:22)

−

⊲ Theorem B.1

⊲ Lemma B.11

wi (cid:22)

(1 + ǫmp)vi is false.

⊲ Lemma B.10

+ M

eS, eS)−

1

·

((M

eS,

∗

)⊤

V h))

⊲

·

e

Γ

p

M )

(R⊤l
(cid:16)
1

((Ql + Rl ·

R⊤R
−
δm))
)
S,
e
e
∗
1/2R⊤R
(cid:17)
P
V −
δm))
)
M
·
e
e
e
⊲ Increasing the randomness counter, and using the new randomness next time
(cid:17)
V 1/2h
V 1/2(I
P )

V 1/2(I
M
Γ
·
e
δs = t
e
Γ
((Ql, eS + Rl ·
e

⊲ Compute
δx =
S + Rl ·
((Ql,
·
e
e
⊲ Compute

e
⊲ Implicitly maintain x = x +

e
((Q + Rl ·

(R⊤l
(cid:16)

e
V 1/2h

(R⊤l

(R⊤l

M )

h))

h))

p

p

eS,

−

−

Γ

V

V

e

e

e

∗

·

·

·

·

·

·

·

·

·

·

·

na

| ≤
V 1/2h

S
|
P )
e

−

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

28:

29:

30:
31:

V

Let
∆
V
←
−
e
√V
Γ
V
←
−
e
e
1
∆−
((
δm ←
p
S
S,
e
e
e
e
e
V h

−

δx ←
e
δs ←
l
←
e
u1 ←
u2 ←

V −

e
t
·
l + 1
e
u1 +
u2 −

V h

V h + 1

eSδm

e
p
u3 + 0
t
u4 −

22:

e
V h + t1

u3 ←
u4 ←
23:
24: end procedure
p
25:
26: private : procedure Move()
27:

if l > √n or t

tpre/2

eSδm

e

≥

u1 + F u2, s

x
Initialize(A, x, s, W , ǫmp, a, b)

u3 + F u4

←

←

else
x
←
end if

x +

δx, s

32:
33: return (x, s)
e
34: end procedure
35:
36: end datastructure

←

s +

δs

e

⊲ Implicitly maintain s = s + t

V −

e

e
1/2

P

e
V 1/2h

e

e

e

⊲ Lemma B.9
⊲ Variance is large enough

⊲ Algorithm 1

⊲ Update x, s

x
where
k
L2 = maxx D2

H
kF denotes the Frobenius norm of square matrix x, and let L1 = maxx Dxψ[h]/
k

kF ,

2
F where h is the vectorization of matrix H.
k
For the completeness, we deﬁne the potential at the k-th round by

H
xψ[h, h]/
k

Φk =

m

Xi=1

ψ(x(k)

τk(i))

gi ·

32

where τk(i) is the permutation such that
should be

.)

| · |

x(k)
τk(i)kF ≥ k
k

x(k)
τk(i+1)kF . (Note that in [CLS19]

k · kF

Rewriting the potential, and bounding it. Following the ideas in [CLS19], we can rewrite
Φk into two terms: the ﬁrst term is w move, and the second term is v move. For the
Φk+1 −
completeness, we still provide a proof.

Φk+1 −

Φk =

=

m

Xi=1
m

Xi=1

gi ·

gi ·

ψ(x(k+1)
τ (i)
(cid:16)
ψ(y(k)
(cid:16)

π(i))

ψ(x(k)
i

)

)

−

ψ(x(k)
i

−
W move

(cid:17)

−

m

Xi=1

)

(cid:17)

gi ·

π(i))

ψ(y(k)
(cid:16)

−
V move

ψ(x(k+1)
τ (i)

)

(cid:17)

}

Using Lemma B.12, we can bound the ﬁrst term. Using Lemma B.15, we can bound the second
term.

{z

}

{z

|

|

B.2 Initialization time, update time, query time, move time, multiply time

Remark B.3. In terms of implementing this data-structure, we only need three operations Ini-
tialize, Update, and Query. However, in order to make the proof more understoodable, we split
Update into many operations : FullUpdate, PartialUpdate, Multiply and Move. We give
a list of operations in Table 2.

Lemma B.4 (Initialization). The initialization time of data-structure CentralPathMainte-
nance (Algorithm 1) is O(nω+o(1)).

Proof. The running time is mainly dominated by two parts, the ﬁrst part is computing A⊤(AV A⊤)−
this takes O(n2dω

2) time.

−

1A,

The second part is computing R

V M . This takes O(nω+o(1)) time.

Lemma B.5 (Update time). The update time of data-structure CentralPathMaintenance
(Algorithm 2) is O(rgrn2+o(1)) where r is the number of indices we updated in V .

p

e

Proof. It is trivially follows from combining Lemma B.6 and Lemma B.7.

Lemma B.6 (Partial Update time). The partial update time of data-structure CentralPath-
Maintenance (Algorithm 2) is O(n1+a).

Proof. We ﬁrst analyze the running time of F update, the update equation of F in algorithm is

F new
F

←

←

which can be implemented as

F + ((
F new

V new)1/2

V )1/2)M
(

−

e

e

←
where we only need to change na row-blocks of F . It takes O(n1+a) time.

−

e

e

F

F + ((

V new)1/2

V )1/2)M
(

Similarly, for the update time of G.

33

Next we analyze the update time of u1, the update equation of u1 is

u1 ←

u1 + (F

−

F new)u2

Note that the diﬀerence between F and F new is only na row-blocks, thus it takes n1+a time to
update.

Finally we analyze the update time of x. Let

S denote the blocks where

V and

V new are diﬀerent.

x

S ←
b

S + (F u2)
(u1)
b
S
b
b

e

e

This also can be done in n1+a time, since

S indicates only na blocks.

Therefore, the overall running time is O(n1+a).

b

Lemma B.7 (Full Update time). The full update time of data-structure CentralPathMainte-
nance (Algorithm 3) is O(rgrn2+o(1)) where r is the number of indices we updated in V .

Proof. The update equation we use for Q is

Qnew

Q + R

(Γ

·

·

←

M new) + R

√V

·

·

(M new

M ).

−

It can be re-written as

Qnew

Q + R

(Γ

·

·

←

M new) + R

√V

·

(
−

M
,S ·
∗

·

(∆−

1
S,S + MS,S)−

1

,S)⊤)
(M
∗

·

The running time of computing second term is multiplying a n
The running time of computing third term is also dominated by multiplying a n
another r

n matrix.

r matrix with another r

×

×

n matrix.
r matrix with

×

×

Thus running time of processing Q update is the same as the processing M update.
For the running time of other parts, it is dominated by the time of updating M and Q.
Therefore, the rest of the proof is almost the same as Lemma 5.4 in [CLS19], we omitted here.

Lemma B.8 (Query time). The query time of data-structure CentralPathMaintenance (Al-
gorithm 1) is O(n) time.

Proof. This takes only O(n) time, since we stored x and s.

Lemma B.9 (Move time). The move time of data-structure CentralPathMaintenance (Algo-
1/2+o(1)) amortized cost per iteration.
rithm 4) is O(nω+o(1)) time in the worst case, and is O(nω

−

Proof. In one case, it takes only O(n) time. For the other case, the running time is dominated by
Initialize, which takes nω+o(1) by Lemma B.7.

Lemma B.10 (Multiply time). The multiply time of data-structure CentralPathMaintenance
(Algorithm 4) is O(nb + n1+a+o(1)) for dense vector
k0) for
h
k
sparse vector h.

k0 = n, and is O(nb + naω+o(1) + na
h
k

Proof. We ﬁrst analyze the running time of computing vector δm, the equation is

δm ←

(cid:16)

((

∆

S)−
e

S,
e

1 + M

S,
e

1

S)−
e

·

(M

S,
e

∗

e

34

)⊤

V h

(cid:17)

p

e

where
V .

∆ =

V

−

V . Let

r =

i

∈ eS ni = O(r) where r is the number of blocks are diﬀerent in

V and

e

e

It contains several parts:
1. Computing

e
(

V h)

P

M ⊤
eS ·
1
∆−
eS, eS
f

R

∈
S)−
e

1

r takes O(
k0.
h
r)
e
k
RO(
r) that is the inverse of a O(
O(
e
e

r)
e

r)

e

O(

r) matrix takes

×

O(

O(

×

e

∈

S,
e

+ M
p
e

2. Computing (
rω+o(1)) time.
3. Computing matrix-vector multiplication between O(
e
)⊤
r)
Thus, the running time of computing δm is
e

V h) takes O(

1 vector ((

r2) time.

p

f

M

eS,

×

∗

e
rω+o(1) +

r2) = O(
r

e
r
O(

k0 +
h
k

e

e

e

k0 +
h
k

rω+o(1)).

r)

O(

e
r) matrix ((

∆

e
S + M
e

S,
e

S)−
e

S,
e

1) and

×

Next, we want to analyze the update equation of

e

e

δx

e

e

V h

−

(R⊤l
(cid:16)

·

((Ql + Rl

∆M )

p
√V has O(r) non-zero blocks.

p

e

V

e

h))
e

(R⊤l

((Ql, eS + Rl

·

ΓM

eS)

·

−

·

e

δm))

(cid:17)

It is clear that the running time is dominated by the second term in the equation. We only focus

δx ←
e
Γ =

V

where

p
e
on that term.

e
−

e

·

e
1. Computing R⊤l Ql
2. Computing R⊤l Rl
p
k0 time, computing Rl ·
h
V h takes
∆M
p
k
e

V h takes O(bn) time, because Ql, Rl ∈
V h takes O(bn + b
∆M
e
p

r +

e

r

r

(

∆M
e
p

e

∆M
p
e
p

V h) takes nb.
e

(Rl
p
e
Last, the update equation of u1, u2, u3, u4 only takes the O(n) time.
p
Finally, we note that r
≤
Thus, overall the running time of the Multiply is

p

e

e

e

e

e

Rb

×

n.

k0) time. The reason is, computing
h
k
r, then ﬁnally computing R⊤l
·

V h) takes b

O(na) due to the guarantee of FullUpdate and PartialUpdate.

r
O(

k0 +
h
k

rω+o(1) +

r2 + b

r
r + nb) = O(

e

e

e

e

rω+o(1) + nb)
k0 +
h
k
k0 + rω+o(1) + nb)
h
= O(r
k
e
k0 + naω+o(1) + nb)
= O(na
h
k

e

r2
where the ﬁrst step follows from b
r = O(r), and the last step follows from r = O(na).
If h is the dense vector, then the overall time is
e

nb and

≤

e

r

rω+o(1), and the second step follows from

≤

e

e

O(nb + n1+a + naω+o(1)).

Based on Lemma 5.5 in [CLS19], we know that aω

time.

If h is a sparse vector, then the overall time is

1 + a. Thus, it becomes O(nb + n1+a+o(1))

≤

O(nb + na

k0 + naω+o(1)).
h
k

Lemma B.11 (MultiplyMove). The running time of MultiplyMove (Algorithm B.11) is the
Multiply time plus Move time.

35

B.3 Bounding W move

The goal of this section is to analyze the movement of W . [CLS19] provided a scalar version of W
move, here we provide a matrix version.

Lemma B.12 (W move, matrix version of Lemma 5.7 in [CLS19]).

m

Xi=1

E

gi ·

h

ψ(y(k)

π(i))

−

ψ(x(k)

π(i))
i

Proof. In scalar version, [CLS19] used absolute (
Frobenius norm (
We separate the term into two :

kkF ) to measure each x(k)

. Let I

i

| · |

⊆

= O(C1 + C2/ǫmp)

log n

·

·

(n−

a/2 + nω

5/2).

−

p
) to measure each x(k)

i
[m] be the set of indices such that

. In matrix version, we use
1.

x(k)
i kF ≤
k

m

Xi=1

E[ψ(y(k)

π(i))

gi ·

−

ψ(x(k)

π(i))] =

I
Xi
∈

gπ−1(i) ·

E[ψ(y(k)

i

)

−

ψ(x(k)
i

)] +

gπ−1(i) ·

E[ψ(y(k)

i

)

ψ(x(k)
i

)].

−

I c
Xi
∈

Case 1. Let us consider the terms from I.
Let vec(y(k)

) denote the vectorization of matrix y(k)

i

tion of x(k)

i

. Mean value theorem shows that

. Similarly, vec(x(k)

i

) denotes the vectoriza-

i

ψ(y(k)
i

)

−

ψ(x(k)
i

) =

vec(y(k)

i −

x(k)
i

)⊤ψ′′(ζ)vec(y(k)

i −

x(k)
i

)

ψ′(x(k)
i
h
ψ′(x(k)
i
ψ′(x(k)
i
h
L2
+
2 k

), y(k)

i −

), y(k)

i −
)−

), (v(k)
i

i

+

+

x(k)
i

1
2
L2
x(k)
i
2 k
i
1/2(w(k+1)
i
1/2(w(k+1)
i

−

(v(k)
i

)−

≤ h
=

x(k)
2
F
i k
)(v(k)
i

y(k)
i −
w(k)
i
)(v(k)
i

−
w(k)
i

)−

1/2

i

1/2

)−

2
F
k

where the second step follows from deﬁnition of L2 (see Part 4 of Lemma B.17).

Taking conditional expectation given w(k) on both sides

E[ψ(y(k)

i

)

ψ(x(k)
i

)]

−

≤ h

i

1/2(E[w(k+1)

), (v(k)
i
(v(k)
1/2(w(k+1)
E[
i
i
k
1/2(E[w(k+1)
)−

]

+

)−

)−

ψ′(x(k)
i
L2
2
(v(k)
L1k
i
L2
(v(k)
E[
+
i
2
k
1/2(w(k)
(v(k)
)−
L1k
i
L2
+
2 k
(v(k)
i

1/2(w(k)

(v(k)
i

)−

)−

)−

i

i

≤

≤

= L1k

i

−
1/2(w(k+1)
i

)1/2

2
k
)1/2

· k
4
k

1/2(w(k)

i

1/2

i
2
F ]
k

)(v(k)
i

)−

1/2

)−

1/2

kF
1/2

)−

2
F ]
k
1/2(E[w(k+1)

]

i

−
1/2(w(k+1)
i

)−

]

)−

−
w(k)
i

w(k)
i
)(v(k)
i

−
w(k)
i
)(v(k)
i
)(v(k)
w(k)
i
i
−
(w(k)
i
(w(k)
E[
i
k
L2
2 k

(v(k)
i

)−

·

)−

1/2(w(k)

i

)1/2

2
k

·

βi +

w(k)
i

)−

)(w(k)
i
)(w(k)
i

w(k)
i

1/2

)−

−
)1/2

4
k

·

γi

kF
1/2

2
F ]
k

(27)

where the second step follows from deﬁnition of L2 (see Part 4 of Lemma B.17), the third step
follows from

A

B

, and the last step follows from deﬁning βi and γi as follows:
k
(w(k)
i

1/2(E[w(k+1)

)(w(k)
i

w(k)
i

)−

)−

1/2

]

i

AB
k

kF ≤ k

kF · k

βi =

(cid:13)
γi = E
(cid:13)
(cid:13)

h(cid:13)
(cid:13)
(cid:13)

−

(w(k)
i

)−

1/2(w(k+1)
i

w(k)
i

)(w(k)
i

)−

−

36

F
2

.

F

i

(cid:13)
(cid:13)
1/2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

1/2

β2
i

!

2

(cid:17)

I
Xi
∈

(29)

To upper bound

i

I gπ−1(i)
∈

E[ψ(y(k)

i

)

ψ(x(k)
i

)], we need to bound the following two terms,

gπ−1(i)L1

)−

1/2(w(k)

i

)1/2

βi, and

I
Xi
∈

P

(v(k)
i
(cid:13)
(cid:13)
(cid:13)

gπ−1(i)

L2
2

I
Xi
∈

(v(k)
i
(cid:13)
(cid:13)
(cid:13)

For the ﬁrst term (which is related to β) in Eq. (28), we have

−
2

(cid:13)
(cid:13)
(cid:13)

)−

1/2(w(k)

i

)1/2

4

γi.

(28)

gπ−1(i)L1k

(v(k)
i

)−

1/2(w(k)

i

)1/2

2βi ≤  

k

I
Xi
∈

I (cid:16)
Xi
∈

gπ−1(i)L1k

(v(k)
i

)−

1/2(w(k)

i

)1/2

2
k

n

1/2

O(L1)

g2
i ·
≤
Xi=1
k2).
g
= O(C1L1k

C 2
1

!

where the ﬁrst step follows from Cauchy-Schwarz inequality, the second step follows from ni = O(1)
and

(v(k)
i
k
For the second term (which is related to γ) in Eq. (28), we have

2 = O(1).
k

1/2(w(k)

)1/2

)−

i

gπ−1(i)

L2
2 k

(v(k)
i

)−

1/2(w(k)

i

)1/2

4niγi ≤
k

O(L2)

I
Xi
∈

m

·

Xi=1

gi ·

g
γi = O(C2L2k

k2).

(30)

Putting Eq. (27), Eq. (29) and Eq. (30) together, and using several facts L1 = O(1), L2 =
5/2) (from Lemma B.13)

a/2 + nω

√log n

O(n−

−

O(1/ǫmp) (from part 4 of Lemma B.17) and
gives us

g
k

k2 ≤

·

gπ−1(i) ·

E[ψ(yk)
i )

ψ(x(k)
i

)]

−

≤

O(C1 + C2/ǫmp)

(n−

a/2 + nω

5/2).

−

log n

·

·

p

I
Xi
∈

(Note that, the above Equation is the same as [CLS19].)

Case 2. Let us consider the terms from I c.
For each i
where ǫmp ≤
on the i

x(k)
I c, we know
i kF ≥
∈
k
1/2, then ψ(y(k)
y(k)
1/4. If
i kF ≥
k
y(k)
I c such that
i kF < 1/2.
k
y(k)
I c with
i kF < 1/2, we have
k

∈
For each i

∈

i

1. We observe that ψ(x) is constant for

(2ǫmp)2,
) = 0. Therefore, we only need to focus

2
F ≥
k

x
k

ψ(x(k)
i

)

−

1
2

<

=

=

)−

y(k)
i −
k
(v(k)
)−
i
k
(v(k)
i
k
(v(k)
i
(v(k)
i
k
3
(w(k)
i
2 k

)−

)−

≤ k
=

≤

x(k)
i kF
1/2(w(k+1)
i
−
1/2(w(k)
)1/2
1/2(w(k)
)1/2
i
(v(k)
1/2w(k)
i
i

i

·

)−

1/2(w(k+1)
i

−

1/2

)−

)(v(k)
kF
i
1/2(w(k+1)
i

w(k)
i
(w(k)
)−
i
(w(k)
i

k · k
1/2
)−

)−
(w(k)
i
)(w(k)
i

k · k
w(k)
i

)−

1/2

kF

w(k)
i

−
1/2(w(k+1)
w(k)
i
i
−
1/2(w(k+1)
i

)−

)(w(k)
)−
i
)(w(k)
i
w(k)
i

−

1/2

·
1/2

)−
)(w(k)
i

(w(k)
i

1/2

)1/2(v(k)
)−
kF
i
)1/2(v(k)
(w(k)
)−
i

i

kF · k
1/2
)−

kF

1/2

k

(31)

where the last step follows from

y(k)
i kF =
k

w(k+1)
i
v(k)
i

k

I

kF ≤

−

1/2.

37

 
It is obvious that Eq. (31) implies

(w(k)
i
k

)−

1/2(w(k+1)
i

−

w(k)
i

)(w(k)
i

)−

1/2

kF > 1/3 > 1/4.

But this is impossible, since we assume it is

Thus, we have

1/4.

≤

gπ−1(i) ·

E[ψ(y(k)

i

)

−

ψ(x(k)
i

)] = 0.

I c
Xi
∈

We state a Lemma that was proved in previous work [CLS19].

Lemma B.13 (Lemma 5.8 in [CLS19]).

n

1/2

g2
i

!

Xi=1

log n

·

≤

p

O(n−

a/2 + nω

5/2)

−

B.4 Bounding V move

In previous work, [CLS19] only handled the movement of V in scalar version. Here, the goal of is
to understand the movement of V in matrix version. We start to give some deﬁnitions about block
diagonal matrices.

Deﬁnition B.14. We deﬁne block diagonal matrices X (k), Y (k), X (k+1) and Y
follows

(k)

⊗i

[m]

∈

Rni×

ni as

x(k)
i =

w(k)
i
v(k)
i

I,

−

y(k)
i =

w(k+1)
i
v(k)
i

I,

−

x(k+1)
i

=

w(k+1)
i
v(k+1)
i

I,

−

y(k)
i =

w(k+1)
i
v(k)
i

I.

−

Let ǫw denote the error between W and W

Lemma B.15 (V move, matrix version of Lemma 5.9 in [CLS19]). We have,

1/2

W −
i
k

(W i −

Wi)W −
i

1/2

ǫw.

kF ≤

n

gi ·

(cid:16)

Xi=1

ψ(y(k)

π(i))

−

ψ(x(k+1)
τ (i)

)

(cid:17)

Ω(ǫmprkgrk / log n).

≥

Proof. To prove the Lemma, similarly as [CLS19], we will split the proof into two cases.

Before getting into the details of each case, let us ﬁrst understand several simple facts which are
useful in the later proof. Note that from the deﬁnition of the algorithm, we only change the block if
y(k)
i kF is larger than the error between wi and wi. Hence, all the changes only decreases the norm,
k
namely ψ(y(k)

) for all i. So is their sorted version ψ(y(k)

ψ(xτ (i))(k+1) for all i.

ψ(x(k+1)
i

)

i

π(i))

≥

≥

38

 
Case 1. The procedure exits the while loop when 1.5rk ≥

n.

Let u∗ denote the largest u such that
If u∗ = rk, we have that

y(k)
π(u)kF ≥
k

ǫmp.

If u∗ 6

= rk, using the condition of the loop, we have that

y(k)
π(rk)kF ≥
k

ǫmp ≥

ǫmp/100.

y(k)
π(rk)kF ≥
k

(1

−

1/ log n)log1.5 rk−
1/ log n)log1.5 n

ǫmp

·

log1.5 u∗

y(k)
π(u∗)kF

· k

(1
−
ǫmp/100.

≥

≥

where the last step follows from n
Recall the deﬁnition of x(k+1)

τ (i)

4.

following sense,

≥
. We can lower bound the LHS in the Lemma statement in the

n

Xi=1

gi ·

π(i))

ψ(y(k)
(cid:16)

−

ψ(x(k+1)
τ (i)

)

(cid:17)

≥

≥

2n/3

Xi=n/3+1
2n/3

Xi=n/3+1
2n/3

gi ·

π(i))

ψ(y(k)
(cid:16)

−

ψ(x(k+1)
τ (i)

)

(cid:17)

(Ω(ǫmp)

gi ·

−

O(ǫw))

Ω(ǫmp)

gi ·

≥

Xi=n/3+1
= Ω(rkgrk ǫmp).

where the second step follows from
i < 2n/3.

y(k)
π(i)kF ≥ k
k

y(k)
π(rk)kF ≥

(1

y(k)
O(ǫw))
π(rk)kF ≥
k

−

ǫmp/200 for all

Case 2. The procedure exits the while loop when 1.5rk < n and

Using the same argument as Case 1, we have

y(k)
π(1.5rk)kF < (1
k
−

y(k)
π(rk)kF .
1/ log n)
k

y(k)
π(rk)kF ≥
k
Using Part 3 of Lemma B.17 and the following fact

ǫmp/100.

we can show that

y(k)
π(1.5r)kF < min
k

y(k)
π(r)kF ·
k

(1

−

ǫmp,

(cid:16)

1/ log n)

,

(cid:17)

ψ(y(k)

π(1.5r))

ψ(y(k)

π(r)) = Ω(ǫmp/ log n).

−
π(1.5r)) to ψ(y(k)
Now the question is, how to relax ψ(y(k)
kF for all i. Hence, we have ψ(y(k)
π(i))

y(k)
Note that
i kF ≥ k
k
Recall the deﬁnition of y, y, π and π,

x(k+1)
i

π(1.5r)) and how to relax ψ(y(k)

π(r)) to ψ(y(k)

π(r))

ψ(x(k+1)
τ (i)

) for all i.

≥

(32)

y(k)
i =

w(k+1)
i
v(k)
i

I,

−

y(k)
i =

w(k+1)
i
v(k)
i

I.

−

39

and π and π denote the permutations such that

Using Fact B.16 and

k · k2 = Θ(1)

y(k)
π(i)kF ≥ k
k

y(k)
π(i+1)kF and
k · kF when the matrix has constant dimension
y(k)
π(i) −
k

y(k)
π(i)kF ≤

O(ǫw).

y(k)
π(i)kF ≥ k
k

y(k)
π(i+1)kF .

where ǫw is the error between W and W .

Next,

∀

i, we have

ψ(y(k)

π(i)) = ψ(y(k)
π(i))

O(ǫwǫmp)

±

(33)

1.
Next, we note that all the blocks the algorithm updated must lies in the range i = 1,
After the update, the error of rk of these block becomes so small that its rank will much higher
, 3rk
than rk. Hence, rk/2 of the unchanged blocks in the range i = 1,
2 will move earlier in the
rank. Therefore, the rk/2-th element in x(k+1) must be larger than the 3
2 rk-th element in y(k). In
short, we have ψ(x(k+1)

2 −

· · ·

· · ·

)

rk/2.

π(1.5rk)) for all i

ψ(y(k)
τ (i)
Putting it all together, we have

≤

≥

, 3rk

n

Xi=1
rk

gi ·

≥

≥

≥

≥

≥

≥

Xi=rk/2
rk

Xi=rk/2
rk

Xi=rk/2
rk

Xi=rk/2
rk

Xi=rk/2
rk

Xi=rk/2

ψ(x(k+1)
τ (i)

)

−

π(i))

ψ(y(k)
(cid:16)
gi ·

π(i))

ψ(y(k)
(cid:16)

gi ·

π(i))

ψ(y(k)
(cid:16)

−

−

gi ·

π(i))

ψ(y(k)
(cid:16)

−

(cid:17)
ψ(x(k+1)
τ (i)

)

(cid:17)

ψ(y(k+1)

π(1.5rk))
(cid:17)

ψ(y(k+1)

π(1.5rk))

−

O(ǫwǫmp)

(cid:17)

by ψ(x(k+1)

τ (i)

)

ψ(y(k)

π(1.5rk)),

≤

i

∀

≥

rk/2

by (33)

gi ·

π(rk))

ψ(y(k)
(cid:16)

−

ψ(y(k+1)

π(1.5rk))

−

O(ǫwǫmp)

(cid:17)

by ψ(y(k)

π(i))

ψ(y(k)

π(rk)),

i

∀

∈

≥

[rk/2, rk]

ǫmp
log n

)

−

grk ·

Ω(

(cid:16)

Ω(

grk ·

ǫmp
log n

)

O(ǫwǫmp)

(cid:17)

by (32)

by ǫw < O(1/ log n)

= Ω (ǫmprkgrk / log n) .

Therefore, we complete the proof.

Fact B.16. Given two length n positive vectors a, b. Let a be sorted such that ai ≥
denote the permutation such that bπ(i) ≥
i
∈

bπ(i+1). If for all i

bπ(i)| ≤

bi| ≤

ai −
|

ai −
|

ǫai.

[n],

[n],

∈

ai+1. Let π
ǫai. Then for all

Proof. Case 1. π(i) = i. This is trivially true.

Case 2. π(i) < i. We have

bπ(i) ≥

bi ≥

(1

−

ǫ)ai

40

Since π(i) < i, we know that there exists a j > i such that π(j) < π(i). Then we have

Combining the above two inequalities, we have (1

Case 3. π(i) > i. We have

bπ(i) ≤

bπ(j) ≤

(1 + ǫ)ai

(1 + ǫ)aj ≤
ǫ)ai ≤

−

bπ(i) ≤

(1 + ǫ)ai.

bπ(i) ≤
Since π > i, we know that there exists j < i such that π(j) > π(i). Then we have

(1 + ǫ)ai

bi ≤

Combining the above two inequalities gives us (1

Therefore, putting all the three cases together completes the proof.

bπ(i) ≥

bπ(j) ≥

(1

−

ǫ)aj ≥
ǫ)ai ≤

−

(1

ǫ)ai.

−
bπ(i) ≤

(1 + ǫ)ai.

B.5 Potential function ψ

[CLS19] used a scalar version potential function. Here, we generalize it to the matrix version.

−

Lemma B.17 (Matrix version of Lemma 5.10 in [CLS19]). Let function ψ : square matrix
(deﬁned as Eq. (26)) satisﬁes the following properties :
1. Symmetric (ψ(x) = ψ(
x
2. If
y
kF ≥ k
k
= Ω(1/ǫmp),
f ′(x)
3.
|
|
def
Dxψ[H]
4. L1
= maxx
H
kF
5. ψ(x) is a constant for
Proof. Let f : R+ →

x)) and ψ(0) = 0
ψ(y)
[(0.01ǫmp)2, ǫ2
def
= maxx
2ǫmp

kF , then ψ(x)
x
∀
= 2 and L2
x
k

R be deﬁned as

xψ[H,H]
2
F
k

= 10/ǫmp

kF ≥

mp]

D2

≥

∈

H

k

k

R

→

x2
2ǫ3
mp

,

ǫmp −
ǫmp,

f (x) = 



We can see that

(4ǫ2

x)2

mp−
18ǫ3
mp

,

x

x

x

∈

∈

∈

[0, ǫ2

mp];
mp, 4ǫ2
mp, +

(ǫ2
(4ǫ2

mp];
).

∞

f (x)′ = 

0,


,

x
ǫ3
mp
4ǫ2
mp−
9ǫ3
mp

x

x

x

,

∈

∈

[0, ǫ2

mp];
mp, 4ǫ2
mp, +

(ǫ2
(4ǫ2

mp];
).

∈
1
ǫmp

∞
and maxx |

x

1
ǫ3
mp

,

and f (x)′′ = 


−
0,

1
9ǫ3
mp

,

[0, ǫ2
mp];
mp, 4ǫ2
(ǫ2
(4ǫ2
mp, +

mp];

∈

∈

x

x

x
|

| ∈

).

∞

f (x)′′| ≤

1
ǫ3
mp

X
. Let ψ(x) = f (
k

2
F ).
k



It implies that maxx |

f (x)′| ≤

Proof of Part 1,2 and 5. These proofs are pretty standard from deﬁnition of ψ.
Proof of Part 3. This is trivially following from deﬁnition of scalar function f .
Proof of Part 4. By chain rule, we have

2
F )
Dxψ[h] = 2f ′(
X
k
k
2
F )
X
xψ[h, h] = 2f ′′(
k
k

D2

tr[XH]
(tr[XH])2 + 2f ′(
x
k

·

·

2
F )
k

·

tr[H 2]

41

where x is the vectorization of matrix X and h is the vectorization of matrix H. We can upper
bound

Dxψ[h]
|

| ≤

X
f ′(
2
k
|

2
F )
k

| · |

tr[XH]

| ≤

X
f ′(
2
k
|

2
F )
k

X

H

kF · k

kF

| · k

Then, we have

X
f ′(
k
|

2
F )
k

| · k

X

F /ǫ3
3
X
k
k
(4ǫ2
kF = 
mp − k

0,

1,
mp ≤
2
X
X
F )
k
k

kF /9ǫmp ≤

2/3,

X
k
X
k
X
k

kF ∈
kF ∈
kF ∈

[0, ǫmp]
(ǫmp, 2ǫmp]
)
(2ǫmp, +

∞

It implies that

H
2
Dxψ[h]
k
|
By case analysis, we have

| ≤


kF ,

x.

∀

X
f ′′(
k
|

2
F )
k

| · k

X

1
ǫ3
2
mp k
F ≤ (
k
0,

X

2
F ≤
k

4/ǫmp,

X
k

2
F ∈
k
2
X
F ∈
k
k

[0, 4ǫ2

mp]
(4ǫmp, +

)

∞

We can also upper bound

X
f ′′(
2
k
|
f ′′(
X
2
|
k
4
2
ǫmp k

| ·

2
F )
k
2
F )
k
H

(tr[XH])2 + 2
2
X
f ′(
F )
|
k
| ·
k
kF )2 + 2
X
f ′(
k
|
H

H

kF k
1
ǫmp k

X
(
| ·
k
2
F + 2
k

·

2
F
k

tr[H 2]
2
F )
k

| · k

H

2
F
k

·
10
ǫmp k

H

2
F .
k

D2
|

xψ[h, h]

| ≤

≤

≤

=

B.6

x and x are close

Lemma B.18 (x and x are close in term of
Rb
sketching matrix R

n, we have

×

∈

V −

1). With probability 1

−

δ over the randomness of

e
xik eV −1
xi −
k

i ≤

ǫx

ǫx = O(α log2(n/δ)

n1/4
√b

·

), b is the size of sketching matrix.

Proof. Recall the deﬁnition of

δx and δx, we have

δx,i −
e

δx,i =

V 1/2
i

(I

R⊤R
e

P )

V 1/2h

V 1/2
i

(I

−

−

−

P )

V 1/2h =

V 1/2
i

P
(

−

R⊤R

P )

V 1/2h

For iteration t, the deﬁnition should be
e
e
V (t)
i

δ(t)
x,i = (

e

δ(t)
x,i −

e

e

e

e

e

e

e

)1/2(

P (t)

(R(t))⊤R(t)

P (t))(

V (t))1/2h.

−

For any i, let k be the current iteration, ki be the last when we changed the

e

e

e

e

e

that

x(k)
i −

x(k)
i =

k

Xt=ki

δ(t)
x,i −

δ(t)
x,i

e

42

Vi. Then, we have

e

V (t)
because we have x(ki)
i
iteration ki to k for the block i. (However, the whole other parts of matrix
consider

(guaranteed by our algorithm). Since

i = x(ki)

e

i

did not change during
V could change). We

(x(k)

i −

x(k)
i

)⊤

·

V (k)
(
i

1

)−

(x(k)

i −

·

x(k)
i

) =

e

k





Xt=ki
k

e

k

δ(t)
x,i −

⊤

δ(t)
x,i

V (k)
(
i

1

)−

·

e

(I

e

(R(t))⊤R(t))

P (t)(

δ(t)
x,i −

δ(t)
x,i

2



· 

Xt=ki



e
V (t))1/2h(t)

=

−

(cid:13)
(cid:13)
Xt=ki (cid:16)
(cid:13)
(cid:13)
(cid:13)
block i. We deﬁne random vector Xt ∈
(cid:13)
⊤R(t))

V (t))1/2h(t)

P (t)(

e

e

.

∈
R(t)

.

i(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:17)
Rni as follows:

We consider block i and a coordinate j

Xt =

(I

−

(cid:16)

Let (Xt)j denote the j-th coordinate of Xt, for each j
e
By Lemma E.5 in Section E, we have for each t,

E[Xt] = 0,

and E[(Xt)2

j ] =

i

(cid:17)

[ni].
e
∈

1
P (t)(
(
b k

2
2

V (t))1/2h(t))ik
e

and with probability 1

δ,

−

e

Now, we apply Bernstein inequality (Lemma E.3),

e

P (t)(
(
(Xt)j| ≤ k
|

V (t))1/2h(t))ik2
e

log(n/δ)
√b

:= M.

Choosing τ = 103 √T
√b

exp

# ≤

 −

t

τ 2/2
E[(Xt)2

j ] + M τ /3 !

Pr

(Xt)j > τ

"

t
X
log2(n/δ)

P

P (t)(
(
· k
(Xt)j > 103 √T
√b

V (t))1/2h(t))ik2
e
log2(n/δ)

e

Pr

"

t
X

exp

≤

 −

T
P (t)(
(
b k

106 T
V (t))1/2h(t))ik
e

exp(

−
Now, taking a union, we have

100 log(n/δ))
e

≤

P (t)(
(
· k
b log4(n/δ)

V (t))1/2h(t))ik2
#
e
e
V (t))1/2h(t))ik
P (t)(
(
· k
2 + 103 √T
log3(n/δ)
P (t)(
(
k
e
e

2

b

2
2/2
V (t))1/2h(t))ik
e

2
2/3 !

e

k

(I

(cid:13)
(cid:13)
Xt=ki (cid:16)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

(R(t))⊤R(t))

P (t)(

V (t))1/2h(t)

e

e

(cid:17)

i(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

= O

O

≤

√T
√b

√T
√b

log2(n/δ)

P (t)(
(

V (t))1/2h(t))i

(cid:13)
(cid:13)
(cid:13)
log2(n/δ)α

!

e

e

2!

(cid:13)
(cid:13)
(cid:13)

where we use that

V (t))1/2h(t))ik2 = O(α), ni = O(1).
V (t))1/2h(t))ik2 ≤ k
((
Finally, we use the fact that the algorithm reset x = x, s = s in less than √n iterations.
e
e

P (t)(
(
k

e

43

 
 
B.7

s and s are close

Lemma B.19 (s and s are close). With probability 1
R

n, we have

Rb

×

∈

δ over the randomness of sketching matrix

−

1

t−

si −
k

sik eVi ≤

ǫs,

ǫs = O(α log2(n/δ)

n1/4
√b

·

, and b is the size of sketching matrix.

Proof. Recall the deﬁnition of

δs, δs, we have

δs,i −
e
The rest of the proof is identical to Lemma B.18 except we use also the fact we make s = s whenever
e
e
our t changed by a constant factor. We omitted the details here.

δs,i = t

(R⊤R

V −
i

I)

−

P

e

e

V 1/2h

1/2

B.8 Data structure is maintaining (x, s) implicitly over all the iterations

Lemma B.20. Over all the iterations, u1 + F u2 is always maintaining x implicitly, u3 + Gu4 is
always maintaining s implicitly.

Proof. We only focus on the PartialUpdate. The FullUpdate is trivial, we ignore the proof.

For x.
Note that M is not changing. Let’s assume that u1 + F u2 = x, we want to show that
1 + F newunew
unew

2 = xnew.

which is equivalent to prove

Let ∆u1 = unew

1 −

1 + F newunew
unew

2 −

(u1 + F u2) = δx

u1 be the change of u1 over iteration t, then

∆u1 =

V newh + (F

F new)u2

−

u2 be the change of u2 over iteration t, then

Let ∆u2 = unew

2 −

e
V new)1/2h + 1
(

eS(
By deﬁnition of δx at iteration t, we have

∆u2 =

−

e

∆−

1
eS, eS

+ M

eS, eS)−

1M ⊤
eS

V new)1/2h.
(

e

e

δx =

V newh

−

We can compute

e

V newM

V newh

V newM

−

p

e

eS(

∆−

1
eS, eS

+ M

eS, eS)−

1(M

eS)⊤

V newh

.

p

e

(cid:17)

e

F u2)

(u1 + F u2)
F u2)

2 −
2 −
F new)u2 + (F newunew
u2)

2 −

2 −

(cid:16)p

p

e

e
1 + F newunew
unew
= ∆u1 + (F newunew
=

V newh + (F
V newh + F new(unew
e
V newh + F new∆u2
e
V newh
e
= δx
e

F new

p

−

−

=

=

=

e

e

44

V newh + F new1

S (
e

∆−

1
eS, eS

+ M

S,
e

S)−
e

1(M

S)⊤
e

V newh

p

e

where we used F new =

V newM in the last step.

For s.
We have

p

e

Gnew =

1

V new

M, G =

1

V

M

Let ∆u3 = unew

3 −

p
u3 be the change of u3 over iteration t, then
e

p

e

Let ∆u4 = unew

4 −

u4 be the change of u4 over iteration t, then

∆u3 = (G

Gnew)u4

−

By deﬁnition of δs in iteration t,

∆u4 = t

∆u2

·

δs =

1

V new

M

V new(th)

p

e

1

−

V new

p

e

M

eS(

∆−

1
eS, eS

+ M

eS, eS)−

1(M

eS)⊤

e

V new(th)

!

p

e

p

We can compute
e
(unew

3 + Gnewunew

4

)

−

Gu4)

(u3 + Gu4) = ∆u3 + (Gnewunew
= (G
−
= Gnew(unew
= Gnewt∆u2
= δs

4 −
Gnew)u4 + (Gnewunew
u4)

4 −

4 −

Gu4)

where the last step follows by deﬁnition of ∆u2.

45

 
C Combining Robust Central Path with Data Structure

The goal of this section is to combine Section A and Section B.

Statement
Lem. C.1, Thm. B.1
Lem. C.1, Thm. B.1
RobustIPM Alg in Sec. A

Notation Choice of Parameter
C1
C2
ǫmp
T
α
b
ǫx
ǫs
ǫw
a

Θ(1/ log2 n)
Θ(1/ log4 n)
Θ(1/ log2 n)
Θ(√n log2 n log(n/δ)) Thm. 4.2
Θ(1/ log2 n)
Θ(√n log6(nT )
Θ(1/ log3 n)
Θ(1/ log3 n)
Θ(1/ log3 n)
min(2/3, αm)

RobustIPM Alg in Sec. A
Lem. B.18, Lem. B.19, Lem. C.2
Lem. B.18
Lem. B.19
Lem. C.2
αm is the dual exponent of MM batch size

Comment
ℓ2 accuracy of W sequence
ℓ4 accuracy of W sequence
accuracy for data structure
#iterations
step size in Hessian norm
sketch size
accuracy of x (respect to x)
accuracy of s (respect to s)
accuracy of W (respect to W )

Table 3: Summary of parameters

C.1 Guarantee for W matrices

Lemma C.1 (Guarantee of a sequence of W ). Let xnew = x + δx. Let W new = (
W = (

1. Then we have

2φ(x))−

∇

∇

2φ(xnew))−

1 and

m

Xi=1 (cid:13)
(cid:13)
m
(cid:13)

Xi=1 (cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
where C2 = Θ(α2) and C1 = Θ(α).
(cid:13)

Proof. For each i

[m], we have

∈

1/2

w−
i

(wnew

i −

wi)w−
i

2

1/2

1/2

w−
i

(wnew

i −

1/2

wi)w−
i

1/2

w−
i

(wnew

i −

1/2

wi)w−
i

C 2
1 ,

C 2
2 ,

1
4

.

F ≤

(cid:13)
(cid:13)
4
(cid:13)

F ≤

F ≤

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

1/2

W −
i

(W new

Wi)W −
i

i −
(W new

i −

1/2

W −
i

Wi)W −
i

2

1/2

F

(cid:13)
1/2
(cid:13)
(cid:13)

2

2φ(xi))1/2(

∇

2φ(xnew
i

1

)−

(cid:13)
(cid:13)
= ni
(cid:13)

= ni

≤  

= ni

(cid:13)
(cid:13)
(
(cid:13)
∇
(cid:13)
(cid:13)
(cid:13)
(1

(1
100nik

xnew
i −
2
xik

− k
xnew
i −

∇

xik∇
2φ(xi),

≤

2φ(xi))2 −

1
!

(cid:13)
(cid:13)
(cid:13)
− ∇
2

2φ(xi)−

1)(

∇

2φ(xi))1/2

2

2φ(xi))2 −

1
!

2φ(xi))1/2

(
∇

∇

(cid:13)
(cid:13)
2φ(xi)−
(cid:13)

2φ(xi))1/2

1(

∇

2

(cid:13)
(cid:13)
(cid:13)

xnew
i −

− k

1

xik∇
1

·

(cid:13)
(cid:13)
2
(cid:13)

46

 
where the second step follows by Theorem 2.3.

In our problem, we assume that ni = O(1). It remains to bound

xnew
i −
k

xik

2

∇

2φ(xi) =

δx,ik
k

2

∇

2φ(xi) .

δx,ik
k

xi = α2
2
i

where the last step follows from deﬁnition αi =

Then, we have

δx,ikxi.
k

m

Xi=1

xnew
i −
k

xik

2

∇

2φ(xi) ≤

m

Xi=1

O(α2
i )

≤

O(α2).

where the last step follows by Lemma A.1.

Lemma C.2 (Accuracy of W ). Let x and x be the vectors maintained by data-structure Stochas-
2φ(x))−
ticProjectionMaintenance. Let W = (

1. Then we have

1 and W = (

2φ(x))−

∇
(wi −

1/2

w−
i
k

1/2

wi)w−
i

ǫw,

kF ≤

∇

where ǫw = O

α log2(nT )

, b is the size of sketching matrix.

Proof. By similar calculation, we have

(cid:16)

n1/4
√b

·

(cid:17)

1/2

w−
i
k

(wi −

wi)w−
i

1/2

kF = O(1)

xi −

· k

xik∇

2φ(xi).

Then, using Lemma B.18 with δ = 1/T

xi −
k

xik∇

2φ(xi) ≤

O

α log2(nT )

√n1/4

·

√b !

.

C.2 Main result

The goal of this section is to prove our main result.

Theorem C.3 (Main result, formal version of Theorem 1.1). Consider a convex problem

Ax=b,x

min
∈

Qm

i=1 Ki

c⊤x

where Ki are compact convex set. For each i
∈
φi for Ki. Also, we are given x(0) = arg minx

[m], we are given a νi-self concordant barrier function

i φi(xi). Assume that

1. Diameter of the set: For any x

∈
2. Lipschitz constant of the program:

Q
c
k2 ≤
k

m
i=1 Ki, we have that

P

x
k

k2 ≤

R.

L.

47

 
Algorithm 5 Robust Central Path

4:

3:

6:

5:

m do

i ))1/2 if γt

1: procedure CentralPathStep(x, s, t, λ, α)
2:

→
φi(xi)
si/t +
∇
µt
2φi(xi)−1
ik∇
i )/γt
exp(λγt
i
(Pm
i=1 exp(2λγt
ct
α
i ·
·
2φ(x))−

for i = 1
µt
i ←
γt
i ← k
ct
i ←
hi ← −
end for
W
(
∇
return h, W
9:
10: end procedure
11:
12: procedure RobustCentralPath(mp, t, λ, α)
13:

i ≥

µt
i

←

8:

7:

1

96√α and ct

i ←

(x, s)

h, W

←

←

mp.Query()

CentralPathStep(x, s, t, λ, α)

mp.Update(W )
mp.MultiplyMove(h, t)

⊲ Figure out direction h
⊲ According to Eq. (6)
⊲ According to Eq. (7)

0 otherwise

⊲ According to Eq. (9)

⊲ According to Eq. (8)

⊲ Computing block-diagonal matrix W

⊲ Lemma A.8
⊲ Standing at (x, s) implicitly via data-structure
⊲ Standing at (x, s) explicitly via data-structure
⊲ Algorithm 1, Lemma B.8

23:
24: end procedure

⊲ Algorithm 2, Lemma B.5
⊲ Algorithm 4, Lemma B.10, Lemma B.9
s + δs, achieved by data-structure implicitly
δs, achieved by data-structure explicitly
s +
x

⊲ If x is far from x, then x

←

⊲ x
⊲ x

←
←

x + δx, s
δx, s
x +

←
←

e

e

Algorithm 6 Our main algorithm (More detailed version of RobustIPM in Section 4)

1: procedure Main(A, b, c, φ, δ)
216 log(m), α
2:
min( 1
λ , δ)
min(2/3, αm)

3:

4:

10α

20λ−

2 , κ

·

←

←

←

2−

2−

log log(1/δ)

210√ν log6(n/δ)

λ
←
δ
←
a
←
bsketch
Modify the ERM(A, b, c, φ) and obtain an initial x and s
CentralPathMaintenance mp
mp.Initialize(A, x, s, α, a, bsketch)
ν
t
while t > δ2/(4ν) do
κ
√ν )t

m
i=1 νi

←
←

1
P

⊲ Theorem 1.1, Theorem C.3

⊲ Choose the target accuracy
⊲ Choose the batch size
⊲ Choose the size of sketching matrix

⊲ Algorithm 1, Theorem B.1
⊲ Algorithm 1, Lemma B.4
⊲ νi are the self-concordant parameters of φi

−

(1

←

tnew
RobustCentralPath(mp, t, λ, α)
t
end while
Return an approximate solution of the original ERM according to Section D

tnew

←

⊲ Algorithm 5

16:
17: end procedure

14:

15:

16:
17:

18:

19:

20:

21:

22:

5:

6:

7:

8:

9:
10:

11:

12:

13:

14:

15:

48

Then, the algorithm Main ﬁnds a vector x such that

c⊤x

≤

Ax=b,x

min
∈

Qm

i=1 Ki

c⊤x + LR

δ,

·

Ax
k

−

b

k1 ≤

3δ

· 

R

x

∈

m


Ki.

Yi=1

Ai,j|
|

+

b
k

k1

,



Xi,j

in time

O(nω+o(1) + n2.5
−

α/2+o(1) + n2+1/6+o(1))

O(log(n/δ)).

·

where ω is the exponent of matrix multiplication [Wil12, LG14], and α is the dual exponent of matrix
multiplication [LGU18].

e

Proof. The number of iterations is

O(√ν log2(m) log(ν/δ)) = O(√n log2(n) log(n/δ)).

For each iteration, the amortized cost per iteration is

O(nb + n1+a + n1.5) + O(C1/ǫmp + C2/ǫ2

1/2+o(1) + n2
−

= O(nb + n1+a + n1.5) + O(α + α2)
·
= O(nb + n1+a + n1.5) + O(1/ log4 n)
= O(n1.5+o(1) log6 log(1/δ) + n1+a+o(1)) + O(nω

−
(nω

(nω

·

a/2+o(1)) + O(nω

−
a/2+o(1)) + O(nω
a/2+o(1)).

1/2+o(1) + n2
−

−

1/2+o(1))

−

−

(nω
mp)
·
1/2+o(1) + n2
−
1/2+o(1) + n2
−

−

a/2+o(1)) + O(nω
1/2+o(1))

1/2+o(1))

−

where the last step follows from choice of b (see Table 3).

Finally, we have

total time

= #iterations

cost per iteration

·
√n log2 n log(n/δ)

(cid:0)

#iterations

(cid:1)

O

·

n1.5+o(1) log6 log(1/δ) + n1+a+o(1) + nω
(cid:16)

cost per iteration

1/2+o(1) + n2
−

−

a/2+o(1)

}

{z

|

n1.5+a+o(1) + nω+o(1) + n2.5
−
(cid:16)
n2+1/6+o(1) + nω+o(1) + n2.5
−
(cid:16)

a/2+o(1)

·
(cid:17)
αm/2+o(1)

log(n/δ)

·
log(n/δ)

·

·

log6 log(1/δ)

{z

log6 log(1/δ)

(cid:17)
where we pick a = min(2/3, αm) and αm is the dual exponent of matrix multiplication[LGU18].

Thus, we complete the proof.

Corollary C.4 (Empirical risk minimization). Given convex function fi(y) : R
y :
solution x∗ ∈
{
Given a matrix A
M
b
with
k2 ≤
k

R. Suppose the
4√n
R
.
M
}
Rd
M and A has no redundant constraints, and a vector b

-Ball(0, R). Suppose fi is L-Lipschitz in region

A
k
R. We can ﬁnd x

Rd lies in ℓ
×

→
y
| ≤
|

∞
n with

Rd s.t.

k ≤
∈

·
∈

Rd

∈

·

·

n

Xi=1

fi(a⊤i x + bi)

≤

min
Rd
x
∈

n

Xi=1

49

fi(a⊤i x + bi) + δM R

= O

|
= O

= O

(cid:17)

}

in time

O(nω+o(1) + n2.5
−

α/2+o(1) + n2+1/6+o(1))

O(log(n/δ)).

·

where ω is the exponent of matrix multiplication [Wil12, LG14], and α is the dual exponent of matrix
multiplication [LGU18].

e

Proof. It follows from applying Theorem C.3 on convex program (1) with an extra constraint x∗
lies in ℓ

-Ball(0, R). Note that in program (1), ni = 2. Thus m = O(n).

∞

D Initial Point and Termination Condition

We ﬁrst need some result about self concordance.

Lemma D.1 (Theorem 4.1.7, Lemma 4.2.4 in [Nes98]). Let φ be any ν-self-concordant barrier.
Then, for any x, y

domφ, we have

∈

x

i ≤

ν,

x

i ≥

y
k
−
1 +
y
k
x∗kx∗
x
k
ν + 2√ν.

−

x

2
x
k
x
kx
−

.

1, we have that x

domφ.

∈

≤

φ(y)

h∇

Let x∗ = arg minx φ(x). For any x

∈

φ(x), y

h∇

−

φ(x), y

− ∇
−
Rn such that

≤
Lemma D.2. Consider a convex problem minAx=b,x
For each i
x(0) = arg minx

i φi(xi). Assume that

−

∈

x∗
k

y

kx∗

P

1. Diameter of the set: For any x

∈
2. Lipschitz constant of the program:

c
Q
k2 ≤
k
For any δ > 0, the modiﬁed program minAx=b,x
∈

L.

m
i=1 Ki, we have that

x
k

k2 ≤

R.

Qm

i=1 Ki×

c⊤x with

R+

i=1 Ki c⊤x where Ki are compact convex set.
Qm
[m], we are given a νi-self concordant barrier function φi for Ki. Also, we are given

∈

A = [A

b

|

−

Ax(0)], b = b, and c =

c

δ
LR ·
1

(cid:21)

(cid:20)

satisﬁes the following:

1. x =

, y = 0d and s =

x(0)
1

(cid:20)

(cid:21)
where φ(x) =

m
i=1 φi(xi)

−

c

δ
LR ·
1
(cid:21)
(cid:20)
log(xm+1).

are feasible primal dual vectors with

s+
k

∇

φ(x)

k∗x ≤

δ

2. For any x such that Ax = b, x

c⊤x + δ2,
the vector x1:n (x1:n is the ﬁrst n coordinates of x) is an approximate solution to the original
convex program in the following sense

minAx=b,x
∈

i=1 Ki ×

i=1 Ki×

R+ and c⊤x

Qm

P

R+

Q

≤

∈

m

c⊤x1:n ≤

Ax=b,x

min
∈

Qm

i=1 Ki

c⊤x + LR

δ,

·

Ai,j|
|

+

b
k

k1

,



Xi,j

Ax1:n −
k

b

k1 ≤

3δ

R

· 

x1:n ∈

m


Ki.

Yi=1

50

Proof. For the ﬁrst result, straightforward calculations show that (x, y, s) are feasible.

To compute

s +
k

∇

φ(x)

k∗x, note that

s +
k

φ(x)
∗x =
k

∇

k

δ
LR ·

c
k∇

2φ(x(0))−1.

Rn such that

Lemma D.1 shows that x
x(0) = arg minx

≤
i φi(xi). Hence, for any v such that v⊤∇
m
x(0)
i=1 Ki and hence
k2 ≤
k2 ≤
k
P
R2
2φ(x(0)))−
I. Hence, we have
·

R. This implies

kx(0)

(
∇
Q

x
k

v
k

−

(cid:22)

±

x(0)

∈

v

1

1, we have that x
2φ(x(0))v
R for any v⊤∇

≤

m
i=1 Ki because

∈

1, we have that x(0)
Q
2φ(x(0))v

∈
±
1. Hence,

v

≤

s +
k

φ(x)
∗x =
k

∇

k

δ
LR ·

c
k∇

2φ(x(0))−1

δ
L ·

≤ k

c
k2 ≤

δ.

For the second result, we let OPT = minAx=b,x

For any feasible x in the original problem, x =

i=1 Ki c⊤x and OPT = minAx=b,x

Qm

i=1 Ki×

∈

c⊤x.

R+

is a feasible in the modiﬁed problem. There-

Qm
∈
x
0

(cid:21)

(cid:20)

fore, we have that

OPT

δ
LR ·

≤

c⊤x =

δ
LR ·

OPT.

Given a feasible x with additive error δ2. Write x =

c⊤x which is

δ
LR ·

c⊤x1:n + τ . Then, we have

x1:n
τ

(cid:20)

(cid:21)

for some τ

≥

0. We can compute

δ
LR ·

c⊤x1:n + τ

≤

OPT + δ2

δ
LR ·

≤

OPT + δ2.

(34)

Hence, we can upper bound the OPT of the transformed program as follows:

c⊤x1:n =

LR
δ ·

δ
LR

c⊤x1:n ≤

LR
δ

δ
LR ·

(cid:18)

OPT + δ2

= OPT + LR

(cid:19)

δ,

·

where the second step follows by (34).
For the feasibility, we have that τ
0 c⊤x

δ
LR ·
LR and that c⊤x1:n ≤

≤ −

OPT = minAx=b,x
that

≤

≥

c⊤x1:n + δ
δ + δ + δ because
LR. The constraint in the new polytope shows

OPT + δ2

LR ·

≤

Rewriting it, we have Ax1:n −

Ax1:n + (b

Ax(0))τ = b.

−

b = (Ax(0)

b)τ and hence

−
Ax1:n −
k

b

Ax(0)

k1 ≤ k

b

k1 ·

−

τ.

Lemma D.3. Let φi(xi) be a νi-self-concordant barrier. Suppose we have si
i

[m], A⊤y + s = c and Ax = b. Suppose that

∇
1 for all i, we have that

t +

φi(xi) = µi for all

∈

where x∗ = arg minAx=b,x

Qm

i=1 Ki c⊤x and ν =

∈

c, x

h

i ≤ h

µik∗x,i ≤
k
c, x∗

+ 4tν

i
m
i=1 νi.

P

51

Proof. Let xα = (1
ν. Hence, we have

α)x+αx∗ for some α to be chosen. By Lemma D.1, we have that
ν
. Hence, we have

x

φ(xα), x∗ −

xαi ≤

h∇

−
1

α ≥ h∇

−

να

1

−

α ≥ h∇

φ(xα), x∗ −
x

i

φ(xα), xα −
φ(xα)

i
φ(x), xα −
− ∇
xα,i −
xik
k
xα,i −
1 +
k
α2
x∗i −
k
x∗i −
1 + α
k

xikxi
xik
xikxi −

2
xi

2
xi

+

h∇
m

Xi=1
m

=

≥

≥

x

+

µ

i

D
µ, xα −

h

s
t

−

x

i −

m

α

µik
k

∗xik

, xα −
1
t

c

D
x∗i −

x

E
A⊤y, xα −

−

x

E

xikxi −

c, x∗

x

.

i

−

α
t h

Xi=1
where we used Lemma D.1 on the second ﬁrst, Axα = Ax on the second inequality. Hence, we have

Xi=1

c, x
h

i
t ≤

h

c, x∗i
t

+

1

ν

−
1 for all i, we have

Using

µik∗xi ≤
k

c, x
h

i
t ≤

c, x∗i
h
t

+

1

ν

Setting α = 1
2 , we have
is always larger than 1.

c, x
h

i ≤ h

+

α

m

Xi=1

µik
k

∗xik

x∗i −

xikxi −

m

Xi=1

2
xi

xik
x∗i −
α
k
x∗i −
1 + α
k

xikxi

.

m

+

xikxi

x∗i −
k
x∗i −
1 + α
Xi=1
k
+ 2t(ν + m)

xikxi ≤
c, x∗i

≤ h

α

−
c, x∗i

h

c, x∗i
t

+

1

+

α

m
α

.

ν

−

+ 4tν because the self-concordance νi

E Basic Properties of Subsampled Hadamard Transform Matrix

This section provides some standard calculations about sketching matrices, it can be found in previ-
ous literatures [PSW17]. Usually, the reason for using subsampled randomized Hadamard/Fourier
transform [LDFU13] is multiplying the matrix with k vectors only takes kn log n time. Unfor-
tunately, in our application, the best way to optimize the running is using matrix multiplication
directly (without doing any fast Fourier transform [CT65], or more fancy sparse Fourier transform
[HIKP12b, HIKP12a, Pri13, IKP14, IK14, PS15, CKPS16, Kap16, Kap17, NSW19]). In order to
have an easy analysis, we still use subsampled randomized Hadamard/Fourier matrix.

E.1 Concentration inequalities

We ﬁrst state a useful for concentration,

Lemma E.1 (Lemma 1 on page 1325 of [LM00]). Let X
variable with k degrees of freedom. Each one has zero mean and σ2 variance. Then

2
k be a chi-squared distributed random

∼ X

Lemma E.2 (Khintchine’s Inequality). Let σ1,
z1,

, zn be real numbers. Then there are constants C, C ′ > 0 so that

· · ·

sign random variables, and let

· · ·

Pr[X
−
Pr[kσ2

kσ2

X

−

≥

≥

(2√kt + 2t)σ2]
2√ktσ2]

exp(

exp(

−

t)

≤

t)

−

≤
, σn be i.i.d.

Pr

n

"(cid:12)
Xi=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ziσi

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ct

z
k

k2

≥

# ≤

exp(

−

C ′t2)

52

Lemma E.3 (Bernstein Inequality). Let X1,
Suppose that

· · ·

M almost surely, for all i. Then, for all positive t,

, Xn be independent zero-mean random variables.

Xi| ≤
|

n

Xi > t

Pr

"

exp

# ≤

 −

n
j=1

t2/2
E[X 2

j ] + M t/3 !

Xi=1
E.2 Properties obtained by random projection

P

∈

Rb

×, where Σ is an n

Remark E.4. The Subsampled Randomized Hadamard Transform [LDFU13] can be deﬁned as
n diagonal matrix with i.i.d. diagonal entries Σi,i in which
R = SHnΣ
1 with probability 1/2. Hn refers to the Hadamard
Σi,i = 1 with probability 1/2, and Σi,i =
n matrix S samples b coordinates
matrix of size n, which we assume is a power of 2. The b
of n dimensional vector uniformly at random. If we replace the deﬁnition of sketching matrix in
Lemma E.5 by Subsampled Randomized Hadamard Transform and let R = SHn, then the same
proof will go through.

−

×

×

Lemma E.5 (Expectation, variance, absolute guarantees for sketching a ﬁxed vector). Let h
Rb
be a ﬁxed vector. Let R
×
+1/√b with probability 1/2 and
where each entry is 1 with probability 1/2 and

Rn
n denote a random matrix where each entry is i.i.d. sampled from
n denote a diagonal matrix
1 with probability 1/2. Let R = RΣ, then we have

1/√b with probability 1/2. Let Σ

Rn

−

∈

∈

∈

×

−

E[R⊤Rh] = h, E[(R⊤Rh)2
i ]

h2
i +

≤

1
b k

2
2, Pr
h
k

(R⊤Rh)i −
|

hi|

>

h
k2
k

(cid:20)

log(n/δ)
√b

δ.

≤

(cid:21)

Proof. Let Ri,j denote the entry at i-th row and j-th column in matrix R
denote the vector in i-th column of R.

∈

Rb

×

n. Let R

Rb

,i ∈
∗

We ﬁrst show expectation,

E[(R⊤Rh)i] = E[
,ih⊤
R, R
h
∗
n

b

]
i





Xj=1
b

Xl=1

Rj,lRj,ihl


b

= E

= E





Xj=1
= hi + 0
= hi

+ E

R2
j,ihi






Xj=1 Xl
i
[n]
\
∈

Rj,lRj,ihl


53

Secondly, we prove the variance is small

E[(R⊤Rhi)2] = E[
R, R
h

,ih⊤
∗

2]
i

b

n

2

= E

= E

Xl=1

Rj,lRj,ihl


b





R2

j,ihi +

Xj=1 Xl
i
[n]
\
∈
2









Xj=1
b









Xj=1
b

2

Rj,lRj,ihl


b




b

= E





R2
j,ihi


b

Xj=1




+ E





Xj=1 Xl
i
[n]
\
∈

= C1 + C2 + C3,



+ 2 E





R2

j′,ihi

Xj=1 Xl
i
[n]
\
∈

Rj,lRj,ihl


Xj′=1
2



Rj,lRj,ihl






where the last step follows from deﬁning those terms to be C1, C2 and C3. For the term C1, we have

C1 = h2
i

E





R2

j,i

b

Xj=1





2





= h2
i

E

b


Xj=1


R4

j,i +

Xj′
=j

R2

j,iR2

j′,i

1
b2 + b(b

b

·

1)

−

1
b2

·

= h2
i

(cid:18)

= h2
i

(cid:19)





For the second term C2,

C2 = 0.

For the third term C3,

b

2

C3 = E

= E

b

=





Xj=1 Xl
i
[n]
\
∈

b




Xj=1 Xl
i
[n]
\
∈

1
1
d
d

Xj=1 Xl
i
[n]
\
∈



Rj,lRj,ihl


+ E

j,ih2

j,lR2

l 

R2

h2
l + 0

≤


1
d k

2
h
2
k

b





Xj=1 Xl
i
[n]
\
∈

Rj,lRj,ihl

Rj′,l′Rj′,ihl′

j Xl′
Xj′
i
[n]
[b]
\
∈
\
∈

l
\





Therefore, we have

E[(R⊤Rh)2
i ]

C1 + C2 + C3 ≤

≤

h2
i +

1
b k

2
2.
h
k

Third, we prove the worst case bound with high probability. We can write (R⊤Rh)i −

hi as

54

6
follows

(R⊤Rh)i −

hi =

,ih⊤
R, R
h
∗
n

b

i −

hi

Rj,l ·

Rj,i ·

hl −

hi

Xl=1

Xj=1
b

R2
j,ihi −

hi +

Xj=1
b

b

Xj=1 Xl
i
[n]
\
∈

Rj,lRj,i ·

hl

hl

by R2

j,i = 1/b

Rj,lRj,i ·

,l, R
,ii
∗
∗

Xj=1 Xl
i
[n]
\
∈
R
hlh

i
[n]
Xl
\
∈

=

=

=

=

=

i
[n]
Xl
\
∈
First, we apply Khintchine’s inequality, we have

hl · h

σlR

,l, σiR
∗

,ii
∗

Pr 

(cid:12)
(cid:12)
i
[n]
Xl
(cid:12)
\
∈
(cid:12)
(cid:12)
(cid:12)
and choose t =
For each l




p

hl ·

R
σl · h

,l, σiR
∗

Ct

≥

,ii(cid:12)
∗
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)





i
[n]
Xl
\
∈

log(n/δ).

= i, using [LDFU13] we have

by R

,l = σlR
∗

,l

∗

h2
l (
R
h

,l, σiR
∗

)2

,ii
∗

1/2










exp(

−

≤

C ′t2)

Pr

R

"|h

,l, R
∗

∗

,ii| ≥ p

log(n/δ)
√b

# ≤

δ/n.

Taking a union bound over all l

i, we have
[n]
\

∈

(R⊤Rh)i −
|

h
hi| ≤ k
k2

log(n/δ)
√b

with probability 1

δ.

−

Acknowledgments

The authors would like to thank Haotian Jiang, Swati Padmanabhan, Ruoqi Shen, Zhengyu Wang,
Xin Yang and Peilin Zhong for useful discussions.

55

6
