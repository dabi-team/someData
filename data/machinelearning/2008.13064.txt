Towards Demystifying Dimensions of Source Code Embeddings

Md Rafiqul Islam Rabin
University of Houston
mrabin@central.uh.edu

Omprakash Gnawali
University of Houston
odgnawal@central.uh.edu

Arjun Mukherjee
University of Houston
amukher6@central.uh.edu

Mohammad Amin Alipour
University of Houston
maalipou@central.uh.edu

0
2
0
2

p
e
S
9
2

]

G
L
.
s
c
[

3
v
4
6
0
3
1
.
8
0
0
2
:
v
i
X
r
a

ABSTRACT
Source code representations are key in applying machine learning
techniques for processing and analyzing programs. A popular ap-
proach in representing source code is neural source code embeddings
that represents programs with high-dimensional vectors computed
by training deep neural networks on a large volume of programs.
Although successful, there is little known about the contents of
these vectors and their characteristics.

In this paper, we present our preliminary results towards better
understanding the contents of code2vec neural source code em-
beddings. In particular, in a small case study, we use the code2vec
embeddings to create binary SVM classifiers and compare their
performance with the handcrafted features. Our results suggest
that the handcrafted features can perform very close to the highly-
dimensional code2vec embeddings, and the information gains are
more evenly distributed in the code2vec embeddings compared to
the handcrafted features. We also find that the code2vec embeddings
are more resilient to the removal of dimensions with low informa-
tion gains than the handcrafted features. We hope our results serve
a stepping stone toward principled analysis and evaluation of these
code representations.

CCS CONCEPTS
• Computing methodologies → Learning latent representa-
tions; • Software and its engineering → General program-
ming languages.

KEYWORDS
Code Representation, Code Embeddings, Models of Code

ACM Reference Format:
Md Rafiqul Islam Rabin, Arjun Mukherjee, Omprakash Gnawali, and Mo-
hammad Amin Alipour. 2020. Towards Demystifying Dimensions of Source
Code Embeddings . In Proceedings of the 1st ACM SIGSOFT International
Workshop on Representation Learning for Software Engineering and Pro-
gram Languages (RL+SE&PL’20, Co-located with ESEC/FSE), 2020, Virtual,
USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/nnnnnnn.
nnnnnnn

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
RL+SE&PL’20, Co-located with ESEC/FSE, 2020, Virtual, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
The availability of a large number of mature source code reposi-
tories has fueled the growth of “Big Code” that attempts to devise
data-driven approaches in the analysis and reasoning of the pro-
grams [4, 15] by discovering and utilizing commonalities within
software artifacts. Such approaches have enabled a host of exciting
applications e.g., prediction of data types in dynamically typed lan-
guages [20], detection of the variable naming issues [5], or repair
of software defects [18].

Deep neural networks have accelerated innovations in Big Code
and have greatly enhanced the performance of prior traditional
approaches. The performance of deep neural networks in cognitive
tasks such as method name prediction [6] or variable naming [5]
has reached or exceeded the performance of other data-driven
approaches. The performance of neural networks has encouraged
researchers to increasingly adopt the neural networks in processing
source code.

Source code representation is the cornerstone of using neural
networks in processing programs. Numerous work on devising
representations for code in certain tasks [4, 15]. In such represen-
tations, the code is represented by a vector of numbers, called
embeddings, resulted from training on millions of lines of source
code or program traces. The current state of practice in devising
such representations includes decisions about the length of code
embeddings, code features included in learning, etc. The current
approach is highly empirical and tedious; moreover, the analysis
and evaluation of the source embeddings are nontrivial.

While there are an increasing number of work on the interpreta-
tion and analysis of neural networks for source code, e.g., [14], [33],
[28], and [32], to the best of our knowledge there is no work to look
at the internal of source code embeddings. In addition to facilitating
the interpretation of the behavior of neural models, understand-
ing the source code embeddings would enable researchers and
practitioners to optimize neural models, and potential can provide
methodologies to objectively compare different representations.

In this work, we report our initial attempts for demystifying
the dimensions of source code embeddings, which is aimed at a
better understanding of the embedding vectors by analyzing their
values and comparing them with understandable features. In partic-
ular, we report the results of our preliminary analysis of code2vec
embeddings [11], a popular code representation for method name
prediction task. More specifically, we use the code2vec embeddings
to build SVM models and compare them with SVM models trained
on the handcrafted features. We analyze the statistical characteris-
tics of the dimensions in the embeddings.

 
 
 
 
 
 
RL+SE&PL’20, Co-located with ESEC/FSE, 2020, Virtual, USA

MRI Rabin, A Mukherjee, O Gnawali, MA Alipour

Our results suggest that the handcrafted features can perform
very close to the highly-dimensional code2vec embeddings, and
the information gains are more evenly distributed in the code2vec
embeddings compared to the handcrafted features. We also find
that the code2vec embeddings are more resilient to the removal
of dimensions with low information gains than the handcrafted
features.
Contributions. This paper makes the following contributions.

• It provides an in-depth analysis of dimensions in code2vec
source code embeddings in a small number of methods.
• It compares the performance of handcrafted features with

naive representations and code2vec embeddings.

2 BACKGROUND
The code2vec [11] source code representation uses bags of paths in
the abstract syntax tree (AST) of programs to represent programs.
The model encodes the AST path between leaf nodes and uses an
attention mechanism to compute a learned weighted average of
the path vectors in order to produce a single code vector of 384
dimensions for each program.

The code2vec [11] was initially introduced to predict the name of
method [6], given the method’s body. Figure 1 depicts an example
of this task wherein a neural model based on code2vec correctly
predicts the name of the method in the Figure as swap.

Table 1: Top-Ten method name and feature list.

Name of Method Feature List

equals
main
setUp
onCreate
toString
run
hashCode
init
execute
get

Instance, Boolean, equals, This
Println, String
Super, setup, New, build, add
Bundle, onCreate, setContentView, R
toString, format, StringBuilder, append, +
Handler, error, message
hashCode, TernaryOperator
init, set, create
CommandLine, execute, response
Return, get

Table 2: Additional code complexity features.

LOC, Block, Basic Block, Parameter, Local Variable,
Global Variable, Loop, Jump, Decision, Condition,
Instance, Function, TryCatch, Thread

GitHub. Overall, it contains about 16M methods where almost 3.5M
methods have a unique name. We chose ten most-frequent method
names in the Java-Large dataset and the corresponding method
bodies to create a new dataset, Top-Ten, for further analysis.

The reason for restricting our analysis to these methods is
twofold. First, the sheer number of method names in Java-Large
prohibits a scalable manual inspection and analysis for all methods.
Second, the distribution of method names in Java-Large conforms
to the power-law; that is, relatively few method names appear
frequently in the dataset while the rest of method names appear
rarely in the dataset. Therefore, the performance of any classifier
on Java-Large heavily relies on its performance on the few
frequent method names. Column “Name of Method” in Table 1 lists
the names of ten most-frequent methods that we chose for our
analysis.
Deduplication of the Top-Ten dataset. As noted by [1], the
Java-Large dataset suffers from duplicate methods that can inflate
the results of the prediction. We removed duplicate methods in
the dataset following the steps outlined in [1] and used the same
parameters for deduplication thresholds: key-jaccard-threshold,
t0 = 0.8 and jaccard-threshold, t1 = 0.7.
Dataset for each Top-Ten method. For each method M in Top-
Ten, we create a training set that constitutes from 1000 randomly
selected positive examples (methods with name M), and 1000 ran-
domly selected negative examples (any method but M) from the
deduplicated Top-Ten training set. For the validation set and test
set, we select all the positive examples and the same number of ran-
domly selected negative examples from the deduplicated Top-Ten
validation set and test set, respectively. Table 3 shows the size of
the dataset for each Top-Ten method.

Figure 1: An example of method name prediction by
code2vec[11].

3 METHODOLOGY
To evaluate the code2vec code representation we follow the work-
flow in Figure 2. We first select a few methods in which we are
interested in the analysis of their representations. We then man-
ually select features that best can predict their names. Next, we
create binary classifiers for predicting the name of those methods
with code2vec embeddings and handcrafted features. Finally, we
evaluate and compare the performance of the trained classification
models. In the rest of this section, we will describe dataset and
selection of methods, feature extraction, classifier creation, baseline
classifiers, and evaluation metrics.

3.1 Dataset and Method Selection
Top-Ten dataset. We use the Java-Large dataset [9] that contains
9K Java projects in the training set, 200 Java projects in the valida-
tion set, and 300 Java projects in the test set that were collected from

Towards Demystifying Dimensions of Source Code Embeddings

RL+SE&PL’20, Co-located with ESEC/FSE, 2020, Virtual, USA

Figure 2: Workflow in this study.

Table 3: Size of the dataset for each Top-Ten method.

Name of Method #Training

#Validation #Test

equals
main
setUp
onCreate
toString
run
hashCode
init
execute
get

2000
2000
2000
2000
2000
2000
2000
2000
2000
2000

1212
1220
1220
1876
586
876
534
892
498
780

1778
2032
1424
1484
1278
1558
770
2504
702
670

3.2 Extracting Handcrafted Features
Method-only features. For each method, two authors do their
best effort to draw discriminant features by inspecting the training
dataset. Table 1 shows the handcrafted features for each method,
in total, 33 features for ten methods.
Code complexity features. An important metric of interest might
be adding code complexity features. Similar methods may have cer-
tain patterns such as the number of lines, variables, or conditions.
Therefore, we further extend the handcrafted features with an ad-
ditional 14 code complexity features shown in Table 2. Thus, the
handcrafted features become a union of 47 features including the
code complexity features. Note that we only focus on the simpler
code complexity features shown in Table 2 as our study is lim-
ited to the methods, and thus the class level or project level code
complexity metrics do not apply to our study.
Feature Extraction. We use the JavaParser [37] tool to parse
the methods in the dataset and extract the handcrafted features.
We consider the 33 handcrafted features of methods (Table 1) as
(a) binary vectors, and (b) numeric vectors. For the binary vectors,
we use 1 and 0 that denote the presence or absence of individual
features in the method, respectively. For the numeric vectors, we
count the number of occurrences of features in the program, and in
the end, we normalize them using StandardScaler [31] to map the

distribution of values to a mean value of 0 and a standard deviation
of 1. The 14 complexity features (Table 2) are always considered as
numeric values.

3.3 Classification Models
Support Vector Machines. Support Vector Machines (SVM) are
one of the most popular traditional supervised learning algorithms
that can be used for classification and regression on linear and non-
linear data [13, 17, 24]. SVM uses the concept of linear discriminant
and maximum margin to classify between classes. Given the labeled
training data points, SVM learns a decision boundary to separate
the positive points from the negative points. The decision boundary
is also known as the maximum margin separating hyperplane that
maximizes the distance to the nearest data points of each class. The
decision boundary can be a straight line classifying linear data in
a two-dimensional space (i.e. linear SVM using linear kernel) or
can be a hyperplane classifying non-linear data by mapping into a
higher-dimensional space (i.e. non-linear SVM using RBF kernel).
Classifiers. For each method M, we create two SVM classification
models: SVM-Handcrafted and SVM-code2vec. SVM-code2vec
uses the code2vec embeddings of the programs in training the SVM
model, which is a single fixed-length code embedding (384 dimen-
sions) that represents the source code as continuous distributed
vectors for predicting method names. SVM-Handcrafted uses the
vector of the handcrafted features (33 dimensions without complex-
ity features, and 47 dimensions with complexity features) to train
an SVM model.
Training We use the SV Ml iдht 1, an implementation of Support
Vector Machines (SVMs) in C [27], to train the classification models
in the experiments.

Since the performance of SVM depends on its hyper-parameters,
we run the grid search algorithm [12] for hyper-parameter opti-
mization. We train SVMs with tuned parameters on handcrafted
features and code2vec embeddings for each method name.

3.4 Naive Sequence-based Neural Baselines
We also create two sequence-based baselines to compare our hand-
crafted features: (a) CharSeq where the program is represented by

1http://svmlight.joachims.org/

RL+SE&PL’20, Co-located with ESEC/FSE, 2020, Virtual, USA

MRI Rabin, A Mukherjee, O Gnawali, MA Alipour

a sequence of characters in the program, and (b) TokenSeq where
a sequence of tokens in the program represent the program.
CharSeq. For character-based representation, we first remove
comments from the body of the method and save the body as a
plain string. Then we create a list of ASCII 2 characters by filtering
out all non-ASCII characters from the string of body. After that,
we create a character-based vocabulary with the unique ASCII
characters found in the training+validation set of the Top-Ten
dataset (the character-based vocabulary stores 94 unique ASCII
characters). Finally, we encode the method body by representing
each character with its index in the character-based vocabulary.
TokenSeq. For token-based representation, we modify the JavaTo-
kenizer tool [1] to get the sequence of Java tokens from the body of
the method. After that, we create a token-based vocabulary with the
unique tokens found in the training+validation set of the Top-Ten
dataset (the token-based vocabulary stores 108106 unique tokens).
Finally, we encode the method body by representing each token
with its index in the token-based vocabulary.
Training Naive Models. We train 2-layer bi-directional GRUs [16]
with PyTorch 3 on character-based representation (CharSeq) and
token-based representation (TokenSeq) for predicting the method
name. The classifier on CharSeq and TokenSeq are referred to as
GRU-CharSeq and GRU-TokenSeq, respectively.

3.5 Evaluation Metrics
We use the following metrics as commonly used in the literature [5,
11] to evaluate the performance of handcrafted features. Suppose, tp
denotes the number of true positives, tn denotes the number of true
negatives, f p denotes the number of false positives, and f n denotes
the number of false negatives in the results of the classification of
a method on the test data.

Accuracy indicates how many predicted examples are correct. It
is the ratio of the correctly predicted examples to the total examples
of the class.

Accuracy =

tp + tn
tp + tn + f p + f n

Precision indicates how many predicted examples are true posi-
tives. It is the ratio of the correctly predicted positive examples to
the total predicted positive examples.

Precision =

tp
tp + f p

Recall indicates how many true positives examples are correctly
predicted. It is the ratio of the correctly predicted positive examples
to the total examples of the class.

Recall =

tp
tp + f n

F1-Score is the harmonic mean of precision (P) and recall (R).

F1–Score =

2
P −1 + R−1

= 2 .

P . R
P + R

2https://en.wikipedia.org/wiki/ASCII (character code 0-127 in ASCII-table)
3https://pytorch.org/docs/stable/generated/torch.nn.GRU.html

Table 4: Result of handcrafted features on Top-Ten dataset.

Method

Feature Vectors

Precision Recall

equals

main

setUp

onCreate

toString

run

hashCode

init

execute

get

HC(Binary)
HC(Norm)
HC(Binary)+CX(Norm)
HC(Norm)+CX(Norm)

HC(Binary)
HC(Norm)
HC(Binary)+CX(Norm)
HC(Norm)+CX(Norm)

HC(Binary)
HC(Norm)
HC(Binary)+CX(Norm)
HC(Norm)+CX(Norm)

HC(Binary)
HC(Norm)
HC(Binary)+CX(Norm)
HC(Norm)+CX(Norm)

HC(Binary)
HC(Norm)
HC(Binary)+CX(Norm)
HC(Norm)+CX(Norm)

HC(Binary)
HC(Norm)
HC(Binary)+CX(Norm)
HC(Norm)+CX(Norm)

HC(Binary)
HC(Norm)
HC(Binary)+CX(Norm)
HC(Norm)+CX(Norm)

HC(Binary)
HC(Norm)
HC(Binary)+CX(Norm)
HC(Norm)+CX(Norm)

HC(Binary)
HC(Norm)
HC(Binary)+CX(Norm)
HC(Norm)+CX(Norm)

HC(Binary)
HC(Norm)
HC(Binary)+CX(Norm)
HC(Norm)+CX(Norm)

98.54
98.20
99.21
98.99

94.62
91.70
94.72
91.04

87.70
78.90
90.26
87.53

100.00
100.00
99.86
100.00

93.41
93.56
95.57
94.81

62.03
60.51
69.24
69.55

97.06
96.85
98.95
98.19

74.73
73.55
77.72
75.43

76.25
63.60
80.67
76.36

86.76
84.96
89.89
88.54

98.88
97.98
98.54
98.76

96.85
94.59
97.15
94.98

86.10
90.87
93.68
92.70

92.99
92.86
93.13
92.45

97.65
95.46
94.52
94.37

61.87
75.74
66.75
70.09

94.29
95.84
97.92
98.44

94.25
92.17
90.58
91.69

86.89
94.59
82.05
92.02

95.82
91.04
95.52
92.24

F1-Score
98.71
98.09
98.87
98.87

95.72
93.12
95.92
92.97

86.89
84.46
91.94
90.04

96.37
96.30
96.38
96.08

95.48
94.50
95.04
94.59

61.95
67.27
67.97
69.82

95.65
96.34
98.43
98.31

83.36
81.81
83.66
82.77

81.22
76.06
81.35
83.46

91.07
87.89
92.62
90.35

4 RESULTS
In this section, we will describe the experimental results including
choice of handcrafted features, comparison of classifiers, and visu-
alization. Each classifier is trained on the corresponding training
set, tuned on the validation set, and later evaluated on a sepa-
rate test set. In this section, the classifiers on CharSeq, TokenSeq,
HC(Binary)+CX(Norm), and code2vec feature vectors are referred
to as GRU-CharSeq, GRU-TokenSeq, SVM-Handcrafted, and
SVM-code2vec, respectively.

Towards Demystifying Dimensions of Source Code Embeddings

RL+SE&PL’20, Co-located with ESEC/FSE, 2020, Virtual, USA

Table 5: Type of feature vectors.

Feature Vectors

Definition

CharSeq

TokenSeq

HC(Binary)

HC(Norm)

A sequence of ASCII characters represented
by its index in a character-based vocabulary.
A sequence of Java tokens represented by its
index in a token-based vocabulary.
The 33 handcrafted features of methods as
binary vectors.
The 33 handcrafted features of methods as
numeric vectors.

HC(Binary)+CX(Norm) HC(Binary) with the additional 14 complexity

features as numeric vectors.

HC(Norm)+CX(Norm) HC(Norm) with the additional 14 complexity

code2vec

features as numeric vectors.
The code vectors of 384 dimensions from
code2vec model [11].

Table 6: Average results on the Top-Ten dataset.

Feature Vectors

Accuracy Precision Recall

CharSeq
TokenSeq
HC(Binary)
HC(Norm)
HC(Binary)+CX(Norm)
HC(Norm)+CX(Norm)
code2vec

38.65
70.58
88.32
86.27
90.14
89.36
93.73

26.02
60.38
87.11
84.18
89.61
88.04
95.54

38.65
70.59
90.56
92.11
90.98
91.77
91.38

F1-Score
30.57
63.37
88.64
87.58
90.22
89.73
93.24

HC(Norm)+CX(Norm). This can suggest that only the presence of
features can be used to recognize a method, instead of counting the
number of occurrences of features.

(cid:27)

(cid:24)

Observation 1: The presence of a feature can be used to
recognize a method instead of counting the number of occur-
rences of that feature in programs. On average, the choice
of binary vectors has increased the F1-Score up to 1% than
the numeric vectors.

(cid:26)

(cid:25)

Impact of Additional Complexity Features. Figure 4 de-
4.1.2
picts the importance of complexity features on the quality of
handcrafted features. We compare method-only features and
method+complexity features on (a) binary vectors: HC(Binary)
vs. HC(Binary)+CX(Norm), and (b) numeric vectors: HC(Norm)
vs. HC(Norm)+CX(Norm). In Figure 4(a) and 4(b), the blue line
and orange line shows the F1-Score of method-only features and
method+complexity features, respectively. According to Figure 4(a),
in most cases, the HC(Binary)+CX(Norm) are comparatively better
than the HC(Binary) except for the ‘toString’ method where the dif-
ference is 0.44%. Similarly, in most cases, the HC(Norm)+CX(Norm)
are comparatively better than the HC(Norm) in Figure 4(b) except
for the ‘main’ and ‘onCreate’ methods where the difference are
0.15% and 0.22%, respectively. The average F1-Score of Table 6
also shows that the HC(Binary)+CX(Norm) is almost 1.6% better
than the HC(Binary) and the HC(Norm)+CX(Norm) is almost
2.2% better than the HC(Norm). This can suggest that the code
complexity features can be useful to better recognize a method,
especially for some methods (i.e. ‘setUp’, ‘run’, ‘hashCode’, and
‘execute’) where the improvements for additional code complexity
features are almost 3 ∼ 7%.

(cid:27)

(cid:24)

4.1 Choice of Handcrafted Features
Table 4 shows the detailed result of handcrafted features on the
Top-Ten dataset where the bold values represent the best results
and the underlined values represent the second-best result. In this
table, “HC” stands for handcrafted features. “HC(Binary)” and
“HC(Norm)” denote the handcrafted features as binary vectors and
numeric vectors, respectively. Similarly, “CX(Norm)” is to indicate
the additional complexity features as numeric vectors.

4.1.1 Binary vectors vs. Numeric vectors. Figure 3 depicts how the
choice of presence (binary vectors) or number of occurrences (nu-
meric vectors) influences the quality of handcrafted features. We
compare binary vectors and numeric vectors on (a) method-only
features: HC(Binary) vs. HC(Norm), and (b) method+complexity
features: HC(Binary)+CX(Norm) vs. HC(Norm)+CX(Norm). In
Figure 3(a) and 3(b), the blue line shows the F1-Score when the
features are considered as binary vectors and the orange line shows
the F1-Score when the features are considered as numeric vec-
tors. According to Figure 3(a), in most cases, the HC(Binary) are
comparatively better than the HC(Norm) except for the ‘run’ and
‘hashCode’ methods where the difference are 5.32% and 0.69%, re-
spectively. Similarly, in most cases, the HC(Binary)+CX(Norm) are
comparatively better than the HC(Norm)+CX(Norm) in Figure 3(b)
except for the ‘run’ and ‘execute’ methods where the difference are
1.85% and 2.11%, respectively. The average F1-Score of Table 6 also
shows that the HC(Binary) is almost 1% better than the HC(Norm)
and the HC(Binary)+CX(Norm) is almost 0.5% better than the

Observation 2: The code complexity features can be useful
to better recognize a method along with the method-only
features. On average, the additional code complexity features
have increased the F1-Score up to 2.2% than the method-only
features.

(cid:26)

(cid:25)

4.2 Comparison of Classifiers
Table 7 shows the detailed result of different feature vectors on the
Top-Ten dataset where the bold values represent the best results
and the underlined values represent the second-best result. We also
draw the commonly used explanatory data plots (barplots over
method names in Figure 5a and boxplots over feature vectors in
Figure 5b) to visually show the distribution of results on the Top-
Ten dataset. As shown in the previous section (Section 4.1), in
most cases, the binary vectors perform relatively better than the
numeric vectors, and the code complexity features also improve the
performance for handcrafted features. Therefore, in this section,
we mainly compare the result of HC(Binary)+CX(Norm) from
handcrafted features.

SVM-Handcrafted vs.

Sequence-based Baselines. In
4.2.1
this section, we compare our handcrafted features against the
following two sequence-based baselines:
(a) a sequence of
ASCII characters (CharSeq), and (b) a sequence of Java tokens
(TokenSeq). According to Table 7 and Figure 5a, for all methods,
our SVM-Handcrafted outperforms both GRU-CharSeq and
GRU-TokenSeq by a large margin for predicting method name.

RL+SE&PL’20, Co-located with ESEC/FSE, 2020, Virtual, USA

MRI Rabin, A Mukherjee, O Gnawali, MA Alipour

(a) Method-only features.

(b) Method+Complexity features.

Figure 3: Binary vectors vs. Numeric vectors.

(a) Binary vectors.

(b) Numeric vectors.

Figure 4: Impact of additional complexity features.

(a) Barplots over method names.

(b) Boxplots over feature vectors.

Figure 5: Comparison of classifiers on Top-Ten dataset.

Towards Demystifying Dimensions of Source Code Embeddings

RL+SE&PL’20, Co-located with ESEC/FSE, 2020, Virtual, USA

Table 7: Result of different feature vectors on Top-Ten
dataset.

Method

Feature Vectors

Precision Recall

equals

main

setUp

onCreate

toString

run

hashCode

init

execute

get

CharSeq
TokenSeq
HC(Binary)+CX(Norm)
code2vec

CharSeq
TokenSeq
HC(Binary)+CX(Norm)
code2vec

CharSeq
TokenSeq
HC(Binary)+CX(Norm)
code2vec

CharSeq
TokenSeq
HC(Binary)+CX(Norm)
code2vec

CharSeq
TokenSeq
HC(Binary)+CX(Norm)
code2vec

CharSeq
TokenSeq
HC(Binary)+CX(Norm)
code2vec

CharSeq
TokenSeq
HC(Binary)+CX(Norm)
code2vec

CharSeq
TokenSeq
HC(Binary)+CX(Norm)
code2vec

CharSeq
TokenSeq
HC(Binary)+CX(Norm)
code2vec

CharSeq
TokenSeq
HC(Binary)+CX(Norm)
code2vec

50.97
99.20
99.21
99.55

0.00
84.38
94.72
98.72

26.12
42.93
90.26
99.26

59.89
94.70
99.86
100.00

51.64
85.14
95.57
97.37

25.36
37.96
69.24
86.30

30.18
74.70
98.95
99.74

0.00
0.00
77.72
88.74

2.44
41.04
80.67
93.44

13.55
43.77
89.89
92.33

74.02
97.53
98.54
99.10

0.00
65.94
97.15
98.52

59.83
89.19
93.68
94.10

87.74
91.51
93.13
99.06

74.02
88.73
94.52
98.44

27.47
51.99
66.75
62.26

52.99
97.40
97.92
99.74

0.00
0.00
90.58
87.54

0.28
31.34
82.05
85.19

10.15
92.24
95.52
89.85

F1-Score
60.37
98.36
98.87
99.32

0.00
74.03
95.92
98.62

36.36
57.96
91.94
96.61

71.19
93.08
96.38
99.53

60.84
86.90
95.04
97.90

26.37
43.88
67.97
72.33

38.45
84.55
98.43
99.74

0.00
0.00
83.66
88.14

0.51
35.54
81.35
89.12

11.60
59.37
92.62
91.07

Even in some cases, GRU-CharSeq (i.e. main, init, and execute)
and GRU-TokenSeq (i.e. init) fail to predict the method name.
The boxplots in Figure 5b indicates that the variance of F1-Scores
among methods are also very significant for GRU-CharSeq and
GRU-TokenSeq. The average F1-Score of Table 6 also shows that
the SVM-Handcrafted is 59.65% and 26.85% better than the
GRU-CharSeq and the GRU-TokenSeq, respectively.

(cid:15)

(cid:12)

Observation 3: The handcrafted features significantly out-
perform the sequence of characters (by 59.65%) and the se-
quence of tokens (by 26.85%) for predicting method name.

(cid:14)

(cid:13)

Figure 6: The distribution of information gain for ‘equals’
method.

SVM-Handcrafted vs. SVM-code2vec. In this section, we
4.2.2
compare our handcrafted features with the path-based embedding
of code2vec [11]. According to Table 7 and Figure 5, the SVM-
code2vec performs better than the SVM-Handcrafted but the
difference is not always significant. When the F1-Score of SVM-
code2vec is near perfect (i.e., equals, onCreate, and hashCode), the
F1-Score of SVM-Handcrafted is also higher and very close to the
SVM-code2vec. Similarly, for some other methods (i.e., run, init,
and execute), they both perform relatively worst. However, there are
some cases where the difference between SVM-code2vec and SVM-
Handcrafted is significant, for example, SVM-code2vec shows
almost 8% improvement over SVM-Handcrafted to classify the
‘execute’ method. On the other hand, SVM-Handcrafted is 1.5+%
better than SVM-code2vec to classify the ‘get’ method. The average
F1-Score also shows that the SVM-code2vec obtains around 3%
improvements over the SVM-Handcrafted. This can suggest that
the handcrafted features with a very smaller feature set can achieve
highly comparable results to the higher dimensional embeddings
of deep neural model such as code2vec.

(cid:23)

(cid:20)

Observation 4: The handcrafted features with a very
smaller feature set can achieve highly comparable results to
the higher dimensional embeddings of deep neural model
such as code2vec.

(cid:22)

(cid:21)

4.3 Information Gains and Importance of

Dimensions

Figure 6 depicts the distribution of information gains of each di-
mension, i.e., feature, in the ‘equals’ dataset. It suggests that the
information gains of features in code2vec embeddings is on average
higher than the information gains of features in handcrafted fea-
tures. However, the distribution of information gains in code2vec
embeddings is symmetric while is highly skewed in handcrafted
features.

We used the information gains and created new SVM models
for methods such as ‘main’ and ‘setUp’ by using features with
top 25% of information gains. The F1-score of SVM models for

RL+SE&PL’20, Co-located with ESEC/FSE, 2020, Virtual, USA

MRI Rabin, A Mukherjee, O Gnawali, MA Alipour

binary handcrafted features (HC(Binary)) with features of top 25%
information gains were 93.5% and 80.11%, for ‘main’ and ‘setUp’,
respectively, while these value for top 25% code2vec dimensions
were 98.62% and 96.28%, respectively. It shows that the handcrafted
features suffered a higher loss of performance than their code2vec
embeddings counterparts. It may suggest that a large portion of
code2vec embeddings might be unnecessary for the acceptable
classification, hence, the size of embedding can be reduced.

(cid:27)

(cid:24)

Observation 5: Compare to the handcrafted features, the in-
formation gains are more evenly distributed in the code2vec
embeddings. Moreover, the code2vec embeddings are more
resilient to the removal of dimensions with low information
gains than the handcrafted features.

(cid:26)

(cid:25)

4.4 Visualization of Feature Vectors
To better understand how the features separate the positive and
negative examples in the dataset we used t-SNE [30] to project the
feature vectors in code2vec embeddings and handcrafted features
into two-dimensional space. For illustration, we only visualize the
method with best performing classifiers (i.e. ‘equals’) in Figure 7
and the method with worst performance classifiers (i.e. ‘run’) in Fig-
ure 8, for the code2vec, HC(Binary) and HC(Binary)+CX(Norm),
respectively. Points from the same color (positive examples are in
green color and negative examples are in red color) should tend to
be grouped close to one another.
‘equals’ method. Figure 7 indicates that the data points are gener-
ally well grouped for the best method (‘equals’) where the positive
points are quite distinct from the negative points. The data points
form a cluster of positive points in the middle of Figure 7a and
are almost linearly separable in Figure 7b and 7c. All show a good
measure of separability as the F1-Scores are nearly 100%.
‘run’ method. Figure 8 indicates that the data points are hardly
separable. The F1-Score of Figure 8a is around 10% higher than
Figure 8b, thus the data points appear more scattered in Figure 8b
than in Figure 8a. Similarly, the F1-Score of Figure 8c is around 6%
higher than Figure 8b, thus the data points in Figure 8c are relatively
less scattered than in Figure 8b.

Although t-SNE plots are not objective ways to compare two
embeddings, it may provide an intuition about the separability
of methods based on the corresponding feature embeddings. The
figures might suggest that the high-dimensional code2vec tends to
produce a more complex hypothesis class than necessary, compared
to the handcrafted features. Using too complex hypothesis class
may increase the chances of overfitting in training the models.

5 RELATED WORK
Many studies have been done on the representation of source code
[4, 15] in machine learning models for predicting properties of pro-
grams such as identifier or variable names [2, 5, 10, 35], method
names [3, 6, 9–11, 19, 40], class names [3], types [10, 21, 35], and
descriptions [9, 19]. Allamanis et al. [2] introduced a framework
that processed token sequences and abstract syntax trees of code
to suggest natural identifier names and formatting conventions
on a Java corpus. Allamanis et al. [3] proposed a neural proba-
bilistic language model with manually designed features from Java
projects for suggesting method names and class names. Raychev

et al. [35] converted the program into dependency representation
that captured relationships between program elements and trained
a CRF model for predicting the name of identifiers and predict-
ing the type annotation of variables in JavaScript dataset. Allama-
nis et al. [6] introduced a convolutional attention model for the
code summarization task such as method name prediction with
a sequence of subtokens from Java projects. Alon et al. [10] used
the AST-based representation for learning properties of Java pro-
grams such as predicting variable names, predicting method names,
and predicting full types. Allamanis et al. [5] constructed graphs
from source code that leveraged data flow and control flow for
predicting variable names and detecting variable misuses in C#
projects. Hellendoorn et al. [21] proposed a RNN-based model us-
ing sequence-to-sequence type annotations for type suggestion in
TypeScript and plain JavaScript code. Fernandes et al. [19] com-
bined sequence encoders with graph neural networks that inferred
relations among program elements for predicting name and descrip-
tion of the method in Java and C# projects. Alon et al. [11] used
a bag of path-context from abstract syntax tree to learn the body
of method for predicting the method name of Java projects. Alon
et al. [9] later used an encoder-decoder architecture to encode the
path-context as node-by-node to predict the method name of Java
projects and the code caption of C# projects. Liu et al. [29] used
similar method bodies to spot and refactor inconsistent method
names. Wang and Su [40] embedded the symbolic and concrete
execution traces of Java projects to learn program representations
for method name prediction and semantics classification.

Apart from that, various deep neural embeddings and models
have been also applied to different program analysis or software
engineering tasks such as HAGGIS for mining idioms from source
code [8], Gemini for binary code similarity detection [43], Code
Vectors for code analogies, bug fining and repair/suggestion [22],
Dynamic Program Embeddings for classifying the types of errors
in programs [41], DYPRO for recognizing loop invariants [39], Im-
port2Vec for learning embeddings of software libraries [38], NeurSA
for catching static bugs in code [42], and HOPPITY to detect and fix
bugs in programs [18]. Researchers have also studied the language
model for code completion [23, 34, 36], code suggestion [7], and
code retrieval [25] task.

Moreover, Allamanis et al. [4] survey the taxonomy of probabilis-
tic models of source code and their applications, Jiang et al. [26]
conduct an empirical study on where and why machine learning-
based automated recommendations for method names do work
or do not work, and Chen and Monperrus [15] provide a more
comprehensive survey that includes embeddings based on different
granularities of source code such as tokens, functions or methods,
sequences or method calls, binary code, and other for source code
embeddings.

6 THREATS TO VALIDITY
We have performed a limited exploratory analysis on the ten most
frequent methods in the dataset. Therefore, the results should be
interpreted in the confinement of the limits of our experiment. The
results of SVM-Handcrafted depend on the features that we have
extracted. Despite our best effort, it is possible that the handcrafted
features can be further improved. Moreover, we only analyzed the

Towards Demystifying Dimensions of Source Code Embeddings

RL+SE&PL’20, Co-located with ESEC/FSE, 2020, Virtual, USA

(a) code2vec [F1 = 99.32%]

(b) HC(Binary) [F1 = 98.71%]

(c) HC(Binary)+CX(Norm) [F1 = 98.87%]

Figure 7: The t-SNE plot of the best ‘equals’ method.

(a) code2vec [F1 = 72.33%]

(b) HC(Binary) [F1 = 61.95%]

(c) HC(Binary)+CX(Norm) [F1 = 67.97%]

Figure 8: The t-SNE plot of the worst ‘run’ method.

ten most frequent method names. Therefore, our methodology may
not generalize on different methods unless we include discriminant
features for them. It is possible that experiments on other methods
may produce different results.

7 DISCUSSIONS AND CONCLUSION
The code2vec embeddings are highly-dimensional and are the re-
sults of training over millions of lines of code. Therefore, it is non-
trivial to identify the impacts, if any, of each dimension in storing
semantic or syntactic characteristics of a program. Although we
really did not understand the actual meaning of each dimension
of the code2vec source code embeddings, our results suggest that
few handcrafted features could perform very similar to the highly-
dimensional code2vec embeddings in our experiments. Compare
to the handcrafted features, the information gains are more evenly
distributed in the code2vec embeddings. Moreover, the code2vec

embeddings are more resilient to the removal of dimensions with
low information gains than the handcrafted features.

In this work, we described our preliminary study to demystify
the source code embeddings through a comparison of the code2vec
embeddings with the handcrafted features. Although preliminary,
this work provides some insights into how the features contribute
to the classification task at hand. We hope that this paper helps us
to design a practical framework to objectively analyze and evaluate
dimensions in the source code embeddings. Our source code to
extract Top-Ten handcrafted features and train SV Mliдht models
for method name classification is available at https://github.com/
mdrafiqulrabin/handcrafted-embeddings.

REFERENCES
[1] Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine
learning models of code. In Proceedings of the 2019 ACM SIGPLAN International
Symposium on New Ideas, New Paradigms, and Reflections on Programming and

RL+SE&PL’20, Co-located with ESEC/FSE, 2020, Virtual, USA

MRI Rabin, A Mukherjee, O Gnawali, MA Alipour

Software. 143–153.

[2] Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. 2014.
Learning Natural Coding Conventions. In Proceedings of the 22nd ACM SIG-
SOFT International Symposium on Foundations of Software Engineering (FSE
2014). Association for Computing Machinery, New York, NY, USA, 281âĂŞ293.
https://doi.org/10.1145/2635868.2635883

[3] Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. 2015. Sug-
gesting Accurate Method and Class Names (ESEC/FSE 2015). Association for
Computing Machinery, New York, NY, USA, 38âĂŞ49. https://doi.org/10.1145/
2786805.2786849

[4] Miltiadis Allamanis, Earl T. Barr, Premkumar Devanbu, and Charles Sutton. 2018.
A Survey of Machine Learning for Big Code and Naturalness. ACM Comput. Surv.
51, 4, Article Article 81 (July 2018), 37 pages. https://doi.org/10.1145/3212695

[5] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning
to Represent Programs with Graphs. In International Conference on Learning
Representations. https://openreview.net/forum?id=BJOFETxR-

[6] Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A convolutional at-
tention network for extreme summarization of source code. In International
conference on machine learning. 2091–2100.

[7] Miltiadis Allamanis and Charles Sutton. 2013. Mining Source Code Repositories
at Massive Scale Using Language Modeling. In Proceedings of the 10th Working
Conference on Mining Software Repositories (MSR ’13). IEEE Press, 207âĂŞ216.
[8] Miltiadis Allamanis and Charles Sutton. 2014. Mining idioms from source code.
Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of
Software Engineering - FSE 2014 (2014). https://doi.org/10.1145/2635868.2635901
[9] Uri Alon, Omer Levy, and Eran Yahav. 2019. code2seq: Generating Sequences
from Structured Representations of Code. In International Conference on Learning
Representations. https://openreview.net/forum?id=H1gKYo09tX

[10] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018. A General
Path-Based Representation for Predicting Program Properties. In Proceedings
of the 39th ACM SIGPLAN Conference on Programming Language Design and
Implementation (PLDI 2018). Association for Computing Machinery, New York,
NY, USA, 404âĂŞ419. https://doi.org/10.1145/3192366.3192412

[11] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learn-
ing Distributed Representations of Code. Proc. ACM Program. Lang. 3, POPL,
Article 40 (Jan. 2019), 29 pages. https://doi.org/10.1145/3290353

[12] JF Bard. 1982. A grid search algorithm for the linear bilevel programming problem.
In Proceedings of the 14th Annual Meeting of the American Institute for Decision
Science. 256–258.

[13] Asa Ben-Hur and Jason Weston. 2010. A User’s Guide to Support Vector Machines.
Humana Press, Totowa, NJ, 223–239. https://doi.org/10.1007/978-1-60327-241-
4_13

[14] N. D. Q. Bui, Y. Yu, and L. Jiang. 2019. AutoFocus: Interpreting Attention-Based
Neural Networks by Code Perturbation. In 2019 34th IEEE/ACM International
Conference on Automated Software Engineering (ASE). 38–41. https://doi.org/10.
1109/ASE.2019.00014

[15] Zimin Chen and Martin Monperrus. 2019. A literature study of embeddings on

source code. arXiv preprint arXiv:1904.03061 (2019).

[16] Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase
Representations using RNN Encoder–Decoder for Statistical Machine Translation.
In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP). Association for Computational Linguistics, Doha, Qatar,
1724–1734. https://doi.org/10.3115/v1/D14-1179

[17] Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine

learning 20, 3 (1995), 273–297.

[18] Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang.
2020. Hoppity: Learning Graph Transformations to Detect and Fix Bugs in
Programs. In International Conference on Learning Representations.
https://
openreview.net/forum?id=SJeqs6EFvB

[19] Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Struc-
tured Neural Summarization. In 7th International Conference on Learning Rep-
resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.
https://openreview.net/forum?id=H1ersoRqtm

[20] Vincent J Hellendoorn, Christian Bird, Earl T Barr, and Miltiadis Allamanis. 2018.
Deep learning type inference. In Proceedings of the 2018 26th acm joint meeting
on european software engineering conference and symposium on the foundations of
software engineering. 152–162.

[21] Vincent J. Hellendoorn, Christian Bird, Earl T. Barr, and Miltiadis Allamanis. 2018.
Deep Learning Type Inference. In Proceedings of the 2018 26th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the Foundations
of Software Engineering (ESEC/FSE 2018). Association for Computing Machinery,
New York, NY, USA, 152âĂŞ162. https://doi.org/10.1145/3236024.3236051
[22] Jordan Henkel, Shuvendu K. Lahiri, Ben Liblit, and Thomas Reps. 2018. Code
vectors: understanding programs through embedded abstracted symbolic traces.

Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering - ESEC/FSE
2018 (2018). https://doi.org/10.1145/3236024.3236085

[23] Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu.
2012. On the Naturalness of Software. In Proceedings of the 34th International
Conference on Software Engineering (ICSE ’12). IEEE Press, 837âĂŞ847.

[24] Chih-Wei Hsu, Chih-Chung Chang, Chih-Jen Lin, et al. 2003. A practical guide

to support vector classification.

[25] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizing Source Code using a Neural Attention Model. In Proceedings of
the 54th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers). Association for Computational Linguistics, Berlin, Germany,
2073–2083. https://doi.org/10.18653/v1/P16-1195

[26] L. Jiang, H. Liu, and H. Jiang. 2019. Machine Learning Based Recommendation of
Method Names: How Far are We. In 2019 34th IEEE/ACM International Conference
on Automated Software Engineering (ASE). 602–614.

[27] T. Joachims. 1999. Making large-Scale SVM Learning Practical. In Advances in
Kernel Methods - Support Vector Learning, B. Schölkopf, C. Burges, and A. Smola
(Eds.). MIT Press, Cambridge, MA, Chapter 11, 169–184.

[28] H. J. Kang, T. F. BissyandÃľ, and D. Lo. 2019. Assessing the Generalizability of
Code2vec Token Embeddings. In 2019 34th IEEE/ACM International Conference on
Automated Software Engineering (ASE). 1–12. https://doi.org/10.1109/ASE.2019.
00011

[29] Kui Liu, Dongsun Kim, Tegawendé F. Bissyandé, Taeyoung Kim, Kisub Kim, Anil
Koyuncu, Suntae Kim, and Yves Le Traon. 2019. Learning to Spot and Refactor
Inconsistent Method Names. In Proceedings of the 41st International Conference
on Software Engineering (ICSE ’19). IEEE Press, 1âĂŞ12. https://doi.org/10.1109/
ICSE.2019.00019

[30] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.

Journal of machine learning research 9, Nov (2008), 2579–2605.

[31] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine
Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830.
[32] Md. Rafiqul Islam Rabin, Nghi D. Q. Bui, Yijun Yu, Lingxiao Jiang, and Moham-
mad Amin Alipour. 2020. On the Generalizability of Neural Program Analyz-
ers with respect to Semantic-Preserving Program Transformations.
https:
//arxiv.org/abs/2008.01566

[33] Md Rafiqul Islam Rabin, Ke Wang, and Mohammad Amin Alipour. 2019. Testing
Neural Program Analyzers. In 34th IEEE/ACM International Conference on Auto-
mated Software Engineering (Late Breaking Results-Track). https://arxiv.org/abs/
1908.10711

[34] Veselin Raychev, Pavol Bielik, and Martin Vechev. 2016. Probabilistic Model
for Code with Decision Trees. SIGPLAN Not. 51, 10 (Oct. 2016), 731âĂŞ747.
https://doi.org/10.1145/3022671.2984041

[35] Veselin Raychev, Martin Vechev, and Andreas Krause. 2015. Predicting Pro-
gram Properties from "Big Code". In Proceedings of the 42nd Annual ACM
SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL
’15). Association for Computing Machinery, New York, NY, USA, 111âĂŞ124.
https://doi.org/10.1145/2676726.2677009

[36] Veselin Raychev, Martin Vechev, and Eran Yahav. 2014. Code Completion with
Statistical Language Models. In Proceedings of the 35th ACM SIGPLAN Conference
on Programming Language Design and Implementation (PLDI ’14). Association
for Computing Machinery, New York, NY, USA, 419âĂŞ428. https://doi.org/10.
1145/2594291.2594321

[37] Nicholas Smith, Danny van Bruggen, and Federico Tomassetti. 2017. JavaParser:

visited. Leanpub, oct. de (2017).

[38] Bart Theeten, Frederik Vandeputte, and Tom Van Cutsem. 2019. Import2Vec
Learning Embeddings for Software Libraries. In Proceedings of the 16th Interna-
tional Conference on Mining Software Repositories (MSR ’19). IEEE Press, Piscat-
away, NJ, USA, 18–28. https://doi.org/10.1109/MSR.2019.00014

[39] Ke Wang. 2019. Learning Scalable and Precise Representation of Program Se-

mantics. arXiv preprint arXiv:1905.05251 (2019).

[40] Ke Wang and Zhendong Su. 2020. Blended, Precise Semantic Program Embed-
dings (PLDI 2020). Association for Computing Machinery, New York, NY, USA,
121âĂŞ134. https://doi.org/10.1145/3385412.3385999

[41] Ke Wang, Zhendong Su, and Rishabh Singh. 2018. Dynamic Neural Program
Embeddings for Program Repair. In International Conference on Learning Repre-
sentations. https://openreview.net/forum?id=BJuWrGW0Z

[42] Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang. 2019. Learning a Static

Bug Finder from Data. arXiv:cs.SE/1907.05579

[43] Xiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song, and Dawn Song. 2017.
Neural Network-based Graph Embedding for Cross-Platform Binary Code Simi-
larity Detection. Proceedings of the 2017 ACM SIGSAC Conference on Computer and
Communications Security (Oct 2017). https://doi.org/10.1145/3133956.3134018

