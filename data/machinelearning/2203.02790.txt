A Robust Spectral Algorithm for Overcomplete
Tensor Decomposition

Samuel B. Hopkins∗

Tselil Schramm†

Jonathan Shi‡

March 8, 2022

Abstract

We give a spectral algorithm for decomposing overcomplete order-4 tensors, so
long as their components satisfy an algebraic non-degeneracy condition that holds for
nearly all (all but an algebraic set of measure 0) tensors over (ℝ𝑑)⊗4 with rank 𝑛 (cid:54) 𝑑2.
Our algorithm is robust to adversarial perturbations of bounded spectral norm.

Our algorithm is inspired by one which uses the sum-of-squares semideﬁnite
programming hierarchy (Ma, Shi, and Steurer STOC’16), and we achieve comparable
robustness and overcompleteness guarantees under similar algebraic assumptions.
However, our algorithm avoids semideﬁnite programming and may be implemented
as a series of basic linear-algebraic operations. We consequently obtain a much faster
running time than semideﬁnite programming methods: our algorithm runs in time
˜𝑂(𝑛2𝑑3) (cid:54) ˜𝑂(𝑑7), which is subquadratic in the input size 𝑑4 (where we have suppressed
factors related to the condition number of the input tensor).

2
2
0
2

r
a

M
5

]

G
L
.
s
c
[

1
v
0
9
7
2
0
.
3
0
2
2
:
v
i
X
r
a

∗Cornell University. samhop@cs.cornell.edu. Supported by NSF awards 1350196 & 1408673, and a

Microsoft PhD Fellowship.

†MIT and Harvard. tselil@mit.edu. Supported by NSF awards CCF 1565264 & CNS 1618026.
‡Cornell University. jshi@cs.cornell.edu. Supported by NSF awards 1350196 & 1408673.

 
 
 
 
 
 
Contents

1

Introduction
1.1 Our Results
.
1.2 Related works .

.

.
.

.
.

.
.

.
.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.

2 Overview of algorithm

3 Preliminaries

1
4
6

9

14

4 Tools for analysis and implementation

15
4.1 Robustness and spectral perturbation . . . . . . . . . . . . . . . . . . . . . . 15
4.2 Eﬃcient implementation and runtime analysis . . . . . . . . . . . . . . . . . 15

5 Lifting

5.1 Algebraic identiﬁability argument
5.2 Robustness arguments .

17
. . . . . . . . . . . . . . . . . . . . . . . . 19
. 21

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Rounding

23
6.1 Recovering candidate whitened and squared components . . . . . . . . . .
. 24
6.2 Extracting components from the whitened squares . . . . . . . . . . . . . . . 26
6.3 Testing candidate components . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
6.4 Putting things together .

7 Combining lift and round for ﬁnal algorithm

31

.

.

.

.

.

.

8 Condition number of random tensors
.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

34
. 39
8.1 Notation .
8.2 Fourth Moment Identities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
8.3 Matrix Product Identities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
8.4 Naive Spectral Norm Estimate . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
8.5 Oﬀ-Diagonal Second Moment Estimates . . . . . . . . . . . . . . . . . . . . . 43
. 45
8.6 Matrix Decoupling .
. 46
.
8.7 Putting It Together
. 47
.
.
8.8 Omitted Proofs

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.
.
.

.

References

A Tools for analysis and implementation

B Notes on Table 1

C Simulations for condition number of random tensors

49

52

54

55

1

Introduction

Tensors are higher-order analogues of matrices: multidimensional arrays of numbers.
They have broad expressive power: tensors may represent higher-order moments of a
probability distribution [AGH+14], they are natural representations of cubic, quartic, and
higher-degree polynomials [RM14, HSS15], and they appear whenever data is multimodal
(e.g. in medical studies, where many factors are measured) [AABB+07, BS05, HLMK]. Due
to these reasons, in recent decades tensors have emerged as fundamental structures in
machine learning and signal processing.

The notion of rank extends from matrices to tensors: a rank-1 tensor in (ℝ𝑑)⊗𝑘 is a tensor
that can be written as a tensor product 𝑢(1) ⊗ · · · ⊗ 𝑢(𝑘) of vectors 𝑢(1), . . . , 𝑢(𝑘) ∈ ℝ𝑑. Any
tensor T ∈ (ℝ𝑑)⊗𝑘 can be expressed as a sum of rank-1 tensors, and the rank of T is the
minimum number of terms needed in such a sum. As is the case for matrices, we are often
interested in tensors of low rank: low-rank structure in tensors often carries interpretable
meaning about underlying data sets or probability distributions, and the tensors that arise
in many applications are low-rank [AGH+14].

Tensor decomposition is the natural inverse problem in the context of tensor rank: given

a 𝑑-dimensional symmetric 𝑘-tensor T ∈ (ℝ𝑑)⊗𝑘 of the form

T =

(cid:213)

𝑖(cid:54)𝑛

𝑎⊗𝑘
𝑖 + E,

for vectors 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑 and an (optional) error tensor E ∈ (ℝ𝑑)⊗𝑘, we are asked to
output vectors 𝑏1, . . . , 𝑏𝑛 as close as possible to 𝑎1, . . . , 𝑎𝑛 (e.g. minimizing the Euclidean
distance (cid:107)𝑏𝑖 − 𝑎𝑖 (cid:107)). The goal is to accomplish this with an algorithm that is as eﬃcient as
possible, under the mildest-possible assumptions on 𝑘,𝑎1, . . . , 𝑎𝑛, and E.

While tensor rank decomposition is a generalization of rank decomposition for matrices,
decomposition for tensors of order 𝑘 (cid:62) 3 diﬀers from the matrix case in several key ways.

1. (Uniqueness) Under mild assumptions on the vectors 𝑎1, . . . , 𝑎𝑛, tensor decomposi-
tions are unique (up to permutations of [𝑛]), while matrix decompositions are often
unique only up to unitary transformation.

2. (Overcompleteness) Tensor decompositions often remain unique even when the
number of factors 𝑛 is larger than the ambient dimension 𝑑 (up to 𝑛 = 𝑂(𝑑𝑘−1)), while
a 𝑑 × 𝑑 matrix can have only 𝑑 eigenvectors or 2𝑑 singular vectors.

These features make tensor decompositions suitable for many applications where matrix
factorizations are insuﬃcient. However, there is another major diﬀerence:

3. (Computational Intractability) While many matrix decompositions — eigendecompo-
sitions, singular value decompositions, 𝐿𝑈-factorizations, and so on — can be found

1

in polynomial time, tensor decomposition is NP-hard in general [HL13].

In spite of the NP-hardness of general tensor decomposition, many special cases turn
out to admit polynomial-time algorithms. A classical algorithm, often called Jennrich’s
algorithm, recovers the components 𝑎1, . . . , 𝑎𝑛 from T when they are linearly independent
(which requires 𝑛 (cid:54) 𝑑) and E = 0 using simultaneous diagonalization [Har70, DLDMV96].
More sophisticated algorithms improve on Jennrich’s in their tolerance to overcom-
pleteness (and the resulting lack of linear independence) and robustness to nontrivial
error tensors E. The literature now contains a wide variety of techniques for tensor
decomposition: the major players are iterative methods (tensor power iteration, stochastic
gradient descent, and alternating minimization), spectral algorithms, and convex programs.
Convex programs, and in particular the sum-of-squares semideﬁnite programming hierarchy
(SoS), require the mildest assumptions on 𝑘, 𝑎1, . . . , 𝑎𝑛 , E among known polynomial-time
algorithms [MSS16]. In pushing the boundaries of what is known to be achievable in
polynomial time, SoS-based algorithms have been crucial. However, the running times of
these algorithms are large polynomials in the input, making them utterly impractical for
applications.

The main contribution of this work is a tensor decomposition algorithm whose robust-
ness to errors and tolerance for overcompleteness are similar to those of the SoS-based
algorithms, but with subquadratic running time. Other algorithms with comparable running
times either require higher-order tensors,1 are not robust in that they require the error E = 0
or E exponentially small, or require linear independence of the components 𝑎1, . . . , 𝑎𝑛 and
hence 𝑛 (cid:54) 𝑑.2

Our algorithm is comparatively simple, and can be implemented with a small number
of dense matrix and matrix-vector multiplication operations, which are fast not only
asymptotically but also in practice.

Concretely, we study tensor decomposition of overcomplete 4-tensors under algebraic
nondegeneracy conditions on the tensor components 𝑎1, . . . , 𝑎𝑛. Algebraic conditions like
ours are the mildest type of assumption on 𝑎1, . . . , 𝑎𝑛 known to lead to polynomial time
algorithms – our algorithm can decompose all but a measure-zero set of 4-tensors of rank
𝑛 (cid:28) 𝑑2, and in particular we make no assumption that the components 𝑎1, . . . , 𝑎𝑛 are
random.3

When 𝑛 (cid:28) 𝑑2, our algorithm approximately recovers a 1 − 𝑜(1) fraction of 𝑎1, . . . , 𝑎𝑛 (up
+ 𝐸, so long as the spectral norm (cid:107)𝐸(cid:107) is (signiﬁcantly) less

to their signs) from 𝑇 = (cid:205)𝑖(cid:54)𝑛 𝑎⊗4

𝑖

1Higher-order tensors are costly because they are larger objects, and for learning applications they often

require a polynomial increase in sample complexity.

2There are also existing robust algorithms which tolerate some overcompleteness when 𝑎1, . . . , 𝑎𝑛 are
assumed to be random; in this paper we study generic 𝑎1, . . . , 𝑎𝑛, which is a much more challenging setting
than random 𝑎1, . . . , 𝑎𝑛 [AGJ14a, HSSS16].

3Although decompositions of 4-th order tensors can remain unique up to 𝑛 ≈ 𝑑3, no polynomial-time

algorithms are known which successfully decompose tensors of overcompleteness 𝑛 (cid:29) 𝑑2.

2

than the minimum singular value of a certain matrix associated to the {𝑎𝑖 }. (In particular,
nonsingularity of this matrix is our nondegeneracy condition on 𝑎1, . . . , 𝑎𝑛.) The algorithm
requires time ˜𝑂(𝑛2𝑑3) (cid:54) 𝑂(𝑑7), which is subquadratic in the input size 𝑑4.

Robustness, Overcompleteness, and Applications to Machine Learning Tensor decom-
position is a common primitive in algorithms for statistical inference that leverage the method
of moments to learn parameters of latent variable models. Examples of such algorithms exist
for independent component analysis / blind source separation [DLCC07], dictionary learn-
ing [BKS15, MSS16, SS17], overlapping community detection [AGHK13, HS17], mixtures
of Gaussians [GHK15], and more.

In these applications, we receive samples 𝑥 ∈ ℝ𝑑 from a model distribution 𝒟(𝜌)
that is a function of parameters 𝜌. The goal is to estimate 𝜌 using the samples. The
method-of-moments strategy is to construct the third- or fourth-order moment tensor
𝔼 𝑥⊗𝑘 (𝑘 = 3, 4) from samples whose expectation 𝔼 𝑥⊗𝑘 = (cid:205)𝑖(cid:54)𝑛 𝑎⊗𝑘
is a low rank tensor
𝑖
with components 𝑎1, . . . , 𝑎𝑛, from which the model parameters 𝜌 can be deduced.4 Since
𝔼 𝑥⊗𝑘 is estimated using samples, the tensor decomposition algorithm used to extract
𝑎1, . . . , 𝑎𝑛 from 𝔼 𝑥⊗𝑘 must be robust to error from sampling.

The sample complexity of the resulting algorithm depends directly on the magnitude
In addition, the greater general

of errors tolerated by the decomposition algorithm.
error-robustness of our result suggests better tolerance of model misspeciﬁcation error.

Some model classes give rise to overcomplete tensors; roughly speaking, this occurs
when the number of parameters (the size of the description of 𝜌) far exceeds 𝑑2, where
𝑑 is the ambient dimension. Typically, in such cases, 𝜌 consists of a collection of vectors
𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑 with 𝑛 (cid:29) 𝑑. Such overcomplete models are widely used; for example,
in the dictionary learning setting, we are given a data set 𝑆 and are asked to ﬁnd a sparse
representation of 𝑆. This is a powerful preprocessing tool, and the resulting representations
are more robust to perturbations, but assembling a truly sparse, eﬀective dictionary often
requires representing 𝑑-dimensional data in a basis with 𝑛 (cid:29) 𝑑 elements [LS00, Ela10].
Recent works also relate the problem of learning neural networks with good generalization
error to tensor decomposition, showing a connection between overcompleteness and the
width of the network [MM18].5

Using tensor decomposition in such settings requires algorithms with practical running
times, error robustness, and tolerance to overcompleteness. The strongest polynomial-time
guarantees for overcomplete dictionary learning and similar models currently rely on

4Any constant 𝑘, rather than just 𝑘 = 3, 4, may lead to polynomial-time learning algorithms, but the cost
is typically gigantic polynomial sample complexity and running time, scaling like 𝑑𝑘, to estimate and store a
𝑘-th order tensor.

5Strictly speaking, this work shows a reduction from tensor decomposition to learning neural nets, but the

connection between width and overcompleteness is direct regardless.

3

overcomplete tensor decomposition via the SoS method [MSS16]; our work is an important
step towards giving lightweight, spectral algorithms for such problems.

1.1 Our Results

Our contribution is a robust, lightweight spectral algorithm for tensor decomposition in the
overcomplete regime. We require that the components satisfy an algebraic non-degeneracy
assumption satisﬁed by all but a measure-0 set of inputs. At a high level, we require that a
certain matrix associated with the components of the tensor have full rank. Though the
assumption may at ﬁrst seem complicated, we give it formally here:

Deﬁnition 1.1. Let Π⊥
be the projector to the orthogonal complement of the subspace of
2,3
(ℝ𝑑)⊗3 that is symmetric in its latter two tensor modes. Equivalently, Π⊥
2(Id − 𝑃2,3),
where 𝑃2,3 is the linear operator that interchanges the second and third modes of (ℝ𝑑)⊗3.

2,3 = 1

Deﬁnition 1.2. Let Πimg(𝑀) denote the projector to the column space of the matrix 𝑀.
Equivalently, Πimg(𝑀) = (𝑀𝑀(cid:62))−1/2𝑀 = 𝑀(𝑀(cid:62)𝑀)−1/2, where (𝑀𝑀(cid:62))−1/2 is the whitening
transform of 𝑀 and is equal to the Moore-Penrose pseudoinverse of (𝑀𝑀(cid:62))1/2.

Deﬁnition 1.3. Vectors 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑 are 𝜅-non-degenerate if the matrix 𝐾(𝑎1, . . . , 𝑎𝑛),
deﬁned below, has minimum singular value at least 𝜅 > 0. If 𝜅 = 0, we say that the {𝑎𝑖 } are
degenerate.

The matrix 𝐾(𝑎1, . . . , 𝑎𝑛) is given by choosing for each 𝑖 ∈ [𝑛] a matrix 𝐵𝑖 whose columns
form a basis for the orthogonal complement of 𝑎𝑖 in ℝ𝑑, assembling the 𝑑3 × 𝑛(𝑑 − 1) matrix
𝐻 whose rows are given by 𝑎𝑖 ⊗ 𝑎𝑖 ⊗ 𝐵(𝑗)
𝑖

as

𝐻 =








2,3Πimg(𝐻(cid:62)).

and letting 𝐾(𝑎1, . . . , 𝑎𝑛) = Π⊥

𝑎(cid:62)
1

⊗ 𝐵(cid:62)
1

⊗ 𝑎(cid:62)
1
...
𝑛 ⊗ 𝑎(cid:62)
𝑎(cid:62)
𝑛 ⊗ 𝐵(cid:62)
𝑛









We note that when 𝑛 (cid:28) 𝑑2 then all but a measure-zero set of unit (𝑎1, . . . , 𝑎𝑛) ∈ ℝ𝑑𝑛
satisfy the condition that 𝜅 > 0. We expect also that for 𝑛 (cid:28) 𝑑2, if 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑 are
independent random unit vectors then 𝜅 (cid:62) Ω(1) – we provide simulations in support of
this in Section C.6

6Furthermore, standard techniques in random matrix theory prove that when 𝑎1, . . . , 𝑎𝑛 are random then
matrices closely related to 𝐾(𝑎1, . . . , 𝑎𝑛) are well-conditioned; for instance this holds (roughly speaking)
if (𝐻(cid:62)
2 𝐻)−1/2 are removed. However, inverses and pseudoinverses of random matrices,
especially those with dependent entries like ours, are infamously challenging to analyze – we leave this
challenge to future work. See Section C for details.

1 𝐻)−1/2 and (𝐻(cid:62)

4

Some previous works on tensor decomposition under algebraic nondegeneracy as-
sumptions also give smoothed analyses of nondegeneracy, showing that small random
-well-conditioned (for diﬀering notions of
perturbations of arbitrary vectors are
well-conditioned-ness) [BCMV14, MSS16]. We expect that a similar smoothed analy-
sis is possible for 𝜅-non-degeneracy, though because of the speciﬁc form of the matrix
𝐾(𝑎1, . . . , 𝑎𝑛) it does not follow immediately from known results. We defer this technical
challenge to future work.

1
poly(𝑑)

Given this non-degeneracy condition, we robustly decompose the input tensor in time
𝜅 ), where we have suppressed factors depending on the smallest singular value of a

˜𝑂( 𝑛2𝑑3
matrix ﬂattening of our tensor.

Theorem (Special case of Theorem 7.3). Suppose that 𝑑 (cid:54) 𝑛 (cid:54) 𝑑2, and that 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑
are 𝜅-non-degenerate unit vectors for 𝜅 > 0, and suppose that T is their 4-tensor perturbed by noise,
T ∈ (ℝ𝑑)⊗4 such that T = (cid:205)𝑖∈[𝑛] 𝑎⊗4
log 𝑑 in its
𝑖
𝑑2 × 𝑑2 reshaping. Suppose further that when reshaped to a 𝑑2 × 𝑑2 matrix, (cid:107)𝑇−1(cid:107) (cid:54) 𝑂(1) and
that (cid:107) (cid:205)𝑖∈[𝑛](𝑎⊗3

+ 𝐸, where 𝐸 is a perturbation such that (cid:107)𝐸(cid:107) (cid:54) 𝜀

)(cid:62)(cid:107) (cid:54) 𝑂(1).

There exists an algorithm decompose with running time ˜𝑂(𝑛2𝑑3𝜅−1), so that for every such
T there exists a subset 𝑆 ⊆ {𝑎1, . . . , 𝑎𝑛 } of size |𝑆| (cid:62) 0.99𝑛, such that decompose(T) with high
probability returns a set of 𝑡 = ˜𝑂(𝑛) unit vectors 𝑏1, . . . , 𝑏𝑡 where every 𝑎𝑖 ∈ 𝑆 is close to some 𝑏 𝑗,
and each 𝑏 𝑗 is close to some 𝑎𝑖 ∈ 𝑆:

)(𝑎⊗3
𝑖

𝑖

∀𝑎𝑖 ∈ 𝑆, max

𝑗

|(cid:104)𝑏 𝑗 , 𝑎𝑖(cid:105)| (cid:62) 1−𝑂

(cid:17) 1/8

,

(cid:16) 𝜀
𝜅2

and

∀𝑗 ∈ [𝑡], max
𝑎𝑖 ∈𝑆

|(cid:104)𝑏 𝑗 , 𝑎𝑖(cid:105)| (cid:62) 1−𝑂

(cid:17) 1/8

.

(cid:16) 𝜀
𝜅2

Furthermore, if 𝑎1, . . . , 𝑎𝑛 are random unit vectors, then with high probability they satisfy the

conditions of this theorem with 𝜅 = Ω(1).

When 𝑛 (cid:54) 𝑑, our algorithm still obtains nontrivial guarantees (though the runtime
asymptotics are dominated by other terms); however in this regime, a combination of the
simpler algorithm of [SS17] and a whitening procedure gives comparable guarantees.

We remark that our full theorem, Theorem 7.3, does not pose as many restrictions
on the {𝑎𝑖 }; we do not generally require that (cid:107)𝑇−1(cid:107) (cid:54) 𝑂(1) or that (cid:107) (cid:205)𝑖(𝑎⊗3
)(cid:62)(cid:107) (cid:54)
𝑂(1). However, allowing these quantities to depend on 𝑑 and 𝑛 aﬀects our runtime and
approximation guarantees, and so to simplify presentation we have made these restrictions
here; we refer the reader to Theorem 7.3 for details.

)(𝑎⊗3
𝑖

𝑖

Furthermore, in the theorem stated above we recover only a 0.99-fraction of the vectors,
and we require the perturbation to have magnitude 𝑂( 1
log 𝑑 ). This is again a particular
choice of parameters in Theorem 7.3, which allows for a four-way tradeoﬀ among accuracy,
magnitude of perturbation, fraction of components recovered, and runtime. For example,
in spectral norm, then we may recover all components in time
if the perturbation is

1
poly(𝑑)

5

Algorithm
[LCC07]
[AGJ17]
[GM17]
[MSS16]
[SS17]

Type
algebraic
iterative
iterative
SDP
spectral

this paper

spectral

Rank
𝑛 (cid:54) 𝑑2
𝑛 (cid:54) 𝑜(𝑑1.5)
𝑛 (cid:54) 𝑂(𝑑2)
𝑛 (cid:54) 𝑑2
𝑛 (cid:54) 𝑑
𝑛 (cid:54) 𝑑2

Robustness
(cid:107)𝐸(cid:107)∞ (cid:54) 2−𝑂(𝑑)
(cid:107)𝐸(cid:107) (cid:54) 𝑜( 𝑛
𝑑2 )
𝐸 = 0
(cid:107)𝐸(cid:107) (cid:54) 0.01
(cid:107)𝐸(cid:107) (cid:54) 𝑂( log log 𝑑
log 𝑑 )
(cid:107)𝐸(cid:107) (cid:54) 𝑂( 1
log 𝑑 )

Assumptions
algebraic
random, warm start
random, warm start
algebraic
orthogonal

algebraic

Runtime
˜𝑂(𝑛3𝑑4)
˜𝑂(𝑛𝑑3)
˜𝑂(𝑛𝑑4)
(cid:62) 𝑛𝑑24
˜𝑂(𝑑2+𝜔)
˜𝑂(𝑛2𝑑3)

Table 1: A comparison of tensor decomposition algorithms for rank-𝑛 4-tensors in (ℝ𝑑)⊗4. Here 𝜔
denotes the matrix multiplication constant. A robustness bound (cid:107)𝐸(cid:107) (cid:54) 𝜂 refers to the requirement
that a 𝑑2 × 𝑑2 reshaping of the error tensor 𝐸 have spectral norm at most 𝜂. Some of the algorithms’
guarantees involve a tradeoﬀ between robustness, runtime, and assumptions; where this is the case,
we have chosen one representative setting of parameters. See Section B for details. Above, “random”
indicates that the algorithm assumes 𝑎1, . . . , 𝑎𝑛 are independent unit vectors (or Gaussians) and
“algebraic” indicates that the algorithm assumes that the vectors avoid an algebraic set of measure 0.

˜𝑂(𝑛2𝑑3𝜅−1); alternatively, if the perturbation has spectral norm 𝜂2 = Θ(1), then we may
recover an 0.99-fraction of components in time ˜𝑂(𝑛2+𝑂(𝜂)𝑑3𝜅−1) up to accuracy 1 − 𝑂( 𝜂
𝜅2 )1/8.
Again, we refer the reader to Theorem 7.3 for the full tradeoﬀ.

Finally, a note about our recovery guarantee: we guarantee that every vector returned
by the algorithm is close to some component, and furthermore that most components will
be close to some vector. It is possible to run a clean-up procedure after our algorithm,
in which nearby approximate components 𝑏 𝑗 are clustered to correspond to a speciﬁc 𝑎𝑖;
depending on the proximity of the 𝑎𝑖 to each other, this may require stronger accuracy
guarantees, and so we leave this procedure as an independent step. Our guarantee does not
include signs, but this is because the tensor T is an even-order tensor, so the decomposition
is only unique up to signings as (−𝑎𝑖)⊗4 = 𝑎⊗4
.

𝑖

1.2 Related works

The literature on tensor decomposition is broad and varied, and we will not attempt to
survey it fully here (see e.g. the survey [KB09] or the references within [AGH+14, GM17]
for a fuller picture). We will give an idea of the relationship between our algorithm and
others with provable guarantees.

For simplicity we focus on order-4 tensors. Algorithms with provable guarantees for
tensor decomposition fall broadly into three classes: iterative methods, convex programs,
and spectral algorithms. For a brief comparison of our algorithm to previous works, we
include Table 1.

6

Iterative methods are a class of algorithms that maintain one (or
Iterative Methods.
sometimes several) estimated component(s) 𝑏, and update the estimate using a variety
of update rules. Some popular update rules include tensor power iteration [AGH+14],
gradient descent [GM17], and alternating-minimization [AGJ14b]. Most of these methods
have the advantage that they are fast; the update steps usually run in time linear in the
input size, and the number of updates to convergence is often polylogarithmic in the input
size.

The performance of the most popular iterative methods has been well-characterized
in some restricted settings; for example, when the components {𝑎𝑖 } are orthogonal or
linearly independent [AGH+14, GHJY15, SV17], or are independently drawn random
vectors [AGJ17, GM17]. Furthermore, many of these analyses require a “warm start,” or an
initial estimate 𝑏 that is more correlated with a component than a typical random starting
point. Few provable guarantees are known for the non-random overcomplete regime, or in
the presence of arbitrary perturbations.

Convex Programming. Convex programs based on the sum-of-squares (SoS) semideﬁnite
programming (SDP) relaxation yield the most general provable guarantees for tensor
decomposition. These works broadly follow a method of pseudo-moments: interpreting the
input tensor (cid:205)𝑖∈[𝑛] 𝑎⊗𝑘
as the 𝑘-th moment tensor 𝔼 𝑋 ⊗𝑘 of a distribution 𝑋 on ℝ𝑑, this
𝑖
approach uses SoS to generate surrogates (or pseudo-moments) for higher moment tensors,
like 𝔼 𝑋 ⊗100𝑘 = (cid:205)𝑖∈[𝑛] 𝑎⊗100𝑘
. It is generally easier to extract the components 𝑎1, . . . , 𝑎𝑛
from (cid:205)𝑖∈[𝑛](𝑎⊗100
} have fewer algebraic
dependencies than the vectors {𝑎𝑖 }, and are farther apart in Euclidean distance. Of course,
𝔼 𝑋 ⊗100𝑘 = (cid:205)𝑖∈[𝑛] 𝑎⊗100𝑘
is not given as input, and even in applications where the input is
negotiable, it may be expensive or impossible to obtain such a high-order tensor. The SoS
method uses semideﬁnite programming to generate a surrogate which is good enough to
be used to ﬁnd the vectors 𝑎1, . . . , 𝑎𝑛

)⊗𝑘 than from (cid:205)𝑖∈[𝑛] 𝑎⊗𝑘
𝑖

, because the vectors {𝑎⊗100

𝑖

𝑖

𝑖

𝑖

Work on sum-of-squares relaxations for tensor decomposition began with the quasi-
polynomial time algorithm of [BKS15]; this algorithm requires only mild well-conditioned-
ness assumptions, but also requires high-order tensors as input, and runs in quasi-
polynomial time. This was followed by an analysis showing that, at least in the setting of
random 𝑎1, . . . , 𝑎𝑛, the SoS algorithm can decompose substantially overcomplete tensors
of order 3 [GM15]. This line of work ﬁnally concluded with the work of Ma, Shi, and
Steurer [MSS16], who give sum-of-squares based polynomial-time algorithms for tensor
decomposition in the most general known settings: under mild algebraic assumptions on
the components, and in the presence of adversarial noise, so long as the noise tensor has
bounded spectral norm in its matrix reshapings.

These SoS algorithms have the best known polynomial-time guarantees, but they
are formidably slow. The work of [MSS16] uses the degree-8 sum-of-squares relaxation,

7

meaning that to ﬁnd each of the 𝑛 components, one must solve an SDP in Ω(𝑑8) variables.
While these results are important in establishing that polynomial-time algorithms exist for
these settings, their runtimes are far from eﬃcient.

Inspired by the mild assumptions
Spectral algorithms from Sum-of-Squares Analyses.
needed by SoS algorithms, there has been a line of work that uses the analyses of SoS in
order to design more eﬃcient spectral algorithms, which ideally work for similarly-broad
classes of tensors.

At a high level, these spectral algorithms use eigendecompositions of speciﬁc matrix
polynomials to directly construct approximate primal and dual solutions to the SoS
semideﬁnite programs, thereby obtaining the previously mentioned “surrogate moments”
without having to solve an SDP. Since the SoS SDPs are quite powerful, constructing
(even approximate) solutions to them directly and eﬃciently is a nontrivial endeavor. The
resulting matrices are only approximately SDP solutions — in fact, they are often far from
satisfying most of the constraints of the SoS SDPs. There is a tradeoﬀ between how well
these spectrally constructed solutions approximate the SoS output and how eﬃciently
the algorithm can be implemented. However, by carefully choosing which constraints to
satisfy, these works are able to apply the SDP rounding algorithms to the approximate
spectrally-constructed solutions (often with new analyses) to obtain similar algorithmic
guarantees.

The work of [HSSS16] was the ﬁrst to adapt the analysis of SoS for random 𝑎1, . . . , 𝑎𝑛
presented by [GM15] to obtain spectral algorithms for tensor decomposition, giving
subquadratic algorithms for decomposing random overcomplete tensors with 𝑛 (cid:54) 𝑂(𝑑4/3).
As SoS algorithms have developed, so too have their faster spectral counterparts.
In
particular, [SS17] adapted some of the SoS arguments presented in [MSS16] to give robust
subquadratic algorithms for decomposing orthogonal 4-tensors in the presence of adversarial
noise bounded only in spectral norm.

Our result builds on the progress of both [MSS16, SS17]. The SoS algorithm of [MSS16]
was the ﬁrst to robustly decompose generic overcomplete tensors in polynomial time.
The spectral algorithm of [SS17] obtains a much faster running time for robust tensor
decomposition, but sacriﬁces overcompleteness. Our work adapts (and improves upon)
the SoS analysis of [MSS16] to give a spectral algorithm for the robust and overcomplete
regime. Our primary technical contribution is the eﬃcient implementation of the lifting
step in the SoS analysis of [MSS16] as an eﬃcient spectral algorithm to generate surrogate
6th order moments; this is the subject of Section 5, and we give an informal description in
Section 2.

FOOBI The innovative FOOBI (Fourth-Order Cumulant-Based Blind Identiﬁcation)
algorithm of [LCC07] was the ﬁrst method with provable guarantees for overcomplete

8

4-th order tensor decomposition under algebraic nondegeneracy assumptions. Like our
algorithm, FOOBI can be seen as a lifting procedure (to an 8-th order tensor) followed by
a rounding procedure. The FOOBI lifting procedure inspires ours – although ours runs
faster because we lift to a 6-tensor rather than an 8-tensor – but the FOOBI rounding step
is quite diﬀerent, and proceeds via a clever simultaneous diagonalization approach. The
advantage our algorithm oﬀers over FOOBI is twofold: ﬁrst, it provides formal, strong
robustness guarantees, and second, it has a faster asymptotic runtime.

𝑖

𝑖=1 𝑎⊗4

To the ﬁrst point: for a litmus test, consider the case that 𝑛 = 𝑑 and 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑
are orthonormal. On input 𝑇 = (cid:205)𝑛
+ 𝐸, our algorithm recovers the 𝑎𝑖 for arbitrary
perturbations 𝐸 so long as they are bounded in spectral norm by (cid:107)𝐸(cid:107) (cid:54) 1/poly log 𝑑.7
We are not aware of any formal analyses of FOOBI when run on tensors with arbitrary
perturbations of this form. Precisely what degree of robustness should be expected from
this modiﬁed FOOBI algorithm is unclear. The authors of [LCC07] do suggest (without
analysis) a modiﬁcation of their algorithm for the setting of nonzero error tensors 𝐸,
involving an alternating-minimization method for computing an approximate simultaneous
diagonalization. Because the problem of approximate simultaneous diagonalization is
non-convex, establishing robustness guarantees for the FOOBI algorithm when augmented
with the approximate simultaneous diagonalization step appears to be a nontrivial technical
endeavor. We think this is an interesting and potentially challenging open question.

Further, while the running time of FOOBI depends on the speciﬁc implementation of
its linear-algebraic operations, we are unaware of any technique to implement it in time
faster than ˜𝑂(𝑛3𝑑4). In particular, the factor of 𝑑4 appears essential to any implementation
of FOOBI; it represents the side-length of a 𝑑4 × 𝑑4 square unfolding of a 𝑑-dimensional
8-tensor, which FOOBI employs extensively. By contrast, our algorithm runs in time
˜𝑂(𝑛2𝑑3), which is (up to logarithmic factors) faster by a factor of 𝑛𝑑.

2 Overview of algorithm

We begin by describing a simple tensor decomposition algorithm for orthogonal 3-tensors:
Gaussian rounding (Jennrich’s algorithm [Har70]). We then build on that intuition to
describe our algorithm.

Orthogonal, undercomplete tensors. Suppose that 𝑢1, . . . , 𝑢𝑑 ∈ ℝ𝑑 are orthonormal
vectors, and that we are given 𝑇 = (cid:205)𝑖∈[𝑑] 𝑢⊗3
. As a ﬁrst attempt at recovering the 𝑢𝑖, one
𝑖
might be tempted to choose the ﬁrst “slice” of 𝑇, the 𝑑 × 𝑑 matrix 𝑇1 = (cid:205)𝑖 𝑢𝑖(1) · 𝑢𝑖𝑢(cid:62)
, and
𝑖
compute its singular value decomposition (SVD). However, if |𝑢𝑖(1)| = |𝑢𝑗(1)| for some

7In contrast, most iterative methods, such as power iteration, can only handle perturbations of spectral

norm at most (cid:107)𝐸(cid:107) (cid:54) 1/poly(𝑑).

9

𝑖 ≠ 𝑗 ∈ [𝑑], the SVD will not allow us to recover these components. In this setting, Gaussian
rounding allows us to exploit the additional mode of 𝑇: If we sample 𝑔 ∼ 𝒩(0, Id𝑑), then we
can take the random ﬂattening 𝑇(𝑔) = (cid:205)𝑖 (cid:104)𝑔, 𝑢𝑖(cid:105) · 𝑢𝑖𝑢(cid:62)
; because the (cid:104)𝑔, 𝑢𝑖(cid:105) are independent
𝑖
standard Gaussians, they are distinct with probability 1, and an SVD will recover the
𝑢𝑖 exactly. Moreover, this algorithm also solves 𝑘-tensor decomposition for orthogonal
tensors with 𝑘 (cid:62) 4, by treating (cid:205)𝑖∈[𝑑] 𝑢⊗𝑘

as the 3-tensor (cid:205)𝑖∈[𝑑] 𝑢⊗𝑘−1

⊗ 𝑢𝑖 ⊗ 𝑢𝑖.

𝑖

𝑖

𝑖

In our setting, we have unit vectors {𝑎𝑖 }𝑖∈[𝑛] ⊂ ℝ𝑑
Challenges of overcomplete tensors.
with 𝑛 > 𝑑, and 𝑇 = (cid:205)𝑖 𝑎⊗4
(focusing for now on the unperturbed case). Since 𝑛 > 𝑑, the
components 𝑎1, . . . , 𝑎𝑛 are not orthogonal: they are not even linearly independent. So, we
cannot hope to use Gaussian rounding as a black box. While the vectors 𝑎1 ⊗ 𝑎1, . . . , 𝑎𝑛 ⊗ 𝑎𝑛
may be linearly independent, the spectral decompositions of the matrix (cid:205)𝑖∈[𝑛](𝑎⊗2
)(cid:62)
are not necessarily useful, since its eigenvectors may not be close to any of the vectors 𝑎𝑖,
and may be unique only up to rotation.

)(𝑎⊗2
𝑖

𝑖

Challenges of perturbations. Returning momentarily to the orthogonal setting with
𝑛 (cid:54) 𝑑, new challenges arise when the perturbation tensor 𝐸 is nonzero. For an orthog-
onal 4-tensor 𝑇 = (cid:205)𝑖∈[𝑑] 𝑢⊗4
+ 𝐸, the Gaussian rounding algorithm produces the matrix
(cid:205)𝑖∈[𝑑](cid:104)𝑔, 𝑢⊗2
(cid:105)𝑢𝑖𝑢𝑖T + 𝐸𝑔 for some 𝑑 × 𝑑 matrix 𝐸𝑔. The diﬃculty is that even if the spectral
norm (cid:107)𝐸(cid:107) (cid:28) 𝜎min((cid:205)𝑖∈[𝑑](𝑢⊗2
)(𝑢⊗2
)T) = 1, the matrix 𝐸𝑔 sums many slices of the tensor 𝐸,
𝑖
and so the spectrum of 𝐸𝑔 can overwhelm that of (cid:205)𝑖∈[𝑑](cid:104)𝑔, 𝑢⊗2

(cid:105)𝑢𝑖𝑢𝑖T.

𝑖

𝑖

𝑖

This diﬃculty is studied in [SS17], where it is resolved by SoS-inspired preprocessing

𝑖

of the tensor 𝑇. We borrow many of those ideas in this work.

Algorithmic strategy. We now give an overview of our algorithm. Algorithm 1 gives
a summarized version of the algorithm, with details concerning robustness and fast
implementation omitted.

There are two main stages to the algorithm: the ﬁrst stage is lifting, where the input rank-
𝑛 4-tensor over ℝ𝑑 is lifted to a corresponding rank-𝑛 3-tensor over a higher dimensional
space ℝ𝑑2; this creates an opportunity to use Gaussian rounding on the newly-created tensor
modes. In the second rounding stage, the components of the lifted tensor are recovered
using a strategy similar to Gaussian rounding and then used to ﬁnd the components of the
input.

This parallels the form of the SoS-based overcomplete tensor decomposition algorithm
of [MSS16], where both stages rely on SoS semideﬁnite programming. Our main technical
contribution is a spectral implementation of the lifting stage; our spectral implementation
of the rounding stage reuses many ideas of [SS17], adapted round the output of our new
lifting stage.

10

𝑖

𝑖

𝑖

𝑖

𝑖

𝑖

). Since {𝑊(𝑎⊗2

)𝑖∈[𝑛] from Span(𝑎⊗2

The lifting works by deriving Span(𝑎⊗3

Lifting. The goal of the lifting stage is to transform the input 𝑇 = (cid:205)𝑖∈[𝑛](𝑎⊗2
)(𝑎⊗2
𝑖
𝑖
an orthogonal 3-tensor. Let 𝑊 = 𝑇−1/2 and observe that the whitened vectors 𝑊(𝑎⊗2
orthonormal; therefore we will want to use 𝑇 to ﬁnd the orthogonal 3-tensor (cid:205)𝑖∈[𝑛](𝑊 𝑎⊗2

simply the column space of the input 𝑇. By transforming Span(𝑎⊗3
we obtain Span(𝑊(𝑎⊗2
) ⊗ 𝑎
projector to their span is in fact equal to (cid:205)𝑖(𝑊(𝑎⊗2
reshaping and a ﬁnal multiplication by 𝑊 away from the orthogonal tensor (cid:205)𝑖(𝑊(𝑎⊗2

)T to
) are
)⊗3.
)𝑖∈[𝑛], where the latter is
) using 𝑊 = 𝑇−1/2,
𝑖
) ⊗ 𝑎𝑖 }𝑖∈[𝑛] are orthonormal, the orthogonal
)T, which is only a
) ⊗ 𝑎
))⊗3.
). It rests on an
algebraic “identiﬁability” argument, which establishes that for almost all problem instances
(all but an algebraic set of measure 0), the subspace Span(𝑎⊗3
) ⊗ ℝ𝑑
intersected with the symmetric subspace Span({𝑥 ⊗ 𝑥 ⊗ 𝑥}𝑥∈ℝ𝑑). Since we can compute
Span(𝑎𝑖 ⊗ 𝑎𝑖) from the input and since the symmetric subspace is easy to describe, we
are able to perform this lifting step eﬃciently. The simplest version of the identiﬁability
argument is given in Lemma 2.1, and a more robust version that includes a condition
number analysis is given in Section 5.1.

The key step is the operation which obtains Span(𝑎⊗3

) is equal to Span(𝑎⊗2

) from Span(𝑎⊗2

)(𝑊(𝑎⊗2

) ⊗ 𝑎

𝑖

𝑖

𝑖

𝑖

𝑖

𝑖

𝑖

𝑖

𝑖

𝑖

Lemma 2.1 (Simple Identiﬁability). Let 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑 with 𝑛 (cid:54) 𝑑2. Let 𝑆 denote Span({𝑎⊗2
})
𝑖
and let 𝑇 denote Span({𝑎⊗3
}) and assume both have dimension 𝑛. Let sym ⊆ (ℝ𝑑)⊗3 be the linear
subspace sym = Span({𝑥 ⊗ 𝑥 ⊗ 𝑥}𝑥∈ℝ𝑑) . For each 𝑖, let {𝑏𝑖,𝑗 } 𝑗∈[𝑑−1] be an arbitrary orthonormal
basis the orthogonal complement of 𝑎𝑖 in ℝ𝑑. Let also

𝑖

𝐾(cid:48)T :=

𝑎1 ⊗ 𝑎1 ⊗ 𝑏1,1 − 𝑎1 ⊗ 𝑏1,1 ⊗ 𝑎1
...
𝑎𝑖 ⊗ 𝑎𝑖 ⊗ 𝑏𝑖,𝑗 − 𝑎𝑖 ⊗ 𝑏𝑖,𝑗 ⊗ 𝑎𝑖
...
𝑎𝑛 ⊗ 𝑎𝑛 ⊗ 𝑏𝑛,𝑑−1 − 𝑎𝑛 ⊗ 𝑏𝑛,𝑑−1 ⊗ 𝑎𝑛













,













Then if 𝐾(cid:48) has full rank 𝑛(𝑑 − 1), it follows that (𝑆 ⊗ ℝ𝑑) ∩ sym = 𝑇.

Proof. To show that 𝑇 ⊆ (𝑆 ⊗ ℝ𝑑) ∩ sym, we simply note that {𝑎𝑖 ⊗ 𝑎𝑖 ⊗ 𝑎𝑖 }𝑖∈[𝑛] form a basis
for 𝑇 and are also each in both 𝑆 ⊗ ℝ𝑑 and sym.

To show that (𝑆 ⊗ ℝ𝑑) ∩ sym ⊆ 𝑇, we take some 𝑦 ∈ (𝑆 ⊗ ℝ𝑑) ∩ sym. Since 𝑦 is symmetric

under mode interchange, we express 𝑦 in two ways as

(cid:213)

𝑦 =

𝑎𝑖 ⊗ 𝑎𝑖 ⊗ 𝑐𝑖 =

𝑖

𝑎𝑖 ⊗ 𝑐𝑖 ⊗ 𝑎𝑖 .

(cid:213)

𝑖

11

Then by subtracting these two expressions for 𝑦 from each other, we ﬁnd

(cid:213)

0 =

𝑖

𝑎𝑖 ⊗ (𝑎𝑖 ⊗ 𝑐𝑖 − 𝑐𝑖 ⊗ 𝑎𝑖).

We express 𝑐𝑖 = (cid:104)𝑎𝑖 , 𝑐𝑖(cid:105)𝑎𝑖 + (cid:205)𝑗 𝛾𝑖𝑗𝑏𝑖𝑗 for some vector 𝛾. Then the symmetric parts cancel
out, leaving

(cid:213)

0 =

𝛾𝑖𝑗 𝑎𝑖 ⊗ (𝑎𝑖 ⊗ 𝑏𝑖𝑗 − 𝑏𝑖𝑗 ⊗ 𝑎𝑖) = 𝐾(cid:48)𝛾 .

𝑖𝑗

Since 𝐾(cid:48) is full rank by assumption, this is only possible when 𝛾 = 0. Therefore, 𝑐𝑖 ∝ 𝑎𝑖 for
(cid:3)
all 𝑖, so that 𝑦 ∈ 𝑇.

Remark 2.2. Although the condition number from the matrix 𝐾(cid:48) here is not the same as the one
derived from 𝐾 from Deﬁnition 1.3, it is oﬀ by at most a multiplicative factor of 2(cid:107)𝑇−1(cid:107)−1/2.
To see this, 𝐾 in Deﬁnition 1.3 is given as 𝐾 = Π⊥
2,3Πimg(𝐻T), whereas we may write
𝐾(cid:48) = 2Π⊥
2 (cid:107)𝐾−1(cid:107) (cid:107)𝐻−1(cid:107).
By [MSS16, Lemma 6.3], (cid:107)𝐻−1(cid:107)2 (cid:62) (cid:107)𝑇−1(cid:107).

2,3Πimg(𝐻T)(𝐻𝐻T)1/2 = 2𝐾(𝐻𝐻T)1/2. Therefore, (cid:107)𝐾(cid:48)−1(cid:107) (cid:62) 1

2,3𝐻T = 2Π⊥

Robustness. To ensure that our algorithm is robust to perturbations 𝐸, we must argue that
the column span of 𝑇 and 𝑇 + 𝐸 are close to each other so long as 𝐸 is bounded in spectral
norm, and furthermore than the lifting operation still produces a subspace 𝑉 which is
close to Span({𝑊(𝑎𝑖 ⊗ 𝑎𝑖) ⊗ 𝑎𝑖 }). This is done via careful application of matrix perturbation
analysis to the identiﬁability argument. By operating with 𝑊 only on third-order vectors
and matrices over (ℝ𝑑)⊗3, we also avoid incurring factors of the fourth-order operator norm
(cid:107)𝑇 (cid:107) in the condition numbers, instead only incurring a much milder sixth-order penalty
(cid:107) (cid:205) 𝑎⊗3

T(cid:107). For details, see Section 5.2.

𝑎⊗3
𝑖

𝑖

If we are given direct access to 𝑇 in the absence of noise, the rounding
Rounding.
stage can be accomplished with Gaussian rounding. However when we allow 𝑇 to be
adversarially perturbed the situation becomes more delicate. Our rounding stage is an
adaptation of [SS17], though some modiﬁcations are required for the additional challenges
of the overcomplete setting. It recovers the components of an approximation of a 3-tensor
with 𝑛 orthonormal components, provided that said approximation is within 𝜀
𝑛 in
Frobenius norm distance. The technique is built around Gaussian rounding, but in order
to have this succeed in the presence of 𝜀
𝑛 Frobenius norm noise, the large singular values
are truncated from the rectangular matrix reshapings of the 3-tensor: this ensures that the
rounding procedure is not entirely dominated by any spectrally large terms in the noise.
, we wish
to extract the 𝑎𝑖. Naively one could simply apply 𝑊 −1, but this can cause errors in the

After we recover approximations of the orthonormal components 𝑏𝑖 ≈ 𝑊 𝑎⊗2

√

√

𝑖

12

recovered vectors to blow up by a factor of (cid:107)𝑊 −1(cid:107). Even when the {𝑎𝑖 } are random vectors,
(cid:107)𝑊 −1(cid:107) = Ω(poly(𝑑)).8 Instead, we utilize the projector to Span{𝑊(𝑎𝑖 ⊗ 𝑎𝑖) ⊗ 𝑎𝑖 } computed
in the lifting step: we lift 𝑏𝑖, project it into the span to obtain a vector close to 𝑊(𝑎𝑖 ⊗ 𝑎𝑖) ⊗ 𝑎𝑖,
and reshape it to a 𝑑2 × 𝑑 matrix whose top right-singular vector is correlated with 𝑎𝑖. This
extraction-via-lifting step allows us to circumvent a loss of (cid:107)𝑊 −1(cid:107) in the error.

Algorithm 1 Sketch of full algorithm, in the absence of noise
Input: A 4-tensor T ∈ (ℝ𝑑)⊗4, so that T = (cid:205)𝑛

𝑖=1 𝑎⊗4
1. Take the square reshaping 𝑇 ∈ ℝ𝑑2×𝑑2 of T and compute its whitening 𝑊 = 𝑇−1/2 and

for unit vectors 𝑎𝑖 ∈ ℝ𝑑.

𝑖

the projector Π2 = 𝑊𝑇𝑊 to the image of 𝑇.

2. Lifting: Compute the lifted tensor T(cid:48) ∈ (ℝ𝑑2)⊗3 so that T(cid:48) = (cid:205)𝑖(𝑊 𝑎⊗2
𝑖

Algorithm 2 for full details).

)⊗3.

(See

(a) Find a basis for the subspace 𝑆3 = (img 𝑇) ⊗ ℝ𝑑 ∩ sym: take 𝑆3 to be the top-𝑛
eigenspace of (Π2 ⊗ Id)Πsym(Π2 ⊗ Id). Then by Lemma 2.1, 𝑆3 = Span(𝑎⊗3

) .

𝑖

(b) Find the projector Π3 to the space (𝑊 ⊗ Id) 𝑆3 = Span(𝑊 𝑎⊗2
(c) Compute the orthogonal 3-tensor: since {𝑊 𝑎⊗2

𝑖

⊗ 𝑎𝑖).

⊗ 𝑎𝑖 } is an orthonormal basis,

𝑖

Π3 =

(cid:213)

𝑖

(𝑊 𝑎⊗2

𝑖 ⊗ 𝑎𝑖)(𝑊 𝑎⊗2

𝑖 ⊗ 𝑎𝑖)T .

Therefore, reshape Π3 as (cid:205)𝑖(𝑊 𝑎⊗2
third mode to obtain T(cid:48).

𝑖

) ⊗ (𝑊 𝑎⊗2

𝑖

) ⊗ (𝑎⊗2

𝑖

) and multiply 𝑊 into the

3. Rounding: Use Gaussian rounding to ﬁnd the components 𝑎𝑖. (In the presence of

noise, this step becomes substantially more delicate; see Algorithms 3 to 5).

(a) Compute a random ﬂattening of T(cid:48) by contracting with 𝑔 ∼ 𝒩(0, Id𝑑2) along the
)(cid:105) · (𝑊 𝑎⊗2

ﬁrst mode, 𝑇(cid:48)(𝑔) = (cid:205)𝑖 (cid:104)𝑔, (𝑊 𝑎⊗2

)(𝑊 𝑎⊗2

)(cid:62)

𝑖

𝑖

(b) Perform an SVD on 𝑇(cid:48)(𝑔) to recover the eigenvectors (𝑊 𝑎⊗2
1
, and re-shape 𝑎⊗2

(c) Apply 𝑊 −1 to each eigenvector to obtain the 𝑎⊗2

), . . . , (𝑊 𝑎⊗2

𝑛 ).

to a matrix

𝑖

𝑖

𝑖

and compute its eigenvector to obtain 𝑎𝑖.

Organization.

The full implementation details and the analysis of our algorithm are given in the
following few sections. First, Section 4 sets up some primitives for spectral subspace

8This is in contrast to (cid:107)𝑊 (cid:107), which is 𝑂(1) in the random case.

13

perturbation analysis and linear-algebraic procedures on which we build the full algorithm
and its analysis. Then Section 5 covers the lifting stage of the algorithm in detail, while
Section 6 elaborates on the rounding stage. Finally, in Section 7 we combine these tools to
prove Theorem 7.3. The appendices some linear-algebraic tools and simulations strongly
suggesting that random tensors with 𝑛 (cid:28) 𝑑2 components have constant condition number
𝜅.

3 Preliminaries

Linear algebra We use Id𝑑 to denote the 𝑑 × 𝑑 identity matrix, or just Id if the dimension is
clear from context. For any subspace 𝑆, we use Π𝑆 to denote the projector to that subspace.
For 𝑀 a matrix, img(𝑀) refers to the image, or columnspace, of 𝑀.

We will, in a slight abuse of notation, use 𝑀−1 to denote the Moore-Penrose pseudo-
inverse of 𝑀. Except where explicitly speciﬁed, this will never be assumed to be equal to
the proper inverse, so that, e.g., in general 𝑀𝑀−1 = Πimg(𝑀) ≠ Id and (𝐴𝐵)−1 ≠ 𝐵−1𝐴−1.

For a matrix 𝐵 ∈ ℝ𝑚×𝑛, we will use the whitening matrix 𝑊 = (𝐵𝐵)−1/2, which maps

the columns of 𝐵 to an orthonormal basis for img(𝐵), so that (𝑊 𝐵)(𝑊 𝐵)(cid:62) = Πimg(𝐵).

We denote by sym ⊆ (ℝ𝑑)⊗3 the linear subspace

sym = Span({𝑥 ⊗ 𝑥 ⊗ 𝑥}𝑥∈ℝ𝑑) .

Note that (Πsym)(𝑖,𝑗,𝑘);(𝑖(cid:48),𝑗(cid:48),𝑘(cid:48)) is (|{𝑖, 𝑗, 𝑘}|!)−1 when {𝑖, 𝑗, 𝑘} = {𝑖(cid:48), 𝑗(cid:48), 𝑘(cid:48)} and zero otherwise.

Tensor manipulations When working with tensors 𝑇 ∈ (ℝ𝑑)⊗𝑘, we will sometimes
reshape the tensors to lower-order tensors or matrices; in this case, if 𝑆1, . . . , 𝑆𝑚 are a
partition of 𝑘, then 𝑇(𝑆1,...,𝑆𝑚) is the tensor given by identifying the modes in each 𝑆𝑖 into
a single mode. For 𝑆 ⊂ [𝑑]𝑘, we will also sometimes use the notation 𝑇(𝑆) to refer to the
entry of 𝑇 indexed by 𝑆.

A useful property of matrix reshapings is that 𝑢 ⊗ 𝑣 reshapes into the outer product
𝑢𝑣T. Linearity allows us to generalize this so, e.g., the reshaping of (𝑈 ⊗ 𝑉)𝑀 for 𝑈 ∈ ℝ𝑛×𝑛
and 𝑉 ∈ ℝ𝑚×𝑚 and 𝑀 ∈ ℝ(𝑛⊗𝑚)×𝑞 is equal to 𝑈 𝑀(cid:48)(𝑉 ⊗ Id𝑞), where 𝑀(cid:48) ∈ ℝ𝑛×(𝑚⊗𝑞) is the
reshaping of 𝑀. Since reshapings can be easily done and undone by exchanging indices,
these identities will sometimes allow more eﬃcient computation of matrix products over
tensor spaces.

We will on occasion use a · as a placeholder in a partially applied multiple-argument

function: for instance 𝜕
𝜕𝑦

𝑓 (·, 𝑦) = limℎ→0

1
ℎ ( 𝑓 (·, 𝑦 + ℎ) − 𝑓 (·, 𝑦)).

14

4 Tools for analysis and implementation

In this section, we brieﬂy introduce some tools which we will use often in our analysis.

4.1 Robustness and spectral perturbation

A key tool in our analysis of the robustness of Algorithm 1 comes from the theory of the
perturbation of eigenvalues and eigenvectors.

The lemma below combines the Davis-Kahan sin-Θ theorem with Weyl’s inequality to

characterize how top eigenspaces are aﬀected by spectral perturbation.

Theorem 4.1 (Perturbation of top eigenspace). Suppose 𝑄 ∈ ℝ𝐷×𝐷 is a symmetric matrix
with eigenvalues 𝜆1 (cid:62) 𝜆2 (cid:62) . . . (cid:62) 𝜆𝐷. Suppose also (cid:101)𝑄 ∈ ℝ𝐷×𝐷 is a symmetric matrix with
(cid:107)𝑄 − (cid:101)𝑄 (cid:107) (cid:54) 𝜀. Let 𝑆 and (cid:101)𝑆 be the spaces generated by the top 𝑛 eigenvectors of 𝑄 and (cid:101)𝑄 respectively.
Then,

sin(𝑆, (cid:101)𝑆) def

= (cid:107)Π𝑆 − Π

(cid:101)𝑆Π𝑆 (cid:107) = (cid:107)Π

(cid:101)𝑆 − Π𝑆Π

(cid:101)𝑆 (cid:107) (cid:54)

Consequently,

(cid:107)Π𝑆 − Π

(cid:101)𝑆 (cid:107) (cid:54)

2𝜀
𝜆𝑛 − 𝜆𝑛+1 − 2𝜀

.

𝜀
𝜆𝑛 − 𝜆𝑛+1 − 2𝜀

.

(4.1)

(4.2)

Proof. We ﬁrst prove the theorem assuming that 𝑄 and (cid:101)𝑄 are symmetric. By Weyl’s
inequality for matrices [Wey12], the 𝑛th eigenvalue of (cid:101)𝑄 is at least 𝜆𝑛 − 2𝜀. By Davis and
Kahan’s sin-Θ theorem [DK70], since the top-𝑛 eigenvalues of (cid:101)𝑄 are all at least 𝜆𝑛 − 2𝜀
and the lower-than-𝑛 eigenvalues of 𝑄 are all at most 𝜆𝑛+1, the sine of the angle between
𝑆 and (cid:101)𝑆 is at most (cid:107)𝑄 − (cid:101)𝑄 (cid:107)/(𝜆𝑛 − 𝜆𝑛+1 − 2𝜀). The ﬁnal bound on (cid:107)Π𝑆 − Π
(cid:101)𝑆 (cid:107) follows by
triangle inequality.

(cid:3)

4.2 Eﬃcient implementation and runtime analysis
It is not immediately obvious how to implement Algorithm 1 in time ˜𝑂(𝑛2𝑑3), since there
are steps that require we multiply or eigendecompose 𝑑3 × 𝑑3 matrices, which if done
naively might take up to Ω(𝑑9) time.

To accelerate our runtime, we must take advantage of the fact that our matrices have
additional structure. We exploit the fact that in certain reshapings our tensors have low-rank
representations. This allows us to perform matrix multiplication and eigendecomposition
(via power iteration) eﬃciently, and obtain a runtime that is depends on the rank rather
than on the dimension.

For example, the following lemma, based upon a result of [AZL16], captures our

eigendecomposition strategy in a general sense.

15

Lemma 4.2 (Implicit gapped eigendecomposition). Suppose a symmetric matrix 𝑀 ∈ ℝ𝑑×𝑑
has an eigendecomposition 𝑀 = (cid:205)𝑗 𝜆𝑗 𝑣 𝑗𝑣 𝑗T, and that 𝑀𝑥 may be computed within 𝑡 time steps for
𝑥 ∈ ℝ𝑑. Then 𝑣1, . . . , 𝑣𝑛 and 𝜆1, . . . , 𝜆𝑛 may be computed in time ˜𝑂(min(𝑛(𝑡 + 𝑛𝑑)𝛿−1/2, 𝑑3)),
where 𝛿 = (𝜆𝑛 − 𝜆𝑛+1)/𝜆𝑛. The dependence on the desired precision is polylogarithmic.

Proof. The 𝑛(𝑡 + 𝑛𝑑)𝛿−1/2 runtime is attained by LazySVD in [AZL16, Corollary 4.3]. While
LazySVD’s runtime depends on nnz(𝑀) where nnz denotes the number of non-zero
elements in the matrix, in the non-stochastic setting nnz(𝑀) is used only as a bound on the
time cost of multiplying a vector by 𝑀, so in our case we may substitute 𝑂(𝑡) instead.

The 𝑑3 time is attained by iterated squaring of 𝑀: in this case, all runtime dependence
(cid:3)

on condition numbers is polylogarithmic.

The following lemma lists some primitives for operations with the tensor T(cid:48) ∈ (ℝ𝑑2)⊗3
in Algorithm 1, by interpreting it as a 6-tensor in (ℝ𝑑)⊗6 and using a low-rank factorization
of the square reshaping of that 6-tensor.
Lemma 4.3 (Implicit tensors). For a tensor T ∈ (ℝ[𝑑]2)⊗3, suppose that the matrix 𝑇 ∈ ℝ[𝑑]3×[𝑑]3
given by 𝑇(𝑖,𝑖(cid:48),𝑗),(𝑘,𝑘(cid:48),𝑗(cid:48)) = T(𝑖,𝑖(cid:48)),(𝑗,𝑗(cid:48)),(𝑘,𝑘(cid:48)) has a rank-𝑛 decomposition 𝑇 = 𝑈𝑉T with 𝑈 , 𝑉 ∈ ℝ𝑑3×𝑛
and 𝑛 (cid:54) 𝑑2. Such a rank decomposition provides an implicit representation of the tensor T. This
implicit representation supports:
Tensor contraction: For vectors 𝑥, 𝑦 ∈ ℝ[𝑑]2, the computation of (𝑥T ⊗ 𝑦T ⊗Id)T or (𝑥T ⊗Id⊗ 𝑦T)T

or (Id ⊗ 𝑥T ⊗ 𝑦T)T in time 𝑂(𝑛𝑑3) to obtain an output vector in ℝ𝑑2.

Spectral truncation: For 𝑅 ∈ ℝ𝑑2×𝑑4 equal to one of the two matrix reshapings 𝑇{1,2}{3} or
(cid:54)1, deﬁned as T after all larger-than-1 singular
𝑇{2,3}{1} of T, an approximation to the tensor T
values in its reshaping 𝑅 are truncated down to 1. Speciﬁcally, letting 𝜌𝑘 be the 𝑘th largest
singular value of 𝑅 for 𝑘 (cid:54) 𝑂(𝑛), this returns an implicit representation of a tensor T(cid:48) such
(cid:54)1(cid:107)𝐹 (cid:54) (1 + 𝛿)𝜌𝑘 (cid:107)T(cid:107)𝐹 and the reshaping of T(cid:48) corresponding to 𝑅 has largest
that (cid:107)T(cid:48) − T
singular value no more than 1 + (1 + 𝛿)𝜌𝑘. The representation of T(cid:48) also supports the tensor
contraction, spectral truncation, and implicit matrix multiplication operations, with no more
than a constant factor increase in runtime. This takes time ˜𝑂(𝑛2𝑑3 + 𝑘(𝑛𝑑3 + 𝑘𝑑2)𝛿−1/2).

Implicit matrix multiplication: For a matrix 𝑅 ∈ ℝ[𝑑]2×[𝑑]2 with rank at most 𝑂(𝑛), an implicit
representation of the tensor (𝑅T ⊗ Id ⊗ Id)T or (Id ⊗ Id ⊗ 𝑅T)T, in time 𝑂(𝑛𝑑4). This output
also supports the tensor contraction, spectral truncation, and implicit matrix multiplication
operations, with no more than a constant factor increase in runtime. Multiplication into the
second mode (Id ⊗ 𝑅T ⊗ Id)T may also be implicitly represented, but without support for the
spectral truncation operation.

The implementation of these implicit tensor operations consists solely of tensor reshap-
ings, singular value decompositions, and matrix multiplication. However, the details get
involved and lengthy, and so we defer their exposition to Section A.

16

5 Lifting

This section presents Algorithm 2, which lifts a well-conditioned 4-tensor T of rank at
most 𝑑2 in (ℝ𝑑)⊗3 to T(cid:48), an orthogonalized version of the 6-tensor in the same components
in (ℝ𝑑2)⊗3; that is, we obtain an orthogonal 3-tensor T(cid:48) whose components correspond to
the orthogonalized Kronecker squares of the components of T. Section 5.1 presents the
identiﬁability argument giving robust algebraic non-degeneracy conditions under which
the algorithm succeeds.

Although we assume that the tensor components 𝑎𝑖 are unit vectors, throughout this
section we will keep track of factors of (cid:107)𝑎𝑖 (cid:107) so as to better elucidate the scaling and
dimensional analysis.

Algorithm 2 Function lift(T, 𝑛)
Input: T ∈ (ℝ𝑑)⊗4, 𝑛 ∈ ℕ with 𝑛 (cid:54) 𝑑2.

1. Use Lemma 4.2 to ﬁnd the top-𝑛 eigenvalues and corresponding eigenvectors of the
square matrix reshaping of T, and call the eigendecomposition 𝑇 = 𝑄Λ𝑄T. This also
yields 𝑊 = 𝑄Λ−1/2𝑄T and Π𝑆 = 𝑄𝑄T.

2. Use Lemma 4.2 again to ﬁnd the top-𝑛 eigendecomposition of Π𝑆⊗ℝ𝑑ΠsymΠ𝑆⊗ℝ𝑑, im-
plementing multiplication by Π𝑆⊗ℝ𝑑 as (Π𝑆⊗ℝ𝑑 𝑣)(·,·,𝑖) = 𝑄𝑄T𝑣(·,·,𝑖) and implementing
Πsym as a sparse matrix. Call the result 𝑅Σ𝑅T and take Π𝑆3 = 𝑅𝑅T.

3. Find a basis 𝐵(cid:48) for the columnspace of 𝑀3 = (𝑊 ⊗ Id)Π𝑆3(𝑊 ⊗ Id). Implement this as

(𝐵(cid:48))( · , · ,𝑖); · = 𝑄Λ−1/2𝑄T𝑅( · , · ,𝑖); · .

4. Use Gram-Schmidt orthogonalization to ﬁnd an orthonormalization 𝐵 of 𝐵(cid:48). Call the

projection operator to this basis Π3 = 𝐵𝐵T.

5. Instantiate an implicit tensor in (ℝ𝑑2)⊗3 with Lemma 4.3, using 𝐵𝐵T as the SVD of its
underlying 𝑑3 × 𝑑3 reshaping. Output this as (Id ⊗ 𝑊 −1 ⊗ Id)T(cid:48), meaning a tensor
which, when 𝑊 −1 is multiplied into its second mode, becomes equal to T(cid:48).

Output: (Id ⊗ 𝑊 −1 ⊗ Id)T(cid:48) ∈ (ℝ𝑑2)⊗3, implicitly as speciﬁed by Lemma 4.3, and Π3 ∈ ℝ𝑑3×𝑑3.

The following two lemmas will argue that the algorithm is correct, and that it is fast.
First, Lemma 5.1 states that the output of Algorithm 2 is an orthogonal 3-tensor whose
components are 𝑊(𝑎𝑖 ⊗ 𝑎𝑖), where the 𝑎𝑖 are the components of the original 4-tensor and
𝑊 is the whitening matrix for the 𝑎𝑖 ⊗ 𝑎𝑖. Furthermore, if the error in the input is small
in spectral norm compared to some condition numbers, the Frobenius norm error in the
output robustly remains within a small constant of

√

𝑛.

17

The main work of the lemma is deferred to Lemma 5.5 in Section 5.2, which repeatedly
applies Davis and Kahan’s sin-Θ theorem (Theorem 4.1) to say that the top eigenspaces
of various matrices in the algorithm are relatively unperturbed in spectral norm by small
spectral norm error in the matrices. After that, we simply bound the Frobenius norm error
of a rank-𝑛 matrix by 2
𝑛 times its spectral norm error, and reason that Frobenius norms
are unchanged by tensor reshapings.

√

Lemma 5.1 (Correctness of lift). Let 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑 and suppose that (cid:101)T = (cid:205)𝑖∈[𝑛] 𝑎⊗4
𝑖
𝑛𝜇−1𝜅2 for some 𝜀 < 1/63, where 𝜎𝑛 is the 𝑛th eigenvalue of (cid:205)𝑖∈[𝑛] 𝑎⊗2
satiﬁes (cid:107)𝐸12;34(cid:107) (cid:54) 𝜀 𝜎2
𝑖
and 𝜇 is the operator norm of (cid:205) (cid:107)𝑎𝑖 (cid:107)−2𝑎⊗3
also 𝑊 = (cid:2)(cid:205)𝑖∈[𝑛](𝑎⊗2
)(𝑎⊗2
𝑖
(Id ⊗ 𝑊 −1 ⊗ Id)(cid:101)T(cid:48) and (cid:101)Π3 of lift((cid:101)T, 𝑛) in Algorithm 2 satisfy

+ E
𝑎⊗2
T
𝑖
T and 𝜅 is the condition number from Lemma 5.3. Let
)T + 𝐸12;34(cid:3) −1/2 Then the outputs

𝑎⊗3
𝑖
)T(cid:3) −1/2 and (cid:101)𝑊 = (cid:2)(cid:205)𝑖∈[𝑛](𝑎⊗2

)(𝑎⊗2
𝑖

𝑖

𝑖

𝑖

(cid:13)
(cid:13)
(cid:48) − (cid:213)
(cid:13)
(cid:101)T
(cid:13)
(cid:13)

𝑖

(cid:107)𝑎𝑖 (cid:107)−2(𝑊(𝑎𝑖 ⊗ 𝑎𝑖))⊗3

(cid:54) 126 𝜀 𝜎−1/2

𝑛

√

𝑛

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)𝐹

and

(cid:13)
(cid:13)(cid:101)Π3 − ΠSpan(𝑊 𝑎⊗2
(cid:13)

𝑖

(cid:13)
(cid:13)
(cid:13)

⊗𝑎

𝑖)

(cid:54) 63 𝜀 .

Proof. We refer to all matrices and spaces computed in the algorithm with an overset tilde to
reﬂect the fact that the algorithm only has access to approximations with error (so (cid:101)𝑆 instead
computed in step 2 as the projector
of 𝑆 in the algorithm, etc.). By Lemma 5.5, the Π
)(cid:107) (cid:54) 18 𝜀𝜎𝑛𝜇−1, and
to the top-𝑛 eigenspace of Π
subsequently, the (cid:101)Π3 computed in steps 3 and 4 as the projector to ( (cid:101)𝑊 ⊗ Id)(cid:101)𝑆3 satisﬁes
(cid:107) (cid:101)Π3 − ΠSpan(𝑊 𝑎⊗2

(cid:101)𝑆⊗ℝ𝑑 satisﬁes (cid:107)Π

(cid:101)𝑆⊗ℝ𝑑ΠsymΠ

− ΠSpan(𝑎⊗3

)(cid:107) (cid:54) 63 𝜀.

(cid:101)𝑆3

(cid:101)𝑆3

⊗𝑎

𝑖

𝑖

𝑖

Since the rank of the error is at most 2𝑛, the Frobenius norm error is at most 126 𝜀

√

since {(cid:107)𝑎𝑖 (cid:107)−1𝑊 𝑎⊗2
is just the sum of the self-outer-products of vectors in that set, so

⊗ 𝑎𝑖 } is an orthonormal set of vectors, the projector to Span(𝑊 𝑎⊗2

𝑖

𝑖

𝑛, and
⊗ 𝑎𝑖)

(cid:13)
(cid:13)

(cid:13)(cid:101)Π3 − (cid:213) (cid:107)𝑎𝑖 (cid:107)−2(𝑊 𝑎⊗2

𝑖 ⊗ 𝑎𝑖)(𝑊 𝑎⊗2

𝑖 ⊗ 𝑎𝑖)T(cid:13)
(cid:13)
(cid:13)𝐹

(cid:54) 126 𝜀

√

𝑛 .

Reshaping the 𝑑3 × 𝑑3 matrix (cid:101)Π3 into a tensor in (ℝ𝑑2)⊗3 does not change the Frobenius
norm error, and ﬁnally, multiplying in the last factor of 𝑊 may contribute a factor of
, so that in the end, (cid:107)(cid:101)T(cid:48) − (cid:205)𝑖 (cid:107)𝑎𝑖 (cid:107)−2(𝑊(𝑎𝑖 ⊗ 𝑎𝑖))⊗3(cid:107)𝐹 (cid:54) 126 𝜀 𝜎−1/2
(cid:107)𝑊 (cid:107) = 𝜎−1/2
(cid:3)
The next lemma states that the running time is ˜𝑂(𝑛2𝑑3) multiplied by some condition
numbers. We assume that asympotically faster matrix multiplications and pseudo-
inversions are not used, so that, for instance, squaring a 𝑑 × 𝑑 matrix takes time Θ(𝑑3).

𝑛.

√

𝑛

𝑛

18

Lemma 5.2 (Running time of lift). Let 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑 and suppose that T = (cid:205)𝑖∈[𝑛] 𝑎⊗4
+ E
𝑖
satisﬁes the conditions stated in Lemma 5.1. Let 𝜎𝑛 be the 𝑛th eigenvalue of (cid:205)𝑖∈[𝑛] 𝑎⊗2
𝑎⊗2
T
𝑖
and 𝜅 the condition number from Lemma 5.3. Then lift(T, 𝑛) in Algorithm 2 runs in time
˜𝑂(𝑛𝑑4𝜎−1/2

+ 𝑛2𝑑3𝜅−1), and the eﬃcient implementation steps are correct.

𝑖

𝑛

Proof. Step 1 of lift invokes Lemma 4.2 on a 𝑑2 × 𝑑2 matrix 𝑇, recovering 𝑛 dimensions
with a spectral gap of 𝛿 = 𝜎𝑛. This requires time ˜𝑂((𝑛𝑑4 + 𝑛2𝑑2)𝜎−1/2

).

𝑛

Step 2 again invokes Lemma 4.2, this time on a 𝑑3 × 𝑑3 matrix Π𝑆⊗ℝ𝑑ΠsymΠ𝑆⊗ℝ𝑑,
recovering 𝑛 dimensions with a spectral gap of at least 𝜅2 − 2𝜀 ∈ Ω(𝜅2). Multiplying by
Π𝑆⊗ℝ𝑑 may be done in time 𝑂(𝑛𝑑3) due to its expression as (Π𝑆⊗ℝ𝑑 𝑣)(·,·,𝑖) = 𝑄𝑄T𝑣(·,·,𝑖), since
the third mode of (ℝ𝑑)⊗3 is unaﬀected by Π𝑆⊗ℝ𝑑 = 𝑄𝑄T ⊗ Id, and this is a concatenation of
𝑑 diﬀerent matrix-vector multiplies that take 𝑂(𝑛𝑑2) time each. Multiplying by Πsym takes
𝑂(𝑑3) time, since the (𝑖, 𝑗, 𝑘)th row of Πsym has at most 6 nonzero entries corresponding
to the diﬀerent permutations of (𝑖, 𝑗, 𝑘). Thus the overall time to multiply a vector by
Π𝑆⊗ℝ𝑑ΠsymΠ𝑆⊗ℝ𝑑 is 𝑂(𝑛𝑑3), so that Lemma 4.2 gives a runtime of ˜𝑂((𝑛2𝑑3 + 𝑛2𝑑3)𝜅−1) for
this step.

Step 3 is a concatenation of 𝑑 diﬀerent matrix products, each of which involves
multiplying a 𝑑2 × 𝑛 matrix 𝑅(·,·,𝑖);· by a 𝑛 × 𝑑2 matrix Λ1/2𝑄T and then multiplying the
resulting 𝑛 × 𝑛 matrix by a 𝑑2 × 𝑛 matrix 𝑄. Each product thus takes 𝑂(𝑛2𝑑2) time,
and since there are 𝑑 of them the entire step takes 𝑂(𝑛2𝑑3) time. The result is equal
to (𝑄Λ1/2𝑄T ⊗ Id𝑑)𝑅 = (𝑊 ⊗ Id)𝑅, whose columns form a basis for the columnspace of
𝑀3 = (𝑊 ⊗ Id)𝑅Σ𝑅T(𝑊 ⊗ Id).

Step 4 applies Gram-Schmidt orthonormalization on 𝑛 vectors in ℝ𝑑3, taking ˜𝑂(𝑛2𝑑3)
+ 𝑛2𝑑3𝜅−1). (cid:3)

time. And step 5 takes constant time. Therefore, lift takes time ˜𝑂(𝑛𝑑4𝜎−1/2

𝑛

5.1 Algebraic identiﬁability argument

The main lemma in this section gives a more careful analysis of the algebraic identiﬁability
argument from Lemma 2.1, in order to obtain a quantitative condition number bound.

Lemma 5.3 (Main Identiﬁability Lemma). Let 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑 with 𝑛 (cid:54) 𝑑2. Let 𝑆 denote
Span({𝑎⊗2
}) and assume both have dimension 𝑛. For each 𝑖, let
{𝑏𝑖,𝑗 } 𝑗∈[𝑑−1] be an arbitrary orthonormal basis for vectors in ℝ𝑑 orthogonal to 𝑎𝑖, and let

}) and let 𝑆3 denote Span({𝑎⊗3

𝑖

𝑖

𝐻T :=

𝑎1 ⊗ 𝑎1 ⊗ 𝑏1,1
...
𝑎𝑖 ⊗ 𝑎𝑖 ⊗ 𝑏𝑖,𝑗
...
𝑎𝑛 ⊗ 𝑎𝑛 ⊗ 𝑏𝑛,𝑑−1













.













19

Let 𝑅 = (𝐻𝐻T)−1/2𝐻 be a column-wise orthonormalization of 𝐻, and let 𝐾 = 1
2(Id − 𝑃2,3)𝑅,
where 𝑃2,3 is the permutation matrix that exchanges the 2nd and 3rd modes of (ℝ𝑑)⊗3. Then if
𝜅 = 𝜎min(𝐾) is non-zero (so that 𝐾 is full rank),

and furthermore,

(𝑆 ⊗ ℝ𝑑) ∩ sym = 𝑆3 ,

(cid:107)Π𝑆⊗ℝ𝑑ΠsymΠ𝑆⊗ℝ𝑑 − Π𝑆3 (cid:107) (cid:54) 1 − 𝜅2 .
Proof. Let 𝑊 = ((cid:205)𝑖 𝑎⊗2
T)−1/2 and let 𝑇 denote the columnspace of (𝑊 2 ⊗ Id)𝐻. The
columns of 𝑊 2𝐻 form a basis for the subspace of 𝑆 ⊗ ℝ𝑑 orthogonal to 𝑆3 since each column
of 𝑊 2𝐻 is orthogonal to every 𝑎⊗3

. Therefore,

𝑎⊗2
𝑖

𝑖

𝑖

Π𝑆⊗ℝ𝑑 = Π𝑆3 + Π𝑇 .

Multiplying this with Πsym and itself and then applying the identities ΠsymΠ𝑆3 = Π𝑆3Πsym =
Π𝑆3 and Π𝑆3Π𝑇 = Π𝑇Π𝑆3 = 0,

Therefore,

Π𝑆⊗ℝ𝑑ΠsymΠ𝑆⊗ℝ𝑑 = Π𝑆3 + Π𝑇ΠsymΠ𝑇 .

(cid:107)Π𝑆⊗ℝ𝑑ΠsymΠ𝑆⊗ℝ𝑑 − Π𝑆3 (cid:107) (cid:54) (cid:107)ΠsymΠ𝑇 (cid:107)2 .

We would thus like to show that (cid:107)ΠsymΠ𝑇 (cid:107)2 (cid:54) 1 − 𝜅2.

Since (cid:107)ΠsymΠ𝑇 (cid:107)2 = max𝑦(cid:48)∈𝑇 (cid:107)Πsym𝑦(cid:48)(cid:107)2/(cid:107)𝑦(cid:48)(cid:107)2 = 1 − min𝑦(cid:48)∈𝑇 (cid:107)(Id − Πsym)𝑦(cid:48)(cid:107)2/(cid:107) 𝑦(cid:48)(cid:107)2, it
is enough to show that min𝑦(cid:48)∈𝑇 (cid:107)(Id − Πsym)𝑦(cid:48)(cid:107)/(cid:107)𝑦(cid:48)(cid:107) (cid:62) 𝜅. By Lemma 5.4, that is implied
by (cid:107)(Id − Πsym)𝑦(cid:107)/(cid:107) 𝑦(cid:107) (cid:62) 𝜅 for 𝑦 ∈ img(𝐻).

Since Πsym (cid:22) Π2,3 where Π2,3 is the projector to the space invariant under interchange
2(Id + 𝑃2,3), we see that (cid:107)(Id −
2(Id − 𝑃2,3)𝑦(cid:107)/(cid:107)𝑦 (cid:107) for 𝑦 ∈ img(𝐻). Since the columns of 𝑅 are an

of the second and third modes of (ℝ𝑑)⊗3 and Π2,3 = 1
Πsym)𝑦(cid:107)/(cid:107) 𝑦(cid:107) (cid:62) (cid:107) 1
orthonormal basis for img(𝐻), for 𝑥 = 𝑅−1𝑦 spanning all of ℝ𝑛 we have

(cid:107)(Id − 𝑃2,3)𝑦(cid:107)
2(cid:107) 𝑦(cid:107)

=

(cid:107)(Id − 𝑃2,3)𝑅𝑥 (cid:107)
2(cid:107)𝑅𝑥 (cid:107)

=

(cid:107)(Id − 𝑃2,3)𝑅𝑥(cid:107)
2(cid:107)𝑥(cid:107)

=

(cid:107)𝐾𝑥(cid:107)
(cid:107)𝑥(cid:107)

.

The expression on the right is the deﬁnition of 𝜅. Therefore, (cid:107)(Id − Πsym)𝑦(cid:107)/(cid:107) 𝑦(cid:107) (cid:62)
(cid:107) 1
(cid:3)
2(Id − 𝑃2,3)𝑦(cid:107)/(cid:107)𝑦 (cid:107) = 𝜅.

Lemma 5.4. For each 𝑖, let {𝑏𝑖,𝑗 } 𝑗∈[𝑑−1] be an arbitrary orthonormal basis for vectors in ℝ𝑑

20

orthogonal to 𝑎𝑖, and let

𝐻T :=

𝑎1 ⊗ 𝑎1 ⊗ 𝑏1,1
...
𝑎𝑖 ⊗ 𝑎𝑖 ⊗ 𝑏𝑖,𝑗
...
𝑎𝑛 ⊗ 𝑎𝑛 ⊗ 𝑏𝑛,𝑑−1













.













Let 𝐻(cid:48) = (𝑊 2⊗Id)𝐻. If (cid:107)(Id−Πsym)𝑦(cid:107) (cid:62) 𝑡 (cid:107) 𝑦(cid:107) for all 𝑦 ∈ img(𝐻), then (cid:107)(Id−Πsym)𝑦(cid:48)(cid:107) (cid:62) 𝑡 (cid:107)𝑦(cid:48)(cid:107)
for all 𝑦(cid:48) ∈ img(𝐻(cid:48)).

) and 𝑆3 = Span(𝑎⊗3

Proof. Let 𝑆 = Span(𝑎⊗2
) ⊆ sym. Observe that img(𝐻(cid:48)) ⊆ 𝑆 ⊗ ℝ𝑑 =
img(𝐻) + 𝑆3. Therefore, for every 𝑦(cid:48) ∈ img(𝐻(cid:48)) there will be some 𝑦 ∈ img(𝐻) and some
𝑧 ∈ 𝑆3 such that 𝑦(cid:48) = 𝑦 + 𝑧. Also, since 𝑆3 ⊥ img(𝐻(cid:48)), we have 𝑧 ⊥ 𝑦(cid:48), and therefore
(cid:107)𝑦(cid:107) = (cid:107)𝑦(cid:48) − 𝑧 (cid:107) (cid:62) (cid:107) 𝑦(cid:48)(cid:107).

𝑖

𝑖

So if the premise of the lemma holds and (cid:107)(Id − Πsym)𝑦(cid:107) (cid:62) 𝑡 (cid:107) 𝑦(cid:107) for all 𝑦 ∈ img(𝐻), it
(cid:3)

will also be the case that (cid:107)(Id − Πsym)𝑦(cid:48)(cid:107) = (cid:107)(Id − Πsym)(𝑦 + 𝑧)(cid:107) (cid:62) 𝑡 (cid:107) 𝑦(cid:107) (cid:62) 𝑡 (cid:107)𝑦(cid:48)(cid:107).

5.2 Robustness arguments

The main lemma of this section gives all of the spectral eigenspace perturbation arguments
needed to argue the correctness and robustness of Algorithm 2. Here we essentially
repeatedly apply Davis and Kahan’s sin-Θ theorem (Theorem 4.1) through a sequence
of linear algebraic transformations, along with triangle inequality and some adding-
and-subtracting, to argue that the desired top eigenspace remains stable against the
spectral-norm errors melded in at each step.

Lemma 5.5 (Subspace perturbation for lift). Let 𝑇 = (cid:205)𝑖∈[𝑛] 𝑎⊗2
𝑖
with (cid:107)𝑇 − (cid:101)𝑇 (cid:107) (cid:54) 𝜀 𝜎2
the operator norm of (cid:205) (cid:107)𝑎𝑖 (cid:107)−2𝑎⊗3
𝑆 = Span({𝑎⊗2

T and let (cid:101)𝑇 be a matrix
𝑛𝜇−1𝜅2 for some 𝜀 < 1/63, where 𝜎𝑛 is the 𝑛th eigenvalue of 𝑇 and 𝜇 is
T and 𝜅 is the condition number from Lemma 5.3. Let

}). Then

𝑎⊗3
𝑖

𝑎⊗2
𝑖

𝑖

𝑖

}) = img(𝑇) and let (cid:101)𝑆 = img((cid:101)𝑇). Also let 𝑆3 = Span({𝑎⊗3
(cid:101)𝑆⊗ℝ𝑑 (cid:1) − Π𝑆3 (cid:107) (cid:54) 18 𝜀 𝜎𝑛𝜇−1 ,
where top𝑛 denotes the top-𝑛 eigenspace. Furthermore, letting (cid:101)𝑆3 = top𝑛(Π
𝑊 = 𝑇−1/2 and (cid:101)𝑊 = (cid:101)𝑇−1/2, we have

(cid:101)𝑆⊗ℝ𝑑ΠsymΠ

(cid:107) top𝑛

(cid:0)Π

𝑖

(cid:101)𝑆⊗ℝ𝑑ΠsymΠ

(cid:101)𝑆⊗ℝ𝑑) and

(cid:107)Π( (cid:101)𝑊 ⊗Id)(cid:101)𝑆3

− Π(𝑊 ⊗Id)𝑆3

(cid:107) (cid:54) 63 𝜀 .

Proof. For brevity, let Π = Π𝑆⊗ℝ𝑑 and let (cid:101)Π = Π

(cid:101)Π Πsym (cid:101)Π − Π Πsym Π = (cid:0)

(cid:101)Π − Π(cid:1) .

(5.1)

(cid:101)𝑆⊗ℝ𝑑. We write
(cid:101)Π − Π(cid:1) Πsym (cid:101)Π + Π Πsym (cid:0)

21

Since (cid:107)𝑇 − (cid:101)𝑇 (cid:107) (cid:54) 𝜀 𝜎2
projectors don’t increase spectral norm, we conclude

𝑛𝜇−1𝜅2, by Theorem 4.1, (cid:107) (cid:101)Π − Π(cid:107) = (cid:107)Π

(cid:101)𝑆 − Π𝑆 (cid:107) (cid:54) 3𝜀𝜎𝑛𝜇−1𝜅2. Since

(cid:107) (cid:101)Π Πsym (cid:101)Π − Π Πsym Π(cid:107) (cid:54) 6𝜀𝜎𝑛𝜇−1𝜅2 .

Furthermore, by Lemma 5.3, Π Πsym Π = Π𝑆3 + 𝑍, where 𝑍 is a symmetric matrix with
(cid:107)𝑍(cid:107) (cid:54) 1 − 𝜅2 whose columnspace is orthogonal to 𝑆3 since Π𝑆3Π𝑆⊗ℝ𝑑ΠsymΠ𝑆⊗ℝ𝑑 = Π𝑆3.
Therefore,

(cid:107) (cid:101)ΠΠsym(cid:101)Π − (Π𝑆3 + 𝑍)(cid:107) (cid:54) 6𝜀𝜎𝑛𝜇−1𝜅2 .
The top-𝑛 eigenspace of (Π𝑆3 + 𝑍) is 𝑆3 and the 𝑛th and (𝑛 + 1)th eigenvalues of (Π𝑆3 + 𝑍)
diﬀer by at least 𝜅2. So by Theorem 4.1,

(cid:107) top𝑛((cid:101)ΠΠsym(cid:101)Π) − Π𝑆3 (cid:107) (cid:54) 18𝜀𝜎𝑛𝜇−1 .

Multiplying by 𝑊 multiplies this error by at most a factor of (cid:107)𝑊 (cid:107)2 = 𝜎−1

𝑛 , so that

(cid:107)(𝑊 ⊗ Id)Π

(cid:101)𝑆3

(𝑊 ⊗ Id) − (𝑊 ⊗ Id)Π𝑆3(𝑊 ⊗ Id)(cid:107) (cid:54) 18 𝜀𝜇−1 .

And (cid:107)( (cid:101)𝑊 ⊗ Id)Π
(5.1) since Π

(cid:101)𝑆3

(cid:101)𝑆3

( (cid:101)𝑊 ⊗ Id) − (𝑊 ⊗ Id)Π

(cid:101)𝑆3

(𝑊 ⊗ Id) has a spectral norm at most 𝜎−1/2

, so that

𝑛

(𝑊 ⊗ Id)(cid:107) (cid:54) 3𝜀 by a decomposition similar to

(cid:107)( (cid:101)𝑊 ⊗ Id)Π

(cid:101)𝑆3

( (cid:101)𝑊 ⊗ Id) − (𝑊 ⊗ Id)Π𝑆3(𝑊 ⊗ Id)(cid:107) (cid:54) 21 𝜀𝜇−1 .

By Lemma 5.6, the smallest eigenvalue of (𝑊 ⊗ Id)Π𝑆3(𝑊 ⊗ Id) is at least 𝜇−1. Therefore,
(cid:3)
(cid:107) (cid:54) 63 𝜀.
by Theorem 4.1, (cid:107)Π(𝑊 ⊗Id)(cid:101)𝑆3

− Π(𝑊 ⊗Id)𝑆3

𝑎⊗3
𝑖

𝑖

The following utility lemma is used to reduce the impact of condition numbers on
the algorithm. It shows that when multiplying a third-order tensor in the span of 𝑎⊗3
by the second-order whitener 𝑊 = 𝑇−1/2 = ((cid:205)𝑖 𝑎⊗2
T)−1/2, the penalty to the error
may be expressed in terms of a sixth-order condition number – the spectral norm of
𝑈 = (cid:205) (cid:107)𝑎𝑖 (cid:107)−2𝑎⊗3

T – instead of the fourth-order one given by 𝑇.

𝑎⊗2
𝑖

𝑖

𝑖

𝑎⊗2
𝑖

The reason this is important is that (cid:205)𝑖 𝑎⊗2

T suﬀers from spurious directions: direc-
𝑖
tions 𝑣 ∈ ℝ⊗2 in which 𝑇𝑣 may be very large, but 𝑣 is not close to any of the 𝑎𝑖 ⊗ 𝑎𝑖, or in
fact any rank-1 2-tensor at all. For example, for 𝑛 random Gaussian vectors, the spurious
direction is given by Φ = 𝔼𝑔∼ℕ(0,1) 𝑔 ⊗ 𝑔, which will have (cid:107)𝑇Φ(cid:107) ≈ 𝑛/𝑑.

The sixth-order object 𝑈 = (cid:205) (cid:107)𝑎𝑖 (cid:107)−2𝑎⊗3

𝑎⊗3
T does not suﬀer with this problem for 𝑛 up to
𝑖
˜𝑂(𝑛2), due to cancellation with the odd number of modes. For instance, 𝑈 𝔼𝑔∼ℕ(0,1) 𝑔⊗3 = 0
and (cid:107)𝑈(Φ ⊗ 𝑢)(cid:107) ≈ 𝑛/𝑑2 for all unit 𝑢 ∈ ℝ𝑑 and 𝑈 generated from random Gaussian vectors.
Lemma 5.6 (Sixth-order condition numbers). Let 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑 with 𝑛 (cid:54) 𝑑2. Let 𝑊 =
((cid:205)𝑖 𝑎⊗2
T. Then for a vector 𝑣 ∈

T)−1/2 have rank 𝑛. Let 𝑈 be the matrix (cid:205) (cid:107)𝑎𝑖 (cid:107)−2𝑎⊗3

𝑖

𝑎⊗2
𝑖

𝑖

𝑎⊗3
𝑖

𝑖

22

Span(𝑎⊗3

𝑖

), the following hold:

(cid:107)(𝑊 ⊗ Id)𝑣(cid:107) (cid:54) (cid:107)𝑈 −1(cid:107)1/2(cid:107)𝑣(cid:107) ,

(cid:107)(𝑊 ⊗ Id)𝑣(cid:107) (cid:62) (cid:107)𝑈 (cid:107)−1/2(cid:107)𝑣(cid:107) .

Proof. Let 𝑣 = (cid:205) 𝜇𝑖 (cid:107)𝑎𝑖 (cid:107)−1𝑎⊗3

𝑖

. Then

(cid:107)(𝑊 ⊗ Id)𝑣(cid:107)2 =

(cid:213) 𝜇𝑖𝜇𝑗 (cid:107)𝑎𝑖 (cid:107)−1(cid:107)𝑎 𝑗 (cid:107)−1(cid:104)𝑊(𝑎 𝑗 ⊗ 𝑎 𝑗), 𝑊(𝑎𝑖 ⊗ 𝑎𝑖)(cid:105)(cid:104)𝑎 𝑗 , 𝑎𝑖(cid:105) =

(cid:213) 𝜇2
𝑖 ,

using the fact that {𝑊(𝑎𝑖 ⊗ 𝑎𝑖)}𝑖 is an orthonormal set of vectors.

(cid:3)

6 Rounding

In this section, we show how to “round” the lifted tensor to extract the components. That
is, assuming we are given the tensor

𝑇 =

(cid:213)

𝑖∈[𝑛]

(𝑊 𝑎⊗2

𝑖

)⊗3 + 𝐸

where 𝐸 is a tensor of Frobenius norm at most 𝜀
𝑎𝑖.

√

𝑛, we show how to ﬁnd the components

Lemma 6.1. Suppose 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑 are unit vectors satisfying the identiﬁability assumption
from Lemma 5.1, and suppose we are given an implicit rank-𝑛 representation of the tensor
𝑇 = (cid:205)𝑖(𝑊 𝑎⊗2
𝑛, and an implicit rank-𝑛 representation of a
matrix Π3 such that (cid:107)Π3 − (cid:205)𝑖((𝑊 𝑎⊗2
) ⊗ 𝑎𝑖)(cid:62)(cid:107) (cid:54) 𝜀 < 1
2.

𝑖
Then for any 𝛽, 𝛿 ∈ (0, 1) so that 𝛽𝛿 = Ω(𝜀) and 𝛿 = Ω(𝜀), there is a randomized algorithm
𝛽 𝑛1+𝑂(𝛽)𝑑3) with ˜𝑂(𝑛2𝑑3) preprocessing time recovers a unit

)⊗3 + 𝐸 ∈ (ℝ𝑑2)⊗3, where (cid:107)𝐸(cid:107)𝐹 (cid:54) 𝜀

) ⊗ 𝑎𝑖)((𝑊 𝑎⊗2

√

𝑖

𝑖

that with high probability in time 𝑂( 1
vector 𝑢 such that for some 𝑖 ∈ [𝑛],

(cid:104)𝑎𝑖 , 𝑢(cid:105)2 (cid:62) 1 − (cid:107)𝑊 (cid:107) · 𝑂

(cid:19) 1/8

,

(cid:18) 𝜀
𝛽

so long as (cid:107)𝑊 (cid:107)

(cid:17) 1/8

(cid:16) 𝜀
𝛽

< 𝐶 for a universal constant 𝐶.

Further, there is an integer 𝑚 (cid:62) (1 − 𝛿)𝑛 so that repeating the above algorithm ˜𝑂(𝑛) times

recovers unit vectors 𝑢1, . . . , 𝑢𝑚 so that (cid:104)𝑢𝑖 , 𝑎𝑖(cid:105)2 (cid:62) 1 − (cid:107)𝑊 (cid:107) · 𝑂

(cid:17) 1/8

(cid:16) 𝜀
𝛿𝛽

for all 𝑖 ∈ [𝑚] (up to

re-indexing), again so long as (cid:107)𝑊 (cid:107)

(cid:17) 1/8

(cid:16) 𝜀
𝛿𝛽

< 𝐶, and with a total runtime of ˜𝑂( 1

𝛽 𝑛2+𝑂(𝛽)𝑑3).

23

We will prove this theorem in four steps. First, in Section 6.1 we will show how
to recover vectors that are (with reasonable probability) correlated with the whitened
Kronecker squares of the components, 𝑊 𝑎⊗2
. In Section 6.2, we’ll give an algorithm that
given a vector close to the whitened square 𝑊 𝑎⊗2
, recovers a vector close to the component
𝑎𝑖. In Section 6.3, we give an algorithm that tests if a vector 𝑎 ∈ ℝ𝑑 is close to one of the
components {𝑎𝑖 }𝑖∈[𝑛]. In these ﬁrst three sections, we omit runtime details; in Section 6.4
we put the arguments together and address runtime details as well.

𝑖

𝑖

6.1 Recovering candidate whitened and squared components

𝑖

Here, we give an algorithm for recovering components that have constant correlation with
the 𝑊 𝑎⊗2
. In this subsection, our result applies in generality to arbitrary orthonormal
vectors 𝑏1, . . . , 𝑏𝑛 ∈ ℝ𝑑2. The algorithm and its analysis follow almost directly from [SS17];
for completeness we re-state the important lemmas here, and detail what little adaptation
is necessary.

Algorithm 3 Rounding to a whitened component
Function round(𝑇, 𝛽, 𝜀):
Input: a tensor 𝑇 ∈ (ℝ𝑑2)⊗3, a spectral gap bound 𝛽, and an error tolerance 𝜀.

1. Decrease the spectral norm of the error term in rectangular reshapings:

(a) compute 𝑇(cid:48), the projection of 𝑇{1,2}{3} to 𝑂, the set of 𝑑4 × 𝑑2 matrices with

spectral norm at most 1

(b) compute 𝑇 (cid:54)1, the projection of 𝑇(cid:48)

{1,3}{2}

norm error).

to 𝑂 (may be done up to 𝜀

√

𝑛 Frobenius

2. Compute a random ﬂattening of 𝑇 (cid:54)1 along the {1} mode: for 𝑔 ∼ 𝒩(0, Id𝑑2), compute

𝑇(𝑔) =

(cid:213)

𝑖∈[𝑑2]

𝑔𝑖 · 𝑇(𝑖, ·, ·).

3. Recover candidate component vectors: compute 𝑢𝐿(𝑔) and 𝑢𝑅(𝑔), the top left- and

right-singular vectors of 𝑇(𝑔) using 𝑂( 1

𝛽 log 𝑑) steps of power iteration.

Output: the candidate components 𝑢𝐿(𝑔) and 𝑢𝑅(𝑔).

Lemma 6.2. Suppose that 𝑏1, . . . , 𝑏𝑛 ∈ ℝ𝑑2 are orthonormal. Then if 𝑇 = (cid:205)𝑖∈[𝑛] 𝑏⊗3
𝑖
tensor 𝐸 with (cid:107)𝐸(cid:107)𝐹 (cid:54) 𝜀
˜𝑂(𝑛𝑂(𝛽)) times will with high probability recover a unit vector 𝑢 such that (cid:104)𝑢, 𝑏𝑖(cid:105)2 (cid:62) 1 − 𝜀

+ 𝐸 for a
𝛿 ) (cid:54) 𝛽 < 1, repeating steps 2 & 3 of Algorithm 3
𝛿𝛽 for

𝑛 and 𝛿 = Ω(𝜀), Ω( 𝜀

√

24

some 𝑖 ∈ [𝑛]. Furthermore, repeating steps 2 & 3 of Algorithm 3 ˜𝑂(𝑛1+𝑂(𝛽)) times will with high
probability recover 𝑚 (cid:62) (1 − 𝛿) · 𝑛 unit vectors 𝑢1, . . . , 𝑢𝑚 such that for each 𝑢𝑖 there exists 𝑗 ∈ [𝑛]
so that (cid:104)𝑢𝑖 , 𝑏 𝑗(cid:105)2 (cid:62) 1 − 𝜀

𝛿𝛽 .9

The proof follows from two lemmas:

Lemma 6.3. The tensor 𝑇 (cid:54)1 computed in step 1 of Algorithm 3 remains close to 𝑆 = (cid:205)𝑖 𝑏⊗3
𝑖
Frobenius norm, (cid:107)𝑇 (cid:54)1 − 𝑆(cid:107)𝐹 (cid:54) 𝜀

𝑛, and furthermore

√

in

(cid:107)𝑇 (cid:54)1

{1,2}{3}

(cid:107) (cid:54) 1

and

(cid:107)𝑇 (cid:54)1

{1,3}{2}

(cid:107) (cid:54) 1.

The proof of Lemma 6.3 is identical to the proof of [SS17, Lemma 4.5], and uses the
fact that distances decrease under projection to convex sets to control the error, and the
fact that the truncation operation is equivalent to multiplication by a contractive matrix to
argue that 𝑇 (cid:54)1 has bounded norm in both reshapings.

√

(cid:107)𝐹 (cid:54) 𝜀

Lemma 6.4. Suppose that in spectral norm (cid:107)𝑇{1,2}{3} (cid:107), (cid:107)𝑇{1,3}{2} (cid:107) (cid:54) 1, and also that (cid:107)𝑇 −
(cid:205)𝑖 𝑏⊗3
𝑛. Let 𝑇(𝑔) be the random ﬂattening of 𝑇 produced in step 2 of Algorithm 3, and
𝑖
let 𝑢𝐿(𝑔) and 𝑢𝑅(𝑔) be the top left- and right-signular vectors of 𝑇(𝑔) respectively. Then there is a
universal constant 𝐶 such that for any 𝛿 > 𝐶 · 𝜀 and Ω( 𝜀
𝛿 ) (cid:54) 𝛽 < 1, for a 1 − 𝛿 fraction of 𝑗 ∈ [𝑛],

(cid:18)

ℙ
𝑔∼𝑁(0,Id)

(cid:104)𝑢𝐿(𝑔), 𝑏 𝑗(cid:105)2 (cid:62) 1 −

𝜀
𝛿𝛽

or

(cid:104)𝑢𝑅(𝑔), 𝑏 𝑗(cid:105)2 (cid:62) 1 −

(cid:19)

𝜀
𝛿𝛽

(cid:62) ˜Ω

(cid:16)

𝑛−1−𝑂(𝛽)(cid:17)

,

and further when this event occurs the ratio of the ﬁrst and second singular values of 𝑇(𝑔) is lower
bounded by 𝛽, 𝜎1(𝑇(𝑔))
𝜎2(𝑇(𝑔))

(cid:62) 1 + 𝛽.

√

Proof. By assumption, 𝑇 (cid:54)1 = 𝑆 + 𝐸 for 𝑆 = (cid:205)𝑖∈[𝑛] 𝑏⊗3
at most 𝜀
have

, and 𝐸 is a tensor of Frobenius norm
𝑖
𝑛 and spectral norms (cid:107)𝐸{1,2}{3} (cid:107) (cid:54) 1 and (cid:107)𝐸{1,3}{2} (cid:107) (cid:54) 1. For 𝑔 ∼ 𝒩(0, Σ−1), we

𝑇(𝑔) = 𝑆(𝑔) + 𝐸(𝑔) = (cid:169)
(cid:173)
(cid:171)

(cid:213)

𝑘∈[𝑛]

(cid:104)𝑔, 𝑏𝑘(cid:105) · 𝑏𝑘𝑏(cid:62)
𝑘 (cid:170)
(cid:174)
(cid:172)

+ (cid:169)
(cid:173)
(cid:171)

(cid:213)

𝑖∈[𝑑2]

,

𝑔𝑖 · 𝐸𝑖(cid:170)
(cid:174)
(cid:172)

where we use 𝐸𝑖 = 𝐸(𝑖, ·, ·) to refer to the 𝑑2 × 𝑑2 matrix given by taking the {2}, {3}
ﬂattening of 𝐸 restricted to coordinate 𝑖 in mode 1.

The proof of the lemma is now identical to that of [SS17, Lemma 4.6 and Lemma 4.7].
There are two primary diﬀerences: the ﬁrst is that in [SS17] the tensor has four modes, and
our tensor eﬀectively has 3 modes. This diﬀerence is negligible, since in [SS17], two of the
four modes are always identiﬁed anyway.

9In particular, if we choose 𝛿 = log log 𝑛 · 𝜀, we will will recover all but 𝜀 · 𝑛 log log 𝑛 of the 𝑏𝑖 in ˜𝑂(𝑛)

repetitions.

25

The second diﬀerence is that we choose parameters diﬀerently. We take the parameter
𝛿 )10; this is to emphasize that for small
𝑛 , one can recover all 𝑚 = 𝑛 of the components. Because the proof is otherwise the

𝛽 appearing in [SS17, Lemma 4.6] so that 𝛽 = Ω( 𝜀
𝜀 (cid:28) 1
same, we merely sketch an overview here.

The ﬁrst term in isolation is a random ﬂattening of an orthogonal tensor, and so with
probability 1 the eigenvectors of the ﬁrst term are precisely the 𝑏𝑘. The second term, which
is the ﬂattening of the noise term, introduces complications; however, the combination of
the spectral norm bound and the Frobenius norm bound on 𝐸 is enough to argue (using a
matrix Bernstein inequality, Markov’s inequality and the orthogonality of the 𝑏𝑖) that the
random ﬂattening of 𝐸 cannot have spectral norm larger than 𝜀/𝛿 in more than 1 − 𝛿 of the
𝑏𝑘’s directions.

To ﬁnish the proof, we perform a large deviation analysis on the coeﬃcients (cid:104)𝑔, 𝑏𝑘(cid:105),
lower bounding the probability that for the 1 − 𝛿 fraction of the 𝑏𝑘 that are not too aligned
with the spectrum of 𝐸, there is a suﬃciently large gap between (cid:104)𝑔, 𝑏𝑘(cid:105) and the (cid:104)𝑔, 𝑏𝑖(cid:105) for
𝑖 ≠ 𝑘 so that 𝑏𝑘 is correlated with the top singular vectors of 𝑇(𝑔).11 The bound on the ratio
(cid:3)
of the singular values comes from [SS17, Lemma 4.7] as well.

Proof of Lemma 6.2. The proof simply follows by applying Lemma 6.3, then Lemma 6.4. (cid:3)

6.2 Extracting components from the whitened squares

We now present the following simple algorithm which recovers a vector close to 𝑎𝑖, given
a vector close to 𝑊(𝑎⊗2
). For convenience we will again work with generic orthonormal
vectors 𝑏𝑖 in place of the 𝑊(𝑎⊗2
), and we will assume we have access to the matrix Π3 (the
approximate projector to Span{𝑊(𝑎⊗2

) ⊗ 𝑎𝑖 }) computed in Algorithm 2.

𝑖

𝑖

𝑖

Lemma 6.5. Suppose 𝑏1, . . . , 𝑏𝑛 ∈ ℝ𝑑2 are orthonormal vectors and 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑, and
Π3 ∈ ℝ𝑑3×𝑑3 is such that (cid:107)Π3 − (cid:205)𝑖 𝑏𝑖𝑏(cid:62)
(cid:107) (cid:54) 𝜀. Then if 𝑢 ∈ ℝ𝑑2 is a unit vector with
⊗ 𝑎𝑖 𝑎(cid:62)
𝑖
𝑖
(cid:104)𝑢, 𝑏𝑖(cid:105)2 (cid:62) 1 − 𝜃 for 𝜃 < 1
10 , then the output 𝑎 ∈ ℝ𝑑 of Algorithm 4 on 𝑢 has the property that
√
|(cid:104)𝑎, 𝑎𝑖(cid:105)| (cid:62) 1 − 4𝜃1/4 − 4
𝜀.

Proof. Let 𝑃3 = (cid:205)𝑖 𝑏𝑖𝑏(cid:62)
. By assmption we can write the approximate projector
𝑖
Π3 = 𝑃3 + 𝐸, for a matrix 𝐸 of spectral norm (cid:107)𝐸(cid:107) (cid:54) 𝜀. Based on these expressions we can
√

⊗ 𝑎𝑖 𝑎(cid:62)
𝑖

10We comment that the parameter 𝑐 appearing in the statement of [SS17, Lemma 4.6] is larger than
2;
this is necessary for the application of [SS17, Lemma 4.7], and is not clear from the lemma statement but is
implicit in the proof.

11We note that to obtain correlation 1 − 𝜀

the statement of the lemma (which has assumed that 2𝜀(1+𝛽)
with the lower bound 0.99).

𝛿𝛽 , one must directly use the proof of [SS17, Lemma 4.7], rather than
𝛽𝛿 (cid:54) 0.01, and replaced the expression 1 − 2𝜀(1+𝛽)
)

𝛽𝛿

26

Algorithm 4 Extracting the component from the whitened square
Function extract(𝑢, Π3):
Input: a unit vector 𝑢 ∈ (ℝ𝑑)⊗2 such that (cid:104)𝑢, 𝑏𝑖(cid:105)2 (cid:62) 1 − 𝜃 for some 𝑖 ∈ [𝑛], and a projector
Π3 ∈ ℝ𝑑3×𝑑3 such that (cid:107)Π3 − (cid:205)𝑖 𝑏𝑖𝑏(cid:62)
𝑖

⊗ 𝑎𝑖 𝑎(cid:62)
𝑖

(cid:107) (cid:54) 𝜀.

1. Compute the matrix 𝑀 = Π3(𝑢𝑢(cid:62) ⊗ Id).

2. Compute the top-left singular vector 𝑣 of 𝑀.

3. Taking the reshaping 𝑉 = 𝑣{3}{1,2}, let 𝑎 = 𝑉𝑢.

Output: the vector 𝑎 ∈ ℝ𝑑

re-express the product,

𝑀 = Π3(𝑢𝑢(cid:62) ⊗ Id) = 𝑃3(𝑢𝑢(cid:62) ⊗ Id) + 𝐸(𝑢𝑢(cid:62) ⊗ Id).

By assumption, the second term is a matrix of spectral norm at most (cid:107)𝐸(cid:107) (cid:54) 𝜀.

We now consider the ﬁrst term. If 𝑢 = 𝑐 · 𝑏𝑖 + 𝑤, then for the ﬁrst term we have

𝑃3(𝑢𝑢(cid:62) ⊗ Id) = 𝑃3 (cid:0)𝑐2 · 𝑏𝑖𝑏(cid:62)

𝑖 ⊗ Id(cid:1) + 𝑃3 (cid:0)(𝑐 · 𝑏𝑖𝑤(cid:62) + 𝑐 · 𝑤𝑏(cid:62)

𝑖 + 𝑤𝑤(cid:62)) ⊗ Id(cid:1)
√

The second term is again a matrix of spectral norm at most 3𝑐 · (cid:107)𝑤(cid:107) = 3𝑐 ·
term can be further simpliﬁed as

1 − 𝑐2. The ﬁrst

𝑃3(𝑐2 · 𝑏𝑖𝑏(cid:62)

𝑖 ⊗ Id) = 𝑐2 · (cid:213)

𝑖

(cid:104)𝑏𝑖 , 𝑏𝑖(cid:105)𝑏𝑖𝑏(cid:62)

𝑖 ⊗ 𝑎𝑖 𝑎(cid:62)

𝑖 = 𝑐2 · 𝑏𝑖𝑏(cid:62)

𝑖 ⊗ 𝑎𝑖 𝑎(cid:62)
𝑖 ,

by the orthogonality of the 𝑏𝑖. This is a rank-1 matrix with singular value 𝑐2. Therefore,
𝑀 = 𝑐2(𝑏𝑖 ⊗ 𝑎𝑖)(𝑏𝑖 ⊗ 𝑎𝑖)(cid:62) + ˜𝐸 where (cid:107) ˜𝐸(cid:107) (cid:54) 𝜀 + 3𝑐
1 − 𝑐2. It follows from Lemma 6.6 that if
𝑐2 (cid:107) ˜𝐸(cid:107).
𝑣 is the top unit left-singular vector of 𝑀, then (cid:104)𝑣, 𝑏𝑖 ⊗ 𝑎𝑖(cid:105)2 (cid:62) 1 − 2

Now, in step 3 when we re-shape 𝑣 to a 𝑑 × 𝑑2 matrix 𝑉 of Frobenius norm 1, because 𝑣
𝑐2 (cid:107) ˜𝐸(cid:107).

+ ˜𝑉 for ˜𝑉 of spectral norm (cid:107) ˜𝑉 (cid:107) (cid:54) (cid:107) ˜𝑉 (cid:107)𝐹 (cid:54)

(cid:113) 2

is a unit vector we have that 𝑉 = 𝑎𝑖𝑏(cid:62)
𝑖
Therefore,

√

𝑉𝑢 = (𝑎𝑖𝑏(cid:62)

𝑖 )(𝑐 · 𝑏𝑖 + 𝑤) + ˜𝑉𝑢 = 𝑐(1 − (cid:104)𝑤, 𝑏𝑖(cid:105)) · 𝑎𝑖 + ˜𝑉𝑢,
and the latter vector has norm at most (cid:107) ˜𝑉 (cid:107), and (cid:104)𝑤, 𝑏𝑖(cid:105) (cid:54) (cid:107)𝑤(cid:107) (cid:54)
substituting 𝑐 =
simpliﬁcations, the conclusion follows.

1 − 𝑐2. Finally,
1 − 𝜃 and using our bound on (cid:107) ˜𝐸(cid:107) and (cid:107) ˜𝑉 (cid:107) and some algebraic
(cid:3)

√

√

Lemma 6.6. Suppose that 𝑀 = 𝑢𝑣(cid:62) + 𝐸 for 𝑢 ∈ ℝ𝑑, 𝑣 ∈ ℝ𝑘 unit vectors and 𝐸 ∈ ℝ𝑑×𝑘 a
matrix of spectral norm (cid:107)𝐸(cid:107) (cid:54) 𝜀. Then if 𝑥, 𝑦 are the top left- and right-singular vectors of 𝑀,

27

|(cid:104)𝑥, 𝑢(cid:105)|, |(cid:104)𝑦, 𝑣(cid:105)| (cid:62) 1 − 2𝜀.

Proof. Let 𝑀 = (cid:205)𝑖 𝜎𝑖 𝑥𝑖 𝑦(cid:62)
𝑖
We have that

be the singular value decomposition of 𝑀, with 𝜎1 (cid:62) · · · (cid:62) 𝜎𝑑.

On the other hand, if with (cid:104)𝑥1, 𝑢(cid:105) = 𝛼 (cid:54) 1 and (cid:104)𝑦1, 𝑣(cid:105) = 𝛽 (cid:54) 1,

1 − 𝜀 (cid:54) 𝑢(cid:62)𝑀𝑣 (cid:54) 𝜎1.

Therefore,

𝜎1 = 𝑥(cid:62)

1 𝑀 𝑦1 (cid:54) 𝛼𝛽 + 𝜀.

|𝛼|, |𝛽| (cid:62) 𝛼𝛽𝑔𝑒1 − 2𝜀,

and thus min{|𝛼|, |𝛽|} (cid:62) 1 − 2𝜀.

(cid:3)

6.3 Testing candidate components

The following algorithm allows us to test whether a candidate component 𝑢 is close to
some component 𝑎𝑖.

Algorithm 5 Testing component membership
Function test(𝑢, 𝜃, Π𝑆3):
Input: A unit vector ˆ𝑢, and the correlation parameter 𝜃. Also, Π3, an approximate projector
to Span{(𝑊 𝑎⊗2

) ⊗ 𝑎𝑖 }.

𝑖

1. Compute 𝜌 = ((𝑊 ˆ𝑢⊗2) ⊗ ˆ𝑢).

2. If (cid:107)Π3𝜌(cid:107)2
2

< (1 − 𝜃)(cid:107)𝜌(cid:107)2
2

, return false. Otherwise, return true.

Lemma 6.7. Let 𝑃3 be the projector to Span{(𝑊 𝑎⊗2
𝑖
that (cid:107)Π3 − 𝑃3(cid:107) (cid:54) 𝜀 < 1
for all 𝑖 ∈ [𝑛], then Algorithm 5 returns false.

) ⊗ 𝑎𝑖 }, and suppose that we have Π3 such
2. Then if Algorithm 5 is run on a vector ˆ𝑢 such that (cid:104) ˆ𝑢, 𝑎𝑖(cid:105)2 (cid:54) 1 − 𝜃 − 2𝜀

Converseley, if Algorithm 5 is run on a vector ˆ𝑢 with (cid:104) ˆ𝑢, 𝑎𝑖(cid:105)2 (cid:62) 1 −
some 𝑖 ∈ [𝑛], then when run on a unit vector ˆ𝑢, Algorithm 5 returns true.

(cid:16) 𝜃−𝜀
10(cid:107)𝑊 (cid:107)

(cid:17) 2

(cid:62) 1 − 1

10 for

Proof. By assumption, we can write Π3 = 𝑃3 + 𝐸 for 𝑃3 the projector to Span{(𝑊 𝑎⊗2
and 𝐸 a matrix of spectral norm at most 𝜀. From this, we have

𝑖

) ⊗ 𝑎𝑖 }

Π3(𝑊 ˆ𝑢⊗2) ⊗ ˆ𝑢 = 𝑃3(𝑊 ˆ𝑢⊗2) ⊗ ˆ𝑢 + 𝐸(𝑊 ˆ𝑢⊗2) ⊗ ˆ𝑢,

(6.1)

28

and (cid:107)𝐸(𝑊 ˆ𝑢⊗2) ⊗ ˆ𝑢(cid:107) (cid:54) 𝜀(cid:107)𝑊 ˆ𝑢⊗2(cid:107). Now, we can write 𝑊 ˆ𝑢⊗2 = (cid:205)𝑖 𝑐𝑖𝑊(𝑎𝑖 ⊗ 𝑎𝑖) + 𝑒, where 𝑒
is orthogonal to Span{𝑊 𝑎⊗2

}, and we can further write

𝑖

(𝑊 ˆ𝑢⊗2) ⊗ ˆ𝑢 =

(cid:213)

𝑖≠𝑗

𝑐𝑖 𝛾𝑗 · (𝑊 𝑎⊗2

𝑖

) ⊗ 𝑏(𝑖)
𝑗

+ (cid:213)

𝑖

𝑐𝑖 𝛾𝑖 · (𝑊 𝑎⊗2

𝑖

) ⊗ 𝑎𝑖 + 𝑒 ⊗ ˆ𝑢,

where {𝑏(𝑖)
𝑗
deﬁnition, 𝑃3(𝑊 𝑎⊗2
Therefore,

𝑖

} 𝑗≠𝑖 is an orthogonal basis for the orthogonal complement of 𝑎𝑖 in ℝ𝑑. By
) ⊗ 𝑎𝑖 }.

𝑗 = 0, as this is orthogonal to every vector in Span{(𝑊 𝑎⊗2

) ⊗ 𝑏(𝑖)

𝑖

𝑃3(𝑊 ˆ𝑢⊗2) ⊗ ˆ𝑢 =

(cid:213)

𝑖

𝑐𝑖 𝛾𝑖 · (𝑊 𝑎⊗2

𝑖

) ⊗ 𝑎𝑖.

Now, if (cid:104) ˆ𝑢, 𝑎𝑖(cid:105)2 (cid:54) 𝜏 for all 𝑖 ∈ [𝑛], then 𝛾2
𝑖

(cid:107)𝑃3(𝑊 ˆ𝑢⊗2) ⊗ ˆ𝑢 (cid:107)2 (cid:54) max𝑖 𝛾2
𝑖
that

· (cid:205)𝑗 𝑐2
𝑗

(cid:54) 𝜏 · (cid:107)𝑊 ˆ𝑢⊗2(cid:107)2
2

(cid:54) 𝜏 for all 𝑖 ∈ [𝑛]. It thus follows that
. Combining this with Eq. (6.1), we have

(cid:107)Π3(𝑊 ˆ𝑢⊗2) ⊗ ˆ𝑢(cid:107)2

2 (cid:54) (𝜏 + 𝜀

√

𝜏 + 𝜀2) · (cid:107)𝑊 ˆ𝑢⊗2(cid:107)2

2 (cid:54) (𝜏 + 2𝜀)(cid:107)𝑊 ˆ𝑢⊗2(cid:107)2
2,

for 𝜀 < 1
returns false.

2. It follows that if (cid:104) ˆ𝑢, 𝑎𝑖(cid:105)2 = 𝜏 < 1 − 𝜃 − 2𝜀 for all 𝑖 ∈ [𝑛], then the algorithm

Converseley, if without loss of generality ˆ𝑢 = 𝜁 · 𝑎1 + ˆ𝑒 for ˆ𝑒 ∈ ℝ𝑑 orthogonal to 𝑎1,
(cid:54) (1 − 𝜁2) · (cid:107)𝑊 (cid:107)2. Measuring the correlation of

then 𝑊 ˆ𝑢⊗2 = 𝜁2 · 𝑊 𝑎⊗2
1
𝑊 ˆ𝑢⊗2 with 𝑊 𝑎⊗2
1

+ 𝑊 𝑒(cid:48) with (cid:107)𝑊 𝑒(cid:48)(cid:107)2
2
, we have that 𝑐1 (cid:62) 𝜁2. Also 𝛾1 (cid:62) 𝜁, which implies

𝑃3(𝑊 ˆ𝑢⊗2) ⊗ ˆ𝑢 = 𝜁3 · (𝑊 𝑎⊗2

1 ) ⊗ 𝑎1 + ˜𝑒.

where ˜𝑒 is a leftover term with (cid:107) ˜𝑒 (cid:107) (cid:54) (cid:112)1 − 𝜁2 · (cid:107)𝑊 ˆ𝑢⊗2(cid:107) + 𝜁2(cid:112)1 − 𝜁2 (where we have used
the PSDness of 𝑊). Combining this with Eq. (6.1),

Π3(𝑊 ˆ𝑢⊗2) ⊗ ˆ𝑢 = 𝜁3(𝑊 𝑎⊗2

1 ) ⊗ 𝑎1 + ˜𝑒 + 𝐸(𝑊 ˆ𝑢⊗2) ⊗ ˆ𝑢.

For convenience let ˆ𝜌 = ˜𝑒 + 𝐸(𝑊 ˆ𝑢⊗2) ⊗ ˆ𝑢; from our previous observations, we have
(cid:107) ˆ𝜌(cid:107) (cid:54) (𝜀 + (cid:112)1 − 𝜁2)(cid:107)𝑊 ˆ𝑢⊗2(cid:107) + 𝜁2(cid:112)1 − 𝜁2.

Now, if (cid:104) ˆ𝑢, 𝑎1(cid:105)2 = 𝜁2 (cid:62) 1 − 𝜂, we have that

(1 − 𝜂) −

√

𝜂(cid:107)𝑊 (cid:107) (cid:54) (cid:107)𝑊 ˆ𝑢⊗2(cid:107) (cid:54) (1 − 𝜂) +

√

𝜂(cid:107)𝑊 (cid:107).

From this,

(cid:107)Π3(𝑊 ˆ𝑢⊗2)(cid:107) (cid:62) 𝜁3 − (cid:107) ˆ𝜌(cid:107) (cid:62) (1 − 𝜂)3/2 − (𝜀 +

√

𝜂)(cid:107)𝑊 ˆ𝑢⊗2(cid:107) − (1 − 𝜂)
√

√

𝜂(cid:107)𝑊 (cid:107)) − (𝜀 +

√

𝜂

𝜂)(cid:107)𝑊 ˆ𝑢⊗2(cid:107) − (1 − 𝜂)

√

𝜂

(cid:62) (cid:112)1 − 𝜂((cid:107)𝑊 ˆ𝑢⊗2(cid:107) −

29

(cid:62) (1 − 𝜀 − 2
(cid:62) (1 − 𝜀 − 5

𝜂)(cid:107)𝑊 ˆ𝑢⊗2(cid:107) − 2
𝜂(cid:107)𝑊 (cid:107))(cid:107)𝑊 ˆ𝑢⊗2(cid:107).

√

√

√

𝜂(cid:107)𝑊 (cid:107),

where we have used that 𝜂 < 1
false.

10 . Thus, if 𝜂 <

(cid:17) 2

(cid:16) 𝜃−𝜀
10(cid:107)𝑊 (cid:107)

, Algorithm 5 does not return
(cid:3)

6.4 Putting things together

Finally, we prove Lemma 6.1.

Proof of Lemma 6.1. By the assumptions of the theorem, we have access to an implicit rank-𝑛
)(cid:62)(cid:17) −1/2
(cid:16)(cid:205)𝑖∈[𝑛](𝑎⊗2
representation of 𝑇 = (cid:205)𝑖∈[𝑛](𝑊(𝑎⊗2
𝑛. For convenience we denote 𝑏𝑖 = 𝑊(𝑎⊗2
and with (cid:107)𝑇 − 𝐸(cid:107)𝐹 (cid:54) 𝜀
). Note that the 𝑏𝑖 are
orthonormal vectors in ℝ𝑑2. We also have implicit access to a rank-𝑛 representation of Π3,
where (cid:107)Π3 − (cid:205)𝑖(𝑏𝑖 ⊗ 𝑎𝑖)(𝑏𝑖 ⊗ 𝑎𝑖)(cid:62)(cid:107) (cid:54) 𝜀.

))⊗3 + 𝐸 ∈ (ℝ𝑑2)⊗3, where 𝑊 =

)(𝑎⊗2
𝑖

√

,

𝑖

𝑖

𝑖

We ﬁrst run step 1 of Algorithm 3 to produce the tensor which we will round. Then,
for ℓ = ˜𝑂(𝑛1+𝑂(𝛽)) independent iterations, we run steps 2 & 3 of Algorithm 3 to produce
candidate whitened squares 𝑢1, . . . , 𝑢ℓ , then run Algorithm 4 on the 𝑢𝑖 to produce candidate
components ˆ𝑢𝑖, and ﬁnally run Algorithm 5 to check if ˆ𝑢𝑖 is close to 𝑎 𝑗 for some 𝑗 ∈ [𝑛].
√
We show that step 1 of Algorithm 3 takes time ˜𝑂(𝑛2𝑑3). Since 𝑇 is at most 𝜀
𝑛
in Frobenius norm away from a tensor that is a rank-𝑛 projector in both rectangular
reshapings 𝑇{1,2},{3} and 𝑇{2,3},{1}, the (2𝑛)th singular values in either reshaping must be
at most 𝜀: otherwise the error term would have over 𝑛 singular values more than 𝜀
√
and therefore Frobenius norm more than 𝜀
𝑛 because it is a rank-𝑛
projector in its square matrix reshaping. Therefore, by Lemma 4.3, step 1 requires time
˜𝑂(𝑛2𝑑3 + 𝑛(𝑛𝑑3 + 𝑛𝑑2)) to return an 𝜀
𝑛-approximation in Frobenius norm to the projected
matrix.12 Note that this step only needs to be carried out once regardless of how many
times the algorithm is invoked for a speciﬁc input 𝑇, so the ˜𝑂(𝑛2𝑑3) runtime is incurred as
a preprocessing cost.

𝑛. Also (cid:107)𝑇 (cid:107)𝐹 =

√

√

Then, again by Lemma 4.3, steps 2 & 3 require time ˜𝑂( 1

𝛽 𝑛𝑑3), since the ratio of the ﬁrst
and second singular values of the the matrix is 1 + Ω(𝛽), and since 𝑂( 1
𝛽 log 𝑑) steps of power
iteration with 𝑇(𝑔) can be implemented by choosing the random direction 𝑔 ∼ 𝒩(0, Id𝑑2),
the starting direction 𝑣1 ∈ ℝ𝑑2, and then computing 𝑣𝑡+1 = (Id ⊗ 𝑔(cid:62) ⊗ 𝑣𝑡)𝑇 (cid:54)1 where 𝑇 (cid:54)1 is
the truncated tensor.

12Some of the lemmas we apply, out of concerns for compatibility with [SS17], assume that the maximum
singular value of 𝑇 (cid:54)1 is at most 1. Though one could re-do the previous analysis with minimal consequences
under the assumption that the spectral norm is at most 1 + 𝜀, for brevity we note that we may instead multiply
the whole tensor by 1
1−𝜀 , and because the tensor has Frobenius norm at most (1 + 3𝜀)
𝑛, this costs at most
4𝜀

𝑛 additional Frobenius norm error.

√

√

30

Thus, if we choose 𝛽, 𝛿 satisfying the requirements of Lemma 6.2, after ˜𝑂(𝑛𝑂(𝛽))
iterations of steps 2 & 3 we will recover a vector 𝑢 ∈ ℝ𝑑2 such that (cid:104)𝑢, 𝑏𝑖(cid:105)2 (cid:62) 1 − 3 𝜀
𝛽 ,
and after ˜𝑂(𝑛1+𝑂(𝛽)) iterations of steps 2 & 3 we will recover vectors 𝑢𝑡1 , . . . , 𝑢𝑡𝑚 so that
(cid:104)𝑢𝑡𝑖 , 𝑏𝑖(cid:105)2 (cid:62) 1 − 3 𝜀

𝛽𝛿 for 𝑚 (cid:62) (1 − 𝛿)𝑛 of the 𝑖 ∈ [𝑛].

(cid:17) 1/4

(cid:16) 3𝜀
𝛿𝛽

Next, applying Lemma 6.5 to each of the good candidate vectors obtained in Algorithm 3,
𝜀 −

Algorithm 4 will give us candidate components ˆ𝑢𝑡1 , . . . , ˆ𝑢𝑡𝑚 so that (cid:104) ˆ𝑢𝑡𝑖 , 𝑎𝑖(cid:105)2 (cid:62) 1 − 4
. Since Π3 has rank 𝑛, we write it as 𝑈𝑈T for 𝑈 ∈ ℝ𝑑3×𝑛. Then we may reshape
4
(𝑢𝑢T ⊗ Id)Π3 as 𝑢𝑢T𝑈(cid:48)(𝑈T ⊗ Id), where 𝑈(cid:48) is the 𝑑2 × 𝑛𝑑 reshaping of 𝑈. Multiplying 𝑢T
through takes 𝑂(𝑛𝑑3) time and then reshaping the result back results in (𝑢𝑢T ⊗ Id)Π3.
Therefore, by Lemma 4.2, each invocation of Algorithm 4 requires ˜𝑂(𝑛𝑑3) operations.

√

Finally, from Lemma 6.7, we know that if we run Algorithm 5 with 𝜃 = 10(cid:107)𝑊 (cid:107) ·

(cid:18)

2𝜀1/4 + 2

(cid:16) 3𝜀
𝛿𝛽

(cid:17) 1/8(cid:19)

+ 2𝜀, we will reject any ˆ𝑢 such that (cid:104) ˆ𝑢, 𝑎𝑖(cid:105)2 (cid:54) 1 − 𝜃 − 2𝜀 for all 𝑖 ∈ [𝑛],

and will keep all of the good outputs of Algorithm 4. Each iteration of Algorithm 5 requires
time 𝑂(𝑑4 + 𝑛𝑑3 + 𝑑3), since we form the vector (𝑊 ˆ𝑢⊗2) ⊗ ˆ𝑢, then multiply with the rank-𝑛
matrix Π3, and ultimately compute a norm.

This completes the proof.

(cid:3)

7 Combining lift and round for ﬁnal algorithm

In this section we describe and analyze our ﬁnal tensor decomposition algorithm, proving
our main theorem.

Algorithm 6 Main algorithm for overcomplete 4-tensor decomposition
Function decompose(𝑇):
Input: a tensor 𝑇 ∈ (ℝ𝑑)⊗4, numbers 𝛽, 𝛿, 𝜀 ∈ (0, 1), numbers 𝜎, 𝜅0 ∈ ℝ(cid:62)0, and 𝑛 (cid:54) 𝑑2.

1. Run lift(𝑇, 𝑛) from Algorithm 2 to obtain an implicit tensor 𝑇(cid:48) and an implicit matrix

Π3, using 𝜎, 𝜅0 as upper bounds on condition numbers 𝜎𝑛 , 𝜅.

2. Run the algorithm speciﬁed by Lemma 6.1 on input (𝑇(cid:48), Π3, 𝜀, 𝛽, 𝛿) with independent

randomness 𝑡 = ˜𝑂(𝑛) times, to obtain vectors 𝑢1, . . . , 𝑢𝑡.

Output: 𝑢1, . . . , 𝑢𝑡

Deﬁnition 7.1 (Signed Hausdorﬀ distance). For sets of vectors 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑 and
𝑏1, . . . , 𝑏𝑚 ∈ ℝ𝑑, we deﬁne the signed Hausdorﬀ distance to be the maximum of the follow-
ing two quantities. (1) max𝑖∈[𝑛] min𝑗∈[𝑚],𝜎∈±1 (cid:107)𝑎𝑖 − 𝜎𝑏 𝑗 (cid:107) and (2) max𝑖∈[𝑚] min𝑗∈[𝑛],𝜎∈±1 (cid:107)𝑏𝑖 −
𝜎𝑎 𝑗 (cid:107).

31

Deﬁnition 7.2 (Condition number of 𝑎1, . . . , 𝑎𝑛). Let 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑. Let {𝑏𝑖𝑗 } 𝑗∈[𝑑−1] be an
arbitrary orthonormal basis for the orthogonal complement of 𝑎𝑖 in ℝ𝑑. Let

𝐻T :=












Let 𝑅 = (𝐻𝐻T)−1/2𝐻 be a column-wise orthonormalization of 𝐻, and let 𝐾 = 1
2(Id − 𝑃2,3)𝑅,
where 𝑃2,3 is the permutation matrix that exchanges the 2nd and 3rd modes of (ℝ𝑑)⊗3. The
condition number 𝜅 of 𝑎1, . . . , 𝑎𝑛 is the minimum singular value of 𝐾.

𝑎1 ⊗ 𝑎1 ⊗ 𝑏1,1
...
𝑎𝑖 ⊗ 𝑎𝑖 ⊗ 𝑏𝑖,𝑗
...
𝑎𝑛 ⊗ 𝑎𝑛 ⊗ 𝑏𝑛,𝑑−1













.

Theorem 7.3. For every 𝑑, 𝑛 ∈ ℕ and 𝜀, 𝛽, 𝛿 ∈ (0, 1) and 𝜎, 𝜅0 ∈ ℝ(cid:62)0 there is a randomized
algorithm decompose𝑑,𝑛,𝜀,𝛽,𝛿,𝜎,𝜅0(𝑇) with the following guarantees. For every set of unit vectors
𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑 and every 𝐸 ∈ (ℝ𝑑)⊗4 such that

1. the operator norm of the square matrix ﬂattening of 𝐸 satisﬁes (cid:107)𝐸12;34 (cid:107)

𝑛𝜇−1𝜅2 (cid:54) 𝜀,

𝜎7

2. 𝜅 = 𝜅(𝑎1, . . . , 𝑎𝑛) (cid:62) 𝜅0

3. 𝜎𝑛 (cid:62) 𝜎

where

1. 𝜎𝑛 is the 𝑛-th singular value of the matrix (cid:205)𝑖(cid:54)𝑛(𝑎⊗2

𝑖

)(𝑎⊗2
𝑖

)T,

2. 𝜇 is the operator norm of (cid:205)𝑖(cid:54)𝑛(𝑎⊗3

𝑖

)(𝑎⊗3
𝑖

)T, and

3. 𝜅 is the condition number of 𝑎1, . . . , 𝑎𝑛 as in Deﬁnition 7.2.

there is a subset 𝑆 ⊆ {𝑎1, . . . , 𝑎𝑛 } of size |𝑆| (cid:62) (1 − 𝛿)𝑛 such that given input 𝑇 = (cid:205)𝑖(cid:54)𝑛 𝑎⊗4
+ 𝐸
the algorithm produces a set 𝐵 = {𝑏1, . . . , 𝑏𝑡 } of 𝑡 = ˜𝑂(𝑛) vectors which with probability at least
0.99 over the randomness in the algorithm has

𝑖

signed-Hausdorff-distance(𝑆, 𝐵) (cid:54) 𝑂

(cid:19) 1/16

.

(cid:18) 𝜀
𝛿𝛽

Furthermore, the algorithm decompose𝑑,𝑛,𝜀,𝛽,𝛿,𝜎,𝜅0 runs in time

˜𝑂

(cid:18) 𝑛𝑑4
√
𝜎

+

𝑛2𝑑3
𝜅0

+

𝑛2+𝑂(𝛽)𝑑3
𝛽

(cid:19)

.

32

We record some intuitive explanations of the parameters in Theorem 7.3.

• 𝜎, 𝜅0 are bounds on the minimum singular values of matrices associated to 𝑎1, . . . , 𝑎𝑛,
used to determine the necessary precision of linear-algebraic manipulations per-
formed by the algorithm. Decreasing 𝜎, 𝜅0 yields an algorithm tolerating less
well-conditioned tensors, at the expense of running time and/or accuracy guarantees.

• 𝛿 determines what fraction of the vectors 𝑎1, . . . , 𝑎𝑛 the algorithm is allowed to fail to
return. By decreasing 𝛿 the algorithm recovers a larger fraction of 𝑎1, . . . , 𝑎𝑛, at the
cost of increasing running time and/or decreasing per-vector accuracy.

• 𝛽 determines the per-vector accuracy of the algorithm. Increasing 𝛽 improves the

accuracy of the algorithm, but with exponential cost in the running time.

• 𝜀 governs the magnitude of allowable noise 𝐸. Increasing 𝜀 yields a more noise-

tolerant algorithm, at the expense of the accuracy of recovered vectors.

We record the following corollary, which follows from Theorem 7.3 by choosing

parameters appropriately.

Corollary 7.4. For every 𝑛, 𝑑 ∈ ℕ and 𝜎 > 0 (independent of 𝑛, 𝑑) there is an algorithm with the
following guarantees. The algorithm takes input 𝑇 = (cid:205)𝑖(cid:54)𝑛 𝑎⊗4

+ 𝐸, and so long as

𝑖

1. 𝜅(𝑎1, . . . , 𝑎𝑛) (cid:62) 𝜎

2. the minimum nonzero eigenvalue of (cid:205)𝑖(cid:54)𝑛(𝑎⊗2

𝑖

)(𝑎⊗2
𝑖

)(cid:62) is at least 𝜎

3. (cid:107) (cid:205)𝑖(cid:54)𝑛(𝑎⊗3

𝑖

)(𝑎⊗3
𝑖

)(cid:62)(cid:107) (cid:54) 1/𝜎, and

4. (cid:107)𝐸12;34(cid:107) (cid:54) poly(𝜎)/(log 𝑛)𝑂(1),

with high probability the algorithm recovers ˜𝑂(𝑛) vectors 𝑏1, . . . , 𝑏𝑡 such that there is a set
𝑆 ⊆ {𝑎1, . . . , 𝑎𝑛 } with |𝑆| (cid:62) (1 − 𝑜(1))𝑛 such that the signed Hausdorﬀ distance from 𝑆 to
{𝑏1, . . . , 𝑏𝑡 } is 𝑜(1), in time ˜𝑂(𝑛2𝑑3/poly(𝜎)).

Furthermore, hypotheses (2),(3) hold for random unit vectors 𝑎1, . . . , 𝑎𝑛 with 𝜎 = 0.1 so long as

𝑛 (cid:54) 𝑑2/(log 𝑛)𝑂(1), and experiments in Appendix C strongly suggest that (1) does as well.

Proof of Theorem 7.3. Let 𝑊 = ((cid:205)𝑖(cid:54)𝑛 𝑎⊗2
𝑖
and matrix Π3 returned by lift satisfy

(𝑎⊗2
𝑖

)(cid:62))−1/2. By Lemma 5.1, the implicit tensor 𝑇(cid:48)

(cid:13)
(cid:13)
𝑇(cid:48) − (cid:213)
(cid:13)
(cid:13)
(cid:13)
𝑖(cid:54)𝑛

(𝑊(𝑎𝑖 ⊗ 𝑎𝑖))⊗3

(cid:54) 𝑂(𝜀𝜎9/2

𝑛

√

𝑛)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)𝐹

33

and

(cid:13)
(cid:13)Π3 − ΠSpan(𝑊 𝑎⊗2
(cid:13)

𝑖

(cid:13)
(cid:13)
(cid:13)

⊗𝑎𝑖)

(cid:54) 𝑂(𝜀𝜎4

𝑛) .

So, by Lemma 6.1, with high probability there is a subset 𝑆 ⊆ {𝑎1, . . . , 𝑎𝑛 } of size 𝑚 (cid:62) (1−𝛿)𝑛
such for each 𝑎𝑖 ∈ 𝑆 there is 𝑢𝑗 among the vectors 𝑢1, . . . , 𝑢𝑡 returned by the rounding
algorithm with

(cid:104)𝑎𝑖 , 𝑢𝑗(cid:105)2 (cid:62) 1 − 𝑂

(cid:18) 1
𝛿1/8

·

1
𝛽1/8

· 𝜀1/8 · (cid:107)𝑊 (cid:107) ·

(cid:19)

√

𝜎𝑛

= 1 − 𝑂

(cid:19) 1/8

(cid:18) 𝜀
𝛿𝛽

where the equality follows because (cid:107)𝑊 (cid:107) = 𝜎−1/2
. Furthermore, each of the vectors 𝑢1, . . . , 𝑢𝑡
𝑛
is similarly close to some 𝑎𝑖 ∈ 𝑆. This proves the claimed upper bound on the Hausdorﬀ
distance.

The running time follows from putting together Lemma 5.2 and the running time
(cid:3)

bounds of Lemma 6.1.

8 Condition number of random tensors

Deﬁnition 8.1. Let Π2,3 : (ℝ𝑑)⊗3 → (ℝ𝑑)⊗3 be the orthogonal projector to the subspace
Span(𝑥 ⊗ 𝑦 ⊗ 𝑦 | 𝑥, 𝑦 ∈ ℝ𝑑) that is invariant under interchange of the second and third
tensor modes. Let Π1,2 be deﬁned similarly. Let Π⊥

2,3 = Id − Π2,3 and Π⊥

1,2 = Id − Π1,2.

Note that Π2,3 = 1

2,3 = 1
the second and third modes, and Π⊥
Formula in representation theory, whereby for any group 𝐺 of linear operators, 1
|𝐺|
is equal to the projection to the common invariant subspace of 𝐺.

2(Id + 𝑃2,3), where 𝑃2,3 is the orthogonal operator that interchanges
2(Id − 𝑃2,3). This follows from the Projection
(cid:205)𝑔∈𝐺 𝑔

Lemma 8.2 (Condition number of basic swap matrix). Let 𝑎1, . . . , 𝑎𝑛 be independent random
𝑑-dimensional unit vectors. Let 𝐵𝑖 ∈ ℝ(𝑑−1)×𝑑 be a random basis for the orthogonal complement
of 𝑎𝑖 in ℝ𝑑. Let 𝑃 ∈ ℝ𝑑3×𝑑3 be the permutation matrix which swaps second and third modes of
(ℝ𝑑)⊗3. Let

𝐴 = 𝔼
𝑎

(𝑎 ⊗ 𝑎 ⊗ Id)(𝑎 ⊗ 𝑎 ⊗ Id)(cid:62) .

Let 𝑅 ∈ ℝ𝑑3×𝑛(𝑑−1) have 𝑛 blocks of dimensions 𝑑3 × (𝑑 − 1), where the 𝑖-th block is

𝑅𝑖 = 𝐴−1/2(𝑎𝑖 ⊗ 𝑎𝑖 ⊗ 𝐵𝑖) − 𝑃𝐴−1/2(𝑎𝑖 ⊗ 𝑎𝑖 ⊗ 𝐵𝑖)

where we abuse notation and denote the PSD square root of the pseudoinverse of 𝐴 by 𝐴−1/2. Then
𝑛, 𝑛, 𝑑3/2).
there is a function 𝑑(cid:48)(𝑑) = Θ(𝑑2) such that 𝔼 (cid:107)𝑅(cid:62)𝑅 − 𝑑(cid:48)(𝑑) · Id(cid:107) (cid:54) 𝑂(log 𝑑)2 · max(𝑑

√

34

In particular, if 𝑑 (cid:28) 𝑛 (cid:28) 𝑑2,

𝔼 (cid:107) 1
𝑑(cid:48)(𝑑)

𝑅(cid:62)𝑅 − Id(cid:107) (cid:54) 𝑂(𝑛(log 𝑑)2/𝑑2) .

Corollary 8.3. Let 𝑎1, . . . , 𝑎𝑛 be independent random 𝑑-dimensional unit vectors with 𝑑 (cid:28) 𝑛.
Then with probability 1−𝑜(1), the condition number 𝜅 of 𝑎1, . . . , 𝑎𝑛 as deﬁned in Lemma 5.3 is at least
(cid:113) 1
T has 𝑛-th eigenvalue 𝜎 ∈ 1− ˜𝑂(𝑛/𝑑2)−𝑂(1/𝑑),
𝑎⊗2
𝑖

𝑑2 ), the matrix 𝑇 = (cid:205) 𝑎⊗2

𝑑 )− ˜𝑂( 𝑛

4 − 1

√

𝑖

−𝑂( 1
and the matrix 𝑈 = (cid:205) 𝑎⊗3

4

2

𝑎⊗3
𝑖

𝑖

T has spectral norm 1 + ˜𝑂(𝑛/𝑑2).

Therefore, decompose𝑑,𝑛,𝜀,𝛽,𝛿,𝜎,𝜅0 when run with error 𝜀, recovers (1 − 𝜎)𝑛 components 𝑎𝑖 with

signed Hausdorﬀ distance 𝑂(𝜀/(𝛿𝛽))1/16 in time ˜𝑂(𝑛𝑑4 + 𝑛2𝑑3 + 𝑛2+𝑂(𝛽)𝑑3/𝛽).

To prove the corollary, we will need an elementary fact.

Fact 8.4. Let 𝑎1, . . . , 𝑎𝑛 be independent random unit vectors in ℝ𝑑. Let 𝑈 = (cid:205)𝑛
probability 1 − 𝑜(1), (cid:107)𝑈 − Πimg(𝑈)(cid:107) (cid:54) ˜𝑂(𝑛/𝑑2) if 𝑛 (cid:29) 𝑑.

𝑖=1 𝑎⊗3

𝑖

𝑎⊗3
𝑖

T. With

The proof may be found in Section 8.8.

Fact 8.5. Let 𝑎1, . . . , 𝑎𝑛 be independent random unit vectors in ℝ𝑑. Let ˆΣ = 𝔼𝑎(𝑎 ⊗ 𝑎)(𝑎 ⊗ 𝑎)T for
𝑎 drawn from the uniform distribution over unit vectors in ℝ𝑑. Then

(cid:107)2𝑑−2 ˆΣ−1/2𝑇 ˆΣ−1/2 − Π ˆΣ−1/2 img(𝑇)(cid:107) (cid:54) ˜𝑂(𝑛/𝑑2) .

Proof. The fact follows by a slight modiﬁcation of [HSSS16, Lemma 5.9]. While Lemma 5.9
in [HSSS16] applies to Gaussian random vectors, not uniform random unit vectors, and
gives a bound of ˜𝑂(𝑛/𝑑3/2), with minor changes, it holds for unit vectors and with a bound
of ˜𝑂(𝑛/𝑑2)).
(cid:3)

Fact 8.6. Suppose 𝐴 and 𝐵 are both subspaces of dimension 𝑛. Suppose 𝜃 ∈ [0, 𝜋/2). Then
(cid:107)Π𝐴 − Π𝐵Π𝐴(cid:107) (cid:54) sin 𝜃 if and only if for every 𝑥 ∈ 𝐴 there is a 𝑦 ∈ 𝐵 so that

(cid:104)𝑥, 𝑦(cid:105)
(cid:107)𝑥(cid:107) (cid:107)𝑦(cid:107)

(cid:62) cos 𝜃 .

Futhermore, when this holds, since (cid:107)Π𝐴 − Π𝐵Π𝐴(cid:107) = (cid:107)Π𝐵 − Π𝐴Π𝐵 (cid:107) by symmetry, the triangle

inequality yields (cid:107)Π𝐴 − Π𝐵 (cid:107) (cid:54) 2 sin 𝜃.

Proof. Consider the product 𝑅 = Π𝐴Π𝐵. Let 𝑅 = 𝑈Σ𝑉T be its singular value decomposition,
with 𝑢𝑖 and 𝑣𝑖 its 𝑖th left- and right-singular vectors and 𝜎𝑖 its 𝑖th singular value. We show
as an intermediate step that 𝜎𝑛 is at least cos 𝜃 if and only if for every 𝑥 ∈ 𝐴 there is a 𝑦 ∈ 𝐵
so that (cid:104)𝑥, 𝑦(cid:105) (cid:62) (cid:107)𝑥(cid:107) (cid:107)𝑦(cid:107) cos 𝜃.

In one direction, suppose 𝜎𝑛 is at least cos 𝜃. Then take 𝑦 = 𝑅T𝑥. We see that
(cid:104)𝑥, 𝑦(cid:105) = 𝑥TΠ𝐴Π𝐵 𝑦 = 𝑥T𝑅𝑦 = (cid:107)𝑅T𝑥(cid:107)2. Since 𝑥 ∈ 𝐴 and dim 𝐴 = 𝑛 and img(𝑅) ⊆ 𝐴 and

35

singular values are non-negative, if 𝜎𝑛 > 0 then the ﬁrst 𝑛 left-singular vectors of 𝑅 must
span 𝐴. Therefore, decomposing 𝑥 = (cid:205) 𝛼𝑖𝑢𝑖, we must have 𝛼𝑖 = 0 for 𝑖 > 𝑛. So, 𝑅T𝑥 =
(cid:205) 𝛼𝑖𝑉Σ𝑈T𝑢𝑖 = (cid:205) 𝛼𝑖 𝜎𝑖𝑣𝑖, and (cid:107)𝑅T𝑥(cid:107)2 = (cid:107) (cid:205) 𝛼𝑖 𝜎𝑖𝑣𝑖 (cid:107)2 = (cid:205) 𝛼2
𝑛 = (cid:107)𝑥(cid:107)2𝜎2
𝜎2
𝑛.
𝑖
So

(cid:107)𝑣𝑖 (cid:107)2 (cid:62) (cid:205) 𝛼2
𝑖

𝜎2
𝑖

(cid:104)𝑥, 𝑦(cid:105)
(cid:107)𝑥(cid:107) (cid:107)𝑦 (cid:107)

=

(cid:107)𝑅T𝑥(cid:107)2
(cid:107)𝑥(cid:107) (cid:107)𝑅T𝑥(cid:107)

=

(cid:107)𝑅T𝑥 (cid:107)
(cid:107)𝑥(cid:107)

(cid:62) 𝜎𝑛 (cid:62) cos 𝜃 .

In the other direction, suppose 𝜎𝑛 < cos 𝜃. Then take 𝑥 = 𝑢𝑛 and for any 𝑦 ∈ 𝐵,
decompose 𝑦 = (cid:205) 𝛽𝑖𝑣𝑖 where again 𝛽𝑖 = 0 for all 𝑖 > 𝑛. So (cid:104)𝑥, 𝑦(cid:105) = 𝑥TΠ𝐴Π𝐵 𝑦 = (𝑅T𝑥)T𝑦 =
𝜎𝑛𝑣𝑛T𝑦 = 𝜎𝑛𝛽𝑛. Since 𝛽𝑛 (cid:54) (cid:107)𝑦(cid:107), for any 𝑦 we must have

(cid:104)𝑥, 𝑦(cid:105)
(cid:107)𝑥(cid:107) (cid:107)𝑦(cid:107)

𝜎𝑛𝛽𝑛
(cid:107)𝑦 (cid:107)

=

(cid:54) 𝜎𝑛 < cos 𝜃 .

Finally, we show that (cid:107)Π𝐴 −Π𝐵Π𝐴(cid:107) (cid:54) sin 𝜃 if and only if 𝜎𝑛 is at least cos 𝜃. This follows
from the Pythagorean theorem, as Π𝐴 = (Π𝐵Π𝐴) + (Π𝐴 − Π𝐵Π𝐴) and also (Π𝐵Π𝐴)T(Π𝐴 −
Π𝐵Π𝐴) = 0. Thus for any vector 𝑣, we see (cid:107)(Π𝐴 − Π𝐵Π𝐴)𝑣(cid:107)2 = (cid:107)Π𝐴𝑣(cid:107)2 − (cid:107)Π𝐵Π𝐴𝑣(cid:107)2 =
(cid:107)Π𝐴𝑣(cid:107)2 − (cid:107)𝑅TΠ𝐴𝑣(cid:107)2.
(cid:3)
Fact 8.7. Let Φ = (cid:205) 𝑒𝑖 ⊗ 𝑒𝑖 for 𝑒𝑖 the elementary basis vectors in ℝ𝑑.
(cid:107)Π2,3𝑥(cid:107) = 1√
2

If 𝑥 ∈ Φ ⊗ ℝ𝑑 then

(cid:112)1 + 1

𝑑 (cid:107)𝑥(cid:107).

Proof. Since 𝑥 ∈ Φ ⊗ ℝ𝑑, we may write it as 𝑥 = Φ ⊗ 𝑢 for some 𝑢 ∈ ℝ𝑑. We directly
calculate:

(cid:107)Π2,3𝑥 (cid:107)2 = (cid:107) 1
= 1
= 1
= 1

2(Id + 𝑃2,3)𝑥(cid:107)2
4 (cid:107)𝑥(cid:107)2 + 1
2 (cid:107)𝑥(cid:107)2 + 1
2 (cid:107)𝑥(cid:107)2 + 1
2 (cid:107)𝑥(cid:107)2 + 1
2 (cid:107)𝑥(cid:107)2 + 1
2 (cid:107)𝑥(cid:107)2 + 1
2 (cid:107)𝑥(cid:107)2(1 + 1

4 (cid:107)𝑃2,3𝑥 (cid:107)2 + 1
2 (cid:104)𝑥, 𝑃2,3𝑥(cid:105)
2 (cid:104)Φ ⊗ 𝑢, 𝑃2,3(Φ ⊗ 𝑢)(cid:105)
2 (cid:104)(cid:213) 𝑒𝑖 ⊗ 𝑒𝑖 ⊗ 𝑢, (cid:213) 𝑒𝑗 ⊗ 𝑢 ⊗ 𝑒𝑗(cid:105)
(cid:213) (cid:213)(cid:104)𝑒𝑖 , 𝑒𝑗(cid:105)(cid:104)𝑒𝑖 , 𝑢(cid:105)(cid:104)𝑒𝑗 , 𝑢(cid:105)
(cid:213)(cid:104)𝑒𝑖 , 𝑢(cid:105)2
2 (cid:107)𝑢 (cid:107)2
𝑑 ) .

2

2

= 1

= 1
= 1
= 1

(cid:3)
Fact 8.8. Let Φ = (cid:205) 𝑒𝑖 ⊗ 𝑒𝑖 for 𝑒𝑖 the elementary basis vectors in ℝ𝑑. Let 𝑢 ∈ sym2 ⊗ ℝ𝑑 and
decompose 𝑢 = 𝑥 + 𝑦 with 𝑥 ∈ Φ ⊗ ℝ𝑑 and 𝑦 ⊥ 𝑥. Then if (cid:107)Π⊥

(cid:107) 𝑦(cid:107), it holds that

2,3𝑦(cid:107) (cid:62) 1√

2

(cid:107)Π⊥

2,3𝑢 (cid:107)2 (cid:62) (cid:16) 1

4 − 1

√

4

− 𝑂( 1
𝑑 )

2

(cid:17)

(cid:107)𝑢 (cid:107)2.

36

Proof. Let 𝑃3,2,1 = (𝑃1,2,3)−1 be the orthogonal linear operator permuting the tensor modes,
so that the 3-cycle (3 2 1) replaces the third mode with the second, the second mode
with the ﬁrst, and the ﬁrst mode with the third again. By a unitary similarity transform
conjugating by 𝑃3,2,1, since 𝑥 ∈ Φ ⊗ ℝ𝑑, we have

(cid:107)Π1,2𝑃2,3𝑥(cid:107) = (cid:107)(𝑃1,2,3Π1,2𝑃3,2,1)(𝑃1,2,3𝑃2,3)𝑥(cid:107)

= (cid:107)Π2,3𝑃1,2𝑥(cid:107)
= (cid:107)Π2,3𝑥(cid:107)
(cid:112)1 + 1
= 1√
2

𝑑 (cid:107)𝑥(cid:107) ,

where the last step uses Fact 8.7. We write, since Π1,2𝑥 = 𝑥,

(cid:107)Π1,2Π⊥

2,3𝑥(cid:107) = 1
2 (cid:107)Π1,2(𝑥 − 𝑃2,3𝑥)(cid:107)
= 1
2 (cid:107)𝑥 − Π1,2𝑃2,3𝑥(cid:107)
2 (cid:107)𝑥 (cid:107) + 1
(cid:54) 1
2 (cid:107)𝑥(cid:107) + 1
= 1
(cid:16) 1
2 + 1

2 (cid:107)Π1,2𝑃2,3𝑥(cid:107)
(cid:112)1 + 1
𝑑 (cid:107)𝑥(cid:107)
√
2
(cid:17)
+ 𝑂( 1
𝑑 )

=

√

2

(cid:107)𝑥(cid:107) ,

2

2

where the second-to-last step substitutes in (8.1).

(8.1)

(8.2)

Therefore, since 𝑢 = 𝑥 + 𝑦 and Fact 8.7 implies (cid:107)Π⊥

2,3𝑥(cid:107)2 (cid:62) ( 1

2 − 𝑂( 1

𝑑 ))(cid:107)𝑥(cid:107)2 and also by

assumption (cid:107)Π⊥

2,3𝑦(cid:107)2 (cid:62) 1

2 (cid:107) 𝑦(cid:107)2,

(cid:107)Π⊥

2,3𝑢 (cid:107)2 = (cid:107)Π⊥

2,3𝑥 (cid:107)2 + (cid:107)Π⊥

= ( 1

2 − 𝑂( 1

𝑑 ))(cid:107)𝑥(cid:107)2 + 1

2,3𝑦(cid:107)2 + (cid:104)𝑦, Π⊥

2,3𝑥(cid:105)
2 (cid:107) 𝑦(cid:107)2 + (cid:104)𝑦, Π⊥

2,3𝑥(cid:105) .

Since 𝑦 ∈ sym2 ⊗ ℝ𝑑 and therefore Π1,2𝑦 = 𝑦, Cauchy-Schwarz implies (cid:104)𝑦, Π⊥
−(cid:107)Π1,2Π⊥
in and then apply Young’s inequality:

2,3𝑥(cid:107) (cid:107) 𝑦(cid:107) and then by (8.2), this is at least −( 1

2,3𝑥(cid:105) (cid:62)
𝑑 ))(cid:107)𝑥(cid:107). We substitute this

2 + 1

+ 𝑂( 1

√
2

2

(cid:107)Π⊥

2 − 𝑂( 1

2 − 𝑂( 1

2,3𝑢 (cid:107)2 (cid:62) (cid:0) 1
(cid:62) (cid:0) 1
(cid:62) (cid:16) 1
4 − 1
(cid:16) 1
4 − 1

=

4

𝑑 )(cid:1) (cid:107)𝑥(cid:107)2 + 1
𝑑 )(cid:1) (cid:107)𝑥(cid:107)2 + 1
− 𝑂( 1
𝑑 )
(cid:17)

(cid:17)

− 𝑂( 1
𝑑 )

√
2

√
2

4

(cid:107)𝑢(cid:107)2 .

√
2

(cid:16) 1
2 + 1
(cid:16) 1
2 + 1

2 (cid:107) 𝑦(cid:107)2 −
2 (cid:107) 𝑦(cid:107)2 −
(cid:16) 1
(cid:107)𝑥(cid:107)2 +
4 − 1

√
2

√
2

4

+ 𝑂( 1
𝑑 )

2

+ 𝑂( 1
𝑑 )
(cid:17)

2
− 𝑂( 1
𝑑 )

(cid:107) 𝑦(cid:107)2

(cid:17)

(cid:17)

(cid:107)𝑥(cid:107) (cid:107)𝑦(cid:107)

2 (cid:107)𝑥(cid:107)2 + 1
( 1

2 (cid:107) 𝑦(cid:107)2)

37

(cid:3)

Fact 8.9. Let 𝑢 ∈ sym2 ⊗ ℝ𝑑. If

(cid:107)Π⊥

2,3𝐴−1/2𝑢(cid:107)
(cid:107)𝐴−1/2𝑢 (cid:107)

(cid:62) 1 − 𝜇

, then

(cid:62) (cid:113) 1

4 − 1

√
2

4

− 𝑂( 1

𝑑 ) .

for 𝜇 (cid:54) 1 − 1√
2

− 2

√
2√
𝑑

(cid:107)Π⊥
2,3𝑢 (cid:107)
(cid:107)𝑢 (cid:107)
Proof. Write 𝑢 = 𝑥 + 𝑦 where 𝑥 = ( 1
inequality and Fact 8.7,

𝑑 ΦΦT ⊗ Id)𝑢 and 𝑦 ⊥ 𝑥. If (cid:107)𝑥(cid:107) > 2(cid:107)𝑦(cid:107), then by triangle

(cid:107)Π⊥
2,3𝑢 (cid:107)
(cid:107)𝑢(cid:107)

(cid:62)

(cid:62)

>

(cid:62)

>

(cid:107)𝑥(cid:107) − 1

2 (cid:107)𝑥(cid:107)

2,3𝑦(cid:107)

(cid:107)Π⊥

(cid:107)Π⊥

2,3𝑥(cid:107) − (cid:107)Π⊥
(cid:107)𝑢 (cid:107)
2,3𝑥(cid:107) − (cid:107) 𝑦(cid:107)
(cid:107)𝑢(cid:107)

(cid:16) 1√

2

(cid:16) 1√

2

(cid:16) 1√

2

(cid:17)

(cid:17)

− 1

− 𝑂( 1
𝑑 )
(cid:107)𝑢 (cid:107)
2 − 𝑂( 1
𝑑 )
(cid:107)𝑥(cid:107) + (cid:107) 𝑦(cid:107)
(cid:17)
2 − 𝑂( 1
𝑑 )
3
2 (cid:107)𝑥(cid:107)

− 1

(cid:107)𝑥(cid:107)

(cid:107)𝑥(cid:107)

> (cid:113) 1

4 − 1

√

4

− 𝑂( 1

𝑑 ) .

2

Thus for the remainder of the argument, we assume (cid:107)𝑥 (cid:107) (cid:54) 2(cid:107)𝑦(cid:107).

𝑑 𝑥 and (cid:107)𝐴−1/2𝑢 (cid:107)2 = 𝑑2
1

(cid:107)𝑦(cid:107)2 + 𝑑(cid:107)𝑥 (cid:107)2 (cid:62) 𝑑2
1

(cid:107) 𝑦(cid:107)2, so by triangle

Then 𝐴−1/2𝑢 = 𝑑1𝑦 +

inequality,

√

(cid:107)Π⊥

2,3𝑦(cid:107) (cid:62) 𝑑−1
(cid:62) 𝑑−1

√

(cid:107)Π⊥

2,3𝑥(cid:107)

𝑑
1 (cid:107)Π⊥
2,3𝐴−1/2𝑢 (cid:107) −
𝑑1
√
𝑑
1 (cid:107)Π⊥
2,3𝐴−1/2𝑢 (cid:107) −
𝑑1
√
𝑑
2,3𝐴−1/2𝑢 (cid:107) − 2
1 (cid:107)Π⊥
𝑑1
1 (1 − 𝜇)(cid:107)𝐴−1/2𝑢 (cid:107) − 40

(cid:107)𝑥(cid:107)

(cid:107)𝑦 (cid:107)
√

𝑑
𝑑1

(cid:62) 𝑑−1

(cid:62) 𝑑−1

(cid:62) (1 − 𝜇)(cid:107)𝑦(cid:107) − 2
(cid:17)
(cid:62) (cid:16)

1 − 𝜇 − 2

√
2√
𝑑

√

𝑑
𝑑1

(cid:107) 𝑦(cid:107)

(cid:107) 𝑦(cid:107) .

(cid:107) 𝑦(cid:107)

38

Therefore, the lemma follows by Fact 8.8, as long as 1 − 𝜇 − 2

√
2√
𝑑

(cid:62) 1√
2

.

(cid:3)

Proof of Corollary 8.3. To lower bound 𝜅, we need a lower bound on the least singular value
of Π⊥
(𝐻𝐻T)−1/2𝐻, where 𝐻 is the matrix with columnwise blocks of 𝑎𝑖 ⊗ 𝑎𝑖 ⊗ 𝐵𝑖. By
2,3
2,3𝐴−1/2𝐻 has all singular values
Lemma C.1, with probability 1 − 𝑜(1), it holds that
within 1 ± ˜𝑂(𝑛/𝑑2). This means that for all 𝑢 ∈ img(𝐻), we have (cid:107)Π⊥
2,3𝐴−1/2𝑢(cid:107)/(cid:107)𝐴−1/2𝑢 (cid:107) (cid:62)
1 − ˜𝑂(𝑛/𝑑2). Therefore, by Fact 8.9, (cid:107)Π⊥
𝑑 ) − ˜𝑂(𝑛/𝑑2). This
inequality holding for all 𝑢 ∈ img(𝐻) is equivalent to 𝜅, the smallest singular value of
− 𝑂( 1
Π⊥
(cid:3)
2,3

(𝐻𝐻T)−1/2𝐻, being at least (cid:113) 1

2,3𝑢(cid:107)/(cid:107)𝑢(cid:107) (cid:62) (cid:113) 1

𝑑 ) − ˜𝑂(𝑛/𝑑2).

4 − 1

4 − 1

− 𝑂( 1

√
2
𝑑1

Π⊥

√

√

4

2

4

2

The remainder of this section is devoted to the proof of Lemma C.1. At a high level,
this proof follows the strategy laid out in in [Ver12] to prove Theorem 5.62 there, but the
random matrix we need to control is much more complicated than is handled there.

8.1 Notation

Throughout we use the following notation.

1. 𝑑, 𝑛 ∈ ℕ are natural numbers.
2. 𝑑1 = (cid:112)(𝑑2 + 2𝑑)/2 = Θ(𝑑) and 𝑑2 = 𝑑−1/2((cid:112)(𝑑 + 2)/2 − 1) = Θ(1).

3. 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑 are iid random unit vectors.

4. 𝐵𝑖 ∈ ℝ𝑑×(𝑑−1) for 𝑖 (cid:54) 𝑛 is a matrix with columns which form a random orthonormal
basis for the orthogonal complement of 𝑎𝑖 in ℝ𝑑. (Chosen independently from 𝑎 𝑗 , 𝐵𝑗
for 𝑗 ≠ 𝑖.)

5. Σ = 𝔼𝑎(𝑎𝑎(cid:62) ⊗ 𝑎𝑎(cid:62)) ∈ ℝ𝑑2×𝑑2 is the 4-th moment matrix of a random 𝑑-dimensional

unit vector.

6. 𝐴 = 𝔼𝑎(𝑎𝑎(cid:62) ⊗ 𝑎𝑎(cid:62) ⊗ Id) = Σ ⊗ Id ∈ ℝ𝑑2×𝑑2×𝑑2 is Σ “lifted” to a 6-tensor.

7. Φ ∈ ℝ𝑑2 is the vector Φ = (cid:205)𝑖(cid:54)𝑑 𝑒 ⊗2

𝑖

, where 𝑒𝑖 is the 𝑖-th standard basis vector.

8. Πsym ∈ ℝ𝑑2×𝑑2 is the projector to the symmetric subspace of ℝ𝑑2 (i.e. the span of

vectors 𝑥⊗2 for 𝑥 ∈ ℝ𝑑).

9. 𝑃 ∈ ℝ𝑑3×𝑑3 is the permutation matrix which swaps second and third tensor modes.

Concretely, (𝑃𝑥)𝑖𝑗𝑘 = 𝑥𝑖𝑘 𝑗 for 𝑖, 𝑗, 𝑘 ∈ [𝑑].

10. 𝑆𝑖, for 𝑖 (cid:54) 𝑛, is the 𝑑3 × 𝑑 − 1 matrix given by 𝑆𝑖 = 𝐴−1/2(𝑎𝑖 ⊗ 𝑎𝑖 ⊗ 𝐵𝑖)

39

11. 𝑅𝑖, for 𝑖 (cid:54) 𝑛, is the 𝑑3 × 𝑑 − 1 matrix given by 𝑅𝑖 = 𝑆𝑖 − 𝑃𝑆𝑖.

12. 𝑅𝑇, for any 𝑇 ⊆ [𝑛], is the 𝑑3 × |𝑇 |(𝑑 − 1) matrix with |𝑇 | blocks of columns, given by

{𝑅𝑖 }𝑖∈𝑇.

13. 𝑅 = 𝑅[𝑛] ∈ ℝ𝑑3×𝑛(𝑑−1) contains all blocks of columns 𝑅𝑖.

8.2 Fourth Moment Identities

Fact 8.10.

(cid:0)𝔼 𝑎𝑎(cid:62) ⊗ 𝑎𝑎(cid:62)(cid:1) −1/2

= 𝑑1Πsym − 𝑑2ΦΦ(cid:62)

and for any unit 𝑥 and matrix 𝑋,

𝐴−1/2(𝑥 ⊗ 𝑥 ⊗ 𝑋) = [𝑑1(𝑥 ⊗ 𝑥) − 𝑑2Φ] ⊗ 𝑋 .

Proof. The ﬁrst statement follows from Fact C.4 in [HSSS16]. For the second, notice that
𝐴 = (𝔼 𝑎𝑎(cid:62) ⊗ 𝑎𝑎(cid:62))−1/2 ⊗ Id, so

𝐴−1/2 =

(cid:34)(cid:114) 𝑑2 + 2𝑑
2

Πsym +

(cid:32)

1 −

(cid:33)

(cid:114) 𝑑 + 2
2

1
√

𝑑

(cid:35)

ΦΦ(cid:62)

⊗ Id .

So we can expand 𝐴−1/2(𝑥 ⊗ 𝑥 ⊗ 𝑌) as

(cid:34)(cid:114) 𝑑2 + 2𝑑
2

Πsym(𝑥 ⊗ 𝑥) +

(cid:32)

1 −

(cid:33)

(cid:114) 𝑑 + 2
2

1
√

𝑑

(cid:35)

ΦΦ(cid:62)(𝑥 ⊗ 𝑥)

⊗ 𝑋 .

Since Πsym(𝑥 ⊗ 𝑥) = (𝑥 ⊗ 𝑥) and Φ(cid:62)(𝑥 ⊗ 𝑥) = (cid:107)𝑥(cid:107)2 = 1, this simplifes to

(cid:34)(cid:114) 𝑑2 + 2𝑑
2

(𝑥 ⊗ 𝑥) +

(cid:32)

1 −

(cid:114) 𝑑 + 2
2

1
√

𝑑

(cid:33)

(cid:35)

Φ

⊗ 𝑋 .

8.3 Matrix Product Identities

Fact 8.11. 𝑆𝑖 = (𝑑1(𝑎𝑖 ⊗ 𝑎𝑖) − 𝑑2Φ) ⊗ 𝐵𝑖.

Proof. Follows from the deﬁnition of 𝑆𝑖 and Fact 8.10.

(cid:3)

(cid:3)

Fact 8.12. 𝑆(cid:62)
𝑖

𝑆𝑗 = (𝑑2
1

(cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)2 − 2𝑑1𝑑2 + 𝑑2

2𝑑)𝐵(cid:62)

𝑖

𝐵𝑗

40

Proof. Expanding via Fact 8.11,

𝑖 𝑆𝑗 = [(𝑑1(𝑎𝑖 ⊗ 𝑎𝑖) − 𝑑2Φ) ⊗ 𝐵𝑖](cid:62)[(𝑑1(𝑎 𝑗 ⊗ 𝑎 𝑗) − 𝑑2Φ) ⊗ 𝐵𝑗]
𝑆(cid:62)

= (𝑑2

1 (cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)2 − 2𝑑1𝑑2 + 𝑑2

2𝑑)𝐵(cid:62)

𝑖 𝐵𝑗 .

Here we have used that Φ(cid:62)(𝑎𝑖 ⊗ 𝑎𝑖) = (cid:107)𝑎𝑖 (cid:107)2 = 1 and that Φ(cid:62)Φ = (cid:107)Φ(cid:107)2 = 𝑑.

Fact 8.13. 𝑃 = 𝑃(cid:62) = 𝑃−1 and hence 𝑃2 = Id

Proof. Exercise.

(cid:3)

(cid:3)

Fact 8.14. For any matrics 𝐵, 𝐵(cid:48) ∈ ℝ𝑑×𝑚, we have (Φ ⊗ 𝐵)(cid:62)𝑃(Φ ⊗ 𝐵(cid:48)) = 𝐵(cid:62)𝐵(cid:48)
Proof. The 𝑠, 𝑡-th entry of (Φ ⊗ 𝐵)(cid:62)𝑃(Φ ⊗ 𝐵(cid:48)) is given by (cid:205)𝑢𝑣𝑤(cid:54)𝑑 Φ𝑢𝑣𝐵𝑠𝑤Φ𝑢𝑤𝐵(cid:48)
𝑡𝑣. The only
nonzero terms come from 𝑢 = 𝑣 = 𝑤, because otherise Φ𝑢𝑣Φ𝑢𝑤 = 0. So this simplifes to
(cid:205)𝑢(cid:54)𝑑 𝐵𝑠𝑢𝐵(cid:48)
(cid:3)

𝑡𝑢 = (𝐵(cid:62)𝐵(cid:48))𝑠𝑡.

Fact 8.15. For any vectors 𝑎, 𝑎(cid:48) ∈ ℝ𝑑 and matrices 𝐵, 𝐵(cid:48) ∈ ℝ𝑑×(𝑑−1), we have (𝑎 ⊗ 𝑎 ⊗ 𝐵)(cid:62)𝑃(𝑎(cid:48) ⊗
𝑎(cid:48) ⊗ 𝐵(cid:48)) = (cid:104)𝑎, 𝑎(cid:48)(cid:105)(𝐵(cid:62)𝑎(cid:48))([𝐵(cid:48)](cid:62)𝑎)(cid:62)

Proof. Since 𝑃 does not touch the ﬁrst mode of (𝑅𝑑)⊗3 it is enough to compute (𝑎 ⊗ 𝐵)(cid:62)𝑃(cid:48)(𝑎(cid:48) ⊗
𝐵(cid:48)) where 𝑃(cid:48) is the mode-swap matrix for (ℝ𝑑)⊗2. The 𝑠, 𝑡-th entry of this matrix is given by

(cid:213)

𝑢𝑣(cid:54)𝑑

𝑎𝑢𝐵𝑠𝑣 𝑎(cid:48)

𝑣𝐵(cid:48)

𝑡𝑢 = ((cid:213)

𝑎𝑣𝐵(cid:48)

𝑡𝑢)((cid:213)

𝑎(cid:48)
𝑣𝐵𝑠𝑣) .

𝑢(cid:54)𝑑

𝑣(cid:54)𝑑

(cid:3)

Fact 8.16. For any vector 𝑎 ∈ ℝ𝑑 and matrices 𝐵, 𝐵(cid:48) ∈ ℝ𝑑×𝑚, we have (𝑎 ⊗ 𝑎 ⊗ 𝐵)(cid:62)𝑃(Φ ⊗ 𝐵(cid:48)) =
(𝐵(cid:62)𝑎)([𝐵(cid:48)](cid:62)𝑎)(cid:62)

Proof. The 𝑠, 𝑡-th entry of the product is given by

(cid:213)

𝑢𝑣𝑤(cid:54)𝑑

𝑎𝑢 𝑎𝑣𝐵𝑠𝑤Φ𝑢𝑤𝐵(cid:48)

𝑡𝑣 =

(cid:213)

𝑢𝑣

𝑎𝑢 𝑎𝑣𝐵𝑠𝑢𝐵(cid:48)

𝑡𝑣 = (𝐵(cid:62)𝑎)𝑠([𝐵(cid:48)](cid:62)𝑎)𝑡

Fact 8.17. 𝑆(cid:62)
𝑖

2𝐵(cid:62)
Proof. Expanding 𝑆𝑖 , 𝑆𝑗 using Fact 8.11, we obtain

(cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)(𝐵(cid:62)
𝑖

𝑃𝑆𝑗 = 𝑑2
1

𝑎𝑖)(cid:62) + 𝑑2

𝑎 𝑗)(𝐵(cid:62)
𝑗

𝑖

𝐵𝑗.

(cid:3)

𝑖 𝑃𝑆𝑗 = [𝑑1(𝑎𝑖 ⊗ 𝑎𝑖) ⊗ 𝐵𝑖](cid:62)𝑃[𝑑1(𝑎 𝑗 ⊗ 𝑎 𝑗) ⊗ 𝐵𝑗]
𝑆(cid:62)
− [𝑑1(𝑎𝑖 ⊗ 𝑎𝑖) ⊗ 𝐵𝑖](cid:62)𝑃[𝑑2Φ ⊗ 𝐵𝑗]
− [𝑑2Φ ⊗ 𝐵𝑖](cid:62)𝑃[𝑑1(𝑎 𝑗 ⊗ 𝑎 𝑗) ⊗ 𝐵𝑗]

41

+ [𝑑2Φ ⊗ 𝐵𝑖](cid:62)𝑃[𝑑2Φ ⊗ 𝐵𝑗]

Simplifying the terms individually using Fact 8.14, Fact 8.15, and Fact 8.16,

(𝑎𝑖 ⊗ 𝑎𝑖 ⊗ 𝐵𝑖)(cid:62)𝑃(𝑎 𝑗 ⊗ 𝑎 𝑗 ⊗ 𝐵𝑗) = (cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)(𝐵(cid:62)
(Φ ⊗ 𝐵𝑖)(cid:62)𝑃(Φ ⊗ 𝐵𝑗) = 𝐵(cid:62)

𝑖 𝐵𝑗

𝑖 𝑎 𝑗)(𝐵(cid:62)

𝑗 𝑎𝑖)(cid:62)

(𝑎𝑖 ⊗ 𝑎𝑖 ⊗ 𝐵𝑖)(cid:62)𝑃(Φ ⊗ 𝐵𝑗) = 0
(Φ ⊗ 𝐵𝑖)(cid:62)𝑃(𝑎 𝑗 ⊗ 𝑎 𝑗 ⊗ 𝐵𝑗) = 0 .

where the last two equalities follow from 𝐵(cid:62)
𝑖

𝑎𝑖 = 0, 𝐵(cid:62)
𝑗

𝑎 𝑗 = 0.

(cid:3)

Fact 8.18. 𝑅(cid:62)
𝑖
particular, 𝑅(cid:62)
𝑖

𝑅 𝑗 = 2(𝑑2
1
𝑅𝑖 = 2(𝑑2
1

(cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)2 − 2𝑑1𝑑2 + 𝑑2
2
− 2𝑑1𝑑2 + 𝑑2
2

(𝑑 − 1))Id.

(𝑑 − 1))𝐵(cid:62)
𝑖

𝐵𝑗 − 2𝑑2
1

(cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)(𝐵(cid:62)
𝑖

𝑎 𝑗)(𝐵(cid:62)
𝑗

𝑎𝑖)(cid:62), and in

Proof. By the deﬁnition of 𝑅𝑖 , 𝑅 𝑗 we expand

𝑖 𝑅 𝑗 = (𝑆𝑖 − 𝑃𝑆𝑖)(cid:62)(𝑆𝑗 − 𝑃𝑆𝑗) .
𝑅(cid:62)

We can expand the product and use Fact 8.13 to get

Applying Fact 8.12 and Fact 8.17, we get

𝑖 𝑅 𝑗 = 2𝑆(cid:62)
𝑅(cid:62)

𝑖 𝑆𝑗 − 2𝑆(cid:62)

𝑖 𝑃𝑆𝑗 .

𝑖 𝑅 𝑗 = 2(𝑑2
𝑅(cid:62)

1 (cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)2 − 2𝑑1𝑑2 + 𝑑2

2𝑑)𝐵(cid:62)

𝑖 𝐵𝑗 − 2𝑑2

1 (cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)(𝐵(cid:62)

𝑖 𝑎 𝑗)(𝐵(cid:62)

𝑗 𝑎𝑖)(cid:62) − 2𝑑2

2𝐵(cid:62)

𝑖 𝐵𝑗 .

Simplifying ﬁnishes the proof.

(cid:3)

8.4 Naive Spectral Norm Estimate

We will need an upper bound on the spectral norm of the matrix 𝑅, which we obtain
by a matrix Chernoﬀ bound. To prove that, we need spectral bounds on certain second
moments.

Fact 8.19. For all 𝑖 (cid:54) 𝑛,

(cid:107) 𝔼 𝑆𝑖𝑆(cid:62)

𝑖 (cid:107) (cid:54) 1 .

Proof. Expanding by the deﬁnition of 𝑆𝑖,

𝔼 𝑆𝑖𝑆(cid:62)

𝑖 = 𝐴−1/2 𝔼(𝑎𝑖 𝑎(cid:62)

𝑖 ⊗ 𝑎𝑖 𝑎(cid:62)

𝑖 ⊗ 𝐵𝑖𝐵(cid:62)

𝑖 )𝐴−1/2 (cid:22) 𝐴−1/2(𝔼 𝑎𝑖 𝑎(cid:62)

𝑖 ⊗ 𝑎𝑖 𝑎(cid:62)

𝑖 ⊗ Id)𝐴−1/2 (cid:22) Id.

since 𝐵𝑖𝐵(cid:62)

𝑖 = Id − 𝑎𝑖 𝑎(cid:62)

𝑖

(cid:22) Id. The last equality uses the deﬁnition of 𝐴.

(cid:3)

42

Fact 8.20. For all 𝑖 (cid:54) 𝑛,

(cid:107) 𝔼 𝑅𝑖𝑅(cid:62)

𝑖 (cid:107) (cid:54) 4 .

Proof. Follows from Fact 8.19 and the deﬁnition of 𝑅𝑖, and the fact that (cid:107)𝑃(cid:107) (cid:54) 1, by the
manipulations

(cid:107) 𝔼(𝑅𝑖𝑅(cid:62)

𝑖 )(cid:107) = (cid:107) 𝔼 𝑆𝑖𝑆(cid:62)

𝑖 − 𝑃𝑆𝑖𝑆(cid:62)

𝑖 − 𝑆𝑖𝑆(cid:62)

𝑖 𝑃 + 𝑃𝑆𝑖𝑆(cid:62)

𝑖 𝑃(cid:107) (cid:54) 4(cid:107) 𝔼 𝑆𝑖𝑆(cid:62)

𝑖 (cid:107) (cid:54) 4 .

Fact 8.21. (cid:107) 𝔼 𝑅𝑅(cid:62)(cid:107) (cid:54) 𝑂(𝑛)

Proof. Since 𝔼 𝑅𝑅(cid:62) = 𝔼 (cid:205)𝑖(cid:54)𝑛 𝑅𝑖𝑅(cid:62)
𝑖

, this follows from the triangle inequality and Fact 8.20.
(cid:3)

Fact 8.22. (cid:107) 𝔼 𝑅(cid:62)𝑅(cid:107) (cid:54) 2(𝑑2
1

− 2𝑑1𝑑2 + 𝑑2
2

(𝑑 − 1)) (cid:54) 𝑂(𝑑2)

(cid:3)

Proof. 𝑅(cid:62)𝑅 is a block matrix with 𝑖𝑗-th block being 𝑅(cid:62)
𝑖
since 𝑅(cid:62)
𝑖
2(𝑑2
1

(𝑑 − 1))Id. So, 𝔼 𝑅(cid:62)𝑅 = 2(𝑑2
1

− 2𝑑1𝑑2 + 𝑑2
2

− 2𝑑1𝑑2 + 𝑑2
2

𝑅 𝑗 is independent of 𝑅 𝑗 and has expectation zero. At the same time 𝑅(cid:62)
𝑖

𝑅 𝑗. If 𝑖 ≠ 𝑗 we have 𝔼 𝑅(cid:62)
𝑖

(𝑑 − 1))Id.

𝑅 𝑗 = 0,
𝑅𝑖 =
(cid:3)

Fact 8.23. 𝔼 (cid:107)𝑅(cid:107) (cid:54) 𝑂(log 𝑑 · max(𝑑,

√

𝑛)).

Proof. First of all, note that 𝔼 𝑅 = 0, because 𝔼(𝑎𝑖 ⊗ 𝑎𝑖 ⊗ 𝐵𝑖) = 𝔼𝑎 𝑎𝑖 ⊗ 𝑎𝑖 ⊗ (𝔼[𝐵𝑖 | 𝑎𝑖]) = 0,
since each column of 𝐵𝑖 is a random unit vector in the orthogonal complement of 𝑎𝑖.

Also, note that with probability 1, each of the column blocks 𝑅𝑖 has (cid:107)𝑅𝑖 (cid:107) (cid:54) 𝑂(𝑑),

because (cid:107)𝑎𝑖 ⊗ 𝑎𝑖 ⊗ 𝐵𝑖 (cid:107) (cid:54) 1 and (cid:107)𝐴−1/2(cid:107) (cid:54) 𝑂(𝑑).

The matrix 𝑅 is a sum of independent random matrices. To apply Matrix Chernoﬀ, we
need the bounds on its second moments, provided by Fact 8.21 and Fact 8.22. The proof is
(cid:3)
concluded by applying Matrix Chernoﬀ.

8.5 Oﬀ-Diagonal Second Moment Estimates

We will eventually need to bound the norms of some oﬀ-diagonal blocks of the matrix
𝑅(cid:62)𝑅. We prove some useful inequalities for that eﬀort now.

Fact 8.24. Let 𝑋 , 𝑌 be real matrices. Then (𝑋 − 𝑌)(𝑋 − 𝑌)(cid:62) (cid:22) 2(𝑋𝑋(cid:62) + 𝑌𝑌(cid:62)).

Proof. By expanding, (𝑋 −𝑌)(𝑋 −𝑌)(cid:62) = 𝑋𝑋(cid:62)+𝑌𝑌(cid:62)−𝑋𝑌(cid:62)−𝑋𝑌(cid:62). Since (𝑋 +𝑌)(𝑋 +𝑌)(cid:62) (cid:23) 0,
we obtain 𝑋𝑋(cid:62) + 𝑌𝑌(cid:62) (cid:23) −𝑋𝑌(cid:62) − 𝑌𝑋(cid:62), ﬁnishing the proof.
(cid:3)

Fact 8.25. For any 𝑆 ⊆ [𝑛] and ﬁxed 𝑎𝑖 for 𝑖 ∉ 𝑆, we have (cid:107) 𝔼𝑅𝑆 𝑅(cid:62)
𝑆

𝑅𝑆𝑅(cid:62)
𝑆

𝑅𝑆 (cid:107) (cid:54) 𝑂(|𝑆| · (cid:107)𝑅𝑆 (cid:107)2).

43

Proof. We know from Fact 8.20 that 𝔼 𝑅𝑖𝑅(cid:62)
𝑖
To prove the ﬁnal bound we push the expectation inside the matrix product:

(cid:22) 4Id. Hence 𝔼 𝑅𝑆𝑅(cid:62)

𝑆 = (cid:205)𝑖∈𝑆 𝔼 𝑅𝑖𝑅(cid:62)

𝑖

(cid:22) 4|𝑆|Id.

𝑅(cid:62)
𝑆

𝔼
𝑅𝑆

𝑅𝑆𝑅(cid:62)

𝑆 𝑅𝑆 = 𝑅(cid:62)

𝑆

(𝔼
𝑅𝑆

𝑅𝑆𝑅(cid:62)

𝑆 )𝑅𝑆 (cid:22) 4|𝑆| · 𝑅(cid:62)

𝑆

𝑅𝑆 (cid:22) 4 · |𝑆| · (cid:107)𝑅𝑆 (cid:107)2 · Id .

Fact 8.26. For any 𝑆 ⊆ [𝑛] and ﬁxed 𝑎𝑖 for 𝑖 ∉ 𝑆, we have (cid:107) 𝔼𝑅𝑆 𝑅(cid:62)
𝑆
𝑂(𝑑3 · (cid:107) (cid:205)

(cid:107)).

𝑖∈𝑆 𝑎𝑖 𝑎(cid:62)

𝑖

(cid:3)

𝑅𝑆𝑅(cid:62)

𝑆

𝑅𝑆 (cid:107) (cid:54) 𝑂(|𝑆| · 𝑑2) +

Proof. Consider the 𝑗, 𝑘-th block of 𝔼𝑅𝑆 𝑅(cid:62)
𝑆

𝑅𝑆𝑅(cid:62)

𝑆

𝑅𝑆, which expands to

𝔼
𝑅 𝑗 ,𝑅𝑘

(cid:213)

𝑖∈𝑆

𝑅(cid:62)

𝑗 𝑅𝑖𝑅(cid:62)

𝑖 𝑅𝑘 .

Since 𝔼 𝑅 𝑗 = 𝔼 𝑅𝑘 = 0, unless 𝑗 = 𝑘 the whole expression vanishes in expectation. Consider
the case 𝑗 = 𝑘. Here we have the matrix (cid:205)
𝑅𝑖
according to Fact 8.18 to get

𝑅 𝑗. We expand the matrix 𝑅(cid:62)
𝑗

𝑖∈𝑆 𝔼 𝑅(cid:62)

𝑅𝑖𝑅(cid:62)
𝑖

𝑗

𝑅(cid:62)

𝑗 𝑅𝑖 = 2(𝑑2

1 (cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)2 − 2𝑑1𝑑2 + 𝑑2

(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

2(𝑑 − 1))𝐵(cid:62)
𝑗 𝐵𝑖
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:125)

(cid:124)

(cid:123)(cid:122)
def
= 𝑋𝑖𝑗

− 2𝑑2

1 (cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)(𝐵(cid:62)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:123)(cid:122)
def
= 𝑌𝑖𝑗

(cid:124)

𝑖 𝑎 𝑗)(cid:62)
𝑗 𝑎𝑖)(𝐵(cid:62)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:125)

We will need the following two spectral bounds.

(cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)2 − 2𝑑1𝑑2 + 𝑑2
2

1. (cid:107) 𝔼 4(𝑑2
1
We note that 𝐵(cid:62)
𝑗
By deﬁnition, |2𝑑1𝑑2|, 𝑑2
2

(𝑑 − 1))2𝐵(cid:62)
𝑗
𝐵𝑗 (cid:22) Id, so it is enough to bound 𝔼 4(𝑑2
1
(cid:54) 𝑂(𝑑2), so we have

𝐵𝑗 (cid:107) (cid:54) 𝑂(𝑑2).

(𝑑 − 1) (cid:54) 𝑂(𝑑) and 𝑑2
1

𝐵𝑖𝐵(cid:62)
𝑖

𝐵𝑖𝐵(cid:62)
𝑖

(cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)2−2𝑑1𝑑2+𝑑2
2

(𝑑−1))2.

𝔼 4(𝑑2

1 (cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)2 − 2𝑑1𝑑2 + 𝑑2

2(𝑑 − 1))2 (cid:54) 𝑂(𝑑4) · 𝔼((cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)2 + 𝑂(1/𝑑))2 (cid:54) 𝑂(𝑑2) .

2. 4𝑑4
1

(cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)2(𝐵(cid:62)
𝑗

We note that (𝐵(cid:62)
𝑖

𝑎 𝑗)(cid:62)(𝐵(cid:62)
𝑖

𝑎𝑖)(𝐵(cid:62)
𝑖
𝑎 𝑗)(cid:62)(𝐵(cid:62)
𝑖

𝑎𝑖)(cid:62) (cid:22) 4𝑑4
1

𝑎 𝑗)(𝐵(cid:62)
𝑗
𝑎 𝑗) = (cid:107)𝐵(cid:62)
𝑎 𝑗 (cid:107)2 (cid:54) 1. So,
𝑖

(cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)2𝐵(cid:62)
𝑗

𝑎𝑖 𝑎(cid:62)
𝑖

𝐵𝑗 .

4𝑑4

1 (cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)2(𝐵(cid:62)

𝑗 𝑎𝑖)(𝐵(cid:62)

𝑖 𝑎 𝑗)(cid:62)(𝐵(cid:62)

𝑖 𝑎 𝑗)(𝐵(cid:62)

𝑗 𝑎𝑖)(cid:62) (cid:22) 4𝑑4

1 (cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)2𝐵(cid:62)

𝑗 𝑎𝑖 𝑎(cid:62)

𝑖 𝐵𝑗 .

We return to bounding (cid:205)
ﬁxed and the expectation is over 𝑅 𝑗.

𝑖∈𝑆 𝔼 𝑅(cid:62)

𝑅𝑖𝑅(cid:62)
𝑖

𝑗

𝑅 𝑗, where we recall that each 𝑅𝑖 in the sum is

44

By Fact 8.24,

(cid:213)

𝑖∈𝑆

𝔼 𝑅(cid:62)

𝑗 𝑅𝑖𝑅(cid:62)

𝑖 𝑅 𝑗 (cid:22) 2 (cid:213)

𝔼 𝑋𝑖𝑗𝑋(cid:62)

𝑖𝑗 + 2 𝔼 𝑌𝑖𝑗𝑌(cid:62)

𝑖𝑗

𝑖∈𝑆

(cid:22) 2 (cid:213)

(cid:107) 𝔼 𝑋𝑖𝑗𝑋(cid:62)

𝑖𝑗 (cid:107) · Id + 2 (cid:213)

𝑌𝑖𝑗𝑌(cid:62)

𝑖𝑗 by triangle inequality

𝑖∈𝑆

(cid:22) 𝑂(|𝑆| · 𝑑2) · Id + 2 (cid:213)

𝑖∈𝑆
𝑌𝑖𝑗𝑌(cid:62)
𝑖𝑗 by (1) above

𝑖∈𝑆
(cid:22) 𝑂(|𝑆| · 𝑑2) · Id + 2 𝔼 𝐵𝑗

(cid:213)

𝑖∈𝑆

4𝑑4

1 (cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)2𝑎𝑖 𝑎(cid:62)

𝑖 𝐵𝑗 by (2) above

(cid:22) 𝑂(|𝑆| · 𝑑2) · Id + 𝑂(𝑑4

1) · 𝔼(cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)2 · (cid:107) (cid:213)

𝑎𝑖 𝑎(cid:62)

𝑖 (cid:107) · Id by (cid:107)𝐵𝑗 (cid:107) (cid:54) 1

(cid:22) 𝑂(|𝑆| · 𝑑2) · Id + 𝑂(𝑑3) · ·(cid:107) (cid:213)

𝑎𝑖 𝑎(cid:62)

𝑖 (cid:107) · Id by 𝔼(cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)2 (cid:54) 𝑂(1/𝑑)

𝑖∈𝑆

𝑖∈𝑆

(cid:3)

8.6 Matrix Decoupling

Fact 8.27 (Block Matrix Decoupling, similar to Lemma 5.63 of [Ver12]). Let 𝑅 be an 𝑁 × 𝑛𝑚
random matrix, consisting of 𝑛 blocks 𝑅𝑖 of dimension 𝑁 × 𝑚. Suppose that the blocks satisfy
𝑅(cid:62)
𝑅𝑖 = Id. For a subset 𝑆 ⊆ [𝑛], let 𝑅𝑆 ∈ ℝ𝑁×𝑛|𝑆| matrix consisting of only the blocks in 𝑆. Let
𝑖
𝑇 ⊆ [𝑛] be uniformly random. Then

𝔼
𝑅

(cid:107)𝑅(cid:62)𝑅 − Id(cid:107) (cid:54) 4 𝔼
𝑅,𝑇

(cid:107)𝑅𝑇 𝑅(cid:62)

[𝑛]\𝑇

(cid:107) .

Proof. Following the argument of Vershynin [Ver12], we note that

(cid:107)𝑅(cid:62)𝑅 − Id(cid:107) = | sup
(cid:107)𝑥 (cid:107)=1

(cid:107)𝑅𝑥(cid:107)2 − 1|

and that for 𝑥 = (𝑥1, . . . , 𝑥𝑛) ∈ ℝ𝑛𝑚,

(cid:107)𝑅𝑥 (cid:107)2 =

(cid:213)

𝑖 𝑅(cid:62)
𝑥(cid:62)

𝑖 𝑅𝑖 𝑥𝑖 + (cid:213)

𝑖 𝑅(cid:62)
𝑥(cid:62)

𝑖 𝑅 𝑗 𝑥 𝑗 = 1 + (cid:213)

𝑖 𝑅(cid:62)
𝑥(cid:62)

𝑖 𝑅 𝑗 𝑥 𝑗

𝑖∈[𝑛]

𝑖≠𝑗∈[𝑛]

𝑖≠𝑗∈[𝑛]

since 𝑅𝑖𝑅(cid:62)

𝑖 = Id. By scalar decoupling (see Vershynin, Lemma 5.60 [Ver12]), for a uniformly

45

random subset 𝑇 ⊆ [𝑛],

(cid:213)

𝑖≠𝑗∈[𝑛]

𝑖 𝑅(cid:62)
𝑥(cid:62)

𝑖 𝑅 𝑗 𝑥 𝑗 = 4 𝔼
𝑇

(cid:213)

𝑖 𝑅(cid:62)
𝑥(cid:62)

𝑖 𝑅 𝑗 𝑥 𝑗 .

𝑖∈𝑇,𝑗∈[𝑛]\𝑇

So,

𝔼
𝑅

(cid:107)𝑅(cid:62)𝑅 − Id(cid:107) = 𝔼
𝑅

| sup
(cid:107)𝑥(cid:107)=1

(cid:107)𝑅𝑥(cid:107)2 − 1|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:213)

𝑖 𝑅(cid:62)
𝑥(cid:62)

𝑖 𝑅 𝑗 𝑥 𝑗

𝑖∈𝑇,𝑗∈[𝑛]\𝑇

(cid:213)

𝑖 𝑅(cid:62)
𝑥(cid:62)

𝑖 𝑅 𝑗 𝑥 𝑗

𝑖∈𝑇,𝑗∈[𝑛]\𝑇

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 𝔼
𝑅

(cid:54) 4 𝔼
𝑅,𝑇

= 4 𝔼
𝑅,𝑇

4 𝔼
𝑇

sup
(cid:107)𝑥(cid:107)=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:107)𝑅(cid:62)

sup
(cid:107)𝑥(cid:107)=1

𝑇 𝑅[𝑛]\𝑇 (cid:107)

where the inequality above follows by Jensen’s inequality.

(cid:3)

8.7 Putting It Together

We are ready to prove Lemma C.1.

Proof of Lemma C.1. We will show that 𝑅(cid:62)𝑅 ∈ ℝ𝑛(𝑑−1)×𝑛(𝑑−1) is close to Θ(𝑑2) · Id. The
(𝑖, 𝑗)-th block of 𝑅(cid:62)𝑅 is given by 𝑅(cid:62)
𝑖
Using Fact 8.18 and the bounds 𝑑1 = Θ(𝑑), 𝑑2 = Θ(1), we obtain,

𝑅 𝑗. Let us ﬁrst consider the diagonal blocks, 𝑅(cid:62)
𝑖

𝑅𝑖.

𝑅𝑖𝑅(cid:62)

𝑖 = Θ(𝑑) · Id .

Next, we bound the norm of the oﬀ-diagonal part of the matrix. Let ℝdiag be the matrix
equal to 𝑅(cid:62) only on diagonal blocks and zero elsewhere. We will use matrix decoupling,
Fact 8.27, to bound 𝔼 (cid:107)𝑅(cid:62)𝑅 − ℝdiag(cid:107). We ﬁnd that for a uniformly random subset 𝑇 ⊆ [𝑛],

𝔼
𝑅

(cid:107)𝑅(cid:62)𝑅 − 𝑅diag(cid:107) (cid:54) 4 𝔼
𝑅,𝑇

(cid:107)𝑅(cid:62)

𝑇 𝑅𝑇 (cid:107)

Now ﬁx 𝑇 ⊆ [𝑛] and for 𝑖 ∈ 𝑇 ﬁx unit vectors 𝑎𝑖 and orthonormal bases 𝐵𝑖 to obtain the
𝑅𝑇 as a sum of independent matrices, one for each

𝑅𝑖 for 𝑖 ∈ 𝑇. We regard the matrix 𝑅(cid:62)
𝑇
𝑖 ∈ 𝑇. The 𝑖-th such matrix 𝑉𝑖 has |𝑇 | blocks; the 𝑗-th block is 𝑅(cid:62)
𝑗

𝑅𝑖.

First we note that (cid:107)𝑉𝑖 (cid:107) (cid:54) 𝑂(𝑑) · (cid:107)𝑅𝑇 (cid:107) with probability one over 𝑅𝑖, since (cid:107)𝑅𝑖 (cid:107) (cid:54) 𝑂(𝑑)

with probability 1, by Fact 8.10.

46

Next, we note the bounds on the variance of 𝑅(cid:62)
𝑇

We conclude that by Matrix Bernstein,

𝑅𝑇 aﬀorded by Fact 8.25 and Fact 8.26.

(cid:107)𝑅(cid:62)

𝑇 𝑅𝑇 (cid:107) (cid:54) log 𝑑 · 𝑂(max((cid:112)|𝑇 | · (cid:107)𝑅𝑇 (cid:107), (cid:112)|𝑇 | · 𝑑, 𝑑3/2(cid:107) (cid:213)

𝑎𝑖 𝑎(cid:62)

𝑖 (cid:107)1/2, 𝑑) .

𝑖∈𝑇

𝔼
𝑅𝑇

So,

𝔼
𝑅

(cid:107)𝑅(cid:62)𝑅 − 𝑅diag(cid:107) (cid:54) 𝑂(log 𝑑) · (cid:169)
𝔼
(cid:173)
𝑇,𝑅
(cid:171)

|𝑇 |1/2 · (cid:107)𝑅𝑇 (cid:107) + 𝑑 𝔼

𝑇,𝑅

(cid:112)|𝑇 | + 𝑑3/2 𝔼 (cid:107) (cid:213)

𝑖∈𝑇

𝑎𝑖 𝑎(cid:62)

𝑖 (cid:107)1/2 + 𝑑(cid:170)
(cid:174)
(cid:172)

Using Fact 8.23, we have 𝔼 |𝑇 |1/2(cid:107)𝑅𝑇 (cid:107) (cid:54)
concentration, 𝔼 (cid:107) (cid:205)

𝑖∈𝑇 (cid:107) (cid:54) log 𝑑 · 𝑂(max(1, 𝑛/𝑑)). So putting it together,

√

𝑛 · 𝑂(log 𝑑 · max(𝑑,

√

𝑛)). By standard matrix

(cid:107)𝑅(cid:62)𝑅 − 𝑅diag(cid:107) (cid:54) 𝑂(log 𝑑)2 · max(𝑑

𝔼
𝑅

√

𝑛, 𝑛, 𝑑3/2) .

(cid:3)

8.8 Omitted Proofs

We turn now to the proof of Fact 8.4. The strategy is much the same as the proof
of Lemma C.1, which is in turn an adaptation of an argument due to Vershynin for
concentration of matrices with independent columns [Ver12].

We will use the following simple matrix decoupling inequality, with 𝐵 being the random

matrix having columns 𝑎⊗3

𝑖

.

Lemma 8.28 (Matrix decoupling, Lemma 5.63 in [Ver12]). Let 𝐵 be an 𝑁 × 𝑛 random matrix
whose columns 𝐵𝑗 satisfy (cid:107)𝐵𝑗 (cid:107) = 1. For any 𝑇 ⊆ [𝑛], let 𝐵𝑇 be the restriction of 𝐵 to the columns
𝑇. Let 𝑇 be a uniformly random set of columns. Then

𝔼 (cid:107)𝐵(cid:62)𝐵 − Id(cid:107) (cid:54) 4 𝔼
𝑇

𝔼
𝐵

(cid:107)(𝐵𝑇)(cid:62)𝐵[𝑛]\𝑇 (cid:107) .

Proof of Fact 8.4. Let 𝐵 have columns 𝑎⊗3
with the goal of applying Lemma 8.28.

𝑖

. Fix 𝑇 ⊆ [𝑛]. We will bound 𝔼 (cid:107)(𝐵𝑇)(cid:62)𝐵[𝑛]\𝑇 (cid:107),

For 𝑖 ∈ 𝑇, the 𝑖-th row of 𝐵(cid:62)
𝑇

𝐵[𝑛]\𝑇 has entries (cid:104)𝑎 𝑗 , 𝑎𝑖(cid:105)3 for 𝑗 ∉ 𝑇. Let us temporarily ﬁx

𝑎𝑖 for 𝑖 ∉ 𝑇; then these rows become independent due to independence of {𝑎 𝑗 } 𝑗∈𝑇.

We think of the matrix 𝐵(cid:62)
𝑇

only the 𝑖-th row of the 𝑖-th matrix 𝑀𝑖 is nonzero, and it consists of entries 𝑀𝑖𝑗 = (𝐵(cid:62)
𝑇
We are going to apply the matrix Bernstein inequality to the sum 𝐵(cid:62)
𝑇

𝐵[𝑛]\𝑇 as consisting of a sum of |𝑇 | independent matrices where
𝐵[𝑛]\𝑇)𝑖𝑗.

𝐵[𝑛]\𝑇 = (cid:205)𝑖∈𝑇 𝑀𝑖.

47

To do so, we need to compute the variance of the sum: we need to bound

(cid:32)

𝜎2 = max

𝔼

𝑀𝑖 𝑀(cid:62)

𝑖 , 𝔼

(cid:213)

𝑖∈𝑇

(cid:33)

𝑀(cid:62)

𝑖 𝑀𝑖

.

(cid:213)

𝑖∈𝑇

(Here the expectation is over 𝑎𝑖 for 𝑖 ∈ 𝑇; we are conditioning on 𝑎𝑖 for 𝑖 ∉ 𝑇.) For the
former, consider that 𝑀𝑖 𝑀(cid:62)
𝑖

has just one nonzero entry,

(cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)6 (cid:54) 𝑂(|[𝑛] \ 𝑇 |/𝑑3) .

𝔼(𝑀𝑖 𝑀(cid:62)

𝑖 )𝑖,𝑖 = 𝔼

(cid:213)

𝑖∉𝑇

(cid:22) 𝑂(|[𝑛] \ 𝑇 |/𝑑3).

Hence 𝔼 (cid:205)𝑖∈𝑇 𝑀𝑖 𝑀(cid:62)
𝑖

Next we bound 𝔼 (cid:205)𝑖∈𝑇 𝑀(cid:62)
𝑖

and 𝑎 a random unit vector. Then 𝔼 (cid:205)𝑖∈𝑇 𝑀(cid:62)
𝑖

𝑀𝑖 = |𝑇 | · 𝔼 𝑟𝑟(cid:62). We may compute that

𝑀𝑖. Let 𝑟 be a random vector with entries (cid:104)𝑎, 𝑎𝑖(cid:105)3 for 𝑖 ∉ 𝑇

(𝔼 𝑟𝑟(cid:62))𝑖𝑗 = 𝔼(cid:104)𝑎, 𝑎𝑖(cid:105)3(cid:104)𝑎, 𝑎 𝑗(cid:105)3 =

𝐶
𝑑3

(cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105) +

1
𝑑3

· 𝑂((cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)3)

where 𝐶 is a universal constant. (This may be seen by comparison to the case that 𝑎 is
replaced by a standard Gaussian and using Wick’s theorem on moments of a multivariate
Gaussian.) Letting 𝐶1 = ((cid:205)𝑖,𝑗∉𝑇 (cid:104)𝑎𝑖 , 𝑎 𝑗(cid:105)6/𝑑6)1/2 and 𝐶2 = (cid:107)𝐴[𝑛]\𝑇 (cid:107) (where 𝐴 has columns
𝑎𝑖), we ﬁnd that 𝜎2 (cid:54) 𝑂(max(|[𝑛] \ 𝑇 |/𝑑3, |𝑇 | · 𝑑−3 · 𝐶2, |𝑇 |𝐶1)).
By applying Matrix Bernstein, for each 𝑡 > 0 we obtain

𝔼

(cid:13)
(cid:13)
(cid:213)
(cid:13)
(cid:13)
(cid:13)

𝑖∈𝑇

𝑀𝑖 · 1((cid:107)𝑀𝑖 (cid:107) (cid:54) 𝑡)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:54) 𝑂(log 𝑑) · (cid:112)max(|[𝑛] \ 𝑇 |/𝑑3, |𝑇 | · 𝑑−3 · 𝐶2, 𝐶1, 𝑡2)

where again the expectation is over only 𝑎𝑖 for 𝑖 ∈ 𝑇. Choosing 𝑡 = ˜𝑂((cid:112)𝑛/𝑑3), by standard
scalar concentration we obtain that ℙ((cid:107)𝑀𝑖 (cid:107) > 𝑡) (cid:54) (𝑑𝑛)−𝜔(1). Hence by Cauchy-Schwarz
we ﬁnd 𝔼 (cid:107) (cid:205)𝑖∈𝑇 𝑀𝑖(1 − 1((cid:107)𝑀𝑖 (cid:107) (cid:54) 𝑡))(cid:107) (cid:54) (𝑑𝑛)−𝜔(1), so all in all,

𝔼

(cid:13)
(cid:13)
(cid:213)
(cid:13)
(cid:13)
(cid:13)

𝑖∈𝑇

𝑀𝑖

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:54) (log 𝑛𝑑)𝑂(1) · (cid:112)max(|[𝑛] \ 𝑇 |/𝑑3, |𝑇 | · 𝑑−3 · 𝐶2, |𝑇 |𝐶1, 𝑛/𝑑3) .

Finally, we have to bound

(cid:112)max(|[𝑛] \ 𝑇 |/𝑑3, |𝑇 | · 𝑑−3 · 𝐶2, |𝑇 |𝐶1, 𝑛/𝑑3)

𝔼
𝑇

𝔼
𝑎𝑖 ,𝑖∉𝑇

which is an upper bound on 𝔼 (cid:107)𝐵(cid:62)
𝑇

𝐵[𝑛]\𝑇 (cid:107). By Cauchy-Schwarz, we may upper bound this

48

by

(𝔼 |[𝑛] \ 𝑇 |/𝑑3)1/2 + (𝔼 |𝑇 | · 𝑑−3 · 𝐶2)1/2 + (𝔼 |𝑇 |𝐶1)1/2 + (𝑛/𝑑3)1/2 .

By standard matrix concentration, 𝔼 𝐶2 (cid:54) 𝑂(1 + 𝑛/𝑑). Clearly 𝔼 |[𝑛] \ 𝑇 | and 𝔼 |𝑇 | (cid:54) 𝑂(𝑛).
Finally, by straightforward computation, 𝔼 𝐶1 (cid:54) 𝑛2/𝑑4.5.

All together, applying Lemma 8.28, we have obtained 𝔼 (cid:107)𝐵(cid:62)𝐵 − Id(cid:107) (cid:54) ˜𝑂(𝑛/𝑑2) for
(cid:3)

𝑛 (cid:29) 𝑑. Since 𝐵(cid:62)𝐵 has the same eigenvalues as 𝐵𝐵(cid:62) = 𝑈, we are done.

Acknowledgements

We thank David Steurer for many helpful conversations regarding the technical content
and presentation of this work.

References

[AABB+07] Evrim Acar, Canan Aykut-Bingol, Haluk Bingol, Rasmus Bro, and Bülent
Yener, Multiway analysis of epilepsy tensors, Bioinformatics 23 (2007), no. 13,
i10–i18. 1

[AGH+14] Animashree Anandkumar, Rong Ge, Daniel J. Hsu, Sham M. Kakade, and
Matus Telgarsky, Tensor decompositions for learning latent variable models, Journal
of Machine Learning Research 15 (2014), no. 1, 2773–2832. 1, 6, 7

[AGHK13] Animashree Anandkumar, Rong Ge, Daniel J. Hsu, and Sham Kakade, A
tensor spectral approach to learning mixed membership community models, COLT,
JMLR Workshop and Conference Proceedings, vol. 30, JMLR.org, 2013,
pp. 867–881. 3

[AGJ14a]

[AGJ14b]

Anima Anandkumar, Rong Ge, and Majid Janzamin, Analyzing tensor power
method dynamics: Applications to learning overcomplete latent variable models,
CoRR abs/1411.1488 (2014). 2

Animashree Anandkumar, Rong Ge, and Majid Janzamin, Guaranteed
non-orthogonal tensor decomposition via alternating rank-1 updates, CoRR
abs/1402.5180 (2014). 7

[AGJ17]

, Analyzing tensor power method dynamics in overcomplete regime, Journal

of Machine Learning Research 18 (2017), 22:1–22:40. 6, 7, 54, 55

49

[AZL16]

Zeyuan Allen-Zhu and Yuanzhi Li, Lazysvd: Even faster svd decomposition yet
without agonizing pain, Advances in Neural Information Processing Systems,
2016, pp. 974–982. 15, 16, 53

[BCMV14] Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vĳayaragha-
van, Smoothed analysis of tensor decompositions, STOC, ACM, 2014, pp. 594–603.
5

[BKS15]

[BS05]

[DK70]

[DLCC07]

Boaz Barak, Jonathan A. Kelner, and David Steurer, Dictionary learning and
tensor decomposition via the sum-of-squares method, STOC, ACM, 2015, pp. 143–
151. 3, 7

C.F. Beckmann and S.M. Smith, Tensorial extensions of independent component
analysis for multisubject fmri analysis, NeuroImage 25 (2005), no. 1, 294 – 311. 1

Chandler Davis and W. M. Kahan, The rotation of eigenvectors by a perturbation.
III, SIAM J. Numer. Anal. 7 (1970), 1–46. MR 0264450 15

Lieven De Lathauwer, Josphine Castaing, and Jean-Franois Cardoso, Fourth-
order cumulant-based blind identiﬁcation of underdetermined mixtures, IEEE
Transactions on Signal Processing 55 (2007), no. 6, 2965–2973. 3

[DLDMV96] Lieven De Lathauwer, Bart De Moor, and Joos Vandewalle, Blind source
separation by simultaneous third-order tensor diagonalization, European Signal
Processing Conference, 1996. EUSIPCO 1996. 8th, IEEE, 1996, pp. 1–4. 2

[Ela10]

[GHJY15]

Michael Elad, Sparse and redundant representations: From theory to applica-
tions in signal and image processing, 1st ed., Springer Publishing Company,
Incorporated, 2010. 3

Rong Ge, Furong Huang, Chi Jin, and Yang Yuan, Escaping from saddle points
- online stochastic gradient for tensor decomposition, Proceedings of The 28th
Conference on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015
(Peter Grünwald, Elad Hazan, and Satyen Kale, eds.), JMLR Workshop and
Conference Proceedings, vol. 40, JMLR.org, 2015, pp. 797–842. 7

[GHK15]

Rong Ge, Qingqing Huang, and Sham M. Kakade, Learning mixtures of
Gaussians in high dimensions [extended abstract], STOC’15—Proceedings of the
2015 ACM Symposium on Theory of Computing, ACM, New York, 2015,
pp. 761–770. MR 3388256 3

[GM15]

Rong Ge and Tengyu Ma, Decomposing overcomplete 3rd order tensors using sum-
of-squares algorithms, APPROX-RANDOM, LIPIcs, vol. 40, Schloss Dagstuhl -
Leibniz-Zentrum fuer Informatik, 2015, pp. 829–849. 7, 8

50

[GM17]

[Har70]

[HL13]

[HLMK]

[HS17]

[HSS15]

[HSSS16]

[KB09]

[LCC07]

[LS00]

[MM18]

Rong Ge and Tengyu Ma, On the optimization landscape of tensor decompositions,
Advances in Neural Information Processing Systems, 2017, pp. 3653–3663. 6,
7

Richard A Harshman, Foundations of the parafac procedure: Models and conditions
for an" explanatory" multi-modal factor analysis. 2, 9

Christopher J. Hillar and Lek-Heng Lim, Most tensor problems are np-hard, J.
ACM 60 (2013), no. 6, 45:1–45:39. 2

Wu Hai-Long, Shibukawa Masami, and Oguma Koichi, An alternating trilinear
decomposition algorithm with application to calibration of HPLC-DAD for simulta-
neous determination of overlapped chlorinated aromatic hydrocarbons, Journal of
Chemometrics 12, no. 1, 1–26. 1

Samuel B Hopkins and David Steurer, Eﬃcient bayesian estimation from few
samples: community detection and related problems, Foundations of Computer
Science (FOCS), 2017 IEEE 58th Annual Symposium on, IEEE, 2017, pp. 379–
390. 3

Samuel B. Hopkins, Jonathan Shi, and David Steurer, Tensor principal compo-
nent analysis via sum-of-square proofs, COLT, JMLR Workshop and Conference
Proceedings, vol. 40, JMLR.org, 2015, pp. 956–1006. 1

Samuel B. Hopkins, Tselil Schramm, Jonathan Shi, and David Steurer, Fast
spectral algorithms from sum-of-squares proofs: tensor decomposition and planted
sparse vectors, STOC, ACM, 2016, pp. 178–191. 2, 8, 35, 40

Tamara G. Kolda and Brett W. Bader, Tensor decompositions and applications,
SIAM Review 51 (2009), no. 3, 455–500. 6

Lieven De Lathauwer, Joséphine Castaing, and Jean-François Cardoso, Fourth-
order cumulant-based blind identiﬁcation of underdetermined mixtures, IEEE Trans.
Signal Processing 55 (2007), no. 6-2, 2965–2973. 6, 8, 9

Michael S. Lewicki and Terrence J. Sejnowski, Learning overcomplete representa-
tions, Neural Comput. 12 (2000), no. 2, 337–365. 3

Marco Mondelli and Andrea Montanari, On the connection between learning
two-layers neural networks and tensor decomposition, CoRR abs/1802.07301 (2018).
3

51

[MSS16]

Tengyu Ma, Jonathan Shi, and David Steurer, Polynomial-time tensor decompo-
sitions with sum-of-squares, FOCS, IEEE Computer Society, 2016, pp. 438–446.
2, 3, 4, 5, 6, 7, 8, 10, 12, 55

[RM14]

[SS17]

[SV17]

[Ver12]

[Wey12]

Emile Richard and Andrea Montanari, A statistical model for tensor PCA, NIPS,
2014, pp. 2897–2905. 1

Tselil Schramm and David Steurer, Fast and robust tensor decomposition with
applications to dictionary learning, COLT, Proceedings of Machine Learning
Research, vol. 65, PMLR, 2017, pp. 1760–1793. 3, 5, 6, 8, 10, 12, 24, 25, 26, 30

Vatsal Sharan and Gregory Valiant, Orthogonalized ALS: A theoretically prin-
cipled tensor decomposition algorithm for practical use, ICML, Proceedings of
Machine Learning Research, vol. 70, PMLR, 2017, pp. 3095–3104. 7

Roman Vershynin, Introduction to the non-asymptotic analysis of random matrices,
Compressed sensing, Cambridge Univ. Press, Cambridge, 2012, pp. 210–268.
MR 2963170 39, 45, 47

Hermann Weyl, Das asymptotische verteilungsgesetz der eigenwerte linearer
partieller diﬀerentialgleichungen (mit einer anwendung auf die theorie der hohlraum-
strahlung), Mathematische Annalen 71 (1912), no. 4, 441–479. 15

A Tools for analysis and implementation

Lemma (Restatement of Lemma 4.3). For a tensor T ∈ (ℝ[𝑑]2)⊗3, suppose that the matrix
𝑇 ∈ ℝ[𝑑]3×[𝑑]3 given by 𝑇(𝑖,𝑖(cid:48),𝑗),(𝑘,𝑘(cid:48),𝑗(cid:48)) = T(𝑖,𝑖(cid:48)),(𝑗,𝑗(cid:48)),(𝑘,𝑘(cid:48)) has a rank-𝑛 decomposition 𝑇 = 𝑈𝑉T with
𝑈 , 𝑉 ∈ ℝ𝑑3×𝑛 and 𝑛 (cid:54) 𝑑2. Such a rank decomposition provides an implicit representation of the
tensor T. This implicit representation supports:
Tensor contraction: For vectors 𝑥, 𝑦 ∈ ℝ[𝑑]2, the computation of (𝑥T ⊗ 𝑦T ⊗Id)T or (𝑥T ⊗Id⊗ 𝑦T)T

or (Id ⊗ 𝑥T ⊗ 𝑦T)T in time 𝑂(𝑛𝑑3) to obtain an output vector in ℝ𝑑2.

Spectral truncation: For 𝑅 ∈ ℝ𝑑2×𝑑4 equal to one of the two matrix reshapings 𝑇{1,2}{3} or
(cid:54)1, deﬁned as T after all larger-than-1 singular
𝑇{2,3}{1} of T, an approximation to the tensor T
values in its reshaping 𝑅 are truncated down to 1. Speciﬁcally, letting 𝜌𝑘 be the 𝑘th largest
singular value of 𝑅 for 𝑘 (cid:54) 𝑂(𝑛), this returns an implicit representation of a tensor T(cid:48) such
(cid:54)1(cid:107)𝐹 (cid:54) (1 + 𝛿)𝜌𝑘 (cid:107)T(cid:107)𝐹 and the reshaping of T(cid:48) corresponding to 𝑅 has largest
that (cid:107)T(cid:48) − T
singular value no more than 1 + (1 + 𝛿)𝜌𝑘. The representation of T(cid:48) also supports the tensor
contraction, spectral truncation, and implicit matrix multiplication operations, with no more
than a constant factor increase in runtime. This takes time ˜𝑂(𝑛2𝑑3 + 𝑘(𝑛𝑑3 + 𝑘𝑑2)𝛿−1/2).

52

Implicit matrix multiplication: For a matrix 𝑅 ∈ ℝ[𝑑]2×[𝑑]2 with rank at most 𝑂(𝑛), an implicit
representation of the tensor (𝑅T ⊗ Id ⊗ Id)T or (Id ⊗ Id ⊗ 𝑅T)T, in time 𝑂(𝑛𝑑4). This output
also supports the tensor contraction, spectral truncation, and implicit matrix multiplication
operations, with no more than a constant factor increase in runtime. Multiplication into the
second mode (Id ⊗ 𝑅T ⊗ Id)T may also be implicitly represented, but without support for the
spectral truncation operation.

Proof.
Tensor contraction We start with multiplication of two vectors 𝑥, 𝑦 ∈ ℝ𝑑2 into two of the
modes of T. Without loss of generality (by interchange of 𝑈 and 𝑉), there are two cases:
we want either to compute the vector ﬂattening of (𝑥 ⊗ Id𝑑)T𝑈𝑉T(𝑦 ⊗ Id𝑑), or, expressing
𝑥 = (cid:205)𝑑
𝑖=0 𝑟𝑖 ⊗ 𝑠𝑖, we want (cid:205)𝑖 (Id𝑑 ⊗ Id𝑑 ⊗ 𝑟𝑖)T𝑈𝑉T(𝑦 ⊗ 𝑠𝑖). For both these cases, we ﬁrst
compute 𝑉T(𝑦 ⊗ Id𝑑).

We compute 𝑉T(𝑦 ⊗ Id𝑑) as [𝑉T(𝑦 ⊗ Id𝑑)]·;𝑖 = 𝑉T

·;(·,·,𝑖)𝑦. This is a concatenation of 𝑑
diﬀerent matrix-vector multiplications using 𝑛 × 𝑑2 matrices, and so it takes 𝑂(𝑛𝑑3) time.
Then to ﬁnd (𝑥T ⊗ Id𝑑)𝑈𝑉T(𝑦 ⊗ Id𝑑), we simply repeat the above procedure to ﬁnd

(𝑥T ⊗ Id𝑑)𝑈 and then multiply the 𝑑 × 𝑛 and 𝑛 × 𝑑 matrices together in 𝑂(𝑛𝑑2) time.
To ﬁnd (cid:205)𝑖 (Id𝑑 ⊗ Id𝑑 ⊗ 𝑟𝑖)T𝑈𝑉T(𝑦 ⊗𝑠𝑖) after ﬁnding the rank decomposition 𝑥 = (cid:205)𝑑
𝑖=0 𝑟𝑖 ⊗
𝑠𝑖 which takes 𝑂(𝑑3) time by SVD, we multiply each 𝑠𝑖 into our computed value of 𝑉T(𝑦 ⊗Id𝑑)
to obtain 𝑑 diﬀerent 𝑛-dimensional vectors 𝑡𝑖 = 𝑉T(𝑦 ⊗ 𝑠𝑖). Since there are 𝑑 of these vectors
and each is a matrix-vector multiplication with an 𝑛 × 𝑑 matrix, this takes 𝑂(𝑛𝑑2) time.
Then (cid:205)𝑖 (Id𝑑 ⊗ Id𝑑 ⊗ 𝑟𝑖)T𝑈𝑡𝑖 can be reshaped as a multiplication of a 𝑑2 × 𝑛𝑑 reshaping of 𝑈
with the vector (cid:205)𝑖 𝑡𝑖 ⊗ 𝑟𝑖. It takes 𝑂(𝑛𝑑3) time to perform the matrix-vector multiplication,
and 𝑂(𝑛𝑑2) time to sum up (cid:205)𝑖 𝑡𝑖 ⊗ 𝑟𝑖.

Spectral truncation Next, we truncate the larger-than-1 singular values of the ({1}, {2, 3})
and ({3}, {1, 2}) matrix reshapings 𝑅 ∈ ℝ𝑑2×𝑑4 of T. Without loss of generality, suppose we
are in the ({3}, {1, 2}) case. In this case, we would like to ﬁnd the right-singular vectors and
singular values of the operator that takes 𝑦 ∈ ℝ𝑑2 to the vector ﬂattening of the 𝑑 × 𝑑3 matrix
𝑈𝑉T(𝑦 ⊗ Id𝑑). Letting 𝑍 be the 𝑛𝑑 × 𝑑2 reshaping of 𝑉, this is the same as (𝑈 ⊗ Id𝑑)𝑍𝑦,
which shares its right-singular vectors with 𝑀 := 𝑍T(𝑈T𝑈 ⊗ Id𝑑)𝑍.

We claim that matrix-vector multiplication by 𝑀 can be implemented in 𝑂(𝑛𝑑3) time,
with 𝑂(𝑛2𝑑3) preprocessing time for computing the product 𝑈T𝑈. The matrix-vector
multiplications by 𝑍 and 𝑍T take time 𝑂(𝑛𝑑3), and then multiplying 𝑍𝑦 by 𝑈T𝑈 ⊗ Id𝑑 is
reshaping-equivalent to multiplying 𝑈T𝑈 into the 𝑛 × 𝑑 matrix reshaping of 𝑍𝑦, which
takes 𝑂(𝑛2𝑑) time with the precomputed 𝑛 × 𝑛 matrix 𝑈T𝑈. Therefore, LazySVD [AZL16,
Corollary 4.4] takes time ˜𝑂(𝑛2𝑑3𝛿−1/2) to yield a rank-𝑘 eigendecomposition 𝑃Λ𝑃T such
that (cid:107)𝑀1/2 − 𝑃Λ1/2𝑃T(cid:107) (cid:54) (1 + 𝛿)𝜌𝑘.

53

To obtain the output T(cid:48) of this procedure, let (𝑃Λ1/2𝑃T − Id)>0 be 𝑃Λ1/2𝑃T − Id with all
of its nonpositive eigenvalues removed: this may be implemented by removing nonpositive
entries from Λ1/2 − Id. Then implicitly multiply (Id − (𝑃Λ1/2𝑃T − Id)>0) into the third
mode of T (although this matrix has rank larger than 𝑛, we may implement it by implicitly
subtracting (Id ⊗ Id ⊗ (𝑃Λ1/2𝑃T −Id)>0)T from T). We are trying to approximate multiplying
(Id − (𝑀1/2 − Id)>0) into the third mode of T, so let Δ = (𝑀1/2 − Id)>0 − (𝑃Λ1/2𝑃T − Id)>0 be
the diﬀerence. Then (cid:107)Δ(cid:107) (cid:54) (cid:107)𝑀1/2 −𝑃Λ1/2𝑃T(cid:107) (cid:54) (1+ 𝛿)𝜌𝑘, so that we suﬀer an additive error
of at most (1 + 𝛿)𝜌𝑘 in spectral norm. And the ﬁnal error in the low-rank representation
is (Δ ⊗ Id)𝑈𝑉T. Since (cid:107)Δ ⊗ Id(cid:107) (cid:54) (1 + 𝛿)𝜌𝑘 and 𝑈𝑉T has Frobenius norm (cid:107)T(cid:107)𝐹, we ﬁnd a
ﬁnal error of (1 + 𝛿)𝜌𝑘 (cid:107)T(cid:107)𝐹 in Frobenius norm.

Implicit matrix multiplication Finally, to implicitly multiply a 𝑑2 × 𝑑2 rank-𝑛 matrix 𝑅
into a mode of T, simply store the singular value decomposition 𝑅 = 𝑃Σ𝑄T. Whenever a
vector needs to be multiplied into that mode in the future, multiply that vector by 𝑅 before
carrying out the implicit tensor operation as previously speciﬁed, and if a vector needs to
be output from that mode, multiply it by 𝑅T before outputting. This incurs a time cost of
𝑂(𝑛𝑑2) per operation.

A special case arises in the spectral truncation operation, where we do not allow implicit
multiplication to have been done in the second mode. Suppose then without loss of
generality that 𝑅 was multiplied into the ﬁrst mode of T and we truncate the ({3}, {1, 2})
matrix reshaping. Then we will have to compute 𝑈T(𝑅𝑅T ⊗ Id𝑑)𝑈 instead of 𝑈T𝑈 in the
preprocessing step. This can be done by multiplying 𝑅T = 𝑄Σ𝑃T with the 𝑑2 × 𝑛𝑑 reshaping
of 𝑈, which takes 𝑂(𝑛2𝑑3) time per future spectral truncation operation.
(cid:3)

B Notes on Table 1

We record a few notes on parameter regimes used to compare various algorithms for tensor
decomposition in Table 1.

• Robust algorithms with algebraic assumptions often require (cid:107)𝐸(cid:107) (cid:54) 𝜎(𝑎1, . . . , 𝑎𝑛),
where 𝜎(𝑎1, . . . , 𝑎𝑛) is some measure of well-conditioned-ness of 𝑎1, . . . , 𝑎𝑛, the details
of which may vary from algorithm to algorithm. In this table we report results
for the setting that 𝜎(𝑎1, . . . , 𝑎𝑛) (cid:62) Ω(1); such values of 𝜎 (for all the notions of
well-conditioned-ness represented) are achieved by random 𝑎1, . . . , 𝑎𝑛.

• The algorithm of [AGJ17] is phrased for 3-tensors rathern than 4-tensors; this is
the origin of the rank bound 𝑛 (cid:54) 𝑑1.5 rather than 𝑑 (cid:54) 𝑛2 achieved by algorithms
for 4-tensors. In general for 𝑘-tensors one expects eﬃcient algorithms to tolerate
overcompleteness 𝑛 (cid:54) 𝑑𝑘/2 (despite tensor rank factorizations remaining unique for

54

much larger 𝑛), so the overcompleteness guarantee of [AGJ17] is comparable to the
other algorithms.

• We have estimated the running time of the SoS algorithm of [MSS16] by assuming that
the semideﬁnite programs involved are solved using standard black-box techniques
(e.g the ellipsiod method).

C Simulations for condition number of random tensors

In this section we report on computer simulations which strongly suggest that if the
components 𝑎1, . . . , 𝑎𝑛 are 𝑛 (cid:28) 𝑑2 random unit vectors from a variety of ensembles, then
with high probability 𝜅(𝑎1, . . . , 𝑎𝑛) (cid:62) Ω(1). The ensembes include:

1. Spherical measure: 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑 are i.i.d. random unit vectors (see Fig. 1).

2. Sparse: 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑 are sampled i.i.d. by choosing 1

4 𝑑 coordinates in [𝑑] uniformly
at random, sampling each of those coordinates from 𝒩(0, 1), and setting the rest to 0
(see Fig. 2).

3. Hypercube: 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑 are i.i.d. samples from {0, 1}𝑑 (see Fig. 3).

4. Spiked covariance: 𝑎1, . . . , 𝑎𝑛 ∈ ℝ𝑑 are sampled from 𝒩(0, Id + 𝜆 · 𝑢𝑢(cid:62)) for a random
unit vector 𝑢 and 𝜆 > 0. We note that in this case, though the covariance matrix
of 𝑎1, . . . , 𝑎𝑛 has condition number 𝑂( 1
𝜆 ), our experimental results support the
hypothesis that 𝜅(𝑎1, . . . , 𝑎𝑛) = Ω(1) for 𝜆 as large as 𝜆 = 1

2 𝑑 (see Fig. 4).

These ensembles are designed to capture a number of characteristics of real data which we
would like the condition number to be robust to: sparsity, discrete values, and correlations
(of relatively extreme magnitude).

In each of these cases, we computed 𝜅 for several values of 𝑛, 𝑑 on with 𝑎1, . . . , 𝑎𝑛 taken
to be i.i.d. uniformly random unit vectors. Our results are consistent with the hypothesis
that (with high probability) 𝜅(𝑎1, . . . , 𝑎𝑛) (cid:62) 𝑐 − ˜𝑂(𝑛/𝑑2) for some absolute constant 𝑐 ≈ 1
2.
We expect the values of 𝑑, 𝑛 employed here – 𝑑 ≈ 10, 𝑛 ≈ 100, so that 𝜅 is the condition
number of a certain random matrix of dimensions about 103 × 103 – to be predictive of
the asymptotic behavior of 𝜅(𝑎1, . . . , 𝑎𝑛), because the spectra of random matrices display
strong concentration even in relatively small dimensions.

We also note that the hypothesis that 𝜅 > 𝑐 − ˜𝑂(𝑛/𝑑2) is well supported by the fact
that relatively standard techniques from random matrix theory yield the same bound for
a closely related random matrix to 𝐾(𝑎1, . . . , 𝑎𝑛) from Deﬁnition 1.3. In particular, the
following may be proved by a long but standard calculation, using Matrix Bernstein and
decoupling inequalities:

55

Figure 1: Condition number as a function of dimension 𝑑 and lifted overcompleteness
𝑛/𝑑2 for vectors sampled from the spherical measure.

Figure 2: Condition number as a function of dimension 𝑑 and lifted overcompleteness
𝑛/𝑑2 for random sparse vectors.

56

Figure 3: Condition number as a function of dimension 𝑑 and lifted overcompleteness
𝑛/𝑑2 for vectors sampled from the Boolean hypercube.

Figure 4: Condition number as a function of dimension 𝑑 and lifted overcompleteness
𝑛/𝑑2 for vectors sampled from 𝒩(0, Id + 1

2 𝑑 · 𝑢𝑢(cid:62)) for a random unit vector 𝑢.

57

Lemma C.1 (Condition number of basic swap matrix). Let 𝑎1, . . . , 𝑎𝑛 be independent random
𝑑-dimensional unit vectors. Let 𝐵𝑖 ∈ ℝ(𝑑−1)×𝑑 be a random basis for the orthogonal complement
of 𝑎𝑖 in ℝ𝑑. Let 𝑃 ∈ ℝ𝑑3×𝑑3 be the permutation matrix which swaps second and third modes of
(ℝ𝑑)⊗3. Let

𝐴 = 𝔼
𝑎

(𝑎 ⊗ 𝑎 ⊗ Id)(𝑎 ⊗ 𝑎 ⊗ Id)(cid:62) .

Let 𝑅 ∈ ℝ𝑑3×𝑛(𝑑−1) have 𝑛 blocks of dimensions 𝑑3 × (𝑑 − 1), where the 𝑖-th block is

𝑅𝑖 = 𝐴−1/2(𝑎𝑖 ⊗ 𝑎𝑖 ⊗ 𝐵𝑖) − 𝑃𝐴−1/2(𝑎𝑖 ⊗ 𝑎𝑖 ⊗ 𝐵𝑖)

where we abuse notation and denote the PSD square root of the pseudoinverse of 𝐴 by 𝐴−1/2. Then
there is a function 𝑑(cid:48)(𝑑) = Θ(𝑑2) such that 𝔼 (cid:107)𝑅(cid:62)𝑅 − 𝑑(cid:48)(𝑑) · Id(cid:107) (cid:54) 𝑂(log 𝑑)2 · max(𝑑
𝑛, 𝑛, 𝑑3/2).
In particular, if 𝑑 (cid:28) 𝑛 (cid:28) 𝑑2,

√

𝔼 (cid:107) 1
𝑑(cid:48)(𝑑)

𝑅(cid:62)𝑅 − Id(cid:107) (cid:54) 𝑂(𝑛(log 𝑑)2/𝑑2) .

The matrix 𝑅 from this lemma diﬀers from 𝐾 only in the use of 𝐴−1/2 in place of
1 𝐻1)−1/2, (𝐻(cid:62)
(𝐻(cid:62)
2 𝐻2)−1/2. While we expect 𝐴−1/2 (a non-random matrix) to be close to both
(𝐻(cid:62)
1 𝐻1)−1/2, (𝐻(cid:62)
2 𝐻2)−1/2 (at least in subspaces close to 𝐼𝑚(𝐻1) and 𝐼𝑚(𝐻2), respectively)
establishing this is a challenging task in random matrix theory – in particular, both inverses
of random matrices and spectra of random matrices with dependent entries are notoriously
diﬃcult to analyze. We leave this challenge to future work.

58

