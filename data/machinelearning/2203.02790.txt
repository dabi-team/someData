A Robust Spectral Algorithm for Overcomplete
Tensor Decomposition

Samuel B. Hopkinsâˆ—

Tselil Schrammâ€ 

Jonathan Shiâ€¡

March 8, 2022

Abstract

We give a spectral algorithm for decomposing overcomplete order-4 tensors, so
long as their components satisfy an algebraic non-degeneracy condition that holds for
nearly all (all but an algebraic set of measure 0) tensors over (â„ğ‘‘)âŠ—4 with rank ğ‘› (cid:54) ğ‘‘2.
Our algorithm is robust to adversarial perturbations of bounded spectral norm.

Our algorithm is inspired by one which uses the sum-of-squares semideï¬nite
programming hierarchy (Ma, Shi, and Steurer STOCâ€™16), and we achieve comparable
robustness and overcompleteness guarantees under similar algebraic assumptions.
However, our algorithm avoids semideï¬nite programming and may be implemented
as a series of basic linear-algebraic operations. We consequently obtain a much faster
running time than semideï¬nite programming methods: our algorithm runs in time
Ëœğ‘‚(ğ‘›2ğ‘‘3) (cid:54) Ëœğ‘‚(ğ‘‘7), which is subquadratic in the input size ğ‘‘4 (where we have suppressed
factors related to the condition number of the input tensor).

2
2
0
2

r
a

M
5

]

G
L
.
s
c
[

1
v
0
9
7
2
0
.
3
0
2
2
:
v
i
X
r
a

âˆ—Cornell University. samhop@cs.cornell.edu. Supported by NSF awards 1350196 & 1408673, and a

Microsoft PhD Fellowship.

â€ MIT and Harvard. tselil@mit.edu. Supported by NSF awards CCF 1565264 & CNS 1618026.
â€¡Cornell University. jshi@cs.cornell.edu. Supported by NSF awards 1350196 & 1408673.

 
 
 
 
 
 
Contents

1

Introduction
1.1 Our Results
.
1.2 Related works .

.

.
.

.
.

.
.

.
.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.

2 Overview of algorithm

3 Preliminaries

1
4
6

9

14

4 Tools for analysis and implementation

15
4.1 Robustness and spectral perturbation . . . . . . . . . . . . . . . . . . . . . . 15
4.2 Eï¬ƒcient implementation and runtime analysis . . . . . . . . . . . . . . . . . 15

5 Lifting

5.1 Algebraic identiï¬ability argument
5.2 Robustness arguments .

17
. . . . . . . . . . . . . . . . . . . . . . . . 19
. 21

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Rounding

23
6.1 Recovering candidate whitened and squared components . . . . . . . . . .
. 24
6.2 Extracting components from the whitened squares . . . . . . . . . . . . . . . 26
6.3 Testing candidate components . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
6.4 Putting things together .

7 Combining lift and round for ï¬nal algorithm

31

.

.

.

.

.

.

8 Condition number of random tensors
.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

34
. 39
8.1 Notation .
8.2 Fourth Moment Identities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
8.3 Matrix Product Identities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
8.4 Naive Spectral Norm Estimate . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
8.5 Oï¬€-Diagonal Second Moment Estimates . . . . . . . . . . . . . . . . . . . . . 43
. 45
8.6 Matrix Decoupling .
. 46
.
8.7 Putting It Together
. 47
.
.
8.8 Omitted Proofs

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.
.
.

.

References

A Tools for analysis and implementation

B Notes on Table 1

C Simulations for condition number of random tensors

49

52

54

55

1

Introduction

Tensors are higher-order analogues of matrices: multidimensional arrays of numbers.
They have broad expressive power: tensors may represent higher-order moments of a
probability distribution [AGH+14], they are natural representations of cubic, quartic, and
higher-degree polynomials [RM14, HSS15], and they appear whenever data is multimodal
(e.g. in medical studies, where many factors are measured) [AABB+07, BS05, HLMK]. Due
to these reasons, in recent decades tensors have emerged as fundamental structures in
machine learning and signal processing.

The notion of rank extends from matrices to tensors: a rank-1 tensor in (â„ğ‘‘)âŠ—ğ‘˜ is a tensor
that can be written as a tensor product ğ‘¢(1) âŠ— Â· Â· Â· âŠ— ğ‘¢(ğ‘˜) of vectors ğ‘¢(1), . . . , ğ‘¢(ğ‘˜) âˆˆ â„ğ‘‘. Any
tensor T âˆˆ (â„ğ‘‘)âŠ—ğ‘˜ can be expressed as a sum of rank-1 tensors, and the rank of T is the
minimum number of terms needed in such a sum. As is the case for matrices, we are often
interested in tensors of low rank: low-rank structure in tensors often carries interpretable
meaning about underlying data sets or probability distributions, and the tensors that arise
in many applications are low-rank [AGH+14].

Tensor decomposition is the natural inverse problem in the context of tensor rank: given

a ğ‘‘-dimensional symmetric ğ‘˜-tensor T âˆˆ (â„ğ‘‘)âŠ—ğ‘˜ of the form

T =

(cid:213)

ğ‘–(cid:54)ğ‘›

ğ‘âŠ—ğ‘˜
ğ‘– + E,

for vectors ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘ and an (optional) error tensor E âˆˆ (â„ğ‘‘)âŠ—ğ‘˜, we are asked to
output vectors ğ‘1, . . . , ğ‘ğ‘› as close as possible to ğ‘1, . . . , ğ‘ğ‘› (e.g. minimizing the Euclidean
distance (cid:107)ğ‘ğ‘– âˆ’ ğ‘ğ‘– (cid:107)). The goal is to accomplish this with an algorithm that is as eï¬ƒcient as
possible, under the mildest-possible assumptions on ğ‘˜,ğ‘1, . . . , ğ‘ğ‘›, and E.

While tensor rank decomposition is a generalization of rank decomposition for matrices,
decomposition for tensors of order ğ‘˜ (cid:62) 3 diï¬€ers from the matrix case in several key ways.

1. (Uniqueness) Under mild assumptions on the vectors ğ‘1, . . . , ğ‘ğ‘›, tensor decomposi-
tions are unique (up to permutations of [ğ‘›]), while matrix decompositions are often
unique only up to unitary transformation.

2. (Overcompleteness) Tensor decompositions often remain unique even when the
number of factors ğ‘› is larger than the ambient dimension ğ‘‘ (up to ğ‘› = ğ‘‚(ğ‘‘ğ‘˜âˆ’1)), while
a ğ‘‘ Ã— ğ‘‘ matrix can have only ğ‘‘ eigenvectors or 2ğ‘‘ singular vectors.

These features make tensor decompositions suitable for many applications where matrix
factorizations are insuï¬ƒcient. However, there is another major diï¬€erence:

3. (Computational Intractability) While many matrix decompositions â€” eigendecompo-
sitions, singular value decompositions, ğ¿ğ‘ˆ-factorizations, and so on â€” can be found

1

in polynomial time, tensor decomposition is NP-hard in general [HL13].

In spite of the NP-hardness of general tensor decomposition, many special cases turn
out to admit polynomial-time algorithms. A classical algorithm, often called Jennrichâ€™s
algorithm, recovers the components ğ‘1, . . . , ğ‘ğ‘› from T when they are linearly independent
(which requires ğ‘› (cid:54) ğ‘‘) and E = 0 using simultaneous diagonalization [Har70, DLDMV96].
More sophisticated algorithms improve on Jennrichâ€™s in their tolerance to overcom-
pleteness (and the resulting lack of linear independence) and robustness to nontrivial
error tensors E. The literature now contains a wide variety of techniques for tensor
decomposition: the major players are iterative methods (tensor power iteration, stochastic
gradient descent, and alternating minimization), spectral algorithms, and convex programs.
Convex programs, and in particular the sum-of-squares semideï¬nite programming hierarchy
(SoS), require the mildest assumptions on ğ‘˜, ğ‘1, . . . , ğ‘ğ‘› , E among known polynomial-time
algorithms [MSS16]. In pushing the boundaries of what is known to be achievable in
polynomial time, SoS-based algorithms have been crucial. However, the running times of
these algorithms are large polynomials in the input, making them utterly impractical for
applications.

The main contribution of this work is a tensor decomposition algorithm whose robust-
ness to errors and tolerance for overcompleteness are similar to those of the SoS-based
algorithms, but with subquadratic running time. Other algorithms with comparable running
times either require higher-order tensors,1 are not robust in that they require the error E = 0
or E exponentially small, or require linear independence of the components ğ‘1, . . . , ğ‘ğ‘› and
hence ğ‘› (cid:54) ğ‘‘.2

Our algorithm is comparatively simple, and can be implemented with a small number
of dense matrix and matrix-vector multiplication operations, which are fast not only
asymptotically but also in practice.

Concretely, we study tensor decomposition of overcomplete 4-tensors under algebraic
nondegeneracy conditions on the tensor components ğ‘1, . . . , ğ‘ğ‘›. Algebraic conditions like
ours are the mildest type of assumption on ğ‘1, . . . , ğ‘ğ‘› known to lead to polynomial time
algorithms â€“ our algorithm can decompose all but a measure-zero set of 4-tensors of rank
ğ‘› (cid:28) ğ‘‘2, and in particular we make no assumption that the components ğ‘1, . . . , ğ‘ğ‘› are
random.3

When ğ‘› (cid:28) ğ‘‘2, our algorithm approximately recovers a 1 âˆ’ ğ‘œ(1) fraction of ğ‘1, . . . , ğ‘ğ‘› (up
+ ğ¸, so long as the spectral norm (cid:107)ğ¸(cid:107) is (signiï¬cantly) less

to their signs) from ğ‘‡ = (cid:205)ğ‘–(cid:54)ğ‘› ğ‘âŠ—4

ğ‘–

1Higher-order tensors are costly because they are larger objects, and for learning applications they often

require a polynomial increase in sample complexity.

2There are also existing robust algorithms which tolerate some overcompleteness when ğ‘1, . . . , ğ‘ğ‘› are
assumed to be random; in this paper we study generic ğ‘1, . . . , ğ‘ğ‘›, which is a much more challenging setting
than random ğ‘1, . . . , ğ‘ğ‘› [AGJ14a, HSSS16].

3Although decompositions of 4-th order tensors can remain unique up to ğ‘› â‰ˆ ğ‘‘3, no polynomial-time

algorithms are known which successfully decompose tensors of overcompleteness ğ‘› (cid:29) ğ‘‘2.

2

than the minimum singular value of a certain matrix associated to the {ğ‘ğ‘– }. (In particular,
nonsingularity of this matrix is our nondegeneracy condition on ğ‘1, . . . , ğ‘ğ‘›.) The algorithm
requires time Ëœğ‘‚(ğ‘›2ğ‘‘3) (cid:54) ğ‘‚(ğ‘‘7), which is subquadratic in the input size ğ‘‘4.

Robustness, Overcompleteness, and Applications to Machine Learning Tensor decom-
position is a common primitive in algorithms for statistical inference that leverage the method
of moments to learn parameters of latent variable models. Examples of such algorithms exist
for independent component analysis / blind source separation [DLCC07], dictionary learn-
ing [BKS15, MSS16, SS17], overlapping community detection [AGHK13, HS17], mixtures
of Gaussians [GHK15], and more.

In these applications, we receive samples ğ‘¥ âˆˆ â„ğ‘‘ from a model distribution ğ’Ÿ(ğœŒ)
that is a function of parameters ğœŒ. The goal is to estimate ğœŒ using the samples. The
method-of-moments strategy is to construct the third- or fourth-order moment tensor
ğ”¼ ğ‘¥âŠ—ğ‘˜ (ğ‘˜ = 3, 4) from samples whose expectation ğ”¼ ğ‘¥âŠ—ğ‘˜ = (cid:205)ğ‘–(cid:54)ğ‘› ğ‘âŠ—ğ‘˜
is a low rank tensor
ğ‘–
with components ğ‘1, . . . , ğ‘ğ‘›, from which the model parameters ğœŒ can be deduced.4 Since
ğ”¼ ğ‘¥âŠ—ğ‘˜ is estimated using samples, the tensor decomposition algorithm used to extract
ğ‘1, . . . , ğ‘ğ‘› from ğ”¼ ğ‘¥âŠ—ğ‘˜ must be robust to error from sampling.

The sample complexity of the resulting algorithm depends directly on the magnitude
In addition, the greater general

of errors tolerated by the decomposition algorithm.
error-robustness of our result suggests better tolerance of model misspeciï¬cation error.

Some model classes give rise to overcomplete tensors; roughly speaking, this occurs
when the number of parameters (the size of the description of ğœŒ) far exceeds ğ‘‘2, where
ğ‘‘ is the ambient dimension. Typically, in such cases, ğœŒ consists of a collection of vectors
ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘ with ğ‘› (cid:29) ğ‘‘. Such overcomplete models are widely used; for example,
in the dictionary learning setting, we are given a data set ğ‘† and are asked to ï¬nd a sparse
representation of ğ‘†. This is a powerful preprocessing tool, and the resulting representations
are more robust to perturbations, but assembling a truly sparse, eï¬€ective dictionary often
requires representing ğ‘‘-dimensional data in a basis with ğ‘› (cid:29) ğ‘‘ elements [LS00, Ela10].
Recent works also relate the problem of learning neural networks with good generalization
error to tensor decomposition, showing a connection between overcompleteness and the
width of the network [MM18].5

Using tensor decomposition in such settings requires algorithms with practical running
times, error robustness, and tolerance to overcompleteness. The strongest polynomial-time
guarantees for overcomplete dictionary learning and similar models currently rely on

4Any constant ğ‘˜, rather than just ğ‘˜ = 3, 4, may lead to polynomial-time learning algorithms, but the cost
is typically gigantic polynomial sample complexity and running time, scaling like ğ‘‘ğ‘˜, to estimate and store a
ğ‘˜-th order tensor.

5Strictly speaking, this work shows a reduction from tensor decomposition to learning neural nets, but the

connection between width and overcompleteness is direct regardless.

3

overcomplete tensor decomposition via the SoS method [MSS16]; our work is an important
step towards giving lightweight, spectral algorithms for such problems.

1.1 Our Results

Our contribution is a robust, lightweight spectral algorithm for tensor decomposition in the
overcomplete regime. We require that the components satisfy an algebraic non-degeneracy
assumption satisï¬ed by all but a measure-0 set of inputs. At a high level, we require that a
certain matrix associated with the components of the tensor have full rank. Though the
assumption may at ï¬rst seem complicated, we give it formally here:

Deï¬nition 1.1. Let Î âŠ¥
be the projector to the orthogonal complement of the subspace of
2,3
(â„ğ‘‘)âŠ—3 that is symmetric in its latter two tensor modes. Equivalently, Î âŠ¥
2(Id âˆ’ ğ‘ƒ2,3),
where ğ‘ƒ2,3 is the linear operator that interchanges the second and third modes of (â„ğ‘‘)âŠ—3.

2,3 = 1

Deï¬nition 1.2. Let Î img(ğ‘€) denote the projector to the column space of the matrix ğ‘€.
Equivalently, Î img(ğ‘€) = (ğ‘€ğ‘€(cid:62))âˆ’1/2ğ‘€ = ğ‘€(ğ‘€(cid:62)ğ‘€)âˆ’1/2, where (ğ‘€ğ‘€(cid:62))âˆ’1/2 is the whitening
transform of ğ‘€ and is equal to the Moore-Penrose pseudoinverse of (ğ‘€ğ‘€(cid:62))1/2.

Deï¬nition 1.3. Vectors ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘ are ğœ…-non-degenerate if the matrix ğ¾(ğ‘1, . . . , ğ‘ğ‘›),
deï¬ned below, has minimum singular value at least ğœ… > 0. If ğœ… = 0, we say that the {ğ‘ğ‘– } are
degenerate.

The matrix ğ¾(ğ‘1, . . . , ğ‘ğ‘›) is given by choosing for each ğ‘– âˆˆ [ğ‘›] a matrix ğµğ‘– whose columns
form a basis for the orthogonal complement of ğ‘ğ‘– in â„ğ‘‘, assembling the ğ‘‘3 Ã— ğ‘›(ğ‘‘ âˆ’ 1) matrix
ğ» whose rows are given by ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— ğµ(ğ‘—)
ğ‘–

as

ğ» =

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°
2,3Î img(ğ»(cid:62)).

and letting ğ¾(ğ‘1, . . . , ğ‘ğ‘›) = Î âŠ¥

ğ‘(cid:62)
1

âŠ— ğµ(cid:62)
1

âŠ— ğ‘(cid:62)
1
...
ğ‘› âŠ— ğ‘(cid:62)
ğ‘(cid:62)
ğ‘› âŠ— ğµ(cid:62)
ğ‘›

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

We note that when ğ‘› (cid:28) ğ‘‘2 then all but a measure-zero set of unit (ğ‘1, . . . , ğ‘ğ‘›) âˆˆ â„ğ‘‘ğ‘›
satisfy the condition that ğœ… > 0. We expect also that for ğ‘› (cid:28) ğ‘‘2, if ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘ are
independent random unit vectors then ğœ… (cid:62) Î©(1) â€“ we provide simulations in support of
this in Section C.6

6Furthermore, standard techniques in random matrix theory prove that when ğ‘1, . . . , ğ‘ğ‘› are random then
matrices closely related to ğ¾(ğ‘1, . . . , ğ‘ğ‘›) are well-conditioned; for instance this holds (roughly speaking)
if (ğ»(cid:62)
2 ğ»)âˆ’1/2 are removed. However, inverses and pseudoinverses of random matrices,
especially those with dependent entries like ours, are infamously challenging to analyze â€“ we leave this
challenge to future work. See Section C for details.

1 ğ»)âˆ’1/2 and (ğ»(cid:62)

4

Some previous works on tensor decomposition under algebraic nondegeneracy as-
sumptions also give smoothed analyses of nondegeneracy, showing that small random
-well-conditioned (for diï¬€ering notions of
perturbations of arbitrary vectors are
well-conditioned-ness) [BCMV14, MSS16]. We expect that a similar smoothed analy-
sis is possible for ğœ…-non-degeneracy, though because of the speciï¬c form of the matrix
ğ¾(ğ‘1, . . . , ğ‘ğ‘›) it does not follow immediately from known results. We defer this technical
challenge to future work.

1
poly(ğ‘‘)

Given this non-degeneracy condition, we robustly decompose the input tensor in time
ğœ… ), where we have suppressed factors depending on the smallest singular value of a

Ëœğ‘‚( ğ‘›2ğ‘‘3
matrix ï¬‚attening of our tensor.

Theorem (Special case of Theorem 7.3). Suppose that ğ‘‘ (cid:54) ğ‘› (cid:54) ğ‘‘2, and that ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘
are ğœ…-non-degenerate unit vectors for ğœ… > 0, and suppose that T is their 4-tensor perturbed by noise,
T âˆˆ (â„ğ‘‘)âŠ—4 such that T = (cid:205)ğ‘–âˆˆ[ğ‘›] ğ‘âŠ—4
log ğ‘‘ in its
ğ‘–
ğ‘‘2 Ã— ğ‘‘2 reshaping. Suppose further that when reshaped to a ğ‘‘2 Ã— ğ‘‘2 matrix, (cid:107)ğ‘‡âˆ’1(cid:107) (cid:54) ğ‘‚(1) and
that (cid:107) (cid:205)ğ‘–âˆˆ[ğ‘›](ğ‘âŠ—3

+ ğ¸, where ğ¸ is a perturbation such that (cid:107)ğ¸(cid:107) (cid:54) ğœ€

)(cid:62)(cid:107) (cid:54) ğ‘‚(1).

There exists an algorithm decompose with running time Ëœğ‘‚(ğ‘›2ğ‘‘3ğœ…âˆ’1), so that for every such
T there exists a subset ğ‘† âŠ† {ğ‘1, . . . , ğ‘ğ‘› } of size |ğ‘†| (cid:62) 0.99ğ‘›, such that decompose(T) with high
probability returns a set of ğ‘¡ = Ëœğ‘‚(ğ‘›) unit vectors ğ‘1, . . . , ğ‘ğ‘¡ where every ğ‘ğ‘– âˆˆ ğ‘† is close to some ğ‘ ğ‘—,
and each ğ‘ ğ‘— is close to some ğ‘ğ‘– âˆˆ ğ‘†:

)(ğ‘âŠ—3
ğ‘–

ğ‘–

âˆ€ğ‘ğ‘– âˆˆ ğ‘†, max

ğ‘—

|(cid:104)ğ‘ ğ‘— , ğ‘ğ‘–(cid:105)| (cid:62) 1âˆ’ğ‘‚

(cid:17) 1/8

,

(cid:16) ğœ€
ğœ…2

and

âˆ€ğ‘— âˆˆ [ğ‘¡], max
ğ‘ğ‘– âˆˆğ‘†

|(cid:104)ğ‘ ğ‘— , ğ‘ğ‘–(cid:105)| (cid:62) 1âˆ’ğ‘‚

(cid:17) 1/8

.

(cid:16) ğœ€
ğœ…2

Furthermore, if ğ‘1, . . . , ğ‘ğ‘› are random unit vectors, then with high probability they satisfy the

conditions of this theorem with ğœ… = Î©(1).

When ğ‘› (cid:54) ğ‘‘, our algorithm still obtains nontrivial guarantees (though the runtime
asymptotics are dominated by other terms); however in this regime, a combination of the
simpler algorithm of [SS17] and a whitening procedure gives comparable guarantees.

We remark that our full theorem, Theorem 7.3, does not pose as many restrictions
on the {ğ‘ğ‘– }; we do not generally require that (cid:107)ğ‘‡âˆ’1(cid:107) (cid:54) ğ‘‚(1) or that (cid:107) (cid:205)ğ‘–(ğ‘âŠ—3
)(cid:62)(cid:107) (cid:54)
ğ‘‚(1). However, allowing these quantities to depend on ğ‘‘ and ğ‘› aï¬€ects our runtime and
approximation guarantees, and so to simplify presentation we have made these restrictions
here; we refer the reader to Theorem 7.3 for details.

)(ğ‘âŠ—3
ğ‘–

ğ‘–

Furthermore, in the theorem stated above we recover only a 0.99-fraction of the vectors,
and we require the perturbation to have magnitude ğ‘‚( 1
log ğ‘‘ ). This is again a particular
choice of parameters in Theorem 7.3, which allows for a four-way tradeoï¬€ among accuracy,
magnitude of perturbation, fraction of components recovered, and runtime. For example,
in spectral norm, then we may recover all components in time
if the perturbation is

1
poly(ğ‘‘)

5

Algorithm
[LCC07]
[AGJ17]
[GM17]
[MSS16]
[SS17]

Type
algebraic
iterative
iterative
SDP
spectral

this paper

spectral

Rank
ğ‘› (cid:54) ğ‘‘2
ğ‘› (cid:54) ğ‘œ(ğ‘‘1.5)
ğ‘› (cid:54) ğ‘‚(ğ‘‘2)
ğ‘› (cid:54) ğ‘‘2
ğ‘› (cid:54) ğ‘‘
ğ‘› (cid:54) ğ‘‘2

Robustness
(cid:107)ğ¸(cid:107)âˆ (cid:54) 2âˆ’ğ‘‚(ğ‘‘)
(cid:107)ğ¸(cid:107) (cid:54) ğ‘œ( ğ‘›
ğ‘‘2 )
ğ¸ = 0
(cid:107)ğ¸(cid:107) (cid:54) 0.01
(cid:107)ğ¸(cid:107) (cid:54) ğ‘‚( log log ğ‘‘
log ğ‘‘ )
(cid:107)ğ¸(cid:107) (cid:54) ğ‘‚( 1
log ğ‘‘ )

Assumptions
algebraic
random, warm start
random, warm start
algebraic
orthogonal

algebraic

Runtime
Ëœğ‘‚(ğ‘›3ğ‘‘4)
Ëœğ‘‚(ğ‘›ğ‘‘3)
Ëœğ‘‚(ğ‘›ğ‘‘4)
(cid:62) ğ‘›ğ‘‘24
Ëœğ‘‚(ğ‘‘2+ğœ”)
Ëœğ‘‚(ğ‘›2ğ‘‘3)

Table 1: A comparison of tensor decomposition algorithms for rank-ğ‘› 4-tensors in (â„ğ‘‘)âŠ—4. Here ğœ”
denotes the matrix multiplication constant. A robustness bound (cid:107)ğ¸(cid:107) (cid:54) ğœ‚ refers to the requirement
that a ğ‘‘2 Ã— ğ‘‘2 reshaping of the error tensor ğ¸ have spectral norm at most ğœ‚. Some of the algorithmsâ€™
guarantees involve a tradeoï¬€ between robustness, runtime, and assumptions; where this is the case,
we have chosen one representative setting of parameters. See Section B for details. Above, â€œrandomâ€
indicates that the algorithm assumes ğ‘1, . . . , ğ‘ğ‘› are independent unit vectors (or Gaussians) and
â€œalgebraicâ€ indicates that the algorithm assumes that the vectors avoid an algebraic set of measure 0.

Ëœğ‘‚(ğ‘›2ğ‘‘3ğœ…âˆ’1); alternatively, if the perturbation has spectral norm ğœ‚2 = Î˜(1), then we may
recover an 0.99-fraction of components in time Ëœğ‘‚(ğ‘›2+ğ‘‚(ğœ‚)ğ‘‘3ğœ…âˆ’1) up to accuracy 1 âˆ’ ğ‘‚( ğœ‚
ğœ…2 )1/8.
Again, we refer the reader to Theorem 7.3 for the full tradeoï¬€.

Finally, a note about our recovery guarantee: we guarantee that every vector returned
by the algorithm is close to some component, and furthermore that most components will
be close to some vector. It is possible to run a clean-up procedure after our algorithm,
in which nearby approximate components ğ‘ ğ‘— are clustered to correspond to a speciï¬c ğ‘ğ‘–;
depending on the proximity of the ğ‘ğ‘– to each other, this may require stronger accuracy
guarantees, and so we leave this procedure as an independent step. Our guarantee does not
include signs, but this is because the tensor T is an even-order tensor, so the decomposition
is only unique up to signings as (âˆ’ğ‘ğ‘–)âŠ—4 = ğ‘âŠ—4
.

ğ‘–

1.2 Related works

The literature on tensor decomposition is broad and varied, and we will not attempt to
survey it fully here (see e.g. the survey [KB09] or the references within [AGH+14, GM17]
for a fuller picture). We will give an idea of the relationship between our algorithm and
others with provable guarantees.

For simplicity we focus on order-4 tensors. Algorithms with provable guarantees for
tensor decomposition fall broadly into three classes: iterative methods, convex programs,
and spectral algorithms. For a brief comparison of our algorithm to previous works, we
include Table 1.

6

Iterative methods are a class of algorithms that maintain one (or
Iterative Methods.
sometimes several) estimated component(s) ğ‘, and update the estimate using a variety
of update rules. Some popular update rules include tensor power iteration [AGH+14],
gradient descent [GM17], and alternating-minimization [AGJ14b]. Most of these methods
have the advantage that they are fast; the update steps usually run in time linear in the
input size, and the number of updates to convergence is often polylogarithmic in the input
size.

The performance of the most popular iterative methods has been well-characterized
in some restricted settings; for example, when the components {ğ‘ğ‘– } are orthogonal or
linearly independent [AGH+14, GHJY15, SV17], or are independently drawn random
vectors [AGJ17, GM17]. Furthermore, many of these analyses require a â€œwarm start,â€ or an
initial estimate ğ‘ that is more correlated with a component than a typical random starting
point. Few provable guarantees are known for the non-random overcomplete regime, or in
the presence of arbitrary perturbations.

Convex Programming. Convex programs based on the sum-of-squares (SoS) semideï¬nite
programming (SDP) relaxation yield the most general provable guarantees for tensor
decomposition. These works broadly follow a method of pseudo-moments: interpreting the
input tensor (cid:205)ğ‘–âˆˆ[ğ‘›] ğ‘âŠ—ğ‘˜
as the ğ‘˜-th moment tensor ğ”¼ ğ‘‹ âŠ—ğ‘˜ of a distribution ğ‘‹ on â„ğ‘‘, this
ğ‘–
approach uses SoS to generate surrogates (or pseudo-moments) for higher moment tensors,
like ğ”¼ ğ‘‹ âŠ—100ğ‘˜ = (cid:205)ğ‘–âˆˆ[ğ‘›] ğ‘âŠ—100ğ‘˜
. It is generally easier to extract the components ğ‘1, . . . , ğ‘ğ‘›
from (cid:205)ğ‘–âˆˆ[ğ‘›](ğ‘âŠ—100
} have fewer algebraic
dependencies than the vectors {ğ‘ğ‘– }, and are farther apart in Euclidean distance. Of course,
ğ”¼ ğ‘‹ âŠ—100ğ‘˜ = (cid:205)ğ‘–âˆˆ[ğ‘›] ğ‘âŠ—100ğ‘˜
is not given as input, and even in applications where the input is
negotiable, it may be expensive or impossible to obtain such a high-order tensor. The SoS
method uses semideï¬nite programming to generate a surrogate which is good enough to
be used to ï¬nd the vectors ğ‘1, . . . , ğ‘ğ‘›

)âŠ—ğ‘˜ than from (cid:205)ğ‘–âˆˆ[ğ‘›] ğ‘âŠ—ğ‘˜
ğ‘–

, because the vectors {ğ‘âŠ—100

ğ‘–

ğ‘–

ğ‘–

ğ‘–

Work on sum-of-squares relaxations for tensor decomposition began with the quasi-
polynomial time algorithm of [BKS15]; this algorithm requires only mild well-conditioned-
ness assumptions, but also requires high-order tensors as input, and runs in quasi-
polynomial time. This was followed by an analysis showing that, at least in the setting of
random ğ‘1, . . . , ğ‘ğ‘›, the SoS algorithm can decompose substantially overcomplete tensors
of order 3 [GM15]. This line of work ï¬nally concluded with the work of Ma, Shi, and
Steurer [MSS16], who give sum-of-squares based polynomial-time algorithms for tensor
decomposition in the most general known settings: under mild algebraic assumptions on
the components, and in the presence of adversarial noise, so long as the noise tensor has
bounded spectral norm in its matrix reshapings.

These SoS algorithms have the best known polynomial-time guarantees, but they
are formidably slow. The work of [MSS16] uses the degree-8 sum-of-squares relaxation,

7

meaning that to ï¬nd each of the ğ‘› components, one must solve an SDP in Î©(ğ‘‘8) variables.
While these results are important in establishing that polynomial-time algorithms exist for
these settings, their runtimes are far from eï¬ƒcient.

Inspired by the mild assumptions
Spectral algorithms from Sum-of-Squares Analyses.
needed by SoS algorithms, there has been a line of work that uses the analyses of SoS in
order to design more eï¬ƒcient spectral algorithms, which ideally work for similarly-broad
classes of tensors.

At a high level, these spectral algorithms use eigendecompositions of speciï¬c matrix
polynomials to directly construct approximate primal and dual solutions to the SoS
semideï¬nite programs, thereby obtaining the previously mentioned â€œsurrogate momentsâ€
without having to solve an SDP. Since the SoS SDPs are quite powerful, constructing
(even approximate) solutions to them directly and eï¬ƒciently is a nontrivial endeavor. The
resulting matrices are only approximately SDP solutions â€” in fact, they are often far from
satisfying most of the constraints of the SoS SDPs. There is a tradeoï¬€ between how well
these spectrally constructed solutions approximate the SoS output and how eï¬ƒciently
the algorithm can be implemented. However, by carefully choosing which constraints to
satisfy, these works are able to apply the SDP rounding algorithms to the approximate
spectrally-constructed solutions (often with new analyses) to obtain similar algorithmic
guarantees.

The work of [HSSS16] was the ï¬rst to adapt the analysis of SoS for random ğ‘1, . . . , ğ‘ğ‘›
presented by [GM15] to obtain spectral algorithms for tensor decomposition, giving
subquadratic algorithms for decomposing random overcomplete tensors with ğ‘› (cid:54) ğ‘‚(ğ‘‘4/3).
As SoS algorithms have developed, so too have their faster spectral counterparts.
In
particular, [SS17] adapted some of the SoS arguments presented in [MSS16] to give robust
subquadratic algorithms for decomposing orthogonal 4-tensors in the presence of adversarial
noise bounded only in spectral norm.

Our result builds on the progress of both [MSS16, SS17]. The SoS algorithm of [MSS16]
was the ï¬rst to robustly decompose generic overcomplete tensors in polynomial time.
The spectral algorithm of [SS17] obtains a much faster running time for robust tensor
decomposition, but sacriï¬ces overcompleteness. Our work adapts (and improves upon)
the SoS analysis of [MSS16] to give a spectral algorithm for the robust and overcomplete
regime. Our primary technical contribution is the eï¬ƒcient implementation of the lifting
step in the SoS analysis of [MSS16] as an eï¬ƒcient spectral algorithm to generate surrogate
6th order moments; this is the subject of Section 5, and we give an informal description in
Section 2.

FOOBI The innovative FOOBI (Fourth-Order Cumulant-Based Blind Identiï¬cation)
algorithm of [LCC07] was the ï¬rst method with provable guarantees for overcomplete

8

4-th order tensor decomposition under algebraic nondegeneracy assumptions. Like our
algorithm, FOOBI can be seen as a lifting procedure (to an 8-th order tensor) followed by
a rounding procedure. The FOOBI lifting procedure inspires ours â€“ although ours runs
faster because we lift to a 6-tensor rather than an 8-tensor â€“ but the FOOBI rounding step
is quite diï¬€erent, and proceeds via a clever simultaneous diagonalization approach. The
advantage our algorithm oï¬€ers over FOOBI is twofold: ï¬rst, it provides formal, strong
robustness guarantees, and second, it has a faster asymptotic runtime.

ğ‘–

ğ‘–=1 ğ‘âŠ—4

To the ï¬rst point: for a litmus test, consider the case that ğ‘› = ğ‘‘ and ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘
are orthonormal. On input ğ‘‡ = (cid:205)ğ‘›
+ ğ¸, our algorithm recovers the ğ‘ğ‘– for arbitrary
perturbations ğ¸ so long as they are bounded in spectral norm by (cid:107)ğ¸(cid:107) (cid:54) 1/poly log ğ‘‘.7
We are not aware of any formal analyses of FOOBI when run on tensors with arbitrary
perturbations of this form. Precisely what degree of robustness should be expected from
this modiï¬ed FOOBI algorithm is unclear. The authors of [LCC07] do suggest (without
analysis) a modiï¬cation of their algorithm for the setting of nonzero error tensors ğ¸,
involving an alternating-minimization method for computing an approximate simultaneous
diagonalization. Because the problem of approximate simultaneous diagonalization is
non-convex, establishing robustness guarantees for the FOOBI algorithm when augmented
with the approximate simultaneous diagonalization step appears to be a nontrivial technical
endeavor. We think this is an interesting and potentially challenging open question.

Further, while the running time of FOOBI depends on the speciï¬c implementation of
its linear-algebraic operations, we are unaware of any technique to implement it in time
faster than Ëœğ‘‚(ğ‘›3ğ‘‘4). In particular, the factor of ğ‘‘4 appears essential to any implementation
of FOOBI; it represents the side-length of a ğ‘‘4 Ã— ğ‘‘4 square unfolding of a ğ‘‘-dimensional
8-tensor, which FOOBI employs extensively. By contrast, our algorithm runs in time
Ëœğ‘‚(ğ‘›2ğ‘‘3), which is (up to logarithmic factors) faster by a factor of ğ‘›ğ‘‘.

2 Overview of algorithm

We begin by describing a simple tensor decomposition algorithm for orthogonal 3-tensors:
Gaussian rounding (Jennrichâ€™s algorithm [Har70]). We then build on that intuition to
describe our algorithm.

Orthogonal, undercomplete tensors. Suppose that ğ‘¢1, . . . , ğ‘¢ğ‘‘ âˆˆ â„ğ‘‘ are orthonormal
vectors, and that we are given ğ‘‡ = (cid:205)ğ‘–âˆˆ[ğ‘‘] ğ‘¢âŠ—3
. As a ï¬rst attempt at recovering the ğ‘¢ğ‘–, one
ğ‘–
might be tempted to choose the ï¬rst â€œsliceâ€ of ğ‘‡, the ğ‘‘ Ã— ğ‘‘ matrix ğ‘‡1 = (cid:205)ğ‘– ğ‘¢ğ‘–(1) Â· ğ‘¢ğ‘–ğ‘¢(cid:62)
, and
ğ‘–
compute its singular value decomposition (SVD). However, if |ğ‘¢ğ‘–(1)| = |ğ‘¢ğ‘—(1)| for some

7In contrast, most iterative methods, such as power iteration, can only handle perturbations of spectral

norm at most (cid:107)ğ¸(cid:107) (cid:54) 1/poly(ğ‘‘).

9

ğ‘– â‰  ğ‘— âˆˆ [ğ‘‘], the SVD will not allow us to recover these components. In this setting, Gaussian
rounding allows us to exploit the additional mode of ğ‘‡: If we sample ğ‘” âˆ¼ ğ’©(0, Idğ‘‘), then we
can take the random ï¬‚attening ğ‘‡(ğ‘”) = (cid:205)ğ‘– (cid:104)ğ‘”, ğ‘¢ğ‘–(cid:105) Â· ğ‘¢ğ‘–ğ‘¢(cid:62)
; because the (cid:104)ğ‘”, ğ‘¢ğ‘–(cid:105) are independent
ğ‘–
standard Gaussians, they are distinct with probability 1, and an SVD will recover the
ğ‘¢ğ‘– exactly. Moreover, this algorithm also solves ğ‘˜-tensor decomposition for orthogonal
tensors with ğ‘˜ (cid:62) 4, by treating (cid:205)ğ‘–âˆˆ[ğ‘‘] ğ‘¢âŠ—ğ‘˜

as the 3-tensor (cid:205)ğ‘–âˆˆ[ğ‘‘] ğ‘¢âŠ—ğ‘˜âˆ’1

âŠ— ğ‘¢ğ‘– âŠ— ğ‘¢ğ‘–.

ğ‘–

ğ‘–

ğ‘–

In our setting, we have unit vectors {ğ‘ğ‘– }ğ‘–âˆˆ[ğ‘›] âŠ‚ â„ğ‘‘
Challenges of overcomplete tensors.
with ğ‘› > ğ‘‘, and ğ‘‡ = (cid:205)ğ‘– ğ‘âŠ—4
(focusing for now on the unperturbed case). Since ğ‘› > ğ‘‘, the
components ğ‘1, . . . , ğ‘ğ‘› are not orthogonal: they are not even linearly independent. So, we
cannot hope to use Gaussian rounding as a black box. While the vectors ğ‘1 âŠ— ğ‘1, . . . , ğ‘ğ‘› âŠ— ğ‘ğ‘›
may be linearly independent, the spectral decompositions of the matrix (cid:205)ğ‘–âˆˆ[ğ‘›](ğ‘âŠ—2
)(cid:62)
are not necessarily useful, since its eigenvectors may not be close to any of the vectors ğ‘ğ‘–,
and may be unique only up to rotation.

)(ğ‘âŠ—2
ğ‘–

ğ‘–

Challenges of perturbations. Returning momentarily to the orthogonal setting with
ğ‘› (cid:54) ğ‘‘, new challenges arise when the perturbation tensor ğ¸ is nonzero. For an orthog-
onal 4-tensor ğ‘‡ = (cid:205)ğ‘–âˆˆ[ğ‘‘] ğ‘¢âŠ—4
+ ğ¸, the Gaussian rounding algorithm produces the matrix
(cid:205)ğ‘–âˆˆ[ğ‘‘](cid:104)ğ‘”, ğ‘¢âŠ—2
(cid:105)ğ‘¢ğ‘–ğ‘¢ğ‘–T + ğ¸ğ‘” for some ğ‘‘ Ã— ğ‘‘ matrix ğ¸ğ‘”. The diï¬ƒculty is that even if the spectral
norm (cid:107)ğ¸(cid:107) (cid:28) ğœmin((cid:205)ğ‘–âˆˆ[ğ‘‘](ğ‘¢âŠ—2
)(ğ‘¢âŠ—2
)T) = 1, the matrix ğ¸ğ‘” sums many slices of the tensor ğ¸,
ğ‘–
and so the spectrum of ğ¸ğ‘” can overwhelm that of (cid:205)ğ‘–âˆˆ[ğ‘‘](cid:104)ğ‘”, ğ‘¢âŠ—2

(cid:105)ğ‘¢ğ‘–ğ‘¢ğ‘–T.

ğ‘–

ğ‘–

ğ‘–

This diï¬ƒculty is studied in [SS17], where it is resolved by SoS-inspired preprocessing

ğ‘–

of the tensor ğ‘‡. We borrow many of those ideas in this work.

Algorithmic strategy. We now give an overview of our algorithm. Algorithm 1 gives
a summarized version of the algorithm, with details concerning robustness and fast
implementation omitted.

There are two main stages to the algorithm: the ï¬rst stage is lifting, where the input rank-
ğ‘› 4-tensor over â„ğ‘‘ is lifted to a corresponding rank-ğ‘› 3-tensor over a higher dimensional
space â„ğ‘‘2; this creates an opportunity to use Gaussian rounding on the newly-created tensor
modes. In the second rounding stage, the components of the lifted tensor are recovered
using a strategy similar to Gaussian rounding and then used to ï¬nd the components of the
input.

This parallels the form of the SoS-based overcomplete tensor decomposition algorithm
of [MSS16], where both stages rely on SoS semideï¬nite programming. Our main technical
contribution is a spectral implementation of the lifting stage; our spectral implementation
of the rounding stage reuses many ideas of [SS17], adapted round the output of our new
lifting stage.

10

ğ‘–

ğ‘–

ğ‘–

ğ‘–

ğ‘–

ğ‘–

). Since {ğ‘Š(ğ‘âŠ—2

)ğ‘–âˆˆ[ğ‘›] from Span(ğ‘âŠ—2

The lifting works by deriving Span(ğ‘âŠ—3

Lifting. The goal of the lifting stage is to transform the input ğ‘‡ = (cid:205)ğ‘–âˆˆ[ğ‘›](ğ‘âŠ—2
)(ğ‘âŠ—2
ğ‘–
ğ‘–
an orthogonal 3-tensor. Let ğ‘Š = ğ‘‡âˆ’1/2 and observe that the whitened vectors ğ‘Š(ğ‘âŠ—2
orthonormal; therefore we will want to use ğ‘‡ to ï¬nd the orthogonal 3-tensor (cid:205)ğ‘–âˆˆ[ğ‘›](ğ‘Š ğ‘âŠ—2

simply the column space of the input ğ‘‡. By transforming Span(ğ‘âŠ—3
we obtain Span(ğ‘Š(ğ‘âŠ—2
) âŠ— ğ‘
projector to their span is in fact equal to (cid:205)ğ‘–(ğ‘Š(ğ‘âŠ—2
reshaping and a ï¬nal multiplication by ğ‘Š away from the orthogonal tensor (cid:205)ğ‘–(ğ‘Š(ğ‘âŠ—2

)T to
) are
)âŠ—3.
)ğ‘–âˆˆ[ğ‘›], where the latter is
) using ğ‘Š = ğ‘‡âˆ’1/2,
ğ‘–
) âŠ— ğ‘ğ‘– }ğ‘–âˆˆ[ğ‘›] are orthonormal, the orthogonal
)T, which is only a
) âŠ— ğ‘
))âŠ—3.
). It rests on an
algebraic â€œidentiï¬abilityâ€ argument, which establishes that for almost all problem instances
(all but an algebraic set of measure 0), the subspace Span(ğ‘âŠ—3
) âŠ— â„ğ‘‘
intersected with the symmetric subspace Span({ğ‘¥ âŠ— ğ‘¥ âŠ— ğ‘¥}ğ‘¥âˆˆâ„ğ‘‘). Since we can compute
Span(ğ‘ğ‘– âŠ— ğ‘ğ‘–) from the input and since the symmetric subspace is easy to describe, we
are able to perform this lifting step eï¬ƒciently. The simplest version of the identiï¬ability
argument is given in Lemma 2.1, and a more robust version that includes a condition
number analysis is given in Section 5.1.

The key step is the operation which obtains Span(ğ‘âŠ—3

) is equal to Span(ğ‘âŠ—2

) from Span(ğ‘âŠ—2

)(ğ‘Š(ğ‘âŠ—2

) âŠ— ğ‘

ğ‘–

ğ‘–

ğ‘–

ğ‘–

ğ‘–

ğ‘–

ğ‘–

ğ‘–

ğ‘–

ğ‘–

Lemma 2.1 (Simple Identiï¬ability). Let ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘ with ğ‘› (cid:54) ğ‘‘2. Let ğ‘† denote Span({ğ‘âŠ—2
})
ğ‘–
and let ğ‘‡ denote Span({ğ‘âŠ—3
}) and assume both have dimension ğ‘›. Let sym âŠ† (â„ğ‘‘)âŠ—3 be the linear
subspace sym = Span({ğ‘¥ âŠ— ğ‘¥ âŠ— ğ‘¥}ğ‘¥âˆˆâ„ğ‘‘) . For each ğ‘–, let {ğ‘ğ‘–,ğ‘— } ğ‘—âˆˆ[ğ‘‘âˆ’1] be an arbitrary orthonormal
basis the orthogonal complement of ğ‘ğ‘– in â„ğ‘‘. Let also

ğ‘–

ğ¾(cid:48)T :=

ğ‘1 âŠ— ğ‘1 âŠ— ğ‘1,1 âˆ’ ğ‘1 âŠ— ğ‘1,1 âŠ— ğ‘1
...
ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— ğ‘ğ‘–,ğ‘— âˆ’ ğ‘ğ‘– âŠ— ğ‘ğ‘–,ğ‘— âŠ— ğ‘ğ‘–
...
ğ‘ğ‘› âŠ— ğ‘ğ‘› âŠ— ğ‘ğ‘›,ğ‘‘âˆ’1 âˆ’ ğ‘ğ‘› âŠ— ğ‘ğ‘›,ğ‘‘âˆ’1 âŠ— ğ‘ğ‘›

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

,

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

Then if ğ¾(cid:48) has full rank ğ‘›(ğ‘‘ âˆ’ 1), it follows that (ğ‘† âŠ— â„ğ‘‘) âˆ© sym = ğ‘‡.

Proof. To show that ğ‘‡ âŠ† (ğ‘† âŠ— â„ğ‘‘) âˆ© sym, we simply note that {ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— ğ‘ğ‘– }ğ‘–âˆˆ[ğ‘›] form a basis
for ğ‘‡ and are also each in both ğ‘† âŠ— â„ğ‘‘ and sym.

To show that (ğ‘† âŠ— â„ğ‘‘) âˆ© sym âŠ† ğ‘‡, we take some ğ‘¦ âˆˆ (ğ‘† âŠ— â„ğ‘‘) âˆ© sym. Since ğ‘¦ is symmetric

under mode interchange, we express ğ‘¦ in two ways as

(cid:213)

ğ‘¦ =

ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— ğ‘ğ‘– =

ğ‘–

ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— ğ‘ğ‘– .

(cid:213)

ğ‘–

11

Then by subtracting these two expressions for ğ‘¦ from each other, we ï¬nd

(cid:213)

0 =

ğ‘–

ğ‘ğ‘– âŠ— (ğ‘ğ‘– âŠ— ğ‘ğ‘– âˆ’ ğ‘ğ‘– âŠ— ğ‘ğ‘–).

We express ğ‘ğ‘– = (cid:104)ğ‘ğ‘– , ğ‘ğ‘–(cid:105)ğ‘ğ‘– + (cid:205)ğ‘— ğ›¾ğ‘–ğ‘—ğ‘ğ‘–ğ‘— for some vector ğ›¾. Then the symmetric parts cancel
out, leaving

(cid:213)

0 =

ğ›¾ğ‘–ğ‘— ğ‘ğ‘– âŠ— (ğ‘ğ‘– âŠ— ğ‘ğ‘–ğ‘— âˆ’ ğ‘ğ‘–ğ‘— âŠ— ğ‘ğ‘–) = ğ¾(cid:48)ğ›¾ .

ğ‘–ğ‘—

Since ğ¾(cid:48) is full rank by assumption, this is only possible when ğ›¾ = 0. Therefore, ğ‘ğ‘– âˆ ğ‘ğ‘– for
(cid:3)
all ğ‘–, so that ğ‘¦ âˆˆ ğ‘‡.

Remark 2.2. Although the condition number from the matrix ğ¾(cid:48) here is not the same as the one
derived from ğ¾ from Deï¬nition 1.3, it is oï¬€ by at most a multiplicative factor of 2(cid:107)ğ‘‡âˆ’1(cid:107)âˆ’1/2.
To see this, ğ¾ in Deï¬nition 1.3 is given as ğ¾ = Î âŠ¥
2,3Î img(ğ»T), whereas we may write
ğ¾(cid:48) = 2Î âŠ¥
2 (cid:107)ğ¾âˆ’1(cid:107) (cid:107)ğ»âˆ’1(cid:107).
By [MSS16, Lemma 6.3], (cid:107)ğ»âˆ’1(cid:107)2 (cid:62) (cid:107)ğ‘‡âˆ’1(cid:107).

2,3Î img(ğ»T)(ğ»ğ»T)1/2 = 2ğ¾(ğ»ğ»T)1/2. Therefore, (cid:107)ğ¾(cid:48)âˆ’1(cid:107) (cid:62) 1

2,3ğ»T = 2Î âŠ¥

Robustness. To ensure that our algorithm is robust to perturbations ğ¸, we must argue that
the column span of ğ‘‡ and ğ‘‡ + ğ¸ are close to each other so long as ğ¸ is bounded in spectral
norm, and furthermore than the lifting operation still produces a subspace ğ‘‰ which is
close to Span({ğ‘Š(ğ‘ğ‘– âŠ— ğ‘ğ‘–) âŠ— ğ‘ğ‘– }). This is done via careful application of matrix perturbation
analysis to the identiï¬ability argument. By operating with ğ‘Š only on third-order vectors
and matrices over (â„ğ‘‘)âŠ—3, we also avoid incurring factors of the fourth-order operator norm
(cid:107)ğ‘‡ (cid:107) in the condition numbers, instead only incurring a much milder sixth-order penalty
(cid:107) (cid:205) ğ‘âŠ—3

T(cid:107). For details, see Section 5.2.

ğ‘âŠ—3
ğ‘–

ğ‘–

If we are given direct access to ğ‘‡ in the absence of noise, the rounding
Rounding.
stage can be accomplished with Gaussian rounding. However when we allow ğ‘‡ to be
adversarially perturbed the situation becomes more delicate. Our rounding stage is an
adaptation of [SS17], though some modiï¬cations are required for the additional challenges
of the overcomplete setting. It recovers the components of an approximation of a 3-tensor
with ğ‘› orthonormal components, provided that said approximation is within ğœ€
ğ‘› in
Frobenius norm distance. The technique is built around Gaussian rounding, but in order
to have this succeed in the presence of ğœ€
ğ‘› Frobenius norm noise, the large singular values
are truncated from the rectangular matrix reshapings of the 3-tensor: this ensures that the
rounding procedure is not entirely dominated by any spectrally large terms in the noise.
, we wish
to extract the ğ‘ğ‘–. Naively one could simply apply ğ‘Š âˆ’1, but this can cause errors in the

After we recover approximations of the orthonormal components ğ‘ğ‘– â‰ˆ ğ‘Š ğ‘âŠ—2

âˆš

âˆš

ğ‘–

12

recovered vectors to blow up by a factor of (cid:107)ğ‘Š âˆ’1(cid:107). Even when the {ğ‘ğ‘– } are random vectors,
(cid:107)ğ‘Š âˆ’1(cid:107) = Î©(poly(ğ‘‘)).8 Instead, we utilize the projector to Span{ğ‘Š(ğ‘ğ‘– âŠ— ğ‘ğ‘–) âŠ— ğ‘ğ‘– } computed
in the lifting step: we lift ğ‘ğ‘–, project it into the span to obtain a vector close to ğ‘Š(ğ‘ğ‘– âŠ— ğ‘ğ‘–) âŠ— ğ‘ğ‘–,
and reshape it to a ğ‘‘2 Ã— ğ‘‘ matrix whose top right-singular vector is correlated with ğ‘ğ‘–. This
extraction-via-lifting step allows us to circumvent a loss of (cid:107)ğ‘Š âˆ’1(cid:107) in the error.

Algorithm 1 Sketch of full algorithm, in the absence of noise
Input: A 4-tensor T âˆˆ (â„ğ‘‘)âŠ—4, so that T = (cid:205)ğ‘›

ğ‘–=1 ğ‘âŠ—4
1. Take the square reshaping ğ‘‡ âˆˆ â„ğ‘‘2Ã—ğ‘‘2 of T and compute its whitening ğ‘Š = ğ‘‡âˆ’1/2 and

for unit vectors ğ‘ğ‘– âˆˆ â„ğ‘‘.

ğ‘–

the projector Î 2 = ğ‘Šğ‘‡ğ‘Š to the image of ğ‘‡.

2. Lifting: Compute the lifted tensor T(cid:48) âˆˆ (â„ğ‘‘2)âŠ—3 so that T(cid:48) = (cid:205)ğ‘–(ğ‘Š ğ‘âŠ—2
ğ‘–

Algorithm 2 for full details).

)âŠ—3.

(See

(a) Find a basis for the subspace ğ‘†3 = (img ğ‘‡) âŠ— â„ğ‘‘ âˆ© sym: take ğ‘†3 to be the top-ğ‘›
eigenspace of (Î 2 âŠ— Id)Î sym(Î 2 âŠ— Id). Then by Lemma 2.1, ğ‘†3 = Span(ğ‘âŠ—3

) .

ğ‘–

(b) Find the projector Î 3 to the space (ğ‘Š âŠ— Id) ğ‘†3 = Span(ğ‘Š ğ‘âŠ—2
(c) Compute the orthogonal 3-tensor: since {ğ‘Š ğ‘âŠ—2

ğ‘–

âŠ— ğ‘ğ‘–).

âŠ— ğ‘ğ‘– } is an orthonormal basis,

ğ‘–

Î 3 =

(cid:213)

ğ‘–

(ğ‘Š ğ‘âŠ—2

ğ‘– âŠ— ğ‘ğ‘–)(ğ‘Š ğ‘âŠ—2

ğ‘– âŠ— ğ‘ğ‘–)T .

Therefore, reshape Î 3 as (cid:205)ğ‘–(ğ‘Š ğ‘âŠ—2
third mode to obtain T(cid:48).

ğ‘–

) âŠ— (ğ‘Š ğ‘âŠ—2

ğ‘–

) âŠ— (ğ‘âŠ—2

ğ‘–

) and multiply ğ‘Š into the

3. Rounding: Use Gaussian rounding to ï¬nd the components ğ‘ğ‘–. (In the presence of

noise, this step becomes substantially more delicate; see Algorithms 3 to 5).

(a) Compute a random ï¬‚attening of T(cid:48) by contracting with ğ‘” âˆ¼ ğ’©(0, Idğ‘‘2) along the
)(cid:105) Â· (ğ‘Š ğ‘âŠ—2

ï¬rst mode, ğ‘‡(cid:48)(ğ‘”) = (cid:205)ğ‘– (cid:104)ğ‘”, (ğ‘Š ğ‘âŠ—2

)(ğ‘Š ğ‘âŠ—2

)(cid:62)

ğ‘–

ğ‘–

(b) Perform an SVD on ğ‘‡(cid:48)(ğ‘”) to recover the eigenvectors (ğ‘Š ğ‘âŠ—2
1
, and re-shape ğ‘âŠ—2

(c) Apply ğ‘Š âˆ’1 to each eigenvector to obtain the ğ‘âŠ—2

), . . . , (ğ‘Š ğ‘âŠ—2

ğ‘› ).

to a matrix

ğ‘–

ğ‘–

ğ‘–

and compute its eigenvector to obtain ğ‘ğ‘–.

Organization.

The full implementation details and the analysis of our algorithm are given in the
following few sections. First, Section 4 sets up some primitives for spectral subspace

8This is in contrast to (cid:107)ğ‘Š (cid:107), which is ğ‘‚(1) in the random case.

13

perturbation analysis and linear-algebraic procedures on which we build the full algorithm
and its analysis. Then Section 5 covers the lifting stage of the algorithm in detail, while
Section 6 elaborates on the rounding stage. Finally, in Section 7 we combine these tools to
prove Theorem 7.3. The appendices some linear-algebraic tools and simulations strongly
suggesting that random tensors with ğ‘› (cid:28) ğ‘‘2 components have constant condition number
ğœ….

3 Preliminaries

Linear algebra We use Idğ‘‘ to denote the ğ‘‘ Ã— ğ‘‘ identity matrix, or just Id if the dimension is
clear from context. For any subspace ğ‘†, we use Î ğ‘† to denote the projector to that subspace.
For ğ‘€ a matrix, img(ğ‘€) refers to the image, or columnspace, of ğ‘€.

We will, in a slight abuse of notation, use ğ‘€âˆ’1 to denote the Moore-Penrose pseudo-
inverse of ğ‘€. Except where explicitly speciï¬ed, this will never be assumed to be equal to
the proper inverse, so that, e.g., in general ğ‘€ğ‘€âˆ’1 = Î img(ğ‘€) â‰  Id and (ğ´ğµ)âˆ’1 â‰  ğµâˆ’1ğ´âˆ’1.

For a matrix ğµ âˆˆ â„ğ‘šÃ—ğ‘›, we will use the whitening matrix ğ‘Š = (ğµğµ)âˆ’1/2, which maps

the columns of ğµ to an orthonormal basis for img(ğµ), so that (ğ‘Š ğµ)(ğ‘Š ğµ)(cid:62) = Î img(ğµ).

We denote by sym âŠ† (â„ğ‘‘)âŠ—3 the linear subspace

sym = Span({ğ‘¥ âŠ— ğ‘¥ âŠ— ğ‘¥}ğ‘¥âˆˆâ„ğ‘‘) .

Note that (Î sym)(ğ‘–,ğ‘—,ğ‘˜);(ğ‘–(cid:48),ğ‘—(cid:48),ğ‘˜(cid:48)) is (|{ğ‘–, ğ‘—, ğ‘˜}|!)âˆ’1 when {ğ‘–, ğ‘—, ğ‘˜} = {ğ‘–(cid:48), ğ‘—(cid:48), ğ‘˜(cid:48)} and zero otherwise.

Tensor manipulations When working with tensors ğ‘‡ âˆˆ (â„ğ‘‘)âŠ—ğ‘˜, we will sometimes
reshape the tensors to lower-order tensors or matrices; in this case, if ğ‘†1, . . . , ğ‘†ğ‘š are a
partition of ğ‘˜, then ğ‘‡(ğ‘†1,...,ğ‘†ğ‘š) is the tensor given by identifying the modes in each ğ‘†ğ‘– into
a single mode. For ğ‘† âŠ‚ [ğ‘‘]ğ‘˜, we will also sometimes use the notation ğ‘‡(ğ‘†) to refer to the
entry of ğ‘‡ indexed by ğ‘†.

A useful property of matrix reshapings is that ğ‘¢ âŠ— ğ‘£ reshapes into the outer product
ğ‘¢ğ‘£T. Linearity allows us to generalize this so, e.g., the reshaping of (ğ‘ˆ âŠ— ğ‘‰)ğ‘€ for ğ‘ˆ âˆˆ â„ğ‘›Ã—ğ‘›
and ğ‘‰ âˆˆ â„ğ‘šÃ—ğ‘š and ğ‘€ âˆˆ â„(ğ‘›âŠ—ğ‘š)Ã—ğ‘ is equal to ğ‘ˆ ğ‘€(cid:48)(ğ‘‰ âŠ— Idğ‘), where ğ‘€(cid:48) âˆˆ â„ğ‘›Ã—(ğ‘šâŠ—ğ‘) is the
reshaping of ğ‘€. Since reshapings can be easily done and undone by exchanging indices,
these identities will sometimes allow more eï¬ƒcient computation of matrix products over
tensor spaces.

We will on occasion use a Â· as a placeholder in a partially applied multiple-argument

function: for instance ğœ•
ğœ•ğ‘¦

ğ‘“ (Â·, ğ‘¦) = limâ„â†’0

1
â„ ( ğ‘“ (Â·, ğ‘¦ + â„) âˆ’ ğ‘“ (Â·, ğ‘¦)).

14

4 Tools for analysis and implementation

In this section, we brieï¬‚y introduce some tools which we will use often in our analysis.

4.1 Robustness and spectral perturbation

A key tool in our analysis of the robustness of Algorithm 1 comes from the theory of the
perturbation of eigenvalues and eigenvectors.

The lemma below combines the Davis-Kahan sin-Î˜ theorem with Weylâ€™s inequality to

characterize how top eigenspaces are aï¬€ected by spectral perturbation.

Theorem 4.1 (Perturbation of top eigenspace). Suppose ğ‘„ âˆˆ â„ğ·Ã—ğ· is a symmetric matrix
with eigenvalues ğœ†1 (cid:62) ğœ†2 (cid:62) . . . (cid:62) ğœ†ğ·. Suppose also (cid:101)ğ‘„ âˆˆ â„ğ·Ã—ğ· is a symmetric matrix with
(cid:107)ğ‘„ âˆ’ (cid:101)ğ‘„ (cid:107) (cid:54) ğœ€. Let ğ‘† and (cid:101)ğ‘† be the spaces generated by the top ğ‘› eigenvectors of ğ‘„ and (cid:101)ğ‘„ respectively.
Then,

sin(ğ‘†, (cid:101)ğ‘†) def

= (cid:107)Î ğ‘† âˆ’ Î 

(cid:101)ğ‘†Î ğ‘† (cid:107) = (cid:107)Î 

(cid:101)ğ‘† âˆ’ Î ğ‘†Î 

(cid:101)ğ‘† (cid:107) (cid:54)

Consequently,

(cid:107)Î ğ‘† âˆ’ Î 

(cid:101)ğ‘† (cid:107) (cid:54)

2ğœ€
ğœ†ğ‘› âˆ’ ğœ†ğ‘›+1 âˆ’ 2ğœ€

.

ğœ€
ğœ†ğ‘› âˆ’ ğœ†ğ‘›+1 âˆ’ 2ğœ€

.

(4.1)

(4.2)

Proof. We ï¬rst prove the theorem assuming that ğ‘„ and (cid:101)ğ‘„ are symmetric. By Weylâ€™s
inequality for matrices [Wey12], the ğ‘›th eigenvalue of (cid:101)ğ‘„ is at least ğœ†ğ‘› âˆ’ 2ğœ€. By Davis and
Kahanâ€™s sin-Î˜ theorem [DK70], since the top-ğ‘› eigenvalues of (cid:101)ğ‘„ are all at least ğœ†ğ‘› âˆ’ 2ğœ€
and the lower-than-ğ‘› eigenvalues of ğ‘„ are all at most ğœ†ğ‘›+1, the sine of the angle between
ğ‘† and (cid:101)ğ‘† is at most (cid:107)ğ‘„ âˆ’ (cid:101)ğ‘„ (cid:107)/(ğœ†ğ‘› âˆ’ ğœ†ğ‘›+1 âˆ’ 2ğœ€). The ï¬nal bound on (cid:107)Î ğ‘† âˆ’ Î 
(cid:101)ğ‘† (cid:107) follows by
triangle inequality.

(cid:3)

4.2 Eï¬ƒcient implementation and runtime analysis
It is not immediately obvious how to implement Algorithm 1 in time Ëœğ‘‚(ğ‘›2ğ‘‘3), since there
are steps that require we multiply or eigendecompose ğ‘‘3 Ã— ğ‘‘3 matrices, which if done
naively might take up to Î©(ğ‘‘9) time.

To accelerate our runtime, we must take advantage of the fact that our matrices have
additional structure. We exploit the fact that in certain reshapings our tensors have low-rank
representations. This allows us to perform matrix multiplication and eigendecomposition
(via power iteration) eï¬ƒciently, and obtain a runtime that is depends on the rank rather
than on the dimension.

For example, the following lemma, based upon a result of [AZL16], captures our

eigendecomposition strategy in a general sense.

15

Lemma 4.2 (Implicit gapped eigendecomposition). Suppose a symmetric matrix ğ‘€ âˆˆ â„ğ‘‘Ã—ğ‘‘
has an eigendecomposition ğ‘€ = (cid:205)ğ‘— ğœ†ğ‘— ğ‘£ ğ‘—ğ‘£ ğ‘—T, and that ğ‘€ğ‘¥ may be computed within ğ‘¡ time steps for
ğ‘¥ âˆˆ â„ğ‘‘. Then ğ‘£1, . . . , ğ‘£ğ‘› and ğœ†1, . . . , ğœ†ğ‘› may be computed in time Ëœğ‘‚(min(ğ‘›(ğ‘¡ + ğ‘›ğ‘‘)ğ›¿âˆ’1/2, ğ‘‘3)),
where ğ›¿ = (ğœ†ğ‘› âˆ’ ğœ†ğ‘›+1)/ğœ†ğ‘›. The dependence on the desired precision is polylogarithmic.

Proof. The ğ‘›(ğ‘¡ + ğ‘›ğ‘‘)ğ›¿âˆ’1/2 runtime is attained by LazySVD in [AZL16, Corollary 4.3]. While
LazySVDâ€™s runtime depends on nnz(ğ‘€) where nnz denotes the number of non-zero
elements in the matrix, in the non-stochastic setting nnz(ğ‘€) is used only as a bound on the
time cost of multiplying a vector by ğ‘€, so in our case we may substitute ğ‘‚(ğ‘¡) instead.

The ğ‘‘3 time is attained by iterated squaring of ğ‘€: in this case, all runtime dependence
(cid:3)

on condition numbers is polylogarithmic.

The following lemma lists some primitives for operations with the tensor T(cid:48) âˆˆ (â„ğ‘‘2)âŠ—3
in Algorithm 1, by interpreting it as a 6-tensor in (â„ğ‘‘)âŠ—6 and using a low-rank factorization
of the square reshaping of that 6-tensor.
Lemma 4.3 (Implicit tensors). For a tensor T âˆˆ (â„[ğ‘‘]2)âŠ—3, suppose that the matrix ğ‘‡ âˆˆ â„[ğ‘‘]3Ã—[ğ‘‘]3
given by ğ‘‡(ğ‘–,ğ‘–(cid:48),ğ‘—),(ğ‘˜,ğ‘˜(cid:48),ğ‘—(cid:48)) = T(ğ‘–,ğ‘–(cid:48)),(ğ‘—,ğ‘—(cid:48)),(ğ‘˜,ğ‘˜(cid:48)) has a rank-ğ‘› decomposition ğ‘‡ = ğ‘ˆğ‘‰T with ğ‘ˆ , ğ‘‰ âˆˆ â„ğ‘‘3Ã—ğ‘›
and ğ‘› (cid:54) ğ‘‘2. Such a rank decomposition provides an implicit representation of the tensor T. This
implicit representation supports:
Tensor contraction: For vectors ğ‘¥, ğ‘¦ âˆˆ â„[ğ‘‘]2, the computation of (ğ‘¥T âŠ— ğ‘¦T âŠ—Id)T or (ğ‘¥T âŠ—IdâŠ— ğ‘¦T)T

or (Id âŠ— ğ‘¥T âŠ— ğ‘¦T)T in time ğ‘‚(ğ‘›ğ‘‘3) to obtain an output vector in â„ğ‘‘2.

Spectral truncation: For ğ‘… âˆˆ â„ğ‘‘2Ã—ğ‘‘4 equal to one of the two matrix reshapings ğ‘‡{1,2}{3} or
(cid:54)1, deï¬ned as T after all larger-than-1 singular
ğ‘‡{2,3}{1} of T, an approximation to the tensor T
values in its reshaping ğ‘… are truncated down to 1. Speciï¬cally, letting ğœŒğ‘˜ be the ğ‘˜th largest
singular value of ğ‘… for ğ‘˜ (cid:54) ğ‘‚(ğ‘›), this returns an implicit representation of a tensor T(cid:48) such
(cid:54)1(cid:107)ğ¹ (cid:54) (1 + ğ›¿)ğœŒğ‘˜ (cid:107)T(cid:107)ğ¹ and the reshaping of T(cid:48) corresponding to ğ‘… has largest
that (cid:107)T(cid:48) âˆ’ T
singular value no more than 1 + (1 + ğ›¿)ğœŒğ‘˜. The representation of T(cid:48) also supports the tensor
contraction, spectral truncation, and implicit matrix multiplication operations, with no more
than a constant factor increase in runtime. This takes time Ëœğ‘‚(ğ‘›2ğ‘‘3 + ğ‘˜(ğ‘›ğ‘‘3 + ğ‘˜ğ‘‘2)ğ›¿âˆ’1/2).

Implicit matrix multiplication: For a matrix ğ‘… âˆˆ â„[ğ‘‘]2Ã—[ğ‘‘]2 with rank at most ğ‘‚(ğ‘›), an implicit
representation of the tensor (ğ‘…T âŠ— Id âŠ— Id)T or (Id âŠ— Id âŠ— ğ‘…T)T, in time ğ‘‚(ğ‘›ğ‘‘4). This output
also supports the tensor contraction, spectral truncation, and implicit matrix multiplication
operations, with no more than a constant factor increase in runtime. Multiplication into the
second mode (Id âŠ— ğ‘…T âŠ— Id)T may also be implicitly represented, but without support for the
spectral truncation operation.

The implementation of these implicit tensor operations consists solely of tensor reshap-
ings, singular value decompositions, and matrix multiplication. However, the details get
involved and lengthy, and so we defer their exposition to Section A.

16

5 Lifting

This section presents Algorithm 2, which lifts a well-conditioned 4-tensor T of rank at
most ğ‘‘2 in (â„ğ‘‘)âŠ—3 to T(cid:48), an orthogonalized version of the 6-tensor in the same components
in (â„ğ‘‘2)âŠ—3; that is, we obtain an orthogonal 3-tensor T(cid:48) whose components correspond to
the orthogonalized Kronecker squares of the components of T. Section 5.1 presents the
identiï¬ability argument giving robust algebraic non-degeneracy conditions under which
the algorithm succeeds.

Although we assume that the tensor components ğ‘ğ‘– are unit vectors, throughout this
section we will keep track of factors of (cid:107)ğ‘ğ‘– (cid:107) so as to better elucidate the scaling and
dimensional analysis.

Algorithm 2 Function lift(T, ğ‘›)
Input: T âˆˆ (â„ğ‘‘)âŠ—4, ğ‘› âˆˆ â„• with ğ‘› (cid:54) ğ‘‘2.

1. Use Lemma 4.2 to ï¬nd the top-ğ‘› eigenvalues and corresponding eigenvectors of the
square matrix reshaping of T, and call the eigendecomposition ğ‘‡ = ğ‘„Î›ğ‘„T. This also
yields ğ‘Š = ğ‘„Î›âˆ’1/2ğ‘„T and Î ğ‘† = ğ‘„ğ‘„T.

2. Use Lemma 4.2 again to ï¬nd the top-ğ‘› eigendecomposition of Î ğ‘†âŠ—â„ğ‘‘Î symÎ ğ‘†âŠ—â„ğ‘‘, im-
plementing multiplication by Î ğ‘†âŠ—â„ğ‘‘ as (Î ğ‘†âŠ—â„ğ‘‘ ğ‘£)(Â·,Â·,ğ‘–) = ğ‘„ğ‘„Tğ‘£(Â·,Â·,ğ‘–) and implementing
Î sym as a sparse matrix. Call the result ğ‘…Î£ğ‘…T and take Î ğ‘†3 = ğ‘…ğ‘…T.

3. Find a basis ğµ(cid:48) for the columnspace of ğ‘€3 = (ğ‘Š âŠ— Id)Î ğ‘†3(ğ‘Š âŠ— Id). Implement this as

(ğµ(cid:48))( Â· , Â· ,ğ‘–); Â· = ğ‘„Î›âˆ’1/2ğ‘„Tğ‘…( Â· , Â· ,ğ‘–); Â· .

4. Use Gram-Schmidt orthogonalization to ï¬nd an orthonormalization ğµ of ğµ(cid:48). Call the

projection operator to this basis Î 3 = ğµğµT.

5. Instantiate an implicit tensor in (â„ğ‘‘2)âŠ—3 with Lemma 4.3, using ğµğµT as the SVD of its
underlying ğ‘‘3 Ã— ğ‘‘3 reshaping. Output this as (Id âŠ— ğ‘Š âˆ’1 âŠ— Id)T(cid:48), meaning a tensor
which, when ğ‘Š âˆ’1 is multiplied into its second mode, becomes equal to T(cid:48).

Output: (Id âŠ— ğ‘Š âˆ’1 âŠ— Id)T(cid:48) âˆˆ (â„ğ‘‘2)âŠ—3, implicitly as speciï¬ed by Lemma 4.3, and Î 3 âˆˆ â„ğ‘‘3Ã—ğ‘‘3.

The following two lemmas will argue that the algorithm is correct, and that it is fast.
First, Lemma 5.1 states that the output of Algorithm 2 is an orthogonal 3-tensor whose
components are ğ‘Š(ğ‘ğ‘– âŠ— ğ‘ğ‘–), where the ğ‘ğ‘– are the components of the original 4-tensor and
ğ‘Š is the whitening matrix for the ğ‘ğ‘– âŠ— ğ‘ğ‘–. Furthermore, if the error in the input is small
in spectral norm compared to some condition numbers, the Frobenius norm error in the
output robustly remains within a small constant of

âˆš

ğ‘›.

17

The main work of the lemma is deferred to Lemma 5.5 in Section 5.2, which repeatedly
applies Davis and Kahanâ€™s sin-Î˜ theorem (Theorem 4.1) to say that the top eigenspaces
of various matrices in the algorithm are relatively unperturbed in spectral norm by small
spectral norm error in the matrices. After that, we simply bound the Frobenius norm error
of a rank-ğ‘› matrix by 2
ğ‘› times its spectral norm error, and reason that Frobenius norms
are unchanged by tensor reshapings.

âˆš

Lemma 5.1 (Correctness of lift). Let ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘ and suppose that (cid:101)T = (cid:205)ğ‘–âˆˆ[ğ‘›] ğ‘âŠ—4
ğ‘–
ğ‘›ğœ‡âˆ’1ğœ…2 for some ğœ€ < 1/63, where ğœğ‘› is the ğ‘›th eigenvalue of (cid:205)ğ‘–âˆˆ[ğ‘›] ğ‘âŠ—2
satiï¬es (cid:107)ğ¸12;34(cid:107) (cid:54) ğœ€ ğœ2
ğ‘–
and ğœ‡ is the operator norm of (cid:205) (cid:107)ğ‘ğ‘– (cid:107)âˆ’2ğ‘âŠ—3
also ğ‘Š = (cid:2)(cid:205)ğ‘–âˆˆ[ğ‘›](ğ‘âŠ—2
)(ğ‘âŠ—2
ğ‘–
(Id âŠ— ğ‘Š âˆ’1 âŠ— Id)(cid:101)T(cid:48) and (cid:101)Î 3 of lift((cid:101)T, ğ‘›) in Algorithm 2 satisfy

+ E
ğ‘âŠ—2
T
ğ‘–
T and ğœ… is the condition number from Lemma 5.3. Let
)T + ğ¸12;34(cid:3) âˆ’1/2 Then the outputs

ğ‘âŠ—3
ğ‘–
)T(cid:3) âˆ’1/2 and (cid:101)ğ‘Š = (cid:2)(cid:205)ğ‘–âˆˆ[ğ‘›](ğ‘âŠ—2

)(ğ‘âŠ—2
ğ‘–

ğ‘–

ğ‘–

ğ‘–

(cid:13)
(cid:13)
(cid:48) âˆ’ (cid:213)
(cid:13)
(cid:101)T
(cid:13)
(cid:13)

ğ‘–

(cid:107)ğ‘ğ‘– (cid:107)âˆ’2(ğ‘Š(ğ‘ğ‘– âŠ— ğ‘ğ‘–))âŠ—3

(cid:54) 126 ğœ€ ğœâˆ’1/2

ğ‘›

âˆš

ğ‘›

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)ğ¹

and

(cid:13)
(cid:13)(cid:101)Î 3 âˆ’ Î Span(ğ‘Š ğ‘âŠ—2
(cid:13)

ğ‘–

(cid:13)
(cid:13)
(cid:13)

âŠ—ğ‘

ğ‘–)

(cid:54) 63 ğœ€ .

Proof. We refer to all matrices and spaces computed in the algorithm with an overset tilde to
reï¬‚ect the fact that the algorithm only has access to approximations with error (so (cid:101)ğ‘† instead
computed in step 2 as the projector
of ğ‘† in the algorithm, etc.). By Lemma 5.5, the Î 
)(cid:107) (cid:54) 18 ğœ€ğœğ‘›ğœ‡âˆ’1, and
to the top-ğ‘› eigenspace of Î 
subsequently, the (cid:101)Î 3 computed in steps 3 and 4 as the projector to ( (cid:101)ğ‘Š âŠ— Id)(cid:101)ğ‘†3 satisï¬es
(cid:107) (cid:101)Î 3 âˆ’ Î Span(ğ‘Š ğ‘âŠ—2

(cid:101)ğ‘†âŠ—â„ğ‘‘ satisï¬es (cid:107)Î 

(cid:101)ğ‘†âŠ—â„ğ‘‘Î symÎ 

âˆ’ Î Span(ğ‘âŠ—3

)(cid:107) (cid:54) 63 ğœ€.

(cid:101)ğ‘†3

(cid:101)ğ‘†3

âŠ—ğ‘

ğ‘–

ğ‘–

ğ‘–

Since the rank of the error is at most 2ğ‘›, the Frobenius norm error is at most 126 ğœ€

âˆš

since {(cid:107)ğ‘ğ‘– (cid:107)âˆ’1ğ‘Š ğ‘âŠ—2
is just the sum of the self-outer-products of vectors in that set, so

âŠ— ğ‘ğ‘– } is an orthonormal set of vectors, the projector to Span(ğ‘Š ğ‘âŠ—2

ğ‘–

ğ‘–

ğ‘›, and
âŠ— ğ‘ğ‘–)

(cid:13)
(cid:13)

(cid:13)(cid:101)Î 3 âˆ’ (cid:213) (cid:107)ğ‘ğ‘– (cid:107)âˆ’2(ğ‘Š ğ‘âŠ—2

ğ‘– âŠ— ğ‘ğ‘–)(ğ‘Š ğ‘âŠ—2

ğ‘– âŠ— ğ‘ğ‘–)T(cid:13)
(cid:13)
(cid:13)ğ¹

(cid:54) 126 ğœ€

âˆš

ğ‘› .

Reshaping the ğ‘‘3 Ã— ğ‘‘3 matrix (cid:101)Î 3 into a tensor in (â„ğ‘‘2)âŠ—3 does not change the Frobenius
norm error, and ï¬nally, multiplying in the last factor of ğ‘Š may contribute a factor of
, so that in the end, (cid:107)(cid:101)T(cid:48) âˆ’ (cid:205)ğ‘– (cid:107)ğ‘ğ‘– (cid:107)âˆ’2(ğ‘Š(ğ‘ğ‘– âŠ— ğ‘ğ‘–))âŠ—3(cid:107)ğ¹ (cid:54) 126 ğœ€ ğœâˆ’1/2
(cid:107)ğ‘Š (cid:107) = ğœâˆ’1/2
(cid:3)
The next lemma states that the running time is Ëœğ‘‚(ğ‘›2ğ‘‘3) multiplied by some condition
numbers. We assume that asympotically faster matrix multiplications and pseudo-
inversions are not used, so that, for instance, squaring a ğ‘‘ Ã— ğ‘‘ matrix takes time Î˜(ğ‘‘3).

ğ‘›.

âˆš

ğ‘›

ğ‘›

18

Lemma 5.2 (Running time of lift). Let ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘ and suppose that T = (cid:205)ğ‘–âˆˆ[ğ‘›] ğ‘âŠ—4
+ E
ğ‘–
satisï¬es the conditions stated in Lemma 5.1. Let ğœğ‘› be the ğ‘›th eigenvalue of (cid:205)ğ‘–âˆˆ[ğ‘›] ğ‘âŠ—2
ğ‘âŠ—2
T
ğ‘–
and ğœ… the condition number from Lemma 5.3. Then lift(T, ğ‘›) in Algorithm 2 runs in time
Ëœğ‘‚(ğ‘›ğ‘‘4ğœâˆ’1/2

+ ğ‘›2ğ‘‘3ğœ…âˆ’1), and the eï¬ƒcient implementation steps are correct.

ğ‘–

ğ‘›

Proof. Step 1 of lift invokes Lemma 4.2 on a ğ‘‘2 Ã— ğ‘‘2 matrix ğ‘‡, recovering ğ‘› dimensions
with a spectral gap of ğ›¿ = ğœğ‘›. This requires time Ëœğ‘‚((ğ‘›ğ‘‘4 + ğ‘›2ğ‘‘2)ğœâˆ’1/2

).

ğ‘›

Step 2 again invokes Lemma 4.2, this time on a ğ‘‘3 Ã— ğ‘‘3 matrix Î ğ‘†âŠ—â„ğ‘‘Î symÎ ğ‘†âŠ—â„ğ‘‘,
recovering ğ‘› dimensions with a spectral gap of at least ğœ…2 âˆ’ 2ğœ€ âˆˆ Î©(ğœ…2). Multiplying by
Î ğ‘†âŠ—â„ğ‘‘ may be done in time ğ‘‚(ğ‘›ğ‘‘3) due to its expression as (Î ğ‘†âŠ—â„ğ‘‘ ğ‘£)(Â·,Â·,ğ‘–) = ğ‘„ğ‘„Tğ‘£(Â·,Â·,ğ‘–), since
the third mode of (â„ğ‘‘)âŠ—3 is unaï¬€ected by Î ğ‘†âŠ—â„ğ‘‘ = ğ‘„ğ‘„T âŠ— Id, and this is a concatenation of
ğ‘‘ diï¬€erent matrix-vector multiplies that take ğ‘‚(ğ‘›ğ‘‘2) time each. Multiplying by Î sym takes
ğ‘‚(ğ‘‘3) time, since the (ğ‘–, ğ‘—, ğ‘˜)th row of Î sym has at most 6 nonzero entries corresponding
to the diï¬€erent permutations of (ğ‘–, ğ‘—, ğ‘˜). Thus the overall time to multiply a vector by
Î ğ‘†âŠ—â„ğ‘‘Î symÎ ğ‘†âŠ—â„ğ‘‘ is ğ‘‚(ğ‘›ğ‘‘3), so that Lemma 4.2 gives a runtime of Ëœğ‘‚((ğ‘›2ğ‘‘3 + ğ‘›2ğ‘‘3)ğœ…âˆ’1) for
this step.

Step 3 is a concatenation of ğ‘‘ diï¬€erent matrix products, each of which involves
multiplying a ğ‘‘2 Ã— ğ‘› matrix ğ‘…(Â·,Â·,ğ‘–);Â· by a ğ‘› Ã— ğ‘‘2 matrix Î›1/2ğ‘„T and then multiplying the
resulting ğ‘› Ã— ğ‘› matrix by a ğ‘‘2 Ã— ğ‘› matrix ğ‘„. Each product thus takes ğ‘‚(ğ‘›2ğ‘‘2) time,
and since there are ğ‘‘ of them the entire step takes ğ‘‚(ğ‘›2ğ‘‘3) time. The result is equal
to (ğ‘„Î›1/2ğ‘„T âŠ— Idğ‘‘)ğ‘… = (ğ‘Š âŠ— Id)ğ‘…, whose columns form a basis for the columnspace of
ğ‘€3 = (ğ‘Š âŠ— Id)ğ‘…Î£ğ‘…T(ğ‘Š âŠ— Id).

Step 4 applies Gram-Schmidt orthonormalization on ğ‘› vectors in â„ğ‘‘3, taking Ëœğ‘‚(ğ‘›2ğ‘‘3)
+ ğ‘›2ğ‘‘3ğœ…âˆ’1). (cid:3)

time. And step 5 takes constant time. Therefore, lift takes time Ëœğ‘‚(ğ‘›ğ‘‘4ğœâˆ’1/2

ğ‘›

5.1 Algebraic identiï¬ability argument

The main lemma in this section gives a more careful analysis of the algebraic identiï¬ability
argument from Lemma 2.1, in order to obtain a quantitative condition number bound.

Lemma 5.3 (Main Identiï¬ability Lemma). Let ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘ with ğ‘› (cid:54) ğ‘‘2. Let ğ‘† denote
Span({ğ‘âŠ—2
}) and assume both have dimension ğ‘›. For each ğ‘–, let
{ğ‘ğ‘–,ğ‘— } ğ‘—âˆˆ[ğ‘‘âˆ’1] be an arbitrary orthonormal basis for vectors in â„ğ‘‘ orthogonal to ğ‘ğ‘–, and let

}) and let ğ‘†3 denote Span({ğ‘âŠ—3

ğ‘–

ğ‘–

ğ»T :=

ğ‘1 âŠ— ğ‘1 âŠ— ğ‘1,1
...
ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— ğ‘ğ‘–,ğ‘—
...
ğ‘ğ‘› âŠ— ğ‘ğ‘› âŠ— ğ‘ğ‘›,ğ‘‘âˆ’1

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

.

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

19

Let ğ‘… = (ğ»ğ»T)âˆ’1/2ğ» be a column-wise orthonormalization of ğ», and let ğ¾ = 1
2(Id âˆ’ ğ‘ƒ2,3)ğ‘…,
where ğ‘ƒ2,3 is the permutation matrix that exchanges the 2nd and 3rd modes of (â„ğ‘‘)âŠ—3. Then if
ğœ… = ğœmin(ğ¾) is non-zero (so that ğ¾ is full rank),

and furthermore,

(ğ‘† âŠ— â„ğ‘‘) âˆ© sym = ğ‘†3 ,

(cid:107)Î ğ‘†âŠ—â„ğ‘‘Î symÎ ğ‘†âŠ—â„ğ‘‘ âˆ’ Î ğ‘†3 (cid:107) (cid:54) 1 âˆ’ ğœ…2 .
Proof. Let ğ‘Š = ((cid:205)ğ‘– ğ‘âŠ—2
T)âˆ’1/2 and let ğ‘‡ denote the columnspace of (ğ‘Š 2 âŠ— Id)ğ». The
columns of ğ‘Š 2ğ» form a basis for the subspace of ğ‘† âŠ— â„ğ‘‘ orthogonal to ğ‘†3 since each column
of ğ‘Š 2ğ» is orthogonal to every ğ‘âŠ—3

. Therefore,

ğ‘âŠ—2
ğ‘–

ğ‘–

ğ‘–

Î ğ‘†âŠ—â„ğ‘‘ = Î ğ‘†3 + Î ğ‘‡ .

Multiplying this with Î sym and itself and then applying the identities Î symÎ ğ‘†3 = Î ğ‘†3Î sym =
Î ğ‘†3 and Î ğ‘†3Î ğ‘‡ = Î ğ‘‡Î ğ‘†3 = 0,

Therefore,

Î ğ‘†âŠ—â„ğ‘‘Î symÎ ğ‘†âŠ—â„ğ‘‘ = Î ğ‘†3 + Î ğ‘‡Î symÎ ğ‘‡ .

(cid:107)Î ğ‘†âŠ—â„ğ‘‘Î symÎ ğ‘†âŠ—â„ğ‘‘ âˆ’ Î ğ‘†3 (cid:107) (cid:54) (cid:107)Î symÎ ğ‘‡ (cid:107)2 .

We would thus like to show that (cid:107)Î symÎ ğ‘‡ (cid:107)2 (cid:54) 1 âˆ’ ğœ…2.

Since (cid:107)Î symÎ ğ‘‡ (cid:107)2 = maxğ‘¦(cid:48)âˆˆğ‘‡ (cid:107)Î symğ‘¦(cid:48)(cid:107)2/(cid:107)ğ‘¦(cid:48)(cid:107)2 = 1 âˆ’ minğ‘¦(cid:48)âˆˆğ‘‡ (cid:107)(Id âˆ’ Î sym)ğ‘¦(cid:48)(cid:107)2/(cid:107) ğ‘¦(cid:48)(cid:107)2, it
is enough to show that minğ‘¦(cid:48)âˆˆğ‘‡ (cid:107)(Id âˆ’ Î sym)ğ‘¦(cid:48)(cid:107)/(cid:107)ğ‘¦(cid:48)(cid:107) (cid:62) ğœ…. By Lemma 5.4, that is implied
by (cid:107)(Id âˆ’ Î sym)ğ‘¦(cid:107)/(cid:107) ğ‘¦(cid:107) (cid:62) ğœ… for ğ‘¦ âˆˆ img(ğ»).

Since Î sym (cid:22) Î 2,3 where Î 2,3 is the projector to the space invariant under interchange
2(Id + ğ‘ƒ2,3), we see that (cid:107)(Id âˆ’
2(Id âˆ’ ğ‘ƒ2,3)ğ‘¦(cid:107)/(cid:107)ğ‘¦ (cid:107) for ğ‘¦ âˆˆ img(ğ»). Since the columns of ğ‘… are an

of the second and third modes of (â„ğ‘‘)âŠ—3 and Î 2,3 = 1
Î sym)ğ‘¦(cid:107)/(cid:107) ğ‘¦(cid:107) (cid:62) (cid:107) 1
orthonormal basis for img(ğ»), for ğ‘¥ = ğ‘…âˆ’1ğ‘¦ spanning all of â„ğ‘› we have

(cid:107)(Id âˆ’ ğ‘ƒ2,3)ğ‘¦(cid:107)
2(cid:107) ğ‘¦(cid:107)

=

(cid:107)(Id âˆ’ ğ‘ƒ2,3)ğ‘…ğ‘¥ (cid:107)
2(cid:107)ğ‘…ğ‘¥ (cid:107)

=

(cid:107)(Id âˆ’ ğ‘ƒ2,3)ğ‘…ğ‘¥(cid:107)
2(cid:107)ğ‘¥(cid:107)

=

(cid:107)ğ¾ğ‘¥(cid:107)
(cid:107)ğ‘¥(cid:107)

.

The expression on the right is the deï¬nition of ğœ…. Therefore, (cid:107)(Id âˆ’ Î sym)ğ‘¦(cid:107)/(cid:107) ğ‘¦(cid:107) (cid:62)
(cid:107) 1
(cid:3)
2(Id âˆ’ ğ‘ƒ2,3)ğ‘¦(cid:107)/(cid:107)ğ‘¦ (cid:107) = ğœ….

Lemma 5.4. For each ğ‘–, let {ğ‘ğ‘–,ğ‘— } ğ‘—âˆˆ[ğ‘‘âˆ’1] be an arbitrary orthonormal basis for vectors in â„ğ‘‘

20

orthogonal to ğ‘ğ‘–, and let

ğ»T :=

ğ‘1 âŠ— ğ‘1 âŠ— ğ‘1,1
...
ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— ğ‘ğ‘–,ğ‘—
...
ğ‘ğ‘› âŠ— ğ‘ğ‘› âŠ— ğ‘ğ‘›,ğ‘‘âˆ’1

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

.

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

Let ğ»(cid:48) = (ğ‘Š 2âŠ—Id)ğ». If (cid:107)(Idâˆ’Î sym)ğ‘¦(cid:107) (cid:62) ğ‘¡ (cid:107) ğ‘¦(cid:107) for all ğ‘¦ âˆˆ img(ğ»), then (cid:107)(Idâˆ’Î sym)ğ‘¦(cid:48)(cid:107) (cid:62) ğ‘¡ (cid:107)ğ‘¦(cid:48)(cid:107)
for all ğ‘¦(cid:48) âˆˆ img(ğ»(cid:48)).

) and ğ‘†3 = Span(ğ‘âŠ—3

Proof. Let ğ‘† = Span(ğ‘âŠ—2
) âŠ† sym. Observe that img(ğ»(cid:48)) âŠ† ğ‘† âŠ— â„ğ‘‘ =
img(ğ») + ğ‘†3. Therefore, for every ğ‘¦(cid:48) âˆˆ img(ğ»(cid:48)) there will be some ğ‘¦ âˆˆ img(ğ») and some
ğ‘§ âˆˆ ğ‘†3 such that ğ‘¦(cid:48) = ğ‘¦ + ğ‘§. Also, since ğ‘†3 âŠ¥ img(ğ»(cid:48)), we have ğ‘§ âŠ¥ ğ‘¦(cid:48), and therefore
(cid:107)ğ‘¦(cid:107) = (cid:107)ğ‘¦(cid:48) âˆ’ ğ‘§ (cid:107) (cid:62) (cid:107) ğ‘¦(cid:48)(cid:107).

ğ‘–

ğ‘–

So if the premise of the lemma holds and (cid:107)(Id âˆ’ Î sym)ğ‘¦(cid:107) (cid:62) ğ‘¡ (cid:107) ğ‘¦(cid:107) for all ğ‘¦ âˆˆ img(ğ»), it
(cid:3)

will also be the case that (cid:107)(Id âˆ’ Î sym)ğ‘¦(cid:48)(cid:107) = (cid:107)(Id âˆ’ Î sym)(ğ‘¦ + ğ‘§)(cid:107) (cid:62) ğ‘¡ (cid:107) ğ‘¦(cid:107) (cid:62) ğ‘¡ (cid:107)ğ‘¦(cid:48)(cid:107).

5.2 Robustness arguments

The main lemma of this section gives all of the spectral eigenspace perturbation arguments
needed to argue the correctness and robustness of Algorithm 2. Here we essentially
repeatedly apply Davis and Kahanâ€™s sin-Î˜ theorem (Theorem 4.1) through a sequence
of linear algebraic transformations, along with triangle inequality and some adding-
and-subtracting, to argue that the desired top eigenspace remains stable against the
spectral-norm errors melded in at each step.

Lemma 5.5 (Subspace perturbation for lift). Let ğ‘‡ = (cid:205)ğ‘–âˆˆ[ğ‘›] ğ‘âŠ—2
ğ‘–
with (cid:107)ğ‘‡ âˆ’ (cid:101)ğ‘‡ (cid:107) (cid:54) ğœ€ ğœ2
the operator norm of (cid:205) (cid:107)ğ‘ğ‘– (cid:107)âˆ’2ğ‘âŠ—3
ğ‘† = Span({ğ‘âŠ—2

T and let (cid:101)ğ‘‡ be a matrix
ğ‘›ğœ‡âˆ’1ğœ…2 for some ğœ€ < 1/63, where ğœğ‘› is the ğ‘›th eigenvalue of ğ‘‡ and ğœ‡ is
T and ğœ… is the condition number from Lemma 5.3. Let

}). Then

ğ‘âŠ—3
ğ‘–

ğ‘âŠ—2
ğ‘–

ğ‘–

ğ‘–

}) = img(ğ‘‡) and let (cid:101)ğ‘† = img((cid:101)ğ‘‡). Also let ğ‘†3 = Span({ğ‘âŠ—3
(cid:101)ğ‘†âŠ—â„ğ‘‘ (cid:1) âˆ’ Î ğ‘†3 (cid:107) (cid:54) 18 ğœ€ ğœğ‘›ğœ‡âˆ’1 ,
where topğ‘› denotes the top-ğ‘› eigenspace. Furthermore, letting (cid:101)ğ‘†3 = topğ‘›(Î 
ğ‘Š = ğ‘‡âˆ’1/2 and (cid:101)ğ‘Š = (cid:101)ğ‘‡âˆ’1/2, we have

(cid:101)ğ‘†âŠ—â„ğ‘‘Î symÎ 

(cid:107) topğ‘›

(cid:0)Î 

ğ‘–

(cid:101)ğ‘†âŠ—â„ğ‘‘Î symÎ 

(cid:101)ğ‘†âŠ—â„ğ‘‘) and

(cid:107)Î ( (cid:101)ğ‘Š âŠ—Id)(cid:101)ğ‘†3

âˆ’ Î (ğ‘Š âŠ—Id)ğ‘†3

(cid:107) (cid:54) 63 ğœ€ .

Proof. For brevity, let Î  = Î ğ‘†âŠ—â„ğ‘‘ and let (cid:101)Î  = Î 

(cid:101)Î  Î sym (cid:101)Î  âˆ’ Î  Î sym Î  = (cid:0)

(cid:101)Î  âˆ’ Î (cid:1) .

(5.1)

(cid:101)ğ‘†âŠ—â„ğ‘‘. We write
(cid:101)Î  âˆ’ Î (cid:1) Î sym (cid:101)Î  + Î  Î sym (cid:0)

21

Since (cid:107)ğ‘‡ âˆ’ (cid:101)ğ‘‡ (cid:107) (cid:54) ğœ€ ğœ2
projectors donâ€™t increase spectral norm, we conclude

ğ‘›ğœ‡âˆ’1ğœ…2, by Theorem 4.1, (cid:107) (cid:101)Î  âˆ’ Î (cid:107) = (cid:107)Î 

(cid:101)ğ‘† âˆ’ Î ğ‘† (cid:107) (cid:54) 3ğœ€ğœğ‘›ğœ‡âˆ’1ğœ…2. Since

(cid:107) (cid:101)Î  Î sym (cid:101)Î  âˆ’ Î  Î sym Î (cid:107) (cid:54) 6ğœ€ğœğ‘›ğœ‡âˆ’1ğœ…2 .

Furthermore, by Lemma 5.3, Î  Î sym Î  = Î ğ‘†3 + ğ‘, where ğ‘ is a symmetric matrix with
(cid:107)ğ‘(cid:107) (cid:54) 1 âˆ’ ğœ…2 whose columnspace is orthogonal to ğ‘†3 since Î ğ‘†3Î ğ‘†âŠ—â„ğ‘‘Î symÎ ğ‘†âŠ—â„ğ‘‘ = Î ğ‘†3.
Therefore,

(cid:107) (cid:101)Î Î sym(cid:101)Î  âˆ’ (Î ğ‘†3 + ğ‘)(cid:107) (cid:54) 6ğœ€ğœğ‘›ğœ‡âˆ’1ğœ…2 .
The top-ğ‘› eigenspace of (Î ğ‘†3 + ğ‘) is ğ‘†3 and the ğ‘›th and (ğ‘› + 1)th eigenvalues of (Î ğ‘†3 + ğ‘)
diï¬€er by at least ğœ…2. So by Theorem 4.1,

(cid:107) topğ‘›((cid:101)Î Î sym(cid:101)Î ) âˆ’ Î ğ‘†3 (cid:107) (cid:54) 18ğœ€ğœğ‘›ğœ‡âˆ’1 .

Multiplying by ğ‘Š multiplies this error by at most a factor of (cid:107)ğ‘Š (cid:107)2 = ğœâˆ’1

ğ‘› , so that

(cid:107)(ğ‘Š âŠ— Id)Î 

(cid:101)ğ‘†3

(ğ‘Š âŠ— Id) âˆ’ (ğ‘Š âŠ— Id)Î ğ‘†3(ğ‘Š âŠ— Id)(cid:107) (cid:54) 18 ğœ€ğœ‡âˆ’1 .

And (cid:107)( (cid:101)ğ‘Š âŠ— Id)Î 
(5.1) since Î 

(cid:101)ğ‘†3

(cid:101)ğ‘†3

( (cid:101)ğ‘Š âŠ— Id) âˆ’ (ğ‘Š âŠ— Id)Î 

(cid:101)ğ‘†3

(ğ‘Š âŠ— Id) has a spectral norm at most ğœâˆ’1/2

, so that

ğ‘›

(ğ‘Š âŠ— Id)(cid:107) (cid:54) 3ğœ€ by a decomposition similar to

(cid:107)( (cid:101)ğ‘Š âŠ— Id)Î 

(cid:101)ğ‘†3

( (cid:101)ğ‘Š âŠ— Id) âˆ’ (ğ‘Š âŠ— Id)Î ğ‘†3(ğ‘Š âŠ— Id)(cid:107) (cid:54) 21 ğœ€ğœ‡âˆ’1 .

By Lemma 5.6, the smallest eigenvalue of (ğ‘Š âŠ— Id)Î ğ‘†3(ğ‘Š âŠ— Id) is at least ğœ‡âˆ’1. Therefore,
(cid:3)
(cid:107) (cid:54) 63 ğœ€.
by Theorem 4.1, (cid:107)Î (ğ‘Š âŠ—Id)(cid:101)ğ‘†3

âˆ’ Î (ğ‘Š âŠ—Id)ğ‘†3

ğ‘âŠ—3
ğ‘–

ğ‘–

The following utility lemma is used to reduce the impact of condition numbers on
the algorithm. It shows that when multiplying a third-order tensor in the span of ğ‘âŠ—3
by the second-order whitener ğ‘Š = ğ‘‡âˆ’1/2 = ((cid:205)ğ‘– ğ‘âŠ—2
T)âˆ’1/2, the penalty to the error
may be expressed in terms of a sixth-order condition number â€“ the spectral norm of
ğ‘ˆ = (cid:205) (cid:107)ğ‘ğ‘– (cid:107)âˆ’2ğ‘âŠ—3

T â€“ instead of the fourth-order one given by ğ‘‡.

ğ‘âŠ—2
ğ‘–

ğ‘–

ğ‘–

ğ‘âŠ—2
ğ‘–

The reason this is important is that (cid:205)ğ‘– ğ‘âŠ—2

T suï¬€ers from spurious directions: direc-
ğ‘–
tions ğ‘£ âˆˆ â„âŠ—2 in which ğ‘‡ğ‘£ may be very large, but ğ‘£ is not close to any of the ğ‘ğ‘– âŠ— ğ‘ğ‘–, or in
fact any rank-1 2-tensor at all. For example, for ğ‘› random Gaussian vectors, the spurious
direction is given by Î¦ = ğ”¼ğ‘”âˆ¼â„•(0,1) ğ‘” âŠ— ğ‘”, which will have (cid:107)ğ‘‡Î¦(cid:107) â‰ˆ ğ‘›/ğ‘‘.

The sixth-order object ğ‘ˆ = (cid:205) (cid:107)ğ‘ğ‘– (cid:107)âˆ’2ğ‘âŠ—3

ğ‘âŠ—3
T does not suï¬€er with this problem for ğ‘› up to
ğ‘–
Ëœğ‘‚(ğ‘›2), due to cancellation with the odd number of modes. For instance, ğ‘ˆ ğ”¼ğ‘”âˆ¼â„•(0,1) ğ‘”âŠ—3 = 0
and (cid:107)ğ‘ˆ(Î¦ âŠ— ğ‘¢)(cid:107) â‰ˆ ğ‘›/ğ‘‘2 for all unit ğ‘¢ âˆˆ â„ğ‘‘ and ğ‘ˆ generated from random Gaussian vectors.
Lemma 5.6 (Sixth-order condition numbers). Let ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘ with ğ‘› (cid:54) ğ‘‘2. Let ğ‘Š =
((cid:205)ğ‘– ğ‘âŠ—2
T. Then for a vector ğ‘£ âˆˆ

T)âˆ’1/2 have rank ğ‘›. Let ğ‘ˆ be the matrix (cid:205) (cid:107)ğ‘ğ‘– (cid:107)âˆ’2ğ‘âŠ—3

ğ‘–

ğ‘âŠ—2
ğ‘–

ğ‘–

ğ‘âŠ—3
ğ‘–

ğ‘–

22

Span(ğ‘âŠ—3

ğ‘–

), the following hold:

(cid:107)(ğ‘Š âŠ— Id)ğ‘£(cid:107) (cid:54) (cid:107)ğ‘ˆ âˆ’1(cid:107)1/2(cid:107)ğ‘£(cid:107) ,

(cid:107)(ğ‘Š âŠ— Id)ğ‘£(cid:107) (cid:62) (cid:107)ğ‘ˆ (cid:107)âˆ’1/2(cid:107)ğ‘£(cid:107) .

Proof. Let ğ‘£ = (cid:205) ğœ‡ğ‘– (cid:107)ğ‘ğ‘– (cid:107)âˆ’1ğ‘âŠ—3

ğ‘–

. Then

(cid:107)(ğ‘Š âŠ— Id)ğ‘£(cid:107)2 =

(cid:213) ğœ‡ğ‘–ğœ‡ğ‘— (cid:107)ğ‘ğ‘– (cid:107)âˆ’1(cid:107)ğ‘ ğ‘— (cid:107)âˆ’1(cid:104)ğ‘Š(ğ‘ ğ‘— âŠ— ğ‘ ğ‘—), ğ‘Š(ğ‘ğ‘– âŠ— ğ‘ğ‘–)(cid:105)(cid:104)ğ‘ ğ‘— , ğ‘ğ‘–(cid:105) =

(cid:213) ğœ‡2
ğ‘– ,

using the fact that {ğ‘Š(ğ‘ğ‘– âŠ— ğ‘ğ‘–)}ğ‘– is an orthonormal set of vectors.

(cid:3)

6 Rounding

In this section, we show how to â€œroundâ€ the lifted tensor to extract the components. That
is, assuming we are given the tensor

ğ‘‡ =

(cid:213)

ğ‘–âˆˆ[ğ‘›]

(ğ‘Š ğ‘âŠ—2

ğ‘–

)âŠ—3 + ğ¸

where ğ¸ is a tensor of Frobenius norm at most ğœ€
ğ‘ğ‘–.

âˆš

ğ‘›, we show how to ï¬nd the components

Lemma 6.1. Suppose ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘ are unit vectors satisfying the identiï¬ability assumption
from Lemma 5.1, and suppose we are given an implicit rank-ğ‘› representation of the tensor
ğ‘‡ = (cid:205)ğ‘–(ğ‘Š ğ‘âŠ—2
ğ‘›, and an implicit rank-ğ‘› representation of a
matrix Î 3 such that (cid:107)Î 3 âˆ’ (cid:205)ğ‘–((ğ‘Š ğ‘âŠ—2
) âŠ— ğ‘ğ‘–)(cid:62)(cid:107) (cid:54) ğœ€ < 1
2.

ğ‘–
Then for any ğ›½, ğ›¿ âˆˆ (0, 1) so that ğ›½ğ›¿ = Î©(ğœ€) and ğ›¿ = Î©(ğœ€), there is a randomized algorithm
ğ›½ ğ‘›1+ğ‘‚(ğ›½)ğ‘‘3) with Ëœğ‘‚(ğ‘›2ğ‘‘3) preprocessing time recovers a unit

)âŠ—3 + ğ¸ âˆˆ (â„ğ‘‘2)âŠ—3, where (cid:107)ğ¸(cid:107)ğ¹ (cid:54) ğœ€

) âŠ— ğ‘ğ‘–)((ğ‘Š ğ‘âŠ—2

âˆš

ğ‘–

ğ‘–

that with high probability in time ğ‘‚( 1
vector ğ‘¢ such that for some ğ‘– âˆˆ [ğ‘›],

(cid:104)ğ‘ğ‘– , ğ‘¢(cid:105)2 (cid:62) 1 âˆ’ (cid:107)ğ‘Š (cid:107) Â· ğ‘‚

(cid:19) 1/8

,

(cid:18) ğœ€
ğ›½

so long as (cid:107)ğ‘Š (cid:107)

(cid:17) 1/8

(cid:16) ğœ€
ğ›½

< ğ¶ for a universal constant ğ¶.

Further, there is an integer ğ‘š (cid:62) (1 âˆ’ ğ›¿)ğ‘› so that repeating the above algorithm Ëœğ‘‚(ğ‘›) times

recovers unit vectors ğ‘¢1, . . . , ğ‘¢ğ‘š so that (cid:104)ğ‘¢ğ‘– , ğ‘ğ‘–(cid:105)2 (cid:62) 1 âˆ’ (cid:107)ğ‘Š (cid:107) Â· ğ‘‚

(cid:17) 1/8

(cid:16) ğœ€
ğ›¿ğ›½

for all ğ‘– âˆˆ [ğ‘š] (up to

re-indexing), again so long as (cid:107)ğ‘Š (cid:107)

(cid:17) 1/8

(cid:16) ğœ€
ğ›¿ğ›½

< ğ¶, and with a total runtime of Ëœğ‘‚( 1

ğ›½ ğ‘›2+ğ‘‚(ğ›½)ğ‘‘3).

23

We will prove this theorem in four steps. First, in Section 6.1 we will show how
to recover vectors that are (with reasonable probability) correlated with the whitened
Kronecker squares of the components, ğ‘Š ğ‘âŠ—2
. In Section 6.2, weâ€™ll give an algorithm that
given a vector close to the whitened square ğ‘Š ğ‘âŠ—2
, recovers a vector close to the component
ğ‘ğ‘–. In Section 6.3, we give an algorithm that tests if a vector ğ‘ âˆˆ â„ğ‘‘ is close to one of the
components {ğ‘ğ‘– }ğ‘–âˆˆ[ğ‘›]. In these ï¬rst three sections, we omit runtime details; in Section 6.4
we put the arguments together and address runtime details as well.

ğ‘–

ğ‘–

6.1 Recovering candidate whitened and squared components

ğ‘–

Here, we give an algorithm for recovering components that have constant correlation with
the ğ‘Š ğ‘âŠ—2
. In this subsection, our result applies in generality to arbitrary orthonormal
vectors ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘2. The algorithm and its analysis follow almost directly from [SS17];
for completeness we re-state the important lemmas here, and detail what little adaptation
is necessary.

Algorithm 3 Rounding to a whitened component
Function round(ğ‘‡, ğ›½, ğœ€):
Input: a tensor ğ‘‡ âˆˆ (â„ğ‘‘2)âŠ—3, a spectral gap bound ğ›½, and an error tolerance ğœ€.

1. Decrease the spectral norm of the error term in rectangular reshapings:

(a) compute ğ‘‡(cid:48), the projection of ğ‘‡{1,2}{3} to ğ‘‚, the set of ğ‘‘4 Ã— ğ‘‘2 matrices with

spectral norm at most 1

(b) compute ğ‘‡ (cid:54)1, the projection of ğ‘‡(cid:48)

{1,3}{2}

norm error).

to ğ‘‚ (may be done up to ğœ€

âˆš

ğ‘› Frobenius

2. Compute a random ï¬‚attening of ğ‘‡ (cid:54)1 along the {1} mode: for ğ‘” âˆ¼ ğ’©(0, Idğ‘‘2), compute

ğ‘‡(ğ‘”) =

(cid:213)

ğ‘–âˆˆ[ğ‘‘2]

ğ‘”ğ‘– Â· ğ‘‡(ğ‘–, Â·, Â·).

3. Recover candidate component vectors: compute ğ‘¢ğ¿(ğ‘”) and ğ‘¢ğ‘…(ğ‘”), the top left- and

right-singular vectors of ğ‘‡(ğ‘”) using ğ‘‚( 1

ğ›½ log ğ‘‘) steps of power iteration.

Output: the candidate components ğ‘¢ğ¿(ğ‘”) and ğ‘¢ğ‘…(ğ‘”).

Lemma 6.2. Suppose that ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘2 are orthonormal. Then if ğ‘‡ = (cid:205)ğ‘–âˆˆ[ğ‘›] ğ‘âŠ—3
ğ‘–
tensor ğ¸ with (cid:107)ğ¸(cid:107)ğ¹ (cid:54) ğœ€
Ëœğ‘‚(ğ‘›ğ‘‚(ğ›½)) times will with high probability recover a unit vector ğ‘¢ such that (cid:104)ğ‘¢, ğ‘ğ‘–(cid:105)2 (cid:62) 1 âˆ’ ğœ€

+ ğ¸ for a
ğ›¿ ) (cid:54) ğ›½ < 1, repeating steps 2 & 3 of Algorithm 3
ğ›¿ğ›½ for

ğ‘› and ğ›¿ = Î©(ğœ€), Î©( ğœ€

âˆš

24

some ğ‘– âˆˆ [ğ‘›]. Furthermore, repeating steps 2 & 3 of Algorithm 3 Ëœğ‘‚(ğ‘›1+ğ‘‚(ğ›½)) times will with high
probability recover ğ‘š (cid:62) (1 âˆ’ ğ›¿) Â· ğ‘› unit vectors ğ‘¢1, . . . , ğ‘¢ğ‘š such that for each ğ‘¢ğ‘– there exists ğ‘— âˆˆ [ğ‘›]
so that (cid:104)ğ‘¢ğ‘– , ğ‘ ğ‘—(cid:105)2 (cid:62) 1 âˆ’ ğœ€

ğ›¿ğ›½ .9

The proof follows from two lemmas:

Lemma 6.3. The tensor ğ‘‡ (cid:54)1 computed in step 1 of Algorithm 3 remains close to ğ‘† = (cid:205)ğ‘– ğ‘âŠ—3
ğ‘–
Frobenius norm, (cid:107)ğ‘‡ (cid:54)1 âˆ’ ğ‘†(cid:107)ğ¹ (cid:54) ğœ€

ğ‘›, and furthermore

âˆš

in

(cid:107)ğ‘‡ (cid:54)1

{1,2}{3}

(cid:107) (cid:54) 1

and

(cid:107)ğ‘‡ (cid:54)1

{1,3}{2}

(cid:107) (cid:54) 1.

The proof of Lemma 6.3 is identical to the proof of [SS17, Lemma 4.5], and uses the
fact that distances decrease under projection to convex sets to control the error, and the
fact that the truncation operation is equivalent to multiplication by a contractive matrix to
argue that ğ‘‡ (cid:54)1 has bounded norm in both reshapings.

âˆš

(cid:107)ğ¹ (cid:54) ğœ€

Lemma 6.4. Suppose that in spectral norm (cid:107)ğ‘‡{1,2}{3} (cid:107), (cid:107)ğ‘‡{1,3}{2} (cid:107) (cid:54) 1, and also that (cid:107)ğ‘‡ âˆ’
(cid:205)ğ‘– ğ‘âŠ—3
ğ‘›. Let ğ‘‡(ğ‘”) be the random ï¬‚attening of ğ‘‡ produced in step 2 of Algorithm 3, and
ğ‘–
let ğ‘¢ğ¿(ğ‘”) and ğ‘¢ğ‘…(ğ‘”) be the top left- and right-signular vectors of ğ‘‡(ğ‘”) respectively. Then there is a
universal constant ğ¶ such that for any ğ›¿ > ğ¶ Â· ğœ€ and Î©( ğœ€
ğ›¿ ) (cid:54) ğ›½ < 1, for a 1 âˆ’ ğ›¿ fraction of ğ‘— âˆˆ [ğ‘›],

(cid:18)

â„™
ğ‘”âˆ¼ğ‘(0,Id)

(cid:104)ğ‘¢ğ¿(ğ‘”), ğ‘ ğ‘—(cid:105)2 (cid:62) 1 âˆ’

ğœ€
ğ›¿ğ›½

or

(cid:104)ğ‘¢ğ‘…(ğ‘”), ğ‘ ğ‘—(cid:105)2 (cid:62) 1 âˆ’

(cid:19)

ğœ€
ğ›¿ğ›½

(cid:62) ËœÎ©

(cid:16)

ğ‘›âˆ’1âˆ’ğ‘‚(ğ›½)(cid:17)

,

and further when this event occurs the ratio of the ï¬rst and second singular values of ğ‘‡(ğ‘”) is lower
bounded by ğ›½, ğœ1(ğ‘‡(ğ‘”))
ğœ2(ğ‘‡(ğ‘”))

(cid:62) 1 + ğ›½.

âˆš

Proof. By assumption, ğ‘‡ (cid:54)1 = ğ‘† + ğ¸ for ğ‘† = (cid:205)ğ‘–âˆˆ[ğ‘›] ğ‘âŠ—3
at most ğœ€
have

, and ğ¸ is a tensor of Frobenius norm
ğ‘–
ğ‘› and spectral norms (cid:107)ğ¸{1,2}{3} (cid:107) (cid:54) 1 and (cid:107)ğ¸{1,3}{2} (cid:107) (cid:54) 1. For ğ‘” âˆ¼ ğ’©(0, Î£âˆ’1), we

ğ‘‡(ğ‘”) = ğ‘†(ğ‘”) + ğ¸(ğ‘”) = (cid:169)
(cid:173)
(cid:171)

(cid:213)

ğ‘˜âˆˆ[ğ‘›]

(cid:104)ğ‘”, ğ‘ğ‘˜(cid:105) Â· ğ‘ğ‘˜ğ‘(cid:62)
ğ‘˜ (cid:170)
(cid:174)
(cid:172)

+ (cid:169)
(cid:173)
(cid:171)

(cid:213)

ğ‘–âˆˆ[ğ‘‘2]

,

ğ‘”ğ‘– Â· ğ¸ğ‘–(cid:170)
(cid:174)
(cid:172)

where we use ğ¸ğ‘– = ğ¸(ğ‘–, Â·, Â·) to refer to the ğ‘‘2 Ã— ğ‘‘2 matrix given by taking the {2}, {3}
ï¬‚attening of ğ¸ restricted to coordinate ğ‘– in mode 1.

The proof of the lemma is now identical to that of [SS17, Lemma 4.6 and Lemma 4.7].
There are two primary diï¬€erences: the ï¬rst is that in [SS17] the tensor has four modes, and
our tensor eï¬€ectively has 3 modes. This diï¬€erence is negligible, since in [SS17], two of the
four modes are always identiï¬ed anyway.

9In particular, if we choose ğ›¿ = log log ğ‘› Â· ğœ€, we will will recover all but ğœ€ Â· ğ‘› log log ğ‘› of the ğ‘ğ‘– in Ëœğ‘‚(ğ‘›)

repetitions.

25

The second diï¬€erence is that we choose parameters diï¬€erently. We take the parameter
ğ›¿ )10; this is to emphasize that for small
ğ‘› , one can recover all ğ‘š = ğ‘› of the components. Because the proof is otherwise the

ğ›½ appearing in [SS17, Lemma 4.6] so that ğ›½ = Î©( ğœ€
ğœ€ (cid:28) 1
same, we merely sketch an overview here.

The ï¬rst term in isolation is a random ï¬‚attening of an orthogonal tensor, and so with
probability 1 the eigenvectors of the ï¬rst term are precisely the ğ‘ğ‘˜. The second term, which
is the ï¬‚attening of the noise term, introduces complications; however, the combination of
the spectral norm bound and the Frobenius norm bound on ğ¸ is enough to argue (using a
matrix Bernstein inequality, Markovâ€™s inequality and the orthogonality of the ğ‘ğ‘–) that the
random ï¬‚attening of ğ¸ cannot have spectral norm larger than ğœ€/ğ›¿ in more than 1 âˆ’ ğ›¿ of the
ğ‘ğ‘˜â€™s directions.

To ï¬nish the proof, we perform a large deviation analysis on the coeï¬ƒcients (cid:104)ğ‘”, ğ‘ğ‘˜(cid:105),
lower bounding the probability that for the 1 âˆ’ ğ›¿ fraction of the ğ‘ğ‘˜ that are not too aligned
with the spectrum of ğ¸, there is a suï¬ƒciently large gap between (cid:104)ğ‘”, ğ‘ğ‘˜(cid:105) and the (cid:104)ğ‘”, ğ‘ğ‘–(cid:105) for
ğ‘– â‰  ğ‘˜ so that ğ‘ğ‘˜ is correlated with the top singular vectors of ğ‘‡(ğ‘”).11 The bound on the ratio
(cid:3)
of the singular values comes from [SS17, Lemma 4.7] as well.

Proof of Lemma 6.2. The proof simply follows by applying Lemma 6.3, then Lemma 6.4. (cid:3)

6.2 Extracting components from the whitened squares

We now present the following simple algorithm which recovers a vector close to ğ‘ğ‘–, given
a vector close to ğ‘Š(ğ‘âŠ—2
). For convenience we will again work with generic orthonormal
vectors ğ‘ğ‘– in place of the ğ‘Š(ğ‘âŠ—2
), and we will assume we have access to the matrix Î 3 (the
approximate projector to Span{ğ‘Š(ğ‘âŠ—2

) âŠ— ğ‘ğ‘– }) computed in Algorithm 2.

ğ‘–

ğ‘–

ğ‘–

Lemma 6.5. Suppose ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘2 are orthonormal vectors and ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘, and
Î 3 âˆˆ â„ğ‘‘3Ã—ğ‘‘3 is such that (cid:107)Î 3 âˆ’ (cid:205)ğ‘– ğ‘ğ‘–ğ‘(cid:62)
(cid:107) (cid:54) ğœ€. Then if ğ‘¢ âˆˆ â„ğ‘‘2 is a unit vector with
âŠ— ğ‘ğ‘– ğ‘(cid:62)
ğ‘–
ğ‘–
(cid:104)ğ‘¢, ğ‘ğ‘–(cid:105)2 (cid:62) 1 âˆ’ ğœƒ for ğœƒ < 1
10 , then the output ğ‘ âˆˆ â„ğ‘‘ of Algorithm 4 on ğ‘¢ has the property that
âˆš
|(cid:104)ğ‘, ğ‘ğ‘–(cid:105)| (cid:62) 1 âˆ’ 4ğœƒ1/4 âˆ’ 4
ğœ€.

Proof. Let ğ‘ƒ3 = (cid:205)ğ‘– ğ‘ğ‘–ğ‘(cid:62)
. By assmption we can write the approximate projector
ğ‘–
Î 3 = ğ‘ƒ3 + ğ¸, for a matrix ğ¸ of spectral norm (cid:107)ğ¸(cid:107) (cid:54) ğœ€. Based on these expressions we can
âˆš

âŠ— ğ‘ğ‘– ğ‘(cid:62)
ğ‘–

10We comment that the parameter ğ‘ appearing in the statement of [SS17, Lemma 4.6] is larger than
2;
this is necessary for the application of [SS17, Lemma 4.7], and is not clear from the lemma statement but is
implicit in the proof.

11We note that to obtain correlation 1 âˆ’ ğœ€

the statement of the lemma (which has assumed that 2ğœ€(1+ğ›½)
with the lower bound 0.99).

ğ›¿ğ›½ , one must directly use the proof of [SS17, Lemma 4.7], rather than
ğ›½ğ›¿ (cid:54) 0.01, and replaced the expression 1 âˆ’ 2ğœ€(1+ğ›½)
)

ğ›½ğ›¿

26

Algorithm 4 Extracting the component from the whitened square
Function extract(ğ‘¢, Î 3):
Input: a unit vector ğ‘¢ âˆˆ (â„ğ‘‘)âŠ—2 such that (cid:104)ğ‘¢, ğ‘ğ‘–(cid:105)2 (cid:62) 1 âˆ’ ğœƒ for some ğ‘– âˆˆ [ğ‘›], and a projector
Î 3 âˆˆ â„ğ‘‘3Ã—ğ‘‘3 such that (cid:107)Î 3 âˆ’ (cid:205)ğ‘– ğ‘ğ‘–ğ‘(cid:62)
ğ‘–

âŠ— ğ‘ğ‘– ğ‘(cid:62)
ğ‘–

(cid:107) (cid:54) ğœ€.

1. Compute the matrix ğ‘€ = Î 3(ğ‘¢ğ‘¢(cid:62) âŠ— Id).

2. Compute the top-left singular vector ğ‘£ of ğ‘€.

3. Taking the reshaping ğ‘‰ = ğ‘£{3}{1,2}, let ğ‘ = ğ‘‰ğ‘¢.

Output: the vector ğ‘ âˆˆ â„ğ‘‘

re-express the product,

ğ‘€ = Î 3(ğ‘¢ğ‘¢(cid:62) âŠ— Id) = ğ‘ƒ3(ğ‘¢ğ‘¢(cid:62) âŠ— Id) + ğ¸(ğ‘¢ğ‘¢(cid:62) âŠ— Id).

By assumption, the second term is a matrix of spectral norm at most (cid:107)ğ¸(cid:107) (cid:54) ğœ€.

We now consider the ï¬rst term. If ğ‘¢ = ğ‘ Â· ğ‘ğ‘– + ğ‘¤, then for the ï¬rst term we have

ğ‘ƒ3(ğ‘¢ğ‘¢(cid:62) âŠ— Id) = ğ‘ƒ3 (cid:0)ğ‘2 Â· ğ‘ğ‘–ğ‘(cid:62)

ğ‘– âŠ— Id(cid:1) + ğ‘ƒ3 (cid:0)(ğ‘ Â· ğ‘ğ‘–ğ‘¤(cid:62) + ğ‘ Â· ğ‘¤ğ‘(cid:62)

ğ‘– + ğ‘¤ğ‘¤(cid:62)) âŠ— Id(cid:1)
âˆš

The second term is again a matrix of spectral norm at most 3ğ‘ Â· (cid:107)ğ‘¤(cid:107) = 3ğ‘ Â·
term can be further simpliï¬ed as

1 âˆ’ ğ‘2. The ï¬rst

ğ‘ƒ3(ğ‘2 Â· ğ‘ğ‘–ğ‘(cid:62)

ğ‘– âŠ— Id) = ğ‘2 Â· (cid:213)

ğ‘–

(cid:104)ğ‘ğ‘– , ğ‘ğ‘–(cid:105)ğ‘ğ‘–ğ‘(cid:62)

ğ‘– âŠ— ğ‘ğ‘– ğ‘(cid:62)

ğ‘– = ğ‘2 Â· ğ‘ğ‘–ğ‘(cid:62)

ğ‘– âŠ— ğ‘ğ‘– ğ‘(cid:62)
ğ‘– ,

by the orthogonality of the ğ‘ğ‘–. This is a rank-1 matrix with singular value ğ‘2. Therefore,
ğ‘€ = ğ‘2(ğ‘ğ‘– âŠ— ğ‘ğ‘–)(ğ‘ğ‘– âŠ— ğ‘ğ‘–)(cid:62) + Ëœğ¸ where (cid:107) Ëœğ¸(cid:107) (cid:54) ğœ€ + 3ğ‘
1 âˆ’ ğ‘2. It follows from Lemma 6.6 that if
ğ‘2 (cid:107) Ëœğ¸(cid:107).
ğ‘£ is the top unit left-singular vector of ğ‘€, then (cid:104)ğ‘£, ğ‘ğ‘– âŠ— ğ‘ğ‘–(cid:105)2 (cid:62) 1 âˆ’ 2

Now, in step 3 when we re-shape ğ‘£ to a ğ‘‘ Ã— ğ‘‘2 matrix ğ‘‰ of Frobenius norm 1, because ğ‘£
ğ‘2 (cid:107) Ëœğ¸(cid:107).

+ Ëœğ‘‰ for Ëœğ‘‰ of spectral norm (cid:107) Ëœğ‘‰ (cid:107) (cid:54) (cid:107) Ëœğ‘‰ (cid:107)ğ¹ (cid:54)

(cid:113) 2

is a unit vector we have that ğ‘‰ = ğ‘ğ‘–ğ‘(cid:62)
ğ‘–
Therefore,

âˆš

ğ‘‰ğ‘¢ = (ğ‘ğ‘–ğ‘(cid:62)

ğ‘– )(ğ‘ Â· ğ‘ğ‘– + ğ‘¤) + Ëœğ‘‰ğ‘¢ = ğ‘(1 âˆ’ (cid:104)ğ‘¤, ğ‘ğ‘–(cid:105)) Â· ğ‘ğ‘– + Ëœğ‘‰ğ‘¢,
and the latter vector has norm at most (cid:107) Ëœğ‘‰ (cid:107), and (cid:104)ğ‘¤, ğ‘ğ‘–(cid:105) (cid:54) (cid:107)ğ‘¤(cid:107) (cid:54)
substituting ğ‘ =
simpliï¬cations, the conclusion follows.

1 âˆ’ ğ‘2. Finally,
1 âˆ’ ğœƒ and using our bound on (cid:107) Ëœğ¸(cid:107) and (cid:107) Ëœğ‘‰ (cid:107) and some algebraic
(cid:3)

âˆš

âˆš

Lemma 6.6. Suppose that ğ‘€ = ğ‘¢ğ‘£(cid:62) + ğ¸ for ğ‘¢ âˆˆ â„ğ‘‘, ğ‘£ âˆˆ â„ğ‘˜ unit vectors and ğ¸ âˆˆ â„ğ‘‘Ã—ğ‘˜ a
matrix of spectral norm (cid:107)ğ¸(cid:107) (cid:54) ğœ€. Then if ğ‘¥, ğ‘¦ are the top left- and right-singular vectors of ğ‘€,

27

|(cid:104)ğ‘¥, ğ‘¢(cid:105)|, |(cid:104)ğ‘¦, ğ‘£(cid:105)| (cid:62) 1 âˆ’ 2ğœ€.

Proof. Let ğ‘€ = (cid:205)ğ‘– ğœğ‘– ğ‘¥ğ‘– ğ‘¦(cid:62)
ğ‘–
We have that

be the singular value decomposition of ğ‘€, with ğœ1 (cid:62) Â· Â· Â· (cid:62) ğœğ‘‘.

On the other hand, if with (cid:104)ğ‘¥1, ğ‘¢(cid:105) = ğ›¼ (cid:54) 1 and (cid:104)ğ‘¦1, ğ‘£(cid:105) = ğ›½ (cid:54) 1,

1 âˆ’ ğœ€ (cid:54) ğ‘¢(cid:62)ğ‘€ğ‘£ (cid:54) ğœ1.

Therefore,

ğœ1 = ğ‘¥(cid:62)

1 ğ‘€ ğ‘¦1 (cid:54) ğ›¼ğ›½ + ğœ€.

|ğ›¼|, |ğ›½| (cid:62) ğ›¼ğ›½ğ‘”ğ‘’1 âˆ’ 2ğœ€,

and thus min{|ğ›¼|, |ğ›½|} (cid:62) 1 âˆ’ 2ğœ€.

(cid:3)

6.3 Testing candidate components

The following algorithm allows us to test whether a candidate component ğ‘¢ is close to
some component ğ‘ğ‘–.

Algorithm 5 Testing component membership
Function test(ğ‘¢, ğœƒ, Î ğ‘†3):
Input: A unit vector Ë†ğ‘¢, and the correlation parameter ğœƒ. Also, Î 3, an approximate projector
to Span{(ğ‘Š ğ‘âŠ—2

) âŠ— ğ‘ğ‘– }.

ğ‘–

1. Compute ğœŒ = ((ğ‘Š Ë†ğ‘¢âŠ—2) âŠ— Ë†ğ‘¢).

2. If (cid:107)Î 3ğœŒ(cid:107)2
2

< (1 âˆ’ ğœƒ)(cid:107)ğœŒ(cid:107)2
2

, return false. Otherwise, return true.

Lemma 6.7. Let ğ‘ƒ3 be the projector to Span{(ğ‘Š ğ‘âŠ—2
ğ‘–
that (cid:107)Î 3 âˆ’ ğ‘ƒ3(cid:107) (cid:54) ğœ€ < 1
for all ğ‘– âˆˆ [ğ‘›], then Algorithm 5 returns false.

) âŠ— ğ‘ğ‘– }, and suppose that we have Î 3 such
2. Then if Algorithm 5 is run on a vector Ë†ğ‘¢ such that (cid:104) Ë†ğ‘¢, ğ‘ğ‘–(cid:105)2 (cid:54) 1 âˆ’ ğœƒ âˆ’ 2ğœ€

Converseley, if Algorithm 5 is run on a vector Ë†ğ‘¢ with (cid:104) Ë†ğ‘¢, ğ‘ğ‘–(cid:105)2 (cid:62) 1 âˆ’
some ğ‘– âˆˆ [ğ‘›], then when run on a unit vector Ë†ğ‘¢, Algorithm 5 returns true.

(cid:16) ğœƒâˆ’ğœ€
10(cid:107)ğ‘Š (cid:107)

(cid:17) 2

(cid:62) 1 âˆ’ 1

10 for

Proof. By assumption, we can write Î 3 = ğ‘ƒ3 + ğ¸ for ğ‘ƒ3 the projector to Span{(ğ‘Š ğ‘âŠ—2
and ğ¸ a matrix of spectral norm at most ğœ€. From this, we have

ğ‘–

) âŠ— ğ‘ğ‘– }

Î 3(ğ‘Š Ë†ğ‘¢âŠ—2) âŠ— Ë†ğ‘¢ = ğ‘ƒ3(ğ‘Š Ë†ğ‘¢âŠ—2) âŠ— Ë†ğ‘¢ + ğ¸(ğ‘Š Ë†ğ‘¢âŠ—2) âŠ— Ë†ğ‘¢,

(6.1)

28

and (cid:107)ğ¸(ğ‘Š Ë†ğ‘¢âŠ—2) âŠ— Ë†ğ‘¢(cid:107) (cid:54) ğœ€(cid:107)ğ‘Š Ë†ğ‘¢âŠ—2(cid:107). Now, we can write ğ‘Š Ë†ğ‘¢âŠ—2 = (cid:205)ğ‘– ğ‘ğ‘–ğ‘Š(ğ‘ğ‘– âŠ— ğ‘ğ‘–) + ğ‘’, where ğ‘’
is orthogonal to Span{ğ‘Š ğ‘âŠ—2

}, and we can further write

ğ‘–

(ğ‘Š Ë†ğ‘¢âŠ—2) âŠ— Ë†ğ‘¢ =

(cid:213)

ğ‘–â‰ ğ‘—

ğ‘ğ‘– ğ›¾ğ‘— Â· (ğ‘Š ğ‘âŠ—2

ğ‘–

) âŠ— ğ‘(ğ‘–)
ğ‘—

+ (cid:213)

ğ‘–

ğ‘ğ‘– ğ›¾ğ‘– Â· (ğ‘Š ğ‘âŠ—2

ğ‘–

) âŠ— ğ‘ğ‘– + ğ‘’ âŠ— Ë†ğ‘¢,

where {ğ‘(ğ‘–)
ğ‘—
deï¬nition, ğ‘ƒ3(ğ‘Š ğ‘âŠ—2
Therefore,

ğ‘–

} ğ‘—â‰ ğ‘– is an orthogonal basis for the orthogonal complement of ğ‘ğ‘– in â„ğ‘‘. By
) âŠ— ğ‘ğ‘– }.

ğ‘— = 0, as this is orthogonal to every vector in Span{(ğ‘Š ğ‘âŠ—2

) âŠ— ğ‘(ğ‘–)

ğ‘–

ğ‘ƒ3(ğ‘Š Ë†ğ‘¢âŠ—2) âŠ— Ë†ğ‘¢ =

(cid:213)

ğ‘–

ğ‘ğ‘– ğ›¾ğ‘– Â· (ğ‘Š ğ‘âŠ—2

ğ‘–

) âŠ— ğ‘ğ‘–.

Now, if (cid:104) Ë†ğ‘¢, ğ‘ğ‘–(cid:105)2 (cid:54) ğœ for all ğ‘– âˆˆ [ğ‘›], then ğ›¾2
ğ‘–

(cid:107)ğ‘ƒ3(ğ‘Š Ë†ğ‘¢âŠ—2) âŠ— Ë†ğ‘¢ (cid:107)2 (cid:54) maxğ‘– ğ›¾2
ğ‘–
that

Â· (cid:205)ğ‘— ğ‘2
ğ‘—

(cid:54) ğœ Â· (cid:107)ğ‘Š Ë†ğ‘¢âŠ—2(cid:107)2
2

(cid:54) ğœ for all ğ‘– âˆˆ [ğ‘›]. It thus follows that
. Combining this with Eq. (6.1), we have

(cid:107)Î 3(ğ‘Š Ë†ğ‘¢âŠ—2) âŠ— Ë†ğ‘¢(cid:107)2

2 (cid:54) (ğœ + ğœ€

âˆš

ğœ + ğœ€2) Â· (cid:107)ğ‘Š Ë†ğ‘¢âŠ—2(cid:107)2

2 (cid:54) (ğœ + 2ğœ€)(cid:107)ğ‘Š Ë†ğ‘¢âŠ—2(cid:107)2
2,

for ğœ€ < 1
returns false.

2. It follows that if (cid:104) Ë†ğ‘¢, ğ‘ğ‘–(cid:105)2 = ğœ < 1 âˆ’ ğœƒ âˆ’ 2ğœ€ for all ğ‘– âˆˆ [ğ‘›], then the algorithm

Converseley, if without loss of generality Ë†ğ‘¢ = ğœ Â· ğ‘1 + Ë†ğ‘’ for Ë†ğ‘’ âˆˆ â„ğ‘‘ orthogonal to ğ‘1,
(cid:54) (1 âˆ’ ğœ2) Â· (cid:107)ğ‘Š (cid:107)2. Measuring the correlation of

then ğ‘Š Ë†ğ‘¢âŠ—2 = ğœ2 Â· ğ‘Š ğ‘âŠ—2
1
ğ‘Š Ë†ğ‘¢âŠ—2 with ğ‘Š ğ‘âŠ—2
1

+ ğ‘Š ğ‘’(cid:48) with (cid:107)ğ‘Š ğ‘’(cid:48)(cid:107)2
2
, we have that ğ‘1 (cid:62) ğœ2. Also ğ›¾1 (cid:62) ğœ, which implies

ğ‘ƒ3(ğ‘Š Ë†ğ‘¢âŠ—2) âŠ— Ë†ğ‘¢ = ğœ3 Â· (ğ‘Š ğ‘âŠ—2

1 ) âŠ— ğ‘1 + Ëœğ‘’.

where Ëœğ‘’ is a leftover term with (cid:107) Ëœğ‘’ (cid:107) (cid:54) (cid:112)1 âˆ’ ğœ2 Â· (cid:107)ğ‘Š Ë†ğ‘¢âŠ—2(cid:107) + ğœ2(cid:112)1 âˆ’ ğœ2 (where we have used
the PSDness of ğ‘Š). Combining this with Eq. (6.1),

Î 3(ğ‘Š Ë†ğ‘¢âŠ—2) âŠ— Ë†ğ‘¢ = ğœ3(ğ‘Š ğ‘âŠ—2

1 ) âŠ— ğ‘1 + Ëœğ‘’ + ğ¸(ğ‘Š Ë†ğ‘¢âŠ—2) âŠ— Ë†ğ‘¢.

For convenience let Ë†ğœŒ = Ëœğ‘’ + ğ¸(ğ‘Š Ë†ğ‘¢âŠ—2) âŠ— Ë†ğ‘¢; from our previous observations, we have
(cid:107) Ë†ğœŒ(cid:107) (cid:54) (ğœ€ + (cid:112)1 âˆ’ ğœ2)(cid:107)ğ‘Š Ë†ğ‘¢âŠ—2(cid:107) + ğœ2(cid:112)1 âˆ’ ğœ2.

Now, if (cid:104) Ë†ğ‘¢, ğ‘1(cid:105)2 = ğœ2 (cid:62) 1 âˆ’ ğœ‚, we have that

(1 âˆ’ ğœ‚) âˆ’

âˆš

ğœ‚(cid:107)ğ‘Š (cid:107) (cid:54) (cid:107)ğ‘Š Ë†ğ‘¢âŠ—2(cid:107) (cid:54) (1 âˆ’ ğœ‚) +

âˆš

ğœ‚(cid:107)ğ‘Š (cid:107).

From this,

(cid:107)Î 3(ğ‘Š Ë†ğ‘¢âŠ—2)(cid:107) (cid:62) ğœ3 âˆ’ (cid:107) Ë†ğœŒ(cid:107) (cid:62) (1 âˆ’ ğœ‚)3/2 âˆ’ (ğœ€ +

âˆš

ğœ‚)(cid:107)ğ‘Š Ë†ğ‘¢âŠ—2(cid:107) âˆ’ (1 âˆ’ ğœ‚)
âˆš

âˆš

ğœ‚(cid:107)ğ‘Š (cid:107)) âˆ’ (ğœ€ +

âˆš

ğœ‚

ğœ‚)(cid:107)ğ‘Š Ë†ğ‘¢âŠ—2(cid:107) âˆ’ (1 âˆ’ ğœ‚)

âˆš

ğœ‚

(cid:62) (cid:112)1 âˆ’ ğœ‚((cid:107)ğ‘Š Ë†ğ‘¢âŠ—2(cid:107) âˆ’

29

(cid:62) (1 âˆ’ ğœ€ âˆ’ 2
(cid:62) (1 âˆ’ ğœ€ âˆ’ 5

ğœ‚)(cid:107)ğ‘Š Ë†ğ‘¢âŠ—2(cid:107) âˆ’ 2
ğœ‚(cid:107)ğ‘Š (cid:107))(cid:107)ğ‘Š Ë†ğ‘¢âŠ—2(cid:107).

âˆš

âˆš

âˆš

ğœ‚(cid:107)ğ‘Š (cid:107),

where we have used that ğœ‚ < 1
false.

10 . Thus, if ğœ‚ <

(cid:17) 2

(cid:16) ğœƒâˆ’ğœ€
10(cid:107)ğ‘Š (cid:107)

, Algorithm 5 does not return
(cid:3)

6.4 Putting things together

Finally, we prove Lemma 6.1.

Proof of Lemma 6.1. By the assumptions of the theorem, we have access to an implicit rank-ğ‘›
)(cid:62)(cid:17) âˆ’1/2
(cid:16)(cid:205)ğ‘–âˆˆ[ğ‘›](ğ‘âŠ—2
representation of ğ‘‡ = (cid:205)ğ‘–âˆˆ[ğ‘›](ğ‘Š(ğ‘âŠ—2
ğ‘›. For convenience we denote ğ‘ğ‘– = ğ‘Š(ğ‘âŠ—2
and with (cid:107)ğ‘‡ âˆ’ ğ¸(cid:107)ğ¹ (cid:54) ğœ€
). Note that the ğ‘ğ‘– are
orthonormal vectors in â„ğ‘‘2. We also have implicit access to a rank-ğ‘› representation of Î 3,
where (cid:107)Î 3 âˆ’ (cid:205)ğ‘–(ğ‘ğ‘– âŠ— ğ‘ğ‘–)(ğ‘ğ‘– âŠ— ğ‘ğ‘–)(cid:62)(cid:107) (cid:54) ğœ€.

))âŠ—3 + ğ¸ âˆˆ (â„ğ‘‘2)âŠ—3, where ğ‘Š =

)(ğ‘âŠ—2
ğ‘–

âˆš

,

ğ‘–

ğ‘–

ğ‘–

We ï¬rst run step 1 of Algorithm 3 to produce the tensor which we will round. Then,
for â„“ = Ëœğ‘‚(ğ‘›1+ğ‘‚(ğ›½)) independent iterations, we run steps 2 & 3 of Algorithm 3 to produce
candidate whitened squares ğ‘¢1, . . . , ğ‘¢â„“ , then run Algorithm 4 on the ğ‘¢ğ‘– to produce candidate
components Ë†ğ‘¢ğ‘–, and ï¬nally run Algorithm 5 to check if Ë†ğ‘¢ğ‘– is close to ğ‘ ğ‘— for some ğ‘— âˆˆ [ğ‘›].
âˆš
We show that step 1 of Algorithm 3 takes time Ëœğ‘‚(ğ‘›2ğ‘‘3). Since ğ‘‡ is at most ğœ€
ğ‘›
in Frobenius norm away from a tensor that is a rank-ğ‘› projector in both rectangular
reshapings ğ‘‡{1,2},{3} and ğ‘‡{2,3},{1}, the (2ğ‘›)th singular values in either reshaping must be
at most ğœ€: otherwise the error term would have over ğ‘› singular values more than ğœ€
âˆš
and therefore Frobenius norm more than ğœ€
ğ‘› because it is a rank-ğ‘›
projector in its square matrix reshaping. Therefore, by Lemma 4.3, step 1 requires time
Ëœğ‘‚(ğ‘›2ğ‘‘3 + ğ‘›(ğ‘›ğ‘‘3 + ğ‘›ğ‘‘2)) to return an ğœ€
ğ‘›-approximation in Frobenius norm to the projected
matrix.12 Note that this step only needs to be carried out once regardless of how many
times the algorithm is invoked for a speciï¬c input ğ‘‡, so the Ëœğ‘‚(ğ‘›2ğ‘‘3) runtime is incurred as
a preprocessing cost.

ğ‘›. Also (cid:107)ğ‘‡ (cid:107)ğ¹ =

âˆš

âˆš

Then, again by Lemma 4.3, steps 2 & 3 require time Ëœğ‘‚( 1

ğ›½ ğ‘›ğ‘‘3), since the ratio of the ï¬rst
and second singular values of the the matrix is 1 + Î©(ğ›½), and since ğ‘‚( 1
ğ›½ log ğ‘‘) steps of power
iteration with ğ‘‡(ğ‘”) can be implemented by choosing the random direction ğ‘” âˆ¼ ğ’©(0, Idğ‘‘2),
the starting direction ğ‘£1 âˆˆ â„ğ‘‘2, and then computing ğ‘£ğ‘¡+1 = (Id âŠ— ğ‘”(cid:62) âŠ— ğ‘£ğ‘¡)ğ‘‡ (cid:54)1 where ğ‘‡ (cid:54)1 is
the truncated tensor.

12Some of the lemmas we apply, out of concerns for compatibility with [SS17], assume that the maximum
singular value of ğ‘‡ (cid:54)1 is at most 1. Though one could re-do the previous analysis with minimal consequences
under the assumption that the spectral norm is at most 1 + ğœ€, for brevity we note that we may instead multiply
the whole tensor by 1
1âˆ’ğœ€ , and because the tensor has Frobenius norm at most (1 + 3ğœ€)
ğ‘›, this costs at most
4ğœ€

ğ‘› additional Frobenius norm error.

âˆš

âˆš

30

Thus, if we choose ğ›½, ğ›¿ satisfying the requirements of Lemma 6.2, after Ëœğ‘‚(ğ‘›ğ‘‚(ğ›½))
iterations of steps 2 & 3 we will recover a vector ğ‘¢ âˆˆ â„ğ‘‘2 such that (cid:104)ğ‘¢, ğ‘ğ‘–(cid:105)2 (cid:62) 1 âˆ’ 3 ğœ€
ğ›½ ,
and after Ëœğ‘‚(ğ‘›1+ğ‘‚(ğ›½)) iterations of steps 2 & 3 we will recover vectors ğ‘¢ğ‘¡1 , . . . , ğ‘¢ğ‘¡ğ‘š so that
(cid:104)ğ‘¢ğ‘¡ğ‘– , ğ‘ğ‘–(cid:105)2 (cid:62) 1 âˆ’ 3 ğœ€

ğ›½ğ›¿ for ğ‘š (cid:62) (1 âˆ’ ğ›¿)ğ‘› of the ğ‘– âˆˆ [ğ‘›].

(cid:17) 1/4

(cid:16) 3ğœ€
ğ›¿ğ›½

Next, applying Lemma 6.5 to each of the good candidate vectors obtained in Algorithm 3,
ğœ€ âˆ’

Algorithm 4 will give us candidate components Ë†ğ‘¢ğ‘¡1 , . . . , Ë†ğ‘¢ğ‘¡ğ‘š so that (cid:104) Ë†ğ‘¢ğ‘¡ğ‘– , ğ‘ğ‘–(cid:105)2 (cid:62) 1 âˆ’ 4
. Since Î 3 has rank ğ‘›, we write it as ğ‘ˆğ‘ˆT for ğ‘ˆ âˆˆ â„ğ‘‘3Ã—ğ‘›. Then we may reshape
4
(ğ‘¢ğ‘¢T âŠ— Id)Î 3 as ğ‘¢ğ‘¢Tğ‘ˆ(cid:48)(ğ‘ˆT âŠ— Id), where ğ‘ˆ(cid:48) is the ğ‘‘2 Ã— ğ‘›ğ‘‘ reshaping of ğ‘ˆ. Multiplying ğ‘¢T
through takes ğ‘‚(ğ‘›ğ‘‘3) time and then reshaping the result back results in (ğ‘¢ğ‘¢T âŠ— Id)Î 3.
Therefore, by Lemma 4.2, each invocation of Algorithm 4 requires Ëœğ‘‚(ğ‘›ğ‘‘3) operations.

âˆš

Finally, from Lemma 6.7, we know that if we run Algorithm 5 with ğœƒ = 10(cid:107)ğ‘Š (cid:107) Â·

(cid:18)

2ğœ€1/4 + 2

(cid:16) 3ğœ€
ğ›¿ğ›½

(cid:17) 1/8(cid:19)

+ 2ğœ€, we will reject any Ë†ğ‘¢ such that (cid:104) Ë†ğ‘¢, ğ‘ğ‘–(cid:105)2 (cid:54) 1 âˆ’ ğœƒ âˆ’ 2ğœ€ for all ğ‘– âˆˆ [ğ‘›],

and will keep all of the good outputs of Algorithm 4. Each iteration of Algorithm 5 requires
time ğ‘‚(ğ‘‘4 + ğ‘›ğ‘‘3 + ğ‘‘3), since we form the vector (ğ‘Š Ë†ğ‘¢âŠ—2) âŠ— Ë†ğ‘¢, then multiply with the rank-ğ‘›
matrix Î 3, and ultimately compute a norm.

This completes the proof.

(cid:3)

7 Combining lift and round for ï¬nal algorithm

In this section we describe and analyze our ï¬nal tensor decomposition algorithm, proving
our main theorem.

Algorithm 6 Main algorithm for overcomplete 4-tensor decomposition
Function decompose(ğ‘‡):
Input: a tensor ğ‘‡ âˆˆ (â„ğ‘‘)âŠ—4, numbers ğ›½, ğ›¿, ğœ€ âˆˆ (0, 1), numbers ğœ, ğœ…0 âˆˆ â„(cid:62)0, and ğ‘› (cid:54) ğ‘‘2.

1. Run lift(ğ‘‡, ğ‘›) from Algorithm 2 to obtain an implicit tensor ğ‘‡(cid:48) and an implicit matrix

Î 3, using ğœ, ğœ…0 as upper bounds on condition numbers ğœğ‘› , ğœ….

2. Run the algorithm speciï¬ed by Lemma 6.1 on input (ğ‘‡(cid:48), Î 3, ğœ€, ğ›½, ğ›¿) with independent

randomness ğ‘¡ = Ëœğ‘‚(ğ‘›) times, to obtain vectors ğ‘¢1, . . . , ğ‘¢ğ‘¡.

Output: ğ‘¢1, . . . , ğ‘¢ğ‘¡

Deï¬nition 7.1 (Signed Hausdorï¬€ distance). For sets of vectors ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘ and
ğ‘1, . . . , ğ‘ğ‘š âˆˆ â„ğ‘‘, we deï¬ne the signed Hausdorï¬€ distance to be the maximum of the follow-
ing two quantities. (1) maxğ‘–âˆˆ[ğ‘›] minğ‘—âˆˆ[ğ‘š],ğœâˆˆÂ±1 (cid:107)ğ‘ğ‘– âˆ’ ğœğ‘ ğ‘— (cid:107) and (2) maxğ‘–âˆˆ[ğ‘š] minğ‘—âˆˆ[ğ‘›],ğœâˆˆÂ±1 (cid:107)ğ‘ğ‘– âˆ’
ğœğ‘ ğ‘— (cid:107).

31

Deï¬nition 7.2 (Condition number of ğ‘1, . . . , ğ‘ğ‘›). Let ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘. Let {ğ‘ğ‘–ğ‘— } ğ‘—âˆˆ[ğ‘‘âˆ’1] be an
arbitrary orthonormal basis for the orthogonal complement of ğ‘ğ‘– in â„ğ‘‘. Let

ğ»T :=

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°
Let ğ‘… = (ğ»ğ»T)âˆ’1/2ğ» be a column-wise orthonormalization of ğ», and let ğ¾ = 1
2(Id âˆ’ ğ‘ƒ2,3)ğ‘…,
where ğ‘ƒ2,3 is the permutation matrix that exchanges the 2nd and 3rd modes of (â„ğ‘‘)âŠ—3. The
condition number ğœ… of ğ‘1, . . . , ğ‘ğ‘› is the minimum singular value of ğ¾.

ğ‘1 âŠ— ğ‘1 âŠ— ğ‘1,1
...
ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— ğ‘ğ‘–,ğ‘—
...
ğ‘ğ‘› âŠ— ğ‘ğ‘› âŠ— ğ‘ğ‘›,ğ‘‘âˆ’1

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

.

Theorem 7.3. For every ğ‘‘, ğ‘› âˆˆ â„• and ğœ€, ğ›½, ğ›¿ âˆˆ (0, 1) and ğœ, ğœ…0 âˆˆ â„(cid:62)0 there is a randomized
algorithm decomposeğ‘‘,ğ‘›,ğœ€,ğ›½,ğ›¿,ğœ,ğœ…0(ğ‘‡) with the following guarantees. For every set of unit vectors
ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘ and every ğ¸ âˆˆ (â„ğ‘‘)âŠ—4 such that

1. the operator norm of the square matrix ï¬‚attening of ğ¸ satisï¬es (cid:107)ğ¸12;34 (cid:107)

ğ‘›ğœ‡âˆ’1ğœ…2 (cid:54) ğœ€,

ğœ7

2. ğœ… = ğœ…(ğ‘1, . . . , ğ‘ğ‘›) (cid:62) ğœ…0

3. ğœğ‘› (cid:62) ğœ

where

1. ğœğ‘› is the ğ‘›-th singular value of the matrix (cid:205)ğ‘–(cid:54)ğ‘›(ğ‘âŠ—2

ğ‘–

)(ğ‘âŠ—2
ğ‘–

)T,

2. ğœ‡ is the operator norm of (cid:205)ğ‘–(cid:54)ğ‘›(ğ‘âŠ—3

ğ‘–

)(ğ‘âŠ—3
ğ‘–

)T, and

3. ğœ… is the condition number of ğ‘1, . . . , ğ‘ğ‘› as in Deï¬nition 7.2.

there is a subset ğ‘† âŠ† {ğ‘1, . . . , ğ‘ğ‘› } of size |ğ‘†| (cid:62) (1 âˆ’ ğ›¿)ğ‘› such that given input ğ‘‡ = (cid:205)ğ‘–(cid:54)ğ‘› ğ‘âŠ—4
+ ğ¸
the algorithm produces a set ğµ = {ğ‘1, . . . , ğ‘ğ‘¡ } of ğ‘¡ = Ëœğ‘‚(ğ‘›) vectors which with probability at least
0.99 over the randomness in the algorithm has

ğ‘–

signed-Hausdorff-distance(ğ‘†, ğµ) (cid:54) ğ‘‚

(cid:19) 1/16

.

(cid:18) ğœ€
ğ›¿ğ›½

Furthermore, the algorithm decomposeğ‘‘,ğ‘›,ğœ€,ğ›½,ğ›¿,ğœ,ğœ…0 runs in time

Ëœğ‘‚

(cid:18) ğ‘›ğ‘‘4
âˆš
ğœ

+

ğ‘›2ğ‘‘3
ğœ…0

+

ğ‘›2+ğ‘‚(ğ›½)ğ‘‘3
ğ›½

(cid:19)

.

32

We record some intuitive explanations of the parameters in Theorem 7.3.

â€¢ ğœ, ğœ…0 are bounds on the minimum singular values of matrices associated to ğ‘1, . . . , ğ‘ğ‘›,
used to determine the necessary precision of linear-algebraic manipulations per-
formed by the algorithm. Decreasing ğœ, ğœ…0 yields an algorithm tolerating less
well-conditioned tensors, at the expense of running time and/or accuracy guarantees.

â€¢ ğ›¿ determines what fraction of the vectors ğ‘1, . . . , ğ‘ğ‘› the algorithm is allowed to fail to
return. By decreasing ğ›¿ the algorithm recovers a larger fraction of ğ‘1, . . . , ğ‘ğ‘›, at the
cost of increasing running time and/or decreasing per-vector accuracy.

â€¢ ğ›½ determines the per-vector accuracy of the algorithm. Increasing ğ›½ improves the

accuracy of the algorithm, but with exponential cost in the running time.

â€¢ ğœ€ governs the magnitude of allowable noise ğ¸. Increasing ğœ€ yields a more noise-

tolerant algorithm, at the expense of the accuracy of recovered vectors.

We record the following corollary, which follows from Theorem 7.3 by choosing

parameters appropriately.

Corollary 7.4. For every ğ‘›, ğ‘‘ âˆˆ â„• and ğœ > 0 (independent of ğ‘›, ğ‘‘) there is an algorithm with the
following guarantees. The algorithm takes input ğ‘‡ = (cid:205)ğ‘–(cid:54)ğ‘› ğ‘âŠ—4

+ ğ¸, and so long as

ğ‘–

1. ğœ…(ğ‘1, . . . , ğ‘ğ‘›) (cid:62) ğœ

2. the minimum nonzero eigenvalue of (cid:205)ğ‘–(cid:54)ğ‘›(ğ‘âŠ—2

ğ‘–

)(ğ‘âŠ—2
ğ‘–

)(cid:62) is at least ğœ

3. (cid:107) (cid:205)ğ‘–(cid:54)ğ‘›(ğ‘âŠ—3

ğ‘–

)(ğ‘âŠ—3
ğ‘–

)(cid:62)(cid:107) (cid:54) 1/ğœ, and

4. (cid:107)ğ¸12;34(cid:107) (cid:54) poly(ğœ)/(log ğ‘›)ğ‘‚(1),

with high probability the algorithm recovers Ëœğ‘‚(ğ‘›) vectors ğ‘1, . . . , ğ‘ğ‘¡ such that there is a set
ğ‘† âŠ† {ğ‘1, . . . , ğ‘ğ‘› } with |ğ‘†| (cid:62) (1 âˆ’ ğ‘œ(1))ğ‘› such that the signed Hausdorï¬€ distance from ğ‘† to
{ğ‘1, . . . , ğ‘ğ‘¡ } is ğ‘œ(1), in time Ëœğ‘‚(ğ‘›2ğ‘‘3/poly(ğœ)).

Furthermore, hypotheses (2),(3) hold for random unit vectors ğ‘1, . . . , ğ‘ğ‘› with ğœ = 0.1 so long as

ğ‘› (cid:54) ğ‘‘2/(log ğ‘›)ğ‘‚(1), and experiments in Appendix C strongly suggest that (1) does as well.

Proof of Theorem 7.3. Let ğ‘Š = ((cid:205)ğ‘–(cid:54)ğ‘› ğ‘âŠ—2
ğ‘–
and matrix Î 3 returned by lift satisfy

(ğ‘âŠ—2
ğ‘–

)(cid:62))âˆ’1/2. By Lemma 5.1, the implicit tensor ğ‘‡(cid:48)

(cid:13)
(cid:13)
ğ‘‡(cid:48) âˆ’ (cid:213)
(cid:13)
(cid:13)
(cid:13)
ğ‘–(cid:54)ğ‘›

(ğ‘Š(ğ‘ğ‘– âŠ— ğ‘ğ‘–))âŠ—3

(cid:54) ğ‘‚(ğœ€ğœ9/2

ğ‘›

âˆš

ğ‘›)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)ğ¹

33

and

(cid:13)
(cid:13)Î 3 âˆ’ Î Span(ğ‘Š ğ‘âŠ—2
(cid:13)

ğ‘–

(cid:13)
(cid:13)
(cid:13)

âŠ—ğ‘ğ‘–)

(cid:54) ğ‘‚(ğœ€ğœ4

ğ‘›) .

So, by Lemma 6.1, with high probability there is a subset ğ‘† âŠ† {ğ‘1, . . . , ğ‘ğ‘› } of size ğ‘š (cid:62) (1âˆ’ğ›¿)ğ‘›
such for each ğ‘ğ‘– âˆˆ ğ‘† there is ğ‘¢ğ‘— among the vectors ğ‘¢1, . . . , ğ‘¢ğ‘¡ returned by the rounding
algorithm with

(cid:104)ğ‘ğ‘– , ğ‘¢ğ‘—(cid:105)2 (cid:62) 1 âˆ’ ğ‘‚

(cid:18) 1
ğ›¿1/8

Â·

1
ğ›½1/8

Â· ğœ€1/8 Â· (cid:107)ğ‘Š (cid:107) Â·

(cid:19)

âˆš

ğœğ‘›

= 1 âˆ’ ğ‘‚

(cid:19) 1/8

(cid:18) ğœ€
ğ›¿ğ›½

where the equality follows because (cid:107)ğ‘Š (cid:107) = ğœâˆ’1/2
. Furthermore, each of the vectors ğ‘¢1, . . . , ğ‘¢ğ‘¡
ğ‘›
is similarly close to some ğ‘ğ‘– âˆˆ ğ‘†. This proves the claimed upper bound on the Hausdorï¬€
distance.

The running time follows from putting together Lemma 5.2 and the running time
(cid:3)

bounds of Lemma 6.1.

8 Condition number of random tensors

Deï¬nition 8.1. Let Î 2,3 : (â„ğ‘‘)âŠ—3 â†’ (â„ğ‘‘)âŠ—3 be the orthogonal projector to the subspace
Span(ğ‘¥ âŠ— ğ‘¦ âŠ— ğ‘¦ | ğ‘¥, ğ‘¦ âˆˆ â„ğ‘‘) that is invariant under interchange of the second and third
tensor modes. Let Î 1,2 be deï¬ned similarly. Let Î âŠ¥

2,3 = Id âˆ’ Î 2,3 and Î âŠ¥

1,2 = Id âˆ’ Î 1,2.

Note that Î 2,3 = 1

2,3 = 1
the second and third modes, and Î âŠ¥
Formula in representation theory, whereby for any group ğº of linear operators, 1
|ğº|
is equal to the projection to the common invariant subspace of ğº.

2(Id + ğ‘ƒ2,3), where ğ‘ƒ2,3 is the orthogonal operator that interchanges
2(Id âˆ’ ğ‘ƒ2,3). This follows from the Projection
(cid:205)ğ‘”âˆˆğº ğ‘”

Lemma 8.2 (Condition number of basic swap matrix). Let ğ‘1, . . . , ğ‘ğ‘› be independent random
ğ‘‘-dimensional unit vectors. Let ğµğ‘– âˆˆ â„(ğ‘‘âˆ’1)Ã—ğ‘‘ be a random basis for the orthogonal complement
of ğ‘ğ‘– in â„ğ‘‘. Let ğ‘ƒ âˆˆ â„ğ‘‘3Ã—ğ‘‘3 be the permutation matrix which swaps second and third modes of
(â„ğ‘‘)âŠ—3. Let

ğ´ = ğ”¼
ğ‘

(ğ‘ âŠ— ğ‘ âŠ— Id)(ğ‘ âŠ— ğ‘ âŠ— Id)(cid:62) .

Let ğ‘… âˆˆ â„ğ‘‘3Ã—ğ‘›(ğ‘‘âˆ’1) have ğ‘› blocks of dimensions ğ‘‘3 Ã— (ğ‘‘ âˆ’ 1), where the ğ‘–-th block is

ğ‘…ğ‘– = ğ´âˆ’1/2(ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— ğµğ‘–) âˆ’ ğ‘ƒğ´âˆ’1/2(ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— ğµğ‘–)

where we abuse notation and denote the PSD square root of the pseudoinverse of ğ´ by ğ´âˆ’1/2. Then
ğ‘›, ğ‘›, ğ‘‘3/2).
there is a function ğ‘‘(cid:48)(ğ‘‘) = Î˜(ğ‘‘2) such that ğ”¼ (cid:107)ğ‘…(cid:62)ğ‘… âˆ’ ğ‘‘(cid:48)(ğ‘‘) Â· Id(cid:107) (cid:54) ğ‘‚(log ğ‘‘)2 Â· max(ğ‘‘

âˆš

34

In particular, if ğ‘‘ (cid:28) ğ‘› (cid:28) ğ‘‘2,

ğ”¼ (cid:107) 1
ğ‘‘(cid:48)(ğ‘‘)

ğ‘…(cid:62)ğ‘… âˆ’ Id(cid:107) (cid:54) ğ‘‚(ğ‘›(log ğ‘‘)2/ğ‘‘2) .

Corollary 8.3. Let ğ‘1, . . . , ğ‘ğ‘› be independent random ğ‘‘-dimensional unit vectors with ğ‘‘ (cid:28) ğ‘›.
Then with probability 1âˆ’ğ‘œ(1), the condition number ğœ… of ğ‘1, . . . , ğ‘ğ‘› as deï¬ned in Lemma 5.3 is at least
(cid:113) 1
T has ğ‘›-th eigenvalue ğœ âˆˆ 1âˆ’ Ëœğ‘‚(ğ‘›/ğ‘‘2)âˆ’ğ‘‚(1/ğ‘‘),
ğ‘âŠ—2
ğ‘–

ğ‘‘2 ), the matrix ğ‘‡ = (cid:205) ğ‘âŠ—2

ğ‘‘ )âˆ’ Ëœğ‘‚( ğ‘›

4 âˆ’ 1

âˆš

ğ‘–

âˆ’ğ‘‚( 1
and the matrix ğ‘ˆ = (cid:205) ğ‘âŠ—3

4

2

ğ‘âŠ—3
ğ‘–

ğ‘–

T has spectral norm 1 + Ëœğ‘‚(ğ‘›/ğ‘‘2).

Therefore, decomposeğ‘‘,ğ‘›,ğœ€,ğ›½,ğ›¿,ğœ,ğœ…0 when run with error ğœ€, recovers (1 âˆ’ ğœ)ğ‘› components ğ‘ğ‘– with

signed Hausdorï¬€ distance ğ‘‚(ğœ€/(ğ›¿ğ›½))1/16 in time Ëœğ‘‚(ğ‘›ğ‘‘4 + ğ‘›2ğ‘‘3 + ğ‘›2+ğ‘‚(ğ›½)ğ‘‘3/ğ›½).

To prove the corollary, we will need an elementary fact.

Fact 8.4. Let ğ‘1, . . . , ğ‘ğ‘› be independent random unit vectors in â„ğ‘‘. Let ğ‘ˆ = (cid:205)ğ‘›
probability 1 âˆ’ ğ‘œ(1), (cid:107)ğ‘ˆ âˆ’ Î img(ğ‘ˆ)(cid:107) (cid:54) Ëœğ‘‚(ğ‘›/ğ‘‘2) if ğ‘› (cid:29) ğ‘‘.

ğ‘–=1 ğ‘âŠ—3

ğ‘–

ğ‘âŠ—3
ğ‘–

T. With

The proof may be found in Section 8.8.

Fact 8.5. Let ğ‘1, . . . , ğ‘ğ‘› be independent random unit vectors in â„ğ‘‘. Let Ë†Î£ = ğ”¼ğ‘(ğ‘ âŠ— ğ‘)(ğ‘ âŠ— ğ‘)T for
ğ‘ drawn from the uniform distribution over unit vectors in â„ğ‘‘. Then

(cid:107)2ğ‘‘âˆ’2 Ë†Î£âˆ’1/2ğ‘‡ Ë†Î£âˆ’1/2 âˆ’ Î  Ë†Î£âˆ’1/2 img(ğ‘‡)(cid:107) (cid:54) Ëœğ‘‚(ğ‘›/ğ‘‘2) .

Proof. The fact follows by a slight modiï¬cation of [HSSS16, Lemma 5.9]. While Lemma 5.9
in [HSSS16] applies to Gaussian random vectors, not uniform random unit vectors, and
gives a bound of Ëœğ‘‚(ğ‘›/ğ‘‘3/2), with minor changes, it holds for unit vectors and with a bound
of Ëœğ‘‚(ğ‘›/ğ‘‘2)).
(cid:3)

Fact 8.6. Suppose ğ´ and ğµ are both subspaces of dimension ğ‘›. Suppose ğœƒ âˆˆ [0, ğœ‹/2). Then
(cid:107)Î ğ´ âˆ’ Î ğµÎ ğ´(cid:107) (cid:54) sin ğœƒ if and only if for every ğ‘¥ âˆˆ ğ´ there is a ğ‘¦ âˆˆ ğµ so that

(cid:104)ğ‘¥, ğ‘¦(cid:105)
(cid:107)ğ‘¥(cid:107) (cid:107)ğ‘¦(cid:107)

(cid:62) cos ğœƒ .

Futhermore, when this holds, since (cid:107)Î ğ´ âˆ’ Î ğµÎ ğ´(cid:107) = (cid:107)Î ğµ âˆ’ Î ğ´Î ğµ (cid:107) by symmetry, the triangle

inequality yields (cid:107)Î ğ´ âˆ’ Î ğµ (cid:107) (cid:54) 2 sin ğœƒ.

Proof. Consider the product ğ‘… = Î ğ´Î ğµ. Let ğ‘… = ğ‘ˆÎ£ğ‘‰T be its singular value decomposition,
with ğ‘¢ğ‘– and ğ‘£ğ‘– its ğ‘–th left- and right-singular vectors and ğœğ‘– its ğ‘–th singular value. We show
as an intermediate step that ğœğ‘› is at least cos ğœƒ if and only if for every ğ‘¥ âˆˆ ğ´ there is a ğ‘¦ âˆˆ ğµ
so that (cid:104)ğ‘¥, ğ‘¦(cid:105) (cid:62) (cid:107)ğ‘¥(cid:107) (cid:107)ğ‘¦(cid:107) cos ğœƒ.

In one direction, suppose ğœğ‘› is at least cos ğœƒ. Then take ğ‘¦ = ğ‘…Tğ‘¥. We see that
(cid:104)ğ‘¥, ğ‘¦(cid:105) = ğ‘¥TÎ ğ´Î ğµ ğ‘¦ = ğ‘¥Tğ‘…ğ‘¦ = (cid:107)ğ‘…Tğ‘¥(cid:107)2. Since ğ‘¥ âˆˆ ğ´ and dim ğ´ = ğ‘› and img(ğ‘…) âŠ† ğ´ and

35

singular values are non-negative, if ğœğ‘› > 0 then the ï¬rst ğ‘› left-singular vectors of ğ‘… must
span ğ´. Therefore, decomposing ğ‘¥ = (cid:205) ğ›¼ğ‘–ğ‘¢ğ‘–, we must have ğ›¼ğ‘– = 0 for ğ‘– > ğ‘›. So, ğ‘…Tğ‘¥ =
(cid:205) ğ›¼ğ‘–ğ‘‰Î£ğ‘ˆTğ‘¢ğ‘– = (cid:205) ğ›¼ğ‘– ğœğ‘–ğ‘£ğ‘–, and (cid:107)ğ‘…Tğ‘¥(cid:107)2 = (cid:107) (cid:205) ğ›¼ğ‘– ğœğ‘–ğ‘£ğ‘– (cid:107)2 = (cid:205) ğ›¼2
ğ‘› = (cid:107)ğ‘¥(cid:107)2ğœ2
ğœ2
ğ‘›.
ğ‘–
So

(cid:107)ğ‘£ğ‘– (cid:107)2 (cid:62) (cid:205) ğ›¼2
ğ‘–

ğœ2
ğ‘–

(cid:104)ğ‘¥, ğ‘¦(cid:105)
(cid:107)ğ‘¥(cid:107) (cid:107)ğ‘¦ (cid:107)

=

(cid:107)ğ‘…Tğ‘¥(cid:107)2
(cid:107)ğ‘¥(cid:107) (cid:107)ğ‘…Tğ‘¥(cid:107)

=

(cid:107)ğ‘…Tğ‘¥ (cid:107)
(cid:107)ğ‘¥(cid:107)

(cid:62) ğœğ‘› (cid:62) cos ğœƒ .

In the other direction, suppose ğœğ‘› < cos ğœƒ. Then take ğ‘¥ = ğ‘¢ğ‘› and for any ğ‘¦ âˆˆ ğµ,
decompose ğ‘¦ = (cid:205) ğ›½ğ‘–ğ‘£ğ‘– where again ğ›½ğ‘– = 0 for all ğ‘– > ğ‘›. So (cid:104)ğ‘¥, ğ‘¦(cid:105) = ğ‘¥TÎ ğ´Î ğµ ğ‘¦ = (ğ‘…Tğ‘¥)Tğ‘¦ =
ğœğ‘›ğ‘£ğ‘›Tğ‘¦ = ğœğ‘›ğ›½ğ‘›. Since ğ›½ğ‘› (cid:54) (cid:107)ğ‘¦(cid:107), for any ğ‘¦ we must have

(cid:104)ğ‘¥, ğ‘¦(cid:105)
(cid:107)ğ‘¥(cid:107) (cid:107)ğ‘¦(cid:107)

ğœğ‘›ğ›½ğ‘›
(cid:107)ğ‘¦ (cid:107)

=

(cid:54) ğœğ‘› < cos ğœƒ .

Finally, we show that (cid:107)Î ğ´ âˆ’Î ğµÎ ğ´(cid:107) (cid:54) sin ğœƒ if and only if ğœğ‘› is at least cos ğœƒ. This follows
from the Pythagorean theorem, as Î ğ´ = (Î ğµÎ ğ´) + (Î ğ´ âˆ’ Î ğµÎ ğ´) and also (Î ğµÎ ğ´)T(Î ğ´ âˆ’
Î ğµÎ ğ´) = 0. Thus for any vector ğ‘£, we see (cid:107)(Î ğ´ âˆ’ Î ğµÎ ğ´)ğ‘£(cid:107)2 = (cid:107)Î ğ´ğ‘£(cid:107)2 âˆ’ (cid:107)Î ğµÎ ğ´ğ‘£(cid:107)2 =
(cid:107)Î ğ´ğ‘£(cid:107)2 âˆ’ (cid:107)ğ‘…TÎ ğ´ğ‘£(cid:107)2.
(cid:3)
Fact 8.7. Let Î¦ = (cid:205) ğ‘’ğ‘– âŠ— ğ‘’ğ‘– for ğ‘’ğ‘– the elementary basis vectors in â„ğ‘‘.
(cid:107)Î 2,3ğ‘¥(cid:107) = 1âˆš
2

If ğ‘¥ âˆˆ Î¦ âŠ— â„ğ‘‘ then

(cid:112)1 + 1

ğ‘‘ (cid:107)ğ‘¥(cid:107).

Proof. Since ğ‘¥ âˆˆ Î¦ âŠ— â„ğ‘‘, we may write it as ğ‘¥ = Î¦ âŠ— ğ‘¢ for some ğ‘¢ âˆˆ â„ğ‘‘. We directly
calculate:

(cid:107)Î 2,3ğ‘¥ (cid:107)2 = (cid:107) 1
= 1
= 1
= 1

2(Id + ğ‘ƒ2,3)ğ‘¥(cid:107)2
4 (cid:107)ğ‘¥(cid:107)2 + 1
2 (cid:107)ğ‘¥(cid:107)2 + 1
2 (cid:107)ğ‘¥(cid:107)2 + 1
2 (cid:107)ğ‘¥(cid:107)2 + 1
2 (cid:107)ğ‘¥(cid:107)2 + 1
2 (cid:107)ğ‘¥(cid:107)2 + 1
2 (cid:107)ğ‘¥(cid:107)2(1 + 1

4 (cid:107)ğ‘ƒ2,3ğ‘¥ (cid:107)2 + 1
2 (cid:104)ğ‘¥, ğ‘ƒ2,3ğ‘¥(cid:105)
2 (cid:104)Î¦ âŠ— ğ‘¢, ğ‘ƒ2,3(Î¦ âŠ— ğ‘¢)(cid:105)
2 (cid:104)(cid:213) ğ‘’ğ‘– âŠ— ğ‘’ğ‘– âŠ— ğ‘¢, (cid:213) ğ‘’ğ‘— âŠ— ğ‘¢ âŠ— ğ‘’ğ‘—(cid:105)
(cid:213) (cid:213)(cid:104)ğ‘’ğ‘– , ğ‘’ğ‘—(cid:105)(cid:104)ğ‘’ğ‘– , ğ‘¢(cid:105)(cid:104)ğ‘’ğ‘— , ğ‘¢(cid:105)
(cid:213)(cid:104)ğ‘’ğ‘– , ğ‘¢(cid:105)2
2 (cid:107)ğ‘¢ (cid:107)2
ğ‘‘ ) .

2

2

= 1

= 1
= 1
= 1

(cid:3)
Fact 8.8. Let Î¦ = (cid:205) ğ‘’ğ‘– âŠ— ğ‘’ğ‘– for ğ‘’ğ‘– the elementary basis vectors in â„ğ‘‘. Let ğ‘¢ âˆˆ sym2 âŠ— â„ğ‘‘ and
decompose ğ‘¢ = ğ‘¥ + ğ‘¦ with ğ‘¥ âˆˆ Î¦ âŠ— â„ğ‘‘ and ğ‘¦ âŠ¥ ğ‘¥. Then if (cid:107)Î âŠ¥

(cid:107) ğ‘¦(cid:107), it holds that

2,3ğ‘¦(cid:107) (cid:62) 1âˆš

2

(cid:107)Î âŠ¥

2,3ğ‘¢ (cid:107)2 (cid:62) (cid:16) 1

4 âˆ’ 1

âˆš

4

âˆ’ ğ‘‚( 1
ğ‘‘ )

2

(cid:17)

(cid:107)ğ‘¢ (cid:107)2.

36

Proof. Let ğ‘ƒ3,2,1 = (ğ‘ƒ1,2,3)âˆ’1 be the orthogonal linear operator permuting the tensor modes,
so that the 3-cycle (3 2 1) replaces the third mode with the second, the second mode
with the ï¬rst, and the ï¬rst mode with the third again. By a unitary similarity transform
conjugating by ğ‘ƒ3,2,1, since ğ‘¥ âˆˆ Î¦ âŠ— â„ğ‘‘, we have

(cid:107)Î 1,2ğ‘ƒ2,3ğ‘¥(cid:107) = (cid:107)(ğ‘ƒ1,2,3Î 1,2ğ‘ƒ3,2,1)(ğ‘ƒ1,2,3ğ‘ƒ2,3)ğ‘¥(cid:107)

= (cid:107)Î 2,3ğ‘ƒ1,2ğ‘¥(cid:107)
= (cid:107)Î 2,3ğ‘¥(cid:107)
(cid:112)1 + 1
= 1âˆš
2

ğ‘‘ (cid:107)ğ‘¥(cid:107) ,

where the last step uses Fact 8.7. We write, since Î 1,2ğ‘¥ = ğ‘¥,

(cid:107)Î 1,2Î âŠ¥

2,3ğ‘¥(cid:107) = 1
2 (cid:107)Î 1,2(ğ‘¥ âˆ’ ğ‘ƒ2,3ğ‘¥)(cid:107)
= 1
2 (cid:107)ğ‘¥ âˆ’ Î 1,2ğ‘ƒ2,3ğ‘¥(cid:107)
2 (cid:107)ğ‘¥ (cid:107) + 1
(cid:54) 1
2 (cid:107)ğ‘¥(cid:107) + 1
= 1
(cid:16) 1
2 + 1

2 (cid:107)Î 1,2ğ‘ƒ2,3ğ‘¥(cid:107)
(cid:112)1 + 1
ğ‘‘ (cid:107)ğ‘¥(cid:107)
âˆš
2
(cid:17)
+ ğ‘‚( 1
ğ‘‘ )

=

âˆš

2

(cid:107)ğ‘¥(cid:107) ,

2

2

where the second-to-last step substitutes in (8.1).

(8.1)

(8.2)

Therefore, since ğ‘¢ = ğ‘¥ + ğ‘¦ and Fact 8.7 implies (cid:107)Î âŠ¥

2,3ğ‘¥(cid:107)2 (cid:62) ( 1

2 âˆ’ ğ‘‚( 1

ğ‘‘ ))(cid:107)ğ‘¥(cid:107)2 and also by

assumption (cid:107)Î âŠ¥

2,3ğ‘¦(cid:107)2 (cid:62) 1

2 (cid:107) ğ‘¦(cid:107)2,

(cid:107)Î âŠ¥

2,3ğ‘¢ (cid:107)2 = (cid:107)Î âŠ¥

2,3ğ‘¥ (cid:107)2 + (cid:107)Î âŠ¥

= ( 1

2 âˆ’ ğ‘‚( 1

ğ‘‘ ))(cid:107)ğ‘¥(cid:107)2 + 1

2,3ğ‘¦(cid:107)2 + (cid:104)ğ‘¦, Î âŠ¥

2,3ğ‘¥(cid:105)
2 (cid:107) ğ‘¦(cid:107)2 + (cid:104)ğ‘¦, Î âŠ¥

2,3ğ‘¥(cid:105) .

Since ğ‘¦ âˆˆ sym2 âŠ— â„ğ‘‘ and therefore Î 1,2ğ‘¦ = ğ‘¦, Cauchy-Schwarz implies (cid:104)ğ‘¦, Î âŠ¥
âˆ’(cid:107)Î 1,2Î âŠ¥
in and then apply Youngâ€™s inequality:

2,3ğ‘¥(cid:107) (cid:107) ğ‘¦(cid:107) and then by (8.2), this is at least âˆ’( 1

2,3ğ‘¥(cid:105) (cid:62)
ğ‘‘ ))(cid:107)ğ‘¥(cid:107). We substitute this

2 + 1

+ ğ‘‚( 1

âˆš
2

2

(cid:107)Î âŠ¥

2 âˆ’ ğ‘‚( 1

2 âˆ’ ğ‘‚( 1

2,3ğ‘¢ (cid:107)2 (cid:62) (cid:0) 1
(cid:62) (cid:0) 1
(cid:62) (cid:16) 1
4 âˆ’ 1
(cid:16) 1
4 âˆ’ 1

=

4

ğ‘‘ )(cid:1) (cid:107)ğ‘¥(cid:107)2 + 1
ğ‘‘ )(cid:1) (cid:107)ğ‘¥(cid:107)2 + 1
âˆ’ ğ‘‚( 1
ğ‘‘ )
(cid:17)

(cid:17)

âˆ’ ğ‘‚( 1
ğ‘‘ )

âˆš
2

âˆš
2

4

(cid:107)ğ‘¢(cid:107)2 .

âˆš
2

(cid:16) 1
2 + 1
(cid:16) 1
2 + 1

2 (cid:107) ğ‘¦(cid:107)2 âˆ’
2 (cid:107) ğ‘¦(cid:107)2 âˆ’
(cid:16) 1
(cid:107)ğ‘¥(cid:107)2 +
4 âˆ’ 1

âˆš
2

âˆš
2

4

+ ğ‘‚( 1
ğ‘‘ )

2

+ ğ‘‚( 1
ğ‘‘ )
(cid:17)

2
âˆ’ ğ‘‚( 1
ğ‘‘ )

(cid:107) ğ‘¦(cid:107)2

(cid:17)

(cid:17)

(cid:107)ğ‘¥(cid:107) (cid:107)ğ‘¦(cid:107)

2 (cid:107)ğ‘¥(cid:107)2 + 1
( 1

2 (cid:107) ğ‘¦(cid:107)2)

37

(cid:3)

Fact 8.9. Let ğ‘¢ âˆˆ sym2 âŠ— â„ğ‘‘. If

(cid:107)Î âŠ¥

2,3ğ´âˆ’1/2ğ‘¢(cid:107)
(cid:107)ğ´âˆ’1/2ğ‘¢ (cid:107)

(cid:62) 1 âˆ’ ğœ‡

, then

(cid:62) (cid:113) 1

4 âˆ’ 1

âˆš
2

4

âˆ’ ğ‘‚( 1

ğ‘‘ ) .

for ğœ‡ (cid:54) 1 âˆ’ 1âˆš
2

âˆ’ 2

âˆš
2âˆš
ğ‘‘

(cid:107)Î âŠ¥
2,3ğ‘¢ (cid:107)
(cid:107)ğ‘¢ (cid:107)
Proof. Write ğ‘¢ = ğ‘¥ + ğ‘¦ where ğ‘¥ = ( 1
inequality and Fact 8.7,

ğ‘‘ Î¦Î¦T âŠ— Id)ğ‘¢ and ğ‘¦ âŠ¥ ğ‘¥. If (cid:107)ğ‘¥(cid:107) > 2(cid:107)ğ‘¦(cid:107), then by triangle

(cid:107)Î âŠ¥
2,3ğ‘¢ (cid:107)
(cid:107)ğ‘¢(cid:107)

(cid:62)

(cid:62)

>

(cid:62)

>

(cid:107)ğ‘¥(cid:107) âˆ’ 1

2 (cid:107)ğ‘¥(cid:107)

2,3ğ‘¦(cid:107)

(cid:107)Î âŠ¥

(cid:107)Î âŠ¥

2,3ğ‘¥(cid:107) âˆ’ (cid:107)Î âŠ¥
(cid:107)ğ‘¢ (cid:107)
2,3ğ‘¥(cid:107) âˆ’ (cid:107) ğ‘¦(cid:107)
(cid:107)ğ‘¢(cid:107)

(cid:16) 1âˆš

2

(cid:16) 1âˆš

2

(cid:16) 1âˆš

2

(cid:17)

(cid:17)

âˆ’ 1

âˆ’ ğ‘‚( 1
ğ‘‘ )
(cid:107)ğ‘¢ (cid:107)
2 âˆ’ ğ‘‚( 1
ğ‘‘ )
(cid:107)ğ‘¥(cid:107) + (cid:107) ğ‘¦(cid:107)
(cid:17)
2 âˆ’ ğ‘‚( 1
ğ‘‘ )
3
2 (cid:107)ğ‘¥(cid:107)

âˆ’ 1

(cid:107)ğ‘¥(cid:107)

(cid:107)ğ‘¥(cid:107)

> (cid:113) 1

4 âˆ’ 1

âˆš

4

âˆ’ ğ‘‚( 1

ğ‘‘ ) .

2

Thus for the remainder of the argument, we assume (cid:107)ğ‘¥ (cid:107) (cid:54) 2(cid:107)ğ‘¦(cid:107).

ğ‘‘ ğ‘¥ and (cid:107)ğ´âˆ’1/2ğ‘¢ (cid:107)2 = ğ‘‘2
1

(cid:107)ğ‘¦(cid:107)2 + ğ‘‘(cid:107)ğ‘¥ (cid:107)2 (cid:62) ğ‘‘2
1

(cid:107) ğ‘¦(cid:107)2, so by triangle

Then ğ´âˆ’1/2ğ‘¢ = ğ‘‘1ğ‘¦ +

inequality,

âˆš

(cid:107)Î âŠ¥

2,3ğ‘¦(cid:107) (cid:62) ğ‘‘âˆ’1
(cid:62) ğ‘‘âˆ’1

âˆš

(cid:107)Î âŠ¥

2,3ğ‘¥(cid:107)

ğ‘‘
1 (cid:107)Î âŠ¥
2,3ğ´âˆ’1/2ğ‘¢ (cid:107) âˆ’
ğ‘‘1
âˆš
ğ‘‘
1 (cid:107)Î âŠ¥
2,3ğ´âˆ’1/2ğ‘¢ (cid:107) âˆ’
ğ‘‘1
âˆš
ğ‘‘
2,3ğ´âˆ’1/2ğ‘¢ (cid:107) âˆ’ 2
1 (cid:107)Î âŠ¥
ğ‘‘1
1 (1 âˆ’ ğœ‡)(cid:107)ğ´âˆ’1/2ğ‘¢ (cid:107) âˆ’ 40

(cid:107)ğ‘¥(cid:107)

(cid:107)ğ‘¦ (cid:107)
âˆš

ğ‘‘
ğ‘‘1

(cid:62) ğ‘‘âˆ’1

(cid:62) ğ‘‘âˆ’1

(cid:62) (1 âˆ’ ğœ‡)(cid:107)ğ‘¦(cid:107) âˆ’ 2
(cid:17)
(cid:62) (cid:16)

1 âˆ’ ğœ‡ âˆ’ 2

âˆš
2âˆš
ğ‘‘

âˆš

ğ‘‘
ğ‘‘1

(cid:107) ğ‘¦(cid:107)

(cid:107) ğ‘¦(cid:107) .

(cid:107) ğ‘¦(cid:107)

38

Therefore, the lemma follows by Fact 8.8, as long as 1 âˆ’ ğœ‡ âˆ’ 2

âˆš
2âˆš
ğ‘‘

(cid:62) 1âˆš
2

.

(cid:3)

Proof of Corollary 8.3. To lower bound ğœ…, we need a lower bound on the least singular value
of Î âŠ¥
(ğ»ğ»T)âˆ’1/2ğ», where ğ» is the matrix with columnwise blocks of ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— ğµğ‘–. By
2,3
2,3ğ´âˆ’1/2ğ» has all singular values
Lemma C.1, with probability 1 âˆ’ ğ‘œ(1), it holds that
within 1 Â± Ëœğ‘‚(ğ‘›/ğ‘‘2). This means that for all ğ‘¢ âˆˆ img(ğ»), we have (cid:107)Î âŠ¥
2,3ğ´âˆ’1/2ğ‘¢(cid:107)/(cid:107)ğ´âˆ’1/2ğ‘¢ (cid:107) (cid:62)
1 âˆ’ Ëœğ‘‚(ğ‘›/ğ‘‘2). Therefore, by Fact 8.9, (cid:107)Î âŠ¥
ğ‘‘ ) âˆ’ Ëœğ‘‚(ğ‘›/ğ‘‘2). This
inequality holding for all ğ‘¢ âˆˆ img(ğ») is equivalent to ğœ…, the smallest singular value of
âˆ’ ğ‘‚( 1
Î âŠ¥
(cid:3)
2,3

(ğ»ğ»T)âˆ’1/2ğ», being at least (cid:113) 1

2,3ğ‘¢(cid:107)/(cid:107)ğ‘¢(cid:107) (cid:62) (cid:113) 1

ğ‘‘ ) âˆ’ Ëœğ‘‚(ğ‘›/ğ‘‘2).

4 âˆ’ 1

4 âˆ’ 1

âˆ’ ğ‘‚( 1

âˆš
2
ğ‘‘1

Î âŠ¥

âˆš

âˆš

4

2

4

2

The remainder of this section is devoted to the proof of Lemma C.1. At a high level,
this proof follows the strategy laid out in in [Ver12] to prove Theorem 5.62 there, but the
random matrix we need to control is much more complicated than is handled there.

8.1 Notation

Throughout we use the following notation.

1. ğ‘‘, ğ‘› âˆˆ â„• are natural numbers.
2. ğ‘‘1 = (cid:112)(ğ‘‘2 + 2ğ‘‘)/2 = Î˜(ğ‘‘) and ğ‘‘2 = ğ‘‘âˆ’1/2((cid:112)(ğ‘‘ + 2)/2 âˆ’ 1) = Î˜(1).

3. ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘ are iid random unit vectors.

4. ğµğ‘– âˆˆ â„ğ‘‘Ã—(ğ‘‘âˆ’1) for ğ‘– (cid:54) ğ‘› is a matrix with columns which form a random orthonormal
basis for the orthogonal complement of ğ‘ğ‘– in â„ğ‘‘. (Chosen independently from ğ‘ ğ‘— , ğµğ‘—
for ğ‘— â‰  ğ‘–.)

5. Î£ = ğ”¼ğ‘(ğ‘ğ‘(cid:62) âŠ— ğ‘ğ‘(cid:62)) âˆˆ â„ğ‘‘2Ã—ğ‘‘2 is the 4-th moment matrix of a random ğ‘‘-dimensional

unit vector.

6. ğ´ = ğ”¼ğ‘(ğ‘ğ‘(cid:62) âŠ— ğ‘ğ‘(cid:62) âŠ— Id) = Î£ âŠ— Id âˆˆ â„ğ‘‘2Ã—ğ‘‘2Ã—ğ‘‘2 is Î£ â€œliftedâ€ to a 6-tensor.

7. Î¦ âˆˆ â„ğ‘‘2 is the vector Î¦ = (cid:205)ğ‘–(cid:54)ğ‘‘ ğ‘’ âŠ—2

ğ‘–

, where ğ‘’ğ‘– is the ğ‘–-th standard basis vector.

8. Î sym âˆˆ â„ğ‘‘2Ã—ğ‘‘2 is the projector to the symmetric subspace of â„ğ‘‘2 (i.e. the span of

vectors ğ‘¥âŠ—2 for ğ‘¥ âˆˆ â„ğ‘‘).

9. ğ‘ƒ âˆˆ â„ğ‘‘3Ã—ğ‘‘3 is the permutation matrix which swaps second and third tensor modes.

Concretely, (ğ‘ƒğ‘¥)ğ‘–ğ‘—ğ‘˜ = ğ‘¥ğ‘–ğ‘˜ ğ‘— for ğ‘–, ğ‘—, ğ‘˜ âˆˆ [ğ‘‘].

10. ğ‘†ğ‘–, for ğ‘– (cid:54) ğ‘›, is the ğ‘‘3 Ã— ğ‘‘ âˆ’ 1 matrix given by ğ‘†ğ‘– = ğ´âˆ’1/2(ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— ğµğ‘–)

39

11. ğ‘…ğ‘–, for ğ‘– (cid:54) ğ‘›, is the ğ‘‘3 Ã— ğ‘‘ âˆ’ 1 matrix given by ğ‘…ğ‘– = ğ‘†ğ‘– âˆ’ ğ‘ƒğ‘†ğ‘–.

12. ğ‘…ğ‘‡, for any ğ‘‡ âŠ† [ğ‘›], is the ğ‘‘3 Ã— |ğ‘‡ |(ğ‘‘ âˆ’ 1) matrix with |ğ‘‡ | blocks of columns, given by

{ğ‘…ğ‘– }ğ‘–âˆˆğ‘‡.

13. ğ‘… = ğ‘…[ğ‘›] âˆˆ â„ğ‘‘3Ã—ğ‘›(ğ‘‘âˆ’1) contains all blocks of columns ğ‘…ğ‘–.

8.2 Fourth Moment Identities

Fact 8.10.

(cid:0)ğ”¼ ğ‘ğ‘(cid:62) âŠ— ğ‘ğ‘(cid:62)(cid:1) âˆ’1/2

= ğ‘‘1Î sym âˆ’ ğ‘‘2Î¦Î¦(cid:62)

and for any unit ğ‘¥ and matrix ğ‘‹,

ğ´âˆ’1/2(ğ‘¥ âŠ— ğ‘¥ âŠ— ğ‘‹) = [ğ‘‘1(ğ‘¥ âŠ— ğ‘¥) âˆ’ ğ‘‘2Î¦] âŠ— ğ‘‹ .

Proof. The ï¬rst statement follows from Fact C.4 in [HSSS16]. For the second, notice that
ğ´ = (ğ”¼ ğ‘ğ‘(cid:62) âŠ— ğ‘ğ‘(cid:62))âˆ’1/2 âŠ— Id, so

ğ´âˆ’1/2 =

(cid:34)(cid:114) ğ‘‘2 + 2ğ‘‘
2

Î sym +

(cid:32)

1 âˆ’

(cid:33)

(cid:114) ğ‘‘ + 2
2

1
âˆš

ğ‘‘

(cid:35)

Î¦Î¦(cid:62)

âŠ— Id .

So we can expand ğ´âˆ’1/2(ğ‘¥ âŠ— ğ‘¥ âŠ— ğ‘Œ) as

(cid:34)(cid:114) ğ‘‘2 + 2ğ‘‘
2

Î sym(ğ‘¥ âŠ— ğ‘¥) +

(cid:32)

1 âˆ’

(cid:33)

(cid:114) ğ‘‘ + 2
2

1
âˆš

ğ‘‘

(cid:35)

Î¦Î¦(cid:62)(ğ‘¥ âŠ— ğ‘¥)

âŠ— ğ‘‹ .

Since Î sym(ğ‘¥ âŠ— ğ‘¥) = (ğ‘¥ âŠ— ğ‘¥) and Î¦(cid:62)(ğ‘¥ âŠ— ğ‘¥) = (cid:107)ğ‘¥(cid:107)2 = 1, this simplifes to

(cid:34)(cid:114) ğ‘‘2 + 2ğ‘‘
2

(ğ‘¥ âŠ— ğ‘¥) +

(cid:32)

1 âˆ’

(cid:114) ğ‘‘ + 2
2

1
âˆš

ğ‘‘

(cid:33)

(cid:35)

Î¦

âŠ— ğ‘‹ .

8.3 Matrix Product Identities

Fact 8.11. ğ‘†ğ‘– = (ğ‘‘1(ğ‘ğ‘– âŠ— ğ‘ğ‘–) âˆ’ ğ‘‘2Î¦) âŠ— ğµğ‘–.

Proof. Follows from the deï¬nition of ğ‘†ğ‘– and Fact 8.10.

(cid:3)

(cid:3)

Fact 8.12. ğ‘†(cid:62)
ğ‘–

ğ‘†ğ‘— = (ğ‘‘2
1

(cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)2 âˆ’ 2ğ‘‘1ğ‘‘2 + ğ‘‘2

2ğ‘‘)ğµ(cid:62)

ğ‘–

ğµğ‘—

40

Proof. Expanding via Fact 8.11,

ğ‘– ğ‘†ğ‘— = [(ğ‘‘1(ğ‘ğ‘– âŠ— ğ‘ğ‘–) âˆ’ ğ‘‘2Î¦) âŠ— ğµğ‘–](cid:62)[(ğ‘‘1(ğ‘ ğ‘— âŠ— ğ‘ ğ‘—) âˆ’ ğ‘‘2Î¦) âŠ— ğµğ‘—]
ğ‘†(cid:62)

= (ğ‘‘2

1 (cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)2 âˆ’ 2ğ‘‘1ğ‘‘2 + ğ‘‘2

2ğ‘‘)ğµ(cid:62)

ğ‘– ğµğ‘— .

Here we have used that Î¦(cid:62)(ğ‘ğ‘– âŠ— ğ‘ğ‘–) = (cid:107)ğ‘ğ‘– (cid:107)2 = 1 and that Î¦(cid:62)Î¦ = (cid:107)Î¦(cid:107)2 = ğ‘‘.

Fact 8.13. ğ‘ƒ = ğ‘ƒ(cid:62) = ğ‘ƒâˆ’1 and hence ğ‘ƒ2 = Id

Proof. Exercise.

(cid:3)

(cid:3)

Fact 8.14. For any matrics ğµ, ğµ(cid:48) âˆˆ â„ğ‘‘Ã—ğ‘š, we have (Î¦ âŠ— ğµ)(cid:62)ğ‘ƒ(Î¦ âŠ— ğµ(cid:48)) = ğµ(cid:62)ğµ(cid:48)
Proof. The ğ‘ , ğ‘¡-th entry of (Î¦ âŠ— ğµ)(cid:62)ğ‘ƒ(Î¦ âŠ— ğµ(cid:48)) is given by (cid:205)ğ‘¢ğ‘£ğ‘¤(cid:54)ğ‘‘ Î¦ğ‘¢ğ‘£ğµğ‘ ğ‘¤Î¦ğ‘¢ğ‘¤ğµ(cid:48)
ğ‘¡ğ‘£. The only
nonzero terms come from ğ‘¢ = ğ‘£ = ğ‘¤, because otherise Î¦ğ‘¢ğ‘£Î¦ğ‘¢ğ‘¤ = 0. So this simplifes to
(cid:205)ğ‘¢(cid:54)ğ‘‘ ğµğ‘ ğ‘¢ğµ(cid:48)
(cid:3)

ğ‘¡ğ‘¢ = (ğµ(cid:62)ğµ(cid:48))ğ‘ ğ‘¡.

Fact 8.15. For any vectors ğ‘, ğ‘(cid:48) âˆˆ â„ğ‘‘ and matrices ğµ, ğµ(cid:48) âˆˆ â„ğ‘‘Ã—(ğ‘‘âˆ’1), we have (ğ‘ âŠ— ğ‘ âŠ— ğµ)(cid:62)ğ‘ƒ(ğ‘(cid:48) âŠ—
ğ‘(cid:48) âŠ— ğµ(cid:48)) = (cid:104)ğ‘, ğ‘(cid:48)(cid:105)(ğµ(cid:62)ğ‘(cid:48))([ğµ(cid:48)](cid:62)ğ‘)(cid:62)

Proof. Since ğ‘ƒ does not touch the ï¬rst mode of (ğ‘…ğ‘‘)âŠ—3 it is enough to compute (ğ‘ âŠ— ğµ)(cid:62)ğ‘ƒ(cid:48)(ğ‘(cid:48) âŠ—
ğµ(cid:48)) where ğ‘ƒ(cid:48) is the mode-swap matrix for (â„ğ‘‘)âŠ—2. The ğ‘ , ğ‘¡-th entry of this matrix is given by

(cid:213)

ğ‘¢ğ‘£(cid:54)ğ‘‘

ğ‘ğ‘¢ğµğ‘ ğ‘£ ğ‘(cid:48)

ğ‘£ğµ(cid:48)

ğ‘¡ğ‘¢ = ((cid:213)

ğ‘ğ‘£ğµ(cid:48)

ğ‘¡ğ‘¢)((cid:213)

ğ‘(cid:48)
ğ‘£ğµğ‘ ğ‘£) .

ğ‘¢(cid:54)ğ‘‘

ğ‘£(cid:54)ğ‘‘

(cid:3)

Fact 8.16. For any vector ğ‘ âˆˆ â„ğ‘‘ and matrices ğµ, ğµ(cid:48) âˆˆ â„ğ‘‘Ã—ğ‘š, we have (ğ‘ âŠ— ğ‘ âŠ— ğµ)(cid:62)ğ‘ƒ(Î¦ âŠ— ğµ(cid:48)) =
(ğµ(cid:62)ğ‘)([ğµ(cid:48)](cid:62)ğ‘)(cid:62)

Proof. The ğ‘ , ğ‘¡-th entry of the product is given by

(cid:213)

ğ‘¢ğ‘£ğ‘¤(cid:54)ğ‘‘

ğ‘ğ‘¢ ğ‘ğ‘£ğµğ‘ ğ‘¤Î¦ğ‘¢ğ‘¤ğµ(cid:48)

ğ‘¡ğ‘£ =

(cid:213)

ğ‘¢ğ‘£

ğ‘ğ‘¢ ğ‘ğ‘£ğµğ‘ ğ‘¢ğµ(cid:48)

ğ‘¡ğ‘£ = (ğµ(cid:62)ğ‘)ğ‘ ([ğµ(cid:48)](cid:62)ğ‘)ğ‘¡

Fact 8.17. ğ‘†(cid:62)
ğ‘–

2ğµ(cid:62)
Proof. Expanding ğ‘†ğ‘– , ğ‘†ğ‘— using Fact 8.11, we obtain

(cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)(ğµ(cid:62)
ğ‘–

ğ‘ƒğ‘†ğ‘— = ğ‘‘2
1

ğ‘ğ‘–)(cid:62) + ğ‘‘2

ğ‘ ğ‘—)(ğµ(cid:62)
ğ‘—

ğ‘–

ğµğ‘—.

(cid:3)

ğ‘– ğ‘ƒğ‘†ğ‘— = [ğ‘‘1(ğ‘ğ‘– âŠ— ğ‘ğ‘–) âŠ— ğµğ‘–](cid:62)ğ‘ƒ[ğ‘‘1(ğ‘ ğ‘— âŠ— ğ‘ ğ‘—) âŠ— ğµğ‘—]
ğ‘†(cid:62)
âˆ’ [ğ‘‘1(ğ‘ğ‘– âŠ— ğ‘ğ‘–) âŠ— ğµğ‘–](cid:62)ğ‘ƒ[ğ‘‘2Î¦ âŠ— ğµğ‘—]
âˆ’ [ğ‘‘2Î¦ âŠ— ğµğ‘–](cid:62)ğ‘ƒ[ğ‘‘1(ğ‘ ğ‘— âŠ— ğ‘ ğ‘—) âŠ— ğµğ‘—]

41

+ [ğ‘‘2Î¦ âŠ— ğµğ‘–](cid:62)ğ‘ƒ[ğ‘‘2Î¦ âŠ— ğµğ‘—]

Simplifying the terms individually using Fact 8.14, Fact 8.15, and Fact 8.16,

(ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— ğµğ‘–)(cid:62)ğ‘ƒ(ğ‘ ğ‘— âŠ— ğ‘ ğ‘— âŠ— ğµğ‘—) = (cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)(ğµ(cid:62)
(Î¦ âŠ— ğµğ‘–)(cid:62)ğ‘ƒ(Î¦ âŠ— ğµğ‘—) = ğµ(cid:62)

ğ‘– ğµğ‘—

ğ‘– ğ‘ ğ‘—)(ğµ(cid:62)

ğ‘— ğ‘ğ‘–)(cid:62)

(ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— ğµğ‘–)(cid:62)ğ‘ƒ(Î¦ âŠ— ğµğ‘—) = 0
(Î¦ âŠ— ğµğ‘–)(cid:62)ğ‘ƒ(ğ‘ ğ‘— âŠ— ğ‘ ğ‘— âŠ— ğµğ‘—) = 0 .

where the last two equalities follow from ğµ(cid:62)
ğ‘–

ğ‘ğ‘– = 0, ğµ(cid:62)
ğ‘—

ğ‘ ğ‘— = 0.

(cid:3)

Fact 8.18. ğ‘…(cid:62)
ğ‘–
particular, ğ‘…(cid:62)
ğ‘–

ğ‘… ğ‘— = 2(ğ‘‘2
1
ğ‘…ğ‘– = 2(ğ‘‘2
1

(cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)2 âˆ’ 2ğ‘‘1ğ‘‘2 + ğ‘‘2
2
âˆ’ 2ğ‘‘1ğ‘‘2 + ğ‘‘2
2

(ğ‘‘ âˆ’ 1))Id.

(ğ‘‘ âˆ’ 1))ğµ(cid:62)
ğ‘–

ğµğ‘— âˆ’ 2ğ‘‘2
1

(cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)(ğµ(cid:62)
ğ‘–

ğ‘ ğ‘—)(ğµ(cid:62)
ğ‘—

ğ‘ğ‘–)(cid:62), and in

Proof. By the deï¬nition of ğ‘…ğ‘– , ğ‘… ğ‘— we expand

ğ‘– ğ‘… ğ‘— = (ğ‘†ğ‘– âˆ’ ğ‘ƒğ‘†ğ‘–)(cid:62)(ğ‘†ğ‘— âˆ’ ğ‘ƒğ‘†ğ‘—) .
ğ‘…(cid:62)

We can expand the product and use Fact 8.13 to get

Applying Fact 8.12 and Fact 8.17, we get

ğ‘– ğ‘… ğ‘— = 2ğ‘†(cid:62)
ğ‘…(cid:62)

ğ‘– ğ‘†ğ‘— âˆ’ 2ğ‘†(cid:62)

ğ‘– ğ‘ƒğ‘†ğ‘— .

ğ‘– ğ‘… ğ‘— = 2(ğ‘‘2
ğ‘…(cid:62)

1 (cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)2 âˆ’ 2ğ‘‘1ğ‘‘2 + ğ‘‘2

2ğ‘‘)ğµ(cid:62)

ğ‘– ğµğ‘— âˆ’ 2ğ‘‘2

1 (cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)(ğµ(cid:62)

ğ‘– ğ‘ ğ‘—)(ğµ(cid:62)

ğ‘— ğ‘ğ‘–)(cid:62) âˆ’ 2ğ‘‘2

2ğµ(cid:62)

ğ‘– ğµğ‘— .

Simplifying ï¬nishes the proof.

(cid:3)

8.4 Naive Spectral Norm Estimate

We will need an upper bound on the spectral norm of the matrix ğ‘…, which we obtain
by a matrix Chernoï¬€ bound. To prove that, we need spectral bounds on certain second
moments.

Fact 8.19. For all ğ‘– (cid:54) ğ‘›,

(cid:107) ğ”¼ ğ‘†ğ‘–ğ‘†(cid:62)

ğ‘– (cid:107) (cid:54) 1 .

Proof. Expanding by the deï¬nition of ğ‘†ğ‘–,

ğ”¼ ğ‘†ğ‘–ğ‘†(cid:62)

ğ‘– = ğ´âˆ’1/2 ğ”¼(ğ‘ğ‘– ğ‘(cid:62)

ğ‘– âŠ— ğ‘ğ‘– ğ‘(cid:62)

ğ‘– âŠ— ğµğ‘–ğµ(cid:62)

ğ‘– )ğ´âˆ’1/2 (cid:22) ğ´âˆ’1/2(ğ”¼ ğ‘ğ‘– ğ‘(cid:62)

ğ‘– âŠ— ğ‘ğ‘– ğ‘(cid:62)

ğ‘– âŠ— Id)ğ´âˆ’1/2 (cid:22) Id.

since ğµğ‘–ğµ(cid:62)

ğ‘– = Id âˆ’ ğ‘ğ‘– ğ‘(cid:62)

ğ‘–

(cid:22) Id. The last equality uses the deï¬nition of ğ´.

(cid:3)

42

Fact 8.20. For all ğ‘– (cid:54) ğ‘›,

(cid:107) ğ”¼ ğ‘…ğ‘–ğ‘…(cid:62)

ğ‘– (cid:107) (cid:54) 4 .

Proof. Follows from Fact 8.19 and the deï¬nition of ğ‘…ğ‘–, and the fact that (cid:107)ğ‘ƒ(cid:107) (cid:54) 1, by the
manipulations

(cid:107) ğ”¼(ğ‘…ğ‘–ğ‘…(cid:62)

ğ‘– )(cid:107) = (cid:107) ğ”¼ ğ‘†ğ‘–ğ‘†(cid:62)

ğ‘– âˆ’ ğ‘ƒğ‘†ğ‘–ğ‘†(cid:62)

ğ‘– âˆ’ ğ‘†ğ‘–ğ‘†(cid:62)

ğ‘– ğ‘ƒ + ğ‘ƒğ‘†ğ‘–ğ‘†(cid:62)

ğ‘– ğ‘ƒ(cid:107) (cid:54) 4(cid:107) ğ”¼ ğ‘†ğ‘–ğ‘†(cid:62)

ğ‘– (cid:107) (cid:54) 4 .

Fact 8.21. (cid:107) ğ”¼ ğ‘…ğ‘…(cid:62)(cid:107) (cid:54) ğ‘‚(ğ‘›)

Proof. Since ğ”¼ ğ‘…ğ‘…(cid:62) = ğ”¼ (cid:205)ğ‘–(cid:54)ğ‘› ğ‘…ğ‘–ğ‘…(cid:62)
ğ‘–

, this follows from the triangle inequality and Fact 8.20.
(cid:3)

Fact 8.22. (cid:107) ğ”¼ ğ‘…(cid:62)ğ‘…(cid:107) (cid:54) 2(ğ‘‘2
1

âˆ’ 2ğ‘‘1ğ‘‘2 + ğ‘‘2
2

(ğ‘‘ âˆ’ 1)) (cid:54) ğ‘‚(ğ‘‘2)

(cid:3)

Proof. ğ‘…(cid:62)ğ‘… is a block matrix with ğ‘–ğ‘—-th block being ğ‘…(cid:62)
ğ‘–
since ğ‘…(cid:62)
ğ‘–
2(ğ‘‘2
1

(ğ‘‘ âˆ’ 1))Id. So, ğ”¼ ğ‘…(cid:62)ğ‘… = 2(ğ‘‘2
1

âˆ’ 2ğ‘‘1ğ‘‘2 + ğ‘‘2
2

âˆ’ 2ğ‘‘1ğ‘‘2 + ğ‘‘2
2

ğ‘… ğ‘— is independent of ğ‘… ğ‘— and has expectation zero. At the same time ğ‘…(cid:62)
ğ‘–

ğ‘… ğ‘—. If ğ‘– â‰  ğ‘— we have ğ”¼ ğ‘…(cid:62)
ğ‘–

(ğ‘‘ âˆ’ 1))Id.

ğ‘… ğ‘— = 0,
ğ‘…ğ‘– =
(cid:3)

Fact 8.23. ğ”¼ (cid:107)ğ‘…(cid:107) (cid:54) ğ‘‚(log ğ‘‘ Â· max(ğ‘‘,

âˆš

ğ‘›)).

Proof. First of all, note that ğ”¼ ğ‘… = 0, because ğ”¼(ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— ğµğ‘–) = ğ”¼ğ‘ ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— (ğ”¼[ğµğ‘– | ğ‘ğ‘–]) = 0,
since each column of ğµğ‘– is a random unit vector in the orthogonal complement of ğ‘ğ‘–.

Also, note that with probability 1, each of the column blocks ğ‘…ğ‘– has (cid:107)ğ‘…ğ‘– (cid:107) (cid:54) ğ‘‚(ğ‘‘),

because (cid:107)ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— ğµğ‘– (cid:107) (cid:54) 1 and (cid:107)ğ´âˆ’1/2(cid:107) (cid:54) ğ‘‚(ğ‘‘).

The matrix ğ‘… is a sum of independent random matrices. To apply Matrix Chernoï¬€, we
need the bounds on its second moments, provided by Fact 8.21 and Fact 8.22. The proof is
(cid:3)
concluded by applying Matrix Chernoï¬€.

8.5 Oï¬€-Diagonal Second Moment Estimates

We will eventually need to bound the norms of some oï¬€-diagonal blocks of the matrix
ğ‘…(cid:62)ğ‘…. We prove some useful inequalities for that eï¬€ort now.

Fact 8.24. Let ğ‘‹ , ğ‘Œ be real matrices. Then (ğ‘‹ âˆ’ ğ‘Œ)(ğ‘‹ âˆ’ ğ‘Œ)(cid:62) (cid:22) 2(ğ‘‹ğ‘‹(cid:62) + ğ‘Œğ‘Œ(cid:62)).

Proof. By expanding, (ğ‘‹ âˆ’ğ‘Œ)(ğ‘‹ âˆ’ğ‘Œ)(cid:62) = ğ‘‹ğ‘‹(cid:62)+ğ‘Œğ‘Œ(cid:62)âˆ’ğ‘‹ğ‘Œ(cid:62)âˆ’ğ‘‹ğ‘Œ(cid:62). Since (ğ‘‹ +ğ‘Œ)(ğ‘‹ +ğ‘Œ)(cid:62) (cid:23) 0,
we obtain ğ‘‹ğ‘‹(cid:62) + ğ‘Œğ‘Œ(cid:62) (cid:23) âˆ’ğ‘‹ğ‘Œ(cid:62) âˆ’ ğ‘Œğ‘‹(cid:62), ï¬nishing the proof.
(cid:3)

Fact 8.25. For any ğ‘† âŠ† [ğ‘›] and ï¬xed ğ‘ğ‘– for ğ‘– âˆ‰ ğ‘†, we have (cid:107) ğ”¼ğ‘…ğ‘† ğ‘…(cid:62)
ğ‘†

ğ‘…ğ‘†ğ‘…(cid:62)
ğ‘†

ğ‘…ğ‘† (cid:107) (cid:54) ğ‘‚(|ğ‘†| Â· (cid:107)ğ‘…ğ‘† (cid:107)2).

43

Proof. We know from Fact 8.20 that ğ”¼ ğ‘…ğ‘–ğ‘…(cid:62)
ğ‘–
To prove the ï¬nal bound we push the expectation inside the matrix product:

(cid:22) 4Id. Hence ğ”¼ ğ‘…ğ‘†ğ‘…(cid:62)

ğ‘† = (cid:205)ğ‘–âˆˆğ‘† ğ”¼ ğ‘…ğ‘–ğ‘…(cid:62)

ğ‘–

(cid:22) 4|ğ‘†|Id.

ğ‘…(cid:62)
ğ‘†

ğ”¼
ğ‘…ğ‘†

ğ‘…ğ‘†ğ‘…(cid:62)

ğ‘† ğ‘…ğ‘† = ğ‘…(cid:62)

ğ‘†

(ğ”¼
ğ‘…ğ‘†

ğ‘…ğ‘†ğ‘…(cid:62)

ğ‘† )ğ‘…ğ‘† (cid:22) 4|ğ‘†| Â· ğ‘…(cid:62)

ğ‘†

ğ‘…ğ‘† (cid:22) 4 Â· |ğ‘†| Â· (cid:107)ğ‘…ğ‘† (cid:107)2 Â· Id .

Fact 8.26. For any ğ‘† âŠ† [ğ‘›] and ï¬xed ğ‘ğ‘– for ğ‘– âˆ‰ ğ‘†, we have (cid:107) ğ”¼ğ‘…ğ‘† ğ‘…(cid:62)
ğ‘†
ğ‘‚(ğ‘‘3 Â· (cid:107) (cid:205)

(cid:107)).

ğ‘–âˆˆğ‘† ğ‘ğ‘– ğ‘(cid:62)

ğ‘–

(cid:3)

ğ‘…ğ‘†ğ‘…(cid:62)

ğ‘†

ğ‘…ğ‘† (cid:107) (cid:54) ğ‘‚(|ğ‘†| Â· ğ‘‘2) +

Proof. Consider the ğ‘—, ğ‘˜-th block of ğ”¼ğ‘…ğ‘† ğ‘…(cid:62)
ğ‘†

ğ‘…ğ‘†ğ‘…(cid:62)

ğ‘†

ğ‘…ğ‘†, which expands to

ğ”¼
ğ‘… ğ‘— ,ğ‘…ğ‘˜

(cid:213)

ğ‘–âˆˆğ‘†

ğ‘…(cid:62)

ğ‘— ğ‘…ğ‘–ğ‘…(cid:62)

ğ‘– ğ‘…ğ‘˜ .

Since ğ”¼ ğ‘… ğ‘— = ğ”¼ ğ‘…ğ‘˜ = 0, unless ğ‘— = ğ‘˜ the whole expression vanishes in expectation. Consider
the case ğ‘— = ğ‘˜. Here we have the matrix (cid:205)
ğ‘…ğ‘–
according to Fact 8.18 to get

ğ‘… ğ‘—. We expand the matrix ğ‘…(cid:62)
ğ‘—

ğ‘–âˆˆğ‘† ğ”¼ ğ‘…(cid:62)

ğ‘…ğ‘–ğ‘…(cid:62)
ğ‘–

ğ‘—

ğ‘…(cid:62)

ğ‘— ğ‘…ğ‘– = 2(ğ‘‘2

1 (cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)2 âˆ’ 2ğ‘‘1ğ‘‘2 + ğ‘‘2

(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

2(ğ‘‘ âˆ’ 1))ğµ(cid:62)
ğ‘— ğµğ‘–
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:125)

(cid:124)

(cid:123)(cid:122)
def
= ğ‘‹ğ‘–ğ‘—

âˆ’ 2ğ‘‘2

1 (cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)(ğµ(cid:62)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:123)(cid:122)
def
= ğ‘Œğ‘–ğ‘—

(cid:124)

ğ‘– ğ‘ ğ‘—)(cid:62)
ğ‘— ğ‘ğ‘–)(ğµ(cid:62)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:125)

We will need the following two spectral bounds.

(cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)2 âˆ’ 2ğ‘‘1ğ‘‘2 + ğ‘‘2
2

1. (cid:107) ğ”¼ 4(ğ‘‘2
1
We note that ğµ(cid:62)
ğ‘—
By deï¬nition, |2ğ‘‘1ğ‘‘2|, ğ‘‘2
2

(ğ‘‘ âˆ’ 1))2ğµ(cid:62)
ğ‘—
ğµğ‘— (cid:22) Id, so it is enough to bound ğ”¼ 4(ğ‘‘2
1
(cid:54) ğ‘‚(ğ‘‘2), so we have

ğµğ‘— (cid:107) (cid:54) ğ‘‚(ğ‘‘2).

(ğ‘‘ âˆ’ 1) (cid:54) ğ‘‚(ğ‘‘) and ğ‘‘2
1

ğµğ‘–ğµ(cid:62)
ğ‘–

ğµğ‘–ğµ(cid:62)
ğ‘–

(cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)2âˆ’2ğ‘‘1ğ‘‘2+ğ‘‘2
2

(ğ‘‘âˆ’1))2.

ğ”¼ 4(ğ‘‘2

1 (cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)2 âˆ’ 2ğ‘‘1ğ‘‘2 + ğ‘‘2

2(ğ‘‘ âˆ’ 1))2 (cid:54) ğ‘‚(ğ‘‘4) Â· ğ”¼((cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)2 + ğ‘‚(1/ğ‘‘))2 (cid:54) ğ‘‚(ğ‘‘2) .

2. 4ğ‘‘4
1

(cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)2(ğµ(cid:62)
ğ‘—

We note that (ğµ(cid:62)
ğ‘–

ğ‘ ğ‘—)(cid:62)(ğµ(cid:62)
ğ‘–

ğ‘ğ‘–)(ğµ(cid:62)
ğ‘–
ğ‘ ğ‘—)(cid:62)(ğµ(cid:62)
ğ‘–

ğ‘ğ‘–)(cid:62) (cid:22) 4ğ‘‘4
1

ğ‘ ğ‘—)(ğµ(cid:62)
ğ‘—
ğ‘ ğ‘—) = (cid:107)ğµ(cid:62)
ğ‘ ğ‘— (cid:107)2 (cid:54) 1. So,
ğ‘–

(cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)2ğµ(cid:62)
ğ‘—

ğ‘ğ‘– ğ‘(cid:62)
ğ‘–

ğµğ‘— .

4ğ‘‘4

1 (cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)2(ğµ(cid:62)

ğ‘— ğ‘ğ‘–)(ğµ(cid:62)

ğ‘– ğ‘ ğ‘—)(cid:62)(ğµ(cid:62)

ğ‘– ğ‘ ğ‘—)(ğµ(cid:62)

ğ‘— ğ‘ğ‘–)(cid:62) (cid:22) 4ğ‘‘4

1 (cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)2ğµ(cid:62)

ğ‘— ğ‘ğ‘– ğ‘(cid:62)

ğ‘– ğµğ‘— .

We return to bounding (cid:205)
ï¬xed and the expectation is over ğ‘… ğ‘—.

ğ‘–âˆˆğ‘† ğ”¼ ğ‘…(cid:62)

ğ‘…ğ‘–ğ‘…(cid:62)
ğ‘–

ğ‘—

ğ‘… ğ‘—, where we recall that each ğ‘…ğ‘– in the sum is

44

By Fact 8.24,

(cid:213)

ğ‘–âˆˆğ‘†

ğ”¼ ğ‘…(cid:62)

ğ‘— ğ‘…ğ‘–ğ‘…(cid:62)

ğ‘– ğ‘… ğ‘— (cid:22) 2 (cid:213)

ğ”¼ ğ‘‹ğ‘–ğ‘—ğ‘‹(cid:62)

ğ‘–ğ‘— + 2 ğ”¼ ğ‘Œğ‘–ğ‘—ğ‘Œ(cid:62)

ğ‘–ğ‘—

ğ‘–âˆˆğ‘†

(cid:22) 2 (cid:213)

(cid:107) ğ”¼ ğ‘‹ğ‘–ğ‘—ğ‘‹(cid:62)

ğ‘–ğ‘— (cid:107) Â· Id + 2 (cid:213)

ğ‘Œğ‘–ğ‘—ğ‘Œ(cid:62)

ğ‘–ğ‘— by triangle inequality

ğ‘–âˆˆğ‘†

(cid:22) ğ‘‚(|ğ‘†| Â· ğ‘‘2) Â· Id + 2 (cid:213)

ğ‘–âˆˆğ‘†
ğ‘Œğ‘–ğ‘—ğ‘Œ(cid:62)
ğ‘–ğ‘— by (1) above

ğ‘–âˆˆğ‘†
(cid:22) ğ‘‚(|ğ‘†| Â· ğ‘‘2) Â· Id + 2 ğ”¼ ğµğ‘—

(cid:213)

ğ‘–âˆˆğ‘†

4ğ‘‘4

1 (cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)2ğ‘ğ‘– ğ‘(cid:62)

ğ‘– ğµğ‘— by (2) above

(cid:22) ğ‘‚(|ğ‘†| Â· ğ‘‘2) Â· Id + ğ‘‚(ğ‘‘4

1) Â· ğ”¼(cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)2 Â· (cid:107) (cid:213)

ğ‘ğ‘– ğ‘(cid:62)

ğ‘– (cid:107) Â· Id by (cid:107)ğµğ‘— (cid:107) (cid:54) 1

(cid:22) ğ‘‚(|ğ‘†| Â· ğ‘‘2) Â· Id + ğ‘‚(ğ‘‘3) Â· Â·(cid:107) (cid:213)

ğ‘ğ‘– ğ‘(cid:62)

ğ‘– (cid:107) Â· Id by ğ”¼(cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)2 (cid:54) ğ‘‚(1/ğ‘‘)

ğ‘–âˆˆğ‘†

ğ‘–âˆˆğ‘†

(cid:3)

8.6 Matrix Decoupling

Fact 8.27 (Block Matrix Decoupling, similar to Lemma 5.63 of [Ver12]). Let ğ‘… be an ğ‘ Ã— ğ‘›ğ‘š
random matrix, consisting of ğ‘› blocks ğ‘…ğ‘– of dimension ğ‘ Ã— ğ‘š. Suppose that the blocks satisfy
ğ‘…(cid:62)
ğ‘…ğ‘– = Id. For a subset ğ‘† âŠ† [ğ‘›], let ğ‘…ğ‘† âˆˆ â„ğ‘Ã—ğ‘›|ğ‘†| matrix consisting of only the blocks in ğ‘†. Let
ğ‘–
ğ‘‡ âŠ† [ğ‘›] be uniformly random. Then

ğ”¼
ğ‘…

(cid:107)ğ‘…(cid:62)ğ‘… âˆ’ Id(cid:107) (cid:54) 4 ğ”¼
ğ‘…,ğ‘‡

(cid:107)ğ‘…ğ‘‡ ğ‘…(cid:62)

[ğ‘›]\ğ‘‡

(cid:107) .

Proof. Following the argument of Vershynin [Ver12], we note that

(cid:107)ğ‘…(cid:62)ğ‘… âˆ’ Id(cid:107) = | sup
(cid:107)ğ‘¥ (cid:107)=1

(cid:107)ğ‘…ğ‘¥(cid:107)2 âˆ’ 1|

and that for ğ‘¥ = (ğ‘¥1, . . . , ğ‘¥ğ‘›) âˆˆ â„ğ‘›ğ‘š,

(cid:107)ğ‘…ğ‘¥ (cid:107)2 =

(cid:213)

ğ‘– ğ‘…(cid:62)
ğ‘¥(cid:62)

ğ‘– ğ‘…ğ‘– ğ‘¥ğ‘– + (cid:213)

ğ‘– ğ‘…(cid:62)
ğ‘¥(cid:62)

ğ‘– ğ‘… ğ‘— ğ‘¥ ğ‘— = 1 + (cid:213)

ğ‘– ğ‘…(cid:62)
ğ‘¥(cid:62)

ğ‘– ğ‘… ğ‘— ğ‘¥ ğ‘—

ğ‘–âˆˆ[ğ‘›]

ğ‘–â‰ ğ‘—âˆˆ[ğ‘›]

ğ‘–â‰ ğ‘—âˆˆ[ğ‘›]

since ğ‘…ğ‘–ğ‘…(cid:62)

ğ‘– = Id. By scalar decoupling (see Vershynin, Lemma 5.60 [Ver12]), for a uniformly

45

random subset ğ‘‡ âŠ† [ğ‘›],

(cid:213)

ğ‘–â‰ ğ‘—âˆˆ[ğ‘›]

ğ‘– ğ‘…(cid:62)
ğ‘¥(cid:62)

ğ‘– ğ‘… ğ‘— ğ‘¥ ğ‘— = 4 ğ”¼
ğ‘‡

(cid:213)

ğ‘– ğ‘…(cid:62)
ğ‘¥(cid:62)

ğ‘– ğ‘… ğ‘— ğ‘¥ ğ‘— .

ğ‘–âˆˆğ‘‡,ğ‘—âˆˆ[ğ‘›]\ğ‘‡

So,

ğ”¼
ğ‘…

(cid:107)ğ‘…(cid:62)ğ‘… âˆ’ Id(cid:107) = ğ”¼
ğ‘…

| sup
(cid:107)ğ‘¥(cid:107)=1

(cid:107)ğ‘…ğ‘¥(cid:107)2 âˆ’ 1|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:213)

ğ‘– ğ‘…(cid:62)
ğ‘¥(cid:62)

ğ‘– ğ‘… ğ‘— ğ‘¥ ğ‘—

ğ‘–âˆˆğ‘‡,ğ‘—âˆˆ[ğ‘›]\ğ‘‡

(cid:213)

ğ‘– ğ‘…(cid:62)
ğ‘¥(cid:62)

ğ‘– ğ‘… ğ‘— ğ‘¥ ğ‘—

ğ‘–âˆˆğ‘‡,ğ‘—âˆˆ[ğ‘›]\ğ‘‡

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= ğ”¼
ğ‘…

(cid:54) 4 ğ”¼
ğ‘…,ğ‘‡

= 4 ğ”¼
ğ‘…,ğ‘‡

4 ğ”¼
ğ‘‡

sup
(cid:107)ğ‘¥(cid:107)=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:107)ğ‘…(cid:62)

sup
(cid:107)ğ‘¥(cid:107)=1

ğ‘‡ ğ‘…[ğ‘›]\ğ‘‡ (cid:107)

where the inequality above follows by Jensenâ€™s inequality.

(cid:3)

8.7 Putting It Together

We are ready to prove Lemma C.1.

Proof of Lemma C.1. We will show that ğ‘…(cid:62)ğ‘… âˆˆ â„ğ‘›(ğ‘‘âˆ’1)Ã—ğ‘›(ğ‘‘âˆ’1) is close to Î˜(ğ‘‘2) Â· Id. The
(ğ‘–, ğ‘—)-th block of ğ‘…(cid:62)ğ‘… is given by ğ‘…(cid:62)
ğ‘–
Using Fact 8.18 and the bounds ğ‘‘1 = Î˜(ğ‘‘), ğ‘‘2 = Î˜(1), we obtain,

ğ‘… ğ‘—. Let us ï¬rst consider the diagonal blocks, ğ‘…(cid:62)
ğ‘–

ğ‘…ğ‘–.

ğ‘…ğ‘–ğ‘…(cid:62)

ğ‘– = Î˜(ğ‘‘) Â· Id .

Next, we bound the norm of the oï¬€-diagonal part of the matrix. Let â„diag be the matrix
equal to ğ‘…(cid:62) only on diagonal blocks and zero elsewhere. We will use matrix decoupling,
Fact 8.27, to bound ğ”¼ (cid:107)ğ‘…(cid:62)ğ‘… âˆ’ â„diag(cid:107). We ï¬nd that for a uniformly random subset ğ‘‡ âŠ† [ğ‘›],

ğ”¼
ğ‘…

(cid:107)ğ‘…(cid:62)ğ‘… âˆ’ ğ‘…diag(cid:107) (cid:54) 4 ğ”¼
ğ‘…,ğ‘‡

(cid:107)ğ‘…(cid:62)

ğ‘‡ ğ‘…ğ‘‡ (cid:107)

Now ï¬x ğ‘‡ âŠ† [ğ‘›] and for ğ‘– âˆˆ ğ‘‡ ï¬x unit vectors ğ‘ğ‘– and orthonormal bases ğµğ‘– to obtain the
ğ‘…ğ‘‡ as a sum of independent matrices, one for each

ğ‘…ğ‘– for ğ‘– âˆˆ ğ‘‡. We regard the matrix ğ‘…(cid:62)
ğ‘‡
ğ‘– âˆˆ ğ‘‡. The ğ‘–-th such matrix ğ‘‰ğ‘– has |ğ‘‡ | blocks; the ğ‘—-th block is ğ‘…(cid:62)
ğ‘—

ğ‘…ğ‘–.

First we note that (cid:107)ğ‘‰ğ‘– (cid:107) (cid:54) ğ‘‚(ğ‘‘) Â· (cid:107)ğ‘…ğ‘‡ (cid:107) with probability one over ğ‘…ğ‘–, since (cid:107)ğ‘…ğ‘– (cid:107) (cid:54) ğ‘‚(ğ‘‘)

with probability 1, by Fact 8.10.

46

Next, we note the bounds on the variance of ğ‘…(cid:62)
ğ‘‡

We conclude that by Matrix Bernstein,

ğ‘…ğ‘‡ aï¬€orded by Fact 8.25 and Fact 8.26.

(cid:107)ğ‘…(cid:62)

ğ‘‡ ğ‘…ğ‘‡ (cid:107) (cid:54) log ğ‘‘ Â· ğ‘‚(max((cid:112)|ğ‘‡ | Â· (cid:107)ğ‘…ğ‘‡ (cid:107), (cid:112)|ğ‘‡ | Â· ğ‘‘, ğ‘‘3/2(cid:107) (cid:213)

ğ‘ğ‘– ğ‘(cid:62)

ğ‘– (cid:107)1/2, ğ‘‘) .

ğ‘–âˆˆğ‘‡

ğ”¼
ğ‘…ğ‘‡

So,

ğ”¼
ğ‘…

(cid:107)ğ‘…(cid:62)ğ‘… âˆ’ ğ‘…diag(cid:107) (cid:54) ğ‘‚(log ğ‘‘) Â· (cid:169)
ğ”¼
(cid:173)
ğ‘‡,ğ‘…
(cid:171)

|ğ‘‡ |1/2 Â· (cid:107)ğ‘…ğ‘‡ (cid:107) + ğ‘‘ ğ”¼

ğ‘‡,ğ‘…

(cid:112)|ğ‘‡ | + ğ‘‘3/2 ğ”¼ (cid:107) (cid:213)

ğ‘–âˆˆğ‘‡

ğ‘ğ‘– ğ‘(cid:62)

ğ‘– (cid:107)1/2 + ğ‘‘(cid:170)
(cid:174)
(cid:172)

Using Fact 8.23, we have ğ”¼ |ğ‘‡ |1/2(cid:107)ğ‘…ğ‘‡ (cid:107) (cid:54)
concentration, ğ”¼ (cid:107) (cid:205)

ğ‘–âˆˆğ‘‡ (cid:107) (cid:54) log ğ‘‘ Â· ğ‘‚(max(1, ğ‘›/ğ‘‘)). So putting it together,

âˆš

ğ‘› Â· ğ‘‚(log ğ‘‘ Â· max(ğ‘‘,

âˆš

ğ‘›)). By standard matrix

(cid:107)ğ‘…(cid:62)ğ‘… âˆ’ ğ‘…diag(cid:107) (cid:54) ğ‘‚(log ğ‘‘)2 Â· max(ğ‘‘

ğ”¼
ğ‘…

âˆš

ğ‘›, ğ‘›, ğ‘‘3/2) .

(cid:3)

8.8 Omitted Proofs

We turn now to the proof of Fact 8.4. The strategy is much the same as the proof
of Lemma C.1, which is in turn an adaptation of an argument due to Vershynin for
concentration of matrices with independent columns [Ver12].

We will use the following simple matrix decoupling inequality, with ğµ being the random

matrix having columns ğ‘âŠ—3

ğ‘–

.

Lemma 8.28 (Matrix decoupling, Lemma 5.63 in [Ver12]). Let ğµ be an ğ‘ Ã— ğ‘› random matrix
whose columns ğµğ‘— satisfy (cid:107)ğµğ‘— (cid:107) = 1. For any ğ‘‡ âŠ† [ğ‘›], let ğµğ‘‡ be the restriction of ğµ to the columns
ğ‘‡. Let ğ‘‡ be a uniformly random set of columns. Then

ğ”¼ (cid:107)ğµ(cid:62)ğµ âˆ’ Id(cid:107) (cid:54) 4 ğ”¼
ğ‘‡

ğ”¼
ğµ

(cid:107)(ğµğ‘‡)(cid:62)ğµ[ğ‘›]\ğ‘‡ (cid:107) .

Proof of Fact 8.4. Let ğµ have columns ğ‘âŠ—3
with the goal of applying Lemma 8.28.

ğ‘–

. Fix ğ‘‡ âŠ† [ğ‘›]. We will bound ğ”¼ (cid:107)(ğµğ‘‡)(cid:62)ğµ[ğ‘›]\ğ‘‡ (cid:107),

For ğ‘– âˆˆ ğ‘‡, the ğ‘–-th row of ğµ(cid:62)
ğ‘‡

ğµ[ğ‘›]\ğ‘‡ has entries (cid:104)ğ‘ ğ‘— , ğ‘ğ‘–(cid:105)3 for ğ‘— âˆ‰ ğ‘‡. Let us temporarily ï¬x

ğ‘ğ‘– for ğ‘– âˆ‰ ğ‘‡; then these rows become independent due to independence of {ğ‘ ğ‘— } ğ‘—âˆˆğ‘‡.

We think of the matrix ğµ(cid:62)
ğ‘‡

only the ğ‘–-th row of the ğ‘–-th matrix ğ‘€ğ‘– is nonzero, and it consists of entries ğ‘€ğ‘–ğ‘— = (ğµ(cid:62)
ğ‘‡
We are going to apply the matrix Bernstein inequality to the sum ğµ(cid:62)
ğ‘‡

ğµ[ğ‘›]\ğ‘‡ as consisting of a sum of |ğ‘‡ | independent matrices where
ğµ[ğ‘›]\ğ‘‡)ğ‘–ğ‘—.

ğµ[ğ‘›]\ğ‘‡ = (cid:205)ğ‘–âˆˆğ‘‡ ğ‘€ğ‘–.

47

To do so, we need to compute the variance of the sum: we need to bound

(cid:32)

ğœ2 = max

ğ”¼

ğ‘€ğ‘– ğ‘€(cid:62)

ğ‘– , ğ”¼

(cid:213)

ğ‘–âˆˆğ‘‡

(cid:33)

ğ‘€(cid:62)

ğ‘– ğ‘€ğ‘–

.

(cid:213)

ğ‘–âˆˆğ‘‡

(Here the expectation is over ğ‘ğ‘– for ğ‘– âˆˆ ğ‘‡; we are conditioning on ğ‘ğ‘– for ğ‘– âˆ‰ ğ‘‡.) For the
former, consider that ğ‘€ğ‘– ğ‘€(cid:62)
ğ‘–

has just one nonzero entry,

(cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)6 (cid:54) ğ‘‚(|[ğ‘›] \ ğ‘‡ |/ğ‘‘3) .

ğ”¼(ğ‘€ğ‘– ğ‘€(cid:62)

ğ‘– )ğ‘–,ğ‘– = ğ”¼

(cid:213)

ğ‘–âˆ‰ğ‘‡

(cid:22) ğ‘‚(|[ğ‘›] \ ğ‘‡ |/ğ‘‘3).

Hence ğ”¼ (cid:205)ğ‘–âˆˆğ‘‡ ğ‘€ğ‘– ğ‘€(cid:62)
ğ‘–

Next we bound ğ”¼ (cid:205)ğ‘–âˆˆğ‘‡ ğ‘€(cid:62)
ğ‘–

and ğ‘ a random unit vector. Then ğ”¼ (cid:205)ğ‘–âˆˆğ‘‡ ğ‘€(cid:62)
ğ‘–

ğ‘€ğ‘– = |ğ‘‡ | Â· ğ”¼ ğ‘Ÿğ‘Ÿ(cid:62). We may compute that

ğ‘€ğ‘–. Let ğ‘Ÿ be a random vector with entries (cid:104)ğ‘, ğ‘ğ‘–(cid:105)3 for ğ‘– âˆ‰ ğ‘‡

(ğ”¼ ğ‘Ÿğ‘Ÿ(cid:62))ğ‘–ğ‘— = ğ”¼(cid:104)ğ‘, ğ‘ğ‘–(cid:105)3(cid:104)ğ‘, ğ‘ ğ‘—(cid:105)3 =

ğ¶
ğ‘‘3

(cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105) +

1
ğ‘‘3

Â· ğ‘‚((cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)3)

where ğ¶ is a universal constant. (This may be seen by comparison to the case that ğ‘ is
replaced by a standard Gaussian and using Wickâ€™s theorem on moments of a multivariate
Gaussian.) Letting ğ¶1 = ((cid:205)ğ‘–,ğ‘—âˆ‰ğ‘‡ (cid:104)ğ‘ğ‘– , ğ‘ ğ‘—(cid:105)6/ğ‘‘6)1/2 and ğ¶2 = (cid:107)ğ´[ğ‘›]\ğ‘‡ (cid:107) (where ğ´ has columns
ğ‘ğ‘–), we ï¬nd that ğœ2 (cid:54) ğ‘‚(max(|[ğ‘›] \ ğ‘‡ |/ğ‘‘3, |ğ‘‡ | Â· ğ‘‘âˆ’3 Â· ğ¶2, |ğ‘‡ |ğ¶1)).
By applying Matrix Bernstein, for each ğ‘¡ > 0 we obtain

ğ”¼

(cid:13)
(cid:13)
(cid:213)
(cid:13)
(cid:13)
(cid:13)

ğ‘–âˆˆğ‘‡

ğ‘€ğ‘– Â· 1((cid:107)ğ‘€ğ‘– (cid:107) (cid:54) ğ‘¡)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:54) ğ‘‚(log ğ‘‘) Â· (cid:112)max(|[ğ‘›] \ ğ‘‡ |/ğ‘‘3, |ğ‘‡ | Â· ğ‘‘âˆ’3 Â· ğ¶2, ğ¶1, ğ‘¡2)

where again the expectation is over only ğ‘ğ‘– for ğ‘– âˆˆ ğ‘‡. Choosing ğ‘¡ = Ëœğ‘‚((cid:112)ğ‘›/ğ‘‘3), by standard
scalar concentration we obtain that â„™((cid:107)ğ‘€ğ‘– (cid:107) > ğ‘¡) (cid:54) (ğ‘‘ğ‘›)âˆ’ğœ”(1). Hence by Cauchy-Schwarz
we ï¬nd ğ”¼ (cid:107) (cid:205)ğ‘–âˆˆğ‘‡ ğ‘€ğ‘–(1 âˆ’ 1((cid:107)ğ‘€ğ‘– (cid:107) (cid:54) ğ‘¡))(cid:107) (cid:54) (ğ‘‘ğ‘›)âˆ’ğœ”(1), so all in all,

ğ”¼

(cid:13)
(cid:13)
(cid:213)
(cid:13)
(cid:13)
(cid:13)

ğ‘–âˆˆğ‘‡

ğ‘€ğ‘–

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:54) (log ğ‘›ğ‘‘)ğ‘‚(1) Â· (cid:112)max(|[ğ‘›] \ ğ‘‡ |/ğ‘‘3, |ğ‘‡ | Â· ğ‘‘âˆ’3 Â· ğ¶2, |ğ‘‡ |ğ¶1, ğ‘›/ğ‘‘3) .

Finally, we have to bound

(cid:112)max(|[ğ‘›] \ ğ‘‡ |/ğ‘‘3, |ğ‘‡ | Â· ğ‘‘âˆ’3 Â· ğ¶2, |ğ‘‡ |ğ¶1, ğ‘›/ğ‘‘3)

ğ”¼
ğ‘‡

ğ”¼
ğ‘ğ‘– ,ğ‘–âˆ‰ğ‘‡

which is an upper bound on ğ”¼ (cid:107)ğµ(cid:62)
ğ‘‡

ğµ[ğ‘›]\ğ‘‡ (cid:107). By Cauchy-Schwarz, we may upper bound this

48

by

(ğ”¼ |[ğ‘›] \ ğ‘‡ |/ğ‘‘3)1/2 + (ğ”¼ |ğ‘‡ | Â· ğ‘‘âˆ’3 Â· ğ¶2)1/2 + (ğ”¼ |ğ‘‡ |ğ¶1)1/2 + (ğ‘›/ğ‘‘3)1/2 .

By standard matrix concentration, ğ”¼ ğ¶2 (cid:54) ğ‘‚(1 + ğ‘›/ğ‘‘). Clearly ğ”¼ |[ğ‘›] \ ğ‘‡ | and ğ”¼ |ğ‘‡ | (cid:54) ğ‘‚(ğ‘›).
Finally, by straightforward computation, ğ”¼ ğ¶1 (cid:54) ğ‘›2/ğ‘‘4.5.

All together, applying Lemma 8.28, we have obtained ğ”¼ (cid:107)ğµ(cid:62)ğµ âˆ’ Id(cid:107) (cid:54) Ëœğ‘‚(ğ‘›/ğ‘‘2) for
(cid:3)

ğ‘› (cid:29) ğ‘‘. Since ğµ(cid:62)ğµ has the same eigenvalues as ğµğµ(cid:62) = ğ‘ˆ, we are done.

Acknowledgements

We thank David Steurer for many helpful conversations regarding the technical content
and presentation of this work.

References

[AABB+07] Evrim Acar, Canan Aykut-Bingol, Haluk Bingol, Rasmus Bro, and BÃ¼lent
Yener, Multiway analysis of epilepsy tensors, Bioinformatics 23 (2007), no. 13,
i10â€“i18. 1

[AGH+14] Animashree Anandkumar, Rong Ge, Daniel J. Hsu, Sham M. Kakade, and
Matus Telgarsky, Tensor decompositions for learning latent variable models, Journal
of Machine Learning Research 15 (2014), no. 1, 2773â€“2832. 1, 6, 7

[AGHK13] Animashree Anandkumar, Rong Ge, Daniel J. Hsu, and Sham Kakade, A
tensor spectral approach to learning mixed membership community models, COLT,
JMLR Workshop and Conference Proceedings, vol. 30, JMLR.org, 2013,
pp. 867â€“881. 3

[AGJ14a]

[AGJ14b]

Anima Anandkumar, Rong Ge, and Majid Janzamin, Analyzing tensor power
method dynamics: Applications to learning overcomplete latent variable models,
CoRR abs/1411.1488 (2014). 2

Animashree Anandkumar, Rong Ge, and Majid Janzamin, Guaranteed
non-orthogonal tensor decomposition via alternating rank-1 updates, CoRR
abs/1402.5180 (2014). 7

[AGJ17]

, Analyzing tensor power method dynamics in overcomplete regime, Journal

of Machine Learning Research 18 (2017), 22:1â€“22:40. 6, 7, 54, 55

49

[AZL16]

Zeyuan Allen-Zhu and Yuanzhi Li, Lazysvd: Even faster svd decomposition yet
without agonizing pain, Advances in Neural Information Processing Systems,
2016, pp. 974â€“982. 15, 16, 53

[BCMV14] Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan VÄ³ayaragha-
van, Smoothed analysis of tensor decompositions, STOC, ACM, 2014, pp. 594â€“603.
5

[BKS15]

[BS05]

[DK70]

[DLCC07]

Boaz Barak, Jonathan A. Kelner, and David Steurer, Dictionary learning and
tensor decomposition via the sum-of-squares method, STOC, ACM, 2015, pp. 143â€“
151. 3, 7

C.F. Beckmann and S.M. Smith, Tensorial extensions of independent component
analysis for multisubject fmri analysis, NeuroImage 25 (2005), no. 1, 294 â€“ 311. 1

Chandler Davis and W. M. Kahan, The rotation of eigenvectors by a perturbation.
III, SIAM J. Numer. Anal. 7 (1970), 1â€“46. MR 0264450 15

Lieven De Lathauwer, Josphine Castaing, and Jean-Franois Cardoso, Fourth-
order cumulant-based blind identiï¬cation of underdetermined mixtures, IEEE
Transactions on Signal Processing 55 (2007), no. 6, 2965â€“2973. 3

[DLDMV96] Lieven De Lathauwer, Bart De Moor, and Joos Vandewalle, Blind source
separation by simultaneous third-order tensor diagonalization, European Signal
Processing Conference, 1996. EUSIPCO 1996. 8th, IEEE, 1996, pp. 1â€“4. 2

[Ela10]

[GHJY15]

Michael Elad, Sparse and redundant representations: From theory to applica-
tions in signal and image processing, 1st ed., Springer Publishing Company,
Incorporated, 2010. 3

Rong Ge, Furong Huang, Chi Jin, and Yang Yuan, Escaping from saddle points
- online stochastic gradient for tensor decomposition, Proceedings of The 28th
Conference on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015
(Peter GrÃ¼nwald, Elad Hazan, and Satyen Kale, eds.), JMLR Workshop and
Conference Proceedings, vol. 40, JMLR.org, 2015, pp. 797â€“842. 7

[GHK15]

Rong Ge, Qingqing Huang, and Sham M. Kakade, Learning mixtures of
Gaussians in high dimensions [extended abstract], STOCâ€™15â€”Proceedings of the
2015 ACM Symposium on Theory of Computing, ACM, New York, 2015,
pp. 761â€“770. MR 3388256 3

[GM15]

Rong Ge and Tengyu Ma, Decomposing overcomplete 3rd order tensors using sum-
of-squares algorithms, APPROX-RANDOM, LIPIcs, vol. 40, Schloss Dagstuhl -
Leibniz-Zentrum fuer Informatik, 2015, pp. 829â€“849. 7, 8

50

[GM17]

[Har70]

[HL13]

[HLMK]

[HS17]

[HSS15]

[HSSS16]

[KB09]

[LCC07]

[LS00]

[MM18]

Rong Ge and Tengyu Ma, On the optimization landscape of tensor decompositions,
Advances in Neural Information Processing Systems, 2017, pp. 3653â€“3663. 6,
7

Richard A Harshman, Foundations of the parafac procedure: Models and conditions
for an" explanatory" multi-modal factor analysis. 2, 9

Christopher J. Hillar and Lek-Heng Lim, Most tensor problems are np-hard, J.
ACM 60 (2013), no. 6, 45:1â€“45:39. 2

Wu Hai-Long, Shibukawa Masami, and Oguma Koichi, An alternating trilinear
decomposition algorithm with application to calibration of HPLC-DAD for simulta-
neous determination of overlapped chlorinated aromatic hydrocarbons, Journal of
Chemometrics 12, no. 1, 1â€“26. 1

Samuel B Hopkins and David Steurer, Eï¬ƒcient bayesian estimation from few
samples: community detection and related problems, Foundations of Computer
Science (FOCS), 2017 IEEE 58th Annual Symposium on, IEEE, 2017, pp. 379â€“
390. 3

Samuel B. Hopkins, Jonathan Shi, and David Steurer, Tensor principal compo-
nent analysis via sum-of-square proofs, COLT, JMLR Workshop and Conference
Proceedings, vol. 40, JMLR.org, 2015, pp. 956â€“1006. 1

Samuel B. Hopkins, Tselil Schramm, Jonathan Shi, and David Steurer, Fast
spectral algorithms from sum-of-squares proofs: tensor decomposition and planted
sparse vectors, STOC, ACM, 2016, pp. 178â€“191. 2, 8, 35, 40

Tamara G. Kolda and Brett W. Bader, Tensor decompositions and applications,
SIAM Review 51 (2009), no. 3, 455â€“500. 6

Lieven De Lathauwer, JosÃ©phine Castaing, and Jean-FranÃ§ois Cardoso, Fourth-
order cumulant-based blind identiï¬cation of underdetermined mixtures, IEEE Trans.
Signal Processing 55 (2007), no. 6-2, 2965â€“2973. 6, 8, 9

Michael S. Lewicki and Terrence J. Sejnowski, Learning overcomplete representa-
tions, Neural Comput. 12 (2000), no. 2, 337â€“365. 3

Marco Mondelli and Andrea Montanari, On the connection between learning
two-layers neural networks and tensor decomposition, CoRR abs/1802.07301 (2018).
3

51

[MSS16]

Tengyu Ma, Jonathan Shi, and David Steurer, Polynomial-time tensor decompo-
sitions with sum-of-squares, FOCS, IEEE Computer Society, 2016, pp. 438â€“446.
2, 3, 4, 5, 6, 7, 8, 10, 12, 55

[RM14]

[SS17]

[SV17]

[Ver12]

[Wey12]

Emile Richard and Andrea Montanari, A statistical model for tensor PCA, NIPS,
2014, pp. 2897â€“2905. 1

Tselil Schramm and David Steurer, Fast and robust tensor decomposition with
applications to dictionary learning, COLT, Proceedings of Machine Learning
Research, vol. 65, PMLR, 2017, pp. 1760â€“1793. 3, 5, 6, 8, 10, 12, 24, 25, 26, 30

Vatsal Sharan and Gregory Valiant, Orthogonalized ALS: A theoretically prin-
cipled tensor decomposition algorithm for practical use, ICML, Proceedings of
Machine Learning Research, vol. 70, PMLR, 2017, pp. 3095â€“3104. 7

Roman Vershynin, Introduction to the non-asymptotic analysis of random matrices,
Compressed sensing, Cambridge Univ. Press, Cambridge, 2012, pp. 210â€“268.
MR 2963170 39, 45, 47

Hermann Weyl, Das asymptotische verteilungsgesetz der eigenwerte linearer
partieller diï¬€erentialgleichungen (mit einer anwendung auf die theorie der hohlraum-
strahlung), Mathematische Annalen 71 (1912), no. 4, 441â€“479. 15

A Tools for analysis and implementation

Lemma (Restatement of Lemma 4.3). For a tensor T âˆˆ (â„[ğ‘‘]2)âŠ—3, suppose that the matrix
ğ‘‡ âˆˆ â„[ğ‘‘]3Ã—[ğ‘‘]3 given by ğ‘‡(ğ‘–,ğ‘–(cid:48),ğ‘—),(ğ‘˜,ğ‘˜(cid:48),ğ‘—(cid:48)) = T(ğ‘–,ğ‘–(cid:48)),(ğ‘—,ğ‘—(cid:48)),(ğ‘˜,ğ‘˜(cid:48)) has a rank-ğ‘› decomposition ğ‘‡ = ğ‘ˆğ‘‰T with
ğ‘ˆ , ğ‘‰ âˆˆ â„ğ‘‘3Ã—ğ‘› and ğ‘› (cid:54) ğ‘‘2. Such a rank decomposition provides an implicit representation of the
tensor T. This implicit representation supports:
Tensor contraction: For vectors ğ‘¥, ğ‘¦ âˆˆ â„[ğ‘‘]2, the computation of (ğ‘¥T âŠ— ğ‘¦T âŠ—Id)T or (ğ‘¥T âŠ—IdâŠ— ğ‘¦T)T

or (Id âŠ— ğ‘¥T âŠ— ğ‘¦T)T in time ğ‘‚(ğ‘›ğ‘‘3) to obtain an output vector in â„ğ‘‘2.

Spectral truncation: For ğ‘… âˆˆ â„ğ‘‘2Ã—ğ‘‘4 equal to one of the two matrix reshapings ğ‘‡{1,2}{3} or
(cid:54)1, deï¬ned as T after all larger-than-1 singular
ğ‘‡{2,3}{1} of T, an approximation to the tensor T
values in its reshaping ğ‘… are truncated down to 1. Speciï¬cally, letting ğœŒğ‘˜ be the ğ‘˜th largest
singular value of ğ‘… for ğ‘˜ (cid:54) ğ‘‚(ğ‘›), this returns an implicit representation of a tensor T(cid:48) such
(cid:54)1(cid:107)ğ¹ (cid:54) (1 + ğ›¿)ğœŒğ‘˜ (cid:107)T(cid:107)ğ¹ and the reshaping of T(cid:48) corresponding to ğ‘… has largest
that (cid:107)T(cid:48) âˆ’ T
singular value no more than 1 + (1 + ğ›¿)ğœŒğ‘˜. The representation of T(cid:48) also supports the tensor
contraction, spectral truncation, and implicit matrix multiplication operations, with no more
than a constant factor increase in runtime. This takes time Ëœğ‘‚(ğ‘›2ğ‘‘3 + ğ‘˜(ğ‘›ğ‘‘3 + ğ‘˜ğ‘‘2)ğ›¿âˆ’1/2).

52

Implicit matrix multiplication: For a matrix ğ‘… âˆˆ â„[ğ‘‘]2Ã—[ğ‘‘]2 with rank at most ğ‘‚(ğ‘›), an implicit
representation of the tensor (ğ‘…T âŠ— Id âŠ— Id)T or (Id âŠ— Id âŠ— ğ‘…T)T, in time ğ‘‚(ğ‘›ğ‘‘4). This output
also supports the tensor contraction, spectral truncation, and implicit matrix multiplication
operations, with no more than a constant factor increase in runtime. Multiplication into the
second mode (Id âŠ— ğ‘…T âŠ— Id)T may also be implicitly represented, but without support for the
spectral truncation operation.

Proof.
Tensor contraction We start with multiplication of two vectors ğ‘¥, ğ‘¦ âˆˆ â„ğ‘‘2 into two of the
modes of T. Without loss of generality (by interchange of ğ‘ˆ and ğ‘‰), there are two cases:
we want either to compute the vector ï¬‚attening of (ğ‘¥ âŠ— Idğ‘‘)Tğ‘ˆğ‘‰T(ğ‘¦ âŠ— Idğ‘‘), or, expressing
ğ‘¥ = (cid:205)ğ‘‘
ğ‘–=0 ğ‘Ÿğ‘– âŠ— ğ‘ ğ‘–, we want (cid:205)ğ‘– (Idğ‘‘ âŠ— Idğ‘‘ âŠ— ğ‘Ÿğ‘–)Tğ‘ˆğ‘‰T(ğ‘¦ âŠ— ğ‘ ğ‘–). For both these cases, we ï¬rst
compute ğ‘‰T(ğ‘¦ âŠ— Idğ‘‘).

We compute ğ‘‰T(ğ‘¦ âŠ— Idğ‘‘) as [ğ‘‰T(ğ‘¦ âŠ— Idğ‘‘)]Â·;ğ‘– = ğ‘‰T

Â·;(Â·,Â·,ğ‘–)ğ‘¦. This is a concatenation of ğ‘‘
diï¬€erent matrix-vector multiplications using ğ‘› Ã— ğ‘‘2 matrices, and so it takes ğ‘‚(ğ‘›ğ‘‘3) time.
Then to ï¬nd (ğ‘¥T âŠ— Idğ‘‘)ğ‘ˆğ‘‰T(ğ‘¦ âŠ— Idğ‘‘), we simply repeat the above procedure to ï¬nd

(ğ‘¥T âŠ— Idğ‘‘)ğ‘ˆ and then multiply the ğ‘‘ Ã— ğ‘› and ğ‘› Ã— ğ‘‘ matrices together in ğ‘‚(ğ‘›ğ‘‘2) time.
To ï¬nd (cid:205)ğ‘– (Idğ‘‘ âŠ— Idğ‘‘ âŠ— ğ‘Ÿğ‘–)Tğ‘ˆğ‘‰T(ğ‘¦ âŠ—ğ‘ ğ‘–) after ï¬nding the rank decomposition ğ‘¥ = (cid:205)ğ‘‘
ğ‘–=0 ğ‘Ÿğ‘– âŠ—
ğ‘ ğ‘– which takes ğ‘‚(ğ‘‘3) time by SVD, we multiply each ğ‘ ğ‘– into our computed value of ğ‘‰T(ğ‘¦ âŠ—Idğ‘‘)
to obtain ğ‘‘ diï¬€erent ğ‘›-dimensional vectors ğ‘¡ğ‘– = ğ‘‰T(ğ‘¦ âŠ— ğ‘ ğ‘–). Since there are ğ‘‘ of these vectors
and each is a matrix-vector multiplication with an ğ‘› Ã— ğ‘‘ matrix, this takes ğ‘‚(ğ‘›ğ‘‘2) time.
Then (cid:205)ğ‘– (Idğ‘‘ âŠ— Idğ‘‘ âŠ— ğ‘Ÿğ‘–)Tğ‘ˆğ‘¡ğ‘– can be reshaped as a multiplication of a ğ‘‘2 Ã— ğ‘›ğ‘‘ reshaping of ğ‘ˆ
with the vector (cid:205)ğ‘– ğ‘¡ğ‘– âŠ— ğ‘Ÿğ‘–. It takes ğ‘‚(ğ‘›ğ‘‘3) time to perform the matrix-vector multiplication,
and ğ‘‚(ğ‘›ğ‘‘2) time to sum up (cid:205)ğ‘– ğ‘¡ğ‘– âŠ— ğ‘Ÿğ‘–.

Spectral truncation Next, we truncate the larger-than-1 singular values of the ({1}, {2, 3})
and ({3}, {1, 2}) matrix reshapings ğ‘… âˆˆ â„ğ‘‘2Ã—ğ‘‘4 of T. Without loss of generality, suppose we
are in the ({3}, {1, 2}) case. In this case, we would like to ï¬nd the right-singular vectors and
singular values of the operator that takes ğ‘¦ âˆˆ â„ğ‘‘2 to the vector ï¬‚attening of the ğ‘‘ Ã— ğ‘‘3 matrix
ğ‘ˆğ‘‰T(ğ‘¦ âŠ— Idğ‘‘). Letting ğ‘ be the ğ‘›ğ‘‘ Ã— ğ‘‘2 reshaping of ğ‘‰, this is the same as (ğ‘ˆ âŠ— Idğ‘‘)ğ‘ğ‘¦,
which shares its right-singular vectors with ğ‘€ := ğ‘T(ğ‘ˆTğ‘ˆ âŠ— Idğ‘‘)ğ‘.

We claim that matrix-vector multiplication by ğ‘€ can be implemented in ğ‘‚(ğ‘›ğ‘‘3) time,
with ğ‘‚(ğ‘›2ğ‘‘3) preprocessing time for computing the product ğ‘ˆTğ‘ˆ. The matrix-vector
multiplications by ğ‘ and ğ‘T take time ğ‘‚(ğ‘›ğ‘‘3), and then multiplying ğ‘ğ‘¦ by ğ‘ˆTğ‘ˆ âŠ— Idğ‘‘ is
reshaping-equivalent to multiplying ğ‘ˆTğ‘ˆ into the ğ‘› Ã— ğ‘‘ matrix reshaping of ğ‘ğ‘¦, which
takes ğ‘‚(ğ‘›2ğ‘‘) time with the precomputed ğ‘› Ã— ğ‘› matrix ğ‘ˆTğ‘ˆ. Therefore, LazySVD [AZL16,
Corollary 4.4] takes time Ëœğ‘‚(ğ‘›2ğ‘‘3ğ›¿âˆ’1/2) to yield a rank-ğ‘˜ eigendecomposition ğ‘ƒÎ›ğ‘ƒT such
that (cid:107)ğ‘€1/2 âˆ’ ğ‘ƒÎ›1/2ğ‘ƒT(cid:107) (cid:54) (1 + ğ›¿)ğœŒğ‘˜.

53

To obtain the output T(cid:48) of this procedure, let (ğ‘ƒÎ›1/2ğ‘ƒT âˆ’ Id)>0 be ğ‘ƒÎ›1/2ğ‘ƒT âˆ’ Id with all
of its nonpositive eigenvalues removed: this may be implemented by removing nonpositive
entries from Î›1/2 âˆ’ Id. Then implicitly multiply (Id âˆ’ (ğ‘ƒÎ›1/2ğ‘ƒT âˆ’ Id)>0) into the third
mode of T (although this matrix has rank larger than ğ‘›, we may implement it by implicitly
subtracting (Id âŠ— Id âŠ— (ğ‘ƒÎ›1/2ğ‘ƒT âˆ’Id)>0)T from T). We are trying to approximate multiplying
(Id âˆ’ (ğ‘€1/2 âˆ’ Id)>0) into the third mode of T, so let Î” = (ğ‘€1/2 âˆ’ Id)>0 âˆ’ (ğ‘ƒÎ›1/2ğ‘ƒT âˆ’ Id)>0 be
the diï¬€erence. Then (cid:107)Î”(cid:107) (cid:54) (cid:107)ğ‘€1/2 âˆ’ğ‘ƒÎ›1/2ğ‘ƒT(cid:107) (cid:54) (1+ ğ›¿)ğœŒğ‘˜, so that we suï¬€er an additive error
of at most (1 + ğ›¿)ğœŒğ‘˜ in spectral norm. And the ï¬nal error in the low-rank representation
is (Î” âŠ— Id)ğ‘ˆğ‘‰T. Since (cid:107)Î” âŠ— Id(cid:107) (cid:54) (1 + ğ›¿)ğœŒğ‘˜ and ğ‘ˆğ‘‰T has Frobenius norm (cid:107)T(cid:107)ğ¹, we ï¬nd a
ï¬nal error of (1 + ğ›¿)ğœŒğ‘˜ (cid:107)T(cid:107)ğ¹ in Frobenius norm.

Implicit matrix multiplication Finally, to implicitly multiply a ğ‘‘2 Ã— ğ‘‘2 rank-ğ‘› matrix ğ‘…
into a mode of T, simply store the singular value decomposition ğ‘… = ğ‘ƒÎ£ğ‘„T. Whenever a
vector needs to be multiplied into that mode in the future, multiply that vector by ğ‘… before
carrying out the implicit tensor operation as previously speciï¬ed, and if a vector needs to
be output from that mode, multiply it by ğ‘…T before outputting. This incurs a time cost of
ğ‘‚(ğ‘›ğ‘‘2) per operation.

A special case arises in the spectral truncation operation, where we do not allow implicit
multiplication to have been done in the second mode. Suppose then without loss of
generality that ğ‘… was multiplied into the ï¬rst mode of T and we truncate the ({3}, {1, 2})
matrix reshaping. Then we will have to compute ğ‘ˆT(ğ‘…ğ‘…T âŠ— Idğ‘‘)ğ‘ˆ instead of ğ‘ˆTğ‘ˆ in the
preprocessing step. This can be done by multiplying ğ‘…T = ğ‘„Î£ğ‘ƒT with the ğ‘‘2 Ã— ğ‘›ğ‘‘ reshaping
of ğ‘ˆ, which takes ğ‘‚(ğ‘›2ğ‘‘3) time per future spectral truncation operation.
(cid:3)

B Notes on Table 1

We record a few notes on parameter regimes used to compare various algorithms for tensor
decomposition in Table 1.

â€¢ Robust algorithms with algebraic assumptions often require (cid:107)ğ¸(cid:107) (cid:54) ğœ(ğ‘1, . . . , ğ‘ğ‘›),
where ğœ(ğ‘1, . . . , ğ‘ğ‘›) is some measure of well-conditioned-ness of ğ‘1, . . . , ğ‘ğ‘›, the details
of which may vary from algorithm to algorithm. In this table we report results
for the setting that ğœ(ğ‘1, . . . , ğ‘ğ‘›) (cid:62) Î©(1); such values of ğœ (for all the notions of
well-conditioned-ness represented) are achieved by random ğ‘1, . . . , ğ‘ğ‘›.

â€¢ The algorithm of [AGJ17] is phrased for 3-tensors rathern than 4-tensors; this is
the origin of the rank bound ğ‘› (cid:54) ğ‘‘1.5 rather than ğ‘‘ (cid:54) ğ‘›2 achieved by algorithms
for 4-tensors. In general for ğ‘˜-tensors one expects eï¬ƒcient algorithms to tolerate
overcompleteness ğ‘› (cid:54) ğ‘‘ğ‘˜/2 (despite tensor rank factorizations remaining unique for

54

much larger ğ‘›), so the overcompleteness guarantee of [AGJ17] is comparable to the
other algorithms.

â€¢ We have estimated the running time of the SoS algorithm of [MSS16] by assuming that
the semideï¬nite programs involved are solved using standard black-box techniques
(e.g the ellipsiod method).

C Simulations for condition number of random tensors

In this section we report on computer simulations which strongly suggest that if the
components ğ‘1, . . . , ğ‘ğ‘› are ğ‘› (cid:28) ğ‘‘2 random unit vectors from a variety of ensembles, then
with high probability ğœ…(ğ‘1, . . . , ğ‘ğ‘›) (cid:62) Î©(1). The ensembes include:

1. Spherical measure: ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘ are i.i.d. random unit vectors (see Fig. 1).

2. Sparse: ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘ are sampled i.i.d. by choosing 1

4 ğ‘‘ coordinates in [ğ‘‘] uniformly
at random, sampling each of those coordinates from ğ’©(0, 1), and setting the rest to 0
(see Fig. 2).

3. Hypercube: ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘ are i.i.d. samples from {0, 1}ğ‘‘ (see Fig. 3).

4. Spiked covariance: ğ‘1, . . . , ğ‘ğ‘› âˆˆ â„ğ‘‘ are sampled from ğ’©(0, Id + ğœ† Â· ğ‘¢ğ‘¢(cid:62)) for a random
unit vector ğ‘¢ and ğœ† > 0. We note that in this case, though the covariance matrix
of ğ‘1, . . . , ğ‘ğ‘› has condition number ğ‘‚( 1
ğœ† ), our experimental results support the
hypothesis that ğœ…(ğ‘1, . . . , ğ‘ğ‘›) = Î©(1) for ğœ† as large as ğœ† = 1

2 ğ‘‘ (see Fig. 4).

These ensembles are designed to capture a number of characteristics of real data which we
would like the condition number to be robust to: sparsity, discrete values, and correlations
(of relatively extreme magnitude).

In each of these cases, we computed ğœ… for several values of ğ‘›, ğ‘‘ on with ğ‘1, . . . , ğ‘ğ‘› taken
to be i.i.d. uniformly random unit vectors. Our results are consistent with the hypothesis
that (with high probability) ğœ…(ğ‘1, . . . , ğ‘ğ‘›) (cid:62) ğ‘ âˆ’ Ëœğ‘‚(ğ‘›/ğ‘‘2) for some absolute constant ğ‘ â‰ˆ 1
2.
We expect the values of ğ‘‘, ğ‘› employed here â€“ ğ‘‘ â‰ˆ 10, ğ‘› â‰ˆ 100, so that ğœ… is the condition
number of a certain random matrix of dimensions about 103 Ã— 103 â€“ to be predictive of
the asymptotic behavior of ğœ…(ğ‘1, . . . , ğ‘ğ‘›), because the spectra of random matrices display
strong concentration even in relatively small dimensions.

We also note that the hypothesis that ğœ… > ğ‘ âˆ’ Ëœğ‘‚(ğ‘›/ğ‘‘2) is well supported by the fact
that relatively standard techniques from random matrix theory yield the same bound for
a closely related random matrix to ğ¾(ğ‘1, . . . , ğ‘ğ‘›) from Deï¬nition 1.3. In particular, the
following may be proved by a long but standard calculation, using Matrix Bernstein and
decoupling inequalities:

55

Figure 1: Condition number as a function of dimension ğ‘‘ and lifted overcompleteness
ğ‘›/ğ‘‘2 for vectors sampled from the spherical measure.

Figure 2: Condition number as a function of dimension ğ‘‘ and lifted overcompleteness
ğ‘›/ğ‘‘2 for random sparse vectors.

56

Figure 3: Condition number as a function of dimension ğ‘‘ and lifted overcompleteness
ğ‘›/ğ‘‘2 for vectors sampled from the Boolean hypercube.

Figure 4: Condition number as a function of dimension ğ‘‘ and lifted overcompleteness
ğ‘›/ğ‘‘2 for vectors sampled from ğ’©(0, Id + 1

2 ğ‘‘ Â· ğ‘¢ğ‘¢(cid:62)) for a random unit vector ğ‘¢.

57

Lemma C.1 (Condition number of basic swap matrix). Let ğ‘1, . . . , ğ‘ğ‘› be independent random
ğ‘‘-dimensional unit vectors. Let ğµğ‘– âˆˆ â„(ğ‘‘âˆ’1)Ã—ğ‘‘ be a random basis for the orthogonal complement
of ğ‘ğ‘– in â„ğ‘‘. Let ğ‘ƒ âˆˆ â„ğ‘‘3Ã—ğ‘‘3 be the permutation matrix which swaps second and third modes of
(â„ğ‘‘)âŠ—3. Let

ğ´ = ğ”¼
ğ‘

(ğ‘ âŠ— ğ‘ âŠ— Id)(ğ‘ âŠ— ğ‘ âŠ— Id)(cid:62) .

Let ğ‘… âˆˆ â„ğ‘‘3Ã—ğ‘›(ğ‘‘âˆ’1) have ğ‘› blocks of dimensions ğ‘‘3 Ã— (ğ‘‘ âˆ’ 1), where the ğ‘–-th block is

ğ‘…ğ‘– = ğ´âˆ’1/2(ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— ğµğ‘–) âˆ’ ğ‘ƒğ´âˆ’1/2(ğ‘ğ‘– âŠ— ğ‘ğ‘– âŠ— ğµğ‘–)

where we abuse notation and denote the PSD square root of the pseudoinverse of ğ´ by ğ´âˆ’1/2. Then
there is a function ğ‘‘(cid:48)(ğ‘‘) = Î˜(ğ‘‘2) such that ğ”¼ (cid:107)ğ‘…(cid:62)ğ‘… âˆ’ ğ‘‘(cid:48)(ğ‘‘) Â· Id(cid:107) (cid:54) ğ‘‚(log ğ‘‘)2 Â· max(ğ‘‘
ğ‘›, ğ‘›, ğ‘‘3/2).
In particular, if ğ‘‘ (cid:28) ğ‘› (cid:28) ğ‘‘2,

âˆš

ğ”¼ (cid:107) 1
ğ‘‘(cid:48)(ğ‘‘)

ğ‘…(cid:62)ğ‘… âˆ’ Id(cid:107) (cid:54) ğ‘‚(ğ‘›(log ğ‘‘)2/ğ‘‘2) .

The matrix ğ‘… from this lemma diï¬€ers from ğ¾ only in the use of ğ´âˆ’1/2 in place of
1 ğ»1)âˆ’1/2, (ğ»(cid:62)
(ğ»(cid:62)
2 ğ»2)âˆ’1/2. While we expect ğ´âˆ’1/2 (a non-random matrix) to be close to both
(ğ»(cid:62)
1 ğ»1)âˆ’1/2, (ğ»(cid:62)
2 ğ»2)âˆ’1/2 (at least in subspaces close to ğ¼ğ‘š(ğ»1) and ğ¼ğ‘š(ğ»2), respectively)
establishing this is a challenging task in random matrix theory â€“ in particular, both inverses
of random matrices and spectra of random matrices with dependent entries are notoriously
diï¬ƒcult to analyze. We leave this challenge to future work.

58

