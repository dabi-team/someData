0
2
0
2

c
e
D
0
3

]

R
C
.
s
c
[

1
v
6
1
1
5
1
.
2
1
0
2
:
v
i
X
r
a

IEEE TRANSACTIONS

1

Stack-based Buffer Overﬂow Detection using
Recurrent Neural Networks

William Arild Dahl, L ´aszl ´o Erd ˝odi and Fabio Massimo Zennaro

Abstract—Detecting vulnerabilities in software is a critical challenge in the development and deployment of applications. One of the
most known and dangerous vulnerabilities is stack-based buffer overﬂows, which may allow potential attackers to execute malicious
code. In this paper we consider the use of modern machine learning models, speciﬁcally recurrent neural networks, to detect
stack-based buffer overﬂow vulnerabilities in the assembly code of a program. Since assembly code is a generic and common
representation, focusing on this language allows us to potentially consider programs written in several different programming
languages. Moreover, we subscribe to the hypothesis that code may be treated as natural language, and thus we process assembly
code using standard architectures commonly employed in natural language processing. We perform a set of experiments aimed at
conﬁrming the validity of the natural language hypothesis and the feasibility of using recurrent neural networks for detecting
vulnerabilities. Our results show that our architecture is able to capture subtle stack-based buffer overﬂow vulnerabilities that strongly
depend on the context, thus suggesting that this approach may be extended to real-world setting, as well as to other forms of
vulnerability detection.

Index Terms—Software Vulnerability Detection, Buffer Overﬂow Detection, Stack Buffer Overﬂow, Recurrent Neural Networks, Long
Short-Term Memory.

(cid:70)

1 INTRODUCTION

M ODERN software applications deal with large quan-

tities of data, including critical and sensitive data.
The high demand for services has driven developers to
create software at a fast pace, thus introducing ﬂaws and
bugs in the code. The number of software vulnerabilities
discovered each year has steadily increased over the last
decades, with a record of 16,665 reported discoveries in 2018
[1]. Reducing the number of vulnerabilities and potential
security breaches is a crucial challenge.

Exploiting ﬂaws in code has become a multi-billion
dollar a year business. Malware developed to exploit code
vulnerabilities is an ever-increasing problem for users, cor-
porations, and governments worldwide. When the Shadow
Brokers [2] leaked the EternalBlue exploit [3], which gave
rise to the WannaCry ransomware [4], ensuing damages
have been estimated to be up to 8 billion USD globally.

An approach to reduce the number of vulnerabilities is
based on the use of automatic tools for the identiﬁcation
and the patching of vulnerabilities. In this paper we explore
the use of machine learning techniques for vulnerability
detection. More precisely, we focus on the use of neural
networks to identify the presence of potential stack-based
buffer overﬂow vulnerabilities in assembly code. We make
the assumption that code can be treated as a form of lan-
guage, and we process it using recurrent neural networks
(RNN) based on long short-term memory (LSTM) cells [5].
RNNs are a class of neural networks designed to handle
input vectors of arbitrary lengths, and to extrapolate context
from sequences. Such models were proven to be effective in

tasks related to natural language processing (NLP) and they
can be easily adapted to process code.

To evaluate the effectiveness of RNNs in processing as-
sembly ﬁles in order to spot buffer overﬂow vulnerabilities
we carried out a set of empirical simulations. Our results,
although preliminary, conﬁrm the hypothesis that code may
be dealt as a language, and they corroborate the intuition
that neural network models may be successfully adapted to
carry out vulnerability detection, in particular stack-based
buffer overﬂow.

The rest of the paper is structured as follows. Section 2
brieﬂy introduces relevant concepts from information secu-
rity and from machine learning. Section 3 offers an overview
of the state of the art in the application of machine learning
to vulnerability detection. Section 4 precisely deﬁnes the
problem we aim at tackling, as well as the assumption
we will make in our study. Section 5 provides details on
our methodology, including the process of data generation,
data pre-processing and model design. Section 6 reports a
set of experiments aimed at validating our hypothesis, and
Section 7 offers an overall evaluation of our results. Finally,
Section 8 summarizes our results and suggests possible
future directions of research.

2 BACKGROUND

In this section we provide a brief review of the main
ideas from computer security (buffer overﬂow) and machine
learning (RNNs) that are relevant to our work.

• W.A. Dahl, L. Erd˝odi and F.M. Zennaro are with the Department of

Informatics, University of Oslo, 0316 Oslo, Norway
E-mail: williada@iﬁ.uio.no, laszloe@iﬁ.uio.no, fabiomz@iﬁ.uio.no

Manuscript received:

2.1 Buffer overﬂow

Vulnerabilities in software can have serious consequences
from the point of view of computer security. For instance,

 
 
 
 
 
 
IEEE TRANSACTIONS

2

without proper input data validation or accurate memory
management malicious actors can modify the intended pro-
gram ﬂow of the software, thus leading to arbitrary code
execution. The exploitation of a software vulnerability is
frequently the starting point of complex attacks; for instance,
the vulnerable software can execute a malicious payload
to open a communication channel back to the attacker’s
computer, then download and execute a script from the
attacker’s location. Perfect software would be an ideal so-
lution to protect systems against similar attacks, but such a
possibility is not realistic; based on existing surveys, typical
software vulnerabilities such as buffer overﬂows, use after free,
double free vulnerabilities, access control problems, race conditions
or authentication weaknesses appear frequently in software
products. Despite many defensive measures were taken by
the compilers and the operating systems, several new type
of modern exploitation still appear time by time, e.g. [6]. A
more feasible option to mitigate such risks is to discover
software vulnerabilities as early as possible, for instance
using fuzz testing or code analysis. In this work we focus on
detecting stack-based buffer overﬂows. Finding abnormal
program behavior such as buffer overﬂow errors is crucial
in all computers and devices [7].

An operating system executes binary code in the process
virtual address space. In this space, the operating system
keeps the executable code only of the binary, together with
all the binary code that was linked (e.g., operating system
APIs). In addition to the executable binary code, the vir-
tual address space contains data sections such as the stack
segments for each running thread, heap blocks and global
variables. The partitioning of the memory space allows
the operating system to keep processes separated, and it
guarantees that different running binaries have no direct
access to each other. The weak point of this data structuring
is the fact that the code and the data are together in the same
virtual address space; if there is no appropriate protection in
place, this can lead to different types of memory corruption,
such as executing data as code, or overwriting the code
using data.

Buffer overﬂows compromise the virtual address space
by overﬂowing the storage place that was allocated for the
data. This action can lead to the modiﬁcation of the virtual
address space, for instance, by overwriting the heap chunks
or the stack frames of the running methods inside the binary.
In case of stack-based buffer overﬂows, the return pointer
of a method that is under execution is modiﬁed by over-
running a local variable inside the method. In some simple
cases, stack-based buffer overﬂow may be carried out by ex-
ploiting methods that are vulnerable by default, such as the
C methods gets, puts, strcpy; these methods notoriously lack
input validation, and, if the programmer does not perform
proper size checking, this weakness can be easily used to
successfully overﬂow the buffer. More complex and reﬁned
examples of stack-based buffer overﬂow may happen by
exploiting a chain of several minor vulnerabilities, such as
a chain of wrong input data that the attacker tries to set
intentionally to overrun the vulnerable buffer. Our focus is
mainly on the easiest case where the binary vulnerability
is determined by the use of the aforementioned vulnerable
methods, and where the solution corresponds to detecting
the presence of such method calls without proper input

checking.

2.2 Machine learning

The problem of detecting code that is potentially vulnerable
to stack-based buffer overﬂow can be cast as a machine
learning problem. In other words, we consider the pos-
sibility of training a model which could receive in input
samples of code, and which could produce in output a signal
denoting whether a potential buffer overﬂow vulnerability
is present in the code. Neural networks have proven to be
a powerful and ﬂexible family of models to learn detection
systems; for instance convolutional neural networks (CNNs)
can be very effectively applied to image detection and
recognition [8].

Since our problem requires processing inputs of arbitrary
length (i.e., code made up of a variable number of lines)
and to evaluate chunks of input in context (i.e., deciding
about the vulnerability of certain methods in relation to the
presence of proper input checking), we decided to rely on a
speciﬁc family of neural networks: recurrent neural networks
[9]. A RNN is a model that implements the following
function:

sn = f (xn, sn−1),

where xn represents the nth input (or part of input), and
sn denotes the internal state of the RNN after processing
the nth input. In our case, each input will represent a line
of code in a program, and we will evaluate the last state
sn produced by the RNN to decide if the whole program
contains a buffer overﬂow vulnerability.

A long short-term memory (LSTM) [5] is a speciﬁc vari-
ant of a recurrent neural network which relies on gates to
process and ﬁlter the information. This design choice allows
LSTM to better model long-range dependence in the data.
Such a feature is particularly important in our application
as it allows to capture a wider context for potentially vul-
nerable instructions.

3 RELATED WORK
In this section, we describe state-of-the-art work done on
the problem of vulnerability detection, paying particular
attention to types of data representation and model ar-
chitecture adopted by other researchers. Although work
on vulnerability detection has close connection to malware
detection (see, for reference, recent work such as [10] or [11])
and intrusion detection (refer, for instance, to [12]), we will
focus mainly on vulnerability detection.

In [13], Hovsepyan et al. process Java source code treat-
ing it as plain text, and they train a SVM classiﬁer to decide
on the presence of vulnerabilities in the code. A similar
approach is adopted in [14], where Java source code is
represented via n-grams and synthetic features before being
processed by a SVM trained to detect vulnerable programs.
Representation learning via principal component analysis
has been applied to C source code in order to generate
informative representations for vulnerability detection in
[15]. All these approaches closely resemble our work in
that they subscribe, however implicitly or partially, to the
language;
hypothesis that code can be dealt as natural
however, our work relies on assembly code, making the

IEEE TRANSACTIONS

3

reasonable assumption that source code is often unavailable,
and it applies more versatile models, such as RNNs.

be trained on a substantial amount of data and that can be
deployed in the real world.

The use of CNN and RNN models has been considered
in [16], where Russel et al. trained a vulnerability detection
model on large data sets of real-world C/C++ source code.
Similarly, in [17], a RNN relying on bi-directional LSTM, is
shown to be more effective at vulnerability detection than
other standard pattern-based systems such as Flawﬁnder,
RATS, or Checkmarx. Our approach resembles these studies
in the choice of the family of models we consider (RNNs);
however, once again, we decided not to consider source
code, but, more realistically, assembly code.

A different direction of research has considered ana-
lyzing programs using not only static features, but also
dynamic features, that is, feature generated by a program
at runtime. Grieco et al. proﬁled code for vulnerability
by using a collection of both static and dynamic features
(such as program events), and by training different machine
learning algorithms to learn useful patterns [18]. Wu et
al. developed a model relying only on dynamic features
(such as kernel function calls) and implemented deep neural
networks to carry out vulnerability detection [19]. Our work
does not take into consideration dynamic features, although
the architecture we have chosen would be versatile enough
to be extended in the future to process such features.

4 PROBLEM DEFINITION

In this work, we formulate and address the problem of
discovering stack-based buffer overﬂow vulnerabilities in
a program by processing its assembly code representation
via RNNs.

At its foundation, our work is grounded on the assump-
tion that code constitute a form of language [20]. Code has
a tightly structured syntax, and it can convey meaning in a
similar fashion as written and spoken languages. Through
our experiments, we assess the hypothesis that code may be
processed as a form of language, and that this representation
may be successfully employed to perform vulnearability
detection. To do so, we aim at processing code in the form
of text, with minimal pre-processing based on human prior
knowledge, and using models that proved to be successful
in dealing with natural language processing.

Concerning the speciﬁc form of language considered, we
decided not to focus on one speciﬁc high-level programming
language. Instead we considered code written in assembly
language. Assembly language constitutes a formal language
that provides a middle ground between human-friendly
programming languages and machine instructions. On one
side, programming languages are very succinct and inter-
pretable, but they come with a wide variety of vocabulary
and grammars; models for a given language may hardly be
extended to other languages; moreover, in the real world,
source code of a program may be rarely available. On the
other hand, machine code is heavily hardware dependent
and verbose, as it speciﬁes in ﬁne-grained details elemen-
tary operations to be carried out. The assembly language
offers thus a language that strikes a reasonable balance be-
tween conciseness and availability. Conciseness is important
both to reduce the cost of storing and processing the data;
availability is relevant in order to have models that may

It is worth to underline that, so far, to the best of our
knowledge, limited work has been done to exploit this
speciﬁc level of representation for a program.

5 METHODOLOGY AND IMPLEMENTATION
In this section we discuss our methodological approach and
our practical implementation: how we generated data for
our problem, what sort of processing we performed on
them, and ﬁnally we provide details about the model we
implemented.

Overall, our method is based on the following steps: (i)
generation of a library of safe and vulnerable functions, (ii)
sampling and aggregation of functions into programs, (iii)
compilation of programs into assembly ﬁles, (iv) compres-
sion of the assembly ﬁles by removing redundant informa-
tion, (v) tokenization of the assembly ﬁles, (vi) partitioning
of the samples in training and test data, (vii) training of
the model. This approach provides us with complete con-
trol over every aspect of the simulations, from the data
(size, complexity, and variability) to the model (architecture,
depth, and other training hyper-parameters).

5.1 Data generation

Given the absence of standardized public data sets for stack-
based buffer overﬂows, we ﬁrst considered the challenge of
assembling a suitable data set of vulnerabilities.

Given the availability of code online, a common ap-
proach to collect a data set of vulnerable code is to design
a webscraper to download C source code from open source
projects, like the Linux kernel1, or public repositories, such
as Github2. This approach would return large amounts of
realistic data, although a good subset of the retrieved code
may be umantained and of low quality (e.g., failing to
compile). The main drawback is that all the data would be
unlabelled. Labeling the data, so that it could be used for
supervised learning in our RNN models, would be a non-
trivial challenge; manual labelling would be extremely time-
consuming, requiring several experts to analyze the code to
understand if it has potential buffer overﬂows vulnerabil-
ities or not; automatic labelling using existing tools does,
in a way, defeat the purpose of our research as it would
lead the RNN model to simply learn the function already
encoded in the existing automatic tools. Even more critically,
only a small percentage of the collected data would con-
tain samples of stack-based buffer overﬂow vulnerabilities
which would be relevant in our problem.

Therefore, we opted to create our own dataset. This
approach has some clear advantages: (i) since we deﬁne
each function and program, and we know exactly whether it
is vulnerable to stack-based buffer overﬂow or not, labelling
is automatic and unequivocal; (ii) we have full control over
the code style used, allowing us to produce more or less het-
erogeneous samples; (iii) since the generation is automated,
we can easily scale up the size of the data, thus making the
ﬁtting of complex functions feasible. On the other hand, we

1. https://www.kernel.org/
2. https://github.com/

IEEE TRANSACTIONS

4

acknowledge the fact that custom-made datasets are often
criticized as lacking in realism; while this point is true, we
hold that our samples are representative enough to validate
or to disprove our hypothesis that RNNs may be used
for stack-based buffer overﬂow vulnerability detection. If
we were to prove that our models can successfully detect
unsafe code, more realistic data and deeper network may be
implemented to tackle more realistic challenges.

A sample in our synthetic dataset is a C program ﬁle. A
program ﬁle is made up of a variable number of functions
that are called in the main body of the program ﬁle. In
the next section, we explain how we generate functions and
how we assemble them into programs.

5.1.1 Generating safe and vulnerable functions

We view a function as the atomic component of a C pro-
gram. Each function is built around a system call. Although
there are many potentially vulnerable system calls in the
C library, we restrict our attention to 8 system calls that
are particularly relevant in the context of buffer overﬂows:
strcpy, strncpy, strcat, scanf, sprintf, gets, fgets, memcpy. Some
of these system calls are deemed intrinsically unsafe and
are deprecated, while other C methods are considered safe
and are recommended. However, our labelling does not
depend on this static distinction. We deﬁne a function non-
vulnerable if it correctly uses a safe system call or if it
employs an unsafe system call together with right buffer
checks; vice versa, we deﬁne a function vulnerable if it uses
an unsafe system call with wrong checks or if it misuses a
safe system call. For instance, although strcpy is normally
considered unsafe, we recognize that it may be used safely
when accompanied by proper buffer checks. By taking into
account these more subtle differences, we aim at training a
model that does not just trivially spot the use of deprecated
system calls, but evaluates their vulnerability in context.

For each of the 8 system calls above, we created 15
different functions with a single vulnerable system call. In
total, this leads to 120 different vulnerable functions. These
functions have one and only one vulnerability and use
one and only one weak system call, thus deﬁning simple
functions with a single vulnerability, while preserving the
realism of our data.

For the benign functions, we decided to create an equal
amount of instances. Among these instance, we included
functions properly using safe system calls as well as re-
paired versions of the unsafe functions used to create vul-
nerable functions. Listing 1 presents a vulnerable function
along with its safe counterpart in Listing 2. The two list-
ings show how easy it is to unintentionally introduce a
vulnerability by using the wrong parameter in a C system
call, and how subtle the difference between a safe and an
unsafe function can be. These minimal differences allow us
to verify whether a network is able to discriminate safe and
unsafe programs given the context in which C function calls
happen, and not just by the name of the function called.

Listing 1: A vulnerable function
void memcpySmallIntoLarge ( char * s ) {

char d e s t [ 2 5 6 ] ;
memcpy( dest , s , s t r l e n ( s ) ) ;
p r i n t f (”% s \n ” , d e s t ) ;

}

Listing 2: Repaired version of Listing 1

void memcpySmallIntoLarge ( char * s ) {

char d e s t [ 2 5 6 ] ;
memcpy( dest , s , s i z e o f ( d e s t ) ) ;
p r i n t f (”% s \n ” , d e s t ) ;

}

5.1.2 Generating C programs

From the set of functions we have deﬁned, we generate
programs. We deﬁne positive sample a C program that con-
tains no vulnerable function; conversely, we deﬁne negative
sample a C program that contains at least one vulnerable
function. We acknowledge that functions normally are vul-
nerable through their calling parameters and the state of
the virtual address space. However, we limit ourselves to
deﬁne vulnerable and benign samples without additional
considerations such as those aforementioned.

Given a number Nf of function to be included in a
program, we construct positive samples by randomly sam-
pling Nf safe functions, inserting their deﬁnition into a C
program, and adding function calls in the main() method;
for negative samples, we randomly select a single unsafe
function along with Nf − 1 random safe functions, and
we aggregate them in a C program as done for positive
samples. The resulting programs are carefully checked to
verify whether they are actually safe or unsafe. We followed
a unit testing approach, validating each unit of code through
a range of inputs.

5.2 Data processing

The output of our data generating process is a set of
programs in the form of C source code. Since we do not
want to work at this level of representation, we compile
our data into assembly. Moreover, before feeding the data to
the model, we further process it by removing redundant
information, converting it into token strings, and ﬁnally
partitioning it into training and test data.

5.2.1 Compiling C programs into assembly

First, we translate our source code into assembly code,
using a compiler with Intel syntax for the 64-bit architec-
ture. Compilation happens using the GCC compiler [21],
a versatile and cross-platform compiler widely adopted on
many systems. Our C programs are compiled running the
following command:

gcc -S -fno-asynchronous-unwind-tables

-masm=intel ./fileName.c -o fileName

For the sake of generalization, we rely on a minimal number
of ﬂags aimed at producing as short an assembly as possible;
for details about the ﬂags, please refer to [21]. A snippet of
generated assembly is shown in Listing 3.

Listing 3: Example of part of a C program compiled in
assembly

1 . f i l e ” t e s t
2 . i n t e l s y n t a x n o p r e f i x

f i l e 3 0 . c ”

IEEE TRANSACTIONS

5

t h e s i z e o f

input : ”

. s t r i n g ”%d”

. s t r i n g ” E n t e r

3 . t e x t
4 . glob main
5 . type main , @function
6
7 . LC0 :
8
9 . LC1 :
10
11
12 mov
13 l e a
14 mov
15 c a l l
16
17 push
18 sub
19 mov

rbx ,
r d i ,
eax , 0
printf@PLT

rax
. LC0 [ r i p ]

rbx
rsp , 72
QWORD PTR −104[ rbp ] ,

r d i

This processing step makes our model independent from
the availability of C source code. Real-world executables,
for which the source code has not been released, can still be
decompiled into assembly language. Several tools, such as
IDA Pro by Hex Rays [22], Binary Ninja [23], and NSA’s own
Ghidra [24], allow for decompiling binaries into assembly
code. On the other hand, succesfully reverse engineering
machine code up to C is a more challenging, and still open,
problem. Thus, processing assembly code instead of C codes
makes our model more ﬂexible and generic, potentially able
to deal with a larger class of real-world samples.

5.2.2 Compressing the assembly code

Next, in order to simplify the model and make the learning
process faster, we compress the assembly code by discarding
redundant information.

Several lines of the assembly code generated by the GCC
compiler have limited value, containing either redundant or
useless information for our aims. We then start by removing
the entire preﬁx of each assembly (see lines 1-5 in Listing 3
for an example). The preﬁx is equal for each assembly except
the ﬁeld .ﬁlename which just reports the name of the original
ﬁle; as such, no signiﬁcant information is carried in these
lines of code.

Next, we consider other assembler directives and in-

structions that can be dropped [25]:

• .LC directives are used for storing declared strings
in the C program (see line 7-10 in Listing 3 for an
example). The actual content of a string is not relevant
in our modelling. However, in order to maintain as
much as possible of the code context, notice that we
keep references to the strings (see line 13 in Listing 3
for an example).

• .size directives are generated by compilers to include
auxiliary debugging information in the symbol table.
Compiler information is not relevant in our modelling.
• .ident directives are used by some assemblers to place
tags in object ﬁles. Object generation is not relevant in
our modelling.

• .section directives are used to manage sections in objects.
Object management is not relevant in our modelling.
• endbr instructions [26] in the last part of each assembly

are removed.
The end result is shorter assembly samples that still
preserve the semantics of the code. Notice that in this
processing steps we minimally rely on prior knowledge. An

expert evaluation is indeed needed to decide what parts of
an assembly code to discard. However, we argue that such
choice boils down to a non-case-speciﬁc and procedural
rule that consistently drop pre-determined lines. This pre-
processing does not introduce any form of high-level knowl-
edge, nor does it instantiate any sort of feature carrying
human-injected knowledge. The point of this pre-processing
is simply to reduce the size of the samples in order to limit
the computational burden. It is our conjecture that, if our
model were to be successful, scaled-up models would be
able to successfully process non-compressed versions of the
same assembly.

5.2.3 Tokenization of the assembly code
Following the standard in natural language processsing, we
proceed with a tokenization of the assembly code. Although
the main component of an assembly may be identiﬁed
by a line of code, lines present too much variety to be
reduced to tokens. Instead we split lines of code into atomic
codewords, and we use regular expressions to meaningfully
extract tokens. Regular expressions are designed to preserve
relevant commands and their context, while at the same
time lead to a restricted dictionary of meaningful symbols.
As an example, consider line 19 in Listing 3; its tokenization
returns:

"mov", "QWORD", "PTR", "-104", "[", "]",

",", "rbp", "rdi", "/n"

Notice that in our tokenization we always keep the newline
character ’/n’ to denote the end of lines of code, as we
consider information on the end and the beginning of a line
relevant in our analysis.

5.2.4 Partitioning of the data
At the end of this process, we are left with a data set
where each sample, positive or negative, is a set of assembly
tokens. According to the good practices of machine learning,
we split this data set in a training data set (80% of the
data) and a test data set (20% of the data). Training data
are further partitioned in a set used for training our model
and a development (or validation) set for hyperparameter
optimization. Test data are used only for assessing the
accuracy of our model.

5.3 Model deﬁnition

In order to assess our hypothesis, we adopted a standard
neural network model widely used in NLP and we ap-
plied it to the processing and classiﬁcation of assembly
code. Given the available computational limitation, we im-
plemented an effective, yet lightweight, RNN architecture
designed to discriminate vulnerable and non-vulnerable
samples taking into account both the local and the global
context in each program.

With reference to Figure 1, our architecture is designed as
follows. The model receives as input a tokenized assembly
ﬁle. Each token is mapped to an integer value via a pre-
computed table, thus creating an integer array. This vector
is forwarded to a 512-dimensional embedding layer which
maps the discrete representations into dense representa-
tions. Two hidden LSTM layers follow, with the task of pro-
cessing the data and tracking the inner state of the network;

IEEE TRANSACTIONS

6

temporal dropout is applied to the last layer of these two
layers in order to improve the generalization performance.
Finally, the output of the LSTM layers is forwarded to a
fully-connected sigmoidal layer that produces the decision
of the model. The entire model is trained using a binary
cross-entropy loss [9].

6.1.1 Protocol.
In our ﬁrst simulation we sampled 2000 benign and 2000
vulnerable binaries. Each binary contains 3 functions where
the benign functions are sampled from the set of safe func-
tions excluding repaired versions of vulnerable functions,
while the vulnerable functions are sampled from the whole
set of vulnerable functions.

Training and development were conducted considering
the batch sizes {20, 40, 80, 100}, while the learning rate was
set relatively aggressive considering the values in the set
{0.00025, 0.0005, 0.001, 0.002}.

6.1.2 Results.

Fig. 1: Architecture of our model

Fig. 2: Performance for Simulation 1 using learning rate
10−3 and a batch size of 80.

6 EXPERIMENTS

In this section we present and discuss the simulations we
have done in order to prove or disprove the language
hypothesis applied to assembly code and to verify the
feasibility of detecting stack-based buffer overﬂow using
RNNs.

Our RNN models have been implemented using
PyTorch [27], an open-source machine learning library
for Python. The source code for our model
is avail-
able publicly online at https://github.com/williamadahl/
RNN-for-Vulnerability-Detection.

In all the simulations, we use a training set to train
the model, a development (or validation) set for model
selection, and a test set to evaluate the ﬁnal performance.
We track the loss on training and development set during
training, and we measure the ﬁnal performance in terms of
correct classiﬁcation rate (CCR, or accuracy).

6.1 Simulation 1: Detecting vulnerabilities

In this ﬁrst simulation, we evaluate the network ability
to discriminate between programs composed by safe func-
tions and programs containing different vulnerable func-
tions. Positive and negative samples use clearly different
functions, thus presenting to the network a relatively easy
discrimination task.

We selected the best conﬁguration of hyperparameters
(batch size 80, and learning rate 10−3) in terms of perfor-
mance on the development data set; other conﬁgurations
achieved lower results, and often required more epochs to
achieve convergence.

Figure 2 shows the result of training. Notice the two
scales for the y-axis: on the left we report the correct classi-
ﬁcation rate (CCR) for the training (blue) and the develop-
ment (red) dataset; on the right we report the loss function
for the training dataset (green). At the end of training the
ﬁnal performance on the test set successfully converges to a
CCR of 1.0.

Even with a modest dataset of 4000 samples we can
observe in Figure 2 that we were able to achieve a very
low loss of 0.001 and a CCR of 1.0 over the training and
development set. We can deduce from this experiment that
the neural network exhibits the capacity to classify with very
low error over the subset of functions we deﬁned.

6.1.3 Discussion.

In this simulation, the model we designed proved able to
discriminate with high accuracy between safe programs and
vulnerable programs. However, what we considered is just
a simpliﬁed problem, where positive and negative samples
are clearly different. It may be hypothesized, then, that the
RNN modeled generic patterns in our samples, which may
not be tightly related to buffer overﬂow.

IEEE TRANSACTIONS

7

6.2 Simulation 2: Differentiation between vulnerable
and repaired counterparts

models with a learning rate of 5.0 · 10−3 could achieve a
satisfying CCR, despite showing high instability on average.

To disprove the hypothesis that the RNN is not meaning-
fully capturing buffer overﬂow vulnerabilities, in this sim-
ulation we consider a more challenging scenario, in which
vulnerable functions appear in negative samples, while the
ﬁxed version of the same vulnerable functions appears in
positive samples. Vulnerable functions and their patched
counterparts are very similar in code, often with only one
line differentiating a positive from a negative sample. This
setup allows us to probe thoroughly the discriminative
power of the network.

6.2.1 Protocol.

We sample negative samples as functions containing a vul-
nerability from a single type of system-call, fgets. Positive
samples are built including the patched counterparts of the
vulnerable functions; these patched functions still include
the call to fgets, but in a safe and controlled way. In total, we
train the network with 200 samples of both classes.

We

experimented with

low batch

sizes

set

{2, 5, 10},

the
sampled dataset,
{0.000125, 0.00025, 0.0005, 0.001, 0.002}.

and learning

due

the

to

rates

relative
in

the

6.2.2 Results.

in
small
set

6.2.3 Discussion.

Our model was able to successfully learn to discriminate
between subtly different samples. This conﬁrm that the
RNN model is able to capture subtle and contextual features
of the assembly code that actually relate to buffer overﬂow
vulnerabilities. This result is particularly relevant because,
in the real world, the difference between a safe program and
a vulnerable executable may be as small as a single line of
code; performing a length check on an input, or size check
for the allocation of a large buffer, may distinguish a safe
program from an unsafe one. The high accuracy achieved
by our network suggests that machine learning model may
be successfully deployed to capture subtle ﬂaws in code.

6.3 Simulation 3: Larger datasets and samples

In this last experiment, we test our model on a larger dataset
drawn from an even more extensive range of functions. We
wish to push our model further and simulate more realistic
code samples.

6.3.1 Protocol.
In this ﬁnal experiment we consider a collection of 4000
benign and 4000 vulnerable samples. Each sample consist
of 20 functions, where the benign functions in both positive
and negative samples are drawn from the same distribution.
This distribution does not contain repaired counterparts to
the vulnerable functions in the vulnerability library. The
negative samples are drawn at random from the whole
library of vulnerable functions.

We trained a model made up by a two layer LSTM
architecture with a ﬁxed batch sizes of 80, and a learning
rate in the set {0.0001, 0.00025, 0.0005, 0.001}. We chose
these hyperparameter ranges as they have proven successful
in previous experiments on larger datasets.

6.3.2 Results.

Fig. 3: Performance for Simulation 2 using learning rate
1.25 · 10−5 and a batch size of 10.

The best hyperparameter conﬁguration we found uses a
batch size of 10 and a learning rate of 1.25 · 10−4. Figure
3 shows the dynamics of training, with the model achieving
again a CCR close to 1.00 on the training and development
set. CCR on the test set was similarly close to 1.0.

Different choices for the hyperparameters, such as a
learning rate above 2.5 · 10−3 often got stuck and failed
at learning. We hypothesize that the high similarity among
positive and negative samples combined with a relatively
high learning rate may lead to an oscillatory behaviour,
where the network tends to alternatively overshoot or un-
dershoot with respect to the thin margin separating the
classes. Models trained with lower learning rate achieved
overall better performances, although in some cases, even

Fig. 4: Performance for Simulation 2 using learning rate 5 ·
10−5 and a batch size of 80.

IEEE TRANSACTIONS

8

Figure 4 shows the results achieved for the two layers LSTM
conﬁguration. From the graph, we observe an improvement
in the long-term in terms of CCR on the training set and
the development set. After about 17,500 steps, the neural
network breaks through a plateau, and drastically improves
CCR and reduce loss over the training and the development
set. At termination our model achieved a CCR of 99.00%
and a 0.057 loss over the test set. The neural network was
capable of processing and learning features for samples
of substantial size, and converge within a reasonable time
frame. These observations are encouraging as the ”real
world” code encompass a wide range of sizes and structure.
For this particular experiment we also set up a shallower
architecture with only one layer of LSTM. This simpliﬁed
architecture did not perform as well as the two layer LSTM
within a comparable time frame.

6.3.3 Discussion.

Although quite sensitive to hyperparameter tuning, the
RNN model with the right setting (a batch size of 80 samples
was by far the most successful conﬁguration) yielded a
model that achieved a satisfactory result within the training
time constraint. It is interesting to observe that we had
to apply a relatively low learning rates throughout our
experiment to achieve satisfactory results. In conclusion, we
can state that our neural network proved able to learn from
complex samples made up of multiple functions.

conducted a set of experiments on binary classiﬁcation. Our
results showed that RNNs are able to extract useful features
from the assembly code language and evaluate the context
of the instructions. Such conclusions lend support to the
hypothesis on the similarity between natural language and
programming languages, and provide ground for treating
code as language in future research.

Several possible extensions for further development lie
open ahead. The easiest direction would be to scale up
our model in terms of network depth, dataset size and
realism; this would require a more signiﬁcant computational
effort, but it may further conﬁrm (or, possibly, highlight the
limitations of) the language hypothesis. Another direction
would be to focus on alternative model architectures: we
only considered LSTM layers, but layers with attention
mechanisms [28] may provide ground-breaking results, as
they have done in standard NLP tasks [29]. Our model
could also be enriched to perform multiclass prediction
(thus distinguishing different types of buffer overﬂows) or
to output not only the presence or absence of a vulnerabil-
ity, but also its location (thus allowing to identify and ﬁx
buffer overﬂows more easily). Finally, our approach may be
applied to other types of vulnerabilities; buffer errors have
been the most ubiquitous type of vulnerability for the last 25
years [30], but other vulnerabilities, such as race conditions
or failures to validate input, can be equally harmful and
may be predicted with similar RNN models.

7 DISCUSSION

REFERENCES

Our simulations allows us to conclude that not very deep
RNNs are able to successfully perform binary classiﬁcation
on the problem of detecting the presence of stack-based
buffer overﬂow vulnerabilities. Even when presented with
samples where discriminating between a positive and nega-
tive instance relied heavily on the context, the trained model
was able to differentiate satisfactorily.

The results provide a proof-of-concept validation of the
language hypothesis we presented in Section 4. However,
our conclusions are limited by the relative simplicity of
our data sets and our models. Our self-generated data set
can hardly be used to generalize over the large variety of
real-world data. Overcoming this limitation would require
either the collection and the labeling of large quantities of
code samples, or the reﬁnement of generators used to create
realistic code samples. In this last case, our own generator
may be improved by introducing additional functions, more
complex code structure, and recursive calls. Scaling up the
complexity of the data has to be accompanied by a similar
scaling in the depth and complexity of the RNN model. This
would put more pressure on computational resources, as
longer training and more hyperparameter tuning may be in
order to produce a satisfactory model.

8 CONCLUSION

Our research aimed at exploring the application of the
language hypothesis to assembly code, and more speciﬁ-
cally, at understanding the possibilities and limitations of
stack-based buffer overﬂow vulnerability detection using
RNNs. In order to prove or disprove the hypothesis, we

[1] The National Cybersecurity FFRDC. Common vulnerabilities
and exposures. [Online]. Available: https://www.cvedetails.com/
browse-by-date.php

[2] The Shadow Brokers. [Online]. Available: https://en.wikipedia.

[3]

and D. Davis. Ms17-010

org/wiki/The Shadow Brokers
smb
S. Dillon
remote windows kernel pool
[Online]. Avail-
able: https://www.rapid7.com/db/modules/exploit/windows/
smb/ms17 010 eternalblue

eternalblue

corruption.

[4] Wannacry ransomware attack. [Online]. Available: https://en.

[5]

wikipedia.org/wiki/WannaCry ransomware attack
S. Hochreiter and J. Schmidhuber, “Long short-term memory,”
Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.

[6] C. Wang, B. Chen, Y. Liu, and H. Wu, “Layered object-oriented
programming: Advanced vtable reuse attacks on binary-level de-
fense,” IEEE Transactions on Information Forensics and Security, 2018.
[7] X. Zhai, K. Appiah, S. Ehsan, G. Howells, H. Hu, D. Gu, and K. D.
McDonald-Maier, “A method for detecting abnormal program
behavior on embedded devices,” IEEE Transactions on Information
Forensics and Security, 2018.

[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁ-
cation with deep convolutional neural networks,” in Advances in
neural information processing systems, 2012, pp. 1097–1105.
I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio, Deep learning.
MIT press Cambridge, 2016, vol. 1, no. 2.

[9]

[10] H. S. Anderson and P. Roth, “Ember: an open dataset for train-
ing static pe malware machine learning models,” arXiv preprint
arXiv:1804.04637, 2018.

[11] E. Raff, J. Barker, J. Sylvester, R. Brandon, B. Catanzaro, and
C. Nicholas, “Malware detection by eating a whole exe,” arXiv
preprint arXiv:1710.09435, 2017.

[12] L. Bontemps, J. McDermott, N.-A. Le-Khac et al., “Collective
anomaly detection based on long short-term memory recurrent
neural networks,” in International Conference on Future Data and
Security Engineering. Springer, 2016, pp. 141–152.

[13] A. Hovsepyan, R. Scandariato, W. Joosen, and J. Walden, “Soft-
ware vulnerability prediction using text analysis techniques,” in
Proceedings of the 4th international workshop on Security measurements
and metrics, 2012, pp. 7–10.

IEEE TRANSACTIONS

9

Fabio Massimo Zennaro received his PhD in Machine Learning from
the University of Manchester. Currently he is a postdoc researcher
in the Digital Security and in the Oslo Analytics research groups at
the University of Oslo. His research interests include representation
learning, causality, fairness, and security.

[14] Y. Pang, X. Xue, and A. S. Namin, “Predicting vulnerable soft-
ware components through n-gram analysis and statistical feature
selection,” in 2015 IEEE 14th International Conference on Machine
Learning and Applications (ICMLA).

IEEE, 2015, pp. 543–548.

[15] F. Yamaguchi, F. Lindner, and K. Rieck, “Vulnerability extrapola-
tion: Assisted discovery of vulnerabilities using machine learn-
ing,” in Proceedings of the 5th USENIX conference on Offensive
technologies, 2011, pp. 13–13.

[16] R. Russell, L. Kim, L. Hamilton, T. Lazovich, J. Harer, O. Ozdemir,
P. Ellingwood, and M. McConley, “Automated vulnerability de-
tection in source code using deep representation learning,” in
2018 17th IEEE International Conference on Machine Learning and
Applications (ICMLA).

IEEE, 2018, pp. 757–762.

[17] Z. Li, D. Zou, S. Xu, X. Ou, H. Jin, S. Wang, Z. Deng, and Y. Zhong,
“Vuldeepecker: A deep learning-based system for vulnerability
detection,” arXiv preprint arXiv:1801.01681, 2018.

[18] G. Grieco, G. L. Grinblat, L. Uzal, S. Rawat,

J. Feist, and
L. Mounier, “Toward large-scale vulnerability discovery using
machine learning,” in Proceedings of the Sixth ACM Conference on
Data and Application Security and Privacy, 2016, pp. 85–96.

[19] F. Wu, J. Wang, J. Liu, and W. Wang, “Vulnerability detection
with deep learning,” in 2017 3rd IEEE International Conference on
Computer and Communications (ICCC).
IEEE, 2017, pp. 1298–1302.
[20] M. Allamanis, E. T. Barr, P. Devanbu, and C. Sutton, “A survey of
machine learning for big code and naturalness,” ACM Computing
Surveys (CSUR), vol. 51, no. 4, p. 81, 2018.

[21] T. Christina. Gcc wiki. [Online]. Available: https://gcc.gnu.org/

wiki
[22] Hex-Rays.

Ida pro - hex rays.

[Online]. Available: https:

//www.hex-rays.com/products/ida/

[23] Binary ninja. [Online]. Available: https://binary.ninja/
[24] The National Security Agency. Ghidra.

[Online]. Available:

https://www.nsa.gov/resources/everyone/ghidra/

[25] GNU development

[Online]. Available:
gas-2.9.1/html chapter/as 7.html

team. Using as

assembler directives.
https://ftp.gnu.org/old-gnu/Manuals/

[26] Control-ﬂow

Technology
In-
Avail-
2019.
Corporation,
https://software.intel.com/sites/default/ﬁles/managed/

tel
able:
4d/2a/control-ﬂow-enforcement-technology-preview.pdf

Enforcement
5

Speciﬁcation,

[Online].

[27] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison,
A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,
B. Steiner, L. Fang, J. Bai, and S. Chintala, “Pytorch: An imperative
style, high-performance deep learning library,” in Advances in
Neural Information Processing Systems 32, H. Wallach, H. Larochelle,
A. Beygelzimer, F. d’Alch´e Buc, E. Fox, and R. Garnett, Eds.
Curran Associates, Inc., 2019, pp. 8026–8037.

[28] M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches
to attention-based neural machine translation,” arXiv preprint
arXiv:1508.04025, 2015.

[29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
in Advances in neural information processing systems, 2017, pp. 5998–
6008.

[30] Y. Younan. Sourceﬁre vulnerability research team (vrt). [Online].
https://owasp.org/www-chapter-belgium/assets/

Available:
2013/2013-03-05/OWASP Belgium Yves Younan 2013.pdf

William Arild Dahl graduated with a master’s degree in informatics
at the University of Oslo. His research topic was machine learning for
vulnerability detection. Currently he is working as a DevOps engineer at
Netcompany Norway.

L ´aszl ´o Erd ˝odi has a PhD in cyber security. Currently he is a lecturer and
researcher in the information security research group at the University
of Oslo. His main research ﬁeld is offensive security. He is the leader of
the Hacking Arena.

