9
1
0
2

p
e
S
8
2

]

G
L
.
s
c
[

1
v
8
5
1
3
1
.
9
0
9
1
:
v
i
X
r
a

Accelerating the Computation of UCB and Related Indices
for Reinforcement Learning

Wesley Cowan
Department of Computer Science
Rutgers University
110 Frelinghuysen Road, Piscataway, NJ 08854, USA

Michael N. Katehakis
Department of Management Science and Information Systems
Rutgers University
100 Rockafeller Road, Piscataway, NJ 08854, USA

cwcowan@cs.rutgers.edu

mnk@rutgers.edu

Daniel Pirutinsky
Department of Management Science and Information Systems
Rutgers University
100 Rockafeller Road, Piscataway, NJ 08854, USA

dp771@scarletmail.rutgers.edu

Abstract

In this paper we derive an eﬃcient method for computing the indices associated with an
asymptotically optimal upper conﬁdence bound algorithm (MDP-UCB) of Burnetas and
Katehakis (1997) that only requires solving a system of two non-linear equations with two
unknowns, irrespective of the cardinality of the state space of the Markovian decision pro-
cess (MDP). In addition, we develop a similar acceleration for computing the indices for the
MDP-Deterministic Minimum Empirical Divergence (MDP-DMED) algorithm developed
in Cowan et al. (2019), based on ideas from Honda and Takemura (2011), that involves
solving a single equation of one variable. We provide experimental results demonstrating
the computational time savings and regret performance of these algorithms. In these com-
parison we also consider the Optimistic Linear Programming (OLP) algorithm (Tewari and
Bartlett, 2008) and a method based on Posterior sampling (MDP-PS).

Keywords:
totic optimality, eﬃcient computation

reinforcement learning, bandit problems, Markov decision processes, asymp-

1. Introduction

The practical use of the asymptotically optimal UCB algorithm (MDP-UCB) of Burnetas
and Katehakis (1997) has been hindered (Tewari and Bartlett, 2008; Auer and Ortner,
2007) by the computational burden of the upper conﬁdence bound indices c.f. Eq. (2), that
involves the solution of a non-linear constrained optimization problem of dimension equal to
the cardinality of the state space of the Markovian decision process (MDP) under consider-
ation. In this paper we derive an eﬃcient computational method that only requires solving
a system of two non-linear equations with two unknowns, irrespective of the cardinality of
the state space of the MDP. In addition, we develop a similar acceleration for computing
the indices for the MDP-Deterministic Minimum Empirical Divergence (MDP-DMED) de-
veloped in Cowan et al. (2019), that involves solving a single equation of one variable. In

c(cid:13)2019 Wesley Cowan, Michael N. Katehakis, and Daniel Pirutinsky.

 
 
 
 
 
 
Cowan, Katehakis, and Pirutinsky

Section 4 we present these computationally eﬃcient formulations and provide experimental
results demonstrating the computational time savings.

1.1 Related Work

Many modern ideas of reinforcement learning originate in work done for the multi-armed
bandit problem c.f. Gittins (1979); Gittins et al. (2011), Auer et al. (2002), Whittle (1980),
Weber et al. (1992), Sonin and Steinberg (2016), Mahajan and Teneketzis (2008), Katehakis
and Veinott Jr (1987); Katehakis et al. (1996); Katehakis and Derman (1986).

In addition to the papers upon which the algorithms here are explicitly based, there
are many other approaches for adaptively learning MDPs while minimizing expected regret.
Jaksch et al. (2010) propose an algorithm, UCRL2, a variant of the UCRL algorithm of Auer
and Ortner (2007), that achieves logarithmic regret asymptotically, as well as uniformly
over time. UCRL2, deﬁnes a set of plausible MDPs and chooses a near-optimal policy
for an optimistic version of the MDP through so called “extended value iteration”. This
approach, while similarly optimistic in ﬂavor, is suﬃciently diﬀerent than the algorithms
presented here that we will not be comparing them directly. The algorithms in this paper
act upon the estimated transition probabilities of actions for only our current state, for
a ﬁxed estimated MDP. Speciﬁcally, MDP-UCB and OLP inﬂate the right hand side of
the optimality equations by perturbing the estimated transition probabilities for actions in
the current state. MDP-DMED estimates the rates at which actions should be taken by
exploring nearby plausible transition probabilities for actions in the current state. Finally,
MDP-PS obtains posterior sampled estimates, again, only for, the transition probabilities
for actions in the current state.

Recently, Efroni et al. (2019) show that model-based algorithms (which all the algo-
rithms discussed here are), that use 1-step planning can achieve the same regret perfor-
mance as algorithms that perform full-planning. This allows for a signiﬁcant decrease in
the computational complexity of the algorithms. In particular they propose UCRL2-GP,
which uses a greedy policy instead of solving the MDP as in UCRL2, at the beginning of
each episode. They ﬁnd that this policy matches UCRL2 in terms of regret (up to constant
and logarithmic factors), while beneﬁting from decreased computational complexity. The
setting under consideration however, is a ﬁnite horizon MDP and the regret bounds are in
PAC terms (Dann et al., 2017) and optimal minimax (Osband and Van Roy, 2016). Further
analysis is required to transfer these results to the setting of this paper. Namely, an inﬁnite
horizon MDP with bounds on the asymptotic growth rate of the expected regret. A fruit-
ful direction of study would be to examine the relationship between UCRL2-GP, UCRL2,
and the algorithms presented here, more closely, paying particular attention to the varying
dependencies on the dimensionality of the state space.

Osband and Van Roy (2017) analyze and compare the expected regret and computa-
tional complexity of PS-type algorithms (PSRL therein) versus UCB-type (OFU therein)
algorithms, in the setting of ﬁnite horizon MDPs. The PSRL algorithm presented there
is similar to MDP-PS here. However, their optimistic inﬂation or stochastic optimism is
done across the MDP as a whole, either over plausible MDPs in the case of OFU, or for a
ﬁxed MDP in the PSRL case. By contrast, in this paper we present non-episodic versions
where the inﬂations are done only for the actions of our current state for a ﬁxed estimated

2

Accelerating Computation of UCB and other Indices

MDP. They also argue therein that any OFU approach which matches PSRL in regret per-
formance will likely result in a computationally intractable optimization problem. Through
that lens, the main result of this paper, proving a computationally tractable version of the
optimization problem shows that actually a provably asymptotically optimal UCB approach
can compete with a PS approach both in terms of regret performance as well as computa-
tional complexity. A more thorough analysis is required in order to determine what parts of
our analysis here, with an undiscounted inﬁnite horizon MDP, can carry over to the ﬁnite
horizon MDP setting of Osband and Van Roy (2017) and Osband and Van Roy (2016).

As this is a fast growing area of research, there is a lot of recent work. A good resource for
reinforcement learning problems and their potential solution methods is Bertsekas (2019).
For a more bandit focused approach, Lattimore and Szepesv´ari (2018) has a nice overview
of the current state of the art. Most directly relevant to this paper are Chapters 8, 10,
and 38 therein. Cesa-Bianchi and Lugosi (2006) discuss online learning while minimizing
regret for predicting individual sequences of various forms, with Chapter 6 (bandit related
problems) therein being most relevant here. For other related early work we refer to Mandl
(1974), Borkar and Varaiya (1982), Agrawal et al. (1988a), and Agrawal et al. (1988b).

1.2 Paper Structure

The paper is organized as follows. In Section 2 we formulate the problem under consideration
ﬁrst as a completely known MDP and then as an MDP with unknown transition laws. In
Section 3 we present four simple algorithms 1for adaptively optimizing the average reward
in an unknown irreducible MDP. The ﬁrst is the asymptotically optimal UCB algorithm
(MDP-UCB) of Burnetas and Katehakis (1997) that uses estimates for the MDP and choose
actions by maximizing an inﬂation of the estimated right hand side of the average reward
optimality equations. The second (MDP-DMED) is inspired by the DMED method for the
multi-armed bandit problem developed in Honda and Takemura (2010, 2011) and estimates
the optimal rates at which actions should be taken and attempts to take actions at that rate.
The third is the Optimistic Linear Programming (OLP) algorithm (Tewari and Bartlett,
2008) which is based on MDP-UCB but instead of using the KL divergence to inﬂate
the optimality equations, uses the L1 norm. The fourth (MDP-PS) is based on ideas
of greedy posterior sampling that go back to Thompson (1933) and similar to PSRL in
Osband and Van Roy (2017). The main contribution of this paper is in Section 4, where we
present the eﬃcient formulations and demonstrate the computational time savings. Various
computational challenges and simpliﬁcations are discussed, with the goal of making these
algorithms practical for broader use. In Section 5 we compare the regret performance of
these algorithms in numerical examples and discuss the relative advantages of each. While
no proofs of optimality are presented, the results of numerical experiments are presented
demonstrating the eﬃcacy of these algorithms. Proof of optimality for these algorithms will
be discussed in future works.

1. A version of some of the algorithms and comparisons has appeared in a previous technical note Cowan

et al. (2019).

3

Cowan, Katehakis, and Pirutinsky

2. Formulation

Reinforcement learning problems are commonly expressed in terms of a controllable, prob-
abilistic, dynamic system, where the dynamics must be learned over time. The classical
model for this is that of a discrete time, ﬁnite state and action Markovian decision pro-
cess (MDP). See for example, Derman (1970) and Auer and Ortner (2007). In particular,
learning is necessary when the underlying dynamics (the transition laws) are unknown, and
must be learned by observing the eﬀects of actions and transitions of the system over time.
A ﬁnite MDP is speciﬁed by a quadruple (S, A, R, P ), where S is a ﬁnite state space,
A = [A(x)]x∈S is the action space, with A(x) being the (ﬁnite) set of admissible actions
(or controls) in state x, R = [rx,a]x∈S,a∈A(x), is the expected reward structure and P =
[pa
x,y are respectively the one step
expected reward and transition probability from state x to state y under action a. For
extensions regarding state and action spaces and continuous time we refer to Feinberg et al.
(2016) and references therein.

x,y]x,y∈S,a∈A(x) is the transition law. Here rx,a and pa

When all elements of (S, A, R, P ) are known the model is said to be an MDP with
complete information (CI-MDP). In this case, optimal polices can be obtained via the ap-
propriate version of Bellman’s equations, given the prevailing optimization criterion, state,
action, time conditions and regularity assumptions; c.f. Feinberg et al. (2016), Robbins
(1952). When some of the elements of (S, A, R, P ) are unknown the model is said to be an
MDP with incomplete or partial information (PI-MDP). This is the primary model of inter-
est for reinforcement learning, when some aspect of the dynamics must be learned through
interaction with the system.

For the body of the paper, we consider the following partial information model: the

transition probability vector pa
x

= [pa

x,y]y∈S is taken to be an element of parameter space




p ∈ R|S| :

Θ =



(cid:88)

y∈S

py = 1, ∀y ∈ S, py > 0






,

that is, the space of all |S|-dimensional probability vectors.

The assertion of this parameter space deserves some unpacking. It is at ﬁrst simply a
theoretical convenience—it ensures that for any control policy, the resulting Markov chain
is irreducible. It also represents a complete lack of prior knowledge about the transition
dynamics of the MDP. Knowing that certain state-state transitions are impossible requires
prior model speciﬁc knowledge (such knowing the rules of chess). Learning based purely
on ﬁnite observed data could never conclude that a given transition probability is zero.
Thus, we assert a uniform Bayesian prior on the transition probabilities and therefore the
likelihood associated with p = 0 is 0. In this way, asserting this parameter space starting out
represents a fairly agnostic initial view of the underlying learning problem. A possible future
direction of study is to examine how to eﬃciently incorporate prior knowledge, for instance
modifying the speciﬁed parameter space, into the learning process without compromising on
the learning rate. Killian et al. (2017) and Doshi-Velez and Konidaris (2016) discuss hidden
parameterized transition models, for example, which leverage additional prior knowledge
about the transition probability space.

In the body of this paper, we take this unknown transition law to be the only source of in-
complete information about the underlying MDP; the reward structure R = [rx,a]x∈S,a∈A(x)

4

Accelerating Computation of UCB and other Indices

is taken to be known (at least in expectation), and constant. Much of the discussed al-
gorithms will generalize to the situation where the distribution of rewards must also be
learned, but we reserve this for future work.

Under this model, we deﬁne a sequence of state valued random variables X1, X2, X3, . . .
representing the sequence of states of the MDP (taking X1 = x1 as a given initial state),
and action valued random variables A1, A2, . . . as the action taken by the controller, action
At being taken at time t when the MDP is in state Xt. It is convenient to deﬁne a control
policy π as a (potentially random) history dependent sequence of actions such that π(t) =
π(X1, A1, . . . , Xt−1, At−1, Xt) = At ∈ A(Xt). We may then deﬁne the value of a policy as
the total expected reward over a given horizon of action:

Vπ(T ) = E

(cid:35)

rXt,At

.

(cid:34) T

(cid:88)

t=1

Let Π be the set of all feasible MDP policies π. We are interested in policies that
maximize the expected reward from the MDP. In particular, policies that are capable of
maximizing the expected reward irrespective of the initial uncertainty that exists about the
underlying MDP dynamics (i.e., for all possible P under consideration). It is convenient
then to deﬁne V (T ) = supπ∈Π Vπ(T ). We may then deﬁne the “regret” as the expected loss
due to ignorance of the underlying dynamics,

Rπ(T ) = V (T ) − Vπ(T ).

Note, V, Vπ, Rπ all have an implicit dependence on P , through the dynamics of the states
and eﬀects of the actions.

We are interested in uniformly fast (Burnetas and Katehakis, 1997) policies, policies π
that achieve Rπ(T ) = O(ln T ) for all feasible transition laws P . In this case, despite the
controller’s initial lack of knowledge about the underlying dynamics, she can be assured
that her expected loss due to ignorance grows not only sub-linearly over time, but slower
than any power of T . It is shown in Burnetas and Katehakis (1997) that any uniformly
fast policy has a strict lower bound of logarithmic asymptotic growth of regret, with the
unknown transition law P and reward structure R only inﬂuencing the order coeﬃcient, not
the growth rate. Policies that achieve this lower bound are called asymptotically optimal
c.f. Burnetas and Katehakis (1997).

As ﬁnal notation, it is convenient to deﬁne the speciﬁc data available at any point in
x,y(t) be, respectively, the
time, under a given (understood) policy π:
number of visits to state x, the uses of action a in state x, and the transitions from x to y
under action a, that are observed in the ﬁrst t rounds.

let Tx(t), T a

x (t), T a

In the next subsection, we consider the case of the controller having complete infor-
mation (the best possible case) and use this to motivate notation and machinery for the
remainder of the paper. The body of the paper is devoted to presenting and discussing four
computationally simple algorithm that are either provably asymptotically optimal, or at
least appear to be. While no proofs of optimality are presented, the results of numerical ex-
periments are presented demonstrating the eﬃcacy of these algorithm. Proof of optimality
for these algorithm will be discussed in future works.

5

Cowan, Katehakis, and Pirutinsky

2.1 The Optimal Policy Under Complete Information

Classical results (Burnetas and Katehakis, 1997) show that there is a stationary, determin-
istic policy π (each action depends only on the current state), that realizes the maximal
long term average expected value. That is, a simple Markovian policy π∗ that realizes

= φ∗ = sup
π∈Π
We may characterize this optimal policy in terms of the solution for φ = φ∗(A, P ) and

lim inf
T

lim
T

.

Vπ∗(T )
T

Vπ(T )
T

v = v(A, P ) of the following system of optimality equations:

∀x ∈ S :

φ + vx = max
a∈A(x)

rx,a +





pa
x,yvy

 .

(cid:88)

y∈S

(1)

Given the solution φ and vector v to the above equations, the asymptotically optimal
policy π∗ can be characterized as, whenever in state x ∈ S, take any action a which realizes
the maximum in Eq. (1). We denote the set of such asymptotically optimal actions as
In general, a∗(x, P ) should be taken to denote an action a∗ ∈ O(x, P ). Note,
O(x, P ).
realizing this policy necessarily requires knowledge of P and R, in order to solve the system
of optimality equations.

The solution φ above represents the maximal long term average expected reward of an
optimal policy. The vector v, or more precisely, vx for any x ∈ S, represents in some sense
the immediate value of being in state x relative to the long term average expected reward.
The value vx essentially encapsulates the future opportunities for value available due to
being in state x.

2.2 Optimal Policies Under Unknown Transition Laws

The results of the previous section show that V (T ), the value of the optimal policy, goes
approximately like V (T ) ≈ φ∗T . We begin by characterizing the regret of any arbitrary
policy π, comparing its value relative to this baseline. It will be convenient in what is to
follow to deﬁne the following notation:

L(x, a, p, v) = rx(a) +

(cid:88)

y∈S

pyvy.

The function L represents the value of a given action in a given state, for a given transition
vector—both the immediate reward, and the expected future value of whatever state the
MDP transitions into. The value of an asymptotically optimal action for any state x is thus
given by L∗(x, A, P ) = L(x, a∗(x, P ), pa∗(x,P )
, v(A, P )). It can be shown that the “expected
loss” due to an asymptotically sub-optimal action, taking action a /∈ O(x, P ) when the
MDP is in state x, is in the limit given by

x

∆(x, a, A, P ) = L∗(x, A, P ) − L(x, a, pa
x

, v(A, P )).

In the general (partial or complete information) case, it is shown in Burnetas and Kate-

hakis (1997) that the regret of a given policy π ∈ Π can be expressed asymptotically as

Rπ(T ) = V (T ) − Vπ(T ) =

(cid:88)

(cid:88)

x∈S

a /∈O(x,P )

6

E [T a

x (T )] ∆(x, a, A, P ) + O(1).

Accelerating Computation of UCB and other Indices

Note, the above formula justiﬁes the description of ∆(x, a, A, P ) as the “average loss
due to sub-optimal activation of a in state x”. Additionally, from the above it is clear that
in the case of complete information, when P is known and therefore the asymptotically
optimal actions are computable, the total regret at any time T is bound by a constant. Any
expected loss at time T is due only to ﬁnite horizon eﬀects.

In general, for the unknown transition laws case, we have the following bound due to

Burnetas and Katehakis (1997), for any uniformly fast policy π,

lim inf
T

Rπ(T )
ln T

≥

(cid:88)

(cid:88)

x∈S

a /∈O(x,P )

∆(x, a, A, P )
Kx,a(P )

,

where Kx,a(P ) represents the minimal Kullback-Leibler divergence between pa
and any
x
q ∈ Θ such that substituting q for pa
x in P renders a the unique optimal action for x.
x
Recall, the Kullback-Leibler divergence is given by I(p, q) = (cid:80)
x∈S px ln(px/qx). This is
equivalent to stating that any sub-optimal action must be sampled at least at a minimum
rate, in particular, for a /∈ O(x, P ),

lim inf
T

E [T a
x (T )]
ln T

≥

1
Kx,a(P )

.

This can be interpreted in the following way:
for a sub-optimal action, the “closer” the
transition law is to an alternative transition law that would make it the best action, the more
data we need to distinguish between the truth and this plausible alternative hypothesis, and
therefore the more times we need to sample the action to distinguish the truth. Anything
less than this “base rate”, we risk convincing ourselves of a plausible, sub-optimal hypothesis
and therefore incurring high regret when we act on that belief.

Policies that achieve this lower bound, for all P , are referred to as asymptotically op-
timal. Achieving this bound, or at least the desired logarithmic growth requires careful
exploration of actions. In the next section, we present four algorithms to accomplish this.

3. Algorithms for Optimal Exploration

Common RL algorithms solve the exploration/exploitation dilemma in the following way:
most of the time, select an action (based on the current data) that seems best, otherwise
select some other action. This alternative action selection is commonly done uniformly at
random. As long as this is done infrequently, but not too infrequently, the optimal actions
and policy will be discovered, potentially at the cost of high regret. Minimizing regret re-
quires careful consideration of which alternative actions are worth taking at any given point
in time. The following algorithms are methods for performing this selection; essentially,
instead of blindly selecting from the available actions to explore, each algorithm evalu-
ates the currently available data to determine which action is most worth exploring. Each
accomplishes this through an exploration of the space of plausible transition hypotheses.

The beneﬁt of this is that through careful exploration, optimal (minimal) regret can be
achieved. The cost however, is additional computation. The set of alternative transition
laws is large and high dimensional, and can be diﬃcult to work with. In Section 4 we show
several simpliﬁcations, however, that make this exploration practical.

7

Cowan, Katehakis, and Pirutinsky

3.1 A UCB-Type Algorithm for MDPs Under Uncertain Transitions

Classical upper conﬁdence bound (UCB) decision algorithms (for instance as in multi-armed
bandit problems, c.f. Auer and Ortner (2010), Burnetas and Katehakis (1996), Cowan
et al. (2017)), approach the problem of exploration in the following way:
in each round,
given the current estimated transition law, we consider “inﬂated” estimates of the values
of each actions, by ﬁnding the best (value-maximizing) plausible hypothesis within some
conﬁdence interval of the current estimated transition law. The more data that is available
for an action, the more conﬁdence there is in the current estimate, and the tighter the
conﬁdence interval becomes; the tighter the conﬁdence interval becomes, the less exploration
is necessary for that action. The algorithm we present here is a version of the MDP-UCB
algorithm presented in Burnetas and Katehakis (1996).

At any time t ≥ 1, let xt be the current (given) state of the MDP. We construct the

following estimators:

• Transition Probability Estimators: for each state y and action a ∈ A(xt), construct

ˆPt based on

ˆpa
xt,y =

T a
xt,y(t) + 1
T a
xt(t) + |S|

.

Note the biasing terms (the 1 in the numerator, |S| in the denominator). Including
these, biases the estimated transition probabilities away from 0, so that our estimates
pa
will be in Θ. Additionally, these guarantee that the above is in fact the maximum
xt
likelihood estimate for the transition probability, given the observed data and uniform
priors.

• “Good” Action Sets: construct the following subset of the available actions A(xt),

(cid:110)

ˆAt =

a ∈ A(xt) : T a

xt(t) ≥ (ln Txt(t))2(cid:111)

.

The set ˆAt represents the actions available from state xt that have been sampled fre-
quently enough that the estimates of the associated transition probabilities should be
“good”. In the limit, we expect that sub-optimal actions will be taken only logarith-
mically, and hence for suﬃciently large t, ˆAt will contain only actions that are truly
optimal. If no actions have been taken suﬃciently many times, we take ˆAt = A(xt)
to prevent it from being empty.

• Value Estimates: having constructed these estimators, we compute ˆφt = φ( ˆAt, ˆPt)
and ˆvt = v( ˆAt, ˆPt) as the solution to the optimality equations in Eq. (1), essentially
treating the estimated probabilities as correct and computing the optimal values and
policy for the resulting estimated MDP.

At this point, we implement the following decision rule: for each action a ∈ A(xt), we

compute the following index over the set of possible transition laws:

(cid:26)

ua(t) = sup
q∈Θ

L(xt, a, q, ˆv) : I(ˆpa
xt

, q) ≤

(cid:27)

,

ln t
Txt,a(t)

(2)

8

Accelerating Computation of UCB and other Indices

where I(p, q) = (cid:80)

y py ln(py/qy) is the Kullback-Leibler divergence, and take action

π(t) = arg maxa∈A(xt)ua(t).

This is a natural extension of several classical KL-divergence based UCB algorithms
for the multi-armed bandit problem c.f. Lai and Robbins (1985), Burnetas and Katehakis
(1996), Cowan et al. (2017) taking the view of the L function as the ‘value’ of taking a given
action in a given state, estimated with the current data. In Burnetas and Katehakis (1996),
a modiﬁed version of the above algorithm is in fact shown to be asymptotically optimal.
The modiﬁcation is largely for analytical beneﬁt however, the pure index algorithm as above
shows excellent performance c.f. Figure 3. Further discussion of the performance of this
algorithm is given in Section 5.

An important and legitimate concern to the practical usage of the MDP-UCB algorithm
that has been noted in Tewari and Bartlett (2008) among others, is actually calculating the
index in Eq. (2). This and other issues are discussed in more depth in Section 4, where a
computationally eﬃcient formulation is presented. Additionally, in Section 5, we highlight
beneﬁcial behavior of this algorithm that makes it worth pursuing.

3.2 A Deterministic Minimum Empirical Divergence Type Algorithm for

MDPs Under Uncertain Transitions

In the classical DMED algorithm for multi-armed bandit problems (Honda and Takemura,
2010), rather than considering (inﬂated) values for each action to determine which should
be taken, DMED attempts to estimate how often each action ought to be taken. Recall the
interpretation of Burnetas and Katehakis (1996) given previously, that for any uniformly
fast policy π, for any sub-optimal action a /∈ O(x, P ) we have

lim inf
T

E [T a
x (T )]
ln T

≥

1
Kx,a(P )

,

where Kx,a(P ) measures (via the Kullback-Leibler divergence) how much the transition law
for action a would need to be changed to make action a optimal.

DMED proceeds by the following reasoning. If we estimate that the sub-optimal action
a is close to being optimal (low Kx,a), make sure we take it often enough to diﬀerentiate
between them (ensure T a
x is high). If, on the other hand, we estimate that the sub-optimal
action a is far from being optimal (high Kx,a), we don’t need to take is as often (ensure T a
x
is low). As with the MDP-UCB and OLP algorithms, this requires an exploration of the
possible transition laws “near” the current estimated transition law.

In general, computing the function Kx,a(P ) is not easy. We consider the following

substitute, then:

˜Kx,a(P, v, a∗) = inf
q∈Θ

(cid:110)

I(pa
x

, q) : L(x, a, q, v) ≥ L(x, a∗, pa∗
x

(cid:111)

, v)

.

This is akin to exploratory policy iteration. That is, determining, based on the current
value estimates, how much modiﬁcation would produce an improving action.

The function K measures how far the transition vector associated with x and a must be
perturbed (under the KL-divergence) to make a the optimal action for x. The function ˜K

9

Cowan, Katehakis, and Pirutinsky

measures how far the transition vector associated with x and a must be perturbed (under
the KL-divergence) to make the value of a, as measured by the L-function, no less than the
value of an optimal action a∗. As will be shown in Section 4, ˜K may be computed fairly
simply, in terms of the root of a single non-linear equation.

In this way, we have the following approximate MDP-DMED algorithm (see Honda and
Takemura (2010) and Honda and Takemura (2011) for the multi-armed bandit version of
this algorithm).

At any time t ≥ 1, let xt be the current state, and construct the estimators as in the
MDP-UCB algorithm in Section 3.1, ˆPt, ˆAt, and utilize these to compute the estimated
optimal values, ˆφt = φ( ˆAt, ˆPt) and ˆvt = v( ˆAt, ˆPt).
t = arg maxa∈A(xt)L(xt, a, ˆpa
xt

, ˆvt) be the estimated “best” action to take at time

Let ˆa∗

t. For each a (cid:54)= ˆa∗

t , compute the discrepancies

Dt(a) = ln t/ ˜Kxt,a( ˆPt, ˆvt, ˆa∗

t ) − Txt,a(t).

t

Dt(a) ≤ 0, take π(t) = ˆa∗

t , otherwise, take π(t) = arg maxa(cid:54)=ˆa∗

If maxa(cid:54)=ˆa∗
Following this algorithm, we perpetually reduce the discrepancy between the estimated
sub-optimal actions, and the estimated rate at which those actions should be taken. The
exchange from K to ˜K sacriﬁces some performance in the pursuit of computational sim-
plicity, however it also seems clear from computational experiments that MDP-DMED as
above is not only computationally tractable, but also produces reasonable performance in
terms of achieving small regret c.f. Figure 3. Further discussion of the performance of this
algorithm is given in Section 5.

Dt(a).

t

3.3 Optimistic Linear Programming, Another UCB-Type Algorithm for

MDPs Under Uncertain Transitions

As we have previously noted, Tewari and Bartlett (2008) raises some legitimate compu-
tational concerns. They propose an alternative, algorithm which they term “optimistic
linear programming” (OLP), which is closely related to the MDP-UCB algorithm presented
here. The main diﬀerence between OLP and MDP-UCB is that OLP does not use the
KL divergence to determine the conﬁdence interval. Instead, OLP uses L1 distance, which
allows the resulting index to be computed via solving linear programs. This reduces the
computational complexity at the cost of performance. As we will show in Section 4, the
MDP-UCB optimization problem can be simpliﬁed drastically, to render the use of OLP,
at least with respect to the computational issues, unnecessary. The algorithm we present
here is a version of OLP algorithm presented in Tewari and Bartlett (2008).

At any time t ≥ 1, let xt be the current state, and construct the estimators as in the
MDP-UCB algorithm in Section 3.1, ˆPt, ˆAt, and utilize these to compute the estimated
optimal values, ˆφt = φ( ˆAt, ˆPt) and ˆvt = v( ˆAt, ˆPt).

At this point, we implement the following decision rule: for each action a ∈ A(xt), we
compute the following index, again maximizing value within some distance of the current
estimates:

(cid:40)

ua(t) = sup
q∈Θ

L(xt, a, q, ˆv) : ||ˆpa
xt

− q||1 ≤

10

(cid:115)

(cid:41)

,

2 ln t
T a
xt(t)

Accelerating Computation of UCB and other Indices

and take action

π(t) = arg maxa∈A(xt)ua(t).

3.4 A Thompson-Type Algorithm for MDPs Under Uncertain Transitions

In MDP-UCB, MDP-DMED, and OLP, above, we realized the notion of “exploration”
in terms of considering alternative hypotheses that were “close” to the current estimates
within Θ, interpreting closeness in terms of “plausibility”. In this section, we consider an
alternative form of exploration through random sampling over Θ, based on the current
available data. Given a uniform prior over Θ, the posterior for pa
is given by a Dirichlet
x
distribution with the observed occurrences. Posterior Sampling (MDP-PS) proceeds in the
following way:

At any time t ≥ 1, let xt be the current state, and construct the estimators as in the
MDP-UCB algorithm in Section 3.1, ˆPt, ˆAt, and utilize these to compute the estimated
optimal values, ˆφt = φ( ˆAt, ˆPt) and ˆvt = v( ˆAt, ˆPt).
In addition, generate the following
random vectors:

For each action a ∈ A(xt), let T a

xt(t) = [T a

xt,y(t)]y∈S be the vector of observed transition

counts from state xt to y under action a. Generate the random vector Q according to

Qa(t) ∼ Dir(T a

xt(t)).

The Qa(t) are distributed according to the joint posterior distribution of pa
xt
prior.

with a uniform

At this point, deﬁne the following values as posterior sampled estimates of the potential

value L of each action:

Wa(t) = rxt,a +

Qa

y(t)ˆvy,

(cid:88)

y

and take action π(t) = arg maxa∈A(xt)Wa(t).

In this way, we probabilistically explore likely hypotheses within Θ, and act according

to the action with best hypothesized value.

4. Accelerating Computation
All of the above algorithms require computing the estimated optimality values ˆφt, ˆvt each
round. This is an issue, but eﬃcient linear programming formulations exist to solve the
optimality equations in Eq. (1) see for example Derman (1970). It may also be possible
to adapt the method of Lakshminarayanan et al. (2017) for approximately solving MDPs,
among others, to our undiscounted and potentially changing MDP setting.

However, each of these algorithms additionally has unique computational challenges,
through computations over the high dimensional parameter space Θ due to the typically
high cardinality of the state space.

4.1 MDP-UCB

We will ﬁrst examine the MDP-UCB algorithm from Section 3.1. Recalling the notation that
I(p, q) = (cid:80)
x px ln(px/qx), MDP-UCB has to repeatedly solve the following optimization

11

Cowan, Katehakis, and Pirutinsky

problem:

C(p, v, δ) = sup
q∈Θ

qxvx : I(p, q) ≤ δ

(cid:41)
.

(cid:40)

(cid:88)

x

The index of the MDP-UCB algorithm may be eﬃciently expressed in terms of the C
function above which we will refer to as the q-Formulation.

This represents an |S|-dimensional non-linear constrained optimization problem which

is not, in general, easy to solve.

For mathematical completeness, as well as for practical implementation, we ﬁrst analyze
x pxvx and V = maxx vx, then

some trivial cases. Let µp = (cid:80)

Theorem 1 The value of C(p, v, δ) can be easily found in the following cases:

• If δ < 0 then the optimization problem, C(p, v, δ) is infeasible and we say C(p, v, δ) =

−∞.

• If δ = 0, then C(p, v, δ) = µp.

• If δ > 0 and vx1 = vx2 for all x1, x2 ∈ S, then C(p, v, δ) = µp.

Proof of this theorem is provided in Appendix A.1.
For other cases, we can reduce this to solving a 2 dimensional system of non-linear

equations, with unknowns µ∗

q and λ as follows.

Theorem 2 For any δ > 0 and v such that vx1 (cid:54)= vx2 for some x1, x2 ∈ S,

where

C(p, v, δ) = µ∗
q,

(cid:18)

px ln

1 +

(cid:19)

vx − µ∗
q
λ

= δ,

(cid:88)

x∈S

px

(cid:88)

λ
λ + vx − µ∗
q
x
q < V and λ < µ∗

= 1,

q − V.

µp < µ∗

Proof of this theorem is provided in Appendix A.2.

Solving these systems, which we will refer to as the (µ∗

q, λ)-Formulation, provides dra-
matic speed increases for the implementation of the algorithm (Figure 1). We also note
that the (µ∗
q, λ)-Formulation scales manageably with the dimension of the state space, as
opposed to the q-Formulation. Additionally, the structure of the equations admits several
nice solution methods since, for a given µq, the second equation has a unique solution for
λ in the indicated range, and given that solution, the summation in the ﬁrst equation is
increasing to inﬁnity as a function of µq.

12

Accelerating Computation of UCB and other Indices

4.2 MDP-DMED

Next we examine the MDP-DMED algorithm from Section 3.2. Again, recalling the no-
tation that I(p, q) = (cid:80)
x px ln(px/qx), MDP-DMED has to repeatedly solve the following
optimization problems:

(cid:40)

D(p, v, ρ) = inf
q∈Θ

I(p, q) :

(cid:41)

qxvx ≥ ρ

.

(cid:88)

x

The rate function ˜K of the MDP-DMED algorithm may be eﬃciently expressed in terms
of the D function above which we will refer to as the q-Formulation. This represents an
|S|-dimensional non-linear constrained optimization problems, which is not, in general, easy
to solve.

As before, we consider some trivial cases ﬁrst. Let µp = (cid:80)

x pxvx and V = maxx vx,

then

Theorem 3 The value of D(p, v, ρ) and by extension Dt(a) can be easily found in the
following cases:

• If ρ > V then the optimization problem, D(p, v, ρ) is infeasible and we say D(p, v, ρ) =

∞ and Dt(a) = −Txt,a(t).

• If ρ ≤ µp then D(p, v, ρ) = 0 and we say Dt(a) = ∞.

• If vx1 (cid:54)= vx2 for some x1, x2 ∈ S and ρ = V , then optimization problem D(p, v, ρ)

diverges to inﬁnity and we say D(p, v, ρ) = ∞ and Dt(a) = −Txt,a(t).

Proof of this theorem is provided in Appendix A.3.
For other cases, this optimization problem reduces to solving a 1-dimensional system of

non-linear equations with one unknown, λ, as follows:

Theorem 4 For any v such that vx1 (cid:54)= vx2 for some x1, x2 ∈ S and µp < ρ < V ,
(cid:88)

D(p, v, ρ) =

px ln(1 + (ρ − vx)λ),

where

x

(cid:88)

x

px

ρ − vx
1 + (ρ − vx)λ

= 0,

0 < λ <

1
V − ρ

.

Proof of this theorem is provided in Appendix A.4.

As with the MDP-UCB case, solving this system, which we will refer to as the λ-
Formulation, provides dramatic speed increases for the implementation of the algorithm
(Figure 1). We also note that the λ-Formulation scales manageably with the dimension of
the state space, as opposed to the q-Formulation. Additionally, the λ-Formulation struc-
turally lends itself well to solutions. Over the indicated range, the summation is positive and
constant in the limit as λ → 0, and monotonically decreasing, diverging to negative inﬁnity
as λ → 1/(V − ρ). Hence the solution is unique, and can easily be found via bisection.

13

Cowan, Katehakis, and Pirutinsky

4.3 OLP

Next we examine the OLP algorithm from Section 3.3. OLP has to repeatedly solve the
following optimization problem:

B(p, v, δ) = sup
q∈Θ

qxvx : ||ˆpxt − q||1 ≤ δ

(cid:41)
.

(cid:40)

(cid:88)

x

The index of the OLP algorithm may be eﬃciently expressed in terms of the B function
above. B(p, v, δ) is equivalent to the following linear program:

maxq+,q−

s.t.

(cid:88)

x∈S

(cid:88)

x∈S
(cid:88)

vx(q−

x − q+

x + px),

q+

x + q−

x ≤ δ,

q−

x − q+

x = 0,

x∈S
q+
q+

x − q−
x, q−

x ≥ 0

x ≤ px

∀x ∈ S,

∀x ∈ S.

This represents an |S|-dimensional linear program, which can generally be computed
quite eﬃciently. However, as the dimension of the state space increases we incur a greater
computational burden (Figure 1).

4.4 MDP-PS

The most attractive advantage of MDP-PS is the reduced computational cost, relative to the
other three proposed algorithms (Figure 2). Notice there is no extra optimization problem
that needs to be solved. In the MDP-UCB algorithm, at every time t, we had to iteratively
solve |A(xt)| instances of C(p, v, δ), for OLP |A(xt)| instances of B(p, v, δ), and for MDP-
DMED, |A(xt)| instances of D(p, v, ρ). Under MDP-PS, the computational burden stems
from sampling from the Dirichlet distribution for each action (again, |A(xt)| steps), but
this is a well studied problem with many eﬃciently implemented solutions (see for example
McKay (2003)). Speciﬁc properties of the MDP-PS algorithm may still make these other
algorithms worth pursuing, however, as seen in Section 5.

4.5 Computation Time Comparison

To demonstrate the computational time savings achieved by these simpliﬁcations we ran-
domly generated the parameters for 15 diﬀerent action indices and timed how long each
algorithm took to solve. We repeated this for 4 diﬀerent values of |S|, the dimension of the
state space, 10, 100, 1, 000, and 10, 000. In Figure 1, we plot the mean computation time as

14

Accelerating Computation of UCB and other Indices

Figure 1: Computation time as |S| increases

|S| increases, for each algorithm, [1] MDP-PS, [2] MDP-DMED λ-Formulation, [3] MDP-
UCB (µ∗
q, λ)-Formulation , [4] MDP-DMED q-Formulation , [5] MDP-UCB q-Formulation,
and [6] OLP, along with a 95% conﬁdence interval.

In order to keep the comparisons as equitable as possible, the optimization problem
for all the algorithms (with the exception of MDP-PS) were solved to within 4 digits of
accuracy using TensorFlow for Python (Abadi et al., 2016). MDP-PS used SciPy’s random
Dirichlet generator. They were all run on a MacBook Pro with a 3.1 Ghz i7 processor with
16GB DDR3 RAM.

[3] MDP-UCB (µ∗

The top three fastest algorithms were [1] MDP-PS, [2] MDP-DMED λ-Formulation, and
q, λ)-Formulation. Figure 2 shows these three in more detail.
From Figure 1 we can see the dramatic savings achieved by [2] MDP-DMED using the
λ-Formulation, and [3] MDP-UCB using the (µ∗
q, λ)-Formulation as compared to [4,5] the
q-Formulations.
[6] OLP also suﬀers from increasing computation time as the dimension
of the state space increases. OLP performs the worst in terms of computational time
which is likely due to the fact that we are not using a specialized fast LP solver but rather
TensorFlow.

In Figure 2 we can see the relative performances of the top three algorithms. [1] MDP-
PS, unsurprisingly with the fastest, followed by [2] MDP-DMED using the λ-Formulation
with its single unknown, and then [3] MDP-UCB using the (µ∗
q, λ)-Formulation with its two
unknowns.

15

Cowan, Katehakis, and Pirutinsky

Figure 2: Computation time as |S| increases for the top three performers

The absolute time is not as important as the relative time. There are numerous ways to
achieve signiﬁcantly faster absolute time but our focus here is to demonstrate the relative
speed increase gained by using our simpliﬁcations. In addition, one can get raw computa-
tional time savings by developing a devoted optimizer for problems of this type but if we
restrict to using a generic black box optimizer, the method we employed seems a reasonable
reﬂection of what one would do.

5. Comparison of Performance

In this section we discuss the results of our simulation test of these algorithms on a small
example problem. There is nothing particularly special about the values for this example,
and we observe similar results under other values. Our example had 3 states (x1, x2, and x3)
with 2 available actions (a1 and a2) in each state. Below we show the transition probabilities,
as well as the reward, returned under each action.

P [a1] =

P [a2] =

x1
x2
x3

x1
x2
x3

x2
0.69
0.01
0.46

x2
0.68
0.33
0.35

x3
0.27
0.11
0.52

x3
0.04
0.41
0.22

,

,

x1
0.04
0.88
0.02

x1
0.28
0.26
0.43

16

Accelerating Computation of UCB and other Indices

Figure 3: Average cumulative regret over time for each algorithm

R =

a1
a2

x1
0.13
0.18

x2
0.47
0.71

x3
0.89
0.63

.

If these transition probabilities were known, the optimal policy for this MDP would be

π∗(x1) = a1, π∗(x2) = a2, and π∗(x3) = a1.

We simulated each algorithm 100 times over a time horizon of 10,000 and for each
time step we computed the mean regret as well as the variance. In Figure 3, we plot the
mean regret over time for each algorithm, [1] MDP-PS, [2] MDP-UCB, [3] OLP, and [4]
MDP-DMED, along with a 95% conﬁdence interval for all sample paths.

We can see that all algorithms seem to have logarithmic growth of regret. There are
a few interesting diﬀerences that the plot highlights, at least for these speciﬁc parameter
values:

MDP-DMED has not only the highest ﬁnite time regret, but also large variance that
seems to increase over time. This seems primarily due to the “epoch” based nature of
the algorithm, which results in exponentially long periods when the algorithm may get
trapped taking sub-optimal actions, incurring large regret until the true optimal actions
are discovered. The beneﬁt of this epoch structure is that once the optimal actions are
discovered, they are taken for exponentially long periods, to the exclusion of sub-optimal
actions.

As expected, see Tewari and Bartlett (2008), OLP has a higher ﬁnite time regret when

compared to MDP-UCB, but still achieves logarithmic growth.

17

Cowan, Katehakis, and Pirutinsky

MDP-PS seems to perform best, exhibiting lowest ﬁnite time regret as well as the tightest
variance. This seems largely in agreement with the performance of PS-type algorithms in
other bandit problems as well, in which they are frequently asymptotically optimal c.f.
Cowan et al. (2017) and references therein.

5.1 Algorithm Robustness—Inaccurate Priors

How do these algorithms respond to potentially “unlucky” or non-representative streaks of
data? How does bad initial estimates eﬀect their performance? Can these algorithms be
fooled, and what are the resulting costs before they recover? This is a practically important
question, in terms of data security and risk assessment, but also an important element
of evaluating a learning algorithm. How does the learning agent respond to non-ideal
conditions?

To test these algorithms, we “rigged” or biased the ﬁrst 60 actions and transitions, such
that under the estimated transition probabilities the optimal policy would be to activate
the sub-optimal action in each state. In more detail, let T a
x,y be the number of times we
transitioned from state x to state y under action a. Then we rigged T a so that it started
like so,

T [a1] =

T [a2] =

x1 x2 x3
1
1
8
8
1
1
1
1
8

,

x1 x2 x3
8
1
1
1
1
8
8
1
1

x1
x2
x3

x1
x2
x3

Under the resulting (bad) estimated transition probabilities, we have that the (esti-
mated) optimal policy is ˆπ∗(x1) = a2, ˆπ∗(x2) = a1, and ˆπ∗(x3) = a1, which in fact chooses
the sub-optimal action in each state.

The subsequent performances of the MDP algorithms are plotted in Figure 4. All
algorithms still appear to have logarithmic growth in regret, suggesting they can all ‘recover’
from the initial bad estimates. It is striking though, the extent to which the average regrets
for MDP-DMED and MDP-PS are aﬀected, increasing dramatically as a result, MDP-PS
demonstrating an increase in variance as well. However, the MDP-UCB algorithm seems
relatively stable:
its average regret has barely increased, and maintains a small variance.
Empirically, this phenomenon appears common for the MDP-UCB algorithm under other
extreme conditions. The underlying cause and a rigorous examination of these intuitions,
will be explored in a future work.

6. Conclusion and Future Work

In this paper we have presented four algorithms adapted from classical multi-armed bandit
algorithms that either are provably asymptotically optimal or at least give that appearance

18

Accelerating Computation of UCB and other Indices

Figure 4: Robustness test. MDP-UCB seems to be largely unaﬀected by the inaccurate priors.

in practice. The simpliﬁcations for MDP-UCB and MDP-DMED presented here have been
shown to dramatically reduce the computational burden for these algorithms, rendering
them more useful in practice. As a result, the provably worse performing OLP, no longer
has any advantage over them. MDP-DMED under the λ-Formulation is fast and possibly
optimal, but has a high variance for regret that increases over time. While MDP-PS is
very fast and appears to be optimal, it is highly sensitive to incorrect priors or extreme
sampling errors. MDP-UCB is provably optimal has stable performance under various
extreme conditions, and can be computed quickly using the (µ∗

q, λ)-Formulation.

There are various interesting directions to continue this work, we mention a few potential
avenues here. The idea of “exploring the hypothesis space” is something that extends
immediately to the case of unknown rewards. Each of the algorithms presented here can
generalize immediately to such situations, though the computational simpliﬁcations would
need to be modiﬁed signiﬁcantly.

It would also be of theoretical interest to ﬁnd suﬃcient conditions on the estimators used
to ensure asymptotically optimal performance. This could potentially allow these algorithms
to be modiﬁed to use other state value estimators (for example, Q-learning Watkins (1989))
while maintaining their theoretical guarantees. From a practical computational point of
view we could consider systems where we can’t easily iterate over all possible states, and
how these algorithms can be modiﬁed to address this. These ideas will be explored in future
works.

19

Cowan, Katehakis, and Pirutinsky

Acknowledgments

We acknowledge support for this work from the National Science Foundation, NSF grant
CMMI-1662629.

Appendix A. Proof of Theorems of Section 4

A.1 Proof of Theorem 1

First we restate Theorem 1:

The value of C(p, v, δ) can be easily found in the following cases:

• If δ < 0 then the optimization problem, C(p, v, δ) is infeasible and we say C(p, v, δ) =

−∞.

• If δ = 0, then C(p, v, δ) = µp.

• If δ > 0 and vx1 = vx2 for all x1, x2 ∈ S, then C(p, v, δ) = µp.

Proof Recall that I(p, q) is the KL Divergence from p to q. We then have by Gibb’s
inequality that I(p, q) ≥ 0, with equality if and only if p = q. Thus, if δ < 0 then the
optimization problem is infeasible. If δ = 0 then it has the trivial solution q∗ = p. We
therefore take δ > 0. Now, if vx1 = vx2 for all x1, x2 ∈ S then any feasible probability
vector q is also optimal with C(p, v, δ) = vx = µp.

A.2 Proof of Theorem 2

In this section we will prove Theorem 2, which we restate here.

x pxvx and V = maxx vx. Then for any v such that vx1 (cid:54)= vx2 for some

Let µp = (cid:80)
x1, x2 ∈ S and δ > 0,

where

C(p, v, δ) = µ∗
q,

(cid:18)

px ln

1 +

(cid:19)

vx − µ∗
q
λ

= δ,

(cid:88)

x∈S

px

(cid:88)

λ
λ + vx − µ∗
q
x
q < V and λ < µ∗

= 1,

q − V.

µp < µ∗

Before giving the formal proof, it may be helpful to understand the overall conception of
the proof. The main idea is the use of Lagrange multiplier techniques, which greatly reduces
the dimensionality of the problem to be solved. We are able to exchange from trying to ﬁnd
the optimal probability vector q∗, to a problem where we need only ﬁnd two moments of the
optimal q∗, a dramatic dimension reduction. In the MDP-UCB case, it suﬃces to ﬁnd the
unknown optimal mean of the optimal distribution, q∗, µ∗
q∗/(µp − µ∗
q)
which depends on the optimal, unknown variance.

q, and a value λ = σ2

20

Accelerating Computation of UCB and other Indices

Proof Recall that,

C(p, v, δ) = sup
q∈Θ

(cid:41)

qxvx : I(p, q) ≤ δ

(cid:40)

(cid:88)

x

(3)

Since {q : q ∈ Θ, I(p, q) ≤ δ} is a closed compact set, the supremum will be realized by a
maximum, and we may express the problem of computing C(p, v, δ) in the following form:

maxq µq =

(cid:88)

x∈S

qxvx,

s.t.

(cid:88)

x∈S
(cid:88)

px ln

(cid:19)

(cid:18) px
qx

≤ δ,

qx = 1,

x∈S
qx > 0

x ∈ S.

Let µ∗

q = (cid:80)

x∈S q∗

xvx be the optimal value of the objective function, µp = (cid:80)

and V = maxx vx. First we will argue that,

(4)

(5)

x∈S pxvx,

µp ≤ µ∗

q < V.

To see the ﬁrst inequality, observe that q = p satisﬁes the constraints and is therefore
feasible, hence the objective function at q = p is less than or equal to the optimum: µp ≤ µ∗
q.
To see the second, note that µ∗
q will be an expected value over the {vx}, and hence less
than or equal to the maximum, V . Because the probabilities in q∗ are strictly positive, the
q must actually be strictly less than the maximum: µ∗
expected value µ∗

q < V .

Utilizing Lemma 5 in Appendix B, for any feasible q such that the KL Divergence
constraint is not achieved with equality, a diﬀerent feasible q(cid:48) exists with an improved value
of the objective function. Hence we can rewrite the optimization problem as,

maxq µq =

(cid:88)

x∈S

qxvx,

s.t.

(cid:88)

x∈S
(cid:88)

px ln

(cid:19)

(cid:18) px
qx

= δ,

qx = 1,

x∈S
qx > 0

21

x ∈ S.

(6)

(7)

(8)

Cowan, Katehakis, and Pirutinsky

We now turn to the main task, reducing the dimension of the optimization problem.

Using Lagrange multipliers we have the following auxiliary function,

L(q, λ, µ) =

(cid:88)

x∈S

qxvx + λ

(cid:32)

(cid:88)

x∈S

px ln

(cid:19)

(cid:18) px
qx

(cid:33)

− δ

+ µ

(cid:33)

qx − 1

.

(cid:32)

(cid:88)

x∈S

Note that when using the Lagrange multipliers, we can safely ignore the positivity
inequality constraints in Eq. (8) because they are strict inequalities, thus inactive, and
removing them will not change the local optimum.

Taking partial derivatives, we get,

L(cid:48)

qx(q, λ, µ) = vx −

L(cid:48)

λ(q, λ, µ) =

L(cid:48)

µ(q, λ, µ) =

(cid:88)

x∈S
(cid:88)

x∈S

λpx
qx

px ln

+ µ , ∀x ∈ S,
(cid:18) px
qx

− δ,

(cid:19)

qx − 1.

Setting them to zero, results in the following system of equations for the optimal solution,

q∗,

(cid:88)

x∈S

px ln

λpx
q∗
x

, ∀x ∈ S,

(9)

(cid:19)

vx + µ =
(cid:18) px
q∗
x
(cid:88)

= δ,

q∗
x = 1.

x∈S

We are looking for a solution q∗ to this system, and any such solution will be a global
maximum. To see this, observe that our optimization problem is a convex optimization
problem. This can be seen more easily when put in its original form, as in Eq. (3). We are
maximizing a linear (and thus concave) function, the inequality constraint is convex, and
the equality constraints are aﬃne. Thus, any stationary point will be a local maximum and
any local maximum will be a global maximum. (Boyd and Vandenberghe, 2004)

Multiplying Eq. (9) through by q∗

x, we have,

λpx = q∗

x(vx + µ)

, ∀x ∈ S.

(10)

Summing Eq. (10) over x, we have

We now introduce a quantity, σ2

λ = µ∗

q + µ.

(11)
q∗, the variance under transition law q∗, explicitly deﬁned

as follows

σ2
q∗ =

(cid:88)

x∈S

xv2
q∗

x − µ2

q∗.

22

(12)

Accelerating Computation of UCB and other Indices

Looking at Eq. (10) again, but this time, multiplying through by vx we get,

λpxvx = q∗

xv2

x + q∗

xvxµ , ∀x ∈ S.

Summing this over x yields,

µpλ = σ2

q∗ + µ2

q∗ + µµq∗.

(13)

Equations (11) and (13) form a system of equations with two unknowns µ and λ. Solving

this system yields,

µ =

λ =

σ2
q∗ + µ2

q∗ − µpµq∗

µp − µq∗

,

σ2
q∗
µp − µq∗

.

Substituting them into the ﬁrst equation in the original system Eq. (9), and recalling

the relationship between λ and µ from Eq. (11), we get that for each x:

px
q∗
x

=

=

=

vx
λ
vx
λ
vx
λ

+

+

+

µ
λ

µ
µq∗ + µ
µq∗ + µ − µq∗
µq∗ + µ

= 1 +

vx − µq∗
λ

.

(14)

We can now rewrite the optimization problem in Eq. (3) in terms of our new variables

using Eq. (14).

The positivity constraint in Eq. (8) and recalling that px > 0 for all x ∈ S, yields,

px
q∗
x

= 1 +

vx − µq∗
λ

> 0,

the normalization constraint in Eq. (7) yields,

(cid:88)

x

1 +

px
vx − µq∗
λ

= 1,

and the KL divergence constraint in Eq. (6) yields,

(cid:18)

px ln

1 +

(cid:19)

vx − µ∗
q
λ

= δ.

(cid:88)

x∈S

Observe that µp must be strictly less than µ∗

q. To see this, take q = p, then q is feasible
and the left hand side of Eq. (5) is 0 which is less than δ. Lemma 5 implies there exists
some feasible q(cid:48) with a strictly greater objective function, i.e. µp = µq < µ(cid:48)
q. We also
know that λ < 0 because σ2

q∗ > 0 by deﬁnition in Eq. (12).

q ≤ µ∗

23

Cowan, Katehakis, and Pirutinsky

Thus we can rewrite the optimization problem in Eq. (3) as, follows:

maxµq,λ µq,

s.t.

(cid:88)

x∈S
(cid:88)

x

1 +

(cid:18)

px ln

1 +

(cid:19)

vx − µq
λ

= δ,

px

λ
λ + vx − µq

= 1,

vx − µq
λ

> 0

µp < µq < V and λ < 0.

∀x ∈ S,

(15)

Having established that λ is strictly less than zero we can simplify the last constraint,

Eq. (15), as follows. Let V = maxx vx

Thus we have,

1 +

vx − µq
λ
vx − µq
λ

> 0, ∀x ∈ S

> −1, ∀x ∈ S

vx − µq < −λ, ∀x ∈ S
µq − vx > λ, ∀x ∈ S

=⇒ µq − V > λ.

maxµq,λ µq,

s.t.

(cid:18)

px ln

1 +

(cid:19)

vx − µq
λ

= δ,

px

λ
λ + vx − µq

= 1,

(cid:88)

x∈S
(cid:88)

x

µp < µq < V and λ < µq − V.

Which is just two equations with two unknowns. Recalling that any feasible solution
will be a global maximum by our discussion of the convexity of the optimization problem,
we have the desired result,

C(p, v, δ) = µ∗
q,

24

Accelerating Computation of UCB and other Indices

Where the only unknowns are µ∗

q and λ, and they satisfy these constraints:

(cid:18)

px ln

1 +

(cid:19)

vx − µ∗
q
λ

= δ,

(cid:88)

x∈S

px

(cid:88)

λ
λ + vx − µ∗
q
x
q < V and λ < µ∗

= 1,

q − V.

µp < µ∗

A.3 Proof of Theorem 3

First we restate Theorem 3:

The value of D(p, v, ρ) and by extension Dt(a) can be easily found in the following cases:

• If ρ > V then the optimization problem, D(p, v, ρ) is infeasible and we say D(p, v, ρ) =

∞ and Dt(a) = −Txt,a(t).

• If ρ ≤ µp then D(p, v, ρ) = 0 and we say Dt(a) = ∞.

• If vx1 (cid:54)= vx2 for some x1, x2 ∈ S and ρ = V , then optimization problem D(p, v, ρ)

diverges to inﬁnity and we say D(p, v, ρ) = ∞ and Dt(a) = −Txt,a(t).

Proof For ρ > V = maxx vx, the optimization problem is infeasible because there is no
feasible q that will have an average more than V (i.e. (cid:80)
x qxvx ≤ V ). In that case we take
D(p, v, ρ) = ∞ and the corresponding DMED discrepancy index Dt(a) = −Txt,a(t).

For any ρ ≤ µp, i.e. less than or equal to the expected value under the current estimates,
D(p, v, ρ) = 0 by simply taking q∗ = p and we take the corresponding DMED discrepancy
index Dt(a) = ∞.

If vx1 = vx2 for all x1, x2 ∈ S then µp = vx = V and depending on the value of ρ one of

the previous two situations apply.

If vx1 (cid:54)= vx2 for some x1, x2 ∈ S and ρ = V we have the following. Any feasible q such
that (cid:80)
x qxvx = V must have qx = 0 for some x ∈ S such that vx < V , in which case q
falls outside of Θ - and it is in fact not feasible. We therefore take D(p, v, ρ) = ∞ and the
corresponding DMED discrepancy index Dt(a) = −Txt,a(t).

A.4 Proof of Theorem 4

In this section we will prove Theorem 4, which we restate here. Let V = maxx vx. Then,
for any v such that vx1 (cid:54)= vx2 for some x1, x2 ∈ S and for (cid:80)

x∈S pxvx < ρ < V ,

D(p, v, ρ) =

(cid:88)

x

px ln(1 + (ρ − vx)λ),

25

Cowan, Katehakis, and Pirutinsky

where

(cid:88)

x

px

ρ − vx
1 + (ρ − vx)λ

= 0,

0 < λ <

1
V − ρ

.

Before giving the formal proof, it may be helpful to understand the overall conception of
the proof. The main idea is the use of Lagrange multiplier techniques, which greatly reduces
the dimensionality of the problem to be solved. We are able to exchange from trying to
ﬁnd the optimal probability vector q∗, to a problem where we need only ﬁnd two moments
of the optimal q∗, a dramatic dimension reduction. In the MDP-DMED case we are able to
simplify even further, because the optimal unknown mean µ∗
q is given as ρ, and it suﬃces
to ﬁnd λ = (µ∗

q∗ which is a function of the unknown optimal variance.

q − µp)/σ2

The proof follows along similar lines as the one for MDP-UCB in Appendix A.2.

Proof Recall that,

(cid:40)

D(p, v, ρ) = inf
q∈Θ

I(p, q) :

(cid:41)

qxvx ≥ ρ

.

(cid:88)

x

(16)

We want to show that the inﬁmum in EQ. (16) is realized by a minimum.

Let 0 < (cid:15) < 1 and x∗ = arg max vx. Consider the probability vector q(cid:48) deﬁned as
x = (cid:15)/|S| for x (cid:54)= x∗. For the appropriate choice of (cid:15), we will have
x∗ = 1 − (cid:15) and q(cid:48)
q(cid:48)
(cid:80)
xvx = ρ < V with ﬁnite valued I(p, q(cid:48)). Thus, D(p, v, ρ) ≤ I(p, q(cid:48)) and we can restrict
x q(cid:48)
to only considering q ∈ Θ such that I(p, q) ≤ I(p, q(cid:48)). This feasible set is closed and compact,
and hence the inﬁmum is realized by a minimum over this set. Since I(p, q(cid:48)) is diverging to
inﬁnity as (cid:15) → 0, this minimum must occur in the interior of the constrained feasible region.
Hence the inﬁmum without the additional constraint on feasibility will also be realized by
a minimum within the interior of the set {q ∈ Θ, (cid:80)

x qxvx ≥ ρ}.

Thus, we can rewrite the problem of computing D(p, v, ρ) in the following form:

minq

(cid:88)

x∈S

px ln

px
qx

,

s.t.

(cid:88)

x∈S
(cid:88)

qxvx ≥ ρ,

qx = 1,

x∈S
qx > 0

x ∈ S.

(17)

Here we can use Lemma 6 in Appendix B to observe that for any feasible q where the
constraint in Eq. (17) is strict, we can construct a feasible q(cid:48) with a strictly smaller objective
function (KL divergence w.r.t. p). As such, the optimum must occur when this constraint
is satisﬁed with equality, and the optimization problem can be re-written as so:

26

Accelerating Computation of UCB and other Indices

minq

(cid:88)

x∈S

px ln

px
qx

,

s.t.

(cid:88)

x∈S
(cid:88)

qxvx = ρ,

qx = 1,

x∈S
qx > 0

x ∈ S.

(18)

(19)

(20)

We now turn to the main task, reducing the dimension of the optimization problem.

Using Lagrange multipliers we have the following auxiliary equation,

L(q, λ, µ) = −

(cid:88)

x∈S

px ln

px
qx

+ λ

(cid:32)

(cid:88)

x∈S

(cid:33)

(cid:32)

qxvx − ρ

+ µ

(cid:33)

qx − 1

.

(cid:88)

x∈S

Note when using the Lagrange multipliers, we can safely ignore the positivity constraints
in Eq. (20) because they are strict inequalities, thus inactive, and thus have a Lagrange
multiplier of zero.

Taking partial derivatives, we get,

+ λvx + µ , ∀x ∈ S,

L(cid:48)

qx(q, λ, µ) =

L(cid:48)

λ(q, λ, µ) =

L(cid:48)

µ(q, λ, µ) =

px
qx
(cid:88)

x∈S
(cid:88)

x∈S

qxvx − ρ,

qx − 1.

Setting them to zero, results in the following system of equations for the optimal solution,

q∗,

−

px
q∗
x

= λvx + µ , ∀x ∈ S,

(21)

q∗
xvx = ρ,

(cid:88)

x∈S

q∗
x = 1.

(cid:88)

x∈S

We are looking for a solution q∗ to this system, and any such solution will be a global
minimum. To see this, observe that our optimization problem is a convex optimization
problem. We are minimizing a convex function, with aﬃne equality constraints. Thus, any
stationary point will be a local minimum, and any local minimum will be a global minimum.
(Boyd and Vandenberghe, 2004)

27

Cowan, Katehakis, and Pirutinsky

Consider the ﬁrst equation: multiply through by q∗

x to get −px = λvxq∗

x +µq∗

x. Summing

this over x and simplifying accordingly, we get −1 = λρ + µ.

If we take −px = λvxq∗

x+µq∗

x and multiply through by vx, we get −vxpx = λv2

xq∗

We now introduce two new quantities, ρp, the mean under transition law p, and σ2
variance under transition law q∗, explicitly deﬁned as follows

x+µvxq∗
x.
q∗, the

ρp =

(cid:88)

x

pxvx,

σ2
q∗ =

(cid:88)

x

xq∗
v2

x − ρ2.

(22)

Summing −vxpx = λv2
x + µvxq∗
q∗ + ρ2) + µρ. So we have two equations and two unknowns,

xq∗

λ(σ2

x over x and simplifying accordingly, we get −ρp =

−1 = λρ + µ,
−ρp = λ(σ2

q∗ + ρ2) + µρ.

Solving these for λ and µ we have,

λ =

ρ − ρp
σ2
q∗

,

µ = −1 −

ρ − ρp
σ2
q∗

ρ.

(23)

Substituting them into the ﬁrst equation in the original system Eq. (21), and noting

that Eq. (23) implies µ = −1 − λρ, we get that for each x:

px
q∗
x

= −λvx − µ

= −λvx + 1 + λρ
= 1 + (ρ − vx)λ.

(24)

In order to reduce the original problem to a 1-dimensional problem, we now express each
of the constraints in terms of our new variables using Eq. (24). The positivity constraint
in Eq. (20) and recalling that px > 0 for all x ∈ S, yields,

px
q∗
x

= 1 + (ρ − vx)λ > 0,

the normalization constraint in Eq. (19) yields,

(cid:88)

x

px
1 + (ρ − vx)λ

= 1,

and the mean constraint in Eq. (18) yields,

(cid:88)

x∈S

px
1 + (ρ − vx)λ

vx = ρ.

28

Accelerating Computation of UCB and other Indices

Therefore, we can express the problem in Eq. (16), noting Eq. (24) above for the px/q∗
x

term, as follows:

px ln (1 + λ(ρ − vx)) ,

minλ

(cid:88)

x

s.t.

(cid:88)

x
(cid:88)

px
1 + (ρ − vx)λ
px
1 + (ρ − vx)λ

x∈S
1 + λ(ρ − vx) > 0

= 1,

vx = ρ,

∀x ∈ S.

(25)

We next establish feasible bounds for λ. Observe that the variance, σ2

q∗ is strictly greater
than 0 by deﬁnition in Eq. (22) and by recalling that there exists some x1, x2 ∈ S such that
vx1 (cid:54)= vx2. We also know that ρ > ρp = (cid:80)

x pxvx by assumption. Thus, λ > 0.

Having established that λ is strictly greater than zero we can simplify the last constraint,

Eq. (25), as follows. Let V = maxx vx,

1 + λ(ρ − vx) > 0, ∀x ∈ S

=⇒ 1 + λ(ρ − V ) > 0

1 + λρ − λV > 0

1 + λρ > λV

1 > λ(V − ρ)

1
(V − ρ)

> λ.

Where the last step is justiﬁed by recalling that by assumption V is strictly greater than ρ.

So, 0 < λ <

1
(V − ρ)

and our optimization problem becomes,

px ln (1 + λ(ρ − vx)) ,

minλ

(cid:88)

x

s.t.

= 1,

vx = ρ,

(cid:88)

x
(cid:88)

x∈S

px
1 + (ρ − vx)λ
px
1 + (ρ − vx)λ

0 < λ <

1
(V − ρ)

.

29

(26)

Cowan, Katehakis, and Pirutinsky

Taking a closer look at the normalization constraint, Eq. (26),

0 =

=

=

=

(cid:88)

x
(cid:88)

x
(cid:88)

x
(cid:88)

x

= −λ

px
1 + λ(ρ − vx)

− 1

px

px

px

(cid:18)

1
1 + λ(ρ − vx)

(cid:19)

− 1

(cid:18)

−

1
1 + λ(ρ − vx)
(cid:18) 1 − 1 − λ(ρ − vx)
1 + λ(ρ − vx)
(cid:18) (ρ − vx)

1 + λ(ρ − vx)

(cid:88)

px

x

(cid:19)

.

(cid:19)

1 + λ(ρ − vx)
1 + λ(ρ − vx)
(cid:19)

However, recalling that λ is strictly positive, it must be that (cid:80)

x px

(cid:16) (ρ−vx))
1+λ(ρ−vx)

(cid:17)

= 0.

Hence we have:

px ln (1 + λ(ρ − vx)) ,

minλ

(cid:88)

x

s.t.

px

(cid:18) (ρ − vx))

(cid:19)

1 + λ(ρ − vx)
px
1 + (ρ − vx)λ

vx = ρ,

= 0,

(cid:88)

x
(cid:88)

x∈S

(27)

(28)

0 < λ <

1
(V − ρ)

.

Next we show that any λ that satisﬁes Eq. (27) will also satisfy Eq. (28) and thus we

can remove that constraint,

0 =

=

=

=

(cid:88)

x
(cid:88)

x
(cid:88)

x
(cid:88)

x

(cid:18) (ρ − vx))

(cid:19)

px

+

1 + λ(ρ − vx)
−pxvx
1 + λ(ρ − vx)
−pxvx
1 + λ(ρ − vx)
−pxvx
1 + λ(ρ − vx)

+ ρ

x
(cid:88)

x

+ ρ · 1.

(cid:88)

pxρ
1 + λ(ρ − vx)

px
1 + λ(ρ − vx)

Where the last line is justiﬁed by recalling Eq. (26). Thus we have established that,

(cid:88)

x

−pxvx
1 + λ(ρ − vx)

= −ρ =⇒

(cid:88)

x

pxvx
1 + λ(ρ − vx)

= ρ,

30

Accelerating Computation of UCB and other Indices

which is Eq. (28).

Thus we can write the optimization problem as,

px ln (1 + λ(ρ − vx)) ,

minλ

(cid:88)

x

s.t.

(cid:88)

px

x

(cid:18) (ρ − vx))

(cid:19)

1 + λ(ρ − vx)

= 0,

0 < λ <

1
V − ρ

.

(29)

Recall that any feasible solution will be a global minimum, by our discussion of the
convexity of the optimization problem. To ﬁnd a feasible solution, notice that the derivative
of the objective function with respect to λ is simply the ﬁrst constraint, Eq. (29). Therefore
any stationary point of the objective function will satisfy the constraint, be feasible, and
thus be a global minimum. Hence, we may replace the original optimization problem with
the problem of solving,

(cid:88)

px

x

subject to 0 < λ < 1

V −ρ .

Thus we have the desired result,

(cid:18) (ρ − vx))

(cid:19)

1 + λ(ρ − vx)

= 0,

D(p, v, ρ) =

(cid:88)

x

px ln(1 + (ρ − vx)λ),

Where the only unknown is λ, and it satisﬁes these constraints:

(cid:88)

x

px

ρ − vx
1 + (ρ − vx)λ

= 0,

0 < λ <

1
V − ρ

.

Appendix B. KL Divergence Optimization Lemmas

The purpose of this section is to state and prove a number of lemmas associated with convex
optimization problems involving KL-Divergence terms. They are relevant, but tangential
to most of the content of the paper.

In this section, we take p ∈ Θ to be a distribution over S, with v to be the vector of
intermediate state values. It is convenient to deﬁne µp = (cid:80)
x pxvx and V = maxx vx. The
vector q is taken to be another distribution over S, with possibly zero-valued elements. The
KL Divergence between p and q is given by

px
qx

.

I(p, q) =

(cid:88)

px ln

x

31

Cowan, Katehakis, and Pirutinsky

Lemma 5 Let q ∈ Θ be such that I(p, q) < δ < ∞, and suppose vx1 > vx2 for some
x1, x2 ∈ S. Then there is a valid probability distribution q(cid:48) such that I(p, q(cid:48)) ≤ δ, and

(cid:88)

x∈S

qxvx <

q(cid:48)
xvx.

(cid:88)

x∈S

Proof Consider constructing an alternative q(cid:48) ∈ Θ in the following way. Deﬁne q(cid:48)
qx1 + ∆, q(cid:48)
will be a valid probability distribution vector over S.

x1 =
x = qx for x (cid:54)= x1, x2. Note that for 0 ≤ ∆ < min(qx1, qx2), q(cid:48)

x2 = qx2 − ∆, and q(cid:48)

We have that for ∆ > 0,

q(cid:48)
xvx −

(cid:88)

x

(cid:88)

x

qxvx = (qx1 + ∆)vx1 + (qx2 − ∆)vx2 − qx1vx1 − qx2vx2

= ∆(vx1 − vx2)
> 0.

It remains to show that the KL Divergence I(p, q(cid:48)) does not exceed δ. Note the following

relations,

I(p, q(cid:48)) =

(cid:88)

px ln

px
q(cid:48)
x

px
qx

x
(cid:88)

=

=

x(cid:54)=x1,x2
(cid:88)

px ln

x

px
qx

px ln

+ px1 ln

+ px2 ln

px1
qx1 + ∆
px1
qx1 + ∆

px2
qx2 − ∆

+ px2 ln

px2
qx2 − ∆

− px2 ln

px2
qx2 − ∆

px1
qx1

+ px1 ln

− px1 ln

= I(p, q) + px1 ln

qx1
qx1 + ∆

+ px2 ln

qx2
qx2 − ∆

.

So, if ∆ = 0 then I(p, q(cid:48)) = I(p, q) < δ. Noting that additional terms in the last equation
above are smooth functions of ∆, I(p, q(cid:48)) will not exceed δ in a neighborhood of ∆ = 0.
Thus for suﬃciently small ∆ > 0, the Lemma holds.

Lemma 6 For any q such that

(cid:88)

x∈S

qxvx > ρ ≥

(cid:88)

x∈S

pxvx,

(30)

if vx1 (cid:54)= vx2 for some x1, x2 ∈ S, there exist distributions q(cid:48) such that I(p, q(cid:48)) ≤ I(p, q) and

(cid:88)

x∈S

qxvx >

(cid:88)

x∈S

q(cid:48)
xvx ≥ ρ.

Proof As a consequence of our assumption that (cid:80)
x pxvx, there must be some
vx1 (cid:54)= vx2 such that q puts more weight on the larger and p puts more weight on the smaller.
Let vx1 > vx2, with qx1 > px1 and qx2 < px2.

x qxvx > (cid:80)

32

Accelerating Computation of UCB and other Indices

Consider constructing an alternative distribution q(cid:48) ∈ Θ in the following way. For
x = qx for x (cid:54)= x1, x2. As

0 ≤ ∆ < qx1, deﬁne q(cid:48) by q(cid:48)
before, for ∆ in this range, q(cid:48) ∈ Θ represents a valid probability distribution on S.

x2 = qx2 + ∆, and q(cid:48)

x1 = qx1 − ∆, q(cid:48)

As in the proof of Lemma 5, we have that for ∆ > 0,

q(cid:48)
xvx −

(cid:88)

x

(cid:88)

x

qxvx = (qx1 + ∆)vx1 + (qx2 − ∆)vx2 − qx1vx1 − qx2vx2

= ∆(vx1 − vx2)
> 0.

Taking ∆ suﬃciently small (so that the mean does not drop below ρ), we have that

(cid:88)

x∈S

qxvx >

(cid:88)

x∈S

q(cid:48)
xvx ≥ ρ.

It remains to show that I(p, q(cid:48)) ≤ I(p, q). Similar to the proof of Lemma 5, we have that

I(p, q(cid:48)) = I(p, q) + px1 ln

qx1
qx1 − ∆

+ px2 ln

qx2
qx2 + ∆

.

Hence we see that I(p, q(cid:48)) = I(p, q) when ∆ = 0. Looking at the derivative of I(p, q(cid:48)) with
respect to ∆ at ∆ = 0, we see

d
d∆

I(p, q(cid:48))|∆=0 =

px1
qx1

−

px2
qx2

< 0,

where the last step follows since px1/qx1 < 1 and px2/qx2 > 1, as discussed initially.
Hence while the KL divergences are equal for ∆ = 0, I(p, q(cid:48)) is decreasing within some
small neighborhood, and the KL divergence between p and q(cid:48) is reduced.

References

Martin Abadi et al. Tensorﬂow: A system for large-scale machine learning. In 12th USENIX
Symposium on Operating Systems Design and Implementation (OSDI 16), pages 265–283,
2016.

Agrawal, Rajeev, Demosthenis Teneketzis, and Venkatachalam Anantharam. Asymptoti-
cally eﬃcient adaptive allocation schemes for controlled Markov chains: Finite parameter
space.” Proceedings of the 27th IEEE Conference on Decision and Control, IEEE, pages
1198-1203, 1988.

Agrawal, Rajeev, M. V. Hedge, and Demosthenis Teneketzis. Asymptotically eﬃcient adap-
tive allocation rules for the multiarmed bandit problem with switching cost. IEEE Trans-
actions on Automatic Control, 33(10), 899-906, 1988.

Peter Auer and Ronald Ortner. Logarithmic online regret bounds for undiscounted rein-
In Advances in Neural Information Processing Systems 19, pages

forcement learning.
49–56. MIT Press, 2007.

33

Cowan, Katehakis, and Pirutinsky

Peter Auer and Ronald Ortner. Ucb revisited: Improved regret bounds for the stochastic
multi-armed bandit problem. Periodica Mathematica Hungarica, 61(1-2), 55–65, 2010.

Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed

bandit problem. Machine Learning, 47(2), 235–256, May 2002.

Dimitri Bertsekas. Reinforcement Learning and Optimal Control. Athena Scientiﬁc, 2019.

Vivek Borkar and Pravin Varaiya. Identiﬁcation and adaptive control of Markov chains.

SIAM Journal on Control and Optimization, 20(4), 470-489, 1982.

Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge university press,

2004.

A. N. Burnetas and M. N. Katehakis. Optimal adaptive policies for sequential allocation

problems. Advances in Applied Mathematics, 17, 122–142, 1996.

A. N. Burnetas and M. N. Katehakis. Optimal adaptive policies for Markov decision pro-

cesses. Mathematics of Operations Research, 22(1) 222–255, 1997.

Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge

University Press, 2006.

Wesley Cowan, Junya Honda, and Michael N. Katehakis. Normal bandits of unknown means

and variances. The Journal of Machine Learning Research, 18 (1) 5638–5665, 2017.

Wesley Cowan, Michael N. Katehakis, and Daniel Pirutinsky. Reinforcement learn-
ing: a comparison of ucb versus alternative adaptive policies, 2019. arXiv preprint
arXiv:1909.06019, 2019

Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying pac and regret: Uniform pac
bounds for episodic reinforcement learning. In Advances in Neural Information Processing
Systems, pages 5713-5723, 2017.

Cyrus Derman. Finite State Markovian Decision Processes, volume 19. Academic Press,

Inc., Orlando, FL, USA, 1970.

Finale Doshi-Velez and George Konidaris. Hidden parameter Markov decision processes:
A semiparametric regression approach for discovering latent task parametrizations. In
IJCAI: proceedings of the conference, volume 2016, page 1432. NIH Public Access, 2016.

Yonathan Efroni, Nadav Merlis, Mohammad Ghavamzadeh, and Shie Mannor. Tight re-
gret bounds for model-based reinforcement learning with greedy policies. arXiv preprint
arXiv:1905.11527, 2019

Eugene A. Feinberg, Pavlo O. Kasyanov, and Michael Z. Zgurovsky. Partially observ-
able total-cost Markov decision processes with weakly continuous transition probabilities.
Mathematics of Operations Research, 41 (2) 656–681, 2016.

John Gittins, Kevin Glazebrook, and Richard Weber. Multi-Armed Bandit Allocation In-

dices. John Wiley & Sons, Ltd, March 2011.

34

Accelerating Computation of UCB and other Indices

John C. Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal

Statistical Society: Series B (Methodological), 41 (2) 148–164, 1979.

Junya Honda and Akimichi Takemura. An asymptotically optimal bandit algorithm for

bounded support models. volume 85, pages 67–79, 01 2010.

Junya Honda and Akimichi Takemura. An asymptotically optimal policy for ﬁnite support
models in the multiarmed bandit problem. Machine Learning, 85 (3) 361–391, Dec 2011.

Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforce-

ment learning. Journal of Machine Learning Research, 11 (Apr) 1563–1600, 2010.

Michael N Katehakis and Cyrus Derman. Computing optimal sequential allocation rules in

clinical trials. Lecture notes-monograph series, pages 29–39, 1986.

Michael N Katehakis and Arthur F Veinott Jr. The multi-armed bandit problem: decom-
position and computation. Mathematics of Operations Research, 12 (2) 262–268, 1987.

Michael N Katehakis, Uriel G Rothblum, et al. Finite state multi-armed bandit problems:
Sensitive-discount, average-reward and average-overtaking optimality. The Annals of
Applied Probability, 6 (3) 1024–1034, 1996.

Taylor W. Killian, Samuel Daulton, George Konidaris, and Finale Doshi-Velez. Robust and
eﬃcient transfer learning with hidden parameter Markov decision processes. In Advances
in Neural Information Processing Systems, pages 6250–6261, 2017.

T. L. Lai and H. Robbins. Asymptotically eﬃcient adaptive allocation rules. Advances in

Applied Mathematics, 6 4–22, 1985.

Chandrashekar Lakshminarayanan, Shalabh Bhatnagar, and Csaba Szepesv´ari. A linearly
relaxed approximate linear program for Markov decision processes. IEEE Transactions
on Automatic Control, 63 (4) 1185–1191, 2017.

Tor Lattimore and Csaba Szepesv´ari. Bandit Algorithms. preprint 2018.

P. Mandl. Estimation and control in Markov chains. Advances in Applied Probability, 6(1),

pages 40-60, 1974.

Aditya Mahajan and Demosthenis Teneketzis. Multi-armed bandit problems. In Founda-

tions and Applications of Sensor Management, pages 121–151. Springer, 2008.

David McKay. Information Theory, Inference and Learning Algorithms. 2003.

Ian Osband and Benjamin Van Roy. On lower bounds for regret in reinforcement learning.

arXiv preprint arXiv:1608.02732, 2016.

Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for
reinforcement learning? In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pages 2701–2710. JMLR. org, 2017.

35

Cowan, Katehakis, and Pirutinsky

H. Robbins. Some aspects of the sequential design of experiments. Bull. Amer. Math.

Monthly, 58, pages 527–536, 1952.

Isaac M Sonin and Constantine Steinberg. Continue, quit, restart probability model. Annals

of Operations Research, 241 (1-2) 295–318, 2016.

Ambuj Tewari and Peter L. Bartlett. Optimistic linear programming gives logarithmic
regret for irreducible mdps. In Advances in Neural Information Processing Systems 20,
volume 25, pages 1505–1512, 2008.

William R Thompson. On the likelihood that one unknown probability exceeds another in

view of the evidence of two samples. Biometrika, 25 (3/4) 285–294, 1933.

Christopher John Cornish Hellaby Watkins. Learning from Delayed Rewards. PhD thesis,

King’s College, Cambridge, UK, May 1989.

Richard Weber et al. On the Gittins index for multiarmed bandits. The Annals of Applied

Probability, 2 (4) 1024–1033, 1992.

Peter Whittle. Multi-armed bandits and the Gittins index. Journal of the Royal Statistical

Society: Series B (Methodological), 42 (2) 143–149, 1980.

36

