1
2
0
2

r
a

M
9

]
h
p
-
t
n
a
u
q
[

1
v
8
0
7
5
0
.
3
0
1
2
:
v
i
X
r
a

Machine learning the period ﬁnding algorithm

John George Francis and Anil Shaji
School of Physics, IISER Thiruvananthapuram, Kerala, India 695551∗
(Dated: March 11, 2021)

We use diﬀerentiable programming and gradient descent to ﬁnd unitary matrices that can be
used in the period ﬁnding algorithm to extract period information from the state of a quantum
computer post application of the oracle. The standard procedure is to use the inverse quantum
Fourier transform. Our ﬁndings suggest that that this is not the only unitary matrix appropriate
for the period ﬁnding algorithm, There exist several unitary matrices that can aﬀect out the same
transformation and they are signiﬁcantly diﬀerent from each other as well. These unitary matrices
can be learned by an algorithm. Neural networks can be applied to diﬀerentiate such unitary matrices
from randomly generated ones indicating that these unitaries do have characteristic features that
cannot otherwise be discerned easily.

I.

INTRODUCTION

In the recent past machine learning has caught on as a
viable tool in various ﬁelds and physics has been no ex-
ception. Design of Quantum algorithms has been, in gen-
eral, a relatively slow process since the advent of quan-
tum computation. Starting from the idea put forward by
Feynman [1] there are still only a few remarkable quan-
tum algorithms that stand out distinctly. Deutsch [2]
was the among the ﬁrst to show that there is an algo-
rithm implementable on a quantum computer that runs
in exponentially lesser oracle queries than its classical
counterpart. Simon [3] came up with another example of
a problem and a quantum algorithm that runs exponen-
tially faster. The major breakthrough came in 1994 when
Shor came up with a quantum algorithm for a problem of
great practical signiﬁcance. An algorithm for integer fac-
torisation in polynomial time, at the center of which sits
the quantum period ﬁnding algorithm. The algorithm
identiﬁes the period of a given function, given an ora-
cle which implements evaluation of a periodic function.
Classically, repeated function evaluation until the period
is ascertained takes at least r evaluations where r is the
period of the function. However, with a quantum oracle
one can determine the period in just a few evaluations of
the oracle.

The general structure of the algorithm for oracle type
of problems involves three stages. The ﬁrst is a prepa-
ration stage where the input register of a quantum com-
puter is put into a uniform superposition of all possible
states. This facilitates access to all possible inputs on
which further computation may be done. The second
stage is the oracle evaluation which evaluates the oracle
on this superposition state. The oracle has a well deﬁned
action on every computational basis state of the input
register. Its action on the superposition state therefore
yields a superposition of the evaluations on every basis
state. In essence, all evaluations on possible basis states
are available in superposition after applying the oracle.

∗ johnfrancis15@iisertvm.ac.in

In other words, the ’correct’ answer to the computational
problem is available at the output end of the oracle but
it is hidden within a massive superposition of all possible
outputs of the oracle. The third stage of the computa-
tion is a post processing that is also the clever bit of the
algorithm. The post processing stage is where the infor-
mation that the algorithm aims to determine about the
oracle is extracted from the state after oracle evaluation.
Design of the quantum post processing requires bringing
together knowledge of hidden symmetries of the prob-
lem, deep number theoretic relationships, understanding
of integral transforms and the ability to ’think quantum-
mechanically’ for implementing the desired transforma-
tions on a quantum register. The detailed example of the
period ﬁnding algorithm, explained in Sec. II, further il-
lustrates this point.

In the paper we investigate whether the quantum post-
processing step can be designed with assistance from
machine-learning. This is done for the speciﬁc case of
the quantum period ﬁnding algorithm for which an imple-
mentable post processing step is already known that in-
volves the inverse quantum Fourier transformation. The
post-processing step boils down to a unitary transforma-
tion on the superposition of oracle outputs that leads
to a state on which speciﬁc measurement can reveal the
answer one is looking for. In our case, the period of the
function is the answer that is sought. The objective of the
machine learning scheme is to produce a post-processing
unitary that puts the output register in quantum states
such that the unknown period of the function can be
deduced from its measurement statistics. The overall
strategy is to compute the elements of this unitary post-
processing transformation by minimizing a cost function
that is constructed based on the desired measurement
statistics.

The input into the machine learning algorithm are all
the elements of the post-processing unitary. The cost
function has to be optimized with respect to these in-
puts which,
in turn, are exponential in number with
the respect to the number of input qubits on which the
period ﬁnding algorithm is implemented. Diﬀerentiable
programming is a paradigm of programming where the
derivatives of the output (cost function) can be taken

 
 
 
 
 
 
with respect to the large number of inputs at the cost of
a small overhead using automatic diﬀerentiation [4] (as
opposed to ﬁnite diﬀerence or symbolic diﬀerentiation or
coding the derivatives manually). This allows us to em-
ploy the gradient descent optimization described in detail
in Sec. IV for ﬁnding the matrix elements that minimize
the cost function. In fact, a central part of many deep
learning and other machine learning approaches is back-
propagation which is a special case of diﬀerentiable pro-
gramming. In this paper we use diﬀerentiable program-
ming [5] to ﬁnd the post processing unitary. Signiﬁcantly
we ﬁnd that the post-processing unitary is not unique
even through till date only one such unitary - the in-
verse quantum Fourier transform - is known. We also ﬁnd
that we are able to use neural networks to classify uni-
tary matrices based on their usefulness as post-processing
unitaries for the period ﬁnding algorithm, indicating that
the unitaries produced by the machine learning algorithm
have common features that may not be evident from in-
spection or from their spectral characteristics.

Machine learning approaches have previously been em-
ployed for the analysis and discovery of quantum algo-
rithms. Gepp and Stocks [6] gives a review of the ap-
plication of genetic algorithms to evolve quantum algo-
rithms. Lukaz et al
[7] demonstrate a machine learn-
ing approach to discovering short-depth algorithms for
reducing computational errors on near term quantum
computers. Along the lines of the work presented in
this Paper, Bang et al. [8] use a classical-quantum hy-
brid simulation along with diﬀerential evolution to arrive
at unitary matrices for eﬀecting out the Deutsch-Josza
algorithm [2]. Design of unitary matrices for Simon’s
In [10] the use of a
algorithm is investigated in [9].
variational algorithm to recover a quantum circuit for
Grover’s algorithm [11] is demonstrated. Our work takes
this a step further and considers a quantum algorithm
that is of practical importance, namely the quantum pe-
riod ﬁnding. We also address the what is typically the
hardest part of the algorithm design, namely the post-
processing. Assistance from machine learning in this step
can potentially lead to implementable algorithms for the
likes of the hidden subgroup problem for which an eﬃ-
cient quantum algorithm is believed to exist but is not
known at present in the general case.

In the next section we outline the period ﬁnding algo-
rithm. Sec. III details the method used in implement-
ing a computer program that ﬁnds a unitary matrix for
the post oracle processing. The gradient descent, which
is the optimisation procedure we use,
is discussed in
Sec. IV. We also compare the measurement statistics of
the output states produced by the post-processing uni-
taries generated by the gradient descent algorithm with
that from the standard quantum period ﬁnding algorithm
in this section. We classify the post-processing unitaries
produced by machine learning algorithm using a feed-
forward neural network in Sec. V and discuss the limi-
tations of our approach in Sec. VI. We summarize our
results in section VII.

2

II. THE PERIOD FINDING ALGORITHM

A function that takes n-bit numbers (integers) as input

and outputs n-bit numbers of the form

f : {0, 2, 3, 4, ...2n − 1} → {0, 2, 3, ...2n − 1},

is given. The function is known to be periodic with an
unknown period r i.e f (x + r) = f (x). In addition, it
is stipulated that the values of f do not repeat within
each period, i.e f (i), f (i + 1), f (i + 2), ...f (i + r − 1) are
all unique for all values of i. Since the function is from
integers to integers, this condition excludes only very few
periodic functions. More importantly, this class encom-
passes the types of functions encountered in order ﬁnding
that, in turn, is used in the integer factoring algorithm.
Given the task of determining the period r, one approach
is to compute f (1), f (2)...f (i) until we get a value f (s)
equal to f (1). Then s = r will give the period of the
function. However this approach requires at-least r func-
tion evaluations and r can be exponential in n since the
domain of the function is also exponential in n (r could
be as big as 2n−1). This simple and straightforward ap-
proach whose computational complexity is exponential
in n also turns out to be the best known deterministic
classical algorithm to ﬁnd the period.

The quantum period ﬁnding algorithm evaluates the
period of a function in queries polynomial in n. We
brieﬂy recap the algorithm for completeness and for es-
tablishing the notation we use. The quantum circuit for
the period ﬁnding algorithm is given in Fig. 1.

|0(cid:105)

H ⊗n

QF T

Of (i)

FIG. 1. Quantum circuit for the period ﬁnding algorithm

The circuit consists of a register X of n qubits with
Hilbert space of dimension N = 2n and another register
F with m qubits. All the qubits in both registers X and
F are initialized into the state |0(cid:105). In the ﬁrst step of
the algorithm, Hadamard gates are applied to each qubit
in the register X. After these Hadamard gates the joint
state of the two registers registers is

|ψ(1)(cid:105)XF =

1
√
2n

(cid:88)

i

|i(cid:105) |0(cid:105),

i = 0, . . . , 2n − 1.

The oracle is a unitary transformation acting on the two
registers X and F which implements the function in ques-
tion. It takes the basis state |i(cid:105) |0(cid:105) to |i(cid:105) |f (i)(cid:105). Thus its
action on (cid:12)
XF is to produce the state

(cid:12)ψ(1)(cid:11)

|ψ(2)(cid:105)XF =

1
√
2n

(cid:88)

i

|i(cid:105) |f (i)(cid:105) .

3

A measurement on the second register (F ) that yields a
value f (i0) with probability 1/r collapses |ψ(2)(cid:105)XF to

|ψ(3)(cid:105)XF =

1
(cid:112)[2n/r]

[2n/r]−1
(cid:88)

p=0

|i0 + pr(cid:105) |f (i0)(cid:105) .

(1)

The (inverse) quantum Fourier transform is then applied
on the ﬁrst register as implemented by the correspond-
ing post-processing unitary matrix which is the discrete
Fourier transform matrix. Using

(cid:114)

N
r

N/r−1
(cid:88)

p=0

|i0 + pr(cid:105)

QF T
−−−→

1
√
r

r−1
(cid:88)

q=0

ϕq
i0

|qN/r(cid:105),

where ϕq
is a phase dependent on the oﬀset i0 and q that
i0
is not important for our discussion, we obtain the state
of the two registers after the Fourier transformation as

FIG. 2. Example function with period 8. If the measurement
on the second register yields the answer 7 after the oracle,
then the amplitudes of the ﬁrst register are concentrated on
states |j(cid:105), where j are marked by the red lines in the ﬁgure.

|ψ(4)(cid:105)XF =

1
√
r

r−1
(cid:88)

q=0

|q[2n/r](cid:105) |f (i0)(cid:105) .

(2)

Measuring the ﬁrst register in the computational basis
yields any one of the r possible values, q2n/r with prob-
ability 1/r respectively. The measurement statistics ob-
tained by projecting the register X on to the complete
set of computational basis states is therefore a probability
distribution with equal height peaks at 2n/r intervals if
2n/r is an integer. If 2n/r is not an integer, then the dis-
tribution has narrow peaks around the integer [2n/r]. By
sampling this distribution, ﬁnding the location of these
peaks and with some classical post processing [12], to any
desired level of accuracy one can compute the period, r,
of the function.

For example, suppose we had a function of period 8 and
register X had 5 qubits. After the oracle, if the measured
value on the second register was 7, corresponding to |7(cid:105),
the state of the system would be

|ψ(4)(cid:105)XF =

1
2

3
(cid:88)

t=0

|7 + 8t(cid:105) |7(cid:105) .

Note that the amplitudes on the ﬁrst register are con-
centrated on |j(cid:105) where j correspond to the red lines in
Fig. 2. The periodicity of these amplitudes is revealed by
the ﬁnal post-processing step, namely the Fourier trans-
form. It can be easily seen that the structure of the state
of the X register in |ψ(3)(cid:105)XF is independent of the out-
come of the ﬁrst measurement on the register F . We
can therefore just as well ignore this measurement result
which would be equivalent to tracing over the register F
after the action of the post-processing unitary.

The period ﬁnding problem is a hidden-subgroup prob-
lem (HSP) on the group Zn. The standard way to ap-
proach Abelian HSP on a quantum computer is to put
the system ﬁrst in a uniform superposition of all possi-
ble input states and then apply the oracle. This puts

the ﬁrst register of the system in a state of uniform su-
perposition of cosets of the hidden subgroup [13]. Re-
vealing the hidden subgroup from this state is achieved
by an appropriate Fourier sampling. However for Non
Abelian groups Fourier sampling cannot reveal the infor-
mation about the hidden subgroup and no general algo-
rithm is known for arbitrary non-abelian groups. Ma-
chine learning approaches like the one we use potentially
be used to reveal unitary matrices for the quantum post-
processing for the case of Non Abelian subgroups and
solve the HSP. This could also, in principle, be used to
show that there exists unitaries that can reveal hidden
subgroups albeit for small groups due to the prohibitive
computational cost of simulating quantum computations
on classical computers.

We show here that using gradient descent learning one
can design a unitary matrix that carries out the quan-
tum post oracle processing which reveals the period of
the function. The unitary that is obtained is typically not
identical to the quantum Fourier transform unitary. This
shows that there exists other unitaries that eﬀect out
the same transformation. Gammelmark and Molmer [14]
have previously worked on the quantum Fourier trans-
form and used machine learning to improve the approx-
imations to the transformation whose experimental im-
plementation is simpler. The quantum Fourier transform
implemented as gates is a series of controlled phase shifts
of the form π/2m that connect all the qubits in a regis-
ter to one another. Even if the number of two qubit
gates required in this case scale as a polynomial in N , in
practice, gates connecting each qubit to all other qubits
are diﬃcult to implement. The question addressed using
machine learning in [14] is whether non-standard phase
shifts can approximate the Fourier transform better than
the usual ﬁxed phase shifts if the connectivity between
qubits is restricted. The investigations in [14] leads to
the conclusion that indeed this is the case.
In view of
this result, it is not entirely surprising that there exists

other unitary matrices that can carry out the same post-
processing for extracting period information.

The Probability of getting the output i on measuring
register X is,

4

III. MACHINE LEARNING THE UNITARY
MATRIX

The objective here is for a computer program to ﬁnd a
unitary matrix that does the post processing and reveals
the period using stochastic gradient descent or a simi-
lar optimizer. Stochastic gradient descent is a method
frequently used in machine learning to train neural net-
works. The program simulates the three stages of the
quantum period ﬁnding algorithm. The unitary matrices
in the ﬁrst two stages that put register X in a uniform
superposition and then perform the function evaluation
(oracle) are kept ﬁxed in the simulation. The program is
then tasked with ﬁnding the third stage post-processing
unitary matrix which generates the output state of the
algorithm on which a straightforward measurement will
reveal the period of the function.

We have a set of functions, {fj}, with known periods
that use as the training data. The periods of the func-
tions are limited to less than half the size of the domains
of the functions. For a given function fj from the train-
ing data a corresponding program function to implement
U2 is generated. U2 is the oracle unitary operator which
is deﬁned by U2(α |i(cid:105)) → α |i(cid:105) |f (i)(cid:105) and it depends on
the function fj from the training data set that is used.
The post-processing unitary matrix that we task the pro-
gram to ﬁnd is labeled as M3. The matrix M3 is initially
chosen to be an arbitrary (not necessarily unitary) ma-
trix that converges to a unitary matrix with the desired
property as the program progresses in its iterations. The
number of real independent parameters for an arbitrary
m × m complex matrix is 2m2. An n qubit system has
operators of size 2n × 2n, and therefore the number of
real independent parameters is 2 × 22n.

These 2 × 22n parameters will be tuned in the opti-
mization to get the desired unitary matrix. The X reg-
ister is initialized with |0(cid:105) and transformed through the
three transformations U1 = H ⊗n, U2(fi) and M3. The
ﬁnal state is obtained and the probability distribution
P (i) for getting integer i on measurement of register X
is calculated from this state after tracing over the sec-
ond register as mentioned earlier. This means that the
measurement operator on the second register need not
be implemented in the simulation. Out of the ﬁnal state
of the two registers all we have to do is to pick out the
probabilities corresponding to each computational basis
state of the ﬁrst register and sum over the associated
states of the second. So, in order to get the probability
distribution from the ﬁnal state | ˜ψ(4)(cid:105) (the tilde denoting
the suppressed intermediate measurement), we write the
state in the computational basis as

| ˜ψ(4)(cid:105) =

(cid:88)

i,j

αij |i, j(cid:105)

P (i) =

r−1
(cid:88)

j=0

|αij|2

In other words the probability of getting i on measure-
ment is the sum of the absolute value squares of the am-
plitudes of the states whose ﬁrst 2n bits in binary are a
binary representation of i.

The loss function whose value will be minimized during

the training(optimization) phase is

gloss =

1
2n

(cid:88)

i

(cid:2)Pa(i) − Pd(i)(cid:3)2

+

k
22n

(cid:88)

i,j

|M †

3 M3 − I|2
ij.

(3)
Here Pa (actual probability distribution) is the probabil-
ity distribution of the X register from the training circuit
and Pd (desired probability distribution) is the probabil-
ity distribution of X register from the conventional pe-
riod ﬁnding circuit. The ﬁrst sum in gloss is a measure of
the how close the distributions Pa and Pd are. It is al-
ways positive and goes to zero only if the distributions are
exactly identical. The second term is a unitary penalty
term and it quantiﬁes how far the matrix M3 is from be-
ing a unitary matrix. It is also always a positive number
and goes to zero only if M is a unitary matrix. Thus
when gloss is zero or close to zero, the distributions Pa
and Pd are identical or close to identical and the operator
M3 is unitary/almost unitary.

The loss function, gloss depends on the 2×22n variables
that are elements of M3 for a given function fj from the
training data set. The loss function has to be minimized
with respect to the elements of M3 across all functions in
the training data set. For doing the minimization we use
a variant of the gradient descent algorithm called ADAM
(Adaptive Moment Estimation) [15] which is a popular
choice in the ﬁeld of machine learning to training neural
networks.

FIG. 3. Schematic representation of the optimisation proce-
dure

IV. GRADIENT DESCENT OPTIMISATION

Gradient descent is a procedure to ﬁnd the minimum
of a multi-variable function. Typically we have a function
(over a data set) that we want to minimize:

G( (cid:126)w) =

1
n

n
(cid:88)

j

gj( (cid:126)w),

where each gj( (cid:126)w) corresponds to an instance of the train-
ing data. G( (cid:126)w) is therefore the average over the training
data. At each iteration (cid:126)w is updated by the rule,

(cid:126)w := (cid:126)w − α∇gj( (cid:126)w)

where α is step size, also called the learning rate. This
iteration is performed through for all j, namely all sam-
ples in the training set. This constitutes one epoch of
training. Multiple epochs are repeated till the function
G( (cid:126)w) converges to a desired minimum value. Selection
of the step size (or learning rate), α, is important. A low
value for α makes training reliable but it takes longer
for the training to converge on the minimum. On the
other hand, large α values may lead to the training not
converging or even diverging. This method of iterating
and updating over every data point is called Stochastic
Gradient Descent.

There exist several extensions and variants of the stan-
dard gradient descent that incorporates tuning the learn-
ing rate, adding “momentum” per parameter tuning rate
etc. [16]. A popular choice is ADAM [15], which takes
into account the ﬁrst moment and the second moment of
the gradients. Steps for ADAM optimisation are listed
in Appendix A in pseudo-code form. ADAM was found
to converge the cost function to 0 faster than gradient
descent and was the chosen method for all the optimi-
sations in the following. ADAM takes 3 conﬁguration
parameters for the algorithm, namely the learning rate
α, and the exponential decay rates for moment estimates
β1 and β2. We chose α to be 0.001 while β1 and β2 were
set to 0.9 and 0.99 respectively. The functions g1, g2,
g3... are the cost function values evaluated for diﬀerent
data points in the training data set.
In our use case,
elements of M3 form (cid:126)w and i indexes the training data
is a set functions fi) of various known periods. Note
that gloss not only depends on (cid:126)w but also the training
data-point it is evaluated on, i.e the periodic function fi
deﬁning the unitary matrix U2. The function gloss( (cid:126)w)
is evaluated and minimisation updates applied at a dif-
ferent data-points in every iteration of the loop, cycling
through all the training data points till G( (cid:126)w) converges
to the desired value, 0.

The optimisation procedure for 1 epoch on 5, 6 and 7
qubits took 0.343, 4.41 and 130.8 seconds respectively
on a 28 core Intel Xeon Gold 6132 cpu. We used
TensorFlow[17] (version 1.13) for its diﬀerentiable pro-
gramming features and its optimizers. Higher qubit cases
was found to converge faster. Cost function was found to

5

reduce to a suﬃciently small value (∼ 10−8) after 3000
epochs in the case of 5 qubits and 6 qubits and 2000
epochs in the case of 7 qubits. One run of the program
compiles the TensorFlow computational graph, generates
the dataset, runs the optimisation and saves the gener-
ated unitary to ﬁle. Time required and RAM usage for
each run of the program was 17.71 minutes (1.0 GB),
222.4 minutes (3.8 GB) and 4363.85 minutes (20.2 GB)
for the 5, 6 and 7 qubit cases respectively.

A. Results of the optimization

The program was run and training performed for 5, 6
and 7 qubit sized X registers. Given a size of register
n, the domain of the function is {0, 1, 2, 3, 4, ...2n − 1}.
Periodic functions were generated with period randomly
selected but which is less than half the domain size (16
for n = 5, 32 for n = 6 and 64 for n = 7). Data set
size was 10, 15 and 20 for n = 5, 6 and 7 respectively.
Training was run for 3000 epochs for 5 and 6 qubit cases
and 2000 for 7 qubits, which brought the average loss
function over the data set to the order of 10−8. Several
independent runs of the training were done and each run
converged on a diﬀerent set of 22n parameters (and thus
a diﬀerent unitary matrix) indicating signiﬁcantly that
there exists several other operators other than the inverse
quantum Fourier transform unitary matrix prescribed in
the standard period ﬁnding algorithm that eﬀect out the
same task.

FIG. 4. The cost function as training progresses

In order to show that the distinct post-processing
unitaries produced by the optimization are indeed dif-
fernt from each other in non-trivial ways, we used the
Loschmidt echo. The Loschmidt echo [18] of two unitary
matrices over a state quantiﬁes the diﬀerence between the
two unitary matrices when they act on that state. The
Loschmidt echo of two operators U1 and U2 with respect
a state |ψ(cid:105) is.

L =

(cid:12)
(cid:12)(cid:104)ψ| U †
(cid:12)

(cid:12)
(cid:12)
1 U2 |ψ(cid:105)
(cid:12)

2

.

Loschmidt echo ranges from 0 to 1 and a Loschmidt echo

050010001500200025003000epochs107105103101101lossof 1 on a particular state signiﬁes the unitary matri-
ces have identical action on that state. The Loschmidt
echo for the diﬀerent unitaries obtained from our pro-
cedure were calculated against the inverse quantum
fourier transform unitary matrix for the |ψ0(cid:105)X = |0(cid:105)⊗n.
Loschmidt echo on this state were,
in general, much
smaller than 1. However Loschmidt echo on the state
H ⊗n |0(cid:105) were close to 1, indicating all these matrices have
the same eﬀect on the input state of the period ﬁnding
algorithm, but are diﬀerent otherwise. The Loschmidt
echo over the uniform superposition state and the |0(cid:105)⊗n
state of some of the unitaries the program converged on
are tabulated below,

Loschmidt echo on
uniform superposition
0.99928572
1.00008533
0.99900781
1.0035981
0.99926629

1
2
3
4
5

Loschmidt echo on |0(cid:105)⊗n

0.00239334
0.00803578
0.01292283
0.00297156
0.0106097

A test data set with periodic functions of all period val-
ues was generated, including functions with periods not
in the training data set. The unitary matrix the program
converged on has an average value for the cost function
of the order of 10−8 on the test data set. The Loschmidt
echos on |0(cid:105)⊗n were also much less than 1. The functions
in the test data were not restricted to those having peri-
ods within half the domain size. Even for these, we ﬁnd
that the M3 converged upon by the program has a small
(∼ 10−8) value for the cost function. Thus the program
was able to ﬁnd a unitary matrix that carried out the
post-processing part of the period ﬁnding algorithm that
worked on a wide variety of periodic functions on which
it was tested.

In Fig. 5, it can be seen that the M3 converged upon
by the program gives a probability distribution very close
to the distribution from the period ﬁnding algorithm as
expected. The plot on top is the probability distribu-
tion over i of the target distribution that arises when the
quantum Fourier transform is used on a period 8 function
(n = 6). The plot in the middle is the probability dis-
tribution when the post-processing unitary discovered by
the algorithm is used and the one at the bottom is the
absolute diﬀerence between the two distributions. We
see that the absolute diﬀerence in probabilities are of the
order of 10−3.

In order to see how closely the output statistics from
the various unitaries produced by the algorithm compare
with that produced by the Fourier transform, the fol-
lowing procedure was adopted. Unitaries for 5 qubit, 6
qubit, and 7 qubit cases were generated and the probabil-
ity distributions obtained when these unitaries were used
for post-processing were computed. For the same func-
tions, the output distribution from the quantum Fourier
transform were also generated. The comparison is done
using a distance measure in the space of probability dis-

6

FIG. 5. The ﬁgure on top shows the output probability dis-
tribution for a period 8 function when the quantum post-
processing is done using the Fourier transform. The ﬁgure in
the middle shows the same when the post-processing unitary
obtained from the machine learning procedure is used. The
ﬁgure in the bottom shows the diﬀerence between the two
which is seen to be quite small.

tributions, (cid:80)(pi − qi)2/len(p), where len(p) is the size
of the sample space. It is found that the probability dis-
tributions arising when the generated unitaries are used
are very close to that generated by the Fourier trans-
form. Fig. 6 shows a histogram of the distribution of
these distances for diﬀerent functions of various periods
of the unitaries (7 in each case) that were generated for
5, 6 and 7 qubit cases. The histogram is sharply peaked
around 0, indicating that the two distributions that are
being compared are virtually identical.

V. UNITARY MATRIX CLASSIFICATION
USING NEURAL NETWORKS

A natural question to ask is that if the unitary oper-
ators found by this algorithm have some pattern com-
mon to all of them. There is of course, the obvious simi-

01020304050600.000.020.040.060.080.1001020304050600.000.020.040.060.080.1001020304050600.00000.00250.00500.0075erated ones.

7

FIG. 8. Distribution of phases of randomly generated unitary
matrices

Neural networks [19] are machine learning tools that
can learn complex patterns in data and can be then used
to classify new data based on its learning. Neural net-
works are universal function approximators [20]. A data
set can be thought of as a function from a set of inputs
to outputs and a neural network learns this function dur-
ing its training phase on the data set. To see if a neu-
ral network can learn a common pattern among these
unitaries we trained it to classify the unitaries into two
classes. Those of the type that can be used for quantum
post-processing in the period ﬁnding algorithm and those
that cannot. The full data set consists of unitary matrices
generated by the machine learning algorithm for 6 qubit
systems as well as randomly generated unitaries both in
equal numbers. 1200 of each were generated, thus a total
of 2400 matrices. The unitary matrices output by the al-
gorithm have an associated value of 1 and the randomly
generated unitary has an associated value of 0. Out of
this data set, 1800 unitaries were taken to train the neu-
ral network. A small portion (10%) of the training data
set is separated out and used as the validation data set.
This set is not used for training the network parameters,
but the loss and accuracy of the network evaluated on the
validation data set is tracked during the training phase
to check for over-ﬁtting. The remaining 600 constitute
the test data set for testing the predictions of the neural
network after the training is done. Initially the loss and
prediction accuracy of the training and validation data
set improve continuously. After a point however, the loss
and accuracy of the validation data set become poorer
(higher loss and lower accuracy) while the loss and accu-
racy on the training data set keeps improving. At this
point the neural network is considered to be over-ﬁtting
on the training data set and the training is stopped.

The input layer of the neural network has 22N +1 = 213
(for N = 6 qubits) nodes, which are fed the complex
matrix elements of the unitary matrix ﬂattened into an
array of 22N +1 elements. The network has two hidden
layers. The ﬁrst one has twice the number of nodes as
the input layer (214) and the subsequent hidden layer has

FIG. 6. Histogram showing distribution of distances between
desired and obtained distributions of the various unitary ma-
trices found various periodic functions

larity that they all produce output probability distribu-
tions that are virtually identical to the one produced by
the Fourier transform. However as the analysis of the
Loschmidt echo shows, this feature is particular to the
action of the unitary when the equal superposition state
is the input to the algorithm. Rather than depending on
the ﬁnding similarities between the action of the unitaries
on particular initial states, a deeper questions would be
whether an analysis of the unitaries themselves would
reveal a common pattern.

Direct inspection of these relatively large unitary ma-
trices did not lead to any similarities between them that
are readily identiﬁable. As the next step we attempted
to see if there are any similarities in the spectral fea-
tures of the unitaries. A histogram of the distribution
of eigenphases of the unitaries yields more or less a uni-
form distribution across 20 bins ranging form −π to π as
shown in Fig. 7. A sample of randomly generated unitary

FIG. 7. Distribution of eigenphases of the post-processing
unitary matrices generated by the machine learning proce-
dure.

matrices also show a similar distribution of eigenphases
as seen from Fig. 8. So there appears to be no discernible
pattern in the spectrum of the generated post-processing
unitaries that distinguishes them a set of randomly gen-

0.00.20.40.60.81.0distance1e60100200300400500600no3210123radians05001000150020002500300035004000frequency3210123radians05001000150020002500300035004000frequency8

VI. TRAINING DISTRIBUTION PROBLEM

The desired distribution that the program trains
against is an important point of consideration.
It was
a key piece in the design on the machine learning algo-
rithm and in particular for formulating the cost function
used in gradient descent optimization. Here the target
distribution used was the one that resulted from using
the inverse quantum Fourier transform as post process-
ing unitary. This was possible in the present case be-
cause one known post-processing unitary existed. Given
a problem for which one such solution is not known like
the general hidden subgroup problem, the target distri-
bution against which the machine learning algorithm can
be trained is a question that has to be addressed.

In general, the question of the distribution to be
trained against depends on how we can extract the re-
quired information from the distribution. A trivial prob-
ability distribution for period ﬁnding from which the pe-
riod is easily apparent would be one with a single peak
at r for a function of period r. Extracting the period
from this output state would be as simple as a single
measurement followed by read-out of the measured value.
The machine learning algorithm, when tasked to ﬁnd a
unitary M3 that gives this distribution, failed to reduce
the cost function suﬃciently close to 0 even after multi-
ple attempts. Other choices of target distributions that
were tried included a step function at r and a Gaussian
distribution centered around r truncated at domain end
points, both of which too failed to converge with cost
function suﬃciently close to 0. However when the out-
put distributions of various functions corresponding to a
randomly chosen post-processing unitary was fed as the
target distributions to the program, it does converge to
ﬁnd a unitary matrix that gives the same distributions.
This unitary matrix is again not the same as the original
one that generated the training data. This again hints
that the earlier distributions (peak at r, step function
at r and Gaussian centered around r) are not attainable
from a unitary transformation M3

It is possible that there is no unitary transformation
that can evolve the state after oracle operation into a
state with such a simple output probability distribution.
The other possibility is that looking for such a unitary
might by this method might be computationally infeasi-
ble. Adding ancilla qubits was another strategy that was
employed to see if such a ‘pointer’ type distribution can
arise out of post-processing the oracle output. The entire
system including the ancilla qubits was allowed to evolve
unitarily aiming to get the desired distribution when the
system qubits are subsequently measured. Adding one
ancilla qubit to a 5 qubit system did not show any signiﬁ-
cant decrease in convergence of loss function to 0. Adding
more ancilla qubits makes the computation intractable
on available computers. The problem of identifying a
target distribution that arises from a state that is uni-
tarily connected to the oracle output state in the cases
where there is no known post-processing unitary remains

FIG. 9. Loss function vs epochs during training

29 nodes while the output layer has a single node. All
layers use ReLu [21] (f (x) = x, ∀x > 0; f (x) = 0, ∀x < 0)
activation except for the ﬁnal layer which uses a Sigmoid
activation. Binary cross-entropy is a popular choice of
loss function for binary classiﬁcation and was the chosen
loss function for the Neural Network. When trained, the
output of the single output layer node will indicate if
the matrix is of the type found by the algorithm or not.
The actual value of the output is the probability that
it is of the right type of unitary given the inputs.
If
the output is greater than 0.5 the neural net prediction
is taken as that it is of the right matrix for the period
ﬁnding algorithm and if lesser than 0.5 is a prediction
that it is not. Output values closer to 1 indicate a high
probability that it is of the period ﬁnding form and closer
to 0 indicating it is not. The Neural network was trained
and within 20 training epochs an accuracy of 100% on
the training data set was obtained with an accuracy of
95% on the validation data set. After training the neural
network, when evaluated on the test data set, showed
an accuracy of 95%. The neural network also classiﬁed
the inverse Quantum Fourier Transform matrix as of the
period ﬁnding form with high degree of conﬁdence. The
output of the neural network for the inverse Quantum
Fourier transform was 0.98

FIG. 10. Accuracy vs epochs during training

0.02.55.07.510.012.515.017.50.00.10.20.30.40.50.6lossvalidation loss0.02.55.07.510.012.515.017.50.700.750.800.850.900.951.00accuracyvalidation accuracyone that requires further investigation.

VII. CONCLUSION

Our ﬁndings suggest the unitary matrix for post pro-
cessing the state after the oracle in the period ﬁnding
algorithm is not unique. The machine learning program
converged upon a diﬀerent unitary matrix every time it
was run. This is similar to the results obtained by [8]
who found non uniqueness of the post oracle unitary in
the Deutsch-Josza algorithm. The unitary matrices are
in general signiﬁcantly diﬀerent. A plot of the distribu-
tion of their eigenphases however does not show any in-
teresting features. In fact it is similar to the distribution
of eigenphases of randomly generated unitary matrices.
The action on |0(cid:105) state of each of the unitary matrices
generated by our program is in general diﬀerent as shown
by the Loschmidt echo with respect to this state. On the
other hand, all these unitary matrices have a similar ac-
tion on the uniform superposition state H ⊗n |0(cid:105) which is
the state the oracle acts upon.

Diﬀerent unitary matrices that can implement the

9

same quantum computation are useful with regard to
their ease of implementation on real physical quantum
hardware. Certain unitary operators might be imple-
mentable with fewer gates than others in a particular
hardware context. We also found that there exists a
hidden pattern in the unitary matrices which was dis-
cernible to a neural network when trained on a data set
of unitary matrices generated randomly and those con-
verged on by the program. These numerical experiments
show capabilities of simple feed-forward neural networks
in identifying operators that can eﬀect out the required
transformation of states. The fact that the mechanism of
neural-networks can be used in the domain of quantum
operators is promising. For instance, it is suggestive of
their applications to a more practical use of using gen-
erative networks [22] to design operators that can eﬀect
out required transformations.

ACKNOWLEDGMENTS

A.S. acknowledges the support of DST-SERB through
grant no. EMR/2016/007221 and the QuEST program
of DST through project No. Q113 under Theme 4.

[1] R. P. Feynman, Int. J. Theor. Phys 21 (1982).
[2] D. Deutsch and R. Jozsa, Proceedings of the Royal So-
ciety of London. Series A: Mathematical and Physical
Sciences 439, 553 (1992).

[3] D. R. Simon, SIAM Journal on Computing 26, 1474

(1997).

[4] R. E. Wengert, Commun. ACM 7, 463–464 (1964).
[5] A. G. Baydin, B. A. Pearlmutter, A. A. Radul,

and
J. M. Siskind, Journal of Machine Learning Research 18,
1 (2018).

[6] A. Gepp and P. Stocks, Genetic Programming and Evolv-

able Machines 10, 181 (2009).

[7] L. Cincio, Y. Suba¸sı, A. T. Sornborger, and P. J. Coles,

New Journal of Physics 20, 113022 (2018).

CA, USA, May 7-9, 2015, Conference Track Proceedings,
edited by Y. Bengio and Y. LeCun (2015).

[16] S. Ruder, arXiv preprint arXiv:1609.04747 (2016).
[17] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin,
S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Is-
ard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Lev-
enberg, D. Man´e, R. Monga, S. Moore, D. Murray,
C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever,
K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan,
F. Vi´egas, O. Vinyals, P. Warden, M. Wattenberg,
M. Wicke, Y. Yu, and X. Zheng, “TensorFlow: Large-
scale machine learning on heterogeneous systems,”
(2015), software available from tensorﬂow.org.

[8] J. Bang, J. Ryu, S. Yoo, M. Paw(cid:32)lowski, and J. Lee, New

[18] R. A. Jalabert and H. M. Pastawski, Phys. Rev. Lett.

Journal of Physics 16, 073017 (2014).
[9] K. H. Wan, F. Liu, O. Dahlsten,

and M. S.
Kim, “Learning simon’s quantum algorithm,” (2018),
arXiv:1806.10448 [quant-ph].

86, 2490 (2001).

[19] S. E. Dreyfus, Journal of Guidance, Control, and Dy-
namics 13, 926 (1990), https://doi.org/10.2514/3.25422.
[20] M. Leshno, V. Y. Lin, A. Pinkus, and S. Schocken, Neu-

[10] M. E. S. Morales, T. Tlyachev, and J. Biamonte, Phys.

ral Networks 6, 861 (1993).

Rev. A 98, 062333 (2018).

[11] L. K. Grover, in Proceedings of the Twenty-Eighth Annual
ACM Symposium on Theory of Computing, STOC ’96
(Association for Computing Machinery, New York, NY,
USA, 1996) p. 212–219.

[12] N. D. Mermin, “Period ﬁnding and continued fractions,”
in Quantum Computer Science: An Introduction (Cam-
bridge University Press, 2007) p. 197–200.

[13] C. Lomont, arXiv preprint quant-ph/0411037 (2004).
[14] S. Gammelmark and K. Mølmer, New Journal of Physics

[21] V. Nair and G. E. Hinton, in Proceedings of the 27th
International Conference on International Conference on
Machine Learning, ICML’10 (Omnipress, Madison, WI,
USA, 2010) p. 807–814.

[22] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Ben-
gio, in Proceedings of the 27th International Conference
on Neural Information Processing Systems - Volume 2,
NIPS’14 (MIT Press, Cambridge, MA, USA, 2014) p.
2672–2680.

11, 033017 (2009).

[15] D. P. Kingma and J. Ba, in 3rd International Conference
on Learning Representations, ICLR 2015, San Diego,

10

Appendix A: ADAM optimizer

The function to be minimized is f ( (cid:126)w). The algorithm
requires other parameters α, β1, β2 and an (cid:15)(to prevent
division by zero error). At every iteration the parameters
(cid:126)w are updated by the rules prescribed. Below is the algo-
rithm lifted from the article[15] published by the authors
of the algorithm.

All operations on vectors are element-wise. (cid:126)gt

2 is the

element-wise square of (cid:126)gt that is (cid:126)gt · (cid:126)gt
Require: α
Require: β1, β2 ∈ [0, 1)
Require: (cid:126)w0 ← random initial parameter vector
Require: f ( (cid:126)w), function to be minimized

m0 ← 0
v0 ← 0
t ← 0
while (cid:126)wt not converged do
t ← t + 1
(cid:126)gt ← ∇ (cid:126)wf ( (cid:126)wt−1)
(cid:126)mt ← β1 (cid:126)mt−1 + (1 − β1)(cid:126)gt
2
(cid:126)vt ← β2(cid:126)vt−1 + (1 − β2)(cid:126)gt

√

ˆvt + (cid:15))

ˆmt ← (cid:126)mt/(1 − βt
1)
ˆvt ← (cid:126)vt/(1 − βt
2)
(cid:126)wt ← (cid:126)wt−1 − α ˆmt/(
return (cid:126)wt
end while
Four parameters α, β1, β2 and (cid:15) are set. Vectors (cid:126)m0
and (cid:126)v0 are initialized to (cid:126)0. At every iteration (cid:126)gt (the
gradient at the time step t) is evaluated. Using (cid:126)gt, (cid:126)mt
and (cid:126)vt the exponential moving averages of the ﬁrst mo-
ment and the second moment of the gradient respectively
are calculated, the parameters β1 and beta2 control the
exponential decay rates of the moving averages. These
moving averages are estimates of the ﬁrst moment and
second moments of the gradient. Since the vectors (cid:126)m and
(cid:126)v are initialized to 0 there is an initial bias in these esti-
mates and they are corrected by dividing by (1 − βt
1) and
(1 − βt
2) respectively to give bias corrected moving av-
erages ˆm and ˆv, the article [15] describes further details
regarding the bias correction. Finally the parameters of
the function for the current time step (cid:126)wt are updated
by (cid:126)wt−1 + α ˆmt/((cid:112)(ˆv + (cid:15)). The maximum step size for
the update is shown to be α, (cid:15) is a parameter to prevent
division by zero errors

