Meta-learning from Learning Curves Challenge:
Lessons learned from the First Round and Design of the Second Round

2
2
0
2

g
u
A
4

]

G
L
.
s
c
[

1
v
1
2
8
2
0
.
8
0
2
2
:
v
i
X
r
a

Manh Hung Nguyen 1 Lisheng Sun 1 Nathan Grinsztajn 2 3 4 Isabelle Guyon 1 2 3 5

Abstract

Meta-learning from learning curves is an impor-
tant yet often neglected research area in the Ma-
chine Learning community. We introduce a series
of Reinforcement Learning-based meta-learning
challenges, in which an agent searches for the
best suited algorithm for a given dataset, based
on feedback of learning curves from the envi-
ronment. The ﬁrst round attracted participants
both from academia and industry. This paper an-
alyzes the results of the ﬁrst round (accepted to
the competition program of WCCI 2022), to draw
insights into what makes a meta-learner success-
ful at learning from learning curves. With the
lessons learned from the ﬁrst round and the feed-
back from the participants, we have designed the
second round of our challenge with a new proto-
col and a new meta-dataset. The second round
of our challenge is accepted at the AutoML-Conf
2022 and currently on-going.

1. Background and motivation

Meta-learning has been playing an increasingly important
role in Automated Machine Learning. While it is a natural
capability of living organisms, who constantly transfer ac-
quired knowledge across tasks to quickly adapt to changing
environments, artiﬁcial learning systems are still in their
meta-learning “infancy”. They are only capable, so far, to
transfer knowledge between very similar tasks. At a time
when society is pointing ﬁngers at AI for being wasteful with
computational resources, there is an urgent need for learning
systems, which recycle their knowledge. To achieve that
goal, some research areas have been widely studied, includ-
ing few-shot learning (Wang et al., 2020), transfer learning
(Zhuang et al., 2020), representation learning (Bengio et al.,
2013), continual learning (Delange et al., 2021), life-long
learning (Chen et al., 2018), and meta-learning (Vanschoren,
2018). However, meta-learning from learning curves, an
essential sub-problem in meta-learning (Mohr & van Rijn,
2022), is still under-studied. This motivates the design of
this new challenge series we are proposing.

Learning curves evaluate algorithm incremental perfor-
mance improvement, as a function of training time, number
of iterations, and/or number of examples. Our challenge
design builds on top of previous challenges and work, which
considered other aspects of the problem: meta-learning as
a recommendation problem, but not from learning curves
(Guyon et al., 2019; Liu et al., 2020) and few-shot learn-
ing (El Baz et al., 2021). Analysis of past challenges re-
vealed that top-ranking methods often involve switching
between algorithms during training, including “freeze-thaw”
Bayesian techniques (Swersky et al., 2014). However, pre-
vious challenge protocols did not allow evaluating such
methods separately, due to inter-dependencies between vari-
ous heuristics. Furthermore, we want to study the potential
beneﬁt of learned policies, as opposed to applying hand-
crafted black-box optimization methods.

Our challenge took inspiration from MetaREVEAL
(Nguyen et al., 2021), ActivMetal (Sun-Hosoya et al., 2018),
REVEAL (Sun-Hosoya, 2019), and Freeze-Thaw Bayesian
Optimization (Swersky et al., 2014). A meta-learner needs
to learn to solve two problems at the same time: algorithm
selection and budget allocation. We are interested in meta-
learning strategies that leverage information on partially
trained algorithms, hence reducing the cost of training
them to convergence. We offer pre-computed learning
curves as a function of time, to facilitate benchmarking.
Meta-learners must “pay” a cost emulating computational
time for revealing their next values. Hence, meta-learners
are expected to learn the exploration-exploitation trade-
offs between continuing “training” an already tried good
candidate algorithm and checking new candidate algorithms.

In this paper, we ﬁrst recap design of the ﬁrst round of
our challenge. Then, we present the ﬁrst round results
and compare the participants’ solutions with our baselines.
After the end of the ﬁrst round, we have identiﬁed some
limitations in our design and proposed a new design and a
new meta-dataset for the second round.

2. Design of the ﬁrst round

In this section, we describe the design of the ﬁrst round
of our challenge, including: data, challenge protocol, and

 
 
 
 
 
 
manuscript of paper accepted to CFOL Workshop @ ICML 2022

evaluation. More details can be found on our competition
page 1.

2.1. Learning curve data

Despite the fact that learning curves are widely used, there
were not many learning curve meta-datasets available in
the Machine Learning community at the time of organiz-
ing this challenge. Taking advantage of 30 cross-domain
datasets used in the AutoML challenge (Guyon et al., 2019),
we computed de novo learning curves (both on the valida-
tion sets and the test sets) for 20 algorithms, by submitting
them to the AutoML challenge (Guyon et al., 2019) as post-
challenge submissions. These algorithms are created from
two base algorithms, Random Forest and Gradient Boosting,
but with different values of the max features hyperparame-
ter. We also provided meta-features of each dataset, such as
type of learning task, evaluation metric, time budget, etc. In
this ﬁrst round, we considered a learning curve as a func-
tion of time. Each point on the learning curve corresponds
to the algorithm performance at a certain time.

In addition to the aforementioned real-world meta-dataset,
we synthetically generated 4000 learning curves for the par-
ticipants to practice and get familiar with our starting kit.
The points on each synthetic learning curve are sampled
from a parameterized sigmoid function with its hyperparam-
eters generated from matrix factorizations. This allows us
to create some hidden relationships between algorithms and
datasets (i.e. some algorithms perform well particularly for
some datasets). Details of how this synthetic meta-dataset
was created can be found in (Nguyen et al., 2021).

2.2. Challenge protocol

We organized a novel two-phase competition protocol:

• Development phase: participants make as many sub-
missions as they want 2, which are evaluated on the
validation learning curves.

• Final test phase: no new submissions are accepted in
this phase. The last submission of each participant in
the Development phase is transferred automatically to
this phase. It is evaluated on the test learning curves.

Note that the meta-datasets are never exposed to the partici-
pants in neither phase, because this is a challenge with code
submission (only the participants’ agent sees the data).

This setting is novel because it is uncommon in reinforce-

1https://codalab.lisn.upsaclay.fr/

competitions/753

2up to a comfortable limit that was never reached in our com-
petition. The limit also aims to avoid overﬁtting the validation sets,
though it has been shown that participants are usually aware not to
overﬁt them (Roelofs et al., 2019)

ment learning to have a separate phase for agent “develop-
ment” and agent “testing”. Validation learning curves are
used during the development phase and test learning curves
during the ﬁnal phase, to prevent agent from overﬁtting.
Moreover, we implemented the k-fold meta-cross-validation
(with k=6) to reduce variance in the evaluations of the agents
(i.e. 25 datasets for meta-training and 5 datasets for meta-
testing). The ﬁnal results are averaged over datasets in the
test folds.

During meta-training, learning curves and meta-data col-
lected on 25 datasets are passed to the agent for meta-
learning in any possible ways implemented by the agent.
Then during meta-testing, one dataset is presented to the
agent at a time. The agent interacts back and forth with
an “environment”, similarly to a Reinforcement Learning
setting. It keeps suggesting to reveal algorithms’ validation
learning curves and choosing the current best performing
algorithm based on observations of the partially revealed
learning curves.

2.3. Evaluation

In this challenge series, we want to search for agents with
high “any-time learning” capacities, which means the abil-
ity to have good performances if they were to be stopped
at any point in time. Hence, the agent is evaluated by the
Area under the agents’ Learning Curve (ALC) which is con-
structed using the learning curves of the best algorithms
chosen at each time step (validation learning curves in the
Development phase, and the test learning curves in the Fi-
nal phase). The computation of the ALC is explained in
(Nguyen et al., 2021). The results will be averaged over all
meta-test datasets and shown on the leaderboards. The ﬁnal
ranking is made according to the average test ALC.

As indicated in the competition rules, participants should
make efforts to guarantee the reproducibility of their meth-
ods (e.g. by ﬁxing all random seeds involved). In the Final
Phase, all submissions were run three times with the same
seed, and the run with the worst performance is used for
the ﬁnal ranking3. This penalizes any team who did not ﬁx
their own seeds.

On each dataset Di, we evaluated an agent Aj by the Area
under the Learning curve ALC j
i of the agent on the dataset.
In the ﬁnal ranking, the agent is ranked based on its average
ALC over all datasets (N = 30 datasets):

µj =

(cid:80)N

i=1 ALC j
N

i

(1)

To measure the variability of an agent Aj, we computed

3The output of each run can be found in this Google Drive

folder: [link]

manuscript of paper accepted to CFOL Workshop @ ICML 2022

the standard deviation of ALC scores obtained by the agent
over all datasets:

(cid:115)

σj =

(cid:80)N

i=1(ALC j

i − µj)2

N

(2)

3. Analyses of the ﬁrst round results

Results, ﬁnal rankings, and prizes in the Final phase of the
ﬁrst round are shown in Table 1. The 1st, 2nd, and 3rd
ranked teams qualifying for prizes, as per the challenge
rules4, were awarded prizes of 500$, 300$, and 200$, re-
spectively. In the remainder of this section, we give a more
in-depth view of how each team performed on each dataset
in this round, compared to our baselines.

In this round, the participants are asked to solve two tasks
simultaneously: algorithm selection and budget allocation.
Both of them are crucial to achieving our goal of maximiz-
ing the area under an agent’s learning curve. We found
approaches submitted by the participants using a wide range
of methods, from simple (e.g. using algorithm ranking
and pre-deﬁned values for ∆t) to more sophisticated (e.g.
predicting scores and timestamps of unseen learning curve
points). The results ﬁrst indicate that using both learned
policies (models) for choosing algorithms and spending
time budget (used by team MoRiHa and neptune) yields
better ALC scores than hard-coded ones (e.g. using a ﬁxed
pre-deﬁned list of ∆t in AIpert and our DDQN baseline).

According to Table 2, team MoRiHa obtained the highest
average ALC of 0.43 in the ﬁnal phase. It succeeded in
achieving the highest ALC score on 21 out of 30 datasets.
In addition, it performed notably better than other teams
in some datasets, such as tania, robert, newsgroups, and
marco. They are datasets of either multi-label or multi-class
classiﬁcation tasks with a very high number of features.
Moreover, most of them are sparse datasets, which is often
seen as a challenge for learners. Further investigation will be
done in our future work to understand the outperformance
of MoRiHa’s method on these particular datasets. We will
also encourage team MoRiHato join the second round to
study the robutness of their results. Team neptune has a
score of 0.42 which is very close to the winner’s, followed
by team AIpert with a score of 0.40. Team automl-freiburg,
which was ranked 4th, achieved a slightly lower score (0.37)
than our DDQN baseline (0.38), and so did team automl-
hannover (0.32).

The successes of the top-ranked team can be explained by
the strategies they implemented. Team MoRiHa, which ﬁn-
ished in the 1st place, uses a simple yet efﬁcient approach

4https://codalab.lisn.upsaclay.fr/

competitions/753

that explores the most promising algorithms and avoids wast-
ing time switching between too many different algorithms.
Interestingly, team neptune learns a policy for allocating
time budget using learning curve convergence speed. The
Reinforcement Learning-based approach of team AIpert is
very intuitive as our competition borrows the RL frame-
work to formulate our problem, which also explains our
baseline choice of DDQN. However, by complementing
it with the K-means clustering method, AIpert’s approach
achieved higher performance than our baseline. Both AIpert
and automl-freiburg share the same idea of suggesting algo-
rithms based on dataset similarity.

4. Winning solutions

In this section, we brieﬂy introduce strategies of the win-
ning solutions. More details on their implementations can
be found in our factsheet summary (Appendix A) or in in-
dividual factsheets provided by the winning teams in Table
1.

4.1. Team MoRiHa (1st place)

According to team MoRiHa, they focus on “doing the right
thing at the right time”. Their agent is very goal-oriented (i.e.
maximizing the area under the learning curve) and does not
rely on complex models or expensive computations. They
emphasize the importance of having an accurate schedule
regarding the invested time (choosing ∆t for each query) in
order to avoid wasting time budget. One of their key ﬁndings
is that switching between algorithms during exploration is
very costly and it is rarely beneﬁcial to switch the explored
algorithm more than once. They build a time model for
each algorithm to predict the time when the ﬁrst point
on the algorithm’s learning curve is available. In addition,
they keep a list of algorithms ranked descending based on
their ALC in the meta-training phase. In meta-testing their
algorithm scheduler explores algorithms in an order starting
from the best algorithm. If an algorithm’s learning curve
stales, the next algorithm will be chosen. Time allocation
is done using the time models and a heuristic procedure.
This is the only team having an assumption that the learning
curves are monotonically increasing.

4.2. Team neptune (2nd place)

As described by team neptune, they train a learning curve
predictor to predict unseen points on the learning curves
(i.e. by interpolating the original scores) in order to ﬁnd
the best algorithm for a new dataset. In addition, they train
an algorithm classiﬁer to categorize algorithms into three
groups based on their learning curve convergence speed:
Fast/Median/Slow. Different budget allocation strategies
will be selected according to the algorithm’s convergence
type. Regarding their implementation, they train MLP net-

manuscript of paper accepted to CFOL Workshop @ ICML 2022

Table 1. Final phase ranking of the ﬁrst round. Teams are ranked based on their average ALC scores, which were recorded on the worst
run among three runs. Team MoRiHa was ranked 1st, but not qualiﬁed for a monetary prize. Teams neptune, AIpert, and automl-freiburg
are qualiﬁed for the prizes.

Rank

Team
(username)

ALC
score

Monetary Prize
qualiﬁcation

NO

OK

OK

OK

1

2

3

4

5

6

7

8

MoRiHa
(username: jmhansel)

neptune
(username: neptune)
AIpert
(username: AIpert)
automl-freiburg
(username: automl-freiburg)
automl-hannover
(username: amsks)
pprp
(username: pprp)
arushsharma24
(username: arushsharma24)
Xavier
(username: Xavier)

0.43

0.42

0.40

0.37

0.32

0.24

0.23

0.17

Comments/
source code
This team is not qualiﬁed for
a monetary prize due to close
relation with the organizers
[CODE URL] [FACTSHEET]

[CODE URL] [FACTSHEET]

[CODE URL] [FACTSHEET]

[CODE URL] [FACTSHEET]

[CODE URL] [FACTSHEET]

works to perform both tasks: learning curve prediction and
algorithm classiﬁcation.

4.3. Team AIpert (3rd place)

According to team AIpert, their method aims at uncov-
ering good algorithms as fast as possible, using a low-
computational cost and simple process. The novelty lies in
the combination of an off-policy Reinforcement Learning
method (Q-learning) and K-means clustering model. As
similar datasets usually have the same learning behavior,
organizing similar datasets into groups based on their meta-
features is essential. They thus build a Q-learning matrix for
each dataset cluster (12 clusters in total). In meta-training,
each dataset is seen multiple times and the corresponding
Q-learning matrix is updated. This allows the agents to
be exposed to more situations (different observations and
rewards) on a dataset. In meta-testing, they ﬁrst determine
which cluster the given dataset belongs to. Then, the Q-
learning matrix associated with the cluster is utilized as a
policy to guide the agent. ∆t is not chosen as an absolute
value but from a ﬁxed list of time portions of the total time
budget.

4.4. Team automl-freiburg (4th place)

As described by team automl-freiburg, their algorithm selec-
tion policy is based on a Deep Gaussian Processes (DGP)
surrogate, in a similar way to FSBO (Wistuba & Grabocka,

2021). The surrogate aims to predict the performance of
an algorithm at time t + ∆t, with ∆t chosen by a trained
budget predictor. The DGP is trained during the meta-
training phase, and ﬁne-tuned in the meta-testing phase.
During meta-training, they store the best algorithm for each
dataset to be used as the ﬁrst algorithm to query during
meta-testing. The “best” algorithm is deﬁned as the one
that has the highest y0
, which means they favor algorithms
t0
that achieve high performance early. Given a new dataset
in meta-testing, the best algorithm of the closest dataset
(previously seen in meta-training, based on the euclidean
distance) will be selected. During meta-testing, they keep
updating the observed algorithms and ﬁne-tune the DGP
surrogate.

5. Lessons learned and new design

In this section, we describe the limitations of our original
design and how we addressed them in the second round.

First, one set of limitations arises because we precomputed
learning curves (performance as a function of time) and
therefore have a ﬁxed predetermined sampled time points.
In our ﬁrst challenge edition, we opted to interpolate in
between time points with the last recorded performance. The
criticism of participants was that in real life, if they chose
an in between time point, they would get newer information.
To mitigate that, in the new challenge round, the participants
cannot choose any time point (thus no need to interpolate),

manuscript of paper accepted to CFOL Workshop @ ICML 2022

Table 2. ALC scores of the top 5 methods: MoRiHa (AT01), neptune (AT02), AIpert (AT03), automl-freiburg (AT04), automl-
hannover (AT05), and our baselines: Double Deep Q Network (DDQN), Best on Samples (BOS), Freeze-Thaw BO (FT), Average
Rank (AR), Random Search (RS). The reported scores correspond to the worst of 3 runs for each method. The last row shows the
average ALC scores (in descending order, from left to right) over 30 datasets.

manuscript of paper accepted to CFOL Workshop @ ICML 2022

they have to choose the next pre-computed learning curve
point.

phase (only their submitted agent is exposed to the meta-
datasets).

We thus provide a new type of learning curve for the second
round: learning curve as a function of training data size,
as opposed to learning curves as a function of time. The
time budget for querying a point on a learning curve will be
returned by the environment and not chosen by the agent,
which means that the agent has to pay whatever it costs.

Second, the test learning curves were highly correlated with
the validation curves. Therefore, one could overﬁt the for-
mer by simply overﬁtting the latter. In the second round, the
agent will always be evaluated using the test learning curves
but on a completely different set of datasets in each phase
(Development phase and Final phase).

The second round of our challenge was accepted at the
AutoML-Conf 2022. Participation in the ﬁrst round is not a
prerequisite for the second round. The second round comes
with some new features, including:

• Learning curve: we focus on learning curves as func-
tions of training data size. We thus collected a new
large meta-dataset of such learning curves.

• Competition protocol: Given a portfolio of algo-
rithms, an agent suggests which algorithm and the
amount of training data to evaluate the algorithm on
a new task (dataset) efﬁciently. The agent observes
information on both the training learning curves and
validation learning curves to plan for the next step.
Test learning curves, which are kept hidden, will be
used for evaluating the agent.

• Data split: We use half of the meta-dataset for the De-
velopment phase and the other “fresh” half to evaluate
the agent in the Final phase.

We created a new meta-dataset of pre-computed learning
curves of 40 algorithms with different hyperparameters on
30 datasets used in the AutoML challenge. The algorithms
are created from four base algorithms: K-Nearest Neighbors
(KNN), Multilayer Perceptron (MLP), Adaboost, Stochas-
tic Gradient Descent (SGD), but with different values of
hyperparameters. We added meta-features of datasets and
hyperparameters of algorithms. We respected the data split
of the AutoML challenge to produce three sets of learning
curves for each task, from the training, validation, and
test sets. The type of metric used to compute the learning
curves of the meta-dataset is provided in the meta-features
of the dataset. We also generated a new synthetic meta-
dataset that contains 12000 learning curves, in a similar way
used in the ﬁrst round, but with a new type of learning curve
as explained above (learning curve as a function of training
data size).

In meta-training, the following data is given to the agent
to meta-learn: meta-features of datasets, hyperparameters
of algorithms, training learning curves, validation learning
curves, and test learning curves. While in meta-testing, the
agent interacts with an environment in a Reinforcement
Learning style. Given a portfolio of algorithms, an agent
suggests which algorithm and the amount of training data
to evaluate the algorithm on a new task (dataset) efﬁciently.
The agent observes information on both the training learning
curve and validation learning curve to plan for the next step.
An episode ends when the given time budget is exhausted.
The following two lines of code demonstrate the interactions
between the agent and the environment:

action = trained agent.suggest(observation)

observation, done = env.reveal(action)

The second round is split into three phases:

with:

• Public phase (1 week): participants practice with the

given starting kit and sample data

• Development phase (6 weeks): participants submit
agents that are meta-trained and meta-tested on the
platform. 15 datasets will be used in this phase.

• Final phase (1 week): no further submissions are
made in this phase. Your last submission in the Devel-
opment phase will be forwarded automatically to this
phase and evaluated on 15 fresh datasets (not used in
the Development phase).

Like in the ﬁrst round, this is a competition with code sub-
mission and the participants do not see the data in either

observation : a tuple of
(A, p, t, R train A p, R validation A p), with:

• A: index of the algorithm provided in the previous

action,

• p: decimal fraction of training data used, with the value

of p in [0.1, 0.2, 0.3, ..., 1.0]

• t: the amount of time it took to train A with train-
ing data size of p, and make predictions on the train-
ing/validation/test sets.

• R train A p: performance score on the training set

• R validation A p: performance score on the valida-

tion set

manuscript of paper accepted to CFOL Workshop @ ICML 2022

action: a tuple of (A, p), with:

6. Conclusion

The ﬁrst round results of our challenge have revealed that
agents that learn both policies for selecting algorithms and
allocating time budget are more successful in our challenge
setting. Team MoRiHa, who ﬁnished in 1st place, outper-
formed all other teams and our baselines on two-thirds of
the datasets. We propose a novel setting and a new meta-
dataset for the second round and present baseline results.
DDQN maintains its advantages in the second round, while
an intuitive and simple baseline as BOS can work quite
well in the second round. We are looking forward to see
whether the ﬁndings of the ﬁrst round will be reinforced and
whether participants’ solutions can outperform our baselines
signiﬁcantly in the second round.

For our future work, we want to perform more post-
challenge analyses to verify whether progress was made
in meta-learning from learning curves. First, we would like
to do a point-by-point comparison of the winning meth-
ods in the ﬁrst round, based on their fact sheets. Second,
to investigate further the winning methods and see what
contributed the most to their success, we want to perform
systematic experiments in collaboration with the winners.
More concretely, we will build a common workﬂow and ask
participants to conduct ablation studies. Lastly, we are also
interested in examining the effect of changes in our reward
function hyper-parameters on participants’ performances.

Acknowledgment

We would like to thank challenge participants for provid-
ing feedback and sharing their methods. We are grateful
to Chalearn for donating prizes and Google for provid-
ing computing resources. This work was supported by
ChaLearn, the ANR Chair of Artiﬁcial Intelligence HU-
MANIA ANR-19-CHIA-0022 and TAILOR EU Horizon
2020 grant 952215.

Software and Data

All software (including starting kit and winning so-
is open-sourced on our website (https:
lutions)
//metalearning.chalearn.org/).
The meta-
datasets will remained private on the challenge platform
(Codalab) to serve as a long-lasting benchmark for research
in meta-learning.

• A: index of the algorithm to be trained and tested

• p: decimal fraction of training data used, with the value

of p in [0.1, 0.2, 0.3, ..., 1.0]

The scoring program automatically chooses the best algo-
rithm at each time step (i.e. the algorithm with the highest
validation score found so far, which is different from the
ﬁrst round where it was chosen by the agent) to compute
the agent’s test learning curve (as a function of time spent).
The metric used for ranking on the leaderboard is the Area
under the agent’s Learning Curve (ALC).

5.1. Baseline results of the second round

We re-use our baselines from the ﬁrst round, including:
Double Deep Q Network, Best on Samples, Freeze-Thaw
Baysian Optimization, Average Rank, and Random Search
agents. Their implementations are taken from the ﬁrst round
(Anonymous-paper) with some modiﬁcations in order to
work with our new protocol and meta-dataset. We also
provide them (except the Random Search agent) a smarter
policy for choosing the training data size in each step. Each
agent keeps track of the algorithms (A) and the training data
size (p) tested on the algorithms. Every time it re-selects an
algorithm, it should increase p by 0.1 to go forward on the
algorithm learning curve.

We run each method three times and report the worst run of
each of them in Table 3. Similar to the ﬁrst round, among
the baselines, DDQN still performs best in the second round
with an average ALC of 0.38. It is the winner (co-winner)
of 16 out of 30 datasets with the highest performance dif-
ference compared to other baseline methods seen on the
dataset tania. BOS and FT are not far behind with average
scores of 0.37 and 0.36, respectively. The improvement of
BOS compared to the ﬁrst round can be explained by the
adaptation of its strategy in this round. It tries each method
on a small subset of training data ﬁrst (e.g. 10 percent of
training samples), instead of spending a ﬁxed amount of
time as in the previous round. This should help it not waste
a time budget since choosing an adequate amount of time to
get a new point on learning is no longer necessary in the sec-
ond round. The success of BOS suggests that information
on the ﬁrst point of the learning curve is crucial to decisions
on selecting algorithms.

AR and RS perform poorly with the same score of 0.28. Al-
though focusing only on one algorithm as what AR method
does can bring beneﬁts in some cases (e.g. on dataset ca-
data and didonis), a dataset-dependent policy for selecting
algorithms is still necessary to be successful on multiple
cross-domain tasks.

manuscript of paper accepted to CFOL Workshop @ ICML 2022

Table 3. Baseline results in the second round. We show ALC scores of Double Deep Q Network (DDQN), Best on Samples (BOS),
Freeze-Thaw Bayesian Optimization (FT), Average Rank (AR), and Random Search (RS) on each dataset, in descending order of the
average ALC scores (on the last row), from left to right. The ﬁrst 15 datasets are used in the Development phase, the rest is for the Final
phase.

manuscript of paper accepted to CFOL Workshop @ ICML 2022

Roelofs, R., Shankar, V., Recht, B., Fridovich-Keil, S.,
Hardt, M., Miller, J., and Schmidt, L. A meta-analysis
of overﬁtting in machine learning.
In Wallach, H.,
Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F.,
Fox, E., and Garnett, R. (eds.), Advances in Neural
Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.
neurips.cc/paper/2019/file/
ee39e503b6bedf0c98c388b7e8589aca-Paper.
pdf.

Sun-Hosoya, L.

Meta-Learning as a Markov De-
cision Process.
Theses, Universit´e Paris Saclay
(COmUE), December 2019. URL https://hal.
archives-ouvertes.fr/tel-02422144.

Sun-Hosoya, L., Guyon, I., and Sebag, M. Activmetal:
Algorithm recommendation with active meta learning. In
IAL@PKDD/ECML, 2018.

Swersky, K., Snoek, J., and Adams, R. Freeze-thaw
bayesian optimization. arXiv preprint arXiv:2106.04480,
06 2014.

Vanschoren, J. Meta-learning: A survey. arXiv preprint

arXiv:1810.03548, 10 2018.

Wang, Y., Yao, Q., Kwok, J., and Ni, L. Generalizing from
a few examples: A survey on few-shot learning. ACM
Computing Surveys, 53:1–34, 06 2020. doi: 10.1145/
3386252.

Wistuba, M. and Grabocka, J. Few-shot bayesian opti-
mization with deep kernel surrogates. arXiv preprint
arXiv:2101.07667, 2021. doi: 10.48550/ARXIV.2101.
07667. URL https://arxiv.org/abs/2101.
07667.

Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong,
H., and He, Q. A comprehensive survey on transfer
learning. Proceedings of the IEEE, PP:1–34, 07 2020.
doi: 10.1109/JPROC.2020.3004555.

References

Bengio, Y., Courville, A., and Vincent, P. Representation
learning: A review and new perspectives. IEEE transac-
tions on pattern analysis and machine intelligence, 35:
1798–1828, 08 2013. doi: 10.1109/TPAMI.2013.50.

Chen, Z., Liu, B., Brachman, R., Stone, P., and Rossi, F.
Lifelong Machine Learning. Morgan & Claypool Publish-
ers, 2nd edition, 2018. ISBN 1681733021.

Delange, M., Aljundi, R., Masana, M., Parisot, S., Jia,
X., Leonardis, A., Slabaugh, G., and Tuytelaars, T. A
continual learning survey: Defying forgetting in clas-
IEEE Transactions on Pattern Anal-
siﬁcation tasks.
ysis and Machine Intelligence, pp. 1–1, 2021.
doi:
10.1109/TPAMI.2021.3057446.

El Baz, A., Guyon, I., Liu, Z., van Rijn, J. N., Treguer,
S., and Vanschoren, J. Advances in metadl: Aaai 2021
challenge and workshop. In Guyon, I., van Rijn, J. N.,
Treguer, S., and Vanschoren, J. (eds.), AAAI Workshop
on Meta-Learning and MetaDL Challenge, volume 140
of Proceedings of Machine Learning Research, pp. 1–16.
PMLR, 09 Feb 2021. URL https://proceedings.
mlr.press/v140/el-baz21a.html.

Guyon, I., Sun-Hosoya, L., Boull´e, M., Escalante, H. J.,
Escalera, S., Liu, Z., Jajetic, D., Ray, B., Saeed, M.,
Sebag, M., Statnikov, A., Tu, W.-W., and Viegas,
E. Analysis of the AutoML Challenge Series 2015–
2018, pp. 177–219. Springer International Publish-
ing, Cham, 2019.
doi:
10.1007/978-3-030-05318-5 10. URL https://doi.
org/10.1007/978-3-030-05318-5_10.

ISBN 978-3-030-05318-5.

Liu, Z., Pavao, A., Xu, Z., Escalera, S., Ferreira, F., Guyon,
I., Hong, S., Hutter, F., Ji, R., Nierhoff, T., Niu, K., Pan,
C., Stoll, D., Treguer, S., Wang, J., Wang, P., Wu, C., and
Xiong, Y. Winning solutions and post-challenge analyses
of the ChaLearn AutoDL challenge 2019. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, pp.
17, 2020.

Mohr, F. and van Rijn, J. N. Learning curves for decision
making in supervised machine learning – a survey. arXiv
preprint arXiv:2201.12150, 2022.

Nguyen, M. H., Grinsztajn, N., Guyon, I., and Sun-Hosoya,
L. Metareveal: Rl-based meta-learning from learning
curves. In Workshop on Interactive Adaptive Learning co-
located with European Conference on Machine Learning
and Principles and Practice of Knowledge Discovery in
Databases (ECML PKDD 2021), Bilbao/Virtual, Spain,
September 2021. URL https://hal.inria.fr/
hal-03502358.

manuscript of paper accepted to CFOL Workshop @ ICML 2022

A. Factsheet Summary

In this section, we summarize the information provided in the participants’ factsheets. Only top 5 teams submitted their
factsheets, including: MoRiHa, neptune, AIpert, automl-freiburg, and automl-hannover.

A.1. PRE-PROCESSING & FEATURE ENGINEERING

Question 1: Did you perform any data pre-processing methods?

Figure 1. Pre-processing methods include: Standard Scaler, Feature Scaling, One-hot Encoding, extracting the 90% convergence time and
ﬁnal performances.

Question 2: Did you perform feature engineering methods?

Figure 2. Feature engineering methods include: Clustering; Polynomial features of train num, feat num; Multi-step forecasting for data
augmentation.

A.2. DATA USED FOR LEARNING

Question 3: Did you use all points on the learning curves or only some of them?

manuscript of paper accepted to CFOL Workshop @ ICML 2022

Figure 3. Most of the methods use all points on the learning curves for learning.

Question 4: Did you make use of meta-features of datasets?

Figure 4. All methods took advantage of meta-features of datasets.

Question 5: Did you implement a Hyperparameter Optimization component for your agent using the provided
hyperparameters of algorithms?

Figure 5. Some HPO tools were used, such as Hyperband and FSBO.

Question 6: In case you used either or both meta-features of datasets and algorithms, did it improve the performance

manuscript of paper accepted to CFOL Workshop @ ICML 2022

of your method?

Figure 6. More experiments need to be done to conﬁrm whether meta-features of datasets and algorithms help.

Question 7: In any case, did you ﬁnd the meta-features useful in our meta-learning setting?

Figure 7. Not all participants ﬁnd the provided meta-features useful.

A.3. POLICY CHARACTERISTICS

Question 8: Does your agent learn a policy from datasets in the meta-training phase?

manuscript of paper accepted to CFOL Workshop @ ICML 2022

Figure 8. Most of the agents use a learned policy from the meta-training phase.

Question 9: How does your agent manage the exploration-exploitation trade-offs (revealing a known good algo-
rithm’s learning curve vs. revealing a new algorithm candidate’s learning curve )?

1. With an (cid:15) greedy policy, in a Reinforcement Learning framework, only in meta- training we create different Q-matrices.

In the meta-testing phase, we perform the choice of the new algorithm with the computed Q-matrices.

2. We are very restrictive with switching the explored algorrithm. We preselect the single best performing algorithm from
the validation learning curves statically and only explore other algorithms, if its learning curve is stale. So we strongly
emphasize exploiting the single best algorithm.

3. A modiﬁed Round Robin on the top-k performing algorithms. The incumbent i.e. the value of the top performing
algorithm on the test dataset is challenged by zero budget allocated algorithms. Since the training on the validation data
allows for immediate look up in the test dataset.

4. Bayesian Optimization

5. We ﬁnd the best algorithm by a learning curve predictor.

Question 10: Does your agent switch between learning curves during an episode (i.e. switching between training
different algorithms on the dataset at hand)?

Figure 9. All agents switch between learning curves to ﬁnd the best algorithm for the task at hand.

manuscript of paper accepted to CFOL Workshop @ ICML 2022

Question 11: Does your agent leverage partially revealed learning curves on the dataset at hand?

Figure 10. Some agents do not take into account information of partial learning curves on the task at hand for deciding their actions.

Question 12: Did you make any assumptions about the shapes of the learning curves?

Figure 11. Only one team made an assumption that the learning curves are monotonically increasing.

Question 13: Does your agent predict unseen points on a learning curve?

manuscript of paper accepted to CFOL Workshop @ ICML 2022

Figure 12. 3 out of 5 agents make decisions based on predicted performance scores.

Question 14: Does your agent perform pairwise comparisons of algorithms’ learning curves?

Figure 13. Pairwise comparisons of algorithms’ learning curves have been exploited in 60% of the agents.

Question 15: How does your agent spend the given time budgets (i.e. choosing delta t at each step)?

manuscript of paper accepted to CFOL Workshop @ ICML 2022

Figure 14. Participants use either or both hard-coded policy and learned policy to distribute a given time budget (no one does it randomly).

Question 16: How does your agent choose which algorithm to contribute to its learning curve at each step (i.e.
choosing A star)?

Figure 15. Most of the agents choose the best algorithm so far as A∗, with only one exception that uses Sequential Model Based
Optimisation (SMBO) with a Gaussian Process Surrogate.

Question 17: Which phase did you focus on more to improve your agent’s performance?

manuscript of paper accepted to CFOL Workshop @ ICML 2022

Figure 16. Participants give a slightly higher importance weight to meta-training than meta-testing.

Question 18: Did you build an algorithm ranking?

Figure 17. More than half of the participants build an algorithm ranking and use it as a tool for selecting algorithms.

Question 19: Did you use Reinforcement Learning to train your agent?

manuscript of paper accepted to CFOL Workshop @ ICML 2022

Figure 18. Only one participant applies Reinforcement Learning to train the agent.

A.4. METHOD IMPLEMENTATION

Question 20: What is the percentage of originality of your method/implementation?

Figure 19. The percentage of originality of their methods/implementations ranges from 60% to 80%.

Question 21: Did you use a pre-trained agent?

manuscript of paper accepted to CFOL Workshop @ ICML 2022

Figure 20. Only one participant pre-train their agent using all the provided datasets in meta-training.

Question 22: Is your method strongly based on an existing solution? Which one(s)?

Figure 21. Q-learning and starting kit baselines served as bases for 2 methods, while the other methods were built from scratch.

Question 23: Did you use Neural Networks for your agent?

manuscript of paper accepted to CFOL Workshop @ ICML 2022

Figure 22. Neural Networks were implemented in 3 out of 5 methods.

Question 24: Do you ﬁnd the provided libraries / packages / frameworks sufﬁcient?

Figure 23. The provided libraries/packages/frameworks were sufﬁcient for most of the participants.

Question 25: Check all Python packages/frameworks you used.

manuscript of paper accepted to CFOL Workshop @ ICML 2022

Figure 24. Scikit-learn and Pytorch are the most used packages by participants.

Question 26: Did you use any speciﬁc AutoML / Meta-learning / Hyperparameter Optimization libraries?

Figure 25. Only one participant uses SMAC for hyperparameter optimization.

Question 27: Was it difﬁcult for you to deal with the provided data format of the learning curves and meta-features?

manuscript of paper accepted to CFOL Workshop @ ICML 2022

Figure 26. 40% of participants struggled with the provided data format. Some comments include: rather than nested dictionaries create a
single dictionary with a tuple identiﬁer (dataset, algorithm).

Question 28: How much time did you spend developing your agents?

Figure 27. All participants completed their solutions within 4 weeks.

Question 29: What’s the difﬁculty induced by the computation resource (memory, time budget, etc) constraints?

manuscript of paper accepted to CFOL Workshop @ ICML 2022

Figure 28. The provided computation resource was reasonable to participants.

A.5. USER EXPERIENCE

Question 30: Was the challenge duration enough for you to develop your methods?

Figure 29. The challenge duration was enough for the participants.

Question 31: Your evaluation on the starting kit

manuscript of paper accepted to CFOL Workshop @ ICML 2022

Figure 30. Some improvements on the starting kit need to be made (see below).

Question 32: Which improvements can be made for the starting kit? You are welcome to list all issues and bugs you
ran into.

1. The algorithm meta features were non-informative, the dataset meta features’ categorical columns at some point were
non informative and the categories were not entirely represented in the validation dataset. Also the environment allows
to exploit 0 budgets to improve the agent’s performance in a non realistic way. Also we wanted to use many libraries,
that were not available (e.g pytorch geometric or networkx) or with other versions (e.g. sklearn’s quantile regression)
was not available initially. The validation & test set (on server) were slightly different. In particular, the distribution of
timestamps was dramatically different.

2. Return the whole learning curve after a suggestion, not only the last observed budget and the last observed performance.

3. It may be useful to query the learning curve at speciﬁc iterations or explain clearly the meaning of the timestamps of
the observed samples. If they are random or there is not an underlying pattern or structure, it is difﬁcult to predict the
next budget.

4. The ﬁnal rank should be obtained by running the methods on other meta-train dataset. In the current set-up, the test
curve is highly correlated to the validation curve (seen in the development stage), therefore simply overﬁtting the latter
will probably help to obtain good results in the former.

5. Decreasing the budget when the suggested algorithm+budget tuple is not enough to query a new point in the learning
curve is unrealistic. In the real world, once we decide to run some algorithm for a given time-range or number of
epochs, we will obtain some change in the accuracy (unless the learning curve is in a plateau).

Question 33: Your evaluation on the challenge website:
https://codalab.lisn.upsaclay.fr/competitions/753

manuscript of paper accepted to CFOL Workshop @ ICML 2022

Figure 31. One comment for the challenge website is: the way the ALC is explained is not very straightforward.

