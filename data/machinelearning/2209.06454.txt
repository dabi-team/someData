2
2
0
2

p
e
S
4
1

]

G
L
.
s
c
[

1
v
4
5
4
6
0
.
9
0
2
2
:
v
i
X
r
a

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION

1

Prediction Intervals and Conﬁdence Regions for Symbolic Regression
Models based on Likelihood Proﬁles

F. O. de Franca1 and G. Kronberger2
1Center for Mathematics, Computation and Cognition (CMCC), Heuristics, Analysis and Learning Laboratory (HAL),
Federal University of ABC, Santo Andre, Brazil (e-mail: folivetti@ufabc.edu.br)
2Josef Ressel Center for Symbolic Regression,
Heuristic and Evolutionary Algorithms Laboratory (HEAL),
University of Applied Sciences Upper Austria,
Softwarepark 11, 4232 Hagenberg, Austria

Symbolic regression is a nonlinear regression method which
is commonly performed by an evolutionary computation method
such as genetic programming. Quantiﬁcation of uncertainty of
regression models is important for the interpretation of models
and for decision making. The linear approximation and so-
called likelihood proﬁles are well-known possibilities for the
calculation of conﬁdence and prediction intervals for nonlinear
regression models. These simple and effective techniques have
been completely ignored so far in the genetic programming
literature. In this work we describe the calculation of likelihood
proﬁles in details and also provide some illustrative examples
with models created with three different symbolic regression
algorithms on two different datasets. The examples highlight the
importance of the likelihood proﬁles to understand the limitations
of symbolic regression models and to help the user taking an
informed post-prediction decision.

Index Terms—Nonlinear Regression, Symbolic Regression, Un-
certainty Quantiﬁcation, Prediction Interval, Conﬁdence Interval

I. INTRODUCTION

W HENEVER we build a regression model from data,

we are subject to different sources of uncertainties
such as measurement noise, choice of explanatory variables, or
sampling bias. The inferences made by regression models are
subject to these uncertainties, having a quantiﬁcation of such
uncertainties can help with a decision process. Imagine a re-
gression model that makes inference about the precipitation at
a certain region for a given period of time. Knowing how much
it will rain can help warning people living areas endangered
by landslides. Let us say that the decision threshold to move
people to shelters is a value of 100mm of rain or more. If the
model predicts 90mm of rain, the decision would be to keep
everyone at their homes. But this number accounts only for
the expected values ignoring all the uncertainties involved. By
taking the uncertainties into account, we could have reported
the same expected value of 90mm but with a conﬁdence
interval [75, 105], for example. With this information we can
make a more informed decision so that, if we decide not to
move them out of their homes, we can at least keep personnel
on alert.

The same can be said about

the parameter estimates;
knowing the involved uncertainties can help to interpret the
model. Besides inference, another important task of regression
analysis is association. In this case, we are interested in
understanding the association of a given variable with the
output of the system. Suppose we create a linear regression
model that investigates the association of education level and
height with salary range. If the associated parameters are
1.3, 0.1 for the respective variables, we can say that increasing
education level by 1 will increase the salary range by 1.3 units,
while increasing your height by one unit, increases the salary
range by 0.1 units. If we accept these values as it is, we would
argue that both variables have a positive effect on salary. But,
let us say the conﬁdence interval for these coefﬁcients are
[1.0, 1.6] and [−0.1, 0.2]. In this case, we can be conﬁdent
that the education level does have a positive effect on salary,
but the effect of height may have been simply a random artifact
or sample bias.

As can be seen from these examples, having a conﬁdence
interval for the predictions and the numerical coefﬁcients can
help the practitioner to understand their models and make
better decisions. The calculation of conﬁdence intervals is
well-established for linear regression with Gaussian likelihood
and readily available in most software packages. For nonlinear
models, given we know the regression function and its partial
derivatives, we can also calculate a linear approximation of
the conﬁdence interval [1]. This is frequently called the “delta
method”. Bates and Watts described an even better approach
which use the proﬁle t function [1], also called likelihood
proﬁle, to calculate more accurate conﬁdence intervals, with
the cost of having to apply a nonlinear solver multiple times.
While not as widespread as in linear regression toolboxes,
there are implementations of both the linear approximation
and likelihood proﬁles for generalized linear or nonlinear
regression models1.

Symbolic Regression (SR) is a well-established regression
task [2], where the goal is to ﬁnd the function form and
numerical parameters that best ﬁt a certain dataset. It is most
commonly solved using genetic programming (GP) [3] which
is an evolutionary algorithm. Unlike many opaque nonlin-

Manuscript received XXXX; XXXX. Corresponding author: F. O. de Franca

(email: folivetti@ufabc.edu.br

1The R nls() function is an important example.

 
 
 
 
 
 
IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION

2

ear models (e.g., gradient tree boosting, neural networks), a
symbolic model can bring a balance between accuracy and
interpretability [4, 5, 6]. Most modern SR implementations
allow for a user tunable control of the desired complexity of
the generated model. Also, having a symbolic model, means
that we can study the association or effect of the variable to
the target using partial effects [7, 8]. Most importantly SR
models are nonlinear regression models and therefore we may
use the linear approximation or likelihood proﬁles to quantify
uncertainty of parameter estimates and predictions. Despite
this obvious link, likelihood proﬁles have not been used for SR
models and there is no SR tool available today that can provide
likelihood proﬁles and conﬁdence intervals for the predictions
and parameters.

This paper contains two important contributions. First, we
give clear pseudo-code for the calculation of likelihood pro-
ﬁles, approximate contours for pairwise parameter conﬁdence
regions, and the calculation of prediction intervals for nonlin-
ear models. Second, we demonstrate how the algorithms can
be used in particular for expressions produced by SR tools.
The algorithms are implemented in an open-source software
module written in Python that can be used for uncertainty
quantiﬁcation of SR models as long as they are compatible
with sympy Python library.

The concept of likelihood proﬁles for nonlinear regression
is well-known in the statistics community but the algorithms
published in [1] have some small mistakes which we ﬁxed
in this paper. Additionally, the algorithm for proﬁle-based
prediction intervals is not described at all in [1]. In GP and
SR literature uncertainty quantiﬁcation based on likelihood
proﬁles is completely ignored because the community is seem-
ingly unaware of this technique stemming from the statistics
literature. As a consequence SR implementations usually do
not provide conﬁdence or prediction intervals at all or use var-
ious heuristics such as lower-upper bound estimation (LUBE)
[9]. Another reason is that the calculation of likelihood proﬁles
is only possible after symbolic manipulation of SR expressions
as we demonstrate in this paper. This requires extra work and
can be difﬁcult without the help of computer algebra libraries
such as sympy [10]. The Python module published together
with this paper solves this issue.

The paper is organized as follows: we ﬁrst discuss related
work in Sec. I-A and Sec. II details the calculation of CI in lin-
ear regression, the linear approximation for nonlinear models
and the use of the likelihood proﬁle for nonlinear intervals. In
Sec. III we provide the requirements and algorithmic details of
the CI calculation for symbolic regression models. Following
up, in Sec. IV-A we demonstrate the use of the algorithms.
Finally, in Sec. VI we discuss limitations of the approach and
some additional insights as well as future steps.

A. Related Work

Several

techniques have been described for calculating

prediction or conﬁdence intervals for SR models.

GP using intervals instead of crisp values has been used
together with interval arithmetic to produce prediction in-
the results were not compared to the
tervals in [11], but

delta method or proﬁle-based intervals. Later, the conformal
prediction technique [12] developed in the machine learning
domain was used for GP instead of interval arithmetic in [13].
Ensembles of GP models are used for calculating prediction
uncertainty in [14]. This work is notable because it not only
accounts for the uncertainty of parameters but also for the
uncertainty of model structures identiﬁed by GP. This second
type of uncertainty is ignored by the traditional methods
which focus only on the uncertainty of parameter estimates.
A similar approach based on Bayesian model averaging is
described in [15]. Such ensembling methods can be improved
even more by using the methods described in this paper to
incorporate the prediction uncertainty of the individual models
in the ensemble. Such an approach has been described recently
in [16]. Here the local uncertainty of individual models and
global uncertainty from different structures produced by GP
is combined using a Bayesian approach. For the calculation
of local uncertainty a Laplace approximation is used around
maximum a-posteriori parameter values which is conceptually
related to the delta method for likelihood-based models. Global
uncertainty is determined through Bayesian model averaging.
The lower-upper-bound-estimation (LUBE) method devel-
oped originally for neural networks was used together with GP
for the particular application of wind speed forecasting [17].
LUBE is another heuristic approach developed in the area of
machine learning to approximate prediction intervals. It was
compared to the delta method and Bayesian model selection
for neural networks in [18] and was second only to Bayesian
model selection and produced better results than the delta
method. LUBE-GP is proposed in [9] where a multi-objective
GP approach is used to minimize prediction interval width
and maximize probability coverage. This work is another ex-
ample that completely ignores the delta method and likelihood
proﬁles.

Another Bayesian approach for uncertainty quantiﬁcation
for SR and GP is described in [5], where the main focus is
on using the uncertainty for Bayesian model selection. The
Bayesian approach has the advantage that the full posterior
is taken into account including especially multiple locally
optimal parameterizations. This is in contrast to the methods
proposed in this work which only consider the local neigh-
bourhood around the estimated parameters. A drawback of
Bayesian approaches is the requirement to deﬁne a model prior
and the computational effort required for sampling from the
posterior.

II. CONFIDENCE INTERVALS FOR NONLINEAR
REGRESSION MODELS
In the following we summarize the basics of calculating
conﬁdence intervals for parameters and prediction intervals for
nonlinear regression models using the linear approximation as
well as the likelihood proﬁle method as described in [1].

Given a dataset formed by n samples of p independent
variables X ∈ Rn×p and corresponding dependent (also called
target) measurements y ∈ Rn, a linear regression model
parameterized by numerical coefﬁcients β ∈ Rp is given by:

y = Xβ + (cid:15),

(1)

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION

3

where (cid:15) is called residual and corresponds to the error term
associated with the uncertainty of the collected data. The
linear model assumes that the residuals are i.i.d. normally
distributed with zero mean and that the relationship between
the independent and dependent variables is linear.

The Xβ part of the model is called the expectation function
and it is related to the inference of the expected value of the
target variable (E[y] = Xβ). Speciﬁcally for the linear model,
X is also called the derivative matrix, as the partial derivative
of y w.r.t. β is X.

To determine the values of β we can solve the least square
problem by minimizing the residual sum of the squares (SSR):

SSR(β) = (cid:107)y − Xβ(cid:107)2
2.

The solution to this optimization problem is simply:

ˆβ = (XT X)−1XT y.

(2)

(3)

The residual mean square (s2) or variance estimate is

calculated based on n − p degrees of freedom:

s2 =

SSR( ˆβ)
n − p

.

(4)

The 1 − α conﬁdence interval of a coefﬁcient βi can be
calculated using the upper α/2 percentile for a t-distribution
with n − p degrees of freedom:

where

ˆβi ± se( ˆβi) t(n − p, α/2),

(cid:113)

se i = s

((XT X)−1)ii,

(5)

(6)

where Aii represents the i-th diagonal element of a matrix A.
Similarly we can calculate the 1 − α conﬁdence interval of the
prediction at a new point x as:

(cid:113)

xT ˆβ ± s

xT (XT X)−1x t(n − p, α/2).

(7)

When working with nonlinear models, we are interested in

models:

y = f (X, θ) + (cid:15),

(8)

where θ corresponds to the numerical coefﬁcients of the
model and we assume a probability distribution for (cid:15) (often
a Gaussian distribution). This allows us to estimate θ using
the maximum likelihood method which in this case requires
iterative nonlinear optimization algorithms. These algorithms
update the coefﬁcients in the opposite direction of the gradient.
One example is the Gauss-Newton method that updates an
initial θ0 with the linearization:

θt+1 = θt − (J T J)−1J T (cid:15)t,

(9)

where J is the Jacobian matrix for n observations evaluated
for the current point θt such that

Jkj =

∂(cid:15)k
∂θj

, k = 1 . . . n.

(10)

The iteration is resumed until convergence to a local op-
timum which can be measured by the size of the parameter
increment. The local optimum provides the maximum likeli-
hood estimate for the parameters ˆθ.

A. Linear approximation
Now, given the QR decomposition of J evaluated for ˆθ, we
can calculate the linear approximation for the standard error
se i of the i-th parameter as the length of the i-th row of R−1
(cid:115)(cid:88)

se i = s

(R−1)2
ij.

(11)

j

The residual standard error (rse) can be calculated as:
(cid:115)(cid:88)

rse k = s

(JR−1)2

kj,

(12)

j

where J is evaluated for new points and ˆθ and R−1 is for the
training data X and ˆθ.

The conﬁdence intervals can be calculated the same way as
in Eq. 5. The prediction interval for the expectation function
for any point x can be calculated as:

f (x, ˆθ) ± rse t(n − p, α/2).

(13)

The prediction interval for the full model including the noise
term is:

f (x, ˆθ) ± (rse + s) t(n − p, α/2).

(14)

Notice, though, that these conﬁdence intervals are a linear
approximation of the true intervals and can be “extremly
misleading” [1].

B. Likelihood proﬁle

The likelihood proﬁle [1] gives more accurate conﬁdence
intervals for nonlinear regression models – though without the
guarantee of being exact, as pointed out in [19]. This technique
uses the likelihood proﬁle τ (θi) for the i-th coefﬁcient deﬁned
as:

τ (θi) =

sign(θi − ˆθi)
s

(cid:113)

SSR(θi) − SSR(ˆθ),

(15)

where SSR(θi) is the optimized sum of squared residuals
obtained where the value of θi is held ﬁxed and all other
parameters re-optimized starting from ˆθ. In contrast to Eq. 5,
the CI of θi is the set of all values such that:

− t(n − p, α/2) ≤ τ (θi) ≤ t(n − p, α/2)

(16)

In the following we summarize the algorithms described
in [1]. To create the function τ (θi) we sample a set of points
of τ (ˆθi + δ) for different values of δ. This is done by setting
ˆθi = ˆθi + δ and optimizing the least squares problem while
keeping the i-th coefﬁcient ﬁxed. After that, we calculate the
corresponding value of τ following Eq. 15. To determine the
points to be sampled, we start with a value of δ = se i/step
where step = 8 the initial step size. After calculating a value
of τ we determine the next step based on the slope of τ and

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION

4

step t limiting to a maximum of kmax = 30 points or reaching
a maximum absolute value of (cid:112)f (1 − 0.01, p, n − p) (i.e., the
limit of τ for a 99% conﬁdence interval). We do the same for
decreasing δ values starting from δ = −se i/step and ﬁnally
add the trivial point τ (ˆθi) = 0. In Alg. 1 we describe the
whole process in detail. One caveat with this procedure is that
if, for any reason, the optimization procedure ﬁnds a better
optimum, the current ˆθ should be replaced by the new one and
the process must be restarted from the beginning with the new
parameters values. This can happen if the optimization method
was interrupted before convergence (i.e., reached maximum
iteration) or if the disturbance caused by δ leads to a new
basin of attraction with a better optimum.

Algorithm 2 Algorithm for the calculation of approximated
contour plots for the pairwise parameter conﬁdence regions.
This algorithm assumes the existence of a function called
CubicSpline that returns a cubic splines function based on
the input points. PeriodicCubicSpline returns the spline with
period 2π. F (1 − α, p, n − p) is the critical value for the F-
distribution with p and n − p degrees of freedom.
1: function PREPARESPLINES(i, j, Θ, T, τscale)
gij ← arccos(λθi→τi(Θj)/τscale)
2:
λτj→gij ← CUBICSPLINE(τj, gij)
3:
return λτj→gij
4:
5: end function

Algorithm 1 Likelihood proﬁle algorithm. In this algorithm
˜J is the Jacobian with the i-th column replaced by 0, (cid:15) is the
residue for ˜θ, J i is the i-th column of the Jacobian, and ++
is a list concatenation operator.
1: function PROFILE(i, ˆθ, f, J)
2:
3:

step ] do

step , se i

T ← []
Θ ← []
˜θ ← θ
for all δ ∈ [ −se i
invSlope ← 1
t ← 1
for k = 1 . . . kmax do
˜θi ← θi + δt
˜θ ← nls(˜θ, f, ˜J)
τ ← sign(θi−ˆθi)
T ← T ++ [τ ]
Θ ← Θ ++ [˜θ]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
invSlope ←
(cid:12)
(cid:12)
invSlope ← min(4, max(invSlope, 1/16))
t ← t + invSlope
if |τ | > τmax then

SSR(θi) − SSR(ˆθ)

τ s2
se i(cid:15)T J i

(cid:113)

s

12:
13:
14:
15:
16:
17:

18:
19:
20:
21:
22:

23:
24:
25:
26:
27:
28:

6: function PROFILECONTOUR(i, j, Θ, T, α, steps)
7:
8:
9:
10:
11:

τscale ← (cid:112)p F (1 − α, p, n − p)
λτj→gij ← PREPARESPLINES(i, j, Θ, T, τscale)
λτi→gji ← PREPARESPLINES(j, i, Θ, T, τscale)
angle 0 ← (0, λτi→gij (1))
angle 1 ← (λτi→gji(1), 0)
angle 2 ← (π, λτi→gij (−1))
angle 3 ← (λτi→gji(−1), π)
for k = 0 . . . 3 do

ak ← (angle k0 + angle k1)/2
dk ← angle k0 − angle k1
ak ← sign(dk)ak
dk ← sign(dk)dk

end for
a4 ← a0 + 2π
d4 = d0
λa→d ← PERIODICCUBICSPLINE(a, d)
for k = 1 . . . steps do

x ← 2kπ/(steps − 1) − π
y ← λa→d(x)
τik ← cos(x + y/2)τscale
τjk ← cos(x − y/2)τscale
θik ← λτi→θi(τik)
θjk ← λτj→θj (τjk)

29:
30:
31:
32: end function

end for
return θi, θj

4:
5:
6:
7:
8:
9:

10:

11:

12:
13:

14:

15:
16:

17:
18:
19:
20:
21:

22:

break

end if

end for

end for
T ← T ++ [0]
Θ ← Θ ++ [ˆθ]
return T, Θ

23:
24:
25: end function

With the sampled points for τ (θi) we create the cubic
splines λτi→θi, λθi→τi to interpolate τ → θi and θi → τ and
use the former to ﬁnd the corresponding values of θi for the
boundaries [−t(n − p, α/2), t(n − p, α/2)]. The latter function
is used to create the proﬁle plot of τ (θi) over θi. This plot is
insightful to verify the nonlinearity of the coefﬁcient.

Bates and Watts also sketch an algorithm for the approx-
imation of pairwise parameter conﬁdence regions based on
the likelihood proﬁles [1], described in Alg. 2. The contours
produced by this algorithm are good approximations for the
true conﬁdence regions when the parameter estimates are well-
determined and close to ellipsoid as shown in the example
plots below.

C. Proﬁle-based prediction intervals

Likelihood proﬁles can also be used to calculate nonlinear
prediction intervals. For this purpose we apply the same
algorithm described in Alg. 1 but we have to re-parameterize
the model as described in the following. The idea is to re-
parameterize the model so that one of the parameters is the
output of the model at the evaluation point x0. Let us suppose
the nonlinear model is described as:

ˆy = f (x) = ˆθ0e

ˆθ1x.

(17)

To calculate the prediction interval for ˆy at point x0, we
ﬁrst rearrange the equation to extract one of the coefﬁcients.
Let us pick the ﬁrst coefﬁcient, then we will have:

ˆθ0 = ˆye−ˆθ1x.

(18)

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION

5

We then rename ˆy in the above expression as our new
0 which represents the output of the model in point

parameter θ(cid:48)
x0 and replace in the expectation function:

0e−ˆθ1x0 e
The value for the new parameter θ(cid:48)

f (x)(cid:48) = θ(cid:48)

ˆθ1x.

(19)

0 is calculated via Eq. 17

for the point x.

Now, we apply Alg. 1 with the re-parameterized model
f (x)(cid:48) for the new parameter θ(cid:48)
0 after calculating the standard
errors for the re-parameterized model whereby we use the
training set X for the optimization.

These steps ensure that one of the model parameters equals
the prediction ˆyi at the point x0 which can be easily ver-
iﬁed in Eq. 19. The likelihood proﬁle for this parameter
provides the nonlinear prediction interval in this point. The
re-parameterization and proﬁle calculation has to be repeated
for each point for which we evaluate the prediction interval.

III. LIKELIHOOD PROFILES FOR SYMBOLIC REGRESSION
The algorithms described in the previous section are appli-
cable to any regression model with parameters ﬁtted using the
least squares method. The approach is therefore applicable to
models produced by SR when the numerical parameters are
optimized after the model is generated. This is done in several
state-of-the-art implementations for SR [20, 21, 22].

Another requirement for the applicability is that the pa-
rameters of the model are well-behaved. In particular the
models should not contain linearly dependent parameters.
Models produced by GP systems often do not fulﬁll
this
requirement [23] and have to be simpliﬁed before they can
be used for the likelihood proﬁle calculation.

We assume that the SR implementation produced a model as
an expression which contains the optimized parameters. This
model may for instance be given as a Python expression that
we ﬁrst have to parse into a symbolic form. For the symbolic
representation we have to create:

Replacing the numerical variables to parameters, we have:

f (x) = θ0 +

θ1
θ2x + θ3

.

(21)

In this particular expression, θ1 correlates with θ2, θ3, mak-
ing the problem ill-deﬁned which implies that it is impossible
to calculate a reasonable CI for the parameters. Even the
linear approximation will have overestimated intervals due to
the large standard errors. Unfortunately, GP systems tend to
produce such overparameterized expressions.

To solve this issue we have to apply rewriting rules to
remove linearly dependent parameters. While this can demand
advanced symbolic manipulation algorithms, we can apply
simple rules for frequent patterns. For instance, whenever there
is any part of the expression following the form θi((cid:80)
j,k θjxk),
we simply ﬁx the value of θi so it is no longer an adjustable
parameter. Manual
interaction may be required to remove
all linear dependencies. This task is easily accomplished by
checking the standard errors of parameter estimates and the
parameter correlation matrix.

In our example, we would have the following expression:

f (x) = θ0 +

5.14
θ1x + θ2

.

(22)

Alg. 3 describes this process in details returning the original
values of θ and the symbolic model with all numerical values
replaced with parameters variables.

IV. PROFILET PYTHON LIBRARY
The ProﬁleT Python library provides the ProﬁleT and
SymExpr classes implementing the likelihood proﬁle calcula-
tion for nonlinear regression models described as a sympy-
compatible string. This library is freely available at https:
//github.com/folivetti/proﬁle t with a full documentation and
some usage examples.

• A new symbolic expression replacing all numeric values

A. Demonstration

with parameter variables θ.

• An evaluator function that receives θ and inputs x as
argument and returns the predictions for the data points.
• An evaluator function that receives θ and inputs x as
argument and returns the associated Jacobian matrix.
• A function that rewrites the original expression following
Eq. 19. This should also provide a function that returns
the Jacobian matrix for the re-parameterized model.
The ﬁrst item is straightforward and the second item follows
from the ﬁrst using a recursive tree evaluation function.

Having an expression tree containing only differentiable
functions, it is possible to create the symbolic derivative for
each variable, thus creating evaluators to compose the Jacobian
matrix. For the last item we rely on sympy.

As already mentioned, one possible issue is when two
or more parameters are linearly dependent, leading to ill-
conditioning during the optimization process. Consider the
following symbolic expression:

f (x) = 0.32 +

5.14
3x + 1

.

(20)

To demonstrate the calculation of likelihood proﬁles and
prediction intervals, we use three SR implementations: Heuris-
ticLab (HL), Transformation-Interaction-Rational (TIR), and
Deterministic Symbolic Regression using Grammar Enumer-
ation (DSR), each of which implements a different algorithm
for SR. HeuristicLab uses tree-based GP very similar to
Koza-style GP [24], TIR uses a restricted model structure
and an evolutionary algorithm in combination with ordinary
least squares [22], and DSR uses a tree search algorithm
using a formal grammar that restricts model structures [25].
Coefﬁcients are optimized using the Levenberg-Marquardt
algorithm.

We use the PCB dataset from [1] which contains mea-
surements of the “concentration of polychlorinated biphenyl
residues in a series of lake trout from Cayuga Lake, NY”.
As in [1] we use the logarithm of the concentration as the
target variable. We use this dataset because it can be described
with a very simple model that is easy to understand. For
brevity, we will only show the model generated by HL.
Additionally we used the Kotanchek function (Eq. 24) as in

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION

6

Algorithm 3 Algorithm for symbolic rewriting of SR ex-
pressions. Numbers are replaced by parameters and linear
parameters are removed. The function MulAddPat returns true
if the current node of the tree represents a multiplication of
addition pattern as described on the text.

1: function REWRITE(expr, i, can replace)
2:

if ISNUMBER(expr) then
if can replace then

3:
4:
5:
6:
7:
8:

9:
10:
11:
12:
13:
14:

15:
16:
17:
18:
19:

return [expr], SYMBOL(θi), i + 1

else

return [], expr , i

end if

end if
if ISSYMBOL(expr) then
return [], expr , i

end if
children ← CHILDSOF(expr)
args ← []
values ← []
for child ∈ children do

if ISNUMBER(child ) ∧ MULADDPAT(expr ) then
vals, arg, i ← REWRITE(child, i, F alse)

else

vals, arg, i ← REWRITE(child, i, T rue)

end if
values ← values + vals
args ← args + [arg]

end for
expr ← REPLACECHILDREN(expr, args)
return values, expr, i

20:
21:
22:
23:
24:
25:
26: end function

[26, 27] for which we will compare models produced by
all three SR implementations. We chose these two data sets
since they are both nonlinear and have only one (PCB) or
two variables (Kotanchek), making it easier to visualize the
prediction intervals.

B. Analysis of SR likelihood proﬁles for the PCB dataset

The model for log(PCB ) identiﬁed by HL is

− 3.93 exp(−0.19 age) + 3.13

(23)

with s2 = 0.247 and 3 parameters. This is slightly worse
than the handcrafted model in [1] which has s2 = 0.246
with only two parameters. We used SR with shape-constraints
as described in [28] to enforce that the model is smoothly
monotonically increasing over age similarly to the reference
model. Without these constraints HL produced implausible and
overly complex models. The resulting model cannot be further
simpliﬁed and has no redundant parameters.

Fig. 1 shows the likelihood proﬁles for each parameter. In
this plot the x-axis is the parameter value and the y-axis is
the corresponding value of τ , the value of τ is related to the
degree-of-conﬁdence so, in this particular plot, the values of θ
that corresponds to a −t(n−p, 0.99/2) < τ < t(n−p, 0.99/2)
are the values for a 99% conﬁdence interval. For positive

2
1
0
−1
−2

τ

θ0

θ1

θ2

Fig. 1. Likelihood proﬁles for Eq. 23 using linear approximation (dashed
line) and the likelihood proﬁle (solid line). We can see from this plot that,
for this model, the parameters strongly deviate from linearity.

θ0, θ1

θ0, θ2

θ1, θ2

Fig. 2. Pairwise approximate parameter conﬁdence regions for Eq. 23. The
outer contour shows the 80% region, the inner contour the 50% region.

values of τ , if the proﬁle t plot is above the linear approx-
imation or if it is below the linear approximation when τ is
negative, it means it will have a tighter bound than the linear
approximation to the right or left. We can see that for θ0 the
left and right bounds are smaller than the linear approximation.
In some cases it can also reveal that part of the interval is
unbounded, as we can see in the middle and right plots. In
these plots the construction of any marginal likelihood extends
to −∞ and ∞, as there is no corresponding values of θ for
positive and negative counterparts of τ . In some situations the
intervals are bound only for a certain likelihood interval.

Fig. 2 shows the pairwise likelihood of each pair of pa-
rameters. In this plot we can see how the parameters interact.
When the parameters are close to linear, the pairwise plots will
be ellipsoid, larger correlations leading to elongated ellipses.
Strong nonlinearity leads to “banana-shaped” contours.

In Fig. 3 we can see that both types of prediction intervals
for Eq. 23 contain all of the data points. The linear PI is
inconsistent with the model for low and high age. In these
regions the intervals are not monotonically increasing. Observe
that the proﬁle PI correctly matches the monotonicity enforced
by the model. The proﬁle-based interval is narrower overall
but especially for ages close to zero. Accordingly, the proﬁle-
based PI is much better for the PCB model than the linear
approximation.

C. Comparison of SR models for the Kotanchek function

Next, we use three different SR algorithms to generate
models for the Kotanchek function [26, 27]. The generating
function is given by:

f1(x, y) =

e−(x−1)2
1.2 + (y − 2.5)2

(24)

The training set is generated as described in [26] by random
sampling of 100 observations uniformly from the region

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION

7

)
c
n
o
c
(
g
o
l

6
5
4
3
2
1
0
−1
−2
−3

linear
proﬁle
pred.
data

0

2

4

6

8

10 12 14 16 18

age

Fig. 3. Comparison of linear approximation and likelihood-proﬁle based
prediction intervals (α = 0.05) for the PCB model Eq. 23. Notice that the
color for the proﬁle t region blended with the linear prediction interval.

Estimate Std. err. Correlation matrix
P
3.088e-2
1.932
0
1.982
2.432e-2
1
4.764e-2 3.272e-3
2
3
9.928e-1 9.993e-3
4 -8.554e-3 4.232e-4
5
6

1.00
0.07 1.00
0.88 0.10
0.05 0.97
0.95 0.31

1.00
0.07
0.88

1.818e-4 8.742e-6 -0.90 -0.31 -0.89 -0.27 -0.99 1.00
θ1
2.365e-2 1.096e-3 -0.95 -0.33 -0.82 -0.28 -0.99 0.96 1.00

1.00
0.27

1.00

θ1

θ2

θ3

θ4

θ5

θ6

θ0

θ1

θ2

θ3

θ4

θ5

Fig. 5. Pairwise proﬁle conﬁdence region plots for Eq. 26.

Fig. 4. Maximum likelihood ﬁtting results for Eq. 26.

[0.3, 4]2. The test set contains 45 × 45 points on the regular
grid [−0.2 : 0.1 : 4.2]2. The test set covers a larger space than
the training set which means that extrapolation capability is
necessary.

Table I lists the models found by the three SR algorithms
after simpliﬁcation. The simpliﬁed models are produced by a
partially automatic process using a computer algebra software
with manual intervention to ensure that the expressions have
no redundant parameters.

The original expression produced with HL is:

((θ0y)2 − θ1y + θ2)

exp(θ3θ4xx + θ5x + θ6y + (θ7y)3)θ8 + θ9

(25)

This expression is over-parameterized (θ3, θ4, θ8) and
contains unnecessary nonlinear operations on parameters
(e.g. θ2
0). Algebraic transformation and manual removal of
redundant parameters as well parameters which are effectively
zero leads to the simpliﬁed expression with a new parameter
vector:

exp(θ0y + θ1x + θ2y3 + θ3x2)(θ4y + θ5y3 + θ6)

(26)

The maximum likelihood estimate of the parameters with
standard errors as well as the parameter correlation matrix are
shown in Fig. 4.

Fig. 5 shows contour plots for the pairwise conﬁdence
regions generated with Alg. 1 and Alg. 2. The outer contour
represents the 80% conﬁdence region, the inner contour the

θ2

θ3

θ4

θ5

θ6

θ0

θ1

θ2

θ3

θ4

θ5

Fig. 6. Pairwise proﬁle conﬁdence region plots for the DSR model.

50% conﬁdence region. The plots show that several parameters
have high correlation. Many contours are close to ellipsoids
which indicates that the parameter proﬁles are close to linear
around the maximum likelihood estimate. This implies that the
linear approximation is close to the proﬁle-based PI for this
model.

Fig. 6 shows the same plot for the DSR expression. This plot
reveals a nonlinear relationship between some of the pairs of
parameters. This model not only contains different parameters
with nonlinear behavior but also interaction between most of
the parameters.

In Fig. 7 we can see the same plots for the model created
by TIR. This model is an exponential of a rational polynomial.
Surprisingly,
the uncertainties of this model can be well
approximated by the linear approach despite the nonlinearity
of the exponential function. We can observe thinner ellipses

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION

8

TABLE I
SR MODELS GENERATED BY EACH ALGORITHM FOR THE KOTANCHEK FUNCTION.

Model
exp(θ0y + θ1x − θ2y3 − θ3x2)(θ4y + θ5y3 + θ6)
θ0x + exp(θ1x2) exp(θ2x) exp(θ3y) exp(θ4y2)θ5 + θ6
(cid:16) θ0+θ1y+θ2 cos (x)+θ3 sin (x)+θ4y2
1+θ5x2

exp

(cid:17)

Alg.
HL
DSR

ITEA

s2
5.8e−5
1.55e−4
2.19e−4

θ1

θ2

θ3

θ4

θ5

θ0

θ1

θ2

θ3

θ4

Fig. 7. Pairwise proﬁle conﬁdence region plots for TIR model as depicted in
Table I.

0.25

0.2

0.15

0.1

0.05

0

−0.05

linear
proﬁle
ˆf
f

0 0.5 1 1.5 2 2.5 3 3.5 4

x

Fig. 8. Comparison of linear approximation and likelihood-proﬁle based
prediction intervals (α = 0.05) for Eq. 26.

for terms that uses the same input variable (θ1 and θ4, and
θ2, θ3, θ5) revealing their high correlation.

Fig. 8 shows a small section from the Kotanchek function
for a comparison of the PIs for Eqn. 26. The visualization
shows that both intervals contain the true function. The proﬁle
intervals are narrower which shows the advantage of using
proﬁle intervals over the linear approximation even for this
model where the parameter are close to linear.

V. DISCUSSION
From the above examples, we can see how easily CIs and
PIs can be calculated for SR models. The main requirement
is that the SR model is algebraically simpliﬁed and redundant
parameters are removed. The linear approximation is easy to
calculate and very accurate when model parameters are well-
estimated, i.e. for datasets with low noise or the parameter
effects are close to linear. For models with strong nonlinearity
the likelihood proﬁle can give much better CIs and PIs as
demonstrated with the PCB example.

The likelihood proﬁle can also reveal model

issues in
particular unbounded intervals for parameter estimates. The
pairwise plots can visually aid the practitioner to determine
the relationship between two parameters.

Whenever these plots reveal a problem with the model, we
can try to ﬁx it ﬁnding a new model, rewriting the expression
or reﬁtting the parameters. This can be done manually or
computer-aided with computer algebra systems because SR
uses a symbolic model representation.

Albeit these advantages, there are still some shortcomings
to this approach. One of them is that the optimization of
nonlinear models can have multiple solutions, as it was the
case for the Kotanchek model obtained by TIR. This is not an
easy problem to tackle and it could require an extension using
a Bayesian approach or the use of multi-modal optimization
algorithms, increasing the computational cost.

The computational cost of the likelihood proﬁle calculation
is also another issue, as this requires multiple runs of a
nonlinear optimization. This is most critical for the prediction
intervals as it usually involves many data points and, each data
point may require a number of optimization steps.

VI. CONCLUSION
In this paper we have proposed the use of likelihood
proﬁles for estimating the uncertainties of the parameters and
predictions of nonlinear symbolic regression models. Unlike
the commonly used linear approximation, the likelihood proﬁle
returns more accurate intervals especially for models with
strong nonlinearity.

We gave clear pseudo-code for the algorithms originally
described by Bates and Watts [1], whereby we ﬁxed bugs in the
original source and additionally describe how the likelihood
proﬁles can be used for prediction intervals, which is only
hinted at brieﬂy in the original source.

Together with this proposal, we also provide a freely avail-
able Python library implementing the algorithms that can be
used together with most SR implementations as long as they
generates a valid sympy expression.

We demonstrated the use of likelihood proﬁles with different
data sets and algorithms explaining how to read the generated

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION

9

plots and some possible issues when applying this technique
on ill-conditioned models produced by many GP systems.

In conclusion, the likelihood proﬁle method is a valuable
statistical technique for analysis and validation of SR models
that can help the practitioners to extract additional information
about the model, inspect the validity, ﬁx ill-conditioning, and
understand the limitations of the model.

For the next steps we will investigate the multi-modality
problem, when the nonlinear model can have multiple equally
good parameters, such as sin (θx). The likelihood proﬁle is
only a local approximation around the maximum likelihood
estimate of the parameters and does not consider multiple
almost equally likely local optima. Another direction for
future research is the possibility of Bayesian model averaging
whereby the local uncertainty of SR models is incorporated.

ACKNOWLEDGMENT

This research is funded by Fundac¸ ˜ao de Amparo `a Pesquisa
do Estado de S˜ao Paulo (FAPESP), grant number 2021/12706-
1, Josef Ressel Center for Symbolic Regression, Christian
Doppler Research Association.

REFERENCES

[1] D. Bates and D. Watts, Nonlinear regression analysis
and its applications, ser. Wiley series in probability and
mathematical statistics. Wiley, 1988.

[2] W. La Cava, P. Orzechowski, B. Burlacu, F. O. de Franca,
M. Virgolin, Y. Jin, M. Kommenda, and J. H. Moore,
“Contemporary symbolic regression methods and their
relative performance,” in Thirty-ﬁfth Conference on Neu-
ral Information Processing Systems Datasets and Bench-
marks Track (Round 1), 2021.

[3] J. R. Koza, Genetic Programming: On the Programming
of Computers by Means of Natural Selection. Cam-
bridge, MA, USA: MIT Press, 1992. [Online]. Available:
http://mitpress.mit.edu/books/genetic-programming
[4] M. Kotanchek, G. Smits, and E. Vladislavleva, “Trustable
symbolic regression models: using ensembles, interval
arithmetic and pareto fronts to develop robust and trust-
aware models,” in Genetic Programming Theory and
Practice V, ser. Genetic and Evolutionary Computation.
Ann Arbor: Springer, 17-19May 2007, ch. 12, pp. 201–
220.

[5] G. F. Bomarito, P. E. Leser, N. Strauss, K. Gar-
brecht, and J. Hochhalter, “Automated learning of in-
terpretable models with quantiﬁed uncertainty,” arXiv
preprint arXiv:2205.01626, 2022.

[6] T. Hu, “Genetic programming for interpretable and ex-
plainable machine learning,” in Genetic Programming
Theory and Practice XIX, ser. Genetic and Evolutionary
Computation. Ann Arbor, USA: Springer, Jun. 2-4 2022.
I. Aldeia and F. O. de Franc¸a, “Measuring
feature importance of symbolic regression models using
partial effects,” in Proceedings of
the Genetic and
Evolutionary Computation Conference, ser. GECCO
’21. New York, NY, USA: Association for Computing

[7] G. S.

Machinery, 2021, p. 750–758.
https://doi.org/10.1145/3449639.3459302

[Online]. Available:

[8] G. S. I. Aldeia and F. O. de Franc¸a, “Interpretability in
symbolic regression: a benchmark of explanatory meth-
ods using the feynman data set,” Genetic Programming
and Evolvable Machines, pp. 1–41, 2022.

[9] K. B. Rebuli, M. Giacobini, N. Tallone,

and
L. Vanneschi, “A preliminary study of prediction
programming,”
interval methods with
in
Evolutionary
Proceedings
and
Computation Conference Companion,
ser. GECCO
’22. New York, NY, USA: Association for Computing
Machinery, 2022, p. 530–533.
[Online]. Available:
https://doi.org/10.1145/3520304.3528806

genetic
the Genetic

of

[10] A. Meurer, C. P. Smith, M. Paprocki, O. ˇCert´ık, S. B.
Kirpichev, M. Rocklin, A. Kumar, S. Ivanov, J. K.
Moore, S. Singh, T. Rathnayake, S. Vig, B. E. Granger,
R. P. Muller, F. Bonazzi, H. Gupta, S. Vats, F. Johansson,
F. Pedregosa, M. J. Curry, A. R. Terrel, v. Rouˇcka,
A. Saboo, I. Fernando, S. Kulal, R. Cimrman, and
A. Scopatz, “Sympy: symbolic computing in python,”
PeerJ Computer Science, vol. 3, p. e103, Jan. 2017.
[Online]. Available: https://doi.org/10.7717/peerj-cs.103
[11] L. Sanchez, “Interval-valued ga-p algorithms,” IEEE
Transactions on Evolutionary Computation, vol. 4,
no. 1, pp. 64–72, Apr. 2000.
[Online]. Available:
http://ieeexplore.ieee.org/iel5/4235/18295/00843495.pdf
[12] V. N. Balasubramanian, S.-S. Ho, and V. Vovk,
Eds., Conformal Prediction for Reliable Machine
Learning. Boston: Morgan Kaufmann, 2014. [Online].
Available: https://www.sciencedirect.com/science/article/
pii/B9780123985378000195

[13] P. T. Thuong, N. X. Hoai, and X. Yao, “Combining
conformal prediction and genetic programming for
symbolic interval regression,” in Proceedings of
the
Genetic and Evolutionary Computation Conference,
Berlin, Germany: ACM, 15-
ser. GECCO ’17.
19 Jul. 2017, pp. 1001–1008.
[Online]. Available:
http://doi.acm.org/10.1145/3071178.3071280

[14] K. Parasuraman and A. Elshorbagy, “Toward improving
the reliability of hydrologic prediction: Model structure
uncertainty and its quantiﬁcation using ensemble-based
genetic programming framework,,” Water Resources
Research,
2008.
[Online]. Available: http://www.agu.org/pubs/crossref/
2008/2007WR006451.shtml

p. W12406,

5 Dec.

vol.

44,

[15] A. Agapitos, M. O’Neill, and A. Brabazon, “Ensemble
Bayesian model averaging in genetic programming,” in
Proceedings of the 2014 IEEE Congress on Evolutionary
Computation, C. A. Coello Coello, Ed., Beijing, China,
6-11 Jul. 2014, pp. 2451–2458.

[16] M. Werner, A. Junginger, P. Hennig, and G. Martius,
“Uncertainty in equation learning,” in Proceedings of
the Genetic and Evolutionary Computation Conference
Companion,
New York, NY,
USA: Association for Computing Machinery, 2022, p.
2298–2305. [Online]. Available: https://doi.org/10.1145/
3520304.3533964

ser. GECCO ’22.

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION

10

ch. 17, pp. 283–299.

[28] G. Kronberger, F. O. de Franc¸a, B. Burlacu, C. Haider,
and M. Kommenda, “Shape-constrained symbolic regres-
sion—improving extrapolation with prior knowledge,”
Evolutionary Computation, vol. 30, no. 1, pp. 75–98,
2022.

[17] K. Li, T. Zhang, and R. Wang, “An evolutionary
multi-objective knee-based lower upper bound estimation
method for wind speed interval forecast,” IEEE Transac-
tions on Evolutionary Computation, pp. 1–1, 2021.
[18] A. Khosravi, S. Nahavandi, D. Creighton, and A. F.
Atiya, “Lower upper bound estimation method for con-
struction of neural network-based prediction intervals,”
IEEE Transactions on Neural Networks, vol. 22, no. 3,
pp. 337–346, 2011.

[19] S. L. Quinn, T. J. Harris, and D. W. Bacon, “Notes on
likelihood intervals and proﬁling,” Communications in
Statistics-Theory and Methods, vol. 29, no. 1, pp. 109–
129, 2000.

[20] B. Burlacu, G. Kronberger, and M. Kommenda, “Operon
C++: An efﬁcient genetic programming framework for
symbolic regression,” in Proceedings of the 2020 Genetic
and Evolutionary Computation Conference Companion,
ser. GECCO ’20.
internet: Association for Computing
Machinery, Jul. 8-12 2020, pp. 1562–1570. [Online].
Available: https://doi.org/10.1145/3377929.3398099
[21] M. Kommenda, B. Burlacu, G. Kronberger, and M. Af-
fenzeller, “Parameter identiﬁcation for symbolic regres-
sion using nonlinear least squares,” Genetic Program-
ming and Evolvable Machines, vol. 21, no. 3, pp. 471–
501, 2020.

[22] F. O. de Franc¸a, “Transformation-interaction-rational rep-
resentation for symbolic regression,” in Proceedings of
the Genetic and Evolutionary Computation Conference,
ser. GECCO ’22. New York, NY, USA: Association
for Computing Machinery, 2022, p. 920–928. [Online].
Available: https://doi.org/10.1145/3512290.3528695
“Local optimization often is

ill-
conditioned in genetic programming for
symbolic
regression,” 2022. [Online]. Available: https://arxiv.org/
abs/2209.00942

[23] G. Kronberger,

the

“On

architecture

[24] M. Kommenda, G. Kronberger, S. Wagner, S. Winkler,
and
and M. Affenzeller,
tree-based genetic programming
implementation of
14th
in Proceedings
in HeuristicLab,”
and
Annual Conference Companion
Evolutionary Computation, ser. GECCO ’12. New
for Computing
York, NY, USA: Association
Machinery, 2012, p. 101–108.
[Online]. Available:
https://doi.org/10.1145/2330784.2330801

the
on Genetic

of

[25] L. Kammerer, G. Kronberger, B. Burlacu, S. M. Win-
kler, M. Kommenda, and M. Affenzeller, “Symbolic
regression by exhaustive search: reducing the search
space using syntactical constraints and efﬁcient semantic
structure deduplication,” in Genetic Programming Theory
and Practice XVII. Springer, 2020, pp. 79–99.

[26] E. J. Vladislavleva, G. F. Smits, and D. den Hertog,
“Order of nonlinearity as a complexity measure for
models generated by symbolic regression via pareto ge-
netic programming,” IEEE Transactions on Evolutionary
Computation, vol. 13, no. 2, pp. 333–349, Apr. 2009.

[27] G. Smits and M. Kotanchek, “Pareto-front exploitation
in symbolic regression,” in Genetic Programming Theory
and Practice II. Ann Arbor: Springer, 13-15 May 2004,

