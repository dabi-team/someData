k-Variance: A Clustered Notion of Variance∗

Justin Solomon† , Kristjan Greenewald‡ , and Haikady N. Nagaraja§

Abstract. We introduce k-variance, a generalization of variance built on the machinery of random bipartite
matchings. K-variance measures the expected cost of matching two sets of k samples from a
distribution to each other, capturing local rather than global information about a measure as k
increases; it is easily approximated stochastically using sampling and linear programming. In addition
to deﬁning k-variance and proving its basic properties, we provide in-depth analysis of this quantity in
several key cases, including one-dimensional measures, clustered measures, and measures concentrated
on low-dimensional subsets of Rn. We conclude with experiments and open problems motivated by
this new way to summarize distributional shape.

Key words. Variance, optimal transport, Wasserstein, clustering

AMS subject classiﬁcations. 49Q25, 62G30, 62H30

1. Introduction. A key task in statistics and data science is to describe the shape of a
dataset or distribution in a simple form. The most basic means of summarizing distributions
extract scalar measurements characterizing spread, normality, support, decay, and other aspects
of distributional geometry. Among these measurements, the simplest and most popular choice
is variance, which measures squared deviation of a random variable from its mean.

A scalar is unlikely to capture all relevant or interesting information about a distribution,
and indeed variance is not sensitive to skew, asymmetry, and other structural properties. A
typical way to address this issue is to compute higher-order moments, which—if completely
known—can often reconstruct a distribution. While this solution works mathematically, each
(standarized) moment measures the allotment of mass in a distribution relative to its mean,
which is hard to interpret in the multi-modal or clustered cases.

In this paper, we introduce a generalization of variance we call k-variance, intended to
address some of the issues above. The basic idea of k-variance is to draw 2k samples from a
distribution and to evaluate the transport cost of matching the ﬁrst k samples to the second
k samples. K-variance coincides with variance in the k = 1 case. But, for larger values of k,
samples get matched to closer counterparts in the distribution rather than between diﬀerent
modes, making k-variance a more localized measure of variance.

Our construction of k-variance seems to indicate that a tightly-clustered distribution about

∗Originally posted in December 2020.
Funding: J. Solomon acknowledges the generous support of Army Research Oﬃce grants W911NF1710068 and
W911NF2010168, of Air Force Oﬃce of Scientiﬁc Research award FA9550-19-1-031, of National Science Foundation
grant IIS-1838071, from the CSAIL Systems that Learn program, from the MIT–IBM Watson AI Laboratory, from the
Toyota–CSAIL Joint Research Center, from a gift from Adobe Systems, and from the Skoltech–MIT Next Generation
Program.

†Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge,

MA (jsolomon@mit.edu, http://people.csail.mit.edu/jsolomon/).

‡MIT–IBM Watson AI Lab, Cambridge, MA (Kristjan.H.Greenewald@ibm.com, https://kgreenewald.github.io/)
§Division of Biostatistics, The Ohio State University, Columbus, OH (nagaraja.1@osu.edu, https://cph.osu.edu/

people/hnagaraja)

1

0
2
0
2

c
e
D
3
1

]
T
S
.
h
t
a
m

[

1
v
8
5
9
6
0
.
2
1
0
2
:
v
i
X
r
a

 
 
 
 
 
 
2

SOLOMON, GREENEWALD, AND NAGARAJA

a few means might have high (1-)variance if those means are far apart, but that k-variance of
such a distribution will decay rapidly in k relative to that of a unimodal Gaussian. Indeed, we
will prove that this is the case—but only for measures embedded in dimensions (cid:39) 5. In lower
dimensions, k-variance exhibits surprising—and somewhat counterintuitive—behavior, which
we can capture in detail for one-dimensional k-variance using the theory of order statistics.

K-variance can be approximated using a simple randomized algorithm, wherein we draw
2k points and solve a k × k transportation problem; unsurprisingly, the accuracy of this
easy-to-implement estimator can be improved by averaging over multiple trials. We provide
variance bounds demonstrating that k-variance requires fewer such trials as k and/or the
ambient dimension increases.

We conclude with some experiments demonstrating the behavior of k-variance as a measure

of intra-mode variability, as well as a number of open problems motivated by our work.

Contributions. We introduce a generalization of variance for probability measures on Rn
we call “k-variance,” built on constructions from optimal transport. Beyond introducing
k-variance and its basic properties (section 4), we
• give alternative expressions and bounds for k-variance of probability measures over R

(section 5);

• use results in empirical optimal transport to characterize k-variance of probability measures
concentrated on low-dimensional sets (section 6), higher-dimensional sets (section 7), and
with cluster structure (section 8);

• bound the variance of empirical estimators for k-variance in terms of sample size and

dimension (section 9); and

• provide numerical experiments to demonstrate behavior of k-variance and conﬁrm our

predicted theory (section 10).

2. Related work. For the most part, we incorporate discussion of related work into the
text below as it arises; our work principally uses results from the theory of optimal transport
(cf. [16, 19, 21]) and—in one dimension—from the theory of order statistics (cf. [8]).

Before commencing our technical discussion, however, we note that our work is built on
recent advances in the theory of random Euclidean bipartite matchings. This theory seeks to
characterize the cost of matching two independently-drawn k-samples of a measure to one
another, where the cost of matching two points is proportional to the p-th power of Euclidean
distance. See [3, 9, 10, 11, 13] and references therein for relevant mathematical theory, and see
[6, 23] for applications in other disciplines. While these works focus on bounding the transport
cost in speciﬁc cases or connecting it to physical applications, here we show how the matching
cost can be understood as a generalization of variance useful for characterizing the shape of a
probability measure.

3. Preliminaries. We begin with mathematical preliminaries to establish notation.

3.1. Variance. Our work focuses on generalizing the variance of a random variable X
drawn from a probability measure µ ∈ Prob(Rd), which is the expected squared deviation of
that variable from its mean X := E[X]:

(3.1)

Var(X) := EX∼µ[(cid:107)X − X(cid:107)2
2].

K-VARIANCE: A CLUSTERED NOTION OF VARIANCE

3

A simple argument reveals an alternative formula for variance:

(3.2)

Var(X) =

1
2

EX,Y ∼µ[(cid:107)X − Y (cid:107)2
2].

3.2. Optimal transport. Take µ, ν ∈ Prob(Rd) to be two Radon probability measures.

Then, we can deﬁne the (squared) 2-Wasserstein distance between µ and ν via

(3.3)

W 2

2 (µ, ν) := inf

π∈Π(µ,ν)

E(X,Y )∼π[(cid:107)X − Y (cid:107)2
2],

where Π(µ, ν) ⊆ Prob(Rd × Rd) denotes the set of measure couplings whose marginals are µ
and ν, resp. The Wasserstein distance is a basic object of study in analysis, statistics, machine
learning, and related disciplines. Intuitively, W2(µ, ν) measures the amount of work it takes to
displace µ onto ν as distributions of mass over Rd, where the cost of moving a particle of mass
from x ∈ Rd to y ∈ Rd is (cid:107)x − y(cid:107)2
2; see [16] for a comprehensive introduction, applications, and
related discussion.

Of particular importance to our development is the Wasserstein distance between empirical
j=1 δyj
j=1 ⊂ Rd. In this case, the transport problem (3.3) becomes a linear

measures of the same size, which can be written as µk = 1
k
for some {xi}k
assignment problem with cost Cij := (cid:107)xi − yj(cid:107)2
2:

i=1 δxi and νk = 1
k

i=1, {yj}k

(cid:80)k

(cid:80)k

(3.4)

W 2

2 (µk, νk) =






minT ∈Rk×k

(cid:104)T, C(cid:105)
s.t. T 1 = 1/k

T (cid:62)1 = 1/k
T ≥ 0,

where 1 denotes the vector of all ones. The constraints of (3.4) form a scaled version of the
Birkhoﬀ polytope (set of doubly-stochastic matrices), whose vertices deﬁne bijections between
the xi’s and the yj’s.

There is a probabilistic link between (3.3) and (3.4). For general µ, ν ∈ Prob(Rd), we
can deﬁne an empirical (plug-in) estimator of W 2
2 (µ, ν) by drawing x1, . . . , xk ∼ µ and
y1, . . . , yk ∼ ν and approximating W 2
2 (µ, ν) ≈ W 2
2 (µk, νk) as in (3.4). As derived in [7,
Theorem 2], under straightforward assumptions this approximation converges with rate k−2/d
for large k when d > 4.

4. k-variance. We can introduce optimal transport into the variance formula (3.2) using
2 = W 2
2 (δx, δy). That is, an equivalent formula to
2 (δX , δY )]. This observation immediately suggests

the k = 1 case of (3.4), by writing (cid:107)x − y(cid:107)2
(3.2) is the following: Var(X) = EX,Y ∼µ[W 2
a generalization of variance using optimal transport:

Deﬁnition 4.1 (k-variance). Given a probability measure µ ∈ Prob(Rd) and a parameter

k ∈ N, deﬁne k-variance as

(4.1)

Vark(µ) :=

1
2

· ρ(k, d) · EX1,...,Xk∼µ
Y1,...,Yk∼µ

(cid:32)

(cid:34)
W 2
2

1
k

k
(cid:88)

i=1

δXi,

(cid:33)(cid:35)

δYi

,

1
k

k
(cid:88)

i=1

4

SOLOMON, GREENEWALD, AND NAGARAJA

where ρ(k, d) is the ambient scaling rate chosen to account for the rate at which the expectation
approaches zero:

(4.2)

ρ(k, d) :=






k
k/log k
k2/d

if d = 1
if d = 2
if d > 2.

We deﬁne

(4.3)

Var∞(µ) := lim
k→∞

Vark(µ),

when such a limit exists. For X ∼ µ, we will identify Vark(X) := Vark(µ).
See section 7 for formulas motivating our choice of ρ(k, d).

Several simple properties of Vark(·) in analogy to variance follow from deﬁnitions and

simple properties of W2:

Proposition 4.2 (Basic properties of Vark(·)). We have the following properties for Vark:
(a) Var1(µ) = Var(µ), for µ ∈ Prob(Rd)
(b) Vark(δa) = 0, for a ∈ Rd
(c) Vark(X + a) = Vark(X), for X ∼ µ ∈ Prob(Rd), a ∈ Rd
(d) Vark(c · X) = c2 · Vark(X), for X ∼ µ ∈ Prob(Rd), c ∈ R
(e) Vark(X + ˜X) ≥ Vark(X) + Vark( ˜X), for independent X ∼ µ ∈ Prob(Rd), ˜X ∼ ν ∈

Prob(Rd)

Proof. Property (a) is argued above. Properties (b), (c), and (d) follow from simple
properties of the cost matrix in (3.4) after substituting (4.1). To prove (e), we resort to the
form (3.4). In this case, we can write

Vark(X + ˜X) :=

1
2

· ρ(k, d) · E

X1,...,Xk∼µ; ˜X1,..., ˜Xk∼ν
Y1,...,Yk∼µ; ˜Y1,..., ˜Yk∼µ

(cid:34)

(cid:32)

W 2
2

1
k

k
(cid:88)

i=1

δXi+ ˜Xi

,

(cid:33)(cid:35)

δYi+ ˜Yi

.

1
k

k
(cid:88)

i=1

The cost matrix of the linear program (3.4) in this expectation has entries

Cij = (cid:107)Xi + ˜Xi − Yi − ˜Yi(cid:107)2

2 = (cid:107)Xi − Yi(cid:107)2

2 + (cid:107) ˜Xi − ˜Yi(cid:107)2

2 + 2(Xi − Yi) · ( ˜Xi − ˜Yi).

Splitting the minimization in (3.4) into three minimizations corresponding to the terms in our
expression for Cij above shows:

(cid:34)
Vark(X + ˜X) ≥ Vark(X) + Vark( ˜X) + 2ρ(k, d)E

min
T ∈Bk

(cid:88)

[(Xi − Yi) · ( ˜Xi − ˜Yi)]Tij

ij

where Bk indicates the constraint set in (3.4). By Jensen’s inequality,

(cid:34)
Vark(X + ˜X) ≥ Vark(X) + Vark( ˜X) + 2ρ(k, d)

min
T ∈Bk

(cid:88)

ij

E[(Xi − Yi) · ( ˜Xi − ˜Yi)]Tij

(cid:35)
,

(cid:35)

= Vark(X) + Vark( ˜X) by independence, yielding (e).

K-VARIANCE: A CLUSTERED NOTION OF VARIANCE

5

In the following sections, we seek to provide intuition for Vark(·) in various settings.
We organize our discussion around dimensionality, starting with one-dimensional measures,
proceeding to measures with low-dimensional structures, and then considering the high-
dimensional case. We conclude our theoretical discussion with another structured class of
measures, those containing clusters of high probability.

5. One-dimensional k-variance. The k-variance Vark admits a particularly clean formu-
lation for probability measures over the real numbers R. Here, we derive this alternative
interpretation of Vark, show how it can be used to derive bounds and estimates describing the
behavior of one-dimensional k-variance, and give a limiting formula as k → ∞.

5.1. Alternative formula. In one dimension, the 2-Wasserstein distance W2 between
empirical measures consisting of the same number of points is given by the L2 distance between
the vectors of data points [21]. That is,

(5.1)

W2

(cid:32)

1
k

k
(cid:88)

i=1

δxi,

1
k

k
(cid:88)

i=1

(cid:33)

δyi

=

(cid:118)
(cid:117)
(cid:117)
(cid:116)

k
(cid:88)

i=1

(xi − yi)2,

when x1 ≤ x2 ≤ · · · ≤ xk and y1 ≤ y2 ≤ · · · ≤ yk.

To incorporate this formula into (4.1), take X(i) to be the i-th order statistic of X1, . . . , Xk ∼
i=1 and taking the i-th element of
i=1. Then, for d = 1 we

µ ∈ Prob(R), a random variable obtained by sorting {Xi}k
the sorted list; similarly deﬁne order statics Y(i) for the samples {Yi}k
can write

(5.2)

Vark(µ) =

1
2

· EX1,...,Xk∼µ
Y1,...,Yk∼µ

(cid:34) k

(cid:88)

i=1

(cid:35)

(X(i) − Y(i))2

=

k
(cid:88)

i=1

Var(X(i)),

by linearity of expectation and by applying (5.1) and (3.2). Hence, in one dimension, the
k-variance is exactly the sum of the variances of the order statistics.

Example 5.1 (Uniform distribution). Suppose µ is the uniform distribution on the unit

interval. Then, X(i) ∼ Beta(i, k + 1 − i). Hence,

(5.3)

(5.4)

(5.5)

E(X(i)) =

i
k + 1

Var(X(i)) =

E((X(i) − pi)4) =

=

i(k + 1 − i)
pi(1 − pi)
(k + 1)2(k + 2)
k + 2
3i(k − i + 1)[2(k + 1)2 + i(k − i + 1)(k + 5)]
(k + 1)4(k + 2)(k + 3)(k + 4)

=

3pi(1 − pi)[2 + pi(1 − pi)(k + 5)]
(k + 2)(k + 3)(k + 4)

,

where pi = i/k+1; we include some of the expressions above to assist in our proof of Proposi-
tion 5.5. Substituting (5.4) into our expression for one-dimensional k-variance,

(5.6)

Vark(Unif([0, 1])) =

k
(cid:88)

i=1

Var(X(i)) =

1
(k + 1)2(k + 2)

k
(cid:88)

i=1

i(k + 1 − i) =

k
6(k + 1)

.

6

SOLOMON, GREENEWALD, AND NAGARAJA

This sequence is increasing, and taking a limit as k → ∞ shows Var∞(Unif([0, 1])) = 1/6.

Example 5.2 (Exponential distribution).

Suppose µ is an exponential distribution with
parameter λ. Then, we can sample from the order statistics of µ by drawing iid exponential
variables Zj with rate 1 and computing the following [18]:

X(i) =

1
λ

i
(cid:88)

j=1

Zj
k − j + 1

.

Substituting the variance of an exponential random variable,

Var(X(i)) =

i
(cid:88)

(cid:18)

j=1

1
λ(k − j + 1)

(cid:19)2

.

This gives the following expression for k-variance:

Vark(Exp(λ)) =

1
λ2

k
(cid:88)

i
(cid:88)

i=1

j=1

1
(k − j + 1)2 =

Hk
λ2 ≈ log(k) + γ,

where Hk is the k-th harmonic number and γ is the Euler’s constant. Taking k → ∞ shows
Var∞(Exp(λ)) = ∞.

5.2. Properties of k-variance in 1D. We can immediately derive alternative expres-

sions/bounds for Vark(·) in one dimension by applying properties of order statistics:

Proposition 5.3 (Bounding Vark(·) in 1D). When d = 1, we can write

(5.7)

Vark(µ) = kσ2 −

Moreover, we can bound

(5.8)

Vark(µ) ≥ kσ2 − 2

k
(cid:88)

i=1

(cid:88)

i<j

(cid:0)X (i) − X(cid:1)2

≤ kσ2.

σ(i)σ(j) ·

i(k + 1 − j)
j(k + 1 − i)

,

with equality for uniform distributions. In these expressions, X ∼ µ, σ2 = Var(X), and
σ2
(i) = Var(X(i)) for X1, . . . , Xk ∼ µ.

Proof. We can obtain (5.7) by rearranging a sum:

kσ2 =

k
(cid:88)

i=1

E[(Xi − X)2] =

k
(cid:88)

i=1

E[(X(i) − X)2] =

k
(cid:88)

i=1

E[(X(i) − X (i) + X (i) − X)2]

(5.9)

Removing the ﬁnal term provides inequality (5.7).

= Vark(µ) +

k
(cid:88)

(X (i) − X)2.

i=1

K-VARIANCE: A CLUSTERED NOTION OF VARIANCE

7

To derive (5.8), we rely on a bound on the correlation of order statistics stated in [8, p. 74]

and references therein. In particular, for i < j they show:

(5.10)

Corr(X(i), X(j)) ≤

i(k + 1 − j)
j(k + 1 − i)

,

where Corr(·, ·) denotes the correlation of random variables, with equality when the parent
distribution is uniform. We know (cid:80)
i Xi given the Xi’s are iid variables with
variance σ2; computing the variance of both sides shows

i X(i) = (cid:80)

kσ2 = Vark(µ) + 2

(cid:88)

i<j

Cov(X(i), X(j)).

Substituting (5.10), by deﬁnition of correlation we have

Vark(µ) = kσ2 − 2

(cid:88)

i<j

Cov(X(i), X(j)) ≥ kσ2 − 2

σ(i)σ(j) ·

i(k + 1 − j)
j(k + 1 − i)

,

(cid:88)

i<j

as needed.

Remark 5.4 (Approximating Vark). The expression (5.7) suggests the following means of

approximating Vark(µ) for large k:

(5.11)

Vark(µ) ≈ kσ2 −

k
(cid:88)

i=1

(F −1(pi) − X)2,

where pi = i/k+1 and F −1 is the quantile function associated to µ. Intuitively, this expression
indicates that our index of total local variability is approximately a global variability index
minus an index of between-local-group variability.

Another standard approach to working with order statistics involves Taylor series expansions
about quantiles of the sampled probability measure. Following this strategy yields a useful
approximation to Vark(·) as well as a limiting formula under certain assumptions about the
distribution function:

Proposition 5.5. Using the notation of Proposition 5.3, suppose that σ2 is ﬁnite and that µ
has a diﬀerentiable distribution function f (x) with CDF F (x). Moreover, suppose (i) f (x) > 0
and (ii) f (cid:48)(x)/[f (x)]3 is bounded on F −1((0, 1)). Then, as k → ∞ we have

(5.12)

Vark(µ) ≈

1
k + 2

k
(cid:88)

i=1

pi(1 − pi)
[f (F −1(pi))]2 ,

where pi = i/k+1. As k → ∞, under the assumptions above we have

(5.13)

Vark(µ) →

(cid:90) 1

0

u(1 − u)

[f (F −1(u))]2 du =

(cid:90)

F −1((0,1))

F (x)(1 − F (x))
f (x)

dx.

The rate of convergence of Vark(µ) to the limiting integral Var∞(µ) is of O(1/

√

k).

8

SOLOMON, GREENEWALD, AND NAGARAJA

Proof. Note that X(i)

d= F −1(U(i)) where U(i) is the i-th order statistic from the standard
uniform parent. We begin with a Taylor expansion for F −1(U(i)) given in [2]. With pi = i/k+1,

(5.14)

F −1(U(i)) = F −1(pi) + (U(i) − pi)(F −1(pi))(cid:48) +

1
2

(U(i) − pi)2(F −1(Vi))(cid:48)(cid:48),

for some random variable Vi ∈ (pi, U(i)). Diﬀerentiating inverse functions shows

(5.15)

(F −1(u))(cid:48) =

1
f (F −1(u))

and

(F −1(u))(cid:48)(cid:48) = −

f (cid:48)(F −1(u))
[f (F −1(u))]3 .

Substituting into (5.14) and taking variance of both sides shows

(5.16)

(i) = Var(U(i) − pi)[f (F −1(pi))]−2 +
σ2

1
4

Var((U(i) − pi)2 · (F −1(Vi))(cid:48)(cid:48))

f (F −1(pi))−1Cov(U(i) − pi, (U(i) − pi)2(F −1(Vi))(cid:48)(cid:48)),

+

1
2

where Vark(µ) = (cid:80)k

i=1 σ2

(i).

Applying the identity Var[Y ] ≤ E[Y 2], the variance factor in the second term of (5.16) is
bounded above by E((U(i) − pi)4[(F −1(Vi))(cid:48)(cid:48)]2), which in turn is bounded by M 2 · E((U(i) − pi)4)
where M is an upper bound for (F −1(u))(cid:48)(cid:48) for u ∈ (0, 1). From (5.5), we obtain

k
(cid:88)

i=1

E((U(i) − pi)4) =

k
(cid:88)

i=1

3pi(1 − pi)[2 + pi(1 − pi)(k + 5)]
(k + 2)(k + 3)(k + 4)

=

≈

6
(k + 3)(k + 4)

k
(cid:88)

i=1

pi(1 − pi)
(k + 2)

+

3(k + 5)
(k + 3)(k + 4)

k
(cid:88)

i=1

i (1 − pi)2
p2
(k + 2)

6
k2

(cid:90) 1

0

u(1 − u) du +

3
k

(cid:90) 1

0

u2(1 − u)2 du =

1
k2 +

1
10k

.

Here, the ratios of the ﬁrst and second terms on the right and left side of the approximation ≈
each approach 1 for large k. Thus, we conclude that when summed over i, the second term in
(5.16) contributes an amount of size O(1/k).

The covariance term in (5.16) can be bounded as follows

Cov(· · · ) ≤ [Var(U(i))Var((U(i) − pi)2F −1(Vi))(cid:48)(cid:48))]1/2

≤ [Var(U(i))]1/2[E((U(i) − pi)4M 2]1/2

= M ·

(cid:20) pi(1 − pi)
k + 2

6pi(1 − pi)
(k + 2)(k + 3)(k + 4)

+

(cid:20)

·

(cid:20)

(cid:21)(cid:21)1/2

i (1 − pi)2
3(k + 5)p2
(k + 2)(k + 3)(k + 4)
(cid:21)1/2

·

6
(k + 3)(k + 4)

+

3(k + 5)pi(1 − pi)
(k + 3)(k + 4)

= M ·

< M ·

pi(1 − pi)
k + 2
pi(1 − pi)
k + 2

·

√

C
k + 3

K-VARIANCE: A CLUSTERED NOTION OF VARIANCE

9

for some constant C (C = 3 suﬃces). Summing over i,

k
(cid:88)

i=1

1
2

f (F −1(pi))−1Cov(U(i) − pi, (U(i) − pi)2(F −1(Vi))(cid:48)(cid:48))

≤

M C
√
k + 3
2

1
k + 2

k
(cid:88)

i=1

pi(1 − pi)
f (F −1(pi))

≈

M C
√
k
2

(cid:90) 1

0

u(1 − u)
f (F −1(u))

du =

M C
√
k
2

(cid:90)

F (x)(1 − F (x)) dx,

F −1((0,1))

where the equality follows upon using the transformation u = F (x). If the support of F is
bounded, the integral above is always ﬁnite. Even when the support is inﬁnite, the integral
is ﬁnite whenever the variance or the second moment of F is ﬁnite, by the comparison test:
Finiteness of the variance implies that as x → ∞, x2(1 − F (x)) → 0, and as x → −∞,
x2F (x) → 0. Consequently, the covariance sum is of O(1/

k).

√

Summing the ﬁrst term in (5.16) over i, using (5.3) we ﬁnd

k
(cid:88)

i=1

[f (F −1(pi))]−2Var(U(i) − pi) =

1
k + 2

k
(cid:88)

pi(1 − pi)
[f (F −1(pi))]2 , validating (5.12)

i=1
u(1 − u)
[f (F −1(u))]2 du.

≈

(cid:90) 1

0

The transformation u = F (x) shows that

(cid:90) 1

0

u(1 − u)

[f (F −1(u))]2 du =

(cid:90)

F −1((0,1))

F (x)(1 − F (x))
f (x)

dx,

as desired.

Remark 5.6 (Relationship to [5]). In [5], Bobkov and Ledoux provide a comprehensive
discussion of one-dimensional optimal transport from samples in an attempt to understand
convergence of empirical approximations to a measure in the Wasserstein metric. Their analysis
focuses on the “one-sided” convergence of an empirical approximation to a true measure, while
k-variance is based on the Wasserstein distance between two diﬀerent empirical approximations.
That said, along the way their discussion does make some similar observations to our
discussion above. For instance, their Theorem 4.3 shows the same link to order statistics as
our (5.2). The “J2 functional” deﬁned in their (5.3) is the right-hand side of (5.13); in our
notation, their Theorem 5.1 (and, in particular, their Corollary B.6) implies a bound

(5.17)

Vark(µ) ≤

k
k + 1

J2(µ).

This establishes half of our equality in (5.13). Their results show lim supk→∞ Vark(µ) ≤ J2(µ),
while we are able to show under stronger assumptions that limk→∞ Vark(µ) = J2(µ).

Example 5.7 (Uniform distribution, continued). Continuing Example 5.1, we can apply (5.13)

to compute

(5.18)

Var∞(Unif([0, 1])) =

(cid:90) 1

0

x(1 − x)
1

dx =

1
6

.

As expected, this expression agrees with (5.6) as k → ∞.

10

SOLOMON, GREENEWALD, AND NAGARAJA

Example 5.8 (Weibull distribution with shape parameter α). For this distribution, F (x) =
1 − exp{−xα} and f (x) = αxα−1 exp{−xα} for x > 0, with shape parameter α > 0. As
x → 0+,

F (x)(1 − F (x))
f (x)

=

1 − exp{−xα}
αxα−1

≈

xα
αxα−1 =

x
α

,

and consequently the integral (5.13) is always convergent at the lower limit of integration. As
x → ∞,

F (x)(1 − F (x))
f (x)

=

1 − exp{−xα}
αxα−1

≈

1
αxα−1 ,

and hence the integral (5.13) is convergent at the upper limit if and only if α > 2.

Now, for α > 2, (5.13) implies

Var∞(Weib(α)) =

(cid:90) ∞

0

=

1
α2(2/α − 1)

Upon integration by parts we see that

(cid:90) ∞

0

1 − exp{−xα}
αxα−1

dx =

1
α2

(cid:90) ∞

0

(1 − e−y)y2/α−2 dy

(1 − e−y) d(y2/α−1).

(cid:90) ∞

0

(1 − e−y) d(y2/α−1) =

(1 − e−y)y2/α−1(cid:105)∞
(cid:104)

0

(cid:90) ∞

−

0

e−yy2/α−1 dy.

For α > 2, the ﬁrst term yields 0 at both upper and lower limits, and the second term equals
Γ(2/α). Thus,

Var∞(Weib(α)) =

(cid:40) Γ(2/α)
α(α−2)
∞

when α > 2
when α ≤ 2.

Example 5.9 (Tukey’s symmetric λ distribution). This distribution is deﬁned by its quantile

function F −1(u), given by

(5.19)

F −1(u) =

(cid:26) 1

λ (uλ − (1 − u)λ) when λ (cid:54)= 0
when λ = 0
log(u/(1−u))

for u ∈ [0, 1] and λ ∈ R. When λ = 0, we obtain the standard logistic distribution.

When λ (cid:54)= 0, the quantile density function (F −1(u))(cid:48) is given by uλ−1 + (1 − u)λ−1, and

(F −1(u))(cid:48)(cid:48) = (λ − 1)[uλ−2 − (1 − u)λ−2]

is bounded if and only if λ ≥ 2. Hence, we satisfy the suﬃcient conditions needed for
Proposition 5.5. Thus for λ ≥ 2,

Var∞(Tukey(λ)) =

(cid:90) 1

0

u(1 − u)(F −1(u))(cid:48) du =

(cid:90) 1

0

u(1 − u)[uλ−1 + (1 − u)λ−1] du

(5.20)

= 2 · Beta(λ + 1, 2) =

2
(λ + 1)(λ + 2)

.

The integral on the right is ﬁnite whenever λ > −1, and the expression holds for λ = 0. For
λ ∈ (−1, 2), we can only say that lim supk→∞ Vark(µ) is bounded above by the right-hand side.

K-VARIANCE: A CLUSTERED NOTION OF VARIANCE

11

6. Low-dimensional measures. Having worked out the case of one-dimensional measures,
we now consider measures that have low-dimensional structure but are embedded in a higher-
dimensional space. Speciﬁcally, deﬁne

Deﬁnition 6.1 (ε-fattening and ε-covering number, [20, 22]). For any compact X ⊂ Rd and
S ⊆ X, the ε-fattening of S is Sε := {y : D(y, S) ≤ ε}, where D denotes the Euclidean
distance. The ε-covering number Nε(S) of S is the minimum m such that there exist m points
x1, . . . , xm ∈ Rd with S ⊆ (cid:83)
We borrow a recent bound on empirical transport, specialized to W2:

i Bε(Xi).

Proposition 6.2 ([22], Proposition 15). Suppose supp(µ) ⊆ Sε for some ε > 0, where S
satisﬁes Nε(cid:48)(S) ≤ (3ε(cid:48))−d(cid:48) for all ε(cid:48) ≤ 1/27 and some d(cid:48) > 4. Then, for all k ≤ (3ε)−d(cid:48), we have
2 (µ, ˆµk)] ≤ C1 · k−2/d(cid:48), where C1 = 272(2 + 1/(3d(cid:48)/2−2 − 1)) and µk denotes the k-point
E[W 2
empirical measure.

Translating this to our setting using the triangle inequality, we get

Proposition 6.3 (Vark(·) for low-dimensional distributions). Suppose supp(µ) ⊆ Sε for some
ε > 0, where S satisﬁes Nε(cid:48)(S) ≤ (3ε(cid:48))−d(cid:48) for all ε(cid:48) ≤ 1/27 and some d(cid:48) > 4. Then, for all
k ≤ (3ε)−d(cid:48), we have Vark(µ) ≤ C1 · k2/d−2/d(cid:48).
Unsurprisingly, the proposition above shows that if we measure the d-dimensional k-variance
of an intrinsically d(cid:48)-dimensional measure, at least when 4 < d(cid:48) < d we have Vark(µ) → 0 as
k → ∞. As a special case, we see that empirical measures have k-variance tending to zero for
higher-dimensional measures. Interestingly, this is not the case in low dimensions, as we can
see in the following example:

Example 6.4 (Two-point empirical measures). Take µ = (δ−0.5e1 +δ0.5e1 )/2, constructed from
standard basis vector e1 = (1, 0, . . . , 0) ∈ Rd. In this case, W2 between two k-samples from µ
counts the imbalance in the number of −0.5 vs. 0.5 samples between the two draws. Hence,
Vark(µ) is the expected absolute diﬀerence |A − B| of two binomial variables A, B ∼ B(k, 1/2),
scaled by ρ(k,d)/2k. From [17, eq. (2.9)], for binomially-distributed variables X1, X2 ∼ B(k, p)
we have

E(|X1 − X2|) = 2kp(1 − p) · 2F1

1 − k,

; 2; 4p(1 − p)

,

(cid:18)

(cid:19)

1
2

where 2F1 is Gauss’ hypergeometric function. Substituting p = 1/2 shows

E(|X1 − X2|) =

kΓ(2)
2Γ(1/2)Γ(3/2)

(cid:90) 1

t−1/2(1 − t)k−1 dt =

0

kΓ(k + 1/2)
2Γ(3/2)Γ(k + 1)

=

(cid:19)

(cid:18)2k
k

·

k
22k .

Hence,

Vark(µ) =

(cid:19)

ρ(k, d)
22k+1

(cid:18)2k
k

≈

ρ(k, d)
√
πk
2

=

1
√
2

π

·






√

k

(log k)−1 ·
k2/d−1/2

√

k

if d = 1
if d = 2
if d ≥ 3.

by Stirling’s approximation. So, Vark(µ) diverges for d ≤ 3, converges to 1/2
converges to 0 for d ≥ 5.

√

π for d = 4, and

12

SOLOMON, GREENEWALD, AND NAGARAJA

7. Higher-dimensional measures. A surprising result of our experiments detailed in
section 10 is that one-dimensional k-variance seems to have totally diﬀerent behavior than
k-variance for measures on Rd for large d. While we cannot provide as a complete a story
as section 5 for the one-dimensional case, some results in the theory of random Euclidean
matching are directly relevant to our construction and can provide some insight into the
behavior of Vark(·).

Example 7.1 (Unit cube). Suppose µ = Unif([0, 1]d). Then, for large k we have the following

formula [13, eq. (1.1)]:

(7.1)

EX1,...,Xk∼µ
Y1,...,Yk∼µ

(cid:34)

(cid:32)

W 2
2

1
k

k
(cid:88)

i=1

δXi,

1
k

k
(cid:88)

i=1

(cid:33)(cid:35)

δYi

≈






k−1
(log k)/k
k−2/d

if d = 1
if d = 2
if d ≥ 3.

These formulas motivate our choice of scaling factors ρ(k, d) in Deﬁnition 4.1. [7, Theorem 2]
observes similar rates for d > 4 for general measures with support in the unit ball, but their
upper bound decays more slowly in k than (7.1) for d ≤ 4.

Example 7.2 (Unit square). [4] predicts a similar (log k)/k rate for measures with positive
density on the unit square [0, 1]2. Speciﬁcally for µ = Unif([0, 1]2), we can obtain the following
limit [1, Theorem 1.1]:

lim
k→∞

k
log k

EX1,...,Xk∼µ
Y1,...,Yk∼µ

(cid:32)

(cid:34)
W 2
2

1
k

k
(cid:88)

i=1

δXi,

1
k

k
(cid:88)

i=1

δYi

(cid:33)(cid:35)

=

1
2π

.

Hence, we have Var∞([0, 1]2) = 1/2π.

8. Clustered measures. To give an idea of the value of measuring k-variance for k > 1,
in this section we explore the case of clustered measures, which distinguishes the behavior
of Vark(·) from that of Var1(·). We consider the following deﬁnitions, again from [22] similar
to our discussion in section 6, which provide two ways of identifying clusterable structure in
probability measures:

Deﬁnition 8.1 ((m, σ2)-Gaussian mixture). A distribution µ is an (m, σ2)-Gaussian mixture
if it is a mixture of m Gaussian distributions in Rd, and the trace of the covariance matrix of
each mixture component is bounded above by σ2.

Deﬁnition 8.2 (Clusterable measure). A distribution µ is (m, ∆)-clusterable if supp(µ) lies

in the union of m balls of radius at most ∆.

The following proposition from [22] directly suggests a k-variance bound:

Proposition 8.3 ([22], Propositions 13 and 14).

If µ is a (m, σ2)-Gaussian mixture and

log 1/σ ≥ 25/8, then for all k ≤ m(32σ2 log 1/σ)−2,

(8.1)

E[W 2

2 (µ, ˆµk)] ≤ 84(cid:112)m/k,

where ˆµk is the empirical measure obtained by drawing k samples. The same rate holds for
(m, ∆)-clusterable distributions for all k ≤ m(2∆)−4.

K-VARIANCE: A CLUSTERED NOTION OF VARIANCE

13

Application of the triangle inequality to Proposition 8.3 immediately yields the following:
Proposition 8.4 (Vark for clustered distributions). Suppose d > 4. For the (m, σ2)-Gaussian

mixture case with k ≤ m(32σ2 log 1/σ)−2:

For the (m, ∆)-clusterable case with k ≤ m(2∆)−4:

Vark(µ) ≤

168m1/2
k1/2−2/d

.

Vark(µ) ≤

168m1/2
k1/2−2/d

.

Roughly, this proposition shows that as d increases and k satisﬁes the inequality, clustered
distributions have increasingly small Vark(·), though the rate of increase slows rapidly once d
gets beyond ∼ 10.

9. Variance of empirical k-variance. We conclude our mathematical discussion by consid-
ering the problem of how to compute k-variance in practice. There exists an extremely simple
empirical estimator directly motivated by the expectation in (4.1): simply draw 2k samples,
solve the linear program (3.4), and use the resulting value. Note a simple implementation of
this algorithm takes roughly O(k2d + k3) time, accounting for the time taken to compute the
pairwise cost matrix as well as solving the transport linear program (our implementation uses
[12]). Here we bound the variance of this estimator, roughly showing that fewer trials need to
be averaged to compute k in large dimension.

In detail, we consider the empirical estimator built from n trials:

(cid:100)Vark(µ) :=

ρ(k, d)
2n

n
(cid:88)

j=1

W 2

2 (ˆµj

k, ˆµ(cid:48)j
k ),

where ˆµj

k, ˆµ(cid:48)j

k are independent empirical measures formed from k i.i.d. samples as in (4.1).

The following theorem helps characterize the variance of our estimator above:
Theorem 9.1 (Empirical variance). Suppose µ ∈ Prob(Rd) has support in a set of radius R.
k to be independent empirical measures each constructed
k) for ˆµk and Y k,j := (Y j
k).

For each j ∈ {1, . . . , n}, take ˆµj
from k i.i.d. samples from µ (X k,j := (X j
Then,

1 , . . . , Y j

1, . . . , X j

k ) for ˆµ(cid:48)

k, ˆµ(cid:48)j

(9.1)

(cid:32)
(cid:12)
(cid:12)
(cid:12)(cid:100)Vark(µ) − Vark(µ)

P

(cid:12)
(cid:12) ≥ ρ(k, d)R2
(cid:12)

(cid:114)

(cid:33)

log(kn)
kn

≤

2
k2n2 .

Proof. We use McDiarmid’s inequality:
Lemma 9.2 (McDiarmid’s Inequality, [15]). Let X m := (X1, . . . , Xm) be an m-tuple of X -
valued independent random variables. Suppose g : X m → R is a map that for any i = 1, . . . , m
and x1, . . . , xm, x(cid:48)

i ∈ X satisﬁes

(9.2)

(cid:12)
(cid:12)g(x1, . . . , xm) − g(x1, . . . , xi−1, x(cid:48)

i, xi+1, . . . , xm)(cid:12)

(cid:12) ≤ ci,

14

SOLOMON, GREENEWALD, AND NAGARAJA

for some non-negative {ci}m

i=1. Then for any t > 0:

(9.3a)

(9.3b)

(cid:16)

P

g(X1, . . . , Xm) − Eg(X1, . . . , Xm) ≥ t

(cid:17)

≤ e

− 2t2

(cid:80)m

i=1

c2
i

P

(cid:16)(cid:12)
(cid:12)g(X1, . . . , Xm) − Eg(X1 . . . , Xm)(cid:12)

(cid:12) ≥ t

(cid:17)

− 2t2

(cid:80)m

i=1

≤ 2e

c2
i .

(cid:80)n

Consider 1
n

j=1 W 2
computed, each sample being a pair (xj
the general formula:

k, ˆµ(cid:48)j

2 (ˆµj

k ) as a function of the nk independent samples from which it is
i ). Using Kantorovich–Rubinstein duality, we have

i , yj

W 2

2 (P, Q) = sup

EP [f ] + EQ[g]

(f,g)∈Φ
where Φ = {(f, g) ∈ L1(P ) × L1(Q) : f (x) + g(y) ≤ (cid:107)x − y(cid:107)2}. In our case, separately for each
j, we can write

W 2

2 (ˆµj

k, ˆµ(cid:48)j

k ) = W 2

2

(cid:32)

1
k

k
(cid:88)

(cid:96)=1

,

δxj

(cid:96)

(cid:33)

1
k

k
(cid:88)

(cid:96)=1

δyj

(cid:96)

= sup

(f,g)∈Φ

1
k

k
(cid:88)

(f (xj

(cid:96)) + g(yj

(cid:96) )).

(cid:96)=1

(cid:96), yj
i ) with some (x(cid:48)j

Recall that the (xj
(cid:96) ) are independent across (cid:96) and j. Consider replacing one of the elements
(xj
k and ¯µ(cid:48)j
i , y(cid:48)j
(cid:96) ) are identically distributed,
by symmetry we can set i = 1. We thus bound

k . Since the (xj

i ), forming ¯µj

(cid:96), yj

i , yj

W 2

2 (ˆµj

k, ˆµ(cid:48)j

k ) − W 2

2 (¯µj

k, ¯µ(cid:48)j

k ) = sup
(f,g)∈Φ

(cid:32)

1
k

(f (xj

1) + g(yj

1)) +

(cid:33)

(f (xj

(cid:96)) + g(yj

(cid:96) ))

k
(cid:88)

(cid:96)=2

(cid:32)

1
k

− sup

(f,g)∈Φ

(f (x(cid:48)j

1 ) + g(y(cid:48)j

1 )) +

k
(cid:88)

(cid:96)=2

(f (xj

(cid:96)) + g(yj

(cid:96) ))

(cid:33)

≤

2R2
k

,

where we have assumed the space is bounded with radius R and used the deﬁnition of Φ and
[21, Remark 1.13].

Hence by symmetry and scaling by ρ(k,d)
2n
ρ(k, d)
2n

ρ(k, d)
2n

k, ˆµ(cid:48)j

2 (ˆµj

k ) −

W 2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

W 2

2 (¯µj

(cid:12)
(cid:12)
k, ¯µ(cid:48)j
k )
(cid:12)
(cid:12)

≤

ρ(k, d)R2
kn

,

as in the expression in the theorem we have

satisfying the condition (9.2) for McDiarmid’s inequality for each of the nk random variables
(xj

i ). Therefore, for any t > 0, by (9.3b) we have

i , yj

(W 2

2 (ˆµj

k, ˆµ(cid:48)j

k ) − EW 2

2 (ˆµk, ˆµ(cid:48)

≥ t

 ≤ 2e

− 2knt2

R4ρ(k,d)2 .



(cid:12)
(cid:12)
(cid:12)
k))
(cid:12)
(cid:12)
(cid:12)

P





(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

ρ(k, d)
2n

j=1
(cid:113) log(kn)
kn

then yields

(cid:16)

W 2

2 (ˆµj

k, ˆµ(cid:48)j

k ) − EW 2

2 (ˆµj

k, ˆµ(cid:48)j
k )

≥ ρ(k, d)R2

(cid:114)

log(kn)
kn



 ≤

2
k2n2 .

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Setting t = ρ(k, d)R2
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

ρ(k, d)
2n

P

n
(cid:88)

j=1

Recalling the deﬁnition of Vark(µ), the theorem results.

K-VARIANCE: A CLUSTERED NOTION OF VARIANCE

15

(a) d = 1
Distribution functions

(b) d = 2
Samples

Figure 1. Example distributions for the experiments in subsection 10.1. For d = 1 we show the density
functions corresponding to the diﬀerent colors, and for d = 2 we show samples drawn from the various measures.

Remark 9.3 (Interpretation of Theorem 9.1). In words, as dimension d and size k increase,
we need a smaller number n of independent trials n of k-samples to estimate k-variance
accurately. Eventually, even choosing n = 1 suﬃces.

Remark 9.4 (Alternative forms for Theorem 9.1). Theorem 9.1 is written in terms of the
number n of sets of k replicates. We can rewrite it in terms of k and m = kn, i.e., when a
total of m samples are available and one is choosing a k to partition them. We have for d > 2

(cid:32)
(cid:12)
(cid:12)
(cid:12)(cid:100)Vark(µ) − Vark(µ)

P

(cid:12)
(cid:12) ≥ ρ(k, d)R2
(cid:12)

(cid:114)

(cid:33)

log m
m

≤

2
m2 .

This in a sense reverses the tradeoﬀ, with ﬁner divisions of the m available samples (smaller k)
reducing the overall variance (only slightly for large d, however).

10. Experiments. In this section, we provide some simple experiments demonstrating
the behavior of Vark(·) and suggesting how it might be used to understand properties of
distributions and datasets that are not well-captured by variance alone.

10.1. Gaussian Mixtures. We begin with a synthetic experiment illustrating the behavior
of k-variance in diﬀerent dimensions and in the presence of multimodality. In our experiments,
we consider mixtures Gx := 0.5N (−x·e1, σId×d)+0.5N (x·e1, σId×d) of two isotropic Gaussians,
where e1 ∈ Rd is the ﬁrst standard basis vector in Rd. We choose σ(x) so that Var1(Gx) = 1;
note σ(x) decreases as |x| increases, leading to bimodal/approximately clustered distributions.
See Figure 1 for examples in dimensions 1 and 2.

Figure 2 shows k-variance of Gx as a function of k (horizontal axis) and x (color) in diﬀerent
ambient dimensions d. We use the empirical estimator of k-variance averaged over 10,000 trials
for each point in the plot. We can make a number of observations based on this experiment:
• For d ≥ 3, the k-variance is smaller for clustered distributions (red) than unimodal Gaussians

(blue) with identical (1-)variance.

• The d ∈ {1, 2} cases exhibit unique, nonmonotonic behavior. For instance, when d = 1,
k-variance is highest for the sharply bimodal distributions (red), then decreases for wide-
and-ﬂat distributions (dark red/green), and then increases again for Gaussians (blue).
• For larger dimension d, the curves look smoother. This is a byproduct of the results in

-2-1.5-1-0.500.511.5200.511.5-3-2-10123-2-1.5-1-0.500.511.5216

SOLOMON, GREENEWALD, AND NAGARAJA

d = 1

d = 2

d = 3

d = 4

d = 5

d = 10

d = 50

d = 100

Figure 2. k-variance experiments with Gaussian mixture models (see subsection 10.1) in increasing
dimension. As predicted, k-variance follows similar patterns in d ≥ 3 and is lower for clustered distributions,
but for d ∈ {1, 2} the behavior is diﬀerent. Colors range from bimodal mixtures of low-variance Gaussians (red)
to unimodal Gaussian measures (blue); see Figure 1 for examples in d ∈ {1, 2}.

Figure 3. k-variance of measures supported on low-dimensional hyperplanes in R1000. Each curve corresponds
to a diﬀerent intrinsic dimensionality d(cid:48) marked in the legend; m is the slope of the best-ﬁt curve in the log-log
plot. Note the correlation between m and the intrinsic dimensionality of the measure.

section 9, which predict that the empirical estimator of Vark(·) has lower variance in high
dimension given a ﬁxed number of samples.

10.2. Low-dimensional measures. Now, we consider the case explored in section 6, in
which our probability measure is embedded in a low-dimensional slice of the ambient space Rd.
When d is suﬃciently large, Proposition 6.3 predicts that the k-variance for such a measure
will decay to zero at a rate determined by the intrinsic dimensionality of the measure.

As an initial experiment, we consider Gaussian measures in dimension d = 1000 supported

01020304050k0.511.522.53Empirical k-variance01020304050k0.811.21.41.61.822.2Empirical k-variance01020304050k0.811.21.41.61.822.2Empirical k-variance01020304050k0.80.911.11.21.31.41.5Empirical k-variance01020304050k0.60.70.80.911.11.21.3Empirical k-variance01020304050k0.30.40.50.60.70.80.911.1Empirical k-variance01020304050k0.20.30.40.50.60.70.80.911.1Empirical k-variance01020304050k0.20.30.40.50.60.70.80.911.1Empirical k-variance101102k10-1100Empirical k-variance1: m=-0.872: m=-0.73: m=-0.564: m=-0.465: m=-0.3910: m=-0.2325: m=-0.1250: m=-0.07975: m=-0.061100: m=-0.051250: m=-0.03500: m=-0.02750: m=-0.0151000: m=-0.013K-VARIANCE: A CLUSTERED NOTION OF VARIANCE

17

Figure 4. Similar experiment to Figure 3, now with points in the unit sphere Sd(cid:48)−1 embedded in Rd.

on a d(cid:48)-dimensional hyperplane, where d(cid:48) varies from 1 to d. Here, we create the d(cid:48)-dimensional
measure by creating a Gaussian with covariance

Σd(cid:48) = diag(1/d(cid:48), . . . , 1/d(cid:48)
, 0, . . . , 0
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124)
(cid:125)
d−d(cid:48) slots

(cid:123)(cid:122)
d(cid:48) slots

).

Here, the 1/d(cid:48) entries ensure that the measure has variance 1. Figure 3 plots the k-variance of
the d(cid:48)-dimensional measures on a logarithmic scale; we use the empirical estimator of k-variance
averaged over 1,000 trials.

As predicted by Proposition 6.3, the slopes of the ﬁt lines in Figure 3 cleanly correlated with
intrinsic dimensionality. As d(cid:48) increases, the lines also become smoother, again a byproduct of
the variance bounds in section 9. This is a happy coincidence: We are able to distinguish the
slopes of the diﬀerent lines for large d(cid:48)—even though they are close in value—because we can
estimate Vark(·) more accurately in this regime.

Figure 4 shows a similar experiment to Figure 3, but now the data lies on the sphere
Sd(cid:48)−1 embedded in Rd; we sample uniformly from Sd(cid:48)−1 by normalizing the samples from
the previous experiment to unit length. Once again the trendlines strongly ﬁt the power
law we expect to see, but the slopes are now less negative compared to Figure 3 since the
intrinsic dimensionality has decreased by 1. In particular, note that d(cid:48) = 1 corresponds to a
zero-dimensional sphere S0, i.e. two points on the real line. Since S0 is thus a discrete dataset,
this explains the approximately −1/2 slope in the log-log plot, corresponding to the k−1/2 decay
indicated in section 8.

10.3. Digits. Figure 5 plots approximate k-variance for the MNIST dataset of handwritten
digits [14], separated by digit. We use the stochastic estimator for k-variance from section 9,
where sampling from the distribution of handwritten digits is simulated by a bootstrapped
strategy of sampling from the dataset with replacement. Our distributions in this case are
over R784, representing 28 × 28 images. Given the high ambient dimension and the well-
documented observation from past work that the MNIST digits roughly lie on low-dimensional
submanifolds of R784, we expect k-variance to diminish to zero in this experiment. So, the
relevant measurement is the rate at which this decay occurs.

Beyond varying amounts of variance between diﬀerent digits (k = 1), our experiments also
reveal that the digit “1” has k-variance decaying in k roughly 1.5× faster than the other digits.

101102k10-1100Empirical k-variance1: m=-0.492: m=-0.93: m=-0.744: m=-0.585: m=-0.4710: m=-0.2625: m=-0.1350: m=-0.0875: m=-0.062100: m=-0.052250: m=-0.03500: m=-0.02750: m=-0.0151000: m=-0.01318

SOLOMON, GREENEWALD, AND NAGARAJA

(a) Linear

(b) Log-log

Figure 5. k-variance of each MNIST digit from 0 to 9 in (a) linear and (b) log-log scale. Each digit is

considered as a 282-dimensional vector (d = 784); there are approximately 6,000 images per digit.

This provides a quantitative indicator of the observation that there are fewer variations in the
way “1” is written relative to other digits.

Less importantly, on the far right of the plots we see decay of the k-variance begin to
accelerate. This downward turn occurs roughly at the size of the dataset, because at this scale
the bootstrapped estimator becomes less eﬀective: For extremely large k the dataset looks like
a collection of discrete points rather than a smooth distribution over R784.

11. Conclusion and Future Work. We can compute k-variance easily using a few lines of
code, revealing potentially interesting structure hidden in a dataset or probability distribution.
Hence, it is a straightforward addition to the data analysis toolkit. While its properties in ≤ 4
dimensions are somewhat unexpected, beyond this point k-variance provides an intuitive means
of measuring intra-cluster variance. Somewhat surprisingly given the “curse of dimensionality”
associated to optimal transport [22], we can use fewer data points to estimate k-variance of
high-dimensional measures, as shown in section 9.

Beyond its immediate relevance as an analytical tool, k-variance motivates a wide variety

of challenging research problems moving forward:
• Are there nontrivial pairs of measures µ, ν ∈ Prob(Rd) with Vark(µ) = Vark(ν) for all k ≥ 1?
Under what conditions can a measure be reconstructed from its mean and sequence of
k-variance values?

• Beyond the empirical estimator proposed in this paper, are there more eﬃcient or unbiased

stochastic estimators for k-variance?

• Is it possible to generalize k-variance to a notion of “k-covariance” for d > 1?
• Are there analogs of k-variance for higher-order moments of a measure?
• How do gradient ﬂows of k-variance behave?

Acknowledgments. The authors thank Philippe Rigollet for early feedback and in partic-
ular noticing the connection of our work to random bipartite matching and to [5]; Lawrence
Stewart for early discussion and experiments; David Wu for early discussions and help deriving
combinatorial identities; Mikhail Yurochkin for discussion and feedback; and David Palmer for

02004006008001000k051015202530354045Empirical k-variance0123456789101102103k101Empirical k-variance0: m=-0.21: m=-0.282: m=-0.173: m=-0.174: m=-0.195: m=-0.186: m=-0.27: m=-0.28: m=-0.169: m=-0.2K-VARIANCE: A CLUSTERED NOTION OF VARIANCE

19

assistance running some experiments.

References.

[1] L. Ambrosio and F. Glaudo, Finer estimates on the 2-dimensional matching problem,

Journal de l’´Ecole polytechnique—Math´ematiques, 6 (2019), pp. 737–765.

[2] B. C. Arnold and N. Balakrishnan, Approximations to moments of order statistics,
in Relations, Bounds and Approximations for Order Statistics, Springer, 1989, pp. 73–107.
[3] F. Barthe and C. Bordenave, Combinatorial optimization over two random point

sets, in S´eminaire de Probabilit´es XLV, Springer, 2013, pp. 483–535.

[4] D. Benedetto and E. Caglioti, Euclidean random matching in 2d for non-constant

densities, Journal of Statistical Physics, 181 (2020), pp. 854–869.

[5] S. Bobkov and M. Ledoux, One-dimensional empirical measures, order statistics, and

Kantorovich transport distances, vol. 261, American Mathematical Society, 2019.

[6] S. Caracciolo, C. Lucibello, G. Parisi, and G. Sicuro, Scaling hypothesis for the

Euclidean bipartite matching problem, Physical Review E, 90 (2014), p. 012118.

[7] L. Chizat, P. Roussillon, F. L´eger, F.-X. Vialard, and G. Peyr´e, Faster Wasser-
stein distance estimation with the Sinkhorn divergence, Advances in Neural Information
Processing Systems, 33 (2020).

[8] H. A. David and H. N. Nagaraja, Order statistics, 2003.
[9] J. B. de Monvel and O. Martin, Almost sure convergence of the minimum bipartite

matching functional in Euclidean space, Combinatorica, 22 (2002), pp. 523–530.

[10] S. Dereich, M. Scheutzow, and R. Schottstedt, Constructive quantization: Ap-
proximation by empirical measures, in Annales de l’IHP Probabilit´es et statistiques, vol. 49,
2013, pp. 1183–1203.

[11] V. Dobri´c and J. E. Yukich, Asymptotics for transportation cost in high dimensions,

Journal of Theoretical Probability, 8 (1995), pp. 97–118.

[12] I. S. Duff and J. Koster, On algorithms for permuting large entries to the diagonal of a
sparse matrix, SIAM Journal on Matrix Analysis and Applications, 22 (2001), pp. 973–996.
[13] M. Goldman and D. Trevisan, Convergence of asymptotic costs for random Euclidean

matching problems, arXiv:2009.04128, (2020).

[14] Y. LeCun, C. Cortes, and C. J. Burges, The MNIST database of handwritten digits,

(1998), http://yann.lecun.com/exdb/mnist.

[15] C. McDiarmid, On the method of bounded diﬀerences, in Surveys in Combinatorics
(London Mathematical Soc. Lecture Notes), vol. 141, Cambridge Univ. Press, 1989,
pp. 148–188.

[16] G. Peyr´e and M. Cuturi, Computational optimal transport: With applications to data

science, Foundations and Trends in Machine Learning, 11 (2019), pp. 355–607.

[17] T. Ramasubban, The mean diﬀerence and the mean deviation of some discontinuous

distributions, Biometrika, 45 (1958), pp. 549–556.

[18] A. R´enyi, On the theory of order statistics, Acta Mathematica Academiae Scientiarum

Hungarica, 4 (1953), pp. 191–231.

[19] F. Santambrogio, Optimal transport for applied mathematicians, Birk¨auser, NY, 55

(2015), p. 94.

[20] M. Talagrand, Concentration of measure and isoperimetric inequalities in product

20

SOLOMON, GREENEWALD, AND NAGARAJA

spaces, Publications Math´ematiques de l’Institut des Hautes Etudes Scientiﬁques, 81
(1995), pp. 73–205.

[21] C. Villani, Topics in Optimal Transportation, no. 58, American Mathematical Society,

2003.

[22] J. Weed and F. Bach, Sharp asymptotic and ﬁnite-sample rates of convergence of
empirical measures in Wasserstein distance, Bernoulli, 25 (2019), pp. 2620–2648.

[23] J. E. Yukich, Probability theory of classical Euclidean optimization problems, Springer,

2006.

