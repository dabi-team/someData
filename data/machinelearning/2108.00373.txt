SPEAR : Semi-supervised Data Programming in Python

Guttu Sai Abhishek1∗, Harshad Ingole1∗, Parth Laturia1∗, Vineeth Dorna1∗,
Ayush Maheshwari1∗, Rishabh Iyer2, Ganesh Ramakrishnan1
1Indian Institute of Technology Bombay
2The University of Texas at Dallas

2
2
0
2

t
c
O
5

]

G
L
.
s
c
[

3
v
3
7
3
0
0
.
8
0
1
2
:
v
i
X
r
a

Abstract

We present SPEAR, an open-source python li-
brary for data programming with semi super-
vision. The package implements several re-
cent data programming approaches including
facility to programmatically label and build
SPEAR facilitates weak su-
training data.
pervision in the form of heuristics (or rules)
and association of noisy labels to the train-
ing dataset. These noisy labels are aggre-
gated to assign labels to the unlabeled data
for downstream tasks. We have implemented
several label aggregation approaches that ag-
gregate the noisy labels and then train using
the noisily labeled set in a cascaded manner.
Our implementation also includes other ap-
proaches that jointly aggregate and train the
model for text classiﬁcation tasks. Thus, in
our python package, we integrate several cas-
cade and joint data-programming approaches
while also providing the facility of data pro-
gramming by letting the user deﬁne label-
ing functions or rules. The code and tutorial
notebooks are available at https://github.
com/decile-team/spear.
Further, exten-
sive documentation can be found at https:
//spear-decile.readthedocs.io/. Video
tutorials demonstrating the usage of our pack-
age are available here. We also present some
real-world use cases of SPEAR.

1

Introduction

Supervised machine learning approaches require
large amounts of labeled data to train robust ma-
chine learning models. For classiﬁcation tasks
such as spam detection, (movie) genre categoriza-
tion, sequence labelling, and so on, modern ma-
chine learning systems rely heavily on human-
annotated gold labels. Creating labeled data can
be a time-consuming and expensive procedure that
necessitates a signiﬁcant amount of human effort.
To reduce dependence on human-annotated labels,

∗Authors contributed equally

various techniques such as semi-supervision, dis-
tant supervision, and crowdsourcing have been
proposed.
In order to help reduce the subjec-
tivity and drudgery in the labeling process, sev-
eral recent data programming approaches (Bach
et al., 2019; Chatterjee et al., 2020; Awasthi et al.,
2020; Maheshwari et al., 2021) have proposed the
use of human-crafted labelling functions or auto-
matic LFs (Maheshwari et al., 2022a) to weakly
associate labels with the training data. Users en-
code supervision in the form of labelling functions
(LFs), which assign noisy labels to unlabeled data,
reducing dependence on human labeled data. LFs
can deﬁned as ﬁrst-order logic rules as a compo-
sition of semantic role attributes (Sen et al., 2020)
or syntactic grammar rules (Sahay et al., 2021).

While most data-programming approaches
cited above provide their source code in the pub-
lic domain, a uniﬁed package providing access
to all data programming approaches is however
missing.
In this work, we describe SPEAR, a
python package that implements several existing
data programming approaches while also provid-
ing a platform for integrating and benchmark-
ing newer ones. Inspired by frameworks such as
Snorkel (Lison et al., 2021; Ratner et al., 2017;
Zhang et al., 2021) and algorithm based labeling
in Matlab1, we provide a facility for users to de-
ﬁne LFs. Further, we develop and integrate several
recent data programming models that uses these
LFs. We provide many easy-to-use jupyter note-
books and video tutorials for helping new users get
quickly started. Though we provide implementa-
tion on 5 text datasets, our package can be easily
integrated with vision and speech datasets as well.
The users can get started by installing the package
using the below command.

pip install decile-spear

1https://www.mathworks.com/help/vision/ug/create-

automation-algorithm-for-labeling.html

 
 
 
 
 
 
Figure 1: Flow of the SPEAR library.

In Table 1, we compare our library with other
existing packages such as Wrench (Zhang et al.,
2021), SkWeak(Lison et al., 2021), Imply Loss
(Awasthi et al., 2020), Snorkel (Bach et al., 2019)
and Matlab. Wrench (Zhang et al., 2021) pro-
vides facility for semi-supervised and unsuper-
vised label aggregation approaches, however, it
does not provide mechanism to ﬁnd useful sub-
set of unlabeled data and deﬁning continuous LFs.
SkWeak (Lison et al., 2021) does not integrate
semi-supervised LA approaches in the package.
SPEAR addresses the shortcomings of existing
packages by providing features such as designing
of discrete and continuous LFs, integrating un-
supervised and semi-supervised aggregation ap-
proaches and facility to choose labeled set using
subset selection approaches.

2 Package Flow

The SPEAR package consists of three components
(and they are applied in the same order): (i) De-
signing LFs, (ii) applying LFs, and (iii) applying a
label aggregator (LA).
Initially, the user is expected to declare an enum
class listing all the class labels. The enum class as-
sociates the numeric class label with the readable
class name. As part of (i), SPEAR provides the fa-
cility for manually creating LFs. LFs can be in the
form of regex rules as well. Additionally, we also
provide the facility of declaring a @preprocessor
decorator to use an external library such as spacy2,
nltk, etc. which can be optionally invoked by the
LFs. Thereafter, as part of (ii), the LFs can be ap-
plied on the unlabeled (and labeled) set using an

2https://spacy.io

apply function that returns a matrix of dimension
#LFs × #instances. The matrix is then provided as
input to the selected label aggregator (LA) in (iii),
as shown in Figure1. We integrate several LA op-
tions into SPEAR. Each LA aggregates multiple
noisy labels (obtained from the LFs) to associate
a single class label with an instance. Additionally,
we have also implemented in SPEAR, several joint
learning approaches that employ semi-supervision
and feature information. The high-level ﬂow of
the SPEAR library is presented in Figure 1.

3 Designing and Applying LFs

User interacts with the library by designing label-
ing functions. Similar to Ratner et al. (2017), la-
beling functions are python functions which take
a candidate as an input and either associates class
label or abstains. However, continuous LFs re-
turns a continuous score in addition to the class
label. These continuous LFs are more natural to
program and lead to improved recall (Chatterjee
et al., 2020).

3.1 Designing LFs

SPEAR uses a @labeling_function() decorator
to deﬁne a labeling function. Each LF, when ap-
plied on an instance, can either return a class label
or not return anything, i.e. abstain. The LF decora-
tor has an additional argument that accepts a list of
preprocessors. Each preprocessor can be either de-
clared as a pre-deﬁned function or can employ ex-
ternal libraries. The pre-processor transforms the
data point before applying the labeling function.

@labeling_function(cont_scorer, resources,

preprocessors, label)

def CLF1(x,**kwargs):

Unlabeled SetDesigning LFs@preprocessor@continuous_scorer@labeling_functionLAJoint-Learning(JL)CAGEOnly-LL2RPRImplyLossLabeled Set      Apply LFs on dataPreLabels.generate_pickle()def how_LF():    if text contains ‘how’:return Descriptiondef how_long_LF():    if text contains ‘how long’:return NumericPackage

Snorkel(Ratner et al.,
2017)
Imply Loss (Awasthi
et al., 2020)
Matlab
SkWeak (Lison et al.,
2021)
Wrench (Zhang et al.,
2021)
SPEAR

Designing &
applying LFs
(cid:51)

(cid:55)

(cid:51)
(cid:51)

(cid:51)

(cid:51)

Continuous
LFs

Unsup LA Semi-sup

LA

(cid:55)

(cid:55)

(cid:55)
(cid:51)

(cid:55)

(cid:51)

(cid:55)

(cid:55)

(cid:55)
(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:55)
(cid:55)

(cid:51)

(cid:51)

Table 1: Comparison of SPEAR against available packages.

Labeled-data
subset selection
(cid:55)

(cid:55)

(cid:55)
(cid:55)

(cid:55)

(cid:51)

return label if kwargs["continuous_score"] >=

threshold else ABSTAIN

The LF can express pattern matching rules in
the form of heuristics, distant supervision by using
external knowledge bases and other data resources
to label datapoints. LFs on SMS dataset can be
seen in the example notebook here.

Continuous LFs:
In the discrete LFs, users
construct heuristic patterns based on dictionary
lookups or thresholded distance for the classiﬁca-
tion tasks. However, the keywords in hand-crafted
dictionaries might be incomplete. Chatterjee et al.
(2020) proposed a comprehensive alternative that
design continuous valued LFs that return scores
derived from soft match between words in the sen-
tence and the dictionary.

SPEAR provides the facility to declare contin-
uous LFs, each of which returns the associated
label along with a conﬁdence score using the
@continuous_scorer decorator. The continuous
score can be accessed in the LF deﬁnition through
the keyword argument continuous_score. As
evident from Table 1, no other existing package
provisions for both semi-supervised aggregation
and subset selection modules.

@continuous_scorer()
def similarity(sentence,**kwargs):
word_vecs = featurizer(sentence)
keyword_vecs = featurizer(kwargs["keywords"])
return similarity(word_vecs,keyword_vecs)

3.2 Applying LFs

Once LFs are deﬁned, users can analyse labeling
functions by calculating coverage, overlap, con-
ﬂicts, empirical accuracy for each LF which helps

to re-iterate on the process by reﬁning new LFs.
The metrics can be visualised within the SPEAR
tool, either in the form of a table or graphs as
shown in Figure 2.

PreLabels is the master class which encapsu-
lates a set of LFs, the dataset to label and enum
of class labels. PreLabels facilitates the process of
applying the LFs on the dataset, and of analysing
and reﬁning the LF set. We provide functions to
store labels assigned by LFs and associated meta-
data such as mapping of class name to numeric
class labels on the disk in the form json ﬁle(s).
The pre-labeling performed using the LFs can be
consolidated into labeling performed using several
consensus models described in Section 4.

sms_pre_labels = PreLabels(name="sms",

data=X_V, gold_labels=Y_V,
data_feats=X_feats_V, rules=rules,

labels_enum=ClassLabels, num_classes=2)

4 Models

We implement several data-programming ap-
proaches in this demonstration that includes sim-
ple baselines such as fully-supervised,
semi-
supervised and unsupervised approaches.

4.1

Joint Learning (Maheshwari et al., 2021)

The joint learning (JL) module implements a semi-
supervised data programming paradigm that learns
a joint model over LFs and features.
JL has
two key components, viz., feature model (fm) and
graphical model (gm) and their sum is used as a
training objective. During training, the JL requires
labeled (L), validation (V), test (T ) sets consisting
of true labels and an unlabeled (U) set whose true
labels are to be inferred. The model API closely

Figure 2: LF analysis on the SMS dataset presented in the form of graph visualization within the SPEAR tool. The
statistics include precision, coverage, conﬂict and empirical accuracy for each LF.

follows that of scikit-learn (Pedregosa et al., 2011)
to make the package easily accessible to the ma-
chine learning audience. The primary functions
are: (1) fit_and_predict_proba, which trains
using the prelabels assigned by LFs and true labels
of L data and predicts the probabilities of labels
for each instance of U data (2) fit_and_predict,
similar to the previous one but which predicts
labels of U using maximum posterior probabil-
ities (3) predict_(fm/gm)_proba, predicts the
probabilities, using feature model(fm)/graphical
model(gm) (4) predict_(fm/gm), predicts labels
using fm/gm based on learned parameters. We
also provide functions save or load_params to
save or load the trained parameters.

As another unique feature (c.f. Table 1), our
library supports a subset-selection framework that
makes the best use of human-annotation efforts.
The L set can be chosen using submodular
functions such as facility location, max cover,
etc. We utilise the submodlib3 library for the
subset selection algorithms. The function alterna-
tives for subset selection are rand_subset,
unsup_subset,
sup_subset_indices,
sup_subset_save_files.

3https://github.com/decile-team/submodlib

4.2 Only-L

In this, the classiﬁer P (y|x) is trained only on the
labeled data. Following Maheshwari et al. (2021),
we provide facility to use either Logistic Regres-
sion or a 2-layered neural network. Our package is
ﬂexible to allow other architectures to be plugged-
in as well.

4.3 CAGE (Chatterjee et al., 2020)

This accepts both continuous and discrete LFs.
Further, each LF has an associated quality guide
component, that refers to the fraction of times the
LF predicts the correct label; this stabilises train-
ing in absence of V set.
In our package, CAGE
accepts U and T sets during training. CAGE has
member functions similar to (except there are no
fm or gm variants to predict_proba, predict
functions in Cage) JL module, with different ar-
guments, serving the same purpose. It should be
noted that this model doesn’t need labeled(L) or
validation(V) data.

4.4 Learning to Reweight (L2R) (Ren et al.,

2018)

This method is an online meta-learning approach
for reweighting training examples with a mix of
U and L. It leverages validation set to determine
and adaptively assigns importance weights to ex-
amples based on the gradient direction. This does

Figure 3: Experiments on SMS, IMDB and MIT-R dataset and comparison with various approaches. We use JL
combined with supervised subset selection for obtaining numbers.

not employ additional parameters to weigh or de-
noise individual rules.

4.5 L + USnorkel (Ratner et al., 2017)
This method trains a supervised classiﬁer on L set
and Snorkel’s generative model on U set. Snorkel
is a generative model that models class probabil-
ities based on discrete LFs for consensus on the
noisy and conﬂicting labels.
It assigns a linear
weight to each rule based on an agreement objec-
tive and label examples in U.

4.6 Posterior Regularization (PR) (Hu et al.,

2016)

This is a method that enables to simultaneously
learn from L and logic rules by jointly learning
a rule and feature network in a teacher-student
setup. The student network learns parameter θ us-
ing the L set and teacher networks attempts to im-
itates the student network in a joint learning man-
ner. The teacher network encodes logic rules as a
regularization term in the overall loss objective.

4.7

Imply Loss (Awasthi et al., 2020)

This approach uses additional information in the
form of labeled rule exemplars and trains with a
denoised rule-label loss. They leverage both rules
and labeled data by mapping each rule with exem-
plars of correct ﬁrings (i.e., instantiations) of that
rule. Their joint training algorithms denoise over-
generalized rules and train a classiﬁcation model.
It has two main components:

1. Rule Network: It learns to predict whether
a given rule has overgeneralized on a given

sample using latent coverage variables.

2. Classiﬁcation Network: It is trained on L and
U to predict the output label and maximize
the accuracy on unseen test instances using a
soft implication loss.

This module contains the following primary

classes:

1. DataFeeder - It will essentially take all the
parameters as input and create a data feeder
class with all
these parameters as its at-
tributes.

-

2. HighLevelSupervisionNetwork (HLS)

It
will take the 2 networks, the mode or the
approach that needs to be used to train the
model, the required parameters, the direc-
tory storing model checkpoints at different
instances and the instances and labels from
the labeled dataset (L) and create an object
named "hls".

HLS object will have many member functions of
which the 2 signiﬁcant are:
(a) hls.train: This function, when called with the
required mode, will train the 2 network attributes
of the object.
(b) hls.test: It supports 3 types of testing:

(i) test_w: this will test the rule network and the

related model of the object.

(ii) test_f:

this will test the classiﬁcation net-

work and the related model of the object.

(iii) test_all: this will test both the networks and

models of the class.

Macro-F15060708090SMSMITRIMDBOnly-LL2RL+USnorkelPosterior RegImplyLossJoint Learning (JL)JL with Subset SelectionFigure 4: Mutiple LFs generated from post-editor edits based on semantic and lexical features while editing science
(domain-speciﬁc) document in English.

5 Experiments

We prepared jupyter tutorial notebooks for two
standard text classiﬁcation datasets, namely SMS,
YouTube and TREC. We took LFs on these
datasets from Awasthi et al. (2020) and train us-
ing approaches implemented in this paper. Fig-
ure 3 shows performance of various approaches
implemented using our package on additional two
datasets, MIT-R and IMDB. We can integrate im-
age classiﬁcation tasks by deﬁning appropriate
feature extraction module and rules.

6 Use Cases

SPEAR is employed in project UDAAN4 for re-
ducing post editing efforts. UDAAN (Maheshwari
et al., 2022b) is a post-editing workbench to trans-
late content in native languages. Based on the post
editor’s patterns of changes to the target language
document, candidate labeling functions are gener-
ated (based on a combination of heuristics and lin-
guistic patterns) by the UDAAN workbench (c.f.
Figure 4 for examples of LFs). Based on these
LFs, SPEAR gets invoked on a combination of the
edited (i.e., labeled) data and the not yet edited
(i.e., unlabeled) data to present consolidated ed-
its to the post-editor. This use case has been pre-
sented in the ﬂow chart in Figure 4. We present
the appropriate incorporation of SPEAR into the

4https://www.udaanproject.org/

post-editing environment of an ecosystem such as
for translation (UDAAN) or even for Optical Char-
acter Recognition5 or Automatic Speech Recogni-
tion (ASR).

As a part of COVID-19 third wave prepared-
ness, SPEAR was used by the Municipal Corpora-
tion of Greater Mumbai (MCGM)’s Health Ward6
for predicting the COVID-19 status of patients to
help in preliminary diagnosis.

6.1 Demonstration Case

For the demonstration use case, apart from the
use cases outlined in the previous section, we can
choose a text classiﬁcation dataset and form regex
or continuous rules by observing few data points.
Once LFs are developed, we can easily compare
the LFs with any of the semi- and un-supervised
algorithms present in the package.

7 Conclusion and Future Work

SPEAR is a uniﬁed package for semi-supervised
data programming that quickly annotates training
data and train machine learning models. It eases
the use of developing LFs and label aggregation
approaches. This allows for better reproducibil-
ity, benchmarking and easier ML development in
low-resource settings such as textual post-editing.

5https://www.cse.iitb.ac.in/~ocr/
6https://colab.research.google.com/drive/

1tNUObqSDypUos7YNvnqvemALlkrrsB0z

sped  accelerated<Semantic context>… electrons …Pre-edit: Highly charged electrons are sped more.Post-edit: Highly charged electrons are accelerated more.LF1गमर्मीऊष्माSemantic context: ईंधन टैंकSource: उच्च मैक संख्या पर उड़ानों के लए, ईंधन टैंक उच्च गमर्मी का अनुभव कर सकता है।Target: उच्च मैक संख्या पर उड़ानों के लए, ईंधन टैंक उच्च ऊष्मा का अनुभव कर सकता है।LFsped  accelerated<Lexical right of keyword>… moreLF2SPEARnth LF Presently, we are integrating automatic LF induc-
tion approaches such as Snuba (Varma and Ré,
2018) that uses a small labeled set to induce LFs
automatically. This will signiﬁcantly increase the
scope of datasets without needing human interven-
tion in designing LFs. The package is written in
Python3 and open-sourced with a MIT License7
open for community contribution.

8 Acknowledgements

We thank anonymous reviewers for providing con-
structive feedback. Ayush Maheshwari is sup-
ported by a Fellowship from Ekal Foundation
(www.ekal.org). Ganesh Ramakrishnan is grate-
ful to IBM Research, India (speciﬁcally the IBM
AI Horizon Networks - IIT Bombay initiative) as
well as the IIT Bombay Institute Chair Professor-
ship for their support and sponsorship.

References

Abhijeet Awasthi, Sabyasachi Ghosh, Rasna Goyal,
and Sunita Sarawagi. 2020. Learning from rules
In 8th Interna-
generalizing labeled exemplars.
tional Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. OpenReview.net.

Stephen H Bach, Daniel Rodriguez, Yintao Liu, Chong
Luo, Haidong Shao, Cassandra Xia, Souvik Sen,
Alex Ratner, Braden Hancock, Houman Alborzi,
et al. 2019. Snorkel drybell: A case study in deploy-
ing weak supervision at industrial scale. In Proceed-
ings of the 2019 International Conference on Man-
agement of Data, pages 362–375.

Oishik Chatterjee, Ganesh Ramakrishnan, and Sunita
Sarawagi. 2020. Robust data programming with
precision-guided labeling functions. In AAAI.

Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard
Hovy, and Eric Xing. 2016. Harnessing deep
neural networks with logic rules. arXiv preprint
arXiv:1603.06318.

Pierre Lison, Jeremy Barnes, and Aliaksandr Hubin.
skweak: Weak supervision made easy for
2021.
NLP. In Proceedings of the 59th Annual Meeting of
the Association for Computational Linguistics and
the 11th International Joint Conference on Natu-
ral Language Processing: System Demonstrations,
pages 337–346, Online. Association for Computa-
tional Linguistics.

Ayush Maheshwari, Oishik Chatterjee, KrishnaTeja
Killamsetty, Rishabh K. Iyer, and Ganesh Ramakr-
ishnan. 2021.
Data programming using semi-
supervision and subset selection. In Proceedings of

7https://opensource.org/licenses/MIT

the 59th Annual Meeting of the Association for Com-
putational Linguistics.

Ayush Maheshwari, Krishnateja Killamsetty, Ganesh
Ramakrishnan, Rishabh Iyer, Marina Danilevsky,
and Lucian Popa. 2022a. Learning to robustly ag-
gregate labeling functions for semi-supervised data
In Findings of the Association for
programming.
Computational Linguistics: ACL 2022, pages 1188–
1202.

Ayush Maheshwari, Ajay Ravindran, Venkatapathy
Subramanian, Akshay Jalan, and Ganesh Ramakr-
ishnan. 2022b. Udaan – machine learning based
post-editing tool for document translation.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn:
Machine learning in python. the Journal of machine
Learning research, 12:2825–2830.

Alexander J Ratner, Stephen H Bach, Henry R Ehren-
berg, and Chris Ré. 2017. Snorkel: Fast training set
generation for information extraction. In Proceed-
ings of the 2017 ACM international conference on
management of data, pages 1683–1686.

Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel
Urtasun. 2018. Learning to reweight examples for
In International Conference
robust deep learning.
on Machine Learning, pages 4334–4343.

Atul Sahay, Anshul Nasery, Ayush Maheshwari,
Ganesh Ramakrishnan, and Rishabh Iyer. 2021.
Rule augmented unsupervised constituency parsing.
In Findings of the Association for Computational
Linguistics: ACL-IJCNLP 2021, pages 4923–4932.

Prithviraj Sen, Marina Danilevsky, Yunyao Li, Sid-
dhartha Brahma, Matthias Boehm, Laura Chiticariu,
and Rajasekar Krishnamurthy. 2020. Learning ex-
plainable linguistic expressions with neural induc-
tive logic programming for sentence classiﬁcation.
In Proceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 4211–4221.

Paroma Varma and Christopher Ré. 2018. Snuba: au-
tomating weak supervision to label training data. In
Proceedings of the VLDB Endowment. International
Conference on Very Large Data Bases, volume 12,
page 223. NIH Public Access.

Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yam-
ing Yang, Mao Yang, and Alexander Ratner. 2021.
Wrench: A comprehensive benchmark for weak su-
pervision. In Thirty-ﬁfth Conference on Neural In-
formation Processing Systems Datasets and Bench-
marks Track (Round 2).

