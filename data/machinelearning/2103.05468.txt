CNN-BASED SPOKEN TERM DETECTION AND LOCALIZATION WITHOUT DYNAMIC
PROGRAMMING

Tzeviya Sylvia Fuchs*, Yael Segal*, Joseph Keshet

Bar-Ilan University, Ramat Gan, Israel
{fuchstz, segalya, jkeshet}@cs.biu.ac.il

1
2
0
2

r
a

M
7

]
S
A
.
s
s
e
e
[

1
v
8
6
4
5
0
.
3
0
1
2
:
v
i
X
r
a

ABSTRACT

In this paper, we propose a spoken term detection algorithm for si-
multaneous prediction and localization of in-vocabulary and out-of-
vocabulary terms within an audio segment. The proposed algorithm
infers whether a term was uttered within a given speech signal or not
by predicting the word embeddings of various parts of the speech
signal and comparing them to the word embedding of the desired
term. The algorithm utilizes an existing embedding space for this
task and does not need to train a task-speciﬁc embedding space.
At inference the algorithm simultaneously predicts all possible lo-
cations of the target term and does not need dynamic programming
for optimal search. We evaluate our system on several spoken term
detection tasks on read speech corpora.

Index Terms— spoken term detection, event detection, speech

processing, convolutional neural networks

1. INTRODUCTION

Spoken term detection (STD) is the problem of determining whether
and where a target term or multi-term phrase has been uttered in a
speech recording. In this setting, the target term was not necessar-
ily given at training time. The STD problem could be deﬁned in
one of two setups.
In the ﬁrst setup, the term is given in a pho-
netic or grapheme form, and in the second setup, the term is given
in an acoustic form. The second setup is referred to as “query-by-
example” (QbE) [1, 2, 3].

Most methods for STD are based on dynamic programming to
solve the search optimization problem over an input signal or to al-
low duration variations of the target term using dynamic time warp-
ing [4, 5, 6]. This is done by breaking the signal into time seg-
ments and then recursively ﬁnding the optimal match of each seg-
ment. More recent works use various forms of deep neural networks
for the QbE problem. Some approaches deﬁne an audio-embedding
space such that if the term was uttered in the input audio example,
it would be projected to an embedding vector near the embedding of
the audio example [3, 7]. Speciﬁcally, in [7], they create a shared
embedding space for both audio data and the grapheme represen-
tation of terms. Other works use Siamese networks to create the
embedding space [8, 9, 10].

The focus of this work is to propose an alternative to the dy-
namic programming method by allowing a network to predict all
possible occurrences of a given term. The proposed network oper-
ates as a multi-dimensional regressor and predicts all occurrences
of the target term and its locations simultaneously. In that sense we
trade the time-complexity of the dynamic programming, which is of-

*These authors contributed equally to this work

ten polynomial in the segment duration and linear in the input signal
duration, with the size and the depth of the network.

This work extends the SpeechYOLO model [11]. The SpeechY-
OLO model identiﬁes acoustic objects in an audio signal similar to
the way that objects are detected in images [12, 13]. The model was
designed for the keyword spotting problem, where the goal was to
spot and locate a predeﬁned set of words that were given at train-
ing time. The novelty of this work is the extension of the keyword
spotter to handle terms which are not seen during the training phase.
The prediction of previously unseen terms is a challenging con-
cept. For images, it was proposed to identify unseen objects by their
similarity to the weighted sum of the conﬁdence values of previously
learned objects [14]. Such a superposition of visual objects is not
applicable to spoken objects, since terms in speech cannot generally
be considered as a weighted sum of other terms. Furthermore, the
detection of an unseen term is challenging as it is similar to segment-
ing a speech signal to separate terms in an unsupervised manner. We
overcome the challenge by deﬁning unseen terms by their phonetic
content and introducing them to the model using the phonetic em-
bedding of the term.

Our spoken term detection system, which we call Embed-
dingSpeechYOLO (ESY), is designed as follows: given a predeﬁned
embedding space, the model receives as input an audio ﬁle and a
term in its phonetical form, which is converted to its embedding
vector from the given embedding space. The ESY model then learns
to predict the correct term-segmentation of the audio ﬁle it received
as input. It simultaneously learns to predict the correct term embed-
ding for every term segment it ﬁnds. Finally, its decision of whether
or not the term appeared in the audio ﬁle depends on the cosine
similarity between the given term-embedding and the predicted one.
This paper is organized as follows. In Section 2 we formally
introduce the classiﬁcation and localization problem setting. We
present our proposed method in Section 3, and in Section 4 we
show experimental results and various applications of our proposed
method. Finally, concluding remarks and future directions are dis-
cussed in Section 5.

2. PROBLEM SETTING

In the STD task, we are provided with a speech utterance and a term.
The goal is to decide whether or not the term appeared within the
speech utterance, and estimate its location if it was. The term is pro-
vided as a sequence of phonemes. Although the training set contains
many different terms, the STD model should be able to detect any
term, not only those already seen in the training phase.

Formally, the speech signal is represented by a sequence of
acoustic feature vectors ¯x = (x1, . . . , xT ), where each feature
vector is D dimensional xt ∈ RD for all 1 ≤ t ≤ T . A sequence of

 
 
 
 
 
 
Lq phonemes of a term q is denoted as ¯pq = (p1, . . . , pLq ), where
pl ∈ P for all 1 ≤ l ≤ Lq and P is the set of phoneme symbols.
We denote by P ∗ the set of all ﬁnite length sequences over P.

We further deﬁne a pre-trained embedding function F : P ∗ →
RK , whose input is a sequence ¯pq that composes a certain term q.
The output of the embedding function is a K-dimensional feature
vector f . The term q is thus represented by the embedding vector f q.
We assume that N instances of term q were pronounced in the
utterance ¯x, where N ≥ 0. Each of these N events is deﬁned by
its phonetical embedding and its time location. Each such event e
start, tq
is deﬁned formally by the the tuple e = (f q, tq
end), where
q ∈ P ∗ is the actual term that was pronounced, and tq
start and tq
end
are its start and end times, respectively. Our goal is therefore to ﬁnd
all the events in an utterance, so that for each event the correct object
q is identiﬁed as well as its beginning and end times.

3. MODEL

We now describe our model formally. We assume that the input
utterance x is of a ﬁxed size T . We divide the input-time to C non-
overlapping equal sections called cells. Each cell is in charge of
detecting a single event (at most) in its time-span. That is, the i-th
cell, denoted ci, is in charge of the portion [tci , tci+1 − 1], where
tci is the start-time of the cell and tci+1 − 1 is its end-time, for
1 ≤ i ≤ C.

The cell is also in charge of localizing the detected event. The
localization is deﬁned relative to the cell’s boundaries. Speciﬁcally,
the location of the event is deﬁned by the time t ∈ [tci , tci+1 − 1],
which is the center of the event relative to the cell’s boundaries, and
∆t, the duration of the event. Note that ∆t can be longer than the
time-span of the cell. Using this notation the event span is [tci + t −
∆t/2, tci + t + ∆t/2].

In addition to localizing, each cell predicts an embedding vector
f (cid:48) that corresponds to the acoustic feature vectors appearing within
the boundaries of the predicted values t and ∆t. The goal is for the
predicted embedding vector to be an estimate of the true embedding
vector of the term q.

The inference is performed as follows. Given a speech signal ¯x
and a term represented as an embedding vector f q, the model pre-
dicts for each of the C cells: (i) a proposed embedding vector f (cid:48);
(ii) the center of the term t(cid:48) relative to the start-time of the cell; and
(iii) its span ∆t(cid:48). In order to determine whether a term q has been
uttered within the input audio segment, we use the cosine similarity
function. If cos(f q, f (cid:48)) is greater than a certain threshold φ, we say
that the term has occurred. The value of φ is tuned on the validation
set.

We conclude this section by describing the training procedure.
Our model, ESY, is implemented as a convolutional neural network.
The initial convolution layers of the network extract features from
the utterance. The output features of these layers, of size RM , are
concatenated to the input embedding vector of the term q and they
are both served as an input to a fully connected layer. The output of
this layer is a tensor of size C × (K + 2).

Each example in the training set is the tuple (¯x, f q, tq

end)
of the acoustic signal ¯x, the term represented by its embedding f q,
and its start and end time frames, tq
end. Denote by Θ the
set of parameters of the model. Denote by 1q
i the indicator that is 1 if
the term q was uttered within the cell ci, and 0 otherwise. Formally,

start and tq

start, tq

1q

i =

(cid:26) 1
0

tci ≤ tq ≤ tci+1 − 1
otherwise

,

(1)

where tq is deﬁned as the center of event q. When indicating that the
term is not in the cell we will use (1 − 1q

i ).

The loss function is deﬁned as a sum over several terms, each of
which takes into consideration a different aspect of the model. The
ﬁrst loss is in charge of improving the detection of the correct term.
In our setting this is manifested by maximizing the cosine similarity
between the predicted embedding and the given target embedding:

(cid:96)1(¯x, f q; Θ) =

C
(cid:88)

i=1

(cid:16)
1 − cos(f q, f (cid:48))

(cid:17)

,

1q
i

(2)

where f (cid:48) is the embedding predicted by the network using the pa-
rameters Θ. This loss strives to achieve a cosine value between the
embedding vectors which is close to 1.

The second loss function is focused on the case where the term
is not uttered in a speciﬁc cell; therefore it is aimed at minimizing
the similarity between the embedding vectors, for the cases where
the predicted embedding does not acoustically represent the target
term embedding:

(cid:96)2(¯x, f q; Θ) =

C
(cid:88)

(1 − 1q

i ) dissim(f q, f (cid:48)) ,

(3)

i=1

where dissim(e1, e2) is a dis-similarity function between the
embedding vectors e1 and e2. We evaluated three such functions,
namely, (i) dissim(e1, e2) = |0−cos(e1, e2)|, (ii) dissim(e1, e2) =
| − 1 − cos(e1, e2)|, or (iii) dissim(e1, e2) = |0 − cos2(e1, e2)|.
The third option is more gentle and slightly reduces the dissimilarity
value [15].

The last loss function is aimed at ﬁnding the exact localization
if the term is found in the speciﬁc cell. It has two terms: one is the
mean square error between the predicted and correct term center-
time relative to the corresponding cell, and the second is the mean
square error between the square-root of term durations:

(cid:96)3(¯x, tq

start, tq

end; Θ) =
(cid:20)
C
(cid:88)

1q
i

(cid:0)ti − t(cid:48)

i

(cid:1)2

+ λ(cid:48) (cid:16)√

∆ti − (cid:112)

∆t(cid:48)
i

(cid:17)2(cid:21)

,

(4)

i=1

where ti and ∆ti are computed from tq
cell timing tci . The values t(cid:48)
with the parameters Θ.

i and ∆t(cid:48)

start and tq

end relative to i-th
i are part of the network output

Overall the loss function is deﬁned as follows:

(cid:96)(¯x, f q, tq

start, tq

end; Θ) = λ1(cid:96)1(¯x, f q; Θ)
+ λ2(cid:96)2(¯x, f q; Θ) + λ3(cid:96)3(¯x, tq

start, tq

end; Θ) .

(5)

The λ values are used to assign different weights to the different
parts of the loss function.

4. EXPERIMENTS

We used a convolutional neural network that is similar to the VGG19
architecture [16].
It had 16 convolutional layers and a fully con-
nected layer, and the ﬁnal layer predicted both event embeddings
and coordinates. The model is described in detail in Fig. 1. It was
trained using Adam [17] and a learning rate of 10−3. We pretrained
our convolutional network using the Google Command dataset [18]
for predicting 30 predeﬁned terms. We later replaced the last linear
layer in order to perform prediction of our desired output. The size
of this new ﬁnal layer is C × (K + 2).

Acoustic-to-Text AP Acoustic-to-Acoustic AP

Siamese CNN [8]
Siamese LSTM [9]
Multi-View
ESY

0.897
0.835

0.549*
0.671*
0.806*

Table 1. AP measure for several single-word STD algorithms on
Switchboard test set. In the Acoustic-to-Text setup, the term is given
in textual form, whereas in the Acoustic-to-Acoustic setup, the term
is given as an acoustic signal. *results reported from paper.

4.2. STD with a single word setup

In this part a single word appears in every speech utterance. For these
experiments, we use the Switchboard dataset. Since every audio ﬁle
contains a single word, we set the number of cells C = 1. The
embedding size is set to be K = 1024. The loss’s λ values are
λ1 = 0.5, λ2 = 1 and λ3 = 2, and were tuned on the validation set.
We obtain the embedding function F by implementing [7] for
acoustic word embeddings. We refer to their work as Multi-View. In
their work, they train two LSTM networks; one receives an audio
signal, and the other receives a sequence of letters corresponding to
a term that has or has not been uttered within the audio signal. Their
goal is to bring the LSTMs’ outputs to produce embedding vectors.
They use a contrastive loss function, which is designed to make the
distance between the LSTMs’ outputs smaller if the input term has
been uttered in the audio signal, than if the term has not been uttered
in the audio signal. We denote the embedding vectors created by the
Multi-View algorithm as MV vectors. Thus, we assign Multi-View to
create our embedding space from which the embedding vectors f q
will be drawn - with one difference: we used phonemes to represent
our term, instead of letters. This is due to the fact that phonemes are
better than graphemes for representing the sound of a term. If there
were several ways to pronounce a term, we used the average of the
embedding vectors of all pronunciations.

We compare our work to an STD system implemented by [7],
using the aforementioned Multi-View algorithm. We also compare
to [8] and [9], in which the authors present an STD system for a
single word setup, using a Siamese CNN and a Siamese LSTM, re-
spectively. Results are shown in Table 1.

In [8] and [9], the reference term is an acoustic signal (an
acoustic-to-acoustic setup), as opposed to [7] and our work (an
acoustic-to-text setup).
In this context, we ﬁnd the acoustic-to-
acoustic setup to be very similar to the acoustic-to-text setup, since
in both cases the predictions are based on the embedding vectors’
similarity, and are independent of the reference term’s form.

As seen in Table 1, the Multi-View algorithm achieved better
results than those of the ESY algorithm. Note that ESY uses the
MV embedding vectors, each representing a single word, as its tar-
get. Therefore, the STD capabilities of ESY are bounded by those of
Multi-View, but are still sufﬁciently high. Furthermore, its AP values
are signiﬁcantly higher than those of [8] and [9].

4.3. STD with a multiple words setup

In this part several words appear in every speech utterance. For these
experiments, we use the Librispeech dataset. We set the number of
cells C = 3, to cover the possibility of several words occurring in
the speech signal. The embedding size is set to be K = 1024, as
before. The loss’s λ values are λ1 = 1, λ2 = 0.5 and λ3 = 3, and
were tuned on the validation set.

To obtain the embedding vectors of our chosen terms, we trained

Fig. 1. The detection network has 16 convolutional layers followed
by concatenating the output with a given term embedding and a lin-
ear layer. Every convolutional layer is followed by BatchNorm and
ReLU.

We divided our experiments into two parts: ﬁrst, we evaluated
ESY’s prediction abilities in a dataset where a single word was artic-
ulated in every speech utterance. We then extended our experiments
to speech utterances with more than one word. We evaluated our
experiments using the Average Precision (AP) and Maximum Term
Weight Value (MTWV) measures.

4.1. Datasets

In our experiments, we use the following three datasets:

The ﬁrst dataset is the Switchboard corpus of English conver-
sational telephone speech. We used the same setup as in [7]; the
training set consisted of word segments whose duration ranged from
50 to 200 frames (0.5 – 2 seconds). The train/dev/test splits con-
tained 9971/10966/11024 pairs of acoustic segments and character
sequences, corresponding to 1687/3918/3390 unique words. Mel-
frequency cepstral coefﬁcients (MFCC) features with ﬁrst and sec-
ond order derivatives were extracted and cepstral mean and variance
normalization was applied.

The second dataset we used is the LibriSpeech corpus [19],
which was derived from read audio books. The training set con-
sisted of 960 hours of speech. For the test set, we used test clean,
which has approximately 5 hours of speech. The audio ﬁles were
aligned to their given transcriptions using the Montreal Forced
Aligner (MFA) [20]. We used this dataset with two types of fea-
tures: MFCC and Short-Time Fourier Transform (STFT) features.
For the MFCC features, we use ﬁrst and second order derivatives for
the sound ﬁles, using the librosa package [21]. These features
were computed on a 25 ms window, with a shift of 10 ms. The
STFT features were computed on a 20 ms window, with a shift of 10
ms. We deﬁne a list of search terms as follows: we randomly chose
approximately 10,000 terms for training (excluding stop words). We
assigned 1,500 terms for the test set, half of which are In-Vocabulary
(IV) terms, i.e. they also appear in the training set, and half of which
are Out-Of-Vocabulary (OOV) terms, i.e. they do not appear in the
training set. We assign 1,500 terms for the validation set as well,
half of which are IV and half of which are OOV.

The third dataset we used is the TIMIT corpus [22]. We used this
dataset for testing purposes and not for training. For this corpus, we
used the MFCC features, with the aforementioned parameters. The
test set was composed of 80 terms that were suggested as a bench-
mark in [23]. For each term, we used all utterances in which the term
appeared, and chose another 80 utterances in which the term has not
appeared. The utterances were taken from the TIMIT test set.

6464224224conv1128128112conv225625625625656conv351251251251228conv451251251251214conv5Mconcat1K+2Cfc61K1Kdissim function

|0 − cos(e1, e2)|
| − 1 − cos(e1, e2)|
|0 − cos2(e1, e2)|

In-Vocabulary (IV) Out-Of-Vocabulary(OOV)
AP
AP
0.1
0.679
0.03
0.498
0.18
0.72

IOU
0.677
0.596
0.697

IOU
0.846
0.805
0.852

Table 2. AP and IOU values for ESY using the LibriSpeech dataset,
for three proposed dis-similarity functions.

the Multi-View algorithm on the LibriSpeech-360 dataset, which we
segmented into single words, thus creating MV vectors. We then
trained ESY on audio ﬁles that were 1.5 seconds long and containing
several words, while using these MV vectors as the representation f q
of the term q that was being searched for.

Table 2 presents the localization and prediction results of our al-
gorithm with the various versions of the dis-similarity function men-
tioned in Section 3. We measure localization using the Intersection-
Over-Union (IOU) metric, which has been calculated only on in-
stances that have been correctly detected. Our algorithm’s IOU val-
ues are exceptionally high in all settings. We ﬁnd that it performed
surprisingly well even on the OOV terms.

We measure our prediction ability using the AP metric. The re-
sults in Table 2 show that the |0 − cos2(e1, e2)| dis-similarity func-
tion gave the best results. We believe that this is due to its soft correc-
tion of mistakes. Note that the dis-similarity function is used in the
loss function when the term did not occur in the given cell. However,
we have no knowledge of what has been uttered in that cell; it may
have been a term acoustically similar to the search term. Therefore,
in such a case, a softer penalty is more appropriate.

To compare our algorithm’s performance, we adapted the Multi-
View STD system from [7] to match the multiple words setup. We
were inspired by previous works for QbE [24, 25]; the QbE algo-
rithm in [24] learned single-word embeddings using some embed-
ding function F ; for evaluation, it then used F to embed the term
and the full speech utterance (which consisted of several words). The
two embeddings were then compared in order to decide whether or
not the term occurred within the speech utterance. Accordingly, and
for fair comparison, we trained the Multi-View model to create MV
embedding vectors, using the single-word setup on the LibriSpeech
dataset. Then for evaluation, we used the same model to embed both
a single term and a corresponding full speech utterance (of length 1.5
seconds), and compared the two embeddings. As opposed to [24],
in our case the term is embedded by its phonetical form and not by
its acoustic form. We denote this setup as Multi-View-QbE, as it was
inspired by a QbE system.

Further, we used an additional version of the MV vectors, where
as opposed to the Multi-View-QbE setup, this time they were trained
in a multi-word setup. Speciﬁcally, they were trained on 1.5 second
long segments, where each segment could contain several words.
(This is different from their paper’s original setup [7], where the
algorithm searched for a term in an audio segment of a single word).
We denote this system as Multi-View-STD.

Results for the AP measure for both Multi-View-QbE and Multi-
View-STD are presented in Table 3. The ESY algorithm achieved best
results. The Multi-View-STD algorithm, which was trained specif-
ically on multi-word segments performed better than Multi-View-
QbE, which was trained on single-word segments. Notice that in this
case, ESY is not bounded by the performance of Multi-View since the
speech utterances contain more than one word. We further present
MTWV results for Multi-View-STD and ESY in Table 4. The ESY
algorithm once again achieved better results, this time when using
the STFT features.

Multi-View-QbE (MFCC)
Multi-View-STD (MFCC)
ESY (MFCC)
Multi-View-QbE (STFT)
Multi-View-STD (STFT)
ESY (STFT)

In-Vocabulary (IV)
0.003
0.245
0.72
0.002
0.09
0.71

Out-Of-Vocabulary (OOV)
0.003
0.06
0.18
0.003
0.028
0.17

Table 3. AP evaluation of ESY vs. Multi-View on LibriSpeech
dataset. ESY results used |0 − cos2(e1, e2)| dis-similarity function.

Multi-View-STD (MFCC)
ESY (MFCC)
Multi-View-STD (STFT)
ESY (STFT)

In-Vocabulary (IV)
0.010
0.330
0.000
0.348

Out-Of-Vocabulary (OOV)
0.005
0.136
0.000
0.142

Table 4. MTWV evaluation of ESY vs. Multi-View on LibriSpeech
dataset. ESY results used |0 − cos2(e1, e2)| dis-similarity function.

In our previous experiments, the dataset that the ESY algorithm
was tested on was from the same distribution as the dataset with
which we trained our embedding function F . Therefore, we tested
our model using the TIMIT dataset’s test set. The majority of the
TIMIT’s selected terms did not appear in the LibriSpeech training
set. We use the ESY model that has been trained on the LibriSpeech
dataset with MFCC features. We received an AP value of 0.606.
TIMIT’s results are better than those achieved on LibriSpeech in the
OOV setup; this could be explained by the fact that TIMIT’s dataset
is a cleaner dataset. Nevertheless, it proves that our model achieves
satisfactory results even when using a dataset that is from a distribu-
tion other than the one we trained on.

In addition, we compare ESY’s localization capabilities with
those of MFA. To do so, we train the MFA and ESY algorithm
on LibriSpeech, and test both models on the test set of the TIMIT
corpus. The forced aligner, MFA, uses a complete transcription of
the words uttered in the audio ﬁle to perform its alignments. This
is in contrast to the ESY algorithm, which receives no information
about the terms uttered within the given speech signal. Therefore,
due to the fact that MFA receives additional textual information, we
considered its localization ability to be an “upper bound” to ours.
The IOU measure was used to compare both algorithms’ output
alignments with TIMIT’s given word alignments. The IOU of ESY
on TIMIT was 0.73, while MFA achieved 0.9. Thus, we found that
ESY’s IOU value, while lower than MFA’s, was sufﬁciently high.

5. CONCLUSIONS

In this work we presented an end-to-end system for detection and
localization of textual terms. We evaluate our system on the spoken
term detection task for both single-word and multiple-words setups.
We further present results regarding localization. Future work in-
cludes improving the AP values for the OOV terms, and combining
the SpeechYOLO algorithm for IV terms with the ESY algorithm for
OOV terms.

6. ACKNOWLEDGEMENTS

T. S. Fuchs is sponsored by the Malag scholarship for outstanding
doctoral students in the high tech professions. Y. Segal is sponsored
by the Ministry of Science & Technology, Israel. The authors would
like to thank Sharadhi Alape Suryanarayana for helpful comments
on this paper.

[16] Karen Simonyan and Andrew Zisserman, “Very deep convo-
lutional networks for large-scale image recognition,” ” in In-
ternational Conference on Learning Representations (ICLR),
2015.

[17] Diederik P Kingma and Jimmy Ba, “Adam: A method for
3rd International Conference for

stochastic optimization,”
Learning Representations, San Diego, 2015.

[18] Pete Warden,

“Speech commands:
limited-vocabulary speech recognition,”
arXiv:1804.03209, 2018.

A dataset
for
arXiv preprint

[19] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev
Khudanpur, “Librispeech: an asr corpus based on public do-
main audio books,” in 2015 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP). IEEE,
2015, pp. 5206–5210.

[20] Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael
Wagner, and Morgan Sonderegger, “Montreal forced aligner:
Trainable text-speech alignment using kaldi.,” in Interspeech,
2017, pp. 498–502.

[21] Brian McFee, Matt McVicar, Stefan Balke, Vincent Lostanlen,
Carl Thom´e, Colin Raffel, Dana Lee, Kyungyun Lee, Oriol
Nieto, Frank Zalkow, and et al., “librosa/librosa: 0.6.3,” Feb
2019.

[22] John S Garofolo, Lori F Lamel, William M Fisher, Jonathan G
Fiscus, and David S Pallett, “Darpa timit acoustic-phonetic
continous speech corpus cd-rom. nist speech disc 1-1.1,” NASA
STI/Recon technical report n, vol. 93, 1993.

[23] J. Keshet, D. Grangier, and S. Bengio, “Discriminative key-
word spotting,” Speech Communication, vol. 51, no. 4, pp.
317–329, 2009.

[24] Shane Settle, Keith Levin, Herman Kamper, and Karen
Livescu, “Query-by-example search with discriminative neural
acoustic word embeddings,” Proc. Interspeech 2017, 2017.

[25] Keith Levin, Aren Jansen, and Benjamin Van Durme, “Seg-
mental acoustic indexing for zero resource keyword search,”
in 2015 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP). IEEE, 2015, pp. 5828–5832.

7. REFERENCES

[1] Dhananjay Ram, Lesly Miculicich Werlen,

and Herv´e
Bourlard, “Cnn based query by example spoken term detec-
tion.,” in Interspeech, 2018, pp. 92–96.

[2] Bolaji Yusuf, Batuhan Gundogdu, and Murat Saraclar, “Low
resource keyword search with synthesized crosslingual exem-
plars,” IEEE/ACM Transactions on Audio, Speech, and Lan-
guage Processing, vol. 27, no. 7, pp. 1126–1135, 2019.

[3] Dhananjay Ram, Lesly Miculicich, and Herv´e Bourlard, “Mul-
tilingual bottleneck features for query by example spoken term
detection,” IEEE Automatic Speech Recognition and Under-
standing Workshop, 2019.

[4] T. Fuchs and J. Keshet, “Spoken term detection automatically
adjusted for a given threshold,” IEEE Journal of Selected Top-
ics in Signal Processing, vol. 11, no. 8, pp. 1310–1317, 2017.

[5] Rohit Prabhavalkar, Joseph Keshet, Karen Livescu, and Eric
Fosler-Lussier,
“Discriminative spoken term detection with
limited data,” in Symposium on Machine Learning in Speech
and Language Processing, 2012.

[6] Bolaji Yusuf and Murat Saraclar, “An empirical evaluation of
dtw subsampling methods for keyword search,” Proc. Inter-
speech 2019, pp. 2673–2677, 2019.

[7] Wanjia He, Weiran Wang, and Karen Livescu, “Multi-view re-
current neural acoustic word embeddings,” Proc. ICLR, 2017.

[8] Herman Kamper, Weiran Wang, and Karen Livescu, “Deep
convolutional acoustic word embeddings using word-pair side
in 2016 IEEE International Conference on
information,”
Acoustics, Speech and Signal Processing (ICASSP). IEEE,
2016, pp. 4950–4954.

[9] Shane Settle and Karen Livescu,

“Discriminative acous-
tic word embeddings: Recurrent neural network-based ap-
proaches,” in 2016 IEEE Spoken Language Technology Work-
shop (SLT). IEEE, 2016, pp. 503–510.

[10] Ziwei Zhu, Zhiyong Wu, Runnan Li, Helen Meng, and Lian-
hong Cai, “Siamese recurrent auto-encoder representation for
in Interspeech,
query-by-example spoken term detection.,”
2018, pp. 102–106.

[11] Yael Segal, Tzeviya Sylvia Fuchs, and Joseph Keshet,
“Speechyolo: Detection and localization of speech objects,”
Proc. Interspeech 2019, pp. 4210–4214, 2019.

[12] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi, “You only look once: Uniﬁed, real-time object de-
tection,” in Proceedings of the IEEE conference on computer
vision and pattern recognition, 2016, pp. 779–788.

[13] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg,
“SSD: Single shot multibox detector,” in European conference
on computer vision. Springer, 2016, pp. 21–37.

[14] Berkan Demirel, Ramazan Gokberk Cinbis, and Nazli Ikizler-
Cinbis, “Zero-shot object detection by hybrid region embed-
ding,” in BMVC, 2018.

[15] Gabriel Synnaeve, Thomas Schatz, and Emmanuel Dupoux,
“Phonetics embedding learning with side information,” in 2014
IEEE Spoken Language Technology Workshop (SLT). IEEE,
2014, pp. 106–111.

