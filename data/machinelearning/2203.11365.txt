Noname manuscript No.
(will be inserted by the editor)

Towards a Change Taxonomy for Machine Learning
Pipelines

Empirical Study of ML Pipelines Related to Academic
Publications.

Aaditya Bhatia · Ellis E. Eghan ·
Manel Grichi · William G. Cavanagh ·
Zhen Ming (Jack) Jiang · Bram Adams

Received: date / Accepted: date

Abstract Machine Learning (ML) academic publications commonly provide
open-source implementations on GitHub, allowing their audience to replicate,
validate, or even extend machine learning algorithms, data sets and metadata.
However, thus far little is known about the degree of collaboration activity
happening on such ML research repositories, in particular regarding (1) the
degree to which such repositories receive contributions from forks, (2) the na-
ture of such contributions (i.e., the types of changes), and (3) the nature of

Aaditya Bhatia (Ph.D. at SAIL) the corresponding author and · Bram Adams (Director of
MCIS Lab)
Queen’s University
Kingston, ON, Canada
E-mail: {aaditya.bhatia, bram.adams}@queensu.ca

Ellis E. Eghan
Assistant Professor, University of Cape Coast, Ghana
E-mail: elliseghan@gmail.com

Manel Grichi
Lead Data Scientist, VibroSystM Inc., Montreal, Canada
E-mail: grichimanel@gmail.com

William G. Cavanagh
Student, Polytechnique Montreal, Canada
E-mail: william.glazer-cavanagh@polymtl.ca

Zhen Ming (Jack) Jiang
Associate Professor and director of SCALE Lab, York University, Canada
E-mail: zmjiang@cse.yorku.ca

Aaditya Bhatia is the corresponding author

2
2
0
2

p
e
S
2
1

]
E
S
.
s
c
[

2
v
5
6
3
1
1
.
3
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
2

Bhatia et. al.

changes that are not contributed back to forks, which might represent missed
opportunities. In this paper, we empirically study contributions to 1,346 ML
research repositories and their 67,369 forks, both quantitatively and quali-
tatively, by building on Hindle et al.’s seminal taxonomy of code changes.
We found that while ML research repositories are heavily forked, only 9% of
the forks made modiﬁcations to the forked repository. 42% of the latter sent
changes to the parent repositories, half of which (52%) were accepted by the
parent repositories. Our qualitative analysis on 539 contributed and 378 local
(fork-only) changes extends Hindle et al.’s taxonomy with two new top-level
change categories related to ML (Data and Dependency Management), and
15 new sub-categories, including nine ML-speciﬁc ones (input data, param-
eter tuning, pre-processing, training infrastructure, model structure, pipeline
performance, sharing, validation infrastructure, and output data)). While the
changes that are not contributed back by the forks mostly concern domain-
speciﬁc features and local experimentation (e.g., parameter tuning), the origin
repositories do miss out on a non-negligible 15.4% of Documentation changes,
13.6% of Feature changes and 11.4% of Bug ﬁx changes.

Keywords Machine Learning · Change Taxonomy · GitHub Collaborations ·
Contribution Management

1 Introduction

The notion of “Software Engineering for Machine Learning/Artiﬁcial Intelli-
gence” (SE4ML/SE4AI) is becoming widespread in the software engineering
community, with software engineering conferences featuring dedicated tracks
and with dedicated venues appearing (RAISE, SEMLA, CAIN). The term
“Machine learning software” can mean diﬀerent things depending on the con-
text, spanning across a wide range of software projects from project-speciﬁc
applications to third-party ML frameworks, to ML pipelines using such frame-
works.

At one end of the spectrum, Machine learning (ML) frameworks like
Tensorﬂow1 and PyTorch2 provide generic implementations of ML classiﬁca-
tion, regression, recommendation, and clustering algorithms, for use in any
possible domain. At another end of the spectrum, end user applications
integrate models into their code base to make domain-speciﬁc predictions.
Since those models are domain-speciﬁc, an infrastructure is needed to con-
tinuously ingest data, perform pre-processing, build, tune and evaluate ML
models speciﬁc to a given domain (e.g., image recognition or fraud detection),
by orchestrating scripts and ML framework tools that produce datasets, mod-
els and evaluation/execution metadata [1]. This infrastructure is called an ML
pipeline [2,3], and forms the core of any organization’s ML activities, catering
to interdisciplinary teams of data scientists, data engineers, and developers [4].

1 https://www.tensorflow.org
2 https://pytorch.org

Towards a Change Taxonomy for Machine Learning Pipelines

3

An important subset of ML pipelines is produced in the context of pub-
lished academic papers [5]. Typically, researchers would upload a preprint of
their work on a repository like ArXiv, including a GitHub repository with the
pipeline code and (potentially) a labeled data set to train the pipeline on.
Alternatively, other researchers or open-source developers might open-source
an implementation or dataset of such a paper. Such code and data allow the
open-source community to leverage state-of-the-art ML research to develop
applications and beneﬁt from the publications’ ideas. Popular online indexes
like PapersWithCode3 or ModelDepot4 provide searchable lists of papers, and
their associated artifacts like implementations and/or datasets. Given the pop-
ularity5 of these ML pipeline open-source projects, the remainder of this paper
focuses on this subset of ML pipeline projects.

While ML pipelines, including those related to an academic publication,
typically are shared in the form of GitHub repositories6, an important ques-
tion is to what extent such projects beneﬁt from the open-source collaboration
models leveraged by traditional (non-AI) GitHub projects, as opposed to just
being an “online backup” or “replication package”. The typical GitHub col-
laborative coding model would see the OSS community fork an ML research
project [6], make changes to the source code, and push those changes back
to the original project using Pull Requests (PRs). The developers of the orig-
inal project would then check such PRs and accept or reject the proposed
changes. Accepted PRs would then lead to code changes being merged in the
ML research repository, a so-called upstream change. Oftentimes, community
developers could also make changes for their own use that they would not con-
tribute back, i.e., so-called downstream changes. GitHub’s collaborative forking
model has been known to improve the productivity of multifaceted software
development and management tasks like making new features, handling is-
sues, sharing knowledge, adding documentation, and managing upstream and
downstream code [6].

While OSS collaborations in non-ML software are extensively studied by
researchers [7,8,9,10,6], this is not the case in the context of ML. Certain
assumptions of established software engineering activities like requirements
engineering, software design, and quality assurance are no longer valid [11,
12], while a typical ML development team no longer only features traditional
developer and tester roles, but also data scientists and data engineers [2]. Such
multi-disciplinary collaboration leads to a wide range of diﬀerent artifacts
other than source code that require proper versioning and traceability with
the code [1].

Among the emerging software engineering practices replacing existing mod-
els of (multi-disciplinary) collaboration in the context of SE4ML, the types of

3 https://paperswithcode.com
4 https://modeldepot.io/
5 In August 2022, PapersWithCode indexed more than 77,640 academic AI publications

along with their code bases.

6 In the remainder of this paper, we use the term “ML research repositories” to identify

such ML pipelines.

4

Bhatia et. al.

code changes made on typical ML pipelines need to be explored to capture the
nature of such community-based changes and compare them to pre-ML types
of changes. In particular, in 2008, Hindle et al. [13] presented a seminal taxon-
omy of code changes for traditional (non-ML) software, which identiﬁed seven
code change dimensions, along with 24 sub-categories of changes (Table 1).
Despite being 13 years old at the time of writing this paper, the taxonomy is
still authoritative today. However, the advent of the ML era within the col-
laborative development environment calls for the need to substantially revise
this taxonomy.

Hence, this paper empirically studies changes in ML research reposito-
ries and their forks, using a mixed-methods approach. First, we quantitatively
mine the community collaborations to 1,346 ML research repositories (contain-
ing implementations of 1,144 arXiv publications) obtained via ModelDepot’s
“Deep Search” engine to analyze the behavior involving their forks, i.e., how
active is online collaboration on ML research repositories? We then perform a
large-scale qualitative analysis of 539 upstream and 378 downstream changes,
adapting Hindle et al. [13]’s taxonomy of code changes. Notably, we address
the following research questions:

– RQ1: To what extent do ML research repositories form the basis

of other contributors’ work?
ML research repositories are heavily and transitively forked, yet overall
only 9% of forks made modiﬁcations. 41.6% of the latter forks sent changes
back to the parent ML research repositories (i.e., upstream changes), half
of which (52%) were accepted by the parent repositories. The time taken to
merge those pull requests is about four times faster than for NPM packages
on GitHub [14] and 24 times faster than for GitHub projects in general [15].
– RQ2: What are the types of changes in ML research repositories?
Using the seminal code change taxonomy of Hindle et al. [13] as a starting
point, we identiﬁed two new categories of changes in ML research repos-
itories, namely Data, and Dependency Management. Furthermore, we re-
ﬁned the taxonomy with 16 new sub-categories of changes. Nine of the
sub-categories (i.e., input data, parameter tuning, pre-processing, training
infrastructure, model structure, pipeline performance, sharing, validation
infrastructure, and output data) are ML-speciﬁc, while seven (i.e., add de-
pendency, remove dependency, update dependency, ﬁle permissions, inter-
nal documentation, add auto-generated code, and program-metadata) are
more general sub-categories.

– RQ3: How do downstream changes diﬀer from upstream changes

in ML research repositories?
Manual comparison of changes contributed back by the forks to the ori-
gin ML repository (upstream changes) with changes not contributed back
(downstream changes) shows that downstream changes typically are domain-
oriented and add input/output data, perform parameter tuning, add new
functional features, and perform other non-functional changes like inden-
tation, refactoring or cleaning up the source code. In contrast to this,

Towards a Change Taxonomy for Machine Learning Pipelines

5

upstream changes beneﬁt the parent repository by updating dependencies
or ﬁxing bugs for the parent repository. Both downstream and upstream
contributions add documentation, and ﬁx bugs.

The remainder of the paper is organized as follows: Section 3 presents the
data collection and design of our study. Section 2 summarises the related
literature. Section 4 discusses the motivation, approach, and results for each of
our research questions. Section 6 discusses threats to the validity of our study
while Section 5 explains the implications of our ﬁndings. Finally, Section 7
concludes the paper.

2 Related Work

2.1 Code Change Classiﬁcation

Prior work in code change taxonomies initiated in 1976 with Swanson et al.’s
work on identifying changes during software maintenance in terms of correc-
tive, adaptive and perfective [16]. The goal of such taxonomies originated from
the need to enhance software decision-making.

These changes were adopted as extended-Swanson categories by Hindle et
al. [13] in the latter seminal taxonomy of software changes in 2008. A detailed
description of Hindle et al.’s taxonomy is provided in Table 1. Despite its
important role, the taxonomy is in need of updates. For instance, software
development has become more collaborative since 2008, due to platforms like
GitHub, leading to additional change types that would need to be added to
the taxonomy in Table 1. Furthermore, the types of changes required in an
ML setting like ML pipelines could lead to further missing change types, which
this paper aims to study. Hence, our qualitative study builds on Hindle et al.’s
change taxonomy, extending it with two new high-level change categories and
15 new sub-categories of changes.

Later work shifted direction from establishing taxonomies to automated
classiﬁcation of code changes in terms of activities deﬁned by such change
taxonomies. In Hindle et al.’s [17] later publication, the authors automatically
classify maintenance changes into corrective, adaptive, perfective, feature addi-
tion, and non-functional improvement categories using ML techniques. Yan et
al. [18] improved this approach of classifying code changes using a Discrimina-
tive Probability Latent Semantic Analysis (DPLSA) approach, which showed
its beneﬁts in multi-category classiﬁcations of code change activities during
the evolution of software. Recently, in 2021, Ghadab et al. [19] further impro-
vised the classiﬁcation of code changes using BERT (Bidirectional Encoder
Representations from Transformers) approach.

Code change taxonomies are used for a variety of purposes. In 2009, Ben-
estad et al. [20] performed a literature survey on publications that assessed
the impact of individual code changes on the maintenance and evolution of
software systems. Wu et al. [21] extracted missing links between bugs and

6

Bhatia et. al.

Table 1: Hindle et al.’s taxonomy of change types for traditional SE [13].

Testing
Internationalization Adding language support other than English
Refactor

Category

Sub-Category
Bug ﬁx

Maintenance

Cross
Maintenance

Parameter
change
Debug

Documentation

list

Meta-Program

Build/Conﬁg

Non-Functional
Source Code
Change

Source
Management

Clean up

Indent
Token replace
Merge
Source control
Versioning
Branching
External

Implementation

Feature
Platform

Module
Management

Legal

Add module
Move module
Remove module
Licence

Deﬁnition
Fixing bugs (e.g., adding exception control, conditional state-
ments)
Cross-cutting changes (e.g., logging)
Performing activities during maintenance cycle other than ﬁx-
ing bugs
Updating in the parameters list

Setting up debug, tracking process (e.g., printing variable val-
ues, execution times)
Changing the software documentation (e.g., read-me ﬁle, code
comments)
Changing build or work-space conﬁguration ﬁles
setup.txt or .yml)
Adding unit tests, bench-marking, changing test environment

(e.g.,

Structural changes without changing the behavior (e.g., re-
naming variables, optimizing code)
Deleting code not used by the program (e.g., print statements,
comments, unused imports)
Adding proper indentation or formatting the code
Renaming tokens like variable or method names
Merging commits or pull requests
Managing repository ﬁles (e.g, adding ﬁles to git ignore)
Changing the software release version
Creating a side development branch from the main branch
Code submitted by developers who are not a part of the core
team
Adding new functional features
Changing hardware or platform-speciﬁc code (e.g., changing
GPU hardware acceleration, changing ﬁle access for a new
platform)
Adding modules/directories/ﬁles
Moving modules/directories/ﬁles
Removing modules/directories/ﬁles
Changing copyright or authorship

committed changes by creating an automated tool, Relink. Furthermore, Bis-
syand´e et al.[22] evaluated the eﬃcacy of linking bug reports to code changes by
benchmarking Relink against alternative bug-linking solutions. Cort´es-Coy et
al. [23] used code changes to automatically generate commit messages. Farago
et al. [24] studied code changes to understand the impact of change operations
like add/update/delete on ISO/IEC-9126 quality attributes of the software.
Software developers and researchers developing such tools may wish to incul-
cate our extended taxonomy of code changes to better support the development
of ML systems.

Towards a Change Taxonomy for Machine Learning Pipelines

7

2.2 Multi-repository software development via Forking

Many researchers study collaborative development. For instance, Zhou et al.[7]
identiﬁed eﬃcient practices for developers collaborating using forks. The au-
thors build regression models to correlate eﬃcient practices with respect to the
behavior around forking. They found how the modularity of a code base and
its contributions, as well as upfront management of which bugs require ﬁxing
by contributors, correlate with higher contribution volume and pull request
acceptance.

Later, in 2020, in a follow-up work [6], Zhou et al. elucidated the percep-
tions around “hard forks” (forks that split development into a competing line
of a new repository), against those of “social forks” (forks that create a public
copy of the repository on a social website like BitBucket or GitHub). While
hard forking traditionally has been considered a bad practice for developers
and users [25], the authors found that the perceptions around hard forking have
changed in modern times. Nowadays, hard forks emerge out of social forks, and
are seen as a positive non-competitive alternative to the original repository.
Constantino et al. [26] identiﬁed the rationales, processes, and challenges be-
hind collaborative activities on GitHub by conducting surveys. The authors
found that GitHub collaborations contribute to software development, issue
management, repository management, and documentation tasks.

Brisson et al. [27] studied collaborations on GitHub projects by analyzing
transitive forks, user statistics, pull requests, and issues. Furthermore, Biazzini
et al. [8] identiﬁed dispersion metrics for fork-induced code changes. Ren et
al. [28] developed a web UI for the management of forking-based collaborations
with features like fork searching and tagging. Other research [29,30] studies the
nature of upstream contributions in the form of Pull Requests and identiﬁes
the nature of competing contributions.

However, none of this prior research studies the collaborative development
of ML software. We build on the results from prior studies to compare the
forking dynamics of ML research repositories.

2.3 Software engineering for machine learning

ML systems are substantially diﬀerent from traditional software systems and
hence need dedicated research. For example, Washizaki et al. found that ML
software engineering design patterns diﬀer from those of non-ML software [11].
Furthermore, in 2020, Ozkaya [12] illustrated how the stochastic nature of ML
changes the software development practices in ML. Overall, Mart´ınez et al. [31]
performed a literature review on Software Engineering for AI-based systems.
Recent widespread advances in ML have instigated researchers to study the
maintenance activities and challenges in ML code. In 2019, Amershi et al. [2]
uncovered the challenges in managing ML software at Microsoft, in particu-
lar identifying the typical ML pipeline and corresponding software activities.
Furthermore, Zhang et al. [32] and Arpteg et al. [33] studied the software

8

Bhatia et. al.

challenges faced in deep learning applications. Sambasivan et al. [34] identi-
ﬁed data engineering challenges for which multiple roles of data engineers (like
data collectors, annotators, ML developers, and data licensing teams) require
powerful data infrastructure in order to support machine learning processes.
In the context of the data lifecycle used for ML, Polyzotis et al. [35] illustrated
the challenges faced at Google. O’Leary and Uchida [36] also studied problems
with creating ML pipelines from existing code at Google.

The work of Fan et al. [5] is the closest to our paper. While the authors
study a similar dataset as ours (i.e., 1,149 academic ML(AI) repositories refer-
encing ArXiv publications), the authors focus on characterizing popular versus
unpopular academic repositories in terms of the number of stars on GitHub,
and analyzing factors correlations between the number of paper citations and
GitHub repository metrics. However, our paper is the ﬁrst to study the extent
of actual OSS collaborations happening on ML research repositories (instead
of paper activity based on those repositories), and to manually identify the
nature of code changes performed on such ML pipeline projects.

3 Data Collection and Experiment Setup

Fig. 1: Data collection and processing steps.

Figure 1 presents an overview of our data collection procedure along with
the design of our empirical study to address the research questions of the
introduction. RQ1 quantitatively studies the OSS collaboration characteristics
on ML research repositories, while RQ2 and RQ3 perform a qualitative analysis
on the nature of changes performed during this collaboration.

ModelDepot.io"ArXiv" referencefilter1,346 ML repositoriesData  CollectionQuantitative Analysis of ML PipelinesRepository  Stats Analyzer6,069 Non-Trivial Forks67,369 Repository ForksQualitative Analysis of ML Pipelines95% ConfidenceInterval 5% Confidence  Level378DownstreamCommitsFork Stats Analyzer207 PRshaving 539UpstreamCommits2,509 Pull  RequestsTop 5%Repositories With MaximumNon-TrivialForks 445 Pull  Requests10817DownstreamCommitsTowards a Change Taxonomy for Machine Learning Pipelines

9

3.1 Data Collection

ModelDepot was a popular online model store containing 1) a catalogue of
pre-trained ML models, and 2) a GitHub search engine for ML model pipeline
implementations called “Deep Search”. The latter engine eﬀectively was an
index of GitHub projects related to ML, allowing to search the projects based
on name, ML framework (e.g., Tensorﬂow), programming language, and model
category (i.e., “computer vision”, “natural language”, “reinforcement learn-
ing”, “generative” and “audio”). At the time this study was conducted in
Summer 2019, ModelDepot indexed over 50,000 ML implementations.

While both ModelDepot and its major competitor at that time, Paper-
sWithCode, indexed GitHub repositories, we selected ModelDepot for our
study because it was the most popular and diverse at the time of crawling the
data (i.e., 50,000 model implementations compared to 8,500 on paperswith-
code.com7). The fact that ModelDepot did not require manual contributions
to register new models, but leveraged its automated “Deep Search” engine
to track new ML model repositories, was another reason why we opted for
ModelDepot. To collect the ModelDepot data, we built a scraper (crawler) to
mine ML projects in the “Computer Vision” and “Machine Learning” cate-
gories, which were the two most common categories of models8. After sorting
by the search engine’s “best match” feature, we then focused on the top 5,000
non-fork projects.

Since this paper focuses on the evolution of ML model pipelines produced
by researchers or inspired by the work of researchers, we ﬁltered the 5,000
crawled projects using string matching to check the readme ﬁles for the pres-
ence of the term “arxiv” (referring to an ArXiv URL of an academic paper).
This yielded 1,346 ML research repositories as our dataset for this study.
Within this dataset, 1,144 unique academic papers were referenced and 23%
of these publications were referenced more than once. One of these publica-
tions, “Deep Residual for Image Recognition”9 was referenced the most (46)
by the repositories in our dataset.

To further verify the soundness of our dataset, we did a separate analysis
to check the quality of the studied ArXiv publications. To do so, from our
dataset of 1,346 ML research repositories, we used a 95% conﬁdence interval
and a 10% conﬁdence interval to obtain a sample of 94 repositories. For each
of the 94 repositories, we checked whether their referenced ArXiv papers:

– were peer-reviewed?
– involved authors from the industry?
– had any of the authors amongst the people contributing to the repository’s

development?

7 https://twitter.com/paperswithcode/status/1091315540092768257
8 https://web.archive.org/web/20190404211946/https://modeldepot.io/search/

results?q=

9 https://arxiv.org/abs/1512.03385

10

Bhatia et. al.

To check whether publications were peer-reviewed, we looked at whether
the ArXiv paper was also published at another venue, by leveraging scholar.
google.com. Next, to check whether the authors were from the industry, we
looked at the email addresses and the designations of the authors presented
in the publication. Finally, we check whether the authors contributed to the
repository.

As such, we found that all repositories from our inspected sample of 94
repositories are implementations of academic ML research. The ArXiv pa-
per(s) referenced in the repositories’ README ﬁle clearly showed the inten-
tion to implement the published algorithms, not just mentioning the work as
a comparison or reference. Only 30% of the repositories were developed
by the referenced publications’ authors themselves, while the other
other 70% were implemented by other researchers or by open-source develop-
ers. We observed that the latter repositories basically referenced the paper(s)
they were implementing. Since those repositories still represent an implemen-
tation of an ML publication shared with the open-source community, all of
these represent genuine ML pipeline research repositories that we focus on in
our research.

For instance, a repository named darkﬂow10 referenced two academic pub-
lications1112 to indicate that they implemented the image processing algorithm
YOLO, proposed by said research. Another repository, ActivityRecognition13
presented multiple publications141516 within the “Reference Papers” section
of its ReadMe, for the same reason. While both repositories have no indication
that one of the publications’ authors was a contributor, the repositories clearly
indicate that they implemented the referenced academic research.

Furthermore, 81% of the repositories implemented peer-reviewed
publications, indicating that the implemented research is high quality. Fi-
nally, 59% of the repositories referenced publications involving au-
thors from the industry, indicating how repository development focuses on
industry-relevant research.

3.2 Quantitative Analysis of Forking in ML Research Repositories (RQ1)

To analyze the dynamics of OSS collaboration through forks of ML research
repositories (RQ1), we use the GitHub Search API17 to analyze each of the
1,346 repositories in our dataset. We perform our quantitative analysis via

10 https://github.com/thtrieu/darkflow
11 https://arxiv.org/pdf/1506.02640.pdf
12 https://arxiv.org/pdf/1612.08242.pdf
13 https://github.com/mohammed-elkomy/two-stream-action-recognition
14 https://arxiv.org/pdf/1406.2199.pdf
15 https://arxiv.org/pdf/1604.07669.pdf
16 https://arxiv.org/pdf/1507.02159.pdf
17 https://docs.github.com/en/rest/reference/search

Towards a Change Taxonomy for Machine Learning Pipelines

11

Table 2: Metrics used in RQ1. The bolded metrics are adapted from Brisson
et al. [27]. The abbreviation “repo” is used for “repository”.

Deﬁnition
# users who starred the repo
# forks created from the repo

Metric
Star#
Fork#
Non-empty forks # forks where the forked code base is modiﬁed
PR#
# PRs sent to the parent repo by its forks
PR Accept%
(# PRs merged into parent repo) / (PR#)
# Days between creation of a repo and its ﬁrst fork
First Fork Time
Final Fork Time # Days between creation of a repo and its ﬁnal fork

eight metrics related to forking as described in Table 2. Four out of these
metrics (the bolded ones in Table 2) are adapted from Brisson et al. [27].

Given a large number of forks, Brisson et al. [27] suggested that fork data
is noisy. For this reason, we introduce the concept of non-empty forks to
identify forks with modiﬁcations. Such non-empty forks contain at least one
commit that does not occur in the parent repository (downstream changes), or
contributed back at least one commit via a pull request (upstream changes).

To identify forks with downstream changes, we ﬁrst calculate for each fork
F the set of commits SF whose commit id does not occur in the parent repos-
itory. Since the resulting set SF of fork commits could still contain commits
that have been merged upstream through rebasing (changing their commit id),
we then check for each commit in SF to see if any commit in the parent repos-
itory has the same commit message subject, author name, and author date,
since those metadata ﬁelds have been found to be stable during rebase [37].
If so, we remove those commits from SF , since they also exist in the parent
repository. Since we may be missing cases where a fork had all of its commits
merged as PRs, we then check the list of forks that submitted PRs using the
GitHub Search API, and add such forks into SF . If the resulting SF is not
empty, we consider each F in SF to be a non-empty fork.

We used the Star# and Fork# metrics to measure the popularity of repos-
itories, as indicated by Borges et al. [38]. We computed the First Fork time
and Final fork time metrics to indicate the temporal aspects of ML research
repositories. The First Fork time indicates the speed of the OSS community
in adapting an ML implementation, whereas the ﬁnal fork time indicates the
longevity of collaboration activities on ML pipeline code.

In addition, we calculate for each repository the number of forks, both di-
rect and transitive, as a measure of the value of these repositories for the OSS
community in terms of collaborative potential. We call the direct forks of a
repository level-one transitivity, while a transitivity of level-two indicates the
transitive forks of the direct (level-one) forks, and so on. The higher the pro-
portion of repositories with at least one level-two fork, the more collaboration
seems to happen.

12

Bhatia et. al.

3.3 Qualitative Analysis of Change Types in Forks (RQ2/RQ3)

Since ML software has obtained a prominent place in software engineering, and
the nature of the machine learning software lifecycle is substantially diﬀerent
from that of traditional software [2], one would expect further change types
to be added. ML practitioners use data pre-processing techniques, iteratively
tune model hyperparameters for obtaining the most optimal ML model under
the data science life cycle. Reusing ML software requires users to understand
the rationale behind the ML implementations, instigating users to change their
code for documenting the internal working of the ML code. All of this has led
to a variety of types of artifacts other than source code that require changes as
well [1]. In this study, we build on Hindle et al.’s taxonomy to identify the types
of changes in ML research repositories by studying a statistically signiﬁcant
random sample of 1) fork PRs merged by the parent ML research repository
(i.e., upstream changes) and 2) commits within the forked repositories that
were not submitted as PRs (i.e., downstream changes). We describe our
qualitative analysis process below:

1. Selection of repositories for sampling.

Since not all forked repositories made changes, let alone sent them as PRs
upstream, we ﬁrst select the repositories having the most active forks. To
do so, we ordered the repositories by our metric non-empty forks and
obtained the top 5 percentile, i.e., 23 repositories. Overall, these 23 repos-
itories had 10,817 downstream commits and 445 PRs. From these, using a
95% conﬁdence level and 5% conﬁdence interval, we obtain a statistically
representative sample of 1) 207 PRs containing 539 upstream commits
(upstream sample)18, and 2) 378 downstream commits (Downstream
Sample).
Both samples were stratiﬁed, such that repositories with a higher propor-
tion of non-empty forks had more data points in the samples. Since a PR
can comprise multiple commits, we selected all the commits for the sam-
ple of 207 PRs and obtained 539 upstream commits. Similar to obtaining
the stratiﬁed upstream sample, we used the proportion of #Commits with
respect to the non-empty forks of each of the 23 parent repositories for
creating the stratiﬁed downstream sample.

2. Study Participants. We used teams from two universities to manu-
ally classify the types of changes in upstream and downstream commits.
University-A classiﬁed downstream changes while University-B classi-
ﬁed upstream changes. Both teams included two or more grad students,
and one faculty member, all having knowledge of ML and non-ML soft-
ware design and development. Due to the large-scale nature of our study,
we employed four coders in team A and three coders in Team B.

3. Extending taxonomy of changes. From the sample of 378 downstream
commits, Team-A ﬁrst performed a pilot study on an initial sample of 78
commits to validate the extent to which Hindle’s taxonomy [13] was able

18 PRs on GitHub are not limited to just one commit.

Towards a Change Taxonomy for Machine Learning Pipelines

13

to classify code changes or required reﬁnements. The 78 commits were
distributed across the three coders of Team-A such that each commit had
two coders and each coder had 26 commits in common with each of the
other coders. The assignment of commits to each coder was anonymous.
Each coder then individually coded their 52 (2 x 26) assigned commits,
identifying all types of changes within the commits under study (more than
one type of change could apply). In cases where the change (sub-)type could
not be found using Hindle’s change categories, the coders individually could
create a new category. Once ﬁnished, the coders met online discussing only
the new types of changes that they had identiﬁed, without considering the
speciﬁc commits tagged with these new types. The proposed new types
could be merged, renamed, or removed until a consensus was reached.
Team-A then re-labeled their samples using the enhanced Hindle’s taxon-
omy. Once done, the coding results were combined into a spreadsheet.
In the ﬁrst phase, each coder had to check the commits they were as-
signed that had conﬂicting coding results. This was done asynchronously
by adding comments on the spreadsheet. If a coder was in accord with the
other coder’s interpretation, a disagreement was resolved; otherwise, it was
left open. In a second phase, the remaining disagreement cases were then
discussed in person by Team-A, possibly reﬁning the taxonomy.
With this ﬁnal version of the taxonomy, Team-A started labeling the re-
mainder of its samples, using the same style of assignment as for the initial
78 (i.e., anonymously sharing the same number of commits with each other
coder). In parallel, Team-B was assigned the 207 PRs in a similar manner.
While both teams could still make changes to the taxonomy during this
coding, we observed saturation in the labels after tagging the initial set
of 78 downstream commits, i.e., no new (sub-)categories were identiﬁed in
the later part of labeling the 300 downstream samples and 539 upstream
samples.

4. Calculation of inter-rater agreement. Once coding was ﬁnished, both
teams individually used the spreadsheet-based and in-person resolution of
disagreements used initially for the ﬁrst 78 cases. To calculate inter-rater
agreement, since each sample was rated by two participants, we use Krip-
pendoﬀ’s Alpha [39] as our metric for the inter-rater agreement. This metric
supports multi-label classiﬁcation by multiple participants. A similar ap-
proach of obtaining inter-rater agreement in a multi-rater setting was used
by Heng et al. [40] to manually tag logging data, and Salza et al. [41] to
classify mobile app updates.
Across the three coding activities (78 and 300 commits for Team-A, and 207
PRs for Team-B), the teams reported high agreements with a Krippendoﬀ’s
α=98% on the sample of 378 downstream changes and a Krippendoﬀ’s
α=92% for the sample of 539 upstream changes. These values of inter-
rater agreements are high and reﬂect the statistical robustness of our data
labeling results.

14

Bhatia et. al.

Given that the ﬁnal change taxonomy spans 39 change sub-categories, and
that we coded 378 downstream commits and 207 upstream PRs (containing
539 commits) across two teams of seven coders (and two universities), the
resulting empirical study was non-empty. For instance, in the downstream
change19 performed by fork “BoseAslCohort” for the project “Youtube-8m”,
the authors manually inspected changes for 27 changed ﬁles, which included
11,755 code additions. Overall, it took an estimated six man-months to ﬁnish
the qualitative study.

3.4 Replication Package

All the scripts along with the mined data are provided in the replication pack-
age20

4 Case Studies

4.1 RQ1: To what extent do ML research repositories form the basis of other
contributors’ work?

Motivation. Currently, there is no empirical evidence regarding the extent to
which 1) open-sourcing ML research code helps the OSS community in building
new applications and 2) the OSS community contributes and helps maintain
the original ML research implementations. In contrast, for non-ML software,
prior research [7,8,27,29,30] has studied the nature of multi-repository devel-
opment and maintenance of OSS projects. Hence, in this RQ, we analyze the
OSS development activities around research-based ML pipeline repositories.
Approach. As discussed in Section 3, we extract 1,346 GitHub repositories
having references to machine learning ArXiv publications, then use the GitHub
Search API21 to obtain the metrics identiﬁed in Table 2.

Results.

4.1.1 Non-Empty Forks

Only 9% of forks of the ML research repositories have modiﬁcations
to the forked source code (i.e., have non-empty Forks). Overall, 82.5%
of the 1,346 repositories had forks. Figure 2 shows the cumulative percentage
of those repositories with a fork having at least one Non-Empty Forks. Since
51.6% of the repositories do not have any fork modiﬁcation (only empty forks),
and the slope of the curve is gentle and linear until 90%, the percentage of ML

19 https://github.com/BoseAslCohort/youtube-8m/commit/
c1b01315bafc24e83248cd862a9324bb21d4d52d
20 GitHub link replication package will be provided after the publication.
21 https://docs.github.com/en/rest/reference/repos

Towards a Change Taxonomy for Machine Learning Pipelines

15

Fig. 2: Percentage of Non-Empty Forks across studied repositories. 52% repos-
itories do not have any fork modiﬁcation to the forked source code.

research repositories with non-empty Forks is low.

4.1.2 Popularity of ML research repositories

ML research repositories have a high median star# of 22 and fork#
of 8. These numbers stand in stark comparison to the datasets used by prior
research for non-ML repositories. We observed a median of zero stars and forks
for the replication dataset provided by Brisson et al. [27], consisting of 13,431
projects. The comparisons of star# and forks# for Brisson’s dataset with our
study are statistically signiﬁcant with Wilcoxon Rank sum p − value < 0.01
Star# and fork# are highly correlated (Spearman ρ=0.94) as shown
by the data distribution in Figure 3. 20% of the repositories have less than ﬁve
stars and ﬁve forks as indicated by the darker color at the low end of star#
and fork# in Figure 3.

These results again contrast to the low correlation of 0.45 found by Brisson
et al. on their non-ML dataset of 13,431 repositories, suggesting a much weaker
connection between stars and forks. Several hypotheses might explain this
contradiction, and require future work to be validated. For example, due to
the current hype of AI technologies, ML repositories might be substantially
more popular than non-ML repositories. It could also be that, due to the
quick succession of new AI algorithms, the OSS community uses forks for
the purpose of “bookmarking” or keeping copies of interesting ML research

50%60%70%80%90%100%Percentage of Repositories0%20%40%60%80%100%Percentage of Non-Trivial Forks16

Bhatia et. al.

Fig. 3: Hexbin for Star# (logged) and Fork# (logged) indicates high correla-
tion. Popularity can be indicated by either of the metrics.

implementations [42]. One indication of the latter hypothesis could be the
high percentage (91%) of forks without any code change (i.e., trivial forks)
that we found earlier.

4.1.3 Speed and Longevity of Forking

Forks on ML repositories appear as fast as the 11th day (median
11.5), while fork-based collaboration sustains a median of 2.6 years.
Figure 4 shows the distribution of the time of the ﬁrst and the ﬁnal (at the
time of analysis) fork for each repository in our dataset. A median ML research
repository receives its ﬁrst forks on the 11th day after creation date, while an
ML research repository is forked till a median of 2.6 years of the creation of
the repository. Although the final fork time may be impacted by the time
of our analysis, nevertheless, a value of 2.6 years deﬁnitely shows that the ML
repositories are not just data dumps but can foster online collaboration.

4.1.4 Transitive Forking

20% of ML research repositories have transitive forks (i.e., fork
repositories with their own forks). We observed 67,369, 1,581, 44, and
7 cases of direct forks, level-two, level-three, and level-four forking transitiv-
ity for 1,110, 226, 28, and 3 ML research repositories respectively. Hence, 226

0.51.01.52.02.53.03.5# Forks (Log Base 10) 0.00.51.01.52.02.53.03.54.0# Stars (Log Base 10) 01020304050Repository CountTowards a Change Taxonomy for Machine Learning Pipelines

17

Fig. 4: The ﬁrst forking time represents the speed of the open source com-
munity in adapting ML research repositories, whereas the ﬁnal forking time
represents the longevity of adaptations.

out of 1,346 repositories, i.e., 20%, have at least one transitive fork (since
the 28 with level-three forks are a subset of the 226, etc.). The ML research
repository with the highest forking transitivity in our dataset includes the
Autopilot-TensorFlow project22, which is an implementation of self-driving
car research23. This project has 281 direct forks, 10 level-two, 3 level-three,
and 5 level-four transitive forks.

We compare our ﬁndings to the fork transitivity results reported by Brisson
et al. [27], who conducted an analysis of the March 2019 GHTorrent dataset
and reported 12171 level-one, 778 level-two, 84 level-three, 11 level-four, and
2 level-ﬁve forks. A χ2 test of independence between these ﬁndings and ours,
yielded a p − value < 0.01, representing a statistically signiﬁcant diﬀerence in
data distribution between ML and non-ML repositories.

4.1.5 Upstream Contribution

41.6% of Non-Empty Forks send changes back to the original reposito-
ries in the form of upstream contributions to the ML research repos-
itory. A total of 607 pull requests were submitted upstream by Non-Empty

22 https://github.com/SullyChen/Autopilot-TensorFlow
23 https://arxiv.org/pdf/1604.07316.pdf

First ForkFinal Fork020040060080010001200Time in Days18

Bhatia et. al.

Fig. 5: Time spent in sending PRs (left) and merging the PRs into the upstream
parent repository (right).

Forks, out of which 316 were merged into the original repositories. This re-
sulted in 52.1% acceptance. This value is slightly lower than that of a recent
study on NPM packages by Dey and Mockus [14], who reported a PR accep-
tance rate of 60%.

27.5% of the upstream PRs were submitted on the same day as
that of the creation of the fork. In particular, a fork takes a median of 22
hours to submit a PR. After receiving a PR, the parent ML research repository
takes a median of seven hours to review the upstream changes before deciding
on them, as shown by the violin plots in Figure 5. This is approximately
four times faster than the median PR acceptance times (27.7 hours) for NPM
packages on GitHub [14]. Another study performed on 1.9 million PRs on
GitHub in 2013 by Gousios et al. [15] reported a median of seven days to
merge a PR.

PR CreationTimePR MergingTime010002000300040005000600070008000HoursTowards a Change Taxonomy for Machine Learning Pipelines

19

Fig. 6: Enhanced version of Hindle’s change taxonomy. The bolded change
(sub-)categories were identiﬁed by this study.

: Summary of RQ1

The OSS community forks the ML research as fast as a median of 11.5
days after the creation of the repository, and forking continues until a
median of 2.6 years. ML research repositories are heavily and transi-
tively forked, yet only 9% of the forks are non-empty. Of those, 43%
sent upstream changes back to the parent ML research repositories,
with a 52% acceptance rate. PR merge times of ML research reposito-
ries are faster than the values reported by prior research for non-ML
repositories.

4.2 RQ2: What are the types of changes in ML research repositories?

Motivation. Since Hindle et al.’s taxonomy of changes focused on traditional
software systems known in 2008 [13], this research question performs a qualita-
tive analysis to identify the types of changes made in ML research repositories,
possibly extending Hindle’s change taxonomy. Through this, we wish to help
software practitioners in building and maintaining ML software, which not
only involves code changes but also changes to many other kinds of artifacts
(e.g., dataset and models) [1]. Furthermore, training/education teams need
an understanding of the types of code changes to better equip students and
novice developers in supporting ML applications.
Approach. In RQ1, we observed that 52% of the PRs sent by Non-Empty
Forks are merged into the parent ML research repository. In this research
question, we qualitatively analyze 1) the types of changes that were merged
with the parent repositories, which we call upstream changes; and 2) the
types of changes that were performed within the forked repositories, but not

ImplementationPlatform FeatureMaintenanceParameter Tuning Pre-processing Training Infrastructure Model Structure Pipeline Performance Cross cutting concern Debug Maintenance Bug FixDependencyManagementRemove Dependency Update Dependency Add DependencyDataInput Data Output Data Project  Metadata Meta ProgramSharing Validation Infrastructure Internal Documentation External Documentation Internationalization Testing Build/Config LegalLicenseNon Functional Code ChangesToken replace Indent Refactor CleanupType of Changes in MLand Traditional SoftwareModule ManagementAdd autogenerated Code Split Remove Move Add SourceManagementFile Permission Branch Versioning External Source Control Merge20

Bhatia et. al.

pushed to the parent repository, which we call downstream changes. Using
the sampling and coding approach discussed in Section 3.3, the coders of team
A and Team B validated and enhanced Hindle et al.’s taxonomy [13]. This sec-
tion reports on the new change (sub-) categories identiﬁed in the analyzed
code changes of ML repositories.

Results. Hindle et al.’s change taxonomy [13] was extended with
two new categories and 15 new sub-categories of changes. Only one
of the two new high-level change categories was ML-speciﬁc, i.e., Data, while
the other one, i.e., Dependency Management, represents an update to the orig-
inal taxonomy related to modern library dependency management activities
(which were less relevant 13 years ago). A graphical summary of the extended
taxonomy of change (sub-) categories is provided in Figure 6. In the subsec-
tions below, we brieﬂy describe and illustrate each new (sub-) category and
how it complements the existing taxonomy:

4.2.1 Maintenance

Code changes performing software maintenance activities. We identiﬁed four
new maintenance change sub-categories in the context of machine learning.

– Pre-processing:

Deﬁnition: Source code changes related to manipulation, cleaning, and
ﬁltering of data before feeding it to the model training or inference com-
ponents of an ML pipeline.
Explanation: ML model training needs high-quality, clean data [43] mak-
ing data pre-processing a vital part of the ML pipeline [2]. Pre-processing
changes internally mutate and clean the ingested data such that it can be
consumed by the ML model, for example by changing the text-embeddings
for NLP data. In contrast to the Data category, which deals with the inges-
tion or egestion of external data to/from the ML pipeline, pre-processing
changes deal with internal data manipulation, and hence is a maintenance
activity.
Notable Instances: In an image processing application [44], pre-processing
changes involve changing image color formats (Grayscale, RBG, BGR) be-
fore feeding the images to the ML model.

– Parameter Tuning

Deﬁnition: Changes made to hard-coded (hyper-)parameter values for tweak-
ing the performance or functionality of an ML pipeline.
Explanation: A machine learning pipeline consists of many diﬀerent phases
(e.g., data preprocessing, feature extraction, model training, and valida-
tion) that, together, aim to generate models with the best ﬁt possible. Each
of these phases [2] involves choosing values for various thresholds, model
hyper-parameters, and other conﬁguration options, many of which have
hard-coded values. Hence, ML pipeline developers often ﬁnd themselves
tweaking these hard-coded values while building the model or preprocess-

Towards a Change Taxonomy for Machine Learning Pipelines

21

ing the data (i.e., parameter tuning). Even in the case of model hyper-
parameters, which are optimized during the model training process, their
initial (hard-coded) value or range often has to be chosen well for quicker
convergence during training. In contrast to model structural changes, which
change the model building code at a structural level, parameter changes
are performed at the variable (value) level.
Notable Instances: The model hyper-parameter variable, weights regular-
izer in project youtube-8m [45], was changed from 1e−5 to 1e−8. In another
image processing application [46], adding a sliding window variable for pre-
processing of video frames is a parameter tuning change.

– Model Structure:

Deﬁnition: Structural change to the source code responsible for training
the machine learning model.
Explanation: Model structural changes involve changing the code encom-
passing the structure of deep learning models or the various functions or
modules that manipulate the model during training, such as adding func-
tions for dropout layers, loss functions, or regularizers for a model class.
These changes are diﬀerent from parameter changes since they are at a
structural level rather than the variable level.
Notable Instances In ﬁle train val.py of tf-faster-rcnn project24, the
model structure was changed to accommodate features related to image and
mask height and width, which manifested in numerous structural changes
to the model building process [47].

– Training Infrastructure:

Deﬁnition: Pipeline-level changes performed for training the model. Expla-
nation: Amershi et al. [2] identiﬁed the canonical components of a typi-
cal ML pipeline, such as data wrangling, feature engineering, and model
training . Making changes in one component (for instance, a diﬀerent
dataset schema) may manifest in other pipeline components (e.g., data
pre-processing, feature engineering, model training, etc.). As such, train-
ing infrastructure changes correspond to any pipeline-level changes to the
logic driving the ML model training phases. This is similar to how Hin-
dle et al. [13]’s original Build/Conﬁg change sub-category that focuses on
the logic driving the compilation (build) process, in contrast to structural
changes, which instead change the structure (e.g., the number of hidden or
dropout layers) of the model being trained.
Notable Instances: While adding a new demo in a semantic segmentation
project [48], 12 ﬁles pertaining to the ML pipeline were changed. This was
accompanied by a new training driver script25 for the new demo data.
Clearly, the model building pipeline had to be updated at multiple places
to accommodate this new data.

24 https://github.com/shikorab/tf-faster-rcnn
25 https://github.com/TSchattschneider/PointCNN/commit/
1827a79b2ede15007a06d327d95f10bc0753420

22

Bhatia et. al.

– Pipeline Performance:

Deﬁnition: Any change pertaining to the eﬃciency of the ML pipeline in
terms of run-time.
Explanation: ML operations are computationally expensive and time-consuming.
Iteratively retraining ML pipelines to ﬁnd optimal values for model hyper-
parameters and data cleaning conﬁgurations exacerbates performance needs.
Hence, this sub-category of code changes involves any changes improving
the run-time eﬃciency of the ML pipeline.
Notable Instances: Re-writing speciﬁc ML operations related to Principle
Components Analysis (PCA) in Tensorﬂow in a project [49] enabled higher
computation eﬃciency. In particular, CPU utilization dropped from 5,600%
to 240%.

4.2.2 Meta Program

As identiﬁed by Hindle et al., Meta program changes update the metadata of
the program (i.e., data required by the project, but not the source code). For
instance, makeﬁles, and readme (external-documentation) ﬁles. We identiﬁed
three new change sub-categories.

– Sharing.

Deﬁnition: Changes in the way the source code of ML projects are pre-
sented or deployed to enable better collaboration between diﬀerent roles
involved in an ML project.
Explanation: In the modern collaborative development era, projects are
shared with other developers and end users [6]. In the case of ML pipelines,
such changes involve converting python scripts into Jupyter notebooks bet-
ter suited for understanding and working with complex ML operations [50];
or sharing the dependency environment via Docker containers, enabling
others to quickly deploy and run experiments on their infrastructure.
Notable Instances: A docker ﬁle was created for the project Neural-style [51].
Another project changed the demo jupyter notebooks ﬁles [52] to dissemi-
nate the developed ML project and its parameters.

– Validation Infrastructure.

Deﬁnition: Changes made to the ML model validation component of an
ML pipeline [2].
Explanation: Validation changes involve changes to any modules or compo-
nents responsible for driving the evaluation of a trained ML model’s (accu-
racy) performance, possibly comparing to the performance of prior trained
models or earlier iterations of the trained model. This kind of change is
similar to Training Infrastructure changes but focuses on the validation
infrastructure instead of the training infrastructure.
Notable Instances: The ﬁle evaluate3.py was added in an image process-
ing project [53] to evaluate the model’s performance by comparing the
predicted labels (annotations on images) against the true labels.

– Internal documentation

Deﬁnition: Changes that explain the internal workings of the ML code to

Towards a Change Taxonomy for Machine Learning Pipelines

23

developers.
Explanation: Internal documentation changes clarify the ﬁne-grained im-
plementation of the code, with developers and data scientists as the in-
tended audience. Such changes not only add code comments but could
also add log statements to the code, for example, a succession of print
statements, to better comprehend the workings of ML pipeline opera-
tions. Internal documentation changes diﬀer from external documentation,
since the latter explicitly document a project for end-users, typically using
README ﬁles or API documentation.
We include internal documentation as an augmentation to Hindle et al.’s [13]
Documentation sub-category, as they did not provide any distinction be-
tween internal and external documentation.
Notable Instances: In a FasterRCNN project [54], ambiguous internal docu-
mentation about an internal ﬂag variable using DEFINE bool was rectiﬁed.
In another project [55], the grammar of the comments that explain the
internal working of the code was ﬁxed.

4.2.3 Module management

As identiﬁed by prior research, module management changes the way ﬁles
are named and organized into source code modules. In addition to Hindle et
al.’s sub-categories (i.e., add, rename and delete module), we identiﬁed a new
change sub-category, Adding auto-generated code.

– Adding auto-generated code

Deﬁnition Adding new ﬁles to the project that are generated automatically
by external tools, alternative IDEs, or varying environment conﬁgurations.
Notable InstancesA commit involving 8,491 added lines of code and 3,813
deleted lines of code across three C ﬁles [56] corresponded to a re-generated
C implementation of the Non-maximum Suppression (NMS) algorithm,
typically used for selecting the best bounding boxes of objects in an image.
Since pure Python implementations of this algorithm are not scalable, data
scientists tend to use the Cython dialect of Python, which allows generating
eﬃcient C code.

4.2.4 Data Category [NEW]

Any change to the infrastructure that handles ingestion/egestion of domain-
speciﬁc data (e.g., for training or testing) required by an ML pipeline, or to the
metadata of said data (e.g., directory paths). Note that this category does not
involve committing actual data ﬁles, since Git repositories are not the right
place to store large-scale data.

– Input Data.

Deﬁnition: Code changes to the logic responsible for loading data or in-
gesting external data into an ML pipeline.

24

Bhatia et. al.

Explanation: ML pipelines need to deal with a variety of data storage plat-
forms (e.g., CSV ﬁles, SQL database, Kafka, data lakes) to obtain domain-
speciﬁc input data. Hence, this sub-category of changes relates to the logic
of dealing with such data platforms and the data schemas of ingested data.
Notable Instances: File extract tfrecords main.py in project, Youtube-8m [57],
added functionality to load external video frames data and feed it in the
right data format to the ML pipeline.

– Output Data.

Deﬁnition: Changing the way the output data of the ML program is stored.
Explanation: Output data changes pertain to the way the results/output
of the ML pipeline’s are saved to the ﬁle system. Such changes may be
needed to improve the integration of a model or its prediction results into
an end-user application (e.g., UI applications or dashboards), or in other
pipelines.
Notable Instances: The faster-rcnn demo program was changed to save its
output to an image ﬁle [58].

– Project Metadata.

Deﬁnition: Changing the metadata of an ML pipeline’s input/output data.
Explanation: ML pipelines contain a variety of metadata about the input
and output data that they ingest/egest, such as paths of base directories
or speciﬁc data ﬁles, license information of said data, etc. Hence, project
metadata changes include adding, updating or deleting such metadata.
This does not include changes to the actual data (pre-processing) or the
infrastructure used to ingest/egest such data (Input Data/Output Data),
only to the project metadata.
Notable Instances: Project directories for loading various model artifacts
like model graphs and pretrained models were updated in a facenet imple-
mentation [59].

4.2.5 Source Management

Hindle et al. described Source management as changes performed due to the
way a version control system is being used by a project. Along with the ﬁve sub-
categories identiﬁed by Hindle et al. [13], we identiﬁed one new sub-category.

– Changing ﬁle permissions.

Deﬁnition: Changes adding, updating, or removing ﬁle permissions (like
executability of a script).
Explanation. Traditional programs and ML operations are often run on
shared high-performance computers (typically Unix-based servers). Man-
aging ﬁle permissions is essential for assigning ownership of ﬁles while deal-
ing with multiple users, thereby enforcing security.
Notable Instances: The ﬁle start.sh was given 775 permissions [60] since
it’s previous 664 permission did not allow the script to be executed by the
owner of the ﬁle or its Unix user group.

Towards a Change Taxonomy for Machine Learning Pipelines

25

Fig. 7: ML-speciﬁc sub-categories mapped to Amershi’s ML pipeline architec-
ture [2].

4.2.6 Dependency Management [NEW]

While we identiﬁed this new category related to handling third-party depen-
dencies (e.g., libraries or packages of a Linux distribution) on code changes of
the studied ML pipelines, the management of such dependencies is common
across both ML and non-ML projects [61,62, 63].

– Add Dependency Adopting a new third-party dependency in the source

code.
Deﬁnition: Adoption of a new third-party dependency typically requires
adding the name and version of the dependency to a conﬁguration ﬁle, as
well as adding import statements to various ﬁles in the source code, in
order to declare the dependency to compilers or interpreters.
Notable Instances: Addition of new import statements like
‘‘from tensorflow.python.lib.io import file io’’ [64].

– Remove Dependency Stopping the adoption of a third-party depen-

dency.
Deﬁnition: Removing an unused import statement from a source code ﬁle,
or even removing the actual third-party dependency from the list of de-
pendencies of a ﬁle.
Notable Instances: Removal of unused import statements [64].

– Update Dependency After the adoption of a dependency, changes might

be needed to the metadata of the dependency.
Deﬁnition: This change category involves updating the metadata of a de-
pendency, for example, to keep the dependency compatible with the code
base, or vice versa. This typically includes updating the dependency ver-
sion.
Notable Instances: Change of the cloudml-gpu runtime version from “1.0”
to “1.8” [65].

4.2.7 Mapping the updated change taxonomy to Amershi’s ML pipeline
architecture

In Figure 7, we provide an association between the new categories of code
changes identiﬁed in this research to the ML pipeline architecture of Amershi et

Data CleaningModel TrainingFeature EngineeringModel EvaluationModelMonitoringParameter TrainingInfrastructurePerformance Pre-processing ChangeInput Data  ChangeOutputData  ChangeDataCollectionModelRequirementsModel  DeploymentValidation ChangeModel Structure Change26

Bhatia et. al.

Fig. 8: Percentage of change sub-categories present in the 378 samples of
Downstream commits and 539 samples of Upstream commits. Values above
y = 0.0% (plotted using a lighter color palette) indicate the percentage of up-
stream changes containing a speciﬁc change sub-category, whereas values below
y = 0.0% (darker color palette) indicate the same for downstream changes.

al. [2]. We notice that most (5) of the identiﬁed change categories apply to the
data cleaning and model training phases, followed by the feature engineering
(3) and model evaluation (2) phases.

On the other hand, none of the change types map to the initial phases of
model requirements, and data collection, since those involve tasks performed
by management and data engineers, respectively. Similarly, the end phases,
namely, model deployment and model monitoring, are geared towards third-
party applications where the trained model is integrated and deployed by
MLOps engineers into (amongst others) dashboards, UI applications, and
back-end servers for prediction.

: Summary of RQ2

Hindle et al.’s taxonomy of software code changes [13] had to be ex-
tended with two high-level change categories (ML-speciﬁc Data, and
generic Dependency management). We also extended the taxonomy by
identifying 16 new sub-categories of changes, seven nine of which (i.e.,
input data, parameter tuning, pre-processing, training infrastructure,
model structure, pipeline performance, sharing, validation infrastruc-
ture, and output data) are ML-speciﬁc.

LicenseRemove DependencyUpdate DependencyAdd DependencyVersioningFile permissionSource ControlMergeProject MetadataInput DataOutput DataAdd Regenerated ModuleRemove ModuleMove ModuleAdd ModulePlatformFeatureToken ReplaceIndentRefactorCleanupInternationalizationSharingValidation InfrastructureTestingInternal DocumentationBuild/ConfigExternal DocumentationCross Cutting ConcernPipeline PerformanceDebugPre-processingModel StructureMaintenanceTraining InfrastructureParameter TuningBug Fix15.0%10.0%5.0%0.0%5.0%10.0%15.0%20.0%25.0%Percent of samplesImplementationMaintenanceModule Mgmnt.Dep. Mgmnt.Non Fsc.Scs. Mgmnt.MetaDataMisc.Legal.Towards a Change Taxonomy for Machine Learning Pipelines

27

4.3 RQ3: How do downstream changes diﬀer from upstream changes in ML
research repositories?

Motivation. In RQ1, we observed that 41.6% of Non-Empty Forks submit
upstream contributions back to the ML research repositories. Since this means
that contributions by more than half of the forks were never sent upstream,
it is interesting to understand the nature of such contributions, i.e., what did
the ML community need in addition to the original development in the par-
ent repository (merged PRs), and what did the authors of the ML repository
miss out on (code changes not contributed back)? In particular, the upstream
changes studied in this paper help to identify the missing aspects of the original
ML parent repository contributed back by the OSS community. Conversely,
understanding the downstream changes studied in this paper helps us deter-
mine to what extent essential features or contributions have been missed.
Approach. This RQ uses the sample of 378 downstream changes and 539
upstream changes labeled with high inter-rater agreements in RQ2, but this
time to analyze the prevalence of each change sub-category of the taxonomy
in Figure 6. In particular, we compute the percentage of upstream and down-
stream changes for each (sub-)category, then compare our ﬁndings between
downstream and upstream changes. These results are summarized in Figure 8.
Results. For both upstream and downstream commits, heavy changes
occur in Documentation (Meta program), bug ﬁxes and Model train-
ing (Maintenance); and adding new features (Implementation). Fig-
ure 8 shows substantial peaks in the Maintenance and Meta program cate-
gories. We attribute such results to the nature of the data science life cycle,
where ML pipelines require substantial maintenance activities during experi-
mentation with and tweaking of data and models. ML tasks include multiple
iterations of updating data pre-processing, tuning parameters, updating model
building code, and improving pipeline performance.

Maintenance changes are much more prevalent in downstream
commits than in upstream commits. This is visible through the higher
percentages of non functional and data changes in downstream commits.
We attribute this imbalance to downstream users adapting ML research for
their domain-speciﬁc tasks, rather than improving the upstream repository
for generic usage. In order to improve personal understanding of the ML code,
downstream users added changes from the new change sub-category, internal
documentation, such as added print statements. In contrast, we found no cases
of internal documentation in upstream commits.

We also observed nine cases of our new sub-category change evaluation
for downstream changes, while none for upstream commits. Intuitively, down-
stream developers needed scripts to train and test models (potentially after
making some other changes) against their domain-speciﬁc data. Conversely,
there were 45 cases of our new change sub-category, Dependency Manage-
ment, in upstream commits. Such changes add, update or remove ML library
dependencies. Finally, accepted PRs were merged into either a project’s main

28

Bhatia et. al.

branch or alternative branches, which led to more instances of merging changes
(Source Management) in upstream commits than in downstream commits.

Noticeable instances of the most popular pre-AI change categories.
In the results of RQ3, we notice that 3 of the top 4 most occurring change types
across upstream and downstream commits, i.e., Bug Fix, Documentation, and
Feature, belong to Hindle’s pre-AI taxonomy. Hence, here we provide some
examples of those change sub-categories in the context of ML-based pipeline
projects.

1. External Documentation: In the project TensorBox, the README.md ﬁle
was updated to present information about how to manually download ex-
ternal dependencies (e.g., CUDA version) and how to set up and conﬁg-
ure the corresponding runtime environment [66]. This was a downstream
change.

2. Bug Fix: In the project tf-faster-rcnn, a bug in the test rpn function was
ﬁxed in a downstream change as indicated by its commit message [67]. In
another project, Kitti-Seg, the order of height and width parameters was
incorrectly swapped, as indicated by the commit message for the upstream
change [68].

3. Feature: In a downstream commit for project PointCNN [69], new features
were added allowing to set GPU ﬂags and the CUDA path, and to create
project directories for saving the model, if not yet existing.

: Summary of RQ3

Both upstream and downstream contributions to ML research repos-
itory add documentation, ﬁx bugs and add features. Downstream de-
velopers change input/output data, perform parameter tuning, add new
functional features, and perform other non-functional changes like in-
dentation, refactoring, or cleaning up the source code. Such changes are
domain oriented. On the other hand, upstream changes beneﬁt the par-
ent repository by updating dependencies or ﬁxing bugs for the parent
repository.

5 Implications

In this section, we discuss the implications of our ﬁndings for software re-
searchers, ML practitioners, the OSS ML community, toolsmith engineers,
and ML educators.

5.1 Implications for Researchers

We extended Hindle et al.’s taxonomy of code changes [13] with two new cat-
egories and 15 new sub-categories of changes. Researchers can use our

Towards a Change Taxonomy for Machine Learning Pipelines

29

extended taxonomy to obtain a holistic picture of software changes
in ML pipelines. In particular, nine (input data, parameter tuning, pre-
processing, training infrastructure, model structure, pipeline performance, shar-
ing, validation infrastructure, and output data)) of the ML-related categories of
code changes indicate a need for revising and adapting existing best practices
towards the needs of software engineering for ML systems. This is only exacer-
bated by the prevalence of the Internal and External Documentation change
sub-categories, indicating diﬃculties for developers to comprehend complex
ML code and keep track of hefty ML pipelines.

Our work also updates existing code-change dimensions towards
modern SE paradigms in SE4AI. Even though some code change cate-
gories identiﬁed by Hindle et al. still apply in the context of ML pipelines, we
were able to better understand their applicability within our context of ML
pipelines. For instance, studying software bugs has been an important focus
of the software engineering community for decades. With the advent of ML,
recently many studies [70,71,72,73] started focusing on bugs in the machine
learning domain. However, thus far the scope of these types of ML studies
is limited to machine learning frameworks, while bugs in the diﬀerent phases
and components of actual ML pipelines or even end-user ML applications are
not yet explored in depth. For example, initial studies found that the data
wrangling phase introduces a variety of pipeline-level [2] challenges, including
pipeline-level bugs.

As another example, we split the “documentation” category of Hindle’s
change taxonomy into “internal” and “external” documentation, since our
analysis of ML pipeline code changes made it especially apparent that both
cater to a diﬀerent audience in the modern SE paradigm of collaborative devel-
opment. In particular, the (external) API-level or application-level documen-
tation is aimed toward black-box (re)use of a given project, while the ﬁned-
grained (internal) documentation instead explains the inner working and state
of processing of the code to people interested in changing, or at least better
understanding, it. Such a distinction may not have been that obvious 13 years
ago [13].

5.2 Implications for Toolsmiths

An updated taxonomy of code changes can help toolsmiths in adapt-
ing and innovating software engineering tools. As mentioned in Sec-
tion 2, code change data is used for a variety of purposes such as extraction
of missing traceability links [21], auto-generation of commit messages [23],
and analysis of quality impact [24]. At the same time, current development
environments and tools used by developers need to be modernized as well.

Our observed instances of code changes spanning across the Maintenance,
Dependency Management, Source management, and Data domains imply a
need for toolsmiths to better support ML engineering teams in handling re-
quirements, managing data dependencies, conﬁgurations, training ML models.

30

Bhatia et. al.

For example, given that many developers add comments to the code to bet-
ter understand the ML logic, code bases might get polluted. The boom of
Jupyter Notebooks [74] for data science only provides a workaround to this
problem [75], which might not scale to real-life ML practices of large systems.
Hence, perhaps less invasive annotation or other functionality is required in
future IDEs.

As another example, code changes play an important role in tracking
bugs [76,77,78]. An updated taxonomy of code changes may enable a more
eﬀective automated classiﬁcation of bug reports or code changes submitted for
code review. For example, most automated change classiﬁcation techniques fo-
cus on a limited number of possible change types. Our results could also help
fault localization and defect prediction researchers improve their models.

5.3 Implications for Educators

Software educators may wish to update their curricula to revise future training
of (ML) software engineers. Moreover, novice software developers need training
on practices related to Dependency management, to be better equipped to use
and support ML frameworks. Overall, ML practitioners need to be aware of
the change taxonomy to anticipate future changes that occur in ML software.
Particularly, Table 7 provides a map for educators of the diﬀerent ML-
related change (sub-)types to expect while providing a travel guide for ed-
ucation and training teams. Since ML-based organizations tend to have a
distributed team with overlapping roles ranging from data developers, data
scientists, statisticians, DevOps engineers, to software developers, software
teams may wish to leverage such a map for a clearer understanding of the
roles and responsibilities w.r.t. the nature of development changes performed
by a speciﬁc role.

5.4 Implications for OSS community

Organizations and/or individuals wishing to open-source their ML
repositories should have realistic assumptions. While our ﬁndings show
that organizations and researchers do not necessarily “dump” their ML re-
search implementations on GitHub, but receive and merge open-source con-
tributions, this is not guaranteed. For one, only 9% of forks are Non-trivial
forks, of which 41.6% send contributions upstream via a pull request, about
half of which (52.1%) are accepted into the parent repository (see RQ1).

Two lessons can be learned from this. On the one hand, the ML research
repositories are missing out on almost 60% of forks having contributions that
are never sent back upstream. Even the 41.6% of forks that do contribute
might not contribute back all contributions they have made. While it is OK
for changes like parameter tuning not to be contributed back, the “lost” con-
tributions of forks also include 16% of new bug ﬁxes, 13% of new features for

Towards a Change Taxonomy for Machine Learning Pipelines

31

the ML pipeline, etc. Future work should look into why those were never sent
back.

On the other hand, of those contributions that were propagated back, only
half were merged. Future work should consider the reasons for rejection of this
work, i.e., to what extent was rejection based on the quality of the contribu-
tion versus the contribution being too tied to the contributor’s own use case,
or even versus the responsiveness of the ML repository owners. Whichever the
outcome, and similar to traditional open-source development, receiving many
high-volume contributions requires eﬀort [29]. Researchers can gain leverage
from our updated taxonomy to pay special attention to ML pipeline compo-
nents that are updated while maintaining ML code.

6 Threats to Validity

Threats to Internal Validity.

Qualitative studies can be subject to researcher bias. To minimize this, we
used multiple participants (i.e., two teams with four people in Team-A, and
three people in Team-B) for the manual coding of changes. Both teams had in-
depth knowledge of software development, as well as SE4ML. Furthermore, the
teams pair-wise labeled each sample and achieved high inter-rater agreements
of Kripendoﬀ’s a = 98% for Team-A and a = 92% for Team-B.

Threats to Construct Validity.
In Section 3, we mine ML pipeline repositories implementing algorithms
published in ArXiv papers. For this, we check whether a repository cites an
ArXiv research paper in its README ﬁle. Analysis of a sample of 1,346 such
repositories in Section 1 showed that all repositories citing an ML ArXiv pub-
lication are inﬂuenced by the research and can be termed as a “ML research
repository”, irrespective of whether the repository is created by the authors of
the ML research publication (30%) or by external members of the community
(70%).

Threats to External Validity.
For answering our RQ2, we analyzed both the types of changes made by
PRs merged into the upstream parent repository and by downstream changes
performed within the forks but never sent upstream. However, we do not study
changes rejected by the upstream repository. While future work should analyze
such cases, we feel conﬁdent about the completeness of our taxonomy, as we
reached saturation in obtaining new labels within the initial 78 samples of
downstream commits. No new categories were found in the later part of 300
downstream or any of the 539 upstream commits.

Moreover, we focused on the repositories implementing image processing
or machine learning in ModelDepot, since they were the most popular on
Modeldepot and cover a wide range of popular ML application domains. Future
work should focus on other domains like NLP and Audio Processing.

Finally, we sampled our data for qualitative analysis only from the 23
repositories that were at the top ﬁve percentile of Non-Empty Forks. We put

32

Bhatia et. al.

such a ﬁlter to select repositories with the maximum amount of activity in
terms of downstream commits and pull requests which may thereby manifest
as upstream commits. While we need “popular or active trends” to study rich
and meaningful data that has low noise, nonetheless, this poses a threat to
external validity as is also indicated by prior research [42, 79,80].

Threats to Reliability Validity These threats take into account the
replication of our study. After our data collection process from ModelDepot
ﬁnished and the analysis was well underway, the website was shut down. How-
ever, ModelDepot only pointed to the repositories hosted on GitHub. To mit-
igate this threat, we provide26 our lists of 1,346 repositories, along with the
ArXiv papers cited by these repositories. We also provide a snapshot of the
mined forking data at the time of our analysis for our quantitative investi-
gation of RQ1. The replication data for qualitative analysis of RQ2, RQ3
consists of the labeled sample of upstream and downstream changes to ensure
the reproducibility of our study.

7 Conclusion

Open-source community developers use and reﬁne OSS repositories. Although
prior studies have investigated the nature of open source contributions in non-
ML software, one can imagine the nature of such community changes, as well as
the way in which developers collaborate, to be diﬀerent for Machine Learning
projects. Hence, this paper studies the forking dynamics and the types of
changes performed in 57,369 forks of 1,346 ML pipeline projects related to
research publications.

We found that, while most forks (91%) do not modify an ML research repos-
itory after forking it, 41.6% of the forks with modiﬁcations contribute valu-
able changes to the parent ML research repository, with a 52.1% acceptance
rate. We performed an extensive qualitative study that identiﬁed the types
of changes in ML software. We identiﬁed one new top-level change category,
Data, in the context of ML, and one more generic category (dependency man-
agement). Along with this, we extend the taxonomy of changes by adding 15
new sub-categories, including nine ML-speciﬁc ones (input data, output data,
program data, sharing, change evaluation, parameter tuning, pipeline perfor-
mance, pre-processing, model training) and six generic ones (i.e. adding depen-
dency, removing dependency, updating dependency, ﬁle permissions, internal-
documentation, adding auto-generated code).

Our results aim to help software practitioners in having a better under-
standing of ML changes that can be leveraged while training new developers,
and to support building and maintaining ML software. Furthermore, future
work should look deeper into the reasons why potentially valid Documenta-
tion, Feature and Bug ﬁx changes were not contributed back upstream.

26 GitHub link replication package will be provided after the publication.

Towards a Change Taxonomy for Machine Learning Pipelines

33

Acknowledgement

We thank Greg Wilson for providing insightful ideas and comments for this
work. We also thank Boyuan Chen, Minke Xiu, Javier Rosales and Wanqing
Li for their contributions to the analysis and feedback on this work.

References

1. S. Idowu, D. Str¨uber, and T. Berger, “Asset management in machine learning: A sur-
vey,” in 2021 IEEE/ACM 43rd International Conference on Software Engineering:
Software Engineering in Practice (ICSE-SEIP), 2021, pp. 51–60.

2. S. Amershi, A. Begel, C. Bird, R. DeLine, H. Gall, E. Kamar, N. Nagappan, B. Nushi,
and T. Zimmermann, “Software engineering for machine learning: A case study,” in
2019 IEEE/ACM 41st International Conference on Software Engineering: Software
Engineering in Practice (ICSE-SEIP), 2019, pp. 291–300.

3. N. Nahar, S. Zhou, G. Lewis, and C. K¨astner, “Collaboration challenges in building ml-
enabled systems: Communication, documentation, engineering, and process,” in 2022
IEEE/ACM 44th International Conference on Software Engineering (ICSE), 2022.
4. D. Sato, A. Wider, and C. Windheuser, “Continuous delivery for machine learning,”

https://martinfowler.com/articles/cd4ml.html#DeploymentPipelines, 2019.

5. Y. Fan, X. Xia, D. Lo, A. E. Hassan, and S. Li, “What makes a popular academic AI

repository?” Empirical Software Engineering, vol. 26, no. 1, pp. 1–35, 2021.

6. S. Zhou, B. Vasilescu, and C. K¨astner, “How has forking changed in the last 20 years?
a study of hard forks on github,” in 2020 IEEE/ACM 42nd International Conference
on Software Engineering (ICSE).

IEEE, 2020, pp. 445–456.

7. S. Zhou, B. Vasilescu, and C. Kastner, “What the fork: a study of ineﬃcient and eﬃcient
forking practices in social coding,” in Proceedings of the 2019 27th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the Foundations of
Software Engineering, 2019, pp. 350–361.

8. M. Biazzini and B. Baudry, ““may the fork be with you”: novel metrics to analyze
collaboration on github,” in Proceedings of the 5th international workshop on emerging
trends in software metrics, 2014, pp. 37–43.

9. Y. Hu, J. Zhang, X. Bai, S. Yu, and Z. Yang, “Inﬂuence analysis of github repositories,”

SpringerPlus, vol. 5, no. 1, pp. 1–19, 2016.

10. A. Lima, L. Rossi, and M. Musolesi, “Coding together at scale: Github as a collaborative
social network,” in Eighth international AAAI conference on weblogs and social media,
2014.

11. H. Washizaki, H. Uchida, F. Khomh, and Y.-G. Gu´eh´eneuc, “Studying software engi-
neering patterns for designing machine learning systems,” in 2019 10th International
Workshop on Empirical Software Engineering in Practice (IWESEP).
IEEE, 2019,
pp. 49–495.

12. I. Ozkaya, “What is really diﬀerent in engineering ai-enabled systems?” IEEE Software,

vol. 37, no. 4, pp. 3–6, 2020.

13. A. Hindle, D. M. German, and R. Holt, “What do large commits tell us? a
taxonomical study of
the 2008 International
large commits,” in Proceedings of
Working Conference on Mining Software Repositories, ser. MSR ’08. New York,
NY, USA: Association for Computing Machinery, 2008, p. 99–108. [Online]. Available:
https://doi.org/10.1145/1370750.1370773

14. T. Dey and A. Mockus, “Which pull requests get accepted and why? a study of popular

npm packages,” arXiv preprint arXiv:2003.01153, 2020.

15. G. Gousios, M. Pinzger, and A. v. Deursen, “An exploratory study of the pull-based
software development model,” in Proceedings of the 36th International Conference on
Software Engineering, 2014, pp. 345–355.

16. E. B. Swanson, “The dimensions of maintenance,” in Proceedings of the 2nd interna-

tional conference on Software engineering, 1976, pp. 492–497.

34

Bhatia et. al.

17. A. Hindle, D. M. German, M. W. Godfrey, and R. C. Holt, “Automatic classication of
large changes into maintenance categories,” in 2009 IEEE 17th International Confer-
ence on Program Comprehension.

IEEE, 2009, pp. 30–39.

18. M. Yan, Y. Fu, X. Zhang, D. Yang, L. Xu, and J. D. Kymer, “Automatically classifying
software changes via discriminative topic model: Supporting multi-category and cross-
project,” Journal of Systems and Software, vol. 113, pp. 296–308, 2016.

19. L. Ghadhab, I. Jenhani, M. W. Mkaouer, and M. B. Messaoud, “Augmenting commit
classiﬁcation by using ﬁne-grained source code changes and a pre-trained deep neural
language model,” Information and Software Technology, vol. 135, p. 106566, 2021.
20. H. C. Benestad, B. Anda, and E. Arisholm, “Understanding software maintenance and
evolution by analyzing individual changes: a literature review,” Journal of Software
Maintenance and Evolution: Research and Practice, vol. 21, no. 6, pp. 349–378, 2009.
21. R. Wu, H. Zhang, S. Kim, and S.-C. Cheung, “Relink: recovering links between bugs
and changes,” in Proceedings of the 19th ACM SIGSOFT symposium and the 13th
European conference on Foundations of software engineering, 2011, pp. 15–25.

22. T. F. Bissyand´e, F. Thung, S. Wang, D. Lo, L. Jiang, and L. R´eveill`ere, “Empirical
evaluation of bug linking,” in 2013 17th European Conference on Software Maintenance
and Reengineering, 2013, pp. 89–98.

23. L. F. Cort´es-Coy, M. Linares-V´asquez, J. Aponte, and D. Poshyvanyk, “On automati-
cally generating commit messages via summarization of source code changes,” in 2014
IEEE 14th International Working Conference on Source Code Analysis and Manipu-
lation, 2014, pp. 275–284.

24. C. Farag´o, P. Heged˜us, and R. Ferenc, “The impact of version control operations on
the quality change of the source code,” in International Conference on Computational
Science and Its Applications. Springer, 2014, pp. 353–369.

25. K. Fogel, Producing open source software: How to run a successful free software project.

” O’Reilly Media, Inc.”, 2005.

26. K. Constantino, S. Zhou, M. Souza, E. Figueiredo, and C. K¨astner, “Understanding
collaborative software development: An interview study,” in Proceedings of the 15th
International Conference on Global Software Engineering, 2020, pp. 55–65.

27. S. Brisson, E. Noei, and K. Lyons, “We are family: Analyzing communication in github
software repositories and their forks,” in 2020 IEEE 27th International Conference on
Software Analysis, Evolution and Reengineering (SANER).

IEEE, 2020, pp. 59–69.

28. L. Ren, S. Zhou, and C. K¨astner, “Poster: Forks insight: Providing an overview of github
forks,” in 2018 IEEE/ACM 40th International Conference on Software Engineering:
Companion (ICSE-Companion), 2018, pp. 179–180.

29. M. M. Rahman and C. K. Roy, “An insight into the pull requests of github,” in Pro-
ceedings of the 11th Working Conference on Mining Software Repositories, 2014, pp.
364–367.

30. X. Zhang, Y. Chen, Y. Gu, W. Zou, X. Xie, X. Jia, and J. Xuan, “How do multiple
pull requests change the same code: A study of competing pull requests in github,”
in 2018 IEEE International Conference on Software Maintenance and Evolution (IC-
SME).

IEEE, 2018, pp. 228–239.

31. S. Mart´ınez-Fern´andez, J. Bogner, X. Franch, M. Oriol, J. Siebert, A. Trendowicz, A. M.
Vollmer, and S. Wagner, “Software engineering for ai-based systems: A survey,” arXiv
preprint arXiv:2105.01984, 2021.

32. T. Zhang, C. Gao, L. Ma, M. Lyu, and M. Kim, “An empirical study of common
challenges in developing deep learning applications,” in 2019 IEEE 30th International
Symposium on Software Reliability Engineering (ISSRE).

IEEE, 2019, pp. 104–115.

33. A. Arpteg, B. Brinne, L. Crnkovic-Friis, and J. Bosch, “Software engineering challenges
of deep learning,” in 2018 44th Euromicro Conference on Software Engineering and
Advanced Applications (SEAA).

IEEE, 2018, pp. 50–59.

34. N. Sambasivan, S. Kapania, H. Highﬁll, D. Akrong, P. Paritosh, and L. M. Aroyo,
““everyone wants to do the model work, not the data work”: Data cascades in high-
stakes ai,” in proceedings of the 2021 CHI Conference on Human Factors in Computing
Systems, 2021, pp. 1–15.

35. N. Polyzotis, S. Roy, S. E. Whang, and M. Zinkevich, “Data lifecycle challenges in
production machine learning: a survey,” ACM SIGMOD Record, vol. 47, no. 2, pp.
17–28, 2018.

Towards a Change Taxonomy for Machine Learning Pipelines

35

36. K. O’Leary and M. Uchida, “Common problems with creating machine learning

pipelines from existing code,” 2020.

37. D. M. German, B. Adams, and A. E. Hassan, “Continuously mining distributed ver-
sion control systems: an empirical study of how linux uses git,” Empirical Software
Engineering, vol. 21, no. 1, pp. 260–299, 2016.

38. H. Borges and M. T. Valente, “What’s in a github star? understanding repository star-
ring practices in a social coding platform,” Journal of Systems and Software, vol. 146,
pp. 112–129, 2018.

39. K. Krippendorﬀ, “Computing krippendorﬀ’s alpha-reliability,” 2011.
40. H. Li, W. Shang, B. Adams, M. Sayagh, and A. E. Hassan, “A qualitative study of
the beneﬁts and costs of logging from developers’ perspectives,” IEEE Transactions on
Software Engineering, 2020.

41. P. Salza, F. Palomba, D. Di Nucci, C. D’Uva, A. De Lucia, and F. Ferrucci, “Do develop-
ers update third-party libraries in mobile apps?” in Proceedings of the 26th Conference
on Program Comprehension, 2018, pp. 255–265.

42. E. Kalliamvakou, G. Gousios, K. Blincoe, L. Singer, D. M. German, and D. Damian,
“The promises and perils of mining github,” in Proceedings of the 11th Working
Conference on Mining Software Repositories, ser. MSR 2014. New York, NY,
USA: Association for Computing Machinery, 2014, p. 92–101.
[Online]. Available:
https://doi.org/10.1145/2597073.2597074

43. A. Ng, “Mlops: From model-centric to data-centric ai,” 2021.
44. “Pre-processing example,” https://github.com/lancele/Semantic-Segmentation-Suite/

commit/d50b5c812392614fc2bdaf269921beb1f7086f63, 2018.
example,”

tuning

45. “Parameter

https://github.com/google/youtube-8m/commit/

0e526caace96d3cf6f0686757d568f9ﬀba998b4, 2017.

46. “Model

tuning example,” https://github.com/DeepLabCut/DeepLabCut/commit/

47. “Model

6568c2ba6facf5d90b2c39af7b0f024a40f2b15f, 2017.
example,”
327778b2c4f297b307ﬀ0de552d2bfc47278e290, 2018.

tuning

https://github.com/shikorab/tf-faster-rcnn/commit/

48. “Model

infrastructure

example,” https://github.com/IAC-Team/SemSeg/commit/

efbﬀfbd202cccbd54fca1125ed6de41b5df2f90, 2017.

49. “Performance example,” https://github.com/google/youtube-8m/pull/69, 2018.
50. M. D. Bloice and A. Holzinger, “A tutorial on machine learning and data science tools

with python,” Machine Learning for Health Informatics, pp. 435–480, 2016.
51. “Sharing example,” https://github.com/anishathalye/neural-style/pull/40, 2016.
52. “Sharing

https://github.com/jerichooconnell/tf unet/commit/

example,”

60b67bb964d19dd4a4677f7557dc738838a116e9, 2018.

53. “Change evaluation example,” https://github.com/bethesirius/TensorBox/commit/

1eb41e944494e721f3c4b1a5d287af99f4035a42, 2017.

54. “Comprehension

example,”
3439e33d81df8cd906987ee5889ebc937186114a, 2017.

https://github.com/google/youtube-8m/commit/

55. “Internal

documentation

example,”

https://github.com/CharlesShang/

FastMaskRCNN/commit/0d8ddfaa55dbd3d553b79aed34f40662c46aa45f, 2017.

56. “Adding

auto-generated

ﬁles

example,”

https://github.com/alorozco53/

text-detection-ctpn/commit/f90326f68522f3af3e4cdf5688138685de66bace, 2018.

57. “Input

data

example,”

https://github.com/google/youtube-8m/commit/

58. “Output

4619056162f466293d99e0c59512f8d0f3427fe2, 2017.
example,”
51e0889fbdcd4c48f31def4c1cb05a5a4db04671, 2018.

data

https://github.com/Mappy/tf-faster-rcnn/commit/

59. “Program

data

example,”

https://github.com/Bruceeeee/

facenet/commit/d9e6213cd8286334000ddf75529eba3662cef38a#
diﬀ-dbc5c3b9f46e69236207956b34904d0dea62ﬀ866d442e97bb397ﬀ49a03a86b, 2017.
60. “Change ﬁle permission example,” https://api.github.com/repos/CodeRecipeJYP/
fast-style-transfer/commits/7027a3843fa3d793697da5ba188887629a4d69eb, 2017.
61. A. Decan, T. Mens, and P. Grosjean, “An empirical comparison of dependency network
evolution in seven software packaging ecosystems,” Empirical Software Engineering,
vol. 24, no. 1, pp. 381–416, 2019.

36

Bhatia et. al.

62. I. Pashchenko, D.-L. Vu, and F. Massacci, “A qualitative study of dependency manage-
ment and its security implications,” in Proceedings of the 2020 ACM SIGSAC Confer-
ence on Computer and Communications Security, 2020, pp. 1513–1531.

63. S. Mukherjee, A. Almanza, and C. Rubio-Gonz´alez, “Fixing dependency errors for
python build reproducibility,” in Proceedings of the 30th ACM SIGSOFT International
Symposium on Software Testing and Analysis, 2021, pp. 439–451.

64. “Adding

package

example,”

https://github.com/google/youtube-8m/commit/

09774db80a515b667a91b14fe21a6134f3856c7a, 2019.

65. “Update

package

example,”

https://github.com/google/youtube-8m/commit/

72f42cd938d3cf4f928614a5fcdca237489e7c92, 2018.

66. “External documentation example,” https://github.com/Raochuan89/TensorBox/

commit/aeb45e8fdc100f74aa8cf2fa85b1324483a1ﬀf1, 2017.

67. “Bug

ﬁx

example,”

https://github.com/piaosonglin1985/tf-faster-rcnn/commit/

8e60b9dc92390f1bfb8cf6e62d93bcabbc123c4a, 2019.

68. “Bug

ﬁx

example,”

https://github.com/MarvinTeichmann/KittiSeg/commit/

69. “Feature

ec6b5ccb6f30ac6591d03faa2fa0bf8b1fdbf3ef, 2017.
example,”
891f3e04b44805b066865aeef1275ac6f217c58f, 2018.

https://github.com/tch/PointCNN/commit/

70. Z. Chen, J. M. Zhang, F. Sarro, and M. Harman, “Maat: A novel ensemble approach to
addressing fairness and performance bugs for machine learning software,” in Proceedings
of the 30th ACM Joint European Software Engineering Conference and Symposium on
the Foundations of Software Engineering (ESEC/FSE’22). ACM Press, 2022.

71. S. Tizpaz-Niari, P. ˇCern`y, and A. Trivedi, “Detecting and understanding real-world
diﬀerential performance bugs in machine learning libraries,” in Proceedings of the 29th
ACM SIGSOFT International Symposium on Software Testing and Analysis, 2020, pp.
189–199.

72. D. Cheng, C. Cao, C. Xu, and X. Ma, “Manifesting bugs in machine learning code: An
explorative study with mutation testing,” in 2018 IEEE International Conference on
Software Quality, Reliability and Security (QRS).

IEEE, 2018, pp. 313–324.

73. A. Dwarakanath, M. Ahuja, S. Sikand, R. M. Rao, R. J. C. Bose, N. Dubash, and
S. Podder, “Identifying implementation bugs in machine learning based image classiﬁers
using metamorphic testing,” in Proceedings of the 27th ACM SIGSOFT International
Symposium on Software Testing and Analysis, 2018, pp. 118–128.

74. B. Granger and F. P´erez, “Jupyter: Thinking and storytelling with code and data,”

Authorea Preprints, 2021.

75. J. Wang, L. Li, and A. Zeller, “Better code, better sharing: on the need of analyzing
jupyter notebooks,” in Proceedings of the ACM/IEEE 42nd International Conference
on Software Engineering: New Ideas and Emerging Results, 2020, pp. 53–56.

76. Y. Zhao, H. Leung, Y. Yang, Y. Zhou, and B. Xu, “Towards an understanding of change
types in bug ﬁxing code,” Information and software technology, vol. 86, pp. 37–53, 2017.
77. M. Kim, D. Cai, and S. Kim, “An empirical investigation into the role of api-level refac-
torings during software evolution,” in Proceedings of the 33rd international conference
on software engineering, 2011, pp. 151–160.

78. S. Shivaji, E. J. Whitehead, R. Akella, and S. Kim, “Reducing features to improve code
change-based bug prediction,” IEEE Transactions on Software Engineering, vol. 39,
no. 4, pp. 552–569, 2012.

79. J. A. M. Santos, A. R. Santos, and M. G. Mendon¸ca, “Investigating bias in the search

phase of software engineering secondary studies.” in CIbSE, 2015, p. 488.

80. C. Bird, A. Bachmann, E. Aune, J. Duﬀy, A. Bernstein, V. Filkov, and P. Devanbu,
“Fair and balanced? bias in bug-ﬁx datasets,” in Proceedings of the 7th joint meeting
of the European Software Engineering Conference and the ACM SIGSOFT Symposium
on the Foundations of Software Engineering, 2009, pp. 121–130.

