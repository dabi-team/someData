1
2
0
2

n
a
J

3
1

]
E
S
.
s
c
[

1
v
8
3
9
4
0
.
1
0
1
2
:
v
i
X
r
a

Designing Machine Learning Toolboxes:
Concepts, Principles and Patterns

A Preprint

Franz J. Király ∗
UCL

Markus Löning
UCL

Anthony Blaom
University of Auckland

Ahmed Guecioueur
INSEAD

Raphael Sonabend
UCL

January 14, 2021

Abstract

Machine learning (ML) and AI toolboxes such as scikit-learn or Weka are workhorses
of contemporary data scientiﬁc practice – their central role being enabled by usable yet powerful
designs that allow to easily specify, train and validate complex modeling pipelines. However, despite
their universal success, the key design principles in their construction have never been fully analyzed.
In this paper, we attempt to provide an overview of key patterns in the design of AI modeling
toolboxes, taking inspiration, in equal parts, from the ﬁeld of software engineering, implementation
patterns found in contemporary toolboxes, and our own experience from developing ML toolboxes.
In particular, we develop a conceptual model for the AI/ML domain, with a new type system, called
scientiﬁc types, at its core. Scientiﬁc types capture the scientiﬁc meaning of common elements in ML
workﬂows based on the set of operations that we usually perform with them (i.e. their interface) and
their statistical properties. From our conceptual analysis, we derive a set of design principles and
patterns. We illustrate that our analysis can not only explain the design of existing toolboxes, but
also guide the development of new ones. We intend our contribution to be a state-of-art reference for
future toolbox engineers, a summary of best practices, a collection of ML design patterns which may
become useful for future research, and, potentially, the ﬁrst steps towards a higher-level programming
paradigm for constructing AI.

1

Introduction

Machine learning (ML) applications typically involve a number of steps: one ﬁrst speciﬁes, trains and selects an
appropriate model and then validates and deploys it. ML practitioners write code to evaluate such workﬂows. Application
code often incorporates classes and functions from one or more software packages called toolboxes. Toolboxes provide
prefabricated pieces of code that make it faster to write software. Instead of constructing every piece of software from
scratch, one can simply put together prefabricated pieces of code. Popular examples include scikit-learn [32]
in Python, Weka [20] in Java, MLJ [9] in Julia, and mlr3 [24] or caret [23] in R. Toolboxes have become the
backbone of modern data science and principal tool for ML practitioners in recent years.

The applicability and effectiveness of a toolbox depends crucially on its design. The design determines not only how
easy it is for practitioners to understand and use toolboxes but also how easily they can be tested, maintained and
extended. Despite the importance of toolboxes, the key design principles in their construction have never been fully
discussed. In this paper, we try to close this gap by analyzing common designs of toolboxes and deriving reusable
principles and patterns.

Much of the existing software literature on ML/AI has focused on best practices, design patterns and architecture for
ML systems [29, 36, 30], but little attention has been paid to design at the toolbox level, even though toolboxes are a
central component in ML systems (i.e. kernel software) implementing core algorithmic and mathematical functionality.
In fact, authors often point out that there is a lack of strong abstractions to support ML software, hindering much needed
progress towards a more declarative SQL-like language for ML/AI [36, 45]. Similarly, in the ML literature, while the

∗Corresponding author: f.kiraly@ucl.ac.uk

 
 
 
 
 
 
A preprint - January 14, 2021

importance of toolbox development has been widely recognized [38], discussions on design are still rare. For example,
toolbox developers often subscribe to a set of principles when presenting their work, however these principles typically
remain abstract, with literature primarily discussing features or usage, and not design (see e.g. Buitinck et al. [13]).
While the actual software tends to contain a wealth of design ideas, we are not aware of literature references where data
science speciﬁc patterns, or the process of deriving concrete implementations from design principles, are described or
studied.

In this paper, we make a ﬁrst attempt at consolidating the science of ML toolbox design. Rather than describing a single
toolbox, or analyzing ML/AI software at the system or platform level, this paper focuses speciﬁcally on generalizable
design principles and patterns for ML toolboxes. We investigate ML toolbox design by developing a well-grounded
conceptual model for the ML domain. At its core, we propose a simple but powerful idea called scientiﬁc types – a new
type system which captures the data scientiﬁc purpose of key objects in the ML domain (e.g. data objects, probability
distributions or algorithms). When applying scientiﬁc typing to common ML objects, we are able to derive clear
principles and reusable patterns that can not only explain central aspects of existing toolboxes, but can also guide the
development of new ones.

Toolbox design – much like any software design – is hard. One has to identify the relevant objects, abstract them at the
right level of granularity, deﬁne class interfaces, and specify hierarchies and relationships among them. In comparison
to classical software engineering, ML/AI toolbox development raises a number of additional questions: In which
respects is the ML domain different from other domains? How can we ﬁnd useful abstractions in the ML domain? For
example, how can we identify useful and meaningful categories for ML algorithms? How do we deﬁne interfaces for
different algorithms? Our aim is to motivate and explain the principles and patterns we propose in a way that other
developers, practitioners and decision makers who rely on ML toolboxes can refer to them when assessing designs.

A natural source of relevant ideas, useful formalism and best practices is found in the corpus of software engineering.
Our approach largely falls under domain-driven design as put forward by Evans [16], but also draws on ideas from
“design by contract” [28], “responsibility-driven design” [43, 44, 42] and pattern-oriented design [17, 14]. While much
is direct transfer from existing software engineering practice, there is a substantial aspect in which ML departs from
other domains: algorithms, interfaces and workﬂows are closely intertwined with mathematical and statistical formalism
– to an extent that mathematical objects are not only at the methodological core, but a key element in representation,
workﬂow speciﬁcation and user interaction.

This situation poses unique challenges which we attempt to address in what follows by a combination of formal
mathematical statistics, well-grounded design principles based on our new scientiﬁc type system, adapted state-of-art
architectural patterns from widely used toolboxes, and our own experience of designing multiple ML toolboxes in
different languages (e.g. sktime [27], MLJ [9], mlr3proba [37], pysf [19], skpro [18], mlaut [22]).
In our analysis, we rely on object-oriented programming (OOP), the predominant paradigm for most complex ML/AI
projects. While OOP is often assumed without rationale, we instead motivate speciﬁcally why OOP is especially well
suited for ML toolboxes. Our analysis also extends in theory to other programming paradigms.

We intend our contribution to be a state-of-art reference for future toolbox engineers, a summary of best practices, a
collection of ML design patterns which may become useful for future research, and, potentially, the ﬁrst steps towards
higher-level declarative programming languages for constructing AI.

Summary of contributions

Our main contributions are both theoretical and practical:

• As the basis for ML/AI toolbox design, we propose a theoretical or conceptual model for key objects in the
ML/AI domain. At its core, we develop a new type system called scientiﬁc typing which captures the data
scientiﬁc meaning of key ML objects (e.g. data objects, distributions or algorithms) and translates them into
implementable software blueprints.

• From our analysis, we derive a set of practical, reusable design principles and design patterns that can motivate

and explain existing toolbox designs and can guide future developments.

Structure

The remainder of the paper is organized as follows: In section 2, we deﬁne the problem we are trying to address in
more detail. In section 3, we start developing our conceptual model for the ML/AI domain, which we then complete in
section 4 by introducing our new scientiﬁc type system. From our conceptual model, we ﬁrst derive a set of high-level
design principles in section 5, and then a set of reusable design patterns in section 6. In section 7, we illustrate how our
derived principles and patterns can not only explain existing designs, but also guide new ones. Section 8 concludes.

2

A preprint - January 14, 2021

2 Problem statement

In this section, we state and contextualize the problem we are trying to address in more detail. It will be helpful to
distinguish between two problems in ML research and applications:

Practitioner’s problem. The practitioner’s problem is to solve a certain practical ML problem. For example, classify-
ing emails as spam or forecasting the demand of electricity. This typically involves deﬁning a suitable ML workﬂow
from model speciﬁcation to deployment. To run their workﬂows, ML practitioners write code speciﬁcally for the
application at hand, but often incorporate functionality from existing toolboxes so that we do not have to write everything
from scratch. We usually evaluate our solutions using a quantiﬁable loss function but also take into account other
criteria such as interpretability, fairness or computational efﬁciency.
Developer’s problem. The developer’s problem is to develop toolboxes that help practitioners solve their respec-
tive problem more effectively. For example, developing a toolbox for cross-sectional ML problems as done in
scikit-learn [32]. A toolbox is a collection of related and reusable classes and functions designed to provide
useful, general-purpose functionality. Developing a toolbox involves deﬁning the scope or application domain we
want to cover, identifying key objects, ﬁnding useful abstractions for them, and implementing them in a re-usable
software package. We typically evaluate our toolbox in light of three aspects: its content (what the code does), design
(its language and structure) and performance (how efﬁciently it runs). For example, we want to make sure that the code
is correct, or consistent with some speciﬁcation through testing, that it is readable, and that it is efﬁcient enough to be
practically useful in terms of run time and memory usage.

This paper is primarily concerned with the developer’s problem, and more speciﬁcally with the design aspect. Design
sometimes also refers to the design process. In this paper, we are mainly concerned with design as the result of that
process, but, of course, the discussion in this paper may also inform the design process. Viewed as a result, toolbox
design is about the overall structure and organization of software into components, but also the speciﬁcs of deﬁning key
components and mapping of mathematical concepts and operations onto classes and functions. Also note that design is
not independent of the content and performance concerns. For example, more readable code may be less efﬁcient but be
easier to test. In general, there will be no “pareto optimum” and trade-offs have to be made.

Toolbox design is crucial to their applicability and effectiveness. On a high level, we believe that the better design
choices for ML/AI toolboxes:

• align the language and structure of code with “natural” mathematical, statistical and methodological semantics

(see domain-driven design [16]),

• facilitate user interaction in common data scientiﬁc workﬂows,
• facilitate rapid prototyping and scaling of AI pipelines,
• facilitate reproducibility and transparency of workﬂows and algorithms,
• facilitate checkability and validability of AI (e.g. with respect to algorithmic performance),
• are architecturally parsimonious, i.e. avoid unnecessary conceptualization, formalization and semantics,
• are inter-operable with other major tools in the same domain,
• facilitate maintenance and extensions.

While the importance of toolbox design has been recognized [38], there is still little research on ML/AI toolbox design.
One reason may be that toolbox design seems obvious or trivial, especially in hindsight. After all, popular toolboxes
have been so widely adopted that it is hard to imagine what better (or worse) alternatives would look like. However,
it becomes clear that many questions remain unanswered when one tries to explain their design choices or when
one tries to develop new toolboxes. Example 1 shows a typical supervised classiﬁcation workﬂow as provided by
scikit-learn .

Example 1: Typical supervised classiﬁcation workﬂow with scikit-learn in Python

1 classifier = RandomForestClassifier()
2 classifier.fit(y_train, X_train)
3 y_pred = classifier.predict(X_test)

X_train and y_train denote the training data feature matrix and label vector, X_test is the feature matrix of the test set,
and y_pred the predicted label.
On a practical level, one may ask: why is “classiﬁer” as in RandomForestClassifier a useful and meaningful
algorithm category or type? Is there a formal, mathematical deﬁnition for such types? Why are the main interface

3

A preprint - January 14, 2021

points fit and predict? How is a “classiﬁer” related to other algorithm types? How can they interact? On a
methodological level, it is not even clear what our answers would look like: How do we identify, describe and motivate
abstractions in the ML domain? How do we make sure that our abstractions are both closely linked to the underlying
mathematical concepts but also easily translatable into software?

We believe that toolbox design remains, partly at least, an art, however that there exist some principles that can explain
and guide our answers to these questions. Throughout the paper, we make qualitative arguments to support our principles
drawing on the adherence to common domain-driven design principles [16], design patterns [17, 25] and other best
practices from software engineering [34, 2]. Through our analysis, we believe that we cannot only explain why certain
designs are more successful than others, but also gain new insights to improve future toolbox design.

3 A conceptual model for AI frameworks

In line with domain-driven design [16], we begin by outlining a conceptual model for concepts of relevance in the
context of ML/AI software frameworks. In domain-driven design, development of a “conceptual model” is usually the
initial step in which relevant concepts are delineated and deﬁned. The software design is then derived to closely match
the developed conceptual model. In this work, we go one step further and derive foundational design patterns for the AI
framework setting – that is, not just speciﬁc toolbox designs, but general guiding principles on how to design toolboxes.
It is important to note that in this conceptualization, two perspectives are inextricably linked:

• The realm of implementations: machine data types, algorithmics, input/output speciﬁcations,

• The realm of formal mathematics: formal domains, types, signatures, assumed mathematical object properties.

In consequence, our conceptual model fundamentally relies on ensuring that both perspectives are taken into account
simultaneously.

In this section, we lay much of the conceptual groundwork which we will leverage in the next section to introduce
scientiﬁc typing. As is standard in software engineering, we observe that in our domain of interest some objects vary
more frequently than others. We start by identifying and separating out the more change-prone objects as sensible
points for abstraction. These are:

• Problems or tasks that we want to solve (e.g. regression or classiﬁcation),

• Algorithms that can solve these tasks (e.g. linear regression or a decision tree),

• Related mathematical objects, on the formal level, such as loss functions or distributions.

In particular, we begin developing our conceptual model for AI frameworks by deﬁning:

• The interface viewpoint, which introduces the dual algorithmic/mathematical perspective,

• The problem speciﬁcation: solving a formal “learning task” by adopting the dual perspective,

• The conceptual object model: conceptualization of mathematical objects and algorithms in terms of mathemat-

ical and algorithmic formalism.

3.1 What is ML from an interface perspective? – the learning machine

In practice, an “AI”, “learning machine”, or short “learner”, is a collection of individual algorithms. As a key step in
our conceptualization, we adopt an interface viewpoint for learners – that is, we consider a learning machine as deﬁned
by in which respect it interacts with its environment, e.g. other algorithms, sensors, a human data scientist. Figure 1
schematically depicts a supervised learner, our running example for much of this paper. Typically, in common data
scientiﬁc practice, a user would interact with the supervised learner in one of the following ways:

• Specifying the model class, model parameters, or model structure,

• Ingesting data (more precisely: feature-label pairs) to “ﬁt” or “learn” the (statistical) model,

• Calling a ﬁtted model for prediction on new data (more precisely: features) to “predict” labels.

There are a number of other possible user interactions or interface points. For example, setting or retrieving hyper-
parameters or ﬁtted parameters, updating a ﬁtted model in an online learning setting, or persisting a ﬁtted model for
later deployment.

4

A preprint - January 14, 2021

Figure 1: Conceptual model for a supervised classiﬁcation “learning machine”, schematically depicted from the
interface perspective. The “learning machine” is depicted as a “black box robot”, with black arrows depicting data
related interface points, and hyper-parameters (bottom box and red arrows) constituting another interface point. The
ﬁgure also schematically depicts the sequence by which interface points appear in the most common user journey of the
learning machine. From left to right: ﬁrst (leftmost black arrow), the supervised learner ingests (labelled) training data
to “ﬁt a model” that explains the relation between x/y coordinates in the data (roundness, yellowness) and label (banana,
lemon, frog, apple), from training data that consists of pairs of x/y coordinates and labels. Through the ﬁtting interface,
“ﬁtted model” becomes part of the learning machine’s internal state, depicted as x/y coordinate areas representing certain
labels (red circles with label). This ﬁtted model can then (rightmost black arrows) be used in prediction, for inferring
labels for x/y coordinate points without a label. All operations may be inﬂuenced by hyper-parameter settings (box in
the bottom, red dotted arrows depict this relation). The implementation of the interface points is not speciﬁed and can
differ between different types of algorithms, e.g. random forest or k-nearest neighbors classiﬁer – but all classiﬁcation
algorithms follow this interface. The conceptual model for the supervised classiﬁcation “learning machine” is the totality
of this diagram, rather than any speciﬁc part of it – that is, not only the model or its mathematical speciﬁcation, but the
entire interface with abstract speciﬁcation of relations between inputs, outputs, model speciﬁcation, hyper-parameter
settings, etc.

Ultimately, the interface standpoint enables the ML practitioner to clearly specify what the algorithm does, as opposed
to how it does “it” (or something that is not further speciﬁed). Conversely, systematic adoption of a clean interface
design forces the practitioner – as a measure of scientiﬁc hygiene and transparency – to always specify the purpose that
the algorithm is meant to solve, thus leading to higher quality code, in terms of robustness, transparency and readability.

We would like to make a number of additional remarks about the interface viewpoint:

• An interface speciﬁcation can be used to deﬁne a type of learner: whenever another learner has the “same”
interface, it is considered as having the same type. For example, all supervised learners allow the modes of
access described above – speciﬁcation, ﬁtting, prediction – and this can be used as a deﬁning property subject
to assumptions on their statistical nature.

• It is important to separate the concepts of the “learner” and possible workﬂows it is part of. For example,
predicted labels from a supervised learner can then be used to obtain performance certiﬁcates for the model.
However, note that this is no longer an interaction directly with the learner, but with data artifacts which it has
produced, namely the predictions. Conceptual separation of use cases, workﬂows, interfaces and methods is

5

A preprint - January 14, 2021

common in software engineering practice, but not always present (or necessary) in methodology-oriented data
science. Conceptual hygiene in this respect allows a clear treatment of technological units, as well as clear
formal separation of means from ends.

• The interface viewpoint is a substantially different from common exposition in ML or data science, where
learners are often deﬁned in terms of a mathematical/statistical model, or the algorithms. However, this is
not a contradiction, but complementary: both the mathematical and algorithmic speciﬁcation, as well as the
interface are important and necessary to fully specify a particular learner. In Figure 1, one may imagine the
former “inside” and the latter “outside” of the “black box robot”. The distinction between internal speciﬁcation
and interface allows to formally distinguish speciﬁc learners and types of learners, also a recurring theme in
this paper.

For example,
implementations of different supervised prediction strategies in common toolboxes such
as scikit-learn share the same class/methods interface (e.g. RandomForestClassifier and
KNNClassifier). This interface also abstracted from data representation in the sense that for application to
data, only the reference to the supervised prediction strategy needs to be exchanged in the workﬂow. We will see how
this conceptual property maps onto speciﬁc design patterns in section 6.2.

3.2 No solution without a problem – task speciﬁcation

A crucial aspect of the aforementioned necessity to address the mathematical set-up in architectural designs is a clean
problem speciﬁcation. Generally, ML software does not exist on its own in a vacuum, but serves a speciﬁc purpose.
On the scientiﬁc side, statement of the purpose of an algorithm is necessary for validability – if it isn’t clear what
an algorithm does, no argument can be made that it was better than doing nothing. On the practical side, only an
architecture that allows clean problem speciﬁcation is operationalizable for reproducible and transparent data science
(since absence of such architecture leaves goals unspeciﬁed and uncheckable).

We illustrate what this concretely means in the context of our running example, supervised learning. For this, we
present a (simpliﬁed) “consensus” formulation of the (supervised) learning task, as the subject of further discussion on
architecture (i.e. an example of the practitioner’s problem).2

The supervised learning task

Data speciﬁcation. Consider data consisting of feature-label pairs (X 1, Y1

) i.i.d.∼ (X , Y ), where
(X , Y ) is a generative random variable taking values in X × Y. The domain X is a ﬁnite Cartesian product of
primitive data types (real, ﬁnite/categorical, strings). The domain Y is either continuous, i.e. Y ⊆ (cid:82), or of
ﬁnite cardinality, i.e. #Y < ∞. In the ﬁrst case, the task is called “supervised regression”, in the second
“supervised classiﬁcation”.

), . . . , (X N , YN

Deﬁnition of learning. The goal is to construct a supervised learning functional f , taking values in [X → Y].
), but does not

The functional f may depend on the values of the feature-label pairs (X 1, Y1
have access to the (distribution) law of (X , Y ).

), . . . , (X N , YN

Deﬁnition of success. A (non-random), ﬁxed functional g : X → Y is considered performant/good if the
expected generalization loss (cid:69)[L(g(X ), Y )] is low for some user speciﬁed loss function L : Y × Y → (cid:82), e.g.
(cid:98)y (cid:54)= y]
the squared loss L : (
(cid:98)y − y)2 for regression, or the misclassiﬁcation loss L : (
for classiﬁcation. A learning algorithm f , possibly statistically dependent on (X 1, Y1
), is
considered performant/good if the expected generalization loss (cid:69)[L( f (X ∗), Y ∗)] is low, where (X ∗, Y ∗) ∼
(X , Y ) is independent of (X 1, Y1

(cid:98)y, y) (cid:55)→ 1[
), . . . , (X N , YN

), . . . , (X N , YN

(cid:98)y, y) (cid:55)→ (

) and f .

The three parts of the learning task are crucial. For a clear problem speciﬁcation, we need to know: given what, what
are we doing, and how do we know we did it (well)? More precisely, a task description contains:

(a) a formal data structure speciﬁcation. This must include the relational structure (in database parlance) and
statistical structure assumptions (usually generative). In the supervised learning example, the relational
structure is that of a single data table, rows being index i of X i, Yi, columns being variables/components of X i,
with one column designated as the prediction target (Y ). The statistical assumption is simple but crucial: rows
are assumed to form an i.i.d. sample.

2For a more detailed overview of supervised learning, see e.g. Hastie et al. [21].

6

A preprint - January 14, 2021

(b) a learning interface speciﬁcation. That is, at what point does the learner ingest what information and what
does it return? This is a combination of an algorithmic interface, what objects are related as inputs and
outputs of subroutines, and a statistical interface, in specifying the statistical dependence structure. In the
case of supervised learning, the functional f is computed from the data X i, Yi. As a mathematical object, f in
general depends on the X i, Yi, and is statistically independent of anything else outside the learner. The speciﬁc
algorithm by which f is computed is unspeciﬁed, but needs to follow the given form for the task.

(c) a success speciﬁcation. That is, how is success deﬁned? For a workable architecture, this needs to be
operationalizable – i.e. whether the goal has been reached should be clearly deﬁned, empirically checkable in
the scientiﬁc sense, and algorithmically deﬁned, subject to valid reasoning. Usually, the goal speciﬁcation is
mathematical-statistical, and arguing that a goal is sensible relies on theoretical arguments. In the supervised
learning example, the goal is the aim of achieving a low generalization loss, and it may be measured by
algorithms for quantitative performance estimation and comparisons of algorithms.

We make a number of additional remarks:

• Our conceptual choice to insist on a goal speciﬁcation together with the learner speciﬁcation may seem unusual.
We argue that an AI needs to follow some checkable goal, otherwise its usefulness is not only dubious, but in
fact entirely unfalsiﬁable in the sense of the scientiﬁc method. Thus, not stating the goal in using a learner is
unscientiﬁc or pseudo-scientiﬁc. Note that in many toolboxes like scikit-learn or Weka, the task is
already implicitly speciﬁed by the choice of algorithm type and scope of the toolbox (e.g. when we choose a
regressor from scikit-learn, the implied task is tabular regression).

• Of course, the goal speciﬁcation does not need to involve prediction of something, or predictive modelling
in general. Model parameter or structure inference (e.g. inspection of variable coefﬁcients) in a context of
a model interpretability task, or causal modelling (e.g. building decision rules) in the context of a decision
making task are common, important goals which are not strictly predictive, but may happen in the context
of predictive models. In either case, there is still a need to state operationalizable criteria for what makes
algorithmic outputs sensible, according to the previous argument.

• A task speciﬁcation may be seen to specify interfaces through its change-prone parts: in the supervised learning
example, the data and its type (how many columns etc); the speciﬁc choice of mathematical and algorithmic
form for the learner f ; the speciﬁc choice of loss function L. These objects are interchangeable, as long as they
comply with the task speciﬁcation. From an architectural perspective, they may be seen as natural interfaces,
and conceptual ﬁxed points to leverage for interface design.

• As a subtle point on success speciﬁcation, there is a difference between operationalizing criteria for what makes
a sensible success control (or evaluation) procedure, and the success control procedure itself. In the supervised
learning example, the expected generalization loss is an unknown (generative) quantity, therefore additional
algorithms are needed for its estimation. These algorithms may take different forms, and may be sensible to
varying degree, according to mathematical argument or proof. An operationalizable goal speciﬁcation can be
leveraged to check whether such algorithms claim to conduct a suitable form of success control.

We proceed by illustrating the necessity of items in our task model in a less common example: the learning task of
forecasting. Under the name of “forecasting”, related but distinct tasks are often conﬂated in literature.3 One reason is
that the description of the problem commonly contains only the data structure speciﬁcation, but not a learning interface
speciﬁcation. This situation may lead to pairing the wrong strategy with the problem at hand, or comparing pears with
apples. More concretely, a consensus data speciﬁcation (for the purpose of simpliﬁed exposition, equidistantly-spaced
stationary data) plus a common speciﬁcation of learning, as follows:

The forecasting task

Data speciﬁcation. The data is a partial observation of a stationary sequence of random variables (Zi, i ∈ (cid:78))

taking values in Z ⊆ (cid:82); at a given point in time τ, only Z≤τ := (Zi, i ≤ τ) are observed.

Deﬁnition of learning. The goal is to produce forecasts (cid:98)Z j for Z j, for certain j.

The problem with this task speciﬁcation (apart from the obvious lack of success control) is the incomplete algorithmic
and statistical learning interface deﬁnition: what mathematical and algorithmic form does a learner take? What may (cid:98)X j
depend on, algorithmically and statistically? There are, in fact, (at least) two very common task deﬁnitions this could be
completed to:

3For an introduction to forecasting, see e.g. Box et al. [11].

7

A preprint - January 14, 2021

Deﬁnition of learning, variant 1 (ﬁxed horizon). The goal is to produce forecasts (cid:98)Z j for Z j, for j = τ +

1, . . . , t. For this, only Z1, . . . , Zτ are available as algorithmic input to the learner.

Deﬁnition of learning, variant 2 (sliding window). The goal is to produce forecasts (cid:98)Z j for Z j, for j = τ +
1, . . . , t. To produce (cid:98)Z j, the learner has algorithmic access to, and only to, (cid:98)Z j(cid:48) with j(cid:48) (cid:12) j. In particular, (cid:98)Z j
must be statistically independent of Z≥ j := (Zi, i ≥ j), conditional on Z< j := (Zi, i (cid:12) j).

Suitable goal speciﬁcations also diverge accordingly.

In summary: without a clean task model that separates the two cases, an interface could have conﬂated “forecasters”
which are statistically incompatible. Worse, it could have done so under an identical data model and algorithm interface
– not entirely dissimilar to the problematic situation where there are multiple instances of the same power outlet, but
providing electricity at different voltages or currents4. Thus, without accounting cleanly for both algorithmic and
statistical-mathematical aspects, a satisfactory AI interface architecture cannot be constructed.

3.3 The conceptual object model

Until now, we have discussed the importance of clearly specifying learning problems and the interface viewpoint for
algorithms. We proceed by conceptualizing key objects found in typical ML workﬂows, including algorithms among
others.

There are a number of (conceptual) objects that appear repeatedly as variable parts or interface parts in a ML framework.
On the conceptual level, these objects are all of formal mathematical and algorithmic nature. Examples of recurring
objects are:

• Data frames with deﬁnable and inspectable column types, e.g. numerical, categorical, or complex (in a formal

mathematical sense),

• Non-primitive and composite data objects such as series, bags, shapes, or spatial data,
• Sets and domains, e.g. for specifying ranges, intervals, parameter domains,
• Probability distributions, e.g. arising as return types in Bayesian inference or probabilistic prediction,
• ML algorithms (sometimes also called estimators, strategies, models or learning machines),
• ML pipelines, networks, composites or entire workﬂows,
• Workﬂow elements such as for automated tuning or model performance evaluation.

An important distinction that divides these objects into two categories is that of statefulness. Being an entity object
(also: immutable) or a value object (also: mutable) is a common distinction in computer science:

• Value objects have the same content and properties independent of the condition or context
• Entity objects object have a continuous identity, and may be subject to change of content or properties

The typical “mathematical object” from classical mathematical notation is a value object – an object that, once deﬁned
in common mathematical formalism (e.g. “let x := 42”), remains unchanged within the scope of its existence. Whereas
“learning” objects, such as ML algorithms, are conceived of as entity objects – in fact, their conceptualization is typically
characterized by behavior at state changes, e.g. at ingestion of training data.

In our conceptual model we will hence single out mathematical objects that model “formal mathematical objects” in
the classical sense and are value objects, and learner objects that model learning machines and strategies, and which
are entity objects. An ambiguous case to which we will return later is the nature of complex speciﬁcations, e.g. of
algorithms or workﬂows, which are themselves value objects, but which specify entity objects.

In the list of common example above, the following are mathematical objects in this sense:

• Data frames with deﬁnable and inspectable column types, e.g. numerical, categorical, or complex,
• Non-primitive and composite data objects such as series, bags, shapes, or spatial data,
• Sets and domains,

4i.e. the beneﬁt of being able to plug in all appliances everywhere is offset by the risk of their accidental destruction, making this

interface design choice somewhat questionable, as long as accidental destruction is an undesired feature.

8

A preprint - January 14, 2021

• Probability distributions.

Note that we consider data frames and data objects as value objects, because state change is not a deﬁning conceptual
characteristic of data (although state change may be deﬁning for the special case of data streams). Change of the
raw data or data cleaning – as opposed to algorithmic feature extraction – is something we do not conceptualize as
“within” a ML framework. Overall, value objects are also those for which there is typically unambiguous convention on
mathematical notation in the ML literature.

On the other hand, the following are learner objects, and thus entity objects:

• ML algorithms,
• ML pipelines, networks, composites, or entire workﬂows,
• Workﬂow elements such as for automated tuning or model performance evaluation.

All of these are deﬁned by operations or actions which change an intrinsic “state” of object. For example, in ML
strategies, typically one or multiple data ingestion steps are carried out, e.g. “ﬁtting”, which changes the strategy’s state
from “unﬁtted” to “ﬁtted”.

It is important to note that the ML literature disagrees on conceptualization of ML strategies in terms of mathematical
formalism already: for example, in the supervised learning literature, an algorithm may be represented by as a prediction
functional on test data, or a functional in which both training and test data have to be substituted, thus representing
the ﬁtting process by some of its argument (compare, for example, the framing in Hastie et al. [21, chap. 2] and ?
, chap. 1]). Besides the lack of consistency, we argue that there is also a lack of expressivity which prevents clear
conceptualization on both mathematical and algorithmic levels. We will attempt to remedy this situation partly by
introducing some formalism.

We continue with further details on the conceptual models for value and entity objects.

3.4 The conceptual model for mathematical objects

Typically, mathematical objects (as previously introduced) appear both as internal concepts, e.g. in representation or on
internal interfaces, as well as elements of user interaction, e.g. in interacting with data, specifying models, or inspecting
results of modeling.

In either case, there are common interface and interaction cases in dealing with mathematical objects, i.e. what one
would informally call a “formal mathematical object”. These common interface/interaction cases are:

(i) Deﬁnition: creating inspectable variables that are mathematical objects. For example: “let X be a normal
random variable with mean 0 and variance 42; or, “let myseries be a discrete time series with values in
(cid:82)3, of length 240, time stamps being [...] and values being [...]”.

(ii) Value or parameter inspection: “what is the mean of X ”; or, “what is the value of myseries in January

2020?”

(iii) Type inspection: querying the mathematical type (e.g. domain) of an object: “what domain does X take

values in?”; or, “how long is myseries?”

(iv) Inspection of traits and properties: “is the distribution of X symmetric?”; or, “are the time stamps of

myseries equally spaced?”

By deﬁnition, state change is not an interface case for value objects, since the deﬁnition characterizes value objects as
objects without such an interface or interaction point. Qualitatively, these are also not properties one would commonly
associate with mathematical objects, as per implicit conceptualization and common use. Thus, “classical” mathematical
objects are always value objects in our conceptual model.

Based on these interface cases, we postulate that the following are intrinsic attributes of a mathematical object:

(a) a name or symbol, e.g. X , or myseries
(b) a type, e.g. a “real random variable”, or “real univariate time series”
(c) a domain, e.g. “the set (cid:82)”, or “absolutely continuous distributions over [0, 1]”
(d) a collection of parameters on which the object may depend, being mathematical objects themselves, in

particular possessing name, type and domain themselves5

5Of course, there may not be inﬁnite recursion. Therefore, eventually there are nested parameters without parameters.

9

A preprint - January 14, 2021

(e) traits and properties, e.g. “this random variable has a ﬁnite domain”, or “this time series is univariate”

We postulate these attributes as inextricable attributes based on the related interaction and interface cases. In particular,
we postulate that every mathematical object should be considered with a domain (which could be the universal domain),
and a designated collection of parameters and properties (which could be the empty collection).

As an example for a mathematical object and its attributes, consider the common deﬁnition “Let X be a r.v., distributed
according to the normal N(0, 42)”. Here, one could deﬁne (a) type being “random variable”; (b) being “real random
variable”, (c) domain being “absolutely continuous r.v. over (cid:82); (d) parameters being µ := 0, σ2 := 42, with types of
µ, σ numeric/continuous, domains being µ ∈ (cid:82), σ2 ∈ (cid:82)+; and (e) example traits as X being symmetric, sub-Gaussian,
mesokurtic.

An important subtlety here is deﬁning a mathematical object with variable free parameters, in contrast to a mathematical
object with variable but set parameters, e.g. a generic N(µ, σ2) in contrast to a concrete N(0, 42).

In terms of implementation: Attributes (a) and (b) are already commonly found as deﬁning attributes of any variables in
the base language of common programming languages (e.g. in Python or R usage) and can therefore be regarded as
standard even beyond conceptualization of mathematical objects. Attributes (c), (d) and (e) are typically not found in
vanilla expressivity and need dedicated implementation beyond the base language. This will be addressed by design
patterns in section 5. We will return to implementation details after completing the outline our conceptual model.

3.5 Prior art

Many aspects of our conceptual model are commonly found in the framing of ML theory, or software toolboxes –
however, to our knowledge, this has not been stated as a full conceptual model, or in the context of software design.
The concept of a “learning task” is also not uncommon in taxonomies of data scientiﬁc problems, but typically implicit –
it is rarely made explicit as a concept on its own, possibly since it is not a necessary concept in typical methodological
arguments where it appears as part of the framing. As a partial inspiration of our conceptual model, the mlr toolbox
ecosystem [7, 24] and openML API [40] has software formalism for (concrete) learning tasks, which is similar to our
(generic) tasks on the conceptual level.

4 Scientiﬁc types – a conceptual model for formal objects

The full conceptual object model that we propose relies on the new idea of “scientiﬁc typing” which we introduce in
this section. A scientiﬁc type enable us to abstract mathematical objects, based on the set of operations that we usually
perform with them (i.e. their interface) and key statistical properties, in a way that is both precise enough to be usable in
mathematical statements, but also implementable in software. For example, we may say that a learning algorithm is
of the type “forecaster” where the type in question has a precise mathematical meaning and an associated software
template. To introduce this idea, we extend classical mathematical formalism to stateful objects, along the lines of von
Neumann formalism and modern compiler theory [33].

In the following sections, we will leverage the idea of scientiﬁc typing to derive software design principles and patterns
for implementing the respective formal mathematical objects (e.g. algorithms) in software.

4.1 Scientiﬁc typing for mathematical objects

Having established conceptualization of mathematical objects, we now introduce some accompanying mathematical
formalism. For this, we make use of common formalism in type theory, extended with some less common and novel
aspects. As we would like to make parameters and properties explicit attributes, we leverage and extend classical
notation and type theory.

The general concept of a “type” arises both in mathematical formalism, as well as in modern programming languages
[33]. In both frameworks, any object may have one or multiple of the following:

(a) a name or symbol, e.g. x

(b) a formal, symbolic type, e.g. an “integer”

(c) a value, e.g. 42

10

A preprint - January 14, 2021

In mathematical notation, the “typing colon” is used to denote an object’s type, e.g. the formal statement x : int for
stating that the object denoted by symbol x is of type int. Seasoned programmers will be familiar with the notation in
their favored typed language6, which usually depends on the language (for example, the statement int x in Java).

Assigning a type to a symbol or value is a purely formal act and in-principle arbitrary; it becomes practically useful
through typing rules (and/or mathematical axioms). Most importantly, any type has inhabitants, that is, values that are
admissible for a type. For example, the formal type “integer” is inhabited by the numbers 0, 1, -1, 2, -2, and so on. As
such, mathematical types can often be identiﬁed with their inhabitant sets7, such as stating x : (cid:90) for an integer symbol
x.

From primitive types, type operators allow construction of more complex composite types. Examples are the conjunc-
tion8 (which can be identiﬁed with the Cartesian product × for sets) and the arrow “→”, which allow to construct
function types. This is developed in Table 1 by examples that should highlight the general pattern.

Table 1: Some examples of type speciﬁcation in pseudo-code and formal mathematical type notation
Pseudo-code (typed)
int x

Description

x is an integer

int x = 42

func int f(int)

Mathematical (type theoretic)
x : (cid:90)
x = 42 : (cid:90)
f : (cid:90) → (cid:90)

x is the integer 42

f is a function which takes as input an
integer and outputs an integer

f is the function which takes as input an
integer and outputs its square

x is a pair made of: an integer (1st), and
a real number (2nd)

func nat f(int x) return
x*x
[int,real] x

f : (cid:90) → (cid:78); x (cid:55)→ x 2

x : (cid:90) × (cid:82)

f is the function which takes as input two
integers and outputs their sum

func int f(int x, int y)
return x+y

f : (cid:90) × (cid:90) → (cid:90); (x, y) (cid:55)→ x + y

Notes: We express the example statements in vernacular description (ﬁrst column), pseudo-code of a stylized typed programming
language (second column), and formal mathematical type notation with inhabitant-set identiﬁcation convention (third column).
Mathematical type operators used in the third column are the conjunction type operator, denoted as ×, and the arrow (type
construction or function) operator, denoted as →. A reader may recognize the colon commonly used for range/domain speciﬁcation
in mathematical function deﬁnition as the typing colon, and the arrow in function deﬁnition as the arrow operator (such as in
third/fourth row, third column); whereas the typing colon for domain statement of a variable (such as in ﬁrst row, third column)
would be unusual in contemporary usage, outside of formal type theory.

Another important composite type is that of a structured type. Concretely, a class type is a container with named
types in “slots” (formally: component types) which correspond to (object) variables or functions. The inhabitants of a
structured type are structured objects, where every slot is inhabited by concrete value inhabiting the slot type. While a
more formal discussion can be found in [33], there are no commonly used notational conventions. We proceed with the
following ad-hoc conventions, inspired by UML class diagrams [25]:

• Structured types are denoted by a Python-like struct type header, followed by the slots in the composite,

with name and type. Slots are symbol/type pairs, separated by a typing colon.

• Inhabitants of structured types are called structured objects and denoted by a struct header, followed by

slots in the composite, with name and value. Slots are symbol/value pairs, separated by a typing colon.

• The slots are categorized under one of three headers: params, for “parameter” slots that are immutable per
inhabitant (sometimes also called static); state for state variables; and methods, which are of function
type and correspond to object methods.

6Even in weakly typed, duck typed, or untyped languages, there are usually mechanisms at the compiler or interpreter level that

correspond to formal typing, even if they are not exposed to the user directly.

7Footnote for readers with a theoretical mathematical or methodological background: It should be noted that the statement x : (cid:90)
(“x is of type (cid:90)) is different in quality from saying x ∈ (cid:90) (“x is an element of the set (cid:90)”), even if, as a statement, the one is true
whenever the other is. The difference lies in that the ﬁrst is interpreted as an attribute of the object represented by x (“the object
represented by the symbol x has integer type”), the second as a property of the value of x (“the value of the object represented by x
is an integer”). While this may not be much of a difference on the conceptual level in mathematics, where symbols only serve as
mental reference, there is a substantial difference on the implementation level in a computer, where the symbol may be attached to an
object or in-memory representation which implements the conceptual model. On the formal side, this interpretation is also reﬂected
through differences in axiomatization of type systems, versus axiomatization of set theory.

8One also ﬁnds the symbol “(cid:117)” in literature.

11

A preprint - January 14, 2021

• Implementations (and behavior) of methods can always (but need not) depend on parameters in params as

function parameters.

• If a is a structured type or structured object, with a slot of name y, the slot can be directly referred to by the
symbol a. y, i.e. the symbol for the structured type/object, followed by a dot, followed by the symbol for the
slot name.

For example, the structured type GaussianDistributionSignature which represents the (input/output
signature type) of a Gaussian distribution object (represented by cdf and pdf) could be deﬁned as follows:

struct type GaussianDistributionSignature

params

µ
σ

: (cid:82)
: (cid:82)+

methods cdf : (cid:82) → [0, 1]

pdf : (cid:82) → [0, ∞)

An inhabitant of this type would be the structured object

a :=
struct

µ
σ

params

=
=
methods cdf =
pdf =

1
4
x (cid:55)→ Φ (cid:0) x−µ
σ
x (cid:55)→ 1
(cid:112)
2π exp

(cid:1)

σ

(cid:128)− 1

2

(cid:0) x−µ
σ

(cid:1)2(cid:138)

where Φ is the standard normal cdf. As a typing statement, we can write a : GaussianDistribution. To refer
to the cdf, we can write a.cdf which is identical to the function x (cid:55)→ Φ (cid:0) x−1
It is important to note that the type GaussianDistributionSignature does not fully deﬁne what is a
Gaussian distribution object (as represented by pdf/cdf). In computer scientiﬁc terms, the implementation is missing; in
mathematical terms, the speciﬁcation or deﬁning property is missing. For example, the structured object

(cid:1)

4

struct

µ
σ

params

=
=
methods cdf =
pdf =

1
4
x (cid:55)→ 2µ
x (cid:55)→ 42

also formally inhabits the structured type GaussianDistributionSignature.

To ﬁx this issue, one can now deﬁne the scientiﬁc type of a Gaussian distribution object by adding the compatibility
between the parameters and methods into the deﬁnition. For example, by deﬁning the following: “An object d has type
GaussianDistribution if it has structured type

struct type
µ
σ

params

: (cid:82)
: (cid:82)+

methods cdf : (cid:82) → [0, 1]

pdf : (cid:82) → [0, ∞)

and if the methods have the following form: d.pdf(x) = Φ (cid:128) x−d.µ
d.σ

(cid:138)

, and d.cdf(x) = 1
(cid:112)
2π exp
d.σ

(cid:16)

− 1
2

(cid:128) x−d.µ
d.σ

(cid:138)2(cid:17)

.”

While it is somewhat cumbersome in this case, we have deﬁned, a scientiﬁc type GaussianDistribution. In
our conceptual model, a scientiﬁc type combines:

• A structured type signature with parameters made explicit,

12

A preprint - January 14, 2021

• Assumptions on formal properties that inhabitants of the type need to satisfy, in the form of mathematical

deﬁnition and extrinsic to the structured type signature.

In particular, any object with the type GaussianDistribution will, by deﬁnition, satisfy the compatibility
between parameters and methods expected intuitively from a Gaussian distribution. We can also deﬁne inhabitant sets
in a similar way to deﬁne domains.

Scientiﬁc types, or scitypes for short, as introduced above by example, satisfy the postulates from our conceptual model
of mathematical objects in the following way:

(a,b) name and type statement are intrinsic to the notation/deﬁnition of the structured type

(c) domains can be speciﬁed as inhabitant sets
(d) parameter statements are intrinsic to our extended notation/deﬁnition of the structured type, in the params

block.

(e) traits and properties are encoded by the slots in the methods block.

A reader may like to note a few things:

• While the initial deﬁnition of a scitype may be a bit cumbersome, later reference to the type or an arbitrary
inhabitant can be invoked with a few symbols, e.g. “let d : GaussianDistribution”, and methods or
parameters may also be invoked without prior appearance in the binding statement, e.g. d.µ.

• The conceptualization resolves some items of common ambiguity of mathematical language. For example,
which symbols are to be considered parameters? The resolution is: parameters are explicitly designated as a
matter of deﬁnition. Or more subtle ambiguities around concept identity such as of a “distribution” which,
across literature, may be deﬁned or denoted in terms of different methods or properties (e.g. distribution
deﬁning functions, measures, maps, etc), depending on source and taste of the author. The resolution is: the
object need not be made or deﬁned identical with any of its methods, it “has” the methods as attributes.

• Subject to some formalization burden, the concept of scientiﬁc types can be “properly” formalized, along the
lines of classical type theory. This is not complex and simply amounts to deﬁning inhabitance by satisfaction
of a deﬁnition property, and should be intuitively clear from the example – for readability, we refrain from
adding further formal overhead.

Scitypes for mathematical object may not be directly necessary or fully novel in isolation. However, in the next section,
we will further build on scitypes for mathematical objects to complete the conceptual model for learner objects.

4.2 Scientiﬁc typing – towards a language of ML toolboxes

We continue with introducing our conceptual model for learner objects (which are entity objects), along with an
extension of the scitype concept introduced above, which will be crucial to ﬁnalize our proposed conceptual model.

A central property of modern AI methods and pipelines is their statefulness and compositionality – that is, objects
changing their state in interaction with data, and consisting of multiple parts whose algorithmic methods interact
in intricate ways. Common examples are typical supervised pipelines ingesting data, extracting features, making
predictions; or, neural networks, constructed from layers between which information is propagated. We will use these
two examples – pipelines and neural networks (in their simplest form) – as running examples for the discussion in this
section. The goal is to further develop our conceptual model by developing a formal answer to the question “what is a
pipeline” or “what is a neural network”.
In contemporary toolboxes such as scikit-learn (pipelines) and keras (neural networks), the architectural
design mirrors the common mathematical or methodological conceptualization of composition. In the examples, this
composition is of different types of estimators in pipelines (e.g. feature extractors or predictors) or layers in neural
networks. The composition design is usually implemented through so-called structural object-oriented patterns, which
we will discuss in greater detail later on. The goal for this section is to explain the general ideas by which we can make
precise the above in the two running examples. We later show how these give rise to natural architectural patterns.

Ultimately, we would like to be able to make precise statements that go beyond vague and qualitative description, i.e.
the formal counterpart of, say, an intuitive depiction such as in ﬁgure 1, within a formalism which allows:

(i) Formal conceptualization of, and distinction between, different kinds of algorithms and construction principles,

13

(ii) Abstract deﬁnitions involving these, such as “a sequential pipeline is a mapping (Transformer)n ×
SupervisedLearner → SupervisedLearner with the following properties [...]”, or “a
multi-layer neural network is constructed as InputLayer × (Layer)n × OutputLayer →
NeuralNetwork”,

A preprint - January 14, 2021

(iii) Formally meaningful

expressions

for

Pipeline(PCA(),SVM())
Layer2(), OutputLayer()),
scikit-learn , TensorFlow [1] or PyTorch [31]).

similar

or

of

as
speciﬁcation
NeuralNetwork(InputLayer(), Layer1(),
(e.g.

algorithmic

composites

state-of-art

toolboxes

syntax

such

the

to

in

We argue that the natural formal framework for a conceptual model of AI architectures is rooted in the theory of
higher-order type systems, which are at the foundation of modern programming languages, as well as of modern
mathematical formalism (often implicit). The reason is that higher-order type systems allow to make explicit the kind
of component, its operations, their dependency on settable parameters and input-output relationships, in a precise way
that is usable within mathematical statements, but also as a basis for computation-based reasoning that is machine
implementable. As such, identifying the “right” higher-order type system for AI architecture is also the ﬁrst step
towards an AI-speciﬁc higher-level declarative language for model speciﬁcation, execution, and deployment – similar
to how modern programming languages are removed in abstraction from assembly code speciﬁcation.

Such a type system can be achieved by extending the scientiﬁc typing system for mathematical objects presented above
– resulting in a formal conceptual framework for discussing types of ML components.

• Class types are denoted by a Python-like class type header, followed by the slots in the composite, with

name and type.

• The slots are categorized under one of three headers: params, for “parameter” slots that are immutable
per inhabitant (sometimes also called static); state, for mutable slots that change with state changes of an
inhabitant; methods, which are of function type and correspond to object methods.

• Implementations (and behavior) of methods can always depend on parameters in params as function

parameters.

• When specifying a type, a name of a symbol in scope can be used instead of its type. If a symbol in state is
used in this way in a function in method, the function is interpreted to write to, or read the value from that
state.

• Inhabitants of a class type are deﬁned by values for params and methods, while the state is not made

explicit (as it can be changed by objects).

For example, the class type of a class PowerComputer which computes the n-th power of a number in one of its
slots could be

class type PowerComputer

params
state
methods store_number

n
x

compute_power :

: (cid:78)
: (cid:82)
: (cid:82) → x
x → (cid:82)

Here, the exponent n is a parameter to the class, store_number is of type (cid:82) → (cid:82) and writes to x; the method
compute_power is of type (cid:82) → (cid:82) and reads its input from state variable x. The “obvious” implementations differ
by choices of the parameter n, for example

object Squarer
n = 2
params
methods store_number

:
compute_power :

z (cid:55)→ z
z (cid:55)→ zn

is an inhabitant of this class type which computes the square, and it is an instance of the class

14

A preprint - January 14, 2021

class PowerComputerClass
n : (cid:90)
x

params
state
methods store_number

: (cid:82)
:
compute_power :

z (cid:55)→ z
z (cid:55)→ zn

PowerComputer;

where in said instance the parameter n is set to the value 2. We can thus write, formally, Squarer
in common object/instance constructor syntax, one has Squarer :=
:
PowerComputerClass(n = 2). Note the difference between object, class, and class type: an object is an
instance of a class, both of which have the same class type, inhabited by the object (= value). A class type does not
specify a particular implementation as a class, multiple classes may inhabit the same type. There may be multiple
distinct instances of a class, each with their own parameter values and states.

also,

We are now ready to sketch how formal typing simpliﬁes speciﬁcation in the two ML examples. In the “pipelines”
example (e.g. of pipelines as found in scikit-learn), the key objects are “transformers” and “supervised learners”. We
work through the “supervised learner” example for illustration. First, we deﬁned a suitable class types:

class type SupervisedLearner

params
state
methods fit

paramlist : paramobject
model

: mathobject
:
: X × model → Y

(X × Y)N → model

predict

considered a parametric type in N , X, Y, and where paramobject and mathobject are types of abstract
representation of parameters and model objects respectively (e.g. umbrella types).

The class type is not yet the full scitype, because:

• Conceptually, supervised learners typically can be applied to multiple choices of N , X, Y
• Classiﬁers and regressors are deﬁned by speciﬁc choices of Y
• Admissible input data types X deﬁne speciﬁc sub-types of supervised learners

We can use the class type SupervisedLearner to deﬁne the scitypes:

• A formal object x is of scitype “supervised classiﬁer on primitive features” if it is of the type that is the type
union over SupervisedLearner over the type Y being any ﬁnite set, the type X a ﬁnite product of
primitive scitypes (categoricals, numericals), and N a natural number.

• A formal object x is of scitype “supervised regressor on primitive features” if it is of the type that is the type
union over SupervisedLearner over the type Y being any measurable sub-set of (cid:82), the type X a ﬁnite
product of primitive scitypes (categoricals, numericals), and N a natural number.

Inhabitants of the scitype “supervised classiﬁer on primitive features” are speciﬁc algorithms following the speciﬁed
interface – for example, the following implementation of “always predict the majority class on the training set”:

object MajorityDummyClassifier

state
methods fit

model

: C
:

((x1, y1
(x, c) (cid:55)→ c

predict :

), . . . , (xN , yN

)) (cid:55)→ argmax

c∈C

(cid:80)N

i=1

1[ yi

= c]

where this object has no parameters, i.e. an empty parameter list. Note that this is not the only possible implementation
of “always predict the majority class on the training set”, since instead of only storing the majority class in fit, one
could store all the training data and compute the majority class later in predict. Scitypes for transformers can be
deﬁned similarly; we defer to that until section 7.

We conclude with pointing out some important conceptual distinctions related to the concept of scitypes:

• Between a class in the OOP sense, and a class type. A “class”, as in Python, usually already contains blueprint
implementations for some methods; whereas the class type does not specify an implementation for the method,
and only records its input-output type and state access signature.

15

A preprint - January 14, 2021

• It is important to understand that type systems usually present in programming languages (e.g. ﬂoat, string,
etc.), sometimes called machine types, are not the same as scitypes, nor are they sufﬁcient to inform ML design.
As argued above, the reason is that types in programming languages do not capture statistical properties which
are crucial to identify and abstract elements in the ML domain. Rather, scitypes are separate or orthogonal to
the type systems typically found in programming languages.

• We would also like to note that a scitype is not “meta-data”, i.e. data about data but rather a property of the

data.

Having deﬁned scitypes, we are now in a position to see how scitypes formalize the mapping of mathematical objects
from the ML domain onto classes and operations in a programming language. In the next section, we will derive key
design principles based on our conceptual analysis and scitypes in particular.

4.3 Prior art

We are unaware of literature instances of scitypes for learning strategies, or a mathematical formalization in the context
of type theory. The closest on the formalism side is perhaps the area of type theory for structured objects [33]. On the
conceptual side, scitypes tend to be a frequent, implicit concept appearing in discussion of algorithm types, especially in
the toolbox communities. On the software side, scitypes for data set representations (e.g. scientiﬁc column types of data
frames), have been an increasingly common idea, for example data frame column typing in base R [35], explicit scitype
hints in dabl9, and perhaps most stringently and explicitly realized in the parallel type system implemented by the
ScientificTypes.jl module of the MLJ ecosystem [9], which also introduces the shorthand term “scitype”.
Implicit scityping of algorithms can be thought of as conceptually guiding behind many existing toolbox designs
(e.g. scikit-learn [13], mlr3 [24], MLJ [8]). Though we are unaware of a fully explicit implementation
of a full (sci)type system. While at the time being it therefore only has hypothetical status, one may conjecture that
implementations may emerge as the ML software designs matures further.

5 Design principles

Having introduced our conceptual model, we now aim to derive practical design principles that can be used to develop
ML toolboxes. In particular, we propose three guiding principles for ML toolbox design:

• Architectural separation of key conceptual layers for data, algorithmic learning strategies, tasks, and
workﬂows – this is a special case of the common “separation of concerns”, “layering” or “decoupling” design
principles, applied to the conceptual categories previously outlined.

• Scitype-driven object and interface model, especially algorithmic learning strategies and mathematical
objects (e.g. data sets, distributions, models). Interface points should mirror the conceptual abstractions of
deﬁning character for the scitype. For example, parameter setting, ﬁtting and prediction deﬁning a supervised
prediction strategy and its interface. This can be seen as a special case of the “encapsulation”, “modularity”
and “interface abstraction” design principles.

• Declarative syntax and symbolic speciﬁcation, motivated by the scitype formalism and minimizing boiler-
plate code. For example, speciﬁcation of supervised learning pipelines should be as close as possible to their
mathematical deﬁnition in the scitype formalism. This can be seen as a special case of “symbolic abstraction”
and “language abstraction” design principles.

We proceed by laying out more details on each of these principles and how they are obtained from the conceptual model,
using some of the running examples.

5.1 Architectural separation of conceptual layers

To identify the right architectural layers means identifying key abstraction points – i.e. parts of the use case that may be
“switched out” while others are kept constant. For example, the same algorithm can be applied to different data sets, or
two algorithms of a similar kind may be applied to the same data set. Thus, both algorithms and data sets should be
abstraction points.

In this vein, based on the conceptual model one can identify four key abstraction points and thus architectural layers:

9https:\github.com/amueller/dabl

16

A preprint - January 14, 2021

• Data sets and data related concerns; representation and manipulation of concrete data tables. For example, the

concept of a data frame and operations on it.

• Algorithmic learning strategies and related concerns; speciﬁcation of concrete learning strategies, and learning
strategy speciﬁc operations such as data ingestion or application of the algorithm. For example, the concept
of a speciﬁc supervised prediction algorithm, such as a random forest, including functionality for ﬁtting and
prediction.

• Learning tasks and related concerns; speciﬁcation of the aim of learning, performance indicators. For example,
the supervised learning task in abstraction, or in concrete reference to a data set and speciﬁc performance
metrics such as the mean squared predictive error.

• Workﬂows and related concerns; as special cases, workﬂows for evaluation, inspection, deployment or
monitoring, with key concerns being speciﬁcation and execution. For example, the concept of a predictive
benchmarking experiment, or model interpretability diagnostic.

We postulate that these layers should be the basis for abstraction within and separation between, since they delineate
“independent moving parts” in the conceptual model. While there are some intrinsic conditionalities and dependencies
between the layers (e.g. only certain workﬂows make sense for supervised learning algorithms or tasks), varying parts
from key use cases are typically localized within just one of the layers, e.g., choice of data set, choice of learning
strategy, etc – in line with our conceptualization of the task in section 3.2.
We derive some important consequences from this postulate, to address some common pitfalls or anti-patterns10

• Separation of learning strategies and data. Models can be ﬁtted to data or applied to data, but speciﬁcation
of learning strategies should be separate from how the data is speciﬁed. Algorithmic application of learning
strategies, such as ﬁtting or prediction, should be separate from operations intrinsic to the data.

– Important consequence 1: Software architecture of learning strategies should be abstract in the sense that

it can be applied to “data sets of a certain kind”, e.g. data frames.

– Anti-pattern 1: Writing code where the learning strategy is speciﬁc and hence inseparable from a particular
data set. The level of generality depends on the use case, but it usually should be substantially beyond a
“data frame with the same column names”.

– Important consequence 2: Data cleaning operations sit in the data layer, data transformations for the
purpose of modeling sit in the learning strategies layer. This may put the same operation in either
depending on purpose. For example, record removal for cleaning purposes to create a consolidated data
set should sit in the data layer. Record removal to prepare ﬁtting of a prediction strategy should sit with
that strategy. Missing data imputation, for the purpose of applying an algorithm that does otherwise not
support missingness, should usually be part of a learning strategy pipeline that includes the imputation, as
the purpose is not intrinsic to the data.

– Anti-pattern 2: Conﬂating concerns of modeling, tuning or model evaluation with data preparation, e.g.
locating imputation or feature extraction for the purpose of modeling not with modeling but with data
loading; or, making the creation of training/test folds for tuning an algorithm part of the data loading
and cleaning workﬂow. It is worthwhile noting that the software engineering anti-pattern of conﬂating
conceptual and architectural layers also easily enables common methodological mistakes along the lines
of information leakage.

• Separation of learning strategies and task speciﬁcation. Fitting and application of learning strategies
requires task information (e.g. target variable, forecasting horizon), but strategy speciﬁcation should be separate
from task speciﬁcation.

– Important consequence 3: Software architecture of learning strategies should be abstract in the sense that
it can be applied to “tasks of a certain kind”, e.g. general supervised prediction tasks, or forecasting tasks.
For example, a general supervised learner should be able to take data sets with any feature/label columns;
a forecaster should work with a number of different forecasting horizons. As passing concrete data sets,
this information should be recognized by, but extrinsic to the learning strategy.

– Anti-pattern 3: An architecture that fails to separate or enforces speciﬁcation of learning strategies and
tasks at the same time. For example, requiring that the parameters of an SVM be speciﬁed together with
the column names to be predicted, that is, an architecture where representation of an abstract learning
strategy is not possible without a concrete speciﬁcation of how it is applied to a speciﬁc learning problem.

10Anti-patterns refer to common but poor solutions to a recurring software design problem in the spirit of Brown et al. [12].

17

A preprint - January 14, 2021

• Separation of learning strategies and workﬂows such as performance evaluation and monitoring. A
learning strategy may be deployed in accordance with a concrete task or within a workﬂow, but the workﬂow
in which a learning strategy is used should be separate from the learning strategy itself.

– Important consequence 4: Concerns speciﬁc to advanced model performance evaluation workﬂows (e.g.
train-test splits, cross-validation, loss functions) should not be considered part of learning strategies or
their design. As before, the location of a speciﬁc methodological concept should be motivated by the
purpose. For example, re-sampling and loss estimation for the purpose of model tuning is part of a
learning strategy, thus should be part of the learning strategy layer. By contrast, re-sampling and loss
estimation for the purpose of performance evaluation is with the purpose of evaluation, thus should be
part of the workﬂow layer.

– Anti-pattern 4: Conﬂation of learning strategies and performance evaluation, e.g. a design where eval-
uation and tuning are considered identical. As in anti-pattern 2, the design mistake facilitates (but is
not identical to) common methodological mistakes, such as evaluating on the training set, or conﬂating
validation and test splits.

– Important consequence 5: Concerns related to monitoring and scrutiny of a learning strategy (e.g. logging,

diagnostics) should be separate from and external to the learning strategy.

– Anti-pattern 5: The “learning strategy that scrutinizes itself”, conﬂating the concern to carry out learning
with the concern to check whether or how it is carried out. As in anti-pattern 2 and 4, the design mistake
facilitates common methodological mistakes, such as circular reasoning in certifying reliability of a
learning strategy.

5.2 Scitype-driven encapsulation

The next set of design principles concerns object-level architecture. Our key postulate is that the design of architectural
units, especially classes and objects, should be driven by scitype, as identiﬁed in conceptual analysis, and formally
deﬁned for algorithmic and mathematical objects. The main reason for postulating this design principle is its centrality
in our conceptual model, which we argue is a formalization of typical data scientiﬁc conceptualization. According
to domain-driven design [16], such conceptual lines should guide architectural decisions, especially if objects are
delineated as mathematical or algorithmic units. Following this reasoning, the postulated correspondence between
scitype and architectural units should be taken into account for:

• Deﬁnition of modules, classes and objects; especially when considering which functionality to “bundle”
• Interfaces of modules, classes, and objects; especially when considering key interface points
• State change schemas of entity objects and classes implementing entity objects

We derive some important consequences for architecture in languages with class/object features:

• Classes, class interfaces, and class templates as the simplest possible representation of scitype. Given
scitypes identiﬁed in the conceptual model, classes and class templates should be primarily considered as
implementation of scitype inhabitants. A scitype deﬁnes a class interface that multiple classes can follow,
ensuring non-proliferation of classes and templates.

– Important consequence 1: Key interface points in the conceptual model or user journey deﬁne the class
architecture for instances of a given scitype. This can be combined with inheritance or template patterns
(see section 6.2 below). For example, the scitype of a supervised learning algorithm maps onto the
familiar fit/predict interface in scikit-learn .

– Anti-pattern 1: Individual objects of the same scitype being implemented in separate, disconnected parts
of the architecture; or, interfaces that are inconsistent between different algorithms of the same scitype.
– Important consequence 2: Scitypes deﬁning entity objects with non-trivial state changes should map onto
classes, interfaces, or other architectural motifs that ensure that the state and its change remain attached
with the “entity”. For example, a supervised prediction algorithm retains its identity when ﬁtted because
the identity as an entity is part of the conceptual model.

– Anti-pattern 2: In implementation architecture, the different states of entity objects are separated. For
example, ﬁtting a supervised prediction algorithm produces a “ﬁtted model” which does not allow to trace
back its entity lineage to the model and parameter speciﬁcation that is ﬁtted.

• Operations with conceptual objects are methods of classes, operations on conceptual objects are opera-
tions on classes. Operations of conceptual objects (e.g. model ﬁtting or application of a ﬁtted model) should
be methods of the class implementing that conceptual object. Operations that work on conceptual objects (e.g.
pipeline building) should be operations on (or with) classes.

18

A preprint - January 14, 2021

– Important consequence 3: During conceptual analysis, use cases, user journeys, and mathematical
deﬁnitions should be scrutinized for which operations of an object are key, in the sense of being a deﬁning
property of its scitype. In the architecture design, these operations should be methods of the class that
implements the object, or otherwise closely coupled to the object (such as by being a queriable part of the
same module). For example, ﬁtting a model should be a method of the class that implements the model.
– Anti-pattern 3: Representation of conceptual objects is architecturally uncoupled from functionality that
is considered “part” of it in the conceptual model. For example, an architecture where it is impossible to
ﬁnd the function that ﬁts a supervised prediction model by application of code, given only the supervised
prediction model.

• Important operations on objects with a scitype deﬁne a higher-order scitype. A higher-order scitype
should be considered in design if it re-occurs in the use case. For example, composition of individual learning
strategies to a pipeline, tuning or ensembling. A higher-order algorithm scitype found during conceptual
analysis should be considered a ﬁrst-class citizen, e.g. in terms interface design and implementation via classes.
– Important consequence 4: A good design will treat “atomic” (zeroth-order) scitypes (such as supervised
learners) and “higher-order” scitypes (such as composition and tuning meta-strategies) consistently. In
particular, class and interface designs will be consistent, inter-operable, and compatible between “atomic”
scitypes and “higher-order” scitypes.

– Anti-pattern 4: Higher-order scitypes being second-class citizens, e.g. through implementing atomic
scitypes as classes, while implementing higher-order scitypes (like tuning) as functions; or, lack of
encapsulation and deﬁned interfaces in higher-order scitypes.

We would like to point out that on ﬁrst sight, some of our stated consequences may seem to exclude functional
programming paradigms and favor imperative (classical) object orientation, e.g. insisting on lineage of entity objects
or methods. This is not true. While it is less enforced by other languages, entity lineage (the object remembers its
“identity”) and method lineage (methods “belonging” to an object can be queried) is implementable in functional object
orientation paradigms such as in R or Julia, e.g. typed dispatch systems [15].

5.3 Declarative syntax and symbolic speciﬁcation

With the scitype formalism readily available, it is a natural idea to leverage it for architectural design, as discussed
above. We go one step further and postulate that scitype should determine syntax of usage, in a notebook-style user
journey or in code development. Most importantly, this should cover the following instances:

• Speciﬁcation of formal algorithmic or mathematical objects, especially of parameter settings, composites, and

higher-order constructs

• Execution or application of formal algorithmic or mathematical objects, especially when using scitype-deﬁning

methods

• Inspection of formal algorithmic or mathematical objects, especially when inspecting parameters, properties,

or state

We motivate this design principle from domain-driven design, more speciﬁcally the principle that architecture should
follow the conceptual model [16]. Since formal algorithmic or mathematical objects are key concepts in the conceptual
model, and manipulation of said objects arises in key interface points and use cases, it follows that the instructions for
such manipulation should be as close to the formal conceptualization as possible – in terms of mathematical objects and
scitypes.

We list a few requirements arising from this high-level principle:

• Encapsulation of low-level functionality. The highest level of code abstraction should be at least at the
level of abstract mathematical and algorithmic objects, and scitypes. It should not be necessary to access or
conﬁgure speciﬁcs of implementation. This is a special case of the “no boilerplate code” design principle,
where, in accordance with the high-level principle, boilerplate is deﬁned as being lower-level than the key
conceptual objects.

• Parsimony of syntax. Syntax of speciﬁcation and usage should be as simple as possible, while being at a

sufﬁcient level of abstraction. This is a special case of the “simplicity” design principle.

• Congruence of syntax and conceptual semantics. Syntax of speciﬁcation and usage should follow, as
closely as possible, the conceptual model of formal algorithmic and mathematical objects, especially the
scitype formalism.

19

A preprint - January 14, 2021

• Consistency of syntax across objects.

Syntax of speciﬁcation and usage should be consistent across
different formal algorithmic and mathematical objects, and across different levels of abstraction. This should
in particular be the case for features or properties shared across objects or layers, such as the representation of
parameters, states, or methods, across objects with different scitypes.

We will use these requirements in developing our generic design patterns in the next section.

5.4 Prior art

As stated in the introductory section 1, we are unaware of a literature reference that formulates re-usable design
principles speciﬁc to ML toolboxes, and systematically ties these to concrete design patterns. To our knowledge, there
is also only a single references discussing high-level principles in the context of the design of a speciﬁc toolbox, [13],
which contains some of the consequences discussed here; compare Section 5.2 consequence 1 with “non-proliferation”,
consequence 3 with “composition”, and layer separation in Section 5.1 with the banana/gorilla/jungle metaphor , and
“consistency” in Section 5.3 with “consistency”.

6 Design patterns

Having derived a set of design principles, we are now in a position to propose re-usable design patterns building onto the
well-known patterns in Gamma et al. [17]. Using object-oriented programming (OOP), each design pattern motivates,
describes and names an important recurring design in ML software. Design patterns make it easier to reuse successful
designs and architectures by making them more accessible to developers of new toolboxes.

We start by reviewing core concepts of OOP and argue that object orientation is a suitable language paradigm to
implement patterns in accordance with the design principles outlined in the previous section. We then develop key
design patterns for the general data scientiﬁc setting, drawing on object-oriented patterns as described for example in
Gamma et al. [17].

In the next section, we illustrate these patterns by examples from contemporary practice and provide a collection of
implementation blueprints.

6.1 The case for object orientation in AI architecture

A core architectural principle – which has already stood the test of time as part of the existing major AI toolboxes11 – is
that of representing learners as objects, in the sense of the programming paradigm commonly called “object orientation”
or “object oriented programming” (OOP). Arguably, one may also consider this paradigm to extend more generally, to
data or mathematical objects in the sense of section 3.4.

OOP is the predominant paradigm for complex ML/AI software and often assumed without rationale. In this section,
we provide a brief overview of the core characteristics of OOP, and more importantly, give key reasons why OOP is
especially well suited for constructing ML/AI toolboxes.

OOP is a general, not ML-speciﬁc motif supported by a programming language, and may take various forms depending
on the host language. For example, it may be of imperative, functional or mixed ﬂavor. Some examples relevant for data
science are: pure class-based OOP in Java or Python; functional, dispatch-based OOP in Julia or Go; and the various
mixed OOP paradigms in R (S3, S4, R6).

Deﬁning features of OOP are:

• Structured objects with variables, e.g. a “duck” type object which may contain object (state) variables such as

number_of_feathers, or is_dead;

• Methods, which are functions speciﬁc to, and/or belonging to the aforementioned structured objects; e.g. a
function quack, waddle, or remove_one_feather, which may mutate internal states, and/or which
may return an output.

• Distinction of classes and objects, with a “class” being a generic blueprint for a concrete “object”, where the
class-object relationship is similar to the type-variable relationship, e.g. type being integer vs a concrete
integer typed variable with value 42. For example, there may be multiple concrete “object” instances
Huey, Dewie, Louie of a generic class called duck.

11To name a few: Weka [20], mlr3 [24], scikit-learn [13].

20

A preprint - January 14, 2021

• Class inheritance (or type ordered dispatch in functional OOP) and object type polymorphism. Informally,
inheritance allows us to model a containedness hierarchy of classes, e.g. duck being a “sub-class” (i.e. a
kind) of bird. Object type polymorphism means that methods, object variables, and functions for classes
deﬁned as a sub-class will automatically default to those of the super-class, unless overwritten. e.g. it makes
sense to deﬁne generic number_of_feathers and waddle for a class bird, then sub-class duck
and penguin, where quack is implemented only for duck.

Optional, common features of OOP are:

• Class-speciﬁc constructor and destructor methods, called for creation and upon destruction of an object;
• Strong typing, weak typing or duck typing, depending on whether and how a class is considered as a type

according to the language’s type system;

• Distinction of public, private or protected for variables and methods, depending on whether access is possible

from outside an object or not;

• Distinction of static vs mutable objects, variables or methods, depending on whether they may be changed

after construction of the object;

• Distinction of class variables/methods vs object variables/methods, depending on whether they are common to

all objects of a class, or only speciﬁc to an object instance;

• Multiple inheritance, decorators and mix-ins, depending on whether a class may simultaneously inherit from

multiple super-classes, or have behavior modiﬁed by subsidiary templates;

• Multiple dispatch, depending on whether methods may dispatch on more than one inheritance/order hierarchy.

Most of the optional features may be mimicked by architectural patterns in an OOP paradigm with the deﬁning features,
even if not available as part of the host language. Examples are adoption of a naming convention for variables or
methods, to signify private/public access; or, to signify constructor/destructor behavior for methods.

We argue that OOP is the natural software engineering paradigm for an architectural framework for ML/AI. The typical
use case for OOP is for a conceptual model involving objects which

• may exist in multiple instances (objects) following a common blueprint (class), e.g. instances 1 and 2 of linear
model on data batch A; instance 3 of linear model on data batch B; or different instances of an entity object,
such as different instances of probability distributions

• possess multiple internal states between which they can transition, e.g. newly set-up model vs ﬁtted model

after data ingestion

• are subject to a deﬁned collection of actions or methods run repeatedly within, or across, instances – e.g.

ingesting data for ﬁtting or predicting with a supervised learner

• interact via deﬁned interfaces with each other, e.g. supervised learner object ingesting data from a data

container object using a fit method; or AI pipeline object being composed of learner objects

Table 2 collates further parallels between natural features of the introduced conceptual models and features of OOP.

The main architectural patterns for AI, to be discussed in this paper, arise in the context of OOP models of the above
ﬂavor, as application of so-called architectural design patterns in OOP. These, together with some necessary formalism,
will be further expanded upon in detail, in section 6.2 and thereafter.

Generally, an “OOP design pattern” is the term given to any reusable solution for a common software design problem,
using OOP. Table 3 gives a brief description of generic OOP design patterns referenced in this paper. Readers are
referred to [17] for a full glossary of these terms and examples.

6.2 Design patterns for ML toolboxes

We now present a number of general software engineering design patterns for ML toolboxes. Generally an “OOP design
pattern” is the term given to any reusable solution for a common software design problem, using OOP. The patterns we
propose in this section are derived from the high-level principles in 5 and the conceptual model in 3, building onto
the well-known patterns in Gamma et al. [17]. In this sense, the proposed patterns are “canonical” as they are largely
implied by the choice of formalization as a scientiﬁc type.

For our exposition on key patterns, we will recapitulate some basics of design patterns. Table 3 gives a brief description
of generic OOP design patterns referenced in this paper. For a more detailed but non-ML-speciﬁc exposition of theses
patterns, readers are referred to Gamma et al. [17].

21

A preprint - January 14, 2021

Table 2: Schematic mapping of typical OOP features onto AI/ML objects

OOP feature

Class vs object

Methods
Inheritance

constructor
Class variables

Object variables

Mutable classes
Static classes

Ducks and birds
blueprint class duck vs concrete objects Huey,
Dewey and Louie
walk, quack
duck sub-classes bird, which implements
remove_one_feather
setting up a duck
properties of every duck e.g. swims, can quack

duck
properties
of
number_of_feathers
ducks (can ingest bread)
(all ducks are mutable)

instances

e.g.

Multiple inheritance

Object interaction

the Indian Runner duck implements run from
running_animal
Indian Runner duck herd

AI and learning machines
e.g. blueprint class SVM vs objects my_SVM1,
my_SVM2 that can be ﬁtted to different data
fit, predict, inspect
SVM sub-classes supervised_learner,
which implements set_params
setup learner structure and parameters
properties of every (sup.) SVM e.g. supervised,
kernel method
Fitted model state and hyper-parameters, e.g.
weights, regularization parameters
learners (can ingest data)
formal mathematical objects e.g. distribution, sam-
pler
SVM may sub-class supervised learners and also
kernel learners for methods
composite ML pipeline

Notes: The table describes typical OOP features (ﬁrst column) on aspects of the conceptual models for AI objects introduced in
sections 3.2 and 3.4 (third column), aligned with a supporting correspondence based on the “ducks” metaphor (second column).

Table 3: Overview of key design patterns from Gamma et al. [17]
Deﬁnition

Pattern

Type

Template
Strategy

Behavioral Deﬁnes a common interface pattern through an abstract template class and inheritance.
Behavioral Deﬁnes a family of interchangeable algorithms, with the speciﬁc implementation to be selected at

Visitor
Composite
Decorator
Adapter
Facade

Behavioral
Structural
Structural
Structural
Structural

run-time.
Implements operations as visitor objects to be applied to other objects.
Blueprint for composing a class built from individual “component” classes.
Adds responsibilities to an object dynamically; a type of wrapper.
Converts the interface of a class into another; a type of wrapper.
Provides a high-level interface abstraction to modules or lower level implementation.

The patterns we present here are:

• Universal interface points for mathematical and algorithmic objects. These are the interface points that

all formal objects should share, based on our conceptual model.

• The scitype template/strategy pattern. A templating pattern that can be used to deﬁne and support

scitype-speciﬁc interfaces and implementations of concrete algorithms with that scitype.

• Composition pattern 1: The modiﬁcation pattern. Implements an operation that modiﬁes an object while

preserving the scitype, for example tuning.

• Composition pattern 2: The homogeneous composition pattern. Implements an operation where multiple
objects are combined into one, with a known resultant scitype, for example ensembling with ﬁxed scitype.
• Composition pattern 3: Scitype-inhomogeneous composition patterns. Implements an operation where
multiple objects, possibly of different scitypes, are combined into one, possibly of different resultant sci-
types depending on parameters or component scitypes, for example pipelining (chaining transformation and
prediction algorithms).

• The contraction factory. Implements bulk-conversion of a composite into an atomic (non-composite) object.
• The reduction/composition factory. Implements bulk-application or transformation of a formal object to a

closely related scitype.

• The co-strategy pattern. An abstraction to be used for encapsulating instructions or operations in relation to

formal objects.

We illustrate these data scientiﬁc design patterns with a selection of contemporary examples from scikit-learn
12. Further implementation examples from existing toolboxes is given in section 7.

12Throughout the paper, we rely on the latest release available at the time of writing: v0.24.

22

A preprint - January 14, 2021

Table 4: Universal interface points for all formal objects

Interface point

Speciﬁcation

Scitype inspection
Domain inspection
Parameter inspection

Parameter setting
Trait/property inspection

Object persistence

Description

construction of the object, and speciﬁcation of hyper-parameters, e.g. constructing a normal distribu-
tion with mean=0, variance=1
query that returns the scitype of the object, e.g. a distribution
query that returns the domain of the object, e.g. absolutely continuous distributions over the reals
query that returns names, values, and type/domain of hyper-parameters, e.g. retrieving the value (=1)
and type (real number) of mean
setting values of hyper-parameters, e.g. setting mean to 2
query that returns names, values, and type/domain of traits and properties, e.g. kurtosis of the
distribution
saving and retrieving objects, e.g. storing the distribution once constructed

6.2.1 Mapping formal objects onto classes and objects

Before discussing patterns, we introduce some terminology in this section that will allow us to discuss patterns more
clearly. For clarity of terminology, we will use the term “formal objects” to refer to the conceptual level, while the use
of “class” and “object” will refer to the software and implementation side.

We start by discussing how “formal objects”, i.e. mathematical and algorithmic objects (e.g. distributions, supervised
learners) from the conceptual model map onto classes and objects in the OOP paradigm. In line with the discussion in
section 6.1:

• Objects as representations of concrete formal objects and their current state. Concrete formal objects,
e.g. the Gaussian distribution with mean 0 and variance 1, are represented by objects. If the formal object is an
entity object, the object representation also carries the information of the current state, e.g. in the case of a
ﬁtted linear regression model, coefﬁcients would also be represented in the (software) object.

• Blueprints for formal objects are represented as classes. Objects representing concrete formal objects are
instances of classes that represent blueprints of a “kind” of formal object and which are possibly parametric,
e.g. Gaussian distributions with mean and variance being parameters. The class in question will contain full
implementation of functionality for possible instances, including speciﬁcs of representation, creation, and state
change.

For terminological clarity, we will refer to (software) objects as in the ﬁrst bullet point as “formal instances” or “formal
instance objects”. We will refer to classes as in the second bullet point as “formal concretes” or “formal concrete
classes”.

This terminology will be useful for discussing architecture since not all concrete classes in the usual sense (instantiable
classes with a full implementation) are concrete classes representing (conceptual) formal objects; similarly, not all
classes are concrete, and not all classes representing (conceptual) formal objects are formal concretes. Further, not all
classes occurring in a software architecture will be related to formal objects.

6.2.2 Universal interface points

The conceptual model as discussed in sections 4.1 and 4.2 implies a number of key interface points that implementations
of any formal objects should have – we call these “universal interface points” (universal for implementations of formal
objects). We list the universal interface points in table 4.

We note that parameters and traits/properties are separate concepts by deﬁnition of the interface – we explicitly require
these concepts to be separated by the interface, due to them being separate in the conceptual model (rather than
implementation necessity). Further, it is important that the respective universal interface point not only returns values,
but also the set of parameters or traits – this is an important requirement for later patterns (e.g. composition or reduction).

Entity objects have additional universal interface points related to handling of object state, which we list in table 5. We
intentionally do not include state change as a universal interface point for entity objects, since the number and quality
of interface points will depend on the scitype of the object.

The implementation of this pattern is as follows:

• Use of the template pattern and inheritance for universal interface points. Abstract functionality for
universal interface points should be implemented by a “universal base class” template. Any class implementing

23

A preprint - January 14, 2021

Table 5: Universal interface points for entity objects

Interface point

Description

State inspection
State variable inspection

query that returns the state of the object, e.g. ﬁtted or unﬁtted for a linear regression model
query that returns names, values, and type/domain of state deﬁning variables, e.g. regression coefﬁ-
cients

a formal object should inherit from this (or a suitable) universal base class, in the process being endowed
with functionality of the universal interface points. This follows the “template pattern” in Gamma et al. [17].
For example, the universal base class would implement general parameter functionality or trait inspection.
Depending on case or language, universal interface points for entity objects may also be part of that base class
(and not used for value objects), or added by mix-in or decorator patterns.

• One method (at most) per universal interface point. Universal interface points should map onto at most
one core interface method – e.g. there should be one method by which all parameters can be inspected, as
opposed to one method per parameter. Speciﬁc scitypes may implement “shorthand” methods for individual
parameters or traits, but the pattern requires a universal method per interface point.

• Universal method signatures. Methods and usage of universal interface points is designed to be universal

across formal objects of any scitype.

• Compositionality and templating. Interface implementations need to satisfy the templating and composi-

tionality requirements in the next two patterns.

Example: The universal base class of the scikit-learn toolbox is the BaseEstimator class. It imple-
ments some of our suggested universal interface points; for example, get_params for parameter inspection,
and generic functionality for construction and speciﬁcation in the constructor (__init__). Notably absent from
BaseEstimator are scitype inspection, or state variable inspection (inspection is object speciﬁc and added on
descendant level, and not universal).

6.2.3 The scitype template/strategy pattern

The central pattern used in contemporary toolbox architectures, and one that is often thought of as “deﬁning” for
“scikit-learn-like” toolbox is the scitype template/strategy pattern. It can also be considered as the primary pattern
following from the design principles in section 5.2. As already evident from the naming, this pattern follows two
generic template and strategy patterns from Gamma et al. [17], in the speciﬁc context of scitypes.

The conceptual purpose of the scitype/strategy pattern is to provide both abstraction and conformity for formal instance
objects of a given scitype, e.g. distributions, or supervised classiﬁers.

The pattern applies to formal concrete classes that implement speciﬁc formal objects of a particular scitype S, e.g.
concrete probability distributions (e.g. Gaussian distributions) that follow the “distribution” scitype, or concrete learning
algorithms (e.g. random forest classiﬁer) that follow the “supervised classiﬁer” scitype.

The basic implementation pattern is as follows:

• Use of the template pattern and inheritance for scitype speciﬁc methods. Abstract functionality for
universal interface points should be implemented by a “scitype base class” template. Any formal concrete of
a scitype S should inherit from the scitype base class for the scitype S, in the process being endowed with
functionality speciﬁc to the scitype S, and interface patterns characteristic of the scitype S. This follows
the “template pattern” from [17]. Alternatively, the template can be adhered to by a combination of implicit
speciﬁcation and inheritance applied to formal concretes.

• Use of the strategy pattern for formal concretes of the same scitype. Formal concretes of the same scitype
S should behave exchangeably within key use cases and workﬂows, and implement the same interface that is
necessary for key behaviors and interactions. For example, all supervised classiﬁers implementing fit and
predict methods with the same signature which are called within the ﬁtting/deployment use case. This is
ensured by adhering to the “strategy pattern” from [17] in relation to the scitype base class.

In concrete implementation, the formal concrete classes should (compatibly) inherit both from the universal base
class (see above) as well as adhere to a scitype speciﬁc strategy pattern. This can be handled in multiple ways where
suitability may also depend on language idioms. For example, multiple inheritance, the scitype base class inheriting
from the universal base class, or combination of inheritance and implicit convention.

24

A preprint - January 14, 2021

Example: supervised classiﬁers and regressors (for i.i.d./tabular data) deﬁne two common scitypes; these are scitypes
central to scikit-learn’s use cases. In scikit-learn, supervised classiﬁers and regressors adhere to the
template/strategy pattern. Classiﬁers and regressors are required to implement scitype-deﬁning fit and predict
methods with a uniﬁed interface; functionality is enforced by post-hoc validity checks rather than explicit inheri-
tance. There are also scitype speciﬁc base classes for regressors and classiﬁers, namely RegressorMixin and
ClassifierMixin, but their remit is mainly accessory functionality (scoring) and scitype inspection (signposting
by inheritance), rather than handling the fit/predict interface.

6.2.4 Scitype composition patterns

In this paragraph, we cover a number of construction patterns for use cases that involve implementations of operations
on formal objects. The key concepts on the formal side that are covered by the patterns are:

• Modiﬁcation: Taking a formal object and building another formal object out of it, of the same scitype.

Common examples are tuning or output thresholding.

• Composition: Taking multiple formal objects and using them to build another formal object. Common

examples are ensembling or pipelining.

• Reduction: Taking one or multiple formal objects and building another formal object, of different scitype.

In the above, we call the formal objects out of which the result is built the components, and the result a composite.
If the composite’s scitype varies dependent on the component scitypes, we call the operation output-polymorphic,
otherwise output-monomorphic. An operation can be, at the same, time a composition and a reduction, if the composite
is of different scitype than at least one of the components (one may also deﬁne “reduction in the strict sense” as a
composition where all component scitypes are different from the composite scitype).

With formal scitypes as in section 4.2, we can give a more formal deﬁnition in terms of (sci-)type notation. The reader
content with intuition as given above may skip the formalism.

→ B, where Ai and B are
Deﬁnition: A monomorphic composition operation is an function f of type A1
scitypes. Ai are called component scitypes, and B is called the resultant scitype (of the composite) of the operation f
and of its type. The operation f is called: a modiﬁcation if N = 1 and A1
= B; a reduction if there is an index i such that
B (cid:54)= Ai. A (general) composition operation is a scitype union of monomorphic composition operations, i.e. a function g
∨ · · · ∨ CM where Ci are types of monomorphic composition operations. g is called output-monomorphic
of the type C1
if the resultant scitypes of Ci are all identical (varying i); it is called output-polymorphic if the resultant scitypes of the
Ci (varying i) are non-identical. Deﬁnition end.
We present a basic implementation pattern for composites and reducts, which we will later vary depending on the type
of composition (e.g. modiﬁcation, reduction, output-polymorphism). In vanilla form, it applies to output-monomorphic
composition operations only, and is built as follows:

× · · · × AN

• Implementation by a class of the resultant scitype pattern. The composition operation is modeled by a
concrete class which has the scitype of the resulting composite. e.g. thresholding a supervised regressor’s
output is implemented as a class that also follows the supervised regressor scitype and its scitype speciﬁc
template/strategy pattern. This is in line with principles 3 and 4 in section 5.2.

• Composition and parameter setting at construction. The components are passed to the composite as
objects at construction or initialization, together with parameter settings speciﬁc to the composite, using a
combination of the “composite pattern” and the “factory pattern” from Gamma et al. [17]. This also leads to a
ﬁrst-order-like speciﬁcation syntax in accordance with requirements in section 5.3.

• Components are variables of the composite. The components are treated as variables of the composite.
• Component state as part of the composite’s state. The components become part of the composite’s state,

potentially changing state whenever the composite changes state.

• Compositionality of universal and scitype speciﬁc interfaces. Component interface points should be
visible via the composite interface points, e.g. hyper-parameter interfaces, or state interfaces. This also puts
additional requirements on the implementation of the universal interface points as well as scitype speciﬁc
interface points, such as conventions on how to access hyper-parameters of components via the hyper-parameter
interface of the composite, or how to list state variables of components via the state variable interface of the
composite. This is to ensure concordance with principles 3 and 4 in section 5.2.

An interesting, and often misunderstood, consequence of the pattern is the “currying out of method inputs” – that
is, that inputs to component methods do not become part of the (software implementation of the) composite. For

25

A preprint - January 14, 2021

example, the operation of tuning a supervised learner, if very naively implemented, takes the supervised learner and
some training/validation data on which the best parameters are determined; the output is the supervised learner whose
parameter are set to the “best parameters”. However, this is not following the composite pattern as outlined above: if
it is correctly followed, the data is not an input to the composition operation. Instead, tuning is performed inside the
ﬁtting method of the composite, which has access to the data in deployment. Since the data is passed to the ﬁtting later
anyway (namely: at deployment), there is no need to pass the data to the composition operation too.

We proceed with outlining some variation patterns depending on the nature of the composition operation.

• Adaptor pattern for scitype change. For polymorphic composition operations that are output-monomorphic,
the adapter pattern from Gamma et al. [17] is suitable, in the form of adapting potentially different input
scitypes to the interface of the resultant scitype.

• Strategy dispatch for output-polymorphic composition.

If there can be multiple composite scitypes
dependent on component scitype, this behavior can be modeled by strategy dispatch on input scitype. This
can be achieved with a suitable application of the strategy pattern (selecting by component scitype), by run
time decoration (dependent on component scitype), by method dispatch (on component scitype), or a suitable
combination thereof.

• Wrapper and visitor patterns for simple modiﬁcations. In the case of modiﬁcation or reducts with a single
component (or few ﬁxed scitype component), wrapper and visitor patterns from Gamma et al. [17] may be
alternatives worthwhile considering.

Example: In scikit-learn, the tuning operation GridSearchCV and pipelining operation Pipeline
adhere to the composite pattern. Both operations are output-polymorphic. GridSearchCV addresses the output-
polymorphism by method polymorphism (the score method); Pipeline addresses the output-polymorphism by
method dispatch (to predict vs transform dependent on resultant scitype).

6.2.5 Compile-time composition and contraction

There are situations in which operations may be preferable on the class level (i.e. statically) rather than on the level of
objects or instances (i.e. at run-time), as in the composition patterns above.

The most common such situations are as follows:

• Modiﬁcation or composition at compile-time. Creating a class whose instances are composites as if

obtained by the scitype composition/reduction pattern.

• Contraction to an atom. Turning a composite with speciﬁc parameter settings into a pre-deﬁned object –
e.g. creating a class whose instances are fully speciﬁed supervised prediction pipelines with a given parameter
speciﬁcation.

• Contraction of components. Removing the component hierarchy while leaving parameters settable at

construction at compile-time

• Contraction of parameters. Setting some parameters or components to speciﬁc values at compile-time.

The primary use cases for compile-time composition and contraction and not run-time composition are:

• leveraging blueprints to create other blueprints, e.g. bulk-converting loss functions for supervised regression to

loss functions for supervised prediction of numerical-valued sequences or segmentation.

• “ﬁxing” a reference implementation for an important composite; e.g. for convenience in frequent re-use,
transparency, reproducibility or replication; a common example is the common composite of a grid-search
tuned support vector machines with prior input normalization, as the method does not tend to perform well in
isolation.

We proceed with outlining some variation patterns depending on the nature of the composition operation.

• Compile-time decoration. This is using the “decorator pattern” or “wrapper pattern” from Gamma et al.
[17] (at compile-time), and is mostly suitable for operations with a single component, e.g. modiﬁcation or
simple reductions. The pattern is used to decorate or wrap the component on class-level.

• Compile-time factories. This is using the “factory pattern” from Gamma et al. [17] (at compile-time). A
typical implementation would suitably construct the ﬁnal composite at construction of the factory class and
return itself.

26

A preprint - January 14, 2021

Example: Any “naive” deﬁnition of shorthands for a frequently used composites is compile-time composition. Applying
standard reductions such as thresholding in probabilistic classiﬁers can be seen as an instance of compile-time reduction.
scikit-learn does not seem to make widespread use of this pattern.

6.2.6 The co-strategy pattern

In handling implementations of formal objects, it may be useful to abstract recurring motifs in passing information,
speciﬁcally in:

• Object construction and parameter manipulation. For example, setting the values of hyper-parameters at

construction of a formal object, for example specifying the kernel function in a support vector machine.

• Passing arguments to scitype speciﬁc interface methods. For example, specifying task information such
as variables to predict, or a forecasting horizon, to the fit/predict methods of a learning algorithm.

The pattern relies on conceptualizing the recurring motif as an object in its own right, with its own scitype – notably, not
from ﬁrst principles, but in secondary relation to the “primary object(s)” from which it is abstracted. Implementation, in
consequence, relies on:

• Use of the strategy pattern for the motif, considered in its own right, and the scitype template/strategy
pattern to implement a “conceptual kind” of motif. For example, speciﬁcation of an abstract learning task in
relation to a data set on which a learning strategy is employed, or speciﬁcation of cross-validation parameters
used in tuning.

• Methods implement operations intrinsic to the motif. Functionality that is speciﬁc to the motif without
the context should be located with the motif implementing class rather than the primary object. For example,
if the motif abstracted is a data transformation, functionality to execute the data transformation should be
localized with the speciﬁcation of the data transformation.

• Consistency of co-strategies. If the same conceptual motif occurs as parameters or arguments in the context
of multiple operations, it should be handled by the same co-strategy or co-strategy scitype. For example, if
cross-validation speciﬁcations appear in tuning operations and evaluation workﬂows, they should be handled
by the same class or scitype, as opposed to one class for tuning and one class for evaluation.

On a side note, co-strategies often implement what one may conceptually a “speciﬁcation”, for behavior or an algorithm
– however, we expressly avoid that term since “speciﬁcation pattern” is an established design pattern with incongruent
meaning.
Example: Cross-validation speciﬁcations (such as the cross-validators in the model_selection module of
scikit-learn ) can be seen as a co-strategy obtained from encapsulating the “specify the parameters for cross-
validation” in tuning via GridSearchCV. Transformers, a widely used conceptualization of data manipulation
operations (such as feature extractions) used as part of a pipeline composite, can be considered a co-strategy obtained
from encapsulating prior data manipulation steps in the modiﬁcation that is “ﬁrst do the data manipulation, then apply
the supervised learner”.

6.3 Prior art

To our knowledge, this content is entirely novel in the sense of abstract and systematic formulation, while drawing
inspiration from the non-ML-speciﬁc work of Gamma et al. [17], and domain-driven design practice put forward in
Evans [16]. Many of the patterns presented here have been implicitly used in the design or implementation of existing
state-of-art toolboxes (also see examples above), especially Weka [20], scikit-learn [32, 41, 13], mlr3 [24, 7],
and MLJ [9] – but not named or explicitly identiﬁed as generic patterns. The co-strategy pattern, and especially the
encapsulation of task information into objects, is partially inspired by the APIs of mlr [7, 24] and the openML
platform [40].

7 Selected implementation examples

In this section, we illustrate how our proposed principles and patterns from section 5 and 6 apply to concrete interface
designs. We focus on learning algorithms as the key objects in typical ML workﬂows, including predictors, transformers
and composite models such as pipelines. We illustrate not only how our analysis can explain central aspects of
contemporary toolboxes. We will use scikit-learn as our running example, but similar designs can be found in
other toolboxes such as mlr3 [24], MLJ [9] or Weka [20].

27

A preprint - January 14, 2021

Many of the proposed designs are not new, but they are for the ﬁrst time derived from general design patterns. Through
our systematic approach, we also identify a number of novel aspects which do not seem to be covered by contemporary
state-of-art designs, such as uniﬁed handling of ﬁtted parameters for AI algorithms. In addition, we will illustrate how
our analysis can guide the design of new toolboxes using some examples from our work on sktime [27], a new
uniﬁed toolbox for ML with time series.

In general, we proceed in our designs as follows:

• As argued in section 3.2, any interface design for algorithms starts with a problem speciﬁcation. The algorithm
scitype is then deﬁned as the counterpart to the task. The formal problem speciﬁcation, or task, deﬁnes what to
solve – the precise ML problem in question – in contrast to the algorithm scitype which deﬁnes how to solve it.
Mathematically, the task can be understood as the codomain of the algorithm scitype. While algorithms are
functions that, given some data, ﬁt and return a function, the task deﬁnes the set of destination into which all
of the returned functions of the algorithm are constrained to fall.

• In practice, a task speciﬁcation also contains the information necessary to specify a particular learning task in
regard to some practical problem at hand. For example, in supervised learning, the task contains at least the
name of the target variable. In forecasting, it contains at least the forecasting horizon, that is the time points
one wants to predict.

• From the task, we can then derive an algorithm scitype. As discussed in section 4, a scitype deﬁnes both an
abstract class interface and key statistical properties. Following the scitype template/strategy pattern deﬁned
in section 6.2, each algorithm scitype is implemented as an abstract base class, while speciﬁc algorithms are
implemented as concrete classes inheriting the interface speciﬁcation from the abstract base class.

• The template/strategy principle ensures that many ML models which differ only with respect to their internal
algorithm but have the same purpose, also share the same interface. It separate the “what” from the “how”, and
encapsulates the intricacies of the solution (“how”) behind a clear interface (“what”), in line with the idea of
“intention-revealing” interfaces from domain-driven design [16]. This avoids exposing complex, model-speciﬁc
implementations to users, and makes it easier to understand, compare and extend models.

• Scitype-driven encapsulation of algorithms also ensures that algorithms are decoupled from other common
workﬂow elements such as data objects and other generated artifacts such as predictions, as discussed in
section 5.1.

• We give a list of key interface points for all learning algorithms, regardless of their speciﬁc scitype in table 6.

Additional interface points for all scitypes are listed in table 4.

• As argued in section 3.3, algorithms are conceived of as entity objects. Entity objects characterized by having
state changes, but with a thread of continuity that need to be maintained (e.g. ﬁtted and unﬁtted model). Often
the state spans several variables, and changes to those variables must be coordinated in order to maintain the
consistency of the state. To manage the state changes through an interface, we conceive of learning algorithms
as entity objects. As such, they possess multiple internal states between which they can transition, and states
can be accessed and modiﬁed through several operations (e.g. ﬁtting). In addition, the learning algorithm
that is used during ﬁtting is often tightly coupled to the prediction functional used during prediction. In other
words, different prediction functionals require different learning algorithms. Construing learning algorithms as
entity objects helps to maintain a close connection between the ﬁtted prediction functional and the learning
algorithm used to ﬁt it. The additional interface points that we consider important for learning algorithms as
entity objects are listed in table 5.

• Traits of concrete algorithms can be captured by algorithm tags, implemented as class variables of concrete
algorithm classes with shared functionality residing the abstract base class. Traits can be used for ﬁner
categorization of different algorithms of the same scitype. This is especially useful for testing where one may
want to run the same tests on all algorithms of the same category. Traits are also useful for guiding users in
algorithm choice, as they allow to query and ﬁlter algorithms by their traits. For example, one may deﬁne a
trait for whether a supervised classiﬁer can handle multi-label problems.

In the following, we will discuss concrete examples, including supervised learners, transformers and composition
algorithms such as pipelines and ensembles from scikit-learn, and forecasters and reduction algorithms from sktime.

7.1 Supervised learners

Following the “consensus” formulation of the supervised learning task introduced in section 3.2, we can deﬁne a scitype
for a supervised learner as a combination of a class type specifying the interface and statistical properties. The class
type can be deﬁne as follows:

28

Table 6: Common interface points for learning algorithms

Interface point

Description

A preprint - January 14, 2021

Model speciﬁcation Construction and hyper-parameter speciﬁcation
Model estimation
Application
Updating
Inspection

Data ingestion and parameter estimation (e.g. fit)
Typically data transformation or generation of predictions (e.g. predict, transform)
Online learning, e.g. update or partial_fit in scikit-learn
While encapsulation will hide some information, key information such as hyper-parameters and ﬁtted
parameters should be accessible for users (e.g. get_params and get_fitted_params)
Saving ﬁtted models for deployment (e.g. save)

Model persistence

class type SupervisedLearner

params
state
methods fit

paramlist : paramobject
model

: mathobject
:
: X × model → Y

(X × Y)N → model

predict

The cross-sectional nature of the input data, X, implies two key statistical properties: (i) permutation invariability for the
i.i.d. samples and (ii) feature permutation invariability. This means that, when represented in tabular form, the output of
fit and transform should not change if the order of the rows (samples) and/or columns (features) is permuted.

Following the strategy/template pattern, the class type can serve as a blueprint for the abstract base class which other con-
crete implementations of supervised learners implement. Example 2 shows the implementation in scikit-learn.

Example 2: scikit-learn ’s API for supervised learning (classiﬁcation)

1 classifier = RandomForestClassifier()
2 classifier.fit(y_train, X_train)
3 y_pred = classifier.predict(X_test)

X_train and y_train denote the training data feature matrix and label vector, X_test is the feature matrix of the test set,
and y_pred the predicted label.
When comparing the SupervisedLearner scitype with scikit-learn’s core API, it becomes clear that this scitype
is central to scikit-learn’s interface. In particular, the universal base class of the scikit-learn toolbox
is the BaseEstimator class. It implements some of our suggested universal interface points. For example,
get_params for parameter inspection, and generic functionality for construction and speciﬁcation in the constructor
(__init__).
We can, of course, further distinguish the SupervisedLearner class type into classiﬁer and regressor types,
according to the domain of the target variable, Y. Classiﬁers and regressors are required to implement scitype-
deﬁning fit and predict methods with a uniform interface. In scikit-learn, there are also scitype speciﬁc base
classes for regressors and classiﬁers, namely RegressorMixin and ClassifierMixin, but their remit is
mainly accessory functionality (scoring) and scitype inspection (signposting by inheritance), rather than handling
the fit/predict interface. As such, supervised classiﬁers and regressors in scikit-learn adhere to the
template/strategy pattern. Note that, in scikit-learn, this pattern is enforced through conventions and unit testing,
following “design by contract” principles [28], rather than strict inheritance from abstract base classes.
Notably absent from BaseEstimator is a uniform interface for state variable inspections. Instead, inspection
is speciﬁc to concrete classes and added on a descendant level. Instead, we argue that algorithms should also have
a uniform interface for inspecting inference results (e.g. ﬁtted coefﬁcients) – something which is currently missing
from most major toolboxes. This would allow to develop composition classes which operate on ﬁtted parameters of
component algorithms.

7.2 Forecasters

Having illustrated how scitype-based patterns can motivate and explain core design choices in the existing toolboxes
such as scikit-learn, we will now discuss how they can guide the design of new ones. We use the design of sktime’s API
for forecasting [27, 26] as an example.

29

We introduced the forecasting learning task in section 3.2. Following the same process as before, we can derive an
algorithm scitype from the task speciﬁcation. The class type can be speciﬁed as follows:

class type Forecaster

A preprint - January 14, 2021

params
state
methods fit

paramlist : paramobject
model

: mathobject
: Z → model
:

T × model → Z

predict

The time series nature of the input data implies two key statistical properties: (i) orderedness of time-series observations
and (ii) shared domain of the observed time points in Z and forecasting horizon T. This means that shufﬂing the
observations may change the results of fit.
Example 3 shows the implementation in sktime .

Example 3: sktime ’s API for (univariate) forecasting

1 forecaster = ExponentialSmoothing()
2 fh = [1, 2, 3]
3 forecaster.fit(y_train)
4 y_pred = forecaster.predict(fh)

y_train denotes the training time series, fh the forecasting horizon for the three steps ahead relative to the end of the training
series, and y_pred the predicted values for the given forecasting horizon.

Note that we pass the forecasting horizon, i.e. task information, explicitly to the algorithm. In the previous example
from scikit-learn, we provide necessary task information, such as the target variable, by through separation of input
arguments, i.e. we pass two arguments, one for the target variable and one for the feature data to the fit method,
with y_train being the designated target variable and X_train the feature data. By contrast, in this example, we
deﬁne a separate fh object to pass that information to the forecasting algorithm following the co-strategy pattern in
section 6.2. Both designs adhere to the design principle of keeping the speciﬁcation of task information – i.e. the target
name or forecasting horizon – separate from speciﬁcation of the algorithm as described in section 5.1.

Also note that in contrast to the tabular (or cross-sectional) setting, there is no established standard interface design for
the forecasting setting, or other related time series learning problems. Instead, the ecosystem is fragmented with various
specialized toolkits for speciﬁc algorithm families [27]. We hope that our design principles and patterns presented here
can guide future toolbox development and enable us to provide advanced time series toolbox capabilities.

7.3 Composition

Until now, we have discussed primitive (or atomic) learning algorithms. In this section, we turn to composite algorithms.
Composite algorithms, sometimes also called meta-algorithms, consist of and operate on one or more component
algorithms. Common examples include tuning, pipelining or ensembling.

7.3.1 Pipelining, ensembling and tuning

In section 6.2, we outline the key composition patterns for ML toolboxes. Scitypes extend directly to higher-order
scitypes to describe composite algorithms. For example, one of most common composite algorithms is a pipeline
which, in its simplest form, chains multiple prior data transformations with a ﬁnal learning algorithm. As a higher-order
scitype, this can be expressed as:
Pipeline: (Transformer)n × SupervisedLearner → SupervisedLearner,

where a pipeline is deﬁned as taking n transformers and a ﬁnal supervised learner and composing them into a single,
composite supervised learner. Similarly, we can deﬁne an ensemble of supervised learners:
Ensemble: (SupervisedLearner)n → SupervisedLearner.

The composite algorithm adhere to the scitype composition pattern as described in section 6.2. As before, we can
follow the scitype template/strategy pattern and encapsulate each higher-order scitype as an abstract base class, using
the Pipeline scitype together with the SupervisedLearner as a blueprint for an abstract base class.

The key advantage of the pattern is that composite and individual algorithms can be treated uniformly. Primitive and
composite object become interchangeable at run time. For example, a pipeline has the same scitype as its ﬁnal learning

30

A preprint - January 14, 2021

algorithm. Likewise, the ensemble is of the same scitype as the component algorithms. As a consequence, users (and
other client algorithms) can ignore the difference between compositions of algorithms and individual algorithms. For
example, in scikit-learn, tuning procedure GridSearchCV and pipelining operation Pipeline adhere to
the composite pattern.

We make a few additional remarks:

• We encapsulate composition algorithms in separate classes instead of assigning additional methods to the base
class mainly for two reasons: First, composition approaches are not part of the simplest possible representation
of scitypes following the principle laid out in section 5.2. Second, there are often different concrete variants of
composition algorithms. For example there are different algorithms for tuning (e.g. full grid search, randomized
grid search, successive halving). Encapsulating composition algorithm in separate classes makes it easy to add
new meta algorithms without changing any other class. In addition, many composition learning algorithm are
often general and not closely tied to speciﬁc learning algorithms. They are often applicable to any algorithm
of the same scitype, however, there are some specialized approaches closely connected to concrete algorithm
implementations which are implemented in separate classes.

• Composite algorithms require consistent interfaces as imposed by the strategy/template pattern. This is
because the composite algorithm usually forwards a method call to its component algorithms, and may
performs additional operations before and/or after forwarding. For example, when ensembling algorithms, the
composite algorithm will forward the ﬁt call to all individual ensemble members, when calling predict, it will
forward the predict call to all ensemble members and then return an aggregated prediction. The forwarding
only works if the composition class can expect a consistent interface.

• In order for ML composition to work well, composition must allow nested hyper-parameter and parameter
access. Hyper-parameters of component algorithms must be accessible and settable from the interface of the
composite algorithm.

• Compositions may also make use of the decorator pattern from Gamma et al. [17]. In this context, this pattern
allows to assign responsibilities to an object dynamically at run time, depending on its component models. For
example, a composite algorithm may expose some methods or parameters from its component algorithms, but
the exact methods and ﬁelds it exposes depend on the type of component algorithm speciﬁed at run time. For
example, this is implemented in scikit-learn in the GridSearchCV class.

7.3.2 Transformers

Data transformations are commonly encapsulated as transformers. The main reason for encapsulating transformations
in transformer classes is that we do not keep track of data changes in the data object itself, which is usually conceived
of as an immutable value object (see section 3). Instead, we treat data transformations as part of model speciﬁcation.
Note that unlike the learning algorithms discussed so far, transformers by themselves do not correspond to any learning
task. We can, however, still deﬁne scitypes for transformations, based on their key operations and statistical properties
in the context of learning tasks that they are applied to. In terms of scitypes, we can distinguish “primary scitypes” that
are directly addressing a formal task and “secondary scitypes” which arise as operations related to primary scitypes
(e.g. transformers). For example, viewed in the context of supervised learning pipelines, transformers need to be
compatible with the established scitype for supervised learners, i.e. separate methods for ﬁtting, application of the ﬁtted
model (predict and transform respectively), as well as the common hyper-parameter interface. In this sense,
transformers can be conceived of as modiﬁcations to supervised learners together with the pipelines that they are part of.
We can see how the transformer API in scikit-learn implements the scitype.

class type Transformer

params
state
methods fit

paramlist : paramobject
model

: mathobject
:
: X × model → U

(X × Y)N → model

predict

Note that this is a cross-sectional transformer scitype, given the cross-sectional domain of the input data X and
transformed output data U. As a consequence, the same statistical properties hold as for the SupervisedLearner
scitype. In principle, transformer scitypes for other data formats are possible, for example for time series data (e.g.
Box-Cox transformations). Example 4 shows the implementation in scikit-learn .

31

A preprint - January 14, 2021

Example 4: scikit-learns ’s API for transformers

1

2

3

transformer = PCA()
transformer.fit(X, y)
Xt = transformer.transform(X)

X and y denote the some cross-sectional data and target vector, and Xt the transformed data.

7.3.3 Reduction

The ﬁnal example we discuss is that of reduction. Note that reduction sometimes also refers to dimensionality reduction.
We here instead refer to reduction as an approach to convert one learning task into another one, allowing us to apply an
algorithm for one task to solve another task [6, 3]. A classical example in supervised learning is one-vs-all classiﬁcation,
reducing k-way multi-category classiﬁcation to k binary classiﬁcation tasks.

Another common example is reducing forecasting to supervised regression Bontempi et al. [10]. This typically involves
the transformation of the time series data that one wants to forecast into the required tabular input format, so that one
can then ﬁt and apply any regression algorithm. The higher-order scitype for this particular reduction approach can be
expressed as follows:
ForecastingToRegressionReduction: SupervisedLearner → Forecaster
where the composition algorithm takes in any algorithm of the scitype SupervisedLearner and adapts it to the
interface of the Forecaster scitype.

While reductions are not new, we are the ﬁrst to propose encapsulating them as meta-estimators in a separate class
following the composition pattern in section 6.2. Reductions have several key properties that make them well suited to
be expressed as meta-estimators:

• Modularity. Reductions convert any algorithm for a particular task into an algorithm for a new task. Applying
some reduction approach to n base algorithms gives n new algorithms for the new task. Any progress on the
base algorithm immediately transfers to the new task, saving both research and software development effort
[4, 5].

• Tunability. Most reductions require modeling choices that we may want to optimize. For example, we may
want to tune the window length or select among different strategies for generating forecasts (e.g. “direct” or
“recursive”) [39, 10]. By expressing reductions as meta-estimators, we expose these choices via the common
interface as tunable hyper-parameters.

• Composability. Reductions are composable. They can be composed to solve more complicated problems
[4, 5]. For example, we can ﬁrst reduce forecasting to time series regression which in turn can be reduced to
tabular regression via feature extraction.

• Adaptability. Reductions are adapters. They adapt the interface of the base algorithm to the interface required
for solving the new task, following the adapter pattern in Gamma et al. [17] and the scitype reduction pattern
in section 6.2. This lets users apply algorithms to learning tasks that could not otherwise not be applied to
those tasks because of incompatible interfaces and to reuse the common tuning and model evaluation tools
appropriate for the new task.

8 Conclusion

ML toolboxes have become the workhorses of modern data scientiﬁc practice. However, despite their universal success,
the key principles in their design have never been fully analyzed.

In this paper, we presented a ﬁrst attempt at analyzing ML toolbox design. Following a domain-driven design
methodology, we ﬁrst developed a conceptual model for common objects in the ML/AI domain. At its core, we
proposed a new type system, called scientiﬁc typing, which captures the data scientiﬁc purpose and meaning of based
on the set of operations that we usually perform with them (i.e. their interface) and their statistical properties. The
proposed conceptual model is both well-grounded in the underlying mathematical and statistical formalism and easily
translatable into software.

In our analysis, we combined ideas from classical software engineering, type theory and formal mathematical statistics,
as well as insights from established toolboxes in the ML/AI domain. From our analysis, we derived a set of key design
principles and reusable patterns using object-oriented programming. We illustrated that these principles and patterns
can not only explain the design of existing toolboxes such as scikit-learn, but also guide the development of new ones.

32

A preprint - January 14, 2021

We hope that our contribution can serve as a state-of-the-art reference for ML practitioners and developers, and,
potentially, as a ﬁrst step towards a higher-level declarative language for constructing AI.

Acknowledgments

We would like to thank Viktor Kazakov for his feedback and discussions on a ﬁrst draft of the paper. We would also
like to thank the contributor communities of the distr6 , mlr3proba , MLJ and sktime projects for their
indirect contributions and feedback.

Markus Löning’s contribution was supported by the UK Economic and Social Research Council (ESRC grant: ES-
P000592-1), the Consumer Data Research Centre (CDRC) (ESRC grant: ES-L011840-1), and The Alan Turing Institute
(EPSRC grant: EP-N510129-1).

Authors’ contributions

FK initiated the project and provided key ideas. All authors contributed to the development of ideas, primarily through
collaboration in open source projects: distr6 , mlr3 , mlaut , MLJ , sktime , and AG’s MSc thesis ( pysf
). AB introduced the term “scitype” and the formal idea in the context of data containers and the MLJ package. FK
conceived the concept of abstract scitypes of formal objects beyond data containers. FK and ML led and coordinated
the writing effort, with all authors contributing at least in an editor role. All authors are jointly responsible for content
and exposition.

References

[1] Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay
Ghemawat, Geoffrey Irving, Michael Isard, and Others. TensorFlow: A system for large-scale machine learning.
In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pages 265–283, 2016.

[2] Len Bass, Paul Clements, and Rick Kazman. Software architecture in practice. Addison-Wesley Professional,

2003.

[3] Alina Beygelzimer, Varsha Dani, Tom Hayes, John Langford, and Bianca Zadrozny. Error limiting reductions
between classiﬁcation tasks. In Proceedings of the 22nd international conference on Machine learning, pages
49–56. ACM, 2005.

[4] Alina Beygelzimer, John Langford, and Bianca Zadrozny. Weighted one-against-all. In American Association for

Artiﬁcial Intelligence (AAAI), pages 720–725, 2005.

[5] Alina Beygelzimer, John Langford, and Bianca Zadrozny. Machine learning techniques—reductions between

prediction quality metrics. In Performance Modeling and Engineering, pages 3–28. Springer, 2008.

[6] Alina Beygelzimer, Hal Daumé, John Langford, and Paul Mineiro. Learning reductions that really work.

Proceedings of the IEEE, 104(1):136–147, 2015.

[7] Bernd Bischl, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Erich Studerus, Giuseppe Casalicchio,
and Zachary M. Jones. mlr: Machine Learning in R. Journal of Machine Learning Research, 17(170):1–5, 2016.
URL http://jmlr.org/papers/v17/15-066.html.

[8] Anthony Blaom, Thibaut Lienart, Yiannis Simillides, Diego Arenas, Vollmersj, Mosè Giordano, Okon Samuel,
Ayush Shridhar, Ayush Shridhar, Ed, Swenkel, Julian Samaroo, Evalparse, Júlio Hofﬁmann, Sjvollmer,
Michael Krabbe Borregaard, Kevin Squire, Pshashk, Lhnguyen-vn, Azev77, Ashrya Agrawal, Venkateshprasad,
Robert Hönig, Nils, Kryohi, Julia TagBot, Evelina Gabasova, Dilum Aluthge, and Cédric St-Jean. MLJ: A
Machine Learning Framework for Julia, apr 2020. URL https://zenodo.org/record/3765808.

[9] Anthony D Blaom, Franz Kiraly, Thibaut Lienart, Yiannis Simillides, Diego Arenas, and Sebastian J Vollmer.
MLJ: A Julia package for composable machine learning. Journal of Open Source Software, 5(55):2704, 2020. doi:
10.21105/joss.02704. URL https://doi.org/10.21105/joss.02704.

[10] Gianluca Bontempi, Souhaib Ben Taieb, and Yann-Aël Le Borgne. Machine Learning Strategies for Time Series

Forecasting. In Business Intelligence, pages 62–77. Springer, Berlin, Heidelberg, 2013.

33

A preprint - January 14, 2021

[11] George E. P. Box, Gwilym M. Jenkins, and Gregory C. Reinsel. Time series analysis: Forecasting and control:

Fourth edition. 2013. ISBN 9781118619193. doi: 10.1002/9781118619193.

[12] William J Brown, Raphael C Malveau, Thomas J Mowbray, and John Wiley. AntiPatterns: Refactoring Software ,

Architectures, and Projects in Crisis. Wiley, 1998. ISBN 0849329949.

[13] Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas C Müller, Olivier Grisel, Vlad
Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake Vanderplas, Arnaud Joly,
Brian Holt, and Gaël Varoquaux. API design for machine learning software: experiences from the scikit-learn
project. In ECML PKDD 2013 Workshop on Languages for Data Mining and Machine Learning, 2013. URL
https://github.com/scikit-learn.

[14] Frank Buschmann, Kelvin Henney, and Douglas Schimdt. Pattern-Oriented Software Architecture: On Patterns

And Pattern Language, Volume 5, volume 5. John wiley & sons, 2007.

[15] John M Chambers and Others. Object-oriented programming, functional programming and R. Statistical Science,

29(2):167–180, 2014.

[16] Eric Evans. Domain-driven design: tackling complexity in the heart of software. Addison-Wesley Professional,

2004.

[17] Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. Design Patterns – Elements of Reusable Object-
Oriented Software. Addison Wesley Longman, Inc., 1997. ISBN 9780201715941. doi: 10.1093/carcin/bgs084.

[18] Frithjof Gressmann and Franz J. Király. skpro: A domain-agnostic modelling framework for probabilistic super-
vised learning. openreview.net, 2018. URL https://openreview.net/forum?id=B1gIZHtAFQ.

[19] Ahmed Guecioueur. pysf: Supervised forecasting of sequential data in Python, 2018. URL https://pypi.

org/project/pysf/.

[20] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. The WEKA
data mining software. ACM SIGKDD Explorations Newsletter, 11(1):10, nov 2009. ISSN 19310145. doi:
10.1145/1656274.1656278. URL http://portal.acm.org/citation.cfm?doid=1656274.
1656278.

[21] Trevor Hastie, Robert T. Tibshirani, and Jerome Friedman. The Elements of Statistical Learning, vol-
ume 1. Springer, 2 edition, 2009. ISBN 978-0-387-84857-0. doi: 10.1007/b94608. URL http://www.
springerlink.com/index/10.1007/b94608.

[22] Viktor Kazakov and Franz J Király. Machine Learning Automation Toolbox (MLaut).

arXiv preprint

arXiv:1901.03678, 2019.

[23] Max Kuhn. Building Predictive Models in R: Using the caret Package. Journal of Statistical Software, 28(5),

2008. doi: 10.18637/jss.v028.i05. URL http://www.jstatsoft.org/v28/i05/.

[24] Michel Lang, Martin Binder, Jakob Richter, Patrick Schratz, Florian Pﬁsterer, Stefan Coors, Quay Au, Giuseppe
Casalicchio, Lars Kotthoff, and Bernd Bischl. mlr3: A modern object-oriented machine learning framework in R.
Journal of Open Source Software, 4(44):1903, 2019.

[25] Craig Larman. Applying UML and patterns: an introduction to object oriented analysis and design and interative

development. Pearson Education India, 2012.

[26] Markus Löning and Franz J. Király. Forecasting with sktime: Designing sktime’s New Forecasting API and

Applying It to Replicate and Extend the M4 Study. ArXiv e-prints, 2020.

[27] Markus Löning, Anthony Bagnall, Sajaysurya Ganesh, Viktor Kazakov, Jason Lines, and Franz J Király. sktime:
A Uniﬁed Interface for Machine Learning with Time Series. Workshop on Systems for ML at NeurIPS 2019, 2019.

[28] Bertrand Meyer. Object-oriented software construction, volume 2. Prentice hall Englewood Cliffs, 1997.

[29] Soroosh Nalchigar, Eric Yu, Yazan Obeidi, Sebastian Carbajales, John Green, and Allen Chan. Solution patterns
for machine learning. In International Conference on Advanced Information Systems Engineering, pages 627–642.
Springer, 2019.

34

A preprint - January 14, 2021

[30] Elizamary Nascimento, Anh Nguyen-Duc, Ingrid Sundbø, and Tayana Conte. Software engineering for artiﬁcial
intelligence and machine learning software: A systematic literature review. arXiv preprint arXiv:2011.03751,
2020.

[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, and Others. Pytorch: An imperative style, high-performance deep
learning library. In Advances in neural information processing systems, pages 8026–8037, 2019.

[32] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David
Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. Scikit-learn: Machine Learning in
Python. The Journal of Machine Learning Research, 12:2825–2830, 2011. URL https://dl.acm.org/
citation.cfm?id=2078195.

[33] Benjamin C Pierce and C Benjamin. Types and programming languages. 2002.

[34] Roger S Pressman. Software engineering: a practitioner’s approach. Palgrave macmillan, 2005.

[35] R Core Team. R: A Language and Environment for Statistical Computing. Technical report, R Foundation for

Statistical Computing, Vienna, Austria, 2014.

[36] David Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary,
Michael Young, Jean-Francois Crespo, and Dan Dennison. Hidden technical debt in machine learning systems. In
Advances in neural information processing systems, pages 2503–2511, 2015.

[37] Raphael Sonabend, Franz J Király, Andreas Bender, Bernd Bischl, and Michel Lang. mlr3proba: Machine

Learning Survival Analysis in R. arXiv preprint arXiv:2008.08080, 2020.

[38] Sören Sonnenburg, Mikio L Braun, Cheng Soon Ong, Samy Bengio, Leon Bottou, Geoffrey Holmes, Yann LeCun,
Klaus-Robert Müller, Fernando Pereira, Carl Edward Rasmussen, and Others. The need for open source software
in machine learning. Journal of Machine Learning Research, 8(Oct):2443–2466, 2007.

[39] Souhaib Ben Taieb. Machine learning strategies for multi-step-ahead time series forecasting. PhD thesis, Universit

Libre de Bruxelles, Belgium, 2014.

[40] Joaquin Vanschoren, Jan N Van Rijn, Bernd Bischl, and Luis Torgo. OpenML: networked science in machine

learning. ACM SIGKDD Explorations Newsletter, 15(2):49–60, 2014.

[41] Gaël Varoquaux, Lars Buitinck, Gilles Louppe, Olivier Grisel, F. Pedregosa, and A. Mueller. Scikit-learn:
Machine Learning Without Learning the Machinery. GetMobile: Mobile Computing and Communications, 19
(1):29–33, jun 2015. doi: 10.1145/2786984.2786995. URL http://dl.acm.org/citation.cfm?
doid=2786984.2786995.

[42] Rebecca Wirfs-Brock and Alan McKean. Object design: roles, responsibilities, and collaborations. Addison-

Wesley Professional, 2003.

[43] Rebecca Wirfs-Brock and Brian Wilkerson. Object-oriented design: a responsibility-driven approach. ACM

sigplan notices, 24(10):71–75, 1989.

[44] Rebecca Wirfs-Brock, Brian Wilkerson, and Lauren Wiener. Designing object-oriented software. 1990.

[45] A Zheng. The challenges of building machine learning tools for the masses. In SE4ML:SoftwareEngineer- ing for

Machine Learning (NeurIPS 2014 Workshop), 2014.

35

