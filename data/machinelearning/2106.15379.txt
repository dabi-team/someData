To appear as a part of an upcoming textbook on dimensionality reduction and manifold learning.

2
2
0
2

g
u
A
3

]
L
M

.
t
a
t
s
[

2
v
9
7
3
5
1
.
6
0
1
2
:
v
i
X
r
a

Uniﬁed Framework for Spectral Dimensionality Reduction, Maximum
Variance Unfolding, and Kernel Learning By Semideﬁnite Programming:
Tutorial and Survey

Benyamin Ghojogh
Department of Electrical and Computer Engineering,
Machine Learning Laboratory, University of Waterloo, Waterloo, ON, Canada

BGHOJOGH@UWATERLOO.CA

Ali Ghodsi
Department of Statistics and Actuarial Science & David R. Cheriton School of Computer Science,
Data Analytics Laboratory, University of Waterloo, Waterloo, ON, Canada

ALI.GHODSI@UWATERLOO.CA

Fakhri Karray
Department of Electrical and Computer Engineering,
Centre for Pattern Analysis and Machine Intelligence, University of Waterloo, Waterloo, ON, Canada

KARRAY@UWATERLOO.CA

Mark Crowley
Department of Electrical and Computer Engineering,
Machine Learning Laboratory, University of Waterloo, Waterloo, ON, Canada

MCROWLEY@UWATERLOO.CA

Abstract

laxed MVU, and landmark MVU for big data.

This is a tutorial and survey paper on uniﬁcation
of spectral dimensionality reduction methods,
kernel learning by Semideﬁnite Programming
(SDP), Maximum Variance Unfolding (MVU) or
Semideﬁnite Embedding (SDE), and its variants.
We ﬁrst explain how the spectral dimensionality
reduction methods can be uniﬁed as kernel Prin-
cipal Component Analysis (PCA) with different
kernels. This uniﬁcation can be interpreted as
eigenfunction learning or representation of ker-
nel in terms of distance matrix. Then, since the
spectral methods are uniﬁed as kernel PCA, we
say let us learn the best kernel for unfolding the
manifold of data to its maximum variance. We
ﬁrst brieﬂy introduce kernel learning by SDP for
the transduction task. Then, we explain MVU
in detail. Various versions of supervised MVU
using nearest neighbors graph, by class-wise un-
folding, by Fisher criterion, and by colored MVU
are explained. We also explain out-of-sample ex-
tension of MVU using eigenfunctions and kernel
mapping. Finally, we introduce other variants of
MVU including action respecting embedding, re-

1. Introduction
Dimensionality reduction algorithms can be divided into
three categories which are spectral, probabilistic, and neu-
ral network-based methods (Ghojogh, 2021). Spectral di-
mensionality reduction methods deal with geometry of
data and sometimes reduce to an eigenvalue or generalized
eigenvalue problem (Ghojogh et al., 2019a). Various spec-
tral methods have been proposed during decades. Some
of the most well-known spectral methods are Principal
Component Analysis (PCA) (Ghojogh & Crowley, 2019),
Multidimensional Scaling (MDS), Isomap (Ghojogh et al.,
2020b), spectral clustering, Laplacian eigenmap, diffusion
map (Ghojogh et al., 2021a), and Locally Linear Embed-
ding (LLE) (Ghojogh et al., 2020a). After development
of many of these methods, it was found out that the spec-
tral methods are all learning eigenfunctions and can be re-
duced to kernel PCA with different kernels (Bengio et al.,
2003b;c;a; 2004; 2006). Moreover, from the formulation of
MDS, we know that we can write kernel in terms of the dis-
tance matrix (Ghojogh et al., 2020b). It was shown in (Ham
et al., 2004) that the spectral methods can be seen as kernel
PCA with different kernels where kernels are constructed
from various distance matrices. Hence, the spectral meth-
ods can be uniﬁed as kernel PCA theoretically. This kernel-
based uniﬁed framework for the spectral dimensionality re-

 
 
 
 
 
 
2

ing by SDP for the transduction task in Section 4. MVU or
SDE is explained in detail in Section 5. We explain super-
vised MVU using nearest neighbors graph, by class-wise
unfolding, by Fisher criterion, and by colored MVU in Sec-
tion 6. Out-of-sample extensions of MVU using eigenfunc-
tions and kernel mapping are explained in Section 7. Some
other variants of MVU including action respecting embed-
ding, relaxed MVU, and landmark MVU for big data are
introduced in Section 8. Finally, Section 9 concludes the
paper.

Required Background for the Reader
This paper assumes that the reader has general knowledge
of calculus, linear algebra, and basics of optimization.

2. Uniﬁed Framework for Spectral Methods
After development of many spectral dimensionality reduc-
tion methods, it was found out by researchers that these
methods all can be uniﬁed as kernel Principal Component
Analysis (PCA) (Ghojogh & Crowley, 2019) with different
kernels. This is mostly because the spectral methods re-
duce to an eigenvalue or a generalized eigenvalue problem
(Ghojogh et al., 2019a). This uniﬁcation can be analyzed
from different perspectives including eigenfunction learn-
ing, having kernels in kernel PCA, and generalized embed-
ding. In the following, we explain these points of view.

2.1. Learning Eigenfunctions
After some initial work was done in (Williams & Seeger,
2000) on using eigenfunctions in machine learning, Bengio
et. al. developed eigenfunctions learning and showed that
the spectral methods actually learn eigenfunctions (Bengio
et al., 2003b;c;a; 2004; 2006).
In the following, we re-
peat some contents from our other tutorial (Ghojogh et al.,
2021b) for the sake of explanation. Refer to (Ghojogh
et al., 2021b) for more details and proofs about eigenfunc-
tions and embedding using eigenfunctions.
First, consider The eigenvalue problem for the kernel ma-
trix (Ghojogh et al., 2019a):

Kvk = δkvk,

∀k ∈ {1, . . . , n},

(1)

where vk is the k-th eigenvector and δk is the correspond-
ing eigenvalue.

Deﬁnition 1 (Eigenfunction (Ghojogh et al., 2021b)). Con-
sider a linear operator O which can be applied on a func-
tion f . If applying this operator on the function results in a
multiplication of function to a constant:

Of = λf,

(2)

then the function f is an eigenfunction for the operator O
and the constant λ is the corresponding eigenvalue.

Figure 1. The intrinsic dimensionality for Swiss roll (above) and
trefoil knot (below). Suppose a small ant, which cannot see the
whole manifold together and can only see its front, traverses the
manifold once completely. The dimensionality that the ant feels
by this traversing is the intrinsic dimensionality. For example, the
intrinsic dimensionalities of 2D Swiss roll and 3D trefoil knot are
one and two, respectively, because the ant reaches an end of line
in the former (so it is like a 1D line) and the ant reaches to its
starting point in the latter (so it is like a 2D circle).

duction encouraged the researchers to obtain the best kernel
matrix for every speciﬁc dataset.
Around the time of discovery of this uniﬁcation, it was
found out in (Lanckriet et al., 2004) that the kernel ma-
trix can be learned using Semideﬁnite Programming (SDP)
(Vandenberghe & Boyd, 1996). This kernel learning was
proposed for the goal of transduction, i.e., learning the la-
bels of an unlabeled part of data. The fact that the kernel
can be learned by SDP inspired researchers to use SDP for
learning the best kernel for dimensionality reduction and
manifold unfolding. They said now that we knew the spec-
tral methods can be seen as kernel PCA with different ker-
nels, let us learn the best kernel for manifold unfolding to
its maximum variance. Hence, Semideﬁnite Embedding
(SDE) (Weinberger et al., 2004; Weinberger & Saul, 2004;
Weinberger et al., 2005; Weinberger & Saul, 2006b) was
proposed which was later renamed to Maximum Variance
Unfolding (MVU) (Weinberger & Saul, 2006a). MVU un-
folds manifold to its maximum variance in its intrinsic di-
mensionality. For understanding the intuition of intrinsic
dimensionality, see Fig. 1. The task of manifold unfold-
ing to its maximum variance is depicted by an example in
Fig. 2. After proposal of MVU, various versions of MVU
were developed such as supervised MVU (Zhang et al.,
2005; Liu et al., 2005; Song et al., 2007; Wei et al., 2016),
landmark MVU (Weinberger et al., 2005), action respect-
ing embedding (Bowling et al., 2005), out-of-sample ex-
tensions (Chin & Suter, 2008), relaxed MVU (Hou et al.,
2008), etc.
The remainder of this paper is organized as follows. We ex-
plain uniﬁcation of spectral dimensionality reduction meth-
ods as kernel PCA in Section 2. Required background on
SDP is introduced in Section 3. We introduce kernel learn-

3

Figure 2. Iterative unfolding of a nonlinear Swiss roll manifold using maximum variance unfolding.

Now, in the feature space H, consider a kernel operator
Kp as (Williams & Seeger, 2000), (Bengio et al., 2003a,
Section 3):

(cid:90)

(Kpf )(x) :=

k(x, y) f (y) p(y) dy,

(3)

where f ∈ H and the density function p(y) can be approxi-
mated empirically. The eigenfunction problem for this ker-
nel operator is (Ghojogh et al., 2021b):

(Kpfk)(x) = λkfk(x),

∀k ∈ {1, . . . , n},

(4)

where fk(.) is the k-th eigenfunction and λk is the corre-
sponding eigenvalue.
Lemma 1 (Relation of Eigenfunctions and Eigenvectors
for Kernel (Bengio et al., 2003a, Proposition 1), (Bengio
et al., 2003c, Theorem 1)). Consider a training dataset
{xi ∈ Rd}n
i=1 and the eigenvalue problem (1) where
vk ∈ Rn and δk are the k-th eigenvector and eigenvalue
of matrix K ∈ Rn×n. If vki is the i-th element of vector
vk, the eigenfunction for the point x and the i-th training
point xi are:

n
(cid:88)

vki

˘k(xi, x),

√

n
δk
√

i=1
n vki,

fk(x) =

fk(xi) =

(5)

(6)

respectively, where ˘k(xi, x) is the centered kernel.
If x
is a training point, ˘k(xi, x) is the centered kernel over
training data and if x is an out-of-sample point,
then
˘k(xi, x) = ˘kt(xi, x) is between training set and the out-
of-sample point (n.b. see (Ghojogh et al., 2021b) for infor-
mation on kernel centering).
Theorem 1 (Embedding from Eigenfunctions of Kernel
Operator (Bengio et al., 2003a, Proposition 1), (Bengio
et al., 2003c, Section 4)). Consider a dimensionality reduc-
tion algorithm which embeds data into a low-dimensional
embedding space. Let the embedding of the point x be
Rp (cid:51) y(x) = [y1(x), . . . , yp(x)](cid:62) where p ≤ n. The
k-th dimension of this embedding is:

The Theorem 1 has been widely used for out-of-sample
(test data) embedding in many spectral dimensionality re-
duction algorithms (Bengio et al., 2003a). This theorem
also explains why spectral methods can all be seen as ker-
nel PCA for learning eigenfunctions. In the following, we
justify this insight into uniﬁcation of the spectral methods.

2.2. Uniﬁed Framework as Kernel PCA
Many spectral dimensionality reduction methods can be re-
duced to kernel PCA where the eigenvectors of the kernel
matrix or eigenfunctions of the kernel operator are used for
embedding as was stated in Lemma 1 and Theorem 1. This
uniﬁcation was analyzed in the following two categories of
papers:

• papers for eigenfunction learning:

(Bengio et al.,

2003b;c;a; 2004; 2006)

• papers for uniﬁcation by kernels constructed from dis-
tance matrices: (Ham et al., 2004) and (Strange &
Zwiggelaar, 2014, Table 2.1) and (Ghojogh et al.,
2019b)

In the following, we explain this uniﬁcation of spectral
methods as kernel PCA using both approaches together.

– Principal Component analysis (PCA):
As kernel PCA makes use of kernel trick for kernelization,
it is equivalent to PCA when a linear kernel is used (Gho-
jogh et al., 2021b). Hence, PCA (Ghojogh & Crowley,
2019) is equivalent to kernel PCA by the linear kernel:

K = X (cid:62)X.

(8)

– Multidimensional Scaling (MDS):
According to our derivations in (Ghojogh et al., 2020b) or
(Ghojogh et al., 2021b), MDS (Cox & Cox, 2008) is equiv-
alent to kernel PCA with the kernel:

K = −

1
2

HDH,

(9)

yk(x) =

(cid:112)

δk

fk(x)
√
n

=

1
√
δk

n
(cid:88)

i=1

vki

˘k(xi, x),

(7)

where D is the squared Euclidean distance matrix and
H := I − (1/n)11(cid:62) is the centering matrix.

where ˘k(xi, x) is the centered training or out-of-sample
kernel depending on whether x is a training or an out-of-
sample point.

– Spectral Clustering:
Let W ∈ Rn×n be the adjacency matrix of points (Gho-
jogh et al., 2021a). Spectral clustering (Weiss, 1999; Ng

et al., 2001; Ghojogh et al., 2021a) uses the normalized
:=
adjacency matrix as its kernel matrix. Suppose Di,i
(cid:80)n
j=1 W ij is the (i, i)-th element of the diagonal degree
matrix of adjacency matrix. Hence, the (i, j)-th element of
kernel is (Weiss, 1999; Bengio et al., 2003a):

Kij =

W ij
(cid:112)Dii Djj

,

(10)

where Kij = k(xi, xj) for the kernel function k(., .). This
relation of kernel matrix with the adjacency matrix makes
sense because kernel is a notion of similarity (Ghojogh
et al., 2021b).

– Laplacian Eigenmap:
The solution of Laplacian eigenmap (Belkin & Niyogi,
2001; Ghojogh et al., 2021a) is the generalized eigenvalue
problem (L, D) (Ghojogh et al., 2019a) where L := D −
W is the Laplacian of the adjacency matrix W . Accord-
ing to (Weiss, 1999, Normalization Lemma 1) , the eigen-
vectors of the generalized eigenvalue problem (L, D) are
equivalent to the eigenvectors of the normalized kernel in
Eq. (10).
In addition to the above analysis (Bengio et al., 2003a),
there exists another analysis for Laplacian eigenmap stated
in (Ham et al., 2004). Consider the Laplacian of graph of
data. It can, for example, be the Laplacian of RBF adja-
cency matrix. It was mentioned above that the solution of
Laplacian eigenmap is the generalized eigenvalue problem
(L, D). However, there exists another form of optimiza-
tion for Laplacian eigenmap whose solution is the eigen-
value problem for L (Ghojogh et al., 2021a). As the opti-
mization of Laplacian eigenmap is minimization, its eigen-
vectors are sorted from the smallest to largest eigenvalues
(Ghojogh et al., 2019a). However, the optimization in PCA
and kernel PCA is maximization and the eigenvectors are
sorted from the largest to smallest eigenvalues. Hence, to
have equivalency with kernel PCA, we should modify min-
imization to maximization. One way to do this is to replace
L with (Ham et al., 2004):

Rn×n (cid:51) K = L†,

(11)

where † denotes the pseudo-inverse of matrix. This replace-
ment changes minimization to maximization because the
Laplacian matrix is positive semideﬁnite (Ghojogh et al.,
2021a) and eigenvalues of inverse of a positive semideﬁ-
nite matrix are equivalent to the reciprocal of eigenvalues
that matrix while the eigenvectors remain the same. Hence,
the order of eigenvalues and eigenvectors become reversed.
It is also important that the pseudo-inverse of Laplacian
used as the kernel should be double-centered because ker-
nel is double-centered in kernel PCA (Ghojogh & Crowley,
(11) is already double-
2019).

It is interesting that Eq.

4

centered (Ham et al., 2004):

L1 = L†1 = 0 =⇒ HL†H = L†,

where H := I − (1/n)11(cid:62) is the centering matrix. Note
that L1 = L†1 = 0 holds because the row summation of
Laplacian matrix is zero (Ghojogh et al., 2021a).

– Isomap:
The kernel in Isomap is (Tenenbaum et al., 2000; Ghojogh
et al., 2020b):

K = −

1
2

HD(g)H,

(12)

where D(g) is the geodesic distance matrix whose elements
are the (squared) approximation of geodesic distances us-
ing piece-wise Euclidean distances (Ghojogh et al., 2020b).
it can be seen as kernel PCA with the above-
Hence,
mentioned kernel (Ham et al., 2004).

– Locally Linear Embedding (LLE):
The solution of LLE is the eigenvectors of M := (I −
W )(cid:62)(I − W ) where I is the identity matrix and W is the
weight matrix obtained from the linear reconstruction step
in LLE (Roweis & Saul, 2000; Ghojogh et al., 2020a). As
the optimization of LLE is minimization, its eigenvalues
are sorted from smallest to largest (Ghojogh et al., 2019a).
However, PCA and kernel PCA have maximization in their
formulation. Hence, to have equivalency with kernel PCA,
we should modify minimization to maximization. One way
to do this is to replace M with (Sch¨olkopf et al., 2002;
Bengio et al., 2003b):

Rn×n (cid:51) K = µI − M ,

(13)

where µ > 0. It is suggested in (Ham et al., 2004) to set
µ to the largest eigenvalue λmax of M so that the kernel
K becomes positive deﬁnite. This replacement of M with
the provided kernel changes minimization to maximization
because:

arg max(K)

(13)
= arg max(−M ) = arg min(M ).

Another possible replacement exists and that is replacing
M with its pseudo-inverse (Alipanahi & Ghodsi, 2011,
Section 2.1):

Rn×n (cid:51) K = M †,

(14)

whose justiﬁcation is similar to justiﬁcation of pseudo-
inverse of Laplacian for Laplacian eigenmap.

– Diffusion Map:
Diffusion map (Coifman & Lafon, 2006) is a Laplacian-
based method (Ghojogh et al., 2021a) and can be seen as

Table 1. The summary of kernels for uniﬁcation of spectral
methods as kernel PCA

Method

PCA

MDS

Spectral clustering

Laplacian eigenmap

Isomap

LLE

Diffusion map

Kernel
X (cid:62)X
− 1
2 HDH
W ij√

Dii Djj
L† or W ij√

− 1

Dii Djj
2 HD(g)H
M † or λmaxI − M
M t

a special case of kernel PCA with a speciﬁc kernel (Ham
et al., 2004), (Strange & Zwiggelaar, 2014, Chapter 2). Let
M denote the random-walk graph Laplacian normalization
of Laplacian, i.e., M := (D(α))−1L(α) where α is a pa-
rameter. At time t, we can say that M t is the probability of
going from xi to xj. The solution of diffusion map at time
t is the eigenvectors of M t sorted from largest to smallest
eigenvalues (Ghojogh et al., 2021a). Hence, we can con-
sider the following kernel for equivalency of diffusion map
and kernel PCA:

Rn×n (cid:51) K = M t,

(15)

because kernel PCA also has maximization in its formula-
tion.

2.3. Summary of Kernels in Spectral Methods
As we saw, kernels in some algorithms such as PCA and
MDS are almost closed-form and can be computed almost
fast. However, some methods such as Isomap require a
piece of code to calculate their kernels. We can have a
summary of kernels for uniﬁcation of spectral dimension-
ality reduction methods as kernel PCA. A similar summary
exists in (Strange & Zwiggelaar, 2014, Table 2.1). We sum-
marize the kernels in Table 1.

2.4. Generalized Embedding
It was demonstrated above that many of the spectral dimen-
sionality reduction methods belong to a uniﬁed framework.
Therefore, there can be generalized embedding methods
which generalize the spectral methods to broader algo-
rithms. Graph Embedding (GE) (Yan et al., 2005; 2006;
Ghojogh et al., 2021a) showed that many spectral methods,
including Laplacian eigenmap, locality preserving projec-
tion, PCA and kernel PCA, Fisher Discriminant Analysis
(FDA) and kernel FDA, MDS, Isomap, and LLE, are spe-
cial cases of a model named graph embedding. Another
generalized subspace learning method, named Roweis Dis-

5

criminant Analysis (RDA), also generalized PCA, super-
vised PCA, and FDA as uniﬁed methods. These gener-
alized methods justify from another perspective that why
spectral methods can be uniﬁed.

3. Background on Semideﬁnite Programming
Here, we review some optimization background required
for solving SDP used in MVU and kernel learning.

3.1. Unconstrained Optimization
Consider the following optimization problem:

minimize
x

f (x).

(16)

where f (.) is a convex function. Iterative optimization can
be ﬁrst-order or second-order.
Iterative optimization up-
dates solution iteratively:

x ← x + ∆x,

(17)

until ∆x becomes very small which is the convergence of
optimization.
In the ﬁrst-order optimization, the step of
updating is ∆x := −∇f (x) where ∇f (.) denotes the gra-
dient of function. Near the optimal point x∗, gradient is
very small so the second-order Taylor series expansion of
function becomes:

(x − x∗)
f (x) ≈ f (x∗) + ∇f (x∗)(cid:62)
(cid:125)

(cid:124)

(cid:123)(cid:122)
≈ 0

+

1
2

(x − x∗)(cid:62)∇2f (x∗)(x − x∗)

≈ f (x∗) +

1
2

(x − x∗)(cid:62)∇2f (x∗)(x − x∗),

where ∇2f (x) denotes the second-order gradient (Hes-
sian) of function. This shows that the function is almost
quadratic near the optimal point. Following this intuition,
Newton’s method uses Hessian ∇2f (x) in its updating
step:

∆x := −∇2f (x)−1∇f (x).

(18)

3.2. Equality Constrained Optimization
The optimization problem may have equality constraint:

minimize
x

f (x)

subject to Ax = b.

(19)

After a step of update by ∆x = u, this optimization be-
comes:

minimize
x

f (x + u)

subject to A(x + u) = b.

(20)

The Lagrangian of this optimization problem is (Boyd
et al., 2004):

L = f (x + u) + ν(cid:62)(A(x + u) − b),

where ν is the dual variable. The second-order Taylor se-
ries expansion of function f (x + u) is:

functions such as logarithm. Logarithmic barrier approxi-
mates the indicator function by:

6

f (x + u) ≈ f (x) + ∇f (x)(cid:62)u +

1
2

u(cid:62)∇2f (x∗) u.

(21)

Substituting this into the Lagrangian gives:

L = f (x) + ∇f (x)(cid:62)u +

1
2

u(cid:62)∇2f (x∗) u

+ ν(cid:62)(A(x + u) − b).

According to Karush–Kuhn–Tucker (KKT) conditions, the
primal and dual residuals must be zero:

∇xL = ∇f (x) + ∇2f (x)(cid:62)u + u(cid:62) ∇3f (x∗)
(cid:125)

(cid:124)

(cid:123)(cid:122)
≈ 0

u

+ A(cid:62)ν set= 0 =⇒ ∇2f (x)(cid:62)u + A(cid:62)ν = −∇f (x),

∇ν L = A(x + u) − b

(a)
= Au set= 0,

(22)

(23)

where we have ∇3f (x∗) ≈ 0 because the gradient of func-
tion at the optimal point vanishes and (a) is because of the
constraint Ax − b = 0 in problem (19). Eqs. (22) and (23)
can be written as a system of equations:
(cid:20)∇2f (x)(cid:62) A(cid:62)
0

(cid:21)
(cid:20)−∇f (x)
0

(cid:21) (cid:20)u
ν

(24)

A

=

(cid:21)

.

Solving this system of equations gives the desired step u
for updating the solution at the iteration.

3.3. Inequality Constrained Optimization
The optimization problem may have equality constraint:

minimize
x

f (x)

subject to fi(x) ≤ 0,

i ∈ {1, . . . , m},

(25)

Ax = b.

Barrier methods, also known as interior-point methods,
convert inequality constrained problems to equality con-
strained or unconstrained problems. Ideally, we can do this
conversion using the indicator function I(.) which is zero
if its input condition is satisﬁed and is inﬁnity otherwise
(n.b. the indicator function in optimization literature is not
like the indicator in data science which is one if its input
condition is satisﬁed and is zero otherwise). The problem
is converted to:

minimize
x

f (x) +

m
(cid:88)

i=1

I(fi(x) ≤ 0)

(26)

subject to Ax = b.

The indicator function is not differentiable because it is not
smooth. Hence, we can approximate it with differentiable

I(fi(x) ≤ 0) ≈ −

1
t

log(−fi(x)),

(27)

where the approximation becomes more accurate by t →
∞. It changes the problem to:

minimize
x

f (x) −

1
t

m
(cid:88)

i=1

log(−fi(x))

(28)

subject to Ax = b.

This optimization problem is an equality constrained op-
timization problem which we already explained how to
solve.

3.4. Semideﬁnite Programming
An optimization problem in the following form is a
Semideﬁnite Programming (SDP) (Vandenberghe & Boyd,
1996):

minimize
X

tr(C(cid:62)X)

subject to tr(A(cid:62)
tr(D(cid:62)
X (cid:23) 0,

i X) = bi,
i X) ≤ ei,

i ∈ {1, . . . , m1},

i ∈ {1, . . . , m2},

(29)

where C, Ai’s, and Di’s are constant matrices and bi’s and
ei’s are constant vectors. Note that the trace terms may be
written in summation forms. The interior-point method,
or the barrier method, introduced before, can be used for
solving SDP (Nesterov & Nemirovskii, 1994; Boyd et al.,
2004). Optimization toolboxes such as CVX (Grant et al.,
2008) often use interior-point method for solving optimiza-
tion problems such as SDP. Note that this method is itera-
tive and SDP solving usually is time consuming especially
for large matrices. Also note that SDP is a convex opti-
mization problem so it has only one local optimum which
is the global optimum (Boyd et al., 2004).

4. Kernel Learning for Transduction
Kernel learning by semideﬁnite programming was initially
proposed in (Lanckriet et al., 2004) for the goal of trans-
duction. Transduction is a task in which the labeling of
a not-completely-labeled dataset gets complete.
In other
words, using the labeled part of data, the embedding for
the unlabeled part of data is also calculated. It seems that
the paper (Lanckriet et al., 2004) has inspired the authors of
MVU or SDE (Weinberger et al., 2004) to use semideﬁnite
programming in kernel learning for the task of dimension-
ality reduction. As kernel learning for transduction is not
completely related to dimensionality reduction, we brieﬂy
introduce it and do not enter its details. The reader can re-
fer to (Lanckriet et al., 2004) or its summary in (Karimi,
2017) for more information about it.

Consider a dataset which is partially labeled. The labeled
and unlabeled sets of data can be the training and test sets,
respectively. Let the training and test sets be denoted by
{(xi, yi)}ntr
i=ntr+1, respectively, where ntr is
the number of labeled training points and nte is the number
of unlabeled test points and n := ntr + nte. The kernel
matrix has the following sub-matrices:

i=1 and {xi}nte

Rn×n (cid:51) K =

(cid:20)Ktr,tr Ktr,te
Kte,tr Kte,te,

(cid:21)

,

(30)

where Ktr,tr ∈ Rntr×ntr , Ktr,te ∈ Rntr×nte , Kte,tr ∈
Rnte×ntr , and Kte,te ∈ Rnte×nte. We will see later that
MVU (Weinberger et al., 2004) learns the best kernel ma-
trix for manifold unfolding. However, in contrast to kernel
learning in MVU which learns the kernel matrix by opti-
mization, the kernel learning proposed in (Lanckriet et al.,
2004) ﬁnds the best kernel among a set of available kernels.
Let K := {K1, . . . , Km} be the set of kernels whose el-
ements can be different kernels such as linear kernel, RBF
kernel, Laplacian kernel, etc (Ghojogh et al., 2021b). Ker-
nel learning in this section learns the best kernel as one of
the kernels in the set.
We can learn the kernel among the sets of kernels by the
following optimization problem (Lanckriet et al., 2004,
Theorem 16):

t

minimize
K,ν,δ,λ,t
subject to tr(K) = c1,

K ∈ K,

ν ≥ 0,

δ ≥ 0,
(cid:20) G(Ktr) + τ I
(1 + ν − δ + λy)(cid:62)

1 + ν − δ + λy
t − 2 c2 δ(cid:62)1

(cid:21)

(cid:23) 0,

(31)
where c1, c2, and τ are constants, and the (i, j)-th ele-
ment of G(Ktr) is deﬁned as Gij(Ktr) := yiyjKij. Note
that Kij is the (i, j)-th element of the kernel matrix and
we consider a binary classiﬁcation here so the labels are
yi ∈ {−1, 1}, ∀i. The derivation of this problem is avail-
able in (Karimi, 2017, Appendix B). This optimization
problem is in one of the forms of semideﬁnite program-
ming problem (Vandenberghe & Boyd, 1996). See Section
3 for more information on semideﬁnite programming and
how it is solved. As it is a SDP, it is a convex problem and
has only one local optimum (Boyd et al., 2004).
After learning the kernel K ∈ K, it can be used for pre-
dicting the labels of unlabeled part of data, i.e., the test
dataset. The paper (Lanckriet et al., 2004) uses kernel Sup-
port Vector Machine (SVM) (Vapnik, 1995) for predicting
labels using the learned kernel. Using kernelization tech-
niques (Ghojogh et al., 2021b), the predictor of labels in

7

kernel SVM becomes:

f (x) =

n
(cid:88)

i=1

αi k(xi, x) + b,

(32)

where k(., .) is the kernel function (Ghojogh et al., 2021b)
which determines the elements of the kernel matrix Kij =
k(xi, xj). Also, α = [α1, . . . , αn](cid:62) and b is the bias. By
using the learned kernel K in Eq. (32), one can predict the
labels of unlabeled test data. As was mentioned before, this
kernel learning may have inspired the authors of MVU or
SDE (Weinberger et al., 2004) to propose kernel learning
for dimensionality reduction.

5. Maximum Variance Unfolding (or

Semideﬁnite Embedding)

Kernel can be learned using SDP for the sake of dimension-
ality reduction and manifold unfolding. In the following,
we introduce kernel learning using SDP for this goal.

5.1. Intuitions and Comparison with Kernel PCA
As we saw in Section 2, most of the spectral dimensional-
ity reduction methods can be uniﬁed as kernel PCA with
different kernels. Therefore, let us learn the best kernel in
dimensionality reduction for every speciﬁc dataset.
Semideﬁnite Embedding (SDE) (Weinberger et al., 2004;
Weinberger & Saul, 2004; Weinberger et al., 2005; Wein-
berger & Saul, 2006b), which was renamed later to Max-
imum Variance Unfolding (MVU) (Weinberger & Saul,
2006a), aims to ﬁnd the best kernel which unfolds the man-
ifold of data to its maximum variance. It learns the best
kernel for manifold unfolding using semideﬁnite program-
ming. An example manifold unfolding is shown in Fig. 2
where MVU gradually unfolds the nonlinear manifold to
its extreme variance by iterations of semideﬁnite program-
ming.
By MVU, the embedding of data has its maximum variance
in the embedding space. In this sense, the goal is similar to
the goal of PCA (Ghojogh & Crowley, 2019) but there are
major differences between MVU and PCA and kernel PCA.
Some differences and similarities are:

• MVU performs embedding in the feature space or the
so-called Reproducing Kernel Hilbert Space (RKHS);
so does kernel PCA. However, PCA performs in the
input space.

• MVU learns the best kernel while kernel PCA uses a
ready kernel such as the Radial Basis Function (RBF)
kernel.

• MVU is a nonlinear method. Kernel PCA transforms
data to the feature space and then applies linear PCA
to it. See (Ghojogh et al., 2021b) for more explanation
on the difference of these two approaches.

• MVU is an iterative algorithm because it solves
semideﬁnite programming iteratively (see Section 3).
However, kernel PCA and PCA are almost closed
form. We use the word “almost” because solv-
ing the eigenvalue decomposition (EVD) or singular
value decomposition (SVD) requires some iterations
by algorithms such as the power method or Jordan’s
method (Stewart, 1993) but we can see the solution
of EVD or SVD as a black box. The iterative solu-
tion of semideﬁnite programming in MVU is much
more complicated and time consuming although the
task done by MVU is more promising in manifold un-
folding.

Assume we have a d-dimensional dataset X := {xi ∈
Rd}n
i=1. We aim to ﬁnd a p-dimensional embedding of this
dataset, denoted by Y := {yi ∈ Rp}n
i=1 where p ≤ d
and usually p (cid:28) d. Let X := [x1, . . . , xn] ∈ Rd×n and
Y := [y1, . . . , yn] ∈ Rp×n. The embedding Y is sup-
posed to be the maximum variance unfolding of the mani-
fold of data. As mentioned before, SDE (Weinberger et al.,
2004; Weinberger & Saul, 2004; Weinberger et al., 2005;
Weinberger & Saul, 2006b) or MVU (Weinberger & Saul,
2006a) performs this task. MVU embeds data in the feature
space or RKHS; in other words, the embedding space is the
feature space (RKHS). Hence:

yi = φ(xi),

∀i ∈ {1, . . . , n}.

(33)

5.2. Local Isometry
Deﬁnition 2 (Isometric Manifolds (Weinberger & Saul,
2004)). Two Riemannian manifolds are isometric if there
is a diffeomorphism such that the metric on one of them
pulls back to the metric on the other one. In other words,
isometry is is a smooth invertible mapping which locally
looks like an afﬁne transformation, i.e., a rotation and a
translation. Hence, isometry preserves the local distances
on the manifold.

We use the notion of isometry between the data X and
their embedding Y. In other words, the local structure of
data should be preserved in the embedding space (Saul &
Roweis, 2003). The datasets X and Y are locally isometric
if they have similar rotation and translation relations be-
tween neighbor points. Let xj and xl be neighbors of xi
so that they form a triangle. This triangle should also exist
in the low dimensional embedding space with some rota-
tion and translation. Hence, for isometry, we should have
equal relative angles of points:

(yi − yj)(cid:62)(yi − yl) = (xi − xj)(cid:62)(xi − xl),

(34)

because inner product is proportional to the cosine of angle.
A special case of Eq. (34) is l = j:

(cid:107)yi − yj(cid:107)2

2 = (cid:107)xi − xj(cid:107)2
2.

(35)

8

We denote the Gram (kernel) matrices of points in the in-
put and embedding spaces by Rn×n (cid:51) G := X (cid:62)X and
Rn×n (cid:51) K := Y (cid:62)Y , respectively. Let Gij and Kij de-
note the (i, j)-th element of G and K, respectively. We
have:

2 = (xi − xj)(cid:62)(xi − xj)

(cid:107)xi − xj(cid:107)2
= x(cid:62)

i xi + x(cid:62)

j xj − 2 x(cid:62)

i xj = Gii + Gjj − 2 Gij.

Likewise, in the embedding space we have:

2 = (yi − yj)(cid:62)(yi − yj)

(cid:107)yi − yj(cid:107)2
= y(cid:62)

i yi + y(cid:62)

j yj − 2 y(cid:62)

i yj

(33)
= φ(xi)(cid:62)φ(xi) + φ(xj)(cid:62)φ(xj) − 2 φ(xi)(cid:62)φ(xj)
(a)
= Kii + Kjj − 2 Kij,

(36)

where (a) is because of kernel trick (Ghojogh et al.,
2021b). Note that Eq. (36) is in fact the distance of points
in RKHS equipped with kernel k (Sch¨olkopf, 2001; Gho-
jogh et al., 2021b):

(cid:107)yi − yj(cid:107)2
2

(33)
= (cid:107)φ(xi) − φ(xj)(cid:107)2

k = Kii + Kjj −2Kij.

Therefore, Eq. (35) is simpliﬁed to:

Kii + Kjj − 2 Kij = Gii + Gjj − 2 Gij.

(37)

Some version of MVU uses k-Nearest Neighbors (kNN)
in the local isometry (Weinberger et al., 2004). This ver-
sion forms a kNN graph between points of training data
points. Hence, we know the k neighbors of every point in
the dataset. Let τij be one if xj is a neighbor of xi and
zero otherwise:

τij :=

(cid:26) 1 xj ∈ kNN(xi)
0 Otherwise.

(38)

Considering the neighboring points only to have local
isometry modiﬁes Eq. (37) slightly:

τij(Kii + Kjj − 2 Kij) = τij(Gii + Gjj − 2 Gij).

(39)

5.3. Centering
We also want the embeddings of points to have zero mean:

n
(cid:88)

i=1

yi = 0.

(40)

This removes the translational degree of freedom. Accord-
ing to Eq. (33), centered embedding data means centered
pulled data in the feature space. According to (Ghojogh
et al., 2021b), this is equivalent to double centering the ker-
nel matrix, resulting in:

1
n

n
(cid:88)

n
(cid:88)

i=1

j=1

Kij = 0 =⇒

n
(cid:88)

n
(cid:88)

i=1

j=1

Kij = 0.

(41)

Another justiﬁcation for this is as follows. From Eq. (40),
we have:

(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

(cid:12)
2
(cid:12)
(cid:12)

yi

=

n
(cid:88)

n
(cid:88)

y(cid:62)

i yj

(33)
=

n
(cid:88)

n
(cid:88)

i=1

j=1

φ(xi)(cid:62)φ(xj)

i=1
n
(cid:88)

j=1
n
(cid:88)

i=1

j=1

(a)
=

Kij

(40)
= 0,

where (a) is because of the kernel trick (Ghojogh et al.,
2021b).

5.4. Positive Semideﬁniteness
As we saw so far, we are expressing embedding using ker-
nels because we assumed the embedding space is the fea-
ture space. Therefore, optimization can be performed over
the kernel matrix rather than the embedding points. How-
ever, a valid Mercer kernel should be symmetric and posi-
tive semideﬁnite (Ghojogh et al., 2021b). Hence, the ker-
nel matrix should be constrained to belong to the cone of
semideﬁnite matrices:

K ∈ Sn
+,

or K (cid:23) 0.

(42)

5.5. Manifold Unfolding
The goal of MVU is unfolding the manifold of data with
its maximum variance, as shown in Fig. 2. According to
the deﬁnition of variance, MVU maximizes the following
quantitative:

T (Y ) :=

1
2n

n
(cid:88)

n
(cid:88)

i=1

j=1

(cid:107)yi − yj(cid:107)2
2.

(43)

Lemma 2 (Boundedness of Variance of Embedding (Wein-
berger & Saul, 2006b)). The value of T (Y ) is bounded.

Proof. Suppose ηij = 1 if xj is one of the k-nearest neigh-
bors (kNN) of xi; otherwise, ηij = 0. Let:

(cid:16)

τ := max

i,j

ηij (cid:107)xi − xj(cid:107)2

(cid:17) (a)

< ∞,

(44)

where (a) is because (cid:107)xi − xj(cid:107)2 < ∞. Assuming that
the kNN graph is connected, the longest path is at most nτ ,
i.e., (cid:107)yi − yj(cid:107)2 ≤ nτ . Therefore, an upper bound on Eq.
(43) is:

T (Y ) ≤

1
2n

n
(cid:88)

n
(cid:88)

(nτ )2 =

i=1

j=1

1
2n

n2(nτ )2 =

n3τ 2
2

(44)
< ∞.

Q.E.D.

According to Lemma 2,
bounded so we can maximize it.

the variance of embedding is

The Eq. (43) can be simpliﬁed as:

9

T (Y ) :=

1
2n

n
(cid:88)

n
(cid:88)

(cid:107)yi − yj(cid:107)2
2

i=1
n
(cid:88)

j=1
n
(cid:88)

(36)
=

1
2n

=

1
2n

(41)
=

1
2n

(Kii + Kjj − 2Kij)

i=1
(cid:104) n
(cid:88)

j=1

n
(cid:88)

Kii +

Kjj − 2

n
(cid:88)

n
(cid:88)

(cid:105)

Kij

i=1

j=1

i=1
(cid:104) n
(cid:88)

j=1
n
(cid:88)

Kii +

i=1

j=1

(cid:105)

=

Kjj

(cid:104)

2

1
2n

n
(cid:88)

i=1

(cid:105)

Kii

=

1
n

n
(cid:88)

i=1

Kii ∝

n
(cid:88)

i=1

Kii

(a)
= tr(K),

(45)

where tr(.) denotes the trace of matrix and (a) is because
trace of a matrix is equivalent to the summation of its di-
agonal elements. The Eq. (45) makes sense because kernel
is a measure of similarity of points (Ghojogh et al., 2021b)
so it is related to the distance of points and the variance of
unfolding.
In summary, MVU maximizes the variance of embedding,
i.e. Eq. (45), with constraints of Eqs. (37), (41), and (42).
Hence, it solves the following optimization problem:

maximize
K

tr(K)

subject to Kii + Kjj − 2 Kij = Gii + Gjj − 2 Gij,
∀i, j ∈ {1, . . . , n},

Kij = 0,

n
(cid:88)

n
(cid:88)

i=1

j=1

K (cid:23) 0,

(46)
which is a semideﬁnite programming problem (Vanden-
berghe & Boyd, 1996). See Section 3 for more information
on semideﬁnite programming and how it is solved.
It is
valuable that Eq. (46) is a convex optimization problem so
it has only one local optimum which is the global optimum
(Boyd et al., 2004).
The Eq. (46) is using Eq. (37) for local isometry. Some
versions of MVU use Eq. (39) as the local isometry con-
straint:

maximize
K
subject to

tr(K)

τij(Kii + Kjj − 2 Kij)

= τij(Gii + Gjj − 2 Gij),
∀i, j ∈ {1, . . . , n},

(47)

Kij = 0,

n
(cid:88)

n
(cid:88)

i=1

j=1

K (cid:23) 0,

Note that if we see the entire dataset as one class and set
k = n, Eq. (47) becomes equivalent to Eq. (46). Using
kNN in Eq. (47) makes optimization slightly more efﬁcient
and faster because it does not compute kernel between all
points; although, computation of kNN graph can be time-
consuming.

5.6. Spectral Embedding
After kernel is found by solving optimization (46), we cal-
culate the eigenvalues and eigenvectors of the kernel (Gho-
jogh et al., 2019a) and then the embedding of every point
is obtained by Eq. (48) in the following lemma. The ob-
tained embedding dataset has the maximum variance in the
embedding space. Note that for ﬁnding the intrinsic dimen-
sionality of manifold, denoted by p ≤ n, we sort the eigen-
values from largest to smallest and a huge gap between two
successive eigenvalues shows a good cut-off for the num-
ber of required dimensions. A scree plot can be used for
example to visualize this cut-off. For better understanding
of intrinsic dimensionality, see the examples in Fig. 1.

Lemma 3 (Embedding from Eigenvectors of Kernel Matrix
(Ghojogh et al., 2021b)). Let vk = [vk1, . . . , vkn](cid:62) and δk
be the k-th eigenvector and eigenvalue of kernel matrix,
respectively. We can compute the embedding of point x,
denoted by y(x) = [y1(x), . . . , yp(x)](cid:62) (where p ≤ n)
using the eigenvector of kernel as:

yk(x) =

(cid:112)

δk vki.

(48)

Proof. See (Ghojogh et al., 2021b) for proof.

Another way for justifying Eq. (48) is as follows (Wein-
berger & Saul, 2006b). From the eigenvalue decomposition
of kernel matrix, we have (Ghojogh et al., 2019a):

10

6. Supervised Maximum Variance Unfolding
MVU is an unsupervised manifold learning method. There
exist several variants for supervised MVU or supervised
SDE which make use of class labels. In the following, we
introduce these methods.

6.1. Supervised MVU Using kNN within Classes
One of the methods for supervised MVU or supervised
SDE is (Zhang et al., 2005; Liu et al., 2005) which uses
k-Nearest Neighbors (kNN) within classes.
It modiﬁes
Eq. (38) in a way that the kNN graph is formed between
points of every class and not amongst the entire data points.
Hence, we know the k neighbors of every point in each
class. Let τij be one if xi and xi belong to the same class
and xj is a neighbor of xi; otherwise, it is zero. The opti-
mization problem of MVU is the same as Eq. (47) where
τij has been computed differently.

6.2. Supervised MVU by Class-wise Unfolding
The method named SMVU1 proposed in (Wei et al., 2016)
is a supervised MVU method which unfolds manifold
class-wise. Let Gc denote the set of points belonging to
the class c and ¯xc be the mean of class c. For the class c,
we deﬁne the representative xc to be the closest point of
class to the mean of class:

xc := min
xi∈Gc

(cid:107)xi − ¯xc(cid:107)2
2.

(51)

Note that, rather than the above deﬁnition, one can deﬁne
xc to be the medoid of class which is the closest point to all
points of the class; however, computation of medoid can be
more time-consuming.
Assume we have C classes denoted by {c1, . . . , cC}. Re-
call that local isometry yielded Kii + Kjj − 2 Kij =
(cid:107)xi − xj(cid:107)2

2. For class-wise local isometry, we can have:

K = V ∆V (cid:62)

=⇒ Kij =

n
(cid:88)

k=1

δk vki vkj,

∀i, j ∈ {1, . . . , n},

(49)

Kcici + Kcj cj − 2 Kcicj = α2(cid:107)xci − xcj (cid:107)2
2,

(52)

where the columns of V are the eigenvectors and the di-
agonal elements of ∆ are eigenvalues. Also kernel can be
stated as (Ghojogh et al., 2021b):

Kij = φ(xi)(cid:62)φ(xj)

(33)
= y(cid:62)

i yj.

(50)

Considering both Eqs. (49) and (50) gives:

y(cid:62)

i yj =

n
(cid:88)

k=1

δk vki vkj

(a)
=

n
(cid:88)

(cid:112)

k=1

δk vki

(cid:112)

δk vkj

=⇒ yi(k) =

(cid:112)

δk vki,

where yi(k) is the k-th element of yi and (a) is allowed be-
cause kernel matrix is positive semideﬁnite (see Eq. (42))
so its eigenvalues, δk’s, are nonnegative. This equation is
equal to Eq. (48).

∀i, j ∈ {1, . . . , C}, where α > 1 is a hyperparameter (Wei
:= φ(xci)(cid:62)φ(xcj ) is the kernel
et al., 2016) and Kcicj
between representatives of classes i and j. The paper (Wei
et al., 2016) considers the following pairs of classes:

Kcici + Kci+1ci+1 − 2 Kcici+1 = α2(cid:107)xci − xcj (cid:107)2
2,
Kcici + KcC cC − 2 KcicC = α2(cid:107)xci − xC(cid:107)2
2,

∀i, j ∈ {1, . . . , C}. We deﬁne:

Γc =

1
2

nc(cid:88)

nc(cid:88)

i=1

j=1

Γ :=

C
(cid:88)

c=1

Γc
nc

.

(Kii + Kjj − 2Kij), ∀xi, xj ∈ Gc,

(53)

(54)

where nc denotes the number of points in class c. We max-
imize this term to maximize the variance of unfolding for
each class. The SDP optimization for kernel learning is
(Wei et al., 2016):

maximize
K

C
(cid:88)

c=1

Γc
nc

subject to
τij(Kii + Kjj − 2 Kij) = τij(Gii + Gjj − 2 Gij),

∀i, j ∈ {1, . . . , n},

Kcici + Kci+1ci+1 − 2 Kcici+1 = α2(cid:107)xci − xcj (cid:107)2
2,
Kcici + KcC cC − 2 KcicC = α2(cid:107)xci − xC(cid:107)2
2,

∀i, j ∈ {1, . . . , C},

Kij = 0,

n
(cid:88)

n
(cid:88)

i=1

j=1

K (cid:23) 0,

(55)

where τij is the same as deﬁned before in Eq. (38).

6.3. Supervised MVU by Fisher Criterion
The method named SMVU2 proposed in (Wei et al., 2016)
is a supervised MVU method which unfolds manifold by
Fisher criterion; hence, we name this method Fisher-MVU.
Fisher criterion maximizes the between-class scatter and
minimizes the within-class scatter. The within-class scat-
ter in the embedding space (or feature space equipped with
kernel k) is:

σW :=

(a)
=

C
(cid:88)

c=1

C
(cid:88)

c=1

1
nc

1
nc

(cid:88)

xi∈Gc

(cid:88)

xi∈Gc

(cid:107)φ(xi) − φ(xc)(cid:107)2
k

(Kii + Kcc − 2Kci),

(56)

where Kii := φ(xi)(cid:62)φ(xi), Kcc := φ(xc)(cid:62)φ(xc), and
Kci := φ(xc)(cid:62)φ(xi), and (a) is because of distance in
the feature space (Sch¨olkopf, 2001; Ghojogh et al., 2021b).
The between-class scatter in the embedding space (or fea-
ture space equipped with kernel k) is:

C
(cid:88)

C
(cid:88)

ci=1

cj =1

C
(cid:88)

C
(cid:88)

σB :=

(a)
=

(cid:107)φ(xci ) − φ(xcj )(cid:107)2
k

(Kcici + Kcj cj − 2Kcicj ).

(57)

ci=1

cj =1

11

necessarily one of the points, its embedding (in the feature
space) is not available. Therefore, the scatter of all points
from classes is used for computation of between-class scat-
ter. One of the forms for Fisher criterion is (Fukunaga,
1990):

Γ = C × (σB − σW ),

(58)

which should be maximized. The SDP optimization for
kernel learning is (Wei et al., 2016):

maximize
K
subject to

C(σB − σW )

τij(Kii + Kjj − 2 Kij)

= τij(Gii + Gjj − 2 Gij),
∀i, j ∈ {1, . . . , n},

(59)

Kij = 0,

n
(cid:88)

n
(cid:88)

i=1

j=1

K (cid:23) 0.

6.4. Supervised MVU by Colored MVU
One of the supervised approches for MVU is colored MVU
(Song et al., 2007) which uses some side information such
as labels.
In its formulation, it uses the Hilbert-Schmidt
Independence Criterion (HSIC) (Gretton et al., 2005) be-
tween embedded data points {yi = φ(xi)}n
i=1 and labels
{li}n

i=1:

HSIC :=

1

(n − 1)2 tr(KlHKH)

(a)
=

1

(n − 1)2 tr(HKHK l),

(60)

where (a) is because of cyclic property of trace, K ∈
Rn×n and Kl ∈ Rn×n are kernel matrices over embed-
ded points and labels, respectively, and Rn×n (cid:51) H :=
I−(1/n)11(cid:62) is the centering matrix. HSIC is a measure of
dependence between two random variables (Ghojogh et al.,
2021b). Colored MVU maximizes the HSIC between the
embedded points and the labels; in other words, the depen-
dence of embedding and labels is maximized to be super-
vised. Therefore, colored MVU maximizes Eq. (60), i.e.
tr(HKHK l) or tr(KHKlH), rather than maximizing
tr(K) which is done in MVU (Song et al., 2007):

maximize
K
subject to

tr(HKHK l)

τij(Kii + Kjj − 2 Kij)

= τij(Gii + Gjj − 2 Gij),
∀i, j ∈ {1, . . . , n},

(61)

where (a) is because of distance in the feature space
(Sch¨olkopf, 2001; Ghojogh et al., 2021b). Note that the
between-class scatter has another form which uses the
mean of classes; however, as the mean of a class is not

Kij = 0,

n
(cid:88)

n
(cid:88)

i=1

j=1

K (cid:23) 0.

Note that as kernel is a soft measure of similarity (Ghojogh
et al., 2021b), the labels or side information {li}n
i=1 can
be soft labels (e.g., regression labels) or hard labels (e.g.,
classiﬁcation labels). In case of hard labels, one of the best
choices for the Kl is delta kernel (Barshan et al., 2011)
where the (i, j)-th element of kernel is:

Kl(i, j) = δli,lj :=

(cid:26) 1
0

if li = lj,
if li (cid:54)= lj,

(62)

where δli,lj is the Kronecker delta which is one if the xi
and xj belong to the same class.

7. Out-of-sample Extension of MVU
There are several approaches for out-of-sample extension
of MVU. In the following, we introduce some of these ap-
proaches which are approaches using eigenfunctions (Chin
& Suter, 2008) and kernel mapping (Gisbrecht et al., 2015).
There exist some other methods for out-of-sample exten-
sion such as (Bunte et al., 2012) which we do not cover
here.

7.1. Out-of-sample Extension Using Eigenfunctions
One of the methods for out-of-sample extension of MVU is
using eigenfunctions (Chin & Suter, 2008). Recall Eq. (5)
which relates the eigenvectors of kernel function and eigen-
functions of kernel operator. According to (Schwaighofer
et al., 2005), this equation can be slightly modiﬁed to:

fk(x) =

n
(cid:88)

i=1

bki r(xi, x) =

n
(cid:88)

i=1

bki r(x, xi),

(63)

where r(., .) is an auxiliary smoothing kernel such as the
RBF kernel, and bki is the i-th element of:
Rn (cid:51) bk := (R + ηI)−1vk,

(64)

where vk is the k-th eigenvector of kernel matrix K ∈
Rn×n over training data, and R ∈ Rn×n is the smooth-
ing kernel matrix on the n training data points using kernel
r(., .) and η > 0 is a regularization parameter for stable
inverse of matrix.
According to the Mercer’s theorem (Ghojogh et al., 2021b),
the kernel can be written as (Chin & Suter, 2008):

k(x, xi) =

n
(cid:88)

j=1

λjψj(x)ψj(xi)

(a)
= r(x)(cid:62)(R + ηI)−1K(R + ηI)−1r(xi)

(65)

where (a) is because of Eqs.
[r(x1, x), . . . , r(xn, x)](cid:62) ∈ Rn, {ψj}n
j=1
are the eigenfunctions and eigenvalues of the kernel opera-
tor k, respectively. According to Eq. (7), we have:

(63) and (64), r(x) :=

j=1 and {λj}n

yk(x) =

1
√
δk

n
(cid:88)

i=1

vki k(x, xi)

(65)
= pk r(x),

(66)

12

where δk is the k-th eigenvalue of kernel matrix K and
(Chin & Suter, 2008):

R1×n (cid:51) pk := δ−1/2

k

k R(R + ηI)−1K(R + ηI)−1.
v(cid:62)
(67)

Therefore, for a point x which can be an out-of-sample
point, the k-th dimension of embedding is calculated using
(66) and considering the top p ≤ n dimensions of
Eq.
embedding gives Rp (cid:51) y(x) = [y1(x), . . . , yp(x)](cid:62).

7.2. Out-of-sample Extension Using Kernel Mapping
There is a kernel mapping method (Gisbrecht et al., 2015)
to embed the out-of-sample data in MVU. We introduce
this method here. We deﬁne a map which maps any data
point as x (cid:55)→ y(x), where:

Rp (cid:51) y(x) :=

n
(cid:88)

j=1

αj

k(x, xj)
(cid:96)=1 k(x, x(cid:96))

(cid:80)n

,

(68)

and αj ∈ Rp, and xj and x(cid:96) denote the j-th and (cid:96)-th train-
ing data points, respectively. The k(x, xj) is a kernel such
as the Gaussian kernel:

k(x, xj) = exp(

−||x − xj||2
2
2 σ2
j

),

where σj is calculated as (Gisbrecht et al., 2015):

σj := γ × min

(||xj − xi||2),

i

(69)

(70)

where γ is a small positive number.
Assume we have already embedded the training data points
using MVU; therefore, the set {yi}n
i=1 is available. If we
map the training data points, we want to minimize the fol-
lowing least-squares cost function in order to get y(xi)
close to yi for the i-th training point:

minimize
αj ’s

n
(cid:88)

i=1

||yi − y(xi)||2
2,

(71)

where the summation is over the training data points. We
can write this cost function in matrix form as below:

minimize
A

||Y − K(cid:48)(cid:48)A||2
F ,

(72)

where Rn×p (cid:51) Y := [y1, . . . , yn](cid:62) and Rn×p (cid:51) A :=
[α1, . . . , αn](cid:62). The K(cid:48)(cid:48) ∈ Rn×n is the kernel matrix
whose (i, j)-th element is deﬁned to be:

K(cid:48)(cid:48)(i, j) :=

k(xi, xj)
(cid:96)=1 k(xi, x(cid:96))

(cid:80)n

.

(73)

The Eq.
value is zero. Therefore, the solution to this equation is:

(72) is always non-negative; thus, its smallest

Y − K(cid:48)(cid:48)A = 0 =⇒ Y = K(cid:48)(cid:48)A

(a)
=⇒ A = K(cid:48)(cid:48)† Y ,

(74)

where K(cid:48)(cid:48)† is the pseudo-inverse of K(cid:48)(cid:48):

K(cid:48)(cid:48)† = (K(cid:48)(cid:48)(cid:62)K(cid:48)(cid:48))−1K(cid:48)(cid:48)(cid:62),

(75)

and (a) is because K(cid:48)(cid:48)† K(cid:48)(cid:48) = I.
Finally, the mapping of Eq. (68) for the nt out-of-sample
data points is:

Y t = K(cid:48)(cid:48)

t A,

(76)

where the (i, j)-th element of the out-of-sample kernel ma-
trix K(cid:48)(cid:48)

t ∈ Rnt×n is:

K(cid:48)(cid:48)

t (i, j) :=

k(x(t)
, xj)
i
(cid:96)=1 k(x(t)
is the i-th out-of-sample data point, and xj and

, x(cid:96))

(77)

(cid:80)n

,

i

where x(t)
i
x(cid:96) are the j-th and (cid:96)-th training data points.

8. Other Variants of Maximum Variance

Unfolding

8.1. Action Respecting Embedding
Most of dimensionality reduction methods,
including
MVU, do not care about the order of points. However,
some data may consist of temporal information; for exam-
ple, the frames of a video where the order of images mat-
ters. In this case, a variant of MVU is required which con-
siders the temporal information when unfolding the mani-
fold of data. Action Respecting Embedding (ARE) (Bowl-
ing et al., 2005) is an MVU variant caring about order of
points. It has various applications in reinforcement learn-
ing and robotics (Bowling et al., 2007). ARE has the same
constraints in the optimization of MVU plus one additional
constraint about temporal information. Assume two actions
(e.g., rotation or transformation of image or a combination
of rotation and transformation), denoted by ai and aj, are
applied on data points xi and xj to result in xi+1 and xj+1,
respectively:

xi

ai−→ xi+1,
aj−→ xj+1.
Consider the same actions in the embedding space of
MVU, i.e. the feature space:

xj

φ(xi) ai−→ φ(xi+1),
aj−→ φ(xj+1).

φ(xj)

If the two actions ai and aj are equal, the distances of em-
bedded points should remain the same before and after the
action:
ai = aj =⇒ (cid:107)φ(xi) − φ(xj)(cid:107)2
k

= (cid:107)φ(xi+1) − φ(xj+1)(cid:107)2
k

(a)
=⇒ Kii + Kjj − 2Kij

= Ki+1,i+1 + Kj+1,j+1 − 2Ki+1,j+1, (78)

13

Figure 3. Visualizing the possible problem of short circuit: (a)
data points lying on a Swiss roll manifold, (b) Correct kNN graph
for the manifold, (c) and incorrect kNN graph having short cir-
cuits (shown by red edges) by some larger value for k.

where (a) is because of distance in the feature space
(Sch¨olkopf, 2001; Ghojogh et al., 2021b). Finally, given
actions {ai}n
i=1, the optimization in ARE is:

tr(K)

maximize
K
subject to
Kii + Kjj − 2 Kij = Gii + Gjj − 2 Gij,

Kii +Kjj − 2Kij = Ki+1,i+1 +Kj+1,j+1 − 2Ki+1,j+1,

∀i, j ∈ {1, . . . , n},

∀i, j : ai = aj,

Kij = 0,

n
(cid:88)

n
(cid:88)

i=1

j=1

K (cid:23) 0,

(79)
which is a SDP problem. The solution of this problem gives
a kernel for unfolding the manifold of data where the tem-
poral information of actions is taken into account. After
ﬁnding the kernel from optimization, the unfolded embed-
ding is calculated by Eq. (48).

8.2. Relaxed Maximum Variance Unfolding
Two problems exist in MVU which are addressed in relaxed
MVU (Hou et al., 2008). In the following, we explain these
problems and how relaxed MVU resolves them.

8.2.1. SHORT CIRCUITS IN kNN GRAPH
For some k values, some short circuits may happen in kNN
graph. For example, see Fig. 3. These short circuits result
in incorrect unfolding of manifold. Let kNN(xi) denote
the set of k nearest neighbor points of xi. Consider a kNN
of data where v(i, j) is an edge between xi and xj in this
graph. We deﬁne the deviation of an edge v(i, j) as (Hou
et al., 2008):

d(v(i, j)) :=

1
|kNN(xi) ∪ kNN(xj)|

×

(cid:88)

(cid:107)xl − x(mid)

ij (cid:107)2
2,

(80)

xl∈kNN(xi)∪kNN(xj )

ij

where x(mid)
:= (xi + xj)/2 and |.| is the size of set.
This deviation is related to the density of points; that is,
the lower the density, the larger the deviation. We sort the
deviation of points from smallest to largest and can discard
the points from kNN which are larger than a threshold. A
scree plot can be used for ﬁnding the suitable threshold.

8.2.2. RESCALING LOCAL DISTANCES
In some cases, the mapping is conformal but not isomet-
ric. Conformal maps are locally isometric but up to a scale.
Let s(xi) denote the average distance of xi to its k nearest
neighbors. Assuming that the original sampling in the in-
put space is uniform, the conformal scaling factor between
points xi and xj is (s(xi)s(xj))1/2 (De Silva & Tenen-
baum, 2003). Relaxed MVU shows to be robust to violation
of this assumption (Hou et al., 2008). Relaxed MVU scales
the distances of points in the local isometry constraint:

τij(Kii + Kjj − 2 Kij)

= τij(cid:107)(s(xi)s(xj))1/2(xi − xj)(cid:107)2
2.

(81)

8.3. Landmark Maximum Variance Unfolding for Big

Data

As explained in Section 3, MVU uses interior-point method
which is slow especially for big data. Landmark SDE or
Landmark MVU (Weinberger et al., 2005) uses randomly
selected landmarks from data points. Let n and m (cid:28) n
denote the total number of points and the number of land-
marks, respectively, and {µα}m
i=1 be the land-
marks. Every point can be reconstructed linearly by the
landmarks:

α=1 ⊂ {xi}n

(cid:98)xi =

m
(cid:88)

α=1

qiα µα,

(82)

where qiα, ∀i, α are the reconstruction weights.
Inspired
by Locally Linear embedding (LLE) (Ghojogh et al.,
2020a), every embedded point should also be reconstructed
from the embedded landmarks, denoted by {(cid:96)α}m
α=1 =
{φ(µα)}m
α=1, with the same reconstruction weights:

(cid:98)yi =

m
(cid:88)

α=1

qiα (cid:96)α.

(83)

Kernel can be stated as inner product of points in the feature
space (Ghojogh et al., 2021b):

Kij = φ(xi)(cid:62)φ(xj)

(33)
= y(cid:62)

i yj =⇒ Kij ≈ (cid:98)y(cid:62)

i (cid:98)yj.

(83)
=⇒ Kij ≈ qiα(cid:96)(cid:62)

α (cid:96)βqiβ =⇒ K = V LV (cid:62),

(84)

where Qiα = qiα, Q ∈ Rn×m, and L = (cid:96)(cid:62)
α (cid:96)β ∈ Rm×m.
This decomposition of L shows that L (cid:23) 0 which will
be used as one of the constraints in optimization. Now,

14

consider linear reconstruction of points by all points in the
training set:

min
wij

n
(cid:88)

i=1

(cid:107)xi −

n
(cid:88)

j=1

wijxj(cid:107)2
2,

(85)

where wij’s are the reconstruction weights. This optimiza-
tion also exists in LLE and can be restated as (Ghojogh
et al., 2020a):

min
M

n
(cid:88)

n
(cid:88)

i=1

j=1

M ijxixj,

(86)

where Rn×n (cid:51) M := (I − W )(cid:62)(I − W ) where W ij =
wij. The matrix M can be found by solving the least
(86). Considering the m (cid:28) n
squares problem in Eq.
landmarks, we can decompose the matrix M as:

M ≈

(cid:20) M ll ∈ Rm×m

M lu ∈ Rm×(n−m)

M ul ∈ R(n−m)×m M uu ∈ R(n−m)×(n−m)

(cid:21)

.

(87)

As the reconstruction weights qiα, ∀i, α can be seen as a
subset of reconstruction weights wij, we can write Q using
the parts of matrix M (Weinberger et al., 2005):

Rn×m (cid:51) Q =

(cid:20)
I m×m
(M uu)−1M ul

(cid:21)

.

(88)

Note that this usage of a small part of matrix (because
m (cid:28) n) is inspired by the Nystr¨om method (Ghojogh et al.,
2021b).
Landmark MVU solves the following SDP optimization
problem (Weinberger et al., 2005):

maximize
L

subject to

tr(QLQ(cid:62))

τij((QLQ(cid:62))ii + (QLQ(cid:62))jj − 2 (QLQ(cid:62))ij)
≤ τij(Gii + Gjj − 2 Gij),
∀i, j ∈ {1, . . . , n},

(QLQ(cid:62))ij = 0,

n
(cid:88)

n
(cid:88)

i=1

j=1

L (cid:23) 0,

(89)
where the optimization variable is the small matrix L ∈
Rm×m rather than the large matrix K ∈ Rn×n; hence,
a big problem is reduced to an efﬁcient small one. Note
that the paper (Weinberger et al., 2005) has converted the
equality of local isometry constraint to inequality. This is
okay because it has made the constraint more restricted and
harder than the equality constraint. After solving the opti-
mization problem to ﬁnd the optimal L, we use Eqs. (88)
and (84) to calculate K and then embedding is calculated
using Eq. (48).

8.4. Other Improvements over Maximum Variance

Unfolding and Kernel Learning

An existing book chapter on MVU is (Wang, 2012, Chap-
ter 9). There have been other improvements over MVU.
There exist some other improvements over MVU and ker-
nel learning by SDP whose details we do not cover in this
paper. We list these improvements here. An application of
MVU in nonlinear process monitoring can be found in (Liu
et al., 2014). SDP has also been used for kernel matrix
completion (Graepel, 2002) and low-rank kernel learning
(Kulis et al., 2006). Maximum covariance unfolding (Ma-
hadevan et al., 2011) has been proposed for bimodal man-
ifold unfolding. We can also interpret MVU as a regular-
ized shortest path problem on the graph of data (Paprotny
& Garcke, 2012); hence, it can be related to the Isomap
algorithm (Ghojogh et al., 2020b).

9. Conclusion
In this paper, we ﬁrst explained how the spectral dimen-
sionality reduction methods can be uniﬁed as kernel PCA
with different kernels using eigenfunction learning and ker-
nel construction by distance matrices. Then, we said as the
spectral methods are uniﬁed as kernel PCA, let us learn the
best kernel for unfolding the manifold of data. We intro-
duced MVU and its variants as some methods for learning
the best kernel for manifold unfolding using SDP optimiza-
tion problems.

Acknowledgement
About the background on semideﬁnite programming, great
videos of Convex Optimization I and II by Prof. Stephen
Boyd exist
in the channel of Stanford University on
YouTube. Videos on unifying spectral methods as well as
MVU and ARE exist by Prof. Ali Ghodsi at University of
Waterloo available on YouTube.

References
Alipanahi, Babak and Ghodsi, Ali. Guided locally lin-
ear embedding. Pattern recognition letters, 32(7):1029–
1035, 2011.

Barshan, Elnaz, Ghodsi, Ali, Azimifar, Zohreh, and
Jahromi, Mansoor Zolghadri. Supervised principal com-
ponent analysis: Visualization, classiﬁcation and regres-
sion on subspaces and submanifolds. Pattern Recogni-
tion, 44(7):1357–1371, 2011.

15

MDS, eigenmaps, and spectral clustering. Advances
in neural information processing systems, 16:177–184,
2003a.

Bengio, Yoshua, Vincent, Pascal, Paiement, Jean-Franc¸ois,
Delalleau, O, Ouimet, M, and LeRoux, N. Learning
eigenfunctions of similarity:
linking spectral cluster-
ing and kernel PCA. Technical report, Departement
d’Informatique et Recherche Operationnelle, 2003b.

Bengio, Yoshua, Vincent, Pascal, Paiement, Jean-Franc¸ois,
Delalleau, Olivier, Ouimet, Marie, and Le Roux, Nico-
Spectral clustering and kernel PCA are learn-
las.
ing eigenfunctions.
Technical report, Departement
d’Informatique et Recherche Operationnelle, Technical
Report 1239, 2003c.

Bengio, Yoshua, Delalleau, Olivier, Roux, Nicolas Le,
Paiement, Jean-Franc¸ois, Vincent, Pascal, and Ouimet,
Marie. Learning eigenfunctions links spectral embed-
ding and kernel PCA. Neural computation, 16(10):
2197–2219, 2004.

Bengio, Yoshua, Delalleau, Olivier, Le Roux, Nicolas,
Paiement, Jean-Franc¸ois, Vincent, Pascal, and Ouimet,
In Feature
Marie. Spectral dimensionality reduction.
Extraction, pp. 519–550. Springer, 2006.

Bowling, Michael, Ghodsi, Ali, and Wilkinson, Dana. Ac-
tion respecting embedding. In Proceedings of the 22nd
international conference on Machine learning, pp. 65–
72, 2005.

Bowling, Michael, Wilkinson, Dana, Ghodsi, Ali, and Mil-
stein, Adam. Subjective localization with action respect-
In Robotics Research, pp. 190–202.
ing embedding.
Springer, 2007.

Boyd, Stephen, Boyd, Stephen P, and Vandenberghe,
Lieven. Convex optimization. Cambridge university
press, 2004.

Bunte, Kerstin, Biehl, Michael, and Hammer, Barbara. A
general framework for dimensionality-reducing data vi-
sualization mapping. Neural Computation, 24(3):771–
804, 2012.

Chin, Tat-Jun and Suter, David. Out-of-sample extrapola-
tion of learned manifolds. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 30(9):1547–1556,
2008.

Belkin, Mikhail and Niyogi, Partha. Laplacian eigenmaps
and spectral techniques for embedding and clustering. In
Nips, volume 14, pp. 585–591, 2001.

Coifman, Ronald R and Lafon, St´ephane. Diffusion maps.
Applied and computational harmonic analysis, 21(1):5–
30, 2006.

Bengio, Yoshua, Paiement, Jean-franc¸cois, Vincent, Pas-
cal, Delalleau, Olivier, Roux, Nicolas, and Ouimet,
Marie. Out-of-sample extensions for LLE, Isomap,

Cox, Michael AA and Cox, Trevor F. Multidimensional
scaling. In Handbook of data visualization, pp. 315–347.
Springer, 2008.

De Silva, Vin and Tenenbaum, Joshua B. Global ver-
sus local methods in nonlinear dimensionality reduction.
Advances in neural information processing systems, pp.
721–728, 2003.

Fukunaga, Keinosuke.

Introduction to statistical pattern

recognition. Academic Press, 1990.

Ghojogh, Benyamin. Data Reduction Algorithms in Ma-
chine Learning and Data Science. PhD thesis, Univer-
sity of Waterloo, 2021.

Ghojogh, Benyamin and Crowley, Mark. Unsupervised
and supervised principal component analysis: Tutorial.
arXiv preprint arXiv:1906.03148, 2019.

Ghojogh, Benyamin, Karray, Fakhri, and Crowley, Mark.
Eigenvalue and generalized eigenvalue problems: Tuto-
rial. arXiv preprint arXiv:1903.11240, 2019a.

Ghojogh, Benyamin, Samad, Maria N, Mashhadi,
Sayema Asif, Kapoor, Tania, Ali, Wahab, Karray,
Fakhri, and Crowley, Mark. Feature selection and fea-
ture extraction in pattern analysis: A literature review.
arXiv preprint arXiv:1905.02845, 2019b.

Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
Locally linear embedding and
arXiv preprint

Crowley, Mark.
its variants: Tutorial and survey.
arXiv:2011.10925, 2020a.

Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
Crowley, Mark. Multidimensional scaling, Sammon
arXiv
mapping, and Isomap: Tutorial and survey.
preprint arXiv:2009.08136, 2020b.

Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
Laplacian-based dimensionality re-
Crowley, Mark.
duction including spectral clustering, Laplacian eigen-
map, locality preserving projection, graph embedding,
and diffusion map: Tutorial and survey. arXiv preprint
arXiv:2106.02154, 2021a.

Ghojogh, Benyamin, Karray, Fakhri, and Crowley, Mark.
Reproducing kernel Hilbert space, Mercer’s theorem,
eigenfunctions, Nystr¨om method, and use of kernels in
machine learning: Tutorial and survey. arXiv preprint
arXiv:2106.08443, 2021b.

Gisbrecht, Andrej, Schulz, Alexander, and Hammer, Bar-
bara. Parametric nonlinear dimensionality reduction us-
ing kernel t-sne. Neurocomputing, 147:71–82, 2015.

Graepel, Thore. Kernel matrix completion by semideﬁnite
programming. In International Conference on Artiﬁcial
Neural Networks, pp. 694–699. Springer, 2002.

Grant, Michael, Boyd, Stephen, and Ye, Yinyu. CVX: Mat-
lab software for disciplined convex programming, 2008.

16

Gretton, Arthur, Bousquet, Olivier, Smola, Alex, and
Sch¨olkopf, Bernhard. Measuring statistical dependence
with Hilbert-Schmidt norms. In International conference
on algorithmic learning theory, pp. 63–77. Springer,
2005.

Ham,

Jihun, Lee, Daniel D, Mika, Sebastian, and
Sch¨olkopf, Bernhard. A kernel view of the dimensional-
ity reduction of manifolds. In Proceedings of the twenty-
ﬁrst international conference on Machine learning, pp.
47, 2004.

Hou, Chenping, Jiao, Yuanyuan, Wu, Yi, and Yi, Dongyun.
Relaxed maximum-variance unfolding. Optical Engi-
neering, 47(7):077202, 2008.

Karimi, Amir-Hossein. A summary of the kernel matrix,
and how to learn it effectively using semideﬁnite pro-
gramming. arXiv preprint arXiv:1709.06557, 2017.

Kulis, Brian, Sustik, M´aty´as, and Dhillon, Inderjit. Learn-
ing low-rank kernel matrices. In Proceedings of the 23rd
international conference on Machine learning, pp. 505–
512, 2006.

Lanckriet, Gert RG, Cristianini, Nello, Bartlett, Peter,
Ghaoui, Laurent El, and Jordan, Michael I. Learning the
kernel matrix with semideﬁnite programming. Journal
of Machine learning research, 5(Jan):27–72, 2004.

Liu, Ning, Bai, Fengshan, Yan, Jun, Zhang, Benyu, Chen,
Zheng, and Ma, Wei-Ying. Supervised semi-deﬁnite
embedding for email data cleaning and visualization.
In Asia-Paciﬁc Web Conference, pp. 972–982. Springer,
2005.

Liu, Yuan-Jui, Chen, Tao, and Yao, Yuan. Nonlinear pro-
cess monitoring and fault isolation using extended max-
imum variance unfolding. Journal of process control, 24
(6):880–891, 2014.

Mahadevan, Vijay, Wong, Chi, Pereira, Jose, Liu, Tom,
Vasconcelos, Nuno, and Saul, Lawrence. Maximum co-
variance unfolding: Manifold learning for bimodal data.
Advances in Neural Information Processing Systems, 24:
918–926, 2011.

Nesterov, Yurii and Nemirovskii, Arkadii.

Interior-point
polynomial algorithms in convex programming. SIAM,
1994.

Ng, Andrew, Jordan, Michael, and Weiss, Yair. On spectral
clustering: Analysis and an algorithm. Advances in neu-
ral information processing systems, 14:849–856, 2001.

Paprotny, Alexander and Garcke, Jochen. On a connec-
tion between maximum variance unfolding, shortest path
In Artiﬁcial Intelligence and
problems and Isomap.
Statistics, pp. 859–867. PMLR, 2012.

17

Weinberger, Kilian Q and Saul, Lawrence K. Unsupervised
learning of image manifolds by semideﬁnite program-
ming. International journal of computer vision, 70(1):
77–90, 2006b.

Weinberger, Kilian Q, Sha, Fei, and Saul, Lawrence K.
Learning a kernel matrix for nonlinear dimensionality re-
duction. In Proceedings of the twenty-ﬁrst international
conference on Machine learning, pp. 106, 2004.

Weinberger, Kilian Q, Packer, Benjamin, and Saul,
Lawrence K. Nonlinear dimensionality reduction by
semideﬁnite programming and kernel matrix factoriza-
tion. In AISTATS, 2005.

Weinberger, KQ and Saul, LK. Unsupervised learning of
image manifolds by semideﬁnite programming. In Pro-
ceedings of the 2004 IEEE Computer Society Conference
on Computer Vision and Pattern Recognition, volume 2.
IEEE, 2004.

Weiss, Yair. Segmentation using eigenvectors: a unifying
view. In Proceedings of the seventh IEEE international
conference on computer vision, volume 2, pp. 975–982.
IEEE, 1999.

Williams, Christopher and Seeger, Matthias. The effect of
the input density distribution on kernel-based classiﬁers.
In Proceedings of the 17th international conference on
machine learning, 2000.

Yan, Shuicheng, Xu, Dong, Zhang, Benyu, and Zhang,
Hong-Jiang. Graph embedding: A general framework
for dimensionality reduction. In 2005 IEEE Computer
Society Conference on Computer Vision and Pattern
Recognition (CVPR’05), volume 2, pp. 830–837. IEEE,
2005.

Yan, Shuicheng, Xu, Dong, Zhang, Benyu, Zhang, Hong-
Jiang, Yang, Qiang, and Lin, Stephen. Graph embedding
and extensions: A general framework for dimensionality
IEEE transactions on pattern analysis and
reduction.
machine intelligence, 29(1):40–51, 2006.

Zhang, Benyu, Yan, Jun, Liu, Ning, Cheng, Qiansheng,
Chen, Zheng, and Ma, Wei-Ying. Supervised semi-
deﬁnite embedding for image manifolds. In 2005 IEEE
International Conference on Multimedia and Expo, pp.
4–pp. IEEE, 2005.

Roweis, Sam T and Saul, Lawrence K. Nonlinear dimen-
sionality reduction by locally linear embedding. Science,
290(5500):2323–2326, 2000.

Saul, Lawrence K and Roweis, Sam T. Think globally, ﬁt
locally: unsupervised learning of low dimensional man-
ifolds. Journal of Machine Learning Research, 4:119–
155, 2003.

Sch¨olkopf, Bernhard. The kernel trick for distances. Ad-
vances in neural information processing systems, pp.
301–307, 2001.

Sch¨olkopf, Bernhard, Smola, Alexander J, and Bach, Fran-
cis. Learning with kernels: support vector machines,
regularization, optimization, and beyond. MIT press,
2002.

Schwaighofer, Anton, Tresp, Volker, and Yu, Kai. Learn-
ing gaussian process kernels via hierarchical bayes. In
Advances in neural information processing systems, pp.
1209–1216, 2005.

Song, Le, Smola, Alexander J, Borgwardt, Karsten M,
Gretton, Arthur, et al. Colored maximum variance un-
folding. In Nips, pp. 1385–1392. Citeseer, 2007.

Stewart, Gilbert W. On the early history of the singu-
lar value decomposition. SIAM review, 35(4):551–566,
1993.

Strange, Harry and Zwiggelaar, Reyer. Open Problems in
Spectral Dimensionality Reduction. Springer, 2014.

Tenenbaum, Joshua B, De Silva, Vin, and Langford,
John C. A global geometric framework for nonlinear di-
mensionality reduction. Science, 290(5500):2319–2323,
2000.

Vandenberghe, Lieven and Boyd, Stephen. Semideﬁnite

programming. SIAM review, 38(1):49–95, 1996.

Vapnik, Vladimir. The nature of statistical learning theory.

Springer science & business media, 1995.

Wang, Jianzhong. Geometric structure of high-dimensional
data and dimensionality reduction, volume 5. Springer,
2012.

Wei, Chihang, Chen, Junghui, and Song, Zhihuan. Devel-
opments of two supervised maximum variance unfolding
algorithms for process classiﬁcation. Chemometrics and
Intelligent Laboratory Systems, 159:31–44, 2016.

Weinberger, Kilian Q and Saul, Lawrence K. An introduc-
tion to nonlinear dimensionality reduction by maximum
variance unfolding. In Proceedings of the AAAI Confer-
ence on Artiﬁcial Intelligence, volume 6, pp. 1683–1686,
2006a.

