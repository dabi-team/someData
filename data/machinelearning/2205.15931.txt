2
2
0
2

y
a
M
1
3

]
E
N
.
s
c
[

1
v
1
3
9
5
1
.
5
0
2
2
:
v
i
X
r
a

The Environmental Discontinuity Hypothesis for Down-Sampled Lexicase
Selection
Ryan Boldi1, Thomas Helmuth2 and Lee Spector3,1

1University of Massachusetts, Amherst, MA 01003
2Hamilton College, Clinton, NY 13323
3Amherst College, Amherst, MA 01002
rbahlousbold@umass.edu

Abstract

Down-sampling training data has long been shown to im-
prove the generalization performance of a wide range of ma-
chine learning systems. Recently, down-sampling has proved
effective in genetic programming (GP) runs that utilize the
lexicase parent selection technique. Although this down-
sampling procedure has been shown to signiﬁcantly improve
performance across a variety of problems, it does not seem to
do so due to encouraging adaptability through environmental
change. We hypothesize that the random sampling that is per-
formed every generation causes discontinuities that result in
the population being unable to adapt to the shifting environ-
ment. We investigate modiﬁcations to down-sampled lexicase
selection in hopes of promoting incremental environmental
change to scaffold evolution by reducing the amount of jar-
ring discontinuities between the environments of successive
generations. In our empirical studies, we ﬁnd that forcing in-
cremental environmental change is not signiﬁcantly better for
evolving solutions to program synthesis problems than sim-
ple random down-sampling. In response to this, we attempt
to exacerbate the hypothesized prevalence of discontinuities
by using only disjoint down-samples to see if it hinders per-
formance. We ﬁnd that this also does not signiﬁcantly dif-
fer from the performance of regular random down-sampling.
These negative results raise new questions about the ways in
which the composition of sub-samples, which may include
synonymous cases, may be expected to inﬂuence the perfor-
mance of machine learning systems that use down-sampling.

Introduction
Genetic Programming (GP) is a supervised learning tech-
nique that takes inspiration from evolution to create com-
puter programs that solve a variety of problems. In GP, the
speciﬁcations for a program are deﬁned in terms of input-
output cases. Mimicking an evolutionary process, a set of
initial programs are created at random. Then, the programs
are evaluated on the input cases and compared to the ground
truth output value to generate error values for each individ-
ual. A set of these programs that GP deems appropriate are
selected, mutated, and sent to the next generation. This is
repeated until a solution is found that passes all the training
cases, or a limit is reached. There is a very important inter-
action at play during this process over the course of evolu-
tion. Namely, the interaction between the individuals in the

population and the training cases that they might eventually
be evaluated on. This interaction could roughly be thought
of as the interaction between a creature and its environment
during evolution.

The selection step, one of the most important aspects of
this evolutionary system, has been the subject of extensive
study in GP and beyond. This is the step where individ-
uals are picked from the current population to be parents
for the offspring in the next generation. The selection step
also dictates how many children each parent gets to have. In
the past, methods performing this selection rely on aggre-
gated ﬁtness metrics i.e. they evaluate every individual on
every test case, and sum the ﬁtness across these test cases
to generate a single ﬁtness value. The individuals are then
selected based on this single value using a selection method
like ﬁtness proportionate selection or tournament selection.
Lexicase selection (Helmuth et al., 2014) is a parent selec-
tion method that does not rely on aggregate ﬁtness or perfor-
mance metrics. Instead, individuals that are elite on a ran-
domly shufﬂed ordering of the training cases are selected.
This tends to result in the selection of specialist individuals
that trade mediocre performance across all test cases for elite
performance on a few (Helmuth et al., 2020). It has empir-
ically been shown to improve the performance of evolution
across a wide range of problem domains.

Using the analogy of a creature and its environment, lex-
icase selection can be thought of as using the random series
of challenges that happen to occur during a creature’s life
to dictate whether its genes survive to the next generation
or not. Note that this seems very similar to what truly hap-
pens in evolution in nature: creatures must be able to survive
the subset of life-threatening situations that happen to occur
during their lifetime. If a deadly situation that they are poor
at avoiding never occurs, this negative performance never
affects their selectability. Although lexicase selection seems
to build a good analogy with biological evolution, it seems
to be missing one key element: environmental change. This
is because although each selected individual was ultimately
elite on a possibly different set of cases, every single training
case could be used every single generation. This is because

 
 
 
 
 
 
of lexicase selection’s random shufﬂing of the entire train-
ing set. Every selection event (there is one selection event
for every individual in the next population), a new random
case ordering is used. Since the cases that come towards
the beginning of a shufﬂe are generally more likely to be
used in selection (as earlier cases are responsible for ﬁlter-
ing out the vast majority of individuals in the selection pool),
most (if not all) cases end up being used at least once every
generation. This means that the environment for the entire
population is generally constant from one generation to the
next (although the individuals are exposed to a different set
of cases for every selection).

Hernandez et al. (2019) recently proposed down-sampled
lexicase selection as a method to reduce the number of in-
dividual evaluations performed by using a subset of the en-
tire training set every generation. Beyond simply reducing
computational effort, this method also seems to cause en-
vironmental change between generations as the set of cases
that the individuals are exposed to changes over time. This
results in cases that are not currently in the down-sample ex-
erting no selection pressure on the population. In terms of
the environment analogy, this can be thought of as the set of
all possible challenges that an individual can face being con-
strained by where it is located. It is unlikely that the same
creature would need to solve the problems that occur when
climbing a tree and swimming in the deep ocean. When us-
ing full lexicase selection, however, individuals could (and
usually do) get tested on their ability to perform many com-
binations of tasks that are not constrained in any way over
evolutionary time. Empirically, runs using down-sampled
lexicase selection have been shown to signiﬁcantly improve
the performance of GP when compared to standard lexicase
selection. However, Helmuth and Spector (2021) found evi-
dence for the beneﬁt of down-sampled lexicase selection not
arising from the environmental change it causes. Instead, the
beneﬁt might be due to an increased number of individuals
being evaluated across evolution.

Although not found to be the driver behind down-sampled
lexicase selection’s success, evolutionary change has been
found to be an important contributer to the evolvability of an
evolutionary system (Levins, 1968). Kashtan et al. (2007)
found that a varying environment facilitates faster evolution-
ary adaptation in biological simulations. This was later ex-
plored in using dynamic environments to improve the speed
when performing Grammatical Evolution, a variant of GP,
runs (O’Neill et al., 2011). Changing environments has also
been found to affect the speed and effecitveness of evolution
in populations of Saccharomyces cerevisiae yeast as well as
digital organisms (Canino-Koning et al., 2019). However,
the authors are not aware of any work attempting to cause
gradual environmental change when performing GP runs
with lexicase selection.

In this paper, we attempt to promote gradual environmen-
tal change in hopes of improving the performance of evo-

lutionary runs. Promoting incremental change might have
the effect of reducing the jarring discontinuities between the
set of cases in down-samples for generations in close suc-
cession. This would make it easier for the population to
adapt to the changing environment and ultimately result in
more evolutionary success. First, we propose a variant of
down-sampled lexicase selection, rolling down-sampled lex-
icase selection that iteratively removes and adds cases to the
sample over evolutionary time as an analogy to incremen-
tal environmental change in nature (e.g.a river slowly cut-
ting through a forest over many generations). We ﬁnd that
rolling with a step size of 1, or removing and adding one
case at a time is not signiﬁcantly better or worse than down-
sampled lexicase selection at the 0.05 and 0.1 down-sample
rate (5% and 10% of training cases in down-sample, respec-
tively). Upon performing an experiment where we change
the step size, we ﬁnd more evidence that suggests that rolling
is not better than simple random down-sampling. These re-
sults hint that either the random down-samples are indeed
not jarringly different from each other when considering the
behaviors they are selecting for, or that randomly rolling is
not the way to promote environmental change.

In order to determine the extent to which the case differ-
ences between our down-samples are indeed jarring when
randomly down-sampling, we attempt to move in the oppo-
site direction of rolling. By doing this, we will be able deter-
mine whether large differences between the cases in succes-
sive down-samples affects the efﬁcacy of our evolutionary
runs. To do so, we use disjoint samples that would result
in there being no identical cases in generations that come
in close succession. With these disjoint samples, we aim
to exacerbate the hypothesised jarring-ness that exists when
using purely random samples to determine whether it is de-
structive to performance. GP runs using disjoint samples
are found to not have a signiﬁcant difference in performance
when compared to simple random down-sampling.

Our experiments highlight the fact that the jarring differ-
ences between samples when randomly down-sampling do
not have a signiﬁcant detrimental effect on the success rate
of GP runs. This is probably due to the large presence of
synonymous cases in the training data. These synonymous
cases mean that random down-samples are not actually jar-
ringly different from each other in practice as although the
case labels are entirely different, these cases can measure
the same behavior and result in the same individual being
selected. Although the alterations to down-sampled lexicase
selection proposed in this paper do not improve on the so-
lution rates of runs using this selection method, we ﬁnd that
the lack of a detrimental effect due to jarring discontinu-
ities due to synonymous cases between the down-samples in
practice to be a meaningful discovery that opens a promis-
ing new research direction: how does one leverage (or avoid)
synonymous cases to maximise the success rate of GP runs
using lexicase selection?

Background and Related Work

Down-sampled Lexicase Selection

Lexicase Selection
Lexicase selection (Spector, 2012; Helmuth et al., 2014) is
a parent selection technique that does not consider aggre-
gate performance metrics to select parents for the next gen-
eration. Instead, lexicase selection selects individuals based
on their performance on a random ordering of the training
cases. First proposed to solve modal problems in GP, Lexi-
case selection has been applied in a vast range of domains,
including rule based learning systems (Aenugu and Spector,
2019), symbolic regression (La Cava et al., 2016), machine
learning (La Cava and Moore, 2020a,b; Ding and Spector,
2022) and evolutionary robotics (Moore and Stanton, 2017,
2021; Huizinga and Clune, 2018). Lexicase selection has
been demonstrated to improve solution and generalization
rates when compared to tournament selection and implicit
ﬁtness sharing (Helmuth et al., 2014).

The

algorithm,
Helmuth et al. (2020), is outlined below:

selection

lexicase

adapted

from

1. candidates is set to initially contain the entire population.

2. cases is set to initially contain the entire training set shuf-

ﬂed in a random order.

3. Collect individuals that have identical error vectors, and
maintain only one from each of these identical groups (for
performance reasons).

4. Until a parent is selected:

(a) Remove all individuals from candidates that are not

exactly the best on the ﬁrst case in cases.

(b) If only one individual remains in candidates, this be-

comes the selected parent.

(c) If there is only one case left in cases, pick an individual
from candidates at random to become the parent.

(d) Else, remove the ﬁrst case from cases.

Down-sampling Training Data
Down-sampling has often been presented in the GP lit-
erature and beyond as a method to reduce computa-
tion and improve the generalization of evolutionary runs.
Gathercole and Ross (1994) propose Dynamic Subset Se-
lection, where subsets of training data are picked to re-
duce the overall computational effort. Schmidt and Lipson
(2005) co-evolve test cases and individuals, where the set
of test cases every generation is smaller than the entire
training set. Other methods of down-sampling are also
prevalent in the ﬁeld (Hmida et al., 2019; Giacobini et al.,
Down-sampling is of-
2002; Martinez et al., 2014).
ten used in Machine Learning in general
to help im-
prove generalization rates and reduce computational over-
head of using large-scale datasets (Zogaj et al., 2021;
Katharopoulos and Fleuret, 2018).

Down-sampled lexicase Selection, ﬁrst proposed for ex-
pensive evolutionary robotics runs (Moore and Stanton,
2017) and later formalized for GP (Hernandez et al., 2019;
Ferguson et al., 2019), is a method of decreasing the number
of training cases that need to be evaluated every generation
for lexicase selection. This results in parent selection re-
quiring fewer total evaluations per generation. Instead of all
individuals in the population being evaluated on all training
cases to select parents, they are instead only evaluated on
a subset of the training set. This subset can be chosen to
be any size, but the common values are 5%, 10% or 25% of
the size of the entire training set. Compared to some more
recent methods to reduce the number of evaluations needed
to select an individual with lexicase selection (Ding et al.,
2022; de Melo et al., 2019), down-sampling provides a sure-
ﬁre way to reduce the number of program executions needed
as the size of the down-sample is chosen at will. These saved
individual evaluations can be used to decrease runtimes, or
can be used to evaluate more individuals and/or generations
with the same ﬁxed computational budget (Hernandez et al.,
2019; Helmuth and Spector, 2021). In this work, we will in-
crease the number of generations such that the same number
of individuals are evaluated as they would be when utilizing
regular lexicase selection. For example, at a down-sampling
rate of 0.1, (10% of full training set), we increase the maxi-
mum generational limit by a factor of 10 as this leads to the
same number of individual evaluations per run as without
performing down-sampling.

Hernandez et al. (2019), Ferguson et al. (2019) and more
recently, Helmuth and Spector (2021) all ﬁnd that down-
sampled lexicase selection signiﬁcantly improves the solu-
tion rate of GP runs across a variety of program synthe-
sis benchmark problems. Helmuth and Spector (2021) ﬁnd
evidence for down-sampled lexicase selection’s beneﬁt be-
ing derived from a larger amount of program space being
searched. Although it was a working hypothesis before
that work, environmental change was ruled out as the likely
cause of the beneﬁts that down-sampled lexicase presents.
To reach this conclusion, Helmuth and Spector (2021) com-
pare down-sampled lexicase selection with truncated lexi-
case selection, a variant of lexicase selection that allows for
all cases to be used all generations, but simply cuts lexicase
selection off after 10% of the cases are used (Spector et al.,
2018). Note that the difference between truncated and down-
sampled lexicase selection is that down-sampled samples the
cases before randomly shufﬂing, and uses the same sample
for every single selection in the generation. Truncated lex-
icase, on the other hand, shufﬂes and then samples for ev-
ery selection event, meaning every individual selection event
uses a possibly different set of cases. These experiments
show that down-sampled lexicase does not improve on trun-
cated lexicase selection, providing evidence for the fact that
environmental change is not a driver of the success of down-

Parameter
runs per problem
population size
training set size
maximum generations
variation operator

Value
50
1000
200
300
UMAD

Table 1: PushGP system parameters for the Fuel Cost and
Snow Day problems. Note that in our experiments our maxi-
mum generation limit is set to different values depending on
down-sampling rate.

sampled lexicase selection. We hypothesise that when using
down-sampled lexicase selection, the lack of continuity of
cases between generations might result in jarring disconti-
nuities between the environments used in successive genera-
tions. These jarring discontinuities might result in members
of the population struggling to adapt to difﬁcult cases as they
are taken out of the case pool before the population can gain
a foothold on them. If true, this would be limiting the po-
tential of down-sampled lexicase selection. In this work, we
hope to study and potentially remedy this by causing incre-
mental environmental change, where there is some similar-
ity between the down-samples so that individuals have more
than one generation to adapt to the cases in current sam-
ple. In doing this, we explore whether or not down-sampled
lexicase selection indeed does create jarring discontinuities
between successive generations and what effects this has in
practice.

Methods
The experiments in this paper were performed with the
PushGP (Spector et al., 2005; Spector and Robinson, 2002)
framework. PushGP is a GP system that evolves computer
programs written in the push programming language. The
push programming language is a stack-based language that
was designed speciﬁcally for genetic programming runs. It
has the advantage of facilitating the evolution of programs
that use multiple types and complex programming concepts
such as conditional execution, recursion and iteration. For
this paper, we use Propeller1, a Clojure implementation of
PushGP. The PushGP system parameters that we used can
be found in Table 1.

The problems used in this paper come from the second
program synthesis benchmark suite (Helmuth and Kelly,
2021). This benchmark suite contains introductory program-
ming problems that require programs to use a variety of data
types and complex control ﬂow structures. Speciﬁcally, the
two problems we have chosen are Fuel Cost and Snow Day
as these are problems where down-sampled lexicase selec-
tion has shown promise, but still has more room to improve.
Programs that successfully solve Fuel Cost must take a vec-

1https://github.com/lspector/propeller

tor of positive integers, divide each by 3, round the result
down to the nearest integer, and subtract 2. Then, the pro-
gram must return the sum of all the new integers in the vec-
tor. The second problem, Snow Day, requires solution pro-
grams to take an integer representing a number of hours and
three ﬂoats representing how much snow is on the ground,
the rate of snow fall, and the proportion of snow melting per
hour. These solution programs must return the amount of
snow on the ground after the amount of hours given.

We propose two different methods of down-sampling the
training data for lexicase selection, where one is a direct re-
sponse to experimental results from the other in hopes of
understanding why the former was not successful. The ﬁrst
of these methods, rolling lexicase selection, was an attempt
to promote incremental environmental change to lexicase se-
lection in hopes of scaffolding the evolution of PushGP pro-
grams by reducing the magnitude of discontinuities between
down-samples in successive generations. The methods here
describe experiments ﬁrst comparing rolling lexicase selec-
tion with a step size of 1 to random down-sampling and
full lexicase selection as a preliminary experiment. Then,
an analysis on the effect of step size on solution rate was
conducted on two problems in hopes of understanding the
results of the ﬁrst experiment. Finally, a new method, dis-
joint down-sampled lexicase selection is proposed as an at-
tempt to verify the effect that jarring discontinuities have on
an evolving population of GP programs in practice.

Rolling Lexicase Rolling lexicase selection is a modiﬁ-
cation of down-sampled lexicase selection that iteratively
changes the down-sample every generation, as opposed to
all at once. This would have the effect of creating a multi-
generational ﬁlter that does not have abrupt environmental
changes like that for random down-sampling. We believe
that the inclusion of incremental environmental change to
this system where the environment is shifting would allow
the members of the population to adapt more efﬁciently,
driving larger success rates. To do this, we maintain a set of
cases across generations in the active down-sample of cases.
A new hyperparameter, the step size (s) is deﬁned to be the
number of cases we drop out of the current down-sample and
replace with new cases from the entire training set. When
this step size is equal to the down-sample size, this is ex-
actly the same as performing down-sampled lexicase selec-
tion. Thus, this method can be thought of as a relaxation of
down-sampled lexicase selection where we can choose the
rate at which the environment changes. We also introduce
two different versions of rolling lexicase, bag and queue, in
hopes of understanding the differences a case being in the
down-sample for a consistent amount of time would have.
Using the bag method, the down-sample of cases are un-
ordered and are dropped and added from the down-sample
at random. i.e. if we need to drop out s cases from the down-
sample, these s cases are chosen at random when using the

Down-sampling Rate

0.05

0.1

1

Method

Downsample Size

Step Size

Type

Max Gens

Successes

p-value vs Random

Rolling

Random

Rolling

Random

Lexicase

10

1

Bag

6000

41

.80

10

1

Queue

6000

40

1.00

10

10

N/A

6000

39

20

1

Bag

3000

35

.36

20

1

Queue

3000

36

.48

20

20

N/A

3000

40

200

200

N/A

300

17

Table 2: Successes out of 50 runs for the Fuel Cost problem comparing rolling lexicase selection to simple random down-
sampling and the baseline of full lexicase selection. Both rolling and random down-sampling signiﬁcantly outperform the
baseline. There are no signiﬁcant differences between rolling and randomly down-sampled lexicase selection at the 0.05 level.
These and subsequent p-values have been calculated using a pairwise chi-squared test to show the signiﬁcance of the difference
between the performance of the rolling methods compared to that of the control (random sampling) at the same down-sampling
level.

bag method. This can be thought of as using a First In Ran-
dom Out (FIRO) rolling strategy. On the other hand, the
queue method of rolling lexicase selection offers a First In
First Out (FIFO) rolling strategy. This means that the cases
are removed from the down-sample in the order that they
are added. Each case will therefore spend the exact same
amount of time in the down-sample before being rolled out
of it. On average, however, both of these methods of rolling
would result in cases staying in the sample for roughly the
same amount of time.

The rolling lexicase selection algorithm is outlined below.
Note that this algorithm describes how the sample changes
over evolutionary time as opposed to how a parent is se-
lected. We use the lexicase selection algorithm as it is pre-
sented above to select our parents using the down-samples
we have selected. We deﬁne s to be the step size, d the
down-sampling rate, N the training set size, n = N × d the
down-sample size, and g = G
d the generational limit (where
G is the set generational limit for regular lexicase selection).

1. candidates is set to initially contain the entire population.

2. cases is set to contain the entire training set.

3. case-sample is set to initially contain a random sample of

size n from cases.

4. Until solution found or the generational limit g is reached:

(a) Evaluate all individuals on the cases in case-sample,

generating individual error vectors of length n.

(b) If any individuals pass all the cases in case-sample, re-
evaluate the best individual on the cases in cases. If
these are all passed as well, a candidate solution has
been found. Re-evaluate the best individual on the test
set.
If this individual passes all of these cases, this
counts as a success.

(c) Else, using these error vectors, use lexicase selection to

select a set of parents.

(d) Apply variational operators on these parents to produce

the next population.

(e) if using the bag variety of rolling, remove s random
cases from case-sample, and add s new ones from
cases (that are not already used). If using the queue
variety of rolling, dequeue s random cases from case-
sample, and enqueue s new ones from cases.

Disjoint Lexicase Disjoint down-sampled lexicase selec-
tion is proposed as a method that does close to the opposite
of rolling lexicase selection. As opposed to maintaining a
small set of cases across generations, disjoint down-sampled
lexicase selection ensures that not only are cases not main-
tained across generations, they are not even allowed to re-
enter the down-sample until every other case has been used
up once. This can be thought of (and indeed how it is imple-
mented) as ﬁrst selecting n random partitions of the train-
ing set of size N , where each partition is of size N
n , and
picking one of these partitions every generation until they
run out. When this happens, re-partition the training data,
and repeat. With this treatment, cases will not be repeated
for a number of generations. We predict that this method
will take the jarring discontinuities between down-samples
to a more extreme level than that with simple random down-
sampling. By doing this, we hope to see whether label-wise
discontinuities (the differences between the value of the in-
put and output cases) in practice result in real discontinuities
in which behaviors are selected for across generations. We
compare this variant of down-sampling to rolling and ran-
domly down-sampled lexicase selection to explore the ef-
fects a more jarring environmental change might have on
the evolution of solutions to GP problems.

Results and Discussion
Preliminary Experiment First, we ran a comparison of
rolling lexicase with a step size of 1, random down-sampled
lexicase, and full lexicase selection on the Fuel Cost prob-
lem. This experiment was meant to be a preliminary ex-
ploration into incrementally varying the environment for GP
runs. The results from these runs can be found in Table 2.

The results from this experiment lead us to believe that
there was no signiﬁcant improvement when incrementally
shifting the environment for down-sampled lexicase selec-
tion. Although not statistically signiﬁcant, the relative suc-
cess rates between rolling and down-sampled lexicase at the
down-sampling levels of 0.05 and 0.1 suggested that the pro-
portion of cases in the down-sample that are rolled might be
important to consider. At the 0.05 down-sampling rate, cases
are entirely refreshed (approximately for the bag method)
every 10 generations. When using a 0.1 down-sampling
rate, this number doubles to 20. The trend here seems to
suggest that the longer it takes to refresh the training set,
the lower success rates become. In order to explore the ef-
fect the rate of environmental change has on the evolution of
GP solutions, we conduct experiments varying the step sizes
for rolling lexicase selection across two different program
synthesis problems. This experiment is outlined in the next
section.

It is also interesting that we did not observe any differ-
ences between the bag and queue methods of maintaining
a sample for rolling lexicase selection. This might be due
to the fact that the individuals might require multiple gener-
ations to adapt to certain hard cases, while other cases are
passed by a lot of individuals and are no longer informative.
Since we are randomly selecting the next case to be added
in, which could be at any level of importance for the popula-
tion, any regularity in rolling with the queue method would
not result in any meaningful differences to simply randomly
rolling. Dropping out the case that was added ﬁrst might
simply have the same effect as dropping out a random case,
as the only time this dropping procedure signiﬁcantly affects
the population is if it drops out an important case, which are
placed randomly throughout the down-sample. In short, the
difference between using a bag and a queue seems to be neg-
ligible when rolling due to different cases measuring similar
things and the fact that cases will be around for the same
amount of time on average.

Step Size Variations
In order to test our reasoning for
the preliminary experiment’s negative results, we attempt
to vary the evolutionary time it takes for a down-sample to
be entirely refreshed. To do this, we repeated the prelimi-
nary experiments with step sizes other than 1 for the Fuel
Cost and Snow Day problems. To keep the comparisons
consistent, we used a down-sampling rate of 0.1, and the
bag method for rolling (as the queue method did not per-
form differently to the bag method). The Fuel Cost results

Method

Step Size

Successes

p-value

Rolling

Random

1

35

.35

3

32

.12

5

32

.12

10

36

19

36

.48

.48

20

40

Table 3: Rolling at different rates. Effect of step size on
success rate for the Fuel Cost problem, using the bag vari-
ety of rolling down-sampled lexicase selection. p-values are
shown to quantify the signiﬁcance of the difference between
each rolling method and purely random down-sampling.

Method

Step Size

Successes

p-value

Rolling

Random

2

23

.60

5

20

.30

10

22

.54

19

18

.16

20

26

Table 4: Rolling at different rates. Effect of step size on the
success rate for the Snow Day problem, using the bag va-
riety of rolling down-sampled lexicase selection. Although
not important for this experiment, regular lexicase selection
achieves 7 successes out of 50 runs.

can be found in Table 3, and the Snow Day results can be
found in Table 4. These results further reinforce the claim
that rolling lexicase selection is not signiﬁcantly better than
simple random downsampling. No individual run has a sta-
tistically signiﬁcant difference in performance to random
down-sampling, but there might be a low magnitude signal
when considering multiple runs using rolling lexicase at dif-
ferent steps sizes across two different problems. All 9 of
the rolling lexicase runs performed worse than the randomly
down-sampled run did. In fact, most of the runs have a hand-
ful fewer successes. This leads us to believe that, despite
our intuition and biological inspiration, random incremental
environmental change seems to not only not be an improve-
ment on simple random down-sampling, but it might even
be worse. While we believed that allowing for incremental
environmental change would create a multi-generational ﬁl-
ter that does not have jarring discontinuities and would pro-
vide for evolutionary advantages, it seems like achieving this
ends through randomly rolling the cases is not an effective
strategy. Whilst not that strong of a trend, it seems possible
that the runs that have a lower step size perform worse than
those at a higher step size.

A possible reason for the neutral or negative effect rolling
lexicase selection has is that cases that represent certain
niches might be left out for a long time (as it takes so long
to go over the entire training set with a small step size). Say,
for example, that we were evolving a program to perform
division. The test case representing a division by zero is ob-
viously a very important case to consider for the selection

Down-sampling Rate

0.01

0.05

0.1

1

Down-sampling Type

Random

Disjoint

Random

Disjoint

Random

Disjoint

Lexicase

Down-sample Size

Max-Gens

Successes

p-value vs Random

2

30000

33

2

30000

33

1.00

10

6000

39

10

6000

42

.61

20

3000

40

20

3000

33

.18

200

300

17

Table 5: A comparison of disjoint down-sampled lexicase selection, simple random down-sampled lexicase selection, and
regular (full) lexicase selection on the Fuel Cost problem across a range of down-sampling rates. Using disjoint samples as
opposed to random down-sampling results in no statistically signiﬁcant difference in success rates on this problem. All runs
using random or disjoint lexicase selection result in a signiﬁcant improvement on full lexicase selection.

Down-sampling Type Random Disjoint

Lexicase

Down-sample Size

Max-Gens

Successes

20

3000

26

20

3000

19

200

300

7

Table 6: A comparison of disjoint lexicase selection to ran-
dom down-sampling at the 0.1 down-sampling rate on the
Snow Day problem. As a control, the regular lexicase selec-
tion success rate is shown on the right. There are no statisti-
cally signiﬁcant differences between the success rates when
using random down-sampling and disjoint down-sampling
(p=.23). Both random and rolling down-sampled lexicase
selection signiﬁcantly improve on (full) lexicase selection
(p < 0.02).

of a candidate solution. When using full lexicase selection,
this case always has a chance to be placed near the begin-
ning of a shufﬂe, and therefore will always have a chance
to exert selection pressure. When down-sampling, this case
might be left out for a few generations, but will continually
cycle in every once in a while. When rolling, however, this
case might be out of the current case pool for many gener-
ations, which might be long enough for the individuals to
catastrophically forget how to divide by zero. This would
result in certain niches getting closed out of the population,
resulting in slight evolutionary performance losses. There is
an intricate trade-off between having a case around for long
enough for the population to adapt to it and not having the
other cases out of the down-sample for too long that the pop-
ulation forgets how to solve them. For the Fuel Cost prob-
lem, the step sizes of 3 and 5 seem to perform worse than
the rest, which is possibly the place where this trade-off is
not made optimally.

We also believe that the prevalence of synonymous cases
plays an extremely important role in the above comparisons.
Synonymous cases are cases in the training set that do not
contain exactly the same labels, but measure very similar be-
havior. These can be thought of as a set of cases that would

be passed by a very similar set of individuals in the popula-
tion every generation. This means that two sets of cases that
are entirely different from each other when it comes to in-
put and output labels could result in the selection of the very
same set of parents. It could be that the existence of these
cases would further dilute the effect rolling would have on
down-sampled lexicase selection as a randomly picked set
of cases might result in the exact same individual being se-
lected to that if we maintained a few cases from the last gen-
eration. In practice, we expect that the cases used in our pro-
gram synthesis problems are somewhat or even highly syn-
onymous, with many cases testing the same behavior of pro-
grams in very similar ways. This would explain the neutral
or negative effect that rolling seems to have on our down-
sampled lexicase selection runs at a variety of step sizes.

Disjoint Samples Due to the lack of success of rolling
the down-samples between generations, we test whether go-
ing in the opposite direction through disjoint samples would
have a signiﬁcant effect on the success rate of our GP runs.
If randomly down-sampling the training data does indeed
result in jarring discontinuities between generations, this
method of down-sampling would only exacerbate the effect
of doing so. To test this, we performed experiments compar-
ing lexicase selection with random down-sampling and dis-
joint down-sampling on the Fuel Cost and Snow Day Prob-
lems. Those results can be found in Table 5 and 6.

This set of results shows that using disjoint down-samples
is likely not signiﬁcantly different to regular random down-
sampling. While going against our prior intuition, it seems
that these disjoint samples would not signiﬁcantly change
the way selection works. A likely reason for these results
could also be due to the prevalence of synonymous cases in
our training set, which would mean that two samples that
are entirely disjoint might end up being very similar when it
comes to which parent is selected using them. Although dis-
joint down-sampling does not allow the exact same case to
be in temporally close generations, these synonymous cases
can and ultimately do end up in successive generations. This
would result in no signiﬁcant difference between disjoint

down-sampling and random down-sampling, as supported
by our empirical results.

It is also possible that disjoint down-sampling is perform-
ing worse than random when we increase the size of the
down-sample to 20. A possible reason for this could be that
when the down-samples are big enough, all cases that are
synonymous could be randomly placed in the same down-
sample, resulting in similar catastrophic forgetting to that
which we hypothesize could have happened when rolling.
For example, if all 5 cases that represent the same case
are placed in the same down-sample, this entire case niche
will not be used for the next 9 generations, which could be
enough time for the population to forget how to solve this
case.

Conclusion and Future work
In this paper, we investigate the hypothesis that down-
sampled lexicase selection causes discontinuities between
the generations of program synthesis GP runs. To do this,
we present two new methods of down-sampling the train-
ing data for lexicase selection: rolling (down-sampled) lexi-
case selection and disjoint (down-sampled) lexicase selec-
tion. Rolling lexicase selection is an attempt to encour-
age incremental environmental change in order to better al-
low the population to adapt to their changing environment
by reducing the amount of jarring discontinuities between
consecutive generations. Through an experiment where we
only vary the down-samples by one case every generation,
we ﬁnd that incrementally changing environments randomly
seems to not improve on regular random down-sampled lex-
icase selection. To test whether we were rolling too slowly,
we repeated the experiment, varying the step size. We
ﬁnd that no intermediate step size signiﬁcantly outperforms
down-sampled lexicase selection either.

In order to investigate the reasons that rolling lexicase
selection was not as efﬁcacious as we expected, we per-
formed a third set of experiments using disjoint samples.
This method of down-sampling the training set was designed
to take the discontinuities that we predicted exist when ran-
domly down-sampling to the extreme. To do this, we pro-
pose disjoint lexicase selection, whereby the set of train-
ing cases are split into disjoint sets, and each set is used
for one generation. Once all cases are used once, we re-
partition the cases, allowing for the cases to be used for a
second time. Through these experiments, we ﬁnd that dis-
joint down-sampling does not signiﬁcantly affect the solu-
tion rates when compared to random down-sampling for lex-
icase selection. While using purely disjoint samples would
result in forced jarring discontinuities when considering the
input and output labels of the cases, we hypothesize that this
does not indeed result in jarring discontinuities in the behav-
iors being measured due to the large presence of synony-
mous cases. When the down-samples are large enough, it
is possible that each down-sample could include all synony-

mous cases that measure the same behavior. This could lead
to slightly reduced performance when using disjoint down-
sampled lexicase selection due to the population forgetting
how to perform that behavior (because that case-niche is not
used for multiple generations).

Despite the fact that these results do not present a way to
improve success rates through environmental change in lexi-
case selection, we believe that this work highlights a promis-
ing direction for future exploration. As opposed to randomly
down-sampling the training cases, perhaps a more intelligent
approach would result in evolutionary runs that beneﬁt from
lower computational costs, without the loss of information
from the omission of some vital cases in the training set. In
particular, we believe sampling methods that take into ac-
count case synonymy, unlike rolling and disjoint samples,
may lead to better results. This could perhaps be achieved by
the dynamic collection of population statistics regarding the
interaction between the individuals and the training cases.
These statistics would then be used to select down-samples
that are likely to maintain behavioral niches of individuals,
while moderating the effort put into synonymous cases.

Acknowledgements
This material is based upon work supported by the Na-
tional Science Foundation under Grant No. 1617087. Any
opinions, ﬁndings, and conclusions or recommendations ex-
pressed in this publication are those of the authors and do not
necessarily reﬂect the views of the National Science Foun-
dation.

This work was performed in part using high performance
computing equipment obtained under a grant from the Col-
laborative R&D Fund managed by the Massachusetts Tech-
nology Collaborative.

The authors would like to thank Charles Ofria, Alexan-
der Lalejini, Jose Guadalupe Hernandez, Edward Pantridge,
Anil Saini and Li Ding for discussions that helped shape this
work.

References
Aenugu, S. and Spector, L. (2019). Lexicase selection in learning
classiﬁer systems. In Proceedings of the Genetic and Evolu-
tionary Computation Conference, pages 356–364.

Canino-Koning, R., Wiser, M. J., and Ofria, C. (2019). Fluctuat-
ing environments select for short-term phenotypic variation
leading to long-term exploration.

de Melo, V. V., Vargas, D. V., and Banzhaf, W. (2019). Batch
tournament selection for genetic programming: The quality
of lexicase, the speed of tournament. In Proceedings of the
Genetic and Evolutionary Computation Conference, GECCO
’19, page 994–1002, New York, NY, USA. Association for
Computing Machinery.

Ding, L., Boldi, R., Helmuth, T., and Spector, L. (2022). Lexicase
selection at scale. In Genetic and Evolutionary Computation
Conference Companion (GECCO ’22 Companion), July 9–
13, 2022, Boston, MA, USA.

Ding, L. and Spector, L. (2022). Optimizing neural networks with
In International Conference on

gradient lexicase selection.
Learning Representations.

La Cava, W. and Moore, J. H. (2020b). Learning feature spaces for
regression with genetic programming. Genetic Programming
and Evolvable Machines, 21(3):433–467.

Ferguson, A. J., Hernandez, J. G., Junghans, D., Lalejini, A., Dol-
son, E., and Ofria, C. (2019). Characterizing the effects of
random subsampling and dilution on lexicase selection.
In
Banzhaf, W., Goodman, E., Sheneman, L., Trujillo, L., and
Worzel, B., editors, Genetic Programming Theory and Prac-
tice XVII, pages 1–23, East Lansing, MI, USA. Springer.

Gathercole, C. and Ross, P. (1994). Dynamic training subset se-
In
lection for supervised learning in genetic programming.
Davidor, Y., Schwefel, H.-P., and M¨anner, R., editors, Par-
allel Problem Solving from Nature III, volume 866 of LNCS,
pages 312–321, Jerusalem. Springer-Verlag.

Giacobini, M., Tomassini, M., and Vanneschi, L. (2002). Limit-
ing the number ﬁtness cases in genetic programming using
statistics. In Merelo-Guervos, J. J., Adamidis, P., Beyer, H.-
G., Fernandez-Villacanas, J.-L., and Schwefel, H.-P., editors,
Parallel Problem Solving from Nature - PPSN VII, number
2439 in Lecture Notes in Computer Science, LNCS, pages
371–380, Granada, Spain. Springer-Verlag.

Helmuth, T. and Kelly, P. (2021). Psb2: The second program syn-

thesis benchmark suite. CoRR, abs/2106.06086.

Helmuth, T., Pantridge, E., and Spector, L. (2020). On the impor-
tance of specialists for lexicase selection. Genetic Program-
ming and Evolvable Machines, 21(3):349–373.

Helmuth, T. and Spector, L. (2021). Problem-solving beneﬁts of
down-sampled lexicase selection. Artiﬁcial Life, pages 1–21.

Helmuth, T., Spector, L., and Matheson, J. (2014). Solving uncom-
promising problems with lexicase selection. IEEE Transac-
tions on Evolutionary Computation, 19(5):630–643.

Hernandez, J. G., Lalejini, A., Dolson, E., and Ofria, C. (2019).
Random subsampling improves performance in lexicase se-
lection. In GECCO ’19: Proceedings of the Genetic and Evo-
lutionary Computation Conference Companion, pages 2028–
2031, Prague, Czech Republic. ACM.

Hmida, H., Hamida, S. B., Borgi, A., and Rukoz, M. (2019). A
new adaptive sampling approach for genetic programming.
In 2019 Third International Conference on Intelligent Com-
puting in Data Sciences (ICDS).

Huizinga, J. and Clune, J. (2018). Evolving multimodal robot
behavior via many stepping stones with the combinato-
rial multi-objective evolutionary algorithm. arXiv preprint
arXiv:1807.03392.

Kashtan, N., Noor, E., and Alon, U. (2007). Varying environments
can speed up evolution. Proceedings of the National Academy
of Sciences, 104(34):13711–13716.

La Cava, W., Spector, L., and Danai, K. (2016). Epsilon-lexicase
selection for regression. In Proceedings of the Genetic and
Evolutionary Computation Conference 2016, pages 741–748.

Levins, R. (1968). Evolution in changing environments. Princeton

University Press.

Martinez, Y., Trujillo, L., Naredo, E., and Legrand, P. (2014). A
comparison of ﬁtness-case sampling methods for symbolic
regression with genetic programming. In Tantar, A.-A., Tan-
tar, E., Sun, J.-Q., Zhang, W., Ding, Q., Schuetze, O., Em-
merich, M., Legrand, P., Del Moral, P., and Coello Coello,
C. A., editors, EVOLVE - A Bridge between Probability, Set
Oriented Numerics, and Evolutionary Computation V, vol-
ume 288 of Advances in Intelligent Systems and Computing,
pages 201–212, Peking. Springer.

Moore, J. M. and Stanton, A. (2017). Lexicase selection outper-
forms previous strategies for incremental evolution of vir-
tual creature controllers. In Knibbe, C., Beslon, G., Parsons,
D. P., Misevic, D., Rouzaud-Cornabas, J., Bred`eche, N., Has-
sas, S., 0001, O. S., and Soula, H., editors, Proceedings of
the Fourteenth European Conference Artiﬁcial Life, ECAL
2017, Lyon, France, September 4-8, 2017, pages 290–297.
MIT Press.

Moore, J. M. and Stanton, A. (2021). Objective Sampling Strate-
gies for Generalized Locomotion Behavior with Lexicase Se-
lection. volume ALIFE 2021: The 2021 Conference on Arti-
ﬁcial Life of ALIFE 2021: The 2021 Conference on Artiﬁcial
Life. 73.

O’Neill, M., Nicolau, M., and Brabazon, A. (2011). Dynamic envi-
ronments can speed up evolution with genetic programming.
In Proceedings of the 13th Annual Conference Companion on
Genetic and Evolutionary Computation, GECCO ’11, page
191–192, New York, NY, USA. Association for Computing
Machinery.

Schmidt, M. and Lipson, H. (2005). Co-evolution of ﬁtness max-
In Rothlauf, F., editor, Late
imizers and ﬁtness predictors.
breaking paper at Genetic and Evolutionary Computation
Conference (GECCO’2005), Washington, D.C., USA.

Spector, L. (2012). Assessment of problem modality by differential
performance of lexicase selection in genetic programming: a
preliminary report. In Proceedings of the 14th annual confer-
ence companion on Genetic and evolutionary computation,
pages 401–408.

Spector, L., Cava, W. L., Shanabrook, S., Helmuth, T., and
Pantridge, E. (2018). Relaxations of lexicase parent selec-
tion.
In Banzhaf, W., Olson, R. S., Tozier, W., and Riolo,
R., editors, Genetic Programming Theory and Practice XV,
pages 105–120, Cham. Springer International Publishing.

Katharopoulos, A. and Fleuret, F. (2018). Not all samples are
created equal: Deep learning with importance sampling. In
ICML.

Spector, L., Klein, J., and Keijzer, M. (2005). The push3 execution
stack and the evolution of control. In In Proc. Gen. and Evol.
Comp. Conf, pages 1689–1696. ACM Press.

La Cava, W. and Moore, J. H. (2020a). Genetic programming
In Proceedings of
approaches to learning fair classiﬁers.
the 2020 Genetic and Evolutionary Computation Conference,
pages 967–975.

Spector, L. and Robinson, A. (2002). Genetic Programming
and Autoconstructive Evolution with the Push Programming
Language. Genetic Programming and Evolvable Machines,
3(1):7–40.

Zogaj, F., Cambronero, J. P., Rinard, M. C., and Cito, J. (2021).
Doing more with less: Characterizing dataset downsampling
for automl. Proc. VLDB Endow., 14(11):2059–2072.

