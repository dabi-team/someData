1
2
0
2

p
e
S
9

]
L
M

.
t
a
t
s
[

1
v
7
4
4
4
0
.
9
0
1
2
:
v
i
X
r
a

Modeling Massive Spatial Datasets Using a
Conjugate Bayesian Linear Regression Framework

Sudipto Banerjee

UCLA Department of Biostatistics
650 Charles E. Young Drive South
Los Angeles, CA 90095-1772.

Abstract: Geographic Information Systems (GIS) and related technologies have gen-
erated substantial interest among statisticians with regard to scalable methodologies
for analyzing large spatial datasets. A variety of scalable spatial process models have
been proposed that can be easily embedded within a hierarchical modeling framework
to carry out Bayesian inference. While the focus of statistical research has mostly been
directed toward innovative and more complex model development, relatively limited
attention has been accorded to approaches for easily implementable scalable hierar-
chical models for the practicing scientist or spatial analyst. This article discusses how
point-referenced spatial process models can be cast as a conjugate Bayesian linear
regression that can rapidly deliver inference on spatial processes. The approach al-
lows exact sampling directly (avoids iterative algorithms such as Markov chain Monte
Carlo) from the joint posterior distribution of regression parameters, the latent pro-
cess and the predictive random variables, and can be easily implemented on statistical
programming environments such as R.

Keywords and phrases: Bayesian linear regression, Exact sampling-based inference,
Gaussian process, Low-rank models, Nearest-Neighbor Gaussian Processes, Sparse
models.

1. Introduction

Statistical modeling and analysis for spatial and spatial-temporal data continue to receive
much attention due to enhancements in computerized Geographic Information Systems
(GIS) and accompanying technologies. Bayesian hierarchical spatiotemporal process models
have become widely deployed statistical tools for researchers to better understand the
complex nature of spatial and temporal variability See, for example, the books Cressie
(1993), Stein (1999), Moller and Waagepetersen (2003), Schabenberger and Gotway (2004),
Gelfand et al. (2010), Cressie and Wikle (2011) and Banerjee et al. (2014) for a variety of
statistical methods in diverse applications domains.

Spatial data analysis is conveniently carried out by embedding a spatial process within

the familiar hierarchical modeling paradigm,

[data | process] × [process | parameters] × [parameters] .

(1)

Modeling for point-referenced data, which refers to data referenced by points with co-
ordinates (latitude-longitude, Easting-Northing etc.), proceeds from a random ﬁeld that
1

 
 
 
 
 
 
Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

2

introduces dependence among any ﬁnite collection of random variables. Formally, the ran-
dom ﬁeld is a stochastic process deﬁned as an uncountable set of random variables, say
{w((cid:96)) : (cid:96) ∈ L}, over a domain of interest L. This uncountable set is endowed with a proba-
bility law specifying the joint distribution for any ﬁnite subset of random variables. Spatial
processes are usually constructed assuming L ⊆ (cid:60)d (usually d = 2 or 3) or, perhaps, as
a subset of points on a sphere or ellipsoid. In spatiotemporal settings L = S × T , where
S ⊂ (cid:60)d and T ⊂ [0, ∞) are the space and time domains, respectively, and (cid:96) = (s, t) is a
space-time coordinate with spatial location s ∈ S and time point t ∈ T (see, e.g., Gneiting
and Guttorp, 2010, for details).

Gaussian random ﬁelds are speciﬁed with a covariance function cov{w((cid:96)), w((cid:96)(cid:48))} =
Kθ((cid:96), (cid:96)(cid:48)) for any two points (cid:96) and (cid:96)(cid:48) in L. If U and V are ﬁnite sets comprising n and
m points in L, respectively, then Kθ(U, V) denotes the n × m matrix whose (i, j)-th ele-
ment is evaluated using the covariance function Kθ(·, ·) between the i-th point U and the
j-th point in V. If U or V comprises a single point, Kθ(U, V) is a row or column vector,
respectively. A valid spatiotemporal covariance function ensures that Kθ(U, U) is positive
deﬁnite for any ﬁnite set U, which we will denote simply as Kθ if the context is clear. A cus-
tomary speciﬁcation models {w((cid:96)) : (cid:96) ∈ L} as a zero-centered Gaussian process, denoted as
w((cid:96)) ∼ GP (0, Kθ(·, ·)). For any ﬁnite collection U = {(cid:96)1, (cid:96)2, . . . , (cid:96)n} in L, the n × 1 random
vector wU = (w((cid:96)1)), w((cid:96)2), . . . , w((cid:96)n))(cid:62) is distributed as N (0, Kθ), where Kθ = Kθ(U, U).
Further details on valid spatial (and spatiotemporal) covariance functions can be found in
Gneiting and Guttorp (2010), Cressie (1993), Stein (1999), Gelfand et al. (2010), Cressie
and Wikle (2011) and Banerjee et al. (2014) and numerous references therein.

If y((cid:96)) represents a variable of interest at point (cid:96), then a customary spatial regression

model at (cid:96) is

y((cid:96)) = x(cid:62)((cid:96))β + w((cid:96)) + (cid:15)((cid:96)) ,

(2)

where x((cid:96)) is a p × 1 (p < n) vector of spatially referenced predictors, β is the p × 1 vector
of slopes, and w((cid:96)) ∼ GP (0, Kθ(·, ·)) is the spatial or spatiotemporal process and (cid:15)((cid:96)) is
a white noise process modeling measurement error or ﬁne scale variation attributed to
disturbances at distances smaller than the minimum observed separations in space and/or
time. We now embed (2) and the spatial process within the Bayesian hierarchical model

p(θ, β, τ ) × N (w | 0, Kθ) × N (y | Xβ + w, Dτ ) ,

(3)

where y = (y((cid:96)1), y((cid:96)2), . . . , y((cid:96)n))(cid:62) is the n × 1 vector of observed outcomes, X is the n × p
matrix of regressors with i-th row x(cid:62)((cid:96)i) and Dτ is the covariance matrix for (cid:15)((cid:96)) over
{(cid:96)1, (cid:96)2, . . . , (cid:96)n}. A common speciﬁcation is Dτ = τ 2In, where τ 2 is called the “nugget.” The
hierarchy is completed by assigning prior distributions to β, θ and τ .

For ﬁtting (3) to large spatial datasets, a substantial computational expense is incurred
from the size of Kθ. Since θ is unknown, each iteration of the model ﬁtting algorithm
will involve decomposing or factorizing Kθ, which typically requires ∼ n3 ﬂoating point
operations (ﬂops) and order of ∼ n2 for memory requirements. In geostatistical settings,

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

3

data are almost never observed on regular grids and the conﬁguration of points are typically
highly irregular. The covariance models that have been demonstrated to be most eﬀective
for inference do not, in general, result in any computationally exploitable structure for
Kθ, which makes the matrix computations prohibitive for large values of n. For Gaussian
likelihoods, one can integrate out the random eﬀects w from (3) and work with the posterior

p(θ, β, τ | y) ∝ p(θ, β, τ ) × N (y | Xβ, Kθ + Dτ ) .

(4)

This reduces the parameter space to {τ 2, θ, β} by excluding the high-dimensional vector w,
but one still needs to work with Kθ + Dτ , which is n × n. These are referred to as “big-n”
or “high-dimensional” problems in geostatistics.

There is already a substantial literature on high-dimensional spatial and spatiotempo-
ral modeling and we do not attempt to undertake a comprehensive review here; see, e.g.,
Banerjee (2017) for a focused review on some popular Bayesian approaches and Heaton
et al. (2019) for a comparative evaluation for a variety of contemporary statistical methods.
These papers, and the references therein, oﬀer a variety of algorithmic and model-based
approaches for large data. Some published methods have scalable implementations into the
millions (see, e.g., Katzfuss, 2017; Abdulah et al., 2018; Huang and Sun, 2018; Finley et al.,
2019; Zhang et al., 2019) but often require specialized high-performance computer archi-
tectures and libraries harnessing parallel processing or graphical processing units. Also,
uncertainty quantiﬁcation on the spatial process while maintaining ﬁdelity to the under-
lying probability model may also be challenging. With the advent of a new generation of
data products, there is a need for some simpler implementations that can be run on mod-
est computing architectures by practicing spatial analysts. This requires new directions in
thinking about high-dimensional spatial problems. Here, we will show how some elementary
conjugate Bayesian linear regression models can be exploited to conduct Bayesian analy-
sis for massive spatial datasets. While a common underlying idea is to approximate the
underlying spatial process with a scalable alternative, we will ensure that such approxima-
tions will result in well-deﬁned probability models. In this sense, these approaches can be
described as model-based solutions for very large spatial datasets that can be executed on
modest computing environments. One exception to the fully model-based approach will be
a divide and conquer approach that we brieﬂy review, where an approximation to the full
posterior distribution for the entire data is constructed from several posterior distributions
of smaller subsets of the data.

The balance of the paper proceeds as follows. The next section brieﬂy reviews dimen-
sion reduction and sparsity inducing spatial models. Section 3 presents some standard
distribution theory for Bayesian linear regression and outlines how scalable spatial process
models can be cast into such frameworks. A synposis of some simulation experiments and
data analysis examples are provided in Section 4. Section 5 presents an alternative ap-
proach based upon dividing and conquering the data, known as meta-kriging. The paper
concludes with some further discussion in Section 6.

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

4

2. Dimension reduction and sparsity

Dimension reduction (Wikle and Cressie, 1999) is among the most conspicuous of ap-
proaches for handling large spatial datasets. This customarily proceeds by representing or
approximating the spatial process in terms of the realizations of a latent process over a
smaller set of points, often referred to as knots. Thus,

w((cid:96)) ≈ ˜w((cid:96)) =

r
(cid:88)

j=1

bθ((cid:96), (cid:96)∗

j )z((cid:96)∗

j ) = b(cid:62)

θ ((cid:96))z,

(5)

2, . . . , (cid:96)∗

where z((cid:96)) is a well-deﬁned (usually unobserved) process and bθ(·, ·) is a family of basis
functions or kernels, possibly depending upon some parameters θ. The collection of r loca-
tions {(cid:96)∗
1, (cid:96)∗
r} are the knots, bθ((cid:96)) and z are r × 1 vectors with components bθ((cid:96), (cid:96)∗
j )
j ), respectively. Therefore, ˜w = Bθz, where ˜w = ( ˜w((cid:96)1), ˜w((cid:96)2), . . . , ˜w((cid:96)n))(cid:62) and Bθ
and z((cid:96)∗
is n × r with (i, j)-th element bθ((cid:96)i, (cid:96)∗
j )’s and the n × r
matrix Bθ. Choosing r << n eﬀectuates dimension reduction because ˜w((cid:96)), as deﬁned in
(5), spans only an r-dimensional space. When n > r, the joint distribution of ˜w is singular.
Nevertheless, we construct a valid stochastic process with covariance function

j ). We work with r (instead of n) z((cid:96)∗

cov( ˜w((cid:96)), ˜w((cid:96)(cid:48))) = b(cid:62)

θ ((cid:96))Vzbθ((cid:96)(cid:48)) ,

(6)

where Vz is the variance-covariance matrix (also depends upon parameter θ) for z. From
(6), we see that, even if bθ(·, ·) is stationary, the induced covariance function is not. If the
z’s are Gaussian, then ˜w((cid:96)) is a Gaussian process. Every choice of basis functions yields a
process and there are too many choices to enumerate here. Wikle Wikle (2010) oﬀers an
excellent overview of low rank models.

Some choices of basis functions can be more computationally eﬃcient than others de-
pending upon the speciﬁc application. For example, Cressie and Johannesson (2008) (also
see Shi and Cressie (2007)) discuss “Fixed Rank Kriging” (FRK) by constructing Bθ using
very ﬂexible families of non-stationary covariance functions to carry out high-dimensional
kriging, Cressie et al. (2010) extend FRK to spatiotemporal settings calling the procedure
“Fixed Rank Filtering” (FRF), Katzfuss and Cressie (2012) provide eﬃcient constructions
for Bθ for massive spatiotemporal datasets, and Katzfuss (2013) uses spatial basis func-
tions to capture medium to long range dependence and tapers the residual w((cid:96)) − ˜w((cid:96)) to
capture ﬁne scale dependence. Multiresolution basis functions (Nychka et al., 2002, 2015)
have been shown to be eﬀective in building computationally eﬃcient nonstationary models.
These papers amply demonstrate the versatility of low-rank approaches using diﬀerent ba-
sis functions. An alternative approach speciﬁes z((cid:96)) itself as a spatial process. This process
is called the “parent process” and one can derive a low-rank process ˜w((cid:96)) from the parent.
One such derivation emerges from truncating the Karhunen-Lo`eve (inﬁnite) basis expan-
sion for a Gaussian process to a ﬁnite number of terms to obtain a low-rank process (see,
e.g., Rasmussen and Williams, 2005; Banerjee et al., 2014). This is equivalent to projecting

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

5

the parent process on a lower-dimensional subspace determined by a partial realization
of the parent over r knots of the process. This yields the predictive process and several
variants aimed at improving the approximation (Banerjee et al., 2008, 2010; Sang et al.,
2011; Sang and Huang, 2012; Katzfuss, 2017); also see (Finley et al., 2015) and Banerjee
(2017) for computational details on eﬃciently implementing Gaussian predictive processes.
While dimension reduction methods have been applied extensively and eﬀectively to an-
alyze spatial data sets in the order of n ∼ 104, their computational eﬃciency and inferential
performance tend to struggle at even larger scales (Banerjee, 2017). More recently, there
has been substantial developments in full rank models that exploit sparsity. We introduce
sparsity either in the covariance matrix or its inverse (the precision matrix). Covariance
tapering (Furrer et al., 2006; Kaufman et al., 2008; Du et al., 2009) is in the spirit of
the former by modeling var{w} = Kθ (cid:12) Ktap,ν, where Ktap,ν is a sparse covariance ma-
trix formed from a compactly supported, or tapered, covariance function with tapering
parameter ν and (cid:12) denotes the element wise (or Hadamard) product of two matrices. The
Hadamard product retains positive deﬁniteness, so Kθ (cid:12) Ktap,ν is positive deﬁnite. Fur-
thermore, Ktap,ν is sparse because a tapered covariance function is equal to 0 for all pairs
of locations separated by a distance beyond a threshold ν. Covariance tapering is undoubt-
edly an attractive approach for constructing sparse covariance matrices, but its practical
implementation for full Bayesian inference will generally require eﬃcient sparse Cholesky
decompositions, numerically stable determinant computations and, perhaps most impor-
tantly, eﬀective memory management. These issues are yet to be tested for truly massive
spatiotemporal datasets with n ∼ 105 or more.

One could also devise models with sparse precision matrices. For ﬁnite-dimensional dis-
tributions conditional and simultaneous autoregressive (CAR and SAR) models (see, e.g.,
Cressie, 1993; Banerjee et al., 2014, and references therein) adopt this approach for are-
ally referenced datasets. The CAR models are special instances of Gaussian Markov ran-
dom ﬁelds or GMRFs (Rua and Held, 2005) that have led to the popular quadrature
based Integrated Nested Laplace Approximation (INLA) algorithms Rue et al. (2009) for
Bayesian inference and to the approximation of Gaussian processesLindgren et al. (2011).
These approaches can be computationally eﬃcient for certain classes of covariance func-
tions with stochastic partial diﬀerential equations (SPDE) representations (including the
versatile Mat´ern class), but their inferential performance on spatiotemporal or multivariate
Gaussian processes (perhaps speciﬁed through more general covariance or cross-covariance
functions) embedded within Bayesian hierarchical models is yet to be fully developed or
assessed for massive datasets.

One could also construct massively scalable sparsity-inducing Gaussian processes using
essentially the techniques used in graphical Gaussian models by exploiting the relationship
between the Cholesky decomposition of a positive deﬁnite matrix and conditional indepen-
dence. For Gaussian processes in particular, recent developments on the Nearest Neighbor
Gaussian Processes (NNGP) (Datta et al., 2016a,b; Banerjee, 2017; Finley et al., 2019)
have proceeded from GP likelihoods using directed acyclic graphs (or DAGs) as used by

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

6

Vecchia Vecchia (1988) and Stein et al.Stein et al. (2004). The NNGP is a Gaussian pro-
cess whose ﬁnite-dimensional realizations will have sparse precision matrices. Other related
papers using the approximation in Vecchia (1988) include Stroud et al. (2017), Guinness
(2018), Katzfuss and Guinness (2017), and Katzfuss et al. (2018). Shi et al. Shi et al. (2017)
recently used the NNGP for uncertainty quantiﬁcation and Ma et al. Ma and Kang (2017)
used it as a part of a rich class of fused Gaussian process models.

Full Bayesian inference for low-rank and sparse Gaussian process models require iter-
ative algorithms such as Markov chain Monte Carlo (MCMC) or INLA. Details of these
implementations can be found in the aforementioned references. In the following section, we
will discuss how these spatial models can be embedded within a Bayesian linear regression
framework and provide some practical strategies for inference based upon direct (exact)
sampling from the posterior distribution.

3. Conjugate Bayesian models for massive datasets

3.1. Conjugate Bayesian linear geostatistical models

A conjugate Bayesian linear regression model is written as

y | β, σ2 ∼ N (Xβ, σ2Vy) ;

β | σ2 ∼ N (β | µβ, σ2Vβ) ;

σ2 ∼ IG(aσ, bσ) ,

(7)

where y is an n × 1 vector of observations of the dependent variable, X is an n × p matrix
(assumed to be of rank p) of independent variables (covariates or predictors) and its ﬁrst
column is usually taken to be the intercept, Vy is a ﬁxed (i.e., known) n×n positive deﬁnite
matrix, µβ, Vβ, aσ and bσ are assumed to be ﬁxed hyper-parameters specifying the prior
distributions on the regression slopes β and the scale σ2. This model is easily tractable and
the posterior distribution is

p(β, σ2 | y) = IG(σ2 | a∗

σ, b∗
σ)
(cid:125)

× N (β | M m, σ2M )
(cid:125)
(cid:123)(cid:122)
p(β | σ2,y)

(cid:124)

,

(8)

(cid:123)(cid:122)
p(σ2 | y)

(cid:124)

(cid:110)

(cid:111)

β V −1
µ(cid:62)

β µβ + y(cid:62)V −1

σ = bσ + (1/2)
β µβ + X (cid:62)V −1

σ = aσ + n/2, b∗
y X and m = V −1

where a∗
β +
X (cid:62)V −1
y y. Sampling from the joint posterior distribution of
{β, σ2} is achieved by ﬁrst sampling σ2 ∼ IG(a∗
σ) and then sampling β ∼ N (M m, σ2M )
for each sampled σ2. This yields marginal posterior samples from p(β | y), which is a non-
central multivariate t distribution but we do not need to work with its complicated density
function. See Gelman et al. (2013) for further details on the conjugate Bayesian linear
regression model and sampling from its posterior.

y y − m(cid:62)M m

, M −1 = V −1

σ, b∗

We will adapt (7) to accommodate (3) or (4). Let us ﬁrst consider (4) with the customary
speciﬁcation Dτ = τ 2I and let Kθ = σ2R(φ), where R(φ) is a correlation matrix whose
entries are given by a correlation function ρ(φ; (cid:96)i, (cid:96)j). Thus, θ = {σ2, φ}, where σ2 is

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

7

the spatial variance component and φ is a spatial decay parameter controlling the rate at
which the spatial correlation decays with separation between points. A simple example is
ρ(φ; (cid:96)i, (cid:96)j) = exp(−φ(cid:107)(cid:96)i − (cid:96)j(cid:107)), although much richer choices are available (see, e.g., Ch 3
in Banerjee et al., 2014). Therefore, we can write Kθ = σ2Vy, where Vy = R(φ) + δ2I and
δ2 = τ 2/σ2 is the ratio between the “noise” variance and “spatial” variance. If we assume
that φ and δ2 are ﬁxed and that the prior on {β, σ2} are as in (7), then we have reduced
(4) to (7) and direct sampling from its posterior is easily achieved as described below (8).
We will return to the issue of ﬁxing {φ, δ2} shortly.

Let us turn to accommodating (3) within (7), which would include directly sampling the
spatial random eﬀects w from their marginal posterior p(w | y). Here, it is instructive to
write the joint distribution of y and w in (3) as a linear model,











y
µβ
0
(cid:124) (cid:123)(cid:122) (cid:125)
y∗






(cid:124)

X In
Ip O
O In
(cid:123)(cid:122)
X∗






(cid:125)

=

=

(cid:35)

(cid:34)

β
w
(cid:124) (cid:123)(cid:122) (cid:125)
γ

+

+






,





η1
η2
η3
(cid:124) (cid:123)(cid:122) (cid:125)
η

,

(9)

where η ∼ N (0, σ2Vy∗) and Vy∗ =


δ2In O

O Vβ

O

O R(φ)

O
O


. If we assume that δ2 and φ are ﬁxed


at known values, then Vy∗ is ﬁxed. We have a conjugate Bayesian linear regression model
y∗ = X∗γ + η, where γ has a ﬂat prior and σ2 ∼ IG(aσ, bσ). Thus,

p(γ, σ2 | y) = IG(σ2 | a∗

σ, b∗
σ)
(cid:125)

× N (γ | M∗m∗, σ2M∗)
(cid:125)
(cid:123)(cid:122)
p(γ | σ2,y)

(cid:124)

,

(10)

(cid:123)(cid:122)
p(σ2 | y)

(cid:124)

where a∗

σ = aσ + n/2, b∗
∗ V −1

y∗ y∗ − m(cid:62)
y∗ y∗. The posterior mean of γ is ˆγ = M∗m∗ =

σ = bσ + (1/2)

∗ V −1
y(cid:62)

∗ M∗m∗
X (cid:62)

m∗ = X (cid:62)
y∗ y∗, which
is the generalized least squares estimate obtained from the augmented linear system in (9).
Sampling from the posterior proceeds analogous to that described below (8).

∗ V −1

∗ V −1

y∗ X∗

X (cid:62)

∗ = X (cid:62)
(cid:17)−1

∗ V −1

y∗ X∗ and

, M −1

(cid:16)

(cid:110)

(cid:111)

From the preceding account we see that ﬁxing the spatial range decay parameter φ and
the noise-to-spatial variance ratio δ2 casts the Bayesian geostatistical model into a conju-
gate framework that will allow inference on {β, w, σ2}. Note that multiplying the posterior
samples of σ2 by the ﬁxed quantity δ2 fetches us the posterior samples of τ 2. Therefore,
we neglect uncertainty in φ and, partially, for one of the variance components due to ﬁx-
ing their ratio. This, however, provides the computational advantage that inference can be
carried out without resorting to expensive iterative algorithms such as MCMC that require
several iterations before sampling from the posterior distribution. This computational ben-
eﬁt becomes especially relevant when handling massive spatial data. Furthermore, ﬁxing
the values of δ2 and φ is not entirely unreasonable given that these parameters are weakly

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

8

identiﬁed by the data (Zhang, 2004) and diﬃcult to learn from the posterior. Nevertheless,
the inference will depend upon these ﬁxed parameters so we discuss a practical approach
to ﬁx φ and δ2 at reasonable values.

3.2. Choosing φ and δ2

We can set values for φ and δ2 by conducting some simple spatial exploratory data anal-
ysis using the “variogram”. Several practical algorithms exist for empirically calculating
the variogram (or semivariogram) from observations using ﬁnite sample moments. Many
of these methods for variograms are now oﬀered in user-friendly R packages hosted by
the Comprehensive R Archive Network (CRAN) (https://cran.r-project.org). As one
example, Finley et al. Finley et al. (2019) investigate the impact of tree cover and occur-
rence of forest ﬁres on forest height. They ﬁrst ﬁt an ordinary linear regression of the form
yF H = β0 + β1xtree + β2xﬁre + (cid:15) and then compute a variogram for the residuals from the
ordinary linear regression.

Fig 1: Variogram of the residuals from non-spatial regression indicates strong spatial pat-
tern

Figure 1 depicts the variogram, which informs about the process parameters. The lower
horizontal line represents the “nugget” or the micro-scale variation captured by the mea-
surement error variance component τ 2. The top horizontal line represents the “sill” (or
ceiling) which is the total variation captured by σ2 + τ 2. Therefore, the diﬀerence between
the two horizontal lines is called the “partial sill” and is captured by σ2. Finally, the vertical
line represents the distance beyond which the variogram ﬂattens or the covariance tends
to zero. One can provide “eye-ball” estimates for these quantities and, in particular, ﬁx
the values of φ and δ2 = τ 2/σ2. Fixing these values from the variogram yields the desired
highly accessible conjugate framework and the models can be estimated without resorting
to Markov chain Monte Carlo (MCMC) as described earlier. Note that instead of {φ, δ2},
we could also have ﬁxed φ and any one of the variance components, σ2 or τ 2, which would
also yield a conjugate model with exact distribution theory. The one slight advantage of
ﬁxing δ2 is that we will get the posterior samples of both σ2 and τ 2, the latter obtained

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

9

simply as σ2δ2.

The above crude estimates can be improved using a K-fold cross-validation. We split
the data randomly into K diﬀerent folds. Let S[k] be the k-th folder of observed points
and let S[−k] denote the observed points outside of S[k]. For each k, we compute the pre-
dictive mean E[y(S[k]) | y(S[−k])]. We then compute the “Root Mean Square Predictive
Error” (RMSPE) (Yeniay and Goktas, 2002) and choose the value of {φ, δ2} correspond-
ing to the smallest RMSPE from a grid of candidate values. The range of the grid is
based on interpretation of the hyper-parameters. We suggest a reasonably wide range for
δ2 (e.g., [0.001, 1000]), which accommodates one variance component substantially dom-
inating the other in either direction. For the spatial decay φ we suggest a lower bound
of
maximum inter-site distance , which, based upon the exponential covariance function,
indicates that the spatial correlation drops below 0.05 at the maximum inter-site distance,
and an upper bound that can be initially set as 100 times of the lower bound. Functions
like variofit in the R package geoR (Ribeiro Jr and Diggle, 2012) can provide empirical
estimates for {φ, δ2} from an empirical variogram. After initial ﬁtting, we can shrink the
range and reﬁne the grid of the candidate values for more precise estimators.

3

3.3. Conjugate Bayesian geostatistical models for massive spatial data

Conjugate models can be estimated by sampling directly from their joint posterior density
and, therefore, completely obviate problems associated with MCMC convergence. This is
a major computational beneﬁt. However, the challenges in analyzing massive spatial data
do not quite end here. When the number of spatial locations providing measurements are
in the order of millions as in Finley et al. (2019), then the matrices Kθ, Vy or Vy∗ that
we encountered earlier in diﬀerent model parametrizations will be too massive to be eﬃ-
ciently loaded on to the machine’s memory, let alone be computed with. This precludes
eﬃcient likelihood computations and has led several researchers to propose models speciﬁ-
cally adapted for spatial analysis. We brieﬂy present adaptations of (9) using two diﬀerent
classes of models for massive spatial data: (i) low-rank process models and (ii) NNGP
models.

As discussed in Section 2, in low rank models the n × 1 spatial eﬀect w in (3) is replaced
by Bθz, where Bθ is the n × r matrix whose i-th row is b(cid:62)
θ ((cid:96)i). Dimension reduction is
achieved by ﬁxing r to be much smaller than n so that we only deal with r random eﬀects
instead of n. The framework in (9) can be easily adapted to this situation as below:











y
µβ
0
(cid:124) (cid:123)(cid:122) (cid:125)
y∗






(cid:124)

X Bθ
Ip O
O Ir
(cid:123)(cid:122)
X∗






(cid:125)

=

=

(cid:35)

(cid:34)

β
z
(cid:124) (cid:123)(cid:122) (cid:125)
γ

+

+






,





η1
η2
η3
(cid:124) (cid:123)(cid:122) (cid:125)
η

,

(11)

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

10



where η ∼ N (0, σ2Vy∗) and Vy∗ =


δ2In O O

O Vβ O

O Vz
O
and Vz is now r × r instead of the n × n matrix R(φ) in (9). Computations for (11) proceed
analogous to those for (8), but beneﬁts accrue in terms of storage and the number of
ﬂoating point operations (ﬂops) when conducting the exact conjugate Bayesian analysis
for this model.


 is (n + p + r) × (n + p + r) and ﬁxed,

We now outline the construction of sparse NNGP models. These can be regarded as a
special case of Gaussian Markov Random Fields (GMRFs) with a neighborhood structure
speciﬁed using a directed acyclic graph (DAG). The computational beneﬁts for NNGP
models accrue from the ease of inverting sparse matrices. This is immediate from noting that
the expense to obtain V −1
in (10) is dominated by R(φ)−1. Therefore, if R(φ)−1 is easily
y∗
available then the inference for γ = {β, w} will be inexpensive. Modeling sparse R(φ)−1 can
be easily achieved as follows. Writing N (w | 0, σ2Rφ) as p(w1) (cid:81)n
i=2 p(wi | w1, w2, . . . , wi−1)
is equivalent to the following set of linear models,

w1 = 0 + η1

and

wi = ai1w1 + ai2w2 + · · · + ai,i−1wi−1 + ηi

for i = 2, . . . , n ,

or, more compactly, simply w = Aw + η, where A is n × n strictly lower-triangular with
elements aij = 0 whenever j ≥ i and η ∼ N (0, D) and D is diagonal with diagonal entries
d11 = var{w1} and dii = var{wi | wj : j < i} for i = 2, . . . , n. From the structure of A it is
evident that I − A is unit lower-triangular, hence nonsingular, and Rφ = (I − A)−1D(I −
A)−(cid:62).

We now introduce sparsity in R−1

φ = (I − A)(cid:62)D(I − A) by letting aij = 0 whenever
j ≥ i (since A is strictly lower-triangular) and also whenever (cid:96)j is not among the m nearest
neighbors of (cid:96)i, where m is ﬁxed by the user to be a small number. It turns out that a very
eﬀective approximation emerges by recognizing that the lower-triangular elements of A
are precisely the coeﬃcients of a linear combination of w((cid:96)j)’s equating to the conditional
expectation E[w((cid:96)i) | {w((cid:96)j) : j < i}]. Thus, the m × 1 vector ˜ai of non-zero entries in
the i-th row of A are obtained by solving the m × m linear system ˜Rφ,Ni,Ni ˜ai = Rφ,Ni,i,
where ˜Rφ,Ni,Ni is the m × m principal submatrix extracted from Rφ corresponding to the
m neighbors of i (indexed by elements of a neighbor set Ni) and Rφ,Ni,i is the m × 1 vector
extracted by choosing the m indices in Ni from the i-th column of Rφ. Once ˜ai is obtained,
the i-th diagonal entry of D is obtained as dii = Rφ[i, i] − ˜a(cid:62)
i Rφ,Ni,i. These computations
need to be carried out for each i = 2, . . . , n (note that for i = 1, d11 = σ2 and a11 = 0),
but m can be kept very small (say 5 or 10 even if n 107) so that the expense is O(nm3)
and still feasible. The details can be found in Banerjee (2017). This notion is familiar in
Gaussian Graphical models and have been used in Vecchia (1988) and, more recently, in
Datta et al. (2016a) and Finley et al. (2019) to tackle massive amounts of spatial locations.

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

11

The framework in (9) now assumes the form











y
µβ
0
(cid:124) (cid:123)(cid:122) (cid:125)
y∗






(cid:124)

In
O

X
Ip
O D−1/2(I − A)
(cid:123)(cid:122)
X∗






(cid:125)

=

=

(cid:35)

(cid:34)

β
w
(cid:124) (cid:123)(cid:122) (cid:125)
γ

+

+






,





η1
η2
η3
(cid:124) (cid:123)(cid:122) (cid:125)
η

,

(12)



where η ∼ N (0, σ2Vy∗) and Vy∗ =


 is (2n+p)×(2n+p) and ﬁxed with much


δ2In O O

O Vβ O

O In
O
greater sparsity. While this approach can also be subsumed into the framework of (9), its
eﬃcient implementation on standard computing architectures needs careful consideration
and involves solving a large linear system with (n + p) × (n + p) coeﬃcient matrix X (cid:62)
∗ X∗.
This matrix is large, but is sparse because of (I − A)(cid:62)D−1(I − A). Since (I − A) has at
most m + 1 nonzero entries in each row, an upper bound of nonzero entries in (I − A) is
n(m+1) and, therefore, the upper bound in (I −A)(cid:62)D−1(I −A) is n(m+1)2. This sparsity
can be exploited by sparse linear solvers such as conjugate gradient methods that can be
implemented on modest computing environments.

Sampling from the joint posterior distribution p(γ, σ2 | y∗) is achieved in the following
manner. First, the least-squares estimate ˆγ is obtained using a sparse least-square solver
using a preconditioned conjugate gradient algorithm. Subsequently, σ2 is sampled from
its marginal posterior density IG(a∗, b∗), where a∗ = aσ + n/2 and b∗ = bσ + (1/2)(y∗ −
X∗ˆγ)(cid:62)(y∗ − X∗ˆγ), and we sample one value of γ from N
y∗ X∗
using each
∗ X∗ˆγ = X (cid:62)
sampled value of σ2. In general, solving X (cid:62)
3 (n + p)3) ﬂops, but
when p (cid:28) n, the structure of X∗ and X (cid:62)
∗ X∗ ensures memory requirements in the order of
n(m + 1)2 and the computational complexity in the order of nm + n(m + 1)2 ﬂops. Details
on such implementations on modest computing platforms can be found in (Zhang et al.,
2019).

ˆγ, σ2 (cid:16)
∗ V −1
∗ y∗ requires O( 1

(cid:17)−1(cid:19)

X (cid:62)

(cid:18)

3.4. Spatial prediction

Let ˜L = { ˜(cid:96)1, ˜(cid:96)2, . . . , ˜(cid:96)˜n} be a set of ˜n locations where we wish to predict the outcome y((cid:96)).
Let ˜Y be an ˜n×1 vector with i-th element ˜Y (˜(cid:96)i) and let ˜w be the ˜n×1 vector with elements
w(˜(cid:96)i). The predictive model augments the joint distribution p(θ, w, β, τ, y) to

p(θ, τ, β, w, y, ˜w, ˜Y ) = p(θ, τ, β) × p(w | θ) × p( ˜w | w, θ) × p(y | β, w, τ ) × p( ˜Y | β, ˜w, τ ) .

(13)

The factorization in (13) also implies that ˜Y and w are conditionally independent of each
other given ˜w and β. Predictive inference for spatial data evaluates the posterior predictive

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

12

distribution p( ˜Y , ˜w | y). This is the joint posterior distribution for the outcomes and the
spatial eﬀects at locations in ˜L. This distribution is easily derived from (13) as

p( ˜Y , ˜w, β, w, θ, τ | y) ∝ p(β, w, θ, τ | y) × p( ˜w | w, θ) × p( ˜Y | β, ˜w, τ ) .

(14)

Sampling from (14) is achieved by ﬁrst sampling {β, w, θ, τ } from p(β, w, θ, τ | y). For each
drawn sample, we make one draw of the ˜n × 1 vector ˜w from p( ˜w | w, θ) and then, using
this sampled ˜w, we make one draw of ˜Y from p( ˜Y | β, ˜w, τ ). The resulting samples of ˜w and
˜Y will be draws from the desired posterior predictive distribution p( ˜w, ˜Y | y). This delivers
inference on both the latent spatial random eﬀect ˜w and the outcome ˜Y at arbitrary
locations since L can be any ﬁnite collection of samples. Summarizing these distributions
by computing their sample means, standard errors, and the 2.5-th and 97.5-th quantiles
(to produce a 95% credible interval) yields point estimates with associated uncertainty
quantiﬁcation.

It is instructive to see how the entire inference for Gaussian outcomes can be cast into
an augmented linear regression model. The predictive model for ˜Y can be written as a
spatial regression

˜Y = ˜Xβ + ˜w + ˜(cid:15) ;

˜w = Cw + ω ,

(15)

where ˜X is the ˜n × p matrix of predictors observed at locations in ˜L and ˜(cid:15) ∼ N (0, ˜Dτ ),
where ˜(cid:15) is the ˜n × 1 vector with elements (cid:15)(˜(cid:96)i). The second equation in (15) expresses the
relationship between the spatial eﬀects ˜w across the unobserved locations in ˜L and the
spatial eﬀects across the observed locations in L. Since there is one underlying random
ﬁeld over the entire domain, the covariance function for the random ﬁeld speciﬁes the
˜n × n coeﬃcient matrix C. In particular, if w ∼ N (0, Kθ), then C = Kθ( ˜L, L)K−1
and
ω ∼ N (0, Fθ), where Fθ = Kθ( ˜L, ˜L) − Kθ( ˜L, L)K−1
θ Kθ(L, ˜L). The model for the data and
the predictions is combined into

θ



















y
µβ
0
0
0
(cid:124) (cid:123)(cid:122) (cid:125)
y∗








(cid:124)

O
X In O
Ip O O
O
O C −I˜n O
˜X O I˜n −I˜n
(cid:123)(cid:122)
X∗








(cid:125)

=

=















β
w
˜w
˜Y
(cid:124) (cid:123)(cid:122) (cid:125)
γ








,











η1
η2
η3
η4
η5
(cid:124) (cid:123)(cid:122) (cid:125)
η

+

+

,

(16)

where η ∼ N



0,

















Dτ O O O O
O Vβ O O O
O O Kθ O O
O O O Fθ O
O O O O ˜Dτ



















. If locations where predictions are sought are

ﬁxed by study design, then ﬁtting (16) using the Bayesian conjugate framework can be

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

13

beneﬁcial. On the other hand, one can ﬁrst estimate {β, w, σ2} and store samples from
their posterior distribution. Then, for any arbitrary set of points in ˜L, for each stored
sample of the parameters we draw one sample of ˜w ∼ N (Cw, Fθ) followed by one draw of
˜Y ∼ N ( ˜Xβ + ˜w, ˜Dτ ). The resulting { ˜w, ˜Y } will be the desired posterior predictive samples
for the latent spatial process and the unobserved outcomes. Again, the advantage of this
formulation is that an eﬃcient least squares algorithm to solve (16) that can exploit the
sparsity of the design matrix X∗ will immediately deliver inference on the regression slopes
(β), the spatial process (w) at observed points, the interpolated process ( ˜w) at unobserved
points, and the predicted response ( ˜Y ) all at once.

4. Illustrative examples

We present a part of some simulation experiments conducted in (Zhang et al., 2019), where
we generated data using the spatial regression model in (2) over a set of n = 1200 spatial
locations within a unit square and using an exponential covariance function to specify the
spatial process. While 1200 spatial locations may seem too modest, we use this to draw
comparisons with a full GP model that will be too expensive for large datasets. The model
included an intercept and a single predictor generated from a standard normal distribution.
We ﬁt a full Gaussian process based model (labeled as full GP in Table 1) using the
spBayes package in R, a latent NNGP model with m = 10 neighbors using the sequential
MCMC algorithm described in Datta et al. (2016a) (using the spNNGP package), and the
conjugate latent NNGP model described in the preceding section with m = 10 neighbors.
We will refer to the latent NNGP model ﬁtted using MCMC (with all process parame-
ters unknown) as simply the NNGP or latent NNGP model, while we will explicitly use
“conjugate” to describe the conjugate latent NNGP model.

These models were trained using n = 1000 observations, while the remaining 200 obser-
vations were withheld to assess predictive performance. The ﬁxed parameters{φ, δ2} for the
conjugate latent NNGP model were picked through the K-fold cross-validation algorithm
described in Section 3.2. The intercept and slope parameters in β were assigned improper
ﬂat priors and an IG(2, b) (mean b) prior was used for σ2. For the latent NNGP and full
GP models, the spatial decay φ was modeled using a fairly wide uniform prior U (2.2, 220)
prior and Inverse-Gamma priors IG(2, b) (mean b) were used for the nugget (τ 2) and the
partial sill (σ2) in order to compare the conjugate Bayesian models with other models.
The shape parameter was ﬁxed at 2 and the scale parameter was set from the empirical
estimate provided by the variogram using the geoR package (Ribeiro Jr and Diggle, 2012).
The parameter estimates and performance metrics are provided in Table 1. Table 1 presents
parameter estimates and performance metrics for the candidate models. The inference for
β is almost indistinguishable across the three models. The full GP and the NNGP fully
estimate {σ2, τ 2, φ} using MCMC and yield very similar results. The conjugate NNGP does
not estimate φ and estimates {σ2, τ 2} subject to the constraint that their ratio δ2 is ﬁxed.
This results, expectedly, in slightly narrower credible intervals for σ2 and τ 2. Overall, the

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

14

Table 1
Simulation study summary table: posterior mean (2.5%, 97.5%) percentiles

β0
β1
σ2
τ 2
φ
KL-D
MSE(w)
RMSPE
time(s)

True
1
-5
2
0.2
16
–
–
–
–

Full GP
1.07(0.72, 1.42)
-4.97 (-5.02, -4.91)
1.94 (1.63, 2.42)
0.14 (0.07, 0.23)
19.00 (13.92, 23.66)
4.45(1.16, 9.95)
297.45(231.62, 444.79 )
0.94
2499 + 23147

NNGP
1.10 (0.74, 1.43)
-4.97 (-5.02, -4.91)
1.95 (1.63, 2.41)
0.15 (0.06, 0.24)
18.53 (14.12, 24.17)
5.13(1.66, 11.39)
303.38(228.18, 429.54)
0.94
109.5

Conj NNGP
1.06 (0.76, 1.46)
-4.97 (-5.02, -4.91)
1.94 (1.77, 2.12)
0.17 (0.16, 0.19)
17.65
3.58(1.27, 8.56)
313.28 (258.96, 483.75)
0.94
12 + 0.6

parameter estimates are very comparable across the models.

Turning to model comparisons, Zhang et al.Zhang et al. (2019) computed the poste-
rior distribution of the Kullback-Leibler divergence (KL-D) by computing it between each
candidate model and the full GP for each posterior sample. The KL-D values presented
in Table 1 show no signiﬁcant diﬀerences between the three models in their separation
from the true full GP model. The root mean-squared prediction error (RMSPE) values
(computed from the hold-out set of 200 locations) across all three models are also similar,
further corroborating the comparable predictive performance of the conjugate model with
the full Gaussian process.

In terms of timing (presented in seconds in Table 1), the recorded time of the conjugate
models includes the time for choosing hyper-parameters through cross-validation and (“+”)
the time for sampling from the posterior distribution. The recorded time of the full GP
model consists of the time for MCMC sampling and (“+”) the time for recovering the
regression coeﬃcients and predictions. The full latent NNGP model is 200 times faster
than the full Gaussian process based model, while the conjugate latent NNGP model uses
one tenth of the time required by the latent NNGP model to obtain similar inference on
the regression coeﬃcients and latent process. Further simulation experiments conducted
by Zhang et al. (Zhang et al., 2019) also show that interpolation of the latent process is
almost indistinguishable between the conjugate and full models.

Next, we present a second simulation example using exactly the same setup as in the pre-
ceding example, but with n = 12, 000 spatial locations. Here, we ﬁt a latent NNGP model
using the MCMC algorithm in Datta et al. (2016a) and the conjugate latent NNGP model.
We used 10, 000 locations for training the models while the remaining 2000 locations were
used for predictive assessment. We summarized the results from the latent NNGP model
using a post burn-in posterior sample for 10, 000 iterations. This was deemed adequate
based upon the customary convergence diagnostics available in the coda and mcse pack-
ages within the R computing environment (Plummer et al., 2006; Flegal and Jones, 2011).
The inference from the conjugate latent NNGP model were based on 300 samples. This is
suﬃcient for the conjugate latent NNGP model since the conjugate model provides inde-
pendent samples from the exact posterior distribution. The full MCMC-based NNGP model

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

15

(a) True

(b) NNGP

(c) Conjugate NNGP

(d) CIs of w from NNGP (e) CIs of w from conjugate

NNGP

Fig 2: Interpolated maps of (a) the true generated surface, (b) the posterior means of the
spatial latent process w(s) for the NNGP and (c) posterior means of w(s) for the conjugate
latent NNGP. The 95% conﬁdence intervals for the spatial eﬀects w from (d) the NNGP and
(e) the conjugate NNGP. The NNGP models were all ﬁt using m = 10 nearest neighbors.

took about 1268 seconds to deliver full Bayesian inference, while the conjugate model took
only 99+14 = 113 seconds (99 seconds for the cross-validation to ﬁx {φ, δ2} and 13 seconds
for sampling from the posterior distribution). We found that the RMSPE values for the full
latent NNGP and the conjugate model computed using the 2000 hold-out locations were
almost identical (0.67 up to 2 decimal places).

The parameter estimates from the full NNGP and conjugate NNGP models in this larger
simulation experiment reveal essentially the same story as in Table 1 so we do not repeat
them here. Instead, we focus on the estimation of the latent process and the predictive
performance for the two models. Figure 2 shows interpolated surfaces from the simulation
example: 2(a) shows an interpolated map of the “true” spatial latent process w, while
2(b) and 2(c) present the posterior means of w(s) over the entire domain obtained from
the full latent NNGP model and the conjugate latent NNGP model, respectively. The
recovered spatial residual surfaces are almost indistinguishable, and are comparable to the
true interpolated surface of w(s). Figure 2(d)–(e) present the 95% credible intervals for

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

16

(a) Posterior mean of sea-surface temperature (b) Posterior predictive mean of latent spatial ef-

fects

Fig 3: Posterior predictive maps of sea-surface temperature (in degree centigrade) and latent
spatial eﬀects. The land is colored in gray, locations in the ocean without observations are
indicated in yellow.

the spatial eﬀects w from the latent NNGP model and the conjugate latent NNGP model.
These intervals are plotted against the true values of w from the generated model. We
found that 9567 out of 10000 credible intervals successfully included the true value for the
conjugate model, while the corresponding number was a very comparable 9584 for the full
NNGP model.

Turning to a real example, we present a synopsis of the analysis in Zhang et al. (2019) of
a spatial dataset from NASA comprising sea surface temperature (in degrees Centigrade)
observations over 2,827,252 spatial locations of which approximately 90% (2,544,527) were
used for model ﬁtting and the rest were withheld for cross-validatory predictive assessment.
Details of the dataset can be found in http://modis-atmos.gsfc.nasa.gov/index.html
and details on the analysis can be found in Zhang et al. (2019). The salient feature of the
analysis is that a conjugate Bayesian framework for the NNGP model as in (12) was able
to deliver full inference including the estimation of the spatial latent eﬀects in about 2387
seconds. Sampling from the posterior distribution was achieved using direct sampling as
described below (12). Since this algorithm is fast and directly samples from the posterior,
hence there is no burn-in period for convergence, it was run over a grid of values of {δ2, φ}.
For each such value, a posterior predictive assessment over the cross-validatory hold-out
set was carried out and the value of {δ2, φ} producing the least RMSPE was selected as
optimal inputs for which the estimates of {γ, σ2} were presented.

5. Spatial Meta-Kriging

A diﬀerent approach toward BIG DATA problems relies upon divide and conquer methods.
The idea here is divide and conquer (or map and reduce) by pooling posterior inference

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

17

across a partition of data subsets. Once again consider the Bayesian linear regression model

p(β, σ2 | y) ∝ IG(σ2 | aσ, bσ) × N (β | µβ, σ2Vβ) × N (y | Xβ, σ2Vy) ,

(17)

where y is N × 1, X is N × p, β is p × 1, Vy is a ﬁxed N × N covariance matrix,
µβ is a ﬁxed p × 1 vector and Vβ is a ﬁxed p × p matrix. The joint posterior density
p(β, σ2 | y) is available in closed form as p(β, σ2 | y) = p(σ2 | y) × p(β | σ2, y) , where the
marginal posterior density p(σ2 | y) = IG(σ2 | a∗, b∗) and the conditional posterior density
p(β | σ2, y) = N (β | M m, σ2M ) with a∗ = aσ + N/2, b∗ = bσ + c/2, m = V −1
y y,
M −1 = V −1
β V −1
y y −m(cid:62)M m. Therefore, exact posterior
β µβ +y(cid:62)V −1
inference can be carried out by ﬁrst sampling σ2 from IG(a∗, b∗) and then sampling β from
N (M m, σ2M ) for each sampled value of σ2. This results in samples from p(β, σ2 | y). Be-
sides the ﬁxed hyperparameters in the prior distributions, this exercise requires computing
m, M and c.

y X and c = µ(cid:62)

β µβ + X (cid:62)V −1

β +X (cid:62)V −1

Now consider a situation where N is large enough so that memory requirements for
computing (17) is unfeasible. One possible resolution is to replace the likelihood in (17)
with a composite likelihood that assumes independence across blocks formed by partitioning
the data into K subsets. We partition the N × 1 vector y into K subvectors with yk as
the nk × 1 subvector forming the k-th subvector, where (cid:80)K
k=1 nk = N . The size of the
k-th subset is nk. These sizes need not be the same across k, but will be chosen in a
manner so that each of the subsets can be ﬁtted easily with the computational resources
available. Also, let Xk be the nk × p matrix of predictors corresponding to yk and let Vyk
be the marginal correlation matrix for yk. The conjugate Bayesian model with a block-
ind∼ N (0, σ2Vyk ).
independent composite likelihood assumes that yk = Xkβ + (cid:15)k, where (cid:15)k
The Bayesian speciﬁcation is completed by assigning priors to σ2 and β as in (17). If
we distribute the analysis to K diﬀerent computing cores, where the k-th core ﬁts the
above model but only with the likelihood N (yk | Xkβ, σ2Vyk ), then the quantities needed
for sampling from the full p(β, σ2 | y) can be computed entirely using quantities obtained
from the individual subsets of the data. For each k = 1, 2, . . . , K we independently compute
mk = V −1
β + X (cid:62)
Xk based upon the k-th subset of the
data. We then combine them to obtain m = (cid:80)K
β µβ) and M −1 =
(cid:80)K
β ). Subsequently, we compute c = µ(cid:62)
yk −
m(cid:62)M m. Therefore, sampling from the posterior distribution of β and σ2 given the entire
dataset can be achieved using quantities computed independently from each of the K
smaller subsets of the data. There is no need to interact between the subsets and one
does not require to store or compute with large objects based upon the entire dataset.
This computation can also be done sequentially. We ﬁrst obtain the posterior distribution
p(β, σ2 | y1) based only upon the ﬁrst data set. This posterior becomes the prior for the
next step and we obtain p(β, σ2 | y1, y2) ∝ p(β, σ2 | y1) × p(y2 | β, σ2) and so on until we
arrive at p(β, σ2 | y1, y2, . . . , yK) ∝ p(β, σ2 | y1, y2, . . . , yK−1) × p(yK | β, σ2).

k V −1
yk
k=1(mk − (1 − 1/K)V −1

k − (1 − 1/K)V −1

β µβ + (cid:80)K

β µβ + X (cid:62)

yk and M −1

k = V −1

k=1(M −1

k=1 y(cid:62)

k V −1
yk

k V −1
yk

β V −1

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

18

Clearly such exact recovery of the full posterior crucially depends on the conditional
independence across the diﬀerent data blocks (e.g., p(yk | β, σ2, y1, . . . , yk−1) = p(yk | β, σ2)
for each k = 2, . . . , K). While this works for uncorrelated outcomes, as in standard linear
regression, such recovery is precluded for spatial and spatiotemporal process models and,
more generally, for correlated data. Nevertheless, we can develop a general approximation
framework for obtaining the full posterior from posterior densities calculated over smaller
subsets. One general way to pool information across these individual posteriors is to use the
unique Geometric Median (GM) of the subset posteriors, as developed by Minsker (Minsker,
2015). Assume that the individual posterior densities pk ≡ p(Ω | yk) reside on a Banach

K
(cid:88)

(cid:107)pk − π(cid:107)ρ,

k=1

space H equipped with norm (cid:107) · (cid:107). The GM is deﬁned as π∗(· | y) = arg min
π∈H

(cid:90)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1 , y(cid:62)

where y = (y(cid:62)

(cid:13)
(cid:13)
ρ(Ω, ·)d(π1 − π2)(Ω)
(cid:13)
(cid:13)

K)(cid:62). The norm quantiﬁes the distance between any two posterior
, where ρ(·) is a positive-

2 , . . . , y(cid:62)
densities π1(·) and π2(·) as (cid:107)π1 − π2(cid:107)ρ =
deﬁnite kernel function. Assume ρ(z1, z2) = exp(−(cid:107)z1 − z2(cid:107)2). The GM is unique and
lies in the convex hull of the individual posteriors, so π∗(Ω | y) is a legitimate probability
k=1 αρ,k(y)pk,(cid:80)K
density. Speciﬁcally, π∗(Ω | y) = (cid:80)K
k=1 αρ,k(y) = 1, each αρ,k(y) being a
function of ρ, y, so that (cid:82)
Ω π∗(Ω | y)dΩ = 1. Computing the GM π∗ ≡ π∗(Ω | y) is achieved
by an iterative algorithm that estimates αρ,k(y) from the subset posteriors pk for each
k = 1, 2, . . . , K. To further elucidate, we use a well known result that the GM π∗ satisﬁes
π∗ =

, so that αρ,k(y) = (cid:107)pk−π∗(cid:107)−1

. There is no apparent closed-form

(cid:80)K
k=1
(cid:80)K

k=1

(cid:107)pk−π∗(cid:107)−1
ρ pk
(cid:107)pk−π∗(cid:107)−1

ρ

(cid:80)K

j=1

ρ
(cid:107)pk−π∗(cid:107)−1

ρ

solution for αρ,k(y) satisfying this equation, so Weiszfeld’s algorithm (Minsker, 2015) is
used to estimate these functions.

This approach has been extended to spatial process settings by Guhaniyogi and Banerjee
(Guhaniyogi and Banerjee, 2018, 2019). The advantage here is that one can use existing
Bayesian geostatistical software to sample from the posterior distributions of the diﬀerent
subsets. This can be performed either in parallel over multiple cores or across diﬀerent
machines altogether. One then needs to save only the post burn-in samples and execute
Weiszfeld’s algorithm to these samples. Weiszfeld’s algorithm is extremely fast and easy to
program.

6. Discussion

This article has attempted to provide a brief overview of how some Bayesian geostatistical
models designed for large spatial and/or spatiotemporal datasets can be further scaled
up to analyze massive datasets with observed locations in the order of 106 or more by
exploiting the familiar theory of conjugate Bayesian linear regression models and adapting
them to incorporate latent spatial processes. The resulting distribution theory is available
in closed form, thereby circumventing the need for iterative algorithms such as MCMC or
INLA. We have also provided a brief overview of a distributed approach (spatial meta-

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

19

kriging) that relies upon analyzing exclusive subsets of the data and combining them to
approximate the full posterior in the spirit of a spatial meta-analysis.

Of course, this requires some compromise in terms of full Bayesian inference. Some
parameters need to be provided as ﬁxed inputs for the distribution theory to be available
in closed form. Learning about these input parameters will be done using exploratory data
analysis and cross-validation methods. A practical approach that seems to be quite eﬀective
for analyzing massive datasets in modest computing environments is to choose the optimal
value of the process parameters based upon the minimum RMSPE over hold-out locations.
While such approaches may produce slightly shrunk credible and prediction intervals due
to the eﬀect of ﬁxing a parameter, the eﬀect is seen to be moderate in practical spatial
analysis and the approach could form a useful tool for quick spatial analysis within the
Bayesian paradigm for massive spatial datasets. However, the method of learning about
these parameters is still ad-hoc and can possibly be improved with more sophisticated
optimization methods. Nevertheless, the approach outlined here can be a useful tool in the
spatial analyst’s toolbox for exploring Bayesian spatial regression at massive scales. We also
point out that the conjugate Bayesian linear regression framework can accommodate almost
all of the model-based GP approximations for dimension reduction or sparsity induction.
Any spatial covariance structure that leads to eﬃcient computations can, in principle, be
used.

While the article has focused on the NNGP as a choice for introducing sparsity in the
model, more general GMRF speciﬁcations are also admissible here. In fact, there has been
much recent activity within the framework of Vecchia approximations (see, e.g., Katzfuss
and Guinness, 2017; Katzfuss et al., 2018), where the models are being derived using
DAGs over the expanded set of observations and process realizations. While certainly
promising, their beneﬁts and improvements over GMRFs are yet to be demonstrated in
large scale case studies. For Vecchia type of likelihoods, there is also interest in choosing
the number of neighbors. First, it should be intuitively clear that DAGs constructed using
shrunk neighbor sets will yield probability models farther away from the full model as
the neighbor sets get smaller. To see this, consider a random vector w = (w(cid:62)
B)(cid:62),
where A and B are mutually exclusive sets containing indices for the elements of w, and let
p(w) = p(wA)p(wB | wA) denote the joint probability density for w. Consider two submodels
p1(w) = p(wA) × p(wB | wN1B ) and p2(w) = p(wA) × p(wB | wN2B ), where N2B ⊂ N1B ⊂ A.
The model p2 will be farther than p1 from p in the terms of the Kullback-Leibler divergence:

A, w(cid:62)

KL(p(cid:107)(cid:107)p2) − KL(p(cid:107)(cid:107)p1) =

(cid:90) (cid:26)

log

(cid:90)

(cid:90)

log

log

(cid:90) (cid:26)(cid:90)

=

=

=

(cid:18) p(w)
p2(w)
(cid:19)

(cid:18) p1(w)
p2(w)

(cid:19)

− log

(cid:18) p(w)
p1(w)
(cid:90)

p(w)dw =

log

(cid:19)(cid:27)

p(w)dw

(cid:18) p(wB | wN1B )
p(wB | wN2B )

(cid:19)

p(w)dw

(cid:18) p(wB | wN1B )
p(wB | wN2B )

(cid:19)

log

(cid:18) p(wB | wN1B )
p(wB | wN2B )

p(wB | wN1B )p(wN1B )dwBdwN1B

(cid:19)

(cid:27)

p(wB | wN1B )dwB

p(wN1B )dwN1B ≥ 0 ,

(18)

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

20

where we have used the fact that A \ N1B is mutually exclusive of N1B and, crucially,
also of N2B (since N2B ⊂ N1B) to legitimately integrate out wA\N1B . The ﬁnal conclusion
follows from a customary application of Jensen’s inequality to show that the inner integral
in the last equation is non-negative. Equation (18) provides an alternate distribution-free
proof of a result for Gaussian likelihoods by Guinness (Theorem 1 in Guinness (2018)).
These results also indicate that the ordering of the variables to construct the approximation
can aﬀect model performance and certain designs to determine the ordering can produce
improved results (as demonstrated in Guinness (2018)). Datta et al. Datta et al. (2016b)
argued against ﬁxing the neighborhoods in spatiotemporal contexts (since neighbors in
space and neighbors in time may not align) and demonstrate a computationally eﬃcient
method to learn about neighbors in spatiotemporal domains.

Finally, we point toward a few future directions of research in this domain. Much of
the spatial literature on modeling massive spatial data have focused upon scalability of
models and algorithms. There is still work to be done on evaluating the inferential perfor-
mance of these models at such massive scales. How important is uncertainty quantiﬁcation
at such scales? How do GP based approaches compare with deep learning with neural
networks in spatial analysis? Another area where the cross-validatory learning approaches
for process hyperparameters will struggle is in multivariate contexts, where the number of
hyperparameters is higher than here. These are some areas of research where we believe
the statistical community still has much to oﬀer.

References

Abdulah, S., Ltaief, H., Sun, Y., Genton, M., and Keyes, D. (2018). “ExaGeoStat: A high
performance uniﬁed software for geostatistics on manycore systems.” IEEE Transactions
on Parallel and Distributed Systems, 29: 2771–2784. 3

Banerjee, S. (2017). “High-Dimensional Bayesian Geostatistics.” Bayesian Analysis, 12:

583–614. 3, 5, 10

Banerjee, S., Carlin, B. P., and Gelfand, A. E. (2014). Hierarchical modeling and analysis

for spatial data. CRC Press, Boca Raton, FL. 1, 2, 4, 5, 7

Banerjee, S., Finley, A. O., Waldmann, P., and Ericcson, T. (2010). “Hierarchical Spatial
Process Models for Multiple Traits in Large Genetic Trials.” Journal of the American
Statistical Association, 105: 506–521. 5

Banerjee, S., Gelfand, A. E., Finley, A. O., and Sang, H. (2008). “Gaussian Predictive
Process Models for Large Spatial Datasets.” Journal of the Royal Statistical Society,
Series B , 70: 825–848. 5

Cressie, N. (1993). Statistics for Spatial Data. Wiley-Interscience, revised edition. 1, 2, 5
Cressie, N. and Johannesson, G. (2008). “Fixed Rank Kriging for Very Large Data Sets.”

Journal of the Royal Statistical society, Series B , 70: 209–226. 4

Cressie, N., Shi, T., and Kang, E. L. (2010). “Fixed Rank Filtering for Spatio-temporal

Data.” Journal of Computational and Graphical Statistics, 19: 724–745. 4

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

21

Cressie, N. A. C. and Wikle, C. K. (2011). Statistics for Spatio-temporal Data. Wiley series

in probability and statistics. Hoboken, N.J. Wiley.
URL http://opac.inria.fr/record=b1133266 1, 2

Datta, A., Banerjee, S., Finley, A. O., and Gelfand, A. E. (2016a). “Hierarchical Nearest-
Neighbor Gaussian Process Models for Large Geostatistical Datasets.” Journal of the
American Statistical Association, 111: 800–812.
URL http://dx.doi.org/10.1080/01621459.2015.1044091 5, 10, 13, 14

Datta, A., Banerjee, S., Finley, A. O., Hamm, N. A. S., and Schaap, M. (2016b). “Non-
separable Dynamic Nearest-Neighbor Gaussian Process Models for Large spatio-temporal
Data With an Application to Particulate Matter Analysis.” Annals of Applied Statistics,
10: 1286–1316.
URL http://dx.doi.org/10.1214/16-AOAS931 5, 20

Du, J., Zhang, H., and Mandrekar, V. S. (2009). “Fixed-domain Asymptotic Properties of

Tapered Maximum Likelihood Estimators.” Annals of Statistics, 37: 3330–3361. 5

Finley, A. O., Banerjee, S., and Gelfand, A. E. (2015). “spBayes for Large Univariate and
Multivariate Point-Referenced Spatio-Temporal Data Models.” Journal of Statistical
Software, 63(13): 1–28.
URL http://www.jstatsoft.org/v63/i13/ 5

Finley, A. O., Datta, A., Cook, B. C., Morton, D. C., Andersen, H. E., and Banerjee,
S. (2019). “Eﬃcient algorithms for Bayesian Nearest Neighbor Gaussian Processes.”
Journal of Computational and Graphical Statistics, 28(2): 401–414. 3, 5, 8, 9, 10

Flegal, J. and Jones, G. (2011). “Implementing Markov chain Monte Carlo: Estimating
with conﬁdence.” In Brooks, S., Gelman, A., Jones, G., and Meng, X. (eds.), Handbook
of Markov Chain Monte Carlo, 175–197. Chapman and Hall/CRC Press, Boca Raton,
FL. 14

Furrer, R., Genton, M. G., and Nychka, D. (2006). “Covariance Tapering for Interpolation
of Large Spatial Datasets.” Journal of Computational and Graphical Statistics, 15: 503–
523. 5

Gelfand, A., Diggle, P., Fuentes, M., and Guttorp, P. (2010). Handbook of Spatial Statistics.

Boca Raton, FL: CRC Press. 1, 2

Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. (2013).
Bayesian Data Analysis, 3rd Edition. Chapman & Hall/CRC Texts in Statistical Science.
Chapman & Hall/CRC. 6

Gneiting, T. and Guttorp, P. (2010). “Continuous-parameter Spatio-temporal Processes.”
In Gelfand, A., Diggle, P., Fuentes, M., and Guttorp, P. (eds.), Handbook of Spatial
Statistics, 427–436. CRC Press, Boca Raton, FL. 2

Guhaniyogi, R. and Banerjee, S. (2018). “Meta-Kriging: Scalable Bayesian Modeling and

Inference for Massive Spatial Datasets.” Technometrics, 60(4): 430–444.
URL https://doi.org/10.1080/00401706.2018.1437474 18

— (2019). “Multivariate spatial meta-kriging.” Statistics and Probability Letters, 144: 3–8.

URL https://doi.org/10.1080/00401706.2018.1437474 18

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

22

Guinness, J. (2018). “Permutation and Grouping Methods for Sharpening Gaussian Process

Approximations.” Technometrics, 60(4): 415–429.
URL https://doi.org/10.1080/00401706.2018.1437476 6, 20

Heaton, M., Datta, A., Finley, A., Furrer, R., Guinness, J., Guhaniyogi, R., Gerber, F.,
Gramacy, R., Hammerling, D., Katzfuss, M., Lindgren, F., Nychka, D., Sun, F., and
Zammit-Mangion, A. (2019). “Methods for Analyzing Large Spatial Data: A Review and
Comparison.” Journal of Agricultural, Biological and Environmental Statistics, 24(3):
398–425.
URL https://doi.org/10.1007/s13253-018-00348-w 3

Huang, H. and Sun, Y. (2018). “Hierarchical low-rank approximation of likelihoods for
large spatial datasets.” Journal of Computational and Graphical Statistics, 27: 110–118.
3

Katzfuss, M. (2013). “Bayesian nonstationary modeling for very large spatial datasets.”

Environmetrics, 24: 189–200. 4

— (2017). “A multi-resolution approximation for massive spatial datasets.” Journal of the

American Statistical Association, 112: 201–214.
URL http://dx.doi.org/10.1080/01621459.2015.1123632 3, 5

Katzfuss, M. and Cressie, N. (2012). “Bayesian hierarchical spatio-temporal smoothing for

very large datasets.” Environmetrics, 23: 94–107. 4

Katzfuss, M. and Guinness, J. (2017). “A General Framework for Vecchia Approximations

of Gaussian Processes.” arXiv preprint arXiv:1708.06302 . 6, 19

Katzfuss, M., Guinness, J., Gong, W., and Zilber, D. (2018). “Vecchia approximations of

Gaussian-process predictions.” arXiv preprint arXiv:1805.03309 . 6, 19

Kaufman, C. G., Scheverish, M. J., and Nychka, D. W. (2008). “Covariance Tapering
for Likelihood-Based Estimation in Large Spatial Data Sets.” Journal of the American
Statistical Association, 103: 1545–1555. 5

Lindgren, F., Rue, H., and Lindstrom, J. (2011). “An explicit link between Gaussian
ﬁelds and Gaussian Markov random ﬁelds: the stochastic partial diﬀerential equation
approach.” Journal of the Royal Statistical Society: Series B (Statistical Methodology),
73(4): 423–498.
URL http://dx.doi.org/10.1111/j.1467-9868.2011.00777.x 5

Ma, P. and Kang, E. L. (2017). “Fused Gaussian Process for Very Large Spatial Data.”

arXiv:1702.08797v3 . 6

Minsker, S. (2015). “Geometric median and robust estimation in banach spaces.” Bernoulli ,

21: 2308–2335. 18

Moller, J. and Waagepetersen, R. P. (2003). Statistical Inference and Simulation for Spatial

Point Processes. Chapman and Hall, ﬁrst edition. 1

Nychka, D., Bandyopadhyay, S., Hammerling, D., Lindgren, F., and Sain, S. (2015). “A
Multiresolution Gaussian Process Model for the Analysis of Large Spatial Datasets.”
Journal of Computational and Graphical Statistics, 24(2): 579–599.
URL http://dx.doi.org/10.1080/10618600.2014.914946 4

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

23

Nychka, D., Wikle, C., and Royle, J. A. (2002). “Multiresolution models for nonstationary

spatial covariance functions.” Statistical Modelling, 2(4): 315–331. 4

Plummer, M., Best, N., Cowles, K., and Vines, K. (2006). “CODA: Convergence Diagnosis

and Output Analysis for MCMC.” R News, 6(1): 7–11.
URL https://journal.r-project.org/archive/ 14

Rasmussen, C. E. and Williams, C. K. I. (2005). Gaussian Processes for Machine Learning.

Cambridge, MA: The MIT Press, ﬁrst edition. 4

Ribeiro Jr, P. J. and Diggle, P. J. (2012). geoR: a package for geostatistical analysis. R

package version 1.7-4.
URL https://cran.r-project.org/web/packages/geoR 9, 13

Rua, H. and Held, L. (2005). Gaussian Markov Random Fields : Theory and Applications.
Monographs on statistics and applied probability. Chapman and Hall/CRC Press, Boca
Raton, FL.
URL http://opac.inria.fr/record=b1119989 5

Rue, H., Martino, S., and Chopin, N. (2009). “Approximate Bayesian inference for latent
Gaussian models by using integrated nested Laplace approximations.” Journal of the
Royal Statistical Society: Series B (Statistical Methodology), 71(2): 319–392.
URL http://dx.doi.org/10.1111/j.1467-9868.2008.00700.x 5

Sang, H. and Huang, J. Z. (2012). “A Full Scale Approximation of Covariance Functions for
Large Spatial Data Sets.” Journal of the Royal Statistical society, Series B , 74: 111–132.
5

Sang, H., Jun, M., and Huang, J. (2011). “Covariance approximation for large multivariate
spatial datasets with an application to multiple climate model errors.” Annals of Applied
Statistics, 4: 2519–2548. 5

Schabenberger, O. and Gotway, C. A. (2004). Statistical Methods for Spatial Data Analysis.

Chapman and Hall/CRC Press, Boca Raton, FL, ﬁrst edition. 1

Shi, H., Kang, E. L., Konomi, B. A., Vemaganti, K., and Madireddy, S. (2017). “Uncertainty
Quantiﬁcation Using the Nearest Neighbor Gaussian Process.” In Chen, D.-G., Jin, Z.,
Li, G., Li, Y., Liu, A., and Zhao, Y. (eds.), New Advances in Statistics and Data Science,
89–107. Cham, Switzerland: Springer International Publishing.
URL https://doi.org/10.1007/978-3-319-69416-0_6 6

Shi, T. and Cressie, N. (2007). “Global Statistical Analysis of MISR Aerosol Data: A
Massive Data Product From NASA’s Terra Satellite.” Environmetrics, 18: 665–680. 4
Stein, M. L. (1999). Interpolation of Spatial Data: Some Theory for Kriging. Springer,

ﬁrst edition. 1, 2

Stein, M. L., Chi, Z., and Welty, L. J. (2004). “Approximating Likelihoods for Large Spatial

Data Sets.” Journal of the Royal Statistical society, Series B , 66: 275–296. 6

Stroud, J., Stein, M. L., and Lysen, S. (2017). “Bayesian and Maximum Likelihood Es-
timation for Gaussian Processes on an Incomplete Lattice.” Journal of Computational
and Graphical Statistics, 26: 108–120.
URL http://dx.doi.org/10.1080/10618600.2016.1152970 6

Sudipto Banerjee/Massive Spatial Data Modeling Using Bayesian Linear Regression

24

Vecchia, A. V. (1988). “Estimation and Model Identiﬁcation for Continuous Spatial Pro-

cesses.” Journal of the Royal Statistical society, Series B , 50: 297–312. 6, 10

Wikle, C. and Cressie, N. (1999). “A dimension reduced approach to space-time Kalman

ﬁltering.” Biometrika, 86: 815–829. 4

Wikle, C. K. (2010). “Low-Rank Representations for Spatial Processes.” Handbook of
Spatial Statistics, 107–118. Gelfand, A. E., Diggle, P., Fuentes, M. and Guttorp, P.,
editors, Chapman and Hall/CRC, pp. 107-118. 4

Yeniay, O. and Goktas, A. (2002). “A comparison of partial least squares regression with
other prediction methods.” Hacettepe Journal of Mathematics and Statistics, 31(99):
99–101. 9

Zhang, H. (2004). “Inconsistent estimation and asymptotically equal interpolations in
model-based geostatistics.” Journal of the American Statistical Association, 99(465):
250–261. 8

Zhang, L., Datta, A., and Banerjee, S. (2019). “Practical Bayesian Modeling and Inference
for Massive Spatial Datasets On Modest Computing Environments.” Statistical Analysis
and Data Mining: The ASA Data Science Journal , 12(3): 197–209.
URL https://doi.org/10.1002/sam.11413 3, 11, 13, 14, 16

