9
1
0
2

t
c
O
1
3

]
S
D
.
s
c
[

2
v
2
3
8
5
0
.
6
0
9
1
:
v
i
X
r
a

The Communication Complexity of Optimization∗

Santosh S. Vempala
Georgia Tech
vempala@cc.gatech.edu

Ruosong Wang
Carnegie Mellon University
ruosongw@andrew.cmu.edu

David P. Woodruﬀ
Carnegie Mellon University
dwoodruf@cs.cmu.edu

Abstract

We consider the communication complexity of a number of distributed optimization prob-
lems. We start with the problem of solving a linear system. Suppose there is a coordinator
together with s servers P1, . . . , Ps, the i-th of which holds a subset A(i)x = b(i) of ni constraints
Rd for which
of a linear system in d variables, and the coordinator would like to output an x
A(i)x = b(i) for i = 1, . . . , s. We assume each coeﬃcient of each constraint is speciﬁed using
L bits. We ﬁrst resolve the randomized and deterministic communication complexity in the
Θ(sd2L), respectively.
point-to-point model of communication, showing it is
We obtain similar results for the blackboard communication model. As a result of indepen-
dent interest, we show the probability a random matrix with integer entries in
is
invertible is 1

2−Θ(dL), whereas previously only 1

e
2−Θ(d) was known.

Θ(d2L + sd) and

2L, . . . , 2L

{−

∈

e

}

When there is no solution to the linear system, a natural alternative is to ﬁnd the solution
minimizing the ℓp loss, which is the ℓp regression problem. While this problem has been stud-
ied, we give improved upper or lower bounds for every value of p
1. One takeaway message
is that sampling and sketching techniques, which are commonly used in earlier work on dis-
tributed optimization, are neither optimal in the dependence on d nor on the dependence on the
approximation ε, thus motivating new techniques from optimization to solve these problems.

≥

−

−

Towards this end, we consider the communication complexity of optimization tasks which
generalize linear systems, such as linear, semideﬁnite, and convex programming. For linear
programming, we ﬁrst resolve the communication complexity when d is constant, showing it is
Θ(sL) in the point-to-point model. For general d and in the point-to-point model, we show an
Ω(d2L + sd) lower bound. In fact, we show if one perturbs the co-
O(sd3L) upper bound and an
eﬃcients randomly by numbers as small as 2−Θ(L), then the upper bound is
e
O(sd2L)+poly(dL),
e
and so this bound holds for almost all linear programs. Our study motivates understanding the
bit complexity of linear programming, which is related to the running time in the unit cost
RAM model with words of O(log(nd)) bits, and we give the fastest known algorithms for linear
programming in this model.

e

e

∗

Santosh S. Vempala was supported in part by NSF awards CCF-1717349 and DMS-1839323. Ruosong Wang and
David P. Woodruﬀ were supported in part by Oﬃce of Naval Research (ONR) grant N00014-18-1-2562. Part of this
work was done while the authors were visiting the Simons Institute for the Theory of Computing.

 
 
 
 
 
 
1

Introduction

Large-scale optimization problems often cannot ﬁt into a single machine, and so they are dis-
tributed across a number s of machines. That is, each of servers P1, . . . , Ps may hold a subset of
constraints that it is given locally as input, and the goal of the servers is to communicate with
each other to ﬁnd a solution satisfying all constraints. Since communication is often a bottleneck
in distributed computation, the goal of the servers is to communicate as little as possible.

There are several diﬀerent standard communication models, including the point-to-point model
In the point-to-point model, each pair of servers can talk directly
and the blackboard model.
with each other. This is often more conveniently modeled by looking at the coordinator model, for
which there is an extra server called the coordinator, and all communication must pass through the
coordinator. This is easily seen to be equivalent, from a total communication perspective, to the
point-to-point model up to a factor of 2, for forwarding messages from server Pi to server Pj, and
a term of log s per message to indicate which server the message should be forwarded to. Another
model of computation is the blackboard model, in which there is a shared broadcast channel among
1 servers and
all the s servers. When a server sends a message, it is visible to each of the other s
determines who speaks next, based upon an agreed upon protocol. We mostly consider randomized
communication, in which for every input, we require the coordinator to output the solution to
the optimization problem with high probability. For linear systems we also consider deterministic
communication complexity.

−

A number of recent works in the theory community have looked at studying speciﬁc optimization
problems in such communication models, such as principal component analysis [43, 47, 17] and
kernel [11] and robust variants [71, 28], computing higher correlations [43], ℓp regression [67, 28]
and sparse regression [19], estimating the mean of a Gaussian [73, 33, 19], database problems
[36, 70], clustering [21], statistical [68], graph problems [54, 68] and many, many more.

There are also a large number of distributed learning and optimization papers, for example
[10, 73, 75, 1, 18, 48, 72, 23, 55, 31, 29, 30, 40, 60, 59, 74, 6, 42]. With a few exceptions, these works
do not study general communication complexity, but rather consider speciﬁc classes of algorithms.
Namely, a number of these works only allow gradient and Hessian computations in each round, and
do not allow arbitrary communication. Another aspect of these works is that they typically do not
count total bit complexity, but rather only count number of rounds, whereas we are interested in
total communication. In a number of optimization problems, the bit complexity of storing a single
number in an intermediate computation may be as large as storing the entire original optimization
problem. It is therefore infeasible to transmit such a number. While one could round this number,
the eﬀect of rounding is often unclear, and could destroy the desired approximation guarantee. One
exception to the above is the work of [65], which studies the problem in which there are two servers,
each holding a convex function, who would like to ﬁnd a solution so as to minimize the sum of the
two functions. The upper bounds are in a diﬀerent communication model than ours, where the
functions are added together, while the lower bounds only apply to a restricted class of protocols.
Noticeably absent from previous work is the communication complexity of solving linear sys-
tems, which is a fundamental primitive in many optimization tasks. Formally, suppose there is a
coordinator together with s servers P1, . . . , Ps, the i-th of which holds a subset A(i)x = b(i) of ni
Rd
constraints of a d-dimensional linear system, and the coordinator would like to output an x
for which A(i)x = b(i) for i = 1, . . . , s. We further assume each coeﬃcient of each constraint is
speciﬁed using L bits. The ﬁrst question we ask is the following.

∈

1

Question 1.1. What is the communication complexity of solving a linear system?

y
k

kp = (

p)1/p is its ℓp norm.

minimizing the ℓp loss, which is the ℓp regression problem minx∈Rd
n
dimensional vector y,
i=1 |

yi|
In the distributed ℓp regression problem, each server has a matrix A(i)
Rd so that
Ax
k
kp. Note that here A
b

When there is no solution to the linear system, a natural alternative is to ﬁnd the solution
kp, where for an n-
Rni×d and a vector
P
Rni, and the coordinator would like to output an x
b(i)
kp is approximately
Rn×d is the
Ax′
minimized, namely, that
(1 + ε) minx′
k
s
matrix obtained by stacking the matrices A(1), . . . , A(s) on top of each other, where n =
i=1 ni.
Rn is the vector obtained by stacking the vectors b(1), . . . , b(s) on top of each other. We
Also, b
assume that each entry of A and b is an L-bit integer, and we are interested in the randomized
communication complexity of this problem.

∈
b
−

Ax
k

Ax
k

kp ≤

∈
−

P

−

−

∈

∈

∈

b

b

While previous work [50, 67] has looked at the distributed ℓp regression problem, such work
is based on two main ideas: sampling and sketching. Such techniques reduce a large optimiza-
tion problem to a much smaller one, thereby allowing servers to send succinct synopses of their
constraints in order to solve a global optimization problem.

Sampling and sketching are the key techniques of recent work on distributed low rank approx-
imation [67, 43] and regression algorithms. A natural question, which will motivate our study of
more complex optimization problems below, is whether other techniques in optimization can be
used to obtain more communication-eﬃcient algorithms for these problems.

Question 1.2. Are there tractable optimization problems for which sampling and sketching tech-
niques are suboptimal in terms of total communication?

To answer Question 1.2 it is useful to study optimization problems generalizing both linear
systems and ℓp regression for certain values of p. Towards this end, we consider the communication
complexity of linear, semideﬁnite, and convex programming. Formally, in the linear programming
problem, suppose there is a coordinator together with s servers P1, . . . , Ps, the i-th of which holds
b(i) of ni constraints of a d-dimensional linear system, and the coordinator, who
a subset A(i)x
≤
Rd for which cT x is maximized subject to
holds a vector c
∈
A(i)x
b(i) for i = 1, . . . , s. We further assume each coeﬃcient of each constraint, as well as the
objective function c, is speciﬁed using L bits.

Rd, would like to output an x

≤

∈

Question 1.3. What is the communication complexity of solving a linear program?

One could try to implement known linear programming algorithms as distributed protocols.
The main challenge here is that known linear programming algorithms operate in the real RAM
model of computation, meaning that basic arithmetic operations on real numbers can be performed
in constant time. This is problematic in the distributed setting, since it might mean real numbers
need to be communicated among the servers, resulting in protocols that could have inﬁnite com-
munication. Thus, controlling the bit complexity of the underlying algorithm is essential, and this
motivates the study of linear programming algorithms in the unit cost RAM model of computation,
meaning that a word is O(log(nd)) bits, and only basic arithmetic operations on words can be
performed in constant time. Such a model is arguably more natural than the real RAM model. If
one were to analyze the fastest linear programming algorithms in the unit cost RAM model, their
time complexity would blow up by poly(dL) factors, since the intermediate computations require
manipulating numbers that grow exponentially large or small. Surprisingly, we are not aware of
any work that has addressed this question:

2

Question 1.4. What is the best possible running time of an algorithm for linear programming in
the unit cost RAM model?

As far as time complexity is concerned, it is not even known if linear programming is inherently
Indeed, a long line of work on interior point
more diﬃcult than just solving a linear system.
methods, with the current most recent work of [25], suggests that solving a linear program may
not be substantially harder than solving a linear system. One could ask the same question for
communication.

Question 1.5. Is solving a linear program inherently harder than solving a linear system? What
about just checking the feasibility of a linear program versus that of a linear system?

Recent Independent Work. A recent independent work [7] also studies solving linear programs
in the distributed setting, although their focus is to study the tradeoﬀ between round complexity
and communication complexity in low dimensions, while our focus is to study the communication
complexity in arbitrary dimensions. Note, however, that we also provide nearly optimal bounds for
constant dimensions for linear programming in both coordinator and blackboard models.

1.1 Our Contributions

We make progress on answering the above questions, with nearly tight bounds in many cases.

For a function f , we let

O(f ) = f polylog(sndL/ε) and similarly deﬁne

Θ and

Ω.

e
1.1.1 Linear Systems

e

e

We begin with linear systems, for which we obtain a complete answer for both randomized and

deterministic communication, in both coordinator and blackboard models of communication.

Theorem 1.6. In the coordinator model, the randomized communication complexity of solving a
Θ(sd2L). In the
Θ(d2L + sd), while the deterministic communication complexity is
linear system is
blackboard model, both the randomized communication complexity and the deterministic communi-
cation complexity are

Θ(d2L + s).

e

e

Theorem 1.6 shows that randomization provably helps for solving linear systems. The theorem

e

also shows that in the blackboard model the problem becomes substantially easier.

1.1.2 Approximate Linear Systems, i.e., ℓp Regression

We next study the ℓp regression problem in both the coordinator and blackboard models of
communication. Finding a solution to a linear system is a special case of ℓp regression; indeed in
the case that there is an x for which Ax = b we must return such an x to achieve (1 + ε) relative
error in objective function value for ℓp regression. Consequently, our lower bounds for linear systems
apply also to ℓp regression for any ε > 0.

We ﬁrst summarize our results in Table 1 and Table 2 for constant ε. We state our results
primarily for randomized communication. However, in the case of ℓ2 regression, we also discuss
deterministic communication complexity.

One of the main takeaway messages from Table 1 is that sampling-based approaches, namely
those based upon the so-called Lewis weights [27], would require Ω(dp/2) samples for ℓp regression

3

Error Measure
ℓ1 (randomized)
ℓ1 (deterministic)
ℓ2 (randomized)
ℓ2 (deterministic)
ℓp for constant p > 2
ℓ∞

Upper Bound
O(sd2L)
O(sd2L)
e
O(sd2L)
e
O(sd2L)
e
O(sd3L)
e
O(sd3L)
e
e

Lower Bound
Ω(d2L + sd)
Ω(sd2L)
e
Ω(d2L + sd)
e
Ω(sd2L)
e
Ω(d2L + sd)
Ω(d2L + sd)
e
e

e

Theorem

Theorem 7.1, 3.8
Theorem 7.1, 3.6
Theorem 6.1, 3.8
Theorem 6.1, 3.6
Theorem 8.3, 3.8
Theorem 8.1, 3.8

Table 1: Summary of our results for ℓp regression in the coordinator model for constant
ε.

when p > 2, and thus communication. Another way for solving ℓp regression for p > 2 is via
sketching, as done in [67], but then the communication is Ω(n1−2/p). Our method, which is deeply
O(sd3L) communication.
tied to linear programming, discussed more below, solves this problem in
Thus, this gives a new method, departing from sampling and sketching techniques, which achieves
much better communication. Our method involves embedding ℓp into ℓ∞, and then using distributed
algorithms for linear programming to solve ℓ∞ regression.

e

Error Measure
ℓ1
ℓ2
ℓp for constant p > 2
ℓ∞

Upper Bound
O(s + d2L)
O(s + d2L)
e
sd + d4L, sd3L
{
e
sd + d4L, sd3L
{

O(min
O(min

)
}
)
}

Lower Bound
Ω(s + d2L)
Ω(s + d2L)
e
Ω(s + d2L)
e
Ω(s + d2L)
e
e

Theorem

Theorem 7.3, 3.8
Theorem 6.3, 3.8
Theorem 8.3, 3.8
Theorem 8.1, 3.8

Table 2: Summary of our results for ℓp regression in the blackboard model for constant
ε.

As with linear systems, one takeaway message from the results in Table 2 is that the problems
have signiﬁcantly more communication-eﬃcient upper bounds in the blackboard model than in the
coordinator model. Indeed, here we obtain tight bounds for ℓ1 and ℓ2 regression, matching those
that are known for the easier problem of linear systems.

We next describe our results for non-constant ε in both the coordinator model and the black-

board model. Here we focus on ℓ1 and ℓ2, which illustrate several surprises.

Error Measure
ℓ1
ℓ2 (randomized)
ℓ2 (deterministic)

e

Upper Bound
O(min(sd2L + d2L

ε2 , sd3L
ε )

O(sd2L)
O(sd2L)
e
e

Lower Bound
Ω(d2L + sd)
Ω(d2L + sd)
e
Ω(sd2L)
e

Theorem

Theorem 7.3, 7.4, 3.8
Theorem 6.1, 3.8
Theorem 6.1, 3.6

Table 3: Summary of our results for ℓ1 and ℓ2 regression in the coordinator model for
general ε.

e

One of the most interesting aspects of the results in Table 3 is our dependence on ε for ℓ1

4

regression, where for small enough ε relative to sd, we achieve a 1/ε instead of a 1/ε2 dependence.
We note that all sampling [27] and sketching-based solutions [67] to ℓ1 regression have a 1/ε2
dependence. Indeed, this dependence on ε comes from basic concentration inequalities. In contrast,
our approach is based on preconditioned ﬁrst-order methods described in more detail below.

Error Measure Upper Bound
O(s + d2L
ε2 )
O(s + d2L
ε )
e
e

ℓ1
ℓ2

Lower Bound

Theorem

Ω(s + d
Ω(s + d

ε + d2L) for s > Ω(1/ε)
ε1/2 + d2L) for s > Ω(1/√ε)

e

Theorem 7.3, 3.8, 5.2
Theorem 6.3, 3.8, 5.3

Table 4: Summary of our results for ℓ1 and ℓ2 regression in the blackboard model for
general ε.

e

A takeaway message from Table 4 is that our lower bound shows some dependence on ε is
necessary both for ℓ1 and ℓ2 regression, provided ε is not too small. This shows that in the
O(d2L + s) upper bound for these problems as for
blackboard model, one cannot obtain the same
linear systems, thereby separating their complexity from that of solving a linear system.

1.1.3 Linear Programming

e

One of our main technical ingredients is to recast ℓp regression problems as linear programming
problems and develop the ﬁrst communication-eﬃcient solutions for distributed linear program-
ming. Despite this problem being one of the most important problems that we know how to
solve in polynomial time, we are not aware of any previous work considering its communication
complexity in generality besides a recent independent work [7]

First, when the dimension d is constant, we obtain nearly optimal upper and lower bounds.

Theorem 1.7. In constant dimensions, the randomized communication complexity of linear pro-
Ω(s + L) in the blackboard model. Our upper
Θ(sL) in the coordinator model and
gramming is
Rd, while the lower bounds hold
bounds allow the coordinator to output the solution vector x
∈
already for testing if the linear program is feasible. Here the
) notation
Ω(
) notation and the
Θ(
·
·
suppress only polylog(sL) factors.

e

e

Despite the fact that we do not have tight upper bounds matching the

e
Ω(s + L) lower bounds in
the blackboard model, under the additional assumption that each constraint in the linear program is
placed on a random server, we develop an algorithm with a matching
O(s + L) communication cost.
Partitioning constraints randomly across servers instead is common in distributed computation, see,
e.g., [8]. Neverthelss we leave it as an open problem in the blackboard model in constant dimensions,
to remove this requirement.

e

e

e

Θ(s + L) in both models. Again, the

For solving a linear system in constant dimensions, the randomized communication complexity
is
) notation suppresses only polylog(sL) factors. Thus,
Θ(
·
in the coordinator model, we separate the communication complexity of these problems. We can
also separate the complexities in the blackboard model if we instead look at the feasibility problem.
Here instead of requiring the coordinator to output the solution vector, we just want to see if the
linear system or linear program is feasible. We have the following theorem for this.

e

e

Theorem 1.8. In constant dimensions, the randomized communication complexity of checking
whether a system of linear equations is feasible is O(s log L) in either the coordinator or blackboard
model of communication.

5

Combining Theorem 1.7 and Theorem 1.8, we see that for feasibility in the blackboard model,
O(s) bits, and thus

Ω(s + L) bits, while linear system feasibility takes

linear programming requires
we separate these problems in the blackboard model as well.
e

Returning to linear programs, we next consider the complexity in arbitrary dimensions.

e

∈

e

Theorem 1.9. In the coordinator model, the randomized communication complexity of exactly
cT x : Ax
with n constraints in dimension d and all coeﬃcients
solving a linear program max
{
Ω(d2L + sd). Here the
O(sd3L). Moreover it is lower bounded by
speciﬁed by L-bit numbers is
upper and lower bounds require the coordinator to output the solution vector x

Rd.

≤

}

b

e

The lower bound in Theorem 1.9 just follows from our lower bound for linear systems. The
upper bound is based on an optimized distributed cutting-plane algorithm. We describe the idea
below.

e

While the upper bound is

O(sd3L), one can further improve it as follows. We show that if the
coeﬃcients of A in the input to the linear program are perturbed independently by i.i.d. discrete
Gaussians with variance as small as 2−Θ(L), then we can improve the upper bound for solving this
O(sd2L + d4L), where now the success probability of the algorithm is taken
perturbed problem to
over both the randomness of the algorithm and the random input instance, which is formed by a
random perturbation of a worst-case instance. Note that this is an improvement for suﬃciently
large s. Our model coincides with the well-studied smooth complexity model of linear programming
[61, 14, 62]. However, a major diﬀerence is that the variance of the perturbation needs to be at
least inverse polynomial in their works, whereas we allow our variance to be as small as 2−Θ(L).

e

Theorem 1.10. In the smoothed complexity model with discrete Gaussians of variance 2−Θ(L),
with n
the communication complexity of exactly solving a linear program max
constraints in dimension d and all coeﬃcients speciﬁed by L-bit numbers, with probability at least
O(sd2L+d4L) in the coordinator
9/10 over the input distribution and randomness of the protocol, is
model.

cT x : Ax
{

≤

}

b

While our focus in this paper is on communication, our upper bounds also give a new technique
for improving the time complexity in the unit cost RAM model of linear programming, where
arithmetic operations on words of size O(log(nd)) can be performed in constant time. For this
fundamental problem we obtain the fastest known algorithm even in the non-smoothed setting of
linear programming.

e

Theorem 1.11. The time complexity of solving an n
O(ndωL + poly(dL)) in the unit cost RAM model.

×

d linear program with L-bit coeﬃcients is

We note that this is for solving an LP exactly in the RAM model with words of size O(log(nd))
e
bits. The current fastest linear programming algorithms [45, 46, 25] state the bounds in terms of
additive error ε, which incurs a multiplicative factor of at least Ω(dL) to solve the problem exactly.
Also such algorithms manipulate large numbers at intermediate points in the algorithm, which
are at least L bits, which could take Ω(L) time to perform a single operation on. It seems that
transferring such results to the unit cost RAM model with O(log(nd)) bit words incurs time at least
Ω(nd2.5L2 + dw+1.5L2). This holds true even of the recent work [25], which focuses on the setting
n = O(d) and does not improve the leading nd2.5L2 term. Even such a bit-complexity bound
needs careful checking of the number of bits required as recent improvements use sophisticated
inverse maintenance methods to save on the number of operations (an exercise that was carried out
thoroughly for the Ellipsoid method in [34]).

6

1.1.4

Implications for Convex Optimization and Semideﬁnite Programming

Our upper bounds also extend to more general convex optimization problems. For these, we
must modify the problem statement to ﬁnding an ε-additive approximation rather than the exact
solution. We obtain the following upper bound for a convex program in Rd.

∈

Theorem 1.12. The communication complexity of solving the convex optimization problem min
x
i Ki}
point y s.t. cT y

RBn, one per server, to within an additive error ε, i.e., ﬁnding a

for convex sets Ki ⊆
OP T + ε and y

i Ki + εBn is O(sd2 log(Rd/ε) log d).

T
If the objective function is not known to all servers, we incur an additional O(sdL) commu-
d symmetric matrices and n linear constraints this
O(sd4 log(1/ε)). Note that we can simply send all the constraints to one server in

nication. For semideﬁnite programs with d
gives a bound of
O(nd2L) communication, so this is always an upper bound.

T

×

≤

∈

cT x :
{

e

1.2 Our Techniques

1.2.1 Linear Systems

To solve linear systems in the distributed setting, the coordinator can go through the servers
one by one. The coordinator and all servers maintain the same set C of linearly independent linear
equations. For each server Pi, if there is a linear equation stored by Pi that is linearly independent
with linear equations in C, then Pi sends that linear equation to all other servers and adds that
linear equation into C. In the end, C will be a maximal set of linearly independent equations, and
thus the coordinator can simply solve the linear equations in C. This protocol is deterministic and
has communication complexity O(sd2L) in the coordinator model and O(s+d2L) in the blackboard
model, since at most d linear equations will be added into the set C.

In fact, the preceding protocol is optimal for deterministic protocols, even just for testing the
feasibility of linear systems. To prove lower bounds, we ﬁrst prove the following new theorem about
random matrices which may be of independent interest.

Theorem 1.13 (Informal version of Theorem 3.1). Let R be a d
integer entries in

. The probability that R is invertible is 1
}
The previous best known probability bound was only 1

2L, . . . , 2L

2−Θ(d) [63, 16]; we stress that the
results of [16] are not suﬃcient 1 to prove our stronger bound with the extra factor of L in the
exponent, which is crucial for our lower bound.

d matrix with i.i.d. random

2−Θ(dL).

−

{−

−

×

H ⊆

With Theorem 1.13, in Lemma 3.3, we use the probabilistic method to construct a set of
= 2Ω(d2L) matrices
,

Rd×d with integral entries in [
= T −1ed, where ed is the d-th standard basis vector.

|H|
S−1ed 6
Now consider any deterministic protocol for testing the feasibility of linear systems. Suppose
, then the entire linear system is
the linear system on the i-th server is Hix = ed for some Hi ∈ H
feasible if and only if H1 = H2 = . . . = Hs. This is equivalent to the problem in which each server
), and the goal is to test whether all strings are the same
receives a binary string of length log(

2L, 2L], such that for any S, T

∈ H

−

|H|

1We have veriﬁed this with Philip Matchett Wood, who is an author of [16]. The issue is that in their Corollary
1.2, they have an explicit constraint on the cardinality of the set S, i.e., |S| = O(1). In their Theorem 2.2, it is
assumed that |S| = no(n). Thus, as far as we are aware, there are no known results suﬃcient to prove our singularity
probability bound.

7

)) for this problem can
or not. In the coordinator model, a deterministic lower bound of Ω(s log(
be proved using the symmetrization technique in [54, 69], which gives an optimal Ω(sd2L) lower
bound. An optimal Ω(s + d2L) deterministic lower bound can also be proved in the blackboard
model. The formal analysis is given in Section 3.3.

|H|

For solving linear systems, an Ω(d2L) lower bound holds even for randomized algorithms in the
coordinator model. When there is only a single server which holds a linear system Hx = ed for
, in order for the coordinator to know the solution x = H −1ed, standard information-
some H
) bits of communication is necessary, which gives an Ω(d2L)
theoretic argument shows that log(
lower bound. This idea is formalized in Section 3.4. A natural question is whether the O(sd2L)
upper bound is optimal for randomized protocols.

∈ H

|H|

We ﬁrst show that in order to test feasibility, it is possible to achieve a communication complexity
of O(sd2 log(dL)), which can be exponentially better than the bound for deterministic protocols.
The idea is to use hashing. With randomness, the servers can ﬁrst agree on a random prime number
p, and test the feasibility over the ﬁnite ﬁeld Fp. It suﬃces to have the prime number p randomly
generated from the range [2, poly(dL)], and thus the L factor in the communicataion complexity
of deterministic protocols can be improved to log p = log(dL). However, it is still unclear if solving
linear systems in the coordinator model will require Ω(sd2L) bits of communication for randomized
protocols.

e

Quite surprisingly, we show that O(sd2L) is not the optimal bound for randomized protocols,
Θ(d2L+sd). In the deterministic protocol with communication complexity
and the optimal bound is
O(sd2L), most communication is wasted on synchronizing the set C, which requires the servers to
send linear equations to all other servers. In our new protocol, only the coordinator maintains the
set C. The issue now, however, is that the servers no longer know which linear equation they own is
linearly independent with those equations in C. On the other hand, each server can simply generate
a random linear combination of all linear equations it owns. We can show that if a server does
have a linear equation that is linearly independent with those in C, with constant probability, the
random linear combination is also linearly independent with those in C, and thus the coordinator
can add the random linear combination into C. Notice that taking random linear combinations
to preserve the rank of a matrix is a special case of dimensionality reduction or sketching, which
comes up in a number of applications, see, for example compressed sensing [20, 13], data streams
[4], and randomized numerical linear algebra [66]. Here though, a crucial diﬀerence is that we just
need the fact that if a set of vectors S is not contained in the span of another set of vectors V , then
a random linear combination of the vectors in S is also not in the span of V with high probability.
This allows us to adaptively take as few linear combinations as possible to solve the linear system,
enabling us to achieve much lower communication than would be possible by just sketching the
linear systems at each server and non-adaptively combining them.

If we implement this protocol na¨ıvely, then the communication complexity will be

O(d2L+sdL),
O(dL) communication
since at most d linear equations will be added into C, and there is an
complexity associated with each of them. Furthermore, even if a server does not have any linear
equation that is linearly independent with C, it still needs to send random linear combinations to
the coordinator, which would require
O(sd), we
can still use the hashing trick mentioned before. If a server generates a random linear combination,
it can ﬁrst test whether the linear combination is linearly independent with C over the ﬁnite ﬁeld
p, for a random prime p chosen in [2, poly(dL)]. This will reduce the communication complexity
O(d) for each test. If the linear equation is indeed linearly independent with C, then the server
to

O(sdL) communication. To improve this further to

e

e

e

e

e

8

sends the original linear equation (without taking the residual modulo p) to the coordinator. Again
the total communication complexity for sending the original linear equations is upper bounded by
O(d2L). Thus, the total communication complexity is upper bounded by
O(d2L + sd). See Section
4.2 for the formal analysis.

By a reduction from the OR of s

1 copies of the two-server set-disjointness problem to solving
−
Ω(sd) lower bound, which holds even for testing feasibility of
linear systems, we can prove an extra
d as characteristic vectors of subsets of
linear systems. Here the idea is to interpret vectors in
0, 1
}
{
e
[d]. One of the servers will ﬁx the solution of the linear system to be a predeﬁned vector x. Each
server Pi has a single linear equation aT
i x = 1. By interpreting vectors as sets, aT
i x = 1 implies the
set represented by ai and x are intersecting. Thus, the servers are actually solving the OR of s
1
copies of the two-server set-disjointness problem, which is known to have
Ω(sd) communication
complexity [54, 68]. This lower bound is formally given in Section 3.4.

−

e

1.2.2 Linear Regression

e

b

e

−

Ax

For an ℓ2 regression instance minx k

k2, the optimal solution can be calculated using the
normal equations, i.e., the optimal solution x satisﬁes AT Ax = AT b. This already gives a simple yet
nearly optimal deterministic protocol for ℓ2 regression in the coordinator model: the coordinator
calculates AT A and AT b using only
O(sd2L) bits of communication by collecting the covariance
O(sd2L) communication complexity matches
matrices from each server and summing them up. The
our lower bound for solving linear systems for deterministic protocols in the coordinator model.
e
However, when implemented in the blackboard model, the communication complexity of this proto-
O(sd2L). To improve this bound, we ﬁrst show how to eﬃciently obtain approximations
col is still
to leverage scores in both models. Our protocol is built upon the algorithm in [26], but implemented
O(sd2L) communication complexity in the
in a distributed manner. The resulting algorithm has
O(s + d2L) communication complexity in the blackboard model. With
coordinator model but only
O(d/ε2) rows of the matrix A to
approximate leverage scores, the coordinator can then sample
obtain a subspace embeeding, at which point it will be easy to calculate a (1 + ε)-approximate solu-
tion to the ℓ2 regression problem. The number of sampled rows can be further improved to
O(d/ε)
using S´arlos’s argument [57] since solving ℓ2 regression does not necessarily require a full (1 + ε)
O(s + d2L/ε) in
subspace embedding, which results in a protocol with communication complexity
the blackboard model. Full details can be found in Section 6.

e

e

e

e

e

One may wonder if the dependence on 1/ε is necessary for solving ℓ2 regression in the blackboard
model.
In Section 5, we show that some dependence on 1/ε is actually necessary. We show
an Ω(d/√ε) lower bound whenever s > Ω(1/√ε). The hardness follows from the fact that if
the matrix A satisﬁes A(i) = I for all i
[s], then the optimal solution is just the average of
b(1), b(2), . . . , b(s). Thus, if we can get suﬃciently good approximation to the ℓ2 regression problem,
then we can actually recover the sum of b(1), b(2), . . . , b(s), at which point we can resort to known
communication complexity lower bound in the blackboard model [54]. This argument will also give
an Ω(d/ε) lower bound for (1 + ε)-approximate ℓ1 regression in the blackboard model, whenever
s > Ω(1/ε). The formal analysis can be found in Section 5.

∈

e

For ℓ1 regression, we can no longer use the normal equations. However, we can obtain approx-
imations to ℓ1 Lewis weights by using approximations to leverage scores, as shown in [27]. With
approximate ℓ1 Lewis weights of the A matrix, the coordinator can then obtain a (1+ε) ℓ1 subspace
O(d/ε2) rows. This will give an O(sd2L + d2L/ε2) upper bound for (1 + ε)-
embedding by sampling
approximate ℓ1 regression in the coordinator model, and an O(s + d2L/ε2) upper bound in the

e

9

blackboard model. It is unclear if the number of sampled rows can be further reduced since there
is no known ℓ1 version of S´arlos’s argument. A natural question is whether the 1/ε2 dependence is
optimal. We show that the dependence on ε can be further improved to 1/ε, by using optimization
techniques, or more speciﬁcally, ﬁrst-order methods. Despite the fact that the objective function of
ℓ1 regression is neither smooth nor strongly-convex, it is known that by using Nesterov’s Acceler-
ated Gradient Descent and smoothing reductions [51], one can solve ℓ1 regression using only O(1/ε)
full gradient calculations. On the other hand, the complexity of ﬁrst-order methods usually has
dependences on various parameters of the input matrix A, which can be unbounded in the worst
case. Fortunately, recent developments in ℓ1 regression [32] show how to precondition the matrix
A by simply doing an ℓ1 Lewis weights sampling, and then rotating the matrix appropriately. By
carefully combining this preconditioning procedure with Accelerated Gradient Descent, we obtain
O(sd3L/ε) in the
an algorithm for (1+ε)-approximate ℓ1 regression with communication complexity
coordinator model, which shows it is indeed possible to improve the ε dependence for ℓ1 regression.
A formal analysis is given in Section 7.

e

For general ℓp regression, if we still use Lewis weights sampling, then the number of sampled
rows and thus the communication complexity will be Ω(dp/2). Even worse, when p =
, Lewis
weights sampling will require an unbounded number of samples. However, ℓ∞ regression can be
easily formulated as a linear program, which we show how to solve exactly in the distributed
setting. Inspired by this approach, we further develop a general reduction from ℓp regression to
linear programming. Our idea is to use the max-stability of exponential random variables [3] to
embed ℓp into ℓ∞, write the optimization problem in ℓ∞ as a linear program and then solve the
problem using linear program solvers. However, such embeddings based on exponential random
variables usually produce heavy-tailed random variables and makes the dilation bound hard to
analyze. Here, since our goal is just to solve a linear regression problem, we only need the dilation
bound for the optimal solution of the regression problem. The formal analysis in Section 8 shows
O(d/ε2)
that (1 + ε)-approximate ℓp regression can be reduced to solving a linear program with
variables, which implies a communication protocol for ℓp regression without the Ω(dp/2) dependence.

∞

e

1.2.3 Linear and Convex Programs

We adapt two diﬀerent algorithms from the literature for eﬃcient communication and implement
them in the distributed setting. The ﬁrst is Clarkson’s algorithm, which works by sampling O(d2)
constraints in each iteration and ﬁnds an optimal solution to this subset; the sampling weights
are maintained implicitly. In each iteration the total communication is O(d3L) for gathering the
O(sd2L) per round to send the solution to this subset of constraints
constraints and an additional
to all servers. This solution is used to update the sampling weights. Clarkson’s algorithm has the
nice guarantee that it needs only O(d log n) rounds with high probability. A careful examination
of this algorithm shows that the bit complexity of the computation (not the communication) is
dominated by checking whether a proposed solution satisﬁes all constraints, i.e., computing Ax for
O(ndωL) in the unit cost RAM model
a given x. We show this can be done with time complexity
and this is the leading term of the claimed time bound.

e

Notice that the

O(sd3L) term in the communication complexity of Clarkson’s algorithm comes
from the fact that the protocol needs to send an optimal solution x∗ of a linear program with size
O(d2)
d for a total of O(d log n) times. However, when each server Pi receives x∗, all Pi will do
is to check whether x∗ satisﬁes the constraints stored on Pi or not. Notice that here entries in the
constraints have bit complexity L, whereas the solution vector x∗ has bit complexity
O(dL) for each

×

e

e

10

e

entry. Intuitively, for most linear programs, we don’t need such a high precision for the solution
vector x∗. This leads to the idea of smoothed analysis. We show that if the coeﬃcients of A in the
input to the linear program are perturbed independently by i.i.d. discrete Gaussians with variance
as small as 2−Θ(L), then we can improve the upper bound for solving this perturbed problem to
O(sd2L+d4L). The reason here is that with Gaussian noise, we can round each entry of the solution
O(L), which would suﬃce for verifying whether x∗ satisﬁes the
vector x∗ to have bit complexity
e
constraints or not, for most linear programs. Full details regarding Clarkson’s algorithm and the
smoothed analysis model can be found in Section 10.

e

One minor drawback of Clarkson’s algorithm is it has a dependence on log n.

In constant
Ω(s + L) lower bound in the blackboard model holds only when n = 2Ω(L), in

dimensions, our
which case the communication complexity of Clarkson’s algorithm will be

O(sL + L2).

e

Under the additional assumption that each constraint in the linear program is placed on a
e
random server, we develop an algorithm with communication complexity
O(s + L) in the black-
board model. To achieve this goal, we modify Seidel’s classical algorithm and implement it in the
distributed setting. Seidel’s algorithm beneﬁts from the additional assumption from two aspects.
On the one hand, Seidel’s classical algorithm needs to go through all the constraints in a random
order, which can be easily achieved now since all constraints are placed on a random server. On
the other hand, Seidel’s classical algorithm needs to make a recursive call each time it ﬁnds one of
n
i=1 d/i = Θ(d log n) recursive
d constraints that determines the optimal solution, and will make
calls in expectation. To implement Seidel’s algorithm in the distributed setting, each time we ﬁnd
one of the d constraints that determines the optimal solution, the current server also needs to
broadcast that constraint. Thus, na¨ıvely we need to broadcast O(d log n) constraints during the
execution, which would result in O(s + L log n) communication. Under the additional assumption,
with good probability, the ﬁrst server P1 stores at least Ω(n/s) constraints. Since the ﬁrst server
P1 does not need to make any recursive calls or broadcasts, the total number of recursive calls (and
thus broadcasts) will be

n
i=Ω(n/s) d/i = Θ(d log s). The formal analysis is given in Section 12.

P

e

P

For convex programming, we have to use a more general algorithm. We use a reﬁned version
of the classical center-of-gravity method. The basic idea is to round violated constraints that are
used as cutting planes to O(d log d) bits. We optimize over the ellipsoid method in the following
two ways. First, we round the violated constraint sent in each iteration by locally maintaining an
ellipsoid to ensure the rounding error does not aﬀect the algorithm. Roughly speaking, each server
maintains a well-rounded current feasible set, and the number of bits needed in each round is thus
O(d). Secondly, we use the center of gravity method to make sure the volume is cut by a
only
1/d) factor in each iteration, even when constraints are rounded.
constant factor rather than a (1
See Section 11 for the formal analysis.

−

e

2 Preliminaries

2.1 Notation

For m matrices A(1)

Rd×n2, . . . , A(m)
A(m)] to
denote the matrix in Rd×(n1+n2+···+nm) whose ﬁrst n1 columns are the same as A(1), the next n2
columns are the same as A(2), . . . , and the last nm columns are the same as A(m).

Rd×nm, we use [A(1) A(2)

Rd×n1, A(2)

· · ·

∈

∈

∈

For a matrix A

Ax
{
the columns of the matrix A. For a set of vectors S

Rn×d, we use span(A) =

∈

Rd

x
to denote the subspace spanned by
∈
Rd, we use span(S) to denote the subspace

}

|
⊆

11

spanned by the vectors in S. For a set of linear equations C, we also span(C) to denote all linear
combinations of linear equations in C. We use Ai to denote the i-th column of A and Ai to denote
the i-th row of A. We use A† to denote the Moore-Penrose inverse of A. We use rank(A) to denote
the rank of A over the real numbers and rankp(A) to denote the rank of A over the ﬁnite ﬁeld Fp.
Rd, we use
x
xi|
k
to denote their inner product.

to denote its ℓp norm. For two vectors x

and y, we use

kp =

d
i=1 |

1/p

p

For a vector x
x, y
h

∈
i
For matrices A and B, we say A

(cid:16)P

(cid:17)

≈κ B if and only if

1
κ

B

A

(cid:22)

(cid:22)

κB,

where
(cid:22)
deﬁnite.

refers to the L¨owner partial ordering of matrices, i.e., A

B if B

−

(cid:22)

A is positive semi-

2.2 Models of Computation and Problem Settings

We study the distributed linear regression problem in two distributed models: the coordinator
model (a.k.a. the message passing model) and the blackboard model. The coordinator model rep-
resents distributed computation systems with point-to-point communication, while the blackboard
model represents those where messages can be broadcasted to each party.

In the coordinator model, there are s

2 servers P1, P2, . . . , Ps, and one coordinator. These
s servers can directly send messages to the coordinator through a two-way private channel. The
computation is in terms of rounds: at the beginning of each round, the coordinator sends a message
to some of the s servers, and then each of those servers that have been contacted by the coordinator
sends a message back to the coordinator.

≥

In the alternative blackboard model, the coordinator is simply a blackboard where the s servers
P1, P2, . . . , Ps can share information; in other words, if one server sends a message to the coordina-
tor/blackboard then the other s
1 servers can see this information without further communication.
The order for the servers to send messages is decided by the contents of the blackboard.

−

For both models we measure the communication cost which is deﬁned to be the total number

of bits sent through the channels.

∈

In the distributed linear system problem, there is a data matrix A

[A(1) b(1)], [A(2) b(2)], . . . , [A(s) b(s)]
}
{

Rn×d and a vector b of
2L, 2L], where L is the bit complexity.
observed values. All entries in A and b are integers between [
The matrix [A b] is distributed row-wise among the s servers P1, P2, . . . , Ps. More speciﬁcally, for
each server Pi, there is a matrix [A(i) b(i)] stored on Pi, which is a subset of rows of [A b]. Here
we assume
is a partition of all rows in [A b]. The goal of the
feasibility testing problem is to design a protocol, such that upon termination of the protocol, the
coordinator reports whether the linear system Ax = b is feasible or not. The goal of the linear
system solving problem is to design a protocol, such that upon termination of the protocol, either
Rd, such that Ax∗ = b, or the coordinator reports the linear
the coordinator outputs a vector x∗
system Ax = b is infeasible. It can be seen that the linear system solving problem is strictly harder
than the feasibility testing problem.

−

∈

In the distributed linear regression problem, there is a data matrix A

Rn×d and a vector b of
observed values, which is distributed in the same way as in the distributed linear system problem.
The goal of the distributed ℓp regression problem is to design a protocol, such that upon termination
of the protocol, the coordinator outputs a vector x∗

Rd to minimize

∈

b

Ax
k

−

kp.

∈

12

In the distributed linear programming problem, there is a matrix A

Rn×d and a vector b,
which is distributed in the same way as in the distributed linear system problem. The goal of the
feasibility testing problem is to design a protocol, such that upon termination of the protocol, the
coordinator reports whether the linear program Ax
b is feasible or not. In the linear programming
solving problem, the goal is to design a protocol, such that upon termination of the protocol, the
coordinator outputs a vector x∗
b is satisﬁed. There can also be a vector
c
c, x
h
under the constraint that Ax

Rd which is known to all servers, and in this case the goal is to minimize (or maximize)

≤
Rd such that Ax∗

≤

b.

∈

∈

∈

i

≤

2.3 Row Sampling Algorithms

Deﬁnition 2.1 ([26]). Given a matrix A

∈

Rn×d. The leverage score of a row Ai is deﬁned to be

Given another matrix B
be

∈

τi(A) = Ai(AT A)†(Ai)T .

Rn′×d, the generalized leverage score of a row Ai w.r.t. B is deﬁned to

τ B
i (A) =

Ai(BT B)†(Ai)T

(

∞

ker(B),

if Ai
⊥
otherwise.

Deﬁnition 2.2 ([27]). Given a matrix A
[n] we have
weights such that for each i

∈

Rn×d. The ℓ1 Lewis weights

∈

wi}
{

n
i=1 are the unique

where W is the diagonal matrix formed by putting

(cid:16)

(cid:17)
n
i=1 on the diagonal.

wi}
{

wi = τi

W

−1/2

A

,

Theorem 2.1 (ℓ2 Matrix Concentration Bound, Lemma 4 in [26]). There exists an absolute con-
stant C such that for any matrix A

Rn×d and any set of sampling values pi satisfying

∈

Cτi(A) log dε−2,

pi ≥

if we generate a matrix S with N =
vector, times p−1/2

with probability pi/N , then with probability at least 0.99, for all vector x

n
i=1 pi rows, each chosen independently as the i-th basis
Rd,

i

P

∈

(1

Ax
ε)
k

−

SAx

k2 ≤ k

k2 ≤

Ax
(1 + ε)
k

k2.

Theorem 2.2 (ℓ1 Matrix Concentration Bound, Theorem 7.1 in [27]). There exists an absolute
constant C such that for any matrix A

Rn×d and any set of sampling values pi satisfying

if we generate a matrix S with N =
vector, times p−1

i with probability pi/N , then with probability at least 0.99, for all vectors x

P

∈

Cwi log dε−2,

∈
pi ≥
n
i=1 pi rows, each chosen independently as the i-th basis
Rd,

k1 ≤
−
n
i=1 are the ℓ1 Lewis weights of the matrix A.

k1 ≤ k

Ax
ε)
k

SAx

(1

Here

wi}
{

Ax
(1 + ε)
k

k1.

13

3 Communication Complexity Lower Bound for Linear Systems

3.1 The Hard Instance

In this section, we construct a family of matrices, which will be used to prove a communication

complexity lower bound in the subsequent section.

We ﬁrst introduce generalized binomial distributions.

Deﬁnition 3.1. For any 0
or
same distribution as the sum of t i.i.d. copies of

∈ {−
≤
1 with probability µ/2, and 0 with probability 1

1, let

−

≤

B

µ

(µ)

1, 0, 1
}
µ. Let

(µ)
t
B
(µ). For simplicity we use

be a random variable which takes +1
be a random variable with the
Bt to denote
and

−

B

B

(1) and

B

(1)
t

B

, respectively.

We need the following theorem on the singularity probability of discrete random matrices.

Theorem 3.1. Let Mn ∈
same distribution as

Bt, for suﬃciently large t,

Rn×n be a matrix whose entries are i.i.d. random variables with the

where C > 0 is an absolute constant.

Pr [Mn is singular]

t−Cn,

≤

The proof of Theorem 3.1 closely follows previous approaches for bounding the singularity
1 matrices (see, e.g., [41, 63, 64, 16].). For completeness, we include a

probability of random
proof of Theorem 3.1 in Section 13.

±

Lemma 3.2. For any d > 0 and suﬃciently large t, there exists a set of matrices
with integral entries in [

= tΩ(d2) and

t, t] for which

−

|T |

Rd×(d−1)

T ⊆

1. For any T

, rank(T ) = d

∈ T

1;

2. For any S, T

∈ T

such that S

−
= T , span([S T ]) = Rd.

Proof. We use the probabilisitic method to prove existence. We use Bad
set

Rd×(d−1) to denote the

⊂

Bad =

B
{

Rd×(d−1)

Pr[X

span(B)]

t−Cd/2 or rank(B) < d

≥
Rd is a vector whose entries are i.i.d. random variables with the same distribution as

−

∈

∈

|

,
1
}

∈

where X
Bt and C is the constant in Theorem 3.1.
same distribution as

Consider a random matrix A
Bt, we have

∈

since otherwise, if we use X
the same distribution as

∈
Bt, we have

Rd×(d−1) whose entries are i.i.d. random variables with the

Pr[A

(1)
Rd to denote a vector whose entries are i.i.d. random variables with

≤

Bad]

∈

t−Cd/2,

Pr[rank([A X]) < d]

Pr[rank([A X]) < d

≥
>t−Cd,

A

|

∈

Bad]

·

Pr[A

∈

Bad]

which violates Theorem 3.1.

14

6
For any ﬁxed A

Rd×(d−1)
i.i.d. random variables with the same distribution as

Bad, consider a random matrix B
Bt. We have,

∈

\

Rd×(d−1) whose entries are

∈

Pr[span([A B]) = Rd]

1

−

≥

Pr

d−1

"

\i=1

Bi ∈

span(A)

# ≥

t−Cd(d−1)/2,

1

−

(2)

which follows from the deﬁnition of Bad and the independence of columns of B.

Now we construct a multiset

of

1) and with i.i.d. entries having the same distribution as

= tCd(d−1)/6 matrices chosen with replacement, each of
Bt. By (1) and linearity

S

|S|

dimension d
of expectation, we have

(d

−

×

Bad
]
|

≤

tCd(d−1)/6

t−Cd/2.

·

|S ∩

We use

E[
E1 to denote the even that
Bad

|S ∩

4E[

|S ∩

Bad
]
|

≤

4tCd(d−1)/6

t−Cd/2,

·

| ≤

which holds with probability at least 3/4 by using Markov’s inequality.

We use

E2 to denote the event that
Bad,

S

∀

∈ S \

T

∀

∈ S \ {

Using a union bound and (2), the probability that

S

, span([S T ]) = Rd.
}
E2 holds is at least
t−Ω(d2).

1

− |S|

2t−Cd(d−1)/2 = 1

which implies there exists a set

Thus by a union bound, the probability that both
E1 and
tΩ(d2).
= T , we have

Bad. Since
T
and furthermore for any S, T

E1 holds, we have

such that S

such that

|T | ≥

S \

=

S

∈ T

−
E1 and
E2 hold is strictly larger than zero,
E2 hold simultaneously. Now we consider
are distinct,

E2 implies that all elements in

T

span([S T ]) = Rd.

Lemma 3.3. For any d > 0 and suﬃciently large t, there exists a set of matrices
integral entries in [

= tΩ(d2) and

t, t] for which

−

|H|

Rd×d with

H ⊆

1. For any T

∈ H

, T is non-singular;

= T −1ed, where ed is the d-th standard basis vector.

T

constructed in Lemma 3.2. For each T

, we add [T ei]T into
span(T ). Clearly [T ei]T is non-singular

∈ T

, where ei is the i-th standard basis vector such that ei /
∈

H
since rank(T ) = d

span(T ).

1 and ei /
∈

−
Now suppose there exists S, T
Rd such that Sx = ed and T x = ed. This implies there exists some S′, T ′

such that S−1ed = T −1ed, which means there exists some
x
such that
(S′)T x = 0 and (T ′)T x = 0. However, x must be 0d since span([S′ T ′]) = Rd, which implies
Sx = T x = 0

= ed. Thus for any S, T

= T −1ed.

∈ H

∈ T

∈

, S−1ed 6

∈ H

2. For any S, T

, S−1ed 6
Proof. Consider the matrix set

∈ H

15

6
6
3.2 Deterministic Lower Bound for the Equality Problem

In this section, we prove our deterministic communication complexity lower bound for the
Equality problem in the coordinator model, which will be used as an intermediate problem in
n. The
In the Equality problem, each server Pi receives a binary string ti ∈ {
0, 1
Section 3.3.
}
goal is to test whether t1 = t2 = . . . = ts. We will prove an Ω(sn) lower bound for deterministic
communication protocols.

The case s = 2 has a well-known Ω(n) lower bound.

Lemma 3.4 (See, e.g., [44, p11]). Any deterministic protocol for solving the Equality problem with
s = 2 requires Ω(n) bits of communication.

Our plan is to reduce the case s = 2 to the case s > 2, using the symmetrization technique
for the Equality problem
is CP (n) where n is the length of the binary
.

[54, 69]. Suppose there exists a deterministic communication protocol
with s servers, and the communication complexity of
strings received by the servers. We show how to solve the case s = 2 using

P

P

Suppose Alice receives a binary string x

n−1. We show that by using the protocol
0, 1
{
}
O(CP (n)/s) communication. Thus by Lemma 3.4, we must have

∈ {
P

P
n−1 and Bob receives a binary string y
0, 1
∈
}
, they can judge whether x = y or not using

which implies CP (n) = Ω(sn).

O(CP (n)/s)

Ω(n),

≥

|

P
S
|

= 2n−1, such that for any t

is deterministic, by averaging, there exists a ﬁxed server Pi and a ﬁxed set S

Since
S, when all servers have the same input t, the total
with size
communication complexity beteen Pi and the coordinator is upper bounded by 2CP (n)/s. Now we
n−1. Alice plays the role of server Pi in
, and sets the input of Pi to
ﬁx a bijection g : S
0, 1
P
}
be g(x). Bob plays the role of the coordinator and all servers Pj for i
= j, and sets the input of
, Alice and Bob need to communicate if and
Pj to be g(y) for all i
only if server Pi needs to communicate with the coordinator. If the total amount of communication
between Alice and Bob exceeds 2CP (n)/s then they terminate and return x
= y. Alice and Bob
return x = y if and only if the protocol

= j. To simulate the protocol

returns g(x) = g(y).

→ {

⊆ {

P

∈

n
0, 1
}

∈

Now we analyze the correctness and the eﬃciency of the reduction. When x = y, we have
g(x) = g(y)
S, and by deﬁnition of Pi and S, we must have the total communication complexity
between Pi and the coordinator, and thus that between Alice and Bob, is upper bounded by
= y,
2CP (n)/s. Also the protocol must return g(x) = g(y) due to the correctness of
either the total amount of communication between Alice and Bob exceeds 2CP (n)/s, in which case
they will return x

= g(y) due to its correctness.

= y. Otherwise

returns g(x)

. When x

P

P

Formally, we have proved the following theorem.

P

Theorem 3.5. Any deterministic protocol for solving the Equality problem with s servers in the
coordinator model requires Ω(sn) bits of communication.

3.3 Deterministic Lower Bound for Testing Feasibility of Linear Systems

In this section, we prove our deterministic communication complexity lower bound for testing

the feasibility of linear systems, in the coordinator model and the blackboard model.

Theorem 3.6. For any deterministic protocol

,

P

16

6
6
6
6
6
6
•

•

can test whether Ax = b is feasible or not in the coordinator model, then the communi-

If
cation complexity of

P

is Ω(sd2L);

P

can test whether Ax = b is feasible or not in the blackboard model, then the communication

If
complexity of

P

is Ω(s + d2L);

P

In the hard instance, each
Proof. Consider the set
. The linear system stored on each server is just Hix = ed.
server Pi receives a matrix Hi ∈ H
Due to Lemma 3.3, the entire linear system is feasible if and only if H1 = H2 = . . . = Hs. Since
= 2Ω(d2L), we can reduce the Equality problem in Section 3.2 to solving a linear system, with

constructed in Lemma 3.3 with t = 2L.

|H|
n = Θ(d2L). By Theorem 3.6, this implies an Ω(sd2L) lower bound in the coordinator model.

H

In the blackboard model, the Ω(d2L) bound follows from the case when s = 2. When s = 2,
the blackboard model is essentially the same as the coordinator model, up to constants in the
communication complexity. The Ω(s) lower bound follows from the fact that each server needs to
communicate at least 1 bit.

3.4 Randomized Lower Bound for Solving Linear Systems

In this section, we prove randomized communication complexity lower bounds for solving linear
systems. We ﬁrst prove an Ω(d2L) lower bound, which already holds for the case s = 2. When
s = 2 the coordinator model and the blackboard model are equivalent in terms of communication
complexity, and thus we shall not distinguish these two models in the remaining part of this proof.
constructed in Lemma 3.3 with t = 2L. In the hard instance, only server P1
Consider the set
H
, and the goal is to let the coordinator output the solution to the linear
receives a matrix H
∈ H
2 ed. Thus,
and H1 6
system Hx = ed. For any two H1, H2 ∈ H
by standard information-theoretic arguments, in order for the coordinator to output the solution
to Hx = ed, the communication complexity is at least Ω(log(

= H2, we must have H −1

)) = Ω(d2L).

1 ed 6

= H −1

Formally, we have proved the following theorem.

|H|

Theorem 3.7. Any randomized protocol that succeeds with probability at least 0.99 for solving
linear systems requires Ω(d2L) bits of communication in the coordinator model and the blackboard
model. The lower bound holds even when s = 2.

Now we prove another lower bound of

In the hard instance, the last server Ps receives a vector
e
on server Ps are simply x =
vector to be some predeﬁned binary vector
ai ∈ {

d, and the linear equation stored on Pi is
0, 1
}

b

b

x, i.e., the solution vector x should be exactly

Ω(sd) for solving linear systems in the coordinator model.
d, and the linear equations stored
0, 1
}
x. This forces the solution
1 servers each receive a vector

x. The remaining s

∈ {

x

−

b

Also, it is guaranteed that for each i
Here we interpret the vector

[d], and interpret each
vector ai also as the characteristic vector a set Sai. Thus, testing the feasibility of the linear system
is equivalent to testing whether the set Sbx owned by the server Ps is disjoint with the set owned
1 copies of the two-player set-disjointness problem.
by any other player, which is the OR of s

= 0 or 1.
x as the characteristic vector of a set Sbx ⊆

[s],

∈

x

b

b

i

ajxj = 1.

b
d

Xj=1
ai,
h

−

17

The communication complexity for the latter problem has been studied in [54, 68]. Combining
Lemma 2.2 in [54] with Theorem 1 in [68], for any communication protocol that succeeds with
1/s3, the communication complexity is lower bounded by Ω(sd). By standard
probability 1
repetition arguments, this implies for any randomized communication protocol that succeeds with
probability at least 0.99, the communication complexity is lower bounded by Ω(sd/ log s).

−

Combining this lower bound and the trivial Ω(s) lower bound in the blackboard model with

Theorem 3.7, we have the following theorem.

Theorem 3.8. Any randomized protocol that succeeds with probability at least 0.99 for solving
Ω(sd + d2L) bits of communication in the coordinator model and Ω(s + d2L)
linear systems requires
bits of communication in the blackboard model.
e

4 Communication Protocols for Linear Systems

4.1 Testing Feasibility of Linear Systems

In this section we present a randomized communication protocol for testing feasibility of linear
systems, which has communication complexity O(sd2 log(dL)) in the coordinator model and O(s +
d2 log(dL)) in the blackboard model. The protocol is described in Figure 1.

1. The coordinator generates a random prime number p in [2, poly(dL)] and sends p to all

servers.

2. Each server Pi tests the feasibility of its own linear system. If the linear system is infeasible

then Pi terminates the protocol.

3. Each server maintains the same set of linear equations C. At the beginning C is the

empty set.

4. For i = 1, 2, . . . , s

(a) Pi checks whether the linear system formed by all linear equations in C and all linear
equations stored on Pi is feasible over the ﬁnite ﬁeld Fp. Pi terminates the protocol
if it is infeasible.

(b) For each linear equation stored on Pi that is linearly independent with linear equations
in C over the ﬁnite ﬁeld Fp, Pi sends that linear equation to all servers, after taking
the residual of each entry modulo p. All servers add that linear equation into C.

Figure 1: Randomized protocol for testing feasibility

We ﬁrst bound the communication complexity of the protocol in Figure 1. Clearly, Step 1
has communication complexity at most O(s log(dL)). During the execution of the whole protocol,
at most d linear equations will be added into C. The communication complexity for sending
each linear equation is O(sdL log p) in the coordinator model, and O(dL log p) in the blackboard

18

model. Thus, the total communication complexity is O(sd2 log(dL)) in the coordinator model, and
O(s + d2 log(dL)) in the blackboard model.

To prove the correctness of this protocol, we need the following lemma.

Lemma 4.1. Given a matrix A
r. Suppose p is chose uniformly at random from all primes numbers in [2, poly(rL)].

Rm×n where each entry is an integer in [

−

∈

2L, 2L] and rank(A) =

(i) rankp(A)

rank(A);

≤

(ii) With probability at least 0.99, rankp(A) = rank(A).

×

Proof. The point (i) is immediate. For point (ii), there exists a square submatrix A′ of A with
r which is non-singular over real numbers, which implies the determinant of A′ is non-zero
size r
as a real number. Since all entries of A′ are integers in [
2L, 2L], the determinant of A′ as a real
r!2rL, r!2rL]. Thus, the determinant of A′ has at most poly(rL) prime
number is an integer in [
factors. According to the Prime Number Theorem, there are at least n distinct prime numbers in
the range [2, n2], for suﬃciently large n. Thus, by adjusting constants, p is not a prime factor of
the determinant of A′ with probability at least 0.99, in which case rank(A) = rankp(A).

−

−

−

Notice that the protocol in Figure 1 is basically testing the feasibility of the linear system over
the ﬁnite ﬁeld Fp, for a randomly chosen prime number p. Before the execution of the i-th loop of
Step 4, the set C is a maximal set of linearly independent equations for all linear equations stored
on the ﬁrst i
1 servers P1, P2, . . . , Pi−1. Here the linear independence is deﬁned over the ﬁnite ﬁeld
Fp. During the execution of the i-th loop of Step 4, server Pi considers each linear equation stored
on itself one by one, sends the linear equation to all other servers and adds the linear equation
to set C if that linear equation is linearly independent with all existing linear equations in C. If
server Pi ﬁnds that the set C becomes infeasible after adding linear equations stored on Pi, then
Pi terminates the protocol.

Rn×d and b

Consider a linear system Ax = b where A

Rn and all entries of A and b are
∈
2L, 2L]. If Ax = b is feasible over the real numbers, then it will also be
integers in the range [
feasible over the ﬁnite ﬁeld Fp. If Ax = b is infeasible, then we have rank([A b]) > rank(A). By
d + 1, with probability at least 0.99,
Lemma 4.1, rankp(A)
rankp([A b]) = rank([A b]), which implies with probability 0.99, Ax = b is still infeasible over the
ﬁnite ﬁeld Fp. Since the protocol in Figure 1 tests the feasibility of the linear system over the ﬁnite
ﬁeld Fp, the correctness follows.

rank(A), and since rank([A b])

≤

≤

−

∈

Formally, we have proved the following theorem.

Theorem 4.2. The protocol in Figure 1 is a randomized protocol for testing feasibility of linear
systems and has communication complexity O(sd2 log(dL)) in the coordinator model and O(s +
d2 log(dL)) in the blackboard model. The protocol succeeds with probability at least 0.99.

4.2 Solving Linear Systems

In this section we present communication protocols for solving linear systems. We start with de-
terministic protocols, in which case we can get a protocol with communication complexity O(sd2L)
in the coordinator model and O(s + d2L) in the blackboard model.

In order to solve linear systems, we can still use the protocol in Figure 1, but we don’t use the
prime number p any more. In Step 4a of the protocol, we no longer check the feasibility over the

19

ﬁnite ﬁeld. In Step 4b of the protocol, we no longer takes the residual modulo p before sending
the linear equations. At the end of the protocol, each server can use the set of linear equations
C, which is a maximal set of linear equations of the original linear system, to solve the linear
system. The communication complexity is O(sd2L) in the coordinator model and O(s + d2L) in
the blackboard model since at most d linear equations will be added into the set C, and each linear
equation requires O(dL) bits to describe.

Formally, we have proved the following theorem.

Theorem 4.3. There exists a deterministic protocol for solving linear systems which has commu-
nication complexity O(sd2L) in the coordinator model and O(s + d2L) in the blackboard model.

Now we turn to randomized protocols. We describe a protocol for solving linear systems with
O(d2L+sd) in the coordinator model. The description is given in Figure

communication complexity
2.

e

1. Each server Pi tests the feasibility of their own linear system.

If the linear system is
infeasible then Pi terminates the protocol. Otherwise, each server Pi ﬁnds a maximal set
of linearly independent linear equations, say Si.

2. The coordinator maintains a set of linear equations C. At the beginning C is the empty

set.

3. For i = 1, 2, . . . , s

(a) Repeat the followings for O(log d) times

i. Server Pi calculates a linear equation c =

i.i.d. random signs. Pi sends the linear equation c to the coordinator.

P
ii. The coordinator terminates the protocol if C

is infeasible. Otherwise if c
is not a linear combination of those linear equations in C, then the coordinator
adds c into C, and then goes to Step 3a, i.e., repeats another O(log d) times.

c
}
∪ {

t∈Si

rt ·

t, here

rt}t∈Si is a set of
{

4. The coordinator obtains the solution by solving all equations in C.

Figure 2: Randomized protocol for solving linear systems in the coordinator model

Now we prove the correctness of the protocol. We ﬁrst note a few simple properties of the
.
[s], after executing the i-th loop of Step 3, we have C
⊆
is infeasible, then
c
(cid:17)
(cid:16)S
}
∪ {

span(Si). This means if C

protocol. For each i
∈
Furthermore, at Step 3(a)ii, we must have c
the original linear system must be infeasible.
Thus, it suﬃces to show that for each i

}
is infeasible, then the protocol is terminated, and otherwise after executing the i-th loop of Step 3,
we have span(C) = span

[s], if there exists s

Si such that

s
∪ {

j<i Sj

j≤i Sj

span

(cid:16)S

∈

∈

∈

(cid:17)

.

j≤i Sj

Suppose before the execution of the i-th loop of Step 3 we have span(C) = span

(cid:16)S

(cid:17)

j<i Sj

and

(cid:16)S

(cid:17)

20

j<i Sj

span
cases here.
(cid:16)S
Case 1: span

(cid:17)

is feasible, and the protocol is executing the i-th loop of Step 3. There are two

(cid:16)S
Si, s /
∈

= span
j≤i Sj
ing the i-th loop of Step 3.

(cid:17)

(cid:16)S

= span(C). In this case, C will remain unchanged dur-

j<i Sj

(cid:17)

Case 2: There exists s

1/2, the linear equation c calculated at Step 3(a)i satisﬁes c /
∈
since for any linear combination c =
either c /
∈

2s and s /
∈

rt ·
c =
−

span(C), since c

span(C) or

span(C). In this case, we claim that with probability at least
span(C). This can be seen
c, then

t, if we ﬂip the sign of rs and obtain
span(C).

c /
∈

t∈Si

P

±

∈

Thus, if there exists s
of the linear equations c calculated at Step 3(a)i satisﬁes c /
∈
terminates if C

Si such that s /
∈

∈

b

b

span(C), then with probability 1

1/ poly(d), at least one
span(C), in which case the protocol
=

−

j<i Sj

is infeasible, or c is added into C otherwise. Thus, if span

b

span

j≤i Sj

, then after the execution of the i-th loop of Step 3, with probability at least

(cid:16)S

(cid:17)

c
}
∪ {

(cid:17)

(cid:17)

(cid:17)

−

(cid:16)S

j<i Sj

j≤i Sj

j≤i Sj

= span

(cid:16)S
1/ poly(d), either the protocol (correctly) terminates, or we have span(C) = span

.
1
The correctness of the protocol just follows by applying a union bound over all i such that
(cid:17)
. Notice that there are at most d such i we need to apply a
span
union bound over.

(cid:16)S
Now we analyze the communication complexity of the protocol. Notice that at most d linear
equations will be added into C, and thus the total communication complexity associated with
sending c when c is added into C is upper bounded by O(d2L). Furthermore, if C
is infeasible
at Step 3(a)ii, then the protocol terminates and thus the communication complexity for sending
c associated such that case is upper bounded by O(dL). Furthermore, for each i, server Pi will
send O(log d) diﬀerent linear equations c to the coordinator, and if we implement the protocol
O(sdL). Thus, the total
na¨ıvely, then the total communication complexity is upper bounded by
communication complexity of the whole protocol is upper bounded by

O(sdL + d2L).

c
}
∪ {

(cid:16)S

However, using Lemma 4.1 and the same argument as in Section 4.1, to implement Step 3(a)ii,
it suﬃces to check if C
c
is feasible and if c is a linear combination of existing linear equations in
}
∪ {
C, over the ﬁnite ﬁeld Fp, for a random prime number p
[2, poly(dL)]. The correctness still follows
since this check fails with probability at most 0.01. After this modiﬁcation, the communication
complexity is now upper bounded by

∈

e

e

O(sd + d2L).
Formally, we have proved the following theorem.

Theorem 4.4. The protocol described in Figure 2 is a randomized protocol for solving linear systems
which has communication complexity
) notation
O(
·
hides only polylog(dL) factors. The protocol succeeds with probability at least 0.99.

O(sd + d2L) in the coordinator model. Here the

e

5 Communication Complexity Lower Bounds for Linear Regres-

sions in the Blackboard Model

e

e

In this section, we prove communication complexity lower bounds for linear regression in the

blackboard model.

We ﬁrst deﬁne the k-XOR problem and the k-MAJ problem. In the blackboard model, each
d. In the k-XOR problem, at the end of a communication
0, 1
}

server Pi receives a binary string xi ∈ {

21

6
6
protocol, the coordinator correctly outputs the coordinate-wise XOR of these vectors, for at least
0.99d coordinates. In the k-MAJ problem, at the end of a communication protocol, the coordinator
correctly outputs the coordinate-wise majority of these vectors, for at least 0.99d coordinates.

We need the following lemma for our lower bound proof.

Lemma 5.1. Any randomized communication protocol that solves the k-XOR problem or the k-
MAJ problem and succeeds with probability at least 0.99 has communication complexity Ω(dk).

Proof. The lower bound for k-XOR directly follows from [54, Theorem 1.1]. Now we prove the
lower bound for k-MAJ.

−

1 binary strings z1, z2, . . . , z2k−1 ∈ {

First, consider a communication problem with two players. Alice receives a binary string x
d. Bob receives a binary string y
0, 1
}

∈
d and 2k
0, 1
∈
}
{
d. These 2k + 1 binary strings
d and the same 2k
0, 1
0, 1
{
}
}
x, y, z1, z2, . . . , z2k−1 are generated uniformly at random conditioned on the following constraint:
[d], the i-th coordinate of x, y, z1, z2, . . . , z2k−1 contains either k zeros or
for each coordinate i
k +1 zeros. For each coordiante i
[d], whether the i-th coordinate of x, y, z1, z2, . . . , z2k−1 contains
k zeros or k + 1 zeros is also chosen uniformly at random. In this communication problem, the goal
of Alice is to output the vector y.

1 binary strings z1, z2, . . . , z2k−1 ∈ {

−

∈

∈

∈

Now we prove a lower bound the communication problem deﬁned above. Notice that for each
[d], if x, z1, z2, . . . , z2k−1 contains exactly k zeros and k ones at the i-th coordinate,
coordinate i
then the i-th coordinate of y will be uniformly at random. By a Chernoﬀ bound, with high
probability, there exists a set S
S, the i-th
S
|
coordinate of x, z1, z2, . . . , z2k−1 contains exactly k zeros and k ones. By standard information-
theoretic arguments, if at the end of the communication protocol, with constant probability, Alice
correctly outputs the value of yi for at least 9/10 fraction of i
S, then the expected communication
complexity is lower bounded by Ω(d), even with public randomness. See, e.g., Lemma 2.1 in [54]
for a formal proof.

d/10, such that for each i

[d] with size

| ≥

⊆

∈

∈

P

P

P

. To simulate

Now we reduce the problem mentioned above to (2k + 1)-MAJ and prove an Ω(dk) lower bound.
Given any protocol
for the (2k+1)-MAJ problem with expected communication complexity CP on
the distribution x, y, z1, z2, . . . , z2k−1 mentioned above, Alice and Bob ﬁrst use public randomness
to choose two distinct servers s1 and s2 uniformly at random, and then Alice and Bob simulate the
protocol
, Alice plays the role of server s1 and Bob plays the role of server s2.
They both play the roles of all other players. Alice sets the input of s1 to be x, and Bob sets the
1 servers are set to be z1, z2, . . . , z2k−1. To simulate
input of s2 to be y. The inputs of the other 2k
, Alice and Bob need to communicate if and only if server s1 or server s2 needs to communicate
P
with the coordinator since all other communication can be simulated by Alice and Bob themselves.
By symmetry, the expected communication complexity between Alice and Bob is upper bounded
by 2CP /k. Furthermore, at the end of the protocol, Alice and Bob have the coordinate-wise majority
S,
of x, y, z1, z2, . . . , z2k−1, for at least 0.99d coordinates. Thus, for at least 9/10 fraction of i
Alice knows the majority of the i-th coordinates of x, y, z1, z2, . . . , z2k−1. However, by deﬁnition of
S, the majority of the i-th coordinates of x, y, z1, z2, . . . , z2k−1 is exactly yi. Thus, by the Ω(d) lower
bound mentioned above, we must have 2CP /k = Ω(d), which implies an Ω(dk) lower bound.

−

∈

Now we give a reduction from k-MAJ to (1 + ε)-approximate ℓ1 regression in the blackboard
model, and prove an Ω(d/ε) lower bound when s > Ω(1/ε). In the hard case we assume s = Θ(1/ε),
and we simply ignore all other servers if s > Θ(1/ε). For each server Pi, its matrix A(i) is set to
d. Notice that in such case, we can calculate
0, 1
be the identity matrix Id ∈
}

Rd×d, and b(i)

∈ {

22

the ℓ1 regression value separately for each coordinate xj. The optimal solution can be achieved by
taking xj to be mj, where mj is the majority of b(1)
. Notice that the ℓ1 regression
j
value associated with the j-th coordinate in the optimal solution is upper bounded by s = Θ(1/ε),
and thus the total ℓ1 regression value is upper bounded by sd = Θ(d/ε) in the optimal solution.
Furthermore, if
0.1, then the ℓ1 regression value associated with the j-th coordinate by
using xj will be at least 0.1 larger than the ℓ1 regression value associated with the j-th coordinate
in the optimal solution.

, . . . , b(s)
j

mj| ≥

xj −
|

, b(2)
j

xi −
|

Now consider a (1 + ε)-approximate solution x. We claim that for at least 0.99d coordinates
xi of x, we have
0.1. The claim follows since otherwise, the total ℓ1 regression value
of x would be at least 0.099d larger than the optimal ℓ1 regression value, which would again be
larger than 1 + ε times the optimal ℓ1 regression value, by adjusting the constant in s = Θ(1/ε).
Thus, from a (1 + ε)-approximate solution x to the ℓ1 regression problem, we can solve the k-MAJ
problem with k = Θ(1/ε), which implies an Ω(d/ε) lower bound.

mi| ≤

Formally, we have proved the following theorem.

Theorem 5.2. When s > Ω(1/ε), any randomized protocol that succeeds with probability at least
0.99 for solving (1 + ε)-approximate ℓ1 regression requires Ω(d/ε) bits of communication in the
blackboard model.

Now we give a reduction from k-XOR to (1 + ε)-approximate ℓ2 regression in the blackboard
model, and prove an Ω(d/√ε) lower bound when s > Ω(1/√ε).
In the hard case we assume
s = Θ(1/√ε), and we simply ignore all other servers if s > Θ(1/√ε). For each server Pi, its matrix
Rd×d, and b(i)
d. For ℓ2 regression, the optimal
A(i) is set to be the identity matrix Id ∈
0, 1
}
solution can be achieved by taking xj to be aj, where aj is the average of b(1)
. Notice
j
that the squared ℓ2 regression value associated with the j-th coordinate in the optimal solution is
upper bounded by s = Θ(1/√ε), and thus the total squared ℓ2 regression value is upper bounded by
Ω(√ε), then the squared ℓ2 regression value associated
sd = Θ(d/√ε). Furthermore, if
2) = Θ(√ε) larger than the
with the j-th coordinate by using xj will be at least Θ(s
squared ℓ2 regression value associated with the j-th coordinate in the optimal solution.

, . . . , b(s)
j

xj −
· |

aj| ≥

xj −
|

, b(2)
j

aj|

∈ {

xi −
|

ai| ≤

Now consider a (1 + ε)-approximate solution x. We claim that for at least 0.99d coordinates xi
O(√ε). The claim follows since otherwise, the total squared ℓ2 regression
of x, we have
value of x would be at least Ω(d√ε) larger than the optimal squared ℓ2 regression value, which
would again be larger than 1 + ε times the optimal squared ℓ2 regression value, by adjusting the
O(√ε), we can exactly
constant in s = Θ(1/√ε). Notice that for those coordinates with
xi −
|
recover ai from xi, since ai is the average of b(1)
and thus ai is an integer multiple
of 1/s = Θ(√ε). This also implies we can recover the XOR of b(1)
. Thus, from a
(1 + ε)-approximate solution x to the ℓ2 regression problem, we can solve the k-XOR problem with
k = Θ(1/√ε), which implies an Ω(d/√ε) lower bound.
Formally, we have proved the following theorem.

ai| ≤
, b(2)
i

, . . . , b(s)

, . . . , b(s)

, b(2)
i

i

i

i

i

Theorem 5.3. When s > Ω(1/√ε), any randomized protocol that succeeds with probability at least
0.99 for solving (1 + ε)-approximate ℓ2 regression requires Ω(d/√ε) bits of communication in the
blackboard model.

6 Communication Protocols for ℓ2 Regression

In this section, we design distributed protocols for solving the ℓ2 regression problem.

23

6.1 A Deterministic Protocol

In this section, we design a simple deterministic protocol for ℓ2 regression in the distributed

setting with communication complexity

O(sd2L) in the coordinator model.

According to the normal equations, the optimal solution to the ℓ2 regression problem minx k
Ax
−
k2 can be attained by setting x∗ = (AT A)†AT b. In Figure 3, we show how to calculate AT A and
b
AT b in the distributed model.

e

1. Each server Pi calculates (A(i))T A(i) and (A(i))T b(i), and then sends them to the coordi-

nator.

2. The coordinator calculates AT A =

calculates x = (AT A)†

AT b.

·

P

s
i=1(A(i))T A(i) and AT b =

s
i=1(A(i))T b(i), and then

P

Figure 3: Protocol for ℓ2 regression in the coordinator model

Notice that the bit complexity of entries in AT A and AT b is O(L+log n) since the bit complexity
of entries in A and b is L, which implies the communication complexity of the protocol in Figure 3
is O(sd2(L + log n)), in both the coordinator model and the blackboard model.

Theorem 6.1. The protocol in Figure 3 is a deterministic protocol which exactly solves ℓ2 regres-
sion, and the communication complexity is O(sd2(L + log n)), in both the coordinator model and
the blackboard model.

6.2 A Protocol in the Blackboard Model

In this section, we design a recursive protocol for obtaining constant approximations to leverage
scores in the distributed setting, which is described in Figure 4. We then show how to solve ℓ2
regression by using this protocol.

The protocol described in Figure 4 is basically Algorithm 2 in [26] for approximating leverage
scores, implemented in the distributed setting. Using Lemma 8 in [26], the protocol returns an
O(d log d)

A such that

d matrix

×

e

Ax
Ω(1)
k

Ax

k2 ≤ k

k2 ≤

Ax
O(1)
k

k2

for all x
to leverage scores of all rows in A(i) by calculating τ

Rd, with constant probability. Each server Pi can then obtain constant approximations
e
A
i (A).

∈

e

Now we analyze the communication complexity of the protocol. Notice that this recursive
algorithm has O(log(n/d)) levels of recursion. Step 1 has communication complexity O(sd2L log d)
in the coordinator model and O(d2L log d) in the blackboard model, and will be executed at most
once during the whole protocol. At Step 4, we can assume each p−1/2
is a power of two between 1
and poly(n), since we can discard all rows whose pi < 1/ poly(n) and increase each pi by a constant
factor. In order to implement the sampling process in Theorem 2.1 in the distributed setting, each
server Pi sends the summation of pi for all rows in A(i) to the coordinator. After receiving all
these summations, the coordinator decides the number of rows to be sampled from each A(i) and

i

24

Input: An n

×



, where A(i) is stored on server Pi.

A(1)
A(2)
...
A(s)






d matrix A = 




d matrix

×

Output: An O(d log d)

A such that

k2
Rd, which is stored on the coordinator and all servers.

k2 ≤ k

k2 ≤

Ax
O(1)
k

Ax
Ω(1)
e
k

Ax

e

for all x

∈
1. If n

≤

O(d log d), then each server Pi sends A(i) to the coordinator, and then the coordi-

nator sends A to each server, and returns.

2. Each server Pi locally uniformly samples half of the rows from A(i) to form

each server takes

A(i)

′

as input and invokes the protocol recursively to compute
(cid:1)

(cid:0)

(cid:0)

(cid:1)

′

′

A(1)
A(2)
...
A(s)



(cid:0)
(cid:0)

(cid:1)
(cid:1)
′

A′x
such that Ω(1)
k

A′ = 





A′, each server Pi calculates generalized leverage scores (Deﬁnition 2.1) of all rows

k2 for all x

A′x
O(1)
k

k2 ≤ k

k2 ≤







Rd.

A′x

f

∈

(cid:0)

(cid:1)

3. Using

′

A(i)

. Then
A′ for

f

4. The coordinator and all servers obtain

A by sampling and rescaling O(d log d) rows using

in A(i) with respect to

A′.

f

f

Theorem 2.1, by setting pi ≥
τ

C

τi log d, where

e

if row Ai is sampled in Step 2,
otherwise.

e
A
i (A)
e
1

1+ 1
e
A
τ
i

(A)

τi =

e






Figure 4: Protocol for approximating leverage scores in the distributed setting

sends these numbers back to each server. The communication complexity of this step is at most
O(s log n). Each server Pi samples and rescales the rows accordingly, and then sends these sampled
rows to the coordinator, and the coordinator sends all sampled rows back to all servers. Notice that
the bit complexity of all entries in the sampled rows is at most O(L + log n) since p−1/2
is an integer
between 1 and poly(n). Thus, in the blackboard model, the total communication complexity at each
recursive level of the protocol is upper bounded by O(d2 log d(L + log n)+ s log n), which implies the
communication complexity of the whole protocol is at most O((d2 log d(L+log n)+s log n)
log(n/d))
in the blackboard model. A similar analysis shows that the communication complexity of the whole
protocol is

O(sd2L) in the coordinator model.

·

i

e

25

Lemma 6.2. The protocol described in Figure 4 is a randomized protocol with communication
O(s+d2L) in the blackboard model, such that with
complexity
constant probability, upon termination of the protocol, each server Pi has constant approximations
to leverage scores of all rows in A(i).
e

O(sd2L) in the coordinator model and

e

Our protocol for solving the ℓ2 regression problem in the blackboard model is described in
Figure 5. It is guaranteed that the vector x calculated at Step 3 is a (1 + ε)-approximate solution,
with constant probability. A na¨ıve approach for obtaining (1 + ε)-approximate solution to the
ℓ2 regression problem will be using Theorem 2.1 to obtain SA and Sb such that with constant
probability, for all x

Rd,

∈

(1

Ax
ε)
k

b

k2 ≤ k

S(Ax

b)

k2 ≤

Ax
(1 + ε)
k

b

k2.

−

−

−
By doing so, the number of sampled rows should be O(d log d/ε2) according to Theorem 2.1. How-
ever, as shown in Theorem 36 of [24], in order to obtain a (1 + ε)-approximate solution to the
ℓ2 regression problem (instead of obtaining a (1 + ε) subspace embedding), it suﬃces to sample
O(d log d + d/ε) rows from [A b].

−

Now we analyze the communication complexity of the protocol in Figure 5 in the blackboard
O(d2L +
model. By Lemma 6.2, the communication complexity of Step 1 is upper bounded by
s). Similar to Step 4 of the protocol described in Figure 4, the sampling process in Step 2 can
O(s + d2L/ε). Thus, the total communication
be implemented with communication complexity
complexity is

e

O(s + d2L/ε) in the blackboard model.
e

e

1. Use the protocol in Figure 4 to approximate leverage scores of A.

2. The coordinator obtains SA and Sb by sampling and rescaling O(d/ε + d log d) rows of

[A b], using the sampling process in Theorem 2.1.

3. The coordinator calculates x = minx k

SAx

Sb

k2.

−

Figure 5: Protocol for ℓ2 regression in the blackboard model

Theorem 6.3. The protocol described in Figure 5 is a randomized protocol which returns a (1 + ε)-
approximate solution to ℓ2 regression with constant probability, and the communication complexity
is

O(s + d2L/ε) in the blackboard model .

e

7 Communication Protocols for ℓ1 Regression

In this section, we design distributed protocols for solving the ℓ1 regression problem.

7.1 A Simple Protocol

In this section, we design a simple protocol for obtaining a (1 + ε)-approximate solution to the

ℓ1 regression problem in the distributed setting. The protocol is described in Figure 6.

26

Rd,

∈
k1,

×
S(i)A(i)x

1. Each server Pi calculates an O(d log d/ε2)

n matrix S(i) such that for all x

(1

A(i)x
ε)
k

−

−

b(i)

k1 ≤ k

S(i)b(i)

k1 ≤

A(i)x
(1 + ε)
k

−

b(i)

−

and then sends S(i)A(i) and S(i)b(i) to the coordinator.

S(1)A(1)
S(2)A(2)
...
S(s)A(s)

S(1)b(1)
S(2)b(2)
...
S(s)b(s)



,






b = 





e

2. Let

A = 





e


. The coordinator solves minx k





Ax

e

−

b

k1.
e

Figure 6: Protocol for ℓ1 regression in the coordinator model

∈
k1.

To implement Step 1, each server Pi calculates the ℓ1 Lewis weights of [A(i) b(i)] and uses

Theorem 2.2 to randomly generate a matrix S(i). Pi then checks whether for all x

Rd

−

−

−

(1

b(i)

k1 ≤

S(i)b(i)

S(i)A(i)x

A(i)x
ε)
k

A(i)x
(1 + ε)
k1 ≤ k
k
Rd. Since (3) is satisﬁed
If not, Pi randomly generates another S(i) until (3) is satisﬁed for all x
with constant probability, the number of independent trials is O(1) in expectation. Furthermore,
each server can locally check whether (3) holds or not by e.g., verifying on an ε-net. Notice that
the use of randomness is not critical here, since each server Pi can locally enumerate all possible
S(i) up to a speciﬁc precision, instead of using Theorem 2.2 to randomly generate a matrix S(i).
Each server Pi will eventually ﬁnd a matrix S(i) which satisﬁes (3), whose existence is guaranteed
by Theorem 2.2.

b(i)

(3)

−

∈

Now we prove the correctness of the protocol. Notice that it is guaranteed that for any x

and any i

[s],

∈

∈

(1

A(i)x
ε)
k

−

−

b(i)

S(i)A(i)x

k1 ≤ k

S(i)b(i)

k1 ≤

A(i)x
(1 + ε)
k

−

b(i)

k1.

−

It implies that

Ax
k

−

e

and

s

Xi=1

b

k1 =
e

S(i)A(i)x
k

−

S(i)b(i)

k1 ≥

s

(1

Xi=1

A(i)x
ε)
k

−

−

b(i)

k1 = (1

Ax
ε)
k

−

b

k1

−

Rd

Ax
k

−

e

b

k1 =
e

s

Xi=1

S(i)A(i)x
k

−

S(i)b(i)

k1 ≤

(1 + ε)

s

Xi=1

A(i)x
k

−

b(i)

Ax
k1 = (1 + ε)
k

b

k1.

−

Thus, the vector x calculated at Step 2 is a (1 + ε)-approximate solution to the ℓ1 regression
problem.

Finally, we analyze the communication complexity of the protocol. Similar to the analysis in
Section 6.2, we may assume all 1/pi in the sampling process of Theorem 2.2 are integers between 1

27

and poly(n). Thus, the bit complexity of all entries in S(i)A(i) and S(i)b(i) is at most O(L + log n),
which implies the communication complexity of Step 1 is O(sd2 log d
(L + log n)/ε2) in both the
coordinator model and the blackboard model.

·

Theorem 7.1. The protocol described in Figure 6 is a deterministic protocol which returns a (1+ε)-
O(sd2L/ε2)
approximate solution to the ℓ1 regression problem, and the communication complexity is
in both the coordinator model and the blackboard model.

e

7.2 A Protocol Based on ℓ1 Lewis Weights Sampling

In this section, we ﬁrst design a protocol for obtaining constant approximations to ℓ1 Lewis
weights in the distributed setting, which is described in Figure 7, and then solves the ℓ1 regression
problem based on this protocol.

1. Each server Pi initializes wi = 1 for all rows in A(i).

2. For t = 1, 2, . . . , T

(a) Each server Pi obtains constant approximations to leverage scores of W −1/2A, for all
rows stored on Pi using the protocol in Lemma 6.2, where W is the diagonal matrix
formed by putting the elements of w on the diagonal.

(b) Set wi = (wiτi(W −1/2A))1/2.

Figure 7: Protocol for approximating ℓ1 Lewis weights

The protocol described in Figure 7 is basically the algorithm in Section 3 of [27] for approximat-
ing ℓ1 Lewis weights, implemented in the distributed setting. Using the same analysis, by setting
T = O(log log n), we can show wi are constant approximations to the ℓ1 Lewis weights of A. Now
we show that we can assume all w−1/2

are integers between 1 and 2

e
O(L).

i

Without loss of generality we assume each row of A contains at least one non-zero entry. Since
our goal here is to calculate constant approximations to the ℓ1 Lewis weights, using the analysis in
Section 3 in [27], we only need constant approximations to wi during the execution of the algorithm.
Furthermore, since leverage scores τi(W −1/2A) are at most 1 (see, e.g., Section 2.4 in [66]), we can
prove by induction that wi ≤
1 during the execution of the algorithm. Thus, we may assume
1 and w−1/2
w−1/2
i
Now we show that wi ≥
≤

are integers.
e
O(L). We prove this claim by induction. At the beginning of the
e
O(L) by the induction hypothesis, we know all
algorithm, wi = 1 for all 1
n. Assume wi ≥
entries in W −1/2A are integers between 1 and 2

eO(L). Using Lemma 2 in [26], we know

2−
i

2−

≥

≤

i

τi(W −1/2A) =

min
(W −1/2A)T x=(W −1/2A)i k

x

2
2.
k

By the Cauchy-Schwarz inequality, in order that (W −1/2A)T x = (W −1/2A)i, we must have

2− eO(L)/√n

x
k

k2 ≥

28

since otherwise all entries in (W −1/2A)T x are less than 1, which violates the assumption that all
entries in W −1/2A are integers and each row of A contains at least one non-zero entry. Furthermore,
the number of iterations is at most O(log log n), which implies wi ≥
Thus, all entries in W −1/2A have bit complexity
Using Lemma 6.2, the communication complexity is
O(sd2L) in the coordinator model.

O(L) during the execution of the algorithm.
O(s + d2L) in the blackboard model, and
e

e
O(L) by induction.

2−

Lemma 7.2. The protocol described in Figure 7 is a randomized protocol with communication
e
O(sd2L) in the blackboard model, such that with
complexity
constant probability, upon termination of the protocol, each server Pi has constant approximations
to the ℓ1 Lewis weights of all rows in A(i).

O(s + d2L) in the blackboard model and

e

e

e

Our protocol for solving the ℓ1 regression problem in the blackboard model is described in Figure
8. By Theorem 2.2, it is guaranteed that the vector x calculated at Step 3 is a (1 + ε)-approximate
solution, with constant probability.

Now we analyze the communication complexity of the protocol in Figure 8. By Lemma 7.2,
O(d2L + s) in the blackboard model
O(sd2) in the coordinator model. Similar to Step 4 of the protocol described in Figure 4, the
O(s + d2L/ε2) in
O(s + d2L/ε2) in the blackboard model.

the communication complexity of Step 1 is upper bounded by
and
sampling process in Step 2 can be implemented with communication complexity
both models. Thus, the total communication complexity is
In the coordinator model, the total communication complexity is

O(sd2L + d2L/ε2).

e

e

e

1. Use the protocol in Figure 7 to approximate ℓ1 Lewis weights of [A b].

e

e

2. The coordinator obtains SA and Sb by sampling and rescaling O(d log d/ε2) rows of [A b],

using the sampling process in Theorem 2.2.

3. The coordinator calculates x = minx k

SAx

Sb

k1.

−

Figure 8: Protocol for ℓ1 regression in the blackboard model

Theorem 7.3. The protocol described in Figure 8 is a randomized protocol which returns a (1 + ε)-
approximate solution to the ℓ1 regression problem with constant probability, and the communication
O(sd2L + d2L/ε2) in the coordinator model.
complexity is

O(s + d2L/ε) in the blackboard model and

7.3 A Protocol Based on Accelerated Gradient Descent

e

e

In this section, we present a protocol for the ℓ1 regression problem in the coordinator model,

whose communication complexity is

O(sd3L/ε).

We need the following deﬁnition in [32].

Deﬁnition 7.1 ([32]). Suppose n
if the following hold:

≥

e
d. A matrix A

∈

1. AT A

≈O(1) I;

29

Rn×d is approximately isotropic row-bounded

2. For all rows of A,

Ai
k

2
2 ≤
k

O(d/n).

Before presenting the protocol, we ﬁrst present a preconditioning procedure in Figure 9, which
will later be used in the protocol for ℓ1 regression. The communication complexity of the this

1. Use the protocol in Figure 7 to approximate ℓ1 Lewis weights of [A b].

2. Obtain SA and Sb by sampling and rescaling O(d log n/ε2) rows of [A b], using the
sampling process in Theorem 2.2. Here each server only samples the rows but does not
send these rows to the coordinator.

3. Use the protocol in Figure 4 to obtain an O(d log d)

d matrix

SA such that

×

SAx
Ω(1)
k

SAx

k2 ≤ k

k2 ≤

SAx
O(1)
k

k2

f

for all x

Rd.

∈

f

4. Each server Pi locally computes a QR-decomposition

SA = QR.

Figure 9: A preconditioning procedure for ℓ1 regression

f

O(sd2L) by Lemma 6.2 and Lemma 7.2. Similar to the analysis
protocol in the coordinator model is
in Section 6.2, we can also assume the bit complexity of all entries in SA and Sb is O(L + log n).
Furthermore, by Theorem 2.2, a (1 + ε)-approximate solution to the ℓ1 regression problem

e

SAR−1x

min

x k

Sb

k1

−

(4)

is a (1 + O(ε))-approximate solution to the original ℓ1 regression problem

min

x k

Ax

b

k1.

−

Thus, we will focus on the ℓ1 regression problem in (4) in the remaining part of this section

To show (SAR−1)T SAR−1

need to show (SAR−1)T SAR−1
where N = O(d log n/ε2) is the number of rows of SAR−1.

Now we show SAR−1 is approximately isotropic row-bounded as in Deﬁnition 7.1. We only
≈O(1) I and all rows of SAR−1 satisfy
2
2 = O(d/N )
k
≈O(1) I, it is equivalent to show that for all x
x
Ω(1)
k

2
2 ≤ k
k
SAR−1 is an orthogonal matrix Q, which implies for all x

(SAR−1)i
k

x
O(1)
k

Notice that

SAR−1x

2
2 ≤
k

2
2.
k

Rd,

(5)

∈

Rd

∈

f

SAR−1x
k

k2 =

x
k

k2.

(6)

Combining (6) and the fact that

f

SAx
Ω(1)
k

SAx

k2 ≤ k

k2 ≤

SAx
O(1)
k

k2

f

30

for all x

Rd, we can prove (5), which implies (SAR−1)T SAR−1

∈

To show

2
2 = O(d/N ), we use Lemma 29 in [32], which states that with constant
k
probability, the leverage scores of SA satisfy τi(SA) = O(d/N ) for all i. Since leverage scores are
invariant under change of basis (see, e.g., Section 2.4 in [66]), we have for all i,

(SA)iR−1
k

≈O(1) I.

(SA)iR−1(SAR−1)T SAR−1((SA)iR−1)T = O(d/N ).

Since (SAR−1)T SAR−1

≈O(1) I, we have

Thus, SAR−1 is approximately isotropic row-bounded.

(SA)iR−1
k

2
2 = O(d/N ).
k

Now we describe our protocol for ℓ1 regression in Figure 10. Our protocol ﬁrst uses the precon-
ditioning procedure in Figure 9 and then uses Nesterov’s accelerated gradient descent [52] to solve
the ℓ1 regression problem

SAR−1x

min

x k

Sb

k1.

−

Furthermore, we invoke a smoothing reduction JointAdaptRegSmooth in [2] to obtain better depen-
dence on ε.

1. Each server invokes the protocol in Figure 9 to obtain R, and calculates R−1.

2. Each server invokes the protocol in Figure 3 to get an exact solution to the ℓ2 regression

problem

x0 = min

SAx

x k

Sb

2
2.
k

−

3. Each server locally initializes x = Rx0 which is the optimal solution to the the ℓ2 regression

problem

SAR−1x

min

x k

Sb

2
2,
k

−

and then uses Nesterov’s accelerated gradient descent [52] and JointAdaptRegSmooth in
[2] to solve the ℓ1 regression problem

SAR−1x

min

x k

Sb

k1.

−

Figure 10: Accelerated gradient descent for ℓ1 regression

In order to implement Nesterov’s accelerated gradient descent in the distributed setting, each
server Pi maintains the current solution x. In each round, servers communicate to calculate the
current gradient vector. Once all servers receive the gradient vector, they can update their current
solution x locally and proceed to the next round. Analysis in [2] (Example C.3) shows that when
JointAdaptRegSmooth is applied to Nesterov’s accelerated gradient descent, after O(G√Θ/δ) full
gradient calculations, the algorithm will output a vector x such that

Ax
k

−

b

k1 ≤

min

x k

Ax

b

k1 + O(δ),

−

31

−

−

Sb

x∗

SAR−1x
k

Ax∗
k

k1. Furthermore, Lemma 15 in [32] shows that G

where we assume
2
x
2 ≤
k
k
optimal solution to the ℓ2 regression problem minx k
x∗
b
x
k2 ≤
−
k
−
Ax∗
δ = ε
−
k
O(d/ε) full gradient calculations.

k1 is G-Lipschitz continuous and the initial solution x satisﬁes
Θ. Since SAR−1 is approximately isotropic row-bounded and the initial vector x is the
2
SAR−1x
Sb
2, Lemma 19 in [32] shows that
k
√nd. By setting
d/n
k1, we can calculate a (1 + ε)-approximate solution to the ℓ1 regression problem using
b
p
Both Nesterov’s accelerated gradient descent [52] and JointAdaptRegSmooth in [2] require an
k1, which be can be obtained by using the algorithm
It remains to design an protocol to calculate the gradient vector of the smoothed objective func-
k1, in the distributed setting. We show this can be done with communication

estimation (up to a constant factor) of
in Section 7.1 to obtain an O(1)-approximate solution

O(sd2L). By using JointAdaptRegSmooth in [2], the new objective function will be

tion for
complexity

x and then calculating

SAR−1x
k

Ax∗
k

A
k

k1.

Sb

−

≤

−

−

−

x

b

b

b

b

e

n

Xi=1

where

(SA)iR−1, x
fλt(
h

i −

(Sb)i) +

σt
2 k

x

2
2

x0k

−

(7)

x2
2λt
x
|

fλt(x) =

(
Rd known to each server.

| −

λt
2

x
|

if
λt
| ≤
otherwise

,

R and x0 ∈

Each server can locally calculate the gradient vector of the σt

for some λt, σt ∈
2
2 term, since σt and x0
is known to each server. In the remaining part of this section, we focus on designing an algorithm
for calculating the gradient vector of the ﬁrst term in (7).

x
2 k

x0k

−

For the ﬁrst term in (7), we have

(SA)iR−1, x
fλt(
h

∇

i−

(Sb)i) =

(SA)iR−1, x
(
h
i −
(SA)iR−1, x
sign(
h

(Sb)i)/λt ·
(Sb)i)
i −

·

(

(SA)iR−1
(SA)iR−1

(SA)iR−1, x

if
otherwise

|h

(Sb)i

λt

.

| ≤

i −

(8)
Notice that we cannot directly let each server Pi calculate the gradient vectors using (8), send the
gradient vectors to the coordinator and calculate the summation, since the bit complexity of R−1
can be unbounded. Instead, we deal the two cases in (7) by using two diﬀerent approaches.

When

|h

(SA)iR−1, x

(SA)iR−1, x
(SA)iR−1 can be unbounded, all entries in the vector sign(
h

(SA)iR−1, x
> λt, notice that although the bit complexity of sign(
h
(SA)i
(Sb)i)
O(L) and R−1 is a matrix known to each server. Thus, for each server

(Sb)i

i −

i−

|

·

i−

(Sb)i)
have bit complexity at most
P , it sends

·

e

(SA)iR−1, x
sign(
h

i −

(Sb)i)

·

(SA)i

to the coordinator, for each row (SA)i which is stored on P and satisﬁes
After receiving from each server, the coordinator calculates

X

(SA)iR−1, x

|h

(Sb)i

|

i−

> λt.

(SA)iR−1, x
sign(
h

i −

(Sb)i)

·

(SA)i

for all rows (SA)i that satisfy
can then recover the gradient vector. The total communication for this case is at most

> λt, and sends it to each server. All servers

(SA)iR−1, x

(Sb)i

i −

|

X
|h

O(sdL).

e

32

When

(SA)iR−1, x

(Sb)i

λt,

|h

i −
| ≤
(SA)iR−1, x
(
h
=xT

R−1

T

i −

(Sb)i)/λt ·
(SA)iR−1
(((SA)i)T (SA)i)R−1/λt −

(Sb)i(SA)iR−1/λt.

Thus, for each server P , it sends

(cid:0)

(cid:1)

and

((SA)i)T (SA)i

X

(Sb)i(SA)i

to the coordinator, for each row (SA)i which is stored on P and satisﬁes
After receiving from each server, the coordinator calculates

X

(SA)iR−1, x

|h

(Sb)i

λt.

| ≤

i−

and

((SA)i)T (SA)i

X

(Sb)i(SA)i

for all rows (SA)i that satisfy
can then recover the gradient vector. The total communication for this case is at most

λt, and sends it to each server. All servers

(SA)iR−1, x

O(sd2L).

(Sb)i

i −

| ≤

|h

X

Thus, the total communication complexity of the protocol in Figure 10 is

Theorem 7.4. The protocol described in Figure 10 is a randomized protocol which returns a (1+ε)-
approximate solution to the ℓ1 regression problem with constant probability, and the communication
complexity is

O(sd3L/ε) in the coordinator model.

e

O(sd3L/ε).
e

8 Communication Protocols for ℓp Regression

e

In this section, we design distributed protocols for solving the ℓp regression problem, including

p =

.

∞

8.1 Communication Protocols for ℓ∞ Regression

Any ℓ∞ regression instance minx k

Ax

b

k∞ can be formulated as the following linear program,

−

minimize v

subject to

Ai, x
h
Ai, x
h

i −

i −

bi
bi

v,

≤

≥ −

v,

which has 2n constraints and d + 1 variables. Thus, any linear programming protocol implies a
protocol for solving the ℓ∞ regression problem, with the same communication complexity. Using
the linear program solvers in Section 10 and Section 11, we have the following theorem.

Theorem 8.1. ℓ∞ regression can be solved deterministically and exactly with communication com-
O(sd3L) in the coordinator model, and randomly and exactly with communication complexity
plexity
sd + d4L, sd3L
O(min
{
e

) in the blackboard model.
}

e

33

8.2 Communication Protocols for ℓp Regression When p > 2

In this section, we introduce an approach that reduces (1 + ε)-approximate ℓp regression to
O(d/ε2) variables. Our main idea is to use the max-stability of exponential
linear programs with
random variables [3] to embed ℓp into ℓ∞. Such idea was previously used to construct subspace
embeddings for the ℓp norm [67]. However, since our goal here is to solve linear regression instead
of providing an embedding for the whole subspace, we can achieve a much better approximation
ratio than previous work [67].

e

Theorem 8.2. For any matrix A
×
n random diagonal matrices, whose diagonal entries are i.i.d. random variables with the same
distribution as E−1/p, where E is an exponential random variable. If R = O(d log(d/ε)/ε2), then
with constant probability, the following holds:

Rn×d and constant p > 2, let D(1), D(2), . . . , D(R) be n

∈

1.

2. For all x

Rd,

∈

R

Xi=1

R

D(i)(Ax∗
k

−

b)

k∞ ≤

(1 + ε)CpR

Ax∗
k

b

kp;

−

D(i)(Ax
k

−

b)

k∞ ≥

(1

−

ε)CpR

Ax
k

b

kp.

−

Xi=1

Here x∗
−
constant which is the expectation of E−1/p for an exponential random variable E.

Rd is the optimal solution to the ℓp regression problem minx k

Ax

∈

b

kp and Cp is a

The proof of Theorem 8.2 can be found in Section 8.3.
Now we prove with constant probability, the optimal solution to the optimization problem

R

Xi=1

x = argminx

b

D(i)(Ax
k

b)

k∞

−

satisﬁes

where x∗

kp ≤
Rd is the optimal solution to the ℓp regression problem minx k

−

−

∈

Ax∗
(1 + 3ε)
k

b

kp,

x

A
k

b

b

Notice that with constant probability,

Ax

b

kp.

−

b

kp ≤

−

x

A
k
1 + ε
b
1
−

Ax∗

≤

ε k

−

b)

R
D(i)(A
x
i=1 k
−
ε)CpR
(1
−
b
Ax∗
(1 + 3ε)
k

kp ≤

P
b

b

kp.

−

k∞

R
D(i)(Ax∗
i=1 k
(1

−
ε)CpR

−

≤

P

Thus, we have reduced (1 + ε)-approxiamte ℓp regression to

argminx

R

Xi=1

D(i)(Ax
k

k∞.
b)

−

34

b)

k∞

(9)

∈

[R], we use vi to represent the value of

The optimzation problem in (9) can be written as a linear program with R + d =

O(d/ε2)
D(i)(Ax
variables. For each i
k∞ as in Section 8.1,
b)
k
R
and the goal is to minimize
i=1 vi. Furthermore this reduction can be easily implemented in the
distributed setting since each server can independently generate random variables in D(i) associated
with its own input rows in [A b]. We can round each entry in D(i) to its nearest integer mutiple of
poly(ε/d), which is enough for the correctness of Theorem 8.2, but increases the bit complexity of
each entry by at most an O(log(d/ε)) factor.

P

−

e

Using the linear program solvers in Section 10 and Section 11, we have the following theorem.

Theorem 8.3. (1 + ε)-approximate ℓp regression can be solved by a randomized protocol with
O(sd3L/ε6) in the coordinator model, or by a randomized protocol with
communication complexity
O(min
communication complexity
e
e

sd3L/ε6, sd/ε2 + d4L/ε8
{

) in the blackboard model.
}

8.3 Proof of Theorem 8.2

We need the following Bernstein-type lower tail inequality which is due to Maurer [49].

Lemma 8.4 ([49]). Suppose X1, X2, . . . , Xn are independent positive random variables that satisfy
E[X 2

n
i=1 Xi. For any t > 0 we have

. Let X =

i ] <

∞

P

Pr[X

E[X]

t]

−

≤

exp

≤

t2

−

2

(cid:18)

n
i=1 E[X 2
i ]

.

(cid:19)

We use the standard ε-net construction of a subspace in [15].

P

≥

Deﬁnition 8.1. For any p

1, for a given A

B is an ε-net of B if for any y

with size

N ⊆
Lemma 8.5 ([15]). For a given A
1
}
Lemma 8.6 (Auerbach basis [9]). For any matrix A
U of the column space of A, such that

(3/ε)d.

|N | ≤

∈

∈
B, there exists a

Rn×d, let B =
y

∈
Rn×d, there exists an ε-net

∈ N

x
Ax
{
such that

|

b

N ⊆

Rd,
∈
y
k
B =

Ax
k
y
kp ≤
x

−
Ax
b
{

|

. We say
kp = 1
}
ε.

Rd,

Ax
k

kp =

∈

Rn×d and p

1, there exists a basis matrix

[d], and for any vector x

≥

Rd,

∈

∈

Uikp = 1 for all i
k
kp .
k∞ ≤ k

x
k

U x

∈

Now we give the proof of Theorem 8.2. Notice that for any ﬁxed vector y

Rn,

∈

y
k∞ ∼ k
where E is an exponential random variable. Moreover, when p > 2, both E[(E−1/p)2] and
Var[E−1/p] are bounded by a constant.
D(i)y
We have E[
k

kp. By linearity of expectation, we also have

y
k∞] = Cpk

kpE−1/p

D(i)y
k

R

E

"

Xi=1

D(i)y
k

k∞

#

= CpR

y
k

kp.

We use U
three events

∈
E1,

Rn×(d+1) to denote an Auerbach basis of the column space of
E2,

E3. Here C is an absolute constant.

A = [A b]. We create

e

35

D(i)(Ax∗
• E1:
b)
k
−
D(i)Ujk∞ ≤
• E2:
k
,
• E3: for all y

∈ N

k∞ ≤
C(R

C

R1/p

·

· k
d)1/p for all i

Ax∗

b

kp for all i

−
[R] and j

∈
[d + 1].

[R].

∈

∈

·

R

Diy
k

k∞ ≥

(1

−

ε/3)CpR

y
k

kp,

Xi=1
is a (poly(ε/d))-net of

where

|N | ≤

N
(d/ε)O(d).

Ax
{

|

x

∈

Rd+1,

Ax
k

. By Lemma 8.5 we have
kp = 1
}

e

e

According to the cumulative density function of E−1/p for an exponential random variable E,
E2 also holds with

E1 holds with constant probability. Similarly,
∈ N

, using Maurer’s inequality in Lemma 8.4, we have

[R],
and a union bound over i
constant probability. For each y

∈

Pr

R

"

Xi=1

Diy
k

k∞ < (1

−

ε/3)CpR

y
k

kp

# ≤

exp

−

(cid:18)

(ε

CpR)2
·
O(R)

.

(cid:19)

Thus for R = O(d log(d/ε)/ε2), by using a union bound for all y
holds.

∈ N

, with constant probability

E3

Conditioned on

E1, using Bernstein’s inequality, we have

Pr

R

"
Xi=1

D(i)(Ax∗
k

k∞ > (1 + ε)CpR
b)

−

Ax∗
k

b

kp

−

exp

# ≤

−

(cid:18)

Thus, for R = O(d log(d/ε)/ε2) and p > 2,

O(R) + O(ε

(ε

CpR)2

·

CpR1+1/p)

·

.

(cid:19)

R

Xi=1

D(i)(Ax∗
k

−

b)

k∞ ≤

(1 + ε)CpR

Ax∗
k

b

kp

−

holds with constant probability, which implies Part 1 of Theorem 8.2.

Now for any y = U x with

y
k
E2, we have,

1. Conditioned on

kp = 1, by deﬁnition of the Auerbach basis we have

x
k

y
k∞ ≤ k

kp ≤

R

Xi=1

D(i)y
k

k∞ ≤

Xi=1

R

d+1

R

d+1

D(i)Uj ·
k

xjk∞ ≤

D(i)Ujk∞ ≤
k

poly(d/ε).

Xj=1
kp = 1. We claim y can be written as

Xj=1

Xi=1

y
k

Consider any y =

Ax with

where for any j

≥

yj
kyj kp ∈ N

and (ii)

yj
k

kp ≤

poly(ε/d)j .

According to the deﬁnition of a (poly(ε/d))-net, there exists a vector y0

for which

y0

kp ≤

poly(ε/d) and

y0
k

kp = 1. If y = y0 then we stop. Otherwise we consider the vector

∈ N

y
−
k
y−y0
.
ky−y0kp

e

0 we have (i)

∞

y =

yj,

Xj=0

36

Again we can ﬁnd a vector

y1

such that

∈ N

y−y0
ky−y0kp −

y1

poly(ε/d) and

y1
k

kp = 1. Here we

p ≤

set y1 =

y0
Thus, conditioned on

kp ·

y
k

−

(cid:13)
y1 and continue this process inductively.
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
E3, we have for any y =

b
E2 and

b
Ax with

y
k

kp = 1,

b

b

R

R

R

∞

D(i)y
k

k∞ ≥

D(i)y0
k

k∞ −

Xi=1
Xi=1
b, by homogeneity, we still have

−

Xi=1

Xj=1

For any y = Ax

e
D(i)yj
k

(1

−

k ≥

ε)CpR.

R

Xi=1

D(i)y
k

k∞ ≥

(1

−

ε)CpR

Ax
k

b

kp,

−

which implies Part 2 of Theorem 8.2.

9 Communication Complexity Lower Bound for Linear Program-

ming

In this section, we prove a communication complexity lower bound for testing feasibility of

linear programs.

We need the following lemma to construct our hard instance.

Lemma 9.1. Let L be a suﬃciently large integer. We use mi ∈

R2 to denote the vector

mi =

i
2L , 1

−

2

i2
4L

·

.

(cid:19)

(cid:18)

For any 1

i, j

2L/100, we have

1.

≤
2
mik
2 ≥
k
2. For any i

≤
1 + 1

24L+2 ;

= j,

Proof. For any 1

i

≤

1.

mi, mji ≤
h
2L/100, we have

≤

mik
k

2
2 =

i2
4L +

1

(cid:18)

−

2

2

i2
4L

·

(cid:19)

= 1 +

i4

24L+2 ≥

1 +

1
24L+2 .

i, j

For any 1

mi, mji
h

=

≤
ij
4L +

≤

1

(cid:18)

2L/100 and i

= j, we have

i2
4L

·

−

2

1

−

2

·

(cid:19)

(cid:18)

j2
4L

·

(cid:19)

= 1

−

j)2
(i
4L +
−
2
·

i2j2
42L+1 ≤

1

−

2

1
4L +

·

i2j2
42L+1 ≤

1.

Now we reduce the lopsided set disjiontness problem to testing feasibility of linear programs.
[U ], and
[U ]. The goal is to test whether there exists i such that

In this problem, for a choice of universe size U , the last server Ps receives an element u
for each i < s, server Pi receives a set Si ⊆

∈

37

6
6
Si. We reduce this problem with U = 2L/100 to testing the feasibility of linear programs for

u
d = 2, where L is the bit complexity of the linear program.

∈

mv, x
h

For the reduction, server Ps adds a constraint x = mu, for the element u
I.e., server Ps forces the solution x to be mu. For each i < s, for each v
constraint
i ≤
program is feasible if and only if u /
∈

[U ] that Ps receives.
Si, server Pi adds a
1. Here mu and mv are as deﬁned in Lemma 9.1. By Lemma 9.1, this linear

In the remaining part of this section, we show the lopsided set disjointness problem has an
Ω(s log U/ log s) randomized communication complexity lower bound in the coordinator model,
which implies an Ω(s log L/ log s) lower bound for testing feasiblity of linear programming, even for
d = 2. An Ω(s + L) lower bound also holds in the blackboard model, since when s = 2 the coordi-
nator model is equivalent to the blackboard model, up to a constant factor in the communication
complexity.

i<s Si.

∈
∈

S

∈

∈

⊆

[U ]. The goal is to test whether u

We ﬁrst consider the two-player case, in which Alice receives an element u

[U ] and Bob receives
S or not. Let µ be the distribution where u is chosen
a set S
U is included
uniformly at random from [U ], and S is a subset of [U ] such that each element u
S, and
independently with probability 1/2. Let µy be the conditional distribution of µ given u
µn be the conditional distribution of µ given u /
S. In [5, Section 2.2], it has been shown that any
∈
communication protocol that succeeds with probability at least 2/3 on the distribution µ requires
Ω(log U ) bits of communication in the worst case. By applying Markov’s inequality and stopping
the protocol early once the communication complexity is too large, this implies any randomized
protocol that succeeds with probability at least 3/4 on the distribution µ requires Ω(log U ) bits
of communication in expectation.
In fact, this implies a stronger hardness result, that for any
protocol that succeeds with probability at least 3/4 on µ, its expected communication complexity
is Ω(log U ) on both µy and µn.

∈

∈

P

succeeds with probability at least 1

Consider a new distribution µ′ which is µy with probability 1/s2 and µn with probability 1
1/100s2. Then by averaging

1/s2.
Suppose a protocol
suc-
ceeds with probaility at least 4/5 on both µn and µy, which implies the expected communication
is Ω(log U ) on both µy and µn. Now by linearity of expectation, the expected
complexity of
P
on µ′ is lower bounded by Ω(log U ). This, in particular, implies
communication complexity of
1/100s2 on µ′ should have expected com-
any protocol that succeeds with probability at least 1
munication complexity Ω(log U ). At this point, Theorem 1.1 in [68] implies that for the s-player
1/s3 has worst case
case, any communication protocol that succeeds with probability at least 1
communication complexity at least Ω(s log U ). By standard repitition arguments this implies an
Ω(s log U/ log s) lower bound for protocols that succeed with constant probability.

−
P

−

−

−

P

Formally, we have the following theorem.

Theorem 9.2. Any randomized protocol that succeeds with probability at least 0.99 for testing
feasibility of linear programs requires Ω(s log L/ log s) bits of communication in the coordinator
model and Ω(s + L) bits of communication in the blackboard model. The lower bound holds even
when d = 2.

Notice that by Theorem 4.2, testing feasibility of linear systems for d = 2 requires only O(s log L)
randomized communication complexity. This shows an exponential separation between testing
feasibility of linear systems and linear programs, in the communication model.

38

10 Clarkson’s Algorithm

10.1 The Communication Complexity

In this section, we discuss how to implement Clarkson’s algorithm to solve linear programs in
the distributed setting. The protocol is described in Figure 11. During the protocol, each server
Pi maintains a multi-set Hi of constraints (i.e., each constraint can appear more than once in Hi).
Initially, Hi is the set of constraints stored on Pi. Furthermore, the coordinator maintains
,
Hi|
|
which is initially set to be the number of constraints stored on each server.

1. The coordinator obtains 9d2 constraints R, by sampling uniformly at random from H1 ∪

H2 ∪

. . .

∪

Hs.

2. The coordinator calculates the optimal solution xR, which is the optimal solution to the
linear program satisfying all constraints in R. The coordinator sends xR to each server.

3. Each server Pi calculates the total number of constraints that are stored on Pi and violated
.
xR violates h
}

where Vi =

by xR, i.e.,

s
i=1 |

Vi|

where V =

h
{

∈

H

R

xR violates h
}

|

\

and

Vi|
|
4. The coordinator calculates

to each server.

Hi |
∈
=

h
{
V
|

|

P

= 0 then xR is a feasible solution and the protocol terminates.

, then each server updates Hi ←
H
|
.
Vi|
|

Hi∪

Vi and the coordinator updates

sends

V
|
(a) If

|
V
|
2
V
9d−1 |
|
| ≤
+
Hi|
Hi| ← |
|
(c) Goto Step 1.

(b) If

|

Figure 11: Clarkson’s Algorithm

The protocol in Figure 11 is basically Clarkson’s algorithm [22], implemented in the distributed
setting. Using the analysis in [22], the expected number of iterations is O(d log n). The correctness
also directly follows from the analysis in [22]. Now we analyze the communication complexity for
each iteration.

To implement the sampling process in Step 1, the coordinator ﬁrst determines the number of
constraints to be sampled from each server Pi and sends this number to Pi. The total communication
complexity for this step is O(s log n) in both the coordinator model and the blackboard model.
Then each server Pi samples accordingly and sends these constraints to the coordinator. The total
communication for this step is O(d3L) in both models.

To implement Step 2, we ﬁrst verify the bit complexity of the optimal solution xR. One of the
b. From polyhedral theory we know that
optimal solutions xR is a vertex of the polyhedron Ax
there exists a non-singular subsystem of Ax
c, such that xR is the unique solution of
Bx = c. Thus, by Cramer’s rule, each entry of x is a fraction whose numerator and denominator are
d!2dL and d!2dL, and thus can be represented by using at most O(dL + d log d)
integers between
bits. This implies the bit complexity of all entries in the vector x calculated at Step 2 is upper

≤
b, say Bx

≤

≤

−

39

O(d2L). Thus the communication complexity for Step 2 is upper bounded by

O(sd2L)
bounded by
O(d2L) in the blackboard model. The communication complexity
in the coordinator model and
e
of the last two steps of the protocol is upper bounded by O(s log n) in both models. Thus, the
O(sd + d4L)
expected communication complexity is
in the blackboard model.

O(sd3L + d4L) in the coordinator model and

e

e

e
Theorem 10.1. The expected communication complexity of the protocol in Figure 11 is
d4L) in the coordinator model and

O(sd + d4L) in the blackboard model

e

O(sd3L +

e

10.2 Running Time of Clarkson’s Algorithm in Unit Cost RAM

e

In this section, we show how to implement Clarkson’s algorithm in the unit cost RAM model
O(ndωL + poly(dL)),

on words of size O(log(nd)) so that the running time is upper bounded by
and prove Theorem 1.11.

A description of Clarkson’s algorithm can be found in Figure 11. This algorithm runs in
O(d log n) rounds in expectation. In each round, it samples O(d2) constraints R, and calculates an
optimal solution xR that satisﬁes all constraints in R. This optimal solution xR can be calculated
using any polynomial time linear programming algorithm, which always has running time poly(dL).
The bottleneck in the unit cost RAM model is Step 4 of the algorithm in Figure 11, i.e., for each
of the n constraints, testing whether xR satisﬁes the constraint or not. Formally, we just need to
output AxR, and then compare each entry with b. In the remaining part of this section we show
how to caculate AxR in

O(ndωL) time

e

e

O(L) bits of (xR)i, . . . , the last

Since each entry of xR has bit complexity

O(dL), we ﬁrst calculate a d
O(L), and the entry Xi,1, Xi,2, . . . , Xi,d consists of the ﬁrst

d matrix X, where each
O(L) bits
O(L) bits of (xR)i. Now we calculate A
X.
e
O(L), and caculating the matrix mutilplication of
X can therefore be
O(ndω−1L) time. Given AX, one can then easily calculate AxR in
O(ndω−1L) +

entry of X has bit complexity
of (xR)i, the second
Since all entries in A and X have bit complexity
e
d matrices with bit complexity
two d
×
dω
calculated in
O((n/d)
O(ndL) time. Thus, the total expected running time is upper bounded by O(d log n(
O(ndL) + poly(dL))) =
e
e
10.3 Smoothed Analysis of Communication Complexity

O(ndωL + poly(dL)).

O(dωL) time [39], A

O(L) requires only

L) =

×

e

e

e

e

e

e

e

e

e

e

·

·

·

·

In this section we deﬁne our model for smoothed analysis of communication complexity of

communication protocols for solving linear programming.

For a randomized communication protocol
complexity on the linear programming instance

P

, we use CP (A, b, c) to denote its communication

cT x,

min
Ax≤b

(10)

∈

∈

Rn×d, b

Rn and c

Rd. The standard deﬁnition [61] of smoothed analysis assumes
where A
that each entry of A is perturbed by i.i.d. Gaussian noise with zero mean and standard deviation
σ. However, since we are measuring the communication complexity in terms of bit complexity,
we cannot allow the noise to be arbitrary real numbers. Instead, in our model, we use discrete
Gaussian random variables as the noise.

∈

Formally, we use trunct : R

R to denote the function that rounds a real number to its
nearest integer multiple of 2−t. For notational convenience, we deﬁne trunc∞(x) = x. We say

→

40

a communication protocol solves the linear program instance (10) with smoothed communication
complexity SCP,σ,t(A, b, c) if with probability at least 0.99, the protocol correctly solves the instance

min
(A+Gt,σ)x≤b

cT x

(11)

SCP,σ,t(A, b, c), where all entries of Gt,σ are
with communication complexity CP (A + Gt,σ, b, c)
i.i.d. copies of trunct(g) and g is a Gaussian random variable with zero mean and σ
1 standard
deviation. Here the probability is deﬁned over the randomness of the protocol and the noise Gt,σ.
, G∞,σ is a matrix whose all entries are i.i.d. Gaussian random variables
Notice that when t =
with standard deviation σ.

∞

≤

≤

10.4 Smoothed Analysis of Clarkson’s Algorithm

1. The coordinator obtains 9d2 constraints R, by sampling uniformly at random from H1 ∪

H2 ∪

. . .

∪

Hs.

2. The coordinator calculates the optimal solution xR, which is the optimal solution to the
linear program satisfying all constraints in R. The coordinator rounds each entry
xR,
of xR to its nearest integer multiple of δ = O(1/ poly(nd
and sends

2L/σ)) to obtain

xR to each server.

·

3. Each server Pi calculates the total number of constrains that are stored on Pi and violated
.
xR violates h
}

where Vi =

xR, i.e.,

by

b
Vi|
|
4. The coordinator calculates

s
i=1 |
b

Vi|

where V =

h
{

∈

H

R

xR violates h
}

|

\

and

b

Hi |
∈
=

h
{
V
|

|

P

= 0 then xR is a feasible solution and the protocol terminates.

b

H
, then each server updates Hi ←
|
.
Vi|
|

Hi ∪

Vi and the coordinator updates

to each server.

b
sends

V
|
(a) If

|
V
|
2
V
9d−1 |
|
| ≤
+
Hi|
Hi| ← |
|
(c) Goto Step 1.

(b) if

|

Figure 12: Smoothed Clarkson’s Algorithm

In this section, we present our variant of Clarkson’s algorithm for solving smoothed linear
programming instances. The protocol is described in Figure 12. The main diﬀerence is in Step 2,
where the coordinator rounds each entry of the solution xR before sending it to other servers.

10.4.1 Correctness of the Protocol

We ﬁrst prove the correctness of the protocol. Our plan is to show if t = Ω(log(nd/σ) + L),
then our modiﬁed Clarkson’s algorithm follows the computation path of the original Clarkson’s
algorithm in Figure 11 when executing on the perturbed instance, with high probability, and thus
prove the correctness of the protocol.

41

We need the following bound on the condition number of a matrix.

Lemma 10.2. For a matrix B
and t

Ω(log(nd/σ) + L), we have

∈

≥

Rd×d with all entries in [0, 2L

1], for any integer n > 0, σ

1

≤

−

B + Gσ,tk2 ≥
Pr[
k

poly(nd

2L/σ)]

·

1

−

≤

1/ poly(nd)

and

(B + Gσ,t)−1
Pr[
k
Proof. To prove the ﬁrst inequality, notice that

k2 ≥

poly(nd

2L/σ)]

·

1

−

≤

1/ poly(nd).

B + Gσ,tk2 ≤ k
k

B
B + Gσ,tkF ≤ k

kF +

Gσ,tkF ≤
k

poly(d2L) +

Gσ,tkF .
k

Thus, the ﬁrst inequality just follows from tail inequalities of the Guassian distribution.

(B + Gσ,t)−1
k

To analyze

random variables of Gσ,t before applying the truncation operation. Notice that this implies
Gσ,∞k2 ≤
probability 1

k2, we write Gσ,∞ to denote a matrix whose entries are the Gaussian
Gσ,t −
k
1/ poly(nd/σ). We invoke Theorem 3.3 in [56], which states that with

2−t
1/ poly(nd),

poly(d)

≤

·

−

(B + Gσ,∞)−1
k

k2 ≤

poly(nd/σ),

which implies with probability 1

1/ poly(nd),

−

(B + Gσ,t)−1
k

k2 =

inf
kxk2=1 k

(B + Gσ,t)x

(cid:18)
(B + Gσ,∞ + (Gσ,t −
2L/σ).

=

≤

inf
kxk2=1 k

(cid:18)
poly(nd

·

−1

k2

(cid:19)
−1

Gσ,∞))x

k2

(cid:19)

≤

(cid:18)

inf
kxk2=1 k

(B + Gσ,∞)x

k2 − k

Gσ,t −

Gσ,∞k2

−1

(cid:19)

Lemma 10.3. During the execution of the protocol in Figure 12, each time Step 2 is executed, if
1/ poly(nd), xR satisﬁes
xR 6

= 0, with probability at least 1

−

1/ poly(nd

2L/σ)

·

xRk2 ≤

≤ k

poly(nd

2L/σ).

·

Proof. From polyhedral theory we know that there exists a non-singular subsystem of the sampled
9d2 constraints R, say Bx

c, such that xR is the unique solution of Bx = c.
= 0, since each entry of B was pertubed by a discrete Gaussian noise, and all entries of c

If c

≤

are integers in the range [0, 2L

1], by Lemma 10.2 we have

−
xRk2 ≤ k
k

B−1

c
k2 ≤
k2k

poly(nd

2L/σ).

·

Furthermore, since

B
k

k2 ≤

poly(nd

2L/σ),

·
xRk2 ≥
k

1/ poly(nd

2L/σ).

·

If c = 0, then we must have xR = 0, since Bx = c is non-singular and thus xR = 0 is the unique

solution.

42

6
Now we create a family of events

∞
i=1. We use

{Ei}

loop of the execution of the protocol in Figure 12, for each constraint h /
∈
be satisﬁed by xR if and only if it can be satisﬁed by
xR can always satisfy them, by deﬁnition of xR.

Ei to denote the event that, during the i-th
R, the constraint h can
xR. Notice that for those constraints in R,

Now we show that for each

1/ poly(nd). By
showing this, we have actually shown our algorithm follows the computation path of the original
Clarkson’s algorithm in Figure 11 when executing on the perturbed instance, with high probability.
Since the original Clarkson’s algorithm in Figure 11 terminates in O(d log n) rounds with probability
at least 0.999, the correcntess of our algorithm follows by applying a union bound over all events

Ei, the probability that
b

Ei holds is at least 1

−

{Ei}

O(d log n)
i=1
To show that for each

.

a union bound over all constraints, it suﬀces to show that for each constaint h /
∈
h if and only if

Ei, the probability that
xR can satisfy h, with probability 1

Ei holds is at least 1
1/ poly(nd).

−

1/ poly(nd), by applying
R, xR can satisfy

−

Lemma 10.4. For each constraint h /
∈
only if

b
xR can satisfy h.

R, with probability 1

−

1/ poly(nd), xR can satisfy h if and

b

xR = xR = 0, in which case the lemma follows trivially. Thus we assume

= 0 in the remaining part of this proof.
The constraint h can be written as (ah + gσ,t)x

Proof. If xR = 0, then
xR 6
and all entries of gσ,t ∈
zero mean and σ standard deviation. Notice that since h /
∈
are independent. By Lemma 10.3, with probability at least 1

R,
b
Rd are i.i.d. copies of trunct(g) and g is a Gaussian random variable with
R, the vector gσ,t and the vector xR

bh, for some vector ah ∈

Rd and some bh ∈

1/ poly(nd),

≤

xRk2 ≤
Furthermore, the probability that xR can satisfy h but
but

xR can satisfy h, is at most

1/ poly(nd

≤ k

·

2L/σ)

−
poly(nd

2L/σ).

·

xR cannot satisfy h, or xR cannot satisfy h

b

Pr[

gσ,t, xRi| ≤ |h
|h

b
ah + gσ,t, xR −

].
xRi|

We ﬁrst analyze the right hand side of the inequality. Notice that
gσ,tk2 ≤
k
distribution and σ
Cauchy-Schwarz,

poly(nd) with probability at least 1

2L), and
ahk2 ≤
k
1/ poly(nd) by tail inequalities of the Gaussian
2L/σ). Thus by
1/ poly(dn

−
xRk2 ≤

1. Moreover,

xR −
k

poly(d)

poly(d

≤

≤

b

δ

·

·

·

Pr[

ah + gσ,t, xR −
|h

xRi| ≤

b

1/ poly(dn

2L/σ)]

·

1

−

≥

1/ poly(nd).

On the other hand, if we write gσ,∞ to denote a vector whose entries are the Gaussian random

variables of gσ,t before applying the truncation operation, then

Thus, by taking t = Ω(log(nd/σ) + L), we have

gσ,∞ −
k

gσ,tk2 ≤

poly(d)2−t.

b

gσ,t, xRi| ≥ |h
|h

gσ,∞, xRi| −

poly(d)2−t

xRk2 ≥ |h
k

gσ,∞, xRi| −

1/ poly(nd

2L/σ).

By the lower tail inequality of the Gaussian distribution and the fact that
2L/σ), we have with probability at least 1

1/ poly(nd),

−
gσ,t, xRi| ≥
|h
Thus, the lemma follows by appropriately adjusting the constant in O(1/ poly(nd

1/ poly(nd

2L/σ).

·

·
xRk2 ≥
k

1/ poly(nd

·

2L/σ)) = δ.

·

43

10.4.2 Communication Complexity of the Algorithm

The analysis in the preceding section shows that with high probability, our modiﬁed Clarkson’s
algorithm follows the computation path of the original Clarkson’s algorithm, and thus also termi-
nates within O(d log n) rounds with probability at least 0.999. Furthermore, with high probability,
the discrete Gaussian noise of all entries is upper bounded by O(nd). Thus, the bit complexity of
O(d(L + t)), with high probability.
sending each constraint will be

e

The sampling process in Step 1 requires

O(d3(L + t) + s) bits of communication to sample O(d2)
xR. Since we round
constraints. To implement Step 2, we need to verify the bit complexity of
each entry of xR to its nearest integer multiple of δ, and by Lemma 10.3, with high probability,
xR is upper bounded by
xRk2 ≤
k
O(sd(L + log(1/σ))). The communication complexity of the last two steps of the protocol is
O(sd2(L +
still upper bounded by O(s log n). Thus, the smoothed communication complexity is
log(1/σ)) + d4(L + t)) in the coordinator model.
e

2L/σ), the communication compleixty for sending

poly(nd

e

b

b

·

Theorem 10.5. For t = Ω(log(nd/σ) + L), the protocol in Figure 12 correctly solves smoothed
linear programming with probability at least 0.99, and the smoothed communication compleixty is

e

SCP,σ,t ≤

O(sd2(L + log(1/σ)) + d4(L + t))

in the coordinator model.

e

11 The Center of Gravity Method

In this section, we discuss how to implement the center-of-gravity cutting-plane method [35] in

the distributed setting. The description of the protocol can be found in Figure 13.

The servers each maintain a polytope P (the same one for all servers), adding a constraint in

each iteration. Each server also maintains the center of the polytope z and its covariance C.

∈

−

ε√d.

Rd, its ε-rounding

For any vector a
BT a
take the unit vector BT a/
k
BT a
BT a/
a
So we have
k2k2 ≤
k
k

a w.r.t. to C is deﬁned as follows: Let B = C 1/2. We
k2, round it down to the nearest multiple of ε in each coordinate.
If each server were to report the exact violated constraint, the volume of P would drop by a
constant factor in each iteration. To reduce the communication, we round the constraint and shift
it away a bit to make sure that the rounded constraint (1) is still valid for the target LP and (2)
it is close enough that the volume still drops by a constant factor.
Lemma 11.1 ([12]). Let z be the center of gravity of an isotropic convex body K in Rd. Then, for
any halfspace H within distance t of z, we have

e

e

vol(K

H)

∩

≥

1
e −

t

vol(K).

(cid:18)
Lemma 11.2. For ε < 0.1/d√d, the volume of the polytope P maintained by each server drops by
a constant factor in each iteration.

(cid:19)

Proof. Assume without loss of generality that P is isotropic. If the centroid z = 0 is not feasible,
we get a violated constraint such that the entire feasible region lies in the halfspace a
0 with
a
k

k2 = 1. Now we replace a by

a. As a result,

≤

x

·

x

a
·
e

a

·

≤

x +

a
k

a

x
k2k

k2 ≤

−

e

e

44

εd3/2.

1. Set P = [

−

R, R]n, z = 0, C = I.

2. Repeat, while no feasible solution found, and at most T times:

(a) Set z to be the centroid of P and C to be the covariance matrix of the uniform

distribution over P , i.e., C = Ex∼P (x

z)(x

z)T .

−
(b) Each server Pi checks z against their subset of constraints. If a violated constraint
z > bi is found, and no violation has been reported by other servers so far, then
a

a
it rounds a to
broadcast by the server that ﬁrst found the violation.

If a violation has been reported, it uses the

a and broadcasts.

−

·

(c) Set P = P

x : C −1/2

e

∩ {

x

a

·

≤

C −1/2

a

z + εd3/2

C −1/2
k

a

.
k2}

·

e

e

e

e

Figure 13: Center-of-Gravity Algorithm

Here we used the fact that in isotropic position any convex body is contained in a ball of radius d,
d for all of P and therefore for the feasible region. Thus the constraint imposed by the
so
k2 ≤
algorithm is valid.

x
k

Next, we note that the distance of the constraint from the origin is at most εd3/2, so for
ε < 0.1/d3/2, it is less than 0.1 (in isotropic position). By Lemma 11.1, with t = 0.1, the volume of
P drops by a constant factor.

Theorem 11.3. The protocol in Figure 13 is a deterministic protocol for solving linear programming
with communication complexity O(sd3L log2 d) in both the coordinator model and the blackboard
model.

Proof. The algorithm runs for T = O(d2L log d) rounds. To see this we note that the each vertex
of the feasible region is the solution of a subset of the linear equalities taken to be equalities. Thus,
each coordinate of each vertex is a ratio of two determinants of matrices whose entries are L-bit
numbers and so the maximum distance of a vertex from the origin is R = dO(dL), which upper
bounds the volume by dO(d2L). The smallest any coordinate can be is similarly d−O(dL). The
minimum volume we need to go to is the volume spanned by a simplex of vertices, which itself is
a determinant with entries of this size. Thus, the volume is at least d−O(d2L). Since the volume
of the polytope maintained drops by a constant factor in each iteration2, the number of rounds
is O(d2L log d). Each round includes a broadcast of a single vector, with O(d log d) bits. This is
because the size of the ε-net used is dO(d). By viewing the objective function as a constraint, we
note that the volume bounds used above apply to the optimization version as well. At the end, we
use diophantine approximation to get an exact solution [34].

A similar argument applies to general convex programming.

Theorem 11.4. The communication complexity of the protocol in Figure 13 for solving convex
programming is O(sd2 log d log(Rd/ε)).

2The ellipsoid method uses the same argument, except that each round reduces the volume by only (1 − 1/d) [34].

45

Proof. The initial volume is at most Rd and the algorithm can stop when the volume is (d/ε)−O(d).
Therefore the number of rounds is O(d log(Rd/ε)). Each round uses O(sd log d) bits giving the ﬁnal
bound.

12 Seidel’s Algorithm

We give an alternative constant dimensional linear programming algorithms in the blackboard
model, based on Seidel’s classical algorithm [58]. Here we additionally assume that each constraint
in the linear program is placed on a random server. This assumption is essential to get rid of the
log n dependence in the communication complexity. Here we also assume that the linear program
is bounded.

i ≤

a, x
h

To implement Seidel’s algorithm in the blackboard model, we go through all servers P1, P2, . . . , Ps,
and for each server Pi, we go through all constraints stored on Pi in a random order. We maintain
the optimal solution x∗ to the set of constraints that we have already went through. For a new
b, the current server ﬁrst checks whether the constraint is satisﬁed or not. The
constraint
current server proceeds to the next constraint if it is indeed satisﬁed. If it is not satisﬁed, then
b must be one of the d constraints that determines the current
the current constraint
optimal solution. In this case, the current server broadcasts the current constraint
b to all
other servers, and makes a recursive call to ﬁgure out the optimal solution, by adding an equality
constraint
= b to the set of constraints. Notice that if the ﬁrst server P1 ﬁnds a violated
constraint, P1 does not need to broadcast the violated constraint, since P1 can simply add the
equality constraint to the beginning of all constraints owned by P1.

a, x
h

a, x
h

a, x
h

i ≤

i ≤

i

One major diﬀerence between the classical Seidel’s algorithm and our implementation is that
each time we make a recursive call, we do not randomly permute the constraints again in the
recursive calls. Instead, the order that we go through the servers is ﬁxed, in diﬀerent recursive calls.
Due to this diﬀerence, there will be subtle dependence between the communication complexity of
diﬀerent recursive calls.

E

We use

to denote the event that the number of constraints stored on P1 is at least Ω(n/s). By
a Chernoﬀ bound,
holds with probability at least 0.99. For the i-th constraint (in the order that
we go through all constraints), we let the random variable Vi be 1 if the i-th constraint is one the
d constraints that determines the optimal solution among the ﬁrst i constrains, and 0 otherwise.

E

Since each constraint in the linear program is placed on a random server, by standard backward
,
analysis, E[Vi] = d/i. Furthermore, E[Vi | E
E
the ﬁrst Ω(n/s) constraints will not be broadcasted and there will be no recursive calls associated
with them, since they are stored on the ﬁrst server P1. Thus, conditioned on
, the expected
number of broadcasts (and thus recursive calls) is upper bounded by

] = O(d/i). However, conditioned on

E[Vi]/Pr[
E

≥

E

]

n

Xi=Ω(n/s)

O(d/i) = O(d log s).

We use

Fd to denote the event that there are at most O(d2 log s) recursive calls made at the
, by Markov’s
top layer of the recursive tree corresponding to Seidel’s algorithm. Conditioned on
inequality,
Fd−1 to denote the
Fd holds with probability at least 1
−
event that there are at at most O((d2 log s)2) recursive calls made at the second layer of the recursive
, again by Markov’s inequality,
tree corresponding to Seidel’s algorithm. Conditioned on

1/(100d). Similarly, we use

E

Fd and

E

46

Fd−1 holds with probability at least 1
conditioned on
number of broadcasts is upper bounded by O(logd s).

, with probability at least 0.99,

1/(100d). We similarly deﬁne
Fi holds for all i

−

∈

E

Fd−2,

F1. Thus,
Fd−3, . . . ,
[d]. Which implies the total

In each broadcast, the current server needs to broadcast the current constraint, which has bit
complexity O(dL). Moreover, after the recursive call, the current server broadcasts the current
solution vector x∗. By polyhedral theory, x∗ can be achieved by setting d inequality constraints to
be equality constraints. Thus, by Cramer’s rule, each entry of x∗ has bit complexity O(d log d+dL).
logd s), with probability
The total communication complexity is hence upper bounded by O(s + L
at least 0.9. Here the randomness is over the initial random assignment of each constraint and the
random coins tossed by the algorithm.

·

Theorem 12.1. Seidel’s algorithm can be used to solve linear programs in constant dimension with
O(s + L logd s) communication in the blackboard model, if each constraint in the linear program is
placed on a random server. Here the randomness is over the initial random assignment of each
constraint and the random coins tossed by the algorithm.

13 Singularity Probability

The goal of this section is to prove Theorem 3.1. We restate it here for convenience.

Theorem 3.1. (restated) Let Mn be a matrix whose entries are i.i.d. random variables with the
same distribution as

Bt, for suﬃciently large t,

Pr [Mn is singular]

t−Cn,

≤

where C > 0 is an absolute constant.

Our proof of Theorem 3.1 follows very closely the proof of Theorem 1.5 in [63]. Throughout

this section we use λ to denote t−1/2. We use Xi to denote the i-th row of Mn.

We need the following lemma on generalized binomial distributions.

Lemma 13.1. We have

and

Pr[

Bt = 0]
(λe−λ)
t

= 0]

Pr[

B

c1t−1/2

≤

c2t−1/4.

≤

(12)

(13)

Here c1 and c2 are absolute constants.

Proof. By Stirling’s approximation we have

Pr[

Bt = 0] =

t
t/2

(cid:19)

(cid:18)

/2t = Θ(t−1/2),

which proves (12). To prove (13), by a Chernoﬀ bound swe have that the number of non-zero terms
Ω(t1/2)). Conditioned
is Θ(tλe−λ) = Θ(t1/2) with probability 1

exp(
in the summation of
−
on this event, we can then prove (13) by using the same estimation as (12).

(λe−λ)
t

−

B

The following lemma is a direct implication of Lemma 13.1 and Odlyzko’s results in [53]. See

also Lemma 2.1 in [63] and Section 3.2 in [41].

47

Lemma 13.2. Let W
variables with the same distribution as

Rn be an arbitrary subspace and X (µ)
(µ)
t

. We have

⊆

B

Rn whose entries are i.i.d. random

∈

Pr[X

∈

W ] =

c1t−1/2
(cid:16)

(cid:17)

n−dim(W )

and

Pr[X (λe−λ)

By Lemma 5.1 in [63], we have

W ]

∈

≤

(cid:16)

c2t−1/4

n−dim(W )

.

(cid:17)

Pr[X1, X2, . . . , Xn are linearly independent]

to(n)Pr[X1, X2, . . . , Xn span a hyperplane],

≤

which implies we only need to consider the case when X1, X2, . . . , Xn span a hyperplane.
We say a hyperplane V is non-trivial if V is spanned by its intersection with
n. Notice that a hyperplane V has
}

1, 0, 1, . . . , t

1), . . . ,

1, t

−

−

{−

t,

(t

−

−

Pr[X1, X2, . . . , Xn span V ] > 0

only when V is non-trivial. Thus, we focus only on non-trivial hyperplanes in the remaining part
of the proof.

Deﬁnition 13.1. Let X
as
Bt. For a hyperplane V
multiple of 1/nt such that

∈
⊆

Rn whose entries are i.i.d. random variables with the same distribution
Rn, deﬁne the discrete codimension d(V ) of V to be the unique integer

−d(V )−1/nt

c1t−1/2

(cid:16)

(cid:17)

Pr[X

V ]

∈

≤

≤

c1t−1/2

−d(V )

.

(cid:16)

(cid:17)

According to the deﬁnition, it is clear from Lemma 13.2 that 1
We ﬁrst dispose hyperplanes with high discrete codimension using the following lemma, which

O(n).

d(V )

≤

≤

is a direct corollary of Lemma 1 in [41].

Rn whose entries are i.i.d. random variables with the same distribution

Lemma 13.3. Suppose X
as

Bt, then

∈

Pr





[V |d(V )≥d0

X1, X2, . . . , Xn span V

n

·



≤

c1t−1/2
(cid:16)

−d0

.

(cid:17)

Let 1/2

≥

ε > 0 be a constant to be determined. Using Lemma 13.3, we have

Pr





[V |d(V )≥(ε−o(1))n

X1, X2, . . . , Xn span V

n

·



≤

c1t−1/2
(cid:16)

(ε−o(1))n

= t−Ω(n).

(cid:17)

Thus, in the remaining part of the proof we will focus only on the case when d(V )

≤
We say a hyperplane V to be non-degenerate if its normal vector n(V ) satisﬁes

o(1))n .
k0 ≥
k0 to denote the number of non-zero entries in the normal
log log n/ log t
⌈
vector n(V ). The following lemma, which is a simple adaption of Lemma 5.3 in [63], provides a
crude estimation of the number of degenerate hyperplanes.

. Here we use

−
n(V )
k

n(V )
k

(ε

⌉





48

Lemma 13.4. The number of degenerate non-trivial hyperplanes is at most to(n).

Combining Lemma 13.2 and Lemma 13.4, we then have

Pr[X1, X2, . . . , Xn span V ]

≤

XV |V is degenerate
Thus, we can just focus on non-degenerate hyperplanes.

XV |V is degenerate

Pr[X1, X2, . . . , Xn ∈

V ]

≤

to(n)

c1t−1/2
(cid:16)

(cid:17)

n

= t−Ω(n).

The following theorem, which ﬁrst appeared in [41] as Theorem 2 (see also Section 7 in [63]), is

based on Fourier-analytic arguments by Hal´asz [38, 37].

Theorem 13.5. Suppose V
i.i.d. random variables with the same distribution as
positive integer such that 4λk2 < 1. We have

⊆

B

Rn is a non-trivial hyperplane. Let Y (µ)

Rn whose entries are
(µ), λ < 1 be a positive number and k be a

∈

Pr[Y

V ]

∈

≤

(cid:18)

k(1

1
4λk2)

−

+

1

4λ

1

−

e−(1−4λ)kn(V )k0/(4k2)

Pr[Y (λe−λ)

(cid:19)

V ],

∈

where we use n(V ) to denote the normal vector of V and
entries of n(V ).

n(V )
k

k0 to denote the number of non-zero

Corollary 13.6. Suppose W
whose entries are i.i.d. random variables with the same distribution as
t, we have

⊆

Rn is a non-degenerate non-trivial hyperplane. Let X (µ)

Rn

(µ)
t

. For suﬃciently large

∈

O(t−1/4)Pr[X (λe−λ)

W ]

∈

≤

B
t−1/5Pr[X (λe−λ)

W ].

∈

Pr[X

W ]

∈

≤

Proof. We note that

Pr[X (µ)

X (µ), n(W )
W ] = Pr[
i
h

∈

= 0] = Pr



Y (µ)
i,j ni(W ) = 0



,

(14)

n

t

Xi=1

Xj=1





i,j are i.i.d. random variables with the same distribution as

where Y (µ)
(µ) and ni(W ) is the i-th
coordinate of the normal vector n(W ). This enables one to apply Theorem 13.5. Notice that when
applying Theorem 13.5 we have
k0, since each non-zero entry of n(W ) appears
n(W )
k
t times in the summation of (14). Recall that λ = t−1/2. We set k to be an integer which is at
least Ω(t1/4). Since V is non-degenerate, we have
, which
t
n(V )
k
implies

log log n/ log t

n(W )
k

k0 = t

n(V )
k

k0 = t

k0 ≥

· ⌈

B

⌉

1
The correctness of the corollary thus follows from our choice of k.

4λ

−

1

e−(1−4λ)kn(V )k0/(4k2) = o(1).

For a non-degenerate non-trivial hyperplane V which satisﬁes 1

d(V )

(ε

−

≤

≤

o(1))n, deﬁne

AV to be the event that
, X (λe−λ)
X (λe−λ)

1

2

, . . . , X (λe−λ)

(1−η)n, X ′

1, X ′

2, . . . , X ′

(η−ε′)n are linearly independent in V ,

where X (λe−λ)
with the same distribution as

are independent random vectors in Rn whose entries are i.i.d. random variables
i are random vectors in Rn whose entries are i.i.d.

(λe−λ)
t

and X ′

i

B

49

random variables with the same distribution as
1/2

ε > 0 is a constant to be determined.

≥
We ﬁrst prove that

Bt. Here η = 3d(V )/n and ε′ = min

η, ε
}
{

where

Pr[AV ]

≥

t(1−η)n/5

c1t−1/2
(cid:16)

(1−ε′)n·d(V )+o(n)

.

(cid:17)

To prove this, we deﬁne A′

V to be the event that

X (λe−λ)

1

, X (λe−λ)

2

, . . . , X (λe−λ)

(1−η)n, X ′

1, X ′

2, . . . , X ′

(η−ε′)n ∈

V.

By Corollary 13.6,

Now we show that

Pr[A′

V ]

≥

(1−ε′)n·d(V )+o(n)

.

t(1−η)n/5

c1t−1/2
(cid:16)
A′
Pr[AV |

(cid:17)
V ] = t−o(n).

According to the deﬁnition of discrete codimension d(V ), we have

By Corollary 13.6 we have

Pr[X

V ]

(1

−

≥

∈

O(1/n))

c1t−1/2

d(V )

.

(cid:16)

(cid:17)

Pr[X (λe−λ)

V ]

∈

≥

t1/5(1

−

On the other hand, by Lemma 13.2, we have

O(1/n))

c1t−1/2

d(V )

.

Thus,

Pr[X (λe−λ)

c2t−1/4

W ]

∈

≤

(cid:16)

(cid:16)

n−dim(W )

(cid:17)

.

(cid:17)

Pr[X (λe−λ)

X (λe−λ)

W

|

∈

V ]

∈

≤

(t1/5(1

−

O(1/n)))−1

which implies

√t/c1
(cid:16)

d(V )

·

(cid:16)

(cid:17)

c2t−1/4

n−dim(W )

(cid:17)

, . . . , X (λe−λ)

k+1

are independent

X (λe−λ)

1

, . . . , X (λe−λ)

k

are independent

A′
V

∧

Pr

X (λe−λ)
h

1

1

≥

(t1/5(1

√t/c1
(cid:16)
Using the estimation given above, for suﬃciently large t, we have

O(1/n)))−1

−

−

(cid:17)

(cid:17)

.

·

d(V )

n−k

|
c2t−1/4
(cid:16)

,

i

Pr

X (λe−λ)

1

, . . . , X (λe−λ)

(1−η)n are independent

since (1

−

η)n = n

−

h
3d(V ).

t−o(n)

A′
V

|

≥

i

50

Similarly, when ε′ < η, i.e., ε′ = ε, we have

Pr

(cid:2)
|

1
≥

1
≥

−

−

1

X (λe−λ)
X (λe−λ)

, . . . , X (λe−λ)
, . . . , X (λe−λ)

(1−η)n, X ′
(1−η)n, X ′

1

1, . . . , X ′
1, . . . , X ′
d(V )

O(1/n))−1

(1

−

√t/c1
(cid:16)
k+(ε−η)n

·

(cid:17)

c1t−1/2
(cid:16)

1
100

√t/c1

(cid:16)

(cid:17)

.

k+1 are independent

k are independent
n−k−(1−η)n

∧

AV

(cid:3)

(cid:17)

(d(V )

(ε

−

≤

o(1))n)

Again we have

Pr[X (λe−λ)

1

, X (λe−λ)

2

, . . . , X (λe−λ)

(1−η)n, X ′

1, X ′

2, . . . , X ′

(η−ε′)n are independent

A′
V ] = Pr[AV |

|

A′

V ] = t−o(n).

We deﬁne BV to be the event that X1, X2, . . . , Xn span the hyperplane V . Since AV and BV are
independent, we have

Pr[BV ] = Pr[AV ∧

BV ]/Pr[AV ]

Pr[AV ∧

≤

BV ]t−(1−η)n/5

√t/c1

(1−ε′)n·d(V )+o(n)

.

(15)

(cid:16)

(cid:17)

Consider a set

X (λe−λ)

1

, X (λe−λ)

2

, . . . , X (λe−λ)

(1−η)n, X ′

1, X ′

2, . . . , X ′

(η−ε′)n, X1, X2, . . . , Xn

which satisﬁes AV ∧

BV . There exist ε′n

1 vectors

−

Xj1, Xj2, . . . , Xjε

′

n−1

such that

X (λe−λ)

1

, X (λe−λ)

2

, . . . , X (λe−λ)

(1−η)n, X ′

1, X ′

2, . . . , X ′

(η−ε′)n, Xj1, Xj2, . . . , Xjε

′

n−1 span V .

= 2nh(ε′)+o(n), we can just assume ji = i. Here we use h(ε′)

By using a union bound of size
to denote the binary entropy function. Thus,

n
ε′n−1

Pr[AV ∧

BV ]

(cid:1)
(cid:0)
X (λe−λ)
, X (λe−λ)

1

2

2nh(ε′)+o(n)Pr

≤
Pr[Xε′n, Xε′n+1, . . . , Xn ∈

h

·

, . . . , X (λe−λ)

(1−η)n, X ′

1, X ′

2, . . . , X ′

(η−ε′)n, X1, X2, . . . , Xε′n−1 span V

V ].

i

Thus, by using (15) and Lemma 13.2 we have

Pr[BV ]

t−(1−η)n/5
X (λe−λ)

≤
Pr

√t/c1
(cid:16)
(cid:17)
, X (λe−λ)

2

1

(1−ε′)nd(V )+o(n)

, . . . , X (λe−λ)

·
1, X ′
(1−η)n, X ′

((1−ε′)n+1)d(V )

2nh(ε′)+o(n)

c1t−1/2
(cid:16)
2, . . . , X ′
(η−ε′)n, X1, X2, . . . , Xε′n−1 span V

(cid:17)

·

h

Notice that

.

i

Pr

X (λe−λ)

1

, X (λe−λ)

2

XV

h

, . . . , X (λe−λ)

(1−η)n, X ′

1, X ′

2, . . . , X ′

(η−ε′)n, X1, X2, . . . , Xε′n−1 span V

1.

≤

i

51

Thus, for any 1

d0 ≤

≤

(ε

−

o(1))n and suﬃciently large t, we have

Pr[BV ]

XV |d(V )=d0

≤

≤

≤

≤

≤

(1−ε′)nd0+o(n)

t−(1−η)n/5

√t/c1
(cid:16)

(cid:17)

t−(1−η)n/5+nh(ε′)/5−d0/5+o(n)
t(−1+3d0/n+h(ε)−d0/n)n/5+o(n)
t(−1+2ε+h(ε))n/5+o(n)
t−εn/5+o(n).

2nh(ε′)+o(n)

c1t−1/2

((1−ε′)n+1)d0

(cid:16)

(cid:17)

·

ε

tnh(ε′)/5 for suﬃciently large t. The third inequality
Here the second inequality follows since 2nh(ε′)
) on [0, 1/2] and the fact that 0 <
is due to the monotonicity of the binary entropy function h(
·
ε′
ε. The last inequality
follows by setting ε to be the solution of h(ε) + 3ε = 1. A numerical calculation shows that
ε > 0.177. Theorem 3.1 thus follows by using a union bound for all possible d0, which has at most
O(n2t) = to(n) diﬀerent valid values and setting C = ε/5.

1/2. The fourth inequality follows from the fact that d0/n

≤

≤

≤

≤

We remark that the choice of parameters here is mainly for simplicity and not optimized.

14 Discussion

The lens of communication complexity reveals surprising structure about well-known optimiza-
tion problems. A very interesting open question is to fully resolve the randomized communication
complexity of linear programming as a function of s, d, and L. Another interesting direction is
to design more eﬃcient linear programming algorithms in the RAM model with unit cost oper-
ations on words of size O(log(nd)) bits; such algorithms while being inherently useful may also
give rise to improved communication protocols. While our regression algorithms illustrated various
shortcomings of previous techniques, there are still interesting gaps in our bounds to be resolved.

References

[1] Alekh Agarwal, Olivier Chapelle, Miroslav Dud´ık, and John Langford. A reliable eﬀective
terascale linear learning system. Journal of Machine Learning Research, 15(1):1111–1133,
2014.

[2] Zeyuan Allen-Zhu and Elad Hazan. Optimal black-box reductions between optimization ob-
jectives. In Advances in Neural Information Processing Systems, pages 1614–1622, 2016.

[3] Alexandr Andoni. High frequency moments via max-stability. In Acoustics, Speech and Signal
Processing (ICASSP), 2017 IEEE International Conference on, pages 6364–6368. IEEE, 2017.

[4] Alexandr Andoni et al. Eigenvalues of a matrix in the streaming model. In Proceedings of
the twenty-fourth annual ACM-SIAM symposium on Discrete algorithms, pages 1729–1737.
Society for Industrial and Applied Mathematics, 2013.

52

[5] Alexandr Andoni, Piotr Indyk, and Mihai Patrascu. On the optimality of the dimensionality
reduction method. In Foundations of Computer Science, 2006. FOCS’06. 47th Annual IEEE
Symposium on, pages 449–458. IEEE, 2006.

[6] Yossi Arjevani and Ohad Shamir. Communication complexity of distributed convex learning
and optimization. In Advances in Neural Information Processing Systems 28: Annual Confer-
ence on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec,
Canada, pages 1756–1764, 2015.

[7] Sepehr Assadi, Nikolai Karpov, and Qin Zhang. Distributed and streaming linear programming
in low dimensions. In Proceedings of the 38th ACM SIGMOD-SIGACT-SIGAI Symposium on
Principles of Database Systems, pages 236–253. ACM, 2019.

[8] Sepehr Assadi and Sanjeev Khanna. Randomized composable coresets for matching and ver-
In Proceedings of the 29th ACM Symposium on Parallelism in Algorithms and

tex cover.
Architectures, SPAA 2017, Washington DC, USA, July 24-26, 2017, pages 3–12, 2017.

[9] Herman Auerbach. On the area of convex curves with conjugate diameters. PhD thesis, PhD

thesis, University of Lw´ow, 1930.

[10] Maria-Florina Balcan, Avrim Blum, Shai Fine, and Yishay Mansour. Distributed learning,
In COLT 2012 - The 25th Annual Conference on

communication complexity and privacy.
Learning Theory, June 25-27, 2012, Edinburgh, Scotland, pages 26.1–26.22, 2012.

[11] Maria-Florina Balcan, Yingyu Liang, Le Song, David P. Woodruﬀ, and Bo Xie. Communication
eﬃcient distributed kernel principal component analysis.
In Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco,
CA, USA, August 13-17, 2016, pages 725–734, 2016.

[12] D. Bertsimas and S. Vempala. Solving convex programs by random walks. J. ACM, 51(4):540–

556, 2004.

[13] P Bickel, P Diggle, S Fienberg, U Gather, I Olkin, and S Zeger. Springer series in statistics.

2009.

[14] Avrim Blum and John Dunagan. Smoothed analysis of the perceptron algorithm for linear
In Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete

programming.
algorithms, pages 905–914. Society for Industrial and Applied Mathematics, 2002.

[15] J. Bourgain, J. Lindenstrauss, and V. Milman. Approximation of zonoids by zonotopes. Acta

mathematica, 162(1):73–141, 1989.

[16] Jean Bourgain, Van H Vu, and Philip Matchett Wood. On the singularity probability of

discrete random matrices. Journal of Functional Analysis, 258(2):559–603, 2010.

[17] Christos Boutsidis, David P. Woodruﬀ, and Peilin Zhong. Optimal principal component anal-
ysis in distributed and streaming models. In Proceedings of the 48th Annual ACM SIGACT
Symposium on Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016,
pages 236–249, 2016.

53

[18] Stephen P. Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed
optimization and statistical learning via the alternating direction method of multipliers. Foun-
dations and Trends in Machine Learning, 3(1):1–122, 2011.

[19] Mark Braverman, Ankit Garg, Tengyu Ma, Huy L. Nguyen, and David P. Woodruﬀ. Com-
munication lower bounds for statistical estimation problems via a distributed data processing
inequality. In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Com-
puting, STOC 2016, Cambridge, MA, USA, June 18-21, 2016, pages 1011–1020, 2016.

[20] Emmanuel J Cand`es and Terence Tao. Decoding by linear programming. IEEE Transactions

on Information Theory, 51(12):4203–4215, 2005.

[21] Jiecao Chen, He Sun, David P. Woodruﬀ, and Qin Zhang. Communication-optimal distributed
clustering. In Advances in Neural Information Processing Systems 29: Annual Conference on
Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages
3720–3728, 2016.

[22] Kenneth L Clarkson. Las vegas algorithms for linear and integer programming when the

dimension is small. Journal of the ACM (JACM), 42(2):488–499, 1995.

[23] Kenneth L. Clarkson and David P. Woodruﬀ. Numerical linear algebra in the streaming model.
In Proceedings of the 41st Annual ACM Symposium on Theory of Computing, STOC 2009,
Bethesda, MD, USA, May 31 - June 2, 2009, pages 205–214, 2009.

[24] Kenneth L Clarkson and David P Woodruﬀ. Low rank approximation and regression in input
sparsity time. In Proceedings of the forty-ﬁfth annual ACM symposium on Theory of computing,
pages 81–90. ACM, 2013.

[25] M. B. Cohen, Y. Tat Lee, and Z. Song. Solving Linear Programs in the Current Matrix

Multiplication Time. In STOC, 2019.

[26] Michael B Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco, Richard Peng, and Aaron
Sidford. Uniform sampling for matrix approximation. In Proceedings of the 2015 Conference
on Innovations in Theoretical Computer Science, pages 181–190. ACM, 2015.

[27] Michael B Cohen and Richard Peng. ℓp row sampling by lewis weights. In Proceedings of the

forty-seventh annual ACM symposium on Theory of computing, pages 183–192. ACM, 2015.

[28] Graham Cormode, Charlie Dickens, and David P. Woodruﬀ. Leveraging well-conditioned
bases: Streaming and distributed summaries in minkowski p-norms. In Proceedings of the 35th
International Conference on Machine Learning, ICML 2018, Stockholmsm¨assan, Stockholm,
Sweden, July 10-15, 2018, pages 1048–1056, 2018.

[29] Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan. Better mini-batch al-
In Advances in neural information processing

gorithms via accelerated gradient methods.
systems, pages 1647–1655, 2011.

[30] Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online
prediction using mini-batches. Journal of Machine Learning Research, 13(Jan):165–202, 2012.

54

[31] John C Duchi, Alekh Agarwal, and Martin J Wainwright. Dual averaging for distributed
optimization: Convergence analysis and network scaling. IEEE Transactions on Automatic
control, 57(3):592–606, 2012.

[32] David Durfee, Kevin A Lai, and Saurabh Sawlani. ℓ1 regression using lewis weights precondi-
tioning and stochastic gradient descent. In Conference On Learning Theory, pages 1626–1656,
2018.

[33] Ankit Garg, Tengyu Ma, and Huy L. Nguyen. On communication cost of distributed statistical
estimation and dimensionality.
In Advances in Neural Information Processing Systems 27:
Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014,
Montreal, Quebec, Canada, pages 2726–2734, 2014.

[34] M. Grotschel, L. Lovsz, and A. Schrijver. Geometric algorithms and combinatorial optimiza-

tion. Springer, 1988.

[35] B. Grunbaum. Partitions of mass-distributions and convex bodies by hyperplanes. Paciﬁc J.

Math., 10:1257–1261, 1960.

[36] Dirk Van Gucht, Ryan Williams, David P. Woodruﬀ, and Qin Zhang. The communication
complexity of distributed set-joins with applications to matrix multiplication. In Proceedings
of the 34th ACM Symposium on Principles of Database Systems, PODS 2015, Melbourne,
Victoria, Australia, May 31 - June 4, 2015, pages 199–212, 2015.

[37] G Hal´asz. Estimates for the concentration function of combinatorial number theory and prob-

ability. Periodica Mathematica Hungarica, 8(3-4):197–211, 1977.

[38] G´abor Hal´asz. On the distribution of additive arithmetic functions. Acta Arithmetica,

1(27):143–152, 1975.

[39] David Harvey and Joris van der Hoeven. On the complexity of integer matrix multiplication.

Journal of Symbolic Computation, 89:1–8, 2018.

[40] Martin Jaggi, Virginia Smith, Martin Tak´ac, Jonathan Terhorst, Sanjay Krishnan, Thomas
Hofmann, and Michael I Jordan. Communication-eﬃcient distributed dual coordinate ascent.
In Advances in neural information processing systems, pages 3068–3076, 2014.

[41] Jeﬀ Kahn, J´anos Koml´os, and Endre Szemer´edi. On the probability that a random
is singular. Journal of the American Mathematical Society, 8(1):223–240, 1995.

±

1-matrix

[42] Daniel M Kane, Roi Livni, Shay Moran, and Amir Yehudayoﬀ. On communication complexity

of classiﬁcation problems. arXiv preprint arXiv:1711.05893, 2017.

[43] Ravi Kannan, Santosh Vempala, and David P. Woodruﬀ. Principal component analysis and
higher correlations for distributed data. In Proceedings of The 27th Conference on Learning
Theory, COLT 2014, Barcelona, Spain, June 13-15, 2014, pages 1040–1057, 2014.

[44] Eyal Kushilevitz and Noam Nisan. Communication Complexity. Cambridge University Press,

New York, NY, USA, 1997.

55

[45] Yin Tat Lee and Aaron Sidford. Path ﬁnding methods for linear programming: Solving linear
programs in ˜o(vrank) iterations and faster algorithms for maximum ﬂow. In 55th IEEE An-
nual Symposium on Foundations of Computer Science, FOCS 2014, Philadelphia, PA, USA,
October 18-21, 2014, pages 424–433, 2014.

[46] Yin Tat Lee and Aaron Sidford. Eﬃcient inverse maintenance and faster algorithms for linear
programming. In IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS
2015, Berkeley, CA, USA, 17-20 October, 2015, pages 230–249, 2015.

[47] Yingyu Liang, Maria-Florina Balcan, Vandana Kanchanapally, and David P. Woodruﬀ. Im-
proved distributed principal component analysis. In Advances in Neural Information Process-
ing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December
8-13 2014, Montreal, Quebec, Canada, pages 3113–3121, 2014.

[48] Dhruv Mahajan, S Sathiya Keerthi, S Sundararajan, and L´eon Bottou. A parallel sgd method

with strong convergence. arXiv preprint arXiv:1311.0636, 2013.

[49] Andreas Maurer. A bound on the deviation probability for sums of non-negative random

variables. J. Inequalities in Pure and Applied Mathematics, 4(1):15, 2003.

[50] Xiangrui Meng and Michael W. Mahoney. Low-distortion subspace embeddings in input-
sparsity time and applications to robust linear regression. In Symposium on Theory of Com-
puting Conference, STOC’13, Palo Alto, CA, USA, June 1-4, 2013, pages 91–100, 2013.

[51] Yu Nesterov. Smooth minimization of non-smooth functions. Mathematical programming,

103(1):127–152, 2005.

[52] Yurii Nesterov. A method of solving a convex programming problem with convergence rate o

(1/k2). In Soviet Mathematics Doklady, volume 27, pages 372–376, 1983.

[53] Andrew M Odlyzko. On subspaces spanned by random selections of

combinatorial theory, Series A, 47(1):124–133, 1988.

1 vectors.

journal of

±

[54] Jeﬀ M Phillips, Elad Verbin, and Qin Zhang. Lower bounds for number-in-hand multiparty
communication complexity, made easy. In Proceedings of the twenty-third annual ACM-SIAM
symposium on Discrete Algorithms, pages 486–501. SIAM, 2012.

[55] Peter Richt´arik and Martin Tak´aˇc. Distributed coordinate descent method for learning with

big data. The Journal of Machine Learning Research, 17(1):2657–2681, 2016.

[56] Arvind Sankar, Daniel A Spielman, and Shang-Hua Teng. Smoothed analysis of the condition
numbers and growth factors of matrices. SIAM Journal on Matrix Analysis and Applications,
28(2):446–476, 2006.

[57] Tamas Sarlos. Improved approximation algorithms for large matrices via random projections.
In Foundations of Computer Science, 2006. FOCS’06. 47th Annual IEEE Symposium on, pages
143–152. IEEE, 2006.

[58] Raimund Seidel. Linear programming and convex hulls made easy. In Proceedings of the sixth

annual symposium on Computational geometry, pages 211–215. ACM, 1990.

56

[59] Ohad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In Com-
munication, Control, and Computing (Allerton), 2014 52nd Annual Allerton Conference on,
pages 850–857. IEEE, 2014.

[60] Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-eﬃcient distributed optimization
using an approximate newton-type method. In International conference on machine learning,
pages 1000–1008, 2014.

[61] Daniel Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex
algorithm usually takes polynomial time. In Proceedings of the thirty-third annual ACM sym-
posium on Theory of computing, pages 296–305. ACM, 2001.

[62] Daniel A Spielman and Shang-Hua Teng. Smoothed analysis of termination of linear program-

ming algorithms. Mathematical Programming, 97(1-2):375–404, 2003.

[63] Terence Tao and Van Vu. On random

±
Structures & Algorithms, 28(1):1–23, 2006.

1 matrices: singularity and determinant. Random

[64] Terence Tao and Van Vu. On the singularity probability of random bernoulli matrices. Journal

of the American Mathematical Society, 20(3):603–628, 2007.

[65] John N Tsitsiklis and Zhi-Quan Luo. Communication complexity of convex optimization.

Journal of Complexity, 3(3):231–243, 1987.

[66] David P Woodruﬀ et al. Sketching as a tool for numerical linear algebra. Foundations and

Trends R
(cid:13)

in Theoretical Computer Science, 10(1–2):1–157, 2014.

[67] David P. Woodruﬀ and Qin Zhang. Subspace embeddings and ℓp-regression using exponential
random variables. In COLT 2013 - The 26th Annual Conference on Learning Theory, June
12-14, 2013, Princeton University, NJ, USA, pages 546–567, 2013.

[68] David P. Woodruﬀ and Qin Zhang. When distributed computation is communication expen-
sive. In Distributed Computing - 27th International Symposium, DISC 2013, Jerusalem, Israel,
October 14-18, 2013. Proceedings, pages 16–30, 2013.

[69] David P Woodruﬀ and Qin Zhang. An optimal lower bound for distinct elements in the message
passing model. In Proceedings of the twenty-ﬁfth annual ACM-SIAM symposium on Discrete
algorithms, pages 718–733. Society for Industrial and Applied Mathematics, 2014.

[70] David P. Woodruﬀ and Qin Zhang. Distributed statistical estimation of matrix products
with applications. In Proceedings of the 37th ACM SIGMOD-SIGACT-SIGAI Symposium on
Principles of Database Systems, Houston, TX, USA, June 10-15, 2018, pages 383–394, 2018.

[71] David P. Woodruﬀ and Peilin Zhong. Distributed low rank approximation of implicit func-
tions of a matrix. In 32nd IEEE International Conference on Data Engineering, ICDE 2016,
Helsinki, Finland, May 16-20, 2016, pages 847–858, 2016.

[72] Tianbao Yang. Trading computation for communication: Distributed stochastic dual coordi-
nate ascent. In Advances in Neural Information Processing Systems, pages 629–637, 2013.

57

[73] Yuchen Zhang, John C. Duchi, and Martin J. Wainwright. Communication-eﬃcient algorithms
for statistical optimization. Journal of Machine Learning Research, 14(1):3321–3363, 2013.

[74] Yuchen Zhang and Xiao Lin. Disco: Distributed optimization for self-concordant empirical

loss. In International conference on machine learning, pages 362–370, 2015.

[75] Martin Zinkevich, Markus Weimer, Alexander J. Smola, and Lihong Li. Parallelized stochastic
gradient descent.
In Advances in Neural Information Processing Systems 23: 24th Annual
Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9
December 2010, Vancouver, British Columbia, Canada., pages 2595–2603, 2010.

58

