2
2
0
2

n
u
J

1

]
I

A
.
s
c
[

2
v
8
9
6
3
0
.
1
0
2
2
:
v
i
X
r
a

Veriﬁed Probabilistic Policies
for Deep Reinforcement Learning

Edoardo Bacci and David Parker

University of Birmingham, Birmingham, United Kingdom
{exb461,d.a.parker}@bham.ac.uk

Abstract. Deep reinforcement learning is an increasingly popular tech-
nique for synthesising policies to control an agent’s interaction with its
environment. There is also growing interest in formally verifying that
such policies are correct and execute safely. Progress has been made
in this area by building on existing work for veriﬁcation of deep neu-
ral networks and of continuous-state dynamical systems. In this paper,
we tackle the problem of verifying probabilistic policies for deep rein-
forcement learning, which are used to, for example, tackle adversarial
environments, break symmetries and manage trade-oﬀs. We propose an
abstraction approach, based on interval Markov decision processes, that
yields probabilistic guarantees on a policy’s execution, and present tech-
niques to build and solve these models using abstract interpretation,
mixed-integer linear programming, entropy-based reﬁnement and prob-
abilistic model checking. We implement our approach and illustrate its
eﬀectiveness on a selection of reinforcement learning benchmarks.

1

Introduction

Reinforcement learning (RL) is a technique for training a policy used to govern
the interaction between an agent and an environment. It is based on repeated
explorations of the environment, which yield rewards that the agent should aim
to maximise. Deep reinforcement learning combines RL and deep learning, by
using neural networks to store a representation of a learnt reward function or
optimal policy. These methods have been increasingly successful across a wide
range of challenging application domains, including for example, autonomous
driving [30], robotics [19] and healthcare [49].

In safety critical domains, it is particularly important to assure that policies
learnt via RL will be executed safely, which makes the application of formal
veriﬁcation to this problem appealing. This is challenging, especially for deep
RL, since it requires reasoning about multi-dimensional, continuous state spaces
and complex policies encoded as deep neural networks.

There are several approaches to assuring safety in reinforcement learning,
often leveraging ideas from formal veriﬁcation, such as the use of temporal logic
to specify safety conditions, or the use of abstract interpretation to build dis-
cretised models. One approach is shielding (e.g., [1]), which synthesises override
mechanisms to prevent the RL agent from acting upon bad decisions; another is

 
 
 
 
 
 
constrained or safe RL (e.g. [17]), which generates provably safe policies, typi-
cally by restricting the training process to safe explorations.

An alternative approach, which we take in this paper, is to verify an RL
policy’s correctness after it has been learnt, rather than placing restrictions on
the learning process or on its deployment. Progress has been made in the formal
veriﬁcation of policies for RL [6] and also for the speciﬁc case of deep RL [28,3,4],
in the latter case by building on advances in abstraction and veriﬁcation tech-
niques for neural networks; [3] also exploits the development of eﬃcient abstract
domains such as template polyhedra [42], previously applied to the veriﬁcation
of continuous-space and hybrid systems [7,16].

A useful tool in reinforcement learning is the notion of a probabilistic pol-
icy (or stochastic policy), which chooses randomly between available actions in
each state, according to a probability distribution speciﬁed by the policy. This
brings a number of advantages (similarly to mixed strategies [39] in game the-
ory and contextual bandits [34]), such as balancing the exploration-exploitation
tradeoﬀ [18], dealing with partial observability of the environment [40], handling
multiple objectives [47] or learning continuous actions [38].

In this paper, we tackle the problem of verifying the safety of probabilistic
policies for deep reinforcement learning. We deﬁne a formal model of their exe-
cution using (continuous-state, ﬁnite-branching) discrete-time Markov processes.
We then build and solve sound abstractions of these models. This approach was
also taken in earlier work [4], which used Markov decision process abstractions
to verify deep RL policies in which actions may exhibit failures.

However, a particular challenge for probabilistic policies, as generated by
deep RL, is that policies tend to specify very diﬀerent action distributions across
states. We thus propose a novel abstraction based on interval Markov decision
processes (IMDPs), in which transitions are labelled with intervals of probabil-
ities, representing the range of possible events that can occur. We solve these
IMDPs, over a ﬁnite time horizon, which we show yields probabilistic guarantees,
in the form of upper bounds on the actual probability of the RL policy leading
the agent to a state designated to be unsafe.

We present methods to construct IMDP abstractions using template poly-
hedra as an abstract domain, and mixed-integer linear programming (MILP) to
reason symbolically about the neural network policy encoding and a model of
the RL agent’s environment. We extend existing MILP-based methods for neu-
ral networks to cope with the softmax encoding used for probabilistic policies.
Naive approaches to constructing these IMDPs yield abstractions that are too
coarse, i.e., where the probability intervals are too wide and the resulting safety
probability bounds are too high be useful. So, we present an iterative reﬁne-
ment approach based on sampling which splits abstract states via cross-entropy
minimisation based on the uncertainty of the over-approximation.

We implement our techniques, building on an extension of the probabilistic
model checker PRISM [32] to solve IMDPs. We show that our approach suc-
cessfully veriﬁes probabilistic policies trained for several reinforcement learning
benchmarks and explore trade-oﬀs in precision and computational eﬃciency.

2

Related work. As discussed above, other approaches to assuring safety in re-
inforcement learning include shielding [1,5,52,31,25] and constrained or safe RL
[17,21,13,45,22,37,26,23]. By contrast, we verify policies independently, without
limiting the training process or imposing constraints on execution.

Formal veriﬁcation of RL, but in a non-probabilistic setting includes: [6],
which extracts and analyses decision trees; [28], which checks safety and liveness
properties for deep RL; and [3], which also uses template polyhedra and MILP
to build abstractions, but to check (non-probabilistic) safety invariants.

In the probabilistic setting, perhaps closest is our earlier work [4], which
uses abstraction for ﬁnite-horizon probabilistic veriﬁcation of deep RL, but for
non-probabilistic policies, thus using a simpler (MDP) abstraction, as well as a
coarser (interval) abstract domain and a diﬀerent, more basic approach to re-
ﬁnement. Another approach to generating formal probabilistic guarantees is [14],
which, unlike us, does not need a model of the environment and instead learns
an approximation and produces probably approximately correct (PAC) guaran-
tees. Probabilistic veriﬁcation of neural network policies on partially observable
models, but for discrete state spaces, was considered in [10].

There is also a body of work on verifying continuous space probabilistic
models and stochastic hybrid systems, by building ﬁnite-state abstractions as,
e.g., interval Markov chains [33] or interval MDPs [36,11], but these do not
consider control policies encoded as neural networks. Similarly, abstractions of
discrete-state probabilistic models use similar ideas to our approach, notably via
the use of interval Markov chains [15] and stochastic games [27].

2 Background

We ﬁrst provide background on the two key probabilistic models used in this
paper: discrete-time Markov processes (DTMPs), used to model RL policy exe-
cutions, and interval Markov decision processes (IMDPs), used for abstractions.

Notation. We write Dist(X) for the set of discrete probability distributions
over a set X, i.e., functions µ : X → [0, 1] where (cid:80)
x∈X µ(x) = 1. The support
of µ, denoted supp(µ), is deﬁned as supp(µ) = {x ∈ X | µ(x) > 0}. We use the
same notation where X is uncountable but where µ has ﬁnite support. We write
P(X) to denote the powerset of X and vi for the ith element of a vector v.

Deﬁnition 1 (Discrete-time Markov process). A (ﬁnite-branching) discrete-
time Markov process is a tuple (S, S0, P, AP , L), where: S is a (possibly uncount-
ably inﬁnite) set of states; S0 ⊆ S is a set of initial states; P : S × S → [0, 1] is
a transition probability matrix, where (cid:80)
s(cid:48)∈supp(P(s,·)) P(s, s(cid:48)) = 1 for all s ∈ S;
AP is a set of atomic propositions; and L : S → P(AP ) is a labelling function.

A DTMP begins in some initial state s0 ∈ S0 and then moves between states
at discrete time steps. From state s, the probability of making a transition to
state s(cid:48) is P(s, s(cid:48)). Note that, although the state space of DTMPs used here is
continuous, each state only has a ﬁnite number of possible successors. This is

3

always true for our models (where transitions represent policies choosing between
a ﬁnite number of actions) and simpliﬁes the model.

A path through a DTMP is an inﬁnite sequence of states s0s1s2 . . . such that
P(si, si+1) > 0 for all i. The set of all paths starting in state s is denoted Path(s)
and we deﬁne a probability space Pr s over Path(s) in the usual way [29]. We use
atomic propositions (from the set AP ) to label states of interest for veriﬁcation,
e.g., to denote them as safe or unsafe. For b ∈ AP , we write s |= b if b ∈ L(s).
The probability of reaching a b-labelled state from s within k steps is:

Pr s(♦(cid:54)kb) = Pr s({s0s1s2 · · · ∈ Path(s) | si |= b for some 0 (cid:54) i (cid:54) k})

which, since DTMPs are ﬁnite-branching models, can be computed recursively:

Pr s(♦(cid:54)kb) =






1
0

if s |= b
if s (cid:54)|= b ∧ k=0

(cid:80)

s(cid:48)∈supp(P(s,·)) P(s, s(cid:48)) · Pr s(cid:48)(♦(cid:54)k−1b) otherwise.

To build abstractions, we use interval Markov decision processes (IMDPs).

Deﬁnition 2 (Interval Markov decision process). An interval Markov de-
cision process is a tuple (S, S0, P, AP , L), where: S is a ﬁnite set of states;
S0 ⊆ S are initial states; P : S×N×S → (I∪0) is the interval transition probabil-
ity function, where I is the set of probability intervals I = {[a, b] | 0 < a (cid:54) b (cid:54) 1},
assigning either a probability interval or the probability exactly 0 to any transi-
tion; AP is a set of atomic propositions; and L:S→P(AP ) is a labelling function.

Like a DTMP, an IMDP evolves through states in a state space S, starting
from an initial state s0 ∈ S0. In each state s ∈ S, an action j must be chosen.
Because of the way we use IMDPs, and to avoid confusion with the actions taken
by RL policies, we simply use integer indices j ∈ N for actions. The probability
of moving to each successor state s(cid:48) then falls within the interval P(s, j, s(cid:48)).

To reason about IMDPs, we use policies, which resolve the nondeterminism
in terms of actions and probabilities. A policy σ of the IMDP selects the choice
to take in each state, based on the history of its execution so far. In addition,
we have a so-called environment policy τ which selects probabilities for each
transition that fall within the speciﬁed intervals. For a policy σ and environment
policy τ , we have a probability space Pr σ,τ
over the set of inﬁnite paths starting
(♦(cid:54)kb) of
in state s. As above, we can deﬁne, for example, the probability Pr σ,τ
reaching a b-labelled state from s within k steps, under σ and τ .

s

s

If ψ is an event of interest deﬁned by a measurable set of paths (e.g., ♦(cid:54)kb),
we can compute (through robust value iteration [48]) lower and upper bounds
on, e.g., maximum probabilities, over the set of all allowable probability values:

Pr max min

s

(ψ) = sup

σ

inf
τ

Pr σ,τ
s

(ψ)

and Pr max,max

s

(ψ) = sup

σ

Pr σ,τ
s

(ψ)

sup
τ

4

3 Modelling and Abstraction of Reinforcement Learning

We begin by giving a formal deﬁnition of our model for the execution of a
reinforcement learning system, under the control of a probabilistic policy. We
also deﬁne the problem of verifying that this policy is executed safely, namely
that the probability of visiting an unsafe system state, within a speciﬁed time
horizon, is below an acceptable threshold.

Then we deﬁne abstractions of these models, given an abstract domain over
the states of the model, and show how an analysis of the resulting abstraction
yields probabilistic guarantees in the form of sound upper bounds on the prob-
ability of a failure occurring. In this section, we make no particular assumption
about the representation of the policy, nor about the abstract domain.

3.1 Modelling and Veriﬁcation of Reinforcement Learning

Our model takes the form of a controlled dynamical system over a continuous
n-dimensional state space S ⊆ Rn, assuming a ﬁnite set of actions A performed
at discrete time steps. A (time invariant) environment E : S × A → S describes
the eﬀect of executing an action in a state, i.e., if st is the state at time t and
at is the action taken in that state, we have st+1 = E(st, at).

We assume a reinforcement learning system is controlled by a probabilistic
policy, i.e., a function of the form π : S → Dist(A), where π(s)(a) speciﬁes
the probability with which action a should be taken in state s. Since we are
interested in verifying the behaviour of a particular policy, not in the problem of
learning such a policy, we ignore issues of partial observability. We also do not
need to include any deﬁnition of rewards.

Furthermore, since our primary interest here is in the treatment of proba-
bilistic policies, we do not consider other sources of stochasticity, such as the
agent’s perception of its state or the environment’s response to an action. Our
model could easily be extended with other discrete probabilistic aspects, such as
the policy execution failure models considered in [4].

Combining all of the above, we deﬁne an RL execution model as a (continuous-
space, ﬁnite-branching) discrete-time Markov process (DTMP). In addition to a
particular environment E and policy π, we also specify a set S0 ⊆ S of possible
initial states and a set Sfail ⊆ S of failure states, representing unsafe states.

Deﬁnition 3 (RL execution model). Assuming a state space S ⊆ Rn and
action set A, and given an environment E : S × A → S, policy π : S → Dist(A),
initial states S0 ⊆ S and failure states Sfail ⊆ S, the corresponding RL execution
model is the DTMP (S, S0, P, AP , L) where AP = {fail }, for any s ∈ S, fail ∈
L(s) iﬀ s ∈ Sfail and, for states s, s(cid:48) ∈ S:

P(s, s(cid:48)) =

(cid:88)

{π(s)(a) | a ∈ A s.t. E(s, a) = s(cid:48)} .

The summation in Deﬁnition 3 is required since distinct actions a and a(cid:48) applied
in state s could result in the same successor state s(cid:48).

5

Then, assuming the model above, we deﬁne the problem of verifying that an
RL policy executes safely. We consider a ﬁxed time horizon k ∈ N and an error
probability threshold psafe , and the check that the probability of reaching an
unsafe state within k time steps is always (from any start state) below psafe .

Deﬁnition 4 (RL veriﬁcation problem). Given a DTMP model of an RL
execution, as in Deﬁnition 3, a time horizon k ∈ N and a threshold psafe ∈ [0, 1],
the RL veriﬁcation problem is to check that Pr s(♦(cid:54)kfail ) (cid:54) psafe for all s ∈ S0.

In practice, we often tackle a numerical version of the veriﬁcation problem,
and instead compute the worst-case probability of error for any start state p+ =
inf{Pr s(♦(cid:54)kfail ) | s ∈ S0} or (as we do later) an upper bound on this value.

3.2 Abstractions for Veriﬁcation of Reinforcement Learning

Because our models of RL systems are over continuous state spaces, in order to
verify them in practice, we construct ﬁnite abstractions. These represent an over-
approximation of the original model, by grouping states with similar behaviour
into abstract states, belonging to some abstract domain ˆS ⊆ P(S).

Such abstractions are usually necessarily nondeterministic since an abstract
state groups states with similar, but distinct, behaviour. For example, abstrac-
tion of a probabilistic model such as a discrete-time Markov process could be
captured as a Markov decision process [4]. However, a further source of com-
plexity for abstracting probabilistic policies, especially those represented as deep
neural networks, is that states can also vary widely with regards to the proba-
bilities with which policies select actions in those states.

So, in this work we represent abstractions as interval MDPs (IMDPs), in
which transitions are labelled with intervals, representing a range of diﬀerent
possible probabilities. We will show that solving the IMDP (i.e., computing the
maximum ﬁnite-horizon probability of reaching a failure state) yields an upper
bound on the corresponding probability for the model being abstracted.

Below, we deﬁne this abstraction and state its correctness, ﬁrst focusing
separately on abstractions of an RL system’s environment and policy, and then
combining these into a single IMDP abstraction.

Assuming an abstract domain ˆS ⊆ P(S), we ﬁrst require an environment ab-
straction ˆE : ˆS × A → ˆS, which soundly over-approximates the RL environment
E : S × A → S, as follows.

Deﬁnition 5 (Environment abstraction). For environment E : S × A → S
and set of abstract states ˆS ⊆ P(S), an environment abstraction is a function
ˆE : ˆS × A → ˆS such that: for any abstract state ˆs ∈ ˆS, concrete state s ∈ ˆs and
action a ∈ A, we have E(s, a) ∈ ˆE(ˆs, a).

Additionally, we need, for any RL policy π, a policy abstraction ˆπ, which gives
a lower and upper bound on the probability with which each action is selected
within the states grouped by each abstract state.

6

Deﬁnition 6 (Policy abstraction). For a policy π : S → Dist(A) and a set
of abstract states ˆS ⊆ P(S), a policy abstraction is a pair (ˆπL, ˆπU ) of functions
of the form ˆπL : ˆS × A → [0, 1] and ˆπU : ˆS × A → [0, 1], satisfying the following:
for any abstract state ˆs ∈ ˆS, concrete state s ∈ ˆs and action a ∈ A, we have
ˆπL(ˆs, a) (cid:54) π(s, a) (cid:54) ˆπU (ˆs, a).

Finally, combining these notions, we can deﬁne an RL execution abstraction,
which is an IMDP abstraction of the execution of an policy in an environment.

Deﬁnition 7 (RL execution abstraction). Let E and π be an RL environ-
ment and policy, DTMP (S, S0, P, AP , L) be the corresponding RL execution
model and ˆS ⊆ P(S) be a set of abstract states. Given also a policy abstraction
ˆπ of π and an environment abstraction ˆE of E, an RL execution abstraction is
an IMDP ( ˆS, ˆS0, ˆP, AP , ˆL) satisfying the following:

– for all s ∈ S0, s ∈ ˆs for some ˆs ∈ ˆS0;
– for each ˆs ∈ ˆS, there is a partition {ˆs1, . . . , ˆsm} of ˆs such that, for each

j ∈ {1, . . . , m} we have ˆP(ˆs, j, ˆs(cid:48)) = [ ˆPL(ˆs, j, ˆs(cid:48)), ˆPU (ˆs, j, ˆs(cid:48))] where:

ˆPL(ˆs, j, ˆs(cid:48)) = (cid:80) (cid:110)
ˆPU (ˆs, j, ˆs(cid:48)) = (cid:80) (cid:110)

ˆπL(ˆsj, a) | a ∈ A s.t. ˆE(ˆsj, a) = ˆs(cid:48)(cid:111)
ˆπU (ˆsj, a) | a ∈ A s.t. ˆE(ˆsj, a) = ˆs(cid:48)(cid:111)

– AP = {fail } and fail ∈ ˆL(ˆs) iﬀ fail ∈ L(s) for some s ∈ ˆs.

Intuitively, each abstract state ˆs is partitioned into groups of states ˆsj that be-
have the same under the speciﬁed environment and policy abstractions. The
nondeterministic choice between actions j ∈ {1, . . . , m} in abstract state ˆs, each
of which corresponds to the state subset ˆsj, allows the abstraction to overap-
proximate the behaviour of the original DTMP model.

Finally, we state the correctness of the abstraction, i.e., that solving the
IMDP provides upper bounds on the probability of policy execution resulting in
a failure. This is formalised as follows (see the appendix for a proof).

Theorem 1. Given a state s ∈ S of an RL execution model DTMP, and an
abstract state ˆs ∈ ˆS of the corresponding abstraction IMDP for which s ∈ ˆs:

Pr s(♦(cid:54)kfail ) (cid:54) Pr max max

ˆs

(♦(cid:54)kfail ).

In particular, this means that we can tackle the RL veriﬁcation problem of
checking that the error probability is below a threshold psafe for all possible
start states (see Deﬁnition 4). We can do this by ﬁnding an abstraction for
which Pr max max

(♦(cid:54)kfail ) (cid:54) psafe for all initial abstract states ˆs ∈ ˆS0.

Although Pr max min

(♦(cid:54)kfail ) is not necessarily a lower bound on the failure

ˆs

ˆs

probability, the value may still be useful to guide abstraction reﬁnement.

7

4 Template-based Abstraction of Neural Network Policies

We now describe in more detail the process for constructing an IMDP abstrac-
tion, as given in Deﬁnition 7, to verify the execution of an agent with its environ-
ment, under the control of a probabilistic policy. We assume that the policy is
encoded in neural network form and has already been learnt, prior to veriﬁcation,
and we use template polyhedra to represent abstract states.

The overall process works by building a k-step unfolding of the IMDP, start-
ing from a set of initial states ˆS0 ⊆ S. For each abstract state ˆs explored during
this process, we need to split ˆs into an appropriate partition {ˆs1, . . . , ˆsm}. Then,
for each ˆsj ∈ ˆs and each action a ∈ A, we determine lower and upper bounds on
the probabilities with which a is selected in states in ˆsj, i.e., we construct a policy
abstraction (ˆπL, ˆπU ). We also ﬁnd the successor abstract state that results from
executing a in ˆsj, i.e., we build an environment abstraction ˆE. Construction of
the IMDP then follows directly from Deﬁnition 7.

In the following sections, we describe our techniques in more detail. First,
we give brief details of the abstract domain used: bounded polyhedra. Next, we
describe how to construct policy abstractions via MILP. Lastly, we describe how
to partition abstract states via reﬁnement. We omit details of the environment
abstraction since we reuse the symbolic post operator over template polyhedra
given in [3], also performed with MILP. This supports environments speciﬁed
as linear, piecewise linear or non-linear systems deﬁned with polynomial and
transcendental functions. The latter is dealt with using linearisation, subdividing
into small intervals and over-approximating using interval arithmetic.

Further details of the algorithms in this section can be found in [2].

4.1 Bounded Template Polyhedra

Recall that the state space of our model S ⊆ Rn is over n real-valued variables.
We represent abstract states using template polyhedra [42], which are convex
subsets of Rn, deﬁned by constraints in a ﬁnite set of directions ∆ ⊂ Rn (in
other words, the facets of the polyhedra are normal to the directions in ∆). We
call a ﬁxed set of directions ∆ ⊂ Rn a template.

Given a (convex) abstract state ˆs ⊆ Rn, a ∆-polyhedron of ˆs is deﬁned as

the tightest ∆-polyhedron enclosing ˆs:

∩{{s : (cid:104)δ, s(cid:105) (cid:54) sup{(cid:104)δ, s(cid:105) : s ∈ ˆs}} : δ ∈ ∆},

where (cid:104)·, ·(cid:105) denotes scalar product. In this paper, we restrict our attention to
bounded template polyhedra (also called polytopes), in which every variable in
the state space is bounded by a direction of the template, since this is needed
for our reﬁnement scheme.

Important special cases of template polyhedra are rectangles (i.e., intervals)
and octagons. Later, in Section 5, we will present an empirical comparison of
these diﬀerent abstract domains applied to our setting, and show the beneﬁts of
the more general case of template polyhedra.

8

4.2 Constructing Policy Abstractions

We focus ﬁrst on the abstraction of the RL policy π : S → Dist(A), assuming
there are k actions: A = {a1, . . . , ak}. Let π be encoded by a neural network
comprising n input neurons, l hidden layers, each containing hi neurons (1 (cid:54)
i (cid:54) l), and k output neurons, and using ReLU activation functions.

The policy is encoded as follows. We use variable vectors z0 . . . , zl+1 to denote
the values of the neurons at each layer. The current state of the environment is
fed to the input layer z0, each hidden layer’s values are as follows:

zi = ReLU(Wizi−1 + bi) for i = 1, . . . , l

and the output layer is zl+1 = Wl+1zl, where each Wi is a matrix of weights
connecting layers i−1 and i and each bi is a vector of biases. In the usual fash-
ion, ReLU(z) = max(z, 0). Finally, the k output neurons yield the probability
assigned by the policy to each action. More precisely, the probability that the
encoded policy selects action aj is given by pj based on a softmax normalisation
of the output layer:

pj = softmax(zl+1)j =

l+1

ezj
i=1 ezi

(cid:80)k

l+1

For an abstract state ˆs, we compute the policy abstraction, i.e., lower and upper
bounds ˆπL(ˆs, aj) and ˆπU (ˆs, aj) for all actions aj (see Deﬁnition 6), via mixed-
integer linear programming (MILP), building on existing MILP encodings of
neural networks [46,12,9]. The probability bounds cannot be directly computed
via MILP due to the nonlinearity of the softmax function so, as a proxy, we
maximise the corresponding entry (the jth logit) of the output layer (l+1). For
the upper bound (the lower bound is computed analogously), we optimise:

maximize
subject to

zj
l+1
z0 ∈ ˆs,
0 (cid:54) zi − Wizi−1 − bi (cid:54) M z(cid:48)
i
0 (cid:54) zi (cid:54) M − M z(cid:48)
i
(cid:54) 1
0 (cid:54) z(cid:48)
i
zl+1 = Wl+1zl,

for i = 1, . . . , l,
for i = 1, . . . , l,
for i = 1, . . . , l,

(1)

over the variables z0 ∈ Rn, zl+1 ∈ Rk and zi ∈ Rhi, z(cid:48)

i ∈ Zhi for 1 (cid:54) i (cid:54) l.

Since abstract state ˆs is a convex polyhedron, the initial constraint z0 ∈ ˆs
on the vector of values z0 fed to the input layer is represented by |∆| linear
inequalities. ReLU functions are modelled using a big-M encoding [46], where
i and M ∈ R is a constant representing an
we add integer variable vectors z(cid:48)
upper bound for the possible values of neurons.

We solve 2k MILPs to obtain lower and upper bounds on the logits for
all k actions. We then calculate bounds on the probabilities of each action by
combining these values as described below. Since the exponential function in

9

softmax is monotonic, it preserves the order of the intervals, allowing us to
compute the bounds on the probabilities achievable in ˆs.

Let xlb,i and xub,i denote the lower and upper bounds, respectively, obtained
l+1 in (1) above). Then,

for each action ai via MILP (i.e., the optimised values zi
the upper bound for the probability of choosing action aj is yub,j:

yub,j = softmax(zub,j)

where

zi
ub,j =

(cid:26) xub,i

if i = j

1 − xlb,i otherwise

and where zub,j is an intermediate vector of size k. Again, the computation for
the lower bound is performed analogously.

4.3 Reﬁnement of Abstract States

As discussed above, each abstract state ˆs in the IMDP is split into a partition
{ˆs1, . . . , ˆsm} and, for each ˆsi, the probability bounds ˆπL(ˆsi, a) and ˆπU (ˆsi, a) are
determined for each action a. If these intervals are two wide, the abstraction is
too coarse and the results uninformative. To determine a good partition (i.e., one
that groups states with similar behaviour in terms of the probabilities chosen by
the policy), we use reﬁnement, repeatedly splitting ˆsi into ﬁner partitions.
(ˆsi), as:

We deﬁne the maximum probability spread of ˆsi, denoted ∆max

ˆπ

∆max
ˆπ

(ˆsi) = max
a∈A

(ˆπU (ˆsi, a) − ˆπL(ˆsi, a))

(ˆsi) falls below a speciﬁed threshold φ. Varying φ

and we reﬁne ˆsi until ∆max
ˆπ
allows us to tune the desired degree of precision.
When reﬁning, our aim is minimise ∆max

(ˆsi), i.e., to group areas of the
state space that have similar probability ranges, but also to minimise the num-
ber of splits performed. We try to ﬁnd a good compromise between improving
the accuracy of the abstraction and reducing partition growth, which generates
additional abstract states and increases the size of the IMDP abstraction.

ˆπ

ˆπ

Calculating the range ∆max

(ˆsi) can be done by using MILP to compute each
of the lower and upper bounds ˆπL(ˆsi, a) and ˆπU (ˆsi, a). However, this may be
time consuming. So, during the ﬁrst part of reﬁnement for each abstract state,
we sample probabilities for some states to compute an underestimate of the true
range. If the sampled range is already wide enough to trigger further reﬁnement,
we do so; otherwise we calculate the exact range of probabilities using MILP to
check whether there is a need for further reﬁnement.

Each reﬁnement step comprises three phases, described in more detail below:
(i) sampling policy probabilities; (ii) selecting a direction to split; (iii) splitting.
Figure 1 gives an illustrative example of a full reﬁnement.

Sampling the neural network policy. We ﬁrst generate a sample of the
probabilities chosen by the policy within the abstract state. Since this is a con-
vex region, we sample state points within it randomly using the Hit & Run
method [44]. We then obtain, from the neural network, the probabilities of pick-
ing actions at each sampled state. We consider each action a separately, and

10

Fig. 1: Sampled policy probabilities for one action in an abstract state (left)
and the template polyhedra partition generated through reﬁnement (right).

then later split according to the most promising one (i.e., with the widest prob-
ability spread across all actions). The probabilities for each a are computed in
a one-vs-all fashion: we generate a point cloud representing the probability of
taking that action as opposed to any other action.

The number of samples used (and hence the time needed) is kept ﬁxed,
rather than ﬁxing the density of the sampled points. We sample 1000 points per
abstract state split but this parameter can be tuned depending on the machine
and the desired time/accuracy tradeoﬀ. This ensures that ever more accurate
approximations are generated as the size of the polyhedra decreases.

Choosing candidate directions. We reﬁne abstract states (represented as
template polyhedra) by bisecting them along a chosen direction from the set ∆
used to deﬁne them. Since the polyhedra are bounded, we are free to pick any one.
To ﬁnd the direction that contributes most to reducing the probability spread,
we use cross-entropy minimisation to ﬁnd the optimal boundary at which to split
each direction, and then pick the direction that yields the lowest value.

Let ˜S be the set of sampled points and ˜Ys denote the true probability of
choosing action a in each point s ∈ ˜S, as extracted from the probabilistic policy.
For a direction δ, we project all points in ˜S onto δ and sort them accordingly,
i.e., we let ˜S = {s1, . . . , sm}, where m = | ˜S| and index i is sorted by (cid:104)δ, si(cid:105).
We determine the optimal boundary for splitting in direction δ by ﬁnding the
optimal index k that splits ˜S into {s1, . . . , sk} and {sk+1, . . . , sm}. To do so, we
ﬁrst deﬁne the function Y k,δ

classifying the ith point according to this split:

i

Y k,δ
i =

(cid:26) 1 if i (cid:54) k
0 if i > k

and then minimise, over k, the binary cross entropy loss function:

H(Y k,δ, ˜Y ) = −

1
m

(cid:88)m

i=1

(cid:16)
Y k,δ
i

log( ˜Ysi ) + (1 − Y k,δ

i

(cid:17)
) log(1 − ˜Ysi)

11

1.00.250.00.50.75which reﬂects how well the true probability for each point ˜Ys matches the sepa-
ration into the two groups.

One problem with this approach is that, if the distribution of probabilities
is skewed to strongly favour some probabilities, a good decision boundary may
not be picked. To counter this, we perform sample weighting by grouping the
sampled probabilities into small bins, and counting the number of samples in
each bin to calculate how much weight to give to each sample.

Abstract state splitting. Once a direction δ and bisection point sk are chosen,
the abstract state is split into two with a corresponding pair of constraints that
splits the polyhedron. Because we are constrained to the directions of the tem-
plate, and the decision boundary is highly non-linear, sometimes the bisection
point falls close to the interval boundary and the resulting slices are extremely
thin. This would cause the creation of an unnecessarily high number of polyhe-
dra, which we prevent by imposing a minimum size of the split relative to the
dimension chosen. By doing so we are guaranteed a minimum degree of progress
and the complex shapes in the non-linear policy space which are not easily classi-
ﬁed (such as non-convex shapes) are broken down into more manageable regions.

5 Experimental Evaluation

We evaluate our approach by implementing the techniques described in Sec-
tion 4 and applying them to 3 reinforcement learning benchmarks, analysing
performance and the impact of various conﬁgurations and optimisations.

5.1 Experimental Setup

Implementation. The code is developed in a mixture of Python and Java. Neu-
ral network manipulation is done through Pytorch [51], MILP solution through
Gurobi [20], graph analysis with networkX [50] and cross-entropy minimisation
with Scikit-learn [41]. IMDPs are constructed and solved using an extension of
PRISM [32] which implements robust value iteration [48]. The code is available
from https://github.com/phate09/SafeDRL.

Benchmarks. We use the following three RL benchmark environments:

(i) Bouncing ball
[24]: The agent controls a ball with height p and vertical
velocity v, choosing to either hit the ball downward with a paddle, adding speed,
or do nothing. The ball accelerates while falling and bounces on the ground losing
10% of its energy; it eventually stops bouncing if its height is too low and it is out
of reach of the paddle. The initial heights and speed vary. In our experiments,
we consider two possible starting regions: “large” (S0 = L), where p ∈ [5, 9] and
v ∈ [−1, 1], and “small” (S0 = S), where p ∈ [5, 9] and v ∈ [−0.1, 0]. The safety
constraint is that the ball never stops bouncing.

(ii) Adaptive cruise control [3]: The problem has two vehicles i ∈ {lead , ego},
whose state is determined by variables xi and vi for the position and speed of

12

(a) Intervals: |ˆs| = 450

(b) Octagons: |ˆs| = 334

(c) Templates: |ˆs| = 25

Fig. 2: Policy abstractions for an abstract state from the adaptive cruise con-
trol benchmark, using diﬀerent abstract domains (see Figure 1 for legend).

each car, respectively. The lead car proceeds at constant speed (28 m s−1), and
the agent controls the acceleration (±1 m s−2) of ego using two actions. The
range of possible start states allows a relative distance of [3, 10] metres and the
speed of the ego vehicle is in [26, 32] m/s. Safety means preserving xlead (cid:62) xego.

(iii) Inverted pendulum: This benchmark is a modiﬁed (discrete action) version
of the “Pendulum-v0” environment from the OpenAI Gym [8] where an agent
applies left or right rotational force to a pole pivoting around one of its ends,
with the aim of balancing the pole in an upright position. The state is modelled
by 2 variables: the angular position and velocity of the pole. We consider initial
conditions of an angle [−0.05, 0.05] and speed [−0.05, 0.05]. Safety constitutes
remaining within a range of positions and velocities such that an upright position
can be recovered. This benchmark is more challenging than the previous two: it
allows 3 actions (noop, push left, push right) and the dynamics of the system
are highly non-linear, making the problem more complex.

Policy training. All agents have been trained using proximal policy optimisa-
tion (PPO) [43] in actor-critic conﬁguration with Adam optimiser. The training
is distributed over 8 actors with 10 instances of each environment, managing the
collection of results and the update of the network with RLlib [35]. Hyperpa-
rameters have been mostly kept unchanged from their default values except the
learning rate and batch size which have been set to 5×10−4 and 4096, respec-
tively. We used a standard feed forward architecture with 2 hidden layers (size
32 for the bouncing ball and size 64 for the adaptive cruise control and inverted
pendulum problems) and ReLU activation functions.

Abstract domains. The abstraction techniques we present in Section 4 are
based on the use of template polyhedra as an abstract domain. As special cases,
this includes rectangles (intervals) and octagons. We use both of these in our
experiments, but also the more general case of arbitrary bounded template poly-
hedra. In the latter case, we choose a set of directions by sampling a represen-
tative portion of the state space where the agent is expected to operate, and

13

Benchmark
environment

k

Abs.
dom.

Bouncing
ball (S0 = S)

20 Rect
20 Oct

Bouncing
ball (S0 = L)

20 Rect
20 Oct
20 Rect
20 Oct

φ

0.1
0.1

0.1
0.1
0.1
0.1

Adaptive
cruise
control

7 Rect 0.33
7
Oct 0.33
7 Temp 0.33
0.5
7 Rect
0.5
7
Oct
0.5
7 Temp
7 Rect 0.33
7
Oct 0.33
7 Temp 0.33
0.5
7 Rect
0.5
Oct
7
0.5
7 Temp

Inverted
pendulum

6 Rect
6 Rect

0.5
0.5

Contain. Num. Num.
poly. visited

check

IMDP Prob. Runtime

size

bound

(min.)

(cid:51)
(cid:51)

(cid:51)
(cid:51)
(cid:55)
(cid:55)

(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:51)
(cid:55)

337
352

1727
2489
18890
13437

1522
1415
2440
593
801
1102
11334
7609
6710
3981
2662
2809

1494
5436

28
66

5534
3045

411
484

7796
6273

0.0
0.0

0.63
0.0

0 23337 0.006
0 16837

0.0

4770 10702 0.084
6394 0.078
2299
0.47
9234
2475
0.62
3776
1589
0.12
3063
881
0.53
4045
1079
0 24184 0.040
0 16899 0.031
0 14626 0.038
0.17
0
0.12
0
0.16
0

8395
5895
6178

3788 14726 0.057
0 16695 0.057

1
2

30
33
91
111

85
60
70
29
30
34
176
152
113
64
52
48

71
69

Table 1: Veriﬁcation results for the benchmark environments

choosing appropriate slopes for the directions to better represents the decision
boundaries. The eﬀect of the choice of diﬀerent template can be seen in Fig 2
where we show a representative abstract state and how the reﬁnement algorithm
is aﬀected by the choice of template: as expected, increasing the generality of
the abstract domain results in a smaller number of abstract states.

Containment checks. Lastly, we describe an optimisation implemented for
construction of IMDP abstractions, whose eﬀectiveness we will evaluate in the
next section. When calculating the successors of abstract states to construct an
IMDP, we sometimes ﬁnd that successors that are partially or fully contained
within previously visited abstract states. Against the possible trade-oﬀ of de-
creasing the accuracy of the abstraction, we can attempt to reduce the total size
of the IMDP that is constructed by aggregating together states which are fully
contained within previously visited abstract states.

5.2 Experimental Results

Table 1 summarises the experimental results across the diﬀerent benchmark
environments; k denotes the time horizon considered. We use a range of con-
ﬁgurations, varying: the abstract domain used (rectangles, octagons or general
template polyhedra); the maximum probability spread threshold φ and whether
the containment check optimisation is used.

14

The table lists, for each case: the number of independent polyhedra generated,
the number of instances in which polyhedra are contained in previously visited
abstract states and aggregated together; the ﬁnal size of the IMDP abstraction
(number of abstract states); the generated upper bound on the probability of
encountering an unsafe state from an initial state; and the runtime of the whole
process. Experiments were run on a 4-core 4.2 GHz PC with 64 GB RAM.

Veriﬁcation successfully produced probability bounds for all environments
considered. Typically, the values of k shown are the largest time horizons we
could check, assuming a 3 hour timeout for veriﬁcation. The majority of the
runtime is for constructing the abstraction, not solving the IMDP.

As can be seen, the various conﬁgurations result in diﬀerent safety probability
bounds and runtimes for the same environments, so we are primarily interested in
the impact that these choices have on the trade-oﬀ between abstraction precision
and performance. We summarise ﬁndings for each benchmark separately.

Bouncing ball. These are the quickest abstractions to construct and verify due
to the low number of variables and the simplicity of the dynamics. For both
initial regions considered, we can actually verify that it is fully safe (maximum
probability 0). However, for the larger one, rectangles (particular with contain-
ment checks) are not accurate enough to show this.

Two main areas of the policy are identiﬁed for reﬁnement: one where it can
reach the ball and should hit it and one where the ball is out of reach and the
paddle should not be activated to preserve energy. But even for threshold φ = 0.1
(lower than used for other benchmarks), rectangular abstractions resulted in
large abstract states containing most of the other states visited by the agent,
and which ultimately overlapped with the unsafe region.

Adaptive cruise control. On this benchmark, we use a wider range of con-
ﬁgurations. Firstly, as expected, for smaller values of the maximum probability
spread threshold φ, the probability bound obtained is lower (the overestima-
tion error from the abstraction decreases, making it closer to the true maximum
probability) but the abstraction size and runtime increase. Applying the con-
tainment check for previously visited states has a similar eﬀect: it helps reduce
the computation time, but at the expense of overapproximation (higher bounds)
The choice of abstract domain also has a signiﬁcant impact. Octagons yield
more precise results than rectangles, for the same values of φ, and also produce
smaller abstractions (and therefore lower runtime). On the other hand, general
template polyhedra (chosen to better approximate the decision boundary) do
not appear to provide an improvement in time or precision on this example,
instead causing higher probability bounds, especially when combined with the
containment check. Our hypothesis is that this abstract domains groups large
areas of the state space (as shown in Fig. 2) and this eventually leads to overlaps
with the unsafe region.

Inverted pendulum. This benchmark is more challenging and, while we suc-
cessfully generate bounds on the probability of unsafe behaviour, for smaller
values of φ and other abstract domains, experiments timed out due to the high

15

(a) Rectangles

(b) Octagons

Fig. 3: Reﬁned policy abstractions from the inverted pendulum benchmark

number of abstract states generated and the time needed for MILP solution.
The abstract states generated were suﬃciently small that the containment check
could be used to reduce runtime without increasing the probability bound.

Figure 3 illustrates abstraction applied to a state space fragment from this
benchmark using both rectangles and octagons. It shows the probability of choos-
ing one of three actions, coded by RGB colour: noop (red), right (green) and left
(blue), The X axis represents angular speed and the Y axis represents the angle
of the pendulum in radians. Notice the grey area towards the centre where all
3 actions have the same probability, the centre right area with yellow tints (red
and green), and the centre left area with purple tints (red and blue). Towards
the bottom of the heatmap, the colour fades to green as the agent tries to push
the pendulum so that it spins and balances once it reaches the opposite side.

6 Conclusion

We presented an approach for verifying probabilistic policies for deep reinforce-
ment learning agents. This is based on a formal model of their execution as
continuous-space discrete time Markov process, and a novel abstraction repre-
sented as an interval MDP. We propose techniques to implement this framework
with MILP and a sampling-based reﬁnement method using cross-entropy min-
imisation. Experiments on several RL benchmarks illustrate its eﬀectiveness and
show how we can tune the approach to trade oﬀ accuracy and performance.

Future work includes automating the selection of an appropriate template for
abstraction and using lower bounds from the abstraction to improve reﬁnement.

Acknowledgements. This project has received funding from the European
Research Council (ERC) under the European Union’s Horizon 2020 research
and innovation programme (grant agreement No. 834115, FUN2MODEL).

16

Appendix: Proof of Theorem 1

We provide here a proof of Theorem 1, from Section 3, which states that:

Given a state s ∈ S of an RL execution model DTMP, and abstract state ˆs ∈ ˆS
of the corresponding controller abstraction IMDP for which s ∈ ˆs, we have:

Pr s(♦(cid:54)kfail ) (cid:54) Pr max max

ˆs

(♦(cid:54)kfail )

By the deﬁnition of Pr max max
and some environment policy τ in the IMDP such that:
(♦(cid:54)kfail )

Pr s(♦(cid:54)kfail ) (cid:54) Pr σ,τ

ˆs

ˆs

(·), it suﬃces to show that there is some policy σ

(2)

Recall that, in the construction of the IMDP (see Deﬁnition 7), an abstract state
ˆs is associated with a partition of subsets ˆsj of ˆs, each of which is used to deﬁne
the j-labelled choice in state ˆs. Let σ be the policy that picks in each state s
(regardless of history) the unique index js such that s ∈ ˆsjs . Then, let τ be
the environment policy that selects the upper bound of the interval for every
transition probability. We use function ˆPτ to denote the chosen probabilities,
i.e., we have ˆPτ (ˆs, js, ˆs(cid:48)) = ˆPU (ˆs, js, ˆs(cid:48)) for any ˆs, js, ˆs(cid:48).

The probabilities Pr σ,τ

(♦(cid:54)kfail ) for these policies, starting in ˆs, are deﬁned

ˆs

similarly to those for discrete-time Markov processes (see Section 2):

Pr σ,τ
ˆs

(♦(cid:54)kfail ) =






(cid:80)
ˆs(cid:48)∈supp( ˆP(ˆs,js,·))

1
0

if ˆs |= fail
if ˆs (cid:54)|= fail ∧ k=0

ˆP(ˆs, js, ˆs(cid:48))·Pr σ,τ

ˆs(cid:48) (♦(cid:54)k−1fail ) otherwise.

Since this is deﬁned recursively, we prove (2) by induction over k. For the case
k = 0, the deﬁnitions of Pr s(♦(cid:54)0fail ) and Pr ˆs(♦(cid:54)0fail ) are equivalent: they
equal 1 if s |= fail (or ˆs |= fail ) and 0 otherwise. From Deﬁnition 7, s |= fail
implies ˆs |= fail . Therefore, Pr s(♦(cid:54)0fail ) (cid:54) Pr σ,τ

(♦(cid:54)0fail ).

ˆs

Next, for the inductive step, we will assume, as the inductive hypothesis,
ˆs(cid:48) (♦(cid:54)k−1fail ) for s(cid:48) ∈ S and ˆs(cid:48) ∈ ˆS with s(cid:48) ∈ ˆs(cid:48). If

that Pr s(cid:48)(♦(cid:54)k−1fail ) (cid:54) Pr σ,τ
ˆs |= fail then Pr σ,τ

(♦(cid:54)kfail ) = 1 (cid:62) Pr s(♦(cid:54)kfail ). Otherwise we have:

ˆs
(♦(cid:54)kfail )

Pr σ,τ
ˆs
ˆs(cid:48)∈supp( ˆPτ (ˆs,js,·))

= (cid:80)
= (cid:80)
= (cid:80)
(cid:62) (cid:80)
(cid:62) (cid:80)

ˆs(cid:48)∈supp( ˆPU (ˆs,js,·))
a∈A πU (ˆs, a) · Pr ˆE(ˆsj ,a)(♦(cid:54)k−1fail )
a∈A π(s, a) · Pr ˆE(ˆsj ,a)(♦(cid:54)k−1fail )
a∈A π(s, a) · Pr E(s,a)(♦(cid:54)k−1fail )

s(cid:48)∈supp(P(s,·)) P(s, s(cid:48)) · Pr s(cid:48)(♦(cid:54)k−1fail )

= (cid:80)
= Pr s(♦(cid:54)kfail )
which completes the proof.

17

ˆPτ (ˆs, js, ˆs(cid:48)) · Pr ˆs(cid:48)(♦(cid:54)k−1fail ) by defn. of σ and Pr σ,τ
ˆPU (ˆs, js, ˆs(cid:48)) · Pr ˆs(cid:48)(♦(cid:54)k−1fail ) by defn. of τ

ˆs

(♦(cid:54)kfail )

by defn. of ˆPU (ˆs, j, ˆs(cid:48))
since s ∈ ˆs and by Defn.6
by induction and since, by
Defn. 5, E(s, w) ∈ ˆE(ˆsj, w)
by defn. of P(s, s(cid:48))
by defn. of Pr s(♦(cid:54)kfail )

References

1. Alshiekh, M., Bloem, R., Ehlers, R., K¨onighofer, B., Niekum, S., Topcu, U.: Safe
reinforcement learning via shielding. In: Proc. 32nd AAAI Conference on Artiﬁcial
Intelligence (AAAI’18). pp. 2669–2678 (2018)

2. Bacci, E.: Formal Veriﬁcation of Deep Reinforcement Learning Agents. Ph.D. the-

sis, School of Computer Science, University of Birmingham (2022)

3. Bacci, E., Giacobbe, M., Parker, D.: Verifying reinforcement learning up to inﬁnity.
In: Proc. 30th International Joint Conference on Artiﬁcial Intelligence (IJCAI’21).
pp. 2154–2160 (2021)

4. Bacci, E., Parker, D.: Probabilistic guarantees for safe deep reinforcement learn-
ing. In: Proc. 18th International Conference on Formal Modelling and Analysis of
Timed Systems (FORMATS’20). LNCS, vol. 12288, pp. 231–248. Springer (2020)
5. Bastani, O.: Safe Reinforcement Learning with Nonlinear Dynamics via Model
Predictive Shielding. In: Proceedings of the American Control Conference. pp.
3488–3494 (2021)

6. Bastani, O., Pu, Y., Solar-Lezama, A.: Veriﬁable reinforcement learning via policy
extraction. In: Proc. 2018 Annual Conference on Neural Information Processing
Systems (NeurIPS’18). pp. 2499–2509 (2018)

7. Bogomolov, S., Frehse, G., Giacobbe, M., Henzinger, T.A.: Counterexample-guided

reﬁnement of template polyhedra. In: TACAS (1). pp. 589–606 (2017)

8. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J.,

Zaremba, W.: OpenAI Gym (6 2016)

9. Bunel, R., Turkaslan, I., Torr, P., Kohli, P., Kumar, P.: A uniﬁed view of piece-
wise linear neural network veriﬁcation. In: Proc. 32nd International Conference on
Neural Information Processing Systems (NIPS’18). pp. 4795–4804 (2018)

10. Carr, S., Jansen, N., Topcu, U.: Task-aware veriﬁable RNN-based policies for par-
tially observable Markov decision processes. Journal of Artiﬁcial Intelligence Re-
search 72, 819–847 (2021)

11. Cauchi, N., Laurenti, L., Lahijanian, M., Abate, A., Kwiatkowska, M., Cardelli,
L.: Eﬃciency through uncertainty: Scalable formal synthesis for stochastic hybrid
systems. In: 22nd ACM International Conference on Hybrid Systems: Computation
and Control (2019)

12. Cheng, C.H., N¨uhrenberg, G., Ruess, H.: Maximum resilience of artiﬁcial neural
networks. In: Proc. International Symposium on Automated Technology for Veri-
ﬁcation and Analysis (ATVA’17). LNCS, vol. 10482, pp. 251–268 (2017)

13. Cheng, R., Orosz, G., Murray, R.M., Burdick, J.W.: End-to-end safe reinforcement
learning through barrier functions for safety-critical continuous control tasks. In:
AAAI. pp. 3387–3395. AAAI Press (2019)

14. Delgrange, F., Ann Now e, G.A.P.: Distillation of RL policies with formal guaran-
tees via variational abstraction of markov decision processes. In: Proc. 36th AAAI
Conference on Artiﬁcial Intelligence (AAAI’22) (2022)

15. Fecher, H., Leucker, M., Wolf, V.: Don’t know in probabilistic systems. In: Valmari,

A. (ed.) Proc. SPIN’06. LNCS, vol. 3925, pp. 71–88. Springer (2006)

16. Frehse, G., Giacobbe, M., Henzinger, T.A.: Space-time interpolants. In: CAV (1).

pp. 468–486. Springer (2018)

17. Fulton, N., Platzer, A.: Safe reinforcement learning via formal methods: Toward
safe control through proof and learning. In: AAAI. pp. 6485–6492. AAAI Press
(2018)

18

18. Garc´ıa, J., Fern´andez, F.: Probabilistic policy reuse for safe reinforcement learning.
ACM Transactions on Autonomous and Adaptive Systems 13(3), 1–24 (2018)
19. Gu, S., Holly, E., Lillicrap, T.P., Levine, S.: Deep reinforcement learning for robotic
manipulation with asynchronous oﬀ-policy updates. In: Proc. 2017 IEEE Interna-
tional Conference on Robotics and Automation (ICRA’17). pp. 3389–3396 (2017)

20. Gurobi Optimization, LLC: Gurobi Optimizer Reference Manual (2021)
21. Hasanbeig, M., Abate, A., Kroening, D.: Logically-constrained neural ﬁtted q-

iteration. In: AAMAS. pp. 2012–2014. IFAAMAS (2019)

22. Hasanbeig, M., Abate, A., Kroening, D.: Cautious reinforcement learning with
logical constraints. In: AAMAS. pp. 483–491. International Foundation for Au-
tonomous Agents and Multiagent Systems (2020)

23. Hunt, N., Fulton, N., Magliacane, S., Hoang, T.N., Das, S., Solar-Lezama, A.:
Veriﬁably safe exploration for end-to-end reinforcement learning. In: Proc. 24th
International Conference on Hybrid Systems: Computation and Control (HSCC’21)
(2021)

24. Jaeger, M., Jensen, P.G., Larsen, K.G., Legay, A., Sedwards, S., Taankvist, J.H.:
Teaching Stratego to play ball: Optimal synthesis for continuous space MDPs. In:
ATVA. pp. 81–97. Springer (2019)

25. Jansen, N., K¨onighofer, B., Junges, S., Serban, A., Bloem, R.: Safe reinforcement
learning using probabilistic shields. In: Proc. 31st International Conference on Con-
currency Theory (CONCUR’20). vol. 171, pp. 31–316 (2020)

26. Jin, P., Zhang, M., Li, J., Han, L., Wen, X.: Learning on Abstract Domains: A
New Approach for Veriﬁable Guarantee in Reinforcement Learning (jun 2021)
27. Kattenbelt, M., Kwiatkowska, M., Norman, G., Parker, D.: A game-based
abstraction-reﬁnement framework for Markov decision processes. Formal Methods
in System Design 36(3), 246–280 (2010)

28. Kazak, Y., Barrett, C.W., Katz, G., Schapira, M.: Verifying deep-RL-driven sys-
tems. In: Proceedings of the 2019 Workshop on Network Meets AI & ML, Ne-
tAI@SIGCOMM’19. pp. 83–89. ACM (2019)

29. Kemeny, J., Snell, J., Knapp, A.: Denumerable Markov Chains. Springer-Verlag,

2nd edn. (1976)

30. Kendall, A., Hawke, J., Janz, D., Mazur, P., Reda, D., Allen, J., Lam, V., Bewley,

A., Shah, A.: Learning to drive in a day. In: ICRA. pp. 8248–8254. IEEE (2019)

31. K¨onighofer, B., Lorber, F., Jansen, N., Bloem, R.: Shield Synthesis for Reinforce-
ment Learning. In: Lecture Notes in Computer Science (including subseries Lecture
Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics). vol. 12476
LNCS, pp. 290–306. Springer, Cham (oct 2020)

32. Kwiatkowska, M., Norman, G., Parker, D.: PRISM 4.0: Veriﬁcation of probabilistic
real-time systems. In: Proc. 23rd International Conference on Computer Aided
Veriﬁcation (CAV’11). LNCS, vol. 6806, pp. 585–591. Springer (2011)

33. Lahijania, M., Andersson, S.B., Belta, C.: Formal veriﬁcation and synthesis for
discrete-time stochastic systems. IEEE Transactions on Automatic Control 60(8),
2031–2045 (2015)

34. Langford, J., Zhang, T.: The epoch-greedy algorithm for contextual multi-armed
bandits. Advances in neural information processing systems 20(1), 96–1 (2007)
35. Liang, E., Liaw, R., Nishihara, R., Moritz, P., Fox, R., Goldberg, K., Gonzalez, J.,
Jordan, M., Stoica, I.: RLlib: Abstractions for distributed reinforcement learning.
In: Dy, J., Krause, A. (eds.) Proceedings of the 35th International Conference on
Machine Learning. Proceedings of Machine Learning Research, vol. 80, pp. 3053–
3062. PMLR (10–15 Jul 2018)

19

36. Lun, Y.Z., Wheatley, J., D’Innocenzo, A., Abate, A.: Approximate abstractions of
markov chains with interval decision processes. In: Proc. 6th IFAC Conference on
Analysis and Design of Hybrid Systems (2018)

37. Ma, H., Guan, Y., Li, S.E., Zhang, X., Zheng, S., Chen, J.: Feasible Actor-Critic:

Constrained Reinforcement Learning for Ensuring Statewise Safety (2021)

38. Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver,
D., Kavukcuoglu, K.: Asynchronous methods for deep reinforcement learning. In:
Balcan, M.F., Weinberger, K.Q. (eds.) Proc. 33rd International Conference on
Machine Learning. vol. 48, pp. 1928–1937. PMLR (2016)

39. Osborne, M.J., et al.: An introduction to game theory, vol. 3. Oxford university

press New York (2004)

40. Papoudakis, G., Christianos, F., Albrecht, S.V.: Agent modelling under partial
observability for deep reinforcement learning. In: Proceedings of the Neural Infor-
mation Processing Systems (NeurIPS) (2021)

41. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O.,
Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A.,
Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E.: Scikit-learn: Machine
learning in Python. Journal of Machine Learning Research 12, 2825–2830 (2011)
42. Sankaranarayanan, S., Sipma, H.B., Manna, Z.: Scalable analysis of linear systems

using mathematical programming. In: VMCAI. pp. 25–41. Springer (2005)

43. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy

optimization algorithms. arXiv:1707.06347 (2017)

44. Smith, R.L.: Eﬃcient Monte Carlo procedures for generating points uniformly

distributed over bounded regions. Operations Research 32(6), 1296–1308 (1984)

45. Srinivasan, K., Eysenbach, B., Ha, S., Tan, J., Finn, C.: Learning to be Safe: Deep

RL with a Safety Critic (2020)

46. Tjeng, V., Xiao, K., Tedrake, R.: Evaluating Robustness of Neural Networks with

Mixed Integer Programming (2017)

47. Vamplew, P., Dazeley, R., Barker, E., Kelarev, A.V.: Constructing stochastic mix-
ture policies for episodic multiobjective reinforcement learning tasks. In: Proc.
Australasian Conference on Artiﬁcial Intelligence. LNCS, vol. 5866, pp. 340–349.
Springer (2009)

48. Wolﬀ, E., Topcu, U., Murray, R.: Robust control of uncertain Markov decision
processes with temporal logic speciﬁcations. In: Proc. 51th IEEE Conference on
Decision and Control (CDC’12). pp. 3372–3379 (2012)

49. Yu, C., Liu, J., Nemati, S., Yin, G.: Reinforcement learning in healthcare: A survey.

ACM Computing Surveys 55(1), 1–36 (2021)

50. Networkx - network analysis in python. https://networkx.github.io/, accessed:

2020-05-07

51. Pytorch. https://pytorch.org/, accessed: 2020-05-07
52. Zhu, H., Magill, S., Xiong, Z., Jagannathan, S.: An inductive synthesis frame-
work for veriﬁable reinforcement learning. In: Proceedings of the ACM SIGPLAN
Conference on Programming Language Design and Implementation (PLDI). pp.
686–701. Association for Computing Machinery (jun 2019)

20

