Submitted manuscript No.
(will be inserted by the editor)

Scenic: A Language for Scenario Speciﬁcation
and Data Generation

Daniel J. Fremont · Edward Kim · Tommaso
Dreossi · Shromona Ghosh · Xiangyu Yue ·
Alberto L. Sangiovanni-Vincentelli · Sanjit
A. Seshia

0
2
0
2

t
c
O
3
1

]
L
P
.
s
c
[

1
v
0
8
5
6
0
.
0
1
0
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract We propose a new probabilistic programming language for the design
and analysis of cyber-physical systems, especially those based on machine learn-
ing. Speciﬁcally, we consider the problems of training a system to be robust to
rare events, testing its performance under diﬀerent conditions, and debugging fail-
ures. We show how a probabilistic programming language can help address these
problems by specifying distributions encoding interesting types of inputs, then
sampling these to generate specialized training and test data. More generally,
such languages can be used to write environment models, an essential prerequisite
to any formal analysis. In this paper, we focus on systems like autonomous cars
and robots, whose environment at any point in time is a scene, a conﬁguration
of physical objects and agents. We design a domain-speciﬁc language, Scenic, for
describing scenarios that are distributions over scenes and the behaviors of their
agents over time. As a probabilistic programming language, Scenic allows assign-
ing distributions to features of the scene, as well as declaratively imposing hard
and soft constraints over the scene. We develop specialized techniques for sam-
pling from the resulting distribution, taking advantage of the structure provided
by Scenic’s domain-speciﬁc syntax. Finally, we apply Scenic in a case study on a
convolutional neural network designed to detect cars in road images, improving its
performance beyond that achieved by state-of-the-art synthetic data generation
methods.

Keywords scenario description language · synthetic data · deep learning ·
probabilistic programming · debugging · automatic test generation · simulation

CR Subject Classiﬁcation D.2.5, D.3.2, I.2.6, I.2.9

Preliminary versions of this article appeared in the Proceedings of the 40th ACM SIGPLAN
Conference on Programming Language Design and Implementation (PLDI 2019) [17] and as
an April 2018 UC Berkeley technical report [14].

Daniel J. Fremont
University of California, Santa Cruz
E-mail: dfremont@ucsc.edu

Edward Kim · Tommaso Dreossi · Shromona Ghosh · Xiangyu Yue · Alberto L. Sangiovanni-
Vincentelli · Sanjit A. Seshia
University of California, Berkeley

 
 
 
 
 
 
2

Daniel J. Fremont et al.

Fig. 1: Three scenes generated from a single ∼20-line Scenic program representing
bumper-to-bumper traﬃc.

1 Introduction

Machine learning (ML) is increasingly used in safety-critical applications, thereby
creating an acute need for techniques to gain higher assurance in ML-based sys-
tems [51, 53, 1]. ML has proved particularly eﬀective at the diﬃcult perceptual
tasks (e.g., vision) arising in cyber-physical systems like autonomous vehicles which
operate in heterogeneous, complex physical environments. Thus, there is a press-
ing need to tackle several important problems in the design of such ML-based
cyber-physical systems, including:

• training the system to be robust, correctly responding to events that happen

only rarely;

• testing the system under a variety of conditions, especially unusual ones, and
• debugging the system to understand the root cause of a failure and eliminate it.

The traditional ML approach to these problems is to gather more data from the
environment, retraining the system until its performance is adequate. The major
diﬃculty here is that collecting real-world data can be slow and expensive, since
it must be preprocessed and correctly labeled before use. Furthermore, it may
be diﬃcult or impossible to collect data for corner cases that are rare and even
dangerous but nonetheless necessary to train and test against: for example, a car
accident. As a result, recent work has investigated training and testing systems
with synthetically generated data, which can be produced in bulk with correct labels
and giving the designer full control over the distribution of the data [28, 27, 57, 30].
A challenge to the use of synthetic data is that it can be highly non-trivial to
generate meaningful data, since this usually requires modeling complex environ-
ments [53]. Suppose we wanted to train a neural network on images of cars on a
road. If we simply sampled uniformly at random from all possible conﬁgurations
of, say, 12 cars, we would get data that was at best unrealistic, with cars facing
sideways or backward, and at worst physically impossible, with cars intersecting
each other. Instead, we want scenes like those in Fig. 1, where the cars are laid out
in a consistent and realistic way. Furthermore, we may want scenes that are not
only realistic but represent particular scenarios of interest for training or testing,
e.g., parked cars, cars passing across the ﬁeld of view, or bumper-to-bumper traﬃc
as in Fig. 1. In general, we need a way to guide data generation toward scenarios
that make sense for our application.

We argue that probabilistic programming languages (PPLs) [26] provide a
natural solution to this problem. Using a PPL, the designer of a system can con-
struct distributions representing diﬀerent input regimes of interest, and sample

Scenic: A Language for Scenario Speciﬁcation and Data Generation

3

from these distributions to obtain concrete inputs for training and testing. More
generally, the designer can model the system’s environment, with the program be-
coming a speciﬁcation of the distribution of environments under which the system
is expected to operate correctly with high probability. Such environment models
are essential for any formal analysis: in particular, composing the system with the
model, we obtain a closed program which we could potentially prove properties
about to establish the correctness of the system.

In this paper, we focus on designing and analyzing ML-based cyber-physical
systems. We refer to the environment of such a system at any point in time as a
scene, a conﬁguration of objects in space (including dynamic agents, such as vehi-
cles) along with their features. We develop a domain-speciﬁc scenario description
language, Scenic, to specify such environments. Scenic is a probabilistic program-
ming language, and a Scenic scenario deﬁnes a distribution over both scenes and
the behaviors of the dynamic agents in them over time. As we will see, the syntax
of the language is designed to simplify the task of writing complex scenarios, and
to enable the use of specialized sampling techniques. In particular, Scenic allows
the user to both construct objects in a straightforward imperative style and impose
hard and soft constraints declaratively. It also provides readable, concise syntax for
spatial and temporal relationships: constructs for common geometric relationships
that would otherwise require complex non-linear expressions and constraints, as
well as temporal constructs like interrupts for building complex dynamic behav-
iors in a modular way. In addition, Scenic provides a notion of classes allowing
properties of objects to be given default values depending on other properties: for
example, we can deﬁne a Car so that by default it faces in the direction of the road
at its position. More broadly, Scenic uses a novel approach to object construction
which factors the process into syntactically-independent speciﬁers which can be
combined in arbitrary ways, mirroring the ﬂexibility of natural language. Finally,
Scenic provides constructs to generalize simple scenarios by adding noise or by
composing multiple scenarios together.

Generic scenario
(“a car on the road”)

Structured scenario
(“a badly-parked car”)

The variety of constructs in Scenic makes
it possible to model scenarios anywhere on
a spectrum from concrete scenes (i.e. indi-
vidual test cases) to extremely broad classes
of abstract scenarios (see Fig. 2). A scenario
can be reached by moving along the spec-
trum from either end: the top-down approach
is to progressively constrain a very general
scenario, while the bottom-up approach is to
generalize from a concrete example (such as
a known failure case), for example by adding
random noise. Probably most usefully, one
can write a scenario in the middle which is
far more general than simply adding noise
to a single scene but has much more struc-
ture than a completely random scene: for example, the traﬃc scenario depicted in
Fig. 1. We will illustrate all three ways of developing a scenario, which as we will
see are useful for diﬀerent training, testing, and debugging tasks.

Fig. 2: Spectrum of scenarios,
from general to speciﬁc.

Example + noise
(“a car near 1.2 m × 4 m”)

Concrete example
(“a car at 1.2 m × 4 m”)

Generating scenarios from a Scenic program requires sampling from the prob-
ability distribution it implicitly deﬁnes. This task is closely related to the infer-

4

Daniel J. Fremont et al.

ence problem for imperative PPLs with observations [26]. While Scenic could be
implemented as a library on top of such a language, we found that clarity and
concision could be signiﬁcantly improved with new syntax (speciﬁers and inter-
rupts in particular) diﬃcult to implement as a library. Furthermore, while Scenic
could be translated into existing PPLs, using a new language allows us to im-
pose restrictions enabling domain-speciﬁc sampling techniques not possible with
general-purpose PPLs. In particular, we develop algorithms which take advan-
tage of the particular structure of distributions arising from Scenic programs to
dramatically prune the sample space.

We also integrate Scenic as the environment modeling language for VerifAI,
a tool for the formal design and analysis of AI-based systems [8]. VerifAI allows
writing system-level speciﬁcations in Metric Temporal Logic [33] and performing
falsiﬁcation, running simulations and monitoring for violations of the speciﬁca-
tions. VerifAI provides several search techniques, including active samplers that
use feedback from earlier simulations to try to drive the system towards violations.
We make these techniques available from Scenic using syntax to deﬁne external
parameters which are sampled by VerifAI or another external tool. Such param-
eters need not have a ﬁxed distribution of values: in particular, we can deﬁne a
prior distribution, but then use cross-entropy optimization [50] to drive the dis-
tribution towards one that is concentrated on values that tend to lead to system
failures [15].

We demonstrate the utility of Scenic in training, testing, and debugging ML-
based cyber-physical systems. Our ﬁrst case study is on SqueezeDet [61], a convo-
lutional neural network for object detection in autonomous cars. For this task, it
has been shown [30] that good performance on real images can be achieved with
networks trained purely on synthetic images from the video game Grand Theft
Auto V (GTAV [47]). We implemented a sampler for Scenic scenarios, using it
to generate scenes which were rendered into images by GTAV. Our experiments
demonstrate using Scenic to:
• evaluate the accuracy of the ML model under particular conditions, e.g. in good

or bad weather,

• improve performance in corner cases by emphasizing them during training: we
use Scenic to both identify a deﬁciency in a state-of-the-art car detection data
set [30] and generate a new training set of equal size but yielding signiﬁcantly
better performance, and

• debug a known failure case by generalizing it in many directions, exploring
sensitivity to diﬀerent features and developing a more general scenario for re-
training: we use Scenic to ﬁnd an image the network misclassiﬁes, discover the
root cause, and ﬁx the bug, in the process improving the network’s performance
on its original test set (again, without increasing training set size).

These experiments show that Scenic can be a very useful tool for understanding
and improving perception systems.

While this case study is performed in the domain of visual perception for
autonomous driving, and uses one particular simulator (GTAV), we stress that
Scenic is not speciﬁc to either. In Sec. 3 we give an example of a diﬀerent do-
main, namely robotic motion planning (using the Webots simulator [40]), and
in Sec. 6.2.2 we use Scenic and VerifAI to falsify an autonomous agent in the
CARLA driving simulator [7]. The latter experiment demonstrates Scenic’s use-

Scenic: A Language for Scenario Speciﬁcation and Data Generation

5

fulness applied not only to perception components in isolation but to entire closed-
loop cyber-physical systems. In fact, since the conference version of this paper we
have successfully applied Scenic in two industrial case studies on large ML-based
systems [15, 19]: an aircraft navigation system from Boeing (tested in the X-Plane
ﬂight simulator [35]) and the Apollo autonomous driving platform [3] (tested in
the LGSVL driving simulator [48] and on an actual test track). Generally, Scenic
can produce data of any desired type (e.g. RGB images, LIDAR point clouds, or
trajectories from dynamical simulations) by interfacing it to an appropriate sim-
ulator. This requires only two steps: (1) writing a small Scenic library deﬁning
the types of objects supported by the simulator, as well as the geometry of the
workspace; (2) writing an interface layer converting the conﬁgurations output by
Scenic into the simulator’s input format (and, for dynamic scenarios, transferring
simulator state back into Scenic). While the current version of Scenic is primarily
concerned with geometry, leaving the details of rendering up to the simulator, the
language allows putting distributions on any parameters the simulator exposes:
for example, in GTAV the meshes of the various car models are ﬁxed but we can
control their overall color. We have also used Scenic to specify distributions over
parameters on system dynamics, such as mass.

In summary, the main contributions of this work are:

– Scenic, a domain-speciﬁc probabilistic programming language for describing
scenarios: distributions over spatio-temporal conﬁgurations of physical objects
and agents;

– a methodology for using PPLs to design and analyze cyber-physical systems,

especially those based on ML;

– domain-speciﬁc algorithms for sampling from the distribution deﬁned by a

Scenic program;

– a case study using Scenic to analyze and improve the accuracy of a practical
deep neural network used for perception in an autonomous driving context
beyond what is achieved by state-of-the-art synthetic data generation methods.

The paper is structured as follows: we begin with an overview of our approach
in Sec. 2. Section 3 gives examples highlighting the major features of Scenic and
motivating various choices in its design. In Sec. 4 we describe the Scenic language
in detail, and in Sec. 5 we discuss its formal semantics and our sampling algorithms.
Section 6 describes the setup and results of our car detection case study and other
experiments. Finally, we discuss related work in Sec. 7 and conclude in Sec. 8 with
a summary and directions for future work.

An early version of this paper appeared as [14], extended and published as [17].
This paper further extends [17] by generalizing Scenic to dynamic scenarios (in-
cluding new spatiotemporal pruning techniques), adding constructs for composing
scenarios, and integrating Scenic within the broader VerifAI toolkit.

2 Using PPLs to Design and Analyze ML-Based Cyber-Physical Systems

We propose a methodology for training, testing, and debugging ML-based cyber-
physical systems using probabilistic programming languages. The core idea is to

6

Daniel J. Fremont et al.

Fig. 3: Tool ﬂow using Scenic to train, test, and debug a cyber-physical system.

use PPLs to formalize general operation scenarios, then sample from these distri-
butions to generate concrete environment conﬁgurations. Putting these conﬁgura-
tions into a simulator, we obtain images or other sensor data which can be used
to test and train the system. The general procedure is outlined in Fig. 3. For a
demonstration of this paradigm on an industrial system, proceeding from falsiﬁ-
cation through failure analysis, retraining, and validation, see [15]. Note that the
training/testing datasets need not be purely synthetic: we can generate data to
supplement existing real-world data (possibly mitigating a deﬁciency in the latter,
while avoiding overﬁtting). Furthermore, even for models trained purely on real
data, synthetic data can still be useful for testing and debugging, as we will see
below. Now we discuss the three design problems from the Introduction in more
detail.

Testing under Diﬀerent Conditions. The most straightforward problem is that of
assessing system performance under diﬀerent conditions. We can simply write
scenarios capturing each condition, generate a test set from each one, and evaluate
the performance of the system on these. Note that conditions which occur rarely
in the real world present no additional problems: as long as the PPL we use
can encode the condition, we can generate as many instances as desired. If we
do not have particular conditions in mind, we can write a very general scenario
describing the expected operation regime of the system (e.g., the “Operational
Design Domain” (ODD) of an autonomous vehicle [56]) and perform falsiﬁcation,
looking for violations of the system’s speciﬁcation.

Training on Rare Events. Extending the previous application, we can use this proce-
dure to help ensure the system performs adequately even in unusual circumstances
or particularly diﬃcult cases. Writing a scenario capturing these rare events, we
can generate instances of them to augment or replace part of the original train-
ing set. Emphasizing these instances in the training set can improve the system’s
performance in the hard case without impacting performance in the typical case.
In Sec. 6.3 we will demonstrate this for car detection, where a hard case is when
one car partially overlaps another in the image. We wrote a Scenic program to
generate a set of these overlapping images. Training the car-detection network on
a state-of-the-art synthetic dataset obtained by randomly driving around inside

ScenicSampler / VerifAIScenicProgramTraining/TestDataSimulatorScenes/ActionsSystemFailure CasesWorld StateReal DataControlsSystemSpecificationparkedCar.scenicScenic: A Language for Scenario Speciﬁcation and Data Generation

7

the simulated world of GTAV and capturing images periodically [30], we ﬁnd its
performance is signiﬁcantly worse on the overlapping images. However, if we keep
the training set size ﬁxed but increase the proportion of overlapping images, per-
formance on such images dramatically improves without harming performance on
the original generic dataset.

Debugging Failures. Finally, we can use the same procedure to help understand and
ﬁx bugs in the system. If we ﬁnd an environment conﬁguration where the system
fails, we can write a scenario reproducing that particular conﬁguration. Having the
conﬁguration encoded as a program then makes it possible to explore the neigh-
borhood around it in a variety of diﬀerent directions, leaving some aspects of the
scene ﬁxed while varying others. This can give insight into which features of the
scene are relevant to the failure, and eventually identify the root cause. The root
cause can then itself be encoded into a scenario which generalizes the original fail-
ure, allowing retraining without overﬁtting to the particular counterexample. We
will demonstrate this approach in Sec. 6.4, starting from a single misclassiﬁcation,
identifying a general deﬁciency in the training set, replacing part of the training
data to ﬁx the gap, and ultimately achieving higher performance on the original
test set.

For all of these applications we need a PPL which can encode a wide range of
general and speciﬁc environment scenarios. In the next section, we describe the
design of a language suited to this purpose.

3 The Scenic Language

We use Scenic scenarios from our autonomous car case study to motivate and il-
lustrate the main features of the language, focusing on features that make Scenic
particularly well-suited for the domain of specifying scenarios for cyber-physical
systems. We begin by describing how Scenic can deﬁne spatial relationships be-
tween objects to model scenarios like “a badly-parked car”, moving on to temporal
relationships for dynamic scenarios like “a badly-parked car, which pulls into the
road as you approach”. Finally, we outline Scenic’s support for composing multiple
scenarios together to produce more complex ones.

3.1 Basic Scenarios

Classes, Objects, Geometry, and Distributions. To start, suppose we want scenes of
one car viewed from another on the road. We can simply write:

1
2
3

from scenic.simulators.gta.model import *
ego = Car
Car

First, we import Scenic’s world model for the GTAV simulator: a Scenic library
containing everything speciﬁc to our case study, including the class Car and in-
formation about the locations of roads (from now on we suppress this line). Only
general geometric concepts are built into Scenic.

8

Daniel J. Fremont et al.

The second line creates a Car and assigns it to the special variable ego specifying
the ego object which is the reference point for the scenario. In particular, rendered
images from the scenario are from the perspective of the ego object (it is a syntax
error to leave ego undeﬁned). Finally, the third line creates an additional Car.
Note that we have not speciﬁed the position or any other properties of the two
cars: this means they are inherited from the default values deﬁned in the class Car.
Object-orientation is valuable in Scenic since it provides a natural organizational
principle for scenarios involving diﬀerent types of physical objects. It also improves
compositionality, since we can deﬁne a generic Car model in a library like the GTAV
world model and use it in diﬀerent scenarios. Our deﬁnition of Car begins as follows
(slightly simpliﬁed):

1
2
3

class Car:

position: Point on road
heading: roadDirection at self.position

Here road is a region (one of Scenic’s primitive types) deﬁned in the GTAV
world model to specify which points in the workspace are on a road. Similarly,
roadDirection is a vector ﬁeld specifying the prevailing traﬃc direction at such
points. The operator F at X simply gets the direction of the ﬁeld F at point X,
so the default value for a car’s heading is the road direction at its position. The
default position, in turn, is a Point on road (we will explain this syntax shortly),
which means a uniformly random point on the road.

The ability to make random choices like this is a key aspect of Scenic. Scenic’s
probabilistic nature allows it to model real-world stochasticity, for example encod-
ing a distribution for the distance between two cars learned from data. This in
turn is essential for our application of PPLs to training perception systems: us-
ing randomness, a PPL can generate training data matching the distribution the
system will be used under. Scenic provides several basic distributions (and allows
more to be deﬁned). For example, we can write

1

Car offset by (Range(-10, 10), Range(20, 40))

to create a car that is 20–40 m ahead of the camera. The notation Range(X , Y )
creates a uniform distribution over the given continuous range, and (X , Y ) cre-
ates a pair, interpreted here as a vector given by its xy coordinates.

Local Coordinate Systems. Using offset by as above overrides the default position
of the Car, leaving the default orientation (along the road) unchanged. Suppose
for greater realism we don’t want to require the car to be exactly aligned with the
road, but to be within say 5◦. We could try:

1
2

Car offset by (Range(-10, 10), Range(20, 40)),

facing Range(-5, 5) deg

but this is not quite what we want, since this sets the orientation of the Car in
global coordinates (i.e. within 5◦ of North). Instead we can use Scenic’s general
operator X relative to Y , which can interpret vectors and headings as being in
a variety of local coordinate systems:

1
2

Car offset by (Range(-10, 10), Range(20, 40)),

facing Range(-5, 5) deg relative to roadDirection

Scenic: A Language for Scenario Speciﬁcation and Data Generation

9

If we want the heading to be relative to the ego car’s orientation, we simply write
Range(-5, 5) deg relative to ego.

Notice that since roadDirection is a vector ﬁeld, it deﬁnes a coordinate sys-
tem at each point, and an expression like 15 deg relative to field does not
deﬁne a unique heading. The example above works because Scenic knows that
Range(-5, 5) deg relative to roadDirection depends on a reference position,
and automatically uses the position of the Car being deﬁned. This is a feature of
Scenic’s system of speciﬁers, which we explain next.

Readable, Flexible Speciﬁers. The syntax offset by X and facing Y for specifying
positions and orientations may seem unusual compared to typical constructors in
object-oriented languages. There are two reasons why Scenic uses this kind of
syntax: ﬁrst, readability. The second is more subtle and based on the fact that in
natural language there are many ways to specify positions and other properties,
some of which interact with each other. Consider the following ways one might
describe the location of an object:

1. “is at position X ” (absolute position);
2. “is just left of position X ” (position based on orientation);
3. “is 3 m west of the taxi” (relative position);
4. “is 3 m left of the taxi” (a local coordinate system);
5. “is one lane left of the taxi” (another local coordinate system);
6. “appears to be 10 m behind the taxi” (relative to the line of sight);
7. “is 10 m along the road from the taxi” (following a vector ﬁeld; consider a

curving road).

These are all fundamentally diﬀerent from each other: e.g., (4) and (5) diﬀer if the
taxi is not parallel to the lane.

Furthermore, these speciﬁcations combine other properties of the object in
diﬀerent ways: to place the object “just left of” a position, we must ﬁrst know the
object’s heading; whereas if we wanted to face the object “towards” a location, we
must instead know its position. There can be chains of such dependencies: “the
car is 0.5 m left of the curb” means that the right edge of the car is 0.5 m away
from the curb, not the car’s position, which is its center. So the car’s position
depends on its width, which in turn depends on its model. In a typical object-
oriented language, this might be handled by computing values for position and
other properties and passing them to a constructor. For “a car is 0.5 m left of the
curb” we might write:

1
2
3
4

# hypothetical Python-like language
m = Car.defaultModelDistribution.sample()
pos = curb.offsetLeft(0.5 + m.width / 2)
car = Car(pos, model=m)

Notice how m must be used twice, because m determines both the model of the car
and (indirectly) its position. This is inelegant and breaks encapsulation because
the default model distribution is used outside of the Car constructor. The latter
problem could be ﬁxed by having a specialized constructor or factory function,

1

car = CarLeftOfBy(curb, 0.5)

10

Daniel J. Fremont et al.

but these would proliferate since we would need to handle all possible combinations
of ways to specify diﬀerent properties (e.g. do we want to require a speciﬁc model?
Are we overriding the width provided by the model for this speciﬁc car?). Instead
of having a multitude of such monolithic constructors, Scenic factors the deﬁnition
of objects into potentially-interacting but syntactically-independent parts:

1

Car left of spot by 0.5, with model BUS

Here left of X by D and with model M are speciﬁers which do not have an
order, but which together specify the properties of the car. Scenic works out the
dependencies between properties (here, position is provided by left of, which
depends on width, whose default value depends on model) and evaluates them in
the correct order. To use the default model distribution we would simply leave oﬀ
with model BUS; keeping it aﬀects the position appropriately without having to
specify BUS more than once.

Specifying Multiple Properties Together. Recall that we deﬁned the default position
for a Car to be a Point on road: this is an example of another speciﬁer, on region,
which speciﬁes position to be a uniformly random point in the given region. This
speciﬁer illustrates another feature of Scenic, namely that speciﬁers can specify
multiple properties simultaneously. Consider the following scenario, which creates
a parked car given a region curb deﬁned in the GTAV world model:

1
2

spot = OrientedPoint on visible curb
Car left of spot by 0.25

The function visible region returns the part of the region that is visible from the
ego object. The speciﬁer on visible curb will then set position to be a uniformly
random visible point on the curb. We create spot as an OrientedPoint, which is
a built-in class that deﬁnes a local coordinate system by having both a position
and a heading. The on region speciﬁer can also specify heading if the region has
a preferred orientation (a vector ﬁeld) associated with it: in our example, curb is
oriented by roadDirection. So spot is, in fact, a uniformly random visible point
on the curb, oriented along the road. That orientation then causes the car to be
placed 0.25 m left of spot in spot’s local coordinate system, i.e. away from the
curb, as desired.

In fact, Scenic makes it easy to elaborate the scenario without needing to alter
the code above. Most simply, we could specify a particular model or non-default
distribution over models by just adding with model M to the deﬁnition of the Car.
More interestingly, we could produce a scenario for badly-parked cars by adding
two lines:

1
2
3
4

spot = OrientedPoint on visible curb
badAngle = Uniform(1.0, -1.0) * Range(10, 20) deg
Car left of spot by 0.5,

facing badAngle relative to roadDirection

This will yield cars parked 10-20◦ oﬀ from the direction of the curb, as seen
in Fig. 4. This illustrates how speciﬁers greatly enhance Scenic’s ﬂexibility and
modularity.

Scenic: A Language for Scenario Speciﬁcation and Data Generation

11

Fig. 4: A scene of a badly-parked car.

Declarative Speciﬁcations of Hard and Soft Constraints. Notice that in the scenarios
above we never explicitly ensured that the two cars will not intersect each other.
Despite this, Scenic will never generate such scenes. This is because Scenic en-
forces several default requirements: all objects must be contained in the workspace,
must not intersect each other, and must be visible from the ego object.1 Scenic also
allows the user to deﬁne custom requirements checking arbitrary conditions built
from various geometric predicates. For example, the following scenario produces a
car headed roughly towards us, while still facing the nominal road direction:

1
2
3

carB = Car offset by (Range(-10, 10), Range(20, 40)),

with viewAngle 30 deg

require carB can see ego

Here we have used the X can see Y predicate, which in this case is checking that
the ego car is inside the 30◦ view cone of the second car. If we only need this
constraint to hold part of the time, we can use a soft requirement specifying the
minimum probability with which it must hold:

1

require[0.5] carB can see ego

Hard requirements, called “observations” in other PPLs (see, e.g., [26]), are very
convenient in our setting because they make it easy to restrict attention to par-
ticular cases of interest. They also improve encapsulation, since we can restrict an
existing scenario without altering it (we can simply import it in a new Scenic pro-
gram that includes additional require statements). Finally, soft requirements are
useful in ensuring adequate representation of a particular condition when gener-
ating a training set: for example, we could require that at least 90% of the images
have a car driving on the right side of the road.

Mutations. Scenic provides a simple mutation system that improves composition-
ality by providing a mechanism to add variety to a scenario without changing its

1 The last requirement ensures that the object will aﬀect the rendered image. It can be
disabled on a per-object basis, for example in dynamic scenarios where the object is initially
out of sight but may interact with the ego object later on.

12

Daniel J. Fremont et al.

Fig. 5: Webots scene of a Mars rover in a debris ﬁeld with a bottleneck.

code. This is useful, for example, if we have a scenario encoding a single concrete
scene obtained from real-world data and want to quickly generate variations. For
instance:

1
2
3

taxi = Car at (120, 300), facing 37 deg, ...
...
mutate taxi

This will add Gaussian noise to the position and heading of taxi, while still
enforcing all built-in and custom requirements. The standard deviation of the
noise can be scaled by writing, for example, mutate taxi by 2 (which adds twice
as much noise), and we will see later that it can be controlled separately for
position and heading.

Multiple Domains and Simulators. We conclude this section by illustrating a sec-
ond application domain, namely generating workspaces to test motion planning
algorithms, and Scenic’s ability to work with diﬀerent simulators. A robot like a
Mars rover able to climb over rocks can have very complex dynamics, with the
feasibility of a motion plan depending on exact details of the robot’s hardware
and the geometry of the terrain. We can use Scenic to write a scenario generating
challenging cases for a planner to solve. Figure 5 shows a scene, visualized using an
interface we wrote between Scenic and the Webots robotics simulator [40], with
a bottleneck between the robot and its goal that forces the planner to consider
climbing over a rock. The Scenic code for this scenario is given in Appendix A.

Even within a single application domain, such as autonomous driving, Scenic
enables writing cross-platform scenarios that will work without change in multi-
ple simulators. This is made possible by what we call abstract application domains:

Scenic: A Language for Scenario Speciﬁcation and Data Generation

13

Fig. 6: Scenes sampled from the same Scenic program in CARLA and LGSVL.

Scenic world models which deﬁne object classes and other world information like
our GTAV world model, but which are abstract, simulator-agnostic protocols that
can be implemented by models for particular simulators. For example, Scenic
includes an abstract domain for autonomous driving, scenic.domains.driving,
which loads road networks from standard formats, providing a uniform API for
referring to lanes, maneuvers, and other aspects of road geometry. The driving
domain also provides generic Car and Pedestrian classes, complete with imple-
mentations of common dynamic behaviors (covered in the next section) like lane
following. These make it straightforward to implement complex driving scenarios,
which are then guaranteed to work in any simulator supporting the driving do-
main. Figure 6 illustrates this, showing the exact same Scenic code being used to
generate scenarios in both the CARLA [7] and LGSVL [48] simulators.

3.2 Dynamic Scenarios

Having seen the basic constructs Scenic provides for deﬁning objects and their
spatial relationships, we now outline Scenic’s support for dynamic scenarios which
also deﬁne the temporal properties of objects.

Agents, Actions, and Behaviors. In Scenic, we call objects which take actions over
time dynamic agents, or simply agents. These are ordinary Scenic objects, so we
can still use all of the syntax described in the previous section to deﬁne their initial
positions, orientations, etc. In addition, we specify their dynamic behavior using
a built-in property called behavior. Using one of the behaviors deﬁned in Scenic’s
driving library, we can write for example:

1
2

model scenic.domains.driving.model
Car with behavior FollowLaneBehavior

A behavior deﬁnes a sequence of actions for the agent to take, which need not
be ﬁxed but can be probabilistic and depend on the state of the agent or other
objects. In Scenic, an action is an instantaneous operation executed by an agent,
like setting the steering angle of a car or turning on its headlights. Most actions
are speciﬁc to particular application domains, and so diﬀerent sets of actions are
provided by diﬀerent simulator interfaces. For example, the Scenic driving domain
deﬁnes a SetThrottleAction for cars.

14

Daniel J. Fremont et al.

Fig. 7: Diagram showing interaction between Scenic and a simulator during the
execution of a dynamic scenario.

To deﬁne a behavior, we write a function which runs over the course of the
scenario, periodically issuing actions. Scenic uses a discrete notion of time, so at
each time step the function speciﬁes zero or more actions for the agent to take.
For example, here is a very simpliﬁed version of the FollowLaneBehavior above:

1
2
3
4

behavior FollowLaneBehavior():

while True:

throttle, steering = ...
take SetThrottleAction(throttle), SetSteerAction(steering)

# compute controls

We intend this behavior to run for the entire scenario, so we use an inﬁnite
loop. In each step of the loop, we compute appropriate throttle and steering con-
trols, then use the take statement to take the corresponding actions. When that
statement is executed, Scenic pauses the behavior until the next time step of the
simulation, whereupon the function resumes and the loop repeats.

Execution of Behaviors. When there are multiple agents, all of their behaviors run
in parallel, as illustrated in Fig. 7; each time step, Scenic sends their selected
actions to the simulator to be executed and advances the simulation by one step.
It then reads back the state of the simulation, updating the position, speed, etc.
of each object.

Since behaviors run dynamically during simulations, they can access the cur-
rent state of the world to decide what actions to take. Consider the following
behavior:

1
2
3
4

behavior WaitUntilClose(threshold=15):

while (distance from self to ego) > threshold:

wait

do FollowLaneBehavior()

Here, we repeatedly query the distance from the agent running the behavior
(self) to the ego car; as long as it is above a threshold, we use the wait statement,
to take no action. Once the threshold is met, we start driving by using the do state-
ment to invoke the FollowLaneBehavior we saw above. Since FollowLaneBehavior
runs forever, we will never return to the WaitUntilClose behavior.

Behavior Arguments and Random Parameters. The example above also shows how
behaviors may take arguments, like any Scenic function. Here, threshold is an

ScenicSimulatorBehavior 1Behavior N…actionsstateScenic: A Language for Scenario Speciﬁcation and Data Generation

15

argument to the behavior which has default value 15 but can be customized, so
we could write for example:

1
2
3

ego = Car
carB = Car visible, with behavior WaitUntilClose
carC = Car visible, with behavior WaitUntilClose(20)

Both carB and carC will use the WaitUntilClose behavior, but independent

copies of it with thresholds of 15 and 20 respectively.

Unlike ordinary Scenic code, control ﬂow constructs such as if and while are
allowed to depend on random variables inside a behavior. Any distributions deﬁned
inside a behavior are sampled at simulation time, not during scene sampling.
Consider the following behavior:

1
2
3
4
5
6
7
8

behavior AvoidPedestrian():
threshold = Range(4, 7)
while True:

if self.distanceToClosest(Pedestrian) < threshold:

strength = TruncatedNormal(0.8, 0.02, 0.5, 1)
take SetBrakeAction(strength), SetThrottleAction(0)

else:

take SetThrottleAction(0.5), SetBrakeAction(0)

Here, the value of threshold is sampled only once, at the beginning of the
scenario when the behavior starts running. The value strength, on the other hand,
is sampled every time control reaches line 5, so that every time step when the car
is braking we use a slightly diﬀerent braking strength (0.8 on average, but with
Gaussian noise added with standard deviation 0.02, truncating the possible values
to between 0.5 and 1).

Interrupts. It is frequently useful to take an existing behavior and add a compli-
cation to it; for example, suppose we want a car that follows a lane, stopping
whenever it encounters an obstacle. Scenic provides a concept of interrupts which
allows us to reuse the basic FollowLaneBehavior without having to modify it.

1
2
3
4
5

behavior FollowAvoidingObstacles():

try:

do FollowLaneBehavior()

interrupt when self.distanceToClosest(Object) < 5:

take SetBrakeAction(1)

This try-interrupt statement has similar syntax to the Python try statement
(and in fact allows except clauses to catch exceptions just as in Python, as we’ll
see later), and begins in the same way: at ﬁrst, the code block after the try:
(the body) is executed. At the start of every time step during its execution, the
condition from each interrupt clause is checked; if any are true, execution of the
body is suspended and we instead begin to execute the corresponding interrupt
handler. In the example above, there is only one interrupt, which ﬁres when we
come within 5 meters of any object. When that happens, FollowLaneBehavior is
paused and we instead apply full braking for one time step. In the next step, we
will resume FollowLaneBehavior wherever it left oﬀ, unless we are still within 5
meters of an object, in which case the interrupt will ﬁre again.

16

Daniel J. Fremont et al.

If there are multiple interrupt clauses, successive clauses take precedence over
those which precede them. Furthermore, such higher-priority interrupts can ﬁre
even during the execution of an earlier interrupt handler. This makes it easy to
model a hierarchy of behaviors with diﬀerent priorities; for example, we could
implement a car which drives along a lane, passing slow cars and avoiding collisions,
along the following lines:

1
2
3
4
5
6
7

behavior Drive():

try:

do FollowLaneBehavior()

interrupt when self.distanceToNextObstacle() < 20:

do PassingBehavior()

interrupt when self.timeToCollision() < 5:

do CollisionAvoidance()

Here, the car begins by lane following, switching to passing if there is a car
or other obstacle too close ahead. During either of those two sub-behaviors, if
the time to collision gets too low, we switch to collision avoidance. Once the
CollisionAvoidance behavior completes, we will resume whichever behavior was
interrupted earlier. If we were in the middle of PassingBehavior, it will run to com-
pletion (possibly being interrupted again) before we ﬁnally resume FollowLaneBehavior.
As this example illustrates, when an interrupt handler completes, by default we
resume execution of the interrupted code. If this is undesired, the abort statement
can be used to cause the entire try-interrupt statement to exit. For example, to
run a behavior until a condition is met without resuming it afterward, we can
write:

1
2
3
4
5
6
7

behavior ApproachAndTurnLeft():

try:

do FollowLaneBehavior()

interrupt when (distance from self to intersection) < 10:

abort

# cancel lane following

do WaitForTrafficLightBehavior()
do TurnLeftBehavior()

This is a common enough use case of interrupts that Scenic provides a short-

hand notation:

1
2
3
4

behavior ApproachAndTurnLeft():

do FollowLaneBehavior() until (distance from self to intersection) < 10
do WaitForTrafficLightBehavior()
do TurnLeftBehavior()

Scenic also provides a shorthand for interrupting a behavior after a certain

period of time:

1
2

behavior DriveForAWhile():

do FollowLaneBehavior() for 30 seconds

The alternative form do behavior for n steps uses time steps instead of real

simulation time.

Finally, note that when try-interrupt statements are nested, interrupts of the
outer statement take precedence. This makes it easy to build up complex behaviors

Scenic: A Language for Scenario Speciﬁcation and Data Generation

17

in a modular way. For example, the behavior Drive we wrote above is relatively
complicated, using interrupts to switch between several diﬀerent sub-behaviors. We
would like to be able to put it in a library and reuse it in many diﬀerent scenarios
without modiﬁcation. Interrupts make this straightforward; for example, if for a
particular scenario we want a car that drives normally but suddenly brakes for 5
seconds when it reaches a certain area, we can write:

1
2
3
4
5
6
7

behavior DriveWithSuddenBrake():

haveBraked = False
try:

do Drive()

interrupt when self in targetRegion and not haveBraked:

do StopBehavior() for 5 seconds
haveBraked = True

With this behavior, Drive operates as it did before, interrupts ﬁring as ap-
propriate to switch between lane following, passing, and collision avoidance. But
during any of these sub-behaviors, if the car enters the targetRegion it will im-
mediately brake for 5 seconds, then pick up where it left oﬀ.

Stateful Behaviors. As the last example shows, behaviors can use local variables
to maintain state, which is useful when implementing behaviors which depend
on actions taken in the past. To elaborate on that example, suppose we want a
car which usually follows the Drive behavior, but every 15-30 seconds stops for 5
seconds. We can implement this behavior as follows:

1
2
3
4
5
6
7
8
9

behavior DriveWithRandomStops():

delay = Range(15, 30) seconds
last_stop = 0
try:

do Drive()

interrupt when simulation.currentTime - last_stop > delay:

do StopBehavior() for 5 seconds
delay = Range(15, 30) seconds
last_stop = simulation.currentTime

Here delay is the randomly-chosen amount of time to run Drive for, and
last_stop keeps track of the time when we last started to run it. When the time
elapsed since last_stop exceeds delay, we interrupt Drive and stop for 5 seconds.
Afterwards, we pick a new delay before the next stop, and save the current time
in last_stop, eﬀectively resetting our timer to zero.

Requirements and Monitors. Just as you can declare spatial constraints on scenes
using the require statement, you can also impose constraints on dynamic scenar-
ios. For example, if we don’t want to generate any simulations where carA and
carB are simultaneously visible from the ego car, we could write:

1

require always not ((ego can see carA) and (ego can see carB))

The require always condition statement enforces that the given condition
must hold at every time step of the scenario; if it is ever violated during a simula-
tion, we reject that simulation and sample a new one. Similarly, we can require that

18

Daniel J. Fremont et al.

a condition hold at some time during the scenario using the require eventually
statement:

1

require eventually ego in intersection

You can also use the ordinary require statement inside a behavior to require
that a given condition hold at a certain point during the execution of the behavior.
For example, here is a simple elaboration of the WaitUntilClose behavior we saw
above:

1
2
3
4
5

behavior WaitUntilClose(threshold=15):

while (distance from self to ego) > threshold:

require self.distanceToClosest(Pedestrian) > threshold
wait

do FollowLaneBehavior()

The requirement ensures that no pedestrian comes close to self until the ego

does; after that, we place no further restrictions.

To enforce more complex temporal properties like this one without modifying
behaviors, you can deﬁne a monitor. Like behaviors, monitors are functions which
run in parallel with the scenario, but they are not associated with any agent and
any actions they take are ignored. Here is a monitor for the property “carA and
carB enter the intersection before carC”:

1
2
3
4
5
6
7
8
9

monitor CarCEntersLast:

seenA, seenB = False, False
while not (seenA and seenB):

require carC not in intersection
if carA in intersection:

seenA = True

if carB in intersection:

seenB = True

wait

We use the variables seenA and seenB to remember whether we have seen carA
and carB respectively enter the intersection. The loop will iterate as long as at least
one of the cars has not yet entered the intersection, so if carC enters before either
carA or carB, the requirement on line 4 will fail and we will reject the simulation.
Note the necessity of the wait statement on line 9: if we omitted it, the loop could
run forever without any time actually passing in the simulation.

Preconditions and Invariants. Even general behaviors designed to be used in mul-
tiple scenarios may not operate correctly from all possible starting states: for
example, FollowLaneBehavior assumes that the agent is actually in a lane rather
than, say, on a sidewalk. To model such assumptions, Scenic provides a notion of
guards for behaviors. Most simply, we can specify one or more preconditions:

1
2
3

behavior MergeInto(newLane):

precondition: self.lane is not newLane and self.road is newLane.road
...

Here, the precondition requires that whenever the MergeInto behavior is exe-
cuted by an agent, the agent must not already be in the destination lane but should

Scenic: A Language for Scenario Speciﬁcation and Data Generation

19

be on the same road. We can add any number of such preconditions; like ordinary
requirements, violating any precondition causes the simulation to be rejected.

Since behaviors can be interrupted, it is possible for a behavior to resume exe-
cution in a state it doesn’t expect: imagine a car which is lane following, but then
swerves onto the shoulder to avoid an accident; na¨ıvely resuming lane following, we
ﬁnd we are no longer in a lane. To catch such situations, Scenic allows us to deﬁne
invariants which are checked at every time step during the execution of a behavior,
not just when it begins running. These are written similarly to preconditions:

1
2
3

behavior FollowLaneBehavior():
invariant: self in road
...

While the default behavior for guard violations is to reject the simulation, in
some cases it may be possible to recover from a violation by taking some additional
actions. To enable this kind of design, Scenic signals guard violations by raising
a GuardViolation exception which can be caught like any other exception; the
simulation is only rejected if the exception propagates out to the top level. So
to model the lane-following-with-collision-avoidance behavior suggested above, we
could write code like this:

1
2
3
4
5
6
7
8

behavior Drive():

while True:
try:

do FollowLaneBehavior()

interrupt when self.distanceToClosest(Object) < 5:

do CollisionAvoidance()

except InvariantViolation:

# FollowLaneBehavior has failed

do GetBackOntoRoad()

When any object comes within 5 meters, we suspend lane following and switch
to collision avoidance. When the latter completes, FollowLaneBehavior will be
resumed; if its invariant fails because we are no longer on the road, we catch the
resulting InvariantViolation exception and run a GetBackOntoRoad behavior to
restore the invariant. The whole try statement then completes, so the outermost
loop iterates and we begin lane following once again.

Terminating the Scenario. By default, scenarios run forever, unless a time limit is
speciﬁed when running the Scenic tool. However, scenarios can also deﬁne termi-
nation criteria using the terminate when statement; for example, we could decide
to end a scenario as soon as the ego car travels at least a certain distance:

1
2
3

start = Point on road
ego = Car at start
terminate when (distance to start) >= 50

Additionally, the terminate statement can be used inside behaviors and mon-
itors: if it is ever executed, the scenario ends. For example, we can use a monitor
to terminate the scenario once the ego spends 30 time steps in an intersection:

1
2

monitor StopAfterTimeInIntersection:

totalTime = 0

Daniel J. Fremont et al.

20

3
4
5
6
7

while totalTime < 30:

if ego in intersection:
totalTime += 1

wait

terminate

3.3 Compositional Scenarios

The previous two sections showed how Scenic allows us to model both the spatial
and temporal aspects of a scenario. Scenic also provides facilities for deﬁning
scenarios as reusable modules and composing them in various ways. These features
make it possible to write a library of simple scenarios which can then be used as
building blocks to construct many more complex scenarios.

Modular Scenarios. To deﬁne a named, reusable scenario, optionally with tunable
parameters, Scenic provides the scenario statement. For example, here is a sce-
nario which creates a parked car on the shoulder of the ego’s current lane (assuming
there is one), using some APIs from the driving library:

1
2
3
4
5

scenario ParkedCar(gap=0.25):

precondition: ego.laneGroup._shoulder != None
setup:

spot = OrientedPoint on visible ego.laneGroup.curb
parkedCar = Car left of spot by gap

The setup block contains Scenic code which executes when the scenario is
instantiated, and which can deﬁne classes, create objects, declare requirements,
etc. as in any of the example scenarios we saw above. Additionally, we can de-
ﬁne preconditions and invariants, which operate in the same way as for dynamic
behaviors. Having now deﬁned the ParkedCar scenario, we can use it in a more
complex scenario, potentially multiple times:

1
2
3
4
5

scenario Main():

setup:

ego = Car

compose:

do ParkedCar(), ParkedCar(0.5)

Here our Main scenario itself only creates the ego car; then its compose block
orchestrates how to run other modular scenarios. In this case, we invoke two copies
of the ParkedCar scenario in parallel, specifying in one case that the gap between
the parked car and the curb should be 0.5 m instead of the default 0.25. So the
scenario will involve three cars in total, and as usual Scenic will automatically
ensure that they are all on the road and do not intersect.

Parallel and Sequential Composition. The scenario above is an example of parallel
composition, where we use the do statement to run two scenarios at the same time.
We can also use sequential composition, where one scenario begins after another
ends. This is done the same way as in dynamic behaviors: in fact, the compose block

Scenic: A Language for Scenario Speciﬁcation and Data Generation

21

of a scenario is executed in essentially the same way as a monitor, and allows all
the same control-ﬂow constructs. For example, we could write a compose block as
follows:

1
2
3

while True:

do ParkedCar(gap=0.25) for 30 seconds
do ParkedCar(gap=0.5) for 30 seconds

Here, a new parked car is created every 30 seconds2, with the distance to the
curb alternating between 0.25 and 0.5 m. Note that without the for 30 seconds
qualiﬁer, we would never get past line 2, since the ParkedCar scenario does not
deﬁne any termination conditions using terminate when (or terminate) and so
runs forever by default. If instead we want to create a new car only when the ego
has passed the current one, we can use a do-until statement:

1
2
3

while True:

subScenario = ParkedCar(gap=0.25)
do subScenario until distance past subScenario.parkedCar > 10

Note how we can refer to the parkedCar variable created in the ParkedCar
scenario as a property of the scenario. Combined with the ability to pass objects
as parameters of scenarios, this is convenient for reusing objects across scenarios.

Interrupts, Overriding, and Initial Scenarios. The try-interrupt statement used in
behaviors can also be used in compose blocks to switch between scenarios. For
example, suppose we already have a scenario where the ego is following a leadCar,
and want to elaborate it by adding a parked car which suddenly pulls in front of
the lead car. We could write a compose block as follows:

1
2
3
4
5

try:

following = FollowingScenario()
do following

interrupt when distance to following.leadCar < 10:

do ParkedCarPullingAheadOf(following.leadCar)

If the ParkedCarPullingAheadOf scenario is deﬁned to end shortly after the
parked car ﬁnishes entering the lane, the interrupt handler will complete and
Scenic will resume executing FollowingScenario on line 3 (unless the ego is still
within 10 m of the lead car).

Suppose that we want the lead car to behave diﬀerently while ParkedCarPullingAheadOf
is running; for example, perhaps the behavior for the lead car deﬁned in FollowingScenario
does not handle a parked car suddenly pulling in. To enable changing the behavior
or other properties of an object in a sub-scenario, Scenic provides the override
statement, which we can use as follows:

1
2
3
4

scenario ParkedCarPullingAheadOf(target):

setup:

override target with behavior FollowLaneAvoidingCollisions
parkedCar = Car left of ...

2 In a real implementation, we would probably want to require that the parked car is not

initially visible from the ego, to avoid the sudden appearance of cars out of nowhere.

22

Daniel J. Fremont et al.

Here we override the behavior property of target for the duration of the sce-
nario, reverting it back to its original value (and thereby continuing to execute the
old behavior) when the scenario terminates. The override object speciﬁer ,
. . .
statement has the same syntax as an object deﬁnition, and can specify any prop-
erties of the object except for dynamic properties like position or speed which are
updated every time step by the simulator (and can only be indirectly controlled
by taking actions).

In order to allow writing scenarios which can both stand on their own and be
invoked during another scenario, Scenic provides a special conditional statement
testing whether we are inside the initial scenario, i.e., the very ﬁrst scenario to run.

1
2
3
4
5
6
7
8
9
10

scenario TwoLanePedestrianScenario():

setup:

if in initial scenario: # create ego car on random 2-lane road

roads = filter(lambda r: len(r.lanes) == 2, network.roads)
road = Uniform(*roads) # pick uniformly from list
ego = Car on road

else:

# use existing ego car; require it is on a 2-lane road

require len(ego.road.lanes) == 2
road = ego.road

Pedestrian on visible road.sidewalkRegion, with behavior ...

Random Selection of Scenarios. For very general scenarios, like “driving through
a city, encountering typical human traﬃc”, we may want a variety of diﬀerent
events and interactions to be possible. We saw above how we can write behaviors
for individual agents which choose randomly between possible actions; Scenic
allows us to do the same with entire scenarios. Most simply, since scenarios are
ﬁrst-class objects, we can write functions which operate on them, perhaps choosing
a scenario from a list of options based on some complex criterion:

1
2

chosenScenario = pickNextScenario(ego.position, ...)
do chosenScenario

However, some scenarios may only make sense in certain contexts; for example,
a scenario involving a car running a red light can take place only at an intersection.
To facilitate modeling such situations, Scenic provides variants of the do statement
which choose scenarios to run randomly amongst only those whose preconditions
are satisﬁed:

1
2
3

do choose RedLightRunner, Jaywalker, ParkedCar
do choose {RedLightRunner: 2, Jaywalker: 1, ParkedCar: 1}
do shuffle RedLightRunner, Jaywalker, ParkedCar

Here, line 1 checks the preconditions of the three given scenarios, then executes
one (and only one) of the enabled scenarios. If for example the current road has
no shoulder, then ParkedCar will be disabled and we will have a 50/50 chance of
executing either RedLightRunner or Jaywalker (assuming their preconditions are
satisﬁed). If none of the three scenarios are enabled, Scenic will reject the simula-
tion. Line 2 shows a non-uniform variant, where RedLightRunner is twice as likely
to be chosen as each of the other scenarios (so if only ParkedCar is disabled, we will
pick RedLightRunner with probability 2/3; if none are disabled, 2/4). Finally, line

Scenic: A Language for Scenario Speciﬁcation and Data Generation

23

3 is a shuﬄed variant, where all three scenarios will be executed, but in random
order3.

All of the examples we have seen above illustrate the versatility of Scenic in
modeling a wide range of interesting scenarios. Complete Scenic code for the
bumper-to-bumper scenario of Fig. 1, the Mars rover scenario of Fig. 5, as well as
other scenarios used as examples in this section or in our experiments, along with
images of generated scenes, can be found in Appendix A.

4 Syntax of Scenic

Scenic is an object-oriented PPL, with programs consisting of sequences of state-
ments built with standard imperative constructs including conditionals, loops,
functions, and methods (which we do not describe further, focusing on the new
elements). Compared to other imperative PPLs, the major restriction of Scenic,
made in order to allow more eﬃcient sampling, is that conditional branching may
not depend on random variables (except in behaviors). The novel syntax, outlined
above, is largely devoted to expressing spatiotemporal relationships in a concise
and ﬂexible manner. Figure 8 gives a formal grammar for Scenic, which we now
describe in detail.

4.1 Data Types

Scenic provides several primitive data types:

Booleans expressing truth values.
Scalars ﬂoating-point numbers, which can be sampled from various distributions

(see Table 1).

Vectors representing positions and oﬀsets in space, constructed from coordinates

in meters with the syntax (X, Y) 4.

Headings representing orientations in space. Conveniently, in 2D these are a single
angle (in radians, anticlockwise from North). By convention the heading of a
local coordinate system is the heading of its y-axis, so, for example, (-2, 3)
means 2 meters left and 3 ahead.

Vector Fields associating an orientation to each point in space. For example, the
shortest paths to a destination or (in our case study) the nominal traﬃc direc-
tion.

Regions representing sets of points in space. These can have an associated vector
ﬁeld giving points in the region preferred orientations (e.g. the surface of an
object could have normal vectors, so that objects placed randomly on the
surface face outward by default).

In addition, Scenic provides objects, organized into single-inheritance classes
specifying a set of properties their instances must have, together with correspond-
ing default values (see Fig. 8). Default value expressions are evaluated each time

3 Respecting preconditions, so in particular the simulation will be rejected if at some point

none of the remaining scenarios to execute are enabled.

4 The Smalltalk-like [21] syntax X @ Y used in earlier versions of Scenic is also legal.

24

Daniel J. Fremont et al.

program := (statement)∗
boolean := True | False | booleanOp

scalar := number | distrib | scalarOp
distrib := baseDist | resample(distrib)
vector := (scalar , scalar ) | Point
| vectorOp

heading := scalar | OrientedPoint

| headingOp

direction := heading | vectorField

value := boolean | scalar | vector

| direction | region
| object | object.property

classDef

:= class class[(superclass)]:
(property: value)∗

object := class speciﬁer , . . .
speciﬁer := with property value

| posSpec | headSpec

behavior := behavior name(params):

(precondition: boolean)∗
(invariant: boolean)∗
(statement)∗

try := try:

(statement)∗

(interrupt when boolean:

(statement)∗)∗
(except exception:
(statement)∗)∗

scenario := scenario name(params):

(precondition: boolean)∗
(invariant: boolean)∗
[setup:

(statement)∗]

[compose:

(statement)∗]

Fig. 8: Simpliﬁed Scenic grammar. Point and OrientedPoint are instances of the
corresponding classes. See Tab. 5 for statements, Fig. 10 for operators, Tab. 1 for
baseDist, and Tables 3 and 4 for posSpec and headSpec.

Table 1: Built-in distributions. All parameters are scalar s except value.

Syntax

Distribution

Range(low , high)
Uniform(value, . . . )
Discrete({value: weight, . . . })
Normal(mean, stdDev )
TruncatedNormal(mean, stdDev , low , high)

uniform on continuous interval
uniform over discrete values
discrete with weights
normal (Gaussian)
normal, truncated to the given window

an object is created. Thus if we write weight: Range(1, 5) when deﬁning a class
then each instance will have a weight drawn independently from Range(1, 5). De-
fault values may use the special syntax self.property to refer to one of the other
properties of the object, which is then a dependency of this default value. In our
case study, for example, the width and length of a Car are by default derived from
its model.

Physical objects in a scene are instances of Object, which is the default super-
class when none is speciﬁed. Object descends from the two other built-in classes:
its superclass is OrientedPoint, which in turn subclasses Point. These represent
locations in space, with and without an orientation respectively, and so provide
the fundamental properties heading and position. Object extends them by deﬁn-
ing a bounding box with the properties width and length, as well as temporal
information like speed and behavior. Table 2 lists the properties of these classes
and their default values.

To allow cleaner notation, Point and OrientedPoint are automatically inter-
preted as vectors or headings in contexts expecting these (as shown in Fig. 8). For
example, we can write taxi offset by (1, 2) and 30 deg relative to taxi in-
stead of taxi.position offset by (1, 2) and 30 deg relative to taxi.heading.
Ambiguous cases, e.g. taxi relative to limo, are illegal (caught by a simple type
system); the more verbose syntax must be used instead.

Scenic: A Language for Scenario Speciﬁcation and Data Generation

25

Table 2: Properties of the built-in classes Point, OrientedPoint, and Object.

Property

Default Meaning

position
viewDistance
mutationScale
positionStdDev

heading
viewAngle
headingStdDev

width
length
speed
velocity
angularSpeed
behavior
allowCollisions
requireVisible
regionContainedIn

(0, 0)
50
0
1

0
360◦
5◦

1
1
0
(0, 0)
0
None
false
true
None

position in global coordinates
distance for ‘can see’ predicate
overall scale of mutations
mutation σ for position

heading in global coordinates
angle for ‘can see’ predicate
mutation σ for heading

width of bounding box
length of bounding box
speed of object
velocity (default from speed, heading)
angular speed (in rad/s)
dynamic behavior, if any
collisions allowed
must be visible from ego
region object must be contained in

4.2 Expressions

Scenic’s expressions are mostly straightforward, largely consisting of the arith-
metic, boolean, and geometric operators shown in Fig. 10. The meanings of these
operators are largely clear from their syntax, so we defer complete deﬁnitions of
their semantics to the Appendix [18]. Figure 9 illustrates several of the geometric
operators (as well as some speciﬁers, which we will discuss in the next section).
Various points to note:

• X can see Y uses a simple model where a Point can see a certain distance, and
an OrientedPoint restricts this to the sector along its heading with a certain
angle (see Table 2). An Object is visible iﬀ its bounding box is.

• X relative to Y interprets X as an oﬀset in a local coordinate system deﬁned
by Y . Thus (-3, 0) relative to Y yields 3 m West of Y if Y is a vector, and
3 m left of Y if Y is an OrientedPoint. If deﬁning a heading inside a speciﬁer,
either X or Y can be a vector ﬁeld, interpreted as a heading by evaluating
it at the position of the object being speciﬁed. So we can write for example
Car at (120, 70), facing 30 deg relative to roadDirection.

• visible region yields the part of the region visible from the ego, so we can
write for example Car on visible road. The form region visible from X uses
X instead of ego.

• front of Object, front left of Object, etc. yield the corresponding points on

the bounding box of the object, oriented along the object’s heading.
Two types of Scenic expressions are more complex: distributions and object
deﬁnitions. As in a typical imperative probabilistic programming language, a dis-
tribution evaluates to a sample from the distribution. Thus the program

1
2

x = Range(0, 1)
y = (x, x)

26

Daniel J. Fremont et al.

Point beyond P by (-2, 1)

1

2

P offset by (0, -2)

P

apparent heading of P

2

Object behind P by 2

1

left of ego

2

ego

Point offset by (1, 2)
or
(1, 2) relative to ego

back right of ego

Fig. 9: Various Scenic operators and speciﬁers applied to the ego object and an
OrientedPoint P. Instances of OrientedPoint are shown as bold arrows.

scalarOperator := max(scalar , . . . ) | min(scalar , . . . )

| -scalar | abs(scalar ) | scalar (+ | *) scalar
| relative heading of heading [from heading]
| apparent heading of OrientedPoint [from vector ]
| distance to vector [from vector ]
| distance [of OrientedPoint] past vector
| angle [from vector ] to vector

booleanOperator := not boolean
| boolean (and | or) boolean
| scalar (== | != | < | > | <= | >=) scalar
| (Point | OrientedPoint) can see (vector | Object)
| (vector | Object) in region
headingOperator := scalar deg

| vectorField at vector
| direction relative to direction

vectorOperator := vector relative to vector

| vector offset by vector
| vector offset along direction by vector

regionOperator := visible region

| region visible from (Point | OrientedPoint)

orientedPointOperator :=

vector relative to OrientedPoint

| OrientedPoint offset by vector
| (front | back | left | right) of Object
| (front | back) (left | right) of Object

Fig. 10: Operators by result type.

does not make y uniform over the unit box, but rather over its diagonal. For conve-
nience in sampling multiple times from a primitive distribution, Scenic provides
a resample(D) function returning an independent5 sample from D, one of the

5 Conditioned on the values of the distribution’s parameters (e.g. low and high for a uniform

interval), which are not resampled.

Scenic: A Language for Scenario Speciﬁcation and Data Generation

27

Table 3: Speciﬁers for position. Those in the second group also optionally specify
heading.

Speciﬁer

at vector
offset by vector
offset along direction by vector
(left | right) of vector [by scalar ]
(ahead of | behind) vector [by scalar ]
beyond vector by vector [from vector ]
visible [from (Point | OrientedPoint)]

Dependencies

—
—
—
heading, width
heading, length
—
—

(in | on) region
(left | right) of (OrientedPoint | Object) [by scalar ]
(ahead of | behind) (OrientedPoint | Object) [by scalar ]
following vectorField [from vector ] for scalar

—
width
length
—

distributions in Tab. 1. Scenic also allows deﬁning custom distributions beyond
those in the Table.

The second type of complex Scenic expressions are object deﬁnitions. These are
the only expressions with a side eﬀect, namely creating an object in the generated
scene. More interestingly, properties of objects are speciﬁed using the system of
speciﬁers discussed above, which we now detail.

4.3 Speciﬁers

As shown in the grammar in Fig. 8, an object is created by writing the class name
followed by a (possibly empty) comma-separated list of speciﬁers. The speciﬁers
are combined, possibly adding default speciﬁers from the class deﬁnition, to form
a complete speciﬁcation of all properties of the object. Arbitrary properties (in-
cluding user-deﬁned properties with no meaning in Scenic) can be speciﬁed with
the generic speciﬁer with property value, while Scenic provides many more spec-
iﬁers for the built-in properties position and heading, shown in Tables 3 and 4
respectively.

In general, a speciﬁer is a function taking in values for zero or more properties,
its dependencies, and returning values for one or more other properties, some of
which can be speciﬁed optionally, meaning that other speciﬁers will override them.
For example, on region speciﬁes position and optionally speciﬁes heading if the
given region has a preferred orientation. If road is such a region, as in our case
study, then Object on road will create an object at a position uniformly random in
road and with the preferred orientation there. But since heading is only speciﬁed
optionally, we can override it by writing Object on road, facing 20 deg.

Speciﬁers are combined to determine the properties of an object by evaluating
them in an order ensuring that their dependencies are always already assigned. If
there is no such order or a single property is speciﬁed twice, the scenario is ill-
formed. The procedure by which the order is found, taking into account properties
that are optionally speciﬁed and default values, will be described in the next
section.

28

Daniel J. Fremont et al.

Table 4: Speciﬁers for heading.

Speciﬁer

facing heading
facing vectorField
facing (toward | away from) vector
apparently facing heading [from vector ]

Deps.

—
position
position
position

As the semantics of the speciﬁers in Tables 3 and 4 are largely evident from
their syntax, we defer exact deﬁnitions to the Appendix [18]. We brieﬂy discuss
some of the more complex speciﬁers, referring to the examples in Fig. 9:

• behind vector means the object is placed with the midpoint of its front edge at

the given vector, and similarly for ahead/left/right of vector .

• beyond A by O from B means the position obtained by treating O as an oﬀset
in the local coordinate system at A oriented along the line of sight from B. In
this and other speciﬁers, if the from B is omitted, the ego object is used by
default. So for example beyond taxi by (0, 3) means 3 m directly behind the
taxi as viewed by the camera (see Fig. 9 for another example).

• The heading optionally speciﬁed by left of OrientedPoint, etc. is that of the
OrientedPoint (thus in Fig. 9, P offset by (0, -2) yields an OrientedPoint
facing the same way as P). Similarly, the heading optionally speciﬁed by the
following vectorField speciﬁer is that of the vector ﬁeld at the speciﬁed position.
• apparently facing H means the object has heading H with respect to the line
of sight from ego. For example, apparently facing 90 deg would orient the
object so that the camera views its left side head-on.

4.4 Statements

Finally, we discuss Scenic’s statements, listed in Table 5. Class and object deﬁni-
tions have been discussed above, and variable assignment behaves in the standard
way.

Selecting a World Model. The model name statement speciﬁes that the Scenic pro-
gram is written for the given Scenic world model. It is equivalent to the statement
from name import * (as in Python), importing everything from the given Scenic
module, but can be overridden from the command-line when running the Scenic
tool. This enables writing cross-platform scenarios using abstract domains like
scenic.domains.driving, then executing them in particular simulators by overrid-
ing the model with a more speciﬁc module (e.g. scenic.simulators.carla.model).

Global Parameters. The statement param name = value,
. . . assigns values to global
parameters of the scenario. These have no semantics in Scenic but provide a
general-purpose way to encode arbitrary global information. For example, in our
case study we used parameters time and weather to put distributions on the time
of day and the weather conditions during the scene.

Scenic: A Language for Scenario Speciﬁcation and Data Generation

29

Table 5: Statements (excluding if, while, def, import, etc. from Python). Those
in the second group are only legal inside behaviors, monitors, and compose blocks.

Syntax

Meaning

model name
name = value
param name = value, . . .
classDefn (see Fig. 8)
object
behavior
monitor
scenario
require boolean
require[number ] boolean
require always boolean
require eventually boolean
terminate when boolean
mutate name, . . .

[by number ]

select world model
variable assignment
global parameter assignment
class deﬁnition
object deﬁnition
behavior deﬁnition
monitor deﬁnition
(modular) scenario deﬁnition
hard requirement
soft requirement
always dynamic requirement
eventually dynamic requirement
termination condition
enable mutation

take action, . . .
wait
terminate
do name, . . .
do name, . . . for scalar (seconds | steps)
do name, . . . until boolean
try (see Fig. 8)
abort
override name speciﬁer , . . .

invoke action(s)
invoke no actions this step
end scenario immediately
invoke sub-behavior(s)/sub-scenario(s)
invoke with time limit
invoke until condition
try-interrupt statement
abort try-interrupt statement
override object properties dynamically

Behaviors and Monitors. The behavior statement (see Fig. 8) deﬁnes a dynamic
behavior. A behavior deﬁnition has the same structure as a function deﬁnition, ex-
cept: 1) it may begin with any number of precondition: boolean and invariant: boolean
lines deﬁning preconditions and invariants; 2) it may use the statements in the sec-
ond section of Tab. 5, which are not allowed in ordinary functions. The monitor
statement has the same structure as a behavior statement but deﬁnes a monitor.

Modular Scenarios. The scenario statement (see Fig. 8) deﬁnes a modular scenario
which can be invoked from another scenario. Scenario deﬁnitions begin like behav-
ior deﬁnitions, with a name, parameters, preconditions, and invariants. However,
the body of a scenario consists of two parts, either of which can be omitted: a setup
block and a compose block. The setup block contains code that runs once when
the scenario begins to execute, and is a list of statements like a top-level Scenic
program6. The compose block orchestrates the execution of sub-scenarios during a
dynamic scenario, and may use do and any of the other statements allowed inside
behaviors (except take, which only makes sense for an individual agent).

Requirements. The require boolean statement requires that the given condition
hold in all generated scenarios (equivalently to observe statements in other prob-
abilistic programming languages; see e.g. [41, 6]). The variant require[p] boolean

6 In fact, a top-level Scenic program is equivalent to an unnamed scenario deﬁnition with
no parameters, preconditions, invariants, or compose block, and whose start block consists of
the whole program.

30

Daniel J. Fremont et al.

adds a soft requirement that need only hold with some probability p (which must
be a constant). We will discuss the semantics of these in the next section. The
require always and require eventually variants deﬁne requirements that must
hold in every and some time step of the scenario respectively.

Mutation. The mutate instance,
. . . by number statement adds Gaussian noise
with the given standard deviation (default 1) to the position and heading prop-
erties of the listed objects (or every Object, if no list is given). For example,
mutate taxi by 2 would add twice as much noise as mutate taxi. The noise can
be controlled separately for position and heading, as we discuss in the next sec-
tion.

Termination Conditions. The terminate when boolean statement deﬁnes a condi-
tion which is monitored as in require eventually, but which when true causes
the scenario to end. The terminate statement can be called inside a behavior,
monitor, or compose block to end the scenario immediately.

Actions. The take action,
. . . statement can be used inside behaviors to select one
or more actions7 for the agent to take in the current time step. The wait statement
means no actions are taken in this time step (which makes sense inside monitors
and compose blocks). When either of these statements is executed, the behavior is
suspended until one time step has elapsed; then its invariants are checked (raising
an InvariantViolation exception if any are violated) and it is resumed.

. . . statement has the same
Invoking Other Behaviors and Scenarios. The do name,
structure as the take statement, but invokes one or more behaviors (if in a be-
havior) or scenarios (if in a compose block). It does not return until the sub-
behavior/sub-scenario terminates, so multiple time steps may pass (unlike take).
Early termination can be enabled by adding a for scalar seconds/steps clause,
which enforces a maximum time limit, or an until boolean clause, which adds an
arbitrary termination criterion. When the do statement returns, the invariants of
the calling behavior/scenario are checked as above.

Interrupts. The try statement (see Fig. 8) consists of a try: block and one or more
interrupt when boolean: and except exception: blocks, each containing arbitrary
lists of statements. As described in Sec. 3.2, when a try statement executes, the
conditions for each interrupt when block are checked at each time step. While
none of them are true, the try block executes. When an interrupt condition be-
comes true, the body of the corresponding block is executed (with lower blocks
preempting those above), suspending any behaviors/scenarios that were executing
in the try block until the interrupt handler completes (at which point the invari-
ants of the suspended behavior/scenario are checked as usual). Any exceptions
raised in the try block or any interrupt handler can be caught by except blocks
as in the Python try statement. Additionally, any block may execute the abort
statement to immediately terminate the entire try statement.

7 The statement will accept lists and tuples of actions, in order to support taking a number
of actions that is not ﬁxed, i.e., if myActions is a list of actions, we can write take myActions.

Scenic: A Language for Scenario Speciﬁcation and Data Generation

31

Overrides. The override name speciﬁer ,
. . . statement may be used inside a sce-
nario deﬁnition to override properties of an object during a dynamic scenario.
It has the same structure as an object deﬁnition, with override and the name
of the object replacing the class, so for example given an object taxi we could
write override taxi with aggression 3 to set the aggression property of taxi
to 3. Dynamic properties read back from the simulator at every time step, like
position, cannot be overridden since they are controlled using actions and not
direct assignments. Properties overridden by a scenario revert to their original
values when the scenario terminates. When the behavior property is overridden,
the original behavior is suspended, then resumed at the end of the scenario.

5 Semantics and Scenario Generation

5.1 Semantics of Scenic

The output of a Scenic program has two parts: ﬁrst, a scene consisting of an
assignment to all the properties of each Object deﬁned in the scenario, plus any
global parameters deﬁned with param. For dynamic scenarios, this scene forms the
initial state of the scenario, which then changes after each time step according
to the actions taken by the agents. Since actions and their eﬀects are domain-
speciﬁc (consider for example the diﬀerent physics involved for aerial, ground, and
underwater vehicles), dynamic Scenic scenarios do not directly deﬁne trajectories
for objects. Instead, the second part of the output of a Scenic program is a policy,
a function mapping the history of past scenes to the choice of actions for the
agents in the current time step8. This pair of a scene and a policy is what we
mean formally by the scenario generated by a Scenic program.

Since Scenic is a probabilistic programming language, the semantics of a pro-
gram is actually a distribution over possible outputs, here scenarios. As for other
imperative PPLs, the semantics can be deﬁned operationally as a typical inter-
preter for an imperative language but with two diﬀerences. First, the interpreter
makes random choices when evaluating distributions [52]. For example, the Scenic
statement x = Range(0, 1) updates the state of the interpreter by assigning a
value to x drawn from the uniform distribution on the interval (0, 1). In this way
every possible run of the interpreter has a probability associated with it. Second,
every run where a require statement (the equivalent of an “observation” in other
PPLs) is violated gets discarded, and the run probabilities appropriately normal-
ized (see, e.g., [26]). For example, adding the statement require x > 0.5 above
would yield a uniform distribution for x over the interval (0.5, 1).

Scenic uses the standard semantics for assignments, arithmetic, loops, func-
tions, and so forth. Below, we deﬁne the semantics of the main constructs unique
to Scenic. See the Appendix [18] for a more formal treatment.

Soft Requirements. The statement require[p] B is interpreted as require B with
probability p and as a no-op otherwise: that is, it is interpreted as a hard require-
ment that is only checked with probability p. This ensures that the condition B will

8 In fact the policy is a probabilistic function, since behaviors can make random choices,
and it can also return special values indicating that the scenario should terminate or that it
has violated a requirement and should be discarded, as we discuss below.

32

Daniel J. Fremont et al.

hold with probability at least p in the induced distribution of the Scenic program,
as desired.

Speciﬁers and Object Deﬁnitions. As we saw above, each speciﬁer deﬁnes a function
mapping values for its dependencies to values for the properties it speciﬁes. When
an object of class C is constructed using a set of speciﬁers S, the object is deﬁned
as follows (see the Appendix [18] for details):

1. If a property is speciﬁed (non-optionally) by multiple speciﬁers in S, an ambi-

guity error is raised.

2. The set of properties P for the new object is found by combining the properties
speciﬁed by all speciﬁers in S with the properties inherited from the class C.
3. Default value speciﬁers from C are added to S as needed so that each property
in P is paired with a unique speciﬁer in S specifying it, with precedence order:
non-optional speciﬁer, optional speciﬁer, then default value.

4. The dependency graph of the speciﬁers S is constructed. If it is cyclic, an error

is raised.

5. The graph is topologically sorted and the speciﬁers are evaluated in this order

to determine the values of all properties P of the new object.

Mutation. The mutate X by N statement sets the special mutationScale property
to N (the mutate X form sets it to 1). At the end of evaluation of the Scenic pro-
gram, but before requirements are checked, Gaussian noise is added to the position
and heading properties of objects with nonzero mutationScale. The standard de-
viation of the noise is the value of the positionStdDev and headingStdDev property
respectively (see Table 2), multiplied by mutationScale.

Dynamic Constructs. As suggested in Sec. 4.4, behaviors and monitors are corou-
tines: they usually execute like ordinary functions, but are suspended when they
take an action (or wait) until one time step has passed. Scenarios behave similarly:
in their compose blocks, using wait causes them to wait for one step, and any sub-
scenarios they invoke using do run recursively; scenarios without compose blocks
do nothing in a time step other than check whether any of their terminate when
conditions have been met or their require always conditions violated.

The output of the policy of a dynamic Scenic program is deﬁned according to

the following procedure:

1. Run the compose blocks of all currently-running scenarios for one time step.
If any require conditions fail, discard the simulation. If instead the top-level
scenario ﬁnishes its compose block (if any), one of its terminate when conditions
is true, or it executes terminate, set a ﬂag to remember this (we use a ﬂag rather
than terminating immediately since we need to ensure that all requirements
are satisﬁed before terminating).

2. Check all require always conditions of currently-running scenarios; if any fail,

discard the simulation.

3. Run all monitors of currently-running scenarios for one time step. As above,
discard the simulation if any require conditions fail, and set the terminate ﬂag
if the terminate statement is executed.

4. If the ﬂag is set, check that all require eventually conditions were satisﬁed

at some time step: if so, terminate the simulation; otherwise, discard it.

Scenic: A Language for Scenario Speciﬁcation and Data Generation

33

5. Run all the behaviors of dynamic agents for one time step, gathering their
actions and discarding the simulation or setting the terminate ﬂag as in (3).

6. Repeat (4) to check the terminate ﬂag.
7. Return the choice of actions selected by the dynamic agents.

The problem of sampling scenes from the distribution deﬁned by a Scenic pro-
gram is essentially a special case of the sampling problem for imperative PPLs
with observations (since soft requirements can also be encoded as observations).
While we could apply general techniques for such problems9, the domain-speciﬁc
design of Scenic enables specialized sampling methods, which we discuss below.
We also note that the scenario generation problem is closely related to control im-
provisation, an abstract framework capturing various problems requiring synthesis
under hard, soft, and randomness constraints [16]. Scenario improvisation from a
Scenic program can be viewed as an extension with a more detailed randomness
constraint given by the imperative part of the program.

5.2 Domain-Speciﬁc Sampling Techniques

The geometric nature of the constraints in Scenic programs, together with Scenic’s
lack of conditional control ﬂow outside behaviors, enable domain-speciﬁc sampling
techniques inspired by robotic path planning methods. Speciﬁcally, we can use
ideas for constructing conﬁguration spaces to prune parts of the sample space
where the objects being positioned do not ﬁt into the workspace. Furthermore, by
combining spatial and temporal constraints, we can prune some initial scenes by
proving that they force a requirement to be violated at some future point during a
dynamic scenario. We describe several pruning techniques below, deferring formal
statements of the algorithms to the Appendix [18].

Pruning Based on Containment. The simplest technique applies to any object X
whose position is uniform in a region R and which must be contained in a re-
gion C (e.g. the road in our case study). If minRadius is a lower bound on the
distance from the center of X to its bounding box, then we can restrict R to
R ∩ erode(C, minRadius). This is sound, since if X is centered anywhere not in the
restriction, then some point of its bounding box must lie outside of C.

Pruning Based on Orientation. The next technique applies to scenarios placing con-
straints on the relative heading and the maximum distance M between objects X
and Y , which are oriented with respect to a vector ﬁeld that is constant within
polygonal regions (such as our roads). For each polygon P , we ﬁnd all polygons
Qi satisfying the relative heading constraints with respect to P (up to a pertur-
bation if X and Y need not be exactly aligned to the ﬁeld), and restrict P to
P ∩ dilate(∪Qi, M ). This is also sound: suppose X can be positioned at x in poly-
gon P . Then Y must lie at some y in a polygon Q satisfying the constraints, and
since the distance from x to y is at most M , we have x ∈ dilate(Q, M ).

9 Note however that the presence of dynamic agents complicates the use of standard PPL
techniques, since the fact that the physics relating actions to their eﬀects on the world is
not modeled in Scenic means that the program eﬀectively contains an unknown, black-box
function.

34

Daniel J. Fremont et al.

Pruning Based on Size. In the setting above of objects X and Y aligned to a
polygonal vector ﬁeld (with maximum distance M ), we can also prune the space
using a lower bound on the width of the conﬁguration. For example, in our bumper-
to-bumper scenario we can infer such a bound from the offset by speciﬁers in the
program. We ﬁrst ﬁnd all polygons that are not wide enough to ﬁt the conﬁguration
according to the bound: call these “narrow”. Then we restrict each narrow polygon
P to P ∩ dilate(∪Qi, M ) where Qi runs over all polygons except P . To see that this
is sound, suppose object X can lie at x in polygon P . If P is not narrow, we do
not restrict it; otherwise, object Y must lie at y in some other polygon Q. Since
the distance from x to y is at most M , as above we have x ∈ dilate(Q, M ).

Pruning Based on Reachability. Finally, we can prune initial positions for objects
which make it impossible to reach a goal location within the duration of the
scenario; for example, a car which travels down a road and then runs a red light
must start suﬃciently close to an intersection. Suppose an object is required to
enter a region R within T time (either by an explicit require eventually statement
or a precondition of a behavior or scenario guaranteed to eventually execute) and
we have an upper bound S on the object’s speed. Then we can prune away all
initial positions of the object which do not lie within a distance D = ST of R, i.e.,
we can restrict its initial positions to dilate(R, D). If the object is also required to
stay within some containing region C (e.g., a road) for the entire duration of the
scenario, we can compute a tighter value of D by considering only paths that lie
within C.

After pruning the space as described above, our implementation uses rejection
sampling, generating scenes from the imperative part of the scenario until all re-
quirements are satisﬁed. While this samples from exactly the desired distribution,
it has the drawback that a huge number of samples may be required to yield a
single valid scene (in the worst case, when the requirements have probability zero
of being satisﬁed, the algorithm will not even terminate). However, we found in
our experiments that all reasonable scenarios we tried required only several hun-
dred iterations at most, yielding a sample within a few seconds. Furthermore, the
pruning methods above could reduce the number of samples needed by a factor of
3 or more (see the Appendix [18] for details of our experiments). In future work
it would be interesting to see whether Markov chain Monte Carlo methods pre-
viously used for probabilistic programming (see, e.g., [41, 44, 60]) could be made
eﬀective in the case of Scenic.

6 Experiments

We demonstrate the three applications of Scenic discussed in Sec. 2: testing a
system under particular conditions, either a perception component in isolation
(6.2.1) or a dynamic closed-loop system (6.2.2), training a system to improve
accuracy in hard cases (6.3), and debugging failures (6.4). We begin by describing
the general experimental setup.

Scenic: A Language for Scenario Speciﬁcation and Data Generation

35

6.1 Experimental Setup

For our main case study, we generated scenes in the virtual world of the video
game Grand Theft Auto V (GTAV) [47]. We wrote a Scenic world model deﬁning
Regions representing the roads and curbs in (part of) this world, as well as a type
of object Car providing two additional properties10: model, representing the type
of car, with a uniform distribution over 13 diverse models provided by GTAV, and
color, representing the car color, with a default distribution based on real-world
car color statistics [9]. In addition, we implemented two global scene parameters:
time, representing the time of day, and weather, representing the weather as one
of 14 discrete types supported by GTAV (e.g. “clear” or “snow”).

GTAV is closed-source and does not expose any kind of scene description lan-
guage. Therefore, to import scenes generated by Scenic into GTAV, we wrote a
plugin based on DeepGTAV11. The plugin calls internal functions of GTAV to
create cars with the desired positions, colors, etc., as well as to set the camera
position, time of day, and weather.

Our experiments used SqueezeDet [61], a convolutional neural network real-
time object detector for autonomous driving12. We used a batch size of 20 and
trained all models for 10,000 iterations unless otherwise noted. Images captured
from GTAV with resolution 1920 × 1200 were resized to 1248 × 384, the resolution
used by SqueezeDet and the standard KITTI benchmark [20]. All models were
trained and evaluated on NVIDIA TITAN XP GPUs.

We used standard metrics precision and recall to measure the accuracy of de-
tection on a particular image set. The accuracy is computed based on how well
the network predicts the correct bounding box, score, and category of objects in
the image set. Details are in the Appendix [18], but in brief, precision is deﬁned
as tp/(tp + f p) and recall as tp/(tp + f n), where true positives tp is the number of
correct detections, false positives f p is the number of predicted boxes that do not
match any ground truth box, and false negatives f n is the number of ground truth
boxes that are not detected.

6.2 Testing and Falsiﬁcation

We begin with the most straightforward application of Scenic, namely generating
specialized data to test a system under particular conditions. We demonstrate
both using a static scenario to test a perception component, and using a dynamic
scenario to falsify a closed-loop system.

6.2.1 Testing a Perception Module

When testing a model, one may be interested in a particular operation regime.
For instance, an autonomous car manufacturer may be more interested in certain
road conditions (e.g. desert vs. forest roads) depending on where its cars will be

10 For the full deﬁnition of Car, see the Appendix [18]; the deﬁnitions of road, curb, etc. are
a few lines loading the corresponding sets of points from a ﬁle storing the GTAV map (see the
Appendix for how this ﬁle was generated).
11 https://github.com/aitorzip/DeepGTAV
12 Used industrially, for example by DeepScale (http://deepscale.ai/).

36

Daniel J. Fremont et al.

mainly used. Scenic provides a systematic way to describe scenarios of interest
and construct corresponding test sets.

To demonstrate this, we ﬁrst wrote very general scenarios describing static
scenes of 1–4 cars (not counting the camera), specifying only that the cars face
within 10◦ of the road direction. We generated 1,000 images from each scenario,
yielding a training set Xgeneric of 4,000 images, and used these to train a model
Mgeneric as described in Sec. 6.1. We also generated an additional 50 images from
each scenario to obtain a generic test set Tgeneric of 200 images.

Next, we specialized the general scenarios in opposite directions: scenarios for
good/bad road conditions ﬁxing the time to noon/midnight and the weather to
sunny/rainy respectively, generating specialized test sets Tgood and Tbad.

Evaluating Mgeneric on Tgeneric, Tgood, and Tbad, we obtained precisions of
83.1%, 85.7%, and 72.8%, respectively, and recalls of 92.6%, 94.3%, and 92.8%.
This shows that, as might be expected, the model performs better on bright days
than on rainy nights. This suggests there might not be enough examples of rainy
nights in the training set, and indeed under our default weather distribution rain
is less likely than shine. This illustrates how specialized test sets can highlight the
weaknesses and strengths of a particular model. In Sec. 6.3, we go one step further
and use Scenic to redesign the training set and improve model performance.

6.2.2 Falsifying a Dynamic Closed-Loop System

Next, we demonstrate how we can use a dynamic Scenic scenario to test a closed-
loop system, using VerifAI’s falsiﬁcation facilities to monitor and analyze coun-
terexamples to a system-level speciﬁcation. We tested an autonomous agent13 in
the CARLA [7] driving simulator, for which we wrote a similar Scenic world model
as we did for GTAV. This agent consists of a planner and controller (but no per-
ception components) which implement basic driving behaviors including abiding
by traﬃc lights, lane following, and collision avoidance.

We wrote a Scenic program describing a scenario where the ego vehicle (i.e. the
autonomous agent) is performing a right turn at an intersection, yielding to the
crossing traﬃc. As the ego approaches the intersection, the traﬃc light turns green,
but a crossing car runs the red light. The ego vehicle has to decide either to yield
or make a right turn. The crossing car executes a reactive behavior where it slows
down to maintain a minimum distance with any car in front.

We allowed three environment parameters to vary in this scenario:

– The traﬃc light’s transition from red to green is triggered when the distance
between the ego and the crossing car reaches a threshold, which was uniformly
random between 10–20 m.

– The crossing car’s speed was uniformly random between 5–12 m/s.
– The scenario takes place at a random 4-way intersection in the CARLA map.
To demonstrate how Scenic programs can be written in a generic, map-agnostic
style, we used the same Scenic code on two diﬀerent CARLA maps (Town05
and Town03).

We formulated a safety speciﬁcation for the autonomous agent in Metric Tem-
poral Logic, stating that the distance between the agent and the crossing car must

13 https://github.com/carla-simulator/carla/blob/dev/PythonAPI/examples/
automatic_control.py

Scenic: A Language for Scenario Speciﬁcation and Data Generation

37

Fig. 11: Falsiﬁcation results in CARLA. Top: Town05; bottom: Town03.

be greater than 5 meters at all times. Giving this speciﬁcation and the Scenic pro-
gram to VerifAI, we generated 2,000 scenarios for each map. VerifAI monitored
each simulation and computed the robustness value ρ of the MTL speciﬁcation,
which measures how strongly the speciﬁcation was satisﬁed [33] (negative values
meaning it was violated).

Our results are shown in Fig. 11. On the left, we plot ρ as a function of the traﬃc
light trigger threshold and the speed of the crossing car. Each dot represents one
simulation, with redder colors indicating smaller ρ, i.e., being closer to violating the
safety speciﬁcation. We found a signiﬁcant number of violations, approximately
21% and 17% of tests on Town05 and Town03 respectively. From the plots we
observe broadly similar behavior across the two maps, with the distance when
the traﬃc light switch occurs being the dominant factor controlling failures of the
autonomous agent (most failures occurring for values of 15–25 m).

On the right side of Fig. 11, we plot the average value of ρ at each intersection,
with color again indicating the average value of ρ and the size of each dot being
proportional to its variance. We can see that some intersections are much easier
or harder for the autonomous agent to handle. Investigating some of the most
extreme intersections, we observed that those with 4-lane legs and a turning radius
of about 6.5 m caused the agent to fail most frequently. Re-testing the agent at
such intersections, we found that this geometry often created a situation where
the agent and the crossing car were merging into the same lane simultaneously,
instead of one car completing its maneuver before the other.

These results show how we can use Scenic to ﬁnd scenarios where a closed-loop
system violates its speciﬁcation. In 6.4, we will further show how Scenic can help
us diagnose the root causes of failures and eliminate them through retraining.

1015202530Traffic Light Switch Distance (m)56789101112Crossing Car Speed (m/s)Falsification Test Results051015202530rho20015010050050100X Position10050050100Y PositionTested Ego Vehicle Locations0246810average rho1015202530Traffic Light Switch Distance (m)56789101112Crossing Car Speed (m/s)Falsification Test Results0510152025303540rho1005005010015010050050100150505101520253038

1
2
3
4
5
6
7

wiggle = Range(-10 deg, 10 deg)
ego = Car with roadDeviation wiggle
c = Car visible,

with roadDeviation resample(wiggle)
leftRight = Uniform(1.0, -1.0) * Range(1.25, 2.75)
Car beyond c by (leftRight, Range(4, 10)),
with roadDeviation resample(wiggle)

Daniel J. Fremont et al.

Fig. 12: A scenario where one car partially occludes another. The property
roadDeviation is deﬁned in Car to mean its heading relative to the roadDirection.

Fig. 13: Two scenes generated from the partial-occlusion scenario.

6.3 Training on Rare Events

In the synthetic data setting, we are limited not by data availability but by the
cost of training. The natural question is then how to generate a synthetic data set
that as eﬀective as possible given a ﬁxed size. In this section we show that over-
representing a type of input that may occur rarely but is diﬃcult for the model can
improve performance on the hard case without compromising performance in the
typical case. Scenic makes this possible by allowing the user to write a scenario
capturing the hard case speciﬁcally.

For our car detection task, an obvious hard case is when one car substantially
occludes another. We wrote a simple scenario, shown in Fig. 12, which generates
such scenes by placing one car behind the other as viewed from the camera, oﬀset
left or right so that it is at least partially visible; Fig. 13 shows some of the resulting
images. Generating images from this scenario we obtained a training set Xoverlap
of 250 images and a test set Toverlap of 200 images.

For a baseline training set we used the “Driving in the Matrix” synthetic
data set [30], which has been shown to yield good car detection performance even
on real-world images14. Like our images, the “Matrix” images were rendered in
GTAV; however, rather than using a PPL to guide generation, they were produced
by allowing the game’s AI to drive around randomly while periodically taking

14 We use the “Matrix” data set since it is known to be eﬀective for car detection and was not
designed by us, making the fact that Scenic is able to improve it more striking. The results
of this experiment also hold under the Average Precision (AP) metric used in [30], as well as
in a similar experiment using the Scenic generic two-car scenario from the last section as the
baseline. See Appendix [18] for details.

Scenic: A Language for Scenario Speciﬁcation and Data Generation

39

Table 6: Performance of models trained on 5,000 images from Xmatrix or a mixture
with Xoverlap, averaged over 8 training runs with random selections of images from
Xmatrix.

Mixture
%

100 / 0
95 / 5

Tmatrix

Toverlap

Precision

Recall

Precision

Recall

72.9 ± 3.7
73.1 ± 2.3

37.1 ± 2.1
37.0 ± 1.6

62.8 ± 6.1
68.9 ± 3.2

65.7 ± 4.0
67.3 ± 2.4

screenshots. We randomly selected 5,000 of these images to form a training set
Xmatrix, and 200 for a test set Tmatrix. We trained SqueezeDet for 5,000 iterations
on Xmatrix, evaluating it on Tmatrix and Toverlap. To reduce the eﬀect of jitter
during training we used a standard technique [2], saving the last 10 models in steps
of 10 iterations and picking the one achieving the best total precision and recall.
This yielded the results in the ﬁrst row of Tab. 6. Although Xmatrix contains many
images of overlapping cars, the precision on Toverlap is signiﬁcantly lower than for
Tmatrix, indicating that the network is predicting lower-quality bounding boxes for
such cars15.

Next we attempted to improve the eﬀectiveness of the training set by mixing in
the diﬃcult images produced with Scenic. Speciﬁcally, we replaced a random 5%
of Xmatrix (250 images) with images from Xoverlap, keeping the overall training
set size constant. We then retrained the network on the new training set and
evaluated it as above. To reduce the dependence on which images were replaced,
we averaged over 8 training runs with diﬀerent random selections of the 250 images
to replace. The results are shown in the second row of Tab. 6. Even altering only
5% of the training set, performance on Toverlap signiﬁcantly improves. Critically,
the improvement on Toverlap is not paid for by a corresponding decrease on Tmatrix:
performance on the original data set remains the same. Thus, by allowing us to
specify and generate instances of a diﬃcult case, Scenic enables the generation of
more eﬀective training sets than can be obtained through simpler approaches not
based on PPLs.

6.4 Debugging Failures

In our ﬁnal experiment, we show how Scenic can be used to generalize a single
input on which a model fails, exploring its neighborhood in a variety of diﬀerent
directions and giving insight into which features of the scene are responsible for
the failure. The original failure can then be generalized to a broader scenario
describing a class of inputs on which the model misbehaves, which can in turn
be used for retraining. We selected one scene from our ﬁrst experiment, shown
in Fig. 14, consisting of a single car viewed from behind at a slight angle, which
Mgeneric wrongly classiﬁed as three cars (thus having 33.3% precision and 100%
recall). We wrote several scenarios which left most of the features of the scene
ﬁxed but allowed others to vary. Speciﬁcally, scenario (1) varied the model and

15 Recall is much higher on Toverlap, meaning the false-negative rate is better; this is pre-
sumably because all the Toverlap images have exactly 2 cars and are in that sense easier than
the Tmatrix images, which can have many cars.

40

Daniel J. Fremont et al.

Fig. 14: The misclassiﬁed image, with the predicted bounding boxes.

Table 7: Performance of Mgeneric on diﬀerent scenarios representing variations of
the image in Fig. 14.

Scenario

Precision Recall

(1) varying model and color
(2) varying background
(3) varying local position, orientation

(4) varying position but staying close
(5) any position, same apparent angle
(6) any position and angle
(7) varying background, model, color

(8) staying close, same apparent angle
(9) staying close, varying model

80.3
50.5
62.8

53.1
58.9
67.5
61.3

52.4
58.6

100
99.3
100

99.3
98.6
100
100

100
100

color of the car, (2) left the position and orientation of the car relative to the
camera ﬁxed but varied the absolute position, eﬀectively changing the background
of the scene, and (3) used the mutation feature of Scenic to add a small amount
of noise to the car’s position, heading, and color. For each scenario we generated
150 images and evaluated Mgeneric on them. As seen in Tab. 7, changing the model
and color improved performance the most, suggesting they were most relevant to
the misclassiﬁcation, while local position and orientation were less important and
global position (i.e. the background) was least important.

To investigate these possibilities further, we wrote a second round of variant
scenarios, also shown in Tab. 7. The results conﬁrmed the importance of model
and color (compare (2) to (7)), as well as angle (compare (5) to (6)), but also
suggested that being close to the camera could be the relevant aspect of the car’s
local position. We conﬁrmed this with a ﬁnal round of scenarios (compare (5) and

Scenic: A Language for Scenario Speciﬁcation and Data Generation

41

Table 8: Performance of Mgeneric after retraining, replacing 10% of Xgeneric with
diﬀerent data.

Replacement Data

Precision Recall

Original (no replacement)
Classical augmentation

Close car
Close car at shallow angle

82.9
78.7

87.4
84.0

92.7
92.1

91.6
92.1

(8)), which also showed that the eﬀect of car model is small among scenes where
the car is close to the camera (compare (4) and (9)).

Having established that car model, closeness to the camera, and view angle
all contribute to poor performance of the network, we wrote broader scenarios
capturing these features. To avoid overﬁtting, and since our experiments indicated
car model was not very relevant when the car is close to the camera, we decided
not to ﬁx the car model. Instead, we specialized the generic one-car scenario from
our ﬁrst experiment to produce only cars close to the camera. We also created a
second scenario specializing this further by requiring that the car be viewed at a
shallow angle.

Finally, we used these scenarios to retrain Mgeneric, hoping to improve perfor-
mance on its original test set Tgeneric (to better distinguish small diﬀerences in
performance, we increased the test set size to 400 images). To keep the size of the
training set ﬁxed as in the previous experiment, we replaced 400 one-car images
in Xgeneric (10% of the whole training set) with images generated from our sce-
narios. As a baseline, we used images produced with classical image augmentation
techniques implemented in imgaug [31]. Speciﬁcally, we modiﬁed the original mis-
classiﬁed image by randomly cropping 10%–20% on each side, ﬂipping horizontally
with probability 50%, and applying Gaussian blur with σ ∈ [0.0, 3.0].

The results of retraining Mgeneric on the resulting data sets are shown in Tab. 8.
Interestingly, classical augmentation actually hurt performance, presumably due
to overﬁtting to relatively slight variants of a single image. On the other hand,
replacing part of the data set with specialized images of cars close to the camera
signiﬁcantly reduced the number of false positives like the original misclassiﬁcation
(while the improvement for the “shallow angle” scenario was less, perhaps due to
overﬁtting to the restricted angle range). This demonstrates how Scenic can be
used to improve performance by generalizing individual failures into scenarios that
capture the essence of the problem but are broad enough to prevent overﬁtting
during retraining.

7 Related Work

Data Generation and Testing for ML. There has been a large amount of work on
generating synthetic data for speciﬁc applications, including text recognition [28],
text localization [27], robotic object grasping [57], and autonomous driving [30, 11].
Closely related is work on domain adaptation, which attempts to correct diﬀerences
between synthetic and real-world input distributions. Domain adaptation has en-
abled synthetic data to successfully train models for several other applications in-

42

Daniel J. Fremont et al.

cluding 3D object detection [37, 54], pedestrian detection [58], and semantic image
segmentation [49]. Such work provides important context for our paper, showing
that models trained exclusively on synthetic data (possibly domain-adapted) can
achieve acceptable performance on real-world data. The major diﬀerence in our
work is that we provide, through Scenic, language-based systematic data genera-
tion for any cyber-physical system.

Some works have also explored the idea of using adversarial examples (i.e.
misclassiﬁed examples) to retrain and improve ML models (e.g., [62, 59, 23]). In
particular, Generative Adversarial Networks (GANs) [22], a particular kind of
neural network able to generate synthetic data, have been used to augment training
sets [36, 39]. The diﬀerence with Scenic is that GANs require an initial training
set/pretrained model and do not easily incorporate declarative constraints, while
Scenic produces synthetic data in an explainable, programmatic fashion requiring
only a simulator.

Model-Based Test Generation. Techniques using a model to guide test generation
have long existed [4]. A popular approach is to provide example tests, as in muta-
tional fuzz testing [55] and example-based scene synthesis [12]. While these meth-
ods are easy to use, they do not provide ﬁne-grained control over the generated
data. Another approach is to give rules or a grammar specifying how the data can
be generated, as in generative fuzz testing [55], procedural generation from shape
grammars [42], and grammar-based scene synthesis [29]. While grammars allow
much greater control, they do not easily allow enforcing global properties. This is
also true when writing a program in a domain-speciﬁc language with nondetermin-
ism [10]. Conversely, constraints as in constrained-random veriﬁcation [43] allow
global properties but can be diﬃcult to write. Scenic improves on these methods
by simultaneously providing ﬁne-grained control, enforcement of global properties,
speciﬁcation of probability distributions, and simple imperative syntax.

Probabilistic Programming Languages. The semantics (and to some extent, the syn-
tax) of Scenic are similar to that of other probabilistic programming languages
such as Prob [26], Church [24], and BLOG [41]. In probabilistic programming the
focus is usually on inference rather than generation (the main application in our
case), and in particular to our knowledge probabilistic programming languages
have not previously been used for test generation. However, the most popular
inference techniques are based on sampling and so could be directly applied to
generate scenes from Scenic programs, as we discussed in Sec. 5.

Several probabilistic programming languages have been used to deﬁne gen-
erative models of objects and scenes: both general-purpose languages such as
WebPPL [25] (see, e.g., [46]) and languages speciﬁcally motivated by such ap-
plications, namely Quicksand [45] and Picture [34]. The latter are in some sense
the most closely-related to Scenic, although neither provides specialized syntax or
semantics for dealing with geometry or dynamic behaviors (Picture also was used
only for inverse rendering, not data generation). The main advantage of Scenic
over these languages is that its domain-speciﬁc design permits concise representa-
tion of complex scenarios and enables specialized sampling techniques.

Scenario Description Languages for Autonomous Driving. Recently, formal dynamic
scenario description languages have been proposed for the domain of autonomous

Scenic: A Language for Scenario Speciﬁcation and Data Generation

43

driving. The Paracosm language [38] is used to model dynamic scenarios with a
reactive and synchronous model of computation. However, it is not a PPL, so it
lacks probability distributions and declarative constraints; it also does not pro-
vide constructs like Scenic’s interrupts which allow easy customization of generic
behavior models. The Measurable Scenario Description Language (M-SDL) [13],
introduced after the ﬁrst version of Scenic, does provide declarative constraints,
as well as compositional features similar to those we introduced in this paper.
However, compared to both of these languages, Scenic has several distinguishing
features: (i) it provides a much higher-level, declarative way of specifying geomet-
ric constraints; (ii) it is fundamentally a probabilistic programming language (as
opposed to M-SDL where distributions are optional), and (iii) it is not speciﬁc to
the autonomous driving domain (as demonstrated in [17, 15]).

8 Conclusion

In this paper, we introduced Scenic, a probabilistic programming language for
specifying distributions over conﬁgurations of physical objects and the behaviors
of dynamic agents. We showed how Scenic can be used to generate synthetic
data sets useful for a variety of tasks in the design of robust ML-based cyber-
physical systems. Speciﬁcally, we used Scenic to generate specialized test sets
and falsify a system, improve the robustness of a system by emphasizing diﬃcult
cases in its training set, and generalize from individual failure cases to broader
scenarios suitable for retraining. In particular, by training on hard cases generated
by Scenic, we were able to boost the performance of a car detector neural network
(given a ﬁxed training set size) signiﬁcantly beyond what could be achieved by
prior synthetic data generation methods [30] not based on PPLs.

In future work we plan to conduct experiments applying Scenic to a variety
of additional domains, applications, and simulators. As we mentioned in the In-
troduction, we have already successfully applied Scenic to aircraft [15], and we
are currently investigating applications in further domains including underwater
vehicles and indoor robots. We also plan to extend the Scenic language itself
in several directions, including allowing user-deﬁned speciﬁers and describing 3D
scenes. Finally, we are exploring ways to combine Scenic with automated analy-
ses: in particular, reducing the human burden of writing Scenic programs through
algorithms for synthesizing or adapting such programs (e.g. [32]), and improving
the eﬃciency of falsiﬁcation by performing white-box analyses of the system.

References

1. Amodei, D., Olah, C., Steinhardt, J., Christiano, P.F., Schulman, J., Man´e, D.: Concrete

problems in AI safety. CoRR abs/1606.06565 (2016)

2. Arlot, S., Celisse, A.: A survey of cross-validation procedures for model selection. Statist.
Surv. 4, 40–79 (2010). DOI 10.1214/09-SS054. URL https://doi.org/10.1214/09-SS054

3. Baidu: Apollo (2020). URL https://apollo.auto/
4. Broy, M., Jonsson, B., Katoen, J.P., Leucker, M., Pretschner, A.: Model-Based Testing
of Reactive Systems: Advanced Lectures (Lecture Notes in Computer Science). Springer-
Verlag New York, Inc., Secaucus, NJ, USA (2005)

5. Cartucho, J.: mean average precision. https://github.com/Cartucho/mAP (2019)

44

Daniel J. Fremont et al.

6. Claret, G., Rajamani, S.K., Nori, A.V., Gordon, A.D., Borgstr¨om, J.: Bayesian inference
using data ﬂow analysis. In: Proceedings of the 2013 9th Joint Meeting on Foundations
of Software Engineering, pp. 92–102. ACM (2013)

7. Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., Koltun, V.: CARLA: An open urban

driving simulator. In: Conference on Robot Learning, CoRL, pp. 1–16 (2017)

8. Dreossi, T., Fremont, D.J., Ghosh, S., Kim, E., Ravanbakhsh, H., Vazquez-Chanlatte, M.,
Seshia, S.A.: VerifAI: A toolkit for the formal design and analysis of artiﬁcial intelligence-
based systems. In: I. Dillig, S. Tasiran (eds.) Computer Aided Veriﬁcation - 31st Interna-
tional Conference, CAV 2019, New York City, NY, USA, July 15-18, 2019, Proceedings,
Part I, Lecture Notes in Computer Science, vol. 11561, pp. 432–442. Springer (2019).
DOI 10.1007/978-3-030-25540-4 25. URL https://github.com/BerkeleyLearnVerify/
VerifAI

9. DuPont: Global

color popularity
//web.archive.org/web/20130818022236/http://www2.dupont.com/Media_Center/
en_US/color_popularity/Images_2012/DuPont2012ColorPopularity.pdf

automotive

(2012).

report

URL https:

10. Elmas, T., Burnim, J., Necula, G., Sen, K.: CONCURRIT: A domain speciﬁc language for
reproducing concurrency bugs. In: Proceedings of the 34th ACM SIGPLAN Conference on
Programming Language Design and Implementation, PLDI ’13, pp. 153–164. Association
for Computing Machinery, New York, NY, USA (2013). DOI 10.1145/2491956.2462162.
URL https://doi.org/10.1145/2491956.2462162

11. Filipowicz, A., Liu, J., Kornhauser, A.: Learning to recognize distance to stop signs using

the virtual world of grand theft auto 5. Tech. rep., Princeton University (2017)

12. Fisher, M., Ritchie, D., Savva, M., Funkhouser, T., Hanrahan, P.: Example-based synthesis
of 3d object arrangements. In: ACM SIGGRAPH 2012, SIGGRAPH Asia ’12 (2012)

13. Foretellix: Measurable scenario description language.

https://www.foretellix.com/

wp-content/uploads/2020/07/M-SDL_LRM_OS.pdf (2020)

14. Fremont, D., Yue, X., Dreossi, T., Ghosh, S., Sangiovanni-Vincentelli, A.L., Seshia, S.A.:
Scenic: Language-based scene generation. Tech. Rep. UCB/EECS-2018-8, EECS Depart-
ment, University of California, Berkeley (2018). URL http://www2.eecs.berkeley.edu/
Pubs/TechRpts/2018/EECS-2018-8.html

15. Fremont, D.J., Chiu, J., Margineantu, D.D., Osipychev, D., Seshia, S.A.: Formal analysis
In: 32nd

and redesign of a neural network-based aircraft taxiing system with VerifAI.
International Conference on Computer Aided Veriﬁcation (CAV) (2020)

16. Fremont, D.J., Donz´e, A., Seshia, S.A., Wessel, D.: Control improvisation. In: 35th IARCS
Annual Conference on Foundation of Software Technology and Theoretical Computer Sci-
ence (FSTTCS), LIPIcs, vol. 45, pp. 463–474 (2015)

17. Fremont, D.J., Dreossi, T., Ghosh, S., Yue, X., Sangiovanni-Vincentelli, A.L., Seshia, S.A.:
In: K.S. McKinley,
Scenic: a language for scenario speciﬁcation and scene generation.
K. Fisher (eds.) Proceedings of the 40th ACM SIGPLAN Conference on Programming
Language Design and Implementation (PLDI), pp. 63–78. ACM (2019). DOI 10.1145/
3314221.3314633

18. Fremont, D.J., Kim, E., Dreossi, T., Ghosh, S., Yue, X., Sangiovanni-Vincentelli, A.L.,
Seshia, S.A.: Scenic: A language for scenario speciﬁcation and data generation (2020).
URL https://arxiv.org/abs/1809.09310

19. Fremont, D.J., Kim, E., Pant, Y.V., Seshia, S.A., Acharya, A., Bruso, X., Wells, P., Lemke,
S., Lu, Q., Mehta, S.: Formal scenario-based testing of autonomous vehicles: From sim-
ulation to the real world. In: 2020 IEEE Intelligent Transportation Systems Conference,
ITSC 2020, pp. 913–920. IEEE (2020). URL https://arxiv.org/abs/2003.07739

20. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the kitti vision
benchmark suite. In: Computer Vision and Pattern Recognition, CVPR, pp. 3354–3361
(2012). DOI 10.1109/CVPR.2012.6248074

21. Goldberg, A., Robson, D.: Smalltalk-80: The Language and its Implementation. Addison-

Wesley, Reading, Massachusetts (1983)

22. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,
Courville, A., Bengio, Y.: Generative adversarial nets. In: Advances in neural information
processing systems, pp. 2672–2680 (2014)

23. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial examples.

CoRR abs/1412.6572 (2014)

24. Goodman, N., Mansinghka, V.K., Roy, D., Bonawitz, K., Tenenbaum, J.B.: Church: A
In: Uncertainty in Artiﬁcial Intelligence 24

universal language for generative models.
(UAI), pp. 220–229 (2008)

Scenic: A Language for Scenario Speciﬁcation and Data Generation

45

25. Goodman, N.D., Stuhlm¨uller, A.: The Design and Implementation of Probabilistic Pro-

gramming Languages. http://dippl.org (2014). Accessed: 2018-7-11

26. Gordon, A.D., Henzinger, T.A., Nori, A.V., Rajamani, S.K.: Probabilistic programming.

In: FOSE 2014, pp. 167–181. ACM (2014)

27. Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in natural
In: Computer Vision and Pattern Recognition, CVPR, pp. 2315–2324 (2016).

images.
DOI 10.1109/CVPR.2016.254. URL https://doi.org/10.1109/CVPR.2016.254

28. Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Synthetic data and artiﬁcial

neural networks for natural scene text recognition. CoRR abs/1406.2227 (2014)

29. Jiang, C., Qi, S., Zhu, Y., Huang, S., Lin, J., Yu, L.F., Terzopoulos, D., Zhu, S.C.: Con-
ﬁgurable 3d scene synthesis and 2d image rendering with per-pixel ground truth using
stochastic grammars. International Journal of Computer Vision pp. 1–22 (2018)

30. Johnson-Roberson, M., Barto, C., Mehta, R., Sridhar, S.N., Rosaen, K., Vasudevan, R.:
Driving in the matrix: Can virtual worlds replace human-generated annotations for real
world tasks? In: International Conference on Robotics and Automation, ICRA, pp. 746–
753 (2017). DOI 10.1109/ICRA.2017.7989092. URL https://doi.org/10.1109/ICRA.
2017.7989092

31. Jung, A.: imgaug (2018). URL https://github.com/aleju/imgaug
32. Kim, E., Gopinath, D., Pasareanu, C.S., Seshia, S.A.: A programmatic and semantic ap-
proach to explaining and debugging neural network based object detectors.
In: 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, pp.
11125–11134. IEEE (2020). DOI 10.1109/CVPR42600.2020.01114

33. Koymans, R.: Specifying real-time properties with metric temporal logic. Real-time sys-

tems 2(4), 255–299 (1990)

34. Kulkarni, T., Kohli, P., Tenenbaum, J.B., Mansinghka, V.K.: Picture: A probabilistic
programming language for scene perception. In: IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 4390–4399 (2015)

35. Laminar Research: X-plane 11 (2019). URL https://www.x-plane.com/
36. Liang, X., Hu, Z., Zhang, H., Gan, C., Xing, E.P.: Recurrent topic-transition gan for visual

paragraph generation. arXiv preprint arXiv:1703.07022 (2017)

37. Liebelt, J., Schmid, C.: Multi-view object class detection with a 3d geometric model. In:
Computer Vision and Pattern Recognition, CVPR, pp. 1688–1695 (2010). DOI 10.1109/
CVPR.2010.5539836. URL https://doi.org/10.1109/CVPR.2010.5539836

38. Majumdar, R., Mathur, A., Pirron, M., Stegner, L., Zuﬀerey, D.: Paracosm: A language

and tool for testing autonomous driving systems (2019)

39. Marchesi, M.: Megapixel size image creation using generative adversarial networks. arXiv

preprint arXiv:1706.00082 (2017)

40. Michel, O.: Webots: Professional mobile robot simulation. International Journal of Ad-

vanced Robotic Systems 1(1), 39–42 (2004)

41. Milch, B., Marthi, B., Russell, S.: Blog: Relational modeling with unknown objects. In:
ICML 2004 workshop on statistical relational learning and its connections to other ﬁelds,
pp. 67–73 (2004)

42. M¨uller, P., Wonka, P., Haegler, S., Ulmer, A., Gool, L.V.: Procedural modeling of buildings.

ACM Trans. Graph. 25(3), 614–623 (2006). DOI 10.1145/1141911.1141931

43. Naveh, Y., Rimon, M., Jaeger, I., Katz, Y., Vinov, M., Marcus, E., Shurek, G.: Constraint-
based random stimuli generation for hardware veriﬁcation. In: Proc. of AAAI, pp. 1720–
1727 (2006)

44. Nori, A.V., Hur, C.K., Rajamani, S.K., Samuel, S.: R2: An eﬃcient mcmc sampler for

probabilistic programs. In: AAAI, pp. 2476–2482 (2014)

45. Ritchie, D.: Quicksand: A lightweight embedding of probabilistic programming for proce-
dural modeling and design. In: 3rd NIPS Workshop on Probabilistic Programming (2014).
URL https://dritchie.github.io/pdf/qs.pdf

46. Ritchie, D.: Probabilistic programming for procedural modeling and design. Ph.D. thesis,

Stanford University (2016). URL https://purl.stanford.edu/vh730bw6700

47. Rockstar Games: Grand theft auto v. Windows PC version (2015). URL https://www.

rockstargames.com/games/info/V

48. Rong, G., Shin, B.H., Tabatabaee, H., Lu, Q., Lemke, S., Moˇzeiko, M., Boise, E., Uhm,
G., Gerow, M., Mehta, S., Agafonov, E., Kim, T.H., Sterner, E., Ushiroda, K., Reyes, M.,
Zelenkovsky, D., Kim, S.: LGSVL Simulator: A high ﬁdelity simulator for autonomous
driving (2020). URL https://arxiv.org/abs/2005.03778

46

Daniel J. Fremont et al.

49. Ros, G., Sellart, L., Materzynska, J., V´azquez, D., L´opez, A.M.: The SYNTHIA dataset:
A large collection of synthetic images for semantic segmentation of urban scenes.
In:
Computer Vision and Pattern Recognition, CVPR, pp. 3234–3243 (2016). DOI 10.1109/
CVPR.2016.352. URL https://doi.org/10.1109/CVPR.2016.352

50. Rubinstein, R.Y., Kroese, D.P.: The Cross-Entropy Method: A Uniﬁed Approach to Com-
binatorial Optimization, Monte-Carlo Simulation, and Machine Learning. Springer, New
York, NY (2004). DOI 10.1007/978-1-4757-4321-0

51. Russell, S., Dietterich, T., Horvitz, E., Selman, B., Rossi, F., Hassabis, D., Legg, S.,
Suleyman, M., George, D., Phoenix, S.: Letter to the editor: Research priorities for robust
and beneﬁcial artiﬁcial intelligence: An open letter. AI Magazine 36(4) (2015)

52. Saheb-Djahromi, N.: Probabilistic LCF. In: Mathematical Foundations of Computer Sci-

ence, pp. 442–451. Springer (1978)

53. Seshia, S.A., Sadigh, D., Sastry, S.S.: Towards veriﬁed artiﬁcial intelligence (2016)
54. Stark, M., Goesele, M., Schiele, B.: Back to the future: Learning shape models from 3d
CAD data. In: British Machine Vision Conference, BMVC, pp. 1–11 (2010). DOI 10.5244/
C.24.106. URL https://doi.org/10.5244/C.24.106

55. Sutton, M., Greene, A., Amini, P.: Fuzzing: Brute Force Vulnerability Discovery. Addison-

Wesley (2007)

56. Thorn, E., Kimmel, S., Chaka, M.: A framework for automated driving system testable
cases and scenarios. Tech. Rep. DOT HS 812 623, National Highway Traﬃc Safety Admin-
istration (2018). URL https://www.nhtsa.gov/sites/nhtsa.dot.gov/files/documents/
13882-automateddrivingsystems_092618_v1a_tag.pdf

57. Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., Abbeel, P.: Domain randomiza-
tion for transferring deep neural networks from simulation to the real world. In: Inter-
national Conference on Intelligent Robots and Systems, IROS, pp. 23–30 (2017). DOI
10.1109/IROS.2017.8202133. URL https://doi.org/10.1109/IROS.2017.8202133

58. Vazquez, D., Lopez, A.M., Marin, J., Ponsa, D., Geronimo, D.: Virtual and real world
adaptation for pedestrian detection. IEEE transactions on pattern analysis and machine
intelligence 36(4), 797–809 (2014)

59. Wong, S.C., Gatt, A., Stamatescu, V., McDonnell, M.D.: Understanding data augmen-
tation for classiﬁcation: when to warp? In: Digital Image Computing: Techniques and
Applications (DICTA), 2016 International Conference on, pp. 1–6. IEEE (2016)

60. Wood, F., Meent, J.W., Mansinghka, V.: A new approach to probabilistic programming

inference. In: Artiﬁcial Intelligence and Statistics, pp. 1024–1032 (2014)

61. Wu, B., Iandola, F.N., Jin, P.H., Keutzer, K.: Squeezedet: Uniﬁed, small, low power fully
convolutional neural networks for real-time object detection for autonomous driving. In:
Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops,
pp. 446–454 (2017). DOI 10.1109/CVPRW.2017.60. URL https://doi.org/10.1109/
CVPRW.2017.60

62. Xu, Y., Jia, R., Mou, L., Li, G., Chen, Y., Lu, Y., Jin, Z.: Improved relation classiﬁcation by
deep recurrent neural networks with data augmentation. arXiv preprint arXiv:1601.03651
(2016)

Scenic: A Language for Scenario Speciﬁcation and Data Generation

47

A Gallery of Scenarios

This section presents Scenic code for a variety of scenarios from our autonomous car case
study (and the robot motion planning example used in Sec. 3), along with images rendered
from them. The scenarios range from simple examples used above to illustrate diﬀerent aspects
of the language, to those representing interesting road conﬁgurations like platoons and lanes
of traﬃc.

Contents

A.1 The scenic.simulators.gta.model Module . . . . . . . . . . . . . . . . . . . .
A.2 The Simplest Possible Scenario . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.3 A Single Car
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.4 A Badly-Parked Car . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.5 An Oncoming Car
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.6 Adding Noise to a Scene . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.7 Two Cars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.8 Two Overlapping Cars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.9 Four Cars, in Poor Driving Conditions . . . . . . . . . . . . . . . . . . . . . . .
A.10 A Platoon, in Daytime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.11 Bumper-to-Bumper Traﬃc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.12 Robot Motion Planning with a Bottleneck . . . . . . . . . . . . . . . . . . . . .

48
49
50
51
52
53
55
56
57
58
59
61

48

Daniel J. Fremont et al.

A.1 The scenic.simulators.gta.model Module

All the scenarios below begin with a line (not shown here) importing the Scenic world model
for the GTA simulator, scenic.simulators.gta.model, which as explained above contains all
deﬁnitions speciﬁc to our autonomous car case study. These include the deﬁnitions of the
regions road and curb, as well as the vector ﬁeld roadDirection giving the prevailing traﬃc
direction at each point on the road. Most importantly, it also deﬁnes Car as a type of object:

1
2
3
4
5
6
7
8
9
10
11

class Car:

position: Point on road
heading: (roadDirection at self.position) \

+ self.roadDeviation

roadDeviation: 0
width: self.model.width
height: self.model.height
viewAngle: 80 deg
visibleDistance: 30
model: CarModel.defaultModel()
color: CarColor.defaultColor()

Most of the properties are inherited from Object or are self-explanatory. The property
roadDeviation, representing the heading of the car with respect to the local direction of the
road, is purely a syntactic convenience; the following two lines are equivalent:

1
2

Car facing 10 deg relative to roadDirection
Car with roadDeviation 10 deg

The world model also deﬁnes a few convenience subclasses of Car with diﬀerent default
properties. For example, EgoCar overrides model with the ﬁxed car model we used for the ego
car in our interface to GTA V.

Scenic: A Language for Scenario Speciﬁcation and Data Generation

49

A.2 The Simplest Possible Scenario

This scenario, creating a single car with no speciﬁed properties, was used as an example in
Sec. 3.

1
2

ego = Car
Car

Fig. 15: Scenes generated from a Scenic scenario representing a single car (with
reasonable default properties).

50

A.3 A Single Car

Daniel J. Fremont et al.

This scenario is slightly more general than the previous, allowing the car (and the ego car)
to deviate from the road direction by up to 10◦. It also speciﬁes that the car must be visible,
which is in fact redundant since this constraint is built into Scenic, but helps guide the sam-
pling procedure. This scenario was also used as an example in Sec. 3.

1
2
3

wiggle = (-10 deg, 10 deg)
ego = EgoCar with roadDeviation wiggle
Car visible, with roadDeviation resample(wiggle)

Fig. 16: Scenes generated from a Scenic scenario representing a single car facing
roughly the road direction.

Scenic: A Language for Scenario Speciﬁcation and Data Generation

51

A.4 A Badly-Parked Car

This scenario, creating a single car parked near the curb but not quite parallel to it, was used
as an example in Sec. 3.

1
2
3
4

ego = Car
spot = OrientedPoint on visible curb
badAngle = Uniform(1.0, -1.0) * (10, 20) deg
Car left of spot by 0.5, facing badAngle relative to roadDirection

Fig. 17: Scenes generated from a Scenic scenario representing a badly-parked car.

52

Daniel J. Fremont et al.

A.5 An Oncoming Car

This scenario, creating a car 20–40 m ahead and roughly facing towards the camera, was used
as an example in Sec. 3. Note that since we do not specify the orientation of the car when
creating it, the default heading is used and so it will face the road direction. The require
statement then requires that this orientation is also within 15◦ of facing the camera (as the
view cone is 30◦ wide).

1
2
3

ego = Car
car2 = Car offset by (-10, 10) @ (20, 40), with viewAngle 30 deg
require car2 can see ego

Fig. 18: Scenes generated from a Scenic scenario representing a car facing roughly
towards the camera.

Scenic: A Language for Scenario Speciﬁcation and Data Generation

53

A.6 Adding Noise to a Scene

This scenario, using Scenic’s mutation feature to automatically add noise to an otherwise
completely-speciﬁed scenario, was used in the experiment in Sec. 6.4 (it is Scenario (3) in Ta-
ble 7). The original scene, which is exactly reproduced by this scenario if the mutate statement
is removed, is shown in Fig. 20.

1
2
3
4
5
6
7
8
9
10
11

param time = 12 * 60
param weather = ’EXTRASUNNY’

# noon

ego = EgoCar at -628.7878 @ -540.6067,

facing -359.1691 deg

Car at -625.4444 @ -530.7654, facing 8.2872 deg,
with model CarModel.models[’DOMINATOR’],
with color CarColor.byteToReal([187, 162, 157])

mutate

Fig. 19: Scenes generated from a Scenic scenario adding noise to the scene in
Fig. 20.

54

Daniel J. Fremont et al.

Fig. 20: The original misclassiﬁed image in Sec. 6.4.

Scenic: A Language for Scenario Speciﬁcation and Data Generation

55

A.7 Two Cars

This is the generic two-car scenario used in the experiments in Secs. 6.2 and 6.3.

1
2
3
4

wiggle = (-10 deg, 10 deg)
ego = EgoCar with roadDeviation wiggle
Car visible, with roadDeviation resample(wiggle)
Car visible, with roadDeviation resample(wiggle)

Fig. 21: Scenes generated from a Scenic scenario representing two cars, facing
close to the direction of the road.

56

Daniel J. Fremont et al.

A.8 Two Overlapping Cars

This is the scenario used to produce images of two partially-overlapping cars for the experi-
ment in Sec. 6.3.

1
2
3
4
5
6
7

wiggle = (-10 deg, 10 deg)
ego = EgoCar with roadDeviation wiggle

c = Car visible, with roadDeviation resample(wiggle)

leftRight = Uniform(1.0, -1.0) * (1.25, 2.75)
Car beyond c by leftRight @ (4, 10), with roadDeviation resample(wiggle)

Fig. 22: Scenes generated from a Scenic scenario representing two cars, one par-
tially occluding the other.

Scenic: A Language for Scenario Speciﬁcation and Data Generation

57

A.9 Four Cars, in Poor Driving Conditions

This is the scenario used to produce images of four cars in poor driving conditions for the
experiment in Sec. 6.2. Without the ﬁrst two lines, it is the generic four-car scenario used in
that experiment.

1
2
3
4
5
6
7
8
9

param weather = ’RAIN’
param time = 0 * 60

# midnight

wiggle = (-10 deg, 10 deg)
ego = EgoCar with roadDeviation wiggle
Car visible, with roadDeviation resample(wiggle)
Car visible, with roadDeviation resample(wiggle)
Car visible, with roadDeviation resample(wiggle)
Car visible, with roadDeviation resample(wiggle)

Fig. 23: Scenes generated from a Scenic scenario representing four cars in poor
driving conditions.

58

Daniel J. Fremont et al.

A.10 A Platoon, in Daytime

This scenario illustrates how Scenic can construct structured object conﬁgurations, in this
case a platoon of cars. It uses a helper function provided by gtaLib for creating platoons start-
ing from a given car, shown in Fig. 24. If no argument model is provided, as in this case, all cars
in the platoon have the same model as the starting car; otherwise, the given model distribution
is sampled independently for each car. The syntax for functions and loops supported by our
Scenic implementation is inherited from Python.

1
2
3
4

1
2
3
4
5
6
7

param time = (8, 20) * 60
ego = Car with visibleDistance 60
c2 = Car visible
platoon = createPlatoonAt(c2, 5, dist=(2, 8))

# 8 am to 8 pm

def createPlatoonAt(car, numCars, model=None, dist=(2, 8), shift=(-0.5, 0.5), wiggle=0):

lastCar = car
for i in range(numCars-1):

center = follow roadDirection from (front of lastCar) for resample(dist)
pos = OrientedPoint right of center by shift,

facing resample(wiggle) relative to roadDirection

lastCar = Car ahead of pos, with model (car.model if model is None else resample(model))

Fig. 24: Helper function for creating a platoon starting from a given car.

Fig. 25: Scenes generated from a Scenic scenario representing a platoon of cars
during daytime.

Scenic: A Language for Scenario Speciﬁcation and Data Generation

59

A.11 Bumper-to-Bumper Traﬃc

This scenario creates an even more complex type of object structure, namely three lanes of
traﬃc. It uses the helper function createPlatoonAt discussed above, plus another for placing
a car ahead of a given car with a speciﬁed gap in between, shown in Fig. 26.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

1
2
3
4

depth = 4
laneGap = 3.5
carGap = (1, 3)
laneShift = (-2, 2)
wiggle = (-5 deg, 5 deg)
modelDist = CarModel.defaultModel()

def createLaneAt(car):

createPlatoonAt(car, depth, dist=carGap, wiggle=wiggle, model=modelDist)

ego = Car with visibleDistance 60
leftCar = carAheadOfCar(ego, laneShift + carGap, offsetX=-laneGap, wiggle=wiggle)
createLaneAt(leftCar)

midCar = carAheadOfCar(ego, resample(carGap), wiggle=wiggle)
createLaneAt(midCar)

rightCar = carAheadOfCar(ego, resample(laneShift) + resample(carGap), offsetX=laneGap, wiggle=wiggle)
createLaneAt(rightCar)

def carAheadOfCar(car, gap, offsetX=0, wiggle=0):

pos = OrientedPoint at (front of car) offset by (offsetX @ gap),

facing resample(wiggle) relative to roadDirection

return Car ahead of pos

Fig. 26: Helper function for placing a car ahead of a car, with a speciﬁed gap in
between.

60

Daniel J. Fremont et al.

Fig. 27: Scenes generated from a Scenic scenario representing bumper-to-bumper
traﬃc.

Scenic: A Language for Scenario Speciﬁcation and Data Generation

61

A.12 Robot Motion Planning with a Bottleneck

This scenario illustrates the use of Scenic in another domain (motion planning) and with
another simulator (Webots [40]). Figure 28 encodes a scenario representing a rubble ﬁeld of
rocks and pipes with a bottleneck between a robot and its goal that forces the path planner
to consider climbing over a rock. The code is broken into four parts: ﬁrst, we import a small
library deﬁning the workspace and the types of objects, then create the robot at a ﬁxed
position and the goal (represented by a ﬂag) at a random position on the other side of the
workspace. Second, we pick a position for the bottleneck, requiring it to lie roughly on the
way from the robot to its goal, and place a rock there. Third, we position two pipes of varying
lengths which the robot cannot climb over on either side of the bottleneck, with their ends far
enough apart for the robot to be able to pass between. Finally, to make the scenario slightly
more interesting we add several additional obstacles, positioned either on the far side of the
bottleneck or anywhere at random. Several resulting workspaces are shown in Fig. 29.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

import mars
ego = Rover at 0 @ -2
goal = Goal at (-2, 2) @ (2, 2.5)

halfGapWidth = (1.2 * ego.width) / 2
bottleneck = OrientedPoint offset by (-1.5, 1.5) @ (0.5, 1.5), facing (-30, 30) deg
require abs((angle to goal) - (angle to bottleneck)) <= 10 deg
BigRock at bottleneck

leftEnd = OrientedPoint left of bottleneck by halfGapWidth,

rightEnd = OrientedPoint right of bottleneck by halfGapWidth,

facing (60, 120) deg relative to bottleneck

facing (-120, -60) deg relative to bottleneck

Pipe ahead of leftEnd, with height (1, 2)
Pipe ahead of rightEnd, with height (1, 2)

BigRock beyond bottleneck by (-0.5, 0.5) @ (0.5, 1)
BigRock beyond bottleneck by (-0.5, 0.5) @ (0.5, 1)
Pipe
Rock
Rock
Rock

Fig. 28: A Scenic representing rubble ﬁelds with a bottleneck so that the direct
route to the goal requires climbing over rocks.

62

Daniel J. Fremont et al.

Fig. 29: Workspaces generated from the scenario in Fig. 28, viewed in Webots from
a ﬁxed camera.

Scenic: A Language for Scenario Speciﬁcation and Data Generation

63

B Semantics of Scenic

In this section we give a precise semantics for Scenic expressions and statements, building up
to a semantics for a complete program as a distribution over scenes.

Contents

B.1 Notation for State and Semantics . . . . . . . . . . . . . . . . . . . . . . . . . .
B.2 Semantics of Expressions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.3 Semantics of Statements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.4 Semantics of a Scenic Program . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.5 Sampling Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

63
63
64
66
67

B.1 Notation for State and Semantics

We will precisely deﬁne the meaning of Scenic language constructs by giving a small-step
operational semantics. We will focus on the aspects of Scenic that set it apart from ordinary
imperative languages, skipping standard inference rules for sequential composition, arithmetic
operations, etc. that we essentially use without change. In rules for statements, we will denote
a state of a Scenic program by (cid:104)s, σ, π, O(cid:105), where s is the statement to be executed, σ is the
current variable assignment (a map from variables to values), π is the current global parameter
assignment (for param statements), and O is the set of all objects deﬁned so far. In rules for
expressions, we use the same notation, although we sometimes suppress the state on the right-
hand side of rules for expressions without side eﬀects: (cid:104)e, σ, π, O(cid:105) → v means that in the state
(σ, π, O), the expression e evaluates to the value v without side eﬀects.

Since Scenic is a probabilistic programming language, a single expression can be evaluated
diﬀerent ways with diﬀerent probabilities. Following the notation of [52, 6], we write →p for
a rewrite rule that ﬁres with probability p (probability density p, in the case of continuous
distributions). We will discuss the meaning of such rules in more detail below.

B.2 Semantics of Expressions

As explained in the previous section, Scenic’s expressions are straightforward except for distri-
butions and object deﬁnitions. As in a typical imperative probabilistic programming language,
a distribution evaluates to a sample from the distribution, following the ﬁrst rule in Fig. 30.
For example, if baseDist is a uniform interval distribution and the parameters evaluate to
low = 0 and high = 1, then the distribution can evaluate to any value in [0, 1] with probability
density 1.

The semantics of object deﬁnitions are given by the second rule in Fig. 30. First note the
side eﬀect, namely adding the newly-deﬁned object to the set O. The premises of the rule
describe the procedure for combining the speciﬁers to obtain the overall set of properties for
the object. The main step is working out the evaluation order for the speciﬁers so that all their
dependencies are satisﬁed, as well as deciding for each speciﬁer which properties it should
specify (if it speciﬁes a property optionally, another speciﬁer could take precedence). This is
done by the procedure resolveSpeciﬁers, shown formally as Alg. 1 and which essentially does
the following:

Let P be the set of properties deﬁned in the object’s class and superclasses, together with
any properties speciﬁed by any of the speciﬁers. The object will have exactly these proper-
ties, and the value of each p ∈ P is determined as follows. If p is speciﬁed non-optionally by
multiple speciﬁers the scenario is ill-formed. If p is only speciﬁed optionally, and by multi-
ple speciﬁers, this is ambiguous and we also declare the scenario ill-formed. Otherwise, the
value of p will be determined by its unique non-optional speciﬁer, unique optional speci-
ﬁer, or the most-derived default value, in that order: call this speciﬁer sp. Construct a di-
rected graph with vertices P and edges to p from each of the dependencies of sp (if a de-
pendency is not in P , then a speciﬁer references a nonexistent property and the scenario

64

Daniel J. Fremont et al.

is ill-formed). If this graph has a cycle, there are cyclic dependencies and the scenario is
ill-formed (e.g. Car left of 0 @ 0, facing roadDirection: the heading must be known to
evaluate left of vector , but facing vectorField needs position to determine heading). Oth-
erwise, topologically sorting the graph yields an evaluation order for the speciﬁers so that all
dependencies are available when needed.

The rest of the rule in Fig. 30 simply evaluates the speciﬁers in this order, accumulating the
results as properties of self so they are available to the next speciﬁer, ﬁnally creating the new
object once all properties have been assigned. Note that we also accumulate the probabilities
of each speciﬁer’s evaluation, since speciﬁers are allowed to introduce randomness themselves
(e.g. the on region speciﬁer returns a random point in the region).

As noted above the semantics of the individual speciﬁers are mostly straightforward, and
exact deﬁnitions are given in Appendix C. To illustrate the pattern we precisely deﬁne two
speciﬁers in Fig. 30: the with property value speciﬁer, which has no dependencies but can
specify any property, and the facing vectorField speciﬁer, which depends on position and
speciﬁes heading. Both speciﬁers evaluate to maps assigning a value to each property they
specify.

B.3 Semantics of Statements

The semantics of class and object deﬁnitions have been discussed above, while rules for the
other statements are given in Fig. 31. As can be seen from the ﬁrst rule, variable assignment
behaves in the standard way. Parameter assignment is nearly identical, simply updating the
global parameter assignment π instead of the variable assignment σ.

As noted above, the require boolean statement is equivalent to an observe in other lan-
guages, and following [6] we model it by allowing the “Hard Requirement” rule in Fig. 31 to
only ﬁre when the condition is satisﬁed (then turning the requirement into a no-op). If the
condition is not satisﬁed, no rules apply and the program fails to terminate normally. When

Distributions
(cid:104)params, σ, π, O(cid:105) → θ

v ∈ dom baseDist(θ)

(cid:104)baseDist(params), σ, π, O(cid:105) →Pθ (v) v

Object Definitions

resolveSpeciﬁers(class, speciﬁers) = ((s1, p1), . . . , (sn, pn))
(cid:104)s1, σ[self/⊥], π, O(cid:105) →r1 (cid:104)v1, σ1, π, O1(cid:105)
(cid:104)s2, σ1[self.p1/v1(p1)], π, O1(cid:105) →r2 (cid:104)v2, σ2, π, O2(cid:105)
...
(cid:104)sn, σn−1[self.pn−1/vn−1(pn−1)], π, On−1(cid:105) →rn (cid:104)vn, σn, π, On(cid:105)
inst = newInstance(class, σn[self.pn/vn(pn)](self))
(cid:104)class speciﬁers, σ, π, O(cid:105) →r1...rn (cid:104)inst, σ, π, On ∪ {inst}(cid:105)

‘with’ specifier

(cid:104)E, σ, π, O(cid:105) → (cid:104)v, σ, π, O(cid:48)(cid:105)
(cid:104)with property E, σ, π, O(cid:105) → {property (cid:55)→ v}

‘facing vectorField’ specifier
(cid:104)vectorField, σ, π, O(cid:105) → v

(cid:104)self.position, σ, π, O(cid:105) → p

(cid:104)facing vectorField, σ, π, O(cid:105) → {heading (cid:55)→ v(p)}

Fig. 30: Semantics of expressions (excluding operators, deﬁned in Appendix C),
and two example speciﬁers. Here baseDist is viewed as a function mapping param-
eters θ to a distribution with density function Pθ, and newInstance(class, props)
creates a new instance of a class with the given property values.

Scenic: A Language for Scenario Speciﬁcation and Data Generation

65

Algorithm 1 resolveSpeciﬁers (class, speciﬁers)

(cid:46) gather all speciﬁed properties

1: specForProperty ← ∅
2: optionalSpecsForProperty ← ∅
3: for all speciﬁers S in speciﬁers do
4:
5:
6:
7:
8:
9:

specForProperty (P ) ← S

for all properties P speciﬁed non-optionally by S do

if P ∈ dom specForProperty then

syntax error: property P speciﬁed twice

for all properties P speciﬁed optionally by S do
optionalSpecsForProperty (P ).append(S)

(cid:46) ﬁlter optional speciﬁcations

if P ∈ dom specForProperty then

10: for all properties P ∈ dom optionalSpecsForProperty do
11:
12:
13:
14:
15:

if |optionalSpecsForProperty (P )| > 1 then
syntax error: property P speciﬁed twice

specForProperty (P ) ← optionalSpecsForProperty (P )[0]

continue

(cid:46) add default speciﬁers as needed

16: defaults ← defaultValueExpressions (class)
17: for all properties P ∈ dom defaults do
18:
19:

specForProperty (P ) ← defaults (P )

if P (cid:54)∈ dom specForProperty then

(cid:46) build dependency graph

for all dependencies D of S do

20: G ← empty graph on dom specForProperty
21: for all speciﬁers S ∈ dom specForProperty do
22:
23:
24:
25:
26: if G is cyclic then
27:

if D (cid:54)∈ dom specForProperty then

syntax error: speciﬁers have cyclic dependencies
(cid:46) construct speciﬁer and property evaluation order

add an edge in G from specForProperty (D) to S

syntax error: missing property D required by S

28: specsAndProps ← empty list
29: for all speciﬁers S in G in topological order do
30:
31: return specsAndProps

specsAndProps.append((S, {P | specForProperty (P ) = S}))

deﬁning the semantics of entire Scenic scenarios below we will discard such non-terminating
executions, yielding a distribution only over executions where all hard requirements are satis-
ﬁed.

The statement require[p] boolean requires only that its condition hold with at least
probability p. There are a number of ways the semantics of such a soft requirement could be
deﬁned: we choose the natural deﬁnition that require[p] B is equivalent to a hard requirement
require B that is only enforced with probability p. This is reﬂected in the two corresponding
rules in Fig. 31, and clearly ensures that the requirement B will hold with probability at least
p, as desired.

Since the mutation statement mutate instance, . . . by number only causes noise to be
added at the end of execution, as discussed above, its rule Fig. 31 simply sets a property on the
object(s) indicating that mutation is enabled (and giving the scale of noise to be added). The
noise is actually added by the ﬁrst of two special rules that apply only once the program has
been reduced to pass and so computation has ﬁnished. This rule ﬁrst looks up the values of the
properties mutationScale, positionStdDev, and headingStdDev for each object. Respectively,
these specify the overall scale of the noise to add (by default zero, i.e. mutation is disabled) and
factors allowing the standard deviation for position and heading to be adjusted individually.

66

Daniel J. Fremont et al.

Variable/Parameter Assignments
(cid:104)E, σ, π, O(cid:105) → (cid:104)v, σ, π, O(cid:48)(cid:105)
(cid:104)x = E, σ, π, O(cid:105) → (cid:104)pass, σ[x/v], π, O(cid:48)(cid:105)
(cid:104)param x = E, σ, π, O(cid:105) → (cid:104)pass, σ, π[x/v], O(cid:48)(cid:105)

Hard Requirements

(cid:104)B, σ, π, O(cid:105) → (cid:104)True, σ, π, O(cid:48)(cid:105)
(cid:104)require B, σ, π, O(cid:105) → (cid:104)pass, σ, π, O(cid:48)(cid:105)

Soft Requirements
(cid:104)require[p] B, σ, π, O(cid:105) →p (cid:104)require B, σ, π, O(cid:105)
(cid:104)require[p] B, σ, π, O(cid:105) →1−p (cid:104)pass, σ, π, O(cid:105)

Mutations
(cid:104)mutate obj i by s, σ, π, O(cid:105) → (cid:104)pass, σ, π, O[σ(obj i).mutationScale/s](cid:105)

Termination, Step 1: Apply Mutations

O = {o1, . . . , on}

∀i ∈ {1, . . . , n} :

Si = O(oi.mutationScale)

psi = O(oi.positionStdDev)

hsi = O(oi.headingStdDev)

(cid:104)Normal(0, Si · psi), σ, π, O(cid:105) →ri,x (cid:104)ni,x, σ, π, O(cid:105)
(cid:104)Normal(0, Si · psi), σ, π, O(cid:105) →ri,y (cid:104)ni,y, σ, π, O(cid:105)
(cid:104)Normal(0, Si · hsi), σ, π, O(cid:105) →ri,h (cid:104)ni,h, σ, π, O(cid:105)

posi = O(oi.position) + (ni,x, ni,y)

headi = O(oi.heading) + ni,h

(cid:104)pass, σ, π, O(cid:105) →r0,xr0,y r0,h... (cid:104)Done, σ, π, O[oi.position/posi][oi.heading/headi](cid:105)

Termination, Step 2: Check Default Requirements
O = {o1, . . . , on}

∀i : boundingBox (oi) ⊆ workspace

∀i (cid:54)= j : oi.allowCollisions ∨ oj .allowCollisions ∨ boundingBox (oi) ∩ boundingBox (oj ) = ∅
∀i : ¬oi.requireVisible ∨ (cid:104)ego can see oi, σ, π, O(cid:105) → True
(cid:104)Done, σ, π, O(cid:105) → (π, O)

Fig. 31: Semantics of statements (excluding class deﬁnitions and standard rules
for sequential composition). Done denotes a special state ready for the ﬁnal ter-
mination rule to run.

The rule then independently samples Gaussian noise with the desired standard deviation for
each object and adds it to the position and heading properties.

Finally, after mutations are applied, the last rule in Fig. 31 checks Scenic’s three built-in
hard requirements. Similarly to the rule for hard requirements, this last rule can only ﬁre if all
the built-in requirements are satisﬁed, otherwise preventing the program from terminating. If
the rule does ﬁre, the ﬁnal result is the output of the scenario: the assignment π to the global
parameters, and the set O of all deﬁned objects.

B.4 Semantics of a Scenic Program

As we have just deﬁned it, every time one runs a Scenic program its output is a scene
consisting of an assignment to all the properties of each Object deﬁned in the scenario, plus
any global parameters deﬁned with param. Since Scenic allows sampling from distributions,
the imperative part of a scenario actually induces a distribution over scenes, resulting from
the probabilistic rules of the semantics described above. Speciﬁcally, for any execution trace
the product of the probabilities of all rewrite rules yields a probability (density) for the trace
(see e.g. [6]). The declarative part of a scenario, consisting of its require statements, modiﬁes
this distribution. As mentioned above, hard requirements are equivalent to “observations” in
other probabilistic programming languages, conditioning the distribution on the requirement
being satisﬁed. In particular, if we discard all traces which do not terminate (due to violating
a requirement), then normalizing the probabilities of the remaining traces yields a distribution

Scenic: A Language for Scenario Speciﬁcation and Data Generation

67

Algorithm 2 pruneByHeading (map, A, M, δ)

for all polygons Q in map do

1: map’ ← ∅
2: for all polygons P in map do
3:
4:
5:
6:
7: return map’

map’ ← map’ ∪ (Q(cid:48) ∩ P )

Q(cid:48) ← dilate(Q, M )
if P ∩ Q(cid:48) (cid:54)= ∅ ∧ relHead(P, Q) ± 2δ ∈ A then

Algorithm 3 pruneByWidth (map, M, minWidth)

1: narrowP olys ← narrow(map, minW idth)
2: map’ ← map \ narrowP olys
3: for all polygons P in narrowPolys do
4:
5:
6: return map’

U ← (cid:83)
map’ ← map’ ∪ (P ∩ U )

Q∈map\{P } dilate(Q, M )

over traces, and therefore scenes, that satisfy all our requirements. This is the distribution
deﬁned by the Scenic scenario.

B.5 Sampling Algorithms

This section gives pseudocode for the domain-speciﬁc sampling techniques described in Sec. 5.2.
Algorithm 2 implements pruning by orientation, pruning a set of polygons map given an allowed
range of relative headings A, a distance bound M , and a bound δ on the heading deviation
between an object and the vector ﬁeld at its position.

Algorithm 3 similarly implements pruning by size, given map and M as above, plus a
bound minWidth on the minimum width of the conﬁguration. Here the subroutine narrow
ﬁnds all polygons which are thinner than this bound.

68

Daniel J. Fremont et al.

C Detailed Semantics of Speciﬁers and Operators

This section provides precise semantics for Scenic’s speciﬁers and operators, which were in-
formally deﬁned above.

Contents

C.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
C.2 Speciﬁers for position . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
C.3 Speciﬁers for position and optionally heading . . . . . . . . . . . . . . . . . .
C.4 Speciﬁers for heading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
C.5 Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

68
68
70
70
72

C.1 Notation

Since none of the speciﬁers and operators have side eﬀects, to simplify notation we write
(cid:75)
for the value of the expression X in the current state (rather than giving inference rules).
Throughout this section, S indicates a scalar , V a vector , H a heading, F a vectorField, R
a region, P a Point, and OP an OrientedPoint. Figure 32 deﬁnes notation used in the rest
of the semantics. In forwardEuler, N is an implementation-deﬁned parameter specifying how
many steps should be used for the forward Euler approximation when following a vector ﬁeld
(we used N = 4).

X
(cid:74)

(cid:104)x, y(cid:105) = point with the given XY coordinates

rotate ((cid:104)x, y(cid:105) , θ) = (cid:104)x cos θ − y sin θ, x sin θ + y cos θ(cid:105)

oﬀsetLocal (OP, v) =

OP.position
(cid:75)
(cid:74)

+ rotate (v,

)
OP.heading
(cid:75)
(cid:74)

Disc (c, r) = set of points in the disc centered at c and with radius r

Sector (c, r, h, a) = set of points in the sector of Disc (c, r) centered along h and with angle a

boundingBox (O) = set of points in the bounding box of object O

visibleRegion (X) =






,
Sector (
X.position
X.viewDistance
(cid:75)
(cid:74)
(cid:74)
,
)
X.viewAngle
X.heading
(cid:74)
(cid:75)
(cid:74)
(cid:75)
,
Disc (
X.viewDistance
X.position
(cid:74)
(cid:75)
(cid:74)

)
(cid:75)

,
(cid:75)

orientation (R) = preferred orientation of R if any; otherwise ⊥

X ∈ OrientedPoint
X ∈ Point

uniformPointIn (R) = a uniformly random point in R

forwardEuler (x, d, F ) = result of iterating the map x (cid:55)→ x + rotate ((cid:104)0, d/N (cid:105) ,

F
(cid:74)

(x)) a total of N times on x
(cid:75)

Fig. 32: Notation used to deﬁne the semantics.

C.2 Speciﬁers for position

Figure 33 gives the semantics of the position speciﬁers. The ﬁgure writes the semantics as
a vector value; the semantics of the speciﬁer itself is to assign the position property of the
object being speciﬁed to that value. Several of the speciﬁers refer to properties of self: as
explained in Sec. 4, this refers to the object being constructed, and the semantics of object
construction are such that speciﬁers depending on other properties are only evaluated after
those properties have been speciﬁed (or an error is raised, if there are cyclic dependencies).

Scenic: A Language for Scenario Speciﬁcation and Data Generation

69

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

at V
(cid:74)
offset by V
(cid:74)
offset along H by V
(cid:74)
left of V
(cid:74)
right of V
(cid:74)
ahead of V
(cid:74)
behind V
(cid:74)
left of V by S
(cid:74)
right of V by S
(cid:74)
ahead of V by S
(cid:74)
behind V by S
(cid:74)
beyond V1 by V2
(cid:74)
beyond V1 by V2 from V3
(cid:75)
(cid:74)
visible
(cid:74)
(cid:75)
visible from P
(cid:74)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

=

=

=

=

=

=

=

=

=

=

=

=

=

=

(cid:75)

(cid:75)

(cid:75)

(cid:75)

V
(cid:74)
V relative to ego.position
(cid:74)
(cid:75)
ego.position offset along H by V
(cid:74)
left of V by 0
(cid:74)
(cid:75)
right of V by 0
(cid:74)
ahead of V by 0
(cid:74)
behind V by 0
(cid:74)
(cid:75)
/2 −
+ rotate ((cid:104)−
V
)
, 0(cid:105) ,
S
self.width
self.heading
(cid:75)
(cid:74)
(cid:74)
(cid:75)
(cid:74)
(cid:75)
(cid:74)
/2 +
+ rotate ((cid:104)
V
)
, 0(cid:105) ,
S
self.width
self.heading
(cid:74)
(cid:74)
(cid:75)
(cid:75)
(cid:74)
(cid:74)
(cid:75)
+ rotate ((cid:104)0,
V
)
(cid:105) ,
/2 +
S
self.heading
self.height
(cid:74)
(cid:75)
(cid:74)
(cid:75)
(cid:74)
(cid:75)
(cid:74)
)
(cid:105) ,
S
V
/2 −
+ rotate ((cid:104)0, −
self.heading
self.height
(cid:75)
(cid:74)
(cid:75)
(cid:74)
(cid:74)
(cid:74)
(cid:75)
beyond V1 by V2 from ego.position
(cid:75)
(cid:74)
))
, arctan (
V1
(cid:74)
(cid:75)
(cid:75)
visible from ego
(cid:75)
(cid:74)

V2
+ rotate (
(cid:74)

V3
(cid:74)

V1
(cid:74)

−

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

= uniformPointIn (visibleRegion (P ))

Fig. 33: Semantics of position speciﬁers, given as the value v such that the speciﬁer
evaluates to the map position (cid:55)→ v.

70

Daniel J. Fremont et al.

C.3 Speciﬁers for position and optionally heading

Figure 34 gives the semantics of the position speciﬁers that also optionally specify heading.
The ﬁgure writes the semantics as an OrientedPoint value; if this is OP , the semantics of the
speciﬁer is to assign the position property of the object being constructed to OP.position,
and the heading property of the object to OP.heading if heading is not otherwise speciﬁed
(see Sec. 4 for a discussion of optional speciﬁers).

(cid:40)

OrientedPoint (x,
OrientedPoint (x, ⊥)

(x)) orientation (R) (cid:54)= ⊥
orientation (R)
(cid:75)
(cid:74)

otherwise

, with x = uniformPointIn (

R
(cid:74)

)
(cid:75)

=

=

=

=

=

=

=

=

=

in R
(cid:74)

(cid:75)

=

on R
(cid:74)

(cid:75)

ahead of O
(cid:74)
behind O
(cid:74)
left of O
(cid:74)
right of O
(cid:74)
ahead of OP
(cid:74)
behind OP
(cid:74)
left of OP
(cid:74)
right of OP
(cid:74)
ahead of OP by S
(cid:74)
behind OP by S
(cid:74)
left of OP by S
(cid:74)
right of OP by S
(cid:74)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

ahead of (front of O)
(cid:74)
(cid:75)
behind (back of O)
(cid:74)
(cid:75)
left of (left of O)
(cid:74)
right of (right of O)
(cid:74)
(cid:75)
ahead of OP by 0
(cid:74)
behind OP by 0
(cid:74)
(cid:75)
left of OP by 0
(cid:74)
(cid:75)
right of OP by 0
(cid:74)

(cid:75)

(cid:75)

= OrientedPoint (oﬀsetLocal (OP, (cid:104)0,
/2 +
S
self.height
(cid:74)
(cid:75)
(cid:74)
/2 −
= OrientedPoint (oﬀsetLocal (OP, (cid:104)0, −
self.height
(cid:75)
(cid:74)
= OrientedPoint (oﬀsetLocal (OP, (cid:104)−
/2 −
self.width
(cid:75)
(cid:74)
/2 +
= OrientedPoint (oﬀsetLocal (OP, (cid:104)
self.width
(cid:75)
(cid:74)

(cid:105)),
)
OP.heading
(cid:74)
(cid:75)
(cid:75)
)
(cid:105)),
S
OP.heading
(cid:75)
(cid:74)
(cid:75)
(cid:74)
)
, 0(cid:105)),
OP.heading
(cid:75)
(cid:74)
(cid:75)
)
, 0(cid:105)),
OP.heading
(cid:75)
(cid:74)
(cid:75)

S
(cid:74)
S
(cid:74)

following F for S
(cid:74)
following F from V for S
(cid:74)

(cid:75)

(cid:75)

=

=

following F from ego.position for S
(cid:74)
follow F from V for S
(cid:74)

(cid:75)

(cid:75)

Fig. 34: Semantics of position speciﬁers that optionally specify heading. If o is
the OrientedPoint given as the semantics above, the speciﬁer evaluates to the map
{position (cid:55)→ o.position, heading (cid:55)→ o.heading}.

C.4 Speciﬁers for heading

Figure 35 gives the semantics of the heading speciﬁers. As for the position speciﬁers above,
the ﬁgure indicates the heading value assigned by each speciﬁer.

Scenic: A Language for Scenario Speciﬁcation and Data Generation

71

facing H
(cid:74)
facing F
(cid:74)
facing toward V
(cid:74)
facing away from V
(cid:74)
apparently facing H
(cid:74)
apparently facing H from V
(cid:74)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

=

H
(cid:74)
F
(cid:74)

=

(cid:75)
)
(
self.position
(cid:75)
(cid:75)
(cid:74)
= arctan (

−

= arctan (

(cid:75)

V
(cid:74)
self.position
(cid:74)

)
self.position
(cid:75)
(cid:74)
)
(cid:75)

V
(cid:74)

−

(cid:75)

=

=

apparently facing H from ego.position
(cid:74)
(cid:75)
H
(cid:74)

self.position
(cid:74)

+ arctan (

V
(cid:74)

)
(cid:75)

−

(cid:75)

(cid:75)

Fig. 35: Semantics of heading speciﬁers, given as the value v such that the speciﬁer
evaluates to the map heading (cid:55)→ v.

72

C.5 Operators

Daniel J. Fremont et al.

Finally, Figures 36–41 give the semantics for Scenic’s operators, broken down by the type of
value they return. We omit the semantics for ordinary numerical and Boolean operators (max,
+, or, >=, etc.), which are standard.

relative heading of H
(cid:74)
relative heading of H1 from H2
(cid:74)
apparent heading of OP
(cid:74)
apparent heading of OP from V
(cid:74)
distance to V
(cid:74)
distance from V1 to V2
(cid:74)
angle to V
(cid:74)
angle from V1 to V2
(cid:74)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

=

=

=

=

−

H2
(cid:74)

relative heading of H from ego.heading
(cid:75)
(cid:74)
H1
(cid:74)
apparent heading of OP from ego.position
(cid:74)
(cid:75)
OP.heading
OP.position
(cid:74)
(cid:75)
(cid:75)
(cid:74)
distance from ego.position to V
(cid:74)
V2
= |
(cid:74)
angle from ego.position to V
(cid:74)
V1
(cid:74)

= arctan (

− arctan (

V1
(cid:74)

V2
(cid:74)

))
(cid:75)

V
(cid:74)

)
(cid:75)

|
(cid:75)

−

−

−

=

=

(cid:75)

(cid:75)

(cid:75)

(cid:75)

Fig. 36: Scalar operators.

P can see O
(cid:74)
V is in R
(cid:74)
O is in R
(cid:74)

(cid:75)

(cid:75)

(cid:75)

P
= visibleRegion (
(cid:74)

=

∈

V
(cid:74)

(cid:75)

R
(cid:74)

(cid:75)

O
= boundingBox (
(cid:74)

O
) ∩ boundingBox (
(cid:74)
(cid:75)

) (cid:54)= ∅
(cid:75)

) ⊆
(cid:75)

R
(cid:74)

(cid:75)

Fig. 37: Boolean operators.

F at V
(cid:74)
F1 relative to F2
(cid:74)
H relative to F
(cid:74)
F relative to H
(cid:74)
H1 relative to H2
(cid:74)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

=

=

=

=

=

(
F
(cid:74)
(cid:75)
F1
(cid:74)
H
(cid:74)
H
(cid:74)
(cid:75)
H1
(cid:74)

V
)
(cid:75)
(cid:74)
) +
(
self.position
(cid:75)
(cid:74)
(cid:75)
+

F2
(cid:74)
F
)
(
self.position
(cid:74)
(cid:75)
(cid:74)
(cid:75)
F
)
(
self.position
(cid:74)
(cid:75)
(cid:75)
(cid:74)
H2
+
(cid:74)

+

(cid:75)

(cid:75)

(cid:75)

Fig. 38: Heading operators.

)
(
self.position
(cid:75)
(cid:74)
(cid:75)

Scenic: A Language for Scenario Speciﬁcation and Data Generation

73

V1 offset by V2
(cid:74)
V1 offset along H by V2
(cid:74)
V1 offset along F by V2
(cid:74)

(cid:75)

(cid:75)

(cid:75)

=

=

=

V1
(cid:74)
V1
(cid:74)
V1
(cid:74)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

+

V2
(cid:74)
V2
+ rotate (
(cid:74)
V2
+ rotate (
(cid:74)

,
(cid:75)
,
(cid:75)

H
(cid:74)
F
(cid:74)

)
(cid:75)
V1
(
(cid:74)
(cid:75)

))
(cid:75)

Fig. 39: Vector operators.

visible R
(cid:74)
R visible from P
(cid:74)

(cid:75)

(cid:75)

=

=

R visible from ego
(cid:75)
(cid:74)
P
∩ visibleRegion (
R
(cid:74)
(cid:74)

(cid:75)

)
(cid:75)

Fig. 40: Region operators.

OP offset by V
(cid:74)
V relative to OP
(cid:74)
follow F for S
(cid:74)
follow F from V for S
(cid:74)
front of O
(cid:74)
back of O
(cid:74)
left of O
(cid:74)
right of O
(cid:74)
front left of O
(cid:74)
back left of O
(cid:74)
front right of O
(cid:74)
back right of O
(cid:74)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

=

V relative to OP
(cid:74)

(cid:75)

= OrientedPoint (oﬀsetLocal (OP,
=

)
OP.heading
(cid:75)
(cid:74)
follow F from ego.position for S
(cid:74)
F
(cid:74)

V
(y)) where y = forwardEuler (
(cid:74)
(cid:75)

= OrientedPoint (y,
=

),
(cid:75)

V
(cid:74)

(cid:75)

(cid:75)

/2(cid:105) relative to O
(cid:104)0,
O.height
(cid:75)
(cid:74)
(cid:74)
(cid:75)
/2(cid:105) relative to O
(cid:104)0, −
O.height
(cid:75)
(cid:74)
(cid:74)
(cid:104)−
/2, 0(cid:105) relative to O
O.width
(cid:75)
(cid:74)
(cid:74)
/2, 0(cid:105) relative to O
(cid:104)
O.width
(cid:75)
(cid:74)
(cid:74)
/2(cid:105) relative to O
/2,
(cid:104)−
O.height
O.width
(cid:75)
(cid:75)
(cid:74)
(cid:74)
(cid:74)
(cid:75)
/2(cid:105) relative to O
/2, −
(cid:104)−
O.height
O.width
(cid:75)
(cid:74)
(cid:75)
(cid:74)
(cid:74)
/2(cid:105) relative to O
/2,
(cid:104)
O.height
O.width
(cid:75)
(cid:75)
(cid:74)
(cid:74)
(cid:74)
(cid:75)
/2(cid:105) relative to O
/2, −
(cid:104)
O.height
O.width
(cid:75)
(cid:74)
(cid:75)
(cid:74)
(cid:74)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

=

=

=

=

=

=

=

Fig. 41: OrientedPoint operators.

,
(cid:75)

S
(cid:74)

,
(cid:75)

F
(cid:74)

)
(cid:75)

74

Daniel J. Fremont et al.

D Additional Experiments

This section gives additional details on the
experiments and describes an experiment anal-
ogous to that of Sec. 6.3 but using the generic
two-car Scenic scenario as a baseline.

Additional Details on Experimental Setup

Table 9: Average precision (AP) re-
sults for the experiments in Table 6.

Training Data
Xmatrix / Xoverlap

Testing Data

Tmatrix

Toverlap

100% / 0%
95% / 5%

36.1 ± 1.1
36.0 ± 1.0

61.7 ± 2.2
65.8 ± 1.2

Since GTAV does not provide an explicit rep-
resentation of its map, we obtained an ap-
proximate map by processing a bird’s-eye schematic
view of the game world16. To identify points
on a road, we converted the image to black
and white, eﬀectively turning roads white
and everything else black. We then used edge
detection to ﬁnd curbs, and computed the
nominal traﬃc direction by ﬁnding for each
curb point X the nearest curb point Y on the
other side of the road, and assuming traﬃc
ﬂows perpendicular to the segment XY (this
was more robust than using the directions of
the edges in the image). Since the resulting
road information was imperfect, some gener-
ated scenes placed cars in undesired places
such as sidewalks or medians, and we had to
manually ﬁlter the generated images to re-
move these. With a real simulator, e.g. We-
bots, this is not necessary.

We now deﬁne in detail the metrics used
to measure the performance of our models.
Let ˆy = f (x) be the prediction of the model
f for input x. For our task, ˆy encodes bound-
ing boxes, scores, and categories predicted by
f for the image x. Let Bgt be a ground truth
box (i.e. a bounding box from the label of a
training sample that indicates the position
of a particular object) and Bˆy be a box pre-
dicted by the model. The Intersection over
Union (IoU) is deﬁned as IoU (Bgt, Bˆy) =
area (Bgt∩Bˆy)/area (Bgt∪Bˆy), where area (X)
is the area of a set X. IoU is a common eval-
uation metric used to measure how well pre-
dicted bounding boxes match ground truth
boxes. We adopt the common practice of con-
sidering Bˆy a detection for Bgt if IoU (Bgt, Bˆy) >
0.5.

Precision and recall are metrics used to
measure the accuracy of a prediction on a
particular image. Intuitively, precision is the
fraction of predicted boxes that are correct,
while recall is the fraction of objects actu-
ally detected. Formally, precision is deﬁned
as tp/(tp + f p) and recall as tp/(tp + f n),
where true positives tp is the number of cor-
rect detections, false positives f p is the num-
ber of predicted boxes that do not match any

16 https://www.gtafivemap.com/

ground truth box, and false negatives f n is
the number of ground truth boxes that are
not detected. We use average precision and
recall to evaluate the performance of a model
on a collection of images constituting a test
set.

Overlapping Scenario Experiments

In Sec. 6.3 we showed how we could improve
the performance of squeezeDet trained on
the Driving in the Matrix dataset [30] by re-
placing part of the training set with images
of overlapping cars. We used the standard
precision and recall metrics deﬁned above;
however, [30] uses a diﬀerent metric, AP (which
stands for Average Precision, but is not sim-
ply the average of the precision over the test
images). For completeness, Table 9 shows the
results of our experiment measured in AP
(as computed using [5]). The outcome is the
same as before: by using the mixture, per-
formance on overlapping images signiﬁcantly
improves, while performance on the original
dataset is unchanged.

For a cleaner comparison of overlapping
vs. non-overlapping cars, we also ran a ver-
sion of the experiment in Sec. 6.3 using the
generic two-car Scenic scenario as a base-
line. Speciﬁcally, we generated 1,000 images
from that scenario, obtaining a training set
Xtwocar. We also generated 1,000 images from
the overlapping scenario to get a training set
Xoverlap.

Note that Xtwocar did contain images
of overlapping cars, since the generic two-
car scenario does not constrain the cars’ lo-
cations. However, the average overlap was
much lower than that of Xoverlap, as seen in
Fig. 42 (note the log scale): thus the over-
lapping car images are highly “untypical”
of generic two-car images. We would like to
ensure the network performs well on these
diﬃcult images by emphasizing them in the
training set. So, as before, we constructed
various mixtures of the two training sets, ﬁx-
ing the total number of images but using
diﬀerent ratios of images from Xtwocar and

Scenic: A Language for Scenario Speciﬁcation and Data Generation

75

)
s
e
g
a
m

i

f
o

r
e
b
m
u
n
(
0
1
g
o
l

3

2

1

0

Table 10: Performance of models
trained on mixtures of Xtwocar and
Xoverlap and tested on both, averaged
over 8 training runs. 90/10 indicates
a 9:1 mixture of Ttwocar/Toverlap.

Mixture

Precision

Recall

Precision

Recall

Ttwocar

Toverlap

100/0
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
90/10
IOU
80/20
70/30

Xoverlap

Xtwocar

96.5 ± 1.0
95.3 ± 2.1
96.5 ± 0.7
96.5 ± 0.9

95.7 ± 0.5
96.2 ± 0.5
96.0 ± 0.6
96.5 ± 0.6

94.6 ± 1.1
93.9 ± 2.5
96.2 ± 0.5
96.0 ± 1.6

82.1 ± 1.4
86.9 ± 1.7
89.7 ± 1.4
90.1 ± 1.8

Intersection Over Union
Fig. 42:
(IOU) distribution for two-car and
overlapping training sets (log scale).

Xoverlap. We trained the network on each of
these mixtures and evaluated their perfor-
mance on 400-image test sets Ttwocar and
Toverlap from the two-car and overlapping
scenarios respectively.

To reduce the eﬀect of randomness in
training, we used the maximum precision and
recall obtained when training for 4,000 through
5,000 steps in increments of 250 steps. Addi-
tionally, we repeated each training 8 times,
using a random mixture each time: for ex-
ample, for the 90/10 mixture of Xtwocar and
Xoverlap, each training used an independent
random choice of which 90% of Xtwocar to
use and which 10% of Xoverlap.

As Tab. 10 shows, we obtained the same
results as in Sec. 6.3: the model trained purely
on generic two-car images has high preci-
sion and recall on Ttwocar but has drasti-
cally worse recall on Toverlap. However, de-
voting more of the training set to overlap-
ping cars gives a large improvement to re-
call on Toverlap while leaving performance
on Ttwocar essentially the same. This again
demonstrates that we can improve the per-
formance of a network on diﬃcult corner cases
by using Scenic to increase the representa-
tion of such cases in the training set.

