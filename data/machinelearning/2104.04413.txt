Provable Repair of Deep Neural Networks

Matthew Sotoudeh
University of California, Davis
Davis, CA, USA
masotoudeh@ucdavis.edu

Aditya V. Thakur
University of California, Davis
Davis, CA, USA
avthakur@ucdavis.edu

Abstract
Deep Neural Networks (DNNs) have grown in popularity
over the past decade and are now being used in safety-critical
domains such as aircraft collision avoidance. This has moti-
vated a large number of techniques for finding unsafe behav-
ior in DNNs. In contrast, this paper tackles the problem of cor-
recting a DNN once unsafe behavior is found. We introduce
the provable repair problem, which is the problem of repair-
ing a network 𝑁 to construct a new network 𝑁 ′ that satisfies
a given specification. If the safety specification is over a fi-
nite set of points, our Provable Point Repair algorithm can
find a provably minimal repair satisfying the specification,
regardless of the activation functions used. For safety speci-
fications addressing convex polytopes containing infinitely
many points, our Provable Polytope Repair algorithm can
find a provably minimal repair satisfying the specification
for DNNs using piecewise-linear activation functions. The
key insight behind both of these algorithms is the introduc-
tion of a Decoupled DNN architecture, which allows us to
reduce provable repair to a linear programming problem.
Our experimental results demonstrate the efficiency and ef-
fectiveness of our Provable Repair algorithms on a variety
of challenging tasks.

CCS Concepts: • Computing methodologies → Neural
networks; • Theory of computation → Linear program-
ming; • Software and its engineering → Software post-
development issues.

Keywords: Deep Neural Networks, Repair, Bug fixing

1
2
0
2

r
p
A
5
2

]

G
L
.
s
c
[

2
v
3
1
4
4
0
.
4
0
1
2
:
v
i
X
r
a

1 Introduction
Deep neural networks (DNNs) [21] have been successfully
applied to a wide variety of problems, including image recog-
nition [40], natural-language processing [14], medical diag-
nosis [38], aircraft collision avoidance [33], and self-driving
cars [10]. However, DNNs are far from infallible, and mis-
takes made by DNNs have led to loss of life [20, 42] and
wrongful arrests [29, 30]. This has motivated recent advances
in understanding [22, 57], verifying [4, 6, 18, 34, 52], and test-
ing [47, 50, 56, 58] of DNNs. In contrast, this paper addresses
the problem of repairing a DNN once a mistake is discovered.
Consider the following motivating scenario: we have a
trained SqueezeNet [32], a modern convolutional image-
recognition DNN consisting of 18 layers and 727,626 parame-
ters. It has an accuracy of 93.6% on the ImageNet dataset [13].
After deployment, we find that certain images are misclassi-
fied. In particular, we see that SqueezeNet has an accuracy
of only 18% on the Natural Adversarial Examples (NAE)
dataset [28]. Figure 1 shows one such image whose actual
class is Fox Squirrel, but the DNN predicts Sea Lion with
99% confidence. We would like to repair (or patch) the trained
SqueezeNet to ensure that it correctly classifies such images.
To repair the DNN, one could retrain the network us-
ing the original training dataset augmented with the newly-
identified buggy inputs. Retraining, however, is extremely
inefficient; e.g., training SqueezeNet takes days or weeks
using state-of-the-art hardware. Worse, the original train-
ing dataset is often not available for retraining; e.g., it could
be private medical information, sensitive intellectual prop-
erty, or simply lost. These considerations are more important
with privacy-oriented regulations that require companies to
delete private data regularly and upon request. Retraining
can also make arbitrary changes to the DNN and, in many
cases, introduce new bugs into the DNN behavior. These
issues make it infeasible, impossible, and/or ineffective to
apply retraining in many real-world DNN-repair scenarios.
One natural alternative to retraining is fine tuning, where
we apply gradient descent to the trained DNN but only us-
ing a smaller dataset collected once buggy inputs are found.
While this reduces the computational cost of repair and does
not require access to the original training dataset, fine-tuning
significantly increases the risk of drawdown, where the net-
work forgets things it learned on the original, larger training
dataset in order to achieve high accuracy on the buggy in-
puts [37]. In particular, fine tuning provides no guarantees
that it makes minimal changes to the original DNN.

 
 
 
 
 
 
Conference’17, July 2017, Washington, DC, USA

Matthew Sotoudeh and Aditya V. Thakur

Figure 1. Natural adver-
sarial example

Figure 2. Fog-corrupted
digit

The effectiveness of both retraining and fine tuning to
repair the DNN is extremely sensitive to the specific hyper-
parameters chosen; viz., training algorithm, learning rate,
momentum rate, etc. Importantly, because gradient descent
cannot disprove the existence of a better solution were some
other hyperparameters picked, one might have to try a large
number of potential hyperparameter combinations in the
hope of finding one that will lead to a successful repair. This
is a time-consuming process that significantly reduces the
effectiveness of such techniques in practice.

Based on the above observations, we can deduce the fol-
lowing requirements for our DNN repair algorithm, where 𝑁
is the buggy DNN, 𝑋 is the set of buggy inputs, and 𝑁 ′ is the
repaired DNN: (P1) efficacy: 𝑁 ′ should correctly classify all
inputs in 𝑋 ; (P2) generalization: 𝑁 ′ should correctly classify
inputs similar to those in 𝑋 ; (P3) locality: 𝑁 ′ should behave
the same as 𝑁 on inputs that are dissimilar to those in 𝑋 ;
(P4) efficiency: the repair algorithm should be efficient.

This paper presents a novel technique for Provable Point-
wise Repair of DNNs that is effective, generalizing, localized,
and efficient (§5). Given a DNN 𝑁 and a finite set of points
𝑋 along with their desired output, our Provable Pointwise
Repair algorithm synthesizes a repaired DNN 𝑁 ′ that is guar-
anteed to give the correct output for all points in 𝑋 . To ensure
locality, our algorithm can provably guarantee that the repair
(difference in parameters) from 𝑁 to 𝑁 ′ is the smallest such
single-layer repair. Our Provable Pointwise Repair algorithm
makes no restrictions on the activation functions used by 𝑁 .
Provable repair of even a single layer of the DNN is a for-
mally NP-hard problem, and completely infeasible in prac-
tice even using state-of-the-art SMT solvers [19]. The use of
non-linear activation functions implies that changing even
a single weight can have a non-linear effect on the output
of the DNN. However, if the final layer of the DNN is linear
(instead of a non-linear activation function), then repairing
just the output layer is actually a linear programming (LP)
problem [19] solvable in polynomial time [39].

The key insight to our approach is the introduction of a
new DNN architecture, called Decoupled DNNs or DDNNs.
DDNNs strictly generalize the notion of DNNs, meaning
every DNN can be trivially converted into an equivalent
DDNN. We will show that repairing any single layer in a
DDNN reduces to an LP problem. This allows us to compute

the smallest such single-layer repair with respect to either
the ℓ1 or ℓ∞ norm, and, thus, reduce forgetting.

This paper also introduces an algorithm for Provable Poly-
tope Repair of DNNs (§6), which is like Provable Point Repair
except the set of points 𝑋 is infinite and specified as a union
of convex polytopes (hereafter just “polytopes”) in the input
space of the network.

Consider a trained DNN for classifying handwritten digits,
which has an accuracy of 96.5% on the MNIST dataset [41].
After deployment, we find that the accuracy of the network
drops to 20% on images corrupted with fog; Figure 2 shows
an example of one such fog-corrupted image from MNIST-
C [46]. We would like to repair the network to correctly
classify such fog-corrupted images. However, we might also
want to account for different amounts of fog. Let 𝐼 and 𝐼𝑓
be an uncorrupted and fog-corrupted image, respectively.
Then each image along the line from 𝐼 to 𝐼𝑓 is corrupted by a
different amount of fog. We can use Provable Polytope Repair
so that the DNN correctly classifies all infinitely-many such
foggy images along the line from 𝐼 to 𝐼𝑓 .

Consider an aircraft collision-avoidance network [33] that
controls the direction an aircraft should turn based on the
relative position of an attacking aircraft. We may want this
DNN to satisfy certain properties, such as never instructing
the aircraft to turn towards the attacker when the attacker is
within a certain distance. Our Provable Polytope Repair algo-
rithm can synthesize a repaired DNN that provably satisfies
such safety properties on an infinite set of input scenarios.
The main insight for solving Provable Polytope Repair
is that, for piecewise-linear DDNNs, repairing polytopes
(with infinitely many points) is equivalent to Provable Point
Repair on finitely-many key points. These key points can
be computed for DDNNs using prior work on computing
symbolic representations of DNNs [54, 55]. This reduction
is intuitively similar to how the simplex algorithm reduces
optimizing over a polytope with infinitely many points to op-
timizing over the finitely-many vertex points. As illustrated
by the above two scenarios, there are practical applications
in which the polytopes used in the repair specification are
low-dimensional subspaces of the input space of the DNNs.
We evaluate the efficiency and efficacy of our Provable
Repair algorithms compared to fine tuning (§7). The repairs
by our algorithms generalize to similarly-buggy inputs while
avoiding significant drawdown, or forgetting.

The contributions of the paper are:

• We introduce Decoupled DNNs, a new DNN architec-
ture that enables efficient and effective repair (§4).

• An algorithm for Provable Point Repair (§5).
• An algorithm for Provable Polytope Repair of piecewise-

linear DNNs (§6).

• Experimental evaluation of Provable Repair (§7).

§2 describes preliminaries; §3 presents an overview of our
approach; §8 describes related work; §9 concludes.

Provable Repair of DNNs

Conference’17, July 2017, Washington, DC, USA

2 Preliminaries
A feed-forward DNN is a special type of loop-free computer
program that computes a vector-valued function. DNNs are
often represented as layered DAGs. An input to the network
is given by associating with each node in the input layer
one component of the input vector. Then each node in the
second layer computes a weighted sum of the nodes in the
input layer according to the edge weights. The output of
each node in the second layer is the image of this weighted
sum under some non-linear activation function associated
with the layer. This process is repeated until output values
at the final layer are computed, which form the components
of the output vector.

Although we will use the above DAG definition of a DNN
for the intuitive examples in § 3, for most of our formal
theorems we will use an entirely equivalent definition of
DNNs, below, as an alternating concatenation of linear and
non-linear functions.

Definition 2.1. A Deep Neural Network (DNN) with layer
sizes 𝑠0, 𝑠1, . . . , 𝑠𝑛 is a list of tuples (𝑊 (1), 𝜎 (1) ), . . . , (𝑊 (𝑛), 𝜎 (𝑛) ),
where each 𝑊 (𝑖) is an 𝑠𝑖 × 𝑠𝑖−1 matrix and 𝜎 (𝑖) : R𝑠𝑖 → R𝑠𝑖
is some activation function.
Definition 2.2. Given a DNN 𝑁 with layers (𝑊 (𝑖), 𝜎 (𝑖) )
we say the function associated with the DNN is a function
𝑁 : R𝑠0 → R𝑠𝑛 given by 𝑁 ((cid:174)𝑣) = (cid:174)𝑣 (𝑛) where (cid:174)𝑣 (0) (cid:66) (cid:174)𝑣 and
(cid:174)𝑣 (𝑖) (cid:66) 𝜎 (𝑖) (𝑊 (𝑖) (cid:174)𝑣 (𝑖−1) ).

For ease of exposition, we have assumed: (i) that every
layer in the DNN is fully-connected, i.e., parameterized by
an entire weight matrix, and (ii) that the activation func-
tions 𝜎 (𝑖) have the same domain and range. However, our
algorithms do not rely on these conditions, and in fact, we
use more complicated DNNs (such as Convolutional Neural
Networks) in our evaluations (§7).

There are a variety of activation functions used for 𝜎 (𝑖) , in-
cluding ReLU, Hyperbolic Tangent, (logistic) Sigmoid, Aver-
agePool, and MaxPool [21]. In our examples, we will use the
ReLU function, defined below, due to its simplicity and use
in real-world DNN architectures, although our algorithms
and theory work for arbitrary activation functions.

Definition 2.3. The ReLU Activation Function is a vector-
valued function R𝑛 → R𝑛 defined component-wise by

𝑅𝑒𝐿𝑈 ((cid:174)𝑣)𝑖 =

(cid:40)𝑣𝑖
0

if 𝑣𝑖 ≥ 0
otherwise,

where 𝑅𝑒𝐿𝑈 ((cid:174)𝑣)𝑖 is the 𝑖th component of the output vector
𝑅𝑒𝐿𝑈 ((cid:174)𝑣) and 𝑣𝑖 is the 𝑖th of the input vector (cid:174)𝑣.

Of particular note for our polytope repair algorithm (§6),
some of the most common activation functions (particularly
ReLU) are piecewise-linear.

Definition 2.4. A function 𝑓 : R𝑛 → R𝑚 is piecewise-linear
(PWL) if its input domain can be partitioned into finitely-
many polytopes 𝑋1, 𝑋2, . . . , 𝑋𝑛 such that, for each 𝑋𝑖 , there
exists some affine function 𝑓𝑖 such that 𝑓 (𝑥) = 𝑓𝑖 (𝑥) for every
𝑥 ∈ 𝑋𝑖 .

This paper uses the terms ‘linear’ and ‘affine’ interchange-
ably. It follows from Definition 2.4 that compositions of PWL
functions are themselves PWL. Hence, a DNN using only
PWL activation functions is also PWL in its entirety.

For a network using PWL activation functions, we can
always associate with each input to the network an activation
pattern, as defined below.

Definition 2.5. Let 𝑁 be a DNN using only PWL activation
functions. Then an activation pattern 𝛾 is a mapping from
each activation function 𝜎 ( 𝑗) to a linear region 𝛾 (𝜎 ( 𝑗) ) of
𝜎 ( 𝑗) . We say an activation pattern 𝛾 holds for a vector (cid:174)𝑣 if,
for every layer 𝑗, we have 𝑊 ( 𝑗) (cid:174)𝑣 ( 𝑗−1) ∈ 𝛾 (𝜎 ( 𝑗) ).

Recall that (cid:174)𝑣 is the input to the first layer of the network
while (cid:174)𝑣 ( 𝑗−1) is the intermediate input to the 𝑗th layer. For
example, suppose 𝑁 is a ReLU network where 𝛾 holds for
vector 𝑣, then 𝛾 (𝜎 ( 𝑗) ) is exactly the set of nodes in layer 𝑗
with positive output when evaluating the DNN on input (cid:174)𝑣.
Let 𝑁 be a DNN that uses only PWL activation func-
tions. Then we notate by 𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠 (𝑁 ) the set of polytopes
𝑋1, 𝑋2, . . . , 𝑋𝑛 that partition the domain of 𝑁 such that the
conditions in Definition 2.4 hold. In particular, we will use
the partitioning for which we can assign each 𝑋𝑖 a unique
activation pattern 𝛾𝑖 such that 𝛾𝑖 holds for all (cid:174)𝑣 ∈ 𝑋𝑖 .

When appropriate, for polytope 𝑃 in the domain of 𝑁 , we
will notate by 𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠 (𝑁 , 𝑃) a partitioning 𝑋1, 𝑋2, . . . , 𝑋𝑛
of 𝑃 that meets the conditions in Definition 2.4. Formally,
we have 𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠 (𝑁 , 𝑃) (cid:66) 𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠 (𝑁↾𝑃 ), where 𝑁↾𝑃
is the restriction of 𝑁 to domain 𝑃.

Consider the ReLU DNN 𝑁1 shown in Figure 3(a), which
has one input 𝑥, one output 𝑦, and three so-called hidden
nodes ℎ1, ℎ2, and ℎ3 using ReLU activation function. We will
consider the input-output behavior of this network for the
domain 𝑥 ∈ [−1, 2]. The linear regions of 𝑁1 are shown
visually in Figure 3(c) as colored intervals on the 𝑥 axis,
which each map into the postimage according to some affine
mapping which is specific to that region. In particular, we
have three linear regions:

𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠 (𝑁1, [−1, 2]) = {[−1, 0], [0, 1], [1, 2]}.

(1)

Each linear region corresponds to a particular activation
pattern on the hidden nodes; i.e., which ones are in the zero
region or the identity region. The first linear region, [−1, 0]
(red), corresponds to the activation pattern where only ℎ1 is
activated. The second linear region, [0, 1] (blue), corresponds
to the activation pattern where only ℎ2 is activated. Finally,
the third linear region, [1, 2] (green), corresponds to the
activation pattern where both ℎ2 and ℎ3 are activated.

Conference’17, July 2017, Washington, DC, USA

Matthew Sotoudeh and Aditya V. Thakur

1

𝑥

−1
1
1

ℎ3

ℎ2

1
−1

𝑦

1

𝑥

−1
2

1

ℎ3

ℎ2

1
−1

𝑦

−1

−1

ℎ1
(a) DNN 𝑁1

−1

−1

ℎ1
(b) DNN 𝑁2

1

1

0.5

0

𝑦
t
u
p
t
u
O
−0.5

0.5

0

𝑦
t
u
p
t
u
O
−0.5

−1

−1 −0.5 0 0.5 1 1.5 2
Input 𝑥

−1

−1 −0.5 0 0.5 1 1.5 2
Input 𝑥

(c) Input-output plot of 𝑁1

(d) Input-output plot of 𝑁2

Figure 3. Example DNNs and their input-output behavior.
The ℎ𝑖 nodes have ReLU activation functions. Colored bars
on the 𝑥 axis denote the linear regions.

In practice, we can quickly compute 𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠 (𝑁 , 𝑃) for
either large 𝑁 with one-dimensional 𝑃 or medium-sized 𝑁
with two-dimensional 𝑃. We use the algorithm of Sotoudeh
and Thakur [55] for computing 𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠 (𝑁 , 𝑃) when 𝑃 is
one- or two-dimensional.
Definition 2.6. A linear program (LP) with 𝑛 constraints on
𝑚 variables is a triple (𝐴, (cid:174)𝑏, (cid:174)𝑐) where 𝐴 is an 𝑛 × 𝑚 matrix, (cid:174)𝑏
is an 𝑛-dimensional vector, and (cid:174)𝑐 is an 𝑚-dimensional vector.
A solution to the linear program is an 𝑚-dimensional vector
(cid:174)𝑥 such that (i) 𝐴 (cid:174)𝑥 ≤ (cid:174)𝑏, and (ii) (cid:174)𝑐 · (cid:174)𝑥 is minimal among all (cid:174)𝑥
satisfying (i).

Linear programs can be solved in polynomial time [39],
and many efficient, industrial-grade LP solvers such as the
Gurobi solver [25] exist. Through the addition of auxiliary
variables, it is also possible to encode in an LP the objective
of minimizing the ℓ1 and/or ℓ∞ norms of (cid:174)𝑥 [8].

3 Overview
This paper discusses how to repair DNNs to enforce precise
specifications, i.e., constraints on input-output behavior.

3.1 Provable Pointwise Repair

The first type of specification we will consider is a point
repair specification. In this scenario, we are given a finite
set of input points along with, for each such point, a subset
of the output region which we would like that point to be
mapped into by the network.

Consider DNN 𝑁1 in Figure 3(a). We see that 𝑁1(0.5) =
−0.5 and 𝑁1(1.5) = −1. We want to repair it to form a new
network 𝑁 ′ such that

(−1 ≤ 𝑁 ′(0.5) ≤ −0.8) ∧ (−0.2 ≤ 𝑁 ′(1.5) ≤ 0).

(2)

We formalize this point specification as (𝑋, 𝐴·, 𝑏 ·) where 𝑋
is a finite collection of repair points 𝑋 = {𝑋1 = 0.5, 𝑋2 = 1.5},
and we associate with each 𝑥 ∈ 𝑋 a polytope in the out-
put space defined by 𝐴𝑥, 𝑏𝑥 that we would like it to be
,𝑏𝑋1 =
mapped into by 𝑁 ′. In this case, we can let 𝐴𝑋1 =
(cid:104)−0.8
1

representing the poly-
hedral constraints 𝐴𝑋1 𝑁 ′(𝑋1) ≤ 𝑏𝑋1 ∧ 𝐴𝑋2 𝑁 ′(𝑋2) ≤ 𝑏𝑋2 .
These constraints are equivalent to Equation 2.

, and 𝑏𝑋2 =

,𝐴𝑋2 =

(cid:104) 0
0.2

(cid:104) 1
−1

(cid:104) 1
−1

(cid:105)

(cid:105)

(cid:105)

(cid:105)

The general affine constraint form we use is very expres-
sive. For example, it can express constraints such as “the
𝑖th output component is larger than all others,” which for a
multi-label classification network is equivalent to ensuring
that the point is classified with label 𝑖.
The two roles of a ReLU. At first glance, it is tempting
to directly encode the DNN in an SMT solver like Z3 [12]
and attempt to solve for weight assignments that cause the
desired classification. However, in practice this quickly be-
comes infeasible even for networks with very few nodes.

To understand the key reason for this infeasibility, con-
sider what happens when a single weight in 𝑁1 is modified
to construct the new DNN 𝑁2, shown in Figure 3(b). In par-
ticular, the weight on 𝑥 → ℎ3 is changed from a 1 to a 2.
Comparing Figure 3(d) with Figure 3(c), we see that changing
this weight has caused two distinct changes in the plot:

1. The linear function associated with the green region

has changed, and

2. Simultaneously, the linear regions themselves (shown
on the 𝑥 axis) have changed, with the green region
growing to include parts of the space originally in the
blue region. In particular, 𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠 (𝑁2, [−1, 2]) =
{[−1, 0], [0, 0.5], [0.5, 2]}, different from Equation 1.
We use coupling to refer to the fact that the weights in
a ReLU DNN simultaneously control both of these aspects.
This coupling causes repair of DNNs to be computationally
infeasible, because the impact of changing a weight in the
network with respect to the output of the network on a fixed
input is non-linear; it ‘jumps’ every time the linear region
that the point falls into changes. This paper shows that de-
coupling these two roles leads to a generalized class of neural
networks along with a polynomial-time repair algorithm.
Decoupling activations from values. The key insight of
this paper is a novel DNN architecture, Decoupled DNNs
(DDNNs), defined in § 4, that strictly generalizes standard
feed-forward DNNs while at the same time allowing us to
decouple the two roles that the parameters play.

Figure 4(a) shows a DDNN 𝑁3 equivalent to 𝑁1 from Fig-
ure 3(a). Most notably, every decoupled DNN consists of two
‘sub-networks,’ or channels. The activation channel, shown in
red, is used to determine the positions of the linear regions.
Meanwhile, the value channel determines the output map
within each linear region. The activation channel influences
the value channel via the blue edges, which indicate that the

Provable Repair of DNNs

Conference’17, July 2017, Washington, DC, USA

1

𝑥𝑎

−1

1

1

−1

ℎ𝑎
3

ℎ𝑎
2

ℎ𝑎
1

1
−1

𝑦𝑎

−1

1

𝑥 𝑣

−1
1

1

−1

ℎ𝑣
3

ℎ𝑣
2

ℎ𝑣
1

1

𝑥𝑎

−1

1

1

−1

ℎ𝑎
3

ℎ𝑎
2

ℎ𝑎
1

1
−1

𝑦𝑣

−1

1
−1

𝑦𝑎

−1

1

𝑥 𝑣

−1
2

1

−1

ℎ𝑣
3

ℎ𝑣
2

ℎ𝑣
1

1
−1

𝑦𝑣

−1

(a) Decoupled DNN 𝑁3 =
(𝑁1, 𝑁1).

(b) Decoupled DNN 𝑁4 =
(𝑁1, 𝑁2).

1

1

0.5

𝑣
𝑦
t
u
p
t
u
O
−0.5

0

0.5

𝑣
𝑦
t
u
p
t
u
O
−0.5

0

−1

−1 −0.5 0 0.5 1 1.5 2
Input 𝑥𝑣 (= 𝑥𝑎)

−1

−1 −0.5 0 0.5 1 1.5 2
Input 𝑥𝑣 (= 𝑥𝑎)

(c) Input-output plot of 𝑁3.

(d) Input-output plot of 𝑁4.

Figure 4. Decoupled DNNs 𝑁3 and 𝑁4 and their input-output
behavior. DDNNs 𝑁3 and 𝑁4 have the same activation chan-
nel 𝑁1, but different value channels.

adjacent value node is activated only if the corresponding
activation node is. For example, if the input to ℎ𝑎
2 is negative,
2 will output zero regardless of the input to ℎ𝑣
then ℎ𝑣
2.
To compute the output of a DDNN on a given input 𝑥0,
we first set 𝑥𝑎 = 𝑥0, evaluate the activation channel, and
record which of the hidden nodes ℎ𝑎
𝑖 were active (received a
positive input) or inactive (otherwise). Then, we set 𝑥 𝑣 = 𝑥0
and evaluate the value channel, except instead of activating
a node if its input is non-negative, we activate the node if
the corresponding activation channel node was activated. In
this way, activation nodes can ‘mask’ their corresponding
value nodes, as notated with the blue edges in Figure 4(a).

Now, consider what happens when we change a weight
in only the value channel, as shown in Figure 4(b). In that
scenario, on any given point, the activation pattern for any
given input does not change, and so the locations of the linear
regions on the 𝑥 axis of Figure 4(d) are unchanged from Fig-
ure 4(c). However, what we find is that the linear function
within any given region does change. Note that in this case
only the green line has changed, however in deeper networks
changing any given weight can change all of the lines.
Repair of DDNNs. This observation foreshadows two of
our key theoretical results in § 4. The first theorem (Theo-
rem 4.5) shows that, for any given input, the output of the
DDNN varies linearly on the change of any given weight in
the value channel. In fact, we will show the stronger fact that
the output depends linearly with the change of any layer of
weights in the value channel.

≤

































Δ1
Δ2
Δ3
Δ4

0 −0.5
0.5
0
0 −1.5
1.5
0

0
0
0
0
1.5
1
−1.5 −1

0(cid:3) (cid:174)Δ = −0.5 − 0.5Δ2,
1.5

Using this fact, we can reduce pointwise repair of a single
layer in the DDNN to a linear programming (LP) problem. In
the running example, suppose we want to repair the first
value layer of DDNN 𝑁3 to satisfy Equation 2. Let Δ be the
difference in the first layer weights, where Δ𝑖 is the change in
the weight on edge 𝑥 𝑣 → ℎ𝑣
𝑖 , Δ4 is the change in the weight
on edge 1 → ℎ𝑣
3, and 𝑁 ′ be the DDNN with first-layer value
weights changed by Δ. Then, Theorem 4.5 guarantees that
𝑁 ′(𝑋1) = (cid:2)−0.5(cid:3) + (cid:2)0 −0.5
0
while 𝑁 ′(𝑋2) = (cid:2)−1(cid:3) + (cid:2)0 −1.5
1(cid:3) (cid:174)Δ = −1 − 1.5Δ2 +
1.5Δ3 + Δ4. Hence, we can encode our specification as an LP
like so: (−1 ≤ −0.5 − 0.5Δ2 ≤ −0.8) ∧ (−0.2 ≤ −1 − 1.5Δ2 +
1.5Δ3 + Δ4 ≤ 0), or in a more formal LP form,
−0.3


0.5


1

−0.8










We can then solve for Δ using an off-the-shelf LP solver,
such as Gurobi [25]. We can also simultaneously optimize
a linear objective, such as the ℓ∞ or ℓ1 norm, to find the
satisfying repair with the provably smallest Δ. This helps
ensure locality of the repair and preserve the otherwise-
correct existing behavior of the network. In this case, we
can find that the smallest repair with respect to the ℓ1 norm
is Δ1 = 0, Δ2 = 0.6, Δ3 = 1.13, Δ4 = 0. The corresponding
repaired DDNN 𝑁5 is shown in Figure 5(a) and plotted in Fig-
ure 5(c), where we can see that the repaired network satisfies
the constraints because 𝑁5(0.5) = −0.8 and 𝑁5(1.5) = −0.2.
Notably, the linear regions of 𝑁5 are the same as those of 𝑁1.
Non-ReLU, non-fully-connected, activation functions.
While we have focused in this overview on the ReLU case for
ease of exposition, the key result of Theorem 4.5 also holds
for a generalization of DDNNs using arbitrary activation
functions, such as tanh and sigmoid. Hence, our pointwise
repair algorithm works for arbitrary feed-forward networks.
Similarly, although we have formalized DNNs assuming fully-
connected layers, our approach can repair convolutional and
other similar types of layers as well (as demonstrated in §7.1).

3.2 Provable Polytope Repair

We now consider Provable Polytope Repair. The specification
for provable polytope repair constrains the output of the net-
work on finitely-many polytopes in the input space, each one
containing potentially infinitely many points. For example,
given the DNN 𝑁1 we may wish to enforce a specification
∀𝑥 ∈ [0.5, 1.5]. − 0.8 ≤ 𝑁 ′(𝑥) ≤ −0.4.

(3)

We represent this as a polytope specification with one in-
put polytope, 𝑋 = {𝑃1 = [0.5, 1.5]}, which should map
(cid:105)
to the polytope in the output space given by 𝐴𝑃1 =
,

(cid:104) 1
−1
. The constraint ∀𝑥 ∈ 𝑃1.𝐴𝑃1 𝑁 ′(𝑥) ≤ 𝑏𝑃1 is then

𝑏𝑃1 =
equivalent to the specification in Equation 3.

(cid:104)−0.4
0.8

(cid:105)

Conference’17, July 2017, Washington, DC, USA

Matthew Sotoudeh and Aditya V. Thakur

1

𝑥𝑎

−1

1

1

−1

ℎ𝑎
3

ℎ𝑎
2

ℎ𝑎
1

1
−1

𝑦𝑎

−1

1

𝑥 𝑣

−1
2.13

1.6

−1

ℎ𝑣
3

ℎ𝑣
2

ℎ𝑣
1

1

𝑥𝑎

−1

1

1

−1

ℎ𝑎
3

ℎ𝑎
2

ℎ𝑎
1

1
−1

𝑦𝑣

−1

1
−1

𝑦𝑎

−1

1

𝑥 𝑣

−1

1

0.8

−1

ℎ𝑣
3

ℎ𝑣
2

ℎ𝑣
1

1
−1

𝑦𝑣

−1

(a) Pointwise Repaired
DDNN 𝑁5.

(b) Polytope Repaired
DDNN 𝑁6.

1

1

0.5

𝑣
𝑦
t
u
p
t
u
O
−0.5

0

0.5

𝑣
𝑦
t
u
p
t
u
O
−0.5

0

−1

−1 −0.5 0 0.5 1 1.5 2
Input 𝑥𝑣 (= 𝑥𝑎)

−1

−1 −0.5 0 0.5 1 1.5 2
Input 𝑥𝑣 (= 𝑥𝑎)

(c) Input-output plot of 𝑁5.

(d) Input-output plot of 𝑁6.

Figure 5. Repaired DDNNs.

Reduction of polytope repair to pointwise repair. Our
key insight (Theorem 4.6) is that, for piecewise-linear DDNNs,
if we only change the value channel parameters, then we
can reduce polytope repair to pointwise repair. To see this,
recall that the value channel parameters do not change the
location of the linear regions, only the behavior within each
one. Within each linear region, the behavior of the network
is linear, and, hence, convex. Convexity guarantees that any
given polytope is mapped into another polytope if and only
if its vertices are mapped into that polytope. Note that the as-
sumption of piecewise-linearity is important here: in contrast
to pointwise patching, which works for any feed-forward
DNN, our polytope patching algorithm requires the activa-
tion functions to be piecewise-linear.

In our 1D example, this observation is the fact that a line
lies in the desired interval of [−0.8, −0.4] if and only if its
endpoints do. In fact, the input region of interest in our
example of [0.5, 1.5] overlaps with two of these lines (the
blue and green line segments in Figure 3(c)). Hence, we must
ensure that both of those lines have endpoints in [−0.8, −0.4].
Thus, the polytope specification is met if and only if the
point specification with 𝐾 = {𝐾1 = 0.5, 𝐾2 = 1, 𝐾3 = 1, 𝐾4 =
, 𝑏𝐾1 = 𝑏𝐾2 =
1.5} and 𝐴𝐾1 = 𝐴𝐾2 = 𝐴𝐾3 = 𝐴𝐾4 =

(cid:105)

(cid:104) 1
−1

(cid:105)

(cid:104)−0.4
0.8

𝑏𝐾3 = 𝑏𝐾4 =
is met. We call the points in 𝐾 key points
because the behavior of the repaired network 𝑁 ′ on these
points determines the behavior of the network on all of 𝑃1.
Note that 𝐾2 and 𝐾3 both refer to the same input point, 1.
This is because we need to verify that 𝑁 ′(1) is in the desired
output range when approaching either from the left or the

right, as we want to verify it for both the blue and the green
lines in Figure 4(c). This technicality is discussed in more
detail in Appendix B.

Therefore, we have reduced the problem of repair on poly-
topes to repair on finitely-many key points, which are the
vertices of the polytopes in the specification intersected with
the polytopes defining the linear regions of the DNN. We
can apply the algorithm discussed for pointwise repair to
solve for a minimal fix to the first layer. In particular, we
get the linear constraints: −0.8 ≤ −0.5 − 0.5Δ2 ≤ −0.4,
−0.8 ≤ −1 − Δ2 ≤ −0.4, −0.8 ≤ −1 − Δ2 + Δ3 + Δ4 ≤ −0.4,
and −0.8 ≤ −1 − 1.5Δ2 + 1.5Δ3 + Δ4 ≤ −0.4, for which an
ℓ1-minimal solution is the single weight change Δ2 = −0.2.
The corresponding repaired DDNN is shown in Figure 5(b)
and plotted in Figure 5(d), which shows that the repaired
network satisfies the constraints.

4 Decoupled DNNs
In this section, we formally define the notion of a Decou-
pled Deep Neural Network (DDNN), which is a novel DNN
architecture that will allow for polynomial-time layer repair.
A DDNN is defined similarly to a DNN (Definition 2.1),
except it has two sets of weights; the activation channel has
weights 𝑊 (𝑎,𝑖) and the value channel has weights 𝑊 (𝑣,𝑖) .

Definition 4.1. A Decoupled DNN (DDNN) having layers
of size 𝑠0, . . . , 𝑠𝑛 is a list of triples (𝑊 (𝑎,1),𝑊 (𝑣,1), 𝜎 (1) ), . . .,
(𝑊 (𝑎,𝑛),𝑊 (𝑣,𝑛), 𝜎 (𝑛) ), where 𝑊 (𝑎,𝑖) and 𝑊 (𝑣,𝑖) are 𝑠𝑖 × 𝑠𝑖−1
matrices and 𝜎 (𝑖) : R𝑠𝑖 → R𝑠𝑖 is some activation function.

We now give the semantics for a DDNN. The input (cid:174)𝑣 is du-
plicated to form the inputs (cid:174)𝑣 (𝑎,0) and (cid:174)𝑣 (𝑣,0) to the activation
and value channels, respectively. The semantics of the acti-
vation channel, having activation vectors (cid:174)𝑣 (𝑎,𝑖) , is the same
as for a DNN (Definition 2.2). The semantics for the value
channel with value vectors (cid:174)𝑣 (𝑣,𝑖) is similar, except instead of
using the activation function 𝜎 (𝑖) , we use the linearization
of 𝜎 (𝑖) around the input 𝑊 (𝑎,𝑖) (cid:174)𝑣 (𝑎,𝑖−1) of the corresponding
activation layer, as defined below.
Definition 4.2. Given function 𝑓 : R𝑛 → R𝑚 differentiable
at (cid:174)𝑣0, define the Linearization of 𝑓 around (cid:174)𝑣0 to be the function:
𝐿𝑖𝑛𝑒𝑎𝑟𝑖𝑧𝑒 [𝑓 , (cid:174)𝑣0] ( (cid:174)𝑥) (cid:66) 𝑓 ( (cid:174)𝑣0) + 𝐷 (cid:174)𝑣 𝑓 ( (cid:174)𝑣0) × ( (cid:174)𝑥 − (cid:174)𝑣0).

Above, 𝐷 (cid:174)𝑣 𝑓 ((cid:174)𝑣0) is the Jacobian of 𝑓 with respect to its in-
put at the point point (cid:174)𝑣0. The Jacobian generalizes the notion
of a scalar derivative to vector functions (see Definition A.5).
The output of the DDNN is taken to be the output (cid:174)𝑣 (𝑣,𝑛) of
the value channel. These DDNN semantics are stated below.
Definition 4.3. The function 𝑁 : R𝑠0 → R𝑠𝑛 associated
with the DDNN 𝑁 with layers (𝑊 (𝑎,𝑖),𝑊 (𝑣,𝑖), 𝜎 (𝑖) ) is given
by 𝑁 ((cid:174)𝑣) = (cid:174)𝑣 (𝑣,𝑛) where
(cid:174)𝑣 (𝑎,0) (cid:66) (cid:174)𝑣 (𝑣,0) (cid:66) (cid:174)𝑣,
(cid:174)𝑣 (𝑎,𝑖) (cid:66) 𝜎 (𝑖) (𝑊 (𝑎,𝑖) (cid:174)𝑣 (𝑎,𝑖−1) ), and
(cid:174)𝑣 (𝑣,𝑖) (cid:66) 𝐿𝑖𝑛𝑒𝑎𝑟𝑖𝑧𝑒 [𝜎 (𝑖),𝑊 (𝑎,𝑖) (cid:174)𝑣 (𝑎,𝑖−1) ] (𝑊 (𝑣,𝑖) (cid:174)𝑣 (𝑣,𝑖−1) ).

Provable Repair of DNNs

Conference’17, July 2017, Washington, DC, USA

(cid:174)𝑣 (𝑎,𝑖)

1

1

𝑊 (𝑣,𝑖) (cid:174)𝑣 (𝑣,𝑖−1)

−2

−1

1
𝑊 (𝑎,𝑖) (cid:174)𝑣 (𝑎,𝑖−1)

2

𝑊 (𝑎,𝑖) (cid:174)𝑣 (𝑎,𝑖−1)

𝑊 (𝑖) (cid:174)𝑣 (𝑣,𝑖−1)

−2

−1

1

2

(cid:174)𝑣 (𝑣,𝑖)

−1

(cid:174)𝑣 (𝑣,𝑖)

(a)

−1

(cid:174)𝑣 (𝑎,𝑖)

(b)

Figure 6. (a) Linearized ReLU and (b) Linearized Tanh.

DDNNs can be extended to non-differentiable activation

functions as discussed in Appendix C.

Consider the ReLU activation function in Figure 6(a). We
see that the activation node gets an input of 1 (in red on the
𝑥 axis) and so produces an output (in the activation channel)
of 1. The linearization of the ReLU function around the point
1 is the identity function 𝑓 (𝑥) = 𝑥 (shown in orange). Thus,
we use the function 𝑓 (𝑥) = 𝑥 as the activation function for
the corresponding value node. This means that if the value
node gets an input of, say, −1 as shown in black in Figure 6(a),
then its output will be −1. Effectively, if the input to the
activation node is positive, then the corresponding value node
will be activated (i.e., pass its input through as its output).
On the other hand, if the input to the activation node were
negative, then the linearization would be the zero function
𝑓 (𝑥) = 0. The value node would use that as its activation
function, effectively deactivating it regardless of its input
from the value channel.

Consider also the Tanh activation function (Figure 6(b)).
The activation channel behaves as normal, each node out-
putting the Tanh of its input. For example, if the input to
the activation node is -1 (shown in red), then its output is
tanh(−1) (shown below it in red). However, for the value
channel, we use the linearization of tanh around the input to
the corresponding activation node. In this case, we linearize
Tanh around -1 to get the line shown in orange, which is
used as the activation function for the value channel.

Thus, as we have shown, each node in the activation chan-
nel produces a new activation function to be used in the
value channel.
Key results. The first key result shows that the class of
DDNNs generalizes that of DNNs; for any DNN, the below
theorem gives a trivial construction for an exactly equivalent
DDNN by setting the activation and value channel weights
to be identical to the weights of the DNN.
Theorem 4.4. Let 𝑁 be a DNN with layers (𝑊 (𝑖), 𝜎 (𝑖) ) and 𝑀
be the DDNN with layers (𝑊 (𝑖),𝑊 (𝑖), 𝜎 (𝑖) ). Then, as functions,
𝑁 = 𝑀.
Proof. Let (cid:174)𝑣 be chosen arbitrarily. Let (cid:174)𝑣 (𝑖) be the intermedi-
ates of 𝑁 on (cid:174)𝑣 according to Definition 2.2, and (cid:174)𝑣 (𝑎,𝑖) , (cid:174)𝑣 (𝑣,𝑖) be
the intermediates of 𝑀 on (cid:174)𝑣 according to Definition 4.3.

We now prove, for all 𝑖, that (cid:174)𝑣 (𝑣,𝑖) = (cid:174)𝑣 (𝑎,𝑖) = (cid:174)𝑣 (𝑖) . We
proceed by induction on 𝑖. By definition, (cid:174)𝑣 (𝑎,0) = (cid:174)𝑣 (𝑣,0) = (cid:174)𝑣 (0) .
Now, suppose for sake of induction that (cid:174)𝑣 (𝑎,𝑖) = (cid:174)𝑣 (𝑣,𝑖) = (cid:174)𝑣 (𝑖) .
Then we have by definition and the inductive hypothesis
(cid:174)𝑣 (𝑎,𝑖+1) = 𝜎 (𝑖+1) (𝑊 (𝑖+1) (cid:174)𝑣 (𝑎,𝑖) ) = 𝜎 (𝑖+1) (𝑊 (𝑖+1) (cid:174)𝑣 (𝑖) ) = (cid:174)𝑣 (𝑖+1),

as well as
(cid:174)𝑣 (𝑣,𝑖+1)

= 𝐿𝑖𝑛𝑒𝑎𝑟𝑖𝑧𝑒 [𝜎 (𝑖+1),𝑊 (𝑖+1) (cid:174)𝑣 (𝑎,𝑖) ] (𝑊 (𝑖+1) (cid:174)𝑣 (𝑣,𝑖) )
= 𝐿𝑖𝑛𝑒𝑎𝑟𝑖𝑧𝑒 [𝜎 (𝑖+1),𝑊 (𝑖+1) (cid:174)𝑣 (𝑖) ] (𝑊 (𝑖+1) (cid:174)𝑣 (𝑖) )
= 𝜎 (𝑖+1) (𝑊 (𝑖+1) (cid:174)𝑣 (𝑖) )
= (cid:174)𝑣 (𝑖+1),

(Definition)

(Ind. Hyp.)

(Linearization)

(Definition)

because linearizations are exact at their center point. By
induction, then, (cid:174)𝑣 (𝑣,𝑖) = (cid:174)𝑣 (𝑖) for 0 ≤ 𝑖 ≤ 𝑛, and in particular
(cid:174)𝑣 (𝑣,𝑛) = (cid:174)𝑣 (𝑛) . But this is by definition 𝑀 ((cid:174)𝑣) = 𝑁 ((cid:174)𝑣), and as (cid:174)𝑣
was chosen arbitrarily, this gives us 𝑁 = 𝑀 as functions. □

Our next result proves that the output of a DDNN varies
linearly with changes in any given value channel layer weights.
Note that DDNNs are not linear functions with respect to
their input, only with respect to the value weights.
Theorem 4.5. Let 𝑗 be a fixed index and 𝑁 be DDNN with
layers (𝑊 (𝑎,𝑖),𝑊 (𝑣,𝑖), 𝜎 (𝑖) ). Then, for any (cid:174)𝑣, 𝑁 ((cid:174)𝑣) varies lin-
early as a function of 𝑊 (𝑣,𝑗) .
Proof. Changing 𝑊 (𝑣,𝑗) does not modify the values of (cid:174)𝑣 (𝑎,𝑖)
or (cid:174)𝑣 (𝑣,𝑖) for 𝑖 < 𝑗, hence (i) we can assume WLOG that 𝑗 = 1,
and (ii) all of the (cid:174)𝑣 (𝑎,𝑖) s remain constant as we vary 𝑊 (𝑣,1) .
Consider now the value of
(cid:174)𝑣 (𝑣,1) = 𝐿𝑖𝑛𝑒𝑎𝑟𝑖𝑧𝑒 [𝜎 (1),𝑊 (𝑎,1) (cid:174)𝑣 (𝑎,0) ] (𝑊 (𝑣,1) (cid:174)𝑣 (𝑣,0) ). This is by
definition an linear function of 𝑊 (𝑣,1) (cid:174)𝑣 (𝑣,0) , which is in turn
an linear function of 𝑊 (𝑣,1) .

Now, consider any 𝑖 > 1. We have by definition (cid:174)𝑣 (𝑣,𝑖) =
𝐿𝑖𝑛𝑒𝑎𝑟𝑖𝑧𝑒 [𝜎 (𝑖),𝑊 (𝑎,𝑖) (cid:174)𝑣 (𝑎,𝑖−1) ] (𝑊 (𝑣,𝑖) (cid:174)𝑣 (𝑣,𝑖−1) ), which, because
we are fixing𝑊 (𝑣,𝑖) for 𝑖 > 1, is an linear function with respect
to (cid:174)𝑣 (𝑣,𝑖−1) .

We showed that (cid:174)𝑣 (1) is linear with respect to 𝑊 (𝑣,1) , while
(cid:174)𝑣 (𝑣,𝑖) for 𝑖 > 1 is linear with respect to 𝑣 (𝑣,𝑖−1) . But composi-
tions of linear functions are also linear, hence in total (cid:174)𝑣 (𝑣,𝑛)
is linear with respect to 𝑊 (𝑣,1) as claimed.
□

Our final result proves that modifying only the value

weights in a DDNN does not change its linear regions.
Theorem 4.6. Let 𝑁 be a PWL DNN with layers (𝑊 (𝑖), 𝜎 (𝑖) )
and define a DDNN 𝑀 with layers (𝑊 (𝑖),𝑊 (𝑣,𝑖), 𝜎 (𝑖) ). Then,
within any linear region in 𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠 (𝑁 ), 𝑀 is also linear.
Proof. Within any linear region of 𝑁 , all of the activations
are the same. This means that all of the linearizations used in
the computation of (cid:174)𝑣 (𝑣,𝑖+1) do not change in the linear region.
Therefore, considering only the value channel, we can write
𝑀 ((cid:174)𝑣) = (cid:174)𝑣 (𝑣,𝑛) as a concatenation of linear functions, which
□
is linear with respect to the input.

Conference’17, July 2017, Washington, DC, USA

Matthew Sotoudeh and Aditya V. Thakur

5 Provable Pointwise Repair
This section defines and gives an algorithm for provable
pointwise repair.

Definition 5.1. Let 𝑁 : R𝑛 → R𝑚. Then (𝑋, 𝐴·, 𝑏 ·) is a
pointwise repair specification if 𝑋 is a finite subset of R𝑛
and for each 𝑥 ∈ 𝑋 , 𝐴𝑥 is a 𝑘𝑥 × 𝑚 matrix while 𝑏𝑥 is a
𝑘𝑥 -dimensional vector.

Definition 5.2. Let 𝑁 : R𝑛 → R𝑚 and (𝑋, 𝐴·, 𝑏 ·) be some
pointwise repair specification. Then 𝑁 satisfies (𝑋, 𝐴·, 𝑏 ·),
written 𝑁 ⊩ (𝑋, 𝐴·, 𝑏 ·), if 𝐴𝑥 𝑁 (𝑥) ≤ 𝑏𝑥 for every 𝑥 ∈ 𝑋 .

Definition 5.3. Let 𝑁 be a DDNN and (𝑋, 𝐴·, 𝑏 ·) be some
pointwise repair specification. Then another DDNN 𝑁 ′ is
a repair of 𝑁 if 𝑁 ′ ⊩ (𝑋, 𝐴·, 𝑏 ·). It is a minimal repair of 𝑁
if |𝜃 ′ − 𝜃 | is minimal among all repairs, where 𝜃 and 𝜃 ′ are
parameters of 𝑁 and 𝑁 ′, respectively, and |·| is some user-
defined measure of size (e.g., ℓ1 norm). It is a minimal layer
repair if it is minimal among all repairs that only modify a
single, given layer.

Assumptions on the DNN. For point repair, we require
only that the activation functions be (almost-everywhere)
differentiable so that we can compute a Jacobian. This is
already the case for every DNN trained via gradient descent,
however even this requirement can be dropped with slight
modification to the algorithm (see Appendix C). Of particular
note, we do not require that the DNN be piecewise-linear.
Algorithm. Algorithm 1 presents our pointwise repair al-
gorithm, which reduces provable pointwise repair to an LP.
𝑝𝑎𝑟𝑎𝑚𝑠 (𝐿) returns the parameters of layer 𝐿. The notation
𝑁 ′(𝑥) refers to the Jacobian of the DDNN 𝑁 ′(𝑥)
𝐷𝑝𝑎𝑟𝑎𝑚𝑠 (𝑁 ′𝑣
𝑖 )
as a function of the parameters 𝑝𝑎𝑟𝑎𝑚𝑠 (𝑁 ′𝑣
𝑖 ) of the 𝑖th value
channel layer, i.e., 𝑊 (𝑣,𝑖) , while fixing input 𝑥. Given a set
of affine constraints 𝐶, 𝑆𝑜𝑙𝑣𝑒 (𝐶) returns a solution to the
set of constraints or ⊥ if the constraints are infeasible. 𝑆𝑜𝑙𝑣𝑒
also guarantees to return the optimal solution according to
some user-defined objective function, e.g., minimizing the
ℓ1 or ℓ∞ norm. Finally, 𝐷𝑒𝑐𝑜𝑢𝑝𝑙𝑒𝑑𝑁 𝑒𝑡𝑤𝑜𝑟𝑘 (𝑎, 𝑣) constructs
a decoupled neural network with activation layers 𝑎 and
value layers 𝑣. The next two theorems show the correctness,
minimality, and running time of Algorithm 1.

Theorem 5.4. Given a DNN 𝑁 , a layer index 𝑖, and a point re-
pair specification (𝑋, 𝐴·, 𝑏 ·), let 𝑁 ′ = PointRepair(𝑁 , 𝑖, 𝑋, 𝐴·, 𝑏 ·).
If 𝑁 ′ ≠ ⊥, then 𝑁 ′ ⊩ (𝑋, 𝐴·, 𝑏 ·) and (cid:174)Δ is a minimal layer re-
pair. Otherwise, if 𝑁 ′ = ⊥, then no such single-layer DDNN
repair satisfying the specification exists for the 𝑖th layer.

Proof. Lines 2–3 construct a DDNN 𝑁 ′ equivalent to the
DNN 𝑁 (Theorem 4.4). For each point 𝑥 ∈ 𝑋 , line 5 consid-
ers the linearization of 𝑁 ′ around the parameters for the value
channel layer 𝑁 ′𝑣
, namely: 𝑁 ′(𝑥; (cid:174)Δ) ≈ 𝑁 ′(𝑥; 0) + 𝐽 𝑥 (cid:174)Δ where
𝑖
𝑁 ′(𝑥; (cid:174)Δ) is the output of the DDNN when the parameters

Algorithm 1: PointRepair(𝑁 , 𝑖, 𝑋, 𝐴·, 𝑏 ·)
Input: A DNN 𝑁 defined by a list of its layers.
A layer index 𝑖 to repair.
A finite set of points 𝑋 .
For each point 𝑥 ∈ 𝑋 a specification 𝐴𝑥 , 𝑏𝑥 asserting
𝐴𝑥 𝑁 ′(𝑥) ≤ 𝑏𝑥 where 𝑁 ′ is the repaired network.
Output: A repaired DDNN 𝑁 ′ or ⊥.
/* 𝐶 is a set of linear constraints on the
parameter delta (cid:174)Δ each of the form (𝐴, 𝑏)
asserting 𝐴 (cid:174)Δ ≤ 𝑏.

1 𝐶 ← ∅

/* Decouple the activation, value layers

2 𝑁 ′𝑎, 𝑁 ′𝑣 = 𝑐𝑜𝑝𝑦 (𝑁 ), 𝑐𝑜𝑝𝑦 (𝑁 )

/* Construct DDNN 𝑁 ′ equivalent to DNN 𝑁

3 𝑁 ′ ← 𝐷𝑒𝑐𝑜𝑢𝑝𝑙𝑒𝑑𝑁 𝑒𝑡𝑤𝑜𝑟𝑘 (𝑁 ′𝑎, 𝑁 ′𝑣)
4 for 𝑥 ∈ 𝑋 do

5

/* Jacobian wrt parameters of layer 𝑁 ′𝑣
𝑖
𝐽 𝑥 ← 𝐷𝑝𝑎𝑟𝑎𝑚𝑠 (𝑁 ′𝑣
𝑁 ′(𝑥)
𝑖 )
/* Encoded constraint 𝐴𝑥 (𝑁 (𝑥) + 𝐽 𝑥 (cid:174)Δ) ≤ 𝑏𝑥
𝐶 ← 𝐶 ∪ {(𝐴𝑥 𝐽 𝑥 , 𝑏𝑥 − 𝐴𝑥 𝑁 (𝑥))}

6
7 (cid:174)Δ ← 𝑆𝑜𝑙𝑣𝑒 (𝐶)
8 if (cid:174)Δ = ⊥ then return ⊥

/* Update value layer 𝑖.
𝑖 ) ← 𝑝𝑎𝑟𝑎𝑚𝑠 (𝑁 ′𝑣

𝑖 ) + (cid:174)Δ

9 𝑝𝑎𝑟𝑎𝑚𝑠 (𝑁 ′𝑣
10 return 𝑁 ′

*/

*/

*/

*/

*/

*/

of the 𝑖th layer are changed by (cid:174)Δ. In particular, by Theo-
rem 4.4 if (cid:174)Δ = 0, 𝑁 ′ is equivalent to 𝑁 (as a function). Hence
𝑁 ′(𝑥; (cid:174)Δ) ≈ 𝑁 (𝑥) + 𝐽 𝑥 (cid:174)Δ. Finally, according to Theorem 4.5,
this linear approximation is exact for the DDNN when we
only modify the parameters for a single value channel layer
𝑁 ′𝑣
𝑖

, i.e., 𝑁 ′(𝑥; (cid:174)Δ) = 𝑁 (𝑥) + 𝐽 𝑥 (cid:174)Δ.
Thus, after the for loop, the set 𝐶 contains constraints
asserting that 𝐴𝑥 𝑁 ′(𝑥; (cid:174)Δ) ≤ 𝑏𝑥 , which are exactly the con-
straints which our algorithm needs to guarantee. Finally, we
solve the constraints for (cid:174)Δ using an LP solver and return the
final DDNN. Hence, if 𝑆𝑜𝑙𝑣𝑒 returns ⊥, there is no satisfying
repair. If it returns a repair, then the LP solver guarantees
that it satisfies the constraints and no smaller (cid:174)Δ exists. □

Theorem 5.5. Algorithm 1 halts in polynomial time with
respect to the size of the point repair specification (𝑋, 𝐴·, 𝑏 ·).

Proof. The LP corresponding to 𝐶 has one row per row of
𝐴𝑥 and one column per weight in 𝑁 ′𝑣
, both of which are
𝑖
included in the size of the input. Thus, as LPs can be solved
□
in polynomial time, the desired result follows.

Notably, the above proof assumes the Jacobian computa-
tion on line 5 takes polynomial time; this is the case for all
common activation functions used in practice. The authors
are not aware of any actual or proposed activation function
that would violate this assumption.

Provable Repair of DNNs

Conference’17, July 2017, Washington, DC, USA

6 Provable Polytope Repair
This section defines and gives an algorithm for provable
polytope repair.
Definition 6.1. Let 𝑁 : R𝑛 → R𝑚. Then (𝑋, 𝐴·, 𝑏 ·) is a
polytope repair specification if 𝑋 is a finite set of bounded
convex polytopes in R𝑛 and for each 𝑃 ∈ 𝑋 , 𝐴𝑃 is a 𝑘𝑃 × 𝑚
matrix while 𝑏𝑃 is a 𝑘𝑃 -dimensional vector.
Definition 6.2. Let 𝑁 : R𝑛 → R𝑚 and (𝑋, 𝐴·, 𝑏 ·) be a poly-
tope repair specification. Then 𝑁 satisfies (𝑋, 𝐴·, 𝑏 ·), written
𝑁 ⊩ (𝑋, 𝐴·, 𝑏 ·), if 𝐴𝑃 𝑁 (𝑥) ≤ 𝑏𝑃 for every 𝑃 ∈ 𝑋 and 𝑥 ∈ 𝑃.

Definition 6.3. Let 𝑁 be a DDNN and (𝑋, 𝐴·, 𝑏 ·) be some
polytope repair specification. Then another DDNN 𝑁 ′ is a
repair of 𝑁 if 𝑁 ′ ⊩ (𝑋, 𝐴·, 𝑏 ·). It is a minimal repair of 𝑁
if |𝜃 ′ − 𝜃 | is minimal among all repairs, where 𝜃 and 𝜃 ′ are
parameters of 𝑁 and 𝑁 ′ respectively, and |·| is some user-
defined measure of size (e.g., ℓ1 norm). It is a minimal layer
repair if it is minimal among all repairs that only modify a
single, given layer.

The quantification in the definition of 𝑁 ⊩ (𝑋, 𝐴·, 𝑏 ·) is
over an infinite set of points 𝑥 ∈ 𝑃. The rest of this section
is dedicated to reducing this infinite quantification to an
equivalent finite one.
Assumptions on the DNN. For polytope repair, we as-
sume that the activation functions used by the DNN are
piecewise-linear (Definition 2.4). This allows us to exactly
reduce polytope repair to point repair, meaning a satisfying
repair exists if and only if the corresponding point repair
problem has a solution. §9 discusses future work on extend-
ing this approach to non-PWL activation functions.
Algorithm. Our polytope repair algorithm is presented
in Algorithm 2. We reduce the polytope specification (𝑋, 𝐴·, 𝑏 ·)
to a provably-equivalent point specification (𝑋 ′, 𝐴′ ·, 𝑏′ ·). For
each polytope in the polytope repair specification, we assert
the same constraints in the point specification except only
on the vertices of the linear regions of 𝑁 on that polytope.
The next two theorems show the correctness, minimality,
and running time of Algorithm 2.

Theorem 6.4. Let 𝑁 ′ = PolytopeRepair(𝑁 , 𝑖, 𝑋, 𝐴·, 𝑏 ·) for
a given DNN 𝑁 , layer index 𝑖, and polytope repair specification
(𝑋, 𝐴·, 𝑏 ·). If 𝑁 ′ ≠ ⊥, then 𝑁 ′ ⊩ (𝑋, 𝐴·, 𝑏 ·) and (cid:174)Δ is a minimal
layer repair. Otherwise, if 𝑁 ′ = ⊥, then no such single-layer
DDNN repair satisfying the specification exists for the 𝑖th layer.

Proof. Consider an arbitrary 𝑃 ∈ 𝑋 and arbitrary linear re-
gion 𝑅 ∈ 𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠 (𝑁 , 𝑃). By Theorem 4.6, linear regions
are the same in the original network 𝑁 and repaired net-
work 𝑁 ′. Hence 𝑅 is also a linear region in 𝑁 ′. Thus, on
𝑅, 𝑁 ′ is equivalent to some linear function. It is a known
result in convex geometry that linear functions map poly-
topes to polytopes and vertices to vertices, i.e., the vertices
of the postimage 𝑁 ′(𝑅) are given by 𝑁 ′(𝑣) for each vertex

Algorithm 2: PolytopeRepair(𝑁 , 𝑖, 𝑋, 𝐴·, 𝑏 ·)
Input: A piecewise-linear DNN 𝑁 with 𝑛 inputs and 𝑚

outputs defined by a list of its layers.

A layer index 𝑖 to repair.
A finite set of polytopes 𝑋 .
For each polytope 𝑃 ∈ 𝑋 a specification 𝐴𝑃 , 𝑏𝑃 asserting
𝐴𝑃 𝑁 ′(𝑥) ≤ 𝑏𝑃 for every 𝑥 ∈ 𝑃 where 𝑁 ′ is the repaired
network.
Output: A repaired DDNN 𝑁 ′ or ⊥.
/* Point repair specification

*/

1 (𝑋 ′, 𝐴′ ·, 𝑏′ ·) ← (∅, ∅, ∅)
2 for 𝑃 ∈ 𝑋 do
3

for 𝑅 ∈ 𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠 (𝑁 , 𝑃) do
for 𝑣 ∈ 𝑉 𝑒𝑟𝑡𝑖𝑐𝑒𝑠 (𝑅) do

𝑋 ′.𝑝𝑢𝑠ℎ(𝑣)
𝐴′𝑣, 𝑏′𝑣 ← 𝐴𝑃 , 𝑏𝑃

4

5

6

7 return PointRepair(𝑁 , 𝑖, 𝑋 ′, 𝐴′ ·, 𝑏′ ·)

𝑣 of 𝑅. The polytope 𝑁 ′(𝑅) is contained in another poly-
tope if and only if its vertices are. Therefore, 𝑁 ′(𝑅) is con-
tained in the polytope defined by 𝐴𝑃, 𝑏𝑃 if and only if its
vertices 𝑁 ′(𝑣) are. Because 𝑃 and 𝑅 were chosen arbitrarily
and contain all of 𝑋 , the constructed point repair specifica-
tion (𝑋 ′, 𝐴′ ·, 𝑏′ ·) is equivalent to the polytope repair speci-
fication (𝑋, 𝐴·, 𝑏 ·). The claimed results then follow directly
□
from Theorem 5.4.

Theorem 6.5. Algorithm 2 halts in polynomial time with
respect to the size of the polytope repair specification (𝑋, 𝐴·, 𝑏 ·)
and the number of vertex points 𝑣.

Proof. This follows directly from the time bounds we have
□
established earlier on PointRepair in Theorem 5.5.

The running time of Algorithm 2 depends on the number
of linear regions and the number of vertices in each region
(line 4). Although in the worst case there are exponentially-
many such linear regions, theoretical results indicate that, for
an 𝑛-dimensional polytope 𝑃 and network 𝑁 with 𝑚 nodes,
we expect |𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠 (𝑁 , 𝑃)| = 𝑂 (𝑚𝑛) [26, 27]. Sotoudeh
and Thakur [55] show efficient computation of one- and
two-dimensional 𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠 for real-world networks.

7 Experimental Evaluation
In this section, we study the efficacy and efficiency of Prov-
able Repair (PR) on three different tasks. The experiments
were designed to answer the following questions:
RQ1 How effective is PR in finding weights that satisfy the

repair specification?

RQ2 How much does PR cause performance drawdown, on

regions of the input space not repaired?

RQ3 How well do repairs generalize to enforce analogous
specifications on the input space not directly repaired?

Conference’17, July 2017, Washington, DC, USA

Matthew Sotoudeh and Aditya V. Thakur

RQ4 How efficient is PR on different networks and dimen-
sionalities, and where is most of the time spent?

Terms used. Buggy network is the DNN before repair, while
the fixed or repaired network is the DNN after repair. Repair
layer is the layer of the network that we applied provable
repair to. Repair set is the pointwise repair specification
or the polytope repair specification used to synthesize the
repaired network. Generalization set is a set of points (or
polytopes) that are disjoint from but simultaneously similar
to the repair specification. Drawdown set is a set of points (or
polytopes) that are disjoint from and not similar to the repair
specification. Efficacy is the percent of the repair set that is
classified correctly by the repaired network, i.e., the accuracy
of the repaired network on the repair set. Our theoretical
guarantees ensure that Provable Repair efficacy is always
100%. Generalization Efficacy is computed by subtracting the
accuracy on the generalization set of the buggy network
from that of the repaired network. Higher generalization
efficacy implies better generalization of the fix. Drawdown
is computed by subtracting the accuracy on the drawdown
set of the repaired network from that of the buggy network.
Lower drawdown is better, implying less forgetting.
Fine-Tuning Baselines. We compare Provable Repair (PR)
to two baselines. The first baseline performs fine-tuning (FT)
using gradient descent on all parameters at once, as proposed
by [53]. FT runs gradient descent until all repair set points
are correctly classified.

The second baseline, modified fine-tuning (MFT), is the
same as FT except (a) MFT fine-tunes only a single layer,
(b) MFT adds a loss term penalizing the ℓ0 and ℓ∞ norms of
the repair, (c) MFT reserves 25% of the repair set as a holdout
set, and (d) it stops once the accuracy on the holdout set
begins to drop. Note that this approach does not achieve full
efficacy; hence, it is not a valid repair algorithm (it does not
repair the DNN). However, because of the early-stopping,
MFT should have lower drawdown.

In all cases PR, FT, and MFT were given the same repair set
(which included a number of non-buggy points). However,
for polytope repair it is necessary to sample from that infinite
repair set to form a finite repair set for FT and MFT, using
the same number of randomly-sampled points as key points
in the PR algorithm.
Evaluation Platform. All experiments were run on an
Intel® Xeon® Silver 4216 CPU @ 2.10GHz. BenchExec [9]
was used to ensure reproducibility and limit the experiment
to 32 cores and 300 GB of memory. The PyTorch framework
was used for performing linear algebra computations [49].
The experiments were run entirely on CPU. We believe that
performance can be improved (i) by utilizing GPUs and (ii) by
using TensorFlow [2], which has explicit support for Jacobian
computations. We used Gurobi [25] to solve the LP problems.
The code to reproduce our experimental results is available
at https://github.com/95616ARG/PRDNN.

7.1 Task 1: Pointwise ImageNet Repair

Buggy network. SqueezeNet [32], a modern ImageNet con-
volutional neural network. We slightly modified the standard
model [1], removing all output nodes except for those of the
9 classes used (see below). The resulting network has 18 lay-
ers, 727,626 parameters, and an accuracy of 93.6% on these
classes using the official ImageNet validation set.
Repair set. The Natural Adversarial Examples (NAE) dataset,
which are images commonly misclassified by modern Ima-
geNet networks [28]. This dataset was also used by Sinitsin
et al. [53]. For our nine classes (chosen alphabetically from
the 200 total in the NAE dataset), the NAE dataset contains
752 color images. The buggy network has an accuracy of
18.6% on these NAE images. To measure scalability of repair,
we ran 4 separate experiments, using subsets of 100, 200, 400,
and all 752 NAE images as the repair specification.
Repair layer. PR and MFT was used to repair each of the 10
feed-forward fully-connected or convolution layers. Table 1
only lists the PR and MFT results for the layer with the best
drawdown (BD).
Generalization set. The NAE images do not have a com-
mon feature that we would like the network to generalize
from the repair. Thus, we were not able to construct a gener-
alization set to evaluate generalization for Task 1.
Drawdown set. The entire set of approximately 500 vali-
dation images for the nine selected classes from the official
ImageNet validation set [13].
Fine-tuning hyperparameters. Both FT and MFT use stan-
dard SGD with a learning rate of 0.0001 and no momentum,
which were chosen as the best parameters after a small man-
ual search. FT[1] and MFT[1] use batch size 2, while FT[2]
and MFT[2] use batch size 16.
RQ1: Efficacy. When using 100, 200, and 400 points, our
Provable Repair algorithm was able to find a satisfying repair
for any layer, i.e., achieving 100% efficacy. For the 752 point
experiment, Provable Repair was able to find a satisfying
repair when run on 7 out of the 10 layers. It timed out on one
of the layers and on the other two was able to prove that no
such repair exists. FT was also able to find a 100%-efficacy
repair in all four cases.

Meanwhile, the MFT baseline had efficacy of at most 28%,
meaning it only marginally improved the network accuracy
on NAE points from the original accuracy of 18%.
RQ2: Drawdown. Table 1 summarizes the drawdown of
Provably Repaired networks on this task. In all cases, PR
was able to find a layer whose repair resulted in under 6%
drawdown. Extended results are in the appendix Table 4. Per-
layer drawdown is shown in Figure 7(a), where we see that
for this task repairing earlier layers can lead to much higher
drawdown while latter layers result in consistently lower
drawdown. This suggests a heuristic for repairing ImageNet
networks, namely focusing on latter layers in the network.

Provable Repair of DNNs

Conference’17, July 2017, Washington, DC, USA

Table 1. Summary of experimental results for Task 1. D: Drawdown (%), T: Time, BD: Best Drawdown, PR: Provable Repair,
FT: Fine-Tuning baseline, MFT: Modified Fine-Tuning baseline (best layer), E: Efficacy (%). Efficacy of PR and FT is always
100%, hence Efficacy (E) numbers are only provided for MFT.

PR (BD)

FT[1]

FT[2]

MFT[1] (BD)

MFT[2] (BD)

Points D

T

D

T

D

T

E

100
200
400
752

3.6
1.1
5.1
5.3

1m39.0s
2m50.8s
4m45.3s
8m28.1s

10.2
9.6
13.8
15.4

4m31.8s
12m19.5s
34m2.6s
1h22m18.7s

8.2
9.6
11.1
13.4

9m24.0s
26m35.0s
1h9m26.8s
2h33m8.2s

24
21.5
21.25
19.4

D

-0.7
-0.4
-0.4
-0.4

T

E

19.2s
13.4s
29.3s
2m35.8s

28
20
21
18.2

D

0.0
-0.4
-0.4
-0.4

T

4m4.9s
2m54.5s
1m32.1s
1m46.7s

By contrast, FT had consistently worse drawdown across
multiple hyperparameter configurations, always above 8%
and in some cases above 15%. This highlights how the guar-
antee of finding a minimal fix using Provable Repair can lead
to significantly more localized fixes, preventing the DNN
from forgetting what it had learned previously as is often a
major risk when fine-tuning.

The MFT baseline had very low drawdown, but this comes

at the cost of low efficacy.
RQ4: Efficiency. Table 1 also shows the amount of time
taken to repair different numbers of points. Even with all 752
points, PR was able to find the repair with the best drawdown
in under 10 minutes. If the layers are repaired in parallel,
then all single-layer repairs can be completed in 4m, 8m, 18m,
and 1h26m for 100, 200, 400, and 752 points respectively. If
the layers are repaired in sequence, all single-layer repairs
can be completed instead in 15m, 31m, 1h7m, and 4h10m re-
spectively. To understand where time was spent, Figure 7(b)
plots the time taken (vertical axis) against the layer fixed
(horizontal axis) for the 400-point experiments. We have
further divided the time spent into (i) time computing pa-
rameter Jacobians, (ii) time taken for the Gurobi optimization
procedure, and (iii) other. For this model we find that a sig-
nificant amount of time is spent computing Jacobians. This
is because PyTorch lacks an optimized method of computing
such Jacobians, and so we resorted to a sub-optimal serialized
approach.

n
w
o
d
w
a
r
D

60

40

20

0

1,000

500

)
s
(

e
m
T

i

1 2 4 6 8 10 12 14 16 18

1 2 4 6 8 10 12 14 16 18

Repaired Layer

Repaired Layer

(a)

(b)

Figure 7. (a) Drawdown and (b) timing per repair layer when
using 400 images in the repair set for Task 1. Blue: Jacobian,
Red: Gurobi, Brown: Other.

By comparison, FT can take significantly longer. For ex-
ample, on the 752-point experiment we saw FT take over
2 hours. In practice, this is highly dependent on the hyper-
parameters chosen, and hyperparameter optimization is a
major bottleneck for FT in practice. The MFT baseline was
also fast, but this comes at the cost of low efficacy.

7.2 Task 2: 1D Polytope MNIST Repair

Buggy network. The MNIST ReLU-3-100 DNN from Singh
[51], consisting of 3 layers and 88, 010 parameters for classi-
fying handwritten digits. This network has an accuracy of
96.5% on the MNIST test set.
Repair set. We ran separate experiments with 10, 25, 50,
or 100 lines; each line was constructed by taking as one
endpoint an uncorrupted MNIST handwritten digit image
and the other endpoint that same MNIST image corrupted
with fog from MNIST-C [46]. If 𝐼 is the uncorrupted image
and 𝐼 ′ is the fog-corrupted image, then this specification
states that all (infinite) points along the line from 𝐼 to 𝐼 ′ must
have the same classification as 𝐼 . The buggy network has an
accuracy of 20.0% on the corrupted endpoints used in the
line repair specification.

Note that, unlike Provable Polytope Repair, both FT and
MFT are given finitely-many points sampled from these lines
— they cannot make any guarantees about other points on
these lines that are not in its sampled set.
Repair layer. We ran two repair experiments, repairing
each of the last two layers. Because the network is fully-
connected, the first layer has a very large number of nodes
(because it is reading directly from the 784-dimensional input
image), leading to a very large number of variables in the
constraints. By comparison, SqueezeNet used convolutional
layers, so the size of the input does not matter.
Generalization set. The MNIST-C fog test set consisting
of 10, 000 fog-corrupted MNIST images as the generalization
set. The accuracy of the buggy network is 19.5% on this
generalization set.
Drawdown set. The drawdown set is the official MNIST
test set, which contains 10, 000 (uncorrupted) MNIST images.
The images in this test set are the exactly the uncorrupted

Conference’17, July 2017, Washington, DC, USA

Matthew Sotoudeh and Aditya V. Thakur

versions of those in the generalization set. The buggy net-
work has 96.5% accuracy on the drawdown set.
Fine-tuning hyperparameters. Both use standard SGD
with a batch size of 16 and momentum 0.9, chosen as the best
parameters after a small manual search. FT[1] and MFT[1]
use a learning rate of 0.05 while FT[2] and MFT[2] use a
learning rate of 0.01.

Table 2 and Table 3 summarize the results for Task 2 when
repairing Layer 2 and Layer 3. The “Lines” column lists the
number of lines in the repair specification. The “Points” col-
umn lists the number of key points in the 𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠, i.e.,
the size of the constructed 𝑋 ′ in Algorithm 2.
RQ1: Efficacy. PR always found a repaired network, i.e.,
one guaranteed to correctly classify all of the infinitely-many
points on each of the lines used in the repair specification.
FT could usually find a repaired network that achieved
100% accuracy on its sampled repair points, but could not
make any guarantee about the infinitely-many other points
in the line specification. Furthermore, in one configuration
FT timed out after getting stuck in a very bad local min-
ima, resulting in a network with near-chance accuracy. This
highlights how extremely sensitive FT is to the choice of
hyperparameters, a major inconvenience when attempting
to apply the technique in practice when compared to our
hyperparameter-free LP formulation.

Meanwhile, MFT was only able to achieve at most 71.3%
efficacy, and like FT this does not ensure anything about the
infinitely-many other points in the specification.
RQ2: Drawdown. Provable Repair results in low draw-
down on this task. Repairing Layer 2 consistently results
in less drawdown, with a drawdown of 2.4% when repairing
using all 100 lines. However, even when repairing Layer 3 the
drawdown is quite low, always under 6%. FT has significantly
worse drawdown, up to 56.0% even when FT terminates suc-
cessfully. This highlights how the Provable Repair guarantee
of finding the minimal repair significantly minimizes for-
getting. Here again, FT is extremely sensitive to hyperpara-
maters, with a different hyperparameter choice often leading
to order of magnitude improvements in drawdown.

MFT achieved low drawdown, but at the cost of worse

generalization and efficacy.
RQ3: Generalization. Provable Repair results in signif-
icant generalization, improving classification accuracy of
fog-corrupted images not part of the repair set, regardless of
the layer repaired. For instance, repairing Layer 3 using 100
lines resulted in generalization of 46%; that is, the accuracy
improved from 19.5% for the buggy network to 65.5% for
the fixed network. In some scenarios FT has slightly bet-
ter generalization, however this comes at the cost of higher
drawdown. Furthermore, Provable Repair tends to have bet-
ter generalization when using fewer lines (i.e., smaller repair
set), which highlights how FT has a tendency to overfit small
training sets. We also note that FT again shows extreme
variability (2–10×) in generalization performance between

different hyperparameter choices, even when it successfully
terminates with a repaired network. MFT had consistently
worse generalization than PR, sometimes by multiple orders
of magnitude.
In addition to the time results in Table 2,
RQ4: Efficiency.
we did a deeper analysis of the time taken by various parts
of the repair process for the 100-line experiment. For Layer 2,
repairing completed in 655.7 seconds, with 1.0 seconds taken
to compute 𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠, 8.0 seconds for computing Jacobians,
623.6 seconds in the LP solver, and 23.1 seconds in other
tasks. For Layer 3, repairing completed in 18.4 seconds, with
1.0 seconds computing 𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠, 1.0 seconds computing
Jacobians, 12.9 seconds in the LP solver, and 3.5 seconds
in other tasks. We find that repairing the lines was quite
efficient, with the majority of the time taken by the Gurobi LP
solver. In contrast, for Task 1 the majority of time was spent
computing Jacobians. This is because we implemented an
optimized Jacobian computation for feed-forward networks.
We see that the time taken to repair depends on the partic-
ular layer that is being repaired. There is little overhead from
our constraint encoding process or computing LinRegions.
Because the majority of the time is spent in the Gurobi LP
solver, our algorithm will benefit greatly from the active
and ongoing research and engineering efforts in producing
significantly faster LP solvers.

FT was also fast. However, again we note that for some
choices of hyperparameters FT gets stuck and cannot find
a repaired network with better-than-chance accuracy. This
highlights how sensitive such gradient-descent-based ap-
proaches are to their hyperparameters, in contrast to our
LP-based formulation that is guaranteed to find the minimal
repair, or prove that none exists, in polynomial time. Finally,
MFT was consistently fast but at the cost of consistently
worse efficacy and generalization.

7.3 Task 3: 2D Polytope ACAS Xu Repair
Buggy network: Task 3 uses the 𝑁2,9 ACAS Xu network [33],
which has 7 layers and 13,350 parameters. The network takes
a five-dimensional representation of the scenario around the
aircraft, and outputs one of five possible advisories.
Repair set. Katz et al. [34] show that 𝑁2,9 violates the
safety property 𝜙8. However, we cannot directly use 𝜙8 as a
polytope repair specification because (i) 𝜙8 concerns a five-
dimensional polytope, and existing techniques for computing
𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠 only scale to two dimensions on ACAS-sized neu-
ral networks, and (ii) 𝜙8 specifies that the output advisory
can be one of two possibilities, a disjunction that cannot
be encoded as an LP. To circumvent reason (i), Task 3 uses
10 randomly-selected two-dimensional planes (slices) that
contain violations to property 𝜙8. To circumvent reason (ii),
Task 3 strengths 𝜙8 based on the existing behavior of the
network. For each key point in the two-dimensional slice,
we compute which of the two possibilities was higher in

Provable Repair of DNNs

Conference’17, July 2017, Washington, DC, USA

Table 2. Summary of experimental results for Task 2. D: Drawdown (%), G: Generalization (%), T: Time, PR: provable repair,
FT: fine-tuning baseline. ∗ means fine-tuning diverged and timed out after 1000 epochs, the results shown are from the last
iteration of fine-tuning before the timeout.

PR (Layer 2)

PR (Layer 3)

FT[1]

Lines

Points

10
25
50
100

1730
4314
8354
16024

D

1.3
1.8
2.6
2.4

G

30.7
35.5
38.3
42.9

T

1m55.1s
2m46.5s
4m29.3s
10m55.7s

D

5.7
5.5
5.9
5.9

G

32.1
38.3
44.5
46.0

T

D

G

T

1.7s
3.7s
8.0s
18.4s

56.0
36.5
85.2∗
31.4

4.2
22.4
-8.2∗
37.7

0.4s
1.2s
29m36.5s∗
3.1s

FT[2]

G

27.5
51.0
55.8
60.0

T

0.6s
0.4s
0.8s
1.6s

D

8.3
3.8
4.7
3.2

Table 3. Summary of modified fine-tuning results for Task 2. E: Efficacy (%), D: Drawdown (%), G: Generalization (%), T: Time,
MFT: modified fine-tuning baseline. Note that the modified fine-tuning does not satisfy all of the hard constraints; therefore, it
is not in fact repairing the network. However, it does result in lower drawdown.

MFT[1] (Layer 2)

MFT[1] (Layer 3)

MFT[2] (Layer 2)

MFT[2] (Layer 3)

Lines

E

10
25
50
100

66.5
67.3
71.3
69.7

D

1.9
0.6
0.6
0.6

G

T

14.3
16.4
17.9
11.9

0.7s
1.0s
1.7s
2.2s

E

60.7
57.4
61.5
63.7

D

0.1
0.3
0.1
0.1

G

3.6
2.4
1.7
2.3

T

0.5s
38.3s
1.6s
2.2s

E

70.3
65.8
70.5
69.8

D

0.5
0.6
0.7
0.4

G

T

E

D

16.8
16.9
17.5
12.9

0.4s
1.0s
1.1s
3.3s

58.4
56.1
59.7
62.7

-0.05
0.03
0.1
0.05

G

1.3
1.0
0.8
0.5

T

0.5s
1.0s
1.6s
5.2s

the buggy network 𝑁2,9, and use the higher one as the de-
sired output advisory. Notably, any network that satisfies
this strengthened property also satisfies property 𝜙8.
Repair layer. We used the last layer as the repair layer. The
other layers were unsatisfiable, i.e., Algorithm 2 returned ⊥.
Generalization set. 5, 466 counterexamples to the safety
property 𝜙8 that were not in the repair set. These coun-
terexamples were found by computing 𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠 on 12
two-dimensional slices randomly selected from 𝑅.
Drawdown set. A similarly randomly-sampled set of 5, 466
points that were correctly classified by the buggy network.
Generalization and drawdown sets have the same size.
Fine-tuning hyperparameters. Both FT and MFT use stan-
dard SGD with learning rate of 0.001, momentum 0.9, and
batch size 16 chosen as best from a small manual search.
RQ1: Efficacy. Provable Polytope Repair was able to prov-
ably repair all 10 two-dimensional slices in the repair set, i.e.,
synthesize a repaired network that satisfies safety property
𝜙8 on all infinitely-many points on the 10 2D repair slices.
By contrast, both FT and MFT had negative efficacy; viz.,
while the original network misclassified only 3 points in the
sampled repair set, the FT-repaired network misclassified 181
points and the MFT-repaired networks misclassified between
10 and 50 points.
RQ2: Drawdown. The drawdown for Provable Repair was
zero: the fixed network correctly classified all 5, 466 points
in the drawdown set. By contrast, FT led to 650 of the 5, 466

points that were originally classified correctly to now be clas-
sified incorrectly. This highlights again how fine-tuning can
often cause forgetting. For all layers, MFT had a drawdown
of less than 1%.
RQ3: Generalization. 5, 176 out of 5, 466 points in the gen-
eralization set were correctly classified in the Provably Re-
paired network; only 290 were incorrectly classified. Recall
that all 5, 466 were incorrectly classified in the buggy net-
work. Thus, the generalization is 94.69%.

FT left only 216 points incorrectly classified, resulting
in a slightly better generalization of 95.8%. However, this
comes at the cost of introducing new bugs into the network
behavior (see Drawdown above) and failing to achieve 100%
efficacy even on the finitely-many repair points it was given.
MFT had a generalization of 100% for the last two layers, and
a generalization of less than 10% for the remaining layers.
It took a total of 21.2 secs. to Provably
RQ4: Efficiency.
Repair the network using the 10 two-dimensional slices;
computing 𝐿𝑖𝑛𝑅𝑒𝑔𝑖𝑜𝑛𝑠 took 1.5 secs.; 1.8 secs. to compute
Jacobians; 7.0 secs. for the Gurobi LP solver; and 10.9 secs. for
other tasks.

FT never fully completed; we timed it out after 1000 epochs
taking 1h18m9.9s. This highlights the importance of our
theoretical guarantees that Provable Repair will either find
the minimal fix or prove that no fix exists in polynomial time.
MFT completed within 3 seconds for all layers.

Conference’17, July 2017, Washington, DC, USA

Matthew Sotoudeh and Aditya V. Thakur

8 Related Work
Closest to this paper is Goldberger et al. [19], which can
be viewed as finding minimal layer-wise fixes for a DNN
given a pointwise specification. However, their algorithm
is exponential time and their underlying formulation is NP-
Complete. By contrast, DDNNs allow us to reduce this repair
problem to an LP. Furthermore, [19] only addresses point-
wise repair, and not provable polytope repair. These issues
are demonstrated in the experimental results; whereas [19] is
able to repair only 3 points even after running for a few days,
we repair entire polytopes (infinitely many points, reduced
to over 150,000 key points) in under thirty seconds for the
same ACAS Xu DNN. Furthermore, reliance on Marabou [35]
means [19] is restricted to PWL activation functions. Our
provable pointwise repair algorithm is applicable to DNNs
using non-PWL activations such as Tanh and Sigmoid.

Kauschke et al. [36] focuses on image-recognition models
under distributional shift, but polytope repair is not consid-
ered. The technique learns a predictor that estimates whether
the original network will misclassify an instance, and a re-
paired network that fixes the misclassification.

Sinitsin et al. [53] proposed editable neural networks, which
train the DNN to be easier to manipulate post-training. Un-
like our approach, their technique does not provide provable
guarantees of efficacy or minimality, and does not support
polytope repair. When the original training dataset is unavail-
able, their approach reduces to the fine-tuning technique we
used as a baseline. Similarly, Tramèr et al. [59] injects adver-
sarial examples into training data to increase robustness.

Robust training techniques [17] apply gradient descent to
an abstract interpretation that computes an over-approximation
of the model’s output set on some polytope input region.
Such approaches have the same sensitivity to hyperparame-
ters as retraining and fine-tuning techniques. Furthermore,
they use coarse approximations designed for pointwise ro-
bustness. These coarse approximations blow up on the larger
input regions considered in our experiments, making the ap-
proach ineffective for repairing such properties.

GaLU networks [16] can be thought of as a variant of
decoupled networks where activations and values get re-
coupled after every layer. Thus, multi-layer GaLU networks
do not satisfy the key theoretical properties of DDNNs.

Alshiekh et al. [3], Zhu et al. [61] ensure safety of re-
inforcement learning controllers by synthesizing a shield
guaranteeing it satisfies a temporal logic property. Bastani
et al. [7] present policy extraction for synthesizing provably
robust decision tree policy for deep reinforcement learning.
Prior work on DNN verification focused on safety and ro-
bustness [4, 6, 11, 15, 18, 31, 34, 35, 52]. More recent research
tackles testing of DNNs [23, 44, 45, 47, 50, 56, 58, 60]. Our
algorithms can fix errors found by such tools.

9 Conclusion and Future Work
We introduced provable repair of DNNs, and presented algo-
rithms for pointwise and polytope repair; the former handles
specifications on finitely-many inputs, and the latter han-
dles a symbolic specification about infinite sets of points.
We introduced Decoupled DNNs, which allowed us to reduce
provable pointwise repair to an LP problem. For the common
class of piecewise-linear DNNs, our polytope repair algo-
rithm can provably reduce the polytope repair problem to a
pointwise repair problem. Our extensive experimental eval-
uation on three different tasks demonstrate that pointwise
and polytope repair are effective, generalize well, display
minimal drawdown, and scale well.

The introduction of provable repairs opens many exciting
directions for future work. We can employ sound approxima-
tions of linearizations to improve performance, and support
non-piecewise-linear activation functions for polytope re-
pair. Repairing multiple layers could be achieved by using
the natural generalization of our LP formulation to a QCQP
[5], or by iteratively applying our LP formulation to different
layers. Future work may repair the activation parameters,
or convert the resulting DDNN back into a standard, feed-
forward DNN while still satisfying the specification (e.g.,
to reduce the small computational overhead of the DDNN).
Exploring learning-theoretic properties of the repair process,
the trade-off between generalization and drawdown during
repair, and heuristics for choosing repair layers, are all very
interesting and important lines of future research to make
provable repair even more useful. Future work could explore
repairing Recurrent Neural Networks (RNNs) using linear
temporal logic specifications. Future work may explore how
to speed up repair using hardware accelerators beyond the
native support provided by PyTorch. Finally, experiment-
ing with different objectives, relaxations, or solving methods
may lead to even more efficient mechanisms for DNN repair.

Acknowledgments
We thank our shepherd Osbert Bastani and the other re-
viewers for their feedback and suggestions. This work is
supported in part by NSF grant CCF-2048123 and a Facebook
Probability and Programming research award.

References
[1] 2019. A collection of pre-trained, state-of-the-art models in the ONNX
format. https://github.com/onnx/models. Accessed: 2019-05-01.
[2] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis,
Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving,
Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry
Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Va-
sudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.
2016. TensorFlow: A System for Large-Scale Machine Learning. In
12th USENIX Symposium on Operating Systems Design and Implemen-
tation (OSDI). https://www.usenix.org/conference/osdi16/technical-
sessions/presentation/abadi

Provable Repair of DNNs

Conference’17, July 2017, Washington, DC, USA

[3] Mohammed Alshiekh, Roderick Bloem, Rüdiger Ehlers, Bettina
Könighofer, Scott Niekum, and Ufuk Topcu. 2018. Safe Reinforce-
ment Learning via Shielding. In Thirty-Second AAAI Conference on
Artificial Intelligence (AAAI-18). https://www.aaai.org/ocs/index.php/
AAAI/AAAI18/paper/view/17211

[4] Greg Anderson, Shankara Pailoor, Isil Dillig, and Swarat Chaudhuri.
2019. Optimization and abstraction: a synergistic approach for an-
alyzing neural network robustness. In 40th ACM SIGPLAN Confer-
ence on Programming Language Design and Implementation (PLDI).
https://doi.org/10.1145/3314221.3314614

[18] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov,
Swarat Chaudhuri, and Martin T. Vechev. 2018. AI2: Safety and Ro-
bustness Certification of Neural Networks with Abstract Interpreta-
tion. In 2018 IEEE Symposium on Security and Privacy (SP). https:
//doi.org/10.1109/SP.2018.00058

[19] Ben Goldberger, Guy Katz, Yossi Adi, and Joseph Keshet. 2020. Minimal
Modifications of Deep Neural Networks using Verification. In 23rd In-
ternational Conference on Logic for Programming, Artificial Intelligence
and Reasoning (LPAR), Vol. 73. https://easychair.org/publications/
paper/CWhF

[5] David P Baron. 1972. Quadratic programming with quadratic con-

[20] Richard Gonzales. 2019.

straints. Naval Research Logistics Quarterly 19, 2 (1972).

[6] Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vy-
tiniotis, Aditya V. Nori, and Antonio Criminisi. 2016. Measuring Neural
Net Robustness with Constraints. In Advances in Neural Information
Processing Systems 29 (NeurIPS). https://proceedings.neurips.cc/paper/
2016/hash/980ecd059122ce2e50136bda65c25e07-Abstract.html
Pu,

Solar-Lezama.
Verifiable Reinforcement Learning via Policy Extrac-
Information Processing Systems 31
https://proceedings.neurips.cc/paper/2018/hash/

2018.
tion. In Advances in Neural
(NeurIPS).
e6d8545daa42d5ced125a4bf747b3688-Abstract.html

[7] Osbert Bastani, Yewen

and Armando

[8] Kathleen Zhou Benjamin Granger, Marta Yu. 2014. Optimization with
absolute values. https://optimization.mccormick.northwestern.edu/
index.php/Optimization_with_absolute_values.

[9] Dirk Beyer. 2016. Reliable and Reproducible Competition Results
with BenchExec and Witnesses (Report on SV-COMP 2016). In 22nd
International Conference on Tools and Algorithms for the Construction
and Analysis of Systems (TACAS). https://doi.org/10.1007/978-3-662-
49674-9_55

[10] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard
Firner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Mon-
fort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, and Karol
Zieba. 2016. End to End Learning for Self-Driving Cars.
(2016).
arXiv:1604.07316 http://arxiv.org/abs/1604.07316

[11] Rudy Bunel, Ilker Turkaslan, Philip H. S. Torr, Pushmeet Kohli, and
Pawan Kumar Mudigonda. 2018. A Unified View of Piecewise Linear
Neural Network Verification. In Advances in Neural Information Pro-
cessing Systems 31 (NeurIPS). https://proceedings.neurips.cc/paper/
2018/hash/be53d253d6bc3258a8160556dda3e9b2-Abstract.html
[12] Leonardo Mendonça de Moura and Nikolaj Bjørner. 2008. Z3: An
Efficient SMT Solver. In 14th International Conference on Tools and
Algorithms for the Construction and Analysis of Systems (TACAS). https:
//doi.org/10.1007/978-3-540-78800-3_24

[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li.
2009. ImageNet: A large-scale hierarchical image database. In 2009
IEEE Computer Society Conference on Computer Vision and Pattern
Recognition (CVPR). https://doi.org/10.1109/CVPR.2009.5206848
[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
2019. BERT: Pre-training of Deep Bidirectional Transformers for Lan-
guage Understanding. In 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics: Human Language
Technologies (NAACL-HLT). https://doi.org/10.18653/v1/n19-1423
[15] Rüdiger Ehlers. 2017. Formal Verification of Piece-Wise Linear Feed-
Forward Neural Networks. In 15th International Symposium on Au-
tomated Technology for Verification and Analysis (ATVA).
https:
//doi.org/10.1007/978-3-319-68167-2_19

[16] Jonathan Fiat, Eran Malach, and Shai Shalev-Shwartz. 2019. De-
arXiv:1906.05032 http:

(2019).

coupling Gating from Linearity.
//arxiv.org/abs/1906.05032

[17] Marc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Timon Gehr,
Ce Zhang, and Martin T. Vechev. 2019. DL2: Training and Query-
ing Neural Networks with Logic. In 36th International Conference on
Machine Learning (ICML) (Proceedings of Machine Learning Research,
Vol. 97). http://proceedings.mlr.press/v97/fischer19a.html

Feds Say Self-Driving Uber SUV Did
NPR

Not Recognize Jaywalking Pedestrian In Fatal Crash.
https://www.npr.org/2019/11/07/777438412/feds-say-self-driving-
uber-suv-did-not-recognize-jaywalking-pedestrian-in-fatal-.
Accessed: 2020-06-06.

[21] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep

Learning. MIT Press. http://www.deeplearningbook.org.

[22] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Ex-
plaining and Harnessing Adversarial Examples. In 3rd International
Conference on Learning Representations (ICLR). http://arxiv.org/abs/
1412.6572

[23] Divya Gopinath, Mengshi Zhang, Kaiyuan Wang, Ismet Burak Kadron,
Corina S. Pasareanu, and Sarfraz Khurshid. 2019. Symbolic Execution
for Importance Analysis and Adversarial Generation in Neural Net-
works. In 30th IEEE International Symposium on Software Reliability
Engineering, (ISSRE). https://doi.org/10.1109/ISSRE.2019.00039
[24] Ronald L. Graham, Donald E. Knuth, and Oren Patashnik. 1994.
Concrete Mathematics: A Foundation for Computer Science, 2nd Ed.
https://www-cs-faculty.stanford.edu/%7Eknuth/gkp.html

[25] LLC Gurobi Optimization. 2020. Gurobi Optimizer Reference Manual.

http://www.gurobi.com.

[26] Boris Hanin and David Rolnick. 2019. Complexity of Linear Regions in
Deep Networks. In 36th International Conference on Machine Learning
(ICML) (Proceedings of Machine Learning Research, Vol. 97).
http:
//proceedings.mlr.press/v97/hanin19a.html

[27] Boris Hanin and David Rolnick. 2019. Deep ReLU Networks Have Sur-
prisingly Few Activation Patterns. In Advances in Neural Information
Processing Systems 32 (NeurIPS). https://proceedings.neurips.cc/paper/
2019/hash/9766527f2b5d3e95d4a733fcfb77bd7e-Abstract.html
[28] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt,
(2019).

and Dawn Song. 2019. Natural Adversarial Examples.
arXiv:1907.07174 http://arxiv.org/abs/1907.07174

[29] Alex Hern. 2017. Facebook translates ’good morning’ into ’attack
them’, leading to arrest. https://www.theguardian.com/technology/
2017/oct/24/facebook-palestine-israel-translates-good-morning-
attack-them-arrest. Accessed: 2020-06-06.

[30] Kashmir Hill. 2020. Wrongfully Accused by an Algorithm. New
York Times. https://www.nytimes.com/2020/06/24/technology/facial-
recognition-arrest.html. Accessed: 2020-06-06.

[31] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. 2017.
Safety Verification of Deep Neural Networks. In 29th International
Conference on Computer Aided Verification (CAV). https://doi.org/10.
1007/978-3-319-63387-9_1

[32] Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han,
William J. Dally, and Kurt Keutzer. 2016. SqueezeNet: AlexNet-level
accuracy with 50x fewer parameters and <1MB model size. (2016).
arXiv:1602.07360 http://arxiv.org/abs/1602.07360

[33] Kyle D. Julian, Mykel J. Kochenderfer, and Michael P. Owen. 2018.
Deep Neural Network Compression for Aircraft Collision Avoidance
Systems. (2018). arXiv:1810.04240 http://arxiv.org/abs/1810.04240
[34] Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J.
Kochenderfer. 2017. Reluplex: An Efficient SMT Solver for Verifying
Deep Neural Networks. In 29th International Conference on Computer
Aided Verification (CAV). https://doi.org/10.1007/978-3-319-63387-9_5

Conference’17, July 2017, Washington, DC, USA

Matthew Sotoudeh and Aditya V. Thakur

[35] Guy Katz, Derek A. Huang, Duligur Ibeling, Kyle Julian, Christopher
Lazarus, Rachel Lim, Parth Shah, Shantanu Thakoor, Haoze Wu, Alek-
sandar Zeljic, David L. Dill, Mykel J. Kochenderfer, and Clark W. Bar-
rett. 2019. The Marabou Framework for Verification and Analysis of
Deep Neural Networks. In 31st International Conference on Computer
Aided Verification (CAV). https://doi.org/10.1007/978-3-030-25540-
4_26

[36] Sebastian Kauschke, David Hermann Lehmann, and Johannes
Fürnkranz. 2019. Patching Deep Neural Networks for Nonstationary
Environments. In International Joint Conference on Neural Networks
(IJCNN). https://doi.org/10.1109/IJCNN.2019.8852222

[37] Ronald Kemker, Marc McClure, Angelina Abitino, Tyler L. Hayes,
and Christopher Kanan. 2018. Measuring Catastrophic Forgetting in
Neural Networks. In Proceedings of the Thirty-Second AAAI Conference
on Artificial Intelligence (AAAI-18). https://www.aaai.org/ocs/index.
php/AAAI/AAAI18/paper/view/16410

[38] Daniel S. Kermany, Michael Goldbaum, Wenjia Cai, Carolina C.S. Valen-
tim, Huiying Liang, Sally L. Baxter, Alex McKeown, Ge Yang, Xiaokang
Wu, Fangbing Yan, Justin Dong, Made K. Prasadha, Jacqueline Pei,
Magdalene Y.L. Ting, Jie Zhu, Christina Li, Sierra Hewett, Jason Dong,
Ian Ziyar, Alexander Shi, Runze Zhang, Lianghong Zheng, Rui Hou,
William Shi, Xin Fu, Yaou Duan, Viet A.N. Huu, Cindy Wen, Edward D.
Zhang, Charlotte L. Zhang, Oulan Li, Xiaobo Wang, Michael A. Singer,
Xiaodong Sun, Jie Xu, Ali Tafreshi, M. Anthony Lewis, Huimin Xia,
and Kang Zhang. 2018.
Identifying Medical Diagnoses and Treat-
able Diseases by Image-Based Deep Learning. Cell 172, 5 (2018).
https://doi.org/10.1016/j.cell.2018.02.010

[39] Leonid Genrikhovich Khachiyan. 1979. A polynomial algorithm in
linear programming. In Doklady Akademii Nauk, Vol. 244. Russian
Academy of Sciences.

[40] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2017. Ima-
geNet classification with deep convolutional neural networks. Com-
mun. ACM 60, 6 (2017). https://doi.org/10.1145/3065386

[41] Yann LeCun, Corinna Cortes, and CJ Burges. 2010. MNIST handwritten

digit database. (2010). http://yann.lecun.com/exdb/mnist

[42] Dave Lee. 2016. US opens investigation into Tesla after fatal crash.
BBC. https://www.bbc.co.uk/news/technology-36680043. Accessed:
2020-06-06.

[43] Lynn Harold Loomis and Shlomo Sternberg. 1968. Advanced calculus.

World Scientific.

[44] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li,
Chunyang Chen, Ting Su, Li Li, Yang Liu, Jianjun Zhao, and Yadong
Wang. 2018. DeepGauge: multi-granularity testing criteria for deep
learning systems. In 33rd ACM/IEEE International Conference on Auto-
mated Software Engineering (ASE). https://doi.org/10.1145/3238147.
3238202

[45] Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu,
Chao Xie, Li Li, Yang Liu, Jianjun Zhao, and Yadong Wang. 2018. Deep-
Mutation: Mutation Testing of Deep Learning Systems. In 29th IEEE
International Symposium on Software Reliability Engineering (ISSRE).
https://doi.org/10.1109/ISSRE.2018.00021

[46] Norman Mu and Justin Gilmer. 2019. MNIST-C: A Robustness Bench-
mark for Computer Vision. (2019). arXiv:1906.02337 http://arxiv.org/
abs/1906.02337

[47] Augustus Odena, Catherine Olsson, David G. Andersen, and Ian J.
Goodfellow. 2019. TensorFuzz: Debugging Neural Networks with
Coverage-Guided Fuzzing. In 36th International Conference on Machine
Learning (ICML). http://proceedings.mlr.press/v97/odena19a.html

[48] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward
Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga,

and Adam Lerer. 2017. Automatic differentiation in PyTorch. (2017).
[49] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia
Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chil-
amkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chin-
tala. 2019. PyTorch: An Imperative Style, High-Performance Deep
Learning Library. In Advances in Neural Information Processing Sys-
tems 32 (NeurIPS). https://proceedings.neurips.cc/paper/2019/hash/
bdbca288fee7f92f2bfa9f7012727740-Abstract.html

[50] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. Deep-
Xplore: Automated Whitebox Testing of Deep Learning Systems.
In 26th Symposium on Operating Systems Principles (SOSP). https:
//doi.org/10.1145/3132747.3132785

[51] Gagandeep Singh. 2019. ETH Robustness Analyzer for Neural Net-
works (ERAN). https://github.com/eth-sri/eran. Accessed: 2019-05-01.
[52] Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin T. Vechev.
2019. An abstract domain for certifying neural networks. Proc. ACM
Program. Lang. 3, POPL (2019). https://doi.org/10.1145/3290354
[53] Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov,
and Artem Babenko. 2020. Editable Neural Networks. In 8th In-
ternational Conference on Learning Representations (ICLR).
https:
//openreview.net/forum?id=HJedXaEtvS

[54] Matthew Sotoudeh and Aditya V. Thakur. 2019. Computing Linear
Restrictions of Neural Networks. In Advances in Neural Information
Processing Systems 32 (NeurIPS). https://proceedings.neurips.cc/paper/
2019/hash/908075ea2c025c335f4865f7db427062-Abstract.html
[55] Matthew Sotoudeh and Aditya V. Thakur. 2021. SyReNN: A Tool for
Analyzing Deep Neural Networks. In 27th International Conference
on Tools and Algorithms for the Construction and Analysis of Systems
(TACAS). https://doi.org/10.1007/978-3-030-72013-1_15

[56] Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta
Kwiatkowska, and Daniel Kroening. 2018. Concolic testing for deep
neural networks. In 33rd ACM/IEEE International Conference on Auto-
mated Software Engineering (ASE). https://doi.org/10.1145/3238147.
3238172

[57] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,
Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014.
Intrigu-
ing properties of neural networks. In 2nd International Conference on
Learning Representations (ICLR). http://arxiv.org/abs/1312.6199
[58] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. DeepTest:
automated testing of deep-neural-network-driven autonomous cars.
In 40th International Conference on Software Engineering (ICSE). https:
//doi.org/10.1145/3180155.3180220

[59] Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian J. Goodfellow,
Dan Boneh, and Patrick D. McDaniel. 2018. Ensemble Adversarial
Training: Attacks and Defenses. In 6th International Conference on
Learning Representations (ICLR). https://openreview.net/forum?id=
rkZvSe-RZ

[60] Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen, Yang
Liu, Jianjun Zhao, Bo Li, Jianxiong Yin, and Simon See. 2019. Dee-
pHunter: a coverage-guided fuzz testing framework for deep neural
networks. In 28th ACM SIGSOFT International Symposium on Software
Testing and Analysis (ISSTA). https://doi.org/10.1145/3293882.3330579
[61] He Zhu, Zikang Xiong, Stephen Magill, and Suresh Jagannathan. 2019.
An inductive synthesis framework for verifiable reinforcement learn-
ing. In 40th ACM SIGPLAN Conference on Programming Language
Design and Implementation (PLDI). https://doi.org/10.1145/3314221.
3314638

Provable Repair of DNNs

Conference’17, July 2017, Washington, DC, USA

A Linear Approximations of Vector

Functions

This section discusses linear approximations of vector func-
tions. The key definition is Definition A.5, which defines the
Jacobian. An example is given in §A.3. Readers comfortable
with Definition A.5 and §A.3, i.e., taking the Jacobian of a
vector-valued function with respect to a matrix parameter,
are welcome to skip this section.

In this work, we will be using the theory of linear approxi-
mations in two major ways. First, they will be used to define
Decoupled DNNs (§4) where we take the activation function
for a value channel layer to be a linear approximation of
the activation function for the activation channel. Second,
they will be used to investigate how a DDNN’s output varies
as the weights of a particular layer in the value channel
are varied. Our key result (Theorem 4.5) implies that, when
modifying a single layer, the linear approximation is exact,
allowing us to encode the repair problem as an LP.

Our notation is closest to Loomis and Sternberg [43], al-
though the definitions are entirely equivalent to those of-
ten used in undergraduate-level calculus and real analysis
courses. We assume an intuitive familiarity with the concept
of a limit.

A.1 The Scalar Case
Suppose we have some scalar-valued function 𝑓 : R → R. A
central question of calculus is whether 𝑓 can be locally well-
approximated by a linear function around a point 𝑥0. In other
words, we want to find some slope 𝑠 such that 𝑓 (𝑥0 + Δ) ≈
𝑓 (𝑥0) + 𝑠Δ when Δ is close to zero. Note that the right-hand
side is a linear function of Δ. This is not yet a particularly
rigorous statement of what properties we would like this
linear approximation to have. This is due to the imprecise
notion of what we mean by ≈ and ‘close to zero.’ To make our
notion of approximation more precise, we will use the little-
oh notation, [24] as defined below. Intuitively, we say that
𝑓 (𝑥) = 𝑜 (𝑔(𝑥)) as 𝑥 → 𝑥0 if 𝑔(𝑥) is “meaningfully larger”
than 𝑓 (𝑥) close to 𝑥0.
Definition A.1. For functions 𝑓 , 𝑔 we say 𝑓 (𝑥) = 𝑜 (𝑔(𝑥))
as 𝑥 approaches 𝑥0 if lim𝑥→𝑥0

𝑓 (𝑥)
𝑔 (𝑥) = 0.

We will now rephrase our approximation requirement as:
𝑓 (𝑥0 +Δ) = 𝑓 (𝑥0) +𝑠Δ+𝑜 (Δ) as Δ approaches 0. In particular,
this notation means that 𝑓 (𝑥0 + Δ) − (𝑓 (𝑥0) + 𝑠Δ) = 𝑜 (Δ).
Intuitively, the left hand side of this equation is an error
term, i.e., how far away our linear approximation is from
the true value of the function. If our linear approximation is
reasonable, this error term should become arbitrarily small
as we get closer to 𝑥0.

Notably, we do not just want the error term to approach
zero, because then for any continuous function we could sim-
ply use the horizontal line (𝑠 = 0) as the linear approximation.
For example, we generally do not think of 𝑓 (𝑥) = |𝑥 | as be-
ing well-approximated by any line at 𝑥 = 0. Hence, we need

to place some additional requirement on how fast the error
decreases as Δ approaches zero. Because we are attempting
to approximate the function with a linear function, we ex-
pect the error to decrease faster than any other linear-order
function. This is captured by the 𝑜 (Δ) on the right-hand
side. The 𝑜 (Δ) bound can also be motivated by noting that,
if a function is well-approximated around some point by
a line, then that line should be unique. In other words, if
there were some other 𝑡 for which 𝑓 (𝑥0 + Δ) ≈ 𝑓 (𝑥0) + 𝑡 Δ,
then we would like to ensure that 𝑡 = 𝑠. Ensuring the error
term is 𝑜 (Δ) ensures this fact, as subtracting the two gives
(𝑠−𝑡 )Δ
us (𝑠 − 𝑡)Δ = 𝑜 (Δ), i.e.,
Δ → 0, which implies 𝑠 = 𝑡.
Duly motivated, and noting that this 𝑠 if it exists is unique,
we can formally define the scalar derivative:
Definition A.2. Suppose 𝑓 : R → R and 𝑥0 ∈ R. We say
that 𝑓 is differentiable at 𝑥0 if there exists 𝑠 ∈ R such that
𝑓 (𝑥0 + Δ) = 𝑓 (𝑥0) + 𝑠Δ + 𝑜 (Δ) as Δ approaches zero. In that
case, we say 𝑠 is the derivative of 𝑓 at 𝑥0 and write 𝑓 ′(𝑥0) = 𝑠
or

𝑑 𝑓
𝑑𝑥 (𝑥0) = 𝑠.
In fact, expanding the definition of 𝑜 (Δ) using Defini-
tion A.1, we find that this is equivalent to the statement
limΔ→0
= 𝑠, the familiar calculus definition of a derivative.
Example A.3. Consider the function 𝑓 (𝑥) = 𝑥 2, which we
will show is differentiable at any point 𝑥0 with derivative 2𝑥0.
0 + (2𝑥0)Δ + 𝑜 (Δ).
In particular, we must show (𝑥0 + Δ)2 = 𝑥 2
0 + 2𝑥0Δ + Δ2 =
Expanding the left-hand side gives us 𝑥 2
0 + 2𝑥0Δ +𝑜 (Δ). Two of the terms on either side are entirely
𝑥 2
identical, hence this holds if and only if Δ2 = 𝑜 (Δ), and
Δ2
Δ = limΔ→0 Δ = 0. In fact, more
indeed we have limΔ→0
generally for any 𝑐 > 1 we have Δ𝑐 = 𝑜 (Δ) as Δ → 0.
Thus, we have shown that 𝑓 is differentiable at every 𝑥0 with
derivative 2𝑥0, as expected from calculus.

= 0, or equivalently limΔ→0

𝑓 (𝑥0+Δ)−( 𝑓 (𝑥0)+𝑠Δ)
Δ

𝑓 (𝑥0+Δ)−𝑓 (𝑥0)
Δ

A.2 The Vector Case
: R𝑛 →
We now assume 𝑓 is higher dimensional, i.e., 𝑓
R𝑚, and we ask the same question: is 𝑓 well-approximated
by a linear function around some (cid:174)𝑥0 ∈ R𝑛? The higher-
dimensional analogue to a one-dimensional linear function
is a matrix multiplication followed by a constant addition. In
other words, we would like to find some matrix 𝐽 such that
𝑓 ( (cid:174)𝑥0 + (cid:174)Δ) ≈ 𝑓 ( (cid:174)𝑥0) + 𝐽 (cid:174)Δ when (cid:174)Δ is close to zero. Here again we
run into two issues: what, rigorously, does ≈ and close to zero
mean? Using the insight from the scalar case, we may attempt
to rigorously define this as 𝑓 ( (cid:174)𝑥0 + (cid:174)Δ) = 𝑓 ( (cid:174)𝑥0) + 𝐽 (cid:174)Δ + 𝑜 ( (cid:174)Δ)
as (cid:174)Δ approaches zero. However, expanding the definition of
little-oh, we see this would require us to divide two vectors,
which is not a well-defined operation in general. Instead, we
return to the intuition that we want the error to get closer
to zero significantly faster than (cid:174)Δ does. Here, we need a
notion of ‘close to zero’ for vectors that can be compared

Conference’17, July 2017, Washington, DC, USA

Matthew Sotoudeh and Aditya V. Thakur

via division. This is given by the notion of a norm, defined
below.
Definition A.4. A norm on R𝑛 is any function ∥·∥ : R𝑛 → R
that satisfies: (i) ∥(cid:174)𝑣 ∥ = 0 only if (cid:174)𝑣 = 0, (ii) ∥𝑐 (cid:174)𝑣 ∥ = |𝑐 |∥(cid:174)𝑣 ∥ for
any 𝑐 ∈ R, and (iii) ∥(cid:174)𝑣 + (cid:174)𝑤 ∥ ≤ ∥(cid:174)𝑣 ∥ + ∥ (cid:174)𝑤 ∥.

Intuitively, ∥(cid:174)𝑣 ∥ gives us the distance of (cid:174)𝑣 from the origin.
Many different norms on R𝑛 can be defined, however it can
be shown that they all result in equivalent notions of differ-
entiability and derivative. Two norms are particularly worth
mentioning here (i) the ℓ1 norm defined ∥(cid:174)𝑣 ∥1 = (cid:205)𝑛
𝑖=1|𝑣𝑖 |, and
(ii) the ℓ∞ norm defined ∥(cid:174)𝑣 ∥∞ = max𝑖 |𝑣𝑖 |. These two norms
are particularly useful to us because they can be encoded as
objectives in an LP.

Now that we have a notion of closeness to zero that can
be divided, we can formalize our notion of a differentiable
function in higher dimensions:
Definition A.5. Suppose 𝑓 : R𝑛 → R𝑚 and (cid:174)𝑣0 ∈ R𝑛. We
say that 𝑓 is differentiable at (cid:174)𝑣0 if there exists a matrix 𝐽 such
that 𝑓 ( (cid:174)𝑣0 + (cid:174)Δ) = 𝑓 ( (cid:174)𝑣0) + 𝐽 (cid:174)Δ + 𝑜 (∥ (cid:174)Δ∥) as (cid:174)Δ approaches zero,
where 𝐽 (cid:174)Δ is a matrix-vector multiplication. In that case, we
say 𝐽 is the Jacobian derivative (Jacobian) of 𝑓 at (cid:174)𝑣0 and write
𝐷 (cid:174)𝑣 𝑓 ( (cid:174)𝑣0) = 𝐽 .

Note in the above definition that the error term is a vec-
tor. Denoting it (cid:174)𝐸, the limit corresponding to the little-oh
∥ (cid:174)Δ∥−1 (cid:174)𝐸 = (cid:174)0, i.e.,
definition is actually asserting that lim(cid:174)Δ→(cid:174)0
approaching the zero vector. It can be shown that this is
∥ (cid:174)Δ∥−1∥ (cid:174)𝐸 ∥ = 0.
equivalent to the scalar-valued limit lim(cid:174)Δ→(cid:174)0
By a similar argument to the scalar case, it can be shown
that this 𝐷 (cid:174)𝑣 𝑓 ( (cid:174)𝑣0), if it exists, is unique. In fact, it can also be
shown that the entries of 𝐽 are the partial derivatives of each
of the 𝑚 scalar components of 𝑓 with respect to each of the
𝑛 scalar components of (cid:174)𝑣. Jacobians can be computed auto-
matically using standard automatic differentiation packages
like Paszke et al. [48].

A.3 A Vector Example
Consider the DNN given by the function 𝑓 ((cid:174)𝑣,𝑊 ) = 𝑅𝑒𝐿𝑈 (𝑊 (cid:174)𝑣)
where 𝑊 is a weight matrix with two rows and two columns.
Note that we have written our function 𝑓 as a function of
both (cid:174)𝑣 and 𝑊 , because, in this paper, we will be interested in
(cid:20)1
(cid:21)
how the function behaves as we vary 𝑊 . Now, fix (cid:174)𝑣0 (cid:66)
2

and 𝑊0 (cid:66)

(cid:21)

(cid:20)1
2
3 −4

.

We want to know: is 𝑓 ( (cid:174)𝑣0,𝑊 ), as a function of 𝑊 , dif-
ferentiable at 𝑊0? In fact, it is. To show this, we must pro-
duce some matrix 𝐽 that forms the linear approximation
𝑓 ( (cid:174)𝑣0,𝑊0 + (cid:174)Δ) = 𝑓 ( (cid:174)𝑣0,𝑊0) + 𝐽 (cid:174)Δ + 𝑜 (∥ (cid:174)Δ∥). The reader may
now be concerned, as (cid:174)Δ is a matrix, not a vector. However,
the set of all 2×2 (or generally, 𝑛 ×𝑚) matrices forms a vector
space R2×2 (generally R𝑛×𝑚). We think of 𝐽 as a matrix over

the vector space of matrices. This can be understood using the
fact that R2×2 is isomorphic to R4 (generally R𝑛×𝑚 ≃ R𝑛𝑚).
Hence we can think of 𝑊0 and (cid:174)Δ as flattened versions of
their corresponding matrices, i.e., (cid:174)𝑊0 = (cid:2)1
if
we flatten in row-major order, and similarly for (cid:174)Δ. Then 𝐽 is
a standard matrix with 4 columns (one for every entry in (cid:174)Δ)
and 2 rows (one for every output of 𝑓 ).

3 −4(cid:3)𝑇

2

Using the constructive characterization of the Jacobian

alluded to above, we can find that 𝐽 (cid:66) (cid:104)1
To verify this we consider
(cid:21) (cid:20)1
2

(cid:18) (cid:20)1 + Δ1
3 + Δ3

2 + Δ2
−4 + Δ4

=𝑅𝑒𝐿𝑈

𝑅𝑒𝐿𝑈

2
3 −4

(cid:18) (cid:20)1

(cid:21) (cid:19)

0

2
0

0
0

0
0

(cid:21) (cid:19)

(cid:21) (cid:20)1
2

(cid:105)

works.

+

(cid:20)1
0

2
0

0
0

(cid:21)

0
0










+ 𝑜 ( ∥ (cid:174)Δ ∥)

Δ1


Δ2


Δ3


Δ4


(cid:20)5 + Δ1 + 2Δ2
0

(cid:21)

+

=

which simplifies to 𝑅𝑒𝐿𝑈

𝑜 (∥ (cid:174)Δ∥).

(cid:18) (cid:20) 5 + Δ1 + 2Δ2
−5 + Δ3 + 2Δ4

(cid:21) (cid:19)

In fact, for small enough ∥ (cid:174)Δ∥ there is no error at all in the
approximation. Namely, when Δ1 + 2Δ2 > −5 and Δ3 + 2Δ4 <
5 we have that the first row inside the ReLU is positive and
the second row is negative, so the entire equation becomes

(cid:20)5 + Δ1 + 2Δ2
0

(cid:21)

(cid:20)5 + Δ1 + 2Δ2
0

(cid:21)

=

+ 𝑜 (∥ (cid:174)Δ∥).

for which the error is 0 = 𝑜 (∥ (cid:174)Δ∥) as desired. Therefore,
we say that 𝑓 is differentiable with respect to 𝑊 at (cid:174)𝑣0,𝑊0
with Jacobian 𝐷𝑊 𝑓 ( (cid:174)𝑣0, (cid:174)𝑊0) =
. Note that in
this example the error term was zero for small enough ∥ (cid:174)Δ∥.
This happens more generally only when the activation func-
tions used are piecewise-linear. However, other activation
functions are still differentiable and we can still compute
Jacobians.

(cid:20)1
0

0
0

2
0

0
0

(cid:21)

B Computing Jacobians on Vertices of

Linear Regions

In Algorithm 2 we have ignored a major subtlety in the
reduction. Namely, the vertex points which we will call Algo-
rithm 1 on are points lying on the boundary between linear
regions, and hence are precisely those (almost-nowhere)
points where the network is technically non-differentiable.
To make the algorithm correct, one must be particularly
careful when performing this Jacobian computation. In par-
ticular, we should associate with each vertex 𝑣 in Algorithm 2
the associated linear region 𝑅 which it is representing. Then,
when we repair in Algorithm 1, we should compute the Ja-
cobian using the activation pattern shared by points interior
to linear region 𝑅. In other words, we repair the vertices of a
linear region 𝑅 by assuming that that linear region extends
to its boundaries. Geometrically, we can imagine the surface

Provable Repair of DNNs

Conference’17, July 2017, Washington, DC, USA

of a PWL DNN as a large number of polytopes tiled together,
each having different slopes (normal vectors). When repair-
ing the vertex of a particular linear region, we want to use
the normal vector corresponding to that linear region, not
one of the other adjacent ones. In particular, this means that
the same point may appear multiple times in the final point
repair specification, each time it appears it is instructing to
repair it as if it belongs to a different linear region.

C Non-Differentiable Functions
Interestingly, DDNNs can be extended to non-differentiable
functions. Theorem 4.4 and Theorem 4.5 only require that
the linearization agrees with the actual function at its cen-
ter, not any other property of Definition 4.2. Consequently,
non-differentiable functions can be “linearized” by taking an
arbitrarily-sloped line centered at that point. In fact, this is

what we do in the zero-probability cases where the input to
the ReLU is 0, we arbitrarily (although consistently) pick the
linearization to be the zero line. For polytope repair more
care needs to be taken when deciding how to handle such
points, see Appendix B for details.

As we will see, point repair does not rely on Theorem 4.6,
hence Algorithm 1 works even for non-differentiable acti-
vation functions. However, polytope repair relies on Theo-
rem 4.6 hence on the fact that it is an actual linearization.
However, this is somewhat of a curiosity, as all common
activation functions are differentiable almost-everywhere in
order to apply gradient descent.

D Evaluation Tables (Extended)
Table 4 gives extended results for the ImageNet Task 1 ex-
periments.

Conference’17, July 2017, Washington, DC, USA

Matthew Sotoudeh and Aditya V. Thakur

Table 4. Summary of experimental results for Task 1. Efficacy column gives the number of layers (out of 10) for which a
satisfying repair could be found. ∗Gurobi timed out on one of the layers; for the other two layers, Gurobi was able to prove
that no repair exists. These three cases are not included in the timing numbers for the last row.

Points

Efficacy

Drawdown (%)

Time

FT[1]

FT[2]

Best Worst

Fastest

Fastest

Slowest

Best Drawdown

Drawdown

Time

Drawdown

Time

100
200
400
800

10 / 10
10 / 10
10 / 10
7∗ / 10

3.6
1.1
5.1
5.3

39.0
40.8
51.4
58.8

39.0
31.4
45.0
58.8

46.3s
1m26.8s
3m2.8s
6m7.9s

3m40.4s
7m20.8s
17m55.6s
58m9.7s

1m39.0s
2m50.8s
4m45.3s
8m28.1s

10.2
9.6
13.8
15.4

4m31.8s
12m19.5s
34m2.6s
1h22m18.7s

8.2
9.6
11.1
13.4

9m24.0s
26m35.0s
1h9m26.8s
2h33m8.2s

