2
2
0
2

y
a
M
6
2

]

G
L
.
s
c
[

1
v
8
9
0
3
1
.
5
0
2
2
:
v
i
X
r
a

Optimal Neural Network Approximation of Wasserstein
Gradient Direction via Convex Optimization

Yifei Wang∗1, Peng Chen †2, Mert Pilanci ‡1 and Wuchen Li§3

1Department of Electrical Engineering, Stanford University

2 Oden Institute for Computational Engineering and Sciences, The University of Texas
at Austin

3Department of Mathematics, University of South Carolina

Abstract. The computation of Wasserstein gradient direction is essential for poste-
rior sampling problems and scientiﬁc computing. The approximation of the Wasserstein
gradient with ﬁnite samples requires solving a variational problem. We study the vari-
ational problem in the family of two-layer networks with squared-ReLU activations, to-
wards which we derive a semi-deﬁnite programming (SDP) relaxation. This SDP can
be viewed as an approximation of the Wasserstein gradient in a broader function family
including two-layer networks. By solving the convex SDP, we obtain the optimal ap-
proximation of the Wasserstein gradient direction in this class of functions. Numerical
experiments including PDE-constrained Bayesian inference and parameter estimation in
COVID-19 modeling demonstrate the eﬀectiveness of the proposed method.

1. Introduction

Bayesian inference plays essential roles in learning model parameters from the observa-
tional data with applications in inverse problems, scientiﬁc computing, information science,
and machine learning (Stuart, 2010). The central problem in Bayesian inference is to draw
samples from a posterior distribution, which characterizes the parameter distribution given
data and a prior distribution.

The Wasserstein gradient ﬂow (Otto, 2001; Ambrosio et al., 2005; Junge et al., 2017)
has shown to be eﬀective in drawing samples from a posterior distribution, which have
attracted increasing attention in recent years. For instance, the Wasserstein gradient
ﬂow of Kullback-Leibler (KL) divergence connects to the overdampled Langevin dynam-
ics. The time-discretization of the overdamped Langevin dynamics renders the classical
Langevin Monte Carlo Markov Chain (MCMC) algorithm. In this sense, the computa-
tion of Wasserstein gradient ﬂow yields a diﬀerent viewpoint for sampling algorithms. In
particular, the Wasserstein gradient direction also provides a deterministic update of the
particle system (Carrillo et al., 2021b). Based on the approximation or generalization
of the Wasserstein gradient direction, many eﬃcient sampling algorithms have been de-
veloped, including Wasserstein gradient descent (WGD) with kernel density estimation
(KDE) (Liu et al., 2019), Stein variational gradient descent (SVGD) (Liu & Wang, 2016),
and neural variational gradient descent (di Langosco et al., 2021), etc.

∗wangyf18@stanford.edu
†
peng@oden.utexas.edu
‡
pilanci@stanford.edu
§
wuchen@mailbox.sc.edu

1

 
 
 
 
 
 
2

Meanwhile, neural networks exhibit tremendous optimization and generalization perfor-
mance in learning complicated functions from data. They also have wide applications in
Bayesian inverse problems (Rezende & Mohamed, 2015; Onken et al., 2020; Kruse et al.,
2019; Lan et al., 2021). According to the universal approximation theorem of neural net-
works (Hornik et al., 1989; Lu et al., 2017), any arbitrarily complicated functions can be
learned by a two-layer neural network with non-linear activations and a suﬃcient number
of neurons. Functions represented by neural networks naturally provide an approximation
towards the Wasserstein gradient direction.

However, due to the nonlinear and nonconvex structure of neural networks, optimization
algorithms including stochastic gradient descent may not ﬁnd the global optima of the
training problem. Recently, based on a line of works (Pilanci & Ergen, 2020; Sahiner
et al., 2020; Bartan & Pilanci, 2021), the regularized training problem of two-layer neural
networks with ReLU/polynomial activation can be formulated as a convex program. The
optimal solution of the convex program renders a global optimum of the nonconvex training
problem.

In this paper, we study a variational problem, whose optimal solution corresponds to
the Wasserstein gradient direction. Focusing on the family of two-layer neural networks
with squared ReLU activation, we formulate the regularized variational problem in terms
of samples. Directly training the neural network to miminize the loss may get the neural
network stuck at local minima or saddle points and it often leads to biased sample distri-
bution from the posterior. Instead, we analyze the convex dual problem of the training
problem and study its semi-deﬁnite program (SDP) relaxation by analyzing the geometry
of dual constraints. The resulting SDP is practically solvable and it can be eﬃciently
optimized by convex optimization solvers such as CVXPY (Diamond & Boyd, 2016). We
then derive the corresponding relaxed bidual problem (dual of the relaxed dual problem).
Thus, the optimal solution of the dual problem yields an optimal approximation of the
Wasserstein gradient direction in a broader function family. We also present practical
implementation and analyze the choice of the regularization parameter. Numerical re-
sults including PDE-constrained inference problems and Covid-19 parameter estimation
problems illustrate the eﬀectiveness of our proposed method.

1.1. Related works. The time and spatial discretizations of Wasserstein gradient ﬂows
are extensively studied in literature (Jordan et al., 1998; Junge et al., 2017; Carrillo et al.,
2021a,b; Bonet et al., 2021; Liutkus et al., 2019; Frogner & Poggio, 2020). Recently,
neural networks have been applied in solving or approximating Wasserstein gradient ﬂows
(Mokrov et al., 2021; Lin et al., 2021b,a; Alvarez-Melis et al., 2021; Bunne et al., 2021;
Hwang et al., 2021; Fan et al., 2021). For sampling algorithms, di Langosco et al. (2021)
learns the transportation function by solving an unregularized variational problem in the
family of vector-output deep neural networks. Compared to these studies, we focus on
a convex SDP relaxation of the varitional problem induced by the Wasserstein gradient
direction. Meanwhile, Feng et al. (2021) form the Wasserstein gradient direction as the
mininimizer the Bregman score and they apply deep neural networks to solve the induced
variational problem.

2. Background

3

In this section, we brieﬂy review the Wasserstein gradient descent, and present its vari-
ational formulation. In particular, we focus on the Wasserstein gradient descent direction
of KL divergence functional. Later on, we design a neural network convex optimization
problems to approximate Wasserstein gradient in samples.

2.1. Wasserstein gradient descent. Consider an optimization problem in the proba-
bility space:

inf
ρ∈P

DKL(ρ(cid:107)π) =

(cid:90)

ρ(log ρ − log π)dx,

(1)

Here the integral is taken over Rd and the objective functional DKL(ρ(cid:107)π) is the KL di-
vergence from ρ to π. The variable is the density function ρ in the space P = {ρ ∈
C∞(Rd)| (cid:82) ρdx = 1, ρ > 0}. The function π ∈ C∞(Rd) is a known probability density
function of the posterior distribution. By solving the optimization problem (1), we can
generate samples from the posterior distribution.

A known fact (Villani, 2003, Chapter 8.3.1) is that the Wasserstein gradient descent

ﬂow for the optimization problem (1) satisﬁes

(cid:18)

∂tρt =∇ ·

ρt∇

(cid:19)

DKL(ρt(cid:107)π)

δ
δρt

=∇ · (ρt(∇ log ρt − ∇ log π))
=∆ρt − ∇ · (ρt∇ log π),

where ρt(x) = ρ(x, t) and δ
is the L2 ﬁrst variation operator w.r.t. ρt. In above third
δρt
equality, a fact ρt∇ log ρt = ∇ρt is used. Here ∇ · F denotes the divergence of a vector
valued function F : Rd → Rd and ∆ is the Laplace operator. This equation is also known
as the gradient drift Fokker-Planck equation. It corresponds to the following updates in
terms of samples:

dxt = −(∇ log ρt(xt) − ∇ log π(xt))dt,
(2)
where xt follows the distribution of ρt. Clearly, when ρt = π, the above dynamics reach the
equilibrium, which implies that the samples xt are generated by the posterior distribution.

To solve the Wasserstein gradient ﬂow (2), we consider a forward Eulerian discretization
l } are samples drawn from ρl. The update

in time. In the l-th iteration, suppose that {xn
rule of Wasserstein gradient descent (WGD) on the particle system {xn

l } follows

xn
l+1 = xn
where Φl : Rd → R is a function which approximates log ρl − log π.

l − αl∇Φl(xn

l ),

(3)

2.2. Variational formulation of WGD. Given the particles {xn}N
n=1, we design the
following variational problem to choose a suitable function Φ approximating the function
log ρ − log π. Consider

inf
Φ∈C1(Rd)

(cid:90)

1
2

(cid:107)∇Φ − (∇ log ρ − ∇ log π)(cid:107)2

2ρdx.

(4)

The objective functional evaluates the least-square discrepancy between ∇ log ρ − ∇ log π
and ∇Φ weighted by the density ρ. The optimal solution follows Φ = log ρ − log π, up to

4

a constant shift. Let H ⊆ C1(Rd) be a ﬁnite dimensional function space. The following
proposition gives a formulation of (4) in H.

Proposition 1. Let H ⊆ C1(Rd) be a function space. The variational problem (4) in the
domain H is equivalent to

(cid:90)

1
2

inf
Φ∈H

(cid:107)∇Φ(cid:107)2

2ρdx +

(cid:90)

(cid:90)

∆Φρdx +

(cid:104)∇ log π, ∇Φ(cid:105) ρdx.

(5)

Remark 1. A similar variational problem has been studied in (di Langosco et al., 2021).
If we replace ∇Φ for Φ ∈ H by a vector ﬁeld Ψ in certain function family, then, the
quantity in (5) is the negative regularized Stein discrepancy deﬁned in (di Langosco et al.,
2021) between ρ and π based on Ψ.

Therefore, by replacing the density ρ by ﬁnite samples {xn}N

n=1 ∼ ρ, the problem (5)

in terms of ﬁnite samples forms

inf
Φ∈H

1
N

N
(cid:88)

n=1

(cid:18) 1
2

(cid:107)∇Φ(xn)(cid:107)2

2 + ∆Φ(xn)

(cid:19)

+

1
N

N
(cid:88)

n=1

(cid:104)∇ log π(xn), ∇Φ(xn)(cid:105) .

(6)

3. Optimal neural network approximation of Wasserstein gradient

In this section, we focus on functional space H of functions represented by two-layer
neural networks. We derive the primal and dual problem of the regularized Wasserstein
variational problems. By analyzing the dual constraints, a convex SDP relaxation of
the dual problem is obtained. We also present a practical implementation estimation of
∇ log ρ − ∇ log π and discuss the choice of the regularization parameter.

Let ψ be an activation function. Consider the case where H is a class of two-layer neural

network with the activation function ψ(x):

H =

(cid:110)

Φθ ∈ C1(Rd)|Φθ(x) = αT ψ(W T x)

(cid:111)

,

(7)

where θ = (W, α) is the parameter in the neural network with W ∈ Rd×m and α ∈ Rm.

Remark 2. We can extend this model to handle the bias term by add an entry of 1 in
x1, . . . , xn.

For two-layer neural networks, we can compute the gradient and Laplacian of Φ ∈ H as

follows:

∇Φθ(x) =

m
(cid:88)

i=1

αiwiψ(cid:48)(wT

i x) = W (ψ(cid:48)(W T x) ◦ α),

∆Φθ(x) =

m
(cid:88)

i=1

αi(cid:107)wi(cid:107)2

2ψ(cid:48)(cid:48)(wT

i x).

(8)

(9)

5

Here ◦ represents the element-wise multiplication. By adding a regularization term to the
variational problem (6), we obtain
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
N
(cid:88)

(cid:13)
2
(cid:13)
(cid:13)
i xn)
(cid:13)
(cid:13)
2

i xn), ∇ log π(xn)

αiwiψ(cid:48)(wT

αiwiψ(cid:48)(wT

(cid:42) m
(cid:88)

1
2N

min
θ

N
(cid:88)

N
(cid:88)

m
(cid:88)

m
(cid:88)

1
N

(10)

n=1

n=1

i=1

i=1

(cid:43)

+

+

1
N

n=1

i=1

αi(cid:107)wi(cid:107)2

2ψ(cid:48)(cid:48)(wT

i xn) +

R(θ),

β
2

where β > 0 is the regularization parameter. We focus on the squared ReLU activation
+ = (max{z, 0})2. Note that a non-vanishing second derivative is required
ψ(z) = (z)2
for the Laplacian term in (9), which makes the ReLU activation inadequate. For this
activation function, we consider the following regularization function

R(θ) =

m
(cid:88)

i=1

((cid:107)wi(cid:107)3

2 + |αi|3).

(11)

Remark 3. We note that ∇Φθ(x) and ∆Φθ(x) are all piece-wise degree-3 polynomials
of the parameters θ. Hence, we consider a speciﬁc cubic regularization term (11), analo-
gous to (Bartan & Pilanci, 2021). By choosing this regularization term, we can derive a
simpliﬁed convex dual problem.

By rescaling the ﬁrst and second-layer parameters, the regularized variational problem

(10) can be formulated as follows.

Proposition 2 (Primal problem). The regularized variational problem (10) is equivalent
to

min
W,α

1
2

N
(cid:88)

n=1

N
(cid:88)

+

m
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
i=1
(cid:42) m
(cid:88)

αiwiψ(cid:48)(wT

(cid:13)
2
(cid:13)
(cid:13)
i xn)
(cid:13)
(cid:13)

N
(cid:88)

m
(cid:88)

+

n=1

i=1

αi(cid:107)wi(cid:107)2

2ψ(cid:48)(cid:48)(wT

i xn)

αiwiψ(cid:48)(wT

i xn), ∇ log π(xn)

+ ˜β(cid:107)α(cid:107)1,

(cid:43)

(12)

n=1

i=1

s.t. (cid:107)wi(cid:107)2 ≤ 1, i ∈ [m],

where ˜β = 3 · 2−5/3N β.

For simplicity, we write Y =

zn = (cid:80)m
can simplify the problem (12) to

i=1 αiwiψ(cid:48)(xT



∇ log π(x1)T
...
∇ log π(xN )T
n wi) for n ∈ [N ] and denote Z = (cid:2)z1





 ∈ RN ×d. We introduce the slack variable


. . . zN

(cid:3)T ∈ RN ×d. Then, we

min
W,α,Z

1
2

(cid:107)Z(cid:107)2

F +

N
(cid:88)

m
(cid:88)

n=1

i=1

αi(cid:107)wi(cid:107)2

2ψ(cid:48)(cid:48)(wT

i xn) + tr(Y T Z) + ˜β(cid:107)α(cid:107)1,

s.t. zn =

m
(cid:88)

i=1

αiwiψ(cid:48)(xT

n wi), n ∈ [N ], (cid:107)wi(cid:107)2 ≤ 1, i ∈ [m].

(13)

Based on the above reformulation, we can derive the dual problem of (13) as follows.

6

Proposition 3 (Dual problem). The dual problem of the regularized variational problem
(13) is

max
Λ∈RN ×d

−

1
2

(cid:107)Λ + Y (cid:107)2

F , s.t. max

w:(cid:107)w(cid:107)2≤1

N
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
n=1

(cid:107)w(cid:107)2

2ψ(cid:48)(cid:48)(xT

n w) − λT

n wψ(cid:48)(xT

n w)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ ˜β,

(14)

which provides a lower-bound on (13).

3.1. Analysis of dual constraints and the relaxed dual problem. Now, we analyze
the constraint

max
w:(cid:107)w(cid:107)2≤1

N
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
n=1

(cid:107)w(cid:107)2

2ψ(cid:48)(cid:48)(wT xn) − λT

n wψ(cid:48)(xT

≤ ˜β

(cid:12)
(cid:12)
(cid:12)
n w)
(cid:12)
(cid:12)

in the dual problem. We note that this constraint is closely related to the regularization
parameter, which we will discuss later. For simplicity, we take ψ(cid:48)(cid:48)(0) = 0 as the subgradient
of ψ(cid:48)(z) at z = 0, i.e., taking the left derivative of ψ(cid:48)(z) at z = 0. Let X = [x1, . . . , xN ]T ∈
RN ×d. Denote the set of all possible hyper-plane arrangements corresponding to the rows
of X as

S = {D = diag(I(Xw ≥ 0))|w ∈ Rd, w (cid:54)= 0}.
(15)
Here I(s) = 1 if the statement s is correct and I(s) = 0 otherwise. Let p = |S| be the
cardinality of S, and write S = {D1, . . . , Dp}. According to (Cover, 1965), we have the
upper bound p ≤ 2r

, where r = rank(X).

(cid:17)r

(cid:16) e(N −1)
r

Based on the analysis of the dual constraints, we can derive a convex SDP as a relaxed

dual problem. It gives a lower bound for the optimal value of the dual problem (14).

Proposition 4 (Relaxed Dual problem). Consider the following SDP:

max −

1
2

(cid:107)Λ + Y (cid:107)2
F ,

s.t. ˜Aj(Λ) + ˜Bj +

N
(cid:88)

n=0

n H (j)
r(j,−)

n + ˜βed+1eT

d+1 (cid:23) 0,

(16)

− ˜Aj(Λ) − ˜Bj +

N
(cid:88)

n=0

n H (j)
r(j,+)

n + ˜βed+1eT

d+1 (cid:23) 0,

r(j,−) ≥ 0, r(j,+) ≥ 0, j ∈ [p].

The variables are Λ ∈ RN ×d and r(j,−), r(j,+) ∈ Rn+1 for j ∈ [p]. For j ∈ [p], we denote
(cid:21)
0
Aj(Λ) = −ΛT DjX − X T DjΛ, Bj = 2 tr(Dj)Id, ˜Aj(Λ) =
0

(cid:20)Aj(Λ) 0
0
0

(cid:20)Bj
0

, ˜Bj =

(cid:21)

,

H (j)

0 =

(cid:21)

(cid:20)Id
0
0 −1

and H (j)

n =

(cid:20)

0
(1 − 2(Dj)nn)xT
n

(1 − 2(Dj)nn)xn
0

(cid:21)

ed+1 ∈ Rd+1 satisﬁes that (ed+1)i = 0 for i ∈ [d] and (ed+1)d+1 = 1.

, n ∈ [N ] The vector

The optimal value of (16) gives a lower bound on the dual problem (14), and hence on

the primal problem (13).

In the following proposition, we derive the relaxed bi-dual problem. It can be viewed

as a convex relaxation of the primal problem (13).

Proposition 5 (Relaxed bi-dual problem). The dual of the relaxed dual problem (16) is
as follows

min

1
2

(cid:107)Z + Y (cid:107)2

F −

1
2

(cid:107)Y (cid:107)2

F +

p
(cid:88)

j=1

tr( ˜Bj(S(j,+) − S(j,−))) + ˜β

(cid:16)

tr

p
(cid:88)

j=1

(S(j,+) + S(j,−))ed+1eT

d+1

(cid:17)

,

7

s.t. Z =

p
(cid:88)

j=1

˜A∗

j (S(j,−) − S(j,+)), tr(S(j,−)H (j)

n ) ≤ 0, tr(S(j,+)H (j)

n ) ≤ 0, n = 0, . . . , N, j ∈ [p],

in variables Z ∈ RN ×d, S(j,+), S(j,−) ∈ Sd+1
of the linear operator Aj.

+

for j ∈ [p]. Here A∗

(17)
j is the adjoint operator

As (16) is a convex problem and the Slater’s condition is satisﬁed, the optimal values
of (16) and (17) are same. We can show that any feasible solutions of the primal problem
(12) can be mapped to feasible solutions of (17).

Theorem 1. Suppose that (Z, W, α) is feasible to the primal problem (13). Then, there
exist matrices {S(j,+), S(j,−)}p
j=1)
is feasible to the relaxed bi-dual problem (17). Moreover, the objective value of the relaxed
bi-dual problem (17) at (Z, {S(j,+), S(j,−)}p
j=1) is the same as objective value of the primal
problem (13) at (Z, W, α).

j=1 constructed from (W, α) such that (Z, {S(j,+), S(j,−)}p

Let J(Z, {S(j,+), S(j,−)}p

j=1) denote the objective value of the relaxed bi-dual prob-
lem (17) at a feasible solution (Z, {S(j,+), S(j,−)}p
j=1). Let (Z∗, W ∗, α∗) denote a glob-
ally optimal solution of the primal problem (13). By Theorem 1, there exist matrices
{S(j,+), S(j,−)}p
j=1 such that (Z∗, {S(j,+), S(j,−)}p
j=1) is a feasible solution of the relaxed
bi-dual problem (17) and J(Z∗, {S(j,+), S(j,−)}p
j=1) is the same as the objective value of
(13) at its global minimum (Z∗, W ∗, α∗). On the other hand, let ( ˜Z∗, { ˜S(j,+), ˜S(j,−)}p
j=1)
denote an optimal solution of the relaxed bi-dual problem (17). From the optimality of
( ˜Z∗, { ˜S(j,+), ˜S(j,−)}p

j=1), we have

J( ˜Z∗, { ˜S(j,+), ˜S(j,−)}p

j=1) ≤ J(Z∗, {S(j,+), S(j,−)}p

j=1).

(18)

Note that at (Z∗, W ∗, α∗) we obtain the optimal approximation of ∇ log ρ − ∇ log π at
x1, . . . , xN in the family of two-layer squared-ReLU networks (7). Smaller or equal objec-
tive value of the relaxed bi-dual problem (17) can be achieved at ( ˜Z∗, { ˜S(j,+), ˜S(j,−)}p
j=1)
than at (Z∗, {S(j,+), S(j,−)}p
j=1). Therefore, we can view ˜Z∗ gives an optimal approxima-
tion of ∇ log ρ − ∇ log π evaluated on x1, . . . , xN in a broader function family including
the two-layer squared ReLU neural networks.

From the derivation of the relaxed bi-dual problem, we have the relation ˜Z∗ = −Λ∗ − Y ,
where (Λ∗, {r(j,+), r(j,−)) is optimal to the relaxed dual problem (16) and ( ˜Z∗, { ˜S(j,+), ˜S(j,−)}p
is optimal to the relaxed bi-dual problem (17). Therefore, by solving Λ∗ from the relaxed
dual problem (16), we can use −Λ∗ −Y as the approximation of ∇ log ρ−∇ log π evaluated
on x1, . . . , xN .

j=1)

Remark 4. We note that solving the proposed convex optimization problem 16 renders
the approximation of the Wasserstein gradient direction. Compared to the two-layer ReLU

8

networks, it induces a broader class of functions represented by {S(j,+), S(j,−)}p
contains more variables than the neural network function.

j=1. This

3.2. Practical implementation. Although the number p of all possible hyper-plane
arrangements is upper bounded by 2r((N −1)e/r)r with r = rank(X), it is computationally
costly to enumerate all possible p matrices D1, . . . , Dp to represent the constraints in the
relaxed dual problem (4). In practice, we ﬁrst randomly sample M i.i.d. random vectors
u1, . . . , uM ∼ N (0, Id) and generate a subset ˆS of S as follows:

ˆS = {diag(I(Xuj ≥ 0)|j ∈ [M ]}.
Then, we optimize the randomly sub-sampled version of the relaxed dual problem based
on the subset ˆS and obtain the solution Λ. We then use −Λ − Y as the direction to update
the particle system X.

(19)

If the regularization parameter is too large, then we will have −Λ − Y = 0, which makes
the particle system unchanged. Therefore, to ensure that ˜β is not too large, we decay ˜β by
a factor γ1 ∈ (0, 1). This also appears in (Ergen et al., 2021). On the other hand, if ˜β is too
small resulting the relaxed dual problem (4) infeasible, we increase ˜β by multiplying γ−1
2 ,
where γ2 ∈ (0, 1). Detailed explanation of the adjustment of the regularization parameter
can be found in Appendix C. The overall algorithm is summarized in Algorithm 1.

Algorithm 1 Convex neural Wasserstein descent

Require: initial positions {xn

0 }N

n=1, step size αl,

γ1, γ2 ∈ (0, 1).

initial regularization parameter ˜β0,

1: while not converge do
2:

Form Xl and Yl based on {xn
n=1.
Solve Λl from the relaxed dual problem (16) with ˜β = ˜βl.
if the relaxed dual problem with ˜β = ˜βl is infeasible then

n=1 and {∇ log π(xn

l )}N

l }N

Set Xl+1 = Xl for n ∈ [N ] and set ˜βl+1 = γ−1
2

˜βl.

Update Xl+1 = Xl + αl(Λl + Yl) for n ∈ [N ] and set ˜βl+1 = γ1 ˜βl.

3:

else

4:
5:
6:
7:
8:
9: end while

end if

We note that the randomly subsampled version of the relaxed dual problem (16) involves
2N ˆp inequality constraints and 2ˆp linear matrix inequality constraints with size (d + 1) ×
(d + 1). For high-dimensional problems, i.e., d is large, the computational cost of solving
(16) can be large. In this case, we apply the dimension-reduction techniques (Zahm et al.,
2018; Chen & Ghattas, 2020; Wang et al., 2021) to reduce the parameter dimension d to
a data-informed intrinsic dimension ˆd, which is often very low, i.e., ˆd (cid:28) d.

4. Numerical experiments

In this section, we present numerical results to compare WGD approximated by neural
networks (WGD-NN) and WGD approximated using convex optimization formulation of
neural networks (WGD-cvxNN). The performance of the two methods is assessed by the
sample goodness-of-ﬁt of the posterior. For WGD-NN, in each iteration, it updates the
particle system using (3) with a function Φ represented by a two-layer squared ReLU

neural network. The parameters of the neural network is obtained by directly solving
the nonconvex optimization problem (10). We note that it takes longer time by WGD-
cvxNN (compared to WGD-NN and WGD) to solve the convex optimization problem.
However, this optimization time is often dominated by the time in likelihood evaluation
if the model is expensive to solve. Moreover, the induced SDPs have speciﬁc structures
of many similar constraints, whose solution can be accelerated by designing a specialized
convex optimization solver. This is left for future work.

9

4.1. A toy example. We test the performance of WGD on a bimodal 2-dimensional
double-banana posterior distribution introduced in (Detommaso et al., 2018). We ﬁrst
generate 300 posterior samples by a Stein variational Newton (SVN) method (Detommaso
et al., 2018) as the reference, as shown in Figure 1. We evaluate the performance of WGD-
NN and WGD-cvxNN by calculating the maximum mean discrepancy (MMD) between
their samples in each iteration and the reference samples.
In the comparison, we use
N = 50 samples and run for 100 iterations with step sizes αl = 10−3. For WGD-cvxNN,
we set β = 1, γ1 = 0.95 and γ2 = 0.9510. For WGD-NN, we use m = 200 neurons and
optimize the regularized training problem (10) using all samples with the Adam optimizer
(Kingma & Ba, 2014) with learning rate 10−3 for 200 sub-iterations. We also set the
regularization parameter β = 1 and decrease it by a factor of 0.95 in each iteration. We
ﬁnd that this setup of parameters is more suitable.

The posterior density and the sample distributions by WGD-cvxNN and WGD-NN at
the ﬁnal step of 100 iterations are shown in Figure 1. It can be observed that WGD-cvxNN
provides more representative samples than WGD-NN for the posterior density.

Figure 1. Posterior density and sample distributions by WGD-cvxNN
and WGD-NN at the ﬁnal step of 100 iterations, compared to the reference
SVN samples (right).

In Figure 2, we plot the MMD of the samples by WGD-cvxNN and WGD-NN compared
to the reference SVN samples at each iteration. We observe that the samples by WGD-
cvxNN achieves much smaller MMD than those of WGD-NN compared to the reference
SVN samples, which is consistent with the results shown in Figure 1. For WGD-cvxNN,
it takes 572s in total, while for WGD-NN, it takes 16s in total. WGD-cvxNN takes much
longer time than WGD-NN as WGD-cvxNN aims to solve for the global minimum of the
relaxed convex dual problem.

10

Figure 2. MMD of WGD-cvxNN and WGD-NN samples compared to the
reference SVN samples.

4.2. PDE-constrained nonlinear Bayesian inference. In this experiment, we con-
sider a nonlinear Bayesian inference problem constrained by the following partial diﬀer-
ential equation (PDE) with application to subsurface (Darcy) ﬂow in a physical domain
D = (0, 1)2,

v + ex∇u = 0
∇ · v = h

in D,
in D,

(20)

where u is pressure, v is velocity, h is force, ex is a random (permeability) ﬁeld equipped
with a Gaussian prior x ∼ N (x0, C) with covariance operator C = (−δ∆ + γI)−α where
we set δ = 0.1, γ = 1, α = 2 and x0 = 0. We impose Dirichlet boundary conditions u = 1
on the top boundary and u = 0 on the bottom boundary, and homogeneous Neumann
boundary conditions on the left and right boundaries for u. We use a ﬁnite element
method with piecewise linear elements for the discretization of the problem, resulting in
81 dimensions for the discrete parameter. The data is generated as pointwise observation of
the pressure ﬁeld at 49 points equidistantly distributed in (0, 1)2, corrupted with additive
5% Gaussian noise. We use a DILI-MCMC algorithm Cui et al. (2016) with 10000 eﬀective
samples to compute the sample mean and sample variance, which are used as the reference
values to assess the goodness of the samples by pWGD-cvxNN and pWGD-NN.

Figure 3. Ten trials and the RMSE of the sample mean (top) and sample
variance (bottom) by pWGD-NN and pWGD-cvxNN at diﬀerent iterations.
Nonlinear inference problem.

We run pWGD-cvxNN and pWGD-NN with 64 samples for ten trials with step size
αl = 10−3, where we set β = 10, γ1 = 0.95, and γ2 = 0.9510 for both methods. The RMSE
of the sample mean and sample variance are shown in Figure 3 for the two methods at

020406080# iterations1.21.00.80.60.40.20.0Log10(RMSE of mean)pWGD-NNpWGD-cvxNN020406080# iterations0.80.60.40.20.00.20.40.6Log10(RMSE of variance)pWGD-NNpWGD-cvxNN11

each of the iterations. We can observe that pWGD-cvxNN achieves smaller errors for
both the sample mean and the sample variance compared to pWGD-NN at each iteration.
Moreover, pWGD-cvxNN provides much smaller variation of the sample mean and sample
variance for the ten trials compared to pWGD-NN.

4.3. Bayesian inference for COVID-19. In this experiment, we use Bayesian inference
to learn the dynamics of the transmission and severity of COVID-19 from the recorded data
for New York state, as studied in Chen & Ghattas (2020). We use the model, parameter,
and data as in Chen & Ghattas (2020). More speciﬁcally, we use a compartmental model
for the modeling of the transmission and outcome of COVID-19. We take the number
of hospitalized cases as the observation data to infer a social distancing parameter, a
time-dependent stochastic process that is equipped with a Tanh–Gaussian prior to model
the transmission reduction eﬀect of social distancing, which becomes 96 dimensions after
discretization.

We run a projected Stein variational gradient descent (pSVGD) method Chen & Ghattas
(2020) as the reference, and run pWGD-cvxNN and pWGD-NN using 64 samples for 100
iterations with step size αl = 10−3, where we set β = 10, γ1 = 0.95, and γ2 = 0.9510 for
both methods as in the last example. From Figure 4 we can observe that pWGD-cvxNN
produces more consistent results with pSVGD than pWGD-NN for both the sample mean
and 90% credible interval, both in the inference of the social distancing parameter and in
the prediction of the hospitalized cases.

Figure 4. Comparison of pWGD-cvxNN and pWGD-NN to the reference
by pSVGD for Bayesian inference of the social distancing parameter (top)
from the data of the hospitalized cases (bottom) with sample mean and
90% credible interval.

5. Conclusion

In the context of variational Wasserstein gradient descent methods for Bayesian infer-
ence, we consider the approximation of the Wasserstein gradient direction by the gradient
of functions in the family of two-layer neural networks. We propose a convex SDP relax-
ation of the dual of the variational primal problem, which can be solved eﬃciently using
convex optimization methods instead of directly training the neural network as a noncon-
vex optimization problem. In particular, we established that the gradient obtained by the

MarAprMayJun0.00.20.40.60.81.0NN vs pSVGD social distancingmean_NNmean_pSVGDMarAprMayJuncvxNN vs pSVGD social distancing0.00.20.40.60.81.0mean_cvxNNmean_pSVGDMarAprMayJun05000100001500020000NN vs pSVGD # hospitalizedmean_NNmean_pSVGDMarAprMayJuncvxNN vs pSVGD # hospitalized05000100001500020000mean_cvxNNmean_pSVGD12

new formulation and convex optimization is at least as good as the optimal approxima-
tion of the Wasserstein gradient direction by functions in the family of two-layer neural
networks, which is demonstrated by various numerical experiments. In future works, we
expect to extend our convex neural network approximations to generalized Wasserstein
ﬂows.

References

Alvarez-Melis, D., Schiﬀ, Y., and Mroueh, Y. Optimizing functionals on the space of
probabilities with input convex neural networks. arXiv preprint arXiv:2106.00774, 2021.
Ambrosio, L., Gigli, N., and Savar´e, G. Gradient ﬂows: in metric spaces and in the space

of probability measures. Springer Science & Business Media, 2005.

Bartan, B. and Pilanci, M. Neural spectrahedra and semideﬁnite lifts: Global convex
optimization of polynomial activation neural networks in fully polynomial-time. arXiv
preprint arXiv:2101.02429, 2021.

Bonet, C., Courty, N., Septier, F., and Drumetz, L. Sliced-wasserstein gradient ﬂows.

arXiv preprint arXiv:2110.10972, 2021.

Bunne, C., Meng-Papaxanthos, L., Krause, A., and Cuturi, M. Jkonet: Proximal optimal
transport modeling of population dynamics. arXiv preprint arXiv:2106.06345, 2021.
Carrillo, J. A., Craig, K., Wang, L., and Wei, C. Primal dual methods for wasserstein

gradient ﬂows. Foundations of Computational Mathematics, pp. 1–55, 2021a.

Carrillo, J. A., Matthes, D., and Wolfram, M.-T. Lagrangian schemes for wasserstein

gradient ﬂows. Handbook of Numerical Analysis, 22:271–311, 2021b.

Chen, P. and Ghattas, O. Projected stein variational gradient descent. Advances in Neural

Information Processing Systems, 33:1947–1958, 2020.

Cover, T. M. Geometrical and statistical properties of systems of linear inequalities with
IEEE transactions on electronic computers, (3):

applications in pattern recognition.
326–334, 1965.

Cui, T., Law, K. J., and Marzouk, Y. M. Dimension-independent likelihood-informed

mcmc. Journal of Computational Physics, 304:109–137, 2016.

Detommaso, G., Cui, T., Spantini, A., Marzouk, Y., and Scheichl, R. A stein variational

newton method. arXiv preprint arXiv:1806.03085, 2018.

di Langosco, L. L., Fortuin, V., and Strathmann, H. Neural variational gradient descent.

arXiv preprint arXiv:2107.10731, 2021.

Diamond, S. and Boyd, S. CVXPY: A Python-embedded modeling language for convex

optimization. Journal of Machine Learning Research, 17(83):1–5, 2016.

Ergen, T., Sahiner, A., Ozturkler, B., Pauly, J., Mardani, M., and Pilanci, M. Demystify-
ing batch normalization in relu networks: Equivalent convex optimization models and
implicit regularization. arXiv preprint arXiv:2103.01499, 2021.

Fan, J., Taghvaei, A., and Chen, Y. Variational wasserstein gradient ﬂow. arXiv preprint

arXiv:2112.02424, 2021.

Feng, X., Gao, Y., Huang, J., Jiao, Y., and Liu, X. Relative entropy gradient sampler for

unnormalized distributions. arXiv preprint arXiv:2110.02787, 2021.

Frogner, C. and Poggio, T. Approximate inference with wasserstein gradient ﬂows. In
International Conference on Artiﬁcial Intelligence and Statistics, pp. 2581–2590. PMLR,
2020.

Hornik, K., Stinchcombe, M., and White, H. Multilayer feedforward networks are universal

approximators. Neural networks, 2(5):359–366, 1989.

13

Hwang, H. J., Kim, C., Park, M. S., and Son, H. The deep minimizing movement scheme.

arXiv preprint arXiv:2109.14851, 2021.

Jeyakumar, V. and Li, G. Trust-region problems with linear inequality constraints: exact
sdp relaxation, global optimality and robust optimization. Mathematical Programming,
147(1):171–206, 2014.

Jordan, R., Kinderlehrer, D., and Otto, F. The variational formulation of the fokker–

planck equation. SIAM journal on mathematical analysis, 29(1):1–17, 1998.

Junge, O., Matthes, D., and Osberger, H. A fully discrete variational scheme for solving
nonlinear fokker–planck equations in multiple space dimensions. SIAM Journal on
Numerical Analysis, 55(1):419–443, 2017.

Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

Kruse, J., Detommaso, G., Scheichl, R., and K¨othe, U. Hint: Hierarchical invert-
ible neural transport for density estimation and bayesian inference. arXiv preprint
arXiv:1905.10687, 2019.

Lan, S., Li, S., and Shahbaba, B. Scaling up bayesian uncertainty quantiﬁcation for inverse

problems using deep neural networks. arXiv preprint arXiv:2101.03906, 2021.

Lin, A. T., Fung, S. W., Li, W., Nurbekyan, L., and Osher, S. J. Alternating the popula-
tion and control neural networks to solve high-dimensional stochastic mean-ﬁeld games.
Proceedings of the National Academy of Sciences, 118(31), 2021a.

Lin, A. T., Li, W., Osher, S., and Mont´ufar, G. Wasserstein proximal of gans. arXiv

preprint arXiv:2102.06862, 2021b.

Liu, C., Zhuo, J., Cheng, P., Zhang, R., and Zhu, J. Understanding and accelerating
particle-based variational inference. In International Conference on Machine Learning,
pp. 4082–4092. PMLR, 2019.

Liu, Q. and Wang, D. Stein variational gradient descent: A general purpose bayesian
inference algorithm. In Advances in neural information processing systems, pp. 2378–
2386, 2016.

Liutkus, A., Simsekli, U., Majewski, S., Durmus, A., and St¨oter, F.-R. Sliced-wasserstein
ﬂows: Nonparametric generative modeling via optimal transport and diﬀusions.
In
International Conference on Machine Learning, pp. 4104–4113. PMLR, 2019.

Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. The expressive power of neural networks:
A view from the width. In Proceedings of the 31st International Conference on Neural
Information Processing Systems, pp. 6232–6240, 2017.

Mokrov, P., Korotin, A., Li, L., Genevay, A., Solomon, J., and Burnaev, E. Large-scale

wasserstein gradient ﬂows. arXiv preprint arXiv:2106.00736, 2021.

Onken, D., Fung, S. W., Li, X., and Ruthotto, L. Ot-ﬂow: Fast and accurate continuous

normalizing ﬂows via optimal transport. arXiv preprint arXiv:2006.00104, 2020.

Otto, F. The geometry of dissipative evolution equations: the porous medium equation.

Communications in Partial Diﬀerential Equations, 26(1-2):101–174, 2001.

Pilanci, M. and Ergen, T. Neural networks are convex regularizers: Exact polynomial-time
convex optimization formulations for two-layer networks. In International Conference
on Machine Learning, pp. 7695–7705. PMLR, 2020.

Rezende, D. and Mohamed, S. Variational inference with normalizing ﬂows. In Interna-

tional conference on machine learning, pp. 1530–1538. PMLR, 2015.

Sahiner, A., Ergen, T., Pauly, J., and Pilanci, M. Vector-output relu neural network prob-
lems are copositive programs: Convex analysis of two layer networks and polynomial-
time algorithms. arXiv preprint arXiv:2012.13329, 2020.

14

Stuart, A. M. Inverse problems: a Bayesian perspective. Acta numerica, 19:451–559, 2010.
Villani, C. Topics in optimal transportation. American Mathematical Soc., 2003.
Wang, Y., Chen, P., and Li, W. Projected wasserstein gradient descent for high-

dimensional bayesian inference. arXiv preprint arXiv:2102.06350, 2021.

Zahm, O., Cui, T., Law, K., Spantini, A., and Marzouk, Y. Certiﬁed dimension reduction

in nonlinear bayesian inverse problems. arXiv preprint arXiv:1807.03712, 2018.

Appendix A. Codes for numerical experiment

15

All codes for the numerical experiment can be found in https://github.com/ai-submit/

OptimalWasserstein.

Appendix B. Additional numerical experiment

B.1. PDE-constrained linear Bayesian inference. In this experiment, we consider
a linear Bayesian inference problem constrained by a partial diﬀerential equation (PDE)
model for contaminant diﬀusion in environmental engineering in domain D = (0, 1),

−κ∆u + νu = x

in D,

where x is a contaminant source ﬁeld parameter in domain D, u is the contaminant
concentration which we can observe at some locations, κ and ν are diﬀusion and reaction
coeﬃcients. For simplicity, we set κ, ν = 1, u(0) = u(1) = 0, and consider 15 pointwise
observations of u with 1% noise, equidistantly distributed in D. We consider a Gaussian
prior distribution x ∼ N (0, C) with covariance given by a diﬀerential operator C = (−δ∆+
γI)−α with δ, γ, α > 0 representing the correlation length and variance, which is commonly
used in geoscience. We set δ = 0.1, γ = 1, α = 1. In this linear setting, the posterior is
Gaussian with the mean and covariance given analytically, which are used as reference
to assess the sample goodness. We solve this forward model by a ﬁnite element method
with piece-wise linear elements on a uniform mesh of size 2k, k ≥ 1. We project this
high-dimensional parameter to the data-informed low dimensions as in Wang et al. (2021)
to alleviate the curse of dimensionality when applying WGD-cvxNN and WGD-NN, which
we call pWGD-cvxNN and pWGD-NN, respectively. For k = 4 we have 17 dimensions for
the discrete parameter and 4 dimensions after projection.

We run pWGD-cvxNN and pWGD-NN using 16 samples for 200 iterations with αl =
10−3, β = 5, γ1 = 0.95, and γ2 = 0.9510 for both methods. We use m = 200 neu-
rons for pWGD-NN and train it by the Adam optimizer for 200 sub-iterations as in the
ﬁrst example. From Figure 5, we observe that pWGD-cvxNN achieves better root mean
squared error (RMSE) than pWGD-NN for both the sample mean and the sample variance
compared to the reference.

Figure 5. Ten trials and the RMSE of the sample mean (top) and sample
variance (bottom) by pWGD-NN and pWGD-cvxNN at diﬀerent iterations.
Linear inference problem.

0255075100125150175200# iterations1.21.00.80.60.40.20.0Log10(RMSE of mean)pWGD-NNpWGD-cvxNN0255075100125150175200# iterations1.00.80.60.40.20.00.20.4Log10(RMSE of variance)pWGD-NNpWGD-cvxNN16

Appendix C. Choice of the regularization parameter

As the constraints in the relaxed dual problem (16) depends on the regularization pa-
rameter ˜β, it is possible that for small ˜β, the relaxed dual problem (16) is infeasible.
Consider the following SDP

min ˜β, s.t. ˜Aj(Λ) + ˜Bj +

N
(cid:88)

n=0

n H (j)
r(j,−)

n + ˜βed+1eT

d+1 (cid:23) 0,

− ˜Aj(Λ) − ˜Bj +

N
(cid:88)

n=0

n H (j)
r(j,+)

n + ˜βed+1eT

d+1 (cid:23) 0,

(21)

r(j,−) ≥ 0, r(j,+) ≥ 0, j ∈ [p].

Here the variables are ˜β, Λ and {r(j,+), r(j,−)}p
j=1. Let ˜β1 be the optimal value of the
above problem. Then, only for ˜β ≥ ˜β1, there exists Λ ∈ RN ×d satisfying the constraints
in (16). In other words, the relaxed dual problem (16) is feasible. We also note that ˜β1
only depends on the samples X and it does not depend on the value of ∇ log π evaluated
on x1, . . . , xN . On the other hand, consider the following SDP

min ˜β, s.t. ˜Aj(Y ) + ˜Bj +

N
(cid:88)

n=0

n H (j)
r(j,−)

n + ˜βed+1eT

d+1 (cid:23) 0,

− ˜Aj(Y ) − ˜Bj +

N
(cid:88)

n=0

n H (j)
r(j,+)

n + ˜βed+1eT

d+1 (cid:23) 0,

(22)

r(j,−) ≥ 0, r(j,+) ≥ 0, j ∈ [p],

where the variables are ˜β and {r(j,+), r(j,−)}p
j=1. Let ˜β2 be the optimal value of the above
problem. For ˜β ≥ ˜β2, as Y is feasible for the constraints in (16), the optimal value of the
relaxed dual problem (16) is 0. In short, only when ˜β ∈ [ ˜β1, ˜β2], the variational problem
(16) is non-trivial. To ensure that solving the relaxed dual problem (16) gives a good
approximation of the Wasserstein gradient direction, we shall avoid choosing ˜β either too
small or too large.

Appendix D. Proofs

D.1. Proof of Proposition 1.

Proof. We ﬁrst note that

(cid:90)

(cid:90)

(cid:107)∇Φ − ∇ log ρ + ∇ log π(cid:107)2

2ρdx

(cid:107)∇Φ(cid:107)2

2ρdx +

(cid:90)

(cid:104)∇ log π − ∇ log ρ, ∇Φ(cid:105) ρdx

(23)

1
2
1
2

=

+

(cid:90)

1
2

(cid:107)∇ log ρ − ∇ log π(cid:107)2

2ρdx.

17

We notice that the term 1
2
integration by parts, we can compute that

(cid:82) (cid:107)∇ log ρ − ∇ log π(cid:107)2

2ρdx does not depend on Φ. Utilizing the

(cid:90)

(cid:104)∇ log ρ, ∇Φ(cid:105) ρdx =

(cid:90) (cid:28) ∇ρ
ρ

(cid:29)

, ∇Φ

ρdx

(cid:90)

=

(cid:104)∇ρ, ∇Φ(cid:105) dx

(cid:90)

= −

∆Φρdx.

(24)

Therefore, the variational problem (4) is equivalent to

inf
Φ∈C∞(Rd)

(cid:90)

1
2

(cid:107)∇Φ(cid:107)2

2ρdx +

(cid:90)

(cid:104)∇ log π, ∇Φ(cid:105) ρdx +

(cid:90)

∆Φρdx.

(25)

By restricting the domain C∞(Rd) to H, we complete the proof.

(cid:3)

D.2. Proof of Proposition 2.

Proof. Suppose that ˆwi = β−1
i ∈ [m]. Let θ(cid:48) = {( ˆwi, ˆαi)}m

i wi and ˆαi = β2
i=1. We note that

i αi, where βi > 0 is a scale parameter for

ˆαi ˆwiψ(cid:48)( ˆwT

i xn) = βiαiwiψ(cid:48) (cid:0)β−1

i wT

i xn

(cid:1) = αiwiψ(cid:48)(wT

i xn),

and

ˆαi(cid:107) ˆwi(cid:107)2

2ψ(cid:48)(cid:48)( ˆwT

i xn) = αi(cid:107)wi(cid:107)2

2ψ(cid:48)(cid:48)( ˆwT

i xn) = αi(cid:107)wi(cid:107)2

2ψ(cid:48)(cid:48)(wT

i xn).

(26)

(27)

This implies that Φθ(x) = Φθ(cid:48)(x) and ∇ · Φθ(x) = ∇ · Φθ(cid:48)(x). For the regularization term
R(θ), we note that

(cid:107) ˆwi(cid:107)3

2 + (cid:107)ˆαi(cid:107)3

2 =β6

i |αi|3 + β−3

i (cid:107)wi(cid:107)3
2
1
β−3
i (cid:107)wi(cid:107)3
2

=β6

i |αi|3 +
=3 · 2−2/3(cid:107)wi(cid:107)2

2|αi|.

2 +

1
2

β−3
i (cid:107)wi(cid:107)3
2

(28)

The optimal scaling parameter is given by αi = 2−1/9 (cid:107)wi(cid:107)1/3
2
|αi|1/3
1
does not change (cid:107)wi(cid:107)2
(cid:80)m
β
2 R(θ) becomes

2|αi|, we can simply let (cid:107)wi(cid:107)2 = 1. Thus, the regularization term
(cid:3)
i=1 (cid:107)ui(cid:107)1. This completes the proof.

. As the scaling operation

˜β
N

D.3. Proof of Proposition 3.

18

Proof. Consider the Lagrangian function

L(Z, W, α, Λ) =

1
2

(cid:107)Z(cid:107)2

F +

N
(cid:88)

m
(cid:88)

αi(cid:107)wi(cid:107)2

2ψ(cid:48)(cid:48)(wT

i xn) + tr(Y T Z) + ˜β(cid:107)α(cid:107)1

i=1

n=1
(cid:32)

λT
n

zn −

m
(cid:88)

(cid:33)

αiwiψ(cid:48)(xT

n wi)

+

N
(cid:88)

n=1

= ˜β(cid:107)α(cid:107)1 +

m
(cid:88)

αi

i=1
N
(cid:88)

(cid:0)(cid:107)wi(cid:107)2

2ψ(cid:48)(cid:48)(wT

i xn) − λT

n wiψ(cid:48)(xT

mwi)(cid:1)

(29)

n=1
i=1
F + tr((Y + Λ)T Z).

(cid:107)Z(cid:107)2

+

1
2

For ﬁxed W , the constraints on Z and α are linear and the strong duality holds. Thus,
we can exchange the order of minZ,α and maxΛ. Thus, we can compute that

min
Z,W,α

max
Λ
max
Λ

min
α,Z

= min
W

L(Z, W, α, Λ)

L(Z, W, α, Λ)

= min
W

max
Λ

min
α,Z

˜β(cid:107)α(cid:107)1 +

m
(cid:88)

i=1

= min
W

max
Λ

−

1
2

(cid:107)Λ + Y (cid:107)2

F +

n=1
(cid:32)

m
(cid:88)

i=1

I

max
wi:(cid:107)wi(cid:107)2≤1

N
(cid:88)

αi

(cid:0)(cid:107)wi(cid:107)2

2ψ(cid:48)(cid:48)(wT

i xn) − λT

n wiψ(cid:48)(xT

mwi)(cid:1) +

1
2

(cid:107)Z(cid:107)2

F + tr((Y + Λ)T Z)

N
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
n=1

(cid:107)wi(cid:107)2

2ψ(cid:48)(cid:48)(wT

i xn) − yT

n wiψ(cid:48)(xT

n wi)

(cid:33)

≤ ˜β

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(30)

By exchanging the order of min and max, we can derive the dual problem:

max
Λ

min
W

−

1
2

(cid:107)Λ + Y (cid:107)2

F +

m
(cid:88)

I

i=1

(cid:32)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

N
(cid:88)

n=1

(cid:107)wi(cid:107)2

2ψ(cid:48)(cid:48)(wT

i xn) − yT

n wiψ(cid:48)(xT

n wi)

(cid:33)

≤ ˜β

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= max
Λ

−

= max
Λ

−

1
2

1
2

(cid:107)Λ + Y (cid:107)2

F s.t.

(cid:107)wi(cid:107)2

2ψ(cid:48)(cid:48)(wT

i xn) − yT

n wiψ(cid:48)(xT

n wi)

(cid:107)Λ + Y (cid:107)2

F s.t. max

2ψ(cid:48)(cid:48)(wT xn) − yT

n wψ(cid:48)(xT

N
(cid:88)

max
wi:(cid:107)wi(cid:107)2≤1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
n=1
N
(cid:88)

(cid:107)w(cid:107)2

max
wi:(cid:107)wi(cid:107)2≤1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
n=1

w:(cid:107)w(cid:107)2≤1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ ˜β, i ∈ [m]

(cid:12)
(cid:12)
(cid:12)
n w)
(cid:12)
(cid:12)

≤ ˜β, i ∈ [m]

(31)
(cid:3)

This completes the proof.

D.4. Proof of Proposition 4.

Proof. Based on the hyper-plane arrangements D1, . . . , Dp, the dual constraint is equiva-
lent to that for all j ∈ [p],

(cid:12)
(cid:12)2 tr(Dj)(cid:107)w(cid:107)2

2 − 2wT ΛT DjXw(cid:12)

(cid:12) ≤ ˜β

(32)

holds for all w ∈ Rd satisfying (cid:107)w(cid:107)2 ≤ 1, (2Dj − I)Xw ≥ 0. This is equivalent to say that
for all j ∈ [p]

− ˜β ≥ min 2 tr(Dj)(cid:107)w(cid:107)2

2 − 2wT ΛT DjXw,
s.t. (cid:107)w(cid:107)2 ≤ 1, 2(Dj − I)Xw ≥ 0,

˜β ≤ max 2 tr(Dj)(cid:107)w(cid:107)2

2 − 2wT ΛT DjXw,

s.t. (cid:107)w(cid:107)2 ≤ 1, 2(Dj − I)Xw ≥ 0.

19

(33)

From a convex optimization perspective, the natural idea to interpret the constraint (33)
is to transform the minimization problem into a maximization problem. We can rewrite
the minimization problem in (33) as a trust region problem with inequality constraints:
wT (Bj + Aj(Λ)) w,

min
w∈Rd
s.t. (cid:107)w(cid:107)2 ≤ 1, (2Dj − I)Xw ≥ 0.

(34)

As the problem (34) is a convex problem, by taking the dual of (34) w.r.t. w, we can
transform (34) into a maximization problem. However, as (34) is a trust region problem
with inequality constraints, the dual problem of (34) can be very complicated. According
to (Jeyakumar & Li, 2014), the optimal value of the problem (34) is bounded by the
optimal value of the following SDP

min
Z∈Sd+1

tr(( ˜Aj(Λ) + ˜Bj)Z),

s.t. tr(H (j)

n Z) ≤ 0, n = 0, . . . , N,

Zd+1,d+1 = 1, Z (cid:23) 0.

from below.

Lemma 1. The dual problem of SDP (35) takes the form

max −γ, s.t. S = ˜Aj(Λ) + ˜Bj +

N
(cid:88)

n=0

rnH (j)

n + γed+1eT

d+1, r ≥ 0, S (cid:23) 0,

in variables r =


 ∈ RN +1 and γ ∈ R.







r0
...
rN

Proof. Consider the Lagrangian

L(Z, r, γ) = tr(( ˜Aj(y) + ˜Bj)Z) +

N
(cid:88)

n=0

rn tr(H (j)

n Z) + γ(tr(Zed+1eT

d+1) − 1),

(35)

(36)

(37)

where r ∈ RN +1
+
problem (36).

and γ ∈ R. By minimizing L(Z, r, γ) w.r.t. Z ∈ Sd+1

+ , we derive the dual
(cid:3)

The constraints on Λ in the dual problem (14) include that the optimal value of (35) is
bounded from below by − ˜β. According to Lemma 1, this constraint is equivalent to that
there exist r ∈ RN +1 and γ such that

− γ ≥ − ˜β, S = ˜Aj(Λ) + ˜Bj +

N
(cid:88)

n=0

rnH (j)

n + γed+1eT

d+1, r ≥ 0, S (cid:23) 0.

(38)

20

As ed+1eT
there exist r ∈ RN +1 such that

d+1 is positive semi-deﬁnite, the above condition on Λ is also equivalent to that

˜Aj(Λ) + ˜Bj +

N
(cid:88)

n=0

rnH (j)

n + ˜βed+1eT

d+1 (cid:23) 0, r ≥ 0.

Therefore, the following convex set of Λ

(cid:110)
Λ : ˜Aj(Λ) + ˜Bj +

N
(cid:88)

n=0

n H (j)
r(j,−)

n + ˜βed+1eT

d+1 (cid:23) 0, r(j,−) ≥ 0

is a subset of the set of Λ satisfying the dual constraints

(cid:26)

Λ :

min
(cid:107)w(cid:107)2≤1,(2Dj −I)w≥0

wT (Bj + Aj(Λ)) w ≥ − ˜β

(cid:27)

On the other hand, the constraint on Λ

max
(cid:107)w(cid:107)2≤1,(2Dj −I)w≥0

wT (Bj + Aj(Λ)) w ≤ ˜β

is equivalent to

min
(cid:107)w(cid:107)2≤1,(2Dj −I)w≥0

−wT (Bj + Aj(Λ)) w ≥ − ˜β.

(39)

(cid:111)

(40)

(41)

(42)

(43)

By applying the previous analysis on the above trust region problem, the following convex
set of Λ

(cid:110)
Λ : − ˜Aj(Λ) − ˜Bj +

N
(cid:88)

n=0

n H (j)
r(j,+)

n + ˜βed+1eT

d+1 (cid:23) 0, r(j,+) ≥ 0

(cid:111)

(44)

is a subset of the set of Λ satisfying the dual constraints

(cid:26)

Λ :

max
(cid:107)w(cid:107)2≤1,(2Dj −I)w≥0

wT (Bj + Aj(Λ)) w ≤ ˜β

(cid:27)

.

(45)

Therefore, replacing the dual constraint maxw:(cid:107)w(cid:107)2≤1
˜β by

(cid:12)
(cid:80)N
(cid:12)
(cid:12)

n=1 (cid:107)w(cid:107)2

2ψ(cid:48)(cid:48)(wT xn) − yT

n wψ(cid:48)(xT

n w)

(cid:12)
(cid:12)
(cid:12) ≤

˜Aj(Λ) + ˜Bj +

N
(cid:88)

n=0

n H (j)
r(j,−)

n + ˜βed+1eT

d+1 (cid:23) 0, j ∈ [p],

− ˜Aj(Λ) − ˜Bj +

N
(cid:88)

n=0

n H (j)
r(j,+)

n + ˜βed+1eT

d+1 (cid:23) 0, j ∈ [p],

(46)

r(j,−) ≥ 0, r(j,+) ≥ 0, j ∈ [p].

we obtain the relaxed dual problem. As its feasible domain is a subset of the feasible
domain of the dual problem, the optimal value of the relaxed dual problem gives a lower
(cid:3)
bound for the optimal value of the dual problem.

D.5. Proof of Proposition 5.

Proof. Consider the Lagrangian function

21

L(Λ, r, S) = −

1
2

(cid:107)Λ + Y (cid:107)2

2 −

(cid:32)

(cid:32)

tr

S(j,−)

˜Aj(Λ) + ˜Bj +

n H (j)
r(j,−)

n +

(cid:33)(cid:33)

˜β
2

ed+1eT

d+1

N
(cid:88)

n=0

p
(cid:88)

j=1
(cid:32)

(cid:32)

tr

S(j,+)

− ˜Aj(Λ) − ˜Bj +

n H (j)
r(j,+)

n +

(cid:33)(cid:33)

ed+1eT

d+1

,

˜β
2

N
(cid:88)

n=0

−

p
(cid:88)

j=1

where we write

(cid:16)

(cid:16)

r =

S =

r(1,−), . . . , r(p,−), r(1,+), . . . , r(p,+)(cid:17)
S(1,−), . . . , S(p,−), S(1,+), . . . , S(p,+)(cid:17)

∈ (cid:0)RN +1(cid:1)2p
(cid:17)2p
(cid:16)

,

∈

Sd+1
+

(47)

(48)

.

Here we write Sd+1
bi-dual problem (17).

+ = {S ∈ Sd+1|S (cid:23) 0}. By maximizing w.r.t. Λ and r, we derive the

D.6. Proof of Theorem 1. Suppose that (Z, W, α) is a feasible solution to (12). Let
Dj1, . . . , Djk be the enumeration of {diag(I(Xwi ≥ 0))|i ∈ [m]}. For i ∈ [k], we let

S(ji,+) =

(cid:88)

l:αl≥0,diag(I(Xwl≥0))=Dji

αl

(cid:21)

(cid:20)wlwT
wT
l

l wl
1

, S(ji,−) = 0,

(49)

and

S(ji,+) = 0, S(ji,−) = −

(cid:88)

l:αl<0,diag(I(Xwl≥0))=Dji

αl

(cid:21)

(cid:20)wlwT
wT
l

l wl
1

.

(50)

For j /∈ {j1, . . . , jk}, we simply set S(j,+) = 0, S(j,−) = 0. As (cid:107)wi(cid:107)2 ≤ 1 and Dji = I(Xwi ≥
0), we can verify that tr(S(j,−)H (j)
n ) ≤ 0 are satisﬁed for j = j1, . . . , jm
and n = 0, 1, . . . , N . This is because for n = 0, as H (ji)

n ) ≤ 0, tr(S(j,+)H (j)

, it follows that

(cid:21)

0 =

(cid:20)Id
0
0 −1

tr(S(ji,+)H (ji)

0

) =

(cid:88)

αl((cid:107)wl(cid:107)2 − 1) ≤ 0,

l:αl≥0,diag(I(Xwl≥0))=Dji

tr(S(ji,−)H (ji)

0

) = −

(cid:88)

αl((cid:107)wl(cid:107)2 − 1) ≤ 0.

l:αl<0,diag(I(Xwl≥0))=Dji

For n = 1, . . . , N , we have

tr(S(ji,+)H (ji)

0

) =

(cid:88)

2αl(1 − 2(Dji)nn)xT

n wl ≤ 0,

l:αl≥0,diag(I(Xwl≥0))=Dji

tr(S(ji,−)H (ji)

0

) = −

(cid:88)

αl(1 − 2(Dji)nn)xT

n wl ≤ 0.

l:αl<0,diag(I(Xwl≥0))=Dji

(51)

(52)

22

Based on the above transformation, we can rewrite the bidual problem in the form of

the primal problem (13). For S ∈ Sd+1, we note that

tr(S ˜Aj(Λ))

= − tr((ΛT DjX + X T DjΛ)S1:d,1:d)
= − 2 tr(ΛT DjXS1:d,1:d),

where S1:d,1:d denotes the d × d block of S consisting the ﬁrst d rows and columns. This
implies that ˜A∗

˜Aji(S(ji,+) − S(ji,−)) = −

j (S) = −2DjXS1:d,1:d. Hence, we have
(cid:88)
2αlDjiXwlwT

l = −

(cid:88)

2αl(Xwl)+wT
l .

Therefore, we have

p
(cid:88)

j=1

l:diag(I(Xwl≥0)

l:diag(I(Xwl≥0)

˜A∗

j (S(j,−) − S(j,+)) = 2

m
(cid:88)

αi(Xwi)+wT
i .

i=1
i=1 αiwi(xT

As n-th row of Z satisﬁes that zn = 2 (cid:80)m

n wi)+, this implies that

Z = 2

m
(cid:88)

i=1

αi(Xwi)+wT

i =

p
(cid:88)

j=1

˜A∗

j (S(j,−) − S(j,+)).

Hence (Z, {(S(j,−), (S(j,−)}p

j=1) is feasible to the relaxed bi-dual problem (17).

We can also compute that

p
(cid:88)

j=1

tr( ˜Bj(S(j,+) − S(j,−))) = 2

m
(cid:88)

αi

N
(cid:88)

i=1

n=1

I(xT

n wi ≥ 0)(cid:107)wi(cid:107)2
2,

and

(cid:16)

tr

p
(cid:88)

j=1

(S(j,+) + S(j,−))ed+1eT

d+1

(cid:17)

=

m
(cid:88)

i=1

|αi|.

Thus, the primal problem (13) with (Z, W, α) and the relaxed bi-dual problem (17) with
(Z, {(S(j,−), (S(j,−)}p
(cid:3)

j=1) have the same objective value.

