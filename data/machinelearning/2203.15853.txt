2
2
0
2

r
a

M
9
2

]

G
L
.
s
c
[

1
v
3
5
8
5
1
.
3
0
2
2
:
v
i
X
r
a

Near-optimality for inﬁnite-horizon restless bandits
with many arms

Xiangyu Zhang
Cornell University, xz556@cornell.edu

Peter I. Frazier
Cornell University pf98@cornell.edu

Restless bandits are an important class of problems with applications in recommender systems, active

learning, revenue management and other areas. We consider inﬁnite-horizon discounted restless bandits with

many arms where a ﬁxed proportion of arms may be pulled in each period and where arms share a ﬁnite state

space. Although an average-case-optimal policy can be computed via stochastic dynamic programming, the

computation required grows exponentially with the number of arms N . Thus, it is important to ﬁnd scalable

policies that can be computed eﬃciently for large N and that are near optimal in this regime, in the sense

that the optimality gap (i.e. the loss of expected performance against an optimal policy) per arm vanishes

for large N . However, the most popular approach, the Whittle index, requires a hard-to-verify indexability

condition to be well-deﬁned and another hard-to-verify condition to guarantee a o(N ) optimality gap. We

present a method resolving these diﬃculties. By replacing a global Lagrange multiplier used by the Whittle

index with a sequence of Lagrangian multipliers, one per time period up to a ﬁnite truncation point, we

derive a class of policies, called ﬂuid-balance policies, that have a O(

√

N ) optimality gap. Unlike the Whittle
√

index, ﬂuid-balance policies do not require indexability to be well-deﬁned and their O(

N ) optimality gap

bound holds universally without suﬃcient conditions. We also demonstrate empirically that ﬂuid-balance

policies provide state-of-the-art performance on speciﬁc problems.

Key words : restless bandit, Markov decision processes, Whittle index

1.

Introduction

We study a stochastic control problem called the inﬁnite-horizon restless bandit. In this problem,

a decision maker is responsible for managing N Markov decision processes (called “arms”) whose

states are fully observed and belong to a common ﬁnite state space. For each arm in each time

period, the decision maker can either activate the arm (also called “pullng” the arm) or idle it.

This arm then generates a random reward. This reward’s probability distribution depends in a

known way on the action taken and the arm’s current state. A known transition kernel depending

on the action and the arm’s current state then determines the probability distribution over the

arm’s state in the next time period. When making decisions, the decision maker needs to respect

a “budget” constraint in each period that constrains the number of arms that can be activated in

each period. The objective is to maximize the expected total discounted reward over an inﬁnite

time horizon.

1

 
 
 
 
 
 
2

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

The inﬁnite-horizon restless bandit problem was ﬁrst formulated by Whittle (1980) and has

since attracted much theoretical and practical interest. Many real-world decision-making problems

are naturally formulated as restless bandits, with applications arising in network communication

(Liu and Zhao 2008), unmanned aerial vehicles tracking (Le Ny et al. 2006), revenue management

(Brown and Smith 2020) and active learning (Chen et al. 2013).

In principle, the problem can be solved by value iteration or other standard methods for max-

imizing the inﬁnite-horizon expected total discounted reward in a stochastic dynamic program

(Powell 2007). Unfortunately, the dimension of the collective description of arms’ states grows lin-

early with N . Thus, the computation required grows exponentially with respect to N due to the

“curse of dimensionality” (Powell 2007).

Because optimality appears unachievable by computationally tractable algorithms, theoretical

analysis of restless bandits has focused on asymptotic optimality for the asymptotic regime where

the budget constraint grows proportionally with N . First deﬁned by Whittle (1980), an asymp-

totically optimal policy is one whose optimality gap (the diﬀerence between the given strategy’s

expected performance and that of an optimal policy, brieﬂy, opt gap) divided by the number of

arms vanishes as the number of arms grows.

Asymptotic optimality has been hard to guarantee. The most popular approach to restless ban-

dits, the Whittle index, was conjectured to be asymptotically optimal by Whittle (1980). However,

Weber and Weiss (1990) shows that this is false: there are problems where the Whittle index fails to

be asymptotically optimal. That work also shows that the Whittle index is asymptotically optimal,

but only if a certain hard-to-verify suﬃcient condition is met: that a diﬀerential equation charac-

terizing the dynamics of the Whittle index in a certain ﬂuid limit has a globally stable equilibrium.

Moreover, for the Whittle index to be well deﬁned, the problem must satisfy a so-called “indexabil-

ity” condition, which may not be met and is hard to verify in practice. Another popular approach

is simulation-based (Meshram and Kaza 2020, Nakhleh et al. 2021). However, the simulation-based

method from Meshram and Kaza (2020) and Nakhleh et al. (2021) does not provide a theoretical

guarantee on performance.

Although the existing inﬁnite-horizon restless bandit policies of which we are aware suﬀer from

diﬃculty in guaranteeing asymptotic optimality and, in the case of the Whittle index, challenges

in establishing indexability and coping with its absence, some recent work shows that life is much

easier for ﬁnite-horizon restless bandits.

For example, for ﬁnite-horizon restless bandits in the same asymptotic regime where budgets
grow proportionally with N , Hu and Frazier (2017) propose a policy with o(N ) opt gap, thus being
N )
asymptotically optimal. Later, Brown and Smith (2020) proposes policies with stronger O(
opt gaps. Moreover, Zhang and Frazier (2021) propose a class of policies with at most O(
N )

√

√

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

3

opt gaps and surprisingly O(1) opt gap if a non-degeneracy condition is met. These rates for opt

gaps hold universally, unlike the hard-to-verify conditions required for the Whittle index to be

asymptotically optimal. Moreover, neither policy requires an indexability condition. These papers

overcome the challenges articulated above despite basing their analysis on the same Lagrangian

relaxation technique proposed and used by Whittle (1980).

We argue in this paper that a key diﬀerence in the approach enabled these ﬁnite-horizon anal-

yses to achieve asymptotic optimality and to avoid challenges in establishing indexability: their

Lagrangian relaxation uses a sequence of Lagrange multipliers, one for each time period, while the

Whittle index uses a single global Lagrange multiplier. Using a time-varying Lagrange multiplier

is intuitive in the ﬁnite-horizon setting: the ﬁnite horizon causes the problem to be non-stationary,

naturally inspiring a time-inhomogeneous approach.

We show in this paper that this time-inhomogeneous approach can be generalized to the inﬁnite-

horizon setting to overcome the shortcomings of the Whittle index and other past approaches to

the inﬁnite-horizon restless bandit. That a time-inhomogeneous approach would be relevant to the

inﬁnite-horizon setting may, at ﬁrst glance, seem surprising: the inﬁnite-horizon problem is station-

ary, implying the existence of stationary optimal policies, and suggesting that asymptotically opti-

mal policies should also be stationary. Part of our contribution is to explain why non-stationarity

is an important tool for providing asymptotic optimality in stationary inﬁnite-horizon problems.

We provide a novel class of computationally scalable non-stationary inﬁnite-horizon restless ban-

dit policies called “ﬂuid-balance” policies. We show that they are asymptotically optimal, achieving
N ) opt gap. This result does not require indexability or other suﬃcient conditions beyond
a O(

√

those deﬁning the problem we study, such as arms’ states belonging to a ﬁnite state space and

state transitions that are conditionally independent across arms. Moreover, despite being time-

inhomogeneous in an inﬁnite horizon problem, we show that they can be computed in ﬁnite time.

They are computed by considering a ﬁnite linear program formed by truncating the inﬁnite-horizon
N )
problem. Truncating at the O(log N )-th period allows ﬂuid-balance policies to achieve a O(

√

opt gap.

This requires going substantially beyond applying a previously proposed ﬁnite-horizon policy to
N ) opt gaps in the ﬁnite-horizon setting proposed in

the truncated problem. Policies with O(

√

Brown and Smith (2020) and Zhang and Frazier (2021) have opt gap bounds that depend expo-

nentially on the time horizon. Simply applying one of these policies and its associated performance

bound to a truncated problem (and leveraging discounting to bound the reward obtained after

√

truncation) results in an opt gap bound that grows faster than

N . Our ﬂuid-balance policies and

their analysis are speciﬁcally adapted to the inﬁnite-horizon setting to circumvent this challenge.

4

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

We give intuition for why a non-stationary approach can resolve the past challenges in inﬁnite-

horizon restless bandits. Index policies, such as the Whittle index and the policies that we propose,

operate by deﬁning a priority or “index” for each state and then pulling arms in order from the

ones in the highest priority to the lowest priority until the budget constraint on pulls in the current

time period is exhausted. Essentially, non-stationary approaches are beneﬁcial because performing

well with an index policy requires using a diﬀerent priority order over states in each time period.

Understanding the need to use a time-dependent priority order relies on a linear programming

analysis of the so-called ﬂuid approximation. In this approximation, we take the limit as the number

of arms grows large while scaling up the budget constraint. A policy can be understood as taking

an occupation measure (a vector comprising the fraction of arms in each state at a particular time)

as input and deciding the fraction of arms in each state to pull. In the ﬂuid limit, an optimal

policy’s decisions in a time period can be understood as pulling arms according to their marginal

beneﬁt from high to low until all resources are consumed. This marginal beneﬁt in each period

depends on the occupation measure in that period. Critically, under an optimal policy in the ﬂuid

limit, the occupation measure changes over time. This causes the optimal ranking over states to

vary across periods. Thus, matching the optimal policy in the ﬂuid limit requires an index policy

to use a diﬀerent a priority order in diﬀerent time periods.

The rest of the paper is structured as follows. First, Section 2 discusses relevant past work on

inﬁnite-horizon restless bandits and the novelty of our work. Then, Section 3 formulates the restless

bandit formally as a Markov decision process and Section 4 discusses a standard linear programming

relaxation used to support analysis of restless bandits in the past literature. Our proposed ﬂuid-

balance policies and analysis are also based on this relaxation. Section 5 then introduces a technical
condition, diﬀusion regularity that is suﬃcient for a O(
N ) opt gap. Section 6 proposes our ﬂuid-
√
N ) opt gap. Section
balance policies and shows that are diﬀusion regular and thus have a O(

√

7 uses two numerical experiments to explore the performance of ﬂuid-balance policies. Finally,

Section 8 concludes the paper and discusses possible future work.

2. Literature Review

This section ﬁrst reviews approaches speciﬁcally designed for the inﬁnite-horizon setting. It then

reviews recent progress in the ﬁnite-horizon setting motivating our approach.

2.1.

Inﬁnite-horizon restless bandits

The inﬁnite-horizon restless bandit problem was ﬁrst formulated by Whittle (1980). Since then, the

problem has attracted substantial research interest, both from theoretical and practical perspec-

tives. Here we review two main streams of this research: the Whittle index and simulation-based

approaches.

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

5

Whittle index When the restless bandit problem was ﬁrst formulated in Whittle (1980), this

paper also proposed an index policy, the so-called Whittle index, as a solution. The Whittle index

is deﬁned by considering a problem with a single arm in which one can pull the arm, paying a cost,

or idle it. The Whittle index for a state is the cost that makes an optimal policy indiﬀerent between

pulling the arm and idling it. This implies a ranking over states that, intuitively, is the same as

ranking by a state’s “marginal productivity”: the diﬀerence in discounted long-run reward between

activating and idling an arm in this state in the original problem (Ni˜no-Mora 2007). Intuitively, it

should be a good policy to simply pull the arms in the states with the highest marginal productivity.

Then Whittle index policy does exactly this: it activates arms according to their indices, from high

to low, until all resources are used.

Although intuitively promising, Whittle (1980) noticed that the willingness to pull an arm in

a single-arm problem is not always monotone: it may be optimal to pull the arm when the cost-

per-pull is low, idle it when the cost-per-pull is in an intermediate range, and pull it when the

cost-per-pull is high. In such settings, the Whittle index is not well-deﬁned and its link to marginal

productivity is lost. Whittle (1980) conjectured that indexability would imply asymptotic optimal-

ity: the diﬀerence between the Whittle index’s expected performance and that of an optimal policy

divided by the number of arms vanishes as the number of arms grows, allowing a constant fraction

of the arms to be pulled per time period. Later, however, Weber and Weiss (1990) provided a

counterexample to Whittle’s conjecture: the Whittle index can fail to be asymptotically optimal

even when the indexability condition is satisﬁed.

Responding to the challenge of establishing indexability, Gittins et al. (2011), Nino-Mora (2001)

establish alternate suﬃcient conditions for indexability and Glazebrook et al. (2006) characterize

some indexable restless bandit families. Liu and Zhao (2008, 2010) and Le Ny et al. (2008) show

their studied system is indexable. Guha and Munagala (2007, 2008), Guha et al. (2010), Ansell

et al. (2003) and Jacko and Nino-Mora (2007) have extended these ideas to more general settings

e.g. convex reward, convex resource budget, stochastic arriving and leaving arms, etc. Nevertheless,

establishing indexability remains challenging for most problems and typically entails additional

theoretical work that must be done on a problem-by-problem basis.

When a problem is not indexable, multiple values satisfy the conditions that usually deﬁne

the Whittle index. Thus, attempting to deploy a Whittle index policy in practice without ﬁrst

verifying indexability requires the implementation to explicitly handle this non-uniqueness. The

use of implementations assuming a unique Whittle index value in non-indexable problems creates

a risk that Whittle index computation produces errors or fails to converge. Also, the intuition for

why a Whittle index policy would perform well relies on indexability. When indexability is lacking,

6

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

policies prioritizing arms based on a Whittle index computation (while handling non-uniqueness)

may be less likely to perform well.

If indexability can be veriﬁed, establishing asymptotic optimality requires verifying the addi-

tional suﬃcient conditions discussed above. Past literature suggests that this may be even more

diﬃcult than verifying indexability. Most work using Whittle indices does not prove its asymptotic

optimality in the problem studied (Liu and Zhao 2008, Le Ny et al. 2008) or only proves it in a

speciﬁc parameter regime (Liu and Zhao 2010, Verloop 2016). Instead, past literature often relies

on numerical simulation to justify the Whittle index’s performance.

Thus, despite its popularity, the diﬃculty of verifying indexability and the additional conditions

needed for asymptotic optimality remain a challenge when applying Whittle index policy in real-

world problems.

Simulation-based approaches Responding to the limitations of the Whittle index policy,

simulation-based approaches have been developed. For example, Meshram and Kaza (2020) develop

rollout-based heuristic policies and Nakhleh et al. (2021) and Wang et al. (2021) develop a deep

reinforcement learning strategy, using neural networks to approximate the value function. Numer-

ical performance on a collection of benchmark problem instances is their primary concern rather

than theoretical guarantees. A policy that performs well in the problem instances simulated may

perform poorly in other closely-related problem instances, and so performing well in a simulation-

based study may not guarantee good performance across a wider range of problem instances faced

after a policy is deployed to the ﬁeld.

Moreover, if all benchmark policies included in a numerical study are asymptotically suboptimal,

a new asymptotically optimal policy has the potential to signiﬁcantly outperform all of them. Thus,

identifying new asymptotically optimal policies is of signiﬁcant interest.

2.2. Finite-horizon restless bandits

While the Whittle index faces challenges in verifying indexability and the additional conditions

required for asymptotic optimality, recent progress on ﬁnite-horizon restless bandits provides algo-

rithms without these drawbacks in this alternate setting.

In rapid succession, Hu and Frazier (2017), Zayas-Caban et al. (2019), Brown and Smith (2020)
N ) opt gaps respec-

proposed index policies and show that they have o(N ), O(

N log N ) and O(

√

√

tively. Then, Zhang and Frazier (2021) proposed a class of index policies generalizing Brown and

Smith (2020) and Hu and Frazier (2017), showing that this larger class of policies have at most a
O(

N ) opt gap and, surprisingly, a O(1) opt gap if a non-degeneracy condition is met.

√

Unlike the Whittle index, these index policies do not require an indexability condition to be

well-deﬁned. Moreover, they come with performance guarantees that do not require verifying extra

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

7

√

suﬃcient conditions: O(
2017, O(

√

N log N ) for Zayas-Caban et al. 2019.

N ) for Zhang and Frazier 2021, Brown and Smith 2020, Hu and Frazier

We build on ideas in these ﬁnite-horizon papers to develop policies and analysis for the inﬁnite-

horizon setting that avoids the drawbacks of past inﬁnite-horizon work: we develop policies that

are asymptotically optimal and do not require indexability or other suﬃcient conditions.

This requires substantial additional analysis. One might hope to simply truncate the inﬁnite-
N ) ﬁnite-horizon

horizon problem, apply a previously proposed ﬁnite-horizon policy with a O(

√

performance guarantee to this truncated problem, and choose the truncation point large enough

that the reward obtained afterward is a small part of the overall reward. This, however, does not

produce a guarantee of asymptotic optimality in the inﬁnite-horizon setting.

√

Indeed, previously proposed policies known to have O(

N ) opt gap in the ﬁnite-horizon setting

have performance guarantees that depend exponentially on T : the opt gap for a problem with
N )O(exp(αT )) with α > 0 for both Brown and Smith (2020) (see its Proposition
horizon T is O(

√

5) and Zhang and Frazier (2021) (see its proof of Lemma 5). Thus, applying either policy and its

√

associated bound to an inﬁnite-horizon discounted problem truncated at T with discount factor γ
N )O(exp(αT )) on the opt gap realized up to the truncation time and
would have a bound of O(
a bound of O(N γT ) on the opt gap realized after the truncation time. Choosing T to minimize the
sum of these bounds would not provide a O(

N ) opt gap bound.

√

3. System Model

This section formulates the restless bandit problem as a Markov decision process (MDP).

Model The decision maker faces N arms. Each arm is as an MDP, which is associated with a

state space and an action space. The arms share the same ﬁnite state space S and the same binary
action space A = {0, 1}. For arm i, we let st,i ∈ S denote its state and at,i ∈ A the action applied in
period t ∈ {1, 2, 3, ...}.

As we move from time period t to t + 1, each arm i transitions to its new state st+1,i inde-
pendently given its current state st,i and the action applied at,i. We use a kernel to describe

this stochastic transition. The kernel is assumed known to the decision maker and is denoted by
P = {p(s, a, s′)}s,s′∈S,a∈A where

p(s, a, s′) ∶= P[st+1,i = s′∣st,i = s, at,i = a],

and p(s, a, s′) gives the probability of an arm transitioning to state s′ conditioned on its current

state being s and action a being taken. We assume that each arm shares the same transition kernel

and the transition kernel is time-homogeneous. Thus, as we write above, the transition kernel P

does not depend on the arm index i or time index t. For simplicity, we assume each arm starts

8

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

from a common state s∗ at t = 1. Our analysis also applies if each arm’s initial state is chosen

independently from a common distribution.

The decision maker must pull ⌊αN ⌋ arms in each period, which we refer as the budget constraint.

The constant α is also known to the decision maker.

In each period, an arm generates a real-valued reward r(s, a) that is a deterministic function
of its state s ∈ S and the action applied a ∈ A. The decision maker’s objective is to maximize the

total inﬁnite-horizon expected discounted reward collected across all N arms with discount factor

γ while respecting the budget constraint in each period.

To formulate this N -arm decision-making problem as a MDP, we introduce some additional
notation. These N arms form a new MDP, which we refer as the joint MDP. The state space S of
this joint MDP is the Cartesian product of N single-arm state spaces S: S = S × S × ... × S. Similarly,
the action space A of the joint MDP is the Cartesian product of N single-arm action spaces A:
A = A × A × ... × A. At period t, we denote the state of the joint MDP as st = (st,1, ..., st,N ) and the
action of the joint MDP as at = (at,1, ..., at,N ), where i-th components of st and at refer respectively

to the state of arm i and the action applied to it.

Since the state of each arm evolves independently given its previous state and the action applied,

the probability of transitioning from one state to another in the joint MDP is the product of the

each arm’s transition probability. Mathematically speaking,

P[st+1∣st, at] =

N
∏
i=1

P[st+1,i∣st,i, at,i].

To clearly describe the budget constraint, we introduce a norm in the state space A. For an

element a = (a1, a2, ..., aN ) ∈ A, its norm ∣a∣ ∶= ∑N
i=1 ai is the sum of its components, noting that these
components are non-negative. This norm ∣at∣ gives the number of arms pulled in period t. Thus,
we can write our budget constraint as ∣at∣ = ⌊αN ⌋ for each period t.

malize this, we deﬁne the joint MDP’s reward function R ∶ S × A → R via R(st, at) = ∑N

The reward of the joint MDP is the sum of the rewards generated by each individual arm. To for-
i=1 r(st,i, at,i).
A policy π ∶ S × {1, 2, ...} → A is a mapping from the product of the state space and set of
possible times to the action space. Under a policy π, the action taken in period t is at = π(st, t).

The decision maker’s objective is to choose a policy π that maximizes the joint MDP’s total

expected discounted reward while respecting budget constraints. Mathematically speaking, this is

the following stochastic constrained optimization problem,

max
π

Eπ

γtR(st, at)

∞
∑
t=1
s.t. ∣at∣ = ⌊αN ⌋, ∀t;

(1)

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

9

where Eπ takes the expectation under the distribution on states, actions, and rewards induced by

the policy π.

To measure the performance of a policy π, we deﬁne its value function,

VN (π) ∶= Eπ

∞
∑
t=1

γtR(st, at),

as the expected total discounted reward collected by this policy. An optimal policy has performance

V ∗
N

∶= supπ VN (π). We deﬁne the optimality gap (opt gap) of a policy π as

V ∗
N

− VN (π),

i.e., the diﬀerence in performance between this policy and an optimal policy. The smaller the opt

gap, the better the policy.

As a MDP with a large but ﬁnite state space, Problem 1 can be solved in principle via dynamic

programming. However, the time complexity of this approach grows exponentially with the number

of arms N because of the so-called curse of dimensionality (Powell 2007): the joint MDP has a state
space whose cardinality is ∣S∣N . Thus, we would like to ﬁnd policies that are both computationally

tractable and have strong theoretical performance guarantees in the regime with many arms.

4. Background: Preliminary Results and Notation

This section describes a linear programming relaxation of the restless bandit problem. This is a

standard technique from the restless bandit literature. It is not part of our contribution and we

introduce it simply to provide a self-contained treatment of our research contribution.

The relaxation provides an upper bound on an optimal policy’s performance, which can in turn

bound the opt gap of any feasible policy. Also, the relaxation can be solved eﬃciently and its

solution will provide insights into the design of an asymptotically optimal policy in later sections.

Linear Programming Relaxation Following (Hu and Frazier 2017, Zayas-Caban et al. 2019,

Brown and Smith 2020, Zhang and Frazier 2021), which apply linear programming relaxations to

ﬁnite-horizon restless bandits, we describe an equivalent relaxation for the inﬁnite-horizon problem.

We emphasize that this is not part of our research contribution and is introduced so that we can

deﬁne notation and so that our treatment can be self-contained.

Instead of solving the original problem (1) with cardinality constraints on the number of arms

pulled in each period, we consider a modiﬁed version where these cardinality constraints are relaxed

to constraints on the expected number of arms pulled:

ˆVN ∶= max

π

Eπ

γtR(st, at)

∞
∑
t=1
s.t. Eπ∣at∣ = αN, ∀t.

(2)

10

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

From now on we assume α is a rational number and N (an integer) is chosen such that αN is also

an integer. Thus, we drop the ﬂoor operator in the constraint (2). When αN is not an integer,

analysis in Appendix 9.2 shows that the rounding error caused by the ﬂoor operator does not aﬀect

any asymptotic analysis in later sections.

To support computation, we consider a version of problem (2) that is truncated at some horizon
T ≤ ∞, introducing approximation error discussed below. In some problems with special structure,
truncation will be unnecessary for computation allowing us to choose T = ∞, while in others it will
be necessary, requiring T < ∞. We denote the truncated relaxed problem and truncated original

problem as

and

ˆVN (T ) ∶= max

Eπ

γtR(st, at)

T
∑
t=1
s.t. Eπ∣at∣ = αN, ∀t;

π

V ∗
N

(T ) ∶= max

γtR(st, at)

Eπ

T
∑
t=1
s.t. ∣at∣ = αN, ∀t.

π

(3)

(4)

This truncated relaxed problem (regardless of the value of T ) can be decomposed across arms

by an analysis similar to Fenchel duality, allowing us to solve (3) with N arms via an equivalent

single-arm problem. Second, since the feasible policies for the truncated relaxed problem (3) is a

superset of the feasible policies for the truncated original problem (4), the value of (3) provides an

upper bound on the value of (4). We state these properties formally in Lemma 1, whose proof is

left to Appendix 9.1.

Lemma 1. V ∗
N

(T ) ≤ ˆVN (T ) = N ˆV1(T ).

The single-arm truncated relaxed problem ˆV1(T ) can also be formulated as a linear program.
Choosing the components of the occupation measure xt(s, a) ∶= Pπ[st = s, at = a] as decision vari-
ables, the single-arm truncated relaxed problem can be formulated as

max

T
∑
t=1
s.t. ∑
a
∑
s
∑
a

xt(s, a)r(s, a)

γt ∑
s,a

xt+1(s′, a) = ∑

xt(s, a)p(s, a, s′), ∀t ≤ T − 1, s ∈ S;

s,a

xt(s, 1) = α, ∀t ≤ T ;

x1(s∗, a) = 1, ∑

x1(s, a) = 1, ∑

xt(s, a) ≥ 0, ∀t ≤ T.

s,a

s,a

(5)

The ﬁrst constraint ensures ﬂow balance in each time period. The second constraint ensures that

the budget constraint on the expected number of arms pulled is satisﬁed in each period. The third,

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

11

forth and ﬁfth constraints ensure that the occupation measure forms a probability measure in each
period. Denoting the solution to (5) by {xt(s, a)}t,s,s, we have ˆV1(T ) = ∑T
t=1 γt ∑s,a xt(s, a)r(s, a).
Additional Notation Starting in the next section, we analyze the optimal occupation measure

and the number of arms in each state under a stochastic sample path. To support this analysis, we

introduce some additional notations here.

Given an optimal occupation measure solving (5), we let zt(s) ∶= ∑a xt(s, a) denote the probability
that an arm being in state s at period t. For notational simplicity, we use the vectors zt = (zt(s))s
and xt = (xt(s, a))s,a to denote the distribution over an arm’s state and the distribution over an
arm’s state-action pair given an optimal occupation measure.

path. We let Z N
t

We are also interested in the realized number of arms in each state under a stochastic sample
(s, a) denote the
(s) denote the number of arms in state s in period t and let X N
t
(s) =
(s, a) for any t and s. Similar to the vector notation used to describe an optimal occupation

number of arms in state s for which we took action a taken in period t. We have that Z N
t
∑a X N
t
measure, we use the vectors Z N
t

(s))s and X N

(s, a))s,a.

= (X N
t

= (Z N
t

t

Starting from Section 5, we will be interested in deviations between the realized number of

arms under a stochastic sample path from an optimal occupation measure. To characterize this

deviation, we deﬁne the following diﬀusion statistics:

(s) = Z N

t

˜Z N
t

√

(s) − N zt(s)
N

, ˜X N
t

(s, a) = X N

t

(s, a) − N xt(s, a)
N

√

.

We also use the vectors ˜Z N
t

= ( ˜Z N
t

(s))s and ˜X N

t

= ( ˜X N
t

(s, a))s,a for notational simplicity.

With this new notation, we can rewrite a policy π in term of its diﬀusion statistics. Given a

policy π ∶ S × {1, 2, ...} → A, we can rewrite it as a sequence of mappings {˜πt,N }t s.t.

π(Z N

t , t) = X N

t

⇐⇒ ˜πN
t

( ˜Z N
t

) = ˜X N
t .

We refer to the sequence {˜πt,N }t as the induced maps from policy π. Section 5 characterizes a class
of policies satisfying a property called diﬀusion regularity, which is deﬁned in term of their induced

maps.

5. Diﬀusion Regularity Conditions

This section deﬁnes a property called diﬀusion regularity and shows that policies possessing this

property satisfy a bound on their corresponding diﬀusion statistics’ ﬁrst moments. This diﬀusion

regularity property is shown to be satisﬁed by the ﬂuid-balance policies proposed in section 6 and

the bound shown here is a tool used to understand their performance theoretically in that section.

The intuition behind the diﬀusion regularity condition is that as long as ˜X N

t remains bounded by
t+1 is also bounded by another term that does not grow

a term that does not grow with N , then ˜Z N

12

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

with N . The ﬁrst three conditions in the diﬀusion regularity condition are similar to conditions

proposed in Zhang and Frazier (2021) and the last condition is added speciﬁcally for our inﬁnite-
horizon setting to guarantee that diﬀusion statistics ˆZ N

t accumulate noise at no more than a linear

rate over time.

We now deﬁne diﬀusion regularity.
Definition 1. A policy π is diﬀusion regular up to period T if its induced maps ˜πt,N satisfy the

following conditions, where ∣ ⋅ ∣ is the Euclidean L1-norm:

• For any t ≤ T , there is a constant C1 > 0 such that for all N, θ1 and θ2,

∣˜πt,N (θ1) − ˜πt,N (θ2)∣ ≤ C1∣θ1 − θ2∣;

• For any t ≤ T , there is a constant C2 > 0 such that for all N ,

∣˜πt,N (0)∣ ≤ C2;

• For any t ≤ T , there is a map ˜πt,∞ such that for all θ, ˜πt,N (θ) → ˜πt,∞(θ) as N → +∞;
• For any t ≤ T , there is a constant C3 > 0 such that for all N, θ and s ∈ S, we have

∣˜πt,N (θ)(s, a)∣ ≤ ∣θ(s)∣ + C3.

∑
a∈A

If a policy π is diﬀusion regular up to period T , its diﬀusion statistics’ ﬁrst moments are bounded

above by a linear function of time (Lemma 2). The proof of Lemma 2 may be found in the Appendix.

Lemma 2. If a policy π is diﬀusion regular, then there exists constant c1 and c2 (neither depends

on T ), s.t. for all t ≤ T and N (N could be inﬁnity),

6. Fluid-balance policy

E[∣ ˜Z N
t

∣] ≤ c1 + c2t.

This section deﬁnes ﬂuid-balance policies, and shows that all ﬂuid-balance policies are diﬀusion
regular and achieve an O(

N ) + O(N γT ) opt gap.

√

Roughly speaking, a ﬂuid-balance policy is parameterized by two components: an optimal solu-

tion of the LP relaxation and a prioritization scores over states. The resulting ﬂuid-balance policy

pulls arms respecting two rules: a consistency rule and a prioritization rule. The consistency rule
requires that the diﬀusion statistics ˜X N
(s) for each state s. The
t

(s, ⋅) share the same sign as ˜Z N
t

prioritization rule requires pulling arms according to the prioritization score as much as possible

while respecting the consistency rule.

Formally, a ﬂuid-balance policy is parameterized by an occupation measure {xt(s, a)}t,s,a solving
truncated Problem (5) up to period T and “priority-score” functions {Pt(⋅)}t≤T assigning each

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

Algorithm 1 Fluid-balance policy

Input: optimal occupation measure (xt(s, a))t≤T,s∈S,a∈A, priority-score functions {Pt}t≤T .

1: for t = 1, 2, ..., T do

2:

3:

4:

5:

6:

7:

8:

Input: the number Zt(s) of arms in state s and their associated diﬀusion statistics ˜Z N
For each state s, set Xt(s, 1) ← min{Zt(s), ⌈xt(s, 1)N +
while ∑s Xt(s, 1) > ⌊αtN ⌋ do

N ∣ ˜Z N
t

(s)∣⌉}

√

t

Find the state s with the lowest priority-score such that

Xt(s, 1) > max{0, ⌊xt(s, 1)N −

√

N ∣ ˜Z N
t

(s)∣⌋}

Xt(s, 1) ← Xt(s, 1) − 1

end while
For each state s, pull Xt(s, 1) arms in state s

13

(s)

9: end for

state a time-dependent real number. The ﬂuid-balance policy with these components is deﬁned by

Algorithm 1.

We can show that any ﬂuid-balance policy is diﬀusion regular and thus satisﬁes the bound on
the expected L1− norm of its diﬀusion statistic provided in Lemma 2. Moreover, it actually satisﬁes

a more explicit bound than Lemma 2. These statements are shown in the following lemma, whose

proof appears in the Appendix.

Lemma 3. Any ﬂuid-balance policy π is diﬀusion regular, and Eπ[∣ ˜Z N

t

∣] ≤ 2t∣S∣2 for t ≤ T .

Now we are able to show our main results

Theorem 1. Given any ﬂuid-balance policy π, V ∗
N

− VN (π) = O(

√

N ) + O(N γT ).

Proof of Theorem 1 Denote π∗ the optimal policy maximizing the inﬁnite-horizon Problem (1).

Then by denoting B ∶= maxs,a ∣r(s, a)∣,

V ∗
N

− VN (π) = Eπ∗

T
∑
t=1

γtr(si,t, ai,t) − Eπ

T
∑
t=1

γtr(si,t, ai,t) + Eπ∗

∞
∑
t=T +1

γtr(si,t, ai,t) − Eπ

∞
∑
t=T +1

γtr(si,t, ai,t)

≤ ˆVN (T ) − Eπ

T
∑
t=1

γtr(si,t, ai,t) + 2

∞
∑
t=T +1

γtBN

where the last inequality is due to the deﬁnition of ˆVN (T ).

We deal with these two terms ˆVN (T ) − Eπ ∑T

t=1 γtr(si,t, ai,t) and 2 ∑∞

t=T +1 γtBN separately. For

the ﬁrst term,

ˆVN (T ) − Eπ

T
∑
t=1

γtr(si,t, ai,t) = −

N Eπ

√

T
∑
t=1

∑
s,a

γtr(s, a) ˜X N
t

(s, a) ≤

√

N BEπ [

T
∑
t=1

γt∣ ˜Z N
t

∣] .

14

Recall Lemma 3, we have

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

ˆVN (T ) − Eπ

T
∑
t=1

γtr(si,t, ai,t) ≤

√

N BEπ [

T
∑
t=1

γt∣ ˜Z N
t

∣] ≤

√

N B

T
∑
t=1

γt2t∣S∣2 ≤

√

N

2∣S∣2γB
(1 − γ)2

√

= O(

N ).

For the second term,

2

∞
∑
t=T +1

γtBN = 2Bγ
1 − γ

N γT = O(N γT ).

Combining above analysis together, we conclude V ∗
N

− VN (π) = O(

Based on Theorem 1, we can show the following proposition:

√

N ) + O(N γT ).

Corollary 1. Choosing T = 1

2 log 1

γ

N implies that for any ﬂuid-balance policy π,we have

V ∗
N

− VN (π) = O(

√

N ).

◻

(6)

7. Numerical Experiment

This section illustrates the performance of ﬂuid-balance policies through two numerical experi-

ments, focusing on their advantage over the Whittle index policy.

The ﬁrst experiment studies a simple non-indexable example encapsulating a tradeoﬀ that is

important in many more complex real-world decision problems. The decision-maker can either

generate a substantial reward over a short time horizon or generate a steady small amount of

reward over a very long horizon. We refer to this as the Slow-and-steady Problem, borrowing from

the idiom “slow and steady wins the race”. We show this example is not indexable, and thus the

Whittle index is not well-deﬁned. However, the ﬂuid-balance policy can be computed analytically
without truncation (i.e., the truncation point is T = ∞) and is actually the optimal policy.

In the second experiment, we compare the ﬂuid-balance policy against the Whittle index for a

discounted version of a problem studied in Fu et al. (2019) and Biswas et al. (2021). Although this

problem is indexable, the ﬂuid-balance policy outperforms the Whittle index by over 30%. This

problem is drawn from the literature studying bandits with unknown transition kernels, where

the Whittle index policy’s performance is used to represent the performance achievable when

transition kernels are known. Our results suggest that the ﬂuid-balance policy better represents

the performance achievable with full information and thus might be a better benchmark than the

Whittle index policy.

7.1. The Slow-and-Steady Problem: Large Fleeting Rewards vs. Small Steady

Rewards

This section constructs a simple non-indexable problem reﬂecting an important tradeoﬀ arising

in real-world decision making: should we generate one large reward immediately, or generate a

sequence of small rewards over a much longer period. We refer to this as the “slow-and-steady

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

15

problem,” echoing the proverb “slow and steady wins the race” often uttered when considering

such tradeoﬀs. We show that the Whittle index is not well deﬁned for the slow-and-steady problem

while the ﬂuid-balance policy we consider is not just optimal asymptotically but also optimal for

ﬁnite N .

Problem Deﬁnition In the slow-and-steady problem there are two states that generate non-zero

rewards: the “Steady” state and the “Brief“ state. Once an arm enters the Steady state, it stays
there and generates a reward of α ≥ 0 each time it is activated. An arm in the Brief state stays in
the Brief state until it is activated, at which point it generates a reward of β ≥ 0 and transitions

to the “End” state. We think of α as being small and β as being large. Once an arm is in the End

state, it stays in that state.

Before transitioning into either the Steady or Brief states, an oscillates between the

“Uncommitted-Steady” and “Uncommitted-Brief” states until the arm is activated. When acti-

vated, the arm transitions into the End state with a small probability (cid:15) and transitions into the

Steady state (if it was in the Uncommitted-Steady state when it was activated) or the Brief state

(if it was in the Uncommitted-Brief state when it was activated) otherwise. For technical reasons,

we also have a “Pre-Steady” state, which always transitions into the Steady state regardless of

whether it was activated.

Figure 1 shows the transition dynamics between states, where nodes represent states and edges

between nodes represent transition probabilities. Formally, the transition kernel is given by:

P[st+1 = Steady ∣ st = Steady, at = a] = 1, ∀a ∈ {0, 1},

P[st+1 = End ∣ st = Brief, at = 1] = 1, P[st+1 = Brief ∣ st = Brief, at = 0] = 1,

P[st+1 = End ∣ st = End, at = a] = 1, ∀a ∈ {0, 1},

P[st+1 = Uncommitted-Brief ∣ st = Uncommitted-Steady, at = 0] = 1,

P[st+1 = Uncommitted-Steady ∣ st = Uncommitted-Brief, at = 0] = 1.

P[st+1 = Steady ∣ st = Uncommitted-Steady, at = 1] = 1 − (cid:15),

P[st+1 = End ∣ st = Uncommitted-Steady, at = 1] = (cid:15),

P[st+1 = Brief ∣ st = Uncommitted-Brief, at = 1] = 1 − (cid:15),

P[st+1 = End ∣ st = Uncommitted-Brief, at = 1] = (cid:15),

P[st+1 = Steady ∣ st = Pre-Steady, at = a] = 1.

Formally, the reward function is given by

r(Steady, 1) = α, r(Brief, 1) = β,

16

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

Figure 1

State transition diagram for Slow-and-Steady Problem. Each node stands for a state. Edges between

nodes stand for transitions: dashed lines represent transitions when an arm is idled; solid lines represent transition

when an arm is activated. Unless the transition probability is speciﬁed on the edge, it is 1. Similarly, unless the

reward is speciﬁed on an edge, it is 0.

with rewards for all other state-action pairs set to 0. We seek to maximize the expected inﬁnite-
horizon discounted reward with discount factor γ = 1 − (cid:15).

We assume our parameters satisfy 0 < (1 + 1
1−γ α, i.e. the Brief state generates
γ2
a somewhat larger reward than Steady state, but not too much larger. Also we assume (cid:15) < 1
8 ,
i.e. the Uncommitted-Brief and Uncommitted-Steady states transition into the Brief and Steady

) α < β <

γ

states with a reasonably high probability. In these parameter ranges, and for the budget and initial

occupation measure chosen below, we show below that the problem is not indexable.

The budget is set so that we can pull ⌊γN ⌋ arms out of the N total arms in each period. In the
initial time period, ⌊φ1N ⌋ arms in the Uncommitted-Steady state, ⌊φ2N ⌋ arms are in the End state
and ⌊φ3N ⌋ arms are in the Pre-Steady state, where φ1 = 2 − 1
− 2 and φ3 = 1 − γ. These
parameters {φi}i=1,2,3 are chosen so that activating all arms in the Pre-Steady and Uncommitted-
Steady states respects the constraint.

γ , φ2 = γ + 1

γ

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

17

Analysis We ﬁrst show, in the following proposition, that this problem is not indexable. Thus,

the Whittle index is not well-deﬁned. Proofs of this and other propositions in this section appear

in the appendix.

Proposition 1. The slow-and-steady problem is not indexable.

Towards deﬁning a ﬂuid-balance policy, we show in the next proposition that the inﬁnite-horizon

linear programming relaxation of the slow-and-steady problem deﬁned above permits an analytical

solution. This allows deﬁning ﬂuid balance policies without truncation (i.e., the truncation point

used is T = ∞).

Proposition 2. Consider the policy that pulls all arms in the Uncommitted-Steady and

Pre-Steady in the ﬁrst period, then pulls all arms in Steady from the second period onward. This

policy is optimal in the linear programming relaxation (2) of the slow-and-steady problem.

Using this optimal policy for the relaxed problem, we construct a ﬂuid-balance policy. This

ﬂuid balance policy π activates all arms in Uncommitted-Steady and Pre-Steady states in the ﬁrst

period, then activates as many arms in the Steady state as possible starting from the second period.

If there are fewer than ⌊γN ⌋ arms in the Steady state, it activates arms in the End state to meet

the budget.

Theorem 1 implies that the ﬂuid-balance policy π is asymptotically optimal, i.e., its opt gap is
√

N ). Surprisingly, this ﬂuid-balance policy is not only asymptotically optimal, but optimal,

O(

i.e., its opt gap is 0. This is shown in the following proposition.

Proposition 3. The ﬂuid-balance policy π is an optimal policy for the slow-and-steady problem.

The bound on the opt gap of a ﬂuid-balance policy in Theorem 1 is derived by comparing a

ﬂuid-balance policy’s reward expected total discounted reward against the optimal reward of the

linear programming relaxation (which is an upper bound on the value of an optimal policy) rather

than the value of an optimal policy in the original (unrelaxed) problem. Since the ﬂuid-balance

policy π is optimal in the slow-and-steady problem, its opt gap is 0. The optimal reward of the

linear programming relaxation, however, is strictly bigger than that of an optimal policy, causing

the gap to the linear programming relaxation to remain O(

N ). This is shown in the following

√

proposition.

Proposition 4. ˆV ∗
N

− VN (π) = θ

√

N + O(1), where θ =

√

(1−γ)(2γ−1)
2π

.

18

7.2. Bandit literature benchmark

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

There is a stream of literature studying problems similar to the one we study, but where state

transitions are unknown, e.g. Fu et al. (2019) and Biswas et al. (2021). Rather than designing

policies based on knowledge of the problem’s state transition kernel, as in the restless bandit

problem we study, this stream of literature designs algorithms that estimate the state transition

kernel while simultaneously choosing actions and collecting rewards. A common practice in this

literature is to benchmark a proposed algorithm’s performance against a full-information policy,

such as the Whittle index policy, on a speciﬁc problem.

In this section, we study a problem based on one of the most commonly used benchmark problems

from this literature. This problem is based on Fu et al. (2019) and Biswas et al. (2021), who study
an undiscounted version in which N /10 arms are pulled per period. In the setting where the budget
is N /10 arms are pulled per period, a numerical study shows that the Whittle index is actually

also a ﬂuid balance policy, and thus is asymptotically optimal by Theorem 1. When the budget is
N /2 arms per period, however, we ﬁnd that the ﬂuid-balance policy outperforms the Whittle index

by over 30%, suggesting that ﬂuid-balance policies provide better full-information benchmark.

Problem Deﬁnition In the problem that we study, there are 4 diﬀerent states: {0, 1, 2, 3}. Tran-
s,s′∈S for action a = 0 and a = 1 are given by

sition kernels Pa = p(s, 0, s′)

P0 =

1/2
0
⎛
1/2 1/2
⎜
⎜
⎜
0
⎝
0

0
0
1/2 1/2
0

1/2
0
0
1/2 1/2

⎞
⎟
⎟
⎟
⎠

, P1 =

1/2 1/2
⎛
⎜
0
⎜
⎜
0
⎝
1/2

0
1/2 1/2
0
0

0
0
1/2 1/2
1/2
0

⎞
⎟
⎟
⎟
⎠

.

The reward solely depends on the state and is unaﬀected by the action:

r(0, a) = −1, r(1, a) = 0, r(2, a) = 0, r(3, a) = 1; ∀a ∈ {0, 1}.

We set the discount factor to γ = 1/2 and require N /2 arms to be pulled per period. Initially,

there are N /6 arms in state 0, N /3 arms in state 1 and N /2 arms in state 2.

Analysis Via direct calculation, we can show that this problem is indexable. Ranking states from

the highest to the lowest according to the Whittle index, we have that the Whittle index prioritizes

state 2 over state 1, and state 1 over state 3. It is unknown whether the Whittle index policy is

asymptotically optimal in this problem, though numerical experiments below suggest that it is not.

To compute the ﬂuid-balance policy, Since this inﬁnite-horizon problem’s linear programming
relaxation does not permit an analytical solution, we solve the truncated version up to T = 100.

This provides an accurate approximation of the upper bound implied by the linear programming

relaxation because the total reward after period 100 is less than 2−100, much smaller than the

precision 10−17 of a 64-bit ﬂoating point number.

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

19

After solving the truncated relaxation problem, we need a ﬁnal piece before implementing a

ﬂuid-balance policy: the prioritization score. We adhere to Whittle index here, where we rank states
from high to low as state 2 > state 1 > state 0 > state 3.

Figure 2 compares the performance of the Whittle index and ﬂuid-balance policies. Simulation

with 2000 independent replications is used to estimate the performance of each policy via the

sample mean and 95% conﬁdence intervals. As we can see, the ﬂuid-balance policy outperforms

the Whittle index in both the small-N and large-N regimes.

(a)

(b)

Figure 2

Performance comparison between Whittle index and ﬂuid-balance policy. The left panel shows the

average reward per arm versus number of arms (N ), where we compare the upper bound from the linear

programming relaxation with the performance of the Whittle index and ﬂuid-balance policies as estimated via

simulation. The right panel shows an upper bound on the opt gap (the upper bound from the linear programming

relaxation minus a simulation-based estimate of the expected total discounted reward) versus the number of arms

(N ). The Whittle index’s opt gap grows linearly with N , consistent with a lack of asymptotic optimality, while the

ﬂuid-balance policy’s opt gap grows sublinearly, consistent with asymptotic optimality and our result from

Theorem 1 that its opt gap is O(

N ).

√

8. Conclusion and Future Work
In this paper, we propose a class of policies, called ﬂuid-balance policies, which achieve an O(

√

N )

opt gap universally as the number of arms N grows large. Unlike the Whittle index policy, ﬂuid-

balance policies do not require an indexability condition to be well-deﬁned and our results show

they are asymptotically optimal without the need for diﬃcult-to-verify suﬃcient conditions.

Although we restrict our analysis to restless bandits, we believe the techniques and insights

we develop here can be generalized to multi-action multi-resource restless bandits, also known as

weakly coupled Markov Decision Processes. Another interesting direction for future work would

20

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

be characterizing a non-degeneracy condition, similar to Zhang and Frazier (2021), suﬃcient for a
ﬂuid balance policy to achieve a O(1) optimality gap in an inﬁnite-horizon restless bandit.

References
Ansell P, Glazebrook KD, Nino-Mora J, O’Keeﬀe M (2003) Whittle’s index policy for a multi-class queueing

system with convex holding costs. Mathematical Methods of Operations Research 57(1):21–39.

Biswas A, Aggarwal G, Varakantham P, Tambe M (2021) Learn to intervene: An adaptive learning policy

for restless bandits in application to preventive healthcare. arXiv preprint arXiv:2105.07965 .

Brown DB, Smith JE (2020) Index policies and performance bounds for dynamic selection problems. Man-

agement Science .

Chen X, Lin Q, Zhou D (2013) Optimistic knowledge gradient policy for optimal budget allocation in

crowdsourcing. International conference on machine learning, 64–72.

Farias VF, Madan R (2011) The irrevocable multiarmed bandit problem. Operations Research 59(2):383–399.

Fu J, Nazarathy Y, Moka S, Taylor PG (2019) Towards q-learning the whittle index for restless bandits.

2019 Australian & New Zealand Control Conference (ANZCC), 249–254 (IEEE).

Gittins J, Glazebrook K, Weber R (2011) Multi-armed bandit allocation indices (John Wiley & Sons).

Glazebrook KD, Ruiz-Hernandez D, Kirkbride C (2006) Some indexable families of restless bandit problems.

Advances in Applied Probability 38(3):643–672.

Guha S, Munagala K (2007) Approximation algorithms for budgeted learning problems. Proceedings of the

thirty-ninth annual ACM symposium on Theory of computing, 104–113 (ACM).

Guha S, Munagala K (2008) Sequential design of experiments via linear programming. arXiv preprint

arXiv:0805.2630 .

Guha S, Munagala K, Shi P (2010) Approximation algorithms for restless bandit problems. Journal of the

ACM (JACM) 58(1):3.

Hu W, Frazier P (2017) An asymptotically optimal index policy for ﬁnite-horizon restless bandits. arXiv

preprint arXiv:1707.00205 .

Jacko P, Nino-Mora J (2007) Time-constrained restless bandits and the knapsack problem for perishable

items. Electronic Notes in Discrete Mathematics 28:145–152.

Le Ny J, Dahleh M, Feron E (2006) Multi-agent task assignment in the bandit framework. Proceedings of

the 45th IEEE Conference on Decision and Control, 5281–5286 (IEEE).

Le Ny J, Dahleh M, Feron E (2008) Multi-uav dynamic routing with partial observations using restless

bandit allocation indices. 2008 American Control Conference, 4220–4225 (IEEE).

Liu K, Zhao Q (2008) A restless bandit formulation of opportunistic access: Indexablity and index policy. 2008

5th IEEE Annual Communications Society Conference on Sensor, Mesh and Ad Hoc Communications

and Networks Workshops, 1–5 (IEEE).

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

21

Liu K, Zhao Q (2010) Indexability of restless bandit problems and optimality of whittle index for dynamic

multichannel access. IEEE Transactions on Information Theory 56(11):5547–5567.

Meshram R, Kaza K (2020) Simulation based algorithms for markov decision processes and multi-action

restless bandits. arXiv preprint arXiv:2007.12933 .

Nakhleh K, Ganji S, Hsieh PC, Hou IH, Shakkottai S (2021) Neurwin: Neural whittle index network for

restless bandits via deep rl. Thirty-Fifth Conference on Neural Information Processing Systems.

Nino-Mora J (2001) Restless bandits, partial conservation laws and indexability. Advances in Applied Prob-

ability 33(1):76–98.

Ni˜no-Mora J (2007) Dynamic priority allocation via restless bandit marginal productivity indices. Top

15(2):161–198.

Powell WB (2007) Approximate Dynamic Programming: Solving the curses of dimensionality, volume 703

(John Wiley & Sons).

Rockafellar RT (1970) Convex analysis princeton university press. Princeton, NJ .

Verloop IM (2016) Asymptotically optimal priority policies for indexable and nonindexable restless bandits.

The Annals of Applied Probability 26(4):1947–1995.

Wang K, Shat S, Chen H, Perrault A, Doshi-Velez F, Tambe M (2021) Learning mdps from features:

Predict-then-optimize for sequential decision problems by reinforcement learning. arXiv preprint

arXiv:2106.03279 .

Weber RR, Weiss G (1990) On an index policy for restless bandits. Journal of Applied Probability 27(3):637–

648.

Whittle P (1980) Multi-armed bandits and the gittins index. Journal of the Royal Statistical Society: Series

B (Methodological) 42(2):143–149.

Zayas-Caban G, Jasin S, Wang G (2019) An asymptotically optimal heuristic for general nonstationary ﬁnite-

horizon restless multi-armed, multi-action bandits. Advances in Applied Probability 51(3):745–772.

Zhang X, Frazier PI (2021) Restless bandits with many arms: Beating the central limit theorem. arXiv

preprint arXiv:2107.11911 .

22

9. Appendix

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

This section provides all technical proof in the main paper.

9.1. Proof for Lemma 1

In original problem (1) the budget constraint is in sense of cardinality, while the expectation

constraint is need for relaxation problem (3). So a wider class of policy is feasible in the relaxation

problem, which implies

V ∗
N

(T ) ≤ ˆV ∗
N

(T ).

(7)

To prove ˆV ∗
N

(T ) = N ˆV ∗
1

(T ), we use Lagrangian Relaxation similar to Farias and Madan (2011),

Guha and Munagala (2008) as the key idea in the following argument.

Through imitating straightforwardly the proof of Fenchel Duality Theorem (Rockafellar 1970),

max
π

min
λ

Eπ

T
∑
t=1

γtR(st, at) + λt(αtN − ∣at∣) = min

λ

max
π

Eπ

T
∑
t=1

γtR(st, at) + λt(αtN − ∣at∣)

(8)

where λ = (λ1, λ2, ..., λT ).

The let-hand side of Equation (8) equals to ˆV ∗
N

(T ). On the right hand side, for ﬁxed λ,

Eπ

T
∑
t=1

γtR(st, at) + λt(αtN − ∣at∣) = Eπ

T
∑
t=1

N
∑
i=1

γtr(st,i, at,i) + λt(αt − at,i).

Since all arms share the same transition kernel and reward function,

Eπ

T
∑
t=1

N
∑
i=1

γtr(st,i, at,i) + λt(αt − at,i) = N Eπ

T
∑
t=1

γtr(st,1, at,1) + λt(αt − at,1).

So we conclude

min
λ

max
π

Eπ

T
∑
t=1

γtR(st, at) + λt(αtN − ∣at∣) = N min

λ

max
π

Eπ

T
∑
t=1

γtr(st,1, at,1) + λt(αt − at,1).

(9)

By using Fenchel Duality again on the one-arm problem,

ˆV ∗
1

(T ) = max

π

Eπ

min
λ

= min
λ

max
π

Eπ

T
∑
t=1
T
∑
t=1

γtr(st,1, at,1) + λt(αt − at,1)

γtr(st,1, at,1) + λt(αt − at,1).

(10)

To summarize Equation (8), (9) and (10) together,

ˆV ∗
N

(T ) = N ˆV ∗
1

(T ).

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

23

9.2. Discussion of the rounding error in budget constraints

We want to show a rounding error in the relaxation Problem (3) results in at most a constant

diﬀerence in the optimal objective value. Mathematically speaking, denote
⎧⎪⎪
⎫⎪⎪
Eπ
⎨
⎬
⎪⎪⎩
⎪⎪⎭
⎧⎪⎪
Eπ
⎨
⎪⎪⎩

E∣at∣ = ⌊αtN ⌋, ∀t ≤ T

E∣at∣ = αtN, ∀t ≤ T

γtR(st, at)

γtR(st, at)

(T ) = max

(T ) = max

T
∑
t=1

T
∑
t=1

ˆV ∗
N,R

RRRRRRRRRRR
RRRRRRRRRRR

ˆV ∗
N

π

π

,

⎫⎪⎪
⎬
⎪⎪⎭

.

Then ∣ ˆV ∗
N

− ˆV ∗

N,R

∣ ≤ c, where c does not depend on N . Thus, all our analysis on the asymptotic

regime of opt gap holds true since the LP relaxation upper bound (in rounded version) deviates

from the unrounded version at most a constant away, not aﬀecting the asymptotic analysis.

The proof of the above statement is straight forward. As seen from Lemma 1, there exists a

single-arm pulling strategy which pulls αt arms per period in expectation and achieves objective
value ˆV ∗
1 . Thus, we can pull N − 1 arms according to this strategy and pull the only arm left with
probability ⌊αtN ⌋ − αt(N − 1) at period t. Thus, we show

Similarly, we can show

N − 1
N

N − 1
N

ˆV ∗
N

(T ) − ˆV ∗

N,R

(T ) ≤ maxs,a r(s, a)

1 − γ

ˆV ∗
N,R

(T ) − ˆV ∗
N

(T ) ≤ maxs,a r(s, a)

1 − γ

.

.

Combining the above two inequality concludes the statement.

9.3. Proof of Lemma 2

To prove Lemma 2, ﬁrst notice

Z N
t+1

(s) = ∑

s′∈S,a∈A

N
∑
i=1

1(st,i = s′, at,i = a, st+1,i = s)

(11)

where 1(st,i = s′, at,i = a, st+1,i = s) is the indicator function of event {st,i = s′, at,i = a, st+1,i = s}. By
dynamic equation (11) in a vector form,

Z N
t+1

= ∑

s′∈S,a∈A

BN
t

(s′, a),

(s′, a) is the sum of X N
t

(s′, a) independent ∣S∣-dimensional Bernoulli random variable
where BN
t
with mean (p(s′, a, s))s∈S. For simplicity, we denote p(s′, a) ∶= (p(s′, a, s))s∈S in the following proof.
With this new notation,

BN
t

(s′, a) ∼ Binomial(X N
t

(s′, a), p(s′, a)).

24

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

Recall that X N
t

(s′, a) can be decomposed as N xt(s′, a) +

√

N ˜X N
t

(s′, a). According to Lemma 4,

there exists two random variables C N
t

(s′, a) and ∆N
t

(s′, a), s.t.

BN
t

(s′, a) = C N
t

(s′, a) + ∆N
t

(s′, a),

(12)

and that, conditionally on X N
t

(s′, a), have marginal distribution:

C N
t

(s′, a) ∼ Binomial(⌈N xt(s′, a)⌉, p(s′, a)),

√

∆N
t

(s′, a) ∼ sgn( ˜X N
t

(s′, a))Binomial(⌊

N ∣ ˜X N
t

(s′, a)∣⌋, p(s′, a)).

By equation (12) and recall the deﬁnition of diﬀusion statistics,

˜Z N
t+1

= 1√
N

∑
s′∈S,a∈A

C N
t

(s′, a) − xt(s′, a)p(s′, a)N + 1√
N

∑
s′∈S,a∈S

∆N
t

(s′, a).

Now consider the L1-norm of ˜Z N
t ,

∣ ˜Z N
t+1

∣ ≤ ∣ 1√
N

≤ ∑

∑
s′∈S,a∈S
∣ ˜X N
t

s∈S,a∈A

∆N
t

(s′, a)∣ + 1√
N

∑
s′∈S,a∈A

∣C N
t

(s′, a) − xt(s′, a)p(s′, a)N ∣

(s′, a)∣ + 1√
N

∑
s′∈S,a∈A

∣C N
t

(s′, a) − xt(s′, a)p(s′, a)N ∣

≤ ∣ ˜Z N
t

∣ + C3∣S∣
√
N

+ 1√
N

∑
s′∈S,a∈A

∣C N
t

(s′, a) − xt(s′, a)p(s′, a)N ∣,

where the last inequality is due to diﬀusion regularity of the policy so that ∑a∈A
∣ ˜Z N
t

. Thus,

(s)∣ + C3√
N

∣ ˜X N
t

(s, a)∣ ≤

E∣ ˜Z N
t+1

∣ = E∣ ˜Z N
t

≤ E∣ ˜Z N
t

≤ E∣ ˜Z N
t

∣ + C3∣S∣
√
N
∣ + C3∣S∣
√
N
∣ + C3∣S∣
√
N

+ 1√
N
+ 1√
N

+ 1√
N

∑
s′∈S,a∈A

∑
s′∈S,a∈A

E∣C N
t

(s′, a) − xt(s′, a)p(s′, a)N ∣

(s′, a) − xt(s′, a)p(s′, a)N ∣

E∣C N
t
√

∑
s′∈S,a∈A,s∈S

1 + N xt(s′, a)
4

,

where the last inequality is by Lemma 5.

So

E∣ ˜Z N
t+1

∣ ≤ E∣ ˜Z N
t

= E∣ ˜Z N
t

+ 1√
N

∣ + C3∣S∣
√
N
∣ + C3∣S∣ + 1
2

≤ E∣ ˜Z N
t

∣ + C3∣S∣ + 2∣S∣2.

√

1 + N xt(s′, a)
4

∑
s′∈S,a∈A,s∈S
√

∑
s′∈S,a∈A,s∈S

xt(s′, a) + 1

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

Thus, by induction we have

E∣ ˜Z N
t+1

∣ ≤ t(2∣S∣2 + C3∣S∣) + sup

N

E∣ ˜Z N
1

∣

Taking c1 = supN

E∣ ˜Z N
1

∣ and c2 = 2∣S∣2 + C3∣S∣ concludes the proof.

We state and prove the following Lemma 4 and 5.

25

(13)

◻

Lemma 4. Suppose random variable S is a Binomial random variable with parameter n and p,

i.e., distributed as the sum of n i.i.d. Bernoulli r.v.s with mean p. Then for a given non-negative
integer m, there exists random variable S1 and S2, s.t. S = S1 + S2, and

S1 ∼ Binomial(m, p), S2 ∼ sgn(n − m)Binomial(∣n − m∣, p),

where sgn(⋅) is the sign function.

Proof of Lemma 4 There exists a sequence of i.i.d random variables Xi ∼ Binomial(1, p), s.t.

S =

n
∑
i=1

Xi.

If n > m, taking S1 = ∑m
− ∑m

j=n+1 Xj concludes the proof.

i=1 Xi, S2 = ∑m

j=m+1 Xj concludes the proof. If n ≤ m, taking S1 = ∑m

i=1 Xi, S2 =
◻

Lemma 5. Suppose there are n i.i.d Bernoulli random variable X1, X2, ..., Xn with mean p. Then

E∣

n
∑
i=1

Xi − np∣ ≤

√

n
4

.

Proof of Lemma 5 Direct calculation shows

E∣

n
∑
i=1

Xi − np∣ ≤

¿
`
`(cid:192)E∣

n
∑
i=1

Xi − np∣2 =

√

np(1 − p) ≤

√

n
4

.

9.4. Proof of Lemma 3

Given a ﬂuid-balance policy π, we directly check the induced map ˜πt,N satisﬁes all three conditions

in Deﬁnition 1.

Veriﬁcation of Condition 1 Write the induced map in the component form ˜πt,N = (˜π1

),
(1 ≤ i ≤ ∣S∣) is continuous, piece-wise
and a direct calculation shows each component function ˜πi
linear, and has bounded gradient when exits. Mathematically speaking, there exits a constant ˜C1,
s.t., for any θ, any t and any N ,

t,N , ..., ˜π∣S∣

t,N

t,N

∣∇˜πi

t,N

(θ)∣ ≤ ˜C1, when ∇˜πi

t,N

(θ) exits.

For any θ1 and θ2, there exits a sequence (θ0

1,2, θ1

1,2, ..., θm
1,2

) lies on the line segment between θ1

and θ2, s.t.

26

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

1. ˜πi

t,N restricted on line segment between θj

1,2 and θj+1

1,2 is linear for j = 0, 1, ..., m − 1

2. θ0

1,2

= θ1 and θm

1,2

= θ2.

Thus

∣˜πi

t,N

(θ1) − ˜πi

t,N

(θ2)∣ ≤

m−1
∑
j=0

∣˜πi

t,N

(θj

1,2

) − ˜πi

t,N

(θj+1
1,2

)∣ ≤

m−1
∑
j=0

˜C1∣θj

1,2

− θj+1
1,2

∣ = ˜C1∣θ1 − θ2∣.

So by taking C1 = ∣S∣ ˜C1,

∣˜πt,N (θ1) − ˜πt,N (θ2)∣ ≤

∣S∣
∑
i=1

∣˜πi

t,N

(θ1) − ˜πi

t,N

(θ2)∣ ≤

∣S∣
∑
i=1

˜C1∣θ1 − θ2∣ = C1∣θ1 − θ2∣.

Veriﬁcation of Condition 2 Direct calculation shows ˜πt,N (0) = 0.
Veriﬁcation of Condition 3 Direct calculation shows ˜πt,∞( ˜Z ∞
Veriﬁcation of Condition 4 Direct calculation shows ˜Xt,N (s, 0) and ˜X N

t

) is a piece-wise linear map.

(s, 1) has the same sign

t

with ˜Z N
t

(s).

To summarize, we prove the induced map of any ﬂuid-balance policy satisﬁes all four conditions

in Deﬁnition 1. Thus, any ﬂuid-balance policy is diﬀusion regular.

As for the moment bound, notice ˜Z N
1

calculation of the coeﬃcient in (13) gives Eπ[∣ ˜Z N

t

= 0 and C3 = 0 for any ﬂuid balance policy. Thus, direct
◻

∣] ≤ 2t∣S∣2 for t ≤ T .

9.5. Proof of Proposition 1

We show that

• with Lagrangian penalty λ = 0, state A is active while state B is inactive;
• with Lagrangian penalty λ = α, state A is inactive while state B is active.

Thus, the problem is not indexable. Now we analyze these two cases separately.

To be consistent with Whittle (1980), we denote Vλ(x) for the reward-to-go function of initial

state x with Lagrangian penalty λ.

With Lagrangian penalty λ = α, we can see Vλ(A′) = 0, Vλ(C) = 0, Vλ(B∗) = 0 and Vλ(B′) = β − α

via direct calculation. Thus, we have

Vλ(A) = max{γVλ(B), −α},

Vλ(B) = max{γVλ(A), −α + γ(1 − (cid:15))Vλ(B′)}.

Solving the above equations gives us

Vλ(A) = γ(γ2(β − α) − α), Vλ(B) = γ2(β − α) − α.

Thus, state A is inactive and state B is active.

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

27

With Lagrangian penalty λ = 0, we can see Vλ(A′) = α

1−γ , Vλ(C) = 0, Vλ(B∗) = 0 and Vλ(B′) = β via

direct calculation. Thus, we have

Vλ(A) = max{γVλ(B), (1 − (cid:15))γVλ(A′)},

Vλ(B) = max{γVλ(A), γ(1 − (cid:15))Vλ(B′)}.

Solving the above equations gives us

Vλ(A) = αγ2
1 − γ

, Vλ(B) = αγ3
1 − γ

.

Thus, state A is active and state B is inactive.

9.6. Proof of Proposition 2

Regardless of the budget constraint,

• the maximal reward generated from an initial state A0 is up bounded by γα
1−γ ,
• the maximal reward could be generated from an initial state A is up bounded by (1 − (cid:15)) γα
1−γ .

Thus, the maximal reward generated from N arms is up bounded by

φ1N (1 − (cid:15)) γα
1 − γ

+ φ3N

γα
1 − γ

= γα
1 − γ

γN.

Now we calculate the reward generated by policy π which pulls all arms in A0 and A in the ﬁrst

period, and then pulls all arms in A′ starting from the second period.

In the ﬁrst period, zero reward is generated. In the second period, there are φ3N +φ1N (1−(cid:15)) = γN
arms in state A′ in expectation. Starting from second period, we pull all arms in state A′. Thus,
we generate γN ∗ α

1−γ γN amounts of reward.

1−γ γ = γα

9.7. Proof of Proposition 3

We only need to show that the optimal policy pulls all arms in state A in the ﬁrst period.

If not, then under the optimal policy π∗, there exists an arm x in state A being idled and an

arm y in state A0 or C being pulled in the ﬁrst period. Now we construct a policy ˆπ which pulls
arm x in the ﬁrst period with satisfying VN (ˆπ) ≥ VN (π∗).

The deﬁnition of ˆπ is pretty simple. It pull x and idles y in the ﬁrst period, and starting from

the second period, ˆπ takes the exact same action with π for every arm. Then for any trajectory ω,
we can calculate the reward generated from arm x by π∗ and ˆπ and show VN (ˆπ) ≥ VN (π∗).

9.8. Proof of Proposition 4
Under ﬂuid-balance policy π, ˆV ∗
N

− VN (π) = γα
1−γ

E[γN − min{Z, γN }], where Z is number of arms in

state A′ starting from the second period.

28

Thus, we have

Xiangyu and Peter: Inﬁnite-horizon restless bandits with many arms

where Ii is a Bernoulli r.v. with probability γ. Thus,

Z = (1 − γ)N +

(2− 1
γ )N
∑
i=1

Ii

E[γN − min{Z, γN }] = E

⎡
(2− 1
γ )N
⎢
⎢
∑
⎢
⎢
⎣
i=1

Ii − γ

⎤
+
⎥
⎥
⎥
⎥
⎦

.

By concentration inequality, we know E[γN − min{Z, γN }] = θ

√

N + O(1) where θ =

√

(1−γ)(2γ−1)
2π

.

