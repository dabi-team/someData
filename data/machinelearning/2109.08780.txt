Long-Range Modeling of Source Code Files with eWASH: Extended
Window Access by Syntax Hierarchy

Colin B. Clement
Microsoft Cloud and AI
colin.clement@microsoft.com

Shuai Lu
Microsoft Research
shuailu@microsoft.com

Xiaoyu Liu
Microsoft Cloud and AI
lixiaoyu@microsoft.com

Michele Tufano
Microsoft Cloud and AI
mitufano@microsoft.com

Dawn Drain
Microsoft Cloud and AI
dawndrain95@gmail.com

Nan Duan
Microsoft Research
nanduan@microsoft.com

Neel Sundaresan
Microsoft Cloud and AI
neels@microsoft.com

Alexey Svyatkovskiy
Microsoft Cloud and AI
alsvyatk@microsoft.com

Abstract

Statistical language modeling and translation
with transformers have found many success-
ful applications in program understanding and
generation tasks, setting high benchmarks for
tools in modern software development environ-
ments. The ﬁnite context window of these neu-
ral models means, however, that they will be
unable to leverage the entire relevant context
of large ﬁles and packages for any given task.
While there are many efforts to extend the
context window, we introduce an architecture-
independent approach for leveraging the syn-
tactic hierarchies of source code for incor-
porating entire ﬁle-level context into a ﬁxed-
length window. Using concrete syntax trees
of each source ﬁle we extract syntactic hier-
archies and integrate them into context win-
dow by selectively removing from view more
speciﬁc, less relevant scopes for a given task.
We evaluate this approach on code generation
tasks and joint translation of natural language
and source code in Python programming lan-
guage, achieving a new state-of-the-art in code
completion and summarization for Python in
the CodeXGLUE benchmark. We also intro-
duce new CodeXGLUE benchmarks for user-
experience-motivated tasks: code completion
with normalized literals, method body comple-
tion/code summarization conditioned on ﬁle-
level context.

1

Introduction

Large transformer models (Vaswani et al., 2017)
and the pre-training/ﬁne-tuning paradigm (Devlin

et al., 2018; Lewis et al., 2019; Radford et al.,
2018) have become an essential part of state of
the art natural language processing. Beyond the
domain of natural language, these models and pro-
cedures have enabled rapid progress in the soft-
ware engineering space, including applications in
code completion (Svyatkovskiy et al., 2020, 2019;
Clement et al., 2020; Raychev et al., 2014; Bruch
et al., 2009), natural language to code (NL2Code),
code feature summarization (Clement et al., 2020;
Moreno et al., 2013; Scalabrino et al., 2017; Wan
et al., 2018; Alon et al., 2018; Moreno et al., 2014),
code search (Husain et al., 2019; Feng et al., 2020),
unit test generation (Tufano et al., 2020) and even
bug ﬁxing (Drain et al., 2021) and detection (Zhai
et al., 2020).

A major difference between transformers and
their antecedents like recurrent neural networks
(RNN) is their strictly enforced ﬁnite context win-
dow. Whereas an RNN can iteratively consume as
many tokens as is required, transformers can only
consume up to a ﬁnite amount decided at training
time. Further, it is impractical to simply expand
the window as the memory and compute require-
ments of the attention mechanism scale quadrati-
cally with context length. There have been efforts
to economically expand the window by modifying
the attention mechanism with low-rank queries and
keys (Beltagy et al., 2020), sparse connections (Par-
mar et al., 2018; Zaheer et al., 2020), and more
recently approximation by kernel methods (Choro-
manski et al., 2021). There have also been meth-

1
2
0
2

p
e
S
7
1

]

G
L
.
s
c
[

1
v
0
8
7
8
0
.
9
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
ods developed to condition generation on retrieved
documents (Lewis et al., 2020b,a) for knowledge-
intensive applications. Complimentary to these ap-
proaches, and consistent with any sequence model
or architecture, we propose a method for extracting
the most important features distant from the task
at hand, implemented in this case using the syntax
tree of source code.

A source code document has nested scopes and
references to other documents/libraries, and soft-
ware engineering tasks must leverage knowledge
at all of these scales. Even single source code
ﬁles often exceed the length of the context window,
and so it is clear that progress in modeling source
code requires overcoming this limitation. Instead
of proposing a novel transformer architecture capa-
ble of ingesting more context, we propose a method
of compressing the context of source code ﬁles us-
ing the syntax of the programming language itself.
In this paper we introduce eWASH: Extended Win-
dow Access by Syntax Hierarchy, which leverages
the syntax hierarchy of source code to give our
models longer ranged vision by prioritizing higher
level scopes and names for source code elements
in a ﬁle which are not immediately in focus. Us-
ing eWASH assumes that higher level scopes like
function signatures summarize the whole method,
and viewed in this way eWASH could be applied to
natural language by extracting key terms in a long
document or summarizing features distant from the
task at hand.

We start by explaining eWASH with a moti-
vating example and deﬁne its features for three
important software engineering tasks. The ﬁrst
is code completion, in which some number of
tokens are predicted to extend a partially com-
plete source code ﬁle. The second is method
completion, wherein a whole method body is pre-
dicted from a method signature and docstring. The
third is code summarization or docstring gener-
ation, wherein a method is mapped to a natural
language docstring. We then discuss the Python
training data and the models employed in this study,
which include the auto-regressive GPT-C (Svy-
atkovskiy et al., 2020), an eWASH extended ver-
sion called XGPT-C, Python method/docstring pre-
diction model PyMT5 (Clement et al., 2020), simi-
larly named XPyMT5, and Performer (Choroman-
ski et al., 2021) and Reformer (Kitaev et al., 2020)
baselines. We demonstrate through experiments
the value of eWASH for the source code domain

with state of the art performance for code comple-
tion and method completion and code summariza-
tion or docstring generation. Finally, we study
user-experience motivated properties of XGPT-
C, and extend the source code benchmark set
CodeXGLUE (Lu et al., 2021) with two new tasks:
literal-normalized code completion and method
completion. Surprisingly, eWASH excels at code
completion even when the context window is not a
limiting factor.

2 Motivating example

In this paper we consider three software engi-
neering tasks: code completion, method comple-
tion, and docstring completion or code summa-
rization. Code completion is the auto-regressive
prediction of one or more tokens conditioned on a
provided context. Figure 1 shows an incomplete
Python program implementing a neural network.
A developer would like a model which can pre-
dict the body of ConvNet.forward (method
completion) by composing the layers deﬁned in
ConvNet.__init__ and globally imported op-
erations. There is a limited context window, illus-
trated at the left of the ﬁgure, and so while the
model can (in this fortunate case) see the layer def-
initions, it is ignorant of the imports and the global
LOGGER object.

In many cases predicting a whole method is not
easy, but asking for a few members or a completed
line (Svyatkovskiy et al., 2020) is still desirable.
The bottom of ﬁg. 1 shows the code completion task
ﬁnishing an assignment and completing a line in
a partial implementation of ConvNet.forward.
Here, again, crucial information is missing from
the input of the model, which will prevent it from
being able to reference the imports. Further, one
can easily imagine a scenario in which more spu-
rious information is fed into the model instead of,
for example, the deﬁnitions of the neural network
layers in the __init__. How can we ensure the
model is shown important information for predic-
tions?

2.1 Extended Window Access by Syntax

Hierarchy

Software developers carefully organize their code
into scopes and portable elements using hierarchies
like methods and classes, so we hypothesize that
labels of these scopes are more important for long-
range modeling than their contents. We propose

Figure 1: An example scenario where both method completion (top right) and code completion (bottom right)
are performed by our eWASH models XPyMT5 and XGPT-C, respectively. Method completion aims to predict
a whole method body conditioned on a signature and docstring and other context, and code completion aims to
predict any number of tokens to complete a member, line, or even a scope context conditioned on any incomplete
code string. In this case XPyMT5 is tasked with predicting the whole body of ConvNet.forward. In this case
it’s clear that both the layers assigned in ConvNet.__init__ and the import statements above are important
information. In another case XGPT-C aims to complete the assignment, and again the class attributes and import
statements are important. Both models have a limit context window illustrated at left, and exclude important
information.

eWASH, Extended Window Access by Syntax Hier-
archy, in which we compress the context provided
to our model by prioritizing, for example, function
signatures over function bodies. Since most code is
written inside methods, we center method bodies as
the focus of the modeling task for eWASH, calling
each method being modeled the ‘focal method.’

Figure 2 shows how eWASH uses syntax hi-
erarchies to prioritize elements of the context
of the method and code completion example
of Fig. 1. The focal method in this case is
ConvNet.forward, but could be any other
method in the module. The most important part
for modeling the body of this focal method is its
signature and docstring (if present) and contain-
ing class deﬁnition (if the focal method is a class
method). After this we prioritize global import
statements and assigned values (but not yet the as-
signed expressions), followed by class attributes,
peer class method signatures, class docstring, peer
class method docstrings, and ﬁnally global expres-
sions and the code bodies of peer class methods.

In practice, eWASH is implemented by taking
the concrete syntax tree of the source ﬁle and orga-

nizing the syntactic elements in our priority list, to-
kenizing each element, and then descending the pri-
ority list, taking elements until the context window
has been ﬁlled. For training the method comple-
tion of XPyMT5, we arrange the eWASH context
in the input with a control code to indicate which
method is to be completed (# target body in
Fig. 1), and we arrange the target to be the method
body. eWASH yields N total training samples from
a ﬁle with N total methods and class methods. For
docstring completion or code summarization, the
source contains the method signature and body, and
the target contains the desired docstring, and a con-
trol code is used to instruct the model which task
it is to perform, just like PyMT5 (Clement et al.,
2020).

For code completion, as we use an auto-
regressive decoder in the form of XGPT-C there
is no special ‘position,’ and so we create a rolling
window across the focal method body. We reserve
3/4 (768/1024 tokens) of the tokens for the context,
and 1/4 (256/1024 tokens) for the rolling window
of the body. In the case of a method which exceeds
256 tokens, the training sample for that method is

decomposed into multiple ‘windows,’ and one ﬁle
yields at least N training samples for a ﬁle with N
method and class method deﬁnitions.

Figure 2: An illustration of the syntax hierarchy of the
input context in the method completion example from
Fig. 1. The eWASH (Extended Window Access by Syn-
tax Hierarchy) method selectively ﬁlls the model con-
text going down the ﬁle, in the order of priority level in-
dicated, and stops when the token budget of the model
context is ﬁlled. eWASH presupposes that names of en-
tities at higher scopes are more relevant to the task at
hand than entities at lower scopes.

3 Dataset

3.1 Pre-training

The data for training is the same for both XGPT-C
and XPyMT5, and consists of all 5+ star GitHub
repositories which are primarily Python, ﬁltered by
ﬁles which were either Python 3 compliant or were
successfully ﬁxed by lib2to3. Further, there was
a time limit of 10 seconds placed on the parsing
process to eliminate ﬁles which are essentially data
ﬁles as they tend to contain very large lists for
example. Table 2 shows summary statistics of this
dataset for a sense of scale.

3.2 Fine-Tuning and Evaluation

For evaluation and ﬁne-tuning of code completion
we used the Py150 (Raychev et al., 2016) from
CodeXGLUE (Lu et al., 2021), and for method
and docstring completion we used the CodeSearch-
Net (Husain et al., 2019) dataset. Py150 is larger
than CodeSearchNet for Python, but has selected
repositories with good docstring coverage, allow-
ing better evaluation of the method/docstring com-
pletion task.

4 Baseline Models

We consider state-of-the-art transformer models
for code completion and code summarization tasks
in the CodeXGLUE benchmark as our baselines.
Namely, the generative pre-trained transformer
model for code (Svyatkovskiy et al., 2020) (GPT-C)
and the Python method text-to-text transfer trans-
former model (Clement et al., 2020) (PyMT5).
We also experiment with two memory-efﬁcient
transformers—Reformer (Kitaev et al., 2020) and
Performer (Choromanski et al., 2021)—which en-
ables modeling of context lengths in excess of 1024
tokens.

4.1 GPT-C

GPT-C is an auto-regressive language model pre-
trained on a large unsupervised source code cor-
pora. Treating the source code data as a sequence
of lexically-deﬁned tokens, GPT-C extracts train-
ing samples as a sliding window from each source
code ﬁle. This baseline uses an approach based on
statistical language modeling of source code, with
several normalization rules extracted from concrete
syntax tree of a program. To overcome the issue
of different styles and white space or tab conven-
tions, it transforms the code into symbolic program
tokens using custom tokenizer and regenerates the
code with a common style. During pre-processing,
GPT-C parses program code in each ﬁle, extracts
information about token types, normalizes uncom-
mon literals, trains a sub-token vocabulary, and
encodes the ﬁles into sub-token sequences. This
is done both for training and inference. GPT-C
decoder-only model has about 125M parameters
and a context length of 1024 tokens.

4.2 PyMT5

PyMT5 is a transformer encoder-decoder model
jointly pre-trained on a large-scale corpus of
Python source code and natural language con-
tained in the docstring summaries. PyMT5 train-
ing samples are supervised pairs of function
code features—function signatures, docstrings and
bodies—extracted by means of a parser. PyMT5 is
ﬁnetuned to translate between all non-degenerate
combinations of code features in a multi-modal set-
ting, e.g. simultaneously signature and docstring
to body, signature to docstring and body, signature
to body, etc. PyMT5 only uses information from
a single method and naturally is missing imports,
peer class and method deﬁnitions, and global as-

Model

Performer
Reformer
XGPT-C

GPT-C, Norm Literals
XGPT-C, Norm Literals

PPL ROUGE-L Prec. Recall Edit dist. EM@5 (%) Size

2.06
2.02
1.35

1.83
1.25

0.69
0.70
0.85

0.81
0.90

0.80
0.81
0.93

0.94
0.96

85.1
86.3
90.8

89.0
93.7

41.2
46.1
49.4

46.3
62.4

125M
116M
125M

125M
125M

Table 1: Evaluation results comparing XGPT-C decoder-only model trained with extended hierarchical context
with baselines on code completion from sampled methods. Model performance metrics are reported on test samples
from the CodeXGLUE code completion task as described in Sec. 7.2.

Lang

Repos

Files Methods Classes

Lines

Python

238k

4.3M 15M

10.6M 1.4B

Table 2: Summary statistics of our Python parallel cor-
pus compared to others presented in the literature. CSN
contains 500k Python methods with docstrings, among
6 other languages. Our parallel corpus is 3× as large
as the next largest, and over 15× the size of the next
largest Python parallel corpus.

signments. PyMT5 has 406M parameters and a
context width of 1024 tokens for both the encoder
and decoder.

4.3 Memory-Efﬁcient Transformers

Reformer and Performer transformer models at-
tempt to break the infamous quadratic attention bot-
tleneck and allow for efﬁcient modeling with much
longer than the standard 1024 token context win-
dow. Reformer (Kitaev et al., 2020) includes three
memory optimizations: reversible layers (to trade
off memory with time), axial positional embed-
dings, and a bucketed attention. Performer (Choro-
manski et al., 2021) develops a linear approxi-
mation to the attention layer AV ≈ σ(QT K)V ,
where K, Q, and V are the key, query and value
matrices of the attention mechanism and A is the
softmax kernel approximation, and exploits the lin-
earity to improve computational efﬁciency.

5 Deﬁning the Tasks at Hand

5.1 Code completion

Code completion is the auto-regressive completion
of source code tokens, as illustrated in the bottom
of Fig. 1. We perform code completion as deﬁned
in CodeXGLUE (Lu et al., 2021) as well as us-
ing normalized literals. The literal normalization
improves user experience of the code completion
tool (Svyatkovskiy et al., 2020) by abstracting per-

sonally identiﬁable information and encouraging
the model to focus on code modeling over arbitrary
strings. Names, the phone number, IP addresses,
and more may be preserved in the string or numeric
literals. We normalize the literals in source codes
to some special tokens. Considering that frequently
used literals may contain useful information, e.g.
"__main__" or "utf-8", we preserve the 200
most frequent string and 30 most frequent numeric
literals.

5.2 Method completion

Method completion is the prediction of a method
body implementation conditioned on a signature,
optional docstring, and any more context. The
authors of PyMT5 performed this task using no
context beyond the focal method, and XPyMT5
uses eWASH compressed ﬁle-level context. We
contribute method and docstring completion con-
ditioned on ﬁle-level information as a task to
CodeXGLUE, based on the CodeSearchNet dataset
in order to bolster its user-experience motivated
tasks.

5.3 Docstring Completion/Code

Summarization

Docstring completion is the prediction of a doc-
string conditioned on a focal method and op-
tional context, and was also performed by PyMT5
on focal-methods alone. Code summarization is
closely related, as docstrings often express a sum-
mary of their method, but also include annotated
arguments, return values, exceptions, and even test
cases via doctest. We train on docstring com-
pletion but evaluate on the CodeSearchNet dataset
which attempts to remove everything but the sum-
mary which is assumed to be in the ﬁrst paragraph
of the docstring.

6 Model Training

6.1 XGPT-C

We trained XGPT-C on the Python dataset de-
scribed in 3. Each training sample is a method body
along with its corresponding extended context. In
XGPT-C, we follow GPT-C (Svyatkovskiy et al.,
2020), using a multi-layer Transformer decoder
as the model architecture and the causal language
model as the training objective. We use 12 layers
Transformer decoder, 12 attention heads with 768
hidden dimension in total and sentencepiece 1 BPE
vocabulary of size 50,000. The total number of
model parameters is 125M. The pre-training period
takes 2 weeks on sixteen 32GB Tesla V100 GPUs,
and all hyperparameters were left as Svyatkovskiy
et al. (2020).

6.2 XPyMT5

We trained XPyMT5 on the same Python dataset
as XGPT-C. Similar to PyMT5, Each Python ﬁle
yielded between N and 3N training samples where
N is the number of methods and class methods in
the ﬁle. Each method teaches the model to com-
plete the method body conditioned on its signature
(and docstring if it exists), to predict the docstring
(if it exists) from the method, and to predict the
whole method from just the docstring (if it exists).
In this way, XPyMT5 also can jointly predict code
and natural language, but we did not include all de-
generate combinations like PyMT5 as the training
set was already much larger due to the extended
context.

XPyMT5 uses the same whitespace-augmented
GPT-2 (Radford et al., 2018) tokenizer as PyMT5,
with about a vocabulary size of 50,000, and is the
same architecture and hyperparameters as PyMT5
with 12 layers and 406M parameters. XPyMT5
was trained on 16 32GB Tesla V100 GPUs for 4
weeks, about 10 epochs total, using the same hyper-
parameters as reported by Clement et al. (2020).
XPyMT5 was initialized with the English pre-
trained BART (Lewis et al., 2019) weights (with
whitespace embeddings) and pre-trained using the
BART de-noising objective for 5 weeks on the same
hardware as above.

6.3 Reformer/Performer

We trained both Performer and Reformer models
on the Python dataset described in 3 but without

1https://github.com/google/sentencepiece

eWASH. Each training sample is a whole source
code ﬁle literal normalization applied. We adapt the
open-sourced model implementations, 23 setting
the architecture parameters of each to be as close to
the same parameter count as XGPT-C as possible.
Both used 12 layers, a context length of 4096, and
768 hidden dimensions. All other hyperparameters
were unchanged from their default.

7 Evaluation

7.1 Metrics

The metrics we used to evaluate eWASH and thus
XGPT-C and XPyMT5 are consistent with GPT-C
and PyMT5 and other works in the literature. We
report the longest common subsequence ROUGE-L
as we expect in a developer tool scenario that users
will want predicted code with the fewest edits. To
that end, we also report the edit distance between
the truth and hypothesis. In order to compare to
other code completion models we report Exact-
Match@N (EM) metrics (Rajpurkar et al., 2016),
which counts the fraction of exactly correct predic-
tions of some length (@N). For method and doc-
string completion we report BLEU-4 and ROUGE-
L metrics, but not exact matches as it is too strict
to meaningfully interpret for longer source-target
pairs. For method completion we also report the
fraction of syntactically correct methods as judged
by Python 3.8 syntax.

7.2 Experimental Conditions

We aim to evaluate how well XGPT-C model can in-
fer developers’ true intents. We randomly selected
833 unique Python functions from the code com-
pletion test benchmark in CodeXGLUE (Lu et al.,
2021), and, except for the ﬁrst two tokens in each
line, prompted the model at all other points inside
the methods. The predictions are compared to the
true continuation of the code. For method and doc-
string completion, the CodeSearchNet repositories
and speciﬁc commit hashes were re-downloaded
in order to extract the eWASH features in addi-
tion to the individual methods released. We will
release this expanded CSN dataset and task to
CodeXGLUE to improve its user-experience moti-
vated metrics. Inference in all cases was performed
with beam search with a beam width of 5.

2https://github.com/lucidrains/reformer-pytorch
3https://github.com/lucidrains/performer-pytorch

Figure 3: Comparing baseline GPT-C with XGPT-C in an ofﬂine evaluation of ExactMatch@1-5 code completion
as a function of total token context length for the normalized literal scenario. Surprisingly, eWASH leads XGPT-C
to beneﬁt most over GPT-C at the shorter context lengths. XGPT-C also more exactly predicts tokens with longer
context as well.

Figure 4: Comparing baseline GPT-C with XGPT-C in an ofﬂine evaluation of ExactMatch@1-5 code completion
as a function of local, same-line context length for the normalized literal scenario. XGPT-C is better than GPT-C
by all measures, showing how leveraging longer range context can most help developers even when a line being
edited has a few tokens.

Exact Match
@1 @2 @3 @4 @5

GPT-C top-1
top-5
XGPT-C top-1
top-5

96.0
98.8
98.0
98.9

68.5
81.0
81.7
94.3

56.3
70.5
71.9
87.7

49.6
63.5
66.6
83.1

46.3
59.6
62.4
79.7

Total

63.1
74.5
75.9
88.7

Table 3:
Code
CodeXGLUE test
overall EM results for XGPT-C and GPT-C.

completion evaluated on the
set by ExactMatch@1-5 and

7.2.1 Code Completion Evaluation Results

As shown in Table 1, eWASH allows XGPT-C to
beat both the GPT-C baseline and the memory ef-
ﬁcient transformers on all the metrics computed.
About 10% of our Python test ﬁles were greater
than 1024 tokens in length, and evaluating sepa-
rately on that subset yielded slight improvements
of Performer/Reformer, but Performer only beat
XGPT-C in terms of EM@5 at 55.9%. These evalu-
ations were performed for source code inside meth-
ods, as the eWASH technique follows the syntactic

hierarchy used by developers. Note that the bottom
lines are trained and evaluated on the normalized
literal dataset. XGPT-C sees a large absolute in-
crease in ExactMatch@5 of 13% with normalized
literals showing that, in addition to protecting user
data, normalizing literals is an important part of a
good IDE programming assistant.

Hellendoorn et al. (2019) showed that artiﬁcial
evaluation scenarios are often much more forgiv-
ing than real-world scenarios. To better evaluate
whether these models can predict a developer’s
intent we compute the ExactMatch@1-5 bench-
mark, described in Sec. 7.1, broken down by total
token context and length of same-line context. Fig-
ure 3 shows EM@1-5 metrics for the normalized
literal scenario binned by the context length for the
completion for GPT-C (left) and XGPT-C (right).
It is clear that in all measured cases eWASH al-
lows XGPT-C to better predict exact matches. Per-
haps most strikingly, the largest relative increase in
EM occurs for shorter context lengths, so that the

syntactic hierarchy hypothesis underlying eWASH
appears most beneﬁcial for context lengths well
within the context window.

Figure 4 shows the same EM metrics broken
down by same-line context length, to test how much
the most proximal tokens matter for prediction. We
see the same overall beneﬁt of eWASH in XGPT-C,
and only a slow increase as a function of same-
line context. The average line length in our data is
18 tokens, so with 7 tokens of same-line context,
XGPT-C can complete 5 tokens exactly more than
80% of the time while GPT-C can do so just shy
of 60% of the time. Again, this is very interesting
as eWASH confers great beneﬁt even when con-
text lengths do not exceed the context window, and
supports our hypothesis that user-deﬁned syntax hi-
erarchies are very important signals for predicting
method bodies.

Modern IDE environments like Visual Studio In-
tellicode (Visual Studio Intellicode) can present
multiple predictions, which Hellendoorn et al.
(2019) showed can improve real-world user accep-
tance. Table 3 shows the overall ExactMatch@1-5
metrics for code completion regardless of context
length. XGPT-C is the clear winner again for all
the EM metrics, boosting total exact matches by
over 12% for top-1 predictions and reaching 88.7%
overall for top-5 predictions. We interpret this to
mean that eWASH will enable superior on-line user
acceptance of code completions.

7.3 Method Completion Evaluation Results

We evaluate eWASH for method generation, illus-
trated in the top of Fig. 1. Table 4 shows the com-
parison between XPyMT5 and PyMT5 and, and
PyMT5 is superior in all the source-target compar-
ison metrics. Syntax correctness is slightly lower,
but the difference is not necessarily meaningful.
The ROUGE-L metrics are dramatically improved,
and is not necessarily surprising as XPyMT5 is con-
ditioned on much more information than PyMT5.
The syntax correctness of our ﬁne-tuned models is
slightly lower than the 92.1% reported by Clement
et al. (2020).

7.4 Docstring Completion Evaluation Results

Table 5 compares XPyMT5 to PyMT5 for docstring
completion (or code summarization as CodeSearch-
Net removes the variable annotations). Again there
is a large improvement in performance across all
metrics, with a striking doubling of the ROUGE-L
F1 score with eWASH features.

8 Conclusions

Inspired by the performance of transformer models,
their limited context window size, and the espe-
cially long-range nature of source code as docu-
ments, we developed Extended Window Access
by Syntax Hierarchy. Our hypothesis was that the
syntax hierarchy imposed by developers is a real
signal of importance in a task context, and that
methods, containing most lines of code, are most
dependent on the higher-level scopes of their ﬁle-
level attributes. Our XGPT-C results for code com-
pletion supported this hypothesis, and, strikingly,
offered most relative beneﬁt for shorter context
lengths. We showed with strict exact match metrics
that eWASH allows a large relative improvement in
code completion predictions. Finally we show dra-
matic improvement in method completion and code
summarization with XPyMT5. eWASH can be ap-
plied to any programming language and in principal
any language with hierarchical syntactic or stylistic
structure. For this reason we believe eWASH to be
a general purpose modeling approach for more op-
timally using ﬁnite context windows on structured
documents, and could improve natural language un-
derstanding tasks as well. Further, any model, even
the largest GPT-3 language model (Brown et al.,
2020) can leverage the eWASH feature. Accom-
panying this manuscript we submit 3 new tasks to
CodeXGLUE to bolster its user-experience moti-
vated metrics: literal-normalized code completion,
method-level code completion, and method/doc-
string completion conditioned on whole-ﬁle con-
text.

References

Uri Alon, Shaked Brody, Omer Levy, and Eran Ya-
hav. 2018. code2seq: Generating sequences from
structured representations of code. arXiv preprint
arXiv:1808.01400.

Iz Beltagy, Matthew E Peters, and Arman Cohan.
2020. Longformer: The long-document transformer.
arXiv preprint arXiv:2004.05150.

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. arXiv preprint arXiv:2005.14165.

Marcel Bruch, Martin Monperrus, and Mira Mezini.
2009. Learning from examples to improve code
In Proceedings of the 7th
completion systems.
joint meeting of the European software engineering

Model

RL Prec. Recall F1

BLEU-4 Syntax (%) Model size

PyMT5 Baseline
XPyMT5

0.33
0.52

0.46
0.64

0.35
0.55

0.27
0.31

89%
88%

406M
406M

Table 4: Evaluation results for XPyMT5 multi-mode encoder-decoder model trained with extended hierarchical
context and various baselines on the task of method generation given a natural language description. Model
performance metrics are reported on CodeSearchNet test sample in Python programming language.

Model

RL Prec. Recall F1

BLEU-4 Model size

CoTexT (CodeXGLUE leader)
PyMTBaseline
XPyMT5

0.32
0.58

0.37
0.66

0.32
0.66

0.197
0.28
0.47

406M
406M

Table 5: Detailed evaluation results for XPyMT5 model trained with extended hierarchical context and various
baseline on code summarization task. Model performance metrics are reported on CodeSearchNet test sample in
Python programming language.

conference and the ACM SIGSOFT symposium on
the foundations of software engineering, pages 213–
222.

Krzysztof Choromanski, Valerii Likhosherstov, David
Dohan, Xingyou Song, Andreea Gane, Tamas Sar-
los, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
Lukasz Kaiser, David Belanger, Lucy Colwell, and
Adrian Weller. 2021. Rethinking attention with per-
formers. ICLR 2021.

Colin Clement, Dawn Drain, Jonathan Timcheck,
Alexey Svyatkovskiy, and Neel Sundaresan. 2020.
Pymt5: Multi-mode translation of natural language
In Proceed-
and python code with transformers.
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
9052–9065.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Dawn Drain, Chen Wu, Alexey Svyatkovskiy, and
Generating bug-ﬁxes
arXiv preprint

Neel Sundaresan. 2021.
using pretrained transformers.
arXiv:2104.07896.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,
Ting Liu, Daxin Jiang, et al. 2020. Codebert: A
pre-trained model for programming and natural lan-
guages. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing:
Findings, pages 1536–1547.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis
Allamanis, and Marc Brockschmidt. 2019. Code-
searchnet challenge: Evaluating the state of seman-
tic code search. arXiv preprint arXiv:1909.09436.

Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
2020. Reformer: The efﬁcient transformer. ICLR.

Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-
men Aghajanyan, Sida Wang, and Luke Zettle-
moyer. 2020a. Pre-training via paraphrasing. arXiv
preprint arXiv:2006.15020.

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.
Bart: Denoising sequence-to-sequence pre-training
for natural language generation,
translation, and
comprehension. arXiv preprint arXiv:1910.13461.

Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020b. Retrieval-augmented gen-
arXiv
eration for knowledge-intensive nlp tasks.
preprint arXiv:2005.11401.

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin B. Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-
dong Zhou, Linjun Shou, Long Zhou, Michele Tu-
fano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-
daresan, Shao Kun Deng, Shengyu Fu, and Shujie
Liu. 2021. Codexglue: A machine learning bench-
mark dataset for code understanding and generation.
CoRR, abs/2102.04664.

Vincent J Hellendoorn, Sebastian Proksch, Harald C
Gall, and Alberto Bacchelli. 2019. When code com-
pletion fails: A case study on real-world comple-
In 2019 IEEE/ACM 41st International Con-
tions.
ference on Software Engineering (ICSE), pages 960–
970. IEEE.

Laura Moreno, Jairo Aponte, Giriprasad Sridhara, An-
drian Marcus, Lori Pollock, and K Vijay-Shanker.
2013. Automatic generation of natural language
In 2013 21st Inter-
summaries for java classes.
national Conference on Program Comprehension
(ICPC), pages 23–32. IEEE.

Visual Studio Intellicode. 2021. Visual studio intelli-
code. https://visualstudio.microsoft.
com/services/intellicode/.
Accessed:
2021-02-08.

Yao Wan, Zhou Zhao, Min Yang, Guandong Xu,
Haochao Ying, Jian Wu, and Philip S Yu. 2018. Im-
proving automatic source code summarization via
deep reinforcement learning. In Proceedings of the
33rd ACM/IEEE International Conference on Auto-
mated Software Engineering, pages 397–407.

Manzil Zaheer, Guru Guruganesh, Avinava Dubey,
Joshua Ainslie, Chris Alberti, Santiago Ontanon,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
et al. 2020. Big bird: Transformers for longer se-
quences. arXiv preprint arXiv:2007.14062.

Juan Zhai, Xiangzhe Xu, Yu Shi, Guanhong Tao,
Minxue Pan, Shiqing Ma, Lei Xu, Weifeng Zhang,
Lin Tan, and Xiangyu Zhang. 2020. Cpc: Automat-
ically classifying and propagating natural language
comments via program analysis. In Proceedings of
the ACM/IEEE 42nd International Conference on
Software Engineering, pages 1359–1371.

A Appendix

A.1 Code Completion Above and Below the

Context Window

The low performance of the memory efﬁcient trans-
formers in the main text is puzzling, so we decom-
posed our test set on code completion into the 90%
with fewer than 1024 tokens (the window of GPT-
C and XGPT-C), and 10% with more than 1024
tokens.

Laura Moreno, Gabriele Bavota, Massimiliano
Di Penta, Rocco Oliveto, Andrian Marcus, and
Gerardo Canfora. 2014. Automatic generation of
In Proceedings of the 22nd ACM
release notes.
SIGSOFT International Symposium on Foundations
of Software Engineering, pages 484–495.

Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz
Kaiser, Noam Shazeer, Alexander Ku, and Dustin
In International
Tran. 2018.
Conference on Machine Learning, pages 4055–4064.
PMLR.

Image transformer.

Alec Radford, Karthik Narasimhan, Tim Salimans,
and Ilya Sutskever. 2018.
Improving language
understanding by generative pre-training. URL
https://s3-us-west-2.
com/openai-
assets/researchcovers/languageunsupervised/language
understanding paper. pdf.

amazonaws.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016.
Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250.

Veselin Raychev, Pavol Bielik, and Martin Vechev.
2016. Probabilistic model for code with decision
trees. ACM SIGPLAN Notices, 51(10):731–747.

Veselin Raychev, Martin Vechev, and Eran Yahav. 2014.
Code completion with statistical language models.
In Proceedings of the 35th ACM SIGPLAN Confer-
ence on Programming Language Design and Imple-
mentation, pages 419–428.

Simone Scalabrino, Gabriele Bavota, Christopher Ven-
dome, Mario Linares-Vásquez, Denys Poshyvanyk,
and Rocco Oliveto. 2017. Automatically assessing
code understandability: How far are we? In 2017
32nd IEEE/ACM International Conference on Auto-
mated Software Engineering (ASE), pages 417–427.
IEEE.

Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,
Intellicode compose:
and Neel Sundaresan. 2020.
In ESEC/FSE
code generation using transformer.
’20: 28th ACM Joint European Software Engineer-
ing Conference and Symposium on the Founda-
tions of Software Engineering, Virtual Event, USA,
November 8-13, 2020, pages 1433–1443. ACM.

Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, and
Neel Sundaresan. 2019. Pythia: Ai-assisted code
completion system. In Proceedings of the 25th ACM
SIGKDD International Conference on Knowledge
Discovery & Data Mining, pages 2727–2735.

Michele Tufano, Dawn Drain, Alexey Svyatkovskiy,
and Neel Sundaresan. 2020. Generating accurate as-
sert statements for unit test cases using pretrained
transformers.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
In Advances in neural information pro-
you need.
cessing systems, pages 5998–6008.

Model

Performer
Reformer
XGPT-C

GPT-C, Norm Literals
XGPT-C, Norm Literals

ROUGE-L Prec. Recall Edit dist. EM@5 (%)

0.73
0.76
0.85

0.90
0.91

0.82
0.84
0.94

0.94
0.96

89.6
93.6
90.9

90.9
94.1

47.9
55.9
49.3

49.3
63.9

Table 6: Evaluation results comparing XGPT-C on code completion from sampled methods from ﬁles with more
than 1024 tokens. Model performance metrics are reported on test samples from the CodeXGLUE code completion
task as described in the main text.

Model

Performer
Reformer
XGPT-C

GPT-C, Norm Literals
XGPT-C, Norm Literals

ROUGE-L Prec. Recall Edit dist. EM@5 (%)

0.68
0.70
0.85

0.81
0.90

0.80
0.81
0.93

0.89
0.97

84.9
86.0
90.8

91.1
93.6

40.5
45.1
49.4

45.3
62.2

Table 7: Evaluation results comparing XGPT-C on code completion from sampled methods from ﬁles with more
than 1024 tokens. Model performance metrics are reported on test samples from the CodeXGLUE code completion
task as described in the main text.

