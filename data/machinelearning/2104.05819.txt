Learning from Executions for Semantic Parsing

Bailin Wang, Mirella Lapata and Ivan Titov
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
bailin.wang@ed.ac.uk, {mlap, ititov}@inf.ed.ac.uk

1
2
0
2

r
p
A
2
1

]
L
C
.
s
c
[

1
v
9
1
8
5
0
.
4
0
1
2
:
v
i
X
r
a

Abstract

list all 3 star rated thai restaurants

Semantic parsing aims at
translating natu-
ral language (NL) utterances onto machine-
interpretable programs, which can be executed
against a real-world environment. The ex-
pensive annotation of utterance-program pairs
has long been acknowledged as a major bot-
tleneck for the deployment of contemporary
neural models to real-life applications. In this
work, we focus on the task of semi-supervised
learning where a limited amount of annotated
data is available together with many unlabeled
NL utterances. Based on the observation that
programs which correspond to NL utterances
must be always executable, we propose to en-
courage a parser to generate executable pro-
grams for unlabeled utterances. Due to the
large search space of executable programs,
conventional methods that use approximations
based on beam-search such as self-training and
top-k marginal likelihood training, do not per-
Instead, we view the problem
form as well.
of learning from executions from the perspec-
tive of posterior regularization and propose a
set of new training objectives. Experimen-
tal results on OVERNIGHT and GEOQUERY
show that our new objectives outperform con-
ventional methods, bridging the gap between
semi-supervised and supervised learning.

1

Introduction

Semantic parsing is the task of mapping nat-
ural language (NL) utterances to meaning repre-
sentations (aka programs) that can be executed
against a real-world environment such as a knowl-
edge base or a relational database. While neural
sequence-to-sequence models (Dong and Lapata,
2016; Jia and Liang, 2016a) have achieved much
success in this task in recent years, they usually re-
quire a large amount of labeled data (i.e., utterance-
program pairs) for training. However, annotat-
ing utterances with programs is expensive as it
requires expert knowledge of meaning representa-
tions (e.g., lambda calculus, SQLs) and the envi-

Program Candidates
Gold Exe
(cid:55)
select restaurant where star_rating = thai (cid:55)
(cid:55)
(cid:55)
select restaurant where cuisine > 3
(cid:55) (cid:51)
select restaurant where star_rating = 3
select restaurant where star_rating = 3

and cuisine = thai

(cid:51) (cid:51)

Figure 1: Candidate programs for an utterance can be
classiﬁed by executability (Exe); note that the gold pro-
gram is always in the set of executable programs. We
propose to ultilize the weak yet freely available signal
of executablility for learning.

ronment against which they are executed (e.g., a
knowledge base, a relational database). An alter-
native to annotation is to collect answers (or deno-
tations) of programs, rather than programs them-
selves (Liang et al., 2013; Berant et al., 2013). In
this work, we focus on the more extreme setting
where there are no annotations available for a large
number of utterances. This setting resembles a
common real-life scenario where massive numbers
of user utterances can be collected when deploying
a semantic parser (Iyer et al., 2017). Effectively
utilizing the unlabeled data makes it possible for
a semantic parser to improve over time without
human involvement.

Our key observation is that not all candidate pro-
grams for an utterance will be semantically valid.
This implies that only some candidate programs
can be executed and obtain non-empty execution
results.1 As illustrated in Figure 1, executability is
a weak signal that can differentiate between seman-
tically valid and invalid programs. On unlabeled
utterances, we can encourage a parser to only focus
on executable programs ignoring non-executable
ones. Moreover, the executability of a program

1In the rest of this paper, we extend the meaning of ‘exe-
cutability’, and use it to refer to the case where a program is
executable and obtains non-empty results.

 
 
 
 
 
 
can be obtained from an executor for free without
requiring human effort. Executability has previ-
ously been used to guide the decoding of a semantic
parser (Wang et al., 2018). We take a step further
to directly use this weak signal for learning from
unlabeled utterances.

To learn from executability, we resort
to
marginal likelihood training, i.e., maximizing the
marginal likelihood of all executable programs for
an unlabeled NL utterance. However, the space
of all possible programs is exponentially large, as
well as the space of executable ones. Hence, simply
marginalizing over all executable programs is in-
tractable. Typical approximations use beam search
to retrieve a handful of (‘seen’) programs, which
are used to approximate the entire space. Using
such approximations can lead to optimization get-
ting trapped in undesirable local minima. For ex-
ample, we observe that encouraging a model to
exploit seen executable programs hinders explo-
ration and reinforces the preference for shorter pro-
grams, as discussed in Section 6.3. This happens
because shorter programs are both more likely to
be among ‘seen’ programs (probably due to using
locally-normalized autoregressive modeling) and
more likely to be executable. To alleviate these
issues, we derive three new alternative objectives,
relying on a new interpretation of marginal likeli-
hood training from the perspective of posterior reg-
ularization. Our proposed objectives encode two
kinds of inductive biases: 1) discouraging seen
non-executable programs, which plays a similar
role to encouraging seen executable ones but does
not share its drawback of hindering exploration;
2) encouraging sparsity among executable pro-
grams, which encourages a parser to only focus on
a subset of executable programs by softly injecting
a sparsity constraint. This is desirable, as there are
only one or few correct programs for each utter-
ance (see Figure 1), and an accurate parser should
assign probability mass only to this subset. We col-
lectively call these objectives X-PR, as a shorthand
for Execution-guided Posterior Regularization.

We conduct experiments on two representative
semantic parsing tasks: text-to-LF (logical form)
parsing over a knowledge base and text-to-SQL
(Zelle and Mooney, 1996) parsing over a relational
database. Concretely, we evaluate our methods on
the OVERNIGHT (Wang et al., 2015a) and GEO-
QUERY datasets. We simulate the semi-supervised
learning setting by treating 70% of the training

data as unlabeled. Empirical results show that our
method can substantially boost the performance of
a parser, trained only on labeled data, by utilizing
a large amount of unlabeled data.

Our contributions are summarized as follows:
• We show how to exploit unlabeled utterances
by taking advantage of their executability.

• To better learn from executability, we propose
a set of new objectives based on posterior reg-
ularization.

• Our method can help a base parser achieve
substantially better performance by utilizing
unlabeled data.

Our code, datasets, and splits are publicly avail-
able at https://github.com/berlino/
tensor2struct-public.

2 Related Work

Semi-Supervised Semantic Parsing In the con-
text of semantic parsing, semi-supervised models
using limited amounts of parallel data and large
amounts of unlabeled data treat either utterances
or programs as discrete latent variables and in-
duce them in the framework of generative mod-
els (Koˇciský et al., 2016; Yin et al., 2018). A chal-
lenge with these methods is that (combinatorially)
complex discrete variables make optimization very
hard, even with the help of variational inference. In
this work, we seek to directly constrain the discrimi-
native parser with signals obtained from executions.
Our method can potentially be integrated into these
generative models to regularize discrete variables.

(Underspeciﬁed)
Sequence-Level Rewards
There have been attempts in recent years to
integrate sequence-level rewards into sequence-
to-sequence training as a way of accommodating
task-speciﬁc objectives. For example, BLEU can
be optimized for coherent text generation (Bosse-
lut et al., 2018) and machine translation (Wu
et al., 2018) via reinforcement learning or beam-
search (Wiseman and Rush, 2016). In this work,
we resort to marginal likelihood training to exploit
binary executability rewards for semantic parsing
(i.e., whether a program is executable or not),
which has been shown to be more effective than
REINFORCE (Guu et al., 2017).

More importantly, our binary reward is under-
speciﬁed, i.e., there exist many spurious programs
that enjoy the same reward as the gold program.

This issue of learning from underspeciﬁed re-
wards underlies many weakly-supervised tasks,
e.g., learning from denotations (Liang et al., 2013;
Berant et al., 2013), weakly supervised question
answering (Min et al., 2019). Previous work tried
to model latent alignments (Wang et al., 2019) be-
tween NL and programs to alleviate this issue. In
this work, we take an orthogonal direction and pro-
pose several training objectives that alleviate the
impact of spurious programs.

Execution for Semantic Parsing Execution has
been utilized in semantic parsing (Wang et al.,
2018) and the related area of program synthe-
sis (Chen et al., 2019). These approaches ex-
ploit the execution of partial programs to guide
the search for plausible complete programs. Al-
though partial execution is feasible for SQL-style
programs, it cannot be trivially extended to general
meaning representation (e.g., logical forms). In
this work, we explore a more general setting where
execution can be only obtained from complete pro-
grams.

3 Executability as Learning Signal

In this section, we formally deﬁne our semi-
supervised learning setting and show how to in-
corporate executability into the training objective
whilst relying on the marginal likelihood training
framework. We also present two conventional ap-
proaches to optimizing marginal likelihood.

3.1 Problem Deﬁnition

i)}N

Given a set of labeled NL-program pairs
{(xl
i, yl
i=1 and a set of unlabeled NL utterances
{xj}M
j=1, where N and M denote the sizes of the
respective datasets, we would like to learn a neural
parser p(y|x, θ), parameterized by θ, that maps ut-
terances to programs. The objective to minimize
consists of two parts:

J =

1
N

N
(cid:88)

i=1

Lsup(xl

i, yl

i) + λ

1
M

M
(cid:88)

j=1

Lunsup(xi)

(1)
where Lsup and Lunsup denote the supervised and
unsupervised loss, respectively. For labeled data,
we use the negative log-likelihood of gold pro-
grams; for unlabeled data, we instead use the log
marginal likelihood (MML) of all executable pro-

grams. Speciﬁcally, they are deﬁned as follows:

Lsup(x, y) = − log p(y|x, θ)

Lunsup(x) = − log

R(y)p(y|x, θ)

(cid:88)

y

(2)

(3)

where R(y) is a binary reward function that returns
1 if y is executable and 0 otherwise. In practice, this
function is implemented by running a task-speciﬁc
executor, e.g., a SQL executor.

Another alternative to unsupervised loss is RE-
INFORCE (Sutton et al., 1999), i.e., maximize the
expected R(y) with respect to p(y|x, θ). However,
as presented in Guu et al. (2017), this objective
usually underperforms MML, which is consistent
with our initial experiments.2

3.2 Self-Training and Top-K MML

MML in Equation (3) requires marginalizing
over all executable programs which is intractable.
Conventionally, we resort to beam search to explore
the space of programs and collect executable ones.
To illustrate, we can divide the space of programs
into four parts based on whether they are executable
and observed, as shown in Figure 2a. For example,
programs in PSE ∪ PSN are seen in the sense that
they are retrieved by beam search. Programs in
PSE ∪ PUE are all executable, though only programs
in PSE can be directly observed.

Two common approximations of Equation (3)
are Self-Training (ST) and Top-K MML, and they
are deﬁned as follows:

LST(x, θ) = − log p(y∗|x, θ)

Ltop-k(x, θ) = − log

(cid:88)

y∈PSE

p(y|x, θ)

(4)

(5)

where y∗ denotes the most probable program, and
it is approximated by the most probable one from
beam search.

It is obvious that both methods only exploit pro-
grams in PSE, i.e., executable programs retrieved by
beam search. In cases where a parser successfully
includes the correct programs in PSE, both approxi-
mations should work reasonably well. However, if
a parser is uncertain and PSE does not contain the
gold program, it would then mistakenly exploit in-
correct programs in learning, which is problematic.
A naive solution to improve Self-Training or
Top-K MML is to explore a larger space, e.g., in-
crease the beam size to retrieve more executable
2We review the comparison between REINFORCE and

MML in the appendix.

Seen Programs

Unseen Programs

LST(x, θ) = − log p(y∗|x, θ)

Ltop-k(x, θ) = − log

(cid:88)

p(y|x, θ)

Executable

Programs

*
PSE

PUE

y∈PSE
Lrepulsion(x, θ) = − log (cid:0)1 −

(cid:88)

p(y|x, θ)(cid:1)

y∈PSN

Non-Executable

Programs

PSN

PUN

(a) Partitioned program space. Red asterisk denotes the
most probable executable program y∗. P stands for pro-
gram; subscript S stands for seen, U for unseen, E for
executable, and N for non-executable.

Lgentle(x, θ) = −p(PSE ∪ PSN) log

− p(PUE ∪ PUN) log

(cid:88)

y∈PSE
(cid:88)

p(y|x, θ)

p(y|x, θ)

Lsparse(x, θ) = −

(cid:88)

y∈PSE

y∈PUE∪PUN

qsparse(y) log p(y|x, θ)

(b) Five objectives to approximate MML.

Figure 2: In (a) the program space is partitioned along two dimentions: executability and observability. In (b) we
show two commonly used objectives (Self-Training and Top-K MML) and the three objectives proposed in this
work.

programs. However, this would inevitably increase
the computation cost of learning. We also show in
the appendix that increasing beam size, after it ex-
ceeds a certain threshold, is no longer beneﬁcial for
learning. In this work, we instead propose better
approximations without increasing beam size.

the KL-divergence between Q and p, which is:

JQ(θ) = DKL[Q||p(y|x, θ)]

= min
q∈Q

DKL[q(y)||p(y|x, θ)]

(6)

By deﬁnition, the objective has the following

upper bound:

4 Method

We ﬁrst present a view of MML in Equation (3)
from the perspective of posterior regularization.
This new perspective helps us derive three alter-
native approximations of MML: Repulsion MML,
Gentle MML, and Sparse MML.

4.1 Posterior Regularization

Posterior regularization (PR) allows to inject lin-
ear constraints into posterior distributions of gener-
ative models, and it can be extended to discrimina-
tive models (Ganchev et al., 2010). In our case, we
try to constrain the parser p(y|x, θ) to only assign
probability mass to executable programs. Instead
of imposing hard constraints, we softly penalize
the parser if it is far away from a desired distribu-
tion q(y), which is deﬁned as Eq[R(y)] = 1. Since
R is a binary reward function, q(y) is constrained
to only place mass on executable programs whose
rewards are 1. We denote all such desired distribu-
tions as the family Q.

Speciﬁcally, the objective of PR is to penalize

J (θ, q) = DKL[q(y)||p(y|x, θ)]
(cid:88)

= −

q(y) log p(y|x, θ) − H(q) (7)

y

where q ∈ Q, H denotes the entropy. We can
use block-coordinate descent, an EM iterative algo-
rithm to optimize it.

E : qt+1 = arg min

q∈Q

DKL[q(y)||p(y|x, θt)]

M : θt+1 = arg min

−

θ

(cid:88)

y

qt+1(y)[log p(y|x, θ)]

During the E-step, we try to ﬁnd a distribution
q from the constrained set Q that is closest to the
current parser p in terms of KL-divergence. We
then use q as a ‘soft label’ and minimize the cross-
entropy between q and p during the M-step. Note
that q is a constant vector and has no gradient wrt.
θ during the M-step.

The E-step has a closed-form solution:

qt+1(y) =

(cid:40) p(y|x,θt)
p(PSE∪PUE)
0

y ∈ PSE ∪ PUE
otherwise

(8)

y(cid:48)∈PSE∪PUE

where p(PSE ∪ PUE) = (cid:80)
p(y(cid:48)|x, θt).
qt+1(y) is essentially a re-normalized version of p
over executable programs. Interestingly, if we use
the solution in the M-step, the gradient wrt. θ is
equivalent to the gradient of MML in Equation (3).
That is, optimizing PR with the EM algorithm is
equivalent to optimizing MML.3 The connection
between EM and MML is not new, and it has been
well-studied for classiﬁcation problems (Amini and
Gallinari, 2002; Grandvalet and Bengio, 2004). In
our problem, we additionally introduce PR to ac-
commodate the executability constraint, and instan-
tiate the general EM algorithm.

Although the E-step has a closed-form solution,
computing q is still intractable due to the large
search space of executable programs. However, this
PR view provides new insight on what it means to
approximate MML. In essence, conventional meth-
ods can be viewed as computing an approximate so-
lution of q. Speciﬁcally, Self-Training corresponds
to a delta distribution that only focuses on the most
probable y∗.

qt+1
ST (y) =

(cid:26) 1
0

y = y∗
otherwise

Top-K MML corresponds to a re-normarlized dis-
tribution over PSE.

qt+1
top-k(y) =

(cid:40) p(y|x,θt)
p(PSE)
0

y ∈ PSE
otherwise

Most importantly, this perspective leads us to de-
riving three new approximations of MML, which
we collectively call X-PR.

4.2 Repulsion MML and Gentle MML

As mentioned previously, Self-Training and Top-
K MML should be reasonable approximations in
cases where gold programs are retrieved, i.e., they
are in the seen executable subset (PSE in Figure 2a).
However, if a parser is uncertain, i.e., beam search
cannot retrieve the gold programs, exclusively ex-
ploiting PSE programs is undesirable. Hence, we
consider ways of taking unseen executable pro-
grams (PUE in Figure 2a) into account. Since we
never directly observe unseen programs (PUE or
PUN), our heuristics do not discriminate between ex-
ecutable and non-executable programs (PUE ∪ PUN).
In other words, upweighting PUE programs will
inevitably upweight PUN.

3Please see the appendix for the proof and analysis of PR.

Based on the intuition that the correct program is
included in either seen executable programs (PSE)
or unseen programs (PUE and PUN), we can sim-
ply push a parser away from seen non-executable
programs (PSN). Hence, we call such method
Repulsion MML. Speciﬁcally, the ﬁrst heuristic
approximates Equation (8) as follows:

qt+1
repulsion(y) =

(cid:40) p(y|x,θt)
1−p(PSN)
0

y (cid:54)∈ PSN
otherwise

Another way to view this heuristic is that we
distribute the probability mass from seen non-
executable programs (PSN) to other programs. In
contrast, the second heuristic is more ‘conserva-
tive’ about unseen programs as it tends to trust seen
executable PSN programs more. Speciﬁcally, the
second heuristic uses the following approximations
to solve the E-step.

qt+1
gentle(y) =






p(PSE∪SN)
p(PSE) p(y|x, θt)
p(y|x, θt)
0

y ∈ PSE

y ∈ PUE ∪ PUN

y ∈ PSN

Intuitively,

it shifts the probability mass of
seen non-executable programs (PSN) directly to
seen executable programs (PSE). Meanwhile,
it neither upweights nor downweights unseen
programs. We call this heuristic Gentle MML.
Compared with Self-Training and Top-K MML,
Repulsion MML and Gentle MML lead to better
exploration of the program space, as only seen non-
executable (PSN) programs are discouraged.

4.3 Sparse MML

Sparse MML is based on the intuition that in
most cases there is only one or few correct pro-
grams among all executable programs. As men-
tioned in Section 2, spurious programs that are exe-
cutable, but do not reﬂect the semantics of an utter-
ance are harmful. One empirical evidence from pre-
vious work (Min et al., 2019) is that Self-Training
outperforms Top-K MML for weakly-supervised
question answering. Hence, exploiting all seen
executable programs can be sub-optimal. Follow-
ing recent work on sparse distributions (Martins
and Astudillo, 2016; Niculae et al., 2018), we pro-
pose to encourage sparsity of the ‘soft label’ q.
Encouraging sparsity is also related to the mini-
mum entropy and low-density separation princi-
ples which are commonly used in semi-supervised
learning (Grandvalet and Bengio, 2004; Chapelle
and Zien, 2005).

To achieve this, we ﬁrst interpret the entropy
term H in Equation (7) as a regularization of q. It is
known that entropy regularization always results in
a dense q, i.e., all executable programs are assigned
non-zero probability. Inspired by SparseMax (Mar-
tins and Astudillo, 2016), we instead use L2 norm
for regularization. Speciﬁcally, we replace our PR
objective in Equation (7) with the following one:

Jsparse(θ, q) = −

(cid:88)

y

q(y) log p(y|s, θ) +

1
2

||q||2
2

where q ∈ Q. Similarly, it can be optimized by the
EM algorithm:

E : qt+1 = SparseMaxQ(log p(y|x, θt))
M : θt+1 = arg min
−

(cid:88)

qt+1(y)[log p(y|x, θ)]

θ

y

where the top-E-step can be solved by the
SparseMax operator, which denotes the Euclidean
projection from the vector of logits log p(y|x, θt)
to the simplex Q. Again, we solve the E-step
approximately. One of the approximations is to
use top-k SparseMax which constrain the num-
ber of non-zeros of q to be less than k. It can be
solved by using a top-k operator and followed by
SparseMax (Correia et al., 2020). In our case, we
use beam search to approximate the top-k operator
and the resulting approximation for the E-step is
deﬁned as follows:

qt+1
sparse = SparseMaxy∈PSE

(cid:0) log p(y|x, θt)(cid:1)

Intuitively, qt+1
sparse occupies the middle ground be-
tween Self-Training (uses y∗ only) and Top-K
MML (uses all PSE programs). With the help of
sparsity of q introduced by SparseMax, the M-step
will only promote a subset of PSE programs.

Summary We propose three new approxima-
tions of MML for learning from executions. They
are designed to complement Self-Training and Top-
K MML via discouraging seen non-executable pro-
grams and introducing sparsity. In the following
sections, we will empirically show that they are
superior to Self-Training and Top-K MML for
semi-supervised semantic parsing. The approxi-
mations we proposed may also be beneﬁcial for
learning from denotations (Liang et al., 2013; Be-
rant et al., 2013) and weakly supervised question
answering (Min et al., 2019), but we leave this to
future work.

5 Semantic Parsers

In principle, our X-PR framework is model-
agnostic, i.e., it can be coupled with any semantic
parser for semi-supervised learning. In this work,
we use a neural parser that achieves state-of-the-art
performance across semantic parsing tasks. Specif-
ically, we use RAT-SQL (Wang et al., 2020) which
features a relation-aware encoder and a grammar-
based decoder. The parser was originally devel-
oped for text-to-SQL parsing, and we adapt it to
text-to-LF parsing. In this section, we brieﬂy re-
view the encoder and decoder of this parser. For
more details, please refer to Wang et al. (2020).

5.1 Relation-Aware Encoding

Relation-aware encoding is originally designed
to handle schema encoding and schema linking
for text-to-SQL parsing. We generalize these two
notions for both text-to-LF and text-to-SQL parsing
as follows:

• enviroment encoding: encoding enviroments,
i.e., a knowledge base consisting of a set of
triples; a relational database represented by its
schema

• enviroment linking: linking mentions to in-
tended elements of environments, i.e., men-
tions of entities and properties of knowledge
bases; mentions of tables and columns of rela-
tional databases

Relation-aware attention is introduced to inject dis-
crete relations between environment items, and be-
tween the utterance and environments into the self-
attention mechanism of Transformer (Devlin et al.,
2019). The details of relation-aware encoding can
be found in the appendix.

5.2 Grammar-Based Decoding

Typical sequence-to-sequence models (Dong
and Lapata, 2016; Jia and Liang, 2016a) treat pro-
grams as sequences, ignoring their internal struc-
ture. As a result, the well-formedness of generated
programs cannot be guaranteed. Grammar-based
decoders aim to remedy this issue. For text-to-LF
parsing, we use the type-constrained decoder pro-
posed by Krishnamurthy et al. (2017); for text-to-
SQL parsing, we use an AST (abstract syntax tree)
based decoder following Yin and Neubig (2018).
Note that grammar-based decoding can only ensure
the syntactic correctness of generated programs.
Executable programs are additionally semantically
correct. For example, all programs in Figure 1 are

OVERNIGHT

Model

BASKETBALL BLOCKS CALENDAR HOUSING PUBLICATIONS RECIPES RESTAURANTS

SOCIAL Avg.

Lower bound
Self-Traing
Top-K MML
Repulsion MML
Gentle MML
Sparse MML
Upper Bound

82.2
84.7
83.1
84.9
84.1
83.9
87.7

54.1
52.6
55.3
56.3
58.1
58.6
62.9

64.9
67.9
68.5
70.8
70.2
72.6
82.1

61.4
59.8
56.9
60.9
63.0
60.3
71.4

64.3
68.6
67.7
70.3
71.5
75.2
78.9

72.7
80.1
73.7
79.8
78.7
80.6
82.4

71.7
71.1
69.9
72.0
72.3
72.6
82.8

76.7
77.4
76.2
78.3
76.4
77.8
80.8

68.5
70.3
68.9
71.7
71.8
72.7
78.6

GEO

60.6
64.2
61.3
64.9
65.6
67.4
74.2

Table 1: Execution accuracy of supervised and semi-supervised models on all domains of OVERNIGHT and GEO-
QUERY. In semi-supervised learning, 30% of the original training examples are treated as labeled and the re-
maining 70% as unlabeled. Lower bound refers to supervised models that only use labeled examples and discard
unlabeled ones whereas upper bound refers to supervised models that have access to gold programs of unlabeled
examples. Avg. refers to the average accuracy of the eight OVERNIGHT domains. We average runs over three
random splits of the original training data for semi-supervised learning.

well-formed, but the ﬁrst two programs are seman-
tically incorrect.

6 Experiments

To evaluate X-PR, we present experiments on
semi-supervised semantic parsing. We also analyze
how the objectives affect the training process.

6.1 Semi-Supervised Learning

We simulate the setting of semi-supervised learn-
ing on standard text-to-LF and text-to-SQL parsing
benchmarks. Speciﬁcally, we randomly sample
30% of the original training data as the labeled
data, and use the rest 70% as the unlabeled data.
For text-to-LF parsing, we use the OVERNIGHT
dataset (Wang et al., 2015a), which has eight dif-
ferent domains, each with a different size rang-
ing between 801 and 4,419; for text-to-SQL pars-
ing, we use GEOQUERY (Zelle and Mooney, 1996)
which contains 880 utterance-SQL pairs. The semi-
supervised setting is very challenging as leveraging
only 30% of the original training data would re-
sult in only around 300 labeled examples in four
domains of OVERNIGHT and also in GEOQUERY.

Supervised Lower and Upper Bounds As base-
lines, we train two supervised models. The ﬁrst
one only uses the labeled data (30% of the original
training data) and discards the unlabeled data in the
semi-supervised setting. We view this baseline as a
lower bound in the sense that any semi-supervised
method is expected to surpass this. The second one
has extra access to gold programs for the unlabeled
data in the semi-supervised setting, which means
it uses the full original training data. We view this
baseline as an upper bound for semi-supervised
learning; we cannot expect to approach it as the

executability signal is much weaker than direct
supervision. By comparing the performance of
the second baseline (upper bound) with previous
methods (Jia and Liang, 2016b; Herzig and Berant,
2017; Su and Yan, 2017), we can verify that our
semantic parsers are state-of-the-art. Please refer to
the Appendix for detailed comparisons. Our main
experiments aim to show how the proposed objec-
tives can mitigate the gap between the lower- and
upper-bound baselines by utilizing 70% unlabeled
data.

Semi-Supervised Training and Tuning We use
to optimize Equa-
stochastic gradient descent
tion (1). At each training step, we sample two
batches from the labeled and unlabeled data, re-
spectively. In preliminary experiments, we found
that it is crucial to pre-train a parser on supervised
data alone; this is not surprising as all of the ob-
jectives for learning from execution rely on beam
search which would only introduce noise with an
untrained parser. That is, λ in Equation (1) is set to
0 during initial updates, and is switched to a normal
value afterwards.

We leave out 100 labeled examples for tuning
the hyperparameters. The hyperparameters of the
semantic parser are only tuned for the development
of the supervised baselines, and are ﬁxed for semi-
supervised learning. The only hyperparameter we
tune in the semi-supervised setting is the λ in Equa-
tion (1), which controls how much the unsuper-
vised objective inﬂuences learning. After tuning,
we use all the labeled examples for supervised train-
ing and use the last checkpoints for evaluation on
the test set.

(a) Average Ratios.

(b) Coverage of gold programs.

Figure 3: Effect of different learning objectives in terms of average ratios and coverage (view in color).

6.2 Main Results

Our experiments evaluate the objectives pre-
sented in Figure 2 under a semi-supervised learning
setting. Our results are shown in Table 1.

Self-Training and Top-K MML First, Top-K
MML, which exploits more executable programs
than Self-Training, does not yield better perfor-
mance in six domains of OVERNIGHT and GEO-
QUERY. This observation is consistent with Min
et al. (2019) where Top-K MML underperforms
Self-Training for weakly-supervised question an-
swering. Self-Training outperforms the lower
bound in ﬁve domains of OVERNIGHT, and on
average. In contrast, Top-K MML obtains a similar
performance to the lower bound in terms of average
accuracy.

In

X-PR Objectives
domain
of
each
the objective
OVERNIGHT and GEOQUERY,
that achieves the best performance is always
within X-PR. In terms of average accuracy in
OVERNIGHT, all our objectives perform better
than Self-Training and Top-K MML. Among
X-PR, Sparse MML performs best in ﬁve domains
of OVERNIGHT, leading to a margin of 4.2%
compared with the lower bound in terms of average
In GEOQUERY, Sparse MML also
accuracy.
obtain best performance.

a

similar

accuracy

Based on the same intuition of discouraging
seen non-executable programs, Repulsion MML
achieves
to
average
Gentle MML in OVERNIGHT.
In contrast,
Gentle MML tends to perform better in domains
whose parser are weak (such as HOUSING,
In
BLOCKS) indicated by their lower bounds.
GEOQUERY, Gentle MML performs
slightly
better than Repulsion MML. Although it does not

perform better than Repulsion MML, it retrieves
more accurate programs and also generates longer
programs (see next section for details).

To see how much labeled data would be needed
for a supervised model to reach the same accuracy
as our semi-supervised models, we conduct experi-
ments using 40% of the original training examples
as the labeled data. The supervised model achieves
72.6% on average in OVERNIGHT, implying that
‘labeling’ 33.3% more examples would yield the
same accuracy as our best-performning objective
(Sparse MML).

6.3 Analysis

To better understand the effect of different objec-
tives, we conduct analysis on the training process
of semi-supervised learning. For the sake of brevity,
we focus our analysis on the CALENDAR domain
but have drawn similar conclusions for the other
domains.

Length Ratio During preliminary experiments,
we found that all training objectives tend to favor
short executable programs for unlabeled utterances.
To quantify this, we deﬁne the metric of average
ratio as follows:

ratio =

(cid:80)

(cid:80)

i
(cid:80)

y∈PSE(xi) |y|
i |xi||PSE(xi)|

(9)

where PSE(xi) denotes seen executable programs
of xi, |x|, |y| denotes the length of an utterance and
a program, respectively, and |PSE(xi)| denotes the
number of seen executable programs. Intuitively,
average ratio reveals the range of programs that
an objective is exploiting in terms of length. This
metric is computed in an online manner, and xi is
a sequence of data fed to the training process.

As shown in Figure 3a, Top-K MML favors
shorter programs, especially during the initial steps.
In contrast, Repulsion MML and Gentle MML pre-
fer longer programs. For reference, we can com-
pute the gold ratio by assuming PSE(xi) only con-
tains the gold program. The gold ratio for CAL-
ENDAR is 2.01, indicating that all objectives are
still preferring programs that are shorter than gold
programs. However, by not directly exploiting
seen executable programs, Repulsion MML and
Gentle MML alleviate this issue compared with
Top-K MML.

Coverage Next, we analyze how much an objec-
tive can help a parser retrieve gold programs for
unlabeled data. Since the orignal data contains the
gold programs for the unlabeled data, we ultilize
them to deﬁne the metric of coverage as follows:
(cid:80)

coverage =

i I[ˆyi ∈ PSE(xi)]
i |xi|

(cid:80)

(10)

where I is an indicator function, ˆyi denotes the gold
program of an utterance xi. Intuitively, this metric
measures how often a gold program is captured in
PSE. As shown in Figure 3b, Self-Training, which
only exploits one program at a time, is relatively
weak in terms of retrieving more gold programs.
In contrast, Repulsion MML retrieves more gold
programs than the others.

As mentioned in Section 4.3, SparseMax can be
viewed as an interpolation between Self-Training
and Top-K MML. This is also reﬂected in both
metrics: Sparse MML always occupies the middle-
ground performance between ST and Top-K MML.
Interestingly, although Sparse MML is not best in
terms of both diagnostic metrics, it still achieves
the best accuracy in this domain.

7 Conclusion

In this work, we propose to learn a semi-
supervised semantic parser from the weak yet
freely available executability signals. Due to the
large search space of executable programs, conven-
tional approximations of MML training, i.e, Self-
Training and Top-K MML, are often sub-optimal.
We propose a set of alternative objectives, namely
X-PR, through the lens of posterior regularization.
Empirical results on semi-supervised learning show
that X-PR can help a parser achieve substantially
better performance than conventional methods, fur-
ther bridging the gap between semi-supervised
learning and supervised learning. In the future, we

would like to extend X-PR to related tasks such as
learning from denotations and weakly supervised
question answering.

Acknowledgements We would like to thank the
anonymous reviewers for their valuable comments.
We gratefully acknowledge the support of the Euro-
pean Research Council (Titov: ERC StG Broad-
Sem 678254; Lapata: ERC CoG TransModal
681760) and the Dutch National Science Founda-
tion (NWO VIDI 639.022.518).

References

Massih-Reza Amini and Patrick Gallinari. 2002. Semi-
supervised logistic regression. In ECAI, pages 390–
394.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533–1544, Seattle, Wash-
ington, USA. Association for Computational Lin-
guistics.

Antoine Bosselut, Asli Celikyilmaz, Xiaodong He,
Jianfeng Gao, Po-Sen Huang, and Yejin Choi. 2018.
Discourse-aware neural rewards for coherent text
generation. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), pages 173–
184, New Orleans, Louisiana. Association for Com-
putational Linguistics.

Olivier Chapelle and Alexander Zien. 2005. Semi-
supervised classiﬁcation by low density separation.
In AISTATS, volume 2005, pages 57–64. Citeseer.

Xinyun Chen, Chang Liu, and Dawn Song. 2019.
Execution-guided neural program synthesis. In 7th
International Conference on Learning Representa-
tions, ICLR 2019, New Orleans, LA, USA, May 6-9,
2019. OpenReview.net.

Gonçalo M. Correia, Vlad Niculae, Wilker Aziz, and
André F. T. Martins. 2020. Efﬁcient marginalization
of discrete and structured latent variables via spar-
sity. In Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Informa-
tion Processing Systems 2020, NeurIPS 2020, De-
cember 6-12, 2020, virtual.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
In Proceedings of the 2019 Conference
standing.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
33–43, Berlin, Germany. Association for Computa-
tional Linguistics.

Kuzman Ganchev, Joao Graça, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. The Journal of
Machine Learning Research, 11:2001–2049.

Yves Grandvalet and Yoshua Bengio. 2004. Semi-
supervised learning by entropy minimization.
In
Advances in Neural Information Processing Systems
17 [Neural Information Processing Systems, NIPS
2004, December 13-18, 2004, Vancouver, British
Columbia, Canada], pages 529–536.

Kelvin Guu, Panupong Pasupat, Evan Liu, and Percy
Liang. 2017. From language to programs: Bridg-
ing reinforcement learning and maximum marginal
likelihood. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1051–1062, Van-
couver, Canada. Association for Computational Lin-
guistics.

Jonathan Herzig and Jonathan Berant. 2017. Neural Se-
mantic Parsing over Multiple Knowledge-bases. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), volume 2, pages 623–628, Strouds-
burg, PA, USA.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant
Krishnamurthy, and Luke Zettlemoyer. 2017. Learn-
ing a neural semantic parser from user feedback. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 963–973, Vancouver, Canada.
Association for Computational Linguistics.

Robin Jia and Percy Liang. 2016a. Data recombination
for neural semantic parsing. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
12–22, Berlin, Germany. Association for Computa-
tional Linguistics.

Robin Jia and Percy Liang. 2016b. Data Recombina-
In Proceedings
tion for Neural Semantic Parsing.
of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 12–22, Stroudsburg, PA, USA.

Tomáš Koˇciský, Gábor Melis, Edward Grefenstette,
Chris Dyer, Wang Ling, Phil Blunsom,
and
Karl Moritz Hermann. 2016. Semantic parsing with
In Pro-
semi-supervised sequential autoencoders.
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1078–
1087, Austin, Texas. Association for Computational
Linguistics.

Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gard-
ner. 2017. Neural semantic parsing with type con-
In Proceedings
straints for semi-structured tables.
of the 2017 Conference on Empirical Methods in
Natural Language Processing, pages 1516–1526,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.

Percy Liang, Michael I. Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389–446.

André F. T. Martins and Ramón Fernandez Astudillo.
2016. From softmax to sparsemax: A sparse model
In Pro-
of attention and multi-label classiﬁcation.
ceedings of the 33nd International Conference on
Machine Learning, ICML 2016, New York City,
NY, USA, June 19-24, 2016, volume 48 of JMLR
Workshop and Conference Proceedings, pages 1614–
1623. JMLR.org.

Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and
Luke Zettlemoyer. 2019. A discrete hard EM ap-
proach for weakly supervised question answering.
In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 2851–
2864, Hong Kong, China. Association for Computa-
tional Linguistics.

Vlad Niculae, André F. T. Martins, Mathieu Blondel,
and Claire Cardie. 2018. Sparsemap: Differentiable
In Proceedings of the
sparse structured inference.
35th International Conference on Machine Learning,
ICML 2018, Stockholmsmässan, Stockholm, Sweden,
July 10-15, 2018, volume 80 of Proceedings of Ma-
chine Learning Research, pages 3796–3805. PMLR.

Yu Su and Xifeng Yan. 2017. Cross-domain seman-
tic parsing via paraphrasing. In Proceedings of the
2017 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1235–1246, Copen-
hagen, Denmark.

Richard S Sutton, David A McAllester, Satinder P
Singh, Yishay Mansour, et al. 1999. Policy gradient
methods for reinforcement learning with function ap-
proximation. In NIPs, volume 99, pages 1057–1063.
Citeseer.

Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr
Polozov, and Matthew Richardson. 2020. RAT-SQL:
Relation-aware schema encoding and linking for
text-to-SQL parsers. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 7567–7578, Online. Association
for Computational Linguistics.

Bailin Wang, Ivan Titov, and Mirella Lapata. 2019.
Learning semantic parsers from denotations with la-
tent structured alignments and abstract programs. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the

9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 3774–
3785, Hong Kong, China. Association for Computa-
tional Linguistics.

Chenglong Wang,

Kedar

Tatwawadi, Marc
Brockschmidt, Po-Sen Huang, Yi Mao, Olek-
sandr Polozov, and Rishabh Singh. 2018. Robust
text-to-sql
execution-guided
generation with
decoding. arXiv preprint arXiv:1807.03100.

Yushi Wang, Jonathan Berant, and Percy Liang. 2015a.
In Proceed-
Building a semantic parser overnight.
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pages 1332–1342,
Beijing, China. Association for Computational Lin-
guistics.

Yushi Wang, Jonathan Berant, and Percy Liang. 2015b.
Building a Semantic Parser Overnight. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), volume 1, pages
1332–1342, Stroudsburg, PA, USA.

Sam Wiseman and Alexander M. Rush. 2016.
Sequence-to-sequence learning as beam-search op-
timization. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1296–1306, Austin, Texas. Association
for Computational Linguistics.

Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-
Yan Liu. 2018. A study of reinforcement learning
In Proceedings of
for neural machine translation.
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing, pages 3612–3621, Brus-
sels, Belgium. Association for Computational Lin-
guistics.

Pengcheng Yin and Graham Neubig. 2018. TRANX: A
transition-based neural abstract syntax parser for se-
mantic parsing and code generation. In Proceedings
of the 2018 Conference on Empirical Methods in
Natural Language Processing: System Demonstra-
tions, pages 7–12, Brussels, Belgium. Association
for Computational Linguistics.

Pengcheng Yin, Chunting Zhou, Junxian He, and Gra-
ham Neubig. 2018. StructVAE: Tree-structured la-
tent variable models for semi-supervised semantic
In Proceedings of the 56th Annual Meet-
parsing.
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 754–765, Mel-
bourne, Australia. Association for Computational
Linguistics.

John M Zelle and Raymond J Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the national con-
ference on artiﬁcial intelligence, pages 1050–1055.

A Method

MML vs. RL Another choice for learning from
executions is to maximize the expected reward:

JRL(x, θ) = Ep(y|x,θ)[R(y)]

(11)

Recall that our MML objective is deﬁned as:

JMML(x, θ) = log

(cid:88)

y

R(y)p(y|x, θ)

(12)

As shown in previous work (Guu et al., 2017), the
gradients (wrt. θ) of RL and MML have the same
form:

∇θJ (x, θ) =

(cid:88)

y

q(y)∇θ log p(y|x, θ)

(13)

(cid:80)

where q can be viewed as a soft-label that is in-
duced from p. For RL, qRL(y) = pθ(y|x, θ)R(y);
R(y)pθ(y|x)
for MML, qMML(y) =
y(cid:48) R(y(cid:48))pθ(y(cid:48)|x) =
pθ(y|x, R(y) = 1). Compared with RL, MML
additionally renormalize the p over executable pro-
grams to obtain q. As a result, MML outputs a
‘stronger’ gradient than RL due to the renormaliza-
tion. In our initial experiments, we found that RL
is hard to converge whereas MML is not.

Proof of the E-Step Solution Denote the set of
executable programs as V. Since q(y) is 0 for non-
executable programs, we only need to compute it
for executable programs in V. We need to solve the
following optimization problem:

min
q

−

(cid:88)

y∈V

q(y) log p(y|x, θ) +

q(y) log q(y)

(cid:88)

y∈V

s.t.

(cid:88)

y∈V

q(y) = 1

q(y) ≥ 0

(14)

By solving it with KKT conditions, we can see
that q(y) ∝ p(y|x, θ). Since q(y) needs to sum up
to 1, it is easy to obtain that q(y) = p(y|x,θ)
y∈V p(y|x,θ) .

(cid:80)

B Experiments

Relation-Aware Encoding of Semantic Parsers
Relation-aware encoding was originally introduced
for text-to-SQL parsing in Wang et al. (2020) to ac-
commodate discrete relations among schema items
(e.g., tables and columns) and linking between an

Type of x

Type of y

Edge label

Description

Entity

Entity

Entity

Property

Property

Entity

Utterance Token Entity

RELATED-F
RELATED-R

there exists a property p s.t. (x, p, y) ∈ K
there exists a property p s.t. (y, p, x) ∈ K

HAS-PROPERTY-F
HAS-PROPERTY-R

there exists an entity e s.t. (x, y, e) ∈ K
there exists an entity e s.t. (e, y, x) ∈ K

PROP-TO-ENT-F
PROP-TO-ENT-R

EXACT-MATCH
PARTIAL-MATCH

there exists an entity e s.t. (y, x, e) ∈ K
there exists an entity e s.t. (e, x, y) ∈ K

x and y are the same word
token x is contained in entity y

Entity

Utterance Token

Utterance Token Property

Property

Utterance Token

EXACT-MATCH-R
PARTIAL-MATCH-R

y and x are the same word
token y is contained in entity x

P-EXACT-MATCH
P-PARTIAL-MATCH

x and y are the same word
token x is contained in property y

P-EXACT-MATCH-R
P-PARTIAL-MATCH-R token y is contained in property y

y and x are the same word

Table 2: Relation types used for text-to-LF parsing.

i }N −1

utterance and schema items. Let {x(0)
i=0 de-
note the input to the parser consisting of NL to-
kens and the (linearized version of) environment;
relation-aware encoding changes the multi-head
self-attention (with H heads and hidden size dx) as
follows:

x(n)
i W (n,h)

Q

e(n,h)
ij =

α(n,h)
ij = softmaxj
n
(cid:88)

z(n,h)
i

=

K + rK

ij )(cid:62)

(x(n)
j W (n,h)
(cid:112)dz/H
(cid:9)

(cid:8)e(n,h)
ij

(15)

α(n,h)
ij

(x(n)

j W (n,h)

V

+ rV

ij ).

j=1

ij

denotes the attention weights of head

where α(n,h)
h at layer n, 0 ≤ h < H, 0 ≤ n < N , and
W (h)
Q , W (h)
V ∈ Rdx×(dx/H). Most impor-
tantly, rK
ij are key and value embeddings
of the discrete relation rij between items i and
j. They are incorporated to bias attention towards
discrete relations.

K , W (h)
ij and rV

In this work, we re-use the relations from Wang
et al. (2020) for our text-to-SQL parsing task on
the GEOQUERY dataset. For text-to-LF parsing
on the OVERNIGHT dataset, we elaborate on how
we deﬁne discrete relations. The input of text-to-
LF parsing is an utterance and a ﬁxed knowledge
base K which is represented as a set of triples in
the form of (entity1, property, entity2). To feed
the input into a transformer, we ﬁrst linearize it
into an ordered sequence which contains a list of
utterance tokens, followed by a sequence of entities
and properties. To model the relations among the

items of this sequence, we deﬁne relations between
each pair of items, denoted by (x, y), in Table 2.

Statitics of Datasets The numbers of examples
in each domain are shown in Table 3. Four do-
mains of OVERNIGHT and GEOQUERY contain
only around 1000 examples.

Results of Supervised Models The results of su-
pervised models (upper bounds) are shown in Ta-
ble 4. Our parser achieves the best performance
among models without cross-domain training. This
conﬁrms that we use a strong base parser for our
experiments on semi-supervised learning.

Beam Size We try the beam size from
{4, 8, 16, 32, 64} and ﬁnally picks 16 which per-
forms best in the OVERNIGHT dataset. We also try
a larger beam size of 128 during preliminary exper-
iments. However, the model is extremely slow to
train and does not outperform the one with beam
size 16.

Semi-Supervised Learning with 10% Data
We also investigate a semi-supervised setting where
only 10% of the original training data are treated
as labeled data. This is more challenging as four
domains of OVERNIGHT and GEOQUERY only
have around 100 labeled utterance-program pairs
in this setting. The results are shown in Table 5. In
terms of average accuracy, Sparse MML achieves
the best performance. The margin of improve-
ment, compared to the lower bound, is relatively
smaller than using 30% data (3% vs 4.2%). As
all objectives rely on beam search, a weak base
parser, as indicated by the lower bound, is probably

BASKETBALL BLOCKS CALENDAR HOUSING PUBLICATIONS RECIPES RESTAURANTS

SOCIAL GEO

all

1952

1995

837

941

801

1080

1657

4419

880

Table 3: Data sizes of the OVERNIGHT and GEOQUERY dataset.

Model

BASKETBALL BLOCKS CALENDAR HOUSING PUBLICATIONS RECIPES RESTAURANTS

SOCIAL Avg.

Wang et al. (2015b)
Jia and Liang (2016b)
Herzig and Berant (2017)
Su and Yan (2017)
Ours
Herzig and Berant (2017)∗
Su and Yan (2017)∗

46.3
85.2
85.2
86.2
87.7
86.2
88.2

41.9
58.1
61.2
60.2
62.9
62.7
62.7

74.4
78.0
77.4
79.8
82.1
82.1
82.7

54.5
71.4
67.7
71.4
71.4
78.3
78.8

59.0
76.4
74.5
78.9
78.9
80.7
80.7

70.8
79.6
79.2
84.7
82.4
82.9
86.1

75.9
76.2
79.5
81.6
82.8
82.2
83.7

48.2
81.4
80.2
82.9
80.8
81.7
83.1

58.8
75.8
75.6
78.2
78.6
79.6
80.8

Table 4: Test accuracy of supervised models on all domains for OVERNIGHT. Models with ∗ are augmented with
cross-domain training.

OVERNIGHT

Model

BASKETBALL BLOCKS CALENDAR HOUSING PUBLICATIONS RECIPES RESTAURANTS

SOCIAL Avg.

Lower Bound
Self-Traing
Top-K MML
Repulsion MML
Gentle MML
Sparse MML
Upper Bound

68.0
66.5
70.3
70.8
68.3
73.4
87.7

35.3
37.8
36.8
37.3
38.1
42.1
62.9

40.5
48.2
42.3
44.6
45.8
46.4
82.1

34.4
39.2
30.7
37.8
39.2
38.1
71.4

39.1
41.0
37.3
39.8
43.5
42.2
78.9

45.8
41.2
44.4
44.4
44.9
43.1
82.4

59.3
63.0
61.1
60.2
59.9
63.0
82.8

63.7
63.8
62.2
67.8
64.0
64.9
80.8

48.3
50.1
48.1
50.3
50.5
51.3
78.6

GEO

42.7
44.1
40.1
44.8
46.6
45.2
74.2

Table 5: Execution accuracy of supervised and semi-supervised models on all domains of OVERNIGHT and GEO-
QUERY. In semi-supervised learning, 10% of the original training examples are treated as labeled and the re-
maining 90% as unlabeled. Lower bound refers to supervised models that only use labeled examples and discard
unlabeled ones whereas upper bound refers to supervised models that have access to gold programs of unlabeled
examples. Avg. refers to the average accuracy of the eight OVERNIGHT domains.

harder to improve. By taking account of unseen
programs, Repulsion MML and Gentle MML tend
to be more useful in domains such as HOUSING,
PUBLICATIONS, RECIPES where the base parsers
are very weak (accuracy is around 40%). Moreover,
Gentle MML performs best in GEOQUERY.

