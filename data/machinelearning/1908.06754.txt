9
1
0
2

v
o
N
5
1

]

G
L
.
s
c
[

4
v
4
5
7
6
0
.
8
0
9
1
:
v
i
X
r
a

A NEW DETERMINISTIC TECHNIQUE FOR SYMBOLIC
REGRESSION

A PREPRINT

Daniel Rivero
Centro de investigación CITIC
Department of Computer Science and Information Technology
University of A Coruña
A Coruña, Spain
daniel.rivero@udc.es

Enrique Fernandez-Blanco
Centro de investigación CITIC
Department of Computer Science and Information Technology
University of A Coruña
A Coruña, Spain
enrique.fernandez@udc.es

November 18, 2019

ABSTRACT

Based on a solid mathematical background, this paper proposes a new method for Symbolic Regres-
sion which allows ﬁnding mathematical expressions from a dataset. Oppositely to other methods,
such as Genetic Programming, this is deterministic approach which does not require the creation of a
population of initial solutions. Instead of it, a simple expression is being grown until it ﬁts the data.
The experiments performed show that the results are as good as other Machine Learning methods, in
a very low computational time. Another advantage of this technique is that the complexity of the
expressions can be limited, so the system can return mathematical expressions that can be easily
analyzed by the user, in opposition to other techniques like GSGP.

Keywords Symbolic Regression · Machine Learning · Artiﬁcial Intelligence

1

Introduction

In Machine Learning, supervised learning allows to ﬁnd models which represent the relationship between a series
of inputs and outputs. Nowadays, there are many different techniques for ﬁnding these models such as Artiﬁcial
Neural Networks [1]. However, many of these methods, even they show good results in the modelling, do not give
hints about the true relationship between inputs and outputs. In many environments, a black-box model is not enough,
since the objective is to ﬁnd an equation that the expert can analyse and thus increase the knowledge about the system
being modelled. In this sense, the search of transparent ML models, showing a clear relationship between inputs and
outputs is today a research hot topic, and nowadays there are conferences such as FAT (ACM Conference on Fairness,
Accountability and Transparency) devoted to this ﬁeld.

This work proposes a new method for symbolic regression: from a dataset (inputs/outputs), this method allows to ﬁnd
mathematical expressions that can reproduce this relationship. These expressions will be of beneﬁt to many experts to
understand the behaviour of a system.

Nowadays, there are few different methods for symbolic regression. The most used ones are based on Genetic
Programming (GP) [2] [3] which is a general-purpose evolutionary technique. The ﬁrst and most common representation

 
 
 
 
 
 
A PREPRINT - NOVEMBER 18, 2019

in GP is tree-shaped. However, recently other different representations have arisen, such as Linear GP [4], Stack-based
Genetic Programming [5], Cartesian Genetic Programming (CGP) [6], or Positional Cartesian Genetic Programming
(PCGP) [7]. However, trees are still the most used codiﬁcation for mathematical expressions for symbolic regression
tasks.

GP works from an initial population of trees, that undergo an evolutionary process with the execution of selection,
crossover, mutation and replacement operators. These operators are based on randomness, and this makes the whole
process a time-consuming task. For instance, the original crossover operator proposes the random combination of
mathematical expressions, although there are different approaches that try to combine useful parts [8][9]. On its
side, mutation operator also makes random changes in a mathematical expression. All of this makes that although
the evolutionary process is a search driven by the ﬁtness function, it needs the computation of many mathematical
expressions that are not going to be part of the ﬁnal expression. Also, although the algorithms behind GP are well-known,
its global behaviour is still to be studied, and the obtained results lack of a mathematical basis. However, as a system
capable of performing symbolic regression, it has been successfully applied to real-world problems such as integrated
circuit design [10] or civil engineering [11].

Recently, a new type of GP, called Geometric Semantic Genetic Programming (GSGP) has arisen [12]. This approach
works in the so-called geometric space, which is conformed by the outputs of the GSGP programs (semantics). In
this space, the targets are another point in the semantic space. Its performance, measured in results and time, is much
higher than GP. However, a big drawback of this technique is that its result is a tree with an excessive number of nodes,
usually higher than 1015 [13] Although there are works that try to reduce this number [14], the resulting program are
still very complex, many times as a sum of different mathematical expressions. On another hand, it was demonstrated
that the resulting expression that the ﬁnal expression it is a sum on the expressions generated on the ﬁrst generation
[13]. This fact, having as result oversized and very complex expressions, has become a big drawback of this technique
since the evolved expressions are not understandable by a human being. However, it has been successfully applied in
many real-world environments such as ﬁnancial [15] or biomedical [16].

Symbolic regression is a research ﬁeld that was hardly been explored outside of GP literature, with very few works
describing non-evolutionary approaches [17][13]. However, although their computation time is very low and they show
good training results, they are based on the combination of different functions that work as a basis on a L2 space. Thus,
the resulting expression is the weighted sum of a series of expressions. As was already said, this was also demonstrated
to happen in GSGP [13]. This leads again to having a system that returns expressions that are not understandable by a
human.

Therefore, there is still the need of one technique that can generate simple mathematical expressions, easily understand-
able by the human, with a mathematical basis, that works in short time. This work presents a technique with all of these
features. An additional advantage of this technique is that it is deterministic.

This work is not based on GP or GSGP. The only similarities between this work and GP is that the expression has
shape of a tree as in traditional GP, and that we work in the semantic space as in GSGP. As opposed to GP or GSGP,
the system proposed in this paper does not need to generate (and evaluate) a population of expressions. Instead of
it, a single expression is being sequentially improved, and each change performed to it is guaranteed to improve its
performance.

2 Model

Usually when working in Machine Learning, the user has a dataset arranged as a matrix with dimensions NxL or LxN,
being L the number of variables or features, and N the number of patters. In supervised learning, a target matrix is also
needed, with dimension of NxT or TxN, accordingly with the previous, being T the number of outputs. In the case of
symbolic regression, since a single equation is desired, T=1. The most common way of working with this is creating
a L-dimensional space, in which each dimension corresponds to each variable. Thus, each pattern is a point in that
L-dimensional space.

However, in this paper we work with a N-dimensional space, with one dimension for each pattern. Therefore, each
variable corresponds to a single point in this space, and the targets are also a point in this space. Moreover, the output of
a model (not limited to being a equation) gives one value for each pattern, thus making a vector in this space. Therefore,
each model is represented also as a point in this space. In GSGP, this space is called "semantic space", because the
output of each model is called the semantic.

Similarly to GSGP, a model has a semantic, i.e., a vector in the semantic space composed by the outputs oi for each
pattern. Since the targets ti are also a point in the semantic space, the Euclidean distance between these two points can
be measured with equation 1.

2

A PREPRINT - NOVEMBER 18, 2019

Figure 1: Example of a tree

distance =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

N
(cid:88)

i=1

(oi − ti)2

(1)

This equation corresponds to the square root of the SSE (Sum Squared Error) of the model. Thus, ﬁnding a better model
(i.e., a model with a lower SSE) is equivalent to ﬁnding a model closer to the target point. Therefore, a N-dimensional
sphere is being created, and any model inside this sphere will be a better model (i.e., with a lower SSE). This sphere has
as radius the square root of the SSE (Root Sum Squared Error, RSSE), with the following equation:

N
(cid:88)

i=1

(oi − ti)2 < SSE

(2)

Also, in order to minimize the impact of having a large or low number of patterns, this number N is usually inserted
into the equation, having as result the Mean Square Error equation to minimize, shown in equation 3

1
N

N
(cid:88)

i=1

(oi − ti)2 < M SE

(3)

Again, a new model with outputs oi that complies with the restriction of equation 3 is a model with a lower MSE, and
closer to the target. If the model being processed can undergo different improvements, the best improvement will be the
one that makes the model closer to the target point. This is the improvement that makes the new outputs maximize the
reduction in MSE, given by equation 4

reduction = M SE −

1
N

N
(cid:88)

i=1

(oi − ti)2

(4)

This turns the problem of improving the model into an optimization problem: the more positive the result of equation 4
is, the better the performance of the new model will be. If different improvements are possible, then the one selected
will be the one that maximizes equation 4. If none of these possible improvements lead to having positive values in
equation 4, then the improvement process has ﬁnished.

As was previously stated, the equation being developed has the shape of a tree. As in GP, we will distinguish two types
of nodes: terminal, or leaves of the tree, and non-terminal, or functions. As terminal nodes we use only those variables
of the problem, and constants. As functions we use the four arithmetic operators: +,-,*,/. Note that we do not use "%" as
protected division. In GP it was necessary to protect that operation because, as a result of many different combinations,
chances are that some divisions by zero will be performed. In this case, the tree being developed will always be correct,
with no divisions by 0.

3

A PREPRINT - NOVEMBER 18, 2019

Table 1: Description of the tree of the example

Node

Semantic

1
2
3
4
5
6
7
8
9
10
11

(-1, 0.286, 0.8)
(2, 2, 2)
(-2, 7, 2.5)
(-3, 6, 1.5)
(3, 3, 3)
(-1, 0.5, 2)
(1,1,1)
(2, 0.5, -1)
(2, 2, 2)
(1, 4, -2)
(1, 1, 1)

2

1

1

1

1

1

Equation
3 ((oi − 5)2 + (oi − 4)2 + (oi − 1)2)
3 (( oi+10
−7 )2 + ( oi−2.5
)2 + ( oi−28
−2.5 )2)
3 (( −5·oi+2
)2 + ( −oi+2
)2 + ( −4·oi+2
)2)
oi
oi
oi
oi+1 )2 + ( −oi+1
oi+1 )2 + ( −4·oi−2
3 (( −5·oi−3
oi+1 )2)
oi+0.5 )2 + ( −oi+2
3 (( −5·oi+3
oi−1 )2 + ( −4·oi−1
oi+2 )2)
)2 + ( oi−3
)2 + ( −2·oi−12
3 (( −3·oi−15
oi+3 )2)
oi+3
oi+3
oi+2.5 )2 + ( oi−2
oi+1 )2 + ( −2·oi−11
3 (( −3·oi−9
oi+4 )2)
−oi+4 )2 + ( −oi−2
−oi+4 )2 + ( 2·oi−14
3 (( 3·oi−18
−oi+4 )2)
−oi+16 )2 + ( −oi+4
3 (( 3·oi−18
−oi+4 )2 + ( 2·oi−56
−oi−8 )2)
4·oi−2 )2 + ( −2·oi−2
4·oi−2 )2 + ( −14·oi+4
3 (( −18·oi+6
4·oi−2 )2)
)2 + ( −oi+0.5
)2 + ( −4·oi−22
3 (( −5·oi+17
oi+1.5 )2)
oi+6
oi−3

1

1

1

1

1

1

S

∅
∅
{(0, 0, 0)}
{(−1, −1, −1)}
{(1, −0.5, −2)}
{(0, 0, 0), (−3, −3, −3)}
{(2, 0.5, −1), (−1, −2.5, −4)}
{(1, 1, 1), (4, 4, 4)}
{(1, 4, −2), (4, 16, −8)}
{(0, 0, 0), (2, 2, 2), (0.5, 0.5, 0.5)}
{(3, −6, −1.5)}

Figure 1 shows an example of tree. This tree has 11 nodes, 4 of them are non-terminal (labeled as 1, 3, 4 and 8), and 5
are terminal (labeled as 2, 4, 5, 7, 9 and 11). This tree represents the following expression:

2

3
1− 2
x3

+ 1

(5)

The tree being developed will have a semantic determined by the outputs to each pattern. As has already been said,
this tree can be a terminal or non-terminal. In the second case, the root of the tree will be any of the four arithmetic
operators, and each of its children will be another tree, with their corresponding semantic and their corresponding
points in the semantic space. Therefore, a tree with n nodes is represented in the semantic space as the semantic point
of the root of the tree, but also as n-1 different points. If any of these nodes is modiﬁed, then its semantic point will be
moved. This has the consequence that the overall evaluation values of the root of the tree are changed and the semantic
point of the root of the tree will be moved too. Note that constants and variables (terminal nodes of the tree) also have a
semantic value, representing one point in the semantic space.

Table 1 shows, in the second column, the semantics of each node of the tree shown in ﬁg. 1. All of the nodes, including
constant and variables have their semantic, i.e., they are points in the search space. The variable x3 takes the values of 1
for the ﬁrst pattern, 4 for the second and -2 for the third. Note that each terminal node representing a constant k has a
semantic (k, k, ..., k), since it evaluates to k for each data point.

Fig 2 shows an example of an expression with a semantic represented by the point p1, and the targets situated at the
point t. The circle around the targets represent the different expressions that have the same RSSE as p1. Thus, any
expression inside this circle has a lower RSSE. In this case, the expression with a semantic represented by the point p2
is inside the circle, so the expression p1 can be replaced by p2 with a reduction of the RSSE. Note that a lower RSSE
leads to having a lower SSE, which also leads to having a lower MSE. This reduction can be calculated as described in
equation 4. This ﬁgure also shows another expression, p3, that is outside the circle. This means that replacing p1 for p3
leads to having a higher RSSE, SSE and MSE. In this case, the calculation of the reduction returns a negative value. As
has been already explained, only positive values in reduction lead to an improvement.

The key idea of this work is that we may not have a tree that makes a positive reduction in equation 4, moving its
semantic to get closer to the target point. However, it might be easier to modify one node in any branch in order to
move the root towards the target. This modiﬁcation is done by changing that subtree into another. The question is how
to ﬁnd the new subtree that will substitute it. In order to ﬁnd it, it is necessary to calculate the MSE from the outputs of
this node. In this sense, each node of the tree has associated an equation to calculate the MSE from its outputs. As
happens with the root, this equation can be used to quantify the improvement of the overall result if the outputs oi of
this node are modiﬁed. This equation represents a shape in the semantic space, and, for any node of the tree, has the
following shape:

M SE =

1
N

N
(cid:88)

i=1

(

ai · oi − bi
ci · oi − di

)2

4

(6)

A PREPRINT - NOVEMBER 18, 2019

Figure 2: Example of N-dimensional sphere generation

where oi are the outputs of that node, and ai, bi, ci and di are four vectors (i=1,...,N) characterizing the equation for this
node. These vectors will be different for each node of the tree, thus having a different equation on each node. However,
given a tree, the result of each equation of each node applied to its outputs will be the same MSE value. Therefore, a
ﬁrst step of this algorithm is to calculate these vectors for each node.

This process is recursively done from each non-terminal node to its child nodes: each non-terminal node, from the
outputs of their child nodes (whether they are terminal or non-terminal) and its ai, bi, ci and di vectors, calculates the
vectors for each of its children, and then this process is repeated for each non-terminal children. For the root of the tree,
ai = 1, bi = ti, ci = 0 and di = −1, leading to equation 3. For the rest of the non-terminal nodes, the calculation of
the vectors for each children is done in the following way:

• Sum operation. In this case, the output of the node is written as oi = xi + yi, being xi and yi the outputs

(semantics) of its two children. The equation for the ﬁrst child becomes the following:

1
N

N
(cid:88)

i=1

(

ai · (xi + yi) − bi
ci · (xi + yi) − di

)2 =

N
(cid:88)

(

1
N

ai · xi − (bi − ai · yi)
ci · xi − (di − ci · yi)

)2

(7)

i=1
i = bi − ai · yi, c(cid:48)

i = ci and d(cid:48)

i = di − ci · yi. For the second

which has the shape of equation 6 with a(cid:48)
child, the equation is very similar:

i = ai, b(cid:48)

1
N

N
(cid:88)

i=1

(

ai · (xi + yi) − bi
ci · (xi + yi) − di

)2 =

N
(cid:88)

(

1
N

ai · yi − (bi − ai · xi)
ci · yi − (di − ci · xi)

)2

(8)

i = ai, b(cid:48)
which has the shape of equation 6 with a(cid:48)
If the operator sum is the root of the tree, then a(cid:48)
case, the resulting equations for the two children are the following

i=1
i = bi − ai · xi, c(cid:48)
i = ai = 1, c(cid:48)

i = ci and d(cid:48)

i = di − ci · xi.

i = ci = 0, d(cid:48)

i = di = −1, and bi = ti. In this

1
N

1
N

N
(cid:88)

i=1

N
(cid:88)

i=1

(xi − (ti − yi))2

(yi − (ti − xi))2

(9)

(10)

and can be interpreted for each child as "move the target value substracting from it the value of the output of
the other child" and apply the original equation. Thus, as was done in the root node, two spheres in the space
are created, with centers in t-y (for the ﬁrst child) and t-x (for the second child), i.e., the target values for the
root of the tree has been moved to a different position for each of the children. If any tree is found which its
outputs, applied to equations 9 or 10 has a lower value, then the MSE will be reduced. From another point of
view, if a subtree is found inside one of these spheres, the corresponding ﬁrst or second child of the root can be
replaced by this new subtree. As a consequence, the semantic of the tree will move towards the target, having
an improvement in the overall result. In general, from a shape given by equation 6 the sum operation creates
two new identical shapes in other points.
Figure 3 a) shows an example of a tree situated in p. This tree is (p1 + p2). The calculation of the equations for
each child leads to having a similar shape, but translated according to the values of p1 and p2. The resulting
shapes (in this case, spheres) still have the semantics in the border.

5

p1p2tRSSE1RSSE2p3RSSE3A PREPRINT - NOVEMBER 18, 2019

Figure 3: Examples of calculating new shapes for the sum operation (a), and multiplication operation (b)

• Substraction operation. This case is very similar to the previous one: the output of the node is written as
oi = xi − yi, being xi and yi the outputs (semantics) of its two children. The equation for the ﬁrst child
becomes the following:

1
N

N
(cid:88)

i=1

(

ai · (xi − yi) − bi
ci · (xi − yi) − di

)2 =

N
(cid:88)

(

1
N

ai · xi − (bi + ai · yi)
ci · xi − (di + ci · yi)

)2

(11)

i=1
i = bi + ai · yi, c(cid:48)

i = ci and d(cid:48)

i = di + ci · yi. For the second

which has the shape of equation 6 with a(cid:48)
child, the equation is:

i = ai, b(cid:48)

1
N

N
(cid:88)

i=1

(

ai · (xi − yi) − bi
ci · (xi − yi) − di

)2 =

1
N

N
(cid:88)

i=1

(

−ai · yi − (bi − ai · xi)
−ci · yi − (di − ci · xi)

)2 =

1
N

N
(cid:88)

i=1

(

ai · yi − (ai · xi − bi)
ci · yi − (ci · xi − di)

)2

(12)

which has the shape of equation 6 with a(cid:48)
Again, if this operator is used as root of the tree, the sphere of the root will be moved to t+y for the ﬁrst
child and x-t for the second child, but the idea is the same: if a tree is found inside any of these spheres, the
corresponding child can be replaced with this tree and the overall result will be improved. In general, the sum
and substraction operations creates two new identical shapes in other points.

i = ai · xi − bi, c(cid:48)

i = ci · xi − di.

i = ci and d(cid:48)

i = ai, b(cid:48)

• Multiplication operation. In this case, the output of the node is written as oi = xi · yi, being xi and yi the

outputs (semantics) of its two children. The equation for the ﬁrst child becomes the following:

1
N

N
(cid:88)

i=1

(

ai · (xi · yi) − bi
ci · (xi · yi) − di

)2 =

N
(cid:88)

(

1
N

(ai · yi) · xi − bi
(ci · yi) · xi − di

i=1
i = ci · yi and d(cid:48)
i = bi, c(cid:48)

)2

(13)

i = di. For the second child, the

which has the shape of equation 6 with a(cid:48)
equation is very similar:

i = ai · yi, b(cid:48)

N
(cid:88)

(

1
N

ai · (xi · yi) − bi
ci · (xi · yi) − di

)2 =

N
(cid:88)

(

1
N

(ai · xi) · yi − bi
(ci · xi) · yi − di

)2

(14)

i=1
If the multiplication operator is the root of the tree, then a(cid:48)
i = di = −1, and bi = ti.
In this case, taking into account that these equations allow the calculation of the MSE, consequently, the
resulting equations for the two children are the following, :

i = ai = 1, c(cid:48)

i = ci = 0, d(cid:48)

i=1

These equations can be rewritten as

1
N

1
N

N
(cid:88)

i=1

N
(cid:88)

i=1

(yi · xi − ti)2 = M SE

(xi · yi − ti)2 = M SE

6

(15)

(16)

p1p2tt-p1t-p2pp1tpp2t/p2t/p11122a)b)A PREPRINT - NOVEMBER 18, 2019

N
(cid:88)

i=1

N
(cid:88)

i=1

)2

(xi − ti
yi
( SSE
)2
yi

)2

(yi − ti
xi
( SSE
)2
xi

= 1

= 1

(17)

(18)

and therefore are the equations of ellipsis. These equations can be interpreted for each child as "move the
target value through the line determined by the target vector, and shrink or extend the radius of the sphere
in each dimension according to the other child outputs" and apply the original equation. Thus, the sphere of
the root of the tree becomes ellipses in each of the two children. However, the reasoning is the same: if a
new subtree is found inside these new shapes (ellipsis), then the result of the application of equation 15 or 16
will be equal to a lower MSE, and its semantic will move towards the target. In general, the multiplication
operation creates new shapes in other points, which are distortions of this one.
Figure 3 a) shows an example of a tree situated in p. This tree is (p1 · p2), with p1 = (1, 2) and p2 = (2, 1).
The calculation of the equations for each child leads to having the sphere translated and shrink according to the
values of p1 and p2. The resulting shapes are ellipsis, and, as in the rest of the cases, still have the semantics in
the border.
The use of the operations of sum, substraction and multiplication allows the building of complex trees, and
having a semantic space with different N-dimensional ellipsoids. If a model is found inside one of these
ellipsoids, the corresponding node can be changed with that model. When this is done, the semantic of the root
of the tree get closer to the target, the MSE gets lower, the radius of the sphere of the root of the tree shrinks
and the rest of the spheres/ellipses move from their places and shrinks in the same amount.

• Division operator. In this case, the output of the node is written as oi = xi/yi, being xi and yi the outputs

(semantics) of its two children. The equation for the ﬁrst child becomes the following:

1
N

N
(cid:88)

i=1

(

ai · (xi/yi) − bi
ci · (xi/yi) − di

)2 =

N
(cid:88)

(

1
N

ai·xi−bi·yi
yi
ci·xi−di·yi
yi

i=1
i = bi · yi, c(cid:48)
i = ai, b(cid:48)

)2 =

1
N

N
(cid:88)

i=1

(

ai · xi − bi · yi
ci · xi − di · yi

)2

(19)

i = ci and d(cid:48)

i = di · yi. For the second child, the

which has the shape of equation 6 with a(cid:48)
equation is:

1
N

N
(cid:88)

i=1

(

ai · (xi/yi) − bi
ci · (xi/yi) − di

)2 =

1
N

N
(cid:88)

i=1

(

ai·xi−bi·yi
yi
ci·xi−di·yi
yi

)2 =

1
N

N
(cid:88)

i=1

(

−bi · yi − (−ai · xi)
−di · yi − (−ci · xi)

)2 =

1
N

N
(cid:88)

i=1

(

bi · yi − (ai · xi)
di · yi − (ci · xi)

)2

(20)

which has the shape of equation 6 with a(cid:48)
If this operator is the root of the tree, then a(cid:48)
the resulting equations for the two children are the following:

i = ai = 1, c(cid:48)

i = ai · xi, c(cid:48)

i = bi, b(cid:48)

i = di and d(cid:48)

i = ci · xi.

i = ci = 0, d(cid:48)

i = di = −1, and bi = ti. In this case,

1
N

N
(cid:88)

i=1

(

xi − ti · yi
yi

)2 =

1
N

N
(cid:88)

i=1

(

1
yi

· xi − ti)2

1
N

N
(cid:88)

i=1

(

−ti · yi + xi
yi

)2 =

1
N

N
(cid:88)

i=1

(

xi
yi

− ti)2

(21)

(22)

In the ﬁrst case, the effect is similar to the multiplication operation: the target value for the ﬁrst child is moved
along the line given by the target vector, and the radius is extended/shrink according to the values of the
second child. In the second case, the sphere is turned into a completely different shape. Therefore, the division
operation can transform ellipsoids into different shapes.

The third column of table 1 shows the equations calculated for each of the nodes of the tree shown in ﬁg. 1. The target
vector used was (5,4,1).

The division operator is different from the rest for a very important reason: in the other three, the domain is all IR.
However, in the division operator the domain is restricted. This makes it have to be used with care. For example,

7

A PREPRINT - NOVEMBER 18, 2019

ﬁgure 1 shows an example of a tree with the division operator. The expression representing this tree is shown on eq. 5.
However, it can be simpliﬁed to the following:

x3 − 2
2 · x3 − 1

(23)

However, both expressions are different, since the second is deﬁned in any value of x3 that belongs to IR − {0.5}, while
in the ﬁrst one the values of x3 that are not in the domain are 0, 2 and 0.5. Note that this last value is the same value in
the ﬁrst expression.

One could argue that the values that are not in the domain in the ﬁrst equation are "cancelled" in successive divisions,
i.e., giving rise to inﬁnite values in the ﬁrst division, and to 0 in the next. However, more complex cases may arise, as in
the following equation:

1
− 4·x4
2·x3

3·x1
x3

(24)

When calculating the equations in each node of the tree, in the previous example shown in ﬁg. 5 it can be seen that the
denominator of the equation corresponding to the variable x3 has a root in 0.5, that is, the domain of that equation is the
same as the domain of the equation 23 (simpliﬁed). In general, as you go down the equations in the tree, the domain of
these equations will be equal to the domain of the ﬁrst division operation performed in the tree. This effect is produced
by the way the equation for the second child of the division operator is constructed, shown in eq. 20. In that equation,
this step

1
N

N
(cid:88)

i=1

(

ai·xi−bi·yi
yi
ci·xi−di·yi
yi

)2 =

1
N

N
(cid:88)

i=1

(

ai · xi − bi · yi
ci · xi − di · yi

)2

(25)

is correct for values of yi (cid:54)= 0. Therefore, for that node, any semantic in the second child with any yi = 0 is outside the
domain of this node. For example, a node with a semantic of (1, −2, 0) can not be used as second child. Moreover, as
the second child can be a tree, its semantic depends on its nodes and operations, since the result of these operations can
lead to having a semantic with any yi = 0. For example, in the tree shown in ﬁg. 1, the node 4 represents a division and
thus its second child can not take the value of 0. If the variable x3 takes the value of 2, then this denominator of the
division operator of equation 4 will take a value of 0. Therefore, it is necessary to keep track of which values are not in
the domain of a function, and, when applying new operators, operate with them consistently to calculate the new values
that are not in the domain in the division operations.

Therefore, when calculating the equations it is necessary to take into account the domains of the operators already
explored from the top of the tree. Since the +, - and * operators are deﬁned in all IR, only the division operator has to be
taken into account. The way to do this is to extend the deﬁnition of the equations of each node. Until now, these were
deﬁned with four vectors, ai, bi, ci and di. In addition to these vectors, it is necessary to add a set of semantics S that
provoke any operation out of domain, in this case divisions by zero, in some upper node of the tree. As said, initially,
that is, for the root node of the tree, ai = 1, bi = ti, ci = 0 and di = 1, and also S = ∅.

As for the vectors ai, bi, ci and di, the values of the S set must be calculated for each node. The process is similar to
the one described: each non-terminal node makes the calculation of ai, bi, ci, di and S for each children. Therefore, at
the same time a(cid:48)
i are calculated for each child node, a new S(cid:48) set is calculated for this node. Given a node
with an operation and a S set, for each child the calculation method is the following:

i and d(cid:48)

i, c(cid:48)

i, b(cid:48)

1. S(cid:48) = ∅
2. For each semantic s ∈ S, calculate s(cid:48) depending on the node operation. The way to calculate for one child is
to perform the inverse of the node operation with respect to the other child. If x and y the two semantics of the
ﬁrst and second children respectively, the process is, for each operation and each child:

• Sum operation:

– First child: s = s(cid:48) + y ⇒ s(cid:48) = s − y.
– Second child: s = x + s(cid:48) ⇒ s(cid:48) = s − x.

• Substraction operation:

– First child: s = s(cid:48) − y ⇒ s(cid:48) = s + y.
– Second child: s = x − s(cid:48) ⇒ s(cid:48) = x − s.

8

A PREPRINT - NOVEMBER 18, 2019

Figure 4: Example of a tree with invalid operations

Table 2: Description of the tree of the example with invalid operations

Node

Semantic

S

1
2
3
4
5
6
7
8
9
10
11
12
13

(1.33, Inf, Inf )
(2, 2, 2)
(1.5, 0, 0)
(1.5, 1, 0)
(3, 2, 0)
(2, 2, 2)
(0, −1, 0)
(0, 1, 0)
(1, −1, 5)
(2, 0, 6)
(1, 0, 3)
(2, 2, 2)
(1, 1, 1)

∅
∅
{(0, 0, 0)}
{(0, 1, 0)}
{(0, 2, 0)}
{(0, 0, 0), (Inf, 2, N aN )}
{(−1.5, −1, 0)}
{(−1.5, 1, 0)}
{(Inf, −1, N aN )}
{(Inf, 0, N aN )}
{(Inf, 0, N aN )}
{(Inf, Inf, N aN )}
{(Inf, 1, N aN )}

• Multiplication operation:

– First child: s = s(cid:48) · y ⇒ s(cid:48) = s/y.
– Second child: s = x · s(cid:48) ⇒ s(cid:48) = s/x.

• Division operation:

– First child: s = s(cid:48)/y ⇒ s(cid:48) = s · y.
– Second child: s = x/s(cid:48) ⇒ s(cid:48) = x/s.

Since s, s(cid:48), x and y are vectors, the operations described are element-wise operations.
Once s(cid:48) is calculated, add it to S(cid:48).

3. In the case of the second child of the division operation, add the semantic s(cid:48)

i = 0 to S(cid:48), since this semantic is

not in the domain of this operation.

For each node, the set S contains the semantics that make any operation of any previous node with a value out of
domain, i.e., each s semantic in S has "forbidden" values. An important remark is that, if s ∈ S has a semantic of
"forbidden" values si, an invalid semantic x will be a semantic in which any value xi = si, i.e., they don’t have to be
equal all of the values.

The fourth column of table 1 shows the S sets for each node of the tree shown in ﬁg. 1. Nodes 1 and 2, before any
division operation was performed, have S = ∅. Nodes 3, 4, 5 and 11, when only one division has been performed, have
one element in S. Nodes 6, 7, 8 and 9, when 2 divisions have been performed, have 2 elements in S. Finally, when the
calculation of the equations of node 10 is reached, 3 different divisions have been done, and therefore the number of
elements in S is 3. Note that each node used as divisor (nodes 3, 6 and 10) have the element (0,0,0) in S, meaning that
no value of their semantics can be 0.

9

A PREPRINT - NOVEMBER 18, 2019

Another example is shown in Fig. 4, which has its semantics and S sets described on table 2. Note that this tree is
invalid, since divisions by zero are performed. thus, this tree will never be generated by this system. However, it is used
here as an example for a better explanation of the following descriptions.
Special care has to be taken when calculating each semantic s(cid:48) from s for the multiplication and division operators. In
this case, divisions by 0 may occur in the following situations:

• In the case of the multiplication operation, for the ﬁrst child when any element of yi (semantic of the second
child) is 0 or, for the second child, when any element of xi (semantic of the ﬁrst child) is 0, i.e., a multiplication
of 0 · yi or xi · 0 is being made. In both cases, the result of the multiplication is equal to 0, and the calculation
of the new values out of domain are s(cid:48) = s/y (for the ﬁrst child) and s(cid:48) = s/x (for the second). Two situations
are possible:

– si (cid:54)= 0: the "forbidden value" is not 0. In this case, si/xi or si/yi equals to inﬁnite (Inf ). This means
that any value is valid for xi or yi because xi · 0 = 0 (cid:54)= si or 0 · yi = 0 (cid:54)= si and therefore they are never
out of the domain. An example of this can be seen in Fig. 4 and table 2, in the S set of node 9 (ﬁrst value
of the only element).

– si = 0: the "forbidden value" is 0. In this case, si/xi or si/yi equals to "not a number" (N aN ). This
means that there are not valid values for xi and yi, since for any xi or yi values, xi · 0 = 0 = si or
0 · yi = 0 = si, and therefore they are always out of the domain. An example of this can be seen in Fig.
4 and table 2, in the S set of node 9 (third value of the only element).

• In case of the division operation, for the ﬁrst child, the reasoning is the same as in the multiplication operation.
• For the second child of the division operation, s = x/s(cid:48) the operation to calculate the new "forbidden"
semantic is s(cid:48) = x/s. Division by zero will take place when any si = 0: the "forbidden value" of the result of
the division operation is 0. Two situations may occur:

– xi (cid:54)= 0. In this case, the result of this operation is s(cid:48)

i = xi/si = Inf , meaning that any yi value is valid,

since xi/yi (cid:54)= si = 0 if xi (cid:54)= 0.
This can be argued, since a value of yi = 0 is not valid for this division. However, the calculation of each
s(cid:48) corresponds to values out of the domain of previous division operators, not for this one. For the current
division operation, as was already explained, the semantic s(cid:48) = 0 is added to S, and therefore the value
of yi = 0 is not valid.
An example of this can be seen in Fig. 4 and table 2, in the S set of node 6 (ﬁrst value of the second
element).

– xi = 0. In this case, the result of the division operation is xi/yi = 0 = si. The calculation of s(cid:48)

i is the
i = xi/si = 0/0 = N aN , meaning that there is not valid values for yi, i.e., for any value yi,

following: s(cid:48)
xi/yi = 0 = si.
Again, these calculations are valid for yi (cid:54)= 0. The case yi = 0 is excluded with the addition of the
semantic s(cid:48)
An example of this can be seen in Fig. 4 and table 2, in the S set of node 6 (third value of the second
element).

i = 0 to S;

Therefore, for each s ∈ S, a value labeled as Inf represents that any value is valid, while a value labeled as N aN
represents that any value is invalid. The operations between these values are done in the usual way, if x ∈ IR:

• Any operation between x and N aN has N aN as result. This means that if any value is out of domain in a
speciﬁc node, then any value will also be out of domain in each children, no matter what operation is done in
this node. An example of this can be seen in Fig. 4 and table 2, in the S sets of nodes 10 and 13 (third value of
the only element). Also, node 12 shows another example (third value of the only element).

• s(cid:48)

• Sum and substraction operations between x and Inf return Inf . This means that if any value is valid (the
domain is IR) for the result of the operation, then any value is also valid for each children. An example of this
can be seen in Fig. 4 and table 2, in the S sets of nodes 10 and 13 (ﬁrst value of the only element).
i = Inf /xi = Inf (xi = 0 or xi (cid:54)= 0). This can happen in a when s(cid:48) is being calculated as one of the
children in a multiplication operation. This means that if any value is valid as result of a multiplication
operation (Inf ), then each child can have any value. An example of this can be seen in Fig. 4 and table 2, in
the S set of node 12 (ﬁrst and second values of the only element).
i = xi/Inf = 0 (xi = 0 or xi (cid:54)= 0). This can happen when s(cid:48) is being calculated as the second child in a
division operation (i.e., the S set of the denominator) with the semantic of the ﬁrst child xi. This means that
even any value (denoted as Inf ) is valid as result of a division operation in which the numerator is a valid
number, the denominator must different from 0 (0 is not in the domain).

• s(cid:48)

10

A PREPRINT - NOVEMBER 18, 2019

• s(cid:48)

• s(cid:48)

i = Inf · yi = Inf when xi (cid:54)= 0. This can happen when s(cid:48) is being calculated as the ﬁrst child in a division
operation (i.e., the S set of the numerator) with the semantic of the denominator yi (cid:54)= 0. This means that if the
result of a division operation can be any value (Inf ) and the denominator is different to 0, then the ﬁrst child
can take any value in IR.
i = Inf · yi = N aN when yi = 0. This can happen when s(cid:48) is being calculated as the ﬁrst child in a division
operation (i.e., the S set of the numerator) with the semantic of the denominator xi (cid:54)= 0. This means that even
if the result of a division operation can be any value, if the second child evaluates to 0, then there is no valid
value for the ﬁrst child.

• Operations between N aN and/or Inf values will not happen because these operations take place between
the semantics of the S set, which may contain N aN and Inf values, and semantics of the nodes of the tree,
which may not. The deﬁnition of the S set is done to prevent the semantics of the nodes from having N aN
and Inf values.

Any semantic x to be applied the equation 6 of any node in order to compute the MSE from that node must be ﬁrst
checked with its S set. For each semantic s in S, if there is any value in which si = xi or si = N aN , then that
semantic can not be used because it would lead to having out-of-domain values in preceeding nodes. In the case in any
which si = N aN , since there are no valid values, for simplicity reasons, the whole S set is labeled as N aN .

With this method, once the tree has been evaluated, the equation can be calculated for each node. Therefore, before a
tree is going to be improved, each node has to be evaluated, and this information has to be stored on its corresponding
node. This evaluation process goes from the bottom of the tree to the top. After it, the values of ai, bi, ci, di and S of
each node have to be calculated. This process goes from the top, with values of ai = 1, ci = 0, di = −1, bi = ti and
S = ∅, to the bottom of the tree, following the described equations.

Once these values have been found for each node, the search for subtrees that can substitute a node begins. For any
subtree that could substitute a node, ﬁrst its semantic is checked with the set S of the node. If all of the values are valid,
then the semantic is evaluated on the equation 6 of that node. This subtree can replace the node if the result of the
following equation is positive:

M SE −

1
N

N
(cid:88)

i=1

(

ai · oi − bi
ci · oi − di

)2

(26)

If there are several subtrees that could replace a node, the selected subtree will be the one with the highest positive
value in this equation. The search for subtrees has 4 different methods: search for constants (constant search), search
for variables (variable search), search for constants combined with variables (constant-variable search) and search for
constants combined with expressions (constant-expression search). These searches do not take place in those nodes in
which in which S = N aN , since there are no valid values. However, since this method always builds correct trees, no
nodes with S = N aN are expected. The following subsections discuss each of these searches.

2.1 Search for constants

One of the biggest problems in GP is constant generation. In the ﬁrst approaches, constant generation was left to the
evolutionary process. An ephemeral random constant was included into the terminal set, so each time it was selected in
the building of a tree, a random constant in a predeﬁned interval was generated. The building of a useful value was left
to the evolutionary process, by successive combination of these random constants. Even there are some approaches
using gradient descend [18][19], this technique is still slow, demanding a high number of useless operations, being very
inefﬁcient.

Within a tree, constants take place as a terminal node. Therefore, they can be seen as trees with a single node,
representing its constant value, and therefore they are also considered as a model, with a semantic. The particularity of
the semantic of a constant k is that all of the elements of the vector semantic take the same value: oi = k. In other
words, the semantics of all of the constants are situated on the line span(1, 1, ..., 1).

Given a node of the tree, with its corresponding equation, the objective is to ﬁnd the constant that maximizes equation
26, i.e., the constant k that minimizes the following equation:

1
N

N
(cid:88)

i=1

(

ai · oi − bi
ci · oi − di

)2 =

1
N

N
(cid:88)

i=1

(

ai · k − bi
ci · k − di

)2

(27)

11

A PREPRINT - NOVEMBER 18, 2019

Figure 5: Examples of a) constant search, b) variable search, c) constant-variable search, d) constant-expression search

The way to ﬁnd this constant is to calculate the derivative of this expression, set it equal to zero, and calculate the value
k. Being the original equation a sum of squared expressions, any value of k that makes the derivative equal to 0 will be
a minimum, since equation 27 does not have any maximum. The derivative of this expression is the following equation:

2
N

N
(cid:88)

i=1

(bi · ci − ai · di)

ai · k − bi
(ci · k − di)3

(28)

In general, calculating the values in which this expression becomes 0 is a time-consuming task, because it involves
building a 3N-order polynomial and ﬁnding its roots, with N possibly being very high. Also, there are many possible
values for k. However, this calculation can be simpliﬁed in some common situations:

• ci = 0 and ai = 0 (i=1,...,N). In this case, the function is constant and has no minimum.
• ci = 0 (i=1,...,N) and any di = 0. In this case, the function can not be calculated and therefore there is no

minimum.

• ci = 0 and di (cid:54)= 0 (i=1,...,N). In this case, the only minimum value of k is given by:

k =

(cid:80)N

i=1
(cid:80)N

i=1

ai·bi
d2
i
a2
i
d2
i

(29)

This equation can be simpliﬁed in the following situations:

– ai are constants (ai = ka) and di are constants (di = kd). The case with kd = −1 happens when no

division has been performed yet. The minimum k is given by:

– ai are constants (ai = ka) and di are not constants. The minimum k is given by:

k =

1
ka · N

N
(cid:88)

i=1

bi

1
ka

(cid:80)N

i=1

(cid:80)N

i=1

bi
d2
i
1
d2
i

k =

12

(30)

(31)

x1kk+x1k*x11/x1-x1k-x1k/x1tpkk+pk*ptktx1x2x3ta)c)b)d)A PREPRINT - NOVEMBER 18, 2019

– ai are not constants and di are constants (di = kd). The minimum k is given by:

k =

(cid:80)N

i=1 ai · bi
(cid:80)N
i=1 a2
i

(32)

• Any ci = 0 and di = 0 (i=1,...,N). In this case, the function can not be calculated and therefore has no

minimum.

• ci (cid:54)= 0 and di = 0 (i=1,...,N). In this case, the only minimum value of k is given by:

k =

(cid:80)N

i=1

(cid:80)N

i=1

b2
i
c2
i
ai·bi
c2
i

(33)

As happens with equation 29, this equation can be simpliﬁed for the cases in which bi and/or ci are constants.
• ci and di are constants ci = kc and di = kd (i=1,...,N), and kc (cid:54)= 0 and kd (cid:54)= 0. In this case, the only minimum

value of k is given by:

k =

(cid:80)N

(cid:80)N

i=1 bi · (bi · kc − ai · kd)
i=1 ai · (bi · kc − ai · kd)

(34)

In any of these cases, once the minimum k has been found, it has to be checked with the S set. For each semantic s ∈ S,
if any value si = k, then this search is unsuccessful.

If none of these situations take place, as was stated, the ﬁnding of the minimum values can be a time-demanding task.
However, in this work we propose the alternative solution of evaluating a set of points that can have a low value in
equation 6. These values are the zeros on each term inside the sum. These values are given by bi/ai. In general, the
minimum value of equation 6 will not be any of these values. However, one of them will take a close value. In order to
avoid rounding errors, in this calculations those zeros with values close to any poles (di/ci) are excluded.

Therefore, in the general case the process is the following:

1. Take the N zi = bi/ai zero values.
2. Compare each zi with all of the values of each s ∈ S. If there is any coincidence, delete zi, since it is not valid.
3. Exclude those values which are too close to any root of the denominator of the equation 6.
4. Evaluate the remaining zi with equation 6 and select the one with the lowest value (lowest MSE) as k.

Once a value of k has been found for a node, if the result of equation 26 is positive, then this search was successful and
a single-node terminal tree representing this constant can substitute this node in the tree, leading to an improvement in
the MSE.

This process allows the creation and reﬁnement of constants. However, this is not limited to changing the value of one
constant for another (i.e., one terminal node representing a constant for another terminal node representing a constant).
It is important to highlight that this process of ﬁnding a constant can be performed for each node of the tree to be
improved, whether it is a constant, variable or a non terminal node. The user may decide on which nodes he wants this
search to be performed. Therefore, the node selected to be replaced by a constant can be a non-terminal. In this case, if
this search is successful, the tree is being reduced, having the result a lower number of nodes.

In general, it has been found that, as result of this search, most of the times a constant is changed into another. Therefore,
this search technique is very useful to reﬁne the constant values of the nodes of the tree. Sometimes a variable or a
non-terminal node (thus reducing the tree) is changed by a constant. However, since these situations occur very few
times, much computational time can be saved by performing this search only on constant nodes, and exploring the rest
of the nodes only when needed. As in the rest of the searches, the user can decide whether he wants to perform this
search only on constant nodes and maybe skip an interesting improvement, or perform a full search on all of the nodes
of the tree.

Figure 5 a) shows and example of a constant search. The shape given by equation 6 has an intersection with the grey
line deﬁned as span(1,1,...,1). Any constant node is on this line. Therefore, any constant node on this line and inside the
ellipsis would give a lower MSE than the given tree. This method returns the constant that returns the lowest MSE for
this shape.

13

A PREPRINT - NOVEMBER 18, 2019

2.2 Search for variables

This process is easier to calculate than the previous one. As happens with constants, each variable is represented by a
terminal node, with a semantic. Therefore, variables are also points in the search space.

Given a node of the tree, with its equation 6 and its S set, the ﬁrst to do is to check among all of the variables if their
semantics are valid with the S set. For those variables with valid semantics, the equation 26 is evaluated with their
semantics. Again, a positive value means a reduction in MSE. These are variables inside the shape of that node in the
semantic space. The selected variable will be the one with the highest reduction in MSE.

As in the previous search process, any node of a tree can be selected to be changed with a variable, being this node
constant, variable or non-terminal. Therefore, the number of nodes in the tree can not grow with this method, as in the
previous one, and the tree can be reduced. As in the previous search, the user may decide on which nodes he wants to
perform this search. Obviously, if this search on being performed on variable nodes, those variables equal to the nodes
being explored are excluded.

Also, as happened in the previous search, it has been found that this search is successful most of the times on constant
nodes. Therefore, computational time can be saved if this search is performed only on constant nodes, and exploring
the rest of the nodes only when needed.

Those variables with constant values (the same values for each datapoint) are excluded from this computation, since
they are constants and could be optimised by the previous method.

Figure 5 b) shows and example of a variable search. The variable x3 was found to be inside the shape given by equation
6. Therefore, the given tree can be replaced by this variable, having an improvement in MSE. Variables x1 and x2 do
not lead to improving MSE.

2.3 Search for variables combined with constants

With the two search methods already described, the number of nodes of the tree can become lower. However, often
there is the necessity to ﬁnd larger and more complex trees to get closer to the solution. This subsection allows the
ﬁnding of simple subtrees with 3 nodes to replace another node of the tree. If this node to be replaced is a terminal
node, then the tree will grow.

The idea behind this search method is a combination of the two previous methods. After the calculation of each equation
on the nodes of the tree, it is possible not to ﬁnd a constant or a variable that improves the MSE (a constant or a variable
inside any of the shapes in the space). However, although a variable x is not inside any of the shapes, the models k + x,
k − x, k · x or k/x may be inside one of the shapes, for any value of k. These four expressions represent, each of it,
a line in the semantic space. This search method will look for the intersection of one of these lines with each of the
shapes. Looking for intersections is equivalent to looking for the best value of k. Therefore, this third method proposes
the search of a variable combined with a constant, with one of the for arithmetic operations. For each variable x, the
possibilites are:

• Sum operation. Although the variable x represents a single point in the search space, the expression (k + x)
represents a line in the search space. This line goes parallel to the line in which constants are situated.
Therefore, if the variable x is not inside any of the shapes, any section of the line (k + x) may be inside of the
shapes. The expression (x + k) is represented by a tree with three nodes: a non-terminal node representing the
sum, and two nodes, with the constant k and the variable x.
The objective here is, given a node with its equation calculated, to ﬁnd a constant value that maximizes
equation 26 for the semantic of the tree (k + x). To do this, a new equation is calculated from the ai, bi, ci,
di and S values of the equation of this node. As the constant is going to be situated as the ﬁrst child of the
sum operation, this new equation is calculated as in equation 9, with a(cid:48)
i = ci and
d(cid:48)
i = di − ci · xi, where xi are the values of the variable x for each pattern. Also, S(cid:48) is calculated from S
with the method previously described for the sum operator. With this new equation, the constant optimization
process described in section 2.1 is performed. As a result, an improvement value is returned, a positive one
indicates that this node can be replaced with (k + x).

i = bi − ai · xi, c(cid:48)

i = ai, b(cid:48)

• Minus operation. Similarly with the previous case, the expression (k − x) represents a line the space, parallel
to the constant line, and parallel to (k + x) line. Note that in the previous case the position of the constant and
variable is indifferent: (k + x) and (x + k) leads to the same expression. However, in this case the constant
must be the ﬁrst argument of the minus operation, and the variable the second. Otherwise, we would be again
in the previous case. to keep coherence, in all of these four cases the constant is going to be the ﬁrst argument.

14

A PREPRINT - NOVEMBER 18, 2019

The process is similar in all of the four cases: given a node with an equation represented by ai, bi, ci, di
and S, a new equation is calculated, this time from equation 19: with a(cid:48)
i = ci,
d(cid:48)
i = di + ci · xi, and S(cid:48) calculated from S with the method previously described for the substraction operation
Once this equation was calculated, the constant optimization process is performed.

i = bi + ai · xi, c(cid:48)

i = ai, b(cid:48)

• Multiplication operation. In this case, the expression (k · x) represents a line in the search space in which the
vector x is included, i.e., all of the vectors in (k · x) are collinear to x. The objective here is the same: ﬁnd the
value of k that minimizes equation 6.
Given a node of the tree with an equation represented by ai, bi, ci, di and S, a new equation is calculated, this
i = di, and S(cid:48) calculated from S with the method
i = ci · xi, d(cid:48)
time from equation 13: a(cid:48)
previously described for the multiplication operation. With this equation, the previous constant search process
is undergone, having as result a value of reduction in MSE.

i = ai · xi, b(cid:48)

i = bi, c(cid:48)

• Division operation. In this case, the expression (k/x) represents a line in the search space in which the vector
given by the values 1/xi is included, i.e., all of the vectors in (k/x) are collinear to the vector given by 1/xi.
With the same objective as in the previous cases, and given a node, from the equation of this node a new one is
calculated, with equation 19: a(cid:48)
i = di · xi, and S(cid:48) calculated from S with the
method previously described for the division operation. The same constant optimization is performed.
As in the minus operation, this operation does not allow changing the order of the children. If (x/k) was chosen
instead of (k/x), then an operation similar to the previous one (constant-variable search with multiplication
operation) will be being performed, having as result the value of 1/k.

i = bi · xi, c(cid:48)

i = ai, b(cid:48)

i = ci, d(cid:48)

In this search, for each of the four operations, a set S(cid:48) has to be calculated from the set S of the node. If S(cid:48) = N aN ,
then the constant search for that operation, node and variable does not take place, since there are not possible values for
the constant. A particular case happens in the division operator when the variable has any 0 in its semantic. In this case,
the result of the calculation of S(cid:48) is N aN , excluding those variables with 0 values in semantics from being the second
child of a division operation.

This process can be done for each node of the tree, whether it is a constant, variable or non-terminal, each variable and
each of the four operations. As a result, the combination selected is the one that returns a higher value in equation 26, in
case it is higher than 0. This combination states which node can be changed and which 3-node tree insert in its place.
The node to be changed can be terminal or non-terminal, so this search process can lead to having the tree increase
or decrease the number of nodes. However, it has been found that most of the times this search is successful only on
terminal nodes (constants and variables), and sometimes on non-terminal nodes. As in previous searches, performing
this search only on terminal nodes may save computational time.

Figure 5 c) shows and example of a constant-variable search. The shape given by equation 6 does not have an
intersection with the grey line deﬁned as span(1,1,...,1), and there are not variables inside this shape. Therefore constant
and variables searches would not be successful. However, from the variable x1 four different lines can be deﬁned:
k + x1, k − x1, k ∗ x1 and k/x1. In this case, the grey lines k + x1 and k ∗ x1 have an intersection with the shape, so
in both cases a constant can be calculated to have an expression k + x1 or k ∗ x1 that improves MSE. The given tree
would be replaced with this expression.

As in the previous search, those variables with constant values (the same values for each datapoint) are excluded from
this computation.

It is important to highlight that this search has to be carefully used so it does not "hide" a constant search. For instance,
the branch "(3.2 + x3)" can be selected for this search. As a result, this node could be changed by "(3.5 + x3)" in this
constant-variable search. However, in practice what was done is to change the constant node from a value of 3.2 to a
value of 3.5. For this reason, for a speciﬁc variable and each of the four operations (+,-,*,/), this search is not performed
on nodes representing the same operation, with a constant as ﬁrst child and the same variable as second child.

Moreover, even with the described precaution, it is possible to use this operation to "hide" a constant search. Another
example could be in the tree (3.4 + x2), in which x2 could be replaced by (0.1 + x2). This has as result the tree
(3.4 + (0.1 + x2)), which has a semantic equivalent to (3.5 + x2), and could be generated by a constant search. Another
example could be ((1.2 · x1) + x1). In this tree, the second x1 could be changed into (0.5 · x1), and the resulting tree
would be ((1.2 · x1) + (0.5 · x1)), which has a semantic equivalent to (1.7 · x1) or ((0.7 · x1) + x1). This last tree could
be found from the original one, by performing a constant search on the constant leaf.

In general, it is hard to determine when a constant-variable search can "hide" a constant search. This situation makes
the trees unnecessarily large, with possibly a high number of nodes (see section 2.6). If a constraint to the size of the
tree is set, then the algorithm could be prematurely stop because of having too many nodes that could be simpliﬁed.

15

A PREPRINT - NOVEMBER 18, 2019

One way to avoid this problem is, before this search takes place, optimize all of the constants. This process is described
in section 2.6, and it is based on constant search.

Another possibility is to perform a constant search when this search is done. With this approach, a modiﬁcation of a
constant and a constant-variable search that modiﬁes the same constant will have the same MSE reduction. The ﬁrst of
them (constant search) would be the chosen to modify the tree.

2.4 Search for expressions combined with constants

The previous search allows the tree to grow by replacing terminal nodes with small branches. However, once a part of
the tree has been built, it will unlikely be modiﬁed, and its modiﬁcation involves the deletion of a branch and substitution
by a constant, variable or constant-variable branch. This can be undesirable, since that deleted part of the tree has
proved to be useful. Therefore, a method that allows the modiﬁcation of a branch without deleting it is desirable.

This search allows the tree to grow by modifying a non-terminal node of the tree. This modiﬁcation is done as in the
previous cases: ﬁnding a better branch and replacing the whole subtree with this branch. However, in this case, the
branch contains the previous subtree, so it it not deleted but modiﬁed. For instance, a node 2.5/(3 + x2) can be replaced
by the branch (3.4 + 2.5/(3 + x2)), in which the second child is the previous subtree.

The idea behind this search method is similar to constant-variable search. In this case, the node selected represents a
point p in the semantic space in the border of the MSE shape. There may be a constant k that makes (k + p) inside the
MSE shape, thus improving the MSE. As in the previous search, (k + p) is the line that goes through p, parallel to the
constant line (1,1,...,1). Similarly, there may be a constant k that makes (k ∗ p) inside the MSE shape. (k ∗ p) is the line
that goes through p and (0,0,...,0). In both cases, ﬁnding a value for k means ﬁnding a point in the corresponding line
inside the MSE shape. Other approaches such as (k − p) or (k/p) are not explored, because they do not begin in the
border of the MSE shape and thus are not likely to return an improvement in MSE. In the ﬁrst case, (k − p), the line
goes though −p instead of p, and in the second case, (k/p), the line goes through 1/p instead of p.

As opposite to the previous searches, this search is performed only on non-terminal nodes, because if it is performed on
a terminal node:

• If this node is a constant, then a constant search is being performed.
• If this node is a variable, then a constant-variable search is being performed.

The behaviour of this search is strongly similar to the constant-variable search. In the case of performing a (k + p)
search on a node p with its equation, a constant value that maximizes equation 26 for the semantic of the tree (k + p)
has to be found. To do this, a new equation is calculated from the ai, bi, ci, di and S values of the equation of this
node. As the constant is going to be situated as the ﬁrst child of the sum operation, this new equation is calculated as in
equation 9, with a(cid:48)
i = di − ci · pi, where pi are the values of the node p for each
pattern, and S(cid:48) is calculated from S with the method described for the sum operation. With this new equation, the
constant optimization process described in section 2.1 is performed. As a result, an improvement value is returned, a
positive one indicates that this node can be replaced with (k + p).

i = bi − ai · pi, c(cid:48)

i = ai, b(cid:48)

i = ci, d(cid:48)

If a (k · p) search is performed on a node with an equation represented by ai, bi, ci, di and S, a new equation is
i = di, and S(cid:48) is calculated from S with the
i = bi, c(cid:48)
calculated, this time from equation 13: a(cid:48)
method described for the multiplication operation. With this equation, a constant search process is undergone, having as
result a value of reduction in MSE.

i = ai · pi, b(cid:48)

i = ci · pi, d(cid:48)

With this search, the size of tree is always increased in two nodes.

Figure 5 d) shows and example of a constant-expression search. In this case, the node is represented by the semantic p.
From this point, two lines can be deﬁned: k + p and k · p, having both of them a part inside the shape. Thus, a constant
can be calculated for each line, and the tree can be replaced by either k + p or k · p. Only sum and multiplication
operations are considered because on k − p and k/p the nodes −p and 1/p are not tangent to the shape and it is less
likely to ﬁnd a line with an intersection with the shape.

As happens with constant-variable search, constant-expression search can hide a constant search. For example, if the
tree is (3 + 2 · x1) and the operation (k + p) is used, then the result would be (k + (3 + 2 · x1)), which has a semantic
equivalent to (k + 3) + (2 · x1), and could be generated by a constant search on the ﬁrst constant leaf.

One way to partially avoid this situation is not to perform constant-expression searches with the operation (k + p) on
non-terminal nodes with the operation "+" or "-", and one of the children is a constant. In the same way, constant-
expression searches with the operation (k · p) on non-terminal nodes with the operation "*" or "/", and one of the
children is a constant should be avoided too.

16

A PREPRINT - NOVEMBER 18, 2019

Even with these precautions, a constant-expression search can hide a constant search. For example, the tree ((2 −
x4) + (4 · x4)) with the operation (k + p), would result k + ((2 − x4) + (4 · x4)), which has a semantic equivalent to
((k + 2) − x4) + (4 · x4). This last tree could be found by a constant search.

Another example could be the tree (3 + (2 · x1)), in which the node where the constant-expression search takes place is
p = (2 · x1). In this case, a (k + p) constant-expression search could take place and p could be replaced by (k + (2 · x1)).
In this example, the ﬁnal tree would be (3 + (k + (2 · x))), which has a semantic equivalent to ((3 + k) + (2 · x)). This
tree could be found by means of a constant search.

Therefore, the same situation as in previous search happens. It is hard to know in advance when a constant-variable
search or a constant-expression search will hide a constant search. As was already stated, this leads to the problem of
having too large trees, and too early stopping of the algorithm. The same two solutions can be applied: performing an
optimization of all of the constants before these searches, or perform a constant search at the same time.

2.5 Deletion of parts of the tree

The deletion of a branch of the tree implies replacing its father with its "sibling" branch. For instance, in the tree
(2 · x1 + 3 · x2), the deletion of the branch 3 · x2 leads to have (2 · x1+?) and then replace the root with the ﬁrst child,
having as result 2 · x1. In this example, this is equivalent to having performed a constant search in the branch 3 · x2
and having as result an improvement in MSE with the constant 0, leading to the tree (2 · x1 + 0). If the root were the
operators of -, * or /, the corresponding constants would be 0, 1 and 1.

Therefore, in all of the cases deleting a branch of the tree is similar to performing a constant search in that branch, and
having as result one of those constants, which is very unlikely to happen. For this reason, the deletion of parts of the
tree is not considered in this method, because it can be performed by constant, variable and constant-variable searches.

2.6 Optimization of constants

As the tree is being built, each time a constant is generated, its value is the best for that tree. However, as the
improvement process keeps going on, the tree will be modiﬁed. This makes that this previously calculated constant
value will not be the best for the new tree and, therefore, this tree does not return the best possible result with that
structure. This happens with all of the constants of the tree. Therefore, each time the tree is modiﬁed, an optimization of
the constants could be performed in order to ﬁnd their new best values. Also, it has been found that, after a tree has been
modiﬁed, constant search almost always leads to have reduction in MSE for several iterations of only constant search.

For this reason, a constant optimization process is proposed. This process is simply based on examining the tree, taking
all of the constant nodes and perform successively the following steps on each of them:

1. Calculate the equation in this node.

2. Perform a constant search, having as result a value of MSE reduction.

3. If this reduction of MSE is higher than a value (see section 2.8 for details of this parameter), then modify the

constant of the node with the result of the search.

Once these steps are executed on a constant node, the process goes into the following one. When the last constant node
has been processed, this process begins again with the ﬁrst. This process ﬁnishes when none of the constant nodes can
be optimized. Note that before that point is reached, it is possible that each node has been optimized several times.

A different constant optimization process could be considered, in which in each iteration a constant search is performed
on all of the constant nodes, and the one with the highest reduction (in case it is higher than the parameter value) is
modiﬁed. However, this would need the calculation of all of the equations of the constant nodes in each iteration. Since
the calculation of an equation needs the calculation of the equations of the previous nodes and the constant nodes
are always leaves of the tree, this implies the calculation of a high number of equations of the tree. This can be a
time-demanding task. For this reason, the described approach is proposed instead, which is much more efﬁcient, since
only a small number of equations need to be calculated in each iteration. However, if the user wishes to perform a
constant optimization process in which in each iteration the constant to be changed is the one with the highest MSE
reduction, this can be done by repeately executing constant searches on constant nodes.

As it is obvious, if there is only one constant in the tree, then only one iteration is performed.

The optimization of constants can be very useful to prevent some successful searches that in practice only modify the
value of a constant, making an artiﬁcial growth of the tree. For instance, in the tree (2 + x) a constant-variable search
can be performed at the variable x, having as result the tree (2 + (3 + x)). This tree returns the same results as (5 + x),

17

A PREPRINT - NOVEMBER 18, 2019

which could be obtained simply with a constant search on the constant 2. In this example, the constant-variable search
result was hiding a constant search, making the tree unnecessarily larger. If a constraint to the size of the tree is used
(see section 2.7), then this growth of the tree makes a premature stop of the algorithm. An additional problem of having
too large trees is that the computational time is increased because all of the searches are applied in a larger number of
nodes.

As was already described, one way to prevent these situations is, before the selected searches take place, perform an
optimization of all of the constants of the tree. If this is done, obviously constant search on constants should not be
performed after the optimization of constants.

2.7 Constraints to the tree

As one of the objectives of this work is to ﬁnd simple expressions easy to analyse by a human, it is interesting to limit
the complexity of these expressions. With this objective, two constraints are used: height of the tree and number of
nodes. The user may make use of any of these two, or none.

Both restrictions make effect in the constant-variable and constant-expression searches described in section 2.3. In the
other two searches there is no need to apply complexity constraints, because in both searches the result would be a tree
with the same or less complexity. Only on constant-variable and constant-expression searches the complexity of the tree
is increased.

The application of these two constraints is very straightforward. If the maximum height constraint is being used, instead
of performing these two searches in all of the nodes of the tree, they will be done only in the nodes whose depth is
lower than the maximum height. If the maximum number of nodes (n) constraint is being used, the tree has r nodes and
each node of the tree represents a subtree with s nodes, then the constant search is performed only on those nodes that
meet the constraint n − r + s ≥ 3. The constant-expression search is performed if n − r ≥ 2

These two constraints are closely related. Since only binary operations, arithmetic functions, are used, the height
constraint sets a limit to the number of nodes. The number of nodes of a tree of height h can be up to 2h−1. Moreover,
in this tree these 2h−1 must be balanced, so a height limit is a constraint to the number of nodes, but also to the structure
of these nodes in the tree. Also, it was already demonstrated that a binary tree with n nodes has as average a height
πn [20]. Therefore, to allow the building of a tree with n nodes without any structure constraint, the maximum
of 2
πn−1 nodes, which is a very large
height should be higher than 2
number. For this reason, the height limit is not used, and the only constraint is the number of nodes.

πn, which would lead to having more than 22

√

√

√

Setting a limit on the complexity of the tree has two interesting features. First, it allows the obtaining simple easy-
to-understand expressions. Second, it allows the obtaining models with better generalization abilities and controlling
overﬁtting. The described system has the ability to grow until a good enough result on the training set is found. However,
an excessive growth, resulting in a very large tree and a very complex model, may lead to overﬁtting the training set. for
this reason, the user may set a limit to this growth in the maximum number of nodes that the tree can have. Thus, this
parameter will be important and the experiments will be performed with it.

2.8 Algorithm

The method proposed in this section allows the creation of trees with a low MSE value. However, differently from GP
and GSGP in which many different trees are created, in this method a simple tree is continuously improved.

The algorithm begins with a simple initial tree made of a single node representing a constant. This constant is the
point in the constant line closer to the target point. To calculate it, set the constraint that the vectors k and k-t must be
perpendicular, and therefore the point product < k, k − t >= 0. Developing this expression leads to

0 =

N
(cid:88)

i=1

k · (k − ti) =

N
(cid:88)

i=1

k2 −

N
(cid:88)

i=1

k · ti = N · k2 − k ·

N
(cid:88)

i=1

ti

(35)

And therefore k = 1
i=1 ti is the average value of the targets. As this constant node is already the best value, it will
N
not be optimised due to the constant search process. This value could also be obtained with a constant search from
equation 29 (ai = 1, bi = ti, ci = 0, di = −1, S = ∅).

(cid:80)N

After this initial tree has been created, the iterative process begins. On each iteration, the constant search, variable
search, constant-variable and constant-expression search can be performed on any nodes, whether they are constant,
variable or non-terminal nodes, following a speciﬁc strategy. For instance, some possible strategies could be:

18

A PREPRINT - NOVEMBER 18, 2019

1. Perform the four searches at the same time, and substitute the corresponding node with the result of the strategy
with a higher reduction in MSE, in case it is positive. This is the strategy that makes a full search, however, it is
also the most time-demanding strategy, since much computational time is wasted in performing operations that
will not lead to improving the tree. IT is important to remark that in this strategy constant search is performed
in constants, together with constant-variable search and constant-expression search, to avoid the mentioned
problem of "hiding" changes in constants and tree growing.

2. Perform the four searches at the same time, however, in constant search only perform this search in variables
and non-terminal nodes. After any modiﬁcation is done to the tree, perform the constant optimization process.
This optimization ensures that the next constant-variable and constant-expression searches do not "hide" a
change in constants.
These two strategies can be seen as similar in the sense that in the second one a constant optimization step
is performed on each iteration, while in the ﬁrst one a constant search on the constant nodes of the tree is
performed on each iteration. Therefore, in the ﬁrst strategy successive iterations may perform a constant
reﬁnement similar to the constant optimization performed in the second strategy. However, this constant
reﬁnement performed in the ﬁrst strategy will be stopped when any other search returns a higher MSE reduction,
while in the second strategy the constant reﬁnement is stopped when no MSE reduction is found in the constant
optimization process.

3. In order to save computational time, those searches that are found not to be successful most of the times can
be avoided. Also, the searched can be performed sequentially, and when one of the searches is successful, the
rest are not performed. This strategy proposes to run the following steps on each iteration:
(a) Perform variable search only on constant nodes.
(b) If the previous search was not successful, perform constant-expression search (this search is always only

performed on non-terminal nodes).

(c) If any of the two previous searches were not successful, perform constant-variable search on terminal

nodes.

(d) If none of the three previous searches was successful, perform together:

• Constant search on variable and non-terminal nodes.
• Variable search on variable and non-terminal nodes.
• Constant-variable search on non-terminal nodes.

(e) If any of the previous searches was successful, perform a constant optimization.
The objective of these search is to perform those searches that are unlikely to be successful only when the rest
are not successful. This usually happens when the tree has reached the maximum limit. As it can be seen,
those searches that are performed when this happens are those that allow the tree to become smaller.

4. In a similar way to the ﬁrst and second strategies, this fourth strategy is like the third, but without the constant
optimization process at the end. Instead of it, the ﬁrst step of this strategy is a constant search on constant
nodes (reﬁnement of constant values). If a constant is modiﬁed, then no further searches are performed and
this iteration is ﬁnished. If no constant is modiﬁed, then a strategy similar to strategy 3 is executed, except for
the ﬁnal constant optimization, which is not performed.

These four strategies were used in the experiments because they describe two different situations: an exhaustive strategy
in which in each iteration all of the searches are performed in all of the nodes, and a more light strategy in which in
each iteration the while tree is examined only when no previous search was successful. In both strategies, their variants
with and without constant optimization are explored too.

However, the user may decide to use other strategies he may think to be useful. Other examples are the following:

• Perform the searches consecutively. On each of them, modify the tree accordingly.
• Perform constant and/or variable search and, if no modiﬁcation is done to the tree, constant-variable search. If

not modiﬁcation is done, perform constant-expression search.

• Perform constant search. If no modiﬁcation is done, perform variable search. If no modiﬁcation is done in this
second search, perform constant-expression search. If no modiﬁcation is done in this third search, perform
constant-variable search.

Constant search is the less computational expensive search, while constant-variable search is the most time consuming.
So, a good strategy should keep a balance between high improvements and fast computation.

This process is iteratively performed until a stopping criteria is met. These criteria can be conﬁgured by the user. This
work proposes the use of the following criteria:

19

A PREPRINT - NOVEMBER 18, 2019

• The number of iterations exceeds a ﬁxed value.
• The MSE in the current tree reaches the goal value set by the user.
• The tree could not be improved in the last iteration. A hyperparameter deﬁning the minimum improvement to

consider that a search has been successful is needed.

Therefore, this system can be conﬁgured with a low number of hyperparameters:

• Maximum number of iterations to be executed. Default value: inﬁnite.
• Goal in MSE. Default value: 0.
• Minimum of improvement in the MSE for any search. A search is found to be successful if the reduction
in MSE is positive and higher to the previous MSE value multiplied by this parameter. For example, if this
parameter takes a value of 10−2, then a search will be successful only if the reduction in MSE is greater
than 1% of the MSE. This value is also applied to the constant optimization process described in section 2.6.
Default value: 10−6.

• Maximum number of nodes of the tree. Default value: inﬁnite,
• Strategy to be used on each iteration.

Although the execution of this system involves a great number of calculations, these can be efﬁciently performed with
element-wise operations and parallel executions. element-wise operations can be performed on the evaluation of the
equations of each node and in the expressions of the search for constants. Parallel code execution can be performed in
the three search processes, because they involve the evaluation of functions in all of the nodes of the tree. Therefore, the
use of Single Instruction, Multiple Data (SIMD) programming [21] can be of a great beneﬁt for this algorithm.

An important feature of this algorithm is that it is deterministic, i.e., each time it is run with the same dataset it will
return the same result. This makes that the number of times it has to be run for each parameter conﬁguration is only one.

This algorithm was implemented in Julia [22]. The source code of this technique will be provided so all of the
experiments described in this paper can be repeated, and the system can be used by anyone to perform their own
experiments or make use of it in their researches.

3 Example of equation development

This section describes the application of this system in order to develop an easy and well-known expression: Newton’s
law of universal gravitation [23]. This equation measures the force of attraction between two masses. This force is
proportional to the product of both masses, and inversely proportional to the square of their distance. This equation
is given by 36, in which M1 and M2 are the masses, d is the distance, and G = 6.67392 · 10−11 is the gravitational
constant.

F = G ·

M1 · M2
d2

(36)

This equation is of application to arbitrarily large or small masses and/or distances, i.e., it can be applied either to the
force between small particles and big planets. In this example, this second case will be explored, in order to show
that the system can return good results with arbitrarily different values, since masses and distances belong to different
ranges.

In order to apply this equation, 1000 random patterns were built. Each pattern is composed by:

• x1: Mass of the ﬁrst planet: a random number between 1023 and 1025
• x2: Mass of the second planet: a random number between 1023 and 1025
• x3: Distance between both planets: a random number between 108 and 1012
• target: the result of the application of equation 36

With this dataset, the system was conﬁgured with the following hyperparameters:

• Maximum height of the tree: Inﬁnite
• Maximum number of nodes: Inﬁnite

20

A PREPRINT - NOVEMBER 18, 2019

Table 3: Summary of the execution for developing Newton’s law of universal gravitation

Iteration

Node selected
for substitution

Resulting expression

MSE

0
1
2
3
4

-
9.185106275464827 · 1020
1.1526861538137098 · 1030
434063.5718753305
3.932029293203675 · 1019

9.185106275464827 · 1020
(1.1526861538137098 · 1030/x3)
((434063.5718753305 · x2)/x3)
(((3.932029293203675 · 10−19 · x1) · x2)/x3)
((((6.673919999999998 · 10−11/x3) · x1) · x2)/x3)

2.7645 · 1043
2.1717 · 1043
1.9183 · 1043
5.1733 · 1042
1.3863 · 1012

Strategy Minimum average Minimum MSE

Table 4: Conﬁgurations that returned the best MSE in the training sets
Test
MSE

Maximum
number of nodes

MSE training

1
2
3
4

6.218 (0.733)
5.954 (0.621)
4.933 (0.485)
5.217 (0.561)

reduction
10−10
10−8
10−5
10−6

195
200
200
200

27.366 (18.874)
50.707 (64.974)
26.096 (38.273)
132.638 (341.447)

• Strategy: The ﬁrst strategy described on section 2.8
• Goal in MSE. This parameter is found to be very important in this example, since we are working with values
in a very wide range, from very small (G) to very large (x1 and x2). The targets were also in a large scale,
from 1012N to 1022N . Due to the limitation of the use of ﬂoating point values, even when 64 bits are used for
their storage, when working with such a wide range, unaccuracies may occur due to operating with numbers
on very different scales. For instance, the operation 1020 + 10−10 − 1020 returns an incorrect value of 0. This
may result that, even the correct expression has been obtained, the MSE calculated is not 0 due to unaccuracies,
and the system keeps trying to improve this expression with inaccurate calculations. For this reason, a goal
MSE of 0 was not used in this example. Instead of it, the goal used was the average value of the targets.

Table 3 shows a summary of an execution of this system with the described dataset. As was described, before the
execution is performed, an initial tree with a single constant node is built. In this execution, this constant had as
value 9.185106275464827 · 1020. From this initial tree, 4 iterations were performed until an expression with a MSE
below the goal was found. All of the operations performed were constant-variable searches, and the nodes selected for
substitutions were the constants. This example also shows the ability of this system to develop the constants needed for
the expressions to be returned. For this reason, the constants are shown on this table with a large number of decimals.
Finally, as a result of this execution, the expression ((((6.673919999999998 · 10−11/x3) · x1) · x2)/x3) is returned by
the system, which is similar to equation 36.

4 Experiments

The previous section showed that the system described in section 2 can be successfully used as tool for developing
mathematical expressions. These mathematical expressions can also be used as a ML model. This section describes
the experiments carried out with this objective, on a real-world problem. This is a well-known problem, in which the
objective is to predict the median value of a home in the area of Boston Mass. The information was collected by the
U.S Census Service and is available from the StatLib archive (http://lib.stat.cmu.edu/datasets/boston). This dataset was

Table 5: Conﬁgurations that returned the best MSE in the test sets

Strategy Minimum average Minimum MSE

MSE test

21.782 (17.435)
14.579 (7.026)
11.644 (6.624)
11.725 (6.075)

1
2
3
4

reduction
10−3
10−6
10−6
10−10

21

Maximum
number of nodes

Training
MSE

200
105
105
150

6.496 (0.629)
9.565 (1.374)
7.902 (0.556)
6.145 (0.645)

A PREPRINT - NOVEMBER 18, 2019

(a) Mean training, strategy 1

(b) Mean test, strategy 1

(c) Median test, strategy 1

(d) Mean training, strategy 2

(e) Mean test, strategy 2

(f) Median test, strategy 2

(g) Mean training, strategy 3

(h) Mean test, strategy 3

(i) Median test, strategy 3

(j) Mean training, strategy 4

(k) Mean test, strategy 4

(l) Median test, strategy 4

Figure 6: MSE results in training and test for the housing problem

(a) Mean training

(b) Mean test

(c) Median test

Figure 7: MSE results in training and test for the housing problem

22

50100150200-10-8-6-4-251015202530Maximum number of nodesMinimum MSE reduction (log10)50100150200-10-8-6-4-251015202530Maximum number of nodesMinimum MSE reduction (log10)50100150200-10-8-6-4-251015202530Maximum number of nodesMinimum MSE reduction (log10)50100150200-10-8-6-4-251015202530Maximum number of nodesMinimum MSE reduction (log10)50100150200-10-8-6-4-251015202530Maximum number of nodesMinimum MSE reduction (log10)50100150200-10-8-6-4-251015202530Maximum number of nodesMinimum MSE reduction (log10)50100150200-10-8-6-4-251015202530Maximum number of nodesMinimum MSE reduction (log10)50100150200-10-8-6-4-251015202530Maximum number of nodesMinimum MSE reduction (log10)50100150200-10-8-6-4-251015202530Maximum number of nodesMinimum MSE reduction (log10)50100150200-10-8-6-4-251015202530Maximum number of nodesMinimum MSE reduction (log10)50100150200-10-8-6-4-251015202530Maximum number of nodesMinimum MSE reduction (log10)50100150200-10-8-6-4-251015202530Maximum number of nodesMinimum MSE reduction (log10)2040608010051015202530Strategy 1Strategy 2Strategy 3Strategy 4Maximum number of nodesMSE2040608010051015202530Strategy 1Strategy 2Strategy 3Strategy 4Maximum number of nodesMSE2040608010051015202530Strategy 1Strategy 2Strategy 3Strategy 4Maximum number of nodesMSEA PREPRINT - NOVEMBER 18, 2019

Table 6: Summary of the results with a MLP

Epoch Training MSE

Test MSE

4347
49926

4.827 (0.125)
1.739 (0.065)

12.399 (6.869)
19.791 (14.779)

originally described at [24]. From this publication, it has been extensively used throughout the literature as benchmark.
This dataset has only 506 data points, and 13 input variables.

The experiments carried out in this part have as objective to show the applicability of this technique. In this sense,
as shown on section 2.7, it is important to limit the complexity of the trees. For this reason, the experiments will be
focused on studying the impact of the parameters described in section 2.7 that limits the size of the tree: maximum
number of nodes.

Also, the hyperparameter of minimum improvement in MSE plays an important role. For this reason, it was chosen
for performing the experiments. The values chosen for this parameter are 10−1, 10−2, 10−3, 10−4, 10−5, 10−6, 10−7,
10−8, 10−9 and 10−10.

The experiments done in this section were run with the four strategies explained in section 2.8.

For each parameter value studied, a 10-fold cross-validation was performed. Since this system is deterministic, only
one training in each fold was done. The results shown here are the average of the training results and average and
median values of the test results. In order to correctly compare the results, in all of the experiments performed, the
same training and test sets were used.

An important measure of the performance of this system is the computational time. All of the the experiments were run
on a Intel(R) Xeon(R) CPU E5-2650 v3, with a frequency of 2.30GHz. A measure of time was performed on each of
the experiments performed.

A grid search technique [25] was used to ﬁnd the best combination of minimum MSE improvement and maximum
number of nodes. Figure 6 shows 12 heatmaps with the MSE results of these experiments. On each heatmap ﬁgure, in
the x axis the maximum number of nodes is situated, and in the y axis the minimum reduction is located. In order to
show this graph more easily, this hyperparameter is shown as log10. Also, those MSE values higher than 35 or lower
than 5 were cropped to those limits.

First, ﬁgures 6a, 6d, 6g and 6j show, for each strategy, the heatmaps for the average training MSEs obtained for the
different values of the two hyperparameters. For each combination of the two hyperparameters, these ﬁgures show the
average of the 10 MSE training values obtained from the cross-validation process. As it was expected, as the maximum
number of nodes is increased, the complexity of the resulting expression grows too, and it is able to better ﬁt the training
set, having a lower MSE. However, this has the risk of overﬁtting the training set. For this reason, ﬁgures 6b, 6e, 6h
and 6k show, for each strategy, the average results of the 10 folds in the test sets. In some cases very high values in
MSE were obtained. This shows that this system can be very sensitive to outliers. This also has a big impact when
calculating the average of 10 test values. Therefore, in order to minimize this impact, median results on the test sets can
be reported. These results are shown on ﬁgures 6c, 6f, 6i and 6l, one for each strategy. As it can be seen, the best results
in test were not obtained with a very high number of nodes, i.e., with very high complex expressions. It seems that a
number of nodes between 20 and 100 returns the best results in test for this problem. Regarding the minimum MSE
reduction hyperparameter, no clear tendency can be concluded from these results.

Figures on 7 show a comparison between the MSE results in the different strategies. These ﬁgures show the average
values in training (ﬁg. 7a), test (ﬁg. 7b) and median values in test (ﬁg. 7c). A minimum MSE reduction of 10−6 was
chosen for the four strategies. Also, a maximum number of nodes between 20 and 100 was used. These ﬁgures clearly
show the reduction of MSE as the number of nodes and the complexity of the expressions grow. Also, it seems that the
best results are obtained with the third and fourth strategies.

Another interesting information is obtained when examining the height of the resulting trees. Figure 8 shows 4
heatmaps, one for each strategy, with the average heights of the 10 trees obtained in the 10-fold for each hyperparameter
conﬁguration. As was expected, higher number of nodes leads to having higher heights. However, these heights are not
in the scale of log2(n), being n the number of nodes. This shows that the system, with a limit of the number of nodes,
ﬁnds better results with very unbalanced trees rather than with balanced trees. This fact supports the idea of not using
an hyperparameter to limit the height of the tree.

Finally, ﬁgures in 9 show, for each strategy, the computational time (in seconds) for each hyperparameter conﬁguration.
The results of this measures were cropped when the time was higher than 150 seconds. As was expected, as the trees

23

A PREPRINT - NOVEMBER 18, 2019

(a) Strategy 1

(b) Strategy 2

(c) Strategy 3

(d) Strategy 4

Figure 8: Averages heights for the housing problem

are higher, the computational time is higher too, because a larger number of nodes have to be explored. Also, these
ﬁgures show that the computational time increases with lower minimum MSE reduction. Therefore, for fast executions
higher values of this parameter are preferred. With respect to the strategies, the third and fourth strategies are sensitively
faster than the ﬁrst and second. In comparison with ﬁgure 7, ﬁgure 10 shows the computational time for the same
hyperparameters values. This graph shows the increase of time as the number of nodes is increased, and the difference
in the speed of each strategy.

4.1 Discussion

As it can be seen on these graphs, the system is able to return good results in training and test. An interesting feature
is that the number of nodes needed to obtain these results is not very high (around 100 nodes), as opposed to other
systems like GSGP, which returns much more complex expressions. Table 4 shows a summary of the conﬁgurations
that returned the best training results for each strategy. As was expected, the best training results were obtained with the
most complex trees (highest number of nodes). This table also shows the test results returned by these conﬁgurations.
Even the training results show a great ability to ﬁt the training set, these conﬁgurations did not return the best results in
test. Table 5 shows the same information, but this time for the conﬁgurations that returned the best results in the test set.

However, in order to correctly measure the generalization ability of this system, it has been compared with one of the
most popular Machine Learning systems: Artiﬁcial Neural Networks (ANNs). An ANN with two hidden layers, with
10 neurons in the ﬁrst hidden layer and 3 neurons in the second hidden layer was used. This topology was chosen to
be complex enough in order to allow the ANN to ﬁt the training data at least as good as this system, and compare the
generalization abilities of both systems. An ADAM optimizer was used, with a learning rate of 0.01. The same 10-fold
training/test sets were used for the training of the ANN. However, since this process is not deterministic, for each set
the training process was repeated 50 times and averaged. In each training, 50000 epochs were done. Figure 11 shows
the average training process for the 10 folds. This graph shows the training and test results for each epoch. As it is

24

50100150200-10-8-6-4-2051015202530Maximum number of nodesMinimum MSE reduction (log10)50100150200-10-8-6-4-2051015202530Maximum number of nodesMinimum MSE reduction (log10)50100150200-10-8-6-4-2051015202530Maximum number of nodesMinimum MSE reduction (log10)50100150200-10-8-6-4-2051015202530Maximum number of nodesMinimum MSE reduction (log10)A PREPRINT - NOVEMBER 18, 2019

(a) Strategy 1

(b) Strategy 2

(c) Strategy 3

(d) Strategy 4

Figure 9: Average time for the housing problem

Figure 10: Average computational time for different maximum number of nodes values and a minimum MSE reduction
of 10−6

25

50100150200-10-8-6-4-2020406080100120140Maximum number of nodesMinimum MSE reduction (log10)50100150200-10-8-6-4-2020406080100120140Maximum number of nodesMinimum MSE reduction (log10)50100150200-10-8-6-4-2020406080100120140Maximum number of nodesMinimum MSE reduction (log10)50100150200-10-8-6-4-2020406080100120140Maximum number of nodesMinimum MSE reduction (log10)20406080100010203040Strategy 1Strategy 2Strategy 3Strategy 4Maximum number of nodesTimeTime with minimum reduction 1.0e-6A PREPRINT - NOVEMBER 18, 2019

Figure 11: Results of the MLP training process

usual, while the training results are still improving, the test results reach a point in which begin to worsen. Finally, the
training process is stopped when the maximum number of epochs is reached. In this point, the ANN has overﬁt the
training set, with a bad result in the test set. Table 5 shows a summary of the best results in the training set, reached
at epoch 49926, and in the test set, at epoch 4347. As it can be seen, the training result is much lower than the ones
obtained with the system described in this paper, because a complex topology was used, leading to having a complex
system. However, as table 4 suggests, better training results could be obtained with a larger number of nodes.

The test results of table 6 can be compared with the ones shown in table 5. To perform this comparison, a paired
two-tailored sample t-test was used between the best test results of each strategy and the best test results of the ANN. In
each of this four tests, a signiﬁcance level of α = 0.05 was used. As result, all of the four tests returned that the mean
results are not signiﬁcantly different. Therefore, the system described in this paper is able to return test results as good
as an ANN.

Another important hyperparameter was analysed: the minimum MSE improvement. From the resulting graphs, it is
clear that this parameter should not take very high values (10−1). However, there is no clear relation between this value
and the obtained results measured in MSE. However, lower values in this parameter make computational time much
longer. Joining these two considerations, values not very low (10−10) and not very low (10−1) should be given to this
parameter. Since computational time increases with lower minimum MSE reductions, but MSE in test does not seem to
signiﬁcatively improve, leaving a higher value to this value seems to be a good choice. For this reason, the selected
value was 10−6 for the four strategies.

Regarding the four strategies used for the experiments, the ﬁrst one returns the worst results, while being also the one
that takes the highest computational time. Therefore, performing all of the searches in all of the nodes does not seem to
be a good choice, since some of these searches are unlikely to be successful in many nodes. Strategy 2 returns better
results than strategy 1, which supports the idea that it is convenient to optimize the constants of the tree before making
any modiﬁcation to its structure. However, this strategy still takes too much time, because in each iteration all of the
searches are performed on all of the nodes. Finally, strategies 3 and 4 return the best results. As ﬁgure 10 shows, those
searches that use the constant optimization process (searches 2 and 3) take less time to perform the training than their
counterparts without constant optimization (searched 1 and 4 respectively).

Regarding the height of the trees, the graph with the tree height shows that the resulting trees are not balanced, and
therefore setting a tree height limit would not be a good alternative.

5 Conclusions

This work presents a novel technique for Symbolic Regression. In this ﬁeld, the most used technique is GP, which is
based on evolutionary processes. Thus, this ﬁeld had an important lack of mathematical-based methods. The technique
presented in this work allows the obtaining of mathematical expressions that can model an input-output relationship.

Also, the expressions obtained by this method can have a limit in complexity. This allows the obtaining of expressions
that can be easily analysed by humans, in contrast with other techniques such as GSGP, that return very large expressions.
The analysis of these expressions is usually one of the objectives of Symbolic Regression.

26

1×10⁴2×10⁴3×10⁴4×10⁴5×10⁴0102030TrainingTestEpochTest MSEA PREPRINT - NOVEMBER 18, 2019

Results on section 4 show that this technique can return good results in real-world problems. The results in the
conﬁgurations with a high number of nodes show a very small MSE. This shows the capacity of the system. However,
for generalization purposes, setting a limit on the complexity allows the obtaining expressions with good generalization.
Also, the computational time has been measured, and this system has shown to return good results in short time. The
strategy that returned the lowest MSE in test and fastest results only took some seconds to build the tree.

An additional advantage of this system used for Machine Learning purposes is that the returned model is an standard
equation and thus it can be used in any programming language with no need to import any ML library. Moreover, this
expression can be used in any system other than programming environments. For instance, it can be easily used in a
spreadsheet as opposed to other systems such as Neural Networks.

6 Future Works

This work opens a wide new research ﬁeld in Symbolic Regression. As was described throughout in the paper, many
research works are still to be done by the research community. Some of the possible developments could be:

• In the constant search, ﬁnd an easy expression or method to compute the minimum of equation 6 in the case
when ci and di are vectors. One possibility could be to use gradient descent [26] to minimize this function.

• Perform a comparison of different search strategies in different problems to ﬁnd a strategy that behaves better

in most of them.

• A new constant optimization algorithm could be proposed. In the one described in this paper, each constant is
alternatively optimised in different iterations, which needs the computation of the equations of the correspond-
ing nodes in each iteration. An alternative could be to change the value of several (or all) constants in each
iteration, thus making this process faster.

• As the limit of the complexity of the tree has been proven to be an important factor, new ways of limiting this
complexity can be found. For instance, setting a limit to the number of sum, minus, multiplication or division
operations that can be used.

• Limiting the complexity of the models is a common way to avoid overﬁtting. However, other methods such
as the use of a validation set can also be used. This possibility could be explored, with the advantage of not
needing to set the limit of number of neurons or height of the tree.

• In order to obtain expressions even easier to be understandable by humans, information about the structure of
the desired expressions could be give. For instance, many times the desired expression is a division of two
expressions, with no other division performed in these two parts. This structure, as well as any other could be
given to the system. This could have as additional feature that the search process would be sped up.

• An interesting possibility could be to extend the variable search not only with variables but also with any
subtrees. Those parts of the tree that are going to be replaced with another could be stored in a structure like
a "node pool" and be used in the search later. The idea is that if once they were useful, they might become
useful again later, when the tree is modiﬁed.

Also, this work could also be a basis to new ways of combining models not being necessarily mathematical expressions.
Any model (for instance, an ANN) can be represented in the semantic space as one point, that could fall inside one of
the shapes. Therefore, the tree being developed could combine mathematical expressions with other type of models.
This could be easily done with a search similar to the variable search, that could be called "model search", and a
"constant-model search" In this sense, this technique would allow the building of ensembles of models.

Acknowledgments

The experiments described in this work were performed on computers in the Supercomputing Center of Galicia
(CESGA). Daniel Rivero and Enrique Fernández-Blanco would also like to thank the support provided by the NVIDIA
Research Grants Program.

Disclosure statement

No potential conﬂict of interest was reported by the authors

27

A PREPRINT - NOVEMBER 18, 2019

References

[1] Simon S. Haykin. Neural networks and learning machines. Pearson Education, Upper Saddle River, NJ, third

edition, 2009.

[2] John R. Koza. Genetic Programming: On the Programming of Computers by Means of Natural Selection. MIT

Press, Cambridge, MA, USA, 1992.

[3] Riccardo Poli, William B. Langdon, and Nicholas Freitag McPhee. A Field Guide to Genetic Programming. Lulu

Enterprises, UK Ltd, 2008.

[4] Basic Concepts of Linear Genetic Programming, pages 13–34. Springer US, Boston, MA, 2007.

[5] T. Perkis. Stack-based genetic programming. In Proceedings of the First IEEE Conference on Evolutionary

Computation. IEEE World Congress on Computational Intelligence, pages 148–153 vol.1, June 1994.

[6] Julian Miller and Andrew Turner. Cartesian genetic programming. In Proceedings of the Companion Publication
of the 2015 Annual Conference on Genetic and Evolutionary Computation, GECCO Companion ’15, pages
179–198, New York, NY, USA, 2015. ACM.

[7] Dennis G. Wilson, Julian F. Miller, Sylvain Cussat-Blanc, and Hervé Luga. Positional cartesian genetic program-

ming. CoRR, abs/1810.04119, 2018.

[8] Gabriel Kronberger, Stephan Winkler, Michael Affenzeller, and Stefan Wagner. On crossover success rate in
genetic programming with offspring selection. In Leonardo Vanneschi, Steven Gustafson, Alberto Moraglio,
Ivanoe De Falco, and Marc Ebner, editors, Genetic Programming, pages 232–243, Berlin, Heidelberg, 2009.
Springer Berlin Heidelberg.

[9] Krzysztof Krawiec. Medial crossovers for genetic programming. In Alberto Moraglio, Sara Silva, Krzysztof
Krawiec, Penousal Machado, and Carlos Cotta, editors, Genetic Programming, pages 61–72, Berlin, Heidelberg,
2012. Springer Berlin Heidelberg.

[10] T. McConaghy and G. G. E. Gielen. Template-free symbolic performance modeling of analog circuits via
canonical-form functions and genetic programming. IEEE Transactions on Computer-Aided Design of Integrated
Circuits and Systems, 28(8):1162–1175, Aug 2009.

[11] Juan Pérez, Mónica Miguélez Rico, Juan Rabuñal, and Fernando Abella. Applying genetic programming to civil

engineering in the improvement of models, codes and norms. pages 452–460, 10 2008.

[12] Alberto Moraglio, Krzysztof Krawiec, and Colin G. Johnson. Geometric semantic genetic programming. In Carlos
A. Coello Coello, Vincenzo Cutello, Kalyanmoy Deb, Stephanie Forrest, Giuseppe Nicosia, and Mario Pavone,
editors, Parallel Problem Solving from Nature - PPSN XII, pages 21–31, Berlin, Heidelberg, 2012. Springer Berlin
Heidelberg.

[13] Tomasz Pawlak. Geometric semantic genetic programming is overkill. volume 9594, 03 2016.

[14] Joao Francisco B. S. Martins, Luiz Otávio Vilas Boas Oliveira, Luis Fernando Miranda, Felipe Casadei, and
Gisele L. Pappa. Solving the exponential growth of symbolic regression trees in geometric semantic genetic
programming. CoRR, abs/1804.06808, 2018.

[15] James Mcdermott, Alexandros Agapitos, Anthony Brabazon, and Michael O’Neill. Geometric semantic genetic

programming for ﬁnancial data. pages 215–226, 04 2014.

[16] Z. Zhu, A. K. Nandi, and M. W. Aslam. Adapted geometric semantic genetic programming for diabetes and breast
cancer classiﬁcation. In 2013 IEEE International Workshop on Machine Learning for Signal Processing (MLSP),
pages 1–5, Sep. 2013.

[17] Trent Mcconaghy. FFX: Fast, Scalable, Deterministic Symbolic Regression Technology, pages 235–260. 01 2011.

[18] Q. Chen, B. Xue, and M. Zhang. Generalisation and domain adaptation in gp with gradient descent for symbolic

regression. In 2015 IEEE Congress on Evolutionary Computation (CEC), pages 1137–1144, May 2015.

[19] Gabriel Kronberger, Michael Kommenda, Andreas Promberger, and Falk Nickel. Predicting friction system
performance with symbolic regression and genetic programming with factor variables. pages 1278–1285, 07 2018.

[20] Philippe Flajolet and Andrew Odlyzko. The average height of binary trees and other simple trees. Journal of

Computer and System Sciences, 25(2):171 – 213, 1982.

[21] Joo Manuel Paiva Cardoso, Jos Gabriel de Figueiredo Coutinho, and Pedro C. Diniz. Embedded Computing
for High Performance: Efﬁcient Mapping of Computations Using Customization, Code Transformations and
Compilation. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1st edition, 2017.

28

A PREPRINT - NOVEMBER 18, 2019

[22] Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. Julia: A fresh approach to numerical computing.

SIAM review, 59(1):65–98, 2017.

[23] I. Newton. Philosophiae naturalis principia mathematica. J. Societatis Regiae ac Typis J. Streater, 1687.
[24] Otto R. Harrison and Daniel L. Rubinfeld. Hedonic prices and the demand for clean air. 1978.
[25] Sebastian Raschka. Python Machine Learning. Packt Publishing, Birmingham, UK, 2015.
[26] J.A. Snyman and D.N. Wilke. Practical Mathematical Optimization: Basic Optimization Theory and Gradient-

Based Algorithms. Springer Optimization and Its Applications. Springer International Publishing, 2018.

29

