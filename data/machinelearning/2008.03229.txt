Towards Sample Efﬁcient Agents through Algorithmic Alignment (Student
Abstract)

Mingxuan Li*, Michael L. Littman,
Department of Computer Science, Brown University, Providence, RI 02912-1910
mingxuan li@brown.edu, mlittman@cs.brown.edu

1
2
0
2

t
c
O
1
2

]
I

A
.
s
c
[

5
v
9
2
2
3
0
.
8
0
0
2
:
v
i
X
r
a

Abstract

In this work, we propose and explore Deep Graph Value
Network (DeepGV) as a promising method to work around
sample complexity in deep reinforcement-learning agents us-
ing a message-passing mechanism. The main idea is that the
agent should be guided by structured non-neural-network al-
gorithms like dynamic programming. According to recent ad-
vances in algorithmic alignment, neural networks with struc-
tured computation procedures can be trained efﬁciently. We
demonstrate the potential of graph neural network in support-
ing sample efﬁcient learning by showing that Deep Graph
Value Network can outperform unstructured baselines by a
large margin in solving Markov Decision Process (MDP).
We believe this would open up a new avenue for struc-
tured agents design. See https://github.com/drmeerkat/Deep-
Graph-Value-Network for the code.

Introduction

Deep reinforcement-learning algorithms have produced
breakthroughs in recent years. However, agents with pow-
erful non-linear function approximators also require large
amounts of experience to learn. In this work, the question
we focus on is how to get the best of both neural networks
and non-neural network algorithms (e.g. Dynamic Program-
ming) to learn effectively.

One intuitive solution is to execute those algorithms
with neural networks. Different datasets favor different net-
work structures and computational process. Recent work has
shown that better algorithmic alignment improves sample
complexity and generalization (Xu, Li et al. 2020). Given
the fact that GNNs align well with dynamic programming
(DP), we presume value iteration, as a probabilistic ver-
sion of Bellman-ford algorithm, should also be solvable
with GNNs. Thus, we propose Deep Graph Value Network
(DeepGV), a message-passing framework, to robustly solve
the given MDP. We empirically verify its effectiveness as a
general MDP solver and its potential towards building gen-
eral structured agents.

*Blog: https://mingxuan.me

Copyright © 2021, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Algorithmic Alignment
We adopt the theoretical framework of (Xu, Li et al. 2020).
In this section, we will brieﬂy review the basic deﬁnitions.
For more details, please refer to Xu, Li et al. (2020).
Deﬁnition 2.1. (PAC learning and sample complexity).
Fix an error parameter (cid:15) > 0 and failure probability δ ∈
(0, 1). Suppose {xi, yi}M
i=1 are i.i.d. samples from distribu-
tion D, and the data satisﬁes yi = g(xi) for some under-
lying function g. Let f = A (cid:0){xi, yi}M
(cid:1) be the function
generated by a learning algorithm A. Then, g is (M, (cid:15), δ)-
learnable with A if

i=1

Px∈D[||f (x) − g(x)|| ≤ (cid:15)] ≥ 1 − δ.

(1)

The sample complexity CA(g, (cid:15), δ) is

min{M |Px∈D[||f (x) − g(x)|| ≤ (cid:15)] ≥ 1 − δ}.

(2)

Deﬁnition 2.2. (Algorithmic alignment). Let g be a reason-
ing function and F a neural network with n modules fi. The
module functions g1, ..., gn generate g for F if, by replac-
ing fi with gi, the network simulates g. Then, F(M, (cid:15), δ)-
algorithmically aligns with g if (1) g1, ..., gn generate g and
(2) there are learning algorithms Ai for the fi such that
n · maxi CAi(gi, (cid:15), δ) ≤ M .

From the deﬁnition, it is clear that only if each module of
the neural network aligns well with the underlying function
modules can we reduce M to its minimum.

Deep Graph Value Network
Due to the space limitation, for general context of MDP
and graph neural network, please refer to Sutton and Barto
(2018); Battaglia, Hamrick et al. (2018). In each iteration
of message distribution and aggregation, every node ﬁrst
updates its embeddings according to information from its
neighbours and edges between them. After some iteration
of execution, we aggregate all the information and extract
answers from it with another network. As shown in Fig. 1,
comparing general GNN updates with value iteration, they
share quite similar underlying computational steps, which
can be simulated by neural networks easily.

The framework of Deep Graph Value Network is as fol-
lows. The network takes a complete graph G(V, E) as input
with different attributes assigned to nodes and edges. Each
node represents a state in the given MDP (cid:104)S, A, R, γ, T (cid:105).

 
 
 
 
 
 
Figure 1: An example of how generally GNNs align with
value iteration. Both GNNs and VI share the same loop
structure, which does not need to be learned. Thus, it would
be relatively simpler for GNNs than generic Multi-layer Per-
ceptrons (MLPs) to learn to execute value iteration.

Edges between nodes represent actions with rewards bind-
ing to them.

(a) Value ranking accuracy varying grid world size.

In iteration k+1, the node embedding h(k+1)

i

is calculated

(b) Training set size vs. test accuracy.

as follows,



fψ

CAT
a∈A

(cid:88)

(cid:16)

A(k+1)
i,j

r(i, a, j) + γh(k)

j

(cid:17)



 ,

(3)

j∈N (i)

where CAT(·) is a concatenate operator, r is the embedded
reward attributes attached to edges, γ is the discount fac-
tor and Ai,j is the transition probability learned by a self-
attention mechanism. The part inside the square brackets
calculates the Q-value for state i and the local aggregate
function fγ learns to execute max. The attention mechanism
is learned as follows,

A(k+1)
i,j

=

(cid:80)

a(k+1)
i,j

= fθ(h(k)

)

exp(e(k+1)
i,j
l∈N (i) exp(e(k+1)
, h(k)
).
i

i,l

i

,

)

(4)

(5)

After K iterations, we aggregate information from all the
nodes and pass them through an MLP to get value-ranking
predictions:

(cid:18)

ˆy = fω

CAT
i∈V

(cid:19)

.

h(K)
i

(6)

Experiments and Discussion
In this section, we apply our framework to solve MDPs and
compare its performance with that of vanilla MLPs. Intu-
itively, unlike DeepGV, MLP doesn’t align well with dy-
namic programming. Thus, we expect to see that DeepGV
can easily be trained and outperforms the MLP by a large
margin.

We evaluate the networks on two different types of MDPs.
The ﬁrst one is a slippery N × N grid world without obsta-
cles and ﬁre states. The second type has obstacles and ﬁre
states. We generate different instances of these two kinds
of MDPs and evaluate two algorithms on each. To solve a
given MDP, the input is randomly initialized state values,
state coordinates and the queried state coordinates. The ex-
pected output is the value rank of the queried state.

Figure 2: Test accuracy and sample complexity on two types
of grid world. In each sub-ﬁgure, the LHS result comes from
plain grid world while the RHS result comes from grid world
with traps. (a) MLP fails on 8 × 8 world while DeepGV per-
forms well. (b) As the training set size increases, DeepGV
achieves good performance with fewer training examples.

As shown in ﬁg. 2, when we ﬁx the training set size and
increase the task difﬁculty (the size of grid world), DeepGV
can still be trained to a satisfying level while MLP perfor-
mance drops severely in the 8 × 8 grid world. When we var-
ied the training set size and ﬁxed the size of the grid world
at 8 × 8, there is a clear gap between MLP and DeepGV
performance. These results empirically support Theorem 3.6
from (Xu, Li et al. 2020) that better algorithmic alignment
induces better sample complexity. Overall, our experiment
veriﬁes the intuition that DeepGV can be trained easily and
outperforms the MLP by a large margin.

In conclusion, we presented Deep Graph Value Network
(DeepGV), a graph network structure that can efﬁciently
learn to solve MDPs by executing a similar computational
procedure to that of value iteration. Our preliminary exper-
imental results show the performance gap between struc-
tured networks and unstructured networks in solving rela-
tional tasks suggesting the promising potential of message-
passing mechanisms in building structured RL agents. We
believe this opens an exciting new avenue for future work
on designing high-efﬁciency agents incorporating algorith-
mic prior.

References
Battaglia, P. W.; Hamrick, J. B.; et al. 2018. Relational in-
ductive biases, deep learning, and graph networks. CoRR
abs/1806.01261.
Sutton, R. S.; and Barto, A. G. 2018. Reinforcement learn-
ing: An introduction. MIT press.
Xu, K.; Li, J.; et al. 2020. What Can Neural Networks Rea-
son About? In ICLR.

