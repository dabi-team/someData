Less is More: Summary of Long Instructions is Better for Program
Synthesis

Kirby Kuznia

Swaroop Mishra

Mihir Parmar

Chitta Baral

Arizona State University,
{kkuznia, srmishr1, mihirparmar, chitta}@asu.edu

Abstract

Despite the success of large pre-trained lan-
guage models (LMs) such as Codex, they show
below-par performance on the larger and more
complicated programming related questions.
We show that LMs beneﬁt from the summa-
rized version of complicated questions. Our
ﬁndings show that superﬂuous information of-
ten present in problem description such as
human characters, background stories, names
(which are included to help humans in under-
standing a task) does not help models in un-
derstanding a task. To this extent, we cre-
ate a meta-dataset from the frequently used
APPS dataset for the program synthesis task.
Our meta-dataset consists of human and syn-
thesized summary of the long and complicated
programming questions. Experimental results
on Codex show that our proposed approach
outperforms baseline by 8.13% on an aver-
age in terms of strict accuracy. Our analy-
sis shows that summary signiﬁcantly improve
performance for introductory (9.86%) and in-
terview (11.48%) related programming ques-
tions. However, it shows improvement by a
small margin (∼ 2%) for competitive program-
ming questions, implying the scope for future
research direction.1

1

Introduction

Recently, large pre-trained LMs have been proven
pivotal in programming-related tasks (Wang et al.,
2021; Chen et al., 2021; Hendrycks et al., 2021;
Lu et al., 2021; Papineni et al., 2002). Program
synthesis aims to generate a code given the natural
language description of a problem. Programming
requirements in these problems vary in terms of
complexity from 3-5 line simple function to mul-
tiple functions that uses advanced data structures.
However, LMs such as Codex show below-par per-
formance on the long and complicated program-

ming questions. We observe that the natural lan-
guage description of the program becomes long
and complicated when there is superﬂuous infor-
mation (see section 2.1.1 for a speciﬁc example).
The goal of adding this information to the descrip-
tion is to make it more understandable to humans.
However, we ﬁnd that this information confuses
model in understanding a task2. We propose that
removing the excess information and providing the
model with the exact speciﬁcations of the problem
can improve the performance of the LMs (Mishra
et al., 2021a).

To remove excess information3, we summarize the
descriptions of the program in such a way that it
does not lose important speciﬁcations. We use the
APPS dataset (Hendrycks et al., 2021) which is a
collection of coding problems from different on-
line sources and create a meta-dataset consisting of
human and synthesized summaries of the long and
complicated programming questions. This dataset
has 3 levels of difﬁculty similar to APPS: (1) intro-
ductory, (2) interview, and (3) competition.

We perform all experiments using the GPT-based
Codex model on the proposed meta-dataset and
show that the summarized version of complicated
questions improves (8.13%) performance on pro-
gram synthesis tasks. Our analysis shows that
summaries signiﬁcantly improve performance for
introductory (9.86%) and interview (11.48%) re-
lated programming questions. However, it shows
improvement by small margin (∼ 2%) for com-
petitive programming questions. Considering that
automatic evaluation of program does not reward
for partial correctness, we perform qualitative eval-
uation on our meta-dataset and ﬁnd that original
questions often confuse models in understanding
the underlying problem, as models latch on to some
spurious words in text (e.g. the word ‘list’ in ques-

1Code and data is available at https://github.com/kurbster/

Prompt-Summarization

2See example in Appendix A
3Instructions for creating summaries given in Appendix L

2
2
0
2

r
a

M
6
1

]
L
C
.
s
c
[

1
v
7
9
5
8
0
.
3
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
tion makes the model design a list even though
the underlying problem is on graphs ). We further
analyze model performance on different types of
summaries (i.e., basic, expert, and synthetic) and
provide instruction-design principles that can help
future research on prompting in program synthesis.

2 Method

2.1 Dataset

We use the APPS dataset (Hendrycks et al., 2021)
to create summaries. We randomly select prob-
lems from the APPS dataset for this purpose. We
create 414 human summaries and 7820 synthetic
summaries. Table 1 shows the statistics of the gen-
erated summaries.

Data Source Difﬁculty

# of Problems

Human

Studio21

GPT-3

Introductory
Interview
Competition

Total

Introductory
Interview
Competition

Total

Introductory
Interview
Competition

Total

145
123
105

373

1588
4551
1286

7425

194
267
244

705

Table 1: Statistics of the proposed meta-dataset.

2.1.1 Human Generated Summaries
For human-generated summaries, we ask computer
science students at Arizona State University (ASU)
to read and understand the original question, then
create summaries in two steps4. First, we ask them
to create a basic summary of the given problem.
They are instructed to remove any information that
is repeated and any hypothetical information with-
out concrete instructions. For example, if the prob-
lem construct a fake company or situation, to help
a human understand, we replace the fake situation
with direct instructions. Full example is included
in Appendix A. Second, we ask them to create an
expert summary of the problem. To create this,
they further summarize their ﬁrst summary. This
expert summary includes the absolute minimum in-
formation for an expert to understand the problem.
We would not expect a novice to understand these

4Instructions given to each student are in Appendix L

prompts. An example of expert summaries is given
in Appendix A.3.

2.1.2 Synthetic Summaries

We have generated synthetic summaries of pro-
gram descriptions using jumbo (178B), large (7.5B)
Studio21 model (Lieber et al., 2021) and GPT-3
Davinci model (175B) (Brown et al., 2020). To
generate a summary, we provide these models with
a few examples in the in-context learning setup
(Brown et al., 2020) from the human-generated
summaries. For the few-shot examples, we use
expert level summaries.

Studio21 We use ﬁve examples with the large
model, and three examples with jumbo model5.
For both models, we use a temperature of 0.3, and
topP of 1. For the format of our prompt, we use De-
Jargonizer template6 with a change to their header
as shown in Appendix B. We create a total of 7, 510
synthetic summaries using these models.

GPT-3 We use three examples for GPT-3 model.
We empirically set temperature to 0.05, topP to
1, frequency penalty to 0.01, presence penalty to
0.05. To generate prompts, we followed their tl;dr
template7 as shown in Appendix B. We create 500
synthetic summaries using this model. We do not
summarize any introductory problems with GPT-3
because we have limited resources and we want to
test the efﬁcacy of the model on harder problems.

2.2 Model
We use OpenAI Codex8 to build baselines and pro-
posed approach.

Baseline To create a baseline, we have used orig-
inal program descriptions given in APPS dataset as
prompts for the Codex model.

Proposed Approach In this approach, we have
used summaries of original program descriptions
given in the APPS dataset as prompts for the Codex
model.

3 Experimental Setup

All
the experiments are performed using the
davinci − codex (Chen et al., 2021) model pro-

5Examples are included in Appendix B
6https://studio.ai21.com/
7https://beta.openai.com/playground/p/
default-tldr-summary?model=text-davinci-001
8https://openai.com/blog/openai-codex/

Difﬁculty

Introductory
Interview
Competition

Weighted Average

Baseline

42.96
37.70
4.76

30.35

AP

Basic

50.00
41.80
5.71

Expert

Baseline

Expert

Baseline

50.00
44.26
5.71

44.20
36.52
4.00

30.31

51.82
46.96
6.00

43.23
37.70
4.76

30.43

34.69

35.50

36.65

37.22

EWPR

Basic

51.45
45.54
6.00

BWPR

Basic

50.35
41.80
5.71

Expert

50.35
44.26
5.71

34.78

35.59

Table 2: Results of baseline and proposed model in terms of Strict Accuracy (SAcc). All results are in %. AP:
All Problems, EWPR: Either Worst Problem Removal, BWPR: Both Worst Problem Removal (see explanation in
section 3).

Difﬁculty

Introductory
Interview
Competition

Weighted Average

AP

EWPR

Baseline

Proposed

Baseline

Proposed

42.96
37.70
4.76

30.35

52.82
49.18
6.67

38.48

44.53
38.66
4.81

31.11

54.74
50.42
6.73

39.44

Table 3: Results when taking the best summary for each
problem. Further explained in 4.1. The EWPR baseline
is different from 2 because a different set of problems
have been removed.

vided through OpenAI9. At inference time, we use
a modiﬁed version of the evaluation code10 pro-
vided by Hendrycks et al. (2021). This evaluation
code has four different outputs for each test case:
(1) -2: the code has a syntax error and can not run,
(2) -1: the code is syntactically correct but has a
run time error, (3) 0: the code run without any er-
rors but fail the test case, and (4) 1: the code run
without any error and pass the test case. Similar
to (Chen et al., 2021), we implement a timeout for
the code at inference time. If a test case takes more
than 4 seconds to run then we throw an exception
and count that test case as a −1.

Experiments To show effectiveness of the pro-
posed approach, we have performed three different
experiments using human generated summaries:

1. All problems from basic and expert sum-
maries are used at inference time. We term
this experiment as All Problems (AP).

2. We eliminate problems that perform worst11
in the ﬁrst experiment for either basic or ex-
pert summaries. We term this experiment as
Either Worst Problem Removal (EWPR).

9See the implementation and parameter details in Ap-

pendix D

10https://github.com/hendrycks/apps/blob/main/eval/test_

one_solution.py

11Deﬁnition of the worst problem is given in Appendix E

3. We eliminate problems that perform worst in
the ﬁrst experiment for both basic and expert
summaries. We term this experiment as Both
Worst Problem Removal (BWPR).

We conduct ﬁrst and second experiments using syn-
thetic summaries as well. Since synthetic sum-
maries generated by GPT-3 and Studio21 are for
different problem, we do not perform third experi-
ment.

Metric
In (Austin et al., 2021a), they show that
the BLEU metric (Papineni et al., 2002) does not
correlate well with synthesis performance. Thus,
we use the Strict Accuracy (SAcc) as our evaluation
metric for all experiments (see Appendix C).

4 Results and Analysis

4.1 Results for Human Generated

Summaries

Table 2 shows the results for baseline, human gen-
erated basic, and expert summaries evaluated on
Codex models in terms of SAcc. From the Table
2, we can observe that both the summary-based
models show on an average superior performance
(∼ 6%) compared to baseline. In particular, when
calculating results for every problem, basic and ex-
pert summary-based models outperform baseline
by 5.01%, and 5.74% respectively. Further analy-
sis shows that expert summary-based model shows
improved performance by ∼ 1% compared to the
basic summary-based model.

Our analysis shows that many problems where the
basic summary would fail, however, the expert sum-
mary would succeed and vice-versa. Thus, we
choose the best summary for each problem after
evaluating both summaries and calculate the results
for the best summaries. Table 3 shows results when
taking the best summary for each problem. We
observe a 9.86%, 11.48%, and 1.91% increase on

Model

Difﬁculty

AP

EWPR

Baseline

Proposed

Baseline

Proposed

GPT-3

Introductory
Interview
Competition

Weighted Average

Studio21

Introductory
Interview
Competition

Weighted Average

41.75
20.30
2.87

20.17

39.53
12.28
1.67

11.53

38.66
18.80
3.28

18.89

31.63
11.00
1.21

9.66

41.11
18.18
2.73

19.14

39.04
10.57
1.38

10.61

41.67
20.66
3.64

20.55

36.36
12.37
1.38

10.98

Table 4: Results of baseline and proposed model (All results are in %). AP: All Problems, EWPR: Either Worst
Problem Removal. 704 summaries generated by GPT-3 and 1501 summaries generated by Studio21 used for
inference.

SAcc for introductory, interview, and competition
level problems, respectively.

4.2 Results for Synthetic Summaries

Table 4 shows the results for baseline, synthetic
summaries generated by GPT-3, and Studio21 in
terms of SAcc for two experiments. For AP ex-
periment, we can observe that the performance
of baseline outperforms synthetic summary-based
models. However, proposed model shows on an av-
erage similar performance compared to baseline for
EWPR experiment. Moreover, Appendix G shows
the results for top 500 and top 1000 summaries
from GPT-3 and Studio21, respectively.

4.3 Analysis

Why do competitive problems have less perfor-
mance compared to the baseline for expert sum-
maries? We can closely observe from the Table
2 that expert summary-based model shows on an
average similar performance compared to basic-
summary based model for introductory and inter-
view type questions, however, the performance
of expert summary-based model deprecates for
competition type of questions. We believe that
expert summaries remove important information
compared to basic summaries12 which might not af-
fect introductory or interview type problems where
concise information is enough. However, this in-
formation can be crucial to model when it comes
to competition-level problems.

Why eliminating the worst problems help?
From Tables 2 and 4, we can observe that EWPR
and BWPR have improved performance compared

12See examples in Appendix A and A.3

to AP for both human and synthetically generated
summaries. By analyzing the summarized worst
problems, we notice a difference in the summariza-
tion style which shows that these summaries are
outliers and do not match the distribution of the
other summaries. This can cause a problem in syn-
thesizing a good program since the model loses
important information. Hence, we believe that
eliminating the worst problems improves model
performance.

Is there any possible bias in the meta-dataset?
Given that our human-generated summaries are
crowd-sourced, there will be some bias in the
dataset. This is conﬁrmed by our analysis because
there was a speciﬁc group of problems that per-
form worse in both experiments. Some details that
are critical to one person can be trivial to others.
In the context of generating expert summaries, as-
sumptions about expert knowledge can vary. Thus,
different people will be biased towards excluding
more information while others will exclude less.
This bias causes drift in the dataset and hinders
the model’s performance. Similar to Mishra et al.
(2021b), we can provide a template for what is
expected from the crowd workers to reduce bias.

5 Related Work

In the past, there are several methods including
semantic parsing (Ge and Mooney, 2005), deduc-
tive approaches, enumerative and stochastic search,
and constraint solving which have gained atten-
tion for program synthesis (Gulwani et al., 2017).
With the advent of machine/deep learning, Balog
et al. (2016) introduced a neural network based
model for solving programming competition-style

problems. Devlin et al. (2017) used sequence-to-
sequence approach to do program synthesis. Fur-
thermore, Hendrycks et al. (2021) introduced the
APPS dataset for testing the accuracy of large LMs
on program synthesis. This dataset consists of
10, 000 different coding challenges gathered from
online websites such as Codewars.com, AtCoder,
Kattis, and Codeforces. These problems cover a
wide variety of programming topics and consists
of 3 different difﬁculty levels: (1) introductory, (2)
interview, and (3) competition. Hendrycks et al.
(2021) leveraged the GPT-Neo model (Black et al.,
2021) which they ﬁne-tune for this task using APPS
dataset. CodeT5 model (Wang et al., 2021) uti-
lizes many different training objectives. Recently,
Austin et al. (2021b) explore limitations of large
language models and propose two new benchmarks,
MBPP and MathQA-Python. The Codex model
(Chen et al., 2021) is an advanced code generation
model that powers GitHub’s Copilot.

6 Conclusion

This paper introduces a summarization-based ap-
proach for efﬁcient program synthesis. Experi-
mental results show that the proposed approach
improves the performance of the existing Codex
model by on an average ∼ 8% across various levels
of programming questions provided by the APPS
dataset. Further, this paper proposes a meta-dataset
consisting of ∼ 500 human-generated basic and ex-
pert level summaries as well as ∼ 8k synthetically
generated summaries by GPT-3 and Studio21; this
can be helpful for future research on writing better
instructions for the program synthesis task.

Acknowledgement

We thank CSE 576 NLP class students at Arizona
State University (ASU) for helping with the data
creation process.

References

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021a.
Program synthesis with large language models. arXiv
preprint arXiv:2108.07732.

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021b.
Program synthesis with large language models. arXiv
preprint arXiv:2108.07732.

Matej Balog, Alexander L Gaunt, Marc Brockschmidt,
Sebastian Nowozin, and Daniel Tarlow. 2016. Deep-
coder: Learning to write programs. arXiv preprint
arXiv:1611.01989.

Sid Black, Leo Gao, Phil Wang, Connor Leahy, and
Stella Biderman. 2021. Gpt-neo: Large scale autore-
If
gressive language modeling with mesh-tensorﬂow.
you use this software, please cite it using these meta-
data, 58.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877–1901.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri
Edwards, Yuri Burda, Nicholas Joseph, Greg Brock-
man, et al. 2021. Evaluating large language models
trained on code. arXiv preprint arXiv:2107.03374.

Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju,
Rishabh Singh, Abdel-rahman Mohamed, and Push-
meet Kohli. 2017. Robustﬁll: Neural program learn-
ing under noisy i/o. In International conference on ma-
chine learning, pages 990–998. PMLR.

Ruifang Ge and Raymond Mooney. 2005. A statisti-
cal semantic parser that integrates syntax and seman-
tics. In Proceedings of the Ninth Conference on Com-
putational Natural Language Learning (CoNLL-2005),
pages 9–16.

Sumit Gulwani, Oleksandr Polozov, Rishabh Singh,
et al. 2017. Program synthesis. Foundations and
Trends® in Programming Languages, 4(1-2):1–119.

Dan Hendrycks, Steven Basart, Saurav Kadavath, Man-
tas Mazeika, Akul Arora, Ethan Guo, Collin Burns,
Samir Puranik, Horace He, Dawn Song, et al. 2021.
Measuring coding challenge competence with apps.
arXiv preprint arXiv:2105.09938.

Opher Lieber, Or Sharir, Barak Lenz, and Yoav
Shoham. 2021. Jurassic-1: Technical details and evalu-
ation. White Paper. AI21 Labs.

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021.
Codexglue: A machine learning benchmark dataset for
arXiv preprint
code understanding and generation.
arXiv:2102.04664.

Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin
Choi, and Hannaneh Hajishirzi. 2021a. Reframing in-
structional prompts to gptk’s language. arXiv preprint
arXiv:2109.07830.

Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2021b. Cross-task generalization
via natural language crowdsourcing instructions. arXiv
preprint arXiv:2104.08773.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
annual meeting of the Association for Computational
Linguistics, pages 311–318.

Yue Wang, Weishi Wang, Shaﬁq Joty, and Steven CH
Hoi. 2021. Codet5: Identiﬁer-aware uniﬁed pre-trained
encoder-decoder models for code understanding and
generation. arXiv preprint arXiv:2109.00859.

A Example of removing fake information

A.1 Original Prompt

Codefortia is a small island country located some-
where in the West Paciﬁc. It consists of n settle-
ments connected by m bidirectional gravel roads.
Curiously enough, the beliefs of the inhabitants
require the time needed to pass each road to be
equal either to a or b seconds. It’s guaranteed that
one can go between any pair of settlements by
following a sequence of roads.

Codefortia was recently struck by the ﬁnancial
crisis. Therefore, the king decided to abandon
some of the roads so that:

it will be possible to travel between each pair of
cities using the remaining roads only, the sum of
times required to pass each remaining road will
be minimum possible (in other words, remaining
roads must form minimum spanning tree, using
the time to pass the road as its weight), among
all the plans minimizing the sum of times above,
the time required to travel between the king’s
residence (in settlement 1) and the parliament
house (in settlement p) using the remaining roads
only will be minimum possible.

The king, however, forgot where the parliament
house was. For each settlement p = 1, 2, . . . , n,
can you tell what is the minimum time required
to travel between the king’s residence and the
parliament house (located in settlement p) after
some roads are abandoned?

—–Input—–

The ﬁrst line of the input contains four integers
n, m, a and b (2 ≤ n ≤ 70, n − 1 ≤ m ≤ 200,
1 ≤ a < b ≤ 107) — the number of settlements
and gravel roads in Codefortia, and two possible
travel times. Each of the following lines contains
three integers u, v, c (1 ≤ u, v ≤ n, u (cid:54)= v,
c ∈ {a, b}) denoting a single gravel road between
the settlements u and v, which requires c minutes
to travel.

You can assume that the road network is con-
nected and has no loops or multiedges.

—–Output—–

Output a single line containing n integers. The
p-th of them should denote the minimum possi-
ble time required to travel from 1 to p after the
selected roads are abandoned. Note that for each
p you can abandon a different set of roads.

—–Examples—–

Input
5 5 20 25
1 2 25
2 3 25
3 4 20
4 5 20
5 1 20

Output
0 25 60 40 20

Input
6 7 13 22
1 2 13
2 3 13
1 4 22
3 4 13
4 5 13
5 6 13
6 1 13

Output
0 13 26 39 26 13

—–Note—–

The minimum possible sum of times required to
pass each road in the ﬁrst example is 85 — exactly
one of the roads with passing time 25 must be
abandoned. Note that after one of these roads is
abandoned, it’s now impossible to travel between
settlements 1 and 3 in time 50.

We can see the author of the problem is trying to
describe a fully-connected graph with n nodes and
m edges each with a weight a or b. Thus, this
paragraph can be summarized as:

A.2 Basic Summary

You are given a graph of n nodes and m bidirec-
tional edges. The cost for each edge is either a or
b. The graph is fully-connected, so you can travel
between any pair of nodes using the edges.

For each node p = 1, 2, . . . , n, you need to re-
move some edges so that: It will be possible to
travel between each pair of nodes using the re-
maining edges only, and the sum of times required
to pass each remaining road will be the minimum
possible. You should output the minimum time
required to travel between node 1 and node p.

—–Input—–

The ﬁrst line of the input contains four integers
n, m, a and b (2 ≤ n ≤ 70, n − 1 ≤ m ≤ 200,
1 ≤ a < b ≤ 107) — the number of nodes
and edges in the graph, and two possible travel
times. Each of the following lines contains three
integers u, v, c (1 ≤ u, v ≤ n, u (cid:54)= v, c ∈
{a, b}) denoting an edge between the nodes u
and v, which has cost c.

You can assume that the graph is connected and
has no loops or multiedges.

—–Output—–

Output a single line containing n integers. The
p-th of them should denote the minimum possi-
ble post required to travel from 1 to p after the
selected edges are abandoned. Note that for each
p you can abandon a different set of edges.

—–Examples—–

Input
5 5 20 25
1 2 25

2 3 25
3 4 20
4 5 20
5 1 20

Output
0 25 60 40 20

Input
6 7 13 22
1 2 13
2 3 13
1 4 22
3 4 13
4 5 13
5 6 13
6 1 13

Output
0 13 26 39 26 13

A.3 Expert Summary

However, we can assume that an expert would
already know what a minimum spanning tree is.
Thus, we can remove this detailed description of
an MST.

You are given a connected graph of n nodes
and m bidirectional edges. For each node p =
1, 2, . . . , n, you need to ﬁnd a minimum spanning
tree. Then output the minimum cost required to
travel between node 1 and node p.

—–Input—–

The ﬁrst line of the input contains four integers
n, m, a and b (2 ≤ n ≤ 70, n − 1 ≤ m ≤ 200,
1 ≤ a < b ≤ 107) — the number of nodes
and edges in the graph, and two possible travel
times. Each of the following lines contains three
integers u, v, c (1 ≤ u, v ≤ n, u (cid:54)= v, c ∈
{a, b}) denoting an edge between the nodes u
and v, which has cost c.

You can assume that the graph is connected and
has no loops or multiedges.

—–Output—–

Output a single line containing n integers. The
p-th of them should denote the minimum possi-
ble post required to travel from 1 to p after the
selected edges are abandoned. Note that for each
p you can abandon a different set of edges.

—–Examples—–

Input
5 5 20 25
1 2 25
2 3 25
3 4 20
4 5 20
5 1 20

Output
0 25 60 40 20

Input
6 7 13 22
1 2 13
2 3 13
1 4 22
3 4 13
4 5 13
5 6 13
6 1 13

Output
0 13 26 39 26 13

B Prompt templates

Studio21 Here is our template for Studio21.

The following sentences contain computer
science jargon. Rewrite them using simple words.
Jargon: <ORIGINAL>
Simple: <SUMMARY>

Jargon: <ORIGINAL>
Simple: <SUMMARY>

Jargon: <ORIGINAL>
Simple: <SUMMARY>

Jargon: <ORIGINAL>
Simple:

The few-shot examples were chosen randomly
from the human generated expert summaries. Here
is an example of the prompt:

GPT3 Here is our template for GPT3.

Summarize the following paragraph: Original:
<ORIGINAL>
Summary: <SUMMARY>

Original: <ORIGINAL>
Summary: <SUMMARY>

Original: <ORIGINAL>
Summary: <SUMMARY>

Original: <ORIGINAL> Summary:

Codex Here is our default template for Codex,
which is used when there is no starter code pro-
vided. When there is starter code provided the
docstring remains the same but the code after the
doc string will be what is provided.

Python3
"""
<PROBLEM DESCRIPTION>
"""
def code():

C Strict Accuracy

Summary

Difﬁculty

Strict Accuracy (SAcc) is the percentage of prob-
lems that passed every test case. The formula to
calculate SAcc is given below:

Basic

Expert

strict acc :=

problems with 100% accuracy
total number of problems

(1)

StudioAI21

Introductory
Interview
Competition

Introductory
Interview
Competition

Introductory
Interview
Competition

Introductory
Interview
Competition

AP

145
123
105

145
123
105

215
627
659

194
266
244

EWPR

BWPR

141
113
100

140
116
100

187
558
578

180
242
220

144
123
105

144
123
105

-
-
-

-
-
-

GPT3

Table 5: These are the numbers of problems in each
split of the dataset. For GPT and Studio21 we did not
look at problems that were worse or same for both ex-
periments because there was insigniﬁcant overlap be-
tween the two experiments.

our initial experiment, this was the amount of prob-
lems we tested for each model. However, in our
ﬁnal experiment we changed our conﬁgurations
and generated more problems. For a comparison,
we took the top performing summaries and and
reported those results.

H Generated Code

In ﬁgure 1 is the code that was generated for
the example mentioned in Appendix A and A.3.
Given that the Codex model was prompted with
the def code() : the model did not generate that
function deﬁnition or the call to that function. That
was added in afterwards, but everything inside that
function was generated by Codex. The originally
generated code (far left) fails with a −1 because it
did not take in the input correctly. It added in an-
other line p = int(input()), which most likely refers
to the p mentioned in the original text. The expert
summary generated code (middle) fails every test
case. The basic summary generated code (right)
passed 16/19 (84%) test cases and was the only
code to pass at least 1 test case.

I StudioAI21 Generated Code

Below is an example of a competition problem
where StudioAI21 summarized the prompt too
much but Codex was still able to produce viable
code. Here is the original prompt:

Given that, we are only generating one code solu-
tion for each problem our strict accuracy is compa-
rable to (Chen et al., 2021)’s metric raw pass@1.

D Codex Conﬁguration

We did a small test with 75 summaries to ﬁnd our
hyper-parameters for Codex. We set temperature to
0, topP to 1, frequency penalty to 0.2, and presence
penalty to 0. We did not provide few-shot examples
to Codex since we want to see if summarization
only could improve the performance of the Codex
model.

E Worst Problems and Statistics

Using the test case labels as deﬁned in section 3
we deﬁned a test case as getting worse if it’s la-
bel (result) was lower. Then we deﬁned a problem
as worse if every test case had a lower label. Our
methodology behind this was, if we removed prob-
lems that had a worse accuracy, then it would be a
non-trivial result that accuracy improved. Also, if
we removed problems with worse accuracy, then a
problem that originally had all 0 labels (all False
test cases) would score the same if the summary
had all −1 labels (runtime error) or a −2 (syntax
error). So, we removed problems which every test
case performed worse, to see if removing these
outliers would improve results. You can see the
overall breakdown of each split in table 5.

F Average length of Problems and

Solutions

Table 6 represents the statistics for average length
of problems and solutions for original and summa-
rized prompts.

G Abbreviated Synthetic Results

In table 7, we show the results for our synthetic
summaries when taking the top 500 and 1000 sum-
maries for GPT3 and StudioAI21, respectively. In

Cengiz recently learned Fibonacci numbers and
now he is studying different algorithms to ﬁnd
them. After getting bored of reading them, he
came with his own new type of numbers that he

Experiment Original Len

Summary Len Orig Code Len

Summary Code Len

Code Solution Len

Summary

Expert

GPT

1147

1147

1386

StudioAI21

1646

937

869

1011

1114

339

339

437

602

349

343

392

473

671

671

748

721

Table 6: The average length of the original/summarized prompt and generated code. The average length of the
code solutions is the average len of the solutions provided by the creators of the APPS dataset. A problem could
have one or multiple solutions. The length is reported in characters.

Figure 1: On the far left is the code generated by the original prompt. The middle is the code generated by the
expert summary. The right is the code generated by the basic summary.

Model

Difﬁculty

AP

EWPR

Baseline

Proposed

Baseline

Proposed

GPT-3

Introductory
Interview
Competition

Weighted Average

Studio21

Introductory
Interview
Competition

Weighted Average

41.97
25.27
4.80

26.60

39.91
15.97
2.57

16.90

38.86
27.47
6.40

26.60

31.92
14.50
2.57

14.50

41.11
24.86
4.88

25.83

39.25
13.23
2.71

15.10

41.67
28.25
6.50

27.71

36.56
15.47
2.71

15.64

Table 7: Results when taking the top 500 GPT prob-
lems and top 1000 Studio problems

named XORinacci numbers. He deﬁned them as
follows: f (0) = a; f (1) = b; f (n) = f (n −
1) ⊕ f (n − 2) when n > 1, where ⊕ denotes the
bitwise XOR operation.

You are given three integers a, b, and n, calculate
f (n).

You have to answer for T independent test cases.

respectively.

—–Output—–

For each test case, output f (n).

—–Example—–
Input
3
3 4 2
4 5 0
325 265 1231232

Output
7
4
76

—–Note—–

In the ﬁrst example, f (2) = f (0) ⊕ f (1) =
3 ⊕ 4 = 7.

—–Input—–

Here is the summary that StudioAI21 generated:

The input contains one or more independent test
cases.

The ﬁrst line of input contains a single integer T
(1 ≤ T ≤ 103), the number of test cases.

You are given three integers a, b, and n. Calculate
f (n).

—–Input—–

Each of the T following lines contains three space-
separated integers a, b, and n (0 ≤ a, b, n ≤ 109)

The input contains one or more independent test
cases.

The ﬁrst line of input contains a single integer T
(1 ≤ T ≤ 103), the number of test cases.

Each of the T following lines contains three space-
separated integers a, b, and n (0 ≤ a, b, n ≤ 109)
respectively.

—–Output—–

For each test case, output f (n).

—–Example—–
Input
3
3 4 2
4 5 0
325 265 1231232

Output
7
4
76

—–Note—–

In the ﬁrst example, f (2) = f (0) ⊕ f (1) =
3 ⊕ 4 = 7.

Because any input/output examples provided by
the prompt are appended to the summary, Codex
was able to ﬁgure out the pattern in the problem
and generate code that was almost correct. In ﬁgure
2, the solution (left) used the pattern in the problem
and simplify by taking n mod 3. The Studio21
summary code (right) recognizes this pattern but
erroneously does not take the modulus of the num-
ber. The original code (center) also makes the same
mistake by not taking the modulus, but also brute
forces the answer. This shows that the model did
not recognize the pattern in this problem because
of the superﬂuous details. Even though Studio21
might have summarized too much, the model was
still able to make an improvement and understand
the pattern in the problem more.

J StudioAI21 Generated Code

Here is an example of a summary made by Stu-
dioAI21 where the qualitative aspect of the code
but it still failed. Here is the original prompt:

Given is a tree G with N vertices. The vertices
are numbered 1 through N, and the i-th edge con-
nects Vertex ai and Vertex bi. Consider painting
the edges in G with some number of colors. We
want to paint them so that, for each vertex, the
colors of the edges incident to that vertex are all
different. Among the colorings satisfying the con-
dition above, construct one that uses the minimum
number of colors.

—–Constraints—–
- 2 ≤ N ≤ 105

- 1 ≤ ai < bi ≤ N
- All values in input are integers.
- The given graph is a tree.

in the

is given from Standard Input

—–Input—–
Input
following format:
N
a1 b1
a2 b2
...
aN −1 bN −1

—–Output—–
Print N lines.
The ﬁrst line should contain K, the number of
colors used.
The (i+1)-th line (1 ≤ i ≤ N − 1) should contain
ci, the integer representing the color of the i-th
edge, where 1 ≤ ci ≤ K must hold.
If there are multiple colorings with the minimum
number of colors that satisfy the condition, print-
ing any of them will be accepted.

—–Sample Input—–
3
1 2
2 3

—–Sample Output—–
2
1
2

Here is the summary that StudioAI21 generated:

You are given a tree with N vertices numbered
from 1 to N. You need to ﬁnd the minimum
possible number of colors to paint the edges
such that vertices are painted with different color.
—–Constraints—–
- 2 ≤ N ≤ 105
- 1 ≤ ai < bi ≤ N
- All values in input are integers.
- The given graph is a tree.

in the

is given from Standard Input

—–Input—–
Input
following format:
N
a1 b1
a2 b2
...
aN −1 bN −1

—–Output—–
Print N lines.
The ﬁrst line should contain K, the number of
colors used.
The (i+1)-th line (1 ≤ i ≤ N − 1) should contain
ci, the integer representing the color of the i-th
edge, where 1 ≤ ci ≤ K must hold.
If there are multiple colorings with the minimum

Figure 2: The left is the code generated using the original prompt. The right is the code generated when using the
StudioAI21 generated summary.

number of colors that satisfy the condition, print-
ing any of them will be accepted.

—–Sample Input—–
3
1 2
2 3

—–Sample Output—–
2
1
2

In 3 the left is the original solution which fails
with a −2 because the runtime of the algorithm is
exponential. Note that it tries to create a list of all
possible edge colorings which is O(2N ). The right
is the code produced when using the StudioAI21
summary. You can see that this code is much closer
to solving the problem and produces an efﬁcient
algorithm. However, this fails with a −2 because
it tries to print the sum of a boolean (near the end
before the last for loop). Which fails in python
because a bool is not iterable.

Here is a problem where StudioAI21’s summary in-
creased the accuracy to 100%. Here is the original
prompt:

Polycarpus has a sequence, consisting of n non-
negative integers: a1, a2, ..., an.

Let’s deﬁne function f(l, r) (l, r are integer, 1 ≤
l ≤ r ≤ n) for sequence a as an operation of bit-
wise OR of all the sequence elements with indexes
from l to r. Formally: f(l, r) = al|al + 1|...|ar.

Polycarpus took a piece of paper and wrote out
the values of function f(l, r) for all l, r (l, r are
integer, 1 ≤ l ≤ r ≤ n). Now he wants to know,
how many distinct values he’s got in the end.

Help Polycarpus, count the number of distinct
values of function f(l, r) for the given sequence a.

Expression x|y means applying the operation of
bitwise OR to numbers x and y. This operation
exists in all modern programming languages, for
example, in language C++ and Java it is marked
as "|", in Pascal — as "or".

—–Input—–

The ﬁrst line contains integer n (1 ≤ n ≤ 105)
— the number of elements of sequence a. The
second line contains n space-separated integers
a1, a2, ..., an (0 ≤ ai ≤ 106) — the elements of
sequence a.

—–Output—–

Print a single integer — the number of distinct
values of function f(l, r) for the given sequence a.

Please, do not use the lld speciﬁer to read or write
64-bit integers in C++. It is preferred to use cin,
cout streams or the I64d speciﬁer.

—–Examples—–
Input
3
1 2 0

Output
4
Input
10
1 2 3 4 5 6 1 2 9 10

Output
11

—–Note—–

In the ﬁrst test case Polycarpus will have 6 num-
bers written on the paper: f(1, 1) = 1, f(1, 2) =
3, f(1, 3) = 3, f(2, 2) = 2, f(2, 3) = 2, f(3, 3) = 0.
There are exactly 4 distinct numbers among them:
0, 1, 2, 3.

Here is the summary that StudioAI21 generated.
Not exactly as we would expect as the prompt still
mentions the ﬁctional Polycarpus.

Polycarpus has a sequence, consisting of n non-
negative integers: a1, a2, ..., an. Let’s deﬁne
function f(l, r) (l, r are integer, 1 l r n) for se-
quence a as an operation of bitwise OR of all
the sequence elements with indexes from l to r.
Formally: f(l, r) = al|al + 1|...

—–Input—–

The ﬁrst line contains integer n (1 ≤ n ≤ 105)
— the number of elements of sequence a. The
second line contains n space-separated integers

Figure 3: The left is the code generated using the original prompt. The right is the code generated when using the
StudioAI21 generated summary.

a1, a2, ..., an (0 ≤ ai ≤ 106) — the elements of
sequence a.

—–Output—–

Print a single integer — the number of distinct
values of function f(l, r) for the given sequence a.

Please, do not use the lld speciﬁer to read or write
64-bit integers in C++. It is preferred to use cin,
cout streams or the I64d speciﬁer.

—–Examples—–
Input
3
1 2 0

Output
4
Input
10
1 2 3 4 5 6 1 2 9 10

Output
11

—–Note—–

In the ﬁrst test case Polycarpus will have 6 num-
bers written on the paper: f(1, 1) = 1, f(1, 2) =
3, f(1, 3) = 3, f(2, 2) = 2, f(2, 3) = 2, f(3, 3) = 0.
There are exactly 4 distinct numbers among them:
0, 1, 2, 3.

In 4 the left is the original solution which gets 77%
accuracy. The right is the summary code which
gets 100% accuracy.

K GPT Generated Summaries

Here are two summaries where GPT perfectly sum-
marized the prompt and gave a concise description
of what the task was. In both cases the original
prompt did not have 100% accuracy but the sum-
marized prompt did have 100% accuracy.

Here is the original prompt for the ﬁrst question:

Božo is a strange little boy. Every day he tires his
friends with strange questions. Today’s question
is: how many integers in the interval [A, B] are
there such that the sum of their digits is S, and
which is the smallest such number?

Write a program that answers Božo’s question so
that he can get some sleep.

—–Input—–
The input contains three integers A, B and S
(1 ≤ A ≤ B < 1015, 1 ≤ S ≤ 135).

—–Output—–
The ﬁrst line should contain the number of inte-
gers in the interval with the digit sum equal to
S.

The second line should contain the smallest such
integer.

The input data will guarantee that the ﬁrst number
is at least 1.

—–Examples—–
Sample Input 1:
1 9 5
Sample Output 1:
1
5

Sample Input 2:
1 100 10
Sample Output 2:
9
19

Here is the summary that GPT Generated:

Write a program that calculates the smallest inte-
ger in the given interval whose sum of digits is
equal to the given sum.

—–Input—–
The input contains three integers A, B and S

Figure 4: The left is the code generated using the original prompt. The right is the code generated when using the
StudioAI21 generated summary.

(1 ≤ A ≤ B < 1015, 1 ≤ S ≤ 135).

of the robot.

—–Output—–
The ﬁrst line should contain the number of inte-
gers in the interval with the digit sum equal to
S.

The second line should contain the smallest such
integer.

The input data will guarantee that the ﬁrst number
is at least 1.

—–Examples—–
Sample Input 1:
1 9 5
Sample Output 1:
1
5

Sample Input 2:
1 100 10
Sample Output 2:
9
19

The second line contains two integers x2, y2
(−109 ≤ x2, y2 ≤ 109) — the ﬁnish position
of the robot.

—–Output—–

Print the only integer d — the minimal number of
steps to get the ﬁnish position.

—–Examples—–
Input
0 0
4 5

Output
5

Input
3 4
6 1

Output
3

—–Note—–

In 5 you can see the original code on the left and
the summary code on the right. There is a subtle
difference but it’s that difference that improved the
problem from 33% accuracy to 100%.

Here is the original prompt for another question.

Professor GukiZ makes a new robot. The robot
are in the point with coordinates (x1, y1) and
should go to the point (x2, y2). In a single step the
robot can change any of its coordinates (maybe
both of them) by one (decrease or increase). So
the robot can move in one of the 8 directions. Find
the minimal number of steps the robot should
make to get the ﬁnish position.

—–Input—–

line contains two integers x1, y1
The ﬁrst
(−109 ≤ x1, y1 ≤ 109) — the start position

In the ﬁrst example robot should increase both
of its coordinates by one four times, so it will be
in position (4, 4). After that robot should sim-
ply increase its y coordinate and get the ﬁnish
position.

In the second example robot should simultane-
ously increase x coordinate and decrease y coor-
dinate by one three times.

Here is the summary that GPT3 generated:

The robot can move in one of the 8 directions.
Find the minimal number of steps the robot should
make to get the ﬁnish position.

—–Input—–

line contains two integers x1, y1
The ﬁrst
(−109 ≤ x1, y1 ≤ 109) — the start position
of the robot.

Figure 5: The left is the code generated using the original prompt. The right is the code generated when using the
GPT3 generated summary.

The second line contains two integers x2, y2
(−109 ≤ x2, y2 ≤ 109) — the ﬁnish position
of the robot.

—–Output—–

Print the only integer d — the minimal number of
steps to get the ﬁnish position.

—–Examples—–
Input
0 0
4 5

Output
5

Input
3 4
6 1

Output
3

—–Note—–

In the ﬁrst example robot should increase both
of its coordinates by one four times, so it will be
in position (4, 4). After that robot should sim-
ply increase its y coordinate and get the ﬁnish
position.

In the second example robot should simultane-
ously increase x coordinate and decrease y coor-
dinate by one three times.

In 6 you can see the original code on the left and
the summary code on the right. There is a subtle
difference but it’s that difference that improved the
problem from 20% accuracy to 100%.

the summary.txt ﬁle then starting from the top of
the prompt follow the steps and remove words/lines
as necessary.

These are the rough steps for making a summary.
Following these steps will create the most consis-
tency in our dataset. However, you should summa-
rize as you see ﬁt. First, read through the prompt
and understand what it’s asking, then follow these
steps to help create a summary.

1. Directly state what is given in the problem.

• Most problems start by setting the scene,

to help humans understand.

• Start the problems by explicitly telling

the model what the input is.

• You are given . . .

2. Remove any notes given in the prompt.

• They are usually reemphasizing points,
which is redundant and not needed in the
summary.

• This includes the −N otes− section at

the bottom of the ﬁle.

• If there is pertinent information given
from a note, include it in the prompt with-
out describing it as a note.

L Human Generated Instructions

3. Remove any text in parenthesis.

The section below was given to each crowd worker
as instructions to follow when creating the regular
and expert summaries.

L.1 Summarization

Create a ﬁle called summary.txt this will con-
tain your summary of the prompt.
It’s recom-
mended that you copy the question.txt ﬁle into

• Most of the text in parenthesis is repeat-
ing the information that precede them.

• If the text in parenthesis provides more
context or information, then remove the
preceding text.

• Keep any parenthesis if it is describing
constraints, such as the minimum and

Figure 6: The left is the code generated using the original prompt. The right is the code generated when using the
GPT3 generated summary.

maximum values for the input etc...

4. Remove any made up people, places, things,

etc...

• These abstractions are made to help hu-
mans understand but confuse the model.

• The prompts often mention things like
Codef ortia or P olycarp, try to replace
these with the word you.

• Any text visualizing what the problem is

asking, should be removed.

5. If the Input or Output section reference an

abstraction they should be changed.

• Overall, these sections are ﬁne. How-
ever, if they mentioned something you re-
moved in the previous steps, they should
be changed to reﬂect that.

• If these sections repeat themselves re-

move any redundancies.

out? The difference between the original and ex-
pert summary, is the original summary may in-
clude something obvious, whereas the expert so-
lution should be the absolute bare minimum. To
create summary.txt you want to remove super-
ﬂuous details from the original prompt. To create
expert.txt you want to remove details that an ex-
pert would ﬁnd obvious, from the summary.

For example, in problem 2000 (which is compet-
itive difﬁculty) the summary mentions ’It will be
possible to travel between each pair of nodes . . . ,
and the sum of times . . . will be the minimum possi-
ble’. This process is describing a minimum span-
ning tree so you can just say ’Find a minimum
spanning tree’.

Also, if the prompt included an example and subse-
quent explanation, that should remain in the sum-
mary but should be removed from the expert sum-
mary. An expert already understands the problem
and does not need any extra explanation. You
should still keep the −Examples− section.

• In most cases these sections will be left

Takeaways

alone.

L.2 Expert Summary

Create a ﬁle called expert.txt this will contain
It’s recom-
an expert summary of the prompt.
mended that you copy the summary.txt ﬁle into
the expert.txt ﬁle then starting from the top of
the prompt remove words/lines as necessary. You
should aim for the expert prompt to be 2 − 4 lines.

Imagine you are describing the prompt to a se-
nior software engineer. What else could you trim

• Removing made up people, places, and things
from the prompt improved the quality of code
generated.

• The optimal summarization depends on the

difﬁculty of the problem.

• Synthetically generate summaries were close

to maintaining accuracy.

• With more rigorous instructions, human sum-
maries could be made with less noise which
would further improve synthetic summary

generation.

M Superﬂuous Information Confusing

the Model

Here is an example of an interview level string
problem where the original prompt got 0% and both
human generated summaries got 100% accuracy.
The question wants you to write code that will
return the number of unique character in the given
string.

M.1 Original Prompt

You have initially a string of N characters, de-
noted by A1,A2...AN. You have to print the size
of the largest subsequence of string A such that
all the characters in that subsequence are distinct
ie. no two characters in that subsequence should
be same.

A subsequence of string A is a sequence that can
be derived from A by deleting some elements
and without changing the order of the remaining
elements.

—–Input—– First line contains T, number of test-
cases. Each testcase consists of a single string in
one line. Each character of the string will be a
small alphabet(ie. ’a’ to ’z’).

—–Output—– For each testcase, print the required
answer in one line.

—–Constraints—–
- 1 ≤ T ≤ 10
- Subtask 1 (20 points):1 ≤ N ≤ 10
- Subtask 2 (80 points):1 ≤ N ≤ 105

—–Example—–
Input:
2
abc
aba

Output: 3
2

—–Explanation—– For ﬁrst testcase, the whole
string is a subsequence which has all distinct char-
acters.

In second testcase, the we can delete last or ﬁrst
’a’ to get the required subsequence.

M.2 Basic Summary

—–Output—– For each testcase, print the required
answer in one line.

—–Constraints—–
- 1 ≤ T ≤ 10
- Subtask 1 (20 points):1 ≤ N ≤ 10
- Subtask 2 (80 points):1 ≤ N ≤ 105

—–Example—–
Input:
2
abc
aba

Output: 3
2

M.3 Expert Summary

You have to remove duplicates and print the length
of unique characters of the given string.

—–Input—– First line contains T, number of test-
cases. Each testcase consists of a single string in
one line. Each character of the string will be a
small alphabet(ie. ’a’ to ’z’).

—–Output—– For each testcase, print the required
answer in one line.

—–Constraints—–
- 1 ≤ T ≤ 10
- Subtask 1 (20 points):1 ≤ N ≤ 10
- Subtask 2 (80 points):1 ≤ N ≤ 105

—–Example—–
Input:
2
abc
aba

Output: 3
2

M.4 Generated Code

The original code (left) does not accomplish the
task but rather prints the count of the most fre-
quent character. The model was unable to distin-
guish what the task was given the verbose prompt.
However, the basic and expert summaries make the
task clear and the model produces the same code.
Which properly solves the challenge.

You are given N string. You have to identify the
duplicates and print the length of the new string
as a combination of unique characters only.

—–Input—– First line contains T, number of test-
cases. Each testcase consists of a single string in
one line. Each character of the string will be a
small alphabet(ie. ’a’ to ’z’).

N Made Up Information Confusing the

Model

Here is an example of an interview level problem
where the original prompt got 0% and the expert
generated summary got 100% accuracy.

Figure 7: The left is the code generated by the original prompt. The middle is the code generated by the expert
summary. The right is the code generated by the basic summary.

—–Output:—–
For each test case, output in a single line answer
as displayed on the screen.

—–Constraints—–
- 1 ≤ T ≤ 106
- 1 ≤ N ≤ 106

—–Sample Input:—–
1
7

—–Sample Output:—–
21

N.1 Original Prompt

The chef was searching for his pen in the garage
but he found his old machine with a display
If some numbers
and some numbers on it.
entered then some different output occurs on the
display. Chef wants to crack the algorithm that
the machine is following. Example to identify the
pattern :

Input Output
9
5
1
2

36
10
0
1

—–Input:—–
- First-line will contain T , the number of test cases.
Then the test cases follow. - Each test case con-
tains a single line of input, N .

—–Output:—–
For each test case, output in a single line answer
as displayed on the screen.

—–Constraints—–
- 1 ≤ T ≤ 106
- 1 ≤ N ≤ 106

—–Sample Input:—–
1
7

—–Sample Output:—–
21

N.2 Expert Summary

Write a code to print the average of the multi-
plication of a given number N with N-1 integer.
1

—–Input:—–
- First-line will contain T , the number of test cases.
Then the test cases follow. - Each test case con-
tains a single line of input, N .

Figure 8: The left is the code generated by the expert summary. The right is the code generated by the original
prompt.

