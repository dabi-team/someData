2
2
0
2

g
u
A
8

]

G
L
.
s
c
[

1
v
7
8
2
4
0
.
8
0
2
2
:
v
i
X
r
a

Workshop Track - 1st Conference on Lifelong Learning Agents, 2022

CONTINUAL REINFORCEMENT LEARNING WITH TELLA

Neil Fendley1

Cash Costello1

Eric Nguyen1

Gino Perrotta1

Corey Lowman1

1Johns Hopkins University Applied Physics Lab
First.Last@jhuapl.edu
United States

ABSTRACT

Training reinforcement learning agents that continually learn across multiple environments is a chal-
lenging problem. This is made more difﬁcult by a lack of reproducible experiments and standard
metrics for comparing different continual learning approaches. To address this, we present TELLA,
a tool for the test and evaluation of lifelong learning agents. TELLA provides speciﬁed, reproducible
curricula to lifelong learning agents while logging detailed data for evaluation and standardized
analysis. Researchers can deﬁne and share their own curricula over various learning environments
or run against a curriculum created under the DARPA Lifelong Learning Machines (L2M) Program.

1

INTRODUCTION

In the last decade, reinforcement learning (RL) with deep neural networks has been successfully applied in a wide
variety of domains (Arulkumaran et al., 2017). In typical RL scenarios, the RL agent learns a single task, deﬁned as
a single Partially Observable Markov Decision Process (POMDP). However, it is more realistic that agents encounter
nonstationary tasks or may interact with multiple tasks. For example, an agent controlling an unmanned ground vehicle
(UGV) may encounter a novel weather condition (e.g., icy road at dusk) and deal with new states (lighting at dusk or
radar reﬂectance with ice) and new transitions (steering actions have a different effect on an icy road compared to a dry
road). Consequently, the ﬁeld of continual reinforcement learning has received increased research interest in the past
few years (Khetarpal et al., 2020; Parisi et al., 2019). Typically, continual learning indicates that the agent encounters
a sequence of tasks and has to master each task while maintaining performance on the previously-learned tasks; and
lifelong learning is indicates the agent is able to use the knowledge from previously-learned tasks to become a better
learner over time (learn new tasks better and faster). For purposes of this paper, we use continual learning as the more
general term encompassing both kinds of abilities.

The assessment of continual learning poses a challenge beyond that of developing the algorithm. Reinforcement
learning is extremely variable (Chan et al., 2020)(Agarwal et al., 2021), and careful evaluation is needed to assess
whether an agent can reliably and robustly learn a given task. The challenge is compounded when the agent has to
learn multiple tasks, as the results depend on the precise characteristics of the tasks, their presentation sequence, the
number of runs (i.e., the number of times the the sequence is repeated), and so on. This variability implies that, in order
to compare different RL agents, it is essential to exercise the agents on carefully controlled and replicable scenarios.

To address the above use case, we introduce an environment, TELLA1, test and evaluation of lifelong learning agents,
that provides a standardized evaluation framework for episodic lifelong reinforcement learning. TELLA is a framework
for the speciﬁcation and execution of experiments in lifelong reinforcement learning. It interfaces with existing col-
lections of environments using the OpenAI Gym API (Brockman et al., 2016), provides ﬂexible, standard structures
for the deﬁnition of RL agents and curricula, and enables calculation of standard metrics for continual and lifelong
learning. Using TELLA, researchers can create and share benchmarks by providing curricula as standardized Python
code which can be used to repeatably train and evaluate agents. This allows for replicable results and makes assess-
ing progress in continual learning algorithms feasible. Note that TELLA itself is not a common benchmark problem
(although several curricula are included in the package). It is a tool to aid in both the development and use of user-
speciﬁed benchmarks.

TELLA was developed to standardize evaluation and comparison of agents in the DARPA Lifelong Learning Machines
(L2M) program and has been successfully used to evaluate a variety of different continual RL agent architectures on a
common set of benchmark tasks and scenarios.

1 TELLA is available at https://github.com/lifelong-learning-systems/tella.

1

 
 
 
 
 
 
Workshop Track - 1st Conference on Lifelong Learning Agents, 2022

2 RELATED WORK

There have been several efforts to develop environments to facilitate such assessment. One line of effort is to deﬁne
benchmark problems, such as OpenAI’s Procgen Benchmark (Cobbe et al., 2019), Meta-World’s multi-task environ-
ments (Yu et al., 2019), the Arcade Learning Environment’s 50+ Atari games (Bellemare et al., 2013), or the speciﬁc
subset of six Atari games speciﬁed by Schwarz et al. (2018). However, benchmark problems are not sufﬁcient for
standardized evaluation, since experiments using these benchmarks may still differ signiﬁcantly in terms of the order
of tasks and the amount of interaction the agent is allowed to take in each task (i.e., the amount of data collected
for learning). Another line of effort has to been establish ground truth with standard metrics: ShinRL (Kitamura &
Yonetani, 2021) deﬁnes tasks with known optimal solutions (Q functions); CORA (Powers et al., 2021) includes a
set of benchmark tasks, standard scenarios and metrics. However, this line of effort does not address an extensible
way to assess black-box RL agents, using reproducible experiments with parameterized tasks, customizable scenarios
(curricula), multiple runs, logging and continual learning metrics.

We note that the recently-introduced Avalanche RL (Lucchesi et al., 2022) is a parallel effort with similar goals,
though there are differences of emphasis. In particular, TELLA allows ﬂexible evaluation scenarios and schedules, and
is agnostic to the internal structure and learning procedure of the agent (such as optimization or number of models)
and is thus better able to accommodate multi-model RL architectures.

3 DESIGN PRINCIPLES

To succeed as a benchmarking tool for continual reinforcement learning, TELLA meets several conceptual require-
ments: TELLA must be compatible with curricula and agents authored independently, it must be conﬁgurable to permit
varied agent and environment design, and it must execute experiments in a repeatable and efﬁciently scalable manner.

3.1 COMPATIBLE

TELLA deﬁnes a standard structure for agents and curricula which is designed for compatibility even when the agent
and curriculum are developed separately. This standardization is accomplished by requiring that agents implement a
speciﬁc interface that can be normally be achieved with a simple wrapper around the original agent code. Curricula
are speciﬁed as sequences of agent-environment interactions with all environments required to use the OpenAI Gym
API (Brockman et al., 2016) and share action and observation spaces. TELLA does not restrict the contents of an agent
implementation, but the agent must respond to and provide feedback for the curriculum event callbacks. This design
ensures that TELLA’s experiment execution method has the necessary connections to drive and record the interactions
between agent and curriculum.

3.2 CONFIGURABLE

TELLA aims for minimal restrictions on curricula and agents. Curricula can be deﬁned for any environments which
conform to the OpenAI Gym API. Interactions between agent and environment are managed by a method within
TELLA, rather than by the agent itself. While this re-frames some of the way an agent receives data, it does not funda-
mentally change the algorithm. To verify this, an example agent is provided with the TELLA library which organizes
the contents of the Stable Baselines3 (Rafﬁn et al., 2021) Proximal Policy Optimization (PPO) agent (Schulman et al.,
2017) into the TELLA callbacks. The original and reorganized agents perform identically.

3.3 REPEATABLE

Experiments in TELLA are constructed to permit exact repeatability when possible. Random number generator (RNG)
seed values are provided and logged for the agent and curriculum. Example curricula and agents are included to
demonstrate the use of these seeds.

3.4 SCALABLE

TELLA permits experiment acceleration by environment vectorization. Vectorization allows for an agent to interact
with multiple environments in parallel. In order to scale to curricula containing thousands of environments, TELLA
delays the construction of the environments within a curriculum until they are needed by an agent. Environments are
closed when they are not in use. This is useful for environments such as Unity and Starcraft which may have signiﬁcant
memory and runtime requirements.

2

Workshop Track - 1st Conference on Lifelong Learning Agents, 2022

Figure 1: TELLA pipeline diagram

Figure 2: Performance of an agent over a curriculum
of 6 custom Minigrid tasks generated with TELLA and
L2Metrics package

4

TELLA DESIGN

4.1 CURRICULA

Term
Curriculum
Block
Task Block
Task Variant

Deﬁnition
Sequence of Blocks
Sequence of Task Blocks
Sequence of Task Variants
An RL environment

In order to standardize the training and evaluation of reinforcement learning methods, we deﬁne a curriculum structure.
A curriculum consists of training blocks and evaluation blocks in sequence. Each block is a collection of tasks and
associated parameters within each task. Figure 2 shows a curriculum with 6 different tasks. Each of these tasks and
associated parameters, called task variants, deﬁne an environment to be interacted with by the agent.

For each interaction with the environment, the agent will be given the observations and must return the actions to use.
The environment will then return a reward, the new observation of the environment, and a done signal, shown in Figure
1. A done signal indicates an episode is ﬁnished and that the environment must be reset to the beginning.

A reinforcement learning agent will increase its performance the more data it has to train on, so comparing across
agents requires us to regulate interactions with the environment in some way. We do this by limiting exposure to
each task variant, either in the number of steps an agent is allowed to perform or the number of episodes that can be
completed. We provide support for both step and episode limiting because which is more relevant can be application
dependent. By limiting the interactions with the environment, we can calculate an agent’s sample efﬁciency, which is
the amount of samples required to reach the agent’s measured performance.

4.2 CREATING CURRICULA

In order to create a curriculum one must deﬁne the sequence of training and evaluation blocks. The block type is
indicated to the agent, and rewards are hidden from the agent during evaluation. Each block represents a sequence
of task variants. If the order of task variants within a block is chosen randomly, the seed for repeatable RNG will
be recorded. Each task variant deﬁnes an environment, any speciﬁc parameters needed to repeatably initialize that
environment, and a limit on the agent’s interaction as either steps or episodes.

We also provide an InterleavedEvalCurriculum, which interleaves the same evaluation block between each training
block. This enables users to specify the evaluation block once rather than duplicating it.

4.3

TELLA AGENT

The agent class requires implementing two methods, and a number of optional events that it can use to organize
functionality. The ﬁrst main method is the ‘choose actions’ method which receives a list of observations and must
return a list of actions. Each observation in the list corresponds to one environment, and, when vectorized environments
are used, there will be an observation for each environment. The second main method is the ‘receive transitions’

3

Workshop Track - 1st Conference on Lifelong Learning Agents, 2022

method, which allows the agent to receive the result of applying the actions to the environments. A transition is
deﬁned as a tuple of the observation, the action that was taken, the reward for that action, whether the episode is
complete, and the next observation after applying the action. ‘Receive transitions’ receives a list of transitions, with
one transition per environment. This interaction can be summarized as:

actions = agent.choose_actions(observations)
transitions = vec_env.step(actions)
agent.receive_transitions(transitions)

Other events the agent can use are related to events in the curriculum, and TELLA automatically calls these methods
on the agent at the appropriate time in the curriculum.

4.4

TELLA EXPERIMENT

An experiment in TELLA consists of a curriculum, an agent, and a number of times to run the agent through the
curriculum. Each single run of the agent through the curriculum is called a lifetime. TELLA will automatically
determine new seeds for each lifetime in the experiment.

For a single lifetime, TELLA will recursively iterate over every block, task block, and task variant, and run the inter-
action between the task variant’s environment and the agent. Once interaction with a task variant is complete, it will
move onto the next task variant or step up a level.

The output of a single lifetime is a directory that contains all the episode information from each training and evaluation
block. The ﬁles logged in the directory can be consumed by a framework such as l2metrics (New et al., 2022), to
produce lifelong learning metrics.

To make running experiments with an agent easy, TELLA includes a function to turn any valid TELLA agent into
a command line interface. This creates a ﬁle that allows you to specify arguments and start experiments from the
command line. Alternatively the experiment can be called directly if you know which curriculum you want to run.

5 CONCLUSION AND FUTURE WORK

Continual reinforcement learning is a research problem that requires an algorithm to learn across dynamic environ-
ments. Algorithms in this setting struggle to remember information from previous tasks and to learn new tasks efﬁ-
ciently. To complicate matters, benchmarking continual reinforcement learning algorithms is made more difﬁcult by a
lack of standardized procedures.

To address this difﬁculty in evaluating continual learning algorithms, TELLA standardizes the training and evaluation
of episodic continual learning algorithms using curricula. A curriculum is a sequence of training and evaluation
tasks with controlled access to training data generated by the environment. By using a curriculum, we can calculate
many metrics, including: performance maintenance, to measure catastrophic forgetting; sample efﬁciency, by limiting
the amount of data; and forward transfer, to measure how much an agent is retaining information from previous
environments. TELLA was developed as part of DARPA’s L2M program, and served as a framework for a common
evaluation of L2M lifelong learning agents.

However, there is still future work to be done, both within continual learning and TELLA. TELLA is focused on the task
of episodic learning, as opposed to continuous learning in one dynamic environment. TELLA also restricts all access
to data in the environment to be pre-speciﬁed in the curriculum. This means there is no online control for the agent to
adjust the future curriculum. TELLA could be extended to allow agents to control the future blocks in the curriculum,
which could potentially host active learning or early stopping experiments.

6 ACKNOWLEDGMENTS

This effort was funded by the DARPA Lifelong Learning Machines (L2M) Program. The views, opinions, and/or
ﬁndings expressed are those of the author(s) and should not be interpreted as representing the ofﬁcial views or policies
of the Department of Defense or the U.S. Government.

4

Workshop Track - 1st Conference on Lifelong Learning Agents, 2022

REFERENCES

Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C. Courville, and Marc G. Bellemare. Deep re-
inforcement learning at the edge of the statistical precipice. CoRR, abs/2108.13264, 2021. URL https:
//arxiv.org/abs/2108.13264.

Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. Deep Reinforcement Learning:
A Brief Survey. IEEE Signal Processing Magazine, 34(6):26–38, November 2017. ISSN 1053-5888. doi: 10.1109/
MSP.2017.2743240. URL http://ieeexplore.ieee.org/document/8103164/.

M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform

for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, jun 2013.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech

Zaremba. Openai gym, 2016.

Stephanie C. Y. Chan, Samuel Fishman, John Canny, Anoop Korattikara, and Sergio Guadarrama. Measuring the
Reliability of Reinforcement Learning Algorithms. arXiv:1912.05663 [cs, stat], February 2020. URL http:
//arxiv.org/abs/1912.05663. arXiv: 1912.05663.

Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark

reinforcement learning. CoRR, abs/1912.01588, 2019. URL http://arxiv.org/abs/1912.01588.

Khimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup. Towards Continual Reinforcement Learning: A
Review and Perspectives. arXiv:2012.13490 [cs], December 2020. URL http://arxiv.org/abs/2012.
13490. arXiv: 2012.13490.

Toshinori Kitamura and Ryo Yonetani. ShinRL: A Library for Evaluating RL Algorithms from Theoretical and Prac-
tical Perspectives. arXiv:2112.04123 [cs], December 2021. URL http://arxiv.org/abs/2112.04123.
arXiv: 2112.04123.

Nicol`o Lucchesi, Antonio Carta, Vincenzo Lomonaco, and Davide Bacciu. Avalanche RL: a Continual Reinforcement
Learning Library. arXiv:2202.13657 [cs], March 2022. URL http://arxiv.org/abs/2202.13657. arXiv:
2202.13657.

Alexander New, Megan Baker, Eric Nguyen, and Gautam Vallabha. Lifelong learning metrics. CoRR, abs/2201.08278,

2022. URL https://arxiv.org/abs/2201.08278.

German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter. Continual Lifelong Learning
ISSN 08936080. doi: 10.1016/j.

with Neural Networks: A Review. Neural Networks, 113:54–71, May 2019.
neunet.2019.01.012. URL http://arxiv.org/abs/1802.07569. arXiv: 1802.07569.

Sam Powers, Eliot Xing, Eric Kolve, Roozbeh Mottaghi, and Abhinav Gupta. CORA: Benchmarks, Baselines, and
Metrics as a Platform for Continual Reinforcement Learning Agents. arXiv:2110.10067 [cs], October 2021. URL
http://arxiv.org/abs/2110.10067. arXiv: 2110.10067.

Antonin Rafﬁn, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-
baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):
1–8, 2021. URL http://jmlr.org/papers/v22/20-1364.html.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization

algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347.

Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan
In Jennifer
Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for continual learning.
Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, vol-
ume 80 of Proceedings of Machine Learning Research, pp. 4528–4537. PMLR, 10–15 Jul 2018. URL https:
//proceedings.mlr.press/v80/schwarz18a.html.

Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-
In Conference on Robot

world: A benchmark and evaluation for multi-task and meta reinforcement learning.
Learning (CoRL), 2019. URL https://arxiv.org/abs/1910.10897.

5

