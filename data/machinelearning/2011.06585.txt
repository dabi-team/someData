0
2
0
2

v
o
N
2
1

]

G
L
.
s
c
[

1
v
5
8
5
6
0
.
1
1
0
2
:
v
i
X
r
a

Sparse PCA: Algorithms, Adversarial Perturbations and
Certiﬁcates

Tommaso d’Orsi∗

Pravesh K. Kothari†

Gleb Novikov‡

David Steurer§

November 13, 2020

Abstract

We study eﬃcient algorithms for Sparse PCA in standard statistical models (spiked co-
variance in its Wishart form). Our goal is to achieve optimal recovery guarantees while being
resilient to small perturbations. Despite a long history of prior works, including explicit studies
of perturbation resilience, the best known algorithmic guarantees for Sparse PCA are fragile
and break down under small adversarial perturbations.

We observe a basic connection between perturbation resilience and certifying algorithms that
are based on certiﬁcates of upper bounds on sparse eigenvalues of random matrices. In contrast
to other techniques, such certifying algorithms, including the brute-force maximum likelihood
estimator, are automatically robust against small adversarial perturbation.

We use this connection to obtain the ﬁrst polynomial-time algorithms for this problem that
are resilient against additive adversarial perturbations by obtaining new eﬃcient certiﬁcates for
upper bounds on sparse eigenvalues of random matrices. Our algorithms are based either on
basic semideﬁnite programming or on its low-degree sum-of-squares strengthening depending
on the parameter regimes. Their guarantees either match or approach the best known guarantees
of fragile algorithms in terms of sparsity of the unknown vector, number of samples and the
ambient dimension.

To complement our algorithmic results, we prove rigorous lower bounds matching the
gap between fragile and robust polynomial-time algorithms in a natural computational model
based on low-degree polynomials (closely related to the pseudo-calibration technique for sum-
of-squares lower bounds) that is known to capture the best known guarantees for related
statistical estimation problems. The combination of these results provides formal evidence of
an inherent price to pay to achieve robustness.

Beyond these issues of perturbation resilience, our analysis also leads to new algorithms
for the fragile setting, whose guarantees improve over best previous results in some parameter
regimes (e.g. if the sample size is polynomially smaller than the dimension).

∗ETH Zürich. Supported by Steurer’s ERC Consolidator Grant.
†Carnegie-Mellon University. Part of this work done while at Princeton University and the Institute for Advanced

Study.

‡ETH Zürich.
§ETH Zürich. Supported by an ERC Consolidator Grant.

 
 
 
 
 
 
Contents

1 Introduction

1.1 Results . . . . . .

. . . . .
Sharp bounds for the Wishart model .

. . . . .
1.1.1
. . . . .
1.1.2 Additional Results: Practical Algorithms and Experiments . . . . .

. . . . . .
. . . . . .

. . . . . .
. . . . . .

. . . . . .

. . . . .

2 Techniques

2.1 Perturbation-resilience from Sparse Eigenvalue Certiﬁcates . . . .
. . . . . .
. . . . . .
2.2 Algorithms that Certify Sparse Eigenvalues .
. . . . . .
. . . . . .
. . . . .
2.3 New Certiﬁcates via basic SDP . . .
. . . . . .
2.4 New certiﬁcates via higher-level Sum-of-Squares . . . .
. . . . . .
2.4.1 Certiﬁcates via Certiﬁable Subgaussianity . . . .
. . . . . .
. . . . . .
2.4.2 Certiﬁcates via Limited Brute Force .
2.5 Concrete lower bounds for perturbation-resilient algorithms . . .
. . . . . .
2.6 Beyond limitations of CT via low-degree polynomials .
. . . . . .
. . . . . .

2.6.1 Polynomials based algorithm . . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

3 Preliminaries

. . . . . .
3.1 Pseudo-distributions . . .
3.2
. . . . . .
Sum-of-squares proofs . .
3.3 Low-degree likelihood Ratio . . . . .

. . . . . .
. . . . . .
. . . . . .
3.3.1 Background on Classical Decision Theory . . . .
3.3.2 Background on the Low-degree Method . . . . .

. . . . .
. . . . .
. . . . .

. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

4 Resilience of the basic SDP and Certiﬁed Upper Bounds

4.1 Basic Certiﬁcates for Sparse Quadratic Forms . . . . . .
. . . . . .
4.2 The basic SDP Algorithm . . . . . .

. . . . .

. . . . . .
. . . . . .

. . . . .
. . . . .

5 Resilience of SoS and Stronger Certiﬁed Upper Bounds

5.1
5.2
5.3

SoS Certiﬁcates for Sparse Eigenvalues via Certiﬁable Subgaussianity . . .
. . . . .
SoS Certiﬁcates for Sparse Eigenvalues via Limited Brute Force
. . . . .
. . . . . .
SoS Algorithms .

.
. . . . . .

. . . . . .

. . . . .

. . . . .

6 Unconditional lower bounds for distinguishing

. . . . . .

. . . . .
Spiked covariance model with sparsity . . . .

. . . . . .
6.1 Low-degree polynomials .
6.2
. . . . . .
6.3 Almost Gaussian vector in random subspace . . . . . .
6.4 Chi-squared-divergence and orthogonal polynomials .
6.5
. . . . .
6.6 Almost Gaussian vector in random subspace (proof) . .

Spiked covariance model with sparsity (proof)

. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

7 Polynomial-based algorithm with the right log factor
7.1 Low degree polynomials as estimators . . . .
. . . . .
7.2 Computation in polynomial time . .

. . . . . .
. . . . . .

. . . . . .
. . . . . .

. . . . .
. . . . .

1
4
. . . . . .
. . . . . . 10
. . . . . . 11

13
. . . . . . 13
. . . . . . 14
. . . . . . 15
. . . . . . 16
. . . . . . 16
. . . . . . 17
. . . . . . 17
. . . . . . 19
. . . . . . 19

20
. . . . . . 21
. . . . . . 22
. . . . . . 23
. . . . . . 24
. . . . . . 24

25
. . . . . . 26
. . . . . . 27

31
. . . . . . 31
. . . . . . 34
. . . . . . 36

38
. . . . . . 38
. . . . . . 39
. . . . . . 41
. . . . . . 43
. . . . . . 45
. . . . . . 50

55
. . . . . . 56
. . . . . . 61

8 Fast Spectral Algorithms for Recovery

8.1 Algorithm recovers u with high probability .
8.2 Algorithm recovers v with high probability .

. . . . . .
. . . . . .

. . . . . .
. . . . . .

. . . . .
. . . . .

63
. . . . . . 66
. . . . . . 73

9 Experiments

References

A Relationship with Clustering mixture of subgaussians

B Comparison with the Wigner model

C Thresholding Algorithms are Fragile
C.1 SVD with Thresholding is Fragile
.
C.2 Diagonal Thresholding is Fragile . .
C.3 Covariance Thresholding is Fragile .

. . . . . .
. . . . . .
. . . . . .
C.3.1 Proving covariance thresholding fragile . . . . .

. . . . .
. . . . .
. . . . .

75

78

82

83

. . . . . .
. . . . . .
. . . . . .
. . . . . .

. . . . .
. . . . .
. . . . .
. . . . .

83
. . . . . . 84
. . . . . . 84
. . . . . . 86
. . . . . . 89

D Covariance Thresholding doesn’t work with large signal and small sample size

E Statistical Lower bound for Recovery

F Existence of the Adversarial Distribution of Model 6.6

G Matrix concentration bounds

H Linear Algebra

I Experimental Setup

92

94

97

99

102

104

1 Introduction

Sparse principal component analysis (sparse PCA) is a fundamental primitive in high-dimensional
ℝ𝑑
statistics. Given a collection of vectors 𝑦1, . . . , 𝑦𝑛
= 1 maximally correlated with the vectors, commonly measured by the empirical
𝑣0k
with
k
. The structure we impose on 𝑣0 is sparsity, that is, an upper bound on the
𝑦𝑖 , 𝑣0i}𝑖
variance of
number of its non-zero entries.

ℝ𝑑, we seek a “structured” direction 𝑣0 ∈

{h

∈

∈[

𝑛

]

0, Id𝑑
(

Spiked covariance model. A widely studied statistical model for sparse PCA is the spiked covari-
ance model (also called Wishart model). Here, 𝑦1, . . . , 𝑦𝑛 are independent draws from the distribution
ℝ𝑑. (For simplicity, we will assume that
𝑁
𝑣 for 𝑣0 with correlation1
the sparsity parameter 𝑘 is known.) The goal is to compute an estimate
ˆ
2 > 𝑐 for an absolute constant 𝑐 > 0. (Here, we
bounded away from 0 so that
k
square the inner product because 𝑣0 is identiﬁable only up to sign.)

for an unknown 𝑘-sparse unit vector 𝑣0 ∈
= 1 and

𝑣, 𝑣0i
h ˆ

𝑣0𝑣𝑇
0 )

𝑣
k ˆ

+

𝛽

·

In order to simplify our discussion, we hide multiplicative factors logarithmic in 𝑑 using
. Similarly, we hide absolute constant multiplicative factors using the standard
, Ω

the notation ˜𝑂
notations ., 𝑂

and Θ

(·)
(·)

(·)

.
(·)

𝑛

{

𝑦𝑖

If we ignore computational eﬃciency, we can achieve optimal statistical guarantees for sparse
PCA in the spiked covariance model by the following kind of exhaustive search: among all 𝑘-by-𝑘
principal submatrices of the empirical covariance matrix of the vectors
, ﬁnd one with
maximum eigenvalue and output a corresponding eigenvector (e.g., [AW09, BR13a, BR13b]). This
procedure achieves constant correlation with high probability as long as 𝑛 >
.
})
However, the running-time is exponential in 𝑘. When the number of samples 𝑛 is signiﬁcantly
smaller than the ambient dimension 𝑑 as well as the sparsity parameter 𝑘, an alternative approach
is to ﬁnd a unit vector 𝑢 such that 𝑢T𝑌 is close to a 𝑘-sparse vector. This procedure also works for
𝑛 >

˜𝑂
The spiked covariance model exhibit a sharp transition in the top eigenvalue for 𝑛 & 𝑑

𝛽2 (called
BPP transition [BBP05] in reference to the authors’ names). In this regime, called strong-signal regime,
the following spectral algorithm matches the optimal statistical guarantees of exhaustive search:
compute the top right singular vector of 𝑌 and restrict it to the 𝑘 largest entries [KNV+15]. We refer
to this algorithm as SVD with thresholding.

and the running time is exponential in 𝑛.

𝛽, 𝛽2

𝛽, 𝛽2

min

min

˜𝑂

}𝑖

})

∈[

/

/

𝑘

𝑘

{

{

(

(

]

Whenever 𝑛 . 𝑑

𝛽2 , principal component analysis of

cannot be used to recover 𝑣0.
One of the best known polynomial-time algorithms for this regime (called weak-signal-regime) is
diagonal thresholding [JL09]: restrict the empirical covariance matrix to the principal submatrix
that contains the 𝑘 largest diagonal entries and output the top eigenvector of this submatrix. This
algorithm succeeds with high probability whenever 𝑛 & 𝑘2
𝑘 — almost quadratically worse than
exhaustive search.2 Similar guarantees were shown to be achievable in polynomial time through

𝛽2 log 𝑑

}𝑖

𝑦𝑖

∈[

{

𝑛

]

1Instead of asking for the correlation to be bounded away from 0, we could also ask for it to approach 1. Alternatively,
we could ask to recover the support of 𝑣0. At the granularity of our discussion here, these measures of success are
equivalent in most regards.

2We remark that [DKWB19] provides an algorithm that interpolates between Diagonal Thresholding and brute force
) if
log 𝑑. Whenever our discussion will revolve around polynomial time algorithms, we will simply talk about

log 𝑑, the algorithm recover the sparse vector in time 𝑑𝑂
/

search. Concretely, given any natural number 𝑡 6 𝑛
𝛽 & 𝑘
√𝑡𝑛
Diagonal Thresholding.

(

𝑡

p

1

a semideﬁnite relaxation [dGJL05, AW09] (which we refer to as the basic SDP) (see Section 4
for a precise formulation). A large and diverse body of work [AW09, CMW13, BR13a, KNV+15,
HKP+17a, DKWB19] has been dedicated to the question of understanding if this quadratic gap
between the sample sizes required for computationally eﬃcient and ineﬃcient methods is inherent
or if better polynomial-time algorithms exist for this problem. Hardness results addressing this
question take two forms: either reductions from conjecturally hard problems, such as planted clique
[BR13a] or concrete lower bounds against restricted classes of algorithm such as the sum-of-squares
[MW15, HKP+17b] or low-degree polynomials [DKWB19].

While these results provide evidence that a quadratic gap between polynomial-time algorithms
and exhaustive search is inherent in the weak signal regime, it turns out that a logarithmic im-
provement over diagonal thresholding is possible (for a broad parameter range): in the regime
𝑘 6 √𝑑
2, a more sophisticated algorithm called covariance thresholding [KNV+15, DM14] succeeds
/
for 𝑛 & max
. See Theorem G.12. This turns into an asymptotic improvement over
, but requires the constraint 𝑛 > 𝑘2. For
) 6 𝑘2 6 𝑜
diagonal thresholding in the settings 𝑑1
1
−
)
example, if 𝛽 = 1 and 𝑘2 = 𝜀𝑑 for some small enough 𝜀 > 0, covariance thresholding works with
𝑛 & 𝑘2 log

𝜀 and diagonal thresholding requires 𝑛 & 𝑘2 log 𝑑.

𝑘2
𝛽2 log 𝑑

𝑘2 , 𝑘2

𝑑

𝜀

n

o

(

𝑜

(

1
/
(

, while SVD requires 𝑛 & 𝑘2
)

/

Adversarial entry-wise perturbations.
In a seminal work, Huber [Hub81] asked how the guaran-
tees of estimators—designed to work under the assumption of observing Gaussian noise—would
change if the data were roughly normal, but not exactly so, thus broadening the circumstances
under which the performance of an estimator should be judged. This is especially relevant if we
consider that in many real world problems, data may be preprocessed, or the precision of an indi-
vidual input may be limited. For example, digital images may use few bits to encode a pixel and
discard all residual information. For these reasons, it is not desirable for an estimator to drastically
change its response as the input changes between 𝑌 and 𝑌
𝐸 for a small perturbation matrix 𝐸. In
this sense, the robustness of an estimator is an important aspect for understanding its performances
in real-world environments [Mor07, FMDF16].

+

It turns out that the algorithmic landscape for sparse PCA changes drastically in the presence
of adversarial perturbations, where an adversary may change each entry of the input vectors
𝑦1, . . . , 𝑦𝑛 by a small amount. On the one hand, exhaustive search and the basic SDP continue to
give the same guarantees as in the vanilla single-spike model. On the other hand, all aforementioned
thresholding algorithms are highly sensitive to small adversarial perturbations.

√𝑛

Concretely, in the strong signal regime 𝛽 . 𝑑
1
/
(

𝑛, it is possible to adversarially perturb the vectors
𝑦1, . . . , 𝑦𝑛 by at most ˜𝑂
per entry such that SVD with thresholding achieves only vanishing
correlation. Indeed an adversarial perturbation with this eﬀect can be viewed as a whitening
transformation and corresponds to a natural generative process for 𝑦1, . . . , 𝑦𝑛, where the vectors
are chosen randomly from an 𝑛-dimensional subspace containing an approximately sparse vector
(see Section 6). We also show that adversarial perturbations of this magnitude can fool diagonal
thresholding and covariance thresholding (see Appendix C).

/

)

2

𝛽, 1

min

𝛽
𝑘 ·

Sparse eigenvalues certiﬁcates.
It is remarkable to notice the stark contrast that appears when
instead adversarial perturbations are used against the basic SDP3, indeed it is easy to show that
the algorithm succeeds whenever adversarial perturbations are bounded (in absolute value) by
. If, for example, we assume 𝛽 > 1 and consider the regime in which diagonal
}
thresholding works, that is 𝛽 >
q
, this bound means the algorithm can aﬀord perturbations
)
bounded by 𝑂
1
. This is even more remarkable when one notices that for perturbations larger
(
)
/
4
𝑛1
an adversary could plant a matrix with 𝑘-sparse norm greater than 𝛽𝑛, thus fooling
than ˜𝑂
/
)
even the exhaustive search algorithm(see Appendix C) (moreover, this adversary can completely
remove the signal from 𝑌).

1
/
(

4
𝑛1
/

√𝑛

˜𝑂

p

/

𝑘

{

(

Considering these observations, it is only natural to ask what is the reason that makes some algo-
rithms robust4 to corruptions while others turn out to be highly susceptible to small perturbations
in the samples. This lead us to the central questions of this paper:

Is there some inherent property that makes an algorithm resilient to adversarial perturbations?

In the context of Sparse PCA, we answer this question showing how algorithms that come with
certiﬁcates of sparse quadratic forms5 are intrinsically better in the sense that small perturbations –
which by virtue of being small cannot signiﬁcantly change the sparse eigenvalues of the instance –
cannot be used to fool them. In contrast, fragile algorithms – which do not produce such certiﬁcates
– may be fooled by adversarial perturbations into outputting an estimation uncorrelated with the
sparse vector 𝑣0.

We remark that the insight obtained in this analysis also led us to new improvements in the

single spiked covariance model.

Certiﬁcation and the cost of resilience. The robustness of semideﬁnite programs had already
been noted in the literature. For the stochastic block model, eﬃcient spectral algorithms (see
[AS16]) are known to recover the partitions up to the (conjectured) computational threshold.6
However, few adversarial edge deletions and additions can fool such estimators. On the other
hand, algorithms based on semideﬁnite programming were shown to be resilient to adversarial
perturbations[FK01, GV14, MPW16, MS16, MMV16, BMR19], albeit far from the Kesten-Stigum
thresold in general settings.7 The underlying question of this line of work is whether the additional
property of resilience comes "for free".

In the context of this paper, with the idea of certiﬁcation mechanisms being a suﬃcient al-
gorithmic property for adversarial resilience, it becomes relevant to look into the limitations of
certiﬁcation algorithms as well. For the Sherrington-Kirkpatric problem [SK75] – the problem of

3We remark that a certain informal notion of robustness to entry-wise perturbations of the basic SDP program
was already argued in [dGJL05]. Additionally, in [BR13a] the authors observed that the algorithm is robust to small
perturbations of the empirical covariance matrix. We allow here more general perturbations.

4In this paper we will interchangeably use the terms robust and resilient.
5For a matrix 𝑀
∈
𝑘-sparse norm of 𝑀 as

𝑑 we study the values of the quadratic form
max

𝑀𝑣
. We sometimes refer to the 𝑘-sparse unit vector 𝑣 that maximizes

2 at 𝑘-sparse vectors 𝑣. We deﬁne the
as

𝑀𝑣

𝑀𝑣

ℝ𝑑

×

k

k

k

k

a sparse eigenvector, and to the corresponding value as a sparse eigenvalue.

6Called the Kesten-Stigum threshold.
7Another qualitative diﬀerence between the semideﬁnite programs studied in the paper above and other families of

=1,𝑣 𝑘-sparsek

k

𝑣

k

k

algorithms is the resilience to monotonic perturbations (see [FK01, MPW16] ).

3

maximizing the quadratic form 𝑥T𝑊 𝑥 where 𝑥
and 𝑊 is a symmetric random matrix
∈
with iid Gaussian entries above the diagonal – [Mon19] showed (modulo a reasonable conjecture)
(cid:8)
that for any 𝜀 > 0 there exists a polynomial-time optimization algorithm returning a value 𝜀-close
to the optimum. Conversely, [BKW20a] proved that no low-degree polynomial can obtain an 𝜀-
close certiﬁcate for the problem. Thus suggesting that certiﬁcation may be a inherently harder task
than optimization.

1
/

√𝑛

±

(cid:9)

𝑛

For sparse PCA in the strong signal regime, we observe a strikingly steep statistical price to pay
for robustness, in the form of a lower bound on the guarantees of low-degree polynomials. That is
a fundamental separation between the power of fragile and resilient algorithms.

1.1 Results

So far, we have generically said that an algorithm is "robust" if it recovers the planted signal
even in the presence of malicious noise. However, several issues arise if one tries to make this
vague deﬁnition more concrete. At ﬁrst, one could say that robust algorithms achieve comparable
guarantees both in the presence and the absence of adversarial corruptions. Yet, in general, this
interpretation makes little sense. Malicious perturbations may remove part of the signal, making the
guarantees of the fragile settings statistically impossible to achieve or –as we will see for the sparse
PCA in certain regimes– they might make the goal of achieving such guarantees computationally
much harder, thus at the very least forcing us to spend a signiﬁcantly higher amount of time to
obtain the same aforementioned guarantees.

For this reason, in many settings it will make sense to say that an algorithm is resilient if it
recovers the sparse signal in the presence of adversarially chosen perturbations even though its
guarantees may not fair well when compared to those achievable in the fragile settings.

The second fundamental aspect concerns the desirable degree of robustness that an algorithm
should possess. Indeed, any reasonable algorithm can likely tolerate suﬃciently small adversarial
perturbations. Therefore, it is important to quantify the magnitude of the perturbations we ask
algorithms to tolerate. Here, we also expect this magnitude to decrease monotonically with the
signal strength 𝛽. A natural concrete way to formalize this idea is the following: the algorithm
should be expected to obtain correlation bounded away from zero, as long as 𝑣0 remains the principal sparse
component. That is, as long as the vector maximizing the 𝑘-sparse norm of 𝑌 is correlated with 𝑣0,
then the algorithm should be able to output an estimator correlated with 𝑣0.

Concretely, these observations lead us to the following problem formulation.

Problem 1.1 (Robust sparse PCA). Given a matrix of the form

𝑌 = 𝑊

ℝ𝑑 is a unit 𝑘-sparse vector,

T
𝛽𝑢0𝑣0

+

𝐸, where

+

p

𝑁

0, Id𝑛
(

)

is a standard Gaussian vector,

𝑁

0, 1
)
(

𝑛

𝑑 is a Gaussian matrix and 𝑊 , 𝑢0, 𝑣0 are distributionally independent,

×

• 𝑣0 ∈
• 𝑢0 ∼
• 𝑊

∼

(1.1)

4

• 𝐸

∈

ℝ𝑛

×

𝑑 is an arbitrary perturbation matrix satisfying8

Return a unit vector

𝑣 having non-vanishing correlation with 𝑣0.
ˆ

.

𝐸

k

k∞

q

p

𝛽

𝑘

/

·

min

{

𝛽, 1

.

}

(1.2)

)

𝑘

/

𝛽

(
p

𝑛, 𝑑, 𝑘, 𝛽, 𝛿, 𝑝

To get an intuition why bound Eq. (1.2) is canonical, observe that for 𝛽 > Ω
1
)
(

adversarial
could remove all information about 𝑣0 (see Section 2.1). With
perturbations of magnitude ˜𝑂
this formalization of the problem we can now unambiguously deﬁne robust algorithms. Speciﬁcally,
we say that an algorithm is
, with
)
probability at least 𝑝 it outputs a unit vector

𝑣 such that 1
ˆ
Note that the exhaustive search algorithms described in introduction can also recover 𝑣0 in
the presence of the adversarial matrix 𝐸 from Problem 1.1. So we can assume that 𝑛 > log 𝑑 since
) using exhaustive search if 𝑛 >
otherwise Problem 1.1 can be solved in time 𝑑𝑂
1
.
})
To better keep track of the multiple results presented in the section, we provide three tables
summarizing the results of this works, each result is then individually discussed in the paragraphs
below.

–perturbation resilient if, for parameters
)
𝑣, 𝑣0i

𝑛, 𝑑, 𝑘, 𝛽

2 6 𝛿.

𝛽, 𝛽2

− h ˆ

min

˜𝑂

/

𝑘

{

(

(

(

(

Strong Signal Regime

Algorithm

Succeeds if

SVD with thresholding

𝛽 &

Sum of squares, Theorem 1.2

𝑑
𝑛 +
𝑑
𝑘

q
𝛽 & 𝑘
√𝑛

Running
Time

𝑂

𝑛𝑑 log 𝑛

log𝑡

1 𝑛
+

𝑡 𝑡

·

·

𝑡

𝑑𝑂
(cid:0)
(

)

𝑘 log 𝑑
𝑛
𝑡
for 𝑑 & 𝑛𝑡

1
/

Spectral
Theorem 1.8

algorithm,

𝛽 & 𝑘
√𝑛

(cid:1)

3
1
/

(cid:0)

𝑑
𝑘

for 𝑑 & 𝑛3 log 𝑑 log 𝑛

𝑂

𝑛𝑑 log 𝑛

(cid:16)

Resilient

No

Yes

*9

(cid:1)

(cid:17)

(cid:0)

(cid:1)

Table 1: Algorithmic landscape in the strong signal regime. The spectral algorithm is provably
resilient to the adversary used to fool SVD with thresholding but we do not expect it to be resilient
to arbitrary adversaries.

8In non-robust settings, we simply enforce the constraint
9Resilient to the distribution of Theorem 6.7

= 0.

𝐸

k

k∞

5

Algorithm

(Generalized)
thresholding

diagonal

Covariance thresholding

Basic SDP, Theorem 1.4

Sum of squares, Theorem 1.5

Low-degree
Theorem 1.6

polynomials,

Weak Signal Regime

Succeeds if

𝛽 & 𝑘
√𝑛

𝑡

·

𝛽 & 𝑘
√𝑛

log 𝑑 for 𝑡 6 1

ln 𝑑 min

𝑑, 𝑛

{

}

p
log 𝑑

𝑘2 for 𝑘 . √𝑑 and 𝑘 . √𝑛

q
𝛽 & min

2

log

𝑘
√𝑛
(cid:16)
log 𝑑 for 𝑡 6 1

r

+

(cid:26)

𝑑
𝑛

𝑑
𝑘2 +
(cid:17)
ln 𝑑 min
𝑜

𝑑
𝑛

, 𝑑
𝑛 +
𝑑, 𝑛

(cid:27)

q
{
}
) . 𝑘2 . 𝑑
1

(

log 𝑑
log 𝑛 for 𝑑1
−

·

𝑡

𝛽 & 𝑘
√𝑛
𝛽 & 𝑘
log 𝑑
p
√𝑛
and 𝑛 & log5 𝑑
q

𝑘2 +

Running
Time

Resilient

𝑛𝑂

)𝑑𝑂
1

(

(

𝑡

) No

𝑛𝑂

)𝑑𝑂
1

1
) No

(

(

𝑛𝑂

)𝑑𝑂
1

1
)

(

(

𝑛𝑂

)𝑑𝑂
1

(

(

𝑡

)

Yes

Yes

𝑛𝑂

)𝑑𝑂
1

1
) No

(

(

Table 2: Algorithmic landscape in the weak signal regime.

Computational Lower Bounds for Polynomials

Work

Polynomials of degree 𝐷 cannot distinguish if

Settings

Fragile

[DKWB19]

Fragile

Theorem 1.7

Resilient

Theorem 1.3

𝛽 .

𝑑
𝑛 ,

(cid:26)q

𝑑
𝑛 ,

𝛽 .

(
q
𝛽 6 𝑂
𝑛0.99𝑡
1
−

(cid:16)

𝑘
√𝐷𝑛

𝑘 log

(cid:27)
2
+
(cid:16)
√𝐷𝑛

𝐷𝑑
𝑘2

𝑘
√𝑛

𝑑
𝑘

𝑡

1
/

(cid:0)

(cid:1)

(cid:17)

(cid:17)

)
for 𝛽𝑛

/

𝑘 6 𝑛0.49 and 𝑑 6

Up to degree
𝐷 6 𝑜

𝑛

(

)

𝐷 6 𝑛

log2 𝑛

𝐷 6 𝑛0.001

Table 3: Computational landscape for low-degree polynomials.

Resilient algorithms in the strong signal regime. With the above discussion in mind, one may
ask whether the same guarantees known for the single spike covariance model may also be achieved
in the presence of adversarial perturbations. In the strong signal regime 𝛽 &
𝑛, this amounts
to ﬁnding a robust and eﬃcient algorithm that achieves the same guarantees as SVD with thresh-
olding. As we will see however, this is most likely impossible. That is, we will provide compelling
evidence that resilient algorithms cannot match the guarantees of fragile algorithms in the strong signal
regime.

p

𝑑

/

𝑛 . 𝛽 . 𝑑

𝑑

Since for

𝑛 adversarial perturbations of the order ˜𝑂
can change the top
eigenvalue of the covariance matrix, PCA arguments cannot be used to obtain resilient algorithms.
Thus intuitively, this suggests that diﬀerent kinds of certiﬁcates are needed.

1
/
(

p

/

/

)

√𝑛

𝑡

We provide a Sum-of-Squares algorithm that recovers in time 𝑑𝑂

) the sparse vector whenever
𝑛 & 𝑘
. The key contribution is indeed an eﬃcient algorithm to certify
)
upper bounds on random quadratic forms. For subgaussian10 low-rank quadratic forms, these
upper bounds approach information-theoretically optimal bounds.

and 𝑑1
/

Ω
˜
(

𝑡 >

𝛽 ·

1
/

𝑛

𝑑
𝑘

𝑡

(cid:1)

(cid:0)

(

𝑡

10Formally we require a stronger property, we need matrices to be certiﬁably subgaussian.

6

Concretely, for an 𝑛-by-𝑑 matrix 𝑊 with i.i.d. Gaussian entries, with high probability the degree-
𝑡

𝑡 sum-of-squares algorithm (with running time 𝑑𝑂
)) certiﬁes an upper bound of 𝑂
𝑘
(
Ω
on the quadratic form 𝑄
𝑛
˜
(
certiﬁcates, a robust algorithm for Sparse PCA follows then as a speciﬁc corollary.

2 over all 𝑘-sparse unit vectors 𝑥 if 𝑑1
/

1
𝑡
/
)−
· (
)
. With these
)

𝑊 𝑥

𝑡 >

=

𝑥

𝑑

/

𝑘

k

k

(

)

·

(

𝑡

It is important to notice how this result for sparse PCA is interesting regardless of its resilience
, the algorithm approaches the information theoretic optimal
)

properties. As 𝑡 approaches log

𝑑

/

𝑘

(

𝑘
𝛽 ·

bound 𝑂

log

𝑑

𝑘

. For example, consider the case 𝑛 = 2

(

(

/
of Squares algorithm works in time 𝑑
guarantees, while exhaustive search takes time exponential in 𝑛.

log2 𝑛
(

= 𝑛𝑂

√log 𝑑

))

𝑂

(cid:16)

(cid:16)

(cid:17)

Θ

√log 𝑑

. If also 𝑑
𝑘

= 2

(cid:17)

Θ

√log 𝑑

(cid:16)

(cid:17)

, the Sum

) with information theoretically optimal

The speciﬁc algorithmic result is shown in the following theorem.

Theorem 1.2 (Perturbation Resilient Algorithm in the Strong Signal Regime). Given an 𝑛-by-𝑑 matrix
𝑌 of the form,

𝑌 =

𝛽

T
𝑢0𝑣0

·
+
ℝ𝑑, a Gaussian matrix 𝑊

+

𝑊

𝐸 ,

𝑑 satisfying

𝑁

∼
𝐸
k∞

k

0, 1
)
(
.
𝛽

𝑛

𝑑, a vector 𝑢0 ∈
×
𝛽, 1
𝑘
}
{
·

min

/

ℝ𝑛 independent
.

for 𝛽 > 0, a unit 𝑘-sparse vector 𝑣0 ∈
2 = Θ
of 𝑊 with
For 𝑡

, and a matrix 𝐸
𝑢0k
)
k
ℕ suppose that 𝑑 & 𝑛𝑡 log𝑡
1
+
∈

𝑛

(

p
ℝ𝑛
×
∈
𝑡 𝑡 and
𝑛
(
)
𝛽 & 𝑘
𝑛 ·

p

p

𝑡

1
/

.

𝑑
𝑘

𝑡

·

(cid:0)
Then, there exists an algorithm that computes in time 𝑑𝑂

𝑡

(cid:1)
) a unit vector

(

ℝ𝑑 such that

𝑣
ˆ

∈

with probability at least 0.99.

1

𝑣, 𝑣0i

− h ˆ

2 6 0.01

𝑡 >

In any case, the fundamental limitation of the above algorithm is the requirement 𝑑1
/

.
)
This constraint makes it impossible to match the guarantees of SVD+ thresholding in most regimes,
but a priori it remains unclear why better robust algorithms could not be designed. To provide
formal evidence that without the requirement 𝑑1
achieving the kind of guarantees of
/
Theorem 1.2 may be computationally intractable, we make use of a remarkably simple method
(sometimes called analysis of the low degree likelihood ratio), developed in a recent line of work on
the the sum of squares hierarchy [BHK+16, HS17, HKP+17b, Hop18]. That is, we show that in
the restricted computational model of low-degree polynomials11, there is no eﬃcient algorithm
that can improve over the Sum-of-Squares algorithm. This hardness results suggests a fundamental
separation between fragile and resilient algorithms, in other words, an inherent cost to pay in exchange for
perturbation-resilience.

Ω
˜
(

Ω
˜
(

𝑡 >

𝑛

𝑛

)

𝑑 matrices of the form 𝑌 =

𝐸 where 𝐸 is a perturbation
Concretely, we construct 𝑛
+
such that, whenever 𝑑 is signiﬁcantly smaller than 𝑛𝑡,
matrix with entries bounded by ˜𝑂
multilinear polynomials of degree at most 𝑛0.001 cannot distinguish these 𝑌’s from 𝑛
𝑑 Gaussian
(cid:1)
matrices (in the sense that w.h.p. every such polynomial takes roughly the same values under both
distributions). These ideas are formalized in the theorem below.

1
/

√𝑛

𝑊

p

+

×

×

(cid:0)

T
𝛽𝑢0𝑣0

11As we will argue in Section 3.3, being indistinguishable with respect to low degree polynomials is an important

indication of computational hardness.

7

Theorem 1.3 (Lower Bound for Resilient Algorithms in the Strong Signal Regime, Informal). Let 𝑡
be a constant and let 𝑑 6 𝑛0.99𝑡

1. Suppose that
−

𝛽 6 𝑂

𝑘
𝑛 ·

(cid:18)

𝑡

𝑑

𝑘

𝑡

1
/
)

/

· (

.

(cid:19)

and12 𝛽𝑛
/
𝐸
where
k∞

k

𝑘 6 𝑛0.49. Then, there exists a distribution 𝜇 over 𝑛

6

˜𝑂

√𝑛
1
/

, with the following properties:

• 𝜇 is indistinguishable from the Gaussian distribution 𝑁

(cid:0)

(cid:1)

polynomials of degree at most 𝑛0.001 in the sense described in Section 3.3.2,

𝑑 matrices 𝑌 of the form 𝑌 =

×

𝛽𝑢0𝑣𝑇

0 +

𝑊

𝐸

+

p
𝑛 with respect to all multilinear

𝑑

×

0, 1
)
(

• the jointly-distributed random variables 𝑊, 𝑢0, 𝑣0 are independent,

• the marginal distribution of 𝑣0 is supported on unit vectors with entries in

• the marginal distribution of 𝑢0 is uniform over

1, 1

𝑛 ,

}

{−

• the marginal distribution of 𝑊 is 𝑁

𝑛

𝑑.

×

0, 1
)
(

1
/

√𝑘
√𝑘, 0, 1
/

−

,

o

n

𝑛

×

0, 1
)
(

Informally speaking, Theorem 1.3 conveys the following message. Any resilient algorithm for
Sparse PCA can also distinguish the distribution 𝜇 over 𝑛-by-𝑑 matrices 𝑌 from the Gaussian
𝑑. Therefore, if an estimator returned by this algorithm can be approximated
distribution 𝑁
by low-degree polynomials, then this algorithm cannot certify upper bounds of sparse eigenvalues
of Gaussian matrices that are sharp enough to signiﬁcantly improve the guarantees of Theorem 1.2.
Sparse principal component analysis is intimately related to the problem of learning Gaussian
mixtures. Indeed, for a vector 𝑣0 with entries in
, sparse PCA can be rephrased as the
problem of learning a non-uniform mixture 𝑀 of three subgaussian distributions, one centered at
𝑢0. As we will see, this is true even for the
zero, one centered at
distribution 𝜇 used in Theorem 1.3 Thus, from this perspective the result also provides interesting
p
insight on the complexity of this problem. The theorem suggests that to distinguish between 𝑀 and
𝑑, an algorithm would either need 𝑑 & 𝑛𝑡 samples or should
a standard Gaussian 𝑊
not be computable by polynomials of degree at most 𝑛0.001 (see Appendix A).

𝑢0 and the last at

√𝑘, 0
1
/

0, 1
)
(

{±

p

𝑁

−

∼

𝛽

𝛽

/

/

𝑘

𝑘

}

×

𝑛

·

·

/

𝑑

p

Resilient algorithms in the weak signal regime. Having cleared the picture for eﬃcient al-
gorithms in the strong signal regime, we may focus our attention to the weak signal settings
𝛽 .
𝑛. Surprisingly, in these settings adversarial perturbations do not change the computa-
tional landscape of the problem. As a matter of fact, a robust algorithm was already known. In
fragile settings, the basic SDP program was proved (e.g. see [BR13b]) to have the same guarantees
and
as diagonal thresholding. But as the algorithm can certify the upper bounds
ℝ𝑑 and matrices 𝑀
𝑑
𝑛
𝑊 𝑥

𝑛 log 𝑑 over 𝑘-sparse unit vectors 𝑥

k
(where 𝐶 > 0 is some absolute constant), it is therefore resilient to adversarial corruptions.
We improve this latter upper bound showing that the algorithm can also certify the inequality

2 6 𝑘
k
𝑑, 𝑊
×

k
∞
0, 1
)
(

𝑀 𝑥
ℝ𝑛

𝑀
𝑁

2 6 𝑛

· k
∼

k
∈

𝐶 𝑘

p

+

∈

k

×

2

12This constraint is used to ensure that inequalities of the form 𝛽 &

for any 𝐷 6 𝑛0.001 are never satisﬁed.
Informally speaking, we restrict our statement to the settings where algorithms with guarantees similar to diagonal
thresholding do not work.

𝑘
√𝑛

𝐷

·

8

2 6 𝑛

𝑊 𝑥
k
})
and leading us to the following result.
p

𝑛 log
(

min

𝐶 𝑘

𝑘2, 𝑛

+

𝑑

/

{

k

, thus matching the guarantees of covariance thresholding

Theorem 1.4 (Perturbation Resilient Algorithm in the Weak Signal Regime). Given an 𝑛-by-𝑑 matrix
𝑌 of the form,

𝑌 =

𝛽

T
𝑢0𝑣0

·
+
ℝ𝑑, a Gaussian matrix 𝑊

+

𝑊

𝐸 ,

for 𝛽 > 0, a unit 𝑘-sparse vector 𝑣0 ∈
2 = Θ
of 𝑊 with

𝑛

𝑢0k
k
Suppose that

, and a matrix 𝐸
)

∈

(

𝑑 satisfying

×

k

p
ℝ𝑛

𝑁

∼
𝐸
k∞

0, 1
)
(
.
𝛽

𝑛

𝑑, a vector 𝑢0 ∈
×
𝛽, 1
𝑘
}
{
·

min

/

ℝ𝑛 independent
.

p

p

Then, there exists an algorithm that uses the basic SDP program for sparse PCA, and computes in

𝛽 & min

𝑘
√𝑛 s

log

2

+

(cid:18)

𝑑
𝑘2 +

𝑑
𝑛

𝑑
𝑛 + r

𝑑
𝑛

,

(cid:19)





ℝ𝑑 such that

.





1

𝑣, 𝑣0i

− h ˆ

2 6 0.01

polynomial time a unit vector

𝑣
ˆ

∈

with probability at least 0.99.

Theorem 1.4 says that among polynomial time algorithms, in the weak signal regime or when-
ever 𝛽 < 1, the basic SDP achieves the best known guarantees. Furthermore, in contrast to thresh-
olding and PCA algorithms, it works even in the presence of adversarial corruptions.

High degree certiﬁcates in the weak signal regime. A consequential observation of the previous
paragraphs is that, perhaps, the Sum-of-Squares algorithm of larger degree can improve over the
guarantees of the basic SDP even in the weak signal regime. Indeed in many settings, these
guarantees can be improved observing that the (degree 𝑡) Sum-of-Squares algorithm can certify
𝑛
). Hence oﬀering a smooth
upper bounds of the form
k
trade-oﬀ between sample complexity and running time.

log 𝑑 in time 𝑑𝑂

2 6 𝑛

𝑊 𝑥

+

/

𝑘

k

)

𝑡

(

𝑡

(
p

Theorem 1.5 (Perturbation Resilient Algorithm via Limited Exhaustive Search). Given an 𝑛-by-𝑑
matrix 𝑌 of the form,

𝑌 =

𝛽

T
𝑢0𝑣0

𝑊

𝐸 ,

for 𝛽 > 0, a unit 𝑘-sparse vector 𝑣0 ∈
2 = Θ
of 𝑊 with

𝑑 satisfying
×
)
Suppose that for some positive integer 𝑡 6 1
𝑑, 𝑛
ln 𝑑 min

and a matrix 𝐸

𝑢0k

p
ℝ𝑛

·
+
ℝ𝑑, a Gaussian matrix 𝑊
𝐸
k
,
}

+

∈

𝑛

{

k

(

𝑁

𝑛

0, 1
)
(
.
𝛽
/

×
𝑘

𝑑, a vector 𝑢0 ∈
.
𝛽, 1
min
}
{
·

ℝ𝑛 independent

∼
k∞

p

p

Then, there exists an algorithm that computes in time 𝑛𝑂

)𝑑𝑂
1

(

(

𝑡

) a unit vector

ℝ𝑑 such that

𝑣
ˆ

∈

𝛽 & 𝑘
√𝑛𝑡

p

log 𝑑 .

with probability 0.99.

1

𝑣, 𝑣0i

− h ˆ

2 6 0.01

9

Whenever 𝑘2 6 𝑑1
−

Ω

1
), Theorem 1.5 provides better guarantees than Theorem 1.4 (with worse

(

running time).

It is also interesting to compare this result with the bound of Theorem 1.2. For some 𝑡, we can
determine the parameter regimes when one theorem provides better guarantees then the other
for running time 𝑑𝑂
1
. Then there exist
+
constants 0 < 𝐶 < 𝐶′ such that:

2 & 𝑑 & 𝑡 𝑡 𝑛𝑡
+

). Assume that

1𝑛𝑡
+

log 𝑛

log 𝑛

1
)

1
+

+

(

𝑡

(

𝑡

𝑡

𝑡

𝑡

(cid:0)

(cid:0)
𝑛
𝑡 log 𝑑, so in this case the guarantees in Theorem 1.5 are

(cid:1)

(cid:1)

• If 𝑘2 6 𝑑
better.

𝐶𝑡

𝑡, we get 𝑡
)

·

· (

𝑡

1
/

>

𝑑
𝑘

• If 𝑘2 > 𝑑

·

2

𝑛 log2 𝑛

Theorem 1.2 are better.
(cid:16)

(cid:17)

(cid:0)

𝐶′𝑡

p

(cid:1)
𝑡, we get 𝑡

)

𝑡

1
/

<

𝑑
𝑘

·

(cid:0)

(cid:1)

p

𝑛
𝑡 log 𝑑, so in this case the guarantees in

· (

Informally speaking, these conditions show that the guarantees in Theorem 1.2 are better when
the vector is only mildly sparse: 𝑘2

𝑑, and the number of samples 𝑛 is very small.

Theorem 1.5, along with Theorem 1.4 and Theorem 1.2 provides also a nice consequence,
namely it allows us to state that for the problem of Sparse PCA, the Sum-of-Squares algorithm achieves
the best known guarantees among perturbation resilient polynomial time algorithms. Furthermore, under
the restrict computational model of low-degree polynomials, these guarantees are nearly optimal.

≫

1.1.1 Sharp bounds for the Wishart model

In the regime where 𝑘 6 √𝑑, covariance thresholding succeeds for 𝛽 & 𝑘
𝑘2 . This turns
√𝑛
𝑜
into an asymptotic improvement over diagonal thresholding in the settings 𝑑1
q
but
(
−
requires a constraint on the sample complexity of the form 𝑛 > 𝑘2, for which there is no evidence in
the known lower bounds. This picture raises the following questions: can we obtain guarantees of
𝑘2 even for 𝑛 6 𝑘2? And furthermore, can we improve over this logarithmic
the form 𝛽 & 𝑘
√𝑛
factor?

log 𝑑
) 6 𝑘2 6 𝑜
1

log 𝑑

𝑑

)

(

q

Studying low-degree polynomials we improve over this incomplete picture providing a new al-
log 𝑑
𝑘2

gorithm which succeed in recovering the sparse vector in polynomial time whenever 𝛽 & 𝑘
√𝑛

log
1
/

𝑑
𝑘2

and 𝑛 & 𝑑
in a signiﬁcantly large set of parameters.

(cid:17) +

(cid:16)

log5 𝑑. Thus obtaining an asymptotic improvement over diagonal thresholding

q

log 𝑑
Concretely, the algorithm improves over the state-of-the-art whenever 𝑑1
𝑘2
/
Ω

log5 𝑑 . 𝑛 .
1
). In other words, the algorithm requires much fewer samples than covariance thresholding.

+

(

𝑑1
−
This result is captured by the theorem below.

Theorem 1.6 (Polynomials based Algorithm for the Strong Signal Regime). Given an 𝑛-by-𝑑 matrix
𝑌 of the form,

for a unit vector 𝑣0 ∈
6 𝑂
2 = 1, 𝔼 𝑢4
𝔼 𝑢𝑖
1
𝑖
)
(

ℝ𝑑 with entries in

and a matrix 𝑊

{±

∈

𝑌 =

𝛽

𝑢0𝑣𝑇

𝑊 ,

·
√𝑘, 0
p
1
/
ℝ𝑛

0 +
, a vector 𝑢0 with i.i.d. entries satisfying 𝔼 𝑢𝑖 = 0,
= 1,

𝑑 with i.i.d. entries satisfying 𝔼

= 0, 𝔼

}

𝑊𝑖𝑗

×

𝑊 2
𝑖𝑗

(cid:2)

(cid:3)

h

i

10

such that 𝑊 and 𝑢0 are independent; suppose that 𝑛 & log5 𝑑, 𝑑1
−

𝑜

) 6 𝑘2 6 𝑑
1

(

2, and

/

𝛽 & 𝑘

√𝑛 s

log

𝑑
𝑘2

(cid:18)

+

(cid:19)

log 𝑑
log 𝑛

.

Then, there exists a probabilistic algorithm that computes in polynomial time a unit vector

ℝ𝑑 such that

𝑣
ˆ

∈

with probability at least 0.99.

1

𝑣, 𝑣0i

− h ˆ

2 6 0.01

Along with Theorem 1.6, we provide a ﬁne-grained lower bound that in many settings matches
the known algorithmic guarantees for the single spiked model. Some relevant lower bounds were
already known. In [BR13a] the authors used a reduction to the planted clique problem to provide
evidence that in the weak signal regime13 eﬃcient algorithms cannot recover the sparse vector if
. In [DKWB19] similar lower bound was obtained: in the weak signal regime low-degree
𝛽
polynomials cannot succeed if 𝛽 . 𝑘
√𝑛
of diagonal thresholding by a logarithmic factor. Here, we show that whenever 𝑘2 6 𝑑1
−

. This lower bounds fall short of matching the guarantees
1
),

𝑘
√𝑛

≪

Ω

(

polynomials of degree 𝑂

cannot recover the sparse vector for 𝛽 . min

𝑑

𝑛 , 𝑘
√𝑛

log 𝑑

.

(cid:27)
In particular, we provide strong evidence that in the weak signal regime, in the settings where
our polynomials based algorithm does not improve over the state-of-the-art, the known eﬃcient
algorithms (diagonal thresholding, basic SDP) are optimal up to constant factors.

(cid:26)q

p

log 𝑑
(

)

Theorem 1.7 (Lower Bound for Standard Sparse PCA, Informal). There exists a distribution 𝜇𝑘 over
𝑘-sparse 𝑑-dimensional unit vectors such that if 𝑌 is an 𝑛-by-𝑑 matrix of the form

𝑌 =

T
𝑢0𝑣0

𝛽

·

𝑊 ,

+

0, Id𝑛
for a vector 𝑣0 sampled from 𝜇𝑘, a Gaussian matrix 𝑊
)
(
such that 𝑣0, 𝑢0, 𝑊 are distributionally independent, then the distribution of 𝑌 is indistinguishable from
log2 𝑛 in the sense
the Gaussian distribution 𝑁
×
described in Section 3.3.2, whenever

𝑑 with respect to all polynomials of degree 𝐷 6 𝑛

𝑑 and a Gaussian vector 𝑢0 ∼

0, 1
)
(

0, 1
)
(

𝑁

𝑁

∼

/

×

𝑛

𝑛

p

𝛽 . min

𝑑
𝑛

,

𝑘
√𝐷𝑛

log

2

(cid:18)

𝑑

𝐷

·
𝑘2

+

.

(cid:19) )

(r

1.1.2 Additional Results: Practical Algorithms and Experiments

From a practical perspective, the main issue with the results of Theorem 1.2 is the reliance on
solving large semideﬁnite programs, something that is often computationally too expensive to
do in practice for the large-scale problems that arise in machine learning. In the same fashion of
[HSSS16], from the insight of the SoS analysis we develop a fast spectral algorithm (which we will
call SVD-t) with guarantees matching Theorem 1.2 for degree 𝑡 6 3 for some interesting family of
𝑛𝑑 log 𝑛
adversaries. Our algorithm runs in time 𝑂
, which for high dimensional settings, can be

13Actually the parameter regime they considered is a proper subset of the weak signal regime.

(cid:0)

(cid:1)

11

considerably faster than algorithms that rely on computing the covariance matrix14. Furthermore,
while not showing robustness of the algorithm (indeed the algorithm cannot certify upper bounds),
we prove that SVD-t succeeds under the adversarial perturbations which are enough to prove
Theorem 1.3. Such adversarial settings are especially interesting since the problem has a nice
geometric description in which the objective is to recover an approximately sparse vector planted in
a random subspace. (see Section 6.6) We remark that it is not known how to generalize the algorithm
for larger 𝑡. Finally, we complement this result with experiments on synthetic data which highlights
how in many practical settings the algorithm outperforms (and outruns) diagonal thresholding. The
following theorem presents the guarantees of the algorithm in the spiked covariance model.

Theorem 1.8 (Fast Spectral Algorithm for the Strong Signal Regime, Informal). Given an 𝑛-by-𝑑
matrix 𝑌 of the form,

for 𝛽 > 0, a unit 𝑘-sparse vector 𝑣0 ∈
𝑢0 ∼
0, Id𝑛
(
Theorem 1.3 for 𝑡 = 3.15

0, 1
)
(
such that 𝑣0, 𝑢0, 𝑊 are distributionally independent, and 𝐸

𝑁

𝑁

∼

∈

)

𝑌 =

T
𝛽𝑢0𝑣0

𝑊

𝐸 ,

+
ℝ𝑑, a Gaussian matrix 𝑊

p

+

Suppose that 𝑑 & 𝑛3 log 𝑑 log 𝑛, 𝑘 & 𝑛 log 𝑛 and

𝑛

𝑑, a Gaussian vector
𝑑 is a matrix from

×
ℝ𝑛

×

𝛽 & 𝑘

3
1
/

.

𝑑
𝑘

(cid:19)

√𝑛 (cid:18)
Then there exits an algorithm that computes in time 𝑂

1

𝑣0,

𝑣
ˆ

i

− h

with probability at least 0.99.

Outline and Notation

(

a unit vector

𝑣
ˆ

∈

ℝ𝑑 such that

𝑛𝑑 log 𝑛

)

6 0.01

We conclude our introduction with an outline of the structure of the paper and some notation.

In Section 2 we give an overview of the techniques and the ideas required to obtain the results.
Preliminary information are presented in Section 3. Section 4 and Section 5 contains the results
for the basic SDP and the Sum-of-Squares algorithms. In Section 6 we show our lower bounds on
polynomials and in Section 7 we use such polynomials to prove Theorem 1.6. Finally, in Section 8
and Section 9 we describe our fast algorithms, formally prove some of their properties and compare
them with known algorithms through experiments.

Additionally, we discuss the relationship with the problem of clustering mixture of Gaussians
in Appendix A. In Appendix B we describe the picture for the Wigner model. Appendix C contains
formal proofs that thresholding algorithms are not robust and Appendix D shows why Covariance
Thresholding fails for small sample size. We also provide an information theoretic bound in
Appendix E.

𝑌 is necessary for covariance thresholding, the diagonal thresholding algorithm can run in

T
14While computing 𝑌
𝑑𝑛

𝑘2

.

time 𝑂

(

+

)

15More precisely, to prove Theorem 1.3 we consider a speciﬁc distribution over matrices 𝐸 (this distribution depends

on 𝑣0, 𝑢0 and 𝑊), and here we mean that 𝐸 is sampled from this distribution.

12

k1 =

k

𝑖,𝑗

𝑑
∈[
Í

](cid:12)
(cid:12)

Notation. We say that a unit vector 𝑣

ℝ𝑑 is ﬂat if its entries are in
for some 𝑡. For a
𝑑, we will denote its entry 𝑖𝑗 with 𝑀𝑖𝑗. Depending on the context we may refer to
matrix 𝑀
the 𝑖-th row or the 𝑖-th column of 𝑀 with 𝑀𝑖 or 𝑚𝑖, we will specify it each time to avoid ambiguity.
𝑑, we
We call

the "absolute norm" of 𝑀. For a Gaussian matrix 𝑊

ℝ𝑛

, 0

𝑀

𝑁

±

∈

∈

o

n

×

𝑛

1
√𝑡

𝑀𝑖𝑗

0, 1
)
(

×

∼

(cid:12)
denote with 𝑤1, . . . , 𝑤𝑑 its columns. For a vector 𝑣
(cid:12)
absolute constant multiplicative factors using the standard notations ., 𝑂
multiplicative factors logarithmic in 𝑑 using the notation ˜𝑂
. For a set 𝑆
(·)
]𝑖𝑗 = 𝑀𝑖𝑗 if
the matrix with entries 𝑀
𝑀
𝑆
𝑆
[
[
]
ℝ𝑑
ℝ, we deﬁne 𝜂𝜏(
ℝ𝑑
𝑑 and 𝜏
𝑀
otherwise. For a matrix 𝑀
) ∈
> 𝜏
𝑀𝑖𝑗

ℝ𝑛, we denote its 𝑗-th entry as 𝑣 𝑗. We hide
, Ω
, we hide
𝑑
, and a matrix
]𝑖𝑗 = 0
𝑆
𝑑 to be the matrix with entries

𝑑, we denote by 𝑀

]
𝑆, and 𝑀

and Θ
𝑑

(·)
⊆ [
𝑖, 𝑗

(·)
] × [

𝑀𝑖𝑗

) ∈

ℝ𝑑

(·)

if

∈

∈

∈

∈

×

×

×

[

(

𝑀

𝜂𝜏(

)𝑖𝑗 =

0

(

otherwise.
(cid:12)
(cid:12)

(cid:12)
(cid:12)

Furthermore, we deﬁne 𝜁𝜏(

𝑀

) ∈

ℝ𝑑

×

𝑑 to be the matrix with entries

𝑀

𝜁𝜏(

)𝑖𝑗 =

𝑀𝑖𝑗

0

(

−

sign

𝑀𝑖𝑗

𝜏

·

(cid:0)

(cid:1)

if

𝑀𝑖𝑗

> 𝜏

otherwise.
(cid:12)
(cid:12)

(cid:12)
(cid:12)

Additional notation will be introduced when needed.

Remark 1.9 (Strong and weak signal regimes in robust settings). The attentive reader may have
noticed how the notions of strong and weak signal regime should diﬀer in the robust settings.
Indeed there is no easy algorithm that looks at the spectrum of 𝑌 and begins to work as 𝛽

𝑑

q

𝑛 . In this sense, in the presence of an adversary the bound 𝛽 .

𝑑
approaches
𝑛 looses signiﬁcance.
However we will continue using these terms to orientate ourselves and implicitly describe which
are the desirable guarantees an algorithm should possess in a given regime. For this reason, when
talking about weak-signal regime, our discussion will implicitly revolve around settings in which
𝛽 & 𝑘
√𝑛

log 𝑑
𝑘2 .

q

q

2 Techniques

2.1 Perturbation-resilience from Sparse Eigenvalue Certiﬁcates

Here we outline the structure of our Sum-of-Squares algorithm and the basic SDP. We assume the
reader to be familiar with the knowledge in Section 3.

How robust should an algorithm be?
In light of our discussion in Section 1.1, we would like
eﬃcient algorithms to be as resilient as exhaustive search. In order for such brute-force algorithm
to recover the sparse vector 𝑣0, there must be no other sparse vector 𝑥 far from 𝑣0 such that
. This also means that the adversary should not be able to plant a 𝑘-sparse vector 𝑧
𝑌𝑣
𝑌𝑥
k
far from 𝑣0 such that
. To see what bound to enforce on the adversarial matrix, ﬁrst
k
observe that if 𝐸 were the zero matrix then

𝑌𝑣0k

k ≈ k

𝑌𝑥

&

k

k

k

=

𝑌𝑣0 k

k

𝑊 𝑣0 +
(cid:13)
(cid:13)
(cid:13)

𝛽𝑢0

&

p
13

(cid:13)
(cid:13)
(cid:13)

p

𝛽𝑛.

𝑛

+

Now consider the following adversarial matrix, let 𝑥 be a 𝑘-sparse unit vector with entries in

√𝑘

0,

1
/

±

n
o
high probability
𝑏
sign

𝑥 𝑗

𝑧𝑖

·

·

(

and such that the intersection between supp

𝑊 𝑥

k

k ≈

√𝑛. So let 𝑧 = 1
𝑊 𝑥

k
, where 𝑏 > 0 is some parameter that we will choose later. Then
)

k

𝑥

𝑣0}
is the empty set. With
{
𝑊 𝑥 and deﬁne 𝐸 as the matrix with entries 𝐸𝑖𝑗 =

and supp

}

{

=

𝑌𝑥

k

k

𝑊

𝐸

𝑥

)

k

+

k(

=

𝑊 𝑥

k +

𝑏√𝑘

𝑧

√𝑛

𝑏√𝑘.

≈

+

(cid:13)
(cid:13)
whenever √𝑛
(cid:13)

𝑏√𝑘 >

(cid:17)
𝑛

(cid:13)
(cid:13)
𝛽𝑛. The inequality is true for 𝑏 &
(cid:13)
+

k

(cid:16)

+

. In other words, the perturbation matrix must satisfy the bound:

p

Consequently,

𝑌𝑥

>

k

𝑌𝑣0 k

k

k
𝛽, 1

𝛽𝑛
𝑘 ·

min

q

(cid:8)p

(cid:9)

𝐸

k

k∞

6

Ω
˜

𝛽
𝑘 ·

 r

min

𝛽, 1

np

.

!

o

(Bound-1)

For a set of parameters 𝑑, 𝑛, 𝑘, 𝛽, we call an algorithm perturbation resilient if it can successfully
recover the sparse vector for any adversarial perturbation satisfying bound Bound-1.

k

→

𝐸

k1

Remark 2.1. In the proofs presented in the paper, we will measure adversarial corruptions with the
2, which denotes the largest norm of a column of 𝐸. Clearly this choice allows for a
norm
larger class of adversaries. There are a two main reasons behind our choice. The ﬁrst one being that
the adversarial matrices we consider are more naturally described using such norm. Furthermore,
this norm has a direct correspondence with the inﬁnity norm of the adversarial perturbation in the
covariance matrix. Indeed,
2. This also will allow us to draw a better comparison
between the Wishart and the Wigner model. We remark that the reasoning above can be used as
well to show the bound:

∞
(cid:13)
(cid:13)

2
1
→

𝐸T𝐸

(cid:13)
(cid:13)

=

𝐸

k

k

𝐸

k

k1

→

2 . min

𝛽𝑛
𝑘 ·

(r

min

𝛽, 1

np

.

)

o

(Bound-2)

2.2 Algorithms that Certify Sparse Eigenvalues

For simplicity of the discussion we illustrate the idea of sparse eigenvaluex certiﬁcates for the
T
Wigner model: 𝑌 = 𝛾𝑣0𝑣0
𝑑
×
and 𝐸 is some matrix with small entries. Denote the set of 𝑘-sparse unit vectors by 𝑆𝑘. The starting
idea is to turn the following intuition into an identiﬁability proof and then a Sum of Squares
𝑣 is a 𝑘-sparse unit vector which maximizes 𝑣T𝑌𝑣 over 𝑆𝑘 and 𝛾 is large enough, then
program: if
ˆ
with high probability

ℝ𝑑 is a 𝑘-sparse unit vector, 𝑊

𝐸, where 𝛾 > 0, 𝑣0 ∈

2 > 0.99.

0, 1
)
(

𝑊

𝑁

∼

+

+

𝑑

𝑣, 𝑣0i
h ˆ
Concretely, observe that

on one side

on the other

T
𝑣0
T
𝑣
ˆ

𝑌𝑣0 = 𝛾
𝑣 = 𝛾
𝑌
ˆ

T
𝑣0
+
𝑣, 𝑣0i
h ˆ

𝑊 𝑣0 +
T
𝑊
𝑣
+ ˆ

2

𝑣
ˆ

T
𝑣0

𝐸𝑣0,
T
𝑣
+ ˆ

𝐸

𝑣.
ˆ

Combining the two and rearranging we obtain the inequality

2 > 1

𝑣, 𝑣0i
h ˆ

1
𝛾

𝑂

−

max
𝑆𝑘
𝑣
∈

(cid:18)

T
𝑣

𝑊 𝑣

+

max
𝑆𝑘
𝑣
∈

T
𝑣

𝐸𝑣

.

(cid:19)

14

Now, this is where certiﬁed upper bounds come in to the picture. There is an easy certiﬁcate
(capture by SoS and the basic SDP) of the fact that for any matrix 𝑀, max𝑣
𝑘
Using such bound we get

𝑆𝑘 𝑣T𝑀𝑣 6

k∞

𝑀

k

∈

𝑣, 𝑣0i
h ˆ

2 > 1

1
𝛾

𝑂

−

T
𝑣

𝑊 𝑣

max
𝑆𝑘
𝑣
∈

(cid:18)

𝐸

𝑘

k

+

.

k∞(cid:19)

(2.1)

Eq. (2.1) already shows how an algorithm that can certify sparse eigenvalues is perturbation re-
𝑘, the inequality becomes
silient (in the sense of the previous paragraph). Indeed for

= 𝜀

𝐸

𝛾

k

k∞

·

/

𝑣, 𝑣0i
h ˆ

2 > 1

𝑂

𝜀

(

) −

−

1
𝛾

𝑂

max
𝑆𝑘
𝑣
∈

(cid:18)

T
𝑣

𝑊 𝑣

.

(cid:19)

(2.2)

𝑆𝑘 𝑣T𝑊 𝑣 it can obtain.
For the Wishart Model 𝑌 =

∈

T
𝛽𝑢0𝑣0

At this point, the guarantees of the algorithm depend only on the speciﬁc certiﬁed upper bound
on max𝑣

need to work with 𝑌T𝑌
𝑊T𝑊
guarantees of the algorithm depend only on the certiﬁed upper bound on max𝑣
it can obtain. For the rest of our preliminary discussion we go back to the Wishart model.

𝐸, the reasoning is essentially the same. However we
𝑛Id and carefully bound the cross terms. Similar to the Wigner model, the
𝑣

𝑆𝑘 𝑣T

𝑛Id

𝑊

p

−

+

+

−

∈

(cid:0)

2.3 New Certiﬁcates via basic SDP

For a matrix 𝑀

∈

ℝ𝑑

×

𝑑, the basic SDP program16

argmax

T
𝑌
h

𝑌, 𝑋

𝑋

(cid:23)

i

0, Tr 𝑋 = 1,

𝑋

k

k1 6 𝑘

can certify two types of upper bound:

(cid:8)

h

(cid:12)
(cid:12)

𝑀 , 𝑋

𝑀 , 𝑋

i

6

6

k

𝑀

𝑀

k∞ ·
.

(cid:9)

𝑘

0, Tr 𝑋 = 1. These are enough to
The ﬁrst follows using
capture standard principal component analysis as well as diagonal and covariance thresholding.

𝑋

(cid:23)

k

h

i
k1 6 𝑘 and the second applying 𝑋

k

k

Speciﬁcally, Eq. (2.5) can be used to certify the upper bound

(cid:17)
obtaining the guarantees of PCA – and Eq. (2.4) the bound
, as
in diagonal thresholding17. Now these results were already known, but surprisingly a combination
𝑊T𝑊
. Thus allowing us
of the two bounds can also be used to show
−
h
to match the guarantees of covariance thresholding.

𝑛Id, 𝑋

𝑛 log

6 𝑘

𝑘2

p

𝑑

/

𝑘

(cid:17)

(cid:16)

i

i

)

(

·

·

𝑊T𝑊
h
𝑊T𝑊
h

−

−
𝑛Id, 𝑋

𝑛Id, 𝑋

6 𝑂

𝑑

√𝑑𝑛

–

i
6 𝑂

+
(cid:16)
𝑛 log 𝑑

p

Concretely, using the notation from the introduction,

(cid:1)

(2.3)

(2.4)

(2.5)

16Recall

𝑋

k1 =

k

𝑋𝑖𝑗

𝑖,𝑗
17A more careful analysis can get 𝑘

∈[
Í

𝑑

](cid:12)
(cid:12)

(cid:12)
(cid:12)

T

𝑊

𝑊
h

−

𝑛Id, 𝑋

=

i

𝜂𝜏
h

𝑊
h

𝑊
T
(cid:0)

𝑊

T

𝑊

, 𝑋

𝑛Id
T

𝑊

−
𝜂𝑡

𝑊
(cid:1)

i+
, 𝑋

.

i

−

is the "absolute norm".

(cid:0)

(cid:1)

, but we ignore it here.

𝑛 log
(

𝑑

/

𝑘

)

·

p

15

Here 𝑊T𝑊

𝜂𝑡

𝑊T𝑊

is a matrix with entries bounded (in absolute value) by 𝜏 for which we

can plug in Eq. (2.4) and get

(cid:0)

(cid:1)

−

The same argument cannot be used for 𝜂𝜏

close (up to an addition of 𝑛
·
thresholding. Hence, taking 𝜏 =

T

𝑊

𝑊
h

𝜂𝑡

𝑊

, 𝑋

6 𝜏

𝑘

−

i
, but note that this matrix is suspiciously
Id) to the thresholded covariance matrix obtained in covariance

(cid:0)

·

T

𝑊
𝑊T𝑊
(cid:1)

(cid:0)

(cid:1)

and using Eq. (2.5), we get

𝑛 log

𝑘2

𝑑

(

/

)

p
𝑊

T

𝑊

𝜂𝜏
h

𝑛Id

, 𝑋

i

−

(cid:0)

(cid:1)

6 𝑂

𝑘

𝑛 log

r

𝑑
𝑘2 !

,

where we get the spectral bound (almost) for free by the analysis in [DM14].

2.4 New certiﬁcates via higher-level Sum-of-Squares

2.4.1 Certiﬁcates via Certiﬁable Subgaussianity

𝑛

×

ℝ𝑑.

𝑑. In particular we can exploit Gaussian moments bound 𝔼

The Sum-of-Squares algorithm can certify more reﬁned bounds on sparse eigenvalues of 𝑊
𝑁
𝑡

0, 1
)
(
ℕ, 𝑢
∈
Concretely let’s see how to use such property to obtain an identiﬁability proof of a bound on
𝑑 be the indicator
the 𝑘-sparse norm of 𝑊. To this end let 𝑣 be a 𝑘-sparse vector and let 𝑠
vector of its support (here we drop the subscript 𝑣0 to ease the notation). Using Cauchy-Schwarz,

∼
2𝑡 for all

𝑊𝑖 , 𝑢
h

2𝑡 6 𝑡 𝑡

0, 1

∈ {

· k

∈

𝑢

}

k

i

4 =

𝑊 𝑣

k

k

𝑣𝑖

𝑊𝑖 , 𝑊 𝑣
h

i!

Õ𝑖6𝑑

Then applying Holder’s inequality with 1
/
𝑘,

2

6

𝑣2
𝑖

!  

Õ𝑖6𝑑
𝑝

+

1
/

𝑠2
𝑖 h

𝑊𝑖 , 𝑊 𝑣

2

6

i

!

𝑠2
𝑖 h

𝑊𝑖 , 𝑊 𝑣

2

.

!

i

Õ𝑖6𝑑

Õ𝑖6𝑑
𝑡 = 1, and using the fact that 𝑠 is binary with norm

𝑠2
𝑖 h

𝑊𝑖 , 𝑊 𝑣

6

2

i

!

Õ𝑖6𝑑

This gets us to,

𝑝

1
/

𝑠

2𝑝
𝑖

!

Õ𝑖6𝑑

𝑊𝑖 , 𝑊 𝑣
h

i

2𝑡

!

Õ𝑖6𝑑

𝑡

1
/

6

𝑊 𝑣

k

2

k

·

1
𝑘1
/
−

𝑡

𝑊𝑖 ,
h

1
𝑊 𝑣

k

k

𝑊 𝑣

2𝑡

i

!

Õ𝑖6𝑑

𝑡

1
/

.

2 6 𝑘1
1
/
−

𝑡

𝑊 𝑣

k

k

𝑊𝑖 ,
h

1
𝑊 𝑣

k

k

𝑊 𝑣

2𝑡

i

!

·  

Õ𝑖6𝑑

𝑡

1
/

.

(2.6)

Now, whenever 𝑑 & 𝑛𝑡𝑡 𝑡 log𝑡 𝑛 , the 𝑡-moment of the column vectors 𝑊1 . . . , 𝑊𝑑 converges with
high probability. That is, for any unit vector 𝑢,

1
𝑑

Õ𝑖6𝑑

𝑊𝑖 , 𝑢
h

i

2𝑡 6 𝑂

𝑡 𝑡

.

)

(

(2.7)

Thus, combining Eq. (2.6) and Eq. (2.7) we can conclude

2 . 𝑘1
1
/
−

𝑡

𝑊 𝑣

k

k

𝑡

𝑑1
/

·

𝑡 .

·

The catch is that all the steps taken can be written as polynomial inequalities of degree at most
𝑡
. So we can certify the same bound through the Sum-of-Squares proof system.
)

(

𝑂

16

 
 
 
 
 
 
 
 
2.4.2 Certiﬁcates via Limited Brute Force

Whenever the sparse vector 𝑣0 is almost ﬂat, that is when for all 𝑖

supp

| ∈
, the guarantees of diagonal thresholding can be improved at the cost of increasing its

∈

{

|

we have

𝑣0𝑖

𝑣0}

1
𝐶√𝑘

, 𝐶
√𝑘

running time (see [DKWB19]).
h

i

Diagonal thresholding can be viewed as selecting the 𝑘 vectors of the standard basis 𝑒1, . . . , 𝑒𝑑
2, and then returning a top eigenvector of the covariance matrix projected onto
maximizing
the span of such vectors. Indeed this formulation has an intuitive generalization, namely instead
of looking at 1-sparse vectors, the algorithm could look into 𝑡-sparse vectors 𝑢 with entries in

𝑌𝑒𝑖

k

k

𝑘
𝑡

√𝑡, 0

𝑛

(cid:1)

(cid:0)

×

±

, pick the top

and use them to recover 𝑣0.

1
/
This idea can be translated into a certiﬁed upper bound for the sparse eigenvalues of 𝑊
0, 1
)
(

(cid:8)
(cid:9)
∼
𝑑. Although we will be able to recover general sparse vectors, for the sake of this discussion
𝑁
we assume 𝑣0 is ﬂat.18 Let’s denote the set of 𝑡-sparse ﬂat vectors by
ℝ𝑑 be a 𝑘-sparse
𝑡. Let 𝑣0 ∈
𝒩
𝑘.
vector and denote with 𝐷 the uniform distribution over the vectors in
𝑡 such that
𝒩
That is, the set of vectors 𝑢 such that supp
and with sign pattern matching the sign
pattern of 𝑣 restricted to supp
{
Note that for any matrix 𝑀

𝑢, 𝑣0i

𝑣0}

supp

} ⊆

p

𝑑,

=

𝑢

𝑢

/

{

{

h

𝑡

.
}
ℝ𝑑
∈

×

T
𝑣0

𝑀𝑣0 =

𝑘
𝑡

𝔼
𝐷
𝑢

∼

𝔼
𝐷
𝑢′∼

T
𝑢

𝑀𝑢′ .

This equality per se is not interesting, but for a Gaussian matrix 𝑊
bility,

𝑁

0, 1
)
(

∼

𝑛

×

𝑑, with high proba-

max
𝑢,𝑢′∈𝒩
Thus, combining the two we get

𝑡

(cid:12)
(cid:12)

T
𝑢

T

𝑊

𝑊

−

(cid:0)

𝑛Id

𝑢′

6 𝑂

𝑛𝑡 log 𝑑

.

(cid:1)

(cid:12)
(cid:12)

(cid:16)p

(cid:17)

T
𝑣0

T

𝑊

𝑊

(cid:0)

𝑛Id

−

(cid:1)

𝑣0 =

T
𝑢

T

𝑊

𝑊

𝑛Id

𝑢′

−
T

T
𝑢

𝑊

𝑊

𝔼
𝐷

𝑘
𝑡
6 𝑘
𝑡
6 𝑘
√𝑡

(cid:0)
max
𝑢,𝑢′∈𝒩

𝑡

(cid:12)
(cid:0)
(cid:12)
𝑛 log 𝑑 ,

(cid:1)
𝑛Id

𝑢′

−

(cid:1)

(cid:12)
(cid:12)

p
𝑘
𝑛 log 𝑑. This certiﬁcates can be proved using
which allows us to conclude that
√𝑡
Sum-of-Squares, hence allowing us to improve over the basic SDP by a factor 𝑡 in the settings
𝑘2 6 𝑑1
−

𝑊 𝑣0 k

2 6 𝑛

1
).

p

+

Ω

k

(

2.5 Concrete lower bounds for perturbation-resilient algorithms

Sparse principal component analysis is what we often call a planted problem. These are problems
that ask to recover some signal hidden by random or adversarial noise. The easiest way one
could formulate a planted problem is its distinguishing version: where given two distributions, a
null distribution without structure and a planted distribution containing the hidden signal, the

18So the Sum-of-Squares algorithm works in more general settings than the algorithm from [DKWB19].

17

objective is to determine with high probability whether a given instance was sampled from one
distribution or the other.

A common strategy to provide evidence for information-computation gap in a certain planted
problem is to prove that powerful classes of eﬃcient algorithms are unable to solve it in the
(conjecturally) hard regime. Indeed our goal here will be that of constructing two distributions
under which low-degree polynomials take roughly the same values and hence cannot distinguish
(in the sense of Section 3.3) from which distribution the instance 𝑌 was sampled. Since low-degree
𝐸 (and therefore cannot solve the
polynomials cannot tell if 𝑌 has indeed the form 𝑊
problem), this would mean they cannot be used to improve over the guarantees of Theorem 1.2.

T
𝛽𝑢0𝑣0

+

+

Our null distribution 𝜈 will be the standard Gaussian 𝑁

𝑑. However, the main question
T
is how to design the planted distribution 𝜇. Recall 𝑌 takes the form 𝑊
𝛽𝑢0𝑣0
𝐸. If we set
𝐸 = 0, then our planted distribution corresponds to the single spike covariance model. We could
get a lower bound for such problem (see Section 6) but this would not help us in showing that the
guarantees of Theorem 1.2 are tight. On the other hand, if for example we choose 𝐸 with the goal
of planting a large eigenvalue, then the problem of distinguishing between 𝜈 and 𝜇 may become
even easier than without adversarial perturbations.

0, 1
)
(

p

+

+

×

𝑛

p

+

+

p

T
𝛽𝑢0𝑣0

This suggests that we should choose 𝐸 very carefully, in particular we should design 𝐸 so
that 𝑌 = 𝑊
𝐸 appears – to the eyes of a low-degree polynomial estimator – as a
Gaussian distribution. Our approach will be that of constructing 𝐸 so that the ﬁrst few moments
of 𝜇 will be Gaussian. This will lead us to Theorem 1.3 through two basic observations: ﬁrst,
given two distributions with same ﬁrst 2𝑡 moments, computing those ﬁrst 2𝑡 moments won’t help
distinguishing between the two distributions. Second, for a Gaussian distribution 𝑁
, at least
)
𝑛𝑡 samples are required in order for the 2𝑡-th moment of the empirical distribution to converge to
𝔼

𝑤 ⊗
Concretely, we consider the following model: we choose iid gaussian vectors 𝑧1, . . . , 𝑧𝑛
(cid:2)
0, 1
)
(

1 ∼
−
ℝ𝑑 with iid symmetric (about zero) coordinates that satisﬁes

(cid:3)
𝑑, and a random vector 𝑧0 ∈

0, Id𝑛
(

2𝑡

.

𝑁
the following properties:

1. 𝑧0 has approximately 𝑘 large coordinates (larger than 𝜆

𝛽𝑛

/

≈

𝑘 by absolute value).

2. For any coordinate of 𝑧0 its ﬁrst 2𝑡

p
2 moments coincide with moments of 𝑁

higher 𝑟-moments (for even 𝑟) are close to 𝑘

𝑑 𝜆𝑟 .

−

0, 1
, and its
)
(

Then we obtain the matrix 𝑌
with rows 𝑧⊤0 , 𝑧⊤1 , . . . , 𝑧⊤𝑛
T
−
𝑊
𝛽𝑢0𝑣0

ℝ𝑛

𝑑 applying a random rotation 𝑅

𝑑 matrix
1. It is not diﬃcult to see that indeed such 𝑌 can be written as 𝑌 =

𝑛 to the 𝑛

ℝ𝑛

×

∈

∈

×

×

𝐸, as in the model of Problem 1.1.

+

p

+
Now, assume for simplicity that 𝑡 is constant and denote the distribution of 𝑌 described above
𝑑 by 𝜈. An immediate consequence of our
0, 1
by 𝜇 and the standard Gaussian distribution 𝑁
)
(
construction is that for any polynomial 𝑝 of degree at most 2𝑡
𝑌
𝑌
. Fur-
−
)
(
(
thermore, in order to reliably tell the diﬀerence between 𝔼𝜇
𝑌
𝑝′(
𝑊
𝑝′(
for a polynomial
(cid:3)
)
of even degree 𝑟 > 2𝑡 (say up to 𝑟 = 𝑛0.001), we will need a precise estimate of such 𝑟-th moments
(cid:2)
2>𝑡 samples. This eﬀect is then shown by proving that for multilinear polyno-
and hence at least 𝑛𝑟
/
𝑘 6 𝑛0.49, then the low-degree analogue
𝑌
mials 𝑝
(
of 𝜒2-divergence

is close to zero. Note that for technical reasons our

of degree 𝐷 6 𝑛0.001, if 𝑑 6 𝑛0.99𝑡

1 and 𝛽𝑛
−
2
))

𝜇
∼
and 𝔼𝜈
(cid:2)

𝔼𝜈 𝑝
(

2, 𝔼𝑌

= 𝔼𝑌

𝑌
(

𝑝

𝑝

/

(cid:2)

(cid:3)

(cid:3)

(cid:3)

(cid:2)

∼

×

)

)

)

𝑛

𝜈

max
of degree 6𝐷

𝑝

𝑌
(

)

𝑌
(
)−
𝕍𝜈 𝑃

𝔼𝜇 𝑝
𝑌
(

)

18

analysis is restricted to the multilinear polynomials. As shown in [BHK+16, HS17, Hop18] and as
it will be evident from the single spike model lower bound in Section 6.5, this restricted model of
computation captures the best known algorithms for many planted problems.

2.6 Beyond limitations of CT via low-degree polynomials

An important aspect of the computation of lower bounds for low-degree polynomials is that
they may provide valuable insight on how to construct an optimal algorithm. Indeed low-degree
polynomials capture many spectral properties of linear operators; for example, the largest singular
value of a 𝑑-dimensional linear operator with a spectral gap can be approximated by . log 𝑑 degree
polynomial in its entries.

We discuss here how they can be used to improve over the guarantees of Covariance Thresh-

olding

log 𝑑

Why Covariance Thresholding doesn’t work with small sample size.
In order to improve over
Covariance Thresholding, the ﬁrst question we need to understand is whether the algorithm could
actually work in a larger set of parameters than the one currently known. The answer is no. Recall
2 and 𝑛 6 𝑑 Covariance Thresholding (with an appropriate choice of thresholding
that for 𝑘2 6 𝑑
/
parameter 𝜏) works if 𝛽 & 𝑘
this is
√𝑛
asymptotically better than the guarantees of SVD, SVD+Thresholding and Diagonal Thresholding.
cannot have better

It is not diﬃcult to see that Covariance Thresholding with 𝜏 > Ω
(
guarantees than Diagonal Thresholding. So we consider 𝜏 6 𝑜
𝑛 log 𝑑
p
(
and 𝑛 > 𝑘2 imply 𝑛 > 𝑑1
𝑜
1
). The assumption 𝑛 > 𝑑1
1
)
−
−
p
is crucial for Covariance Thresholding. To show this, it is enough to prove that for some unit
𝑥
𝑣0
𝛽𝑛, this
are uncorrelated with
would mean that for 𝛽
(cid:12)
(cid:12)

1
). Indeed, as on the other hand
𝑛 the top eigenvectors of 𝜂𝜏(

𝑛 , and so for 𝑛 > 𝑘2 and 𝑑1

T𝜂𝜏(
𝑣0
𝑌⊤𝑌
(cid:12)
(cid:12)

𝑛 log 𝑑
.
)

ℝ𝑑, 𝑥T𝜂𝜏(

Note that 𝑑1
−

) 6 𝑘2 6 𝑜
1

) 6 𝑘2 6 𝑜
1

in these settings SVD+Thresholding has signiﬁcantly better

𝑛Id
)

𝑛Id
)

log 𝑑

𝑌⊤𝑌

𝑌⊤𝑌

𝑘2 +

q

≈

−

−

−

∈

𝑑

𝑑

1
)

−

(

(

)

)

)

𝑜

𝑜

𝑜

𝑜

(

(

(

(

(

𝑛Id
)
≪
𝑑
𝑛 ≪

𝑥 > 𝑑1
(
−
𝑜
1
𝑑1
(
)/
−
𝑜
𝑑1
−
𝑛

𝑣0. Additionally, since
guarantees.

q

(cid:16)

(cid:17)

ℝ𝑑 satisﬁes

∈

An 𝑥 satisfying our inequality is easy to ﬁnd, for example any row 𝑊1, . . . , 𝑊𝑛
T𝜂𝜏(

2 with high probability (see Appendix D).

𝑊𝑖 > 𝑑1
𝑛Id
−
)

1
) k

𝑌⊤𝑌

𝑊𝑖

−

k

𝑜

(

𝑊𝑖

Hence Covariance Thresholding doesn’t provide better guarantees than SVD or Diagonal

Thresholding if 𝑛 6 𝑑1
−

Ω

) (for example, if 𝑛 = 𝑑0.99).
1

(

2.6.1 Polynomials based algorithm

Theorem 1.7 shows that if 𝑘2 6 𝑑1
, it is unlikely that polynomial
−
time algorithms can solve the problem. So to get an asymptotic improvement over Diagonal
Thresholding we need 𝑘2 > 𝑑1
−

) and 𝛽 6 𝑜
1

log 𝑑

𝑘
√𝑛

1
).

p

(cid:17)

(cid:16)

𝑜

(

(

Ω

However, note that there is no condition 𝑛 > 𝑑1
−

1
) in our lower bound. This suggests that
there might be an algorithm that is asymptotically better than SVD and Diagonal Thresholding
for small 𝑛, for example 𝑛 = 𝑑0.99 or 𝑛 = 𝑑0.01. Indeed, we show that there exists a polynomial
) 6
1
as long as 𝑑1
time algorithm that can recover the sparse vector 𝑣0 with entries in
−
{
log 𝑛 and 𝑛 & log5 𝑑. In particular, if 𝑑1
𝑘2 6 𝑑
and 𝑑0.01 6
−

√𝑘
1
}
±
/
) 6 𝑘2 6 𝑜
1
(

log 𝑑

2, 𝛽 & 𝑘
√𝑛

log 𝑑

0,

𝑑

𝑜

𝑜

𝑜

(

(

(

)

/

𝑘2 +

q

19

𝑛 6 𝑑0.99, this algorithm has asymptotically better guarantees than Diagonal Thresholding, SVD,
SVD+Thresholding, and Covariance Thresholding.

(

(

)

)

(

(

)

×

𝑂

𝑂

log 𝑑

𝑛𝑑

𝑛𝑑

ℝ𝑑

) ∈

𝑌
(

log 𝑑
(

Our algorithm is based on the approach introduced in [HS17] for commutinity detection
in stochastic block model. An informal description of the algorithm is as follows: we compute
𝑑 whose entries are polynomials 𝑃𝑗 𝑗′(
some symmetric matrix 𝑃
in the entries of 𝑌
𝑌
of degree 𝑂
. The algorithm outputs a top eigenvector of this matrix, which we prove to
)
be highly correlated with 𝑣0. Note that since the degrees of involved polynomials are 𝑂
log 𝑑
,
)
(
simple evaluation takes time
). However, we can compute a very good approximation
1
) using a color coding technique (this part of the
to the values of these polynomials in time
algorithm uses internal randomness).
More precisely, for 𝑗, 𝑗′ ∈ [
we compute multilinear polynomials 𝑃𝑗 𝑗′(
𝑑
𝑌
]
such that for every 𝑗 ≠ 𝑗′, 𝔼 𝑃𝑗 𝑗′(
, and for every 𝑗
𝑌
, 𝑃𝑗 𝑗
𝑌
𝑗′)
𝑗
𝑣0(
(
)
)
T
2
𝑌
𝑣0𝑣0
that variance of 𝑃𝑗 𝑗′(
𝑌
𝑃
F < 𝑜
)
) −
(
T
𝑜
𝑣0𝑣0
𝑌
𝑃
, so the top eigenvector of 𝑃
,
1
1
1
)
(
(
k
−
)
(
𝑣0.
or
−
To bound the variance, we represent each monomial as a bipartite multigraph 𝐺 =

log 𝑑
of degree 𝑂
)
(
= 0. Then we show
. This implies that with probability
is highly correlated with either 𝑣0

𝑅, 𝐶 , 𝐸
,
)
with bipartition 𝑅
which correspond to columns
of 𝑌. Since the variance is a sum of monomials, we compute the contribution of each monomial and
bound the number of corresponding multigraphs. Finally, we show that there exists a polynomial
log 𝑛 and 𝑛 & log5 𝑑, there
such that in the parameter regime 𝑑1
−
is no group of monomials with large contribution in the variance, so we can conclude that this
polynomial is a good estimator.

= 𝑣0(
is small so that 𝔼
k
2
F < 𝑜

which corresponds to rows of 𝑌 and 𝐶

2, 𝛽 & 𝑘
√𝑛

) 6 𝑘2 6 𝑑
1

1
)
(
𝑌
)
(

log 𝑑

𝑘2 +

)
)

⊂ [

⊂ [

∈ [

) −

log 𝑑

q

𝑛

𝑑

𝑑

/

k

k

]

]

]

(

𝑜

(

log 𝑑
(

After showing that there are good polynomial estimators of degree 𝑂

, we approximately
)
compute them using color coding. All monomials of the polynomials 𝑃𝑗 𝑗′ that we consider have the
same structure (in the sence that the graphs corresponding to these monomials are isomorphic).
Each of them has the same number 𝑟 of vertices which correspond to rows and the same number 𝑐
in 𝑟 color and each
of vertices which correspond to columns. We show that for each coloring of
1
) compute the sum of monomials of 𝑃𝑗 𝑗′ colored
coloring of
[
exactly in colores from
. If we average these values over large enough set of random
𝑂
colorings (of size

𝑟
[
]
1
)), we get a value very close to 𝑃𝑗 𝑗′(
𝑌

One important advantage of this polynomial-based algorithm is that we only need the following
= 1.19 All previously known
assumptions on 𝑊: that the entries of 𝑊 are i.i.d., 𝔼 𝑊𝑖𝑗 = 0 and 𝔼 𝑊 2
𝑖𝑗
algorithms require bounds on entries or the spectral norm of 𝑊T𝑊 (or related matrices, e.g.
thresholded 𝑊T𝑊), so they require 𝜒2 tail bounds.

in 𝑐 colors, we can in time

and

𝑛𝑑

𝑛𝑑

.
)

𝑛

𝑑

𝑂

𝑐

]

[

]

[

]

)

(

)

(

(

(

3 Preliminaries

In this section, we introduce preliminary notions which will be used in the rest of the paper. We
start by deﬁning pseudo-distributions and sum-of-squares proofs (see the lecture notes [BS16] for
more details and the appendix in [MSS16] for proofs of the propositions appearing here). Then we
introduce the low-degree likelihood ratio (see [Hop18] for details).

19Indeed, prior work [DHS20] observed that polynomial-based algorithms require only ﬁrst and second moment

conditions on the noise entries for a broad range of matrix and tensor estimation problems.

20

Let 𝑥 =

𝑥1, 𝑥2, . . . , 𝑥𝑛

(

be a tuple of 𝑛 indeterminates and let ℝ
[

𝑥

]

)

with real coeﬃcients and indeterminates 𝑥1, . . . , 𝑥𝑛. We say that a polynomial 𝑝
squares (sos) if there are polynomials 𝑞1, . . . , 𝑞𝑟 such that 𝑝 = 𝑞2

𝑞2
𝑟 .

be the set of polynomials
is a sum-of-
𝑥

ℝ
[

∈

]

1 + · · · +

3.1 Pseudo-distributions

Pseudo-distributions are generalizations of probability distributions. We can represent a discrete
(i.e., ﬁnitely supported) probability distribution over ℝ𝑛 by its probability mass function 𝐷 : ℝ𝑛
→
ℝ such that 𝐷 > 0 and
= 1. Similarly, we can describe a pseudo-distribution by
its mass function. Here, we relax the constraint 𝐷 > 0 and only require that 𝐷 passes certain
low-degree non-negativity tests.

supp

Í

𝐷

𝑥

𝐷

)

(

∈

𝑥

(

)

𝑓

)

(

𝑥

𝑥

𝑥 𝐷

𝑥 𝐷

= 1 and

Concretely, a level-ℓ pseudo-distribution is a ﬁnitely-supported function 𝐷 : ℝ𝑛

ℝ such that
2. (Here, the
summations are over the support of 𝐷.) A straightforward polynomial-interpolation argument
Í
-pseudo distribution satisﬁes 𝐷 > 0 and is thus an actual probability
shows that every level-
distribution. We deﬁne the pseudo-expectation of a function 𝑓 on ℝ𝑑 with respect to a pseudo-
distribution 𝐷, denoted 𝔼𝐷

2 > 0 for every polynomial 𝑓 of degree at most ℓ
)

→
/

∞

Í

𝑥

𝑥

)

(

(

𝑓

𝑥

(

)

(

, as
)

𝔼𝐷

𝑥

(

)

𝑓

𝑥

(

)

=

𝐷

𝑓

𝑥

(

)

(

𝑥

)

.

Õ𝑥

(3.1)

The degree-ℓ moment tensor of a pseudo-distribution 𝐷 is the tensor 𝔼𝐷
ℓ . In
particular, the moment tensor has an entry corresponding to the pseudo-expectation of all mono-
mials of degree at most ℓ in 𝑥. The set of all degree-ℓ moment tensors of probability distribution
is a convex set. Similarly, the set of all degree-ℓ moment tensors of degree 𝑑 pseudo-distributions
is also convex. Key to the algorithmic utility of pseudo-distributions is the fact that while there
can be no eﬃcient separation oracle for the convex set of all degree-ℓ moment tensors of an actual
probability distribution, there’s a separation oracle running in time 𝑛𝑂
) for the convex set of the
degree-ℓ moment tensors of all level-ℓ pseudodistributions.

1, 𝑥1, 𝑥2, . . . , 𝑥𝑛

)⊗

)(

ℓ
(

𝑥

(

Fact 3.1 ([Sho87, Par00, Nes00, Las01]). For any 𝑛, ℓ
separation oracle (in the sense of [GLS81]):

∈

ℕ, the following set has a 𝑛𝑂

ℓ
(

)-time weak

𝔼𝐷

𝑥

(

1, 𝑥1, 𝑥2, . . . , 𝑥𝑛

)(

𝑑

⊗

)

|

degree-d pseudo-distribution 𝐷 over ℝ𝑛

.

(3.2)

(cid:8)

This fact, together with the equivalence of weak separation and optimization [GLS81] allows
us to eﬃciently optimize over pseudo-distributions (approximately)—this algorithm is referred to
as the sum-of-squares algorithm.

(cid:9)

The level-ℓ sum-of-squares algorithm optimizes over the space of all level-ℓ pseudo-distributions

that satisfy a given set of polynomial constraints—we formally deﬁne this next.

=

Deﬁnition 3.2 (Constrained pseudo-distributions). Let 𝐷 be a level-ℓ pseudo-distribution over ℝ𝑛.
𝑓1 > 0, 𝑓2 > 0, . . . , 𝑓𝑚 > 0
be a system of 𝑚 polynomial inequality constraints. We say
Let
}
that 𝐷 satisﬁes the system of constraints
and every
sum-of-squares polynomial ℎ with deg ℎ

, if for every 𝑆

⊆ [

𝒜

𝒜

𝑚

{

]

at degree 𝑟, denoted 𝐷 𝑟 𝒜
6 ℓ ,
+
{
𝑓𝑖 > 0 .
𝔼𝐷 ℎ
Í

deg 𝑓𝑖 , 𝑟

𝑆 max

}

∈

𝑖

·

𝑆
Ö𝑖
∈

21

Î

if and

𝒜

We write 𝐷

(without specifying the degree) if 𝐷

holds approximately if the above inequalities are satisﬁed up to an error of 2−

𝑓𝑖
k
denotes the Euclidean norm20 of the coﬃcients of a polynomial in the monomial basis.

𝑆 k

∈

0 𝒜

holds. Furthermore, we say that 𝐷 𝑟 𝒜
ℎ
, where
· k

k ·

𝑛ℓ

𝑖

𝒜

k·k

We remark that if 𝐷 is an actual (discrete) probability distribution, then we have 𝐷

only if 𝐷 is supported on solutions to the constraints

.

𝒜

We say that a system
𝑥

2 6 𝑀

of the form

of polynomial constraints is explicitly bounded if it contains a constraint

𝒜
. The following fact is a consequence of Fact 3.1 and [GLS81],

{k

k

}

Fact 3.3 (Eﬃcient Optimization over Pseudo-distributions). There exists an
that, given any explicitly bounded and satisﬁable system21
outputs a level-ℓ pseudo-distribution that satisﬁes

𝒜
approximately.

)-time algorithm
𝑚
of 𝑚 polynomial constraints in 𝑛 variables,

+

𝑛

)

(

𝑂

ℓ
(

𝒜

3.2 Sum-of-squares proofs

Let 𝑓1, 𝑓2, . . . , 𝑓𝑟 and 𝑔 be multivariate polynomials in 𝑥. A sum-of-squares proof that the constraints
𝑓1 > 0, . . . , 𝑓𝑚 > 0
{
such that

consists of sum-of-squares polynomials

imply the constraint

𝑔 > 0

)𝑆

𝑝𝑆

⊆[

{

}

}

𝑚

(

]

𝑔 =

𝑚
Õ𝑆
⊆[

]

𝑝𝑆

Π𝑖

∈

·

𝑆 𝑓𝑖 .

(3.3)

We say that this proof has degree ℓ if for every set 𝑆
𝑓𝑖 > 0
most ℓ . If there is a degree ℓ SoS proof that

𝑚
⊆ [
𝑖 6 𝑟

{

|

, the polynomial 𝑝𝑆Π𝑖
𝑔 > 0
implies

∈
, we write:

{

}

𝑆 𝑓𝑖 has degree at

𝑓𝑖 > 0

{

|

𝑖 6 𝑟

} ℓ {

.

}

(3.4)

]
}
𝑔 > 0

Sum-of-squares proofs satisfy the following inference rules. For all polynomials 𝑓 , 𝑔 : ℝ𝑛

ℝ
ℝ𝑛 such that each of the coordinates of

→

and for all functions 𝐹 : ℝ𝑛
the outputs are polynomials of the inputs, we have:

ℝ𝑚, 𝐺 : ℝ𝑛

ℝ𝑘, 𝐻 : ℝ𝑝

→

→

→

𝑓 > 0, 𝑔 > 0
𝑔 > 0

𝑓

}

, 𝒜 ℓ {

𝑓 > 0

,

}

𝒜 ℓ

ℓ ′ {
+

}

+

𝒜 ℓ {

𝒜 ℓ {
,

𝒜 ℓ ℬ

𝒜 ℓ

𝐻

𝐹

(

{

)

𝐶

} ℓ {

ℬ ℓ ′
𝐶
ℓ ′
·
𝐹 > 0
{
> 0

} ℓ

𝐺 > 0

deg
(

·

𝐻

) {

𝐺

(

}
𝐻

.

> 0

}

)

𝑔 > 0

𝒜 ℓ ′ {
𝑔 > 0
𝑓

·

}

}

(addition and multiplication)

(transitivity)

(substitution)

Low-degree sum-of-squares proofs are sound and complete if we take low-level pseudo-

distributions as models.

Concretely, sum-of-squares proofs allow us to deduce properties of pseudo-distributions that

satisfy some constraints.

20The choice of norm is not important here because the factor 2−
21Here, we assume that the bitcomplexity of the constraints in

is

𝑛

(

𝑚

)

+

𝑂

1
).

(

𝒜

𝑛ℓ

swamps the eﬀects of choosing another norm.

22

for a level-ℓ pseudo-distribution 𝐷 and there exists a sum-of-squares

Fact 3.4 (Soundness). If 𝐷 𝑟 𝒜
, then 𝐷
proof

.

𝒜 𝑟′ ℬ

𝑟
·
If the pseudo-distribution 𝐷 satisﬁes

𝑟′ ℬ

𝑟′+

𝒜

only approximately, soundness continues to hold if
𝐵 (number of bits

we require an upper bound on the bit-complexity of the sum-of-squares
required to write down the proof).

𝒜 𝑟′

In our applications, the bit complexity of all sum of squares proofs will be 𝑛𝑂

ℓ
(

all numbers in the input have bit complexity 𝑛𝑂
pseudo-distributions that satisfy polynomial constraints approximately.

(

) (assuming that
1
)). This bound suﬃces in order to argue about

The following fact shows that every property of low-level pseudo-distributions can be derived

by low-degree sum-of-squares proofs.
Fact 3.5 (Completeness). Suppose 𝑑 > 𝑟′ > 𝑟 and
𝑛
𝑖=1 𝑥2
at most 𝑟, and
for some ﬁnite 𝐵.
𝑖
𝑔 > 0

6 𝐵

Let

𝒜

{

satisﬁes 𝐷

}

𝒜 ⊢ {
be a polynomial constraint. If every degree-𝑑 pseudo-distribution that satisﬁes 𝐷 𝑟 𝒜
}
𝑔 > 0
𝑟′ {

, then for every 𝜀 > 0, there is a sum-of-squares proof

𝒜 𝑑 {

𝑔 >

Í

−

𝜀

}

}

.

also

We will repeatedly use the following SoS version of Cauchy-Schwarz inequality and its gener-

is a collection of polynomial constraints with degree

alization, Hölder’s inequality:

Fact 3.6 (Sum-of-Squares Cauchy-Schwarz). Let 𝑥, 𝑦

ℝ𝑑 be indeterminites. Then,

∈

𝑥,𝑦

4





2

6

𝑥𝑖 𝑦𝑖

!

Õ𝑖

𝑥2
𝑖

Õ𝑖

!  

Õ𝑖

𝑦2
𝑖

! 



We will also use the following fact that shows that spectral certiﬁcates are captured within the

SoS proof system.

Fact 3.7 (Spectral Certiﬁcates). For any 𝑚

𝑢
2

h

×
𝑢, 𝐴𝑢

𝑚 matrix 𝐴,

6

𝐴

k

kk

𝑢

2
2

k

i

.

We will also use the following Cauchy-Schwarz inequality for pseudo-distributions.

(cid:8)

(cid:9)

Fact 3.8 (Cauchy-Schwarz for Pseudo-distributions). Let 𝑓 , 𝑔 be polynomials of degree at most 𝑑 in
indeterminate 𝑥

ℝ𝑑. Then, for any degree d pseudo-distribution 𝐷, 𝔼𝐷

𝔼𝐷

𝔼𝐷

𝑔2

𝑓 𝑔

𝑓 2

6

.

∈

3.3 Low-degree likelihood Ratio

p

[

]

[

]
p

[

]

The low-degree likelihood ratio is a proxy to model eﬃciently computable functions. It is closely
related to the pseudo-calibration technique and it has been developed in a recent line of work on
the Sum-of-Squares hierarchy [BHK+16, HS17, HKP+17b, Hop18]. Our description is also based
on [BKW20b].

The objects of study are distinguishing versions of planted problems, in which given two
distributions and an instance, the goal is to decide from which distribution the instance was
sampled. For example, in the context of Sparse PCA, the distinguishing formulation takes the form
of deciding whether the matrix 𝑌 was sampled according to the (planted) distribution as described
𝑑. In general, we denote
in 1.1, or if it was sampled from the (null) Gaussian distribution 𝑁
with 𝜈 the null distribution and with 𝜇 the planted distribution with the hidden structure.

0, 1
)
(

×

𝑛

23

 
 
3.3.1 Background on Classical Decision Theory

From the point of view of classical Decision Theory, the optimal algorithm to distinguish between
, the
two distribution is well-understood. Given distributions 𝜈 and 𝜇 on a measurable space
22 is the optimal function to distinguish whether 𝑌
likelihood ratio 𝐿
𝜈 or
𝑌

:= 𝑑ℙ𝜇(
𝑌
𝜇 in the following sense.

𝑑ℙ𝜈(
𝑌

𝑌
(

𝒮
∼

)/

)

)

∼

Proposition 3.9. [NP33] If 𝜇 is absolutely continuous with respect to 𝜈, then the unique solution of the
optimization problem

max 𝔼
𝜇

𝑓

𝑌
(

)

subject to 𝔼
𝜈

𝑓

𝑌
(

2
)

= 1

is the normalized likelihood ratio 𝐿

(cid:2)
𝔼𝜈

)/

𝑌
(

(cid:3)
𝐿

𝑌
(

2
)

(cid:2)

(cid:3)

and the value of the optimization problem is 𝔼𝜈

𝐿

𝑌
(

2
)

.

(cid:3)
Similarly, arguments about statistical distinguishability are known as well. Unsurprisingly, the

(cid:2)

(cid:2)

(cid:3)

likelihood ratio plays a major role here as well. The key concept is the Le Cam’s contiguity.

Deﬁnition 3.10. [Cam60] Let 𝜇 =
a common probability space
𝑛, ℙ𝜇(
for 𝐴𝑛

(cid:0)
𝒮
0 then ℙ𝜈(
𝐴𝑛

) →

∈ 𝒮

𝐴𝑛

(cid:1)
) →

0.

ℕ and 𝜈 =

𝜇𝑛

∈
𝑛. Then 𝜇 and 𝜈 are contiguous, written 𝜇 ⊳ 𝜈, if as 𝑛

ℕ be sequences of probability measures on
, whenever

𝜈𝑛

)𝑛

(

∈

𝑛

→ ∞

Contiguity allows us to capture the idea of indistinguishability of probability measures. Indeed
two contiguous sequences 𝜇, 𝜈 of probability measures are indistinguishable in the sense than
there is no function 𝑓 :
𝑌
𝜇 and
(
𝑓
𝜈. The key tool now is the so called Second Moment
Method, which allows us to establish contiguity through the likelihood ratio.

𝑛
𝒮
= 0 with high probability whenever 𝑌

= 1 with high probability whenever 𝑌

such that 𝑓

→ {

𝑌
(

0, 1

∼

∼

}

)

)

Proposition 3.11. If 𝔼𝜈

𝐿𝑛

𝑌
(

2
)

remains bounded as 𝑛

, then 𝜇 ⊳ 𝜈.

→ ∞

This discussion allows us to argue whether a given function can be used to distinguish between

(cid:2)

(cid:3)

our planted and null distributions.

3.3.2 Background on the Low-degree Method

The main problem with the likelihood ratio is that it is in general hard to compute, thus we need to
restrict these classical analysis to the space of eﬃciently computable functions. Concretely, we use
low-degree multivariate polynomials in the entries of the observation 𝑌 as a proxy for eﬃciently
computable functions. Denoting with ℝ6𝐷
the space of polynomials in 𝑌 of degree at most 𝐷
we can establish a low-degree version of the Neyman-Pearson lemma.

𝑌
[

]

Proposition 3.12 (e.g. [Hop18]). The unique solution of the optimization problem

max
ℝ6𝐷[
𝑌
∈

]

𝑓

𝔼
𝜇

𝑓

𝑌
(

)

subject to 𝔼
𝜈

𝑓

𝑌
(

2
)

= 1

(cid:3)
is the normalized orthogonal projection 𝐿6𝐷
𝑌
(
and the value of the optimization problem is 𝔼𝜈

(cid:2)

𝔼𝜈
)/
𝐿6𝐷

𝐿6𝐷
2
𝑌
(cid:2)
)
(

2
)

𝑌
(
.

(cid:3)

(cid:3)

(cid:2)
of the likelihood ratio 𝐿

𝑌
(

)

onto ℝ6𝐷

𝑌
[

]

22The Radon-Nikodym derivative

(cid:2)

(cid:3)

24

It is important to remark that at the heart of our discussion, there is the belief that in the
study of planted problems, low-degree polynomials capture the computational power of eﬃciently
computable functions. This can be phrased as the following conjecture.

Conjecture 3.13 (Informal). [BHK+16, HS17, HKP+17b, Hop18] For "nice" sequences of probability
measures 𝜇 and 𝜈, if there exists 𝐷 = 𝐷
remains bounded as
𝑑

log 𝑑
, then there is no polynomial-time algorithm that distinguishes in the sense described in 3.3.1.23

for which 𝔼𝜈

> 𝜔

𝐿6𝐷

𝑌
(

2
)

𝑑

(

)

(cid:0)

(cid:1)

(cid:2)

(cid:3)

→ ∞

A large body of work provide support for this conjecture (see any of the citations above),
mostly in the form of evidence of an intimate relation between polynomials and Sum of Squares
algorithms and lower bounds. For a more in detail discussion we point the interested reader to
[HKP+17b, Hop18].

4 Resilience of the basic SDP and Certiﬁed Upper Bounds

In this section we show the guarantees of the basic SDP algorithm [dGJL05, AW09], thus proving
Theorem 1.4.

2

×

k

k

∞

∈

𝑀

ℝ𝑑

𝑀 𝑥

2 6 𝑘

We will ﬁrst prove that for any matrix 𝑀

𝑑 the basic SDP can certify an upper bound
on 𝑘-sparse quadratic forms over 𝑀. Furthermore we will show that for
k
· k
𝑑 this bound can be signiﬁcantly improved in various
random Gaussian matrices 𝑊
ways, depending on the regime. Most notably, we will show that the basic SDP can certify a bound
, thus matching the guarantees of Covariance Thresholding.
k
As a corollary, we also get that for 𝛽 < 1 the algorithm achieves the best known guarantees among
polynomial time algorithms in both the fragile and the robust settings.
Formally the Sparse PCA problem can be deﬁned as follows.

0, 1
)
(

2 6 𝑛

𝑛 log

𝑘2, 𝑛

min

𝑊 𝑥

p

})

𝑁

+

∼

𝑑

/

𝑘

{

k

×

(

𝑛

Problem 4.1. Given an instance 𝑌 of 1.1 let ˆ
Σ𝑣
ˆ

argmax

T
𝑣

k
k0 is the number of non-zero entries in 𝑣.

(cid:12)
(cid:12)
(cid:12)

n

k

where

𝑣

k

𝑣

2 = 1,

𝑣

k0 6 𝑘

k

o

Σ = 𝑌T𝑌. Then the Sparse PCA problem is deﬁned by

Solving Problem 4.1 is NP-hard in general [MWA06, Nat95, KNV+15], however the following

concrete SDP relaxation [dGJL05] can be eﬃciently solved

argmax

Σ, 𝑋
h ˆ

i

𝑋

(cid:23)

0, Tr 𝑋 = 1,

𝑋

k1 6 𝑘

k

o

n

(cid:12)
(cid:12)
(cid:12)

𝑋𝑖𝑗

is the "absolute norm". We will show how to recover 𝑣0 using such

(SDP-1)

𝑋

where

k
program.

k1 =

𝑖,𝑗

𝑑
∈[
Í

](cid:12)
(cid:12)

(cid:12)
(cid:12)

We start by restating some of the notation from the introduction. For a set 𝑆
]𝑖𝑗 = 𝑀𝑖𝑗 if
the matrix with entries 𝑀
𝑆
[
ℝ𝑑
ℝ, we deﬁne 𝜂𝜏(
𝑀
) ∈

𝑑, we denote by 𝑀
]𝑖𝑗 = 0 otherwise. For a matrix 𝑀

𝑑 and 𝜏

𝑆
]
ℝ𝑑

[
∈

ℝ𝑑

∈

∈

×

×

×

𝑑
𝑖, 𝑗

𝑑
, and
]
] × [
𝑆, and
) ∈
𝑑 to be the matrix

⊆ [
(

a matrix 𝑀
𝑀
with entries

𝑆

[

23We do not explain what "nice" means and direct the reader to [Hop18].

=

𝑀

𝜂𝜏(

)

0

(

𝑀𝑖𝑗

if

𝑀𝑖𝑗

> 𝜏

otherwise.
(cid:12)
(cid:12)

(cid:12)
(cid:12)

25

Furthermore, we deﬁne 𝜁𝜏(

𝑀

) ∈

ℝ𝑑

×

𝑑 to be the matrix with entries

=

𝑀

𝜁𝜏(

)

𝑀𝑖𝑗

0

(

−

sign

𝑀𝑖𝑗

𝜏

·

(cid:0)

(cid:1)

if

𝑀𝑖𝑗

> 𝜏

otherwise.
(cid:12)
(cid:12)

(cid:12)
(cid:12)

4.1 Basic Certiﬁcates for Sparse Quadratic Forms

We show here what certiﬁcates over sparse quadratic forms SDP-1 can provide. These certiﬁcates
are already enough to match the best known guarantees in the weak signal regime. The ﬁrst
observation is that it is straightforward to bound the product between 𝑋 and matrices with small
inﬁnity norm. By construction of 𝑋 this is indeed a certiﬁcate of an upper bound over 𝑘-sparse
quadratic forms.

Lemma 4.2. For 𝑘

ℕ, let 𝑋

∈

∈

ℝ𝑑

×

𝑑 such that

k

𝑋

k1 6 𝑘. Then for any matrix 𝑀
6 𝑘

𝑀

.

ℝ𝑑

𝑑

×

∈

𝑀 , 𝑋

|h

i|

· k

k∞

Proof. The Lemma follows immediately by choice of 𝑋,

𝑋 , 𝑀

=

i|

|h

𝑑
Õ𝑖,𝑗
∈[

]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

𝑀𝑖𝑗 𝑋𝑖𝑗(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

6

𝑀𝑖𝑗 𝑋𝑖𝑗

6

𝑑
Õ𝑖,𝑗
∈[

](cid:12)
(cid:12)

(cid:12)
(cid:12)

𝑀

k

k∞

𝑋𝑖𝑗

6 𝑘

𝑑
Õ𝑖,𝑗
∈[

](cid:12)
(cid:12)

(cid:12)
(cid:12)

𝑀

.

k∞

· k

(cid:3)

Now we improve this bound for random matrices. In particular we look into the Hilbert-Schmidt

inner product

𝑊T𝑊

𝜂𝜏
h

𝑛Id

, 𝑋

.

i

−

(cid:0)
Lemma 4.3. Let 𝑋
0, 1
𝑊
)
(

𝑁

∼

×

𝑛

×
𝑑, then with probability 1

∈

𝑜

1
)
(

−

ℝ𝑑

𝑑 be a positive semideﬁnite matrix such that Tr 𝑋 = 1 and

(cid:1)

𝑋

k

k1 6 𝑘. Let

T

𝑊

𝑊
h

−

𝑛Id, 𝑋

i

6 𝑂

min

𝑘

(

s

𝑛 log

1

(cid:18)

𝑑
𝑘2 +

+

𝑑
𝑘√𝑛 (cid:19)

𝑛

+

√𝑑𝑛

, 𝑑

+

.

)!

(cid:12)
(cid:12)

Proof. By Theorem G.6,

(cid:12)
(cid:12)

𝑊T𝑊

k

𝑛Id

k

−

6 𝑂

𝑑

√𝑑𝑛

with probability 1

𝑑−

10, so by Lemma H.5

−

+

(cid:16)

T

𝑊

𝑊
h

−

𝑛Id, 𝑋

i

(cid:17)
6 𝑂

(cid:12)
(cid:12)

(cid:12)
(cid:12)

−

Let 𝐷

] × [
𝜏 > 0 we can rewrite the matrix

⊆ [

be the set of diagonal entries of
𝑊T𝑊

𝑛Id

as

𝑑

𝑑

]

T

𝑊

𝑊

−

𝑛Id =

T

𝑊

𝑊

𝑛Id

−

(cid:0)
𝐷
] +

[

T

𝜂𝜏

𝑊

(cid:1)
𝑊

(cid:0)
Now, by Fact G.4 with probability 1

(cid:1)

−

𝑑

+
(cid:16)
𝑊T𝑊

√𝑑𝑛

.

(cid:17)
𝑛Id

−

and ¯𝐷 its complement. For any

𝑛Id

−
𝑊T𝑊

(cid:0)
𝑜
,
1
)
(
(cid:13)
(cid:0)
= 𝜁𝜏
[ ¯𝐷
(cid:13)
]

𝑛Id

(cid:1) (cid:2)
−
T

𝑊

(cid:3)
𝑛Id

𝑊

(cid:1)
−

(cid:0)
¯𝐷

T

𝑊

+

[

(cid:0)
𝐷

]
(cid:13)
𝑛Id
(cid:13)

(cid:1)

(cid:1)

𝑊

−
6 10

𝜂𝜏

𝑊

T

𝑊

𝑛Id

−

𝑛Id

−

.

¯𝐷

(cid:1)(cid:1) (cid:2)
𝑛 log 𝑑. Furthermore,

(cid:0)

(cid:3)

p
] +

𝑀 ,

[ ¯𝐷

𝜂𝜏

𝑊

T

𝑊

−

(cid:0)

(cid:1)

(cid:0)

26

 
where 𝑀

such that

ℝ𝑑
×
𝑊T𝑊

𝑑 is a matrix with
k
6 𝐶

𝑛Id

𝑀

𝑑

∈
𝜁𝜏

−

]

[ ¯𝐷
. If 𝑑 6 𝑛,

(cid:1)

(cid:13)
(cid:13)

𝑛 log

1

r

(cid:16)

(cid:13)
(cid:13)
+

(cid:0)
𝑑
𝑘2 +

𝑑
𝑘√𝑛

𝑛

+

(cid:17)

k∞

√𝑑𝑛

+

(cid:16)

6 𝜏 and by Theorem G.12 there is a constant 𝐶 > 1

exp

𝜏2
𝐶𝑛

i

−

h

(cid:17)

with probability 1

𝑜

. Let 𝜏 = 10𝐶
1
)
(

·

−

2√𝑑𝑛

·  

𝑘√𝑛

𝑘

+

+

𝑛

𝑑

(

+

𝑘

) +

𝑘√𝑑 !

6 10𝐶 𝑘√𝑛 .

𝜁𝜏

𝑊

T

𝑊

𝑛Id

[ ¯𝐷

]

−

(cid:13)
(cid:13)
If 𝑘2 6 𝑛 6 𝑑,

(cid:0)

(cid:1)

(cid:13)
(cid:13)

6 3𝐶 𝑘

𝑛

q

𝑘√𝑛

+

𝜁𝜏

𝑊

T

𝑊

𝑛Id

[ ¯𝐷

]

−

6 𝐶 𝑘2

And if 𝑛 6 max

𝑘2, 𝑑

,

}

{

(cid:13)
(cid:13)

(cid:0)

(cid:1)

(cid:13)
(cid:13)

𝜁𝜏

𝑊

T

𝑊

𝑛Id

[ ¯𝐷

]

−

6 𝐶

𝑛

(

+

𝑘√𝑛

So, applying Lemma H.5, we get

(cid:13)
(cid:13)

(cid:0)

(cid:1)

(cid:13)
(cid:13)

p

2𝑑

𝑘2

·

(cid:18)

𝑑

(cid:19)

+

6 2𝐶 𝑘√𝑛 .

2𝑑
𝑘√𝑛

𝑑 (cid:19)

+

) ·

𝑛

(cid:18)

+

6 4𝐶 𝑘√𝑛 .

T

𝑊

𝑊

𝜂𝜏
h

𝑛Id

−

, 𝑋

¯𝐷

Since 𝑋 is 𝑘-bounded,

(cid:0)

(cid:12)
(cid:12)

(cid:1) (cid:2)

(cid:3)

i

(cid:12)
(cid:12)

(cid:13)
(cid:13)

(cid:0)

6

𝜁𝜏

𝑊

T

𝑊

𝑛Id

−

¯𝐷

𝑀 , 𝑋

i|

+ |h

6 2𝑘𝜏.

(cid:12)
Hence with probability 1
(cid:12)

T

𝑊

𝑊

h

(cid:0)
−

𝑜

1
)
(

T

𝑊

𝑊
h

−

𝑛Id, 𝑋

(cid:12)
(cid:12)

i

(cid:12)
(cid:12)

(cid:3)(cid:13)
(cid:13)

, 𝑋

(cid:1) (cid:2)

¯𝐷

6 𝑘𝜏.

i

(cid:12)
(cid:12)

𝑛Id

−

−

𝜂𝜏

𝑊

T

𝑊

𝑛Id

−

(cid:0)

(cid:1)(cid:1) (cid:2)

(cid:3)

6 30𝐶 𝑘

𝑛 log

2

(cid:18)

s

𝑑
𝑘2 +

+

𝑛

+

6 100𝐶 𝑘

𝑛 log

2

(cid:18)

s

𝑑
𝑘2 +

+

𝑛

+

𝑑
𝑘√𝑛 (cid:19)
𝑑
𝑘√𝑛 (cid:19)

+

since if 𝑘 6 log 𝑑, log

2

(cid:16)

> 1

2 log 𝑑.

𝑑
𝑘2

+

(cid:17)

4.2 The basic SDP Algorithm

10

𝑛 log 𝑑

p

,

(cid:3)

Having providing certiﬁcates on sparse quadratic form, we can now use Eq. (SDP-1) to obtain a
robust algorithm for Sparse PCA.

Algorithm 4.4 (SDP-based Algorithm).

Input: Sample matrix 𝑌 =

·
Estimate: The sparse vector 𝑣0.

p

𝛽

𝑢0𝑣𝑇

0 +

𝑊

𝐸

+

∈

ℝ𝑛

×

𝑑 from 1.1.

Operation:

1. Compute matrix 𝑋

∈

ℝ𝑑

×

𝑑 solving program SDP-1.

27

2. Output top eigenvector

𝑣 of 𝑋.
ˆ

Indeed we will show that Algorithm 4.4 is perturbation resilient (in the sense of Appendix C)
and its guarantees matches those of the state-of-the-art fragile algorithms such as SVD, Diagonal
Thresholding and Covariance Thresholding. The following theorem formalize this result.

Theorem 4.5. Let 𝑌 be a 𝑛-by-𝑑 matrix of the form,

𝑌 =

T
𝑢0𝑣0

𝛽

·

𝑊

+

+

𝐸 ,

ℝ𝑛

for a unit 𝑘-sparse vector 𝑣0 ∈
𝑑 and a Gaussian matrix 𝑊
𝐸
algorithm 4.4 outputs a unit vector
𝑣
ˆ

ℝ𝑑, a standard Gaussian vector 𝑢0 ∼
𝑁
ℝ𝑑 such that with probability 1

, an arbitrary matrix
0, Id𝑛
)
(
𝑑 such that 𝑊 , 𝑢0, are distributionally independent. Then
,
1
)
(

0, 1
)
(

∼
∈

𝑁

−

∈

𝑜

×

×

𝑛

p

1

𝑣0,

𝑣
ˆ

i

− h

2 . 𝑘

𝛽𝑛 ·

𝑞

+ s

𝑘
𝛽𝑛  r

log

𝑑
𝑘 + k

𝐸

k1

→

2

! ·  

1

+

1

𝛽 !

.

where 𝑞 := min

𝑑
𝑘2 +
Furthermore, the same kind of guarantees hold if 𝑢0 is a vector with

𝑛 log

𝑑
𝑘√𝑛

(cid:26)r

and

k1

+

𝐸

→

2

(cid:27)

k

(cid:16)

(cid:17)

+

+

𝑛

, 𝑑

√𝑑𝑛
𝑘

2 denotes the largest norm of a column of 𝐸.

𝑢0k

k

2 = Θ

𝑛

(

)

independent of 𝑊.

p

We prove Theorem 4.5 through the result below, which will be useful in the Sum-of-Squares

proofs as well.

Theorem 4.6 (Meta-theorem). Let 𝑌 be a 𝑛-by-𝑑 matrix of the form,

𝑌 =

T
𝑢0𝑣0

𝛽

·

𝑊

+

+

𝐸 ,

ℝ𝑛

for a unit 𝑘-sparse vector 𝑣0 ∈
𝑑 and a Gaussian matrix 𝑊
𝐸
0, 1
×
)
(
Σ, 𝑋
Let 𝑋 be a feasible solution of SDP-1 satisfying
h ˆ

𝑁

∼

∈

×

𝑛

p

ℝ𝑑, a standard Gaussian vector 𝑢0 ∼

, an arbitrary matrix
0, Id𝑛
)
(
𝑑 such that 𝑊 , 𝑢0, 𝑣0 are distributionally independent.

𝑁

>

i

T
Σ, 𝑣0𝑣0
h ˆ

. Then with probability 1

i

𝑜

,
1
)
(

−

T
𝑣0𝑣0

, 𝑋

1

− h

. 1

𝛽𝑛 ·

i

T

𝑊

𝑊
h

−

𝑛Id, 𝑋

i

𝑘
𝛽𝑛  r

log

𝑑
𝑘 + k

𝐸

+ s

k1

→

2

! ·  

1

+

1

𝛽 !

,

2 denotes the largest norm of a column of 𝐸. Furthermore, the same kind of guarantees hold if

p

where
𝐸
k1
𝑢0 is a vector with

→

k

2 = Θ

𝑛

independent of 𝑊.

)
Indeed Theorem 4.6 immediately implies Theorem 4.5.

k

(

𝑢0k

(cid:12)
(cid:12)

Proof of Theorem 4.5. Assume Theorem 4.6 is true. By deﬁnition 𝑋 satisﬁes its premises. By
Lemma 4.3

T

𝑊

𝑊
h

−

𝑛Id, 𝑋

i

6 𝑂

min

Applying Lemma H.3 the result follows.

(cid:12)
(cid:12)

𝑘

(

s

𝑛 log

1

(cid:18)

𝑑
𝑘2 +

+

𝑑
𝑘√𝑛 (cid:19)

𝑛

+

√𝑑𝑛

, 𝑑

+

.

)!

(cid:3)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Now let’s prove Theorem 4.6. First we look into cross-terms containing the signal.

28

 
Lemma 4.7. Let 𝑌 be as in Theorem 4.6 and suppose 𝐸

ℝ𝑛

×

𝑑 is a matrix with maximal column norm

2 6 𝑏. Let 𝑋 be a feasible solution to SDP-1. Then with probability 1

∈

𝑜

,
1
)
(

−

𝐸

k

k1

→

𝑊
h

(cid:12)
(cid:12)
(cid:12)
,
1
)
(

p
𝑢0k

T

T
𝛽𝑢0𝑣0

, 𝑋

6 𝑂

𝛽𝑛 𝑘 log

𝑑
𝑘 !

.

i

 r

(cid:12)
(cid:12)
(cid:12)
. Let 𝑔 = 1
𝑢0
)
k

Proof. With probability 1
𝑔

6 𝑂
. Let 𝑆 be the set of 𝑘 largest coordinates in 𝑔, and let 𝑔′ = 𝑔
0, 1
)
(

√𝑛
(

𝑁

∼

−

𝑜

k

k

vector 𝑔′′ has entries bounded by 𝑂

(cid:18)q
Lemma G.10). Hence by Lemma H.5,

log 𝑑
𝑘

(cid:19)

and

𝑔′k

k

6 𝑂

(cid:18)q

(cid:19)

𝑊T𝑢0. Since 𝑢0 and 𝑊 are independent,
. Then 𝑔 = 𝑔′ +
𝑔′′, where
[
with probability 1

(by

𝑆

𝑜

]

𝑘 log 𝑑
𝑘

1
)
(

−

T
𝛽𝑢0𝑣0

, 𝑋

T

𝑊
h

p

(cid:12)
(cid:12)
(cid:12)

6 𝑂

6 𝑂

i

(cid:12)
(cid:12)
(cid:12)

T
𝑔′𝑣0

, 𝑋

𝑛𝛽

h

i

+

𝑛𝛽

h

T
𝑔′′𝑣0

, 𝑋

i

(cid:16)p

 r

(cid:12)
(cid:12)

𝛽𝑛 𝑘 log

(cid:12)
𝑑
(cid:12)
𝑘 ! +

p

(cid:12)
(cid:12)
𝑛𝛽

𝑂

(cid:16)p

h

(cid:12)
(cid:12)

T
𝑔′′𝑣0

, 𝑋

.

(cid:17)
(cid:12)
(cid:12)
i

(cid:17)
(cid:12)
(cid:12)

By Lemma H.6 and Lemma H.5,

T
𝑔′′𝑣0

, 𝑋

h

6

i

The desired bound follows from Lemma 4.2, since the entries of 𝑔′′
with probability 1

𝑜

𝑔′′

𝑔′′

T, 𝑋

𝑣0𝑣0

T, 𝑋

6

i

i · h

𝑔′′

𝑔′′

T, 𝑋

.

i

h

q

h

q

(cid:12)
(cid:12)

(cid:0)

(cid:1)

(cid:12)
(cid:12)

−

.
1
)
(

(cid:0)
𝑔′′

(cid:1)
T are bounded by 𝑂

(cid:0)

(cid:1)

log 𝑑
𝑘 )
(
(cid:3)

Lemma 4.8. Let 𝑌 be as in Theorem 4.6 and suppose 𝐸

ℝ𝑛

×

𝑑 is a matrix with maximal column norm

2 6 𝑏. Let 𝑋 be a feasible solution to SDP-1. Then with probability 1

∈

𝑜

,
1
)
(

−

𝐸

k

k1

→

Proof. With probability 1
of 𝑧 are bounded by 𝑂

𝑜
−
𝑏√𝑛

T
𝐸

T
𝛽𝑢0𝑣0

, 𝑋

6 𝑂

𝑏

𝛽𝑛 𝑘

.

h

i
(cid:17)
. Let 𝑧 = 𝐸T𝑢0. With probability 1
√𝑛
,
1
(
(
)
)
. By Lemma H.6 and Lemma H.5,

p
6 𝑂
𝑢0k

(cid:12)
(cid:12)
(cid:12)
k

p

(cid:12)
(cid:12)
(cid:12)

(cid:16)

𝑜

1
)
(

−

the entries

(cid:0)
T
𝑧𝑣0

, 𝑋

6

i

(cid:1)

(cid:12)
(cid:12)

h

p

h

(cid:12)
(cid:12)

𝑧𝑧T, 𝑋

𝑣0𝑣0

T, 𝑋

6

i

i · h

𝑧𝑧T, 𝑋

i

h

p

6 𝑂

𝑏√𝑛 𝑘

.

(cid:16)

(cid:17)

(cid:3)

The following lemma shows how to bound the remaining cross-terms.

Lemma 4.9. Let 𝑌 be as in Theorem 4.6 and suppose 𝐸
2 6 𝑏. Let 𝑋 be a feasible solution to SDP-1. Then

𝐸

k

k1

→

ℝ𝑛

×

𝑑 is a matrix with maximal column norm

∈

T
𝐸

𝑊

T

𝑊

𝐸, 𝑋

6 2𝑏√𝑘𝑛

h
(cid:12)
Proof. Applying Fact H.4 with setting 𝐴 =
(cid:12)
immediately get

+

(cid:12)
(cid:12)

i

𝑊
(

−

𝑏2𝑘

T

𝑊

𝑊
h

+

−

𝑛Id, 𝑋

.

i

𝐸

T
)

(cid:12)
𝑊
(cid:12)
(

·

𝑐

𝐸

)

·

−

for some 𝑐 > 0 and 𝐵 = 𝑋 we

(cid:12)
(cid:12)

+

𝑐

T
𝐸

𝑊

+

T

𝑊

𝐸, 𝑋

𝑐

h

(cid:12)
(cid:12)

i

(cid:12)
(cid:12)

T

6

𝑊
h

𝑊 , 𝑋

𝑐2

h

i +

T
𝐸

𝐸, 𝑋

= 𝑛

T

𝑊

𝑊

+ h

−

i

𝑛Id, 𝑋

𝑐2

h

i +

T
𝐸

𝐸, 𝑋

.

i

29

By Lemma 4.2

T
𝐸

𝑊

h

+

T

𝑊

𝐸, 𝑋

Minimizing over 𝑐, we get

(cid:12)
(cid:12)

6 1
𝑐

𝑛

+

𝑊
h

T

𝑊

𝑛Id, 𝑋

i

+

𝑐

·

−

𝑏2𝑘 .

(cid:0)

(cid:12)
(cid:12)

(cid:12)
(cid:1)
(cid:12)

i

(cid:12)
(cid:12)

T
𝐸

𝑊

+

T

𝑊

𝐸, 𝑋

h

(cid:12)
(cid:12)

i

(cid:12)
(cid:12)

6 2𝑏

𝑘𝑛

6 2𝑏√𝑘𝑛
p
6 2𝑏√𝑘𝑛

+

+

+

𝑘

· |h
𝑘

2𝑏

𝑏2𝑘
p

+

𝑊T𝑊

𝑛Id, 𝑋

−
𝑊T𝑊
T

𝑊

i|
𝑛Id, 𝑋

−
𝑛Id, 𝑋

−

· |h
𝑊
h

i

i|
.

(cid:12)
(cid:12)

(cid:3)

We are now ready to prove Theorem 4.6.

Proof of Theorem 4.6. Opening up the product,

(cid:12)
(cid:12)

𝑛

Σ, 𝑋
h ˆ

i

=

Σ
h ˆ
=𝛽

k

−
𝑢0k
𝑊
T
𝐸
T
𝐸

+ h

+ h

+ h

𝛽

h

+

𝑛Id
2

𝑛Id, 𝑋
+
T
𝑣0𝑣0

, 𝑋

i

i +
𝑛Id, 𝑋

i

T

h
𝑊

−
𝐸, 𝑋

𝑊

i
𝑊
+
T
𝑣0𝑢0

𝑊

T

𝐸, 𝑋

i
T
𝑊

+

Applying Lemmata 4.2, 4.3, 4.9, 4.8, 4.7 and we get

p

Σ, 𝑋
h ˆ

i

6 𝛽

𝑢0k

k

2

h

T
𝑣0𝑣0

, 𝑋

𝑛

2

𝑊
h

+

i +

T

𝑊

−

𝑛Id, 𝑋

T
𝑢0𝑣0

+

T
𝑣0𝑢0

𝐸

+

T
𝐸

T
𝑢0𝑣0

, 𝑋

.

i

2𝑏2𝑘

+

2𝑏√𝑘𝑛

𝑂

+

log

𝑑
𝑘 +

𝑏

!

  r

𝛽𝑛 𝑘

.

!

p

i

+

(cid:12)
(cid:12)

Furthermore, by choice of 𝑋,

Σ, 𝑋
h ˆ

i

>

T
Σ, 𝑣0𝑣0
h ˆ
Σ
h ˆ
>𝛽

𝑛Id
2

=

i

−
𝑛

T
𝑛Id, 𝑣0𝑣0

(cid:12)
(cid:12)

i

+
𝑊

T

T
𝑛Id, 𝑣0𝑣0
−
T
𝐸, 𝑣0𝑣0

i

i
T
T
𝐸, 𝑣0𝑣0
(cid:12)
(cid:12)
T

𝑊

𝑊

+

(cid:12)
(cid:12)
i
T
(cid:12)
𝑢0𝑣0
(cid:12)

𝑊

𝑊

+
T
𝑣0𝑢0

h

h

+
𝑢0k
𝑊
h
T
𝐸
T
𝐸

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
p
(cid:12)
(cid:12)
𝑢0k

𝛽

h

k

−

−

−

−

>𝛽

k

>𝛽

𝑢0k

k

T
𝑣0𝑢0

𝐸

+

+

T
𝐸

T
𝑢0𝑣0

T
, 𝑣0𝑣0

2

2

𝑛

2

𝑊
h

−

+

T

𝑊

−

T
𝑛Id, 𝑣0𝑣0

2𝑏2𝑘

−

i

−

2𝑏√𝑘𝑛

(cid:12)
(cid:12)
𝑂

𝑛

+

−

T

𝑊

𝑊
h

−

(cid:12)
(cid:12)
T
𝑛Id, 𝑣0𝑣0

𝑏2𝑘

−

i

−

𝑏√𝑘𝑛

(cid:12)
(cid:12)

(cid:12)
(cid:12)

30

i

(cid:12)
(cid:12)
(cid:12)
−

𝑂

log

  r

𝑑
𝑘 +

𝑏

!

𝛽𝑛 𝑘

!

p

log

𝑑
𝑘 +

𝑏

!

−  r

𝛽𝑛 𝑘

.

!

p

 
𝑜

𝑊T𝑊
Now by Theorem G.9
h
𝑊T𝑊
. Let 𝑚 =
1
1
(cid:12)
h
)
(
(cid:12)
rearranging, we get
(cid:12)
(cid:12)

−

−

−
𝑛Id, 𝑋

i

+

(cid:12)
(cid:12)

(cid:12)
(cid:12)

T
𝑛Id, 𝑣0𝑣0

i
𝑊T𝑊
(cid:12)
h
(cid:12)

−

/
i

(cid:12)
(cid:12)

6 10𝑘 log
𝑑
(
T
𝑛Id, 𝑣0𝑣0

20

𝑛 𝑘 log

𝑘
𝑘
with probability
) +
)
. Combining the two inequalities and

𝑑

/

(

p

2

𝛽

𝑢0k

k

·

1

− h

T
𝑣0𝑣0

, 𝑋

6 𝑂

𝑚

𝑏2𝑘

+

+

𝑏√𝑘𝑛

i

(cid:0)

With probability 1

𝑜

,
1
)
(

𝑢0k

k

−

2 > 𝑛

/

(cid:1)
2. Recall that 𝑏 6

𝛽𝑛
𝑘 . Hence

q

log

+  r

𝑑
𝑘 +

𝑏

!

𝛽𝑛 𝑘

.

!

p

T
𝑣0𝑣0

, 𝑋

1

− h

6 1
𝛽𝑛

i

𝑂

𝑚

𝛽𝑛 𝑘 log

+ r

𝑑
𝑘 +

1

+

𝛽

𝑏√𝑘𝑛

(cid:0)

(cid:1)

The result follows rearranging and observing that with probability 1
𝑘 log
by Lemma G.9.

𝑑

𝑑

𝑘

𝑘

(

/

) +

𝑘𝑛 log
(

/

)

p

𝑜

,
1
)
(

−

(cid:12)
(cid:12)

.

!

𝑊T𝑊
h

−

T
𝑛Id, 𝑣0𝑣0

.
(cid:3)

i

(cid:12)
(cid:12)

5 Resilience of SoS and Stronger Certiﬁed Upper Bounds

In this section we prove Theorem 1.2 and Theorem 1.5. We will show that the Sum-of-Squares
algorithm can certify various upper bounds on sparse eigenvalues. In Section 5.1 we will prove
increasingly stronger certiﬁed upper bounds on sparse eigenvalues of subgaussian matrices. These
certiﬁed upper bounds will require increasingly stronger assumptions on 𝑑 and 𝑛, but for degree
will approach information theoretic guarantees. In Section 5.2 we will prove alternative
log
certiﬁed upper bounds fo sparse eigenvalues of Gaussian matrices. These bounds will not require
any additional assumption on 𝑑 and 𝑛. We will then use these bounds in Section 5.3 to obtain
maximally robust algorithms for Sparse PCA.

𝑑

/

𝑘

)

(

5.1 SoS Certiﬁcates for Sparse Eigenvalues via Certiﬁable Subgaussianity

𝑠,𝑣 be the following system of quadratic constraints. Observe for any

Let
is a 𝑘-sparse unit vector supported on coordinates 𝑖 such that 𝑠𝑖 = 1.

𝒜

𝑠, 𝑣

(

)

satisfying

𝑠,𝑣, 𝑣

𝒜

(5.1)

𝑖

𝑖

∀

∀

𝑑

𝑑

.

.

]

]

∈ [

∈ [

𝑑
𝑖=1 𝑠𝑖 = 𝑘
𝑠2
= 𝑠𝑖
𝑖
𝑣𝑖 = 𝑣𝑖

Í
𝑠𝑖
𝑑

·

𝑣2
𝑖

= 1

Õ𝑖=1




𝑠,𝑣 :

𝒜




ℝ𝑛

We prove a certiﬁed upper bound for sparse eigenvalues of random rectangular matrices
𝑑 with independent subgaussian entries. This upper bound diﬀers considerably from
𝑊
the one obtained using SDP-1. Let us recall the deﬁnition of subgaussian random variables before
proceeding.





∈

×

Deﬁnition 5.1 (𝐶-Subgaussian Random Variables). A ℝ-valued random variable 𝑥 is said to be
𝐶-subgaussian if for every 𝑡, 𝔼

𝑡 6 𝐶 𝑡

𝑥

2𝑡 𝑡
/

2.
/

|

|

31

 
 
Let 𝑊1, 𝑊2, . . . , 𝑊𝑑 be the columns of 𝑊. We will use the following lemma:

ℝ𝑛 be independently drawn from a product distribution with each
Lemma 5.2. Let 𝑊1, 𝑊2, . . . , 𝑊𝑑
1-subgaussian coordinates with mean 0 and variance 1. Then, with probability at least 0.99 over the draw of
𝑊1, 𝑊2, . . . , 𝑊𝑑,

∈

𝑢
2𝑡

1
𝑑

(

Õ𝑖6𝑑

𝑊𝑖 , 𝑢
h

i

2𝑡 6

𝑢

2𝑡
2

k

k

𝑡 𝑡

+

𝑛𝑡

2 log(
/

𝑡

𝐶′𝑡

𝑛

(

)(

𝑡

)

2
1
+
)/
√𝑑

.

!)

for some absolute constant 𝐶′ > 0.

We will prove the lemma whenever the columns of 𝑊 are certiﬁably subgaussian. Informally,
certiﬁably subgaussianity means that a random variable has its moments upper-bounded as in the
the deﬁnition above and that this bound has a SoS proof. Formally, we have:

Deﬁnition 5.3 (Certiﬁable Subgaussianity). A ℝ𝑛-valued random variable 𝑌 is said to be 𝑡-
certiﬁably 𝐶-subgaussian if for all 𝑡′ 6 𝑡, 2𝑡
𝑑 is
𝑌, 𝑢
h
said to be 𝑡-certiﬁably 𝐶-subgaussian if the uniform distribution on the columns of 𝑊 is 𝑡-certiﬁably
𝐶-subgaussian.

. A matrix 𝑊

2𝑡 6 𝐶 𝑡𝑡 𝑡

𝑌, 𝑢
h

ℝ𝑛

𝔼

𝔼

∈

n

o

i

i

×

(cid:1)

(cid:0)

𝑢

2

𝑡

Certiﬁable subgaussianity has, by now, appeared in several works [KS17b, KS17a, HL18,

KKM18] that employ the sum-of-squares method for statistical estimation problems.

Given the above lemma, to prove Lemma 5.2, we need to show certiﬁed subgaussianity of 𝑊

when 𝑊 is a random matrix in ℝ𝑛

×

𝑑. To show this, we will use the following fact:

Fact 5.4 (Certiﬁable Subgaussianity of Product Subgaussians, Lemma 5.9, Page 25 of [KS17b]). Let
𝑌 be a ℝ𝑑-valued random variable with independent, 𝐶-subgaussian coordinates of mean 0 and variance 1.
Then, 𝑌 is 𝑡-certiﬁably 𝐶-subgaussian for every 𝑡.

We are now ready to prove Lemma 5.2.

Proof of Lemma 5.2. We have:

𝑢
2𝑡

1
𝑑  

(

𝑊𝑖 , 𝑢
h

i

Õ𝑖6𝑑
Using Fact 3.7 and

2𝑡

𝔼

𝑊𝑖 , 𝑢
h

i

−

2𝑡

=

!

*

𝑢 ⊗

𝑡 ,

𝑡

𝑢 ⊗

=

2
2

𝑢

k

k

2𝑡
2 , we have:

1
𝑑

Õ𝑖6𝑑

𝑡

𝑊 ⊗
𝑖

𝑡

𝑊 ⊗
𝑖

⊤

𝔼

𝑡

𝑊 ⊗
𝑖

𝑡

𝑊 ⊗
𝑖

−

(cid:0)

(cid:1) (cid:0)

(cid:1)

(cid:0)

(cid:1) (cid:0)

𝑡

𝑢 ⊗

.

+)

⊤

!

(cid:1)

𝑢
2𝑡

1
𝑑

(

(cid:13)
(cid:13)
𝔼

(cid:13)
(cid:13)
2𝑡
i

−

𝑊𝑖 , 𝑢
h

𝑊𝑖 , 𝑢
h

i

2𝑡 6

𝑢

2𝑡
2 ·

k

k

1
𝑑

Õ𝑖6𝑑

(cid:13)
(cid:13)
(cid:13)
From Lemma G.3, we know that with probability at least 0.99 over the draw of 𝑊1, 𝑊2, . . . , 𝑊𝑑, it
(cid:13)
holds that:

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:1) (cid:0)

(cid:1) (cid:0)

(cid:18)

(cid:1)

(cid:1)

(cid:0)

(cid:0)

𝑡

𝑊 ⊗
𝑖

𝑡

⊤

𝑊 ⊗
𝑖

𝔼

𝑡

𝑊 ⊗
𝑖

𝑡

⊤

𝑊 ⊗
𝑖

.

(5.2)

−

)

(cid:13)
(cid:18)
(cid:13)
Using Fact 5.4,
(cid:13)
(cid:13)

1
𝑑

𝑡

𝑊 ⊗
𝑖

𝑡

𝑊 ⊗
𝑖

⊤

𝔼

𝑡

𝑊 ⊗
𝑖

−

𝑊 ⊗
𝑖

𝑛𝑡

2 log(
/

𝑡

𝑡

⊤

6

(cid:0)

(cid:1) (cid:0)

(cid:1)

𝑢
2𝑡

(cid:0)

(cid:8)

(cid:19)(cid:13)
(cid:13)
(cid:1)
(cid:13)
(cid:13)
2𝑡 6 𝑡 𝑡

(cid:1) (cid:0)

𝔼

𝑊𝑖 , 𝑢
h

i

32

𝑢

2𝑡
2

k

k

.

(cid:9)

𝐶′𝑡

𝑛

(

)(

𝑡

)

.

2
1
+
)/
√𝑑

(5.3)

(5.4)

 
 
Combining (5.2), (5.3) and (5.4), we have:

𝑢
2𝑡

1
𝑑

(

Õ𝑖6𝑑

𝑊𝑖 , 𝑢
h

i

2𝑡 6

𝑢

2𝑡
2

k

k

𝑡 𝑡

+

𝑛𝑡

2 log(
/

𝑡

𝐶′𝑡

𝑛

(

)(

𝑡

)

2
1
+
)/
√𝑑

.

!)

(cid:3)

Lemma 5.2 implies the following lemma:

Lemma 5.5. Let 𝑊 satisfy the assumptions of Lemma 5.2. Suppose that 𝑑 > 𝑡 𝑡 𝑛𝑡 log(
probability at least 0.99,

𝑡

1
)
+

𝑛

. Then with
)

(

for some absolute constant 𝐶′ > 0.

𝑠,𝑣
2𝑡

𝑠,𝑣

𝒜

𝑊 𝑣

k

4𝑡
2 6 𝑑𝑘𝑡

1
−

𝐶′𝑡

(

𝑡

)

k

𝑊 𝑣

2𝑡
2

k

.

(cid:9)

k

(cid:8)

and Cauchy-Schwarz inequality, we have:

Proof. For 𝑢 = 𝑊 𝑣, using

𝑠
𝑠,𝑣 2𝑡

𝒜

{

𝑠𝑖𝑣𝑖 = 𝑣𝑖

𝑖

| ∀

}

2𝑡

𝑠,𝑣,𝑢
2𝑡

𝑠,𝑣

𝒜

Using

𝑠
𝑠,𝑣 2𝑡

𝒜

{

𝑠𝑡
𝑖

1
−

= 𝑠2
𝑖

𝑠𝑖𝑣𝑖

𝑊𝑖 , 𝑢
h

i!

Õ𝑖6𝑑

6

𝑣2
𝑖

!

Õ𝑖6𝑑

, we have:

𝑖

}




| ∀

𝑠,𝑣

𝒜

Now, using

𝑠,𝑣
2

𝑠,𝑣

𝒜

{

Í

𝑠,𝑣,𝑢
𝑡



𝑖 𝑠𝑖 = 𝑘


}

2𝑡

6

𝑠𝑖𝑣𝑖

𝑊𝑖 , 𝑢
h

i!

Õ𝑖6𝑑

𝑣2
𝑖

!

Õ𝑖6𝑑

Õ𝑖6𝑑

𝑊𝑖 , 𝑢

𝑠2
𝑖 h

2

i

!

and Lemma 5.2, we have:

𝑠,𝑣,𝑢
𝑡

𝑠,𝑣

𝒜

𝑠𝑖

𝑊𝑖 , 𝑢
h

i

𝑡

2

!

=

1𝑑
−

2𝑡
2

𝑢

k

k

𝑡 𝑡

+

n

Õ𝑖6𝑑
6 𝑘𝑡

𝑠𝑡
𝑖

1
−

𝑡

6

𝑊𝑖 , 𝑢
h

i

2

!

𝐶′𝑡

𝑛

(

)(

𝑡

)

𝑡

2
1
+
)/
√𝑑

Õ𝑖6𝑑
𝑛𝑡

2 log(
/

𝑡

1
−

𝑠𝑡
𝑖

!

Õ𝑖6𝑑

2𝑡

𝑊𝑖 , 𝑢
h

i

!

Õ𝑖6𝑑

!

o

Plugging back 𝑢 = 𝑊 𝑣, we get the desired bound.

Now we are ready to derive the certiﬁed upper bound on

𝑊 𝑣

k

2
2.

k

(5.5)

(cid:3)

Lemma 5.6. Suppose that 𝑑 > 𝐶∗𝑡 𝑡𝑛𝑡 log𝑡
distribution satisfying
×
Then, with probability at least 0.99 over the draw of 𝑊1, 𝑊2, . . . , 𝑊𝑑,

𝑠,𝑣. Let 𝑊

ℝ𝑛

𝒜

∈

𝑛

for large enough absolute constant 𝐶∗. Let 𝐷 be a pesudo-
(
𝑑 with i.i.d. 1-subgaussian entries with mean 0 and variance 1.

)

for some absolute constant 𝐶′ > 0.

𝔼𝐷

𝑊 𝑣

k

k

2
2 6 𝐶′

·

𝑑1
/

𝑡 𝑘1
−

1
𝑡 𝑡 ,

33

𝑠2
𝑖 h

𝑊𝑖 , 𝑢

2

i

!

Õ𝑖6𝑑

𝑡

𝑡

𝑡

𝑡









 
 
 
 
 
 
 
 
 
 
 
 
Proof. Using Lemma 5.5 and taking pseudo-expectations with respect to 𝐷 that satisﬁes
have:

𝒜

𝑠,𝑣, we

𝑊 𝑣

k

k

2𝑡
2 6

2
1
/

, and by

𝔼𝐷

𝑊 𝑣

k

4𝑡
2

k

(cid:16)

(cid:17)

𝔼𝐷

𝑊 𝑣

k

k

4𝑡
2 6 𝑑𝑘𝑡

1
−

𝐶′𝑡

(

)

𝑡 𝔼𝐷

𝑊 𝑣

k

2𝑡
2 .

k

By Cauchy-Schwarz inequality for pseudo-distributions, 𝔼𝐷

Hölder’s indequality

𝔼𝐷

𝑊 𝑣

k

2
2

k

(cid:16)

2𝑡

(cid:17)

Taking 𝑡-th roots gives: 𝔼𝐷

𝑊 𝑣

k

k

𝔼𝐷

k
(cid:16)
2 6 𝐶′ ·

2

𝑡

2
2

k

(cid:17)
𝑡 𝑘1
−

𝑊 𝑣

𝑑1
/

6 𝔼𝐷

𝑊 𝑣

k

k

4𝑡
2 . Thus, we have:

6 𝑑𝑘𝑡

1
−

𝐶′𝑡

(

𝑡 .

)

1
𝑡 𝑡.

(cid:3)

5.2 SoS Certiﬁcates for Sparse Eigenvalues via Limited Brute Force

We show here that, using additional constraints over the system
certiﬁed upper bounds on the sparse eigenvalues of Gaussian matrices 𝑊.

𝒜

𝑠,𝑣, we can provide diﬀerent

𝑡 be a set of all vectors with values in

Let
We start with a deﬁnition.

𝒮

0, 1

}

{

that have exactly 𝑡 nonzero coordinates.

Deﬁnition 5.7. For any 𝑢

𝑡 we deﬁne a polynomial in variables 𝑠1, . . . , 𝑠𝑑 =: 𝑠

∈ 𝒮

=

𝑝𝑢

𝑠

(

)

1
−

·

𝑘
𝑡

(cid:19)

(cid:18)

𝑠𝑖 .

Ö𝑖
supp
{
∈

𝑢

}

Note that if 𝑣 denotes a 𝑘-sparse vector and 𝑠 is the indicator of its support, then for any 𝑢

𝑡,

∈ 𝒮

=

𝑝𝑢

𝑠

(

)

1
−

𝑘
𝑡
0
((cid:0)

(cid:1)

if supp

𝑢

{

} ⊆

supp

𝑣

{

}

otherwise

Now consider the following system

𝑠,𝑣 of polynomial constraints.

ℬ

𝑠,𝑣 :

ℬ

𝑖

∀

𝑑

,

]

∈ [

𝑖

∀

𝑑

,

]

∈ [

𝑖

∀

𝑑

,

]

∈ [

𝑠2
= 𝑠𝑖
𝑖
𝑠𝑖 = 𝑘

]
𝑣𝑖 = 𝑣𝑖
𝑣2
= 1
𝑖

𝑖

𝑑
∈[
Í
𝑠𝑖
·

𝑖

𝑑
∈[
Í
𝑝𝑢

Õ𝑢
𝑡
∈𝒮
𝑢𝑖 𝑝𝑢

Õ𝑢
𝑡
∈𝒮

]

𝑠

(

)

= 1

=

𝑠

(

)

𝑡
𝑘 ·

𝑠𝑖









We will use the following preliminary fact.

Fact 5.8. Let 𝑊
×
principle submatrices of 𝑊T𝑊

0, 1
)
(

𝑁

∼

𝑛

𝑑, let 𝑛 > log 𝑑 and let 𝑡 6 1

log 𝑑 min

𝑑, 𝑛

{
𝑡 have spectral norm bounded by 𝑂

}


. Then with probability 1

𝑛Id of size 𝑡

−

×

𝑛𝑡 log 𝑑

.

(cid:16)p

(cid:17)

34

(5.6)

𝑜

1
)
(

−

all

𝑑
𝑡
p
(cid:1)
(cid:0)

Proof. Fix a 𝑡
6 𝐶
k
k
possible

𝑁

×

𝑛𝑡 log 𝑑 with probability at least 1

𝑡 principal submatrix 𝑁. By Theorem G.6 there exists a constant 𝐶 > 0, such that
𝑑10𝑡. The fact follows taking a union bound over all
(cid:3)

−

submatrices.

We are now ready to show the upper bound on quadratic forms of sparse vectors.

Theorem 5.9. Let 𝑊
1

𝑜

,
1
)
(

−

𝑁

0, 1
)
(

∼

𝑛

×

𝑑. Then there exists a constant 𝐶 > 0 such that with probability at least

𝑠,𝑣

ℬ

𝑠,𝑣
4𝑡

𝑘
√𝑡

𝐶

·

−

(cid:26)

Proof. Note that

T
𝑛 log 𝑑 6 𝑣

T

𝑊

𝑊

𝑛Id

𝑣 6 𝐶

−

p

(cid:0)

(cid:1)

𝑘
√𝑡

·

p

𝑛 log 𝑑

.

(cid:27)

T =

𝑠𝑠

𝑠,𝑣
2𝑡

(

𝑘2
𝑡2

𝑠,𝑣

ℬ

T
𝑢′𝑢

𝑠

𝑝𝑢′(

)

𝑝𝑢

𝑠

(

.

))

Õ𝑢,𝑢′∈𝒮

𝑡

For vectors 𝑥, 𝑦

∈

ℝ𝑑 we denote the vector with entries 𝑥𝑖

𝑦𝑖 by

𝑥 𝑦

. It follows that

·

𝑠,𝑣

ℬ

𝑠,𝑣

ℬ

𝑠,𝑣
4𝑡

𝑠,𝑣
4𝑡

T =

𝑣𝑣

T =

𝑣𝑣

(cid:8)

(

Let 𝑀 =

𝑊T𝑊

𝑛Id

. Then

−

𝑣𝑠

(
𝑘2
𝑡2

𝑣𝑠

T
)

)(

(cid:0)

(cid:1)

(cid:9)
𝑣𝑢′
(

)(

𝑣𝑢

T
)

𝑠

𝑝𝑢′(

)

𝑝𝑢

𝑠

(

.

))

Õ𝑢,𝑢′∈𝒮

𝑡

(cid:0)

(cid:1)
𝑠,𝑣

ℬ

𝑠,𝑣
4𝑡

𝑠,𝑣
4𝑡

𝑠,𝑣
4𝑡

(cid:8)

(

(

T
𝑣

𝑀𝑣 =

T
𝑀 , 𝑣𝑣

h

T
𝑣

𝑀𝑣 =

𝑘2
𝑡2

𝑀 ,

h

i

(cid:9)

𝑣𝑢′

(

)(

𝑣𝑢

T
)

𝑠

𝑝𝑢′(

)

𝑝𝑢

𝑠

(

)i)

Õ𝑢,𝑢′∈𝒮

𝑡

T
𝑣

𝑀𝑣 =

𝑘2
𝑡2

Õ𝑢,𝑢′∈𝒮

𝑡

𝑣𝑢

(

T
)

𝑀

𝑣𝑢′

𝑠

𝑝𝑢′(

)

)

𝑝𝑢

𝑠

(

(

))

Now for any 𝑢, 𝑢′ ∈ 𝒮
𝑡,
=
𝑣𝑢

𝑣𝑢′

𝑠,𝑣

ℬ

T
)
𝑣𝑢

𝑣𝑢

𝑣𝑢

𝑀
T
)
T
)
T
)

(
𝑀

𝑀

𝑀

)
𝑣𝑢′
(
𝑣𝑢′
(
𝑣𝑢′
(

)

)

)

(
6

6

6

𝑣𝑢

T
)
𝑣𝑢

(

(

(

𝑣𝑢

𝑣𝑢

T

𝑊
T
)
T
)
T
)

𝑊

𝑀

𝑀

𝑊
T

(
𝑊

(
𝑣𝑢
(
𝑣𝑢
(

𝑣𝑢′

𝑛

) −

𝑣𝑢

) + (
𝑣𝑢′

𝑣𝑢′

) + (

) + (

T
)
T
)
𝑀

𝑣𝑢
(
𝑣𝑢′
T
)
T
)

𝑀

(

𝑣𝑢′
T

(
𝑊

)
𝑊
(cid:9)
(

𝑣𝑢′

𝑣𝑢

𝑛

) +
.

)

·

k(

(cid:0)

𝑣𝑢′

(

𝑣𝑢′

2𝑛

(

) −

T
)

(

𝑣𝑢

2

)k

+ k(

𝑣𝑢′

)
𝑣𝑢′
(cid:9)
)k

𝑠,𝑣
2
𝑠,𝑣
2
𝑠,𝑣
2
𝑠,𝑣
2

(
2
(
2
(
2
(

(cid:8)

(cid:8)

(cid:8)

(cid:8)

where the ﬁrst equality follows by deﬁnition, the second using the fact that for any 𝑁
𝑎, 𝑏
𝑏
Similar derivation shows that

. The last follows from the fact that
𝑣𝑢′)
2
(

𝑎
k
−
𝑣𝑢′)
.
(cid:8)
(

T, 𝑁
)
ℬ

> 0
i
𝑠,𝑣
4𝑡

𝑠,𝑣
2
T𝑀

𝑣𝑢′)
(

T𝑀
)

𝑣𝑢
(

ℝ𝑑,

𝑠,𝑣
2

𝑣𝑢

h(

6

)(

𝑠,𝑣

−

−

∈

(cid:8)

𝑏

𝑎

𝑎

T𝑀
𝑣𝑢
)
Now let 𝑞 be the maximal norm of any 𝑡
×
𝑠𝑖 > 0
6 𝑡. Since
,
}

(cid:9)
−
(cid:8)
{

supp

𝑠,𝑣
4𝑡

𝑣𝑢

{(

ℬ

𝑠,𝑣

𝑡,

|

ℬ

∈ 𝒮

𝑢

(cid:9)
) − (
𝑡 principal submatrices of 𝑀. Note that for any

−(

𝑠,𝑣
4𝑡

𝑠,𝑣

(cid:9)

(cid:9)

2

𝑣𝑢

2
(

−

T
)

(

𝑣𝑢′

)

(cid:1)(cid:9)

0, and
2 > 0
.

(cid:23)
𝑏
k

)}|
𝑀𝑣 6 𝑘2
2𝑡2

𝑠,𝑣

ℬ

𝑠,𝑣
4𝑡

T
𝑣

(

> 0

,

𝑝𝑢

𝑠

(

)

(cid:8)
𝑣𝑢′
(

)

(cid:9)
𝑝𝑢

)

𝑠

𝑝𝑢′(

(cid:1)

𝑠

(

))

𝑣𝑢

T
)

𝑀

(

𝑣𝑢

𝑣𝑢′

𝑀

T
)

) + (

Õ𝑢,𝑢′∈𝒮

𝑡

(

(cid:0)

35

𝑞

(cid:0)

𝑣𝑢

2

)k

+

𝑞

𝑣𝑢′

2

)k

· k(

· k(

𝑠

𝑝𝑢′(

)

𝑝𝑢

𝑠

(

(cid:1)

))

𝑣𝑢

k(

)k

2𝑝𝑢

𝑠

(

) 

𝑠

𝑝𝑢′(

Õ𝑢′∈𝒮

𝑡

)! +

𝑣𝑢′

2𝑝𝑢′(

𝑠

)k

k(

𝑝𝑢

𝑠

(

)! !)

) 

Õ𝑢
𝑡
∈𝒮

𝑡

Õ𝑢,𝑢′∈𝒮
𝑘2
2𝑡2  

Õ𝑢
𝑡
∈𝒮

T
𝑣

𝑀𝑣 6 𝑘2
2𝑡2

T
𝑣

𝑀𝑣 6 𝑞

T
𝑣

𝑀𝑣 6 𝑞

T
𝑣

𝑀𝑣 6 𝑞

𝑠,𝑣
4𝑡

𝑠,𝑣
4𝑡

𝑠,𝑣
4𝑡

𝑠,𝑣
4𝑡

(

(

(

(

𝑡

Õ𝑢′∈𝒮
2𝑝𝑢′(

𝑠

)! )

𝑘2
2𝑡2  

Õ𝑢
𝑡
∈𝒮

𝑣𝑢

k(

)k

2𝑝𝑢

𝑠

(

) +

𝑣𝑢′

k(

)k

Õ𝑢′∈𝒮

𝑡

𝑘2
𝑡2

Õ𝑢
𝑡
∈𝒮

𝑣𝑢

k(

)k

2𝑝𝑢

𝑠

.

)

(

)

Here the second inequality follows from choice of 𝑞, the third uses the fact that

𝑠,𝑣
4𝑡

𝑠,𝑣

ℬ

𝑝𝑢

𝑠

(

)!

= 1

)

( 

𝑢
𝑡
∈𝒮
Í

. Finally observe that

𝑠,𝑣

ℬ

𝑠,𝑣
4𝑡

𝑠,𝑣
4𝑡

𝑠,𝑣
4𝑡

𝑠,𝑣
4𝑡

(

Õ𝑢
𝑡
∈𝒮

(

Õ𝑢
𝑡
∈𝒮

(

Õ𝑢
𝑡
∈𝒮

(

Õ𝑢
𝑡
∈𝒮

𝑣𝑢

2𝑝𝑢

𝑠

(

)

)k

k(

=

𝑣𝑢

2𝑝𝑢

𝑠

(

)

)k

k(

=

𝑣𝑢

2𝑝𝑢

𝑠

(

)

)k

k(

=

𝑑

Õ𝑖=1

Õ𝑢
𝑡
∈𝒮
𝑑

𝑖 𝑢2
𝑣2
𝑖 ·

𝑝𝑢

𝑠

(

))

𝑣2
𝑖

𝑑

Õ𝑢
𝑡
∈𝒮

𝑣2
𝑖 𝑠𝑖

𝑢𝑖

𝑝𝑢

𝑠

(

·

))

)

Õ𝑖=1
𝑡
𝑘

Õ𝑖=1

𝑣𝑢

2𝑝𝑢

𝑠

(

)

)k

k(

=

𝑡
𝑘 )

,

where we used the facts 𝑐𝐵𝑠,𝑣

𝑢
𝑡
∈𝒮
Í
there exists an absolute constant 𝐶 > 0 such that with probability 1
with probability 1

𝑜

−

(

𝑠,𝑣
2

𝑣𝑖 = 𝑣𝑖

{

𝑠𝑖

}

·

and 𝑐𝐵𝑠,𝑣

𝑠,𝑣
4𝑡

𝑢𝑖 𝑝𝑢

𝑠

= 𝑡

𝑘 ·
(
)
, 𝑞 6 𝐶
1
)
(

𝑜

𝑠𝑖

. By Fact 5.8,

)
𝑛𝑡 log 𝑡. Hence

,
1
)
(

−

𝑠,𝑣
4𝑡

𝑠,𝑣

ℬ

(cid:26)

T
𝑣

𝑀𝑣 6 𝐶

𝑘
√𝑡

p

𝑛 log 𝑑

.

(cid:27)

𝑠,𝑣
4𝑡

𝑠,𝑣

ℬ

−

(cid:26)

T
𝑣

𝑀𝑣 6 𝐶

𝑘
√𝑡

p

𝑛 log 𝑑

.

(cid:27)

p

(cid:3)

Similar derivation shows that

5.3 SoS Algorithms

We now use the certiﬁed upper bounds from the previous sections to obtain eﬃcient algorithms
for Sparse PCA with adversarial errors, thus proving Theorem 1.2 and Theorem 1.5 which we
formally restate.

36

Theorem 5.10. Suppose 𝑑 & 𝑛𝑡 log𝑡

(

𝑛

𝑡 𝑡 for 𝑡

ℕ. Let 𝑌 be an 𝑛-by-𝑑 matrix of the form,

∈

)
𝑌 =

T
𝑢0𝑣0

𝛽

·

𝑊

+

+

𝐸 ,

ℝ𝑛

for a unit 𝑘-sparse vector 𝑣0 ∈
𝐸
we can compute in time 𝑑𝑂

𝑑 and a Gaussian matrix 𝑊

ℝ𝑑, a standard Gaussian vector 𝑢0 ∼

, an arbitrary matrix
0, Id𝑛
)
(
𝑑 such that 𝑊 , 𝑢0 are distributionally independent. Then

0, 1
𝑁
×
)
(
ℝ𝑑 such that with probability at least 0.99,
𝑣
) a unit vector
ˆ
∈

𝑁

∼

∈

×

𝑛

(

𝑡

p

1

𝑣, 𝑣0i

− h ˆ

2 . 𝑘

𝛽𝑛 ·

𝑡

·

𝑡

1
/

𝑑
𝑘

(cid:0)

(cid:1)

1
𝛽 + s

+

𝑘
𝛽𝑛  r

log

𝑑
𝑘 + k

𝐸

k1

→

2

! ·  

1

+

1

𝛽 !

,

2 denotes the largest norm of a column of 𝐸. Furthermore, the same kind of guarantees hold if

p

𝐸
where
k1
𝑢0 is a vector with

→

k

𝑢0k

k

2 = Θ

𝑛

(

)

independent of 𝑊.

Theorem 5.11. Suppose 𝑛 & log 𝑑 and 𝑡 6 𝑘. Let 𝑌 be an 𝑛-by-𝑑 matrix of the form,

𝑌 =

T
𝑢0𝑣0

𝛽

·

𝑊

+

+

𝐸 ,

for a unit 𝑘-sparse vector 𝑣0 ∈
𝐸
)𝑑𝑂
we can compute in time 𝑛𝑂
1

𝑑 and a Gaussian matrix 𝑊

ℝ𝑛

∈

×

(

(

𝑡

0, 1
𝑁
)
(
) a unit vector

∼

×
𝑣
ˆ

p

ℝ𝑑, a standard Gaussian vector 𝑢0 ∼

, an arbitrary matrix
0, Id𝑛
)
(
𝑑 such that 𝑊 , 𝑢0 are distributionally independent. Then

𝑁

𝑛

ℝ𝑑 such that with probability 1

𝑜

,
1
)
(

−

∈

1

𝑣, 𝑣0i

− h ˆ

2 . 𝑘

𝛽 · r

log 𝑑
𝑛𝑡 + s

𝑘
𝛽𝑛  r

log

𝑑
𝑘 + k

𝐸

k1

→

2

! ·  

1

+

1

𝛽 !

,

2 denotes the largest norm of a column of 𝐸. Furthermore, the same kind of guarantees hold if

p

where
𝐸
k1
𝑢0 is a vector with

→

k

𝑢0k

k

2 = Θ

𝑛

(

)

independent of 𝑊.

We will prove Theorem 5.10 and Theorem 5.11 using Algorithm 5.12.

Algorithm 5.12 (Algorithm for Sparse PCA with Adversarial Corruptions).

Given: Sample matrix 𝑌 =

·
Estimate: The sparse vector 𝑣0.

p

𝛽

𝑢0𝑣𝑇

0 +

𝑊

𝐸

+

∈

ℝ𝑛

×

𝑑 from model 1.1, system

𝑠𝑣

𝒞

∈ {𝒜

𝑠,𝑣 ,

𝑠,𝑣

}

ℬ

Operation:

1. ﬁnd a level-4𝑡 pseudo-distribution 𝐷 that satisﬁes

2. Output a top eigenvector

𝑣 of 𝔼 𝑣𝑣T.
ˆ

𝑠,𝑣 and maximizes 𝔼

𝒞

𝑌𝑣

k

2
2.

k

𝑠,𝑣 also satisﬁes

Let us analyze the algorithm. The ﬁrst observation is that any pseudo-distribution satisfying
𝑠,𝑣 is a feasible
𝑠,𝑣. Next we show that any pseudo-distribution satisfying
ℬ
solution to SDP-1. This will allows us to use Theorem 4.6 and conclude the proofs of Theorem 5.10
and Theorem 5.11.

𝒜

𝒜

Lemma 5.13. Let 𝐷 be any pseudo-distribution of degree > 4 satisfying
solution to SDP-1.

𝒜

𝑠,𝑣. Then 𝔼𝐷 𝑣𝑣T is a feasible

37

Proof. Since 𝐷 satisﬁes
with entries in
1
+
pseudo-distributions,

{−

1,

}

𝑠,𝑣, Tr 𝔼𝐷 𝑣𝑣T = 𝔼𝐷
=

𝒜
such that

𝔼𝐷 𝑣𝑣T

1

𝑖6𝑑 𝑣2
𝑖
𝑥𝑥T, 𝔼𝐷 𝑣𝑣T
Í
h

i

= 1. Now, there exists a vector 𝑥

ℝ𝑑

. By Cauchy-Schwarz inequality for

∈

T
𝑥𝑥

h

(cid:13)
(cid:13)
T
, 𝔼𝐷 𝑣𝑣

= 𝔼𝐷

i

(cid:13)
(cid:13)

Õ𝑖,𝑗6𝑑

𝑥𝑖 𝑠𝑖𝑣𝑖 𝑥 𝑗 𝑠 𝑗𝑣 𝑗

𝑖 𝑥2
𝑥2

𝑗 𝑠2

𝑖 𝑠2

𝑗

·

𝔼𝐷

s

Õ𝑖,𝑗6𝑑

𝑖 𝑣2
𝑣2

𝑗

6

𝔼𝐷

s
= 𝔼𝐷

Õ𝑖,𝑗6𝑑
𝑠2
𝑖

Õ𝑖6𝑑

=𝑘 .

The result follows as 𝔼 𝑣𝑣T

0.

(cid:23)

(cid:3)

We can now ﬁnish the analyses using the certiﬁed upper bounds from the previous sections.

Proof of Theorem 5.10. Let 𝐷 be the pseudo-distribution in Algorithm 5.12. By Lemma 5.6, with
1
probability at least 0.99, 𝔼𝐷
𝑡 𝑡
outputs 𝑣0 satisﬁes
immediately get,

. Note that since the pseudo-distribution that
𝑠,𝑣, by Lemma 5.13, 𝔼𝐷 𝑣𝑣T satisﬁes the premises of Theorem 4.6. Then we

2
2 6 𝑂

𝑡 𝑘1
−

𝑑1
/

𝑊 𝑣

𝒜

k

k

(cid:16)

(cid:17)

𝔼𝐷

1

−

𝑣, 𝑣0i

h

2 . 𝑘

𝛽𝑛 ·

𝑡

·

𝑡

1
/

𝑑
𝑘

(cid:1)
The result follows applying Lemma H.3.

(cid:0)

Similarly,

1
𝛽 + s

+

𝑘
𝛽𝑛  r

log

𝑑
𝑘 + k

𝐸

2

k1

→

! ·  

1

+

1

𝛽 !

.

p

(cid:3)

Proof of Theorem 5.11. Let 𝐷 be the pseudo-distribution in Algorithm 5.12. By Theorem 5.9,

with probability 1

𝑜

,
1
)
(

−

𝔼𝐷 𝑣T

𝑊T𝑊

𝑛Id

𝑣

6 𝑂

−

(cid:16)
distribution that outputs 𝑣0 with probability 1 satisﬁes
(cid:12)
(cid:12)
premises of Theorem 4.6. Then we immediately get,

(cid:12)
(cid:12)

(cid:0)

(cid:1)

𝑘
√𝑡

ℬ

𝑛 log 𝑑

. Note that since the pseudo-
𝑠,𝑣, by Lemma 5.13, 𝔼𝐷 𝑣𝑣T satisﬁes the
p

(cid:17)

𝔼𝐷

1

−

𝑣, 𝑣0i

h

2 .

𝑘
𝛽√𝑛𝑡

log 𝑑

𝑘
𝛽𝑛  r

log

𝑑
𝑘 + k

𝐸

+ s

k1

→

2

! ·  

1

+

1

𝛽 !

.

The result follows applying Lemma H.3.

p

(cid:3)

6 Unconditional lower bounds for distinguishing

6.1 Low-degree polynomials

The goal of this section is to formalize our lower bounds. In light of the discussions in Section 2.5
and Section 3.3 we study distinguishing problems between two distributions over matrices: the
null distribution 𝜈, which in our case is a standard Gaussian, and the planted distribution 𝜇 that

38

contains some sparse signal hidden in random (and adversarial) noise. That is, given an instance
𝑌 sampled either from the null or from the planted distribution, the goal is to determine whether
𝑌 contains a planted signal. We will show that a large class of polynomial time algorithms (cap-
turing the best known algorithms) cannot distinguish between the null and the planted case even
when information-theoretically possible. Speciﬁcally, we will show that low degree polynomial
estimators cannot solve these problem. Similarly to [HKP+17b, HS17, DKWB19], we study the low
degree analogue of the 𝜒2-divergence between probability measures.

Deﬁnition 6.1. Let 𝜇 and 𝜈 be probability distributions over ℝ𝑛
×
and 0 < 𝕍𝜈 𝑓 <
functions 𝑓
respect to 𝜈 is deﬁned as

ℝ such that

𝔼𝜇 𝑓

: ℝ𝑛

→

∞

<

×

𝑑

(cid:12)
(cid:12)
𝜇
(

𝜒2

(cid:12)
(cid:12)

𝜈

)

k

𝔼𝜇 𝑓

𝔼𝜈 𝑓

−
𝕍𝜈 𝑓

2

.

(cid:1)

= sup
𝐹 (cid:0)
𝑓

∈

𝑑, and denote by 𝐹 the set of all
. The 𝜒2-divergence of 𝜇 with

∞

Note that this value is related to the likelihood ratio 𝐿 described in Section 3.3: the fraction in

Recall that, if 𝜒2

the right hand side is maximized for 𝑓 = 𝐿, and 𝜒2
𝜈

= 𝔼𝜈 𝐿2
is bounded, then 𝜇 and 𝜈 are information-theoretically indistinguishable
in the sense of Section 3.3.1. The low-degree analogue of 𝜒2-divergence is deﬁned similarly. Denote
by ℝ
𝑌
is the space of
]
[
polynomials of 𝑛

6𝐷 the set of polynomials of degree at most 𝐷 in ℝ
𝑌
[
𝑑 variables corresponding to the entries of 𝑌).

(where ℝ
𝑌
[

𝜇
(

𝜇
(

1.

−

𝜈

k

k

]

]

)

)

·

Deﬁnition 6.2. Let 𝐷 > 0 and let 𝜇 and 𝜈 be probability distributions over ℝ𝑛
ℝ
absolutely continious and for all 𝑝
𝑌
[
of 𝜇 with respect to 𝜈 is deﬁned as

and 𝕍𝜈 𝑝 <

𝔼𝜇 𝑝

6𝐷,

𝑑 such that 𝜈 is
. The degree-𝐷 𝜒2-divergence

∞

∞

<

∈

×

]

𝜒2
𝜇
6𝐷 (

𝜈

)

k

where we assume that 0
/

0 = 0.

= sup
ℝ
𝑌
𝑝
[

∈

]

6𝐷 (cid:0)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

𝔼𝜇 𝑝

𝔼𝜈 𝑝

−
𝕍𝜈 𝑝

2

,

(cid:1)

Note that since 𝜈 is absolutely continuous, the denominator 𝕍𝜈 𝑝 is zero if and only if 𝑝 is

constant (and in this case the numerator is also zero).

6.2 Spiked covariance model with sparsity

The ﬁrst problem we will look into is a variant of the standard sparse spiked covariance model
which we use to prove the lower bound in Theorem 1.6.

Problem 6.3. (Spiked Covariance Model with Sparsity) Given a matrix 𝑌

ℝ𝑛

×

𝑑, decide whether:

∈

𝑁

0, 1
)
(

𝑛

×

𝐻0: 𝑌 = 𝑊 where 𝑊

𝑑 is a standard Gaussian matrix.

𝐻1: 𝑌 = 𝑊

∼
T, where 𝑊, 𝑢, and 𝑣 are mutually independent, 𝑊
𝛽𝑢0𝑣0
standard Gaussian matrix, 𝑢
𝑢𝑖 distributed symmetrically around zero such that

𝑑 is a
ℝ𝑛 is a random vector with i.i.d. 1-subgaussian coordinates
ℝ𝑑 is a

6 𝑅 for some 𝑅 > 1, and 𝑣

0, 1
)
(

𝑢𝑖

p

𝑁

∼

+

∈

×

𝑛

|

|

∈

39

random vector with i.i.d. coordinates 𝑣𝑖 that take values

𝑣𝑖 =

1
√𝑘

−
1
√𝑘
0

with probability 𝑘

with probability 𝑘

2𝑑,

/
2𝑑,

|

𝑜

/

∈

.

otherwise.



Let’s take a moment to compare the model in 𝐻1 with model 1.1. First, note that we require

6 𝑅, which formally does not hold for Gaussian distribution for any 𝑅
ℝ. So to get lower
𝑢𝑖
|
bounds for the single spike model we should consider not 𝐻1, but 𝐻′1 such that 𝑢 from 𝐻1 is
log 𝑛 for all
replaced by a Gaussian vector 𝑢′ ∼ 𝒩(
1 6 𝑖 6 𝑛. If 𝑢 is drawn from a truncated Gaussian distribution 𝑢𝑖 = sign
, 𝐶
,
(
}
then all 𝑢𝑖 are 1-subgaussian and 𝑢𝑖 6 𝑅 = 𝐶
log 𝑛. Moreover, with high probability over 𝑢′,
𝑢𝑖 = 𝑢′𝑖 for all 𝑖. Hence if it is hard to distinguish between 𝐻0 and 𝐻1 with 𝑅 = 𝐶
log 𝑛, it is
, then with high probability 𝑣 is
also hard to distinguish between 𝐻0 and 𝐻′1. Second, if 𝑘
˜𝑘 = 𝑘
-sparse. Also note that 𝑣 might not be a unit vector, but with high probability over 𝑣
1
1
(
))
(
its norm is 1
a ˜𝑘 = 𝑘

. So with high probability over 𝑣, 𝑌 from 𝐻1 is equal to 𝑊
1
(
)
𝑣
-sparse unit vector and

. However, with high probability
)
𝑢′𝑖 )

𝑣T, where
˜

𝑢′𝑖 |
|
min

𝑣 is
˜

0, Id𝑛

→ ∞

𝑛, then the algorithm that just computes the top singular value
of 𝑌 can distinguish between 𝐻0 and 𝐻1. On the other hand if 𝛽 & 𝑘
log 𝑑, then Diagonal
√𝑛
Thresholding can distinguish between 𝐻0 and 𝐻1. Under Conjecture 3.13 (see[Hop18, BKW20b] for
a formal discussion), the following theorem provides formal evidence that there is no polynomial
6

time algorithm that can improve over the guarantees of Diagonal Thresholding whenever
𝑘 6 𝑑1
2
−
/
(
regimes where √𝑑 6 𝑘
bound does not match the best know guarantees only by a factor 𝑜
can be found in Section 6.5.

1
). Furthermore the theorem also implies that SVD with Thresholding is optimal in
𝑘2 . Finally we remark that in settings where 𝑘 = 𝑑1

1
) the lower
(
. The proof of theorem
)

log 𝑑
(

log 𝑑

𝑑
log 𝑑

+
1
))
(
Recall now that if 𝛽 &

˜𝛽 = 𝛽

.
1
))
(

log 𝑛

𝑢′𝑖 |
p

= 𝛽

˜𝛽𝑢

1
(

1
(

/k

q

q

q

p

p

p

p

p

{|

+

+

+

+

𝑑

/

𝑜

𝑜

𝑜

Ω

k

−

𝑜

Theorem 6.4. Let 𝜈 and 𝜇 be the null and the planted distributions of Problem 6.3 respectively. If 𝐷 6 𝑛
and

/

𝑅4

where 0 < 𝜀 < 1

1000 , then for any nonconstant polynomial 𝑝 : ℝ𝑛

𝑑

×

𝛽 = 𝜀

min

·

𝑑
𝑛

,

𝑘
√𝐷𝑛 ·

𝑑

𝐷

·
𝑘2

ln

(cid:18)

nr

(cid:18) (cid:12)
(cid:12)
(cid:12)
(cid:12)

1

+

,

(cid:19) o

ℝ of degree at most 𝐷,

(cid:19) (cid:12)
(cid:12)
(cid:12)
(cid:12)
→

𝔼𝜇 𝑝

(cid:0)

𝑌
) −
(
𝕍𝜈 𝑝

𝔼𝜈 𝑝
𝑌
(

)

2

𝑌
(

)

(cid:1)

6 𝜒2

𝜇
6𝐷 (

𝜈

)

k

. 𝜀2 .

Theorem 6.4 implies that polynomials of degree 𝐷 6 𝑛

sample from the single spike model and a sample of standard Gaussian vectors if 𝛽
. In particular, if 𝑘 6 𝑑1
2
−
/

log2 𝑛 cannot distinguish between a
𝑛 and
), then polynomials of degree . log 𝑑 cannot solve
1

log

≪

𝛽

𝑑

/

/

Ω

(

p

𝑘
√𝐷𝑛

≪

𝐷
𝑑
·
𝑘2

(cid:17)
the problem for 𝛽

(cid:16)

𝑘
𝑛

log 𝑑.

≪

We remark that in [DKWB19] the authors provided a similar hardness result for low-degree
log2 𝑛), but it does

polynomials. Their lower bound works for all 𝐷 6 𝑜

(not only for 𝐷 6 𝑛

p

𝑛

(

)

/

40

not contain a log 𝑑 factor. In particular, if 𝑘 6 𝑑1
𝑘
𝛽
√𝐷𝑛
Diagonal Thresholding.

log 𝑑, and their lower bound is 𝛽

𝑘
√𝐷𝑛

≪

≪

p

Ω

(

) and 𝐷 6 𝑛
1

log2 𝑛, our lower bound is
/−
. Hence we are able to show a tight bound for

/

In [HKP+17b] a lower bound for a similar model is presented. More concretely, the authors
obtained exponential lower bounds for the Sum-of-Square Hierarchy in the Wigner model 𝑌 =
𝛽𝑣𝑣T. Their results do not directly apply in our settings as the covariance matrix 𝑌T𝑌 is far
𝑊
from being Gaussian.

+

The same reasoning used in Theorem 6.4 can be used to obtain an information theoretic lower

bound.

Theorem 6.5. Let 𝜈 and 𝜇 be the null and the planted distributions of Problem 6.3 respectively. If

where 0 < 𝜀 < 1

1000 , then for any 𝑓 : ℝ𝑛

𝑑

×

→

𝛽 = 𝜀

𝑅2 ·

min

𝑑
𝑛

,

𝑘
𝑛 ·

ln

𝑛
𝑑
·
𝑘2

nr

(cid:18)

(cid:19) (cid:12)
(cid:18) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ℝ such that 0 < 𝕍𝜈 𝑓
𝑌
(cid:12)
(cid:12)
(

)

+

1

,

(cid:19) o
<

∞

,

𝔼𝜇 𝑓

(cid:0)

𝑌
) −
(
𝕍𝜈 𝑓

𝔼𝜈 𝑓
𝑌
(

)

2

𝑌
(

)

(cid:1)

6 𝜒2

𝜇
(

k

𝜈

)

. 𝜀2 .

avoided using diﬀerent techniques, indeed for 𝑅

We remark that the term 𝑅2 is a consequence of our analysis of 𝜒2-divergence and can be
, Theorem E.1 provides tighter guarantees.
1
𝜔
)
(
𝑛 log 𝑑

∈
Additionally, we point out that for 𝛽 < 1 a bound of

𝑘2 can be achieved as shown in [BR13a].

𝑘

6.3 Almost Gaussian vector in random subspace

q

In this section we prove Theorem 1.3. Concretely, we will show that in the presence of adversarial

, so that the degree 𝑡 SoS Algorithm 5.12
corruptions, whenever 𝑡
outperforms other known algorithms, no multilinear polynomial of degree . 𝑛0.001can obtain
(cid:1)
similar guarantees unless 𝑑 &

(cid:0)

(cid:0)

(cid:1)

·

.

𝑡

𝑛𝑡𝑡 𝑡

Ω
˜

1
/

𝑡 & 𝑛0.499 and 𝑑 >

𝑑
𝑘

Ω
˜

𝑛
ln2 𝑡

Similarly to Section 6.2 we design a speciﬁc distinguishing problem. In order to prove a lower
bound in the presence of adversarial corruptions, we need to carefully chose the adversarial matrix.

(cid:17)

(cid:16)

Problem 6.6. (Almost-Gaussian vector in a random subspace) Given a matrix Y in ℝ𝑛
whether:

𝑑, decide

×

𝑁

0, 1
)
(

𝑛

×

𝐻0: 𝑌 = 𝑊 where 𝑊

𝑑 is a standard Gaussian matrix.

𝐻1: 𝑌 = 𝜆𝑢

∼
𝐸, where 𝑊
√𝑛 with probability 1
1
vector with i.i.d. coordinates that take values
/
/
is a vector with i.i.d. coordinates that take values

𝑑 is a standard Gaussian matrix, 𝑢

0, 1
)
(

𝑣T
˜

𝑊

𝑁

±

+

+

∼

×

𝑛

∈
2 each, and

ℝ𝑛 is a unit
ℝ𝑑
𝑣
˜

∈

2 ,

2 ,

/

/

𝑣𝑖 =

−
1

0





1 with probability 𝛿

with probability 𝛿

otherwise,

41

for some 𝛿
following distribution. Let 𝑠 > 0 be the largest even number such that 𝛿𝜆𝑠 6 2−
(cid:0)
𝑗

T, where 𝑣′ is sampled according to the
10𝑠 . For all

. Furthermore 𝐸 = 𝑢

0, 1
]

𝑣′ −

𝑊T𝑢

∈ [

𝑑

(cid:1)

,

∈ [
]
– if

𝑣 𝑗 ≠ 0, then 𝑣′𝑗
˜

= 0 ;

– otherwise 𝑣′𝑗 is sampled from the distribution 𝜂 that has ﬁnite support supp

𝜂

10𝑠, 10𝑠

]

[−

and moments:

(cid:0)

(cid:1)

⊆

𝜆𝑟 𝛿

𝑟

(
6

!!

1
−
)
−
1
𝛿
−
10𝑠
(

𝑟

)

𝑥𝑟 =

𝔼
𝑥
𝜂

if 0 6 𝑟 6 𝑠 and even
if 𝑟 > 𝑠

2 and even



Proposition F.2 shows that if 𝛿𝜆𝑠 6 2−
10𝑠 , then such 𝜂 exists. Note that for 𝑠 = 0 the condition

10𝑠 is always satisﬁed, so 𝑠 in the problem description is well-deﬁned. Also note that if

if 𝑟 is odd.

0

∼

+

𝛿𝜆𝑠 6 2−
𝑠 = 0, 𝑣′ is just a zero vector.

If 𝛿𝑑

→ ∞

unit vector 𝑣0 = 1
𝑣
k ˜

, then with high probability
𝑣 (where 𝑘 := 𝛿𝑑
𝑜
1
k ˜
(
We will use the notation 𝑣 = 𝜆
𝑣′. Note that the coordinates of 𝑣 are independent, have
𝑣
˜
coordinates

−
+
Gaussian moments up to 𝑠, and with high probability 𝑣 has at least 𝛿𝑑
𝑣 𝑗

𝑜
1
1
))
−
(
(
, 𝛽 = 𝑘𝜆2
1
𝑛 (

-sparse. So 𝜆𝑢
𝑜

𝑣 is 𝛿𝑑
˜
1
)))
(

𝑣T =
˜
and 𝑢0 = √𝑛𝑢.
p

T for 𝑘-sparse

𝛽𝑢0𝑣0

1
))
(

1
))
(

1
(

−

+

𝜆

𝑜

.

∈ {±

}

Geometric description. The planted distribution can be also described in geometric terms, where
the problem becomes that of distinguishing between a subspace spanned by independent Gaussian
vectors or a subspace spanned by independent Gaussian vectors and the planted vector 𝑣.

𝑜

−

The construction is the following: at ﬁrst we sample a signal vector 𝑣
1
(

ℝ𝑑 that has at least
coordinates with absolute values at least 𝜆 (using the construction described above).
ℝ𝑑, and perform a random
𝑤1, . . . ,
1) on
˜
−

𝑛 with ﬁrst column vector 𝑢 (such that 𝑈 is independent of 𝑣,

1 i.i.d. standard Gaussian vectors

𝑤1, . . . ,
˜

1 ∈
−

𝑤𝑛
˜

𝑤𝑛
˜

−

∈

×

𝛿𝑑
1
))
(
Then we sample 𝑛
rotation 𝑈
𝑤1, . . . ,
𝑣,
˜

∈
𝑤𝑛
˜

ℝ𝑛

1. That is,
−

This formulation is equivalent to the one described above. Indeed,

𝑌 = 𝑈

𝑣𝑇
𝑤𝑇
1
˜
...
𝑤𝑇
𝑛
1
˜
−

·

©

.

ª
®
®
®
®
¬

𝑌 = 𝑢𝑣𝑇

«
𝑛

1
−

+

Õ𝑖=1

𝑤𝑇
𝑢𝑖 ˜
𝑖 ,

where 𝑢, 𝑢1, . . . , 𝑢𝑛
(singular) Gaussian supported in the hyperplane orthogonal to 𝑢, and
standard Gaussian supported in the same hyperplane.

1 are the columns of 𝑈. Note that
−

Í

is distributed as a standard
𝑢𝑢𝑇
𝑊 is also a
)

Id
(

−

𝑛

1
−

𝑤𝑇
𝑖=1 𝑢𝑖 ˜
𝑖

The theorem below provides a lower bound for the Problem 6.6. The proof is in section 6.6.

42

 
 
 
 
Theorem 6.7. Suppose that 𝑛 6 𝑑, 1 6 𝐷 6 𝑛0.33, 0 < 𝛿 < 1, 𝑘 = 𝛿𝑑, 𝜆 > 2, and let 𝑠 > 2 be the
maximal even number such that 𝛿𝜆𝑠 6 2−
1. Let 𝜈 and 𝜇 denote respectively the null
and planted distribution (with parameters 𝛿, 𝜆, 𝑠) of Problem 6.6. Suppose that 𝜆 > 1000√𝑡 ln 𝑡 and that
𝜆4𝐷𝑡2 ln2 𝑡 = 𝑜

10𝑠 , and 𝑡 = 𝑠

log2

as 𝑛

. If

+

𝑛

1

2

/

·

𝑑
𝑘2

+

(cid:16)

(cid:16)

(cid:16)

(cid:17)

(cid:17) (cid:17)

→ ∞

𝑑 = 𝑜

1
𝜆4 ·

𝑛
ln2 𝑡

𝐶

(cid:18)

·

𝑡

𝐷 (cid:19)

!

·

(for some constant 𝐶 that does not depend on 𝑛, 𝑑, 𝛿, 𝜆, 𝑠 and 𝐷), then for any non-constant

as 𝑛
multilinear polynomial 𝑝 : ℝ𝑛

→ ∞

𝑑

×

→

ℝ of degree at most 𝐷,

𝔼𝜇 𝑝

(cid:0)

𝑌
) −
(
𝕍𝜈 𝑝

𝔼𝜈 𝑝
𝑌
(

)

2

𝑌
(

)

(cid:1)

0 ,

→

as 𝑛

.

→ ∞
Let’s try to illustrate the meaning of Theorem 6.7. If 𝜆 > 𝐵

log 𝑑 for suﬃciently large 𝐵, then
𝛿 is so that 𝛿𝜆𝑠 = 2−
. Here Algorithm 5.12 can distinguish
p
→ ∞
between the null and the planted distribution if 𝑑 & 𝑛𝑡 log𝑡
𝑡 𝑡 in polynomial time. Indeed, in this
𝑛
)
(
𝑣 such that
case with probability at least 0.99 the algorithm 5.12 outputs
ˆ

10𝑠 for an even constant 𝑠 and 𝛿𝑑

1

𝑣, 𝑣0i

− h ˆ

𝑡

1
/

2 . 1

𝜆2  

𝑡

1
𝛿

(cid:19)

·

(cid:18)

𝐸

2
1
→

2

k

!

+ k

6 𝑡

1
1𝛿
+

(cid:19)

𝜆𝑠

·

(cid:18)

𝑡

1
/

𝐸

k

2

k1
𝜆

→

2

=

(cid:19)

220𝑡
𝜆1
/

𝑡 +

(cid:18)

𝐸

k

2

k1
𝜆

→

2

.

(cid:19)

+

(cid:18)

The ﬁrst term tends to 0 and

𝐸

k

k1

→

2 can be bounded as follows:

𝐸

k

k1

→

2 6

𝑣′

𝑢

(

k

T
)

k1

→

2 + k

T
𝑢𝑢

𝑊

2 6 max
16𝑖6𝑑

k1

→

𝑣′𝑖 +

max
16𝑖6𝑑

T
𝑢

𝑊

. 𝑠

𝑖

+

log 𝑑 ,

(cid:1)
since 𝑢T𝑊 is a standard Gaussian vector. Hence for suﬃciently large 𝐵,
𝑣, 𝑣0i
h ˆ

If in addition 𝐷 6 𝑛0.001 and 𝜆 6 𝑛0.24, then the conditions of Theorem 6.7 are satisﬁed. Hence
in this case for 𝑑 6 𝑛0.999𝑡
1 no multilinear polynomial of degree at most 𝑛0.001 can distinguish
−
. Furthermore note that if 𝜆4 & 𝑛 log 𝑑,
between the planted and the null distribution as 𝑛
then Diagonal thresholding can distinguish between the planted and the null distribution in
1 ). Finally, it easy to see that exhaustive search works as long
polynomial time (even if 𝑑
−
as 𝜆 &

𝑛0.999𝑡

→ ∞

log 𝑑

p
> 0.99.

≪

𝑘.

(cid:0)

/

p

6.4 Chi-squared-divergence and orthogonal polynomials

Recall that given a hypothesis testing problem with null distribution 𝜈 and planted distribution 𝜇,
we say a polynomial 𝑝

6𝐷 cannot distinguish between 𝜇 and 𝜈 if

𝑌
(

) ∈

ℝ
𝑌
[

]

𝔼𝜇 𝑝

(cid:12)
(cid:12)

𝑌
(
) −
𝕍𝜈 𝑝

𝔼𝜈 𝑝

𝑌
(

)

𝑌
(

)
(cid:12)
(cid:12)

6 𝑜

.

1
)
(

(6.1)

So, if for some distinguishing problem this ratio is small for all 𝑝
6𝐷, then polynomial
estimators of degree at most 𝐷 cannot solve this distinguishing problem. The key observation used

∈

]

ℝ
𝑌
[

p

43

 
to prove bounds for low degree polynomials is the fact that the polynomial which maximizes the
ratio (6.1) has a convenient characterization in terms of orthogonal polynomials with respect to the
null distribution.

Formally, for any linear subspace of polynomials

6𝐷 and any absolutely continuous
probability distribution 𝜈 such that all polynomials of degree at most 2𝐷 are 𝜈-integrable, one can
deﬁne an inner product in the space

6𝐷 as follows

6𝐷

𝒮

⊆

]

ℝ
𝑌
[

𝒮

(cid:10)
Hence we can talk about orthonormal basis in

𝒮

(cid:11)

6𝐷 with respect to this inner product.

𝑝, 𝑞

∀

6𝐷

∈ 𝒮

𝑝, 𝑞

= 𝔼
𝑌
𝜈

∼

𝑝

𝑞

𝑌
(

)

𝑌
(

)

.

6𝐷

Proposition 6.8. Let
6𝐷 be a linear subspace of polynomials of dimension 𝑁. Suppose that
𝒮
𝑑 such that any polynomial of degree at most 𝐷 is
𝜈 and 𝜇 are probability distributions over 𝑌
×
𝜇-integrable and any polynomial of degree at most 2𝐷 is 𝜈-integrable. Suppose also that 𝜈 is absolutely
continuous. Let

with respect to 𝜈. Then

𝑁
𝑖=1 be an orthonormal basis in

ℝ𝑛

𝜓𝑖

6𝐷

⊆

∈

]

ℝ
𝑌
[

𝑌
(

)}

{

𝑌
[

]

𝒮

max
𝑝
∈𝒮

6𝐷 (cid:0)

𝔼𝜇 𝑝
𝑌
(
𝔼𝜈 𝑝2

)
𝑌
(

(cid:1)
)

2

𝑁

=

Õ𝑖=1 (cid:18)

𝜓𝑖

𝔼
𝜇

2

.

(cid:19)

Proof. For any 𝑝

6𝐷

∈ 𝒮

𝔼
𝜇

𝑝

𝑌
(

)

= 𝔼
𝜇

)}

Since the system

𝜓𝑖

𝑌
(

{

Hence we get

𝑁

𝑝𝑖𝜓𝑖

𝑌
(

)

𝑁

=

𝑝𝑖 𝔼
𝜇

𝜓𝑖

𝑌
(

)

6

𝑁

𝑝2
𝑖

Õ𝑖=1
𝑁
𝑖=1 is orthonormal with respect to 𝜈,

Õ𝑖=1

Õ𝑖=1

2
1
/

𝑁

!

Õ𝑖=1 (cid:18)

2

2
1
/

𝔼
𝜇

𝜓𝑖

𝑌
(

)

!

(cid:19)

.

𝑝2

𝔼
𝜈

𝑌
(

)

=

𝑝2
𝑖 .

𝑁

Õ𝑖=1

𝔼𝜇 𝑝

𝔼𝜈 𝑝2

)

1
2

𝑌
(
𝑌
(
𝜓𝑖

)
𝑌′)
(cid:1)
(

𝑁

6

"

2

2
1
/

.

𝜓𝑖

𝔼
𝜇

#

(cid:19)

Õ𝑖=1 (cid:18)
𝑌
maximizes the ratio.
(

𝜓𝑖

𝑁
𝑖=1

𝔼𝑌′∼
(cid:0)

𝜇

(cid:3)

Note that the polynomial

)
From now on we assume that the distribution 𝜈 is Gaussian. In this case a useful orthonormal

Í

(cid:3)

(cid:2)

basis in ℝ
𝑌
[

]

6𝐷 is the system of Hermite polynomials.

:

:

]

𝑛

{
]}

𝑛
𝑖
∈ [
. For 𝑗

, let 𝐼𝛼 :=
𝑛

To work with Hermite polynomials we introduce some useful notation. For a multi-index 𝛼
𝛼 for some 𝑗
𝑑
:
over
]
] × [
[
, and similarly let 𝐽𝛼,𝑖 :=
𝛼 for some 𝑖
𝑖, 𝑗
) ∈
(
𝑑,
𝑖, 𝑗
𝑑
𝑗
𝛼
{
∈ [
) ∈
(
𝛼𝑖𝑗
𝑋 𝛼 :=
𝑛
𝛼 𝑋
can be represented as a bipartite
Î
𝑖𝑗
[
(
multigraph 𝐺𝛼 =
has multiplicity 𝛼𝑖𝑗. In this representation
Î
the set 𝐽𝛼,𝑖 corresponds to the neighborhood of the vertex 𝑖 and the set 𝐼𝛼,𝑗 corresponds to the
neighborhood of 𝑗. If 𝛼 is multilinear, 𝐺𝛼 is just a graph (i.e. multiplicity of each edge is 1).

𝑖, 𝑗
(
𝑑
∈ [
]
∈ [
. We will use the notation 𝛼! :=
}
. Note that every multi-index 𝛼 over
𝑖, 𝑗
𝐼𝛼

𝑑
]}
𝑖, 𝑗
(
𝛼 𝛼𝑖𝑗! and for a matrix 𝑋
)∈
𝑑

and similarly 𝐽𝛼 :=

such that each edge

) ∈
, let 𝐼𝛼,𝑗 :=

𝐽𝛼, 𝐸𝛼)

∈ [
:
]
𝑖,𝑗

] × [

∈ [

∈ [

) ∈

ℝ𝑛

Ð

𝛼

∈

𝑛

𝑖,𝑗

𝑑

)∈

{

{

{

}

}

×

]

]

]

(

(

𝑖

𝑗

44

 
 
For a multi-index 𝛼 over

𝑛

[

] × [

𝑑

the corresponding Hermite polynomial is

]
𝑌
𝐻𝛼(

)

=

𝐻𝛼𝑖𝑗(

𝑌𝑖𝑗

)

,

𝐼𝛼, 𝑗
𝐽𝛼 Ö𝑖
∈
ℤ is a degree 𝑙 one variable Hermite polynomial, deﬁned as follows

Ö𝑗
∈

where 𝐻𝑙 for 𝑙

∈

=

𝐻𝑙

𝑥

(

)

Õ06𝑟6𝑙

𝑟 is even

𝑙

−

𝑟

𝑙

−
2

1
2

(cid:19)

−

(cid:18)

1
𝑙

−
2

𝑟

!

𝑟!

𝑥𝑟 .

(cid:0)
= 1. Hence by applying Proposition 6.8 to the subspace of polynomials such that

(cid:1)

Note that 𝐻
𝔼𝜈 𝑝
𝑌
(

𝑌
∅(
= 0, we get

)

)

Corollary 6.9. Let 𝜈 be Gaussian. Suppose that the distribution 𝜇 is so that any polynomial of degree at
most 𝐷 is 𝜇-integrable. Then

max
ℝ
𝑌
]
[
∈

𝑝

6𝐷 (cid:0)

𝔼𝜇 𝑝

𝑌
) −
(
𝕍𝜈 𝑝

𝔼𝜈 𝑝
𝑌
(

)

2

𝑌
(

)

=

(cid:1)

𝔼
𝜇

𝑌
𝐻𝛼(

)

2

.

(cid:19)

Õ0<
𝛼
|
|
6𝐷 the space of multilinear polynomials of degree at most 𝐷 (we do not include
6𝐷 ). Note that multilinear Hermite polynomials 𝐻𝛼 (which correspond

ℳ

6𝐷 (cid:18)

Denote by

constant polynomials in
to multilinear multiindices 𝛼) are exactly

ℳ

They form a basis in the space
Applying Proposition 6.8 to the space

ℳ

6𝐷 (for 0 <

6 𝐷). Let’s denote

6𝐷 :=

6𝐷

ℋ

6𝐷 .

ℳ

ℋℳ

Ñ

𝑌
𝐻𝛼(

)

=

𝑦𝑖,𝑗 .

𝐼𝛼, 𝑗
𝐽𝛼 Ö𝑖
Ö𝑗
∈
∈
𝛼
6𝐷 we get

|

|

ℳ

Corollary 6.10. Let 𝜈 be Gaussian. Suppose that the distribution 𝜇 is so that any polynomial of degree at
most 𝐷 is 𝜇-integrable. Then

max
𝑝
∈ℳ

6𝐷 (cid:0)

𝔼𝜇 𝑝

𝑌
) −
(
𝕍𝜈 𝑝

𝔼𝜈 𝑝
𝑌
(

)

2

𝑌
(

)

(cid:1)

= max
𝑝
∈ℳ

6𝐷 (cid:0)

2

𝔼𝜇 𝑝
𝑌
(
𝔼𝜈 𝑝2

)
𝑌
(

(cid:1)
)

=

𝔼
𝜇

𝑌
𝐻𝛼(

)

2

.

(cid:19)

𝑌
Õ𝐻𝛼
(

)∈ℋℳ

6𝐷 (cid:18)

Hence the key part of proving lower bounds for low degree polynomial estimators is bounding

𝔼𝜇 𝐻𝛼(
𝑌

.
)

6.5 Spiked covariance model with sparsity (proof)

In this section we prove Theorems 6.4 and 6.5 .

The proofs will be based on two steps: ﬁrst, we compute the expectation of Hemrite polynomials

under the planted distribution, then we bound their total contribution.

We will need the following fact about Hermite polynomials:

Fact 6.11. For any 𝑐

ℝ and any 𝑙

ℕ

∈

∈

𝔼

𝑤

0,1
)

∼𝒩(

𝐻𝑙

𝑤

(

𝑐

)

+

=

𝑐 𝑙
𝑙!

.

45

The following Lemma is a generalization (and a corrollary) of Fact 6.11.

Lemma 6.12. Let 𝑋
standard Gausian matrix independent of 𝑋. Then for any multi-index 𝛼 over

𝑑 be a random matrix such that all moments of 𝑋 exist. Let 𝑊

ℝ𝑛

∈

×

𝑛

[

𝑑

,

]

] × [

𝔼 𝐻𝛼(
𝑊

𝑋

)

+

=

𝔼 𝑋 𝛼
𝛼!

.

Proof. By Fact 6.11,

𝔼 𝐻𝛼(
𝑊

𝑋

)

+

= 𝔼
𝑋
= 𝔼
𝑋

𝔼
𝑊 [

𝑊

𝐻𝛼(
𝔼
𝑊

𝑋

𝑋

) |
+
𝑊𝑖𝑗
𝐻𝛼𝑖𝑗(

𝑖,𝑗
𝛼
Ö(
)∈

]

𝑋𝑖𝑗

)

+

𝑋

(cid:3)

(cid:12)
(cid:12)

𝑋

(cid:2)
𝛼𝑖𝑗
𝑖𝑗
𝛼𝑖𝑗!

= 𝔼
𝑋

𝑖,𝑗
𝛼
Ö(
)∈
𝔼 𝑋 𝛼
𝛼!

.

=

ℝ𝑛

𝑑 be a

×

∈

(cid:3)

Now we can exactly compute the expectation of Hermite polynomials under the planted distri-

bution.

Lemma 6.13. Let 𝛼 be a multi-index over
every vertex of the multigraph 𝐺𝛼 has even degree, then

] × [

𝑛

𝑑

[

]

and let 𝜇 be the planted distribution of Problem 6.3. If

𝔼
𝑌
𝜇

𝑌
𝐻𝛼(

=

1
𝛼!

𝛽
𝑘

𝛼

2
|/

|

𝐽𝛼

|

|

𝑘
𝑑

𝔼 𝑢

𝑖

deg𝐺𝛼 (
𝑖

)

(cid:19)
(cid:12)
(cid:12)
(cid:12)
and if at least one vertex of 𝐺𝛼 has odd degree, then 𝔼𝜇 𝐻𝛼(
𝑌
Proof. By lemma 6.12,

(cid:19)

(cid:18)

(cid:18)

∼

)
(cid:12)
(cid:12)
(cid:12)

·

𝐼𝛼
Ö𝑖
∈
= 0.

)

𝔼
𝑌
𝜇

∼

=

𝑌
𝐻𝛼(

)

𝔼

𝛼

𝛽𝑢𝑣𝑇
𝛼!

)

(
p

=

𝛽|

𝛼

2
|/
𝛼!

𝔼

𝑢

𝛼𝑖𝑗
𝑖 𝑣

𝛼𝑖𝑗
𝑗

=

𝐼𝛼
Ö𝑖
∈
𝐽𝛼
𝑗
∈

𝛽|

𝛼

2
|/
𝛼!

𝛼𝑖𝑗

𝐽𝛼,𝑖

𝑗

∈

𝔼 𝑢

𝑖
Í

𝐼𝛼
Ö𝑖
∈

!

𝐽𝛼
Ö𝑗
∈
©

=
𝑗
Notice that deg𝐺𝛼 (
𝐼𝛼,𝑖 𝛼𝑖𝑗 and deg𝐺𝛼 (
)
least one vertex of 𝐺𝛼 has odd degree, then 𝔼𝑌
Í

)

∈

𝑖

𝑗

𝑖

=
𝐽𝛼, 𝑗 𝛼𝑖𝑗. By symmetry of each 𝑢𝑖 and 𝑣 𝑗, if at
«
∈
𝑌
𝜇 𝐻𝛼(
Í

= 0.

)

∼

Now assume that each vertex has even degree. Then 𝔼 𝑣

deg𝐺𝛼 (

𝑗

) and

𝑗

deg𝐺𝛼 (
𝑗

)

= 𝑘
𝑑

1
√𝑘

(cid:17)

(cid:16)
deg𝐺𝛼 (
𝑗

𝑗

)

𝔼 𝑣

·

𝐽𝛼
Ö𝑗
∈

𝔼 𝑢

𝑖

deg𝐺𝛼 (
𝑖

)

.

·

𝐼𝛼
Ö𝑖
∈

(cid:3)

𝔼
𝑌
𝜇

∼

𝑌
𝐻𝛼(

)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

=

𝛽|

𝛼

2
|/
𝛼!

= 1
𝛼!

𝛽
𝑘

(cid:18)

𝔼 𝑢

𝑖

deg𝐺𝛼 (
𝑖

)

𝐼𝛼
Ö𝑖
∈
2
𝛼
|/
|

(cid:19)

𝐽𝛼

|

|

𝑘
𝑑

(cid:19)

(cid:18)

46

𝛼𝑖𝑗

𝐼𝛼, 𝑗

𝑖

∈

.

𝔼 𝑣

𝑗
Í

ª
®
¬

 
 
Lemma 6.14. Let 𝐺𝛼 =
Then

𝐼𝛼 , 𝐽𝛼, 𝐸𝛼)

(

be a bipartite multigraph and let 𝑢 be the same as in the Problem 6.3.

𝔼 𝑢

𝑖

deg𝐺𝛼 (
𝑖

)

6 2|

𝛼

|

𝑅|

𝛼

2
|−

|

𝐼𝛼

| .

·

𝐼𝛼
Ö𝑖
∈

Proof. If 𝑅 = 1 the statement is true. Assume that 𝑅 > 1. Let’s denote deg𝐺𝛼 (

𝑖

)

= 𝑥𝑖. Notice that

𝔼 𝑢 𝑥𝑖
𝑖

6

min

𝐼𝛼
Ö𝑖
∈
𝐼𝛼 , 𝑥𝑖 > 2𝑅2
∈
𝑥𝑖/
2

Ö𝑖
𝐼𝛼
∈
and 𝑋′′ =

𝑅𝑥𝑖 ,

{

𝑥𝑖 /
2

.

}

𝑥𝑖
2

(cid:16)

(cid:17)

𝑖

|

∈

{

𝐼𝛼 min

𝑥𝑖
{
𝑅𝑥𝑖 ,

{
is maximal (for ﬁxed

Let’s denote 𝑋′ =
𝑖
𝑥𝑖
value
2
to 2, or 𝑋′ is empty and there can be only one 𝑥𝑖
(cid:0)
(cid:1)
for some 𝑥𝑎
∈
∈
𝑥𝑖 /
2
𝑅𝑥𝑖 ,
𝐼𝛼 min
}
∈
If 𝑋′ is empty, then

. Let’s show that if the
𝑋′′ are equal
|
𝑋′′ that is greater than 2. Indeed, if 𝑥𝑎 > 𝑥𝑏 > 2
𝑋′′, we can increase 𝑥𝑎 by 2 and decrease 𝑥𝑏 by 2. This operation increases
.

), then either all 𝑥𝑖

𝐼𝛼 , 2 6 𝑥𝑖 < 2𝑅2

𝐼𝛼 , 𝑥𝑏
𝑥𝑖
2

𝑥𝑖
|
𝛼

∈
and

𝐼𝛼 |

Î

∈

∈

}

}

{

}

𝑖

|

|

𝑖

Î

(cid:0)

(cid:1)

min

𝑅𝑥𝑖 ,

{

𝑥𝑖
2

𝐼𝛼
Ö𝑖
∈

where 𝑠 = max

(cid:16)
𝑋′′}
.
{
Now assume that all 𝑥𝑖

𝑥𝑖

∈

(cid:17)

𝑠

2
/

6

𝑥𝑖 /
2

6

}

𝑠
2

(cid:17)

(cid:16)

𝑠
2

(cid:16)

(cid:17)

𝛼

2
−|
|/

|

𝐼𝛼

1
|+

6 |

𝛼
2

|

𝑅|

𝛼

2
|−

|

𝐼𝛼

| 6 2|

𝛼

|

𝑅|

𝛼

2
|−

|

𝐼𝛼

| ,

·

𝑋′′ are equal to 2 and 𝑋′ is nonempty. In this case 2𝑅2 6

∈

, so

𝛼

|

|

𝑅𝑥𝑖 ,

min

{

𝑥𝑖/
2

}

𝑥𝑖
2

(cid:16)

(cid:17)

𝐼𝛼
Ö𝑖
∈

6 𝑅|

𝛼

2
(|
|−

𝑋′′|−
1
) 6 |

𝛼
|
2 ·

𝑅|

𝛼

2
|−

|

𝑋′′| .

Notice that

Hence

𝛼

|

|

> 2𝑅2

𝐼𝛼 | − |

(|

𝑋′′

|) + |

𝑋′′

|

= 2

𝑋′′

|

|

1

−

𝑅2

2𝑅2

,

𝐼𝛼 |

|

1

−

𝑅2 +

2

𝐼𝛼 |

|

1

(cid:0)
=

1

(cid:18)

𝑅2

𝑅2

−

+

𝑅2

(|

1

(cid:19)

−

𝛼

2

𝐼𝛼 |)

|

.

| −

+

1

(cid:1)

𝛼

|

| −

2

|

𝑋′′

|

6

𝛼

|

| − |

𝛼

|

1

It follows that

𝔼 𝑢 𝑥𝑖
𝑖

6 |

𝛼
|
2 ·

𝑅

(cid:16)

1
+

1
𝑅2

−

1

(cid:17)

𝛼

2
|−

|

(|

𝐼𝛼

|) 6 |

𝛼
|
2 ·

𝑒(|

𝛼

2
|−

|

𝐼𝛼

2
|)/

·

𝑅|

𝛼

2
|−

|

𝐼𝛼

| 6 2|

𝛼

|

𝑅|

𝛼

2
|−

|

𝐼𝛼

| ,

·

𝐼𝛼
Ö𝑖
∈

where we used the fact that for 𝑅 > 1,

ln 𝑅
𝑅2

1 < 1
2 .

Lemma 6.15. Let
= 𝐵,
𝐼𝛼 |

= 𝐴,

𝐽𝛼|

|

|

𝐸
𝒢(
)
𝐸𝛼 |
|

−
be a set of all bipartate multigraphs 𝐺𝛼 =
= 𝐸 and each vertex of 𝐺𝛼 has even nonzero degree. Then

𝐼𝛼 , 𝐽𝛼, 𝐸𝛼)

(

such that 𝐼𝛼 ⊆ [

(cid:3)

𝑛

, 𝐽𝛼 ⊆ [

]

𝑑

]

,

𝔼
𝑌
𝜇

∼

𝑌
𝐻𝛼(

)

(cid:19)

2

6

𝑛
𝐴𝑅4

𝐴

·

(cid:17)

(cid:16)

Õ𝛼 : 𝐺𝛼
∈𝒢(

𝐴,𝐵,𝐸

(cid:18)

)

𝐴𝑅2𝛽

60

·

𝑑, 𝑘

·

ln

𝑑

(

·

𝐸

/

· (|

𝑘2

)| +

1
)

𝐸

.

o

ª
®
®
¬

min

√𝐸

n

©

«

47

 
 
Proof. We can assume that 𝐴 6 𝐸
choose 𝐴 vertices from
6.13 and Lemma 6.14

𝑛

]

[

2 and 𝐵 6 𝐸
𝑑

/

and 𝐵 vertices from

[

2 (otherwise
/
. Then we choose 𝛼𝑖𝑗 so that
]

𝐴, 𝐵, 𝐸

𝒢(

)

is empty). We have to
= 𝐸. By Lemma

𝛼

|

|

𝔼
𝑌
𝜇

∼

𝑌
𝐻𝛼(

)

(cid:19)

2

6

𝑛
𝐴

(cid:18)

(cid:19) (cid:18)

𝑛
𝐵

Õ𝛼 : 𝐺𝛼
∈𝒢(

𝐴,𝐵,𝐸

)(cid:18)

1
𝛼!

2

(cid:19)

(cid:18)

𝐸

𝛽
𝑘

(cid:19)

𝑘
𝑑

(cid:18)
𝐸

2𝐵

22𝐸

·

𝑅2𝐸

4𝐴

−

2𝐵

·

𝑅2𝐸

4𝐴 .
−

(cid:19)

𝑘
𝑑

(cid:19)

1
𝛼!

𝛽
𝑘

(cid:18)

(cid:19)

(cid:18)

(cid:19) Õ|
=𝐸(cid:18)
𝛼
|
𝑛
𝐵

𝑛
𝐴

(cid:19) (cid:18)

(cid:19) Õ|
=𝐸
𝛼
|

6 4𝐸

·

(cid:18)

By the multinomial theorem,

Therefore,

1
𝛼!

𝐴

= (

𝐸
)

𝐵
·
𝐸!

.

=𝐸
𝛼
Õ|
|

𝔼
𝑌
𝜇

∼

𝑌
𝐻𝛼(

)

(cid:19)

2

6 4𝐸

𝑒

𝑛

·
𝐴

𝐴

𝑒

(cid:18)

(cid:17)

𝑑

·
𝐵

(cid:19)

·

(cid:16)

𝐵

𝑒

(

·

Õ𝛼 : 𝐺𝛼
∈𝒢(

𝐴,𝐵,𝐸

)(cid:18)

6 4𝐸 𝑒 𝐴

+

𝐵

𝐸

+

𝐵𝐸

·

𝐸𝐸

𝐵𝐵 ·

·

6 30𝐸

6 60𝐸

𝑛
𝐴𝑅4

𝑛
𝐴𝑅4

𝐴

𝐴

(cid:17)

(cid:17)

·

·

(cid:16)

(cid:16)

𝐵
𝐸 ·

𝐵
𝐸 ·

·  

·  

𝑥 6 2.

where we used the inequality 𝑥1
/
𝐸, then

If 𝑘2 > 𝑑

·

𝐴
·
𝐸𝐸

𝐵

(cid:19)
𝐵

(cid:18)
𝐸
/

𝐵

𝐸
)

𝛽
𝑘

(cid:18)

𝐸

·

(cid:19)
𝑘2
𝐸
𝑑

·

𝑘2
𝑑

(cid:18)

(cid:18)

𝐸
𝐵

(cid:18)
𝐵

𝐸
/

(cid:19)
𝑘2
𝐸

𝑑

(cid:18)

·

(cid:19)

𝛽
𝑘

𝐸

(cid:19)

(cid:18)

2𝐵

𝑘
𝑑

(cid:19)

𝑅2𝐸

4𝐴

−

·

𝑅2𝐸

4𝐴

−

𝐵

𝐸
/

·

(cid:19)

·

(cid:16)
𝐴𝑅2

𝑛
𝐴

𝐴

𝐴𝐸

𝐸

(cid:17)
𝛽
𝑘 !

·

𝐴𝑅2

·

𝐸

,

𝛽
𝑘 !

·

𝐵

𝐸
/

𝐵
𝐸 ·

𝑘2
𝐸

·

(cid:19)

𝑑

(cid:18)

𝐴𝑅2

·

𝛽
𝑘

6 1
2

·

2
1
/

𝑘2
𝐸

·

(cid:19)

𝑑

(cid:18)

𝐴𝑅2

·

𝛽
𝑘

·

6

𝐴𝑅2𝛽
√𝐸

𝑑

·

,

and if 𝑘2 < 𝑑

𝐸, then

·

𝐵
𝐸 ·

𝑘2
𝐸
𝑑

·

(cid:18)

(cid:19)

𝐵

𝐸
/

𝐴𝑅2

·

𝛽
𝑘

·

6 min

1

𝑒 ln

n

𝐸
𝑑
·
𝑘2

𝐴𝑅2

𝛽
𝑘

·

6

,

1
2

·

o

since 𝑥𝑎𝑥 6

𝑒 ln

1
1
/

(

𝑎

)

(cid:17)
for all 𝑥 > 0 and 0 < 𝑎 < 1. Hence

(cid:16)

𝐴𝑅2𝛽

𝑘

·

ln

𝐸
𝑑
·
𝑘2

(cid:16)

(cid:16)

(cid:17)

1

+

(cid:17)

𝔼
𝑌
𝜇

∼

𝑌
𝐻𝛼(

)

(cid:19)

2

6

𝑛
𝐴𝑅4

𝐴

·

(cid:17)

(cid:16)

Õ𝛼 : 𝐺𝛼
∈𝒢(

𝐴,𝐵,𝐸

)(cid:18)

𝐴𝑅2𝛽

60

·

𝑑, 𝑘

·

ln

𝑑

(

·

𝐸

/

· (|

𝑘2

)| +

1
)

min

√𝐸

n

©

«

48

o

ª
®
®
¬

(cid:3)

,

𝐸

.

 
 
Proof of Theorem 6.4. If 𝐴 6 𝐸

2 6 𝑛

𝑅4 , the function

/

is a monotone in 𝐴. Hence

Therefore, by Lemma 6.15,

𝑛
𝐴𝑅4

(cid:16)

𝐴

𝐴𝐸

·

𝑛
𝐴𝑅4

(cid:17)

(cid:16)
𝐴

𝐴𝑅2

𝐸 6

𝑛𝐸

(

2 .
/

𝐸
)

(cid:1)

·

(cid:0)

(cid:17)

2

=

𝔼
𝑌
𝜇

∼

𝑌
𝐻𝛼(

)

(cid:19)

Õ0<
𝛼
|
|

6𝐷 (cid:18)

Õ26𝐸6𝐷 Õ16𝐴6𝐸
16𝐵6𝐸

2 Õ𝛼 : 𝐺𝛼

∈𝒢(

𝐴,𝐵,𝐸

2
/
/

𝑌
𝐻𝛼(

)

𝔼
𝑌
𝜇

∼

)(cid:18)

2

(cid:19)

Õ26𝐸6𝐷 Õ16𝐴6𝐸
16𝐵6𝐸

2
2 ©
/
/

min

√𝐸

n

60

·
𝑑, 𝑘

·

𝐸2
4 ·

Õ26𝐸6𝐷

«

min

©

𝑛,

𝑑

/

np

6

6

6

·
𝑑

√𝑛√𝐸

ln

(

· (|

60𝛽

𝛽

𝑘2

𝐸

/

·

)| +

1
)

𝐸

o

ª
®
®
¬

𝐸

ln

𝐸

𝑑

(

·

/

· (|

𝑘2

)| +

√𝐸𝑛

𝑘

/

(cid:16)

(cid:17)

120𝛽

1
)

𝐸

o

.

ª
®
®
¬

Õ26𝐸6𝐷

©

«
min

np

𝑛,

𝑑

/

√𝐸𝑛

𝑘

/

(cid:17)

(cid:16)

ln

𝐸

𝑑

(

·

/

· (|

𝑘2

)| +

1
)

If

where 0 < 𝜀 < 1

1000 , then

𝛽 = 𝜀

min

·

𝑑
𝑛

,

𝑘
√𝐷𝑛 ·

«

nr

𝔼
𝑌
𝜇

∼

𝑌
𝐻𝛼(

)

(cid:19)

Õ0<
𝛼
|
|

6𝐷 (cid:18)

ln

𝐷

·
𝑘2

(cid:18)

(cid:18) (cid:12)
(cid:12)
(cid:12)
(cid:12)
6 10002

2

1

,

(cid:19) o

𝑑

·

+

(cid:19) (cid:12)
(cid:12)
(cid:12)
(cid:12)
𝜀2 ,

and using Corollary 6.9 we get the desired conclusion.

Proof of Theorem 6.5. If 𝐸
that 𝐸

𝑅4. Thus

2 > 𝑛

/

/

By Lemma 6.15,

2 6 𝑛

/

/

𝑅4, we get the same bound as in the theorem 6.4. So we can assume

𝑛
𝐴𝑅4

𝐴

(cid:17)

(cid:16)

6 𝑒 𝑛

/

𝑅4 6 𝑒𝐸

2 .
/

o

ª
®
®
¬

(cid:3)

2

=

𝔼
𝑌
𝜇

∼

𝑌
𝐻𝛼(

)

(cid:19)

Õ0<
𝛼
|
|

6𝐷 (cid:18)

Õ26𝐸6𝐷 Õ16𝐴6𝐸
16𝐵6𝐸

2
2 ©
/
/

min

√𝐸

n

60√𝑒

𝐴𝑅2

·

𝑑, 𝑘

ln

𝑑

(

· (|

·

·

·

𝛽

𝑘2

𝐸

/

)| +

1
)

100𝑅2𝛽

𝐸

o

ª
®
®
¬

𝐸

𝐴,

𝑑

/

·

𝐴

𝑘

(

/

) · (|

ln

𝑑

(

·

𝐴

/

𝑘2

)| +

1
)

o

ª
®
®
¬

𝐸2
4 ·

6

Õ26𝐸6𝐷

«

min

√𝐸

n
49

©

«

 
 
 
 
 
 
 
 
 
 
If

where 0 < 𝜀 < 1

1000 , then

6

Õ26𝐸6𝐷

©

min

np

200𝑅2𝛽

𝑛,

𝑑

/

𝑛

𝑘

(

/

) · (|

ln

𝑑

(

·

𝑛

/

𝑘2

)| +

1
)

𝛽 = 𝜀

𝑅2 ·

«
min

𝑑
𝑛

,

𝑘
𝑛 ·

nr

𝔼
𝑌
𝜇

∼

𝑌
𝐻𝛼(

)

(cid:19)

Õ0<
𝛼
|
|

6𝐷 (cid:18)

ln

𝑛
𝑑
·
𝑘2

(cid:18)

(cid:18) (cid:12)
(cid:12)
(cid:12)
(cid:12)
6 10002

2

+

(cid:19) (cid:12)
(cid:12)
(cid:12)
(cid:12)
𝜀2 ,

·

1

,

(cid:19) o

and using Corollary 6.9 we get the desired conclusion.

𝐸

.

o

ª
®
®
¬

(cid:3)

6.6 Almost Gaussian vector in random subspace (proof)

In this section we focus on Problem 6.6 and prove that if 𝑑 is signiﬁcantly less than 𝑛 𝑠
degree multilinear polynomial can distinguish between the planted and the null distribution.

1, no low
2
+
/

The proof of Theorem 6.7 relies on key lemmata which we provide below. The proof itself is

then presented at the end of the section.

Lemma 6.16. Let 𝛼 be a multiindex over

𝑛

[

𝑑

]

𝑌
such that 𝐻𝛼(

] × [

) ∈ ℋℳ

6𝐷 . Then

𝔼
𝜇

𝑌
𝐻𝛼(

)

= 𝔼

𝐽𝛼,𝑖 |

𝔼

𝜎|
𝑖

(cid:18)

𝐼𝛼
𝑖

∈

Î


, 𝜎𝑖 := √𝑛𝑢𝑖 and 𝑧𝑖𝑗 := 𝜎𝑖𝑤𝑖𝑗.



𝐽𝛼
𝑗
∈
Î

(cid:19)

where 𝑗

, 𝑖

𝑑

]

𝑛

]

∈ [

∈ [

𝑧𝑖𝑗

1
√𝑛 (

𝑣 𝑗

−

1
√𝑛

+

𝑧𝑙 𝑗

𝑛
Õ𝑙
]
∈[

,

)


ª

®


¬


𝑖

𝐼𝛼, 𝑗
∈
Î

©

«

Proof. We drop the subscript 𝛼 for the exposition of the proof.

𝑧𝑙 𝑗

𝑛
Õ𝑙
]
∈[

𝑧𝑙 𝑗



ª

®


¬


(as 𝜎𝑖𝑤𝑖𝑗 = 𝑤𝑖𝑗
𝜎𝑖

)

𝔼
𝜇

𝑌
𝐻𝛼(

)

= 𝔼
𝜇

= 𝔼

= 𝔼

𝑤𝑖𝑗

𝑦𝑖,𝑗

𝐽
𝑗
∈
Î

𝐼𝑗
𝑖
∈
Î

𝐽
𝑗
∈
Î

𝐼
𝑖
∈
Î

(cid:2)

𝐽
𝑗
∈
Î

𝐽
𝑗
∈
Î

𝐼𝑗 
𝑖
∈

Î



𝜎𝑖


𝐼𝑗
𝑖
∈
Î

𝑤𝑖𝑗

+

𝑢𝑖

𝑣 𝑗

− h

𝑢, 𝑤 𝑗

i

(cid:1)(cid:3)

(cid:0)

𝜎𝑖

+

1
√𝑛

𝑣 𝑗

1
𝑛

−

©

𝑧𝑖𝑗

1
«
√𝑛

𝑣 𝑗

1
𝑛

−

+

= 𝔼

= 𝔼

= 𝔼








©

𝜎𝑖
«

!

𝐽
𝑗
∈
Î

𝐼𝑗
𝑖
∈
Î

©

𝐽
𝑗
∈
Î

𝐼𝑗
𝑖
∈
Î

𝑛
Õ𝑙
]
∈[

𝑧𝑖𝑗

+

1
√𝑛

𝑣 𝑗

−

ª
®
1
¬
𝑛

𝑛
Õ𝑙
]
∈[

𝑧𝑖𝑗

1
√𝑛 (

𝑣 𝑗

−

1
√𝑛

+

𝔼

𝜎𝑖

!

𝐽
𝑗
∈
Î

𝐼𝑗
𝑖
∈
Î

«
𝐼𝑗
𝑖
∈
Î

𝐽
𝑗
∈
Î

50








©

«

𝑧𝑙 𝑗



ª

®


𝑧𝑙 𝑗
¬

𝑛
Õ𝑙
]
∈[

)


ª

®


¬


 
 
 
 
 
 
 
 
 
= 𝔼

𝐽𝑖 |

𝜎|
𝑖

𝔼

𝑧𝑖𝑗

1
√𝑛 (

𝑣 𝑗

1
√𝑛

𝑧𝑙 𝑗

)
ª
®
¬
An immediate consequence of Lemma 6.16 is the following statement:

𝐼𝑗
𝑖
∈
Î

𝑛
Õ𝑙
]
∈[

𝐼
𝑖
∈
Î

𝐽
𝑗
∈
Î








+

−

«

©

(cid:18)

(cid:19)

.








(cid:3)

Corollary 6.17. Let 𝛼 be a multiindex over
𝑖

(respectively,

𝐼𝛼) such that

𝐼𝛼,𝑗

𝐽𝛼,𝑖

[

𝑛

𝑌
such that 𝐻𝛼(
𝑑
] × [
= 0.
) is odd, then 𝔼𝜇 𝐻𝛼(
𝑌
)

]

|

|

|

|

∈

) ∈ ℋ

6𝐷. If there exists 𝑗

𝐽𝛼 (or

∈

In the following lemma we use the fact that ﬁrst 𝑠 moments of coordinates of 𝑣 coincide with

Gaussian moments.

Lemma 6.18. Let 𝑠 be the parameter of the planted distribution, let 𝛼 be a multiindex over
that there exists 𝑗0 ∈

6 𝑠. Then 𝔼𝜇 𝐻𝛼(
𝑌
Proof. For simplicity we will the subscript 𝛼. If 𝔼

𝐽𝛼 such that

𝐼𝑗0 |

= 0.

)

|

𝐽𝑖 |

𝜎|
𝑖

= 0, the statement is obviously true.

𝑛

[

𝑑

]

]×[

. Suppose

(cid:18)
= 1 (notice that this expectation can be only 0 or 1). Thus

(cid:19)

𝐼
𝑖
∈
Î

Assume that 𝔼

𝐽𝑖 |

𝜎|
𝑖

(cid:18)

𝐼
𝑖
∈
Î

𝔼
𝜇

𝑌
𝐻𝛼(

)

=

𝐽
Ö𝑗
∈

(cid:19)

𝔼

𝐼𝑗
𝑖
∈
Î

©
𝑧𝑖𝑗0 +
«

= 𝔼

𝑖

𝐼𝑗0
∈
Î

©

«

𝑧𝑖𝑗

1
√𝑛 (

𝑣 𝑗

−

1
√𝑛

+

1
√𝑛 (

𝑣 𝑗0 −

1
√𝑛

𝑛
Õ𝑙
]
∈[

𝑧𝑙 𝑗

𝑛
Õ𝑙
]
∈[

)
ª
®
¬
·

𝑧𝑙 𝑗0)
ª
®
¬

𝔼

𝐼𝑗
𝑖
∈
Î

𝑗0
𝐽
Ö𝑗
\{
∈

}

𝑧𝑖𝑗

1
√𝑛 (

𝑣 𝑗

−

1
√𝑛

+

©

«

𝑧𝑙 𝑗

𝑛
Õ𝑙
]
∈[

.

)
ª
®
¬

Since ﬁrst 𝑠 moments of 𝑣 𝑗0 coincide with Gaussian moments,

𝔼

𝑖

𝐼𝑗0
∈
Î

©

𝑧𝑖𝑗0 +

1
√𝑛 (

𝑣 𝑗0 −

1
√𝑛

𝑛
Õ𝑙
]
∈[

where 𝜁 is a standard Gaussian variable that
1
𝑧𝑖𝑗0 +
√𝑛 (
𝜁, 𝑧1𝑗0 , . . . , 𝑧𝑛 𝑗0:

. Let’s show that 𝜉

«
𝑧𝑙 𝑗0)

𝑛
∈[
Í

1
√𝑛

𝑁

−

𝜁

]

𝑙

𝑧𝑙 𝑗0)
ª
®
¬
∼

= 𝔼

𝑖

𝑧𝑖𝑗0 +

1
√𝑛 (

𝐼𝑗0
∈
Î
is independent

©

1
√𝑛

,

𝜁

−

𝑛
Õ𝑙
]
∈[

𝑧𝑙 𝑗0)
ª
®
from all 𝑧𝑖𝑗0 . Let 𝜉𝑖 =
¬
ℝ𝑛 is a linear transformation of

«
0, Id𝑛
. 𝜉
)
(

∈

𝜉 = 𝐴

©

𝜁
𝑧1𝑗0
...
𝑧𝑛 𝑗0

,

, 1
𝑛 , . . . , 1
𝑛 ,

1
(

−

1
𝑛 )

, 1
𝑛 , . . . , 1
𝑛 )

. The rows of 𝐴

ª
®
®
®
®
1
¬
√𝑛

where 𝐴 is an 𝑛

𝑛

1
)

+

× (

matrix with rows 𝐴𝑖

T =
«

(

are orthonormal: for all 𝑖

𝑛

]

∈ [
1
1
𝑛 + (

T
𝐴𝐴

(

)

𝑖𝑖 =

𝑖

}

1
+
{z
1
𝑛 −

|
1
𝑛 +

1
𝑛2

= 1 ,

1
2
𝑛 )

+

𝑛

−
𝑛2

1

−

= 1

2
𝑛 +

1
𝑛2 +

−

51

 
 
 
 
 
 
 
 
 
 
  
  
and for all diﬀerent 𝑖, 𝑙

𝑛

∈ [
]
𝑖𝑙 = 1

T
𝐴𝐴

)
Hence 𝐴𝐴T = Id𝑛 and 𝜉

(

2
1
𝑛 (

−

1
𝑛 ) +

𝑛

−
𝑛2

2

= 1

𝑛 −

2
𝑛 +

2
𝑛2 +

1
𝑛 −

2
𝑛2

= 0 .

𝑛 −

𝑁

0, Id𝑛
(

. Therefore,
)

∼

𝔼

𝑖

𝐼𝑗0
∈
Î

©

𝑧𝑖𝑗0 +

1
√𝑛 (

𝑣 𝑗0 −

1
√𝑛

𝑛
Õ𝑙
]
∈[

𝜉𝑖 = 0 .

= 𝔼

𝑖

𝐼𝑗0
∈
Î

𝑧𝑙 𝑗0)
ª
®
¬

(cid:3)

, 𝐼𝑗

𝑑

]

∈ [

𝑛

]

⊆ [

with even

Lemma 6.19. Let 𝑠, 𝛿, 𝜆 be the same as in the statement of Theorem 6.7. Let 𝑗
cardinality

> 𝑠. Then, if

𝐼𝑗

|

|

«

6 𝜆2
100 ,

𝐼𝑗

|

|

and if

𝐼𝑗

|

|

> 𝜆2
100 ,

𝐼𝑗
𝑖
∈
Î

©

«

𝔼








𝔼

𝑧𝑖𝑗

1
√𝑛 (

𝑣 𝑗

−

1
√𝑛

+

𝑧𝑙

𝑛
Õ𝑙
]
∈[

)
ª
®
¬








𝑧𝑖𝑗

1
√𝑛 (

𝑣 𝑗

−

+

1
√𝑛

𝑧𝑙

6

6 𝛿

220

𝑠
·
√𝑛

(cid:18)

𝜆

·

𝐼𝑗 |

|

,

(cid:19)

100

𝐼𝑗

|

𝐼𝑗 |

|

.

!

|
√𝑛
p

𝑛
Õ𝑙
]
∈[
Proof. We drop the subscript 𝑗 to simplify the notation (in particular, in this proof we denote 𝑣 𝑗 by
𝑣). By symmetry of the Gaussian distribution, opening up the product we see that in order for a
monomial to have non-zero expectation, for any left end term 𝑧𝑖 there must be a corresponding
right term 1

𝑣

©

«

𝐼𝑗
𝑖
∈
Î








1
√𝑛

√𝑛 (

−

𝑧𝑙

. Hence:
)

)


ª

®


¬


𝑙

𝑛
∈[
Í

]

𝑧𝑖

𝑣

1
√𝑛 (

−

1
√𝑛

+

𝑧𝑙

𝑛
Õ𝑙
]
∈[

𝔼

𝐼
𝑖
∈
Î








©

«

)


ª

®


¬


Since 𝑣 is symmetric:

𝐼

|

2𝑟

|−

𝔼

𝑧2
𝑖

𝑣

𝑖

!

𝑟
]
∈[
Î



©



By Cauchy–Schwarz:

«


1
√𝑛

−

𝑛
Õ𝑙
]
∈[

𝑖

𝑟
∈[
Î

]

𝔼








1
√𝑛

𝑧2
𝑖

!

©

«

𝑧𝑙

𝑛
Õ𝑙
]
∈[

𝑧𝑙

ª
®
¬

2𝑚








ª
®
¬

𝑟

𝐼

|

|−









.

.
















ª
®
¬

𝑧𝑙

2𝑚

ª
®
¬

𝐼

2
|/

|

=

Õ𝑟=0 (cid:18)

𝐼
|
|
2𝑟

2𝑟
𝑟

(cid:19)

(cid:19) (cid:18)

𝔼

=

1
𝐼
𝑛 |

2
|/

𝐼

2
|/

|

Õ𝑟=0 (cid:18)

𝐼
|
|
2𝑟

(cid:19) (cid:18)

𝑖

𝑟
]
∈[
Î







2𝑟

𝑟

𝑧𝑖

𝐼

𝑛 |

!

1
2
−
|/

𝑟

2
/

𝑣

−

1
√𝑛

𝑧𝑙

𝑛
Õ𝑙
]
∈[

𝐼

|

𝑧𝑙

ª
®
2𝑟
|−
¬

©

«
−

1
√𝑛

𝑛
Õ𝑙
]
∈[

𝔼

(cid:19)









𝑖

𝑟
]
∈[
Î

𝑣

𝑧2
𝑖

!

©

«

𝐼

𝑣 |

2𝑟

|−

2𝑚

−

𝔼
[

𝔼

]

𝐼

𝑟

2
−
|/

|

=

Õ𝑚=0









𝑧2
𝑖

! ·

1
√𝑛

©

«

𝑖

𝑟
]
∈[
Î








4𝑚

2
1
/

𝑛
Õ𝑙
]
∈[

6

𝔼

𝑧4
𝑖

!

𝑖

𝑟
∈[
Î

]

2
1
/

𝔼

©

«

©

«

52

6 3𝑟

2
/

2𝑚

· (

𝑚 .

)

1
√𝑛

𝑧𝑙

𝑛
Õ𝑙
]
∈[

ª
®
¬

ª
®
®
¬

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Hence,

𝑖

𝑟
]
∈[
Î

𝔼









𝑧2
𝑖

!

𝑣

−

1
√𝑛

©

«

𝐼

|

2𝑟

|−

𝑧𝑙

𝑛
Õ𝑙
]
∈[

ª
®
¬









𝐼

𝑟

2
−
|/

|

6

Õ𝑚=0

𝐼

𝑣 |

2𝑟

|−

2𝑘

−

𝔼

[

3𝑟

2
/

2𝑚

· (

𝑚

)

] ·

𝐼

𝑟

2
−
|/

|

6 3𝑟

2
/

Õ𝑚=0 (cid:16)

𝐼

2
|/

|

6 3|

𝐼

4
|/

𝐼

𝛿𝜆|

2𝑚

|−

Õ𝑚=0(cid:16)
. Thus 2𝑀 >

𝐼

𝛿𝜆|

2𝑟

|−

2𝑚

−

10√𝑠 ln 𝑠

+ (

2𝑟

|−

2𝑚

−

𝐼

|

)

10√𝑠 ln 𝑠

+ (

2𝑚

|−

𝐼

|

)

(cid:17)

2𝑚

· (

2𝑚

𝑚

)

· (

𝑚 .

(cid:17)

)

Let 𝑀 = max

get:

𝛿𝜆|

𝐼

| ,

𝐼

2,
|/

𝐼

|

| |

10√𝑠 ln 𝑠
(

)|

𝐼

|}

{

𝐼

𝛿𝜆|

2𝑚

|−

10√𝑠 ln 𝑠

+ (

2𝑚

|−

𝐼

)|

(cid:17)

𝑚. We

2𝑚

)

· (

𝑧𝑖

𝑣

1
√𝑛 (

−

1
√𝑛

+

𝑧𝑙

𝑛
Õ𝑙
]
∈[

𝔼

𝐼
𝑖
∈
Î








©

«

)
ª
®
¬








𝐼
|
|
2𝑟

2𝑟
𝑟

(cid:19)

(cid:19) (cid:18)

3|

𝐼

4
|/

𝐼

· |

| ·

2𝑀

2|

𝐼

|

·

·

3|

𝐼

4
|/

·

2|

𝐼

2
|/

𝑀

·

(cid:16)

𝐼

2
|/

|

Õ𝑟=0 (cid:18)
2|

𝐼

|

2
|/

𝐼

6 1
𝑛 |
6 1
𝑛 |

𝐼

6

2 ·
𝐼

|

|/
10
√𝑛 (cid:19)

|

·

𝑀

Consider the case

𝐼

|

|

(cid:18)
100 . In this case, 𝑀 6 10|

> 𝜆2

𝐼

| · |

𝐼

| |

𝐼

2. Hence
|/

𝔼

𝑧𝑖

1
√𝑛 (

𝑣

−

1
√𝑛

+

𝐼
𝑖

∈

Î
©


6 𝜆2
𝐼

|
|
«

1.8, and if
2
)

100 . If
𝐼

|

𝐼
|
>

|
|

𝑧𝑙

6

𝑛
Õ𝑙
]
∈[

)


ª

®

> 10𝑠, then 𝛿𝜆|

¬

1.8, then
𝜆
𝜆
(
(
𝜆2 6 0.1
/

2
)
/
100𝑠

Now consider the case
𝜆
(

inequality holds if

6

𝐼

|

|

/

2
)
since 𝜆2 > 10000𝑠 ln 𝑠. If

𝜆
(

/

𝑠

/|

𝐼

|

𝐼

|

|

6 0.1

𝜆

·

𝜆

· (

2
)

/

p
< 10𝑠, then
|

𝐼

|

𝐼

|

| |

𝐼

2 <
|/

100

𝐼

|

|

.

𝐼

|

|

!

√𝑛
p

𝐼

2. Indeed, the
|/

𝐼

| >
𝑠
2
/|
)

/

𝐼

𝜆

𝜆

· (

/

·

𝐼

𝑠 >

|

|

|−

𝜆
(
|

𝐼
2
| |
)
/
is monotone in 𝐼, so
𝐼
|
|
ln 𝜆 6 1
p
1
2
/
2
)

𝜆 ,

𝑧𝑖

𝑣

1
√𝑛 (

−

1
√𝑛

+

𝑧𝑙

𝑛
Õ𝑙
]
∈[

𝔼

𝐼
𝑖
∈
Î








©

«

)
ª
®
¬








10
√𝑛 (cid:19)
10
√𝑛 (cid:19)
10
√𝑛 (cid:19)
220

6

6

6

(cid:18)

(cid:18)

(cid:18)

6 𝛿

|

|

𝐼

|

|

𝐼

|

|

10√𝑠 ln 𝑠
(
𝐼

|. Therefore,

𝐼

)|

max

{

·

𝛿𝜆|

𝐼

| ,

10√𝑠 ln 𝑠
(

)

𝐼

|

|

}

𝛿𝜆𝑠

2 max
+

𝐼

𝜆|

𝑠

2
),
+

|−(

{

1
𝛿𝜆𝑠

10√𝑠 ln 𝑠

2 (
+

𝐼

|

|

)

}

𝛿𝜆𝑠

2 max
+

𝜆|

𝐼

𝑠

), 210𝑠
2
+

|−(

10√𝑠 ln 𝑠
(

)

𝐼

|

|

}

{
𝐼

|

!

·

√𝑠 ln 𝑠
√𝑛

𝜆

·

53

|

.

(cid:3)

 
 
 
 
 
 
 
We are now ready to prove Theorem 6.7.

Proof of Theorem 6.7. For all positive integers 𝐴, 𝐵, 𝐵′, 𝐸 and 𝐸′ consider the set
= 𝐴,
of bipartite graphs 𝐺𝛼 such that
𝐽𝛼 | |

= 𝐸, 𝐵′ =

= 𝐵 and

𝛼

𝑗

|

|

|

𝑠
𝒢
𝐼𝑗

𝐴, 𝐵, 𝐵′, 𝐸, 𝐸′)
(
6 𝜆2
, 𝐸′ is a
100 }

∈
, and all vertices of 𝐺𝛼 have even degree strictly

{

|

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

number of edges adjacent to
greater than 𝑠. Let 𝐵′′ = 𝐵

𝐼𝛼 |
𝐽𝛼 |
𝐼𝑗
{
|
𝐵′ and 𝐸′′ = 𝐸

∈

𝑗

|

𝐽𝛼 |
|
6 𝜆2
100 }
𝐸′.

−

−

By lemma 6.19,

𝔼
𝜇

𝑌
𝐻𝛼(

)

𝑌
Õ𝐻𝛼
(

)∈ℋℳ

6𝐷 (cid:18)

2

6

(cid:19)

6𝐸6𝐷

𝑠
2
Õ2
)
+
(

Õ𝐴,𝐵,𝐵′,𝐸′
𝐴,𝐵,𝐵′,𝐸,𝐸′)

≠

∅

𝑠
𝒢

(

6

6𝐸6𝐷

𝑠
2
Õ2
)
+
(

Õ𝐴,𝐵,𝐵′,𝐸′
𝐴,𝐵,𝐵′,𝐸,𝐸′)

≠

∅

𝑠
𝒢

(

(cid:18)

·

(cid:16)

·

𝐸
𝐴𝐵
)
𝐸!

(

𝑛
𝐴

𝑑
𝐵

(cid:19) (cid:18)

𝛿2𝐵′

(cid:19)
220

·

√𝑠 ln 𝑠
√𝑛

𝜆

·

!

2𝐸′

2𝐸′′

100√𝐸
√𝑛 (cid:19)

(cid:18)

𝑒𝑛
𝐴

𝐴

(cid:18)

(cid:17)

𝑒𝐴𝐵
𝐸

𝐸

(cid:19)
220

𝑑𝐵

𝛿2

𝐵′

(cid:0)

(cid:1)

·

√𝑠 ln 𝑠
√𝑛

𝜆

·

!

2𝐸′

2𝐸′′

.

100√𝐸
√𝑛 (cid:19)

(cid:18)

𝐴

(

𝑠

/

+

𝑒𝑛
𝐴

2
)

2 = 𝑜
𝑛
,
)
and 𝐵′′ 6 100

Since 𝐴 6 𝐷
is monotone in 𝐴. Also notice that if
𝜆2 6 𝐸′′/(
𝑠
𝐸′/(
one if and only if there exists 𝐴 such that

. Let 𝜙
2
)
(
𝐴, 𝐵′ +
(
Consider the case 𝛿2𝑑 > 1. Assume that 𝑑 = 𝑜

𝐵′, 𝐵′′, 𝐸′, 𝐸′′)
𝐵′′, 𝐵′, 𝐸′ +
1
𝜆4 ·

𝑛
2120 ln2 𝑠𝐷

𝐸′′/
(cid:1)
(cid:0)
·

+
𝑠
𝒢

𝐸′′, 𝐸′)

𝑠

(

, 𝐵′ 6
be an zero-one indicator that is

𝐴, 𝐵, 𝐵′, 𝐸, 𝐸′)

𝑠
𝒢

≠

∅

(

≠
∅
2
2
)/
+

.

. Since 𝐷 6 𝑛0.33 and

(cid:18)

(cid:16)

(cid:17)

(cid:19)

𝜆2 > 100000𝑠 ln 𝑠, 𝑑 = 𝑜

𝑛
1020𝐷3

(cid:18)(cid:16)

𝜆2

200
/

(cid:17)

(cid:19)

. Hence

𝔼
𝜇

𝑌
𝐻𝛼(

)

𝑌
Õ𝐻𝛼
(

)∈ℋℳ

6𝐷 (cid:18)

2

6

(cid:19)

Õ06𝐵′,𝐵′′6𝐷

2 Õ06𝐸′,𝐸′′6𝐷

/

6

Õ06𝐵′,𝐵′′6𝐷

2 Õ06𝐸′,𝐸′′6𝐷

/

𝜙

(

𝐵′, 𝐵′′, 𝐸′, 𝐸′′

𝛿2𝑑

𝐵′

(cid:18)

(cid:1)

)
(cid:0)
𝐸′′/
2

𝐸

2
/

𝐸

𝐸

𝑠

2
+
2𝐸′

(cid:19)

𝑑𝐵′′

(cid:18)
𝜆

·

𝑛
𝐸

)

(cid:16)

(cid:17)
√𝑠 ln 𝑠
√𝑛

𝐵′, 𝐵′′, 𝐸′, 𝐸′′

𝜙

(

𝛿2𝑑

230

𝐵′

·

(cid:1)

·

(cid:0)

2𝐸′′

105√𝐸
√𝑛 (cid:19)
𝐸′/
2

(cid:18)

!
2120𝜆4𝐷 ln2 𝑠
𝑛

(cid:19)

𝑑𝐵′′

·

1020𝐷3
𝑛

𝐵′

2
2
)/
+

(cid:18)
𝑠

(

!
𝐵′

(cid:19)
𝑠

2
2
)/
+

𝑜

1
)
(

+

!

(cid:19)

2

+

∞

Õ𝐵′′=1 

𝑑

(cid:18)

1020𝐷3
𝑛

(cid:19)

𝐵′′

𝜆2

200
/

!

𝛿2𝑑

𝜆4𝑑

(cid:18)

(cid:18)

6 2

6 2

6 𝑜

∞

Õ𝐵′=1 

∞

Õ𝐵′=1 
.
1
)
(

2120𝜆4𝐷 ln2 𝑠
𝑛

2100𝐷 ln2 𝑠
𝑛

(

(cid:19)

54

 
 
 
Now condiser the case 𝛿2𝑑 < 1. Since 𝜆 > 210, 𝜆2𝑠

2 > 1
+

𝛿 and

2𝑠
(

2
)

+

ln 𝜆 > ln

1
𝛿

(cid:18)

(cid:19)

> ln

1
𝛿

(cid:18)

+

(cid:19)

ln

1
𝛿𝑑

(cid:19)

(cid:18)

= ln

1
𝛿2𝑑

.

(cid:19)

(cid:18)

Since 𝜆2 > 100000𝑠 ln 𝑠, 𝜆2

𝜆4𝐷𝑠2 ln2 𝑠 = 𝑜

𝑛 log2

𝛿2𝑑

/

100 > ln

1
𝛿2 𝑑

and 𝐵′′ 6 𝐸

(cid:16)
. It follows that

(cid:17)

ln

𝛿2𝑑

/|

(cid:0)

|

(cid:1)

. Let 𝑀 = max

𝐵′, 𝐵′′}

{

. Recall that

(cid:16)

𝔼
𝜇

(cid:0)

2

𝑌
𝐻𝛼(

)

(cid:19)

𝑌
Õ𝐻𝛼
(

)∈ℋℳ

6𝐷 (cid:18)

(cid:1)(cid:17)
6

Õ06𝐵′,𝐵′′6𝐷

2 Õ06𝐸′,𝐸′′6𝐷

/

𝐸

2
/

𝑀𝐸

𝑛
𝐸

)

(cid:16)

(cid:17)
√𝑠 ln 𝑠
√𝑛

2𝐸′

𝜆

·

!

2𝐵′
𝐸′ 𝑀

·

(cid:1)

)
(cid:18)
(cid:0)
𝐸′′/
2

𝐵′, 𝐵′′, 𝐸′, 𝐸′′

𝜙

(

𝛿2𝑑

230

𝐵′

·

(cid:1)

·

(cid:0)

𝐵′, 𝐵′′, 𝐸′, 𝐸′′

𝛿2𝑑

𝜙

(

𝑑𝐵′′

·

(cid:18)

1020𝐷3
𝑛

(cid:19)

2𝐸′′

𝑑𝐵′′

(cid:18)

105√𝐸
√𝑛 (cid:19)
2120𝜆4𝑠2 ln2 𝑠
𝑛

𝐸′/
2

(cid:19)

Õ06𝐵′,𝐵′′6𝐷

2 Õ06𝐸′,𝐸′′6𝐷

/

6

6

Õ06𝐵′′6𝐷

2 Õ06𝐸′,𝐸′′6𝐷 Õ06𝐵′6𝐸′

/

𝜙

(

𝐵′, 𝐵′′, 𝐸′, 𝐸′′

2120𝜆4𝐷𝑠2 ln2 𝑠
𝛿2𝑑

ln2

𝑛

(

)

𝐸′/
2

(cid:19)

)
(cid:18)
𝐸′′/
2

𝑑𝐵′′

·

1020𝐷3
𝑛

(cid:18)

∞

𝑑

(cid:18)

Õ𝐵′′=1 

(cid:19)
1020𝐷3
𝑛

𝐵′′

𝜆2

200
/

(cid:19)

!

2130𝜆4𝐷𝑠2 ln2 𝑠
𝛿2𝑑

ln2

𝑛

(

)

𝐸′/
2

(cid:19)

2

+

6 ∞

Õ𝐸′=1(cid:18)
.
1
)
(

6 𝑜

By Corollary 6.9, we get the desired colclusion.

(cid:3)

7 Polynomial-based algorithm with the right log factor

In this section we will prove the following theorem.

Theorem 7.1. Let 𝑑1
−
𝛽 > 0 and

𝑜

) 6 𝑘2 6 𝑜
1

(

𝑑

(

)

and 𝑛 > Ω

log5 𝑑

as 𝑑

(cid:16)

(cid:17)

• 𝑢0 ∈
𝔼 𝑢0(

• 𝑊

×
∈
and 𝔼 𝑊 2
𝑖𝑗

.
1
)
(

ℝ𝑛 is a random vector with independent entries such that for all 𝑖
4 6 𝑂
𝑖
)
ℝ𝑛

𝑑 is a random matrix with independent entries such that for all 𝑖
= 1, and 𝑊 and 𝑢0 are independent.

∈

• 𝑣0 ∈

ℝ𝑑 is a (non-random) unit vector with entries in

0,

{

√𝑘
1
/

.

}

±

55

. Let 𝑌 =

T
𝛽𝑢0𝑣0

𝑊, where

+

→ ∞

p

𝑛

, 𝔼 𝑢0(

𝑖

2 = 1 and
)

]

∈ [

𝑛 and 𝑗

𝑑

]

∈ [

, 𝔼 𝑊𝑖𝑗 = 0

 
Suppose that

𝛽 > 𝐶∗

𝑘
√𝑛 s

log

𝑑
𝑘2 +

log 𝑑
log 𝑛

for some large enough constant 𝐶∗. Then there exists a probabilistic algorithm that given 𝑌 as input, in time
𝑛𝑑

1
) outputs a unit vector

ℝ𝑑 such that

𝑂

(

(

)

𝑣
ˆ

∈

1
)
(
(with respect to the distribution of 𝑌 and the randomness of the algoruthm).

𝑣, 𝑣0i

− h ˆ

1

2 6 𝑜

𝑜

with probability 1

1
)
(
Remark 7.2. The algorithm also works for Ω
(
can make it arbitrarily small by increasing 𝐶∗), but

−

𝑑

)

6 𝑘2 6 𝑑

/
𝑣, 𝑣0i
h ˆ

2 in the sense that 1
2 might not tend to one in this regime.

𝑣, 𝑣0i

2 is small (we

− h ˆ

The advantage of algorithm 7.1 compared to Covariance Thresholding (and other algorithms)

is that it works for 𝛽 = 𝑜

min

𝑘
√𝑛

log 𝑑

,

𝑑
𝑛

and small 𝑛 (for example, 𝑛 = 𝑑0.99, or 𝑛 = 𝑑0.01),

(cid:18)
while Covariance Thresholding can work with 𝛽 = 𝑜

q

n

o

(cid:19)

min

𝑘
√𝑛

log 𝑑

,

𝑑
𝑛

only if 𝑛 > 𝑑1
−

𝑜

1
) (see

(

(cid:18)

n

o

q

(cid:19)

Theorem D.1). Another advantage is that other algorithms for sparse PCA use many properties of
Gaussian distribution (for example, they use 𝜒2 tail bounds), while this algorithm requires only
assumptions on ﬁrst two moments of 𝑊.

The algorithm 7.1 will use low degree polynomials to estimate the entries of 𝑣0𝑣0

T. We give a

precise description of polynomials that we use in the following subsection.

7.1 Low degree polynomials as estimators

To work with polynomials we introduce the following notation:

[

]

{

𝑑

𝑛

∈ [

] × [

𝑖, 𝑗
𝑗

, let 𝐼𝛼 :=
𝑛

𝑖
{
. For 𝑗

For a multi-index 𝛼 over
:

𝑑
𝑛
and similarly
]}
]
𝐽𝛼 :=
𝑖, 𝑗
𝛼 for some 𝑖
𝑗
, and
𝛼
}
(
(
) ∈
) ∈
𝛼𝑖𝑗
similarly let 𝐽𝛼,𝑖 :=
𝑖, 𝑗
𝑑
𝛼 𝑋
:
. Note that
𝛼
×
𝑖𝑗
(
∈ [
can be represented as a bipartite multigraph 𝐺𝛼 =
𝑛
𝐽𝛼 , 𝐸𝛼)
𝐼𝛼
every multi-index 𝛼 over
] × [
has multiplicity 𝛼𝑖𝑗. In this representation the set 𝐽𝛼,𝑖 corresponds to
𝑖, 𝑗
such that each edge
the neighborhood of the vertex 𝑖 and the set 𝐼𝛼,𝑗 corresponds to the neighborhood of 𝑗. If 𝛼 is
multilinear, 𝐺𝛼 is just a graph (i.e. multiplicity of each edge is 1).

:=
𝑑 denote 𝑋 𝛼 :=

∈ [
. For a matrix 𝑋

𝑖, 𝑗
) ∈
, let 𝐼𝛼,𝑗

𝑑
∈ [
𝑛
:
]
𝑖,𝑗

𝛼 for some 𝑗

]
∈ [
∈

:
(
𝑑
]
ℝ𝑛

) ∈
𝑑
]

(
Î

]
[
}

∈ [

∈ [

Ð

]}

)∈

{

}

{

{

(

𝑖

Now we deﬁne the graphs which represent the monomials that we will use.

ℕ and let 𝑏

Deﬁnition 7.3. Let 𝑙
We deﬁne
𝐽𝛼 =
}
there are exactly 𝑏 diﬀerent vertices 𝑖𝑠1, . . . , 𝑖𝑠𝑏

ℕ be an odd number, and ﬁx two diﬀerent 𝑗0 ∈ [
∈

𝑑
.
]
𝐼𝛼 has degree 2,
(diﬀerent from 𝑗0 and 𝑗𝑙), and for any 1 6 𝑠 6 𝑙

to be the set of bipartite graphs with 2𝑏𝑙 edges such that any 𝑖
𝑑
]
𝐼𝛼 that are adjacent to both 𝑗𝑠

for distinct 𝑗1, . . . , 𝑗𝑙

𝒢
𝑗0, 𝑗1, . . . , 𝑗𝑙

)
1, 𝑗𝑙
−

and 𝑗𝑙

𝑗0 𝑗𝑙 (

𝑏, 𝑙

∈ [

∈

∈

𝑑

{

]

1, 𝑗𝑠 (see Fig. 1).
−

1 ∈ [
−
∈

Now we are ready to deﬁne the polynomials that we will use.

Deﬁnition 7.4. Let 𝑏 be the smallest odd number that is greater than 𝐶∗∗ ·
for some
constant 𝐶∗∗ > 100, and let 𝑙 be the smallest integer such that 𝑏𝑙 > log 𝑑 and let 𝑗0 < 𝑗𝑙. We deﬁne

𝑘2 +

(cid:16)

(cid:17)

log 𝑑

log 𝑑
log 𝑛

𝑃𝑗0 𝑗𝑙 (
𝑌

)

=

1
𝜅

𝑌𝛼 ,

Õ𝛼
∈𝒢𝑗0 𝑗𝑙 (

𝑏,𝑙

)

56

𝑖11

𝑖12

𝑖13

𝑗1

𝑖21

𝑖22

𝑖23

𝑗2

𝑖31

𝑖32

𝑖33

𝑗3

𝑖41

𝑖42

𝑖43

Figure 1: A graph from

𝑗0 𝑗𝑙 (

𝒢

𝑏, 𝑙

)

for 𝑏 = 3 and 𝑙 = 4.

𝑗4

𝑗0

where

𝜅 = 𝑘

𝑘

𝑙

+
is a normalization factor so that 𝔼 𝑃𝑗0 𝑗𝑙 (
𝑌

· · · (

−

𝑛

− (

𝑏

1
)

𝑙
−
𝑏

𝑏𝑙

𝛽
𝑘

(cid:19)

(cid:19) (cid:18)

𝑘

·

𝑛
2
𝑏
· · ·
) ·
(cid:19)
(cid:18)
= 𝑣 𝑗0 𝑣 𝑗𝑙 .

)

(cid:18)

Note that under assumptions of Theorem 7.1, 𝑏 = 𝑜
log 𝑑
, so 𝑙
)
(
𝑌
The following lemma shows that the expectation of 𝑃𝑗0 𝑗𝑙 (

)

→ ∞
is indeed 𝑣0(

.

𝑖

𝑣0(

)

𝑗

.
)

Lemma 7.5.

Proof. By construction of 𝛼,

𝔼 𝑃𝑗0 𝑗𝑙 (
𝑌

)

= 𝑣0(

𝑗0)

𝑣0(

𝑗𝑙

)

,

𝔼 𝑌𝛼 =

𝑙

1
−

1
𝜅

𝛽𝑏𝑙

1
𝑘𝑏

(cid:18)

(cid:19)

𝑣0(

𝑗0)

𝑏 𝑣0(

𝑗𝑙

)

𝑏 =

𝑙

1
𝜅

𝛽
𝑘

(cid:18)

(cid:19)

𝑘

𝑣0(

𝑗0)

𝑣0(

𝑗𝑙

)

.

·

= 0, the statement is true. Assume that it is not zero. Number of nonzero terms is
(cid:3)

, so we get the desired equality.

−(

𝑛

𝑏

𝑗𝑙

𝑣0(
𝑗0)
𝑙
𝑘
+
−

If 𝑣0(
𝑘
· · ·
· · · (
(cid:0)
𝑌
Now let’s bound the variance of 𝑃𝑗0 𝑗𝑙 (

)
2
) ·

𝑙
1
)
−
𝑏

𝑛
𝑏

(cid:1)

(cid:0)

(cid:1)

.
)

Lemma 7.6. Suppose that 𝛽 = 𝐶

𝑘
√𝑛 ·

·

√𝑏 for some constant 𝐶 > 100. Then

Proof. For simplicity we will write

instead of

𝕍
𝜇

𝑌
𝑃𝑗0 𝑗𝑙 (

)

6 𝑜

(cid:18)

𝑣0(

𝑗0)

2𝑣0(

𝑗𝑙

2
)

+

1
𝑑

𝑣0(

2

𝑗0)

𝑗𝑙

𝑣0(

2
)

+

+

1
𝑑2

.

(cid:19)

(cid:1)

(cid:0)
𝑏, 𝑙
𝑗0 𝑗𝑙 (

.
)

𝒢

𝒢
= 1
𝜅2

𝑌

𝔼 𝑌𝛼𝑌𝛼′. Note that 𝔼 𝑌𝛼𝑌𝛼′ cannot be negative.

𝑗0 𝑗𝑙 (
that correspond to non-intersecting graphs have contribution at most
)

𝛼,𝛼′∈𝒢

Í

)

We need to bound 𝔼 𝑃2
The terms in 𝔼 𝑃2
2

𝑗0 𝑗𝑙 (

𝑌

. Indeed,

𝔼 𝑃𝑗0 𝑗𝑙 (
𝑌

(cid:0)

)

(cid:1)

𝑆diﬀ :=

𝔼 𝑌𝛼𝑌𝛼′ =

𝔼 𝑌 𝛼 𝔼 𝑌 𝛼′ 6

=
𝐼𝛼′
Õ𝐼𝛼
∩
∅
=
𝐽𝛼′
𝑗0,𝑗𝑙 }
∩
{
To bound the other terms will need the following lemma:

=
𝐼𝛼′
Õ𝐼𝛼
∩
∅
=
𝐽𝛼′
𝑗0,𝑗𝑙 }
∩
{

𝐽𝛼

𝐽𝛼

57

𝔼
𝜇

𝑝

𝑌
(

)

(cid:18)

2

.

(cid:19)

Lemma 7.7. Let 𝑆 be a set of pairs

𝑖, 𝑗

(

)

such that that for 𝛼, 𝛼′ ∈ 𝒢
𝑌𝛼𝑌𝛼′ =

,

𝑌2
𝑖𝑗𝑔

𝑌
(

)

where 𝑔

𝑌
(

)

is some monomial. Then

𝑆
𝑖,𝑗
Ö(
)∈

𝑌𝛼𝑌𝛼′ =

𝔼
𝜇

1

(

+

𝑜

1
))
(

𝔼
𝜇

.

𝑔

𝑌
(

)

Proof. Assume that 𝑆 =

𝑖′, 𝑗′)}

{(

(i.e.

𝑆

|

|

= 1). Since 𝛽 = 𝑂

𝑘
√𝑛

√𝑏

,

(cid:16)

(cid:17)

𝔼
𝜇

𝑌𝛼𝑌 𝛼′ = 𝔼
𝜇

𝑤𝑖′ 𝑗′ +

𝛽𝑢𝑖′𝑣 𝑗′

p

(cid:16)

2

(cid:17)

Hence for arbitrary 𝑆

𝑔

𝑌
(

)

= 𝔼
𝜇

𝑔

𝑌
(

) +

𝛽 𝔼
𝜇

𝑢2
𝑖′

𝑣2
𝑗′

𝑔

𝑌
(

)

= 𝔼
𝜇

𝑔

𝑌
(

) +

𝑂

𝑏
𝑛 !

 r

𝔼
𝜇

.

𝑔

𝑌
(

)

𝑌𝛼𝑌 𝛼′ =

1

𝔼
𝜇

𝑆

|

|

𝑂

+

 r

𝑏
𝑛 !!

𝔼
𝜇

.

𝑔

𝑌
(

)

Since

𝑆

|

|

6 𝑂

𝑏𝑙

(

)

6 𝑂

log 𝑑
(

)

Lemma 7.7 implies that

6 𝑜

(
p
𝛼 𝔼𝜇(

𝑛
𝑏 )
𝑌𝛼

Í

, we get the desired bound.

. Note that

2 6

)

𝑜

1
(

+

1
))|𝒢|
(
𝑙

𝜅 =

1
(

−

𝑜

1
))
(

(cid:18)

𝑛
𝑏

𝑘𝑙

(cid:19)

(cid:18)

𝑏𝑙

,

𝛽
𝑘

(cid:19)

since 𝑙 6 𝑜

√𝑘
(

)

and 𝑏𝑙 6 𝑂

log 𝑑

6 𝑜

√𝑛

. Similarly,

=

1
(

−

𝑜

1
)) ·
(

𝑑𝑙

1
−

|𝒢|

𝑌𝛼

𝔼
𝜇 (

2 6
)

Õ𝛼
Here we used 𝑏𝑙 > log 𝑑.

(cid:0)
1
(

+

(cid:1)
1
)) ·
(

𝑜

(cid:0)

𝑑𝑙

1
−

𝑙

(cid:1)
𝑛
𝑏

(cid:18)

(cid:19)

6

𝑜

1
(

+

𝜅2

1
))
(

1
𝑑

(cid:18)

(cid:19)  (cid:18)

𝑘2𝑏
𝛽2𝑛

𝑏

(cid:19)

(cid:18)

𝑙

𝑛
𝑏

𝑙

(cid:1)

(cid:0)

𝑑
𝑘2

(cid:19) !

(cid:3)

. Hence

6 𝜅2
𝑑10

.

To bound the other terms in 𝔼 𝑃2

𝑌

we deﬁne some notions related to the multigraphs which

)

𝑗0 𝑗𝑙 (

correspond to 𝑌 𝛼𝑌𝛼′.
We will call 𝑗

∈

∈

𝐽𝛼 circles and 𝑖

𝐼𝛼 boxes. A circle 𝑗 is called blocked if each box that is adjacent
to 𝑗 has two parrallel edges to 𝑗. Equivalently, if 𝑣 𝑗 appears in 𝑌𝛼𝑌𝛼′ only in squared parentheses,
2.
i.e. only in parantheses of the form
)
If two blocked circles are adjacent to the same box (which means that they are adjacent to
exactly 𝑏 same boxes), we call such circles consecutive. A maximal sequence of consecutive blocked
circles is called a blocked segment. The endpoints of a blocked segment are circles from this segment
that are either 𝑗0, 𝑗𝑙, or circles that share a box with a non-blocked circle.

𝛽𝑢𝑖𝑣 𝑗

𝑤𝑖𝑗

p

+

(

A block segment is called closed if the boxes that are adjacent to the endpoints of this block
are adjacent to exactly two circles. If this condition is satisﬁed only for one endpoint, we call such
segment half-open, and if it is not satisﬁed for both endpoints, we call it open.

A block segment that contains 𝑗0 is called the leftmost and the segment that contains 𝑗𝑙 is called

the rightmost. Other segments are called intermediate.

We can group the terms diﬀerent from

𝑌𝛼
(

2 in the following way: Let
)

58

 
• 𝑞1 be the number of circles in the leftmost block segment

• 𝑞2 be the number of circles in the rightmost block segment

• 𝑞3 be the number of circles in intermediate block segments

• 𝑠𝑐 be the number of closed intermediate segments

• 𝑠𝑜 be the number of open intermediate segments

• 𝑠ℎ be the number of half-open intermediate segments

• 𝑟 be the number of circles of degree 4𝑏 that are not blocked and do not share any box with

blocked circles.

• 𝑚𝑎 (for 𝑎

∈ {
circles are not blocked.

}

3, 4

) be the number of boxes that are adjacent to exactly 𝑎 circles such that these

• 𝑚 be the number of boxes of degree 4 which are adjacent to exactly two non-blocked circles

and at least one of these circles doesn’t share a box with blocked circles.

Then we group terms in a way such that these parameters 𝑞1, 𝑞2, 𝑞3, 𝑠𝑐, 𝑠𝑜, 𝑠ℎ, 𝑟, 𝑚2, 𝑚3, 𝑚4 are
equal for all terms 𝑌𝛼𝑌𝛼′ inside one group.

Let’s ﬁx the parameters (𝑞1, 𝑞2, 𝑞3, 𝑠𝑐, 𝑠𝑜, 𝑠ℎ, 𝑟, 𝑚2, 𝑚3, 𝑚4) and compute the contribution of

nonzero terms 𝔼 𝑌 𝛼𝑌𝛼′ which correspond to graph with these parameters.

We will use the following way of counting: for every 𝑌𝛼𝑌𝛼′ such that 𝛼 and 𝛼′ have common
obtained from 𝛼′ by replacing

box/circle of some type (diﬀerent from 𝑗0, 𝑗𝑙) we consider 𝛼′′ ∈ 𝒢
each box/circle of this type by another box/circle that is not in 𝛼.

𝛼, 𝛼′)

𝑛
∈ [
𝑜
1
−
(

6 𝐶′. Then 𝔼 𝑌𝛼𝑌𝛼′′ >

We start with 𝑚4 > 0. For every nonzero 𝑌𝛼𝑌𝛼′ with 𝑚4 > 0 let 𝑀4(

be the set of boxes that
that are obtained from 𝛼′ by replacing
are adjacent to 4 non-blocked circles. Consider all 𝛼′′ ∈ 𝒢
by some box that is not in 𝛼. Then 𝑌𝛼𝑌𝛼′′ has parameter 𝑚4 = 0. Recall
𝛼, 𝛼′)
each box from 𝑀4(
𝑚4 𝔼 𝑌 𝛼𝑌𝛼′.
, 𝔼 𝑢4
that there exists a constant 𝐶′ such that for all 𝑖
𝑖
]
𝑛𝑚4, while number of diﬀerent 𝑌𝛼𝑌𝛼′ such
Number of such 𝛼′′ (for ﬁxed 𝛼, 𝛼′) is is at least
1
))
(
2𝑚4.
that 𝑌𝛼𝑌𝛼′′ could be obtain from them using the procedure described above is is at most
)
Hence the contribution of terms for which 𝑚4 = 0 is larger than the contribution of terms with
𝑚4 > 0 by a factor

log4 𝑑
since 𝑛 > Ω
1
)
(
(
Similarly, for every nonzero 𝑌𝛼𝑌𝛼′ with 𝑚3 > 0 let 𝑀3(
𝛼, 𝛼′)

to 3 non-blocked circles. Consider all 𝛼′′ ∈ 𝒢
𝑀4(
𝛼′′ (for ﬁxed 𝑌 𝛼𝑌𝛼′) is is at least
𝑜
−
could be obtain from them is at most
larger than the contribution of terms with 𝑚3 > 0 by a factor

be the set of boxes that are adjacent
that are obtained from 𝛼′ by replacing each box from
𝑚3
𝔼 𝑌𝛼𝑌 𝛼′. Number of such
𝑏𝑚3, while number of diﬀerent 𝑌 𝛼𝑌𝛼′ such that 𝑌𝛼𝑌𝛼′′
(cid:17)
𝑛
1
))(
(
)
2𝑚3. Hence the contribution of terms for which 𝑚3 = 0 is
2𝑏𝑙
)
(

by some box that is not in 𝛼. Then 𝔼 𝑌𝛼𝑌 𝛼′′ >

. Note that this factor is 𝜔

𝛼, 𝛼′)

𝑛
3𝑏2𝑙2𝐶′

1
))
(

1
(

1
(

2𝑏𝑙

1
𝐶′

.
)

𝑚4

−

𝛽
𝑘

𝑜

(cid:17)

(cid:16)

(cid:16)

(

(cid:0)

(cid:1)

Note that this factor is 𝜔

1
)
(

𝑚3

>

𝛽𝑛
3𝑘𝑏3𝑙2

𝑚3

2
/

.

𝑛
10𝑏5𝑙4

(cid:16)

(cid:17)

(cid:18)

(cid:19)
log5 𝑑
since 𝑛 > Ω
(

.
)

59

For every nonzero 𝑌𝛼𝑌𝛼′ with 𝑚 > 0 and 𝑚3 = 𝑚4 = 0, let 𝑀

be the set of these 𝑚 boxes
be the set of non-blocked circles of degree 4𝑏 which do not share any box with

𝛼, 𝛼′)

(

and let 𝑅
𝛼, 𝛼′)
blocked circles.

(

𝑚

𝑚

𝑜

1
(

𝛼, 𝛼′)
(
𝛽2
𝑘2

Consider all 𝛼′′ ∈ 𝒢

that are obtained from 𝛼′ by replacing each box from 𝑀

by some
box that is not in 𝛼 and each circle from 𝑅
by some circle from the support of 𝑣0 that is not
in 𝛼′. It follows that 𝔼 𝑌𝛼𝑌𝛼′′ >
𝔼 𝑌𝛼𝑌 𝛼′. Number of such 𝑌𝛼𝑌𝛼′′ (for ﬁxed 𝑌𝛼𝑌𝛼′)
𝑛
𝑘𝑟. Number of diﬀerent 𝑌 𝛼𝑌𝛼′ such that 𝑌𝛼𝑌𝛼′′ could be obtain from them
𝑜
1
1
is at least
𝑏
−
(
))
(
2𝑚
2𝑟 𝑚𝑟
. Indeed, number of ways to choose circles which replace 𝑅
4𝑏𝑙
2𝑙
is
is at most
(cid:1)
(cid:0)
)
)
(
(
2𝑟 . Once these circles are chosen, number of ways to choose boxes which replace
2𝑙
bounded by
)
(
𝑚𝑡 for each chosen circle 𝑗𝑡
𝛼, 𝛼′)
𝑀
˜
such that
) of numbers of
possibilities to choose 2 subsets of size

𝑟 , and the product (over all 𝑡
𝑚𝑡 in a set of size 4𝑏. This product is bounded by
˜

is bounded by the number of ways to choose the numbers

𝑚𝑡 = 𝑚, which is bounded by

𝛼, 𝛼′)

𝛼, 𝛼′)

𝑟
𝑡=1 ˜

1
))
(

2𝑏
(

∈ [

𝑟
𝑚

−

𝑚

(cid:17)

(cid:16)

𝑟

]

(

)

(

(

(cid:1)

(cid:0)

Í

2𝑚
)

4𝑏
(
2

𝑚1!
( ˜
)

2
𝑚𝑟!
)

· · · ( ˜

2𝑚

6

4𝑏
(

)

2𝑚

.

𝑟
𝑚

(cid:16)

(cid:17)

Hence the contribution of terms with 𝑚 = 0 is larger than the contribution of terms with 𝑚 > 0 by
a factor

1
(

−

𝑜

1
))
(

𝑚

𝛽2𝑛
𝑘2𝑏

(cid:18)

(cid:19)

𝑘𝑟

2𝑙
(

2𝑟

−
)

2𝑏
(

2𝑟

−
)

4𝑏
(

2𝑚

−
)

(cid:16)

𝑚
𝑟

Note that this factor is 𝜔

since 𝑏 = 𝑜

1
)
(

log 𝑑
(

)

2𝑚

> 𝐶 𝑚

𝑘
5𝑏𝑙

𝑚
𝑟𝑏

(cid:18)

(cid:17)

(cid:16)
and 𝑏𝑙 6 log 𝑑 6 𝑂

(cid:17)
log 𝑘
(

(cid:19)
.
)

𝑟

2𝑚

𝑟

/

> 𝐶 𝑚

2−

𝑂

𝑏

(

)

𝑘
5𝑙𝑏

(cid:18)

𝑟

.

(cid:19)

)−

2𝑙
(

𝛼, 𝛼′)

. Let 𝑅′(

2𝑟 which is 𝜔

𝑟 > 0 and 𝑚 = 0 by a factor 𝑘𝑟

Similarly, the contribution of terms with 𝑟 = 0 is larger than the contribution of terms with
.
1
)
(

For every nonzero 𝑌𝛼𝑌𝛼′ with 𝑚4 = 𝑚3 = 𝑚 = 𝑟 = 0 and 𝑞3 > 0 let 𝑄

𝛼, 𝛼′)
be a set of blocked
be a set of boxes adjacent to circles from
be a set of non-blocked circles of degree 4𝑏 which share a box with some
be a set of
𝛼, 𝛼′)
. We
2𝑠𝑐.
+

𝛼, 𝛼′)
∈
and which are adjacent to circles from 𝑅′(
= 𝑠ℎ
= 𝑏𝑞3 +
= 𝑞3,

circles in intermediate blocked segments and 𝑀′(
𝑄
𝛼, 𝛼′)
(
𝑄 (which means that it shares exactly 𝑏 boxes with some 𝑗
𝑗
∈
boxes of degree 4 which are not from 𝑀′(
denote 𝑚′′ =
𝛼, 𝛼′)|
(
|
Consider all 𝛾′ ∈ 𝒢
𝑀′′(
𝛼, 𝛼′) ∪

. Note that
that are obtained from 𝛼′ using the folowing procedure: each box from
𝛼, 𝛼′)
𝑀′(
is replaced by some circle from the support of 𝑣0 that is not in 𝛼′. Also consider all 𝛾 that are
obtained from 𝛼 by replacing circles from 𝑄 by a circle from the support of 𝑣0 that is not in 𝛾′.

is replaced by some box that is not in 𝛼, each circle from 𝑄

𝛼, 𝛼′)
𝛼, 𝛼′)|

𝑄). Let 𝑀′′(

𝛼, 𝛼′) ∪

𝛼, 𝛼′)|

𝛼, 𝛼′)|

𝛼, 𝛼′)

𝛼, 𝛼′)

𝑀′′(

𝑀′(

𝑅′(

𝑅′(

𝑏𝑠,

𝑄

(

(

|

|

|

It follows that

𝔼 𝑌𝛾𝑌𝛾′ >

1
(

−

𝑜

1
))
(

𝛽
𝑘

2𝑏𝑞3

2𝑏𝑠𝑐

+

𝑏𝑠 ℎ +

2𝑚′′

+

𝔼 𝑌𝛼𝑌𝛼′ .

(cid:19)
(cid:18)
2𝑠𝑐 . Note that 𝛼 and 𝛼′
Number of diﬀerent 𝑌 𝛾𝑌 𝛾′ (for ﬁxed 𝑌𝛼𝑌𝛼′) is
1
−
(
might contain now circles that are not from the support of 𝑣0 (they should be in 𝑄). By similar
argument as for the case 𝑚 > 0, the number of diﬀerent 𝑌𝛼𝑌𝛼′ such that 𝑌𝛾𝑌𝛾′ could be obtain
from them is at most

𝑚′′ 𝑘2𝑞3

1
))
(

𝑠 ℎ +

𝑏𝑞3

𝑛
𝑏

𝑏𝑠

𝑜

+

+

+

(cid:0)

(cid:1)

2𝑚′′

.

10𝑙
(

4𝑠 𝑑𝑞3
)

4𝑏
(

2𝑚′′
)

𝑠ℎ

2𝑠𝑐

+
𝑚′′

(cid:18)

(cid:19)

60

Hence the contribution of terms with 𝑞3 = 0 is larger than the contribution of terms with 𝑞3 > 0 by
a factor

𝛽
𝑘

1
(

−

𝑜

1
))
(

(cid:18)
which is at least

2𝑏𝑞3

2𝑏𝑠𝑐

+

𝑏𝑠 ℎ +

2𝑚′′

+

(cid:19)

𝑏𝑞3

𝑏𝑠

+

+

𝑚′′

𝑛
𝑏

(cid:16)

(cid:17)

𝑘2𝑞3

+

2𝑠𝑐

𝑠 ℎ +

10𝑙
(

4𝑠 𝑑−
−
)

𝑞3

4𝑏
(

2𝑚′′

−
)

2𝑚′′

−

,

𝑠ℎ

2𝑠𝑐

+
𝑚′′

(cid:18)

(cid:19)

𝑞3

𝑘2
𝑑

·

(cid:19)

𝐶 𝑏

(cid:18)

𝐶 𝑏𝑠𝑐

+

𝑚′′

𝑛
10𝑙
(

4𝑏
)

(cid:18)

𝑏𝑠0

(cid:19)

(cid:18)

𝛽𝑛
𝑘𝑏

𝑏𝑠 ℎ

𝑘

(cid:19)

(cid:18)

2−
·
10𝑙
(

𝑂

(
4
)

2𝑠𝑐

𝑠 ℎ

+

𝑏

)

.

(cid:19)

Since 𝐶 𝑏 > 10𝑏 𝑑

𝑘2 , this factor is 𝜔

.
1
)
(

Hence we can conclude that if 𝑞1 = 𝑞2 = 0, then the contribution of the terms that are diﬀerent

from 𝑆diﬀ is 𝑜

𝑆diﬀ)
.

(

Now consider the case when 𝑞1 > 0 and the other parameters are zero. There are two cases, the
ﬁrst is when the leftmost blocked segment is half-open and the second when it is closed. In the
ﬁrst case the contribution of such terms is bounded by

1
(

+

𝑜

1
)) ·
(

𝑑𝑞1

1
−

𝑏𝑞1

·

𝑛
𝑏

(cid:16)

(cid:17)

𝑘2𝑙

2
2
(
−
−

𝑞1

1
)
−

𝛽𝑛
𝑘𝑏

(cid:19)

·

(cid:18)

2𝑙

𝑏

(

2𝑞1

−

1
)
+

𝑗𝑙

𝑘𝑣0(

2 6 2
)

 (cid:18)
𝑏
6 2−

(

𝑞1

1
−

𝑘2𝑏
𝛽2𝑛

𝑏

𝑑
𝑘2 !

(cid:19)

𝑏

𝜅2
𝑘

𝑘
𝛽

(cid:18)

(cid:19)

𝑗𝑙

𝑣0(

2
)

𝑞1

1
)𝑑−
−

10

𝜅2
𝑘

·

𝑗𝑙

𝑣0(

2 ,
)

where we used

𝑏 =

𝛽

𝑘

(

/

)

𝑛
𝐶𝑏

𝑏

2 > 𝑑10. In the second case, we get
/

1
(

+

𝑜

1
)) ·
(

𝑑𝑞1

1
−

(cid:0)
𝑏𝑞1

(cid:1)
𝑘2𝑙

·

𝑛
𝑏

(cid:16)

(cid:17)

2𝑞1

−

1
−

𝛽𝑛
𝑘𝑏

(cid:19)

·

(cid:18)

2𝑏

𝑞1

𝑙

(

−

)

𝑗𝑙

𝑘𝑣0(

where we used 𝐶 𝑏 > 10𝑏 𝑑
𝑘2 .

2 6 2
)

  (cid:18)
6 10−

𝑘2𝑏
𝛽2𝑛
(cid:19)
𝑏𝑞1 𝜅2
𝑑

𝑞1

1
−

𝑏

𝑑
𝑘2 !

𝑘2𝑏
𝛽2𝑛

𝑏

𝜅2
𝑘2

(cid:19)

(cid:18)

𝑗𝑙

𝑣0(

2
)

𝑗𝑙

𝑣0(

2 ,
)

Similarly, in the case when 𝑞2 > 0 and the other parameters are zero, the contribution is

bounded by 10−

𝑏𝑞2 𝜅2

𝑑 𝑣0(

2.
𝑗0)

Similar computations show that if 𝑞1 > 0 and 𝑞2 > 0 (and the other parameters are zero), then

the contribution is bounded by 𝑜

𝜅2
𝑑2

. Therefore, dividing by 𝜅2, we get

𝕍
𝜇

𝑌
𝑃𝑗0 𝑗𝑙 (

)

(cid:16)
6 𝑜

(cid:17)
𝑣0(

(cid:18)

𝑗0)

2𝑣0(

𝑗𝑙

2
)

+

1
𝑑

(cid:0)

𝑣0(

2

𝑗0)

𝑗𝑙

𝑣0(

2
)

+

+

(cid:1)

1
𝑑2

.

(cid:19)

7.2 Computation in polynomial time

Since for 𝑗0 ≠ 𝑗𝑙, 𝑃𝑗0 𝑗𝑙 (
𝑌
can use a color coding technique to (approximately) evaluate 𝑃 in time 𝑑𝑂

, simple evaluation takes time 𝑑Θ
)
1
).

has degree Θ

log 𝑑
(

)

(

(cid:3)

log 𝑑

(

). However, we

Let 𝑃

𝑗0 > 𝑗𝑙.

𝑌
(

)

be a matrix such that for all 𝑗0 < 𝑗𝑙

𝑑

𝑌
, its entries are just 𝑃𝑗0 𝑗𝑙 (

, 𝑃𝑗0 𝑗𝑙
)

]

∈ [

= 𝑃𝑗0 𝑗𝑙 for

61

Lemma 7.8. Suppose that the conditions of Theorem 7.1 are satisﬁed. There exists a probabilistic algorithm
that given 𝑌 as input, in time

𝑑 such that

ℝ𝑑

𝑛𝑑

𝑂

1

×

(

(

)

) outputs a matrix ˆ𝑃
6 𝑜

𝑃

k ˆ𝑃

−

2
F

k

∈

,

1
)
(

(with respect to the distribution of 𝑌 and the randomness of the algoruthm).

𝑜

with probability 1

1
)
(
Proof. Let’s ﬁx 𝑗, 𝑗′ ∈ [
colorings. Note that we can compute

−

𝑑

]

such that 𝑗 < 𝑗′ and let 𝑎 :

𝑑

[

] → [

𝑙

1
]

+

and 𝑐 :

𝑛

[

] → [

𝑏𝑙

]

be ﬁxed

𝑌
𝑝𝑎𝑐 𝑗 𝑗′(

)

= 1
𝜅 ·

𝑙

(

(

𝑙

1
1
+
)
!
1
(
)

𝑏𝑙
(
)
𝑏𝑙
!
)

+
𝑙
+

𝑙

Õ𝛼
∈𝒢𝑗 𝑗′(

𝑏,𝑙

)

1

𝑎

𝐽𝛼)

(

[

=

𝑙

[

1
]] ·

𝑐

1
[

𝐼𝛼)

(

+

=

𝑏𝑙

[

]] ·

𝑌𝛼

in time 𝑑𝑂
To do this, we compute a matrix whose rows and columns are indexed by

)2𝑂
1

).

𝑏𝑙

(

(

𝑗
∈ [
𝑆′ \

|

𝑑
]
𝑆
|

𝑙
, 𝑆
⊆ [
= 1, 𝑀

]
⊂

, 𝑀

𝑏𝑙
]
𝑀′ \

⊆ [
|

𝑀′,

𝑀

|

, such that an entry

𝑗, 𝑆, 𝑀
= 𝑏. If the entry is not zero, this is equal to

𝑗′, 𝑆′, 𝑀′

,

(cid:1)

(cid:0)

(cid:1)(cid:3)

is not 0 if and only if 𝑆

𝑗, 𝑆, 𝑀

(

, where
)
𝑆′,

⊂

(cid:2)(cid:0)
= 𝑀′

𝑐

1
[

𝐵

)

(

𝑌𝑖𝑗𝑌𝑖𝑗′ .

𝑀

]

\

𝐵
Ö𝑖
∈

𝑛

Õ𝐵
𝑏)
∈(
𝑚1, . . . , 𝑚𝑏

Now, denote 𝑀′ \
compute for all 1 6 𝑟 6 𝑏

𝑀 =

{

, where 𝑚1 <

}

· · ·

< 𝑚𝑏. To compute the entry we can

=

𝑇

𝑟

[

]

𝑛

Õ𝐵𝑟
𝑟)
∈(

𝐵𝑟

1

𝑐

[

(

)

=

{

𝑚1 . . . , 𝑚𝑟

𝑌𝑖𝑗𝑌𝑖𝑗′ .

}]

𝐵𝑟
Ö𝑖
∈

Note that

𝑇

𝑟

[

1
]

+

=

𝑇

𝑟

𝑌𝑖𝑗𝑌𝑖𝑗′ ,
]

[

𝑖
: 𝑐
𝑛
Õ𝑖
)
(
]
∈[
+
so we can compute the entry of the matrix 𝑇
in time 𝑂
𝑏
time 𝑛𝑏𝑑𝑂

) = 𝑑𝑂

)2𝑂
1

1
).

𝑏𝑙

]

[

(

(

(

1

=𝑚𝑟

𝑛𝑏

, and all entries can be computed in
)

(

If we then compute the 𝑙-th power of this matrix, which takes time 𝑑𝑂

1
), the entry

(

𝑗0,

,

∅

∅

,

,

𝑗𝑙 ,

𝑏𝑙

𝑙

,

[

]

[

]

𝑏𝑙
!
!
1
of the resulting matrix contains 𝜅 (
)
(
)
+
𝑙
1
𝑏𝑙
𝑙
1
+
)
+
(

(

𝑙

)

𝑌
𝑙 𝑝𝑎𝑐 𝑗 𝑗′(

.
)

(cid:1)

(cid:0)

(cid:1)(cid:3)

Denote by 𝜇 the distribution of 𝑌. Let 𝑎, 𝑐 be independent random colorings (𝑎 is sampled from
colors and 𝑐 from uniform distributions over
1
+
]
= 𝑃𝑗 𝑗′(
. Note that since for any 𝛼, 𝛼′ ∈ 𝒢
𝑌
,
)
)

(cid:2)(cid:0)
uniform distributions over colorings of
colorings of
𝔼𝜇 𝑌𝛼𝑌𝛼′ > 0, and

in
]
[
in 𝑏𝑙 colors). Thus 𝔼𝑎𝑐 𝑝𝑎𝑐 𝑗 𝑗′(
𝑌
)

𝑗 𝑗′(

𝑏, 𝑙

𝑛

𝑑

[

]

[

𝑙

𝔼
𝜇

𝕍
𝑎𝑐

𝑝𝑎𝑐 𝑗 𝑗′

6 𝔼
𝜇

𝔼
𝑎𝑐

𝑝2
𝑎𝑐 𝑗 𝑗′(

𝑌

)

6 1

𝜅2  

(
𝑙

(

𝑙

!
1
(
)
𝑙
1
+

𝑏𝑙

!
)
𝑏𝑙

(

)

+
1
)

+

2

𝑙 !

Õ𝛼,𝛼′∈𝒢

𝔼
𝜇

𝑌 𝛼𝑌𝛼′ 6 𝑑5 𝔼
𝜇

𝑃2

𝑌

,

)

𝑗 𝑗′(

since

𝑙

𝑏𝑙
1
!
!
(
(
)
)
+
𝑙
1
𝑏𝑙
𝑙
1
+
)
+

(

)

(

2

𝑙

6 𝑒4𝑏𝑙 6 𝑑5.

Hence with probability at least 1

(cid:17)

(cid:16)

𝑑−

5 (with respect to 𝜇), 𝕍𝑎𝑐 𝑝𝑎𝑐 6 𝑑10 𝔼𝜇 𝑃2

𝑌

.
)

𝑗 𝑗′(

−

62

Let ˆ𝑃𝑗 𝑗′(
𝑌
)
Thus 𝔼𝑆 ˆ𝑃𝑗 𝑗′(
𝑌

= 1
𝑎,𝑐
𝑆
(
|
= 𝑃𝑗 𝑗′(
𝑌
Í

)

|

)

𝑌
𝑆 𝑝𝑎𝑐 𝑗 𝑗′(
)∈
and with probability at least 1

, where 𝑆 is a set of 𝑑20 independent random colorings
)

5,

𝑑−

−

𝑎, 𝑐

.
)

(

𝕍
𝑌
𝑆 ˆ𝑃𝑗 𝑗′(

)

6 1
𝑑10

𝔼
𝜇

𝑃2

𝑗 𝑗′(

𝑌

6 1
𝑑10

.

)

hence with probability at least 1

𝑑−

3 (with respect to 𝑆)

−

k ˆ𝑃

−

𝑃

2
F

k

6 1
𝑑3

,

where ˆ𝑃 is a symmetric matrix that is zero on the diagonal and whose entries for all 𝑗 < 𝑗′ are ˆ𝑃𝑗 𝑗′.
Therefore, with probability 1

𝑜

1
)
(

−

k ˆ𝑃

−

𝑃

2
F

k

6 𝑜

.

1
)
(

Proof of Theorem 7.1. By Lemma 7.5 and Lemma 7.6,

𝔼

𝑃

k

−

T
𝑣0𝑣0

=

2
F

k

𝕍 𝑃𝑗 𝑗′ +

Õ𝑗≠𝑗′

𝑑
Õ𝑗
]
∈[

𝑗

𝑣0(

4 6 𝑜
)

1
) +
(

1
𝑘

6 𝑜

.

1
)
(

Hence by Markov’s inequality

𝑃

k

−

T
𝑣0𝑣0

2
F

k

6 𝑜

1
)
(

with probability 1

𝑜

. By Lemma 7.8 we can compute in time
1
)
(

(

𝑛𝑑

)

−

𝑂

(

1

) a matrix ˆ𝑃 such that

k ˆ𝑃

−

𝑃

2
F

k

6 𝑜

.

1
)
(

Hence

k ˆ𝑃

−

T
𝑣0𝑣0

k

2 6

Therefore, by Lemma H.3, the top eigenvector

with probability 1

𝑜

.
1
)
(

−

1

− h ˆ

6 𝑜

.

1
)
(

k

2
F

T
𝑣0𝑣0

−

k ˆ𝑃
𝑣 of ˆ𝑃 satisﬁes
ˆ
2 6 𝑜
𝑣, 𝑣0i

1
)
(

(cid:3)

(cid:3)

Remark 7.9. Note that the proof of Theorem 7.1 also shows that in the case Ω
(
2 6 2−
1
choost 𝐶∗∗ > 1000 and get 1
zero as 𝑑
.

2,
/
𝑏. Hence, if the constant 𝐶∗ from the theorem statement is at least 105, we can
1000, but as long as 𝑛 > 𝑑Ω
1
), the error doesn’t tend to

𝑣, 𝑣0i

𝑣, 𝑣0i

2 6 2−

6 𝑘2 6 𝑑

− h ˆ

− h ˆ

𝑑

)

(

→ ∞

8 Fast Spectral Algorithms for Recovery

One key limitation of Algorithm 5.12 is the reliance on solving large semideﬁnite programs, some-
thing that is often computationally too expensive to do in practice for the large-scale problems that
arise in machine learning. So, inspired by the SoS program used in 5.12, in this section we present
a fast spectral algorithm which recovers the sparse vector 𝑣0 in time 𝑂
. Our algorithm

𝑛𝑑 log 𝑛

63

(cid:0)

(cid:1)

}

∈ {

2, 4, 6

which we call SVD-𝑡, for 𝑡
, is a slight modiﬁcation of a fast spectral algorithm presented
in [HSSS16]. Such algorithm recovers a sparse vector planted in a random subspace. The algorithm
was also based on the analysis of a degree-4 Sum-of-Squares algorithm introduced in [BKS14]. We
remark that for 𝑡 = 2 Algorithm 8.1 corresponds to the SVD with thresholding algorithm outlined
in Section 1. In non-robust settings, as well as in the adversarial model 6.6, the algorithm achieves
high correlation under conditions similar (up to logarithmic terms) to those of the Sum-of-Squares
algorithm 5.12 (of degree 2, 4 and 6).

Algorithm 8.1 (SVD-𝑡: Sparse Vector Recovery).

Given: Sample matrix 𝑌

ℝ𝑛

𝑑, let 𝑦1, . . . , 𝑦𝑑

×

∈

∈

ℝ𝑛 be its columns. Degree 𝑗

2, 4, 6

.

}

∈ {

Estimate: The sparse vector 𝑣0.

Operation:

1. Compute the top eigenvector

𝑢 of the matrix
ˆ
𝐴 :=

𝑐 𝑗

𝑦𝑖 , 𝑛

(

Õ𝑖
𝑑
]
∈[
:= 1, 𝑐4(

)

ℝ, 𝑐2(

𝑥, 𝑡

∈

T

𝑦𝑖 𝑦𝑖

) ·

:=

𝑥, 𝑡

)

𝑥

k

(cid:16)

2

k

− (

𝑡

1
)

−

(cid:17)

𝑥, 𝑡

, 𝑐6(

)

:=

ℝ𝑛 , 𝑡

where for 𝑥
2
𝑐4(
−
)
2. Compute

𝑥

∈
𝑡
2
.
1
−
(
)
𝑢T𝑌.
𝑣 =
(cid:1)
ˆ
ˆ

(cid:0)

3. Threshold the vector

𝑣 in the following way (for some ﬁxed 𝜏 > 0):
ˆ

𝑖

∀

𝑑

]

, 𝜂

𝑣
( ˆ

)

∈ [

𝑖 =

𝑣𝑖 ,
ˆ
0,

(

4. Output the thresholded vector 𝜂

𝑣
( ˆ

.
)

if

𝑣𝑖
| ˆ

> 𝜏
√𝑘
|
otherwise

(

}

𝑛𝑑

∈ {

𝑦1, 𝑛

2, 4, 6

, the terms 𝑚𝑗

log 𝑛
(
𝑦𝑖, 𝑧

𝑦𝑑, 𝑛
Remark 8.2 (Running Time of the Algorithm). For 𝑗
)
. Correctness of the algorithm will be proved showing that 𝐴 has
are computable in time 𝑂
)
at least constant spectral gap. This means that we can compute the top eigenvalue with power
iteration using 𝑂
matrix-vector multiplications . A matrix multiplication requires computing
𝑚𝑖 = 𝑐6(
𝑛𝑑
.
)
𝑣 can be computed in time 𝑂
Then,
. In conclusion the algorithm runs in
ˆ
)
time 𝑂
(

𝑚𝑖 𝑎𝑖. Both operations take time 𝑂

for each 𝑖 and then taking the sum

To get an intuition on the algorithm, consider SVD-6 and the simpler adversarial model 𝑌 =
𝛽𝑢𝑣T.That is, the Single Spike Model 6.3 with the noise projected into the

𝑑
∈[
in time 𝑂
Í

, . . . , 𝑚𝑗

𝑛𝑑 log 𝑛

and 𝜂

𝑦𝑖 , 𝑛

𝑣
( ˆ

𝑛𝑑

.
)

)h

]
(

𝑑

i

)

(

(

)

)

)

(

(

𝑖

2 𝑢𝑢T

1
𝑢

𝑊
Id
+
space orthogonal to 𝑢.24 Now for 𝑖
(cid:16)

−

(cid:17)

k

k

p

24Note that the estimate

𝑢 obtained by SVD-2 is the same returned by the standard SVD.
ˆ

supp

∈

𝑣

{

,

}
2

2

𝑦𝑖

𝑛

−

(cid:17)

(cid:20) (cid:16)(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

T

2𝑛

𝑦𝑖 𝑦𝑖

(cid:21)

−

𝛽3𝑛3
𝑘3

,

≈

(cid:13)
(cid:13)
(cid:13)
(cid:13)

64

while for 𝑖

𝑑

] \

∈ [

supp

𝑣

,

}

{

𝔼

2

𝑦𝑖

Indeed, the coeﬃcient 𝑐6(
𝑛3, the sum
for 𝑑

𝑦𝑖 , 𝑛

)

≫

on the other hand

𝑑

𝑖

∈[

]\

supp
Í

{

𝑣

}

2

𝑛

−

−

T

2𝑛

𝑦𝑖 𝑦𝑖

= 𝑂

.

1
)
(

(cid:21)

(cid:17)

(cid:13)
(cid:13)

(cid:20)(cid:16)(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
has the eﬀect of "killing" the expectation for Gaussian vectors. Then
(cid:13)
T will be concentrated around its expectation 𝑑, while
𝑐6(

𝑦𝑖 , 𝑛

𝑦𝑖 𝑦𝑖

(cid:13)
(cid:13)
(cid:13)
(cid:13)

)

2

(cid:17)

−

T

2𝑛

𝑦𝑖 𝑦𝑖

(cid:21)

2

𝑦𝑖

𝑛

−

(cid:13)
(cid:13)

Õ𝑖
supp
{
∈

𝑣

} (cid:20)(cid:16)(cid:13)
(cid:13)

𝛽3𝑛3
𝑘2

.

≈

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

the leading eigenvector of 𝐴 will be highly correlated with 𝑢.

Hence, for 𝛽 & 𝑘
𝑛

𝑑
𝑘

3
1
/

(cid:0)

(cid:1)

algorithm that works for 𝛽 & 𝑘
𝑛

We remark that it is an open question how these ideas could be generalize to construct an
𝑛𝑡. Here, as a proof of concept, we show that SVD-6
3
1
/

succeed under the planted model in 6.6 when 𝑑
. In order to deﬁne the
adversarial perturbations, we will use the notation introduced for Problem 6.6, we recall that with

𝑛3 and 𝛽

and 𝑑

≫

≫

≫

1
/

𝑘
𝑛

𝑑
𝑘

𝑑
𝑘

(cid:1)

(cid:0)

𝑡

(cid:0)

(cid:1)

high probability 𝜆 =

1
(
Theorem 8.3. Consider a matrix of the form,

1
))
(

q

±

𝑜

𝛽𝑛
𝑘 .

𝑌 = 𝑊

T
𝜆𝑢𝑣

+

𝑢

𝑣′

+

−

T

𝑢

𝑊

T

(cid:1)
𝑑, a random unit vector 𝑢, a 𝑘-sparse vector 𝑣 with entries in
0, 1
for a Gaussian matrix 𝑊
)
(
and a vector 𝑣′ as deﬁned in 6.6. For 𝑑 & 𝑛3 log 𝑑 log 𝑛, 𝜆 & √log 𝑑
such that
degree 6 returns a vector 𝜂

1
}
and 𝑘 > 𝑛 log 𝑛, Algorithm 8.1 with

0,

𝑁

±

∼

{

×

(cid:0)

𝑛

𝜏

𝑣
( ˆ

)

(cid:13)
(cid:13)
with probability at least 0.99. Furthermore, for 𝑑
𝑘𝜆6 +

(cid:13)
(cid:13)

𝜂

𝑣
( ˆ

) −

𝑣

6 𝑂

𝑑
𝑘𝜆6 +

𝜏

√𝑘

·

(cid:18)
(cid:19)
𝜏 6 1 and 𝛽 = 𝜆2 𝑘
𝑛 ,

1

−

𝜂
h
𝜂

𝑣
( ˆ
𝑣
( ˆ

, 𝑣

)
2

· k

2

i
𝑣

3
1
/

𝑘
𝑛𝛽

𝑑
𝑘

(cid:19)

(cid:18)

𝜏2

.

!

+

6

2

k

)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

We remark that the second inequality of the theorem follows from the ﬁrst by direct substitution
is close to a unit vector. Comparing this result with Theorem 5.10 we
3
1
/

and using the fact that 𝜂
see that both SVD-6 8.1 and degree-6 SoS 5.12 need 𝛽 & 𝑘
𝑛
with the sparse vector.

in order to achieve correlation 0.9

𝑣
( ˆ

𝑑
𝑘

)

To prove Theorem 8.3, we will ﬁrst show that the vector

the true vector 𝑢. Then, thresholding the vector
we will prove two results. First,

𝑢 computed by the algorithm is close to
ˆ
𝑢T𝑌 we will obtain a vector close to 𝑣. Concretely,
ˆ

65

 
Lemma 8.4. Consider a matrix of the form,

𝑌 = 𝑊

T
𝜆𝑢𝑣

+

𝑢

𝑣′

+

−

T

𝑢

𝑊

T

0, 1
for a Gaussian matrix 𝑊
)
(
and a vector 𝑣′ as deﬁned in 6.6. Let

𝑁

∼

𝑛

(cid:1)
𝑑, a random unit vector 𝑢, a 𝑘-sparse vector 𝑣 with entries in
×
𝑢
ˆ

ℝ𝑛 be the top eigenvector of the matrix

(cid:0)

∈

0,

1

}

±

{

𝑑
Õ𝑖
∈[
]
Then for 𝑑 > 𝐶∗𝑛3 log 𝑑 log 𝑛, 𝑛 > 10 log 𝑑

𝑦𝑖 , 𝑛

𝑐6(

) ·

𝑦𝑖 𝑦𝑖

T

.

𝑢

k

𝑢
− ˆ

k

6 𝑂

𝑑
𝑘𝜆6 +

1
𝜆 + p

𝑛 log 𝑛
𝜆√𝑘

!

with probability at least 0.999, where 𝐶∗ is a universal constants.

Second,

Lemma 8.5. Let
𝜆 & √log 𝑑

𝜏

, then with probability at least 1

−
exp

k
𝑛

(−

)

−

𝑢 be a vector such that
ˆ

𝑢
k ˆ

𝑢

6 𝜀 for some 0 6 𝜀 6 1

10 and let

−
ℝ𝑑 is the vector with coordinates

𝑣
k ˆ

.

𝑣

k

𝜀

(

+

𝜏

)

√𝑘 ,

where 𝜂

𝑣
( ˆ

) ∈

𝑖 =

𝜂

𝑣
( ˆ

)

𝑣𝑖 ,
ˆ
0,

(

> 𝜏

if

𝑣𝑖
| ˆ

|
otherwise.

It is easy to see how the two results immediately imply Theorem 8.3.
Lemma 8.4 is proved in Section 8.1, in Section 8.2 we prove Lemma 8.5.

8.1 Algorithm recovers u with high probability

The goal of this Section is to prove Lemma 8.4.

𝑣 = 1
ˆ

𝜆√𝑘 ˆ

𝑢T𝑌. If

(8.1)

By rotational symmetry of the Gaussian distribution, we may assume without loss of generality
𝑣𝑣T.

that 𝑢 = 𝑒1. Now, for vectors 𝑣, 𝑧

ℝ𝑛, deﬁne 𝑀

𝑣, 𝑧

:=

𝑛

𝑛

𝑣

𝑧

2

2

∈

(

)

k

+

k

− (

−

𝑖

)

(

(cid:20)(cid:16)

𝑢, 𝑤

𝑣′(

(cid:21)
Recall that the adversarial vector 𝑣′ is, by construction, orthogonal to the sparse vector 𝑣. Hence
our strategy will be the following, ﬁrst we bound the contribution of terms of the form 𝑀
𝑤, 𝛾𝑒1)
and 𝑀
. Note that the ﬁrst type of terms arise due to the noise, the second ones due
)
to the adversarial distribution. Then, lower bounding 𝑀
, we will be able to show that
)
𝑢, 𝑤𝑖

𝑤𝑖, 𝛾𝑖 𝑒1)(cid:13)
(cid:13)
𝑖
(cid:13)
(cid:13)
]
terms will play a minor role.
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
results act as building blocks for the bound, which is then shown in Lemma 8.10.

supp
Í
First we bound the contribution of the Gaussian part. We will use Bernstein Inequality, the next

𝜆𝑢, 𝑤
(
𝑣′(

with high probability. Cross-

≫ (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

𝜆𝑢, 𝑤
(

supp
Í

)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

𝑑
∈[
Í

𝑀

𝑀

𝑀

∈[

]\

(cid:17)

(

(

(

)

∈

𝑖

𝑣

𝑑

𝑣

}

{

}

{

𝑖

𝑖

1
)

2
(

−

1
)

−

66

 
Fact 8.6. Let 𝑥

𝑁

0, Id𝑛
(

,
)

∼

Proof.

𝔼

𝑥

k

k

2𝑥2
𝑖

=

𝔼

𝑥

k

k

4𝑥2
𝑖

=

𝔼

𝔼

𝑥

𝑥

k

k

k

k

2𝑥2
𝑖
4𝑥2
𝑖

2

= 𝑛
+
= 𝑛2

6𝑛

8.

+

+

𝔼 𝑥2

𝑖 𝑥2

𝔼 𝑥4
𝑖

= 𝑛

2

+

𝑗 +

,𝑗≠𝑖

𝑛
Õ𝑗
]
∈[

𝑛
Õ𝑗,𝑘
∈[
𝑗≠𝑖,𝑘≠𝑖,𝑗≠𝑘

]

𝔼 𝑥2

𝑖 𝑥2

𝑗 𝑥2

𝑘 +

𝑛
Õ𝑗,𝑘
∈[
]
𝑗=𝑘≠𝑖

𝔼 𝑥4

𝑗 𝑥2

𝑖 +

𝔼 𝑥4

𝑖 𝑥2

𝑗 +

𝔼 𝑥6
𝑖

2

𝑛
Õ𝑗,𝑘
∈[
]
𝑗≠𝑘=𝑖

𝑛

=
𝑛
(
= 𝑛2

1
)(
6𝑛

−

+

+

−
8.

2
) +

𝑛

3
(

1
) +

6
(

𝑛

1
) +

−

−

15

We bound the spectral norm of the expectation of the terms 𝑀

𝑤, 𝛾𝑒1)
.

(

Lemma 8.7. Let 𝑤

𝑁

0, Id𝑛
(

−

∼

T
𝑒1𝑒1

, 𝛾
)
𝔼 𝑀

k

ℝ. Then

∈
𝑤, 𝛾𝑒1)k

(

= 𝛾4

8𝛾2

8.

+

+

Proof. We need only to look into diagonal entries. By construction, 𝔼

2 = 𝑛

𝑤

k

k

−

1 =: 𝑚,

𝑤

k

𝛾𝑒1k

+

2

𝑚

−

𝔼

(cid:20)(cid:16)

2

(cid:17)

−

2𝑚

𝑤2
𝑖

= 𝔼

(cid:21)

= 𝔼

Applying Fact 8.6,

(cid:20)(cid:16)
k

(cid:16)

2

k

4

𝑤

k

𝑤

k

+

𝛾2

𝑚

−

+

𝛾4

+

𝑚2

2

(cid:17)

+

𝑤2
𝑖

(cid:21)
2𝛾2

2𝑚

−

𝑤

2

k

k

2𝑚

𝑤

k

−

2

k

−

2𝛾2𝑚

−

2𝑚

𝑤2
𝑖

(cid:17)

2

𝔼

𝑤

k
(cid:20)(cid:16)
= 𝑚2
= 𝛾4

+

+

+

𝛾𝑒1k
6𝑚
8𝛾2

+

+

2

𝑚

(cid:17)
𝛾4

−

8

8.

+

2𝑚

𝑤2
𝑖

𝑚2

(cid:21)

+

2𝛾2𝑚

4𝛾2

+

−

2𝑚2

4𝑚

−

−

2𝛾2𝑚

2𝑚

−

−

+

(cid:3)

(cid:3)

𝑀

The second property we need is a high probability bound on the maximum value of
𝑤, 𝛾𝑒1)k
k
Lemma 8.8. Let 𝑤

ℝ. Then for any 𝑞 > 1, with probability at least 1

𝑞,

𝑁

(

.

T
𝑒1𝑒1

2𝑒−

−

0, Id𝑛
(

−

∼

, 𝛾
∈
)
6 𝐶
𝑤, 𝛾𝑒1)k

where 𝐶 is a universal constant.

𝑀

(

k

𝛾4𝑛

+

(cid:0)

𝑛 max

𝑛 𝑞, 𝑞2

,

(cid:8)

(cid:9)(cid:1)

67

Proof. For simplicity of the notation let 𝑚 = 𝑛

1, and let 𝑝 = max

𝑞, √𝑚 𝑞

. By Fact G.4,

−

ℙ

2 ∉

𝑚

𝑤

k

−

10𝑝, 𝑚

10𝑝

+

k

(cid:8)
𝑞.
6 2𝑒−

(cid:9)

(cid:16)
Hence with probability at least 1

(cid:2)
𝑞,

2𝑒−

−

2

2

𝑚

(cid:17)

−

−

𝑤

k

𝛾𝑒1k

+

(cid:20) (cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:3) (cid:17)

2

𝛾𝑒1k

−

𝑚

2𝑚

=

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:20)(cid:16)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
h(cid:0)
6𝐶

6

k

(cid:20)(cid:16)
𝛾2

𝑤

k

𝑤

+

2

k

+

10𝑝

+

𝛾4

+

𝑝2

(cid:1)

2𝑚

−

2

(cid:17)
2

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

𝛾2

2

−

−

2𝑚

−

𝑚

(cid:17)
2𝑚

i

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:0)
for some universal constant 𝐶 > 0. The result follows.

(cid:1)

(cid:3)

And ﬁnally, the last ingredient we need for our Bernstein inequality is a bound on the variance.

Lemma 8.9. Let 𝑤

𝑁

∼

0, Id𝑛
(

T
𝑒1𝑒1

−
𝑤, 𝛾𝑒1)

2

, 𝛾
)
∈
6 𝐶

𝔼 𝑀

(

ℝ. Then

𝛾8𝑛

+

𝑛 max

log4 𝑛𝛾, 𝑛2 log 𝑛𝛾

,

(cid:8)

(cid:9)(cid:17)

for a universal constant 𝐶 > 0.

(cid:13)
(cid:13)

(cid:16)

(cid:13)
(cid:13)

Proof. For simplicity of the notation let 𝑚 = 𝑛
2 ∉
the event
Then,

10𝑝, 𝑚

10𝑝

𝑚

𝑤

+

−

=

ℰ

k

k

(cid:8)

(cid:2)

(cid:3) (cid:9)

1. Fix 𝑞 = 50 log 𝑚𝛾 and 𝑝 = max

−
, which happens with probability at least 1
(cid:9)

(cid:8)

𝑞, √𝑚 𝑞

. Deﬁne
𝑞.
2𝑒−

−

𝑤

k

𝛾𝑒1k

+

2

𝑚

−

(cid:20)(cid:16)

2

2

(cid:17)

2𝑚

−

(cid:21)

6 𝐶

𝛾8

𝑝4

.

+

(cid:0)

(cid:1)

By triangle inequality,

𝔼 𝑀

2

𝑤, 𝛾𝑒1)

(

(cid:13)
(cid:13)
We bound the ﬁrst term,

=

6

(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

𝔼

𝔼

ℙ

ℙ

(ℰ)

(ℰ)

𝑀

𝑀

(

(

(cid:2)

(cid:2)

2

2

𝑤, 𝛾𝑒1)
𝑤, 𝛾𝑒1)

ℰ

ℰ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ℙ

+

+

(cid:3)
(cid:3)(cid:13)
(cid:13)

¯ℰ
ℙ
(cid:0)
(cid:13)
(cid:13)

(cid:0)

𝔼

(cid:1)
¯ℰ

2

𝑀
𝔼
(cid:2)

𝑤, 𝛾𝑒1)
(
𝑀
(

¯ℰ
2
𝑤, 𝛾𝑒1)
(cid:12)
(cid:12)

(cid:1)

(cid:2)

(cid:3)(cid:13)
¯ℰ
(cid:13)

.

(cid:3)(cid:13)
(cid:13)

(cid:12)
(cid:12)

ℙ

𝔼

𝑀

2

𝑤, 𝛾𝑒1)

(

(ℰ)

(cid:13)
(cid:13)

(cid:2)

ℰ

(cid:12)
(cid:12)

(cid:3)(cid:13)
(cid:13)

6
𝑂
6𝑂
(cid:13)
(cid:13)
6𝑂
6𝑂

(cid:0)

(cid:0)

𝛾8
𝛾8𝑚
(cid:0)(cid:0)
𝛾8𝑚
𝛾8𝑚

+

+

+

+

𝑝4
𝑚
𝑚𝑝4
(cid:1)
𝑚𝑝4
𝑚𝑝4

𝔼

𝔼

(cid:1)
𝔼
(cid:1)(cid:13)
(cid:13)
.
(cid:1)(cid:13)
(cid:13)

(cid:2)

(cid:2)

T
𝑤𝑤
T
𝑤𝑤
(cid:2)
T
𝑤𝑤

ℰ

ℰ

(cid:3)(cid:13)
(cid:13)
(cid:3)(cid:13)
(cid:13)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:3)(cid:13)
(cid:13)

To bound the second term, observe that
For 𝑖

ℕ, deﬁne the event

∈

𝑀

(cid:0)
2
𝑤, 𝛾𝑒1)

(

6 𝑂

(cid:1)
𝑚12

(cid:16)

(cid:13)
(cid:13)

𝛾12

12

𝑤

k

+ k

+

(cid:17)

(cid:13)
(cid:13)

for any 𝛾, 𝑤, 𝑚 > 1.

𝑞𝑖 :=

ℰ

𝑤

2

k

∈

k

(cid:26)

𝑚

−

(cid:20)

2

𝑚 𝑞

q

𝑖

· (

1
) −

2𝑞

𝑖

(

1
)

+

+

, 𝑚

+

2

𝑚 𝑞

q

𝑖

· (

1
) +

2𝑞

𝑖

(

1
)

+

+

(cid:21) (cid:27)

68

By construction ℙ

𝑞𝑖)

(ℰ

6 2 max

𝑒−

ℙ

¯ℰ

𝔼

𝑀

(cid:0)

(cid:1)

(cid:2)

(cid:13)
(cid:13)

concluding the proof.

𝑤

2 ∉

𝑚

∩

k

k

n
𝑞𝑖/
4

𝑞2
𝑖
4𝑚 , 𝑒−

h
and ¯ℰ ⊆

(cid:27)

¯ℰ

(cid:12)
(cid:12)

(cid:3)(cid:13)
(cid:13)

6

ℙ

ℕ
Õ𝑖
(cid:13)
∈
6𝑂
(cid:13)
1
)
(

,

(cid:0)

(cid:26)
𝑤, 𝛾𝑒1)

2

(

2

𝑚 𝑞𝑖

2𝑞𝑖, 𝑚

−

+

2

𝑚 𝑞𝑖

+

−

2𝑞𝑖

.

p

p
𝑞𝑖. By choice of 𝑞, it follows that

io

ℕℰ
𝑖
∈
Ð
𝑞𝑖

𝔼

ℰ

𝑀

(

2

𝑤, 𝛾𝑒1)

𝑞𝑖

ℰ

(cid:1)

(cid:2)

(cid:12)
(cid:12)

(cid:3)(cid:13)
(cid:13)

(cid:3)

We can now apply Bernstein Inequality G.7:

𝑁

0, Id𝑛
(
,

−

T
𝑒1𝑒1

, let
)

𝛾1|

|

, . . . ,

𝛾𝑙

|

|

6 𝛾

ℝ. Then for 𝑙 > 𝐶∗ ·

∈

∼
𝛾𝑛

Lemma 8.10. Let 𝑤1, . . . , 𝑤𝑙
𝑙
max

, 𝑛 log3

𝑛3 log

𝛾𝑛

𝑙

(

+

)

(

+

(cid:8)

with probability at least 1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2𝑙−
(cid:13)

𝑙
Õ𝑖
]
∈[
10

𝑀

−

−

Proof. By triangle inequality,

)

(

(cid:9)
𝑤𝑖, 𝛾𝑖 𝑒1)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

6 𝑙

𝛾4

(

8
) +

+

𝐶∗𝛾4

𝑙𝑛 log 𝑛

p

𝑛−

10, where 𝐶∗ is a universal constant.

𝑙
Õ𝑖
]
∈[
By Lemma 8.7,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

𝑀

𝔼 𝑀

6

(

𝑤𝑖, 𝛾𝑖 𝑒1)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

𝑙
Õ𝑖
]
∈[

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(

𝑤𝑖 , 𝛾𝑖 𝑒1)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

𝑙
Õ𝑖
]
∈[

𝑀

𝑤𝑖 , 𝛾𝑖 𝑒1) −

(

𝔼 𝑀

𝑙
Õ𝑖
]
∈[

(

.

𝑤𝑖 , 𝛾𝑖 𝑒1)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

𝔼 𝑀

(

𝑙
Õ𝑖
]
∈[

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
and 𝑝 := max
(cid:13)

𝑙𝛾4.

6 8𝑙

8𝑙𝛾2

+

+

𝑤𝑖 , 𝛾𝑖 𝑒1)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
10𝑝, 𝑚
. Deﬁne the event
(cid:13)
+
k
𝑞. By Lemma 8.8, with probability at least 1
(cid:8)
−

2𝑒−
(cid:9)

2 ∉

𝑚

𝑤

−

−

=

ℰ

k

(cid:2)

10𝑝
2𝑙−

,
10,
(cid:3) (cid:9)

𝑙

Let 𝑞 = 100 log
𝑚𝛾
which happens with probability at least 1
for each 𝑖

+

(cid:8)

(

)

𝑙

,

𝑞, √𝑚 𝑞

∈ [

𝑀

(

k

]
𝑤𝑖 , 𝛾𝑖 𝑒1) −

𝔼 𝑀

𝑤𝑖, 𝛾𝑖 𝑒1)k

(

6 𝐶

8

8𝛾2

𝛾4

+

+

+

𝛾4𝑛

+

𝑛 max

𝑛 log 𝑙, log2 𝑙

,

for a constant 𝐶 > 0. Hence, by Lemma 8.9, applying Bernstein Inequality G.7

(cid:8)

(cid:16)

𝑀

𝑤𝑖, 𝛾𝑖 𝑒1) −

(

(

𝔼 𝑀

(

𝑙
Õ𝑖
]
∈[

6 𝐶′

𝑡

·

𝑙𝑛 log 𝑛

𝛾4

·

p

𝑤𝑖 , 𝛾𝑖 𝑒1))(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

with probability at least 1

10

2𝑙−

−

−

𝑒−(

𝑡

1
)
−

log 𝑛, where 𝐶′ is a universal constant.

The next lemma will be used to bound the contribution of the adversarial vector 𝑣′.

69

(cid:9)(cid:17)

(cid:3)

Lemma 8.11. Let
1
1
)
−

𝑒−(

log 𝑛

|
2𝑙−

𝑡

𝑎1|

10

−

−

, . . . ,

𝑎𝑙

|

|

6 𝑎

∈

ℝ. Let 𝑤

𝑁

0, Id𝑛
(

−

∼

T
𝑒1𝑒1

. Then, with probability at least
)

𝑀

𝑎𝑖 𝑒1, 𝑤

(

𝑙
Õ𝑖
]
∈[
where 𝑡 > 1 and 𝐶 > 0 is a universal constant.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

6 𝐶

𝑡

𝑙 log 𝑛𝑎2

𝑎4

max

·

·

+

p

)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1 and 𝑞 = 10 log 𝑙 and 𝑝 := max

(cid:16)

n

log 𝑙,

𝑛 log 𝑙

p

o(cid:17)

𝑞, √𝑚 𝑞

. By Fact G.4,

Proof. For simplicity let 𝑚 = 𝑛

−

Hence, as in Lemma 8.8

ℙ

𝑤

k

k

(cid:16)

2 ∉

𝑚

10𝑝, 𝑚

10𝑝

+

−

(cid:8)
6 2𝑒−

𝑞.

(cid:9)

(cid:3) (cid:17)

This implies,

𝑤

k

𝑎𝑖 𝑒1k

+

2

𝑚

−

(cid:20)(cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2𝑚

6 𝑂

𝑎4
𝑖 +

(cid:0)

𝑝2

6 𝑂

𝑎4

(cid:1)

(cid:0)

𝑝2

.

+

(cid:1)

(cid:1)
We have everything we need to apply Hoeﬀding Inequality G.8

(cid:0)

(cid:13)
(cid:13)

𝑎𝑖 𝑒1, 𝑤

𝑀

(

2
)

𝑎12

+

𝑝4𝑎4

.

(cid:2)

2

−

(cid:17)

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
6 𝑂

(cid:13)
(cid:13)

𝑀

𝑎𝑖 𝑒1, 𝑤

)(cid:13)
(cid:13)
(cid:13)
(cid:13)
for 𝑝 > 1 and a universal constant 𝐶.
(cid:13)
(cid:13)

𝑙
Õ𝑖
]
∈[

(

ℙ

(cid:13)
(cid:13)
(cid:13)
©
(cid:13)
(cid:13)
(cid:13)
«

> 𝐶

𝑡

·

·

𝑙 log 𝑛

𝑎6

+

𝑝2𝑎2

6 𝑒−(

𝑡

1
)
−

log 𝑛 ,

p

(cid:0)

(cid:1)ª
®
¬

(cid:3)

The last intermediate result, is a high probability lower bound on the spectral norm of the
, that is, the matrix corresponding to the sum of the columns that contain
)

𝜆𝑒1, 𝑤
(

𝑀

matrix

∈
the spike.

𝑖

𝑣

{

}

supp
Í

Lemma 8.12. Let 𝜁1, . . . , 𝜁𝑙
1
}
auniversal constant 𝐶 > 0, suppose 𝑙 > 𝐶

∈ {−

1,

+

and 𝑤1, . . . , 𝑤𝑙
𝑡 log 𝑛
max

·

·

𝑁

0, Id𝑛
∼
−
(
𝑛3 log 𝑙, log2 𝑙

T
𝑒1𝑒1
. Let 𝛾
)
. Then

∈

ℝ, for 𝑡 > 1 and

𝜁𝑖𝛾𝑒1, 𝑤𝑖

𝑀

(

𝑙
Õ𝑖
]
∈[

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2𝑙−
(cid:13)

10.

(cid:9)

>

𝑙𝛾6
2

,

(cid:8)

)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

with probability at least 1

𝑡 log 𝑛

2𝑒−

−

−

Proof. Now,

𝑙
Õ𝑖
]
∈[

𝜁𝑖𝛾𝑒1, 𝑤𝑖

𝑀

(

=

)

=

©

«







𝑤𝑖

k

𝜁𝑖 𝛾𝑒1k

+

2

𝑚

−

2

(cid:17)

2𝑚

−

𝑙
Õ𝑖
∈[

] (cid:20) (cid:16)

(cid:21)

T
𝛾2𝑒1𝑒1

ª
®
𝑤𝑖
¬
k

𝛾4𝑙

+

𝑚2𝑙

2𝑚𝑙

−

−

2𝛾2𝑚𝑙

+

©

«

70

𝑙
Õ𝑖
]
∈[

4

k

−

2𝑚

𝑤𝑖

k

2

k

+

2𝛾2

𝑤𝑖

k

2

k

T
𝛾2𝑒1𝑒1

.



ª

®


¬


 
 
 
We bound the terms in the parenthesis. Recall that by construction

𝔼

𝑚2𝑙

2𝑚𝑙

−

−

2𝛾2𝑚𝑙

+

𝑙
Õ𝑖
]
∈[

©

𝑤𝑖

k

4

k

−

2𝑚

𝑤𝑖

k

2

k

+

2𝛾2

𝑤𝑖

k

2

k

For 𝑞 := 10 log 𝑙 and 𝑝 = max

√𝑚 𝑞, 𝑞

, we can condition on the event,

«

(cid:12)
(cid:12)
which happens with probability at least 1
(cid:12)

2𝑒−

(cid:8)
:=

ℰ

𝑖

(cid:9)
∈ [

𝑙

]

2

𝑤

k

k

∀
(cid:8)

𝑚

10𝑝, 𝑚

10𝑝

,

−

∈
𝑞. Then, by Hoeﬀding Inequality G.8,

(cid:3) (cid:9)

+

(cid:2)

= 0.



ª

®


¬


𝑚2𝑙

2𝑚𝑙

−

−

2𝛾2𝑚𝑙

−

+

𝑤𝑖

k

4

k

−

2𝑚

𝑤𝑖

k

2

k

+

2𝛾2

𝑤𝑖

k

> 𝐶

𝑡

·

·

√𝑙

𝑝2

𝑚𝑝

+

+

𝑙
Õ𝑖
]
∈[

©

«

2

k

(cid:12)
(cid:12)

(cid:12)

ª
(cid:12)

®
𝛾2𝑝
(cid:12)

(cid:12)

¬

(cid:1)








𝑙
Õ𝑖
]
∈[

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)








with probability at most 2𝑒−
follows that

𝑡, for 𝑡 > 1 and a universal constant 𝐶. By assumption on 𝛾, 𝑙 and 𝑛, it

(cid:0)

𝑀

𝜁𝑖𝛾𝑒1, 𝑤𝑖

T
𝛾2𝑒1𝑒1

>

T
𝑒𝑖 𝑒1

=

𝑙𝛾6
2

𝑙𝛾6
2

.

(

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
We are now ready to prove the main result of the section. Combining Lemma 8.13 with Lemma

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

𝑙
Õ𝑖
]
∈[

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:3)

)

8.12 and an application of Lemma H.2 we immediately get Lemma 8.4.

Lemma 8.13. Let 𝑌 be deﬁned as in Theorem 8.3, let 𝑑 > 𝐶
𝑛 > log 𝑑 and large enough constants 𝐶 , 𝐶∗, with probability at least 0.999,

·

𝑛3 log 𝑑 log 𝑛 > 100. For 𝑘𝜆6 > 𝐶∗𝑑,

2

𝑦𝑖

] (cid:20)(cid:16)(cid:13)
(cid:13)
where 𝑀 is a matrix such that

𝑑
Õ𝑖
∈[

(cid:13)
(cid:13)

𝑚

−

2

(cid:17)

−

2𝑚

𝑦𝑖 𝑦𝑖

T =

(cid:21)

Õ𝑖
supp
{
∈

𝑣

}

𝑀

𝑣

𝑖

𝜆𝑒1, 𝑤
)

(

(

) +

𝑀 ,

𝑀

k

k

6 𝑂

𝑑

𝑘𝜆5

+

+

𝑘𝑛 log 𝑛𝜆5

.

(cid:16)
1. Recall the notation used in the algorithm with 𝑐6(

p

(cid:17)

Proof. Let 𝑚 = 𝑛

−

Then we can rewrite the matrix 𝐴 computed by SVD-6 as,

𝑦𝑖 , 𝑛

=

)

𝑚

−

2

(cid:17)

−

2𝑚

.

(cid:21)

2

𝑦𝑖

(cid:20)(cid:16)(cid:13)
(cid:13)

(cid:13)
(cid:13)

2

𝑦𝑖

𝑑
Õ𝑖
∈[

] (cid:20) (cid:16)(cid:13)
(cid:13)

(cid:13)
(cid:13)

𝑚

−

2

(cid:17)

−

2𝑚

𝑦𝑖 𝑦𝑖

T =

(cid:21)

Õ𝑖
supp
{
∈

𝑣

}

𝑀

𝑣

𝑖

𝜆𝑒1, 𝑤
)

(

(

) +

𝑤𝑖 , 𝑣

𝑀

(

𝑖

𝜆𝑒1)
)

(

𝑣

}

𝑤𝑖, 𝑣′

𝑀

(

𝑖

(

)

Õ𝑖
supp
{
∈
𝑒1) +

𝑀

𝑣′

𝑖

(

)

(

𝑒1, 𝑤

)

𝑣

𝑖

𝜆𝑒1, 𝑛
)

(

)

+

(cid:0)

𝑣

}

Õ𝑖
𝑑
supp
{
∈[
]\
T
𝑤𝑖 𝑒1
𝑖
𝑣

(

)

𝑣

𝑖

(

)

+

T

𝑒1𝑤𝑖

(cid:1)

+

+

𝑑
Õ𝑖
supp
]\
∈[

{

𝑣

}
𝑤𝑖

𝑐6(

Õ𝑖
supp
{
∈

𝑣

}

71

 
 
We ﬁrst bound the cross-terms,

+

𝑑
Õ𝑖
supp
]\
∈[

{

𝑣

}

𝑤𝑖

𝑐6(

𝑣′

𝑖

𝑒1)

)

(

𝑣′

𝑖

(

)

+

T
𝑤𝑖 𝑒1

T

𝑒1𝑤𝑖

.

𝑣′

𝑖

(

)

+

(cid:0)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
6

𝑤𝑖

𝑐6(

𝑣′

𝑖

𝑒1)

)

(

𝑣′

𝑖

(

)

+

𝑣

{

}

𝑑
Õ𝑖
supp
]\
∈[

T
𝑤𝑖 𝑒1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
T
2𝑒1𝑒1
)

𝑤𝑖

𝑐6(

𝑣′

𝑖

𝑒1)

)

(

𝑣′

𝑖

(

+

𝑤𝑖

𝑐6(

𝑣′

𝑖

𝑒1)

)

(

𝑣′

+

𝑖

T
2𝑒1𝑒1
)

(

𝑑
Õ𝑖
supp
]\
∈[

{

𝑣

}

𝑑
Õ𝑖
supp
]\
∈[

{

𝑣

}

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

6

And

𝑤𝑖

𝑐6(

+

𝑣′

𝑖

𝑒1)

)

(

𝑣′

𝑖

2𝑤𝑖𝑤𝑖
)

(

T

𝑤𝑖

𝑐6(

+

𝑣′

𝑖

𝑒1)

)

(

𝑣′

𝑖

2𝑤𝑖𝑤𝑖
)

(

T

𝑑
Õ𝑖
supp
]\
∈[

{

𝑣

}

𝑑
Õ𝑖
supp
]\
∈[

{

𝑣

}

2
1
/

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
+ (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

𝑤𝑖

𝑐6(

𝑣

𝑖

𝜆𝑒1, 𝑛
)

(

)

𝑣

(

𝑖

)

+

𝑣

}

Õ𝑖
supp
{
∈

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
6

T
𝑤𝑖 𝑒1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
T
2𝑒1𝑒1
)

2
1
/

𝑤𝑖

𝑐6(

𝑣

𝑖

𝜆𝑒1, 𝑛
)

𝑣

𝑖

}

𝑣

(

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Õ𝑖
supp
{
∈

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Observe that, by construction of the vector 𝑣′ in Model 6.6 and since 𝑘𝜆6 > 𝐶∗𝑑, for a large enough
(cid:13)
6 100. Moreover we get that with probability at least
𝑣′(
constant 𝐶∗, we get that for all 𝑖
0.999, all the following inequalities hold.

Õ𝑖
supp
{
∈

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∈ [

+

)|

𝑑

]

)

)

(

(

𝑖

𝑣

,

}

|

𝑤𝑖

𝑐6(

𝑣

𝑖

𝜆𝑒1, 𝑛
)

𝑤𝑖𝑤𝑖

(cid:1)

2
1
/

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2
1
/

T

By Lemma 8.12,

By Lemma 8.10,

By Lemma 8.11,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

𝑀

𝑣

𝑖

𝜆𝑒1, 𝑤𝑖
)

(

(

Õ𝑖
supp
{
∈

𝑣

}

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

> 𝑘𝜆6
2

.

)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

𝑤𝑖, 𝑣

𝑀

(

𝑖

(

Õ𝑖
supp
{
∈

𝑣

}

𝑤𝑖, 𝑣′

𝑀

(

(

𝑑
Õ𝑖
supp
]\
∈[

{

𝑣

}

𝜆𝑒1)(cid:13)
)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
𝑒1)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

)

𝑖

6 𝑂

𝑘𝜆4

(cid:16)

+

p

𝑘𝑛 log 𝑛𝜆4

(cid:17)

6 𝑂

𝑑

(cid:16)

+

p

𝑑𝑛 log 𝑛

6 𝑂

𝑑

.

)

(

(cid:17)

𝑀

𝑣′

𝑖

(

)

(

𝑒1, 𝑤

𝑑
Õ𝑖
supp
]\
∈[

{

𝑣

}

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

6 𝑂

𝑑 log 𝑛 max

log 𝑑,

𝑛 log 𝑑

.

n

p

o(cid:17)

(cid:16)p

72

All in all we get,

2

𝑦𝑖

(cid:13)
(cid:13)
𝑑
Õ𝑖
(cid:13)
∈[
(cid:13)
6 𝑂
(cid:13)
(cid:13)

] (cid:20) (cid:16)(cid:13)
(cid:13)
𝑑
+

(cid:16)

(cid:13)
(cid:13)

p

𝑚

−

2

(cid:17)

−

2𝑚

𝑦𝑖 𝑦𝑖

T

(cid:21)

−

𝑑𝑛 log 𝑛

+

√𝑑𝑛 log 𝑑

𝑀

𝑣

Õ𝑖
supp
{
∈
√𝑑 log 𝑑

}

+

The result follows.

𝑣

𝑖

𝜆𝑒1, 𝑤𝑖
)

(

(

𝑘𝜆5

+

+

p

)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

𝑘𝑛 log 𝑛𝜆5

.

(cid:17)

(cid:3)

8.2 Algorithm recovers v with high probability

𝛽𝑢𝑣T

𝑢. Since several algorithms
We now show how to obtain a good estimate of the sparse vector 𝑣 from
ˆ
try to recover ﬁrst 𝑢 and then the sparse vector (e.g. SVD with thresholding) we turn back to the
model 1.1. A corollary for model 6.6 is presented at the end of the section. So, for the rest of the
ℝ𝑛 is a vector such that
section, let 𝑌 =
𝑢

> 0.9√𝑛, and 𝑊
k
The ﬁrst observation is that on one hand the vector 𝑌𝑣 is close to

𝑑. We also assume that 𝑛 6 𝑘 6 𝑑.

𝛽𝑢 with high probability.
On the other hand, the vector 𝑌T𝑢 may be far from the sparse vector; that is, even knowing exactly
𝑢, the thresholding step is required to recover 𝑣. The next theorem provides guarantees on the
achievable correlation with the sparse vector given a vector close to 𝑢. Theorem E.1 shows in which
sense in which these guarantees are information theoretically tight.

ℝ𝑑 is a 𝑘-sparse unit vector, 𝑢

+
0, 1
)
(

𝐸, where 𝑣

𝑊
+
𝑁

p

p

∼

∈

∈

k

×

𝑛

Theorem 8.14. Let

𝑢 be a vector such that
ˆ

𝑢
k ˆ

𝑢

k

−

6 𝜀√𝑛 for some 0 6 𝜀 6 1

10 , and let

If 𝛽 & 𝑘
𝜏2𝑛

log 𝑑

𝐸

2
1
→

2

k

+ k

𝑣 =
ˆ

1

T
𝑢
2 ˆ

𝑌 .

𝑢
· k ˆ
for some 0 < 𝜏 6 1, then with probability at least 1

p

𝛽

k

10 exp

𝑛

,
)

(−

−

(cid:0)

where 𝜂

𝑣
( ˆ

) ∈

(cid:1)

𝑣
( ˆ
ℝ𝑑 is the vector with coordinates

𝜂

k

𝑣

k

) −

. 𝜀

𝜏 ,

+

𝑖 =

𝜂

𝑣
( ˆ

)

𝑣𝑖 ,
ˆ
0,

(

> 𝜏

if

𝑣𝑖
| ˆ
otherwise

|

√𝑘
/

. Let’s rewrite 𝑌 =

𝛽

𝑢𝑣T
ˆ

+

𝑊

+

𝑍

+

𝐸 for a matrix

Proof. Assume that 𝛽 > 104
𝑍 =
𝑢
− ˆ

𝑘
𝜏2𝑛
𝑑. Then for 𝑖

ℝ𝑛

𝑣T

∈

𝑢

𝛽

×

)

(

(cid:0)

·

log 𝑑

∈ [

𝐸
+ k
𝑑
:
]

2
1
→

2

k

(cid:1)

p

T
𝑢
ˆ

𝑍

𝑖

=

𝛽

𝑢, 𝑢
h ˆ

𝑢
− ˆ

i

𝑣𝑖

𝑖

{

𝑣𝑖 = 0

Let 𝑆 =
, 𝑇 =
, 𝐴 =
By Lemma G.11, with probability at least 1
𝑣𝑖 = 0,

/

}

{

𝑖

|

(cid:12)
(cid:12)

(cid:12)
(cid:1)
(cid:0)
(cid:12)
𝑣𝑖 > 𝜏
| ˆ

(cid:12)
p
(cid:12)
√𝑘
(cid:12)
}

(cid:12)
(cid:12)
(cid:12)

𝑣𝑖
|
| |
2 exp
(−

p
6 2𝜏
/
𝑛
,
|
)

𝑖

{
−

p

6 𝜀

𝛽𝑛

𝑢
· k ˆ

𝑣𝑖

.

|

k · |

√𝑘
𝐵

|

and 𝐵 =

𝑖
}
{
6 𝑛. Consider some 𝑖

𝑢𝑊

| ( ˆ

)

𝑖 > 10
log 𝑑
𝑢
.
k ˆ
k
}
𝑇. Since
𝑆
p
∩

∈

𝑖 =

𝑢𝑊
( ˆ

)

2

𝛽

𝑢
· k ˆ

k

𝑣𝑖
· ˆ

𝑢𝐸

− ( ˆ

)

𝑖 > 100

𝑢
𝑢
k ˆ
k
√𝑛 · k ˆ

k

·

p

𝐸

2

k1

→

+ k

(cid:17)

log 𝑑

(cid:16)p
73

𝑢
− k ˆ

𝐸

k1

→

k · k

2 > 10

𝑢
k ˆ

k

log 𝑑 ,

p

which means that 𝑆
> 0.8√𝑛. Hence

𝑢
k ˆ

k

𝑇

∩

⊆

𝐵. Hence

𝑇

|

𝐵

|

\

6

𝑆

|

|

= 𝑘. Note that since 𝜀 6 1

10 and

> 0.9√𝑛,

𝑢

k

k

𝜂
(

𝑣
( ˆ

𝑖

)

−

𝑣𝑖

𝐵
𝑇
Õ𝑖
\
∈

2 =
)

𝐵
𝑇
Õ𝑖
∈
\
6 2

𝑣𝑖
( ˆ

−

𝑣𝑖

2

)

𝑛
𝑢
k ˆ

k

2 𝜀2𝑣2

𝑖 +

2

𝐵
𝑇
Õ𝑖
\
∈
100 log 𝑑

𝐵
𝑇
Õ𝑖
∈
\
6 4𝜀2

4

+

6 4𝜀2

+

𝐵
𝑇
Õ𝑖
∈
\
𝜏2 .

1
𝛽𝑛

(cid:0)

1
𝑢
k ˆ

𝛽

4

k

𝐸

k

+ k

T
𝑢
( ˆ

𝑊

T
2
𝑢
𝑖 + ( ˆ
)

𝐸

2
𝑖
)

(cid:1)

(cid:0)
2
1
→

2

(cid:1)

Note that since

𝐵

|

|

Hence

6 𝑛, By Theorem G.9, with probability at least 1

T
𝑢
( ˆ

𝑊

2
𝑖
)

6 100

2

𝑢
k ˆ

k

·

𝑛 log 𝑑 .

𝐵
Õ𝑖
∈

exp

(−

𝑛

,
)

−

𝜂
(

𝑣
( ˆ

𝑖

)

−

𝑣𝑖

𝐵
𝑇
Õ𝑖
∩
∈

2 =
)

𝐵
𝑇
Õ𝑖
∈
∩
6 2

𝑣𝑖
( ˆ

−

𝑣𝑖

2

)

𝑛
𝑢
k ˆ

k

2 𝜀2𝑣2

𝑖 +

2

log 𝑑

𝛽 +

400

𝜏2 .

Õ𝑖
𝐵
∈
2
𝐸
1
k
→
𝛽

2

4 k

𝐵
Õ𝑖
∈
6 4𝜀2

+

6 4𝜀2

+

1
𝑢
k ˆ

T
𝑢
4 ( ˆ

𝑊

2
𝑖 +
)

2

k

𝐵
Õ𝑖
∈

1
𝑢
k ˆ

k

𝛽

T
𝑢
4 ( ˆ

𝐸

2
𝑖
)

𝛽

If 𝑖

𝑆

\

∈

𝑇, then 𝜂

𝑣
( ˆ

)

𝑖 = 𝑣𝑖 = 0. If 𝑖

𝑆

∈

𝜏
√𝑘

>

>

𝑣𝑖
| ˆ

|

1

−

(cid:18)

√𝑛
𝑢
k ˆ

k

𝑣𝑖

𝜀

|

(cid:19)

hence in this case

𝑢𝑊

|( ˆ

𝑖

)

|

> 0.7

·

0.8

𝑇

𝐴, then

)

∩
∩
𝑢T𝑊
𝑖
( ˆ
2 (cid:12)
𝑢
𝛽
(cid:12)
k ˆ
(cid:12)
log 𝑑 > 10
(cid:12)
p
(cid:12)

− (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

k

| − (cid:12)
(cid:12)
(cid:12)
(cid:12)
p
100
(cid:12)

·

𝑖

> 1.8

𝑢T𝐸
( ˆ
)
2 (cid:12)
𝑢
𝛽
(cid:12)
k ˆ
(cid:12)
(cid:12)
log 𝑑, so 𝑖
(cid:12)

k

𝜏
√𝑘 − (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∈

𝑢T𝑊
( ˆ
𝑢
𝛽
k ˆ

)

k

𝑖
2 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

p
𝐵. Moreover,

0.1

−

𝜏
√𝑘

,

p
6 1.1𝜏

0.9√𝑘 +

𝑣𝑖

|

|

Therefore

𝑇
𝑆
Õ𝑖
∩
∩
∈
It follows that

𝐴

𝜂
(

𝑣
( ˆ

𝑖

)

−

𝑣𝑖

2 =
)

𝑣2
𝑖

6 2

𝑇
𝑆
Õ𝑖
∩
∩
∈

𝐴

𝐵
Õ𝑖
∈

2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
p
(cid:12)
2𝜏2
𝑘 +

p
1
T
𝑢
𝛽𝑛 ( ˆ

𝑊

𝑖

.

)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
𝛽𝑛2 ( ˆ

4

𝐵
Õ𝑖
∈

𝑢𝑊

6 4𝜏2

2
𝑖
)

+

𝜏2 = 5𝜏2 .

𝜂

𝑣
( ˆ

k

) −

𝑣

k

2 6

𝜂
(

𝑣
( ˆ

𝑖

)

−

𝑣𝑖

2
)

+

𝑇
Õ𝑖
∈

Hence with probability at least 1

−

𝑇
𝑆
Õ𝑖
∩
∩
∈
3 exp

𝑛

,
)

(−
𝑣
𝜂
( ˆ

k

𝑣2
𝑖 +

𝐴

𝑇
𝑆
Õ𝑖
∩
∩
∈

𝐴

𝑣2
𝑖

6 8𝜀2

2𝜏2

4𝜏2

+

+

+

5𝜏2 = 8𝜀2

11𝜏2 .

+

𝑣

k

) −

. 𝜀

𝜏 .

+

74

(cid:3)

An immediate consequence is the following corollary.

Corollary 8.15. Consider a matrix of the form,

𝑌 = 𝑊

T
𝜆𝑢𝑣

+

𝑢

𝑣′

+

−

T

𝑢

𝑊

T

for a Gaussian matrix 𝑊

0, 1
)
(
and a vector 𝑣′ as deﬁned in 6.6. Let
𝑛𝑑
then we can compute in time 𝑂

𝑁

∼

𝑛

×

(cid:1)
𝑑, a random unit vector 𝑢, a 𝑘-sparse vector 𝑣 with entries in

(cid:0)

𝑢 be a vector such that
ˆ
an estimator

𝑢

𝑢
k ˆ

−

k

𝑣 such that with probability at least 1
ˆ

−

6 𝜀 for some 0 6 𝜀 6 1

(

)

0,

1
}
{
±
10 . If 𝜆 & √log 𝑑
,
𝑛
exp
)

(−

𝜏

𝑣
k ˆ

𝑣

k

−

.

𝜀

(

+

𝜏

√𝑘 .
)

9 Experiments

In this section we compare the performance of Diagonal Thresholding and SVD of degree 2, 4, 6
as in 8.1 on practical instances. The table below explains the regimes of the ﬁgures presented. We
refer to Robust Sparse PCA as model 1.1 where the adversarial matrix 𝐸 follows the distribution
shown in 6.6. Appendix I contains a detailed report of the experimental setup.

𝑘 > √𝑑

𝑘 6 √𝑑

Standard Sparse PCA

Figure 2a for 𝛽 >

𝑑
𝑛

q
Figure 3 for 𝛽 > 𝑘
√𝑛

log 𝑑
𝑘

Robust Sparse PCA
Figure 2b for 𝛽 > 𝑘
𝑛
3
1
ure 2c for 𝛽 > 𝑘
/
(cid:0)
𝑛

𝑑
𝑘

2
1
/

𝑑
𝑘

(cid:1)

(cid:0)

(cid:1)

Fig-

q

Table 4: Plots

75

(a) Standard Sparse PCA, with 𝑘 > √𝑑, 𝛽 >

𝑑
𝑛

q

(b) Robust Sparse PCA with 𝑘 > √𝑑, 𝛽 > 𝑘
𝑛

𝑑
𝑛

2
1
/

(cid:16)

(cid:17)

(c) Robust Sparse PCA with 𝑘 > √𝑑, 𝛽 > 𝑘
𝑛

𝑑
𝑛

3
1
/

(cid:16)

(cid:17)

Figure 2: Forthe single spiked covariance model with 𝑘 > √𝑑, Figure 2a shows how the SVD algo-
rithms works (with information theoretically optimal guarantees) and Diagonal Thresholding fails.
Figures 2b, 2c show however how adversarial noise immediately breaks SVD with thresholding.
In Figure 2b 𝛽 & 𝑘
, hence as 𝑑 increases and becomes larger than 𝑛2, SVD-4 returns a good
𝑛
𝑛3 when the signal is much
estimate. We point out how how SVD-6 performs well even for 𝑑
larger than 𝑘
𝑛
but as 𝑑 grows towards 𝑛3, SVD-6 approaches correlation 1.

(cid:0)
3
1
. Finally, Figure 2c shows how DT, SVD-4 and SVD-2 fails for 𝛽 = Θ
/

2
1
/

3
1
/

≪

𝑘
𝑛

𝑑
𝑘

𝑑
𝑘

𝑑
𝑘

(cid:1)

,

(cid:16)

(cid:17)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

76

Figure 3: The ﬁgure shows settings in which 𝑘 6 √𝑑. In this regime, among the algorithms con-
sidered, Diagonal Thresholding achieves asymptotically the most correlation. In practical settings

however it is often the case that

recover the signal.

𝑘
√𝑛

log 𝑑 > Ω

p

3
1
/

𝑘
𝑛

𝑑
𝑘

(cid:16)

(cid:0)

(cid:1)

(cid:17)

and hence also SVD-6 can accurately

77

References

[AS16]

Emmanuel Abbe and Colin Sandon, Achieving the ks threshold in the general stochastic
block model with linearized acyclic belief propagation, Advances in Neural Information
Processing Systems 29 (D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett,
eds.), Curran Associates, Inc., 2016, pp. 1334–1342. 3

[AW09]

Arash A. Amini and Martin J. Wainwright, High-dimensional analysis of semideﬁnite
relaxations for sparse principal components, Ann. Statist. 37 (2009), no. 5B, 2877–2921. 1,
2, 25

[AWH13] George B. Arfken, Hans J. Weber, and Frank E. Harris, Chapter 15 - legendre functions,
Mathematical Methods for Physicists (Seventh Edition) (George B. Arfken, Hans J.
Weber, and Frank E. Harris, eds.), Academic Press, Boston, seventh edition ed., 2013,
pp. 715 – 772. 98

[BBP05]

Jinho Baik, Gérard Ben Arous, and Sandrine Péché, Phase transition of the largest eigen-
value for nonnull complex sample covariance matrices, Ann. Probab. 33 (2005), no. 5, 1643–
1697. 1

[BHK+16] Boaz Barak, Samuel B. Hopkins, Jonathan A. Kelner, Pravesh Kothari, Ankur Moitra,
and Aaron Potechin, A nearly tight sum-of-squares lower bound for the planted clique problem,
FOCS, IEEE Computer Society, 2016, pp. 428–437. 7, 19, 23, 25

[BKS14]

Boaz Barak, Jonathan A. Kelner, and David Steurer, Rounding sum-of-squares relaxations,
STOC, ACM, 2014, pp. 31–40. 64

[BKW20a] Afonso S. Bandeira, Dmitriy Kunisky, and Alexander S. Wein, Computational hardness of
certifying bounds on constrained PCA problems, 11th Innovations in Theoretical Computer
Science Conference, ITCS 2020, January 12-14, 2020, Seattle, Washington, USA, 2020,
pp. 78:1–78:29. 4

[BKW20b]

, Computational hardness of certifying bounds on constrained PCA problems, 11th
Innovations in Theoretical Computer Science Conference, ITCS 2020, January 12-14,
2020, Seattle, Washington, USA, 2020, pp. 78:1–78:29. 23, 40, 85

[BMR19]

Jess Banks, Sidhanth Mohanty, and Prasad Raghavendra, Local statistics, semideﬁnite
programming, and community detection, CoRR abs/1911.01960 (2019). 3

[BR13a]

[BR13b]

[BS16]

Quentin Berthet and Philippe Rigollet, Computational lower bounds for sparse PCA, CoRR
abs/1304.0828 (2013). 1, 2, 3, 11, 41

Quentin Berthet and Philippe Rigollet, Optimal detection of sparse principal components
in high dimension, Ann. Statist. 41 (2013), no. 4, 1780–1815. 1, 8

Boaz Barak and David Steurer, Proofs, beliefs, and algorithms through the lens of sum-
of-squares, Course notes: http://www. sumofsquares. org/public/index. html (2016).
20

78

[Cam60]

Lucien Le Cam, Locally asymptotically normal families, Univ. California Publ. Statist.
(1960). 24

[CMW13] T. Tony Cai, Zongming Ma, and Yihong Wu, Sparse pca: Optimal rates and adaptive

estimation, Ann. Statist. 41 (2013), no. 6, 3074–3110. 2

[Das99]

Sanjoy Dasgupta, Learning mixtures of gaussians, FOCS, IEEE Computer Society, 1999,
pp. 634–644. 82

[dGJL05] Alexandre d’Aspremont, Laurent E Ghaoui, Michael I Jordan, and Gert R Lanckriet,
A direct formulation for sparse pca using semideﬁnite programming, Advances in neural
information processing systems, 2005, pp. 41–48. 2, 3, 25

[DHS20]

Jingqiu Ding, Samuel B. Hopkins, and David Steurer, Estimating Rank-One Spikes from
Heavy-Tailed Noise via Self-Avoiding Walks, arXiv e-prints (2020), arXiv:2008.13735. 20

[DKK+16]

Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Al-
istair Stewart, Robust estimators in high dimensions without the computational intractability,
FOCS, IEEE Computer Society, 2016, pp. 655–664. 82

[DKWB19] Yunzi Ding, Dmitriy Kunisky, Alexander S. Wein, and Afonso S. Bandeira,

Subexponential-time algorithms for sparse pca, 2019. 1, 2, 6, 11, 17, 39, 40

[DM14]

[FK01]

Yash Deshpande and Andrea Montanari, Sparse PCA via covariance thresholding, NIPS,
2014, pp. 334–342. 2, 16, 88, 102

Uriel Feige and Joe Kilian, Heuristics for semirandom graph problems, J. Comput. Syst. Sci.
63 (2001), no. 4, 639–671. 3

[FMDF16] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard, Robustness of

classiﬁers: from adversarial to random noise, NIPS, 2016. 2

[GLS81] M. Grötschel, L. Lovász, and A. Schrĳver, The ellipsoid method and its consequences in

combinatorial optimization, Combinatorica 1 (1981), no. 2, 169–197. MR 625550 21, 22

[GV14]

Olivier Guédon and Roman Vershynin, Community detection in sparse networks via
grothendieck’s inequality, Probability Theory and Related Fields 165 (2014). 3

[HKP+17a] Samuel B. Hopkins, Pravesh K. Kothari, Aaron Potechin, Prasad Raghavendra, Tselil
Schramm, and David Steurer, The power of sum-of-squares for detecting hidden structures,
CoRR abs/1710.05017 (2017). 2

[HKP+17b]

, The power of sum-of-squares for detecting hidden structures, FOCS, IEEE Computer

Society, 2017, pp. 720–731. 2, 7, 23, 25, 39, 41

[HL18]

Samuel B. Hopkins and Jerry Li, Mixture models, robustness, and sum of squares proofs,
Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing,
STOC 2018, Los Angeles, CA, USA, June 25-29, 2018, 2018, pp. 1021–1034. 32, 82

79

[Hop18]

Samuel Brink Klevit Hopkins, Statistical inference and the sum of squares method. 7, 19,
20, 23, 24, 25, 40

[HS17]

Samuel B. Hopkins and David Steurer, Eﬃcient bayesian estimation from few samples:
Community detection and related problems, FOCS, IEEE Computer Society, 2017, pp. 379–
390. 7, 19, 20, 23, 25, 39

[HSSS16]

Samuel B. Hopkins, Tselil Schramm, Jonathan Shi, and David Steurer, Fast spectral
algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors, STOC,
ACM, 2016, pp. 178–191. 11, 64

[Hub81]

P.J. Huber, Robust statistics, Wiley Series in Probability and Statistics, Wiley, 1981. 2

[JL09]

Iain M. Johnstone and Arthur Yu Lu, On consistency and sparsity for principal components
analysis in high dimensions, Journal of the American Statistical Association 104 (2009),
no. 486, 682–693, PMID: 20617121. 1

[KKM18] Adam R. Klivans, Pravesh K. Kothari, and Raghu Meka, Eﬃcient algorithms for outlier-
robust regression, Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9
July 2018., 2018, pp. 1420–1430. 32

[KNV+15] Robert Krauthgamer, Boaz Nadler, Dan Vilenchik, et al., Do semideﬁnite relaxations solve
sparse pca up to the information limit?, The Annals of Statistics 43 (2015), no. 3, 1300–1322.
1, 2, 25

[KS17a]

[KS17b]

[Las01]

Pravesh K. Kothari and Jacob Steinhardt, Better agnostic clustering via relaxed tensor
norms, CoRR abs/1711.07465 (2017). 32

Pravesh K. Kothari and David Steurer, Outlier-robust moment-estimation via sum-of-
squares, CoRR abs/1711.11581 (2017). 32, 82

Jean B. Lasserre, New positive semideﬁnite relaxations for nonconvex quadratic programs,
Advances in convex analysis and global optimization (Pythagorion, 2000), Nonconvex
Optim. Appl., vol. 54, Kluwer Acad. Publ., Dordrecht, 2001, pp. 319–331. MR 1846160
21

[LM00]

B. Laurent and P. Massart, Adaptive estimation of a quadratic functional by model selection,
Ann. Statist. 28 (2000), no. 5, 1302–1338. 100

[MMV16] Konstantin Makarychev, Yury Makarychev, and Aravindan Vĳayaraghavan, Learning
communities in the presence of errors, COLT, JMLR Workshop and Conference Proceed-
ings, vol. 49, JMLR.org, 2016, pp. 1258–1291. 3

[Mon19]

Andrea Montanari, Optimization of the sherrington-kirkpatrick hamiltonian, 2019 IEEE
60th Annual Symposium on Foundations of Computer Science (FOCS), IEEE, 2019,
pp. 1417–1433. 4

[Mor07]

Stephan Morgenthaler, A survey of robust statistics, Statistical Methods and Applications
15 (2007), no. 3, 271–293. 2

80

[MPW16] Ankur Moitra, William Perry, and Alexander S. Wein, How robust are reconstruction
thresholds for community detection?, Proceedings of the 48th Annual ACM SIGACT
Symposium on Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21,
2016, 2016, pp. 828–841. 3

[MS16]

Andrea Montanari and Subhabrata Sen, Semideﬁnite programs on sparse random graphs
and their application to community detection, STOC, ACM, 2016, pp. 814–827. 3

[MSS16]

Tengyu Ma, Jonathan Shi, and David Steurer, Polynomial-time tensor decompositions with
sum-of-squares, CoRR abs/1610.01980 (2016). 20

[MV10]

Ankur Moitra and Gregory Valiant, Settling the polynomial learnability of mixtures of
gaussians, FOCS, IEEE Computer Society, 2010, pp. 93–102. 82

[MW15]

Tengyu Ma and Avi Wigderson, Sum-of-squares lower bounds for sparse PCA, NIPS, 2015,
pp. 1612–1620. 2

[MWA06] Baback Moghaddam, Yair Weiss, and Shai Avidan, Generalized spectral bounds for sparse
LDA, Machine Learning, Proceedings of the Twenty-Third International Conference
(ICML 2006), Pittsburgh, Pennsylvania, USA, June 25-29, 2006, 2006, pp. 641–648. 25

[Nat95]

[Nes00]

[NP33]

[Par00]

[PH94]

B. K. Natarajan, Sparse approximate solutions to linear systems, SIAM J. Comput. 24 (1995),
no. 2, 227–234. 25

Yurii Nesterov, Squared functional systems and optimization problems, High performance
optimization, Appl. Optim., vol. 33, Kluwer Acad. Publ., Dordrecht, 2000, pp. 405–440.
MR 1748764 21

J. Neyman and E. S. Pearson, On the problem of the most eﬃcient tests of statistical hypotheses,
Philosophical Transactions of the Royal Society of London. Series A, Containing Papers
of a Mathematical or Physical Character 231 (1933), 289–337. 24

Pablo A Parrilo, Structured semideﬁnite programs and semialgebraic geometry methods in
robustness and optimization, Ph.D. thesis, California Institute of Technology, 2000. 21

Karl Pearson and Olaus Magnus Friedrich Erdmann Henrici, Iii. contributions to the
mathematical theory of evolution, Philosophical Transactions of the Royal Society of
London. (A.) 185 (1894), 71–110. 82

[Rud99]

M. Rudelson, Random vectors in the isotropic position, Journal of Functional Analysis 164
(1999), no. 1, 60 – 72. 99

[RV17]

Oded Regev and Aravindan Vĳayaraghavan, On learning mixtures of well-separated
gaussians, FOCS, IEEE Computer Society, 2017, pp. 85–96. 82

[Sch17]

Konrad Schmüdgen, The moment problem, Springer International Publishing, 2017. 97

[Sho87]

N. Z. Shor, Quadratic optimization problems, Izv. Akad. Nauk SSSR Tekhn. Kibernet.
(1987), no. 1, 128–139, 222. MR 939596 21

81

[SK75]

[Tro12]

[VW02]

[Wai19]

David Sherrington and Scott Kirkpatrick, Solvable model of a spin-glass, Physical review
letters 35 (1975), no. 26, 1792. 3

Joel A. Tropp, User-friendly tail bounds for sums of random matrices, Foundations of
Computational Mathematics 12 (2012), no. 4, 389–434. 100, 101

Santosh Vempala and Grant Wang, A spectral algorithm for learning mixtures of distribu-
tions, FOCS, IEEE Computer Society, 2002, p. 113. 82

Martin J. Wainwright, High-dimensional statistics: A non-asymptotic viewpoint, Cam-
bridge Series in Statistical and Probabilistic Mathematics, Cambridge University Press,
2019. 95, 100

A Relationship with Clustering mixture of subgaussians

ℝ𝑛 from a
The canonical version of the Gaussian Mixture Model consists of 𝑑 samples 𝑦1, . . . , 𝑦𝑑
mixture of 𝑘 Gaussian probability distributions 𝐺1, . . . , 𝐺𝑘 with means 𝜇1, . . . , 𝜇𝑘 and covariances
> Δ
Σ1, . . . Σ𝑘 close to the identity in the spectral sense
for some parameter Δ of the problem. The goal is then to partition the samples in clusters 𝑆1, . . . , 𝑆𝑘
𝐺𝑖. Notice that the input can be written as 𝑌 = 𝑊
𝑋 where 𝑋 is a matrix
such that
with only 𝑘 distinct columns 𝜇1, . . . , 𝜇𝑘 and 𝑊 is a matrix with independent columns following
distributions 𝑁

. 1, such that

, . . . , 𝑁

𝑆𝑖, 𝑦𝑗

∈ [

𝑖, 𝑗

Σ𝑖

Id

𝜇𝑗

𝜇𝑖

𝑦𝑗

(cid:13)
(cid:13)

(cid:13)
(cid:13)

−

+

−

∀

∀

∈

∈

𝑘

k

k

]

,

∼
0, Σ1)
(

0, Σ𝑘
(

.
)

There is a reach literature concerning this problem both from a statistical and computational
perspective (e.g. see [PH94, Das99, MV10, VW02, DKK+16, RV17, HL18, KS17b]). A simple greedy
algorithm called single-linkage clustering can be designed observing that with high probability,
whenever Δ & 𝑛1
4, pairs of samples from the same cluster are closer in Euclidean distance closer
/
than pairs of samples from diﬀerent clusters. Furthermore, for 𝑘 < 𝑛, as the centers 𝜇1, . . . , 𝜇𝑘 lives
in a 𝑘 dimensional subspace and this space is close to the span of the top 𝑘 singular vectors of 𝑌,
the bound can be pushed down to Δ & 𝑘1
4 projecting the points into this low-dimensional space.
/
This algorithmic barrier of Δ & 𝑘1
4 was broken only recently. Independently [HL18, KS17b]
/
𝛾2

provided SoS algorithms able to reconstruct the clusters for Δ & 𝑘𝛾, for any 𝛾 > 0, in time
using 𝑘𝑑1
/
following intuition: given a set 𝑆𝑖 =
form the same distribution 𝐺𝑖, then the ﬁrst 1
/
the subgaussian bound 𝔼𝑆𝑖
the 𝑡-th moment will not be Gaussian.

)
𝛾 samples. This new approach (which we refer to as the moments method) is based on the
of 𝑑1
𝛾 samples, if most of the samples come
/
𝛾 moments of the empirical distribution will satisfy
(cid:8)
2. Conversely, if samples actually belongs to diﬀerent clusters,
/

𝑦1, . . . , 𝑦𝑑1

. 𝑡 𝑡

𝑑𝑘

𝑦𝑡

1
/

(cid:9)

𝑂

(

)

(

𝛾

/

For our discussion, an important observation is that the same algorithms works for mixtures

of subgaussians.

𝜆

+

𝑊

𝛽𝑢0𝑣0

T as 𝑌 = 𝑊

+
𝜎𝑖 = sign
p
(

It is insightful to see the Sparse PCA problem in this perspective. Rewrite the matrix 𝑌 =
and entries

𝑢𝜎T where
¯
𝑢 := 𝑢0
𝑢 is the vector
𝑢0
¯
¯
q
. Furthermore, notice that the matrix 𝜆
)

𝛽𝑛
𝑘 . The two models are equivalent as with
𝑢𝜎T has only three distinct
high probability
¯
𝑢 and the zero vector. That is, we could see an instance of the canonical Sparse
columns:
¯
PCA problem as a non-uniform mixture of three Gaussian distributions with separation 𝜆. In

ℝ𝑑 is the vector with support supp

∈
and 𝜆 =

𝑢,
− ¯

𝑢0k

𝑣0}

𝛽
𝑘 k

= Θ

𝜆
(

𝑣0,𝑖

,
)

𝜎
¯

q

{

k

k

82

(cid:2)

(cid:3)

this formulation, Diagonal and Covariance Thresholding recovers the sparse vector (and hence
separates the cluster centered at the origin from the others) for 𝜆 >
, the same bound as
the single-linkage algorithm.

4
𝑛1
/

˜𝑂

Consider now the Wishart model 1.1 with adversarial perturbations: 𝑌 = 𝑊

𝐸.
Fix 𝐷 = 1
𝛾, the adversarial matrix 𝐸𝐷 will be the one described in Section 2.5 and Section 6, which
/
can be written as 𝐸𝐷 =
and such
that its non-zero entries are distributionally independent. All in all, this leads us to the formulation

T for some vector 𝑣′ ∈

ℝ𝑑 with support

q
supp

𝑢
𝑢0k ¯

𝑣′ −

𝑣0}

𝑊T

] \

𝑢
¯

𝑢
¯

+

+

𝑑

{

[

𝜎T
· ¯

𝛽
𝑘 k

(cid:0)

(cid:1)

(cid:0)

(cid:1)
𝑌 = 𝑊

𝐸𝐷

𝜆

𝑢
¯

+

T = 𝑊 ′
𝜎
· ¯

+

𝑋 ,

+

+

with 𝑊 ′ = 𝑊
the matrix 𝑋 has three distinct columns:

𝐸𝐷 being a matrix with independent columns and subgaussian moments. Again
𝑢 and the zero vector.
¯

Similarly to the non-robust settings, this formulation can be seen as an non-uniform mixture
of three subgaussian distributions with separation Δ & 𝜆. By the argument shown in Section 6 any
algorithm that can be described as a low-degree polynomial and that tries to cluster these points
using the moments method will be able to detect that 𝑌 is not a single subgaussian distribution
(and hence it is not a good cluster) only using at least 𝑑𝐷 & 𝑘𝑑𝐷 samples.

𝑢,
− ¯

B Comparison with the Wigner model

+

+

𝑊

𝐸, where 𝑊

The Wigner model presents some diﬀerences in the robust settings. Here we consider a matrix
T
𝑌 = 𝛽𝑣0𝑣0
±
n
the jointly-distributed random variables 𝑊, 𝑣0 are independent. The matrix 𝐸 has norm
for some 𝑏
that in order to have an algorithm that outputs a vector
zero with high probability, the adversarial matrix needs to satisfy the bound

and
6 𝑏2
ℝ. An analysis similar to the one made for the Wishart model in Section 2.1 shows
2 is bounded away from
6 𝛽
𝑘 .

𝑑, 𝑣0 is a 𝑘-sparse unit vector with entries in

𝑣 such that
ˆ

𝑣, 𝑣0i
h ˆ

0, 1
)
(

1
√𝑘
𝐸

o
k∞

, 0

𝑁

∼

𝐸

∈

k

×

𝑑

In these settings the simple PCA approach of computing the top eigenvector of 𝑌 and removing
all but the top 𝑘 entries fails. Indeed it suﬃces to plant a matrix 𝐸 = 𝑧𝑧⊤ where 𝑧 is the vector with
and 0 otherwise. For 𝑏𝑑 & 𝛽 the top eigenvector of 𝑌 is almost
entries 𝑧𝑖 = 𝑏 if 𝑖
𝑣0}
orthogonal to 𝑣0, and so is its projection to the top 𝑘 coordinates.

supp

∈ [

𝑑

{

]

k

k∞

The Covariance Thresholding algorithm also can be easily fooled by an adversary with the

same approach to the one previously shown: simply let 𝐸 = 𝑧𝑧⊤ be a rank 1 matrix with supp
supp

=

.

𝑧

{

} ∩

The diﬀerence appears in the Diagonal Thresholding algorithm, which turns out to be per-
diagonal entries will have value
will have
𝛽. The reason behind this diversity is that, in model 1.1 the

turbation resilient. Indeed as
𝑌𝑖𝑖
k∞
|
+
. 1
value bounded by ˜𝑂
(
adversarial perturbation exploits the large norm of the columns of 𝑊.

supp
𝛽. Conversely diagonal entries indexed by 𝑗 ∉ supp

& 1
1

𝑘 , for 𝑖

𝑣0}

𝑣0}

. 𝛽

& 1

k∞)

− k

+
𝐸

+ k

k∞

+

𝐸

𝐸

∈

𝛽

{

{

k

|

𝑣0}

{

∅

C Thresholding Algorithms are Fragile

In this section we formalize the discussions of the introduction and show that SVD with Threshold-
ing, Diagonal Thresholding and Covariance Thresholding are indeed not resilient to adversarial
perturbations.

83

C.1 SVD with Thresholding is Fragile

The polynomial-time algorithm presented in Section 1 for the strong-signal regime is highly sensi-
tive to small adversarial perturbations. Concretely, this can be shown constructing 𝐸 with entries
so that eigenvectors of 𝑌T𝑌 cannot be used to recover 𝑢0.
bounded ˜𝑂
𝛾𝑢0𝑢0
𝑢0k

2 that we will choose later. Then 𝑌 =

√𝑛
Consider 𝐸 =

T𝑊 for some 0 < 𝛾 <

T
𝛽𝑢0𝑣0

1
/
(

)
−

+

k

T
𝛾𝑢0𝑢0

𝑊 and

Id

−

p

(cid:0)

(cid:1)

𝑌𝑌

T

T =𝛽𝑢0𝑢0
𝛽

Id
+
T
𝑢0𝑣0
(cid:0)

−
𝑊

+

p
Hence with high probability,

(cid:0)

T
𝛾𝑢𝑢0

Id

−

(cid:0)

T
𝛾𝑢0𝑢0

T

𝑊𝑊
T
𝛾𝑢0𝑢0
(cid:1)

Id

(cid:0)
+

−
Id

T
𝛾𝑢0𝑢0
(cid:1)

𝑊

T

T
𝑣0𝑢0

−

(cid:1)

(cid:0)

(cid:1)

(cid:1)

1
𝑢0k

k

T
𝑢0

𝑌

k

2 k

2 = 𝑧

𝛽

𝑢0k

k

+

2

𝛾2

2𝑑

𝑢0k

k

+

2𝛾

𝑑

𝑢0k

k

+ ˜𝑂

−

𝛽

/

𝑛

,

(cid:19)

(cid:18)q

where 𝑧 has a 𝜒2-distribution with 𝑑 degrees of freedom. On the other hand notice that for a unit
2 which has the same
vector 𝑥 orthogonal to 𝑢0 and independent of 𝑊, we get
k
𝑑 + ˜𝑂
distribution as 𝑧. So our claim follows choosing 𝛾 so that 2𝛾
𝑢0k
k

Indeed then 𝑢0𝑌 has the same distribution as 𝑧. Now, since with high probability
𝑑

𝑛
.
𝛽
2 6 2𝑛, if
(cid:17)
(cid:16)
p

𝑛 & 𝛽, such a 𝛾 exists.

𝑥T𝑌
k
𝑢0k −

𝑥T𝑊
k
𝑢0k

k
2 = 𝛽

2 =

𝛾2

𝑢0

1
𝑑

/

k

k

·

k

k

2

/

C.2 Diagonal Thresholding is Fragile

Recall that Diagonal Thresholding ﬁnds the top 𝑘 diagonal entries of the covariance matrix and
𝑘 principal submatrix. We shows here that
output a top eigenvector of the corresponding 𝑘
a simple adversary can make diagonal entries in
larger than diagonal entries in
] \
𝑣0}
supp
, hence leading the algorithm to choose a submatrix which contain no information about
the sparse vector.

𝑣0}

supp

×
𝑑
[

{

{

Concretely, the algorithm can be written as follows:

Algorithm C.1 (Diagonal Thresholding).

Given: Sample matrix 𝑌 of form 1.1 where 𝑣0 is a ﬂat vector.

Estimate: The sparse vector 𝑣0.

Operation:

1. Let 𝑆 :=

entries of 𝑌T𝑌.

{

𝑖1, . . . , 𝑖𝑘

} ⊆ [

𝑑

]

be the set of indexes denoting the 𝑘 largest diagonal

2. Output a top eigenvector of 𝑌T𝑌

𝑆

[

𝑆

.
]

×

We start by deﬁning the adversarial matrix.

84

Deﬁnition C.2. Let 𝑏
with columns

∈

𝑅 and denote with 𝑊1, . . . , 𝑊𝑑 the columns of 𝑊. Deﬁne 𝐸 to be the matrix

𝐸𝑖 =

𝑊𝑖

𝑏
𝑊𝑖 k
k
0

(

The result is shown in the theorem below.

if𝑖

𝑑

supp

] \
∈ [
otherwise.

𝑣0}

{

Theorem C.3. Let 𝑛 > 𝜔
𝐸 be as deﬁned in C.2 and

log 𝑑

𝐸

(cid:0)
k

k1

𝑘

, 𝛽 = 𝑜
. Let 𝑌 be sampled according to 1.1 where 𝑣0 is a ﬂat vector. Let
(
)
2 & 𝛽√𝑛
(cid:1)
𝑘 +
→

log 𝑑. Then for each 𝑖

𝑣0}

𝑣0}

supp

supp

and 𝑗

∈ [

∈

𝑑

{

{

]

with probability at least 0.99.

p
𝑌𝑒𝑖
k

k

2 >

2

𝑌𝑒𝑗

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Notice how, for 𝛽 = Θ

log 𝑑
suﬃces to fool Diagonal Thresholding. A perturbation resilient algorithm would succeed as long

the theorem implies that an adversary with

log 𝑑

k1

𝐸

→

k

𝑘
√𝑛

2 &

p

as

𝐸

k

k1

→

2 . min

(cid:16)
𝑛 log 𝑑

p
4
1
,
/

(cid:17)
𝑘 log 𝑑

.

n(cid:0)
Remark C.4. The same adversary also fools the limited exhaustive search algorithm from [BKW20b]
that runs in time 𝑛𝑂

) up to some very large 𝑡 (say, up to some 𝑡 = 𝑛Ω

1
)).

p

o

(cid:1)

(

(

𝑡

Proof of Theorem C.3. Let 𝑏 =

𝐸

k

k1

→

2. We condition our analysis on the event that

𝑖

∀

𝑑

]

∈ [

𝑖

∀

𝑑

]

∈ [

𝑛

40

𝑛 log 𝑑, 𝑛

2

𝑊𝑖

k

k
∈
h
2 6 𝑛
𝑢0k
k
𝑢0, 𝑊𝑖

i

h

+
6 10

−
p
100√𝑛

𝑛 log 𝑑

+

40

𝑛 log 𝑑

.

p

i

p
which happen with probability at least 0.99 by Fact G.4. Denote with 𝑒1, . . . , 𝑒𝑑 the standard basis
vectors in ℝ𝑑. Notice that, by construction of 𝐸, for 𝑖

. Thus,

supp

𝑑

𝑣0}

{

, 𝐸𝑒𝑖
𝐸𝑒𝑖 k
k

= 𝑊 𝑒𝑖
𝑊 𝑒𝑖 k
k

> 𝑛

On the other hand, for 𝑗

supp

,

𝑣0}

{

∈

2 =

𝑊

𝑌𝑒𝑖

k

k

+

𝐸

+
𝑏
𝑊𝑖
𝑏2

(cid:16)

1

(cid:18)
𝑊𝑖

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
k
(cid:13)
> 𝑛

=

+
2

k

+

k
𝑏2

+
𝑏√𝑛

+

+

∈ [

] \

T
𝛽𝑢0𝑣0

𝑒𝑖

2

(cid:17)

p

𝑊𝑖

2

(cid:13)
(cid:13)
(cid:13)

𝑊𝑖

k (cid:19)

+
𝑏√𝑛

(cid:13)
(cid:13)
2𝑏
(cid:13)
k
(cid:13)
𝑂

−

𝑂

−

k
𝑛 log 𝑑

(cid:16)p
𝑛 log 𝑑

.

(cid:17)

(cid:16)p

(cid:17)

𝑌𝑒𝑗

2 =

2

𝑊𝑗

𝛽
𝑘 k

2

𝑢0k

+

𝛽
𝑘 h

𝑊𝑗 , 𝑢0i

+ r

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
6 𝑛

(cid:13)
𝑛𝛽
(cid:13)
𝑘 +

+

𝛽𝑛 log 𝑑
𝑘

!

+ r

𝑂

𝑛 log 𝑑

p

85

 
where the last step follows as 𝛽 = 𝑜

𝑘

(

. Combining the two inequalities,
)

6 𝑛

𝑛𝛽
𝑘 +

+

𝑂

𝑛 log 𝑑

(cid:16)p

(cid:17)

𝑌𝑒𝑖

k

2

k

−

𝑌𝑒𝑗

2 > 𝑏√𝑛

𝑛𝛽
𝑘 −

−

which is larger then zero whenever,

(cid:13)
(cid:13)

(cid:13)
(cid:13)

𝑂

𝑛 log 𝑑

(cid:16)p

(cid:17)

𝑏 > 𝑂

𝛽√𝑛

𝑘 +

(cid:18)

log 𝑑

.

(cid:19)

p

(cid:3)

C.3 Covariance Thresholding is Fragile

In this section, we show how in model 1.1 the Covariance Thresholding algorithm fails to output a
good estimation of the vector 𝑣0 in the presence of an adversarial distribution. Speciﬁcally, we will

show that the algorithm fails for 𝑘 >

. This bound is signiﬁcant in the sense that already

𝑛 log 𝑑
𝑘2

𝐸

q
k

2
1
→

2

k

𝐸

for
observed in the Wigner model, we omit this proof since it is simpler than in the Wishart model.

𝛽𝑛
𝑑 , the algorithm breaks. We remark that a similar phenomenon can also be

2 = 𝑑𝑜

k1

1
)

→

k

(

q

Recall that the central idea behind Covariance Thresholding is to threshold entries of the
empirical covariance matrix. The thresholding operation should remove noise while leaving the
submatrix 𝛽
will then be close to the
sparse vector. The key observation behind the adversary is that it is possible to plant a matrix 𝐸
with small norm
has many large
k1
eigenvalues with eigenspace far from 𝑣0.

T untouched. The top eigenvector of 𝜂𝜏

2 such that the thresholded covariance matrix 𝜂

2𝑣0𝑣0

(cid:1)
𝑌T𝑌

𝑢0k

𝑌T𝑌

𝑛Id

𝑛Id

−

−

𝐸

→

k

k

(cid:0)

(cid:0)

(cid:1)

Consider the Covariance Thresholding algorithm:

Algorithm C.5 (Standard Covariance Thresholding).

Input: Threshold 𝜏, sample matrix 𝑌 =

𝑢0𝑣𝑇
𝑑 where 𝑣0 is 𝑘-sparse, 𝑢0
and 𝑊 have i.i.d subgaussian entries of mean 0 and variance 1 and 𝐸 has column norms
bounded by 𝑏.

0 +

ℝ𝑛

𝑊

p

+

𝐸

∈

𝛽

×

·

Estimate: The sparse vector 𝑣0.

Operation:

1. Compute the thresholded matrix 𝜂𝜏
𝑌T𝑌
(cid:0)

2. Output a top eigenvector

𝑣 of 𝜂𝜏
ˆ

−

−
𝑛Id

𝑛Id

.

.

(cid:1)

𝑌T𝑌

The main result of the section is the Theorem below. Its signiﬁcance is to be read under this
) large
)
will not be correlated with 𝑣0.

perspective: it shows that there exists an adversary that can plant several (i.e. 𝜔
eigenvalues, as a consequence the top eigenvectors of 𝜂𝜏

log 𝑑
(

𝑌T𝑌

𝑛Id

(cid:0)

(cid:1)

−

(cid:0)

(cid:1)

86

∈ [

]

Theorem C.6. Suppose that 𝑘 6 √𝑑 and log10 𝑑 6 𝑛 6 𝑑. Let 𝑌 be of the form 1.1 for a ﬂat vector 𝑣0. Let
𝑟

ℝ be such that 2√𝑛 6 𝜏 6 𝑜

be such that 𝜔

𝑛 log 𝑑

log 𝑑

as 𝑑

𝑛

.

Then with probability at least 1
𝛽𝑛
𝑑 and orthogonal vectors 𝑧1, . . . , 𝑧𝑟 such that

(cid:0)
2 6 𝑑𝑜

→ ∞

(cid:1)
1
)

−

column norm

𝐸

(

) there exists an adversarial matrix 𝐸 with maximal
(cid:16)p

(cid:17)

6 𝑟 6 𝑑𝑜
𝑜

(

1
) and 𝜏
(as 𝑑
1
)
(

∈

→ ∞

k

k1

→

𝑖

∀

𝑟

,

]

∈ [

q

1
𝑧𝑖

k

2 ·

k

𝑧𝑖

T

𝜂𝜏

T
𝑌

𝑌

−

𝑛Id

T
𝑧𝑖 > 𝑣0

𝜂𝜏

T
𝑌

𝑌

𝑛Id

𝑣0

−

(cid:0)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

and

= 0.

𝑧𝑖 , 𝑣0i
h
The theorem shows that with these adversarial perturbations the ﬁrst 𝑟 eigenvectors of the
thresholded covariance matrix are uncorrelated with the sparse vector 𝑣0. Notice that for 𝛽 > 1

𝛽𝑛
a perturbation resilient algorithm should succeed with perturbations bounded by
𝑘 , that is,
much larger (in absolute value) than the ones used to fool Covariance Thresholding. In par-

q

ticular, for 𝛽 = Θ

𝐸

k1
k
ceed for

→

2 6 𝑑−
𝑜
4
1
+
/
(
2 6
𝐸
k1

→

k

log 𝑑
𝑘2

Theorem C.6 implies that already with perturbations satisfying

𝑘
√𝑛
(cid:18)
q
4 the algorithm fails, while a perturbation resilient algorithm would suc-
)𝑛1
1
/
4
𝑛1
˜𝑂
/

(cid:19)

.

Before showing the proof, we provide some intuition.

(cid:0)

(cid:1)

Algorithm Intuition. Let’s ignore cross-terms for a moment and consider the Wishart model
with no adversarial distribution. Then the centered empirical Covariance Matrix looks like

T
𝑌

𝑌

𝑛Id

−

≈

T

𝑊

𝑊

𝑛Id

+

−

T
𝛽𝑛𝑣0𝑣0

.

𝑛 log 𝑑
𝑘2
𝑊T𝑊
q
𝑛Id
> 𝜏 whenever 𝑖, 𝑗
(cid:1)

−

(cid:0)

for some large enough constant 𝐶 > 0, then
will be larger than 𝜏. 25 On the other hand, for
T will survive

, many entries of 𝛽𝑛𝑣0𝑣0

supp

∈

𝑣0}

{

𝑛

−

Θ

If we set the threshold 𝜏 = 𝐶
𝑘4 entries in
𝑑2 exp
𝜏2
/
𝛽 & 𝑘
log 𝑑
𝑘2 as
(cid:1)(cid:3)
(cid:2)
(cid:0)
√𝑛
the thresholding. This means that,
(cid:12)
q
(cid:12)
(cid:12)

≈
T
𝛽𝑛𝑣0𝑣0

𝑖𝑗

(cid:0)

(cid:1)

(cid:12)
(cid:12)
(cid:12)
T
𝑌

𝜂𝜏

(cid:0)

𝑌

𝑛Id

≈

T

𝑊

𝑊

𝑛Id

𝑆

[

] +

−

T
𝛽𝑛𝑣0𝑣0

−

𝑑

has cardinality approximately 𝑘4. If the entries were independent, since the
where 𝑆
fourth moment of each entry is not much larger than the second moment, standard spectral matrix
bounds suggest

] × [

⊆ [

𝑑

]

(cid:1)

(cid:0)

(cid:1)

where 𝜎 6 𝜏 exp

𝐶𝜏2
10𝑛

6 √𝑛

i

−

h

(cid:13)
𝑘
(cid:13)
√𝑑

·

25To see this, recall that in a 𝑑
log 𝑑2

𝑘4 . While entries in 𝑊

𝑊

T

−

q

T

𝑊

𝑊

𝑛Id

𝑆

[

−

6 𝑂

𝜎√𝑑

,

(cid:0)
is a standard deviation of each entry. Hence we get

(cid:1)

(cid:16)

(cid:17)

T

𝑊

𝑊

𝑛Id

𝑆

[

−

6 𝑂

𝑘√𝑛

,

(cid:17)
𝑑 Gaussian matrix, with high probability there are at most 𝑘4 entries larger than

(cid:16)

(cid:1)

(cid:0)

(cid:13)
(cid:13)

×
𝑛Id are dependent, a similar bound will hold.

]
(cid:13)
(cid:13)

]
(cid:13)
(cid:13)

87

and

T
𝛽𝑛𝑣0𝑣0

= 𝛽𝑛.

In conclusion, for 𝛽 & 𝑘
√𝑛

(cid:13)
(cid:13)
The main technical diﬃculty here is that the entries of 𝑊T𝑊 are not independent. In [DM14]

𝑘2 the top eigenvector of 𝜂𝜏

will be close to 𝑣0.

log 𝑑

𝑌T𝑌

𝑛Id

(cid:13)
(cid:13)

−

q

the authors provide a method to bound the spectral norm of the thresholded matrix26.

(cid:0)

(cid:1)

Adversarial Strategy. Now we provide intuition on how to choose 𝐸 such that with constant
probability there exists a vector 𝑧 orthogonal to 𝑣0 for which

T
𝑌

𝑌

𝜂𝜏

𝑛Id

−

T
& 𝑣0

𝜂𝜏

T
𝑌

𝑌

𝑛Id

𝑣0.

−

𝑧T
𝑧

k

k

(cid:0)

𝑧
𝑧

k

k

(cid:1)

∈
supp

ℝ𝑛 be a randomly chosen unit vector orthogonal to 𝑢0, let 𝑧 be a vector such that supp
1

, 𝑧𝑖 = 𝜎𝑖𝑏 for some 𝑏

Let 𝑥
𝑑
∈
[
deﬁne the adversarial matrix as 𝐸 := 𝑥𝑧T, notice that
𝐸
the entry 𝑖𝑗 of the centered empirical covariance matrix

to be set later and 𝜎𝑖
. For 𝑖, 𝑗
𝑏
supp
/
𝑛Id

∼ {±
𝑧
}

and for 𝑖

𝑧
{
}
. We
}
, consider

𝑣0}

supp

k∞

] \

=

∈

𝑧

{

{

{

}

k

(cid:0)

(cid:1)

ℝ
∈
+
6
˜𝑂
𝑌T𝑌
(cid:0)
−

√𝑛
,
(cid:1)

T
𝑌

𝑌

−

𝑛Id

𝑖𝑗

=

𝑤𝑖 , 𝑤 𝑗

h

i + h

𝑤𝑖, 𝑥

i + h

(cid:0)
𝑥, 𝑤 𝑗

(cid:1)
.

𝑧𝑖 𝑧 𝑗

i +

𝑤𝑖 , 𝑤 𝑗

𝑧𝑖 𝑧 𝑗

,

h

i +

(cid:1)

(cid:0)

(cid:12)
(cid:12)
(cid:12)
by construction of 𝑧, the term 𝑧𝑖 𝑧 𝑗 is symmetric and bounded by 𝑏2. Hence for 𝑏2 = 𝑜
(cid:12)
√𝑛
,
(cid:12)
(
)
the thresholding of entry
𝑖𝑗 will depend almost only on the Gaussian contribution
−
𝑊T𝑊
be the set of non-zero entries in 𝜂𝜏
. By independence
of 𝑧 and 𝑊, and since 𝑆 dependence of 𝑧 is very limited, we expect, as in our previous discussion,
(cid:0)
𝑆
|

& 𝑘4. Now consider the quadratic form

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
𝑌T𝑌
𝑑

𝑖𝑗. Let 𝑆

𝑌T𝑌

(cid:0)
] × [

𝑛Id

𝑛Id

𝑛Id

⊆ [

−

−

𝑑

(cid:12)
(cid:12)

(cid:12)
(cid:12)

]

(cid:1)

(cid:0)

(cid:1)

(cid:1)

|

𝑧T
𝑧

T
𝑌

𝑌

𝜂𝜏

𝑛Id

𝑆

[

]

−

𝑧
𝑧

𝑧T
𝑧

≈

T
𝑧𝑧

𝑆

[

]

𝑧
𝑧

k

+

k

𝑧T
𝑧

T

𝑊

𝑊

𝑛Id

𝑆

[

]

−

𝑧
𝑧

.

k

k

k
As argued in the previous paragraph C.3, 𝑧T

k

k

k

k

k

(cid:1)

(cid:0)

(cid:0)

(cid:1)
𝑊T𝑊

6 𝑂

𝑘√𝑛

(cid:1)
. On the other hand,

𝑧

k

k

(cid:0)

𝑧T
𝑧

k

k

T
𝑧𝑧

𝑆

[

]

(cid:0)

(cid:1)

𝑧
𝑧

k

k

= 1
𝑧2

k

𝑧2
𝑖 𝑧2

𝑗

2

k

𝑆
𝑖,𝑗
Õ(
)∈

k

𝑆

(cid:0)

k
𝑧
𝑧

k

k

[
]
𝑏4 & 𝑘4
𝑑

𝑛Id

−

𝑆

(cid:1)

|

2

= |
𝑧
k

k

(cid:0)
𝑏2 & 𝑑1
−

(cid:1)
𝑜
(

)𝑏2.
1

For the signal we instead have 𝑣0
𝑛Id
the top eigenvector of 𝜂
𝑛 > 𝑑1
1
) and that
−
(
k
are bounded by 𝑛 𝑜
1
)/

𝑌T𝑌
.

−
˜𝑂

𝐸

𝑏

𝑜

(

T𝜂𝜏

𝑌T𝑌

𝑆

𝑛Id

𝑣0 . 𝛽𝑛. It follows that setting 𝑏 &

[

]

−

1
)
will not achieve constant correlation with 𝑣0. Recall now that
𝑘2 6 𝑛 𝑜

1
), adversarial perturbations

(cid:0)
(cid:1)
. Hence for 𝛽
)

log 𝑑

𝑘
√𝑛

𝑑1
−

q

(

(

𝛽𝑛
𝑜

√𝑛
(cid:1)

(cid:0)
k∞
q
√𝑛 are enough to fool the algorithm.

≈

/

(

Remark C.7. While this adversarial matrix is enough to break Covariance Thresholding it also
allows an easy ﬁx. Indeed, although the top eigenvector is now almost uncorrelated with 𝑣0, the
eigenspaces spanned by two largest eigenvectors contain a vector close to 𝑣0 and a brute-force
search over such space can be performed in polynomial time. The same approach however can

26Formally, in [DM14] the authors provided a proof for a matrix obtained applying soft-thresholding. As we will see

these can easily be extended to the hard-thresholded matrix 𝜂

𝑊

(cid:16)

88

T

𝑊

−

𝑛Id

.

(cid:17)

be used to build an adversarial matrix 𝐸 such that there exist vectors 𝑧1, . . . , 𝑧𝑟 for which, with
constant probability

𝑖

𝑟

]

∈ [

T
𝑌

𝑌

𝜂𝜏

𝑛Id

−

T
& 𝑣0

𝜂𝜏

T
𝑌

𝑌

𝑛Id

𝑣0.

−

𝑧𝑖T
𝑧𝑖

k

k

𝑧𝑖
𝑧𝑖

k

k

(cid:0)

(cid:1)
The idea is to chose 𝑥1, . . . , 𝑥𝑟 to be orthonormal vectors orthogonal to 𝑢0, and 𝑧1, . . . , 𝑧𝑟 with
non-intersecting supports and the same structure as before. This latter choice of 𝐸 implies that the
space containing eigenvectors associated with large eigenvalues has now dimension at least Ω
𝑟
.
)
(
For 𝑟 > 𝜔
, brute-force search of a vector close to 𝑣0 in this space requires super-polynomial
time.

log 𝑑

(cid:1)

(cid:0)

(cid:0)

(cid:1)

C.3.1 Proving covariance thresholding fragile

Now we formally prove the theorem. First we deﬁne the adversarial matrix.

Deﬁnition C.8 (Adversarial matrix). For 𝑏 > 1, 𝑟
and 𝑣0
0, Id𝑛
0, 1
𝑁
(
)
(
ℝ𝑛 be unit vectors that are
k-sparse, the adversarial matrix is built as follows. Let 𝑥1, . . . , 𝑥𝑟
independent of 𝑊 such that for distinct 𝑖, 𝑗
in sets
supp
, let 𝑧𝑖 be the vector with support 𝑍𝑖 such that:

𝑍1, . . . , 𝑍𝑟 of cardinality

= 0. Partition the set

∈ [
. For each 𝑖

𝑑, 𝑢0 ∼

ℕ, 𝑊

𝑥𝑖 , 𝑥 𝑗

𝑣0}

] \

𝑁

∼

∈

∈

𝑣0

𝑑

{

𝑟

𝑟

i

h

×

[

]

)

𝑛

𝑑

,

{

}|

supp
𝑟

−|

∈ [

]

𝑙

∀

∈

𝑍𝑖 ,

=

𝑧𝑖
𝑙

𝑏

𝑏

(

−

Then

> 0

if

𝑤𝑙 , 𝑥𝑖
otherwise.

i

h

𝐸 :=

𝑥𝑖 𝑧𝑖T

.

𝑟
Õ𝑖
]
∈[

Notice that
Theorem C.6 follows immediately combining Theorem C.9, and Lemma C.10.

k1

→

k

𝐸

2 = 𝑏√𝑟.

Theorem C.9. Let 𝑌 be of the form 1.1 with 𝐸 constructed as in deﬁnition C.8 with 𝜔
and 𝑏 6 4√𝑛. Assume that 𝑑 > 𝑛 > log10 𝑑 and that 𝑘 6 √𝑑. Let 2√𝑛 6 𝜏 6 𝑜
1
) there exists a subset 𝑅

Then with probability at least 1

of size at least 𝑟
(cid:16)p

2𝑑−

Ω

𝑟

(

log 𝑑
(
)
𝑛 log 𝑑

1
)

(

6 𝑟 6 𝑑𝑜
as 𝑑

.

→ ∞

10 such that

(cid:17)

⊆ [

]

−

𝑅,

𝑖

∀

∈

𝑧𝑖

T

𝜂𝜏

T
𝑌

𝑌

−

2 ·

𝑛Id

𝑧𝑖 > 𝑏2

1
)

(

𝑜

𝑑1
−
𝑟

.

·

(cid:0)
Lemma C.10. Suppose the conditions of Theorem C.9 are satisﬁed and that the entries of 𝑣0 are from
. Then with probability 1
0,

and 𝑛 > 𝜔

as 𝑑

√𝑘

𝑂

10

(cid:1)

(cid:0)

(cid:1)

1
/

±

{

}

log 𝑑
(

)

𝑑−

(

)

−

𝜂𝜏(
of Lemma C.10. With probability 1

(cid:12)
(cid:12)

T
𝑣0

𝑂

(

−

𝑑−

−

+

6 𝑂

(cid:12)
(cid:12)

𝑘

10

p

𝑣0

𝑛 log 𝑑

𝑛Id
)
(cid:16)
the entries of 𝜂𝜏(
𝛽𝑛 log 𝑑
𝛽𝑛
𝑘 +
𝑘

6 𝑂

)

𝑌𝑇𝑌

!

+ r

(cid:18)

𝑂

𝛽𝑛
𝑘 +

𝑛 log 𝑑

p

𝑛 log 𝑑

.

(cid:19)

p

𝛽𝑛

.

(cid:17)
𝑛Id
)

−

are bounded by

1
𝑧𝑖

k

k

→ ∞
𝑌𝑇𝑌

89

 
Since 𝑣0 has at most 𝑘 nonzero entries,

T
𝑣0

𝑌𝑇𝑌

𝜂𝜏(

𝑛Id
)

𝑣0

−

6 𝑘

𝑌𝑇𝑌

𝜂𝜏(

· k

−

𝑛Id

)k∞

(cid:12)
(cid:12)

(cid:12)
(cid:12)

6 𝑂

𝛽𝑛

(cid:16)

+

𝑘

𝑛 log 𝑑

.

p

(cid:17)

(cid:3)

Tp prove Theorem C.9 we make use of intermediate steps C.11-C.12. Our plan is to show that
𝑛Id. So, we start

many entries of 𝑧𝑖 𝑧𝑖T survive the thresholding due to the contribution of 𝑊T𝑊
our analysis lower bounding the number of entries of 𝑊T𝑊

−

The following lemma shows that for each vector 𝑧𝑖, many entries in supp

−

𝑛Id that are above the threshold.
𝑧𝑖

supp

𝑧𝑖

will

×

ℝ consider 𝑌 sampled from model 1.1 with 𝐸 as in C.8. For some 10 6 𝑞 6 𝑑𝑜
deﬁne the set

1
)

(

(cid:8)

(cid:9)

(cid:8)

(cid:9)

survive the thresholding.

Lemma C.11. For any 𝑏, 𝑟
∈
let 𝜏 =
𝑟
]

𝑛 log 𝑞. For 𝑖

∈ [

p

𝑆𝑖 :=

𝑗, 𝑙

) ∈

(

n

Then with probability at least 1

exp

−

(cid:0)

supp

𝑧𝑖

×

(cid:8)
𝑑1
−

𝑜

(cid:9)
1
)
(

,

supp

𝑧𝑖

𝑗 ≠ 𝑙,

T
𝑌

𝑌

(cid:8)

(cid:9) (cid:12)
(cid:12)
(cid:12)

(cid:0)

−

𝑛Id

𝑗𝑙

> 𝜏

.

(cid:1)

o

(cid:1)
𝑆𝑖

|

|

>

𝑑2
1000𝑟2 𝑞10

.

Proof. Consider an oﬀ diagonal entry 𝑗𝑙 of 𝜂𝜏
Ω
Since with probability at least 1
(

2 exp

−

−

𝑌T𝑌
𝑛0.2
(cid:0)

)

𝑛Id

−
> 1

(cid:1)
−

such that 𝑗, 𝑙
2𝑑Ω
1
𝑤 𝑗, 𝑥𝑙
),

(

h

for some 𝑖

𝑧𝑖
supp
6 𝑛0.1, we get
(cid:8)

(cid:9)

∈
i

𝑟

.

]

∈ [

𝑤 𝑗, 𝑤𝑙

𝑤 𝑗 , 𝑥

𝑧𝑖
𝑙 + h

i

𝑤𝑙 , 𝑥

i + h

(cid:2)
𝑧𝑖
𝑗 +

i

𝑗 𝑧𝑖
𝑧𝑖

𝑙

ℙ

h

(cid:16)(cid:12)
(cid:12)
(cid:12)

(cid:3)
> 𝜏

(cid:12)
(cid:12)
(cid:12)

> ℙ

1
√𝑛
(cid:16)
> 1
(cid:12)
(cid:12)
10𝑞10

h

.

(cid:17)

𝑤 𝑗, 𝑤𝑙

i

(cid:12)
(cid:12)

> 2

log 𝑞

p

Ω

𝑑−

1
)

(

−

(cid:17)

For ﬁxed 𝑧𝑖 and ﬁxed row 𝑗
each other. Since 𝑟 6 𝑑𝑜

𝑌T𝑌
𝑙 such that
for each 𝑧𝑖, 𝑆𝑖 >

(cid:0)

𝑛Id
𝑗𝑙
−
𝑑2
1000𝑟2 𝑞10 .
(cid:1)

𝑑

, the

𝑤 𝑗, 𝑤𝑙

∈ [

]

h

(

1
), with probability 1
> 𝜏 is at least
𝑑

(for diﬀerent 𝑙
𝑑
100𝑟 𝑞10

exp

∈
= 1

supp

exp

−

i

−

𝑧𝑖
) are independent from
}
{
𝑜
𝑑1
−

number of diﬀerent

1
)

(

1000𝑟 𝑞10 . Hence if with probability at least 1

(cid:1)

(cid:0)

h

i

exp

𝑜

𝑑1
−

(

−

(cid:0)

,

1
)
(cid:3)
(cid:1)

The last ingredient needed for Theorem C.9 is a proof that the cross-terms in the quadratic

form 𝑧𝑖T𝜂𝜏

𝑌T𝑌

−

𝑛Id

𝑧𝑖 do not remove the contribution of the adversarial vector.

(cid:0)

(cid:1)

Lemma C.12. Let 𝑌 be sampled from model 1.1 with 𝐸 as in Deﬁnition C.8. Let 𝑆𝑖 be as in Lemma C.11
Then with probability at least 1
2 ,

𝑧𝑖

T

T

𝑊

𝑊

T

𝑥𝑖 𝑧𝑖T

𝑊

𝑧𝑖 𝑥𝑖T

𝑊

+

𝑧𝑖 > 0.

𝑆𝑖

[

]

+

Proof. For simplicity of the notation we will refer to 𝑥𝑖 , 𝑧𝑖 simply as 𝑥, 𝑧. Opening up the sum,

(cid:0)

(cid:1)

(cid:0)

(cid:1)

T
𝑧

T

𝑊

𝑊

T

T
𝑥𝑧

𝑊

T
𝑧𝑥

𝑊

+

+

(cid:0)

𝑆𝑖

[

]

(cid:1)

𝑧 = 2

𝑆𝑖
𝑗,𝑙
Õ(
)∈

90

𝑤 𝑗 , 𝑤𝑙

𝑧 𝑗 𝑧𝑙

i

+ h

𝑤 𝑗, 𝑥

i

h

𝑏2𝑧 𝑗

𝑤𝑙 , 𝑥

𝑏2𝑧𝑙

i

+ h

> 2

𝑆𝑖
𝑗,𝑙
Õ(
)∈

> 2

𝑆𝑖
𝑗,𝑙
Õ(
)∈
𝑏2𝑧 𝑗 > 0,

h

h

h

𝑤 𝑗, 𝑤𝑙

𝑧 𝑗 𝑧𝑙

i

𝑤 𝑗,

Id

T
𝑥𝑥

−

𝑧 𝑗𝑧𝑙 ,

𝑤𝑙

i

(cid:0)
𝑤𝑙, 𝑥

i

(cid:1)

𝑏2𝑧𝑙 > 0. So it is enough to prove that

using the fact that by construction

𝑤 𝑗 , 𝑥

h

i

𝑑′

ℙ

𝑗,𝑙

Õ𝑗=1 Õ(
𝑆𝑖
©

)∈

𝑤 𝑗,

Id

h

−

T
𝑥𝑥

𝑤𝑙

i

𝑧 𝑗 𝑧𝑙 > 0

> 1
2

.

(cid:0)

(cid:1)

Let

and

𝑎 𝑗𝑙 =

𝑤 𝑗,

h

«
Id

−

T
𝑥𝑥

𝑤𝑙

i

𝑧 𝑗 𝑧𝑙 =

(cid:0)

(cid:1)

𝑤 𝑗, 𝑤𝑙

i − h

h

(cid:0)

ª
®
¬
𝑤 𝑗, 𝑥
ih

𝑤𝑙, 𝑥

𝑧 𝑗 𝑧𝑙

i

·

(cid:1)

𝑝 𝑗𝑙 =
=

(cid:0)
h

𝑤 𝑗 , 𝑥

h
𝑤 𝑗, 𝑥

i

𝑧𝑙
i
𝑏2𝑧 𝑗

+ h

+ h

𝑤𝑙 , 𝑥

i
𝑤𝑙 , 𝑥

𝑧 𝑗
+ h
𝑏2𝑧𝑙

𝑤 𝑗, 𝑥

𝑤𝑖, 𝑥

ih
𝑤 𝑗, 𝑥

i +
𝑤𝑖, 𝑥

+ h

ih

𝑧 𝑗 𝑧𝑙

𝑧 𝑗𝑧𝑙
(cid:1)

i

𝑧 𝑗 𝑧𝑙
𝑏4 .

·

+

i

𝑎 𝑗𝑙

𝑝 𝑗𝑙

> 𝑏2𝜏. Also notice that 𝑝 𝑗𝑙 > 0 and that with

Notice that
probability at least 1

) ∈

𝑗, 𝑙

(

Since

supp

𝑧

{

|

}|

𝑆𝑖 if and only if 𝑗 ≠ 𝑙 and
Ω
(

2 exp

𝑛0.2

−

)

+
, 𝑝 𝑗𝑙 < 𝑏2𝜏.
(cid:12)
(cid:12)

−
= 𝑑′, without loss of generality assume supp

(cid:12)
(cid:12)

(cid:2)

(cid:3)

𝑧

{

}

. For 𝑞

𝑑′]

𝑑′ −

1
]

∈ [

deﬁne

𝑑′−
1

𝑇∗𝑞 :=

Õ𝑗=𝑞 Õ𝑗<𝑙6𝑑′ s.t.
>𝑏2𝜏
𝑎 𝑗𝑙
|

|

Id

−

h

(cid:0)

T
𝑥𝑥

𝑤 𝑗 ,

Id

T
𝑥𝑥

−

𝑧 𝑗 𝑧𝑙 =

𝑤𝑙

i

(cid:1)

(cid:0)

(cid:1)

and

𝑑′−
1

𝑇𝑞 :=

Let 𝑇𝑑′

= 𝑇𝑑′

= 0. For 𝑗

Õ𝑗=𝑞 Õ𝑗<𝑙6𝑑′ s.t.
>𝑏2𝜏
𝑎 𝑗𝑙+

𝑝 𝑗𝑙

|

1
]

|
𝑑′ −
∈ [
𝑇∗𝑗
𝑇∗𝑗 −
1
+

Id

−

h

(cid:0)

T
𝑥𝑥

𝑤 𝑗 ,

Id

T
𝑥𝑥

−

𝑧 𝑗 𝑧𝑙 =

𝑤𝑙

i

(cid:1)

(cid:0)

(cid:1)

consider

=

h

(cid:0)

Õ𝑗<𝑙6𝑑′ s.t.
>𝑏2𝜏
𝑎 𝑗𝑙
|
|

Id

−

T
𝑥𝑥

𝑤 𝑗 ,

Id

T
𝑥𝑥

−

𝑧 𝑗 𝑧𝑙 .

𝑤𝑙

i

(cid:1)

(cid:0)

(cid:1)

=

[
𝑑′−
1

𝑎 𝑗𝑙

𝑎 𝑗𝑙 .

Õ𝑗=𝑞 Õ𝑗<𝑙6𝑑′ s.t.
>𝑏2𝜏
𝑎 𝑗𝑙
|

|

𝑑′−
1

Õ𝑗=𝑞 Õ𝑗<𝑙6𝑑′ s.t.
>𝑏2𝜏
𝑎 𝑗𝑙+

𝑝 𝑗𝑙

|

|

𝑥𝑥T

−

Id

𝑥𝑥T

𝑤 𝑗 does not inﬂuence on the condition

𝑤 𝑗 is symmetric around zero and independent from all 𝑧 𝑗 and all 𝑤𝑙 for 𝑙 > 𝑤 𝑗. Moreover,
Id
−
> 𝑏2𝜏. It follows that the condi-
the sign of
(cid:0)
(cid:1)
tional disribution of 𝑇∗𝑗 given 𝑧 𝑗, 𝑧𝑙, 𝑤𝑙 for 𝑙 > 𝑗 is symmetric around 𝑇∗𝑗
1 and thus by induction
+
> 0
𝑇∗1 is symmetric around zero. It remains to show that ℙ
, which is true since if
> 𝑏2𝜏,
𝑇∗1
(cid:3)
and any 𝑎 𝑗𝑙 < 0 such that

𝑇1 > 0
)
(
> 0, then any 𝑎 𝑗𝑙 > 0 such that

> 𝑇1. Indeed, if 𝑇∗1
𝑝 𝑗𝑙

𝑇∗1
> 𝑏2𝜏 satisﬁes
(cid:1)
(cid:0)

> 0, then 𝑇∗1

> 𝑏2𝜏 satisﬁes

> ℙ
𝑎 𝑗𝑙

> 𝑏2𝜏.

𝑝 𝑗𝑙

𝑎 𝑗𝑙

𝑎 𝑗𝑙

𝑎 𝑗𝑙

𝑎 𝑗𝑙

+

(cid:1)

(cid:0)

|

|

|

|

|

|

|

+

|

|

|

We are now ready to prove Theorem C.9.

91

 
Proof of Theorem C.9. Let 10 6 𝑞 6 𝑑𝑜

(

𝑛 log 𝑞. By construction of 𝑧𝑖,

) so that 𝜏 =
1
T

𝑊

𝑧𝑖T
T
𝑧𝑖T
𝐸
(cid:0)
𝑧𝑖T
(cid:0)

T
𝑢𝑣
T
𝑢𝑣
T
𝐸

𝐸

𝑆𝑖

𝑆𝑖

𝑆𝑖

[

[

[

(cid:1)

(cid:1)

]

]

]

𝑧𝑖 = 0
p
𝑧𝑖 = 0
𝑧𝑖 = 𝑧𝑖T

𝑧𝑖 𝑧𝑖T

𝑧𝑖.

𝑆𝑖

[

]

With probability 1

𝑑Ω

(

−

1
) sum over diagonal entries is bounded by:

(cid:0)

(cid:0)

(cid:1)

(cid:1)

2

𝑧𝑖
𝑗

𝑤 𝑗

k

2

k

−

𝑛

6 𝑂

𝑏2𝑑

𝑛 log 𝑑

6 𝑏2𝑑1.5
+

𝑜

1
) .

(

Õ𝑗 (cid:16)

(cid:17)

(cid:0)
Notice that for diﬀerent 𝑖, 𝑚
]
∈ [
𝑧𝑚 𝑥𝑚T𝑊
𝑊T𝑥𝑚 𝑧𝑚T
T
𝑆𝑚
+
]
)
0.1𝑟 for at least 𝑟
2−

𝑊T𝑊
𝑧𝑚
(
probability at least 1

+

𝑟

[

(cid:0)

(cid:1)

(cid:16)
the events

p
T
𝑧𝑖

(cid:17)
𝑊T𝑊

𝑧𝑖 > 0 and
𝑧𝑚 > 0 are independend. Hence, by Lemma C.12 with
(cid:0)
10 diﬀerent 𝑖

𝑊T𝑥𝑖 𝑧𝑖T

𝑧𝑖 𝑥𝑖T𝑊

𝑆𝑖

+

+

𝑟

]

[

(cid:1)

(cid:0)

(cid:1)

,

/

∈ [

]

−
T
𝑧𝑖

(cid:0)

(cid:1)
𝑛Id

𝑆𝑖
(cid:1)
]
𝑆𝑖

]

[

[

(cid:1)

(cid:1)

T
𝑌
T

𝜂𝜏
𝑧𝑖
=
(cid:1)
(cid:0)
𝑧𝑖
>
(cid:0)
=𝑏4
(cid:0)

T

(cid:1)
𝑆𝑖
(cid:1)

|
𝑑2

𝑌
−
𝑧𝑖 𝑧𝑖T
𝑧𝑖 𝑧𝑖T
(cid:0)
.
(cid:0)
|

𝑧𝑖
𝑧𝑖
𝑧𝑖

𝑧𝑖T

T

𝑊

𝑊

T

𝑥𝑧𝑖T

𝑊

𝑧𝑖 𝑥𝑖T

𝑊

+

𝑧𝑖

𝑆𝑖

[

]

+

+

(cid:0)

(cid:1)

By Lemma C.11

>

𝑆𝑖

|

|

1000𝑟2 𝑞10 . The theorem follows observing that

𝑧𝑖

2 6 𝑑𝑏2
𝑟 .

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:3)

D Covariance Thresholding doesn’t work with large signal and small

sample size

We show here a formal argument that proves Covariance Thresholding doesn’t work for 𝑛 6 𝑘2.
The lower bound 6.4 shows that the assumption 𝑘2 > 𝑑1
1
),
−
the conjecture Conjecture 3.13 implies that it is unlikely that there exists a polynomial time algo-
rithm with asymptotically better guarantees than Diagonal Thresholding. So in this section we
assume that 𝑘2 > 𝑑1
(since
−
otherwise Covariance Thresholding doesn’t have asymptotically better guarantees than Diagonal
Thresholding).

) and that the thresholding parameter 𝜏 satisﬁes 𝜏 6 𝑜
1

) is important, since if 𝑘2 6 𝑑1
1
−

𝑛 log 𝑑

(
p

Ω

)

𝑜

𝑜

(

(

(

Notice that the assumptions 𝑛 > 𝑘2 and 𝑘2 > 𝑑1
−

𝑜

) imply 𝑛 > 𝑑1
1
−

(

𝑜

1
).

(

Theorem D.1. Let 𝑥𝑖 be a unit vector in the direction of the 𝑖-th row of 𝑊. Assume that 𝑘 6 √𝑑,
𝑑Ω
. Also assume that the entries

and √𝑛 6 𝜏 6 𝑜

) 6 𝑛 6 𝑑1
1
−

𝑛 log 𝑑

as 𝑑

Ω

(

(

(

of 𝑣0 are from

0,

{

±

. Then with probability 1

1
) ·

𝑘
√𝑛

), 𝛽 6 𝑑𝑜
1
√𝑘
1
/

}

𝜁𝜏(
Theorem D.1 immediately follows from Lemma D.2 and Lemma D.3.

−

(

(

𝑥𝑖

T
)

𝑌𝑇𝑌

𝑜

1
) .

→ ∞

(cid:16)p
𝑂
(
−
𝑛Id
)

(cid:17)

10

𝑑−

)
𝑥𝑖 > 𝑑1
−

Lemma D.2. Let 𝑆0 be a set of pairs
𝑙 ∉ supp

) ∈ [
, and 𝑗 ≠ 𝑙. Then with probability 1
−
𝑛Id

𝑌𝑇𝑌

𝑣0}

𝑗, 𝑙

𝑥𝑖

𝑑

{

(

(

T
)

𝜁𝜏(

(

−

𝑆0]

)[

2 such that any
]
𝑂

20

𝑑−

)
𝑥𝑖 > 𝑑1
−

𝑜

1
) .

(

𝑗, 𝑙

(

) ∈

𝑆0 satisﬁes 𝑗 ∉ supp

,

𝑣0}

{

92

Proof. Without loss of generality assume that 𝑖 = 1 and denote 𝑥 = 𝑥1. Let’s denote the squared
norm of the ﬁrst row of 𝑊 by 𝑠2. Notice that
𝑧𝑖𝑗, where 𝑧𝑖𝑗 is independent of 𝑥.
𝑤𝑖, 𝑤 𝑗
Hence

= 𝑤1𝑖𝑤1𝑗

+

i

h

𝑥𝑇 𝜁𝜏(

𝑌𝑇𝑌

−

𝑛Id

)[

𝑥 =

𝑆0]

𝑆0
𝑖𝑗
Õ(
)∈

= 1
𝑠2

−
log 𝑑
)
(
𝑥 = 1
𝑠2

Notice that with probability 1
𝜏 > √𝑛, while 𝑤1𝑖𝑤1𝑗 < 𝑂

𝑥𝑇 𝜁𝜏(

𝑌𝑇𝑌

−

𝑛Id

)[

𝑆0]

1

[|h

𝑤𝑖 ,𝑤 𝑗i|

>𝜏

h

]

(cid:0)

𝑤𝑖, 𝑤 𝑗

sign

𝑤𝑖 , 𝑤 𝑗

h

i −

𝜏

𝑥𝑖 𝑥 𝑗

i

(cid:1)

1

[|

𝑤1𝑖𝑤1𝑗+

𝑧𝑖𝑗 |

>𝜏

]

(cid:0)

𝑆0
𝑖𝑗
Õ(
)∈

𝑤1𝑖𝑤1𝑗

30

𝑂
𝑑−
(
with 1

for every survived
𝑂

30

. Hence
)

𝑑−

(

)
−

(cid:0)
𝑧𝑖𝑗

+

(cid:1)
𝑤𝑖 , 𝑤 𝑗

i

𝜏

𝑤1𝑖𝑤1𝑗 .

sign

−

h

𝑖, 𝑗

, sign
)

(

(

(cid:1)
= sign

(cid:0)
𝑧𝑖𝑗

)

(cid:1)
𝑤𝑖 , 𝑤 𝑗

h

(cid:0)

, since

i

(cid:1)

𝑆0
𝑖𝑗
Õ(
)∈
100 log 𝑑
𝑠2

>

−

1
[|

𝑤1𝑖 𝑤1𝑗+

𝑧𝑖𝑗 |

>𝜏

]

(cid:16)

𝑤1𝑖𝑤1𝑗

𝑧

+

𝑖𝑗

(

)∈

𝑆0 −

sign

𝑧𝑖𝑗

𝜏

𝑤1𝑖𝑤1𝑗

(cid:0)
sign

(cid:17)

(cid:1)
𝑧𝑖𝑗

𝜏

1
[|

sign

𝑧𝑖𝑗 −

𝑧𝑖𝑗

𝜏

|

)

(

<100 log 𝑑

]

𝑆0
𝑖𝑗
Õ(
)∈

𝑧𝑖𝑗

−

1
𝑠2

+

1
[|

>𝜏

𝑧𝑖𝑗 |

𝑆0
𝑖𝑗
Õ(
)∈

100 log 𝑑

+

]

𝑤1𝑖𝑤1𝑗

𝑧𝑖𝑗

+

(cid:0)
𝜏

(cid:12)
(cid:1)
(cid:12)
𝑤1𝑖𝑤1𝑗.

sign

𝑧𝑖𝑗

(cid:12)
(cid:12)
−

𝜏

−

𝑧𝑖𝑗

𝑧𝑖𝑗

Notice that ℙ
sign
independent, so the number of 𝑧𝑖𝑗′ such that
(cid:3)
(cid:0)
probability at least 1
2−
most 10𝑝𝑑2 such 𝑧𝑖𝑗 for all

< 100 log 𝑑

𝑖, 𝑗

(cid:2)(cid:12)
(cid:12)

−

(cid:12)
(cid:12)

(cid:1)

(cid:12)
(cid:12)

1000 log 𝑑
√𝑛

<
𝑧𝑖𝑗′ −

sign

(cid:1)

(cid:1)

(cid:0)

(cid:0)
=: 𝑝. If we ﬁx 𝑗′, then for diﬀerent 𝑖, 𝑧𝑖𝑗′ are
𝑧𝑖𝑗′
< 100 log 𝑑 is bounded by 10𝑝𝑑 with
𝑝𝑑 there are at
𝑑
−
, 𝑠2 = Θ
𝑛
𝑛
exp
.
)
𝑂

𝜏

(cid:12)
(cid:12)

(

(cid:1)

(cid:0)

2−
·
Ω
[−
(
𝑑 log2 𝑑
√𝑛

(

)]
.

)

−

𝑝𝑑. Hence by union bound with high probability 1

𝑆0. Notice that with probability at least 1

−
the contribution of the ﬁrst term is

20

𝑑−

(

) ∈
𝑂

−

(

)

So with probability at least 1

Now, by Bernstein inequality G.7, for any ﬁxed 𝑗

1
[|

>𝜏

𝑧𝑖𝑗 |

100 log 𝑑

+

𝑧𝑖𝑗

−

]

(cid:0)

sign

𝑧𝑖𝑗

𝜏

𝑤1𝑖𝑤1𝑗

(cid:0)

(cid:1)

(cid:1)

6 𝑂

𝑛𝑑 log 𝑑

(cid:16)p

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

with probability at least 1

𝑂

30

𝑑−

−

(

.
)

Furthermore, since 𝑧𝑖𝑗, 𝑤1𝑖 and 𝑤1𝑗 are independent, for any 𝑗 such that 𝑤2
1𝑗

> 1

(cid:12)
Õ𝑖
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
𝑤1

k

2

k

Õ𝑖

1
[|

>𝜏

𝑧𝑖𝑗 |

100 log 𝑑

+

]

𝑤2

1𝑖𝑤2

1𝑗

>

1
𝑤1

k

2

k

Õ𝑖

1

𝑧𝑖𝑗 |

[|

>2𝜏

]

𝑤2
1𝑖

> exp

𝑂

𝜏2
(cid:1)𝑛
(cid:0)

(cid:19)

−

(cid:18)

> 𝑑−

𝑜

1
)

(

𝑜

𝑑1
−

1
)

(

. Hence

with probability at least 1

exp

−

−

1
𝑤1

(cid:0)

k

2

k

and therefore, with probability 1

Õ𝑖𝑗
𝑂

−

20

𝑑−

)

(

(cid:1)
𝑧𝑖𝑗 |

1
[|

>𝜏

100 log 𝑑

+

]

𝑤2

1𝑖𝑤2

1𝑗

> 𝑑1
−

𝑜

1
) ,

(

𝑥𝑇 𝜁𝜏(

𝑌𝑇𝑌

−

𝑛Id

)[

𝑆0]

𝑥 > 𝑑1
−

𝑜

1
)

(

𝑑

·

−

𝑂

log2 𝑑
(
√𝑛

)

−

𝑂

𝑛𝑑 log 𝑑

> 𝑑1
−

𝑜

1
) .

(

(cid:16)p

(cid:17)

(cid:3)

93

Lemma D.3. Let 𝑆1 =

𝑑

[

2

]

\

𝑆0. Then with probability 1

Proof. With probability 1

(cid:12)
𝑂
(cid:12)

(

−

20

𝑑−

)

𝑂

𝛽𝑛
𝑘 +

𝑛 log 𝑑

p

𝑥𝑖

T
)

(

𝜁𝜏(

𝑥𝑖

𝑌𝑇𝑌

−

)[

𝑛Id

𝑆1]
the entries of 𝜁𝜏(
𝛽𝑛 log 𝑑
𝑘

6 𝑑𝑜

!

+ r

𝑂

20

𝑑−

−
(
2
6 𝑑1
+
/

𝑜

,
)
1
)

(

(cid:12)
𝑌𝑇𝑌
(cid:12)

𝑛Id

)[

−

√𝑛 .

·
𝑆1]

are bounded by

1
)

(

√𝑛

(cid:16)

+

4√𝑛

6 𝑑𝑜

1
)

(

(cid:17)

√𝑛 .

·

log 𝑑
𝑑

(cid:18)q

(cid:19)

1
𝑑

·

6 𝑑1
2
+
/

𝑜

1
)

(

√𝑛 .

·

20

𝑑−

𝑂
(cid:1)

(

−

)

(cid:3)

Number of nonzero entries in 𝜁𝜏(

𝑆1]
entries of 𝑥𝑖 are bounded by 𝑂

𝑛Id

)[

−

𝑌𝑇𝑌

probability 1

20

𝑑−

𝑂

(

−

)

is at most 𝑑

𝑘2

2𝑘𝑑 6 𝑂

2
𝑑3
/

. With

+

+
. Hence with probability 1

(cid:0)

𝑥𝑖

T
)

𝑌𝑇𝑌

𝜁𝜏(

𝑛Id

)[

𝑆1]

−

𝑥𝑖

6 𝑑𝑜

1
)√𝑛

(

2
𝑑3
/

·

(cid:12)
(cid:12)

(
(cid:12)
(cid:12)

To conclude that under assumptions of Theorem D.1 Covariance Thresholding doesn’t work,

we need the following lemma:

Lemma D.4. Assume that the entries of 𝑣0 are from
probability 1

𝑂

10

𝑑−

−

(

)

T
𝑣0

𝜁𝜏(

𝑌𝑇𝑌

𝑛Id
)

−

𝑣0

6 𝑂

𝑘

𝑛 log 𝑑

𝛽𝑛

.

+

(cid:17)
The proof of Lemma D.4 is the same as the proof of Lemma C.10.

p

(cid:16)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

0,

{

√𝑘
1
/

}

±

and 𝑛 > 𝜔

log 𝑑
(

)

as 𝑑

→ ∞

. Then with

E Statistical Lower bound for Recovery

In this Section we provide an information theoretic lower bound for recovery of the sparse vector,
𝑘 no estimator can achieve correlation 0.9. We remark that the
we will show that for 𝛽
bound was previously known, we include it for completeness. Formally, we prove the following
statement.

𝑛 log 𝑑

≪

𝑘

Theorem E.1. Given an 𝑛-by-𝑑 matrix 𝑌 of the form 𝑌 = 𝑊
a standard Gaussian vector 𝑢0 ∼
𝑣 : ℝ𝑛
ˆ

𝛽𝑢0𝑣0
, and a Gaussian matrix 𝑊
p
)
𝑑 such that

ℝ𝑑, there exists a 𝑘-sparse vector 𝑣

0, Id𝑛
(

√𝑘
1
/

∈ {

→

0,

𝑁

+

±

}

×

𝑑

∼

T for a 𝑘-sparse unit vector 𝑣0 ∈

ℝ𝑑,
𝑑. For any estimator

𝑁

𝑛

×

0, 1
)
(

𝔼

1

−

𝑣
h ˆ

𝑌
(

)

, 𝑣

i

2 > 0.2

5𝑛𝛽
𝑘 log 𝑑
𝑘

.

−

Observe how the theorem compare with the distinguishing lower bound 6.5. While the top
eigenvector of the covariance matrix can distinguish the Gaussian distribution from the planted
distribution if 𝛽 &
𝑛 , Theorem E.1 shows that in any settings it is also required to have 𝛽 &
𝑛 log 𝑑
𝑘
𝑘 simple polynomial
time algorithms can distinguish, but it is information theoretically impossible to have an estimator
achieving correlation 0.9.

𝑘 in order to obtain correlation 0.9. In other words, for 𝑛 . 𝑘2

𝑑 log2 𝑑

q

𝑑

94

 
A standard technique to prove such result is bounding the minimax risk. This can be done
observing that, given an appropriate well-separated set of candidate vectors, any estimator will
erroneously guess which is the true planted vector with large enough probability. We introduce
some standard notions that will be used in the proof, the proof itself can be found at the end of the
section. We follow closely [Wai19].

ℝ𝑑 let 𝑑𝐻
Consider the following notation. For vectors 𝑣1, 𝑣2 ∈
𝑑
𝑑𝐻
0,
𝑏, 𝑡, 𝑙
∈ {
. Given a random variable 𝑥, denote with Σ
(

ℝ𝑑 , 0 < 𝑡 6 𝑑, deﬁne
= 𝑙

∈
𝑦, 𝑏

𝑑𝐻

ℬ(

:=

±

𝑦

{

}

}

}

)

𝑡

|

𝑥
(cid:0)

)

distance. For 𝑏
𝑑
𝑡
0,
{
|
±
support.

(cid:1)

𝑣1, 𝑣2)
(
6 𝑙
𝑦, 𝑏

denote their Hamming
𝑏, 𝑡, 𝑙

:=
its covariance and with

𝑦
∈
its

and

𝒮(

}

)

{
𝒳

(cid:0)

(cid:1)

Deﬁnition E.2. Let 𝜌 be a metric. A 𝛿-packing of a set 𝑇 with respect to a metric 𝜌 is a subset
, 𝑖 ≠ 𝑗. A 𝛿-covering of a set 𝑇 with respect
𝑧1, . . . , 𝑧𝑀
(
) ⊂
]
𝑀
to 𝜌 is a subset
< 𝛿.
∈ [

𝑇 such that 𝜌
𝑧1, . . . , 𝑧𝑀

> 𝛿 for all 𝑖, 𝑗
𝑇,
𝑧

−
𝑇 such that
(cid:1)

∈ [
𝑗
∃

with 𝜌

(cid:0)
) ⊂

𝑀

𝑧 𝑗

𝑧 𝑗

𝑧𝑖

−

∀

∈

𝑧

]

(

The following Lemma lower bounds the size of the largest √𝛿 packing.

(cid:1)

(cid:0)

Lemma E.3. Let 𝛿
𝑑
𝛿
𝑒
𝑘

at least

1
−

𝛿

𝑘

)

(

. There exists a √𝛿-packing of
0, 1
)

∈ (
𝛿𝑘
.

0, 1
/

𝒮(

√𝑘, 𝑘

)

with respect to

k·k2 of cardinality

(cid:1)

(cid:1)
ℬ(

(cid:0)
(cid:0)
Proof. Since Vol

0, 1
/
has cardinality at least
√𝑘, 𝑘
0, 1
number of
/
𝑘.
𝑙
𝑦1 −

𝒮(
>

𝑦2

/

(cid:16)

2

√𝑘, 𝛿𝑘

𝑑
𝑘

2𝑘

/

6

𝑑
𝛿𝑘
2𝑘 >
(cid:0)

)

(cid:17)
𝑑
𝛿𝑘

2𝑘, a 𝛿𝑘-covering of

(cid:1)

𝑑
𝑘

𝑘

𝛿

1
−

(

)

𝛿𝑘

𝛿
𝑒

√𝑘, 𝑘
0, 1
/

with respect to 𝑑𝐻

𝒮(

(·)
)
. This is a lower bound for the 𝛿𝑘-packing
> 𝑙 implies
𝑑, 𝑑𝐻
(cid:3)

𝑦1 −

1
/

√𝑘

𝑦2

0,

±

}

. The lemma follows since for 𝑦1, 𝑦2 ∈ {
(cid:1)
(cid:0)
)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:0)
The main tool used in the Lemma will be the well-known Fano’s Inequality.

p

(cid:1)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Deﬁnition E.4. Let 𝑧, 𝑗 be random variables. Then their mutual information is

where for a r.v. 𝑥 with density function 𝑝

𝑥

𝑥

, 𝐻
)

(

)

(

𝑧, 𝑗

𝐼

(

)

:= 𝐻

𝑧

(

) +

𝐻

(
:=

𝑗

−

) −
𝔼

𝑧, 𝑗

𝐻

(

)

log 𝑝

𝑥

(

)

is the Shannon Entropy.

Lemma E.5. Let 𝑧, 𝑗 be random variables. Suppose 𝑗 is uniformly distributed over
on 𝑗 = 𝐽, for 𝑗
. Then
)

has a Gaussian distribution with covariance Σ
(

∈ [

𝑀

𝑧

]

𝐽

|

(cid:2)

(cid:3)

and that 𝑧 conditioned

𝑀

[

]

𝑧, 𝑗

𝐼

(

)

6 1
2

log det Σ
(

𝑧

) −

1
𝑀

©

Lemma E.6 (Fano’s Inequality). Let 𝜌 be a metric and Φ :
a family of distributions
between distributions
If for all 𝑖, 𝑗

«
and a functional 𝑣 :

with 𝑖 ≠ 𝑗, 𝜌

𝐷1, . . . , 𝐷𝑀

𝒟 →

𝑀

𝒟

𝑣

{

} ⊆ 𝒟
, 𝑣
𝐷𝑖
(
)

(

(

))

∈ [

]

log det Σ
(

𝑧

𝐽

]

|

𝑀
Õ𝐽
∈[
0,

)
ª
®
an increasing function. Given
¬
Ω, consider a 𝑀-ary hypothesis testing problem
𝐷𝑖.

∞) → [

𝑥 = 𝑖

∞)

𝑀

0,

𝑧

[

where 𝑥 is uniformly distributed over
> 2𝛿, then for any estimator
𝐷𝑗

and
Ω:

(

|

]
[
𝒵 →

) ∼

The minimax risk is deﬁned as 𝔪

𝑣 :
ˆ
log 2

Φ

𝔼
𝐷𝑖

𝜌

𝑣, 𝑣
( ˆ

(

𝐷𝑖

))

> Φ

𝛿

(

)

1

−

𝑧, 𝑥

𝐼

(

) +
log 𝑀

sup
𝑀
𝑖

∈[

]

(cid:0)

(𝒟)

(cid:2)
𝑣

(cid:0)

, Φ

𝜌

◦

(cid:1) (cid:3)
= inf
𝑣
ˆ

(cid:1)

sup
𝑀
𝑖

∈[

]

95

(cid:18)
𝔼𝐷𝑖

Φ

𝜌

𝑣, 𝑣
( ˆ

(

𝐷𝑖

))

(cid:2)

(cid:0)

(cid:19)

.

(cid:1) (cid:3)

 
We are now ready to prove the bound, we provide a slightly more general version which

immediately implies Theorem E.1.

𝑣 : ℝ𝑛
ˆ

𝑑

×

→

ℝ𝑑 and 𝛿

0, 1
4

∈

, there exists a 𝑘-sparse vector 𝑣

∈

Theorem E.7. For any estimator
𝑑 such that

√𝑘

0,

1
/

±

{

}

𝔼

𝑣
k ˆ

𝑌
(

) −

𝑣

k

2
2 > 𝛿

1

−

𝑘

where 𝑌 =

𝛽𝑢𝑣T

𝑊 for 𝑊

+

𝑁

0, 1
)
(

∼

𝑛

×

𝑑 and 𝑢

(cid:0)
∼

(cid:1)
(cid:0)
2 log 2

1
(

−
𝑁

𝑛𝛽

+
log 𝑑

4𝛿

𝑘 +

)
.
0, Id𝑛
(
)
Δ1, . . . , Δ𝑀

4𝛿 log 4𝛿
𝑒

!

(cid:1)

p

)

(

𝑘

(cid:1)

4𝛿

4𝛿𝑘

1
−

4𝛿
𝑒

ℝ𝑑

Proof. By Lemma E.3 there exists a 2√𝛿-packing
𝑑
𝑘

with size 𝑀 >
of
𝑑 the set of orthonormal matrices corresponding to a permu-

. Denote by

0, 1
/

√𝑘, 𝑘

tation of the columns along with ﬂip of signs. Notice that for 𝑈
(cid:0)
and 𝑈Δ𝑖
a ﬁxed 𝑈
𝑀
from
E.5,

Δ𝑖
k
𝑈
, deﬁne the corresponding family of vectors 𝑣 𝑗
)
(
) ∈

k2
. For
𝑑 be the random variable generated picking 𝑗 uniformly at random
. By Lemma
)

(cid:0)
(cid:1)
√𝑘, 𝑘
0, 1
∈ 𝒮(
/
𝑈
, let 𝑌
×
(
∈ 𝒰
and then sampling 𝑌

T. For 𝑛 = 1, we denote it by 𝑦
)

k2 =
Δ𝑗
−
= 𝑈Δ𝑗 for 𝑗
)

𝑈Δ𝑗
−
𝑀
∈ [

𝑈Δ𝑖

= 𝑊

𝛽𝑢𝑣 𝑗

𝒰 ⊆

𝑈
(

𝑈
(

𝑈
(

∈ 𝒰

ℝ𝑛

𝒮(

+

}

{

k

×

[

]

]

)

)

,

log det Σ

𝑦

𝑈
(

)

−

1
𝑀

𝔼
𝑈

log det Σ
(

𝑦

𝑈
(

)|

𝐽

p

𝔼
𝑈






𝔼

𝑈






(cid:20)

𝔼
𝑈

log det Σ

log det Σ

(cid:0)

(cid:0)

𝑦

𝑈
(

)

𝑦

𝑈
(

)

(cid:1)

(cid:1)

(cid:1)

log det 𝔼
𝑈

(cid:20)

𝑑 log

1

(cid:20)

(cid:18)

+

(cid:0)

Σ

𝑦

𝑈
(

(cid:1)
log

𝛽
𝑑

(cid:0)

(cid:19)

−

𝑀
Õ𝐽
∈[

]

1

−

𝑀

|𝒰 | Õ𝐽
,𝑈
∈[

𝑀

]

log

𝛽

1
(

+

−

)

(cid:21)

)

−

log

𝛽

1
(

+

)

(cid:21)

𝛽

1

+

(cid:0)

(cid:21)

(cid:1)

𝔼
𝑈

𝐼

𝑦

𝑈
(

; 𝐽
)

)

(

(cid:2)

(cid:3)

6 1
2

=

1
2

=

1
2
6 1
2

=

6

1
2
𝛽
2

log det Σ
(

∈𝒰

)





𝑈
𝑦

(

𝐽

)|

)







using concavity of the log-determinant and the matrix-determinant Lemma. Applying Fano’s
Inequality E.6, for any estimator

ℝ𝑑

𝑑

𝑣 : ℝ𝑛
ˆ

×

sup
𝑀
𝑗

∈[

]

𝑣 𝑗

𝑣
ˆ

−

𝔼
𝐷𝑗

h(cid:13)
(cid:13)

2
2

i

(cid:13)
(cid:13)

→
> 𝛿

(cid:18)

> 𝛿

1

> 𝛿

1

(cid:18)

> 𝛿

1

𝔼𝑈

𝐼

𝑌
(

[

; 𝐽
𝑈
)
(
log 𝑀

)] +

1

−

log 2

(cid:19)
log 2

!

𝑦

; 𝐽
𝑈
(
)
)
log 𝑀

+

(cid:3)

𝑛 𝔼𝑈

𝐼

(

−

−

(cid:2)
2 log 2

𝑛𝛽

+

2 log 𝑀

−

𝑘

1

(

−

4𝛿

)

4𝛿 log 4𝛿
𝑒

!

𝑛𝛽

2 log 2

(cid:19)

+
log 𝑑

𝑘 +
𝑈
(

where we used the fact that by independence of the rows of 𝑌

(cid:0)

96

, 𝐼
)

𝑌
(

𝑈
(

; 𝐽
(cid:1)
)
)

6 𝑛𝐼

𝑦

𝑈
(

; 𝐽
)

.
)

(

(cid:3)

 
 
 
F Existence of the Adversarial Distribution of Model 6.6

𝑥

Let ℝ
[
distribution we will need the following theorem.

]

6𝑠 be the space of one variable polynomials of degree at most 𝑠. To construct the desired

Theorem F.1 (Theorem 1.26 in [Sch17]). Suppose that 𝑚1, . . . , 𝑚𝑠
a linear functional

ℝ such that

= 1 and

𝑥

: ℝ
[

ℒ

6𝑠

]

→

1
ℒ(
)
= 𝑚𝑟 ,

𝑥𝑟

)

ℒ(

1 6 𝑟 6 𝑠 .

ℝ and 𝐾

∈

⊆

ℝ is compact. Consider

𝑝

ℒ(

> 0 for every 𝑝
ℝ
If
𝑥
6𝑠 that is nonnegative on 𝐾, then there exists a ﬁnitely supported probability
]
[
distribution 𝜂 such that supp
𝜂
(

𝜂 𝑥𝑟 = 𝑚𝑟 for 1 6 𝑟 6 𝑠 .
∼

𝐾 and 𝔼𝑥

) ⊆

∈

)

Let’s take the maximal even number 𝑠 such that 𝛿𝜆𝑠 6 2−

10𝑠 . We will show that there exists
a distribution with compact support such that with probability 𝛿 it takes values
𝜆 and its ﬁrst
𝑠 moments coincide with the ﬁrst 𝑠 Gaussian moments. Such a distribution is a mixture 𝜂 =
𝜆 with probability 1
2 each, and 𝜂0 has particular moments
1
𝜂0 +
(
)
−
up to 𝑠.

𝛿𝜂1, where 𝜂1 takes values

±

±

𝛿

Proposition F.2. Suppose that 𝑠 > 2 is even, 0 < 𝛿 < 1, 𝜆 > 2 and 𝛿𝜆𝑠 6 2−
supported probability distribution 𝜂0 such that supp
𝜂0) ⊆ [−
(
where

10√𝑠 ln 𝑠, 10√𝑠 ln 𝑠

10𝑠 . Then there exists a ﬁnitely
𝜂0 𝑥𝑟 = 𝑀𝑟 ,
∼

and 𝔼𝑥

]

if 𝑟 is odd,
if 0 6 𝑟 6 𝑠 and 𝑟 is even.

𝛿𝜆𝑟

,

𝑀𝑟 =

0,
1

𝛿

1
−

(

𝑟

(
Proof. Consider a linear functional
1 6 𝑟 6 𝑠. We need to show that
𝑝

(cid:0)

−

!!
1
)
−
: ℝ
[

10√𝑠 ln 𝑠, 10√𝑠 ln 𝑠

]

[−

ℒ
)

]

𝑥

(cid:1)
6𝑠

ℝ such that
→
> 0 for every polynomial 𝑝
6𝑠

𝑥

ℝ
[

]

ℒ(

. Notice that for any polynomial 𝑝

1
)
ℒ(
ℝ
𝑥
[
∈

= 1 and
= 𝑀𝑟 for
𝑥𝑟
6𝑠 that is nonnegative on

ℒ(

)

]

𝛿

1
(

−

) · ℒ(

𝑝

)

= 𝔼
𝑥

∼𝒩(

0,1
)

𝑝

𝑥

(

) −

𝑝

𝜆
(

) +

𝑝

𝜆

(−

)

.

(cid:1)

∈
𝛿
2

(cid:0)

Consider an arbitrary polynomial 𝑝
If 𝑝 = 0, then obviously
max
06𝑟6𝑠{|

ℒ(
= 1. Since 𝑝 is nonnegative on

𝑝𝑟

|}

𝑝

)

(

)

Í
[−

𝑥

=

𝑠
𝑟=0 𝑝𝑟 𝑥𝑟 that is nonnegative on

10√𝑠 ln 𝑠, 10√𝑠 ln 𝑠
.
]
= 0. So we can assume that 𝑝 ≠ 0 and without loss of generality
,

10√𝑠 ln 𝑠, 10√𝑠 ln 𝑠

[−

]

𝔼

𝑥

0,1
)

∼𝒩(

𝑝

𝑥

(

)

> 1
√2𝜋

1

∫
1
−

𝑥2

2
/

𝑒−

𝑝

𝑥

(

)

1
√2𝜋

+

∫
>10√𝑠 ln 𝑠

𝑥

|

|

𝑥2

2𝑑𝑥 .
/

𝑒−

𝑝

𝑥

(

)

The second integral can be bounded as follows

𝑥2

2
/

𝑒−

6

𝑝

𝑥

(

)

∫
>10√𝑠 ln 𝑠

𝑥

|

(cid:12)
(cid:12)
(cid:12)
|
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

𝑠

Õ𝑟=0

𝑝𝑟

|

|

𝑥

|

|

∫
>10√𝑠 ln 𝑠

Notice that since the function 𝑠 ln 𝑥

10𝑠

𝑥2

𝑟 𝑒−

2𝑑𝑥 6
/

𝑥

|

|

𝑠

(

1
)

+

𝑥2

𝑥𝑠 𝑒−

2𝑑𝑥 .
/

∫
>10√𝑠 ln 𝑠

𝑥

|
|
> 10√𝑠 ln 𝑠,

0.4𝑥2 is monotone for

𝑥

|

|

+

−
𝑥𝑠 𝑒−

𝑥2

2 6 𝑒−
/

10𝑠

−

𝑥2

10
/

97

for all 𝑥 such that

𝑥

|

|

> 10√𝑠 ln 𝑠. Hence

∫
>10√𝑠 ln 𝑠

𝑥

(cid:12)
(cid:12)
(cid:12)
|
(cid:12)
Let’s bound

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
𝑥2
2

exp

𝑥

(

)

1
1 𝑝
−
∫

𝑥2

2𝑑𝑥 >
/

𝑒−

𝑝

𝑥

(

)

1

∫
1
−

−

1

(cid:16)

∫
1
−

(cid:17)
𝑝2
𝑥
(
𝑝
max
61
𝑥
|
|

)
𝑥
(

)

𝑥2

𝑒−

1

To bound
−
Legendre polynomial is
∫

1 𝑝2

𝑥

(

)

𝑥2

2𝑑𝑥
/

𝑒−

6

𝑝

𝑥

(

)

𝑠

(

1
) ·

+

10𝑠

𝑒−

∫
>10√𝑠 ln 𝑠

𝑥

|

|

𝑥2

𝑒−

10𝑑𝑥 6 √10
/

𝑠

(

1
) ·

+

𝑒−

10𝑠 6 𝑒−

8𝑠 .

𝑑𝑥. Since 𝑝

𝑥

(

)

is nonnegative on

,

1, 1
]

[−

/

2
1
/

2𝑑𝑥 > 𝑒−
𝑠
𝑟=0 |

1

𝑝2

𝑥

(

)

𝑑𝑥 >

1

1

𝑠

2
(

1
)

+

∫
1
−

𝑝2

𝑥

(

)

𝑑𝑥 .

𝑝𝑟

|

∫
1
−

Í

𝑑𝑥 we can use Legendre polynomials (see for example [AWH13]). The degree 𝑗

=

𝐿𝑗

𝑥

(

)

𝑗

Õ𝑟=0

𝐿𝑗,𝑟 𝑥𝑟 =

𝑗

Õ𝑟=0 r

2𝑗

1

+
2

2𝑗

·

𝑗

+

𝑟
1
−
2
𝑗

𝑗
𝑟

(cid:18)

(cid:19) (cid:18)

(cid:19)

𝑥𝑟 .

They form an orthonormal system on
=
coeﬃcients 𝑐0, . . . , 𝑐𝑠 such that 𝑝

𝑥

(

)

1, 1
[−
]
𝑠
𝑗=0 𝑐𝑠 𝐿𝑗

and

𝑥

(

)

with respect to the unit weight. Hence there exist

Recall that by assumption max

06𝑟6𝑠{|

𝑝𝑟

|}

Í

1

𝑝2

𝑠

𝑑𝑥 =

𝑐2
𝑗 .

𝑥

(

)

∫
1
−
= 1, so there exists some 𝑟 such that

Õ𝑗=0

= 1. Thus

𝑝𝑟

|

|

1 =

=

𝑝𝑟

|

|

Notice that

6 √𝑠

𝐿𝑗,𝑟

|

|

+

1

1

∫
1
−

and

Notice that

Hence ﬁnally we get

𝑠

𝑠

𝑐 𝑗𝐿𝑗,𝑟

6

𝐿𝑗,𝑟

𝑐 𝑗

|

||

6 max
𝑟6𝑗6𝑠 |

|

𝐿𝑗,𝑟

𝑠

1
)

+

Õ𝑗=𝑟
22𝑠 for 0 6 𝑟 6 𝑗 6 𝑠. Hence we get a bound

Õ𝑗=𝑟

(cid:12)
(cid:12)
(cid:12)

|

(
p

𝑠
𝑗=0 𝑐2
𝑗 .

qÍ

(cid:12)
(cid:12)
(cid:12)
·

𝑒−

𝑝

𝑥

(

)

𝑥2

2𝑑𝑥 > 1
3𝑠

/

𝑠

Õ𝑗=0

>

𝑐2
𝑗

1

+

𝑠

2
(

4𝑠 > 2−

7𝑠 ,

3 2−
1
)

𝔼

𝑥

0,1
)

∼𝒩(

𝑝

𝑥

(

)

> 1
√2𝜋

7𝑠

2−

−

𝑒−

8𝑠 > 2−

8𝑠 .

𝑝

𝜆
(

) +

𝑝

𝜆

(−

𝛿
2

(cid:0)

6 𝛿

𝑠

Õ𝑟=0

𝜆

|

|

)

(cid:1)

𝑟 6 2𝛿𝜆𝑠 6 2−

9𝑠 .

𝛿

1
(

−

) · ℒ(

𝑝

)

= 𝔼
𝑥

∼𝒩(

0,1
)

𝑝

𝑥

(

) −

𝛿
2

𝑝

𝜆
(

) +

𝑝

𝜆

(−

)

> 2−

8𝑠

−

2−

9𝑠 > 0 .

(cid:0)
Therefore by Theorem F.1 there exists a ﬁnitely supported probability distribution 𝜂0 with moments
(cid:3)
𝑀1, . . . , 𝑀𝑠 such that supp
𝜂0) ⊆ [−
(

10√𝑠 ln 𝑠, 10√𝑠 ln 𝑠

]

(cid:1)

.

98

We can assume that 𝜂0 is symmetric (since if 𝑧

𝑤
|
is symmetrically distributed and has the same ﬁrst 𝑠 moments as 𝑧). Thus the mixture distribution
𝜂 =
2 each) is symmetric and has
𝜂0 +
)
Gaussian moments up to 𝑠

𝛿𝜂1 (where 𝜂1 takes values
1:

𝜆 with probability 1

are independend, 𝑧𝑤

𝜂0 and 𝑤

0, 1
)
(

1
(

𝑁

/|

∼

∼

±

−

𝛿

+

𝔼
𝑥
𝜂

∼

𝑥𝑟 = 𝔼
𝑥
∼𝒩(

0,1
)

𝑥𝑟 ,

if 0 6 𝑟 6 𝑠

1 ,

+

and its higher moments satisfy

𝛿𝜆𝑟 6 𝔼
𝜂
∼

𝑥

𝑥𝑟 6 𝛿𝜆𝑟

10𝑠

+ (

𝑟 ,

)

if 𝑟 > 𝑠 is even.

G Matrix concentration bounds

In this section, we use standard tools to establish some matrix concentration inequalities that are
essential to our main results.

Our key tools will be the following general result by Rudelson showing convergence of empirical

covariances of random variables.

Fact G.1 (Theorem 1, [Rud99]). Let 𝑌 be a random vector in the isotropic position. Let 𝑌1, 𝑌2, . . . , 𝑌𝑞 be
𝑞 independent copies of 𝑌. Then, for some absolute constant 𝐶 > 0,

1
𝑞

𝑞

Õ𝑖=1

𝑌𝑖𝑌⊤𝑖 −

𝐼

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

𝔼

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

6 𝐶

log 𝑞
√𝑞

p

𝔼

𝑌

k

(k

·

log 𝑞

log 𝑞 .
1
/
)

We will also use the following simple computation of variances of low-degree polynomials of

product subgaussian random vectors.

|

𝑆

𝑆:

𝑛

⊆ [

𝑌, 𝑢
h

2𝑡 6 𝐶 𝑡

Lemma G.2 (Variance of Polynomials of Independent Subgaussians). Let 𝑌 be a product random
variable on ℝ𝑛 with coordinates of mean 0, variance 1 satisfying 𝔼
𝑡 for every unit vector
2𝑡
(
𝑢 for some absolute constant 𝐶 > 0. Let 𝑝 =
ℝ𝑛 of degree 𝑘 where the
∈
6 𝐶 𝑡
sum ranges of multisets 𝑆

6𝑘 𝑝𝑆 𝑦𝑆 be a polynomial in 𝑦
6𝑘 𝑝2
𝑌
𝑆
(
)
2
Í
Proof. For any polynomial 𝑝, we write
2 to denote the sum of squares of its coeﬃcients in
2
the monomial basis. For any multilinear polynomial 𝑝, observe that 𝔼 𝑝2 =
𝑝
2. For a non-
2 𝑦2
multilinear 𝑝, we write 𝑝 =
𝑆 𝑞𝑆 such that 𝑞𝑆 is a multilinear polynomial of degree at
/
= 0 whenever 𝑆 ≠ 𝑆′. Now,
most 𝑘
. Observe that
𝑆 k
|
𝑆 𝑞2
2
𝔼 𝑝2 =
2.
𝑆
k
|
|
(cid:3)
On the other hand, 𝔼 𝑦2

of size at most 𝑘. Then,
Í
𝑝

2
k
> 1 for any 𝑆, 𝔼 𝑦2
Í
6 𝐶 𝑘
6𝑘 𝔼 𝑦2
max
𝑆

𝑆 𝑦2
2. Further, 𝔼 𝑦2
𝑆′
>
𝑆 𝑞2
𝑞𝑆
𝑆
2𝑘
(

6𝑘
𝑆:
=
𝑝
Í
k
𝑆. Since 𝔼 𝑦2
𝑆
6
2
𝑆 𝑞2
2 ·

2. Thus, 𝔼 𝑝2 >

2 𝔼 𝑦2
/

𝑞𝑆𝑞𝑆′
2

6𝑘 𝑝2
𝑆.

6 𝔼 𝑝2

2𝑡
(

|
6𝑘

k
𝑘 .

𝑆 k

𝑞𝑆

𝑞𝑆

𝑞𝑆

|
k

𝑆
2
2

Í

−

=

𝑆

2
2

𝑝

2

𝑆:

𝑆:

𝑆:

k

k

k

k

k

k

k

k

k

i

]

)

)

)

𝑆

𝑆

𝑆

𝑆

𝑡

|

|

|

|

|

|

|

|

Í

Í

Lemma G.3. Let 𝑌 be a random vector in ℝ𝑛 with independent coordinates of mean 0 and variance 1
2𝑡 6 𝐶 𝑡
satisfying 𝔼
𝑡 for some absolute constant 𝐶 > 0. Then, with probability at least 0.99 over
the draw of 𝑌1, 𝑌2, . . . , 𝑌𝑑 i.i.d. copies of 𝑌,

𝑌, 𝑢
h

2𝑡
(

i

)

1
𝑑

Õ𝑖

𝑡

𝑌⊗
𝑖
(

𝑌 ⊗
𝑖

)(

𝑡

⊤
)

−

𝔼
𝐷
𝑌

∼

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

for some absolute constant 𝐶′ > 0.

𝑛𝑡

2 log(
/

𝑡

6

𝐶′𝑡

𝑛

(

)(

𝑡

)

,

2
1
+
)/
√𝑑

𝑡

𝑌⊗

𝑡

𝑌⊗

⊤

(cid:0)

(cid:1) (cid:0)

(cid:1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

99

𝑡

Proof. Let 𝑀 = 𝔼
𝑢, 𝑌⊗
h
Thus, all eigenvalues of 𝑀 are between 1 and 𝐶 𝑡

)⊤. Then, quadratic forms
𝑡.

of degree at most 𝑡. Thus, using Lemma G.2, we have that

𝑢, 𝑀𝑢

𝑌⊗
(

𝑌 ⊗

)(

i

h

i

𝑡

𝑡

is the variance of polynomial 𝑝 =
𝑡.

𝑢, 𝑀𝑢

6

6

𝑢

𝑢

2

h

i

k

k

2𝐶 𝑡

2𝑡
(

)

2
2

k

k

2𝑍𝑖 for 𝑍𝑖 = 𝑌⊗
1
We will now apply Fact G.1 to the isotropic random vectors 𝑀−
/
𝑖

𝑡

for 1 6 𝑖 6 𝑑.

2𝑡
(

)

Then, we obtain:

𝔼

T
2𝑍
1
2𝑍𝑀−
1
𝑀−
/
/

𝐼

6 𝐶

log 𝑑

𝔼

2𝑍
1
𝑀−
/

log 𝑑
2

log 𝑑
1
/

.

To ﬁnish, we compute 𝔼

(cid:13)
(cid:13)

2𝑍
1
𝑀−
/

−
(cid:13)
log 𝑑 6
(cid:13)

√𝑑

p
2
1
𝑀−
/

(cid:16)
(cid:13)
log 𝑑 𝔼
(cid:13)
k

(cid:17)

(cid:13)
(cid:13)

𝑍

log 𝑑. Next, 𝔼

𝑍

log 𝑑 = 𝔼

𝑌

k
6 1, we obtain:

k
𝔼

k
2𝑍
1
𝑀−
/

k
log 𝑑

𝑡 log 𝑑 6
k
log 𝑑
1
/

6

𝑛(
𝑛𝑡

𝑡

𝑡

𝑡

log 𝑑

2
)
/

log 𝑑𝐶(
2
/

2
/
)
2𝐶 𝑡
/
Thus, for using 𝑛 > log 𝑑 and Fact G.1, 𝔼

log 𝑑. Using
2
1
(cid:13)
(cid:13)
𝑀−
/
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2𝑍T
1
2𝑍𝑀−
1
𝑀−
/
/

(cid:13)
log 𝑑
)(
(cid:13)

((
2.
/

𝑡 log 𝑑

2
)

2
)
/

(cid:13)
(cid:13)

/

(

)

𝑡

𝑡

Markov’s inequality completes the proof.

(cid:13)
(cid:13)

(cid:16)
(cid:13)
(cid:13)
𝑡
2 log(
+

6 𝑛𝑡

/

(cid:13)
(cid:13)
10𝐶𝑡

𝑡

)

𝑛

(

)(

2

1
)/
√𝑑

(cid:17)

. Applying

(cid:3)

𝐼

−

(cid:13)
(cid:13)

We also state here some standard concentration bounds used in the proofs.

Fact G.4. [LM00]Let 𝑋

𝜒2

𝑚, 𝑥 > 0, then

∼

ℙ

𝑋

(cid:16)

−

𝑚 > 2𝑥

+

ℙ

𝑚

(

−

2√𝑚𝑥

6 𝑒−

𝑥

(cid:17)
𝑋 > 𝑥
)

6 𝑒−

𝑥2
4𝑚

Fact G.5. [Wai19] Let 0 < 𝜀 < 1. The 𝑛
is, there exists a set 𝑁𝜀 of unit vectors in ℝ𝑛 of size at most
exists some 𝑣

𝑁𝜀 such that

6 𝜀.

−

𝑢

𝑣

1-dimensional Euclidean sphere has an 𝜀-net of size
such that for any unit vector 𝑢

𝑛

3
𝜀

(cid:0)

(cid:1)

k
∈
Theorem G.6. [Wai19] Let 𝑊

−

k

𝑁

0, 1
)
(

∼

𝑛

×

𝑑. Then with probability 1

exp

(−

𝑡

,
2
)

/

−

𝑊

k

k

6 √𝑛

√𝑑

√𝑡

+

+

𝑛

3
. That
𝜀
ℝ𝑛 there
(cid:1)

(cid:0)
∈

and

T

𝑊

𝑊

𝑛Id

6 𝑑

2√𝑑𝑛

𝑡

4

𝑡

𝑛

𝑑

−
(cid:13)
Theorem G.7 (Matrix Bernstein [Tro12]). Consider a ﬁnite sequence
(cid:13)
self-adjoint matrices in ℝ𝑑1
𝑑2. Assume that each random matrix satisﬁes

p

(cid:13)
(cid:13)

+

+

+

+

(

×

.

)

𝑍𝑘

{

}

of independent, random,

Deﬁne

Then, or all 𝑡 > 0,

𝔼 𝑍𝑘 = 0 and

𝑍𝑘

k

k

6 𝑅 almost surely.

𝜎2 := max

𝔼 𝑍𝑘 𝑍𝑘

T

,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
Õ𝑘
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T

𝔼 𝑍𝑘

𝑍𝑘

.

)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Õ𝑘

((cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ℙ

𝑍𝑘

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Õ𝑘

 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

6

> 𝑡

!

𝑑1 +

(

𝑑2)

exp

(cid:26)

100

𝑡2

2
/
𝑅𝑡

𝜎2

−
+

.

3

(cid:27)

/

Theorem G.8 (Matrix Hoeﬀding [Tro12]). Consider a ﬁnite sequence
self-adjoint matrices in ℝ𝑑

𝑑. Assume that each random matrix satisﬁes

×

𝑍𝑘

{

}

of independent, random,

Then, for all 𝑡 > 0,

where 𝜎2 :=

𝑘 𝐴2
𝑘

.

𝔼 𝑍𝑘 = 0 and 𝑍2

𝑘 (cid:22)

𝐴2

𝑘 almost surely.

6 𝑑 exp

> 𝑡

!

𝑡2
8𝜎2

(cid:27)

−

(cid:26)

ℙ

𝑍𝑘

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Õ𝑘

 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
Theorem G.9 (𝑘-sparse norm of a Gaussian matrix). Let 𝑊
(cid:13)
1 6 𝑘 6 𝑑. Then with probability at least 1

(cid:13)
(cid:13)Í

𝑘

𝑘
𝑒𝑑

−

𝑁

0, 1
)
(

∼

𝑛

×

𝑑 be a Gaussian matrix. Let

(cid:1)
T
𝑢

(cid:0)

ℝ𝑑

𝑊 𝑣 6 √𝑛

𝑘 ln

3
s

+

𝑒 𝑑
𝑘

.

(cid:19)

(cid:18)

max
ℝ𝑛
𝑢
∈
=1
𝑢
k

k

max
𝑘-sparse 𝑣
∈
=1
𝑣

k

k

𝑥

Proof. Let 𝑣 be some 𝑘-sparse unit vector that maximizes the value, and let 𝑆
nonzero coordinates of 𝑣. Consider some ﬁxed (independend of 𝑊) unit 𝑘-sparse vector 𝑥
and the set 𝑆
from 𝑆
by √𝑛
By the union bound, the probability that the norm of 𝑊𝑆

of nonzero coordinates of 𝑥. If we remove from 𝑊 all the rows with indices not
. By Theorem G.6 norm of this matrix is bounded
)
(
𝑑
𝑡
.
𝑘
(−
√𝑡 is at most
(cid:1)

𝑘 Gaussian matrix 𝑊𝑆
√𝑡 with probability at least exp

. Number of all subsets 𝑆
is greater than √𝑛

)
, we get an 𝑛
𝑥
)
√𝑘

be the set of
ℝ𝑑

of size 𝑘 is

𝑑
]
√𝑘

(
+

⊆ [
+

2
)

+

×

+

∈

𝑣

/

)

(

(

(cid:0)

𝑥

𝑣

(

)

Taking 𝑡 = 4𝑘 ln

/
Lemma G.10. Let 𝑤

(

𝑒 𝑑

𝑁

0, 1
)
(

∼

coordinates of 𝑤. Then with probability 1

𝑘

, we get the desired bound.
)

Proof. Let 𝑆 be any ﬁxed subset of
[
𝑆 𝑤2
vector and by Fact G.4, ℙ
𝑖
of size 𝑘, we get
𝑑

∈

𝑖

[

]

(cid:16)Í
ℙ

𝑑
𝑘

(cid:18)

·

(cid:19)

exp

(−

𝑡

2
)

/

6 exp

𝑘 log2(

𝑒 𝑑

𝑘

/

) −

2

.

𝑡

/

(cid:0)

(cid:1)

(cid:3)

𝑑 be a Gaussian vector and let 1 6 𝑘 6 𝑑. Let 𝑆𝑘 be the set of 𝑘 largest

𝑘

,

𝑘
𝑒𝑑

𝑆𝑘 𝑤2
𝑖

𝑖

6 10𝑘 ln

𝑒 𝑑

𝑘

.
)

∈

/
−
of size 𝑘. Then 𝑤 restricted on 𝑆 is a 𝑘-dimensional Gaussian
𝑥. By a union bound over all

(cid:1)
Í
2√𝑘𝑥

subsets of

𝑑
]
> 2𝑥

6 𝑒−

(

(cid:0)

𝑑
𝑘

+

(cid:17)

(cid:0)

(cid:1)

𝑤2
𝑖

> 𝑘

2𝑥

+

+

2√𝑘𝑥

!

6 𝑒−

𝑥

𝑘 log2(

+

𝑒𝑑

𝑘

) .

/

𝑆
Õ𝑖
∈

Taking 𝑥 = 4𝑘 ln

𝑒 𝑑

𝑘

)

/

(

we get the desired bound.

(cid:3)

Lemma G.11. For large enough 𝑛 and 𝑑 such that 𝑛 6 𝑑, let 𝑊
let 𝑢
𝑑

ℝ𝑛 be an arbitrary unit vector (which can possibly depend on 𝑊). For any 𝑡 > 0 let 𝑆𝑡 =
∈
𝑢T𝑊
] | |(

𝑛

×

𝑑 be a Gaussian matrix and

0, 1
)
(

𝑖

{

∈

> 𝑡
}
Then, for any 𝑡 > 3√𝐵 ln 𝑑,

. Also let 𝐵 > 1.
𝑆𝑡

𝐵 with probability at least 1

2 exp

6 𝑛

𝑁

∼

𝑛

[

)

|

𝑖

−

.
)

(−

|

|

/

101

 
Proof. Let 𝑡 > 3√𝐵 ln 𝑑. For any ﬁxed (independend of 𝑊) unit vector 𝑥
standard Gaussian variables. For large enough 𝑑,

ℝ𝑛,

𝑥T𝑊

(

)

∈

𝑖 are i.i.d.

)
Hence the probability that there are 3 6 𝑘 6 𝑑 coordinates that are larger than 𝑡

−

−

|(

/

(cid:3)

(cid:2)

(cid:0)

(cid:1)

|

ℙ

T
𝑥

𝑊

𝑖

> 𝑡

1

6 exp

𝑡2

3

.

ln

+

𝑑
𝑘 −

/

𝑑
𝑘

(cid:18)

(cid:19)

exp

𝑘

−

𝑡2

3

/

·

6 exp

𝑘

1

(cid:20)
ℝ𝑛,

(cid:18)
𝑥
k
𝑛
,
)

(cid:0)

∈
exp

(cid:1)
𝑦
If for unit vectors 𝑥, 𝑦
k
𝑊
probability at least 1
k
Fact G.5, for any 0 < 𝜀 < 1, for 𝜀 = 1
𝑛 log 100
𝑛 log 𝑑
exp
𝐵 is at most
that
(cid:0)

𝑛
2 log 𝑑
> 𝑛
𝑆𝑡

6 exp

−
k

(−

+
/

−

(cid:0)

(cid:1)

|

|

10√𝑑

1 is at most

−
2
3 ·

−

(cid:18)

𝑘𝑡2 ln 𝑑

.

(cid:19)

𝑡2

3

6 exp

𝑘

ln 𝑑

𝑡2

3

6 exp

−

/

(cid:19) (cid:21)
6 𝜀, then
6 𝜀
𝑊 𝑥
6 10√𝑑. Hence if 𝜀 6 1
there exists an 𝜀-net in 𝑛

(cid:0)
𝑊 𝑦

(cid:1) (cid:3)
𝑊
k
,
10√𝑑

(cid:2)
−

k

k

(
(cid:12)
(cid:12)

−

. By Theorem G.6, with
k
𝑥T𝑊
6 1. By

𝑦T𝑊
1-dimensional sphere of size

− (

)

)

𝑖

𝑖

(cid:12)
(cid:12)

(for large enough 𝑑). By the union bound, the probability

(cid:1)
𝑛 ln 𝑑

exp

(cid:18)

2
3 |

𝑆𝑡

𝑡2

|

−

(cid:19)

6 exp

(−

𝑛

.

)

(cid:3)

𝑛 log 𝑑

(cid:16)p

(cid:17)

The next lemma is the main technical challenge of Section 4.

Theorem G.12. [DM14] Let 𝑊
×
and let 𝑁 be the matrix whose diagonal entries are zeros and each non-diagonal entry 𝑁𝑖𝑗 is

log 𝑑
(

0, 1
)
(

→ ∞

as 𝑑

𝑁

∼

)

𝑑, where 𝑛 > 𝜔

. Let 0 6 𝜏 6 𝑜

𝑛

𝑁𝑖𝑗 =

𝑊T𝑊

0
(cid:0)

𝑖𝑗 −

(cid:1)

sign

𝑊T𝑊

(cid:0)

𝑖𝑗 ·

(cid:1)





𝜏

if

𝑊T𝑊

> 𝜏

𝑖𝑗

otherwise
(cid:0)

(cid:12)
(cid:12)
(cid:12)

(cid:1)

𝑜

(cid:12)
(cid:12)
(cid:12)
1
)
(

−

Then there exists an absolute constant 𝐶 > 1 such that with probability 1

𝑁

k

k

6 𝐶

𝑑

(cid:16)

+

√𝑑𝑛

exp

(cid:17)

𝜏2
𝐶𝑛

.

(cid:21)

−

(cid:20)

H Linear Algebra

Lemma H.1. Let 𝑣 and 𝑢 be unit vectors such that

𝑣𝑣T

k

−

𝑢𝑢T

k

6 𝜀. Then

2 > 1

𝑣, 𝑢

h

i

−

2𝜀2.

Proof. Let 𝑤 be a unit vector orthogonal to 𝑢 such that 𝑣 = 𝜌𝑢
Then

1

−

+

𝜌2𝑤 for some positive 𝜌 6 1.

T
𝑣𝑣

T =

𝜌2

T
𝑢𝑢

𝑢𝑢

1
+
q
𝑢𝑢T has rank 2, its Frobenius norm is bounded by 2𝜀, hence

1
q

−

+

−

−

−

+

𝜌

𝜌

1

(cid:1)

(cid:0)

T
𝜌2𝑢𝑤

1

−

(cid:0)

𝜌2

T
𝑤𝑤

.

(cid:1)

p
T
𝜌2𝑤𝑢

Since 𝑣𝑣T

−

It follows that

4𝜀2 >

T
𝑣𝑣

T
𝑢𝑢

2
𝐹

k

−

k

= 2

1

2

𝜌2

−

+

2𝜌2

1

−

𝜌2

= 2

1

𝜌2

.

−

(cid:1)

(cid:0)
2 = 𝜌2 > 1

(cid:0)
2𝜀2 .

−

𝑣, 𝑢

h

i

(cid:1)

(cid:0)

(cid:1)

(cid:3)

102

Lemma H.2. Let 𝑀 be a symmetric matrix such that
top eigenvalue 𝜆1 of 𝑀 satisﬁes

𝑢𝑢T
6 𝜀 and the top eigenvector 𝑣1 of 𝑀 satisﬁes

2 for some unit vector 𝑢. Then the
100𝜀2.
𝑣1, 𝑢

6 𝜀 < 1

2 > 1

𝑀

−

1

k

k

h

i

−

𝜆1 −

|

|

Proof. Consider an eigenvalue decomposition of 𝑀:

𝑀 =

𝑑

Õ𝑗=1

𝜆𝑗𝑣 𝑗𝑣 𝑗

T

,

𝑑

𝑗=1 is an orthonormal basis in ℝ𝑑. By triangle inequality

𝑣 𝑗

}

𝑀

−

T
𝑣1𝑣1

k + k

𝑀

−

T
𝑢𝑢

6

k

𝑀

k

−

T
𝑣1𝑣1

𝜀.

k +

{

k

𝜆𝑑

|
T
𝑢𝑢

and

6

k

k
6 𝜀, 𝑢T𝑀𝑢 > 1

𝑀

T
𝑣1𝑣1

−
k
𝜀, hence

−
𝜆1 − h

𝑣1, 𝑢

1

i
|
2 , so 𝜆1 > 0, and 𝜆1 6
𝑣1, 𝑢

1

2

|h

i

−

|

6 max

{|

1

,

.

𝜆2|}
𝜆1|
−
𝜀 > 1
2 . Notice that

|

> 1

𝜆1|
|
T
=
𝑣1

|

2

−
𝑀𝑣1 − h
𝜀 6 1
1
𝜆1 −
6 2𝜀. By Pythagorean theorem

6 𝜀,

𝑣1, 𝑢

𝑣1, 𝑢

𝜀, so

+

+

i

i

h

2

2

|

|

|

|

6 𝜀.
𝑑
𝑗=1h

𝑣 𝑗 , 𝑢

i

2 = 1, hence

where

>

𝜆1|

|

𝜆2|

|

> . . . >

|

T
𝑣1𝑣1

k

T
𝑣1𝑣1

𝑀

k

−

−

:

k

Let’s bound

Since

𝑀

k

−

𝑢𝑢T

k

hence 𝜆1 >

𝜀 >
By triangle inequality

−

−

𝑑

h

Õ𝑗=2

2 6 2𝜀,

𝑣 𝑗 , 𝑢

i

Í

=

2

i

|

T
𝑣2

|

𝑀𝑣2 − h

𝑣2, 𝑢

i|

6 𝜀,

:

𝜆2|
|
𝑣2, 𝑢
𝜆2 − h

T
𝑣1𝑣1

k

T
𝑢𝑢

k

−

6 4𝜀.

𝑣2, 𝑢

so

h

i

2 6 2𝜀. Now let’s bound

hence

𝜆2|

|

6 3𝜀. Therefore

|

By lemma H.1,

𝑣, 𝑢

h

i

2 > 1

32𝜀2.

−
𝑑, 𝑀

ℝ𝑑

×

Lemma H.3. Let 𝑀
Then the top eigenvector 𝑣1 of 𝑀 satisﬁes
Proof. Write 𝑧 = 𝛼𝑣1 +

𝛼2𝑣

√1

−

(cid:23)

∈

h
where 𝑣

0, Tr 𝑀 = 1 and let 𝑧
𝑂

2 > 1

𝑣1, 𝑧

ℝ𝑑 be a unit vector such that 𝑧T𝑀 𝑧 > 1
.
)

∈
𝜀
(

−

i
is a unit vector orthogonal to 𝑣1.

As 𝑣1

T𝑀𝑣1 > 𝑧T𝑀 𝑧 and 𝑣

T𝑀𝑣
⊥

⊥

⊥
⊥
T
T
𝑀 𝑧 = 𝛼2𝑣1
𝑧
= 𝛼2
> 1

𝜆1 −
𝜀

𝑀𝑣1 +
𝑣

T

⊥

(cid:0)
−
6 𝜀, rearranging

1

−

𝑀𝑣
(cid:0)

𝛼2

𝑣

𝑣
(cid:1)

+

⊥

T

⊥
T

𝑀𝑣

⊥

𝑀𝑣

⊥

⊥

(cid:1)

𝛼2 > 1

𝜀
−
−
𝑣
𝜆1 −

𝑣

T𝑀𝑣
⊥
T𝑀𝑣
⊥

⊥

⊥

> 1

2𝜀.

−

103

(cid:3)

𝜀.

−

(cid:3)

Fact H.4. Let 𝐴, 𝐵

∈
Lemma H.5. Let 𝑋

ℝ𝑑

𝑑, 𝐴, 𝐵

×

0. Then

(cid:23)

𝐴, 𝐵

h

i

> 0.

∈

ℝ𝑑

×

𝑑 be a positive semideﬁnite matrix. Then for any 𝐴

ℝ𝑑

𝑑,

×

∈

𝐴, 𝑋

|h

6

𝐴

k

k ·

i|

Tr 𝑋 .

Proof. Since 𝑋 is positive semideﬁnite, 𝑋 =

𝑑
𝑖=1 𝜆𝑖 = Tr 𝑋. Hence

𝑑
𝑖=1 𝜆𝑖 𝑧𝑖 𝑧𝑖

T for unit vectors 𝑧𝑖 such that 𝜆𝑖 > 0 and

Í

𝐴, 𝑋

|h

i|

=

T

Tr 𝑋

(cid:12)
(cid:12)

Lemma H.6. Let 𝑋

ℝ𝑑

×

∈

𝑑

=

𝐴

𝜆𝑖 Tr 𝑧𝑖 𝑧𝑖

𝐴

T

T

𝜆𝑖 Tr 𝑧𝑖

𝐴𝑧𝑖

(cid:12)
(cid:12)

(cid:12)
Õ𝑖=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
Õ𝑖=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
𝑑 be a positive semideﬁnite matrix. Then for any 𝑎, 𝑏

Õ𝑖=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

𝑑

6

𝜆𝑖

k

=

𝐴

k

𝐴

k

k ·

Tr 𝑋 .

(cid:3)

ℝ𝑑,

∈

Í

=

𝑑

T
𝑎𝑏

, 𝑋

2 6

i
𝑏𝑏T, 𝑋

h
> 0 and

T
𝑎𝑎

, 𝑋

T
𝑏𝑏

, 𝑋

.

i

i · h

h

𝑎𝑎T, 𝑋

> 0. Notice that if the inequality is true for some
ℝ𝑑, it is also true for 𝑐1𝑎, 𝑐2𝑏 for all positive numbers 𝑐1, 𝑐2. So we can assume without loss of
>
𝑎

= 1. Consider

> 0 and

𝑏𝑏T, 𝑋

𝑎𝑎T, 𝑋

=

𝑏

𝑏

𝑏

𝑏

𝑎

𝑎

𝑎

h

h

i

i

T, 𝑋
)

i

h(

−

)(

−

T, 𝑋
)

i

i

h

i

h(

+

)(

+

Proof. By Fact H.4,
𝑎, 𝑏
generality that
0 . We get

∈

h

and

hence

𝑎𝑏T, 𝑋

2 6 1.

i

h

T
𝑎𝑏

, 𝑋

2

h

6

i

h

T
𝑎𝑎

, 𝑋

i + h

T
𝑏𝑏

, 𝑋

6 2

i

2

h

−

T
𝑎𝑏

, 𝑋

T
𝑎𝑎

, 𝑋

6

i

h

i + h

T
𝑏𝑏

, 𝑋

i

6 2 ,

(cid:3)

I Experimental Setup

)

]

𝑑

∼

𝑁

⊆ [

0, Id𝑛
(

In the experiments, the instances were sampled from the planted distributions of models 6.3 and
6.6 with the diﬀerence that 𝑢
and 𝑣 is a 𝑘-sparse unit vector obtained sampling a
and then a unit vector with support 𝑆. All the algorithms returned the
random 𝑘-subset 𝑆
top 𝑘 coordinates of their estimation vector. Figure 2, 2 plot the absolute correlation between 𝑣 and
its estimate. Each plot was obtained averaging multiple independent runs on the same parameters,
for each algorithm the shadowed part corresponds to the interval containing 50% of the results,
the line corresponds to the mean of the results in such interval.
In Figure 2b, the adversarial matrix 𝐸 is sampled according to model 6.6 for 𝑠 = 2, that is the ﬁrst 2
moments of 𝑌 are Gaussian. Similarly, in Figure 2c, 𝐸 is sampled according to model 6.6 for 𝑠 = 4,
so the ﬁrst 4 moments of 𝑌 are Gaussian.

Experiments were done on a laptop computer with a 3.5 GHz Intel Core i7 CPU and 16 GB of

RAM, random instances were obtained using Numpy pseudo-random generator.

104

