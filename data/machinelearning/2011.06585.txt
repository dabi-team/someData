0
2
0
2

v
o
N
2
1

]

G
L
.
s
c
[

1
v
5
8
5
6
0
.
1
1
0
2
:
v
i
X
r
a

Sparse PCA: Algorithms, Adversarial Perturbations and
Certiï¬cates

Tommaso dâ€™Orsiâˆ—

Pravesh K. Kothariâ€ 

Gleb Novikovâ€¡

David SteurerÂ§

November 13, 2020

Abstract

We study eï¬ƒcient algorithms for Sparse PCA in standard statistical models (spiked co-
variance in its Wishart form). Our goal is to achieve optimal recovery guarantees while being
resilient to small perturbations. Despite a long history of prior works, including explicit studies
of perturbation resilience, the best known algorithmic guarantees for Sparse PCA are fragile
and break down under small adversarial perturbations.

We observe a basic connection between perturbation resilience and certifying algorithms that
are based on certiï¬cates of upper bounds on sparse eigenvalues of random matrices. In contrast
to other techniques, such certifying algorithms, including the brute-force maximum likelihood
estimator, are automatically robust against small adversarial perturbation.

We use this connection to obtain the ï¬rst polynomial-time algorithms for this problem that
are resilient against additive adversarial perturbations by obtaining new eï¬ƒcient certiï¬cates for
upper bounds on sparse eigenvalues of random matrices. Our algorithms are based either on
basic semideï¬nite programming or on its low-degree sum-of-squares strengthening depending
on the parameter regimes. Their guarantees either match or approach the best known guarantees
of fragile algorithms in terms of sparsity of the unknown vector, number of samples and the
ambient dimension.

To complement our algorithmic results, we prove rigorous lower bounds matching the
gap between fragile and robust polynomial-time algorithms in a natural computational model
based on low-degree polynomials (closely related to the pseudo-calibration technique for sum-
of-squares lower bounds) that is known to capture the best known guarantees for related
statistical estimation problems. The combination of these results provides formal evidence of
an inherent price to pay to achieve robustness.

Beyond these issues of perturbation resilience, our analysis also leads to new algorithms
for the fragile setting, whose guarantees improve over best previous results in some parameter
regimes (e.g. if the sample size is polynomially smaller than the dimension).

âˆ—ETH ZÃ¼rich. Supported by Steurerâ€™s ERC Consolidator Grant.
â€ Carnegie-Mellon University. Part of this work done while at Princeton University and the Institute for Advanced

Study.

â€¡ETH ZÃ¼rich.
Â§ETH ZÃ¼rich. Supported by an ERC Consolidator Grant.

 
 
 
 
 
 
Contents

1 Introduction

1.1 Results . . . . . .

. . . . .
Sharp bounds for the Wishart model .

. . . . .
1.1.1
. . . . .
1.1.2 Additional Results: Practical Algorithms and Experiments . . . . .

. . . . . .
. . . . . .

. . . . . .
. . . . . .

. . . . . .

. . . . .

2 Techniques

2.1 Perturbation-resilience from Sparse Eigenvalue Certiï¬cates . . . .
. . . . . .
. . . . . .
2.2 Algorithms that Certify Sparse Eigenvalues .
. . . . . .
. . . . . .
. . . . .
2.3 New Certiï¬cates via basic SDP . . .
. . . . . .
2.4 New certiï¬cates via higher-level Sum-of-Squares . . . .
. . . . . .
2.4.1 Certiï¬cates via Certiï¬able Subgaussianity . . . .
. . . . . .
. . . . . .
2.4.2 Certiï¬cates via Limited Brute Force .
2.5 Concrete lower bounds for perturbation-resilient algorithms . . .
. . . . . .
2.6 Beyond limitations of CT via low-degree polynomials .
. . . . . .
. . . . . .

2.6.1 Polynomials based algorithm . . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

3 Preliminaries

. . . . . .
3.1 Pseudo-distributions . . .
3.2
. . . . . .
Sum-of-squares proofs . .
3.3 Low-degree likelihood Ratio . . . . .

. . . . . .
. . . . . .
. . . . . .
3.3.1 Background on Classical Decision Theory . . . .
3.3.2 Background on the Low-degree Method . . . . .

. . . . .
. . . . .
. . . . .

. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

4 Resilience of the basic SDP and Certiï¬ed Upper Bounds

4.1 Basic Certiï¬cates for Sparse Quadratic Forms . . . . . .
. . . . . .
4.2 The basic SDP Algorithm . . . . . .

. . . . .

. . . . . .
. . . . . .

. . . . .
. . . . .

5 Resilience of SoS and Stronger Certiï¬ed Upper Bounds

5.1
5.2
5.3

SoS Certiï¬cates for Sparse Eigenvalues via Certiï¬able Subgaussianity . . .
. . . . .
SoS Certiï¬cates for Sparse Eigenvalues via Limited Brute Force
. . . . .
. . . . . .
SoS Algorithms .

.
. . . . . .

. . . . . .

. . . . .

. . . . .

6 Unconditional lower bounds for distinguishing

. . . . . .

. . . . .
Spiked covariance model with sparsity . . . .

. . . . . .
6.1 Low-degree polynomials .
6.2
. . . . . .
6.3 Almost Gaussian vector in random subspace . . . . . .
6.4 Chi-squared-divergence and orthogonal polynomials .
6.5
. . . . .
6.6 Almost Gaussian vector in random subspace (proof) . .

Spiked covariance model with sparsity (proof)

. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

7 Polynomial-based algorithm with the right log factor
7.1 Low degree polynomials as estimators . . . .
. . . . .
7.2 Computation in polynomial time . .

. . . . . .
. . . . . .

. . . . . .
. . . . . .

. . . . .
. . . . .

1
4
. . . . . .
. . . . . . 10
. . . . . . 11

13
. . . . . . 13
. . . . . . 14
. . . . . . 15
. . . . . . 16
. . . . . . 16
. . . . . . 17
. . . . . . 17
. . . . . . 19
. . . . . . 19

20
. . . . . . 21
. . . . . . 22
. . . . . . 23
. . . . . . 24
. . . . . . 24

25
. . . . . . 26
. . . . . . 27

31
. . . . . . 31
. . . . . . 34
. . . . . . 36

38
. . . . . . 38
. . . . . . 39
. . . . . . 41
. . . . . . 43
. . . . . . 45
. . . . . . 50

55
. . . . . . 56
. . . . . . 61

8 Fast Spectral Algorithms for Recovery

8.1 Algorithm recovers u with high probability .
8.2 Algorithm recovers v with high probability .

. . . . . .
. . . . . .

. . . . . .
. . . . . .

. . . . .
. . . . .

63
. . . . . . 66
. . . . . . 73

9 Experiments

References

A Relationship with Clustering mixture of subgaussians

B Comparison with the Wigner model

C Thresholding Algorithms are Fragile
C.1 SVD with Thresholding is Fragile
.
C.2 Diagonal Thresholding is Fragile . .
C.3 Covariance Thresholding is Fragile .

. . . . . .
. . . . . .
. . . . . .
C.3.1 Proving covariance thresholding fragile . . . . .

. . . . .
. . . . .
. . . . .

75

78

82

83

. . . . . .
. . . . . .
. . . . . .
. . . . . .

. . . . .
. . . . .
. . . . .
. . . . .

83
. . . . . . 84
. . . . . . 84
. . . . . . 86
. . . . . . 89

D Covariance Thresholding doesnâ€™t work with large signal and small sample size

E Statistical Lower bound for Recovery

F Existence of the Adversarial Distribution of Model 6.6

G Matrix concentration bounds

H Linear Algebra

I Experimental Setup

92

94

97

99

102

104

1 Introduction

Sparse principal component analysis (sparse PCA) is a fundamental primitive in high-dimensional
â„ğ‘‘
statistics. Given a collection of vectors ğ‘¦1, . . . , ğ‘¦ğ‘›
= 1 maximally correlated with the vectors, commonly measured by the empirical
ğ‘£0k
with
k
. The structure we impose on ğ‘£0 is sparsity, that is, an upper bound on the
ğ‘¦ğ‘– , ğ‘£0i}ğ‘–
variance of
number of its non-zero entries.

â„ğ‘‘, we seek a â€œstructuredâ€ direction ğ‘£0 âˆˆ

{h

âˆˆ

âˆˆ[

ğ‘›

]

0, Idğ‘‘
(

Spiked covariance model. A widely studied statistical model for sparse PCA is the spiked covari-
ance model (also called Wishart model). Here, ğ‘¦1, . . . , ğ‘¦ğ‘› are independent draws from the distribution
â„ğ‘‘. (For simplicity, we will assume that
ğ‘
ğ‘£ for ğ‘£0 with correlation1
the sparsity parameter ğ‘˜ is known.) The goal is to compute an estimate
Ë†
2 > ğ‘ for an absolute constant ğ‘ > 0. (Here, we
bounded away from 0 so that
k
square the inner product because ğ‘£0 is identiï¬able only up to sign.)

for an unknown ğ‘˜-sparse unit vector ğ‘£0 âˆˆ
= 1 and

ğ‘£, ğ‘£0i
h Ë†

ğ‘£0ğ‘£ğ‘‡
0 )

ğ‘£
k Ë†

+

ğ›½

Â·

In order to simplify our discussion, we hide multiplicative factors logarithmic in ğ‘‘ using
. Similarly, we hide absolute constant multiplicative factors using the standard
, Î©

the notation Ëœğ‘‚
notations ., ğ‘‚

and Î˜

(Â·)
(Â·)

(Â·)

.
(Â·)

ğ‘›

{

ğ‘¦ğ‘–

If we ignore computational eï¬ƒciency, we can achieve optimal statistical guarantees for sparse
PCA in the spiked covariance model by the following kind of exhaustive search: among all ğ‘˜-by-ğ‘˜
principal submatrices of the empirical covariance matrix of the vectors
, ï¬nd one with
maximum eigenvalue and output a corresponding eigenvector (e.g., [AW09, BR13a, BR13b]). This
procedure achieves constant correlation with high probability as long as ğ‘› >
.
})
However, the running-time is exponential in ğ‘˜. When the number of samples ğ‘› is signiï¬cantly
smaller than the ambient dimension ğ‘‘ as well as the sparsity parameter ğ‘˜, an alternative approach
is to ï¬nd a unit vector ğ‘¢ such that ğ‘¢Tğ‘Œ is close to a ğ‘˜-sparse vector. This procedure also works for
ğ‘› >

Ëœğ‘‚
The spiked covariance model exhibit a sharp transition in the top eigenvalue for ğ‘› & ğ‘‘

ğ›½2 (called
BPP transition [BBP05] in reference to the authorsâ€™ names). In this regime, called strong-signal regime,
the following spectral algorithm matches the optimal statistical guarantees of exhaustive search:
compute the top right singular vector of ğ‘Œ and restrict it to the ğ‘˜ largest entries [KNV+15]. We refer
to this algorithm as SVD with thresholding.

and the running time is exponential in ğ‘›.

ğ›½, ğ›½2

ğ›½, ğ›½2

min

min

Ëœğ‘‚

}ğ‘–

})

âˆˆ[

/

/

ğ‘˜

ğ‘˜

{

{

(

(

]

Whenever ğ‘› . ğ‘‘

ğ›½2 , principal component analysis of

cannot be used to recover ğ‘£0.
One of the best known polynomial-time algorithms for this regime (called weak-signal-regime) is
diagonal thresholding [JL09]: restrict the empirical covariance matrix to the principal submatrix
that contains the ğ‘˜ largest diagonal entries and output the top eigenvector of this submatrix. This
algorithm succeeds with high probability whenever ğ‘› & ğ‘˜2
ğ‘˜ â€” almost quadratically worse than
exhaustive search.2 Similar guarantees were shown to be achievable in polynomial time through

ğ›½2 log ğ‘‘

}ğ‘–

ğ‘¦ğ‘–

âˆˆ[

{

ğ‘›

]

1Instead of asking for the correlation to be bounded away from 0, we could also ask for it to approach 1. Alternatively,
we could ask to recover the support of ğ‘£0. At the granularity of our discussion here, these measures of success are
equivalent in most regards.

2We remark that [DKWB19] provides an algorithm that interpolates between Diagonal Thresholding and brute force
) if
log ğ‘‘. Whenever our discussion will revolve around polynomial time algorithms, we will simply talk about

log ğ‘‘, the algorithm recover the sparse vector in time ğ‘‘ğ‘‚
/

search. Concretely, given any natural number ğ‘¡ 6 ğ‘›
ğ›½ & ğ‘˜
âˆšğ‘¡ğ‘›
Diagonal Thresholding.

(

ğ‘¡

p

1

a semideï¬nite relaxation [dGJL05, AW09] (which we refer to as the basic SDP) (see Section 4
for a precise formulation). A large and diverse body of work [AW09, CMW13, BR13a, KNV+15,
HKP+17a, DKWB19] has been dedicated to the question of understanding if this quadratic gap
between the sample sizes required for computationally eï¬ƒcient and ineï¬ƒcient methods is inherent
or if better polynomial-time algorithms exist for this problem. Hardness results addressing this
question take two forms: either reductions from conjecturally hard problems, such as planted clique
[BR13a] or concrete lower bounds against restricted classes of algorithm such as the sum-of-squares
[MW15, HKP+17b] or low-degree polynomials [DKWB19].

While these results provide evidence that a quadratic gap between polynomial-time algorithms
and exhaustive search is inherent in the weak signal regime, it turns out that a logarithmic im-
provement over diagonal thresholding is possible (for a broad parameter range): in the regime
ğ‘˜ 6 âˆšğ‘‘
2, a more sophisticated algorithm called covariance thresholding [KNV+15, DM14] succeeds
/
for ğ‘› & max
. See Theorem G.12. This turns into an asymptotic improvement over
, but requires the constraint ğ‘› > ğ‘˜2. For
) 6 ğ‘˜2 6 ğ‘œ
diagonal thresholding in the settings ğ‘‘1
1
âˆ’
)
example, if ğ›½ = 1 and ğ‘˜2 = ğœ€ğ‘‘ for some small enough ğœ€ > 0, covariance thresholding works with
ğ‘› & ğ‘˜2 log

ğœ€ and diagonal thresholding requires ğ‘› & ğ‘˜2 log ğ‘‘.

ğ‘˜2
ğ›½2 log ğ‘‘

ğ‘˜2 , ğ‘˜2

ğ‘‘

ğœ€

n

o

(

ğ‘œ

(

1
/
(

, while SVD requires ğ‘› & ğ‘˜2
)

/

Adversarial entry-wise perturbations.
In a seminal work, Huber [Hub81] asked how the guaran-
tees of estimatorsâ€”designed to work under the assumption of observing Gaussian noiseâ€”would
change if the data were roughly normal, but not exactly so, thus broadening the circumstances
under which the performance of an estimator should be judged. This is especially relevant if we
consider that in many real world problems, data may be preprocessed, or the precision of an indi-
vidual input may be limited. For example, digital images may use few bits to encode a pixel and
discard all residual information. For these reasons, it is not desirable for an estimator to drastically
change its response as the input changes between ğ‘Œ and ğ‘Œ
ğ¸ for a small perturbation matrix ğ¸. In
this sense, the robustness of an estimator is an important aspect for understanding its performances
in real-world environments [Mor07, FMDF16].

+

It turns out that the algorithmic landscape for sparse PCA changes drastically in the presence
of adversarial perturbations, where an adversary may change each entry of the input vectors
ğ‘¦1, . . . , ğ‘¦ğ‘› by a small amount. On the one hand, exhaustive search and the basic SDP continue to
give the same guarantees as in the vanilla single-spike model. On the other hand, all aforementioned
thresholding algorithms are highly sensitive to small adversarial perturbations.

âˆšğ‘›

Concretely, in the strong signal regime ğ›½ . ğ‘‘
1
/
(

ğ‘›, it is possible to adversarially perturb the vectors
ğ‘¦1, . . . , ğ‘¦ğ‘› by at most Ëœğ‘‚
per entry such that SVD with thresholding achieves only vanishing
correlation. Indeed an adversarial perturbation with this eï¬€ect can be viewed as a whitening
transformation and corresponds to a natural generative process for ğ‘¦1, . . . , ğ‘¦ğ‘›, where the vectors
are chosen randomly from an ğ‘›-dimensional subspace containing an approximately sparse vector
(see Section 6). We also show that adversarial perturbations of this magnitude can fool diagonal
thresholding and covariance thresholding (see Appendix C).

/

)

2

ğ›½, 1

min

ğ›½
ğ‘˜ Â·

Sparse eigenvalues certiï¬cates.
It is remarkable to notice the stark contrast that appears when
instead adversarial perturbations are used against the basic SDP3, indeed it is easy to show that
the algorithm succeeds whenever adversarial perturbations are bounded (in absolute value) by
. If, for example, we assume ğ›½ > 1 and consider the regime in which diagonal
}
thresholding works, that is ğ›½ >
q
, this bound means the algorithm can aï¬€ord perturbations
)
bounded by ğ‘‚
1
. This is even more remarkable when one notices that for perturbations larger
(
)
/
4
ğ‘›1
an adversary could plant a matrix with ğ‘˜-sparse norm greater than ğ›½ğ‘›, thus fooling
than Ëœğ‘‚
/
)
even the exhaustive search algorithm(see Appendix C) (moreover, this adversary can completely
remove the signal from ğ‘Œ).

1
/
(

4
ğ‘›1
/

âˆšğ‘›

Ëœğ‘‚

p

/

ğ‘˜

{

(

Considering these observations, it is only natural to ask what is the reason that makes some algo-
rithms robust4 to corruptions while others turn out to be highly susceptible to small perturbations
in the samples. This lead us to the central questions of this paper:

Is there some inherent property that makes an algorithm resilient to adversarial perturbations?

In the context of Sparse PCA, we answer this question showing how algorithms that come with
certiï¬cates of sparse quadratic forms5 are intrinsically better in the sense that small perturbations â€“
which by virtue of being small cannot signiï¬cantly change the sparse eigenvalues of the instance â€“
cannot be used to fool them. In contrast, fragile algorithms â€“ which do not produce such certiï¬cates
â€“ may be fooled by adversarial perturbations into outputting an estimation uncorrelated with the
sparse vector ğ‘£0.

We remark that the insight obtained in this analysis also led us to new improvements in the

single spiked covariance model.

Certiï¬cation and the cost of resilience. The robustness of semideï¬nite programs had already
been noted in the literature. For the stochastic block model, eï¬ƒcient spectral algorithms (see
[AS16]) are known to recover the partitions up to the (conjectured) computational threshold.6
However, few adversarial edge deletions and additions can fool such estimators. On the other
hand, algorithms based on semideï¬nite programming were shown to be resilient to adversarial
perturbations[FK01, GV14, MPW16, MS16, MMV16, BMR19], albeit far from the Kesten-Stigum
thresold in general settings.7 The underlying question of this line of work is whether the additional
property of resilience comes "for free".

In the context of this paper, with the idea of certiï¬cation mechanisms being a suï¬ƒcient al-
gorithmic property for adversarial resilience, it becomes relevant to look into the limitations of
certiï¬cation algorithms as well. For the Sherrington-Kirkpatric problem [SK75] â€“ the problem of

3We remark that a certain informal notion of robustness to entry-wise perturbations of the basic SDP program
was already argued in [dGJL05]. Additionally, in [BR13a] the authors observed that the algorithm is robust to small
perturbations of the empirical covariance matrix. We allow here more general perturbations.

4In this paper we will interchangeably use the terms robust and resilient.
5For a matrix ğ‘€
âˆˆ
ğ‘˜-sparse norm of ğ‘€ as

ğ‘‘ we study the values of the quadratic form
max

ğ‘€ğ‘£
. We sometimes refer to the ğ‘˜-sparse unit vector ğ‘£ that maximizes

2 at ğ‘˜-sparse vectors ğ‘£. We deï¬ne the
as

ğ‘€ğ‘£

ğ‘€ğ‘£

â„ğ‘‘

Ã—

k

k

k

k

a sparse eigenvector, and to the corresponding value as a sparse eigenvalue.

6Called the Kesten-Stigum threshold.
7Another qualitative diï¬€erence between the semideï¬nite programs studied in the paper above and other families of

=1,ğ‘£ ğ‘˜-sparsek

k

ğ‘£

k

k

algorithms is the resilience to monotonic perturbations (see [FK01, MPW16] ).

3

maximizing the quadratic form ğ‘¥Tğ‘Š ğ‘¥ where ğ‘¥
and ğ‘Š is a symmetric random matrix
âˆˆ
with iid Gaussian entries above the diagonal â€“ [Mon19] showed (modulo a reasonable conjecture)
(cid:8)
that for any ğœ€ > 0 there exists a polynomial-time optimization algorithm returning a value ğœ€-close
to the optimum. Conversely, [BKW20a] proved that no low-degree polynomial can obtain an ğœ€-
close certiï¬cate for the problem. Thus suggesting that certiï¬cation may be a inherently harder task
than optimization.

1
/

âˆšğ‘›

Â±

(cid:9)

ğ‘›

For sparse PCA in the strong signal regime, we observe a strikingly steep statistical price to pay
for robustness, in the form of a lower bound on the guarantees of low-degree polynomials. That is
a fundamental separation between the power of fragile and resilient algorithms.

1.1 Results

So far, we have generically said that an algorithm is "robust" if it recovers the planted signal
even in the presence of malicious noise. However, several issues arise if one tries to make this
vague deï¬nition more concrete. At ï¬rst, one could say that robust algorithms achieve comparable
guarantees both in the presence and the absence of adversarial corruptions. Yet, in general, this
interpretation makes little sense. Malicious perturbations may remove part of the signal, making the
guarantees of the fragile settings statistically impossible to achieve or â€“as we will see for the sparse
PCA in certain regimesâ€“ they might make the goal of achieving such guarantees computationally
much harder, thus at the very least forcing us to spend a signiï¬cantly higher amount of time to
obtain the same aforementioned guarantees.

For this reason, in many settings it will make sense to say that an algorithm is resilient if it
recovers the sparse signal in the presence of adversarially chosen perturbations even though its
guarantees may not fair well when compared to those achievable in the fragile settings.

The second fundamental aspect concerns the desirable degree of robustness that an algorithm
should possess. Indeed, any reasonable algorithm can likely tolerate suï¬ƒciently small adversarial
perturbations. Therefore, it is important to quantify the magnitude of the perturbations we ask
algorithms to tolerate. Here, we also expect this magnitude to decrease monotonically with the
signal strength ğ›½. A natural concrete way to formalize this idea is the following: the algorithm
should be expected to obtain correlation bounded away from zero, as long as ğ‘£0 remains the principal sparse
component. That is, as long as the vector maximizing the ğ‘˜-sparse norm of ğ‘Œ is correlated with ğ‘£0,
then the algorithm should be able to output an estimator correlated with ğ‘£0.

Concretely, these observations lead us to the following problem formulation.

Problem 1.1 (Robust sparse PCA). Given a matrix of the form

ğ‘Œ = ğ‘Š

â„ğ‘‘ is a unit ğ‘˜-sparse vector,

T
ğ›½ğ‘¢0ğ‘£0

+

ğ¸, where

+

p

ğ‘

0, Idğ‘›
(

)

is a standard Gaussian vector,

ğ‘

0, 1
)
(

ğ‘›

ğ‘‘ is a Gaussian matrix and ğ‘Š , ğ‘¢0, ğ‘£0 are distributionally independent,

Ã—

â€¢ ğ‘£0 âˆˆ
â€¢ ğ‘¢0 âˆ¼
â€¢ ğ‘Š

âˆ¼

(1.1)

4

â€¢ ğ¸

âˆˆ

â„ğ‘›

Ã—

ğ‘‘ is an arbitrary perturbation matrix satisfying8

Return a unit vector

ğ‘£ having non-vanishing correlation with ğ‘£0.
Ë†

.

ğ¸

k

kâˆ

q

p

ğ›½

ğ‘˜

/

Â·

min

{

ğ›½, 1

.

}

(1.2)

)

ğ‘˜

/

ğ›½

(
p

ğ‘›, ğ‘‘, ğ‘˜, ğ›½, ğ›¿, ğ‘

To get an intuition why bound Eq. (1.2) is canonical, observe that for ğ›½ > Î©
1
)
(

adversarial
could remove all information about ğ‘£0 (see Section 2.1). With
perturbations of magnitude Ëœğ‘‚
this formalization of the problem we can now unambiguously deï¬ne robust algorithms. Speciï¬cally,
we say that an algorithm is
, with
)
probability at least ğ‘ it outputs a unit vector

ğ‘£ such that 1
Ë†
Note that the exhaustive search algorithms described in introduction can also recover ğ‘£0 in
the presence of the adversarial matrix ğ¸ from Problem 1.1. So we can assume that ğ‘› > log ğ‘‘ since
) using exhaustive search if ğ‘› >
otherwise Problem 1.1 can be solved in time ğ‘‘ğ‘‚
1
.
})
To better keep track of the multiple results presented in the section, we provide three tables
summarizing the results of this works, each result is then individually discussed in the paragraphs
below.

â€“perturbation resilient if, for parameters
)
ğ‘£, ğ‘£0i

ğ‘›, ğ‘‘, ğ‘˜, ğ›½

2 6 ğ›¿.

ğ›½, ğ›½2

âˆ’ h Ë†

min

Ëœğ‘‚

/

ğ‘˜

{

(

(

(

(

Strong Signal Regime

Algorithm

Succeeds if

SVD with thresholding

ğ›½ &

Sum of squares, Theorem 1.2

ğ‘‘
ğ‘› +
ğ‘‘
ğ‘˜

q
ğ›½ & ğ‘˜
âˆšğ‘›

Running
Time

ğ‘‚

ğ‘›ğ‘‘ log ğ‘›

logğ‘¡

1 ğ‘›
+

ğ‘¡ ğ‘¡

Â·

Â·

ğ‘¡

ğ‘‘ğ‘‚
(cid:0)
(

)

ğ‘˜ log ğ‘‘
ğ‘›
ğ‘¡
for ğ‘‘ & ğ‘›ğ‘¡

1
/

Spectral
Theorem 1.8

algorithm,

ğ›½ & ğ‘˜
âˆšğ‘›

(cid:1)

3
1
/

(cid:0)

ğ‘‘
ğ‘˜

for ğ‘‘ & ğ‘›3 log ğ‘‘ log ğ‘›

ğ‘‚

ğ‘›ğ‘‘ log ğ‘›

(cid:16)

Resilient

No

Yes

*9

(cid:1)

(cid:17)

(cid:0)

(cid:1)

Table 1: Algorithmic landscape in the strong signal regime. The spectral algorithm is provably
resilient to the adversary used to fool SVD with thresholding but we do not expect it to be resilient
to arbitrary adversaries.

8In non-robust settings, we simply enforce the constraint
9Resilient to the distribution of Theorem 6.7

= 0.

ğ¸

k

kâˆ

5

Algorithm

(Generalized)
thresholding

diagonal

Covariance thresholding

Basic SDP, Theorem 1.4

Sum of squares, Theorem 1.5

Low-degree
Theorem 1.6

polynomials,

Weak Signal Regime

Succeeds if

ğ›½ & ğ‘˜
âˆšğ‘›

ğ‘¡

Â·

ğ›½ & ğ‘˜
âˆšğ‘›

log ğ‘‘ for ğ‘¡ 6 1

ln ğ‘‘ min

ğ‘‘, ğ‘›

{

}

p
log ğ‘‘

ğ‘˜2 for ğ‘˜ . âˆšğ‘‘ and ğ‘˜ . âˆšğ‘›

q
ğ›½ & min

2

log

ğ‘˜
âˆšğ‘›
(cid:16)
log ğ‘‘ for ğ‘¡ 6 1

r

+

(cid:26)

ğ‘‘
ğ‘›

ğ‘‘
ğ‘˜2 +
(cid:17)
ln ğ‘‘ min
ğ‘œ

ğ‘‘
ğ‘›

, ğ‘‘
ğ‘› +
ğ‘‘, ğ‘›

(cid:27)

q
{
}
) . ğ‘˜2 . ğ‘‘
1

(

log ğ‘‘
log ğ‘› for ğ‘‘1
âˆ’

Â·

ğ‘¡

ğ›½ & ğ‘˜
âˆšğ‘›
ğ›½ & ğ‘˜
log ğ‘‘
p
âˆšğ‘›
and ğ‘› & log5 ğ‘‘
q

ğ‘˜2 +

Running
Time

Resilient

ğ‘›ğ‘‚

)ğ‘‘ğ‘‚
1

(

(

ğ‘¡

) No

ğ‘›ğ‘‚

)ğ‘‘ğ‘‚
1

1
) No

(

(

ğ‘›ğ‘‚

)ğ‘‘ğ‘‚
1

1
)

(

(

ğ‘›ğ‘‚

)ğ‘‘ğ‘‚
1

(

(

ğ‘¡

)

Yes

Yes

ğ‘›ğ‘‚

)ğ‘‘ğ‘‚
1

1
) No

(

(

Table 2: Algorithmic landscape in the weak signal regime.

Computational Lower Bounds for Polynomials

Work

Polynomials of degree ğ· cannot distinguish if

Settings

Fragile

[DKWB19]

Fragile

Theorem 1.7

Resilient

Theorem 1.3

ğ›½ .

ğ‘‘
ğ‘› ,

(cid:26)q

ğ‘‘
ğ‘› ,

ğ›½ .

(
q
ğ›½ 6 ğ‘‚
ğ‘›0.99ğ‘¡
1
âˆ’

(cid:16)

ğ‘˜
âˆšğ·ğ‘›

ğ‘˜ log

(cid:27)
2
+
(cid:16)
âˆšğ·ğ‘›

ğ·ğ‘‘
ğ‘˜2

ğ‘˜
âˆšğ‘›

ğ‘‘
ğ‘˜

ğ‘¡

1
/

(cid:0)

(cid:1)

(cid:17)

(cid:17)

)
for ğ›½ğ‘›

/

ğ‘˜ 6 ğ‘›0.49 and ğ‘‘ 6

Up to degree
ğ· 6 ğ‘œ

ğ‘›

(

)

ğ· 6 ğ‘›

log2 ğ‘›

ğ· 6 ğ‘›0.001

Table 3: Computational landscape for low-degree polynomials.

Resilient algorithms in the strong signal regime. With the above discussion in mind, one may
ask whether the same guarantees known for the single spike covariance model may also be achieved
in the presence of adversarial perturbations. In the strong signal regime ğ›½ &
ğ‘›, this amounts
to ï¬nding a robust and eï¬ƒcient algorithm that achieves the same guarantees as SVD with thresh-
olding. As we will see however, this is most likely impossible. That is, we will provide compelling
evidence that resilient algorithms cannot match the guarantees of fragile algorithms in the strong signal
regime.

p

ğ‘‘

/

ğ‘› . ğ›½ . ğ‘‘

ğ‘‘

Since for

ğ‘› adversarial perturbations of the order Ëœğ‘‚
can change the top
eigenvalue of the covariance matrix, PCA arguments cannot be used to obtain resilient algorithms.
Thus intuitively, this suggests that diï¬€erent kinds of certiï¬cates are needed.

1
/
(

p

/

/

)

âˆšğ‘›

ğ‘¡

We provide a Sum-of-Squares algorithm that recovers in time ğ‘‘ğ‘‚

) the sparse vector whenever
ğ‘› & ğ‘˜
. The key contribution is indeed an eï¬ƒcient algorithm to certify
)
upper bounds on random quadratic forms. For subgaussian10 low-rank quadratic forms, these
upper bounds approach information-theoretically optimal bounds.

and ğ‘‘1
/

Î©
Ëœ
(

ğ‘¡ >

ğ›½ Â·

1
/

ğ‘›

ğ‘‘
ğ‘˜

ğ‘¡

(cid:1)

(cid:0)

(

ğ‘¡

10Formally we require a stronger property, we need matrices to be certiï¬ably subgaussian.

6

Concretely, for an ğ‘›-by-ğ‘‘ matrix ğ‘Š with i.i.d. Gaussian entries, with high probability the degree-
ğ‘¡

ğ‘¡ sum-of-squares algorithm (with running time ğ‘‘ğ‘‚
)) certiï¬es an upper bound of ğ‘‚
ğ‘˜
(
Î©
on the quadratic form ğ‘„
ğ‘›
Ëœ
(
certiï¬cates, a robust algorithm for Sparse PCA follows then as a speciï¬c corollary.

2 over all ğ‘˜-sparse unit vectors ğ‘¥ if ğ‘‘1
/

1
ğ‘¡
/
)âˆ’
Â· (
)
. With these
)

ğ‘Š ğ‘¥

ğ‘¡ >

=

ğ‘¥

ğ‘‘

/

ğ‘˜

k

k

(

)

Â·

(

ğ‘¡

It is important to notice how this result for sparse PCA is interesting regardless of its resilience
, the algorithm approaches the information theoretic optimal
)

properties. As ğ‘¡ approaches log

ğ‘‘

/

ğ‘˜

(

ğ‘˜
ğ›½ Â·

bound ğ‘‚

log

ğ‘‘

ğ‘˜

. For example, consider the case ğ‘› = 2

(

(

/
of Squares algorithm works in time ğ‘‘
guarantees, while exhaustive search takes time exponential in ğ‘›.

log2 ğ‘›
(

= ğ‘›ğ‘‚

âˆšlog ğ‘‘

))

ğ‘‚

(cid:16)

(cid:16)

(cid:17)

Î˜

âˆšlog ğ‘‘

. If also ğ‘‘
ğ‘˜

= 2

(cid:17)

Î˜

âˆšlog ğ‘‘

(cid:16)

(cid:17)

, the Sum

) with information theoretically optimal

The speciï¬c algorithmic result is shown in the following theorem.

Theorem 1.2 (Perturbation Resilient Algorithm in the Strong Signal Regime). Given an ğ‘›-by-ğ‘‘ matrix
ğ‘Œ of the form,

ğ‘Œ =

ğ›½

T
ğ‘¢0ğ‘£0

Â·
+
â„ğ‘‘, a Gaussian matrix ğ‘Š

+

ğ‘Š

ğ¸ ,

ğ‘‘ satisfying

ğ‘

âˆ¼
ğ¸
kâˆ

k

0, 1
)
(
.
ğ›½

ğ‘›

ğ‘‘, a vector ğ‘¢0 âˆˆ
Ã—
ğ›½, 1
ğ‘˜
}
{
Â·

min

/

â„ğ‘› independent
.

for ğ›½ > 0, a unit ğ‘˜-sparse vector ğ‘£0 âˆˆ
2 = Î˜
of ğ‘Š with
For ğ‘¡

, and a matrix ğ¸
ğ‘¢0k
)
k
â„• suppose that ğ‘‘ & ğ‘›ğ‘¡ logğ‘¡
1
+
âˆˆ

ğ‘›

(

p
â„ğ‘›
Ã—
âˆˆ
ğ‘¡ ğ‘¡ and
ğ‘›
(
)
ğ›½ & ğ‘˜
ğ‘› Â·

p

p

ğ‘¡

1
/

.

ğ‘‘
ğ‘˜

ğ‘¡

Â·

(cid:0)
Then, there exists an algorithm that computes in time ğ‘‘ğ‘‚

ğ‘¡

(cid:1)
) a unit vector

(

â„ğ‘‘ such that

ğ‘£
Ë†

âˆˆ

with probability at least 0.99.

1

ğ‘£, ğ‘£0i

âˆ’ h Ë†

2 6 0.01

ğ‘¡ >

In any case, the fundamental limitation of the above algorithm is the requirement ğ‘‘1
/

.
)
This constraint makes it impossible to match the guarantees of SVD+ thresholding in most regimes,
but a priori it remains unclear why better robust algorithms could not be designed. To provide
formal evidence that without the requirement ğ‘‘1
achieving the kind of guarantees of
/
Theorem 1.2 may be computationally intractable, we make use of a remarkably simple method
(sometimes called analysis of the low degree likelihood ratio), developed in a recent line of work on
the the sum of squares hierarchy [BHK+16, HS17, HKP+17b, Hop18]. That is, we show that in
the restricted computational model of low-degree polynomials11, there is no eï¬ƒcient algorithm
that can improve over the Sum-of-Squares algorithm. This hardness results suggests a fundamental
separation between fragile and resilient algorithms, in other words, an inherent cost to pay in exchange for
perturbation-resilience.

Î©
Ëœ
(

Î©
Ëœ
(

ğ‘¡ >

ğ‘›

ğ‘›

)

ğ‘‘ matrices of the form ğ‘Œ =

ğ¸ where ğ¸ is a perturbation
Concretely, we construct ğ‘›
+
such that, whenever ğ‘‘ is signiï¬cantly smaller than ğ‘›ğ‘¡,
matrix with entries bounded by Ëœğ‘‚
multilinear polynomials of degree at most ğ‘›0.001 cannot distinguish these ğ‘Œâ€™s from ğ‘›
ğ‘‘ Gaussian
(cid:1)
matrices (in the sense that w.h.p. every such polynomial takes roughly the same values under both
distributions). These ideas are formalized in the theorem below.

1
/

âˆšğ‘›

ğ‘Š

p

+

Ã—

Ã—

(cid:0)

T
ğ›½ğ‘¢0ğ‘£0

11As we will argue in Section 3.3, being indistinguishable with respect to low degree polynomials is an important

indication of computational hardness.

7

Theorem 1.3 (Lower Bound for Resilient Algorithms in the Strong Signal Regime, Informal). Let ğ‘¡
be a constant and let ğ‘‘ 6 ğ‘›0.99ğ‘¡

1. Suppose that
âˆ’

ğ›½ 6 ğ‘‚

ğ‘˜
ğ‘› Â·

(cid:18)

ğ‘¡

ğ‘‘

ğ‘˜

ğ‘¡

1
/
)

/

Â· (

.

(cid:19)

and12 ğ›½ğ‘›
/
ğ¸
where
kâˆ

k

ğ‘˜ 6 ğ‘›0.49. Then, there exists a distribution ğœ‡ over ğ‘›

6

Ëœğ‘‚

âˆšğ‘›
1
/

, with the following properties:

â€¢ ğœ‡ is indistinguishable from the Gaussian distribution ğ‘

(cid:0)

(cid:1)

polynomials of degree at most ğ‘›0.001 in the sense described in Section 3.3.2,

ğ‘‘ matrices ğ‘Œ of the form ğ‘Œ =

Ã—

ğ›½ğ‘¢0ğ‘£ğ‘‡

0 +

ğ‘Š

ğ¸

+

p
ğ‘› with respect to all multilinear

ğ‘‘

Ã—

0, 1
)
(

â€¢ the jointly-distributed random variables ğ‘Š, ğ‘¢0, ğ‘£0 are independent,

â€¢ the marginal distribution of ğ‘£0 is supported on unit vectors with entries in

â€¢ the marginal distribution of ğ‘¢0 is uniform over

1, 1

ğ‘› ,

}

{âˆ’

â€¢ the marginal distribution of ğ‘Š is ğ‘

ğ‘›

ğ‘‘.

Ã—

0, 1
)
(

1
/

âˆšğ‘˜
âˆšğ‘˜, 0, 1
/

âˆ’

,

o

n

ğ‘›

Ã—

0, 1
)
(

Informally speaking, Theorem 1.3 conveys the following message. Any resilient algorithm for
Sparse PCA can also distinguish the distribution ğœ‡ over ğ‘›-by-ğ‘‘ matrices ğ‘Œ from the Gaussian
ğ‘‘. Therefore, if an estimator returned by this algorithm can be approximated
distribution ğ‘
by low-degree polynomials, then this algorithm cannot certify upper bounds of sparse eigenvalues
of Gaussian matrices that are sharp enough to signiï¬cantly improve the guarantees of Theorem 1.2.
Sparse principal component analysis is intimately related to the problem of learning Gaussian
mixtures. Indeed, for a vector ğ‘£0 with entries in
, sparse PCA can be rephrased as the
problem of learning a non-uniform mixture ğ‘€ of three subgaussian distributions, one centered at
ğ‘¢0. As we will see, this is true even for the
zero, one centered at
distribution ğœ‡ used in Theorem 1.3 Thus, from this perspective the result also provides interesting
p
insight on the complexity of this problem. The theorem suggests that to distinguish between ğ‘€ and
ğ‘‘, an algorithm would either need ğ‘‘ & ğ‘›ğ‘¡ samples or should
a standard Gaussian ğ‘Š
not be computable by polynomials of degree at most ğ‘›0.001 (see Appendix A).

ğ‘¢0 and the last at

âˆšğ‘˜, 0
1
/

0, 1
)
(

{Â±

p

ğ‘

âˆ’

âˆ¼

ğ›½

ğ›½

/

/

ğ‘˜

ğ‘˜

}

Ã—

ğ‘›

Â·

Â·

/

ğ‘‘

p

Resilient algorithms in the weak signal regime. Having cleared the picture for eï¬ƒcient al-
gorithms in the strong signal regime, we may focus our attention to the weak signal settings
ğ›½ .
ğ‘›. Surprisingly, in these settings adversarial perturbations do not change the computa-
tional landscape of the problem. As a matter of fact, a robust algorithm was already known. In
fragile settings, the basic SDP program was proved (e.g. see [BR13b]) to have the same guarantees
and
as diagonal thresholding. But as the algorithm can certify the upper bounds
â„ğ‘‘ and matrices ğ‘€
ğ‘‘
ğ‘›
ğ‘Š ğ‘¥

ğ‘› log ğ‘‘ over ğ‘˜-sparse unit vectors ğ‘¥

k
(where ğ¶ > 0 is some absolute constant), it is therefore resilient to adversarial corruptions.
We improve this latter upper bound showing that the algorithm can also certify the inequality

2 6 ğ‘˜
k
ğ‘‘, ğ‘Š
Ã—

k
âˆ
0, 1
)
(

ğ‘€ ğ‘¥
â„ğ‘›

ğ‘€
ğ‘

2 6 ğ‘›

Â· k
âˆ¼

k
âˆˆ

ğ¶ ğ‘˜

p

+

âˆˆ

k

Ã—

2

12This constraint is used to ensure that inequalities of the form ğ›½ &

for any ğ· 6 ğ‘›0.001 are never satisï¬ed.
Informally speaking, we restrict our statement to the settings where algorithms with guarantees similar to diagonal
thresholding do not work.

ğ‘˜
âˆšğ‘›

ğ·

Â·

8

2 6 ğ‘›

ğ‘Š ğ‘¥
k
})
and leading us to the following result.
p

ğ‘› log
(

min

ğ¶ ğ‘˜

ğ‘˜2, ğ‘›

+

ğ‘‘

/

{

k

, thus matching the guarantees of covariance thresholding

Theorem 1.4 (Perturbation Resilient Algorithm in the Weak Signal Regime). Given an ğ‘›-by-ğ‘‘ matrix
ğ‘Œ of the form,

ğ‘Œ =

ğ›½

T
ğ‘¢0ğ‘£0

Â·
+
â„ğ‘‘, a Gaussian matrix ğ‘Š

+

ğ‘Š

ğ¸ ,

for ğ›½ > 0, a unit ğ‘˜-sparse vector ğ‘£0 âˆˆ
2 = Î˜
of ğ‘Š with

ğ‘›

ğ‘¢0k
k
Suppose that

, and a matrix ğ¸
)

âˆˆ

(

ğ‘‘ satisfying

Ã—

k

p
â„ğ‘›

ğ‘

âˆ¼
ğ¸
kâˆ

0, 1
)
(
.
ğ›½

ğ‘›

ğ‘‘, a vector ğ‘¢0 âˆˆ
Ã—
ğ›½, 1
ğ‘˜
}
{
Â·

min

/

â„ğ‘› independent
.

p

p

Then, there exists an algorithm that uses the basic SDP program for sparse PCA, and computes in

ğ›½ & min

ğ‘˜
âˆšğ‘› s

log

2

+

(cid:18)

ğ‘‘
ğ‘˜2 +

ğ‘‘
ğ‘›

ğ‘‘
ğ‘› + r

ğ‘‘
ğ‘›

,

(cid:19)

ï£±ï£´ï£²
ï£´
ï£³

â„ğ‘‘ such that

.

ï£¼ï£´ï£½
ï£´
ï£¾

1

ğ‘£, ğ‘£0i

âˆ’ h Ë†

2 6 0.01

polynomial time a unit vector

ğ‘£
Ë†

âˆˆ

with probability at least 0.99.

Theorem 1.4 says that among polynomial time algorithms, in the weak signal regime or when-
ever ğ›½ < 1, the basic SDP achieves the best known guarantees. Furthermore, in contrast to thresh-
olding and PCA algorithms, it works even in the presence of adversarial corruptions.

High degree certiï¬cates in the weak signal regime. A consequential observation of the previous
paragraphs is that, perhaps, the Sum-of-Squares algorithm of larger degree can improve over the
guarantees of the basic SDP even in the weak signal regime. Indeed in many settings, these
guarantees can be improved observing that the (degree ğ‘¡) Sum-of-Squares algorithm can certify
ğ‘›
). Hence oï¬€ering a smooth
upper bounds of the form
k
trade-oï¬€ between sample complexity and running time.

log ğ‘‘ in time ğ‘‘ğ‘‚

2 6 ğ‘›

ğ‘Š ğ‘¥

+

/

ğ‘˜

k

)

ğ‘¡

(

ğ‘¡

(
p

Theorem 1.5 (Perturbation Resilient Algorithm via Limited Exhaustive Search). Given an ğ‘›-by-ğ‘‘
matrix ğ‘Œ of the form,

ğ‘Œ =

ğ›½

T
ğ‘¢0ğ‘£0

ğ‘Š

ğ¸ ,

for ğ›½ > 0, a unit ğ‘˜-sparse vector ğ‘£0 âˆˆ
2 = Î˜
of ğ‘Š with

ğ‘‘ satisfying
Ã—
)
Suppose that for some positive integer ğ‘¡ 6 1
ğ‘‘, ğ‘›
ln ğ‘‘ min

and a matrix ğ¸

ğ‘¢0k

p
â„ğ‘›

Â·
+
â„ğ‘‘, a Gaussian matrix ğ‘Š
ğ¸
k
,
}

+

âˆˆ

ğ‘›

{

k

(

ğ‘

ğ‘›

0, 1
)
(
.
ğ›½
/

Ã—
ğ‘˜

ğ‘‘, a vector ğ‘¢0 âˆˆ
.
ğ›½, 1
min
}
{
Â·

â„ğ‘› independent

âˆ¼
kâˆ

p

p

Then, there exists an algorithm that computes in time ğ‘›ğ‘‚

)ğ‘‘ğ‘‚
1

(

(

ğ‘¡

) a unit vector

â„ğ‘‘ such that

ğ‘£
Ë†

âˆˆ

ğ›½ & ğ‘˜
âˆšğ‘›ğ‘¡

p

log ğ‘‘ .

with probability 0.99.

1

ğ‘£, ğ‘£0i

âˆ’ h Ë†

2 6 0.01

9

Whenever ğ‘˜2 6 ğ‘‘1
âˆ’

Î©

1
), Theorem 1.5 provides better guarantees than Theorem 1.4 (with worse

(

running time).

It is also interesting to compare this result with the bound of Theorem 1.2. For some ğ‘¡, we can
determine the parameter regimes when one theorem provides better guarantees then the other
for running time ğ‘‘ğ‘‚
1
. Then there exist
+
constants 0 < ğ¶ < ğ¶â€² such that:

2 & ğ‘‘ & ğ‘¡ ğ‘¡ ğ‘›ğ‘¡
+

). Assume that

1ğ‘›ğ‘¡
+

log ğ‘›

log ğ‘›

1
)

1
+

+

(

ğ‘¡

(

ğ‘¡

ğ‘¡

ğ‘¡

ğ‘¡

(cid:0)

(cid:0)
ğ‘›
ğ‘¡ log ğ‘‘, so in this case the guarantees in Theorem 1.5 are

(cid:1)

(cid:1)

â€¢ If ğ‘˜2 6 ğ‘‘
better.

ğ¶ğ‘¡

ğ‘¡, we get ğ‘¡
)

Â·

Â· (

ğ‘¡

1
/

>

ğ‘‘
ğ‘˜

â€¢ If ğ‘˜2 > ğ‘‘

Â·

2

ğ‘› log2 ğ‘›

Theorem 1.2 are better.
(cid:16)

(cid:17)

(cid:0)

ğ¶â€²ğ‘¡

p

(cid:1)
ğ‘¡, we get ğ‘¡

)

ğ‘¡

1
/

<

ğ‘‘
ğ‘˜

Â·

(cid:0)

(cid:1)

p

ğ‘›
ğ‘¡ log ğ‘‘, so in this case the guarantees in

Â· (

Informally speaking, these conditions show that the guarantees in Theorem 1.2 are better when
the vector is only mildly sparse: ğ‘˜2

ğ‘‘, and the number of samples ğ‘› is very small.

Theorem 1.5, along with Theorem 1.4 and Theorem 1.2 provides also a nice consequence,
namely it allows us to state that for the problem of Sparse PCA, the Sum-of-Squares algorithm achieves
the best known guarantees among perturbation resilient polynomial time algorithms. Furthermore, under
the restrict computational model of low-degree polynomials, these guarantees are nearly optimal.

â‰«

1.1.1 Sharp bounds for the Wishart model

In the regime where ğ‘˜ 6 âˆšğ‘‘, covariance thresholding succeeds for ğ›½ & ğ‘˜
ğ‘˜2 . This turns
âˆšğ‘›
ğ‘œ
into an asymptotic improvement over diagonal thresholding in the settings ğ‘‘1
q
but
(
âˆ’
requires a constraint on the sample complexity of the form ğ‘› > ğ‘˜2, for which there is no evidence in
the known lower bounds. This picture raises the following questions: can we obtain guarantees of
ğ‘˜2 even for ğ‘› 6 ğ‘˜2? And furthermore, can we improve over this logarithmic
the form ğ›½ & ğ‘˜
âˆšğ‘›
factor?

log ğ‘‘
) 6 ğ‘˜2 6 ğ‘œ
1

log ğ‘‘

ğ‘‘

)

(

q

Studying low-degree polynomials we improve over this incomplete picture providing a new al-
log ğ‘‘
ğ‘˜2

gorithm which succeed in recovering the sparse vector in polynomial time whenever ğ›½ & ğ‘˜
âˆšğ‘›

log
1
/

ğ‘‘
ğ‘˜2

and ğ‘› & ğ‘‘
in a signiï¬cantly large set of parameters.

(cid:17) +

(cid:16)

log5 ğ‘‘. Thus obtaining an asymptotic improvement over diagonal thresholding

q

log ğ‘‘
Concretely, the algorithm improves over the state-of-the-art whenever ğ‘‘1
ğ‘˜2
/
Î©

log5 ğ‘‘ . ğ‘› .
1
). In other words, the algorithm requires much fewer samples than covariance thresholding.

+

(

ğ‘‘1
âˆ’
This result is captured by the theorem below.

Theorem 1.6 (Polynomials based Algorithm for the Strong Signal Regime). Given an ğ‘›-by-ğ‘‘ matrix
ğ‘Œ of the form,

for a unit vector ğ‘£0 âˆˆ
6 ğ‘‚
2 = 1, ğ”¼ ğ‘¢4
ğ”¼ ğ‘¢ğ‘–
1
ğ‘–
)
(

â„ğ‘‘ with entries in

and a matrix ğ‘Š

{Â±

âˆˆ

ğ‘Œ =

ğ›½

ğ‘¢0ğ‘£ğ‘‡

ğ‘Š ,

Â·
âˆšğ‘˜, 0
p
1
/
â„ğ‘›

0 +
, a vector ğ‘¢0 with i.i.d. entries satisfying ğ”¼ ğ‘¢ğ‘– = 0,
= 1,

ğ‘‘ with i.i.d. entries satisfying ğ”¼

= 0, ğ”¼

}

ğ‘Šğ‘–ğ‘—

Ã—

ğ‘Š 2
ğ‘–ğ‘—

(cid:2)

(cid:3)

h

i

10

such that ğ‘Š and ğ‘¢0 are independent; suppose that ğ‘› & log5 ğ‘‘, ğ‘‘1
âˆ’

ğ‘œ

) 6 ğ‘˜2 6 ğ‘‘
1

(

2, and

/

ğ›½ & ğ‘˜

âˆšğ‘› s

log

ğ‘‘
ğ‘˜2

(cid:18)

+

(cid:19)

log ğ‘‘
log ğ‘›

.

Then, there exists a probabilistic algorithm that computes in polynomial time a unit vector

â„ğ‘‘ such that

ğ‘£
Ë†

âˆˆ

with probability at least 0.99.

1

ğ‘£, ğ‘£0i

âˆ’ h Ë†

2 6 0.01

Along with Theorem 1.6, we provide a ï¬ne-grained lower bound that in many settings matches
the known algorithmic guarantees for the single spiked model. Some relevant lower bounds were
already known. In [BR13a] the authors used a reduction to the planted clique problem to provide
evidence that in the weak signal regime13 eï¬ƒcient algorithms cannot recover the sparse vector if
. In [DKWB19] similar lower bound was obtained: in the weak signal regime low-degree
ğ›½
polynomials cannot succeed if ğ›½ . ğ‘˜
âˆšğ‘›
of diagonal thresholding by a logarithmic factor. Here, we show that whenever ğ‘˜2 6 ğ‘‘1
âˆ’

. This lower bounds fall short of matching the guarantees
1
),

ğ‘˜
âˆšğ‘›

â‰ª

Î©

(

polynomials of degree ğ‘‚

cannot recover the sparse vector for ğ›½ . min

ğ‘‘

ğ‘› , ğ‘˜
âˆšğ‘›

log ğ‘‘

.

(cid:27)
In particular, we provide strong evidence that in the weak signal regime, in the settings where
our polynomials based algorithm does not improve over the state-of-the-art, the known eï¬ƒcient
algorithms (diagonal thresholding, basic SDP) are optimal up to constant factors.

(cid:26)q

p

log ğ‘‘
(

)

Theorem 1.7 (Lower Bound for Standard Sparse PCA, Informal). There exists a distribution ğœ‡ğ‘˜ over
ğ‘˜-sparse ğ‘‘-dimensional unit vectors such that if ğ‘Œ is an ğ‘›-by-ğ‘‘ matrix of the form

ğ‘Œ =

T
ğ‘¢0ğ‘£0

ğ›½

Â·

ğ‘Š ,

+

0, Idğ‘›
for a vector ğ‘£0 sampled from ğœ‡ğ‘˜, a Gaussian matrix ğ‘Š
)
(
such that ğ‘£0, ğ‘¢0, ğ‘Š are distributionally independent, then the distribution of ğ‘Œ is indistinguishable from
log2 ğ‘› in the sense
the Gaussian distribution ğ‘
Ã—
described in Section 3.3.2, whenever

ğ‘‘ with respect to all polynomials of degree ğ· 6 ğ‘›

ğ‘‘ and a Gaussian vector ğ‘¢0 âˆ¼

0, 1
)
(

0, 1
)
(

ğ‘

ğ‘

âˆ¼

/

Ã—

ğ‘›

ğ‘›

p

ğ›½ . min

ğ‘‘
ğ‘›

,

ğ‘˜
âˆšğ·ğ‘›

log

2

(cid:18)

ğ‘‘

ğ·

Â·
ğ‘˜2

+

.

(cid:19) )

(r

1.1.2 Additional Results: Practical Algorithms and Experiments

From a practical perspective, the main issue with the results of Theorem 1.2 is the reliance on
solving large semideï¬nite programs, something that is often computationally too expensive to
do in practice for the large-scale problems that arise in machine learning. In the same fashion of
[HSSS16], from the insight of the SoS analysis we develop a fast spectral algorithm (which we will
call SVD-t) with guarantees matching Theorem 1.2 for degree ğ‘¡ 6 3 for some interesting family of
ğ‘›ğ‘‘ log ğ‘›
adversaries. Our algorithm runs in time ğ‘‚
, which for high dimensional settings, can be

13Actually the parameter regime they considered is a proper subset of the weak signal regime.

(cid:0)

(cid:1)

11

considerably faster than algorithms that rely on computing the covariance matrix14. Furthermore,
while not showing robustness of the algorithm (indeed the algorithm cannot certify upper bounds),
we prove that SVD-t succeeds under the adversarial perturbations which are enough to prove
Theorem 1.3. Such adversarial settings are especially interesting since the problem has a nice
geometric description in which the objective is to recover an approximately sparse vector planted in
a random subspace. (see Section 6.6) We remark that it is not known how to generalize the algorithm
for larger ğ‘¡. Finally, we complement this result with experiments on synthetic data which highlights
how in many practical settings the algorithm outperforms (and outruns) diagonal thresholding. The
following theorem presents the guarantees of the algorithm in the spiked covariance model.

Theorem 1.8 (Fast Spectral Algorithm for the Strong Signal Regime, Informal). Given an ğ‘›-by-ğ‘‘
matrix ğ‘Œ of the form,

for ğ›½ > 0, a unit ğ‘˜-sparse vector ğ‘£0 âˆˆ
ğ‘¢0 âˆ¼
0, Idğ‘›
(
Theorem 1.3 for ğ‘¡ = 3.15

0, 1
)
(
such that ğ‘£0, ğ‘¢0, ğ‘Š are distributionally independent, and ğ¸

ğ‘

ğ‘

âˆ¼

âˆˆ

)

ğ‘Œ =

T
ğ›½ğ‘¢0ğ‘£0

ğ‘Š

ğ¸ ,

+
â„ğ‘‘, a Gaussian matrix ğ‘Š

p

+

Suppose that ğ‘‘ & ğ‘›3 log ğ‘‘ log ğ‘›, ğ‘˜ & ğ‘› log ğ‘› and

ğ‘›

ğ‘‘, a Gaussian vector
ğ‘‘ is a matrix from

Ã—
â„ğ‘›

Ã—

ğ›½ & ğ‘˜

3
1
/

.

ğ‘‘
ğ‘˜

(cid:19)

âˆšğ‘› (cid:18)
Then there exits an algorithm that computes in time ğ‘‚

1

ğ‘£0,

ğ‘£
Ë†

i

âˆ’ h

with probability at least 0.99.

Outline and Notation

(

a unit vector

ğ‘£
Ë†

âˆˆ

â„ğ‘‘ such that

ğ‘›ğ‘‘ log ğ‘›

)

6 0.01

We conclude our introduction with an outline of the structure of the paper and some notation.

In Section 2 we give an overview of the techniques and the ideas required to obtain the results.
Preliminary information are presented in Section 3. Section 4 and Section 5 contains the results
for the basic SDP and the Sum-of-Squares algorithms. In Section 6 we show our lower bounds on
polynomials and in Section 7 we use such polynomials to prove Theorem 1.6. Finally, in Section 8
and Section 9 we describe our fast algorithms, formally prove some of their properties and compare
them with known algorithms through experiments.

Additionally, we discuss the relationship with the problem of clustering mixture of Gaussians
in Appendix A. In Appendix B we describe the picture for the Wigner model. Appendix C contains
formal proofs that thresholding algorithms are not robust and Appendix D shows why Covariance
Thresholding fails for small sample size. We also provide an information theoretic bound in
Appendix E.

ğ‘Œ is necessary for covariance thresholding, the diagonal thresholding algorithm can run in

T
14While computing ğ‘Œ
ğ‘‘ğ‘›

ğ‘˜2

.

time ğ‘‚

(

+

)

15More precisely, to prove Theorem 1.3 we consider a speciï¬c distribution over matrices ğ¸ (this distribution depends

on ğ‘£0, ğ‘¢0 and ğ‘Š), and here we mean that ğ¸ is sampled from this distribution.

12

k1 =

k

ğ‘–,ğ‘—

ğ‘‘
âˆˆ[
Ã

](cid:12)
(cid:12)

Notation. We say that a unit vector ğ‘£

â„ğ‘‘ is ï¬‚at if its entries are in
for some ğ‘¡. For a
ğ‘‘, we will denote its entry ğ‘–ğ‘— with ğ‘€ğ‘–ğ‘—. Depending on the context we may refer to
matrix ğ‘€
the ğ‘–-th row or the ğ‘–-th column of ğ‘€ with ğ‘€ğ‘– or ğ‘šğ‘–, we will specify it each time to avoid ambiguity.
ğ‘‘, we
We call

the "absolute norm" of ğ‘€. For a Gaussian matrix ğ‘Š

â„ğ‘›

, 0

ğ‘€

ğ‘

Â±

âˆˆ

âˆˆ

o

n

Ã—

ğ‘›

1
âˆšğ‘¡

ğ‘€ğ‘–ğ‘—

0, 1
)
(

Ã—

âˆ¼

(cid:12)
denote with ğ‘¤1, . . . , ğ‘¤ğ‘‘ its columns. For a vector ğ‘£
(cid:12)
absolute constant multiplicative factors using the standard notations ., ğ‘‚
multiplicative factors logarithmic in ğ‘‘ using the notation Ëœğ‘‚
. For a set ğ‘†
(Â·)
]ğ‘–ğ‘— = ğ‘€ğ‘–ğ‘— if
the matrix with entries ğ‘€
ğ‘€
ğ‘†
ğ‘†
[
[
]
â„ğ‘‘
â„, we deï¬ne ğœ‚ğœ(
â„ğ‘‘
ğ‘‘ and ğœ
ğ‘€
otherwise. For a matrix ğ‘€
) âˆˆ
> ğœ
ğ‘€ğ‘–ğ‘—

â„ğ‘›, we denote its ğ‘—-th entry as ğ‘£ ğ‘—. We hide
, Î©
, we hide
ğ‘‘
, and a matrix
]ğ‘–ğ‘— = 0
ğ‘†
ğ‘‘ to be the matrix with entries

ğ‘‘, we denote by ğ‘€

]
ğ‘†, and ğ‘€

and Î˜
ğ‘‘

(Â·)
âŠ† [
ğ‘–, ğ‘—

(Â·)
] Ã— [

ğ‘€ğ‘–ğ‘—

) âˆˆ

â„ğ‘‘

(Â·)

if

âˆˆ

âˆˆ

âˆˆ

âˆˆ

Ã—

Ã—

Ã—

[

(

ğ‘€

ğœ‚ğœ(

)ğ‘–ğ‘— =

0

(

otherwise.
(cid:12)
(cid:12)

(cid:12)
(cid:12)

Furthermore, we deï¬ne ğœğœ(

ğ‘€

) âˆˆ

â„ğ‘‘

Ã—

ğ‘‘ to be the matrix with entries

ğ‘€

ğœğœ(

)ğ‘–ğ‘— =

ğ‘€ğ‘–ğ‘—

0

(

âˆ’

sign

ğ‘€ğ‘–ğ‘—

ğœ

Â·

(cid:0)

(cid:1)

if

ğ‘€ğ‘–ğ‘—

> ğœ

otherwise.
(cid:12)
(cid:12)

(cid:12)
(cid:12)

Additional notation will be introduced when needed.

Remark 1.9 (Strong and weak signal regimes in robust settings). The attentive reader may have
noticed how the notions of strong and weak signal regime should diï¬€er in the robust settings.
Indeed there is no easy algorithm that looks at the spectrum of ğ‘Œ and begins to work as ğ›½

ğ‘‘

q

ğ‘› . In this sense, in the presence of an adversary the bound ğ›½ .

ğ‘‘
approaches
ğ‘› looses signiï¬cance.
However we will continue using these terms to orientate ourselves and implicitly describe which
are the desirable guarantees an algorithm should possess in a given regime. For this reason, when
talking about weak-signal regime, our discussion will implicitly revolve around settings in which
ğ›½ & ğ‘˜
âˆšğ‘›

log ğ‘‘
ğ‘˜2 .

q

q

2 Techniques

2.1 Perturbation-resilience from Sparse Eigenvalue Certiï¬cates

Here we outline the structure of our Sum-of-Squares algorithm and the basic SDP. We assume the
reader to be familiar with the knowledge in Section 3.

How robust should an algorithm be?
In light of our discussion in Section 1.1, we would like
eï¬ƒcient algorithms to be as resilient as exhaustive search. In order for such brute-force algorithm
to recover the sparse vector ğ‘£0, there must be no other sparse vector ğ‘¥ far from ğ‘£0 such that
. This also means that the adversary should not be able to plant a ğ‘˜-sparse vector ğ‘§
ğ‘Œğ‘£
ğ‘Œğ‘¥
k
far from ğ‘£0 such that
. To see what bound to enforce on the adversarial matrix, ï¬rst
k
observe that if ğ¸ were the zero matrix then

ğ‘Œğ‘£0k

k â‰ˆ k

ğ‘Œğ‘¥

&

k

k

k

=

ğ‘Œğ‘£0 k

k

ğ‘Š ğ‘£0 +
(cid:13)
(cid:13)
(cid:13)

ğ›½ğ‘¢0

&

p
13

(cid:13)
(cid:13)
(cid:13)

p

ğ›½ğ‘›.

ğ‘›

+

Now consider the following adversarial matrix, let ğ‘¥ be a ğ‘˜-sparse unit vector with entries in

âˆšğ‘˜

0,

1
/

Â±

n
o
high probability
ğ‘
sign

ğ‘¥ ğ‘—

ğ‘§ğ‘–

Â·

Â·

(

and such that the intersection between supp

ğ‘Š ğ‘¥

k

k â‰ˆ

âˆšğ‘›. So let ğ‘§ = 1
ğ‘Š ğ‘¥

k
, where ğ‘ > 0 is some parameter that we will choose later. Then
)

k

ğ‘¥

ğ‘£0}
is the empty set. With
{
ğ‘Š ğ‘¥ and deï¬ne ğ¸ as the matrix with entries ğ¸ğ‘–ğ‘— =

and supp

}

{

=

ğ‘Œğ‘¥

k

k

ğ‘Š

ğ¸

ğ‘¥

)

k

+

k(

=

ğ‘Š ğ‘¥

k +

ğ‘âˆšğ‘˜

ğ‘§

âˆšğ‘›

ğ‘âˆšğ‘˜.

â‰ˆ

+

(cid:13)
(cid:13)
whenever âˆšğ‘›
(cid:13)

ğ‘âˆšğ‘˜ >

(cid:17)
ğ‘›

(cid:13)
(cid:13)
ğ›½ğ‘›. The inequality is true for ğ‘ &
(cid:13)
+

k

(cid:16)

+

. In other words, the perturbation matrix must satisfy the bound:

p

Consequently,

ğ‘Œğ‘¥

>

k

ğ‘Œğ‘£0 k

k

k
ğ›½, 1

ğ›½ğ‘›
ğ‘˜ Â·

min

q

(cid:8)p

(cid:9)

ğ¸

k

kâˆ

6

Î©
Ëœ

ğ›½
ğ‘˜ Â·

 r

min

ğ›½, 1

np

.

!

o

(Bound-1)

For a set of parameters ğ‘‘, ğ‘›, ğ‘˜, ğ›½, we call an algorithm perturbation resilient if it can successfully
recover the sparse vector for any adversarial perturbation satisfying bound Bound-1.

k

â†’

ğ¸

k1

Remark 2.1. In the proofs presented in the paper, we will measure adversarial corruptions with the
2, which denotes the largest norm of a column of ğ¸. Clearly this choice allows for a
norm
larger class of adversaries. There are a two main reasons behind our choice. The ï¬rst one being that
the adversarial matrices we consider are more naturally described using such norm. Furthermore,
this norm has a direct correspondence with the inï¬nity norm of the adversarial perturbation in the
covariance matrix. Indeed,
2. This also will allow us to draw a better comparison
between the Wishart and the Wigner model. We remark that the reasoning above can be used as
well to show the bound:

âˆ
(cid:13)
(cid:13)

2
1
â†’

ğ¸Tğ¸

(cid:13)
(cid:13)

=

ğ¸

k

k

ğ¸

k

k1

â†’

2 . min

ğ›½ğ‘›
ğ‘˜ Â·

(r

min

ğ›½, 1

np

.

)

o

(Bound-2)

2.2 Algorithms that Certify Sparse Eigenvalues

For simplicity of the discussion we illustrate the idea of sparse eigenvaluex certiï¬cates for the
T
Wigner model: ğ‘Œ = ğ›¾ğ‘£0ğ‘£0
ğ‘‘
Ã—
and ğ¸ is some matrix with small entries. Denote the set of ğ‘˜-sparse unit vectors by ğ‘†ğ‘˜. The starting
idea is to turn the following intuition into an identiï¬ability proof and then a Sum of Squares
ğ‘£ is a ğ‘˜-sparse unit vector which maximizes ğ‘£Tğ‘Œğ‘£ over ğ‘†ğ‘˜ and ğ›¾ is large enough, then
program: if
Ë†
with high probability

â„ğ‘‘ is a ğ‘˜-sparse unit vector, ğ‘Š

ğ¸, where ğ›¾ > 0, ğ‘£0 âˆˆ

2 > 0.99.

0, 1
)
(

ğ‘Š

ğ‘

âˆ¼

+

+

ğ‘‘

ğ‘£, ğ‘£0i
h Ë†
Concretely, observe that

on one side

on the other

T
ğ‘£0
T
ğ‘£
Ë†

ğ‘Œğ‘£0 = ğ›¾
ğ‘£ = ğ›¾
ğ‘Œ
Ë†

T
ğ‘£0
+
ğ‘£, ğ‘£0i
h Ë†

ğ‘Š ğ‘£0 +
T
ğ‘Š
ğ‘£
+ Ë†

2

ğ‘£
Ë†

T
ğ‘£0

ğ¸ğ‘£0,
T
ğ‘£
+ Ë†

ğ¸

ğ‘£.
Ë†

Combining the two and rearranging we obtain the inequality

2 > 1

ğ‘£, ğ‘£0i
h Ë†

1
ğ›¾

ğ‘‚

âˆ’

max
ğ‘†ğ‘˜
ğ‘£
âˆˆ

(cid:18)

T
ğ‘£

ğ‘Š ğ‘£

+

max
ğ‘†ğ‘˜
ğ‘£
âˆˆ

T
ğ‘£

ğ¸ğ‘£

.

(cid:19)

14

Now, this is where certiï¬ed upper bounds come in to the picture. There is an easy certiï¬cate
(capture by SoS and the basic SDP) of the fact that for any matrix ğ‘€, maxğ‘£
ğ‘˜
Using such bound we get

ğ‘†ğ‘˜ ğ‘£Tğ‘€ğ‘£ 6

kâˆ

ğ‘€

k

âˆˆ

ğ‘£, ğ‘£0i
h Ë†

2 > 1

1
ğ›¾

ğ‘‚

âˆ’

T
ğ‘£

ğ‘Š ğ‘£

max
ğ‘†ğ‘˜
ğ‘£
âˆˆ

(cid:18)

ğ¸

ğ‘˜

k

+

.

kâˆ(cid:19)

(2.1)

Eq. (2.1) already shows how an algorithm that can certify sparse eigenvalues is perturbation re-
ğ‘˜, the inequality becomes
silient (in the sense of the previous paragraph). Indeed for

= ğœ€

ğ¸

ğ›¾

k

kâˆ

Â·

/

ğ‘£, ğ‘£0i
h Ë†

2 > 1

ğ‘‚

ğœ€

(

) âˆ’

âˆ’

1
ğ›¾

ğ‘‚

max
ğ‘†ğ‘˜
ğ‘£
âˆˆ

(cid:18)

T
ğ‘£

ğ‘Š ğ‘£

.

(cid:19)

(2.2)

ğ‘†ğ‘˜ ğ‘£Tğ‘Š ğ‘£ it can obtain.
For the Wishart Model ğ‘Œ =

âˆˆ

T
ğ›½ğ‘¢0ğ‘£0

At this point, the guarantees of the algorithm depend only on the speciï¬c certiï¬ed upper bound
on maxğ‘£

need to work with ğ‘ŒTğ‘Œ
ğ‘ŠTğ‘Š
guarantees of the algorithm depend only on the certiï¬ed upper bound on maxğ‘£
it can obtain. For the rest of our preliminary discussion we go back to the Wishart model.

ğ¸, the reasoning is essentially the same. However we
ğ‘›Id and carefully bound the cross terms. Similar to the Wigner model, the
ğ‘£

ğ‘†ğ‘˜ ğ‘£T

ğ‘›Id

ğ‘Š

p

âˆ’

+

+

âˆ’

âˆˆ

(cid:0)

2.3 New Certiï¬cates via basic SDP

For a matrix ğ‘€

âˆˆ

â„ğ‘‘

Ã—

ğ‘‘, the basic SDP program16

argmax

T
ğ‘Œ
h

ğ‘Œ, ğ‘‹

ğ‘‹

(cid:23)

i

0, Tr ğ‘‹ = 1,

ğ‘‹

k

k1 6 ğ‘˜

can certify two types of upper bound:

(cid:8)

h

(cid:12)
(cid:12)

ğ‘€ , ğ‘‹

ğ‘€ , ğ‘‹

i

6

6

k

ğ‘€

ğ‘€

kâˆ Â·
.

(cid:9)

ğ‘˜

0, Tr ğ‘‹ = 1. These are enough to
The ï¬rst follows using
capture standard principal component analysis as well as diagonal and covariance thresholding.

ğ‘‹

(cid:23)

k

h

i
k1 6 ğ‘˜ and the second applying ğ‘‹

k

k

Speciï¬cally, Eq. (2.5) can be used to certify the upper bound

(cid:17)
obtaining the guarantees of PCA â€“ and Eq. (2.4) the bound
, as
in diagonal thresholding17. Now these results were already known, but surprisingly a combination
ğ‘ŠTğ‘Š
. Thus allowing us
of the two bounds can also be used to show
âˆ’
h
to match the guarantees of covariance thresholding.

ğ‘›Id, ğ‘‹

ğ‘› log

6 ğ‘˜

ğ‘˜2

p

ğ‘‘

/

ğ‘˜

(cid:17)

(cid:16)

i

i

)

(

Â·

Â·

ğ‘ŠTğ‘Š
h
ğ‘ŠTğ‘Š
h

âˆ’

âˆ’
ğ‘›Id, ğ‘‹

ğ‘›Id, ğ‘‹

6 ğ‘‚

ğ‘‘

âˆšğ‘‘ğ‘›

â€“

i
6 ğ‘‚

+
(cid:16)
ğ‘› log ğ‘‘

p

Concretely, using the notation from the introduction,

(cid:1)

(2.3)

(2.4)

(2.5)

16Recall

ğ‘‹

k1 =

k

ğ‘‹ğ‘–ğ‘—

ğ‘–,ğ‘—
17A more careful analysis can get ğ‘˜

âˆˆ[
Ã

ğ‘‘

](cid:12)
(cid:12)

(cid:12)
(cid:12)

T

ğ‘Š

ğ‘Š
h

âˆ’

ğ‘›Id, ğ‘‹

=

i

ğœ‚ğœ
h

ğ‘Š
h

ğ‘Š
T
(cid:0)

ğ‘Š

T

ğ‘Š

, ğ‘‹

ğ‘›Id
T

ğ‘Š

âˆ’
ğœ‚ğ‘¡

ğ‘Š
(cid:1)

i+
, ğ‘‹

.

i

âˆ’

is the "absolute norm".

(cid:0)

(cid:1)

, but we ignore it here.

ğ‘› log
(

ğ‘‘

/

ğ‘˜

)

Â·

p

15

Here ğ‘ŠTğ‘Š

ğœ‚ğ‘¡

ğ‘ŠTğ‘Š

is a matrix with entries bounded (in absolute value) by ğœ for which we

can plug in Eq. (2.4) and get

(cid:0)

(cid:1)

âˆ’

The same argument cannot be used for ğœ‚ğœ

close (up to an addition of ğ‘›
Â·
thresholding. Hence, taking ğœ =

T

ğ‘Š

ğ‘Š
h

ğœ‚ğ‘¡

ğ‘Š

, ğ‘‹

6 ğœ

ğ‘˜

âˆ’

i
, but note that this matrix is suspiciously
Id) to the thresholded covariance matrix obtained in covariance

(cid:0)

Â·

T

ğ‘Š
ğ‘ŠTğ‘Š
(cid:1)

(cid:0)

(cid:1)

and using Eq. (2.5), we get

ğ‘› log

ğ‘˜2

ğ‘‘

(

/

)

p
ğ‘Š

T

ğ‘Š

ğœ‚ğœ
h

ğ‘›Id

, ğ‘‹

i

âˆ’

(cid:0)

(cid:1)

6 ğ‘‚

ğ‘˜

ğ‘› log

r

ğ‘‘
ğ‘˜2 !

,

where we get the spectral bound (almost) for free by the analysis in [DM14].

2.4 New certiï¬cates via higher-level Sum-of-Squares

2.4.1 Certiï¬cates via Certiï¬able Subgaussianity

ğ‘›

Ã—

â„ğ‘‘.

ğ‘‘. In particular we can exploit Gaussian moments bound ğ”¼

The Sum-of-Squares algorithm can certify more reï¬ned bounds on sparse eigenvalues of ğ‘Š
ğ‘
ğ‘¡

0, 1
)
(
â„•, ğ‘¢
âˆˆ
Concretely letâ€™s see how to use such property to obtain an identiï¬ability proof of a bound on
ğ‘‘ be the indicator
the ğ‘˜-sparse norm of ğ‘Š. To this end let ğ‘£ be a ğ‘˜-sparse vector and let ğ‘ 
vector of its support (here we drop the subscript ğ‘£0 to ease the notation). Using Cauchy-Schwarz,

âˆ¼
2ğ‘¡ for all

ğ‘Šğ‘– , ğ‘¢
h

2ğ‘¡ 6 ğ‘¡ ğ‘¡

0, 1

âˆˆ {

Â· k

âˆˆ

ğ‘¢

}

k

i

4 =

ğ‘Š ğ‘£

k

k

ğ‘£ğ‘–

ğ‘Šğ‘– , ğ‘Š ğ‘£
h

i!

Ã•ğ‘–6ğ‘‘

Then applying Holderâ€™s inequality with 1
/
ğ‘˜,

2

6

ğ‘£2
ğ‘–

!  

Ã•ğ‘–6ğ‘‘
ğ‘

+

1
/

ğ‘ 2
ğ‘– h

ğ‘Šğ‘– , ğ‘Š ğ‘£

2

6

i

!

ğ‘ 2
ğ‘– h

ğ‘Šğ‘– , ğ‘Š ğ‘£

2

.

!

i

Ã•ğ‘–6ğ‘‘

Ã•ğ‘–6ğ‘‘
ğ‘¡ = 1, and using the fact that ğ‘  is binary with norm

ğ‘ 2
ğ‘– h

ğ‘Šğ‘– , ğ‘Š ğ‘£

6

2

i

!

Ã•ğ‘–6ğ‘‘

This gets us to,

ğ‘

1
/

ğ‘ 

2ğ‘
ğ‘–

!

Ã•ğ‘–6ğ‘‘

ğ‘Šğ‘– , ğ‘Š ğ‘£
h

i

2ğ‘¡

!

Ã•ğ‘–6ğ‘‘

ğ‘¡

1
/

6

ğ‘Š ğ‘£

k

2

k

Â·

1
ğ‘˜1
/
âˆ’

ğ‘¡

ğ‘Šğ‘– ,
h

1
ğ‘Š ğ‘£

k

k

ğ‘Š ğ‘£

2ğ‘¡

i

!

Ã•ğ‘–6ğ‘‘

ğ‘¡

1
/

.

2 6 ğ‘˜1
1
/
âˆ’

ğ‘¡

ğ‘Š ğ‘£

k

k

ğ‘Šğ‘– ,
h

1
ğ‘Š ğ‘£

k

k

ğ‘Š ğ‘£

2ğ‘¡

i

!

Â·  

Ã•ğ‘–6ğ‘‘

ğ‘¡

1
/

.

(2.6)

Now, whenever ğ‘‘ & ğ‘›ğ‘¡ğ‘¡ ğ‘¡ logğ‘¡ ğ‘› , the ğ‘¡-moment of the column vectors ğ‘Š1 . . . , ğ‘Šğ‘‘ converges with
high probability. That is, for any unit vector ğ‘¢,

1
ğ‘‘

Ã•ğ‘–6ğ‘‘

ğ‘Šğ‘– , ğ‘¢
h

i

2ğ‘¡ 6 ğ‘‚

ğ‘¡ ğ‘¡

.

)

(

(2.7)

Thus, combining Eq. (2.6) and Eq. (2.7) we can conclude

2 . ğ‘˜1
1
/
âˆ’

ğ‘¡

ğ‘Š ğ‘£

k

k

ğ‘¡

ğ‘‘1
/

Â·

ğ‘¡ .

Â·

The catch is that all the steps taken can be written as polynomial inequalities of degree at most
ğ‘¡
. So we can certify the same bound through the Sum-of-Squares proof system.
)

(

ğ‘‚

16

 
 
 
 
 
 
 
 
2.4.2 Certiï¬cates via Limited Brute Force

Whenever the sparse vector ğ‘£0 is almost ï¬‚at, that is when for all ğ‘–

supp

| âˆˆ
, the guarantees of diagonal thresholding can be improved at the cost of increasing its

âˆˆ

{

|

we have

ğ‘£0ğ‘–

ğ‘£0}

1
ğ¶âˆšğ‘˜

, ğ¶
âˆšğ‘˜

running time (see [DKWB19]).
h

i

Diagonal thresholding can be viewed as selecting the ğ‘˜ vectors of the standard basis ğ‘’1, . . . , ğ‘’ğ‘‘
2, and then returning a top eigenvector of the covariance matrix projected onto
maximizing
the span of such vectors. Indeed this formulation has an intuitive generalization, namely instead
of looking at 1-sparse vectors, the algorithm could look into ğ‘¡-sparse vectors ğ‘¢ with entries in

ğ‘Œğ‘’ğ‘–

k

k

ğ‘˜
ğ‘¡

âˆšğ‘¡, 0

ğ‘›

(cid:1)

(cid:0)

Ã—

Â±

, pick the top

and use them to recover ğ‘£0.

1
/
This idea can be translated into a certiï¬ed upper bound for the sparse eigenvalues of ğ‘Š
0, 1
)
(

(cid:8)
(cid:9)
âˆ¼
ğ‘‘. Although we will be able to recover general sparse vectors, for the sake of this discussion
ğ‘
we assume ğ‘£0 is ï¬‚at.18 Letâ€™s denote the set of ğ‘¡-sparse ï¬‚at vectors by
â„ğ‘‘ be a ğ‘˜-sparse
ğ‘¡. Let ğ‘£0 âˆˆ
ğ’©
ğ‘˜.
vector and denote with ğ· the uniform distribution over the vectors in
ğ‘¡ such that
ğ’©
That is, the set of vectors ğ‘¢ such that supp
and with sign pattern matching the sign
pattern of ğ‘£ restricted to supp
{
Note that for any matrix ğ‘€

ğ‘¢, ğ‘£0i

ğ‘£0}

supp

} âŠ†

p

ğ‘‘,

=

ğ‘¢

ğ‘¢

/

{

{

h

ğ‘¡

.
}
â„ğ‘‘
âˆˆ

Ã—

T
ğ‘£0

ğ‘€ğ‘£0 =

ğ‘˜
ğ‘¡

ğ”¼
ğ·
ğ‘¢

âˆ¼

ğ”¼
ğ·
ğ‘¢â€²âˆ¼

T
ğ‘¢

ğ‘€ğ‘¢â€² .

This equality per se is not interesting, but for a Gaussian matrix ğ‘Š
bility,

ğ‘

0, 1
)
(

âˆ¼

ğ‘›

Ã—

ğ‘‘, with high proba-

max
ğ‘¢,ğ‘¢â€²âˆˆğ’©
Thus, combining the two we get

ğ‘¡

(cid:12)
(cid:12)

T
ğ‘¢

T

ğ‘Š

ğ‘Š

âˆ’

(cid:0)

ğ‘›Id

ğ‘¢â€²

6 ğ‘‚

ğ‘›ğ‘¡ log ğ‘‘

.

(cid:1)

(cid:12)
(cid:12)

(cid:16)p

(cid:17)

T
ğ‘£0

T

ğ‘Š

ğ‘Š

(cid:0)

ğ‘›Id

âˆ’

(cid:1)

ğ‘£0 =

T
ğ‘¢

T

ğ‘Š

ğ‘Š

ğ‘›Id

ğ‘¢â€²

âˆ’
T

T
ğ‘¢

ğ‘Š

ğ‘Š

ğ”¼
ğ·

ğ‘˜
ğ‘¡
6 ğ‘˜
ğ‘¡
6 ğ‘˜
âˆšğ‘¡

(cid:0)
max
ğ‘¢,ğ‘¢â€²âˆˆğ’©

ğ‘¡

(cid:12)
(cid:0)
(cid:12)
ğ‘› log ğ‘‘ ,

(cid:1)
ğ‘›Id

ğ‘¢â€²

âˆ’

(cid:1)

(cid:12)
(cid:12)

p
ğ‘˜
ğ‘› log ğ‘‘. This certiï¬cates can be proved using
which allows us to conclude that
âˆšğ‘¡
Sum-of-Squares, hence allowing us to improve over the basic SDP by a factor ğ‘¡ in the settings
ğ‘˜2 6 ğ‘‘1
âˆ’

ğ‘Š ğ‘£0 k

2 6 ğ‘›

1
).

p

+

Î©

k

(

2.5 Concrete lower bounds for perturbation-resilient algorithms

Sparse principal component analysis is what we often call a planted problem. These are problems
that ask to recover some signal hidden by random or adversarial noise. The easiest way one
could formulate a planted problem is its distinguishing version: where given two distributions, a
null distribution without structure and a planted distribution containing the hidden signal, the

18So the Sum-of-Squares algorithm works in more general settings than the algorithm from [DKWB19].

17

objective is to determine with high probability whether a given instance was sampled from one
distribution or the other.

A common strategy to provide evidence for information-computation gap in a certain planted
problem is to prove that powerful classes of eï¬ƒcient algorithms are unable to solve it in the
(conjecturally) hard regime. Indeed our goal here will be that of constructing two distributions
under which low-degree polynomials take roughly the same values and hence cannot distinguish
(in the sense of Section 3.3) from which distribution the instance ğ‘Œ was sampled. Since low-degree
ğ¸ (and therefore cannot solve the
polynomials cannot tell if ğ‘Œ has indeed the form ğ‘Š
problem), this would mean they cannot be used to improve over the guarantees of Theorem 1.2.

T
ğ›½ğ‘¢0ğ‘£0

+

+

Our null distribution ğœˆ will be the standard Gaussian ğ‘

ğ‘‘. However, the main question
T
is how to design the planted distribution ğœ‡. Recall ğ‘Œ takes the form ğ‘Š
ğ›½ğ‘¢0ğ‘£0
ğ¸. If we set
ğ¸ = 0, then our planted distribution corresponds to the single spike covariance model. We could
get a lower bound for such problem (see Section 6) but this would not help us in showing that the
guarantees of Theorem 1.2 are tight. On the other hand, if for example we choose ğ¸ with the goal
of planting a large eigenvalue, then the problem of distinguishing between ğœˆ and ğœ‡ may become
even easier than without adversarial perturbations.

0, 1
)
(

p

+

+

Ã—

ğ‘›

p

+

+

p

T
ğ›½ğ‘¢0ğ‘£0

This suggests that we should choose ğ¸ very carefully, in particular we should design ğ¸ so
that ğ‘Œ = ğ‘Š
ğ¸ appears â€“ to the eyes of a low-degree polynomial estimator â€“ as a
Gaussian distribution. Our approach will be that of constructing ğ¸ so that the ï¬rst few moments
of ğœ‡ will be Gaussian. This will lead us to Theorem 1.3 through two basic observations: ï¬rst,
given two distributions with same ï¬rst 2ğ‘¡ moments, computing those ï¬rst 2ğ‘¡ moments wonâ€™t help
distinguishing between the two distributions. Second, for a Gaussian distribution ğ‘
, at least
)
ğ‘›ğ‘¡ samples are required in order for the 2ğ‘¡-th moment of the empirical distribution to converge to
ğ”¼

ğ‘¤ âŠ—
Concretely, we consider the following model: we choose iid gaussian vectors ğ‘§1, . . . , ğ‘§ğ‘›
(cid:2)
0, 1
)
(

1 âˆ¼
âˆ’
â„ğ‘‘ with iid symmetric (about zero) coordinates that satisï¬es

(cid:3)
ğ‘‘, and a random vector ğ‘§0 âˆˆ

0, Idğ‘›
(

2ğ‘¡

.

ğ‘
the following properties:

1. ğ‘§0 has approximately ğ‘˜ large coordinates (larger than ğœ†

ğ›½ğ‘›

/

â‰ˆ

ğ‘˜ by absolute value).

2. For any coordinate of ğ‘§0 its ï¬rst 2ğ‘¡

p
2 moments coincide with moments of ğ‘

higher ğ‘Ÿ-moments (for even ğ‘Ÿ) are close to ğ‘˜

ğ‘‘ ğœ†ğ‘Ÿ .

âˆ’

0, 1
, and its
)
(

Then we obtain the matrix ğ‘Œ
with rows ğ‘§âŠ¤0 , ğ‘§âŠ¤1 , . . . , ğ‘§âŠ¤ğ‘›
T
âˆ’
ğ‘Š
ğ›½ğ‘¢0ğ‘£0

â„ğ‘›

ğ‘‘ applying a random rotation ğ‘…

ğ‘‘ matrix
1. It is not diï¬ƒcult to see that indeed such ğ‘Œ can be written as ğ‘Œ =

ğ‘› to the ğ‘›

â„ğ‘›

Ã—

âˆˆ

âˆˆ

Ã—

Ã—

ğ¸, as in the model of Problem 1.1.

+

p

+
Now, assume for simplicity that ğ‘¡ is constant and denote the distribution of ğ‘Œ described above
ğ‘‘ by ğœˆ. An immediate consequence of our
0, 1
by ğœ‡ and the standard Gaussian distribution ğ‘
)
(
construction is that for any polynomial ğ‘ of degree at most 2ğ‘¡
ğ‘Œ
ğ‘Œ
. Fur-
âˆ’
)
(
(
thermore, in order to reliably tell the diï¬€erence between ğ”¼ğœ‡
ğ‘Œ
ğ‘â€²(
ğ‘Š
ğ‘â€²(
for a polynomial
(cid:3)
)
of even degree ğ‘Ÿ > 2ğ‘¡ (say up to ğ‘Ÿ = ğ‘›0.001), we will need a precise estimate of such ğ‘Ÿ-th moments
(cid:2)
2>ğ‘¡ samples. This eï¬€ect is then shown by proving that for multilinear polyno-
and hence at least ğ‘›ğ‘Ÿ
/
ğ‘˜ 6 ğ‘›0.49, then the low-degree analogue
ğ‘Œ
mials ğ‘
(
of ğœ’2-divergence

is close to zero. Note that for technical reasons our

of degree ğ· 6 ğ‘›0.001, if ğ‘‘ 6 ğ‘›0.99ğ‘¡

1 and ğ›½ğ‘›
âˆ’
2
))

ğœ‡
âˆ¼
and ğ”¼ğœˆ
(cid:2)

ğ”¼ğœˆ ğ‘
(

2, ğ”¼ğ‘Œ

= ğ”¼ğ‘Œ

ğ‘Œ
(

ğ‘

ğ‘

/

(cid:2)

(cid:3)

(cid:3)

(cid:3)

(cid:2)

âˆ¼

Ã—

)

)

)

ğ‘›

ğœˆ

max
of degree 6ğ·

ğ‘

ğ‘Œ
(

)

ğ‘Œ
(
)âˆ’
ğ•ğœˆ ğ‘ƒ

ğ”¼ğœ‡ ğ‘
ğ‘Œ
(

)

18

analysis is restricted to the multilinear polynomials. As shown in [BHK+16, HS17, Hop18] and as
it will be evident from the single spike model lower bound in Section 6.5, this restricted model of
computation captures the best known algorithms for many planted problems.

2.6 Beyond limitations of CT via low-degree polynomials

An important aspect of the computation of lower bounds for low-degree polynomials is that
they may provide valuable insight on how to construct an optimal algorithm. Indeed low-degree
polynomials capture many spectral properties of linear operators; for example, the largest singular
value of a ğ‘‘-dimensional linear operator with a spectral gap can be approximated by . log ğ‘‘ degree
polynomial in its entries.

We discuss here how they can be used to improve over the guarantees of Covariance Thresh-

olding

log ğ‘‘

Why Covariance Thresholding doesnâ€™t work with small sample size.
In order to improve over
Covariance Thresholding, the ï¬rst question we need to understand is whether the algorithm could
actually work in a larger set of parameters than the one currently known. The answer is no. Recall
2 and ğ‘› 6 ğ‘‘ Covariance Thresholding (with an appropriate choice of thresholding
that for ğ‘˜2 6 ğ‘‘
/
parameter ğœ) works if ğ›½ & ğ‘˜
this is
âˆšğ‘›
asymptotically better than the guarantees of SVD, SVD+Thresholding and Diagonal Thresholding.
cannot have better

It is not diï¬ƒcult to see that Covariance Thresholding with ğœ > Î©
(
guarantees than Diagonal Thresholding. So we consider ğœ 6 ğ‘œ
ğ‘› log ğ‘‘
p
(
and ğ‘› > ğ‘˜2 imply ğ‘› > ğ‘‘1
ğ‘œ
1
). The assumption ğ‘› > ğ‘‘1
1
)
âˆ’
âˆ’
p
is crucial for Covariance Thresholding. To show this, it is enough to prove that for some unit
ğ‘¥
ğ‘£0
ğ›½ğ‘›, this
are uncorrelated with
would mean that for ğ›½
(cid:12)
(cid:12)

1
). Indeed, as on the other hand
ğ‘› the top eigenvectors of ğœ‚ğœ(

ğ‘› , and so for ğ‘› > ğ‘˜2 and ğ‘‘1

Tğœ‚ğœ(
ğ‘£0
ğ‘ŒâŠ¤ğ‘Œ
(cid:12)
(cid:12)

ğ‘› log ğ‘‘
.
)

â„ğ‘‘, ğ‘¥Tğœ‚ğœ(

Note that ğ‘‘1
âˆ’

) 6 ğ‘˜2 6 ğ‘œ
1

) 6 ğ‘˜2 6 ğ‘œ
1

in these settings SVD+Thresholding has signiï¬cantly better

ğ‘›Id
)

ğ‘›Id
)

log ğ‘‘

ğ‘ŒâŠ¤ğ‘Œ

ğ‘ŒâŠ¤ğ‘Œ

ğ‘˜2 +

q

â‰ˆ

âˆ’

âˆ’

âˆ’

âˆˆ

ğ‘‘

ğ‘‘

1
)

âˆ’

(

(

)

)

)

ğ‘œ

ğ‘œ

ğ‘œ

ğ‘œ

(

(

(

(

(

ğ‘›Id
)
â‰ª
ğ‘‘
ğ‘› â‰ª

ğ‘¥ > ğ‘‘1
(
âˆ’
ğ‘œ
1
ğ‘‘1
(
)/
âˆ’
ğ‘œ
ğ‘‘1
âˆ’
ğ‘›

ğ‘£0. Additionally, since
guarantees.

q

(cid:16)

(cid:17)

â„ğ‘‘ satisï¬es

âˆˆ

An ğ‘¥ satisfying our inequality is easy to ï¬nd, for example any row ğ‘Š1, . . . , ğ‘Šğ‘›
Tğœ‚ğœ(

2 with high probability (see Appendix D).

ğ‘Šğ‘– > ğ‘‘1
ğ‘›Id
âˆ’
)

1
) k

ğ‘ŒâŠ¤ğ‘Œ

ğ‘Šğ‘–

âˆ’

k

ğ‘œ

(

ğ‘Šğ‘–

Hence Covariance Thresholding doesnâ€™t provide better guarantees than SVD or Diagonal

Thresholding if ğ‘› 6 ğ‘‘1
âˆ’

Î©

) (for example, if ğ‘› = ğ‘‘0.99).
1

(

2.6.1 Polynomials based algorithm

Theorem 1.7 shows that if ğ‘˜2 6 ğ‘‘1
, it is unlikely that polynomial
âˆ’
time algorithms can solve the problem. So to get an asymptotic improvement over Diagonal
Thresholding we need ğ‘˜2 > ğ‘‘1
âˆ’

) and ğ›½ 6 ğ‘œ
1

log ğ‘‘

ğ‘˜
âˆšğ‘›

1
).

p

(cid:17)

(cid:16)

ğ‘œ

(

(

Î©

However, note that there is no condition ğ‘› > ğ‘‘1
âˆ’

1
) in our lower bound. This suggests that
there might be an algorithm that is asymptotically better than SVD and Diagonal Thresholding
for small ğ‘›, for example ğ‘› = ğ‘‘0.99 or ğ‘› = ğ‘‘0.01. Indeed, we show that there exists a polynomial
) 6
1
as long as ğ‘‘1
time algorithm that can recover the sparse vector ğ‘£0 with entries in
âˆ’
{
log ğ‘› and ğ‘› & log5 ğ‘‘. In particular, if ğ‘‘1
ğ‘˜2 6 ğ‘‘
and ğ‘‘0.01 6
âˆ’

âˆšğ‘˜
1
}
Â±
/
) 6 ğ‘˜2 6 ğ‘œ
1
(

log ğ‘‘

2, ğ›½ & ğ‘˜
âˆšğ‘›

log ğ‘‘

0,

ğ‘‘

ğ‘œ

ğ‘œ

ğ‘œ

(

(

(

)

/

ğ‘˜2 +

q

19

ğ‘› 6 ğ‘‘0.99, this algorithm has asymptotically better guarantees than Diagonal Thresholding, SVD,
SVD+Thresholding, and Covariance Thresholding.

(

(

)

)

(

(

)

Ã—

ğ‘‚

ğ‘‚

log ğ‘‘

ğ‘›ğ‘‘

ğ‘›ğ‘‘

â„ğ‘‘

) âˆˆ

ğ‘Œ
(

log ğ‘‘
(

Our algorithm is based on the approach introduced in [HS17] for commutinity detection
in stochastic block model. An informal description of the algorithm is as follows: we compute
ğ‘‘ whose entries are polynomials ğ‘ƒğ‘— ğ‘—â€²(
some symmetric matrix ğ‘ƒ
in the entries of ğ‘Œ
ğ‘Œ
of degree ğ‘‚
. The algorithm outputs a top eigenvector of this matrix, which we prove to
)
be highly correlated with ğ‘£0. Note that since the degrees of involved polynomials are ğ‘‚
log ğ‘‘
,
)
(
simple evaluation takes time
). However, we can compute a very good approximation
1
) using a color coding technique (this part of the
to the values of these polynomials in time
algorithm uses internal randomness).
More precisely, for ğ‘—, ğ‘—â€² âˆˆ [
we compute multilinear polynomials ğ‘ƒğ‘— ğ‘—â€²(
ğ‘‘
ğ‘Œ
]
such that for every ğ‘— â‰  ğ‘—â€², ğ”¼ ğ‘ƒğ‘— ğ‘—â€²(
, and for every ğ‘—
ğ‘Œ
, ğ‘ƒğ‘— ğ‘—
ğ‘Œ
ğ‘—â€²)
ğ‘—
ğ‘£0(
(
)
)
T
2
ğ‘Œ
ğ‘£0ğ‘£0
that variance of ğ‘ƒğ‘— ğ‘—â€²(
ğ‘Œ
ğ‘ƒ
F < ğ‘œ
)
) âˆ’
(
T
ğ‘œ
ğ‘£0ğ‘£0
ğ‘Œ
ğ‘ƒ
, so the top eigenvector of ğ‘ƒ
,
1
1
1
)
(
(
k
âˆ’
)
(
ğ‘£0.
or
âˆ’
To bound the variance, we represent each monomial as a bipartite multigraph ğº =

log ğ‘‘
of degree ğ‘‚
)
(
= 0. Then we show
. This implies that with probability
is highly correlated with either ğ‘£0

ğ‘…, ğ¶ , ğ¸
,
)
with bipartition ğ‘…
which correspond to columns
of ğ‘Œ. Since the variance is a sum of monomials, we compute the contribution of each monomial and
bound the number of corresponding multigraphs. Finally, we show that there exists a polynomial
log ğ‘› and ğ‘› & log5 ğ‘‘, there
such that in the parameter regime ğ‘‘1
âˆ’
is no group of monomials with large contribution in the variance, so we can conclude that this
polynomial is a good estimator.

= ğ‘£0(
is small so that ğ”¼
k
2
F < ğ‘œ

which corresponds to rows of ğ‘Œ and ğ¶

2, ğ›½ & ğ‘˜
âˆšğ‘›

) 6 ğ‘˜2 6 ğ‘‘
1

1
)
(
ğ‘Œ
)
(

log ğ‘‘

ğ‘˜2 +

)
)

âŠ‚ [

âŠ‚ [

âˆˆ [

) âˆ’

log ğ‘‘

q

ğ‘›

ğ‘‘

ğ‘‘

/

k

k

]

]

]

(

ğ‘œ

(

log ğ‘‘
(

After showing that there are good polynomial estimators of degree ğ‘‚

, we approximately
)
compute them using color coding. All monomials of the polynomials ğ‘ƒğ‘— ğ‘—â€² that we consider have the
same structure (in the sence that the graphs corresponding to these monomials are isomorphic).
Each of them has the same number ğ‘Ÿ of vertices which correspond to rows and the same number ğ‘
in ğ‘Ÿ color and each
of vertices which correspond to columns. We show that for each coloring of
1
) compute the sum of monomials of ğ‘ƒğ‘— ğ‘—â€² colored
coloring of
[
exactly in colores from
. If we average these values over large enough set of random
ğ‘‚
colorings (of size

ğ‘Ÿ
[
]
1
)), we get a value very close to ğ‘ƒğ‘— ğ‘—â€²(
ğ‘Œ

One important advantage of this polynomial-based algorithm is that we only need the following
= 1.19 All previously known
assumptions on ğ‘Š: that the entries of ğ‘Š are i.i.d., ğ”¼ ğ‘Šğ‘–ğ‘— = 0 and ğ”¼ ğ‘Š 2
ğ‘–ğ‘—
algorithms require bounds on entries or the spectral norm of ğ‘ŠTğ‘Š (or related matrices, e.g.
thresholded ğ‘ŠTğ‘Š), so they require ğœ’2 tail bounds.

in ğ‘ colors, we can in time

and

ğ‘›ğ‘‘

ğ‘›ğ‘‘

.
)

ğ‘›

ğ‘‘

ğ‘‚

ğ‘

]

[

]

[

]

)

(

)

(

(

(

3 Preliminaries

In this section, we introduce preliminary notions which will be used in the rest of the paper. We
start by deï¬ning pseudo-distributions and sum-of-squares proofs (see the lecture notes [BS16] for
more details and the appendix in [MSS16] for proofs of the propositions appearing here). Then we
introduce the low-degree likelihood ratio (see [Hop18] for details).

19Indeed, prior work [DHS20] observed that polynomial-based algorithms require only ï¬rst and second moment

conditions on the noise entries for a broad range of matrix and tensor estimation problems.

20

Let ğ‘¥ =

ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘›

(

be a tuple of ğ‘› indeterminates and let â„
[

ğ‘¥

]

)

with real coeï¬ƒcients and indeterminates ğ‘¥1, . . . , ğ‘¥ğ‘›. We say that a polynomial ğ‘
squares (sos) if there are polynomials ğ‘1, . . . , ğ‘ğ‘Ÿ such that ğ‘ = ğ‘2

ğ‘2
ğ‘Ÿ .

be the set of polynomials
is a sum-of-
ğ‘¥

â„
[

âˆˆ

]

1 + Â· Â· Â· +

3.1 Pseudo-distributions

Pseudo-distributions are generalizations of probability distributions. We can represent a discrete
(i.e., ï¬nitely supported) probability distribution over â„ğ‘› by its probability mass function ğ· : â„ğ‘›
â†’
â„ such that ğ· > 0 and
= 1. Similarly, we can describe a pseudo-distribution by
its mass function. Here, we relax the constraint ğ· > 0 and only require that ğ· passes certain
low-degree non-negativity tests.

supp

Ã

ğ·

ğ‘¥

ğ·

)

(

âˆˆ

ğ‘¥

(

)

ğ‘“

)

(

ğ‘¥

ğ‘¥

ğ‘¥ ğ·

ğ‘¥ ğ·

= 1 and

Concretely, a level-â„“ pseudo-distribution is a ï¬nitely-supported function ğ· : â„ğ‘›

â„ such that
2. (Here, the
summations are over the support of ğ·.) A straightforward polynomial-interpolation argument
Ã
-pseudo distribution satisï¬es ğ· > 0 and is thus an actual probability
shows that every level-
distribution. We deï¬ne the pseudo-expectation of a function ğ‘“ on â„ğ‘‘ with respect to a pseudo-
distribution ğ·, denoted ğ”¼ğ·

2 > 0 for every polynomial ğ‘“ of degree at most â„“
)

â†’
/

âˆ

Ã

ğ‘¥

ğ‘¥

)

(

(

ğ‘“

ğ‘¥

(

)

(

, as
)

ğ”¼ğ·

ğ‘¥

(

)

ğ‘“

ğ‘¥

(

)

=

ğ·

ğ‘“

ğ‘¥

(

)

(

ğ‘¥

)

.

Ã•ğ‘¥

(3.1)

The degree-â„“ moment tensor of a pseudo-distribution ğ· is the tensor ğ”¼ğ·
â„“ . In
particular, the moment tensor has an entry corresponding to the pseudo-expectation of all mono-
mials of degree at most â„“ in ğ‘¥. The set of all degree-â„“ moment tensors of probability distribution
is a convex set. Similarly, the set of all degree-â„“ moment tensors of degree ğ‘‘ pseudo-distributions
is also convex. Key to the algorithmic utility of pseudo-distributions is the fact that while there
can be no eï¬ƒcient separation oracle for the convex set of all degree-â„“ moment tensors of an actual
probability distribution, thereâ€™s a separation oracle running in time ğ‘›ğ‘‚
) for the convex set of the
degree-â„“ moment tensors of all level-â„“ pseudodistributions.

1, ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘›

)âŠ—

)(

â„“
(

ğ‘¥

(

Fact 3.1 ([Sho87, Par00, Nes00, Las01]). For any ğ‘›, â„“
separation oracle (in the sense of [GLS81]):

âˆˆ

â„•, the following set has a ğ‘›ğ‘‚

â„“
(

)-time weak

ğ”¼ğ·

ğ‘¥

(

1, ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘›

)(

ğ‘‘

âŠ—

)

|

degree-d pseudo-distribution ğ· over â„ğ‘›

.

(3.2)

(cid:8)

This fact, together with the equivalence of weak separation and optimization [GLS81] allows
us to eï¬ƒciently optimize over pseudo-distributions (approximately)â€”this algorithm is referred to
as the sum-of-squares algorithm.

(cid:9)

The level-â„“ sum-of-squares algorithm optimizes over the space of all level-â„“ pseudo-distributions

that satisfy a given set of polynomial constraintsâ€”we formally deï¬ne this next.

=

Deï¬nition 3.2 (Constrained pseudo-distributions). Let ğ· be a level-â„“ pseudo-distribution over â„ğ‘›.
ğ‘“1 > 0, ğ‘“2 > 0, . . . , ğ‘“ğ‘š > 0
be a system of ğ‘š polynomial inequality constraints. We say
Let
}
that ğ· satisï¬es the system of constraints
and every
sum-of-squares polynomial â„ with deg â„

, if for every ğ‘†

âŠ† [

ğ’œ

ğ’œ

ğ‘š

{

]

at degree ğ‘Ÿ, denoted ğ· ğ‘Ÿ ğ’œ
6 â„“ ,
+
{
ğ‘“ğ‘– > 0 .
ğ”¼ğ· â„
Ã

deg ğ‘“ğ‘– , ğ‘Ÿ

ğ‘† max

}

âˆˆ

ğ‘–

Â·

ğ‘†
Ã–ğ‘–
âˆˆ

21

Ã

if and

ğ’œ

We write ğ·

(without specifying the degree) if ğ·

holds approximately if the above inequalities are satisï¬ed up to an error of 2âˆ’

ğ‘“ğ‘–
k
denotes the Euclidean norm20 of the coï¬ƒcients of a polynomial in the monomial basis.

ğ‘† k

âˆˆ

0 ğ’œ

holds. Furthermore, we say that ğ· ğ‘Ÿ ğ’œ
â„
, where
Â· k

k Â·

ğ‘›â„“

ğ‘–

ğ’œ

kÂ·k

We remark that if ğ· is an actual (discrete) probability distribution, then we have ğ·

only if ğ· is supported on solutions to the constraints

.

ğ’œ

We say that a system
ğ‘¥

2 6 ğ‘€

of the form

of polynomial constraints is explicitly bounded if it contains a constraint

ğ’œ
. The following fact is a consequence of Fact 3.1 and [GLS81],

{k

k

}

Fact 3.3 (Eï¬ƒcient Optimization over Pseudo-distributions). There exists an
that, given any explicitly bounded and satisï¬able system21
outputs a level-â„“ pseudo-distribution that satisï¬es

ğ’œ
approximately.

)-time algorithm
ğ‘š
of ğ‘š polynomial constraints in ğ‘› variables,

+

ğ‘›

)

(

ğ‘‚

â„“
(

ğ’œ

3.2 Sum-of-squares proofs

Let ğ‘“1, ğ‘“2, . . . , ğ‘“ğ‘Ÿ and ğ‘” be multivariate polynomials in ğ‘¥. A sum-of-squares proof that the constraints
ğ‘“1 > 0, . . . , ğ‘“ğ‘š > 0
{
such that

consists of sum-of-squares polynomials

imply the constraint

ğ‘” > 0

)ğ‘†

ğ‘ğ‘†

âŠ†[

{

}

}

ğ‘š

(

]

ğ‘” =

ğ‘š
Ã•ğ‘†
âŠ†[

]

ğ‘ğ‘†

Î ğ‘–

âˆˆ

Â·

ğ‘† ğ‘“ğ‘– .

(3.3)

We say that this proof has degree â„“ if for every set ğ‘†
ğ‘“ğ‘– > 0
most â„“ . If there is a degree â„“ SoS proof that

ğ‘š
âŠ† [
ğ‘– 6 ğ‘Ÿ

{

|

, the polynomial ğ‘ğ‘†Î ğ‘–
ğ‘” > 0
implies

âˆˆ
, we write:

{

}

ğ‘† ğ‘“ğ‘– has degree at

ğ‘“ğ‘– > 0

{

|

ğ‘– 6 ğ‘Ÿ

} â„“ {

.

}

(3.4)

]
}
ğ‘” > 0

Sum-of-squares proofs satisfy the following inference rules. For all polynomials ğ‘“ , ğ‘” : â„ğ‘›

â„
â„ğ‘› such that each of the coordinates of

â†’

and for all functions ğ¹ : â„ğ‘›
the outputs are polynomials of the inputs, we have:

â„ğ‘š, ğº : â„ğ‘›

â„ğ‘˜, ğ» : â„ğ‘

â†’

â†’

â†’

ğ‘“ > 0, ğ‘” > 0
ğ‘” > 0

ğ‘“

}

, ğ’œ â„“ {

ğ‘“ > 0

,

}

ğ’œ â„“

â„“ â€² {
+

}

+

ğ’œ â„“ {

ğ’œ â„“ {
,

ğ’œ â„“ â„¬

ğ’œ â„“

ğ»

ğ¹

(

{

)

ğ¶

} â„“ {

â„¬ â„“ â€²
ğ¶
â„“ â€²
Â·
ğ¹ > 0
{
> 0

} â„“

ğº > 0

deg
(

Â·

ğ»

) {

ğº

(

}
ğ»

.

> 0

}

)

ğ‘” > 0

ğ’œ â„“ â€² {
ğ‘” > 0
ğ‘“

Â·

}

}

(addition and multiplication)

(transitivity)

(substitution)

Low-degree sum-of-squares proofs are sound and complete if we take low-level pseudo-

distributions as models.

Concretely, sum-of-squares proofs allow us to deduce properties of pseudo-distributions that

satisfy some constraints.

20The choice of norm is not important here because the factor 2âˆ’
21Here, we assume that the bitcomplexity of the constraints in

is

ğ‘›

(

ğ‘š

)

+

ğ‘‚

1
).

(

ğ’œ

ğ‘›â„“

swamps the eï¬€ects of choosing another norm.

22

for a level-â„“ pseudo-distribution ğ· and there exists a sum-of-squares

Fact 3.4 (Soundness). If ğ· ğ‘Ÿ ğ’œ
, then ğ·
proof

.

ğ’œ ğ‘Ÿâ€² â„¬

ğ‘Ÿ
Â·
If the pseudo-distribution ğ· satisï¬es

ğ‘Ÿâ€² â„¬

ğ‘Ÿâ€²+

ğ’œ

only approximately, soundness continues to hold if
ğµ (number of bits

we require an upper bound on the bit-complexity of the sum-of-squares
required to write down the proof).

ğ’œ ğ‘Ÿâ€²

In our applications, the bit complexity of all sum of squares proofs will be ğ‘›ğ‘‚

â„“
(

all numbers in the input have bit complexity ğ‘›ğ‘‚
pseudo-distributions that satisfy polynomial constraints approximately.

(

) (assuming that
1
)). This bound suï¬ƒces in order to argue about

The following fact shows that every property of low-level pseudo-distributions can be derived

by low-degree sum-of-squares proofs.
Fact 3.5 (Completeness). Suppose ğ‘‘ > ğ‘Ÿâ€² > ğ‘Ÿ and
ğ‘›
ğ‘–=1 ğ‘¥2
at most ğ‘Ÿ, and
for some ï¬nite ğµ.
ğ‘–
ğ‘” > 0

6 ğµ

Let

ğ’œ

{

satisï¬es ğ·

}

ğ’œ âŠ¢ {
be a polynomial constraint. If every degree-ğ‘‘ pseudo-distribution that satisï¬es ğ· ğ‘Ÿ ğ’œ
}
ğ‘” > 0
ğ‘Ÿâ€² {

, then for every ğœ€ > 0, there is a sum-of-squares proof

ğ’œ ğ‘‘ {

ğ‘” >

Ã

âˆ’

ğœ€

}

}

.

also

We will repeatedly use the following SoS version of Cauchy-Schwarz inequality and its gener-

is a collection of polynomial constraints with degree

alization, HÃ¶lderâ€™s inequality:

Fact 3.6 (Sum-of-Squares Cauchy-Schwarz). Let ğ‘¥, ğ‘¦

â„ğ‘‘ be indeterminites. Then,

âˆˆ

ğ‘¥,ğ‘¦

4

ï£±ï£´ï£´ï£²
ï£´ï£´
ï£³

2

6

ğ‘¥ğ‘– ğ‘¦ğ‘–

!

Ã•ğ‘–

ğ‘¥2
ğ‘–

Ã•ğ‘–

!  

Ã•ğ‘–

ğ‘¦2
ğ‘–

! ï£¼ï£´ï£´ï£½
ï£´ï£´
ï£¾

We will also use the following fact that shows that spectral certiï¬cates are captured within the

SoS proof system.

Fact 3.7 (Spectral Certiï¬cates). For any ğ‘š

ğ‘¢
2

h

Ã—
ğ‘¢, ğ´ğ‘¢

ğ‘š matrix ğ´,

6

ğ´

k

kk

ğ‘¢

2
2

k

i

.

We will also use the following Cauchy-Schwarz inequality for pseudo-distributions.

(cid:8)

(cid:9)

Fact 3.8 (Cauchy-Schwarz for Pseudo-distributions). Let ğ‘“ , ğ‘” be polynomials of degree at most ğ‘‘ in
indeterminate ğ‘¥

â„ğ‘‘. Then, for any degree d pseudo-distribution ğ·, ğ”¼ğ·

ğ”¼ğ·

ğ”¼ğ·

ğ‘”2

ğ‘“ ğ‘”

ğ‘“ 2

6

.

âˆˆ

3.3 Low-degree likelihood Ratio

p

[

]

[

]
p

[

]

The low-degree likelihood ratio is a proxy to model eï¬ƒciently computable functions. It is closely
related to the pseudo-calibration technique and it has been developed in a recent line of work on
the Sum-of-Squares hierarchy [BHK+16, HS17, HKP+17b, Hop18]. Our description is also based
on [BKW20b].

The objects of study are distinguishing versions of planted problems, in which given two
distributions and an instance, the goal is to decide from which distribution the instance was
sampled. For example, in the context of Sparse PCA, the distinguishing formulation takes the form
of deciding whether the matrix ğ‘Œ was sampled according to the (planted) distribution as described
ğ‘‘. In general, we denote
in 1.1, or if it was sampled from the (null) Gaussian distribution ğ‘
with ğœˆ the null distribution and with ğœ‡ the planted distribution with the hidden structure.

0, 1
)
(

Ã—

ğ‘›

23

 
 
3.3.1 Background on Classical Decision Theory

From the point of view of classical Decision Theory, the optimal algorithm to distinguish between
, the
two distribution is well-understood. Given distributions ğœˆ and ğœ‡ on a measurable space
22 is the optimal function to distinguish whether ğ‘Œ
likelihood ratio ğ¿
ğœˆ or
ğ‘Œ

:= ğ‘‘â„™ğœ‡(
ğ‘Œ
ğœ‡ in the following sense.

ğ‘‘â„™ğœˆ(
ğ‘Œ

ğ‘Œ
(

ğ’®
âˆ¼

)/

)

)

âˆ¼

Proposition 3.9. [NP33] If ğœ‡ is absolutely continuous with respect to ğœˆ, then the unique solution of the
optimization problem

max ğ”¼
ğœ‡

ğ‘“

ğ‘Œ
(

)

subject to ğ”¼
ğœˆ

ğ‘“

ğ‘Œ
(

2
)

= 1

is the normalized likelihood ratio ğ¿

(cid:2)
ğ”¼ğœˆ

)/

ğ‘Œ
(

(cid:3)
ğ¿

ğ‘Œ
(

2
)

(cid:2)

(cid:3)

and the value of the optimization problem is ğ”¼ğœˆ

ğ¿

ğ‘Œ
(

2
)

.

(cid:3)
Similarly, arguments about statistical distinguishability are known as well. Unsurprisingly, the

(cid:2)

(cid:2)

(cid:3)

likelihood ratio plays a major role here as well. The key concept is the Le Camâ€™s contiguity.

Deï¬nition 3.10. [Cam60] Let ğœ‡ =
a common probability space
ğ‘›, â„™ğœ‡(
for ğ´ğ‘›

(cid:0)
ğ’®
0 then â„™ğœˆ(
ğ´ğ‘›

) â†’

âˆˆ ğ’®

ğ´ğ‘›

(cid:1)
) â†’

0.

â„• and ğœˆ =

ğœ‡ğ‘›

âˆˆ
ğ‘›. Then ğœ‡ and ğœˆ are contiguous, written ğœ‡ âŠ³ ğœˆ, if as ğ‘›

â„• be sequences of probability measures on
, whenever

ğœˆğ‘›

)ğ‘›

(

âˆˆ

ğ‘›

â†’ âˆ

Contiguity allows us to capture the idea of indistinguishability of probability measures. Indeed
two contiguous sequences ğœ‡, ğœˆ of probability measures are indistinguishable in the sense than
there is no function ğ‘“ :
ğ‘Œ
ğœ‡ and
(
ğ‘“
ğœˆ. The key tool now is the so called Second Moment
Method, which allows us to establish contiguity through the likelihood ratio.

ğ‘›
ğ’®
= 0 with high probability whenever ğ‘Œ

= 1 with high probability whenever ğ‘Œ

such that ğ‘“

â†’ {

ğ‘Œ
(

0, 1

âˆ¼

âˆ¼

}

)

)

Proposition 3.11. If ğ”¼ğœˆ

ğ¿ğ‘›

ğ‘Œ
(

2
)

remains bounded as ğ‘›

, then ğœ‡ âŠ³ ğœˆ.

â†’ âˆ

This discussion allows us to argue whether a given function can be used to distinguish between

(cid:2)

(cid:3)

our planted and null distributions.

3.3.2 Background on the Low-degree Method

The main problem with the likelihood ratio is that it is in general hard to compute, thus we need to
restrict these classical analysis to the space of eï¬ƒciently computable functions. Concretely, we use
low-degree multivariate polynomials in the entries of the observation ğ‘Œ as a proxy for eï¬ƒciently
computable functions. Denoting with â„6ğ·
the space of polynomials in ğ‘Œ of degree at most ğ·
we can establish a low-degree version of the Neyman-Pearson lemma.

ğ‘Œ
[

]

Proposition 3.12 (e.g. [Hop18]). The unique solution of the optimization problem

max
â„6ğ·[
ğ‘Œ
âˆˆ

]

ğ‘“

ğ”¼
ğœ‡

ğ‘“

ğ‘Œ
(

)

subject to ğ”¼
ğœˆ

ğ‘“

ğ‘Œ
(

2
)

= 1

(cid:3)
is the normalized orthogonal projection ğ¿6ğ·
ğ‘Œ
(
and the value of the optimization problem is ğ”¼ğœˆ

(cid:2)

ğ”¼ğœˆ
)/
ğ¿6ğ·

ğ¿6ğ·
2
ğ‘Œ
(cid:2)
)
(

2
)

ğ‘Œ
(
.

(cid:3)

(cid:3)

(cid:2)
of the likelihood ratio ğ¿

ğ‘Œ
(

)

onto â„6ğ·

ğ‘Œ
[

]

22The Radon-Nikodym derivative

(cid:2)

(cid:3)

24

It is important to remark that at the heart of our discussion, there is the belief that in the
study of planted problems, low-degree polynomials capture the computational power of eï¬ƒciently
computable functions. This can be phrased as the following conjecture.

Conjecture 3.13 (Informal). [BHK+16, HS17, HKP+17b, Hop18] For "nice" sequences of probability
measures ğœ‡ and ğœˆ, if there exists ğ· = ğ·
remains bounded as
ğ‘‘

log ğ‘‘
, then there is no polynomial-time algorithm that distinguishes in the sense described in 3.3.1.23

for which ğ”¼ğœˆ

> ğœ”

ğ¿6ğ·

ğ‘Œ
(

2
)

ğ‘‘

(

)

(cid:0)

(cid:1)

(cid:2)

(cid:3)

â†’ âˆ

A large body of work provide support for this conjecture (see any of the citations above),
mostly in the form of evidence of an intimate relation between polynomials and Sum of Squares
algorithms and lower bounds. For a more in detail discussion we point the interested reader to
[HKP+17b, Hop18].

4 Resilience of the basic SDP and Certiï¬ed Upper Bounds

In this section we show the guarantees of the basic SDP algorithm [dGJL05, AW09], thus proving
Theorem 1.4.

2

Ã—

k

k

âˆ

âˆˆ

ğ‘€

â„ğ‘‘

ğ‘€ ğ‘¥

2 6 ğ‘˜

We will ï¬rst prove that for any matrix ğ‘€

ğ‘‘ the basic SDP can certify an upper bound
on ğ‘˜-sparse quadratic forms over ğ‘€. Furthermore we will show that for
k
Â· k
ğ‘‘ this bound can be signiï¬cantly improved in various
random Gaussian matrices ğ‘Š
ways, depending on the regime. Most notably, we will show that the basic SDP can certify a bound
, thus matching the guarantees of Covariance Thresholding.
k
As a corollary, we also get that for ğ›½ < 1 the algorithm achieves the best known guarantees among
polynomial time algorithms in both the fragile and the robust settings.
Formally the Sparse PCA problem can be deï¬ned as follows.

0, 1
)
(

2 6 ğ‘›

ğ‘› log

ğ‘˜2, ğ‘›

min

ğ‘Š ğ‘¥

p

})

ğ‘

+

âˆ¼

ğ‘‘

/

ğ‘˜

{

k

Ã—

(

ğ‘›

Problem 4.1. Given an instance ğ‘Œ of 1.1 let Ë†
Î£ğ‘£
Ë†

argmax

T
ğ‘£

k
k0 is the number of non-zero entries in ğ‘£.

(cid:12)
(cid:12)
(cid:12)

n

k

where

ğ‘£

k

ğ‘£

2 = 1,

ğ‘£

k0 6 ğ‘˜

k

o

Î£ = ğ‘ŒTğ‘Œ. Then the Sparse PCA problem is deï¬ned by

Solving Problem 4.1 is NP-hard in general [MWA06, Nat95, KNV+15], however the following

concrete SDP relaxation [dGJL05] can be eï¬ƒciently solved

argmax

Î£, ğ‘‹
h Ë†

i

ğ‘‹

(cid:23)

0, Tr ğ‘‹ = 1,

ğ‘‹

k1 6 ğ‘˜

k

o

n

(cid:12)
(cid:12)
(cid:12)

ğ‘‹ğ‘–ğ‘—

is the "absolute norm". We will show how to recover ğ‘£0 using such

(SDP-1)

ğ‘‹

where

k
program.

k1 =

ğ‘–,ğ‘—

ğ‘‘
âˆˆ[
Ã

](cid:12)
(cid:12)

(cid:12)
(cid:12)

We start by restating some of the notation from the introduction. For a set ğ‘†
]ğ‘–ğ‘— = ğ‘€ğ‘–ğ‘— if
the matrix with entries ğ‘€
ğ‘†
[
â„ğ‘‘
â„, we deï¬ne ğœ‚ğœ(
ğ‘€
) âˆˆ

ğ‘‘, we denote by ğ‘€
]ğ‘–ğ‘— = 0 otherwise. For a matrix ğ‘€

ğ‘‘ and ğœ

ğ‘†
]
â„ğ‘‘

[
âˆˆ

â„ğ‘‘

âˆˆ

âˆˆ

Ã—

Ã—

Ã—

ğ‘‘
ğ‘–, ğ‘—

ğ‘‘
, and
]
] Ã— [
ğ‘†, and
) âˆˆ
ğ‘‘ to be the matrix

âŠ† [
(

a matrix ğ‘€
ğ‘€
with entries

ğ‘†

[

23We do not explain what "nice" means and direct the reader to [Hop18].

=

ğ‘€

ğœ‚ğœ(

)

0

(

ğ‘€ğ‘–ğ‘—

if

ğ‘€ğ‘–ğ‘—

> ğœ

otherwise.
(cid:12)
(cid:12)

(cid:12)
(cid:12)

25

Furthermore, we deï¬ne ğœğœ(

ğ‘€

) âˆˆ

â„ğ‘‘

Ã—

ğ‘‘ to be the matrix with entries

=

ğ‘€

ğœğœ(

)

ğ‘€ğ‘–ğ‘—

0

(

âˆ’

sign

ğ‘€ğ‘–ğ‘—

ğœ

Â·

(cid:0)

(cid:1)

if

ğ‘€ğ‘–ğ‘—

> ğœ

otherwise.
(cid:12)
(cid:12)

(cid:12)
(cid:12)

4.1 Basic Certiï¬cates for Sparse Quadratic Forms

We show here what certiï¬cates over sparse quadratic forms SDP-1 can provide. These certiï¬cates
are already enough to match the best known guarantees in the weak signal regime. The ï¬rst
observation is that it is straightforward to bound the product between ğ‘‹ and matrices with small
inï¬nity norm. By construction of ğ‘‹ this is indeed a certiï¬cate of an upper bound over ğ‘˜-sparse
quadratic forms.

Lemma 4.2. For ğ‘˜

â„•, let ğ‘‹

âˆˆ

âˆˆ

â„ğ‘‘

Ã—

ğ‘‘ such that

k

ğ‘‹

k1 6 ğ‘˜. Then for any matrix ğ‘€
6 ğ‘˜

ğ‘€

.

â„ğ‘‘

ğ‘‘

Ã—

âˆˆ

ğ‘€ , ğ‘‹

|h

i|

Â· k

kâˆ

Proof. The Lemma follows immediately by choice of ğ‘‹,

ğ‘‹ , ğ‘€

=

i|

|h

ğ‘‘
Ã•ğ‘–,ğ‘—
âˆˆ[

]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ğ‘€ğ‘–ğ‘— ğ‘‹ğ‘–ğ‘—(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

6

ğ‘€ğ‘–ğ‘— ğ‘‹ğ‘–ğ‘—

6

ğ‘‘
Ã•ğ‘–,ğ‘—
âˆˆ[

](cid:12)
(cid:12)

(cid:12)
(cid:12)

ğ‘€

k

kâˆ

ğ‘‹ğ‘–ğ‘—

6 ğ‘˜

ğ‘‘
Ã•ğ‘–,ğ‘—
âˆˆ[

](cid:12)
(cid:12)

(cid:12)
(cid:12)

ğ‘€

.

kâˆ

Â· k

(cid:3)

Now we improve this bound for random matrices. In particular we look into the Hilbert-Schmidt

inner product

ğ‘ŠTğ‘Š

ğœ‚ğœ
h

ğ‘›Id

, ğ‘‹

.

i

âˆ’

(cid:0)
Lemma 4.3. Let ğ‘‹
0, 1
ğ‘Š
)
(

ğ‘

âˆ¼

Ã—

ğ‘›

Ã—
ğ‘‘, then with probability 1

âˆˆ

ğ‘œ

1
)
(

âˆ’

â„ğ‘‘

ğ‘‘ be a positive semideï¬nite matrix such that Tr ğ‘‹ = 1 and

(cid:1)

ğ‘‹

k

k1 6 ğ‘˜. Let

T

ğ‘Š

ğ‘Š
h

âˆ’

ğ‘›Id, ğ‘‹

i

6 ğ‘‚

min

ğ‘˜

(

s

ğ‘› log

1

(cid:18)

ğ‘‘
ğ‘˜2 +

+

ğ‘‘
ğ‘˜âˆšğ‘› (cid:19)

ğ‘›

+

âˆšğ‘‘ğ‘›

, ğ‘‘

+

.

)!

(cid:12)
(cid:12)

Proof. By Theorem G.6,

(cid:12)
(cid:12)

ğ‘ŠTğ‘Š

k

ğ‘›Id

k

âˆ’

6 ğ‘‚

ğ‘‘

âˆšğ‘‘ğ‘›

with probability 1

ğ‘‘âˆ’

10, so by Lemma H.5

âˆ’

+

(cid:16)

T

ğ‘Š

ğ‘Š
h

âˆ’

ğ‘›Id, ğ‘‹

i

(cid:17)
6 ğ‘‚

(cid:12)
(cid:12)

(cid:12)
(cid:12)

âˆ’

Let ğ·

] Ã— [
ğœ > 0 we can rewrite the matrix

âŠ† [

be the set of diagonal entries of
ğ‘ŠTğ‘Š

ğ‘›Id

as

ğ‘‘

ğ‘‘

]

T

ğ‘Š

ğ‘Š

âˆ’

ğ‘›Id =

T

ğ‘Š

ğ‘Š

ğ‘›Id

âˆ’

(cid:0)
ğ·
] +

[

T

ğœ‚ğœ

ğ‘Š

(cid:1)
ğ‘Š

(cid:0)
Now, by Fact G.4 with probability 1

(cid:1)

âˆ’

ğ‘‘

+
(cid:16)
ğ‘ŠTğ‘Š

âˆšğ‘‘ğ‘›

.

(cid:17)
ğ‘›Id

âˆ’

and Â¯ğ· its complement. For any

ğ‘›Id

âˆ’
ğ‘ŠTğ‘Š

(cid:0)
ğ‘œ
,
1
)
(
(cid:13)
(cid:0)
= ğœğœ
[ Â¯ğ·
(cid:13)
]

ğ‘›Id

(cid:1) (cid:2)
âˆ’
T

ğ‘Š

(cid:3)
ğ‘›Id

ğ‘Š

(cid:1)
âˆ’

(cid:0)
Â¯ğ·

T

ğ‘Š

+

[

(cid:0)
ğ·

]
(cid:13)
ğ‘›Id
(cid:13)

(cid:1)

(cid:1)

ğ‘Š

âˆ’
6 10

ğœ‚ğœ

ğ‘Š

T

ğ‘Š

ğ‘›Id

âˆ’

ğ‘›Id

âˆ’

.

Â¯ğ·

(cid:1)(cid:1) (cid:2)
ğ‘› log ğ‘‘. Furthermore,

(cid:0)

(cid:3)

p
] +

ğ‘€ ,

[ Â¯ğ·

ğœ‚ğœ

ğ‘Š

T

ğ‘Š

âˆ’

(cid:0)

(cid:1)

(cid:0)

26

 
where ğ‘€

such that

â„ğ‘‘
Ã—
ğ‘ŠTğ‘Š

ğ‘‘ is a matrix with
k
6 ğ¶

ğ‘›Id

ğ‘€

ğ‘‘

âˆˆ
ğœğœ

âˆ’

]

[ Â¯ğ·
. If ğ‘‘ 6 ğ‘›,

(cid:1)

(cid:13)
(cid:13)

ğ‘› log

1

r

(cid:16)

(cid:13)
(cid:13)
+

(cid:0)
ğ‘‘
ğ‘˜2 +

ğ‘‘
ğ‘˜âˆšğ‘›

ğ‘›

+

(cid:17)

kâˆ

âˆšğ‘‘ğ‘›

+

(cid:16)

6 ğœ and by Theorem G.12 there is a constant ğ¶ > 1

exp

ğœ2
ğ¶ğ‘›

i

âˆ’

h

(cid:17)

with probability 1

ğ‘œ

. Let ğœ = 10ğ¶
1
)
(

Â·

âˆ’

2âˆšğ‘‘ğ‘›

Â·  

ğ‘˜âˆšğ‘›

ğ‘˜

+

+

ğ‘›

ğ‘‘

(

+

ğ‘˜

) +

ğ‘˜âˆšğ‘‘ !

6 10ğ¶ ğ‘˜âˆšğ‘› .

ğœğœ

ğ‘Š

T

ğ‘Š

ğ‘›Id

[ Â¯ğ·

]

âˆ’

(cid:13)
(cid:13)
If ğ‘˜2 6 ğ‘› 6 ğ‘‘,

(cid:0)

(cid:1)

(cid:13)
(cid:13)

6 3ğ¶ ğ‘˜

ğ‘›

q

ğ‘˜âˆšğ‘›

+

ğœğœ

ğ‘Š

T

ğ‘Š

ğ‘›Id

[ Â¯ğ·

]

âˆ’

6 ğ¶ ğ‘˜2

And if ğ‘› 6 max

ğ‘˜2, ğ‘‘

,

}

{

(cid:13)
(cid:13)

(cid:0)

(cid:1)

(cid:13)
(cid:13)

ğœğœ

ğ‘Š

T

ğ‘Š

ğ‘›Id

[ Â¯ğ·

]

âˆ’

6 ğ¶

ğ‘›

(

+

ğ‘˜âˆšğ‘›

So, applying Lemma H.5, we get

(cid:13)
(cid:13)

(cid:0)

(cid:1)

(cid:13)
(cid:13)

p

2ğ‘‘

ğ‘˜2

Â·

(cid:18)

ğ‘‘

(cid:19)

+

6 2ğ¶ ğ‘˜âˆšğ‘› .

2ğ‘‘
ğ‘˜âˆšğ‘›

ğ‘‘ (cid:19)

+

) Â·

ğ‘›

(cid:18)

+

6 4ğ¶ ğ‘˜âˆšğ‘› .

T

ğ‘Š

ğ‘Š

ğœ‚ğœ
h

ğ‘›Id

âˆ’

, ğ‘‹

Â¯ğ·

Since ğ‘‹ is ğ‘˜-bounded,

(cid:0)

(cid:12)
(cid:12)

(cid:1) (cid:2)

(cid:3)

i

(cid:12)
(cid:12)

(cid:13)
(cid:13)

(cid:0)

6

ğœğœ

ğ‘Š

T

ğ‘Š

ğ‘›Id

âˆ’

Â¯ğ·

ğ‘€ , ğ‘‹

i|

+ |h

6 2ğ‘˜ğœ.

(cid:12)
Hence with probability 1
(cid:12)

T

ğ‘Š

ğ‘Š

h

(cid:0)
âˆ’

ğ‘œ

1
)
(

T

ğ‘Š

ğ‘Š
h

âˆ’

ğ‘›Id, ğ‘‹

(cid:12)
(cid:12)

i

(cid:12)
(cid:12)

(cid:3)(cid:13)
(cid:13)

, ğ‘‹

(cid:1) (cid:2)

Â¯ğ·

6 ğ‘˜ğœ.

i

(cid:12)
(cid:12)

ğ‘›Id

âˆ’

âˆ’

ğœ‚ğœ

ğ‘Š

T

ğ‘Š

ğ‘›Id

âˆ’

(cid:0)

(cid:1)(cid:1) (cid:2)

(cid:3)

6 30ğ¶ ğ‘˜

ğ‘› log

2

(cid:18)

s

ğ‘‘
ğ‘˜2 +

+

ğ‘›

+

6 100ğ¶ ğ‘˜

ğ‘› log

2

(cid:18)

s

ğ‘‘
ğ‘˜2 +

+

ğ‘›

+

ğ‘‘
ğ‘˜âˆšğ‘› (cid:19)
ğ‘‘
ğ‘˜âˆšğ‘› (cid:19)

+

since if ğ‘˜ 6 log ğ‘‘, log

2

(cid:16)

> 1

2 log ğ‘‘.

ğ‘‘
ğ‘˜2

+

(cid:17)

4.2 The basic SDP Algorithm

10

ğ‘› log ğ‘‘

p

,

(cid:3)

Having providing certiï¬cates on sparse quadratic form, we can now use Eq. (SDP-1) to obtain a
robust algorithm for Sparse PCA.

Algorithm 4.4 (SDP-based Algorithm).

Input: Sample matrix ğ‘Œ =

Â·
Estimate: The sparse vector ğ‘£0.

p

ğ›½

ğ‘¢0ğ‘£ğ‘‡

0 +

ğ‘Š

ğ¸

+

âˆˆ

â„ğ‘›

Ã—

ğ‘‘ from 1.1.

Operation:

1. Compute matrix ğ‘‹

âˆˆ

â„ğ‘‘

Ã—

ğ‘‘ solving program SDP-1.

27

2. Output top eigenvector

ğ‘£ of ğ‘‹.
Ë†

Indeed we will show that Algorithm 4.4 is perturbation resilient (in the sense of Appendix C)
and its guarantees matches those of the state-of-the-art fragile algorithms such as SVD, Diagonal
Thresholding and Covariance Thresholding. The following theorem formalize this result.

Theorem 4.5. Let ğ‘Œ be a ğ‘›-by-ğ‘‘ matrix of the form,

ğ‘Œ =

T
ğ‘¢0ğ‘£0

ğ›½

Â·

ğ‘Š

+

+

ğ¸ ,

â„ğ‘›

for a unit ğ‘˜-sparse vector ğ‘£0 âˆˆ
ğ‘‘ and a Gaussian matrix ğ‘Š
ğ¸
algorithm 4.4 outputs a unit vector
ğ‘£
Ë†

â„ğ‘‘, a standard Gaussian vector ğ‘¢0 âˆ¼
ğ‘
â„ğ‘‘ such that with probability 1

, an arbitrary matrix
0, Idğ‘›
)
(
ğ‘‘ such that ğ‘Š , ğ‘¢0, are distributionally independent. Then
,
1
)
(

0, 1
)
(

âˆ¼
âˆˆ

ğ‘

âˆ’

âˆˆ

ğ‘œ

Ã—

Ã—

ğ‘›

p

1

ğ‘£0,

ğ‘£
Ë†

i

âˆ’ h

2 . ğ‘˜

ğ›½ğ‘› Â·

ğ‘

+ s

ğ‘˜
ğ›½ğ‘›  r

log

ğ‘‘
ğ‘˜ + k

ğ¸

k1

â†’

2

! Â·  

1

+

1

ğ›½ !

.

where ğ‘ := min

ğ‘‘
ğ‘˜2 +
Furthermore, the same kind of guarantees hold if ğ‘¢0 is a vector with

ğ‘› log

ğ‘‘
ğ‘˜âˆšğ‘›

(cid:26)r

and

k1

+

ğ¸

â†’

2

(cid:27)

k

(cid:16)

(cid:17)

+

+

ğ‘›

, ğ‘‘

âˆšğ‘‘ğ‘›
ğ‘˜

2 denotes the largest norm of a column of ğ¸.

ğ‘¢0k

k

2 = Î˜

ğ‘›

(

)

independent of ğ‘Š.

p

We prove Theorem 4.5 through the result below, which will be useful in the Sum-of-Squares

proofs as well.

Theorem 4.6 (Meta-theorem). Let ğ‘Œ be a ğ‘›-by-ğ‘‘ matrix of the form,

ğ‘Œ =

T
ğ‘¢0ğ‘£0

ğ›½

Â·

ğ‘Š

+

+

ğ¸ ,

â„ğ‘›

for a unit ğ‘˜-sparse vector ğ‘£0 âˆˆ
ğ‘‘ and a Gaussian matrix ğ‘Š
ğ¸
0, 1
Ã—
)
(
Î£, ğ‘‹
Let ğ‘‹ be a feasible solution of SDP-1 satisfying
h Ë†

ğ‘

âˆ¼

âˆˆ

Ã—

ğ‘›

p

â„ğ‘‘, a standard Gaussian vector ğ‘¢0 âˆ¼

, an arbitrary matrix
0, Idğ‘›
)
(
ğ‘‘ such that ğ‘Š , ğ‘¢0, ğ‘£0 are distributionally independent.

ğ‘

>

i

T
Î£, ğ‘£0ğ‘£0
h Ë†

. Then with probability 1

i

ğ‘œ

,
1
)
(

âˆ’

T
ğ‘£0ğ‘£0

, ğ‘‹

1

âˆ’ h

. 1

ğ›½ğ‘› Â·

i

T

ğ‘Š

ğ‘Š
h

âˆ’

ğ‘›Id, ğ‘‹

i

ğ‘˜
ğ›½ğ‘›  r

log

ğ‘‘
ğ‘˜ + k

ğ¸

+ s

k1

â†’

2

! Â·  

1

+

1

ğ›½ !

,

2 denotes the largest norm of a column of ğ¸. Furthermore, the same kind of guarantees hold if

p

where
ğ¸
k1
ğ‘¢0 is a vector with

â†’

k

2 = Î˜

ğ‘›

independent of ğ‘Š.

)
Indeed Theorem 4.6 immediately implies Theorem 4.5.

k

(

ğ‘¢0k

(cid:12)
(cid:12)

Proof of Theorem 4.5. Assume Theorem 4.6 is true. By deï¬nition ğ‘‹ satisï¬es its premises. By
Lemma 4.3

T

ğ‘Š

ğ‘Š
h

âˆ’

ğ‘›Id, ğ‘‹

i

6 ğ‘‚

min

Applying Lemma H.3 the result follows.

(cid:12)
(cid:12)

ğ‘˜

(

s

ğ‘› log

1

(cid:18)

ğ‘‘
ğ‘˜2 +

+

ğ‘‘
ğ‘˜âˆšğ‘› (cid:19)

ğ‘›

+

âˆšğ‘‘ğ‘›

, ğ‘‘

+

.

)!

(cid:3)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Now letâ€™s prove Theorem 4.6. First we look into cross-terms containing the signal.

28

 
Lemma 4.7. Let ğ‘Œ be as in Theorem 4.6 and suppose ğ¸

â„ğ‘›

Ã—

ğ‘‘ is a matrix with maximal column norm

2 6 ğ‘. Let ğ‘‹ be a feasible solution to SDP-1. Then with probability 1

âˆˆ

ğ‘œ

,
1
)
(

âˆ’

ğ¸

k

k1

â†’

ğ‘Š
h

(cid:12)
(cid:12)
(cid:12)
,
1
)
(

p
ğ‘¢0k

T

T
ğ›½ğ‘¢0ğ‘£0

, ğ‘‹

6 ğ‘‚

ğ›½ğ‘› ğ‘˜ log

ğ‘‘
ğ‘˜ !

.

i

 r

(cid:12)
(cid:12)
(cid:12)
. Let ğ‘” = 1
ğ‘¢0
)
k

Proof. With probability 1
ğ‘”

6 ğ‘‚
. Let ğ‘† be the set of ğ‘˜ largest coordinates in ğ‘”, and let ğ‘”â€² = ğ‘”
0, 1
)
(

âˆšğ‘›
(

ğ‘

âˆ¼

âˆ’

ğ‘œ

k

k

vector ğ‘”â€²â€² has entries bounded by ğ‘‚

(cid:18)q
Lemma G.10). Hence by Lemma H.5,

log ğ‘‘
ğ‘˜

(cid:19)

and

ğ‘”â€²k

k

6 ğ‘‚

(cid:18)q

(cid:19)

ğ‘ŠTğ‘¢0. Since ğ‘¢0 and ğ‘Š are independent,
. Then ğ‘” = ğ‘”â€² +
ğ‘”â€²â€², where
[
with probability 1

(by

ğ‘†

ğ‘œ

]

ğ‘˜ log ğ‘‘
ğ‘˜

1
)
(

âˆ’

T
ğ›½ğ‘¢0ğ‘£0

, ğ‘‹

T

ğ‘Š
h

p

(cid:12)
(cid:12)
(cid:12)

6 ğ‘‚

6 ğ‘‚

i

(cid:12)
(cid:12)
(cid:12)

T
ğ‘”â€²ğ‘£0

, ğ‘‹

ğ‘›ğ›½

h

i

+

ğ‘›ğ›½

h

T
ğ‘”â€²â€²ğ‘£0

, ğ‘‹

i

(cid:16)p

 r

(cid:12)
(cid:12)

ğ›½ğ‘› ğ‘˜ log

(cid:12)
ğ‘‘
(cid:12)
ğ‘˜ ! +

p

(cid:12)
(cid:12)
ğ‘›ğ›½

ğ‘‚

(cid:16)p

h

(cid:12)
(cid:12)

T
ğ‘”â€²â€²ğ‘£0

, ğ‘‹

.

(cid:17)
(cid:12)
(cid:12)
i

(cid:17)
(cid:12)
(cid:12)

By Lemma H.6 and Lemma H.5,

T
ğ‘”â€²â€²ğ‘£0

, ğ‘‹

h

6

i

The desired bound follows from Lemma 4.2, since the entries of ğ‘”â€²â€²
with probability 1

ğ‘œ

ğ‘”â€²â€²

ğ‘”â€²â€²

T, ğ‘‹

ğ‘£0ğ‘£0

T, ğ‘‹

6

i

i Â· h

ğ‘”â€²â€²

ğ‘”â€²â€²

T, ğ‘‹

.

i

h

q

h

q

(cid:12)
(cid:12)

(cid:0)

(cid:1)

(cid:12)
(cid:12)

âˆ’

.
1
)
(

(cid:0)
ğ‘”â€²â€²

(cid:1)
T are bounded by ğ‘‚

(cid:0)

(cid:1)

log ğ‘‘
ğ‘˜ )
(
(cid:3)

Lemma 4.8. Let ğ‘Œ be as in Theorem 4.6 and suppose ğ¸

â„ğ‘›

Ã—

ğ‘‘ is a matrix with maximal column norm

2 6 ğ‘. Let ğ‘‹ be a feasible solution to SDP-1. Then with probability 1

âˆˆ

ğ‘œ

,
1
)
(

âˆ’

ğ¸

k

k1

â†’

Proof. With probability 1
of ğ‘§ are bounded by ğ‘‚

ğ‘œ
âˆ’
ğ‘âˆšğ‘›

T
ğ¸

T
ğ›½ğ‘¢0ğ‘£0

, ğ‘‹

6 ğ‘‚

ğ‘

ğ›½ğ‘› ğ‘˜

.

h

i
(cid:17)
. Let ğ‘§ = ğ¸Tğ‘¢0. With probability 1
âˆšğ‘›
,
1
(
(
)
)
. By Lemma H.6 and Lemma H.5,

p
6 ğ‘‚
ğ‘¢0k

(cid:12)
(cid:12)
(cid:12)
k

p

(cid:12)
(cid:12)
(cid:12)

(cid:16)

ğ‘œ

1
)
(

âˆ’

the entries

(cid:0)
T
ğ‘§ğ‘£0

, ğ‘‹

6

i

(cid:1)

(cid:12)
(cid:12)

h

p

h

(cid:12)
(cid:12)

ğ‘§ğ‘§T, ğ‘‹

ğ‘£0ğ‘£0

T, ğ‘‹

6

i

i Â· h

ğ‘§ğ‘§T, ğ‘‹

i

h

p

6 ğ‘‚

ğ‘âˆšğ‘› ğ‘˜

.

(cid:16)

(cid:17)

(cid:3)

The following lemma shows how to bound the remaining cross-terms.

Lemma 4.9. Let ğ‘Œ be as in Theorem 4.6 and suppose ğ¸
2 6 ğ‘. Let ğ‘‹ be a feasible solution to SDP-1. Then

ğ¸

k

k1

â†’

â„ğ‘›

Ã—

ğ‘‘ is a matrix with maximal column norm

âˆˆ

T
ğ¸

ğ‘Š

T

ğ‘Š

ğ¸, ğ‘‹

6 2ğ‘âˆšğ‘˜ğ‘›

h
(cid:12)
Proof. Applying Fact H.4 with setting ğ´ =
(cid:12)
immediately get

+

(cid:12)
(cid:12)

i

ğ‘Š
(

âˆ’

ğ‘2ğ‘˜

T

ğ‘Š

ğ‘Š
h

+

âˆ’

ğ‘›Id, ğ‘‹

.

i

ğ¸

T
)

(cid:12)
ğ‘Š
(cid:12)
(

Â·

ğ‘

ğ¸

)

Â·

âˆ’

for some ğ‘ > 0 and ğµ = ğ‘‹ we

(cid:12)
(cid:12)

+

ğ‘

T
ğ¸

ğ‘Š

+

T

ğ‘Š

ğ¸, ğ‘‹

ğ‘

h

(cid:12)
(cid:12)

i

(cid:12)
(cid:12)

T

6

ğ‘Š
h

ğ‘Š , ğ‘‹

ğ‘2

h

i +

T
ğ¸

ğ¸, ğ‘‹

= ğ‘›

T

ğ‘Š

ğ‘Š

+ h

âˆ’

i

ğ‘›Id, ğ‘‹

ğ‘2

h

i +

T
ğ¸

ğ¸, ğ‘‹

.

i

29

By Lemma 4.2

T
ğ¸

ğ‘Š

h

+

T

ğ‘Š

ğ¸, ğ‘‹

Minimizing over ğ‘, we get

(cid:12)
(cid:12)

6 1
ğ‘

ğ‘›

+

ğ‘Š
h

T

ğ‘Š

ğ‘›Id, ğ‘‹

i

+

ğ‘

Â·

âˆ’

ğ‘2ğ‘˜ .

(cid:0)

(cid:12)
(cid:12)

(cid:12)
(cid:1)
(cid:12)

i

(cid:12)
(cid:12)

T
ğ¸

ğ‘Š

+

T

ğ‘Š

ğ¸, ğ‘‹

h

(cid:12)
(cid:12)

i

(cid:12)
(cid:12)

6 2ğ‘

ğ‘˜ğ‘›

6 2ğ‘âˆšğ‘˜ğ‘›
p
6 2ğ‘âˆšğ‘˜ğ‘›

+

+

+

ğ‘˜

Â· |h
ğ‘˜

2ğ‘

ğ‘2ğ‘˜
p

+

ğ‘ŠTğ‘Š

ğ‘›Id, ğ‘‹

âˆ’
ğ‘ŠTğ‘Š
T

ğ‘Š

i|
ğ‘›Id, ğ‘‹

âˆ’
ğ‘›Id, ğ‘‹

âˆ’

Â· |h
ğ‘Š
h

i

i|
.

(cid:12)
(cid:12)

(cid:3)

We are now ready to prove Theorem 4.6.

Proof of Theorem 4.6. Opening up the product,

(cid:12)
(cid:12)

ğ‘›

Î£, ğ‘‹
h Ë†

i

=

Î£
h Ë†
=ğ›½

k

âˆ’
ğ‘¢0k
ğ‘Š
T
ğ¸
T
ğ¸

+ h

+ h

+ h

ğ›½

h

+

ğ‘›Id
2

ğ‘›Id, ğ‘‹
+
T
ğ‘£0ğ‘£0

, ğ‘‹

i

i +
ğ‘›Id, ğ‘‹

i

T

h
ğ‘Š

âˆ’
ğ¸, ğ‘‹

ğ‘Š

i
ğ‘Š
+
T
ğ‘£0ğ‘¢0

ğ‘Š

T

ğ¸, ğ‘‹

i
T
ğ‘Š

+

Applying Lemmata 4.2, 4.3, 4.9, 4.8, 4.7 and we get

p

Î£, ğ‘‹
h Ë†

i

6 ğ›½

ğ‘¢0k

k

2

h

T
ğ‘£0ğ‘£0

, ğ‘‹

ğ‘›

2

ğ‘Š
h

+

i +

T

ğ‘Š

âˆ’

ğ‘›Id, ğ‘‹

T
ğ‘¢0ğ‘£0

+

T
ğ‘£0ğ‘¢0

ğ¸

+

T
ğ¸

T
ğ‘¢0ğ‘£0

, ğ‘‹

.

i

2ğ‘2ğ‘˜

+

2ğ‘âˆšğ‘˜ğ‘›

ğ‘‚

+

log

ğ‘‘
ğ‘˜ +

ğ‘

!

  r

ğ›½ğ‘› ğ‘˜

.

!

p

i

+

(cid:12)
(cid:12)

Furthermore, by choice of ğ‘‹,

Î£, ğ‘‹
h Ë†

i

>

T
Î£, ğ‘£0ğ‘£0
h Ë†
Î£
h Ë†
>ğ›½

ğ‘›Id
2

=

i

âˆ’
ğ‘›

T
ğ‘›Id, ğ‘£0ğ‘£0

(cid:12)
(cid:12)

i

+
ğ‘Š

T

T
ğ‘›Id, ğ‘£0ğ‘£0
âˆ’
T
ğ¸, ğ‘£0ğ‘£0

i

i
T
T
ğ¸, ğ‘£0ğ‘£0
(cid:12)
(cid:12)
T

ğ‘Š

ğ‘Š

+

(cid:12)
(cid:12)
i
T
(cid:12)
ğ‘¢0ğ‘£0
(cid:12)

ğ‘Š

ğ‘Š

+
T
ğ‘£0ğ‘¢0

h

h

+
ğ‘¢0k
ğ‘Š
h
T
ğ¸
T
ğ¸

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
p
(cid:12)
(cid:12)
ğ‘¢0k

ğ›½

h

k

âˆ’

âˆ’

âˆ’

âˆ’

>ğ›½

k

>ğ›½

ğ‘¢0k

k

T
ğ‘£0ğ‘¢0

ğ¸

+

+

T
ğ¸

T
ğ‘¢0ğ‘£0

T
, ğ‘£0ğ‘£0

2

2

ğ‘›

2

ğ‘Š
h

âˆ’

+

T

ğ‘Š

âˆ’

T
ğ‘›Id, ğ‘£0ğ‘£0

2ğ‘2ğ‘˜

âˆ’

i

âˆ’

2ğ‘âˆšğ‘˜ğ‘›

(cid:12)
(cid:12)
ğ‘‚

ğ‘›

+

âˆ’

T

ğ‘Š

ğ‘Š
h

âˆ’

(cid:12)
(cid:12)
T
ğ‘›Id, ğ‘£0ğ‘£0

ğ‘2ğ‘˜

âˆ’

i

âˆ’

ğ‘âˆšğ‘˜ğ‘›

(cid:12)
(cid:12)

(cid:12)
(cid:12)

30

i

(cid:12)
(cid:12)
(cid:12)
âˆ’

ğ‘‚

log

  r

ğ‘‘
ğ‘˜ +

ğ‘

!

ğ›½ğ‘› ğ‘˜

!

p

log

ğ‘‘
ğ‘˜ +

ğ‘

!

âˆ’  r

ğ›½ğ‘› ğ‘˜

.

!

p

 
ğ‘œ

ğ‘ŠTğ‘Š
Now by Theorem G.9
h
ğ‘ŠTğ‘Š
. Let ğ‘š =
1
1
(cid:12)
h
)
(
(cid:12)
rearranging, we get
(cid:12)
(cid:12)

âˆ’

âˆ’

âˆ’
ğ‘›Id, ğ‘‹

i

+

(cid:12)
(cid:12)

(cid:12)
(cid:12)

T
ğ‘›Id, ğ‘£0ğ‘£0

i
ğ‘ŠTğ‘Š
(cid:12)
h
(cid:12)

âˆ’

/
i

(cid:12)
(cid:12)

6 10ğ‘˜ log
ğ‘‘
(
T
ğ‘›Id, ğ‘£0ğ‘£0

20

ğ‘› ğ‘˜ log

ğ‘˜
ğ‘˜
with probability
) +
)
. Combining the two inequalities and

ğ‘‘

/

(

p

2

ğ›½

ğ‘¢0k

k

Â·

1

âˆ’ h

T
ğ‘£0ğ‘£0

, ğ‘‹

6 ğ‘‚

ğ‘š

ğ‘2ğ‘˜

+

+

ğ‘âˆšğ‘˜ğ‘›

i

(cid:0)

With probability 1

ğ‘œ

,
1
)
(

ğ‘¢0k

k

âˆ’

2 > ğ‘›

/

(cid:1)
2. Recall that ğ‘ 6

ğ›½ğ‘›
ğ‘˜ . Hence

q

log

+  r

ğ‘‘
ğ‘˜ +

ğ‘

!

ğ›½ğ‘› ğ‘˜

.

!

p

T
ğ‘£0ğ‘£0

, ğ‘‹

1

âˆ’ h

6 1
ğ›½ğ‘›

i

ğ‘‚

ğ‘š

ğ›½ğ‘› ğ‘˜ log

+ r

ğ‘‘
ğ‘˜ +

1

+

ğ›½

ğ‘âˆšğ‘˜ğ‘›

(cid:0)

(cid:1)

The result follows rearranging and observing that with probability 1
ğ‘˜ log
by Lemma G.9.

ğ‘‘

ğ‘‘

ğ‘˜

ğ‘˜

(

/

) +

ğ‘˜ğ‘› log
(

/

)

p

ğ‘œ

,
1
)
(

âˆ’

(cid:12)
(cid:12)

.

!

ğ‘ŠTğ‘Š
h

âˆ’

T
ğ‘›Id, ğ‘£0ğ‘£0

.
(cid:3)

i

(cid:12)
(cid:12)

5 Resilience of SoS and Stronger Certiï¬ed Upper Bounds

In this section we prove Theorem 1.2 and Theorem 1.5. We will show that the Sum-of-Squares
algorithm can certify various upper bounds on sparse eigenvalues. In Section 5.1 we will prove
increasingly stronger certiï¬ed upper bounds on sparse eigenvalues of subgaussian matrices. These
certiï¬ed upper bounds will require increasingly stronger assumptions on ğ‘‘ and ğ‘›, but for degree
will approach information theoretic guarantees. In Section 5.2 we will prove alternative
log
certiï¬ed upper bounds fo sparse eigenvalues of Gaussian matrices. These bounds will not require
any additional assumption on ğ‘‘ and ğ‘›. We will then use these bounds in Section 5.3 to obtain
maximally robust algorithms for Sparse PCA.

ğ‘‘

/

ğ‘˜

)

(

5.1 SoS Certiï¬cates for Sparse Eigenvalues via Certiï¬able Subgaussianity

ğ‘ ,ğ‘£ be the following system of quadratic constraints. Observe for any

Let
is a ğ‘˜-sparse unit vector supported on coordinates ğ‘– such that ğ‘ ğ‘– = 1.

ğ’œ

ğ‘ , ğ‘£

(

)

satisfying

ğ‘ ,ğ‘£, ğ‘£

ğ’œ

(5.1)

ğ‘–

ğ‘–

âˆ€

âˆ€

ğ‘‘

ğ‘‘

.

.

]

]

âˆˆ [

âˆˆ [

ğ‘‘
ğ‘–=1 ğ‘ ğ‘– = ğ‘˜
ğ‘ 2
= ğ‘ ğ‘–
ğ‘–
ğ‘£ğ‘– = ğ‘£ğ‘–

Ã
ğ‘ ğ‘–
ğ‘‘

Â·

ğ‘£2
ğ‘–

= 1

Ã•ğ‘–=1

ï£¼ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£½
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´

ğ‘ ,ğ‘£ :

ğ’œ

ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´

â„ğ‘›

We prove a certiï¬ed upper bound for sparse eigenvalues of random rectangular matrices
ğ‘‘ with independent subgaussian entries. This upper bound diï¬€ers considerably from
ğ‘Š
the one obtained using SDP-1. Let us recall the deï¬nition of subgaussian random variables before
proceeding.

ï£¾

ï£³

âˆˆ

Ã—

Deï¬nition 5.1 (ğ¶-Subgaussian Random Variables). A â„-valued random variable ğ‘¥ is said to be
ğ¶-subgaussian if for every ğ‘¡, ğ”¼

ğ‘¡ 6 ğ¶ ğ‘¡

ğ‘¥

2ğ‘¡ ğ‘¡
/

2.
/

|

|

31

 
 
Let ğ‘Š1, ğ‘Š2, . . . , ğ‘Šğ‘‘ be the columns of ğ‘Š. We will use the following lemma:

â„ğ‘› be independently drawn from a product distribution with each
Lemma 5.2. Let ğ‘Š1, ğ‘Š2, . . . , ğ‘Šğ‘‘
1-subgaussian coordinates with mean 0 and variance 1. Then, with probability at least 0.99 over the draw of
ğ‘Š1, ğ‘Š2, . . . , ğ‘Šğ‘‘,

âˆˆ

ğ‘¢
2ğ‘¡

1
ğ‘‘

(

Ã•ğ‘–6ğ‘‘

ğ‘Šğ‘– , ğ‘¢
h

i

2ğ‘¡ 6

ğ‘¢

2ğ‘¡
2

k

k

ğ‘¡ ğ‘¡

+

ğ‘›ğ‘¡

2 log(
/

ğ‘¡

ğ¶â€²ğ‘¡

ğ‘›

(

)(

ğ‘¡

)

2
1
+
)/
âˆšğ‘‘

.

!)

for some absolute constant ğ¶â€² > 0.

We will prove the lemma whenever the columns of ğ‘Š are certiï¬ably subgaussian. Informally,
certiï¬ably subgaussianity means that a random variable has its moments upper-bounded as in the
the deï¬nition above and that this bound has a SoS proof. Formally, we have:

Deï¬nition 5.3 (Certiï¬able Subgaussianity). A â„ğ‘›-valued random variable ğ‘Œ is said to be ğ‘¡-
certiï¬ably ğ¶-subgaussian if for all ğ‘¡â€² 6 ğ‘¡, 2ğ‘¡
ğ‘‘ is
ğ‘Œ, ğ‘¢
h
said to be ğ‘¡-certiï¬ably ğ¶-subgaussian if the uniform distribution on the columns of ğ‘Š is ğ‘¡-certiï¬ably
ğ¶-subgaussian.

. A matrix ğ‘Š

2ğ‘¡ 6 ğ¶ ğ‘¡ğ‘¡ ğ‘¡

ğ‘Œ, ğ‘¢
h

â„ğ‘›

ğ”¼

ğ”¼

âˆˆ

n

o

i

i

Ã—

(cid:1)

(cid:0)

ğ‘¢

2

ğ‘¡

Certiï¬able subgaussianity has, by now, appeared in several works [KS17b, KS17a, HL18,

KKM18] that employ the sum-of-squares method for statistical estimation problems.

Given the above lemma, to prove Lemma 5.2, we need to show certiï¬ed subgaussianity of ğ‘Š

when ğ‘Š is a random matrix in â„ğ‘›

Ã—

ğ‘‘. To show this, we will use the following fact:

Fact 5.4 (Certiï¬able Subgaussianity of Product Subgaussians, Lemma 5.9, Page 25 of [KS17b]). Let
ğ‘Œ be a â„ğ‘‘-valued random variable with independent, ğ¶-subgaussian coordinates of mean 0 and variance 1.
Then, ğ‘Œ is ğ‘¡-certiï¬ably ğ¶-subgaussian for every ğ‘¡.

We are now ready to prove Lemma 5.2.

Proof of Lemma 5.2. We have:

ğ‘¢
2ğ‘¡

1
ğ‘‘  

(

ğ‘Šğ‘– , ğ‘¢
h

i

Ã•ğ‘–6ğ‘‘
Using Fact 3.7 and

2ğ‘¡

ğ”¼

ğ‘Šğ‘– , ğ‘¢
h

i

âˆ’

2ğ‘¡

=

!

*

ğ‘¢ âŠ—

ğ‘¡ ,

ğ‘¡

ğ‘¢ âŠ—

=

2
2

ğ‘¢

k

k

2ğ‘¡
2 , we have:

1
ğ‘‘

Ã•ğ‘–6ğ‘‘

ğ‘¡

ğ‘Š âŠ—
ğ‘–

ğ‘¡

ğ‘Š âŠ—
ğ‘–

âŠ¤

ğ”¼

ğ‘¡

ğ‘Š âŠ—
ğ‘–

ğ‘¡

ğ‘Š âŠ—
ğ‘–

âˆ’

(cid:0)

(cid:1) (cid:0)

(cid:1)

(cid:0)

(cid:1) (cid:0)

ğ‘¡

ğ‘¢ âŠ—

.

+)

âŠ¤

!

(cid:1)

ğ‘¢
2ğ‘¡

1
ğ‘‘

(

(cid:13)
(cid:13)
ğ”¼

(cid:13)
(cid:13)
2ğ‘¡
i

âˆ’

ğ‘Šğ‘– , ğ‘¢
h

ğ‘Šğ‘– , ğ‘¢
h

i

2ğ‘¡ 6

ğ‘¢

2ğ‘¡
2 Â·

k

k

1
ğ‘‘

Ã•ğ‘–6ğ‘‘

(cid:13)
(cid:13)
(cid:13)
From Lemma G.3, we know that with probability at least 0.99 over the draw of ğ‘Š1, ğ‘Š2, . . . , ğ‘Šğ‘‘, it
(cid:13)
holds that:

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:1) (cid:0)

(cid:1) (cid:0)

(cid:18)

(cid:1)

(cid:1)

(cid:0)

(cid:0)

ğ‘¡

ğ‘Š âŠ—
ğ‘–

ğ‘¡

âŠ¤

ğ‘Š âŠ—
ğ‘–

ğ”¼

ğ‘¡

ğ‘Š âŠ—
ğ‘–

ğ‘¡

âŠ¤

ğ‘Š âŠ—
ğ‘–

.

(5.2)

âˆ’

)

(cid:13)
(cid:18)
(cid:13)
Using Fact 5.4,
(cid:13)
(cid:13)

1
ğ‘‘

ğ‘¡

ğ‘Š âŠ—
ğ‘–

ğ‘¡

ğ‘Š âŠ—
ğ‘–

âŠ¤

ğ”¼

ğ‘¡

ğ‘Š âŠ—
ğ‘–

âˆ’

ğ‘Š âŠ—
ğ‘–

ğ‘›ğ‘¡

2 log(
/

ğ‘¡

ğ‘¡

âŠ¤

6

(cid:0)

(cid:1) (cid:0)

(cid:1)

ğ‘¢
2ğ‘¡

(cid:0)

(cid:8)

(cid:19)(cid:13)
(cid:13)
(cid:1)
(cid:13)
(cid:13)
2ğ‘¡ 6 ğ‘¡ ğ‘¡

(cid:1) (cid:0)

ğ”¼

ğ‘Šğ‘– , ğ‘¢
h

i

32

ğ‘¢

2ğ‘¡
2

k

k

.

(cid:9)

ğ¶â€²ğ‘¡

ğ‘›

(

)(

ğ‘¡

)

.

2
1
+
)/
âˆšğ‘‘

(5.3)

(5.4)

 
 
Combining (5.2), (5.3) and (5.4), we have:

ğ‘¢
2ğ‘¡

1
ğ‘‘

(

Ã•ğ‘–6ğ‘‘

ğ‘Šğ‘– , ğ‘¢
h

i

2ğ‘¡ 6

ğ‘¢

2ğ‘¡
2

k

k

ğ‘¡ ğ‘¡

+

ğ‘›ğ‘¡

2 log(
/

ğ‘¡

ğ¶â€²ğ‘¡

ğ‘›

(

)(

ğ‘¡

)

2
1
+
)/
âˆšğ‘‘

.

!)

(cid:3)

Lemma 5.2 implies the following lemma:

Lemma 5.5. Let ğ‘Š satisfy the assumptions of Lemma 5.2. Suppose that ğ‘‘ > ğ‘¡ ğ‘¡ ğ‘›ğ‘¡ log(
probability at least 0.99,

ğ‘¡

1
)
+

ğ‘›

. Then with
)

(

for some absolute constant ğ¶â€² > 0.

ğ‘ ,ğ‘£
2ğ‘¡

ğ‘ ,ğ‘£

ğ’œ

ğ‘Š ğ‘£

k

4ğ‘¡
2 6 ğ‘‘ğ‘˜ğ‘¡

1
âˆ’

ğ¶â€²ğ‘¡

(

ğ‘¡

)

k

ğ‘Š ğ‘£

2ğ‘¡
2

k

.

(cid:9)

k

(cid:8)

and Cauchy-Schwarz inequality, we have:

Proof. For ğ‘¢ = ğ‘Š ğ‘£, using

ğ‘ 
ğ‘ ,ğ‘£ 2ğ‘¡

ğ’œ

{

ğ‘ ğ‘–ğ‘£ğ‘– = ğ‘£ğ‘–

ğ‘–

| âˆ€

}

2ğ‘¡

ğ‘ ,ğ‘£,ğ‘¢
2ğ‘¡

ğ‘ ,ğ‘£

ğ’œ

Using

ğ‘ 
ğ‘ ,ğ‘£ 2ğ‘¡

ğ’œ

{

ğ‘ ğ‘¡
ğ‘–

1
âˆ’

= ğ‘ 2
ğ‘–

ğ‘ ğ‘–ğ‘£ğ‘–

ğ‘Šğ‘– , ğ‘¢
h

i!

Ã•ğ‘–6ğ‘‘

6

ğ‘£2
ğ‘–

!

Ã•ğ‘–6ğ‘‘

, we have:

ğ‘–

}

ï£±ï£´ï£´ï£²
ï£´ï£´
ï£³
| âˆ€

ğ‘ ,ğ‘£

ğ’œ

Now, using

ğ‘ ,ğ‘£
2

ğ‘ ,ğ‘£

ğ’œ

{

Ã

ğ‘ ,ğ‘£,ğ‘¢
ğ‘¡

ï£±ï£´ï£´ï£²
ï£´ï£´
ğ‘– ğ‘ ğ‘– = ğ‘˜
ï£³

}

2ğ‘¡

6

ğ‘ ğ‘–ğ‘£ğ‘–

ğ‘Šğ‘– , ğ‘¢
h

i!

Ã•ğ‘–6ğ‘‘

ğ‘£2
ğ‘–

!

Ã•ğ‘–6ğ‘‘

Ã•ğ‘–6ğ‘‘

ğ‘Šğ‘– , ğ‘¢

ğ‘ 2
ğ‘– h

2

i

!

and Lemma 5.2, we have:

ğ‘ ,ğ‘£,ğ‘¢
ğ‘¡

ğ‘ ,ğ‘£

ğ’œ

ğ‘ ğ‘–

ğ‘Šğ‘– , ğ‘¢
h

i

ğ‘¡

2

!

=

1ğ‘‘
âˆ’

2ğ‘¡
2

ğ‘¢

k

k

ğ‘¡ ğ‘¡

+

n

Ã•ğ‘–6ğ‘‘
6 ğ‘˜ğ‘¡

ğ‘ ğ‘¡
ğ‘–

1
âˆ’

ğ‘¡

6

ğ‘Šğ‘– , ğ‘¢
h

i

2

!

ğ¶â€²ğ‘¡

ğ‘›

(

)(

ğ‘¡

)

ğ‘¡

2
1
+
)/
âˆšğ‘‘

Ã•ğ‘–6ğ‘‘
ğ‘›ğ‘¡

2 log(
/

ğ‘¡

1
âˆ’

ğ‘ ğ‘¡
ğ‘–

!

Ã•ğ‘–6ğ‘‘

2ğ‘¡

ğ‘Šğ‘– , ğ‘¢
h

i

!

Ã•ğ‘–6ğ‘‘

!

o

Plugging back ğ‘¢ = ğ‘Š ğ‘£, we get the desired bound.

Now we are ready to derive the certiï¬ed upper bound on

ğ‘Š ğ‘£

k

2
2.

k

(5.5)

(cid:3)

Lemma 5.6. Suppose that ğ‘‘ > ğ¶âˆ—ğ‘¡ ğ‘¡ğ‘›ğ‘¡ logğ‘¡
distribution satisfying
Ã—
Then, with probability at least 0.99 over the draw of ğ‘Š1, ğ‘Š2, . . . , ğ‘Šğ‘‘,

ğ‘ ,ğ‘£. Let ğ‘Š

â„ğ‘›

ğ’œ

âˆˆ

ğ‘›

for large enough absolute constant ğ¶âˆ—. Let ğ· be a pesudo-
(
ğ‘‘ with i.i.d. 1-subgaussian entries with mean 0 and variance 1.

)

for some absolute constant ğ¶â€² > 0.

ğ”¼ğ·

ğ‘Š ğ‘£

k

k

2
2 6 ğ¶â€²

Â·

ğ‘‘1
/

ğ‘¡ ğ‘˜1
âˆ’

1
ğ‘¡ ğ‘¡ ,

33

ğ‘ 2
ğ‘– h

ğ‘Šğ‘– , ğ‘¢

2

i

!

Ã•ğ‘–6ğ‘‘

ğ‘¡

ğ‘¡

ğ‘¡

ğ‘¡

ï£¼ï£´ï£´ï£½
ï£´ï£´
ï£¾

ï£¼ï£´ï£´ï£½
ï£´ï£´
ï£¾

 
 
 
 
 
 
 
 
 
 
 
 
Proof. Using Lemma 5.5 and taking pseudo-expectations with respect to ğ· that satisï¬es
have:

ğ’œ

ğ‘ ,ğ‘£, we

ğ‘Š ğ‘£

k

k

2ğ‘¡
2 6

2
1
/

, and by

ğ”¼ğ·

ğ‘Š ğ‘£

k

4ğ‘¡
2

k

(cid:16)

(cid:17)

ğ”¼ğ·

ğ‘Š ğ‘£

k

k

4ğ‘¡
2 6 ğ‘‘ğ‘˜ğ‘¡

1
âˆ’

ğ¶â€²ğ‘¡

(

)

ğ‘¡ ğ”¼ğ·

ğ‘Š ğ‘£

k

2ğ‘¡
2 .

k

By Cauchy-Schwarz inequality for pseudo-distributions, ğ”¼ğ·

HÃ¶lderâ€™s indequality

ğ”¼ğ·

ğ‘Š ğ‘£

k

2
2

k

(cid:16)

2ğ‘¡

(cid:17)

Taking ğ‘¡-th roots gives: ğ”¼ğ·

ğ‘Š ğ‘£

k

k

ğ”¼ğ·

k
(cid:16)
2 6 ğ¶â€² Â·

2

ğ‘¡

2
2

k

(cid:17)
ğ‘¡ ğ‘˜1
âˆ’

ğ‘Š ğ‘£

ğ‘‘1
/

6 ğ”¼ğ·

ğ‘Š ğ‘£

k

k

4ğ‘¡
2 . Thus, we have:

6 ğ‘‘ğ‘˜ğ‘¡

1
âˆ’

ğ¶â€²ğ‘¡

(

ğ‘¡ .

)

1
ğ‘¡ ğ‘¡.

(cid:3)

5.2 SoS Certiï¬cates for Sparse Eigenvalues via Limited Brute Force

We show here that, using additional constraints over the system
certiï¬ed upper bounds on the sparse eigenvalues of Gaussian matrices ğ‘Š.

ğ’œ

ğ‘ ,ğ‘£, we can provide diï¬€erent

ğ‘¡ be a set of all vectors with values in

Let
We start with a deï¬nition.

ğ’®

0, 1

}

{

that have exactly ğ‘¡ nonzero coordinates.

Deï¬nition 5.7. For any ğ‘¢

ğ‘¡ we deï¬ne a polynomial in variables ğ‘ 1, . . . , ğ‘ ğ‘‘ =: ğ‘ 

âˆˆ ğ’®

=

ğ‘ğ‘¢

ğ‘ 

(

)

1
âˆ’

Â·

ğ‘˜
ğ‘¡

(cid:19)

(cid:18)

ğ‘ ğ‘– .

Ã–ğ‘–
supp
{
âˆˆ

ğ‘¢

}

Note that if ğ‘£ denotes a ğ‘˜-sparse vector and ğ‘  is the indicator of its support, then for any ğ‘¢

ğ‘¡,

âˆˆ ğ’®

=

ğ‘ğ‘¢

ğ‘ 

(

)

1
âˆ’

ğ‘˜
ğ‘¡
0
((cid:0)

(cid:1)

if supp

ğ‘¢

{

} âŠ†

supp

ğ‘£

{

}

otherwise

Now consider the following system

ğ‘ ,ğ‘£ of polynomial constraints.

â„¬

ğ‘ ,ğ‘£ :

â„¬

ğ‘–

âˆ€

ğ‘‘

,

]

âˆˆ [

ğ‘–

âˆ€

ğ‘‘

,

]

âˆˆ [

ğ‘–

âˆ€

ğ‘‘

,

]

âˆˆ [

ğ‘ 2
= ğ‘ ğ‘–
ğ‘–
ğ‘ ğ‘– = ğ‘˜

]
ğ‘£ğ‘– = ğ‘£ğ‘–
ğ‘£2
= 1
ğ‘–

ğ‘–

ğ‘‘
âˆˆ[
Ã
ğ‘ ğ‘–
Â·

ğ‘–

ğ‘‘
âˆˆ[
Ã
ğ‘ğ‘¢

Ã•ğ‘¢
ğ‘¡
âˆˆğ’®
ğ‘¢ğ‘– ğ‘ğ‘¢

Ã•ğ‘¢
ğ‘¡
âˆˆğ’®

]

ğ‘ 

(

)

= 1

=

ğ‘ 

(

)

ğ‘¡
ğ‘˜ Â·

ğ‘ ğ‘–

ï£¼ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£½
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´

ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´

ï£³

We will use the following preliminary fact.

Fact 5.8. Let ğ‘Š
Ã—
principle submatrices of ğ‘ŠTğ‘Š

0, 1
)
(

ğ‘

âˆ¼

ğ‘›

ğ‘‘, let ğ‘› > log ğ‘‘ and let ğ‘¡ 6 1

log ğ‘‘ min

ğ‘‘, ğ‘›

{
ğ‘¡ have spectral norm bounded by ğ‘‚

}

ï£¾
. Then with probability 1

ğ‘›Id of size ğ‘¡

âˆ’

Ã—

ğ‘›ğ‘¡ log ğ‘‘

.

(cid:16)p

(cid:17)

34

(5.6)

ğ‘œ

1
)
(

âˆ’

all

ğ‘‘
ğ‘¡
p
(cid:1)
(cid:0)

Proof. Fix a ğ‘¡
6 ğ¶
k
k
possible

ğ‘

Ã—

ğ‘›ğ‘¡ log ğ‘‘ with probability at least 1

ğ‘¡ principal submatrix ğ‘. By Theorem G.6 there exists a constant ğ¶ > 0, such that
ğ‘‘10ğ‘¡. The fact follows taking a union bound over all
(cid:3)

âˆ’

submatrices.

We are now ready to show the upper bound on quadratic forms of sparse vectors.

Theorem 5.9. Let ğ‘Š
1

ğ‘œ

,
1
)
(

âˆ’

ğ‘

0, 1
)
(

âˆ¼

ğ‘›

Ã—

ğ‘‘. Then there exists a constant ğ¶ > 0 such that with probability at least

ğ‘ ,ğ‘£

â„¬

ğ‘ ,ğ‘£
4ğ‘¡

ğ‘˜
âˆšğ‘¡

ğ¶

Â·

âˆ’

(cid:26)

Proof. Note that

T
ğ‘› log ğ‘‘ 6 ğ‘£

T

ğ‘Š

ğ‘Š

ğ‘›Id

ğ‘£ 6 ğ¶

âˆ’

p

(cid:0)

(cid:1)

ğ‘˜
âˆšğ‘¡

Â·

p

ğ‘› log ğ‘‘

.

(cid:27)

T =

ğ‘ ğ‘ 

ğ‘ ,ğ‘£
2ğ‘¡

(

ğ‘˜2
ğ‘¡2

ğ‘ ,ğ‘£

â„¬

T
ğ‘¢â€²ğ‘¢

ğ‘ 

ğ‘ğ‘¢â€²(

)

ğ‘ğ‘¢

ğ‘ 

(

.

))

Ã•ğ‘¢,ğ‘¢â€²âˆˆğ’®

ğ‘¡

For vectors ğ‘¥, ğ‘¦

âˆˆ

â„ğ‘‘ we denote the vector with entries ğ‘¥ğ‘–

ğ‘¦ğ‘– by

ğ‘¥ ğ‘¦

. It follows that

Â·

ğ‘ ,ğ‘£

â„¬

ğ‘ ,ğ‘£

â„¬

ğ‘ ,ğ‘£
4ğ‘¡

ğ‘ ,ğ‘£
4ğ‘¡

T =

ğ‘£ğ‘£

T =

ğ‘£ğ‘£

(cid:8)

(

Let ğ‘€ =

ğ‘ŠTğ‘Š

ğ‘›Id

. Then

âˆ’

ğ‘£ğ‘ 

(
ğ‘˜2
ğ‘¡2

ğ‘£ğ‘ 

T
)

)(

(cid:0)

(cid:1)

(cid:9)
ğ‘£ğ‘¢â€²
(

)(

ğ‘£ğ‘¢

T
)

ğ‘ 

ğ‘ğ‘¢â€²(

)

ğ‘ğ‘¢

ğ‘ 

(

.

))

Ã•ğ‘¢,ğ‘¢â€²âˆˆğ’®

ğ‘¡

(cid:0)

(cid:1)
ğ‘ ,ğ‘£

â„¬

ğ‘ ,ğ‘£
4ğ‘¡

ğ‘ ,ğ‘£
4ğ‘¡

ğ‘ ,ğ‘£
4ğ‘¡

(cid:8)

(

(

T
ğ‘£

ğ‘€ğ‘£ =

T
ğ‘€ , ğ‘£ğ‘£

h

T
ğ‘£

ğ‘€ğ‘£ =

ğ‘˜2
ğ‘¡2

ğ‘€ ,

h

i

(cid:9)

ğ‘£ğ‘¢â€²

(

)(

ğ‘£ğ‘¢

T
)

ğ‘ 

ğ‘ğ‘¢â€²(

)

ğ‘ğ‘¢

ğ‘ 

(

)i)

Ã•ğ‘¢,ğ‘¢â€²âˆˆğ’®

ğ‘¡

T
ğ‘£

ğ‘€ğ‘£ =

ğ‘˜2
ğ‘¡2

Ã•ğ‘¢,ğ‘¢â€²âˆˆğ’®

ğ‘¡

ğ‘£ğ‘¢

(

T
)

ğ‘€

ğ‘£ğ‘¢â€²

ğ‘ 

ğ‘ğ‘¢â€²(

)

)

ğ‘ğ‘¢

ğ‘ 

(

(

))

Now for any ğ‘¢, ğ‘¢â€² âˆˆ ğ’®
ğ‘¡,
=
ğ‘£ğ‘¢

ğ‘£ğ‘¢â€²

ğ‘ ,ğ‘£

â„¬

T
)
ğ‘£ğ‘¢

ğ‘£ğ‘¢

ğ‘£ğ‘¢

ğ‘€
T
)
T
)
T
)

(
ğ‘€

ğ‘€

ğ‘€

)
ğ‘£ğ‘¢â€²
(
ğ‘£ğ‘¢â€²
(
ğ‘£ğ‘¢â€²
(

)

)

)

(
6

6

6

ğ‘£ğ‘¢

T
)
ğ‘£ğ‘¢

(

(

(

ğ‘£ğ‘¢

ğ‘£ğ‘¢

T

ğ‘Š
T
)
T
)
T
)

ğ‘Š

ğ‘€

ğ‘€

ğ‘Š
T

(
ğ‘Š

(
ğ‘£ğ‘¢
(
ğ‘£ğ‘¢
(

ğ‘£ğ‘¢â€²

ğ‘›

) âˆ’

ğ‘£ğ‘¢

) + (
ğ‘£ğ‘¢â€²

ğ‘£ğ‘¢â€²

) + (

) + (

T
)
T
)
ğ‘€

ğ‘£ğ‘¢
(
ğ‘£ğ‘¢â€²
T
)
T
)

ğ‘€

(

ğ‘£ğ‘¢â€²
T

(
ğ‘Š

)
ğ‘Š
(cid:9)
(

ğ‘£ğ‘¢â€²

ğ‘£ğ‘¢

ğ‘›

) +
.

)

Â·

k(

(cid:0)

ğ‘£ğ‘¢â€²

(

ğ‘£ğ‘¢â€²

2ğ‘›

(

) âˆ’

T
)

(

ğ‘£ğ‘¢

2

)k

+ k(

ğ‘£ğ‘¢â€²

)
ğ‘£ğ‘¢â€²
(cid:9)
)k

ğ‘ ,ğ‘£
2
ğ‘ ,ğ‘£
2
ğ‘ ,ğ‘£
2
ğ‘ ,ğ‘£
2

(
2
(
2
(
2
(

(cid:8)

(cid:8)

(cid:8)

(cid:8)

where the ï¬rst equality follows by deï¬nition, the second using the fact that for any ğ‘
ğ‘, ğ‘
ğ‘
Similar derivation shows that

. The last follows from the fact that
ğ‘£ğ‘¢â€²)
2
(

ğ‘
k
âˆ’
ğ‘£ğ‘¢â€²)
.
(cid:8)
(

T, ğ‘
)
â„¬

> 0
i
ğ‘ ,ğ‘£
4ğ‘¡

ğ‘ ,ğ‘£
2
Tğ‘€

ğ‘£ğ‘¢â€²)
(

Tğ‘€
)

ğ‘£ğ‘¢
(

â„ğ‘‘,

ğ‘ ,ğ‘£
2

ğ‘£ğ‘¢

h(

6

)(

ğ‘ ,ğ‘£

âˆ’

âˆ’

âˆˆ

(cid:8)

ğ‘

ğ‘

ğ‘

Tğ‘€
ğ‘£ğ‘¢
)
Now let ğ‘ be the maximal norm of any ğ‘¡
Ã—
ğ‘ ğ‘– > 0
6 ğ‘¡. Since
,
}

(cid:9)
âˆ’
(cid:8)
{

supp

ğ‘ ,ğ‘£
4ğ‘¡

ğ‘£ğ‘¢

{(

â„¬

ğ‘ ,ğ‘£

ğ‘¡,

|

â„¬

âˆˆ ğ’®

ğ‘¢

(cid:9)
) âˆ’ (
ğ‘¡ principal submatrices of ğ‘€. Note that for any

âˆ’(

ğ‘ ,ğ‘£
4ğ‘¡

ğ‘ ,ğ‘£

(cid:9)

(cid:9)

2

ğ‘£ğ‘¢

2
(

âˆ’

T
)

(

ğ‘£ğ‘¢â€²

)

(cid:1)(cid:9)

0, and
2 > 0
.

(cid:23)
ğ‘
k

)}|
ğ‘€ğ‘£ 6 ğ‘˜2
2ğ‘¡2

ğ‘ ,ğ‘£

â„¬

ğ‘ ,ğ‘£
4ğ‘¡

T
ğ‘£

(

> 0

,

ğ‘ğ‘¢

ğ‘ 

(

)

(cid:8)
ğ‘£ğ‘¢â€²
(

)

(cid:9)
ğ‘ğ‘¢

)

ğ‘ 

ğ‘ğ‘¢â€²(

(cid:1)

ğ‘ 

(

))

ğ‘£ğ‘¢

T
)

ğ‘€

(

ğ‘£ğ‘¢

ğ‘£ğ‘¢â€²

ğ‘€

T
)

) + (

Ã•ğ‘¢,ğ‘¢â€²âˆˆğ’®

ğ‘¡

(

(cid:0)

35

ğ‘

(cid:0)

ğ‘£ğ‘¢

2

)k

+

ğ‘

ğ‘£ğ‘¢â€²

2

)k

Â· k(

Â· k(

ğ‘ 

ğ‘ğ‘¢â€²(

)

ğ‘ğ‘¢

ğ‘ 

(

(cid:1)

))

ğ‘£ğ‘¢

k(

)k

2ğ‘ğ‘¢

ğ‘ 

(

) 

ğ‘ 

ğ‘ğ‘¢â€²(

Ã•ğ‘¢â€²âˆˆğ’®

ğ‘¡

)! +

ğ‘£ğ‘¢â€²

2ğ‘ğ‘¢â€²(

ğ‘ 

)k

k(

ğ‘ğ‘¢

ğ‘ 

(

)! !)

) 

Ã•ğ‘¢
ğ‘¡
âˆˆğ’®

ğ‘¡

Ã•ğ‘¢,ğ‘¢â€²âˆˆğ’®
ğ‘˜2
2ğ‘¡2  

Ã•ğ‘¢
ğ‘¡
âˆˆğ’®

T
ğ‘£

ğ‘€ğ‘£ 6 ğ‘˜2
2ğ‘¡2

T
ğ‘£

ğ‘€ğ‘£ 6 ğ‘

T
ğ‘£

ğ‘€ğ‘£ 6 ğ‘

T
ğ‘£

ğ‘€ğ‘£ 6 ğ‘

ğ‘ ,ğ‘£
4ğ‘¡

ğ‘ ,ğ‘£
4ğ‘¡

ğ‘ ,ğ‘£
4ğ‘¡

ğ‘ ,ğ‘£
4ğ‘¡

(

(

(

(

ğ‘¡

Ã•ğ‘¢â€²âˆˆğ’®
2ğ‘ğ‘¢â€²(

ğ‘ 

)! )

ğ‘˜2
2ğ‘¡2  

Ã•ğ‘¢
ğ‘¡
âˆˆğ’®

ğ‘£ğ‘¢

k(

)k

2ğ‘ğ‘¢

ğ‘ 

(

) +

ğ‘£ğ‘¢â€²

k(

)k

Ã•ğ‘¢â€²âˆˆğ’®

ğ‘¡

ğ‘˜2
ğ‘¡2

Ã•ğ‘¢
ğ‘¡
âˆˆğ’®

ğ‘£ğ‘¢

k(

)k

2ğ‘ğ‘¢

ğ‘ 

.

)

(

)

Here the second inequality follows from choice of ğ‘, the third uses the fact that

ğ‘ ,ğ‘£
4ğ‘¡

ğ‘ ,ğ‘£

â„¬

ğ‘ğ‘¢

ğ‘ 

(

)!

= 1

)

( 

ğ‘¢
ğ‘¡
âˆˆğ’®
Ã

. Finally observe that

ğ‘ ,ğ‘£

â„¬

ğ‘ ,ğ‘£
4ğ‘¡

ğ‘ ,ğ‘£
4ğ‘¡

ğ‘ ,ğ‘£
4ğ‘¡

ğ‘ ,ğ‘£
4ğ‘¡

(

Ã•ğ‘¢
ğ‘¡
âˆˆğ’®

(

Ã•ğ‘¢
ğ‘¡
âˆˆğ’®

(

Ã•ğ‘¢
ğ‘¡
âˆˆğ’®

(

Ã•ğ‘¢
ğ‘¡
âˆˆğ’®

ğ‘£ğ‘¢

2ğ‘ğ‘¢

ğ‘ 

(

)

)k

k(

=

ğ‘£ğ‘¢

2ğ‘ğ‘¢

ğ‘ 

(

)

)k

k(

=

ğ‘£ğ‘¢

2ğ‘ğ‘¢

ğ‘ 

(

)

)k

k(

=

ğ‘‘

Ã•ğ‘–=1

Ã•ğ‘¢
ğ‘¡
âˆˆğ’®
ğ‘‘

ğ‘– ğ‘¢2
ğ‘£2
ğ‘– Â·

ğ‘ğ‘¢

ğ‘ 

(

))

ğ‘£2
ğ‘–

ğ‘‘

Ã•ğ‘¢
ğ‘¡
âˆˆğ’®

ğ‘£2
ğ‘– ğ‘ ğ‘–

ğ‘¢ğ‘–

ğ‘ğ‘¢

ğ‘ 

(

Â·

))

)

Ã•ğ‘–=1
ğ‘¡
ğ‘˜

Ã•ğ‘–=1

ğ‘£ğ‘¢

2ğ‘ğ‘¢

ğ‘ 

(

)

)k

k(

=

ğ‘¡
ğ‘˜ )

,

where we used the facts ğ‘ğµğ‘ ,ğ‘£

ğ‘¢
ğ‘¡
âˆˆğ’®
Ã
there exists an absolute constant ğ¶ > 0 such that with probability 1
with probability 1

ğ‘œ

âˆ’

(

ğ‘ ,ğ‘£
2

ğ‘£ğ‘– = ğ‘£ğ‘–

{

ğ‘ ğ‘–

}

Â·

and ğ‘ğµğ‘ ,ğ‘£

ğ‘ ,ğ‘£
4ğ‘¡

ğ‘¢ğ‘– ğ‘ğ‘¢

ğ‘ 

= ğ‘¡

ğ‘˜ Â·
(
)
, ğ‘ 6 ğ¶
1
)
(

ğ‘œ

ğ‘ ğ‘–

. By Fact 5.8,

)
ğ‘›ğ‘¡ log ğ‘¡. Hence

,
1
)
(

âˆ’

ğ‘ ,ğ‘£
4ğ‘¡

ğ‘ ,ğ‘£

â„¬

(cid:26)

T
ğ‘£

ğ‘€ğ‘£ 6 ğ¶

ğ‘˜
âˆšğ‘¡

p

ğ‘› log ğ‘‘

.

(cid:27)

ğ‘ ,ğ‘£
4ğ‘¡

ğ‘ ,ğ‘£

â„¬

âˆ’

(cid:26)

T
ğ‘£

ğ‘€ğ‘£ 6 ğ¶

ğ‘˜
âˆšğ‘¡

p

ğ‘› log ğ‘‘

.

(cid:27)

p

(cid:3)

Similar derivation shows that

5.3 SoS Algorithms

We now use the certiï¬ed upper bounds from the previous sections to obtain eï¬ƒcient algorithms
for Sparse PCA with adversarial errors, thus proving Theorem 1.2 and Theorem 1.5 which we
formally restate.

36

Theorem 5.10. Suppose ğ‘‘ & ğ‘›ğ‘¡ logğ‘¡

(

ğ‘›

ğ‘¡ ğ‘¡ for ğ‘¡

â„•. Let ğ‘Œ be an ğ‘›-by-ğ‘‘ matrix of the form,

âˆˆ

)
ğ‘Œ =

T
ğ‘¢0ğ‘£0

ğ›½

Â·

ğ‘Š

+

+

ğ¸ ,

â„ğ‘›

for a unit ğ‘˜-sparse vector ğ‘£0 âˆˆ
ğ¸
we can compute in time ğ‘‘ğ‘‚

ğ‘‘ and a Gaussian matrix ğ‘Š

â„ğ‘‘, a standard Gaussian vector ğ‘¢0 âˆ¼

, an arbitrary matrix
0, Idğ‘›
)
(
ğ‘‘ such that ğ‘Š , ğ‘¢0 are distributionally independent. Then

0, 1
ğ‘
Ã—
)
(
â„ğ‘‘ such that with probability at least 0.99,
ğ‘£
) a unit vector
Ë†
âˆˆ

ğ‘

âˆ¼

âˆˆ

Ã—

ğ‘›

(

ğ‘¡

p

1

ğ‘£, ğ‘£0i

âˆ’ h Ë†

2 . ğ‘˜

ğ›½ğ‘› Â·

ğ‘¡

Â·

ğ‘¡

1
/

ğ‘‘
ğ‘˜

(cid:0)

(cid:1)

1
ğ›½ + s

+

ğ‘˜
ğ›½ğ‘›  r

log

ğ‘‘
ğ‘˜ + k

ğ¸

k1

â†’

2

! Â·  

1

+

1

ğ›½ !

,

2 denotes the largest norm of a column of ğ¸. Furthermore, the same kind of guarantees hold if

p

ğ¸
where
k1
ğ‘¢0 is a vector with

â†’

k

ğ‘¢0k

k

2 = Î˜

ğ‘›

(

)

independent of ğ‘Š.

Theorem 5.11. Suppose ğ‘› & log ğ‘‘ and ğ‘¡ 6 ğ‘˜. Let ğ‘Œ be an ğ‘›-by-ğ‘‘ matrix of the form,

ğ‘Œ =

T
ğ‘¢0ğ‘£0

ğ›½

Â·

ğ‘Š

+

+

ğ¸ ,

for a unit ğ‘˜-sparse vector ğ‘£0 âˆˆ
ğ¸
)ğ‘‘ğ‘‚
we can compute in time ğ‘›ğ‘‚
1

ğ‘‘ and a Gaussian matrix ğ‘Š

â„ğ‘›

âˆˆ

Ã—

(

(

ğ‘¡

0, 1
ğ‘
)
(
) a unit vector

âˆ¼

Ã—
ğ‘£
Ë†

p

â„ğ‘‘, a standard Gaussian vector ğ‘¢0 âˆ¼

, an arbitrary matrix
0, Idğ‘›
)
(
ğ‘‘ such that ğ‘Š , ğ‘¢0 are distributionally independent. Then

ğ‘

ğ‘›

â„ğ‘‘ such that with probability 1

ğ‘œ

,
1
)
(

âˆ’

âˆˆ

1

ğ‘£, ğ‘£0i

âˆ’ h Ë†

2 . ğ‘˜

ğ›½ Â· r

log ğ‘‘
ğ‘›ğ‘¡ + s

ğ‘˜
ğ›½ğ‘›  r

log

ğ‘‘
ğ‘˜ + k

ğ¸

k1

â†’

2

! Â·  

1

+

1

ğ›½ !

,

2 denotes the largest norm of a column of ğ¸. Furthermore, the same kind of guarantees hold if

p

where
ğ¸
k1
ğ‘¢0 is a vector with

â†’

k

ğ‘¢0k

k

2 = Î˜

ğ‘›

(

)

independent of ğ‘Š.

We will prove Theorem 5.10 and Theorem 5.11 using Algorithm 5.12.

Algorithm 5.12 (Algorithm for Sparse PCA with Adversarial Corruptions).

Given: Sample matrix ğ‘Œ =

Â·
Estimate: The sparse vector ğ‘£0.

p

ğ›½

ğ‘¢0ğ‘£ğ‘‡

0 +

ğ‘Š

ğ¸

+

âˆˆ

â„ğ‘›

Ã—

ğ‘‘ from model 1.1, system

ğ‘ ğ‘£

ğ’

âˆˆ {ğ’œ

ğ‘ ,ğ‘£ ,

ğ‘ ,ğ‘£

}

â„¬

Operation:

1. ï¬nd a level-4ğ‘¡ pseudo-distribution ğ· that satisï¬es

2. Output a top eigenvector

ğ‘£ of ğ”¼ ğ‘£ğ‘£T.
Ë†

ğ‘ ,ğ‘£ and maximizes ğ”¼

ğ’

ğ‘Œğ‘£

k

2
2.

k

ğ‘ ,ğ‘£ also satisï¬es

Let us analyze the algorithm. The ï¬rst observation is that any pseudo-distribution satisfying
ğ‘ ,ğ‘£ is a feasible
ğ‘ ,ğ‘£. Next we show that any pseudo-distribution satisfying
â„¬
solution to SDP-1. This will allows us to use Theorem 4.6 and conclude the proofs of Theorem 5.10
and Theorem 5.11.

ğ’œ

ğ’œ

Lemma 5.13. Let ğ· be any pseudo-distribution of degree > 4 satisfying
solution to SDP-1.

ğ’œ

ğ‘ ,ğ‘£. Then ğ”¼ğ· ğ‘£ğ‘£T is a feasible

37

Proof. Since ğ· satisï¬es
with entries in
1
+
pseudo-distributions,

{âˆ’

1,

}

ğ‘ ,ğ‘£, Tr ğ”¼ğ· ğ‘£ğ‘£T = ğ”¼ğ·
=

ğ’œ
such that

ğ”¼ğ· ğ‘£ğ‘£T

1

ğ‘–6ğ‘‘ ğ‘£2
ğ‘–
ğ‘¥ğ‘¥T, ğ”¼ğ· ğ‘£ğ‘£T
Ã
h

i

= 1. Now, there exists a vector ğ‘¥

â„ğ‘‘

. By Cauchy-Schwarz inequality for

âˆˆ

T
ğ‘¥ğ‘¥

h

(cid:13)
(cid:13)
T
, ğ”¼ğ· ğ‘£ğ‘£

= ğ”¼ğ·

i

(cid:13)
(cid:13)

Ã•ğ‘–,ğ‘—6ğ‘‘

ğ‘¥ğ‘– ğ‘ ğ‘–ğ‘£ğ‘– ğ‘¥ ğ‘— ğ‘  ğ‘—ğ‘£ ğ‘—

ğ‘– ğ‘¥2
ğ‘¥2

ğ‘— ğ‘ 2

ğ‘– ğ‘ 2

ğ‘—

Â·

ğ”¼ğ·

s

Ã•ğ‘–,ğ‘—6ğ‘‘

ğ‘– ğ‘£2
ğ‘£2

ğ‘—

6

ğ”¼ğ·

s
= ğ”¼ğ·

Ã•ğ‘–,ğ‘—6ğ‘‘
ğ‘ 2
ğ‘–

Ã•ğ‘–6ğ‘‘

=ğ‘˜ .

The result follows as ğ”¼ ğ‘£ğ‘£T

0.

(cid:23)

(cid:3)

We can now ï¬nish the analyses using the certiï¬ed upper bounds from the previous sections.

Proof of Theorem 5.10. Let ğ· be the pseudo-distribution in Algorithm 5.12. By Lemma 5.6, with
1
probability at least 0.99, ğ”¼ğ·
ğ‘¡ ğ‘¡
outputs ğ‘£0 satisï¬es
immediately get,

. Note that since the pseudo-distribution that
ğ‘ ,ğ‘£, by Lemma 5.13, ğ”¼ğ· ğ‘£ğ‘£T satisï¬es the premises of Theorem 4.6. Then we

2
2 6 ğ‘‚

ğ‘¡ ğ‘˜1
âˆ’

ğ‘‘1
/

ğ‘Š ğ‘£

ğ’œ

k

k

(cid:16)

(cid:17)

ğ”¼ğ·

1

âˆ’

ğ‘£, ğ‘£0i

h

2 . ğ‘˜

ğ›½ğ‘› Â·

ğ‘¡

Â·

ğ‘¡

1
/

ğ‘‘
ğ‘˜

(cid:1)
The result follows applying Lemma H.3.

(cid:0)

Similarly,

1
ğ›½ + s

+

ğ‘˜
ğ›½ğ‘›  r

log

ğ‘‘
ğ‘˜ + k

ğ¸

2

k1

â†’

! Â·  

1

+

1

ğ›½ !

.

p

(cid:3)

Proof of Theorem 5.11. Let ğ· be the pseudo-distribution in Algorithm 5.12. By Theorem 5.9,

with probability 1

ğ‘œ

,
1
)
(

âˆ’

ğ”¼ğ· ğ‘£T

ğ‘ŠTğ‘Š

ğ‘›Id

ğ‘£

6 ğ‘‚

âˆ’

(cid:16)
distribution that outputs ğ‘£0 with probability 1 satisï¬es
(cid:12)
(cid:12)
premises of Theorem 4.6. Then we immediately get,

(cid:12)
(cid:12)

(cid:0)

(cid:1)

ğ‘˜
âˆšğ‘¡

â„¬

ğ‘› log ğ‘‘

. Note that since the pseudo-
ğ‘ ,ğ‘£, by Lemma 5.13, ğ”¼ğ· ğ‘£ğ‘£T satisï¬es the
p

(cid:17)

ğ”¼ğ·

1

âˆ’

ğ‘£, ğ‘£0i

h

2 .

ğ‘˜
ğ›½âˆšğ‘›ğ‘¡

log ğ‘‘

ğ‘˜
ğ›½ğ‘›  r

log

ğ‘‘
ğ‘˜ + k

ğ¸

+ s

k1

â†’

2

! Â·  

1

+

1

ğ›½ !

.

The result follows applying Lemma H.3.

p

(cid:3)

6 Unconditional lower bounds for distinguishing

6.1 Low-degree polynomials

The goal of this section is to formalize our lower bounds. In light of the discussions in Section 2.5
and Section 3.3 we study distinguishing problems between two distributions over matrices: the
null distribution ğœˆ, which in our case is a standard Gaussian, and the planted distribution ğœ‡ that

38

contains some sparse signal hidden in random (and adversarial) noise. That is, given an instance
ğ‘Œ sampled either from the null or from the planted distribution, the goal is to determine whether
ğ‘Œ contains a planted signal. We will show that a large class of polynomial time algorithms (cap-
turing the best known algorithms) cannot distinguish between the null and the planted case even
when information-theoretically possible. Speciï¬cally, we will show that low degree polynomial
estimators cannot solve these problem. Similarly to [HKP+17b, HS17, DKWB19], we study the low
degree analogue of the ğœ’2-divergence between probability measures.

Deï¬nition 6.1. Let ğœ‡ and ğœˆ be probability distributions over â„ğ‘›
Ã—
and 0 < ğ•ğœˆ ğ‘“ <
functions ğ‘“
respect to ğœˆ is deï¬ned as

â„ such that

ğ”¼ğœ‡ ğ‘“

: â„ğ‘›

â†’

âˆ

<

Ã—

ğ‘‘

(cid:12)
(cid:12)
ğœ‡
(

ğœ’2

(cid:12)
(cid:12)

ğœˆ

)

k

ğ”¼ğœ‡ ğ‘“

ğ”¼ğœˆ ğ‘“

âˆ’
ğ•ğœˆ ğ‘“

2

.

(cid:1)

= sup
ğ¹ (cid:0)
ğ‘“

âˆˆ

ğ‘‘, and denote by ğ¹ the set of all
. The ğœ’2-divergence of ğœ‡ with

âˆ

Note that this value is related to the likelihood ratio ğ¿ described in Section 3.3: the fraction in

Recall that, if ğœ’2

the right hand side is maximized for ğ‘“ = ğ¿, and ğœ’2
ğœˆ

= ğ”¼ğœˆ ğ¿2
is bounded, then ğœ‡ and ğœˆ are information-theoretically indistinguishable
in the sense of Section 3.3.1. The low-degree analogue of ğœ’2-divergence is deï¬ned similarly. Denote
by â„
ğ‘Œ
is the space of
]
[
polynomials of ğ‘›

6ğ· the set of polynomials of degree at most ğ· in â„
ğ‘Œ
[
ğ‘‘ variables corresponding to the entries of ğ‘Œ).

(where â„
ğ‘Œ
[

ğœ‡
(

ğœ‡
(

1.

âˆ’

ğœˆ

k

k

]

]

)

)

Â·

Deï¬nition 6.2. Let ğ· > 0 and let ğœ‡ and ğœˆ be probability distributions over â„ğ‘›
â„
absolutely continious and for all ğ‘
ğ‘Œ
[
of ğœ‡ with respect to ğœˆ is deï¬ned as

and ğ•ğœˆ ğ‘ <

ğ”¼ğœ‡ ğ‘

6ğ·,

ğ‘‘ such that ğœˆ is
. The degree-ğ· ğœ’2-divergence

âˆ

âˆ

<

âˆˆ

Ã—

]

ğœ’2
ğœ‡
6ğ· (

ğœˆ

)

k

where we assume that 0
/

0 = 0.

= sup
â„
ğ‘Œ
ğ‘
[

âˆˆ

]

6ğ· (cid:0)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

ğ”¼ğœ‡ ğ‘

ğ”¼ğœˆ ğ‘

âˆ’
ğ•ğœˆ ğ‘

2

,

(cid:1)

Note that since ğœˆ is absolutely continuous, the denominator ğ•ğœˆ ğ‘ is zero if and only if ğ‘ is

constant (and in this case the numerator is also zero).

6.2 Spiked covariance model with sparsity

The ï¬rst problem we will look into is a variant of the standard sparse spiked covariance model
which we use to prove the lower bound in Theorem 1.6.

Problem 6.3. (Spiked Covariance Model with Sparsity) Given a matrix ğ‘Œ

â„ğ‘›

Ã—

ğ‘‘, decide whether:

âˆˆ

ğ‘

0, 1
)
(

ğ‘›

Ã—

ğ»0: ğ‘Œ = ğ‘Š where ğ‘Š

ğ‘‘ is a standard Gaussian matrix.

ğ»1: ğ‘Œ = ğ‘Š

âˆ¼
T, where ğ‘Š, ğ‘¢, and ğ‘£ are mutually independent, ğ‘Š
ğ›½ğ‘¢0ğ‘£0
standard Gaussian matrix, ğ‘¢
ğ‘¢ğ‘– distributed symmetrically around zero such that

ğ‘‘ is a
â„ğ‘› is a random vector with i.i.d. 1-subgaussian coordinates
â„ğ‘‘ is a

6 ğ‘… for some ğ‘… > 1, and ğ‘£

0, 1
)
(

ğ‘¢ğ‘–

p

ğ‘

âˆ¼

+

âˆˆ

Ã—

ğ‘›

|

|

âˆˆ

39

random vector with i.i.d. coordinates ğ‘£ğ‘– that take values

ğ‘£ğ‘– =

1
âˆšğ‘˜

âˆ’
1
âˆšğ‘˜
0

with probability ğ‘˜

with probability ğ‘˜

2ğ‘‘,

/
2ğ‘‘,

|

ğ‘œ

/

âˆˆ

.

otherwise.

ï£±ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´
Letâ€™s take a moment to compare the model in ğ»1 with model 1.1. First, note that we require
ï£³
6 ğ‘…, which formally does not hold for Gaussian distribution for any ğ‘…
â„. So to get lower
ğ‘¢ğ‘–
|
bounds for the single spike model we should consider not ğ»1, but ğ»â€²1 such that ğ‘¢ from ğ»1 is
log ğ‘› for all
replaced by a Gaussian vector ğ‘¢â€² âˆ¼ ğ’©(
1 6 ğ‘– 6 ğ‘›. If ğ‘¢ is drawn from a truncated Gaussian distribution ğ‘¢ğ‘– = sign
, ğ¶
,
(
}
then all ğ‘¢ğ‘– are 1-subgaussian and ğ‘¢ğ‘– 6 ğ‘… = ğ¶
log ğ‘›. Moreover, with high probability over ğ‘¢â€²,
ğ‘¢ğ‘– = ğ‘¢â€²ğ‘– for all ğ‘–. Hence if it is hard to distinguish between ğ»0 and ğ»1 with ğ‘… = ğ¶
log ğ‘›, it is
, then with high probability ğ‘£ is
also hard to distinguish between ğ»0 and ğ»â€²1. Second, if ğ‘˜
Ëœğ‘˜ = ğ‘˜
-sparse. Also note that ğ‘£ might not be a unit vector, but with high probability over ğ‘£
1
1
(
))
(
its norm is 1
a Ëœğ‘˜ = ğ‘˜

. So with high probability over ğ‘£, ğ‘Œ from ğ»1 is equal to ğ‘Š
1
(
)
ğ‘£
-sparse unit vector and

. However, with high probability
)
ğ‘¢â€²ğ‘– )

ğ‘£T, where
Ëœ

ğ‘¢â€²ğ‘– |
|
min

ğ‘£ is
Ëœ

0, Idğ‘›

â†’ âˆ

ğ‘›, then the algorithm that just computes the top singular value
of ğ‘Œ can distinguish between ğ»0 and ğ»1. On the other hand if ğ›½ & ğ‘˜
log ğ‘‘, then Diagonal
âˆšğ‘›
Thresholding can distinguish between ğ»0 and ğ»1. Under Conjecture 3.13 (see[Hop18, BKW20b] for
a formal discussion), the following theorem provides formal evidence that there is no polynomial
6

time algorithm that can improve over the guarantees of Diagonal Thresholding whenever
ğ‘˜ 6 ğ‘‘1
2
âˆ’
/
(
regimes where âˆšğ‘‘ 6 ğ‘˜
bound does not match the best know guarantees only by a factor ğ‘œ
can be found in Section 6.5.

1
). Furthermore the theorem also implies that SVD with Thresholding is optimal in
ğ‘˜2 . Finally we remark that in settings where ğ‘˜ = ğ‘‘1

1
) the lower
(
. The proof of theorem
)

log ğ‘‘
(

log ğ‘‘

ğ‘‘
log ğ‘‘

+
1
))
(
Recall now that if ğ›½ &

Ëœğ›½ = ğ›½

.
1
))
(

log ğ‘›

ğ‘¢â€²ğ‘– |
p

= ğ›½

Ëœğ›½ğ‘¢

1
(

1
(

/k

q

q

q

p

p

p

p

p

{|

+

+

+

+

ğ‘‘

/

ğ‘œ

ğ‘œ

ğ‘œ

Î©

k

âˆ’

ğ‘œ

Theorem 6.4. Let ğœˆ and ğœ‡ be the null and the planted distributions of Problem 6.3 respectively. If ğ· 6 ğ‘›
and

/

ğ‘…4

where 0 < ğœ€ < 1

1000 , then for any nonconstant polynomial ğ‘ : â„ğ‘›

ğ‘‘

Ã—

ğ›½ = ğœ€

min

Â·

ğ‘‘
ğ‘›

,

ğ‘˜
âˆšğ·ğ‘› Â·

ğ‘‘

ğ·

Â·
ğ‘˜2

ln

(cid:18)

nr

(cid:18) (cid:12)
(cid:12)
(cid:12)
(cid:12)

1

+

,

(cid:19) o

â„ of degree at most ğ·,

(cid:19) (cid:12)
(cid:12)
(cid:12)
(cid:12)
â†’

ğ”¼ğœ‡ ğ‘

(cid:0)

ğ‘Œ
) âˆ’
(
ğ•ğœˆ ğ‘

ğ”¼ğœˆ ğ‘
ğ‘Œ
(

)

2

ğ‘Œ
(

)

(cid:1)

6 ğœ’2

ğœ‡
6ğ· (

ğœˆ

)

k

. ğœ€2 .

Theorem 6.4 implies that polynomials of degree ğ· 6 ğ‘›

sample from the single spike model and a sample of standard Gaussian vectors if ğ›½
. In particular, if ğ‘˜ 6 ğ‘‘1
2
âˆ’
/

log2 ğ‘› cannot distinguish between a
ğ‘› and
), then polynomials of degree . log ğ‘‘ cannot solve
1

log

â‰ª

ğ›½

ğ‘‘

/

/

Î©

(

p

ğ‘˜
âˆšğ·ğ‘›

â‰ª

ğ·
ğ‘‘
Â·
ğ‘˜2

(cid:17)
the problem for ğ›½

(cid:16)

ğ‘˜
ğ‘›

log ğ‘‘.

â‰ª

We remark that in [DKWB19] the authors provided a similar hardness result for low-degree
log2 ğ‘›), but it does

polynomials. Their lower bound works for all ğ· 6 ğ‘œ

(not only for ğ· 6 ğ‘›

p

ğ‘›

(

)

/

40

not contain a log ğ‘‘ factor. In particular, if ğ‘˜ 6 ğ‘‘1
ğ‘˜
ğ›½
âˆšğ·ğ‘›
Diagonal Thresholding.

log ğ‘‘, and their lower bound is ğ›½

ğ‘˜
âˆšğ·ğ‘›

â‰ª

â‰ª

p

Î©

(

) and ğ· 6 ğ‘›
1

log2 ğ‘›, our lower bound is
/âˆ’
. Hence we are able to show a tight bound for

/

In [HKP+17b] a lower bound for a similar model is presented. More concretely, the authors
obtained exponential lower bounds for the Sum-of-Square Hierarchy in the Wigner model ğ‘Œ =
ğ›½ğ‘£ğ‘£T. Their results do not directly apply in our settings as the covariance matrix ğ‘ŒTğ‘Œ is far
ğ‘Š
from being Gaussian.

+

The same reasoning used in Theorem 6.4 can be used to obtain an information theoretic lower

bound.

Theorem 6.5. Let ğœˆ and ğœ‡ be the null and the planted distributions of Problem 6.3 respectively. If

where 0 < ğœ€ < 1

1000 , then for any ğ‘“ : â„ğ‘›

ğ‘‘

Ã—

â†’

ğ›½ = ğœ€

ğ‘…2 Â·

min

ğ‘‘
ğ‘›

,

ğ‘˜
ğ‘› Â·

ln

ğ‘›
ğ‘‘
Â·
ğ‘˜2

nr

(cid:18)

(cid:19) (cid:12)
(cid:18) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
â„ such that 0 < ğ•ğœˆ ğ‘“
ğ‘Œ
(cid:12)
(cid:12)
(

)

+

1

,

(cid:19) o
<

âˆ

,

ğ”¼ğœ‡ ğ‘“

(cid:0)

ğ‘Œ
) âˆ’
(
ğ•ğœˆ ğ‘“

ğ”¼ğœˆ ğ‘“
ğ‘Œ
(

)

2

ğ‘Œ
(

)

(cid:1)

6 ğœ’2

ğœ‡
(

k

ğœˆ

)

. ğœ€2 .

avoided using diï¬€erent techniques, indeed for ğ‘…

We remark that the term ğ‘…2 is a consequence of our analysis of ğœ’2-divergence and can be
, Theorem E.1 provides tighter guarantees.
1
ğœ”
)
(
ğ‘› log ğ‘‘

âˆˆ
Additionally, we point out that for ğ›½ < 1 a bound of

ğ‘˜2 can be achieved as shown in [BR13a].

ğ‘˜

6.3 Almost Gaussian vector in random subspace

q

In this section we prove Theorem 1.3. Concretely, we will show that in the presence of adversarial

, so that the degree ğ‘¡ SoS Algorithm 5.12
corruptions, whenever ğ‘¡
outperforms other known algorithms, no multilinear polynomial of degree . ğ‘›0.001can obtain
(cid:1)
similar guarantees unless ğ‘‘ &

(cid:0)

(cid:0)

(cid:1)

Â·

.

ğ‘¡

ğ‘›ğ‘¡ğ‘¡ ğ‘¡

Î©
Ëœ

1
/

ğ‘¡ & ğ‘›0.499 and ğ‘‘ >

ğ‘‘
ğ‘˜

Î©
Ëœ

ğ‘›
ln2 ğ‘¡

Similarly to Section 6.2 we design a speciï¬c distinguishing problem. In order to prove a lower
bound in the presence of adversarial corruptions, we need to carefully chose the adversarial matrix.

(cid:17)

(cid:16)

Problem 6.6. (Almost-Gaussian vector in a random subspace) Given a matrix Y in â„ğ‘›
whether:

ğ‘‘, decide

Ã—

ğ‘

0, 1
)
(

ğ‘›

Ã—

ğ»0: ğ‘Œ = ğ‘Š where ğ‘Š

ğ‘‘ is a standard Gaussian matrix.

ğ»1: ğ‘Œ = ğœ†ğ‘¢

âˆ¼
ğ¸, where ğ‘Š
âˆšğ‘› with probability 1
1
vector with i.i.d. coordinates that take values
/
/
is a vector with i.i.d. coordinates that take values

ğ‘‘ is a standard Gaussian matrix, ğ‘¢

0, 1
)
(

ğ‘£T
Ëœ

ğ‘Š

ğ‘

Â±

+

+

âˆ¼

Ã—

ğ‘›

âˆˆ
2 each, and

â„ğ‘› is a unit
â„ğ‘‘
ğ‘£
Ëœ

âˆˆ

2 ,

2 ,

/

/

ğ‘£ğ‘– =

âˆ’
1

0

ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´
ï£³

1 with probability ğ›¿

with probability ğ›¿

otherwise,

41

for some ğ›¿
following distribution. Let ğ‘  > 0 be the largest even number such that ğ›¿ğœ†ğ‘  6 2âˆ’
(cid:0)
ğ‘—

T, where ğ‘£â€² is sampled according to the
10ğ‘  . For all

. Furthermore ğ¸ = ğ‘¢

0, 1
]

ğ‘£â€² âˆ’

ğ‘ŠTğ‘¢

âˆˆ [

ğ‘‘

(cid:1)

,

âˆˆ [
]
â€“ if

ğ‘£ ğ‘— â‰  0, then ğ‘£â€²ğ‘—
Ëœ

= 0 ;

â€“ otherwise ğ‘£â€²ğ‘— is sampled from the distribution ğœ‚ that has ï¬nite support supp

ğœ‚

10ğ‘ , 10ğ‘ 

]

[âˆ’

and moments:

(cid:0)

(cid:1)

âŠ†

ğœ†ğ‘Ÿ ğ›¿

ğ‘Ÿ

(
6

!!

1
âˆ’
)
âˆ’
1
ğ›¿
âˆ’
10ğ‘ 
(

ğ‘Ÿ

)

ğ‘¥ğ‘Ÿ =

ğ”¼
ğ‘¥
ğœ‚

if 0 6 ğ‘Ÿ 6 ğ‘  and even
if ğ‘Ÿ > ğ‘ 

2 and even

ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´
Proposition F.2 shows that if ğ›¿ğœ†ğ‘  6 2âˆ’
10ğ‘  , then such ğœ‚ exists. Note that for ğ‘  = 0 the condition
ï£³
10ğ‘  is always satisï¬ed, so ğ‘  in the problem description is well-deï¬ned. Also note that if

if ğ‘Ÿ is odd.

0

âˆ¼

+

ğ›¿ğœ†ğ‘  6 2âˆ’
ğ‘  = 0, ğ‘£â€² is just a zero vector.

If ğ›¿ğ‘‘

â†’ âˆ

unit vector ğ‘£0 = 1
ğ‘£
k Ëœ

, then with high probability
ğ‘£ (where ğ‘˜ := ğ›¿ğ‘‘
ğ‘œ
1
k Ëœ
(
We will use the notation ğ‘£ = ğœ†
ğ‘£â€². Note that the coordinates of ğ‘£ are independent, have
ğ‘£
Ëœ
coordinates

âˆ’
+
Gaussian moments up to ğ‘ , and with high probability ğ‘£ has at least ğ›¿ğ‘‘
ğ‘£ ğ‘—

ğ‘œ
1
1
))
âˆ’
(
(
, ğ›½ = ğ‘˜ğœ†2
1
ğ‘› (

-sparse. So ğœ†ğ‘¢
ğ‘œ

ğ‘£ is ğ›¿ğ‘‘
Ëœ
1
)))
(

ğ‘£T =
Ëœ
and ğ‘¢0 = âˆšğ‘›ğ‘¢.
p

T for ğ‘˜-sparse

ğ›½ğ‘¢0ğ‘£0

1
))
(

1
))
(

1
(

âˆ’

+

ğœ†

ğ‘œ

.

âˆˆ {Â±

}

Geometric description. The planted distribution can be also described in geometric terms, where
the problem becomes that of distinguishing between a subspace spanned by independent Gaussian
vectors or a subspace spanned by independent Gaussian vectors and the planted vector ğ‘£.

ğ‘œ

âˆ’

The construction is the following: at ï¬rst we sample a signal vector ğ‘£
1
(

â„ğ‘‘ that has at least
coordinates with absolute values at least ğœ† (using the construction described above).
â„ğ‘‘, and perform a random
ğ‘¤1, . . . ,
1) on
Ëœ
âˆ’

ğ‘› with ï¬rst column vector ğ‘¢ (such that ğ‘ˆ is independent of ğ‘£,

1 i.i.d. standard Gaussian vectors

ğ‘¤1, . . . ,
Ëœ

1 âˆˆ
âˆ’

ğ‘¤ğ‘›
Ëœ

ğ‘¤ğ‘›
Ëœ

âˆ’

âˆˆ

Ã—

ğ›¿ğ‘‘
1
))
(
Then we sample ğ‘›
rotation ğ‘ˆ
ğ‘¤1, . . . ,
ğ‘£,
Ëœ

âˆˆ
ğ‘¤ğ‘›
Ëœ

â„ğ‘›

1. That is,
âˆ’

This formulation is equivalent to the one described above. Indeed,

ğ‘Œ = ğ‘ˆ

ğ‘£ğ‘‡
ğ‘¤ğ‘‡
1
Ëœ
...
ğ‘¤ğ‘‡
ğ‘›
1
Ëœ
âˆ’

Â·

Â©

.

Âª
Â®
Â®
Â®
Â®
Â¬

ğ‘Œ = ğ‘¢ğ‘£ğ‘‡

Â«
ğ‘›

1
âˆ’

+

Ã•ğ‘–=1

ğ‘¤ğ‘‡
ğ‘¢ğ‘– Ëœ
ğ‘– ,

where ğ‘¢, ğ‘¢1, . . . , ğ‘¢ğ‘›
(singular) Gaussian supported in the hyperplane orthogonal to ğ‘¢, and
standard Gaussian supported in the same hyperplane.

1 are the columns of ğ‘ˆ. Note that
âˆ’

Ã

is distributed as a standard
ğ‘¢ğ‘¢ğ‘‡
ğ‘Š is also a
)

Id
(

âˆ’

ğ‘›

1
âˆ’

ğ‘¤ğ‘‡
ğ‘–=1 ğ‘¢ğ‘– Ëœ
ğ‘–

The theorem below provides a lower bound for the Problem 6.6. The proof is in section 6.6.

42

 
 
 
 
Theorem 6.7. Suppose that ğ‘› 6 ğ‘‘, 1 6 ğ· 6 ğ‘›0.33, 0 < ğ›¿ < 1, ğ‘˜ = ğ›¿ğ‘‘, ğœ† > 2, and let ğ‘  > 2 be the
maximal even number such that ğ›¿ğœ†ğ‘  6 2âˆ’
1. Let ğœˆ and ğœ‡ denote respectively the null
and planted distribution (with parameters ğ›¿, ğœ†, ğ‘ ) of Problem 6.6. Suppose that ğœ† > 1000âˆšğ‘¡ ln ğ‘¡ and that
ğœ†4ğ·ğ‘¡2 ln2 ğ‘¡ = ğ‘œ

10ğ‘  , and ğ‘¡ = ğ‘ 

log2

as ğ‘›

. If

+

ğ‘›

1

2

/

Â·

ğ‘‘
ğ‘˜2

+

(cid:16)

(cid:16)

(cid:16)

(cid:17)

(cid:17) (cid:17)

â†’ âˆ

ğ‘‘ = ğ‘œ

1
ğœ†4 Â·

ğ‘›
ln2 ğ‘¡

ğ¶

(cid:18)

Â·

ğ‘¡

ğ· (cid:19)

!

Â·

(for some constant ğ¶ that does not depend on ğ‘›, ğ‘‘, ğ›¿, ğœ†, ğ‘  and ğ·), then for any non-constant

as ğ‘›
multilinear polynomial ğ‘ : â„ğ‘›

â†’ âˆ

ğ‘‘

Ã—

â†’

â„ of degree at most ğ·,

ğ”¼ğœ‡ ğ‘

(cid:0)

ğ‘Œ
) âˆ’
(
ğ•ğœˆ ğ‘

ğ”¼ğœˆ ğ‘
ğ‘Œ
(

)

2

ğ‘Œ
(

)

(cid:1)

0 ,

â†’

as ğ‘›

.

â†’ âˆ
Letâ€™s try to illustrate the meaning of Theorem 6.7. If ğœ† > ğµ

log ğ‘‘ for suï¬ƒciently large ğµ, then
ğ›¿ is so that ğ›¿ğœ†ğ‘  = 2âˆ’
. Here Algorithm 5.12 can distinguish
p
â†’ âˆ
between the null and the planted distribution if ğ‘‘ & ğ‘›ğ‘¡ logğ‘¡
ğ‘¡ ğ‘¡ in polynomial time. Indeed, in this
ğ‘›
)
(
ğ‘£ such that
case with probability at least 0.99 the algorithm 5.12 outputs
Ë†

10ğ‘  for an even constant ğ‘  and ğ›¿ğ‘‘

1

ğ‘£, ğ‘£0i

âˆ’ h Ë†

ğ‘¡

1
/

2 . 1

ğœ†2  

ğ‘¡

1
ğ›¿

(cid:19)

Â·

(cid:18)

ğ¸

2
1
â†’

2

k

!

+ k

6 ğ‘¡

1
1ğ›¿
+

(cid:19)

ğœ†ğ‘ 

Â·

(cid:18)

ğ‘¡

1
/

ğ¸

k

2

k1
ğœ†

â†’

2

=

(cid:19)

220ğ‘¡
ğœ†1
/

ğ‘¡ +

(cid:18)

ğ¸

k

2

k1
ğœ†

â†’

2

.

(cid:19)

+

(cid:18)

The ï¬rst term tends to 0 and

ğ¸

k

k1

â†’

2 can be bounded as follows:

ğ¸

k

k1

â†’

2 6

ğ‘£â€²

ğ‘¢

(

k

T
)

k1

â†’

2 + k

T
ğ‘¢ğ‘¢

ğ‘Š

2 6 max
16ğ‘–6ğ‘‘

k1

â†’

ğ‘£â€²ğ‘– +

max
16ğ‘–6ğ‘‘

T
ğ‘¢

ğ‘Š

. ğ‘ 

ğ‘–

+

log ğ‘‘ ,

(cid:1)
since ğ‘¢Tğ‘Š is a standard Gaussian vector. Hence for suï¬ƒciently large ğµ,
ğ‘£, ğ‘£0i
h Ë†

If in addition ğ· 6 ğ‘›0.001 and ğœ† 6 ğ‘›0.24, then the conditions of Theorem 6.7 are satisï¬ed. Hence
in this case for ğ‘‘ 6 ğ‘›0.999ğ‘¡
1 no multilinear polynomial of degree at most ğ‘›0.001 can distinguish
âˆ’
. Furthermore note that if ğœ†4 & ğ‘› log ğ‘‘,
between the planted and the null distribution as ğ‘›
then Diagonal thresholding can distinguish between the planted and the null distribution in
1 ). Finally, it easy to see that exhaustive search works as long
polynomial time (even if ğ‘‘
âˆ’
as ğœ† &

ğ‘›0.999ğ‘¡

â†’ âˆ

log ğ‘‘

p
> 0.99.

â‰ª

ğ‘˜.

(cid:0)

/

p

6.4 Chi-squared-divergence and orthogonal polynomials

Recall that given a hypothesis testing problem with null distribution ğœˆ and planted distribution ğœ‡,
we say a polynomial ğ‘

6ğ· cannot distinguish between ğœ‡ and ğœˆ if

ğ‘Œ
(

) âˆˆ

â„
ğ‘Œ
[

]

ğ”¼ğœ‡ ğ‘

(cid:12)
(cid:12)

ğ‘Œ
(
) âˆ’
ğ•ğœˆ ğ‘

ğ”¼ğœˆ ğ‘

ğ‘Œ
(

)

ğ‘Œ
(

)
(cid:12)
(cid:12)

6 ğ‘œ

.

1
)
(

(6.1)

So, if for some distinguishing problem this ratio is small for all ğ‘
6ğ·, then polynomial
estimators of degree at most ğ· cannot solve this distinguishing problem. The key observation used

âˆˆ

]

â„
ğ‘Œ
[

p

43

 
to prove bounds for low degree polynomials is the fact that the polynomial which maximizes the
ratio (6.1) has a convenient characterization in terms of orthogonal polynomials with respect to the
null distribution.

Formally, for any linear subspace of polynomials

6ğ· and any absolutely continuous
probability distribution ğœˆ such that all polynomials of degree at most 2ğ· are ğœˆ-integrable, one can
deï¬ne an inner product in the space

6ğ· as follows

6ğ·

ğ’®

âŠ†

]

â„
ğ‘Œ
[

ğ’®

(cid:10)
Hence we can talk about orthonormal basis in

ğ’®

(cid:11)

6ğ· with respect to this inner product.

ğ‘, ğ‘

âˆ€

6ğ·

âˆˆ ğ’®

ğ‘, ğ‘

= ğ”¼
ğ‘Œ
ğœˆ

âˆ¼

ğ‘

ğ‘

ğ‘Œ
(

)

ğ‘Œ
(

)

.

6ğ·

Proposition 6.8. Let
6ğ· be a linear subspace of polynomials of dimension ğ‘. Suppose that
ğ’®
ğ‘‘ such that any polynomial of degree at most ğ· is
ğœˆ and ğœ‡ are probability distributions over ğ‘Œ
Ã—
ğœ‡-integrable and any polynomial of degree at most 2ğ· is ğœˆ-integrable. Suppose also that ğœˆ is absolutely
continuous. Let

with respect to ğœˆ. Then

ğ‘
ğ‘–=1 be an orthonormal basis in

â„ğ‘›

ğœ“ğ‘–

6ğ·

âŠ†

âˆˆ

]

â„
ğ‘Œ
[

ğ‘Œ
(

)}

{

ğ‘Œ
[

]

ğ’®

max
ğ‘
âˆˆğ’®

6ğ· (cid:0)

ğ”¼ğœ‡ ğ‘
ğ‘Œ
(
ğ”¼ğœˆ ğ‘2

)
ğ‘Œ
(

(cid:1)
)

2

ğ‘

=

Ã•ğ‘–=1 (cid:18)

ğœ“ğ‘–

ğ”¼
ğœ‡

2

.

(cid:19)

Proof. For any ğ‘

6ğ·

âˆˆ ğ’®

ğ”¼
ğœ‡

ğ‘

ğ‘Œ
(

)

= ğ”¼
ğœ‡

)}

Since the system

ğœ“ğ‘–

ğ‘Œ
(

{

Hence we get

ğ‘

ğ‘ğ‘–ğœ“ğ‘–

ğ‘Œ
(

)

ğ‘

=

ğ‘ğ‘– ğ”¼
ğœ‡

ğœ“ğ‘–

ğ‘Œ
(

)

6

ğ‘

ğ‘2
ğ‘–

Ã•ğ‘–=1
ğ‘
ğ‘–=1 is orthonormal with respect to ğœˆ,

Ã•ğ‘–=1

Ã•ğ‘–=1

2
1
/

ğ‘

!

Ã•ğ‘–=1 (cid:18)

2

2
1
/

ğ”¼
ğœ‡

ğœ“ğ‘–

ğ‘Œ
(

)

!

(cid:19)

.

ğ‘2

ğ”¼
ğœˆ

ğ‘Œ
(

)

=

ğ‘2
ğ‘– .

ğ‘

Ã•ğ‘–=1

ğ”¼ğœ‡ ğ‘

ğ”¼ğœˆ ğ‘2

)

1
2

ğ‘Œ
(
ğ‘Œ
(
ğœ“ğ‘–

)
ğ‘Œâ€²)
(cid:1)
(

ğ‘

6

"

2

2
1
/

.

ğœ“ğ‘–

ğ”¼
ğœ‡

#

(cid:19)

Ã•ğ‘–=1 (cid:18)
ğ‘Œ
maximizes the ratio.
(

ğœ“ğ‘–

ğ‘
ğ‘–=1

ğ”¼ğ‘Œâ€²âˆ¼
(cid:0)

ğœ‡

(cid:3)

Note that the polynomial

)
From now on we assume that the distribution ğœˆ is Gaussian. In this case a useful orthonormal

Ã

(cid:3)

(cid:2)

basis in â„
ğ‘Œ
[

]

6ğ· is the system of Hermite polynomials.

:

:

]

ğ‘›

{
]}

ğ‘›
ğ‘–
âˆˆ [
. For ğ‘—

, let ğ¼ğ›¼ :=
ğ‘›

To work with Hermite polynomials we introduce some useful notation. For a multi-index ğ›¼
ğ›¼ for some ğ‘—
ğ‘‘
:
over
]
] Ã— [
[
, and similarly let ğ½ğ›¼,ğ‘– :=
ğ›¼ for some ğ‘–
ğ‘–, ğ‘—
) âˆˆ
(
ğ‘‘,
ğ‘–, ğ‘—
ğ‘‘
ğ‘—
ğ›¼
{
âˆˆ [
) âˆˆ
(
ğ›¼ğ‘–ğ‘—
ğ‘‹ ğ›¼ :=
ğ‘›
ğ›¼ ğ‘‹
can be represented as a bipartite
Ã
ğ‘–ğ‘—
[
(
multigraph ğºğ›¼ =
has multiplicity ğ›¼ğ‘–ğ‘—. In this representation
Ã
the set ğ½ğ›¼,ğ‘– corresponds to the neighborhood of the vertex ğ‘– and the set ğ¼ğ›¼,ğ‘— corresponds to the
neighborhood of ğ‘—. If ğ›¼ is multilinear, ğºğ›¼ is just a graph (i.e. multiplicity of each edge is 1).

ğ‘–, ğ‘—
(
ğ‘‘
âˆˆ [
]
âˆˆ [
. We will use the notation ğ›¼! :=
}
. Note that every multi-index ğ›¼ over
ğ‘–, ğ‘—
ğ¼ğ›¼

ğ‘‘
]}
ğ‘–, ğ‘—
(
ğ›¼ ğ›¼ğ‘–ğ‘—! and for a matrix ğ‘‹
)âˆˆ
ğ‘‘

and similarly ğ½ğ›¼ :=

such that each edge

) âˆˆ
, let ğ¼ğ›¼,ğ‘— :=

ğ½ğ›¼, ğ¸ğ›¼)

âˆˆ [
:
]
ğ‘–,ğ‘—

] Ã— [

âˆˆ [

âˆˆ [

) âˆˆ

â„ğ‘›

Ã

ğ›¼

âˆˆ

ğ‘›

ğ‘–,ğ‘—

ğ‘‘

)âˆˆ

{

{

{

}

}

Ã—

]

]

]

(

(

ğ‘–

ğ‘—

44

 
 
For a multi-index ğ›¼ over

ğ‘›

[

] Ã— [

ğ‘‘

the corresponding Hermite polynomial is

]
ğ‘Œ
ğ»ğ›¼(

)

=

ğ»ğ›¼ğ‘–ğ‘—(

ğ‘Œğ‘–ğ‘—

)

,

ğ¼ğ›¼, ğ‘—
ğ½ğ›¼ Ã–ğ‘–
âˆˆ
â„¤ is a degree ğ‘™ one variable Hermite polynomial, deï¬ned as follows

Ã–ğ‘—
âˆˆ

where ğ»ğ‘™ for ğ‘™

âˆˆ

=

ğ»ğ‘™

ğ‘¥

(

)

Ã•06ğ‘Ÿ6ğ‘™

ğ‘Ÿ is even

ğ‘™

âˆ’

ğ‘Ÿ

ğ‘™

âˆ’
2

1
2

(cid:19)

âˆ’

(cid:18)

1
ğ‘™

âˆ’
2

ğ‘Ÿ

!

ğ‘Ÿ!

ğ‘¥ğ‘Ÿ .

(cid:0)
= 1. Hence by applying Proposition 6.8 to the subspace of polynomials such that

(cid:1)

Note that ğ»
ğ”¼ğœˆ ğ‘
ğ‘Œ
(

ğ‘Œ
âˆ…(
= 0, we get

)

)

Corollary 6.9. Let ğœˆ be Gaussian. Suppose that the distribution ğœ‡ is so that any polynomial of degree at
most ğ· is ğœ‡-integrable. Then

max
â„
ğ‘Œ
]
[
âˆˆ

ğ‘

6ğ· (cid:0)

ğ”¼ğœ‡ ğ‘

ğ‘Œ
) âˆ’
(
ğ•ğœˆ ğ‘

ğ”¼ğœˆ ğ‘
ğ‘Œ
(

)

2

ğ‘Œ
(

)

=

(cid:1)

ğ”¼
ğœ‡

ğ‘Œ
ğ»ğ›¼(

)

2

.

(cid:19)

Ã•0<
ğ›¼
|
|
6ğ· the space of multilinear polynomials of degree at most ğ· (we do not include
6ğ· ). Note that multilinear Hermite polynomials ğ»ğ›¼ (which correspond

â„³

6ğ· (cid:18)

Denote by

constant polynomials in
to multilinear multiindices ğ›¼) are exactly

â„³

They form a basis in the space
Applying Proposition 6.8 to the space

â„³

6ğ· (for 0 <

6 ğ·). Letâ€™s denote

6ğ· :=

6ğ·

â„‹

6ğ· .

â„³

â„‹â„³

Ã‘

ğ‘Œ
ğ»ğ›¼(

)

=

ğ‘¦ğ‘–,ğ‘— .

ğ¼ğ›¼, ğ‘—
ğ½ğ›¼ Ã–ğ‘–
Ã–ğ‘—
âˆˆ
âˆˆ
ğ›¼
6ğ· we get

|

|

â„³

Corollary 6.10. Let ğœˆ be Gaussian. Suppose that the distribution ğœ‡ is so that any polynomial of degree at
most ğ· is ğœ‡-integrable. Then

max
ğ‘
âˆˆâ„³

6ğ· (cid:0)

ğ”¼ğœ‡ ğ‘

ğ‘Œ
) âˆ’
(
ğ•ğœˆ ğ‘

ğ”¼ğœˆ ğ‘
ğ‘Œ
(

)

2

ğ‘Œ
(

)

(cid:1)

= max
ğ‘
âˆˆâ„³

6ğ· (cid:0)

2

ğ”¼ğœ‡ ğ‘
ğ‘Œ
(
ğ”¼ğœˆ ğ‘2

)
ğ‘Œ
(

(cid:1)
)

=

ğ”¼
ğœ‡

ğ‘Œ
ğ»ğ›¼(

)

2

.

(cid:19)

ğ‘Œ
Ã•ğ»ğ›¼
(

)âˆˆâ„‹â„³

6ğ· (cid:18)

Hence the key part of proving lower bounds for low degree polynomial estimators is bounding

ğ”¼ğœ‡ ğ»ğ›¼(
ğ‘Œ

.
)

6.5 Spiked covariance model with sparsity (proof)

In this section we prove Theorems 6.4 and 6.5 .

The proofs will be based on two steps: ï¬rst, we compute the expectation of Hemrite polynomials

under the planted distribution, then we bound their total contribution.

We will need the following fact about Hermite polynomials:

Fact 6.11. For any ğ‘

â„ and any ğ‘™

â„•

âˆˆ

âˆˆ

ğ”¼

ğ‘¤

0,1
)

âˆ¼ğ’©(

ğ»ğ‘™

ğ‘¤

(

ğ‘

)

+

=

ğ‘ ğ‘™
ğ‘™!

.

45

The following Lemma is a generalization (and a corrollary) of Fact 6.11.

Lemma 6.12. Let ğ‘‹
standard Gausian matrix independent of ğ‘‹. Then for any multi-index ğ›¼ over

ğ‘‘ be a random matrix such that all moments of ğ‘‹ exist. Let ğ‘Š

â„ğ‘›

âˆˆ

Ã—

ğ‘›

[

ğ‘‘

,

]

] Ã— [

ğ”¼ ğ»ğ›¼(
ğ‘Š

ğ‘‹

)

+

=

ğ”¼ ğ‘‹ ğ›¼
ğ›¼!

.

Proof. By Fact 6.11,

ğ”¼ ğ»ğ›¼(
ğ‘Š

ğ‘‹

)

+

= ğ”¼
ğ‘‹
= ğ”¼
ğ‘‹

ğ”¼
ğ‘Š [

ğ‘Š

ğ»ğ›¼(
ğ”¼
ğ‘Š

ğ‘‹

ğ‘‹

) |
+
ğ‘Šğ‘–ğ‘—
ğ»ğ›¼ğ‘–ğ‘—(

ğ‘–,ğ‘—
ğ›¼
Ã–(
)âˆˆ

]

ğ‘‹ğ‘–ğ‘—

)

+

ğ‘‹

(cid:3)

(cid:12)
(cid:12)

ğ‘‹

(cid:2)
ğ›¼ğ‘–ğ‘—
ğ‘–ğ‘—
ğ›¼ğ‘–ğ‘—!

= ğ”¼
ğ‘‹

ğ‘–,ğ‘—
ğ›¼
Ã–(
)âˆˆ
ğ”¼ ğ‘‹ ğ›¼
ğ›¼!

.

=

â„ğ‘›

ğ‘‘ be a

Ã—

âˆˆ

(cid:3)

Now we can exactly compute the expectation of Hermite polynomials under the planted distri-

bution.

Lemma 6.13. Let ğ›¼ be a multi-index over
every vertex of the multigraph ğºğ›¼ has even degree, then

] Ã— [

ğ‘›

ğ‘‘

[

]

and let ğœ‡ be the planted distribution of Problem 6.3. If

ğ”¼
ğ‘Œ
ğœ‡

ğ‘Œ
ğ»ğ›¼(

=

1
ğ›¼!

ğ›½
ğ‘˜

ğ›¼

2
|/

|

ğ½ğ›¼

|

|

ğ‘˜
ğ‘‘

ğ”¼ ğ‘¢

ğ‘–

degğºğ›¼ (
ğ‘–

)

(cid:19)
(cid:12)
(cid:12)
(cid:12)
and if at least one vertex of ğºğ›¼ has odd degree, then ğ”¼ğœ‡ ğ»ğ›¼(
ğ‘Œ
Proof. By lemma 6.12,

(cid:19)

(cid:18)

(cid:18)

âˆ¼

)
(cid:12)
(cid:12)
(cid:12)

Â·

ğ¼ğ›¼
Ã–ğ‘–
âˆˆ
= 0.

)

ğ”¼
ğ‘Œ
ğœ‡

âˆ¼

=

ğ‘Œ
ğ»ğ›¼(

)

ğ”¼

ğ›¼

ğ›½ğ‘¢ğ‘£ğ‘‡
ğ›¼!

)

(
p

=

ğ›½|

ğ›¼

2
|/
ğ›¼!

ğ”¼

ğ‘¢

ğ›¼ğ‘–ğ‘—
ğ‘– ğ‘£

ğ›¼ğ‘–ğ‘—
ğ‘—

=

ğ¼ğ›¼
Ã–ğ‘–
âˆˆ
ğ½ğ›¼
ğ‘—
âˆˆ

ğ›½|

ğ›¼

2
|/
ğ›¼!

ğ›¼ğ‘–ğ‘—

ğ½ğ›¼,ğ‘–

ğ‘—

âˆˆ

ğ”¼ ğ‘¢

ğ‘–
Ã

ğ¼ğ›¼
Ã–ğ‘–
âˆˆ

!

ğ½ğ›¼
Ã–ğ‘—
âˆˆ
Â©

=
ğ‘—
Notice that degğºğ›¼ (
ğ¼ğ›¼,ğ‘– ğ›¼ğ‘–ğ‘— and degğºğ›¼ (
)
least one vertex of ğºğ›¼ has odd degree, then ğ”¼ğ‘Œ
Ã

)

âˆˆ

ğ‘–

ğ‘—

ğ‘–

=
ğ½ğ›¼, ğ‘— ğ›¼ğ‘–ğ‘—. By symmetry of each ğ‘¢ğ‘– and ğ‘£ ğ‘—, if at
Â«
âˆˆ
ğ‘Œ
ğœ‡ ğ»ğ›¼(
Ã

= 0.

)

âˆ¼

Now assume that each vertex has even degree. Then ğ”¼ ğ‘£

degğºğ›¼ (

ğ‘—

) and

ğ‘—

degğºğ›¼ (
ğ‘—

)

= ğ‘˜
ğ‘‘

1
âˆšğ‘˜

(cid:17)

(cid:16)
degğºğ›¼ (
ğ‘—

ğ‘—

)

ğ”¼ ğ‘£

Â·

ğ½ğ›¼
Ã–ğ‘—
âˆˆ

ğ”¼ ğ‘¢

ğ‘–

degğºğ›¼ (
ğ‘–

)

.

Â·

ğ¼ğ›¼
Ã–ğ‘–
âˆˆ

(cid:3)

ğ”¼
ğ‘Œ
ğœ‡

âˆ¼

ğ‘Œ
ğ»ğ›¼(

)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

=

ğ›½|

ğ›¼

2
|/
ğ›¼!

= 1
ğ›¼!

ğ›½
ğ‘˜

(cid:18)

ğ”¼ ğ‘¢

ğ‘–

degğºğ›¼ (
ğ‘–

)

ğ¼ğ›¼
Ã–ğ‘–
âˆˆ
2
ğ›¼
|/
|

(cid:19)

ğ½ğ›¼

|

|

ğ‘˜
ğ‘‘

(cid:19)

(cid:18)

46

ğ›¼ğ‘–ğ‘—

ğ¼ğ›¼, ğ‘—

ğ‘–

âˆˆ

.

ğ”¼ ğ‘£

ğ‘—
Ã

Âª
Â®
Â¬

 
 
Lemma 6.14. Let ğºğ›¼ =
Then

ğ¼ğ›¼ , ğ½ğ›¼, ğ¸ğ›¼)

(

be a bipartite multigraph and let ğ‘¢ be the same as in the Problem 6.3.

ğ”¼ ğ‘¢

ğ‘–

degğºğ›¼ (
ğ‘–

)

6 2|

ğ›¼

|

ğ‘…|

ğ›¼

2
|âˆ’

|

ğ¼ğ›¼

| .

Â·

ğ¼ğ›¼
Ã–ğ‘–
âˆˆ

Proof. If ğ‘… = 1 the statement is true. Assume that ğ‘… > 1. Letâ€™s denote degğºğ›¼ (

ğ‘–

)

= ğ‘¥ğ‘–. Notice that

ğ”¼ ğ‘¢ ğ‘¥ğ‘–
ğ‘–

6

min

ğ¼ğ›¼
Ã–ğ‘–
âˆˆ
ğ¼ğ›¼ , ğ‘¥ğ‘– > 2ğ‘…2
âˆˆ
ğ‘¥ğ‘–/
2

Ã–ğ‘–
ğ¼ğ›¼
âˆˆ
and ğ‘‹â€²â€² =

ğ‘…ğ‘¥ğ‘– ,

{

ğ‘¥ğ‘– /
2

.

}

ğ‘¥ğ‘–
2

(cid:16)

(cid:17)

ğ‘–

|

âˆˆ

{

ğ¼ğ›¼ min

ğ‘¥ğ‘–
{
ğ‘…ğ‘¥ğ‘– ,

{
is maximal (for ï¬xed

Letâ€™s denote ğ‘‹â€² =
ğ‘–
ğ‘¥ğ‘–
value
2
to 2, or ğ‘‹â€² is empty and there can be only one ğ‘¥ğ‘–
(cid:0)
(cid:1)
for some ğ‘¥ğ‘
âˆˆ
âˆˆ
ğ‘¥ğ‘– /
2
ğ‘…ğ‘¥ğ‘– ,
ğ¼ğ›¼ min
}
âˆˆ
If ğ‘‹â€² is empty, then

. Letâ€™s show that if the
ğ‘‹â€²â€² are equal
|
ğ‘‹â€²â€² that is greater than 2. Indeed, if ğ‘¥ğ‘ > ğ‘¥ğ‘ > 2
ğ‘‹â€²â€², we can increase ğ‘¥ğ‘ by 2 and decrease ğ‘¥ğ‘ by 2. This operation increases
.

), then either all ğ‘¥ğ‘–

ğ¼ğ›¼ , 2 6 ğ‘¥ğ‘– < 2ğ‘…2

ğ¼ğ›¼ , ğ‘¥ğ‘
ğ‘¥ğ‘–
2

ğ‘¥ğ‘–
|
ğ›¼

âˆˆ
and

ğ¼ğ›¼ |

Ã

âˆˆ

âˆˆ

}

}

{

}

ğ‘–

|

|

ğ‘–

Ã

(cid:0)

(cid:1)

min

ğ‘…ğ‘¥ğ‘– ,

{

ğ‘¥ğ‘–
2

ğ¼ğ›¼
Ã–ğ‘–
âˆˆ

where ğ‘  = max

(cid:16)
ğ‘‹â€²â€²}
.
{
Now assume that all ğ‘¥ğ‘–

ğ‘¥ğ‘–

âˆˆ

(cid:17)

ğ‘ 

2
/

6

ğ‘¥ğ‘– /
2

6

}

ğ‘ 
2

(cid:17)

(cid:16)

ğ‘ 
2

(cid:16)

(cid:17)

ğ›¼

2
âˆ’|
|/

|

ğ¼ğ›¼

1
|+

6 |

ğ›¼
2

|

ğ‘…|

ğ›¼

2
|âˆ’

|

ğ¼ğ›¼

| 6 2|

ğ›¼

|

ğ‘…|

ğ›¼

2
|âˆ’

|

ğ¼ğ›¼

| ,

Â·

ğ‘‹â€²â€² are equal to 2 and ğ‘‹â€² is nonempty. In this case 2ğ‘…2 6

âˆˆ

, so

ğ›¼

|

|

ğ‘…ğ‘¥ğ‘– ,

min

{

ğ‘¥ğ‘–/
2

}

ğ‘¥ğ‘–
2

(cid:16)

(cid:17)

ğ¼ğ›¼
Ã–ğ‘–
âˆˆ

6 ğ‘…|

ğ›¼

2
(|
|âˆ’

ğ‘‹â€²â€²|âˆ’
1
) 6 |

ğ›¼
|
2 Â·

ğ‘…|

ğ›¼

2
|âˆ’

|

ğ‘‹â€²â€²| .

Notice that

Hence

ğ›¼

|

|

> 2ğ‘…2

ğ¼ğ›¼ | âˆ’ |

(|

ğ‘‹â€²â€²

|) + |

ğ‘‹â€²â€²

|

= 2

ğ‘‹â€²â€²

|

|

1

âˆ’

ğ‘…2

2ğ‘…2

,

ğ¼ğ›¼ |

|

1

âˆ’

ğ‘…2 +

2

ğ¼ğ›¼ |

|

1

(cid:0)
=

1

(cid:18)

ğ‘…2

ğ‘…2

âˆ’

+

ğ‘…2

(|

1

(cid:19)

âˆ’

ğ›¼

2

ğ¼ğ›¼ |)

|

.

| âˆ’

+

1

(cid:1)

ğ›¼

|

| âˆ’

2

|

ğ‘‹â€²â€²

|

6

ğ›¼

|

| âˆ’ |

ğ›¼

|

1

It follows that

ğ”¼ ğ‘¢ ğ‘¥ğ‘–
ğ‘–

6 |

ğ›¼
|
2 Â·

ğ‘…

(cid:16)

1
+

1
ğ‘…2

âˆ’

1

(cid:17)

ğ›¼

2
|âˆ’

|

(|

ğ¼ğ›¼

|) 6 |

ğ›¼
|
2 Â·

ğ‘’(|

ğ›¼

2
|âˆ’

|

ğ¼ğ›¼

2
|)/

Â·

ğ‘…|

ğ›¼

2
|âˆ’

|

ğ¼ğ›¼

| 6 2|

ğ›¼

|

ğ‘…|

ğ›¼

2
|âˆ’

|

ğ¼ğ›¼

| ,

Â·

ğ¼ğ›¼
Ã–ğ‘–
âˆˆ

where we used the fact that for ğ‘… > 1,

ln ğ‘…
ğ‘…2

1 < 1
2 .

Lemma 6.15. Let
= ğµ,
ğ¼ğ›¼ |

= ğ´,

ğ½ğ›¼|

|

|

ğ¸
ğ’¢(
)
ğ¸ğ›¼ |
|

âˆ’
be a set of all bipartate multigraphs ğºğ›¼ =
= ğ¸ and each vertex of ğºğ›¼ has even nonzero degree. Then

ğ¼ğ›¼ , ğ½ğ›¼, ğ¸ğ›¼)

(

such that ğ¼ğ›¼ âŠ† [

(cid:3)

ğ‘›

, ğ½ğ›¼ âŠ† [

]

ğ‘‘

]

,

ğ”¼
ğ‘Œ
ğœ‡

âˆ¼

ğ‘Œ
ğ»ğ›¼(

)

(cid:19)

2

6

ğ‘›
ğ´ğ‘…4

ğ´

Â·

(cid:17)

(cid:16)

Ã•ğ›¼ : ğºğ›¼
âˆˆğ’¢(

ğ´,ğµ,ğ¸

(cid:18)

)

ğ´ğ‘…2ğ›½

60

Â·

ğ‘‘, ğ‘˜

Â·

ln

ğ‘‘

(

Â·

ğ¸

/

Â· (|

ğ‘˜2

)| +

1
)

ğ¸

.

o

Âª
Â®
Â®
Â¬

min

âˆšğ¸

n

Â©

Â«

47

 
 
Proof. We can assume that ğ´ 6 ğ¸
choose ğ´ vertices from
6.13 and Lemma 6.14

ğ‘›

]

[

2 and ğµ 6 ğ¸
ğ‘‘

/

and ğµ vertices from

[

2 (otherwise
/
. Then we choose ğ›¼ğ‘–ğ‘— so that
]

ğ´, ğµ, ğ¸

ğ’¢(

)

is empty). We have to
= ğ¸. By Lemma

ğ›¼

|

|

ğ”¼
ğ‘Œ
ğœ‡

âˆ¼

ğ‘Œ
ğ»ğ›¼(

)

(cid:19)

2

6

ğ‘›
ğ´

(cid:18)

(cid:19) (cid:18)

ğ‘›
ğµ

Ã•ğ›¼ : ğºğ›¼
âˆˆğ’¢(

ğ´,ğµ,ğ¸

)(cid:18)

1
ğ›¼!

2

(cid:19)

(cid:18)

ğ¸

ğ›½
ğ‘˜

(cid:19)

ğ‘˜
ğ‘‘

(cid:18)
ğ¸

2ğµ

22ğ¸

Â·

ğ‘…2ğ¸

4ğ´

âˆ’

2ğµ

Â·

ğ‘…2ğ¸

4ğ´ .
âˆ’

(cid:19)

ğ‘˜
ğ‘‘

(cid:19)

1
ğ›¼!

ğ›½
ğ‘˜

(cid:18)

(cid:19)

(cid:18)

(cid:19) Ã•|
=ğ¸(cid:18)
ğ›¼
|
ğ‘›
ğµ

ğ‘›
ğ´

(cid:19) (cid:18)

(cid:19) Ã•|
=ğ¸
ğ›¼
|

6 4ğ¸

Â·

(cid:18)

By the multinomial theorem,

Therefore,

1
ğ›¼!

ğ´

= (

ğ¸
)

ğµ
Â·
ğ¸!

.

=ğ¸
ğ›¼
Ã•|
|

ğ”¼
ğ‘Œ
ğœ‡

âˆ¼

ğ‘Œ
ğ»ğ›¼(

)

(cid:19)

2

6 4ğ¸

ğ‘’

ğ‘›

Â·
ğ´

ğ´

ğ‘’

(cid:18)

(cid:17)

ğ‘‘

Â·
ğµ

(cid:19)

Â·

(cid:16)

ğµ

ğ‘’

(

Â·

Ã•ğ›¼ : ğºğ›¼
âˆˆğ’¢(

ğ´,ğµ,ğ¸

)(cid:18)

6 4ğ¸ ğ‘’ ğ´

+

ğµ

ğ¸

+

ğµğ¸

Â·

ğ¸ğ¸

ğµğµ Â·

Â·

6 30ğ¸

6 60ğ¸

ğ‘›
ğ´ğ‘…4

ğ‘›
ğ´ğ‘…4

ğ´

ğ´

(cid:17)

(cid:17)

Â·

Â·

(cid:16)

(cid:16)

ğµ
ğ¸ Â·

ğµ
ğ¸ Â·

Â·  

Â·  

ğ‘¥ 6 2.

where we used the inequality ğ‘¥1
/
ğ¸, then

If ğ‘˜2 > ğ‘‘

Â·

ğ´
Â·
ğ¸ğ¸

ğµ

(cid:19)
ğµ

(cid:18)
ğ¸
/

ğµ

ğ¸
)

ğ›½
ğ‘˜

(cid:18)

ğ¸

Â·

(cid:19)
ğ‘˜2
ğ¸
ğ‘‘

Â·

ğ‘˜2
ğ‘‘

(cid:18)

(cid:18)

ğ¸
ğµ

(cid:18)
ğµ

ğ¸
/

(cid:19)
ğ‘˜2
ğ¸

ğ‘‘

(cid:18)

Â·

(cid:19)

ğ›½
ğ‘˜

ğ¸

(cid:19)

(cid:18)

2ğµ

ğ‘˜
ğ‘‘

(cid:19)

ğ‘…2ğ¸

4ğ´

âˆ’

Â·

ğ‘…2ğ¸

4ğ´

âˆ’

ğµ

ğ¸
/

Â·

(cid:19)

Â·

(cid:16)
ğ´ğ‘…2

ğ‘›
ğ´

ğ´

ğ´ğ¸

ğ¸

(cid:17)
ğ›½
ğ‘˜ !

Â·

ğ´ğ‘…2

Â·

ğ¸

,

ğ›½
ğ‘˜ !

Â·

ğµ

ğ¸
/

ğµ
ğ¸ Â·

ğ‘˜2
ğ¸

Â·

(cid:19)

ğ‘‘

(cid:18)

ğ´ğ‘…2

Â·

ğ›½
ğ‘˜

6 1
2

Â·

2
1
/

ğ‘˜2
ğ¸

Â·

(cid:19)

ğ‘‘

(cid:18)

ğ´ğ‘…2

Â·

ğ›½
ğ‘˜

Â·

6

ğ´ğ‘…2ğ›½
âˆšğ¸

ğ‘‘

Â·

,

and if ğ‘˜2 < ğ‘‘

ğ¸, then

Â·

ğµ
ğ¸ Â·

ğ‘˜2
ğ¸
ğ‘‘

Â·

(cid:18)

(cid:19)

ğµ

ğ¸
/

ğ´ğ‘…2

Â·

ğ›½
ğ‘˜

Â·

6 min

1

ğ‘’ ln

n

ğ¸
ğ‘‘
Â·
ğ‘˜2

ğ´ğ‘…2

ğ›½
ğ‘˜

Â·

6

,

1
2

Â·

o

since ğ‘¥ğ‘ğ‘¥ 6

ğ‘’ ln

1
1
/

(

ğ‘

)

(cid:17)
for all ğ‘¥ > 0 and 0 < ğ‘ < 1. Hence

(cid:16)

ğ´ğ‘…2ğ›½

ğ‘˜

Â·

ln

ğ¸
ğ‘‘
Â·
ğ‘˜2

(cid:16)

(cid:16)

(cid:17)

1

+

(cid:17)

ğ”¼
ğ‘Œ
ğœ‡

âˆ¼

ğ‘Œ
ğ»ğ›¼(

)

(cid:19)

2

6

ğ‘›
ğ´ğ‘…4

ğ´

Â·

(cid:17)

(cid:16)

Ã•ğ›¼ : ğºğ›¼
âˆˆğ’¢(

ğ´,ğµ,ğ¸

)(cid:18)

ğ´ğ‘…2ğ›½

60

Â·

ğ‘‘, ğ‘˜

Â·

ln

ğ‘‘

(

Â·

ğ¸

/

Â· (|

ğ‘˜2

)| +

1
)

min

âˆšğ¸

n

Â©

Â«

48

o

Âª
Â®
Â®
Â¬

(cid:3)

,

ğ¸

.

 
 
Proof of Theorem 6.4. If ğ´ 6 ğ¸

2 6 ğ‘›

ğ‘…4 , the function

/

is a monotone in ğ´. Hence

Therefore, by Lemma 6.15,

ğ‘›
ğ´ğ‘…4

(cid:16)

ğ´

ğ´ğ¸

Â·

ğ‘›
ğ´ğ‘…4

(cid:17)

(cid:16)
ğ´

ğ´ğ‘…2

ğ¸ 6

ğ‘›ğ¸

(

2 .
/

ğ¸
)

(cid:1)

Â·

(cid:0)

(cid:17)

2

=

ğ”¼
ğ‘Œ
ğœ‡

âˆ¼

ğ‘Œ
ğ»ğ›¼(

)

(cid:19)

Ã•0<
ğ›¼
|
|

6ğ· (cid:18)

Ã•26ğ¸6ğ· Ã•16ğ´6ğ¸
16ğµ6ğ¸

2 Ã•ğ›¼ : ğºğ›¼

âˆˆğ’¢(

ğ´,ğµ,ğ¸

2
/
/

ğ‘Œ
ğ»ğ›¼(

)

ğ”¼
ğ‘Œ
ğœ‡

âˆ¼

)(cid:18)

2

(cid:19)

Ã•26ğ¸6ğ· Ã•16ğ´6ğ¸
16ğµ6ğ¸

2
2 Â©
/
/

min

âˆšğ¸

n

60

Â·
ğ‘‘, ğ‘˜

Â·

ğ¸2
4 Â·

Ã•26ğ¸6ğ·

Â«

min

Â©

ğ‘›,

ğ‘‘

/

np

6

6

6

Â·
ğ‘‘

âˆšğ‘›âˆšğ¸

ln

(

Â· (|

60ğ›½

ğ›½

ğ‘˜2

ğ¸

/

Â·

)| +

1
)

ğ¸

o

Âª
Â®
Â®
Â¬

ğ¸

ln

ğ¸

ğ‘‘

(

Â·

/

Â· (|

ğ‘˜2

)| +

âˆšğ¸ğ‘›

ğ‘˜

/

(cid:16)

(cid:17)

120ğ›½

1
)

ğ¸

o

.

Âª
Â®
Â®
Â¬

Ã•26ğ¸6ğ·

Â©

Â«
min

np

ğ‘›,

ğ‘‘

/

âˆšğ¸ğ‘›

ğ‘˜

/

(cid:17)

(cid:16)

ln

ğ¸

ğ‘‘

(

Â·

/

Â· (|

ğ‘˜2

)| +

1
)

If

where 0 < ğœ€ < 1

1000 , then

ğ›½ = ğœ€

min

Â·

ğ‘‘
ğ‘›

,

ğ‘˜
âˆšğ·ğ‘› Â·

Â«

nr

ğ”¼
ğ‘Œ
ğœ‡

âˆ¼

ğ‘Œ
ğ»ğ›¼(

)

(cid:19)

Ã•0<
ğ›¼
|
|

6ğ· (cid:18)

ln

ğ·

Â·
ğ‘˜2

(cid:18)

(cid:18) (cid:12)
(cid:12)
(cid:12)
(cid:12)
6 10002

2

1

,

(cid:19) o

ğ‘‘

Â·

+

(cid:19) (cid:12)
(cid:12)
(cid:12)
(cid:12)
ğœ€2 ,

and using Corollary 6.9 we get the desired conclusion.

Proof of Theorem 6.5. If ğ¸
that ğ¸

ğ‘…4. Thus

2 > ğ‘›

/

/

By Lemma 6.15,

2 6 ğ‘›

/

/

ğ‘…4, we get the same bound as in the theorem 6.4. So we can assume

ğ‘›
ğ´ğ‘…4

ğ´

(cid:17)

(cid:16)

6 ğ‘’ ğ‘›

/

ğ‘…4 6 ğ‘’ğ¸

2 .
/

o

Âª
Â®
Â®
Â¬

(cid:3)

2

=

ğ”¼
ğ‘Œ
ğœ‡

âˆ¼

ğ‘Œ
ğ»ğ›¼(

)

(cid:19)

Ã•0<
ğ›¼
|
|

6ğ· (cid:18)

Ã•26ğ¸6ğ· Ã•16ğ´6ğ¸
16ğµ6ğ¸

2
2 Â©
/
/

min

âˆšğ¸

n

60âˆšğ‘’

ğ´ğ‘…2

Â·

ğ‘‘, ğ‘˜

ln

ğ‘‘

(

Â· (|

Â·

Â·

Â·

ğ›½

ğ‘˜2

ğ¸

/

)| +

1
)

100ğ‘…2ğ›½

ğ¸

o

Âª
Â®
Â®
Â¬

ğ¸

ğ´,

ğ‘‘

/

Â·

ğ´

ğ‘˜

(

/

) Â· (|

ln

ğ‘‘

(

Â·

ğ´

/

ğ‘˜2

)| +

1
)

o

Âª
Â®
Â®
Â¬

ğ¸2
4 Â·

6

Ã•26ğ¸6ğ·

Â«

min

âˆšğ¸

n
49

Â©

Â«

 
 
 
 
 
 
 
 
 
 
If

where 0 < ğœ€ < 1

1000 , then

6

Ã•26ğ¸6ğ·

Â©

min

np

200ğ‘…2ğ›½

ğ‘›,

ğ‘‘

/

ğ‘›

ğ‘˜

(

/

) Â· (|

ln

ğ‘‘

(

Â·

ğ‘›

/

ğ‘˜2

)| +

1
)

ğ›½ = ğœ€

ğ‘…2 Â·

Â«
min

ğ‘‘
ğ‘›

,

ğ‘˜
ğ‘› Â·

nr

ğ”¼
ğ‘Œ
ğœ‡

âˆ¼

ğ‘Œ
ğ»ğ›¼(

)

(cid:19)

Ã•0<
ğ›¼
|
|

6ğ· (cid:18)

ln

ğ‘›
ğ‘‘
Â·
ğ‘˜2

(cid:18)

(cid:18) (cid:12)
(cid:12)
(cid:12)
(cid:12)
6 10002

2

+

(cid:19) (cid:12)
(cid:12)
(cid:12)
(cid:12)
ğœ€2 ,

Â·

1

,

(cid:19) o

and using Corollary 6.9 we get the desired conclusion.

ğ¸

.

o

Âª
Â®
Â®
Â¬

(cid:3)

6.6 Almost Gaussian vector in random subspace (proof)

In this section we focus on Problem 6.6 and prove that if ğ‘‘ is signiï¬cantly less than ğ‘› ğ‘ 
degree multilinear polynomial can distinguish between the planted and the null distribution.

1, no low
2
+
/

The proof of Theorem 6.7 relies on key lemmata which we provide below. The proof itself is

then presented at the end of the section.

Lemma 6.16. Let ğ›¼ be a multiindex over

ğ‘›

[

ğ‘‘

]

ğ‘Œ
such that ğ»ğ›¼(

] Ã— [

) âˆˆ â„‹â„³

6ğ· . Then

ğ”¼
ğœ‡

ğ‘Œ
ğ»ğ›¼(

)

= ğ”¼

ğ½ğ›¼,ğ‘– |

ğ”¼

ğœ|
ğ‘–

(cid:18)

ğ¼ğ›¼
ğ‘–
ï£®
âˆˆ
ï£¯
Ã
ï£¯
ï£¯
, ğœğ‘– := âˆšğ‘›ğ‘¢ğ‘– and ğ‘§ğ‘–ğ‘— := ğœğ‘–ğ‘¤ğ‘–ğ‘—.
ï£¯
ï£°

ğ½ğ›¼
ğ‘—
âˆˆ
Ã

(cid:19)

where ğ‘—

, ğ‘–

ğ‘‘

]

ğ‘›

]

âˆˆ [

âˆˆ [

ğ‘§ğ‘–ğ‘—

1
âˆšğ‘› (

ğ‘£ ğ‘—

âˆ’

1
âˆšğ‘›

+

ğ‘§ğ‘™ ğ‘—

ğ‘›
Ã•ğ‘™
]
âˆˆ[

,

)
ï£¹
ï£º
Âª
ï£º
Â®
ï£º
ï£º
Â¬
ï£»

ğ‘–

ğ¼ğ›¼, ğ‘—
âˆˆ
Ã

Â©

Â«

Proof. We drop the subscript ğ›¼ for the exposition of the proof.

ğ‘§ğ‘™ ğ‘—

ğ‘›
Ã•ğ‘™
]
âˆˆ[

ğ‘§ğ‘™ ğ‘—

ï£¹
ï£º
Âª
ï£º
Â®
ï£º
ï£º
Â¬
ï£»

(as ğœğ‘–ğ‘¤ğ‘–ğ‘— = ğ‘¤ğ‘–ğ‘—
ğœğ‘–

)

ğ”¼
ğœ‡

ğ‘Œ
ğ»ğ›¼(

)

= ğ”¼
ğœ‡

= ğ”¼

= ğ”¼

ğ‘¤ğ‘–ğ‘—

ğ‘¦ğ‘–,ğ‘—

ğ½
ğ‘—
âˆˆ
Ã

ğ¼ğ‘—
ğ‘–
âˆˆ
Ã

ğ½
ğ‘—
âˆˆ
Ã

ğ¼
ğ‘–
âˆˆ
Ã

(cid:2)

ğ½
ğ‘—
âˆˆ
Ã

ğ½
ğ‘—
âˆˆ
Ã

ğ¼ğ‘— ï£®
ğ‘–
âˆˆ
ï£¯
Ã
ï£¯
ï£¯
ï£¯
ğœğ‘–
ï£°

ğ¼ğ‘—
ğ‘–
âˆˆ
Ã

ğ‘¤ğ‘–ğ‘—

+

ğ‘¢ğ‘–

ğ‘£ ğ‘—

âˆ’ h

ğ‘¢, ğ‘¤ ğ‘—

i

(cid:1)(cid:3)

(cid:0)

ğœğ‘–

+

1
âˆšğ‘›

ğ‘£ ğ‘—

1
ğ‘›

âˆ’

Â©

ğ‘§ğ‘–ğ‘—

1
Â«
âˆšğ‘›

ğ‘£ ğ‘—

1
ğ‘›

âˆ’

+

= ğ”¼

= ğ”¼

= ğ”¼

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

Â©

ğœğ‘–
Â«

!

ğ½
ğ‘—
âˆˆ
Ã

ğ¼ğ‘—
ğ‘–
âˆˆ
Ã

Â©

ğ½
ğ‘—
âˆˆ
Ã

ğ¼ğ‘—
ğ‘–
âˆˆ
Ã

ğ‘›
Ã•ğ‘™
]
âˆˆ[

ğ‘§ğ‘–ğ‘—

+

1
âˆšğ‘›

ğ‘£ ğ‘—

âˆ’

Âª
Â®
1
Â¬
ğ‘›

ğ‘›
Ã•ğ‘™
]
âˆˆ[

ğ‘§ğ‘–ğ‘—

1
âˆšğ‘› (

ğ‘£ ğ‘—

âˆ’

1
âˆšğ‘›

+

ğ”¼

ğœğ‘–

!

ğ½
ğ‘—
âˆˆ
Ã

ğ¼ğ‘—
ğ‘–
âˆˆ
Ã

Â«
ğ¼ğ‘—
ğ‘–
âˆˆ
Ã

ğ½
ğ‘—
âˆˆ
Ã

50

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

Â©

Â«

ğ‘§ğ‘™ ğ‘—

ï£¹
ï£º
Âª
ï£º
Â®
ï£º
ï£º
ğ‘§ğ‘™ ğ‘—
Â¬
ï£»
ğ‘›
Ã•ğ‘™
]
âˆˆ[

)
ï£¹
ï£º
Âª
ï£º
Â®
ï£º
ï£º
Â¬
ï£»

 
 
 
 
 
 
 
 
 
= ğ”¼

ğ½ğ‘– |

ğœ|
ğ‘–

ğ”¼

ğ‘§ğ‘–ğ‘—

1
âˆšğ‘› (

ğ‘£ ğ‘—

1
âˆšğ‘›

ğ‘§ğ‘™ ğ‘—

)
Âª
Â®
Â¬
An immediate consequence of Lemma 6.16 is the following statement:

ğ¼ğ‘—
ğ‘–
âˆˆ
Ã

ğ‘›
Ã•ğ‘™
]
âˆˆ[

ğ¼
ğ‘–
âˆˆ
Ã

ğ½
ğ‘—
âˆˆ
Ã

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

+

âˆ’

Â«

Â©

(cid:18)

(cid:19)

.

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

(cid:3)

Corollary 6.17. Let ğ›¼ be a multiindex over
ğ‘–

(respectively,

ğ¼ğ›¼) such that

ğ¼ğ›¼,ğ‘—

ğ½ğ›¼,ğ‘–

[

ğ‘›

ğ‘Œ
such that ğ»ğ›¼(
ğ‘‘
] Ã— [
= 0.
) is odd, then ğ”¼ğœ‡ ğ»ğ›¼(
ğ‘Œ
)

]

|

|

|

|

âˆˆ

) âˆˆ â„‹

6ğ·. If there exists ğ‘—

ğ½ğ›¼ (or

âˆˆ

In the following lemma we use the fact that ï¬rst ğ‘  moments of coordinates of ğ‘£ coincide with

Gaussian moments.

Lemma 6.18. Let ğ‘  be the parameter of the planted distribution, let ğ›¼ be a multiindex over
that there exists ğ‘—0 âˆˆ

6 ğ‘ . Then ğ”¼ğœ‡ ğ»ğ›¼(
ğ‘Œ
Proof. For simplicity we will the subscript ğ›¼. If ğ”¼

ğ½ğ›¼ such that

ğ¼ğ‘—0 |

= 0.

)

|

ğ½ğ‘– |

ğœ|
ğ‘–

= 0, the statement is obviously true.

ğ‘›

[

ğ‘‘

]

]Ã—[

. Suppose

(cid:18)
= 1 (notice that this expectation can be only 0 or 1). Thus

(cid:19)

ğ¼
ğ‘–
âˆˆ
Ã

Assume that ğ”¼

ğ½ğ‘– |

ğœ|
ğ‘–

(cid:18)

ğ¼
ğ‘–
âˆˆ
Ã

ğ”¼
ğœ‡

ğ‘Œ
ğ»ğ›¼(

)

=

ğ½
Ã–ğ‘—
âˆˆ

(cid:19)

ğ”¼

ğ¼ğ‘—
ğ‘–
âˆˆ
Ã

Â©
ğ‘§ğ‘–ğ‘—0 +
Â«

= ğ”¼

ğ‘–

ğ¼ğ‘—0
âˆˆ
Ã

Â©

Â«

ğ‘§ğ‘–ğ‘—

1
âˆšğ‘› (

ğ‘£ ğ‘—

âˆ’

1
âˆšğ‘›

+

1
âˆšğ‘› (

ğ‘£ ğ‘—0 âˆ’

1
âˆšğ‘›

ğ‘›
Ã•ğ‘™
]
âˆˆ[

ğ‘§ğ‘™ ğ‘—

ğ‘›
Ã•ğ‘™
]
âˆˆ[

)
Âª
Â®
Â¬
Â·

ğ‘§ğ‘™ ğ‘—0)
Âª
Â®
Â¬

ğ”¼

ğ¼ğ‘—
ğ‘–
âˆˆ
Ã

ğ‘—0
ğ½
Ã–ğ‘—
\{
âˆˆ

}

ğ‘§ğ‘–ğ‘—

1
âˆšğ‘› (

ğ‘£ ğ‘—

âˆ’

1
âˆšğ‘›

+

Â©

Â«

ğ‘§ğ‘™ ğ‘—

ğ‘›
Ã•ğ‘™
]
âˆˆ[

.

)
Âª
Â®
Â¬

Since ï¬rst ğ‘  moments of ğ‘£ ğ‘—0 coincide with Gaussian moments,

ğ”¼

ğ‘–

ğ¼ğ‘—0
âˆˆ
Ã

Â©

ğ‘§ğ‘–ğ‘—0 +

1
âˆšğ‘› (

ğ‘£ ğ‘—0 âˆ’

1
âˆšğ‘›

ğ‘›
Ã•ğ‘™
]
âˆˆ[

where ğœ is a standard Gaussian variable that
1
ğ‘§ğ‘–ğ‘—0 +
âˆšğ‘› (
ğœ, ğ‘§1ğ‘—0 , . . . , ğ‘§ğ‘› ğ‘—0:

. Letâ€™s show that ğœ‰

Â«
ğ‘§ğ‘™ ğ‘—0)

ğ‘›
âˆˆ[
Ã

1
âˆšğ‘›

ğ‘

âˆ’

ğœ

]

ğ‘™

ğ‘§ğ‘™ ğ‘—0)
Âª
Â®
Â¬
âˆ¼

= ğ”¼

ğ‘–

ğ‘§ğ‘–ğ‘—0 +

1
âˆšğ‘› (

ğ¼ğ‘—0
âˆˆ
Ã
is independent

Â©

1
âˆšğ‘›

,

ğœ

âˆ’

ğ‘›
Ã•ğ‘™
]
âˆˆ[

ğ‘§ğ‘™ ğ‘—0)
Âª
Â®
from all ğ‘§ğ‘–ğ‘—0 . Let ğœ‰ğ‘– =
Â¬
â„ğ‘› is a linear transformation of

Â«
0, Idğ‘›
. ğœ‰
)
(

âˆˆ

ğœ‰ = ğ´

Â©

ğœ
ğ‘§1ğ‘—0
...
ğ‘§ğ‘› ğ‘—0

,

, 1
ğ‘› , . . . , 1
ğ‘› ,

1
(

âˆ’

1
ğ‘› )

, 1
ğ‘› , . . . , 1
ğ‘› )

. The rows of ğ´

Âª
Â®
Â®
Â®
Â®
1
Â¬
âˆšğ‘›

where ğ´ is an ğ‘›

ğ‘›

1
)

+

Ã— (

matrix with rows ğ´ğ‘–

T =
Â«

(

are orthonormal: for all ğ‘–

ğ‘›

]

âˆˆ [
1
1
ğ‘› + (

T
ğ´ğ´

(

)

ğ‘–ğ‘– =

ğ‘–

}

1
+
{z
1
ğ‘› âˆ’

|
1
ğ‘› +

1
ğ‘›2

= 1 ,

1
2
ğ‘› )

+

ğ‘›

âˆ’
ğ‘›2

1

âˆ’

= 1

2
ğ‘› +

1
ğ‘›2 +

âˆ’

51

 
 
 
 
 
 
 
 
 
 
  
  
and for all diï¬€erent ğ‘–, ğ‘™

ğ‘›

âˆˆ [
]
ğ‘–ğ‘™ = 1

T
ğ´ğ´

)
Hence ğ´ğ´T = Idğ‘› and ğœ‰

(

2
1
ğ‘› (

âˆ’

1
ğ‘› ) +

ğ‘›

âˆ’
ğ‘›2

2

= 1

ğ‘› âˆ’

2
ğ‘› +

2
ğ‘›2 +

1
ğ‘› âˆ’

2
ğ‘›2

= 0 .

ğ‘› âˆ’

ğ‘

0, Idğ‘›
(

. Therefore,
)

âˆ¼

ğ”¼

ğ‘–

ğ¼ğ‘—0
âˆˆ
Ã

Â©

ğ‘§ğ‘–ğ‘—0 +

1
âˆšğ‘› (

ğ‘£ ğ‘—0 âˆ’

1
âˆšğ‘›

ğ‘›
Ã•ğ‘™
]
âˆˆ[

ğœ‰ğ‘– = 0 .

= ğ”¼

ğ‘–

ğ¼ğ‘—0
âˆˆ
Ã

ğ‘§ğ‘™ ğ‘—0)
Âª
Â®
Â¬

(cid:3)

, ğ¼ğ‘—

ğ‘‘

]

âˆˆ [

ğ‘›

]

âŠ† [

with even

Lemma 6.19. Let ğ‘ , ğ›¿, ğœ† be the same as in the statement of Theorem 6.7. Let ğ‘—
cardinality

> ğ‘ . Then, if

ğ¼ğ‘—

|

|

Â«

6 ğœ†2
100 ,

ğ¼ğ‘—

|

|

and if

ğ¼ğ‘—

|

|

> ğœ†2
100 ,

ğ¼ğ‘—
ğ‘–
âˆˆ
Ã

Â©

Â«

ğ”¼

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

ğ”¼

ğ‘§ğ‘–ğ‘—

1
âˆšğ‘› (

ğ‘£ ğ‘—

âˆ’

1
âˆšğ‘›

+

ğ‘§ğ‘™

ğ‘›
Ã•ğ‘™
]
âˆˆ[

)
Âª
Â®
Â¬

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

ğ‘§ğ‘–ğ‘—

1
âˆšğ‘› (

ğ‘£ ğ‘—

âˆ’

+

1
âˆšğ‘›

ğ‘§ğ‘™

6

6 ğ›¿

220

ğ‘ 
Â·
âˆšğ‘›

(cid:18)

ğœ†

Â·

ğ¼ğ‘— |

|

,

(cid:19)

100

ğ¼ğ‘—

|

ğ¼ğ‘— |

|

.

!

|
âˆšğ‘›
p

ğ‘›
Ã•ğ‘™
]
âˆˆ[
Proof. We drop the subscript ğ‘— to simplify the notation (in particular, in this proof we denote ğ‘£ ğ‘— by
ğ‘£). By symmetry of the Gaussian distribution, opening up the product we see that in order for a
monomial to have non-zero expectation, for any left end term ğ‘§ğ‘– there must be a corresponding
right term 1

ğ‘£

Â©

Â«

ğ¼ğ‘—
ğ‘–
âˆˆ
Ã

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

1
âˆšğ‘›

âˆšğ‘› (

âˆ’

ğ‘§ğ‘™

. Hence:
)

)
ï£¹
ï£º
Âª
ï£º
Â®
ï£º
ï£º
Â¬
ï£»

ğ‘™

ğ‘›
âˆˆ[
Ã

]

ğ‘§ğ‘–

ğ‘£

1
âˆšğ‘› (

âˆ’

1
âˆšğ‘›

+

ğ‘§ğ‘™

ğ‘›
Ã•ğ‘™
]
âˆˆ[

ğ”¼

ğ¼
ğ‘–
âˆˆ
Ã

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

Â©

Â«

)
ï£¹
ï£º
Âª
ï£º
Â®
ï£º
ï£º
Â¬
ï£»

Since ğ‘£ is symmetric:

ğ¼

|

2ğ‘Ÿ

|âˆ’

ğ”¼

ğ‘§2
ğ‘–

ğ‘£

ğ‘–

!

ğ‘Ÿ
]
âˆˆ[
Ã

ï£®
ï£¯
Â©
ï£¯
ï£¯
ï£¯
By Cauchyâ€“Schwarz:
ï£¯
Â«
ï£°

1
âˆšğ‘›

âˆ’

ğ‘›
Ã•ğ‘™
]
âˆˆ[

ğ‘–

ğ‘Ÿ
âˆˆ[
Ã

]

ğ”¼

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

1
âˆšğ‘›

ğ‘§2
ğ‘–

!

Â©

Â«

ğ‘§ğ‘™

ğ‘›
Ã•ğ‘™
]
âˆˆ[

ğ‘§ğ‘™

Âª
Â®
Â¬

2ğ‘š

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

Âª
Â®
Â¬

ğ‘Ÿ

ğ¼

|

|âˆ’

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

.

.

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

Âª
Â®
Â¬

ğ‘§ğ‘™

2ğ‘š

Âª
Â®
Â¬

ğ¼

2
|/

|

=

Ã•ğ‘Ÿ=0 (cid:18)

ğ¼
|
|
2ğ‘Ÿ

2ğ‘Ÿ
ğ‘Ÿ

(cid:19)

(cid:19) (cid:18)

ğ”¼

=

1
ğ¼
ğ‘› |

2
|/

ğ¼

2
|/

|

Ã•ğ‘Ÿ=0 (cid:18)

ğ¼
|
|
2ğ‘Ÿ

(cid:19) (cid:18)

ğ‘–

ğ‘Ÿ
]
âˆˆ[
Ã

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
2ğ‘Ÿ
ï£°
ğ‘Ÿ

ğ‘§ğ‘–

ğ¼

ğ‘› |

!

1
2
âˆ’
|/

ğ‘Ÿ

2
/

ğ‘£

âˆ’

1
âˆšğ‘›

ğ‘§ğ‘™

ğ‘›
Ã•ğ‘™
]
âˆˆ[

ğ¼

|

ğ‘§ğ‘™

Âª
Â®
2ğ‘Ÿ
|âˆ’
Â¬

Â©

Â«
âˆ’

1
âˆšğ‘›

ğ‘›
Ã•ğ‘™
]
âˆˆ[

ğ”¼

(cid:19)

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

ğ‘–

ğ‘Ÿ
]
âˆˆ[
Ã

ğ‘£

ğ‘§2
ğ‘–

!

Â©

Â«

ğ¼

ğ‘£ |

2ğ‘Ÿ

|âˆ’

2ğ‘š

âˆ’

ğ”¼
[

ğ”¼

]

ğ¼

ğ‘Ÿ

2
âˆ’
|/

|

=

Ã•ğ‘š=0

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

ğ‘§2
ğ‘–

! Â·

1
âˆšğ‘›

Â©

Â«

ğ‘–

ğ‘Ÿ
]
âˆˆ[
Ã

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

4ğ‘š

2
1
/

ğ‘›
Ã•ğ‘™
]
âˆˆ[

6

ğ”¼

ğ‘§4
ğ‘–

!

ğ‘–

ğ‘Ÿ
âˆˆ[
Ã

]

2
1
/

ğ”¼

Â©

Â«

Â©

Â«

52

6 3ğ‘Ÿ

2
/

2ğ‘š

Â· (

ğ‘š .

)

1
âˆšğ‘›

ğ‘§ğ‘™

ğ‘›
Ã•ğ‘™
]
âˆˆ[

Âª
Â®
Â¬

Âª
Â®
Â®
Â¬

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Hence,

ğ‘–

ğ‘Ÿ
]
âˆˆ[
Ã

ğ”¼

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

ğ‘§2
ğ‘–

!

ğ‘£

âˆ’

1
âˆšğ‘›

Â©

Â«

ğ¼

|

2ğ‘Ÿ

|âˆ’

ğ‘§ğ‘™

ğ‘›
Ã•ğ‘™
]
âˆˆ[

Âª
Â®
Â¬

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

ğ¼

ğ‘Ÿ

2
âˆ’
|/

|

6

Ã•ğ‘š=0

ğ¼

ğ‘£ |

2ğ‘Ÿ

|âˆ’

2ğ‘˜

âˆ’

ğ”¼

[

3ğ‘Ÿ

2
/

2ğ‘š

Â· (

ğ‘š

)

] Â·

ğ¼

ğ‘Ÿ

2
âˆ’
|/

|

6 3ğ‘Ÿ

2
/

Ã•ğ‘š=0 (cid:16)

ğ¼

2
|/

|

6 3|

ğ¼

4
|/

ğ¼

ğ›¿ğœ†|

2ğ‘š

|âˆ’

Ã•ğ‘š=0(cid:16)
. Thus 2ğ‘€ >

ğ¼

ğ›¿ğœ†|

2ğ‘Ÿ

|âˆ’

2ğ‘š

âˆ’

10âˆšğ‘  ln ğ‘ 

+ (

2ğ‘Ÿ

|âˆ’

2ğ‘š

âˆ’

ğ¼

|

)

10âˆšğ‘  ln ğ‘ 

+ (

2ğ‘š

|âˆ’

ğ¼

|

)

(cid:17)

2ğ‘š

Â· (

2ğ‘š

ğ‘š

)

Â· (

ğ‘š .

(cid:17)

)

Let ğ‘€ = max

get:

ğ›¿ğœ†|

ğ¼

| ,

ğ¼

2,
|/

ğ¼

|

| |

10âˆšğ‘  ln ğ‘ 
(

)|

ğ¼

|}

{

ğ¼

ğ›¿ğœ†|

2ğ‘š

|âˆ’

10âˆšğ‘  ln ğ‘ 

+ (

2ğ‘š

|âˆ’

ğ¼

)|

(cid:17)

ğ‘š. We

2ğ‘š

)

Â· (

ğ‘§ğ‘–

ğ‘£

1
âˆšğ‘› (

âˆ’

1
âˆšğ‘›

+

ğ‘§ğ‘™

ğ‘›
Ã•ğ‘™
]
âˆˆ[

ğ”¼

ğ¼
ğ‘–
âˆˆ
Ã

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

Â©

Â«

)
Âª
Â®
Â¬

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

ğ¼
|
|
2ğ‘Ÿ

2ğ‘Ÿ
ğ‘Ÿ

(cid:19)

(cid:19) (cid:18)

3|

ğ¼

4
|/

ğ¼

Â· |

| Â·

2ğ‘€

2|

ğ¼

|

Â·

Â·

3|

ğ¼

4
|/

Â·

2|

ğ¼

2
|/

ğ‘€

Â·

(cid:16)

ğ¼

2
|/

|

Ã•ğ‘Ÿ=0 (cid:18)
2|

ğ¼

|

2
|/

ğ¼

6 1
ğ‘› |
6 1
ğ‘› |

ğ¼

6

2 Â·
ğ¼

|

|/
10
âˆšğ‘› (cid:19)

|

Â·

ğ‘€

Consider the case

ğ¼

|

|

(cid:18)
100 . In this case, ğ‘€ 6 10|

> ğœ†2

ğ¼

| Â· |

ğ¼

| |

ğ¼

2. Hence
|/

ğ”¼

ğ‘§ğ‘–

1
âˆšğ‘› (

ğ‘£

âˆ’

1
âˆšğ‘›

+

ğ¼
ğ‘–
ï£®
âˆˆ
ï£¯
Ã
Â©
ï£¯
ï£¯
6 ğœ†2
ğ¼
ï£¯
|
|
Â«
ï£°
1.8, and if
2
)

100 . If
ğ¼

|

ğ¼
|
>

|
|

ğ‘§ğ‘™

6

ğ‘›
Ã•ğ‘™
]
âˆˆ[

)
ï£¹
ï£º
Âª
ï£º
Â®
ï£º
> 10ğ‘ , then ğ›¿ğœ†|
ï£º
Â¬
ï£»
1.8, then
ğœ†
ğœ†
(
(
ğœ†2 6 0.1
/

2
)
/
100ğ‘ 

Now consider the case
ğœ†
(

inequality holds if

6

ğ¼

|

|

/

2
)
since ğœ†2 > 10000ğ‘  ln ğ‘ . If

ğœ†
(

/

ğ‘ 

/|

ğ¼

|

ğ¼

|

|

6 0.1

ğœ†

Â·

ğœ†

Â· (

2
)

/

p
< 10ğ‘ , then
|

ğ¼

|

ğ¼

|

| |

ğ¼

2 <
|/

100

ğ¼

|

|

.

ğ¼

|

|

!

âˆšğ‘›
p

ğ¼

2. Indeed, the
|/

ğ¼

| >
ğ‘ 
2
/|
)

/

ğ¼

ğœ†

ğœ†

Â· (

/

Â·

ğ¼

ğ‘  >

|

|

|âˆ’

ğœ†
(
|

ğ¼
2
| |
)
/
is monotone in ğ¼, so
ğ¼
|
|
ln ğœ† 6 1
p
1
2
/
2
)

ğœ† ,

ğ‘§ğ‘–

ğ‘£

1
âˆšğ‘› (

âˆ’

1
âˆšğ‘›

+

ğ‘§ğ‘™

ğ‘›
Ã•ğ‘™
]
âˆˆ[

ğ”¼

ğ¼
ğ‘–
âˆˆ
Ã

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

Â©

Â«

)
Âª
Â®
Â¬

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

10
âˆšğ‘› (cid:19)
10
âˆšğ‘› (cid:19)
10
âˆšğ‘› (cid:19)
220

6

6

6

(cid:18)

(cid:18)

(cid:18)

6 ğ›¿

|

|

ğ¼

|

|

ğ¼

|

|

10âˆšğ‘  ln ğ‘ 
(
ğ¼

|. Therefore,

ğ¼

)|

max

{

Â·

ğ›¿ğœ†|

ğ¼

| ,

10âˆšğ‘  ln ğ‘ 
(

)

ğ¼

|

|

}

ğ›¿ğœ†ğ‘ 

2 max
+

ğ¼

ğœ†|

ğ‘ 

2
),
+

|âˆ’(

{

1
ğ›¿ğœ†ğ‘ 

10âˆšğ‘  ln ğ‘ 

2 (
+

ğ¼

|

|

)

}

ğ›¿ğœ†ğ‘ 

2 max
+

ğœ†|

ğ¼

ğ‘ 

), 210ğ‘ 
2
+

|âˆ’(

10âˆšğ‘  ln ğ‘ 
(

)

ğ¼

|

|

}

{
ğ¼

|

!

Â·

âˆšğ‘  ln ğ‘ 
âˆšğ‘›

ğœ†

Â·

53

|

.

(cid:3)

 
 
 
 
 
 
 
We are now ready to prove Theorem 6.7.

Proof of Theorem 6.7. For all positive integers ğ´, ğµ, ğµâ€², ğ¸ and ğ¸â€² consider the set
= ğ´,
of bipartite graphs ğºğ›¼ such that
ğ½ğ›¼ | |

= ğ¸, ğµâ€² =

= ğµ and

ğ›¼

ğ‘—

|

|

|

ğ‘ 
ğ’¢
ğ¼ğ‘—

ğ´, ğµ, ğµâ€², ğ¸, ğ¸â€²)
(
6 ğœ†2
, ğ¸â€² is a
100 }

âˆˆ
, and all vertices of ğºğ›¼ have even degree strictly

{

|

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

number of edges adjacent to
greater than ğ‘ . Let ğµâ€²â€² = ğµ

ğ¼ğ›¼ |
ğ½ğ›¼ |
ğ¼ğ‘—
{
|
ğµâ€² and ğ¸â€²â€² = ğ¸

âˆˆ

ğ‘—

|

ğ½ğ›¼ |
|
6 ğœ†2
100 }
ğ¸â€².

âˆ’

âˆ’

By lemma 6.19,

ğ”¼
ğœ‡

ğ‘Œ
ğ»ğ›¼(

)

ğ‘Œ
Ã•ğ»ğ›¼
(

)âˆˆâ„‹â„³

6ğ· (cid:18)

2

6

(cid:19)

6ğ¸6ğ·

ğ‘ 
2
Ã•2
)
+
(

Ã•ğ´,ğµ,ğµâ€²,ğ¸â€²
ğ´,ğµ,ğµâ€²,ğ¸,ğ¸â€²)

â‰ 

âˆ…

ğ‘ 
ğ’¢

(

6

6ğ¸6ğ·

ğ‘ 
2
Ã•2
)
+
(

Ã•ğ´,ğµ,ğµâ€²,ğ¸â€²
ğ´,ğµ,ğµâ€²,ğ¸,ğ¸â€²)

â‰ 

âˆ…

ğ‘ 
ğ’¢

(

(cid:18)

Â·

(cid:16)

Â·

ğ¸
ğ´ğµ
)
ğ¸!

(

ğ‘›
ğ´

ğ‘‘
ğµ

(cid:19) (cid:18)

ğ›¿2ğµâ€²

(cid:19)
220

Â·

âˆšğ‘  ln ğ‘ 
âˆšğ‘›

ğœ†

Â·

!

2ğ¸â€²

2ğ¸â€²â€²

100âˆšğ¸
âˆšğ‘› (cid:19)

(cid:18)

ğ‘’ğ‘›
ğ´

ğ´

(cid:18)

(cid:17)

ğ‘’ğ´ğµ
ğ¸

ğ¸

(cid:19)
220

ğ‘‘ğµ

ğ›¿2

ğµâ€²

(cid:0)

(cid:1)

Â·

âˆšğ‘  ln ğ‘ 
âˆšğ‘›

ğœ†

Â·

!

2ğ¸â€²

2ğ¸â€²â€²

.

100âˆšğ¸
âˆšğ‘› (cid:19)

(cid:18)

ğ´

(

ğ‘ 

/

+

ğ‘’ğ‘›
ğ´

2
)

2 = ğ‘œ
ğ‘›
,
)
and ğµâ€²â€² 6 100

Since ğ´ 6 ğ·
is monotone in ğ´. Also notice that if
ğœ†2 6 ğ¸â€²â€²/(
ğ‘ 
ğ¸â€²/(
one if and only if there exists ğ´ such that

. Let ğœ™
2
)
(
ğ´, ğµâ€² +
(
Consider the case ğ›¿2ğ‘‘ > 1. Assume that ğ‘‘ = ğ‘œ

ğµâ€², ğµâ€²â€², ğ¸â€², ğ¸â€²â€²)
ğµâ€²â€², ğµâ€², ğ¸â€² +
1
ğœ†4 Â·

ğ‘›
2120 ln2 ğ‘ ğ·

ğ¸â€²â€²/
(cid:1)
(cid:0)
Â·

+
ğ‘ 
ğ’¢

ğ¸â€²â€², ğ¸â€²)

ğ‘ 

(

, ğµâ€² 6
be an zero-one indicator that is

ğ´, ğµ, ğµâ€², ğ¸, ğ¸â€²)

ğ‘ 
ğ’¢

â‰ 

âˆ…

(

â‰ 
âˆ…
2
2
)/
+

.

. Since ğ· 6 ğ‘›0.33 and

(cid:18)

(cid:16)

(cid:17)

(cid:19)

ğœ†2 > 100000ğ‘  ln ğ‘ , ğ‘‘ = ğ‘œ

ğ‘›
1020ğ·3

(cid:18)(cid:16)

ğœ†2

200
/

(cid:17)

(cid:19)

. Hence

ğ”¼
ğœ‡

ğ‘Œ
ğ»ğ›¼(

)

ğ‘Œ
Ã•ğ»ğ›¼
(

)âˆˆâ„‹â„³

6ğ· (cid:18)

2

6

(cid:19)

Ã•06ğµâ€²,ğµâ€²â€²6ğ·

2 Ã•06ğ¸â€²,ğ¸â€²â€²6ğ·

/

6

Ã•06ğµâ€²,ğµâ€²â€²6ğ·

2 Ã•06ğ¸â€²,ğ¸â€²â€²6ğ·

/

ğœ™

(

ğµâ€², ğµâ€²â€², ğ¸â€², ğ¸â€²â€²

ğ›¿2ğ‘‘

ğµâ€²

(cid:18)

(cid:1)

)
(cid:0)
ğ¸â€²â€²/
2

ğ¸

2
/

ğ¸

ğ¸

ğ‘ 

2
+
2ğ¸â€²

(cid:19)

ğ‘‘ğµâ€²â€²

(cid:18)
ğœ†

Â·

ğ‘›
ğ¸

)

(cid:16)

(cid:17)
âˆšğ‘  ln ğ‘ 
âˆšğ‘›

ğµâ€², ğµâ€²â€², ğ¸â€², ğ¸â€²â€²

ğœ™

(

ğ›¿2ğ‘‘

230

ğµâ€²

Â·

(cid:1)

Â·

(cid:0)

2ğ¸â€²â€²

105âˆšğ¸
âˆšğ‘› (cid:19)
ğ¸â€²/
2

(cid:18)

!
2120ğœ†4ğ· ln2 ğ‘ 
ğ‘›

(cid:19)

ğ‘‘ğµâ€²â€²

Â·

1020ğ·3
ğ‘›

ğµâ€²

2
2
)/
+

(cid:18)
ğ‘ 

(

!
ğµâ€²

(cid:19)
ğ‘ 

2
2
)/
+

ğ‘œ

1
)
(

+

!

(cid:19)

2

+

âˆ

Ã•ğµâ€²â€²=1 

ğ‘‘

(cid:18)

1020ğ·3
ğ‘›

(cid:19)

ğµâ€²â€²

ğœ†2

200
/

!

ğ›¿2ğ‘‘

ğœ†4ğ‘‘

(cid:18)

(cid:18)

6 2

6 2

6 ğ‘œ

âˆ

Ã•ğµâ€²=1 

âˆ

Ã•ğµâ€²=1 
.
1
)
(

2120ğœ†4ğ· ln2 ğ‘ 
ğ‘›

2100ğ· ln2 ğ‘ 
ğ‘›

(

(cid:19)

54

 
 
 
Now condiser the case ğ›¿2ğ‘‘ < 1. Since ğœ† > 210, ğœ†2ğ‘ 

2 > 1
+

ğ›¿ and

2ğ‘ 
(

2
)

+

ln ğœ† > ln

1
ğ›¿

(cid:18)

(cid:19)

> ln

1
ğ›¿

(cid:18)

+

(cid:19)

ln

1
ğ›¿ğ‘‘

(cid:19)

(cid:18)

= ln

1
ğ›¿2ğ‘‘

.

(cid:19)

(cid:18)

Since ğœ†2 > 100000ğ‘  ln ğ‘ , ğœ†2

ğœ†4ğ·ğ‘ 2 ln2 ğ‘  = ğ‘œ

ğ‘› log2

ğ›¿2ğ‘‘

/

100 > ln

1
ğ›¿2 ğ‘‘

and ğµâ€²â€² 6 ğ¸

(cid:16)
. It follows that

(cid:17)

ln

ğ›¿2ğ‘‘

/|

(cid:0)

|

(cid:1)

. Let ğ‘€ = max

ğµâ€², ğµâ€²â€²}

{

. Recall that

(cid:16)

ğ”¼
ğœ‡

(cid:0)

2

ğ‘Œ
ğ»ğ›¼(

)

(cid:19)

ğ‘Œ
Ã•ğ»ğ›¼
(

)âˆˆâ„‹â„³

6ğ· (cid:18)

(cid:1)(cid:17)
6

Ã•06ğµâ€²,ğµâ€²â€²6ğ·

2 Ã•06ğ¸â€²,ğ¸â€²â€²6ğ·

/

ğ¸

2
/

ğ‘€ğ¸

ğ‘›
ğ¸

)

(cid:16)

(cid:17)
âˆšğ‘  ln ğ‘ 
âˆšğ‘›

2ğ¸â€²

ğœ†

Â·

!

2ğµâ€²
ğ¸â€² ğ‘€

Â·

(cid:1)

)
(cid:18)
(cid:0)
ğ¸â€²â€²/
2

ğµâ€², ğµâ€²â€², ğ¸â€², ğ¸â€²â€²

ğœ™

(

ğ›¿2ğ‘‘

230

ğµâ€²

Â·

(cid:1)

Â·

(cid:0)

ğµâ€², ğµâ€²â€², ğ¸â€², ğ¸â€²â€²

ğ›¿2ğ‘‘

ğœ™

(

ğ‘‘ğµâ€²â€²

Â·

(cid:18)

1020ğ·3
ğ‘›

(cid:19)

2ğ¸â€²â€²

ğ‘‘ğµâ€²â€²

(cid:18)

105âˆšğ¸
âˆšğ‘› (cid:19)
2120ğœ†4ğ‘ 2 ln2 ğ‘ 
ğ‘›

ğ¸â€²/
2

(cid:19)

Ã•06ğµâ€²,ğµâ€²â€²6ğ·

2 Ã•06ğ¸â€²,ğ¸â€²â€²6ğ·

/

6

6

Ã•06ğµâ€²â€²6ğ·

2 Ã•06ğ¸â€²,ğ¸â€²â€²6ğ· Ã•06ğµâ€²6ğ¸â€²

/

ğœ™

(

ğµâ€², ğµâ€²â€², ğ¸â€², ğ¸â€²â€²

2120ğœ†4ğ·ğ‘ 2 ln2 ğ‘ 
ğ›¿2ğ‘‘

ln2

ğ‘›

(

)

ğ¸â€²/
2

(cid:19)

)
(cid:18)
ğ¸â€²â€²/
2

ğ‘‘ğµâ€²â€²

Â·

1020ğ·3
ğ‘›

(cid:18)

âˆ

ğ‘‘

(cid:18)

Ã•ğµâ€²â€²=1 

(cid:19)
1020ğ·3
ğ‘›

ğµâ€²â€²

ğœ†2

200
/

(cid:19)

!

2130ğœ†4ğ·ğ‘ 2 ln2 ğ‘ 
ğ›¿2ğ‘‘

ln2

ğ‘›

(

)

ğ¸â€²/
2

(cid:19)

2

+

6 âˆ

Ã•ğ¸â€²=1(cid:18)
.
1
)
(

6 ğ‘œ

By Corollary 6.9, we get the desired colclusion.

(cid:3)

7 Polynomial-based algorithm with the right log factor

In this section we will prove the following theorem.

Theorem 7.1. Let ğ‘‘1
âˆ’
ğ›½ > 0 and

ğ‘œ

) 6 ğ‘˜2 6 ğ‘œ
1

(

ğ‘‘

(

)

and ğ‘› > Î©

log5 ğ‘‘

as ğ‘‘

(cid:16)

(cid:17)

â€¢ ğ‘¢0 âˆˆ
ğ”¼ ğ‘¢0(

â€¢ ğ‘Š

Ã—
âˆˆ
and ğ”¼ ğ‘Š 2
ğ‘–ğ‘—

.
1
)
(

â„ğ‘› is a random vector with independent entries such that for all ğ‘–
4 6 ğ‘‚
ğ‘–
)
â„ğ‘›

ğ‘‘ is a random matrix with independent entries such that for all ğ‘–
= 1, and ğ‘Š and ğ‘¢0 are independent.

âˆˆ

â€¢ ğ‘£0 âˆˆ

â„ğ‘‘ is a (non-random) unit vector with entries in

0,

{

âˆšğ‘˜
1
/

.

}

Â±

55

. Let ğ‘Œ =

T
ğ›½ğ‘¢0ğ‘£0

ğ‘Š, where

+

â†’ âˆ

p

ğ‘›

, ğ”¼ ğ‘¢0(

ğ‘–

2 = 1 and
)

]

âˆˆ [

ğ‘› and ğ‘—

ğ‘‘

]

âˆˆ [

, ğ”¼ ğ‘Šğ‘–ğ‘— = 0

 
Suppose that

ğ›½ > ğ¶âˆ—

ğ‘˜
âˆšğ‘› s

log

ğ‘‘
ğ‘˜2 +

log ğ‘‘
log ğ‘›

for some large enough constant ğ¶âˆ—. Then there exists a probabilistic algorithm that given ğ‘Œ as input, in time
ğ‘›ğ‘‘

1
) outputs a unit vector

â„ğ‘‘ such that

ğ‘‚

(

(

)

ğ‘£
Ë†

âˆˆ

1
)
(
(with respect to the distribution of ğ‘Œ and the randomness of the algoruthm).

ğ‘£, ğ‘£0i

âˆ’ h Ë†

1

2 6 ğ‘œ

ğ‘œ

with probability 1

1
)
(
Remark 7.2. The algorithm also works for Î©
(
can make it arbitrarily small by increasing ğ¶âˆ—), but

âˆ’

ğ‘‘

)

6 ğ‘˜2 6 ğ‘‘

/
ğ‘£, ğ‘£0i
h Ë†

2 in the sense that 1
2 might not tend to one in this regime.

ğ‘£, ğ‘£0i

2 is small (we

âˆ’ h Ë†

The advantage of algorithm 7.1 compared to Covariance Thresholding (and other algorithms)

is that it works for ğ›½ = ğ‘œ

min

ğ‘˜
âˆšğ‘›

log ğ‘‘

,

ğ‘‘
ğ‘›

and small ğ‘› (for example, ğ‘› = ğ‘‘0.99, or ğ‘› = ğ‘‘0.01),

(cid:18)
while Covariance Thresholding can work with ğ›½ = ğ‘œ

q

n

o

(cid:19)

min

ğ‘˜
âˆšğ‘›

log ğ‘‘

,

ğ‘‘
ğ‘›

only if ğ‘› > ğ‘‘1
âˆ’

ğ‘œ

1
) (see

(

(cid:18)

n

o

q

(cid:19)

Theorem D.1). Another advantage is that other algorithms for sparse PCA use many properties of
Gaussian distribution (for example, they use ğœ’2 tail bounds), while this algorithm requires only
assumptions on ï¬rst two moments of ğ‘Š.

The algorithm 7.1 will use low degree polynomials to estimate the entries of ğ‘£0ğ‘£0

T. We give a

precise description of polynomials that we use in the following subsection.

7.1 Low degree polynomials as estimators

To work with polynomials we introduce the following notation:

[

]

{

ğ‘‘

ğ‘›

âˆˆ [

] Ã— [

ğ‘–, ğ‘—
ğ‘—

, let ğ¼ğ›¼ :=
ğ‘›

ğ‘–
{
. For ğ‘—

For a multi-index ğ›¼ over
:

ğ‘‘
ğ‘›
and similarly
]}
]
ğ½ğ›¼ :=
ğ‘–, ğ‘—
ğ›¼ for some ğ‘–
ğ‘—
, and
ğ›¼
}
(
(
) âˆˆ
) âˆˆ
ğ›¼ğ‘–ğ‘—
similarly let ğ½ğ›¼,ğ‘– :=
ğ‘–, ğ‘—
ğ‘‘
ğ›¼ ğ‘‹
:
. Note that
ğ›¼
Ã—
ğ‘–ğ‘—
(
âˆˆ [
can be represented as a bipartite multigraph ğºğ›¼ =
ğ‘›
ğ½ğ›¼ , ğ¸ğ›¼)
ğ¼ğ›¼
every multi-index ğ›¼ over
] Ã— [
has multiplicity ğ›¼ğ‘–ğ‘—. In this representation the set ğ½ğ›¼,ğ‘– corresponds to
ğ‘–, ğ‘—
such that each edge
the neighborhood of the vertex ğ‘– and the set ğ¼ğ›¼,ğ‘— corresponds to the neighborhood of ğ‘—. If ğ›¼ is
multilinear, ğºğ›¼ is just a graph (i.e. multiplicity of each edge is 1).

:=
ğ‘‘ denote ğ‘‹ ğ›¼ :=

âˆˆ [
. For a matrix ğ‘‹

ğ‘–, ğ‘—
) âˆˆ
, let ğ¼ğ›¼,ğ‘—

ğ‘‘
âˆˆ [
ğ‘›
:
]
ğ‘–,ğ‘—

ğ›¼ for some ğ‘—

]
âˆˆ [
âˆˆ

:
(
ğ‘‘
]
â„ğ‘›

) âˆˆ
ğ‘‘
]

(
Ã

]
[
}

âˆˆ [

âˆˆ [

Ã

]}

)âˆˆ

{

}

{

{

(

ğ‘–

Now we deï¬ne the graphs which represent the monomials that we will use.

â„• and let ğ‘

Deï¬nition 7.3. Let ğ‘™
We deï¬ne
ğ½ğ›¼ =
}
there are exactly ğ‘ diï¬€erent vertices ğ‘–ğ‘ 1, . . . , ğ‘–ğ‘ ğ‘

â„• be an odd number, and ï¬x two diï¬€erent ğ‘—0 âˆˆ [
âˆˆ

ğ‘‘
.
]
ğ¼ğ›¼ has degree 2,
(diï¬€erent from ğ‘—0 and ğ‘—ğ‘™), and for any 1 6 ğ‘  6 ğ‘™

to be the set of bipartite graphs with 2ğ‘ğ‘™ edges such that any ğ‘–
ğ‘‘
]
ğ¼ğ›¼ that are adjacent to both ğ‘—ğ‘ 

for distinct ğ‘—1, . . . , ğ‘—ğ‘™

ğ’¢
ğ‘—0, ğ‘—1, . . . , ğ‘—ğ‘™

)
1, ğ‘—ğ‘™
âˆ’

and ğ‘—ğ‘™

ğ‘—0 ğ‘—ğ‘™ (

ğ‘, ğ‘™

âˆˆ [

âˆˆ

âˆˆ

ğ‘‘

{

]

1, ğ‘—ğ‘  (see Fig. 1).
âˆ’

1 âˆˆ [
âˆ’
âˆˆ

Now we are ready to deï¬ne the polynomials that we will use.

Deï¬nition 7.4. Let ğ‘ be the smallest odd number that is greater than ğ¶âˆ—âˆ— Â·
for some
constant ğ¶âˆ—âˆ— > 100, and let ğ‘™ be the smallest integer such that ğ‘ğ‘™ > log ğ‘‘ and let ğ‘—0 < ğ‘—ğ‘™. We deï¬ne

ğ‘˜2 +

(cid:16)

(cid:17)

log ğ‘‘

log ğ‘‘
log ğ‘›

ğ‘ƒğ‘—0 ğ‘—ğ‘™ (
ğ‘Œ

)

=

1
ğœ…

ğ‘Œğ›¼ ,

Ã•ğ›¼
âˆˆğ’¢ğ‘—0 ğ‘—ğ‘™ (

ğ‘,ğ‘™

)

56

ğ‘–11

ğ‘–12

ğ‘–13

ğ‘—1

ğ‘–21

ğ‘–22

ğ‘–23

ğ‘—2

ğ‘–31

ğ‘–32

ğ‘–33

ğ‘—3

ğ‘–41

ğ‘–42

ğ‘–43

Figure 1: A graph from

ğ‘—0 ğ‘—ğ‘™ (

ğ’¢

ğ‘, ğ‘™

)

for ğ‘ = 3 and ğ‘™ = 4.

ğ‘—4

ğ‘—0

where

ğœ… = ğ‘˜

ğ‘˜

ğ‘™

+
is a normalization factor so that ğ”¼ ğ‘ƒğ‘—0 ğ‘—ğ‘™ (
ğ‘Œ

Â· Â· Â· (

âˆ’

ğ‘›

âˆ’ (

ğ‘

1
)

ğ‘™
âˆ’
ğ‘

ğ‘ğ‘™

ğ›½
ğ‘˜

(cid:19)

(cid:19) (cid:18)

ğ‘˜

Â·

ğ‘›
2
ğ‘
Â· Â· Â·
) Â·
(cid:19)
(cid:18)
= ğ‘£ ğ‘—0 ğ‘£ ğ‘—ğ‘™ .

)

(cid:18)

Note that under assumptions of Theorem 7.1, ğ‘ = ğ‘œ
log ğ‘‘
, so ğ‘™
)
(
ğ‘Œ
The following lemma shows that the expectation of ğ‘ƒğ‘—0 ğ‘—ğ‘™ (

)

â†’ âˆ
is indeed ğ‘£0(

.

ğ‘–

ğ‘£0(

)

ğ‘—

.
)

Lemma 7.5.

Proof. By construction of ğ›¼,

ğ”¼ ğ‘ƒğ‘—0 ğ‘—ğ‘™ (
ğ‘Œ

)

= ğ‘£0(

ğ‘—0)

ğ‘£0(

ğ‘—ğ‘™

)

,

ğ”¼ ğ‘Œğ›¼ =

ğ‘™

1
âˆ’

1
ğœ…

ğ›½ğ‘ğ‘™

1
ğ‘˜ğ‘

(cid:18)

(cid:19)

ğ‘£0(

ğ‘—0)

ğ‘ ğ‘£0(

ğ‘—ğ‘™

)

ğ‘ =

ğ‘™

1
ğœ…

ğ›½
ğ‘˜

(cid:18)

(cid:19)

ğ‘˜

ğ‘£0(

ğ‘—0)

ğ‘£0(

ğ‘—ğ‘™

)

.

Â·

= 0, the statement is true. Assume that it is not zero. Number of nonzero terms is
(cid:3)

, so we get the desired equality.

âˆ’(

ğ‘›

ğ‘

ğ‘—ğ‘™

ğ‘£0(
ğ‘—0)
ğ‘™
ğ‘˜
+
âˆ’

If ğ‘£0(
ğ‘˜
Â· Â· Â·
Â· Â· Â· (
(cid:0)
ğ‘Œ
Now letâ€™s bound the variance of ğ‘ƒğ‘—0 ğ‘—ğ‘™ (

)
2
) Â·

ğ‘™
1
)
âˆ’
ğ‘

ğ‘›
ğ‘

(cid:1)

(cid:0)

(cid:1)

.
)

Lemma 7.6. Suppose that ğ›½ = ğ¶

ğ‘˜
âˆšğ‘› Â·

Â·

âˆšğ‘ for some constant ğ¶ > 100. Then

Proof. For simplicity we will write

instead of

ğ•
ğœ‡

ğ‘Œ
ğ‘ƒğ‘—0 ğ‘—ğ‘™ (

)

6 ğ‘œ

(cid:18)

ğ‘£0(

ğ‘—0)

2ğ‘£0(

ğ‘—ğ‘™

2
)

+

1
ğ‘‘

ğ‘£0(

2

ğ‘—0)

ğ‘—ğ‘™

ğ‘£0(

2
)

+

+

1
ğ‘‘2

.

(cid:19)

(cid:1)

(cid:0)
ğ‘, ğ‘™
ğ‘—0 ğ‘—ğ‘™ (

.
)

ğ’¢

ğ’¢
= 1
ğœ…2

ğ‘Œ

ğ”¼ ğ‘Œğ›¼ğ‘Œğ›¼â€². Note that ğ”¼ ğ‘Œğ›¼ğ‘Œğ›¼â€² cannot be negative.

ğ‘—0 ğ‘—ğ‘™ (
that correspond to non-intersecting graphs have contribution at most
)

ğ›¼,ğ›¼â€²âˆˆğ’¢

Ã

)

We need to bound ğ”¼ ğ‘ƒ2
The terms in ğ”¼ ğ‘ƒ2
2

ğ‘—0 ğ‘—ğ‘™ (

ğ‘Œ

. Indeed,

ğ”¼ ğ‘ƒğ‘—0 ğ‘—ğ‘™ (
ğ‘Œ

(cid:0)

)

(cid:1)

ğ‘†diï¬€ :=

ğ”¼ ğ‘Œğ›¼ğ‘Œğ›¼â€² =

ğ”¼ ğ‘Œ ğ›¼ ğ”¼ ğ‘Œ ğ›¼â€² 6

=
ğ¼ğ›¼â€²
Ã•ğ¼ğ›¼
âˆ©
âˆ…
=
ğ½ğ›¼â€²
ğ‘—0,ğ‘—ğ‘™ }
âˆ©
{
To bound the other terms will need the following lemma:

=
ğ¼ğ›¼â€²
Ã•ğ¼ğ›¼
âˆ©
âˆ…
=
ğ½ğ›¼â€²
ğ‘—0,ğ‘—ğ‘™ }
âˆ©
{

ğ½ğ›¼

ğ½ğ›¼

57

ğ”¼
ğœ‡

ğ‘

ğ‘Œ
(

)

(cid:18)

2

.

(cid:19)

Lemma 7.7. Let ğ‘† be a set of pairs

ğ‘–, ğ‘—

(

)

such that that for ğ›¼, ğ›¼â€² âˆˆ ğ’¢
ğ‘Œğ›¼ğ‘Œğ›¼â€² =

,

ğ‘Œ2
ğ‘–ğ‘—ğ‘”

ğ‘Œ
(

)

where ğ‘”

ğ‘Œ
(

)

is some monomial. Then

ğ‘†
ğ‘–,ğ‘—
Ã–(
)âˆˆ

ğ‘Œğ›¼ğ‘Œğ›¼â€² =

ğ”¼
ğœ‡

1

(

+

ğ‘œ

1
))
(

ğ”¼
ğœ‡

.

ğ‘”

ğ‘Œ
(

)

Proof. Assume that ğ‘† =

ğ‘–â€², ğ‘—â€²)}

{(

(i.e.

ğ‘†

|

|

= 1). Since ğ›½ = ğ‘‚

ğ‘˜
âˆšğ‘›

âˆšğ‘

,

(cid:16)

(cid:17)

ğ”¼
ğœ‡

ğ‘Œğ›¼ğ‘Œ ğ›¼â€² = ğ”¼
ğœ‡

ğ‘¤ğ‘–â€² ğ‘—â€² +

ğ›½ğ‘¢ğ‘–â€²ğ‘£ ğ‘—â€²

p

(cid:16)

2

(cid:17)

Hence for arbitrary ğ‘†

ğ‘”

ğ‘Œ
(

)

= ğ”¼
ğœ‡

ğ‘”

ğ‘Œ
(

) +

ğ›½ ğ”¼
ğœ‡

ğ‘¢2
ğ‘–â€²

ğ‘£2
ğ‘—â€²

ğ‘”

ğ‘Œ
(

)

= ğ”¼
ğœ‡

ğ‘”

ğ‘Œ
(

) +

ğ‘‚

ğ‘
ğ‘› !

 r

ğ”¼
ğœ‡

.

ğ‘”

ğ‘Œ
(

)

ğ‘Œğ›¼ğ‘Œ ğ›¼â€² =

1

ğ”¼
ğœ‡

ğ‘†

|

|

ğ‘‚

+

 r

ğ‘
ğ‘› !!

ğ”¼
ğœ‡

.

ğ‘”

ğ‘Œ
(

)

Since

ğ‘†

|

|

6 ğ‘‚

ğ‘ğ‘™

(

)

6 ğ‘‚

log ğ‘‘
(

)

Lemma 7.7 implies that

6 ğ‘œ

(
p
ğ›¼ ğ”¼ğœ‡(

ğ‘›
ğ‘ )
ğ‘Œğ›¼

Ã

, we get the desired bound.

. Note that

2 6

)

ğ‘œ

1
(

+

1
))|ğ’¢|
(
ğ‘™

ğœ… =

1
(

âˆ’

ğ‘œ

1
))
(

(cid:18)

ğ‘›
ğ‘

ğ‘˜ğ‘™

(cid:19)

(cid:18)

ğ‘ğ‘™

,

ğ›½
ğ‘˜

(cid:19)

since ğ‘™ 6 ğ‘œ

âˆšğ‘˜
(

)

and ğ‘ğ‘™ 6 ğ‘‚

log ğ‘‘

6 ğ‘œ

âˆšğ‘›

. Similarly,

=

1
(

âˆ’

ğ‘œ

1
)) Â·
(

ğ‘‘ğ‘™

1
âˆ’

|ğ’¢|

ğ‘Œğ›¼

ğ”¼
ğœ‡ (

2 6
)

Ã•ğ›¼
Here we used ğ‘ğ‘™ > log ğ‘‘.

(cid:0)
1
(

+

(cid:1)
1
)) Â·
(

ğ‘œ

(cid:0)

ğ‘‘ğ‘™

1
âˆ’

ğ‘™

(cid:1)
ğ‘›
ğ‘

(cid:18)

(cid:19)

6

ğ‘œ

1
(

+

ğœ…2

1
))
(

1
ğ‘‘

(cid:18)

(cid:19)  (cid:18)

ğ‘˜2ğ‘
ğ›½2ğ‘›

ğ‘

(cid:19)

(cid:18)

ğ‘™

ğ‘›
ğ‘

ğ‘™

(cid:1)

(cid:0)

ğ‘‘
ğ‘˜2

(cid:19) !

(cid:3)

. Hence

6 ğœ…2
ğ‘‘10

.

To bound the other terms in ğ”¼ ğ‘ƒ2

ğ‘Œ

we deï¬ne some notions related to the multigraphs which

)

ğ‘—0 ğ‘—ğ‘™ (

correspond to ğ‘Œ ğ›¼ğ‘Œğ›¼â€².
We will call ğ‘—

âˆˆ

âˆˆ

ğ½ğ›¼ circles and ğ‘–

ğ¼ğ›¼ boxes. A circle ğ‘— is called blocked if each box that is adjacent
to ğ‘— has two parrallel edges to ğ‘—. Equivalently, if ğ‘£ ğ‘— appears in ğ‘Œğ›¼ğ‘Œğ›¼â€² only in squared parentheses,
2.
i.e. only in parantheses of the form
)
If two blocked circles are adjacent to the same box (which means that they are adjacent to
exactly ğ‘ same boxes), we call such circles consecutive. A maximal sequence of consecutive blocked
circles is called a blocked segment. The endpoints of a blocked segment are circles from this segment
that are either ğ‘—0, ğ‘—ğ‘™, or circles that share a box with a non-blocked circle.

ğ›½ğ‘¢ğ‘–ğ‘£ ğ‘—

ğ‘¤ğ‘–ğ‘—

p

+

(

A block segment is called closed if the boxes that are adjacent to the endpoints of this block
are adjacent to exactly two circles. If this condition is satisï¬ed only for one endpoint, we call such
segment half-open, and if it is not satisï¬ed for both endpoints, we call it open.

A block segment that contains ğ‘—0 is called the leftmost and the segment that contains ğ‘—ğ‘™ is called

the rightmost. Other segments are called intermediate.

We can group the terms diï¬€erent from

ğ‘Œğ›¼
(

2 in the following way: Let
)

58

 
â€¢ ğ‘1 be the number of circles in the leftmost block segment

â€¢ ğ‘2 be the number of circles in the rightmost block segment

â€¢ ğ‘3 be the number of circles in intermediate block segments

â€¢ ğ‘ ğ‘ be the number of closed intermediate segments

â€¢ ğ‘ ğ‘œ be the number of open intermediate segments

â€¢ ğ‘ â„ be the number of half-open intermediate segments

â€¢ ğ‘Ÿ be the number of circles of degree 4ğ‘ that are not blocked and do not share any box with

blocked circles.

â€¢ ğ‘šğ‘ (for ğ‘

âˆˆ {
circles are not blocked.

}

3, 4

) be the number of boxes that are adjacent to exactly ğ‘ circles such that these

â€¢ ğ‘š be the number of boxes of degree 4 which are adjacent to exactly two non-blocked circles

and at least one of these circles doesnâ€™t share a box with blocked circles.

Then we group terms in a way such that these parameters ğ‘1, ğ‘2, ğ‘3, ğ‘ ğ‘, ğ‘ ğ‘œ, ğ‘ â„, ğ‘Ÿ, ğ‘š2, ğ‘š3, ğ‘š4 are
equal for all terms ğ‘Œğ›¼ğ‘Œğ›¼â€² inside one group.

Letâ€™s ï¬x the parameters (ğ‘1, ğ‘2, ğ‘3, ğ‘ ğ‘, ğ‘ ğ‘œ, ğ‘ â„, ğ‘Ÿ, ğ‘š2, ğ‘š3, ğ‘š4) and compute the contribution of

nonzero terms ğ”¼ ğ‘Œ ğ›¼ğ‘Œğ›¼â€² which correspond to graph with these parameters.

We will use the following way of counting: for every ğ‘Œğ›¼ğ‘Œğ›¼â€² such that ğ›¼ and ğ›¼â€² have common
obtained from ğ›¼â€² by replacing

box/circle of some type (diï¬€erent from ğ‘—0, ğ‘—ğ‘™) we consider ğ›¼â€²â€² âˆˆ ğ’¢
each box/circle of this type by another box/circle that is not in ğ›¼.

ğ›¼, ğ›¼â€²)

ğ‘›
âˆˆ [
ğ‘œ
1
âˆ’
(

6 ğ¶â€². Then ğ”¼ ğ‘Œğ›¼ğ‘Œğ›¼â€²â€² >

We start with ğ‘š4 > 0. For every nonzero ğ‘Œğ›¼ğ‘Œğ›¼â€² with ğ‘š4 > 0 let ğ‘€4(

be the set of boxes that
that are obtained from ğ›¼â€² by replacing
are adjacent to 4 non-blocked circles. Consider all ğ›¼â€²â€² âˆˆ ğ’¢
by some box that is not in ğ›¼. Then ğ‘Œğ›¼ğ‘Œğ›¼â€²â€² has parameter ğ‘š4 = 0. Recall
ğ›¼, ğ›¼â€²)
each box from ğ‘€4(
ğ‘š4 ğ”¼ ğ‘Œ ğ›¼ğ‘Œğ›¼â€².
, ğ”¼ ğ‘¢4
that there exists a constant ğ¶â€² such that for all ğ‘–
ğ‘–
]
ğ‘›ğ‘š4, while number of diï¬€erent ğ‘Œğ›¼ğ‘Œğ›¼â€² such
Number of such ğ›¼â€²â€² (for ï¬xed ğ›¼, ğ›¼â€²) is is at least
1
))
(
2ğ‘š4.
that ğ‘Œğ›¼ğ‘Œğ›¼â€²â€² could be obtain from them using the procedure described above is is at most
)
Hence the contribution of terms for which ğ‘š4 = 0 is larger than the contribution of terms with
ğ‘š4 > 0 by a factor

log4 ğ‘‘
since ğ‘› > Î©
1
)
(
(
Similarly, for every nonzero ğ‘Œğ›¼ğ‘Œğ›¼â€² with ğ‘š3 > 0 let ğ‘€3(
ğ›¼, ğ›¼â€²)

to 3 non-blocked circles. Consider all ğ›¼â€²â€² âˆˆ ğ’¢
ğ‘€4(
ğ›¼â€²â€² (for ï¬xed ğ‘Œ ğ›¼ğ‘Œğ›¼â€²) is is at least
ğ‘œ
âˆ’
could be obtain from them is at most
larger than the contribution of terms with ğ‘š3 > 0 by a factor

be the set of boxes that are adjacent
that are obtained from ğ›¼â€² by replacing each box from
ğ‘š3
ğ”¼ ğ‘Œğ›¼ğ‘Œ ğ›¼â€². Number of such
ğ‘ğ‘š3, while number of diï¬€erent ğ‘Œ ğ›¼ğ‘Œğ›¼â€² such that ğ‘Œğ›¼ğ‘Œğ›¼â€²â€²
(cid:17)
ğ‘›
1
))(
(
)
2ğ‘š3. Hence the contribution of terms for which ğ‘š3 = 0 is
2ğ‘ğ‘™
)
(

by some box that is not in ğ›¼. Then ğ”¼ ğ‘Œğ›¼ğ‘Œ ğ›¼â€²â€² >

. Note that this factor is ğœ”

ğ›¼, ğ›¼â€²)

ğ‘›
3ğ‘2ğ‘™2ğ¶â€²

1
))
(

1
(

1
(

2ğ‘ğ‘™

1
ğ¶â€²

.
)

ğ‘š4

âˆ’

ğ›½
ğ‘˜

ğ‘œ

(cid:17)

(cid:16)

(cid:16)

(

(cid:0)

(cid:1)

Note that this factor is ğœ”

1
)
(

ğ‘š3

>

ğ›½ğ‘›
3ğ‘˜ğ‘3ğ‘™2

ğ‘š3

2
/

.

ğ‘›
10ğ‘5ğ‘™4

(cid:16)

(cid:17)

(cid:18)

(cid:19)
log5 ğ‘‘
since ğ‘› > Î©
(

.
)

59

For every nonzero ğ‘Œğ›¼ğ‘Œğ›¼â€² with ğ‘š > 0 and ğ‘š3 = ğ‘š4 = 0, let ğ‘€

be the set of these ğ‘š boxes
be the set of non-blocked circles of degree 4ğ‘ which do not share any box with

ğ›¼, ğ›¼â€²)

(

and let ğ‘…
ğ›¼, ğ›¼â€²)
blocked circles.

(

ğ‘š

ğ‘š

ğ‘œ

1
(

ğ›¼, ğ›¼â€²)
(
ğ›½2
ğ‘˜2

Consider all ğ›¼â€²â€² âˆˆ ğ’¢

that are obtained from ğ›¼â€² by replacing each box from ğ‘€

by some
box that is not in ğ›¼ and each circle from ğ‘…
by some circle from the support of ğ‘£0 that is not
in ğ›¼â€². It follows that ğ”¼ ğ‘Œğ›¼ğ‘Œğ›¼â€²â€² >
ğ”¼ ğ‘Œğ›¼ğ‘Œ ğ›¼â€². Number of such ğ‘Œğ›¼ğ‘Œğ›¼â€²â€² (for ï¬xed ğ‘Œğ›¼ğ‘Œğ›¼â€²)
ğ‘›
ğ‘˜ğ‘Ÿ. Number of diï¬€erent ğ‘Œ ğ›¼ğ‘Œğ›¼â€² such that ğ‘Œğ›¼ğ‘Œğ›¼â€²â€² could be obtain from them
ğ‘œ
1
1
is at least
ğ‘
âˆ’
(
))
(
2ğ‘š
2ğ‘Ÿ ğ‘šğ‘Ÿ
. Indeed, number of ways to choose circles which replace ğ‘…
4ğ‘ğ‘™
2ğ‘™
is
is at most
(cid:1)
(cid:0)
)
)
(
(
2ğ‘Ÿ . Once these circles are chosen, number of ways to choose boxes which replace
2ğ‘™
bounded by
)
(
ğ‘šğ‘¡ for each chosen circle ğ‘—ğ‘¡
ğ›¼, ğ›¼â€²)
ğ‘€
Ëœ
such that
) of numbers of
possibilities to choose 2 subsets of size

ğ‘Ÿ , and the product (over all ğ‘¡
ğ‘šğ‘¡ in a set of size 4ğ‘. This product is bounded by
Ëœ

is bounded by the number of ways to choose the numbers

ğ‘šğ‘¡ = ğ‘š, which is bounded by

ğ›¼, ğ›¼â€²)

ğ›¼, ğ›¼â€²)

ğ‘Ÿ
ğ‘¡=1 Ëœ

1
))
(

2ğ‘
(

âˆˆ [

ğ‘Ÿ
ğ‘š

âˆ’

ğ‘š

(cid:17)

(cid:16)

ğ‘Ÿ

]

(

)

(

(

(cid:1)

(cid:0)

Ã

2ğ‘š
)

4ğ‘
(
2

ğ‘š1!
( Ëœ
)

2
ğ‘šğ‘Ÿ!
)

Â· Â· Â· ( Ëœ

2ğ‘š

6

4ğ‘
(

)

2ğ‘š

.

ğ‘Ÿ
ğ‘š

(cid:16)

(cid:17)

Hence the contribution of terms with ğ‘š = 0 is larger than the contribution of terms with ğ‘š > 0 by
a factor

1
(

âˆ’

ğ‘œ

1
))
(

ğ‘š

ğ›½2ğ‘›
ğ‘˜2ğ‘

(cid:18)

(cid:19)

ğ‘˜ğ‘Ÿ

2ğ‘™
(

2ğ‘Ÿ

âˆ’
)

2ğ‘
(

2ğ‘Ÿ

âˆ’
)

4ğ‘
(

2ğ‘š

âˆ’
)

(cid:16)

ğ‘š
ğ‘Ÿ

Note that this factor is ğœ”

since ğ‘ = ğ‘œ

1
)
(

log ğ‘‘
(

)

2ğ‘š

> ğ¶ ğ‘š

ğ‘˜
5ğ‘ğ‘™

ğ‘š
ğ‘Ÿğ‘

(cid:18)

(cid:17)

(cid:16)
and ğ‘ğ‘™ 6 log ğ‘‘ 6 ğ‘‚

(cid:17)
log ğ‘˜
(

(cid:19)
.
)

ğ‘Ÿ

2ğ‘š

ğ‘Ÿ

/

> ğ¶ ğ‘š

2âˆ’

ğ‘‚

ğ‘

(

)

ğ‘˜
5ğ‘™ğ‘

(cid:18)

ğ‘Ÿ

.

(cid:19)

)âˆ’

2ğ‘™
(

ğ›¼, ğ›¼â€²)

. Let ğ‘…â€²(

2ğ‘Ÿ which is ğœ”

ğ‘Ÿ > 0 and ğ‘š = 0 by a factor ğ‘˜ğ‘Ÿ

Similarly, the contribution of terms with ğ‘Ÿ = 0 is larger than the contribution of terms with
.
1
)
(

For every nonzero ğ‘Œğ›¼ğ‘Œğ›¼â€² with ğ‘š4 = ğ‘š3 = ğ‘š = ğ‘Ÿ = 0 and ğ‘3 > 0 let ğ‘„

ğ›¼, ğ›¼â€²)
be a set of blocked
be a set of boxes adjacent to circles from
be a set of non-blocked circles of degree 4ğ‘ which share a box with some
be a set of
ğ›¼, ğ›¼â€²)
. We
2ğ‘ ğ‘.
+

ğ›¼, ğ›¼â€²)
âˆˆ
and which are adjacent to circles from ğ‘…â€²(
= ğ‘ â„
= ğ‘ğ‘3 +
= ğ‘3,

circles in intermediate blocked segments and ğ‘€â€²(
ğ‘„
ğ›¼, ğ›¼â€²)
(
ğ‘„ (which means that it shares exactly ğ‘ boxes with some ğ‘—
ğ‘—
âˆˆ
boxes of degree 4 which are not from ğ‘€â€²(
denote ğ‘šâ€²â€² =
ğ›¼, ğ›¼â€²)|
(
|
Consider all ğ›¾â€² âˆˆ ğ’¢
ğ‘€â€²â€²(
ğ›¼, ğ›¼â€²) âˆª

. Note that
that are obtained from ğ›¼â€² using the folowing procedure: each box from
ğ›¼, ğ›¼â€²)
ğ‘€â€²(
is replaced by some circle from the support of ğ‘£0 that is not in ğ›¼â€². Also consider all ğ›¾ that are
obtained from ğ›¼ by replacing circles from ğ‘„ by a circle from the support of ğ‘£0 that is not in ğ›¾â€².

is replaced by some box that is not in ğ›¼, each circle from ğ‘„

ğ›¼, ğ›¼â€²)
ğ›¼, ğ›¼â€²)|

ğ‘„). Let ğ‘€â€²â€²(

ğ›¼, ğ›¼â€²) âˆª

ğ›¼, ğ›¼â€²)|

ğ›¼, ğ›¼â€²)|

ğ›¼, ğ›¼â€²)

ğ›¼, ğ›¼â€²)

ğ‘€â€²â€²(

ğ‘€â€²(

ğ‘…â€²(

ğ‘…â€²(

ğ‘ğ‘ ,

ğ‘„

(

(

|

|

|

It follows that

ğ”¼ ğ‘Œğ›¾ğ‘Œğ›¾â€² >

1
(

âˆ’

ğ‘œ

1
))
(

ğ›½
ğ‘˜

2ğ‘ğ‘3

2ğ‘ğ‘ ğ‘

+

ğ‘ğ‘  â„ +

2ğ‘šâ€²â€²

+

ğ”¼ ğ‘Œğ›¼ğ‘Œğ›¼â€² .

(cid:19)
(cid:18)
2ğ‘ ğ‘ . Note that ğ›¼ and ğ›¼â€²
Number of diï¬€erent ğ‘Œ ğ›¾ğ‘Œ ğ›¾â€² (for ï¬xed ğ‘Œğ›¼ğ‘Œğ›¼â€²) is
1
âˆ’
(
might contain now circles that are not from the support of ğ‘£0 (they should be in ğ‘„). By similar
argument as for the case ğ‘š > 0, the number of diï¬€erent ğ‘Œğ›¼ğ‘Œğ›¼â€² such that ğ‘Œğ›¾ğ‘Œğ›¾â€² could be obtain
from them is at most

ğ‘šâ€²â€² ğ‘˜2ğ‘3

1
))
(

ğ‘  â„ +

ğ‘ğ‘3

ğ‘›
ğ‘

ğ‘ğ‘ 

ğ‘œ

+

+

+

(cid:0)

(cid:1)

2ğ‘šâ€²â€²

.

10ğ‘™
(

4ğ‘  ğ‘‘ğ‘3
)

4ğ‘
(

2ğ‘šâ€²â€²
)

ğ‘ â„

2ğ‘ ğ‘

+
ğ‘šâ€²â€²

(cid:18)

(cid:19)

60

Hence the contribution of terms with ğ‘3 = 0 is larger than the contribution of terms with ğ‘3 > 0 by
a factor

ğ›½
ğ‘˜

1
(

âˆ’

ğ‘œ

1
))
(

(cid:18)
which is at least

2ğ‘ğ‘3

2ğ‘ğ‘ ğ‘

+

ğ‘ğ‘  â„ +

2ğ‘šâ€²â€²

+

(cid:19)

ğ‘ğ‘3

ğ‘ğ‘ 

+

+

ğ‘šâ€²â€²

ğ‘›
ğ‘

(cid:16)

(cid:17)

ğ‘˜2ğ‘3

+

2ğ‘ ğ‘

ğ‘  â„ +

10ğ‘™
(

4ğ‘  ğ‘‘âˆ’
âˆ’
)

ğ‘3

4ğ‘
(

2ğ‘šâ€²â€²

âˆ’
)

2ğ‘šâ€²â€²

âˆ’

,

ğ‘ â„

2ğ‘ ğ‘

+
ğ‘šâ€²â€²

(cid:18)

(cid:19)

ğ‘3

ğ‘˜2
ğ‘‘

Â·

(cid:19)

ğ¶ ğ‘

(cid:18)

ğ¶ ğ‘ğ‘ ğ‘

+

ğ‘šâ€²â€²

ğ‘›
10ğ‘™
(

4ğ‘
)

(cid:18)

ğ‘ğ‘ 0

(cid:19)

(cid:18)

ğ›½ğ‘›
ğ‘˜ğ‘

ğ‘ğ‘  â„

ğ‘˜

(cid:19)

(cid:18)

2âˆ’
Â·
10ğ‘™
(

ğ‘‚

(
4
)

2ğ‘ ğ‘

ğ‘  â„

+

ğ‘

)

.

(cid:19)

Since ğ¶ ğ‘ > 10ğ‘ ğ‘‘

ğ‘˜2 , this factor is ğœ”

.
1
)
(

Hence we can conclude that if ğ‘1 = ğ‘2 = 0, then the contribution of the terms that are diï¬€erent

from ğ‘†diï¬€ is ğ‘œ

ğ‘†diï¬€)
.

(

Now consider the case when ğ‘1 > 0 and the other parameters are zero. There are two cases, the
ï¬rst is when the leftmost blocked segment is half-open and the second when it is closed. In the
ï¬rst case the contribution of such terms is bounded by

1
(

+

ğ‘œ

1
)) Â·
(

ğ‘‘ğ‘1

1
âˆ’

ğ‘ğ‘1

Â·

ğ‘›
ğ‘

(cid:16)

(cid:17)

ğ‘˜2ğ‘™

2
2
(
âˆ’
âˆ’

ğ‘1

1
)
âˆ’

ğ›½ğ‘›
ğ‘˜ğ‘

(cid:19)

Â·

(cid:18)

2ğ‘™

ğ‘

(

2ğ‘1

âˆ’

1
)
+

ğ‘—ğ‘™

ğ‘˜ğ‘£0(

2 6 2
)

 (cid:18)
ğ‘
6 2âˆ’

(

ğ‘1

1
âˆ’

ğ‘˜2ğ‘
ğ›½2ğ‘›

ğ‘

ğ‘‘
ğ‘˜2 !

(cid:19)

ğ‘

ğœ…2
ğ‘˜

ğ‘˜
ğ›½

(cid:18)

(cid:19)

ğ‘—ğ‘™

ğ‘£0(

2
)

ğ‘1

1
)ğ‘‘âˆ’
âˆ’

10

ğœ…2
ğ‘˜

Â·

ğ‘—ğ‘™

ğ‘£0(

2 ,
)

where we used

ğ‘ =

ğ›½

ğ‘˜

(

/

)

ğ‘›
ğ¶ğ‘

ğ‘

2 > ğ‘‘10. In the second case, we get
/

1
(

+

ğ‘œ

1
)) Â·
(

ğ‘‘ğ‘1

1
âˆ’

(cid:0)
ğ‘ğ‘1

(cid:1)
ğ‘˜2ğ‘™

Â·

ğ‘›
ğ‘

(cid:16)

(cid:17)

2ğ‘1

âˆ’

1
âˆ’

ğ›½ğ‘›
ğ‘˜ğ‘

(cid:19)

Â·

(cid:18)

2ğ‘

ğ‘1

ğ‘™

(

âˆ’

)

ğ‘—ğ‘™

ğ‘˜ğ‘£0(

where we used ğ¶ ğ‘ > 10ğ‘ ğ‘‘
ğ‘˜2 .

2 6 2
)

  (cid:18)
6 10âˆ’

ğ‘˜2ğ‘
ğ›½2ğ‘›
(cid:19)
ğ‘ğ‘1 ğœ…2
ğ‘‘

ğ‘1

1
âˆ’

ğ‘

ğ‘‘
ğ‘˜2 !

ğ‘˜2ğ‘
ğ›½2ğ‘›

ğ‘

ğœ…2
ğ‘˜2

(cid:19)

(cid:18)

ğ‘—ğ‘™

ğ‘£0(

2
)

ğ‘—ğ‘™

ğ‘£0(

2 ,
)

Similarly, in the case when ğ‘2 > 0 and the other parameters are zero, the contribution is

bounded by 10âˆ’

ğ‘ğ‘2 ğœ…2

ğ‘‘ ğ‘£0(

2.
ğ‘—0)

Similar computations show that if ğ‘1 > 0 and ğ‘2 > 0 (and the other parameters are zero), then

the contribution is bounded by ğ‘œ

ğœ…2
ğ‘‘2

. Therefore, dividing by ğœ…2, we get

ğ•
ğœ‡

ğ‘Œ
ğ‘ƒğ‘—0 ğ‘—ğ‘™ (

)

(cid:16)
6 ğ‘œ

(cid:17)
ğ‘£0(

(cid:18)

ğ‘—0)

2ğ‘£0(

ğ‘—ğ‘™

2
)

+

1
ğ‘‘

(cid:0)

ğ‘£0(

2

ğ‘—0)

ğ‘—ğ‘™

ğ‘£0(

2
)

+

+

(cid:1)

1
ğ‘‘2

.

(cid:19)

7.2 Computation in polynomial time

Since for ğ‘—0 â‰  ğ‘—ğ‘™, ğ‘ƒğ‘—0 ğ‘—ğ‘™ (
ğ‘Œ
can use a color coding technique to (approximately) evaluate ğ‘ƒ in time ğ‘‘ğ‘‚

, simple evaluation takes time ğ‘‘Î˜
)
1
).

has degree Î˜

log ğ‘‘
(

)

(

(cid:3)

log ğ‘‘

(

). However, we

Let ğ‘ƒ

ğ‘—0 > ğ‘—ğ‘™.

ğ‘Œ
(

)

be a matrix such that for all ğ‘—0 < ğ‘—ğ‘™

ğ‘‘

ğ‘Œ
, its entries are just ğ‘ƒğ‘—0 ğ‘—ğ‘™ (

, ğ‘ƒğ‘—0 ğ‘—ğ‘™
)

]

âˆˆ [

= ğ‘ƒğ‘—0 ğ‘—ğ‘™ for

61

Lemma 7.8. Suppose that the conditions of Theorem 7.1 are satisï¬ed. There exists a probabilistic algorithm
that given ğ‘Œ as input, in time

ğ‘‘ such that

â„ğ‘‘

ğ‘›ğ‘‘

ğ‘‚

1

Ã—

(

(

)

) outputs a matrix Ë†ğ‘ƒ
6 ğ‘œ

ğ‘ƒ

k Ë†ğ‘ƒ

âˆ’

2
F

k

âˆˆ

,

1
)
(

(with respect to the distribution of ğ‘Œ and the randomness of the algoruthm).

ğ‘œ

with probability 1

1
)
(
Proof. Letâ€™s ï¬x ğ‘—, ğ‘—â€² âˆˆ [
colorings. Note that we can compute

âˆ’

ğ‘‘

]

such that ğ‘— < ğ‘—â€² and let ğ‘ :

ğ‘‘

[

] â†’ [

ğ‘™

1
]

+

and ğ‘ :

ğ‘›

[

] â†’ [

ğ‘ğ‘™

]

be ï¬xed

ğ‘Œ
ğ‘ğ‘ğ‘ ğ‘— ğ‘—â€²(

)

= 1
ğœ… Â·

ğ‘™

(

(

ğ‘™

1
1
+
)
!
1
(
)

ğ‘ğ‘™
(
)
ğ‘ğ‘™
!
)

+
ğ‘™
+

ğ‘™

Ã•ğ›¼
âˆˆğ’¢ğ‘— ğ‘—â€²(

ğ‘,ğ‘™

)

1

ğ‘

ğ½ğ›¼)

(

[

=

ğ‘™

[

1
]] Â·

ğ‘

1
[

ğ¼ğ›¼)

(

+

=

ğ‘ğ‘™

[

]] Â·

ğ‘Œğ›¼

in time ğ‘‘ğ‘‚
To do this, we compute a matrix whose rows and columns are indexed by

)2ğ‘‚
1

).

ğ‘ğ‘™

(

(

ğ‘—
âˆˆ [
ğ‘†â€² \

|

ğ‘‘
]
ğ‘†
|

ğ‘™
, ğ‘†
âŠ† [
= 1, ğ‘€

]
âŠ‚

, ğ‘€

ğ‘ğ‘™
]
ğ‘€â€² \

âŠ† [
|

ğ‘€â€²,

ğ‘€

|

, such that an entry

ğ‘—, ğ‘†, ğ‘€
= ğ‘. If the entry is not zero, this is equal to

ğ‘—â€², ğ‘†â€², ğ‘€â€²

,

(cid:1)

(cid:0)

(cid:1)(cid:3)

is not 0 if and only if ğ‘†

ğ‘—, ğ‘†, ğ‘€

(

, where
)
ğ‘†â€²,

âŠ‚

(cid:2)(cid:0)
= ğ‘€â€²

ğ‘

1
[

ğµ

)

(

ğ‘Œğ‘–ğ‘—ğ‘Œğ‘–ğ‘—â€² .

ğ‘€

]

\

ğµ
Ã–ğ‘–
âˆˆ

ğ‘›

Ã•ğµ
ğ‘)
âˆˆ(
ğ‘š1, . . . , ğ‘šğ‘

Now, denote ğ‘€â€² \
compute for all 1 6 ğ‘Ÿ 6 ğ‘

ğ‘€ =

{

, where ğ‘š1 <

}

Â· Â· Â·

< ğ‘šğ‘. To compute the entry we can

=

ğ‘‡

ğ‘Ÿ

[

]

ğ‘›

Ã•ğµğ‘Ÿ
ğ‘Ÿ)
âˆˆ(

ğµğ‘Ÿ

1

ğ‘

[

(

)

=

{

ğ‘š1 . . . , ğ‘šğ‘Ÿ

ğ‘Œğ‘–ğ‘—ğ‘Œğ‘–ğ‘—â€² .

}]

ğµğ‘Ÿ
Ã–ğ‘–
âˆˆ

Note that

ğ‘‡

ğ‘Ÿ

[

1
]

+

=

ğ‘‡

ğ‘Ÿ

ğ‘Œğ‘–ğ‘—ğ‘Œğ‘–ğ‘—â€² ,
]

[

ğ‘–
: ğ‘
ğ‘›
Ã•ğ‘–
)
(
]
âˆˆ[
+
so we can compute the entry of the matrix ğ‘‡
in time ğ‘‚
ğ‘
time ğ‘›ğ‘ğ‘‘ğ‘‚

) = ğ‘‘ğ‘‚

)2ğ‘‚
1

1
).

ğ‘ğ‘™

]

[

(

(

(

1

=ğ‘šğ‘Ÿ

ğ‘›ğ‘

, and all entries can be computed in
)

(

If we then compute the ğ‘™-th power of this matrix, which takes time ğ‘‘ğ‘‚

1
), the entry

(

ğ‘—0,

,

âˆ…

âˆ…

,

,

ğ‘—ğ‘™ ,

ğ‘ğ‘™

ğ‘™

,

[

]

[

]

ğ‘ğ‘™
!
!
1
of the resulting matrix contains ğœ… (
)
(
)
+
ğ‘™
1
ğ‘ğ‘™
ğ‘™
1
+
)
+
(

(

ğ‘™

)

ğ‘Œ
ğ‘™ ğ‘ğ‘ğ‘ ğ‘— ğ‘—â€²(

.
)

(cid:1)

(cid:0)

(cid:1)(cid:3)

Denote by ğœ‡ the distribution of ğ‘Œ. Let ğ‘, ğ‘ be independent random colorings (ğ‘ is sampled from
colors and ğ‘ from uniform distributions over
1
+
]
= ğ‘ƒğ‘— ğ‘—â€²(
. Note that since for any ğ›¼, ğ›¼â€² âˆˆ ğ’¢
ğ‘Œ
,
)
)

(cid:2)(cid:0)
uniform distributions over colorings of
colorings of
ğ”¼ğœ‡ ğ‘Œğ›¼ğ‘Œğ›¼â€² > 0, and

in
]
[
in ğ‘ğ‘™ colors). Thus ğ”¼ğ‘ğ‘ ğ‘ğ‘ğ‘ ğ‘— ğ‘—â€²(
ğ‘Œ
)

ğ‘— ğ‘—â€²(

ğ‘, ğ‘™

ğ‘›

ğ‘‘

[

]

[

ğ‘™

ğ”¼
ğœ‡

ğ•
ğ‘ğ‘

ğ‘ğ‘ğ‘ ğ‘— ğ‘—â€²

6 ğ”¼
ğœ‡

ğ”¼
ğ‘ğ‘

ğ‘2
ğ‘ğ‘ ğ‘— ğ‘—â€²(

ğ‘Œ

)

6 1

ğœ…2  

(
ğ‘™

(

ğ‘™

!
1
(
)
ğ‘™
1
+

ğ‘ğ‘™

!
)
ğ‘ğ‘™

(

)

+
1
)

+

2

ğ‘™ !

Ã•ğ›¼,ğ›¼â€²âˆˆğ’¢

ğ”¼
ğœ‡

ğ‘Œ ğ›¼ğ‘Œğ›¼â€² 6 ğ‘‘5 ğ”¼
ğœ‡

ğ‘ƒ2

ğ‘Œ

,

)

ğ‘— ğ‘—â€²(

since

ğ‘™

ğ‘ğ‘™
1
!
!
(
(
)
)
+
ğ‘™
1
ğ‘ğ‘™
ğ‘™
1
+
)
+

(

)

(

2

ğ‘™

6 ğ‘’4ğ‘ğ‘™ 6 ğ‘‘5.

Hence with probability at least 1

(cid:17)

(cid:16)

ğ‘‘âˆ’

5 (with respect to ğœ‡), ğ•ğ‘ğ‘ ğ‘ğ‘ğ‘ 6 ğ‘‘10 ğ”¼ğœ‡ ğ‘ƒ2

ğ‘Œ

.
)

ğ‘— ğ‘—â€²(

âˆ’

62

Let Ë†ğ‘ƒğ‘— ğ‘—â€²(
ğ‘Œ
)
Thus ğ”¼ğ‘† Ë†ğ‘ƒğ‘— ğ‘—â€²(
ğ‘Œ

= 1
ğ‘,ğ‘
ğ‘†
(
|
= ğ‘ƒğ‘— ğ‘—â€²(
ğ‘Œ
Ã

)

|

)

ğ‘Œ
ğ‘† ğ‘ğ‘ğ‘ ğ‘— ğ‘—â€²(
)âˆˆ
and with probability at least 1

, where ğ‘† is a set of ğ‘‘20 independent random colorings
)

5,

ğ‘‘âˆ’

âˆ’

ğ‘, ğ‘

.
)

(

ğ•
ğ‘Œ
ğ‘† Ë†ğ‘ƒğ‘— ğ‘—â€²(

)

6 1
ğ‘‘10

ğ”¼
ğœ‡

ğ‘ƒ2

ğ‘— ğ‘—â€²(

ğ‘Œ

6 1
ğ‘‘10

.

)

hence with probability at least 1

ğ‘‘âˆ’

3 (with respect to ğ‘†)

âˆ’

k Ë†ğ‘ƒ

âˆ’

ğ‘ƒ

2
F

k

6 1
ğ‘‘3

,

where Ë†ğ‘ƒ is a symmetric matrix that is zero on the diagonal and whose entries for all ğ‘— < ğ‘—â€² are Ë†ğ‘ƒğ‘— ğ‘—â€².
Therefore, with probability 1

ğ‘œ

1
)
(

âˆ’

k Ë†ğ‘ƒ

âˆ’

ğ‘ƒ

2
F

k

6 ğ‘œ

.

1
)
(

Proof of Theorem 7.1. By Lemma 7.5 and Lemma 7.6,

ğ”¼

ğ‘ƒ

k

âˆ’

T
ğ‘£0ğ‘£0

=

2
F

k

ğ• ğ‘ƒğ‘— ğ‘—â€² +

Ã•ğ‘—â‰ ğ‘—â€²

ğ‘‘
Ã•ğ‘—
]
âˆˆ[

ğ‘—

ğ‘£0(

4 6 ğ‘œ
)

1
) +
(

1
ğ‘˜

6 ğ‘œ

.

1
)
(

Hence by Markovâ€™s inequality

ğ‘ƒ

k

âˆ’

T
ğ‘£0ğ‘£0

2
F

k

6 ğ‘œ

1
)
(

with probability 1

ğ‘œ

. By Lemma 7.8 we can compute in time
1
)
(

(

ğ‘›ğ‘‘

)

âˆ’

ğ‘‚

(

1

) a matrix Ë†ğ‘ƒ such that

k Ë†ğ‘ƒ

âˆ’

ğ‘ƒ

2
F

k

6 ğ‘œ

.

1
)
(

Hence

k Ë†ğ‘ƒ

âˆ’

T
ğ‘£0ğ‘£0

k

2 6

Therefore, by Lemma H.3, the top eigenvector

with probability 1

ğ‘œ

.
1
)
(

âˆ’

1

âˆ’ h Ë†

6 ğ‘œ

.

1
)
(

k

2
F

T
ğ‘£0ğ‘£0

âˆ’

k Ë†ğ‘ƒ
ğ‘£ of Ë†ğ‘ƒ satisï¬es
Ë†
2 6 ğ‘œ
ğ‘£, ğ‘£0i

1
)
(

(cid:3)

(cid:3)

Remark 7.9. Note that the proof of Theorem 7.1 also shows that in the case Î©
(
2 6 2âˆ’
1
choost ğ¶âˆ—âˆ— > 1000 and get 1
zero as ğ‘‘
.

2,
/
ğ‘. Hence, if the constant ğ¶âˆ— from the theorem statement is at least 105, we can
1000, but as long as ğ‘› > ğ‘‘Î©
1
), the error doesnâ€™t tend to

ğ‘£, ğ‘£0i

ğ‘£, ğ‘£0i

2 6 2âˆ’

6 ğ‘˜2 6 ğ‘‘

âˆ’ h Ë†

âˆ’ h Ë†

ğ‘‘

)

(

â†’ âˆ

8 Fast Spectral Algorithms for Recovery

One key limitation of Algorithm 5.12 is the reliance on solving large semideï¬nite programs, some-
thing that is often computationally too expensive to do in practice for the large-scale problems that
arise in machine learning. So, inspired by the SoS program used in 5.12, in this section we present
a fast spectral algorithm which recovers the sparse vector ğ‘£0 in time ğ‘‚
. Our algorithm

ğ‘›ğ‘‘ log ğ‘›

63

(cid:0)

(cid:1)

}

âˆˆ {

2, 4, 6

which we call SVD-ğ‘¡, for ğ‘¡
, is a slight modiï¬cation of a fast spectral algorithm presented
in [HSSS16]. Such algorithm recovers a sparse vector planted in a random subspace. The algorithm
was also based on the analysis of a degree-4 Sum-of-Squares algorithm introduced in [BKS14]. We
remark that for ğ‘¡ = 2 Algorithm 8.1 corresponds to the SVD with thresholding algorithm outlined
in Section 1. In non-robust settings, as well as in the adversarial model 6.6, the algorithm achieves
high correlation under conditions similar (up to logarithmic terms) to those of the Sum-of-Squares
algorithm 5.12 (of degree 2, 4 and 6).

Algorithm 8.1 (SVD-ğ‘¡: Sparse Vector Recovery).

Given: Sample matrix ğ‘Œ

â„ğ‘›

ğ‘‘, let ğ‘¦1, . . . , ğ‘¦ğ‘‘

Ã—

âˆˆ

âˆˆ

â„ğ‘› be its columns. Degree ğ‘—

2, 4, 6

.

}

âˆˆ {

Estimate: The sparse vector ğ‘£0.

Operation:

1. Compute the top eigenvector

ğ‘¢ of the matrix
Ë†
ğ´ :=

ğ‘ ğ‘—

ğ‘¦ğ‘– , ğ‘›

(

Ã•ğ‘–
ğ‘‘
]
âˆˆ[
:= 1, ğ‘4(

)

â„, ğ‘2(

ğ‘¥, ğ‘¡

âˆˆ

T

ğ‘¦ğ‘– ğ‘¦ğ‘–

) Â·

:=

ğ‘¥, ğ‘¡

)

ğ‘¥

k

(cid:16)

2

k

âˆ’ (

ğ‘¡

1
)

âˆ’

(cid:17)

ğ‘¥, ğ‘¡

, ğ‘6(

)

:=

â„ğ‘› , ğ‘¡

where for ğ‘¥
2
ğ‘4(
âˆ’
)
2. Compute

ğ‘¥

âˆˆ
ğ‘¡
2
.
1
âˆ’
(
)
ğ‘¢Tğ‘Œ.
ğ‘£ =
(cid:1)
Ë†
Ë†

(cid:0)

3. Threshold the vector

ğ‘£ in the following way (for some ï¬xed ğœ > 0):
Ë†

ğ‘–

âˆ€

ğ‘‘

]

, ğœ‚

ğ‘£
( Ë†

)

âˆˆ [

ğ‘– =

ğ‘£ğ‘– ,
Ë†
0,

(

4. Output the thresholded vector ğœ‚

ğ‘£
( Ë†

.
)

if

ğ‘£ğ‘–
| Ë†

> ğœ
âˆšğ‘˜
|
otherwise

(

}

ğ‘›ğ‘‘

âˆˆ {

ğ‘¦1, ğ‘›

2, 4, 6

, the terms ğ‘šğ‘—

log ğ‘›
(
ğ‘¦ğ‘–, ğ‘§

ğ‘¦ğ‘‘, ğ‘›
Remark 8.2 (Running Time of the Algorithm). For ğ‘—
)
. Correctness of the algorithm will be proved showing that ğ´ has
are computable in time ğ‘‚
)
at least constant spectral gap. This means that we can compute the top eigenvalue with power
iteration using ğ‘‚
matrix-vector multiplications . A matrix multiplication requires computing
ğ‘šğ‘– = ğ‘6(
ğ‘›ğ‘‘
.
)
ğ‘£ can be computed in time ğ‘‚
Then,
. In conclusion the algorithm runs in
Ë†
)
time ğ‘‚
(

ğ‘šğ‘– ğ‘ğ‘–. Both operations take time ğ‘‚

for each ğ‘– and then taking the sum

To get an intuition on the algorithm, consider SVD-6 and the simpler adversarial model ğ‘Œ =
ğ›½ğ‘¢ğ‘£T.That is, the Single Spike Model 6.3 with the noise projected into the

ğ‘‘
âˆˆ[
in time ğ‘‚
Ã

, . . . , ğ‘šğ‘—

ğ‘›ğ‘‘ log ğ‘›

and ğœ‚

ğ‘¦ğ‘– , ğ‘›

ğ‘£
( Ë†

ğ‘›ğ‘‘

.
)

)h

]
(

ğ‘‘

i

)

(

(

)

)

)

(

(

ğ‘–

2 ğ‘¢ğ‘¢T

1
ğ‘¢

ğ‘Š
Id
+
space orthogonal to ğ‘¢.24 Now for ğ‘–
(cid:16)

âˆ’

(cid:17)

k

k

p

24Note that the estimate

ğ‘¢ obtained by SVD-2 is the same returned by the standard SVD.
Ë†

supp

âˆˆ

ğ‘£

{

,

}
2

2

ğ‘¦ğ‘–

ğ‘›

âˆ’

(cid:17)

(cid:20) (cid:16)(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

T

2ğ‘›

ğ‘¦ğ‘– ğ‘¦ğ‘–

(cid:21)

âˆ’

ğ›½3ğ‘›3
ğ‘˜3

,

â‰ˆ

(cid:13)
(cid:13)
(cid:13)
(cid:13)

64

while for ğ‘–

ğ‘‘

] \

âˆˆ [

supp

ğ‘£

,

}

{

ğ”¼

2

ğ‘¦ğ‘–

Indeed, the coeï¬ƒcient ğ‘6(
ğ‘›3, the sum
for ğ‘‘

ğ‘¦ğ‘– , ğ‘›

)

â‰«

on the other hand

ğ‘‘

ğ‘–

âˆˆ[

]\

supp
Ã

{

ğ‘£

}

2

ğ‘›

âˆ’

âˆ’

T

2ğ‘›

ğ‘¦ğ‘– ğ‘¦ğ‘–

= ğ‘‚

.

1
)
(

(cid:21)

(cid:17)

(cid:13)
(cid:13)

(cid:20)(cid:16)(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
has the eï¬€ect of "killing" the expectation for Gaussian vectors. Then
(cid:13)
T will be concentrated around its expectation ğ‘‘, while
ğ‘6(

ğ‘¦ğ‘– , ğ‘›

ğ‘¦ğ‘– ğ‘¦ğ‘–

(cid:13)
(cid:13)
(cid:13)
(cid:13)

)

2

(cid:17)

âˆ’

T

2ğ‘›

ğ‘¦ğ‘– ğ‘¦ğ‘–

(cid:21)

2

ğ‘¦ğ‘–

ğ‘›

âˆ’

(cid:13)
(cid:13)

Ã•ğ‘–
supp
{
âˆˆ

ğ‘£

} (cid:20)(cid:16)(cid:13)
(cid:13)

ğ›½3ğ‘›3
ğ‘˜2

.

â‰ˆ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

the leading eigenvector of ğ´ will be highly correlated with ğ‘¢.

Hence, for ğ›½ & ğ‘˜
ğ‘›

ğ‘‘
ğ‘˜

3
1
/

(cid:0)

(cid:1)

algorithm that works for ğ›½ & ğ‘˜
ğ‘›

We remark that it is an open question how these ideas could be generalize to construct an
ğ‘›ğ‘¡. Here, as a proof of concept, we show that SVD-6
3
1
/

succeed under the planted model in 6.6 when ğ‘‘
. In order to deï¬ne the
adversarial perturbations, we will use the notation introduced for Problem 6.6, we recall that with

ğ‘›3 and ğ›½

and ğ‘‘

â‰«

â‰«

â‰«

1
/

ğ‘˜
ğ‘›

ğ‘‘
ğ‘˜

ğ‘‘
ğ‘˜

(cid:1)

(cid:0)

ğ‘¡

(cid:0)

(cid:1)

high probability ğœ† =

1
(
Theorem 8.3. Consider a matrix of the form,

1
))
(

q

Â±

ğ‘œ

ğ›½ğ‘›
ğ‘˜ .

ğ‘Œ = ğ‘Š

T
ğœ†ğ‘¢ğ‘£

+

ğ‘¢

ğ‘£â€²

+

âˆ’

T

ğ‘¢

ğ‘Š

T

(cid:1)
ğ‘‘, a random unit vector ğ‘¢, a ğ‘˜-sparse vector ğ‘£ with entries in
0, 1
for a Gaussian matrix ğ‘Š
)
(
and a vector ğ‘£â€² as deï¬ned in 6.6. For ğ‘‘ & ğ‘›3 log ğ‘‘ log ğ‘›, ğœ† & âˆšlog ğ‘‘
such that
degree 6 returns a vector ğœ‚

1
}
and ğ‘˜ > ğ‘› log ğ‘›, Algorithm 8.1 with

0,

ğ‘

Â±

âˆ¼

{

Ã—

(cid:0)

ğ‘›

ğœ

ğ‘£
( Ë†

)

(cid:13)
(cid:13)
with probability at least 0.99. Furthermore, for ğ‘‘
ğ‘˜ğœ†6 +

(cid:13)
(cid:13)

ğœ‚

ğ‘£
( Ë†

) âˆ’

ğ‘£

6 ğ‘‚

ğ‘‘
ğ‘˜ğœ†6 +

ğœ

âˆšğ‘˜

Â·

(cid:18)
(cid:19)
ğœ 6 1 and ğ›½ = ğœ†2 ğ‘˜
ğ‘› ,

1

âˆ’

ğœ‚
h
ğœ‚

ğ‘£
( Ë†
ğ‘£
( Ë†

, ğ‘£

)
2

Â· k

2

i
ğ‘£

3
1
/

ğ‘˜
ğ‘›ğ›½

ğ‘‘
ğ‘˜

(cid:19)

(cid:18)

ğœ2

.

!

+

6

2

k

)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

We remark that the second inequality of the theorem follows from the ï¬rst by direct substitution
is close to a unit vector. Comparing this result with Theorem 5.10 we
3
1
/

and using the fact that ğœ‚
see that both SVD-6 8.1 and degree-6 SoS 5.12 need ğ›½ & ğ‘˜
ğ‘›
with the sparse vector.

in order to achieve correlation 0.9

ğ‘£
( Ë†

ğ‘‘
ğ‘˜

)

To prove Theorem 8.3, we will ï¬rst show that the vector

the true vector ğ‘¢. Then, thresholding the vector
we will prove two results. First,

ğ‘¢ computed by the algorithm is close to
Ë†
ğ‘¢Tğ‘Œ we will obtain a vector close to ğ‘£. Concretely,
Ë†

65

 
Lemma 8.4. Consider a matrix of the form,

ğ‘Œ = ğ‘Š

T
ğœ†ğ‘¢ğ‘£

+

ğ‘¢

ğ‘£â€²

+

âˆ’

T

ğ‘¢

ğ‘Š

T

0, 1
for a Gaussian matrix ğ‘Š
)
(
and a vector ğ‘£â€² as deï¬ned in 6.6. Let

ğ‘

âˆ¼

ğ‘›

(cid:1)
ğ‘‘, a random unit vector ğ‘¢, a ğ‘˜-sparse vector ğ‘£ with entries in
Ã—
ğ‘¢
Ë†

â„ğ‘› be the top eigenvector of the matrix

(cid:0)

âˆˆ

0,

1

}

Â±

{

ğ‘‘
Ã•ğ‘–
âˆˆ[
]
Then for ğ‘‘ > ğ¶âˆ—ğ‘›3 log ğ‘‘ log ğ‘›, ğ‘› > 10 log ğ‘‘

ğ‘¦ğ‘– , ğ‘›

ğ‘6(

) Â·

ğ‘¦ğ‘– ğ‘¦ğ‘–

T

.

ğ‘¢

k

ğ‘¢
âˆ’ Ë†

k

6 ğ‘‚

ğ‘‘
ğ‘˜ğœ†6 +

1
ğœ† + p

ğ‘› log ğ‘›
ğœ†âˆšğ‘˜

!

with probability at least 0.999, where ğ¶âˆ— is a universal constants.

Second,

Lemma 8.5. Let
ğœ† & âˆšlog ğ‘‘

ğœ

, then with probability at least 1

âˆ’
exp

k
ğ‘›

(âˆ’

)

âˆ’

ğ‘¢ be a vector such that
Ë†

ğ‘¢
k Ë†

ğ‘¢

6 ğœ€ for some 0 6 ğœ€ 6 1

10 and let

âˆ’
â„ğ‘‘ is the vector with coordinates

ğ‘£
k Ë†

.

ğ‘£

k

ğœ€

(

+

ğœ

)

âˆšğ‘˜ ,

where ğœ‚

ğ‘£
( Ë†

) âˆˆ

ğ‘– =

ğœ‚

ğ‘£
( Ë†

)

ğ‘£ğ‘– ,
Ë†
0,

(

> ğœ

if

ğ‘£ğ‘–
| Ë†

|
otherwise.

It is easy to see how the two results immediately imply Theorem 8.3.
Lemma 8.4 is proved in Section 8.1, in Section 8.2 we prove Lemma 8.5.

8.1 Algorithm recovers u with high probability

The goal of this Section is to prove Lemma 8.4.

ğ‘£ = 1
Ë†

ğœ†âˆšğ‘˜ Ë†

ğ‘¢Tğ‘Œ. If

(8.1)

By rotational symmetry of the Gaussian distribution, we may assume without loss of generality
ğ‘£ğ‘£T.

that ğ‘¢ = ğ‘’1. Now, for vectors ğ‘£, ğ‘§

â„ğ‘›, deï¬ne ğ‘€

ğ‘£, ğ‘§

:=

ğ‘›

ğ‘›

ğ‘£

ğ‘§

2

2

âˆˆ

(

)

k

+

k

âˆ’ (

âˆ’

ğ‘–

)

(

(cid:20)(cid:16)

ğ‘¢, ğ‘¤

ğ‘£â€²(

(cid:21)
Recall that the adversarial vector ğ‘£â€² is, by construction, orthogonal to the sparse vector ğ‘£. Hence
our strategy will be the following, ï¬rst we bound the contribution of terms of the form ğ‘€
ğ‘¤, ğ›¾ğ‘’1)
and ğ‘€
. Note that the ï¬rst type of terms arise due to the noise, the second ones due
)
to the adversarial distribution. Then, lower bounding ğ‘€
, we will be able to show that
)
ğ‘¢, ğ‘¤ğ‘–

ğ‘¤ğ‘–, ğ›¾ğ‘– ğ‘’1)(cid:13)
(cid:13)
ğ‘–
(cid:13)
(cid:13)
]
terms will play a minor role.
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
results act as building blocks for the bound, which is then shown in Lemma 8.10.

supp
Ã
First we bound the contribution of the Gaussian part. We will use Bernstein Inequality, the next

ğœ†ğ‘¢, ğ‘¤
(
ğ‘£â€²(

with high probability. Cross-

â‰« (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ğœ†ğ‘¢, ğ‘¤
(

supp
Ã

)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ğ‘‘
âˆˆ[
Ã

ğ‘€

ğ‘€

ğ‘€

âˆˆ[

]\

(cid:17)

(

(

(

)

âˆˆ

ğ‘–

ğ‘£

ğ‘‘

ğ‘£

}

{

}

{

ğ‘–

ğ‘–

1
)

2
(

âˆ’

1
)

âˆ’

66

 
Fact 8.6. Let ğ‘¥

ğ‘

0, Idğ‘›
(

,
)

âˆ¼

Proof.

ğ”¼

ğ‘¥

k

k

2ğ‘¥2
ğ‘–

=

ğ”¼

ğ‘¥

k

k

4ğ‘¥2
ğ‘–

=

ğ”¼

ğ”¼

ğ‘¥

ğ‘¥

k

k

k

k

2ğ‘¥2
ğ‘–
4ğ‘¥2
ğ‘–

2

= ğ‘›
+
= ğ‘›2

6ğ‘›

8.

+

+

ğ”¼ ğ‘¥2

ğ‘– ğ‘¥2

ğ”¼ ğ‘¥4
ğ‘–

= ğ‘›

2

+

ğ‘— +

,ğ‘—â‰ ğ‘–

ğ‘›
Ã•ğ‘—
]
âˆˆ[

ğ‘›
Ã•ğ‘—,ğ‘˜
âˆˆ[
ğ‘—â‰ ğ‘–,ğ‘˜â‰ ğ‘–,ğ‘—â‰ ğ‘˜

]

ğ”¼ ğ‘¥2

ğ‘– ğ‘¥2

ğ‘— ğ‘¥2

ğ‘˜ +

ğ‘›
Ã•ğ‘—,ğ‘˜
âˆˆ[
]
ğ‘—=ğ‘˜â‰ ğ‘–

ğ”¼ ğ‘¥4

ğ‘— ğ‘¥2

ğ‘– +

ğ”¼ ğ‘¥4

ğ‘– ğ‘¥2

ğ‘— +

ğ”¼ ğ‘¥6
ğ‘–

2

ğ‘›
Ã•ğ‘—,ğ‘˜
âˆˆ[
]
ğ‘—â‰ ğ‘˜=ğ‘–

ğ‘›

=
ğ‘›
(
= ğ‘›2

1
)(
6ğ‘›

âˆ’

+

+

âˆ’
8.

2
) +

ğ‘›

3
(

1
) +

6
(

ğ‘›

1
) +

âˆ’

âˆ’

15

We bound the spectral norm of the expectation of the terms ğ‘€

ğ‘¤, ğ›¾ğ‘’1)
.

(

Lemma 8.7. Let ğ‘¤

ğ‘

0, Idğ‘›
(

âˆ’

âˆ¼

T
ğ‘’1ğ‘’1

, ğ›¾
)
ğ”¼ ğ‘€

k

â„. Then

âˆˆ
ğ‘¤, ğ›¾ğ‘’1)k

(

= ğ›¾4

8ğ›¾2

8.

+

+

Proof. We need only to look into diagonal entries. By construction, ğ”¼

2 = ğ‘›

ğ‘¤

k

k

âˆ’

1 =: ğ‘š,

ğ‘¤

k

ğ›¾ğ‘’1k

+

2

ğ‘š

âˆ’

ğ”¼

(cid:20)(cid:16)

2

(cid:17)

âˆ’

2ğ‘š

ğ‘¤2
ğ‘–

= ğ”¼

(cid:21)

= ğ”¼

Applying Fact 8.6,

(cid:20)(cid:16)
k

(cid:16)

2

k

4

ğ‘¤

k

ğ‘¤

k

+

ğ›¾2

ğ‘š

âˆ’

+

ğ›¾4

+

ğ‘š2

2

(cid:17)

+

ğ‘¤2
ğ‘–

(cid:21)
2ğ›¾2

2ğ‘š

âˆ’

ğ‘¤

2

k

k

2ğ‘š

ğ‘¤

k

âˆ’

2

k

âˆ’

2ğ›¾2ğ‘š

âˆ’

2ğ‘š

ğ‘¤2
ğ‘–

(cid:17)

2

ğ”¼

ğ‘¤

k
(cid:20)(cid:16)
= ğ‘š2
= ğ›¾4

+

+

+

ğ›¾ğ‘’1k
6ğ‘š
8ğ›¾2

+

+

2

ğ‘š

(cid:17)
ğ›¾4

âˆ’

8

8.

+

2ğ‘š

ğ‘¤2
ğ‘–

ğ‘š2

(cid:21)

+

2ğ›¾2ğ‘š

4ğ›¾2

+

âˆ’

2ğ‘š2

4ğ‘š

âˆ’

âˆ’

2ğ›¾2ğ‘š

2ğ‘š

âˆ’

âˆ’

+

(cid:3)

(cid:3)

ğ‘€

The second property we need is a high probability bound on the maximum value of
ğ‘¤, ğ›¾ğ‘’1)k
k
Lemma 8.8. Let ğ‘¤

â„. Then for any ğ‘ > 1, with probability at least 1

ğ‘,

ğ‘

(

.

T
ğ‘’1ğ‘’1

2ğ‘’âˆ’

âˆ’

0, Idğ‘›
(

âˆ’

âˆ¼

, ğ›¾
âˆˆ
)
6 ğ¶
ğ‘¤, ğ›¾ğ‘’1)k

where ğ¶ is a universal constant.

ğ‘€

(

k

ğ›¾4ğ‘›

+

(cid:0)

ğ‘› max

ğ‘› ğ‘, ğ‘2

,

(cid:8)

(cid:9)(cid:1)

67

Proof. For simplicity of the notation let ğ‘š = ğ‘›

1, and let ğ‘ = max

ğ‘, âˆšğ‘š ğ‘

. By Fact G.4,

âˆ’

â„™

2 âˆ‰

ğ‘š

ğ‘¤

k

âˆ’

10ğ‘, ğ‘š

10ğ‘

+

k

(cid:8)
ğ‘.
6 2ğ‘’âˆ’

(cid:9)

(cid:16)
Hence with probability at least 1

(cid:2)
ğ‘,

2ğ‘’âˆ’

âˆ’

2

2

ğ‘š

(cid:17)

âˆ’

âˆ’

ğ‘¤

k

ğ›¾ğ‘’1k

+

(cid:20) (cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:3) (cid:17)

2

ğ›¾ğ‘’1k

âˆ’

ğ‘š

2ğ‘š

=

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:20)(cid:16)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
h(cid:0)
6ğ¶

6

k

(cid:20)(cid:16)
ğ›¾2

ğ‘¤

k

ğ‘¤

+

2

k

+

10ğ‘

+

ğ›¾4

+

ğ‘2

(cid:1)

2ğ‘š

âˆ’

2

(cid:17)
2

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

ğ›¾2

2

âˆ’

âˆ’

2ğ‘š

âˆ’

ğ‘š

(cid:17)
2ğ‘š

i

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:0)
for some universal constant ğ¶ > 0. The result follows.

(cid:1)

(cid:3)

And ï¬nally, the last ingredient we need for our Bernstein inequality is a bound on the variance.

Lemma 8.9. Let ğ‘¤

ğ‘

âˆ¼

0, Idğ‘›
(

T
ğ‘’1ğ‘’1

âˆ’
ğ‘¤, ğ›¾ğ‘’1)

2

, ğ›¾
)
âˆˆ
6 ğ¶

ğ”¼ ğ‘€

(

â„. Then

ğ›¾8ğ‘›

+

ğ‘› max

log4 ğ‘›ğ›¾, ğ‘›2 log ğ‘›ğ›¾

,

(cid:8)

(cid:9)(cid:17)

for a universal constant ğ¶ > 0.

(cid:13)
(cid:13)

(cid:16)

(cid:13)
(cid:13)

Proof. For simplicity of the notation let ğ‘š = ğ‘›
2 âˆ‰
the event
Then,

10ğ‘, ğ‘š

10ğ‘

ğ‘š

ğ‘¤

+

âˆ’

=

â„°

k

k

(cid:8)

(cid:2)

(cid:3) (cid:9)

1. Fix ğ‘ = 50 log ğ‘šğ›¾ and ğ‘ = max

âˆ’
, which happens with probability at least 1
(cid:9)

(cid:8)

ğ‘, âˆšğ‘š ğ‘

. Deï¬ne
ğ‘.
2ğ‘’âˆ’

âˆ’

ğ‘¤

k

ğ›¾ğ‘’1k

+

2

ğ‘š

âˆ’

(cid:20)(cid:16)

2

2

(cid:17)

2ğ‘š

âˆ’

(cid:21)

6 ğ¶

ğ›¾8

ğ‘4

.

+

(cid:0)

(cid:1)

By triangle inequality,

ğ”¼ ğ‘€

2

ğ‘¤, ğ›¾ğ‘’1)

(

(cid:13)
(cid:13)
We bound the ï¬rst term,

=

6

(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

ğ”¼

ğ”¼

â„™

â„™

(â„°)

(â„°)

ğ‘€

ğ‘€

(

(

(cid:2)

(cid:2)

2

2

ğ‘¤, ğ›¾ğ‘’1)
ğ‘¤, ğ›¾ğ‘’1)

â„°

â„°

(cid:12)
(cid:12)
(cid:12)
(cid:12)

â„™

+

+

(cid:3)
(cid:3)(cid:13)
(cid:13)

Â¯â„°
â„™
(cid:0)
(cid:13)
(cid:13)

(cid:0)

ğ”¼

(cid:1)
Â¯â„°

2

ğ‘€
ğ”¼
(cid:2)

ğ‘¤, ğ›¾ğ‘’1)
(
ğ‘€
(

Â¯â„°
2
ğ‘¤, ğ›¾ğ‘’1)
(cid:12)
(cid:12)

(cid:1)

(cid:2)

(cid:3)(cid:13)
Â¯â„°
(cid:13)

.

(cid:3)(cid:13)
(cid:13)

(cid:12)
(cid:12)

â„™

ğ”¼

ğ‘€

2

ğ‘¤, ğ›¾ğ‘’1)

(

(â„°)

(cid:13)
(cid:13)

(cid:2)

â„°

(cid:12)
(cid:12)

(cid:3)(cid:13)
(cid:13)

6
ğ‘‚
6ğ‘‚
(cid:13)
(cid:13)
6ğ‘‚
6ğ‘‚

(cid:0)

(cid:0)

ğ›¾8
ğ›¾8ğ‘š
(cid:0)(cid:0)
ğ›¾8ğ‘š
ğ›¾8ğ‘š

+

+

+

+

ğ‘4
ğ‘š
ğ‘šğ‘4
(cid:1)
ğ‘šğ‘4
ğ‘šğ‘4

ğ”¼

ğ”¼

(cid:1)
ğ”¼
(cid:1)(cid:13)
(cid:13)
.
(cid:1)(cid:13)
(cid:13)

(cid:2)

(cid:2)

T
ğ‘¤ğ‘¤
T
ğ‘¤ğ‘¤
(cid:2)
T
ğ‘¤ğ‘¤

â„°

â„°

(cid:3)(cid:13)
(cid:13)
(cid:3)(cid:13)
(cid:13)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:3)(cid:13)
(cid:13)

To bound the second term, observe that
For ğ‘–

â„•, deï¬ne the event

âˆˆ

ğ‘€

(cid:0)
2
ğ‘¤, ğ›¾ğ‘’1)

(

6 ğ‘‚

(cid:1)
ğ‘š12

(cid:16)

(cid:13)
(cid:13)

ğ›¾12

12

ğ‘¤

k

+ k

+

(cid:17)

(cid:13)
(cid:13)

for any ğ›¾, ğ‘¤, ğ‘š > 1.

ğ‘ğ‘– :=

â„°

ğ‘¤

2

k

âˆˆ

k

(cid:26)

ğ‘š

âˆ’

(cid:20)

2

ğ‘š ğ‘

q

ğ‘–

Â· (

1
) âˆ’

2ğ‘

ğ‘–

(

1
)

+

+

, ğ‘š

+

2

ğ‘š ğ‘

q

ğ‘–

Â· (

1
) +

2ğ‘

ğ‘–

(

1
)

+

+

(cid:21) (cid:27)

68

By construction â„™

ğ‘ğ‘–)

(â„°

6 2 max

ğ‘’âˆ’

â„™

Â¯â„°

ğ”¼

ğ‘€

(cid:0)

(cid:1)

(cid:2)

(cid:13)
(cid:13)

concluding the proof.

ğ‘¤

2 âˆ‰

ğ‘š

âˆ©

k

k

n
ğ‘ğ‘–/
4

ğ‘2
ğ‘–
4ğ‘š , ğ‘’âˆ’

h
and Â¯â„° âŠ†

(cid:27)

Â¯â„°

(cid:12)
(cid:12)

(cid:3)(cid:13)
(cid:13)

6

â„™

â„•
Ã•ğ‘–
(cid:13)
âˆˆ
6ğ‘‚
(cid:13)
1
)
(

,

(cid:0)

(cid:26)
ğ‘¤, ğ›¾ğ‘’1)

2

(

2

ğ‘š ğ‘ğ‘–

2ğ‘ğ‘–, ğ‘š

âˆ’

+

2

ğ‘š ğ‘ğ‘–

+

âˆ’

2ğ‘ğ‘–

.

p

p
ğ‘ğ‘–. By choice of ğ‘, it follows that

io

â„•â„°
ğ‘–
âˆˆ
Ã
ğ‘ğ‘–

ğ”¼

â„°

ğ‘€

(

2

ğ‘¤, ğ›¾ğ‘’1)

ğ‘ğ‘–

â„°

(cid:1)

(cid:2)

(cid:12)
(cid:12)

(cid:3)(cid:13)
(cid:13)

(cid:3)

We can now apply Bernstein Inequality G.7:

ğ‘

0, Idğ‘›
(
,

âˆ’

T
ğ‘’1ğ‘’1

, let
)

ğ›¾1|

|

, . . . ,

ğ›¾ğ‘™

|

|

6 ğ›¾

â„. Then for ğ‘™ > ğ¶âˆ— Â·

âˆˆ

âˆ¼
ğ›¾ğ‘›

Lemma 8.10. Let ğ‘¤1, . . . , ğ‘¤ğ‘™
ğ‘™
max

, ğ‘› log3

ğ‘›3 log

ğ›¾ğ‘›

ğ‘™

(

+

)

(

+

(cid:8)

with probability at least 1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2ğ‘™âˆ’
(cid:13)

ğ‘™
Ã•ğ‘–
]
âˆˆ[
10

ğ‘€

âˆ’

âˆ’

Proof. By triangle inequality,

)

(

(cid:9)
ğ‘¤ğ‘–, ğ›¾ğ‘– ğ‘’1)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

6 ğ‘™

ğ›¾4

(

8
) +

+

ğ¶âˆ—ğ›¾4

ğ‘™ğ‘› log ğ‘›

p

ğ‘›âˆ’

10, where ğ¶âˆ— is a universal constant.

ğ‘™
Ã•ğ‘–
]
âˆˆ[
By Lemma 8.7,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ğ‘€

ğ”¼ ğ‘€

6

(

ğ‘¤ğ‘–, ğ›¾ğ‘– ğ‘’1)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ğ‘™
Ã•ğ‘–
]
âˆˆ[

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(

ğ‘¤ğ‘– , ğ›¾ğ‘– ğ‘’1)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ğ‘™
Ã•ğ‘–
]
âˆˆ[

ğ‘€

ğ‘¤ğ‘– , ğ›¾ğ‘– ğ‘’1) âˆ’

(

ğ”¼ ğ‘€

ğ‘™
Ã•ğ‘–
]
âˆˆ[

(

.

ğ‘¤ğ‘– , ğ›¾ğ‘– ğ‘’1)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ğ”¼ ğ‘€

(

ğ‘™
Ã•ğ‘–
]
âˆˆ[

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
and ğ‘ := max
(cid:13)

ğ‘™ğ›¾4.

6 8ğ‘™

8ğ‘™ğ›¾2

+

+

ğ‘¤ğ‘– , ğ›¾ğ‘– ğ‘’1)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
10ğ‘, ğ‘š
. Deï¬ne the event
(cid:13)
+
k
ğ‘. By Lemma 8.8, with probability at least 1
(cid:8)
âˆ’

2ğ‘’âˆ’
(cid:9)

2 âˆ‰

ğ‘š

ğ‘¤

âˆ’

âˆ’

=

â„°

k

(cid:2)

10ğ‘
2ğ‘™âˆ’

,
10,
(cid:3) (cid:9)

ğ‘™

Let ğ‘ = 100 log
ğ‘šğ›¾
which happens with probability at least 1
for each ğ‘–

+

(cid:8)

(

)

ğ‘™

,

ğ‘, âˆšğ‘š ğ‘

âˆˆ [

ğ‘€

(

k

]
ğ‘¤ğ‘– , ğ›¾ğ‘– ğ‘’1) âˆ’

ğ”¼ ğ‘€

ğ‘¤ğ‘–, ğ›¾ğ‘– ğ‘’1)k

(

6 ğ¶

8

8ğ›¾2

ğ›¾4

+

+

+

ğ›¾4ğ‘›

+

ğ‘› max

ğ‘› log ğ‘™, log2 ğ‘™

,

for a constant ğ¶ > 0. Hence, by Lemma 8.9, applying Bernstein Inequality G.7

(cid:8)

(cid:16)

ğ‘€

ğ‘¤ğ‘–, ğ›¾ğ‘– ğ‘’1) âˆ’

(

(

ğ”¼ ğ‘€

(

ğ‘™
Ã•ğ‘–
]
âˆˆ[

6 ğ¶â€²

ğ‘¡

Â·

ğ‘™ğ‘› log ğ‘›

ğ›¾4

Â·

p

ğ‘¤ğ‘– , ğ›¾ğ‘– ğ‘’1))(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

with probability at least 1

10

2ğ‘™âˆ’

âˆ’

âˆ’

ğ‘’âˆ’(

ğ‘¡

1
)
âˆ’

log ğ‘›, where ğ¶â€² is a universal constant.

The next lemma will be used to bound the contribution of the adversarial vector ğ‘£â€².

69

(cid:9)(cid:17)

(cid:3)

Lemma 8.11. Let
1
1
)
âˆ’

ğ‘’âˆ’(

log ğ‘›

|
2ğ‘™âˆ’

ğ‘¡

ğ‘1|

10

âˆ’

âˆ’

, . . . ,

ğ‘ğ‘™

|

|

6 ğ‘

âˆˆ

â„. Let ğ‘¤

ğ‘

0, Idğ‘›
(

âˆ’

âˆ¼

T
ğ‘’1ğ‘’1

. Then, with probability at least
)

ğ‘€

ğ‘ğ‘– ğ‘’1, ğ‘¤

(

ğ‘™
Ã•ğ‘–
]
âˆˆ[
where ğ‘¡ > 1 and ğ¶ > 0 is a universal constant.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

6 ğ¶

ğ‘¡

ğ‘™ log ğ‘›ğ‘2

ğ‘4

max

Â·

Â·

+

p

)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1 and ğ‘ = 10 log ğ‘™ and ğ‘ := max

(cid:16)

n

log ğ‘™,

ğ‘› log ğ‘™

p

o(cid:17)

ğ‘, âˆšğ‘š ğ‘

. By Fact G.4,

Proof. For simplicity let ğ‘š = ğ‘›

âˆ’

Hence, as in Lemma 8.8

â„™

ğ‘¤

k

k

(cid:16)

2 âˆ‰

ğ‘š

10ğ‘, ğ‘š

10ğ‘

+

âˆ’

(cid:8)
6 2ğ‘’âˆ’

ğ‘.

(cid:9)

(cid:3) (cid:17)

This implies,

ğ‘¤

k

ğ‘ğ‘– ğ‘’1k

+

2

ğ‘š

âˆ’

(cid:20)(cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2ğ‘š

6 ğ‘‚

ğ‘4
ğ‘– +

(cid:0)

ğ‘2

6 ğ‘‚

ğ‘4

(cid:1)

(cid:0)

ğ‘2

.

+

(cid:1)

(cid:1)
We have everything we need to apply Hoeï¬€ding Inequality G.8

(cid:0)

(cid:13)
(cid:13)

ğ‘ğ‘– ğ‘’1, ğ‘¤

ğ‘€

(

2
)

ğ‘12

+

ğ‘4ğ‘4

.

(cid:2)

2

âˆ’

(cid:17)

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
6 ğ‘‚

(cid:13)
(cid:13)

ğ‘€

ğ‘ğ‘– ğ‘’1, ğ‘¤

)(cid:13)
(cid:13)
(cid:13)
(cid:13)
for ğ‘ > 1 and a universal constant ğ¶.
(cid:13)
(cid:13)

ğ‘™
Ã•ğ‘–
]
âˆˆ[

(

â„™

(cid:13)
(cid:13)
(cid:13)
Â©
(cid:13)
(cid:13)
(cid:13)
Â«

> ğ¶

ğ‘¡

Â·

Â·

ğ‘™ log ğ‘›

ğ‘6

+

ğ‘2ğ‘2

6 ğ‘’âˆ’(

ğ‘¡

1
)
âˆ’

log ğ‘› ,

p

(cid:0)

(cid:1)Âª
Â®
Â¬

(cid:3)

The last intermediate result, is a high probability lower bound on the spectral norm of the
, that is, the matrix corresponding to the sum of the columns that contain
)

ğœ†ğ‘’1, ğ‘¤
(

ğ‘€

matrix

âˆˆ
the spike.

ğ‘–

ğ‘£

{

}

supp
Ã

Lemma 8.12. Let ğœ1, . . . , ğœğ‘™
1
}
auniversal constant ğ¶ > 0, suppose ğ‘™ > ğ¶

âˆˆ {âˆ’

1,

+

and ğ‘¤1, . . . , ğ‘¤ğ‘™
ğ‘¡ log ğ‘›
max

Â·

Â·

ğ‘

0, Idğ‘›
âˆ¼
âˆ’
(
ğ‘›3 log ğ‘™, log2 ğ‘™

T
ğ‘’1ğ‘’1
. Let ğ›¾
)
. Then

âˆˆ

â„, for ğ‘¡ > 1 and

ğœğ‘–ğ›¾ğ‘’1, ğ‘¤ğ‘–

ğ‘€

(

ğ‘™
Ã•ğ‘–
]
âˆˆ[

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2ğ‘™âˆ’
(cid:13)

10.

(cid:9)

>

ğ‘™ğ›¾6
2

,

(cid:8)

)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

with probability at least 1

ğ‘¡ log ğ‘›

2ğ‘’âˆ’

âˆ’

âˆ’

Proof. Now,

ğ‘™
Ã•ğ‘–
]
âˆˆ[

ğœğ‘–ğ›¾ğ‘’1, ğ‘¤ğ‘–

ğ‘€

(

=

)

=

Â©

Â«
ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

ğ‘¤ğ‘–

k

ğœğ‘– ğ›¾ğ‘’1k

+

2

ğ‘š

âˆ’

2

(cid:17)

2ğ‘š

âˆ’

ğ‘™
Ã•ğ‘–
âˆˆ[

] (cid:20) (cid:16)

(cid:21)

T
ğ›¾2ğ‘’1ğ‘’1

Âª
Â®
ğ‘¤ğ‘–
Â¬
k

ğ›¾4ğ‘™

+

ğ‘š2ğ‘™

2ğ‘šğ‘™

âˆ’

âˆ’

2ğ›¾2ğ‘šğ‘™

+

Â©

Â«

70

ğ‘™
Ã•ğ‘–
]
âˆˆ[

4

k

âˆ’

2ğ‘š

ğ‘¤ğ‘–

k

2

k

+

2ğ›¾2

ğ‘¤ğ‘–

k

2

k

T
ğ›¾2ğ‘’1ğ‘’1

.

ï£¹
ï£º
Âª
ï£º
Â®
ï£º
ï£º
Â¬
ï£»

 
 
 
We bound the terms in the parenthesis. Recall that by construction

ğ”¼

ğ‘š2ğ‘™

2ğ‘šğ‘™

âˆ’

âˆ’

2ğ›¾2ğ‘šğ‘™

+

ğ‘™
Ã•ğ‘–
]
âˆˆ[

Â©

ğ‘¤ğ‘–

k

4

k

âˆ’

2ğ‘š

ğ‘¤ğ‘–

k

2

k

+

2ğ›¾2

ğ‘¤ğ‘–

k

2

k

For ğ‘ := 10 log ğ‘™ and ğ‘ = max

âˆšğ‘š ğ‘, ğ‘

, we can condition on the event,

Â«

(cid:12)
(cid:12)
which happens with probability at least 1
(cid:12)

2ğ‘’âˆ’

(cid:8)
:=

â„°

ğ‘–

(cid:9)
âˆˆ [

ğ‘™

]

2

ğ‘¤

k

k

âˆ€
(cid:8)

ğ‘š

10ğ‘, ğ‘š

10ğ‘

,

âˆ’

âˆˆ
ğ‘. Then, by Hoeï¬€ding Inequality G.8,

(cid:3) (cid:9)

+

(cid:2)

= 0.

ï£¹
ï£º
Âª
ï£º
Â®
ï£º
ï£º
Â¬
ï£»

ğ‘š2ğ‘™

2ğ‘šğ‘™

âˆ’

âˆ’

2ğ›¾2ğ‘šğ‘™

âˆ’

+

ğ‘¤ğ‘–

k

4

k

âˆ’

2ğ‘š

ğ‘¤ğ‘–

k

2

k

+

2ğ›¾2

ğ‘¤ğ‘–

k

> ğ¶

ğ‘¡

Â·

Â·

âˆšğ‘™

ğ‘2

ğ‘šğ‘

+

+

ğ‘™
Ã•ğ‘–
]
âˆˆ[

Â©

Â«

2

k

(cid:12)
(cid:12)
ï£¹
(cid:12)
ï£º
Âª
(cid:12)
ï£º
Â®
ğ›¾2ğ‘
(cid:12)
ï£º
(cid:12)
ï£º
Â¬
ï£»
(cid:1)

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

ğ‘™
Ã•ğ‘–
]
âˆˆ[

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

with probability at most 2ğ‘’âˆ’
follows that

ğ‘¡, for ğ‘¡ > 1 and a universal constant ğ¶. By assumption on ğ›¾, ğ‘™ and ğ‘›, it

(cid:0)

ğ‘€

ğœğ‘–ğ›¾ğ‘’1, ğ‘¤ğ‘–

T
ğ›¾2ğ‘’1ğ‘’1

>

T
ğ‘’ğ‘– ğ‘’1

=

ğ‘™ğ›¾6
2

ğ‘™ğ›¾6
2

.

(

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
We are now ready to prove the main result of the section. Combining Lemma 8.13 with Lemma

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ğ‘™
Ã•ğ‘–
]
âˆˆ[

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:3)

)

8.12 and an application of Lemma H.2 we immediately get Lemma 8.4.

Lemma 8.13. Let ğ‘Œ be deï¬ned as in Theorem 8.3, let ğ‘‘ > ğ¶
ğ‘› > log ğ‘‘ and large enough constants ğ¶ , ğ¶âˆ—, with probability at least 0.999,

Â·

ğ‘›3 log ğ‘‘ log ğ‘› > 100. For ğ‘˜ğœ†6 > ğ¶âˆ—ğ‘‘,

2

ğ‘¦ğ‘–

] (cid:20)(cid:16)(cid:13)
(cid:13)
where ğ‘€ is a matrix such that

ğ‘‘
Ã•ğ‘–
âˆˆ[

(cid:13)
(cid:13)

ğ‘š

âˆ’

2

(cid:17)

âˆ’

2ğ‘š

ğ‘¦ğ‘– ğ‘¦ğ‘–

T =

(cid:21)

Ã•ğ‘–
supp
{
âˆˆ

ğ‘£

}

ğ‘€

ğ‘£

ğ‘–

ğœ†ğ‘’1, ğ‘¤
)

(

(

) +

ğ‘€ ,

ğ‘€

k

k

6 ğ‘‚

ğ‘‘

ğ‘˜ğœ†5

+

+

ğ‘˜ğ‘› log ğ‘›ğœ†5

.

(cid:16)
1. Recall the notation used in the algorithm with ğ‘6(

p

(cid:17)

Proof. Let ğ‘š = ğ‘›

âˆ’

Then we can rewrite the matrix ğ´ computed by SVD-6 as,

ğ‘¦ğ‘– , ğ‘›

=

)

ğ‘š

âˆ’

2

(cid:17)

âˆ’

2ğ‘š

.

(cid:21)

2

ğ‘¦ğ‘–

(cid:20)(cid:16)(cid:13)
(cid:13)

(cid:13)
(cid:13)

2

ğ‘¦ğ‘–

ğ‘‘
Ã•ğ‘–
âˆˆ[

] (cid:20) (cid:16)(cid:13)
(cid:13)

(cid:13)
(cid:13)

ğ‘š

âˆ’

2

(cid:17)

âˆ’

2ğ‘š

ğ‘¦ğ‘– ğ‘¦ğ‘–

T =

(cid:21)

Ã•ğ‘–
supp
{
âˆˆ

ğ‘£

}

ğ‘€

ğ‘£

ğ‘–

ğœ†ğ‘’1, ğ‘¤
)

(

(

) +

ğ‘¤ğ‘– , ğ‘£

ğ‘€

(

ğ‘–

ğœ†ğ‘’1)
)

(

ğ‘£

}

ğ‘¤ğ‘–, ğ‘£â€²

ğ‘€

(

ğ‘–

(

)

Ã•ğ‘–
supp
{
âˆˆ
ğ‘’1) +

ğ‘€

ğ‘£â€²

ğ‘–

(

)

(

ğ‘’1, ğ‘¤

)

ğ‘£

ğ‘–

ğœ†ğ‘’1, ğ‘›
)

(

)

+

(cid:0)

ğ‘£

}

Ã•ğ‘–
ğ‘‘
supp
{
âˆˆ[
]\
T
ğ‘¤ğ‘– ğ‘’1
ğ‘–
ğ‘£

(

)

ğ‘£

ğ‘–

(

)

+

T

ğ‘’1ğ‘¤ğ‘–

(cid:1)

+

+

ğ‘‘
Ã•ğ‘–
supp
]\
âˆˆ[

{

ğ‘£

}
ğ‘¤ğ‘–

ğ‘6(

Ã•ğ‘–
supp
{
âˆˆ

ğ‘£

}

71

 
 
We ï¬rst bound the cross-terms,

+

ğ‘‘
Ã•ğ‘–
supp
]\
âˆˆ[

{

ğ‘£

}

ğ‘¤ğ‘–

ğ‘6(

ğ‘£â€²

ğ‘–

ğ‘’1)

)

(

ğ‘£â€²

ğ‘–

(

)

+

T
ğ‘¤ğ‘– ğ‘’1

T

ğ‘’1ğ‘¤ğ‘–

.

ğ‘£â€²

ğ‘–

(

)

+

(cid:0)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
6

ğ‘¤ğ‘–

ğ‘6(

ğ‘£â€²

ğ‘–

ğ‘’1)

)

(

ğ‘£â€²

ğ‘–

(

)

+

ğ‘£

{

}

ğ‘‘
Ã•ğ‘–
supp
]\
âˆˆ[

T
ğ‘¤ğ‘– ğ‘’1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
T
2ğ‘’1ğ‘’1
)

ğ‘¤ğ‘–

ğ‘6(

ğ‘£â€²

ğ‘–

ğ‘’1)

)

(

ğ‘£â€²

ğ‘–

(

+

ğ‘¤ğ‘–

ğ‘6(

ğ‘£â€²

ğ‘–

ğ‘’1)

)

(

ğ‘£â€²

+

ğ‘–

T
2ğ‘’1ğ‘’1
)

(

ğ‘‘
Ã•ğ‘–
supp
]\
âˆˆ[

{

ğ‘£

}

ğ‘‘
Ã•ğ‘–
supp
]\
âˆˆ[

{

ğ‘£

}

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

6

And

ğ‘¤ğ‘–

ğ‘6(

+

ğ‘£â€²

ğ‘–

ğ‘’1)

)

(

ğ‘£â€²

ğ‘–

2ğ‘¤ğ‘–ğ‘¤ğ‘–
)

(

T

ğ‘¤ğ‘–

ğ‘6(

+

ğ‘£â€²

ğ‘–

ğ‘’1)

)

(

ğ‘£â€²

ğ‘–

2ğ‘¤ğ‘–ğ‘¤ğ‘–
)

(

T

ğ‘‘
Ã•ğ‘–
supp
]\
âˆˆ[

{

ğ‘£

}

ğ‘‘
Ã•ğ‘–
supp
]\
âˆˆ[

{

ğ‘£

}

2
1
/

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
+ (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ğ‘¤ğ‘–

ğ‘6(

ğ‘£

ğ‘–

ğœ†ğ‘’1, ğ‘›
)

(

)

ğ‘£

(

ğ‘–

)

+

ğ‘£

}

Ã•ğ‘–
supp
{
âˆˆ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
6

T
ğ‘¤ğ‘– ğ‘’1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
T
2ğ‘’1ğ‘’1
)

2
1
/

ğ‘¤ğ‘–

ğ‘6(

ğ‘£

ğ‘–

ğœ†ğ‘’1, ğ‘›
)

ğ‘£

ğ‘–

}

ğ‘£

(

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ã•ğ‘–
supp
{
âˆˆ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Observe that, by construction of the vector ğ‘£â€² in Model 6.6 and since ğ‘˜ğœ†6 > ğ¶âˆ—ğ‘‘, for a large enough
(cid:13)
6 100. Moreover we get that with probability at least
ğ‘£â€²(
constant ğ¶âˆ—, we get that for all ğ‘–
0.999, all the following inequalities hold.

Ã•ğ‘–
supp
{
âˆˆ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

âˆˆ [

+

)|

ğ‘‘

]

)

)

(

(

ğ‘–

ğ‘£

,

}

|

ğ‘¤ğ‘–

ğ‘6(

ğ‘£

ğ‘–

ğœ†ğ‘’1, ğ‘›
)

ğ‘¤ğ‘–ğ‘¤ğ‘–

(cid:1)

2
1
/

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2
1
/

T

By Lemma 8.12,

By Lemma 8.10,

By Lemma 8.11,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ğ‘€

ğ‘£

ğ‘–

ğœ†ğ‘’1, ğ‘¤ğ‘–
)

(

(

Ã•ğ‘–
supp
{
âˆˆ

ğ‘£

}

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

> ğ‘˜ğœ†6
2

.

)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ğ‘¤ğ‘–, ğ‘£

ğ‘€

(

ğ‘–

(

Ã•ğ‘–
supp
{
âˆˆ

ğ‘£

}

ğ‘¤ğ‘–, ğ‘£â€²

ğ‘€

(

(

ğ‘‘
Ã•ğ‘–
supp
]\
âˆˆ[

{

ğ‘£

}

ğœ†ğ‘’1)(cid:13)
)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
ğ‘’1)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

)

ğ‘–

6 ğ‘‚

ğ‘˜ğœ†4

(cid:16)

+

p

ğ‘˜ğ‘› log ğ‘›ğœ†4

(cid:17)

6 ğ‘‚

ğ‘‘

(cid:16)

+

p

ğ‘‘ğ‘› log ğ‘›

6 ğ‘‚

ğ‘‘

.

)

(

(cid:17)

ğ‘€

ğ‘£â€²

ğ‘–

(

)

(

ğ‘’1, ğ‘¤

ğ‘‘
Ã•ğ‘–
supp
]\
âˆˆ[

{

ğ‘£

}

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

6 ğ‘‚

ğ‘‘ log ğ‘› max

log ğ‘‘,

ğ‘› log ğ‘‘

.

n

p

o(cid:17)

(cid:16)p

72

All in all we get,

2

ğ‘¦ğ‘–

(cid:13)
(cid:13)
ğ‘‘
Ã•ğ‘–
(cid:13)
âˆˆ[
(cid:13)
6 ğ‘‚
(cid:13)
(cid:13)

] (cid:20) (cid:16)(cid:13)
(cid:13)
ğ‘‘
+

(cid:16)

(cid:13)
(cid:13)

p

ğ‘š

âˆ’

2

(cid:17)

âˆ’

2ğ‘š

ğ‘¦ğ‘– ğ‘¦ğ‘–

T

(cid:21)

âˆ’

ğ‘‘ğ‘› log ğ‘›

+

âˆšğ‘‘ğ‘› log ğ‘‘

ğ‘€

ğ‘£

Ã•ğ‘–
supp
{
âˆˆ
âˆšğ‘‘ log ğ‘‘

}

+

The result follows.

ğ‘£

ğ‘–

ğœ†ğ‘’1, ğ‘¤ğ‘–
)

(

(

ğ‘˜ğœ†5

+

+

p

)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ğ‘˜ğ‘› log ğ‘›ğœ†5

.

(cid:17)

(cid:3)

8.2 Algorithm recovers v with high probability

ğ›½ğ‘¢ğ‘£T

ğ‘¢. Since several algorithms
We now show how to obtain a good estimate of the sparse vector ğ‘£ from
Ë†
try to recover ï¬rst ğ‘¢ and then the sparse vector (e.g. SVD with thresholding) we turn back to the
model 1.1. A corollary for model 6.6 is presented at the end of the section. So, for the rest of the
â„ğ‘› is a vector such that
section, let ğ‘Œ =
ğ‘¢

> 0.9âˆšğ‘›, and ğ‘Š
k
The ï¬rst observation is that on one hand the vector ğ‘Œğ‘£ is close to

ğ‘‘. We also assume that ğ‘› 6 ğ‘˜ 6 ğ‘‘.

ğ›½ğ‘¢ with high probability.
On the other hand, the vector ğ‘ŒTğ‘¢ may be far from the sparse vector; that is, even knowing exactly
ğ‘¢, the thresholding step is required to recover ğ‘£. The next theorem provides guarantees on the
achievable correlation with the sparse vector given a vector close to ğ‘¢. Theorem E.1 shows in which
sense in which these guarantees are information theoretically tight.

â„ğ‘‘ is a ğ‘˜-sparse unit vector, ğ‘¢

+
0, 1
)
(

ğ¸, where ğ‘£

ğ‘Š
+
ğ‘

p

p

âˆ¼

âˆˆ

âˆˆ

k

Ã—

ğ‘›

Theorem 8.14. Let

ğ‘¢ be a vector such that
Ë†

ğ‘¢
k Ë†

ğ‘¢

k

âˆ’

6 ğœ€âˆšğ‘› for some 0 6 ğœ€ 6 1

10 , and let

If ğ›½ & ğ‘˜
ğœ2ğ‘›

log ğ‘‘

ğ¸

2
1
â†’

2

k

+ k

ğ‘£ =
Ë†

1

T
ğ‘¢
2 Ë†

ğ‘Œ .

ğ‘¢
Â· k Ë†
for some 0 < ğœ 6 1, then with probability at least 1

p

ğ›½

k

10 exp

ğ‘›

,
)

(âˆ’

âˆ’

(cid:0)

where ğœ‚

ğ‘£
( Ë†

) âˆˆ

(cid:1)

ğ‘£
( Ë†
â„ğ‘‘ is the vector with coordinates

ğœ‚

k

ğ‘£

k

) âˆ’

. ğœ€

ğœ ,

+

ğ‘– =

ğœ‚

ğ‘£
( Ë†

)

ğ‘£ğ‘– ,
Ë†
0,

(

> ğœ

if

ğ‘£ğ‘–
| Ë†
otherwise

|

âˆšğ‘˜
/

. Letâ€™s rewrite ğ‘Œ =

ğ›½

ğ‘¢ğ‘£T
Ë†

+

ğ‘Š

+

ğ‘

+

ğ¸ for a matrix

Proof. Assume that ğ›½ > 104
ğ‘ =
ğ‘¢
âˆ’ Ë†

ğ‘˜
ğœ2ğ‘›
ğ‘‘. Then for ğ‘–

â„ğ‘›

ğ‘£T

âˆˆ

ğ‘¢

ğ›½

Ã—

)

(

(cid:0)

Â·

log ğ‘‘

âˆˆ [

ğ¸
+ k
ğ‘‘
:
]

2
1
â†’

2

k

(cid:1)

p

T
ğ‘¢
Ë†

ğ‘

ğ‘–

=

ğ›½

ğ‘¢, ğ‘¢
h Ë†

ğ‘¢
âˆ’ Ë†

i

ğ‘£ğ‘–

ğ‘–

{

ğ‘£ğ‘– = 0

Let ğ‘† =
, ğ‘‡ =
, ğ´ =
By Lemma G.11, with probability at least 1
ğ‘£ğ‘– = 0,

/

}

{

ğ‘–

|

(cid:12)
(cid:12)

(cid:12)
(cid:1)
(cid:0)
(cid:12)
ğ‘£ğ‘– > ğœ
| Ë†

(cid:12)
p
(cid:12)
âˆšğ‘˜
(cid:12)
}

(cid:12)
(cid:12)
(cid:12)

ğ‘£ğ‘–
|
| |
2 exp
(âˆ’

p
6 2ğœ
/
ğ‘›
,
|
)

ğ‘–

{
âˆ’

p

6 ğœ€

ğ›½ğ‘›

ğ‘¢
Â· k Ë†

ğ‘£ğ‘–

.

|

k Â· |

âˆšğ‘˜
ğµ

|

and ğµ =

ğ‘–
}
{
6 ğ‘›. Consider some ğ‘–

ğ‘¢ğ‘Š

| ( Ë†

)

ğ‘– > 10
log ğ‘‘
ğ‘¢
.
k Ë†
k
}
ğ‘‡. Since
ğ‘†
p
âˆ©

âˆˆ

ğ‘– =

ğ‘¢ğ‘Š
( Ë†

)

2

ğ›½

ğ‘¢
Â· k Ë†

k

ğ‘£ğ‘–
Â· Ë†

ğ‘¢ğ¸

âˆ’ ( Ë†

)

ğ‘– > 100

ğ‘¢
ğ‘¢
k Ë†
k
âˆšğ‘› Â· k Ë†

k

Â·

p

ğ¸

2

k1

â†’

+ k

(cid:17)

log ğ‘‘

(cid:16)p
73

ğ‘¢
âˆ’ k Ë†

ğ¸

k1

â†’

k Â· k

2 > 10

ğ‘¢
k Ë†

k

log ğ‘‘ ,

p

which means that ğ‘†
> 0.8âˆšğ‘›. Hence

ğ‘¢
k Ë†

k

ğ‘‡

âˆ©

âŠ†

ğµ. Hence

ğ‘‡

|

ğµ

|

\

6

ğ‘†

|

|

= ğ‘˜. Note that since ğœ€ 6 1

10 and

> 0.9âˆšğ‘›,

ğ‘¢

k

k

ğœ‚
(

ğ‘£
( Ë†

ğ‘–

)

âˆ’

ğ‘£ğ‘–

ğµ
ğ‘‡
Ã•ğ‘–
\
âˆˆ

2 =
)

ğµ
ğ‘‡
Ã•ğ‘–
âˆˆ
\
6 2

ğ‘£ğ‘–
( Ë†

âˆ’

ğ‘£ğ‘–

2

)

ğ‘›
ğ‘¢
k Ë†

k

2 ğœ€2ğ‘£2

ğ‘– +

2

ğµ
ğ‘‡
Ã•ğ‘–
\
âˆˆ
100 log ğ‘‘

ğµ
ğ‘‡
Ã•ğ‘–
âˆˆ
\
6 4ğœ€2

4

+

6 4ğœ€2

+

ğµ
ğ‘‡
Ã•ğ‘–
âˆˆ
\
ğœ2 .

1
ğ›½ğ‘›

(cid:0)

1
ğ‘¢
k Ë†

ğ›½

4

k

ğ¸

k

+ k

T
ğ‘¢
( Ë†

ğ‘Š

T
2
ğ‘¢
ğ‘– + ( Ë†
)

ğ¸

2
ğ‘–
)

(cid:1)

(cid:0)
2
1
â†’

2

(cid:1)

Note that since

ğµ

|

|

Hence

6 ğ‘›, By Theorem G.9, with probability at least 1

T
ğ‘¢
( Ë†

ğ‘Š

2
ğ‘–
)

6 100

2

ğ‘¢
k Ë†

k

Â·

ğ‘› log ğ‘‘ .

ğµ
Ã•ğ‘–
âˆˆ

exp

(âˆ’

ğ‘›

,
)

âˆ’

ğœ‚
(

ğ‘£
( Ë†

ğ‘–

)

âˆ’

ğ‘£ğ‘–

ğµ
ğ‘‡
Ã•ğ‘–
âˆ©
âˆˆ

2 =
)

ğµ
ğ‘‡
Ã•ğ‘–
âˆˆ
âˆ©
6 2

ğ‘£ğ‘–
( Ë†

âˆ’

ğ‘£ğ‘–

2

)

ğ‘›
ğ‘¢
k Ë†

k

2 ğœ€2ğ‘£2

ğ‘– +

2

log ğ‘‘

ğ›½ +

400

ğœ2 .

Ã•ğ‘–
ğµ
âˆˆ
2
ğ¸
1
k
â†’
ğ›½

2

4 k

ğµ
Ã•ğ‘–
âˆˆ
6 4ğœ€2

+

6 4ğœ€2

+

1
ğ‘¢
k Ë†

T
ğ‘¢
4 ( Ë†

ğ‘Š

2
ğ‘– +
)

2

k

ğµ
Ã•ğ‘–
âˆˆ

1
ğ‘¢
k Ë†

k

ğ›½

T
ğ‘¢
4 ( Ë†

ğ¸

2
ğ‘–
)

ğ›½

If ğ‘–

ğ‘†

\

âˆˆ

ğ‘‡, then ğœ‚

ğ‘£
( Ë†

)

ğ‘– = ğ‘£ğ‘– = 0. If ğ‘–

ğ‘†

âˆˆ

ğœ
âˆšğ‘˜

>

>

ğ‘£ğ‘–
| Ë†

|

1

âˆ’

(cid:18)

âˆšğ‘›
ğ‘¢
k Ë†

k

ğ‘£ğ‘–

ğœ€

|

(cid:19)

hence in this case

ğ‘¢ğ‘Š

|( Ë†

ğ‘–

)

|

> 0.7

Â·

0.8

ğ‘‡

ğ´, then

)

âˆ©
âˆ©
ğ‘¢Tğ‘Š
ğ‘–
( Ë†
2 (cid:12)
ğ‘¢
ğ›½
(cid:12)
k Ë†
(cid:12)
log ğ‘‘ > 10
(cid:12)
p
(cid:12)

âˆ’ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

k

| âˆ’ (cid:12)
(cid:12)
(cid:12)
(cid:12)
p
100
(cid:12)

Â·

ğ‘–

> 1.8

ğ‘¢Tğ¸
( Ë†
)
2 (cid:12)
ğ‘¢
ğ›½
(cid:12)
k Ë†
(cid:12)
(cid:12)
log ğ‘‘, so ğ‘–
(cid:12)

k

ğœ
âˆšğ‘˜ âˆ’ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

âˆˆ

ğ‘¢Tğ‘Š
( Ë†
ğ‘¢
ğ›½
k Ë†

)

k

ğ‘–
2 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

p
ğµ. Moreover,

0.1

âˆ’

ğœ
âˆšğ‘˜

,

p
6 1.1ğœ

0.9âˆšğ‘˜ +

ğ‘£ğ‘–

|

|

Therefore

ğ‘‡
ğ‘†
Ã•ğ‘–
âˆ©
âˆ©
âˆˆ
It follows that

ğ´

ğœ‚
(

ğ‘£
( Ë†

ğ‘–

)

âˆ’

ğ‘£ğ‘–

2 =
)

ğ‘£2
ğ‘–

6 2

ğ‘‡
ğ‘†
Ã•ğ‘–
âˆ©
âˆ©
âˆˆ

ğ´

ğµ
Ã•ğ‘–
âˆˆ

2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
p
(cid:12)
2ğœ2
ğ‘˜ +

p
1
T
ğ‘¢
ğ›½ğ‘› ( Ë†

ğ‘Š

ğ‘–

.

)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
ğ›½ğ‘›2 ( Ë†

4

ğµ
Ã•ğ‘–
âˆˆ

ğ‘¢ğ‘Š

6 4ğœ2

2
ğ‘–
)

+

ğœ2 = 5ğœ2 .

ğœ‚

ğ‘£
( Ë†

k

) âˆ’

ğ‘£

k

2 6

ğœ‚
(

ğ‘£
( Ë†

ğ‘–

)

âˆ’

ğ‘£ğ‘–

2
)

+

ğ‘‡
Ã•ğ‘–
âˆˆ

Hence with probability at least 1

âˆ’

ğ‘‡
ğ‘†
Ã•ğ‘–
âˆ©
âˆ©
âˆˆ
3 exp

ğ‘›

,
)

(âˆ’
ğ‘£
ğœ‚
( Ë†

k

ğ‘£2
ğ‘– +

ğ´

ğ‘‡
ğ‘†
Ã•ğ‘–
âˆ©
âˆ©
âˆˆ

ğ´

ğ‘£2
ğ‘–

6 8ğœ€2

2ğœ2

4ğœ2

+

+

+

5ğœ2 = 8ğœ€2

11ğœ2 .

+

ğ‘£

k

) âˆ’

. ğœ€

ğœ .

+

74

(cid:3)

An immediate consequence is the following corollary.

Corollary 8.15. Consider a matrix of the form,

ğ‘Œ = ğ‘Š

T
ğœ†ğ‘¢ğ‘£

+

ğ‘¢

ğ‘£â€²

+

âˆ’

T

ğ‘¢

ğ‘Š

T

for a Gaussian matrix ğ‘Š

0, 1
)
(
and a vector ğ‘£â€² as deï¬ned in 6.6. Let
ğ‘›ğ‘‘
then we can compute in time ğ‘‚

ğ‘

âˆ¼

ğ‘›

Ã—

(cid:1)
ğ‘‘, a random unit vector ğ‘¢, a ğ‘˜-sparse vector ğ‘£ with entries in

(cid:0)

ğ‘¢ be a vector such that
Ë†
an estimator

ğ‘¢

ğ‘¢
k Ë†

âˆ’

k

ğ‘£ such that with probability at least 1
Ë†

âˆ’

6 ğœ€ for some 0 6 ğœ€ 6 1

(

)

0,

1
}
{
Â±
10 . If ğœ† & âˆšlog ğ‘‘
,
ğ‘›
exp
)

(âˆ’

ğœ

ğ‘£
k Ë†

ğ‘£

k

âˆ’

.

ğœ€

(

+

ğœ

âˆšğ‘˜ .
)

9 Experiments

In this section we compare the performance of Diagonal Thresholding and SVD of degree 2, 4, 6
as in 8.1 on practical instances. The table below explains the regimes of the ï¬gures presented. We
refer to Robust Sparse PCA as model 1.1 where the adversarial matrix ğ¸ follows the distribution
shown in 6.6. Appendix I contains a detailed report of the experimental setup.

ğ‘˜ > âˆšğ‘‘

ğ‘˜ 6 âˆšğ‘‘

Standard Sparse PCA

Figure 2a for ğ›½ >

ğ‘‘
ğ‘›

q
Figure 3 for ğ›½ > ğ‘˜
âˆšğ‘›

log ğ‘‘
ğ‘˜

Robust Sparse PCA
Figure 2b for ğ›½ > ğ‘˜
ğ‘›
3
1
ure 2c for ğ›½ > ğ‘˜
/
(cid:0)
ğ‘›

ğ‘‘
ğ‘˜

2
1
/

ğ‘‘
ğ‘˜

(cid:1)

(cid:0)

(cid:1)

Fig-

q

Table 4: Plots

75

(a) Standard Sparse PCA, with ğ‘˜ > âˆšğ‘‘, ğ›½ >

ğ‘‘
ğ‘›

q

(b) Robust Sparse PCA with ğ‘˜ > âˆšğ‘‘, ğ›½ > ğ‘˜
ğ‘›

ğ‘‘
ğ‘›

2
1
/

(cid:16)

(cid:17)

(c) Robust Sparse PCA with ğ‘˜ > âˆšğ‘‘, ğ›½ > ğ‘˜
ğ‘›

ğ‘‘
ğ‘›

3
1
/

(cid:16)

(cid:17)

Figure 2: Forthe single spiked covariance model with ğ‘˜ > âˆšğ‘‘, Figure 2a shows how the SVD algo-
rithms works (with information theoretically optimal guarantees) and Diagonal Thresholding fails.
Figures 2b, 2c show however how adversarial noise immediately breaks SVD with thresholding.
In Figure 2b ğ›½ & ğ‘˜
, hence as ğ‘‘ increases and becomes larger than ğ‘›2, SVD-4 returns a good
ğ‘›
ğ‘›3 when the signal is much
estimate. We point out how how SVD-6 performs well even for ğ‘‘
larger than ğ‘˜
ğ‘›
but as ğ‘‘ grows towards ğ‘›3, SVD-6 approaches correlation 1.

(cid:0)
3
1
. Finally, Figure 2c shows how DT, SVD-4 and SVD-2 fails for ğ›½ = Î˜
/

2
1
/

3
1
/

â‰ª

ğ‘˜
ğ‘›

ğ‘‘
ğ‘˜

ğ‘‘
ğ‘˜

ğ‘‘
ğ‘˜

(cid:1)

,

(cid:16)

(cid:17)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

76

Figure 3: The ï¬gure shows settings in which ğ‘˜ 6 âˆšğ‘‘. In this regime, among the algorithms con-
sidered, Diagonal Thresholding achieves asymptotically the most correlation. In practical settings

however it is often the case that

recover the signal.

ğ‘˜
âˆšğ‘›

log ğ‘‘ > Î©

p

3
1
/

ğ‘˜
ğ‘›

ğ‘‘
ğ‘˜

(cid:16)

(cid:0)

(cid:1)

(cid:17)

and hence also SVD-6 can accurately

77

References

[AS16]

Emmanuel Abbe and Colin Sandon, Achieving the ks threshold in the general stochastic
block model with linearized acyclic belief propagation, Advances in Neural Information
Processing Systems 29 (D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett,
eds.), Curran Associates, Inc., 2016, pp. 1334â€“1342. 3

[AW09]

Arash A. Amini and Martin J. Wainwright, High-dimensional analysis of semideï¬nite
relaxations for sparse principal components, Ann. Statist. 37 (2009), no. 5B, 2877â€“2921. 1,
2, 25

[AWH13] George B. Arfken, Hans J. Weber, and Frank E. Harris, Chapter 15 - legendre functions,
Mathematical Methods for Physicists (Seventh Edition) (George B. Arfken, Hans J.
Weber, and Frank E. Harris, eds.), Academic Press, Boston, seventh edition ed., 2013,
pp. 715 â€“ 772. 98

[BBP05]

Jinho Baik, GÃ©rard Ben Arous, and Sandrine PÃ©chÃ©, Phase transition of the largest eigen-
value for nonnull complex sample covariance matrices, Ann. Probab. 33 (2005), no. 5, 1643â€“
1697. 1

[BHK+16] Boaz Barak, Samuel B. Hopkins, Jonathan A. Kelner, Pravesh Kothari, Ankur Moitra,
and Aaron Potechin, A nearly tight sum-of-squares lower bound for the planted clique problem,
FOCS, IEEE Computer Society, 2016, pp. 428â€“437. 7, 19, 23, 25

[BKS14]

Boaz Barak, Jonathan A. Kelner, and David Steurer, Rounding sum-of-squares relaxations,
STOC, ACM, 2014, pp. 31â€“40. 64

[BKW20a] Afonso S. Bandeira, Dmitriy Kunisky, and Alexander S. Wein, Computational hardness of
certifying bounds on constrained PCA problems, 11th Innovations in Theoretical Computer
Science Conference, ITCS 2020, January 12-14, 2020, Seattle, Washington, USA, 2020,
pp. 78:1â€“78:29. 4

[BKW20b]

, Computational hardness of certifying bounds on constrained PCA problems, 11th
Innovations in Theoretical Computer Science Conference, ITCS 2020, January 12-14,
2020, Seattle, Washington, USA, 2020, pp. 78:1â€“78:29. 23, 40, 85

[BMR19]

Jess Banks, Sidhanth Mohanty, and Prasad Raghavendra, Local statistics, semideï¬nite
programming, and community detection, CoRR abs/1911.01960 (2019). 3

[BR13a]

[BR13b]

[BS16]

Quentin Berthet and Philippe Rigollet, Computational lower bounds for sparse PCA, CoRR
abs/1304.0828 (2013). 1, 2, 3, 11, 41

Quentin Berthet and Philippe Rigollet, Optimal detection of sparse principal components
in high dimension, Ann. Statist. 41 (2013), no. 4, 1780â€“1815. 1, 8

Boaz Barak and David Steurer, Proofs, beliefs, and algorithms through the lens of sum-
of-squares, Course notes: http://www. sumofsquares. org/public/index. html (2016).
20

78

[Cam60]

Lucien Le Cam, Locally asymptotically normal families, Univ. California Publ. Statist.
(1960). 24

[CMW13] T. Tony Cai, Zongming Ma, and Yihong Wu, Sparse pca: Optimal rates and adaptive

estimation, Ann. Statist. 41 (2013), no. 6, 3074â€“3110. 2

[Das99]

Sanjoy Dasgupta, Learning mixtures of gaussians, FOCS, IEEE Computer Society, 1999,
pp. 634â€“644. 82

[dGJL05] Alexandre dâ€™Aspremont, Laurent E Ghaoui, Michael I Jordan, and Gert R Lanckriet,
A direct formulation for sparse pca using semideï¬nite programming, Advances in neural
information processing systems, 2005, pp. 41â€“48. 2, 3, 25

[DHS20]

Jingqiu Ding, Samuel B. Hopkins, and David Steurer, Estimating Rank-One Spikes from
Heavy-Tailed Noise via Self-Avoiding Walks, arXiv e-prints (2020), arXiv:2008.13735. 20

[DKK+16]

Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Al-
istair Stewart, Robust estimators in high dimensions without the computational intractability,
FOCS, IEEE Computer Society, 2016, pp. 655â€“664. 82

[DKWB19] Yunzi Ding, Dmitriy Kunisky, Alexander S. Wein, and Afonso S. Bandeira,

Subexponential-time algorithms for sparse pca, 2019. 1, 2, 6, 11, 17, 39, 40

[DM14]

[FK01]

Yash Deshpande and Andrea Montanari, Sparse PCA via covariance thresholding, NIPS,
2014, pp. 334â€“342. 2, 16, 88, 102

Uriel Feige and Joe Kilian, Heuristics for semirandom graph problems, J. Comput. Syst. Sci.
63 (2001), no. 4, 639â€“671. 3

[FMDF16] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard, Robustness of

classiï¬ers: from adversarial to random noise, NIPS, 2016. 2

[GLS81] M. GrÃ¶tschel, L. LovÃ¡sz, and A. SchrÄ³ver, The ellipsoid method and its consequences in

combinatorial optimization, Combinatorica 1 (1981), no. 2, 169â€“197. MR 625550 21, 22

[GV14]

Olivier GuÃ©don and Roman Vershynin, Community detection in sparse networks via
grothendieckâ€™s inequality, Probability Theory and Related Fields 165 (2014). 3

[HKP+17a] Samuel B. Hopkins, Pravesh K. Kothari, Aaron Potechin, Prasad Raghavendra, Tselil
Schramm, and David Steurer, The power of sum-of-squares for detecting hidden structures,
CoRR abs/1710.05017 (2017). 2

[HKP+17b]

, The power of sum-of-squares for detecting hidden structures, FOCS, IEEE Computer

Society, 2017, pp. 720â€“731. 2, 7, 23, 25, 39, 41

[HL18]

Samuel B. Hopkins and Jerry Li, Mixture models, robustness, and sum of squares proofs,
Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing,
STOC 2018, Los Angeles, CA, USA, June 25-29, 2018, 2018, pp. 1021â€“1034. 32, 82

79

[Hop18]

Samuel Brink Klevit Hopkins, Statistical inference and the sum of squares method. 7, 19,
20, 23, 24, 25, 40

[HS17]

Samuel B. Hopkins and David Steurer, Eï¬ƒcient bayesian estimation from few samples:
Community detection and related problems, FOCS, IEEE Computer Society, 2017, pp. 379â€“
390. 7, 19, 20, 23, 25, 39

[HSSS16]

Samuel B. Hopkins, Tselil Schramm, Jonathan Shi, and David Steurer, Fast spectral
algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors, STOC,
ACM, 2016, pp. 178â€“191. 11, 64

[Hub81]

P.J. Huber, Robust statistics, Wiley Series in Probability and Statistics, Wiley, 1981. 2

[JL09]

Iain M. Johnstone and Arthur Yu Lu, On consistency and sparsity for principal components
analysis in high dimensions, Journal of the American Statistical Association 104 (2009),
no. 486, 682â€“693, PMID: 20617121. 1

[KKM18] Adam R. Klivans, Pravesh K. Kothari, and Raghu Meka, Eï¬ƒcient algorithms for outlier-
robust regression, Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9
July 2018., 2018, pp. 1420â€“1430. 32

[KNV+15] Robert Krauthgamer, Boaz Nadler, Dan Vilenchik, et al., Do semideï¬nite relaxations solve
sparse pca up to the information limit?, The Annals of Statistics 43 (2015), no. 3, 1300â€“1322.
1, 2, 25

[KS17a]

[KS17b]

[Las01]

Pravesh K. Kothari and Jacob Steinhardt, Better agnostic clustering via relaxed tensor
norms, CoRR abs/1711.07465 (2017). 32

Pravesh K. Kothari and David Steurer, Outlier-robust moment-estimation via sum-of-
squares, CoRR abs/1711.11581 (2017). 32, 82

Jean B. Lasserre, New positive semideï¬nite relaxations for nonconvex quadratic programs,
Advances in convex analysis and global optimization (Pythagorion, 2000), Nonconvex
Optim. Appl., vol. 54, Kluwer Acad. Publ., Dordrecht, 2001, pp. 319â€“331. MR 1846160
21

[LM00]

B. Laurent and P. Massart, Adaptive estimation of a quadratic functional by model selection,
Ann. Statist. 28 (2000), no. 5, 1302â€“1338. 100

[MMV16] Konstantin Makarychev, Yury Makarychev, and Aravindan VÄ³ayaraghavan, Learning
communities in the presence of errors, COLT, JMLR Workshop and Conference Proceed-
ings, vol. 49, JMLR.org, 2016, pp. 1258â€“1291. 3

[Mon19]

Andrea Montanari, Optimization of the sherrington-kirkpatrick hamiltonian, 2019 IEEE
60th Annual Symposium on Foundations of Computer Science (FOCS), IEEE, 2019,
pp. 1417â€“1433. 4

[Mor07]

Stephan Morgenthaler, A survey of robust statistics, Statistical Methods and Applications
15 (2007), no. 3, 271â€“293. 2

80

[MPW16] Ankur Moitra, William Perry, and Alexander S. Wein, How robust are reconstruction
thresholds for community detection?, Proceedings of the 48th Annual ACM SIGACT
Symposium on Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21,
2016, 2016, pp. 828â€“841. 3

[MS16]

Andrea Montanari and Subhabrata Sen, Semideï¬nite programs on sparse random graphs
and their application to community detection, STOC, ACM, 2016, pp. 814â€“827. 3

[MSS16]

Tengyu Ma, Jonathan Shi, and David Steurer, Polynomial-time tensor decompositions with
sum-of-squares, CoRR abs/1610.01980 (2016). 20

[MV10]

Ankur Moitra and Gregory Valiant, Settling the polynomial learnability of mixtures of
gaussians, FOCS, IEEE Computer Society, 2010, pp. 93â€“102. 82

[MW15]

Tengyu Ma and Avi Wigderson, Sum-of-squares lower bounds for sparse PCA, NIPS, 2015,
pp. 1612â€“1620. 2

[MWA06] Baback Moghaddam, Yair Weiss, and Shai Avidan, Generalized spectral bounds for sparse
LDA, Machine Learning, Proceedings of the Twenty-Third International Conference
(ICML 2006), Pittsburgh, Pennsylvania, USA, June 25-29, 2006, 2006, pp. 641â€“648. 25

[Nat95]

[Nes00]

[NP33]

[Par00]

[PH94]

B. K. Natarajan, Sparse approximate solutions to linear systems, SIAM J. Comput. 24 (1995),
no. 2, 227â€“234. 25

Yurii Nesterov, Squared functional systems and optimization problems, High performance
optimization, Appl. Optim., vol. 33, Kluwer Acad. Publ., Dordrecht, 2000, pp. 405â€“440.
MR 1748764 21

J. Neyman and E. S. Pearson, On the problem of the most eï¬ƒcient tests of statistical hypotheses,
Philosophical Transactions of the Royal Society of London. Series A, Containing Papers
of a Mathematical or Physical Character 231 (1933), 289â€“337. 24

Pablo A Parrilo, Structured semideï¬nite programs and semialgebraic geometry methods in
robustness and optimization, Ph.D. thesis, California Institute of Technology, 2000. 21

Karl Pearson and Olaus Magnus Friedrich Erdmann Henrici, Iii. contributions to the
mathematical theory of evolution, Philosophical Transactions of the Royal Society of
London. (A.) 185 (1894), 71â€“110. 82

[Rud99]

M. Rudelson, Random vectors in the isotropic position, Journal of Functional Analysis 164
(1999), no. 1, 60 â€“ 72. 99

[RV17]

Oded Regev and Aravindan VÄ³ayaraghavan, On learning mixtures of well-separated
gaussians, FOCS, IEEE Computer Society, 2017, pp. 85â€“96. 82

[Sch17]

Konrad SchmÃ¼dgen, The moment problem, Springer International Publishing, 2017. 97

[Sho87]

N. Z. Shor, Quadratic optimization problems, Izv. Akad. Nauk SSSR Tekhn. Kibernet.
(1987), no. 1, 128â€“139, 222. MR 939596 21

81

[SK75]

[Tro12]

[VW02]

[Wai19]

David Sherrington and Scott Kirkpatrick, Solvable model of a spin-glass, Physical review
letters 35 (1975), no. 26, 1792. 3

Joel A. Tropp, User-friendly tail bounds for sums of random matrices, Foundations of
Computational Mathematics 12 (2012), no. 4, 389â€“434. 100, 101

Santosh Vempala and Grant Wang, A spectral algorithm for learning mixtures of distribu-
tions, FOCS, IEEE Computer Society, 2002, p. 113. 82

Martin J. Wainwright, High-dimensional statistics: A non-asymptotic viewpoint, Cam-
bridge Series in Statistical and Probabilistic Mathematics, Cambridge University Press,
2019. 95, 100

A Relationship with Clustering mixture of subgaussians

â„ğ‘› from a
The canonical version of the Gaussian Mixture Model consists of ğ‘‘ samples ğ‘¦1, . . . , ğ‘¦ğ‘‘
mixture of ğ‘˜ Gaussian probability distributions ğº1, . . . , ğºğ‘˜ with means ğœ‡1, . . . , ğœ‡ğ‘˜ and covariances
> Î”
Î£1, . . . Î£ğ‘˜ close to the identity in the spectral sense
for some parameter Î” of the problem. The goal is then to partition the samples in clusters ğ‘†1, . . . , ğ‘†ğ‘˜
ğºğ‘–. Notice that the input can be written as ğ‘Œ = ğ‘Š
ğ‘‹ where ğ‘‹ is a matrix
such that
with only ğ‘˜ distinct columns ğœ‡1, . . . , ğœ‡ğ‘˜ and ğ‘Š is a matrix with independent columns following
distributions ğ‘

. 1, such that

, . . . , ğ‘

ğ‘†ğ‘–, ğ‘¦ğ‘—

âˆˆ [

ğ‘–, ğ‘—

Î£ğ‘–

Id

ğœ‡ğ‘—

ğœ‡ğ‘–

ğ‘¦ğ‘—

(cid:13)
(cid:13)

(cid:13)
(cid:13)

âˆ’

+

âˆ’

âˆ€

âˆ€

âˆˆ

âˆˆ

ğ‘˜

k

k

]

,

âˆ¼
0, Î£1)
(

0, Î£ğ‘˜
(

.
)

There is a reach literature concerning this problem both from a statistical and computational
perspective (e.g. see [PH94, Das99, MV10, VW02, DKK+16, RV17, HL18, KS17b]). A simple greedy
algorithm called single-linkage clustering can be designed observing that with high probability,
whenever Î” & ğ‘›1
4, pairs of samples from the same cluster are closer in Euclidean distance closer
/
than pairs of samples from diï¬€erent clusters. Furthermore, for ğ‘˜ < ğ‘›, as the centers ğœ‡1, . . . , ğœ‡ğ‘˜ lives
in a ğ‘˜ dimensional subspace and this space is close to the span of the top ğ‘˜ singular vectors of ğ‘Œ,
the bound can be pushed down to Î” & ğ‘˜1
4 projecting the points into this low-dimensional space.
/
This algorithmic barrier of Î” & ğ‘˜1
4 was broken only recently. Independently [HL18, KS17b]
/
ğ›¾2

provided SoS algorithms able to reconstruct the clusters for Î” & ğ‘˜ğ›¾, for any ğ›¾ > 0, in time
using ğ‘˜ğ‘‘1
/
following intuition: given a set ğ‘†ğ‘– =
form the same distribution ğºğ‘–, then the ï¬rst 1
/
the subgaussian bound ğ”¼ğ‘†ğ‘–
the ğ‘¡-th moment will not be Gaussian.

)
ğ›¾ samples. This new approach (which we refer to as the moments method) is based on the
of ğ‘‘1
ğ›¾ samples, if most of the samples come
/
ğ›¾ moments of the empirical distribution will satisfy
(cid:8)
2. Conversely, if samples actually belongs to diï¬€erent clusters,
/

ğ‘¦1, . . . , ğ‘¦ğ‘‘1

. ğ‘¡ ğ‘¡

ğ‘‘ğ‘˜

ğ‘¦ğ‘¡

1
/

(cid:9)

ğ‘‚

(

)

(

ğ›¾

/

For our discussion, an important observation is that the same algorithms works for mixtures

of subgaussians.

ğœ†

+

ğ‘Š

ğ›½ğ‘¢0ğ‘£0

T as ğ‘Œ = ğ‘Š

+
ğœğ‘– = sign
p
(

It is insightful to see the Sparse PCA problem in this perspective. Rewrite the matrix ğ‘Œ =
and entries

ğ‘¢ğœT where
Â¯
ğ‘¢ := ğ‘¢0
ğ‘¢ is the vector
ğ‘¢0
Â¯
Â¯
q
. Furthermore, notice that the matrix ğœ†
)

ğ›½ğ‘›
ğ‘˜ . The two models are equivalent as with
ğ‘¢ğœT has only three distinct
high probability
Â¯
ğ‘¢ and the zero vector. That is, we could see an instance of the canonical Sparse
columns:
Â¯
PCA problem as a non-uniform mixture of three Gaussian distributions with separation ğœ†. In

â„ğ‘‘ is the vector with support supp

âˆˆ
and ğœ† =

ğ‘¢,
âˆ’ Â¯

ğ‘¢0k

ğ‘£0}

ğ›½
ğ‘˜ k

= Î˜

ğœ†
(

ğ‘£0,ğ‘–

,
)

ğœ
Â¯

q

{

k

k

82

(cid:2)

(cid:3)

this formulation, Diagonal and Covariance Thresholding recovers the sparse vector (and hence
separates the cluster centered at the origin from the others) for ğœ† >
, the same bound as
the single-linkage algorithm.

4
ğ‘›1
/

Ëœğ‘‚

Consider now the Wishart model 1.1 with adversarial perturbations: ğ‘Œ = ğ‘Š

ğ¸.
Fix ğ· = 1
ğ›¾, the adversarial matrix ğ¸ğ· will be the one described in Section 2.5 and Section 6, which
/
can be written as ğ¸ğ· =
and such
that its non-zero entries are distributionally independent. All in all, this leads us to the formulation

T for some vector ğ‘£â€² âˆˆ

â„ğ‘‘ with support

q
supp

ğ‘¢
ğ‘¢0k Â¯

ğ‘£â€² âˆ’

ğ‘£0}

ğ‘ŠT

] \

ğ‘¢
Â¯

ğ‘¢
Â¯

+

+

ğ‘‘

{

[

ğœT
Â· Â¯

ğ›½
ğ‘˜ k

(cid:0)

(cid:1)

(cid:0)

(cid:1)
ğ‘Œ = ğ‘Š

ğ¸ğ·

ğœ†

ğ‘¢
Â¯

+

T = ğ‘Š â€²
ğœ
Â· Â¯

+

ğ‘‹ ,

+

+

with ğ‘Š â€² = ğ‘Š
the matrix ğ‘‹ has three distinct columns:

ğ¸ğ· being a matrix with independent columns and subgaussian moments. Again
ğ‘¢ and the zero vector.
Â¯

Similarly to the non-robust settings, this formulation can be seen as an non-uniform mixture
of three subgaussian distributions with separation Î” & ğœ†. By the argument shown in Section 6 any
algorithm that can be described as a low-degree polynomial and that tries to cluster these points
using the moments method will be able to detect that ğ‘Œ is not a single subgaussian distribution
(and hence it is not a good cluster) only using at least ğ‘‘ğ· & ğ‘˜ğ‘‘ğ· samples.

ğ‘¢,
âˆ’ Â¯

B Comparison with the Wigner model

+

+

ğ‘Š

ğ¸, where ğ‘Š

The Wigner model presents some diï¬€erences in the robust settings. Here we consider a matrix
T
ğ‘Œ = ğ›½ğ‘£0ğ‘£0
Â±
n
the jointly-distributed random variables ğ‘Š, ğ‘£0 are independent. The matrix ğ¸ has norm
for some ğ‘
that in order to have an algorithm that outputs a vector
zero with high probability, the adversarial matrix needs to satisfy the bound

and
6 ğ‘2
â„. An analysis similar to the one made for the Wishart model in Section 2.1 shows
2 is bounded away from
6 ğ›½
ğ‘˜ .

ğ‘‘, ğ‘£0 is a ğ‘˜-sparse unit vector with entries in

ğ‘£ such that
Ë†

ğ‘£, ğ‘£0i
h Ë†

0, 1
)
(

1
âˆšğ‘˜
ğ¸

o
kâˆ

, 0

ğ‘

âˆ¼

ğ¸

âˆˆ

k

Ã—

ğ‘‘

In these settings the simple PCA approach of computing the top eigenvector of ğ‘Œ and removing
all but the top ğ‘˜ entries fails. Indeed it suï¬ƒces to plant a matrix ğ¸ = ğ‘§ğ‘§âŠ¤ where ğ‘§ is the vector with
and 0 otherwise. For ğ‘ğ‘‘ & ğ›½ the top eigenvector of ğ‘Œ is almost
entries ğ‘§ğ‘– = ğ‘ if ğ‘–
ğ‘£0}
orthogonal to ğ‘£0, and so is its projection to the top ğ‘˜ coordinates.

supp

âˆˆ [

ğ‘‘

{

]

k

kâˆ

The Covariance Thresholding algorithm also can be easily fooled by an adversary with the

same approach to the one previously shown: simply let ğ¸ = ğ‘§ğ‘§âŠ¤ be a rank 1 matrix with supp
supp

=

.

ğ‘§

{

} âˆ©

The diï¬€erence appears in the Diagonal Thresholding algorithm, which turns out to be per-
diagonal entries will have value
will have
ğ›½. The reason behind this diversity is that, in model 1.1 the

turbation resilient. Indeed as
ğ‘Œğ‘–ğ‘–
kâˆ
|
+
. 1
value bounded by Ëœğ‘‚
(
adversarial perturbation exploits the large norm of the columns of ğ‘Š.

supp
ğ›½. Conversely diagonal entries indexed by ğ‘— âˆ‰ supp

& 1
1

ğ‘˜ , for ğ‘–

ğ‘£0}

ğ‘£0}

. ğ›½

& 1

kâˆ)

âˆ’ k

+
ğ¸

+ k

kâˆ

+

ğ¸

ğ¸

âˆˆ

ğ›½

{

{

k

|

ğ‘£0}

{

âˆ…

C Thresholding Algorithms are Fragile

In this section we formalize the discussions of the introduction and show that SVD with Threshold-
ing, Diagonal Thresholding and Covariance Thresholding are indeed not resilient to adversarial
perturbations.

83

C.1 SVD with Thresholding is Fragile

The polynomial-time algorithm presented in Section 1 for the strong-signal regime is highly sensi-
tive to small adversarial perturbations. Concretely, this can be shown constructing ğ¸ with entries
so that eigenvectors of ğ‘ŒTğ‘Œ cannot be used to recover ğ‘¢0.
bounded Ëœğ‘‚
ğ›¾ğ‘¢0ğ‘¢0
ğ‘¢0k

2 that we will choose later. Then ğ‘Œ =

âˆšğ‘›
Consider ğ¸ =

Tğ‘Š for some 0 < ğ›¾ <

T
ğ›½ğ‘¢0ğ‘£0

1
/
(

)
âˆ’

+

k

T
ğ›¾ğ‘¢0ğ‘¢0

ğ‘Š and

Id

âˆ’

p

(cid:0)

(cid:1)

ğ‘Œğ‘Œ

T

T =ğ›½ğ‘¢0ğ‘¢0
ğ›½

Id
+
T
ğ‘¢0ğ‘£0
(cid:0)

âˆ’
ğ‘Š

+

p
Hence with high probability,

(cid:0)

T
ğ›¾ğ‘¢ğ‘¢0

Id

âˆ’

(cid:0)

T
ğ›¾ğ‘¢0ğ‘¢0

T

ğ‘Šğ‘Š
T
ğ›¾ğ‘¢0ğ‘¢0
(cid:1)

Id

(cid:0)
+

âˆ’
Id

T
ğ›¾ğ‘¢0ğ‘¢0
(cid:1)

ğ‘Š

T

T
ğ‘£0ğ‘¢0

âˆ’

(cid:1)

(cid:0)

(cid:1)

(cid:1)

1
ğ‘¢0k

k

T
ğ‘¢0

ğ‘Œ

k

2 k

2 = ğ‘§

ğ›½

ğ‘¢0k

k

+

2

ğ›¾2

2ğ‘‘

ğ‘¢0k

k

+

2ğ›¾

ğ‘‘

ğ‘¢0k

k

+ Ëœğ‘‚

âˆ’

ğ›½

/

ğ‘›

,

(cid:19)

(cid:18)q

where ğ‘§ has a ğœ’2-distribution with ğ‘‘ degrees of freedom. On the other hand notice that for a unit
2 which has the same
vector ğ‘¥ orthogonal to ğ‘¢0 and independent of ğ‘Š, we get
k
ğ‘‘ + Ëœğ‘‚
distribution as ğ‘§. So our claim follows choosing ğ›¾ so that 2ğ›¾
ğ‘¢0k
k

Indeed then ğ‘¢0ğ‘Œ has the same distribution as ğ‘§. Now, since with high probability
ğ‘‘

ğ‘›
.
ğ›½
2 6 2ğ‘›, if
(cid:17)
(cid:16)
p

ğ‘› & ğ›½, such a ğ›¾ exists.

ğ‘¥Tğ‘Œ
k
ğ‘¢0k âˆ’

ğ‘¥Tğ‘Š
k
ğ‘¢0k

k
2 = ğ›½

2 =

ğ›¾2

ğ‘¢0

1
ğ‘‘

/

k

k

Â·

k

k

2

/

C.2 Diagonal Thresholding is Fragile

Recall that Diagonal Thresholding ï¬nds the top ğ‘˜ diagonal entries of the covariance matrix and
ğ‘˜ principal submatrix. We shows here that
output a top eigenvector of the corresponding ğ‘˜
a simple adversary can make diagonal entries in
larger than diagonal entries in
] \
ğ‘£0}
supp
, hence leading the algorithm to choose a submatrix which contain no information about
the sparse vector.

ğ‘£0}

supp

Ã—
ğ‘‘
[

{

{

Concretely, the algorithm can be written as follows:

Algorithm C.1 (Diagonal Thresholding).

Given: Sample matrix ğ‘Œ of form 1.1 where ğ‘£0 is a ï¬‚at vector.

Estimate: The sparse vector ğ‘£0.

Operation:

1. Let ğ‘† :=

entries of ğ‘ŒTğ‘Œ.

{

ğ‘–1, . . . , ğ‘–ğ‘˜

} âŠ† [

ğ‘‘

]

be the set of indexes denoting the ğ‘˜ largest diagonal

2. Output a top eigenvector of ğ‘ŒTğ‘Œ

ğ‘†

[

ğ‘†

.
]

Ã—

We start by deï¬ning the adversarial matrix.

84

Deï¬nition C.2. Let ğ‘
with columns

âˆˆ

ğ‘… and denote with ğ‘Š1, . . . , ğ‘Šğ‘‘ the columns of ğ‘Š. Deï¬ne ğ¸ to be the matrix

ğ¸ğ‘– =

ğ‘Šğ‘–

ğ‘
ğ‘Šğ‘– k
k
0

(

The result is shown in the theorem below.

ifğ‘–

ğ‘‘

supp

] \
âˆˆ [
otherwise.

ğ‘£0}

{

Theorem C.3. Let ğ‘› > ğœ”
ğ¸ be as deï¬ned in C.2 and

log ğ‘‘

ğ¸

(cid:0)
k

k1

ğ‘˜

, ğ›½ = ğ‘œ
. Let ğ‘Œ be sampled according to 1.1 where ğ‘£0 is a ï¬‚at vector. Let
(
)
2 & ğ›½âˆšğ‘›
(cid:1)
ğ‘˜ +
â†’

log ğ‘‘. Then for each ğ‘–

ğ‘£0}

ğ‘£0}

supp

supp

and ğ‘—

âˆˆ [

âˆˆ

ğ‘‘

{

{

]

with probability at least 0.99.

p
ğ‘Œğ‘’ğ‘–
k

k

2 >

2

ğ‘Œğ‘’ğ‘—

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Notice how, for ğ›½ = Î˜

log ğ‘‘
suï¬ƒces to fool Diagonal Thresholding. A perturbation resilient algorithm would succeed as long

the theorem implies that an adversary with

log ğ‘‘

k1

ğ¸

â†’

k

ğ‘˜
âˆšğ‘›

2 &

p

as

ğ¸

k

k1

â†’

2 . min

(cid:16)
ğ‘› log ğ‘‘

p
4
1
,
/

(cid:17)
ğ‘˜ log ğ‘‘

.

n(cid:0)
Remark C.4. The same adversary also fools the limited exhaustive search algorithm from [BKW20b]
that runs in time ğ‘›ğ‘‚

) up to some very large ğ‘¡ (say, up to some ğ‘¡ = ğ‘›Î©

1
)).

p

o

(cid:1)

(

(

ğ‘¡

Proof of Theorem C.3. Let ğ‘ =

ğ¸

k

k1

â†’

2. We condition our analysis on the event that

ğ‘–

âˆ€

ğ‘‘

]

âˆˆ [

ğ‘–

âˆ€

ğ‘‘

]

âˆˆ [

ğ‘›

40

ğ‘› log ğ‘‘, ğ‘›

2

ğ‘Šğ‘–

k

k
âˆˆ
h
2 6 ğ‘›
ğ‘¢0k
k
ğ‘¢0, ğ‘Šğ‘–

i

h

+
6 10

âˆ’
p
100âˆšğ‘›

ğ‘› log ğ‘‘

+

40

ğ‘› log ğ‘‘

.

p

i

p
which happen with probability at least 0.99 by Fact G.4. Denote with ğ‘’1, . . . , ğ‘’ğ‘‘ the standard basis
vectors in â„ğ‘‘. Notice that, by construction of ğ¸, for ğ‘–

. Thus,

supp

ğ‘‘

ğ‘£0}

{

, ğ¸ğ‘’ğ‘–
ğ¸ğ‘’ğ‘– k
k

= ğ‘Š ğ‘’ğ‘–
ğ‘Š ğ‘’ğ‘– k
k

> ğ‘›

On the other hand, for ğ‘—

supp

,

ğ‘£0}

{

âˆˆ

2 =

ğ‘Š

ğ‘Œğ‘’ğ‘–

k

k

+

ğ¸

+
ğ‘
ğ‘Šğ‘–
ğ‘2

(cid:16)

1

(cid:18)
ğ‘Šğ‘–

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
k
(cid:13)
> ğ‘›

=

+
2

k

+

k
ğ‘2

+
ğ‘âˆšğ‘›

+

+

âˆˆ [

] \

T
ğ›½ğ‘¢0ğ‘£0

ğ‘’ğ‘–

2

(cid:17)

p

ğ‘Šğ‘–

2

(cid:13)
(cid:13)
(cid:13)

ğ‘Šğ‘–

k (cid:19)

+
ğ‘âˆšğ‘›

(cid:13)
(cid:13)
2ğ‘
(cid:13)
k
(cid:13)
ğ‘‚

âˆ’

ğ‘‚

âˆ’

k
ğ‘› log ğ‘‘

(cid:16)p
ğ‘› log ğ‘‘

.

(cid:17)

(cid:16)p

(cid:17)

ğ‘Œğ‘’ğ‘—

2 =

2

ğ‘Šğ‘—

ğ›½
ğ‘˜ k

2

ğ‘¢0k

+

ğ›½
ğ‘˜ h

ğ‘Šğ‘— , ğ‘¢0i

+ r

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
6 ğ‘›

(cid:13)
ğ‘›ğ›½
(cid:13)
ğ‘˜ +

+

ğ›½ğ‘› log ğ‘‘
ğ‘˜

!

+ r

ğ‘‚

ğ‘› log ğ‘‘

p

85

 
where the last step follows as ğ›½ = ğ‘œ

ğ‘˜

(

. Combining the two inequalities,
)

6 ğ‘›

ğ‘›ğ›½
ğ‘˜ +

+

ğ‘‚

ğ‘› log ğ‘‘

(cid:16)p

(cid:17)

ğ‘Œğ‘’ğ‘–

k

2

k

âˆ’

ğ‘Œğ‘’ğ‘—

2 > ğ‘âˆšğ‘›

ğ‘›ğ›½
ğ‘˜ âˆ’

âˆ’

which is larger then zero whenever,

(cid:13)
(cid:13)

(cid:13)
(cid:13)

ğ‘‚

ğ‘› log ğ‘‘

(cid:16)p

(cid:17)

ğ‘ > ğ‘‚

ğ›½âˆšğ‘›

ğ‘˜ +

(cid:18)

log ğ‘‘

.

(cid:19)

p

(cid:3)

C.3 Covariance Thresholding is Fragile

In this section, we show how in model 1.1 the Covariance Thresholding algorithm fails to output a
good estimation of the vector ğ‘£0 in the presence of an adversarial distribution. Speciï¬cally, we will

show that the algorithm fails for ğ‘˜ >

. This bound is signiï¬cant in the sense that already

ğ‘› log ğ‘‘
ğ‘˜2

ğ¸

q
k

2
1
â†’

2

k

ğ¸

for
observed in the Wigner model, we omit this proof since it is simpler than in the Wishart model.

ğ›½ğ‘›
ğ‘‘ , the algorithm breaks. We remark that a similar phenomenon can also be

2 = ğ‘‘ğ‘œ

k1

1
)

â†’

k

(

q

Recall that the central idea behind Covariance Thresholding is to threshold entries of the
empirical covariance matrix. The thresholding operation should remove noise while leaving the
submatrix ğ›½
will then be close to the
sparse vector. The key observation behind the adversary is that it is possible to plant a matrix ğ¸
with small norm
has many large
k1
eigenvalues with eigenspace far from ğ‘£0.

T untouched. The top eigenvector of ğœ‚ğœ

2 such that the thresholded covariance matrix ğœ‚

2ğ‘£0ğ‘£0

(cid:1)
ğ‘ŒTğ‘Œ

ğ‘¢0k

ğ‘ŒTğ‘Œ

ğ‘›Id

ğ‘›Id

âˆ’

âˆ’

ğ¸

â†’

k

k

(cid:0)

(cid:0)

(cid:1)

Consider the Covariance Thresholding algorithm:

Algorithm C.5 (Standard Covariance Thresholding).

Input: Threshold ğœ, sample matrix ğ‘Œ =

ğ‘¢0ğ‘£ğ‘‡
ğ‘‘ where ğ‘£0 is ğ‘˜-sparse, ğ‘¢0
and ğ‘Š have i.i.d subgaussian entries of mean 0 and variance 1 and ğ¸ has column norms
bounded by ğ‘.

0 +

â„ğ‘›

ğ‘Š

p

+

ğ¸

âˆˆ

ğ›½

Ã—

Â·

Estimate: The sparse vector ğ‘£0.

Operation:

1. Compute the thresholded matrix ğœ‚ğœ
ğ‘ŒTğ‘Œ
(cid:0)

2. Output a top eigenvector

ğ‘£ of ğœ‚ğœ
Ë†

âˆ’

âˆ’
ğ‘›Id

ğ‘›Id

.

.

(cid:1)

ğ‘ŒTğ‘Œ

The main result of the section is the Theorem below. Its signiï¬cance is to be read under this
) large
)
will not be correlated with ğ‘£0.

perspective: it shows that there exists an adversary that can plant several (i.e. ğœ”
eigenvalues, as a consequence the top eigenvectors of ğœ‚ğœ

log ğ‘‘
(

ğ‘ŒTğ‘Œ

ğ‘›Id

(cid:0)

(cid:1)

âˆ’

(cid:0)

(cid:1)

86

âˆˆ [

]

Theorem C.6. Suppose that ğ‘˜ 6 âˆšğ‘‘ and log10 ğ‘‘ 6 ğ‘› 6 ğ‘‘. Let ğ‘Œ be of the form 1.1 for a ï¬‚at vector ğ‘£0. Let
ğ‘Ÿ

â„ be such that 2âˆšğ‘› 6 ğœ 6 ğ‘œ

be such that ğœ”

ğ‘› log ğ‘‘

log ğ‘‘

as ğ‘‘

ğ‘›

.

Then with probability at least 1
ğ›½ğ‘›
ğ‘‘ and orthogonal vectors ğ‘§1, . . . , ğ‘§ğ‘Ÿ such that

(cid:0)
2 6 ğ‘‘ğ‘œ

â†’ âˆ

(cid:1)
1
)

âˆ’

column norm

ğ¸

(

) there exists an adversarial matrix ğ¸ with maximal
(cid:16)p

(cid:17)

6 ğ‘Ÿ 6 ğ‘‘ğ‘œ
ğ‘œ

(

1
) and ğœ
(as ğ‘‘
1
)
(

âˆˆ

â†’ âˆ

k

k1

â†’

ğ‘–

âˆ€

ğ‘Ÿ

,

]

âˆˆ [

q

1
ğ‘§ğ‘–

k

2 Â·

k

ğ‘§ğ‘–

T

ğœ‚ğœ

T
ğ‘Œ

ğ‘Œ

âˆ’

ğ‘›Id

T
ğ‘§ğ‘– > ğ‘£0

ğœ‚ğœ

T
ğ‘Œ

ğ‘Œ

ğ‘›Id

ğ‘£0

âˆ’

(cid:0)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

and

= 0.

ğ‘§ğ‘– , ğ‘£0i
h
The theorem shows that with these adversarial perturbations the ï¬rst ğ‘Ÿ eigenvectors of the
thresholded covariance matrix are uncorrelated with the sparse vector ğ‘£0. Notice that for ğ›½ > 1

ğ›½ğ‘›
a perturbation resilient algorithm should succeed with perturbations bounded by
ğ‘˜ , that is,
much larger (in absolute value) than the ones used to fool Covariance Thresholding. In par-

q

ticular, for ğ›½ = Î˜

ğ¸

k1
k
ceed for

â†’

2 6 ğ‘‘âˆ’
ğ‘œ
4
1
+
/
(
2 6
ğ¸
k1

â†’

k

log ğ‘‘
ğ‘˜2

Theorem C.6 implies that already with perturbations satisfying

ğ‘˜
âˆšğ‘›
(cid:18)
q
4 the algorithm fails, while a perturbation resilient algorithm would suc-
)ğ‘›1
1
/
4
ğ‘›1
Ëœğ‘‚
/

(cid:19)

.

Before showing the proof, we provide some intuition.

(cid:0)

(cid:1)

Algorithm Intuition. Letâ€™s ignore cross-terms for a moment and consider the Wishart model
with no adversarial distribution. Then the centered empirical Covariance Matrix looks like

T
ğ‘Œ

ğ‘Œ

ğ‘›Id

âˆ’

â‰ˆ

T

ğ‘Š

ğ‘Š

ğ‘›Id

+

âˆ’

T
ğ›½ğ‘›ğ‘£0ğ‘£0

.

ğ‘› log ğ‘‘
ğ‘˜2
ğ‘ŠTğ‘Š
q
ğ‘›Id
> ğœ whenever ğ‘–, ğ‘—
(cid:1)

âˆ’

(cid:0)

for some large enough constant ğ¶ > 0, then
will be larger than ğœ. 25 On the other hand, for
T will survive

, many entries of ğ›½ğ‘›ğ‘£0ğ‘£0

supp

âˆˆ

ğ‘£0}

{

ğ‘›

âˆ’

Î˜

If we set the threshold ğœ = ğ¶
ğ‘˜4 entries in
ğ‘‘2 exp
ğœ2
/
ğ›½ & ğ‘˜
log ğ‘‘
ğ‘˜2 as
(cid:1)(cid:3)
(cid:2)
(cid:0)
âˆšğ‘›
the thresholding. This means that,
(cid:12)
q
(cid:12)
(cid:12)

â‰ˆ
T
ğ›½ğ‘›ğ‘£0ğ‘£0

ğ‘–ğ‘—

(cid:0)

(cid:1)

(cid:12)
(cid:12)
(cid:12)
T
ğ‘Œ

ğœ‚ğœ

(cid:0)

ğ‘Œ

ğ‘›Id

â‰ˆ

T

ğ‘Š

ğ‘Š

ğ‘›Id

ğ‘†

[

] +

âˆ’

T
ğ›½ğ‘›ğ‘£0ğ‘£0

âˆ’

ğ‘‘

has cardinality approximately ğ‘˜4. If the entries were independent, since the
where ğ‘†
fourth moment of each entry is not much larger than the second moment, standard spectral matrix
bounds suggest

] Ã— [

âŠ† [

ğ‘‘

]

(cid:1)

(cid:0)

(cid:1)

where ğœ 6 ğœ exp

ğ¶ğœ2
10ğ‘›

6 âˆšğ‘›

i

âˆ’

h

(cid:13)
ğ‘˜
(cid:13)
âˆšğ‘‘

Â·

25To see this, recall that in a ğ‘‘
log ğ‘‘2

ğ‘˜4 . While entries in ğ‘Š

ğ‘Š

T

âˆ’

q

T

ğ‘Š

ğ‘Š

ğ‘›Id

ğ‘†

[

âˆ’

6 ğ‘‚

ğœâˆšğ‘‘

,

(cid:0)
is a standard deviation of each entry. Hence we get

(cid:1)

(cid:16)

(cid:17)

T

ğ‘Š

ğ‘Š

ğ‘›Id

ğ‘†

[

âˆ’

6 ğ‘‚

ğ‘˜âˆšğ‘›

,

(cid:17)
ğ‘‘ Gaussian matrix, with high probability there are at most ğ‘˜4 entries larger than

(cid:16)

(cid:1)

(cid:0)

(cid:13)
(cid:13)

Ã—
ğ‘›Id are dependent, a similar bound will hold.

]
(cid:13)
(cid:13)

]
(cid:13)
(cid:13)

87

and

T
ğ›½ğ‘›ğ‘£0ğ‘£0

= ğ›½ğ‘›.

In conclusion, for ğ›½ & ğ‘˜
âˆšğ‘›

(cid:13)
(cid:13)
The main technical diï¬ƒculty here is that the entries of ğ‘ŠTğ‘Š are not independent. In [DM14]

ğ‘˜2 the top eigenvector of ğœ‚ğœ

will be close to ğ‘£0.

log ğ‘‘

ğ‘ŒTğ‘Œ

ğ‘›Id

(cid:13)
(cid:13)

âˆ’

q

the authors provide a method to bound the spectral norm of the thresholded matrix26.

(cid:0)

(cid:1)

Adversarial Strategy. Now we provide intuition on how to choose ğ¸ such that with constant
probability there exists a vector ğ‘§ orthogonal to ğ‘£0 for which

T
ğ‘Œ

ğ‘Œ

ğœ‚ğœ

ğ‘›Id

âˆ’

T
& ğ‘£0

ğœ‚ğœ

T
ğ‘Œ

ğ‘Œ

ğ‘›Id

ğ‘£0.

âˆ’

ğ‘§T
ğ‘§

k

k

(cid:0)

ğ‘§
ğ‘§

k

k

(cid:1)

âˆˆ
supp

â„ğ‘› be a randomly chosen unit vector orthogonal to ğ‘¢0, let ğ‘§ be a vector such that supp
1

, ğ‘§ğ‘– = ğœğ‘–ğ‘ for some ğ‘

Let ğ‘¥
ğ‘‘
âˆˆ
[
deï¬ne the adversarial matrix as ğ¸ := ğ‘¥ğ‘§T, notice that
ğ¸
the entry ğ‘–ğ‘— of the centered empirical covariance matrix

to be set later and ğœğ‘–
. For ğ‘–, ğ‘—
ğ‘
supp
/
ğ‘›Id

âˆ¼ {Â±
ğ‘§
}

and for ğ‘–

ğ‘§
{
}
. We
}
, consider

ğ‘£0}

supp

kâˆ

] \

=

âˆˆ

ğ‘§

{

{

{

}

k

(cid:0)

(cid:1)

â„
âˆˆ
+
6
Ëœğ‘‚
ğ‘ŒTğ‘Œ
(cid:0)
âˆ’

âˆšğ‘›
,
(cid:1)

T
ğ‘Œ

ğ‘Œ

âˆ’

ğ‘›Id

ğ‘–ğ‘—

=

ğ‘¤ğ‘– , ğ‘¤ ğ‘—

h

i + h

ğ‘¤ğ‘–, ğ‘¥

i + h

(cid:0)
ğ‘¥, ğ‘¤ ğ‘—

(cid:1)
.

ğ‘§ğ‘– ğ‘§ ğ‘—

i +

ğ‘¤ğ‘– , ğ‘¤ ğ‘—

ğ‘§ğ‘– ğ‘§ ğ‘—

,

h

i +

(cid:1)

(cid:0)

(cid:12)
(cid:12)
(cid:12)
by construction of ğ‘§, the term ğ‘§ğ‘– ğ‘§ ğ‘— is symmetric and bounded by ğ‘2. Hence for ğ‘2 = ğ‘œ
(cid:12)
âˆšğ‘›
,
(cid:12)
(
)
the thresholding of entry
ğ‘–ğ‘— will depend almost only on the Gaussian contribution
âˆ’
ğ‘ŠTğ‘Š
be the set of non-zero entries in ğœ‚ğœ
. By independence
of ğ‘§ and ğ‘Š, and since ğ‘† dependence of ğ‘§ is very limited, we expect, as in our previous discussion,
(cid:0)
ğ‘†
|

& ğ‘˜4. Now consider the quadratic form

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ğ‘ŒTğ‘Œ
ğ‘‘

ğ‘–ğ‘—. Let ğ‘†

ğ‘ŒTğ‘Œ

(cid:0)
] Ã— [

ğ‘›Id

ğ‘›Id

ğ‘›Id

âŠ† [

âˆ’

âˆ’

ğ‘‘

(cid:12)
(cid:12)

(cid:12)
(cid:12)

]

(cid:1)

(cid:0)

(cid:1)

(cid:1)

|

ğ‘§T
ğ‘§

T
ğ‘Œ

ğ‘Œ

ğœ‚ğœ

ğ‘›Id

ğ‘†

[

]

âˆ’

ğ‘§
ğ‘§

ğ‘§T
ğ‘§

â‰ˆ

T
ğ‘§ğ‘§

ğ‘†

[

]

ğ‘§
ğ‘§

k

+

k

ğ‘§T
ğ‘§

T

ğ‘Š

ğ‘Š

ğ‘›Id

ğ‘†

[

]

âˆ’

ğ‘§
ğ‘§

.

k

k

k
As argued in the previous paragraph C.3, ğ‘§T

k

k

k

k

k

(cid:1)

(cid:0)

(cid:0)

(cid:1)
ğ‘ŠTğ‘Š

6 ğ‘‚

ğ‘˜âˆšğ‘›

(cid:1)
. On the other hand,

ğ‘§

k

k

(cid:0)

ğ‘§T
ğ‘§

k

k

T
ğ‘§ğ‘§

ğ‘†

[

]

(cid:0)

(cid:1)

ğ‘§
ğ‘§

k

k

= 1
ğ‘§2

k

ğ‘§2
ğ‘– ğ‘§2

ğ‘—

2

k

ğ‘†
ğ‘–,ğ‘—
Ã•(
)âˆˆ

k

ğ‘†

(cid:0)

k
ğ‘§
ğ‘§

k

k

[
]
ğ‘4 & ğ‘˜4
ğ‘‘

ğ‘›Id

âˆ’

ğ‘†

(cid:1)

|

2

= |
ğ‘§
k

k

(cid:0)
ğ‘2 & ğ‘‘1
âˆ’

(cid:1)
ğ‘œ
(

)ğ‘2.
1

For the signal we instead have ğ‘£0
ğ‘›Id
the top eigenvector of ğœ‚
ğ‘› > ğ‘‘1
1
) and that
âˆ’
(
k
are bounded by ğ‘› ğ‘œ
1
)/

ğ‘ŒTğ‘Œ
.

âˆ’
Ëœğ‘‚

ğ¸

ğ‘

ğ‘œ

(

Tğœ‚ğœ

ğ‘ŒTğ‘Œ

ğ‘†

ğ‘›Id

ğ‘£0 . ğ›½ğ‘›. It follows that setting ğ‘ &

[

]

âˆ’

1
)
will not achieve constant correlation with ğ‘£0. Recall now that
ğ‘˜2 6 ğ‘› ğ‘œ

1
), adversarial perturbations

(cid:0)
(cid:1)
. Hence for ğ›½
)

log ğ‘‘

ğ‘˜
âˆšğ‘›

ğ‘‘1
âˆ’

q

(

(

ğ›½ğ‘›
ğ‘œ

âˆšğ‘›
(cid:1)

(cid:0)
kâˆ
q
âˆšğ‘› are enough to fool the algorithm.

â‰ˆ

/

(

Remark C.7. While this adversarial matrix is enough to break Covariance Thresholding it also
allows an easy ï¬x. Indeed, although the top eigenvector is now almost uncorrelated with ğ‘£0, the
eigenspaces spanned by two largest eigenvectors contain a vector close to ğ‘£0 and a brute-force
search over such space can be performed in polynomial time. The same approach however can

26Formally, in [DM14] the authors provided a proof for a matrix obtained applying soft-thresholding. As we will see

these can easily be extended to the hard-thresholded matrix ğœ‚

ğ‘Š

(cid:16)

88

T

ğ‘Š

âˆ’

ğ‘›Id

.

(cid:17)

be used to build an adversarial matrix ğ¸ such that there exist vectors ğ‘§1, . . . , ğ‘§ğ‘Ÿ for which, with
constant probability

ğ‘–

ğ‘Ÿ

]

âˆˆ [

T
ğ‘Œ

ğ‘Œ

ğœ‚ğœ

ğ‘›Id

âˆ’

T
& ğ‘£0

ğœ‚ğœ

T
ğ‘Œ

ğ‘Œ

ğ‘›Id

ğ‘£0.

âˆ’

ğ‘§ğ‘–T
ğ‘§ğ‘–

k

k

ğ‘§ğ‘–
ğ‘§ğ‘–

k

k

(cid:0)

(cid:1)
The idea is to chose ğ‘¥1, . . . , ğ‘¥ğ‘Ÿ to be orthonormal vectors orthogonal to ğ‘¢0, and ğ‘§1, . . . , ğ‘§ğ‘Ÿ with
non-intersecting supports and the same structure as before. This latter choice of ğ¸ implies that the
space containing eigenvectors associated with large eigenvalues has now dimension at least Î©
ğ‘Ÿ
.
)
(
For ğ‘Ÿ > ğœ”
, brute-force search of a vector close to ğ‘£0 in this space requires super-polynomial
time.

log ğ‘‘

(cid:1)

(cid:0)

(cid:0)

(cid:1)

C.3.1 Proving covariance thresholding fragile

Now we formally prove the theorem. First we deï¬ne the adversarial matrix.

Deï¬nition C.8 (Adversarial matrix). For ğ‘ > 1, ğ‘Ÿ
and ğ‘£0
0, Idğ‘›
0, 1
ğ‘
(
)
(
â„ğ‘› be unit vectors that are
k-sparse, the adversarial matrix is built as follows. Let ğ‘¥1, . . . , ğ‘¥ğ‘Ÿ
independent of ğ‘Š such that for distinct ğ‘–, ğ‘—
in sets
supp
, let ğ‘§ğ‘– be the vector with support ğ‘ğ‘– such that:

ğ‘1, . . . , ğ‘ğ‘Ÿ of cardinality

= 0. Partition the set

âˆˆ [
. For each ğ‘–

ğ‘‘, ğ‘¢0 âˆ¼

â„•, ğ‘Š

ğ‘¥ğ‘– , ğ‘¥ ğ‘—

ğ‘£0}

] \

ğ‘

âˆ¼

âˆˆ

âˆˆ

ğ‘£0

ğ‘‘

{

ğ‘Ÿ

ğ‘Ÿ

i

h

Ã—

[

]

)

ğ‘›

ğ‘‘

,

{

}|

supp
ğ‘Ÿ

âˆ’|

âˆˆ [

]

ğ‘™

âˆ€

âˆˆ

ğ‘ğ‘– ,

=

ğ‘§ğ‘–
ğ‘™

ğ‘

ğ‘

(

âˆ’

Then

> 0

if

ğ‘¤ğ‘™ , ğ‘¥ğ‘–
otherwise.

i

h

ğ¸ :=

ğ‘¥ğ‘– ğ‘§ğ‘–T

.

ğ‘Ÿ
Ã•ğ‘–
]
âˆˆ[

Notice that
Theorem C.6 follows immediately combining Theorem C.9, and Lemma C.10.

k1

â†’

k

ğ¸

2 = ğ‘âˆšğ‘Ÿ.

Theorem C.9. Let ğ‘Œ be of the form 1.1 with ğ¸ constructed as in deï¬nition C.8 with ğœ”
and ğ‘ 6 4âˆšğ‘›. Assume that ğ‘‘ > ğ‘› > log10 ğ‘‘ and that ğ‘˜ 6 âˆšğ‘‘. Let 2âˆšğ‘› 6 ğœ 6 ğ‘œ
1
) there exists a subset ğ‘…

Then with probability at least 1

of size at least ğ‘Ÿ
(cid:16)p

2ğ‘‘âˆ’

Î©

ğ‘Ÿ

(

log ğ‘‘
(
)
ğ‘› log ğ‘‘

1
)

(

6 ğ‘Ÿ 6 ğ‘‘ğ‘œ
as ğ‘‘

.

â†’ âˆ

10 such that

(cid:17)

âŠ† [

]

âˆ’

ğ‘…,

ğ‘–

âˆ€

âˆˆ

ğ‘§ğ‘–

T

ğœ‚ğœ

T
ğ‘Œ

ğ‘Œ

âˆ’

2 Â·

ğ‘›Id

ğ‘§ğ‘– > ğ‘2

1
)

(

ğ‘œ

ğ‘‘1
âˆ’
ğ‘Ÿ

.

Â·

(cid:0)
Lemma C.10. Suppose the conditions of Theorem C.9 are satisï¬ed and that the entries of ğ‘£0 are from
. Then with probability 1
0,

and ğ‘› > ğœ”

as ğ‘‘

âˆšğ‘˜

ğ‘‚

10

(cid:1)

(cid:0)

(cid:1)

1
/

Â±

{

}

log ğ‘‘
(

)

ğ‘‘âˆ’

(

)

âˆ’

ğœ‚ğœ(
of Lemma C.10. With probability 1

(cid:12)
(cid:12)

T
ğ‘£0

ğ‘‚

(

âˆ’

ğ‘‘âˆ’

âˆ’

+

6 ğ‘‚

(cid:12)
(cid:12)

ğ‘˜

10

p

ğ‘£0

ğ‘› log ğ‘‘

ğ‘›Id
)
(cid:16)
the entries of ğœ‚ğœ(
ğ›½ğ‘› log ğ‘‘
ğ›½ğ‘›
ğ‘˜ +
ğ‘˜

6 ğ‘‚

)

ğ‘Œğ‘‡ğ‘Œ

!

+ r

(cid:18)

ğ‘‚

ğ›½ğ‘›
ğ‘˜ +

ğ‘› log ğ‘‘

p

ğ‘› log ğ‘‘

.

(cid:19)

p

ğ›½ğ‘›

.

(cid:17)
ğ‘›Id
)

âˆ’

are bounded by

1
ğ‘§ğ‘–

k

k

â†’ âˆ
ğ‘Œğ‘‡ğ‘Œ

89

 
Since ğ‘£0 has at most ğ‘˜ nonzero entries,

T
ğ‘£0

ğ‘Œğ‘‡ğ‘Œ

ğœ‚ğœ(

ğ‘›Id
)

ğ‘£0

âˆ’

6 ğ‘˜

ğ‘Œğ‘‡ğ‘Œ

ğœ‚ğœ(

Â· k

âˆ’

ğ‘›Id

)kâˆ

(cid:12)
(cid:12)

(cid:12)
(cid:12)

6 ğ‘‚

ğ›½ğ‘›

(cid:16)

+

ğ‘˜

ğ‘› log ğ‘‘

.

p

(cid:17)

(cid:3)

Tp prove Theorem C.9 we make use of intermediate steps C.11-C.12. Our plan is to show that
ğ‘›Id. So, we start

many entries of ğ‘§ğ‘– ğ‘§ğ‘–T survive the thresholding due to the contribution of ğ‘ŠTğ‘Š
our analysis lower bounding the number of entries of ğ‘ŠTğ‘Š

âˆ’

The following lemma shows that for each vector ğ‘§ğ‘–, many entries in supp

âˆ’

ğ‘›Id that are above the threshold.
ğ‘§ğ‘–

supp

ğ‘§ğ‘–

will

Ã—

â„ consider ğ‘Œ sampled from model 1.1 with ğ¸ as in C.8. For some 10 6 ğ‘ 6 ğ‘‘ğ‘œ
deï¬ne the set

1
)

(

(cid:8)

(cid:9)

(cid:8)

(cid:9)

survive the thresholding.

Lemma C.11. For any ğ‘, ğ‘Ÿ
âˆˆ
let ğœ =
ğ‘Ÿ
]

ğ‘› log ğ‘. For ğ‘–

âˆˆ [

p

ğ‘†ğ‘– :=

ğ‘—, ğ‘™

) âˆˆ

(

n

Then with probability at least 1

exp

âˆ’

(cid:0)

supp

ğ‘§ğ‘–

Ã—

(cid:8)
ğ‘‘1
âˆ’

ğ‘œ

(cid:9)
1
)
(

,

supp

ğ‘§ğ‘–

ğ‘— â‰  ğ‘™,

T
ğ‘Œ

ğ‘Œ

(cid:8)

(cid:9) (cid:12)
(cid:12)
(cid:12)

(cid:0)

âˆ’

ğ‘›Id

ğ‘—ğ‘™

> ğœ

.

(cid:1)

o

(cid:1)
ğ‘†ğ‘–

|

|

>

ğ‘‘2
1000ğ‘Ÿ2 ğ‘10

.

Proof. Consider an oï¬€ diagonal entry ğ‘—ğ‘™ of ğœ‚ğœ
Î©
Since with probability at least 1
(

2 exp

âˆ’

âˆ’

ğ‘ŒTğ‘Œ
ğ‘›0.2
(cid:0)

)

ğ‘›Id

âˆ’
> 1

(cid:1)
âˆ’

such that ğ‘—, ğ‘™
2ğ‘‘Î©
1
ğ‘¤ ğ‘—, ğ‘¥ğ‘™
),

(

h

for some ğ‘–

ğ‘§ğ‘–
supp
6 ğ‘›0.1, we get
(cid:8)

(cid:9)

âˆˆ
i

ğ‘Ÿ

.

]

âˆˆ [

ğ‘¤ ğ‘—, ğ‘¤ğ‘™

ğ‘¤ ğ‘— , ğ‘¥

ğ‘§ğ‘–
ğ‘™ + h

i

ğ‘¤ğ‘™ , ğ‘¥

i + h

(cid:2)
ğ‘§ğ‘–
ğ‘— +

i

ğ‘— ğ‘§ğ‘–
ğ‘§ğ‘–

ğ‘™

â„™

h

(cid:16)(cid:12)
(cid:12)
(cid:12)

(cid:3)
> ğœ

(cid:12)
(cid:12)
(cid:12)

> â„™

1
âˆšğ‘›
(cid:16)
> 1
(cid:12)
(cid:12)
10ğ‘10

h

.

(cid:17)

ğ‘¤ ğ‘—, ğ‘¤ğ‘™

i

(cid:12)
(cid:12)

> 2

log ğ‘

p

Î©

ğ‘‘âˆ’

1
)

(

âˆ’

(cid:17)

For ï¬xed ğ‘§ğ‘– and ï¬xed row ğ‘—
each other. Since ğ‘Ÿ 6 ğ‘‘ğ‘œ

ğ‘ŒTğ‘Œ
ğ‘™ such that
for each ğ‘§ğ‘–, ğ‘†ğ‘– >

(cid:0)

ğ‘›Id
ğ‘—ğ‘™
âˆ’
ğ‘‘2
1000ğ‘Ÿ2 ğ‘10 .
(cid:1)

ğ‘‘

, the

ğ‘¤ ğ‘—, ğ‘¤ğ‘™

âˆˆ [

]

h

(

1
), with probability 1
> ğœ is at least
ğ‘‘

(for diï¬€erent ğ‘™
ğ‘‘
100ğ‘Ÿ ğ‘10

exp

âˆˆ
= 1

supp

exp

âˆ’

i

âˆ’

ğ‘§ğ‘–
) are independent from
}
{
ğ‘œ
ğ‘‘1
âˆ’

number of diï¬€erent

1
)

(

1000ğ‘Ÿ ğ‘10 . Hence if with probability at least 1

(cid:1)

(cid:0)

h

i

exp

ğ‘œ

ğ‘‘1
âˆ’

(

âˆ’

(cid:0)

,

1
)
(cid:3)
(cid:1)

The last ingredient needed for Theorem C.9 is a proof that the cross-terms in the quadratic

form ğ‘§ğ‘–Tğœ‚ğœ

ğ‘ŒTğ‘Œ

âˆ’

ğ‘›Id

ğ‘§ğ‘– do not remove the contribution of the adversarial vector.

(cid:0)

(cid:1)

Lemma C.12. Let ğ‘Œ be sampled from model 1.1 with ğ¸ as in Deï¬nition C.8. Let ğ‘†ğ‘– be as in Lemma C.11
Then with probability at least 1
2 ,

ğ‘§ğ‘–

T

T

ğ‘Š

ğ‘Š

T

ğ‘¥ğ‘– ğ‘§ğ‘–T

ğ‘Š

ğ‘§ğ‘– ğ‘¥ğ‘–T

ğ‘Š

+

ğ‘§ğ‘– > 0.

ğ‘†ğ‘–

[

]

+

Proof. For simplicity of the notation we will refer to ğ‘¥ğ‘– , ğ‘§ğ‘– simply as ğ‘¥, ğ‘§. Opening up the sum,

(cid:0)

(cid:1)

(cid:0)

(cid:1)

T
ğ‘§

T

ğ‘Š

ğ‘Š

T

T
ğ‘¥ğ‘§

ğ‘Š

T
ğ‘§ğ‘¥

ğ‘Š

+

+

(cid:0)

ğ‘†ğ‘–

[

]

(cid:1)

ğ‘§ = 2

ğ‘†ğ‘–
ğ‘—,ğ‘™
Ã•(
)âˆˆ

90

ğ‘¤ ğ‘— , ğ‘¤ğ‘™

ğ‘§ ğ‘— ğ‘§ğ‘™

i

+ h

ğ‘¤ ğ‘—, ğ‘¥

i

h

ğ‘2ğ‘§ ğ‘—

ğ‘¤ğ‘™ , ğ‘¥

ğ‘2ğ‘§ğ‘™

i

+ h

> 2

ğ‘†ğ‘–
ğ‘—,ğ‘™
Ã•(
)âˆˆ

> 2

ğ‘†ğ‘–
ğ‘—,ğ‘™
Ã•(
)âˆˆ
ğ‘2ğ‘§ ğ‘— > 0,

h

h

h

ğ‘¤ ğ‘—, ğ‘¤ğ‘™

ğ‘§ ğ‘— ğ‘§ğ‘™

i

ğ‘¤ ğ‘—,

Id

T
ğ‘¥ğ‘¥

âˆ’

ğ‘§ ğ‘—ğ‘§ğ‘™ ,

ğ‘¤ğ‘™

i

(cid:0)
ğ‘¤ğ‘™, ğ‘¥

i

(cid:1)

ğ‘2ğ‘§ğ‘™ > 0. So it is enough to prove that

using the fact that by construction

ğ‘¤ ğ‘— , ğ‘¥

h

i

ğ‘‘â€²

â„™

ğ‘—,ğ‘™

Ã•ğ‘—=1 Ã•(
ğ‘†ğ‘–
Â©

)âˆˆ

ğ‘¤ ğ‘—,

Id

h

âˆ’

T
ğ‘¥ğ‘¥

ğ‘¤ğ‘™

i

ğ‘§ ğ‘— ğ‘§ğ‘™ > 0

> 1
2

.

(cid:0)

(cid:1)

Let

and

ğ‘ ğ‘—ğ‘™ =

ğ‘¤ ğ‘—,

h

Â«
Id

âˆ’

T
ğ‘¥ğ‘¥

ğ‘¤ğ‘™

i

ğ‘§ ğ‘— ğ‘§ğ‘™ =

(cid:0)

(cid:1)

ğ‘¤ ğ‘—, ğ‘¤ğ‘™

i âˆ’ h

h

(cid:0)

Âª
Â®
Â¬
ğ‘¤ ğ‘—, ğ‘¥
ih

ğ‘¤ğ‘™, ğ‘¥

ğ‘§ ğ‘— ğ‘§ğ‘™

i

Â·

(cid:1)

ğ‘ ğ‘—ğ‘™ =
=

(cid:0)
h

ğ‘¤ ğ‘— , ğ‘¥

h
ğ‘¤ ğ‘—, ğ‘¥

i

ğ‘§ğ‘™
i
ğ‘2ğ‘§ ğ‘—

+ h

+ h

ğ‘¤ğ‘™ , ğ‘¥

i
ğ‘¤ğ‘™ , ğ‘¥

ğ‘§ ğ‘—
+ h
ğ‘2ğ‘§ğ‘™

ğ‘¤ ğ‘—, ğ‘¥

ğ‘¤ğ‘–, ğ‘¥

ih
ğ‘¤ ğ‘—, ğ‘¥

i +
ğ‘¤ğ‘–, ğ‘¥

+ h

ih

ğ‘§ ğ‘— ğ‘§ğ‘™

ğ‘§ ğ‘—ğ‘§ğ‘™
(cid:1)

i

ğ‘§ ğ‘— ğ‘§ğ‘™
ğ‘4 .

Â·

+

i

ğ‘ ğ‘—ğ‘™

ğ‘ ğ‘—ğ‘™

> ğ‘2ğœ. Also notice that ğ‘ ğ‘—ğ‘™ > 0 and that with

Notice that
probability at least 1

) âˆˆ

ğ‘—, ğ‘™

(

Since

supp

ğ‘§

{

|

}|

ğ‘†ğ‘– if and only if ğ‘— â‰  ğ‘™ and
Î©
(

2 exp

ğ‘›0.2

âˆ’

)

+
, ğ‘ ğ‘—ğ‘™ < ğ‘2ğœ.
(cid:12)
(cid:12)

âˆ’
= ğ‘‘â€², without loss of generality assume supp

(cid:12)
(cid:12)

(cid:2)

(cid:3)

ğ‘§

{

}

. For ğ‘

ğ‘‘â€²]

ğ‘‘â€² âˆ’

1
]

âˆˆ [

deï¬ne

ğ‘‘â€²âˆ’
1

ğ‘‡âˆ—ğ‘ :=

Ã•ğ‘—=ğ‘ Ã•ğ‘—<ğ‘™6ğ‘‘â€² s.t.
>ğ‘2ğœ
ğ‘ ğ‘—ğ‘™
|

|

Id

âˆ’

h

(cid:0)

T
ğ‘¥ğ‘¥

ğ‘¤ ğ‘— ,

Id

T
ğ‘¥ğ‘¥

âˆ’

ğ‘§ ğ‘— ğ‘§ğ‘™ =

ğ‘¤ğ‘™

i

(cid:1)

(cid:0)

(cid:1)

and

ğ‘‘â€²âˆ’
1

ğ‘‡ğ‘ :=

Let ğ‘‡ğ‘‘â€²

= ğ‘‡ğ‘‘â€²

= 0. For ğ‘—

Ã•ğ‘—=ğ‘ Ã•ğ‘—<ğ‘™6ğ‘‘â€² s.t.
>ğ‘2ğœ
ğ‘ ğ‘—ğ‘™+

ğ‘ ğ‘—ğ‘™

|

1
]

|
ğ‘‘â€² âˆ’
âˆˆ [
ğ‘‡âˆ—ğ‘—
ğ‘‡âˆ—ğ‘— âˆ’
1
+

Id

âˆ’

h

(cid:0)

T
ğ‘¥ğ‘¥

ğ‘¤ ğ‘— ,

Id

T
ğ‘¥ğ‘¥

âˆ’

ğ‘§ ğ‘— ğ‘§ğ‘™ =

ğ‘¤ğ‘™

i

(cid:1)

(cid:0)

(cid:1)

consider

=

h

(cid:0)

Ã•ğ‘—<ğ‘™6ğ‘‘â€² s.t.
>ğ‘2ğœ
ğ‘ ğ‘—ğ‘™
|
|

Id

âˆ’

T
ğ‘¥ğ‘¥

ğ‘¤ ğ‘— ,

Id

T
ğ‘¥ğ‘¥

âˆ’

ğ‘§ ğ‘— ğ‘§ğ‘™ .

ğ‘¤ğ‘™

i

(cid:1)

(cid:0)

(cid:1)

=

[
ğ‘‘â€²âˆ’
1

ğ‘ ğ‘—ğ‘™

ğ‘ ğ‘—ğ‘™ .

Ã•ğ‘—=ğ‘ Ã•ğ‘—<ğ‘™6ğ‘‘â€² s.t.
>ğ‘2ğœ
ğ‘ ğ‘—ğ‘™
|

|

ğ‘‘â€²âˆ’
1

Ã•ğ‘—=ğ‘ Ã•ğ‘—<ğ‘™6ğ‘‘â€² s.t.
>ğ‘2ğœ
ğ‘ ğ‘—ğ‘™+

ğ‘ ğ‘—ğ‘™

|

|

ğ‘¥ğ‘¥T

âˆ’

Id

ğ‘¥ğ‘¥T

ğ‘¤ ğ‘— does not inï¬‚uence on the condition

ğ‘¤ ğ‘— is symmetric around zero and independent from all ğ‘§ ğ‘— and all ğ‘¤ğ‘™ for ğ‘™ > ğ‘¤ ğ‘—. Moreover,
Id
âˆ’
> ğ‘2ğœ. It follows that the condi-
the sign of
(cid:0)
(cid:1)
tional disribution of ğ‘‡âˆ—ğ‘— given ğ‘§ ğ‘—, ğ‘§ğ‘™, ğ‘¤ğ‘™ for ğ‘™ > ğ‘— is symmetric around ğ‘‡âˆ—ğ‘—
1 and thus by induction
+
> 0
ğ‘‡âˆ—1 is symmetric around zero. It remains to show that â„™
, which is true since if
> ğ‘2ğœ,
ğ‘‡âˆ—1
(cid:3)
and any ğ‘ ğ‘—ğ‘™ < 0 such that

ğ‘‡1 > 0
)
(
> 0, then any ğ‘ ğ‘—ğ‘™ > 0 such that

> ğ‘‡1. Indeed, if ğ‘‡âˆ—1
ğ‘ ğ‘—ğ‘™

ğ‘‡âˆ—1
> ğ‘2ğœ satisï¬es
(cid:1)
(cid:0)

> 0, then ğ‘‡âˆ—1

> ğ‘2ğœ satisï¬es

> â„™
ğ‘ ğ‘—ğ‘™

> ğ‘2ğœ.

ğ‘ ğ‘—ğ‘™

ğ‘ ğ‘—ğ‘™

ğ‘ ğ‘—ğ‘™

ğ‘ ğ‘—ğ‘™

ğ‘ ğ‘—ğ‘™

+

(cid:1)

(cid:0)

|

|

|

|

|

|

|

+

|

|

|

We are now ready to prove Theorem C.9.

91

 
Proof of Theorem C.9. Let 10 6 ğ‘ 6 ğ‘‘ğ‘œ

(

ğ‘› log ğ‘. By construction of ğ‘§ğ‘–,

) so that ğœ =
1
T

ğ‘Š

ğ‘§ğ‘–T
T
ğ‘§ğ‘–T
ğ¸
(cid:0)
ğ‘§ğ‘–T
(cid:0)

T
ğ‘¢ğ‘£
T
ğ‘¢ğ‘£
T
ğ¸

ğ¸

ğ‘†ğ‘–

ğ‘†ğ‘–

ğ‘†ğ‘–

[

[

[

(cid:1)

(cid:1)

]

]

]

ğ‘§ğ‘– = 0
p
ğ‘§ğ‘– = 0
ğ‘§ğ‘– = ğ‘§ğ‘–T

ğ‘§ğ‘– ğ‘§ğ‘–T

ğ‘§ğ‘–.

ğ‘†ğ‘–

[

]

With probability 1

ğ‘‘Î©

(

âˆ’

1
) sum over diagonal entries is bounded by:

(cid:0)

(cid:0)

(cid:1)

(cid:1)

2

ğ‘§ğ‘–
ğ‘—

ğ‘¤ ğ‘—

k

2

k

âˆ’

ğ‘›

6 ğ‘‚

ğ‘2ğ‘‘

ğ‘› log ğ‘‘

6 ğ‘2ğ‘‘1.5
+

ğ‘œ

1
) .

(

Ã•ğ‘— (cid:16)

(cid:17)

(cid:0)
Notice that for diï¬€erent ğ‘–, ğ‘š
]
âˆˆ [
ğ‘§ğ‘š ğ‘¥ğ‘šTğ‘Š
ğ‘ŠTğ‘¥ğ‘š ğ‘§ğ‘šT
T
ğ‘†ğ‘š
+
]
)
0.1ğ‘Ÿ for at least ğ‘Ÿ
2âˆ’

ğ‘ŠTğ‘Š
ğ‘§ğ‘š
(
probability at least 1

+

ğ‘Ÿ

[

(cid:0)

(cid:1)

(cid:16)
the events

p
T
ğ‘§ğ‘–

(cid:17)
ğ‘ŠTğ‘Š

ğ‘§ğ‘– > 0 and
ğ‘§ğ‘š > 0 are independend. Hence, by Lemma C.12 with
(cid:0)
10 diï¬€erent ğ‘–

ğ‘ŠTğ‘¥ğ‘– ğ‘§ğ‘–T

ğ‘§ğ‘– ğ‘¥ğ‘–Tğ‘Š

ğ‘†ğ‘–

+

+

ğ‘Ÿ

]

[

(cid:1)

(cid:0)

(cid:1)

,

/

âˆˆ [

]

âˆ’
T
ğ‘§ğ‘–

(cid:0)

(cid:1)
ğ‘›Id

ğ‘†ğ‘–
(cid:1)
]
ğ‘†ğ‘–

]

[

[

(cid:1)

(cid:1)

T
ğ‘Œ
T

ğœ‚ğœ
ğ‘§ğ‘–
=
(cid:1)
(cid:0)
ğ‘§ğ‘–
>
(cid:0)
=ğ‘4
(cid:0)

T

(cid:1)
ğ‘†ğ‘–
(cid:1)

|
ğ‘‘2

ğ‘Œ
âˆ’
ğ‘§ğ‘– ğ‘§ğ‘–T
ğ‘§ğ‘– ğ‘§ğ‘–T
(cid:0)
.
(cid:0)
|

ğ‘§ğ‘–
ğ‘§ğ‘–
ğ‘§ğ‘–

ğ‘§ğ‘–T

T

ğ‘Š

ğ‘Š

T

ğ‘¥ğ‘§ğ‘–T

ğ‘Š

ğ‘§ğ‘– ğ‘¥ğ‘–T

ğ‘Š

+

ğ‘§ğ‘–

ğ‘†ğ‘–

[

]

+

+

(cid:0)

(cid:1)

By Lemma C.11

>

ğ‘†ğ‘–

|

|

1000ğ‘Ÿ2 ğ‘10 . The theorem follows observing that

ğ‘§ğ‘–

2 6 ğ‘‘ğ‘2
ğ‘Ÿ .

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:3)

D Covariance Thresholding doesnâ€™t work with large signal and small

sample size

We show here a formal argument that proves Covariance Thresholding doesnâ€™t work for ğ‘› 6 ğ‘˜2.
The lower bound 6.4 shows that the assumption ğ‘˜2 > ğ‘‘1
1
),
âˆ’
the conjecture Conjecture 3.13 implies that it is unlikely that there exists a polynomial time algo-
rithm with asymptotically better guarantees than Diagonal Thresholding. So in this section we
assume that ğ‘˜2 > ğ‘‘1
(since
âˆ’
otherwise Covariance Thresholding doesnâ€™t have asymptotically better guarantees than Diagonal
Thresholding).

) and that the thresholding parameter ğœ satisï¬es ğœ 6 ğ‘œ
1

) is important, since if ğ‘˜2 6 ğ‘‘1
1
âˆ’

ğ‘› log ğ‘‘

(
p

Î©

)

ğ‘œ

ğ‘œ

(

(

(

Notice that the assumptions ğ‘› > ğ‘˜2 and ğ‘˜2 > ğ‘‘1
âˆ’

ğ‘œ

) imply ğ‘› > ğ‘‘1
1
âˆ’

(

ğ‘œ

1
).

(

Theorem D.1. Let ğ‘¥ğ‘– be a unit vector in the direction of the ğ‘–-th row of ğ‘Š. Assume that ğ‘˜ 6 âˆšğ‘‘,
ğ‘‘Î©
. Also assume that the entries

and âˆšğ‘› 6 ğœ 6 ğ‘œ

) 6 ğ‘› 6 ğ‘‘1
1
âˆ’

ğ‘› log ğ‘‘

as ğ‘‘

Î©

(

(

(

of ğ‘£0 are from

0,

{

Â±

. Then with probability 1

1
) Â·

ğ‘˜
âˆšğ‘›

), ğ›½ 6 ğ‘‘ğ‘œ
1
âˆšğ‘˜
1
/

}

ğœğœ(
Theorem D.1 immediately follows from Lemma D.2 and Lemma D.3.

âˆ’

(

(

ğ‘¥ğ‘–

T
)

ğ‘Œğ‘‡ğ‘Œ

ğ‘œ

1
) .

â†’ âˆ

(cid:16)p
ğ‘‚
(
âˆ’
ğ‘›Id
)

(cid:17)

10

ğ‘‘âˆ’

)
ğ‘¥ğ‘– > ğ‘‘1
âˆ’

Lemma D.2. Let ğ‘†0 be a set of pairs
ğ‘™ âˆ‰ supp

) âˆˆ [
, and ğ‘— â‰  ğ‘™. Then with probability 1
âˆ’
ğ‘›Id

ğ‘Œğ‘‡ğ‘Œ

ğ‘£0}

ğ‘—, ğ‘™

ğ‘¥ğ‘–

ğ‘‘

{

(

(

T
)

ğœğœ(

(

âˆ’

ğ‘†0]

)[

2 such that any
]
ğ‘‚

20

ğ‘‘âˆ’

)
ğ‘¥ğ‘– > ğ‘‘1
âˆ’

ğ‘œ

1
) .

(

ğ‘—, ğ‘™

(

) âˆˆ

ğ‘†0 satisï¬es ğ‘— âˆ‰ supp

,

ğ‘£0}

{

92

Proof. Without loss of generality assume that ğ‘– = 1 and denote ğ‘¥ = ğ‘¥1. Letâ€™s denote the squared
norm of the ï¬rst row of ğ‘Š by ğ‘ 2. Notice that
ğ‘§ğ‘–ğ‘—, where ğ‘§ğ‘–ğ‘— is independent of ğ‘¥.
ğ‘¤ğ‘–, ğ‘¤ ğ‘—
Hence

= ğ‘¤1ğ‘–ğ‘¤1ğ‘—

+

i

h

ğ‘¥ğ‘‡ ğœğœ(

ğ‘Œğ‘‡ğ‘Œ

âˆ’

ğ‘›Id

)[

ğ‘¥ =

ğ‘†0]

ğ‘†0
ğ‘–ğ‘—
Ã•(
)âˆˆ

= 1
ğ‘ 2

âˆ’
log ğ‘‘
)
(
ğ‘¥ = 1
ğ‘ 2

Notice that with probability 1
ğœ > âˆšğ‘›, while ğ‘¤1ğ‘–ğ‘¤1ğ‘— < ğ‘‚

ğ‘¥ğ‘‡ ğœğœ(

ğ‘Œğ‘‡ğ‘Œ

âˆ’

ğ‘›Id

)[

ğ‘†0]

1

[|h

ğ‘¤ğ‘– ,ğ‘¤ ğ‘—i|

>ğœ

h

]

(cid:0)

ğ‘¤ğ‘–, ğ‘¤ ğ‘—

sign

ğ‘¤ğ‘– , ğ‘¤ ğ‘—

h

i âˆ’

ğœ

ğ‘¥ğ‘– ğ‘¥ ğ‘—

i

(cid:1)

1

[|

ğ‘¤1ğ‘–ğ‘¤1ğ‘—+

ğ‘§ğ‘–ğ‘— |

>ğœ

]

(cid:0)

ğ‘†0
ğ‘–ğ‘—
Ã•(
)âˆˆ

ğ‘¤1ğ‘–ğ‘¤1ğ‘—

30

ğ‘‚
ğ‘‘âˆ’
(
with 1

for every survived
ğ‘‚

30

. Hence
)

ğ‘‘âˆ’

(

)
âˆ’

(cid:0)
ğ‘§ğ‘–ğ‘—

+

(cid:1)
ğ‘¤ğ‘– , ğ‘¤ ğ‘—

i

ğœ

ğ‘¤1ğ‘–ğ‘¤1ğ‘— .

sign

âˆ’

h

ğ‘–, ğ‘—

, sign
)

(

(

(cid:1)
= sign

(cid:0)
ğ‘§ğ‘–ğ‘—

)

(cid:1)
ğ‘¤ğ‘– , ğ‘¤ ğ‘—

h

(cid:0)

, since

i

(cid:1)

ğ‘†0
ğ‘–ğ‘—
Ã•(
)âˆˆ
100 log ğ‘‘
ğ‘ 2

>

âˆ’

1
[|

ğ‘¤1ğ‘– ğ‘¤1ğ‘—+

ğ‘§ğ‘–ğ‘— |

>ğœ

]

(cid:16)

ğ‘¤1ğ‘–ğ‘¤1ğ‘—

ğ‘§

+

ğ‘–ğ‘—

(

)âˆˆ

ğ‘†0 âˆ’

sign

ğ‘§ğ‘–ğ‘—

ğœ

ğ‘¤1ğ‘–ğ‘¤1ğ‘—

(cid:0)
sign

(cid:17)

(cid:1)
ğ‘§ğ‘–ğ‘—

ğœ

1
[|

sign

ğ‘§ğ‘–ğ‘— âˆ’

ğ‘§ğ‘–ğ‘—

ğœ

|

)

(

<100 log ğ‘‘

]

ğ‘†0
ğ‘–ğ‘—
Ã•(
)âˆˆ

ğ‘§ğ‘–ğ‘—

âˆ’

1
ğ‘ 2

+

1
[|

>ğœ

ğ‘§ğ‘–ğ‘— |

ğ‘†0
ğ‘–ğ‘—
Ã•(
)âˆˆ

100 log ğ‘‘

+

]

ğ‘¤1ğ‘–ğ‘¤1ğ‘—

ğ‘§ğ‘–ğ‘—

+

(cid:0)
ğœ

(cid:12)
(cid:1)
(cid:12)
ğ‘¤1ğ‘–ğ‘¤1ğ‘—.

sign

ğ‘§ğ‘–ğ‘—

(cid:12)
(cid:12)
âˆ’

ğœ

âˆ’

ğ‘§ğ‘–ğ‘—

ğ‘§ğ‘–ğ‘—

Notice that â„™
sign
independent, so the number of ğ‘§ğ‘–ğ‘—â€² such that
(cid:3)
(cid:0)
probability at least 1
2âˆ’
most 10ğ‘ğ‘‘2 such ğ‘§ğ‘–ğ‘— for all

< 100 log ğ‘‘

ğ‘–, ğ‘—

(cid:2)(cid:12)
(cid:12)

âˆ’

(cid:12)
(cid:12)

(cid:1)

(cid:12)
(cid:12)

1000 log ğ‘‘
âˆšğ‘›

<
ğ‘§ğ‘–ğ‘—â€² âˆ’

sign

(cid:1)

(cid:1)

(cid:0)

(cid:0)
=: ğ‘. If we ï¬x ğ‘—â€², then for diï¬€erent ğ‘–, ğ‘§ğ‘–ğ‘—â€² are
ğ‘§ğ‘–ğ‘—â€²
< 100 log ğ‘‘ is bounded by 10ğ‘ğ‘‘ with
ğ‘ğ‘‘ there are at
ğ‘‘
âˆ’
, ğ‘ 2 = Î˜
ğ‘›
ğ‘›
exp
.
)
ğ‘‚

ğœ

(cid:12)
(cid:12)

(

(cid:1)

(cid:0)

2âˆ’
Â·
Î©
[âˆ’
(
ğ‘‘ log2 ğ‘‘
âˆšğ‘›

(

)]
.

)

âˆ’

ğ‘ğ‘‘. Hence by union bound with high probability 1

ğ‘†0. Notice that with probability at least 1

âˆ’
the contribution of the ï¬rst term is

20

ğ‘‘âˆ’

(

) âˆˆ
ğ‘‚

âˆ’

(

)

So with probability at least 1

Now, by Bernstein inequality G.7, for any ï¬xed ğ‘—

1
[|

>ğœ

ğ‘§ğ‘–ğ‘— |

100 log ğ‘‘

+

ğ‘§ğ‘–ğ‘—

âˆ’

]

(cid:0)

sign

ğ‘§ğ‘–ğ‘—

ğœ

ğ‘¤1ğ‘–ğ‘¤1ğ‘—

(cid:0)

(cid:1)

(cid:1)

6 ğ‘‚

ğ‘›ğ‘‘ log ğ‘‘

(cid:16)p

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

with probability at least 1

ğ‘‚

30

ğ‘‘âˆ’

âˆ’

(

.
)

Furthermore, since ğ‘§ğ‘–ğ‘—, ğ‘¤1ğ‘– and ğ‘¤1ğ‘— are independent, for any ğ‘— such that ğ‘¤2
1ğ‘—

> 1

(cid:12)
Ã•ğ‘–
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
ğ‘¤1

k

2

k

Ã•ğ‘–

1
[|

>ğœ

ğ‘§ğ‘–ğ‘— |

100 log ğ‘‘

+

]

ğ‘¤2

1ğ‘–ğ‘¤2

1ğ‘—

>

1
ğ‘¤1

k

2

k

Ã•ğ‘–

1

ğ‘§ğ‘–ğ‘— |

[|

>2ğœ

]

ğ‘¤2
1ğ‘–

> exp

ğ‘‚

ğœ2
(cid:1)ğ‘›
(cid:0)

(cid:19)

âˆ’

(cid:18)

> ğ‘‘âˆ’

ğ‘œ

1
)

(

ğ‘œ

ğ‘‘1
âˆ’

1
)

(

. Hence

with probability at least 1

exp

âˆ’

âˆ’

1
ğ‘¤1

(cid:0)

k

2

k

and therefore, with probability 1

Ã•ğ‘–ğ‘—
ğ‘‚

âˆ’

20

ğ‘‘âˆ’

)

(

(cid:1)
ğ‘§ğ‘–ğ‘— |

1
[|

>ğœ

100 log ğ‘‘

+

]

ğ‘¤2

1ğ‘–ğ‘¤2

1ğ‘—

> ğ‘‘1
âˆ’

ğ‘œ

1
) ,

(

ğ‘¥ğ‘‡ ğœğœ(

ğ‘Œğ‘‡ğ‘Œ

âˆ’

ğ‘›Id

)[

ğ‘†0]

ğ‘¥ > ğ‘‘1
âˆ’

ğ‘œ

1
)

(

ğ‘‘

Â·

âˆ’

ğ‘‚

log2 ğ‘‘
(
âˆšğ‘›

)

âˆ’

ğ‘‚

ğ‘›ğ‘‘ log ğ‘‘

> ğ‘‘1
âˆ’

ğ‘œ

1
) .

(

(cid:16)p

(cid:17)

(cid:3)

93

Lemma D.3. Let ğ‘†1 =

ğ‘‘

[

2

]

\

ğ‘†0. Then with probability 1

Proof. With probability 1

(cid:12)
ğ‘‚
(cid:12)

(

âˆ’

20

ğ‘‘âˆ’

)

ğ‘‚

ğ›½ğ‘›
ğ‘˜ +

ğ‘› log ğ‘‘

p

ğ‘¥ğ‘–

T
)

(

ğœğœ(

ğ‘¥ğ‘–

ğ‘Œğ‘‡ğ‘Œ

âˆ’

)[

ğ‘›Id

ğ‘†1]
the entries of ğœğœ(
ğ›½ğ‘› log ğ‘‘
ğ‘˜

6 ğ‘‘ğ‘œ

!

+ r

ğ‘‚

20

ğ‘‘âˆ’

âˆ’
(
2
6 ğ‘‘1
+
/

ğ‘œ

,
)
1
)

(

(cid:12)
ğ‘Œğ‘‡ğ‘Œ
(cid:12)

ğ‘›Id

)[

âˆ’

âˆšğ‘› .

Â·
ğ‘†1]

are bounded by

1
)

(

âˆšğ‘›

(cid:16)

+

4âˆšğ‘›

6 ğ‘‘ğ‘œ

1
)

(

(cid:17)

âˆšğ‘› .

Â·

log ğ‘‘
ğ‘‘

(cid:18)q

(cid:19)

1
ğ‘‘

Â·

6 ğ‘‘1
2
+
/

ğ‘œ

1
)

(

âˆšğ‘› .

Â·

20

ğ‘‘âˆ’

ğ‘‚
(cid:1)

(

âˆ’

)

(cid:3)

Number of nonzero entries in ğœğœ(

ğ‘†1]
entries of ğ‘¥ğ‘– are bounded by ğ‘‚

ğ‘›Id

)[

âˆ’

ğ‘Œğ‘‡ğ‘Œ

probability 1

20

ğ‘‘âˆ’

ğ‘‚

(

âˆ’

)

is at most ğ‘‘

ğ‘˜2

2ğ‘˜ğ‘‘ 6 ğ‘‚

2
ğ‘‘3
/

. With

+

+
. Hence with probability 1

(cid:0)

ğ‘¥ğ‘–

T
)

ğ‘Œğ‘‡ğ‘Œ

ğœğœ(

ğ‘›Id

)[

ğ‘†1]

âˆ’

ğ‘¥ğ‘–

6 ğ‘‘ğ‘œ

1
)âˆšğ‘›

(

2
ğ‘‘3
/

Â·

(cid:12)
(cid:12)

(
(cid:12)
(cid:12)

To conclude that under assumptions of Theorem D.1 Covariance Thresholding doesnâ€™t work,

we need the following lemma:

Lemma D.4. Assume that the entries of ğ‘£0 are from
probability 1

ğ‘‚

10

ğ‘‘âˆ’

âˆ’

(

)

T
ğ‘£0

ğœğœ(

ğ‘Œğ‘‡ğ‘Œ

ğ‘›Id
)

âˆ’

ğ‘£0

6 ğ‘‚

ğ‘˜

ğ‘› log ğ‘‘

ğ›½ğ‘›

.

+

(cid:17)
The proof of Lemma D.4 is the same as the proof of Lemma C.10.

p

(cid:16)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

0,

{

âˆšğ‘˜
1
/

}

Â±

and ğ‘› > ğœ”

log ğ‘‘
(

)

as ğ‘‘

â†’ âˆ

. Then with

E Statistical Lower bound for Recovery

In this Section we provide an information theoretic lower bound for recovery of the sparse vector,
ğ‘˜ no estimator can achieve correlation 0.9. We remark that the
we will show that for ğ›½
bound was previously known, we include it for completeness. Formally, we prove the following
statement.

ğ‘› log ğ‘‘

â‰ª

ğ‘˜

Theorem E.1. Given an ğ‘›-by-ğ‘‘ matrix ğ‘Œ of the form ğ‘Œ = ğ‘Š
a standard Gaussian vector ğ‘¢0 âˆ¼
ğ‘£ : â„ğ‘›
Ë†

ğ›½ğ‘¢0ğ‘£0
, and a Gaussian matrix ğ‘Š
p
)
ğ‘‘ such that

â„ğ‘‘, there exists a ğ‘˜-sparse vector ğ‘£

0, Idğ‘›
(

âˆšğ‘˜
1
/

âˆˆ {

â†’

0,

ğ‘

+

Â±

}

Ã—

ğ‘‘

âˆ¼

T for a ğ‘˜-sparse unit vector ğ‘£0 âˆˆ

â„ğ‘‘,
ğ‘‘. For any estimator

ğ‘

ğ‘›

Ã—

0, 1
)
(

ğ”¼

1

âˆ’

ğ‘£
h Ë†

ğ‘Œ
(

)

, ğ‘£

i

2 > 0.2

5ğ‘›ğ›½
ğ‘˜ log ğ‘‘
ğ‘˜

.

âˆ’

Observe how the theorem compare with the distinguishing lower bound 6.5. While the top
eigenvector of the covariance matrix can distinguish the Gaussian distribution from the planted
distribution if ğ›½ &
ğ‘› , Theorem E.1 shows that in any settings it is also required to have ğ›½ &
ğ‘› log ğ‘‘
ğ‘˜
ğ‘˜ simple polynomial
time algorithms can distinguish, but it is information theoretically impossible to have an estimator
achieving correlation 0.9.

ğ‘˜ in order to obtain correlation 0.9. In other words, for ğ‘› . ğ‘˜2

ğ‘‘ log2 ğ‘‘

q

ğ‘‘

94

 
A standard technique to prove such result is bounding the minimax risk. This can be done
observing that, given an appropriate well-separated set of candidate vectors, any estimator will
erroneously guess which is the true planted vector with large enough probability. We introduce
some standard notions that will be used in the proof, the proof itself can be found at the end of the
section. We follow closely [Wai19].

â„ğ‘‘ let ğ‘‘ğ»
Consider the following notation. For vectors ğ‘£1, ğ‘£2 âˆˆ
ğ‘‘
ğ‘‘ğ»
0,
ğ‘, ğ‘¡, ğ‘™
âˆˆ {
. Given a random variable ğ‘¥, denote with Î£
(

â„ğ‘‘ , 0 < ğ‘¡ 6 ğ‘‘, deï¬ne
= ğ‘™

âˆˆ
ğ‘¦, ğ‘

ğ‘‘ğ»

â„¬(

:=

Â±

ğ‘¦

{

}

}

}

)

ğ‘¡

|

ğ‘¥
(cid:0)

)

distance. For ğ‘
ğ‘‘
ğ‘¡
0,
{
|
Â±
support.

(cid:1)

ğ‘£1, ğ‘£2)
(
6 ğ‘™
ğ‘¦, ğ‘

denote their Hamming
ğ‘, ğ‘¡, ğ‘™

:=
its covariance and with

ğ‘¦
âˆˆ
its

and

ğ’®(

}

)

{
ğ’³

(cid:0)

(cid:1)

Deï¬nition E.2. Let ğœŒ be a metric. A ğ›¿-packing of a set ğ‘‡ with respect to a metric ğœŒ is a subset
, ğ‘– â‰  ğ‘—. A ğ›¿-covering of a set ğ‘‡ with respect
ğ‘§1, . . . , ğ‘§ğ‘€
(
) âŠ‚
]
ğ‘€
to ğœŒ is a subset
< ğ›¿.
âˆˆ [

ğ‘‡ such that ğœŒ
ğ‘§1, . . . , ğ‘§ğ‘€

> ğ›¿ for all ğ‘–, ğ‘—
ğ‘‡,
ğ‘§

âˆ’
ğ‘‡ such that
(cid:1)

âˆˆ [
ğ‘—
âˆƒ

with ğœŒ

(cid:0)
) âŠ‚

ğ‘€

ğ‘§ ğ‘—

ğ‘§ ğ‘—

ğ‘§ğ‘–

âˆ’

âˆ€

âˆˆ

ğ‘§

]

(

The following Lemma lower bounds the size of the largest âˆšğ›¿ packing.

(cid:1)

(cid:0)

Lemma E.3. Let ğ›¿
ğ‘‘
ğ›¿
ğ‘’
ğ‘˜

at least

1
âˆ’

ğ›¿

ğ‘˜

)

(

. There exists a âˆšğ›¿-packing of
0, 1
)

âˆˆ (
ğ›¿ğ‘˜
.

0, 1
/

ğ’®(

âˆšğ‘˜, ğ‘˜

)

with respect to

kÂ·k2 of cardinality

(cid:1)

(cid:1)
â„¬(

(cid:0)
(cid:0)
Proof. Since Vol

0, 1
/
has cardinality at least
âˆšğ‘˜, ğ‘˜
0, 1
number of
/
ğ‘˜.
ğ‘™
ğ‘¦1 âˆ’

ğ’®(
>

ğ‘¦2

/

(cid:16)

2

âˆšğ‘˜, ğ›¿ğ‘˜

ğ‘‘
ğ‘˜

2ğ‘˜

/

6

ğ‘‘
ğ›¿ğ‘˜
2ğ‘˜ >
(cid:0)

)

(cid:17)
ğ‘‘
ğ›¿ğ‘˜

2ğ‘˜, a ğ›¿ğ‘˜-covering of

(cid:1)

ğ‘‘
ğ‘˜

ğ‘˜

ğ›¿

1
âˆ’

(

)

ğ›¿ğ‘˜

ğ›¿
ğ‘’

âˆšğ‘˜, ğ‘˜
0, 1
/

with respect to ğ‘‘ğ»

ğ’®(

(Â·)
)
. This is a lower bound for the ğ›¿ğ‘˜-packing
> ğ‘™ implies
ğ‘‘, ğ‘‘ğ»
(cid:3)

ğ‘¦1 âˆ’

1
/

âˆšğ‘˜

ğ‘¦2

0,

Â±

}

. The lemma follows since for ğ‘¦1, ğ‘¦2 âˆˆ {
(cid:1)
(cid:0)
)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:0)
The main tool used in the Lemma will be the well-known Fanoâ€™s Inequality.

p

(cid:1)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Deï¬nition E.4. Let ğ‘§, ğ‘— be random variables. Then their mutual information is

where for a r.v. ğ‘¥ with density function ğ‘

ğ‘¥

ğ‘¥

, ğ»
)

(

)

(

ğ‘§, ğ‘—

ğ¼

(

)

:= ğ»

ğ‘§

(

) +

ğ»

(
:=

ğ‘—

âˆ’

) âˆ’
ğ”¼

ğ‘§, ğ‘—

ğ»

(

)

log ğ‘

ğ‘¥

(

)

is the Shannon Entropy.

Lemma E.5. Let ğ‘§, ğ‘— be random variables. Suppose ğ‘— is uniformly distributed over
on ğ‘— = ğ½, for ğ‘—
. Then
)

has a Gaussian distribution with covariance Î£
(

âˆˆ [

ğ‘€

ğ‘§

]

ğ½

|

(cid:2)

(cid:3)

and that ğ‘§ conditioned

ğ‘€

[

]

ğ‘§, ğ‘—

ğ¼

(

)

6 1
2

log det Î£
(

ğ‘§

) âˆ’

1
ğ‘€

Â©

Lemma E.6 (Fanoâ€™s Inequality). Let ğœŒ be a metric and Î¦ :
a family of distributions
between distributions
If for all ğ‘–, ğ‘—

Â«
and a functional ğ‘£ :

with ğ‘– â‰  ğ‘—, ğœŒ

ğ·1, . . . , ğ·ğ‘€

ğ’Ÿ â†’

ğ‘€

ğ’Ÿ

ğ‘£

{

} âŠ† ğ’Ÿ
, ğ‘£
ğ·ğ‘–
(
)

(

(

))

âˆˆ [

]

log det Î£
(

ğ‘§

ğ½

]

|

ğ‘€
Ã•ğ½
âˆˆ[
0,

)
Âª
Â®
an increasing function. Given
Â¬
Î©, consider a ğ‘€-ary hypothesis testing problem
ğ·ğ‘–.

âˆ) â†’ [

ğ‘¥ = ğ‘–

âˆ)

ğ‘€

0,

ğ‘§

[

where ğ‘¥ is uniformly distributed over
> 2ğ›¿, then for any estimator
ğ·ğ‘—

and
Î©:

(

|

]
[
ğ’µ â†’

) âˆ¼

The minimax risk is deï¬ned as ğ”ª

ğ‘£ :
Ë†
log 2

Î¦

ğ”¼
ğ·ğ‘–

ğœŒ

ğ‘£, ğ‘£
( Ë†

(

ğ·ğ‘–

))

> Î¦

ğ›¿

(

)

1

âˆ’

ğ‘§, ğ‘¥

ğ¼

(

) +
log ğ‘€

sup
ğ‘€
ğ‘–

âˆˆ[

]

(cid:0)

(ğ’Ÿ)

(cid:2)
ğ‘£

(cid:0)

, Î¦

ğœŒ

â—¦

(cid:1) (cid:3)
= inf
ğ‘£
Ë†

(cid:1)

sup
ğ‘€
ğ‘–

âˆˆ[

]

95

(cid:18)
ğ”¼ğ·ğ‘–

Î¦

ğœŒ

ğ‘£, ğ‘£
( Ë†

(

ğ·ğ‘–

))

(cid:2)

(cid:0)

(cid:19)

.

(cid:1) (cid:3)

 
We are now ready to prove the bound, we provide a slightly more general version which

immediately implies Theorem E.1.

ğ‘£ : â„ğ‘›
Ë†

ğ‘‘

Ã—

â†’

â„ğ‘‘ and ğ›¿

0, 1
4

âˆˆ

, there exists a ğ‘˜-sparse vector ğ‘£

âˆˆ

Theorem E.7. For any estimator
ğ‘‘ such that

âˆšğ‘˜

0,

1
/

Â±

{

}

ğ”¼

ğ‘£
k Ë†

ğ‘Œ
(

) âˆ’

ğ‘£

k

2
2 > ğ›¿

1

âˆ’

ğ‘˜

where ğ‘Œ =

ğ›½ğ‘¢ğ‘£T

ğ‘Š for ğ‘Š

+

ğ‘

0, 1
)
(

âˆ¼

ğ‘›

Ã—

ğ‘‘ and ğ‘¢

(cid:0)
âˆ¼

(cid:1)
(cid:0)
2 log 2

1
(

âˆ’
ğ‘

ğ‘›ğ›½

+
log ğ‘‘

4ğ›¿

ğ‘˜ +

)
.
0, Idğ‘›
(
)
Î”1, . . . , Î”ğ‘€

4ğ›¿ log 4ğ›¿
ğ‘’

!

(cid:1)

p

)

(

ğ‘˜

(cid:1)

4ğ›¿

4ğ›¿ğ‘˜

1
âˆ’

4ğ›¿
ğ‘’

â„ğ‘‘

Proof. By Lemma E.3 there exists a 2âˆšğ›¿-packing
ğ‘‘
ğ‘˜

with size ğ‘€ >
of
ğ‘‘ the set of orthonormal matrices corresponding to a permu-

. Denote by

0, 1
/

âˆšğ‘˜, ğ‘˜

tation of the columns along with ï¬‚ip of signs. Notice that for ğ‘ˆ
(cid:0)
and ğ‘ˆÎ”ğ‘–
a ï¬xed ğ‘ˆ
ğ‘€
from
E.5,

Î”ğ‘–
k
ğ‘ˆ
, deï¬ne the corresponding family of vectors ğ‘£ ğ‘—
)
(
) âˆˆ

k2
. For
ğ‘‘ be the random variable generated picking ğ‘— uniformly at random
. By Lemma
)

(cid:0)
(cid:1)
âˆšğ‘˜, ğ‘˜
0, 1
âˆˆ ğ’®(
/
ğ‘ˆ
, let ğ‘Œ
Ã—
(
âˆˆ ğ’°
and then sampling ğ‘Œ

T. For ğ‘› = 1, we denote it by ğ‘¦
)

k2 =
Î”ğ‘—
âˆ’
= ğ‘ˆÎ”ğ‘— for ğ‘—
)

ğ‘ˆÎ”ğ‘—
âˆ’
ğ‘€
âˆˆ [

ğ‘ˆÎ”ğ‘–

= ğ‘Š

ğ›½ğ‘¢ğ‘£ ğ‘—

ğ’° âŠ†

ğ‘ˆ
(

ğ‘ˆ
(

ğ‘ˆ
(

âˆˆ ğ’°

â„ğ‘›

ğ’®(

+

}

{

k

Ã—

[

]

]

)

)

,

log det Î£

ğ‘¦

ğ‘ˆ
(

)

âˆ’

1
ğ‘€

ğ”¼
ğ‘ˆ

log det Î£
(

ğ‘¦

ğ‘ˆ
(

)|

ğ½

p

ğ”¼
ğ‘ˆ

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ğ”¼
ï£°
ğ‘ˆ
ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°
(cid:20)

ğ”¼
ğ‘ˆ

log det Î£

log det Î£

(cid:0)

(cid:0)

ğ‘¦

ğ‘ˆ
(

)

ğ‘¦

ğ‘ˆ
(

)

(cid:1)

(cid:1)

(cid:1)

log det ğ”¼
ğ‘ˆ

(cid:20)

ğ‘‘ log

1

(cid:20)

(cid:18)

+

(cid:0)

Î£

ğ‘¦

ğ‘ˆ
(

(cid:1)
log

ğ›½
ğ‘‘

(cid:0)

(cid:19)

âˆ’

ğ‘€
Ã•ğ½
âˆˆ[

]

1

âˆ’

ğ‘€

|ğ’° | Ã•ğ½
,ğ‘ˆ
âˆˆ[

ğ‘€

]

log

ğ›½

1
(

+

âˆ’

)

(cid:21)

)

âˆ’

log

ğ›½

1
(

+

)

(cid:21)

ğ›½

1

+

(cid:0)

(cid:21)

(cid:1)

ğ”¼
ğ‘ˆ

ğ¼

ğ‘¦

ğ‘ˆ
(

; ğ½
)

)

(

(cid:2)

(cid:3)

6 1
2

=

1
2

=

1
2
6 1
2

=

6

1
2
ğ›½
2

log det Î£
(

âˆˆğ’°

)
ï£¹
ï£º
ï£º
ï£º
ï£º
ğ‘ˆ
ğ‘¦
ï£»
(

ğ½

)|

)
ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

using concavity of the log-determinant and the matrix-determinant Lemma. Applying Fanoâ€™s
Inequality E.6, for any estimator

â„ğ‘‘

ğ‘‘

ğ‘£ : â„ğ‘›
Ë†

Ã—

sup
ğ‘€
ğ‘—

âˆˆ[

]

ğ‘£ ğ‘—

ğ‘£
Ë†

âˆ’

ğ”¼
ğ·ğ‘—

h(cid:13)
(cid:13)

2
2

i

(cid:13)
(cid:13)

â†’
> ğ›¿

(cid:18)

> ğ›¿

1

> ğ›¿

1

(cid:18)

> ğ›¿

1

ğ”¼ğ‘ˆ

ğ¼

ğ‘Œ
(

[

; ğ½
ğ‘ˆ
)
(
log ğ‘€

)] +

1

âˆ’

log 2

(cid:19)
log 2

!

ğ‘¦

; ğ½
ğ‘ˆ
(
)
)
log ğ‘€

+

(cid:3)

ğ‘› ğ”¼ğ‘ˆ

ğ¼

(

âˆ’

âˆ’

(cid:2)
2 log 2

ğ‘›ğ›½

+

2 log ğ‘€

âˆ’

ğ‘˜

1

(

âˆ’

4ğ›¿

)

4ğ›¿ log 4ğ›¿
ğ‘’

!

ğ‘›ğ›½

2 log 2

(cid:19)

+
log ğ‘‘

ğ‘˜ +
ğ‘ˆ
(

where we used the fact that by independence of the rows of ğ‘Œ

(cid:0)

96

, ğ¼
)

ğ‘Œ
(

ğ‘ˆ
(

; ğ½
(cid:1)
)
)

6 ğ‘›ğ¼

ğ‘¦

ğ‘ˆ
(

; ğ½
)

.
)

(

(cid:3)

 
 
 
F Existence of the Adversarial Distribution of Model 6.6

ğ‘¥

Let â„
[
distribution we will need the following theorem.

]

6ğ‘  be the space of one variable polynomials of degree at most ğ‘ . To construct the desired

Theorem F.1 (Theorem 1.26 in [Sch17]). Suppose that ğ‘š1, . . . , ğ‘šğ‘ 
a linear functional

â„ such that

= 1 and

ğ‘¥

: â„
[

â„’

6ğ‘ 

]

â†’

1
â„’(
)
= ğ‘šğ‘Ÿ ,

ğ‘¥ğ‘Ÿ

)

â„’(

1 6 ğ‘Ÿ 6 ğ‘  .

â„ and ğ¾

âˆˆ

âŠ†

â„ is compact. Consider

ğ‘

â„’(

> 0 for every ğ‘
â„
If
ğ‘¥
6ğ‘  that is nonnegative on ğ¾, then there exists a ï¬nitely supported probability
]
[
distribution ğœ‚ such that supp
ğœ‚
(

ğœ‚ ğ‘¥ğ‘Ÿ = ğ‘šğ‘Ÿ for 1 6 ğ‘Ÿ 6 ğ‘  .
âˆ¼

ğ¾ and ğ”¼ğ‘¥

) âŠ†

âˆˆ

)

Letâ€™s take the maximal even number ğ‘  such that ğ›¿ğœ†ğ‘  6 2âˆ’

10ğ‘  . We will show that there exists
a distribution with compact support such that with probability ğ›¿ it takes values
ğœ† and its ï¬rst
ğ‘  moments coincide with the ï¬rst ğ‘  Gaussian moments. Such a distribution is a mixture ğœ‚ =
ğœ† with probability 1
2 each, and ğœ‚0 has particular moments
1
ğœ‚0 +
(
)
âˆ’
up to ğ‘ .

ğ›¿ğœ‚1, where ğœ‚1 takes values

Â±

Â±

ğ›¿

Proposition F.2. Suppose that ğ‘  > 2 is even, 0 < ğ›¿ < 1, ğœ† > 2 and ğ›¿ğœ†ğ‘  6 2âˆ’
supported probability distribution ğœ‚0 such that supp
ğœ‚0) âŠ† [âˆ’
(
where

10âˆšğ‘  ln ğ‘ , 10âˆšğ‘  ln ğ‘ 

10ğ‘  . Then there exists a ï¬nitely
ğœ‚0 ğ‘¥ğ‘Ÿ = ğ‘€ğ‘Ÿ ,
âˆ¼

and ğ”¼ğ‘¥

]

if ğ‘Ÿ is odd,
if 0 6 ğ‘Ÿ 6 ğ‘  and ğ‘Ÿ is even.

ğ›¿ğœ†ğ‘Ÿ

,

ğ‘€ğ‘Ÿ =

0,
1

ğ›¿

1
âˆ’

(

ğ‘Ÿ

(
Proof. Consider a linear functional
1 6 ğ‘Ÿ 6 ğ‘ . We need to show that
ğ‘

(cid:0)

âˆ’

!!
1
)
âˆ’
: â„
[

10âˆšğ‘  ln ğ‘ , 10âˆšğ‘  ln ğ‘ 

]

[âˆ’

â„’
)

]

ğ‘¥

(cid:1)
6ğ‘ 

â„ such that
â†’
> 0 for every polynomial ğ‘
6ğ‘ 

ğ‘¥

â„
[

]

â„’(

. Notice that for any polynomial ğ‘

1
)
â„’(
â„
ğ‘¥
[
âˆˆ

= 1 and
= ğ‘€ğ‘Ÿ for
ğ‘¥ğ‘Ÿ
6ğ‘  that is nonnegative on

â„’(

)

]

ğ›¿

1
(

âˆ’

) Â· â„’(

ğ‘

)

= ğ”¼
ğ‘¥

âˆ¼ğ’©(

0,1
)

ğ‘

ğ‘¥

(

) âˆ’

ğ‘

ğœ†
(

) +

ğ‘

ğœ†

(âˆ’

)

.

(cid:1)

âˆˆ
ğ›¿
2

(cid:0)

Consider an arbitrary polynomial ğ‘
If ğ‘ = 0, then obviously
max
06ğ‘Ÿ6ğ‘ {|

â„’(
= 1. Since ğ‘ is nonnegative on

ğ‘ğ‘Ÿ

|}

ğ‘

)

(

)

Ã
[âˆ’

ğ‘¥

=

ğ‘ 
ğ‘Ÿ=0 ğ‘ğ‘Ÿ ğ‘¥ğ‘Ÿ that is nonnegative on

10âˆšğ‘  ln ğ‘ , 10âˆšğ‘  ln ğ‘ 
.
]
= 0. So we can assume that ğ‘ â‰  0 and without loss of generality
,

10âˆšğ‘  ln ğ‘ , 10âˆšğ‘  ln ğ‘ 

[âˆ’

]

ğ”¼

ğ‘¥

0,1
)

âˆ¼ğ’©(

ğ‘

ğ‘¥

(

)

> 1
âˆš2ğœ‹

1

âˆ«
1
âˆ’

ğ‘¥2

2
/

ğ‘’âˆ’

ğ‘

ğ‘¥

(

)

1
âˆš2ğœ‹

+

âˆ«
>10âˆšğ‘  ln ğ‘ 

ğ‘¥

|

|

ğ‘¥2

2ğ‘‘ğ‘¥ .
/

ğ‘’âˆ’

ğ‘

ğ‘¥

(

)

The second integral can be bounded as follows

ğ‘¥2

2
/

ğ‘’âˆ’

6

ğ‘

ğ‘¥

(

)

âˆ«
>10âˆšğ‘  ln ğ‘ 

ğ‘¥

|

(cid:12)
(cid:12)
(cid:12)
|
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ğ‘ 

Ã•ğ‘Ÿ=0

ğ‘ğ‘Ÿ

|

|

ğ‘¥

|

|

âˆ«
>10âˆšğ‘  ln ğ‘ 

Notice that since the function ğ‘  ln ğ‘¥

10ğ‘ 

ğ‘¥2

ğ‘Ÿ ğ‘’âˆ’

2ğ‘‘ğ‘¥ 6
/

ğ‘¥

|

|

ğ‘ 

(

1
)

+

ğ‘¥2

ğ‘¥ğ‘  ğ‘’âˆ’

2ğ‘‘ğ‘¥ .
/

âˆ«
>10âˆšğ‘  ln ğ‘ 

ğ‘¥

|
|
> 10âˆšğ‘  ln ğ‘ ,

0.4ğ‘¥2 is monotone for

ğ‘¥

|

|

+

âˆ’
ğ‘¥ğ‘  ğ‘’âˆ’

ğ‘¥2

2 6 ğ‘’âˆ’
/

10ğ‘ 

âˆ’

ğ‘¥2

10
/

97

for all ğ‘¥ such that

ğ‘¥

|

|

> 10âˆšğ‘  ln ğ‘ . Hence

âˆ«
>10âˆšğ‘  ln ğ‘ 

ğ‘¥

(cid:12)
(cid:12)
(cid:12)
|
(cid:12)
Letâ€™s bound

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
ğ‘¥2
2

exp

ğ‘¥

(

)

1
1 ğ‘
âˆ’
âˆ«

ğ‘¥2

2ğ‘‘ğ‘¥ >
/

ğ‘’âˆ’

ğ‘

ğ‘¥

(

)

1

âˆ«
1
âˆ’

âˆ’

1

(cid:16)

âˆ«
1
âˆ’

(cid:17)
ğ‘2
ğ‘¥
(
ğ‘
max
61
ğ‘¥
|
|

)
ğ‘¥
(

)

ğ‘¥2

ğ‘’âˆ’

1

To bound
âˆ’
Legendre polynomial is
âˆ«

1 ğ‘2

ğ‘¥

(

)

ğ‘¥2

2ğ‘‘ğ‘¥
/

ğ‘’âˆ’

6

ğ‘

ğ‘¥

(

)

ğ‘ 

(

1
) Â·

+

10ğ‘ 

ğ‘’âˆ’

âˆ«
>10âˆšğ‘  ln ğ‘ 

ğ‘¥

|

|

ğ‘¥2

ğ‘’âˆ’

10ğ‘‘ğ‘¥ 6 âˆš10
/

ğ‘ 

(

1
) Â·

+

ğ‘’âˆ’

10ğ‘  6 ğ‘’âˆ’

8ğ‘  .

ğ‘‘ğ‘¥. Since ğ‘

ğ‘¥

(

)

is nonnegative on

,

1, 1
]

[âˆ’

/

2
1
/

2ğ‘‘ğ‘¥ > ğ‘’âˆ’
ğ‘ 
ğ‘Ÿ=0 |

1

ğ‘2

ğ‘¥

(

)

ğ‘‘ğ‘¥ >

1

1

ğ‘ 

2
(

1
)

+

âˆ«
1
âˆ’

ğ‘2

ğ‘¥

(

)

ğ‘‘ğ‘¥ .

ğ‘ğ‘Ÿ

|

âˆ«
1
âˆ’

Ã

ğ‘‘ğ‘¥ we can use Legendre polynomials (see for example [AWH13]). The degree ğ‘—

=

ğ¿ğ‘—

ğ‘¥

(

)

ğ‘—

Ã•ğ‘Ÿ=0

ğ¿ğ‘—,ğ‘Ÿ ğ‘¥ğ‘Ÿ =

ğ‘—

Ã•ğ‘Ÿ=0 r

2ğ‘—

1

+
2

2ğ‘—

Â·

ğ‘—

+

ğ‘Ÿ
1
âˆ’
2
ğ‘—

ğ‘—
ğ‘Ÿ

(cid:18)

(cid:19) (cid:18)

(cid:19)

ğ‘¥ğ‘Ÿ .

They form an orthonormal system on
=
coeï¬ƒcients ğ‘0, . . . , ğ‘ğ‘  such that ğ‘

ğ‘¥

(

)

1, 1
[âˆ’
]
ğ‘ 
ğ‘—=0 ğ‘ğ‘  ğ¿ğ‘—

and

ğ‘¥

(

)

with respect to the unit weight. Hence there exist

Recall that by assumption max

06ğ‘Ÿ6ğ‘ {|

ğ‘ğ‘Ÿ

|}

Ã

1

ğ‘2

ğ‘ 

ğ‘‘ğ‘¥ =

ğ‘2
ğ‘— .

ğ‘¥

(

)

âˆ«
1
âˆ’
= 1, so there exists some ğ‘Ÿ such that

Ã•ğ‘—=0

= 1. Thus

ğ‘ğ‘Ÿ

|

|

1 =

=

ğ‘ğ‘Ÿ

|

|

Notice that

6 âˆšğ‘ 

ğ¿ğ‘—,ğ‘Ÿ

|

|

+

1

1

âˆ«
1
âˆ’

and

Notice that

Hence ï¬nally we get

ğ‘ 

ğ‘ 

ğ‘ ğ‘—ğ¿ğ‘—,ğ‘Ÿ

6

ğ¿ğ‘—,ğ‘Ÿ

ğ‘ ğ‘—

|

||

6 max
ğ‘Ÿ6ğ‘—6ğ‘  |

|

ğ¿ğ‘—,ğ‘Ÿ

ğ‘ 

1
)

+

Ã•ğ‘—=ğ‘Ÿ
22ğ‘  for 0 6 ğ‘Ÿ 6 ğ‘— 6 ğ‘ . Hence we get a bound

Ã•ğ‘—=ğ‘Ÿ

(cid:12)
(cid:12)
(cid:12)

|

(
p

ğ‘ 
ğ‘—=0 ğ‘2
ğ‘— .

qÃ

(cid:12)
(cid:12)
(cid:12)
Â·

ğ‘’âˆ’

ğ‘

ğ‘¥

(

)

ğ‘¥2

2ğ‘‘ğ‘¥ > 1
3ğ‘ 

/

ğ‘ 

Ã•ğ‘—=0

>

ğ‘2
ğ‘—

1

+

ğ‘ 

2
(

4ğ‘  > 2âˆ’

7ğ‘  ,

3 2âˆ’
1
)

ğ”¼

ğ‘¥

0,1
)

âˆ¼ğ’©(

ğ‘

ğ‘¥

(

)

> 1
âˆš2ğœ‹

7ğ‘ 

2âˆ’

âˆ’

ğ‘’âˆ’

8ğ‘  > 2âˆ’

8ğ‘  .

ğ‘

ğœ†
(

) +

ğ‘

ğœ†

(âˆ’

ğ›¿
2

(cid:0)

6 ğ›¿

ğ‘ 

Ã•ğ‘Ÿ=0

ğœ†

|

|

)

(cid:1)

ğ‘Ÿ 6 2ğ›¿ğœ†ğ‘  6 2âˆ’

9ğ‘  .

ğ›¿

1
(

âˆ’

) Â· â„’(

ğ‘

)

= ğ”¼
ğ‘¥

âˆ¼ğ’©(

0,1
)

ğ‘

ğ‘¥

(

) âˆ’

ğ›¿
2

ğ‘

ğœ†
(

) +

ğ‘

ğœ†

(âˆ’

)

> 2âˆ’

8ğ‘ 

âˆ’

2âˆ’

9ğ‘  > 0 .

(cid:0)
Therefore by Theorem F.1 there exists a ï¬nitely supported probability distribution ğœ‚0 with moments
(cid:3)
ğ‘€1, . . . , ğ‘€ğ‘  such that supp
ğœ‚0) âŠ† [âˆ’
(

10âˆšğ‘  ln ğ‘ , 10âˆšğ‘  ln ğ‘ 

]

(cid:1)

.

98

We can assume that ğœ‚0 is symmetric (since if ğ‘§

ğ‘¤
|
is symmetrically distributed and has the same ï¬rst ğ‘  moments as ğ‘§). Thus the mixture distribution
ğœ‚ =
2 each) is symmetric and has
ğœ‚0 +
)
Gaussian moments up to ğ‘ 

ğ›¿ğœ‚1 (where ğœ‚1 takes values
1:

ğœ† with probability 1

are independend, ğ‘§ğ‘¤

ğœ‚0 and ğ‘¤

0, 1
)
(

1
(

ğ‘

/|

âˆ¼

âˆ¼

Â±

âˆ’

ğ›¿

+

ğ”¼
ğ‘¥
ğœ‚

âˆ¼

ğ‘¥ğ‘Ÿ = ğ”¼
ğ‘¥
âˆ¼ğ’©(

0,1
)

ğ‘¥ğ‘Ÿ ,

if 0 6 ğ‘Ÿ 6 ğ‘ 

1 ,

+

and its higher moments satisfy

ğ›¿ğœ†ğ‘Ÿ 6 ğ”¼
ğœ‚
âˆ¼

ğ‘¥

ğ‘¥ğ‘Ÿ 6 ğ›¿ğœ†ğ‘Ÿ

10ğ‘ 

+ (

ğ‘Ÿ ,

)

if ğ‘Ÿ > ğ‘  is even.

G Matrix concentration bounds

In this section, we use standard tools to establish some matrix concentration inequalities that are
essential to our main results.

Our key tools will be the following general result by Rudelson showing convergence of empirical

covariances of random variables.

Fact G.1 (Theorem 1, [Rud99]). Let ğ‘Œ be a random vector in the isotropic position. Let ğ‘Œ1, ğ‘Œ2, . . . , ğ‘Œğ‘ be
ğ‘ independent copies of ğ‘Œ. Then, for some absolute constant ğ¶ > 0,

1
ğ‘

ğ‘

Ã•ğ‘–=1

ğ‘Œğ‘–ğ‘ŒâŠ¤ğ‘– âˆ’

ğ¼

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ğ”¼

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

6 ğ¶

log ğ‘
âˆšğ‘

p

ğ”¼

ğ‘Œ

k

(k

Â·

log ğ‘

log ğ‘ .
1
/
)

We will also use the following simple computation of variances of low-degree polynomials of

product subgaussian random vectors.

|

ğ‘†

ğ‘†:

ğ‘›

âŠ† [

ğ‘Œ, ğ‘¢
h

2ğ‘¡ 6 ğ¶ ğ‘¡

Lemma G.2 (Variance of Polynomials of Independent Subgaussians). Let ğ‘Œ be a product random
variable on â„ğ‘› with coordinates of mean 0, variance 1 satisfying ğ”¼
ğ‘¡ for every unit vector
2ğ‘¡
(
ğ‘¢ for some absolute constant ğ¶ > 0. Let ğ‘ =
â„ğ‘› of degree ğ‘˜ where the
âˆˆ
6 ğ¶ ğ‘¡
sum ranges of multisets ğ‘†

6ğ‘˜ ğ‘ğ‘† ğ‘¦ğ‘† be a polynomial in ğ‘¦
6ğ‘˜ ğ‘2
ğ‘Œ
ğ‘†
(
)
2
Ã
Proof. For any polynomial ğ‘, we write
2 to denote the sum of squares of its coeï¬ƒcients in
2
the monomial basis. For any multilinear polynomial ğ‘, observe that ğ”¼ ğ‘2 =
ğ‘
2. For a non-
2 ğ‘¦2
multilinear ğ‘, we write ğ‘ =
ğ‘† ğ‘ğ‘† such that ğ‘ğ‘† is a multilinear polynomial of degree at
/
= 0 whenever ğ‘† â‰  ğ‘†â€². Now,
most ğ‘˜
. Observe that
ğ‘† k
|
ğ‘† ğ‘2
2
ğ”¼ ğ‘2 =
2.
ğ‘†
k
|
|
(cid:3)
On the other hand, ğ”¼ ğ‘¦2

of size at most ğ‘˜. Then,
Ã
ğ‘

2
k
> 1 for any ğ‘†, ğ”¼ ğ‘¦2
Ã
6 ğ¶ ğ‘˜
6ğ‘˜ ğ”¼ ğ‘¦2
max
ğ‘†

ğ‘† ğ‘¦2
2. Further, ğ”¼ ğ‘¦2
ğ‘†â€²
>
ğ‘† ğ‘2
ğ‘ğ‘†
ğ‘†
2ğ‘˜
(

6ğ‘˜
ğ‘†:
=
ğ‘
Ã
k
ğ‘†. Since ğ”¼ ğ‘¦2
ğ‘†
6
2
ğ‘† ğ‘2
2 Â·

2. Thus, ğ”¼ ğ‘2 >

2 ğ”¼ ğ‘¦2
/

ğ‘ğ‘†ğ‘ğ‘†â€²
2

6ğ‘˜ ğ‘2
ğ‘†.

6 ğ”¼ ğ‘2

2ğ‘¡
(

|
6ğ‘˜

k
ğ‘˜ .

ğ‘† k

ğ‘ğ‘†

ğ‘ğ‘†

ğ‘ğ‘†

|
k

ğ‘†
2
2

Ã

âˆ’

=

ğ‘†

2
2

ğ‘

2

ğ‘†:

ğ‘†:

ğ‘†:

k

k

k

k

k

k

k

k

k

i

]

)

)

)

ğ‘†

ğ‘†

ğ‘†

ğ‘†

ğ‘¡

|

|

|

|

|

|

|

|

Ã

Ã

Lemma G.3. Let ğ‘Œ be a random vector in â„ğ‘› with independent coordinates of mean 0 and variance 1
2ğ‘¡ 6 ğ¶ ğ‘¡
satisfying ğ”¼
ğ‘¡ for some absolute constant ğ¶ > 0. Then, with probability at least 0.99 over
the draw of ğ‘Œ1, ğ‘Œ2, . . . , ğ‘Œğ‘‘ i.i.d. copies of ğ‘Œ,

ğ‘Œ, ğ‘¢
h

2ğ‘¡
(

i

)

1
ğ‘‘

Ã•ğ‘–

ğ‘¡

ğ‘ŒâŠ—
ğ‘–
(

ğ‘Œ âŠ—
ğ‘–

)(

ğ‘¡

âŠ¤
)

âˆ’

ğ”¼
ğ·
ğ‘Œ

âˆ¼

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

for some absolute constant ğ¶â€² > 0.

ğ‘›ğ‘¡

2 log(
/

ğ‘¡

6

ğ¶â€²ğ‘¡

ğ‘›

(

)(

ğ‘¡

)

,

2
1
+
)/
âˆšğ‘‘

ğ‘¡

ğ‘ŒâŠ—

ğ‘¡

ğ‘ŒâŠ—

âŠ¤

(cid:0)

(cid:1) (cid:0)

(cid:1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

99

ğ‘¡

Proof. Let ğ‘€ = ğ”¼
ğ‘¢, ğ‘ŒâŠ—
h
Thus, all eigenvalues of ğ‘€ are between 1 and ğ¶ ğ‘¡

)âŠ¤. Then, quadratic forms
ğ‘¡.

of degree at most ğ‘¡. Thus, using Lemma G.2, we have that

ğ‘¢, ğ‘€ğ‘¢

ğ‘ŒâŠ—
(

ğ‘Œ âŠ—

)(

i

h

i

ğ‘¡

ğ‘¡

is the variance of polynomial ğ‘ =
ğ‘¡.

ğ‘¢, ğ‘€ğ‘¢

6

6

ğ‘¢

ğ‘¢

2

h

i

k

k

2ğ¶ ğ‘¡

2ğ‘¡
(

)

2
2

k

k

2ğ‘ğ‘– for ğ‘ğ‘– = ğ‘ŒâŠ—
1
We will now apply Fact G.1 to the isotropic random vectors ğ‘€âˆ’
/
ğ‘–

ğ‘¡

for 1 6 ğ‘– 6 ğ‘‘.

2ğ‘¡
(

)

Then, we obtain:

ğ”¼

T
2ğ‘
1
2ğ‘ğ‘€âˆ’
1
ğ‘€âˆ’
/
/

ğ¼

6 ğ¶

log ğ‘‘

ğ”¼

2ğ‘
1
ğ‘€âˆ’
/

log ğ‘‘
2

log ğ‘‘
1
/

.

To ï¬nish, we compute ğ”¼

(cid:13)
(cid:13)

2ğ‘
1
ğ‘€âˆ’
/

âˆ’
(cid:13)
log ğ‘‘ 6
(cid:13)

âˆšğ‘‘

p
2
1
ğ‘€âˆ’
/

(cid:16)
(cid:13)
log ğ‘‘ ğ”¼
(cid:13)
k

(cid:17)

(cid:13)
(cid:13)

ğ‘

log ğ‘‘. Next, ğ”¼

ğ‘

log ğ‘‘ = ğ”¼

ğ‘Œ

k
6 1, we obtain:

k
ğ”¼

k
2ğ‘
1
ğ‘€âˆ’
/

k
log ğ‘‘

ğ‘¡ log ğ‘‘ 6
k
log ğ‘‘
1
/

6

ğ‘›(
ğ‘›ğ‘¡

ğ‘¡

ğ‘¡

ğ‘¡

log ğ‘‘

2
)
/

log ğ‘‘ğ¶(
2
/

2
/
)
2ğ¶ ğ‘¡
/
Thus, for using ğ‘› > log ğ‘‘ and Fact G.1, ğ”¼

log ğ‘‘. Using
2
1
(cid:13)
(cid:13)
ğ‘€âˆ’
/
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2ğ‘T
1
2ğ‘ğ‘€âˆ’
1
ğ‘€âˆ’
/
/

(cid:13)
log ğ‘‘
)(
(cid:13)

((
2.
/

ğ‘¡ log ğ‘‘

2
)

2
)
/

(cid:13)
(cid:13)

/

(

)

ğ‘¡

ğ‘¡

Markovâ€™s inequality completes the proof.

(cid:13)
(cid:13)

(cid:16)
(cid:13)
(cid:13)
ğ‘¡
2 log(
+

6 ğ‘›ğ‘¡

/

(cid:13)
(cid:13)
10ğ¶ğ‘¡

ğ‘¡

)

ğ‘›

(

)(

2

1
)/
âˆšğ‘‘

(cid:17)

. Applying

(cid:3)

ğ¼

âˆ’

(cid:13)
(cid:13)

We also state here some standard concentration bounds used in the proofs.

Fact G.4. [LM00]Let ğ‘‹

ğœ’2

ğ‘š, ğ‘¥ > 0, then

âˆ¼

â„™

ğ‘‹

(cid:16)

âˆ’

ğ‘š > 2ğ‘¥

+

â„™

ğ‘š

(

âˆ’

2âˆšğ‘šğ‘¥

6 ğ‘’âˆ’

ğ‘¥

(cid:17)
ğ‘‹ > ğ‘¥
)

6 ğ‘’âˆ’

ğ‘¥2
4ğ‘š

Fact G.5. [Wai19] Let 0 < ğœ€ < 1. The ğ‘›
is, there exists a set ğ‘ğœ€ of unit vectors in â„ğ‘› of size at most
exists some ğ‘£

ğ‘ğœ€ such that

6 ğœ€.

âˆ’

ğ‘¢

ğ‘£

1-dimensional Euclidean sphere has an ğœ€-net of size
such that for any unit vector ğ‘¢

ğ‘›

3
ğœ€

(cid:0)

(cid:1)

k
âˆˆ
Theorem G.6. [Wai19] Let ğ‘Š

âˆ’

k

ğ‘

0, 1
)
(

âˆ¼

ğ‘›

Ã—

ğ‘‘. Then with probability 1

exp

(âˆ’

ğ‘¡

,
2
)

/

âˆ’

ğ‘Š

k

k

6 âˆšğ‘›

âˆšğ‘‘

âˆšğ‘¡

+

+

ğ‘›

3
. That
ğœ€
â„ğ‘› there
(cid:1)

(cid:0)
âˆˆ

and

T

ğ‘Š

ğ‘Š

ğ‘›Id

6 ğ‘‘

2âˆšğ‘‘ğ‘›

ğ‘¡

4

ğ‘¡

ğ‘›

ğ‘‘

âˆ’
(cid:13)
Theorem G.7 (Matrix Bernstein [Tro12]). Consider a ï¬nite sequence
(cid:13)
self-adjoint matrices in â„ğ‘‘1
ğ‘‘2. Assume that each random matrix satisï¬es

p

(cid:13)
(cid:13)

+

+

+

+

(

Ã—

.

)

ğ‘ğ‘˜

{

}

of independent, random,

Deï¬ne

Then, or all ğ‘¡ > 0,

ğ”¼ ğ‘ğ‘˜ = 0 and

ğ‘ğ‘˜

k

k

6 ğ‘… almost surely.

ğœ2 := max

ğ”¼ ğ‘ğ‘˜ ğ‘ğ‘˜

T

,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
Ã•ğ‘˜
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T

ğ”¼ ğ‘ğ‘˜

ğ‘ğ‘˜

.

)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ã•ğ‘˜

((cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

â„™

ğ‘ğ‘˜

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ã•ğ‘˜

 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

6

> ğ‘¡

!

ğ‘‘1 +

(

ğ‘‘2)

exp

(cid:26)

100

ğ‘¡2

2
/
ğ‘…ğ‘¡

ğœ2

âˆ’
+

.

3

(cid:27)

/

Theorem G.8 (Matrix Hoeï¬€ding [Tro12]). Consider a ï¬nite sequence
self-adjoint matrices in â„ğ‘‘

ğ‘‘. Assume that each random matrix satisï¬es

Ã—

ğ‘ğ‘˜

{

}

of independent, random,

Then, for all ğ‘¡ > 0,

where ğœ2 :=

ğ‘˜ ğ´2
ğ‘˜

.

ğ”¼ ğ‘ğ‘˜ = 0 and ğ‘2

ğ‘˜ (cid:22)

ğ´2

ğ‘˜ almost surely.

6 ğ‘‘ exp

> ğ‘¡

!

ğ‘¡2
8ğœ2

(cid:27)

âˆ’

(cid:26)

â„™

ğ‘ğ‘˜

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ã•ğ‘˜

 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
Theorem G.9 (ğ‘˜-sparse norm of a Gaussian matrix). Let ğ‘Š
(cid:13)
1 6 ğ‘˜ 6 ğ‘‘. Then with probability at least 1

(cid:13)
(cid:13)Ã

ğ‘˜

ğ‘˜
ğ‘’ğ‘‘

âˆ’

ğ‘

0, 1
)
(

âˆ¼

ğ‘›

Ã—

ğ‘‘ be a Gaussian matrix. Let

(cid:1)
T
ğ‘¢

(cid:0)

â„ğ‘‘

ğ‘Š ğ‘£ 6 âˆšğ‘›

ğ‘˜ ln

3
s

+

ğ‘’ ğ‘‘
ğ‘˜

.

(cid:19)

(cid:18)

max
â„ğ‘›
ğ‘¢
âˆˆ
=1
ğ‘¢
k

k

max
ğ‘˜-sparse ğ‘£
âˆˆ
=1
ğ‘£

k

k

ğ‘¥

Proof. Let ğ‘£ be some ğ‘˜-sparse unit vector that maximizes the value, and let ğ‘†
nonzero coordinates of ğ‘£. Consider some ï¬xed (independend of ğ‘Š) unit ğ‘˜-sparse vector ğ‘¥
and the set ğ‘†
from ğ‘†
by âˆšğ‘›
By the union bound, the probability that the norm of ğ‘Šğ‘†

of nonzero coordinates of ğ‘¥. If we remove from ğ‘Š all the rows with indices not
. By Theorem G.6 norm of this matrix is bounded
)
(
ğ‘‘
ğ‘¡
.
ğ‘˜
(âˆ’
âˆšğ‘¡ is at most
(cid:1)

ğ‘˜ Gaussian matrix ğ‘Šğ‘†
âˆšğ‘¡ with probability at least exp

. Number of all subsets ğ‘†
is greater than âˆšğ‘›

)
, we get an ğ‘›
ğ‘¥
)
âˆšğ‘˜

be the set of
â„ğ‘‘

of size ğ‘˜ is

ğ‘‘
]
âˆšğ‘˜

(
+

âŠ† [
+

2
)

+

Ã—

+

âˆˆ

ğ‘£

/

)

(

(

(cid:0)

ğ‘¥

ğ‘£

(

)

Taking ğ‘¡ = 4ğ‘˜ ln

/
Lemma G.10. Let ğ‘¤

(

ğ‘’ ğ‘‘

ğ‘

0, 1
)
(

âˆ¼

coordinates of ğ‘¤. Then with probability 1

ğ‘˜

, we get the desired bound.
)

Proof. Let ğ‘† be any ï¬xed subset of
[
ğ‘† ğ‘¤2
vector and by Fact G.4, â„™
ğ‘–
of size ğ‘˜, we get
ğ‘‘

âˆˆ

ğ‘–

[

]

(cid:16)Ã
â„™

ğ‘‘
ğ‘˜

(cid:18)

Â·

(cid:19)

exp

(âˆ’

ğ‘¡

2
)

/

6 exp

ğ‘˜ log2(

ğ‘’ ğ‘‘

ğ‘˜

/

) âˆ’

2

.

ğ‘¡

/

(cid:0)

(cid:1)

(cid:3)

ğ‘‘ be a Gaussian vector and let 1 6 ğ‘˜ 6 ğ‘‘. Let ğ‘†ğ‘˜ be the set of ğ‘˜ largest

ğ‘˜

,

ğ‘˜
ğ‘’ğ‘‘

ğ‘†ğ‘˜ ğ‘¤2
ğ‘–

ğ‘–

6 10ğ‘˜ ln

ğ‘’ ğ‘‘

ğ‘˜

.
)

âˆˆ

/
âˆ’
of size ğ‘˜. Then ğ‘¤ restricted on ğ‘† is a ğ‘˜-dimensional Gaussian
ğ‘¥. By a union bound over all

(cid:1)
Ã
2âˆšğ‘˜ğ‘¥

subsets of

ğ‘‘
]
> 2ğ‘¥

6 ğ‘’âˆ’

(

(cid:0)

ğ‘‘
ğ‘˜

+

(cid:17)

(cid:0)

(cid:1)

ğ‘¤2
ğ‘–

> ğ‘˜

2ğ‘¥

+

+

2âˆšğ‘˜ğ‘¥

!

6 ğ‘’âˆ’

ğ‘¥

ğ‘˜ log2(

+

ğ‘’ğ‘‘

ğ‘˜

) .

/

ğ‘†
Ã•ğ‘–
âˆˆ

Taking ğ‘¥ = 4ğ‘˜ ln

ğ‘’ ğ‘‘

ğ‘˜

)

/

(

we get the desired bound.

(cid:3)

Lemma G.11. For large enough ğ‘› and ğ‘‘ such that ğ‘› 6 ğ‘‘, let ğ‘Š
let ğ‘¢
ğ‘‘

â„ğ‘› be an arbitrary unit vector (which can possibly depend on ğ‘Š). For any ğ‘¡ > 0 let ğ‘†ğ‘¡ =
âˆˆ
ğ‘¢Tğ‘Š
] | |(

ğ‘›

Ã—

ğ‘‘ be a Gaussian matrix and

0, 1
)
(

ğ‘–

{

âˆˆ

> ğ‘¡
}
Then, for any ğ‘¡ > 3âˆšğµ ln ğ‘‘,

. Also let ğµ > 1.
ğ‘†ğ‘¡

ğµ with probability at least 1

2 exp

6 ğ‘›

ğ‘

âˆ¼

ğ‘›

[

)

|

ğ‘–

âˆ’

.
)

(âˆ’

|

|

/

101

 
Proof. Let ğ‘¡ > 3âˆšğµ ln ğ‘‘. For any ï¬xed (independend of ğ‘Š) unit vector ğ‘¥
standard Gaussian variables. For large enough ğ‘‘,

â„ğ‘›,

ğ‘¥Tğ‘Š

(

)

âˆˆ

ğ‘– are i.i.d.

)
Hence the probability that there are 3 6 ğ‘˜ 6 ğ‘‘ coordinates that are larger than ğ‘¡

âˆ’

âˆ’

|(

/

(cid:3)

(cid:2)

(cid:0)

(cid:1)

|

â„™

T
ğ‘¥

ğ‘Š

ğ‘–

> ğ‘¡

1

6 exp

ğ‘¡2

3

.

ln

+

ğ‘‘
ğ‘˜ âˆ’

/

ğ‘‘
ğ‘˜

(cid:18)

(cid:19)

exp

ğ‘˜

âˆ’

ğ‘¡2

3

/

Â·

6 exp

ğ‘˜

1

(cid:20)
â„ğ‘›,

(cid:18)
ğ‘¥
k
ğ‘›
,
)

(cid:0)

âˆˆ
exp

(cid:1)
ğ‘¦
If for unit vectors ğ‘¥, ğ‘¦
k
ğ‘Š
probability at least 1
k
Fact G.5, for any 0 < ğœ€ < 1, for ğœ€ = 1
ğ‘› log 100
ğ‘› log ğ‘‘
exp
ğµ is at most
that
(cid:0)

ğ‘›
2 log ğ‘‘
> ğ‘›
ğ‘†ğ‘¡

6 exp

âˆ’
k

(âˆ’

+
/

âˆ’

(cid:0)

(cid:1)

|

|

10âˆšğ‘‘

1 is at most

âˆ’
2
3 Â·

âˆ’

(cid:18)

ğ‘˜ğ‘¡2 ln ğ‘‘

.

(cid:19)

ğ‘¡2

3

6 exp

ğ‘˜

ln ğ‘‘

ğ‘¡2

3

6 exp

âˆ’

/

(cid:19) (cid:21)
6 ğœ€, then
6 ğœ€
ğ‘Š ğ‘¥
6 10âˆšğ‘‘. Hence if ğœ€ 6 1
there exists an ğœ€-net in ğ‘›

(cid:0)
ğ‘Š ğ‘¦

(cid:1) (cid:3)
ğ‘Š
k
,
10âˆšğ‘‘

(cid:2)
âˆ’

k

k

(
(cid:12)
(cid:12)

âˆ’

. By Theorem G.6, with
k
ğ‘¥Tğ‘Š
6 1. By

ğ‘¦Tğ‘Š
1-dimensional sphere of size

âˆ’ (

)

)

ğ‘–

ğ‘–

(cid:12)
(cid:12)

(for large enough ğ‘‘). By the union bound, the probability

(cid:1)
ğ‘› ln ğ‘‘

exp

(cid:18)

2
3 |

ğ‘†ğ‘¡

ğ‘¡2

|

âˆ’

(cid:19)

6 exp

(âˆ’

ğ‘›

.

)

(cid:3)

ğ‘› log ğ‘‘

(cid:16)p

(cid:17)

The next lemma is the main technical challenge of Section 4.

Theorem G.12. [DM14] Let ğ‘Š
Ã—
and let ğ‘ be the matrix whose diagonal entries are zeros and each non-diagonal entry ğ‘ğ‘–ğ‘— is

log ğ‘‘
(

0, 1
)
(

â†’ âˆ

as ğ‘‘

ğ‘

âˆ¼

)

ğ‘‘, where ğ‘› > ğœ”

. Let 0 6 ğœ 6 ğ‘œ

ğ‘›

ğ‘ğ‘–ğ‘— =

ğ‘ŠTğ‘Š

0
(cid:0)

ğ‘–ğ‘— âˆ’

(cid:1)

sign

ğ‘ŠTğ‘Š

(cid:0)

ğ‘–ğ‘— Â·

(cid:1)

ï£±ï£´ï£²
ï£´
ï£³

ğœ

if

ğ‘ŠTğ‘Š

> ğœ

ğ‘–ğ‘—

otherwise
(cid:0)

(cid:12)
(cid:12)
(cid:12)

(cid:1)

ğ‘œ

(cid:12)
(cid:12)
(cid:12)
1
)
(

âˆ’

Then there exists an absolute constant ğ¶ > 1 such that with probability 1

ğ‘

k

k

6 ğ¶

ğ‘‘

(cid:16)

+

âˆšğ‘‘ğ‘›

exp

(cid:17)

ğœ2
ğ¶ğ‘›

.

(cid:21)

âˆ’

(cid:20)

H Linear Algebra

Lemma H.1. Let ğ‘£ and ğ‘¢ be unit vectors such that

ğ‘£ğ‘£T

k

âˆ’

ğ‘¢ğ‘¢T

k

6 ğœ€. Then

2 > 1

ğ‘£, ğ‘¢

h

i

âˆ’

2ğœ€2.

Proof. Let ğ‘¤ be a unit vector orthogonal to ğ‘¢ such that ğ‘£ = ğœŒğ‘¢
Then

1

âˆ’

+

ğœŒ2ğ‘¤ for some positive ğœŒ 6 1.

T
ğ‘£ğ‘£

T =

ğœŒ2

T
ğ‘¢ğ‘¢

ğ‘¢ğ‘¢

1
+
q
ğ‘¢ğ‘¢T has rank 2, its Frobenius norm is bounded by 2ğœ€, hence

1
q

âˆ’

+

âˆ’

âˆ’

âˆ’

+

ğœŒ

ğœŒ

1

(cid:1)

(cid:0)

T
ğœŒ2ğ‘¢ğ‘¤

1

âˆ’

(cid:0)

ğœŒ2

T
ğ‘¤ğ‘¤

.

(cid:1)

p
T
ğœŒ2ğ‘¤ğ‘¢

Since ğ‘£ğ‘£T

âˆ’

It follows that

4ğœ€2 >

T
ğ‘£ğ‘£

T
ğ‘¢ğ‘¢

2
ğ¹

k

âˆ’

k

= 2

1

2

ğœŒ2

âˆ’

+

2ğœŒ2

1

âˆ’

ğœŒ2

= 2

1

ğœŒ2

.

âˆ’

(cid:1)

(cid:0)
2 = ğœŒ2 > 1

(cid:0)
2ğœ€2 .

âˆ’

ğ‘£, ğ‘¢

h

i

(cid:1)

(cid:0)

(cid:1)

(cid:3)

102

Lemma H.2. Let ğ‘€ be a symmetric matrix such that
top eigenvalue ğœ†1 of ğ‘€ satisï¬es

ğ‘¢ğ‘¢T
6 ğœ€ and the top eigenvector ğ‘£1 of ğ‘€ satisï¬es

2 for some unit vector ğ‘¢. Then the
100ğœ€2.
ğ‘£1, ğ‘¢

6 ğœ€ < 1

2 > 1

ğ‘€

âˆ’

1

k

k

h

i

âˆ’

ğœ†1 âˆ’

|

|

Proof. Consider an eigenvalue decomposition of ğ‘€:

ğ‘€ =

ğ‘‘

Ã•ğ‘—=1

ğœ†ğ‘—ğ‘£ ğ‘—ğ‘£ ğ‘—

T

,

ğ‘‘

ğ‘—=1 is an orthonormal basis in â„ğ‘‘. By triangle inequality

ğ‘£ ğ‘—

}

ğ‘€

âˆ’

T
ğ‘£1ğ‘£1

k + k

ğ‘€

âˆ’

T
ğ‘¢ğ‘¢

6

k

ğ‘€

k

âˆ’

T
ğ‘£1ğ‘£1

ğœ€.

k +

{

k

ğœ†ğ‘‘

|
T
ğ‘¢ğ‘¢

and

6

k

k
6 ğœ€, ğ‘¢Tğ‘€ğ‘¢ > 1

ğ‘€

T
ğ‘£1ğ‘£1

âˆ’
k
ğœ€, hence

âˆ’
ğœ†1 âˆ’ h

ğ‘£1, ğ‘¢

1

i
|
2 , so ğœ†1 > 0, and ğœ†1 6
ğ‘£1, ğ‘¢

1

2

|h

i

âˆ’

|

6 max

{|

1

,

.

ğœ†2|}
ğœ†1|
âˆ’
ğœ€ > 1
2 . Notice that

|

> 1

ğœ†1|
|
T
=
ğ‘£1

|

2

âˆ’
ğ‘€ğ‘£1 âˆ’ h
ğœ€ 6 1
1
ğœ†1 âˆ’
6 2ğœ€. By Pythagorean theorem

6 ğœ€,

ğ‘£1, ğ‘¢

ğ‘£1, ğ‘¢

ğœ€, so

+

+

i

i

h

2

2

|

|

|

|

6 ğœ€.
ğ‘‘
ğ‘—=1h

ğ‘£ ğ‘— , ğ‘¢

i

2 = 1, hence

where

>

ğœ†1|

|

ğœ†2|

|

> . . . >

|

T
ğ‘£1ğ‘£1

k

T
ğ‘£1ğ‘£1

ğ‘€

k

âˆ’

âˆ’

:

k

Letâ€™s bound

Since

ğ‘€

k

âˆ’

ğ‘¢ğ‘¢T

k

hence ğœ†1 >

ğœ€ >
By triangle inequality

âˆ’

âˆ’

ğ‘‘

h

Ã•ğ‘—=2

2 6 2ğœ€,

ğ‘£ ğ‘— , ğ‘¢

i

Ã

=

2

i

|

T
ğ‘£2

|

ğ‘€ğ‘£2 âˆ’ h

ğ‘£2, ğ‘¢

i|

6 ğœ€,

:

ğœ†2|
|
ğ‘£2, ğ‘¢
ğœ†2 âˆ’ h

T
ğ‘£1ğ‘£1

k

T
ğ‘¢ğ‘¢

k

âˆ’

6 4ğœ€.

ğ‘£2, ğ‘¢

so

h

i

2 6 2ğœ€. Now letâ€™s bound

hence

ğœ†2|

|

6 3ğœ€. Therefore

|

By lemma H.1,

ğ‘£, ğ‘¢

h

i

2 > 1

32ğœ€2.

âˆ’
ğ‘‘, ğ‘€

â„ğ‘‘

Ã—

Lemma H.3. Let ğ‘€
Then the top eigenvector ğ‘£1 of ğ‘€ satisï¬es
Proof. Write ğ‘§ = ğ›¼ğ‘£1 +

ğ›¼2ğ‘£

âˆš1

âˆ’

(cid:23)

âˆˆ

h
where ğ‘£

0, Tr ğ‘€ = 1 and let ğ‘§
ğ‘‚

2 > 1

ğ‘£1, ğ‘§

â„ğ‘‘ be a unit vector such that ğ‘§Tğ‘€ ğ‘§ > 1
.
)

âˆˆ
ğœ€
(

âˆ’

i
is a unit vector orthogonal to ğ‘£1.

As ğ‘£1

Tğ‘€ğ‘£1 > ğ‘§Tğ‘€ ğ‘§ and ğ‘£

Tğ‘€ğ‘£
âŠ¥

âŠ¥

âŠ¥
âŠ¥
T
T
ğ‘€ ğ‘§ = ğ›¼2ğ‘£1
ğ‘§
= ğ›¼2
> 1

ğœ†1 âˆ’
ğœ€

ğ‘€ğ‘£1 +
ğ‘£

T

âŠ¥

(cid:0)
âˆ’
6 ğœ€, rearranging

1

âˆ’

ğ‘€ğ‘£
(cid:0)

ğ›¼2

ğ‘£

ğ‘£
(cid:1)

+

âŠ¥

T

âŠ¥
T

ğ‘€ğ‘£

âŠ¥

ğ‘€ğ‘£

âŠ¥

âŠ¥

(cid:1)

ğ›¼2 > 1

ğœ€
âˆ’
âˆ’
ğ‘£
ğœ†1 âˆ’

ğ‘£

Tğ‘€ğ‘£
âŠ¥
Tğ‘€ğ‘£
âŠ¥

âŠ¥

âŠ¥

> 1

2ğœ€.

âˆ’

103

(cid:3)

ğœ€.

âˆ’

(cid:3)

Fact H.4. Let ğ´, ğµ

âˆˆ
Lemma H.5. Let ğ‘‹

â„ğ‘‘

ğ‘‘, ğ´, ğµ

Ã—

0. Then

(cid:23)

ğ´, ğµ

h

i

> 0.

âˆˆ

â„ğ‘‘

Ã—

ğ‘‘ be a positive semideï¬nite matrix. Then for any ğ´

â„ğ‘‘

ğ‘‘,

Ã—

âˆˆ

ğ´, ğ‘‹

|h

6

ğ´

k

k Â·

i|

Tr ğ‘‹ .

Proof. Since ğ‘‹ is positive semideï¬nite, ğ‘‹ =

ğ‘‘
ğ‘–=1 ğœ†ğ‘– = Tr ğ‘‹. Hence

ğ‘‘
ğ‘–=1 ğœ†ğ‘– ğ‘§ğ‘– ğ‘§ğ‘–

T for unit vectors ğ‘§ğ‘– such that ğœ†ğ‘– > 0 and

Ã

ğ´, ğ‘‹

|h

i|

=

T

Tr ğ‘‹

(cid:12)
(cid:12)

Lemma H.6. Let ğ‘‹

â„ğ‘‘

Ã—

âˆˆ

ğ‘‘

=

ğ´

ğœ†ğ‘– Tr ğ‘§ğ‘– ğ‘§ğ‘–

ğ´

T

T

ğœ†ğ‘– Tr ğ‘§ğ‘–

ğ´ğ‘§ğ‘–

(cid:12)
(cid:12)

(cid:12)
Ã•ğ‘–=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
Ã•ğ‘–=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ğ‘‘ be a positive semideï¬nite matrix. Then for any ğ‘, ğ‘

Ã•ğ‘–=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ğ‘‘

6

ğœ†ğ‘–

k

=

ğ´

k

ğ´

k

k Â·

Tr ğ‘‹ .

(cid:3)

â„ğ‘‘,

âˆˆ

Ã

=

ğ‘‘

T
ğ‘ğ‘

, ğ‘‹

2 6

i
ğ‘ğ‘T, ğ‘‹

h
> 0 and

T
ğ‘ğ‘

, ğ‘‹

T
ğ‘ğ‘

, ğ‘‹

.

i

i Â· h

h

ğ‘ğ‘T, ğ‘‹

> 0. Notice that if the inequality is true for some
â„ğ‘‘, it is also true for ğ‘1ğ‘, ğ‘2ğ‘ for all positive numbers ğ‘1, ğ‘2. So we can assume without loss of
>
ğ‘

= 1. Consider

> 0 and

ğ‘ğ‘T, ğ‘‹

ğ‘ğ‘T, ğ‘‹

=

ğ‘

ğ‘

ğ‘

ğ‘

ğ‘

ğ‘

ğ‘

h

h

i

i

T, ğ‘‹
)

i

h(

âˆ’

)(

âˆ’

T, ğ‘‹
)

i

i

h

i

h(

+

)(

+

Proof. By Fact H.4,
ğ‘, ğ‘
generality that
0 . We get

âˆˆ

h

and

hence

ğ‘ğ‘T, ğ‘‹

2 6 1.

i

h

T
ğ‘ğ‘

, ğ‘‹

2

h

6

i

h

T
ğ‘ğ‘

, ğ‘‹

i + h

T
ğ‘ğ‘

, ğ‘‹

6 2

i

2

h

âˆ’

T
ğ‘ğ‘

, ğ‘‹

T
ğ‘ğ‘

, ğ‘‹

6

i

h

i + h

T
ğ‘ğ‘

, ğ‘‹

i

6 2 ,

(cid:3)

I Experimental Setup

)

]

ğ‘‘

âˆ¼

ğ‘

âŠ† [

0, Idğ‘›
(

In the experiments, the instances were sampled from the planted distributions of models 6.3 and
6.6 with the diï¬€erence that ğ‘¢
and ğ‘£ is a ğ‘˜-sparse unit vector obtained sampling a
and then a unit vector with support ğ‘†. All the algorithms returned the
random ğ‘˜-subset ğ‘†
top ğ‘˜ coordinates of their estimation vector. Figure 2, 2 plot the absolute correlation between ğ‘£ and
its estimate. Each plot was obtained averaging multiple independent runs on the same parameters,
for each algorithm the shadowed part corresponds to the interval containing 50% of the results,
the line corresponds to the mean of the results in such interval.
In Figure 2b, the adversarial matrix ğ¸ is sampled according to model 6.6 for ğ‘  = 2, that is the ï¬rst 2
moments of ğ‘Œ are Gaussian. Similarly, in Figure 2c, ğ¸ is sampled according to model 6.6 for ğ‘  = 4,
so the ï¬rst 4 moments of ğ‘Œ are Gaussian.

Experiments were done on a laptop computer with a 3.5 GHz Intel Core i7 CPU and 16 GB of

RAM, random instances were obtained using Numpy pseudo-random generator.

104

