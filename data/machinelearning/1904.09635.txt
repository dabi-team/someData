9
1
0
2

r
p
A
1
2

]
T
S
.
h
t
a
m

[

1
v
5
3
6
9
0
.
4
0
9
1
:
v
i
X
r
a

Achieving the Bayes Error Rate in Synchronization and
Block Models by SDP, Robustly

Yingjie Fei and Yudong Chen
School of Operations Research and Information Engineering
Cornell University
{yf275,yudong.chen}@cornell.edu

Abstract

We study the statistical performance of semideﬁnite programming (SDP) relaxations for
clustering under random graph models. Under the Z2 Synchronization model, Censored Block
Model and Stochastic Block Model, we show that SDP achieves an error rate of the form

(cid:104)

− (cid:0)1 − o(1)(cid:1)¯nI∗(cid:105)
.

exp

Here ¯n is an appropriate multiple of the number of nodes and I∗ is an information-theoretic
measure of the signal-to-noise ratio. We provide matching lower bounds on the Bayes error for
each model and therefore demonstrate that the SDP approach is Bayes optimal. As a corollary,
our results imply that SDP achieves the optimal exact recovery threshold under each model.
Furthermore, we show that SDP is robust: the above bound remains valid under semirandom
versions of the models in which the observed graph is modiﬁed by a monotone adversary.
Our proof is based on a novel primal-dual analysis of SDP under a uniﬁed framework for all
three models, and the analysis shows that SDP tightly approximates a joint majority voting
procedure.

1 Introduction

Clustering and community detection in graphs is an important problem lying at the intersection of
computer science, optimization, statistics and information theory. Random graph models provide
a venue for studying the average-case behavior of these problems. In these models, noisy pairwise
observations are generated randomly according to the unknown clustering structure of the nodes.
In its basic form, such a model involves n nodes divided into two clusters, which can be represented
by a vector σ∗ ∈ {±1}n. For each pair of nodes i and j, one observes a number Ai j ∈ R generated
independently based on the sign of σ∗
j, that is, whether the two nodes are in the same cluster or
not. Given one realization of the random graph A = (Ai j) ∈ Rn×n, the goal is to estimate the vector
σ∗, or equivalently, the matrix Y∗ (cid:66) (σ∗
j) ∈ {±1}n×n. Among the most popular random graph
models are the Z2 Synchronization (Z2) model, Censored Block Model (CBM) and Stochastic
Block Model (SBM), where Ai j follows the Gaussian, censored ±1 and Bernoulli distributions,
respectively (see Section 3 for the details). We consider these three models in this paper.

i σ∗

i σ∗

Clustering is a challenging problem involving discrete and hence non-convex optimization.
SDP relaxations have emerged as an eﬃcient and robust approach to this problem, and recent
work has witnessed the advances in establishing rigorous performance guarantees for SDP (see
Section 2 for a review of this literature). Such guarantees are typically stated in terms of a signal-
to-noise ratio (SNR) measure I∗ that depends on the speciﬁc random model (see Equation (5)).

1

 
 
 
 
 
 
In terms of controlling the estimation error of SDP, the best and most general results to date are
given in the line of work in [31, 23], which proves that the optimal SDP solution (cid:98)Y satisﬁes the
bound

err((cid:98)σsdp, σ∗) (cid:46) 1
n2

(cid:107)(cid:98)Y − Y∗(cid:107)1 (cid:46) exp

(cid:35)

(cid:34)
−

nI∗
C

,

(1)

where C > 0 is a large constant, (cid:107) · (cid:107)1 denotes the entrywise (cid:96)1 norm, and err((cid:98)σsdp, σ∗) denotes
the fraction of nodes mis-clustered by an estimate (cid:98)σsdp ∈ {±1}n, extracted from (cid:98)Y, of the ground-
truth cluster labels σ∗. The above result is, however, unsatisfactory due to the presence of a large
multiplicative constant C in the exponent, rendering the bound fundamentally sub-optimal. In
particular, the interesting regime for proving an error bound is when nI∗ ≤ 2 log n, as otherwise
SDP is already known to attain zero error. With a large C in the exponent, the result in (1)
provides a rather loose, sometimes even uninformative,1 bound in this regime. Moreover, this
sub-optimality is intrinsic to the proof techniques used and cannot be avoided simply by more
careful calculations.

In this paper, we establish a strictly tighter, and essentially optimal, error bound on SDP. Let

¯n = n for Z2 and CBM, and ¯n = n

2 for SBM.

Theorem 1 (Informal). As n → ∞, with probability tending to one, the optimal solution (cid:98)Y of the
SDP relaxation satisﬁes

(cid:107)(cid:98)Y − Y∗(cid:107)1 ≤ exp

(cid:104)

− (cid:0)1 − o(1)(cid:1)¯nI∗(cid:105)

1
n2

,

(2)

Moreover, the explicit label estimate (cid:98)σsdp computed by taking entrywise signs of the top eigenvec-
tor of (cid:98)Y satisﬁes

err((cid:98)σsdp, σ∗) ≤ exp

(cid:104)

− (cid:0)1 − o(1)(cid:1)¯nI∗(cid:105)
.

(3)

In all three models, the error exponent I∗ is a form of Renyi divergence. See Theorem 3 for the
precise statement of our results as well as an explicit, non-asymptotic estimate of the o(1) term.
One should compare this result with the following minimax lower bound, which shows that any
estimator (cid:98)σ must incur an error

err((cid:98)σ, σ∗) ≥ exp

(cid:104)

− (cid:0)1 + o(1)(cid:1)¯nI∗(cid:105)

.

(4)

as the latter represents the best achievable Bayes risk of the problem. For SBM, this bound is
established in [62]; for Z2 and CBM, the above lower bound is new and formally established in
Theorem 2. In view of the above upper and lower bound, we see that SDP achieves the optimal
Bayes error under all three models.

Optimality as a surprise? The result above has come as unexpected to us, as it shows that
relaxing the original discrete clustering problem via SDP incurs essentially no loss in terms of
statistical accuracy. As we discuss in Section 1.1 and further elaborate in Section 5, we prove
this result by showing, via a novel primal-dual analysis, that SDP tightly approximates a majority
voting procedure, and this procedure leads to the optimal error exponent I∗. Interestingly, our
analysis is not tethered to the optimality of (cid:98)Y to the SDP; rather, it only relies on the fact that (cid:98)Y
is feasible and no worse in objective value than Y∗, and thus the bounds (2) and (3) in fact hold
for any matrix Y with these two properties. This kind of leeway in the analysis makes the bounds
robust, as we elaborate next.

1Note that 1

n2 (cid:107)(cid:98)Y − Y∗(cid:107)1 is trivially upper bounded by 2 since (cid:98)Y, Y∗ ∈ [−1, 1]n×n.

2

Robustness. We show that the bounds in Theorem 1 continue to hold under the so-called mono-
tone semirandom model [24], where an adversary is allowed to make arbitrary changes to the
graph in a way that apparently strengthens connections within each cluster and weakens connec-
tions between clusters. While this model seemingly makes the clustering problem easier, they
in fact foil, provably, many existing algorithms, particularly those that over-exploit the speciﬁc
structures of standard SBM in order to achieve tight recovery guarantees [24, 45]. In contrast,
our results show that SDP relaxations enjoy a robustness property that is possessed by few other
algorithms. Importantly, this generalization can be achieved with little extra eﬀort from our main
result (see Theorem 4 and its proof).

Exact recovery. As another illustration of the strength of Theorem 1, we note that it implies
sharp condition for SDP to recover σ∗ exactly. In particular, when ¯nI∗ > (1 + δ) log n for any
positive constant δ, the bound (3) ensures that err((cid:98)σsdp, σ∗) < 1
n and hence err((cid:98)σsdp, σ∗) = 0.
Moreover, the lower bound (4) shows that exact recovery is information-theoretically impossible
when ¯nI∗ < log n. In the literature, establishing such tight exact recovery thresholds often in-
volves specialized and sophisticated arguments, and has been the milestones in the remarkable
recent development on community detection (see Section 2 for a discussion of related work). We
recover these results, for all three models, as a corollary of our main theorem by plugging in the
corresponding expressions of I∗ and ¯n. In fact, the non-asymptotic version of Theorem 1 guar-
antees exact recovery via SDP with an explicit second-order term δ = O(cid:0)1/
log n(cid:1), which is a
reﬁnement of existing results.

(cid:112)

1.1 Primal-dual analysis

Key to the establishment of our results is a novel analysis that exploits both primal and dual
characterizations of the SDP. To set the context, we note that the sub-optimal bound (1) in [23, 29]
is established by utilizing the primal optimality of the SDP solution (cid:98)Y. Their arguments, however,
are too crude to provide a tight estimate of the multiplicative constant C in the exponent. On
the other hand, work on exact recovery for SDP typically makes use of a dual analysis [33, 10];
in particular, the optimality of Y∗ is certiﬁed by showing the existence of a corresponding dual
optimal solution, often explicitly in the form of a diagonal matrix D with Dii = σ∗
j. As
i
this “dual certiﬁcate” D is tied to (and constructed using) Y∗, such a certiﬁcation approach would
only succeed when the SDP indeed admits Y∗ as an optimal solution.

j Ai jσ∗

(cid:80)

Here we are concerned with the setting where the optimal solution (cid:98)Y is diﬀerent from Y∗, and
our goal is to bound their diﬀerence. As it is a priori unknown what (cid:98)Y should look like, we do not
know which matrix to certify or how to construct its associated dual solution, rendering the above
dual certiﬁcation argument inapplicable. Instead, we make use of the fact that (cid:98)Y is feasible to the
SDP and has a better primal objective value than Y∗, that is, (cid:98)Y lies in the sublevel set deﬁned by
Y∗ and the constraints of the SDP. We then characterize the diameter of this sublevel set by using,
perhaps surprisingly, the dual certiﬁcate D of Y∗. Our analysis is thus fundamentally diﬀerent
from the dual certiﬁcation analysis in existing work, which only applies when the sublevel set
consists of a single element Y∗. At the same time, we make use of D in a crucial way to achieve
an exponential improvement over previous primal analysis.

Note that our analysis, and hence our error bounds as well, actually apply to every element
of this sublevel set, not just the optimal solution (cid:98)Y. As can be seen in our proof, this ﬂexibility
plays an important role in establishing the aforementioned robustness results under semirandom
and heterogeneous SBMs. On the other hand, however, with this level of generality we probably
should not expect the second-order o(1) term in our bounds to be optimal.

3

Finally, we emphasize that our results for Z2, CBM and SBM are proved under a uniﬁed
framework. The main proof steps are deterministic and hold for the three models at once; only
certain probabilistic arguments are model-speciﬁc. In Section 5 we outline this proof framework,
and provide intuitions on the majority voting mechanism that drives the error rate e−¯nI∗
. We believe
that this uniﬁed framework may be broadly useful in studying SDP relaxations for other discrete
problems under average-case/probabilistic settings.

1.2 Paper organization

In Section 2, we review related work on Z2, CBM and SBM. In Section 3, we formally introduce
the models and the SDP relaxation approach. In Section 4, we present our main results, with a
discussion on their consequences and comparison with existing work. We outline the main steps
of the proofs and discuss the intuitions in Section 5, with the complete proofs deferred to the
appendix. The paper is concluded in Section 6 with a discussion on future directions.

2 Related work

There is a large array of recent results on community detection and graph clustering, in particular,
under Z2, CBM and SBM. The readers are referred to the surveys [1, 47, 41] for comprehensive
reviews. Without trying to enumerate this body of work, here we restrict attention to those that
study sharp performance bounds, with a particular focus on work on the SDP relaxation approach.
A more detailed, quantitative comparison with our results is provided in Section 4 after our main
theorems.

To begin, we note that existing work has considered several recovery criteria for an estimator
2 ; partial
2 ); exact recovery means err((cid:98)σ, σ∗) = 0 [1].

(cid:98)σ of σ∗: weak recovery means (cid:98)σ is better than random guess, that is, err((cid:98)σ, σ∗) < 1
recovery means err((cid:98)σ, σ∗) ≤ δ for a given δ ∈ (0, 1

2.1 Z2 Synchronization and Censored Block Model
The Z2 model, being a simpliﬁed version of the angular/phase synchronization problem, is studied
in [11], which argues that exact recovery is possible if and only if I∗ > log n
n . The work in [10]
and [5] shows this optimal exact recovery threshold is achieved by SDP and a spectral algorithm,
respectively. The work in [52, 40] considers low-rank matrix estimation under a spiked Wigner
model—of which Z2 is a special case—and identiﬁes the weak recovery threshold.

CBM is considered in [2, 32], which identiﬁes suﬃcient and necessary conditions for exact
recovery. They also show that SDP achieves a sub-optimal exact recovery threshold, which is
further improved to be optimal in [10, 32]. CBM is a special case of the so-called Labelled
SBM, whose weak recovery threshold is studied in [35, 39]. Achieving tight partial/weak recovery
guarantees in CBM is challenging due to the sparsity of the observations. A sub-optimal partial
recovery error bound can be achieved by a spectral algorithm with trimming [18]. The work of
[55] studies a sophisticated spectral algorithm based on the non-backtracking operator or Bethe
Hessian, and shows that it achieves the optimal weak recovery threshold.

For both Z2 and CBM, we establish for the ﬁrst time that SDP has the optimal error rates for
partial recovery. Our results also imply, as an immediate corollary, that SDP achieves the optimal
exact recovery threshold as well as a sub-optimal weak recovery threshold.

4

2.2 Stochastic Block Model

SBM is arguably the most studied out of these three models. Most related to us is a line of work
that characterizes minimax optimal error rates for partial recovery. For the binary symmetric
SBM, the work [62] establishes the aforementioned minimax lower bound (4). They also provide
an exponential-time algorithm that achieves a matching upper bound (up to an o(1) factor in the
exponent). Much research eﬀort focuses on developing computationally feasible algorithms, and
identifying the minimax rates in more general settings [26, 27, 59, 60, 61, 63, 64]. The monograph
[25] provides a review on recent work on this front. We note that this line of work does not consider
the SDP relaxation approach nor deliver robustness guarantees as we do. Nevertheless, we will
compare our results with theirs after stating our main theorems.

For exact recovery under binary symmetric SBM with p, q (cid:16) log n

n , the work in [3, 50] es-
tablishes the suﬃcient and necessary condition (
. Follow-up work develops
eﬃcient algorithms for exact recovery and considers extensions to more general SBMs; see, e.g.,
[6, 8, 5, 37, 53]. As mentioned, our results imply sharp bounds for exact recovery.

q)2 > 2 log n

p −

√

√

n

Weak recovery under the binary symmetric SBM is most relevant in the sparse regime p, q (cid:16) 1
n .
Work of [39, 44, 49] establishes that the necessary and suﬃcient condition of weak recovery is
n(p−q)2
p+q > 2. Subsequent work proves similar phase transitions and shows that various algorithms
achieve weak recovery above the optimal threshold for the SBM with k ≥ 2 and possibly unbal-
anced clusters; see, e.g., [7, 4, 12, 14, 16, 19, 51, 56]. As discussed later, our results also imply
weak recovery guarantees with a sub-optimal constant.

2.3 Optimality and robustness of SDP

For SBM, SDP has been proven to succeed in exact and weak recovery above the corresponding
optimal thresholds (sometimes under additional assumptions). In particular, see [9, 10, 33] for
exact recovery, and [46] for weak recovery. Prior to our work, SDP was not known to achieve the
optimal error rate between the exact and weak recovery regimes. Sub-optimal polynomial rates
are ﬁrst proved in [31], later improved to exponential in [23], and further generalized in [22, 29].
Robustness has been recognized as a distinct feature of the SDP approach as compared to
other more specialized algorithms for SBMs. Work in this direction has established robustness
of SDP against random erasures [32, 34], atypical node degrees [31] and adversarial corruptions
[33, 46, 15, 43]. The work in [45] investigates the relationship between statistical optimality and
robustness under monotone semirandom models; we revisit this result in more details later.

3 Problem Set-up

In this section, we formally deﬁne the models and introduce the SDP relaxation approach.

3.1 Notations

Vectors and matrices are denoted by bold letters. For a vector u, ui and u(i) both denote its i-th
(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)Mi j
entry. For a matrix M, we let Mi j denote its (i, j)-th entry, Tr(M) its trace, and (cid:107)M(cid:107)1 (cid:66) (cid:80)
its entry-wise (cid:96)1 norm. We write M (cid:23) 0 if M is symmetric positive semideﬁnite. The trace inner
product between two matrices is (cid:104)M, G(cid:105) (cid:66) Tr(M(cid:62)G) = (cid:80)
i, j Mi jGi j. Denote by I and J the n × n
identity matrix and all-one matrix, respectively, and denote by 1 the all-one column vector of
length n.

i, j

5

Ber(µ) denotes the Bernoulli distribution with mean µ ∈ [0, 1]. For a positive integer i, let
[i] (cid:66) {1, 2, . . . , i}. For a real number x, (cid:100)x(cid:101) denotes its ceiling and (cid:98)x(cid:99) denotes its ﬂoor. I{·} is the
indicator function. For two non-negative sequences {an} and {bn}, we write an = O(bn), bn = Ω(an)
or an (cid:46) bn if there exists a universal constant C > 0 such that an ≤ Cbn for all n. We write an (cid:16) bn
if both an = O(bn) and an = Ω(bn) hold. Asymptotic statements are with respect to the regime
n → ∞, in which case we write an = o(bn) and bn = ω(an) if limn→∞ an/bn = 0.

3.2 Models

In this section, we formally describe four models for generating the observed matrix A from the
unknown ground-truth label vector σ∗ ∈ {±1}n.

In Z2 [10], each Ai j is generated by adding Gaussian noise to σ∗

i σ∗

j. Therefore, the matrix A

contains noisy observations of the true relative signs between each pair of nodes.

Model 1 (Z2 Synchronization). The observed matrix A ∈ Rn×n is symmetric with its entries
{Ai j, i ≤ j} generated independently by

Ai j ∼ N(σ∗

i σ∗

j, τ2),

where τ > 0 is allowed to scale with n.2

In CBM, each Ai j is generated by ﬂipping σ∗

j with probability (cid:15) and then erasing it with
probability 1 − α. One may interpret A as the edge-censored version of a noisy signed network [2].

i σ∗

Model 2 (Censored Block Model). The observed matrix A ∈ {0, ±1}n×n is symmetric with its
entries {Ai j, i ≤ j} generated independently by

with probability (w.p.) α(1 − (cid:15)),

Ai j =






j

i σ∗
σ∗
i σ∗
−σ∗
0

j w.p. α(cid:15),

w.p. 1 − α,

where α ∈ (0, 1] is allowed to scale with n, and (cid:15) ∈ (0, 1

2 ) is a constant.

In SBM, each Ai j is a Bernoulli random variable, whose mean is higher if σ∗

= 1. There-
fore, A is the adjacency matrix of a random graph in which nodes in the same cluster are more
likely to be connected than those in diﬀerent clusters [36].

i σ∗

j

Model 3 (Binary symmetric SBM). Suppose that the ground-truth σ∗ ∈ {±1}n satisﬁes (cid:104)σ∗, 1(cid:105) = 0.
The observed matrix A ∈ {0, 1}n×n is symmetric with its entries {Ai j, i ≤ j} generated indepen-
dently by

Ai j ∼





Ber(p)

Ber(q)

if σ∗
if σ∗

i σ∗
i σ∗

j

j

= 1,
= −1,

where 0 < q < p < 1 are allowed to scale with n.

2In this and the next three models, we assume that the diagonal entries of A are random, which is inconsequential:
these entries are independent of the ground-truth σ∗, and they have no eﬀect on the solutions of the SDP relaxations (7)
or (8) due to the diagonal constraints therein.

6

In both Model 1 (Z2) and Model 2 (CBM), there can be any number of ±1’s in the ground-truth
label vector σ∗ ∈ {±1}n. In Model 3 (binary symmetric SBM), the cluster labels σ∗ are assumed
to contain the same number of 1’s and −1’s, so the two clusters have equal size. Despite their
simple forms, the above models have been of central importance in studying fundamental limits
of clustering problems [3, 33, 39, 44, 46, 49, 50, 2, 10].

For the purpose of studying the robustness properties of SDP relaxation, we consider a semi-
random generalization of the binary symmetric SBM. In this model, a so-called monotone adver-
sary, upon observing the random adjacency matrix A generated from SBM and the ground-truth
clustering σ∗, modiﬁes A by arbitrarily adding edges between nodes of the same cluster and
deleting edges between nodes of diﬀerent clusters.

Model 4 (Semirandom SBM). A monotone adversary observes A and σ∗ from Model 3, picks an
arbitrary set of pairs of nodes L ⊂ {(i, j) ∈ [n] × [n] : i < j}, and outputs a symmetric matrix
ASR ∈ {0, 1}n×n such that for each i < j,

ASR
i j

=






1

0
Ai j,

if (i, j) ∈ L, σ∗
if (i, j) ∈ L, σ∗
if (i, j) (cid:60) L.

j

i σ∗
i σ∗

j

= 1,
= −1,

Note that the set L is allowed to depend on the realization of A.

Semirandom models have a long history with many variants [13]. Model 4 above has been
considered in [24, 45] for SBM. While seemingly revealing more information about the underlying
cluster structure, the semirandom model in fact destroys many local structures of the basic SBM,
thus frustrating many algorithms that over-exploit such structures.
In contrast, SDP is robust
against the monotone adversary under Model 4, as we shall see in Section 4.3 below.

Remark 1. One may deﬁne semirandom versions of Z2 and CBM in an analogous fashion as
above; that is, the adversary may choose a set L and positive numbers {ci j, i < j}, and then change
Ai j and A ji to Ai j + ci jσ∗
j for each (i, j) ∈ L. It can be shown that SDP achieves the same
performance guarantees in these semirandom settings of Z2 and CBM as in the original models.
For conciseness we omit such details and only focus on the semirandom extension of SBM.

i σ∗

For each model discussed above, we deﬁne a measure of the signal-to-noise ratio (SNR):

I∗ (cid:66)






(2τ2)−1,
(cid:16)√

α(1 − (cid:15)) −
(cid:104)√

√
α(cid:15)
pq + (cid:112)

−2 log

(cid:17)2

,

(cid:105)
(1 − p)(1 − q)

for Model 1,

for Model 2,
for Models 3 and 4.

,

(5)

= −σ∗

In each case, I∗ is a form of Renyi divergence of order 1
2 [28] between the distributions of Ai j
i j(cid:48) = 1.
In particular, for Z2, I∗ is half of the Renyi divergence (or
and Ai j(cid:48) with σ∗
i j
equivalently, the Kullback–Leibler divergence) between N(1, τ2) and N(−1, τ2). For CBM, we
have I∗ ≈ − log(1 − I∗) with the latter being half of the Renyi divergence between two random
variables H and −H, where H has probability mass function α(1−(cid:15))·δ1 +α(cid:15) ·δ−1 +(1−α)·δ0 and δa
denotes the Dirac delta function centered at a.3 In SBM, I∗ is the Renyi divergence between Ber(p)
and Ber(q). These divergences, and their ﬁrst-order approximations (discussed in Section A), are
commonly used as SNR measures in previous work on these models (e.g., [2, 10, 62]).

3In fact, in this case I∗ is the squared Hellinger distance between H and −H.

7

Finally, we deﬁne the following distance measure between two vectors of cluster labels σ, σ(cid:48) ∈

{±1}n:

err(σ, σ(cid:48)) (cid:66) min
g∈{±1}

1
n

(cid:88)

i∈[n]

I{gσi (cid:44) σ(cid:48)
i}.

In words, err(σ, σ(cid:48)) is the fraction of nodes that are assigned a diﬀerent label under σ and σ(cid:48),
modulo a global ﬂipping of signs. With σ∗ being the true labels, err((cid:98)σ, σ∗) measures the relative
error of the estimator (cid:98)σ.

3.3 SDP relaxation

The SDP formulations we consider can be derived as the convex relaxation of the MLE of σ∗.
Under Models 1 or 2, the MLE (cid:98)σmle is given by the solution of the discrete and non-convex
optimization problem

A, σσ(cid:62)(cid:69)
(cid:68)

max
σ∈{±1}n

.

(6)

The MLE under Model 3 includes the extra constraint (cid:104)σ, 1(cid:105) = 0 due to the balanced-cluster
assumption. Derivation of the MLE in this form is now standard; see for example [11, 10] for Z2,
[34] for CBM, and [41] for SBM. Now deﬁne the lifted variable Y = σσ(cid:62), and observe that Y
satisﬁes Y (cid:23) 0, Yii = (σi)2 = 1 for i ∈ [n]. Dropping the constraints that Y has rank one and
binary entries, we obtain the following SDP relaxation of the MLE (6) for Models 1 or 2:

(cid:98)Y = arg max
Y∈Rn×n

(cid:104)A, Y(cid:105)

s.t. Y (cid:23) 0,

Yii = 1, ∀i ∈ [n].

(7)

For Model 3, using the same reasoning and in addition replacing the (cid:104)σ, 1(cid:105) = 0 constraint by
(cid:104)Y, J(cid:105) = (cid:68)

σσ(cid:62), 11(cid:62)(cid:69) = (cid:104)σ, 1(cid:105)2 = 0, we arrive at the relaxation:

(cid:98)Y = arg max
Y∈Rn×n

(cid:104)A, Y(cid:105)

s.t. Y (cid:23) 0,

Yii = 1, ∀i ∈ [n],
(cid:104)Y, J(cid:105) = 0.

(8)

We also use this SDP for the semirandom Model 4.

The optimization problems (7) and (8) are standard SDPs solvable in polynomial time. We
remark that neither SDP requires knowing the parameters of the data generating processes (that is,
τ2, α, (cid:15), p and q in Models 1–3).4 The SDP (7) was considered in [11, 10] and [34] for studying
the exact recovery threshold in Z2 and CBM, respectively, and the SDP (8) was considered in [33]
for exact recovery under the binary symmetric SBM. These formulations can be further traced
back to the work of [24] on SDP relaxation for MIN BISECTION.

We consider the SDP solution (cid:98)Y as an estimate of the ground-truth matrix Y∗ := σ∗(σ∗)(cid:62),
and seek to characterize the accuracy of (cid:98)Y in terms of the (cid:96)1 error (cid:107)(cid:98)Y − Y∗(cid:107)1. Note that (cid:98)Y is not
necessarily a rank-one matrix of the form (cid:98)Y = σσ(cid:62). To extract from (cid:98)Y a vector of binary estimates
of cluster labels, we take the signs of the entries of the top eigenvector of (cid:98)Y (where the sign of 0 is
1, an arbitrary choice). Letting (cid:98)σsdp ∈ {±1}n be the vector obtained in this way, we study the error
of (cid:98)σsdp as an estimate of the ground-truth label vector σ∗, as measured by err((cid:98)σsdp, σ∗).

4The SDP (8) for SBM does require the knowledge of two equal-size clusters.

8

4 Main results

We present our main results in this section. Henceforth, let ¯n := n in Models 1 (Z2) and 2 (CBM),
and ¯n := n
2 in Models 3 and 4 (SBM and its semirandom version). To see why this deﬁnition of
¯n is natural, we note that in Z2 and CBM, the cluster sizes (i.e., numbers of 1’s and −1’s in σ∗)
do not aﬀect the hardness of the problem as the distribution is symmetric, and hence ¯n is simply
the number of nodes; in binary symmetric SBMs, recovery is most diﬃcult when the clusters have
equal size5—which is the setting we consider—and accordingly ¯n is the cluster size.

4.1 Minimax lower bounds

Let (cid:96)1(σ) denote the number of 1’s in σ. To state the lower bounds, we consider the following
parameter space:

Θ(n) (cid:66)





{±1}n ,
(cid:110)
σ ∈ {±1}n : (cid:96)1(σ) ∈

(cid:105)(cid:111)

(cid:104) n
2β , nβ

2

,

for Models 1 and 2,
for Model 3,

(9)

where β is any number larger than 1 + C/n with C > 0 being a large enough numerical constant.
For Z2 and CBM, Θ(n) is the set of all possible cluster label vectors. For SBM, Θ(n) consists
of label vectors with (roughly) equal-sized clusters; here we allow for a slight ﬂuctuation in the
cluster sizes in SBM following [62].6

The following theorem gives the minimax lower bound for each model.

Theorem 2 (Lower bound). For any constant c0 ∈ (0, 1), the following holds for Model 1, Model 2
with I∗ = o(1), and Model 3 with 0 < q < p < 1 − c0. If nI∗ → ∞ as n → ∞, then we have

inf
(cid:98)σ

sup
σ∈Θ(n)

Eσ err((cid:98)σ, σ) ≥ exp (cid:2)−(cid:0)1 + o(1)(cid:1)¯nI∗(cid:3) ,

where Eσ denotes expectation under the distribution of A with σ being the ground truth, and the
inﬁmum is taken over all estimators of the ground truth (i.e., measurable functions of A).

For Models 1 and 2, the proof is given in Section B. For Model 3, the above result is part of

[62, Theorem 2.1].

4.2 Upper bounds on the SDP errors

We next provide our main results on the error rate of the SDP relaxations (7) and (8). Deﬁne the
following sublevel set (or superlevel set to be precise):

Y(A) (cid:66) (cid:110)

(cid:111)
Y ∈ Rn×n : (cid:104)A, Y(cid:105) ≥ (cid:10)A, Y∗(cid:11) , Y is feasible to the SDP

,

(10)

where feasibility is with respect to the program (7) for Model 1 or 2, and to the program (8) for
Model 3 or 4. In words, Y(A) is the set of feasible SDP solutions that attain an objective value
no worse than the ground-truth Y∗. As mentioned, our upper bounds in fact hold for any solution
in Y(A). With a slight abuse of notation, in the sequel we use (cid:98)Y to denote an arbitrary matrix in
Y(A); accordingly, we let (cid:98)σsdp denote the corresponding vector of labels extracted from this (cid:98)Y.
Our main theorem is a non-asymptotic bound on the error rates of the SDP relaxations.

5Otherwise one could recover the large cluster ﬁrst.
6This assumption is not essential but makes the proof therein somewhat simpler.

9

Theorem 3 (Upper bound). For any constants c0, c1 ∈ (0, 1), there exist constants CI∗, Ce, C(cid:48)
e > 0
such that the following holds for Model 1, Model 2, and Model 3 with 0 < c0 p ≤ q < p ≤ 1 − c1.
If nI∗ ≥ CI∗, then with probability at least 1 − 10 exp

(cid:17)
log n

(cid:112)

−

(cid:16)

,

1
n

(cid:107)(cid:98)Y − Y∗(cid:107)1 ≤

err((cid:98)σsdp, σ∗) ≤ exp

n exp



(cid:18)
−



1 − Ce

(cid:114)

(cid:19)
¯nI∗



 ,


1
nI∗


(cid:18)
1 − C(cid:48)
−
e

(cid:114)

(cid:19)

¯nI∗



 ,

1
nI∗

∀(cid:98)Y ∈ Y(A).

The proof is given in Appendices C and E. Note the ﬂoor operation in the ﬁrst inequality
above; consequently, we have (cid:107)(cid:98)Y − Y∗(cid:107)1 = 0 whenever the exponent is strictly less than − log n.
We later explore the implication of this fact for exact recovery.

Remark 2. The assumption c0 p ≤ q for Model 3 is common in the literature on minimax rates
It stipulates that p and q are on the same order (but their diﬀerence can be
[26, 27, 63, 64].
vanishingly small). This is the regime where the clustering problem is hard, and it is the regime
we focus on. The assumption arises from a technical step in our proof, and it is currently not clear
to us whether this assumption is necessary. We would like to point out that in Section 5.1 of [26],
a weaker minimax upper bound is obtained with this assumption dropped.

Letting n → ∞ in Theorem 3, we immediately obtain the following asymptotic result.

Corollary 1 (Upper bound, asymptotic). For any constants c0, c1 ∈ (0, 1), the following holds for
Model 1, Model 2, and Model 3 with 0 < c0 p ≤ q < p ≤ 1 − c1. If nI∗ → ∞, then with probability
1 − o(1),

(cid:107)(cid:98)Y − Y∗(cid:107)1 ≤

(cid:106)
n exp (cid:2)−(cid:0)1 − o(1)(cid:1)¯nI∗(cid:3) (cid:107)
,

1
n
err((cid:98)σsdp, σ∗) ≤ exp (cid:2)−(cid:0)1 − o(1)(cid:1)¯nI∗(cid:3) ,

∀(cid:98)Y ∈ Y(A).

Comparing the upper bound in Corollary 1 with the minimax lower bound in Theorem 2, we
see that the SDP achieves the optimal error rate, up to a second-order o(1) term in the exponent.7
Moreover, Theorem 3 provides an explicit, non-asymptotic upper bound for the o(1) term in the
√
nI∗), yields second-order characterization of vari-
exponent. This bound, taking the form of O(1/
ous recovery thresholds and is strong enough to provide non-trivial guarantees in the sparse graph
nI∗)
regime—these points are discussed in Section 4.4 to follow. We do not expect this O(1/
bound to be information-theoretic optimal, for reasons discussed in Section 1.1.

As a passing note, the above error upper bounds also apply to the MLE solution (cid:98)σmle, since
the optimality of (cid:98)σmle to the program (6) implies that ((cid:98)σmle)((cid:98)σmle)(cid:62) ∈ Y(A). In fact, our proof
of the upper bounds involves showing that the SDP solutions closely approximate the MLE; we
elaborate on this point in Section 5.

√

4.3 Robustness under Semirandom Models

Our next result shows that the error rate of the SDP is unaﬀected by passing to the semirandom
model. Recall the deﬁnition in Equation (10), so Y(ASR) is the sublevel set of the SDP (8) with
ASR as the input.

7We note that Theorem 2 bounds the error in expectation and holds for a parameter space containing σ∗ with slightly
unequal-sized clusters. The results in Theorem 3 and Corollary 1 are high-probability bounds. Extending these upper
bounds to the setting of slightly unequal-sized clusters is possible, albeit tedious; we leave this to future work.

10

Theorem 4 (Semirandom SBM). Suppose that ASR is generated according to Model 4. The
conclusions of Theorem 3 and Corollary 1 continue to hold for the program (8) with Y(A) replaced
by Y(ASR).

Proof. This theorem admits a short proof, thanks to the validity of Theorem 3 for any (cid:98)Y ∈ Y(A).
Recall that ASR is obtained by monotonically modifying the matrix A generated from Model 3
(binary symmetric SBM). Let (cid:98)Y be an arbitrary element of Y(ASR). By deﬁnition of ASR and the
feasibility of (cid:98)Y, we have for all i, j ∈ [n]





i j ≥ Ai j, (cid:98)Yi j − Y ∗
ASR
i j ≤ Ai j, (cid:98)Yi j − Y ∗
ASR

i j ≤ 0,
i j ≥ 0,

if Y ∗
i j
if Y ∗
i j

= 1,
= −1.

The fact (cid:98)Y has objective value no worse than Y∗ under ASR, together with the above inequalities,
implies that 0 ≤ (cid:104)ASR, (cid:98)Y − Y∗(cid:105) ≤ (cid:104)A, (cid:98)Y − Y∗(cid:105). This further implies that (cid:98)Y ∈ Y(A). Therefore,
(cid:3)
invoking Theorem 3 gives the desired result.

Remark 3. As mentioned in Remark 1, one may deﬁne semirandom versions of Models 1 (Z2)
and 2 (CBM). It is easy to see that the proof above applies to these models without change.
Therefore, the SDP approach is also robust under semirandom Z2 and CBM.

As an immediate consequence of Theorem 4, we obtain error bounds for a generalization of

the standard SBM (Model 3) with heterogeneous edge probabilities, where

Ai j

i.i.d.∼



Ber(pi j)

Ber(qi j)

if σ∗
if σ∗

i σ∗
i σ∗

j

j

= 1,
= −1,

with 0 ≤ qi j ≤ q < p ≤ pi j ≤ 1.

Corollary 2 (Heterogeneous SBM). Under the above generalization of Model 3, the conclusions
of Theorem 3 and Corollary 1 continue to hold for the program (8).

Proof. The corollary follows from the same coupling argument as in [23, Appendix V], which
(cid:3)
shows that the Heterogeneous SBM can be reduced to the semirandom Model 4.

The results above show that SDP is insensitive to monotone modiﬁcation and heterogeneous
probabilities. We emphasize that such robustness is by no means automatic. With non-uniformity
in the probabilities, the likelihood function no longer has a known, rigid form, a property heavily
utilized in many algorithms. The monotone adversary can similarly alter the graph structure by
creating hotspots and short cycles. Even worse, the adversary is allowed to make changes after
observing the realized graph,8 thus producing unspeciﬁed dependency among all edges in the
observed data and leading to major obstacles for existing analysis of iterative algorithms.

We would like to mention that the work in [45] shows that the semirandom model makes weak
recovery strictly harder. While not contradicting their results technically, the fact that our error
bounds remain unaﬀected under this model does demand a closer look. We note that our bounds
are optimal only up to a second-order term in the exponent and consequently do not attain the
optimal weak recovery limit. Also, our robustness results on error rates are tied to a speciﬁc form
of SDP analysis (using the sublevel set Y(A)). In comparison, for exact recovery SDP is robust by
design to the semirandom model, as is well recognized in past work [24, 17, 33].

8We therefore strengthen the robustness results in the previous work [23], which does not allow such adaptivity.

11

4.4 Consequences

Theorem 3 and Corollary 1 imply sharp suﬃcient conditions for several types of recovery:

• Exact recovery: Whenever ¯nI∗ ≥ (1+δ) log n for any constant δ > 0, we have (cid:107)(cid:98)Y−Y∗(cid:107)1 = 0
by Corollary 1 (note the ﬂoor operation therein) and hence SDP achieves exact recovery by
itself without any rounding/post-processing steps.

• Second-order reﬁnement: Using the non-asymptotic Theorem 3, we can obtain the fol-
+ C2
log n

lowing reﬁnement of the above result: exact recovery provided that

¯nI∗
log n ≥ 1 + C1√

log n

for some constants C1, C2 > 0.

• Weak recovery: When ¯nI∗ ≥ C for a suﬃciently large constant C, Theorem 3 ensures that

err((cid:98)σsdp, σ∗) < 1

2 and hence SDP achieves weak recovery.

• Sparse regime: Theorem 3 ensures that SDP achieves an arbitrarily small constant error
when nI∗ is a suﬃciently large but ﬁnite constant. This corresponds to the sparse graph
regime with constant expected degrees, namely α, p, q = Θ(1/n) in CBM and SBM. Many
results on minimax rates require nI∗, and hence the degrees, to diverge (e.g., [26, 63]).

Moreover, these conditions remain suﬃcient under the semirandom model. Below we specialize
the above results to each of the three models.

4.4.1 Z2 Synchronization
Recall that I∗ := 1
τ2 ≤

√

n
2 log n+C

log n

2τ2 and ¯n = n under Model 1. Consequently, SDP achieves exact recovery if
. This is a reﬁnement of the best existing threshold τ2 ≤

(2+δ) log n in [5, 10].

n

We also have weak recovery by SDP if τ2 ≤ n

C , which matches, up to constants, the optimal

threshold τ2 < n established in [52, 40].

4.4.2 Censored Block Model
Recall that I∗ := α(cid:0)√
√
(cid:15)(cid:1)2 and ¯n = n under Model 2. Consequently, SDP achieves exact
. This result is a second-order improvement over the threshold
recovery if
log n I∗ ≥ 1 + δ for SDP established in the work [34]. The same work also proves that exact
recovery is impossible if

1 − (cid:15) −
log n I∗ ≥ 1 + C√

log n

n

n

n

log n I∗ < 1 − δ.

Noting that I∗ (cid:16) α(1−2(cid:15))2 (cf. Fact 1(b)), we also have weak recovery by SDP if nα(1−2(cid:15))2 ≥
C, which matches, up to constants, the optimal threshold nα(1 − 2(cid:15))2 > 1 proved in [35, 39, 55].

4.4.3 Stochastic Block Model
Recall that I∗ := −2 log (cid:2) √
lence I∗ = (1 + o(1))(
p −
√
√
recovery if n(

pq + (cid:112)
√

p −

√

(1 − p)(1 − q)(cid:3) and ¯n = n

2 under Model 3, and note the equiva-
q)2 valid for 0 < q (cid:16) p = o(1). Consequently, SDP achieves exact

q)2 ≥ (2 + δ) log n, recovering the result established in [33, 10].

We also have the following reﬁnement: exact recovery provided that nI∗

This result is comparable to the suﬃcient condition
for SDP
established in [33], whereas the necessary and suﬃcient condition for the optimal estimator (MLE)
is

≥ 2 + C√

[3, 50].

+ ω

log n

q)2

n(

√

√

(cid:17)

p−
log n

≥ 2 − log log n
log n

(cid:16) 1
log n

√

n(

√

p−
log n

q)2

log n ≥ 2 + C1√
log n
(cid:17)
+ ω

(cid:16) 1
log n

+ C2

log n .

12

Finally, noting that I∗ (cid:16) (p − q)2/p (cf. Fact 2(b)), we have weak recovery by SDP if n(p −
q)2/p ≥ C. This condition matches, up to constants, the so-called Kesten-Stigum (KS) threshold
n(p − q)2/(p + q) > 2, which is optimal [44, 7, 48, 49].

4.5 Comparison with existing results

In this section we focus on partial recovery under the binary symmetric SBM (Model 3), and
compare with the existing work that derives sharp error rate bounds achievable by polynomial-
time algorithms. To be clear, the algorithms considered in this line of work are very diﬀerent from
ours. In particular, most existing results require a good enough initial estimate of the true clusters.
Obtaining such an initial solution (typically using spectral clustering) is itself a non-trivial task.

(cid:105)

√

√

√

p −

p −

q)2 · n/2

Using neighbor voting and variational inference algorithms, the work in [26, 63] obtains an
error bound of the same form as our Corollary 1, though they do not provide non-asymptotic results
as in our Theorem 3. The work in [60] considers a spectral algorithm and proves the error bound
(cid:104)
for any constant δ > 0 if np → ∞. Recalling I∗ =
err((cid:98)σ, σ∗) ≤ exp
−(1 − δ)(
√
(1 + o(1))(
q)2, we ﬁnd that our Corollary 1 is better as we allow the δ term to vanish. The
recent work in [5] uses a novel perturbation analysis to show that a very simple spectral algorithm
q)2 ≥ δ(cid:48) for any constant
achieves the error bound in Corollary 1 under the assumption n
log n (
δ(cid:48) > 0; their assumption excludes the sparse regime with p, q = o
and is stronger than our
assumption nI∗ → ∞ in Corollary 1. Compared to the above works, another strength of our results
is that we provide an explicit bound for the second-order term in the exponent; we know of few
error rate results (with the exception discussed below) that oﬀer this level of accuracy.

p−
(cid:16) log n
n

√

√

(cid:17)

Concurrently to our work, the paper [64] establishes a tight non-asymptotic error bound for an

EM-type algorithm. Translated to our notation, their bound takes the form

(cid:35)

(cid:32)

(cid:33)

1 + 2

err((cid:98)σ, σ∗) ≤ exp

(cid:34)
−
which is valid under the assumption nI∗ (cid:38) √
np (cid:38) 1. Their assumption is order-wise more
restrictive than that in our Theorem 3, but their error bound has a better second-order term in the
exponent. We do note that their algorithm is fairly technical: it requires data partition and the
leave-one-out tricks to ensure independence, degree truncation to regularize spectral clustering,
and blackbox solvers for K-means and matching problems. In comparison, the SDP approach is
much simpler conceptually.

nI∗ log(np)

nI∗
2

,

Finally, we emphasize that we also provide robustness guarantees under the monotone semi-
random model and non-uniform edge probabilities. In comparison, it is unclear if comparable
robustness results can be established for the algorithms above, as these algorithms and their anal-
yses make substantial use of the properties of the standard SBM, particularly the complete inde-
pendence among edges and the speciﬁc form of the likelihood function.

5 Proof Outline

In this section we outline our proofs of the lower and upper bounds. In the process we provide
insights on how the error rate e−¯nI∗

arises and why SDP achieves it.

5.1 Proof outline of Theorem 2

The intuition behind the lower bound is relatively easy to describe. To illustrate the idea, take as
= 1, ∀i. It is not hard to
an example the Z2 model, where Ai j

i.i.d.∼ N(1, τ2), and assume that σ∗
i

13

see that the error fraction err((cid:98)σ, σ∗) for any estimator (cid:98)σ is lower bounded by the probability of
recovering the label σ∗
1 for the ﬁrst node given true labels of the other nodes; see the Lemma 1 for
the precise argument. For this one-node problem, the optimal Bayes estimate of σ∗
1 is given by the
sign of the majority vote (cid:80)n

j=1 A1 j:

(cid:98)σ1 = arg max
σ∈{±1}

(cid:26)
σ ·

(cid:88)n

j=1

(cid:27)

A1 j

= sign

(cid:18)(cid:88)n

j=1

(cid:19)

;

A1 j

(11)

see Lemma 2. It follows that the error probability of recovering σ∗
1

= 1 is

P(cid:110)

(cid:98)σ1 (cid:44) 1

(cid:111) = P

(cid:26)(cid:88)n

j=1

(cid:27)
A1 j < 0

= exp

(cid:104)

(cid:105)
− (1 + o(1)) · nI(0)

,

where the last step can be justiﬁed in general by the large deviation theory, with I(x) (cid:66) supt>0[tx −
log Eet(−A11)] being the rate function (see, e.g., Cramer’s Theorem [21, Theorem 2.2.3]). In our
setting, a direct calculation suﬃces, as is done in Lemma 3. The error exponent

I(0) = − inf
t>0

(cid:110)

log Eet(−A11)(cid:111) = I∗

(12)

is precisely our SNR measure, a quantity we will encounter again in proving the upper bound.

The above intuition remains valid for CBM and SBM, though the speciﬁc forms of the majority

voting procedure and the rate I∗ vary. The complete proof is given in Section B.

5.2 Proof outline of Theorem 3

To prove the upper bound for SDP, we proceed in three steps:

Step 1: As mentioned in Section 1, we construct a diagonal matrix D with Dii = σ∗
i

j Ai jσ∗
j,
which takes the same form as the “dual certiﬁcate” used in previous work. The construction of D
allows us to establish the basic inequality:
(cid:68)
−D, PT ⊥((cid:98)Y)

A − EA, PT ⊥((cid:98)Y)

for any (cid:98)Y ∈ Y(A);

(cid:69) + (cid:68)

0 ≤

(cid:69)

,

(cid:80)

see the proof of Lemma 6 for the details of this critical step. Here PT ⊥ is an appropriate projection
operator that satisﬁes Tr (cid:2)PT ⊥((cid:98)Y)(cid:3) = 1
n (cid:107)(cid:98)Y − Y∗(cid:107)1, thus exposing the (cid:96)1 error of (cid:98)Y that we seek to
control.

Step 2: We proceed by showing that the second term S 2 := (cid:104)A − EA, PT ⊥((cid:98)Y)(cid:105) in the basic
inequality is negligible compared to the ﬁrst term (cid:104)−D, PT ⊥((cid:98)Y)(cid:105); see Proposition 1 for a quanti-
tative version of this claim, whose proof involves certain trimming argument in the case of CBM
and SBM. Dropping S 2 from the basic inequality hence yields

0 ≤ (cid:104)−D, PT ⊥((cid:98)Y)(cid:105) =

(cid:88)n

i=1

(−Dii)bi,

(13)

where bi := (cid:0)PT ⊥((cid:98)Y)(cid:1)

ii satisﬁes (cid:80)

i∈[n] bi = m := 1

n (cid:107)(cid:98)Y − Y∗(cid:107)1.

Step 3: The bi’s take fractional values in general, but must be bounded in [0, 4] (cf. 4). We

use this fact to upper bound the RHS of (13) by its worst-case value, hence obtaining

0 ≤

(cid:88)n

i=1

(−Dii)bi (cid:46) max
M⊆[n]
|M|=m

(cid:88)

i∈M

(−Dii).

(14)

This argument is reminiscent of the “order statistics” analysis in [23], though here we provide
a more ﬁne-grained bound; see Section C.4 for details. The rest of this step, done in Lemma 9,

14

establishes a probabilistic bound for the RHS of (14) and ultimately gives rise to the error exponent
−I∗. To illustrate the idea, we again consider Z2 with σ∗
j∈[n] Ai j. For
a ﬁxed set M with |M| = m, the RHS of (14) can be controlled using the Chernoﬀ bound:
(cid:27)
> 0

i ≡ 1, in which case Dii = (cid:80)

E exp

(cid:18)(cid:88)n

(cid:18)(cid:88)n

(cid:26)(cid:88)

(cid:88)

(cid:19)(cid:21)

P

(cid:19)

(cid:20)

t

(−Ai j)

(−Ai j)

i∈M

j=1

≤ inf
t>0
(cid:18)

=

inf
t>0

i∈M

(cid:19)nm

j=1
= e−nmI∗

,

E exp(−tA11)

(cid:16) n
m




where the last two steps follow from independence and the expression (12) for I∗. By a union
bound over all

such M’s, we obtain

(cid:17)m

≈

(cid:17)

(cid:16) n
m

(cid:19)

P

i∈M

(cid:88)

(cid:18)(cid:88)n

(−Ai j)

max
M⊆[n]
|M|=m



> 0

If log(n/m) − nI∗ < 0, then RHS above is (cid:28) 1 and thus with high probability the negation of (14)
holds, a contradiction. We therefore must have log(n/m) − nI∗ ≥ 0, which implies the desired error
n ≤ e−nI∗
bound m
. The second-order term in the exponent comes from a more accurate calculation
for Steps 2 and 3.

elog(n/m)−nI∗(cid:17)m

· e−nmI∗ = (cid:16)

(cid:18) n
m

j=1

(cid:19)m

≤

.

The previous arguments are closely connected to our proof for the lower bound outlined above.

Note that the MLE of the entire vector σ∗ = 1 is given by the “joint majority voting” procedure
(cid:26)(cid:88)n

(cid:18)(cid:88)n

(cid:19)(cid:27)

(cid:98)σmle = arg max
σ∈{±1}n

σi ·

i=1

Ai j

;

j=1

one should compare this equation with the “single-node majority voting” in (11). The maximality
of the above (cid:98)σmle over σ∗ = 1, as well as the fact that 1 − (cid:98)σmle
(cid:16)
1 − (cid:98)σmle
i

∈ {0, 2}, implies that

(−Dii)

(−Ai j)

(cid:18)(cid:88)n

(cid:88)n

0 ≤

(cid:88)

(cid:19)

(cid:17)

·

i

i=1

j=1

(cid:46) max
M⊆[n]
|M|=m

i∈M

(cid:80)n

i

i=1(1 − (cid:98)σmle

) = n err((cid:98)σmle, σ∗). Note that this inequality is the same as (14),
if we set m = 1
2
n ≤ e−nI∗
so following the arguments above shows that the MLE satisﬁes the same error bound m
.
We therefore see that the SDP solution closely approximates the MLE in the above precise sense,
and both of them achieve the Bayes rate. The form of the rate e−nI∗
arises from a majority voting
mechanism, in the proofs for both lower and upper bounds.

Again, the above intuition remains valid for CBM and SBM, though the calculation of the

rate I∗ varies. The details of the proof are given in Appendices C and E.

6 Discussion

In this paper, we analyze the error rates of the SDP relaxation approach for clustering under several
random graph models, namely Z2, CBM and the binary symmetric SBM, via a uniﬁed framework.
We show that SDP achieves an exponentially-decaying error with a sharp exponent, matching the
minimax lower bound for all three models. We also show that these results continue to hold under
monotone semirandom models, demonstrating the robustness of SDP.

Immediate future directions include extensions to problems with multiple and unbalanced
clusters, as well as to closely related models such as weighted SBM. It is also of interest to see if

15

better estimates of the second order term can be obtained, and if there is a fundamental tradeoﬀ
between statistical optimality and robustness. More broadly, it would be interesting to explore
the applications of the techniques in this paper in analyzing SDP relaxations for other discrete
problems.

Acknowledgement

Y. Fei and Y. Chen were partially supported by the National Science Foundation CRII award
1657420 and grant 1704828.

Appendices

A Preliminaries

In this section we record several notations and facts that are useful for subsequent proofs.

We ﬁrst deﬁne a random variable H that encapsulates the distributions of the three models:

• For Model 1 (Z2), let H ∼ N(1, τ2).

• For Model 2 (CBM), let H have probability mass function α(1 − (cid:15)) · δ1 + α(cid:15) · δ−1 + (1 − α) · δ0,

where δa denotes the Dirac delta function centered at a.

• For Model 3 (SBM), let H = Y −Z, where Y ∼ Ber(p), Z ∼ Ber(q), and Y, Z are independent.

It can be seen that under Model 1 or 2, we have Ai j ∼ σ∗
= σ∗
tion); under Model 3, we have Aii(cid:48) − Ai j ∼ H if σ∗
i
Let t∗ be the minimizer of the moment generating function t (cid:55)→ Ee−tH, which has the explicit

j H (here ∼ means equality in distribu-

i σ∗
i(cid:48) = −σ∗
j.

expression



1
τ2 ,
2 log 1−(cid:15)
1
(cid:15) ,
2 log p(1−q)
q(1−p) ,
Note that t∗ > 0. We later verify that Ee−tH ≈ e−I∗
deﬁne the quantity

t∗ (cid:66)




1

for Model 1,
for Model 2,
for Model 3.

(15)

for all three models (see Facts 5, 6 and 7). Also

λ∗ (cid:66)





0
2t∗ log 1−q
1
1−p ,

for Model 1 and 2,
for Model 3,

(16)

which plays a role only in Model 3 (SBM).

Finally, for Model 2, we let p (cid:66) α(1 − (cid:15)) and q (cid:66) α(cid:15); this notation is chosen to bring out the

similarity between Models 2 and 3.

We record several simple estimates for the above quantities t∗ and λ∗ as well as the SNR

measure I∗ deﬁned in (5). The proofs are given in Sections A.1 and A.2 to follow.

Fact 1. Under Model 2 with the notation p (cid:66) α(1 − (cid:15)) and q (cid:66) α(cid:15), if 0 < q ≤ p ≤ 1, then

(a) t∗ ≤ 1−(cid:15)
2(cid:15)

· p−q
p .

16

(b) I∗ ∈

(cid:20)

(p−q)2
4p

, (p−q)2
p

(cid:21)

.

Fact 2. Under Model 3, if 0 < q < p < 1, then the following hold.

(a) λ∗ ∈ (q, p).

(b) If in addition p ≤ 1 − c for some constant c ∈ (0, 1), then I∗ (cid:16) (p−q)2

p

.

(c) If in addition p ≤ 1 − c and q ≥ c0 p for some constants c ∈ (0, 1), c0 ∈ (0, 1), then t∗ (cid:46) p−q
p .

A.1 Proof of Fact 1

Recall the shorthands p (cid:66) α(1 − (cid:15)) and q (cid:66) α(cid:15) introduced for Model 2. For part (a) of the fact,
by deﬁnition of t∗ in Equation (15), we have

t∗ = 1
2

(cid:32)

log

1 + p − q
q

(cid:33) (i)
≤

p − q
2q

(ii)= 1 − (cid:15)
2(cid:15)

·

p − q
p

,

where step (i) holds since the fact that 1 + x ≤ ex for x ∈ R implies log(1 + x) ≤ x for x > −1, and
step (ii) holds by the fact that q = (cid:15)

1−(cid:15) p.
For part (b), recalling the deﬁnition of I∗ in Equation (5), we have

(cid:16) √

I∗ =

p −

(cid:17)2 (cid:16) √
√
q
(cid:16) √
p + √
q

p + √
(cid:17)2

(cid:17)2
q

=

(p − q)2
√

p + q + 2

.

pq

Some algebra shows that I∗ ≤ (p−q)2

p

and I∗ ≥ (p−q)2
p+p+2p

= (p−q)2
4p

.

A.2 Proof of Fact 2

For part (a), recalling the deﬁnition of λ∗ in Equation (16), we obtain by direct calculation the
identity

p − λ∗ =

(cid:35)−1 (cid:34)

(cid:34)
log

p(1 − q)
q(1 − p)

p log

p
q

+ (1 − p) log

1 − p
1 − q

(cid:35)

.

The quantity inside the second bracket on the RHS is positive, as it is the KL divergence between
Ber(p) and Ber(q) with p (cid:44) q. We also have log p(1−q)
q(1−p) > 0 since 0 < q < p < 1. It follows that
p − λ∗ > 0 as claimed. A similar argument shows that λ∗ − q > 0.

Part (b) is a partial result of [62, Lemma B.1].
log p
For part (c), recall the deﬁnition t∗ := 1
q
2
q ≥ 1−q

1−p , we have

(cid:16)

If p

(cid:17)

+ log 1−q
1−p

in Eq. (15). We consider two cases.

p
q
where step (i) holds since log(x) ≤ x − 1, ∀x > 0, and step (ii) holds by assumption c0 p ≤ q. If
q ≤ 1−q
p

1−p , we have

t∗ ≤ log

p − q
c0 p

− 1

(ii)
≤

p
q

(i)
≤

,

t∗ ≤ log

1 − q
1 − p

≤

1 − q
1 − p

(i)
≤

− 1

p − q
c

≤

p − q
cp

,

where step (i) holds by the assumption that p ≤ 1 − c. In both cases, we have t∗ (cid:46) p−q

p as claimed.

17

B Proof of Theorem 2

In this section we prove Theorem 2 under Models 1 and 2, following a similar strategy as in the
proof of [62, Theorem 1.1]. We make use of the deﬁnitions and facts given in Section A.

For simplicity, in the sequel we write Θ ≡ Θ(n) := {±1}n. Let φ be the uniform prior over all

the elements in Θ. Deﬁne the global Bayesian risk

Bφ

(cid:0)Θ, (cid:98)σ(cid:1) (cid:66) 1
|Θ|

(cid:88)

σ∈Θ

Eσ err((cid:98)σ, σ),

and the local Bayesian risk for the ﬁrst node

Bφ

(cid:0)Θ, (cid:98)σ(1)(cid:1) (cid:66) 1
|Θ|

(cid:88)

σ∈Θ

Eσ err (cid:0)

(cid:98)σ(1), σ(1)(cid:1).

In the above, the quantity err((cid:98)σ(1), σ(1)) denotes the loss on the ﬁrst node, deﬁned as

err (cid:0)

(cid:98)σ(1), σ(1)(cid:1) (cid:66) 1

(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)Sσ((cid:98)σ)

(cid:88)

I (cid:8)σ(cid:48)(1) (cid:44) σ(1)(cid:9) ,

σ(cid:48)∈Sσ((cid:98)σ)

where Sσ((cid:98)σ) (cid:66) (cid:110)
that these risks are equal.

g(cid:98)σ : g ∈ {±1}, 1
n

(cid:80)

i∈[n]

I{g(cid:98)σi (cid:44) σi} = err((cid:98)σ, σ)

(cid:111)
. The following lemma shows

Lemma 1. Under Models 1 and 2, we have

Bφ

Bφ

inf
(cid:98)σ

(cid:0)Θ, (cid:98)σ(1)(cid:1).

(cid:0)Θ, (cid:98)σ(cid:1) = inf
(cid:98)σ
Proof. The lemma essentially follows from the symmetry/exchangeability property of Models 1
(Z2) and 2 (CBM). Rigorous proof of this intuitive result is however quite technical, as the deﬁ-
nition of clustering error involves a global sign ﬂipping. Fortunately, most of the work has been
done in [62]. In particular, note that the parameter space Θ is closed under permutation in the sense
that for any label vector σ ∈ Θ and any permutation π on [n], the new label vector σ(cid:48) deﬁned by
σ(cid:48)(i) (cid:66) σ(π−1(i)) also belongs to Θ. It can also be seen that both Models 1 (Z2) and 2 (CBM) are
homogeneous, i.e., the distribution of each Ai j is uniquely determined by the sign of σ∗
j. Con-
sequently, for Model 2 (CBM) this lemma immediately follows from Lemma 2.1 in [62], as its
proof applies without change. For Model 1 (Z2) in which the distribution of A is continuous, we
note that the proof of Lemma 2.1 in [62] continues to hold when summations therein are replaced
(cid:3)
by appropriate integrations.

i σ∗

With the above lemma, it suﬃces to lower bound the local Bayes risk. This task can be further
reduced to computing the tail probability of a certain sum of independent copies of the random
variable H deﬁned in Section A. This is done in the following lemma.

Lemma 2. Let φ be the uniform prior over all elements in Θ. Under Models 1 and 2, we have

Bφ

(cid:0)Θ, (cid:98)σ(1)(cid:1) ≥ P





(cid:88)

i∈[n−1]


Zi ≥ 0



,

where {Zi} are i.i.d. copies of −H.

18

This lemma is analogous to Lemma 5.1 in [62]. We provide the proof in Section B.1.
Finally, the lemma below provides an explicit lower bound of the above tail probability in

terms of the SNR measure I∗.

Lemma 3. Let {Zi} be i.i.d. copies of −H. For Model 1 and Model 2 with I∗ = o(1), if nI∗ → ∞,
then there exists ξ = o(1) such that





P

1
n − 1

(cid:88)

i∈[n−1]





Zi ≥ 0

≥ exp (cid:2)− (1 + ξ) (n − 1)I∗(cid:3) .

This lemma is analogous to Lemma 5.2 in [62]. We provide the proof in Section B.2 for

Model 1 (Z2) and in Section B.3 for Model 2 (CBM).

We are now ready to prove Theorem 2. Note that

inf
(cid:98)σ

sup
σ∈Θ

Eσ err((cid:98)σ, σ) ≥ inf
(cid:98)σ

Bφ(Θ, (cid:98)σ),

since the Bayes risk lower bounds the minimax risk. To complete the proof, we continue the above
inequality by successively invoking Lemmas 1, 2 and 3.

B.1 Proof of Lemma 2
Recall that Bφ(Θ, (cid:98)σ(1)) is deﬁned as

Bφ

(cid:0)Θ, (cid:98)σ(1)(cid:1) (cid:66) 1
|Θ|

(cid:88)

σ∈Θ

Eσ err (cid:0)σ(1), (cid:98)σ(1)(cid:1).

For each σ0 ∈ Θ, we generate a new assignment σ[σ0] based on σ0 by setting σ[σ0](1) (cid:66) −σ0(1)
and σ[σ0](i) (cid:66) σ0(i) for all i ∈ [n] \ {1}. It can be seen that σ[σ0] ∈ Θ and the Hamming
distance between σ0 and σ[σ0] is 1. In addition, for any σ1, σ2 ∈ Θ with σ1 (cid:44) σ2, we have
σ[σ1] (cid:44) σ[σ2]. This bijection implies that {σ[σ0] : σ0 ∈ Θ} = Θ. Consequently, continuing
from the last displayed equation we obtain

Bφ(Θ, (cid:98)σ(1)) = 1
|Θ|

(cid:88)

σ0∈Θ

1
2

(cid:16)Eσ0 err (cid:0)

(cid:98)σ(1), σ0(1)(cid:1) + Eσ[σ0] err (cid:0)

(cid:98)σ(1), σ[σ0](1)(cid:1)(cid:17)

,

whence

Bφ(Θ, (cid:98)σ(1)) ≥

inf
(cid:98)σ

1
|Θ|

(cid:88)

σ0∈Θ

1
2

inf
(cid:98)σ

(cid:16)Eσ0 err (cid:0)

(cid:98)σ(1), σ0(1)(cid:1) + Eσ[σ0] err (cid:0)

(cid:98)σ(1), σ[σ0](1)(cid:1)(cid:17)
.

(17)

We proceed to compute the inﬁmum above for a given σ0 ∈ Θ. Let ˜σ be the Bayes estimator
that attains the inﬁmum. Since σ0 and σ[σ0] only diﬀer at the ﬁrst node, we must have ˜σ(i) =
σ[σ0](i) = σ0(i) for all i ∈ [n] \ {1}, and either ˜σ(1) = σ0(1) or ˜σ(1) = σ[σ0](1). Now the problem
is reduced to a test between two distributions Pσ0 and Pσ[σ0]. Since the prior φ is uniform, ˜σ(1)
≷ 1. The following lemma gives the explicit form of
is given by the likelihood ratio test
this test. Here we let J0 (cid:66) {u ∈ [n] \ {1} : σ0(u) = σ0(1)} and J1 (cid:66) {u ∈ [n] : σ0(u) = σ[σ0](1)}.9

Pσ0 (A)
P
σ[σ0](A)

9In Lemma 4 and its proof, we adopt the convention that (cid:80)

u∈J f (u) = 0 and (cid:81)

u∈J f (u) = 1 if J = ∅.

19

Lemma 4. Let σ0 and σ[σ0] be deﬁned as above. Under Models 1 and 2, we have that

˜σ(1) =





if (cid:80)

u∈J0 A1u ≥ (cid:80)

σ0(1),
σ[σ0](1), otherwise.

u∈J1 A1u,

The lemma follows from a routine calculation of the likelihood. We give proof in Section B.1.1

for Model 1 (Z2) and in Section B.1.2 for Model 2 (CBM).

From Lemma 4 we have

Eσ0 err (cid:0) ˜σ(1), σ0(1)(cid:1) = Pσ0

(cid:18) (cid:88)

A1u <

(cid:19)

,

A1u

and

(cid:88)

u∈J1

Eσ[σ0] err (cid:0) ˜σ(1), σ[σ0](1)(cid:1) = Pσ[σ0]

u∈J0
(cid:18) (cid:88)

u∈J0

A1u ≥

(cid:88)

u∈J1

(cid:19)

.

A1u

Recalling the distribution of {A1u}, the deﬁnition of H and that Zi
probabilities above equal P(cid:0) (cid:80)
desired inequality inf(cid:98)σ Bφ(Θ, (cid:98)σ(1)) ≥ P(cid:0) (cid:80)

i.i.d.∼ −H, we see that both
i∈[n−1] Zi ≥ 0(cid:1). Combining with the bound (17), we obtain the

i∈[n−1] Zi ≥ 0(cid:1).

B.1.1 Proof of Lemma 4 for Model 1 (Z2)

Pσ0 (A)
Since σ0 and σ[σ0] only diﬀer at the ﬁrst node, the likelihood ratio
σ[σ0](A) only depends on the
ﬁrst row and column of A. In particular, recalling that {A1u} are Gaussian under Model 1, we have

P

Pσ0(A)
Pσ[σ0](A)

=

=

(cid:81)

(cid:81)

u∈J0 exp

(cid:104)

u∈J0 exp
(cid:16)(cid:80)
−
(cid:16)(cid:80)

(cid:104)

−

exp

exp

u∈J1 exp

(cid:104)
− (A1u − 1)2 /(2τ2)
(cid:104)
− (A1u + 1)2 /(2τ2)
u∈J0 (A1u − 1)2 + (cid:80)
u∈J0 (A1u + 1)2 + (cid:80)

(cid:105)

× (cid:81)
× (cid:81)

(cid:105)
u∈J1 exp
u∈J1 (A1u + 1)2(cid:17)
u∈J1 (A1u − 1)2(cid:17)

(cid:104)

(cid:104)

(cid:105)
− (A1u + 1)2 /(2τ2)
− (A1u − 1)2 /(2τ2)
(cid:105)
/(2τ2)

(cid:105)

(cid:105) .

/(2τ2)

Some algebra shows that

Pσ0(A)
Pσ[σ0](A)

≷ 1 ⇐⇒

(cid:88)

u∈J0

A1u ≷

(cid:88)

u∈J1

A1u.

The result follows from the fact that the likelihood ratio test is Bayes-optimal for binary hypotheses
under a uniform prior.

B.1.2 Proof of Lemma 4 for Model 2 (CBM)

Similarly to the previous section, we recall the distribution of {A1u} under Model 2 to obtain
u∈J1[α(1 − (cid:15))]I{A1u=−1}[α(cid:15)]I{A1u=1}
u∈J1[α(1 − (cid:15))]I{A1u=1}[α(cid:15)]I{A1u=−1} .

u∈J0[α(1 − (cid:15))]I{A1u=1}[α(cid:15)]I{A1u=−1} × (cid:81)
u∈J0[α(1 − (cid:15))]I{A1u=−1}[α(cid:15)]I{A1u=1} × (cid:81)

Pσ0(A)
Pσ[σ0](A)

(cid:81)

(cid:81)

=

Since (cid:15) ∈ (0, 1

2 ), some algebra shows that

Pσ0(A)
Pσ[σ0](A)

≷ 1 ⇐⇒

(cid:88)

u∈J0

A1u ≷

(cid:88)

u∈J1

A1u.

The result follows from the fact that the likelihood ratio test is Bayes-optimal for binary hypotheses
under a uniform prior.

20

B.2 Proof of Lemma 3 for Model 1 (Z2)

Let n(cid:48) (cid:66) n − 1, p(z) be the pdf of Z1, and M(t) be the moment generating function of Z1. Since
Z1 ∼ −H ∼ N(−1, τ2), we can compute M(t) = exp(−t + 1
τ2 as deﬁned in
Equation (15) and the deﬁnition of I∗ in Equation (5), we obtain

2 t2τ2). Recalling t∗ = 1

M (cid:0)t∗(cid:1) = exp

(cid:33)

(cid:32)

−

1
2τ2

= exp (cid:0)−I∗(cid:1) .

Let δ (cid:66) (2nI∗)− 1

4 , S n(cid:48) (cid:66) (cid:80)

i∈[n(cid:48)] Zi and S n(cid:48)(z) (cid:66) (cid:80)
(cid:89)

(cid:90)

i∈[n(cid:48)] zi. We have

P (S n(cid:48) ≥ 0) ≥

p(zi)dz

i∈[n(cid:48)]

{z:S n(cid:48) (z)∈[0,n(cid:48)δ]}
(M (t∗))n(cid:48)
exp (n(cid:48)t∗δ)

(cid:90)

≥

{z:S n(cid:48) (z)∈[0,n(cid:48)δ]}

(cid:89)

i∈[n(cid:48)]

exp (t∗zi) p(zi)
M (t∗)

dz,

where the last step holds since exp (n(cid:48)t∗δ) ≥ exp
(cid:80)
i∈[n(cid:48)] zi = S n(cid:48)(z) ≤ n(cid:48)δ. Let q(w) (cid:66) exp(t∗w)p(w)

M(t∗)

(cid:16)
t∗ (cid:80)
and we have

i∈[n(cid:48)] zi

(cid:17) = (cid:81)

i∈[n(cid:48)] exp (t∗zi) given that

P (S n(cid:48) ≥ 0) ≥ exp (cid:0)−n(cid:48)I∗(cid:1) exp (cid:0)−n(cid:48)t∗δ(cid:1)

(cid:90)

(cid:89)

{z:S n(cid:48) (z)∈[0,n(cid:48)δ]}

i∈[n(cid:48)]

q(zi)dz.

Note that q(w) is a pdf since
i.i.d. random variables with pdf q(w). We have

(cid:82)

w q(w)dw = 1 and q(w) ≥ 0 for any w. Let W1, W2, . . . , Wn(cid:48) be

P (S n(cid:48) ≥ 0) ≥ exp (cid:0)−n(cid:48)t∗δ(cid:1) P

(cid:18) 1
n(cid:48)

(cid:88)

(cid:19)
Wi ∈ [0, δ]

i∈[n(cid:48)]

exp (cid:0)−n(cid:48)I∗(cid:1) (cid:67) Q1Q2 exp (cid:0)−n(cid:48)I∗(cid:1) .

(18)

B.2.1 Controlling Q1
It can be seen that t∗δ = 2I∗ · (2nI∗)− 1
C(cid:48) > 0 we have

4 by the deﬁnitions of t∗ and δ. Therefore, for some constant

Q1 ≥ exp

(cid:33) 1
4

(cid:32)



−C(cid:48)

1
nI∗





.

n(cid:48)I∗

B.2.2 Controlling Q2

Recall that p(w) is the pdf for N(−1, τ2). A closer look at q(w) yields

q(w) = exp

(cid:19)

(cid:18) w
τ2

exp

(cid:33)

(cid:32)

1
2τ2

(cid:32)
−

exp

1

√

2πτ2

(cid:33)

=

1

√

exp

(cid:32)
−

(cid:33)

.

w2
2τ2

2πτ2
(cid:17)

(w + 1)2
2τ2
(cid:16) 1
n(cid:48)

(cid:80)

i∈[n(cid:48)] Wi

and we have V = 1

n(cid:48) Var (W1) =

Therefore, q(w) is the pdf for N(0, τ2). Deﬁne V (cid:66) Var
τ2
n(cid:48) = 1

2n(cid:48)I∗ .
Recall that δ (cid:66) (2nI∗)− 1

4 and n(cid:48) (cid:66) n − 1. Using Chebyshev’s inequality we have

P



(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)


1
n(cid:48)

(cid:88)

i∈[n(cid:48)]

Wi

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)





> δ

≤

V
δ2

≤

√

C
nI∗

,

21

for some constant C > 0, where the second step holds by n (cid:16) n(cid:48). Therefore, there exist some
constants CI∗ > 0 and c(cid:48) ∈ (0, 1) depending only on CI∗ such that

Q2 = P





1
n(cid:48)

(cid:88)

i∈[n(cid:48)]


Wi ∈ [0, δ]







= 1
2

1 − P



(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)


1
n(cid:48)

(cid:88)

i∈[n(cid:48)]

Wi

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

when nI∗ ≥ CI∗ (implied by our assumption nI∗ → ∞).

B.2.3 Putting together

Returning to Equation (18), we have





> δ





≥ c(cid:48)

P (S n(cid:48) ≥ 0) ≥ c(cid:48) · exp

n(cid:48)I∗

· exp (cid:2)−n(cid:48)I∗(cid:3)

(cid:33) 1
4

(cid:32)

1
nI∗



−C(cid:48)




1 + C(cid:48)




+ 1

(cid:33) 1
4

(cid:32)

1
nI∗





1
c(cid:48)





.

n(cid:48)I∗

n(cid:48)I∗ log

= exp


−



The desired inequality follows by taking ξ (cid:66) C(cid:48) (cid:16) 1
our assumption nI∗ → ∞.

nI∗

B.3 Proof of Lemma 3 for Model 2 (CBM)

(cid:17) 1
4 + 1

n(cid:48)I∗ log 1

c(cid:48) and noting that ξ = o(1) under

Let n(cid:48) (cid:66) n − 1, p(z) be the probability mass function of Z1, and M(t) be the moment generating
function of Z1. Recall that p (cid:66) α(1 − (cid:15)) and q (cid:66) α(cid:15). Since Z1 ∼ −H, we can compute M(t) =
(1 − α) + pe−t + qet. Noting that α = p + q and recalling t∗ = 1
q as deﬁned in Equation (15),
we obtain

2 log p

Let δ (cid:66) V

1

4 I∗ 1

2 (t∗)− 1

2 , S n(cid:48) (cid:66) (cid:80)

√

M (cid:0)t∗(cid:1) = (1 − α) + 2
i∈[n(cid:48)] Zi and S n(cid:48)(z) (cid:66) (cid:80)

pq = 1 − I∗.

i∈[n(cid:48)] zi. We have

P (S n(cid:48) ≥ 0) ≥

(cid:88)

{z:S n(cid:48) (z)∈[0,n(cid:48)δ]}
(M (t∗))n(cid:48)
exp (n(cid:48)t∗δ)

≥

(cid:89)

i∈[n(cid:48)]

p(zi)

(cid:88)

{z:S n(cid:48) (z)∈[0,n(cid:48)δ]}

(cid:89)

i∈[n(cid:48)]

exp (t∗zi) p(zi)
M (t∗)

,

where the last step holds since exp (n(cid:48)t∗δ) ≥ exp
(cid:80)
i∈[n(cid:48)] zi = S n(cid:48)(z) ≤ n(cid:48)δ. Let q(w) (cid:66) exp(t∗w)p(w)

M(t∗)

for w ∈ {−1, 0, 1} and we have

(cid:16)
t∗ (cid:80)

i∈[n(cid:48)] zi

(cid:17) = (cid:81)

i∈[n(cid:48)] exp (t∗zi) given that

P (S n(cid:48) ≥ 0) ≥ exp (cid:0)n(cid:48) log(1 − I∗)(cid:1) exp (cid:0)−n(cid:48)t∗δ(cid:1) (cid:88)

(cid:89)

q(zi).

{z:S n(cid:48) (z)∈[0,n(cid:48)δ]}

i∈[n(cid:48)]

Noting that q(w) is a pmf, we let W1, W2, . . . , Wn(cid:48) be i.i.d. random variables with pmf q(w). We
have

P (S n(cid:48) ≥ 0) ≥ exp (cid:0)−n(cid:48)t∗δ(cid:1) P





1
n(cid:48)

(cid:88)

i∈[n(cid:48)]

Wi ∈ [0, δ]





exp (cid:0)n(cid:48) log(1 − I∗)(cid:1) (cid:67) Q1Q2Q3.

(19)

22

B.3.1 Controlling Q2

A closer look at q(w) yields

q(w) =





√

pq,

1
M(t∗)
1
M(t∗) (1 − α),

√

whence Var(W1) = 2
2
n(cid:48) M(t∗) . We need the following estimates.

M(t∗)

pq

pq. Deﬁne V (cid:66) Var

√

if w = 1 or w = −1,
if w = 0,

(cid:16) 1
n(cid:48)

(cid:80)

i∈[n(cid:48)] Wi

(cid:17)

and we have V = 1

n(cid:48) Var (W1) =

Lemma 5. If (cid:15) ∈ (0, 1
there exist constants C, C1 > 0 such that

2 ) is a constant and 0 < q < p ≤ 1 − c for some constant c ∈ (0, 1), then

V ≤

4p
cn

,

V (t∗)2
I∗2

≤

C
nI∗ ,

√

V ≤ C1

t∗

(cid:114)

I∗
n

.

Proof. By Fact 1(b), we have I∗ ≤ (p−q)2

p ≤ p ≤ 1 − c and therefore M (t∗) ≥ c. This implies

√

pq
4
nM (t∗)

≤

4p
cn

,

V ≤

where the ﬁrst step holds by n(cid:48) = n − 1 ≥ 1
C, C(cid:48), C(cid:48)(cid:48) > 0 such that

2 n for n ≥ 2. Furthermore, there exist some constants

V (t∗)2
I∗2

≤ C(cid:48)(cid:48)

(cid:17)2

(cid:16) p−q
p

(cid:19)2

4p
cn
(cid:18)

(p−q)2
p

= C(cid:48)

p
n(p − q)2

≤

C
nI∗ ,

where the ﬁrst step holds by Facts 1(a) and 1(b), and the last step holds by Fact 1(b). Finally, we
have

√

t∗

V ≤ C0

p − q
p

(cid:114)

p
n

≤ C1

(cid:114)

I∗
n

for some constants C0, C1 > 0, where the ﬁrst step holds by Fact 1(a) and the last step holds by
(cid:3)
Fact 1(b).

We return to controlling Q2. Recalling that δ (cid:66) V

1

4 I∗ 1

2 (t∗)− 1

2 and using Chebyshev’s inequal-

ity, we have

P



(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)


1
n(cid:48)

(cid:88)

i∈[n(cid:48)]

Wi

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)





> δ

≤

V
δ2

≤

√

C
nI∗

,

for some constant C > 0, where the second step holds since V
δ2
Therefore, there exist some constants CI∗ > 0 and c(cid:48) ∈ (0, 1) that only depends on CI∗ such that

nI∗ by Lemma 5.

I∗ (cid:46) 1√

V

√

= t∗

Q2 = P





1
n(cid:48)

(cid:88)

i∈[n(cid:48)]


Wi ∈ [0, δ]







= 1
2

1 − P



(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)


1
n(cid:48)

(cid:88)

i∈[n(cid:48)]

Wi

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

when nI∗ ≥ CI∗ (implied by our assumption nI∗ → ∞).

23





> δ





≥ c(cid:48)

B.3.2 Controlling Q1

The last two inequalities of Lemma 5 implies that t∗δ =
Therefore, for some constant C(cid:48) > 0 we have

(cid:113)

√

V (cid:46)

I∗t∗

(cid:114)

(cid:113)

I∗

(cid:46) I∗ (cid:16) 1

nI∗

(cid:17) 1
4 .

I∗
n

Q1 ≥ exp

(cid:33) 1
4

(cid:32)



−C(cid:48)

1
nI∗





.

n(cid:48)I∗

B.3.3 Controlling Q3
By Taylor’s theorem, we have log(1 − I∗) = −I∗ − I∗2
2(1−u)2 for some u ∈ [0, I∗]. This implies that
when I∗ ≤ 1 − c0 for some constant c0 ∈ (0, 1), we have log(1 − I∗) ≥ −I∗ − C2I∗2 for some
constant C2 ∈ (0, 1) that only depends on c0. It follows that

Q3 ≥ exp (cid:2)−(1 + C2I∗)n(cid:48)I∗(cid:3) .

B.3.4 Putting together

Returning to Equation (19), we obtain

P (S n(cid:48) ≥ 0) ≥ c(cid:48) exp (cid:2)−(1 + C2I∗)n(cid:48)I∗(cid:3) exp

(cid:33) 1
4

(cid:32)



−C(cid:48)

1
nI∗





n(cid:48)I∗

= exp


−







1 + C2I∗ + C(cid:48)

(cid:33) 1
4

(cid:32)

1
nI∗

+ 1

n(cid:48)I∗ log





1
c(cid:48)





.

n(cid:48)I∗

The desired inequality follows by taking ξ (cid:66) C2I∗ + C(cid:48) (cid:16) 1
under our assumptions I∗ = o(1) and nI∗ → ∞.

nI∗

(cid:17) 1
4 + 1

n(cid:48)I∗ log 1

c(cid:48) and noting that ξ = o(1)

C Proof of the ﬁrst inequality in Theorem 3

Here we prove the ﬁrst inequality in Theorem 3. The proof of the second inequality is given in
Section E.

C.1 Preliminaries

(cid:113)(cid:80)

i, j M2

(cid:12)(cid:12)(cid:12)Mi j

Recall the deﬁnitions given in Section A. We introduce some additional notations. For a matrix
M, we let (cid:107)M(cid:107)F (cid:66)
i j denote its Frobenius norm, (cid:107)M(cid:107)op its spectral norm (the maximum
(cid:12)(cid:12)(cid:12) its entrywise (cid:96)∞ norm. With another matrix G of the
singular value), and (cid:107)M(cid:107)∞ (cid:66) maxi, j
same shape as M, we use M ≥ G to mean that Mi j ≥ Gi j for all i, j.
σ∗; it can be seen that UU(cid:62) = 1

n Y∗ and in particular U is a singular vector of Y∗.
Deﬁne the projections PT (M) (cid:66) UU(cid:62)M + MUU(cid:62) − UU(cid:62)MUU(cid:62) and PT ⊥(M) (cid:66) M − PT (M)
for any M ∈ Rn×n. Recall that A is the observed matrix from Model 1, 2 or 3. For any (cid:98)Y in the
sublevel set Y(A), we introduce the shorthand γ (cid:66) (cid:107)(cid:98)Y − Y∗(cid:107)1 for the (cid:96)1 error we aim to bound.
Deﬁne the shifted adjacency matrix A0 (cid:66) A − λ∗J, where λ∗ is deﬁned in Equation (16), and the

Let U (cid:66) 1√
n

24

centered adjacency matrix (or noise matrix) W (cid:66) A − EA = A0 − EA0. Crucial to our analysis is
a “dual certiﬁcate” D, which is an n × n diagonal matrix with diagonal matrices

Dii (cid:66)

(cid:88)

j∈[n]

i jY ∗
A0

i j

= σ∗
i

(cid:88)

j∈[n]

i jσ∗
A0
j,

for each i ∈ [n].

See [10] for how D arises as a candidate solution to the dual program of the SDPs (7) and (8),
though we do not rely on the explicit form of the dual program in the subsequent proof.

Let us record some facts about any feasible solution Y to program (7) or (8).

Fact 3. For any Y feasible to program (7) or (8), we have






= 0,
Yi j − Y ∗
i j
Yi j − Y ∗
i j ≤ 0,
Yi j − Y ∗
i j ≥ 0,

if i = j,
if Y ∗
i j
if Y ∗
i j

= 1,
= −1.

Proof. Since Yii = 1 for i ∈ [n] and Y (cid:23) 0, we have (cid:107)Y(cid:107)∞ = 1. The result follows from the fact
(cid:3)
that Y∗ ∈ {±1}n×n.

Fact 4. For any Y feasible to program (7) or (8), the following hold.

(a) PT ⊥ (Y) (cid:23) 0 and Tr [PT ⊥ (Y)] = γ
n .

(b) (cid:107)PT ⊥(Y)(cid:107)∞ ≤ 4.

Proof. For part (a), note that PT ⊥ (Y) = (I − UU(cid:62))Y(I − UU(cid:62)), which is positive semideﬁnite
since Y (cid:23) 0 by feasibility to program (7) or (8). We also have

Tr [PT ⊥ (Y)]

(cid:104)(cid:16)

(i)= Tr
(ii)= Tr
= 1
n

I − UU(cid:62)(cid:17) (cid:0)Y − Y∗(cid:1)(cid:105)
−UU(cid:62)(cid:17) (cid:0)Y − Y∗(cid:1)(cid:105)
(cid:104)(cid:16)
Tr (cid:2)(cid:0)−Y∗(cid:1) (cid:0)Y − Y∗(cid:1)(cid:3) = γ
n

,

where step (i) holds since trace is invariant under cyclic permutations and the matrix I − UU(cid:62) is
ii − Yii = 0 for i ∈ [n].
idempotent, and step (ii) holds since Y ∗

For part (b), the deﬁnition of PT ⊥(·) and direct calculation give

(cid:107)PT ⊥ (Y) (cid:107)∞ ≤ (cid:107)Y(cid:107)∞ + (cid:107)UU(cid:62)Y(cid:107)∞ + (cid:107)YUU(cid:62)(cid:107)∞ + (cid:107)UU(cid:62)YUU(cid:62)(cid:107)∞ ≤ 4,

where the last step holds because for all (i, j), (UU(cid:62))i j = 1

n Y ∗

i j ∈ [− 1

n , 1

n ] and Yi j ∈ [−1, 1].

(cid:3)

We now proceed to the proof of the ﬁrst inequality in Theorem 3. Following the strategy

outlined in Section 5, we perform the proof in three steps.

C.2 Step 1: Basic inequality

As our ﬁrst step, we establish the following critical basic inequality.

Lemma 6. Any (cid:98)Y ∈ Y(A) satisﬁes the inequality

0 ≤

(cid:68)
−D, PT ⊥((cid:98)Y)

(cid:69) + (cid:68)

W, PT ⊥((cid:98)Y)

(cid:69)

.

25

We prove this lemma in Section D.1.

With the basic inequality, we can reduce the problem of bounding the error 1

n (cid:107)(cid:98)Y − Y∗(cid:107)1
to that of studying the two random matrices D (the dual certiﬁcate) and W (the noise matrix). In
particular, recall that the matrix PT ⊥((cid:98)Y) satisﬁes Tr (cid:2)PT ⊥((cid:98)Y)(cid:3) = 1
n γ and the other properties in
Fact 4. The rest of the proof relies only on these properties of PT ⊥((cid:98)Y), and it suﬃces to study how
matrices with such properties interact with D and W.

n γ = 1

Henceforth we use S 1 (cid:66) (cid:68)

to denote the two terms on
the RHS of the basic inequality. As the next two steps of the proof, we ﬁrst control S 2, and then
derive the desired exponential error rate by analyzing the sum S 1 + S 2.

−D, PT ⊥((cid:98)Y)

W, PT ⊥((cid:98)Y)

(cid:69)

and S 2 (cid:66) (cid:68)

(cid:69)

C.3 Step 2: Controlling S 2

The proposition below provides a bound on S 2.

Proposition 1. Under the conditions of Theorem 3, with probability at least 1 − 4√
n
of the following inequalities holds:

, at least one

(cid:114)



 ¯nI∗



 ,


1
nI∗



n exp


−



γ
n

≤

S 2 ≤ CS 2

(cid:114)

γ
n

1 − Ce
¯nI∗
t∗ ,

1
nI∗

where Ce > 0 and CS 2 > 0 are numeric constants.

The proof of this proposition is model-dependent, and is given in Sections D.2, D.3 and D.4 for
Models 1, 2 and 3, respectively.

While technical in its form, Proposition 1 has a simple interpretation: either the desired ex-
ponential error bound already holds, or S 2 is bounded by a small quantity that eventually dictates
the second order term in the error exponent. To proceed, we may assume that the second bound in
Proposition 1 holds. Plugging this bound into the basic inequality in Lemma 6, we obtain that

0 ≤

(cid:68)
(cid:69)
−D, PT ⊥((cid:98)Y)
(cid:124)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:123)(cid:122)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:125)
S 1

+CS 2

γ
n

·

1
t∗

(cid:114)

1
nI∗ ¯nI∗.

(20)

C.4 Step 3: Analyzing S 1 + S 2
If γ = 0 then we are done, so we assume γ > 0 in the sequel. To show that γ decays exponentially
in I∗, we need a simple pilot bound on γ that is polynomial in I∗.

Lemma 7. Suppose that τ > 0 for Model 1, nα ≥ 1 for Model 2, and np ≥ 1 for Model 3. There
exists a constant Cpilot > 0 such that with probability at least 1 − 2(e/2)−2n,

γ ≤ Cpilot

(cid:114)

n3
I∗ .

The proof is model-dependent, and is given in Sections D.5.1, D.5.2 and D.5.3 for Models 1, 2
and 3, respectively. We verify that the premise of Lemma 7 is satisﬁed: for Model 1 this is clear;
for Models 2 and 3, we have p ≥ (p−q)2
(cid:38) I∗ thanks to Facts 1(b) and 2(b) (recall p := α(1 − (cid:15)) in

p

26

Model 2), and nI∗ ≥ CI∗ for some large enough CI∗ > 0 under the premise of Theorem 3.

Now recall that the positive semideﬁnite matrix PT ⊥((cid:98)Y) has non-negative diagonal entries and

satisﬁes Tr

PT ⊥((cid:98)Y)

(cid:104)

(cid:105) = γ

n (Fact 4(a)). Deﬁne the (non-negative) numbers

bi (cid:66) (cid:16)

PT ⊥

(cid:16)
(cid:98)Y

(cid:17)(cid:17)
ii

,

bmax = 4,

β (cid:66) 1
bmax

(cid:88)

i∈[n]

bi =

γ
bmaxn

and the random variables Xi (cid:66) −Dii, which is the i-th diagonal entry of the dual certiﬁcate D
deﬁned in Section C.1. With the above notations, we have

S 1 := (cid:68)

−D, PT ⊥((cid:98)Y)

(cid:88)

(cid:69) =

i∈[n]

Xibi = bmax

(cid:32)

Xi

(cid:33)

,

bi
bmax

(cid:88)

i∈[n]

(21)

where bi
bmax

∈ [0, 1] by Fact 4(b).

To proceed, we shall employ a technique that is reminiscent of the “order statistics argument”
in [22, 23]. Let X(1) ≥ X(2) ≥ · · · ≥ X(n) be the order statistics of {Xi}. Let C be a constant to be
chosen later. For ease of presentation, we deﬁne shorthands η (cid:66) C

(cid:113)

1
nI∗ ,

(cid:20)

(cid:37) (m, c) (cid:66) 1
t∗

(cid:18) ne
m
ϑ (c) (cid:66) exp (cid:2)−(1 − cη)¯nI∗(cid:3) .

(1 + η) log

(cid:19)

− (1 − cη)¯nI∗

(cid:21)

,

and

Below we consider two cases: β > 1 and 0 < β ≤ 1, where we recall that β (cid:66) (cid:80)

i∈[n]

(cid:16) bi
bmax

(cid:17) = γ

bmaxn .

Case 1: β > 1.

In this case, the expression (21) implies that

S 1 ≤ bmax

(cid:88)

X(i) + (β − (cid:98)β(cid:99)) X((cid:100)β(cid:101))





i∈[(cid:98)β(cid:99)]





.

Combining with Equation (20), we obtain that

0 ≤ S 1 + CS 2bmaxβ


≤ bmax



(cid:88)

i∈[(cid:98)β(cid:99)]

(cid:114)

1
t∗
X(i) + CS 2

1
nI∗ ¯nI∗
(cid:114)
1
t∗



1
nI∗ ¯nI∗





+ (β − (cid:98)β(cid:99))

(cid:114)



X((cid:100)β(cid:101)) + CS 2

1
t∗

1
nI∗ ¯nI∗







.

When β is not an integer, the residual term above involving (β − (cid:98)β(cid:99)) is cumbersome. Fortunately,
the following simple lemma (proved in Section D.6) allows us to take the integer part of β.

Lemma 8. Suppose that β ∈ [1, n], and φ1 ≥ φ2 ≥ . . . ≥ φn are n ﬁxed numbers. Deﬁne
V(u) (cid:66) (cid:80)

i∈[(cid:98)u(cid:99)] φi + (u − (cid:98)u(cid:99))φ(cid:100)u(cid:101). If 0 ≤ V(β), then 0 ≤ V(β0) for any β0 ∈ [1, β].

Letting β0 (cid:66) (cid:98)β(cid:99) and invoking Lemma 8, we deduce from the last displayed equation that

0 ≤ bmax

(cid:88)

i∈[β0]



X(i) + CS 2

(cid:114)

1
nI∗ ¯nI∗





1
t∗

27

= bmax

(cid:88)

i∈[β0]

X(i) + bmaxβ0CS 2

(cid:114)

1
t∗

≤ bmax · max
M⊂[n]
|M|=β0





(cid:88)

i∈M





Xi

1
nI∗ ¯nI∗
(cid:114)

+ bmaxβ0CS 2

1
t∗

1
nI∗ ¯nI∗.

The following lemma, proved in Section D.7, provides a tight bound on the ﬁrst sum above.

Lemma 9. Let C be any constant satisfying C ≥ 2

number satisfying 1 ≤ M ≤ max

√

(cid:113)

(cid:27)

2. Let η := C

1
nI∗ and M be any positive
. If nI∗ ≥ CI∗ for some constant CI∗ ≥ 4, then we have

(cid:26)
1, C

(cid:113)

n
I∗

(cid:88)




i∈M





Xi

≤

1
t∗

max
M⊂[n]
|M|=m

(cid:18)
(1 + η)m log

(cid:19)

(cid:18) ne
m

− (1 − 2η)m¯nI∗

(cid:19)

, ∀m = 1, 2, . . . , (cid:98)M(cid:99)

with probability at least 1 − 3 exp

(cid:16)

−

(cid:112)

log n

(cid:17)
.

Set C = C(cid:48)

pilot := Cpilot

bmax

(cid:113)

and η = C

1

nI∗ . Note that Lemma 7 ensures that β0 ≤ β ≤ C(cid:48)

pilot

(cid:113)

n
I∗

(cid:113)

n
I∗ and the above C, we

with high probability.10 Therefore, applying Lemma 9 with M = C(cid:48)
obtain that with high probability,

pilot

(cid:114)

1
nI∗ ¯nI∗
η¯nI∗

= β0 · (cid:37) (β0, 2) + β0

1
0 ≤ β0 · (cid:37) (β0, 2) + CS 2
t∗ β0
CS 2
1
C(cid:48)
t∗

 ,

β0, 2 +

= β0 · (cid:37)

CS 2
C(cid:48)



pilot

pilot

which implies 0 ≤ (cid:37)

(cid:18)
β0, 2 + CS 2
C(cid:48)

pilot

(cid:19)
. Rearranging this inequality using the deﬁnition of (cid:37), we obtain

β0 ≤ en · exp


−



1 − (2 + CS 2/C(cid:48)

pilot)η

1 + η



 .

¯nI∗

To simplify the last RHS, we use the following elementary lemma.

Lemma 10. If 0 ≤ η ≤ 1

Proof. We have 1−C0η
1+η
step (b) holds since η ≤ 1

C0+1 for some C0 > 0, then 1−C0η
= 1 − (1+C0)η
1+η
C0+1 .

(a)
≥ 1 − (1 + C0)η

1+η ≥ 1 − (C0 + 1)η ≥ 0.

(b)
≥ 0, where step (a) holds since η ≥ 0 and
(cid:3)

The premise of Theorem 3, i.e., nI∗ ≥ CI∗ for CI∗ suﬃciently large, implies that η ≤ 1

C0+1 for
1+η ≥ 1 − (C0 + 1)η. Combining with the last

C0 := 2 + CS 2
C(cid:48)
displayed equation, we obtain that

pilot

. Applying the above lemma gives 1−C0η

10We assume that C(cid:48)

pilot

presented later.

β0 ≤ en · ϑ

(cid:16)
1 + C0

(cid:17) =⇒ β0 ≤

(cid:16)
(cid:106)
en · ϑ

1 + C0

(cid:17)(cid:107)

(cid:112) n

I∗ ≥ 1 here; if this is not true, we can skip to the proof under case 0 < β ≤ 1 that is

28

since β0 is an integer. Because γ
n

γ
n

≤ 2bmax

= bmax · β ≤ bmax · 2β0, it follows that
(cid:16)
(cid:106)
(cid:106)
2bmaxen · ϑ

1 + C0

en · ϑ

1 + C0

(cid:17)(cid:107)

≤

(cid:16)

(cid:17)(cid:107)

,

where in the last step we use the fact that c (cid:98)x(cid:99) ≤ (cid:98)cx(cid:99) for any real number x ≥ 0 and integer
c > 0.11 As long as nI∗ ≥ 1, we have 1

(cid:113)

nI∗ ≤

(cid:106)
n · exp

(cid:16)

≤

log(2bmaxe)

(cid:17)

· ϑ

γ
n

(cid:16)
1 + C0

(cid:17)(cid:107)

≤

1
nI∗ and hence


n · ϑ

1 + C0 + 1
C(cid:48)

pilot

log(2bmaxe)



 .


Recalling the deﬁnition of ϑ, we see that we have proved the ﬁrst inequality in Theorem 3.

Case 2: 0 < β ≤ 1.

In this case, continuing from the expression (21), we have

S 1 ≤ bmaxβ · X(1) ≤ bmaxβ · (cid:37) (1, 2) ,
√

where in the last step we apply Lemma 9 with m = M = 1, C = 2
with Equation (20), we obtain that

2 and η = C

(cid:113)

1
nI∗ . Combining

0 ≤ bmaxβ · (cid:37) (1, 2) + CS 2

γ
n
= bmaxβ · (cid:37) (1, 2) + bmaxβ
(cid:33)
(cid:32)
1, 2 +

= bmaxβ · (cid:37)

CS 2
√
2
2

(cid:114)

1
nI∗ ¯nI∗
CS 2
η¯nI∗
√
2
2

1
t∗
1
t∗

,

which implies (cid:37)

(cid:18)

1, 2 + CS 2
√
2

2

(cid:19)

≥ 0. Rearranging this inequality using the deﬁnition of (cid:37), we obtain

1 ≤ en · exp


−



1 − (2 + CS 2/2

1 + η

√

2)η



 .

¯nI∗

Applying Lemma 10 with C0 = 2 + CS 2
displayed equation, we obtain

√

2

2

gives 1−C0η

1+η ≥ 1 − (C0 + 1)η. Combining with the last

1 ≤ en · ϑ (C0 + 1) =⇒ 1 ≤ (cid:98)en · ϑ (C0 + 1)(cid:99) .

But we have

γ
bmaxn

= β ≤ 1 by the case assumption. It follows that

γ
n

≤ bmax (cid:98)en · ϑ (C0 + 1)(cid:99) ≤ (cid:98)bmaxen · ϑ (C0 + 1)(cid:99) ,

where in the last step we use the fact that c (cid:98)x(cid:99) ≤ (cid:98)cx(cid:99) for any real x ≥ 0 and integer c > 0. As
long as nI∗ ≥ 1, we have 1

(cid:113)

nI∗ ≤

1
nI∗ and hence

(cid:106)
n · exp

(cid:16)

≤

log(bmaxe)

(cid:17)

(cid:16)
· ϑ

1 + C0

(cid:17)(cid:107)

γ
n

(cid:36)

≤

n · ϑ

(cid:32)
1 + C0 + 1
√
2
2

(cid:33)(cid:37)

log(bmaxe)

.

Recalling the deﬁnition of ϑ, we see that we have proved the ﬁrst inequality in Theorem 3.

11Proof: we have (cid:98)cx(cid:99) = (cid:98)c (cid:98)x(cid:99) + (cx − c (cid:98)x(cid:99))(cid:99) ≥ (cid:98)c (cid:98)x(cid:99)(cid:99) = c (cid:98)x(cid:99) by noting that cx − c (cid:98)x(cid:99) ≥ 0 and c (cid:98)x(cid:99) is an integer.

29

D Proofs of Technical Lemmas in Section C

D.1 Proof of Lemma 6
Recall the matrices W and D deﬁned in Section C.1. Let d := (D11, . . . , Dnn)(cid:62) ∈ Rn be the vector
(cid:17) = A0σ∗, where ◦ denotes
of diagonal entries of D. Note that Dσ∗ = σ∗ ◦ d = σ∗ ◦
element-wise product. Therefore, we have the identity

(cid:16)
σ∗ ◦ (A0σ∗)

DY∗ = Dσ∗ (cid:0)σ∗(cid:1) (cid:62) = A0σ∗ (cid:0)σ∗(cid:1) (cid:62) = A0Y∗.

To prove the basic inequality, let us ﬁx an arbitrary (cid:98)Y ∈ Y(A) and observe that 0 ≤

A, (cid:98)Y − Y∗(cid:69)
(cid:68)
.

On the other hand, we have

A, (cid:98)Y − Y∗(cid:69) = (cid:68)
(cid:68)

A0 − D, (cid:98)Y − Y∗(cid:69)

thanks to the following facts: (i) (cid:98)Y − Y∗ has zero diagonal and D is a diagonal matrix; (ii) for
Models 1 and 2 we have A0 = A; (iii) for Model 3 we have A0 = A − λ∗J but the program (8) used
(cid:69) = (cid:104)J, Y∗(cid:105) = 0. Using the equality DY∗ = A0Y∗ proved above,
for this model ensures that
we obtain that

(cid:68)
J, (cid:98)Y

A, (cid:98)Y − Y∗(cid:69) = (cid:68)
(cid:68)

A0 − D, (cid:98)Y

(cid:69) = (cid:68)

A0 − D, PT ⊥((cid:98)Y)

A0 − D, PT ((cid:98)Y)

(cid:69) + (cid:68)

(cid:69)

.

By deﬁnition of PT , we can write PT ((cid:98)Y) = σ∗u(cid:62) + v (σ∗) (cid:62) for some u, v ∈ Rn, hence
(cid:69) = (cid:68)

A0 − D, σ∗u(cid:62) + v (cid:0)σ∗(cid:1) (cid:62)(cid:69) = 0

(cid:68)
A0 − D, PT ((cid:98)Y)

because Dσ∗ = A0σ∗. It follows that

A, (cid:98)Y − Y∗(cid:69) = (cid:68)
(cid:68)

A0 − D, PT ⊥((cid:98)Y)

(cid:69) = (cid:68)

(EA − λ∗J) + W − D, PT ⊥((cid:98)Y)

(cid:69)

.

We shall prove later that

Taking this identity as given, we obtain 0 ≤
the proof of Lemma 6.

(cid:68)EA − λ∗J, PT ⊥((cid:98)Y)
(cid:68)
A, (cid:98)Y − Y∗(cid:69)

(cid:69) = 0.
(cid:68)
W − D, PT ⊥((cid:98)Y)

≤

(22)

(cid:69)
, thereby completing

Proof of inequality (22). Under Models 1 and 2, we have EA = cY∗ for some scalar c as well as
λ∗ = 0 as chosen in (16), hence

(cid:68)EA − λ∗J, PT ⊥((cid:98)Y)

(cid:69) = c

(cid:68)
PT ⊥(Y∗), (cid:98)Y

(cid:69) = 0

as desired. Under Model 3, we have the equalities

(cid:10)EA − λ∗J, PT ⊥ (Y)(cid:11) (a)=

(cid:18) p − q
2

Y∗ + p + q
2
(cid:29) (c)= 0,

J − λ∗J, (cid:98)Y

(cid:19)
J − λ∗J

, (cid:98)Y

(cid:29)

(cid:28)

(b)=

PT ⊥
(cid:28) p + q
2
2 Y∗ + p+q

where step (a) holds since EA = p−q
2 J and the projection PT ⊥ is self-adjoint, step (b)
holds since PT ⊥(Y∗) = 0 and PT ⊥(J) = J (because (I − UU(cid:62))J = (I − n−1Y∗)J = J), and step (c)
(cid:69) = 0 by feasibility of the matrix (cid:98)Y ∈ Y(A) to the program (8).
(cid:3)
holds since

(cid:68)
J, (cid:98)Y

30

D.2 Proof of Proposition 1 for Model 1 (Z2)

We shall make use of the following matrix inequality:

(cid:104)G, M(cid:105) ≤ (cid:107)G(cid:107)op Tr(M), ∀G, ∀M (cid:23) 0,

(23)

which is due to the fact that for M (cid:23) 0, Tr(M) equals the sum of its singular values. We also need
the following spectral norm bound, which is from a direct application of [20, Theorem 2.11].

Lemma 11. We have (cid:107)W(cid:107)op ≤ (2 +

√

2)

√

τ2n with probability at least 1 − e−n/2.

Turning to bounding S 2, we have with probability at least 1 − e−n/2,

S 2 := (cid:68)

W, PT ⊥((cid:98)Y)

(cid:69) (a)

(cid:104)

PT ⊥((cid:98)Y)

(cid:105)

≤ (cid:107)W(cid:107)op · Tr
√
γ
(b)
≤ 4τ
n
(cid:114)

n ·

(c)= 4

√
2

γ
n

1
nI∗

nI∗
t∗ ,

where step (a) follows from noting that PT ⊥((cid:98)Y) (cid:23) 0 by Fact 4(a) and applying inequality (23),
step (b) holds by Lemma 11 and Fact 4(a), and step (c) holds by deﬁnitions of I∗ in Equation (5)
and t∗ in Equation (15). Setting CS 2

2 completes the proof of Proposition 1 for Z2.

= 4

√

D.3 Proof of Proposition 1 for Mode 2 (CBM)
Recall that S 2 := (cid:68)
. We control the right hand side by splitting it into two parts, one
involving a trimmed version of W and the other the residual. This technique is similar to those in
[23, 63], but here we use it for a diﬀerent model.

(cid:69)
PT ⊥((cid:98)Y), W

Trimming. We consider an equivalent way of generating A under Mode 2 (CBM). Deﬁne a
symmetric random matrix G ∈ {±1}n×n such that Gii = 0 for i ∈ [n] and
are generated
independently as

(cid:110)
Gi j : i < j

(cid:111)

Gi j =





w.p. 1 − (cid:15),

Y ∗
i j,
−Y ∗
i j, w.p. (cid:15).

The observed matrix A from Model 2 can be equivalently generated by

Ai j =



Gi j, w.p. α,

0

w.p. 1 − α,

independently for i < j,

with Aii = 0 for i ∈ [n].12 We introduce a few additional notations. For a vector v ∈ Rn, we let (cid:107)v(cid:107)0
denote the number of nonzero entries in v. For a matrix M ∈ Rn×n, we let Mup be obtained from M
by zeroing out its lower triangular entries, Mi,: and M:, j be the i-th row and j-th column of M re-
spectively, and we deﬁne the trimmed matrix (cid:101)M (cid:66) (cid:16)
.
With the above notations, we note that Aup and Gup are both matrices with independent entries.

(cid:17)
Mi jI{(cid:107)Mi,:(cid:107)0 ≤ 40αn, (cid:107)M:, j(cid:107)0 ≤ 40αn}

i, j∈[n]

We ﬁrst record a series of lemmas that are useful for our proof to follow.

12As mentioned in Section 3.2, it is inconsequential to change the diagonal entries of A.

31

Lemma 12. For some absolute constant C > 0, we have

P (cid:16)

(cid:107)(cid:103)Aup − αGup(cid:107)op ≥ C

√

αn

(cid:17)

≤

1
n3

.

Proof. Note that (cid:107)Gup(cid:107)∞ ≤ 1 surely. Applying [38, Lemma 3.2] with m and (cid:15) therein set to n and
αn, we obtain P((cid:107)(cid:103)Aup − αGup(cid:107)op ≥ C
αn | Gup) ≤ 1
n3 for any Gup.13 Integrating out Gup yields
(cid:3)
the result.

√

Lemma 13. Let M ∈ Rn×n be a random matrix whose entries Mi j are independent mean-zero
(cid:12)(cid:12)(cid:12) ≤ C(cid:48) for some constant C(cid:48) ≥ 0. Then there exists a constant C > 0
random variables with
such that with probability at least 1 − 2e−n, we have (cid:107)M(cid:107)op ≤ C

(cid:12)(cid:12)(cid:12)Mi j

n.

√

Proof. Such a result is standard. For example, it follows as a corollary of Theorem 4.4.5 in [57]
with m = n, t =
(cid:3)

n and K ≤ C(cid:48)(cid:48) for some constant C(cid:48) since (cid:107)M(cid:107)∞ ≤ C(cid:48).

√

Lemma 14. Let M ∈ {0, 1}n×n be a binary matrix with Mii = 0 for all i ∈ [n], and {Mi j}i, j∈[n] being
independent Bernoulli random variables. Let p(cid:48) (cid:66) maxi j EMi j. Deﬁne T (cid:66) {i ∈ [n] : (cid:80)
j Mi j ≥
40np(cid:48)}, Zi (cid:66) (cid:80)
n for a suﬃciently large
positive constant C, then with probability at least 1 − 1√
n

j Mi jI{i ∈ T }. If p(cid:48) ≥ C
, we have

(cid:12)(cid:12)(cid:12) I{i ∈ T } and Z(cid:48)

(cid:12)(cid:12)(cid:12)Mi j − EMi j

(cid:66) (cid:80)

j

i

(cid:88)

i

Zi ≤ 2

(cid:88)

i

i ≤ 40n2 p(cid:48)e−5np(cid:48)
Z(cid:48)

Proof. Deﬁne the event B := (cid:110)(cid:80)
Applying Lemma C.5 in [63] with p therein set to 2p(cid:48),14 we obtain that P {B} ≤ e−10np(cid:48)
the case np(cid:48) ≥ 1
where C ≤ np(cid:48) < 1

10 log n.
. Under
as claimed. Next consider the case

10 log n, this probability is at most e− log n ≤ 1√

. First consider the case where np(cid:48) ≥ 1

i Zi > 40n2 p(cid:48)e−5np(cid:48)(cid:111)

j Mi jI{i ∈ T } ≥ 40np(cid:48)I{i ∈ T } by deﬁnition of T , we have
(cid:88)

10 log n. Since (cid:80)
(cid:88)

(cid:88)

n

Zi ≤

i

i
(cid:88)

≤ 2

Mi jI{i ∈ T } + np(cid:48) (cid:88)
i
j
(cid:88)
(cid:88)

Mi jI{i ∈ T } = 2

Z(cid:48)
i .

I{i ∈ T }

i

j

i

; note that ε ∈ (0, 1/2] and 21np(cid:48) + 2 log ε−1 ≤ 40np(cid:48) since p(cid:48) ≥ C

n . Applying

Set ε = 20np(cid:48)e−5np(cid:48)
Lemma 8.1 in [54] with the above ε,15 we obtain that


(cid:88)


i > 20n2 p(cid:48)e−5np(cid:48)
Z(cid:48)





P

i

≤ exp

−10n2 p(cid:48)e−5np(cid:48)(cid:17)
(cid:16)

(cid:18)
−10Cne− 1

2 log n

(cid:19)

≤ exp

= exp

(cid:16)
−10C

(cid:17)

√
n

≤

1
√
n

.

Combining the last two displayed equations proves that P {B} ≤ 1√
n

as claimed.

(cid:3)

13[38, Lemma 3.2] involves trimming rows and columns that contain more than 2αn nonzero entries. A closer

inspection of their proof reveals that their bound still applies to our setting, albeit with a possibly larger constant C.
14Inspecting their proof, we see that their bound holds without change for matrices with independent entries.
15Inspecting their proof, we see that their bound holds without change when the means of the Bernoulli are upper

bounded by the same p(cid:48).

32

We are now ready to bound S 2. Observe that

S 2 = 2
= 2
= 2

PT ⊥((cid:98)Y), Wup(cid:69)
(cid:68)
(cid:103)Aup − E (cid:2)Aup | G(cid:3)(cid:17) + (cid:0)E (cid:2)Aup | G(cid:3) − EAup(cid:1) + (cid:16)
(cid:16)
(cid:68)
PT ⊥((cid:98)Y),
(cid:103)Aup − αGup(cid:17) + (cid:0)αGup − αEGup(cid:1) + (cid:16)
(cid:16)
(cid:68)
Aup − (cid:103)Aup(cid:17)(cid:69)
PT ⊥((cid:98)Y),
(cid:105)
(cid:105)
PT ⊥((cid:98)Y)

PT ⊥((cid:98)Y)

(cid:104)

(cid:104)

· (cid:107)(cid:103)Aup − αGup(cid:107)op + 2α Tr
γ
n

(cid:107)(cid:103)Aup − αGup(cid:107)op + 2α

(cid:107)Gup − EGup(cid:107)op + 8(cid:107)Aup − (cid:103)Aup(cid:107)1,

(a)
≤ 2 Tr

(b)
≤ 2

γ
n

Aup − (cid:103)Aup(cid:17)(cid:69)

· (cid:107)Gup − EGup(cid:107)op + 2(cid:107)PT ⊥((cid:98)Y)(cid:107)∞(cid:107)Aup − (cid:103)Aup(cid:107)1

where step (a) follows noting that PT ⊥((cid:98)Y) (cid:23) 0 (by Fact 4(a)) and applying inequality (23), and
step (b) holds by Fact 4(a) and Fact 4(b).

We then apply Lemma 12 to bound (cid:107)(cid:103)Aup − αGup(cid:107)op, Lemma 13 to bound (cid:107)Gup − EGup(cid:107)op,
i j | for
i j |} are independent Bernoulli random variables with means ui j ≤ α).
n of Lemma 14 is satisﬁed by the assumption of this proposition
≤ p ≤ α). It

and Lemma 14 to Aup and (Aup)(cid:62) to bound (cid:107)Aup − (cid:103)Aup(cid:107)1 (we do so by setting Mi j = |Aup
i, j ∈ [n] and noting that {|Aup
Note that the assumption α ≥ C
that nI∗ ≥ CI∗ for some large enough CI∗ > 0 (since Fact 1(b) implies I∗ (cid:46) (p−q)2
follows that with probability at least 1 − 4√
n

, there holds

p

S 2 ≤ C0

√

γ
n

nα + C1n2αe−5nα (cid:67) C0Q1 + C1Q2

for some constants C0, C1 > 0. It remains to bound Q1 and Q2 above.
√

For Q1, Fact 1(b) implies that

I∗ ≤ C(cid:48) p−q√

p for some constant C(cid:48) > 0 and therefore

Q1 = γ

p − q
p − q

(cid:114)

p
n

≤

1
C(cid:48) γ(p − q)

(cid:114)

1
nI∗ .

Bounding Q2 involves some elementary manipulation.

Controlling Q2. We record an elementary inequality.
Lemma 15. There exists a constant CI∗ ≥ 1 such that if nI∗ ≥ CI∗, then pe−5pn ≤ (p − q)e−5nI∗/2.

Proof. Note that pn ≥ (p − q)n ≥ (p−q)2
p n ≥ 1
As long as CI∗ is suﬃciently large, we have pn

C(cid:48) CI∗ for some constant C(cid:48) > 0 by Fact 1(b).

C(cid:48) nI∗ ≥ 1
2 ≤ e5pn/2. These inequalities imply that

p
p − q

≤

pn
2

≤ e5pn/2 ≤ e5(2p−I∗)n/2.

Multiplying both sides by (p − q)e−5pn yields the claimed inequality.

(cid:3)

1

Equipped with the above bound, we are ready to bound Q2. Let κ (cid:66) e−(1−ξ)nI∗
(cid:113)
nI∗ for some constant Ce > 0 such that (cid:98)nκ(cid:99) > 0. If γ

Ce
in Proposition 1 holds and we are done. It remains to consider the case γ
that (cid:98)nκ(cid:99) is a positive integer and γ > n (cid:98)nκ(cid:99) ≥ 1

where ξ (cid:66)
n ≤ (cid:98)nκ(cid:99), then the ﬁrst inequality
n > (cid:98)nκ(cid:99) > 0. We have

= 0 or γ

n

2 n2κ. Hence,

Q2 = αn2e−5αn

(a)
≤ 2pn2e−5pn

33

(b)
≤ 2(p − q)n2e−5nI∗/2
≤ 2(p − q)e−nI∗
(c)
≤ 2(p − q)e−nI∗
≤ 4(p − q)e−nI∗

· n2κ

· γ,

· n2e−nI∗

where step (a) holds since α = 1
and step (c) holds by deﬁnition of κ. Choosing CI∗ > 0 large enough so that e−nI∗/2 ≤

2 ] imply α ∈ [p, 2p], step (b) holds by Lemma 15,
1
nI∗ , we

1−(cid:15) p and (cid:15) ∈ [0, 1

(cid:113)

have Q2 ≤ 4γ(p − q)

(cid:113)

1
nI∗ .

Putting together. Combining the above bounds for Q1 and Q2, we obtain that

S 2 ≤ C2γ(p − q)

(cid:114)

(cid:114)

1
nI∗

= C2

γ
n

1
nI∗ n(p − q)

for some constant C2 > 0. Under the assumption 0 < q ≤ p ≤ 1, we have p − q ≤ C(cid:48) I∗
for
some constant C(cid:48) > 0 by Facts 1(a) and 1(b). This completes the proof of Proposition 1 for CBM.

t∗ · 1−(cid:15)

(cid:15)

D.4 Proof of Proposition 1 for Model 3 (SBM)
Similarly to the proof for Model 2, we control the RHS of S 2 = (cid:68)
by splitting it into
two parts, one involving a trimmed version of W and the other the residual. This technique is
similar to those in [23, 63], but here we provide somewhat tighter bounds.

(cid:69)
PT ⊥((cid:98)Y), W

Trimming. We record a technical lemma concerning a trimmed Bernoulli matrix

Lemma 16. Suppose M ∈ Rn×n is a random matrix with zero on the diagonal and independent
entries {Mi j} with the following distribution: Mi j = 1 − pi j with probability pi j, and Mi j = −pi j
with probability 1 − pi j. Let p(cid:48) (cid:66) maxi j pi j, and let (cid:101)M be the matrix obtained from M by zeroing
out all the rows and columns having more than 40np(cid:48) positive entries. Then there exists some
constant C > 0 such that with probability at least 1 − 1
n2 ,

(cid:107) (cid:101)M(cid:107)op ≤ C

(cid:112)

np(cid:48)

Proof. The claim follows from [23, Lemma 9] with σ2 therein set to p(cid:48).

(cid:3)

Let Wup be obtained from W by zeroing out its lower triangular entries. To bound S 2, we

observe that

S 2 = 2
= 2

(cid:68)
PT ⊥((cid:98)Y), Wup(cid:69)
PT ⊥((cid:98)Y), (cid:103)Wup(cid:69) + 2
(cid:68)
(cid:105)
(cid:104)
PT ⊥((cid:98)Y)

(a)
≤ 2 Tr

PT ⊥((cid:98)Y), Wup − (cid:103)Wup(cid:69)
(cid:68)

· (cid:107) (cid:103)Wup(cid:107)op + 2(cid:107)PT ⊥((cid:98)Y)(cid:107)∞(cid:107)Wup − (cid:103)Wup(cid:107)1

(b)
≤ 2

γ
n

(cid:107) (cid:103)Wup(cid:107)op + 8(cid:107)Wup − (cid:103)Wup(cid:107)1,

where step (a) follows from noting that PT ⊥((cid:98)Y) (cid:23) 0 (by Fact 4(a)) and applying inequality (23),
and step (b) holds by Facts 4(a) and 4(b). We then apply Lemma 16 to (cid:103)Wup to bound (cid:107) (cid:103)Wup(cid:107)op,

34

and apply Lemma 14 to Wup and (Wup)(cid:62) to bound (cid:107)Wup − (cid:103)Wup(cid:107)1.16 Note that the assumption
p(cid:48) ≥ C
n of Lemma 14 is satisﬁed by the assumption of this proposition that nI∗ ≥ CI∗ for some
large enough CI∗ > 0 (since Fact 2(b) implies I∗ (cid:46) (p−q)2
p ≤ p). We conclude that with probability
at least 1 − 1

,

n2 − 2√

n

S 2 ≤ C0

γ
n

√

np + C1n2 pe−5np (cid:67) C0Q1 + C1Q2

for some constants C0, C1 > 0. It remains to control Q1 and Q2 above.

√

For Q1, note that Fact 2(b) implies

I∗ ≤ C(cid:48) p−q√

p for some constant C(cid:48) > 0 and therefore

Q1 = γ

p − q
p − q

(cid:114)

p
n

≤

1
C(cid:48) γ(p − q)

(cid:114)

1
nI∗ .

Bounding Q2 involves some elementary manipulation.

Controlling Q2. We record an elementary inequality.
Lemma 17. There exists a constant CI∗ ≥ 1 such that if nI∗ ≥ CI∗, then pe−5pn/2 ≤ (p − q)e−5nI∗/4.
Proof. Note that pn ≥ (p − q)n ≥ (p−q)2
p n ≥ 1
As long as CI∗ is suﬃciently large, we have pn

C(cid:48) CI∗ for some constant C(cid:48) > 0 by Fact 2(b).

C(cid:48) nI∗ ≥ 1
2 ≤ e5pn/4. These inequalities imply that
≤ e5pn/4 ≤ e5(2p−I∗)n/4.

p
p − q

≤

pn
2

Multiplying both sides by (p − q)e−5pn/2 yields the claimed inequality.

(cid:3)

1

Equipped with the above bound, we are ready to bound Q2. Let κ (cid:66) e−(1−ξ)nI∗/2 where ξ (cid:66)
(cid:113)
nI∗ for some constant Ce > 0 such that (cid:98)nκ(cid:99) > 0. If γ
n ≤ (cid:98)nκ(cid:99), then the ﬁrst inequality
n > (cid:98)nκ(cid:99) > 0. We have

Ce
in Proposition 1 holds and we are done. It remains to consider the case γ
that (cid:98)nκ(cid:99) is a positive integer and γ > n (cid:98)nκ(cid:99) ≥ 1

= 0 or γ

2 n2κ. We therefore have

n

Q2 ≤ pn2e−5pn/2

(a)
≤ (p − q)n2e−5nI∗/4
≤ (p − q)e−nI∗/2 · n2e−(1−ξ)nI∗/2
≤ 2(p − q)e−nI∗/2 · γ

(b)
≤ 2γ(p − q)

(cid:114)

1
nI∗ ,

where step (a) is due to Lemma 17, and step (b) holds because nI∗ ≥ CI∗ for CI∗ suﬃciently large.

Putting together. Combining the above bounds for Q1 and Q2, we obtain that

S 2 ≤ C2γ(p − q)

(cid:114)

1
nI∗

for some constant C2 > 0. Under the assumption 0 < c0 p ≤ q < p ≤ 1 − c1, we have p − q ≤ C(cid:48)I∗
t∗
for a constant C(cid:48) > 0 by Facts 2(c) and 2(b). This completes the proof of Proposition 1 for SBM.

16Here, we assume that A has zero diagonal and therefore W also has zero diagonal. This assumption is inconse-

quential to our proof as mentioned in Section 3.2.

35

D.5 Proof of Lemma 7

In this section, we establish the pilot bound in Lemma 7 under each of the three models.

D.5.1 Proof of Lemma 7 for Model 1 (Z2)

Since (cid:98)Y ∈ Y(A), we have
(cid:68)
(cid:98)Y − Y∗, A

0 ≤

(cid:69) = (cid:68)

(cid:98)Y − Y∗, EA

(cid:69) + (cid:68)

(cid:69)
(cid:98)Y − Y∗, A − EA

.

By Fact 3 with Y = (cid:98)Y and the fact that EA = Y∗, we have
(cid:68)
(cid:98)Y − Y∗, A − EA
have the bound γ ≤

(cid:69)

. We proceed by controlling the RHS as

(cid:68)
(cid:98)Y − Y∗, EA

(cid:69) = −γ. Combining, we

γ ≤ Tr((cid:98)Y) · (cid:107)W(cid:107)op + Tr(Y∗) · (cid:107)W(cid:107)op = 2n(cid:107)W(cid:107)op,

which follows inequality (23) applied to positive semideﬁnite matrices (cid:98)Y and Y∗ satisfying Tr((cid:98)Y) =
Tr(Y∗) = n. Applying the spectral norm bound in Lemma 11, we obtain that with probability at
least 1 − e−n/2,

γ ≤ 8n

(cid:112)

τ2n = 4

√
2

(cid:114)

n3
I∗ ,

where the last step follows from the deﬁnition of I∗ in Equation (5). The proof is completed.

D.5.2 Proof of Lemma 7 for Model 2 (CBM)

Recall that we have introduced the shorthands p (cid:66) α(1 − (cid:15)) and q (cid:66) α(cid:15). Since (cid:98)Y ∈ Y(A), we
have

By Fact 3 and the fact that EA = (p − q)Y∗, we have
have the bound γ ≤ 1
p−q

(cid:69)
(cid:68)
(cid:98)Y − Y∗, A − EA

(cid:68)
(cid:98)Y − Y∗, A

(cid:69) = (cid:68)

0 ≤

(cid:98)Y − Y∗, EA

(cid:69)
(cid:98)Y − Y∗, A − EA

(cid:69) + (cid:68)
(cid:68)
(cid:98)Y − Y∗, EA
. To control the RHS, we compute

.

(cid:69) = −(p − q)γ. Combining, we

(cid:69)
(cid:68)
(cid:98)Y − Y∗, A − EA

≤ 2

sup
Y(cid:23)0,diag(Y)≤1

|(cid:104)Y, A − EA(cid:105)| .

Grothendieck’s inequality [30, 42] guarantees that

sup
Y(cid:23)0,diag(Y)≤1

|(cid:104)Y, A − EA(cid:105)| ≤ KG(cid:107)A − EA(cid:107)∞→1

where KG denotes Grothendieck’s constant (0 < KG ≤ 1.783) and

(cid:107)A − EA(cid:107)∞→1 := sup

x:(cid:107)x(cid:107)∞≤1

(cid:107)(A − EA)x(cid:107)1 = sup

y,z∈{±1}n

(cid:12)(cid:12)(cid:12)y(cid:62)(A − EA)z
(cid:12)(cid:12)(cid:12) .

Set v2 (cid:66) (cid:80)

1≤i≤ j≤n Var(Ai j) and note that

(cid:12)(cid:12)(cid:12)Ai j − EAi j

(cid:12)(cid:12)(cid:12) ≤ 2 for i, j ∈ [n]. For each pair of ﬁxed

vectors y, z ∈ {±1}n, the Bernstein inequality ensures that for each number t ≥ 0,

P (cid:110)(cid:12)(cid:12)(cid:12)y(cid:62)(A − EA)z

(cid:12)(cid:12)(cid:12) > t

(cid:111)

(cid:40)

≤ 2 exp

−

t2
2v2 + 4t/3

(cid:41)

.

√

Setting t =

16nv2 + 8

3 n gives
(cid:40)

P

(cid:12)(cid:12)(cid:12)y(cid:62)(A − EA)z

(cid:12)(cid:12)(cid:12) >

(cid:112)
16nv2 + 8
3

(cid:41)
n

≤ 2e−2n.

36

Applying the union bound and using the fact that v2 ≤ α n2+n
2
probability at least 1 − 22n · 2e−2n = 1 − 2(e/2)−2n,

= p

1−(cid:15) · n2+n

2 , we obtain that with

(cid:107)A − EA(cid:107)∞→1 ≤ 2

(cid:114)

2

p
1 − (cid:15)

(n3 + n2) + 8
3

n.

Combining pieces, we conclude that with probability at least 1 − 2(e/2)−2n,

(cid:69)
(cid:68)
(cid:98)Y − Y∗, A − EA

≤ 8

(cid:114)
2

p
1 − (cid:15)

(n3 + n2) + 32
3

n;

whence

γ ≤

1
p − q

(cid:32)

(cid:114)
2

8

p
1 − (cid:15)

(cid:33)

n

(n3 + n2) + 32
3
(cid:115)

(a)
≤

45
p − q

(cid:114)

p
1 − (cid:15)

(b)
≤

n3

45
C(cid:48)

n3
I∗(1 − (cid:15))

,

for some constant C(cid:48) > 0, where step (a) holds by our assumption p := α(1 − (cid:15)) ≥ 1−(cid:15)
n , and step
(b) follows from Fact 1(b). The proof is completed in view of the assumption of Model 2 that (cid:15) is
a constant.

D.5.3 Proof of Lemma 7 for Model 3 (SBM)

The proof follows similar arguments as those in Section D.5.2. Since (cid:98)Y ∈ Y(A), we have

(cid:68)
(cid:98)Y − Y∗, A

(cid:69) (a)=

0 ≤

(cid:28)
(cid:98)Y − Y∗, A −

(cid:28)
(cid:98)Y − Y∗, EA −

=

(cid:29)
p + q
J
2
p + q
2

J

(cid:29)

+ (cid:68)

(cid:69)
(cid:98)Y − Y∗, A − EA

where step (a) holds since
have

(cid:68)
J, (cid:98)Y

(cid:69) = (cid:104)J, Y∗(cid:105). By Fact 3 and the fact that EA − p+q

2 J = p−q

2 Y∗, we

(cid:28)

(cid:98)Y − Y∗, EA −

(cid:29)
J

= −

p + q
2

p − q
2

γ.

(cid:69)
(cid:68)
(cid:98)Y − Y∗, A − EA

. To control the RHS, we apply Grothendieck’s

Therefore, we have the bound γ ≤ 2
p−q
inequality [30, 42] to obtain

(cid:69)
(cid:68)
(cid:98)Y − Y∗, A − EA

≤ 2

sup
Y(cid:23)0,diag(Y)≤1

|(cid:104)Y, A − EA(cid:105)| ≤ 2KG(cid:107)A − EA(cid:107)∞→1,

where KG is Grothendieck’s constant (0 < KG ≤ 1.783) and

(cid:107)A − EA(cid:107)∞→1 := sup

x:(cid:107)x(cid:107)∞≤1

(cid:107)(A − EA)x(cid:107)1 = sup

y,z∈{±1}n

(cid:12)(cid:12)(cid:12) .
(cid:12)(cid:12)(cid:12)y(cid:62)(A − EA)z

Set v2 (cid:66) (cid:80)

1≤i< j≤n Var(Ai j). For each pair of ﬁxed vectors y, z ∈ {±1}n, the Bernstein inequality

ensures that for each number t ≥ 0,

P (cid:110)(cid:12)(cid:12)(cid:12)y(cid:62)(A − EA)z

(cid:12)(cid:12)(cid:12) > t

(cid:111)

(cid:40)

≤ 2 exp

−

t2
2v2 + 4t/3

(cid:41)

.

37

√

Setting t =

16nv2 + 8

3 n gives
(cid:40)

P

(cid:12)(cid:12)(cid:12)y(cid:62)(A − EA)z

(cid:12)(cid:12)(cid:12) >

(cid:112)
16nv2 + 8
3

(cid:41)
n

≤ 2e−2n.

Applying the union bound and using the fact that v2 ≤ p(n2 + n)/2, we obtain that with probability
at least 1 − 22n · 2e−2n = 1 − 2(e/2)−2n,

(cid:107)A − EA(cid:107)∞→1 ≤ 2

(cid:113)
2p(n3 + n2) + 8
3

n.

Combining pieces, we conclude that with probability at least 1 − 2(e/2)−2n,

(cid:32)
2

· 2KG ·

(cid:113)
2p(n3 + n2) + 8
3

(cid:33)
n

2
p − q
(cid:112)

45

γ ≤

(a)
≤

pn3

p − q

≤

45
C(cid:48)

(cid:114)

n3
I∗ ,

for some constant C(cid:48) > 0, where step (a) holds by our assumption p ≥ 1
from Fact 2(b). The proof is completed.

n and the last step follows

D.6 Proof of Lemma 8

If φi ≥ 0 for all i ∈ [(cid:100)β(cid:101)], then the result follows immediately. Now we assume that at least one of
{φi} is negative. Deﬁne w (cid:66) arg min{i ∈ [(cid:100)β(cid:101)] : φi < 0} to be the smallest index of negative φi. If
β0 ∈ [1, w − 1], we have V(β0) ≥ 0 since φi ≥ 0, ∀i ∈ [1, w − 1]. If β0 ∈ [w − 1, β], we note that
V is decreasing on [w − 1, β] since φi < 0, ∀i ∈ [w, (cid:100)β(cid:101)], hence V(β0) ≥ V(β) ≥ 0. The proof is
completed.

D.7 Proof of Lemma 9

Recall that Xi := −Dii = −σ∗
i

(cid:80)

j∈[n] A0

i jσ∗

j. For clarity of exposition, we deﬁne the shorthands

Lm (cid:66)

Lm,M (cid:66)

max
M⊂[n], |M|=m
(cid:88)

Xi,





,



Xi

(cid:88)

for m ∈ [(cid:98)M(cid:99)],


i∈M
for M ⊂ [n] with |M| = m,

(cid:19)

(cid:18) ne
m

Rm (cid:66) 1
t∗
Pm,M (cid:66) P (cid:0)Lm,M ≥ Rm

i∈M
(cid:18)
(1 + η)m log
(cid:1) ,
Pm (cid:66) P (cid:0)∃M ⊂ [n], |M| = m : Lm,M ≥ Rm
P (cid:66) P (∃m ∈ [(cid:98)M(cid:99)] : Lm ≥ Rm) .

− (1 − 2η)m¯nI∗

for M ⊂ [n] with |M| = m,
(cid:1) ,

(cid:19)

,

for m ∈ [(cid:98)M(cid:99)],

Our goal is to show that P ≤ 3 exp
M ⊂ [n] with |M| = m.

(cid:16)
−

(cid:112)

log n

(cid:17)
. We start the proof by controlling Pm,M for a ﬁxed

38

D.7.1 A closer look at Lm,M
For ﬁxed m and M, the quantity Lm,M is the sum of mn random variables: Lm,M = (cid:80)
j∈[mn] V j. A
technicality is that due to the symmetry of the matrix A, there may exist some j (cid:44) j(cid:48) ∈ [mn] such
that V j and V j(cid:48) identify the same random variable. Let us deﬁne a set to group together all such
random variables. We set

J (cid:66) (cid:110)

j ∈ [mn] : ∃ j(cid:48) ∈ [mn]\{ j} s.t. V j = V j(cid:48)

(cid:111)

.

We also deﬁne the complement of J as J (cid:123) (cid:66) [mn]\J. Note that

m1 (cid:66)

(cid:12)(cid:12)(cid:12)(cid:12)J (cid:123)(cid:12)(cid:12)(cid:12)(cid:12)

= mn − m2 + m

and

m2 (cid:66) 1
2

|J| = m(m − 1)

2

.

It is not hard to see that {V j : j ∈ J (cid:123)} and half of {V j : j ∈ J} are independent. Now we can write

Lm,M =

(cid:88)

j∈J

V j +

(cid:88)

j∈J (cid:123)

V j.

D.7.2 Controlling Pm,M
Recall that t∗ deﬁned in Equation (15) satisﬁes t∗ > 0. Using the Chernoﬀ bound, we have

Pm,M = P (cid:0)Lm,M ≥ Rm
(cid:1)
(cid:1) ≥ exp (cid:0)t∗Rm
= P (cid:0)exp (cid:0)t∗Lm,M




t∗ (cid:88)
E exp
j∈J (cid:123)

≤ exp (cid:0)−t∗Rm

(cid:1) ·

(cid:1)(cid:1)










E exp







·

V j









t∗ (cid:88)

V j

j∈J

(cid:67) Q1Q2Q3.

It suﬃces to control Q1, Q2 and Q3. By deﬁnition of Rm, we have

(cid:20)

Q1 = exp

−(1 + η)m log

(cid:19)

(cid:18) ne
m

+ (1 − 2η)m¯nI∗

(cid:21)

.

As our main step, we show that the following bounds for Q2 and Q3 hold for all three models.

Lemma 18. Under the assumption in Lemma 9, we have

Q2 ≤ exp (cid:2)−(1 − η)m¯nI∗(cid:3)

and

Q3 = 1.

The proof of the lemma is model-dependent, and is given in Sections D.7.4, D.7.5 and D.7.6 for
Models 1, 2 and 3, respectively. Combining the above bounds for Q1, Q2 and Q3, we obtain

Pm,M ≤ exp

= exp

(cid:20)

(cid:20)

−(1 + η)m log

−(1 + η)m log

(cid:19)

(cid:19)

(cid:18) ne
m
(cid:18) ne
m

+ (1 − 2η)m¯nI∗
(cid:21)

− ηm¯nI∗

.

(cid:21)

· exp (cid:2)−(1 − η)m¯nI∗(cid:3) · 1

39

D.7.3 Controlling Pm and P

Using the above bound on Pm,M and applying the union bound, we have

(cid:88)

Pm ≤

M⊂[n]:|M|=m

Pm,M ≤

(cid:33)

(cid:32)

n
m

(cid:20)

exp

−(1 + η)m log

(cid:19)

(cid:18) ne
m

(cid:21)

− ηm¯nI∗

(cid:20)

(a)
≤ exp




(b)
≤

exp

−ηm log
(cid:114)

−C



(cid:19)

(cid:18) ne
m

(cid:21)

− ηm¯nI∗

1
nI∗ log

(cid:19)

(cid:18) ne
m

−

C
2

√

nI∗

m







,

where step (a) holds due to
and the fact that ¯n ≥ n

(cid:17)
(cid:16) n
m

≤

(cid:17)m

(cid:16) en
m

, and step (b) holds due to the deﬁnition that η := C

(cid:113)

1
nI∗

√

n, then

where the last step holds since C ≥ 2. We hence have Pm ≤
then

(cid:104)

exp

(cid:112)

−

(cid:17)(cid:105)m

log n

< 1

2 . (ii) If m >

√

n,

2 . We proceed by considering two cases: (i) If m ≤
(cid:114)

(cid:114)

C

1
nI∗ log

(cid:19)

(cid:18) ne
m

+ C
2

√

nI∗ ≥

2 log

C
2

(cid:112)

≥

log n,

(cid:19)

(cid:18) ne
m
(cid:16)

(cid:114)

C

1
nI∗ log

(cid:19)

(cid:18) ne
m

+ C
2

√

√

C
2

nI∗ ≥
√

nI∗ ≥ log 10

under the assumption in Lemma 9 that C ≥ 2
(cid:2)exp (cid:0)− log 10(cid:1)(cid:3)m = 1

10m . Combining the two cases and applying union bound, we conclude that

2 and nI∗ ≥ CI∗ ≥ 4. We hence have Pm ≤

(cid:88)

Pm

Pm +

√

n
(cid:104)
exp

(cid:16)
−

n<m≤n
(cid:112)

log n

(cid:17)(cid:105)m + n ·

1
√

10

n

(cid:88)

√

1≤m≤
(cid:88)

1≤m<∞

exp

P ≤

≤

≤

(cid:112)

(cid:16)
−
(cid:16)

(cid:17)

log n
(cid:112)

log n

(cid:17) + 1
n

1 − exp
(cid:16)

−
(cid:112)

≤ 2 exp

−

log n

(cid:17) + exp (cid:0)− log n(cid:1) ≤ 3 exp

(cid:16)
−

(cid:112)

log n

(cid:17)

as desired. This completes the proof of Lemma 9.

D.7.4 Proof of Lemma 18 for Model 1 (Z2)

Recall the random variable H ∼ N(1, τ2) deﬁned in Section A. We need the following fact.

Fact 5. Under Model 1, we have the following identities

Eet∗(−H) = e−I∗

and

Ee2t∗(−H) = 1.

Proof. Recall the deﬁnitions I∗ := (2τ2)−1 and t∗ := 1
follow from direct calculation:

τ2 in Equations (5) and (15). The results

Eet∗(−H) = exp

(cid:34)
−t∗ + 1
2

(cid:35)

τ2 (cid:0)t∗(cid:1)2

= e−I∗

,

and

Ee2t∗(−H) = exp

(cid:104)

−2t∗ + 2τ2 (cid:0)t∗(cid:1)2(cid:105) = 1.

(cid:3)

To proceed, note that each of {V j : j ∈ [mn]} is distributed as −H.

40

Controlling Q2. We have

Q2 := E exp



t∗ (cid:88)
j∈J (cid:123)





V j

(a)= exp (cid:2)−m1I∗(cid:3) = exp

(cid:104)

−(mn − m2 + m)I∗(cid:105)

where step (a) follows from Fact 5. If 1 ≥ C

(cid:113)

n

I∗ , then we must have m = (cid:98)M(cid:99) = 1 and

Q2 = exp (cid:2)−mnI∗(cid:3) ≤ exp (cid:2)−(1 − η)mnI∗(cid:3) .

(cid:113)

If 1 ≤ C

n
I∗ , then we have 1 ≤ M ≤ C

(cid:104)

Q2 ≤ exp

≤ exp (cid:2)−(1 − η)mnI∗(cid:3)

(cid:113)

n
I∗ and
−(mn − m2)I∗(cid:105)
(cid:113)

where the last step holds since m ≤ M ≤ C

n

I∗ = nη. Either way, we have the desired inequality.

Controlling Q3. Fact 5 directly implies the desired equality:

Q3 := E exp



t∗ (cid:88)

j∈J





V j

= (cid:16)Ee2t∗V1

(cid:17)m2 = 1.

D.7.5 Proof of Lemma 18 for Model 2 (CBM)

Recall the deﬁnition of the random variable H in Section A. We need the following fact, whose
proof is deferred to the end of this section.

Fact 6. Under Model 2, we have the following identities

Eet∗(−H) = 1 − I∗

and

Ee2t∗(−H) = 1.

To proceed, note that each of {V j : j ∈ [mn]} is distributed as −H.

Controlling Q2. We have

Q2 := E exp



t∗ (cid:88)
j∈J (cid:123)





V j

(a)= (1 − I∗)m1

(b)= exp
(c)
≤ exp

(cid:104)

(cid:104)

(mn − m2 + m) log(1 − I∗)
−(mn − m2 + m)I∗(cid:105)

(cid:105)

(d)
≤ exp (cid:2)−(1 − η)mnI∗(cid:3) ,

where step (a) follows from Fact 6, step (b) follows from the fact that m1 = mn − m2 + m, step
I∗ = nη when
(c) holds since log(1 − x) ≤ −x, ∀x < 1, and step (d) holds since m ≤ M ≤ C

(cid:113)

n

(cid:113)

1 ≤ C

n

I∗ , or m = (cid:98)M(cid:99) = 1 when 1 ≥ C

(cid:113)

n
I∗ . We thus obtain the desired bound on Q2.

41

Controlling Q3. Fact 6 directly implies the desired equality:

Q3 := E exp



t∗ (cid:88)

j∈J





V j

= (cid:16)Ee2t∗(−H)(cid:17)m2 = 1.

Proof of Fact 6. Recall the shorthands p (cid:66) α(1 − (cid:15)) and q (cid:66) α(cid:15) introduced for Model 2; note that
2 log 1−(cid:15)
α = p + q. Also recall the deﬁnitions I∗ := (
in Equations (5) and (15). The results follow from direct calculation:

q)2 and t∗ := 1

α(1 − (cid:15)) −

α(cid:15))2 = (

p −

√

√

√

√

(cid:15)

Eet∗(−H) = (1 − α) + pe−t∗ + qet∗
= (1 − α) + 2

pq = 1 − I∗,

√

and

Ee2t∗(−H) = (1 − α) + pe−2t∗ + qe2t∗

= (1 − α) + q + p = 1.

(cid:3)

D.7.6 Proof of Lemma 18 for Model 3 (SBM)

We record the following fact, whose proof is deferred to the end of this section.

Fact 7. Let Z ∼ Ber(q) and Y ∼ Ber(p). We have the following identities

Eet∗ZEe−t∗Y = e−I∗

,

(cid:16)Eet∗Z(cid:17) 1

2 (cid:16)Ee−t∗Y (cid:17)− 1

2 e−t∗λ∗ = 1,
Ee2t∗ZEe−2t∗Y = 1,

(cid:16)Ee2t∗Z(cid:17) 1

2 (cid:16)Ee−2t∗Y (cid:17)− 1

2 e−2t∗λ∗ = 1.

j} i.i.d.∼ Ber(p). Note that each of {V j : j ∈

Let {Z j}, {Z(cid:48)

j} i.i.d.∼ Ber(q) and independently {Y j}, {Y (cid:48)
[mn]} is distributed as either Z1 − λ∗ or −Y1 + λ∗. We deﬁne the quantities
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:66) 1
2
(cid:66) 1
2

j ∈ J (cid:123) : V j ∼ −Y1 + λ∗(cid:111)(cid:12)(cid:12)(cid:12)(cid:12) ,
j ∈ J (cid:123) : V j ∼ Z1 − λ∗(cid:111)(cid:12)(cid:12)(cid:12)(cid:12) ,
j ∈ J : V j ∼ −Y1 + λ∗(cid:111)(cid:12)(cid:12)(cid:12)(cid:12) ,
(cid:12)(cid:12)(cid:12)(cid:12)
j ∈ J : V j ∼ Z1 − λ∗(cid:111)(cid:12)(cid:12)(cid:12)(cid:12) .
(cid:12)(cid:12)(cid:12)(cid:12)

mp (cid:66)

mq (cid:66)

m(cid:48)
p

m(cid:48)
q

(cid:110)

(cid:110)

(cid:110)

(cid:110)

Note that mp + mq =

(cid:12)(cid:12)(cid:12)(cid:12)J (cid:123)(cid:12)(cid:12)(cid:12)(cid:12)

= m1 := mn − m2 + m and m(cid:48)
p

+ m(cid:48)
q

= 1

2 |J| .

Controlling Q2. Expanding the deﬁnition of Q2, we have

Q2 = E exp





t∗ (cid:88)
j∈[mq]

(Z j − λ∗) − t∗ (cid:88)
j∈[mp]

(Y j − λ∗)





42

= e−t∗λ∗(mq−mp) (cid:16)Eet∗Z1

(cid:17)mp

(cid:17)mq (cid:16)Ee−t∗Y1

(cid:32) Eet∗Z1

Ee−t∗Y1

2 mq

(cid:17) 1
2 mp+ 1

(cid:33) 1
2

e−t∗λ∗


mq−mp



.

= (cid:16)Eet∗Z1Ee−t∗Y1

By Fact 7, we can continue to write

Q2 ≤ exp

≤ exp

≤ exp

(cid:32)

1
2

mp + 1
2

(cid:33)

(cid:33)

I∗

mq

(mn − m2 + m)I∗

(cid:33)

(cid:32)
−

(cid:32)
−

1
2

(cid:18)
−(1 − η)

(cid:19)

,

I∗

mn
2

where the last step holds since m ≤ M ≤ C

(cid:113)

1 ≥ C

n
I∗ . We thus obtain the desired bound on Q2.

(cid:113)

n

I∗ = nη when 1 ≤ C

(cid:113)

n

I∗ , or m = (cid:98)M(cid:99) = 1 when

Controlling Q3. Similar to controlling Q2, we compute


j − λ∗) − 2t∗ (cid:88)
(Z(cid:48)

(Y (cid:48)

j − λ∗)





Q3 = E exp



2t∗ (cid:88)
j∈[m(cid:48)
q]
p) (cid:16)Ee2t∗Z(cid:48)

q−m(cid:48)

1

= e−2t∗λ∗(m(cid:48)

(cid:17)m(cid:48)

+ 1

2 m(cid:48)

1

j∈[m(cid:48)
p]
q (cid:16)Ee−2t∗Y (cid:48)
(cid:17)m(cid:48)
p

(cid:32) Ee2t∗Z(cid:48)

Ee−2t∗Y (cid:48)

1

q

1

(cid:33) 1
2

e−2t∗λ∗

m(cid:48)

q−m(cid:48)
p





= 1,

= (cid:16)Ee2t∗Z(cid:48)

1Ee−2t∗Y (cid:48)

1

(cid:17) 1
2 m(cid:48)

p

where the last step holds due to Fact 7.

Proof of Fact 7. Under Model 3, recall the deﬁnitions I∗ := −2 log
2 log p(1−q)
2t∗ log 1−q
t∗ := 1
ﬁrst equation, we compute

(1 − p)(1 − q)
,
1−p in Equations (5), (15) and (16), respectively. For the

q(1−p) and λ∗ := 1

(cid:104) √

pq + (cid:112)

(cid:105)

Eet∗ZEe−t∗Y = (cid:16)

qet∗ + 1 − q

(cid:17) (cid:16)

(cid:17)
pe−t∗ + 1 − p

= pq + (1 − p)(1 − q) + q(1 − p)et∗ + p(1 − q)pe−t∗
(cid:112)
= pq + (1 − p)(1 − q) + 2
= (cid:16) √
= e−I∗

pq(1 − p)(1 − q)
(cid:17)2

pq + (cid:112)
.

(1 − p)(1 − q)

For the second equation, noting that e2t∗λ∗ = 1−q

1−p , we compute

Eet∗Z
Ee−t∗Y

· e−2t∗λ∗ = qet∗ + 1 − q
pe−t∗ + 1 − p
(cid:113)
p(1−q)
q(1−p)

q

=

+ 1 − q

·

1 − p
1 − q

·

1 − p
1 − q

= 1

(cid:113)

p

q(1−p)
p(1−q)

+ 1 − p

43

and then take the square root of both sides. Finally, the remaining two equations follow from
e2t∗λ∗ = 1−q

1−p and the identities

Ee2t∗Z = qe2t∗ + 1 − q = q

p(1 − q)
q(1 − p)

Ee−2t∗Y = pe−2t∗ + 1 − p = p

+ 1 − q = 1 − q
,
1 − p
+ 1 − p = 1 − p
1 − q

q(1 − p)
p(1 − q)

.

(cid:3)

E Proof of the second inequality in Theorem 3

Fix any (cid:98)Y ∈ Y(A). Note that (cid:98)Y, Y∗ ∈ [−1, 1]n×n by feasibility to the program (7) or (8). It follows
that
(cid:27)
(cid:12)(cid:12)(cid:12)(cid:12)

(cid:26)(cid:12)(cid:12)(cid:12)(cid:12)(cid:98)Yi j − Y ∗

= 2(cid:107)(cid:98)Y − Y∗(cid:107)1.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:98)Yi j − Y ∗

(cid:107)(cid:98)Y − Y∗(cid:107)2

(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12)

i j

i j

·

F ≤ max
i, j∈[n]

i, j∈[n]

Combining with the ﬁrst inequality of Theorem 3, we obtain

(cid:107)(cid:98)Y − Y∗(cid:107)2

F ≤ n2 · 2 exp


−





1 − Ce

(cid:114)



 ¯nI∗





1
nI∗

=: n2 · ε.

= n. It can be seen
Let ˆv be an eigenvector of (cid:98)Y corresponding to the largest eigenvalue with (cid:107)ˆv(cid:107)2
2
that the largest eigenvalue of Y∗ is n with σ∗ being the corresponding eigenvector, and that all the
other eigenvalues are 0. Because (cid:98)Y = Y∗ + ((cid:98)Y − Y∗) and (cid:107)(cid:98)Y − Y∗(cid:107)F ≤
εn, Davis-Kahan theorem
(see, e.g., [58, Corollary 3]) implies that

√

(cid:107)g ˆu − u∗(cid:107)2 = 2

min
g∈{±1}

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

sin

(cid:18) θ
2

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

√
ε

≤ C

for some absolute constant C > 0, where ˆu and u∗ denote the unit-norm eigenvectors associated
with the largest eigenvalues of (cid:98)Y and Y∗, respectively, and θ ∈ [0, π
2 ] denotes the angle between
these two vectors. By deﬁnition ˆv =

nu∗, we obtain that

n ˆu and σ∗ =

√

√

min
g∈{±1}

(cid:107)gˆv − σ∗(cid:107)2

2 ≤ C2εn.

We proceed by relating err((cid:98)σsdp, σ∗) to ming∈{±1} (cid:107)gv − σ∗(cid:107)2
that the minimum is attained by g = 1. Since (cid:98)σsdp
i
(cid:88)

2. Without loss of generality, assume

= sign(ˆvi) by deﬁnition, we have the bound

C2εn ≥ (cid:107)ˆv − σ∗(cid:107)2

2 ≥

(ˆvi − σ∗

i )2I{sign(ˆvi) (cid:44) σ∗
i }

≥

i∈[n]
(cid:88)

i∈[n]

I{sign(ˆvi) (cid:44) σ∗
i }

≥ n · err((cid:98)σsdp, σ∗).

We divide both sides of the above equation by n, and note that the constant 2C2 can be absorbed
into Ce under the assumption that nI∗ ≥ CI∗ for CI∗ suﬃciently large. The result follows.

44

References

[1] Emmanuel Abbe. Community detection and stochastic block models: Recent developments.

Journal of Machine Learning Research, 18(177):1–86, 2018.

[2] Emmanuel Abbe, Afonso S. Bandeira, Annina Bracher, and Amit Singer. Decoding binary
node labels from censored edge measurements: Phase transition and eﬃcient recovery. IEEE
Transactions on Network Science and Engineering, 1(1):10–22, 2014.

[3] Emmanuel Abbe, Afonso S. Bandeira, and Georgina Hall. Exact recovery in the stochastic

block model. IEEE Transactions on Information Theory, 62(1):471–487, 2016.

[4] Emmanuel Abbe, Enric Boix, Peter Ralli, and Colin Sandon. Graph powering and spectral

robustness. arXiv preprint arXiv:1809.04818, 2018.

[5] Emmanuel Abbe, Jianqing Fan, Kaizheng Wang, and Yiqiao Zhong. Entrywise eigenvector
analysis of random matrices with low expected rank. arXiv preprint arXiv:1709.09565, 2017.

[6] Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block mod-
els: Fundamental limits and eﬃcient algorithms for recovery. In IEEE 56th Annual Sympo-
sium on Foundations of Computer Science (FOCS), pages 670–688. IEEE, 2015.

[7] Emmanuel Abbe and Colin Sandon. Detection in the stochastic block model with multiple
clusters: proof of the achievability conjectures, acyclic BP, and the information-computation
gap. arXiv preprint arXiv:1512.09080, 2015.

[8] Emmanuel Abbe and Colin Sandon. Recovering communities in the general stochastic block
model without knowing the parameters. In Advances in Neural Information Processing Sys-
tems, pages 676–684, 2015.

[9] Naman Agarwal, Afonso S. Bandeira, Konstantinos Koiliaris, and Alexandra Kolla. Mul-
In Compressed

tisection in the stochastic block model using semideﬁnite programming.
Sensing and its Applications, pages 125–162. Springer, 2017.

[10] Afonso S. Bandeira. Random laplacian matrices and convex relaxations. Foundations of

Computational Mathematics, 18(2):345–379, 2018.

[11] Afonso S. Bandeira, Nicolas Boumal, and Amit Singer. Tightness of the maximum likelihood
semideﬁnite relaxation for angular synchronization. Mathematical Programming, 163(1-
2):145–167, 2017.

[12] Debapratim Banerjee. Contiguity and non-reconstruction results for planted partition mod-

els: the dense case. Electronic Journal of Probability, 23, 2018.

[13] Avrim Blum and Joel Spencer. Coloring random and semi-random k-colorable graphs. Jour-

nal of Algorithms, 19(2):204–234, 1995.

[14] Charles Bordenave, Marc Lelarge, and Laurent Massoulié. Nonbacktracking spectrum of
random graphs: Community detection and nonregular ramanujan graphs. Annals of Proba-
bility, 46(1):1–71, 2018.

[15] T. Tony Cai and Xiaodong Li. Robust and computationally feasible community detection in
the presence of arbitrary outlier nodes. Annals of Statistics, 43(3):1027–1059, 2015.

45

[16] Francesco Caltagirone, Marc Lelarge, and Léo Miolane. Recovering asymmetric communi-
ties in the stochastic block model. IEEE Transactions on Network Science and Engineering,
5(3):237–246, 2018.

[17] Yudong Chen, Sujay Sanghavi, and Huan Xu. Improved graph clustering. IEEE Transactions

on Information Theory, 60(10):6440–6455, 2014.

[18] Peter Chin, Anup Rao, and Van Vu. Stochastic block model and community detection in
sparse graphs: A spectral algorithm with optimal rate of recovery. In Proceedings of The
28th Conference on Learning Theory (COLT), pages 391–423, Paris, France, July 2015.

[19] Amin Coja-Oghlan, Florent Krzakala, Will Perkins, and Lenka Zdeborova.

Information-
theoretic thresholds from the cavity method. Advances in Mathematics, 333:694–795, 2018.

[20] Kenneth R. Davidson and Stanislaw J. Szarek. Local operator theory, random matrices and
banach spaces. In W.B. Johnson and J. Lindenstrauss, editors, Handbook of the Geometry of
Banach Spaces, volume 1, pages 317–366. Elsevier Science B.V., 2001.

[21] Amir Dembo and Ofer Zeitouni. Large Deviations Techniques and Applications. Stochastic

Modelling and Applied Probability. Springer Berlin Heidelberg, 2010.

[22] Yingjie Fei and Yudong Chen. Hidden integrality of sdp relaxation for sub-gaussian mixture

models. arXiv preprint arXiv:1803.06510, 2018.

[23] Yingjie Fei and Yudong Chen. Exponential error rates of SDP for block models: Beyond
Grothendieck’s inequality. IEEE Transactions on Information Theory, 65(1):551–571, 2019.

[24] Uriel Feige and Joe Kilian. Heuristics for semirandom graph problems. Journal of Computer

and System Sciences, 63(4):639–671, 2001.

[25] Chao Gao and Zongming Ma. Minimax rates in network analysis: Graphon estimation,
community detection and hypothesis testing. arXiv preprint arXiv:1811.06055, 2018.

[26] Chao Gao, Zongming Ma, Anderson Y. Zhang, and Harrison H. Zhou. Achieving optimal
misclassiﬁcation proportion in stochastic block models. The Journal of Machine Learning
Research, 18(1):1980–2024, 2017.

[27] Chao Gao, Zongming Ma, Anderson Y. Zhang, and Harrison H. Zhou. Community detection

in degree-corrected block models. The Annals of Statistics, 46(5):2153–2185, 2018.

[28] Manuel Gil, Fady Alajaji, and Tamas Linder. Rényi divergence measures for commonly used

univariate continuous distributions. Information Sciences, 249:124–131, 2013.

[29] Christophe Giraud and Nicolas Verzelen. Partial recovery bounds for clustering with the

relaxed k means. arXiv preprint arXiv:1807.07547, 2018.

[30] Alexander Grothendieck.

Résumé de la théorie métrique des produits tensoriels
topologiques. Resenhas do Instituto de Matemática e Estatistica da Universidade de São
Paulo, 2(4):401–481, 1953.

[31] Olivier Guédon and Roman Vershynin. Community detection in sparse networks via
Grothendieck’s inequality. Probability Theory and Related Fields, 165(3-4):1025–1049,
2016.

46

[32] Bruce Hajek, Yihong Wu, and Jiaming Xu. Exact recovery threshold in the binary censored

block model. In IEEE Information Theory Workshop (ITW), pages 99–103, 2015.

[33] Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold via
IEEE Transactions on Information Theory, 62(5):2788–2797,

semideﬁnite programming.
2016.

[34] Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold
IEEE Transactions on Information Theory,

via semideﬁnite programming: Extensions.
62(10):5918–5937, 2016.

[35] Simon Heimlicher, Marc Lelarge, and Laurent Massoulié. Community detection in the la-

belled stochastic block model. arXiv preprint arXiv:1209.2910, 2012.

[36] Paul W. Holland, Kathryn B. Laskey, and Samuel Leinhardt. Stochastic blockmodels: Some

ﬁrst steps. Social Networks, 5:109–137, 1983.

[37] Varun Jog and Po-Ling Loh. Information-theoretic bounds for exact recovery in weighted
stochastic block models using the renyi divergence. arXiv preprint arXiv:1509.06418, 2015.

[38] Raghunandan H. Keshavan, Sewoong Oh, and Andrea Montanari. Matrix completion from a
few entries. In IEEE International Symposium on Information Theory, pages 324–328, 2009.

[39] Marc Lelarge, Laurent Massoulié, and Jiaming Xu. Reconstruction in the labelled stochastic
block model. IEEE Transactions on Network Science and Engineering, 2(4):152–163, 2015.

[40] Marc Lelarge and Léo Miolane. Fundamental limits of symmetric low-rank matrix estima-

tion. Probability Theory and Related Fields, pages 1–71, 2017.

[41] Xiaodong Li, Yudong Chen, and Jiaming Xu. Convex relaxation methods for community

detection. arXiv preprint arXiv:1810.00315, 2018.

[42] Joram Lindenstrauss and Aleksander Pełczy´nski. Absolutely summing operators in Lp-

spaces and their applications. Studia Mathematica, 3(29):275–326, 1968.

[43] Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Learning com-
munities in the presence of errors. In 29th Annual Conference on Learning Theory, pages
1258–1291, 2016.

[44] Laurent Massoulié. Community detection thresholds and the weak Ramanujan property. In
Proceedings of the 46th Annual ACM Symposium on Theory of Computing, pages 694–703.
ACM, 2014.

[45] Ankur Moitra, William Perry, and Alexander S. Wein. How robust are reconstruction thresh-
olds for community detection? In Proceedings of the 48th Annual ACM SIGACT Symposium
on Theory of Computing, pages 828–841. ACM, 2016.

[46] Andrea Montanari and Subhabrata Sen. Semideﬁnite programs on sparse random graphs and
their application to community detection. In Proceedings of the 48th Annual ACM SIGACT
Symposium on Theory of Computing (STOC), pages 814–827, 2016.

[47] Cristopher Moore. The computer science and physics of community detection: Landscapes,
phase transitions, and hardness. Bulletin of European Association for Theoretical Computer
Science (EATCS), 1(121), Februrary 2017.

47

[48] Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjec-

ture. arXiv preprint arXiv:1311.4115, 2013.

[49] Elchanan Mossel, Joe Neeman, and Allan Sly. Reconstruction and estimation in the planted

partition model. Probability Theory and Related Fields, 162(3-4):431–461, 2015.

[50] Elchanan Mossel, Joe Neeman, and Allan Sly. Consistency thresholds for the planted bisec-

tion model. Electronic Journal of Probability, 21(21):1–24, 2016.

[51] Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjec-

ture. Combinatorica, 38(3):665–708, 2018.

[52] Amelia Perry, Alexander S. Wein, Afonso S. Bandeira, and Ankur Moitra. Optimality
and sub-optimality of pca for spiked random matrices and synchronization. arXiv preprint
arXiv:1609.05573, 2016.

[53] William Perry and Alexander S. Wein. A semideﬁnite program for unbalanced multisection

in the stochastic block model. arXiv preprint arXiv:1507.05605, 2015.

[54] Elizaveta Rebrova and Roman Vershynin. Norms of random matrices:

local and global

problems. arXiv preprint arXiv:1608.06953, 2016.

[55] Alaa Saade, Marc Lelarge, Florent Krzakala, and Lenka Zdeborová. Spectral detection in
the censored block model. In 2015 IEEE International Symposium on Information Theory
(ISIT), pages 1184–1188. IEEE, 2015.

[56] Ludovic Stephan and Laurent Massoulié. Robustness of spectral methods for community

detection. arXiv preprint arXiv:1811.05808, 2018.

[57] Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in
Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge
University Press, 2018.

[58] Van Vu. Singular vectors under random perturbation. Random Structures & Algorithms,

39(4):526–538, 2011.

[59] Min Xu, Varun Jog, and Po-Ling Loh. Optimal rates for community estimation in the

weighted stochastic block model. arXiv preprint arXiv:1706.01175, 2017.

[60] Se-Young Yun and Alexandre Proutiere. Accurate community detection in the stochastic

block model via spectral algorithms. arXiv preprint arXiv:1412.7335, 2014.

[61] Se-Young Yun and Alexandre Proutiere. Optimal cluster recovery in the labeled stochastic
block model. In Advances in Neural Information Processing Systems, pages 965–973, 2016.

[62] Anderson Y. Zhang and Harrison H. Zhou. Minimax rates of community detection in stochas-

tic block models. The Annals of Statistics, 44(5):2252–2280, 2016.

[63] Anderson Y. Zhang and Harrison H. Zhou. Theoretical and computational guarantees of
mean ﬁeld variational inference for community detection. arXiv preprint arXiv:1710.11268,
2017.

[64] Zhixin Zhou and Ping Li. Non-asymptotic chernoﬀ lower bound and its application to com-
munity detection in stochastic block model. arXiv preprint arXiv:1812.11269, 2018.

48

