Linear Convergence of Randomized Primal-Dual Coordinate Method for
Large-scale Linear Constrained Convex Programming

Daoli Zhu * 1 Lei Zhao * 2

0
2
0
2

g
u
A
9
2

]

C
O
.
h
t
a
m

[

1
v
6
4
9
2
1
.
8
0
0
2
:
v
i
X
r
a

Abstract
Linear constrained convex programming has
many practical applications, including support
vector machine and machine learning portfolio
problems. We propose the randomized primal-
dual coordinate (RPDC) method, a randomized
coordinate extension of the ﬁrst-order primal-dual
method by (Cohen & Zhu, 1984) and (Zhao &
Zhu, 2019), to solve linear constrained convex
programming. We randomly choose a block of
variables based on a uniform distribution, lin-
earize, and apply a Bregman-like function (core
function) to the selected block to obtain simple
parallel primal-dual decomposition. We then es-
tablish almost surely convergence and expected
O(1/t) convergence rate, and expected linear con-
vergence under global strong metric subregularity.
Finally, we discuss implementation details for the
randomized primal-dual coordinate approach and
present numerical experiments on support vector
machine and machine learning portfolio problems
to verify the linear convergence.

1. Introduction

This paper considers linear constrained convex program-
ming (LCCP),

(P): min F (u) = G(u) + J(u)

s.t

Au − b = 0
u ∈ U

,

(1)

where G is a convex smooth function on the closed convex
set U ⊂ Rn; and J is a convex, possibly non-smooth

function on U ⊂ Rn. We assume that J(u) =

N
(cid:80)
i=1

Ji(ui)

*Equal contribution 1Antai College of Economics and Manage-
ment and Sino-US Global Logistics Institute, Shanghai Jiao Tong
University, Shanghai, China 2School of Naval Architecture, Ocean
and Civil Engineering, Shanghai Jiao Tong University, Shanghai,
China. Correspondence to: Lei Zhao <l.zhao@sjtu.edu.cn>.

Proceedings of the 37 th International Conference on Machine
Learning, Online, PMLR 119, 2020. Copyright 2020 by the au-
thor(s).

is additive with respect to the space decomposition

U = U1 × U2 · · · × UN , ui ∈ Ui ⊂ Rni and

N
(cid:88)

ni = n.

i=1

(2)
Each Ji is a convex but possibly non-smooth function on
Ui ⊂ Rni; A = (A1, A2, · · · , AN ) ∈ Rm×n is an appro-
priate partition of A, where Ai is an m × ni matrix and
b ∈ Rm is a vector.

1.1. Motivation

Linear constrained convex programming is an important and
challenging application problem class. We present several
example applications to demonstrate the reasons for interest
in type (P) problems.

1.1.1. SUPPORT VECTOR MACHINE

Support vector machine (SVM) is a popular supervised
learning method (Boser et al., 1992; Cortes & Vapnik,
1995), widely used for pattern recognition (Burges, 1998;
Sch¨olkopf et al., 2000) and classiﬁcation (Chang & Lin,
2011). The SVM problem can be expressed as

(SVM) min

u∈[0,c]n
s.t.

1

2 u(cid:62)Qu − 1(cid:62)
n u
y(cid:62)u = 0

,

where u ∈ Rn are the decision variables, Q ∈ Rn×n is
a symmetric and positive-deﬁnite matrix, c ∈ R is the
upperbound of all variables, y ∈ {−1, 1}n is the vector of
labels, and 1n is an n-dimensional vector of 1s.

1.1.2. MACHINE LEARNING PORTFOLIO PROBLEM

Portfolio optimization (PO) via machine learning has re-
ceived increased attention recently. PO aims to invest in a
group of ﬁnancial assets with instructions by machine, based
on ﬁnancial principles and optimization strategies. Since
this requires considerable quantitative calculation, machine
learning methods are essential to reduce human mistakes
and biases for real-world investment. (Brodie et al., 2009;
Lai et al., 2018; Li et al., 2016; Ho et al., 2015; Shen et al.,
2014) The machine learning portfolio (MLP) problem can

 
 
 
 
 
 
Linear Convergence of RPDC method for Large-scale LCCP

be expressed as

(MLP) min
u∈Rn
s.t.

1

2 u(cid:62)Σu + λ(cid:107)u(cid:107)1
µ(cid:62)u = ρ
1(cid:62)
n u = 1

,

where u ∈ Rn is the decision portfolio vector; Σ ∈ Rn×n,
which is symmetric and positive-deﬁnite, is the estimated
covariance matrix of asset returns; µ ∈ Rn is the expecta-
tion of asset returns; ρ is a predeﬁned prospective growth
rate; and 1n is an n-dimensional vector of 1s.

1.2. Related works

The augmented Lagrangian method (ALM) is an approach
for general LCCP that can overcome the dual Lagrangian
instability and non-differentiability. The augmented La-
grangian for a constrained convex program has the same
solution set as the original constrained convex program.
Consider the following augmented Lagrangian function of
(P),

Lγ(u, p) = F (u) + (cid:104)p, Au − b(cid:105) +

γ
2

(cid:107)Au − b(cid:107)2.

(3)

The ALM for equality-constrained optimization problems
can be expressed as (Hestenes, 1969; Powell, 1969)

ALM
(cid:40) uk+1 = arg min

Lγ(u, pk);
u∈U
pk+1 = pk + γ(Auk+1 − b).

Although ALM has several advantages, it does not preserve
separability, even when the initial problem is separable. One
way to decompose the augmented Lagrangian is to use alter-
nating direction method of multipliers (ADMM) (Fortin &
Glowinski, 1983), which applies a Gauss-Seidel-like mini-
mization strategy. Another way to overcome this difﬁculty
is the auxiliary problem principle of augmented Lagrangian
(APP-AL) (Cohen & Zhu, 1984) method, a fairly general
ﬁrst-order primal-dual decomposition method based on lin-
earizing the augmented Lagrangian of LCCP. The APP-AL
scheme for LCCP can be expressed as

APP-AL



uk+1 = arg min
u∈U



pk+1 = pk + γ(Auk+1 − b),

(cid:104)∇G(uk), u(cid:105) + J(u) + (cid:104)qk, Au(cid:105)
(cid:15) D(u, uk);

+ 1

where qk = pk + γ(Auk − b), D(u, v) = K(u) − K(v) −
(cid:104)∇K(v), u − v(cid:105) is a Bregman like function with K is
strongly convex and gradient Lipschitz. (Zhao & Zhu, 2019)
extended (Cohen & Zhu, 1984) to propose ﬁrst-order primal-
dual augmented Lagrangian methods for nonlinear convex
cone programming with separable and non-separable ob-
jective and constraints as an algorithm (variant auxiliary

problem principle, VAPP). They showed that APP-AL can
be viewed as a forward-backward splitting method to ﬁnd a
solution for (P): wk+1 = (Γk + A)−1(Γk − B)wk, where

A(w) =

(cid:18) ∂J(u) + NU(u)
0m

(cid:19)

and

B(w) =

(cid:18) ∇G(u) + A(cid:62)[p + γ(Au − b)]
b − Au

(cid:19)

are two maximal monotone mappings; w = (u, p);
NU(u) = {ξ : (cid:104)ξ, ζ − u(cid:105) ≤ 0, ∀ζ ∈ U} is the normal
(cid:19)

(cid:18)

cone at u to U; and Γk(w) =

1
(cid:15) ∇K(u)
γ [p − pk] − [Au − b]

1

is a strongly monotone nonlinear Bregman operator. Prob-
lem (P) can be reformulated as an augmented Lagrangian
based inclusion problem,

0 ∈ Hγ(w) =

(cid:18) ∂uLγ(u, p) + NU(u)
∇pLγ(u, p)

(cid:19)



=



∇G(u) + ∂J(u) + A(cid:62)[p + γ(Au − b)]
+NU(u)





(4)

b − Au

= A(w) + B(w).

We propose an iteration based forward-backward splitting
algorithm to solve (4), wk+1 = (Γk + A)−1(Γk − B)wk,
hence (Γk − B)wk ∈ (Γk + A)wk+1. Thus,







0 ∈

1

(cid:15) [∇K(uk+1) − ∇K(uk)] + ∇G(uk)

+A(cid:62)[pk + γ(Auk − b)] + ∂J(uk+1)
+NU(uk+1)

pk+1 − pk − γ(Auk+1 − b)







,

which is the exact inclusion problem formulation for APP-
AL. Zhao and Zhu also proposed linear convergence for
VAPP (Zhao & Zhu, 2019).

Recent big data applications employ very large datasets that
are commonly distributed over different locations. Hence it
is often impractical to assume that optimization algorithms
can traverse an entire dataset once in each iteration, because
this would be very time consuming and/or unreliable, often
resulting in low resource utilization due to synchronization
among different computing units, e.g. CPUs, GPUs, and
cores, in a distributed computing environment. On the other
hand, block coordinate descent (BCD) algorithms can make
progress by using information obtained from a randomly
selected data subsets and, hence provide more ﬂexible imple-
mentation in distributed environments. The main advantage
of BCD is to reduce complexity and memory requirements
per iteration, which becomes increasingly important for
very-large scale problems.

We brieﬂy review some related works on coordinate type
methods.

Linear Convergence of RPDC method for Large-scale LCCP

Two BCD variations are widely employed for problems
without constraints. The ﬁrst BCD variation relates to block-
choosing strategy. One common approach is a cyclic strat-
egy. (Tseng, 2001) proved BCD cyclic strategy convergence
and (Luo & Tseng, 1992) and (Wang & Lin, 2014) proved
local and global linear convergence, respectively, under
speciﬁc assumptions. Another common approach is a ran-
domized strategy. (Nesterov, 2012) studied the convergence
rate of randomized BCD for convex smooth optimization
and (Richt´arik & Tak´aˇc, 2014) and (Lu & Xiao, 2015) sub-
sequently extended Nesterov’s technique to composite opti-
mization. The second BCD variation relates to the point read
to evaluate the gradient in each iteration. The approaches are
called asynchronous BCD if the read points have different
”ages”, and synchronous BCD otherwise. All BCD variants
reviewed above are synchronous BCD. (Liu & Wright, 2015)
and (Liu et al., 2014) established the convergence rate of
asynchronous BCD for composite optimization and convex
smooth optimization, respectively, without constraints.

Few previous studies considered BCD methods for prob-
lems with constraints. (Necoara & Patrascu, 2014) proposed
a random coordinate descent algorithm for an optimization
problem with one linear constraint.
(Gao et al., 2019)
and (Xu & Zhang, 2018) considered a similar scheme to
RPDC, obtaining expected O(1/t) and O(1/t2) rates, re-
spectively. (Xu, 2019) recently proposed an asynchronous
RPDC algorithm with expected O(1/t) rate. However, to
the best of our knowledge, no previous study considered
convergence and linear convergence results for RPDC.

This paper focused on RPDC, the randomized coordinate
extension of APP-AL, as shown in Algorithm 1.

1.3. Main contributions and outline for this paper

We propose the randomized primal-dual coordinate (RPDC)
method based on the ﬁrst-order primal-dual method (Cohen
& Zhu, 1984; Zhao & Zhu, 2019). The RPDC method
randomly updates one block of variables based on a uniform
distribution. The main contributions from this paper are as
follows:

(i) We show that the sequence generated by RPDC con-
verges to an optimal solution with probability 1.

(ii) We show RPDC has expected O(1/t) rate for general

LCCP.

(iii) We establish the expected linear convergence of RPDC

under global strong metric subregularity.

(iv) We show that SVM and MLP problems satisfy global
strong metric subregularity under some reasonable con-
ditions.

(v) Finally, we discuss the implementation details of

RPDC and present numerical experiments on SVM
and MLP problems to verify linear convergence.

The remainder of this paper is organized as follows. Sec-
tion 2 discusses technical preliminaries. Section 3 shows
almost surely convergence and expected O(1/t) conver-
gence rate for RPDC. Section 4 establishes the expected
linear convergence of RPDC under global strong metric
subregularity. Section 5 discusses implementation details
for RPDC and presents numerical experiments on SVM
and MLP problems. Finally, Section 6 summarizes and
concludes the paper.

2. Preliminaries

This section provides some useful preliminaries for subse-
quent discussions and summarizes notations and assump-
tions. We denote vector inner product and Euclidean norm
as (cid:104)·(cid:105) and (cid:107) · (cid:107), respectively.

2.1. Notations and assumptions

Throughout this paper, we make the following standard
assumptions for Problem (P).

Assumption 1 (H1) J is a convex, lower semi-continuous
function (not necessarily differentiable) such that domJ ∩
U (cid:54)= ∅.
(H2) G is convex and differentiable, and its derivative is
Lipschitz with constant BG.
(H3) There exists at least one saddle point for the La-
grangian of (P).

From Assumption 1 and Theorem 3.2.12 (Ortega & Rhein-
boldt, 1970), the following descent property for G holds

G(v) − G(u) − (cid:104)∇G(u), v − u(cid:105) ≤

BG
2

(cid:107)u − v(cid:107)2.

(5)

2.2. Lagrangian and Karush-Kuhn-Tucker mapping

The Lagrangian of (P) is deﬁned as

L(u, p) = F (u) + (cid:104)p, Au − b(cid:105),

(6)

and a saddle point (u∗, p∗) ∈ U × Rm is such that

∀u ∈ U, p ∈ Rm : L(u∗, p) ≤ L(u∗, p∗) ≤ L(u, p∗).

(7)
From Assumption 1, there exist saddle points of L on U ×
Rm, and we denote the set of saddle points as U∗ × P∗. By
deﬁnition, saddle point (u, p) ∈ U∗ × P∗ of L satisﬁes
(cid:26) 0 ∈ ∂uL(u, p) + NU(u);

(8)

0 = −∇pL(u, p).

System (8) can also be considered the Karush-Kuhn-Tucker
(KKT) system of (P). Thus, the saddle point problem of (P)

Linear Convergence of RPDC method for Large-scale LCCP

can be represented as the inclusion problem

0 ∈ H(w) =

(cid:18) ∂uL(u, p) + NU(u)
−∇pL(u, p)

(cid:19)

=

(cid:18) ∇G(u) + ∂J(u) + A(cid:62)p + NU(u)
b − Au

(cid:19)

,

where, we call H the KKT mapping for obvious reasons.

3. Convergence and convergence rate analysis

of RPDC

This section establishes almost surely convergence and ex-
pected O(1/t) convergence rate for RPDC. First, we in-
troduce the following assumption on core function K and
parameters (cid:15) and ρ:

Assumption 2

(i) K is strongly convex with parameter
β and differentiable with its gradient Lipschitz contin-
uous with parameter B on U.

(ii) Parameters (cid:15) and ρ satisfy:

0 < (cid:15) < β/[BG+γλmax(A(cid:62)A)]and 0 < ρ <

2γ
2N − 1
(9)
where λmax(A(cid:62)A) is the largest eigenvalue of A(cid:62)A.

,

Let D(u, v) = K(u) − K(v) − (cid:104)∇K(v), u − v(cid:105) be a Breg-
man like function (core function) (Beck & Teboulle, 2003;
Cohen & Zhu, 1984). Two popular core functions K satisfy
Assumption 2:

1. K(u) = 1

2. K(u) = 1

2 (cid:107)u(cid:107)2, where β = B = 1, and
2 (cid:107)u(cid:107)2

Q, where Q is the Q-quadratic norm

associated with positive deﬁnite matrix Q.

2 (cid:107)u − v(cid:107)2 ≤ D(u, v) ≤ B

From Assumption 2, β
2 (cid:107)u − v(cid:107)2
and Algorithm 1 shows the proposed RPDC method to solve
(P). For the sake of brevity, let us set qk = pk + γ(Auk − b).
Then the primal problem can be expressed as

(APk) min
u∈U

(cid:104)∇i(k)G(uk), ui(k)(cid:105) + Ji(k)(ui(k))
+(cid:104)qk, Ai(k)ui(k)(cid:105) + 1

(cid:15) [K(u) − (cid:104)∇K(uk), u(cid:105)].

If we choose an additive Bregman function (or core func-
tion) with respect to the space decomposition (2), i.e.,

K(u) =

N
(cid:80)
i=1

Ki(ui), then problem (APk) is just a small

optimization problem for selected block i(k). Thus, tak-

ing K(u) =

N
(cid:80)
i=1

(cid:107)u(cid:107)2
2

for (APk), we perform only a block

proximal gradient update for block i(k), where we linearize
the coupled function G(u) and augmented Lagrangian term

Algorithm 1 Proposed randomized primal-dual coordinate
method

for k = 1 to t do

Choose i(k) from {1, . . . , N } with equal probability;
uk+1 = arg min
u∈U

(cid:104)∇i(k)G(uk), ui(k)(cid:105) + Ji(k)(ui(k))
(cid:15) D(u, uk);

+(cid:104)qk, Ai(k)ui(k)(cid:105) + 1

pk+1 = pk + ρ(Auk+1 − b).

end for

(cid:104)p, Au − b(cid:105) + γ
2 (cid:107)Au − b(cid:107)2, and add the proximal term to it.
Indices i(k), k = 0, 1, 2, . . . in Algorithm 1 are random
variables. After k iterations, RPDC generates random out-
put (uk+1, pk+1). We denote Fk as a ﬁltration generated by
the random variable i(0), i(1), . . . , i(k), i.e.,

Fk

def
= {i(0), i(1), . . . , i(k)}, Fk ⊂ Fk+1,

and deﬁne F = (Fk)k∈N, EFk+1 = E(·|Fk) as the condi-
tional expectation with respect to Fk. The conditional ex-
pectation in the i(k) term for given i(0), i(1), . . . , i(k − 1)
is Ei(k).
Let w = (u, p), given w∗ = (u∗, p∗) ∈ U∗ × P∗. Then for
any w, w(cid:48) ∈ U × Rm, we construct the function

Λ(w, w(cid:48)) = D(u(cid:48), u) +

(cid:15)
2N ρ

(cid:107)p − p(cid:48)(cid:107)2

+

+

(cid:15)(N − 1)
N
(cid:15)(N − 2)γ
2N

[L(u, p) − L(u∗, p∗)]

(cid:107)Au − b(cid:107)2

(10)

and we have the following lemma regarding the boundness
of Λ(w, w∗) and Λ(w, w(cid:48)).

Lemma 1 (Boundness of Λ(w, w∗) and Λ(w, w(cid:48)))
Suppose Assumption 1 and 2 hold. w∗ = (u∗, p∗) ∈
U∗ × P∗. Then there exist positive numbers d1, d2 and d3,
such that

(i) Λ(w, w∗) ≥ d1(cid:107)w − w∗(cid:107)2,

(ii) Λ(w, w∗) ≤ d2(cid:107)w − w∗(cid:107)2 + (cid:15)(N −1)

N [L(u, p∗) −

L(u∗, p∗)], and

(iii) Λ(w, w(cid:48)) ≥ −d3(cid:107)p − p∗(cid:107)2;

(cid:26)

with d1 = min

1

2N [N β − (cid:15)γλmax(A(cid:62)A)],
(4N −2)N ρ , N B+(cid:15)(2N −3)γλmax(A(cid:62)A)

(4N −3)(cid:15)

(cid:15)
4N γ

2N

(cid:26)

(cid:27)

,

(cid:27)

,

d2 = max
and d3 = (cid:15)(N −1)2

2γN (N −2) .

Linear Convergence of RPDC method for Large-scale LCCP

To analyze the convergence of RPDC, we need the point
T (wk) = (cid:0)Tu(wk), Tp(wk)(cid:1) generated by one determinis-
tic iteration of APP-AL for given wk,

APP-AL



Tu(wk) = arg min
u∈U

(cid:104)∇G(uk), u(cid:105) + J(u) + (cid:104)qk, Au(cid:105)
(cid:15) D(u, uk);

+ 1



Tp(wk) = pk + γ (cid:2)ATu(wk) − b(cid:3) ,

and the following lemma.

Lemma 2 (Estimation on the variance of Λ(wk, w)) Let
Assumption 1 and 2 hold and {(uk, pk)} be generated by
RPDC. Then

Λ(wk, w) − Ei(k)Λ(wk+1, w)
(cid:15)
N

Ei(k)

(cid:2)L(uk+1, p) − L(u, qk)(cid:3) + d4(cid:107)wk − T (wk)(cid:107)2,

≥

(cid:26)

min

β−(cid:15)[BG+γλmax(A(cid:62)A)]
2

, (cid:15)[2γ−(2N −1)ρ]
2N

where d4 =

max{N 2+2γ2(N 2+2)λmax(A(cid:62)A),4γ2}

(cid:27)

.

3.1. Almost surely convergence of RPDC

Based Lemma 2, we establish the convergence of RPDC.

Theorem 1 (Almost surely convergence) Let the assump-
tions of Lemma 2 hold, then

(i)

+∞
(cid:80)
k=0

(cid:107)wk − T (wk)(cid:107)2 < +∞ a.s..

(ii) The sequence {wk} generated by RPDC is almost

surely bounded.

(iii) Every cluster point of {wk} almost surely is a saddle

point of the Lagrangian for (P).

3.2. Convergence rate analysis for RPDC

This subsection provides the convergence rate of RPDC. We
deﬁne the average sequence for the sequence {(uk, pk)}
generated from Algorithm RPDC and any t > 0 as

¯ut =

(cid:80)t

k=0 uk+1
t + 1

and ¯pt =

(cid:80)t

k=0 qk
t + 1

.

Theorem 2 (Expected primal suboptimality and ex-
pected feasibility)
Let Assumption 1 and 2 hold; (u∗, p∗) be a saddle point,
and M be a bound of dual optimal solution of (P), i.e.,
(cid:107)p∗(cid:107) < M ; and {(uk, pk)} be generated by RPDC. Then
we have the following identities.

(i) Global estimate of expected bifunction values.

EFt

(cid:2)L(¯ut, p) − L(u, ¯pt)(cid:3) ≤

N h(w0, w)
(cid:15)(t + 1)

,

∀u ∈ U, p ∈ Rm, (u, p) could possibly be random
and h(w, w(cid:48)) = Λ(w, w(cid:48)) + d3
d1

Λ(w, w∗) ≥ 0.

(ii) Expected feasibility.

EFt(cid:107)A¯ut − b(cid:107) ≤

N d5
(M − (cid:107)p∗(cid:107))(cid:15)(t + 1)

,

where d5 = sup
(cid:107)p(cid:107)<M

h(w0, (u∗, p)).

(iii) Expected suboptimality.

EFt [F (¯ut) − F (u∗)] ≥ −

EFt [F (¯ut) − F (u∗)] ≤

(cid:107)p∗(cid:107)N d5
(M − (cid:107)p∗(cid:107))(cid:15)(t + 1)
N d5
(cid:15)(t + 1)

.

,

4. Linear convergence of RPDC under global

strong metric subregularity

This section establishes the expected linear convergence
of RPDC. Let φ(w, w∗) = Λ(w, w∗) + (cid:15)
N [L(u, p∗) −
L(u∗, p∗)]. Then φ(w, w∗) ≥ 0 and φ(w, w∗) = 0 if and
only if w = w∗, and following lemma holds for the upper
bound and descent property of function φ(w, w∗).

Lemma 3 (Boundness of φ(w, w∗) and descent inequality)
Suppose Assumption 1 and 2 hold, then the following
statements also hold.

(i) φ(w, w∗) ≥ d1(cid:107)w − w∗(cid:107)2.

(ii) φ(w, w∗) ≤ d2(cid:107)w − w∗(cid:107)2 + (cid:15)[L(u, p∗) − L(u∗, p∗)].

(iii) φ(wk, w∗)−Ei(k)φ(wk+1, w∗) ≥ d4(cid:107)wk−T (wk)(cid:107)2+

(cid:15)

N [L(uk, p∗) − L(u∗, p∗)].

The deﬁnition for global strong metric subregular-
ity (Dontchev & Rockafellar, 2009) is as follows.

Deﬁnition 1 (Global strong metric subregularity) Let
H(x) be a set-valued mapping between real spaces X and
Y. Then H(x) is called global strong metric subregular
at ¯x for ¯y when ¯y ∈ H(¯x) if there exists positive number c
such that

dist(x, ¯x) ≤ cdist (¯y, H(x)) ,

for all x ∈ X.

(11)

The following theorem regarding linear convergence of
RPDC under global strong metric subregularity of the KKT
mapping H : Rn × Rm ⇒ Rn × Rm.

Linear Convergence of RPDC method for Large-scale LCCP

Theorem 3 (Global strong metric subregularity of
H(w) implies linear convergence of RPDC) Suppose As-
sumption 1 and 2 hold. For a given saddle point w∗, if H(w)
is global strong metric subregular at w∗ for 0, then there
exists α ∈ (0, 1) such that

EFk+1φ(wk+1, w∗) ≤ αk+1φ(w0, w∗),

∀k.

(12)

The RPDC scheme with K(u) = 1

2 (cid:107)u(cid:107)2 for SVM is

Choose i(k) from {1, 2, . . . , N } with equal probability
(cid:104)Qi(k)uk, ui(k)(cid:105) − 1(cid:62)
uk+1 ← min

ui(k)

ni(k)

u∈[0,c]n

+(cid:104)pk + γy(cid:62)uk, (yi(k))(cid:62)ui(k)(cid:105) +

1
2(cid:15)

(cid:107)u − uk(cid:107)2;

pk+1 ← pk + ρy(cid:62)uk+1.

Then the R-linear of the sequence {EFk wk} can be ex-
pressed as in the corollary.

Corollary 1 (R-linear rate of {EFk wk}) Suppose the as-
sumptions of Theorem 3 hold and α ∈ (0, 1) is constant.
Then the sequence {EFk wk} converges to the desired saddle
point w∗ at R-linear rate; i.e.,

Thus, the primal subproblem of RPDC has the closed form






uk+1
i(k) = min

(cid:26)

(cid:20)
0, uk

i(k) − (cid:15)

(cid:18)

max

Qi(k)uk − 1ni(k)

+(pk + γy(cid:62)uk)yi(k)

(cid:19)(cid:21)

(cid:27)

, c

,

uk+1
j(cid:54)=i(k) = uk

j(cid:54)=i(k).

(cid:113)
sup k

(cid:107)EFk wk − w∗(cid:107) =

√

α.

lim
k→∞

5. Support vector machine and machine

learning portfolio problem
implementations

This section discusses experiments conducted using MAT-
LAB R2020a on a personal computer with Intel Core i5-
6200U CPU (2.40GHz) and 8.00 GB RAM. We also cal-
culate optimal values for all experiments to check the sub-
optimality of RPDC using the commercial solver CPLEX
12.6.

5.1. Support vector machine problem

Consider the SVM problem,

(SVM) min

u∈[0,c]n
s.t.

1

2 u(cid:62)Qu − 1(cid:62)
n u
y(cid:62)u = 0

,

where u ∈ Rn are the decision variables, and Q ∈ Rn×n
is a symmetric and positive-deﬁnite matrix. Let Q =
N )(cid:62) ∈ Rn×n be an appropriate partition
(Q(cid:62)
of matrix Q and Qi be an ni × n matrix. Then the KKT
mapping for SVM is

2 , · · · , Q(cid:62)

1 , Q(cid:62)

H(w) =

(cid:18) Qu − 1n + py + N[0,c]n (u)

y(cid:62)u

(cid:19)

,

We used two LIBSVM datasets in the experiment:
heart scale (270 data
and
ionosphere scale (351 data and 34 features).
Q was generated using the radial basis function kernel, and
we selected c = 1.

features)

and 13

We partitioned the variables N = 2, 5, 10 blocks,
Thus ni = 135, 54, 27 for
for both cases.
=
the ﬁrst
175
(or 36) for the second
dataset (ionosphere scale).

(heart scale);

(or 176), 70

(or 71), 35

and ni

dataset

In Figure 1, graphs (a-1) and (a-2) show the number of
blocks and (cid:107)wk−w∗(cid:107) with respect to iteration count, respec-
tively; graphs (b-1) and (b-2) show the number of blocks and
suboptimality with respect to iteration count, respectively;
and graphs (c-1) and (c-2) show the number of blocks and
feasibility with respect to iteration count, respectively.

We compared three algorithms: APP-AL by (Cohen & Zhu,
1984) and (Zhao & Zhu, 2019), and RPDC from this paper
with N = 2 and random coordinate descent (RCD) algo-
rithm (Necoara & Patrascu, 2014)) on heart scale and
ionosphere scale problems. Suboptimality and fea-
sibility were measured by |F (u) − F (u∗)| + (cid:107)y(cid:62)u(cid:107) with
F (u) = 1
n u. In Figure 2, graphs (a-1) and (b-1)
show |F (u) − F (u∗)| + (cid:107)y(cid:62)u(cid:107) versus iteration count; and
graphs (a-2) and (b-2) show average computation time per
iteration for the different algorithms. The total number of
iterations required for APP-AL and RPDC are both less than
RCD, APP-AL is faster than RPDC. But computation per
iteration of RPDC is less than APP-AL.

2 u(cid:62)Qu − 1(cid:62)

where w = (u, p). The following proposition shows that the
KKT mapping for SVM is global strong metric subregular.

5.2. Machine learning portfolio problem

Consider the MLP problem.

Proposition 1 Assume there exists at least one component
u∗
i of optimal solution u∗ that satisﬁes 0 < u∗
i < c. Then the
KKT mapping for SVM is global strong metric subregular.

(MLP) min
u∈Rn
s.t.

1

2 u(cid:62)Σu + λ(cid:107)u(cid:107)1
µ(cid:62)u = ρ
1(cid:62)
n u = 1

,

Linear Convergence of RPDC method for Large-scale LCCP

(a) (cid:107)wk − w∗(cid:107)

(b) Suboptimality

(c) Feasibility

Figure 1. Number of blocks, (cid:107)wk − w∗(cid:107), suboptimality, and feasibility with respect to iteration

(a) Comparison for heart scale

(b) Comparison for ionosphere scale

Figure 2. Comparing RPDC, APP-AL and RCD

Linear Convergence of RPDC method for Large-scale LCCP

where u ∈ Rn is the decision portfolio vector, and
Σ ∈ Rn×n, is the symmetric and positive-deﬁnite es-
timated covariance matrix of asset returns. Let Σ =
(Σ(cid:62)
N )(cid:62) ∈ Rn×n be an appropriate partition
of matrix Σ and Σi be an ni × n matrix.

2 , · · · , Σ(cid:62)

1 , Σ(cid:62)

The KKT mapping for MLP is:





H(w) =

Σu + λ∂(cid:107)u(cid:107)1 + p11n + p2µ
µ(cid:62)u − ρ
1(cid:62)
n u − 1



 ,

where w = (u, p). The following proposition shows that the
KKT mapping for MLP is global strong metric subregular.

Proposition 2 Assume there exists at least two components
u∗
j for optimal solution u∗ that satisfy u∗
i and u∗
i (cid:54)= 0 and
u∗
j (cid:54)= 0; and µi (cid:54)= µj. Then the KKT mapping for MLP is
global strong metric subregular.

Therefore, the RPDC scheme with K(u) = 1
is

2 (cid:107)u(cid:107)2 for MLP

Choose i(k) from {1, 2, . . . , N } with equal probability
(cid:104)Σi(k)uk, ui(k)(cid:105) + λ(cid:107)ui(k)(cid:107)1
uk+1 ← min
u∈Rn

+(cid:104)pk + γΘ(uk), Θi(k)(ui(k))(cid:105) +

1
2(cid:15)

(cid:107)u − uk(cid:107)2;

pk+1 ← pk + ρΘ(uk+1),

221, 88 (or 90), 44 (or 46) respectively.
In Figure 1, graphs (a-3) and (a-4) show the number of
blocks and (cid:107)wk −w∗(cid:107) with respect to iteration count; graphs
(b-3) and (b-4) show the number of blocks and suboptimality
with respect to iteration; and graphs (c-3) and (c-4) show the
number of blocks and feasibility with respect to iteration.

6. Conclusions

This paper proposed a randomized primal-dual coordinate
(RPDC) method, a randomized coordinate extension of the
ﬁrst-order primal-dual method proposed by (Cohen & Zhu,
1984) and (Zhao & Zhu, 2019), to solve LCCP. We es-
tablished almost surely convergence and expected O(1/t)
convergence rate for the general convex case, and expected
linear convergence under global strong metric subregularity.
We showed that SVM and MLP problems satisfy global
strong metric subregularity under some reasonable condi-
tions, discussed the implementation details of RPDC, and
presented numerical experiments on SVM and MLP prob-
lems to verify linear convergence. Future study will con-
sider RPDC for nonlinear convex cone programming with
separable and non-separable objective and constraints.

Acknowledgements

This research was supported by NSFC grant 71471112 and
71871140.

where Θ(u) =
(cid:33)
(cid:32)

µ(cid:62)
i(k)ui(k)
1(cid:62)
ui(k)
ni(k)

(cid:19)

(cid:18) µ(cid:62)u − ρ
1(cid:62)
n u − 1

and Θi(k)(ui(k)) =

. Thus, the primal subproblem of RPDC

has the closed form

(cid:40)

uk+1
i(k) = sign(ζ k
uk+1
j(cid:54)=i(k) = uk

j(cid:54)=i(k),

i(k)) (cid:12) max{0, |ζ k

i(k)| − (cid:15)λ1ni(k) },

where
i(k) = uk
ζ k
Two datasets were chosen to validate RPDC performance.

i(k)−(cid:15) (cid:2)Σi(k)uk + (cid:0)µi(k), 1ni(k)

(cid:1) (pk + γΘ(uk))(cid:3).

1. The FF100 dataset from Fama and French benchmark
datasets (Fama & French, 1992), created for different
ﬁnancial segments based on data sampled from the U.S.
stock market. FF100 formed on the basis of size and
book-to-market ratio; and

2. The Standard & Poor’s, USA SP500 dataset, November
2004 to April 2016, containing 442 assets and 595
observations.

We partitioned the variables into N = 2, 5, 10 blocks
i.e., ni = 50, 20, 10 and ni =
for both cases,

Supplementary material for the paper: ”Linear Convergence of Randomized
Primal-Dual Coordinate Method for Large-scale Linear Constrained Convex
Programming”

First of all, we have the following observations:

In algorithm RPDC, the indices i(k), k = 0, 1, 2, . . . are random variables. After k iterations, RPDC method gener-
ates a random output (uk+1, pk+1). Recall the deﬁnition of ﬁltration Fk which is generated by the random variable
i(0), i(1), . . . , i(k), i.e.,

Fk

def
= {i(0), i(1), . . . , i(k)}, Fk ⊂ Fk+1.

Additionally, F = (Fk)k∈N, EFk+1 = E(·|Fk) is the conditional expectation w.r.t. Fk and the conditional expectation in
term of i(k) given i(0), i(1), . . . , i(k − 1) as Ei(k).

Knowing Fk−1 = {i(0), i(1), . . . , i(k − 1)}, we have:

Ei(k)(cid:104)∇i(k)G(uk), (uk − u)i(k)(cid:105) =

1
N

(cid:104)∇G(uk), uk − u(cid:105) ≥

(cid:2)G(uk) − G(u)(cid:3),

1
N

and

Ei(k)

(cid:2)Ji(k)(uk

i(k)) − Ji(k)(ui(k))(cid:3) =

(cid:2)J(uk) − J(u)(cid:3),

1
N

Ei(k)(cid:104)qk, Ai(k)(uk − u)i(k)(cid:105) =

1
N

(cid:104)qk, A(uk − u)(cid:105).

(13)

(14)

(15)

Secondly, reconsidering the point T (wk) = (cid:0)Tu(wk), Tp(wk)(cid:1) generated by one deterministic iteration of APP-AL (Cohen
& Zhu, 1984) for given wk,

APP-AL
(cid:40) Tu(wk) = arg min

u∈U

Tp(wk) = pk + γ (cid:2)ATu(wk) − b(cid:3) ,

(cid:104)∇G(uk), u(cid:105) + J(u) + (cid:104)qk, Au(cid:105) + 1

(cid:15) D(u, uk);

with qk = pk + γ(Auk − b), we have the following observations. The convex combination of uk and Tu(wk) provides the
expected value of uk+1 as following.

or

Ei(k)uk+1 =

1
N

Tu(wk) + (1 −

1
N

)uk,

Tu(wk) = N Ei(k)uk+1 − (N − 1)uk.

Moreover, the point T (wk) satisﬁes that: for any (u, p) ∈ U × Rm,






(cid:104)∇G(uk), u − Tu(wk)(cid:105) + J(u) − J(Tu(wk)) + (cid:104)qk, A(u − Tu(wk))(cid:105)

+ 1

(cid:15) (cid:104)∇K(Tu(wk)) − ∇K(uk), u − Tu(wk)(cid:105) ≥ 0,

γ (cid:2)ATu(wk) − b(cid:3) = Tp(wk) − pk.

(16)

(17)

(18)

Linear Convergence of RPDC method for Large-scale LCCP

7. Proof of Lemma 1

Proof. Take w(cid:48) = w∗ in (9), we have that

Λ(w, w∗) = D(u∗, u) +

= D(u∗, u) +

(cid:15)
2N ρ
(cid:15)
2N ρ

(cid:107)p − p∗(cid:107)2 +

(cid:107)p − p∗(cid:107)2 +

(cid:15)(N − 1)
N
(cid:15)(N − 1)
N

+

(cid:15)(N − 2)γ
2N

(cid:107)Au − b(cid:107)2.

[L(u, p) − L(u∗, p∗)] +

[L(u, p∗) − L(u∗, p∗)] +

(cid:15)(N − 2)γ
2N
(cid:15)(N − 1)
N

(cid:107)Au − b(cid:107)2

(cid:104)p − p∗, Au − b(cid:105)

(19)

(i) Since L(u, p∗) − L(u∗, p∗) ≥ 0 and 1

2γ (cid:107)p − p∗(cid:107)2 + γ

2 (cid:107)Au − b(cid:107)2 + (cid:104)p − p∗, Au − b(cid:105) ≥ 0, (19) follows that

Λ(w, w∗) ≥ D(u∗, u) +

(cid:15)
2N ρ

(cid:107)p − p∗(cid:107)2 −

(cid:15)(N − 1)
2N γ

(cid:107)p − p∗(cid:107)2 −

(cid:15)γ
2N

(cid:107)Au − b(cid:107)2.

From Assumption 2, we have D(u∗, u) ≥ β
inequality follows that

2 (cid:107)u − u∗(cid:107)2. Together with the fact Au∗ = b and ρ < 2γ

2N −1 , above

(cid:26)

with d1 = min

1

2N [N β − (cid:15)γλmax(A(cid:62)A)],

(cid:15)
4N γ

(cid:27)

.

(ii) By Young’s inequality, (19) follows that

Λ(w, w∗) ≥ d1(cid:107)w − w∗(cid:107)2,

Λ(w, w∗) ≤ D(u∗, u) +

+

(cid:15)(N − 1)
N

(cid:15)
2N ρ
1
2γ

[

(cid:107)p − p∗(cid:107)2 +

(cid:15)(N − 1)
N

[L(u, p∗) − L(u∗, p∗)]

(cid:107)p − p∗(cid:107)2 +

γ
2

(cid:107)Au − b(cid:107)2] +

(cid:15)(N − 2)γ
2N

(cid:107)Au − b(cid:107)2.

From Assumption 2, we have D(u∗, u) ≤ B
inequality follows that

2 (cid:107)u − u∗(cid:107)2. Together with the fact Au∗ = b and 2γ > (2N − 1)ρ, above

Λ(w, w∗) ≤ d2(cid:107)w − w∗(cid:107)2 +

with d2 = max

(cid:26)

(4N −3)(cid:15)

(4N −2)N ρ , N B+(cid:15)(2N −3)γλmax(A(cid:62)A)

2N

[L(u, p∗) − L(u∗, p∗)],

(cid:15)(N − 1)
N

(cid:27)

.

(iii) By the deﬁnition of Λ(w, w(cid:48)), we have

Λ(w, w(cid:48)) ≥

[L(u, p) − L(u∗, p∗)] +

(cid:15)(N − 2)γ
2N

(cid:107)Au − b(cid:107)2

=

(cid:15)(N − 1)
N
(cid:15)(N − 1)
N
(cid:15)(N − 1)
N
(cid:15)(N − 1)
N
≥ −d3(cid:107)p − p∗(cid:107)2,

=

≥

[L(u, p) − L(u, p∗)] +

[L(u, p∗) − L(u∗, p∗)] +

[L(u, p) − L(u, p∗)] +

(cid:107)Au − b(cid:107)2

(cid:104)p − p∗, Au − b(cid:105) +

(cid:107)Au − b(cid:107)2

(cid:15)(N − 1)
N
(cid:15)(N − 2)γ
2N
(cid:15)(N − 2)γ
2N

with d3 = (cid:15)(N −1)2

2γN (N −2) .

(cid:15)(N − 2)γ
2N

(cid:107)Au − b(cid:107)2

(20)

(cid:3)

Linear Convergence of RPDC method for Large-scale LCCP

8. Proof of Lemma 2

Proof. Step 1: Estimate (cid:15)
N
For all u ∈ U, the unique solution uk+1 of the primal problem of RPDC is characterized by the following variational
inequality:

(cid:2)L(uk+1, qk) − L(u, qk)(cid:3);

Ei(k)

(cid:104)∇i(k)G(uk), (uk+1 − u)i(k)(cid:105) + Ji(k)(uk+1

i(k) ) − Ji(k)(ui(k)) + (cid:104)qk, Ai(k)(uk+1 − u)i(k)(cid:105)

+

1
(cid:15)

(cid:104)∇K(uk+1) − ∇K(uk), uk+1 − u(cid:105) ≤ 0,

which follows that

(cid:104)∇i(k)G(uk), (cid:0)uk − u − (uk − uk+1)(cid:1)

i(k)(cid:105) + Ji(k)(uk
(cid:0)uk − u − (uk − uk+1)(cid:1)

i(k)) − Ji(k)(ui(k)) − (cid:0)Ji(k)(uk

i(k) )(cid:1)
(cid:104)∇K(uk+1) − ∇K(uk), uk+1 − u(cid:105) ≤ 0.

i(k)) − Ji(k)(uk+1

i(k)(cid:105) +

1
(cid:15)

+(cid:104)qk, Ai(k)

(21)

Observing that for any separable mapping ψ(u) =

Therefore, (21) follows that

N
(cid:80)
i=1

ψi(ui), we have ψi(k)(uk

i(k)) − ψi(k)(uk+1

i(k) ) = ψ(uk) − ψ(uk+1).

(cid:104)∇i(k)G(uk), (uk − u)i(k)(cid:105) + Ji(k)(uk

i(k)) − Ji(k)(ui(k)) + (cid:104)qk, Ai(k)(uk − u)i(k)(cid:105)

≤ (cid:104)∇G(uk), uk − uk+1(cid:105) + J(uk) − J(uk+1) + (cid:104)qk, A(uk − uk+1)(cid:105)

+

1
(cid:15)

(cid:104)∇K(uk+1) − ∇K(uk), u − uk+1(cid:105).

(22)

Taking expectation with respect to i(k) on both side of (22), together the condition expectation (13)-(15), we get

(cid:2)L(uk, qk) − L(u, qk)(cid:3) ≤ Ei(k)

(cid:26)

1
N

(cid:104)∇G(uk), uk − uk+1(cid:105) + J(uk) − J(uk+1)

+(cid:104)qk, A(uk − uk+1)(cid:105) +

1
(cid:15)

(cid:104)∇K(uk+1) − ∇K(uk), u − uk+1(cid:105)

(cid:27)

.

(23)

or

1
N

Ei(k)

(cid:2)L(uk+1, qk) − L(u, qk)(cid:3) ≤ Ei(k)

(cid:26)

(cid:104)∇G(uk), uk − uk+1(cid:105)
(cid:123)(cid:122)
(cid:125)
(cid:124)
a1

+J(uk) − J(uk+1)

+(cid:104)qk, A(uk − uk+1)(cid:105) +

1
N

(cid:2)L(uk+1, qk) − L(uk, qk)(cid:3)
(cid:27)

+

1
(cid:15)
(cid:124)

(cid:104)∇K(uk+1) − ∇K(uk), u − uk+1(cid:105)
(cid:123)(cid:122)
(cid:125)
a2

.

(24)

(25)

By the gradient Lipschitz of G, term a1 in (24) is bounded by

a1 = (cid:104)∇G(uk), uk − uk+1(cid:105) ≤ G(uk) − G(uk+1) +

BG
2

(cid:107)uk − uk+1(cid:107)2.

The simple algebraic operation and Assumption 2 follows that

a2 =

1
(cid:15)

(cid:104)∇K(uk+1) − ∇K(uk), u − uk+1(cid:105) =

≤

1
(cid:15)
1
(cid:15)

(cid:2)D(u, uk) − D(u, uk+1) − D(uk+1, uk)(cid:3)

(cid:2)D(u, uk) − D(u, uk+1)(cid:3) −

β
2(cid:15)

(cid:107)uk − uk+1(cid:107)2.

(26)

Combining (24)-(26), we obtain that

(cid:15)
N

Ei(k)

(cid:2)L(uk+1, qk) − L(u, qk)(cid:3) ≤ (cid:2)D(u, uk) − Ei(k)D(u, uk+1)(cid:3) + Ei(k)

−

β − (cid:15)BG
2

(cid:107)uk − uk+1(cid:107)2

(cid:27)

(cid:26) (cid:15)(N − 1)
N

(cid:2)L(uk, qk) − L(uk+1, qk)(cid:3)
(cid:124)
(cid:125)
(cid:123)(cid:122)
a3

(27)

Linear Convergence of RPDC method for Large-scale LCCP

Since pk+1 = pk + ρ(Auk+1 − b) and qk = pk + γ(Auk − b), term a3 in (27) follows that

a3 = L(uk, qk) − L(uk+1, qk)

= L(uk, pk) − L(uk+1, pk+1) + (cid:104)qk − pk, Auk − b(cid:105) + (cid:104)pk+1 − qk, Auk+1 − b(cid:105)
= L(uk, pk) − L(uk+1, pk+1) + γ(cid:107)Auk − b(cid:107)2 + ρ(cid:107)Auk+1 − b(cid:107)2 − γ(cid:104)Auk − b, Auk+1 − b(cid:105)

= L(uk, pk) − L(uk+1, pk+1) +

≤ L(uk, pk) − L(uk+1, pk+1) +

γ
2
γ
2

(cid:107)Auk − b(cid:107)2 + (ρ −

(cid:107)Auk − b(cid:107)2 + (ρ −

)(cid:107)Auk+1 − b(cid:107)2 +

)(cid:107)Auk+1 − b(cid:107)2

γ
2

γ
2
γ
2

(cid:107)A(uk − uk+1)(cid:107)2

+

γλmax(A(cid:62)A)
2

(cid:107)uk − uk+1(cid:107)2.

Combining (27)-(28), we have that

(28)

(cid:15)
N

Ei(k)

(cid:2)L(uk+1, qk) − L(u, qk)(cid:3) ≤ (cid:2)D(u, uk) − Ei(k)D(u, uk+1)(cid:3) + Ei(k)

(cid:26) (cid:15)(N − 1)
N

(cid:2)L(uk, pk) − L(uk+1, pk+1)(cid:3)

−

+

β − (cid:15)[BG + N −1
N γλmax(A(cid:62)A)]
2

(cid:15)(2ρ − γ)(N − 1)
2N

(cid:107)Auk+1 − b(cid:107)2

(cid:107)uk − uk+1(cid:107)2 +
(cid:27)

(cid:15)γ(N − 1)
2N

(cid:107)Auk − b(cid:107)2

(29)

Step 2: Estimate (cid:15)
N

Ei(k)

(cid:2)L(uk+1, p) − L(uk+1, qk)(cid:3)

L(uk+1, p) − L(uk+1, qk) = (cid:104)p − qk, Auk+1 − b(cid:105)

(cid:104)p − pk, pk+1 − pk(cid:105) − γ(cid:104)Auk − b, Auk+1 − b(cid:105)

(cid:2)(cid:107)p − pk(cid:107)2 − (cid:107)p − pk+1(cid:107)2 + (cid:107)pk − pk+1(cid:107)2(cid:3) − γ(cid:104)Auk − b, Auk+1 − b(cid:105)

(cid:2)(cid:107)p − pk(cid:107)2 − (cid:107)p − pk+1(cid:107)2 + (cid:107)pk − pk+1(cid:107)2(cid:3) +

γ
2

(cid:107)A(uk − uk+1)(cid:107)2

=

=

=

=

1
ρ
1
2ρ
1
2ρ
γ
γ
2
2
(cid:2)(cid:107)p − pk(cid:107)2 − (cid:107)p − pk+1(cid:107)2(cid:3) +

1
2ρ
γ
2
(cid:2)(cid:107)p − pk(cid:107)2 − (cid:107)p − pk+1(cid:107)2(cid:3) +

(cid:107)Auk − b(cid:107)2 −

(cid:107)Auk − b(cid:107)2 +

ρ − γ
2

−

−

(cid:107)Auk+1 − b(cid:107)2

(cid:107)Auk+1 − b(cid:107)2

(since pk+1 = pk + ρ(Auk+1 − b).)

γ
2

(cid:107)A(uk − uk+1)(cid:107)2

γλmax(A(cid:62)A)
2

(cid:107)uk − uk+1(cid:107)2

≤

1
2ρ
γ
2
N on both side of above inequality, we obtain that: ∀p ∈ Rm

(cid:107)Auk − b(cid:107)2 +

ρ − γ
2

−

(cid:107)Auk+1 − b(cid:107)2

Multiply (cid:15)

(cid:2)L(uk+1, p) − L(uk+1, qk)(cid:3) ≤

(cid:15)
N

(cid:2)(cid:107)p − pk(cid:107)2 − (cid:107)p − pk+1(cid:107)2(cid:3) +

(cid:15) 1
N γλmax(A(cid:62)A)
2

(cid:107)uk − uk+1(cid:107)2

(cid:107)Auk − b(cid:107)2 +

(cid:15)(ρ − γ)
2N

(cid:107)Auk+1 − b(cid:107)2.

Taking expectation with respect to i(k) on both side of inequality (31), we have

(cid:15)
N

Ei(k)

(cid:2)L(uk+1, p) − L(uk+1, qk)(cid:3) ≤

(cid:2)(cid:107)p − pk(cid:107)2 − Ei(k)(cid:107)p − pk+1(cid:107)2(cid:3) +

(cid:15) 1
N γλmax(A(cid:62)A)
2

Ei(k)(cid:107)uk − uk+1(cid:107)2

(cid:107)Auk − b(cid:107)2 +

(cid:15)(ρ − γ)
2N

Ei(k)(cid:107)Auk+1 − b(cid:107)2.

(32)

(cid:15)
2N ρ
(cid:15)γ
2N

−

(cid:15)
2N ρ
(cid:15)γ
2N

−

(30)

(31)

Linear Convergence of RPDC method for Large-scale LCCP

Step 3: Estimate the variance of Λ(wk, w).

(cid:26)

min

β−(cid:15)[BG+γλmax(A(cid:62)A)]
2

, (cid:15)[2γ−(2N −1)ρ]
2N

(cid:27)

Summing inequalities (29) and (32), with d4 =

max{N 2+2γ2(N 2+2)λmax(A(cid:62)A),4γ2}

, we have that

Λ(wk, w) − Ei(k)Λ(wk+1, w)

≥ Ei(k)

(cid:2)L(uk+1, p) − L(u, qk)(cid:3) + d4[(cid:0)N 2 + 2γ2(N 2 + 2)λmax(A(cid:62)A)(cid:1) (cid:107)uk − uk+1(cid:107)2 + 4γ2(cid:107)Auk+1 − b(cid:107)2]

(cid:27)

β − (cid:15)[BG + γλmax(A(cid:62)A)]
2

(cid:107)uk − uk+1(cid:107)2 +

(cid:15)[2γ − (2N − 1)ρ]
2N

(cid:107)Auk+1 − b(cid:107)2

(cid:27)

(cid:2)L(uk+1, p) − L(u, qk)(cid:3) +

(cid:26) (cid:15)
N
(cid:26) (cid:15)
N
(cid:26) (cid:15)
N
(cid:26) (cid:15)
N
(cid:2)L(uk+1, p) − L(u, qk)(cid:3) + d4
Ei(k)

(cid:2)L(uk+1, p) − L(u, qk)(cid:3) + d4

(cid:2)L(uk+1, p) − L(u, qk)(cid:3) + d4

≥ Ei(k)

≥ Ei(k)

≥ Ei(k)
(cid:15)
N

=

(cid:2) (cid:0)1 + 2γ2λmax(A(cid:62)A)(cid:1) N 2(cid:107)uk − uk+1(cid:107)2 + 4γ2[(cid:107)A(uk − uk+1)(cid:107)2 + (cid:107)Auk+1 − b(cid:107)2](cid:3)

(cid:27)

(cid:2) (cid:0)1 + 2γ2λmax(A(cid:62)A)(cid:1) N 2(cid:107)uk − uk+1(cid:107)2 + 2γ2(cid:107)Auk − b(cid:107)2(cid:3)

(cid:27)

(cid:2) (cid:0)1 + 2γ2λmax(A(cid:62)A)(cid:1) N 2Ei(k)(cid:107)uk − uk+1(cid:107)2 + 2γ2(cid:107)Auk − b(cid:107)2(cid:3).

(33)

By Jensen’s inequality, (33) follows that

Λ(wk, w) − Ei(k)Λ(wk+1, w) ≥

(cid:15)
N
+d4

(cid:2)L(uk+1, p) − L(u, qk)(cid:3)

Ei(k)
(cid:2) (cid:0)1 + 2γ2λmax(A(cid:62)A)(cid:1) N 2(cid:107)uk − Ei(k)uk+1(cid:107)2 + 2γ2(cid:107)Auk − b(cid:107)2(cid:3). (34)

Since Ei(k)uk+1 − uk = 1

Λ(wk, w) − Ei(k)Λ(wk+1, w) ≥

N [Tu(wk) − uk] in (16), (34) yields that
(cid:15)
N
+d4

(cid:2)L(uk+1, p) − L(u, qk)(cid:3)

Ei(k)
(cid:2) (cid:0)1 + 2γ2λmax(A(cid:62)A)(cid:1) (cid:107)uk − Tu(wk)(cid:107)2 + 2γ2(cid:107)Auk − b(cid:107)2(cid:3).

(35)

Since λmax(A(cid:62)A)(cid:107)uk − Tu(wk)(cid:107)2 ≥ (cid:107)A[uk − Tu(wk)](cid:107)2 and Tp(wk) − pk = γ[ATu(wk) − b], (35) follows that

Λ(wk, w) − Ei(k)Λ(wk+1, w) ≥

≥

≥

(cid:15)
N
+d4
(cid:15)
N
(cid:15)
N

(cid:2)L(uk+1, p) − L(u, qk)(cid:3)

Ei(k)
(cid:2)(cid:107)uk − Tu(wk)(cid:107)2 + 2γ2(cid:107)A[uk − Tu(wk)](cid:107)2 + 2γ2(cid:107)Auk − b(cid:107)2(cid:3)
Ei(k)

(cid:2)L(uk+1, p) − L(u, qk)(cid:3) + d4
(cid:2)L(uk+1, p) − L(u, qk)(cid:3) + d4(cid:107)wk − T (wk)(cid:107)2.

Ei(k)

(cid:2)(cid:107)uk − Tu(wk)(cid:107)2 + γ2(cid:107)ATu(wk) − b(cid:107)2(cid:3)

Then we have the result of Lemma 2.

(cid:3)

Linear Convergence of RPDC method for Large-scale LCCP

9. Proof of Theorem 1 (Almost surely convergence)

Proof.

(i) Take w = w∗ in Lemma 2, we have

Λ(wk, w∗) ≥ Ei(k)Λ(wk+1, w∗) +

(cid:15)
N

Ei(k)

(cid:2)L(uk+1, p∗) − L(u∗, qk)(cid:3) + d4(cid:107)wk − T (wk)(cid:107)2.

(36)

Observe that L(uk+1, p∗) − L(u∗, qk) ≥ 0. From statement (i) of Lemma 1, we have that Λ(wk, w∗) is nonnegative.
Λ(wk, w∗) almost surely exists,
By the Robbins-Siegmund Lemma (Robbins & Siegmund, 1971), we obtain that

lim
k→+∞

+∞
(cid:80)
k=0

(cid:107)wk − T (wk)(cid:107)2 < +∞ a.s..

(ii) Since lim

k→+∞

Λ(wk, w∗) almost surely exists, thus Λ(wk, w∗) is almost surely bounded. Thanks statement (i) of Lemma

1, it implies the sequences {wk} is almost surely bounded.

(iii) From statement (i) we have that

(cid:107)wk − T (wk)(cid:107) = 0 a.s..

lim
k→∞

By variational inequality system (18), we have that any cluster point of a realization sequence generated by RPDC
almost surely is a saddle point of Lagrangian for (P).

(cid:3)

10. Proof of Theorem 2 (Expected primal suboptimality and expected feasibility)

Proof.

(i) Let h(w, w(cid:48)) = Λ(w, w(cid:48)) + d3
d1

2, we obtain that

Λ(w, w∗). By statement (i) and (iii) in Lemma 1, we have h(w, w(cid:48)) ≥ 0. From Lemma

Ei(k)

(cid:15)
N

(cid:2)L(uk+1, p) − L(u, qk)(cid:3) ≤ Λ(wk, w) − Ei(k)Λ(wk+1, w)

Taking expectation with respect to Ft, t > k for above inequality, we obtain that

(cid:15)
N

EFt

(cid:2)L(uk+1, p) − L(u, qk)(cid:3) ≤ EFt[Λ(wk, w) − Λ(wk+1, w)].

Take w = w∗ in (37), we obtain

By the combination of (37) and (38), it follows

0 ≤ EFt[Λ(wk, w∗) − Λ(wk+1, w∗)].

(cid:15)
N

EFt

(cid:2)L(uk+1, p) − L(u, qk)(cid:3) ≤ EFt[h(wk, w) − h(wk+1, w)]

(37)

(38)

(39)

From the deﬁnition of ¯ut and ¯pt, we have ¯ut ∈ U and ¯pt ∈ Rm. From the convexity of set U, Rm and the function
L(u(cid:48), p) − L(u, p(cid:48)) is convex in u(cid:48) and linear in p(cid:48), for all u ∈ U and p ∈ Rm, we have that

EFt

(cid:2)L(¯ut, p) − L(u, ¯pt)(cid:3) ≤ EFt

1
t + 1

t
(cid:88)

k=0

(cid:2)L(uk+1, p) − L(u, qk)(cid:3) ≤

N h(w0, w)
(cid:15)(t + 1)

.

(40)

Linear Convergence of RPDC method for Large-scale LCCP

(ii) If EFt(cid:107)A¯ut − b(cid:107) = 0, statement (ii) is obviously. Otherwise, EFt(cid:107)A¯ut − b(cid:107) (cid:54)= 0 i.e., there is set W such that

P{ω ∈ W|(cid:107)A¯ut − b(cid:107) (cid:54)= 0} > 0. Let ˆp be a random vector:

ˆp(ω) =

(cid:40)

0
M(cid:0)A¯ut−b(cid:1)
(cid:107)A¯ut−b(cid:107)

ω /∈ W

ω ∈ W.

Noted that for ω /∈ W, we have ˆp(ω) = 0 and (cid:107)A¯ut − b(cid:107) = 0. Thus

(cid:104)ˆp(ω), A¯ut − b(cid:105) = M (cid:107)A¯ut − b(cid:107) = 0.

Otherwise, for ω ∈ W, we have that

Together (42) and (43), we have

Moreover, since Au∗ = b, we have

(cid:104)ˆp(ω), A¯ut − b(cid:105) = M (cid:107)A¯ut − b(cid:107).

(cid:104)ˆp, A¯ut − b(cid:105) = M (cid:107)A¯ut − b(cid:107)

L(¯ut, ˆp) − L(u∗, ¯pt) = F (¯ut) + (cid:104)ˆp, A¯ut − b(cid:105) − F (u∗) = F (¯ut) − F (u∗) + M (cid:107)A¯ut − b(cid:107).

Moreover, by taking u = ¯ut in the right hand side of saddle point inequality, we have
F (¯ut) − F (u∗) ≥ −(cid:104)p∗, A¯ut − b(cid:105) ≥ −(cid:107)p∗(cid:107)(cid:107)A¯ut − b(cid:107).

Combine (45) and (46), we have that

(cid:107)A¯ut − b(cid:107) ≤

L(¯ut, ˆp) − L(u∗, ¯pt)
(M − (cid:107)p∗(cid:107))

.

Take expectation on both side of above inequality, we have that

EFt(cid:107)A¯ut − b(cid:107) ≤

EFt[L(¯ut, ˆp) − L(u∗, ¯pt)]
(M − (cid:107)p∗(cid:107))

≤ EFt

≤ EFt

N h(w0, (u∗, ˆp))
(M − (cid:107)p∗(cid:107)) (cid:15)(t + 1)
N d5
(M − (cid:107)p∗(cid:107)) (cid:15)(t + 1)

(by (i))

where d5 = sup
(cid:107)p(cid:107)<M

h(w0, (u∗, p)).

(iii) Again from (45), (46) and statement (ii), statement (iii) is coming.

(41)

(42)

(43)

(44)

(45)

(46)

(47)

(cid:3)

11. Proof of Lemma 3

Proof.

(i) This statement directly follows from the deﬁnition of φ(w, w∗) and statement (i) in Lemma 1.

(ii) This statement directly follows from the deﬁnition of φ(w, w∗) and statement (ii) in Lemma 1.

(iii) By the deﬁnition of φ(w, w∗), we have that.

φ(wk, w∗) − Ei(k)φ(wk+1, w∗)

= Λ(wk, w∗) − Ei(k)

≥ Λ(wk, w∗) − Ei(k)

(cid:26)

(cid:26)

Λ(wk+1, w∗) +

Λ(wk+1, w∗) +

(cid:15)
N
(cid:15)
N

[L(uk, p∗) − L(u∗, p∗)] −

[L(uk, p∗) − L(u∗, p∗)] −

[L(uk+1, p∗) − L(u∗, p∗)]

(cid:27)

[L(uk+1, p∗) − L(u∗, qk)]

(cid:27)

(cid:15)
N
(cid:15)
N

≥ d4[(cid:107)wk − T (wk)(cid:107)2 +

(cid:15)
N

[L(uk, p∗) − L(u∗, p∗)].

(by the deﬁnition of saddle point.)

(by Lemma 2)

(cid:3)

Linear Convergence of RPDC method for Large-scale LCCP

12. Proof of Theorem 3 (Global strong metric subregularity of H(w) implies linear convergence

of RPDC)

Proof. Considering the reference point T (wk) associated with given point wk, we have that

(cid:26) 0 ∈ ∇G(uk) + ∂J(Tu(wk)) + A(cid:62)qk + 1

(cid:2)∇K(Tu(wk)) − ∇K(uk)(cid:3) + NU(Tu(wk))

0 = b − ATu(wk) + 1
γ

(cid:2)Tp(wk) − pk(cid:3)

(cid:15)

Thus

v(T (wk)) =

(cid:18) ∇G(Tu(wk)) − ∇G(uk) + A(cid:62)(Tp(wk) − qk) + 1

(cid:2)∇K(uk) − ∇K(Tu(wk))(cid:3)

(cid:19)

∈ H(T (wk)).

(cid:15)

(cid:2)pk − Tp(wk)(cid:3)

1
γ

From Assumption 1 and 2, there is δ > 0 such that

Since H(w) is global strong metric subregular at w∗ for 0, then

(cid:107)v(T (wk))(cid:107)2 ≤ δ(cid:107)wk − T (wk)(cid:107)2.

(cid:107)T (wk) − w∗(cid:107) ≤ cdist(0, H(T (wk))) ≤ c(cid:107)v(T (wk))(cid:107) ≤ c

δ(cid:107)wk − T (wk)(cid:107).

√

Since (cid:107)wk − w∗(cid:107) ≤ (cid:107)T (wk) − w∗(cid:107) + (cid:107)wk − T (wk)(cid:107), we have

(cid:107)wk − w∗(cid:107) ≤ (c

δ + 1)(cid:107)wk − T (wk)(cid:107).

√

From statement (iii) of Lemma 3, we have that

φ(wk, w∗) − Ei(k)φ(wk+1, w∗) ≥ d4(cid:107)wk − T (wk)(cid:107)2 +

(cid:15)
N

[L(uk, p∗) − L(u∗, p∗)]

≥

d4
δ + 1)2

√

(c

(cid:107)wk − w∗(cid:107)2 +

(cid:15)
N

[L(uk, p∗) − L(u∗, p∗)]

(by (51))

(by (i) of Lemma 3)

(52)

where δ(cid:48) = min{

max{d2(c

√

d4
δ+1)2,d4+1}

,

≥ δ(cid:48){d2(cid:107)wk − w∗(cid:107)2 + (cid:15)[L(uk, p∗) − L(u∗, p∗)]}
≥ δ(cid:48)φ(wk, w∗).
1

N +1 } < 1. It follows that

Ei(k)φ(wk+1, w∗) ≤ αφ(wk, w∗).

where α = 1 − δ(cid:48) ∈ (0, 1). Taking expectation with respect to Fk+1 for above inequality, we obtain that

EFk+1φ(wk+1, w∗) ≤ αk+1φ(w0, w∗).

13. Proof of Corollary 1 (R-linear rate of the sequence {EFkwk})
Proof. By statement (i) in Lemma 3, we have that φ(w, w∗) ≥ d1(cid:107)w − w∗(cid:107)2. By Theorem 3, we have that

Then we have that

EFk φ(wk, w∗) ≤ αkφ(w0, w∗).

EFk (cid:107)wk − w∗(cid:107)2 ≤

αkφ(w0, w∗)
d1

.

By convexity of (cid:107) · (cid:107)2 and Jensen’s inequality, we obtain that

(cid:107)EFk wk − w∗(cid:107) ≤ ˆM (

√

α)k with ˆM =

(cid:115)

φ(w0, w∗)
d1

.

This shows that the sequence {EFk wk} converges to the desired saddle point w∗ at R-linear rate; i.e.,
(cid:113)
sup k

(cid:107)EFk wk − w∗(cid:107) =

α < 1.

√

lim
k→∞

(48)

(49)

(50)

(51)

(53)

(54)

(cid:3)

(cid:3)

Linear Convergence of RPDC method for Large-scale LCCP

14. Proof of Proposition 1

Proof. By the piecewise linear of H(w) and Zheng and Ng (Zheng & Ng, 2014), we have that H(w) is global metric
subregular at w∗ for 0. Since Q is positive-deﬁnite, then problem (SVM) has unique solution u∗. Hence, to show H(w) is
global strongly metric subregular, we need to prove uniqueness of the Lagrangian multiplier for (SVM). Suppose their are
two multipliers p and p(cid:48), thus we have

(cid:26) 0 ∈ Qu∗ − 1n + py + N[0,c]n (u∗)
0 ∈ Qu∗ − 1n + p(cid:48)y + N[0,c]n (u∗)

Since there exists at least one component u∗
we have that

i of optimal solution u∗ satisﬁes 0 < u∗

i < c, then ξi = N[0,c](u∗

i ) = 0. Thus,

(cid:26) Qiu∗ − 1 + yip = 0
Qiu∗ − 1 + yip(cid:48) = 0

We conclude that p = p(cid:48). Therefore H(w) is global strongly metric subregular.

15. Proof of Proposition 2

(55)

(cid:3)

Proof. By the piecewise linear of H(w) and Zheng and Ng (Zheng & Ng, 2014), we have that H(w) is global metric
subregular at w∗ for 0. Since Σ is positive-deﬁnite, then problem (MLP) has unique solution u∗. Hence, to show H(w) is
global strongly metric subregular, we need to prove uniqueness of the Lagrangian multiplier for (MLP). Suppose their are
two pare of multipliers (p1, p2) and (p(cid:48)

2), thus we have

1, p(cid:48)
(cid:26) 0 ∈ Σu∗ + λ∂(cid:107)u∗(cid:107)1 + p1µ + p21n
21n

0 ∈ Σu∗ + λ∂(cid:107)u∗(cid:107)1 + p(cid:48)

1µ + p(cid:48)

Since u∗

i (cid:54)= 0, u∗

j (cid:54)= 0, thus ξi = ∂|u∗

i | and ξj = ∂|u∗

j | are single valued and we have

Σiu∗ + λξi + µip(cid:48)

(cid:26) Σiu∗ + λξi + µip1 + p2 = 0
2 = 0
(cid:26) Σju∗ + λξj + µjp1 + p2 = 0
2 = 0

Σju∗ + λξj + µjp(cid:48)

1 + p(cid:48)

1 + p(cid:48)

It follows that

(cid:26) µi(p1 − p(cid:48)
µj(p1 − p(cid:48)

1) + p2 − p(cid:48)
1) + p2 − p(cid:48)

2 = 0
2 = 0

Since µi (cid:54)= µj, we conclude that p1 = p(cid:48)

1 and p2 = p(cid:48)

2. Therefore H(w) is global strongly metric subregular.

(56)

(57)

(58)

(cid:3)

Linear Convergence of RPDC method for Large-scale LCCP

References

Beck, A. and Teboulle, M. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations

Research Letters, 31(3):167–175, 2003.

Boser, B. E., Guyon, I. M., and Vapnik, V. N. A training algorithm for optimal margin classiﬁers. In Proceedings of the ﬁfth

annual workshop on Computational learning theory, pp. 144–152. ACM, 1992.

Brodie, J., Daubechies, I., De Mol, C., Giannone, D., and Loris, I. Sparse and stable markowitz portfolios. Proceedings of

the National Academy of Sciences, 106(30):12267–12272, 2009.

Burges, C. J. A tutorial on support vector machines for pattern recognition. Data mining and knowledge discovery, 2(2):

121–167, 1998.

Chang, C.-C. and Lin, C.-J. Libsvm: A library for support vector machines. ACM transactions on intelligent systems and

technology (TIST), 2(3):27, 2011.

Cohen, G. and Zhu, D. Decomposition and coordination methods in large scale optimization problems: The nondifferentiable

case and the use of augmented lagrangians. Adv. in Large Scale Systems, 1:203–266, 1984.

Cortes, C. and Vapnik, V. Support-vector networks. Machine learning, 20(3):273–297, 1995.

Dontchev, A. L. and Rockafellar, R. T. Implicit functions and solution mappings. Springer Monographs in Mathematics.

Springer, 208, 2009.

Fama, E. F. and French, K. R. The cross-section of expected stock returns. the Journal of Finance, 47(2):427–465, 1992.

Fortin, M. and Glowinski, R. Chapter iii on decomposition-coordination methods using an augmented lagrangian. In Studies

in Mathematics and Its Applications, volume 15, pp. 97–146. Elsevier, 1983.

Gao, X., Xu, Y.-Y., and Zhang, S.-Z. Randomized primal–dual proximal block coordinate updates. Journal of the Operations

Research Society of China, 7(2):205–250, 2019.

Hestenes, M. R. Multiplier and gradient methods. Journal of optimization theory and applications, 4(5):303–320, 1969.

Ho, M., Sun, Z., and Xin, J. Weighted elastic net penalized mean-variance portfolio design and computation. SIAM Journal

on Financial Mathematics, 6(1):1220–1244, 2015.

Lai, Z.-R., Yang, P.-Y., Fang, L., and Wu, X. Short-term sparse portfolio optimization based on alternating direction method

of multipliers. The Journal of Machine Learning Research, 19(1):2547–2574, 2018.

Li, B., Sahoo, D., and Hoi, S. C. Olps: a toolbox for on-line portfolio selection. The Journal of Machine Learning Research,

17(1):1242–1246, 2016.

Liu, J. and Wright, S. J. Asynchronous stochastic coordinate descent: Parallelism and convergence properties. SIAM Journal

on Optimization, 25(1):351–376, 2015.

Liu, J., Wright, S., R´e, C., Bittorf, V., and Sridhar, S. An asynchronous parallel stochastic coordinate descent algorithm. In

International Conference on Machine Learning, pp. 469–477, 2014.

Lu, Z. and Xiao, L. On the complexity analysis of randomized block-coordinate descent methods. Mathematical

Programming, 152(1-2):615–642, 2015.

Luo, Z.-Q. and Tseng, P. On the convergence of the coordinate descent method for convex differentiable minimization.

Journal of Optimization Theory and Applications, 72(1):7–35, 1992.

Necoara, I. and Patrascu, A. A random coordinate descent algorithm for optimization problems with composite objective

function and linear coupled constraints. Computational Optimization and Applications, 57(2):307–337, 2014.

Nesterov, Y. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization,

22(2):341–362, 2012.

Linear Convergence of RPDC method for Large-scale LCCP

Ortega, J. M. and Rheinboldt, W. C. Iterative solution of nonlinear equations in several variables, volume 30. Siam, 1970.

Powell, M. J. A method for nonlinear constraints in minimization problems. Optimization, pp. 283–298, 1969.

Richt´arik, P. and Tak´aˇc, M. Iteration complexity of randomized block-coordinate descent methods for minimizing a

composite function. Mathematical Programming, 144(1-2):1–38, 2014.

Robbins, H. and Siegmund, D. A convergence theorem for non negative almost supermartingales and some applications. In

Optimizing methods in statistics, pp. 233–257. Elsevier, 1971.

Sch¨olkopf, B., Smola, A. J., Williamson, R. C., and Bartlett, P. L. New support vector algorithms. Neural computation, 12

(5):1207–1245, 2000.

Shen, W., Wang, J., and Ma, S. Doubly regularized portfolio with risk minimization. In Twenty-Eighth AAAI Conference on

Artiﬁcial Intelligence, 2014.

Tseng, P. Convergence of a block coordinate descent method for nondifferentiable minimization. Journal of optimization

theory and applications, 109(3):475–494, 2001.

Wang, P.-W. and Lin, C.-J. Iteration complexity of feasible descent methods for convex optimization. The Journal of

Machine Learning Research, 15(1):1523–1548, 2014.

Xu, Y. Asynchronous parallel primal–dual block coordinate update methods for afﬁnely constrained convex programs.

Computational Optimization and Applications, 72(1):87–113, 2019.

Xu, Y. and Zhang, S. Accelerated primal–dual proximal block coordinate updating methods for constrained convex

optimization. Computational Optimization and Applications, 70(1):91–128, 2018.

Zhao, L. and Zhu, D. First-order primal-dual method for nonlinear convex cone programming.

arXiv preprint

arXiv:1801.00261, 2019.

Zheng, X. Y. and Ng, K. F. Metric subregularity of piecewise linear multifunctions and applications to piecewise linear

multiobjective optimization. SIAM Journal on Optimization, 24(1):154–174, 2014.

