0
2
0
2

v
o
N
6
1

]
L
C
.
s
c
[

3
v
7
3
4
7
0
.
4
0
0
2
:
v
i
X
r
a

Non-Autoregressive Machine Translation with Latent Alignments

Chitwan Saharia∗†
Google Research, Brain Team
sahariac@google.com

William Chan∗
Google Research, Brain Team
williamchan@google.com

Saurabh Saxena
Google Research, Brain Team
srbs@google.com

Mohammad Norouzi
Google Research, Brain Team
mnorouzi@google.com

Abstract

This paper presents two strong methods, CTC
and Imputer, for non-autoregressive machine
translation that model latent alignments with
dynamic programming. We revisit CTC for
machine translation and demonstrate that a
simple CTC model can achieve state-of-the-
art for single-step non-autoregressive machine
translation, contrary to what prior work in-
dicates.
In addition, we adapt the Imputer
model for non-autoregressive machine transla-
tion and demonstrate that Imputer with just 4
generation steps can match the performance of
an autoregressive Transformer baseline. Our
latent alignment models are simpler than many
existing non-autoregressive translation base-
lines; for example, we do not require target
length prediction or re-scoring with an autore-
gressive model. On the competitive WMT’14
En→De task, our CTC model achieves 25.7
BLEU with a single generation step, while Im-
puter achieves 27.5 BLEU with 2 generation
steps, and 28.0 BLEU with 4 generation steps.
This compares favourably to the autoregres-
sive Transformer baseline at 27.8 BLEU.

1 Introduction

interest

Non-autoregressive neural machine translation
(Gu et al., 2018) aims to enable the parallel
generation of output tokens without sacriﬁcing
translation quality. There has been a surge of
in this family of efﬁcient de-
recent
coding models,
resulting in the development
of iterative reﬁnement (Lee et al., 2018), CTC
models (Libovicky and Helcl, 2018),
insertion-
based methods (Stern et al., 2019; Chan et al.,
2019b), edit-based methods (Gu et al., 2019;
Ruis et al., 2019), masked language models
(Ghazvininejad et al., 2019, 2020b), and nor-
malizing ﬂow models (Ma et al., 2019). Some

∗ Equal contribution.
† Work done as part of the Google AI Residency.

tokens
of these methods generate the output
in a constant number of steps (Gu et al., 2018;
Libovicky and Helcl,
2018;
Ghazvininejad et al., 2019, 2020b), while others
require a logarithmic number of generation
steps (Stern et al., 2019; Chan et al., 2019b,a;
Li and Chan, 2019).

2018; Lee et al.,

Recent progress has decreased the gap between
autoregressive and non-autoregressive models’
translation scores. However, non-autoregressive
models often suffer from two main limitations:

1. First, most non-autoregressive models assume
that the output tokens are conditionally inde-
pendent given the input. This leads to the weak-
ness of such models in generating multi-modal
outputs (Gu et al., 2018), and materializes in
the form of token repetitions in the decoded out-
puts. Addressing this limitation generally in-
volves stochastic search algorithms like noisy
parallel decoding (Gu et al., 2018), iterative de-
coding (Ghazvininejad et al., 2019, 2020b), or
simple but less effective heuristic methods such
as collapsing repetitions (Lee et al., 2018).
2. The second limitation of many prior non-
autoregressive models is the requirement of
output length prediction as a pre-process. Au-
toregressive models have the ability to dynami-
cally adjust the output sequence length by emit-
ting an <END> token at any generation step
to stop. Many non-autoregressive models of-
ten require a ﬁxed length decoder. Thus they
train a separate target length prediction module,
and at inference time, ﬁrst predict and condition
on the target length, and then generate the out-
put tokens (Gu et al., 2018). Since the model
needs to commit to a ﬁxed predicted length,
which cannot be changed dynamically, it is of-
ten required to use multiple length candidates
and re-score them to produce the ﬁnal transla-

 
 
 
 
 
 
tion (Ghazvininejad et al., 2019, 2020b).
This paper addresses the limitations of existing
non-autoregressive machine translation models by
using latent alignment models. Latent alignment
models utilize a sequence of discrete latent align-
ment variables to monotonically align the non-
autoregressive predictions of the model and out-
put tokens. Such models use dynamic program-
ming to marginalize out the alignment variables
during training. This paper studies two instances
of latent alignment models including Connection-
ist Temporal Classiﬁcation (CTC) (Graves et al.,
2006, 2013; Graves and Jaitly, 2014) and Im-
puter (Chan et al., 2020). Libovicky and Helcl
(2018) have previously applied CTC to non-
autoregressive machine translation. However, we
report a signiﬁcant improvement over the work of
Libovicky and Helcl (2018) and demonstrate that
CTC can achieve the state-of-the-art in single-
step non-autoregressive machine translation. We
attribute this performance difference primarily
to our use of distillation during training, simi-
lar to Gu et al. (2018). We adapt latent align-
ment models to machine translation and demon-
strate their effectiveness on non-autoregressive
machine translation, advancing state-of-the-art on
WMT’14 En↔De and WMT’16 En↔Ro.

a pre-speciﬁed length. Our alignments have the
same length as the source sequences, and collaps-
ing the alignment’s blank tokens will recover the
target sequence.

Let x denote a source sequence and let y de-
note a target sequence, where yi ∈ V and V is
the target vocabulary. We make two assumptions:
1) there exists a monotonic mapping between the
model’s predictions and the target sequence, and
2) the source sequence is at least as long as the
target sequence, i.e. |x| ≥ |y|. We deﬁne an align-
ment a between x and y as a discrete sequence in
which ai ∈ V ∪ {“ ”}, |a| = |x|, and “ ” is a spe-
cial “blank” token that is removed to convert a to
the target sequence y. We deﬁne a function β(y)
that returns all possible alignments for a sequence
y of a particular length |x|. We also deﬁne the col-
lapsing function β−1(a) such that β−1(a) = y if
a ∈ β(y). To avoid token repetitions, it is useful to
deﬁne the collapsing function β−1(a) as ﬁrst col-
lapsing all consecutive repeated tokens, and then
removing all blank tokens. This formulation fol-
lows CTC precisely (Graves et al., 2006). For in-
stance, given a source sequence x of length 10, and
a target sequence y = (A, A, B, C, D), then a pos-
sible alignment a is ( , A, A, , A, B, B, C, , D).
The log-likelihood of the target sequence is re-

The main contributions of this paper include:

covered by marginalizing the latent alignments:

1. We adapt

latent alignment models to non-

autoregressive machine translation.

2. We achieve a new state-of-the-art of 25.8 BLEU
on WMT’14 En→De for single step non-
autoregressive machine translation.

3. We achieve 27.5 BLEU with 2 step genera-
tion, 28.0 BLEU with 4 step generation, and
28.2 BLEU with 8 step generation for WMT’14
En→De, setting a new state-of-the-art for non-
autoregressive machine translation with a con-
stant number of generation steps.

log pθ(y|x) = log

pθ(a|x)

(1)

X
a∈β(y)

The summation in (1) is typically intractable, since
there are a combinatorial number of alignments.
In the next two sub-sections, we will brieﬂy de-
scribe two variants of latent alignment models that
leverage dynamic programming to tractably com-
pute the log-likelihood, Connectionist Temporal
Classiﬁcation (CTC) (Graves et al., 2006) and Im-
puter (Chan et al., 2020).

2 Latent Alignment Models

2.1 Connectionist Temporal Classiﬁcation

We begin by describing the notion of alignment,
which in the context of this paper is deﬁned as
in the CTC literature (Graves et al., 2006, 2013;
Graves and Jaitly, 2014) and should not be con-
fused with word alignments in machine translation
(Manning et al., 1999; Dyer et al., 2013). Align-
ment is a mapping between a sequence of pre-
dicted tokens and a sequence of target tokens.
Alignment can be constructed by inserting special
“blank tokens” into the target sequence to match

Connectionist Temporal Classiﬁcation (CTC)
(Graves et al., 2006, 2013; Graves and Jaitly,
2014) models the alignment distribution with a
strong conditional independence assumption:

pθ(a|x) =

Y
i

p(ai|x; θ)

(2)

Leveraging this strong conditional

indepen-
dence assumption enables CTC to use an efﬁ-
cient dynamic programming algorithm to exactly

Source: Ein weiterer, besonders wichtiger Faktor sei die Vernetzung von Hochschulen und Unternehmen.

Imputer Decoding:
Another
Another
Another
Another
Another
Another
Another
Another

particularly important
particularly important
particularly important
particularly important
particularly important
particularly important
particularly important
particularly important

factor
factor
factor
factor
factor
factor
factor
factor

is
is
is
is
is
is
is
is

the
the
the
the
the
the
the
the

networking of universities and
networking of universities and
networking of universities and
networking of universities and
networking of universities and
networking of universities and
networking of universities and
networking of universities and

businesses
businesses
businesses
businesses
businesses
businesses
businesses
businesses

.
.
.
.
.
.
.
.

Output: Another particularly important factor is the networking of universities and businesses.

Figure 1: Example of top-k decoding using Imputer with 8 decoding steps. For a sentence of length N , the model imputes ⌈ N
8 ⌉
tokens at every decoding step. In each row, blue underlined tokens are the ones being imputed. Tokens that are not generated
yet are colored gray. Note that “ ” represents the special blank token that is removed for generating the ﬁnal target sentence.

marginalize out the latent alignments:

log pθ(y|x) = log

p(ai|x; θ)

(3)

X
a∈β(y)

Y
i

This allows us to compute the log-likelihood and
its gradient
tractably. We refer the reader to
Graves et al. (2006) for the exact details of the dy-
namic programming algorithm. During inference,
CTC generates the alignment distribution in par-
allel with a single generation step; the output se-
quence can then be recovered by greedy decod-
ing or beam search (Graves et al., 2006). We use
greedy decoding in all our experiments.

2.2

Imputer

The CTC model makes strong conditional inde-
pendence assumption between alignment
token
predictions. This assumption licenses CTC to gen-
erate the entire alignment in parallel, with a sin-
gle generation step independent of the number of
source or target tokens. However, the strong con-
ditional independence assumption limits its capac-
ity to model complex multi-modal distributions.
On the other hand, autoregressive models are ca-
pable of modelling such complex multi-modalities
with the chain rule factorization, but requires n de-
coding steps to generate n tokens during inference.
Imputer (Chan et al., 2020) aims to address these
limitations.

Imputer is an iterative generative model need-
ing only a constant number of generation steps for
inference. It makes conditional independence as-
sumptions within a generation step to achieve par-
allel generation, and models conditional depen-
dencies across generation steps. This approach

has been applied successfully in speech recogni-
tion (Chan et al., 2020), matching autoregressive
models with only a constant number of generation
Imputer models the distribution of align-
steps.
ments pθ(a|x) as:

pθ(a|x) =

X
˜a∈γ(a)

pθ(a|˜a, x)p(˜a|x)

(4)

where ˜a is a (partially masked out) alignment,
and γ(a) is the set of all possible masking per-
mutations of a.
(4) marginalizes over all possi-
ble alignments between the input and output se-
quences, and all possible generation orders. Im-
puter models the next alignment a conditioned on
the previous alignment ˜a:

pθ(a|˜a, x) =

Y
i

p(ai|˜a, x; θ)

(5)

The key insight to Imputer is that we can construct
a log-likelihood lower-bound:

log pθ(y|x)

≥ Ea∼β(y)



E

˜a∼γ(a)



log

pθ(a′|˜a, x)





X
a′∈β′(˜a,a)







(6)
where a′ ∼ β′(˜a, a) captures all possible align-
ments a′ consistent with (˜a, a) (Chan et al., 2020).
This equation can be solved efﬁciently via dy-
namic programming (Chan et al., 2020). This for-
mulation licenses Imputer with an iterative gen-
eration process. Tokens are generated indepen-
dently (and in parallel) within a generation step but
are conditioned on the partially predicted align-
ment ˜a of the last iteration (unlike CTC). In prac-

Alignment Probability
pθ(a|x)

Softmax

Self-Attention

Upsample

Embedding

x: Source Sequence

(a) CTC

Alignment Probability
pθ(a|x, ˜a)

Softmax

Self-Attention

+

Upsample

Embedding

Embedding

x: Source Sequence

˜a: Prior Alignment
(contains masked out tokens)

(b) Imputer

Figure 2: Visualization of the CTC (a) and Imputer (b) architecture for non-autoregressive machine translation.

tice, Imputer uses a constant number of decod-
ing iterations independent of the sequence length
(Chan et al., 2020).

However,

Both CTC and Imputer have seen much success
in tasks like speech recognition (Graves and Jaitly,
2014; Chan et al., 2020).
to the
these latent align-
best of our knowledge,
ment models have not been widely applied
to machine translation, with the exception
of Libovicky and Helcl (2018).
These latent
alignment models hold two key advantages
over prior non-autoregressive machine translation
work (Gu et al., 2018; Ghazvininejad et al., 2019),
namely: the token repetition problem and the tar-
get length prediction problem. We will discuss
them in detail in Section 3.

3 Latent Alignment Models for Machine

Translation

In this section, we will discuss how latent align-
ment models can be adapted to machine transla-
tion, and then describe key advantages offered by
these models. Section 2 identiﬁed two assump-
tions made by latent alignment models: 1) there
exists a monotonic mapping between the model
alignment predictions and the target sequence, and
2) the length of the target sequence is less than
or equal to the length of source sequence,
i.e.
|y| ≤ |x|. We will now address these issues to
adapt our models for machine translation.

Monotonic Assumption. A monotonic struc-
ture between model alignment predictions and the
target sequence is desired for the dynamic pro-
gramming algorithm to marginalize out the latent
alignments in Equation (1). Unlike tasks such as

speech recognition, a monotonic relationship be-
tween the model alignment predictions and the
target sequence may not exist in machine trans-
lation. For instance, speech-to-text is inherently
local, whereas there is typically some global word
reordering in machine translation. We hypothesize
that if we use a powerful deep neural network like
the Transformer (Vaswani et al., 2017), the Trans-
former will have sufﬁcient computational capacity
to learn to reorder the contextual embeddings such
that it is approximately monotonic with the target
sequence. Libovicky and Helcl (2018) also made
a similar assumption.

Length Assumption. By construction, our
alignments are the same length as the source se-
quence, and consequently, we can not generate a
target sequence longer than the source sequence.
This is not a problem for speech recognition, since
the source sequence is generally much longer than
the target sequence; however, this is prohibitively
restrictive for machine translation. This issue was
also discussed in Libovicky and Helcl (2018), and
they proposed a simple solution of up-sampling
the source sequence to s times the original length.
Choosing a sufﬁcient canvas scale of s, we can en-
sure the alignment is long enough to model the tar-
get sequence across our training distribution. We
use a very similar up-sampling method applied
to the embedding matrix of the source sequence.
Given a source sequence embedding x ∈ R|x|×d
with d-dimension and length |x|, we simply up-
sample x′ ∈ Rs·|x|×d via an afﬁne transformation.

3.1 Model Architecture

Our neural architecture is simply a stack of self-
attention layers (Vaswani et al., 2017). The source
sequence is upsampled (to handle longer target
In the Imputer
sequences as described above).
architecture, the input to our self-attention stack
is simply the superpositioning of the upsampled
source and the previous alignment. Our work dif-
fers from the prior method, 1) our uniﬁed archi-
tecture does not have separate encoder decoders
which require cross-attention mechanisms, 2) our
architecture is bidirectional, and does not rely on
causality masks. Figure 2 visualizes our architec-
ture.

3.2 Advantages

Latent alignment models mitigate two common is-
sues shared by many non-autoregressive machine
translation models – token repetition and the re-
quirement for separate target length prediction.

3.2.1 Fewer Token Repetitions
Non-autoregressive sequence models make a con-
ditional independence assumption between token
predictions. This licenses them to parallel token
generation during inference; however, it makes it
difﬁcult to model complex multi-modal distribu-
tions. This is especially true for single-step gen-
eration models which make strong conditional in-
dependence assumptions. During inference, this
conditional independent generation often results in
the token repetition problem, where tokens are er-
roneously repeated in the output sequence.

This issue has been discussed extensively in
prior works (Gu et al., 2018; Lee et al., 2018;
Ghazvininejad et al., 2019) in the context of ma-
chine translation, and has been shown to have a
negative impact on performance. To handle these
repetitions, Gu et al. (2018) used Noisy Parallel
Decoding, wherein they sample a large number
of translation hypotheses and use an autoregres-
sive teacher to re-score them to implicitly penal-
ize translations with more erroneous repetitions.
Lee et al. (2018) adopted a simple but less ef-
fective heuristic of simply removing all consecu-
tive repetitions from the predicted target sequence.
Ghazvininejad et al. (2019) hypothesized that iter-
ative decoding can help remove repetitions by al-
lowing the model to condition on parts of the in-
put, thus collapsing the multi-modal distribution
into a sharper uni-modal distribution. They empir-
ically show that the ﬁrst few decoding iterations

are crucial for removing repetitions resulting in a
sharp increase in performance.

Like other non-autoregressive models,

latent
alignment models also perform conditionally in-
dependent generation, and hence face the issue of
token repetitions. Although they differ from the
other models in that they do not generate the tar-
get sequence directly. Rather, the inference pro-
cess involves the generation of the target align-
ment, followed by collapsing the generated align-
ment into the target sequence using the collapsing
function β−1. Recall by construction, β−1 col-
lapses repeated tokens (Graves et al., 2006), this
inference process enables these models to handle
erroneous repetitions implicitly by naturally col-
In particular, for single-step de-
lapsing them.
coding, we show that our CTC based model re-
moves most of the repetitions while collapsing
the alignment into target sequence, resulting in
a signiﬁcant improvement in translation quality
over prior single step generation models. In addi-
tion, we show that Imputer requires just 4 decod-
ing iterations to achieve state-of-the-art translation
scores on WMT14 En→De, in contrast to 10 itera-
tions used by Mask-Predict (Ghazvininejad et al.,
2019).

3.2.2 No Target Length Prediction Needed

the target sequence.

Many prior non-autoregressive models (Gu et al.,
2018; Ghazvininejad et al., 2019) ﬁrst predict the
target length, then conditioned on the target length
predict
This is needed
because these architectures utilize an encoder-
decoder formulation, and the decoder requires a
ﬁxed canvas size to work with. The length is
ﬁxed, and it cannot be changed dynamically by
the model during decoding. Due to this lack of
ﬂexibility, during inference, one typically samples
multiple length candidates and performs decoding
for each length followed by re-ranking them to get
a ﬁnal translation. This not only requires tuning of
a new hyperparameter for determining the num-
ber of length candidates to sample during infer-
ence but also entails a considerable amount of ex-
tra inference computation.

Our latent alignment models do not require tar-
get length prediction, but rather implicitly deter-
mine the target sequence length through the align-
ment. This is possible since the alignment is of
the same length as the source sequence, thus elim-
inating the requirement of predicting target length
in advance during inference. The caveat is that

Table 1: BLEU comparison for various single step generation models. Our simple CTC model is able to out-
perform all prior single step generation models. †The main difference between our CTC model and prior work
(Libovicky and Helcl, 2018) is that we use data distillation.

Method

Non-Autoregressive

Iterative Reﬁnement (Lee et al., 2018)
NAT with Fertility (Gu et al., 2018)
CTC† (Libovicky and Helcl, 2018)
Mask-Predict (Ghazvininejad et al., 2019)
SMART (Ghazvininejad et al., 2020b)
Auxiliary Regularization (Wang et al., 2019)
Bag-of-ngrams Loss (Shao et al., 2020)
Hint-based Training (Li et al., 2019)
FlowSeq (Ma et al., 2019)
NAT (TCL) (Liu et al., 2020)
Bigram CRF (Sun et al., 2019)
AXE CMLM (Ghazvininejad et al., 2020a)
NAT (EM + ODD) (Sun and Yang, 2020)

Our Work
CTC†
Imputer

Iterations

En→De De→En En→Ro Ro→En

WMT’14

WMT’16

1
1
1
1
1
1
1
1
1
1
1
1
1

1
1

13.9
17.7
17.7
18.0
18.6
20.7
20.9
21.1
21.5
21.9
23.4
23.5
24.5

25.7
25.8

16.7
21.5
19.8
19.3
23.8
24.8
24.6
25.2
26.2
25.6
27.2
27.9
27.9

28.1
28.4

24.5
27.3
19.9
27.3
-
-
28.3
-
29.3
-
-
30.8
-

32.2
32.3

25.7
29.1
24.7
28.2
-
-
29.3
-
30.4
-
-
31.5
-

31.6
31.7

we can not generate a target sequence longer than
the source sequence, which we address in Section
3. Libovicky and Helcl (2018), which also applied
CTC to machine translation, made a similar argu-
ment, and we further extend this to Imputer. Our
approach simpliﬁes the architecture and decoding
process, avoiding a need to build a target length
prediction model and searching over it during in-
ference.

4 Related Work

There has been signiﬁcant prior work on non-
autoregressive iterative methods
for machine
translation (Gu et al., 2018), some of which are:
iterative reﬁnement (Lee et al., 2018), insertion-
based methods (Stern et al., 2019; Chan et al.,
2019a; Li and Chan, 2019),
and conditional
masked language models (Ghazvininejad et al.,
2019, 2020b).
Like insertion-based models
(Stern et al., 2019; Chan et al., 2019c), our work
does not commit to a ﬁxed target length; insertion-
based models can dynamically grow the canvas
size, whereas our work which relies on a latent
alignment can only generate a target sequence up
to a ﬁxed maximum predetermined length. Com-
pared to conditional masked languages models
(Ghazvininejad et al., 2019, 2020b), key differ-
ences are: 1) our models do not require target
length prediction, and 2) we eschew the encoder-
decoder neural architecture formulation, but rather
rely on the single simple decoder architecture.

KERMIT (Chan et al., 2019b,a) also has a similar
neural architecture as us; they also eschew the con-
ventional encoder-decoder architecture and have a
uniﬁed architecture. Our work relies on the su-
perpositioning of the input and output sequences
via the latent alignment, whereas KERMIT relies
on concatenation to process the input and output
sequences. Their work is also more focused on
generative p(x, y) modelling, whereas our work is
focused on conditional modelling p(y|x).

Our CTC work is closely related to and inspired
heavily by Libovicky and Helcl (2018), which ap-
plied CTC single step generation models. The key
difference is that our work used data distillation
for training, and we ﬁnd that distillation provides
a signiﬁcant boost in performance for our CTC
models.

Finally, our work is closely related to the con-
current work of Ghazvininejad et al. (2020a) on
AXE CMLM. Similar to our work, they also as-
sume a latent alignment and use dynamic pro-
gramming for learning. Their work focused on the
single-step generation and demonstrated strong re-
sults, while we apply our models to both single
step and iterative generation.

5 Experiments

Hyperparameters. We follow the base Trans-
former (Vaswani et al., 2017) for our experiments.
However, since our architecture does not contain
an encoder, we double the number of layers in our

Table 2: BLEU comparison for various autoregressive and non-autoregressive models. Imputer is able to match the autore-
gressive Transformer baseline with just 4 generation steps. Numbers reported for Imputer trained with data distilled from big
autoregressive transformer for En ↔ De, and base transformer for En ↔ Ro.

Method

Autoregressive

Base Transformer

Non-Autoregressive

Insertion Transformer (Stern et al., 2019)
KERMIT (Chan et al., 2019b)
Iterative Reﬁnement (Lee et al., 2018)
Mask-Predict (Ghazvininejad et al., 2019)

SMART (Ghazvininejad et al., 2020b)

DisCo (Kasai et al., 2020)
JM-NAT (Guo et al., 2020)

Our Work
Imputer

Iterations

En→De De→En En→Ro Ro→En

WMT’14

WMT’16

n

27.8

31.2

34.3

34.0

≈ log2 n
≈ log2 n
10
4
10
4
10
≈ 4
4

2
4
8

27.4
27.8
21.6
25.9
27.0
27.0
27.7
27.3
27.1

27.5
28.0
28.2

-
30.7
25.5
29.9
30.5
30.9
31.3
31.3
31.5

30.6
31.5
31.8

-
-
29.3
32.5
33.1
-
-
33.2
33.0

33.7
34.3
34.4

-
-
30.2
33.2
33.3
-
-
33.3
33.2

33.4
34.0
34.1

decoder to maintain the same number of parame-
ters. Our models consist of 12 self-attention lay-
ers, with 512 hidden size, 2048 ﬁlter size, and 8
attention heads per layer. We use 0.1 dropout for
regularization. We batch sequences of approxi-
mately same lengths together, with approximately
2048 tokens per batch. We use Adam optimizer
(Kingma and Ba, 2015) with β = (0.9, 0.997) and
ǫ = 10−9. The learning rate warms up to 10−3 in
the ﬁrst 10k steps and then decays with the inverse
square root schedule following the Tensor2Tensor
implementation (Vaswani et al., 2018). We train
all our models for 2M steps. We train the Imputer
using CTC loss (all masked prior alignment) for
1M steps, followed by Bernoulli masking policy
(Chan et al., 2020) for next 1M steps. We aver-
age the 5 checkpoints with the best performance
on the development set to get the ﬁnal model. For
Imputer, we use top-k decoding during inference.
We use canvas scale s = 2 for all our experiments,
meaning we upsample the source sequence by a
factor of 2.

Dataset. We perform experiments on WMT’14
En↔De, using newstest2013 as the development
set, and report newstest2014 as the test set. We
also report our performance on WMT’16 En-
Ro. We use SentencePiece (Kudo and Richardson,
2018) to generate a shared subword vocabulary.
We evaluate the performance of our models with
BLEU (Papineni et al., 2002).

Distillation. We follow prior work (Gu et al.,
2019;

2018; Lee et al.,

Stern et al.,

2018;

Ghazvininejad et al., 2019) and use data dis-
tilled from an autoregressive teacher for training
our models. We use autoregressive base Trans-
formers for generating distilled data. For iterative
generation, we also report
the performance of
Imputer model
trained on data distilled from
autoregressive big Transformers to be comparable
with (Ghazvininejad et al., 2019, 2020b) which
distilled from a big Transformer. For WMT’16
En-Ro, we use the distilled dataset provided by
Ghazvininejad et al. (2019)1. We analyze the
impact of distillation on the performance of our
models in Section 6.3.

5.1 Single Step Decoding

We ﬁrst report the performance of latent alignment
models for single-step decoding. CTC makes
full conditional independence assumption allow-
ing the generation of the entire target sequence
in a single step. We can also perform non-
autoregressive single step generation with Imputer
by imputing all of the tokens at once. Table 1 sum-
marizes the performance of our models and other
non-autoregressive single step generation models.
Our CTC model achieves 25.7 BLEU, and the Im-
puter model achieves 25.8 BLEU for WMT’14
En→De. We ﬁnd that our single step genera-
tion models outperform the autoregressive GNMT
model of Wu et al. (2016) on En→De with 24.6
BLEU. To the best of our knowledge, our CTC
and Imputer models outperform all prior work on

1https://github.com/facebookresearch/Mask-Predict

Table 3: WMT’14 En-De BLEU comparison for distillation
base vs big Transformer, and number of decoding iterations.

Table 4: WMT’14 En↔De repeated token percentage com-
parison for single step generation models.

Model

Iterations En→De De→En

Transformer (Base)
Transformer (Big)

Imputer (Base Distillation)

Imputer (Big Distillation)

n
n

2
4
8
n

2
4
8
n

27.8
29.5

27.3
27.9
27.9
28.3

27.5
28.0
28.2
28.4

31.2
32.2

30.3
30.9
31.1
31.2

30.2
31.0
31.3
31.4

single-step generation on WMT’14 En↔De and
WMT’16 En↔Ro.

5.2

Iterative Decoding

We now analyze the performance of Imputer. Im-
puter uses a constant number of decoding itera-
tions independent of sequence length. We com-
pare our performance with other sub-linear non-
autoregressive models, ranging from models re-
quiring logarithmic to a constant number of de-
coding iterations. Table 2 summarizes the results
of Imputer model.

Our Imputer model using 8 decoding iterations
achieves 28.2 BLEU on En→De, slightly outper-
forming the autoregressive Transformer of 27.8
BLEU. On De→En, we achieve 31.3 BLEU, on
par with the autoregressive Transformer model.
Similarly, on En↔Ro, Imputer matches the per-
formance of the autoregressive teacher using just
4 decoding iterations. We also observe the ro-
bustness of our Imputer model when we reduce
the number of decoding iterations from 8 to 2.
Using only 2 iterations, we obtain 27.5 BLEU
on En→De and 30.2 BLEU on De→En. These
results were trained with distillation from a big
Transformer model. However, even when we dis-
till from the base Transformer as shown in Table 3,
Imputer still performs on par with the autoregres-
sive Transformer achieving 27.9 and 31.1 BLEU
on En→De and De→En respectively. Figure 1
shows an example 8-step iterative decoding by Im-
puter.

6 Analysis

In this section, we present further analysis of our
latent alignment models. We analyze the (1) im-
pact on token repetitions in generated translations,
(2) impact of the number of decoding iterations on

Model

Gold Test Set

En→De De→En

0.04%

0.02%

Mask-Predict (Ghazvininejad et al., 2019)
AXE CMLM (Ghazvininejad et al., 2020a)
CTC (Our Work)

16.72% 12.31%
1.03%
1.41%
0.17% 0.23%

Table 5: Average relative decoding speed-up w.r.t. autore-
gressive greedy decoding baseline on WMT’14 En→De test
set for Imputer.

Iterations

Relative Speed-Up

1 (CTC / Imputer)
2
4
8

×18.6
×9.2
×5.9
×3.9

Imputer, (3) impact of distillation on our models,
and (4) impact of target length on Imputer.

6.1 Token Repetitions

of

repetition
our
rate
We
the
compare
CTC model with
single-step Mask-Predict
(Ghazvininejad et al., 2019) and the concurrent
work AXE CMLM (Ghazvininejad et al., 2020a)
in Table 4. We also report the percentage of
repetitions in the original test set for reference.
We observe a signiﬁcantly lower rate of token
repetitions in our CTC model compared to both
the models. This empirical observation supports
our hypothesis that β−1 helps remove spurious
token repetitions.

6.2 Impact of Number of Decoding Iterations

The number of decoding iterations is an impor-
tant hyperparameter in iterative models, provid-
ing a tunable trade-off between quality and infer-
ence speed. The ideal parallel decoding model
should be robust to the number of decoding itera-
tions, i.e. reducing the number of iterations should
have minimal impact on performance. To analyze
this capability of our Imputer model, we study
the impact of the number of decoding iterations
vs BLEU. We use the Imputer trained with data
distilled from the autoregressive base Transformer
Imputer controls the
teacher for this analysis.
number of decoding iterations through the top-k
hyperparameter, which imputes k tokens per step.
On one end, imputing all the tokens (k = ∞) in
one step results in single-step decoding, while on
the other end, imputing 1 token per step (k = 1)
results in linear autoregressive decoding (but not

BLEU vs. Number of Decoding Iterations

29

28

27

26

25

U
E
L
B

1

Imputer
Autoregressive Teacher

4

2
8
# Decoding Iterations

16

32

N

U
E
L
B

30

27

24

21

18

BLEU vs. Target Sequence Length

N=1
N=2
N=8

≤10

20

30

40

50 ≥ 50

Target Sequence Length

Figure 3: WMT’14 En→De BLEU comparison for different
number of decoding iterations for Imputer.

Table 6: WMT’14 En→De BLEU comparison showing the
impact of distillation.

Method

Iterations Original Distillation

CTC
Imputer

1
4
8

15.6
24.7
25.0

25.4
27.9
27.9

necessarily left-to-right).

Figure 3 shows the BLEU score vs target length
T for WMT’14 En→De test set, where T is the
number of decoding iterations. As expected, the
performance consistently increases with an in-
crease in T . We ﬁnd that Imputer is robust to T ,
sacriﬁcing just 0.6 BLEU points when reducing T
from 8 to 2. We can match the performance of
its autoregressive teacher using just 4 decoding it-
erations. Interestingly, the performance keeps in-
creasing consistently beyond 8 iterations, and even
outperforming the autoregressive teacher slightly.
In the extreme case of autoregressive O(n) decod-
ing, we obtain 28.3 BLEU score, exceeding the
teacher’s performance by 0.5 BLEU points.

6.3

Impact of Distillation

We analyze the impact of distillation on our mod-
els by comparing them to original training data
versus training data from a base Transformer
teacher on the WMT’14 En→De dataset. Ta-
ble 6 summarizes the results. In all cases, mod-
els trained with the distilled data perform signiﬁ-
cantly better than the model trained with the orig-
inal data. We observe that the performance gap
is largest in the case of the CTC model, and de-
creases with an increase in the number of de-
coding iterations. This is consistent with prior
work ﬁnding distillation to improve model quality
(Gu et al., 2018; Zhou et al., 2020).

Figure 4: WMT’14 En→De BLEU comparison for sen-
tences binned by target sequence length for Imputer; N is
the number of decoding iterations.

6.4 Impact of Target Length for Imputer

Figure 4 depicts the impact of number of decoding
iterations bucketed by the target sequence length
N . We use the compare-mt (Neubig et al.,
2019) package to bucket test set examples based
on target sentence length, and compute BLEU
score for each bucket using a different number of
decoding iterations. Increase in the number of de-
coding iterations provides consistent gain across
all buckets.

7 Conclusion

In this paper, we investigated two latent align-
ments models, CTC and Imputer,
for non-
autoregressive machine translation. CTC is a sin-
gle step generation model, while Imputer is an it-
erative generative model requiring only a constant
number of generation steps. Our models rely on
dynamic programming to marginalize out the la-
tent alignments. Unlike many prior works, our
models do not need to perform target length pre-
diction, or re-scoring of candidates and our mod-
els use a simpliﬁed neural architecture without the
need of cross-attention mechanism found in many
prior encoder-decoder architectures. We demon-
strate the ease and effectiveness of the application
of these simple latent alignment models primarily
used in speech recognition to the task of machine
translation. Applying these latent alignment mod-
els for parallel translation of long documents can
be an interesting research direction.

Acknowledgments

We give thanks to Colin Cherry, Geoffrey Hinton,
George Foster, Jakob Uszkoreit, Jamie Kiros, Ju-
lia Kreutzer, Samy Bengio, and Sara Sabour for
research discussions and technical assistance.

References

Harris Chan, Jamie Kiros, and William Chan. 2019a.
Multilingual KERMIT: It’s Not Easy Being Genera-
tive. In NeurIPS: Workshop on Perception as Gen-
erative Reasoning.

William Chan, Nikita Kitaev, Kelvin Guu, Mitchell
Stern, and Jakob Uszkoreit. 2019b. KERMIT: Gen-
erative Insertion-Based Modeling for Sequences. In
arXiv.

William Chan, Chitwan Saharia, Geoffrey Hinton, Mo-
hammad Norouzi, and Navdeep Jaitly. 2020.
Im-
puter: Sequence modelling via imputation and dy-
namic programming. In arXiv.

William Chan, Mitchell Stern, Jamie Kiros, and Jakob
Uszkoreit. 2019c. An Empirical Study of Genera-
tion Order for Machine Translation. In arXiv.

Chris Dyer, Victor Chahuneau, and Noah Smith. 2013.
A Simple, Fast, and Effective Reparameterization of
IBM Model 2. In NAACL.

Marjan Ghazvininejad, Vladimir Karpukhin, Luke
Zettlemoyer, and Omer Levy. 2020a. Aligned Cross
Entropy for Non-Autoregressive Machine Transla-
tion. In arXiv.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and
Luke Zettlemoyer. 2019. Mask-Predict: Parallel De-
coding of Conditional Masked Language Models. In
EMNLP.

Marjan Ghazvininejad, Omer Levy, and Luke Zettle-
moyer. 2020b. Semi-Autoregressive Training Im-
proves Mask-Predict Decoding. In arXiv.

Alex Graves, Santiago Fernandez, Faustino Gomez,
and Jurgen Schmidhuber. 2006. Connectionist Tem-
poral Classiﬁcation: Labelling Unsegmented Se-
quence Data with Recurrent Neural Networks.
In
ICML.

Alex Graves and Navdeep Jaitly. 2014. Towards End-
to-End Speech Recognition with Recurrent Neural
Networks. In ICML.

Alex Graves, Abdel-rahman Mohamed, and Geoffrey
Hinton. 2013. Speech Recognition with Deep Re-
current Neural Networks. In ICASSP.

Jiatao Gu, James Bradbury, Caiming Xiong, Vic-
tor O.K. Li, and Richard Socher. 2018. Non-
Autoregressive Neural Machine Translation.
In
ICLR.

Jungo Kasai, James Cross, Marjan Ghazvininejad, and
Jiatao Gu. 2020. Parallel Machine Translation with
Disentangled Context Transformer. arXiv preprint
arXiv:2001.05136.

Diederik Kingma and Jimmy Ba. 2015. Adam: A

Method for Stochastic Optimization. In ICLR.

Taku Kudo and John Richardson. 2018. Sentence-
Piece: A simple and language independent subword
tokenizer and detokenizer for Neural Text Process-
ing. In EMNLP.

Jason Lee, Elman Mansimov, and Kyunghyun Cho.
2018. Deterministic Non-Autoregressive Neural
In
Sequence Modeling by Iterative Reﬁnement.
EMNLP.

Lala Li and William Chan. 2019. Big Bidirectional In-
sertion Representations for Documents. In EMNLP:
Workshop of Neural Generation and Translation.

Zhuohan Li, Zi Lin, Di He, Fei Tian, Tao Qin, Liwei
Wang, and Tie-Yan Liu. 2019. Hint-Based Train-
ing for Non-Autoregressive Machine Translation .
In EMNLP.

Jindrich Libovicky and Jindrich Helcl. 2018. End-to-
End Non-Autoregressive Neural Machine Transla-
tion with Connectionist Temporal Classiﬁcation. In
EMNLP.

Jinglin Liu, Yi Ren, Xu Tan, Chen Zhang, Tao Qin,
Zhou Zhao, and Tie-Yan Liu. 2020. Task-level cur-
riculum learning for non-autoregressive neural ma-
chine translation. arXiv preprint arXiv:2007.08772.

Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neu-
FlowSeq: Non-
big, and Eduard Hovy. 2019.
Autoregressive Conditional Sequence Generation
with Generative Flow. In EMNLP.

Christopher D Manning, Christopher D Manning, and
Hinrich Sch¨utze. 1999. Foundations of statistical
natural language processing. MIT press.

Graham Neubig, Zi-Yi Dou, Junjie Hu, Paul Michel,
Danish Pruthi, Xinyi Wang, and John Wieting. 2019.
compare-mt: A Tool for Holistic Comparison of
Language Generation Systems. CoRR.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In ACL.

Jiatao Gu, Changhan Wang, and Jake Zhao. 2019. Lev-

enshtein Transformer. In NeurIPS.

Junliang Guo, Linli Xu, and Enhong Chen. 2020.

Laura Ruis, Mitchell Stern, Julia Proskurnia, and
Insertion-Deletion Trans-
William Chan. 2019.
former. In EMNLP: Workshop of Neural Generation
and Translation.

Jointly masked sequence-to-sequence model for non-autoregressive neural machine translation.
In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics, pages
376–385, Online. Association for Computational
Linguistics.

Chenze Shao, Jinchao Zhang, Yang Feng, Fandong
Meng, and Jie Zhou. 2020. Minimizing the Bag-of-
Ngrams Difference for Non-Autoregressive Neural
Machine Translation. In AAAI.

Mitchell Stern, William Chan, Jamie Kiros, and Jakob
Insertion Transformer: Flexible
In

Uszkoreit. 2019.
Sequence Generation via Insertion Operations.
ICML.

Zhiqing Sun, Zhuohan Li, Haoqing Wang, Di He,
Zi Lin, and Zhihong Deng. 2019. Fast Structured
Decoding for Sequence Models. In NeurIPS.

Zhiqing Sun and Yiming Yang. 2020. An EM Ap-
proach to Non-autoregressive Conditional Sequence
Generation. arXiv preprint arXiv:2006.16378.

Ashish Vaswani, Samy Bengio, Eugene Brevdo, Fran-
cois Chollet, Aidan N. Gomez, Stephan Gouws,
Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki
Parmar, Ryan Sepassi, Noam Shazeer, and Jakob
Uszkoreit. 2018. Tensor2Tensor for Neural Machine
Translation. In AMTA.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention Is All
You Need. In NIPS.

Yiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang
Zhai, and Tie-Yan Liu. 2019. Non-Autoregressive
Machine Translation with Auxiliary Regularization.
In AAAI.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason Smith, Jason
Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2016. Google’s
Neural Machine Translation System: Bridging the
Gap between Human and Machine Translation. In
arXiv.

Chunting Zhou, Graham Neubig, and Jiatao Gu.
2020. Understanding Knowledge Distillation in
Non-autoregressive Machine Translation. In ICLR.

