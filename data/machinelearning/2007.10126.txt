Human-like Energy Management Based on Deep Rein-
forcement Learning and Historical Driving Experiences 

Teng Liu, Xiaolin Tang, Xiaosong, Hu, Senior Member, IEEE, Wenhao Tan, Jinwei Zhang 

1 

Abstract—Development  of  hybrid  electric  vehicles  depends on 
an  advanced  and  efficient  energy  management  strategy  (EMS). 
With online and real-time requirements in mind, this article pre-
sents  a  human-like  energy  management  framework  for  hybrid 
electric vehicles according to deep reinforcement learning meth-
ods and collected historical driving data. The hybrid powertrain 
studied  has  a  series-parallel  topology,  and  its  control-oriented 
modeling  is  founded  first.  Then,  the  distinctive  deep  reinforce-
ment learning (DRL) algorithm, named deep deterministic policy 
gradient  (DDPG),  is  introduced.  To  enhance  the  derived  power 
split  controls  in  the  DRL  framework,  the  global  optimal  control 
trajectories  obtained  from  dynamic  programming  (DP)  are  re-
garded  as  expert  knowledge  to  train  the  DDPG  model.  This  op-
eration  guarantees  the  optimality  of  the  proposed  control  archi-
tecture. Moreover, the collected historical driving data based on 
experienced  drivers  are  employed  to  replace  the  DP-based  con-
trols, and thus construct the human-like EMSs. Finally, different 
categories of experiments are executed to estimate the optimality 
and adaptability of the proposed human-like EMS. Improvements 
in fuel economy and convergence rate indicate the effectiveness of 
the constructed control structure. 

Index  Terms—Hybrid  electric  vehicles,  deep  reinforcement 
learning,  human-like,  energy  management  strategy,  dynamic 
programming 

NOMENCLATURE 
Hybrid Electric Vehicles 

Energy Management Strategy 
Reinforcement Learning  
Deep Reinforcement Learning  
Deep Q-learning  

HEV 
PHEV  Plug-in Hybrid Electric Vehicle 
EMS 
RL 
DRL 
DQL 
DDPG  Deep Deterministic Policy Gradient  
NN 
DP 
ICE 

Neural Network 
Dynamic Programming  
Internal-Combustion Engine  

This work was in part supported by the State Key Laboratory of Mechanical 
System  and  Vibration  (Grant  No.  MSV202016),  National  Natural  Science 
Foundation of China (Grant No. 51875054), and Chongqing Natural Science 
Foundation for Distinguished Young Scholars (Grant No. cstc2019jcyjjq0010), 
Chongqing Science and Technology Bureau, China. (Corresponding authors: X. 
Tang and X. Hu.) 
T. Liu, X. Tang, X. Hu, and W. Tan are with Department of Automotive En-
gineering,  Chongqing  University,  Chongqing  400044,  China 
(email: 
tengliu17@gmail.com, 
xiaosonghu@ieee.org, 
201932131033@cqu.edu.cn). 
J.  Zhang  is  with  Department  of  Mechanical  and  Mechatronics  Engineering, 
University of Waterloo, N2L 3G1, Canada. (jinwei.zhang@uwaterloo.ca) 

tangxl0923@cqu.edu.cn, 

Integrated Starter Generator  
State of Charge  

ISG 
SoC 
BSFC  Brake Specific Fuel Consumption  
GPS 
Global Position System  
MDP  Markov Decision Process 

I.   INTRODUCTION 

H 

AVING  potentials  in  energy  conservation  and  pollution 
reduction, the hybrid electric vehicles (HEVs) and plug-in 
(PHEVs)  are  becoming 
hybrid  electric  vehicles 
mass-market  electrified  vehicles  in  the  current  sales  market 
[1-5].  Energy  management  and  powertrain  matching  are  two 
crucial  technologies  to  improve  fuel  economy  and  maintain 
excellent performance for these vehicles [6, 7]. How to operate 
these  vehicles in  an  ideal pattern  for  power distribution  is  an 
arduous  task  called  energy  management  strategies  (EMSs) 
[8-10]. 

require  advanced  EMS 

Sophisticated  multiple  energy  storage  resources  in  hybrid 
to  achieve  power 
powertrain 
cooperation  in  different  driving  situations  [11,  12].  Three 
categories  of  EMSs  are  presented  to  resolve  the  energy 
management  (power  split)  problems  for  different  powertrain 
architectures until now, which are the rule-, optimization- and 
learning-based  policies  [13-15].  How  to  implement  these 
excellent  EMSs  into  real-world  driving  environments  to 
construct  human-like  power  split  controls  is  still  a  research 
priority in the current energy management field. 

Motivated by the remarkable development of deep learning 
and reinforcement learning (RL) in artificial intelligence, deep 
reinforcement  learning  (DRL)  is  regarded  as  a  promising 
methodology to derive intelligent EMS for hybrid powertrain 
[16, 17]. For example, the authors in [18] applied well-known 
deep Q-learning (DQL) to address the continuous optimization 
control problem in energy management and achieve impressive 
performance. To improve the convergence rate, Qi et al. [19] 
used  dueling  DQN  to  solve  the  energy  management  problem 
for  a  parallel  powertrain,  and  the  proposed  control  policy  is 
proven to be better than onboard binary control. Furthermore, 
Ref. [20] and [21] utilized a deep deterministic policy gradient 
(DDPG) to derive the optimal EMS for Prius and series-parallel 
PHEV,  respectively.  The  induced  DDPG-enabled  control 
policy is compared with the conventional DRL methods and is 
certified  to  have  a  better  fuel  economy.  However,  since 
multiple neural networks (NN) exist in these DRL approaches, 
it  is  a  time-consuming  process  to  obtain  mutable  DRL-based 
EMS. Hence, these derived control policies are not able to be 
applied in real-world driving environments. 

 
  
 
 
 
 
2 

Fig. 1. Human-like energy management system with expert knowledge for HEV.

In this article, a human-like energy management framework  
is constructed depending on the DRL technique and collected 
historical  driving  data,  as  depicted  in  Fig.  1.  First,  a  se-
ries-parallel  hybrid  powertrain  is  modeled  and  treated  as  the 
testified  target  of  the  presented  EMS.  Then,  the  modified 
DDPG-enabled control construction is introduced, wherein the 
dynamic  programming  (DP)-based  optimal  global  control  or 
collected  historical  driving  data  from  experienced  drivers  are 
considered as expert knowledge to enhance the searching space 
for control actions. By doing this, a human-like driving policy 
is generated and its optimality and convergence rate are guar-
anteed. Finally, the standard and real-world driving cycles are 
employed  to  evaluate  the  optimality  and  adaptability  of  the 
proposed human-like EMS. 

Three perspectives of contributions and innovations are in-
cluded in this paper: 1) a human-like EMS is presented based 
on the DRL approach and collected historical driving data; 2) a 
modified  DDPG  framework  is  founded,  and  it  is  embedded 
with  DP-enabled  optimal  control  actions;  3)  Guided  by  the 
real-world driving behaviors, the proposed human-like control 
policy is able to adapt to different driving cycles. This paper is 
one attempt to combine the DRL method and real-world driving 
data,  which  is  one  possible  solution  for  online  or  real-time 
power split controls for HEVs/PHEVs. 

The following construction of this paper is arranged as fol-
lows:  the  series-parallel  hybrid  powertrain  and  its  relevant 
energy  management  problem  is  given  in  Section  II.  The  im-
proved DDPG technique, DP method, and collected real-world 
driving data are introduced in Section III. Section IV describes 
the simulation results to estimate the proposed control structure, 
and  its  relevant  optimality  and  adaptability  are  analyzed  and 
certified. Finally, the conclusion and future work are summa-
rized in Section V. 

II.  POWERTRAIN MODELING AND ENERGY MANAGEMENT 
PROBLEM 

The studied hybrid powertrain has a series-parallel topology, 
and its control architecture is shown in Fig. 2 [22].  The primary 
components  are  the  internal-combustion  engine  (ICE),  lithi-
um-ion  battery  pack,  traction  motor,  and  integrated  starter 
generator (ISG).  The energy management controller is capable 
of  distributing  the  output  power  between  ICE  and  battery  in 
order  to  realize  the  optimization  control  objectives.  The  fol-
lowing  content  elucidates  the  mathematical  completeness  of 
this powertrain  modeling.  The  values  of  the  main  parameters 
are exhibited in Table I. 

A.   Powertrain Modeling 
As the driving cycle is given in advance, the vehicle speed 
and acceleration are determined. Thus, the power demand Pd of 
the whole powertrain is represented by three parts as follow: 

                                   (1) 

                                     (2) 

                                 (3) 

                                                 (4) 

where  Pr,  Pa,  and  Pi  are  the  powers  related  to  the  rolling  re-
sistance, aerodynamic drag, and inertial force, respectively. Mv 
is the curb weight, g is the gravity coefficient, ρ is the air den-
sity, Aa is the frontal area, f and CD are the coefficients of rolling 
resistance and aerodynamic drag, respectively. v and a are the 
vehicle speed and acceleration and they are mutable in different 
driving cycles, and hence the power demand changes with the 

DP-based Optimal Control Historical Driving ExperiencesExpert Knowledge to Accelerate Control Action SearchingSeries-parallel PowertrainModified DDPG AlgorithmDRL-based Energy Management Control For Hybrid PowertrainStateRewardActiondraiPPPP=++=rvPfMgv21=2aaDPACvv=ivPMav 
 
 
driving cycle. It implies that the energy management controller 
should adjust its EMS to adapt to different driving conditions. 

Fig. 2. Configuration of studied series-parallel powertrain topology [22].  
The power request is supplied by two onboard energy storage 
systems,  battery,  and  ICE.  In  the  battery,  the  state  of  charge 
(SoC)  indicates  the  remaining  electric  capacity  after  running 
the specific driving cycle. The value of SoC ranges from 0 to 1 
and its variation can be computed by: 

                                         (5) 

where Qc and Ib are the nominal capacity and output current of 
the  battery,  respectively.  To  simulate  the  battery  as  internal 
resistance model [23], the output power Pb and voltage Ub of 
battery are written as follow: 

                                                  (6) 

                                            (7) 

where Voc is the open-circuit voltage, and r0 implies the internal 
resistance. Incorporating the Eq. (5) to (7), the variation of SoC 
is able to be described as: 

      (8) 

To maintain the long service life of the battery, it should not be 
overcharged  or  over-discharged.  Hence,  the  value  of  SoC  is 
assumed  to  belong  to  [0.2,  0.9]  in  this  work.  For  this  se-
ries-parallel powertrain, the nominal capacity of the battery is 
8.1Ah,  with  the  nominal  voltage  of  200V  and  internal  re-
sistance  of  0.25Ω.  Since  the  output  power  of  the  battery  is 
decided by power demand and power of ICE, the SoC can be 
calculated by Eq. (8) at arbitrary time instant. Therefore, SoC is 
chosen  as  one  state  variable  to  suggest  the  performance  of 
control actions. 

TABLE I [22] 
POWERTRAIN PARAMETERS FOR THE STUDIED SERIES-PARALLEL HEV  

Symbol 

Implication 

Curb weight 

Air density 

Values 

1325 kg 

1.225 kg/m3 

Rolling resistance coefficient 

0.012 

Frontal area 

2.16 m2 

Aerodynamic drag coefficient 

0.26 

Gravity coefficient 

Open circuit voltage 

SoCref 

Charge sustaining value 

9.8 m/s2 

150 V 

0.6 

Mv 

ρ 

f 

Aa 

CD 

g 

Voc 

The significant parameter in ICE is the fuel consumption rate, 

3 

which reflects the fuel economy of HEV directly. Modeled by 
static map method, the fuel consumption rate mf is determined 
by the speed and torque of ICE as follow: 

                                    (9) 

where Te and ωe are the torque and speed of ICE, respectively. fe 
is  always  represented  by  the  look-up  table  function,  which 
means  the  brake  specific  fuel  consumption  (BSFC)  curve  of 
ICE is mutable in this work.  

The speed range of ICE in this series-parallel powertrain is 
1000 rpm to 4500 rpm, the peak power is 57 kW at 5000 rpm, 
and  peak  torque  is  115  Nm  at  4200  rpm.  Assuming  power 
demand is known a priori, the output power of ICE can decide 
battery power and SoC, and thus the ICE power is selected as 
control actions in this article with continuous space [22]. 

B.  Energy Management Formulation 
The energy management problem of HEV is converted into 
an  optimization  control  problem  with  a  predefined  objective 
and several constraints. The goal of this problem is to search a 
control sequence to achieve the best control performance. The 
control objective is usually represented by the cost function J 
over a finite time horizon as follow: 

   (10) 

wherein the first term is the fuel economy and the second one 
means the charge sustaining restraint. δ is a positive weighting 
parameter  to  tune  these  two  goals  in  the  cost  function  and  it 
equals 500 in this work. SoCref is a pre-defined factor to guar-
antee the final value of SoC close to its initial value, and it is 
settled as 0.7 in this paper. 

Generally, the cost function is affected by the state variable s
∈S and a∈A. In this paper, the state variables are the vehicle 
speed, acceleration and SoC, and the control action is the power 
of ICE: 

                      (11) 

                             (12) 

To  choose  the  best  control  actions  from  a  normal  working 
area, the defined optimization control problem should follow a 
couple of constraints. It implies that the ICE, battery, ISG, and 
traction motor need work in a reasonable range as: 

                      (13) 

       (14) 

where  the  symbol  max  and  min  denote  the  maximum  and 
minimum value of the relevant variables. The subscript g and e 
indicate the torque and speed related to the generator and motor, 
respectively. In this work, the road scope of the driving cycle 
and influence of temperature for battery characteristics are not 

EngineISGTraction MotorBattery PackDC/DC Switching/bcSoCIQ=−bbbPUI=oc0bbUVIr=−200(4)/(2)ococbcSoCVVrPQr=−−−(,)feeemfT=20[()(())]()()()0()TfSoCrefrefSoCrefJmttdtSoCtSoCSoCtSoCtSoCtSoC=+−=={,,[0.2,0.9]}SvaSoC={[0,57]}eAPminmax,min,max()()bbbSoCSoCtSoCPPtP,min,max,min,max(),,,(),,,xxxxxxtxmgeTTtTxmge== 
 
considered. In the next section, the DDPG algorithm is intro-
duced to derive the human-like EMS with expert knowledge. 
lower and upper bounds of the variables. 

III.  DRL ALGORITHM AND EXPERT KNOWLEDGE 

This  section  aims  to  construct  the  DRL  framework  for  the 
energy management problem of the series-parallel powertrain, 
wherein  the  particular  DDPG  algorithm  is  illuminated.  To 
enhance  the  DDPG  algorithm,  the  optimal  control  strategy 
obtained from DP is taken as expert knowledge to narrow the 
searching space of control actions. Furthermore, the collected 
process of a real-world driving dataset from experienced driv-
ers is described. For an individual driving cycle, the appropriate 
collected ICE behaviors are also treated as expert knowledge to 
train the human-like energy management policy. 

A.  Dynamic Programming Method  
According to Bellman’s principle of optimality, DP enables 
to  acquire  the  optimal global  controls  in  a  multi-step horizon 
optimization control problem by an exhaustive search of state 
variables  and  control  actions  [24].  Many  attempts  have  exe-
cuted  to  apply  DP  to  address  the  HEV’s  energy  management 
problems  [25-27].  However,  limited  by  the  curse  of  dimen-
sionality,  DP  cannot  solve  the  problem  with  a  large  search 
space. Hence, the DP-based control strategy is often regarded 
as a benchmark to evaluate other EMSs. 

Bellman’s principle of optimality indicates that for a N-steps 
optimization control problem, if a(m) (m = 1, 2, …, N) is the 
optimal control sequence over the whole time interval, then the 
truncated sequence a(n) (n = k+1, k+2, …, N) is still the optimal 
control sequence for time horizon from k+1 to N. For example, 
the cost function in Eq. (10) can be rewritten as follow: 

                     (15) 

where φ is a restrictive function on the final value of state var-
iables (SoC), and L is named instantaneous cost function, which 
is the sum of fuel consumption rate and charge sustaining re-
straint.  Then,  the  optimal  cost  function  J*  is  minimizing  or 
maximizing the cost function in Eq. (15) as: 

               (16) 

To search the optimal control at each time step, the Eq. (16) 

can be further formulated as the recursive expression as: 

              (17) 

Executing Eq. (17) through a backward iteration process, the 
2, …, a*
optimal control policy [a*
N] is able to be computed. 
2, …, s*
1, s*
Then the related state variable [s*
N] can be calcu-
lated by a recursive forward process. 

1, a*

In this work, as the DP algorithm could achieve the optimal 
control  policy  for  an  exceptional  driving  cycle,  it  can  be  im-
ported into the DRL framework to be the expert knowledge. It 
implies that the relevant DRL algorithm would not search the 
optimal  controls  from  the  original  space,  as  alternatives,  it 
would gain the controls from the DP-based control policy. By 
doing this, the search space is narrowed, which would improve 
the computational time and convergence rate. 

4 

B.  Historical Driving Experiences 
In real-world driving environments, the experienced drivers 
could manage the driving strategy (indicates power split con-
trols  in  this  article)  depend  on  the  driving  situations.  For  ex-
ample,  the  drivers  would  make  braking  decisions  beforehand 
when they cannot drive through the traffic lights at intersections. 
Moreover,  they  may  make  the  ICE  work  in  the  area  of  high 
efficiency regularly to promote the fuel economy on the high-
way. 

Inspired by these experienced and mature driving policies for 
HEVs, a series of experiments are designed to collect the his-
torical  driving  dataset  in  HEVs.  The  experiments  are  imple-
mented on several kinds of HEVs in Beijing, China [28]. The 
collected  data  includes  vehicle  velocity,  acceleration,  travel 
distance, global position system (GPS) data of the vehicle, the 
output power of ICE  and  battery,  torque,  and  speed  of  motor 
and  generator.  The  mentioned  collected  data  is  related  to  the 
daily life of each HEV, such as morning peak and evening peak. 
Hence,  the  obtained  data  could  contain  the  driving  cycles  in 
highway and urban driving environments.  

Fig. 3 depicts the  terminal  device  and  its  configuration for 
data  collection.  The  parameters  of  the  hybrid  powertrain  are 
recorded from the distributed CAN bus, and the geographical 
information  is  stored  by  the  GPS  module.  The  collected  data 
can be transferred into a cloud corner each day and stored as the 
Excel file. The sampling frequency of this data is 10Hz, which 
means its precision is enough for energy management research. 
Finally, the collection process lasts for 1320 days, 3885 times, 
and 45384 km. A part of them is selected and preprocessed for 
energy management research. 

Fig. 3. Terminal device for historical driving data collection [5]. 

11()(,,)NNNkkkJsLsak−==+1*1min{()(,,)}NNNkkkJsLsak−==+**(1)min{(,,)}NkNkkkJJLsak−−+=+On-Board DiagnosticMemory CardData Collection TerminalGPS ModuleGPS ModuleBench BoardWiringTerminal Device 
 
 
5 

Computational procedure of the modified DDPG algorithm with expert knowledge  

TABLE II [32] 

Modified DDPG algorithm 

1. Initialize critic network θQ and actor network θπ, memory pool K, give initial values for α, β, number of episodes M and ε 

2. Input DP-based control policy or real-world collected driving strategy (taken as expert knowledge) 

3. for the episode in the range [1, M] do 

4. Give initial values for three states v1, a1, SoC1 

5.     for t in the range [0, T] do  

6.        Choose action at = π(st | θπ) according to the expert knowledge (input optimal policy) and exploration noise 

7.        Receive rt  and st+1 based on current action at and state st 

8.        Store transition model (st, at, rt, st+1) in K 

9.        Sample a minibatch of transitions (st, at, rt, st+1) from K with priority experience replay 
10.      Set 

11.      Update the critic by minimizing the loss function:  

12.      Update the actor policy using the sampled policy gradient: 

13.      Store the critic and actor network for experience replay 

14.    end for loop 

15. end for loop 

In [28], the fundamental properties of the collected driving 
experiences  are  compared  and  analyzed,  such  as  acceleration 
interval  distribution,  velocity  interval  distribution,  traction 
states and so on. The relevant results indicate the characteristics 
between standard and real-world driving cycles are extremely 
different.  Thus,  this  paper  would  choose  several  real-world 
driving cycles to derive human-like EMS. It means the driving 
policy (especially the power split controls between battery and 
ICE)  would  be  the  guidance  for  controls  search  in  the  DRL 
framework. By doing this, the search space of control actions is 
narrowed too, and the optimal control actions would only select 
from the collected control in a real-world environment. 

C.  Modified DDPG Algorithm 
Unlike other machine learning methods, RL discusses how to 
choose the best control actions based on the interaction between 
an  agent  and  its  environment  [29].  Its  supreme  advantage  is 
realizing self-improvement through a learning process, which 
is  usually  a  trial-and-error  search.  It  represents  that  the  RL 
agent  will  try  the  control  actions  and  evaluate  them  by  the 
relevant rewards. It is essential that each action in RL affects 
not only the immediate but also the subsequent rewards.  

Markov decision processes (MDPs) are a typical formaliza-
tion of sequential decision making, in which the actions influ-
ence  the  immediate  and  subsequent  rewards  and  states  [30]. 
Hence, MDP is an ideal mathematical formulation of RL, and 
the interaction between the agent and environment is expressed 
as a <S, A, P, R, β>. The S and A are the set of state variables 
and control actions given in Eq. (11) and (12). p∈P represents 
the transition model of the state in the environment, and r∈R is 
the reward model to estimate the selection of action. Finally, β 
is  a  discount  factor  to  balance  the  significance  of  immediate 
and subsequent rewards. 

The objective of the RL algorithm is to determine a control 
policy π to maximum the cumulated rewards. In general, the RL 

algorithms  are  cast  into  policy-based  (policy  gradients  algo-
rithm) and value-based ones (Q-learning and Sarsa algorithms) 
[29]. Two value functions are defined to represent the cumu-
lated rewards in value-based algorithms as follow: 

                      (18) 

   (19) 

where Vπ and Qπ are the value functions followed by the control 
policy π, and p(st+1|st, at) is the transition dynamic from st to st+1. 
Then, the standard Q-learning algorithm is settled to update the 
action-value function Q(s, a) for control action selection: 

   (20) 

where  α  is  the  learning  rate.  In  each  time  step,  the  ε-greedy 
policy is applied to choose the control action. It implies that the 
agent would exploit the best action until now with probability 
1-ε, and explore the environment with probability ε. However, 
it  is  difficult  for  Q-learning  to  handle  the  continuous  action 
spaces because the search space is too ample for greedy policy. 
Instead,  DQL  [31]  is  proposed  to  function  approximately  the 
action-value function Q with a neural network (NN), wherein 
the loss function is described as: 

              (21) 

where l(θQ) is the loss function and θQ is the parameter in the 
neural network.   

To handle the continuous space of control action, the policy 
gradient algorithm embedded NN is used to calculate the loss 
function. Hence, actor critic method is presented to combine the 
Q-learning and policy gradient algorithms [32], the actor aims 
to generate action by interacting with the environment, and the 
critic is responsible for evaluating the action. Finally, DDPG is 

11(,)(,(|)|)QtttttyrsaQsa++=+11()[((,|))(,|)(,|)]QQQQttttttlErQsaQsaQsa++=+−11[(,|)][(,|)(|)]QQttatttLEQsaEQsas++=0()((,))TttttVsErsa==1111(,)(,)(,)(,)ttttttttttsSQsarsapssaQsa++++=++111(,)(,)[max(,)(,)]tttttttttaQsaQsarQsaQsa++++−211()[((,|))](,)(,|)QQtttQtttttlEQsayyrsaQsa++=−=+ 
 
 
 
     
     
proposed to absorb the advantages of deep Q-learning and AC. 
The  policy  gradient  of  the  loss  function  (actor-network)  is 
calculated as follow: 

work is able to obtain optimal control actions with an efficient 
learning process. 

6 

       (22) 

where ▽ indicates the gradient function. The critic network is 
approximated by the Bellman equation [32]: 

         (23) 

To modify the DDPG algorithm in this work, the DP-based 
optimal  control  policy  or  the  real-world  collected  strategy  is 
imported  to  optimize  the  search  space  of  control  actions.  To 
narrow  the  search  space,  the related  computational  efficiency 
and performance would be improved. The pseudo-code of the 
modified DDPG algorithm is displayed in Table II. In this DRL 
framework, the learning rate α is 0.001, discount factor β is 0.95, 
the  number  of  episodes  is  1000,  memory  capacity  and  batch 
size in the NN is 2000 and 64, and ε is equal to 1*0.001t (t is the 
time  step).  To  estimate  the  proposed  human-like  EMS,  the 
DDPG,  DP,  and  DQL  are  compared  in  the  next  section.  The 
optimality,  convergence,  and  adaptability  are  proven  by  dif-
ferent designed experiments. 

IV.  ANALYZATION OF SIMULATION RESULTS 

This  section  discusses  the  control  performance  of  the  pro-
posed modified DDPG-based EMS (Human-like EMS). First, 
the optimality of the presented EMS is assessed by comparing 
with  DP  on  a  standard  driving  cycle.  Then,  on  a  real-world 
driving cycle1, the human-like EMS is evaluated via analyzing 
the comparative results among DDPG, DP, and DQL. Finally, 
the learned human-like control policy is estimated on another 
real-world driving cycle2 to reveal its adaptability for different 
driving situations in real-world environments. 

A.  Optimality of Modified DDPG 

Fig. 4. SoC curves in DP and modified DDPG on the UDDS cycle. 

Fig. 5. Power distribution between ICE and battery in DP and modified DDPG.  

B.  Convergence Rate of Human-like EMS 

is  guaranteed 

theoretically.  To  demonstrate 

In  the  modified  DDPG  algorithm,  the  original  space  of 
control  actions  is limited  by the DP-based control  strategy (a 
sequence of control actions). Thus the optimality of the related 
EMS 
this 
perspective,  the  human-like  EMS  is  compared  with  DP  to 
estimate  its  optimality  on  a  standard  driving  cycle.  Fig.  4 
depicts  the  chosen  cycle  UDDS  and  two  SoC  trajectories  in 
these  two  methods.  It  can  be  discerned  that  these  two  SoC 
curves  are  very  close,  so  the  output  power  of  the  battery  is 
nearly the same (from Eq. (8), SoC is affected by Pb). 

To display the power distribution between ICE and battery, 
Fig.  5  shows  their  power  variation  on  the  studied  standard 
driving cycle (UDDS cycle). Since the ICE power is defined as 
the control action is this work, it is evident that the human-like 
EMS is almost same as the DP-based one (not all the same in 
the black circle). As DP can acquire the global optimal control 
policy  via  an  exhaustive  search  process,  the  optimality  of 
modified DDPG-enabled EMS is deduced. Compared with the 
conventional  DDPG  algorithm,  the  proposed  control  frame-

Fig. 6. Collected real-world cycle1 for human-like EMS generation. 

In this subsection, a comprehensive analysis of four EMSs is 
conducted, and these policies are derived from modified DDPG, 
conventional DDPG, DQL, and DP. The setting parameters of 
DQL  are  the  same  as  those  in  DDPG  to  ensure  equitable 
comparison  (three  DRL  methods).  To  generate  human-like 
EMS  for  the  series-parallel  powertrain,  a  real-world  driving 
cycle1 is leveraged in these four techniques, as described in Fig. 
6.  Furthermore,  the  SoC  curves  of  these  four  situations  are 
given  in  Fig.  7.  In  the  modified  DDPG  approach,  the  search 

11()[((,|))(,|)(,|)]QQttQQttttlErQsaQsaQsa++=+−11[(,|)][(,|)(|)]QttQatttLEQsaEQsas++= 
 
 
 
space of control actions is composed of DP-based control pol-
icy  and  collected  strategy  from  experienced  human  drivers. 
This  design  can  ensure  not  only  the  optimality  but  also  the 
human-like characteristics. As can be seen, the SoC trajectories 
in DQL and conventional DDPG are different from those in the 
other  two  cases,  which  indicates  the  control  policies  of  these 
four  cases  are  not  identical.  Thus,  the  convergence  rate  and 
control performance are further compared. 

Fig. 7. SoC variations of four compared methods on real-world driving cycle1. 

Fig. 8. A total reward of each episode in different DRL approaches. 
TABLE III 
EQUIVALENT FUEL CONSUMPTION FOR DIFFERENT DRL METHODS 

Techniques# 

Fuel Economy*  Training Time (hours) 

DQL 

Conventional DDPG 

Modified DDPG 

108.9 

148.87 

208.15 

10.72 

28.76 

8.24 

# A 2.90 GHz microprocessor with 7.83 GB RAM was used. 
* Equivalent fuel consumption, Unit: mpg (miles per gallon).  

In Fig. 8, the total rewards in each episode of different DRL 
techniques  are  displayed  (the  number  of  training  episodes  is 
1000).  Since  the  reward  represents  the  instantaneous  cost 
function  in  Eq.  (10),  the  proposed  human-like  EMS  could 
achieve a better fuel economy with the same training episodes. 
It is caused by the diverse search spaces of control strategy in 
these situations. Hence, it can be concluded that the modified 
DDPG  could  promote  the  fuel  economy.  Table  III  shows  the 
fuel consumption and training time of these three DRL-based 
EMSs. As the final value of SoC is not the same, the results in 
Table  III  are  the  equivalent  fuel  consumption,  in  which  the 

7 

effects of differences of final SoC are disposed [33]. It is visu-
alized  that  the  modified  DDPG  (indicates  the  relevant  hu-
man-like EMS) is capable of realizing the best control perfor-
mance (indicates fuel economy and calculative efficiency). As 
the  goal  of  human-like  EMS  is  applied  in  real-world,  the 
adaptability of this method is discussed in the next subsection. 

C.  Adaptability of Human-like EMS 

To  further  explain  the  robustness  of  the  built  human-like 
control policy, three DRL-enabled control strategies are eval-
uated on another real-world driving cycle, as depicted in Fig. 9.  
In this experiment, the driving cycle in Fig. 9 is not included in 
the training process. It indicates the critic and actor networks 
are generated from real-world driving cycle1 (Fig. 6), and they 
will be applied on the driving cycle2 in Fig. 9. The simulation 
results  could  reveal  that  the  EMS  in  Section  IV.B  would  be 
adaptive  to  this  new  driving  cycle  or  not.  The  relevant  SoC 
trajectories in these three situations are also displayed in Fig. 9. 
The different variations of these SoC imply the obtained control 
actions are not the same, which indicates the output power and 
working points of ICE in these EMSs are different.  

Fig. 9. SoC trajectories of different DRL approaches on a new driving cycle. 

Fig. 10. Working points of ICE in different DRL methods on a new cycle.  
Fig.  10  gives  the  working  area  of  ICE  in  these  three  DRL 
approaches  after 
the 
high-efficiency area is highlighted in this figure, the modified 
DDPG  propels  ICE  to  operate  continually  in  this  area.  As  a 
result, the ICE could consume less fuel to run the same driving 

training  episodes.  As 

the  same 

High Efficiency Area of ICE 
 
 
 
cycle.  It  can  be  ascribed  to  real-world  driving  experiences, 
which  help  the  ICE  and  battery  export  power  reasonably. 
Hence, the proposed human-like EMS could achieve better fuel 
economy than other methods, which demonstrates the adapta-
bility  of  the  presented  control  framework.  Furthermore,  this 
human-like EMS is possible to be applied in real-time by in-
corporating transfer learning thought, and it will be discussed in 
future work. 

V.  CONCLUSION 

The purpose of this work is to speed up the training process 
of  the  conventional  DDPG  method.  By  considering  the 
DP-based optimal control policy into the DRL framework, the 
optimality  of  the  proposed  EMS  can  be  guaranteed.  Further-
more,  as  we  know,  the  experienced  human  drivers  could 
manage power distribution according to the real-world driving 
conditions. Inspired by this motivation, the real-world driving 
dataset is collected, and a part of it is selected to construct the 
human-like  control  strategy. The  collected  power  distribution 
between ICE and battery is taken as the expert knowledge to 
guide the choice of control in the DDPG algorithm. Owing to 
this operation, the intelligent agent could learn to run the hybrid 
powertrain as a human driver does. The designed experiments 
prove the optimality, convergence rate, and adaptability of the 
founded  human-like  EMS.  Its  control  performance  is  better 
than the DQL and conventional DDPG, and further close to DP.  
Future work could focus on combining the modified DDPG 
and  transfer  learning  to  formulate  an  online  energy  manage-
ment system for hybrid vehicles. As the trained NN could be 
generalized  to  other  real-world  driving  cycles,  the  relevant 
EMS has the potential to be applied in real-time environments. 
Moreover,  the  hardware-in-the-loop (HIL)  experiment  is able 
to be further constructed to demonstrate this research direction. 

REFERENCES 

[1]  T.  Liu,  X.  Tang,  H.  Wang,  H.  Yu,  and  X  Hu,  “Adaptive  Hierarchical 
Energy  Management  Design  for  a  Plug-in  Hybrid  Electric  Vehicle,” 
IEEE Trans. Veh. Technol., vol. 68, no. 12, pp. 11513-11522, 2019. 
[2]  Y. Huang, H. Wang, A. Khajepour, B. Li, J. Ji, K. Zhao, and C. Hu, “A 
review of power management strategies and component sizing methods 
for hybrid vehicles,” Renew Sustain Energy Rev, vol. 96, pp. 132-144, 
2018. 

[3]  T.  Liu,  X.  Hu,  W.  Hu,  Y.  Zou,  “A  heuristic  planning  reinforcement 
learning-based energy management for power-split plug-in hybrid elec-
tric vehicles,” IEEE Trans. Ind. Informat., vol. 15, no. 12, pp. 6436-6445, 
2019. 

[4]  C.  Martinez,  X.  Hu,  D.  Cao,  E.  Velenis,  B.  Gao,  and  M.  Wellers,  M. 
“Energy  management  in  plug-in  hybrid  electric  vehicles:  Recent  pro-
gress and a connected vehicles perspective,” IEEE Trans. Veh. Technol., 
vol. 66, no. 6, pp. 4534-4549, 2017. 

[5]  T. Liu, H. Yu, H. Guo, Y. Qin, and Y. Zou, “Online energy management 
for  multimode  plug-in  hybrid  electric  vehicles,”  IEEE  Trans.  Ind.  In-
format., vol. 15, no. 7, pp. 4352-4361, July 2019. 

[6]  W. Enang, and C. Bannister, “Modelling and control of hybrid electric 
vehicles (A comprehensive review),” Renew Sustain Energy Rev, vol. 74, 
pp. 1210-1239, 2017. 

[7]  X. Tang, D. Zhang, T. Liu, A. Khajepour, H. Yu, and H. Wang, “Research 
on  the  energy  control  of  a  dual-motor  hybrid  vehicle  during  engine 
start-stop process,” Energy, vol. 166, pp. 1181-1193, 2019. 
P.  Zhang,  F.  Yan,  and  C.  Du,  “A  comprehensive  analysis  of  energy 
management strategies for hybrid electric vehicles based on bibliomet-
rics,” Renew Sustain Energy Rev, vol. 48, pp. 88-104, 2015. 

[8] 

[9]  L. Li, C. Yang, Y. Zhang, L. Zhang, and J. Song, “Correctional DP-based 
energy management strategy of plug-in hybrid electric bus for city-bus 
route,” IEEE Trans. Veh. Technol., vol. 64, no. 7, pp. 2792-2803, 2015. 
[10]  T.  Liu,  B.  Wang,  and  C.  Yang,  “Online  Markov  Chain-based  energy 
management for a hybrid tracked vehicle with speedy Q-learning,” En-
ergy, vol. 160, pp. 544-555, 2018. 

8 

[11]  H.  Wang,  Y.  Huang,  and  A.  Khajepour,  “Cyber-Physical  Control  for 
Energy Management of Off-road Vehicles with Hybrid Energy Storage 
Systems,” IEEE/ASME Trans. Mechantroincs, vol. 23, no. 6, pp. 2609- 
2618, 2018.   

[12]  T.  Liu,  and  X.  Hu,  “A  Bi-Level  Control  for  Energy  Efficiency  Im-
provement  of  a  Hybrid  Tracked  Vehicle,”  IEEE  Trans.  Ind.  Informat., 
vol. 14, no. 4, pp. 1616-1625, 2018. 

[13]  M.  Sabri,  K.  Danapalasingam,  and  M.  Rahmat,  “A  review  on  hybrid 
electric vehicles architecture and energy management strategies,” Renew 
Sustain Energy Rev, vol. 53, pp. 1433-1442, 2016. 

[14]  T. Liu, X. Hu, S. E. Li, and D. Cao, “Reinforcement learning optimized 
look-ahead  energy  management  of  a  parallel  hybrid  electric  vehicle,” 
IEEE/ASME Trans. Mechantroincs, vol. 22, no. 4, 1497-1507, 2017. 

[15]  X. Hu, T. Liu, X. Qi, and M. Barth, “Reinforcement Learning for Hybrid 
and Plug-In Hybrid Electric  Vehicle  Energy Management: Recent Ad-
vances and Prospects,” IEEE Trans. Ind. Electron. Magazine, vol. 13, no. 
3, pp. 16-25, 2019. 

[16]  T. Liu, Y. Zou, D. Liu, and F. Sun, “Reinforcement learning of adaptive 
energy  management  with  transition  probability  for  a  hybrid  electric 
tracked vehicle,” IEEE Trans. Ind. Electron., vol. 62, no. 12, 7837-7846, 
2015. 

[17]  Y. Zou, T. Liu, D. X. Liu, and F. C. Sun, “Reinforcement learning-based 
real-time  energy management  for a hybrid tracked vehicle,”  Appl. En-
ergy, vol. 171, pp. 372-382, 2016. 

[18]  J.  Wu,  H.  He,  J.  Peng,  Y.  Li,  and  Z.  Li,  “Continuous  reinforcement 
learning of energy management with deep Q network for a power split 
hybrid electric bus,” Applied energy, vol. 222, pp. 799-811, 2018. 
[19]  X. Qi, Y. Luo, G. Wu, K. Boriboonsomsin, and M. Barth, “Deep rein-
forcement  learning  enabled  self-learning  control  for  energy  efficient 
driving,” Transportation Research Part C: Emerging Technologies, vol. 
99, pp. 67-81, 2019. 

[20]  R. Lian, J. Peng, Y. Wu, H. Tan, and H. Zhang, “Rule-interposing deep 
reinforcement  learning  based  energy  management  strategy  for  pow-
er-split hybrid electric vehicle,” Energy, 117297, 2020. 

[21]  Y.  Wu,  H.  Tan,  J.  Peng,  H.  Zhang,  and  H.  He,  “Deep  reinforcement 
learning  of  energy  management  with  continuous  control  strategy  and 
traffic information for a series-parallel plug-in hybrid electric bus,” Ap-
plied energy, vol. 247, pp. 454-466, 2019. 

[22]  S.  Miller, 

in  Simulink 
“Hybrid  Electric  Vehicle  Model 
(https://www.github.com/mathworks/  Simscape-HEV-Series-Parallel), 
GitHub. Retrieved June 19, 2020. 

[23]  T.  Liu,  Y.  Zou,  and  D.  Liu,  “Energy  management  for  battery  electric 
vehicle with automated mechanical transmission,” International Journal 
of Vehicle Design (IJVD), vol. 70, no. 1, pp. 98-112, 2016. 

[24]  C. C. Lin, H. Peng, J. W. Grizzle, and J. M. Kang, “Power management 
strategy  for  a  parallel  hybrid  electric  truck,”  IEEE  Trans.  Cont.  Syst. 
Techn., vol. 11, no. 6, pp. 839-849, 2003. 

[25]  Y.  Zou,  T.  Liu,  F.  Sun,  H.  Peng,  “Comparative  study  of dynamic  pro-
gramming and Pontryagin’s minimum principle on energy management 
for  a  parallel  hybrid  electric  vehicle,”  Energies,  vol.  6,  no.  4,  pp. 
2305-2318, 2013. 

[26]  Z. Chen, C. C. Mi, J. Xu, X. Gong, and C. You, “Energy management for 
a  power-split  plug-in  hybrid  electric  vehicle  based  on  dynamic  pro-
gramming and neural networks,” IEEE Trans. Veh. Technol., vol. 63, no. 
4, pp. 1567-1580, 2013.  

[27]  J. Liu, J. Hagena, H. Peng, and Z. S. Filipi, “Engine-in-the-loop study of 
the stochastic dynamic programming optimal control design for a hybrid 
electric HMMWV,” International Journal of Heavy Vehicle Systems, vol. 
15, no. 2-4, pp. 309-326, 2008. 

[28]  Qingkai Yang, “Study of the driving cycle of electric  vehicles in Bei-
jing,”  Master  Dissertation,  Beijing  Institute  of  Technology,  Jun.  2017. 
(in Chinese) 

[29]  R. S. Sutton, A. G. Barto, Reinforcement learning: An introduction, MIT 

press, 2018. 

[30]  Y. Liu, S. M. Cheng, and Y. L. Hsueh, “eNB selection for machine type 
communications  using  reinforcement  learning  based  Markov  decision 
process,” IEEE Trans. Veh. Technol., vol. 66, no. 12, pp. 11330-11338, 
2017. 

[31]  D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller, 
“Deterministic policy gradient algorithms,” In ICML, June 2014 
[32]  T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, and D. 
Wierstra, “Continuous control with deep reinforcement learning,” arXiv 
preprint arXiv:1509.02971, 2015. 

[33]  C. Musardo, G. Rizzoni, Y. Guezennec, and B. Staccia, “A-ECMS: An 
adaptive algorithm for hybrid electric vehicle energy management. Eu-
ropean Journal of Control, vol. 11, no. 4-5, pp. 509-524, 2013. 

 
