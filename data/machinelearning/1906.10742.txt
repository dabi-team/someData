Machine Learning Testing:
Survey, Landscapes and Horizons

Jie M. Zhang*, Mark Harman, Lei Ma, Yang Liu

1

9
1
0
2

c
e
D
1
2

]

G
L
.
s
c
[

2
v
2
4
7
0
1
.
6
0
9
1
:
v
i
X
r
a

Abstract—This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing
(ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components
(e.g., the data, learning program, and framework), testing workﬂow (e.g., test generation and test evaluation), and application scenarios
(e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research
focus, concluding with research challenges and promising research directions in ML testing.

Index Terms—machine learning, software testing, deep neural network,

(cid:70)

1 INTRODUCTION

The prevalent applications of machine learning arouse
natural concerns about trustworthiness. Safety-critical ap-
plications such as self-driving systems [1], [2] and medical
treatments [3], increase the importance of behaviour relating
to correctness, robustness, privacy, efﬁciency and fairness.
Software testing refers to any activity that aims to detect
the differences between existing and required behaviour [4].
With the recent rapid rise in interest and activity, testing
has been demonstrated to be an effective way to expose
problems and potentially facilitate to improve the trustwor-
thiness of machine learning systems.

For example, DeepXplore [1], a differential white-box
testing technique for deep learning, revealed thousands of
incorrect corner case behaviours in autonomous driving
learning systems; Themis [5], a fairness testing technique
for detecting causal discrimination, detected signiﬁcant ML
model discrimination towards gender, marital status, or race
for as many as 77.2% of the individuals in datasets to which
it was applied.

In fact, some aspects of the testing problem for machine
learning systems are shared with well-known solutions
already widely studied in the software engineering literat-
ure. Nevertheless, the statistical nature of machine learning
systems and their ability to make autonomous decisions
raise additional, and challenging, research questions for
software testing [6], [7].

Machine learning testing poses challenges that arise
from the fundamentally different nature and construction
of machine learning systems, compared to traditional (rel-
atively more deterministic and less statistically-orientated)
software systems. For instance, a machine learning system

•

•

Jie M. Zhang and Mark Harman are with CREST, University College
London, United Kingdom. Mark Harman is also with Facebook London.
E-mail: jie.zhang, mark.harman@ucl.ac.uk
* Jie M. Zhang is the corresponding author.

Lei Ma is with Kyushu University, Japan.
E-mail: malei@ait.kyushu-u.ac.jp

• Yang Liu is with Nanyang Technological University, Singapore.

E-mail: yangliu@ntu.edu.sg

inherently follows a data-driven programming paradigm,
where the decision logic is obtained via a training procedure
from training data under the machine learning algorithm’s
architecture [8]. The model’s behaviour may evolve over
time, in response to the frequent provision of new data [8].
While this is also true of traditional software systems, the
core underlying behaviour of a traditional system does not
typically change in response to new data, in the way that a
machine learning system can.

Testing machine learning also suffers from a particularly
pernicious instance of the Oracle Problem [9]. Machine learn-
ing systems are difﬁcult to test because they are designed
to provide an answer to a question for which no previous
answer exists [10]. As Davis and Weyuker said [11], for
these kinds of systems ‘There would be no need to write
such programs, if the correct answer were known’. Much
of the literature on testing machine learning systems seeks
to ﬁnd techniques that can tackle the Oracle problem, often
drawing on traditional software testing approaches.

The behaviours of interest for machine learning systems
are also typiﬁed by emergent properties, the effects of which
can only be fully understood by considering the machine
learning system as a whole. This makes testing harder,
because it is less obvious how to break the system into
smaller components that can be tested, as units, in isolation.
From a testing point of view, this emergent behaviour has
a tendency to migrate testing challenges from the unit
level to the integration and system level. For example, low
accuracy/precision of a machine learning model is typically
a composite effect, arising from a combination of the be-
haviours of different components such as the training data,
the learning program, and even the learning framework/lib-
rary [8].

Errors may propagate to become ampliﬁed or sup-
pressed, inhibiting the tester’s ability to decide where the
fault lies. These challenges also apply in more traditional
software systems, where, for example, previous work has
considered failed error propagation [12], [13] and the sub-
tleties introduced by fault masking [14], [15]. However,
these problems are far-reaching in machine learning sys-
tems, since they arise out of the nature of the machine

 
 
 
 
 
 
learning approach and fundamentally affect all behaviours,
rather than arising as a side effect of traditional data and
control ﬂow [8].

For these reasons, machine learning systems are thus
sometimes regarded as ‘non-testable’ software. Rising to
these challenges, the literature has seen considerable pro-
gress and a notable upturn in interest and activity: Figure 1
shows the cumulative number of publications on the topic
of testing machine learning systems between 2007 and June
2019 (we introduce how we collected these papers in Sec-
tion 4.2). From this ﬁgure, we can see that 85% of papers
have appeared since 2016, testifying to the emergence of
new software testing domain of interest: machine learning
testing.

Figure 1: Machine Learning Testing Publications (accumu-
lative) during 2007-2019

In this paper, we use the term ‘Machine Learning Test-
ing’ (ML testing) to refer to any activity aimed at detect-
ing differences between existing and required behaviours
of machine learning systems. ML testing is different from
testing approaches that use machine learning or those that
are guided by machine learning, which should be referred
to as ‘machine learning-based testing’. This nomenclature
accords with previous usages in the software engineering
literature. For example, the literature uses the terms ‘state-
based testing’ [16] and ‘search-based testing’ [17], [18] to
refer to testing techniques that make use of concepts of state
and search space, whereas we use the terms ‘GUI testing’
[19] and ‘unit testing’ [20] to refer to test techniques that
tackle challenges of testing GUIs (Graphical User Interfaces)
and code units.

This paper seeks to provide a comprehensive survey
of ML testing. We draw together the aspects of previous
work that speciﬁcally concern software testing, while sim-
ultaneously covering all types of approaches to machine
learning that have hitherto been tackled using testing. The
literature is organised according to four different aspects:
the testing properties (such as correctness, robustness, and
fairness), machine learning components (such as the data,
learning program, and framework), testing workﬂow (e.g.,
test generation, test execution, and test evaluation), and ap-
plication scenarios (e.g., autonomous driving and machine
translation). Some papers address multiple aspects. For such

2

papers, we mention them in all the aspects correlated (in
different sections). This ensures that each aspect is complete.
Additionally, we summarise research distribution (e.g.,
among testing different machine learning categories),
trends, and datasets. We also identify open problems and
challenges for the emerging research community working
at the intersection between techniques for software testing
and problems in machine learning testing. To ensure that
our survey is self-contained, we aimed to include sufﬁcient
material to fully orientate software engineering researchers
who are interested in testing and curious about testing
techniques for machine learning applications. We also seek
to provide machine learning researchers with a complete
survey of software testing solutions for improving the trust-
worthiness of machine learning systems.

There has been previous work that discussed or sur-
veyed aspects of the literature related to ML testing. Hains et
al. [21], Ma et al. [22], and Huang et al. [23] surveyed secure
deep learning, in which the focus was deep learning security
with testing being one of the assurance techniques. Masuda
et al. [24] outlined their collected papers on software quality
for machine learning applications in a short paper. Ishi-
kawa [25] discussed the foundational concepts that might
be used in any and all ML testing approaches. Braiek and
Khomh [26] discussed defect detection in machine learning
data and/or models in their review of 39 papers. As far as
we know, no previous work has provided a comprehensive
survey particularly focused on machine learning testing.

In summary, the paper makes the following contribu-

tions:

1) Deﬁnition. The paper deﬁnes Machine Learning Test-
ing (ML testing), overviewing the concepts, testing work-
ﬂow, testing properties, and testing components related to
machine learning testing.

2) Survey. The paper provides a comprehensive sur-
vey of 144 machine learning testing papers, across various
publishing areas such as software engineering, artiﬁcial
intelligence, systems and networking, and data mining.

3) Analyses. The paper analyses and reports data on
the research distribution, datasets, and trends that charac-
terise the machine learning testing literature. We observed
a pronounced imbalance in the distribution of research
efforts: among the 144 papers we collected, around 120
of them tackle supervised learning testing, three of them
tackle unsupervised learning testing, and only one paper
tests reinforcement learning. Additionally, most of them (93)
centre on correctness and robustness, but only a few papers
test interpretability, privacy, or efﬁciency.

4) Horizons. The paper identiﬁes challenges, open prob-
lems, and promising research directions for ML testing, with
the aim of facilitating and stimulating further research.

Figure 2 depicts the paper structure. More details of the

review schema can be found in Section 4.

2 PRELIMINARIES OF MACHINE LEARNING

This section reviews the fundamental terminology in ma-
chine learning so as to make the survey self-contained.

Machine Learning (ML) is a type of artiﬁcial intelli-
gence technique that makes decisions or predictions from
data [27], [28]. A machine learning system is typically

23556889122147118144YearNumber of ML testing publications05010015020072008200920102011201220132014201520162017201820193

ing data, the learning program, and frameworks.

There are different types of machine learning. From the
perspective of training data characteristics, machine learn-
ing includes:
Supervised learning: a type of machine learning that learns
from training data with labels as learning targets. It is the
most widely used type of machine learning [34].
Unsupervised learning: a learning methodology that learns
from training data without labels and relies on understand-
ing the data itself.
Reinforcement learning: a type of machine learning where
the data are in the form of sequences of actions, observa-
tions, and rewards, and the learner learns how to take ac-
tions to interact in a speciﬁc environment so as to maximise
the speciﬁed rewards.

Let X = (x1, ..., xm) be the set of unlabelled training
data. Let Y = (c(x1), ..., c(xm)) be the set of labels cor-
responding to each piece of training data xi. Let concept
C : X → Y be the mapping from X to Y (the real pattern).
The task of supervised learning is to learn a mapping
pattern, i.e., a model, h based on X and Y so that the learned
model h is similar to its true concept C with a very small
generalisation error. The task of unsupervised learning is to
learn patterns or clusters from the data without knowing
the existence of labels Y.

Reinforcement

learning guides and plans with the
learner actively interacting with the environment to achieve
a certain goal. It is usually modelled as a Markov decision
process. Let S be a set of states, A be a series of actions.
Let s and s(cid:48) be two states. Let ra(s, s(cid:48)) be the reward after
transition from s to s(cid:48) with action a ∈ A. Reinforcement
learning is to learn how to take actions in each step to
maximise the target awards.

Machine learning can be applied to the following typical

tasks [28]1:

1) Classiﬁcation: to assign a category to each data in-

stance; E.g., image classiﬁcation, handwriting recognition.

2) Regression: to predict a value for each data instance;

E.g., temperature/age/income prediction.

3) Clustering: to partition instances into homogeneous
regions; E.g., pattern recognition, market/image segmenta-
tion.

4) Dimension reduction: to reduce the training complex-

ity; E.g., dataset representation, data pre-processing.

5) Control: to control actions to maximise rewards; E.g.,

game playing.

Figure 3 shows the relationship between different cat-
egories of machine learning and the ﬁve machine learning
tasks. Among the ﬁve tasks, classiﬁcation and regression
belong to supervised learning; Clustering and dimension
reduction belong to unsupervised learning. Reinforcement
learning is widely adopted to control actions, such as to
control AI-game players to maximise the rewards for a game
agent.

In addition, machine learning can be classiﬁed into clas-
sic machine learning and deep learning. Algorithms like
Decision Tree [35], SVM [27], linear regression [36], and
Naive Bayes [37] all belong to classic machine learning.

1 These tasks are deﬁned based on the nature of the problems solved
instead of speciﬁc application scenarios such as language modelling.

Figure 2: Tree structure of the contents in this paper

composed from following elements or terms. Dataset: A set
of instances for building or evaluating a machine learning
model.

At the top level, the data could be categorised as:
• Training data: the data used to ‘teach’ (train) the al-

gorithm to perform its task.

• Validation data: the data used to tune the hyper-

parameters of a learning algorithm.

• Test data: the data used to validate machine learning

model behaviour.

Learning program: the code written by developers to build
and validate the machine learning system.
Framework: the library, or platform being used when build-
ing a machine learning model, such as Pytorch [29], Tensor-
Flow [30], Scikit-learn [31], Keras [32], and Caffe [33].

In the remainder of this section, we give deﬁnitions for

other ML terminology used throughout the paper.
Instance: a piece of data recording the information about an
object.
Feature: a measurable property or characteristic of a phe-
nomenon being observed to describe the instances.
Label: value or category assigned to each data instance.
Test error: the difference ratio between the real conditions
and the predicted conditions.
Generalisation error: the expected difference ratio between
the real conditions and the predicted conditions of any valid
data.
Model: the learned machine learning artefact that encodes
decision or prediction logic which is trained from the train-

Machine Learning TestingIntroduction of MLTWays of Organising Related WorkAnalysis and ComparisonResearch DirectionsPreliminaries of Machine LearningDefinitionComparison with software testing Contents of Machine Learning TestingHow to test: MLT workflowWhere to test: MLT componentsWhat to test: MLT propertiesTesting workflow (how to test)Testing properties (what to test)Test input generationTest oracle generationTest adequacy evaluationBug report analysisDebug and repairCorrectnessModel RelevanceRobustness&SecurityEfficiencyFairnessInterpretabilityApplication ScenarioTesting components (where to test)Data testingLearning program testingFramework testingAutonomous drivingMachine translationNatural language inferenceMachine learning categoriesData setsOpen-source toolsMachine learning propertiesTest prioritisation and reductionPrivacytesting, because it is not only the code that may contain
bugs, but also the data. When we try to detect bugs in data,
one may even use a training program as a test input to check
some properties required for the data.

4

3.2 ML Testing Workﬂow

ML testing workﬂow is about how to conduct ML testing
with different testing activities. In this section, we ﬁrst
brieﬂy introduce the role of ML testing when building
ML models, then present the key procedures and activities
in ML testing. We introduce more details of the current
research related to each procedure in Section 5.

3.2.1 Role of Testing in ML Development

Figure 4 shows the life cycle of deploying a machine learn-
ing system with ML testing activities involved. At the very
beginning, a prototype model is generated based on his-
torical data; before deploying the model online, one needs
to conduct ofﬂine testing, such as cross-validation, to make
sure that the model meets the required conditions. After
deployment, the model makes predictions, yielding new
data that can be analysed via online testing to evaluate how
the model interacts with user behaviours.

There are several reasons that make online testing essen-
tial. First, ofﬂine testing usually relies on test data, while test
data usually fails to fully represent future data [42]; Second,
ofﬂine testing is not able to test some circumstances that
may be problematic in real applied scenarios, such as data
loss and call delays. In addition, ofﬂine testing has no access
to some business metrics such as open rate, reading time,
and click-through rate.

In the following, we present an ML testing workﬂow
adapted from classic software testing workﬂows. Figure 5
shows the workﬂow,
including both ofﬂine testing and
online testing.

3.2.2 Ofﬂine Testing

The workﬂow of ofﬂine testing is shown by the top dotted
rectangle of Figure 5. At the very beginning, developers
need to conduct requirement analysis to deﬁne the expecta-
tions of the users for the machine learning system under test.
In requirement analysis, speciﬁcations of a machine learning
system are analysed and the whole testing procedure is
planned. After that, test inputs are either sampled from the
collected data or generated based on a speciﬁc purpose. Test
oracles are then identiﬁed or generated (see Section 5.2 for
more details of test oracles in machine learning). When the
tests are ready, they need to be executed for developers to
collect results. The test execution process involves building
a model with the tests (when the tests are training data) or
running a built model against the tests (when the tests are
test data), as well as checking whether the test oracles are
violated. After the process of test execution, developers may
use evaluation metrics to check the quality of tests, i.e., the
ability of the tests to expose ML problems.

The test execution results yield a bug report to help
developers to duplicate, locate, and solve the bug. Those
identiﬁed bugs will be labelled with different severity and
assigned for different developers. Once the bug is debugged
and repaired, regression testing is conducted to make sure

Figure 3: Machine learning categories and tasks

Deep learning [38] applies Deep Neural Networks (DNNs)
that uses multiple layers of nonlinear processing units for
feature extraction and transformation. Typical deep learning
algorithms often follow some widely used neural network
structures like Convolutional Neural Networks (CNNs) [39]
and Recurrent Neural Networks (RNNs) [40]. The scope of
this paper involves both classic machine learning and deep
learning.

3 MACHINE LEARNING TESTING

This section gives a deﬁnition and analyses of ML testing.
It describes the testing workﬂow (how to test), testing
properties (what to test), and testing components (where
to test).

3.1 Deﬁnition

A software bug refers to an imperfection in a computer
program that causes a discordance between the existing
and the required conditions [41]. In this paper, we refer the
term ‘bug’ to the differences between existing and required
behaviours of an ML system2.
Deﬁnition 1 (ML Bug). An ML bug refers to any imperfec-
tion in a machine learning item that causes a discordance
between the existing and the required conditions.

We deﬁne ML testing as any activity aimed to detect ML

bugs.
Deﬁnition 2 (ML Testing). Machine Learning Testing (ML
testing) refers to any activity designed to reveal machine
learning bugs.

The deﬁnitions of machine learning bugs and ML testing
indicate three aspects of machine learning: the required
conditions, the machine learning items, and the testing
activities. A machine learning system may have different
types of ‘required conditions’, such as correctness, robust-
ness, and privacy. An ML bug may exist in the data, the
learning program, or the framework. The testing activities
may include test input generation, test oracle identiﬁcation,
test adequacy evaluation, and bug triage. In this survey, we
refer to the above three aspects as testing properties, testing
components, and testing workﬂow, respectively, according
to which we collect and organise the related work.

Note that a test input in ML testing can be much more
diverse in its form than that used in traditional software

2 The existing related papers may use other terms like ‘defect’ or ‘issue’.
This paper uses ‘bug’ as a representative of all such related terms
considering that ‘bug’ has a more general meaning [41].

classic learningdeep learningapplied machine learningtheoretical machine learningmachine learningclassificationalgorithmsupervised learningunsupervised learningreinforcement learningresearchcomponentdataframeworktraining programMachine Learning Testingsupervised learning testing(106/108 ) unsupervised learningtesting(2/108)reinforcement learningtestingclassificationregressionclusteringdimension reductioncontrolmachine learningsupervised learningunsupervised learningreinforcement learningclassificationregressionclusteringdimension reductioncontrol5

ML development because the components are more closely
bonded with each other than traditional software [8], which
indicates the importance of testing each of the ML compon-
ents. We introduce the bug detection in each ML component
below:
Bug Detection in Data. The behaviours of a machine learn-
ing system largely depends on data [8]. Bugs in data affect
the quality of the generated model, and can be ampliﬁed
to yield more serious problems over a period a time [45].
Bug detection in data checks problems such as whether
the data is sufﬁcient for training or test a model (also
called completeness of the data [46]), whether the data is
representative of future data, whether the data contains a
lot of noise such as biased labels, whether there is skew
between training data and test data [45], and whether there
is data poisoning [47] or adversary information that may
affect the model’s performance.
Bug Detection in Frameworks. Machine Learning requires a
lot of computations. As shown by Figure 6, ML frameworks
offer algorithms to help write the learning program, and
platforms to help train the machine learning model, making
it easier for developers to build solutions for designing,
training and validating algorithms and models for com-
plex problems. They play a more important role in ML
development than in traditional software development. ML
Framework testing thus checks whether the frameworks of
machine learning have bugs that may lead to problems in
the ﬁnal system [48].
Bug Detection in Learning Program. A learning program
can be classiﬁed into two parts: the algorithm designed
by the developer or chosen from the framework, and the
actual code that developers write to implement, deploy, or
conﬁgure the algorithm. A bug in the learning program
may arise either because the algorithm is designed, chosen,
or conﬁgured improperly, or because the developers make
typos or errors when implementing the designed algorithm.

3.4 ML Testing Properties

Testing properties refer to what to test in ML testing:
for what conditions ML testing needs to guarantee for a
trained model. This section lists some typical properties
that the literature has considered. We classiﬁed them into
basic functional requirements (i.e., correctness and model
relevance) and non-functional requirements (i.e., efﬁciency,
robustness3, fairness, interpretability).

These properties are not strictly independent of each
other when considering the root causes, yet they are dif-
ferent external manifestations of the behaviours of an ML
system and deserve being treated independently in ML
testing.

3.4.1 Correctness

Correctness measures the probability that the ML system
under test ‘gets things right’.
Deﬁnition 3 (Correctness). Let D be the distribution of
future unknown data. Let x be a data item belonging
to D. Let h be the machine learning model that we are

3 we adopt the more general understanding from software engineering
community [49], [50], and regard robustness as a non-functional re-
quirement.

Figure 4: Role of ML testing in ML system development

the repair solves the reported problem and does not bring
new problems. If no bugs are identiﬁed, the ofﬂine testing
process ends, and the model is deployed.

3.2.3 Online Testing

Ofﬂine testing tests the model with historical data without
in the real application environment. It also lacks the data
collection process of user behaviours. Online testing com-
plements the shortage of ofﬂine testing, and aims to detect
bugs after the model is deployed online.

The workﬂow of online testing is shown by the bottom of
Figure 5. There are different methods of conducting online
testing for different purposes. For example, runtime mon-
itoring keeps checking whether the running ML systems
meet the requirements or violate some desired runtime
properties. Another commonly used scenario is to monitor
user responses, based on which to ﬁnd out whether the new
model is superior to the old model under certain application
contexts. A/B testing is one typical type of such online
testing [43]. It splits customers to compare two versions of
the system (e.g., web pages). When performing A/B testing
on ML systems, the sampled users will be split into two
groups using the new and old ML models separately.

MAB (Multi-Armed Bandit) is another online testing
approach [44]. It ﬁrst conducts A/B testing for a short time
and ﬁnds out the best model, then put more resources on
the chosen model.

3.3 ML Testing Components

To build a machine learning model, an ML software de-
veloper usually needs to collect data, label the data, design
learning program architecture, and implement the proposed
architecture based on speciﬁc frameworks. The procedure of
machine learning model development requires interaction
with several components such as data, learning program,
and learning framework, while each component may con-
tain bugs.

Figure 6 shows the basic procedure of building an ML
model and the major components involved in the process.
Data are collected and pre-processed for use; the learning
program is the code for running to train the model; the
framework (e.g., Weka, scikit-learn, and TensorFlow) offers
algorithms and other libraries for developers to choose
from, when writing the learning program.

Thus, when conducting ML testing, developers may
need to try to ﬁnd bugs in every component including
the data, the learning program, and the framework. In
particular, error propagation is a more serious problem in

formula tree graphsdatabaseformula setfrequent patternsfiltered patternsformula extractionformula transformationfrequent subtree miningobjective analysisknowledgesubjective analysishistorical dataprototype modeldeployed modelpredictionsuser requestoffline testingonline testingadapted modelonline data6

Figure 5: Idealised Workﬂow of ML testing

Figure 6: Components (shown by the grey boxes) involved
in ML model building.

testing. h(x) is the predicted label of x, c(x) is the true
label. The model correctness E(h) is the probability that
h(x) and c(x) are identical:

E(h) = P rx∼D[h(x) = c(x)]

(1)

Achieving acceptable correctness is the fundamental re-
quirement of an ML system. The real performance of an
ML system should be evaluated on future data. Since future
data are often not available, the current best practice usually
splits the data into training data and test data (or training
data, validation data, and test data), and uses test data to
simulate future data. This data split approach is called cross-
validation.

Deﬁnition 4 (Empirical Correctness). Let X = (x1, ..., xm)
be the set of unlabelled test data sampled from D.
Let h be the machine learning model under test. Let
Y (cid:48) = (h(x1), ..., h(xm)) be the set of predicted labels cor-
responding to each training item xi. Let Y = (y1, ..., ym)
be the true labels, where each yi ∈ Y corresponds to
the label of xi ∈ X . The empirical correctness of model
(denoted as ˆE(h)) is:

ˆE(h) =

1
m

m
(cid:88)

i=1

I(h(xi) = yi)

(2)

3.4.2 Model Relevance

A machine learning model comes from the combination
of a machine learning algorithm and the training data. It
is important to ensure that the adopted machine learning
algorithm be not over-complex than just needed [51]. Oth-
erwise, the model may fail to have good performance on
future data, or have very large uncertainty.

The algorithm capacity represents the number of func-
tions that a machine learning model can select (based on
the training data at hand) as a possible solution. It is
usually approximated by VC-dimension [52] or Rademacher
Complexity [53] for classiﬁcation tasks. VC-dimension is the
cardinality of the largest set of points that the algorithm
can shatter. Rademacher Complexity is the cardinality of
the largest set of training data with ﬁxed features that the
algorithm shatters.

We deﬁne the relevance between a machine learning
algorithm capacity and the data distribution as the problem
of model relevance.
Deﬁnition 5 (Model Relevance). Let D be the training
data distribution. Let R(D, A) be the simplest required
capacity of any machine learning algorithm A for D.
R(cid:48)(D, A(cid:48)) is the capacity of the machine learning al-
gorithm A(cid:48) under test. Model relevance is the difference
between R(D, A) and R(cid:48)(D, A(cid:48)).

f = |(R(D, A) − R(cid:48)(D, A(cid:48))|

(3)

Model relevance aims to measure how well a machine
learning algorithm ﬁts the data. A low model relevance
is usually caused by overﬁtting, where the model is too
complex for the data, which thereby fails to generalise to
future data or to predict observations robustly.

Of course, R(cid:48)(D, A), the minimum complexity that is
‘just sufﬁcient’ is hard to determine, and is typically ap-
proximate [42], [54]. We discuss more strategies that could
help alleviate the problem of overﬁtting in Section 6.2.

3.4.3 Robustness

where I is the indicator function; a predicate returns 1 if p is
true, and returns 0 otherwise.

Robustness is deﬁned by the IEEE standard glossary of
software engineering terminology [55], [56] as: ‘The degree

test generationmodel training/predictionbugs identifiedbug triagedebug and repairregression testingreport generationyesstartrequirements analysistest input generationtest oracle generationtest evaluationtest metric designuser splitmetric value collectionresult analysismodel selectionendmodel deploymentNooffline testingonline testingDATAdata collectiondata preparation algorithm selection/designlearning program generationmodel training FRAMEWORKLEARNING PROGRAMformula FIFA1SUMC1A1A1+C2formula F: =IF(A1,SUM(B1,B2),C1)+C2formula FIFrangeSUMrangerangerangemathrangeHLOOKUPROWIFERRORHLOOKUPANDISNUMBERIFISNUMBERIFORGETPIVOTDATAGETPIVOTDATAIFSUMIFCOUNTISNUMBERIFANDIFISBLANKGETPIVOTDATACONCATENATEINDEXMATCHDATEYEARMONTHDAYIFVLOOKUPANDIFVLOOKUPMATCHISERRORGETPIVOTDATAIFERRORVLOOKUPISERRORVLOOKUPVLOOKUPVLOOKUP(1)(2)(3)(4)(5)(6)(8)(9)(11)(12)(13)(10)(14)(15)(17)(18)(19)(20)(16)(21)formumathCOUNTIFCOUNTIF(7)formumathto which a system or component can function correctly in
the presence of invalid inputs or stressful environmental
conditions’. Adopting a similar spirit to this deﬁnition, we
deﬁne the robustness of ML as follows:
Deﬁnition 6 (Robustness). Let S be a machine learning
system. Let E(S) be the correctness of S. Let δ(S)
be the machine learning system with perturbations on
any machine learning components such as the data, the
learning program, or the framework. The robustness
of a machine learning system is a measurement of the
difference between E(S) and E(δ(S)):

r = E(S) − E(δ(S))

(4)

Robustness thus measures the resilience of an ML sys-

tem’s correctness in the presence of perturbations.

A popular sub-category of robustness is called adversarial
robustness. For adversarial robustness, the perturbations are
designed to be hard to detect. Following the work of Katz
et al. [57], we classify adversarial robustness into local
adversarial robustness and global adversarial robustness.
Local adversarial robustness is deﬁned as follows.
Deﬁnition 7 (Local Adversarial Robustness). Let x a test
input for an ML model h. Let x(cid:48) be another test input
generated via conducting adversarial perturbation on x.
Model h is δ-local robust at input x if for any x(cid:48).

∀x(cid:48) : ||x − x(cid:48)||p≤ δ → h(x) = h(x(cid:48))

(5)

||·||p represents p-norm for distance measurement. The
commonly used p cases in machine learning testing are 0, 2,
and ∞. For example, when p = 2, i.e. ||x − x(cid:48)||2 represents
the Euclidean distance of x and x(cid:48). In the case of p = 0,
it calculates the element-wise difference between x and x(cid:48).
When p = ∞, it measures the the largest element-wise
distance among all elements of x and x(cid:48).

Local adversarial robustness concerns the robustness at
one speciﬁc test input, while global adversarial robustness
measures robustness against all inputs. We deﬁne global
adversarial robustness as follows.
Deﬁnition 8 (Global Adversarial Robustness). Let x a test
input for an ML model h. Let x(cid:48) be another test input
generated via conducting adversarial perturbation on x.
Model h is (cid:15)-global robust if for any x and x(cid:48).

∀x, x(cid:48) : ||x − x(cid:48)||p≤ δ → h(x) − h(x(cid:48)) ≤ (cid:15)

(6)

7

issues also include other aspects such as model stealing or
extraction. This survey focuses on the testing techniques
on detecting ML security problems, which narrows the
security scope to robustness-related security. We combine
the introduction of robustness and security in Section 6.3.

3.4.5 Data Privacy

Privacy in machine learning is the ML system’s ability to
preserve private data information. For the formal deﬁnition,
we use the most popular differential privacy taken from the
work of Dwork [58].
Deﬁnition 9 ((cid:15)-Differential Privacy). Let A be a randomised
algorithm. Let D1 and D2 be two training data sets that
differ only on one instance. Let S be a subset of the
output set of A. A gives (cid:15)-differential privacy if

P r[A(D1) ∈ S] ≤ exp((cid:15)) ∗ P r[A(D2) ∈ S]

(7)

In other words, (cid:15)-Differential privacy is a form of (cid:15)-
contained bound on output change in responding to single
input change. It provides a way to know whether any one
individual’s data has has a signiﬁcant effect (bounded by (cid:15))
on the outcome.

Data privacy has been regulated by law makers,
the EU General Data Protection Regula-
for example,
tion (GDPR) [59] and California Consumer Privacy Act
(CCPA) [60]. Current research mainly focuses on how to
present privacy-preserving machine learning,
instead of
detecting privacy violations. We discuss privacy-related re-
search opportunities and research directions in Section 10.

3.4.6 Efﬁciency

The efﬁciency of a machine learning system refers to its
construction or prediction speed. An efﬁciency problem
happens when the system executes slowly or even inﬁnitely
during the construction or the prediction phase.

With the exponential growth of data and complexity of
systems, efﬁciency is an important feature to consider for
model selection and framework selection, sometimes even
more important than accuracy [61]. For example, to deploy
a large model to a mobile device, optimisation, compres-
sion, and device-oriented customisation may be performed
to make it feasible for the mobile device execution in a
reasonable time, but accuracy may sacriﬁce to achieve this.

3.4.7 Fairness

3.4.4 Security

The security of an ML system is the system’s resilience
against potential harm, danger, or loss made via manipu-
lating or illegally accessing ML components.

Security and robustness are closely related. An ML sys-
tem with low robustness may be insecure: if it is less robust
in resisting the perturbations in the data to predict, the
system may more easily fall victim to adversarial attacks;
For example, if it is less robust in resisting training data per-
turbations, it may also be vulnerable to data poisoning (i.e.,
changes to the predictive behaviour caused by adversarially
modifying the training data).

Nevertheless, low robustness is just one cause of security
vulnerabilities. Except for perturbations attacks, security

Machine learning is a statistical method and is widely
adopted to make decisions, such as income prediction and
medical treatment prediction. Machine learning tends to
learn what humans teach it (i.e., in form of training data).
However, humans may have bias over cognition, further
affecting the data collected or labelled and the algorithm
designed, leading to bias problems.

The characteristics that are sensitive and need to be
protected against unfairness are called protected character-
istics [62] or protected attributes and sensitive attributes. Ex-
amples of legally recognised protected classes include race,
colour, sex, religion, national origin, citizenship, age, preg-
nancy, familial status, disability status, veteran status, and
genetic information.

Fairness is often domain speciﬁc. Regulated domains
include credit, education, employment, housing, and public
accommodation4.

To formulate fairness is the ﬁrst step to solve the fairness
problems and build fair machine learning models. The liter-
ature has proposed many deﬁnitions of fairness but no ﬁrm
consensus is reached at this moment. Considering that the
deﬁnitions themselves are the research focus of fairness in
machine learning, we discuss how the literature formulates
and measures different types of fairness in Section 6.5.

3.4.8 Interpretability

Machine learning models are often applied to assist/make
decisions in medical treatment, income prediction, or per-
sonal credit assessment. It may be important for humans
to understand the ‘logic’ behind the ﬁnal decisions, so that
they can build trust over the decisions made by ML [64],
[65], [66].

The motives and deﬁnitions of interpretability are di-
verse and still somewhat discordant [64]. Nevertheless, un-
like fairness, a mathematical deﬁnition of ML interpretabil-
ity remains elusive [65]. Referring to the work of Biran and
Cotton [67] as well as the work of Miller [68], we describe
the interpretability of ML as the degree to which an observer
can understand the cause of a decision made by an ML
system.

Interpretability contains two aspects: transparency (how
the model works) and post hoc explanations (other inform-
ation that could be derived from the model) [64]. Inter-
pretability is also regarded as a request by regulations like
GDPR [69], where the user has the legal ‘right to explana-
tion’ to ask for an explanation of an algorithmic decision that
was made about them. A thorough introduction of ML inter-
pretability can be referred to in the book of Christoph [70].

3.5 Software Testing vs. ML Testing

Traditional software testing and ML testing are different
in many aspects. To understand the unique features of
ML testing, we summarise the primary differences between
traditional software testing and ML testing in Table 1.
1) Component to test (where the bug may exist): traditional
software testing detects bugs in the code, while ML testing
detects bugs in the data, the learning program, and the
framework, each of which play an essential role in building
an ML model.
2) Behaviours under test: the behaviours of traditional
software code are usually ﬁxed once the requirement is
ﬁxed, while the behaviours of an ML model may frequently
change as the training data is updated.
3) Test input: the test inputs in traditional software testing
are usually the input data when testing code; in ML testing,
however, the test inputs in may have more diverse forms.
Note that we separate the deﬁnition of ‘test input’ and
‘test data’. In particular, we use ‘test input’ to refer to the
inputs in any form that can be adopted to conduct machine
learning testing; while ‘test data’ specially refers to the
data used to validate ML model behaviour (see more in
Section 2). Thus, test inputs in ML testing could be, but are

4 To prohibit discrimination ‘in a place of public accommodation on the
basis of sexual orientation, gender identity, or gender expression’ [63].

8

not limited to, test data. When testing the learning program,
a test case may be a single test instance from the test data or
a toy training set; when testing the data, the test input could
be a learning program.
4) Test oracle: traditional software testing usually assumes
the presence of a test oracle. The output can be veriﬁed
against the expected values by the developer, and thus the
oracle is usually determined beforehand. Machine learning,
however, is used to generate answers based on a set of input
values after being deployed online. The correctness of the
large number of generated answers is typically manually
conﬁrmed. Currently, the identiﬁcation of test oracles re-
mains challenging, because many desired properties are dif-
ﬁcult to formally specify. Even for a concrete domain speciﬁc
problem, the oracle identiﬁcation is still time-consuming
and labour-intensive, because domain-speciﬁc knowledge
is often required. In current practices, companies usually
rely on third-party data labelling companies to get manual
labels, which can be expensive. Metamorphic relations [71]
are a type of pseudo oracle adopted to automatically mitig-
ate the oracle problem in machine learning testing.
5) Test adequacy criteria: test adequacy criteria are used
to provide quantitative measurement on the degree of
the target software that has been tested. Up to present,
many adequacy criteria are proposed and widely adopted
in industry, e.g., line coverage, branch coverage, dataﬂow
coverage. However, due to fundamental differences of pro-
gramming paradigm and logic representation format for
machine learning software and traditional software, new
test adequacy criteria are required to take the characteristics
of machine learning software into consideration.
6) False positives in detected bugs: due to the difﬁculty in
obtaining reliable oracles, ML testing tends to yield more
false positives in the reported bugs.
7) Roles of testers: the bugs in ML testing may exist not
only in the learning program, but also in the data or the
algorithm, and thus data scientists or algorithm designers
could also play the role of testers.

4 PAPER COLLECTION AND REVIEW SCHEMA
This section introduces the scope, the paper collection ap-
proach, an initial analysis of the collected papers, and the
organisation of our survey.

4.1 Survey Scope

An ML system may include both hardware and software.
The scope of our paper is software testing (as deﬁned in the
introduction) applied to machine learning.

We apply the following inclusion criteria when collecting
papers. If a paper satisﬁes any one or more of the following
criteria, we will include it. When speaking of related ‘aspects
of ML testing’, we refer to the ML properties, ML compon-
ents, and ML testing procedure introduced in Section 2.
1) The paper introduces/discusses the general idea of ML
testing or one of the related aspects of ML testing.
2) The paper proposes an approach, study, or tool/frame-
work that targets testing one of the ML properties or com-
ponents.
3) The paper presents a dataset or benchmark especially
designed for the purpose of ML testing.

Table 1: Comparison between Traditional Software Testing and ML Testing

Characteristics

Traditional Testing

ML Testing

Component to test
Behaviour under test
Test input
Test oracle
Adequacy criteria
False positives in bugs
Tester

code
usually ﬁxed
input data
deﬁned by developers
coverage/mutation score
rare
developer

data and code (learning program, framework)
change overtime
data or code
deﬁned by developers and labelling companies
unknown
prevalent
data scientist, algorithm designer, developer

9

4) The paper introduces a set of measurement criteria that
could be adopted to test one of the ML properties.

Some papers concern traditional validation of ML model
performance such as the introduction of precision, recall,
and cross-validation. We do not include these papers be-
cause they have had a long research history and have been
thoroughly and maturely studied. Nevertheless, for com-
pleteness, we include the knowledge when introducing the
background to set the context. We do not include the papers
that adopt machine learning techniques for the purpose
of traditional software testing and also those target ML
problems, which do not use testing techniques as a solution.
Some recent papers also target the formal guarantee on
the desired properties of a machine learning system, i.e.,
to formally verify the correctness of the machine learning
systems as well as other properties. Testing and veriﬁcation
of machine learning, as in traditional testing, have their own
advantages and disadvantages. For example, veriﬁcation
usually requires a white-box scenario, but suffers from poor
scalability, while testing may scale, but lacks completeness.
The size of the space of potential behaviours may render
current approaches to veriﬁcation infeasible in general [72],
but speciﬁc safety critical properties will clearly beneﬁt
from focused research activity on scalable veriﬁcation, as
well as testing. In this survey, we focus on the machine
learning testing. More details for the literature review of
the veriﬁcation of machine learning systems can be found
in the recent work of Xiang et al. [73].

4.2 Paper Collection Methodology

To collect the papers across different research areas as much
as possible, we started by using exact keyword searching on
popular scientiﬁc databases including Google Scholar, DBLP
and arXiv one by one. The keywords used for searching
[ML properties] means the set of ML
are listed below.
testing properties including correctness, model relevance,
robustness, efﬁciency, privacy, fairness, and interpretability.
We used each element in this set plus ‘test’ or ‘bug’ as
the search query. Similarly, [ML components] denotes the
set of ML components including data, learning program/-
code, and framework/library. Altogether, we conducted
(3 ∗ 3 + 6 ∗ 2 + 3 ∗ 2) ∗ 3 = 81 searches across the three
repositories before May 15th, 2019.

• machine learning + test|bug|trustworthiness
• deep learning + test|bug|trustworthiness
• neural network + test|bug|trustworthiness
• [ML properties]+ test|bug
• [ML components]+ test|bug
Machine learning techniques have been applied in vari-
ous domains across different research areas. As a result,

Table 2: Paper Query Results

Key Words

Hits Title Body

machine learning test
machine learning bug
machine learning trustworthiness
deep learning test
deep learning bug
deep learning trustworthiness
neural network test
neural network bug
neural network trustworthiness
[ML properties]+test
[ML properties]+bug
[ML components]+test
[ML components]+bug

Query
Snowball
Author feedback
Overall

211
28
1
38
14
2
288
22
5
294
10
77
8

-
-
-
-

17
4
0
9
1
1
10
0
1
5
0
5
2

-
-
-
-

13
4
0
8
1
1
9
0
1
5
0
5
2

50
59
35
144

authors may tend to use very diverse terms. To ensure a
high coverage of ML testing related papers, we therefore
also performed snowballing [74] on each of the related
papers found by keyword searching. We checked the related
work sections in these studies and continue adding the
related work that satisﬁes the inclusion criteria introduced
in Section 4.1, until we reached closure.

To ensure a more comprehensive and accurate survey,
we emailed the authors of the papers that were collected via
query and snowballing, and let them send us other papers
they are aware of which are related with machine learning
testing but have not been included yet. We also asked them
to check whether our description about their work in the
survey was accurate and correct.

4.3 Collection Results

Table 2 shows the details of paper collection results. The
papers collected from Google Scholar and arXiv turned out
to be subsets of those from DBLP so we only present the
results of DBLP. Keyword search and snowballing resulted
in 109 papers across six research areas till May 15th, 2019.
We received over 50 replies from all the cited authors
until June 4th, 2019, and added another 35 papers when
dealing with the author feedback. Altogether, we collected
144 papers.

Figure 7 shows the distribution of papers published in
different research venues. Among all the papers, 38.2% pa-
pers are published in software engineering venues such as
ICSE, FSE, ASE, ICST, and ISSTA; 6.9% papers are published
in systems and network venues; surprisingly, only 19.4%

of the total papers are published in artiﬁcial intelligence
venues such as AAAI, CVPR, and ICLR. Additionally, 22.9%
of the papers have not yet been published via peer-reviewed
venues (the arXiv part).

Figure 7: Publication Venue Distribution

4.4 Paper Organisation

We present the literature review from two aspects: 1) a
literature review of the collected papers, 2) a statistical
analysis of the collected papers, datasets, and tools. The
sections and the corresponding contents are presented in
Table 3.

Table 3: Review Schema

Classiﬁcation

Sec Topic

Testing Workﬂow

Test Input Generation
Test Oracle
Test Adequacy
Test Prioritisation and Reduction
Bug Report Analysis

5.1
5.2
5.3
5.4
5.5
5.6 Debug and Repair
5.7

Testing Framework and Tools

Testing Properties

6.1
Correctness
6.2 Model Revelance
6.3
6.4
6.5
6.6
6.7

Robustness and Security
Efﬁciency
Fairness
Interpretability
Privacy

Testing Components

Application Scenario

Summary&Analysis

7.1
7.2
7.3

Bug Detection in Data
Bug Detection in Learning Program
Bug Detection in Framework

8.1 Autonomous Driving
8.2 Machine Translation
8.3 Natural Language Inference

Timeline
Research Distribution among Categories
Research Distribution among Properties

9.1
9.2
9.3
9.4 Datasets
9.5 Open-source Tool Support

1) Literature Review. The papers in our collection are
organised and presented from four angles. We introduce
the work about different testing workﬂow in Section 5. In
Section 6 we classify the papers based on the ML problems
they target, including functional properties like correctness
and model relevance and non-functional properties like
robustness, fairness, privacy, and interpretability. Section 7
introduces the testing technologies on detecting bugs in
data, learning programs, and ML frameworks, libraries,
or platforms. Section 8 introduces the testing techniques

10

applied in particular application scenarios such as autonom-
ous driving and machine translation.

The four aspects have different focuses of ML testing,
each of which is a complete organisation of the total collec-
ted papers (see more discussion in Section 3.1), as a single
ML testing paper may ﬁt multiple aspects if being viewed
from different angles.
2) Statistical Analysis and Summary. We analyse and
compare the number of research papers on different ma-
chine learning categories (supervised/unsupervised/rein-
forcement
learning), machine learning structures (clas-
sic/deep learning), testing properties in Section 9. We also
summarise the datasets and tools adopted so far in ML
testing.

The four different angles for presentation of related work
as well as the statistical summary, analysis, and comparison,
enable us to observe the research focus, trend, challenges,
opportunities, and directions of ML testing. These results
are presented in Section 10.

5 ML TESTING WORKFLOW
This section organises ML testing research based on the
testing workﬂow as shown by Figure 5.

ML testing includes ofﬂine testing and online testing.
Albarghouthi and Vinitsky [75] developed a fairness spe-
ciﬁcation language that can be used for the development
of run-time monitoring, in detecting fairness issues. Such a
kind of run-time monitoring belongs to the area of online
testing. Nevertheless, current research mainly centres on
ofﬂine testing as introduced below. The procedures that are
not covered based on our paper collection, such as require-
ment analysis and regression testing and those belonging
to online testing are discussed as research opportunities in
Section 10.

5.1 Test Input Generation

We organise the test input generation research based on the
techniques adopted.

5.1.1 Domain-speciﬁc Test Input Synthesis

Test inputs of ML testing can be classiﬁed into two cat-
egories: adversarial inputs and natural inputs. Adversarial
inputs are perturbed based on the original inputs. They may
not belong to normal data distribution (i.e., maybe rarely
exist in practice), but could expose robustness or security
ﬂaws. Natural inputs, instead, are those inputs that belong
to the data distribution of a practical application scenario.
Here we introduce the related work that aims to generate
natural inputs via domain-speciﬁc test input synthesis.

DeepXplore [1] proposed a white-box differential testing
technique to generate test inputs for a deep learning system.
Inspired by test coverage in traditional software testing, the
authors proposed neuron coverage to drive test generation
(we discuss different coverage criteria for ML testing in
Section 5.2.3). The test inputs are expected to have high
neuron coverage. Additionally, the inputs need to expose
differences among different DNN models, as well as be like
real-world data as much as possible. The joint optimisation
algorithm iteratively uses a gradient search to ﬁnd a modi-
ﬁed input that satisﬁes all of these goals. The evaluation of

general3.5%Systems and Network6.9%arXiv22.9%Theory0.7%Artificial Intelligence19.4%Data Mining4.9%Security2.8%Software Engineering38.2%DeepXplore indicates that it covers 34.4% and 33.2% more
neurons than the same number of randomly picked inputs
and adversarial inputs.

To create useful and effective data for autonomous
driving systems, DeepTest [76] performed greedy search
with nine different realistic image transformations: chan-
ging brightness, changing contrast, translation, scaling, ho-
rizontal shearing, rotation, blurring, fog effect, and rain
effect. There are three types of image transformation styles
provided in OpenCV5: linear, afﬁne, and convolutional. The
evaluation of DeepTest uses the Udacity self-driving car
challenge dataset [77]. It detected more than 1,000 erroneous
behaviours on CNNs and RNNs with low false positive
rates6.

Generative adversarial networks (GANs) [78] are al-
gorithms to generate models that approximate the mani-
folds and distribution on a given set of data. GAN has
been successfully applied to advanced image transforma-
tion (e.g., style transformation, scene transformation) that
look at least superﬁcially authentic to human observers.
Zhang et al. [79] applied GAN to deliver driving scene-
based test generation with various weather conditions. They
sampled images from Udacity Challenge dataset [77] and
YouTube videos (snowy or rainy scenes), and fed them into
the UNIT framework7 for training. The trained model takes
the whole Udacity images as the seed inputs and yields
transformed images as generated tests.

Zhou et al. [81] proposed DeepBillboard to generate real-
world adversarial billboards that can trigger potential steer-
ing errors of autonomous driving systems.

To test audio-based deep learning systems, Du et al. [82]
designed a set of transformations tailored to audio in-
puts considering background noise and volume variation.
They ﬁrst abstracted and extracted a probabilistic transition
model from an RNN. Based on this, stateful testing criteria
are deﬁned and used to guide test generation for stateful
machine learning system.

To test the image classiﬁcation platform when classify-
ing biological cell images, Ding et al. [83] built a testing
framework for biological cell classiﬁers. The framework
iteratively generates new images and uses metamorphic
relations for testing. For example, they generate new images
by increasing the number/shape of artiﬁcial mitochondrion
into the biological cell images, which can arouse easy-to-
identify changes in the classiﬁcation results.

Rabin et al. [84] discussed the possibilities of testing
code2vec (a code embedding approach [85]) with semantic-
preserving program transformations serving as test inputs.
To test machine translation systems, Sun et al. [86]
automatically generate test inputs via mutating the words
in translation inputs. In order to generate translation pairs
that ought to yield consistent translations, their approach
conducts word replacement based on word embedding sim-
ilarities. Manual inspection indicates that the test generation
has a high precision (99%) on generating input pairs with
consistent translations.

5 https://github.com/itseez/opencv(2015)
6 The examples of detected erroneous behaviours are available at https:

//deeplearningtest.github.io/deepTest/.

7 A recent DNN-based method to perform image-to-image transforma-

tion [80]

11

5.1.2 Fuzz and Search-based Test Input Generation
Fuzz testing is a traditional automatic testing technique that
generates random data as program inputs to detect crashes,
memory leaks, failed (built-in) assertions, etc, with many
sucessfully application to system security and vulnerability
detection [9]. As another widely used test generation tech-
nique, search-based test generation often uses metaheuristic
search techniques to guide the fuzz process for more ef-
ﬁcient and effective test generation [17], [87], [88]. These
two techniques have also been proved to be effective in
exploring the input space of ML testing:

Odena et al. [89] presented TensorFuzz. TensorFuzz used a
simple nearest neighbour hill climbing approach to explore
achievable coverage over valid input space for Tensorﬂow
graphs, and to discover numerical errors, disagreements
between neural networks and their quantized versions, and
surfacing undesirable behaviour in RNNs.

DLFuzz, proposed by Guo et al. [90], is another fuzz test
generation tool based on the implementation of DeepXplore
with nueron coverage as guidance. DLFuzz aims to generate
adversarial examples. The generation process thus does not
require similar functional deep learning systems for cross-
referencing check like DeepXplore and TensorFuzz. Rather,
it needs only minimum changes over the original inputs
to ﬁnd those new inputs that improve neural coverage but
have different predictive results from the original inputs.
The preliminary evaluation on MNIST and ImageNet shows
that compared with DeepXplore, DLFuzz is able to generate
135% to 584.62% more inputs with 20.11% less time con-
sumption.

Xie et al. [91] presented a metamorphic transformation
based coverage guided fuzzing technique, DeepHunter,
which leverages both neuron coverage and coverage criteria
presented by DeepGauge [92]. DeepHunter uses a more
ﬁne-grained metamorphic mutation strategy to generate
tests, which demonstrates the advantage in reducing the
false positive rate. It also demonstrates its advantage in
achieving high coverage and bug detection capability.

Wicker et al. [93] proposed feature-guided test gen-
eration. They adopted Scale-Invariant Feature Transform
(SIFT) to identify features that represent an image with a
Gaussian mixture model, then transformed the problem of
ﬁnding adversarial examples into a two-player turn-based
stochastic game. They used Monte Carlo Tree Search to
identify those elements of an image most vulnerable as the
means of generating adversarial examples. The experiments
show that their black-box approach is competitive with
some state-of-the-art white-box methods.

Instead of

targeting supervised learning, Uesato et
al. [94] proposed to evaluate reinforcement learning with ad-
versarial example generation. The detection of catastrophic
failures is expensive because failures are rare. To alleviate
the consequent cost of ﬁnding such failures, the authors
proposed to use a failure probability predictor to estimate
the probability that the agent fails, which was demonstrated
to be both effective and efﬁcient.

There are also fuzzers for speciﬁc application scenarios
other than image classiﬁcations. Zhou et al. [95] combined
fuzzing and metamorphic testing to test the LiDAR obstacle-
perception module of real-life self-driving cars, and repor-
ted previously unknown software faults. Jha et al. [96]

investigated how to generate the most effective test cases
(the faults that are most likely to lead to violations of safety
conditions) via modelling the fault injection as a Bayesian
network. The evaluation, based on two production-grade
AV systems from NVIDIA and Baidu, revealed many situ-
ations where faults lead to safety violations.

Udeshi and Chattopadhyay [97] generate inputs for text
classiﬁcation tasks and produce a fuzzing approach that
considers the grammar under test as well as the distance
between inputs. Nie et al. [98] and Wang et al. [99] mutated
the sentences in NLI (Natural Language Inference) tasks to
generate test inputs for robustness testing. Chan et al. [100]
generated adversarial examples for DNC to expose its ro-
bustness problems. Udeshi et al. [101] focused much on
individual fairness and generated test inputs that highlight
the discriminatory nature of the model under test. We give
details about these domain-speciﬁc fuzz testing techniques
in Section 8.

Tuncali et al. [102] proposed a framework for testing
autonomous driving systems. In their work they compared
three test generation strategies: random fuzz test generation,
covering array [103] + fuzz test generation, and covering
array + search-based test generation (using Simulated An-
nealing algorithm [104]). The results indicated that the test
generation strategy with search-based technique involved
has the best performance in detecting glancing behaviours.

5.1.3 Symbolic Execution Based Test Input Generation

Symbolic execution is a program analysis technique to test
whether certain properties can be violated by the software
under test [105]. Dynamic Symbolic Execution (DSE, also
called concolic testing) is a technique used to automatic-
ally generate test inputs that achieve high code coverage.
DSE executes the program under test with random test
inputs and performs symbolic execution in parallel to collect
symbolic constraints obtained from predicates in branch
statements along the execution traces. The conjunction of all
symbolic constraints along a path is called a path condition.
When generating tests, DSE randomly chooses one test
input from the input domain, then uses constraint solving
to reach a target branch condition in the path [106]. DSE
has been found to be accurate and effective, and has been
the primary technique used by some vulnerability discovery
tools [107].

In ML testing, the model’s performance is decided, not
only by the code, but also by the data, and thus symbolic
execution has two application scenarios: either on the data
or on the code.

Symbolic analysis was applied to generate more effect-
ive tests to expose bugs by Ramanathan and Pullum [7].
They proposed a combination of symbolic and statistical
approaches to efﬁciently ﬁnd test cases. The idea is to
distance-theoretically abstract the data using symbols to
help search for those test inputs where minor changes in the
input will cause the algorithm to fail. The evaluation of the
implementation of a k-means algorithm indicates that the
approach is able to detect subtle errors such as bit-ﬂips. The
examination of false positives may also be a future research
interest.

When applying symbolic execution on the machine
learning code, there are many challenges. Gopinath [108]

12

listed three such challenges for neural networks in their
paper, which work for other ML modes as well: (1) the
networks have no explicit branching; (2) the networks may
be highly non-linear, with no well-developed solvers for
constraints; and (3) there are scalability issues because the
structures of the ML models are usually very complex and
are beyond the capabilities of current symbolic reasoning
tools.

Considering these challenges, Gopinath [108] introduced
DeepCheck. It transforms a Deep Neural Network (DNN)
into a program to enable symbolic execution to ﬁnd pixel
attacks that have the same activation pattern as the original
image. In particular, the activation functions in DNN follow
an IF-Else branch structure, which can be viewed as a path
through the translated program. DeepCheck is able to create
1-pixel and 2-pixel attacks by identifying most of the pixels
or pixel-pairs that the neural network fails to classify the
corresponding modiﬁed images.

Similarly, Agarwal et al. [109] apply LIME [110], a local
explanation tool that approximates a model with linear
models, decision trees, or falling rule lists, to help get the
path used in symbolic execution. Their evaluation based on
8 open source fairness benchmarks shows that the algorithm
generates 3.72 times more successful test cases than the
random test generation approach THEMIS [5].

Sun et al. [111] presented DeepConcolic, a dynamic
symbolic execution testing method for DNNs. Concrete ex-
ecution is used to direct the symbolic analysis to particular
MC/DC criteria’ condition, through concretely evaluating
given properties of the ML models. DeepConcolic explicitly
takes coverage requirements as input. The authors report
that it yields over 10% higher neuron coverage than DeepX-
plore for the evaluated models.

5.1.4 Synthetic Data to Test Learning Program
Murphy et al. [112] generated data with repeating values,
missing values, or categorical data for testing two ML
ranking applications. Breck et al. [45] used synthetic training
data that adhere to schema constraints to trigger the hidden
assumptions in the code that do not agree with the con-
straints. Zhang et al. [54] used synthetic data with known
distributions to test overﬁtting. Nakajima and Bui [113]
also mentioned the possibility of generating simple datasets
with some predictable characteristics that can be adopted as
pseudo oracles.

5.2 Test Oracle

Test oracle identiﬁcation is one of the key problems in ML
testing. It is needed in order to enable the judgement of
whether a bug exists. This is the so-called ‘Oracle Prob-
lem’ [9].

In ML testing, the oracle problem is challenging, be-
cause many machine learning algorithms are probabilistic
programs. In this section, we list several popular types of
test oracle that have been studied for ML testing, i.e., meta-
morphic relations, cross-referencing, and model evaluation
metrics.

5.2.1 Metamorphic Relations as Test Oracles
Metamorphic relations was proposed by Chen et al. [114]
to ameliorate the test oracle problem in traditional software

testing. A metamorphic relation refers to the relationship
between the software input change and output change dur-
ing multiple program executions. For example, to test the
implementation of the function sin(x), one may check how
the function output changes when the input is changed from
x to π − x. If sin(x) differs from sin(π − x), this observation
signals an error without needing to examine the speciﬁc val-
ues computed by the implementation. sin(x) = sin(π − x)
is thus a metamorphic relation that plays the role of test
oracle (also named ‘pseudo oracle’) to help bug detection.

In ML testing, metamorphic relations are widely studied
to tackle the oracle problem. Many metamorphic relations
are based on transformations of training or test data that are
expected to yield unchanged or certain expected changes
in the predictive output. There are different granularities
of data transformations when studying the correspond-
ing metamorphic relations. Some transformations conduct
coarse-grained changes such as enlarging the dataset or
changing the data order, without changing each single data
instance. We call these transformations ‘Coarse-grained data
transformations’. Some transformations conduct data trans-
formations via smaller changes on each data instance, such
as mutating the attributes, labels, or pixels of images, and
are referred to as ‘ﬁne-grained’ data transformations in this
paper. The related works of each type of transformations are
introduced below.
Coarse-grained Data Transformation. As early as in 2008,
Murphy et al. [115] discuss the properties of machine
learning algorithms that may be adopted as metamorphic
relations. Six transformations of input data are introduced:
additive, multiplicative, permutative, invertive, inclusive,
and exclusive. The changes include adding a constant to
numerical values; multiplying numerical values by a con-
stant; permuting the order of the input data; reversing the
order of the input data; removing a part of the input data;
adding additional data. Their analysis is on MartiRank,
SVM-Light, and PAYL. Although unevaluated in the initial
2008 paper, this work provided a foundation for determin-
ing the relationships and transformations that can be used
for conducting metamorphic testing for machine learning.

Ding et al. [116] proposed 11 metamorphic relations
to test deep learning systems. At the dataset level, the
metamorphic relations were also based on training data or
test data transformations that were not supposed to affect
classiﬁcation accuracy, such as adding 10% training images
into each category of the training data set or removing one
category of data from the dataset. The evaluation is based
on a classiﬁcation of biological cell images.

Murphy et al.

[117] presented function-level meta-
morphic relations. The evaluation on 9 machine learning
applications indicated that functional-level properties were
170% more effective than application-level properties.
Fine-grained Data Transformation. In 2009, Xie et al. [118]
proposed to use metamorphic relations that were speciﬁc to
a certain model to test the implementations of supervised
classiﬁers. The paper presents ﬁve types of metamorphic
relations that enable the prediction of expected changes to
the output (such as changes in classes, labels, attributes)
based on particular changes to the input. Manual analysis
of the implementation of KNN and Naive Bayes from
Weka [119] indicates that not all metamorphic relations

13

are necessary. The differences in the metamorphic relations
between SVM and neural networks are also discussed in
[120]. Dwarakanath et al. [121] applied metamorphic rela-
tions to image classiﬁcations with SVM and deep learning
systems. The changes on the data include changing the
feature or instance orders, linear scaling of the test features,
normalisation or scaling up the test data, or changing the
convolution operation order of the data. The proposed MRs
are able to ﬁnd 71% of the injected bugs. Sharma and
Wehrheim [122] considered ﬁne-grained data transforma-
tions such as changing feature names, renaming feature
values to test fairness. They studied 14 classiﬁers, none of
them were found to be sensitive to feature name shufﬂing.

Zhang et al. [54] proposed Perturbed Model Validation
(PMV) which combines metamorphic relation and data
mutation to detect overﬁtting. PMV mutates the training
data via injecting noise in the training data to create per-
turbed training datasets, then checks the training accuracy
decrease rate when the noise degree increases. The faster the
training accuracy decreases, the less the machine learning
model overﬁts.

Al-Azani and Hassine [123] studied the metamorphic
relations of Naive Bayes, k-Nearest Neighbour, as well as
their ensemble classiﬁer. It turns out that the metamorphic
relations necessary for Naive Bayes and k-Nearest Neigh-
bour may be not necessary for their ensemble classiﬁer.

Tian et al. [76] and Zhang et al. [79] stated that the
autonomous vehicle steering angle should not change signi-
ﬁcantly or stay the same for the transformed images under
different weather conditions. Ramanagopal et al. [124] used
the classiﬁcation consistency of similar images to serve
as test oracles for testing self-driving cars. The evaluation
indicates a precision of 0.94 when detecting errors in unla-
belled data.

Additionally, Xie et al. [125] proposed METTLE, a meta-
morphic testing approach for unsupervised learning val-
idation. METTLE has six types of different-grained meta-
morphic relations that are specially designed for unsu-
pervised learners. These metamorphic relations manipulate
instance order, distinctness, density, attributes, or inject out-
liers of the data. The evaluation was based on synthetic data
generated by Scikit-learn, showing that METTLE is prac-
tical and effective in validating unsupervised learners. Na-
kajima et al. [113], [126] discussed the possibilities of using
different-grained metamorphic relations to ﬁnd problems in
SVM and neural networks, such as to manipulate instance
order or attribute order and to reverse labels and change
attribute values, or to manipulate the pixels in images.
Metamorphic Relations between Different Datasets. The
consistency relations between/among different datasets can
also be regarded as metamorphic relations that could be
applied to detect data bugs. Kim et al. [127] and Breck et
al. [45] studied the metamorphic relations between training
data and new data. If the training data and new data
have different distributions, the training data may not be
adequate. Breck et al. [45] also studied the metamorphic
relations among different datasets that are close in time:
these datasets are expected to share some characteristics
because it is uncommon to have frequent drastic changes
to the data-generation code.
Frameworks to Apply Metamorphic Relations. Murphy

et al. [128] implemented a framework called Amsterdam
to automate the process of using metamorphic relations to
detect ML bugs. The framework reduces false positives via
setting thresholds when doing result comparison. They also
developed Corduroy [117], which extended Java Modelling
Language to let developers specify metamorphic properties
and generate test cases for ML testing.

We introduce more related work on domain-speciﬁc
metamorphic relations of testing autonomous driving, Dif-
ferentiable Neural Computer (DNC) [100], machine transla-
tion systems [86], [129], [130], biological cell classiﬁcation
[83], and audio-based deep learning systems [82] in Sec-
tion 8.

5.2.2 Cross-Referencing as Test Oracles

Cross-Referencing is another type of test oracle for ML test-
ing, including differential Testing and N-version Program-
ming. Differential testing is a traditional software testing
technique that detects bugs by observing whether similar
applications yield different outputs regarding identical in-
puts [11], [131]. It is a testing oracle for detecting com-
piler bugs [132]. According to the study of Nejadgholi and
Yang [133], 5% to 27% test oracles for deep learning libraries
use differential testing.

Differential testing is closely related with N-version pro-
gramming [134]: N-version programming aims to generate
multiple functionally-equivalent programs based on one
speciﬁcation, so that the combination of different versions
are more fault-tolerant and robust.

Davis and Weyuker [11] discussed the possibilities of
differential testing for ‘non-testable’ programs. The idea
is that if multiple implementations of an algorithm yield
different outputs on one identical input, then at least one of
the implementation contains a defect. Alebiosu et al. [135]
evaluated this idea on machine learning, and successfully
found 16 faults from 7 Naive Bayes implementations and 13
faults from 19 k-nearest neighbour implementation.

Pham et al. [48] also adopted cross referencing to test
ML implementations, but focused on the implementation
of deep learning libraries. They proposed CRADLE, the ﬁrst
approach that focuses on ﬁnding and localising bugs in deep
learning software libraries. The evaluation was conducted
on three libraries (TensorFlow, CNTK, and Theano), 11 data-
sets (including ImageNet, MNIST, and KGS Go game), and
30 pre-trained models. It turned out that CRADLE detects
104 unique inconsistencies and 12 bugs.

DeepXplore [1] and DLFuzz [90] used differential testing
as test oracles to ﬁnd effective test inputs. Those test inputs
causing different behaviours among different algorithms or
models were preferred during test generation.

Most differential testing relies on multiple implementa-
tions or versions, while Qin et al. [136] used the behaviours
of ‘mirror’ programs, generated from the training data as
pseudo oracles. A mirror program is a program generated
based on training data, so that the behaviours of the pro-
gram represent the training data. If the mirror program has
similar behaviours on test data, it is an indication that the
behaviour extracted from the training data suit test data as
well.

Sun et al. [86] applied cross reference in repairing
machine translation systems. Their approach, TransRepair,

14

compares the outputs (i.e., translations) of different mutated
inputs, and picks the output that shares the most similarity
with others as a superior translation candidate.

5.2.3 Measurement Metrics for Designing Test Oracles

Some work has presented deﬁnitions or statistical measure-
ments of non-functional features of ML systems including
robustness [137], fairness [138], [139], [140], and interpretab-
ility [65], [141]. These measurements are not direct oracles
for testing, but are essential for testers to understand and
evaluate the property under test, and to provide some actual
statistics that can be compared with the expected ones. For
example, the deﬁnitions of different types of fairness [138],
[139], [140] (more details are in Section 6.5.1) deﬁne the
conditions an ML system has to satisfy without which the
system is not fair. These deﬁnitions can be adopted directly
to detect fairness violations.

Except for these popular test oracles in ML testing, there
are also some domain-speciﬁc rules that could be applied
to design test oracles. We discussed several domain-speciﬁc
rules that could be adopted as oracles to detect data bugs
in Section 7.1.1. Kang et al. [142] discussed two types of
model assertions under the task of car detection in videos:
ﬂickering assertion to detect the ﬂickering in car bounding
box, and multi-box assertion to detect nested-car bounding.
For example, if a car bounding box contains other boxes,
the multi-box assertion fails. They also proposed some
automatic ﬁx rules to set a new predictive result when a
test assertion fails.

There has also been a discussion about the possibility of
evaluating ML learning curve in lifelong machine learning
as the oracle [143]. An ML system can pass the test oracle if
it can grow and increase its knowledge level over time.

5.3 Test Adequacy

Test adequacy evaluation aims to discover whether the
existing tests have a good fault-revealing ability. It provides
an objective conﬁdence measurement on testing activities.
The adequacy criteria can also be adopted to guide test
generation. Popular test adequacy evaluation techniques
in traditional software testing include code coverage and
mutation testing, which are also adopted in ML testing.

5.3.1 Test Coverage

In traditional software testing, code coverage measures the
degree to which the source code of a program is executed by
a test suite [144]. The higher coverage a test suite achieves, it
is more probable that the hidden bugs could be uncovered.
In other words, covering the code fragment is a necessary
condition to detect the defects hidden in the code. It is often
desirable to create test suites to achieve higher coverage.

Unlike traditional software, code coverage is seldom a
demanding criterion for ML testing, since the decision logic
of an ML model is not written manually but rather it is
learned from training data. For example, in the study of
Pei et al. [1], 100 % traditional code coverage is easy to
achieve with a single randomly chosen test input. Instead,
researchers propose various types of coverage for ML mod-
els beyond code coverage.

Neuron coverage. Pei et al. [1] proposed the ﬁrst coverage
criterion, neuron coverage, particularly designed for deep
learning testing. Neuron coverage is calculated as the ratio
of the number of unique neurons activated by all test inputs
and the total number of neurons in a DNN. In particular, a
neuron is activated if its output value is larger than a user-
speciﬁed threshold.

Ma et al. [92] extended the concept of neuron coverage.
They ﬁrst proﬁle a DNN based on the training data, so that
they obtain the activation behaviour of each neuron against
the training data. Based on this, they propose more ﬁne-
grained criteria, k-multisection neuron coverage, neuron
boundary coverage, and strong neuron activation coverage,
to represent the major functional behaviour and corner
behaviour of a DNN.
MC/DC coverage variants. Sun et al. [145] proposed four
test coverage criteria that are tailored to the distinct features
of DNN inspired by the MC/DC coverage criteria [146].
MC/DC observes the change of a Boolean variable, while
their proposed criteria observe a sign, value, or distance
change of a neuron, in order to capture the causal changes
in the test inputs. The approach assumes the DNN to be a
fully-connected network, and does not consider the context
of a neuron in its own layer as well as different neuron
combinations within the same layer [147].
Layer-level coverage. Ma et al. [92] also presented layer-
level coverage criteria, which considers the top hyperactive
neurons and their combinations (or the sequences) to char-
acterise the behaviours of a DNN. The coverage is evaluated
to have better performance together with neuron coverage
based on dataset MNIST and ImageNet. In their following-
up work [148], [149], they further proposed combinatorial
testing coverage, which checks the combinatorial activation
status of the neurons in each layer via checking the fraction
of neurons activation interaction in a layer. Sekhon and
Fleming [147] deﬁned a coverage criteria that looks for 1) all
pairs of neurons in the same layer having all possible value
combinations, and 2) all pairs of neurons in consecutive
layers having all possible value combinations.
State-level coverage. While aftermentioned criteria, to some
extent, capture the behaviours of feed-forward neural net-
works, they do not explicitly characterise stateful machine
learning system like recurrent neural network (RNN). The
RNN-based ML approach has achieved notable success
in applications that handle sequential inputs, e.g., speech
audio, natural language, cyber physical control signals. In
order to analyse such stateful ML systems, Du et al. [82]
proposed the ﬁrst set of testing criteria specialised for RNN-
based stateful deep learning systems. They ﬁrst abstracted a
stateful deep learning system as a probabilistic transition
system. Based on the modelling, they proposed criteria
based on the state and traces of the transition system, to
capture the dynamic state transition behaviours.
Limitations of Coverage Criteria. Although there are differ-
ent types of coverage criteria, most of them focus on DNNs.
Sekhon and Fleming [147] examined the existing testing
methods for DNNs and discussed the limitations of these
criteria.

Most proposed coverage criteria are based on the struc-
ture of a DNN. Li et al. [150] pointed out the limitations
of structural coverage criteria for deep networks caused by

15

the fundamental differences between neural networks and
human-written programs. Their initial experiments with
natural inputs found no strong correlation between the
number of misclassiﬁed inputs in a test set and its structural
coverage. Due to the black-box nature of a machine learning
system, it is not clear how such criteria directly relate to the
system’s decision logic.

5.3.2 Mutation Testing

In traditional software testing, mutation testing evalu-
ates the fault-revealing ability of a test suite via injecting
faults [144], [151]. The ratio of detected faults against all
injected faults is called the Mutation Score.

In ML testing, the behaviour of an ML system depends
on, not only the learning code, but also data and model
structure. Ma et al. [152] proposed DeepMutation, which
mutates DNNs at the source level or model level, to make
minor perturbation on the decision boundary of a DNN.
Based on this, a mutation score is deﬁned as the ratio of
test instances of which results are changed against the total
number of instances.

Shen et al. [153] proposed ﬁve mutation operators for
DNNs and evaluated properties of mutation on the MINST
dataset. They pointed out that domain-speciﬁc mutation
operators are needed to enhance mutation analysis.

Compared to structural coverage criteria, mutation test-
ing based criteria are more directly relevant to the decision
boundary of a DNN. For example, an input data that is near
the decision boundary of a DNN, could more easily detect
the inconsistency between a DNN and its mutants.

5.3.3 Surprise Adequacy

Kim et al. [127] introduced surprise adequacy to measure
the coverage of discretised input surprise range for deep
learning systems. They argued that test diversity is more
meaningful when being measured with respect to the train-
ing data. A ‘good’ test input should be ‘sufﬁciently but not
overly surprising’ comparing with the training data. Two
measurements of surprise were introduced: one is based
on Keneral Density Estimation (KDE) to approximate the
likelihood of the system having seen a similar input during
training, the other is based on the distance between vectors
representing the neuron activation traces of the given input
and the training data (e.g., Euclidean distance). These cri-
teria can be adopted to detect adversarial examples. Further
investigation is required to determine whether such criteria
enable the behaviour boundaries of ML models to be ap-
proximated in terms of surprise. It will also be interesting for
future work to study the relationship between adversarial
examples, natural error samples, and surprise-based criteria.

5.3.4 Rule-based Checking of Test Adequacy

To ensure the functionality of an ML system, there may be
some ‘typical’ rules that are necessary. Breck et al. [154]
offered 28 test aspects to consider and a scoring system
used by Google. Their focus is to measure how well a given
machine learning system is tested. The 28 test aspects are
classiﬁed into four types: 1) the tests for the ML model itself,
2) the tests for ML infrastructure used to build the model,
3) the tests for ML data used to build the model, and 4)

the tests that check whether the ML system works correctly
over time. Most of them are some must-to-check rules that
could be applied to guide test generation. For example, the
training process should be reproducible; all features should
be beneﬁcial; there should be no other model that is simpler
but better in performance than the current one. Their re-
search indicates that, although ML testing is complex, there
are shared properties to design some basic test cases to test
the fundamental functionality of the ML system.

5.4 Test Prioritisation and Reduction

Test input generation in ML has a very large input space
to cover. On the other hand, we need to label every test in-
stance so as to judge predictive accuracy. These two aspects
lead to high test generation costs. Byun et al. [155] used
DNN metrics like cross entropy, surprisal, and Bayesian
uncertainty to prioritise test inputs. They experimentally
showed that these are good indicators of inputs that expose
unacceptable behaviours, which are also useful for retrain-
ing.

Generating test inputs is also computationally expensive.
Zhang et al. [156] proposed to reduce costs by identi-
fying those test instances that denote the more effective
adversarial examples. The approach is a test prioritisation
technique that ranks the test instances based on their sensit-
ivity to noise, because the instances that are more sensitive
to noise is more likely to yield adversarial examples.

Li et. al [157] focused on test data reduction in oper-
ational DNN testing. They proposed a sampling technique
guided by the neurons in the last hidden layer of a DNN, us-
ing a cross-entropy minimisation based distribution approx-
imation technique. The evaluation was conducted on pre-
trained models with three image datasets: MNIST, Udacity
challenge, and ImageNet. The results show that, compared
to random sampling, their approach samples only half the
test inputs, yet it achieves a similar level of precision.

Ma et al. [158] proposed a set of test selection metrics
based on the notion of model conﬁdence. Test inputs that are
more uncertain to the models are preferred, because they are
more informative and should be used to improve the model
if being included during retraining. The evaluation shows
that their test selection approach has 80% more gain than
random selection.

5.5 Bug Report Analysis

Thung et al. [159] were the ﬁrst to study machine learning
bugs via analysing the bug reports of machine learning sys-
tems. 500 bug reports from Apache Mahout, Apache Lucene,
and Apache OpenNLP were studied. The explored prob-
lems included bug frequency, bug categories, bug severity,
and bug resolution characteristics such as bug-ﬁx time,
effort, and ﬁle number. The results indicated that incorrect
implementation counts for the largest proportion of ML
bugs, i.e., 22.6% of bugs are due to incorrect implementation
of deﬁned algorithms. Implementation bugs are also the
most severe bugs, and take longer to ﬁx. In addition, 15.6%
of bugs are non-functional bugs. 5.6% of bugs are data bugs.
Zhang et al. [160] studied 175 TensorFlow bugs, based
on the bug reports from Github or StackOverﬂow. They

16

studied the symptoms and root causes of the bugs, the ex-
isting challenges to detect the bugs and how these bugs are
handled. They classiﬁed TensorFlow bugs into exceptions
or crashes, low correctness, low efﬁciency, and unknown.
The major causes were found to be in algorithm design and
implementations such as TensorFlow API misuse (18.9%),
unaligned tensor (13.7%), and incorrect model parameter or
structure (21.7%)

Banerjee et al.

[161] analysed the bug reports of
autonomous driving systems from 12 autonomous vehicle
manufacturers that drove a cumulative total of 1,116,605
miles in California. They used NLP technologies to classify
the causes of disengagements into 10 types (A disagreement
is a failure that causes the control of the vehicle to switch
from the software to the human driver). The issues in
machine learning systems and decision control account for
the primary cause of 64 % of all disengagements based on
their report analysis.

5.6 Debug and Repair

Data Resampling. The generated test inputs introduced in
Section 5.1 only expose ML bugs, but are also studied as
a part of the training data and can improve the model’s
correctness through retraining. For example, DeepXplore
achieves up to 3% improvement in classiﬁcation accuracy
by retraining a deep learning model on generated inputs.
DeepTest [76] improves the model’s accuracy by 46%.

Ma et al. [162] identiﬁed the neurons responsible for
the misclassiﬁcation and call them ‘faulty neurons’. They
resampled training data that inﬂuence such faulty neurons
to help improve model performance.
Debugging Framework Development. Dutta et al. [163]
proposed Storm, a program transformation framework to
generate smaller programs that can support debugging for
machine learning testing. To ﬁx a bug, developers usually
need to shrink the program under test to write better bug
reports and to facilitate debugging and regression testing.
Storm applies program analysis and probabilistic reasoning
to simplify probabilistic programs, which helps to pinpoint
the issues more easily.

Cai et al. [164] presented tfdbg, a debugger for ML mod-
els built on TensorFlow, containing three key components:
1) the Analyzer, which makes the structure and intermediate
state of the runtime graph visible; 2) the NodeStepper,
which enables clients to pause, inspect, or modify at a given
node of the graph; 3) the RunStepper, which enables clients
to take higher level actions between iterations of model
training. Vartak et al. [165] proposed the MISTIQUE system
to capture, store, and query model intermediates to help
the debug. Krishnan and Wu [166] presented PALM, a tool
that explains a complex model with a two-part surrogate
model: a meta-model that partitions the training data and
a set of sub-models that approximate the patterns within
each partition. PALM helps developers ﬁnd out the training
data that impacts prediction the most, and thereby target
the subset of training data that account for the incorrect
predictions to assist debugging.
Fix Understanding. Fixing bugs in many machine learning
systems is difﬁcult because bugs can occur at multiple
points in different components. Nushi et al. [167] proposed

a human-in-the-loop approach that simulates potential ﬁxes
in different components through human computation tasks:
humans were asked to simulate improved component states.
The improvements of the system are recorded and com-
pared, to provide guidance to designers about how they can
best improve the system.

Program Repair. Albarghouthi et al. [168] proposed a
distribution-guided inductive synthesis approach to repair
decision-making programs such as machine learning pro-
grams. The purpose is to construct a new program with cor-
rect predictive output, but with similar semantics with the
original program. Their approach uses sampled instances
and the predicted outputs to drive program synthesis in
which the program is encoded based on SMT.

5.7 General Testing Framework and Tools

There has also been work focusing on providing a testing
tool or framework that helps developers to implement test-
ing activities in a testing workﬂow. There is a test framework
to generate and validate test inputs for security testing [169].
Dreossi et al. [170] presented a CNN testing framework
that consists of three main modules: an image generator, a
collection of sampling methods, and a suite of visualisation
tools. Tramer et al. [171] proposed a comprehensive testing
tool to help developers to test and debug fairness bugs with
an easily interpretable bug report. Nishi et al. [172] proposed
a testing framework including different evaluation aspects
such as allowability, achievability, robustness, avoidability
and improvability. They also discussed different levels of
ML testing, such as system, software, component, and data
testing.

Thomas et al.

[173] recently proposed a framework for
designing machine learning algorithms, which simpliﬁes
the regulation of undesired behaviours. The framework
is demonstrated to be suitable for regulating regression,
classiﬁcation, and reinforcement algorithms. It allows one
to learn from (potentially biased) data while guaranteeing
that, with high probability, the model will exhibit no bias
when applied to unseen data. The deﬁnition of bias is user-
speciﬁed, allowing for a large family of deﬁnitions. For a
learning algorithm and a training data, the framework will
either returns a model with this guarantee, or a warning that
it fails to ﬁnd such a model with the required guarantee.

6 ML PROPERTIES TO BE TESTED

ML properties concern the conditions we should care about
for ML testing, and are usually connected with the beha-
viour of an ML model after training. The poor performance
in a property, however, may be due to bugs in any of the
ML components (see more in Introduction 7).

This section presents the related work of testing both
functional ML properties and non-functional ML properties.
Functional properties include correctness (Section 6.1) and
overﬁtting (Section 6.2). Non-functional properties include
robustness and security (Section 6.3), efﬁciency (Section 6.4),
fairness (Section 6.5).

17

6.1 Correctness

Correctness concerns the fundamental function accuracy
of an ML system. Classic machine learning validation is
the most well-established and widely-used technology for
correctness testing. Typical machine learning validation ap-
proaches are cross-validation and bootstrap. The principle
is to isolate test data via data sampling to check whether
the trained model ﬁts new cases. There are several ap-
proaches to perform cross-validation. In hold out cross-
validation [174], the data are split into two parts: one part
becomes the training data and the other part becomes test
data8. In k-fold cross-validation, the data are split into k
equal-sized subsets: one subset used as the test data and
the remaining k − 1 as the training data. The process is
then repeated k times, with each of the subsets serving
as the test data. In leave-one-out cross-validation, k-fold
cross-validation is applied, where k is the total number
of data instances. In Bootstrapping, the data are sampled
with replacement [175], and thus the test data may contain
repeated instances.

There are several widely-adopted correctness measure-
ments such as accuracy, precision, recall, and Area Under
Curve (AUC). There has been work [176] analysing the
disadvantages of each measurement criterion. For example,
accuracy does not distinguish between the types of errors
it makes (False Positive versus False Negatives). Precision
and Recall may be misled when data is unbalanced. An
implication of this work is that we should carefully choose
performance metrics. Chen et al. studied the variability
of both training data and test data when assessing the
correctness of an ML classiﬁer [177]. They derived analytical
expressions for the variance of the estimated performance
and provided an open-source software implemented with
an efﬁcient computation algorithm. They also studied the
performance of different statistical methods when compar-
ing AUC, and found that the F -test has the best perform-
ance [178].

To better capture correctness problems, Qin et al. [136]
proposed to generate a mirror program from the training
data, and then use the behaviours this mirror program
to serve as a correctness oracle. The mirror program is
expected to have similar behaviours as the test data.

There has been a study of the prevalence of correctness
problems among all the reported ML bugs: Zhang et al. [160]
studied 175 Tensorﬂow bug reports from StackOverﬂow QA
(Question and Answer) pages and from Github projects.
Among the 175 bugs, 40 of them concern poor correctness.

Additionally, there have been many works on detecting
data bug that may lead to low correctness [179], [180],
[181] (see more in Section 7.1), test input or test oracle
design [116], [120], [121], [123], [130], [154], [162], and test
tool design [165], [167], [182] (see more in Section 5).

6.2 Model Relevance

Model relevance evaluation detects mismatches between
model and data. A poor model relevance is usually asso-
ciated with overﬁtting or underﬁtting. When a model is too

8 Sometimes a validation set is also needed to help train the model, in
which circumstances the validation set will be isolated from the training
set.

complex for the data, even the noise of training data is ﬁtted
by the model [183]. Overﬁtting can easily happen, especially
when the training data is insufﬁcient, [184], [185], [186].

Cross-validation is traditionally considered to be a useful
way to detect overﬁtting. However, it is not always clear
how much overﬁtting is acceptable and cross-validation
may be unlikely to detect overﬁtting if the test data is
unrepresentative of potential unseen data.

Zhang et al. [54] introduced Perturbed Model Validation
(PMV) to help model selection. PMV injects noise to the
training data, re-trains the model against the perturbed
data, then uses the training accuracy decrease rate to detect
overﬁtting/underﬁtting. The intuition is that an overﬁtted
learner tends to ﬁt noise in the training sample, while an
underﬁtted learner will have low training accuracy regard-
less the presence of injected noise. Thus, both overﬁtting
and underﬁtting tend to be less insensitive to noise and
exhibit a small accuracy decrease rate against noise degree
on perturbed data. PMV was evaluated on four real-world
datasets (breast cancer, adult, connect-4, and MNIST) and
nine synthetic datasets in the classiﬁcation setting. The res-
ults reveal that PMV has better performance and provides a
more recognisable signal for detecting both overﬁtting/un-
derﬁtting compared to 10-fold cross-validation.

An ML system usually gathers new data after deploy-
ment, which will be added into the training data to improve
correctness. The test data, however, cannot be guaranteed to
represent the future data. Werpachowski et al. [42] presents
an overﬁtting detection approach via generating adversarial
examples from test data. If the reweighted error estimate on
adversarial examples is sufﬁciently different from that of the
original test set, overﬁtting is detected. They evaluated their
approach on ImageNet and CIFAR-10.

Gossmann et al.

[187] studied the threat of test data
reuse practice in the medical domain with extensive simula-
tion studies, and found that the repeated reuse of the same
test data will inadvertently result in overﬁtting under all
considered simulation settings.

Kirk [51] mentioned that we could use training time as a
complexity proxy for an ML model; it is better to choose
the algorithm with equal correctness but relatively small
training time.

Ma et al. [162] tried to relieve the overﬁtting problem via
re-sampling the training data. Their approach was found to
improve test accuracy from 75% to 93% on average, based
on an evaluation using three image classiﬁcation datasets.

6.3 Robustness and Security

6.3.1 Robustness Measurement Criteria

Unlike correctness or overﬁtting, robustness is a non-
functional characteristic of a machine learning system. A
natural way to measure robustness is to check the cor-
rectness of the system with the existence of noise [137]; a
robust system should maintain performance in the presence
of noise.

Moosavi-Dezfooli et al. [188] proposed DeepFool that
computes perturbations (added noise) that ‘fool’ deep net-
works so as to quantify their robustness. Bastani et al. [189]
presented three metrics to measure robustness: 1) pointwise

18

robustness, indicating the minimum input change a classi-
ﬁer fails to be robust; 2) adversarial frequency, indicating
how often changing an input changes a classiﬁer’s results;
3) adversarial severity, indicating the distance between an
input and its nearest adversarial example.

Carlini and Wagner [190] created a set of attacks that
can be used to construct an upper bound on the robustness
of a neural network. Tjeng et al. [137] proposed to use the
distance between a test input and its closest adversarial
example to measure robustness. Ruan et al. [191] provided
global robustness lower and upper bounds based on the
test data to quantify the robustness. Gopinath [192] et al.
proposed DeepSafe, a data-driven approach for assessing
DNN robustness: inputs that are clustered into the same
group should share the same label.

More recently, Mangal et al. [193] proposed the deﬁnition
of probabilistic robustness. Their work used abstract inter-
pretation to approximate the behaviour of a neural network
and to compute an over-approximation of the input regions
where the network may exhibit non-robust behaviour.

Banerjee et al. [194] explored the use of Bayesian Deep
Learning to model the propagation of errors inside deep
neural networks to mathematically model the sensitivity of
neural networks to hardware errors, without performing
extensive fault injection experiments.

6.3.2 Perturbation Targeting Test Data

The existence of adversarial examples allows attacks that
may lead to serious consequences in safety-critical applic-
ations such as self-driving cars. There is a whole separate
literature on adversarial example generation that deserves a
survey of its own, and so this paper does not attempt to fully
cover it. Rather, we focus on those promising aspects that
could be fruitful areas for future research at the intersection
of traditional software testing and machine learning.

Carlini and Wagner [190] developed adversarial ex-
ample generation approaches using distance metrics to
quantify similarity. The approach succeeded in generating
adversarial examples for all images on the recently pro-
posed defensively distilled networks [195].

Adversarial input generation has been widely adopted
to test the robustness of autonomous driving systems [1],
[76], [79], [92], [93]. There has also been research on generat-
ing adversarial inputs for NLI models [98], [99](Section 8.3),
malware detection [169], and Differentiable Neural Com-
puter (DNC) [100].

Papernot et al. [196], [197] designed a library to standard-
ise the implementation of adversarial example construction.
They pointed out that standardising adversarial example
generation is important because ‘benchmarks constructed
without a standardised implementation of adversarial ex-
ample construction are not comparable to each other’: it is
not easy to tell whether a good result is caused by a high
level of robustness or by the differences in the adversarial
example construction procedure.

Other techniques to generate test data that check
the neural network robustness include symbolic execu-
tion [108],
fuzz testing [90], combinatorial Test-
ing [148], and abstract interpretation [193]. In Section 5.1,
we cover these test generation techniques in more detail.

[111],

6.3.3 Perturbation Targeting the Whole System

Jha et al. [198] presented AVFI, which used application/soft-
ware fault injection to approximate hardware errors in the
sensors, processors, or memory of the autonomous vehicle
(AV) systems to test the robustness. They also presented
Kayotee [199], a fault injection-based tool to systemat-
ically inject faults into software and hardware compon-
ents of the autonomous driving systems. Compared with
AVFI, Kayotee is capable of characterising error propagation
and masking using a closed-loop simulation environment,
which is also capable of injecting bit ﬂips directly into GPU
and CPU architectural state. DriveFI [96], further presented
by Jha et al., is a fault-injection engine that mines situations
and faults that maximally impact AV safety.

Tuncali et al. [102] considered the closed-loop behaviour
of the whole system to support adversarial example gener-
ation for autonomous driving systems, not only in image
space, but also in conﬁguration space.

6.4 Efﬁciency

The empirical study of Zhang et al. [160] on Tensorﬂow bug-
related artefacts (from StackOverﬂow QA page and Github)
found that nine out of 175 ML bugs (5.1%) belong to efﬁ-
ciency problems. The reasons may either be that efﬁciency
problems rarely occur or that these issues are difﬁcult to
detect.

Kirk [51] pointed out that it is possible to use the
efﬁciency of different machine learning algorithms when
training the model to compare their complexity.

Spieker and Gotlieb [200] studied three training data
reduction approaches, the goal of which was to ﬁnd a
smaller subset of the original training data set with similar
characteristics during model training, so that model build-
ing speed could be improved for faster machine learning
testing.

6.5 Fairness

Fairness is a relatively recently-emerging non-functional
characteristic. According to the work of Barocas and
Selbst [201], there are the following ﬁve major causes of
unfairness.
1) Skewed sample: once some initial bias happens, such bias
may compound over time.
2) Tainted examples: the data labels are biased because of
biased labelling activities of humans.
3) Limited features: features may be less informative or
reliably collected, misleading the model in building the
connection between the features and the labels.
4) Sample size disparity: if the data from the minority group
and the majority group are highly imbalanced, ML model
may the minority group less well.
5) Proxies: some features are proxies of sensitive attributes
(e.g., neighbourhood in which a person resides), and may
cause bias to the ML model even if sensitive attributes are
excluded.

Research on fairness focuses on measuring, discovering,
understanding, and coping with the observed differences
regarding different groups or individuals in performance.
Such differences are associated with fairness bugs, which

19

can offend and even harm users, and cause programmers
and businesses embarrassment, mistrust, loss of revenue,
and even legal violations [171].

6.5.1 Fairness Deﬁnitions and Measurement Metrics

There are several deﬁnitions of fairness proposed in the
literature, yet no ﬁrm consensus [202], [203], [204], [205].
Nevertheless, these deﬁnitions can be used as oracles to
detect fairness violations in ML testing.

To help illustrate the formalisation of ML fairness, we
use X to denote a set of individuals, Y to denote the true
label set when making decisions regarding each individual
in X. Let h be the trained machine learning predictive
model. Let A be the set of sensitive attributes, and Z be
the remaining attributes.
1) Fairness Through Unawareness. Fairness Through Un-
awareness (FTU) means that an algorithm is fair so long
as the protected attributes are not explicitly used in the
decision-making process [206]. It is a relatively low-cost
way to deﬁne and ensure fairness. Nevertheless, sometimes
the non-sensitive attributes in X may contain information
correlated to sensitive attributes that may thereby lead to
discrimination [202], [206]. Excluding sensitive attributes
may also impact model accuracy and yield less effective
predictive results [207].
2) Group Fairness. A model under test has group fairness if
groups selected based on sensitive attributes have an equal
probability of decision outcomes. There are several types of
group fairness.

Demographic Parity is a popular group fairness meas-
urement [208]. It is also named Statistical Parity or Independ-
ence Parity. It requires that a decision should be independent
of the protected attributes. Let G1 and G2 be the two
groups belonging to X divided by a sensitive attribute
a ∈ A. A model h under test satisﬁes demographic parity if
P {h(xi) = 1|xi ∈ G1} = P {h(xj) = 1|xj ∈ G2}.

Equalised Odds is another group fairness approach pro-
posed by Hardt et al. [139]. A model under test h satisﬁes
Equalised Odds if h is independent of the protected attributes
when a target label Y is ﬁxed as yi: P {h(xi) = 1|xi ∈
G1, Y = yi} = P {h(xj) = 1|xj ∈ G2, Y = yi}.

When the target label is set to be positive, Equalised
Odds becomes Equal Opportunity [139]. It requires that the
true positive rate should be the same for all the groups. A
model h satisﬁes Equal Opportunity if h is independent of
the protected attributes when a target class Y is ﬁxed as
being positive: P {h(xi) = 1|xi ∈ G1, Y = 1} = P {h(xj) =
1|xj ∈ G2, Y = 1}.
3) Counter-factual Fairness. Kusner et al. [206] introduced
Counter-factual Fairness. A model satisﬁes Counter-factual
Fairness if its output remains the same when the protected
attribute is ﬂipped to a counter-factual value, and other
variables are modiﬁed as determined by the assumed causal
model. Let a be a protected attribute, a(cid:48) be the counterfac-
tual attribute of a, x(cid:48)
i be the new input with a changed
into a(cid:48). Model h is counter-factually fair if, for any input
xi and protected attribute a: P {h(xi)a = yi|a ∈ A, xi ∈
X} = P {h(x(cid:48)
i)a(cid:48) = yi|a ∈ A, xi ∈ X}. This measurement of
fairness additionally provides a mechanism to interpret the
causes of bias, because the variables other than the protected

i) must be caused by variations in A.

attributes are controlled, and thus the differences in h(xi)
and h(x(cid:48)
4) Individual Fairness. Dwork et al. [138] proposed a use
task-speciﬁc similarity metric to describe the pairs of indi-
viduals that should be regarded as similar. According to
Dwork et al., a model h with individual fairness should
give similar predictive results among similar individuals:
P {h(xi)|xi ∈ X} = P {h(xj) = yi|xj ∈ X} iff d(xi, xj) < (cid:15),
where d is a distance metric for individuals that measures
their similarity, and (cid:15) is tolerance to such differences.
Analysis and Comparison of Fairness Metrics. Although
there are many existing deﬁnitions of fairness, each has
its advantages and disadvantages. Which fairness is the
most suitable remains controversial. There is thus some
work surveying and analysing the existing fairness metrics,
or investigate and compare their performance based on
experimental results, as introduced below.

Gajane and Pechenizkiy [202] surveyed how fairness
is deﬁned and formalised in the literature. Corbett-Davies
and Goel [62] studied three types of fairness deﬁnitions:
anti-classiﬁcation, classiﬁcation parity, and calibration. They
pointed out the deep statistical limitations of each type with
examples. Verma and Rubin [203] explained and illustrated
the existing most prominent fairness deﬁnitions based on a
common, unifying dataset.

Saxena et al. [204] investigated people’s perceptions of
three of the fairness deﬁnitions. About 200 recruited par-
ticipants from Amazon’s Mechanical Turk were asked to
choose their preference over three allocation rules on two
individuals having each applied for a loan. The results
demonstrate a clear preference for the way of allocating
resources in proportion to the applicants’ loan repayment
rates.
Support for Fairness Improvement. Metevier et al. [209]
proposed RobinHood, an algorithm that supports mul-
tiple user-deﬁned fairness deﬁnitions under the scenario
of ofﬂine contextual bandits9. RobinHood makes use of
concentration inequalities [211] to calculate high-probability
bounds and to search for solutions that satisfy the fairness
requirements. It gives user warnings when the requirements
are violated. The approach is evaluated under three applic-
ation scenarios: a tutoring system, a loan approval setting,
and the criminal recidivism, all of which demonstrate the
superiority of RobinHood over other algorithms.

Albarghouthi and Vinitsky [75] proposed the concept of
‘fairness-aware programming’, in which fairness is a ﬁrst-
class concern. To help developers deﬁne their own fairness
speciﬁcations, they developed a speciﬁcation language. Like
assertions in traditional testing, the fairness speciﬁcations
are developed into the run-time monitoring code to enable
multiple executions to catch violations. A prototype was
implemented in Python.

Agarwal et al. [212] proposed to reduce fairness classi-
ﬁcation into a problem of cost-sensitive classiﬁcation (where
the costs of different types of errors are differentiated).
The application scenario is binary classiﬁcation, with the
underlying classiﬁcation method being treated as a black

20

box. The reductions optimise the trade-off between accuracy
and fairness constraints.

Albarghouthi et al. [168] proposed an approach to re-
pair decision-making programs using distribution-guided
inductive synthesis.

6.5.2 Test Generation Techniques for Fairness Testing

Galhotra et al. [5], [213] proposed Themis which considers
group fairness using causal analysis [214]. It deﬁnes fair-
ness scores as measurement criteria for fairness and uses
random test generation techniques to evaluate the degree of
discrimination (based on fairness scores). Themis was also
reported to be more efﬁcient on systems that exhibit more
discrimination.

Themis generates tests randomly for group fairness,
while Udeshi et al. [101] proposed Aequitas, focusing on
test generation to uncover discriminatory inputs and those
inputs essential to understand individual fairness. The gen-
eration approach ﬁrst randomly samples the input space
to discover the presence of discriminatory inputs, then
searches the neighbourhood of these inputs to ﬁnd more in-
puts. As well as detecting fairness bugs, Aeqitas also retrains
the machine-learning models and reduce discrimination in
the decisions made by these models.

Agarwal et al. [109] used symbolic execution together
with local explainability to generate test inputs. The key
idea is to use the local explanation, speciﬁcally Local Inter-
pretable Model-agnostic Explanations10 to identify whether
factors that drive decisions include protected attributes.
The evaluation indicates that the approach generates 3.72
times more successful test cases than THEMIS across 12
benchmarks.

Tramer et al. [171] were the ﬁrst to proposed the concept
of ‘fairness bugs’. They consider a statistically signiﬁcant
association between a protected attribute and an algorithmic
output to be a fairness bug, specially named ‘Unwarran-
ted Associations’ in their paper. They proposed the ﬁrst
comprehensive testing tool, aiming to help developers test
and debug fairness bugs with an ‘easily interpretable’ bug
report. The tool is available for various application areas in-
cluding image classiﬁcation, income prediction, and health
care prediction.

Sharma and Wehrheim [122] sought to identify causes
of unfairness via checking whether the algorithm under
test is sensitive to training data changes. They mutated the
training data in various ways to generate new datasets,
such as changing the order of rows, columns, and shufﬂing
feature names and values. 12 out of 14 classiﬁers were found
to be sensitive to these changes.

6.6 Interpretability

Manual Assessment of Interpretability. The existing work
on empirically assessing the interpretability property usu-
ally includes humans in the loop. That is, manual assess-
ment is currently the primary approach to evaluate in-
terpretability. Doshi-Velez and Kim [65] gave a taxonomy
interpretability:
of evaluation (testing) approaches for

9 A contextual bandit is a type of algorithm that learns to take actions

based on rewards such as user click rate [210].

10 Local Interpretable Model-agnostic Explanations produces decision
trees corresponding to an input that could provide paths in symbolic
execution [110]

application-grounded, human-grounded, and functionally-
grounded. Application-grounded evaluation involves hu-
man experimentation with a real application scenario.
Human-grounded evaluation uses results from human eval-
uation on simpliﬁed tasks. Functionally-grounded evalu-
ation requires no human experiments but uses a quantitative
metric as a proxy for explanation quality, for example, a
proxy for the explanation of a decision tree model may be
the depth of the tree.

Friedler et al. [215] introduced two types of interpretabil-
ity: global interpretability means understanding the entirety
of a trained model; local interpretability means understand-
ing the results of a trained model on a speciﬁc input and the
corresponding output. They asked 1000 users to produce
the expected output changes of a model given an input
change, and then recorded accuracy and completion time
over varied models. Decision trees and logistic regression
models were found to be more locally interpretable than
neural networks.
Automatic Assessment of Interpretability. Cheng et al. [46]
presented a metric to understand the behaviours of an ML
model. The metric measures whether the learner has learned
the object in object identiﬁcation scenario via occluding the
surroundings of the objects.

Christoph [70] proposed to measure interpretability
based on the category of ML algorithms. He claimed that
‘the easiest way to achieve interpretability is to use only
a subset of algorithms that create interpretable models’.
He identiﬁed several models with good interpretability,
including linear regression, logistic regression and decision
tree models.

Zhou et al. [216] deﬁned the concepts of Metamorphic
Relation Patterns (MRPs) and Metamorphic Relation Input
Patterns (MRIPs) that can be adopted to help end users
understand how an ML system works. They conducted
case studies of various systems, including large commercial
websites, Google Maps navigation, Google Maps location-
based search, image analysis for face recognition (including
Facebook, MATLAB, and OpenCV), and the Google video
analysis service Cloud Video Intelligence.
Evaluation of Interpretability Improvement Methods. Ma-
chine learning classiﬁers are widely used in many medical
applications, yet the clinical meaning of the predictive out-
come is often unclear. Chen et al. [217] investigated several
interpretability-improving methods which transform classi-
ﬁer scores to a probability of disease scale. They showed
that classiﬁer scores on arbitrary scales can be calibrated to
the probability scale without affecting their discrimination
performance.

6.7 Privacy

Ding et al. [218] treat programs as grey boxes, and de-
tect differential privacy violations via statistical tests. For
the detected violations, they generate counter examples to
illustrate these violations as well as to help developers
understand and ﬁx bugs. Bichsel et al.
[219] proposed to
estimate the (cid:15) parameter in differential privacy, aiming to
ﬁnd a triple (x, x(cid:48), Φ) that witnesses the largest possible
privacy violation, where x and x(cid:48) are two test inputs and
Φ is a possible set of outputs.

7 ML TESTING COMPONENTS
This section organises the work on ML testing by identifying
the component (data, learning program, or framework) for
which ML testing may reveal a bug.

21

7.1 Bug Detection in Data

Data is a ‘component’ to be tested in ML testing, since the
performance of the ML system largely depends on the data.
Furthermore, as pointed out by Breck et al. [45], it is import-
ant to detect data bugs early because predictions from the
trained model are often logged and used to generate further
data. This subsequent generation creates a feedback loop
that may amplify even small data bugs over time.

Nevertheless, data testing is challenging [220]. Accord-
ing to the study of Amershi et al. [8], the management
and evaluation of data is among the most challenging tasks
when developing an AI application in Microsoft. Breck et
al. [45] mentioned that data generation logic often lacks
visibility in the ML pipeline; the data are often stored in
a raw-value format (e.g., CSV) that strips out semantic
information that can help identify bugs.

7.1.1 Bug Detection in Training Data
Rule-based Data Bug Detection. Hynes et al. [179] pro-
posed data linter– an ML tool, inspired by code linters, to
automatically inspect ML datasets. They considered three
types of data problems: 1) miscoded data, such as mistyping
a number or date as a string; 2) outliers and scaling, such
as uncommon list length; 3) packaging errors, such as du-
plicate values, empty examples, and other data organisation
issues.

Cheng et al. [46] presented a series of metrics to eval-
uate whether the training data have covered all important
scenarios.
Performance-based Data Bug Detection. To solve the prob-
lems in training data, Ma et al. [162] proposed MODE.
MODE identiﬁes the ‘faulty neurons’ in neural networks
that are responsible for the classiﬁcation errors, and tests
the training data via data resampling to analyse whether
the faulty neurons are inﬂuenced. MODE allows to improve
test effectiveness from 75 % to 93 % on average, based on
evaluation using the MNIST, Fashion MNIST, and CIFAR-
10 datasets.

7.1.2 Bug Detection in Test Data

Metzen et al. [221] proposed to augment DNNs with a small
sub-network, specially designed to distinguish genuine data
from data containing adversarial perturbations. Wang et
al. [222] used DNN model mutation to expose adversarial
examples motivated by their observation that adversarial
samples are more sensitive to perturbations [223]. The eval-
uation was based on the MNIST and CIFAR10 datasets. The
approach detects 96.4 %/90.6 % adversarial samples with
74.1/86.1 mutations for MNIST/CIFAR10.

Adversarial examples in test data raise security risks.
Detecting adversarial examples is thereby similar to bug
detection. Carlini and Wagner [224] surveyed ten propos-
als that are designed for detecting adversarial examples
and compared their efﬁcacy. They found that detection
approaches rely on loss functions and can thus be bypassed

when constructing new loss functions. They concluded that
adversarial examples are signiﬁcantly harder to detect than
previously appreciated.

The insufﬁciency of the test data may not able to detect
overﬁtting issues, and could also be regarded as a type of
data bugs. The approaches for detecting test data insufﬁ-
ciency were discussed in Section 5.3, such as coverage [1],
[76], [92] and mutation score [152].

7.1.3 Skew Detection in Training and Test Data

The training instances and the instances that the model
predicts should exhibit consistent features and distributions.
Kim et al. [127] proposed two measurements to evaluate the
skew between training and test data: one is based on Kernel
Density Estimation (KDE) to approximate the likelihood of
the system having seen a similar input during training, the
other is based on the distance between vectors representing
the neuron activation traces of the given input and the
training data (e.g., Euclidean distance).

Breck [45] investigated the skew in training data and
serving data (the data that the ML model predicts after de-
ployment). To detect the skew in features, they do key-join
feature comparison. To quantify the skew in distribution,
they argued that general approaches such as KL divergence
or cosine similarity might not be sufﬁciently intuitive for
produce teams. Instead, they proposed to use the largest
change in probability as a value in the two distributions as
a measurement of their distance.

7.1.4 Frameworks in Detecting Data Bugs

Breck et al. [45] proposed a data validation system for
detecting data bugs. The system applies constraints (e.g.,
type, domain, valency) to ﬁnd bugs in single-batch (within
the training data or new data), and quantiﬁes the distance
between training data and new data. Their system is de-
ployed as an integral part of TFX (an end-to-end machine
learning platform at Google). The deployment in production
provides evidence that the system helps early detection and
debugging of data bugs. They also summarised the type
of data bugs, in which new feature column, unexpected
string values, and missing feature columns are the three
most common.

Krishnan et al. [225], [226] proposed a model train-
ing framework, ActiveClean, that allows for iterative data
cleaning while preserving provable convergence properties.
ActiveClean suggests a sample of data to clean based on the
data’s value to the model and the likelihood that it is ‘dirty’.
The analyst can then apply value transformations and ﬁlter-
ing operations to the sample to ‘clean’ the identiﬁed dirty
data.

In 2017, Krishnan et al. [180] presented a system named
BoostClean to detect domain value violations (i.e., when an
attribute value is outside of an allowed domain) in training
data. The tool utilises the available cleaning resources such
as Isolation Forest [227] to improve a model’s performance.
After resolving the problems detected, the tool is able to
improve prediction accuracy by up to 9% in comparison to
the best non-ensemble alternative.

ActiveClean and BoostClean may involve a human in
the loop of testing process. Schelter et al. [181] focus on the
automatic ‘unit’ testing of large-scale datasets. Their system

22

provides a declarative API that combines common as well
as user-deﬁned quality constraints for data testing. Krish-
nan and Wu [228] also targeted automatic data cleaning
and proposed AlphaClean. They used a greedy tree search
algorithm to automatically tune the parameters for data
cleaning pipelines. With AlphaClean, the user could focus
on deﬁning cleaning requirements and let the system ﬁnd
the best conﬁguration under the deﬁned requirement. The
evaluation was conducted on three datasets, demonstrating
that AlphaClean ﬁnds solutions of up to 9X better than state-
of-the-art parameter tuning methods.

Training data testing is also regarded as a part of a whole
machine learning workﬂow in the work of Baylor et al. [182].
They developed a machine learning platform that enables
data testing, based on a property description schema that
captures properties such as the features present in the data
and the expected type or valency of each feature.

There are also data cleaning technologies such as statist-
ical or learning approaches from the domain of traditional
database and data warehousing. These approaches are not
specially designed or evaluated for ML, but they can be re-
purposed for ML testing [229].

7.2 Bug Detection in Learning Program

Bug detection in the learning program checks whether the
algorithm is correctly implemented and conﬁgured, e.g., the
model architecture is designed well, and whether there exist
coding errors.
Unit Tests for ML Learning Program. McClure [230] in-
troduced ML unit testing with TensorFlow built-in testing
functions to help ensure that ‘code will function as expected’
to help build developers’ conﬁdence.

Schaul et al. [231] developed a collection of unit tests
specially designed for stochastic optimisation. The tests are
small-scale, isolated with well-understood difﬁculty. They
could be adopted in the beginning learning stage to test the
learning algorithms to detect bugs as early as possible.
Algorithm Conﬁguration Examination. Sun et al. [232] and
Guo et al. [233] identiﬁed operating systems, language, and
hardware Compatibility issues. Sun et al. [232] studied 329
real bugs from three machine learning frameworks: Scikit-
learn, Paddle, and Caffe. Over 22% bugs are found to
be compatibility problems due to incompatible operating
systems, language versions, or conﬂicts with hardware. Guo
et al. [233] investigated deep learning frameworks such as
TensorFlow, Theano, and Torch. They compared the learn-
ing accuracy, model size, robustness with different models
classifying dataset MNIST and CIFAR-10.

The study of Zhang et al. [160] indicates that the most
common learning program bug is due to the change of
TensorFlow API when the implementation has not been
updated accordingly. Additionally, 23.9% (38 in 159) of
the bugs from ML projects in their study built based on
TensorFlow arise from problems in the learning program.

Karpov et al. [234] also highlighted testing algorithm
parameters in all neural network testing problems. The
parameters include the number of neurons and their types
based on the neuron layer types, the ways the neurons
interact with each other, the synapse weights, and the
activation functions. However, the work currently remains
unevaluated.

Algorithm Selection Examination. Developers usually have
more than one learning algorithm to choose from. Fu and
Menzies [235] compared deep learning and classic learning
on the task of linking Stack Overﬂow questions, and found
that classic learning algorithms (such as reﬁned SVM) could
achieve similar (and sometimes better) results at a lower
cost. Similarly, the work of Liu et al. [236] found that the
k-Nearest Neighbours (KNN) algorithm achieves similar
results to deep learning for the task of commit message
generation.
Mutant Simulations of Learning Program Faults. Murphy
et al. [117], [128] used mutants to simulate programming
code errors to investigate whether the proposed meta-
morphic relations are effective at detecting errors. They
introduced three types of mutation operators: switching
comparison operators, mathematical operators, and off-by-
one errors for loop variables.

Dolby et al. [237] extended WALA to support static
analysis of the behaviour of tensors in Tensorﬂow learning
programs written in Python. They deﬁned and tracked
tensor types for machine learning, and changed WALA
to produce a dataﬂow graph to abstract possible program
behavours.

7.3 Bug Detection in Framework

The current research on framework testing focuses on study-
ing framework bugs (Section 7.3.1) and detecting bugs in
framework implementation (Section 7.3.2).

7.3.1 Study of Framework Bugs

Xiao et al. [238] focused on the security vulnerabilities of
popular deep learning frameworks including Caffe, Tensor-
Flow, and Torch. They examined the code of popular deep
learning frameworks. The dependency of these frameworks
was found to be very complex. Multiple vulnerabilities
were identiﬁed in their implementations. The most common
vulnerabilities are bugs that cause programs to crash, non-
terminate, or exhaust memory.

Guo et al. [233] tested deep learning frameworks, in-
cluding TensorFlow, Theano, and Torch, by comparing their
runtime behaviour, training accuracy, and robustness, under
identical algorithm design and conﬁguration. The results
indicate that runtime training behaviours are different for
each framework, while the prediction accuracies remain
similar.

Low Efﬁciency is a problem for ML frameworks, which
may directly lead to inefﬁciency of the models built on them.
Sun et al. [232] found that approximately 10% of reported
framework bugs concern low efﬁciency. These bugs are
usually reported by users. Compared with other types of
bugs, they may take longer for developers to resolve.

23

systems, indicated that approximately 22.6% bugs are due
to incorrect algorithm implementations. Cheng et al. [240]
injected implementation bugs into classic machine learn-
ing code in Weka and observed the performance changes
that resulted. They found that 8% to 40% of the logically
non-equivalent executable mutants (injected implementa-
tion bugs) were statistically indistinguishable from their
original versions.
Solutions for Implementation Bug Detection. Some work
has used multiple implementations or differential testing to
detect bugs. For example, Alebiosu et al. [135] found ﬁve
faults in 10 Naive Bayes implementations and four faults
in 20 k-nearest neighbour implementations. Pham et al. [48]
found 12 bugs in three libraries (i.e., TensorFlow, CNTK, and
Theano), 11 datasets (including ImageNet, MNIST, and KGS
Go game), and 30 pre-trained models (see Section 5.2.2).

However, not every algorithm has multiple implement-
ations. Murphy et al. [10], [115] were the ﬁrst to discuss the
possibilities of applying metamorphic relations to machine
learning implementations. They listed several transform-
ations of the input data that should ought not to bring
changes in outputs, such as multiplying numerical values
by a constant, permuting or reversing the order of the input
data, and adding additional data. Their case studies found
that their metamorphic relations held on three machine
learning applications.

Xie et al. [118] focused on supervised learning. They
proposed to use more speciﬁc metamorphic relations to
test the implementations of supervised classiﬁers. They
discussed ﬁve types of potential metamorphic relations on
KNN and Naive Bayes on randomly generated data. In
2011, they further evaluated their approach using mutated
machine learning code [241]. Among the 43 injected faults
in Weka [119] (injected by MuJava [242]), the metamorphic
relations were able to reveal 39. In their work, the test inputs
were randomly generated data.

Dwarakanath et al. [121] applied metamorphic relations
to ﬁnd implementation bugs in image classiﬁcation. For
classic machine learning such as SVM, they conducted
mutations such as changing feature or instance orders, and
linear scaling of the test features. For deep learning models
such as residual networks (which the data features are not
directly available), they proposed to normalise or scale the
test data, or to change the convolution operation order of
the data. These changes were intended to bring no change
to the model performance when there are no implementa-
tion bugs. Otherwise, implementation bugs are exposed. To
evaluate, they used MutPy to inject mutants that simulate
implementation bugs, of which the proposed metamorphic
relations are able to ﬁnd 71%.

7.3.3 Study of Frameworks Test Oracles

7.3.2 Implementation Testing of Frameworks

Many learning algorithms are implemented inside ML
frameworks. Implementation bugs in ML frameworks may
cause neither crashes, errors, nor efﬁciency problems [239],
making their detection challenging.
Challenges in Implementation Bug Detection. Thung et
al. [159] studied machine learning bugs in 2012. Their res-
ults, regarding 500 bug reports from three machine learning

Nejadgholi and Yang [133] studied the approximated oracles
of four popular deep learning libraries: Tensorﬂow, Theano,
PyTorch, and Keras. 5% to 24% oracles were found to be
approximated oracles with a ﬂexible threshold (in contrast
to certain oracles). 5%-27% of the approximated oracles used
the outputs from other libraries/frameworks. Developers
were also found to modify approximated oracles frequently
due to code evolution.

8 APPLICATION SCENARIOS

Machine learning has been widely adopted in different
areas. This section introduces such domain-speciﬁc testing
approaches in three typical application domains: autonom-
ous driving, machine translation, and neural language in-
ference.

8.1 Autonomous Driving

Testing autonomous vehicles has a comparatively long his-
tory. For example, in 2004, Wegener and Bühler compared
different ﬁtness functions when evaluating the tests of
autonomous car parking systems [243]. Testing autonomous
vehicles also has many research opportunities and open
questions, as pointed out and discussed by Woehrle et
al. [244].

More recently, search-based test generation for AV test-
ing has been successfully applied. Abdessalem et al. [245],
[246] focused on improving the efﬁciency and accuracy of
search-based testing of advanced driver assistance systems
(ADAS) in AVs. Their algorithms use classiﬁcation models
to improve the efﬁciency of the search-based test generation
for critical scenarios. Search algorithms are further used
to reﬁne classiﬁcation models to improve their accuracy.
Abdessalem et al. [247] also proposed FITEST, a multi-
objective search algorithm that searches feature interactions
which violate system requirements or lead to failures.

Most of the current autonomous vehicle systems that
have been put
into the market are semi-autonomous
vehicles, which require a human driver to serve as a fall-
back [161], as was the case with the work of Wegener and
Bühler [243]. An issue that causes the human driver to take
control of the vehicle is called a disengagement.

Banerjee et al. [161] investigated the causes and impacts
of 5,328 disengagements from the data of 12 AV manufac-
turers for 144 vehicles that drove a cumulative 1,116,605
autonomous miles, 42 (0.8%) of which led to accidents. They
classiﬁed the causes of disengagements into 10 types. 64% of
the disengagements were found to be caused by the bugs in
the machine learning system, among which the behaviours
of image classiﬁcation (e.g., improper detection of trafﬁc
lights, lane markings, holes, and bumps) were the dominant
causes accounting for 44% of all reported disengagements.
The remaining 20% were due to the bugs in the control and
decision framework such as improper motion planning.

Pei et al. [1] used gradient-based differential testing to
generate test inputs to detect potential DNN bugs and
leveraged neuron coverage as a guideline. Tian et al. [76]
proposed to use a set of image transformation to gener-
ate tests, which simulate the potential noise that could be
present in images obtained from a real-world camera. Zhang
et al. [79] proposed DeepRoad, a GAN-based approach to
generate test images for real-world driving scenes. Their
approach is able to support two weather conditions (i.e.,
snowy and rainy). The images were generated with the
pictures from YouTube videos. Zhou et al. [81] proposed
DeepBillboard, which generates real-world adversarial bill-
boards that can trigger potential steering errors of autonom-
ous driving systems. It demonstrates the possibility of gen-
erating continuous and realistic physical-world tests for
practical autonomous-driving systems.

24

Wicker et al. [93] used feature-guided Monte Carlo Tree
Search to identify elements of an image that are most vul-
nerable to a self-driving system; adversarial examples. Jha et
al. [96] accelerated the process of ﬁnding ‘safety-critical’ is-
sues via analytically modelling the injection of faults into an
AV system as a Bayesian network. The approach trains the
network to identify safety critical faults automatically. The
evaluation was based on two production-grade AV systems
from NVIDIA and Baidu, indicating that the approach can
ﬁnd many situations where faults lead to safety violations.
Uesato et al. [94] aimed to ﬁnd catastrophic failures
in safety-critical agents like autonomous driving in rein-
forcement learning. They demonstrated the limitations of
traditional random testing, then proposed a predictive ad-
versarial example generation approach to predict failures
and estimate reliable risks. The evaluation on TORCS simu-
lator indicates that the proposed approach is both effective
and efﬁcient with fewer Monte Carlo runs.

To test whether an algorithm can lead to a problematic
model, Dreossi et al. [170] proposed to generate training
data as well as test data. Focusing on Convolutional Neural
Networks (CNN), they build a tool to generate natural
images and visualise the gathered information to detect
blind spots or corner cases under the autonomous driving
scenario. Although there is currently no evaluation, the tool
has been made available11.

Tuncali et al. [102] presented a framework that supports
both system-level testing and the testing of those properties
of an ML component. The framework also supports fuzz test
input generation and search-based testing using approaches
such as Simulated Annealing and Cross-Entropy optimisa-
tion.

While many other studies investigated DNN model
testing for research purposes, Zhou et al. [95] combined
fuzzing and metamorphic testing to test LiDAR, which is
an obstacle-perception module of real-life self-driving cars,
and detected real-life fatal bugs.

Jha et al. presented AVFI [198] and Kayotee [199], which
are fault injection-based tools to systematically inject faults
into autonomous driving systems to assess their safety and
reliability.

O’Kelly et al. [72] proposed a ‘risk-based framework’ for
AV testing to predict the probability of an accident in a base
distribution of trafﬁc behaviour (derived from the public
trafﬁc data collected by the US Department of Transport-
ation). They argued that formally verifying correctness of
an AV system is infeasible due to the challenge of formally
deﬁning “correctness” as well as the white-box requirement.
Traditional testing AVs in a real environment requires pro-
hibitive amounts of time. To tackle these problems, they
view AV testing as rare-event simulation problem, then
evaluate the accident probability to accelerate AV testing.

8.2 Machine Translation

Machine translation automatically translates text or speech
from one language to another. The BLEU (BiLingual Eval-
uation Understudy) score [248] is a widely-adopted meas-
urement criterion to evaluate machine translation quality.

11 https://github.com/shromonag/FalsifyNN

It assesses the correspondence between a machine’s output
and that of a human.

Zhou et al. [129], [130] used self-deﬁned metamorphic
relations in their tool ‘MT4MT’ to test the translation con-
sistency of machine translation systems. The idea is that
some changes to the input should not affect the overall
structure of the translated output. Their evaluation showed
that Google Translate outperformed Microsoft Translator for
long sentences whereas the latter outperformed the former
for short and simple sentences. They hence suggested that
the quality assessment of machine translations should con-
sider multiple dimensions and multiple types of inputs.

Sun et al. [86] combine mutation testing and meta-
morphic testing to test and repair the consistency of machine
translation systems. Their approach, TransRepair, enables
automatic test input generation, automatic test oracle gen-
eration, as well as automatic translation repair. They ﬁrst
applied mutation on sentence inputs to ﬁnd translation
inconsistency bugs, then used translations of the mutated
sentences to optimise the translation results in a black-box or
grey-box manner. Evaluation demonstrates that TransRepair
ﬁxes 28% and 19% bugs on average for Google Translate and
Transformer.

Compared with existing model retraining approaches,
TransRepair has the following advantages: 1) more effect-
ive than data augmentation; 2) source code in dependant
(black box); 3) computationally cheap (avoids space and
time expense of data collection and model retraining); 4)
ﬂexible (enables repair without touching other well-formed
translations).

The work of Zheng et al. [249], [250], [251] proposed
two algorithms for detecting two speciﬁc types of machine
translation violations: (1) under-translation, where some
words/phrases from the original text are missing in the
translation, and (2) over-translation, where some words/-
phrases from the original text are unnecessarily translated
multiple times. The algorithms are based on a statistical
analysis of both the original texts and the translations, to
check whether there are violations of one-to-one mappings
in words/phrases.

8.3 Natural Language Inference

A Nature Language Inference (NLI) task judges the infer-
ence relationship of a pair of natural language sentences.
For example, the sentence ‘A person is in the room’ could
be inferred from the sentence ‘A girl is in the room’.

Several works have tested the robustness of NLI models.
Nie et al. [98] generated sentence mutants (called ‘rule-based
adversaries’ in the paper) to test whether the existing NLI
models have semantic understanding. Seven state-of-the-art
NLI models (with diverse architectures) were all unable to
recognise simple semantic differences when the word-level
information remains unchanged.

Similarly, Wang et al. [99] mutated the inference target
pair by simply swapping them. The heuristic is that a good
NLI model should report comparable accuracy between the
original and swapped test set for contradictory pairs and
for neutral pairs, but lower accuracy in swapped test set for
entailment pairs (the hypothesis may or may not be true
given a premise).

25

9 ANALYSIS OF LITERATURE REVIEW

This section analyses the research distribution among dif-
ferent testing properties and machine learning categories. It
also summarises the datasets (name, description, size, and
usage scenario of each dataset) that have been used in ML
testing.

9.1 Timeline

Figure 8 shows several key contributions in the develop-
ment of ML testing. As early as in 2007, Murphy et al. [10]
mentioned the idea of testing machine learning applica-
tions. They classiﬁed machine learning applications as ‘non-
testable’ programs considering the difﬁculty of getting test
oracles. They primarily consider the detection of imple-
mentation bugs, described as “to ensure that an application
using the algorithm correctly implements the speciﬁcation
and fulﬁls the users’ expectations”. Afterwards, Murphy
et al. [115] discussed the properties of machine learning
algorithms that may be adopted as metamorphic relations
to detect implementation bugs.

In 2009, Xie et al. [118] also applied metamorphic testing

on supervised learning applications.

Fairness testing was proposed in 2012 by Dwork et
al. [138]; the problem of interpretability was proposed in
2016 by Burrell [252].

In 2017, Pei et al. [1] published the ﬁrst white-box testing
paper on deep learning systems. Their work pioneered to
propose coverage criteria for DNN. Enlightened by this pa-
per, a number of machine learning testing techniques have
emerged, such as DeepTest [76], DeepGauge [92], Deep-
Concolic [111], and DeepRoad [79]. A number of software
testing techniques has been applied to ML testing, such as
different testing coverage criteria [76], [92], [145], mutation
testing [152], combinatorial testing [149], metamorphic test-
ing [100], and fuzz testing [89].

9.2 Research Distribution among Machine Learning
Categories

This section introduces and compares the research status of
each machine learning category.

9.2.1 Research Distribution between General Machine
Learning and Deep Learning

To explore research trends in ML testing, we classify the col-
lected papers into two categories: those targeting only deep
learning and those designed for general machine learning
(including deep learning).

Among all 144 papers, 56 papers (38.9%) present testing
techniques that are specially designed for deep learning
alone; the remaining 88 papers cater for general machine
learning.

We further investigated the number of papers in each
category for each year, to observe whether there is a trend
of moving from testing general machine learning to deep
learning. Figure 9 shows the results. Before 2017, papers
mostly focus on general machine learning; after 2018, both
general machine learning and deep learning speciﬁc testing
notably arise.

26

Figure 8: Timeline of ML testing research

Table 4: Search hits and testing distribution of super-
vised/unsupervised/reinforcement Learning

Category
Supervised
Unsupervised
Reinforcement

Scholar hits Google hits
73,500k
17,700k
74,800k

1,610k
619k
2,560k

Testing hits
119/144
3/144
1/144

number/ratio of papers that touch each machine learning
category in ML testing. For example, 119 out of 144 papers
were observed for supervised learning testing purpose. The
table suggests that testing popularity of different categories
is not related to their overall research popularity. In par-
ticular, reinforcement learning has higher search hits than
supervised learning, but we did not observe any related
work that conducts direct reinforcement learning testing.

There may be several reasons for this observation. First,
supervised learning is a widely-known learning scenario
associated with classiﬁcation, regression, and ranking prob-
lems [28]. It is natural that researchers would emphasise the
testing of widely-applied, known and familiar techniques
at the beginning. Second, supervised learning usually has
labels in the dataset. It is thereby easier to judge and analyse
test effectiveness.

Nevertheless, many opportunities clearly remain for re-
search in the widely-studied areas of unsupervised learning
and reinforcement learning (we discuss more in Section 10).

9.2.3 Different Learning Tasks

ML involves different tasks such as classiﬁcation, regression,
clustering, and dimension reduction (see more in Section 2).
The research focus on different tasks also appears to exhibit
imbalance, with a large number of papers focusing on
classiﬁcation.

9.3 Research Distribution among Different Testing
Properties

We counted the number of papers concerning each ML
testing property. Figure 10 shows the results. The properties
in the legend are ranked based on the number of papers
that are specially focused on testing the associated property
(‘general’ refers to those papers discussing or surveying ML
testing generally).

From the ﬁgure, 38.7% of the papers test correctness.
26.8% of the papers focus on robustness and security prob-
lems. Fairness testing ranks the third among all the proper-
ties, with 12.0% of the papers.

Figure 9: Commutative trends in general machine learning
and deep learning

9.2.2 Research Distribution among Supervised/Unsuper-
vised/Reinforcement Learning Testing

We further classiﬁed the papers based on the three machine
learning categories: 1) supervised learning testing, 2) un-
supervised learning testing, and 3) reinforcement learning
testing. One striking ﬁnding is that almost all the work
we identiﬁed in this survey focused on testing supervised
machine learning. Among the 144 related papers, there are
currently only three papers testing unsupervised machine
learning: Murphy et al. [115] introduced metamorphic re-
lations that work for both supervised and unsupervised
learning algorithms. Ramanathan and Pullum [7] proposed
a combination of symbolic and statistical approaches to
test the k-means clustering algorithm. Xie et al. [125] de-
signed metamorphic relations for unsupervised learning.
We were able to ﬁnd out only one paper that focused on
reinforcement learning testing: Uesato et al. [94] proposed
a predictive adversarial example generation approach to
predict failures and estimate reliable risks in reinforcement
learning.

Because of this notable imbalance, we sought to un-
derstand whether there was also an imbalance of research
popularity in the machine learning areas. To approximate
the research popularity of each category, we searched terms
‘supervised learning’, ‘unsupervised learning’, and ‘rein-
forcement learning’ in Google Scholar and Google. Table 4
shows the results of search hits. The last column shows the

First paper on ML testing.Murphy et al.200720202008200920102011201220132014201520162017201820192007201120082015201620172018Metamorphic testing applied Murphy et al. & Xie et al.Fairness awareness.Dwork et al.DeepFool on adversarial examplesMoosavi-Dezfooli et al.Interpretabilityawareness.BurrellDeepXplore: the first white-box DL testing technique. Pei et al.DeepTest, DeepGauge, DeepRoad...YearNumber of publications 02550751002007200820092010201120122013201420152016201720182019general machine learningdeep learningNevertheless, for model relevance, interpretability test-
ing, efﬁciency testing, and privacy testing, fewer than 6
papers exist for each category in our paper collection.

27

Figure 10: Research distribution among different testing
properties

9.4 Datasets Used in ML Testing

8 show details concerning widely-adopted
Tables 5 to
datasets used in ML testing research. In each table, the ﬁrst
column shows the name and link of each dataset. The next
three columns give a brief description, the size (the “+”
connects training data and test data if applicable), the testing
problem(s), the usage application scenario of each dataset12.
Table 5 shows the datasets used for image classiﬁcation
tasks. Datasets can be large (e.g., more than 1.4 million
images in ImageNet). The last six rows show the datasets
collected for autonomous driving system testing. Most im-
age datasets are adopted to test correctness, overﬁtting, and
robustness of ML systems.

Table 6 shows datasets related to natural language pro-
cessing. The contents are usually text, sentences, or text ﬁles,
applied to scenarios like robustness and correctness.

The datasets used to make decisions are introduced in
Table 6. They are usually records with personal information,
and thus are widely adopted to test the fairness of the ML
models.

We also calculate how many datasets an ML testing
paper usually uses in its evaluation (for those papers with
an evaluation). Figure 11 shows the results. Surprisingly,
most papers use only one or two datasets in their evalu-
ation; One reason might be training and testing machine
learning models have high costs. There is one paper with as
many as 600 datasets, but that paper used these datasets to
evaluate data cleaning techniques, which has relatively low
cost [179].

We also discuss research directions of building dataset

and benchmarks for ML testing in Section 10.

9.5 Open-source Tool Support in ML Testing

There are several tools specially designed for ML testing.
Angell et al. presented Themis [213], an open-source tool
for testing group discrimination14. There is also an ML test-
ing framework for tensorﬂow, named mltest15, for writing

12 These tables do not list datasets adopted in data cleaning evaluation,

because such studies usually involve hundreds of data sets [180]

14 http://fairness.cs.umass.edu/
15 https://github.com/Thenerdstation/mltest

Figure 11: Number of papers with different amounts of
datasets in experiments

simple ML unit tests. Similar to mltest, there is a testing
framework for writing unit tests for pytorch-based ML sys-
tems, named torchtest16. Dolby et al. [237] extended WALA
to enable static analysis for machine learning code using
TensorFlow.

Compared to traditional testing, the existing tool support
in ML testing is relatively immature. There remains plenty
of space for tool-support improvement for ML testing.

10 CHALLENGES AND OPPORTUNITIES

This section discusses the challenges (Section 10.1) and
research opportunities in ML testing (Section 10.2).

10.1 Challenges in ML Testing

As this survey reveals, ML testing has experienced rapid
recent growth. Nevertheless, ML testing remains at an early
stage in its development, with many challenges and open
questions lying ahead.
Challenges in Test Input Generation. Although a range
of test input generation techniques have been proposed
(see more in Section 5.1), test input generation remains
challenging because of the large behaviour space of ML
models.

Search-based Software Test generation (SBST) [87] uses
a meta-heuristic optimising search technique, such as a
Genetic Algorithm, to automatically generate test inputs. It
is a test generation technique that has been widely used
in research (and deployment [288]) for traditional software
testing paradigms. As well as generating test inputs for test-
ing functional properties like program correctness, SBST has
also been used to explore tensions in algorithmic fairness in
requirement analysis. [205], [289]. SBST has been success-
fully applied in testing autonomous driving systems [245],
[246], [247]. There exist many research opportunities in
applying SBST on generating test inputs for testing other
ML systems, since there is a clear apparent ﬁt between SBST
and ML; SBST adaptively searches for test inputs in large
input spaces.

Existing test input generation techniques focus on gen-
erating adversarial inputs to test the robustness of an ML

16 https://github.com/suriyadeepan/torchtest

38.7%26.8%12.7%12.0%correctnessrobustness&securitygeneralfairnessmodel relevanceinterpretabilityefficiencyNumber of papersNumber of data sets1234689111213600010203040Dataset

MNIST [253]

Table 5: Datasets (1/4): Image Classiﬁcation

Description

Size

Usage

28

Images of handwritten digits

60,000+10,000

Fashion MNIST [254]

MNIST-like dataset of fashion images

70,000

CIFAR-10 [255]

General images with 10 classes

50,000+10,000

ImageNet [256]

Visual recognition challenge dataset

14,197,122

correctness, overﬁt-
ting, robustness

correctness, overﬁt-
ting

correctness, overﬁt-
ting, robustness

correctness, robust-
ness

IRIS ﬂower [257]

SVHN [258]

Fruits 360 [259]

The Iris ﬂowers

House numbers

Dataset with 65,429 images of 95 fruits

Handwritten Letters [260]

Colour images of Russian letters

Balance Scale [261]

Psychological experimental results

DSRC [262]

Wireless communications between vehicles and road
side units

150

overﬁtting

73,257+26,032

correctness,robustness

65,429

1,650

625

10,000

correctness,robustness

correctness,robustness

overﬁtting

overﬁtting, robust-
ness

Udacity challenge [77]

Udacity Self-Driving Car Challenge images

101,396+5,614

robustness

light

chal-

Dashboard camera

18,659+500,000

robustness

Nexar
lenge [263]

trafﬁc

MSCOCO [264]

Object recognition

Autopilot-TensorFlow [265]

Recorded to test the NVIDIA Dave model

KITTI [266]

Six different scenes captured by a VW Passat station
wagon equipped with four video cameras

160,000

45,568

14,999

correctness

robustness

robustness

Table 6: Datasets (2/4): Natural Language Processing

Dataset

bAbI [267]

Description

questions and answers for NLP

Tiny Shakespeare [268]

Samples from actual Shakespeare

Size

Usage

1000+1000

robustness

100,000 char-
acter

correctness

Data

Stack Overﬂow questions and answers

365 ﬁles

correctness

Stanford Natural Language Inference Corpus

Crowd-sourced collection of sentence pairs annotated
with textual entailment information

570,000

433,000

robustness

robustness

correctness

DMV failure reports [272]

AV failure reports from 12 manufacturers in Califor-
nia13

keep
updating

Stack
Dump [269]

Exchange

SNLI [270]

MultiNLI [271]

system. However, adversarial examples are often criticised
because they do not represent real input data. Thus, an in-
teresting research direction is to how to generate natural test
inputs and how to automatically measure the naturalness of
the generated inputs.

There has been work that tries to generate test inputs to
be as natural as possible under the scenario of autonomous
driving, such as DeepTest [76], DeepHunter [92] and Deep-
Road [79], yet the generated images could still suffer from
unnaturalness: sometimes even humans may not recognise
the images generated by these tools. It is both interesting
and challenging to explore whether such kinds of test data
that are meaningless to humans should be adopted/valid in
ML testing.
Challenges on Test Assessment Criteria. There has been a
lot of work exploring how to assess the quality or adequacy

of test data (see more in Section 5.3). However, there is
still a lack of systematic evaluation about how different as-
sessment metrics correlate, or how these assessment metrics
correlate with tests’ fault-revealing ability, a topic that has
been widely studied in traditional software testing [290].
The relation between test assessment criteria and test suf-
ﬁciency remains unclear. In addition, assessment criteria
may provide a way of interpreting and understanding
behaviours of ML models, which might be an interesting
direction for further exploration.
Challenges Relating to The Oracle Problem. The oracle
problem remains a challenge in ML testing. Metamorphic
relations are effective pseudo oracles but, in most cases,
they need to be deﬁned by human ingenuity . A remaining
challenge is thus to automatically identify and construct
reliable test oracles for ML testing.

29

Table 7: Datasets (3/4): Records for Decision Making

Dataset

Description

German Credit [273]

Descriptions of customers with good and bad credit
risks

Adult [274]

Census income

Bank Marketing [275]

Bank client subscription term deposit data

US Executions [276]

Records of every execution performed in the United
States

Fraud Detection [277]

European Credit cards transactions

Berkeley
Data [278]

Admissions

Graduate school applications to the six largest depart-
ments at University of California, Berkeley in 1973

Size

1,000

48,842

45,211

1,437

284,807

4,526

Usage

fairness

fairness

fairness

fairness

fairness

fairness

Broward
COMPAS [279]

County

Score to determine whether to release a defendant

18,610

fairness

MovieLens Datasets [280]

People’s preferences for movies

Zestimate [281]

data about homes and Zillow’s in-house price and
predictions

FICO scores [282]

United States credit worthiness

Law school success [283]

Information concerning law students from 163 law
schools in the United States

100k-20m

2,990,000

fairness

correctness

301,536

21,790

fairness

fairness

Dataset

VirusTotal [284]

Contagio [285]

Drebin [286]

Chess [287]

Table 8: Datasets (4/4): Others

Description

Malicious PDF ﬁles

Clean and malicious ﬁles

Applications from different malware families

Chess game data: King+Rook versus King+Pawn on
a7

Size

5,000

28,760

123,453

3,196

Usage

robustness

robustness

robustness

correctness

Waveform [261]

CART book’s generated waveform data

5,000

correctness

Murphy et al. [128] discussed how ﬂaky tests are likely
to arise in metamorphic testing whenever ﬂoating point
calculations are involved. Flaky test detection is a chal-
lenging problem in traditional software testing [288]. It is
perhaps more challenging in ML testing because of the
oracle problem.

Even without ﬂaky tests, pseudo oracles may be inac-
curate, leading to many false positives. There is, therefore,
a need to explore how to yield more accurate test oracles
and how to reduce the false positives among the reported
issues. We could even use ML algorithm b to learn to detect
false-positive oracles when testing ML algorithm a.
Challenges in Testing Cost Reduction. In traditional soft-
ware testing, the cost problem remains a big problem,
yielding many cost reduction techniques such as test selec-
tion, test prioritisation, and predicting test execution results.
In ML testing, the cost problem could be more serious,
especially when testing the ML component, because ML
component testing usually requires model retraining or
repeating of the prediction process. It may also require data
generation to explore the enormous mode behaviour space.
A possible research direction for cost reduction is to
represent an ML model as an intermediate state to make
it easier for testing.

We could also apply traditional cost reduction tech-
niques such as test prioritisation or minimisation to reduce

the size of test cases without affecting the test correctness.

More ML solutions are deployed to diverse devices and
platforms (e.g., mobile device, IoT edge device). Due to the
resource limitation of a target device, how to effectively test
ML model on diverse devices as well as the deployment
process would be also a challenge.

10.2 Research Opportunities in ML testing

There remain many research opportunities in ML testing.
These are not necessarily research challenges, but may
greatly beneﬁt machine learning developers and users as
well as the whole research community.
Testing More Application Scenarios. Much current re-
search focuses on supervised learning, in particular clas-
siﬁcation problems. More research is needed on problems
associated with testing unsupervised and reinforcement
learning.

The testing tasks currently tackled in the literature,
primarily centre on image classiﬁcation. There remain open
exciting testing research opportunities in many other areas,
such as speech recognition, natural language processing and
agent/game play.
Testing More ML Categories and Tasks. We observed
pronounced imbalance regarding the coverage of testing
techniques for different machine learning categories and

tasks, as demonstrated by Table 4. There are both challenges
and research opportunities for testing unsupervised and
reinforcement learning systems.

al. [205], [289], a good requirements analysis may tackle
many non-functional properties such as fairness.

Existing work is focused on off-line testing. Online-

30

For instance, transfer learning, a topic gaining much
recent interest, focuses on storing knowledge gained while
solving one problem and applying it to a different but
related problem [291]. Transfer learning testing is also im-
portant, yet poorly covered in the existing literature.
Testing Other Properties. From Figure 10, we can see that
most work tests robustness and correctness, while relatively
few papers (less than 3%) study efﬁciency, model relevance,
or interpretability.

Model relevance testing is challenging because the distri-
bution of the future data is often unknown, while the capa-
city of many models is also unknown and hard to measure.
It might be interesting to conduct empirical studies on the
prevalence of poor model relevance among ML models as
well as on the balance between poor model relevance and
high security risks.

For testing efﬁciency, there is a need to test the efﬁciency
at different levels such as the efﬁciency when switching
among different platforms, machine learning frameworks,
and hardware devices.

For testing property interpretability, existing approaches
rely primarily on manual assessment, which checks whether
humans could understand the logic or predictive results of
an ML model. It will be also interesting to investigate the
automatic assessment of interpretability and the detection
of interpretability violations.

There is a lack of consensus regarding the deﬁnitions and
understanding of fairness and interpretability. There is thus
a need for clearer deﬁnitions, formalisation, and empirical
studies under different contexts.

There has been a discussion that machine learning test-
ing and traditional software testing may have different
requirements in the assurance to be expected for different
properties [292]. Therefore, more work is needed to explore
and identify those properties that are most important for
machine learning systems, and thus deserve more research
and test effort.
Presenting More Testing Benchmarks A large number of
datasets have been adopted in the existing ML testing pa-
pers. As Tables 5 to 8 show, these datasets are usually those
adopted for building machine learning systems. As far as
we know, there are very few benchmarks like CleverHans17
that are specially designed for ML testing research purposes,
such as adversarial example construction.

More benchmarks are needed, that are specially de-
signed for ML testing. For example, a repository of machine
learning programs with real bugs would present a good
benchmark for bug-ﬁxing techniques. Such an ML testing
repository, would play a similar (and equally-important)
role to that played by data sets such as Defects4J18 in
traditional software testing.
Covering More Testing Activities. As far we know, re-
quirement analysis for ML systems remains absent in the
ML testing literature. As demonstrated by Finkelstein et

17 https://github.com/tensorﬂow/cleverhans
18 https://github.com/rjust/defects4j

testing deserves more research efforts.

According to the work of Amershi et al. [8], data testing
is especially important. This topic certainly deserves more
research effort. Additionally, there are also many oppor-
tunities for regression testing, bug report analysis, and bug
triage in ML testing.

Due to the black-box nature of machine learning al-
gorithms, ML testing results are often more difﬁcult for
developers to understand, compared to traditional software
testing. Visualisation of testing results might be particularly
helpful in ML testing to help developers understand the
bugs and help with the bug localisation and repair.
Mutating Investigation in Machine Learning System.
There have been some studies discussing mutating machine
learning code [128], [240], but no work has explored how
to better design mutation operators for machine learning
code so that the mutants could better simulate real-world
machine learning bugs. This is another research opportunity.

11 CONCLUSION
We provided a comprehensive overview and analysis of
research work on ML testing. The survey presented the
deﬁnitions and current research status of different ML test-
ing properties, testing components, and testing workﬂows.
It also summarised the datasets used for experiments and
the available open-source testing tools/frameworks, and
analysed the research trends, directions, opportunities, and
challenges in ML testing. We hope this survey will help
software engineering and machine learning researchers to
become familiar with the current status and open opportun-
ities of and for of ML testing.

ACKNOWLEDGEMENT
Before submitting, we sent the paper to those whom we
cited, to check our comments for accuracy and omission.
This also provided one ﬁnal stage in the systematic trawling
of the literature for relevant work. Many thanks to those
members of the community who kindly provided comments
and feedback on earlier drafts of this paper.

REFERENCES

[1]

[2]

[3]

[4]

[5]

Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. Deepx-
plore: Automated whitebox testing of deep learning systems. In
Proceedings of the 26th Symposium on Operating Systems Principles,
pages 1–18. ACM, 2017.
Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao.
Deepdriving: Learning affordance for direct perception in
In Proceedings of the IEEE International
autonomous driving.
Conference on Computer Vision, pages 2722–2730, 2015.
Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud
Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian,
Jeroen Awm Van Der Laak, Bram Van Ginneken, and Clara I
Sánchez. A survey on deep learning in medical image analysis.
Medical image analysis, 42:60–88, 2017.
Paul Ammann and Jeff Offutt.
Cambridge University Press, 2016.
Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. Fairness
testing: testing software for discrimination. In Proceedings of the
2017 11th Joint Meeting on Foundations of Software Engineering,
pages 498–510. ACM, 2017.

Introduction to software testing.

[6] Maciej Pacula. Unit-Testing Statistical Software. http://blog.

[8]

[7]

mpacula.com/2011/02/17/unit-testing-statistical-software/,
2011.
A. Ramanathan, L. L. Pullum, F. Hussain, D. Chakrabarty, and
S. K. Jha. Integrating symbolic and statistical methods for testing
intelligent systems: Applications to machine learning and com-
puter vision. In 2016 Design, Automation Test in Europe Conference
Exhibition (DATE), pages 786–791, March 2016.
Saleema Amershi, Andrew Begel, Christian Bird, Rob DeLine,
Harald Gall, Ece Kamar, Nachi Nagappan, Besmira Nushi, and
Tom Zimmermann. Software engineering for machine learning:
A case study. In Proc. ICSE (Industry Track), pages 291–300, 2019.
Earl T Barr, Mark Harman, Phil McMinn, Muzammil Shahbaz,
and Shin Yoo. The oracle problem in software testing: A survey.
IEEE transactions on software engineering, 41(5):507–525, 2015.
[10] Christian Murphy, Gail E Kaiser, and Marta Arias. An approach
In Proc.

to software testing of machine learning applications.
SEKE, volume 167, 2007.

[9]

[11] Martin D. Davis and Elaine J. Weyuker. Pseudo-oracles for non-
testable programs. In Proceedings of the ACM 81 Conference, ACM
81, pages 254–257, 1981.

[12] Kelly Androutsopoulos, David Clark, Haitao Dan, Mark Har-
man, and Robert Hierons. An analysis of the relationship
between conditional entropy and failed error propagation in
software testing. In Proc. ICSE, pages 573–583, Hyderabad, India,
June 2014.
Jeffrey M. Voas and Keith W. Miller. Software testability: The new
veriﬁcation. IEEE Software, 12(3):17–28, May 1995.

[13]

[14] David Clark and Robert M. Hierons. Squeeziness: An inform-
ation theoretic measure for avoiding fault masking. Information
Processing Letters, 112(8–9):335 – 340, 2012.

[15] Yue Jia and Mark Harman. Constructing subtle faults using
higher order mutation testing (best paper award winner).
In
8th International Working Conference on Source Code Analysis and
Manipulation (SCAM’08), pages 249–258, Beijing, China, 2008.
IEEE Computer Society.

[16] Christopher D Turner and David J Robson. The state-based
testing of object-oriented programs. In 1993 Conference on Software
Maintenance, pages 302–310. IEEE, 1993.

[17] Mark Harman and Bryan F Jones. Search-based software engin-

eering. Information and software Technology, 43(14):833–839, 2001.

[18] Mark Harman and Phil McMinn. A theoretical and empirical
study of search-based testing: Local, global, and hybrid search.
IEEE Transactions on Software Engineering, 36(2):226–247, 2010.

[19] Atif M Memon. Gui testing: Pitfalls and process. Computer,

(8):87–88, 2002.

[20] Koushik Sen, Darko Marinov, and Gul Agha. Cute: a concolic
unit testing engine for c. In ACM SIGSOFT Software Engineering
Notes, volume 30, pages 263–272. ACM, 2005.

[21] G. Hains, A. Jakobsson, and Y. Khmelevsky. Towards formal
methods and software engineering for deep learning: Security,
In 2018
safety and productivity for DL systems development.
Annual IEEE International Systems Conference (SysCon), pages 1–5,
April 2018.

[22] Lei Ma, Felix Juefei-Xu, Minhui Xue, Qiang Hu, Sen Chen,
Bo Li, Yang Liu, Jianjun Zhao, Jianxiong Yin, and Simon See.
Secure deep learning engineering: A software quality assurance
perspective. arXiv preprint arXiv:1810.04538, 2018.

[23] Xiaowei Huang, Daniel Kroening, Marta Kwiatkowska, Wenjie
Ruan, Youcheng Sun, Emese Thamo, Min Wu, and Xinping Yi.
Safety and trustworthiness of deep neural networks: A survey.
arXiv preprint arXiv:1812.08342, 2018.
Satoshi Masuda, Kohichi Ono, Toshiaki Yasue, and Nobuhiro
Hosokawa. A survey of software quality for machine learning
In 2018 IEEE International Conference on Software
applications.
Testing, Veriﬁcation and Validation Workshops (ICSTW), pages 279–
284. IEEE, 2018.

[24]

[25] Fuyuki Ishikawa. Concepts in quality assessment for machine
learning-from test data to arguments. In International Conference
on Conceptual Modeling, pages 536–544. Springer, 2018.

[26] Houssem Braiek and Foutse Khomh. On testing machine learning

[27]

programs. arXiv preprint arXiv:1812.02257, 2018.
John Shawe-Taylor, Nello Cristianini, et al. Kernel methods for
pattern analysis. Cambridge university press, 2004.

[28] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.

Foundations of machine learning. MIT press, 2012.

31

[29] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan,
Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison,
Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.

[30] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,
Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, An-
drew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg,
Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris
Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sut-
skever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay
Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Mar-
tin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.
TensorFlow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorﬂow.org.

[31] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg,
J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot,
and E. Duchesnay. Scikit-learn: Machine Learning in Python .
Journal of Machine Learning Research, 12:2825–2830, 2011.

[32] François Chollet et al. Keras. https://keras.io, 2015.
[33] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev,
Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor
Darrell. Caffe: Convolutional architecture for fast feature embed-
ding. arXiv preprint arXiv:1408.5093, 2014.

[34] Michael I Jordan and Tom M Mitchell. Machine learning: Trends,

[35]

[36]

perspectives, and prospects. Science, 349(6245):255–260, 2015.
S Rasoul Safavian and David Landgrebe. A survey of decision
tree classiﬁer methodology. IEEE transactions on systems, man, and
cybernetics, 21(3):660–674, 1991.
John Neter, Michael H Kutner, Christopher J Nachtsheim, and
William Wasserman. Applied linear statistical models, volume 4.
Irwin Chicago, 1996.

[37] Andrew McCallum, Kamal Nigam, et al. A comparison of event
models for naive bayes text classiﬁcation. In AAAI-98 workshop on
learning for text categorization, volume 752, pages 41–48. Citeseer,
1998.

[38] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learn-

ing. nature, 521(7553):436, 2015.

[39] Yoon Kim. Convolutional neural networks for sentence classiﬁc-

ation. arXiv preprint arXiv:1408.5882, 2014.

[41]

[40] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton.
Speech recognition with deep recurrent neural networks.
In
2013 IEEE international conference on acoustics, speech and signal
processing, pages 6645–6649. IEEE, 2013.
IEEE Std
Ieee standard classiﬁcation for software anomalies.
1044-2009 (Revision of IEEE Std 1044-1993), pages 1–23, Jan 2010.
[42] Roman Werpachowski, András György, and Csaba Szepesvári.
arXiv preprint

Detecting overﬁtting via adversarial examples.
arXiv:1903.02380, 2019.
Juan Cruz-Benito, Andrea Vázquez-Ingelmo,
José Carlos
Sánchez-Prieto, Roberto Therón, Francisco José García-Peñalvo,
and Martín Martín-González. Enabling adaptability in web forms
based on user characteristics detection through a/b testing and
machine learning. IEEE Access, 6:2251–2265, 2018.

[43]

[44] Emilie Kaufmann, Olivier Cappé, and Aurélien Garivier. On
the complexity of best-arm identiﬁcation in multi-armed bandit
models. The Journal of Machine Learning Research, 17(1):1–42, 2016.
[45] Eric Breck, Neoklis Polyzotis, Sudip Roy, Steven Whang, and
Martin Zinkevich. Data Validation for Machine Learning.
In
SysML, 2019.

[46] Chih-Hong Cheng, Georg Nührenberg, Chung-Hao Huang, and
Hirotoshi Yasuoka. Towards dependability metrics for neural
networks. arXiv preprint arXiv:1806.02338, 2018.
Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data poisoning
attacks against autoregressive models. In AAAI, pages 1452–1458,
2016.

[47]

[48] Weizhen Qi Lin Tan Viet Hung Pham, Thibaud Lutellier. Cradle:
Cross-backend validation to detect and localize bugs in deep
learning libraries. In Proc. ICSE, 2019.

[49] Lawrence Chung, Brian A Nixon, Eric Yu, and John Mylopoulos.
Non-functional requirements in software engineering, volume 5.
Springer Science & Business Media, 2012.

[50] Wasif Afzal, Richard Torkar, and Robert Feldt. A systematic
review of search-based testing for non-functional system prop-
erties. Information and Software Technology, 51(6):957–976, 2009.

[51] Matthew Kirk. Thoughtful machine learning: A test-driven approach.

" O’Reilly Media, Inc.", 2014.

[52] Vladimir Vapnik, Esther Levin, and Yann Le Cun. Measuring the
vc-dimension of a learning machine. Neural computation, 6(5):851–
876, 1994.

[53] David S Rosenberg and Peter L Bartlett. The rademacher com-
In Artiﬁcial Intelligence

plexity of co-regularized kernel classes.
and Statistics, pages 396–403, 2007.
Jie Zhang, Earl T Barr, Benjamin Guedj, Mark Harman, and John
Shawe-Taylor. Perturbed Model Validation: A New Framework
to Validate Model Relevance. working paper or preprint, May
2019.
Ieee standard glossary of software engineering terminology. IEEE
Std 610.12-1990, pages 1–84, Dec 1990.

[54]

[55]

[56] Ali Shahrokni and Robert Feldt. A systematic review of software
robustness. Information and Software Technology, 55(1):1–17, 2013.
[57] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J
Kochenderfer. Reluplex: An efﬁcient smt solver for verifying
In International Conference on Computer
deep neural networks.
Aided Veriﬁcation, pages 97–117. Springer, 2017.

[58] Cynthia Dwork. Differential privacy. Encyclopedia of Cryptography

and Security, pages 338–340, 2011.

[59] Paul Voigt and Axel von dem Bussche. The EU General Data Pro-
tection Regulation (GDPR): A Practical Guide. Springer Publishing
Company, Incorporated, 1st edition, 2017.

[60] wikipedia. California Consumer Privacy Act.

https://en.

wikipedia.org/wiki/California_Consumer_Privacy_Act.
[61] R. Baeza-Yates and Z. Liaghat. Quality-efﬁciency trade-offs in
machine learning for text processing. In 2017 IEEE International
Conference on Big Data (Big Data), pages 897–904, Dec 2017.
Sam Corbett-Davies and Sharad Goel. The measure and mis-
measure of fairness: A critical review of fair machine learning.
arXiv preprint arXiv:1808.00023, 2018.

[62]

[63] Drew S Days III. Feedback loop: The civil rights act of 1964 and

its progeny. . Louis ULJ, 49:981, 2004.

[64] Zachary C Lipton. The mythos of model interpretability. arXiv

preprint arXiv:1606.03490, 2016.

[65] Finale Doshi-Velez and Been Kim. Towards a rigorous science of
interpretable machine learning. arXiv preprint arXiv:1702.08608,
2017.

[66] Thibault Sellam, Kevin Lin, Ian Yiran Huang, Michelle Yang, Carl
Vondrick, and Eugene Wu. Deepbase: Deep inspection of neural
networks. In Proc. SIGMOD, 2019.

[67] Or Biran and Courtenay Cotton. Explanation and justiﬁcation in
machine learning: A survey. In IJCAI-17 Workshop on Explainable
AI (XAI), page 8, 2017.

[68] Tim Miller. Explanation in artiﬁcial intelligence: Insights from

the social sciences. Artiﬁcial Intelligence, 2018.

[69] Bryce Goodman and Seth Flaxman. European union regulations
on algorithmic decision-making and a" right to explanation".
arXiv preprint arXiv:1606.08813, 2016.

[70] Christoph Molnar.

Interpretable Machine

[71]

https://christophm.github.io/interpretable-ml-book/,
https://christophm.github.io/interpretable-ml-book/.
Jie Zhang, Junjie Chen, Dan Hao, Yingfei Xiong, Bing Xie,
Lu Zhang, and Hong Mei. Search-based inference of polynomial
In Proceedings of the 29th ACM/IEEE
metamorphic relations.
international conference on Automated software engineering, pages
701–712. ACM, 2014.

Learning.
2019.

[72] Matthew O’Kelly, Aman Sinha, Hongseok Namkoong, Russ
Tedrake, and John C Duchi. Scalable end-to-end autonomous
vehicle testing via rare-event simulation. In Advances in Neural
Information Processing Systems, pages 9827–9838, 2018.

[73] Weiming Xiang, Patrick Musau, Ayana A Wild, Diego Manzanas
Lopez, Nathaniel Hamilton, Xiaodong Yang, Joel Rosenfeld, and
Taylor T Johnson. Veriﬁcation for machine learning, autonomy,
arXiv preprint arXiv:1810.01989,
and neural networks survey.
2018.

[74] Claes Wohlin. Guidelines for snowballing in systematic literature
studies and a replication in software engineering. In Proceedings
of the 18th international conference on evaluation and assessment in
software engineering, page 38. Citeseer, 2014.

[75] Aws Albarghouthi and Samuel Vinitsky. Fairness-aware pro-
gramming. In Proceedings of the Conference on Fairness, Account-
ability, and Transparency, pages 211–219. ACM, 2019.

[76] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. DeepTest:
Automated testing of deep-neural-network-driven autonomous

32

cars. In Proceedings of the 40th International Conference on Software
Engineering, pages 303–314. ACM, 2018.

[77] udacity. Udacity challenge. https://github.com/udacity/self-

[78]

driving-car.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,
David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua
In Advances in neural
Bengio. Generative adversarial nets.
information processing systems, pages 2672–2680, 2014.

[79] Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and
Sarfraz Khurshid. DeepRoad: GAN-based Metamorphic Testing
and Input Validation Framework for Autonomous Driving Sys-
tems. In Proceedings of the 33rd ACM/IEEE International Conference
on Automated Software Engineering, ASE 2018, pages 132–142, 2018.
[80] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised
In Advances in Neural

image-to-image translation networks.
Information Processing Systems, pages 700–708, 2017.

[81] Husheng Zhou, Wei Li, Yuankun Zhu, Yuqun Zhang, Bei Yu,
Lingming Zhang, and Cong Liu. Deepbillboard: Systematic
arXiv
physical-world testing of autonomous driving systems.
preprint arXiv:1812.10812, 2018.

[82] Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Jianjun Zhao, and Yang
Liu. Deepcruiser: Automated guided testing for stateful deep
learning systems. arXiv preprint arXiv:1812.05339, 2018.
Junhua Ding, Dongmei Zhang, and Xin-Hua Hu. A framework
In 2016 IEEE
for ensuring the quality of a big data service.
International Conference on Services Computing (SCC), pages 82–89.
IEEE, 2016.

[83]

[84] Md Raﬁqul Islam Rabin, Ke Wang, and Mohammad Amin Ali-

pour. Testing neural program analyzers, 2019.

[85] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav.
code2vec: Learning distributed representations of code. Proceed-
ings of the ACM on Programming Languages, 3(POPL):40, 2019.
[86] Zeyu Sun, Jie M. Zhang, Mark Harman, Mike Papadakis, and
Lu Zhang. Automatic testing and improvement of machine
translation. In Proc. ICSE (to appear), 2020.

[87] Phil McMinn.

Search-based software test data generation: a
survey. Software testing, Veriﬁcation and reliability, 14(2):105–156,
2004.

[88] Kiran Lakhotia, Mark Harman, and Phil McMinn. A multi-
objective approach to search-based test data generation.
In
Proceedings of the 9th annual conference on Genetic and evolutionary
computation, pages 1098–1105. ACM, 2007.

[89] Augustus Odena and Ian Goodfellow. TensorFuzz: Debugging
Neural Networks with Coverage-Guided Fuzzing. arXiv preprint
arXiv:1807.10875, 2018.
Jianmin Guo, Yu Jiang, Yue Zhao, Quan Chen, and Jiaguang Sun.
Dlfuzz: differential fuzzing testing of deep learning systems. In
Proc. FSE, pages 739–743. ACM, 2018.

[90]

[91] Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Hongxu Chen, Minhui Xue,
Bo Li, Yang Liu, Jianjun Zhao, Jianxiong Yin, and Simon See.
Coverage-guided fuzzing for deep neural networks. arXiv pre-
print arXiv:1809.01266, 2018.

[92] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui
Xue, Bo Li, Chunyang Chen, Ting Su, Li Li, Yang Liu, Jianjun
Zhao, and Yadong Wang. DeepGauge: Multi-granularity Testing
In Proceedings of the 33rd
Criteria for Deep Learning Systems.
ACM/IEEE International Conference on Automated Software Engin-
eering, ASE 2018, pages 120–131, 2018.

[93] Matthew Wicker, Xiaowei Huang, and Marta Kwiatkowska.
Feature-guided black-box safety testing of deep neural networks.
In International Conference on Tools and Algorithms for the Construc-
tion and Analysis of Systems, pages 408–426, 2018.
Jonathan Uesato, Ananya Kumar, Csaba Szepesvari, Tom Erez,
Avraham Ruderman, Keith Anderson, Nicolas Heess, Pushmeet
Kohli, et al. Rigorous agent evaluation: An adversarial approach
In International Conference on
to uncover catastrophic failures.
Learning Representations, 2019.

[94]

[95] Zhi Quan Zhou and Liqun Sun. Metamorphic testing of driver-

[96]

[97]

less cars. Communications of the ACM, 62(3):61–67, 2019.
Saurabh Jha, Subho S Banerjee, Timothy Tsai, Siva KS Hari,
Michael B Sullivan, Zbigniew T Kalbarczyk, Stephen W Keckler,
and Ravishankar K Iyer. Ml-based fault injection for autonomous
vehicles. In Proc. DSN, 2019.
Sakshi Udeshi and Sudipta Chattopadhyay. Grammar based
arXiv preprint
directed testing of machine learning systems.
arXiv:1902.10027, 2019.

[98] Yixin Nie, Yicheng Wang, and Mohit Bansal.
compositionality-sensitivity of nli models.
arXiv:1811.07033, 2018.

Analyzing
arXiv preprint

[99] Haohan Wang, Da Sun, and Eric P Xing. What if we simply swap
the two text fragments? a straightforward yet effective way to
test the robustness of methods to confounding signals in nature
language inference tasks. arXiv preprint arXiv:1809.02719, 2018.

[100] Alvin Chan, Lei Ma, Felix Juefei-Xu, Xiaofei Xie, Yang Liu,
and Yew Soon Ong. Metamorphic relation based adversarial
arXiv preprint
attacks on differentiable neural computer.
arXiv:1809.02444, 2018.

[101] Sakshi Udeshi, Pryanshu Arora, and Sudipta Chattopadhyay.
In Proceedings of the 33rd
Automated directed fairness testing.
ACM/IEEE International Conference on Automated Software Engin-
eering, pages 98–108. ACM, 2018.

[102] Cumhur Erkan Tuncali, Georgios Fainekos, Hisahiro Ito, and
James Kapinski. Simulation-based adversarial test generation
for autonomous vehicles with machine learning components. In
IEEE Intelligent Vehicles Symposium (IV), 2018.

[103] Alan Hartman. Software and hardware testing using combinat-
orial covering suites. In Graph theory, combinatorics and algorithms,
pages 237–266. Springer, 2005.

[104] Scott Kirkpatrick, C Daniel Gelatt, and Mario P Vecchi. Optimiz-

ation by simulated annealing. science, 220(4598):671–680, 1983.

[105] James C King. Symbolic execution and program testing. Commu-

nications of the ACM, 19(7):385–394, 1976.

[106] Lingming Zhang, Tao Xie, Lu Zhang, Nikolai Tillmann, Jonathan
De Halleux, and Hong Mei. Test generation via dynamic sym-
In Software Maintenance
bolic execution for mutation testing.
(ICSM), 2010 IEEE International Conference on, pages 1–10. IEEE,
2010.

[107] Ting Chen, Xiao-song Zhang, Shi-ze Guo, Hong-yuan Li, and Yue
Wu. State of the art: Dynamic symbolic execution for automated
test generation. Future Generation Computer Systems, 29(7):1758–
1773, 2013.

[108] Divya Gopinath, Kaiyuan Wang, Mengshi Zhang, Corina S Pas-
Symbolic execution for deep

areanu, and Sarfraz Khurshid.
neural networks. arXiv preprint arXiv:1807.10439, 2018.

[109] Aniya Agarwal, Pranay Lohia, Seema Nagar, Kuntal Dey, and
Diptikalyan Saha. Automated test generation to detect individual
discrimination in ai models. arXiv preprint arXiv:1809.03260, 2018.
[110] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why
should i trust you?: Explaining the predictions of any classiﬁer.
In Proceedings of the 22nd ACM SIGKDD international conference on
knowledge discovery and data mining, pages 1135–1144. ACM, 2016.
[111] Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta
Kwiatkowska, and Daniel Kroening. Concolic testing for deep
neural networks. In Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering, ASE 2018, pages
109–119, 2018.

[112] Christian Murphy, Gail Kaiser, and Marta Arias. Parameterizing
In Proc.

Random Test Data According to Equivalence Classes.
ASE, RT ’07, pages 38–41, 2007.

[113] Shin Nakajima and Hai Ngoc Bui. Dataset coverage for testing
machine learning computer programs. In 2016 23rd Asia-Paciﬁc
Software Engineering Conference (APSEC), pages 297–304. IEEE,
2016.

[114] Tsong Y Chen, Shing C Cheung, and Shiu Ming Yiu. Meta-
morphic testing: a new approach for generating next test cases.
Technical report, Technical Report HKUST-CS98-01, Department
of Computer Science, Hong Kong University of Science and
Technology, Hong Kong, 1998.

[115] Christian Murphy, Gail E. Kaiser, Lifeng Hu, and Leon Wu. Prop-
erties of machine learning applications for use in metamorphic
testing. In SEKE, 2008.

[116] Junhua Ding, Xiaojun Kang, and Xin-Hua Hu. Validating a deep
In Proceedings of
learning framework by metamorphic testing.
the 2Nd International Workshop on Metamorphic Testing, MET ’17,
pages 28–34, 2017.

[117] Christian Murphy, Kuang Shen, and Gail Kaiser. Using JML
Runtime Assertion Checking to Automate Metamorphic Testing
in Applications Without Test Oracles. In Proc. ICST, pages 436–
445. IEEE Computer Society, 2009.

[118] Xiaoyuan Xie, Joshua Ho, Christian Murphy, Gail Kaiser, Baowen
Xu, and Tsong Yueh Chen. Application of metamorphic testing
to supervised classiﬁers. In Quality Software, 2009. QSIC’09. 9th
International Conference on, pages 135–144. IEEE, 2009.

33

[119] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer,
Peter Reutemann, and Ian H Witten. The weka data mining soft-
ware: an update. ACM SIGKDD explorations newsletter, 11(1):10–
18, 2009.

[120] Shin Nakajima. Generalized oracle for testing machine learning
computer programs. In International Conference on Software Engin-
eering and Formal Methods, pages 174–179. Springer, 2017.
[121] Anurag Dwarakanath, Manish Ahuja, Samarth Sikand, Rag-
hotham M. Rao, R. P. Jagadeesh Chandra Bose, Neville Dubash,
and Sanjay Podder. Identifying implementation bugs in machine
learning based image classiﬁers using metamorphic testing. In
Proceedings of the 27th ACM SIGSOFT International Symposium on
Software Testing and Analysis, pages 118–128, 2018.

[122] Arnab Sharma and Heike Wehrheim. Testing machine learning
algorithms for balanced data usage. In Proc. ICST, pages 125–135,
2019.

[123] Sadam Al-Azani and Jameleddine Hassine. Validation of ma-
chine learning classiﬁers using metamorphic testing and fea-
In International Workshop on Multi-
ture selection techniques.
disciplinary Trends in Artiﬁcial Intelligence, pages 77–91. Springer,
2017.

[124] Manikandasriram Srinivasan Ramanagopal, Cyrus Anderson,
Ram Vasudevan, and Matthew Johnson-Roberson. Failing to
learn: Autonomously identifying perception failures for self-
driving cars. IEEE Robotics and Automation Letters, 3(4):3860–3867,
2018.

[125] Xiaoyuan Xie, Zhiyi Zhang, Tsong Yueh Chen, Yang Liu, Pak-Lok
Poon, and Baowen Xu. Mettle: A metamorphic testing approach
to validating unsupervised machine learning methods, 2018.
[126] Shin Nakajima. Dataset diversity for metamorphic testing of
In International Workshop on Struc-
machine learning software.
tured Object-Oriented Formal Language and Method, pages 21–38.
Springer, 2018.

[127] Jinhan Kim, Robert Feldt, and Shin Yoo. Guiding deep learn-
arXiv preprint

ing system testing using surprise adequacy.
arXiv:1808.08444, 2018.

[128] Christian Murphy, Kuang Shen, and Gail Kaiser. Automatic
In 18th In-
System Testing of Programs Without Test Oracles.
ternational Symposium on Software Testing and Analysis, ISSTA ’09,
pages 189–200, 2009.

[129] L. Sun and Z. Q. Zhou. Metamorphic Testing for Machine Trans-
In 2018 25th Australasian Software Engineering

lations: MT4MT.
Conference (ASWEC), pages 96–100, Nov 2018.

[130] Daniel Pesu, Zhi Quan Zhou, Jingfeng Zhen, and Dave Towey.
A monte carlo method for metamorphic testing of machine
translation services. In Proceedings of the 3rd International Workshop
on Metamorphic Testing, pages 38–45. ACM, 2018.

[131] William M McKeeman. Differential testing for software. Digital

Technical Journal, 10(1):100–107, 1998.

[132] Vu Le, Mehrdad Afshari, and Zhendong Su. Compiler validation
In ACM SIGPLAN Notices,

via equivalence modulo inputs.
volume 49, pages 216–226. ACM, 2014.

[133] Mahdi Nejadgholi and Jinqiu Yang. A study of oracle approx-
imations in testing deep learning libraries. In Proc. ASE, pages
785–796, 2019.

[134] Algirdas Avizienis. The methodology of n-version programming.

Software fault tolerance, 3:23–46, 1995.

[135] Siwakorn Srisakaokul, Zhengkai Wu, Angello Astorga, Oreoluwa
Alebiosu, and Tao Xie. Multiple-implementation testing of super-
vised learning software. In Proc. AAAI-18 Workshop on Engineering
Dependable and Secure Machine Learning Systems (EDSMLS), 2018.
[136] Yi Qin, Huiyan Wang, Chang Xu, Xiaoxing Ma, and Jian Lu.
Syneva: Evaluating ml programs by mirror program synthesis.
In 2018 IEEE International Conference on Software Quality, Reliability
and Security (QRS), pages 171–182. IEEE, 2018.

[137] Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robust-
ness of neural networks with mixed integer programming. arXiv
preprint arXiv:1711.07356, 2017.

[138] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold,
and Richard Zemel. Fairness through awareness. In Proceedings of
the 3rd innovations in theoretical computer science conference, pages
214–226. ACM, 2012.

[139] Moritz Hardt, Eric Price, Nati Srebro, et al. Equality of oppor-
tunity in supervised learning. In Advances in neural information
processing systems, pages 3315–3323, 2016.

[140] Indre Zliobaite. Fairness-aware machine learning: a perspective.

arXiv preprint arXiv:1708.00754, 2017.

[141] Bernease Herman. The promise and peril of human evaluation

for model interpretability. arXiv preprint arXiv:1711.07414, 2017.

[142] Daniel Kang, Deepti Raghavan, Peter Bailis, and Matei Zaharia.
In NeurIPS

Model assertions for debugging machine learning.
MLSys Workshop, 2018.

[143] Lianghao Li and Qiang Yang. Lifelong machine learning test.
In Proceedings of the Workshop on â ˘AIJBeyond the Turing Testâ ˘A˙I of
AAAI Conference on Artiﬁcial Intelligence, 2015.

[144] Jie Zhang, Ziyi Wang, Lingming Zhang, Dan Hao, Lei Zang,
Shiyang Cheng, and Lu Zhang. Predictive mutation testing. In
Proceedings of the 25th International Symposium on Software Testing
and Analaysis, ISSTA 2016, pages 342–353, 2016.

[145] Youcheng Sun, Xiaowei Huang, and Daniel Kroening. Testing
deep neural networks. arXiv preprint arXiv:1803.04792, 2018.
[146] Arnaud Dupuy and Nancy Leveson. An empirical evaluation of
the mc/dc coverage criterion on the hete-2 satellite software. In
Digital Avionics Systems Conference, 2000. Proceedings. DASC. The
19th, volume 1, pages 1B6–1. IEEE, 2000.

[147] Jasmine Sekhon and Cody Fleming. Towards improved testing

for deep learning. In Proc. ICSE(NIER track), pages 85–88, 2019.

[148] Lei Ma, Fuyuan Zhang, Minhui Xue, Bo Li, Yang Liu, Jianjun
Zhao, and Yadong Wang. Combinatorial testing for deep learning
systems. arXiv preprint arXiv:1806.07723, 2018.

[149] Lei Ma, Felix Juefei-Xu, Minhui Xue, Bo Li, Li Li, Yang Liu, and
Jianjun Zhao. Deepct: Tomographic combinatorial testing for
deep learning systems. In Proc. SANER, pages 614–618, 02 2019.
[150] Zenan Li, Xiaoxing Ma, Chang Xu, and Chun Cao. Structural
In

coverage criteria for neural networks could be misleading.
Proc. ICSE(NIER track), pages 89–92, 2019.

[151] Yue Jia and Mark Harman. An analysis and survey of the
IEEE transactions on software

development of mutation testing.
engineering, 37(5):649–678, 2011.

[152] Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix
Juefei-Xu, Chao Xie, Li Li, Yang Liu, Jianjun Zhao, and Yadong
Wang. DeepMutation: Mutation Testing of Deep Learning Sys-
tems, 2018.

[153] Weijun Shen, Jun Wan, and Zhenyu Chen. MuNN: Mutation
Analysis of Neural Networks. In 2018 IEEE International Confer-
ence on Software Quality, Reliability and Security Companion (QRS-
C), pages 108–115. IEEE, 2018.

[154] Eric Breck, Shanqing Cai, Eric Nielsen, Michael Salib, and D Scul-
ley. The ml test score: A rubric for ml production readiness
and technical debt reduction. In Big Data (Big Data), 2017 IEEE
International Conference on, pages 1123–1132. IEEE, 2017.

[155] Taejoon Byun, Vaibhav Sharma, Abhishek Vijayakumar, Sanjai
Rayadurgam, and Darren Cofer. Input prioritization for testing
neural networks. arXiv preprint arXiv:1901.03768, 2019.

[156] Long Zhang, Xuechao Sun, Yong Li, Zhenyu Zhang, and Yang
Feng. A noise-sensitivity-analysis-based test prioritization tech-
nique for deep neural networks. arXiv preprint arXiv:1901.00054,
2019.

[157] Zenan Li, Xiaoxing Ma, Chang Xu, Chun Cao, Jingwei Xu, and
Jian Lu. Boosting Operational DNN Testing Efﬁciency through
Conditioning. In Proc. FSE, page to appear, 2019.

[158] Wei Ma, Mike Papadakis, Anestis Tsakmalis, Maxime Cordy, and

Yves Le Traon. Test selection for deep learning systems, 2019.

[159] Ferdian Thung, Shaowei Wang, David Lo, and Lingxiao Jiang.
An empirical study of bugs in machine learning systems.
In
2012 IEEE 23rd International Symposium on Software Reliability
Engineering (ISSRE), pages 271–280. IEEE, 2012.

[160] Yuhao Zhang, Yifan Chen, Shing-Chi Cheung, Yingfei Xiong, and
Lu Zhang. An empirical study on tensorﬂow program bugs. In
Proceedings of the 27th ACM SIGSOFT International Symposium on
Software Testing and Analysis, pages 129–140, 2018.

[161] S. S. Banerjee, S. Jha, J. Cyriac, Z. T. Kalbarczyk, and R. K.
Iyer. Hands off the wheel in autonomous vehicles?: A systems
In 2018 48th
perspective on over a million miles of ﬁeld data.
Annual IEEE/IFIP International Conference on Dependable Systems
and Networks (DSN), pages 586–597, June 2018.

[162] Shiqing Ma, Yingqi Liu, Wen-Chuan Lee, Xiangyu Zhang, and
Ananth Grama. MODE: Automated Neural Network Model
Debugging via State Differential Analysis and Input Selection.
pages 175–186, 2018.

[163] Saikat Dutta, Wenxian Zhang, Zixin Huang, and Sasa Misailovic.
Storm: Program reduction for testing and debugging probabil-
istic programming systems. 2019.

34

[164] Shanqing Cai, Eric Breck, Eric Nielsen, Michael Salib, and D Scul-
ley. Tensorﬂow debugger: Debugging dataﬂow graphs for ma-
chine learning. In Proceedings of the Reliable Machine Learning in
the Wild-NIPS 2016 Workshop, 2016.

[165] Manasi Vartak, Joana M F da Trindade, Samuel Madden, and
Matei Zaharia. Mistique: A system to store and query model
In Proceedings of the 2018
intermediates for model diagnosis.
International Conference on Management of Data, pages 1285–1300.
ACM, 2018.

[166] Sanjay Krishnan and Eugene Wu.

Palm: Machine learning
In Proceedings of the 2nd
explanations for iterative debugging.
Workshop on Human-In-the-Loop Data Analytics, page 4. ACM,
2017.

[167] Besmira Nushi, Ece Kamar, Eric Horvitz, and Donald Kossmann.
On human intellect and machine failures: Troubleshooting in-
tegrative machine learning systems. In AAAI, pages 1017–1025,
2017.

[168] Aws Albarghouthi, Loris Dâ ˘A ´ZAntoni, and Samuel Drews. Re-
In Inter-
pairing decision-making programs under uncertainty.
national Conference on Computer Aided Veriﬁcation, pages 181–200.
Springer, 2017.

[169] Wei Yang and Tao Xie. Telemade: A testing framework for
learning-based malware detection systems. In AAAI Workshops,
2018.

[170] Tommaso Dreossi, Shromona Ghosh, Alberto Sangiovanni-
Vincentelli, and Sanjit A Seshia. Systematic testing of convo-
lutional neural networks for autonomous driving. arXiv preprint
arXiv:1708.03309, 2017.

[171] Florian Tramer, Vaggelis Atlidakis, Roxana Geambasu, Daniel
Hsu, Jean-Pierre Hubaux, Mathias Humbert, Ari Juels, and
Huang Lin. Fairtest: Discovering unwarranted associations in
In 2017 IEEE European Symposium on
data-driven applications.
Security and Privacy (EuroS&P), pages 401–416. IEEE, 2017.
[172] Yasuharu Nishi, Satoshi Masuda, Hideto Ogawa, and Keiji Uet-
suki. A test architecture for machine learning product. In 2018
IEEE International Conference on Software Testing, Veriﬁcation and
Validation Workshops (ICSTW), pages 273–278. IEEE, 2018.
[173] Philip S Thomas, Bruno Castro da Silva, Andrew G Barto,
Stephen Giguere, Yuriy Brun, and Emma Brunskill.
Pre-
venting undesirable behavior of intelligent machines. Science,
366(6468):999–1004, 2019.

[174] Ron Kohavi et al. A study of cross-validation and bootstrap for
In Ijcai, volume 14,

accuracy estimation and model selection.
pages 1137–1145. Montreal, Canada, 1995.

[175] Bradley Efron and Robert J Tibshirani. An introduction to the

bootstrap. CRC press, 1994.

[176] Nathalie Japkowicz. Why question machine learning evaluation
In AAAI Workshop on Evaluation Methods for Machine

methods.
Learning, pages 6–11, 2006.

[177] Weijie Chen, Brandon D Gallas, and Waleed A Yousef. Classiﬁer
variability: accounting for training and testing. Pattern Recogni-
tion, 45(7):2661–2671, 2012.

[178] Weijie Chen, Frank W Samuelson, Brandon D Gallas, Le Kang,
Berkman Sahiner, and Nicholas Petrick. On the assessment of the
added value of new predictive biomarkers. BMC medical research
methodology, 13(1):98, 2013.

[179] Nick Hynes, D Sculley, and Michael Terry. The data linter:

Lightweight, automated sanity checking for ml data sets. 2017.

[180] Sanjay Krishnan, Michael J Franklin, Ken Goldberg, and Eugene
Wu. Boostclean: Automated error detection and repair for ma-
chine learning. arXiv preprint arXiv:1711.01299, 2017.

[181] Sebastian Schelter, Dustin Lange, Philipp Schmidt, Meltem Ce-
likel, Felix Biessmann, and Andreas Grafberger. Automating
large-scale data quality veriﬁcation. Proceedings of the VLDB
Endowment, 11(12):1781–1794, 2018.

[182] Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel,
Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vi-
han Jain, Levent Koc, et al. Tfx: A tensorﬂow-based production-
scale machine learning platform. In Proceedings of the 23rd ACM
SIGKDD International Conference on Knowledge Discovery and Data
Mining, pages 1387–1395. ACM, 2017.

[183] Douglas M Hawkins. The problem of overﬁtting.

Journal of

chemical information and computer sciences, 44(1):1–12, 2004.
[184] Heang-Ping Chan, Berkman Sahiner, Robert F Wagner, and Nich-
olas Petrick. Classiﬁer design for computer-aided diagnosis:
Effects of ﬁnite sample size on the mean performance of classical

and neural network classiﬁers. Medical physics, 26(12):2654–2668,
1999.

[185] Berkman Sahiner, Heang-Ping Chan, Nicholas Petrick, Robert F
Wagner, and Lubomir Hadjiiski. Feature selection and classiﬁer
performance in computer-aided diagnosis: The effect of ﬁnite
sample size. Medical physics, 27(7):1509–1522, 2000.

[186] Keinosuke Fukunaga and Raymond R. Hayes. Effects of sample
size in classiﬁer design. IEEE Transactions on Pattern Analysis &
Machine Intelligence, (8):873–885, 1989.

[187] Alexej Gossmann, Aria Pezeshk, and Berkman Sahiner. Test data
reuse for evaluation of adaptive machine learning algorithms:
over-ﬁtting to a ﬁxed’test’dataset and a potential solution.
In
Medical Imaging 2018: Image Perception, Observer Performance, and
Technology Assessment, volume 10577, page 105770K. International
Society for Optics and Photonics, 2018.

[188] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal
Frossard. Deepfool: a simple and accurate method to fool deep
neural networks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2574–2582, 2016.

[189] Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios
Vytiniotis, Aditya Nori, and Antonio Criminisi. Measuring
In Advances in neural
neural net robustness with constraints.
information processing systems, pages 2613–2621, 2016.

[190] Nicholas Carlini and David Wagner. Towards evaluating the
In 2017 IEEE Symposium on

robustness of neural networks.
Security and Privacy (SP), pages 39–57. IEEE, 2017.

[191] Wenjie Ruan, Min Wu, Youcheng Sun, Xiaowei Huang, Daniel
Kroening, and Marta Kwiatkowska. Global robustness evalu-
ation of deep neural networks with provable guarantees for l0
norm. arXiv preprint arXiv:1804.05805, 2018.

[192] Divya Gopinath, Guy Katz, Corina S P˘as˘areanu, and Clark Bar-
rett. Deepsafe: A data-driven approach for assessing robustness
In International Symposium on Automated
of neural networks.
Technology for Veriﬁcation and Analysis, pages 3–19. Springer, 2018.
[193] Ravi Mangal, Aditya Nori, and Alessandro Orso. Robustness of
In

neural networks: A probabilistic and practical perspective.
Proc. ICSE(NIER track), pages 93–96, 2019.

[194] Subho S Banerjee, James Cyriac, Saurabh Jha, Zbigniew T Kalbar-
czyk, and Ravishankar K Iyer. Towards a bayesian approach for
assessing faulttolerance of deep neural networks. In Proc. DSN
(extended abstract), 2019.

[195] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and
Ananthram Swami. Distillation as a defense to adversarial per-
turbations against deep neural networks. In 2016 IEEE Symposium
on Security and Privacy (SP), pages 582–597. IEEE, 2016.

[196] Nicolas Papernot, Ian Goodfellow, Ryan Sheatsley, Reuben Fein-
man, and Patrick McDaniel. cleverhans v1.0.0: an adversarial
machine learning library. arXiv preprint arXiv:1610.00768, 2016.

[197] Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfel-
low, Reuben Feinman, Alexey Kurakin, Cihang Xie, Yash Sharma,
Tom Brown, Aurko Roy, Alexander Matyasko, Vahid Behzadan,
Karen Hambardzumyan, Zhishuai Zhang, Yi-Lin Juang, Zhi Li,
Ryan Sheatsley, Abhibhav Garg, Jonathan Uesato, Willi Gierke,
Yinpeng Dong, David Berthelot, Paul Hendricks, Jonas Rauber,
and Rujun Long. Technical report on the cleverhans v2.1.0
arXiv preprint arXiv:1610.00768,
adversarial examples library.
2018.

[198] Saurabh Jha, Subho S Banerjee, James Cyriac, Zbigniew T Kal-
barczyk, and Ravishankar K Iyer. AVFI: Fault injection for
autonomous vehicles. In 2018 48th Annual IEEE/IFIP International
Conference on Dependable Systems and Networks Workshops (DSN-
W), pages 55–56. IEEE, 2018.

[199] Saurabh Jha, Timothy Tsai, Siva Hari, Michael Sullivan, Zbig-
niew Kalbarczyk, Stephen W Keckler, and Ravishankar K Iyer.
Kayotee: A fault injection-based system to assess the safety and
In 3rd
reliability of autonomous vehicles to faults and errors.
IEEE International Workshop on Automotive Reliability & Test, 2018.
[200] Helge Spieker and Arnaud Gotlieb. Towards testing of deep
arXiv preprint

learning systems with training set reduction.
arXiv:1901.04169, 2019.

[201] Solon Barocas and Andrew D Selbst. Big data’s disparate impact.

Cal. L. Rev., 104:671, 2016.

[202] Pratik Gajane and Mykola Pechenizkiy. On formalizing fair-
arXiv preprint

ness in prediction with machine learning.
arXiv:1710.03184, 2017.

[203] Sahil Verma and Julia Rubin. Fairness deﬁnitions explained. In

International Workshop on Software Fairness, 2018.

35

[204] Nripsuta Saxena, Karen Huang, Evan DeFilippis, Goran Radan-
ovic, David Parkes, and Yang Liu. How do fairness deﬁnitions
fare? examining public attitudes towards algorithmic deﬁnitions
of fairness. arXiv preprint arXiv:1811.03654, 2018.

[205] Anthony Finkelstein, Mark Harman, Afshin Mansouri, Jian Ren,
and Yuanyuan Zhang. Fairness analysis in requirements assign-
ments. In 16th IEEE International Requirements Engineering Con-
ference, pages 115–124, Los Alamitos, California, USA, September
2008.

[206] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva.
In Advances in Neural Information Pro-

Counterfactual fairness.
cessing Systems, pages 4066–4076, 2017.

[207] Nina Grgic-Hlaca, Muhammad Bilal Zafar, Krishna P Gummadi,
and Adrian Weller. The case for process fairness in learning:
Feature selection for fair decision making. In NIPS Symposium on
Machine Learning and the Law, Barcelona, Spain, volume 8, 2016.

[208] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez,
and Krishna P Gummadi. Fairness constraints: Mechanisms for
fair classiﬁcation. arXiv preprint arXiv:1507.05259, 2015.

[209] Blossom Metevier, Stephen Giguere, Sarah Brockman, Ari
Kobren, Yuriy Brun, Emma Brunskill, and Philip Thomas. Ofﬂine
Contextual Bandits with High Probability Fairness Guarantees.
In Proceedings of the 33rd Annual Conference on Neural Information
Processing Systems (NeurIPS), December 2019.

[210] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A
contextual-bandit approach to personalized news article recom-
In Proceedings of the 19th international conference on
mendation.
World wide web, pages 661–670. ACM, 2010.

[211] Pascal Massart. Concentration inequalities and model selection.

2007.

[212] Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Lang-
ford, and Hanna Wallach. A reductions approach to fair classiﬁc-
ation. arXiv preprint arXiv:1803.02453, 2018.

[213] Rico Angell, Brittany Johnson, Yuriy Brun, and Alexandra
Meliou. Themis: Automatically testing software for discrimina-
tion. In Proceedings of the 2018 26th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations
of Software Engineering, pages 871–875. ACM, 2018.

[214] Brittany Johnson, Yuriy Brun, and Alexandra Meliou. Causal
testing: Finding defects’ root causes. CoRR, abs/1809.06991, 2018.
[215] Sorelle A. Friedler, Chitradeep Dutta Roy, Carlos Scheidegger,
and Dylan Slack. Assessing the Local Interpretability of Machine
Learning Models. CoRR, abs/1902.03501, 2019.

[216] Zhi Quan Zhou, Liqun Sun, Tsong Yueh Chen, and Dave Towey.
Metamorphic relations for enhancing system understanding and
use. IEEE Transactions on Software Engineering, 2018.

[217] Weijie Chen, Berkman Sahiner, Frank Samuelson, Aria Pezeshk,
and Nicholas Petrick. Calibration of medical diagnostic classiﬁer
scores to the probability of disease. Statistical methods in medical
research, 27(5):1394–1409, 2018.

[218] Zeyu Ding, Yuxin Wang, Guanhong Wang, Danfeng Zhang, and
Daniel Kifer. Detecting violations of differential privacy.
In
Proceedings of the 2018 ACM SIGSAC Conference on Computer and
Communications Security, pages 475–489. ACM, 2018.

[219] Benjamin Bichsel, Timon Gehr, Dana Drachsler-Cohen, Petar
Tsankov, and Martin Vechev. Dp-ﬁnder: Finding differential pri-
vacy violations by sampling and optimization. In Proceedings of
the 2018 ACM SIGSAC Conference on Computer and Communications
Security, pages 508–524. ACM, 2018.

[220] Neoklis Polyzotis, Sudip Roy, Steven Euijong Whang, and Martin
Zinkevich. Data management challenges in production machine
learning. In Proceedings of the 2017 ACM International Conference
on Management of Data, pages 1723–1726. ACM, 2017.

[221] Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian
Bischoff. On detecting adversarial perturbations. arXiv preprint
arXiv:1702.04267, 2017.

[222] Jingyi Wang, Guoliang Dong, Jun Sun, Xinyu Wang, and Peixin
Zhang. Adversarial sample detection for deep neural network
through model mutation testing. In Proc. ICSE, pages 1245–1256,
2019.

[223] Jingyi Wang, Jun Sun, Peixin Zhang, and Xinyu Wang. Detecting
adversarial samples for deep neural networks through mutation
testing. CoRR, abs/1805.05010, 2018.

[224] Nicholas Carlini and David Wagner. Adversarial examples are
not easily detected: Bypassing ten detection methods. In Proceed-
ings of the 10th ACM Workshop on Artiﬁcial Intelligence and Security,
AISec ’17, pages 3–14. ACM, 2017.

[225] Sanjay Krishnan, Jiannan Wang, Eugene Wu, Michael J Franklin,
and Ken Goldberg. Activeclean: interactive data cleaning for stat-
istical modeling. Proceedings of the VLDB Endowment, 9(12):948–
959, 2016.

[226] Sanjay Krishnan, Michael J Franklin, Ken Goldberg, Jiannan
Wang, and Eugene Wu. Activeclean: An interactive data cleaning
In Proceedings of the
framework for modern machine learning.
2016 International Conference on Management of Data, pages 2117–
2120. ACM, 2016.

[227] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest.
In 2008 Eighth IEEE International Conference on Data Mining, pages
413–422. IEEE, 2008.

[228] Sanjay Krishnan and Eugene Wu. Alphaclean: automatic gener-
ation of data cleaning pipelines. arXiv preprint arXiv:1904.11827,
2019.

[229] Erhard Rahm and Hong Hai Do. Data cleaning: Problems and
current approaches. IEEE Data Eng. Bull., 23(4):3–13, 2000.

[230] Nick McClure.

TensorFlow machine learning cookbook.

Packt

Publishing Ltd, 2017.

[231] Tom Schaul, Ioannis Antonoglou, and David Silver. Unit tests for

stochastic optimization. In ICLR 2014, 2014.

[232] X. Sun, T. Zhou, G. Li, J. Hu, H. Yang, and B. Li. An empirical
study on real bugs for machine learning programs. In 2017 24th
Asia-Paciﬁc Software Engineering Conference (APSEC), pages 348–
357, 2017.

[233] An Orchestrated Empirical Study on Deep Learning Frameworks
and Lei Ma Qiang Hu Ruitao Feng Li Li Yang Liu Jianjun Zhao
Xiaohong Li Platforms Qianyu Guo, Xiaofei Xie. An orchestrated
empirical study on deep learning frameworks and platforms.
arXiv preprint arXiv:1811.05187, 2018.

[234] Yu. L. Karpov, L. E. Karpov, and Yu. G. Smetanin. Adaptation
of general concepts of software testing to neural networks. Pro-
gramming and Computer Software, 44(5):324–334, Sep 2018.
[235] Wei Fu and Tim Menzies. Easy over hard: A case study on deep
learning. In Proc. FSE, ESEC/FSE 2017, pages 49–60. ACM, 2017.
[236] Zhongxin Liu, Xin Xia, Ahmed E. Hassan, David Lo, Zhenchang
Xing, and Xinyu Wang. Neural-machine-translation-based com-
mit message generation: How far are we? In Proceedings of the
33rd ACM/IEEE International Conference on Automated Software
Engineering, ASE 2018, pages 373–384. ACM, 2018.

[237] Julian Dolby, Avraham Shinnar, Allison Allain, and Jenna Reinen.
Ariadne: Analysis for machine learning programs. In Proceedings
of the 2Nd ACM SIGPLAN International Workshop on Machine
Learning and Programming Languages, MAPL 2018, pages 1–10,
2018.

[238] Qixue Xiao, Kang Li, Deyue Zhang, and Weilin Xu. Security
In 2018 IEEE Security

risks in deep learning implementations.
and Privacy Workshops (SPW), pages 123–128. IEEE, 2018.
[239] Chase Roberts. How to unit test machine learning code, 2017.
[240] Dawei Cheng, Chun Cao, Chang Xu, and Xiaoxing Ma. Manifest-
ing bugs in machine learning code: An explorative study with
mutation testing. In 2018 IEEE International Conference on Software
Quality, Reliability and Security (QRS), pages 313–324. IEEE, 2018.
[241] Xiaoyuan Xie, Joshua W. K. Ho, Christian Murphy, Gail Kaiser,
Baowen Xu, and Tsong Yueh Chen. Testing and validating
J. Syst.
machine learning classiﬁers by metamorphic testing.
Softw., 84(4):544–558, April 2011.

[242] Yu-Seung Ma, Jeff Offutt, and Yong Rae Kwon. Mujava: an
automated class mutation system. Software Testing, Veriﬁcation
and Reliability, 15(2):97–133, 2005.

[243] Joachim Wegener and Oliver Bühler. Evaluation of different
ﬁtness functions for the evolutionary testing of an autonomous
parking system. In Genetic and Evolutionary Computation Confer-
ence, pages 1400–1412. Springer, 2004.

[244] Matthias Woehrle, Christoph Gladisch, and Christian Heinze-
mann. Open questions in testing of learned computer vision
In International Conference on
functions for automated driving.
Computer Safety, Reliability, and Security, pages 333–345. Springer,
2019.

[245] Raja Ben Abdessalem, Shiva Nejati, Lionel C Briand, and Thomas
Stifter. Testing vision-based control systems using learnable
evolutionary algorithms. In Proc. ICSE, pages 1016–1026. IEEE,
2018.

36

IEEE/ACM International Conference on Automated Software Engin-
eering, pages 63–74. ACM, 2016.

[247] Raja Ben Abdessalem, Annibale Panichella, Shiva Nejati, Lionel C
Briand, and Thomas Stifter. Testing autonomous cars for feature
interaction failures using many-objective search. In Proceedings of
the 33rd ACM/IEEE International Conference on Automated Software
Engineering, pages 143–154. ACM, 2018.

[248] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
Bleu: a method for automatic evaluation of machine translation.
In Proceedings of the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for Computational
Linguistics, 2002.

[249] Wujie Zheng, Wenyu Wang, Dian Liu, Changrong Zhang, Qin-
song Zeng, Yuetang Deng, Wei Yang, Pinjia He, and Tao Xie.
Testing untestable neural machine translation: An industrial case.
2018.

[250] Wujie Zheng, Wenyu Wang, Dian Liu, Changrong Zhang, Qin-
song Zeng, Yuetang Deng, Wei Yang, Pinjia He, and Tao Xie.
Testing untestable neural machine translation: An industrial case.
In ICSE (poster track), 2019.

[251] Wenyu Wang, Wujie Zheng, Dian Liu, Changrong Zhang, Qin-
song Zeng, Yuetang Deng, Wei Yang, Pinjia He, and Tao Xie.
Detecting failures of neural machine translation in the absence of
reference translations. In Proc. DSN (industry track), 2019.
[252] Jenna Burrell. How the machine â ˘AŸthinksâ ˘A ´Z: Understanding
opacity in machine learning algorithms. Big Data & Society,
3(1):2053951715622512, 2016.

[253] Yann LeCun and Corinna Cortes. MNIST handwritten digit

database. 2010.

[254] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist:
a novel image dataset for benchmarking machine learning al-
gorithms. CoRR, abs/1708.07747, 2017.

[255] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10

(canadian institute for advanced research).

[256] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Im-
ageNet: A Large-Scale Hierarchical Image Database. In CVPR09,
2009.
[257] R.A. Fisher.
datasets/iris.

http://archive.ics.uci.edu/ml/

Iris Data Set .

[258] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco,
Bo Wu, and Andrew Y Ng. Reading digits in natural images
with unsupervised feature learning. 2011.

[259] Horea MureÈ ´Zan and Mihai Oltean. Fruit recognition from im-
ages using deep learning. Acta Universitatis Sapientiae, Informatica,
10:26–42, 06 2018.

[260] Olga Belitskaya. Handwritten Letters. https://www.kaggle.

com/olgabelitskaya/handwritten-letters.

[261] Balance Scale Data Set . http://archive.ics.uci.edu/ml/datasets/

balance+scale.
[262] Sharaf Malebary.

Set .
Communications.

DSRC Vehicle Communications Data
http://archive.ics.uci.edu/ml/datasets/DSRC+Vehicle+

[263] Nexar. the nexar dataset. https://www.getnexar.com/challenge-

1/.

[264] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro
Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick.
Microsoft coco: Common objects in context.
In David Fleet,
Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Com-
puter Vision – ECCV 2014, pages 740–755, Cham, 2014. Springer
International Publishing.

[265] Xinlei Pan, Yurong You, Ziyan Wang, and Cewu Lu. Virtual
arXiv
to real reinforcement learning for autonomous driving.
preprint arXiv:1704.03952, 2017.

[266] A Geiger, P Lenz, C Stiller, and R Urtasun. Vision meets robotics:
The kitti dataset. The International Journal of Robotics Research,
32(11):1231–1237, 2013.

[267] Facebook research.
downloads/babi/.

the babi dataset. https://research.fb.com/

[268] Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualiz-
arXiv preprint

ing and understanding recurrent networks.
arXiv:1506.02078, 2015.

[269] Stack Exchange Data Dump .

https://archive.org/details/

stackexchange.

[246] Raja Ben Abdessalem, Shiva Nejati, Lionel C Briand, and Thomas
Stifter. Testing advanced driver assistance systems using multi-
objective search and neural networks. In Proceedings of the 31st

[270] Samuel R Bowman, Gabor Angeli, Christopher Potts, and Chris-
topher D Manning. A large annotated corpus for learning natural
language inference. arXiv preprint arXiv:1508.05326, 2015.

37

[271] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-
coverage challenge corpus for sentence understanding through
In Proceedings of the 2018 Conference of the North
inference.
American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Papers), pages 1112–
1122. Association for Computational Linguistics, 2018.

[272] Califormia dmv failure reports.

https://www.dmv.ca.gov/

portal/dmv/detail/vr/autonomous/autonomousveh_ol316.

[273] Dr. Hans Hofmann. Statlog (german credit data) data set. http://
archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data).
[274] Ron Kohav. Adult data set. http://archive.ics.uci.edu/ml/

datasets/adult.

[275] Sérgio Moro, Paulo Cortez, and Paulo Rita. A data-driven
approach to predict the success of bank telemarketing. Decision
Support Systems, 62:22–31, 2014.

[276] Executions in the united states. https://deathpenaltyinfo.org/

views-executions.

[277] Andrea Dal Pozzolo, Olivier Caelen, Reid A Johnson, and Gian-
luca Bontempi. Calibrating probability with undersampling for
unbalanced classiﬁcation. In Computational Intelligence, 2015 IEEE
Symposium Series on, pages 159–166. IEEE, 2015.

[278] Peter J Bickel, Eugene A Hammel, and J William O’Connell.
Sex bias in graduate admissions: Data from Berkeley. Science,
187(4175):398–404, 1975.

[279] propublica. data for the propublica story ‘machine bias’. https:

//github.com/propublica/compas-analysis/.

[280] F Maxwell Harper and Joseph A Konstan. The movielens data-
sets: History and context. Acm transactions on interactive intelligent
systems (tiis), 5(4):19, 2016.

[281] Zillow. Zillow Prize: Zillowâ ˘A ´Zs Home Value Prediction (Zes-
timate). https://www.kaggle.com/c/zillow-prize-1/overview.

[282] US Federal Reserve. Report to the congress on credit scoring and
its effects on the availability and affordability of credit. Board of
Governors of the Federal Reserve System, 2007.

[283] Law School Admission Council.

longitudinal
bar passage study (nlbps). http://academic.udayton.edu/race/
03justice/legaled/Legaled04.htm.

Lsac national

[284] VirusTotal. Virustotal. https://www.virustotal.com/#/home/

search.

[285] contagio malware dump. http://contagiodump.blogspot.com/

2013/03/16800-clean-and-11960-malicious-ﬁles.html.

[286] Daniel Arp, Michael Spreitzenbarth, Malte Hubner, Hugo Gas-
con, Konrad Rieck, and CERT Siemens. Drebin: Effective and
In
explainable detection of android malware in your pocket.
Ndss, volume 14, pages 23–26, 2014.

[287] Alen Shapiro. UCI chess (king-rook vs. king-pawn) data
https://archive.ics.uci.edu/ml/datasets/Chess+%28King-

set.
Rook+vs.+King-Pawn%29, 1989.

[288] Mark Harman and Peter O’Hearn. From start-ups to scale-ups:
opportunities and open problems for static and dynamic pro-
gram analysis. In 2018 IEEE 18th International Working Conference
on Source Code Analysis and Manipulation (SCAM), pages 1–23.
IEEE, 2018.

[289] Anthony Finkelstein, Mark Harman, Afshin Mansouri, Jian Ren,
and Yuanyuan Zhang. A search based approach to fairness ana-
lysis in requirements assignments to aid negotiation, mediation
and decision making. Requirements Engineering, 14(4):231–245,
2009.

[290] Jie Zhang, Lingming Zhang, Dan Hao, Meng Wang, and
Lu Zhang. Do pseudo test suites lead to inﬂated correlation in
measuring test effectiveness? In Proc. ICST, pages 252–263, 2019.
[291] Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A
survey of transfer learning. Journal of Big data, 3(1):9, 2016.
[292] Shin NAKAJIMA. Quality assurance of machine learning soft-
ware. In 2018 IEEE 7th Global Conference on Consumer Electronics
(GCCE), pages 601–604. IEEE, 2018.

