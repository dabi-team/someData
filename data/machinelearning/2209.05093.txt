Bilevel Optimization for Feature Selection in the
Data-Driven Newsvendor Problem

Breno Serrano∗
School of Management, Technical University of Munich, Germany, breno.serrano@tum.de

Stefan Minner
School of Management and Munich Data Science Institute, Technical University of Munich, Germany,

stefan.minner@tum.de

2
2
0
2

p
e
S
2
1

]

G
L
.
s
c
[

1
v
3
9
0
5
0
.
9
0
2
2
:
v
i
X
r
a

Maximilian Schiﬀer
School of Management and Munich Data Science Institute, Technical University of Munich, Germany, schiﬀer@tum.de

Thibaut Vidal
Department of Mathematical and Industrial Engineering, Polytechnique Montr´eal, Canada,

Department of Computer Science, Pontiﬁcal Catholic University of Rio de Janeiro, Brazil, thibaut.vidal@polymtl.ca

Abstract. We study the feature-based newsvendor problem, in which a decision-maker has access

to historical data consisting of demand observations and exogenous features.

In this setting, we

investigate feature selection, aiming to derive sparse, explainable models with improved out-of-sample

performance. Up to now, state-of-the-art methods utilize regularization, which penalizes the number

of selected features or the norm of the solution vector. As an alternative, we introduce a novel bilevel

programming formulation. The upper-level problem selects a subset of features that minimizes an

estimate of the out-of-sample cost of ordering decisions based on a held-out validation set. The lower-

level problem learns the optimal coeﬃcients of the decision function on a training set, using only

the features selected by the upper-level. We present a mixed integer linear program reformulation

for the bilevel program, which can be solved to optimality with standard optimization solvers. Our

computational experiments show that the method accurately recovers ground-truth features already

for instances with a sample size of a few hundred observations.

In contrast, regularization-based

techniques often fail at feature recovery or require thousands of observations to obtain similar accuracy.

Regarding out-of-sample generalization, we achieve improved or comparable cost performance.

Keywords. Feature Selection; Bilevel Optimization; Newsvendor; Mixed Integer Programming.

∗ Corresponding author

Declarations of interest: none

1

 
 
 
 
 
 
1. Introduction

The newsvendor problem and its variants have served as fundamental building blocks for models in

inventory and supply chain management. In the classical newsvendor problem, a decision-maker opti-

mizes the inventory of a perishable product that has a stochastic demand with a known distribution.

However, having complete knowledge of the demand distribution is a strong assumption that does not

hold in practice: often, the only information available is a limited set of historical data. Against this

background, data-driven approaches became popular and strive to use past demand data to inform

the newsvendor’s ordering decisions.

In this context, we study the feature-based newsvendor problem (cf. Beutel & Minner 2012, Ban

& Rudin 2019) in which the decision-maker has access not only to historical demand observations

but also to a set of feature variables—often referred to as contextual information or covariates—

that may provide partial information about future realizations of the uncertain demand. Companies

nowadays have large amounts of data that are used to train machine learning models with the aim of

improving operational decisions. In practice, such models often suﬀer from overﬁtting to the training

data, or lack explainability, which is crucial, e.g., when dealing with high-stakes decisions. In this

setting, selecting a subset of the available features can lead to sparser, more explainable models with

improved out-of-sample performance. Against this background, we investigate the challenge of feature

selection (cf. Molina et al. 2002, Kuhn & Johnson 2019): given a data set with a possibly large set

of feature variables, we aim to learn a linear decision function for the feature-based newsvendor that

can generalize to out-of-sample data, utilizing only relevant features.

The goal of this paper is to propose an approach to feature selection based on bilevel optimization.

We search for a subset of features that leads to a minimal out-of-sample cost measured on a held-

out data set, when used for training a linear decision function for the feature-based newsvendor. In

the remainder of this section, we ﬁrst review related literature before we detail our contribution and

describe the organization of this paper.

2

1.1. Related Works

Our work relates to the ﬁelds of data-driven optimization for the newsvendor problem, and more

broadly to prescriptive analytics, machine learning, and bilevel programming. We brieﬂy review the

most related papers in the following.

Newsvendor problem. Research on the newsvendor problem often assumed a decision-maker with

full knowledge about the demand distribution, and considered various settings, e.g., with diﬀerent

objectives or utility functions (Chen et al. 2007, Wang & Webster 2009), pricing policies (Petruzzi

& Dada 1999), and multi-product or multi-period settings (Lau & Lau 1996, Kogan & Lou 2003).

For general surveys on newsvendor models and extensions, we refer the interested reader to Khouja

(1999), Qin et al. (2011) and Choi (2012).

In practice, the decision-maker often has only a ﬁnite

set of demand observations and cannot estimate the true underlying distribution, which motivated

works on the distribution-free newsvendor problem. In this context, the seminal work of Scarf (1958)

derived the optimal order quantity that maximizes proﬁt against the worst-case demand distribution,

assuming that only the mean and variance of demand are known. For a review on the distribution-free

newsvendor and extensions thereof, we refer to Gallego & Moon (1993), Moon & Gallego (1994), and

Yue et al. (2006). Later works on this problem variant assumed additional information about the

demand distribution, such as percentiles (Gallego et al. 2001), symmetry, and unimodality (Perakis &

Roels 2008).

In contrast to working with moments or distributional parameters, data-driven approaches build

directly upon a sample of available data that reﬂects realizations of the underlying uncertainty. In this

context, a common solution approach is sample average approximation (SAA) (cf. Kleywegt et al. 2002,

Shapiro 2003). Levi et al. (2007) applied SAA for the single-period featureless newsvendor problem and

established upper bounds on the number of samples required to achieve a speciﬁed relative error. In

this course, Levi et al. (2015), Cheung & Simchi-Levi (2019), and Besbes & Mouchtaki (2021) further

3

improved upon previous SAA bounds. Besbes & Muharremoglu (2013), Sachs & Minner (2014),

and Ban (2020) studied the impact of demand censoring, i.e., a problem variant in which only sales

observations are available but excess demand is not recorded. They derived upper and lower bounds on

the diﬀerence between the cost achieved by a policy and the optimal cost with knowledge of the demand

distribution. Adopting a robust optimization perspective, Bertsimas & Thiele (2005) proposed a data-

driven approach that can be reformulated as a linear program (LP) and trades oﬀ higher proﬁts for

a decrease in the downside risk. Robust optimization approaches were also investigated by Bertsimas

& Thiele (2006) and See & Sim (2010) for a multi-period inventory problem. Finally, many authors

applied data-driven distributionally robust approaches for dealing with uncertainty in the context of

multi-item newsvendor problems (see, e.g., Ben-Tal et al. 2013, Hanasusanto et al. 2015, Wang et al.

2016, and Bertsimas et al. 2018).

Despite numerous extensions to the newsvendor problem, most data-driven approaches consider

only demand data but no feature variables to be available. However, ignoring the presence of features

can lead to inconsistent decisions as shown in Ban & Rudin (2019). In the following, we review papers

that also consider the presence of features in the context of data-driven optimization.

Data-driven optimization. Beyond the newsvendor problem, some recent works have studied the

integration of estimation and optimization.

In particular, Bertsimas & Kallus (2020) proposed a

framework for feature-based stochastic optimization problems based on a weighted SAA approach, in

which the weights are generated by machine learning methods, such as k-nearest neighbors regression,

local linear regression, classiﬁcation and regression trees, or random forests. Elmachtoub & Grigas

(2021) focused on problems with a linear objective and used features to learn a prediction model

for the stochastic cost vector. They proposed a modiﬁed loss function that directly leverages the

structure of the optimization problem instead of minimizing a standard prediction error, such as the

least squares loss. Despite this modiﬁcation, their approach still handles prediction and optimization

4

as separate tasks and does not integrate them into a one-step process. Mandi et al. (2020) further

adapted the approach from Elmachtoub & Grigas (2021) to solve some hard combinatorial problems,

e.g., by proposing tailored warm-starting techniques.

In the context of the feature-based newsvendor, Beutel & Minner (2012) integrated estimation and

optimization by learning a decision function that predicts ordering decisions directly from features,

opposed to ﬁrst estimating the demand and then optimizing the inventory level. The proposed model

formulation is an LP that solves an Empirical Risk Minimization (ERM) problem over a training data

set. Oroojlooyjadid et al. (2020) and Zhang & Gao (2017) applied neural networks to the newsvendor

problem, proposing speciﬁc loss functions that consider the impact of inventory shortage and holding

costs. Huber et al. (2019) provided an empirical evaluation of diﬀerent data-driven approaches for

the feature-based newsvendor and compared their performance against model-based approaches, which

model the uncertainty through a demand distribution assumption. Their experiments on real-world

data showed that data-driven approaches outperform their model-based counterparts in most cases.

Regarding feature selection, Ban & Rudin (2019) extended the model of Beutel & Minner (2012)

by including a regularization term to the objective function, which penalizes the complexity of the

solution, thereby favoring the selection of fewer features. However, feature selection is not the main

focus of Ban & Rudin (2019), and an open challenge remains regarding the speciﬁcation of the reg-

ularization parameter, for which heuristics are often employed. In this work, we avoid regularization

by formalizing the task of feature selection as a bilevel optimization problem for which we provide a

tractable single-level reformulation.

Bilevel optimization in machine learning. Bilevel optimization has been applied in the ﬁeld of

machine learning for hyperparameter optimization (Bennett et al. 2006, 2008, Franceschi et al. 2018,

Mackay et al. 2019) and feature selection (Agor & ¨Ozaltın 2019). In particular, Bennett et al. (2006,

2008) proposed a bilevel program for optimizing the hyperparameters of a support vector regression

5

model. They reformulated the model into a single-level nonlinear program and employed oﬀ-the-shelf

solvers based on Sequential Quadratic Programming (Fletcher & Leyﬀer 2002). Franceschi et al.

(2018) also proposed a bilevel programming approach for hyperparameter optimization, highlighting

connections to meta-learning, and solved it with a gradient-based method.

Only Agor & ¨Ozaltın (2019) addressed feature selection as a bilevel optimization problem in the

context of classiﬁcation models, e.g., Lasso-based logistic regression and support vector machines.

However, their bilevel formulations do not apply to our problem setting, since the feature-based

newsvendor combines aspects from supervised learning, i.e., regression, and data-driven optimiza-

tion. Moreover, the solution method of Agor & ¨Ozaltın (2019) consists of a tailored genetic algorithm,

which does not provide solution-quality guarantees. In contrast, our methodology is based on mixed

integer linear programming (MILP) and allows to optimally solve the proposed bilevel programming

formulations.

1.2. Contribution

We close the research gaps outlined above by proposing a novel bilevel optimization model that

directly incorporates feature selection into solving the data-driven newsvendor problem. Speciﬁcally,

our contribution is fourfold. First, we introduce a bilevel program designed for feature selection, which

we denote the Bilevel Feature Selection (BFS) model. In contrast to regularization-based methods,

which penalize the norm of the solution vector, BFS captures the more intuitive notion of selecting

a subset of features that minimizes an estimate of the out-of-sample cost, measured on a held-out

validation set. We reformulate the bilevel program into a single-level optimization problem, which

we solve to optimality with oﬀ-the-shelf optimization solvers. Second, we extend the BFS model

to accommodate cross-validation strategies, which further improves its solution quality. Third, to

illustrate the drawback of regularization-based methods for feature selection, we present a bilevel

program, which we refer to as Bilevel Hyperparameter Optimization (BHO), that searches for the

6

optimal hyperparameter for the regularized ERM model (cf. Ban & Rudin 2019). BHO formally

describes the optimization model that established hyperparameter optimization methods implicitly

solve by means of heuristics, such as grid search, random search, or Bayesian optimization. Fourth,

we conduct extensive numerical experiments, using synthetic instances with correlated features. We

compare the proposed BFS models against regularization-based methods in terms of out-of-sample

performance and ground-truth feature recovery. We further compare the methods’ behavior under

demand misspeciﬁcation, assuming a nonlinear demand model. Our results show that the proposed

BFS approach consistently achieves higher accuracy in feature recovery. In most cases, we also observe

an improvement in out-of-sample cost performance, i.e., a decrease in test cost.

1.3. Organization

The remainder of this paper is structured as follows. In Section 2, we review the model formulations

for the classical newsvendor and the feature-based newsvendor problem. Section 3 presents the BHO

and the BFS models, and consecutively extends the BFS to cross-validation. Section 4 describes our

experimental design, and Section 5 presents the results comparing the proposed method against state-

of-the-art techniques based on regularization. Section 6 concludes this paper and gives an outlook on

future research.

2. Fundamentals

In the classical newsvendor problem, a risk-neutral decision-maker sets the order quantity of a

product before observing its uncertain demand. Here, the objective is to minimize the expected cost:

E [C(q; d)] ,

min
q≥0

where q is the order quantity, d

∼ D

is the random variable representing the uncertain demand,

C(q; d) := b(d

q)+ + h(q

d)+

−

−

7

(1)

(2)

is the cost of ordering q units and observing demand d, based on the per unit shortage cost b for lost

proﬁts and unit holding cost h, corresponding to the procurement cost of unsold products discounted

by their unit salvage value. If the demand distribution is known, then the optimal decision q∗ is given

at the b/(b + h) quantile of its cumulative distribution function.

In practice, the demand distribution is often not known. We consider the feature-based newsvendor

problem, in which the decision-maker has access to historical demand data and contextual information

given by a set of feature variables x

∈

Rm+1 (cf. Beutel & Minner 2012). Here, the uncertain demand d

and feature variables x follow an (unknown) joint probability distribution (x, d)

∼ X × D

. The

=

Z

decision-maker’s objective is to minimize the expected cost conditioned on the observed features:

min
q≥0

E [C(q(x); d(x))
|

x] .

(3)

One approach to solve the feature-based newsvendor is to separate the estimation and optimization

problems, i.e., one ﬁrst estimates the conditional demand distribution from historical data and then

optimizes the order quantity based on new feature observations. One drawback of this approach

is that the ﬁrst step’s estimation problem does not account for the asymmetry in the newsvendor

cost function, related to under- and over-predicting demand. To address this issue, Beutel & Minner

(2012) proposed to integrate estimation and optimization into a one-step process, by introducing a

linear decision function that maps feature observations directly to ordering decisions. To learn the

optimal coeﬃcients of the decision function, one minimizes the empirical cost over a data set with

demand and feature observations.

Let

(xi, di)

}i∈S be a data set indexed by S =

{

{

1, . . . , n

, where xi is an (m + 1)-dimensional
}

feature vector and di is a scalar demand observation. Let J =

0, . . . , m
{

}

denote the set of feature

indices. We assume that x0

i = 1 represents the feature-independent intercept term, for all i

this setting, Beutel & Minner (2012) consider a linear decision function of the form:

βjxj = β(cid:62)x,

q(x) = β0 +

m
(cid:88)

j=1

8

S. In

∈

(4)

where β

∈

Rm+1 is the parameter vector, whose values are learned by minimizing the empirical cost

on data set S. Upon observing new feature values, the decision-maker can then directly decide upon

the order quantity instead of ﬁrst estimating the uncertain demand.

Since the learned decision function may overﬁt to the in-sample data set S, it is common practice

in machine learning to evaluate the out-of-sample generalization on a separate test data set Stest.

To avoid overﬁtting and improve the out-of-sample generalization, Ban & Rudin (2019) proposed

an extension of Beutel & Minner (2012) by integrating a regularization term into the loss function.

Accordingly, the objective comprises a trade-oﬀ between minimizing the empirical in-sample cost and

the regularization term, with a constant hyperparameter balancing these two terms:

(ERM-(cid:96)p)

min
β

1
S
|

(cid:88)

i∈S

|

C(qi ; di) + λ

β

||

p
||

s.t.

qi = β(cid:62)xi

(5)

(6)

S,

i
∀

∈

where λ

≥

0 is the regularization hyperparameter and

β

||

p is the (cid:96)p-norm of the vector β. Depending

||

on the choice of p in the regularization, the resulting model may be a MILP, an LP, or a second-order

cone program, for (cid:96)0, (cid:96)1, and (cid:96)2-norm regularization, respectively. Eﬀectively, regularization enables

feature selection by penalizing the complexity of the solution, thereby favoring sparse solution vectors.

3. Methodology

We start this section presenting the BHO model, which incorporates hyperparameter ﬁtting in

(ERM-(cid:96)p). Then, we introduce the BFS model as an alternative bilevel program that avoids regular-

ization.

3.1. Bilevel Hyperparameter Optimization (BHO)

In Section 2, we assumed the hyperparameter λ as introduced in (ERM-(cid:96)p) to be given. However,

identifying λ constitutes a challenge in itself as a respective misspeciﬁcation can signiﬁcantly reduce

cost performance. To parametrize λ correctly, one may utilize existing techniques for hyperparameter

9

optimization, which partition the original data set S into a training set T and a validation set V .

On the training set, one learns the model parameters for a ﬁxed hyperparameter value. Using the

validation set, one can then assess the cost of the trained model for a variety of hyperparameter

values, to ﬁnally choose the value λ∗ that leads to a minimum cost on the validation set. Next, we

present the BHO formulation, which models the search for the optimal hyperparameter λ∗ as a bilevel

optimization problem.

We introduce variables ui to model the inventory shortage and variables oi to model the surplus

inventory at the end of period i

V .

T

∪

∈

In the following bilevel programming formulation, the

upper-level (UL) problem searches for an optimal regularization value λ∗

0 that minimizes cost on

≥

the validation set V . In turn, the lower-level (LL) problem solves the feature-based newsvendor, as

stated in (ERM-(cid:96)p), on the training set T :

(BHO-(cid:96)p UL)

C∗

BHO = min
λ≥0

1
V

|

(cid:88)

i∈V

|

(bui + hoi)

s.t. ui

di

−

≥

β(cid:62)xi

oi

ui

β

β(cid:62)xi

di

−

0, oi

0

≥

≥

≥

Ωp(λ),

∈

V

V

V

i
∀

i
∀

i
∀

∈

∈

∈

where Ωp(λ) is the set of optimal solutions β to the lower-level problem, parameterized by λ:

(BHO-(cid:96)p LL)

Ωp(λ) := arg min

β

1
T
|

(cid:88)

i∈T

|

(bui + hoi) + λ

β

||

s.t. ui

di

−

≥

β(cid:62)xi

oi

ui

≥

≥

β(cid:62)xi

di

−

0, oi

0

≥

2
p
||

T

T

T

i
∀

i
∀

i
∀

∈

∈

∈

(7)

(8)

(9)

(10)

(11)

(12)

(13)

(14)

(15)

The upper-level objective (7) minimizes the newsvendor cost on the validation set V and the lower-

level objective (12) minimizes the regularized newsvendor cost on the training set T . Constraints (8)

10

and (13) deﬁne the inventory shortage for period i

V and i

∈

∈

T , respectively, given the decision

function parametrized by β. Constraints (9) and (14) deﬁne the surplus inventory at period i

V

∈

and i

∈

T . Constraints (10), (11), and (15) deﬁne the variable domains.

So far, we deﬁne the BHO formulation in (7)–(15) for a general (cid:96)p-norm, which leads to a diﬀerent

model for diﬀerent p. In the following, we illustrate some properties of BHO under the special case of

the (cid:96)0-norm regularization, which minimizes the number of non-zero elements in the β vector. In this

case, we introduce the binary variable zj to indicate whether coeﬃcient βj is non-zero. The lower-level

problem can then be formulated as a mixed integer program (MIP):

(BHO-(cid:96)0 LL)

Ω0(λ) := arg min

β

1
T
|

(cid:88)

i∈T

|

(bui + hoi) + λ

(cid:88)

zj

j∈J

s.t.

(13)–(15)

βj = 0 if zj = 0

zj

0, 1
}

∈ {

(16)

(17)

(18)

J

J,

j
∀

j
∀

∈

∈

where Constraints (13)–(15) deﬁne the shortage and surplus inventory and Constraints (17) enforce

that βj = 0 if the corresponding feature is not selected.

The BHO formulation (7)–(15) generalizes many common methods for hyperparameter optimiza-

tion. To avoid the high computational eﬀort of solving the BHO model to optimality, existing methods

relax the assumption that λ can take any value in R≥0, and consider a ﬁnite support set Λ

R≥0

⊆

instead (Bergstra & Bengio 2012, Bergstra et al. 2013). For example, suppose the values in Λ are

equally spaced along a grid, i.e., a line segment, then the resulting model corresponds to the well-

known grid search method.

If the values in Λ are randomly selected in a closed region, then the

formulation describes the random search method. Other approaches, e.g., based on Bayesian opti-

mization, would perform an adaptive search, iteratively selecting a value λ for the upper-level variable

and then optimizing the lower-level problem. The iterative selection of new values for λ depends on the

validation performance of previously selected points. In essence, current methods for hyperparameter

11

optimization, such as the examples described above, are heuristics that avoid solving the BHO model

to optimality.

3.2. Bilevel Feature Selection (BFS)

To remedy the drawback of BHO, we introduce a bilevel programming formulation speciﬁcally

designed for feature selection. Instead of penalizing the number of selected features, we propose a

more intuitive model, in which the upper-level problem selects a subset of features that minimize the

empirical cost on a validation set. We then reformulate the resulting model into a single-level problem,

which is computationally more tractable, and ﬁnally compare the proposed BFS and BHO models.

Consider our original data set S, which we partition into a training set T and a validation set V .

We introduce binary variables zj, for j

∈

J, to indicate whether feature j is marked as relevant

(zj = 1) or not (zj = 0).

In the upper-level, BFS selects a subset of features that minimizes the

empirical cost on the validation set V . The lower-level problem then learns the optimal coeﬃcients of

the decision function in the training set T by solving the ERM model using only the features selected

in the upper-level. We formulate the resulting upper-level problem as follows:

(BFS UL)

C∗

BFS = min

z

1
V

|

(cid:88)

i∈V

|

(bui + hoi)

s.t.

(8)–(10)

zj

0, 1

∈ {

}

Π0(z),

β

∈

J

j
∀

∈

where Π0(z) is the set of optimal solutions β to the lower-level problem:

(BFS LL)

Π0(z) := arg min

β

1
T

|

(cid:88)

i∈T

|

(bui + hoi)

s.t.

(13)–(15)

βj = 0 if zj = 0

J

j
∀

∈

12

(19)

(20)

(21)

(22)

(23)

The upper and lower-level objectives (19) and (22) minimize the newsvendor cost on the validation

set V and training set T , respectively. Constraints (8)–(10) and (13)–(15) deﬁne the shortage and

surplus inventory. Constraints (20)–(21) deﬁne the variable domains and Constraints (23) ensure that

βj = 0 if feature j is not selected.

We reformulate Model (19)–(23) by substituting the lower-level problem by its Karush-Kuhn-

Tucker (KKT) conditions (cf. Cao & Chen 2006, Fontaine & Minner 2014). We introduce the dual

variables µi, and γi corresponding to constraints (13) and (14) of the lower-level problem. The equiv-

alent single-level (SL) optimization problem can then be expressed by using indicator constraints:

(BFS SL)

min

1
V
|

(cid:88)

i∈V

|

(bui + hoi)

s.t.

(8)–(10), (13)–(15), (20), (23)

1
T
|

(cid:88)

i∈T

|

(bui + hoi)

(cid:88)

(γi

i∈T

−

≤

µi)di

µi +

γi +

b
T
|
h
T
|

|

|

0

0

≥

≥

(cid:88)

(µi

i∈T

−

γi)xj

i = 0 if zj = 1

µi

≤

0, γi

0

≤

(24)

(25)

T (26)

T (27)

i
∀

∈

i
∀

∈

j
∀

∈

J (28)

T (29)

i
∀

∈

As before, Constraints (8)–(10) and (13)–(15) deﬁne the shortage and surplus inventory. Constraints (20)

and (23) model the selection of features. Constraint (25) represents the optimality condition of the

lower-level problem, by comparing its primal objective value with the corresponding dual objective

value. Constraints (26) and (27) are the dual constraints of the lower-level problem associated with

primal variables ui and oi for i

∈

T . Constraints (28) are the dual constraints related to the primal

variables βj for j

∈

J, and Constraints (29) deﬁne the domain of the dual variables. The single-level

reformulation has 2n + 2

T
|

|

+ 2

J

|

|

variables and 2n + 2

T
|

|

+ 2

J
|

|

+ 1 constraints.

The BFS model shares some similarities with the BHO model. Both models have the same upper-

13

Formulation

BHO ((cid:96)0-norm reg.)

BFS

Upper-level

Lower-level

Objective

minimize validation cost

minimize validation cost

Variables

Objective

Variables

R≥0

λ

∈

minimize training cost +

z

0, 1

∈ {

m+1
}

minimize training cost

regularization
Rm+1, z

β

∈

0, 1
}

m+1

Rm+1

β

∈

∈ {

Table 1: Comparison between BFS and BHO with (cid:96)0-norm regularization

level objective and the lower-level objectives diﬀer only in the presence of the regularization term.

We provide an overview of the main properties of both models in Table 1. The main advantage of

the BFS model regarding tractability is due to the existence of binary variables being limited to the

upper-level problem. Consequently, we can reformulate the BFS model into a MILP and leverage the

power of today’s optimization software to ﬁnd optimal solutions.

Moreover, the following results show that the optimal cost of the BFS model is a lower bound to

the optimal cost of the BHO model when adopting (cid:96)0-norm regularization.

Lemma 1. Given a ﬁxed selection of features z for both BHO and BFS, i.e., zBHO = zBFS = z(cid:48)

(assuming that z(cid:48) is feasible for both problems), solving the remaining problems for the rest of the

decision variables yields optimal solutions βBHO

(cid:12)
(cid:12)z=z(cid:48) = βBFS

(cid:12)
(cid:12)z=z(cid:48) with costs CBHO

(cid:12)
(cid:12)z=z(cid:48) = CBFS

(cid:12)
(cid:12)z=z(cid:48).

Proof. By ﬁxing zBHO = z(cid:48), the regularization term in the lower-level objective becomes constant

and λ does not inﬂuence the optimal solution. Therefore, we can ignore regularization and the

lower-level problem of the BHO becomes equal to the lower-level problem of the BFS, leading to

βBHO

(cid:12)
(cid:12)z=z(cid:48) = βBFS

(cid:12)
(cid:12)z=z(cid:48) as the optimal solution. Since the upper-level objectives are equal in both

models, the optimal costs will be equal: CBHO

(cid:12)
(cid:12)z=z(cid:48) = CBFS

(cid:12)
(cid:12)z=z(cid:48).

Proposition 1. The optimal cost of the BFS model is a lower bound for the optimal cost of the BHO

model with (cid:96)0-norm regularization: C∗

BHO ≥

C∗

BFS.

Proof. Let zBHO = z∗ be the optimal selection of features according to BHO with cost C∗

BHO. Suppose

14

that zBFS = z(cid:48) is a solution to BFS, such that C∗

BHO < CBFS

(cid:12)
(cid:12)z=z(cid:48). We can always improve the cost of

BFS by setting zBFS = z∗ in the upper-level problem. Because of Lemma 1, this will result in a new

solution with cost CBFS

(cid:12)
(cid:12)z=z∗ = C∗

BHO ≥

C∗

BFS.

3.3. Bilevel Feature Selection with Cross-Validation (BFS-CV)

Cross-validation strategies often improve the generalization ability of machine learning models and

prevent overﬁtting by using data re-sampling methods. Accordingly, we extend the BFS model to

cross-validation instead of simple hold-out validation. We consider K training-validation splits of the

data and search for the set of features that minimize the average cost over all K validation sets. For

each k

[K] =

1, . . . , K

, we consider a subset of observations Sk
}

⊆

{

∈

S sampled from the original data

set S. Analogously to BFS, we partition the set Sk into a training set Tk and a validation set Vk. We

introduce variables uik and oik to model the inventory shortage and surplus, respectively, at the end

of period i

Tk

∪

∈

Vk for each training-validation split k

∈

[K]. We then learn the model parameters

Rm+1 using the corresponding training set Tk, and select features by minimizing the average

βk ∈
validation cost over all validation sets Vk for k

[K]. The resulting problem is a bilevel program with

∈

K lower-level problems:

(BFS-CV UL)

min

1
K

K
(cid:88)

k=1

|

1
Vk

(cid:88)

i∈Vk

|

(buik + hoik)

s.t. uik

di

−

≥

β(cid:62)

k xi

oik

uik

≥

≥

β(cid:62)

k xi

di

−

0, oik

0

≥

zj

0, 1

∈ {

}

βk ∈

Πk(z)

k

k

k

∀

∀

∀

∈

∈

∈

[K],

[K],

[K],

i
∀

i
∀

i
∀

∈

∈

∈

Vk

Vk

Vk

J

j
∀

∈

k

∀

∈

[K],

where Πk(z) is the set of optimal solutions corresponding to the kth lower-level problem:

(BFS-CV LL)

Πk(z) := arg min
βk

(buik + hoik)

(cid:88)

i∈Tk

|

1
Tk

|

15

(30)

(31)

(32)

(33)

(34)

(35)

(36)

s.t. uik

di

−

≥

β(cid:62)

k xi

oik

uik

≥

≥

β(cid:62)

k xi

di

−

0, oik

0

≥

βj
k = 0 if zj = 0

i
∀

i
∀

i
∀

∈

∈

∈

Tk

Tk

Tk

J

j
∀

∈

Constraints (31)–(33) and (37)–(39) deﬁne the shortage and surplus inventory for period i

(37)

(38)

(39)

(40)

Vk and

∈

i

Tk, respectively for each split k

∈
Constraints (40) ensure that βj

[K]. Constraints (34) and (35) deﬁne the variable domains and

∈

k = 0 if feature j is not selected for the training-validation split k.

The above model can accommodate diﬀerent cross-validation strategies, such as K-fold, random

permutations (Shuﬄe & Split), or Leave-P-Out cross-validation (see, e.g., Arlot & Celisse 2010, Hastie

et al. 2009). Each particular choice of cross-validation strategy corresponds to a diﬀerent approach

for constructing the subsets Sk and partitioning the data into Tk and Vk. Moreover, the special case

with K = 1 corresponds to the previously introduced BFS model.

Analogously to BFS, Bilevel Feature Selection with cross-validation (BFS-CV) can be reformulated

into a single-level MILP:

(BFS-CV SL)

min
z

1
K

K
(cid:88)

k=1

1
Vk
|

(cid:88)

i∈Vk

|

(buik + hoik)

s.t.

(31)–(34), (37)–(40)

1
Tk
|

(cid:88)

i∈Tk

|

(buik + hoik)

(cid:88)

i∈Tk

≤

(γik

−

µik)di

µik +

γik +

b
Tk
|

h
Tk
|

≥

|

≥

|

0

0

(cid:88)

i∈Tk

(µik

−

γik)xj

i = 0 if zj = 1

µik

≤

0, γik

0

≤

(41)

k

∀

∈

[K]

(42)

k

∀

∈

[K],

i
∀

∈

Tk (43)

k

∀

∈

[K],

i
∀

∈

Tk (44)

k

∀

∈

[K],

j
∀

∈

J (45)

k

∀

∈

[K],

i
∀

∈

Tk (46)

Constraints (42) represents the optimality condition of the lower-level problem, for each training-

16

validation split k

∈

[K]. Constraints (43) and (44) are the dual constraints of the lower-level problem

associated with primal variables uik and oik for i

∈
constraints related to the primal variables βk for k

Tk for k

∈

[K]. Constraints (45) are the dual

[K], and Constraints (46) deﬁne the domain

∈

T
of the dual variables. The single-level reformulation has K(4
|

|

+ 2

+

V

|

|

J
|

) +
|

J
|

|

variables and

T
K(4
|

|

+ 2

V
|

|

+ 2

J
|

|

+ 1) constraints.

4. Experimental Design

To benchmark our approaches, we perform extensive computational experiments on synthetic in-

stances. The goals of our computational experiments are fourfold.

(i) We evaluate the performance of a MILP approach for the proposed BFS and BFS-CV models in

terms of feature selection and generalization to out-of-sample data;

(ii) We compare the performance of our approach against existing regularization-based methods;

(iii) For each method, we compare the performance of hold-out validation against cross-validation;

(iv) We analyze the eﬀect of diﬀerent instance parameters on each method’s performance.

We implemented all methods in C++, using CPLEX 20.1 to solve the respective MILP formulations.

All experiments were performed on machines with Intel Core i7-6700 CPU at 3.40 GHz, with 16 GB

of RAM, and Ubuntu 16.04.6 LTS operating system, under a time limit of 900 seconds. We provide

the source code and data at [to be disclosed after peer-review].

4.1. Instances

We adapt the experimental setup from Zhu et al. (2012) and consider a linear demand model with

additive noise:

dlinear(x) = 5 + β(cid:62)x + (cid:15),

(47)

where β = (0, 2,

2,

−

1, 1, 0, . . . , 0)(cid:62)/√10 is an (m + 1)-dimensional vector representing the ground-

−

truth coeﬃcients. The feature variables x

∈

Rm+1 are drawn from a multivariate Gaussian distribution

17

with mean zero and covariance matrix with entries σij = 0.5|i−j|, for (i, j)

J 2. The noise term follows

∈

a Gaussian distribution (cid:15)

∼ N

(0, σ2

(cid:15) ). We set negative demand values to zero.

We generate instances varying the number of samples n from 40 to 2000 and the number of

features m from 8 to 14. For each conﬁguration, we generate 20 instances to account for variability

in the distributions. Additionally, we generate a separate test set with 1000 observations associated

with each instance, following the same distributions.

Furthermore, we analyze the impact of demand misspeciﬁcation, i.e., when the demand is not

linearly related to the features. We investigate the following nonlinear demand model (cf. Zhu et al.

2012):

dnonlinear(x) = 10 + sin (cid:0)2(β(cid:62)x)(cid:1) + 2 exp (cid:0)

16(β(cid:62)x)2(cid:1) + ϕ(β(cid:62)x)(cid:15),

−

(48)

where ϕ(β(cid:62)x) = 1 for a homoscedastic case and ϕ(β(cid:62)x) = exp(β(cid:62)x) for a heteroscedastic case.

We solve the proposed BFS and BFS-CV models and compare the results against regularization-

based methods from the literature. Since our motivation for feature selection is to provide more

explainable decisions, we focus on methods based on linear decision functions, which are intrinsically

more explainable. We consider the ERM model of Ban & Rudin (2019) with (cid:96)0 and (cid:96)1-norm reg-

ularization and use grid search with 50 break-points to calibrate the regularization parameter. We

run each considered method once using a hold-out validation set and once with Shuﬄe & Split cross-

validation (CV). For hold-out validation, we use half of the samples in each instance as a training set

and the other half as a validation set, following the setting in Zhu et al. (2012). For cross-validation,

we perform K = 50 re-sampling iterations, where we sample a subset of size

Sk
|

|

= min

200, n
}

{

, for

each k

[K].

∈

18

4.2. Performance metrics

We assess the ability of each method to recover the ground-truth feature vector, adopting the

accuracy measure:

α =

1
m

m
(cid:88)

j=1

1(ˆzj = z∗

j ),

(49)

where 1(
) is the indicator function, ˆz = [ˆz1, . . . , ˆzm] is the estimated binary feature vector, and
·
1, . . . , z∗

m] is the ground-truth vector, deﬁned as z∗

j = 0, i.e., feature j is non-

j = 0 if β∗

z∗ = [z∗

informative, otherwise z∗

j = 1. Our deﬁnition of accuracy is analogous to the one commonly adopted

for binary classiﬁcation, where z∗

j ∈ {

0, 1

represents the class assigned to feature j.

}

Additionally, we analyze the cost performance of applying the learned decision functions to out-

of-sample data. We therefore evaluate the out-of-sample cost on a separate test data set with 1000

observations. We report the test cost values of each method

in terms of its percentage deviation

M

relative to the test cost achieved by BFS-CV on the same conﬁguration:

δ(

M

) = 100

×

Ctest

BFS-CV

Ctest

M −
Ctest

BFS-CV

,

(50)

where Ctest

M is the average newsvendor cost of method

M

calculated on the test set. Deviation values

greater than zero indicate that BFS-CV improves upon method

regarding test cost performance,

M

while values below zero indicate that method

achieves lower test cost than BFS-CV.

M

5. Results

First, we present results concerning instances generated by the linear demand model, and then

discuss results on instances with nonlinear demand.

5.1. Linear demand model

We analyze how instance size, number of features, noise level, shortage cost, and holding cost aﬀect

the performance of each method. Unless otherwise stated, we use a setting with n = 1000 samples,

m = 10 features, a shortage cost of b = 2, and a holding cost of h = 1 as reference conﬁguration.

19

For the noise term, we consider σ(cid:15) = 1 as reference conﬁguration for the Gaussian distribution. We

provide results regarding computation times in Appendix A.

Instance size. Figure 1 reports the feature recovery accuracy of the diﬀerent methods for a varying

number of samples n

∈

[40, 2000], averaged over 20 randomly generated instances. In general, BFS-CV

achieves the highest accuracy among all methods and faster convergence for increasing n, with accuracy

values above 0.9 already for instances with 200 samples. In contrast, existing methods often yield

accuracy values below 0.9 and fail to recover the ground-truth features accurately, even for instances

with a larger number of samples. We conﬁrm these results at 5% signiﬁcance level by pairwise Wilcoxon

signed-rank tests, and refer to Appendix B for details on the respective p-values.

Figure 1: Impact of instance size on the accuracy of feature recovery

Besides feature recovery, we evaluate the out-of-sample cost performance of each method. Figure 2

shows the distribution of percentage deviations, where a positive deviation indicates that BFS-CV is

superior to the respective other method. We split the results in three diﬀerent plots based on the

sample size n, classifying each instance as small, medium, or large. BFS-CV outperforms the other

methods in most cases, as the lower quartiles are always above or close to zero. For smaller instances,

test cost deviations can be as high as 30% in the best case. As we increase n, all methods present

improving test cost performance and the variance in the test cost distribution decreases. Yet, BFS-CV

is still superior to the other methods in the wide majority of cases. For large instances, all methods

present mostly positive test cost deviations, with values ranging from

1% to 8%.

−

20

0500100015002000Numberofsamplesn0.50.60.70.80.91Avg.featurerecoveryacc.MethodBFSBFS-CVERM-‘0ERM-‘0(CV)ERM-‘1ERM-‘1(CV)Figure 2: Impact of instance size on the test cost performance of diﬀerent methods, relative to BFS-CV

Number of features. Figure 3 shows average feature recovery accuracy values, where we now ﬁx the

number of samples to n = 1000 and vary the number of features m

8, 10, 12, 14
}

∈ {

. For all methods,

the number of features does not strongly aﬀect the accuracy performance. Notably, BFS-CV achieves

average accuracy values consistently above 0.95, being superior to the other methods, as conﬁrmed by

pairwise Wilcoxon tests.

Figure 3: Impact of number of features m on the accuracy of feature recovery

Regarding test cost performance, Figure 4 shows the distribution of test cost deviations. We focus

on large instances (n

≥

1500) in this analysis, which have lower variance, so that we can isolate the

impact of m on the test cost. For BFS, ERM-(cid:96)0, and ERM-(cid:96)1, the number of features has no strong

inﬂuence on the test cost deviations. In contrast, the performance of ERM-(cid:96)0 (CV) and ERM-(cid:96)1 (CV)

shows increasing deviation values for increasing m. In the majority of cases, BFS-CV outperforms the

other considered methods.

21

BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−100102030Testcostdeviationδ[%]Smallerinstances(n≤500)BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−4−202468Testcostdeviationδ[%]Mediuminstances(500<n<1500)BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−4−202468Testcostdeviationδ[%]Largerinstances(n≥1500)8101214Numberoffeaturesm0.50.60.70.80.91Avg.featurerecoveryacc.MethodBFSBFS-CVERM-‘0ERM-‘0(CV)ERM-‘1ERM-‘1(CV)Figure 4: Impact of number of features m on the test cost performance of diﬀerent methods, relative to BFS-CV

Noise level. We vary the coeﬃcient of variation cv = σ(cid:15)/µ

[0.2, 1] and report the average feature

∈

recovery accuracy for each method in Figure 5. As we increase the level of noise, it becomes harder

to recover the informative features and the accuracy of all methods deteriorates. For cv

[0.2, 0.6],

∈

BFS-CV achieves the highest accuracy among the considered methods. Outside this range, BFS-CV is

outperformed by ERM-(cid:96)0 (CV) for cv = 0.1 and by ERM-(cid:96)0 for cv

0.8, respectively. Still, BFS-CV

≥

generally attains comparatively high accuracy, being superior to most other methods.

Figure 5: Impact of diﬀerent noise levels on the accuracy of feature recovery

Figure 6 illustrates the impact of diﬀerent noise levels on test cost performance, considering large

instances (n

≥

1500). Methods BFS, ERM-(cid:96)0, and ERM-(cid:96)1 mostly outperform BFS-CV for cv

0.4.

≥

In such cases, test cost deviations range from

4% to 4%, indicating that BFS-CV achieves compa-

−

rable results even when its performance is inferior to other methods. Methods ERM-(cid:96)0 (CV) and

ERM-(cid:96)1 (CV) perform comparatively worse, with mostly positive deviations and larger variance in

22

BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−4−202468Testcostdeviationδ[%]Features81012140.20.40.60.81Coeﬃcientofvariationcv0.50.60.70.80.91Avg.featurerecoveryacc.MethodBFSBFS-CVERM-‘0ERM-‘0(CV)ERM-‘1ERM-‘1(CV)Figure 6: Impact of noise level on the test cost performance of diﬀerent methods, relative to BFS-CV

the distribution.

Shortage and holding costs. Figure 7 displays results on the accuracy performance as a function

of the newsvendor ratio b/(b + h), by varying (b, h)

[1, 10]2. In general, BFS-CV has accuracy values

∈

consistently above 0.9 and outperforms the other methods.

Figure 7: Impact of shortage cost b and holding cost h on the accuracy of feature recovery

Figure 8 shows results on test cost performance of each method as a function of b, for large instances

(n

≥

1500), where we ﬁxed h = 1, corresponding to newsvendor ratios b/(b + h)

0.5. ERM-(cid:96)0 (CV)

≥

and ERM-(cid:96)1 (CV) generally perform worse than BFS-CV, since the deviation values are often positive.

For all considered methods, we observe that the test cost deviations range from

6% to 10% and the

−

variance increases with increasing b. We observed similar results for cases with b/(b + h) < 0.5.

23

BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−4−202468Testcostdeviationδ[%]Coeﬃcientofvariationcv0.10.20.40.60.81.00.10.20.30.40.50.60.70.80.9Newsvendorcostb/(b+h)0.50.60.70.80.91Avg.featurerecoveryacc.MethodBFSBFS-CVERM-‘0ERM-‘0(CV)ERM-‘1ERM-‘1(CV)Figure 8: Impact of shortage cost b on the test cost performance of diﬀerent methods, relative to BFS-CV

5.2. Nonlinear demand model

Demand may not be linearly related to the features. Therefore, we investigate how each method

performs under nonlinear demand models, considering homoscedastic and heteroscedastic settings. In

the following, we only present results regarding accuracy performance. Results on test cost perfor-

mance did not provide new insights, since we observed similar patterns as in the case of linear instances

(see Appendix C). Unless otherwise stated, we use the same reference conﬁguration as in the previous

section.

Instance size. Due to the nonlinear structure of the demand models, Figure 9 shows considerably

lower accuracy values compared to instances with linear demand. For heteroscedastic instances, BFS-

CV outperforms existing methods, with accuracy values above 0.9 already for n = 500 samples. For

the homoscedastic case, all methods present inferior accuracy compared to the heteroscedastic case. In

this setting, BFS-CV and ERM-(cid:96)0 present comparable performance, superior to the other considered

methods.

Number of features. Figure 10 shows the average feature recovery accuracy for varying m

∈

8, 10, 12, 14
}
{

, for both homoscedastic and heteroscedastic demand. Similarly as for instances with

linear demand, the number of features does not strongly inﬂuence the accuracy performance for non-

linear instances.

24

BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−5.0−2.50.02.55.07.510.0Testcostdeviationδ[%]Shortagecostb12345678910Figure 9: Impact of instance size on the accuracy of feature recovery

Figure 10: Impact of number of features m on the accuracy of feature recovery

Noise level. For the heteroscedastic case, BFS-CV outperforms existing methods (Figure 11). In

this setting, varying the standard deviation σ(cid:15) does not strongly aﬀect the accuracy performance.

For homoscedastic instances, all methods present inferior accuracy compared to heteroscedastic in-

stances. Here, ERM-(cid:96)0 is superior to other methods for most values of σ(cid:15), but BFS-CV often presents

comparable accuracy.

Shortage and holding costs.

In Figure 12, for heteroscedastic instances, BFS-CV has superior

accuracy than other methods when b/(b + h)

≥

0.3. We observe that all methods perform poorly

when the newsvendor ratio is below 0.3. In particular, the considered methods often do not recover

any features when b/(b + h)

∈

[0.1, 0.2], leading to accuracy values between 0.5 and 0.65. For the

homoscedastic case, the accuracy of all methods decreases with increasing newsvendor ratios, with

values often below 0.8. In this setting, BFS-CV is superior to most methods, but is outperformed by

25

05001000150020000.50.60.70.80.91Avg.featurerecoveryacc.HomoscedasticmodelMethodBFSBFS-CVERM-‘0ERM-‘0(CV)ERM-‘1ERM-‘1(CV)0500100015002000NumberofsamplesnHeteroscedasticmodel81012140.50.60.70.80.91Avg.featurerecoveryacc.HomoscedasticmodelMethodBFSBFS-CVERM-‘0ERM-‘0(CV)ERM-‘1ERM-‘1(CV)8101214NumberoffeaturesmHeteroscedasticmodelFigure 11: Impact of diﬀerent noise levels on the accuracy of feature recovery

Figure 12: Impact of shortage cost b and holding cost h on the accuracy of feature recovery

ERM-(cid:96)0 when b/(b + h)

≥

0.8 and by ERM-(cid:96)1 (CV) when b/(b + h)

[0.2, 0.35].

∈

One comment on these results is in order. Applying a linear decision function to nonlinear data

may naturally lead to poor results, due to the inconsistent dependency structure with respect to

the features.

In some cases, we observed that all methods fail to achieve reasonable accuracy in

feature recovery. However, for the vast majority of settings that we considered, BFS-CV presented

good performance, outperforming the other methods. One possibility for dealing with such nonlinear

instances would be to include additional features by considering nonlinear transformations of the

original features, which we leave for future research.

6. Conclusion

We have presented a novel formulation based on bilevel optimization for incorporating feature

selection in the feature-based newsvendor problem. The proposed BFS models provide an intuitive

26

123450.50.60.70.80.91Avg.featurerecoveryacc.HomoscedasticmodelMethodBFSBFS-CVERM-‘0ERM-‘0(CV)ERM-‘1ERM-‘1(CV)12345Standarddeviationσ(cid:15)Heteroscedasticmodel0.10.20.30.40.50.60.70.80.90.50.60.70.80.91Avg.featurerecoveryacc.HomoscedasticmodelMethodBFSBFS-CVERM-‘0ERM-‘0(CV)ERM-‘1ERM-‘1(CV)0.10.20.30.40.50.60.70.80.9Newsvendorcostb/(b+h)Heteroscedasticmodelapproach speciﬁcally designed for the task of feature selection, in which the upper-level problem

directly selects the subset of relevant features. Our experimental results on synthetic data show

that the proposed methods can accurately recover ground-truth informative features, leading to more

explainable inventory decisions in comparison to previous methods.

There are many possibilities for follow-up works. First, research on tailored solution methods

for BFS and BFS-CV, e.g., based on decomposition strategies for mixed integer programming, may

allow to improve the scalability of the proposed methods when dealing with a large number of fea-

tures. Next, other classes of data-driven optimization problems may beneﬁt from an extension of the

proposed methodology. As an example, the newsvendor problem also captures the fundamental trade-

oﬀs emerging in decisions related to capacity planning. Accordingly, an extension of the proposed

BFS models may be applied to select features in stochastic, data-driven variations of such problems.

Finally, from a broader perspective, incorporating concepts from machine learning into data-driven

optimization problems may lead to crucial advances for both ﬁelds. As exempliﬁed in this work,

feature selection is one among possibly many machine learning tasks that can be integrated into the

decision-making process, especially given the growing need for more explainable decisions.

Acknowledgments

This research has been funded by the Deutsche Forschungsgemeinschaft (DFG, German Research

Foundation) - Projektnummer 277991500. This support is gratefully acknowledged.

References

Agor, J., & ¨Ozaltın, O. Y. (2019). Feature selection for classiﬁcation models via bilevel optimization. Computers &

Operations Research 106:156–168.

Arlot, S., & Celisse, A. (2010). A survey of cross-validation procedures for model selection. Statistics Surveys 4:40–79.

Ban, G.-Y. (2020). Conﬁdence intervals for data-driven inventory policies with demand censoring. Operations Research

68(2):309–326.

27

Ban, G.-Y., & Rudin, C. (2019). The big data newsvendor: Practical insights from machine learning. Operations Research

67(1):90–108.

Ben-Tal, A., Den Hertog, D., De Waegenaere, A., Melenberg, B., & Rennen, G. (2013). Robust solutions of optimization

problems aﬀected by uncertain probabilities. Management Science 59(2):341–357.

Bennett, K. P., Hu, J., Ji, X., Kunapuli, G., & Pang, J.-S. (2006). Model selection via bilevel optimization. In Proceedings

of the IEEE International Joint Conference on Neural Networks (IJCNN). (pp. 1922–1929).

Bennett, K. P., Kunapuli, G., Hu, J., & Pang, J.-S. (2008). Bilevel optimization and machine learning.

In J. M.

Zurada, G. G. Yen, & J. Wang (Eds.), Computational Intelligence: Research Frontiers: IEEE World Congress on

Computational Intelligence (WCCI), Hong Kong, China. Springer, Berlin, Heidelberg (pp. 25–47).

Bergstra, J., & Bengio, Y. (2012). Random search for hyper-parameter optimization. Journal of Machine Learning

Research 13(10):281–305.

Bergstra, J., Yamins, D., & Cox, D. (2013). Making a science of model search: Hyperparameter optimization in hundreds

of dimensions for vision architectures. In S. Dasgupta, & D. McAllester (Eds.), Proceedings of the 30th International

Conference on Machine Learning. Atlanta, Georgia, USA: PMLR, volume 28 of Proceedings of Machine Learning

Research (pp. 115–123).

Bertsimas, D., Gupta, V., & Kallus, N. (2018). Robust sample average approximation. Mathematical Programming

171(1):217–282.

Bertsimas, D., & Kallus, N. (2020). From predictive to prescriptive analytics. Management Science 66(3):1025–1044.

Bertsimas, D., & Thiele, A. (2005). A data-driven approach to newsvendor problems. Technical Report, Massachusetts

Institute of Technology Cambridge, MA.

Bertsimas, D., & Thiele, A. (2006). A robust optimization approach to inventory theory. Operations Research 54(1):150–

168.

Besbes, O., & Mouchtaki, O. (2021). How big should your data really be? Data-driven newsvendor: Learning one sample

at a time. Available at SSRN 3878155.

Besbes, O., & Muharremoglu, A. (2013). On implications of demand censoring in the newsvendor problem. Management

Science 59(6):1407–1424.

Beutel, A.-L., & Minner, S. (2012). Safety stock planning under causal demand forecasting. International Journal of

Production Economics 140(2):637–645.

28

Cao, D., & Chen, M. (2006). Capacitated plant selection in a decentralized manufacturing environment: A bilevel

optimization approach. European Journal of Operational Research 169(1):97–110.

Chen, X., Sim, M., Simchi-Levi, D., & Sun, P. (2007). Risk aversion in inventory management. Operations Research

55(5):828–842.

Cheung, W. C., & Simchi-Levi, D. (2019). Sampling-based approximation schemes for capacitated stochastic inventory

control models. Mathematics of Operations Research 44(2):668–692.

Choi, T.-M. (2012). Handbook of newsvendor problems: Models, extensions and applications, volume 176. Springer, New

York.

Elmachtoub, A. N., & Grigas, P. (2021). Smart “predict, then optimize”. Management Science 68(1):9–26.

Fletcher, R., & Leyﬀer, S. (2002). Nonlinear programming without a penalty function. Mathematical Programming

91(2):239–269.

Fontaine, P., & Minner, S. (2014). Benders decomposition for discrete–continuous linear bilevel problems with application

to traﬃc network design. Transportation Research Part B: Methodological 70:163–172.

Franceschi, L., Frasconi, P., Salzo, S., Grazzi, R., & Pontil, M. (2018). Bilevel programming for hyperparameter op-

timization and meta-learning.

In J. Dy, & A. Krause (Eds.), Proceedings of the 35th International Conference

on Machine Learning. Stockholm, Sweden: PMLR, volume 80 of Proceedings of Machine Learning Research (pp.

1568–1577).

Gallego, G., & Moon, I. (1993). The distribution free newsboy problem: Review and extensions. Journal of the Operational

Research Society 44(8):825–834.

Gallego, G., Ryan, J. K., & Simchi-Levi, D. (2001). Minimax analysis for ﬁnite-horizon inventory models. IIE Transac-

tions 33(10):861–874.

Hanasusanto, G. A., Kuhn, D., Wallace, S. W., & Zymler, S. (2015). Distributionally robust multi-item newsvendor

problems with multimodal demand distributions. Mathematical Programming 152(1):1–32.

Hastie, T., Tibshirani, R., Friedman, J. H., & Friedman, J. H. (2009). The elements of statistical learning: Data mining,

inference, and prediction, volume 2. Springer, New York.

Huber, J., M¨uller, S., Fleischmann, M., & Stuckenschmidt, H. (2019). A data-driven newsvendor problem: From data

to decision. European Journal of Operational Research 278(3):904–915.

Khouja, M. (1999). The single-period (news-vendor) problem: Literature review and suggestions for future research.

Omega 27(5):537–553.

29

Kleywegt, A. J., Shapiro, A., & Homem-de Mello, T. (2002). The sample average approximation method for stochastic

discrete optimization. SIAM Journal on Optimization 12(2):479–502.

Kogan, K., & Lou, S. (2003). Multi-stage newsboy problem: A dynamic model. European Journal of Operational Research

149(2):448–458.

Kuhn, M., & Johnson, K. (2019). Feature engineering and selection: A practical approach for predictive models. Chapman

and Hall/CRC.

Lau, H.-S., & Lau, A. H.-L. (1996). The newsstand problem: A capacitated multiple-product single-period inventory

problem. European Journal of Operational Research 94(1):29–42.

Levi, R., Perakis, G., & Uichanco, J. (2015). The data-driven newsvendor problem: New bounds and insights. Operations

Research 63(6):1294–1306.

Levi, R., Roundy, R. O., & Shmoys, D. B. (2007). Provably near-optimal sampling-based policies for stochastic inventory

control models. Mathematics of Operations Research 32(4):821–839.

Mackay, M., Vicol, P., Lorraine, J., Duvenaud, D., & Grosse, R. (2019). Self-tuning networks: Bilevel optimization of hy-

perparameters using structured best-response functions. In International Conference on Learning Representations

(ICLR). New Orleans, Louisiana, USA.

Mandi, J., Demirovi´c, E., Stuckey, P. J., & Guns, T. (2020). Smart predict-and-optimize for hard combinatorial opti-

mization problems. Proceedings of the AAAI Conference on Artiﬁcial Intelligence 34(2):1603–1610.

Molina, L., Belanche, L., & Nebot, A. (2002). Feature selection algorithms: A survey and experimental evaluation. In

Proceedings of the IEEE International Conference on Data Mining (ICDM). Maebashi City, Japan (pp. 306–313).

Moon, I., & Gallego, G. (1994). Distribution free procedures for some inventory models. Journal of the Operational

Research Society 45(6):651–658.

Oroojlooyjadid, A., Snyder, L. V., & Tak´aˇc, M. (2020). Applying deep learning to the newsvendor problem.

IISE

Transactions 52(4):444–463.

Perakis, G., & Roels, G. (2008). Regret in the newsvendor model with partial information. Operations Research

56(1):188–203.

Petruzzi, N. C., & Dada, M. (1999). Pricing and the newsvendor problem: A review with extensions. Operations Research

47(2):183–194.

Qin, Y., Wang, R., Vakharia, A. J., Chen, Y., & Seref, M. M. (2011). The newsvendor problem: Review and directions

for future research. European Journal of Operational Research 213(2):361–374.

30

Sachs, A.-L., & Minner, S. (2014). The data-driven newsvendor with censored demand observations.

International

Journal of Production Economics 149:28–36.

Scarf, H. (1958). A min-max solution of an inventory problem. In K. Arrow, S. Karlin, & H. Scarf (Eds.), Studies in the

Mathematical Theory of Inventory and Production. Stanford University Press, Stanford (pp. 201–209).

See, C.-T., & Sim, M. (2010). Robust approximation to multiperiod inventory management. Operations Research

58(3):583–594.

Shapiro, A. (2003). Monte carlo sampling methods. In A. Ruszczynski, & A. Shapiro (Eds.), Stochastic Programming,

volume 10 of Handbooks in Operations Research and Management Science. Elsevier, Amsterdam (pp. 353–425).

Wang, C. X., & Webster, S. (2009). The loss-averse newsvendor problem. Omega 37(1):93–105.

Wang, Z., Glynn, P. W., & Ye, Y. (2016). Likelihood robust optimization for data-driven problems. Computational

Management Science 13(2):241–261.

Yue, J., Chen, B., & Wang, M.-C. (2006). Expected value of distribution information for the newsvendor problem.

Operations Research 54(6):1128–1136.

Zhang, Y., & Gao, J. (2017). Assessing the performance of deep learning algorithms for newsvendor problem. In D. Liu,

S. Xie, Y. Li, D. Zhao, & E.-S. M. El-Alfy (Eds.), Neural Information Processing. Springer, Cham (pp. 912–921).

Zhu, L., Huang, M., & Li, R. (2012). Semiparametric quantile regression with high-dimensional covariates. Statistica

Sinica 22(4):1379–1401.

Appendix A. Computation times

In this section, we report the computation times of the diﬀerent methods considered in the ex-

periments. Figures A.13, A.14, A.15, and A.16 show how the solution times scale with the number

of samples n, respectively for m

8, 10, 12, 14
}

∈ {

features. Here, we consider instances with linear

demand model and we ﬁx shortage cost to b = 2, the holding cost to h = 1, and the noise level to

σ(cid:15) = 1.

31

Figure A.13: Solution times (in seconds) for instances with m = 8 features

Figure A.14: Solution times (in seconds) for instances with m = 10 features

Figure A.15: Solution times (in seconds) for instances with m = 12 features

32

BFSBFS-CVERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)10−210−1100101Solutiontime(s)Num.samples40100200400600800100012001400160018002000BFSBFS-CVERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)10−1100101102Solutiontime(s)Num.samples40100200400600800100012001400160018002000BFSBFS-CVERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)10−1100101102Solutiontime(s)Num.samples40100200400600800100012001400160018002000Figure A.16: Solution times (in seconds) for instances with m = 14 features

Considering instances with a larger number of features (m = 14), BFS-CV cannot solve all instances

to optimality within the speciﬁed time limit. Accordingly, Figure A.17 illustrates the distribution of

the optimality gaps (in percentage values) for instances with m = 14 features. In this case, optimality

gaps are often below 5%.

Figure A.17: MIP gaps for m = 14

Appendix B. Complementary results

In the following, we report the p-values of the non-parametric one-sided Wilcoxon signed-rank test,

with null hypothesis that the median diﬀerence in accuracy between the BFS-CV and other methods

is negative. Tables B.2 and B.3 report p-values for instances with linear and nonlinear demand,

respectively.

33

BFSBFS-CVERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)10−1100101102103Solutiontime(s)Num.samples40100200400600800100012001400160018002000140123456MIPgap(%)-BFS-CV-m=14Num.samples40100200400600800100012001400160018002000Figure

BFS

ERM-(cid:96)0

ERM-(cid:96)0 (CV)

ERM-(cid:96)1

ERM-(cid:96)1 (CV)

Figure 1

2.31E-35

8.73E-12

6.88E-23

2.25E-38

1.23E-32

Figure 3

2.53E-14

7.76E-04

4.14E-10

2.32E-14

1.95E-13

Figure 5

3.45E-07

8.89E-02

1.08E-11

1.06E-15

4.47E-13

Figure 7

2.00E-32

6.56E-08

3.96E-23

1.77E-37

5.02E-27

Table B.2: P-values for linear instances.

Figure

Noise

BFS

ERM-(cid:96)0

ERM-(cid:96)0 (CV)

ERM-(cid:96)1

ERM-(cid:96)1 (CV)

Figure 9

Figure 10

Figure 11

Figure 12

Homoscedastic

8.20E-33

3.15E-01

1.77E-53

4.49E-55

2.81E-19

Heteroscedastic

8.92E-56

1.92E-12

1.02E-29

7.49E-65

9.00E-49

Homoscedastic

4.45E-08

7.87E-01

2.66E-13

2.01E-12

2.37E-05

Heteroscedastic

6.90E-13

3.32E-02

8.34E-08

9.35E-15

2.25E-11

Homoscedastic

2.47E-02

9.86E-01

5.60E-11

7.19E-10

6.27E-07

Heteroscedastic

3.05E-16

1.72E-03

9.18E-15

5.51E-20

6.52E-08

Homoscedastic

2.58E-13

3.98E-01

6.28E-25

2.51E-22

2.21E-09

Heteroscedastic

4.86E-13

6.03E-02

4.44E-17

1.72E-26

1.59E-16

Table B.3: P-values for nonlinear instances.

Appendix C. Test cost results for nonlinear instances

We present results regarding test cost performance for instances with nonlinear demand model.

This section follows the same structure as Section 5.2 of the main paper.

Appendix C.1. Instance size

Regarding the impact of instance size n, Figures C.18 and C.19 show test cost results for instances

with heteroscedastic and homoscedastic demand, respectively.

34

Figure C.18: Impact of instance size on the test cost performance (heteroscedastic demand with Gaussian noise)

Figure C.19: Impact of instance size on test cost performance (homoscedastic demand with Gaussian noise)

Appendix C.2. Number of features

Figures C.20 and C.21 show the impact of the number of features m on test cost results for large

instances (n

≥

1500) with heteroscedastic and homoscedastic demand, respectively.

Figure C.20: Impact of number of features m on the test cost (heteroscedastic demand with Gaussian noise)

35

BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−20−100102030Testcostdeviationδ[%]Smallerinstances(n≤500)BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−4−20246Testcostdeviationδ[%]Mediuminstances(500<n<1500)BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−4−202468Testcostdeviationδ[%]Largerinstances(n≥1500)BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−10010203040Testcostdeviationδ[%]Smallerinstances(n≤500)BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−202468Testcostdeviationδ[%]Mediuminstances(500<n<1500)BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−20246Testcostdeviationδ[%]Largerinstances(n≥1500)BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−4−202468Testcostdeviationδ[%]Features8101214Figure C.21: Impact of number of features m on test cost (homoscedastic demand with Gaussian noise)

Appendix C.3. Noise level

Figures C.22 and C.23 show the impact of noise level σ(cid:15) on test cost results for large instances

(n

≥

1500) with heteroscedastic and homoscedastic demand, respectively.

Figure C.22: Impact of noise level σ(cid:15) on test cost (heteroscedastic demand with Gaussian noise)

Figure C.23: Impact of noise level σ(cid:15) on test cost (homoscedastic demand with Gaussian noise)

36

BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−6−4−202468Testcostdeviationδ[%]Features8101214BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−5051015Testcostdeviationδ[%]Std.deviationσ(cid:15)0.51.02.03.04.05.0BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−2.50.02.55.07.510.012.5Testcostdeviationδ[%]Std.deviationσ(cid:15)0.51.02.03.04.05.0Appendix C.4. Shortage cost

Figures C.24 and C.25 show the impact of shortage cost b on test cost results for large instances

(n

≥

1500) with heteroscedastic and homoscedastic demand, respectively.

Figure C.24: Impact of shortage cost b on the test cost (heteroscedastic demand with Gaussian noise)

Figure C.25: Impact of shortage cost b on test cost (homoscedastic demand with Gaussian noise)

Appendix C.5. Holding cost

Figures C.26 and C.27 show the impact of holding cost h on test cost results for large instances

(n

≥

1500) with heteroscedastic and homoscedastic demand, respectively.

Figure C.26: Impact of holding cost h on test cost (heteroscedastic demand with Gaussian noise)

37

BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−5.0−2.50.02.55.07.510.0Testcostdeviationδ[%]Shortagecostb1.02.03.04.05.06.07.08.09.010.0BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−5.0−2.50.02.55.07.510.0Testcostdeviationδ[%]Shortagecostb1.02.03.04.05.06.07.08.09.010.0BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−7.5−5.0−2.50.02.55.07.5Testcostdeviationδ[%]Holdingcosth1.02.03.04.05.06.07.08.09.010.0Figure C.27: Impact of holding cost h on test cost (homoscedastic demand with Gaussian noise)

38

BFSERM-‘0ERM-‘1ERM-‘0(CV)ERM-‘1(CV)−7.5−5.0−2.50.02.55.07.510.012.5Testcostdeviationδ[%]Holdingcosth1.02.03.04.05.06.07.08.09.010.0