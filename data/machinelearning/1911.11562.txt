1
2
0
2

n
a
J

4
1

]
T
S
.
h
t
a
m

[

3
v
2
6
5
1
1
.
1
1
9
1
:
v
i
X
r
a

ADAPTIVE ESTIMATION OF MULTIVARIATE PIECEWISE
POLYNOMIALS AND BOUNDED VARIATION FUNCTIONS BY
OPTIMAL DECISION TREES

By Sabyasachi Chatterjee∗ and Subhajit Goswami†

University of Illinois at Urbana-Champaign and Tata Institute of Fundamental Research

117 Illini Hall
Champaign, IL 61820
sc1706@illinois.edu

1, Homi Bhabha Road
Colaba, Mumbai 400005, India
goswami@math.tifr.res.in

Proposed by Donoho (1997), Dyadic CART is a nonparametric
regression method which computes a globally optimal dyadic deci-
sion tree and ﬁts piecewise constant functions in two dimensions. In
this article we deﬁne and study Dyadic CART and a closely related
estimator, namely Optimal Regression Tree (ORT), in the context of
estimating piecewise smooth functions in general dimensions in the
ﬁxed design setup. More precisely, these optimal decision tree esti-
mators ﬁt piecewise polynomials of any given degree. Like Dyadic
CART in two dimensions, we reason that these estimators can also
be computed in polynomial time in the sample size N via dynamic
programming. We prove oracle inequalities for the ﬁnite sample risk
of Dyadic CART and ORT which imply tight risk bounds for sev-
eral function classes of interest. Firstly, they imply that the ﬁnite
sample risk of ORT of order r ≥ 0 is always bounded by Ck log N
whenever the regression function is piecewise polynomial of degree
r on some reasonably regular axis aligned rectangular partition of
the domain with at most k rectangles. Beyond the univariate case,
such guarantees are scarcely available in the literature for computa-
tionally eﬃcient estimators. Secondly, our oracle inequalities uncover
minimax rate optimality and adaptivity of the Dyadic CART esti-
mator for function spaces with bounded variation. We consider two
function spaces of recent interest where multivariate total variation
denoising and univariate trend ﬁltering are the state of the art meth-
ods. We show that Dyadic CART enjoys certain advantages over these
estimators while still maintaining all their known guarantees.

N

∗Supported by NSF Grant DMS-1916375
†Supported by an IDEX grant from Paris-Saclay and partially by a grant from the Infosys Foundation
Author names are sorted alphabetically.
Keywords and phrases: Optimal Decision Trees, Dyadic CART, Piecewise Polynomial Fitting, Oracle Risk

Bounds, Bounded Variation Function Estimation

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

1

 
 
 
 
 
 
2

CHATTERJEE, S. AND GOSWAMI S.

1. Introduction. Decision Trees are a widely used technique for nonparametric regres-
sion and classiﬁcation. Decision Trees result in interpretable models and form a building
block for more complicated methods such as bagging, boosting and random forests. See Loh
(2014) and references therein for a detailed review. The most prominent example of deci-
sion trees is classiﬁcation and regression trees (CART), proposed by Breiman et al. (1984).
CART operates in two stages. In the ﬁrst stage, it recursively partitions the space of predic-
tor variables in a greedy top down fashion. Starting from the root node, a locally optimal
split is determined by an appropriate optimization criterion and then the process is iterated
for each of the resulting child nodes. The ﬁnal partition or decision tree is reached when a
stopping criterion is met for each resulting node. In the second stage, the ﬁnal tree is pruned
by what is called cost complexity pruning where the cost of a pruned tree thus obtained
is proportional to the number of the leaves of the tree; see Section 9.2 in Friedman et al.
(2001) for details.

A possible shortcoming of CART is that it produces locally optimal decision trees. It is
natural to attempt to resolve this by computing a globally optimal decision tree. However,
computing globally optimal decision tree is computationally a hard problem. It is known
(see Laurent and Rivest (1976)) that computing an optimal (in a particular sense) binary
tree is NP hard. A recent paper of Bertsimas and Dunn (2017) sets up an optimization
problem (see in (Bertsimas and Dunn, 2017, Equation 1)) in the context of classiﬁcation,
which aims to minimize (among all decision trees) misclassiﬁcation error of a tree plus
a penalty proportional to its number of leaves. The paper formulates this problem as an
instance of mixed integer optimization (MIO) and claims that modern MIO developments
allow for solving reasonably sized problems. It then demonstrates extensive experiments
for simulated and real data sets where the optimal tree outperforms the usual CART.
These experiments seem to provide strong empirical evidence that optimal decision trees,
if computed, can perform signiﬁcantly better than CART. Another shortcoming of CART
is that it is typically very hard to theoretically analyze the full algorithm because of the
sequence of data dependent splits. Some results (related to the current paper) exist for
the subtree obtained in the pruning stage, conditional on the maximal tree grown in the
ﬁrst stage; see Gey and Nedelec (2005) and references therein. Theoretical guarantees for
the widely used Random Forests are also typically hard to obtain inspite of much recent
work see Scornet et al. (2015), Wager and Walther (2015), Ishwaran (2015) and references
therein. On the other hand, theoretical analysis for optimal decision trees can be obtained
since it can be seen as penalized empirical risk minimization.

One class of decision trees for which an optimal tree can be computed eﬃciently, in low to
moderate dimensions, is the class of dyadic decision trees. These trees are constructed from
recursive dyadic partitioning. In the case of regression on a two-dimensional grid design,
the paper Donoho (1997) proposed a penalized least squares estimator called the Dyadic
CART estimator. The author showed that it is possible to compute this estimator by a fast
bottom up dynamic program which has linear time computational complexity O(n × n) for
a n × n grid. Moreover, the author showed that Dyadic CART satisﬁes an oracle risk bound
which in turn was used to show that it is adaptively minimax rate optimal over classes of

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

3

anisotropically smooth bivariate functions. Ideas in this paper were later used in Nowak
et al. (2004) in the context of adaptively estimating piecewise Holder smooth functions.
The idea of dyadic partitioning were also used in classiﬁcation in papers such as Scott and
Nowak (2006) and Blanchard et al. (2007) who studied penalized empirical risk minimization
over dyadic decision trees of a ﬁxed maximal depth. They also proved oracle risk bounds
and showed minimax rate optimality for appropriate classes of classiﬁers. Minimax rates of
convergence have also been obtained for various models of dyadic classiﬁcation trees in Lecu´e
(2008). In the related problem of density estimation, dyadic partitioning estimators have
also been studied in the context of estimating piecewise polynomial densities; see Willett
and Nowak (2007). This current paper focusses on the regression setting and follows this
line of work of studying optimal decision trees, proving an oracle risk bound and then
investigating implications for certain function classes of interest. The optimal decision trees
we study in this paper are computable in time polynomial in the sample size.

In particular, in this paper, we study two decision tree methods for estimating regres-
sion functions in general dimensions in the context of estimating some nonsmooth function
classes of recent interest. We focus on the ﬁxed lattice design case like in Donoho (1997). The
ﬁrst method is an optimal dyadic regression tree and is exactly the same as Dyadic CART
in Donoho (1997) when the dimension is 2. The second method is an Optimal Regression
Tree (ORT), very much in the sense of Bertsimas and Dunn (2017), applied to ﬁxed lattice
design regression. Here the estimator is computed by optimizing a penalized least squares
criterion over the set of all — not just dyadic — decision trees. We make the crucial ob-
servation that this estimator can be computed by a dynamic programming approach when
the design points fall on a lattice. Thus, for instance, one does not need to resort to mixed
integer programming and this dynamic program has computational complexity polynomial
in the sample size. This observation may be known to the experts but we are unaware of an
exact reference. Like in Donoho (1997) we show it is possible to prove an oracle risk bound
(see Theorem 2.1) for both of our optimal decision tree estimators. We then apply this
oracle risk bound to three function classes of recent interest by employing approximation
theoretic inequalities and show that these optimal decision trees have excellent adaptive
and worst case performance.

Overall in this paper, we revisit the classical idea of recursive partitioning in the context
of ﬁnding answers to several unsolved questions about some classes of functions of recent
interest in the nonparametric regression literature. In the course of doing so, we have come
up with as well as brought forward several interesting ideas from diﬀerent areas relevant for
the study of regression trees such as dynamic programming, computational geometry and
discrete Sobolev type inequalities for vector / matrix approximation. We believe that the
main novel aspect of the current work is to recognize, prove and point out — by an amal-
gamation of these ideas — that optimal regression trees often provide a better alternative
to the state of the art convex optimization methods in the sense they are simultaneously
(near-) minimax rate optimal, adaptive to the complexity of the underlying signal (under
fewer assumptions) and computationally more eﬃcient for some classes of functions of recent
interest. To the best of our knowledge, our paper is the ﬁrst one among a series of recent

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

4

CHATTERJEE, S. AND GOSWAMI S.

works that shows the eﬃcacy of computationally eﬃcient optimal regression tree estimators
in these particular nonparametric regression problems. We now describe the function classes
we consider in this paper and brieﬂy outline our results and contributions.

• Piecewise Polynomial Functions: We address the problem of estimating multivari-
ate functions that are (or close to) piecewise polynomial of some ﬁxed degree on some
unknown partition of the domain into axis aligned rectangles. This includes function
classes such as piecewise constant/linear/quadratic etc. on axis aligned rectangles.
An oracle, who knows the true rectangular partition, i.e the number of axis aligned
rectangles and their arrangement, can just perform least squares separately for data
falling within each rectangle. This oracle estimator provides a benchmark for adap-
tively optimal performance. The main question of interest to us is how to construct
an estimator which is eﬃciently computable and attains risk as close as possible to
the risk of this oracle estimator. To the best of our knowledge, this question has not
been answered in multivariate settings. In this paper, we propose that our optimal
regression tree (ORT) estimator solves this question to a considerable extent. Sec-
tion 3 describes all of our results under this topic. It is worthwhile to mention here
that we also focus on cases where the true rectangular partition does not correspond
to any decision tree (see Figure 3) which necessarily has a hierarchical structure. We
call such partitions nonhierarchical. Even for such nonhierarchical partitions, we make
the case that ORT continues to perform well (see our results in Section 3.3.1). We
are not aware of nonhierarchical partitions being studied before in the literature. Here
our proof technique uses results from computational geometry which relate the size
of any given (possibly nonhierarchical) rectangular partition to that of the minimal
hierarchical partition reﬁning it.

• Multivariate Bounded Variation Functions: Consider the function class whose
total variation (deﬁned later in Section 4) is bounded by some number. This is a
classical function class for nonparametric regression since it contains functions which
demonstrate spatially heterogenous smoothness; see Section 6.2 in Tibshirani (2015)
and references therein. Perhaps, the most natural estimator for this class of functions
is what is called the Total Variation Denoising (TVD) estimator. The two dimensional
version of this estimator is also very popularly used for image denoising; see Rudin
et al. (1992). It is known that a well tuned TVD estimator is minimax rate optimal
for this class in all dimensions; see H¨utter and Rigollet (2016) and Sadhanala et al.
(2016). Also, in the univariate case, it is known that the TVD estimator adapts to
piecewise constant functions and attains a near oracle risk with parametric rate of
convergence; see Guntuboyina et al. (2020) and references therein. However, even in
two dimensions, the TVD estimator provably cannot attain the near parametric rate of
convergence for piecewise constant truths. This is a result (Theorem 2.3) in a previous
article by the same authors Chatterjee and Goswami (2019).
It would be desirable for an estimator to attain the minimax rate among bounded vari-
ation functions as well as retain the near parametric rate of convergence for piecewise
constant truths in multivariate settings. Our contribution here is to establish that

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

5

Dyadic CART enjoys these two desired properties in all dimensions. We also show
that the Dyadic CART adapts to the intrinsic dimensionality of the function in a par-
ticular sense. Therorem 4.2 is our main result under this topic. Our proof technique
for Theorem 4.2 involves a recursive partitioning strategy to approximate any given
bounded variation function by a piecewise constant function (see Proposition 8.5). We
prove an inequality which can be thought of as the discrete version of the classical
Gagliardo-Sobolev-Nirenberg inequality (see Proposition 8.7) which plays a key role
in the proof.
As far as we are aware, Dyadic CART has not been investigated before in the context of
estimating bounded variation functions. Coupled with the fact that Dyadic CART can
be computed in time linear in the sample size, our results put forth the Dyadic CART
estimator as a fast and viable option for estimating bounded variation functions.

• Univariate Bounded Variation Functions of higher order: Higher order ver-
sions of the space of bounded variation functions has also been considered in nonpara-
metric regression, albeit mostly in the univariate case. One can consider the univariate
function class of all r times (weakly) diﬀerentiable functions, whose r th derivative is
of bounded variation. A seminal result of Donoho and Johnstone (1998) shows that
a wavelet threshholding estimator attains the minimax rate in this problem. Locally
adaptive regression splines, proposed by Mammen and van de Geer (1997), is also
known to achieve the minimax rate in this problem. Recently, Trend Filtering, pro-
posed by Kim et al. (2009), has proved to be a popular nonparametric regression
method. Trend Filtering is very closely related to locally adaptive regression splines
and is also minimax rate optimal over the space of higher order bounded variation
functions; see Tibshirani (2014) and references therein. Moreover, it is known that
Trend Filtering adapts to functions which are piecewise polynomials with regularity
at the knots. If the number of pieces is not too large and the length of the pieces is
not too small, a well tuned Trend Filtering estimator can attain near parametric risk
as shown in Guntuboyina et al. (2020).
Our main contribution here is to show that the univariate Dyadic CART estimator
is also minimax rate optimal in this problem and enjoys near parametric rate of con-
vergence for piecewise polynomials; see Theorem 5.1 and Theorem 5.2. Moreover, we
show that Dyadic CART requires less regularity assumptions on the true function
than what Trend Filtering requires for the near parametric rate of convergence to
hold. Theorem 5.2 follows directly from a combination of our oracle risk bound and
a result about reﬁning an arbitrary (possibly non dyadic) univariate partition to a
dyadic one (see Lemma 8.3). Our proof technique for Theorem 5.1 again involves a
recursive partitioning strategy to approximate any given higher order bounded varia-
tion function by a piecewise polynomial function (see Proposition 8.9). We prove an
inequality (see Lemma 8.10) quantifying the error of approximating a higher order
bounded variation function by a single polynomial which plays a key role in the proof.
Again, as far as we are aware, Dyadic CART has not been investigated before in the
context of estimating univariate higher order bounded variation functions. Coupled
with the fact that Dyadic CART is computable in time linear in the sample size, our

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

6

CHATTERJEE, S. AND GOSWAMI S.

results again provide a fast and viable alternative for estimating univariate higher
order bounded variation functions.

The oracle risk bound in Theorem 2.1 which holds for the optimal decision trees studied in
this paper may imply near optimal results for other function classes as well. In Section 7, we
mention some consequences of our oracle risk bounds for shape constrained function classes.
We then describe a version of our estimators which can be implemented for arbitrary data
with random design and also discuss an extension of our results for dependent noise.

1.1. Problem Setting and Deﬁnitions. Let us denote the d dimensional lattice with N
points by Ld,n := {1, . . . , n}d where N = nd. Throughout this paper we will consider the
standard ﬁxed design setting where we treat the N design points as ﬁxed and located on the
d dimensional grid/lattice Ld,n. One may think of the design points embedded in [0, 1]d and
of the form 1
n (i1, . . . , id) where (i1, . . . , id) ∈ Ld,n. This lattice design is quite commonly
used for theoretical studies in multidimensional nonparametric function estimation (see,
e.g. Nemirovski (2000)). The lattice design is also the natural setting for certain applications
such as image denoising, matrix/tensor estimation. All our results will be for the lattice
design setting. In Section 7, we make some observations and comments about possible
extensions to the random design case.

Letting θ∗ denote the evaluation on the grid of the underlying regression function f , our
observation model becomes y = θ∗ + σZ where y, θ∗, Z are real valued functions on Ld,n and
hence are d dimensional arrays. Furthermore, Z is a noise array consisting of independent
standard Gaussian entries and σ > 0 is an unknown standard deviation of the noise entries.
For an estimator (cid:98)θ, we will evaluate its performance by the usual ﬁxed design expected
mean squared error

1
N
Here (cid:107).(cid:107) refers to the usual Euclidean norm of an array where we treat an array as a vector
in RN .

MSE((cid:98)θ, θ∗) :=

Eθ∗(cid:107)(cid:98)θ − θ∗(cid:107)2.

Let us deﬁne the interval of positive integers [a, b] = {i ∈ Z+ : a ≤ i ≤ b} where Z+ denotes
the set of positive integers. For a positive integer n we also denote the set [1, n] by just
[n]. A subset R ⊂ Ld,n is called an axis aligned rectangle if R is a product of intervals, i.e.
R = (cid:81)d
i=1[ai, bi]. Henceforth, we will just use the word rectangle to denote an axis aligned
rectangle. Let us deﬁne a rectangular partition of Ld,n to be a set of rectangles R such that
(a) the rectangles in R are pairwise disjoint and (b) ∪R∈RR = Ld,n.

Recall that a multivariate polynomial of degree at most r ≥ 0 is a ﬁnite linear combination
of the monomials Πd
i=1 ri ≤ r. It is thus clear that they form a linear
(cid:1). Let us now deﬁne the set of discrete multivariate
space of dimension Kr,d := (cid:0)r+d−1

i=1(xi)ri satisfying (cid:80)d

d−1

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

7

polynomial arrays as

d,n = (cid:8)θ ∈ RLd,n : θ(i1/n, . . . , id/n) =f (i1/n, . . . , id/n) ∀(i1, . . . , id) ∈ [n]d
F (r)

for some polynomial f of degree at most r(cid:9).

For a given rectangle R ⊂ Ld,n and any θ ∈ RLd,n let us denote the array obtained by
restricting θ to R by θR. We say that θ is a degree r polynomial on the rectangle R if
θR = αR for some α ∈ F (r)
d,n.

For a given array θ ∈ RLd,n, let k(r)(θ) denote the smallest positive integer k such that a set
of k rectangles R1, . . . , Rk form a rectangular partition of Ld,n and the restricted array θRi
is a degree r polynomial for all 1 ≤ i ≤ k. In other words, k(r)(θ) is the cardinality of the
minimal rectangular partition of Ld,n such that θ is piecewise polynomial of degree r on the
partition.

1.2. Description of Estimators. The estimators we consider in this manuscript compute
a data dependent decision tree (which is globally optimal in a certain sense) and then ﬁt
polynomials within each cell/rectangle of the decision tree. As mentioned before, computing
decision trees greedily and then ﬁtting a constant value within each cell of the decision tree
has a long history and is what the usual CART does. Fitting polynomials on such greedily
grown decision trees is a natural extension of CART and has also been proposed in the
literature; see Chaudhuri et al. (1994). The main diﬀerence between these estimators and
our estimators is that our decision trees are computed as a global optimizer over the set of
all decision trees. In particular, they are not grown greedily and there is no stopping rule
that is required. The ideas here are mainly inspired by Donoho (1997). We now deﬁne our
estimators precisely.

Recall the deﬁnition of k(r)(θ). A natural estimator which ﬁts piecewise polynomial functions
of degree r ≥ 0 on axis aligned rectangles is the following fully penalized LSE of order r:

(cid:98)θ(r)
all,λ := argmin
θ∈RLd,n

(cid:0)(cid:107)y − θ(cid:107)2 + λk(r)(θ)(cid:1).

Let us denote the set of all rectangular partitions of Ld,n as Pall,d,n. For each rectangular
partition Π ∈ Pall,d,n and each nonnegative integer r, let the (linear) subspace S(r)(Π)
comprise all arrays which are degree r polynomial on each of the rectangles constituting Π.
For a generic subspace S ⊂ RN let us denote its dimension by Dim(S) and the associated
orthogonal projection matrix by OS. Clearly the dimension of the subspace S(r)(Π) is Kr,d|Π|
where |Π| is the cardinality of the partition. Now note that we can also write (cid:98)θ(r)
all,λ =
OS(r)(“Π(λ))y where (cid:98)Π(λ) is a data dependent partition deﬁned as

(1.1)

(cid:98)Π(λ) = argmin

Π:Π∈Pall,d,n

(cid:0)(cid:107)y − OS(r)(Π)y(cid:107)2 + λ|Π|(cid:1).

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

8

CHATTERJEE, S. AND GOSWAMI S.

Thus, computing (cid:98)θ(r)
λ,all really involves optimizing over all rectangular partitions Π ∈ Pall,d,n.
Therefore, one may anticipate that the major roadblock in using this estimator would be
computation. For any ﬁxed d, the cardinality of Pall,d,n is at least stretched-exponential in
N. Thus, a brute force method is infeasible. However, for d = 1, a rectangular partition
is a set of contiguous blocks of intervals which has enough structure so that a dynamic
programming approach is amenable. The set of all multivariate rectangular partitions is a
more complicated object and the corresponding computation is likely to be provably hard.
This is where the idea of Donoho (1997) comes in who considers the Dyadic CART estimator
(for r = 0 and d = 2) for ﬁtting piecewise constant functions. As we will now explain, it
turns out that if we constrain the optimization in (1.1) to optimize over special subclasses of
rectangular partitions of Ld,n, a dynamic programming approach again becomes tractable.
The Dyadic CART estimator is one such constrained version of the optimization problem
in (1.1). We now precisely deﬁne these subclasses of rectangular partitions.

1.2.1. Description of Dyadic CART of order r ≥ 0. Let us consider a generic discrete
interval [a, b]. We deﬁne a dyadic split of the interval to be a split of the interval [a, b] into
two equal intervals. To be concrete, the interval [a, b] is split into the intervals [a, a − 1 +
(cid:100)(b−a+1)/2(cid:101)] and [a+(cid:100)(b−a+1)/2(cid:101), b]. Now consider a generic rectangle R = (cid:81)d
i=1[ai, bi].
A dyadic split of the rectangle R involves the choice of a coordinate 1 ≤ j ≤ d to be split
and then the j-th interval in the product deﬁning the rectangle R undergoes a dyadic split.
Thus, a dyadic split of R produces two sub rectangles R1 and R2 where R2 = R ∩ Rc
1 and
R1 is of the following form for some j ∈ [d],

j−1
(cid:89)

[ai, bi] × [aj, aj − 1 + (cid:100)(bj − aj + 1)/2(cid:101)] ×

R1 =

i=1

d
(cid:89)

[ai, bi].

i=j+1

Starting from the trivial partition which is just Ld,n itself, we can create a reﬁned partition
by dyadically splitting Ld,n. This will result in a partition of Ld,n into two rectangles. We can
now keep on dividing recursively, generating new partitions. In general, if at some stage we
have the partition Π = (R1, . . . , Rk), we can choose any of the rectangles Ri and dyadically
split it to get a reﬁnement of Π with k +1 nonempty rectangles. A recursive dyadic partition
(RDP) is any partition reachable by such successive dyadic splitting. Let us denote the set
of all recursive dyadic partitions of Ld,n as Prdp,d,n. Indeed, a natural way of encoding any
RDP of Ld,n is by a binary tree where each nonleaf node is labeled by an integer in [d]. This
labeling corresponds to the choice of the coordinate that was used for the split.

We can now consider a constrained version of (cid:98)θ(r)
instead of optimizing over Pall,d,n. Let us deﬁne (cid:98)θ(r)
data dependent partition deﬁned as

all,λ which only optimizes over Prdp,d,n
rdp,λ = OS(r)(“Πrdp(λ))y where (cid:98)Πrdp(λ) is a

(cid:98)Πrdp(λ) = argmin

Π:Π∈Prdp,d,n

(cid:0)(cid:107)y − OS(r)(Π)(cid:107)2 + λ|Π|(cid:1).

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

9

The estimator (cid:98)θ(r)
rdp,λ is precisely the Dyadic CART estimator which was proposed in Donoho
(1997) in the case when d = 2 and r = 0. The author studied this estimator for estimating
anisotropic smooth functions of two variables which exhibit diﬀerent degrees of smoothness
in the two variables. However, to the best of our knowledge, the risk properties of the Dyadic
CART estimator (for r = 0) has not been examined in the context of estimating nonsmooth
function classes such as piecewise constant and bounded variation functions. For r ≥ 1, the
above estimator appears to not have been proposed and studied in the literature before. We
call the estimator (cid:98)θ(r)

rdp,λ as Dyadic CART of order r.

1.2.2. Description of ORT of order r ≥ 0. For our purposes, we would need to consider a
larger class of partitions than Prdp,d,n. To generate a RDP, for each rectangle we choose a
dimension to split and then split at the midpoint. Instead of splitting at the midpoint, it is
natural to allow the split to be at an arbitrary position. To that end, we deﬁne a hierarchical
split of the interval to be a split of the interval [a, b] into two intervals, but not necessarily
equal sized. To be concrete, the interval [a, b] is split into the intervals [a, (cid:96)] and [(cid:96) + 1, b]
for some a ≤ (cid:96) ≤ b. Now consider a generic rectangle R = (cid:81)d
i=1[ai, bi]. A hierarchical split
of the rectangle R involves the choice of a coordinate 1 ≤ j ≤ d to be split and then the
j-th interval in the product deﬁning the rectangle R undergoes a hierarchical split. Thus, a
hierarchical split of R produces two sub rectangles R1 and R2 where R2 = R ∩ Rc
1 and R1
is of the following form for some 1 ≤ j ≤ d and aj ≤ (cid:96) ≤ bj,

j−1
(cid:89)

[ai, bi] × [aj, (cid:96)] ×

R1 =

i=1

d
(cid:89)

[ai, bi].

i=j+1

Starting from the trivial partition Ld,n itself, we can now generate partitions by splitting Ld,n
hierarchically. Again, in general if at some stage we obtain the partition Π = (R1, . . . , Rk),
we can choose any of the rectangles Ri and split it hierarchically to obtain k + 1 nonempty
rectangles now. A hierarchical partition is any partition reachable by such hierarchical splits.
We denote the set of all hierarchical partitions of Ld,n as Phier,d,n. Note that a hierarchical
partition is in one to one correspondence with decision trees and thus, Phier,d,n can be
thought of as the set of all decision trees.

Clearly,

Prdp,d,n ⊂ Phier,d,n ⊂ Pall,d,n.

In fact, the inclusions are strict as shown in Figure 1. In particular, there exist partitions
which are not hierarchical.

We can now consider another constrained version of (cid:98)θ(r)
Let us deﬁne (cid:98)θ(r)
as

all,λ which optimizes only over Phier,d,n.
hier,λ = OS(r)(“Πhier(λ))y where (cid:98)Πhier(λ) is a data dependent partition deﬁned

(cid:98)Πhier(λ) = argmin

Π:Π∈Phier,d,n

(cid:0)(cid:107)y − OS(r)(Π)y(cid:107)2 + λ|Π|(cid:1).

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

10

CHATTERJEE, S. AND GOSWAMI S.

Fig 1. Figure (a) is an example of a recursive dyadic partition of the square. Figure (b) is nondyadic but is
a hierarchical partition. Figure (c) is an example of a nonhierarchical partition. An easy way to see this is
that there is no split from top to bottom or left to right.

Although this is a natural extension of Dyadic CART, we are unable to pinpoint an exact
reference where this estimator has been explicitly proposed or studied in the statistics
literature. The above optimization problem is an analog of the optimal decision tree problem
laid out in Bertsimas and Dunn (2017). The diﬀerence is that Bertsimas and Dunn (2017)
is considering classiﬁcation whereas we are considering ﬁxed lattice design regression. Note
that the above optimization problem is diﬀerent from the usual pruning of a tree that is
done at the second stage of CART. Pruning can only result in subtrees of the full tree
obtained in the ﬁrst stage whereas the above optimization is over all rectangular partitions
Π ∈ Phier,d,n. We name the estimator (cid:98)θ(r)
λ,hier as Optimal Regression Tree (ORT) of order r.

rdp,λ and (cid:98)θ(r)

1.3. Both Dyadic CART and ORT of all orders are eﬃciently computable. The crucial fact
about (cid:98)θ(r)
hier,λ is that they can be computed eﬃciently and exactly using dynamic
programming approaches. A dynamic program algorithm to compute (cid:98)Πrdp,λ for d = 2 and
r = 0 was shown in Donoho (1997). This algorithm is extremely fast and can be computed
in O(N ) (linear in sample size) time. The basic idea there can actually be extended to
compute both Dyadic CART and ORT for any ﬁxed r, d with computational complexity
given in the next lemma. The proof is given in Section 8 (in the supplementary ﬁle,).

Lemma 1.1. There exists an absolute constant C > 0 such that the computational com-
plexity, i.e. the number of elementary operations involved in the computation of ORT is
bounded by:

(cid:40)

CN 2 nd,
CN 2 (nd + d3r)

for r = 0

for r ≥ 1

Similarly, the computational complexity of Dyadic CART is bounded by:

(cid:40)

C2d N d,
C2d N d3r

for r = 0

for r ≥ 1.

Remark 1.1. Since the proxy for the sample size N ≥ 2d as soon as n ≥ 2, it does not
make sense to think of d as large when reading the above computational complexity. The
lattice design setting is really meaningful when d is low to moderate and ﬁxed and the number

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

(a)(b)(c)ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

11

of samples per dimension n is growing to ∞. Thus, one should look at the dependence of
the computational complexity on N and treat the factors depending on d as constant.

Remark 1.2. Even for d = 1, the brute force computation time is exponential in N as the
total number of hierarchical partitions is exponential in N.

The rest of the paper is organized as follows. In Section 2 we state our oracle risk bound
for Dyadic CART and ORT of all orders. Section 3 describes applications of the oracle
risk bound for ORT to multivariate piecewise polynomials. In Sections 4 and 5 we state
applications of the oracle risk bound for Dyadic CART to multivariate bounded variation
functions in general dimensions and univariate bounded variation function classes of all
orders respectively. In Section 6 we describe our simulation studies and in Section 7, we
summarize our results, reiterate the main contributions of this paper and discuss some
matters related to our work here. Section 8 contains all the proofs of our results. In Section 9
we state and prove some auxiliary results that we use for proving our main results in the
paper.

Acknowledgements: This research was supported by a NSF grant and an IDEX grant
from Paris-Saclay. S.G.’s research was carried out in part as a member of the Infosys-
Chandrasekharan virtual center for Random Geometry, supported by a grant from the
Infosys Foundation. We thank the anonymous referees for their numerous helpful remarks
and suggestions on an earlier manuscript of the paper. We also thank Adityanand Gun-
tuboyina for many helpful comments. The project started when SG was a postdoctoral
fellow at the Institut des Hautes ´Etudes Scientiﬁques (IHES).

2. Oracle risk bounds for Dyadic CART and ORT.
In this section we describe an
oracle risk bound. We have to set up some notations and terminology ﬁrst. Let S be any
ﬁnite collection of subspaces of RN . Recall that for a generic subspace S ∈ S, we denote its
dimension by Dim(S). For any given θ ∈ RN let us deﬁne

(2.1)

kS(θ) = min{Dim(S) : S ∈ S, θ ∈ S}

where we adopt the convention that the inﬁmum of an empty set is ∞.

For any θ ∈ RN , the number kS(θ) can be thought of as describing the complexity of θ
with respect to the collection of subspaces S. Recall the deﬁnition of the nested classes of
rectangular partitions Prdp,d,n ⊂ Phier,d,n ⊂ Pall,d,n. Also recall that the subspace S(r)(Π)
denotes all arrays which are degree r polynomial on each of the rectangles constituting
Π. For any integer r ≥ 0, these classes of partitions induce their respective collection of
subspaces of RN deﬁned as follows:

a = {S(r)(Π) : Π ∈ Pa,d,n}
S (r)

where a ∈ {rdp, hier, all}. For any θ ∈ RLd,n and any integer r ≥ 0 let us observe its

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

12

CHATTERJEE, S. AND GOSWAMI S.

complexity with respect to the collection of subspaces S(r)
a

is

k

S(r)
a

(θ) = k(r)

a (θ)

where again a ∈ {rdp, hier, all}. Here k(r)
use both notations interchangeably.

all (θ∗) is the same as k(r)(θ∗) deﬁned earlier and we

It is now clear that for any θ ∈ RN we have

(2.2)

all (θ) ≤ k(r)
k(r)

hier(θ) ≤ k(r)

rdp(θ).

We are now ready to an oracle risk bound for all the three estimators (cid:98)θ(r)
The theorem is proved in Section 8.

all,λ, (cid:98)θ(r)

rdp,λ and (cid:98)θ(r)

hier,λ.

(cid:1) was deﬁned earlier.
Theorem 2.1. Fix any integer r ≥ 0 and recall that Kr,d = (cid:0)r+d−1
There exists an absolute constant C > 0 such that for any 0 < δ < 1 if we set λ ≥
, then we have the following risk bounds for a ∈ {rdp, hier, all},
CKr,d

d−1

σ2 log N
δ

E(cid:107)(cid:98)θ(r)

a,λ − θ∗(cid:107)2 ≤ inf
θ∈RN

(cid:2) (1 + δ)
(1 − δ)

(cid:107)θ − θ∗(cid:107)2 +

λ
1 − δ

a (θ)(cid:3) + C
k(r)

σ2
δ (1 − δ)

Let us now discuss about some aspects of the above theorem.

rdp,λ and the ORT estimator (cid:98)θ(r)

all,λ, the Dyadic CART estimator (cid:98)θ(r)

• Theorem 2.1 gives an oracle risk bound for all the three estimators: the fully penalized
LSE (cid:98)θ(r)
hier,λ of all
orders r ≥ 0 in all dimensions d. An upper bound on the risks of these estimators
is given by an oracle risk bound trading oﬀ squared error distance and complexity.
Such a result already appeared in (Donoho, 1997, Theorem 7.2) for the Dyadic CART
estimator in the special case when r = 0 and d = 2 with an additional multiplicative
log N factor in front of the squared distance term. Such an oracle risk bound also
appeared in Nowak et al. (2004) (see equation 8) where an upper bound to the MSE
in terms of approximation error plus estimation error is given. The main points we
want to make by proving the above theorem are that ﬁrstly, it continues to hold for
Dyadic CART and ORT of all orders and in all dimensions. Secondly, this oracle
inequality potentially implies tight bounds for several classes of functions of recent
interest.

• Oracle risk bounds for Dyadic CART type estimators have also been shown for clas-
siﬁcation problems; see, e.g. Blanchard et al. (2007) and Scott and Nowak (2006).

• Due to the inequality in (2.2), the risk bounds are ordered pointwise in θ∗. The risk
bound is best for the fully penalized LSE followed by ORT and then followed by Dyadic
CART. However, the Dyadic CART is the cheapest to compute followed by ORT in
terms of the number of basic operations needed for computation. The computation of

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

13

the fully penalized LSE, of course, is out of the question. Thus, our risk bounds hint
at a natural trade-oﬀ between statistical risk computational complexity.

• Gaussian nature of the noise Z is not essential in the above theorem. The conclusion
of the theorem holds as long as the entries of Z are independent and are sub gaussian
random variables. For the sake of clean exposition, we prove the theorem and its
applications in the next section, for the Gaussian case only. In fact, the assumption
of independence of Gaussian noise can also be relaxed as discussed in Section 7.3.

• The proof (done in Section 8) of the above theorem uses relatively standard techniques
from high dimensional statistics and is similar to the usual proof of the (cid:96)0 penalized
BIC estimator achieving fast rates in high dimensional linear regression (see, e.g,
Theorem 3.14 in Rigollet and H¨utter (2015)). As is probably well known, sharp oracle
inequality with 1+δ
1−δ replaced by 1 seems unreachable with the current proof technique.
Just to reiterate, the point of proving this theorem is to recognize that such a result
would imply near optimal results for function classes that are of interest in this current
manuscript.

• Operationally, to derive risk bounds for Dyadic CART or ORT for some function
class, Theorem 2.1 behooves us to use approximation theoretic arguments. Precisely,
for a given generic θ∗ in the function class, one needs to understand what is the
approximation error in the Euclidean sense, if the approximator θ is constrained to
satisfy krdp(θ) = k or khier(θ) = k for any given integer k. One of the technical
contributions of this paper lie in addressing this approximation theoretic question for
the three classes of functions considered in this paper.

• The risk bounds in Theorem 2.1 as well as the algorithms for computing our estimators
(see Section 8.1 in the supplementary ﬁle) can be adapted for any subspace, not only
the subspace of degree r polynomials. As long as we consider partitions consisting of
axis aligned rectangles, we can choose any subspace of functions to be ﬁtted within
each rectangle of the partition. Polynomials are one of the most natural and classical
subspace of functions which is why we wrote our results for polynomials.

3. Results for Multivariate Piecewise Polynomial Functions.

3.1. The main question. For a given underlying truth θ∗, the oracle estimator (cid:98)θ(r)
(oracle) —
which knows the minimal rectangular partition (R1, . . . , Rk) of θ∗ exactly — has a simple
form. In words, within each rectangle Ri, it estimates θ∗
by the best ﬁtting r-th degree
Ri
polynomial in the least squares sense. It is not hard to check that

MSE((cid:98)θ(r)

(oracle), θ∗) ≤ Kr,d σ2 k(r)

all (θ∗)
N

.

Thus, for any ﬁxed d and r, the MSE of the oracle estimator scales like the number of pieces
k(r)
all (θ∗) divided by the sample size N which is precisely the parametric rate of convergence.
Furthermore, we can show the following minimax lower bound holds.

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

14

CHATTERJEE, S. AND GOSWAMI S.

Lemma 3.1. Fix any positive integers n, d. Fix any integer k such that 3d ≤ k ≤ N = nd
and let Θk,d,n := {θ ∈ RLd,n : k(0)
hier(θ) ≤ k}. There exists a universal constant C such that
the following inequality holds:

inf
(cid:101)θ

sup
θ∈Θk,d,n

E(cid:107)(cid:101)θ − θ(cid:107)2 ≥ C σ2k log

eN
k

.

Here the inﬁmum is over all estimators (cid:101)θ which are measurable functions of the data array
y.

Remark 3.1. For any r ≥ 1, since k(0)
bound also holds for the parameter space {θ ∈ RLd,n : k(r)

hier(θ) ≥ k(0)

all (θ) ≥ k(r)

all (θ) ≤ k}.

all (θ) the same minimax lower

The above minimax lower bound shows that any estimator must incur MSE (in the worst
case) which is the oracle MSE multiplied by an extra log(eN/k) factor. In particular, if
k = o(N ), which is the interesting regime, the extra log N factor is inevitable. We call this
O(cid:0) k

N log(eN/k)(cid:1) rate the minimax rate from here on.

We provide the proof of Lemma 3.1 in Section 8.8. We now ask the following question for
every ﬁxed dimension d and degree r.

Q: Does there exist an estimator which

• attains the minimax rate MSE scaling like O(cid:0)σ2 k(r)

all (θ∗)
N

log N (cid:1) for all θ∗ adaptively,

and

• is possible to compute in polynomial time in the sample size N = nd?

To the best of our knowledge, the above question relating to computationally eﬃcient min-
imax adaptive estimation of piecewise polynomial functions in multivariate settings, even
for piecewise constant functions in the planar case (i.e. r = 0, d = 2), has not been rigor-
ously answered in the statistics literature. The fully penalized least squares estimator (cid:98)θ(r)
all,λ
is naturally suited for our purpose but is likely to be computationally infeasible. The goal
of this section is to show that

• In the two dimensional setting, i.e. d = 2, the ORT estimator attains the minimax
MSE rate adaptively for any truth θ∗. The ORT attains this minimax rate even if the
true underlying rectangular partition is not hierarchical. In particular, we show that
the ORT incurs the oracle MSE with the exponent of log N equalling 1 thus matching
the minimax lower bound in Lemma 3.1 up to constant factors.

• When d > 2, as long as the true underlying rectangular partition satisﬁes natural
regularity conditions such as being hierarchical or fat (deﬁned later in this section),
the ORT estimator continues to attain this minimax rate.

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

15

We prove these results by combining Theorem 2.1 with existing results in computational
geometry. To the best of our knowledge, our results in this section are the ﬁrst of their type.

3.2. Review.
In this section, we review existing results pertaining to our question of attain-
ing the near minimax rate risk in the univariate setting d = 1. Note that in the univariate
setting, the estimator (cid:98)θ(r)
all,λ coincides with the ORT estimator of order r as all univariate
rectangular partitions are hierarchical. Let us ﬁrst discuss the r = 0 case where we are
ﬁtting piecewise constant functions.

In the remainder of the paper the constant involved in O(·) may depend on r and d unless
speciﬁcally mentioned otherwise. Also to lighten the notation, we use ‹O(·) for O(·)poly(log N ).

The univariate ORT estimator was rigorously studied in Boysen et al. (2009) where it is
called the Potts minimizer. This is because the objective function deﬁning the estimator
arises in the Potts model in statistical physics; see Wu (1982). Furthermore, this estimator
can be computed in O(n3) time by dynamic programming as shown in Winkler and Liebscher
(2002). This estimator can be thought of as a (cid:96)0 penalized least squares estimator as it
penalizes k(0)(θ) which is same as the (cid:96)0 norm of the diﬀerence vector Dθ = (θ2−θ1, . . . , θn−
θn−1). It is known that this estimator, properly tuned, indeed nearly attains the minimax
rate risk (for instance, this will be implied by Theorem 2.1).

The other approach is to consider the corresponding (cid:96)1 penalized estimator,

(cid:98)θ(cid:96)1,λ = argmin
θ∈RLn,1

(cid:0)(cid:107)y − θ(cid:107)2 + λ(cid:107)Dθ(cid:107)1

(cid:1).

The above estimator is known as univariate Total Variation Denoising (TVD) estimator and
sometimes also as Fused Lasso in the literature. This estimator is eﬃciently computable as
this is a convex optimization problem. Recent results in Guntuboyina et al. (2020), Dalalyan
et al. (2017) and Ortelli and van de Geer (2018) have shown that, when properly tuned, the
above estimator is also capable of attaining the minimax rate risk under some minimum
length assumptions on θ∗ (see Section 5.0.2 for details).

To generalize the second approach to the multivariate setting, it is perhaps natural to con-
sider the multivariate Total Variation Denoising (TVD) estimator (see H¨utter and Rigollet
(2016), Sadhanala et al. (2016)). However, in a previous manuscript of the authors, it has
been shown that there exists a θ∗ ∈ L2,n with k(0)(θ∗) = 2 such that the risk of the ideally
tuned constrained TVD estimator is lower bounded by CN −3/4, see Theorem 2.3 in Chat-
terjee and Goswami (2019). Thus, even for d = 2, the TVD estimator cannot attain the
‹O( k(θ)
N ) rate of convergence in general. This fact makes us forego the (cid:96)1 penalized approach
and return to (cid:96)0 penalized least squares.

Coming to the general r ≥ 1 case, the literature on ﬁtting piecewise polynomial functions is
diverse. Methods based on local polynomials and spline functions abound in the literature.
However, in general dimensions, we are not aware of any rigorous results of the precise nature

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

16

CHATTERJEE, S. AND GOSWAMI S.

we desire. In the univariate case, there is a family of computationally eﬃcient estimators
which ﬁts piecewise polynomials and attain our goal of nearly achieving the oracle risk as
stated before. This family of estimators is known as trend ﬁltering— ﬁrst proposed in Kim
et al. (2009) and its statistical properties analyzed in Tibshirani (2014). Trend ﬁltering is a
higher order generalization of the univariate TVD estimator which penalizes the (cid:96)1 norm of
higher order derivatives. A continuous version of these estimators, where discrete derivatives
are replaced by continuous derivatives, was proposed much earlier in the statistics literature
by Mammen and van de Geer (1997) under the name locally adaptive regression splines.
The desired risk adaptation property (of any order r) was established in Guntuboyina et al.
(2020), where it was shown that trend ﬁltering (of order r) attains the minimax rate risk
whenever the underlying truth θ∗ is a discrete spline and it satisﬁes some minimum length
assumptions. See Section 5.0.2 for a more detailed discussion. However, to the best of our
knowledge, such bounds are not available in dimension 2 and above. To summarize this
section we can say that beyond the univariate case, our question does not appear to have
been answered. Our goal here is to start ﬁlling this gap in the literature.

Remark 3.2. Although the ﬁxed lattice design setting is commonly studied, recall that in
this setting the sample size N ≥ 2d whenever n ≥ 2. In other words, N is necessarily growing
exponentially with d. Thus, the results in this paper are really meaningful in the asymptotic
regime where d is some ﬁxed ambient dimension and n → ∞. Practically speaking, our
algorithms and the statistical risk bounds we will present in this manuscript are meaningful
for low to moderate dimensions d. Even when d = 2, the question we posed about attaining
the minimax rate risk adaptively for all truths in a computationally feasible way seems a
nontrivial problem.

3.3. Our Results for ORT. Recall that Kr,d is the dimension of the subspace of d dimen-
sional polynomials with degree at most r ≥ 0. An immediate corollary of Theorem 2.1 is
the following.

Corollary 3.2. There exists an absolute constant C > 0 such that by setting λ =
CKr,d σ2 log N we have the following risk bound,

M SE((cid:98)θ(r)

hier,λ, θ∗) ≤

C Kr,d σ2 log N
N

k(r)
hier(θ∗) +

C σ2
N

.

hier(θ∗)
N

Let us discuss some implications of the above corollary. For ORT of order r ≥ 0, a risk
bound scaling like O( k(r)
log N ) is guaranteed for all θ∗. Thus, for instance, if the true
θ∗ is piecewise constant/linear on some arbitrary unknown hierarchical partition of Ld,n,
the corresponding ORT estimator of order 0, 1 respectively achieves the (near) minimax
risk O( k(r)
log N ). Although this result is an immediate implication of Theorem 2.1,
this is the ﬁrst such risk guarantee established for a computationally eﬃcient decision tree
estimator in general dimensions as far as we are aware of.

all (θ∗)
N

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

17

At this point, let us recall that our target is to achieve the ideal upper bound ‹O( k(r)
all (θ∗)
N ) to
the MSE for all θ∗ which is attained by the fully penalized LSE. However, it is perhaps not
eﬃciently computable. The best upper bound to the MSE we can get for a computationally
eﬃcient estimator is ‹O( k(r)

) which is attained by the ORT estimator.

hier(θ∗)
N

A natural question that arises at this point is how much worse is the upper bound for
ORT than the upper bound for the fully penalized LS estimator given in Theorem 2.1.
Equivalently, we know that k(r)
hier(θ∗) in general, but how large can the gap be?
There deﬁnitely exist partitions which are not hierarchical, i.e. that is Phier,d,n is a strict
subset of Pall,d,n as shown in Figure 1.

all (θ∗) ≤ k(r)

In the next section we explore general and possibly nonhierarchical partitions of Ld,n and
state several results which basically imply that ORT incurs MSE at most a constant fac-
tor more than the ideal fully penalized LSE for several natural instances of rectangular
partitions.

3.3.1. Arbitrary partitions. The risk bound for ORT in Theorem 2.1 is in terms of khier(θ∗).
We would like to convert it into a risk bound involving kall(θ∗). A natural way of doing this
would be to reﬁne an arbitrary partition into a hierarchical partition and then count the
number of extra rectangular pieces that arises as a result of this reﬁnement. This begs the
following question of a combinatorial ﬂavour.

Can an arbitrary partition of Ld,n be reﬁned into a hierarchical partition without increasing
the number of rectangles too much?.

Fortunately, the above question has been studied a fair bit in the computational/combinatorial
geometry literature under the name of binary space partitions. A binary space partition
(BSP) is a recursive partitioning scheme for a set of objects in space. The goal is to par-
tition the space recursively until each smaller space contains only one/few of the original
objects. The main questions of interest are, given the set of objects, the minimal cardi-
nality of the optimal partition and an eﬃcient algorithm to compute it. A nice survey of
this area, explaining the central questions and an overview of known results can be found
in T´oth (2005). We will now leverage some existing results in this area which would yield
corresponding risk bounds with the help of Theorem 2.1.

For d = 2, it turns out that any rectangular partition can be reﬁned into a hierarchical one
where the number of rectangular pieces at most doubles. The following proposition is due
to Berman et al. (2002) and states this fact.

Proposition 3.3 (Berman et al. (2002)). Given any partition Π ∈ Pall,2,n there exists a
reﬁnement (cid:101)Π ∈ Phier,2,n such that |(cid:101)Π| ≤ 2|Π|. As a consequence, for any matrix θ ∈ Rn×n
and any nonnegative integer r, we have

hier(θ) ≤ 2k(r)
k(r)

all (θ).

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

18

CHATTERJEE, S. AND GOSWAMI S.

The above proposition applied to Theorem 2.1 immediately yields the following theorem:

Theorem 3.4. Let d = 2. There exists an absolute constant C such that by setting λ =
C Kr,d σ2 log N we have the following risk bound for (cid:98)θhier,λ:

MSE((cid:98)θ(r)

hier,λ, θ∗) ≤

C Kr,d σ2 log N
N

k(r)
all (θ∗) +

C σ2
N

.

Remark 3.3. Thus, in the two dimensional setting d = 2, ORT fulﬁlls the two objectives
of computability in polynomial time and attaining the minimax risk rate adaptively for all
truths θ∗. Thus, this completely solves the main question we posed in the two dimensional
case. To the best of our knowledge, this is the ﬁrst result of its kind in the literature.

For dimensions higher than 2; the best result akin to Proposition 3.3 that is available is due
to Hershberger et al. (2005).

Proposition 3.5 ( Hershberger et al. (2005)). Let d > 2. Given any partition Π ∈ Pall,d,n
d+1
3 . As a consequence, for any
there exists a reﬁnement (cid:101)Π ∈ Phier,d,n such that |(cid:101)Π| ≤ |Π|
array θ ∈ RLd,n and any nonnegative integer r, we have

hier(θ) ≤ (cid:0)k(r)
k(r)

all (θ)(cid:1) d+1
3 .

Remark 3.4. A matching lower bound is also given in Hershberger et al. (2005) for the
case d = 3. Thus, to reﬁne a rectangular partition (of k pieces) into a hierarchical one, one
necessarily increases the number of rectangular pieces to O(k4/3) in the worst case.

The above result suggests that for arbitrary partitions in d dimensions, our current approach
will not yield the near minimax rate of convergence. Nevertheless, we state our risk bound
that is implied by Proposition 3.5.

Theorem 3.6. Let d > 2. There exists an absolute constant C such that by setting λ ≥
C Kr,d σ2 log N we have the following risk bound for (cid:98)θhier,λ:

MSE((cid:98)θ(r)

hier,λ, θ∗) ≤ λ

(cid:0)k(r)

all (θ∗)(cid:1) d+1
N

3

+

C σ2
N

.

all (θ∗)
N

Our approach of reﬁning an arbitrary partition into a hierarchical partition does not seem
to yield the ‹O(cid:0)σ2 k(r)
(cid:1) rate of convergence for ORT in dimension higher than 2 when the
truth is a piecewise polynomial function on an arbitrary rectangular partition. Rectangular
partitions in higher dimensions could be highly complex; with some rectangles being very
“skinny” in some dimensions. However, it turns out that if we rule out such anomalies, then
it is still possible to attain our objective. Let us now deﬁne a class of partitions which rules
out such anomalies.

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

19

Let R be a rectangle deﬁned as R = Πd
i=1[ai, bi] ⊂ Ld,n. Let the sidelengths of R be deﬁned
as ni = bi − ai + 1 for i ∈ [d]. Deﬁne its aspect ratio as A(R) = max{ ni
: (i, j) ∈ [d]2}. For
nj
any α ≥ 1, let us call a rectangle α fat if we have A(R) ≤ α. Now consider a rectangular
partition Π ∈ Pall,d,n. We call Π an α fat partition if each of its constituent rectangles is α
fat. Let us denote the class of α fat partitions of Ld,n as Pfat(α),d,n. As before, we can now
deﬁne the class of subspaces S(r)
fat(α),d,n corresponding to the set of partitions Pfat(α),d,n. For
any array θ∗ and any integer r > 0 we can also denote

k(r)
fat(α)(θ∗) = k

(θ∗).

S(r)
fat(α),d,n

An important result in the area of binary space partitions is that any fat rectangular
partition of Ln,d can be reﬁned into a hierarchical one with the number of rectangular
pieces inﬂated by at most a constant factor. This is the content of the following proposition
which is due to de Berg (1995).

Proposition 3.7 (de Berg (1995)). There exists a constant C(d, α) ≥ 1 depending only
on d and α such that any partition Π ∈ Pfat(α),d,n can be reﬁned into a hierarchical partition
(cid:101)Π ∈ Phier,d,n satisfying

|(cid:101)Π| ≤ C(d, α)|Π|.

Equivalently, for any θ ∈ RLn,d and any non negative integer r we have

hier(θ) ≤ C(d, α)k(r)
k(r)

fat(α)(θ).

The above proposition gives rise to a risk bound for ORT in all dimensions.

Theorem 3.8. For any dimension d there exists an absolute constant C such that by
setting λ ≥ C Kr,d σ2 log n we have the following risk bound for (cid:98)θhier,λ:

E(cid:107)(cid:98)θ(r)

hier,λ − θ∗(cid:107)2 ≤ inf

θ∈RLn,d

(cid:0)2 (cid:107)θ − θ∗(cid:107)2 + λ C(d, α) k(r)

fat(α)(θ)(cid:1) + C σ2.

Remark 3.5. For any ﬁxed dimension d, when θ∗ is piecewise polynomial of degree r on a
fat paritition, the above theorem implies a O(cid:0)σ2 k(r)
log N (cid:1) bound to the MSE of the ORT
estimator (of order r). Thus, for arbitrary fat partitions in any dimension, ORT attains
our objective of enjoying the near minimax rate of convergence and being computationally
eﬃcient. For any ﬁxed dimension d, this is the ﬁrst result of its type that we are aware of.

all (θ∗)
N

Remark 3.6.
It should be mentioned here that the constant C(d, α) scales exponentially
with d, at least in the construction which is due to de Berg (1995). In any case, recall that
all of our results are meaningful when d is low to moderate.

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

20

CHATTERJEE, S. AND GOSWAMI S.

In the previous section, we showed that the ORT
3.4. Our Results for Dyadic CART.
estimator attains the desired ‹O(cid:0)σ2 k(r)
(cid:1) rate for all θ∗ adaptively in dimensions d = 1, 2
and for all θ∗ which are piecewise polynomial on a fat partition in all dimensions d > 2.
Since the ORT is more computationally expensive than Dyadic CART, a natural question
is whether there are analogous results for Dyadic CART. In this case, the relevant question
is

all (θ∗)
N

Can an arbitrary nonhierarchical partition of Ld,n be reﬁned into a recursive dyadic partition
without increasing the number of rectangles too much?.

When d = 1 or d = 2, we can give an argument to show there exists a recursive dyadic
partition reﬁning a given arbitrary rectangular partition with number of rectangles being
multiplied by a log factor. This is the content of our next result which is proved in Section 8.3.

Proposition 3.9. Given any positive integer n and given a partition Π ∈ Pall,1,n with k
intervals, there exists a reﬁnement (cid:101)Π ∈ Prdp,1,n which is a recursive dyadic partition with
at most Ck log(en/k) intervals where C > 0 is an universal constant. Equivalently, for all
θ ∈ RL1,n and all non negative integers r, we have

(3.1)

rdp(θ) ≤ Ck(r)
k(r)

all (θ) log

en
k(r)
all (θ)

.

Moreover, given any positive integer n and an arbitrary partition Π ∈ Pall,2,n of L2,n with
k rectangles there exists a reﬁnement Π(cid:48) ∈ Prdp,2,n which is a recursive dyadic partition
with at most Ck(log n)2 rectangles where C is a universal constant. Equivalently, for all
θ ∈ RL2,n and all non negative integers r, we have

(3.2)

rdp(θ) ≤ C( log n)2k(r)
k(r)

all (θ).

We have not seen the above result (equation (3.2)) stated explicitly in the Statistics lit-
erature. It is probable that this result is known in the combinatorics or computational
geometry literature. However, since we could locate an exact reference, we provide its proof
in Section 8.3.

Remark 3.7. The exponent of log n, which is 1 for d = 1 and 2 for d = 2, cannot be
improved in general. It is now natural to conjecture that a result like above is true for a
general d where the exponent of log n is d. However, we do not know whether this is true
or not. Our current proof for the d = 2 case breaks down and cannot be extended to higher
dimensions. See Remark 8.3 for more explanations on this.

The implication of Proposition 3.9 is the following corollary for Dyadic CART.

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

21

Corollary 3.10. For d = 1 and any integer n, there exists a universal constant C > 0
such that by setting λ = CKr,1 σ2 log n we have the following risk bound,
rdp,λ, θ∗) ≤ CKr,1σ2 k(r)

M SE((cid:98)θ(r)

log n +

log

.

C σ2
N

all (θ∗)
N

n
k(r)
all (θ)

For d = 2 and any integer n, there exists a universal constant C > 0 such that by setting
λ = CKr,2 σ2 log n we have the following risk bound,
rdp,λ, θ∗) ≤ CKr,2 σ2 k(r)

M SE((cid:98)θ(r)

(log N )3 +

.

all (θ∗)
N

C σ2
N

To summarize, Dyadic CART attains the same rate as the ORT with an extra log N factor
when d = 1 and with an extra (log N )2 factor when d = 2. We do not know whether for
d > 2, a result for Dyadic CART analogous to Theorem 3.8 for fat partitions is possible or
not.

4. Results for Multivariate Functions with Bounded Total Variation.
In this
section, we will describe an application of Theorem 2.1 to show that Dyadic CART of order
0 has near optimal (worst case and adaptive) risk guarantees in any dimension when we
consider estimating functions with bounded total variation. Let us ﬁrst deﬁne what we mean
by total variation.

Let us think of Ld,n as the d dimensional regular lattice graph. Then, thinking of θ ∈ RLd,n
as a function on Ld,n we deﬁne

(4.1)

TV(θ) =

(cid:88)

|θu − θv|

(u,v)∈Ed,n

where Ed,n is the edge set of the graph Ld,n. One way to motivate the above deﬁnition is
n , . . . , id
as follows. If we think θ[i1, . . . , in] = f ( i1
n ) for a diﬀerentiable function f : [0, 1]d →
R then the above deﬁnition divided by nd−1 is precisely the Reimann approximation for
(cid:82)
[0,1]d (cid:107)∇f (cid:107)1. Of course, the deﬁnition in (4.1) applies to arbitrary arrays, not just for
evaluations of a diﬀerentiable function on the grid.

The usual way to estimate functions/arrays with bounded total variation is to use the Total
Variation Denoising (TVD) estimator deﬁned as follows:

(cid:98)θλ = argmin
θ∈RLd,n

(cid:0)(cid:107)y − θ(cid:107)2 + λTV(θ)(cid:1).

This estimator was ﬁrst introduced in the d = 2 case by Rudin et al. (1992) for image
denoising. This estimator is now a standard and widely succesful technique to do image
denoising. In the d = 1 setting, it is known (see, e.g. Donoho and Johnstone (1998), Mammen

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

22

CHATTERJEE, S. AND GOSWAMI S.

and van de Geer (1997)) that a well tuned TVD estimator is minimax rate optimal on the
class of all bounded variation signals {θ : TV(θ) ≤ V } for V > 0. It is also known (e.g,
see Guntuboyina et al. (2020), Dalalyan et al. (2017), Ortelli and van de Geer (2018)) that,
when properly tuned, the above estimator is capable of attaining the oracle MSE scaling
like O( k(0)

all (θ∗)
N ), up to a log factor in N.

In the multivariate setting (d ≥ 2), worst case performance of the TVD estimator has been
studied in H¨utter and Rigollet (2016), Sadhanala et al. (2016). These results show that
like in the 1D setting, a well tuned TVD estimator is nearly (up to log factors) minimax
rate optimal over the class {θ ∈ RLd,n : TV(θ) ≤ V } of bounded variation signals in any
dimension.

The goal of this section is to proclaim that the Dyadic CART estimator (cid:98)θ(0)
rdp,λ enjoys similar
statistical guarantees as the TVD estimator and possibly even has some advantages over
TVD which we list below.

• The Dyadic CART estimator (cid:98)θ(0)

rdp,λ is computable in O(N ) time in low dimensions d.
Note that TVD is mostly used for image processing in the d = 2, 3 case. Recall that
the lattice has at least 2d points as soon as n ≥ 2 so it does not make sense to think
of high d. While TVD estimator is the solution of a convex optimization procedure,
there is no known algorithm which computes it provably in O(N ) time to the best
of our knowledge. As we show in Theorem 4.1 and Theorem 4.2, the Dyadic CART
estimator is also minimax rate optimal over the class {θ ∈ RLd,n : TV(θ) ≤ V }. Thus,
the Dyadic CART estimator appears to be the ﬁrst provably linear time computable
estimator achieving the minimax rate, up to log factors, for functions with bounded
total variation.

• We also show that the Dyadic CART estimator is also adaptive to the intrinsic di-
mensionality of the true signal θ∗. We make the meaning of adapting to intrinsic
dimensionality precise later in this section. It is not known whether the TVD estima-
tor demonstrates such adaptivity.

• One corollary of Theorem 2.1 is that the Dyadic CART estimator nearly attains the
oracle risk when the truth θ∗ is piecewise constant on a recursive dyadic partition
of Ln,d. For such signals, the ideally tuned TVD estimator, even in the d = 2 case,
provably cannot attain the oracle risk for such piecewise constant signals in general;
see Theorem 2.3 in Chatterjee and Goswami (2019).

4.0.1. Adaptive Minimax Rate Optimality of Dyadic CART. We now describe risk bounds
for the Dyadic Cart estimator for bounded variation arrays. Let us deﬁne the following class
of bounded variation arrays:

Kd,n(V ) = {θ ∈ Ld,n : TV(θ) ≤ V }

For any generic subset S ⊂ [d], let us denote its cardinality by |S|. For any vector x ∈ [n]d
let us deﬁne xS ∈ [n]|S| to be the vector x restricted to the coordinates given by S. We now

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

23

deﬁne

KS

d,n(V ) = {θ ∈ Kd,n(V ) : θ(x) = θ(y) ∀x, y ∈ [n]d with xS = yS}

In words, KS
n,d(V ) is just the set of arrays in Kd,n(V ) which are a function of the coordinates
within S only. In this section, we will show that the Dyadic CART estimator is minimax
rate optimal (up to log factors) over the parameter space KS
d,n(V ) simultaneously over all
subsets S ⊂ [d]. This means that the Dyadic CART performs as well as an oracle estimator
which knows the subset S. This is what we mean when we say that the Dyadic CART
estimator adapts to intrinsic dimensionality. To the best of our knowledge, such an oracle
property in variable selection is rare in Non Parametric regression. The work in Bertin
and Lecu´e (2008) shows a two step procedure for adapting to instrinsic dimensionality for
multivariate Holder smooth function classes. The only comparable result that we are aware
of for a spatially heterogenous function class is Theorem 3 in Deng and Zhang (2018) which
proves a similar adaptivity result in multivariate isotonic regression.

S ∈ RLs,n. It is easy to check that θ∗

nd−s . Estimating θ∗ is equivalent to estimating the s dimensional array θ∗

Fix a subset S ⊂ [d] and let s = |S|. Consider our Gaussian mean estimation problem
d,n(V ). We could think of θ∗ as nd−s
where it is known that the underlying truth θ∗ ∈ KS
copies of a s dimensional array θ∗
S ∈ Ks,n(VS) where
Vs = V
S where the
noise variance is now reduced to σ2
nd−s because we can average over nd−s elements
per each entry of θ∗
S. Therefore, we now have a reduced Gaussian mean estimation problem
where the noise variance is σ2
S and the parameter space is Kn,s(VS). A tight lower bound
to the minimax risk for the parameter space Kd,n(V ) for arbitrary n, d, V > 0 is available
in Sadhanala et al. (2016). Using the above logic and this existing minimax lower bound
allows us to establish a lower bound to the minimax risk for the parameter space KS
d,n(V ).
The detailed proof is given in Section 8.

S = σ2

Theorem 4.1 (Minimax Lower Bound over KS
S ⊂ [d] such that s = |S| ≥ 2. Let V > 0 and VS = V
There exists a universal constant c > 0 such that

d,n(V )). Fix positive integers n, d and let
nd−s .

nd−s . Similarly, for σ > 0, let σ2

S = σ2

inf
(cid:101)θ∈RLd,n

sup
d,n(V )

θ∈KS

Eθ(cid:107)(cid:101)θ − θ(cid:107)2 ≥ c nd−s min{

σS VS
2s

1 + log(

2 σ s ns
VS

), nsσ2
S,

2

VS
s

+ σ2

S}.

If |S| = 1 then

inf
(cid:101)θ∈RLd,n

sup
d,n(V )

θ∈KS

Eθ(cid:107)(cid:101)θ − θ(cid:107)2 ≥ c nd−1 min{(σ2

SVS)2/3n1/3, n σ2

S, n V 2

S }.

Let us now explain the above result. If we take the subset S = [d] this is exactly the lower
bound in Theorem 2 of Sadhanala et al. (2016). All we have done is stated the same result
for any subset S since we can reduce the estimation problem in KS
d,n(V ) to a s dimensional
estimation problem over Ks,n(VS). The bound is in terms of a minimum of three terms.
It is enough to explain this bound in the case when S = [d] as similar reasoning holds for

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

 
24

CHATTERJEE, S. AND GOSWAMI S.

any subset S with s = |S| ≥ 2. Thinking of σ as a ﬁxed constant, the three terms in the
minimum on the right side corresponds to diﬀerent regimes of V. It can be shown that the
constant array with each entry y attains the V 2 + σ2 rate which is dominant when V is very
small. The estimator y itself attains the N σ2 rate which is dominant when V is very large.
Hence, these regimes of V can be thought of as trivial regimes. In the nontrivial regime, the
lower bound is c min{ σ V
2d

1 + log( 2 σ d N

)}.

»

V

It is also known that a well tuned TVD estimator is minimax rate optimal, in the nontrivial
regime, over Kd,n(V ) for all d ≥ 2, up to log factors; see H¨utter and Rigollet (2016). For
instance, it achieves the above minimax lower bound (up to log factors) in the nontrivial
regime. For this reason, we can deﬁne an oracle estimator (which knows the set S) attaining
the minimax lower bound over KS
d,n(V ) in Theorem 4.1, up to log factors. The oracle
estimator would ﬁrst obtain yS by averaging the observation array y over the coordinates
in SC and then it would apply the s dimensional TVD estimator on yS. Our main point
here is that the Dyadic CART estimator performs as well as this oracle estimator, without
the knowledge of S. In other words, its risk nearly (up to log factors) matches the minimax
lower bound in Theorem 4.1 adaptively over all subsets S ⊂ [d]. This is the content of our
next theorem which is proved in Section 8 (in the supplementary ﬁle).

Theorem 4.2 (Adaptive Risk Bound for Dyadic Cart). Fix any positive integers n, d.
Let θ∗ ∈ KS
d,n(∞) be the underlying truth where S ⊂ [d] is any subset with |S| ≥ 2. Let
S = σ2
V ∗ = TV(θ∗). Let V ∗
nd−s be deﬁned as before. The following risk bound
holds for the Dyadic CART estimator (cid:98)θ(0)
rdp,λ with λ ≥ Cσ2 log N where C is an absolute
constant.

nd−s and σ2

S = V ∗

Eθ∗(cid:107)(cid:98)θ(0)

rdp,λ − θ∗(cid:107)2 ≤ C nd−s min{σSV ∗

S log N, σ2

S log N, (cid:0)(V ∗

S )2 + σ2
S

(cid:1)}

In the case |S| = 1 we have

Eθ∗(cid:107)(cid:98)θ(0)

rdp,λ − θ∗(cid:107)2 ≤ C nd−1 min{(σ2

SVS log N )2/3n1/3, n σ2

S log N, n V 2

S + σ2

S log N }

We think the following is an instructive way to read oﬀ the implications of the above
theorem. Let us consider d ≥ 2 and the S = [d] case. We will only look at the nontrivial
regime even though Dyadic CART remains minimax rate optimal, up to log factors, even
in the trivial regimes. In this case, M SE((cid:98)θ(0)
N ) which is the minimax rate
in the nontrivial regime as given by Theorem 4.1. Now, for many natural instances of θ∗,
the quantity V ∗ = O(nd−1); for instance if θ∗ are evaluations of a diﬀerentiable function
on the grid. This O(nd−1) scaling was termed as the canonical scaling for this problem
by Sadhanala et al. (2016). Therefore, under this canonical scaling for V ∗ we have

rdp,λ, θ∗) = ‹O( σV ∗

M SE((cid:98)θ(0)

rdp,λ, θ∗) = ‹O(

σ
n

) = ‹O(

σ
N 1/d

).

Now let us consider d ≥ 2 and a general subset S ⊂ [d]. In the nontrivial regime, by
Theorem 4.2 we have M SE((cid:98)θ(0)
ns ) which is also the minimax rate over the

rdp,λ, θ∗) = ‹O( σS V ∗

S

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

25

parameter space KS
under this canonical scaling we can write

d,n. Now, V ∗

S = O(ns−1) under the canonical scaling in this case. Thus,

M SE((cid:98)θ(0)

rdp,λ, θ∗) = ‹O(

σS
n

) = ‹O(

σS
N 1/d

).

This is very similar to the last display except σ has been replaced by σS, the actual standard
deviation of this problem. The point is, the Dyadic CART attains this rate without knowing
S. The case when |S| = 1 can be read oﬀ in a similar way.

In
5. Results for Univariate Functions of Bounded Variation of Higher Orders.
this section, we show another application of Theorem 2.1 to a family of univariate function
classes which have been of recent interest. The results in this section would be for the
univariate Dyadic Cart estimator of some order r ≥ 0. As mentioned in Section 1, TV
denoising in the 1D setting has been studied as part of a general family of estimators
which penalize discrete derivatives of diﬀerent orders. These estimators have been studied
in Mammen and van de Geer (1997) , Steidl et al. (2006), Tibshirani (2014), Guntuboyina
et al. (2020) and Kim et al. (2009) who coined the name trend ﬁltering.

To deﬁne the trend ﬁltering estimators here, we ﬁrst need to deﬁne variation of all orders.
For a vector θ ∈ Rn, let us deﬁne D(0)(θ) = θ, D(1)(θ) = (θ2 −θ1, . . . , θn −θn−1) and D(r)(θ),
for r ≥ 2, is recursively deﬁned as D(r)(θ) = D(1)(D(r−1)(θ)). Note that D(r)(θ) ∈ Rn−r.
For simplicity, we denote the operator D(1) by D. For any positive integer r ≥ 1, let us also
deﬁne the r th order variation of a vector θ as follows:

(5.1)

V (r)(θ) = nr−1|D(r)(θ)|1

where |.|1 denotes the usual (cid:96)1 norm of a vector. Note that V (1)(θ) is the usual total variation
of a vector as deﬁned in (4.1).

Remark 5.1. The nr−1 term in the above deﬁnition is a normalizing factor and is written
following the convention adopted in Guntuboyina et al. (2020). If we think of θ as evaluations
of a r times diﬀerentiable function f : [0, 1] → R on the grid (1/n, 2/n . . . , n/n) then the
Reimann approximation to the integral (cid:82)
[0,1] f (r)(t)dt is precisely equal to V (r)(θ). Here f (r)
denotes the rth derivative of f. Thus, for natural instances of θ, the reader can imagine that
V (r) = O(1).

Let us now deﬁne the following class of sequences for any integer r ≥ 1,

(5.2)

BV (r)

n (V ) = {θ ∈ Rn : V (r)(θ) ≤ V }.

Trend Filtering (of order r ≥ 1) estimators are deﬁned as follows for a tuning parameter
λ > 0:

(cid:98)θ(r)
tf,λ = argmin
θ∈Rn

(cid:0)(cid:107)y − θ(cid:107)2 + λV (r)(θ)(cid:1).

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

26

CHATTERJEE, S. AND GOSWAMI S.

Thus, Trend Filtering is penalized least squares where the penalty is proportional to the (cid:96)1
norm of D(r)(θ). As opposed to Trend Filtering, here we will study the univariate Dyadic
CART estimator (of order r−1) which penalizes something similar to the (cid:96)0 norm of D(r)(θ).
We will show that Dyadic CART (of order r − 1) compares favourably with Trend Filtering
(of order r) in the following aspects:

• For a given constant V > 0 and r ≥ 1, n−2r/2r+1 rate is known to be the minimax
rate of estimation over the space BV (r)
n (V ); (see e.g, Donoho and Johnstone (1994)).
A standard terminology in this ﬁeld terms this n−2r/2r+1 rate as the slow rate. It
is also known that a well tuned Trend Filtering estimator is minimax rate optimal
over the parameter space BV (r)
n (V ) and thus attains the slow rate. This result has
been shown in Tibshirani (2014) and Wang et al. (2014) building on earlier results
by Mammen and van de Geer (1997). In Theorem 5.1 we show that Dyadic CART
estimator of order r − 1 is also near minimax rate optimal (up to a log factor) over
the space BV (r)

n (V ) and attains the slow rate.

• It is also known that an ideally tuned Trend Filtering (of order r) estimator can adapt
to (cid:107)Dr(θ)(cid:107)0, the number of jumps in the r th order diﬀerences, under some assump-
tions on θ∗. Such a result has been shown in Guntuboyina et al. (2020) and van de
Geer and Ortelli (2019). In this case, the Trend Filtering estimator of order r attains
the ‹O((cid:107)D(r)(θ)(cid:107)0/n) rate. Standard terminology in this ﬁeld terms this as the fast
rate. In Theorem 5.2 we show that Dyadic CART estimator of order r − 1 attains the
fast rate without any assumptions on θ∗.

• If one desires the fast rate for both piecewise constant and piecewise linear functions,
there is no way to attain this by a single Trend Filtering Estimator. One needs to use
Trend Filtering of order r = 2 to attain fast rates for piecewise linear functions and
r = 1 for piecewise constant functions respectively. In contrast, Dyadic Cart of order
1 attains the fast rate for both piecewise linear and piecewise constant functions. This
is because by our deﬁnition, a piecewise constant function is also piecewise linear. In
general, Dyadic Cart of order r attains fast rate for a piecewise polynomial of degree
r where each piece has degree ≤ r.

• To the best of our knowledge, diﬀerent tuning parameters for Trend Filtering are
needed depending on whether slow rate or fast rate is desired. Thus, technically the
estimators giving the slow rate and the fast rate are diﬀerent. In contrast, the same
tuning parameter gives both the slow rate and the fast rate for Dyadic CART.

• The univariate Dyadic CART estimator of order r ≥ 0 can be computed in linear
O(N ) time. Although Trend Filtering estimators are eﬃciently computable by convex
optimization, we are not aware of a provably O(N ) run time bound (for general r ≥ 1)
on its computational complexity.

Remark 5.2. By Theorem 2.1, the univariate ORT would also satisfy all the risk bounds
that we prove for univariate Dyadic Cart in this section. Recall that for r = 0, the ORT
is precisely the same as the fully penalized least squares estimator (cid:98)θall,λ studied in Boysen
et al. (2009). This is because Phier,n,1 coincides with Pall,1,n as all univariate partitions are
hierarchical. Since the computational complexity of univariate Dyadic Cart is O(N ) and of

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

27

univariate ORT is O(N 3) we focus on univariate Dyadic Cart.

5.0.1. Risk Bounds for Univariate Dyadic CART of all orders. We start with the bound
of n−2r/(2r+1) for the risk of Dyadic CART of order r − 1 for the parameter space BV (r)
n (V ).
We also explicitly state the dependence of the bound on V and σ.

Theorem 5.1 (Slow Rate for Dyadic CART). Fix a positive integer r. Let V r(θ∗) = V.
For the same constant C as in Theorem 2.1, if we set λ ≥ Cσ2 log n we have

(5.3)

M SE((cid:98)θ(r−1)

rdp,λ , θ∗) ≤ Cr

(cid:0) σ2V 1/r log n
n

(cid:1)2r/(2r+1) + Crσ2 log n
n

where Cr is an absolute constant only depending on r.

Remark 5.3. The proof of the above theorem is done in Section 8 (in the supplementary
ﬁle). The proof proceeds by approximating any θ ∈ BV (r)
n (V ) with a vector θ(cid:48) which is
piecewise polynomial of degree r − 1 with an appropriate bound on its number of pieces and
then invoking Theorem 2.1.

Remark 5.4. The above theorem shows that the univariate Dyadic CART estimator of
order r − 1 is minimax rate optimal up to the (log n)2r/(2r+1) factor. The dependence of V
is also optimal in the above bound. Up to the log factor, this upper bound matches the bound
already known for the Trend Filtering estimator of order r; (see e.g, Tibshirani (2014)).

Our next bound shows that the univariate Dyadic CART estimator achieves our goal of
attaining the oracle risk for piecewise polynomial signals.

Theorem 5.2 (Fast Rates for Dyadic CART). Fix a positive integer r and 0 < δ < 1. Let
V r(θ∗) = V. For the same constant C as in Theorem 2.1, if we set λ ≥ Cσ2 log n we have

E(cid:107)(cid:98)θ(r)

rdp,λ − θ∗(cid:107)2 ≤ inf
θ∈RN

(cid:2) (1 + δ)
(1 − δ)

(cid:107)θ − θ∗(cid:107)2 +

λCr
1 − δ

k(r)
all (θ) log(

en
k(r)
all (θ)

)(cid:3) + C

σ2
δ (1 − δ)

where Cr is an absolute constant only depending on r. As a corollary we can conclude that

M SE((cid:98)θ(r)

rdp,λ, θ∗) ≤ Crσ2

k(r)
all (θ∗) log n log(

n

en
k(r)
all (θ∗)

)

.

Proof. The proof follows directly from the risk bound for univariate Dyadic Cart given
in Theorem 2.1 and applying equation (3.1) in Lemma 8.3 which says that k(r)
rdp(θ) ≤
k(r)
all (θ) log(

) for all vectors θ ∈ Rn.

en
k(r)
all (θ)

Let us now put our result in Theorem 5.2 in context. It says that in the d = 1 case, Dyadic
CART achieves our goal of attaining MSE scaling like ‹O(k(r)
all (θ∗)/n) (fast rate) for all θ∗. The

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

28

CHATTERJEE, S. AND GOSWAMI S.

Trend Filtering estimator, ideally tuned, is also capable of attaining this rate of convergence;
(see Theorem 3.1 in van de Geer and Ortelli (2019) and Theorem 2.1 in Guntuboyina et al.
(2020)), under certain minimum length conditions on θ∗. Let us discuss this issue now in
more detail and compare Theorem 5.2 to the comparable result known for Trend Filtering.

5.0.2. Comparison of Fast Rates for Trend Filtering and Dyadic CART. The fast rate
results available for Trend Filtering give bounds on the MSE of the form ‹O(|D(r)(θ∗)|0/n)
where |.|0 refers to the number of nonzero elements of a vector. Now for every vector θ ∈ Rn,
|D(r)(θ)|0 = k if and only if θ equals (f (1/n), ..., f (n/n)) for a discrete spline function f that
is made of k + 1 polynomials each of degree at most r − 1. Discrete splines are piecewise
polynomials with regularity at the knots. They diﬀer from the usual (continuous) splines
in the form of the regularity condition at the knots: for splines, the regularity condition
translates to (higher order) derivatives of adjacent polynomials agreeing at the knots, while
for discrete splines it translates to discrete diﬀerences of adjacent polynomials agreeing at
the knots; see Mangasarian and Schumaker (1971) for details. This fact about the connection
between |D(r)(θ∗)|0 and discrete splines is standard (see e.g., Steidl et al. (2006)) and a proof
can be found in Proposition D.3 in Guntuboyina et al. (2020).

The above discussion then directly implies for any θ ∈ Rn and any r ≥ 1,

(5.4)

k(r−1)
all

(θ) ≤ |D(r)(θ)|0 + 1.

Therefore any bound of the form ‹O(k(r−1)
(θ∗)/n) is automatically also ‹O(|D(r)(θ∗)|0/n).
Thus, Theorem 5.2 implies that the Dyadic CART attains the fast rate whenever Trend
Filtering does so. However, as we now argue, there is a class of functions for which the
Dyadic CART attains the fast rate but Trend Filtering does not.

all

A key point is that a minimum length condition needs to hold for Trend Filtering to attain
fast rates as explained in Guntuboyina et al. (2020). For example, when r = 1, consider the
sequence of vectors in Rn of the form θ∗ = (0, . . . , 0, 1). Clearly, θ∗ is piecewise constant
with 2 pieces. However, to the best of our knowledge, the Trend Filtering estimator (with
the tuning choices proposed in the literature such as the ideal tuning for constrained version
of Trend Filtering as in Guntuboyina et al. (2020)) will not attain a ‹O(1/n) rate for this
sequence of θ∗ since it needs the length of the constant pieces to be O(n). However, the
Dyadic CART estimator does not need any minimum length condition for Theorem 5.2 to
hold and will attain the ‹O(1/n) rate for this sequence of θ∗.

Now let us come to the case when r ≥ 2. It is known that Trend Filtering of order r ﬁts
a discrete spline of degree (r − 1). Thus, if the truth is piecewise polynomial with small
number of pieces but it does not satisfy regularity conditions such as being a discrete spline,
then Trend Filtering cannot estimate well. The reason is that Trend Filtering can only ﬁt
discrete splines. On the other hand, as long as the truth is piecewise polynomial with not
too many pieces, Dyadic CART does not need the regularity conditions to be satisﬁed in

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

29

order to perform well. Let us illustrate this with a simple example in the case when r = 2.
Similar phenomena is true for higher r.

Let’s consider a discontinuous piecewise linear function f ∗ : [0, 1] → R deﬁned as follows:

f ∗(x) =

(cid:40)

x,

for x ≤ 1/2

2x for 1/2 < x ≤ 1

Let θ∗ ∈ Rn such that θ∗ = (f (1/n), . . . , f (n/n)). Clearly, k(1)
all (θ∗) = 2 and by Theorem 5.2,
the Dyadic CART estimator of order 1 attains the ‹O(1/n) rate for this sequence of θ∗. If we
check the vector D(1)(θ∗) it is of the form (a, . . . , a, b, c, . . . , c). It is piecewise constant with
three pieces. However, it does not satisfy the minimum length condition as the middle piece
has length only 1 and not O(n). Thus, the Trend Filtering estimator of order 2 won’t attain
the fast rate for such a sequence of θ∗. In fact, it can be shown that the Trend Filtering
estimator won’t even be able to attain the slow rate and would be inconsistent for such a
sequence of θ∗ simply because Trend Filtering can only ﬁt discrete splines.

Another point worth reiterating is that to the best of our knowledge, the results for Trend
Filtering say that the tuning parameter needs to be set diﬀerently depending on whether
one wants to obtain the slow rates or the fast rates; see Guntuboyina et al. (2020) and van de
Geer and Ortelli (2019). In the case of Dyadic CART, both Theorem 5.1 and Theorem 5.2
hold under the choice of the same tuning parameter. Moreover, Theorem 5.2 says that fast
rates for Dyadic CART of order r − 1 hold whenever the true signal is piecewise polynomial
with few pieces and each polynomial can have degree ranging from 0 to r −1. This is because
for any vector θ ∈ Rn, the complexity measure k(r)(θ) is non increasing in r. This means, if
we use Dyadic Cart of order 2, fast rates are guaranteed for piecewise quadratic, piecewise
linear and piecewise constant signals. However, the same is not true for Trend Filtering
where the order r needs to be set to be 3, 2, 1 depending on whether we want fast rates
for piecewise quadratic or linear or constant signals respectively. Among several advantages
there seems to be only one disadvantage for Dyadic CART versus Trend Filtering. It is
the presence of extra log factors in the risk bounds. All in all, our results indicate that
Univariate Dyadic CART may enjoy certain advantages over Trend Filtering in both the
statistical and computational aspects.

Remark 5.5.
It should be remarked here that wavelet shrinkage methods with appropriate
tuning methods can also attain the slow and fast rates as shown in Donoho and Johnstone
(1998), Donoho and Johnstone (1994). Wavelet shrinkage method can also be computed in
O(n) time. However, as is well known, wavelet methods require n to be a power of 2 and
often there are boundary eﬀects that need to be addressed for the ﬁtted function. Univariate
Dyadic CART seems related to wavelet shrinkage as both arise from dyadic thinking but they
are diﬀerent estimators. The way Dyadic CART has been deﬁned in this article, n does not
need to be a power of 2 and no boundary eﬀects appear for Dyadic CART. In any case, our
point here is not to compare Dyadic CART with wavelet shrinkage but to demonstrate the
eﬃcacy of Dyadic CART in ﬁtting piecewise polynomials.

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

30

CHATTERJEE, S. AND GOSWAMI S.

Fig 2. Figure depicts the ground truth matrix which is piecewise constant on a non hierarchical partition.

6. Simulations.
In this section, we present numerical evidence for our theoretical results.
In our simulations we generated data from ground truths θ∗ with certain size and did monte
carlo repetitions to estimate the MSE. We also ﬁtted a least squares line to log MSE versus
log n. This slope is supposed to give us some indication about the exponent of N in the
rate of convergence of the MSE to 0. To set the tuning parameter λ, we did not do very
systematic optimization. Rather, we made a choice which gave us reasonable results. To
implement ORT and Dyadic CART, we wrote our own code in R. Our codes are very basic
and it is likely that the run times can be speeded up by more eﬃcient implementations. All
our simulations are completely reproducible and our codes are available on request.

6.1. Simulation for two dimensional ORT. We take a ground truth θ∗ of size n × n which
is piecewise constant on a nonhierarchical partition as shown in Figure 2. We varied n in
increments of 5 going from 30 to 50. We generated data y by adding mean 0 Gaussian noise
with standard deviation 0.1 and then applied ORT (of order 0) to y. We replicated this
experiment 50 times for each n. We set the tuning parameter λ to be increasing from 0.1
to 0.18 in increments of 0.02.

For the sake of comparison, we also implemented the constrained version of two dimensional
total variation denoising with ideal tuning (i.e. setting the constraint to be equal to the
actual total variation (TVD) of the truth θ∗). In the low σ limit, it can be proved that this
ideally tuned constrained estimator is better than the corresponding penalized estimator for
every deterministic choice of the tuning parameter. This follows from the results of Oymak
and Hassibi (2013) as described in Section 5.2 in Guntuboyina et al. (2020). In this sense,
we are comparing with the best possible version of TVD.

From Figure 3 we see that ORT outperforms the ideal TVD. The slope of the least squares
line is close to −1 for ORT which agrees with our log N/N rate predicted by our theory
for ORT. The slope of the least squares line also agrees with the ‹O(N −3/4) rate predicted
by Theorem 2.3 in Chatterjee and Goswami (2019). It should be mentioned here that we

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

43521ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

31

Fig 3. This is a log MSE vs log N plot for ORT (in green) and ideal TVD (in blue). The slope for ideal
TVD comes out to be −0.67 and and the slope for ORT comes out to be −0.9

did not take n larger than 50 because our implementation in R of ORT is slow (one run
with n = 50 takes 8 minutes). Recall that the computational complexity of ORT is O(n5)
in this setting. We believe that more eﬃcient implementations might make ORT practically
computable for n in the hundreds. We also chose the standard deviation σ = 0.1 because
for larger σ one needs larger sample size to see the rate of convergence of the MSE.

6.2. Simulation for two dimensional Dyadic CART. Here we compare the performance of
Dyadic CART of order 0 and the constrained version of two dimensional total variation
denoising with ideal tuning.

6.2.1. Two piece matrix. We consider the simplest piecewise constant matrix θ∗ ∈ Rn×n
matrix where θ∗(i, j) = I{j ≤ n/2}. Hence θ∗ just takes two distinct values and the true
rectangular partition is dyadic. Thus, this is expected to be a favourable case for Dyadic
CART. We generated data by adding a matrix of independent standard normals. We took
a sequence of n geometrically increasing from 24 to 29. We chose λ = (cid:96) when n = 2(cid:96).

Our simulations (see Figure 4) suggest that in this case, the ideally tuned constrained TVD
estimator is outperformed by Dyadic CART in terms of statistical risk. The least squares
slope for TVD comes out to be −0.71. Theoretically, it is known that the rate of convergence
for ideal constrained TVD is actually N −3/4; see Theorem 2.3 in Chatterjee and Goswami
(2019). The least squares slope for Dyadic CART comes out to be −1.26. Of course, we
expect the actual rate of convergence for Dyadic CART to be ‹O(N −1) in this case. The
constrained TVD estimator was computed by the convex optimization software MOSEK
(via the R package Rmosek). For n = 29 = 512, Dyadic CART was much faster to compute

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

6.66.87.07.27.47.67.88.0−11−10−9−8−7log Nlog MSE6.66.87.07.27.47.67.88.0−11−10−9−8−7log Nlog MSE32

CHATTERJEE, S. AND GOSWAMI S.

Fig 4. The two ﬁgures are log MSE vs log N plot for ideal TVD (in blue) and Dyadic CART (in green).
For the ﬁgure on the left, the ground truth is piecewise constant with two pieces. The slopes came out to be
−0.71 and −1.23 for ideal TVD and Dyadic CART respectively. For the ﬁgure on the right, the ground truth
is a smooth bump function. The slopes came out to be −0.55 and −0.56 for ideal TVD and Dyadic CART
respectively.

and there was a signiﬁcant diﬀerence in the runtimes. Actually, we did not take n larger
than 29 because the RMosek implementation of TVD was becoming too slow. However,
with our implementation of Dyadic Cart, we could run it for sizes as large as 215 × 215.

6.2.2. Smooth Matrix. We considered the matrix θ∗ ∈ Rn×n matrix where θ∗(i, j) =
sin(i π/n) sin(j π/n). We generated data by adding a matrix of independent standard nor-
mals. We again took a sequence of n geometrically increasing from 24 to 29 and chose λ = (cid:96)
when n = 2(cid:96). The ground truth here is a smooth matrix which is expected to favour TVD
more than Dyadic CART. In this case, we saw (see Figure 4) that the slopes of the least
squares line came out to be around −0.55 for both Dyadic CART and TVD. Recall that
‹O(N −0.5) rate is the minimax rate for bounded variation functions. The ideally tuned TVD
did have a slightly lower MSE than Dyadic CART for our choice of λ in this example.

6.3. Simulation for univariate Dyadic Cart. Here we compare the performance of univari-
ate Dyadic CART of order 1 and the constrained version of Trend Filtering (which ﬁts
piecewise linear functions) with ideal tuning. We consider a piecewise linear function f
given by

f (x) = −44 max(0, x − 0.3) + 48 max(0, x − 0.55) − 56 max(0, x − 0.8) + 0.28x.

A similar function was considered in Guntuboyina et al. (2020) where the knots were at
dyadic points 0.25, 0.5, 0.75 respectively. We intentionally changed the knot points which

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

6789101112−12−10−8−6−4log Nlog MSE6789101112−12−10−8−6−4log Nlog MSE6789101112−6−5−4−3−2log Nlog MSE6789101112−6−5−4−3−2log Nlog MSEADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

33

Fig 5. The ﬁgure on the left is log MSE vs log N (base 2) plot for ideal Trend Filtering (in blue) and Dyadic
CART (in green). The slopes came out to be −0.65 and −0.70 for ideal Trend Filtering and Dyadic Cart
respectively. Figure on the right is an instance of our simulation with sample size N = 256. The red piecewise
linear curve is the ground truth. The green curve is the ideal Trend Filtering ﬁt and the orange curve is the
Dyadic CART ﬁt with λ = 8.

makes the problem harder for Dyadic CART. We considered the ground truth θ∗ to be
evaluations of f on a grid in [0, 1] with spacing 1/N. We then added standard Gaussian
noise to generate data. We took a sequence of sample sizes N geometrically increasing from
27 to 212 and chose λ = (cid:96) when N = 2(cid:96).

The slopes of the least squares lines came out to be −0.65 and −0.7 for Trend Filtering and
Dyadic CART respectively (see Figure 5). These slopes are a bit bigger than the theoretically
expected rate ‹O(N −1) for both the estimators. This could be because of the inherent log
factors. We observed that ideal Trend Filtering indeed has a lower MSE than Dyadic CART.
Since the knots are at non dyadic points, ﬁts from Dyadic CART are forced to make several
knots near the true knot points. This eﬀect is more pronounced for small sample sizes. For
large sample sizes however, both the estimators give high quality ﬁts. For a slightly worse ﬁt
than ideal Trend Filtering, the advantage of Dyadic CART is that it can be computed very
fast. We have implemented Trend Filtering by using RMosek and again, we saw a signiﬁcant
diﬀerence in the running speeds when N is large. The reason we did not take sample size
larger than 212 is again because the RMosek implementation of Trend Filtering became too
slow.

Remark 6.1. Recall that it is not necessary for the sample size to be a power of 2 for
deﬁning and implementing Dyadic CART. We just need to adopt a convention for imple-
menting a dyadic split of a rectangle. In our simulations, we have taken N to be a power
of two because writing the code becomes less messy. Also, we have compared our estimators
to the ideal TVD/Trend Filtering. In practice, both estimators would be cross validated and

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

789101112−6−5−4−3−2log Nlog MSE789101112−6−5−4−3−2log Nlog MSE05010015020025005101534

CHATTERJEE, S. AND GOSWAMI S.

one needs to make a comparison then as well.

7. Discussion. Here we discuss a couple of naturally related matters.

7.1. Implications for Shape Constrained Function Classes. Since our oracle risk bound in
Theorem 2.1 holds for all truths θ∗, it is potentially applicable to other function classes
as well. A similar oracle risk bound was used by Donoho (1997) to demonstrate minimax
rate optimality of Dyadic CART for some anisotropically smooth function classes. Since our
focus here is on nonsmooth function classes we now discuss here some implications of our
results for shape constrained function classes which has been of recent interest.

Consider the class of bounded monotone signals on Ld,n deﬁned as

Md,n = {θ ∈ [0, 1]Ln,d : θ[i1, . . . , id] ≤ θ[j1, . . . , , jd] whenever i1 ≤ j1, . . . , id ≤ jd}.

Estimating signals within this class falls under the purview of isotonic regression. It is known
that the LSE is near minimax rate optimal over Md,n with a ‹O(n−1/d) rate of convergence
for d ≥ 2 and O(n−2/3) rate for d = 1; see, e.g. Chatterjee et al. (2015), Chatterjee et al.
(2018), Han et al. (2019). It can be checked that Theorem 4.2 actually implies that Dyadic
Cart also achieves the ‹O(n−1/d) rate for signals in Md,n as the total variation for such
signals grows like O(nd−1). Thus, Dyadic CART is a near minimax rate optimal estimator
for multivariate Isotonic Regression as well.

Let us now consider univariate convex regression. It is known that the LSE is minimax
rate optimal, attaining the ‹O(n−4/5) rate, over convex functions with bounded entries, see
e.g. Guntuboyina and Sen (2015), Chatterjee (2016). It is also known that the LSE attains
the ‹O(k/n) rate if the true signal is piecewise linear in addition to being convex. Theorem 5.1
and Theorem 5.2 imply both these facts also hold for Univariate Dyadic Cart of order 1.
An advantage of Dyadic CART over convex regression LSE would be that Dyadic CART
is computable in O(n) time whereas such a fast algorithm is not known to exist yet for the
convex regression LSE. Of course, the disadvantage of Dyadic CART here is the presence
of a tuning parameter.

It is an interesting question as to whether Dyadic CART or even ORT can attain near
minimax optimal rates for other shape constrained classes such as multivariate convex
functions etc. More generally, going beyond shape constraints, we believe that Dyadic CART
of some appropriate order r might be minimax rate optimal among multivariate function
classes of bounded variation of higher orders. We leave this as a future avenue of research.

7.2. Arbitrary Design. Our estimators have been designed for the case when the design
points fall on a lattice. It is practically important to have methods which can be implemented
for arbitrary data. Optimal Dyadic Trees which can be implemented for arbitrary data in
the context of classiﬁcation have already been studied in Blanchard et al. (2007), Scott and

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

35

Nowak (2006). We now describe a similar way to deﬁne Optimal Regression Tree (ORT)
ﬁtting piecewise constant functions for arbitrary data.

Suppose we observe pairs (x1, y1), . . . , (xN , yN ) where we assume all the design points lie
on the unit cube [0, 1]d after scaling, if necessary. We can divide [0, 1]d into small cubes
of side length 1/L so that there is a grid of Ld cubes. We can now consider the space of
all rectangular partitions where each rectangle is a union of these small cubes. Thus, each
partition is necessarily a coarsening of the grid. Call this space of partitions PL. Deﬁne FL
to be the space of functions which are piecewise constant on some partition in PL. We can
now deﬁne the optimization problem:

(cid:98)f = argmin
f ∈FL

n
(cid:88)

(cid:0)

i=1

(yi − f (xi))2 + λ|f |(cid:1)

where |f | is the number of constant pieces of f.

One can check that the above optimization problem can be rewritten as an optimization
problem over the space of partitions in PL. The only diﬀerence with the lattice design setup
is that some of the small cubes here might be empty (depending on the value L and the
sample size N ) but a bottom up dynamic program can still be carried out. In this method,
L is an additional tuning parameter which represents the resolution at which we are willing
to estimate f ∗. Theoretical analysis need to be done to ascertain how L should depend on
sample size. The computational complexity would scale like O(L2d+1), so this method can
still be computationally feasible for low to moderate d.

This method is very natural and is likely to be known to the experts but we are not aware of
an exact reference. Like what is done in Bertsimas and Dunn (2017), it might be interesting
to compare the performance of this method to the usual CART for real/simulated datasets
with a few covariates. If the method performs well, theoretical analysis also needs to be
done under the random design set up. We leave this for future work.

7.3. Dependent Errors. The proofs of our results can be extended without much eﬀort to
the case when Z ∼ N (0, Σ) where Σ is a n × n covariance matrix with (cid:96)2-operator norm,
i.e. the maximum eigen value (cid:107)Σ(cid:107)op. The only change now would be that there would be
an extra multiplicative factor (cid:107)Σ(cid:107)op in front of all our risk bounds. Thus, if the maximum
eigenvalue of Σ remains bounded then all our rates of convergence remain the same.

In case of a general covariance matrix, the only change would be in the proof of Theorem 8.1
which in turn relies crucially on Lemma 9.1. In Lemma 9.1 there are two results. One gives
a bound on the expectation and the other gives a tail bound (using Gaussian concentration
inequality for Lipschitz functions) for the random variable

sup
v∈S, v(cid:54)=θ

(cid:104)Z,

v − θ
(cid:107)v − θ(cid:107)

(cid:105)

where θ is an arbitrary vector, S is a ﬁxed subspace of RN and Z ∼ N (0, I).

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

36

CHATTERJEE, S. AND GOSWAMI S.

For a general covariance matrix Σ we now have to give an expectation bound and a tail
probability bound for a random variable of the form

sup
v∈S, v(cid:54)=θ

(cid:104)Σ1/2Z,

v − θ
(cid:107)v − θ(cid:107)

(cid:105)

where Z ∼ N (0, 1). Lemma 9.1 has been written for a general positive semi deﬁnite matrix
Σ and thus gives the required expectation and tail bound. The rest of the proof is then
similar and an extra multiplicative factor (cid:107)Σ(cid:107)op will result as can be checked by the reader.

8. Proofs.

8.1. Proof of Lemma 1.1.

8.1.1. Case: r = 0. Let us ﬁrst consider the case r = 0. Throughout this proof, the
(multiplicative) constant involved in O(·) is assumed to be absolute, i.e. it does not depend
on r or d.

We describe the algorithm to compute the ORT estimator denoted by (cid:98)θ(0)
for any ﬁxed d ≥ 1 we need to compute the minimum:

hier,d,n. Note that

OP T (Ld,n) :=

min
Π:Π∈Phier,d,n

(cid:0)(cid:107)y − Πy(cid:107)2 + K|Π|(cid:1)

and ﬁnd the optimal partition. Here Πy denotes the orthogonal projection of y onto the
subspace S(0)(Π). In this case, Πy is a piecewise constant array taking the mean value of
the entries of yR within every rectangle R constituting Π.

Now, for any given rectangle R ⊂ Ld,n we can deﬁne the corresponding minimum restricted
to R.

(8.1)

OP T (R) = min

Π:Π∈Phier,R

(cid:107)yR − ΠyR(cid:107)2 + λ|Π|.

where we are now optimizing only over the class of hierarchical rectangular partitions of
the rectangle R denoted by Phier,R and |Π| denotes the number of rectangles constituting
the partition Π.

A key point to note here is that due to the “additive nature” of the objective function
over any partition (possibly trivial) of R into two disjoint rectangles, we have the following
dynamic programming principle for computing OP T (R):

(8.2)

OP T (R) = min
R1,R2

( OP T (R1) + OP T (R2), (cid:107)yR − yR(cid:107)2 + λ).

Here (R1, R2) ranges over all possible nontrivial partitions of R into two disjoint rectangles.
Consequently, in order to compute OP T (R) and obtain the optimal heirarchical partition,

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

37

the ﬁrst step is to obtain the corresponding ﬁrst split of R. Let us denote this ﬁrst split by
SP LIT (R).

Let us now make some observations. For any rectangle R, the number of splits possible is at
most dn. This is because in each dimension, there are at most n possible splits. Any split of
R creates two disjoint sub rectangles R1, R2. Suppose we know OP T (R1) and OP T (R2) for
R1, R2 arising out of each possible split. Then, to compute SP LIT (R) we have to compute
the minimum of the sum of OP T (R1) and OP T (R2) for each possible split as well as the
number 1 + (cid:107)yR − ΠyR(cid:107)2 which corresponds to not splitting R at all. Thus, we need to
compute the minimum of at most nd + 1 numbers.

The total number of distinct rectangles of Ld,n is at most n2d = N 2. Any rectangle R has
dimensions n1 × n2 × · · · × nd. Let us denote the number n1 + · · · + nd by Size(R). We are
now ready to describe our main subroutine.

8.1.2. Main Subroutine. For each rectangle R, the goal is to store SP LIT (R) and OP T (R).
We do this inductively on Size(R). We will make a single pass/visit through all distinct
rectangles R ⊂ Ld,n, in increasing order of Size(R). Thus, we will ﬁrst start with all
1 × 1 × · · · × 1 rectangles of size equals d. Then we visit rectangles of size d + 1, d + 2 all
the way to nd. Fixing the size, we can choose some arbitrary order in which we visit the
rectangles.

For 1 × 1 × · · · × 1 rectangles, computing SP LIT (R) and OP T (R) is trivial. Consider
a generic step where we are visiting some rectangle R. Note that we have already com-
puted OP T (R(cid:48)) for all rectangles R(cid:48) with Size(R(cid:48)) < Size(R). For a possible split of R,
it generates two rectangles R1, R2 of strictly smaller size. Thus, it is possible to compute
OP T (R1) + OP T (R2) and store it. We do this for each possible split to get a list of at most
nd + 1 numbers. We also compute (cid:107)yR − yR(cid:107)2 (described later) and add this number to
the list. We now take a minimum of these numbers. This way, we obtain OP T (R) and also
SP LIT (R).

The number of basic operations needed per rectangle here is O(nd). Since there are N 2
rectangles in all, the total computational complexity of the whole inductive scheme scales
like O(N 2 n d).

To compute (cid:107)yR−yR(cid:107)2 for every rectangle R, we can again induct on size in increasing order.
Deﬁne SU M (R) to be the sum of entries of yR and SU M SQ(R) to be the sum of squares
of entries of yR. One can keep storing SU M (R) and SU M SQ(R) and SIZE(R) bottom
up. To compute these quantities for any rectangle R it suﬃces to consider any non trivial
partition of R into two rectangles R1, R2 and then add these quantites previously stored for
R1 and R2. Therefore, this updating step requires constant number of basic operations per
rectangle R. Once we have computed SU M (R) and SU M SQ(R) and SIZE(R) we can then
calculate (cid:107)yR − yR(cid:107)2. Thus, this inductive sub scheme requires lower order computation.

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

38

CHATTERJEE, S. AND GOSWAMI S.

Once we ﬁnish the above inductive scheme, we have stored SP LIT (R) for every rectangle R.
We can now start going topdown, starting from the biggest rectangle which is Ld,n itself. We
can recreate the full optimal partition by using SP LIT (R) to split the rectangles at every
step. Once the full optimal partition is obtained, computing (cid:98)θ just involves computing means
within the rectangles constituting the partition. It can be checked that this step requires
lower order computation as well.

Remark 8.1. The same algorithm can be used to compute Dyadic CART in all dimensions
as well. In this case, the number of possible splits per rectangle is d. Also, in the inductive
scheme, we only need to visit rectangles which are reachable from Ld,n by repeated dyadic
splits. Such rectangles are necessarily of the form of a product of dyadic intervals Here,
dyadic intervals are interval subsets of [n] which are reachable by succesive dyadic splits
of [n]. There are at most 2n dyadic intervals of [n] and thus we need to visit at most
(2n)d = 2d N rectangles.

8.1.3. Case: r ≥ 1. Let us ﬁx a subspace S ⊂ RLd,n and a set of basis vectors B = (b1 : b2 :
· · · : bL) for S. Here, we think of bi as a column vector in RN . For instance, B may consist
of all (discrete) monomials of degree at most r; however the algorithm works for any choice
of S and B. We will abuse notation and also denote by B the matrix obtained by stacking
together the columns of B. Thus B is a N × L matrix; each row corresponds to an entry
of the lattice Ld,n. Also, for any rectangle R ⊂ Ld,n we denote BR to be the matrix of size
|R| × L where only the subset of rows corresponding to the entries in the rectangle R are
present. Also, let’s denote the orthogonal projection matrix BR(BT

RBR)−1BT

R by OBR.

RBR) = (BT
R1

We can now again run the inductive scheme described in the last section. The important
point here is that when computing SP LIT (R) for each rectangle R, one needs to compute
ROBRyR. We can keep storing (BT
R(I −OBR)yR. The dominating task is to compute yT
yT
RBR)
as follows. Note that (BT
BR1) + (BT
BR2) for any partition of R into two sub-
R2
rectangles R1, R2. Thus we can keep computing BT
RBR inductively by adding two matrices.
This is at most O(L2) work, i.e. requires at most O(L2) many elementary operations. The
RBR)−1. This is at most O(L3) work. The next step
major step is computing the inverse (BT
RyR. Again we can compute this by adding (BT
is to compute BT
yR1) + (BT
yR2) which
R1
R2
is O(L) work. Finally we need to post multiply (BT
RBR)−1 by BT
RyR and pre multiply by
(BT

RyR)T . This needs at most O(L2) work.

Remark 8.2. Multiplying Am×n and Bn×p is actually o(mnp) in general if we invoke
Strassen’s algorithm. Here we use the standard O(mnp) complexity for the sake of concrete-
ness.

Per visit to a rectangle R, we also need to compute the minimum of nd+1 numbers which is
O(nd) work. Finally, we need to visit N 2 rectangles in total. Thus the total computational
complexity of ORT would be O[N 2 (nd + L3)]. A similar argument would give that the
computational complexity of Dyadic CART of order r ≥ 1 is O[2d N (d + L3)]. Now note

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

39

that in case we use the polynomial basis, we would have L = O(dr).

8.2. Proof of Theorem 2.1. We will actually prove a more general result which will imply
Theorem 2.1. Let S be any ﬁnite collection of subspaces of RN . Recall that for a generic
subspace S ∈ S, we denote its dimension by Dim(S) and we denote its orthogonal projection
matrix by OS. Also, let Nk(S) = |S : Dim(S) = k, S ∈ S|. Suppose, there exists a constant
c > 0 such that for each k ∈ [N ], we have

(8.3)

Nk(S) ≤ N ck.

Let Θ = ∪S∈SS be the parameter space. Let y = θ∗ + σZ be our observation where θ∗ ∈ Rn
is the underlying mean vector and Z ∼ N (0, I). In this context, recall the deﬁnition of kS(θ)
in (2.1). For a given tuning parameter λ ≥ 0 we now deﬁne the usual penalized likelihood
estimator (cid:98)θλ:

(cid:98)θλ = argmin

θ∈Θ

(cid:0)(cid:107)y − θ(cid:107)2 + λ kS(θ)(cid:1).

Theorem 8.1 (Union of Subspaces). Under the setting as described above, for any 0 <
δ < 1 let us set

λ ≥ C

σ2 log N
δ

for a particular absolute constant C which only depends on c. Then we have the following
risk bound for (cid:98)θλ:

E(cid:107)(cid:98)θλ − θ∗(cid:107)2 ≤ inf
θ∈Θ

(cid:2) (1 + δ)
(1 − δ)

(cid:107)θ − θ∗(cid:107)2 +

λ
1 − δ

kS(θ)(cid:3) + C

σ2
δ (1 − δ)

.

Using Theorem 8.1, the proof of Theorem 2.1 is now straightforward.

Proof of Theorem 2.1. We just have to verify the cardinality bounds (8.3) for the col-
lection of subspaces S (r)
all because it
a
contains the other two. Now Nk(S (r)
all ) is clearly at most the number of distinct rectangles
in Ld,n raised to the power k. The number of distinct rectangles in Ld,n is always upper
bounded by N 2. Thus the bound (8.3) holds with c = 2.

for a ∈ {rdp, hier, all}. It is enough to verify for S (r)

We now give the proof of Theorem 8.1.

Proof. To avoid clutter of notations we will drop the subscript S from kS(·) in this proof.
By deﬁnition, for any arbitrary θ ∈ S, we have

(cid:107)y − (cid:98)θ(cid:107)2 + λ k((cid:98)θ) ≤ (cid:107)y − θ(cid:107)2 + λ k(θ).

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

40

CHATTERJEE, S. AND GOSWAMI S.

Since y = θ∗ + σZ we can equivalently write

(cid:107)θ∗ − (cid:98)θ + σZ(cid:107)2 + λk((cid:98)θ) ≤ (cid:107)θ∗ − θ + σZ(cid:107)2 + λk(θ).

We can further simplify the above inequality by expanding squares to obtain

(cid:107)θ∗ − (cid:98)θ(cid:107)2 ≤ (cid:107)θ∗ − θ(cid:107)2 + λk(θ) + 2 (cid:104)σ Z, (cid:98)θ − θ(cid:105) − λk((cid:98)θ).

Now using the inequality 2ab ≤ 2

δ a2 + δ

2 b2 for arbitrary positive numbers a, b, δ we have

2(cid:104)σ Z, (cid:98)θ − θ(cid:105) ≤

≤

(cid:0)(cid:104)σ Z,

(cid:0)(cid:104)σ Z,

2
δ

2
δ

(cid:98)θ − θ
(cid:107)(cid:98)θ − θ(cid:107)
(cid:98)θ − θ
(cid:107)(cid:98)θ − θ(cid:107)

(cid:105)(cid:1)2 +

δ
2

(cid:107)(cid:98)θ − θ(cid:107)2

(cid:105)(cid:1)2 + δ(cid:107)(cid:98)θ − θ∗(cid:107)2 + δ(cid:107)θ∗ − θ(cid:107)2.

The last two displays therefore let us conclude that for any δ > 0 and all θ ∈ RN the
following pointwise upper bound on the squared error holds:

(8.4) (cid:107)θ∗ − (cid:98)θ(cid:107)2 ≤

(1 + δ)
(1 − δ)

(cid:107)θ∗ −θ(cid:107)2 +

λ
(1 − δ)

k(θ)+

2
δ(1 − δ)

(cid:0)(cid:104)σ Z,

(cid:98)θ − θ
(cid:107)(cid:98)θ − θ(cid:107)

(cid:105)(cid:1)2 −

λ
(1 − δ)

k((cid:98)θ).

To get a risk bound, we now need to upper bound the random variable

L(Z) =

2
δ(1 − δ)

(cid:0)(cid:104)σ Z,

(cid:98)θ − θ
(cid:107)(cid:98)θ − θ(cid:107)

(cid:105)(cid:1)2 −

λ
(1 − δ)

k((cid:98)θ).

For the rest of this proof, C1, C2, C3 would denote constants whose precise value might
change from line to line. We can write

L(Z) ≤ max
k∈[n]

(cid:2)

2
δ(1 − δ)

σ2

sup
S∈S:Dim(S)=k

sup
v∈S

(cid:0)(cid:104)Z,

v − θ
(cid:107)v − θ(cid:107)

(cid:105)(cid:1)2 −

λ
(1 − δ)

k(cid:3).

Fix any number t > 0. Also ﬁx a subspace S ∈ S such that Dim(S) = k. Using (9.1) in
Lemma 9.1 (stated and proved in Section 9) we obtain

P((cid:2)

2
δ(1 − δ)

σ2 sup
v∈S

(cid:0)(cid:104)Z,

v − θ
(cid:107)v − θ(cid:107)

(cid:105)(cid:1)2 −

λ
(1 − δ)

k(cid:3) > t) ≤ C1 exp (cid:0) − C2

(cid:2) t δ (1 − δ)
σ2

+

λ k δ
σ2

(cid:3)(cid:1).

Here we also use the fact that λ would be chosen to be at least bounded below by a constant.
Using a union bound argument we can now write

P((cid:2)

2
δ(1 − δ)

σ2

sup
S∈S:Dim(S)=k

sup
v∈S

(cid:0)(cid:104)Z,

v − θ
(cid:107)v − θ(cid:107)

(cid:105)(cid:1)2 −

λ
(1 − δ)

k(cid:3) > t)

≤ Nk(S) C1 exp (cid:0) − C2

(cid:2) t δ (1 − δ)
σ2

+

λ k δ
σ2

(cid:3)(cid:1).

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

41

Now use the fact that log Nk(S) ≤ c k log N and set λ ≥ C σ2 log N 1
bound on the right hand side of the above display

δ to get a further upper

C1 exp (cid:0) − C2

(cid:2) t δ (1 − δ)
σ2

− k log N (cid:3)(cid:1)

The above two displays along with another union bound argument then lets us conclude

P(L(Z) > t) ≤

n
(cid:88)

k=1

C1 exp (cid:0) − C2

(cid:2) t δ (1 − δ)
σ2

− k log N (cid:3)(cid:1).

Finally, integrating the above inequality with respect to all nonnegative t will then give us
the inequality

EL(Z) ≤ C3

σ2
δ (1 − δ)

.

The above inequality coupled with (8.4) ﬁnishes the proof of the proposition.

8.3. Proof of Proposition 3.9.

Proof of Proposition 3.9. For simplicity of exposition, we will take n = 2k to be a
power of 2 although the same proof will go through for a general n. A subinterval I of [n]
is called a dyadic interval of [n] if it is of the form [(a − 1)2s + 1, a2s] for some integers
0 ≤ s < k and 1 ≤ a < 2k−s. The following lemma characterizes recursive dyadic partitions
in L2,n.

Lemma 8.2. A partition Π ∈ Pall,2,n is a recursive dyadic partition iﬀ each of its con-
stituent rectangles is a product of dyadic intervals of [n].

Proof. The only if part can be shown by an induction on the successive possible steps
of constructing a recursive dyadic partition. For the if part, let us argue as follows. Let Π
be an arbitrary partition such that every rectangle constituting it is a product of dyadic
intervals. If Π is not the trivial partition L2,n then we will argue that there exists a dyadic
split of Ld,n into R1, R2 such that Π is a reﬁnement of the partition just composed of R1, R2.
Equivalently, we would show that there is a coordinate 1 ≤ j ≤ 2 such that by performing
a dyadic split on the j th coordinate, we do not cut any rectangle of Π in its interior. We
will now argue by contradiction. Suppose there is no such coordinate. Take coordinate 1
for example. Then there exists a rectangle R1 = [a1, b1] × [a2, b2] of the partition Π which
is cut in its interior by a dyadic split on the ﬁrst coordinate. This necessarily implies that
a1 < n/2 and b1 > n/2. Since [a1, b1] is a dyadic interval this then necessarily implies that
a1 = 1 and b1 = n. Repeating this argument for coordinate 2, we see that there exists
rectangle R2 of the partition Π which is of the form [a1, b1] × [1, n]. Now clearly it is not
possible for any partition in L2,n to consist of the rectangles R1, R2 together. Thus, we have
arrived at a contradiction.

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

42

CHATTERJEE, S. AND GOSWAMI S.

Now take a coordinate j ∈ [2] for which a dyadic split along that coordinate does not cut
any rectangle of Π in its interior. Perform this dyadic split into R1, R2. Now it is as if
we have two separate problems of the same type within R1 and R2. Again, we can ﬁnd
coordinates to dyadically split within R1 and R2 so that it does not cut any rectangle of Π
in its interior if the partition Π within R1 and R2 are not trivial respectively. We can now
iterate this argument till we reach the partition Π and stop. This shows that Π is in fact a
recursive dyadic partition which ﬁnishes the proof.

We would also need the following observation which is equivalent to equation (3.1) in the
statement of Proposition 3.9 for d = 1.

Lemma 8.3. Given a partition Π ∈ Pall,1,n, there exists a reﬁnement (cid:101)Π ∈ Prdp,1,n such
that

where C > 0 is an absolute constant.

|(cid:101)Π| ≤ C|Π| log2

(cid:1)

(cid:0) n
|Π|

Proof of Lemma 8.3. Let Π be an arbitrary partition of [n] and let us denote |Π| by k.
Let us consider the binary tree associated with forming a RDP of [n]. Consider the following
scheme to obtain a RDP of [n] which is also a reﬁnement of Π. Grow the complete binary
tree till the number of leaves ﬁrst exceed k. At this stage, each node consists of O(n/k)
elements. After this, if at any stage, a node of the binary tree (denoting some interval of [n])
is completely contained within some interval of Π, we do not split that node. Otherwise, we
dyadically split the node. Due to our splitting criteria, in each such round, we split at most
k nodes because the nodes represent disjoint intervals. Also, the number of rounds of such
splitting is at most O(log n
k ). When this scheme ﬁnishes, we clearly get a reﬁnement of Π;
say (cid:101)Π ∈ Prdp,1,n. These observations ﬁnish the proof.

Corollary 8.4. Given any interval I ⊂ [n], there exists atmost C log2 n many dyadic
intervals partitioning I.

Proof. Let I = [a, b]. Let I0 = [1, a − 1] which is empty if a − 1 = 0 and I1 = [b + 1, n]
which is empty if b + 1 > n. Then the rectangles I0, I, I1 form a partition of [n]. Now use
Lemma 8.3 to obtain a recursive dyadic partition with atmost C log2 n intervals. Considering
the intervals of this recursive dyadic partition just within I now ﬁnishes the proof.

Now we are ready to ﬁnish the proof of Proposition 3.9. Take any rectangle R constituting
the partition Π. Let R = [a1, b1] × [a2, b2]. Using Corollary 8.4, for each i ∈ [2], write [ai, bi]
as a union of atmost C log n many disjoint dyadic intervals. As a result, we can view R
itself as a union of atmost (C log n)2 disjoint rectangles each of which has the property that
it is a product of dyadic intervals of [n]. Doing this for each R then gives us a reﬁnement
of Π into atmost k(C log n)2 rectangles each of which is a product of dyadic intervals. By

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

43

Lemma 8.2 this reﬁnement is guaranteed to be a recursive dyadic partition. This ﬁnishes
the proof.

Remark 8.3. A natural question is whether the following holds in any dimension d > 2
and any d dimensional array θ.

rdp(θ) ≤ C( log n)dk(r)
k(r)

all (θ).

Proposition 3.9 shows the above is true when d = 1, 2. Our proof technique breaks down
for higher d. The reason is that Lemma 8.2 is no longer true when d > 2. For a counter
example, consider the case where d = 3 and n = 2. Consider the partition of L3,2 consisting
of rectangles

1. R1 = {1, 2} × {1} × {2}
2. R2 = {2} × {1, 2} × {1}
3. R3 = {1} × {2} × {1, 2}
4. R4 = {1} × {1} × {1}
5. R5 = {2} × {2} × {2}.

One can check that

1. The rectangles R1, . . . , R5 form a partition of L3,2.
2. Each of Ri is a product of dyadic intervals of [2].
3. This partition cannot be a recursive dyadic partition. This is because the ﬁrst dyadic

split itself will necessarily cut one of the rectangles R1, R2, R3 in its interior.

In view of Theorem 2.1, we need to show that if θ ∈ RLd,n
8.4. Proof of Theorem 4.2.
has small total variation, then it can be approximated well by some (cid:101)θ ∈ RLd,n which is
piecewise constant on not too many axis aligned rectangles. To establish this, we need two
intermediate results.

Proposition 8.5. Let θ ∈ RLd,n and δ > 0. Then there exists a Recursive Dyadic Partition
Πθ,δ = (R1, . . . , Rk) ∈ Prdp,d,n such that
(cid:1)
a) k = |Πθ,δ| ≤ 1 + log2 N (cid:0)1 + TV(θ)
b) TV(θRi) ≤ δ ∀i ∈ [k]
c) A(Ri) ≤ 2 ∀i ∈ [k]
where A(R) denotes the aspect ratio of a generic rectangle R.

δ

Proof. In order to prove Proposition 8.5, we ﬁrst describe a general greedy partitioning
scheme — called the (TV, δ) scheme — which takes as input a positive number δ and
outputs a partition satisfying properties a), b) and c). A very similar procedure was used
in Chatterjee and Goswami (2019).

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

44

CHATTERJEE, S. AND GOSWAMI S.

Description of the (TV, δ) scheme. First, let us note that a Recursive Dyadic Partition
(RDP) of Ld,n can be encoded via a binary tree and a labelling of the nonleaf vertices. This
can be seen as follows. Let the root represent the full set Ld,n. If the ﬁrst step of partitioning
is done by dividing in half along coordinate i then label the root vertex by i ∈ [d]. The two
children of the root now represent the subsets of Ld,n given by [n]i−1 × [n/2] × [n]d−i and
its complement. Depending upon the coordinate of the next split, these vertices can now
also be labelled.

In the ﬁrst step of the (TV, δ) scheme, we check whether TV(θ) ≤ δ. If so, then stop and
the root becomes a leaf. If not, then we label the root by 1 and split Ld,n along coordinate
1 in half. The two vertices now represent rectangles R1, R2 say. For i = 1, 2 we then check
whether TV(θRi) ≤ δ. If so, then this node becomes a leaf. Otherwise, we go to the next
step. In this step, we split the node along coordinate 2. We can iterate this procedure until
each node in the binary tree has total variation at most δ. In step i, we label all the nodes
that are split by the number i (mod d). In words, we choose the the splitting coordinate
from 1 to d in a cyclic manner. This ensures that the aspect ratio of each of the rectangles
represented by the leaves is at most 2.

After carrying out this scheme, we would be left with a Recursive Dyadic Partition of Ld,n,
say, Πθ,δ satisfying properties b) and c). In order to show that Πθ,δ also satisﬁes property a),
we need:

Lemma 8.6. Let θ ∈ RLd,n. Then, for any δ > 0, for the (TV, δ) division scheme, we have
the following cardinality bound:

|Πθ,δ| ≤ 1 + log2 N (cid:0)1 +

TV(θ)
δ

(cid:1)

Moreover, each axis aligned rectangle in Pθ,δ has aspect ratio at most 2.

Proof. We say that a vertex of the binary tree is in generation i if its graph distance
to the root is i − 1. Fix any positive integer i. Let us consider the binary tree grown till
step i − 1. Note that all the vertices {v1, . . . , vk} in generation i represent disjoint subsets
of Ld,n. Thus we would have (cid:80)k
i=1 TV(θvi) ≤ V. This means that there can be at most
1 + TV(θ)
vertices of generation i that can be split. Now note that since there are N vertices
in total, the depth of the binary tree can be at most log2 N. Thus there can be at most
log2 N (cid:0)1 + TV(θ)
(cid:1) splits in total and each split increases the number of rectangular blocks
by 1. This proves the cardinality bound. The second assertion is immediate from the fact
that in generation i the split is done on coordinate i.

δ

δ

This lemma together with the preceding discussions now implies our proposition.

8.4.1. Gagliardo Nirenberg Inequality. Our next result concerns the approximation of a
generic array θ with TV(θ) ≤ V by a constant matrix. This result is a crucial ingredient

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

45

of the proof and is a discrete analogue of the Gagliardo-Nirenberg-Sobolev inequality for
compactly supported smooth functions.

Proposition 8.7 (Discrete Gagliardo-Nirenberg-Sobolev Inequality). Let θ ∈ R⊗i∈[d][ni]
and

θ :=

(cid:88)

θ[j1, j2, . . . , jd]/

(cid:89)

ni

be the average of the elements of θ. Then for every d > 1 we have

(j1,j2,...,jd) ∈ ⊗i∈[d][ni]

i∈[d]

m
(cid:88)

(j1,j2,...,jd) ∈ ⊗i∈[d][ni]

|θ[j1, j2, . . . , jd] − θ|

(cid:16)

d
d−1 ≤

1 + max
i,j∈[d]

ni
nj

(cid:17) d

d−1 TV(θ)

d
d−1 .

As a consequence, we also have

m
(cid:88)

(j1,j2,...,jd) ∈ ⊗i∈[d][ni]

|θ[j1, j2, . . . , jd] − θ|2 ≤

(cid:16)

1 + max
i,j∈[d]

(cid:17)2

ni
nj

TV(θ)2 .

Remark 8.4. Although the Gagliardo-Nirenberg-Sobolev inequality is classical for Sobolev
spaces (see, e.g., Chapter 12 in Leoni (2017)), we are not aware of any discrete version
in the literature that applies to arbitrary d dimensional arrays. Also it is not clear if the
inequality in this exact form follows directly from the classical version.

Remark 8.5. The above Discrete Gagliardo Nirenberg inequality was already proved for
the d = 2 case in a previous article by the authors in Chatterjee and Goswami (2019). In
this article we establish this inequality for all d ≥ 2.

Proof. Without loss of generality we may assume that θ = 0. Notice that, for any
(j1, j2, . . . , jd) ∈ ⊗i∈[d][ni], we can write

|θ[j1, j2, . . . , jd]|d ≤

(cid:89)

(cid:88)

i∈[d]

j(cid:48)
i∈[ni]

(cid:12)
(cid:12)θ[j1, . . . , ji−1, j(cid:48)

i, . . . , jd] − θ[j1, . . . , ji−1, j(cid:48)

i − 1, . . . , jd](cid:12)
(cid:12) ,

where θ[j1, . . . , ji−1, 0, . . . , jd] = 0 for all i ∈ [d] and (j1, j2, . . . , jd) ∈ ⊗i∈[d][ni]. For conve-
nience we will henceforth denote the diﬀerence on the right hand side in the above expression
d
as ∇iθ[j1, . . . , j(cid:48)
d−1 obtained
i, . . . , jd]. Now we will sum the upper bound on |θ[j1, j2, . . . , jd]|
from this inequality in each of the variables separately. Let us start with j1:

(cid:88)

|θ[j1, j2, . . . , jd]|

d
d−1

j1∈[n1]

= (cid:0) (cid:88)
j(cid:48)
1∈[n1]
≤ (cid:0) (cid:88)
j(cid:48)
1∈[n1]

|∇1θ[j(cid:48)

1, j2, . . . , jd]|(cid:1) 1

d−1 ·

(cid:88)

(cid:89)

(cid:0) (cid:88)

|∇iθ[j1, . . . , j(cid:48)

i, . . . , jd]|(cid:1) 1

d−1

|∇1θ[j(cid:48)

1, j2, . . . , jd]|(cid:1) 1

d−1 (cid:89)

j1∈[n1]

j(cid:48)
i∈[d]\{1}
i∈[ni]
(cid:0) (cid:88)

|∇iθ[j1, . . . , j(cid:48)

i, . . . , jd]|(cid:1) 1

d−1

i∈[d]\{1}

j1∈[n1], j(cid:48)

i∈[ni]

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

46

CHATTERJEE, S. AND GOSWAMI S.

where in the ﬁnal step we used the H¨older’s inequality. By iterating the similar steps over
all the remaining i ∈ [d], we ultimately get

(8.5)

where

(cid:88)

|θ[j1, j2, . . . , jd]|

d
d−1 ≤

(cid:89)

1
d−1
i

S

.

(j1,j2,...jd)∈⊗i∈[d][ni]

i∈[d]

Si :=

(cid:88)

|∇iθ[j1, . . . , ji, . . . , jd]|

(j1,j2,...jd)∈⊗i∈[d][ni]

However, notice that

(8.6)

Si ≤ TV(θ) +

(cid:88)

|∇iθ[j1, . . . , 1, . . . , jd]|

jk∈[nk], k∈[d]\{i}

where we can recall that θ[j1, . . . , ji−1, 0, . . . , jd] = 0 and hence

∇iθ[j1, . . . , 1, . . . , jd] = θ[j1, . . . , 1, . . . , jd] .

Also since θ = 0, we can write

(8.7) θ[j1, . . . , 1, . . . , jd] =

1
i∈[d] ni

(cid:81)

(cid:88)

(j(cid:48)

1,j(cid:48)

2,...,j(cid:48)

d)∈⊗i∈[d]ni

(θ[j1, . . . , 1, . . . , jd] − θ[j(cid:48)

1, j(cid:48)

2 . . . , j(cid:48)

d]) .

Using this expression we will show that

(8.8)

(cid:88)

jk∈[nk], k∈[d]\{i}

|∇iθ[j1, . . . , 1, . . . , jd] ≤ max
i,j∈[d]

ni
nj

TV(θ)

which along with (8.5) and (8.6) implies the proposition.

We will only deal with the case i = 1 since the other cases are similar. In the remainder
of the proof we will treat the set L := ⊗d
i=1[ni] as an (induced) subgraph of Zd. We will
call the minimum length path between u = (u1, u2, . . . , ud) and v = (v1, v2, . . . , vd) in Zd
as being oriented if its ﬁrst |u1 − v1| steps are along the ﬁrst coordinate axis, the next
|u2 − v2| steps are along the second coordinate axis and so on. Also for any edge e in
L, we denote the diﬀerence between θ evaluated at its two endpoints (oriented along the
direction of increasing coordinate) as ∇eθ. Now writing each diﬀerence appearing in (8.7)
as a telescoping sum of ∇eθ terms along an oriented path we get:

|∇1θ[1, j2, . . . , jd]|

≤

(cid:81)

≤

(cid:81)

1
i∈[d] ni

1
i∈[d] ni

(cid:88)

|θ[1, j2, . . . , jd] − θ[j(cid:48)

1, j(cid:48)

2 . . . , j(cid:48)

d]|

(j(cid:48)

1,j(cid:48)

2,...,j(cid:48)
(cid:88)

d)∈⊗i∈[d]ni
(cid:88)

|∇eθ| ,

j(cid:48)∈⊗i∈[d]ni

e∈πj(cid:48)

j

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

47

where πj(cid:48)
j
(j(cid:48)
2, . . . , j(cid:48)
1, j(cid:48)
in (8.6) we then get

is the oriented (shortest-length) path between j := (1, j2, . . . , jd) and j(cid:48)
:=
d). Summing over all j ∈ ∂1L (vertices in L whose ﬁrst coordinate is 1) as

(cid:88)

j∈∂1L

|∇iθ[1, j2, . . . , jd]| ≤

1
i∈[d] ni

(cid:81)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

e∈L

j∈∂1L

j(cid:48)∈L

πj(cid:48)
j (cid:51)e

|∇eθ| .

Thus the number of times |∇eθ| appears in the above summation for any given edge e is
at most d times the total number of oriented paths containing e whose one endpoint lies
on the boundary of L. Call this number Ne. If e lies along the i-th coordinate axis, then it
follows from the deﬁnition of oriented paths that

(cid:40)

n2
i
(cid:81)

Ne ≤

(cid:81)

j(cid:54)=1,i nj

if i > 1

j∈[d] nj

otherwise .

Plugging this estimate into the previous display immediately yields (8.8).

We are now ready to prove Theorem 4.2.

Proof of Theorem 4.2. Without loss of generality let S = {1, . . . , s} where s = |S| ≥ 2.
Let θ∗ ∈ KS

d,n(V ). Let us denote

R(θ∗) = inf

θ∈RLd,n

(cid:2)(cid:107)θ − θ∗(cid:107)2 + σ2 log N k(0)

rdp(θ)(cid:3).

In view of Theorem 2.1, it suﬃces to upper bound R(θ∗).

Let us deﬁne the s dimensional array θ∗

S which satisﬁes

S(i1, . . . , is) = θ∗(i1, . . . , is, 1, . . . , 1) ∀(i1, . . . , is) ∈ [n]s.
θ∗

For any ﬁxed δ > 0, let Πθ,δ be the RDP of Ls,n that is obtained from applying Lemma 8.6
S. Let (cid:101)θ ∈ RLs,n be deﬁned to be piecewise constant on the
to the s dimensional array θ∗
partition Πθ,δ so that within each rectangle of Πθ,δ the array (cid:101)θ equals the mean of the entries
of θ∗
S inside that rectangle. Each rectangle of Πθ,δ has aspect ratio at most 2 and we have

k(0)
rdp((cid:101)θ) = |Πθ,δ| ≤ C log N

TV(θ∗
S)
δ

.

We can now apply Proposition 8.7 to conclude, within every such rectangle R of Πθ,δ, we
have (cid:107)(cid:101)θR − θ∗

R(cid:107)2 ≤ Cδ2. This gives us

(8.9)

(cid:107)(cid:101)θ − θ∗

S(cid:107)2 =

(cid:88)

R∈Πθ,(cid:15)

(cid:107)(cid:101)θR − θ∗

R(cid:107)2 ≤ Cδ2|Πθ,δ| ≤ Cδ log N TV(θ∗

S).

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

48

CHATTERJEE, S. AND GOSWAMI S.

Now let us deﬁne a d dimensional array θ(cid:48) ∈ Ld,n satisfying for any a ∈ [n]d the following:

(cid:48)

θ

(a) = (cid:101)θ(aS).

Then we have k(0)

rdp(θ(cid:48)) = k(0)

rdp((cid:101)θ). We also have

(8.10)

(cid:48)

(cid:107)θ

− θ∗(cid:107)2 = nd−s(cid:107)(cid:101)θ − θ∗

S(cid:107)2 ≤ Cnd−sδ log N TV(θ∗

S).

We can now upper bound R(θ∗) by setting θ = θ(cid:48) in the inﬁmum and since δ > 0 was
arbitrary we can actually write

(8.11)

R(θ∗) ≤ C inf
δ>0

(cid:0)nd−sδ log N TV(θ∗

C nd−sV ∗

S log N inf
δ>0

(cid:0)δ +

where in the last inequality we have set δ = σS.

S) + σ2 log N
σ2
S
δ

(cid:1) = C nd−sV ∗

TV(θ∗
S)
δ

(cid:1) =

S σS log N.

Now, let us deﬁne θ∗
all entries of θ∗
the mean of all entries of θ∗
by setting θ = θ(cid:48) in the inﬁmum to obtain

S as the constant s dimensional array with the value being the mean of
S. Deﬁne a constant d dimensional array θ(cid:48) ∈ Ld,n where every entry is again
rdp(θ(cid:48)) = 1. In this case, we can bound R(θ∗)

S. Then we have k(0)

(8.12)

R(θ∗) ≤ C(cid:0)nd−s(cid:107)θ∗

S − θ∗

S(cid:107)2 + σ2 log N (cid:1) ≤ Cnd−s (cid:0)(V ∗

S )2 + σ2
S

(cid:1)

where in the last inequality we have again used Proposition 8.7.

Also, by setting θ = θ∗ in the inﬁmum and noting that k(0)

rdp(θ∗) ≤ ns we can write

(8.13)

R(θ∗) ≤ nsσ2 log N ≤ ndσ2

S log N.

Combining the three bounds given in (8.11), (8.12) and (8.13) ﬁnishes the proof of the
theorem in the case when s ≥ 2.

When s = 1 the proof goes along exactly similar lines except there is one main diﬀerence.
In place of using Proposition 8.7 we now have to use Lemma 9.3, stated and proved in
Section 9. We leave the details of this case to be veriﬁed by the reader.

8.5. Proof of Theorem 4.1. Consider the Gaussian mean estimation problem y = θ∗ + σZ
where θ∗ ∈ Kd,n(V ). Let us denote the minimax risk of this problem under squared error
loss by R(V, σ, d, n). A lower bound for R(V, σ, d, n) is already known in the literature when
d ≥ 2 and is due to Theorem 2 in Sadhanala et al. (2016).

Theorem 8.8.
d ≥ 2. Let N = nd. Then there exists positive universal constant c such that we

[Sadhanala et al] Let V > 0, σ > 0 and let n, d be positive integers with

R(V, σ, d, n) = inf

(cid:101)θ∈RLd,n

sup
θ∈Kd,n(V )

Eθ(cid:107)(cid:101)θ − θ(cid:107)2 ≥ c min{

…

1 + log(

σ V
2d

2 σ d N
V

), N σ2,

V 2
d2 + σ2}.

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

49

We are now ready to proceed with the proof.

Proof. Without loss of generality, let S = {1, 2, . . . , s} where s = |S|. For a generic array
θ ∈ Ld,n let us deﬁne for any a ∈ [n]s,

θS(a1, . . . , as) =

1
nd−s

(cid:88)

i1,...,id−s∈[n]d−s

θ(a1, . . . , as, i1, . . . , id−s).

In words, θS ∈ Ln,s is a s dimensional array obtained by averaging θ over the d − s coordi-
nates of Sc.

For any θ∗ ∈ KS
we observe yS = θ∗
the parameter space is Ks,n(VS) and the noise variance is σ2
minimax risk under (s dimensional) squared error by R(VS, σS, s, n)

d,n(V ) we can consider the reduced s dimensional estimation problem where
S + σZS. This is a s dimensional version of our estimation problem where
S. Hence we can denote its

By suﬃciency principle, nd−s multiplied by the minimax risk under (s dimensional) squared
error loss for this reduced problem is equal to the minimax risk of our original d dimensional
problem under the d dimensional squared error loss. That is,

inf
(cid:101)θ(y)∈RLd,n

sup
d,n(V )

θ∈KS

Eθ(cid:107)(cid:101)θ(y) − θ(cid:107)2 = nd−s

inf
(cid:101)θ(yS )∈RLs,n
= nd−sR(VS, σS, s, n).

sup
θ∈Ks,n(VS )

Eθ(cid:107)(cid:101)θ(yS) − θ(cid:107)2

We can now invoke Theorem 8.8 to ﬁnish the proof when s ≥ 2. When s = 1 we note that
the space Mn,V = {θ ∈ Rn : 0 ≤ θ1 · · · ≤ θn ≤ V } ⊂ K1,n(V ). We can now use an existing
minimax lower bound for vector estimation in Mn,V given in Theorem 2.7 in Chatterjee
and Laﬀerty (2018) to ﬁnish the proof.

8.6. Proof of Theorem 5.1. We ﬁrst prove the following proposition about approximation
of a vector in BV (r)

n by a piecewise polynomial vector.

Proposition 8.9. Fix a positive integer r and θ ∈ Rn, and let V r(θ) := V. For any δ > 0,
there exists a θ(cid:48) ∈ Rn such that
a) k(r)
rdp(θ(cid:48)) ≤ Crδ−1/r for a constant Cr depending only on r, and
b) |θ − θ(cid:48)|∞ ≤ V δ where | · |∞ denotes the usual (cid:96)∞-norm of a vector.

Remark 8.6. The above proposition is a discrete version of an analogous result for func-
tions deﬁned on the continuum in Birman and Solomjak (1967). The proof uses a recursive
partitioning scheme and invokes abstract Sobolev embedding theorems which are not appli-
cable to the discrete setting verbatim. We found that we can write a simpler proof for the
discrete version which we now present.

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

50

CHATTERJEE, S. AND GOSWAMI S.

8.7. Proof of Proposition 8.9. We ﬁrst need a lemma quantifying the error when approx-
imating an arbitrary vector θ by a polynomial vector θ(cid:48). This is the content of our next
lemma. Recall that a vector θ is said to be a polynomial of degree r if θ ∈ F (r)
1,n where F (r)
has been deﬁned earlier.

d,n

Lemma 8.10. For any θ ∈ Rn there exists a r − 1 degree polynomial θ(cid:48) such that

(8.14)

(cid:48)

|θ − θ

|∞ ≤ Crnr−1|D(r)(θ)|1 = CrV (r)(θ).

Proof. Any vector α ∈ Rn can be expressed in terms of D(r)(α) and D(j−1)(α)1 for
j = 1, 1 . . . , r as follows:

(8.15)

αi =

i−r
(cid:88)

j=1

å

Çi − j − 1
r − 1

(D(r)(α)j) +

Çi − 1
j − 1

r
(cid:88)

j=1

å

D(j−1)(α)1

where the convention is that (cid:0)a
b
side is 0 unless i > r. This result appears as Lemma D.2 in Guntuboyina et al. (2020).

(cid:1) = 1 and the ﬁrst term in the right hand
(cid:1) = 0 for b > a, (cid:0)0
0

Let us deﬁne θ(cid:48) to be the unique r − 1 degree polynomial vector such that the following
holds for all j ∈ {1, 2, . . . , r},

D(j−1)(θ

(cid:48)

)1 = D(j−1)(θ)1.

Now we apply (8.15) to the vector θ − θ(cid:48) to obtain

(8.16)

(θ − θ

(cid:48)

)i =

i−r
(cid:88)

j=1

å

Çi − j − 1
r − 1

(D(r)(θ)j) ≤ Crnr−1|D(r)(θ)|1.

The ﬁrst equality follows from the last display and the fact that D(r)(θ − θ(cid:48)) = D(r)(θ)
(since D(r) is a linear operator and θ(cid:48) is a r − 1 degree polynomial). The last inequality
follows by using the simple bound (cid:0)n
k

(cid:1) ≤ nk for arbitrary positive integers n, k.

We are now ready to proceed with the proof.

Proof of Proposition 8.9. For the sake of clean exposition, we assume n is a power of
2. The reader can check that the proof holds for arbitrary n as well. For an interval I ⊂ [n]
let us deﬁne

M(I) = |I|r−1|D(r)θI |1

where |I| is the cardinality of I and θI is the vector θ restricted to the indices in I. Let us
now perform recursive dyadic partitioning of [n] according to the following rule. Starting
with the root vertex I = [n] we check whether M(I) ≤ V δ. If so, we stop and the root
becomes a leaf. If not, divide the root I into two equal nodes or intervals I1 = [n/2] and

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

51

I2 = [n/2 + 1 : n]. For i = 1, 2 we now check whether M(Ij) ≤ V δ for j = 1, 2. If so, then
this node becomes a leaf otherwise we keep partitioning. When this scheme halts, we would
be left with a Recursive Dyadic Partition of [n] which are constituted by disjoint intervals.
Let’s say there are k of these intervals denoted by B1, . . . , Bk. By construction, we have
M(Bi) ≤ V δ. We can now apply Lemma 8.10 to θBi and obtain a degree r − 1 polynomial
vector vi ∈ R|Bi| such that |θBi − vi|∞ ≤ V δ. Then we can append the vectors vi to deﬁne a
= vi. Thus, we have |θ − θ(cid:48)|∞ ≤ V δ. Note that, by deﬁnition,
vector θ(cid:48) ∈ Rn satisfying θ(cid:48)
Bi
k(r−1)
rdp

(θ(cid:48)) = k. We now need to show that k ≤ Crδ−1/r.

Let us rewrite M(I) = ( |I|
n
I1, I2, . . . , Ik we have by sub-additivity of the functional V r(θ),

)nr−1|D(r)θI |1. Note that for arbitrary disjoint intervals

r−1

(8.17)

nr−1|D(r)θIj |1 ≤ V r(θ) = V.

(cid:88)

j∈[k]

The entire process of obtaining our recursive partition of [n] actually happened in several
rounds. In the ﬁrst round, we possibly partitioned the interval I = [n] which has size
proportion |I|/n = 1 = 2−0. In the second round, we possibly partitioned intervals having
size proportion 2−1. In general, in the (cid:96) th round, we possibly partitioned intervals having
size proportion 2−(cid:96). Let n(cid:96) be the number of intervals with size proportion 2−(cid:96) that we
divided in round (cid:96). Let us count and give an upper bound on n(cid:96). If we indeed partitioned
I with size proportion 2−(cid:96) then by construction this means

(8.18)

nr−1|D(r)θI |1 >

V δ
2−(cid:96)(r−1)

.

Therefore, by sub-additivity as in (8.17) we can conclude that the number of such divisions
is at most 2−(cid:96)(r−1)
. On the other hand, note that clearly the number of such divisions is
bounded above by 2(cid:96). Thus we conclude

δ

n(cid:96) ≤ min{

2−(cid:96)(r−1)
δ

, 2(cid:96)}.

Therefore, we can assert that

(8.19)

k = 1 +

∞
(cid:88)

l=0

n(cid:96) ≤

∞
(cid:88)

(cid:96)=0

min{

2−(cid:96)(r−1)
δ

, 2(cid:96)} ≤ Crδ−1/r.

In the above, we set n(cid:96) = 0 for (cid:96) exceeding the maximum number of rounds of division
possible. The last summation can be easily performed as there exists a nonnegative integer
(cid:96)∗ = O(δ−1/r) such that

min{

2−(cid:96)(r−1)
δ

, 2(cid:96)} =

(cid:40)

2(cid:96),
2−(cid:96)(r−1)
δ

for (cid:96) < (cid:96)∗
for (cid:96) ≥ (cid:96)∗

This ﬁnishes the proof.

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

52

CHATTERJEE, S. AND GOSWAMI S.

We can now ﬁnish the proof of Theorem 5.1.

Proof of Theorem 5.1. As in the proof of Theorem 4.2, it suﬃces to upper bound

R(θ∗) = inf

θ∈RL1,n

(cid:2)(cid:107)θ − θ∗(cid:107)2 + σ2 log n k(r)

rdp(θ)(cid:3).

By Proposition 8.9, we obtain

R(θ∗) ≤ inf
δ>0

(cid:2)nV 2δ2 + Crδ−1/rσ2 log n(cid:3).

Setting δ = c( σ2V 1/r log n

n

)2r/(2r+1) for an appropriate constant c ﬁnishes the proof.

8.8. Proof of Lemma 3.1. We will ﬁrst need the following lemma. Let (cid:107).(cid:107)0 denote the usual
(cid:96)0 norm equal to the number of non zero entries.

Lemma 8.11. Fix any positive integers n, d and 1 ≤ k ≤ N = nd. Then for any θ ∈ RLd,n,
k(0)
hier(θ) ≤ 3d(cid:107)θ(cid:107)0.

Proof. We will prove the lemma via induction on the dimension d. Let us start with the
base case d = 1. Let 1 ≤ i1 < . . . < i(cid:107)θ(cid:107)0 ≤ n be the support of θ (i.e. the points where
θ has nonzero value). Then θ is constant on each interval of the (hierarchical) partition
Π := {{1, . . . , i1 − 1}, {i1}, . . . , {i(cid:107)θ0−1(cid:107) + 1, . . . , i(cid:107)θ0(cid:107) − 1}, {i(cid:107)θ0(cid:107)}} of L1,n and consequently
khier(θ) ≤ 3(cid:107)θ(cid:107)0. Now suppose the statement holds for some d ≥ 1. Let θ ∈ RLd+1,n and
1 ≤ i1 < . . . < ik0 ≤ n be the horizontal coordinates of the support of θ. Evidently
k0 ≤ (cid:107)θ(cid:107)0. Now, by our induction hypothesis, for every j ∈ [k0], there exists a hierarchical
partition Πj of Ld,n;j := {ij} × [n]d such that θ restricted to Ld,n;j — which we refer to as θj
in the sequel — is constant on each rectangle of Πj and |Πj| ≤ 3d(cid:107)θj(cid:107)0. Then θ is constant
on each rectangle of the partition formed by Πj; j ∈ [k0] and the rectangles {1, . . . , i1 −
1} × [n]d, . . . , {i(cid:107)k0−1(cid:107) + 1, . . . , i(cid:107)k0(cid:107) − 1} × [n]d. Since Π is a hierarchical reﬁnement of the
partition comprising the rectangles {1, . . . , i1 − 1} × [n]d, Ld,n;1, . . . , {i(cid:107)k0−1(cid:107) + 1, . . . , i(cid:107)k0(cid:107) −
1}×[n]d, Ld,n;k0 which is clearly hierarchical, it follows that Π is itself a hierarchical partition.
Finally, notice that

|Π| ≤

(cid:88)

j∈[k0]

|Πj| + 2k0 ≤

(cid:88)

j∈[k0]

3d(cid:107)θj(cid:107)0 + 2(cid:107)θ(cid:107)0 ≤ 3(d + 1)(cid:107)θ(cid:107)0,

thus concluding the induction step.

Now we can ﬁnish the proof of Lemma 3.1.

Proof. Recall that Θk,d,n := {θ ∈ RLd,n : k(0)
RLd,n : (cid:107)θ(cid:107)0 ≤ k}. Then, by Lemma 8.11 we have the set inclusion for any 1 ≤ k ≤ N ,

hier(θ) ≤ k}. Let us denote Θ(sparse)

k

= {θ ∈

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

53

Θ(sparse)

k

⊂ Θ3dk,d,n.

A minimax lower bound (for the squared error) for the parameter space Θ(sparse)
tight up to constants is Cσ2k log eN
instance as Corollary 4.15 in Rigollet and H¨utter (2015). This ﬁnishes the proof.

which is
k . This result is very well known and can be found for

k

9. Auxiliary Results.

Lemma 9.1. Let Z ∼ N (0, In) and S ⊂ Rn denote a subspace. Let Σ be a positive semi-
deﬁnite matrix and Σ1/2 denote its usual square root. Also, let (cid:107)Σ1/2(cid:107)op denote the (cid:96)2-
operator norm of Σ1/2, i.e. the square root of the maximum eigen value of Σ. Then the
following holds for every θ ∈ Rn:

E sup

(cid:104)Σ1/2Z,

v∈S, v(cid:54)=θ

v − θ
(cid:107)v − θ(cid:107)

(cid:105) ≤ (cid:107)Σ1/2(cid:107)op(Dim(S)1/2 + 1).

Also, for any u > 0, we have with probability at least 1 − 2 exp(− u2

2 ),

(9.1)

(cid:0) sup
v∈S,v(cid:54)=θ

(cid:104)Z,

v − θ
(cid:107)v − θ(cid:107)

(cid:105)(cid:1)2 ≤ (cid:107)Σ1/2(cid:107)op

(cid:2)2 Dim(S) + 4 (1 + u2)(cid:3).

Proof. We can write

sup
v∈S, v(cid:54)=θ

(cid:104)Σ1/2Z,

≤ sup

(cid:104)Σ1/2Z,

v∈S, v(cid:54)=θ

v − θ
(cid:107)v − θ(cid:107)

(cid:105) = sup

(cid:104)Σ1/2Z,

v∈S, v(cid:54)=θ
v − OSθ
(cid:112)|v − OSθ|2 + |(I − OS)θ|2

v − OSθ − (I − OS)θ
(cid:112)|v − OSθ|2 + |(I − OS)θ|2

(cid:105)

(cid:105) + sup

(cid:104)Σ1/2Z,

v∈S, v(cid:54)=θ

(I − OS)θ
(cid:112)|v − OSθ|2 + |((I − OS)θ|2

(cid:105)

≤ sup

(cid:104)Σ1/2Z, v(cid:105) + sup

(cid:104)Σ1/2Z, v(cid:105)

v∈S:(cid:107)v(cid:107)≤1

v∈S(cid:48):(cid:107)v(cid:107)≤1

where S(cid:48) is the subspace spanned by the vector (I − OS)θ.

Now we will give a bound on the expectation of both the terms above separately. Let us
work with the ﬁrst term. First note that

sup
v∈S:(cid:107)v(cid:107)≤1

(cid:104)Σ1/2Z, v(cid:105) = sup

(cid:104)OSΣ1/2Z, v(cid:105) = (cid:107)OSΣ1/2Z(cid:107).

v∈S:(cid:107)v(cid:107)≤1

Secondly,

(cid:0)E(cid:107)OSΣ1/2Z(cid:107)(cid:1)2 ≤ E(cid:107)OSΣ1/2Z(cid:107)2 = E ZtΣ1/2OSΣ1/2Z =
T race(Σ1/2OSΣ1/2) ≤ (cid:107)Σ(cid:107)opT race(OS) = (cid:107)Σ(cid:107)opDim(S).

where the second last equality follows from standard facts about Gaussian quadratic forms
and the last inequality follows from the following reasoning.

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

54

CHATTERJEE, S. AND GOSWAMI S.

Consider the spectral decomposition of Σ = P DP t where P is an orthonormal matrix and
D is a diagonal matrix consisting of eigenvalues of Σ. Then, by commutativity of trace, we
have

T race(Σ1/2OSΣ1/2) = T race(ΣOS) = T race(P DP tOS) =
T race(DP tOSP ) ≤ (cid:107)D(cid:107)opT race(P tOSP ) = (cid:107)Σ(cid:107)opT race(OSP P t) = (cid:107)Σ(cid:107)opT race(OS).

Therefore, we can say that

E sup

(cid:104)Σ1/2Z, v(cid:105) ≤ (cid:107)Σ1/2(cid:107)opDim(S)1/2.

v∈S:(cid:107)v(cid:107)≤1

Similarly, for the other term we get

E

sup
v∈S(cid:48):(cid:107)v(cid:107)≤1

(cid:104)Σ1/2Z, v(cid:105) ≤ (cid:107)Σ1/2(cid:107)opDim(S(cid:48))1/2 ≤ (cid:107)Σ1/2(cid:107)op.

This proves the ﬁrst part of the lemma.

Coming to the second part, note that, by symmetry we also have the lower bound

E sup

(cid:104)Σ1/2Z,

v∈S,v(cid:54)=θ

v − θ
(cid:107)v − θ(cid:107)

(cid:105) ≥ E inf

v∈S,v(cid:54)=θ

(cid:104)Σ1/2Z,

v − θ
(cid:107)v − θ(cid:107)

(cid:105) ≥ −(cid:107)Σ1/2(cid:107)op

(cid:0)Dim(S)1/2 + 1(cid:1).

Now we note that supv∈S, v(cid:54)=θ(cid:104)Σ1/2Z, v−θ
(cid:107)v−θ(cid:107) (cid:105) is a Lipschitz function of Z with Lipschitz
constant (cid:107)Σ1/2(cid:107)op. Thus we can use the well-known Gaussian Concentration inequality
(see, e.g. (Ledoux, 2001, Theorem 7.1)), which is stated as Theorem 9.2 for the convenience
of the reader, to conclude that for any u > 0, with probability at least 1 − 2 exp(−u2/2) we
have

(cid:104)Z,

| sup
v∈S

v − θ
(cid:107)v − θ(cid:107)

(cid:105)| ≤ (cid:107)Σ1/2(cid:107)op |

Dim(S) + 1 + u|

»

Using the elementary inequality (a + b)2 ≤ 2a2 + 2b2 now ﬁnishes the proof of (9.1).

Theorem 9.2. Let Z1, Z2, . . . , Zm be independent standard Gaussian variables and f :
Rm (cid:55)→ R be a Lipschitz function with Lipschitz constant 1. Then Ef (Z1, . . . , Zm) is ﬁnite
and

P (|f (Z1, . . . , Zm) − Ef (Z1, . . . , Zm)| > t) ≤ 2 exp−t2/2

for all t ≥ 0.

The following lemma appears as Lemma 7.3 in Chatterjee and Goswami (2019).

Lemma 9.3. Let θ ∈ Rn. Let us deﬁne θ = ((cid:80)n
inequality:

i=1 θi)/n. Then we have the following

n
(cid:88)

(cid:0)θi − θ(cid:1)2 ≤ nTV(θ)2 .

i=1

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

ADAPTIVE ESTIMATION BY OPTIMAL DECISION TREES

55

References.

Berman, P., B. DasGupta, and S. Muthukrishnan (2002). Exact size of binary space partitionings and

improved rectangle tiling algorithms. SIAM Journal on Discrete Mathematics 15 (2), 252–267.

Bertin, K. and G. Lecu´e (2008). Selection of variables and dimension reduction in high-dimensional non-

parametric regression. Electronic Journal of Statistics 2, 1224–1241.

Bertsimas, D. and J. Dunn (2017). Optimal classiﬁcation trees. Machine Learning 106 (7), 1039–1082.
Birman, M. S. and M. Z. Solomjak (1967). Piecewise-polynomial approximation of functions of the classes

wp. Mathematics of the USSR Sbornik 73, 295–317.

Blanchard, G., C. Sch¨afer, Y. Rozenholc, and K.-R. M¨uller (2007). Optimal dyadic decision trees. Machine

Learning 66 (2-3), 209–241.

Boysen, L., A. Kempe, V. Liebscher, A. Munk, and O. Wittich (2009). Consistencies and rates of convergence

of jump-penalized least squares estimators. The Annals of Statistics 37 (1), 157–183.

Breiman, L., J. Friedman, R. Olshen, and C. Stone (1984). Classiﬁcation and regression trees. wadsworth

int. Group 37 (15), 237–251.

Chatterjee, S. (2016). An improved global risk bound in concave regression. Electronic Journal of Statis-

tics 10 (1), 1608–1629.

Chatterjee, S. and S. Goswami (2019). New risk bounds for 2d total variation denoising. arXiv preprint

arXiv:1902.01215 .

Chatterjee, S., A. Guntuboyina, and B. Sen (2015). On risk bounds in isotonic and other shape restricted

regression problems. The Annals of Statistics 43 (4), 1774–1800.

Chatterjee, S., A. Guntuboyina, and B. Sen (2018). On matrix estimation under monotonicity constraints.

Bernoulli 24 (2), 1072–1100.

Chatterjee, S. and J. Laﬀerty (2018). Denoising ﬂows on trees. IEEE Transactions on Information The-

ory 64 (3), 1767–1783.

Chaudhuri, P., M.-C. Huang, W.-Y. Loh, and R. Yao (1994). Piecewise-polynomial regression trees. Statistica

Sinica, 143–167.

Dalalyan, A., M. Hebiri, and J. Lederer (2017). On the prediction performance of the lasso. Bernoulli 23 (1),

552–581.

de Berg, M. (1995). Linear size binary space partitions for fat objects. In European Symposium on Algorithms,

pp. 252–263. Springer.

Deng, H. and C.-H. Zhang (2018). Isotonic regression in multi-dimensional spaces and graphs. arXiv preprint

arXiv:1812.08944 .

Donoho, D. L. (1997). CART and best-ortho-basis: a connection. The Annals of Statistics 25 (5), 1870–1911.
Donoho, D. L. and I. M. Johnstone (1994). Ideal spatial adaptation by wavelet shrinkage. Biometrika 81 (3),

425–455.

Donoho, D. L. and I. M. Johnstone (1998). Minimax estimation via wavelet shrinkage. The Annals of

Statistics 26 (3), 879–921.

Friedman, J., T. Hastie, and R. Tibshirani (2001). The elements of statistical learning, Volume 1. Springer

series in statistics New York.

Gey, S. and E. Nedelec (2005). Model selection for cart regression trees. IEEE Transactions on Information

Theory 51 (2), 658–670.

Guntuboyina, A., D. Lieu, S. Chatterjee, and B. Sen (2020). Adaptive risk bounds in univariate total

variation denoising and trend ﬁltering. The Annals of Statistics 48 (1), 205–229.

Guntuboyina, A. and B. Sen (2015). Global risk bounds and adaptation in univariate convex regression.

Probability Theory and Related Fields 163 (1-2), 379–411.

Han, Q., T. Wang, S. Chatterjee, and R. J. Samworth (2019). Isotonic regression in general dimensions. The

Annals of Statistics 47 (5), 2440–2471.

Hershberger, J., S. Suri, and C. D. T´oth (2005). Binary space partitions of orthogonal subdivisions. SIAM

Journal on Computing 34 (6), 1380–1397.

H¨utter, J.-C. and P. Rigollet (2016). Optimal rates for total variation denoising. In Conference on Learning

Theory, pp. 1115–1146.

Ishwaran, H. (2015). The eﬀect of splitting on random forests. Machine Learning 99 (1), 75–118.

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

56

CHATTERJEE, S. AND GOSWAMI S.

Kim, S.-J., K. Koh, S. Boyd, and D. Gorinevsky (2009). (cid:96)1 trend ﬁltering. SIAM Rev. 51 (2), 339–360.
Laurent, H. and R. L. Rivest (1976). Constructing optimal binary decision trees is np-complete. Information

processing letters 5 (1), 15–17.

Lecu´e, G. (2008). Classiﬁcation with minimax fast rates for classes of bayes rules with sparse representation.

Electronic Journal of Statistics 2, 741–773.

Ledoux, M. (2001). The concentration of measure phenomenon, volume 89 of mathematical surveys and

monographs. American Mathematical Society, Providence, RI.

Leoni, G. (2017). A ﬁrst course in Sobolev spaces. American Mathematical Soc.
Loh, W.-Y. (2014). Fifty years of classiﬁcation and regression trees. International Statistical Review 82 (3),

329–348.

Mammen, E. and S. van de Geer (1997). Locally adaptive regression splines. The Annals of Statistics 25 (1),

387–413.

Mangasarian, O. L. and L. L. Schumaker (1971). Discrete splines via mathematical programming. SIAM

Journal on Control 9 (2), 174–183.

Nemirovski, A. (2000). Topics in non-parametric statistics. Ecole d’Et´e de Probabilit´es de Saint-Flour 28,

85.

Nowak, R., U. Mitra, and R. Willett (2004). Estimating inhomogeneous ﬁelds using wireless sensor networks.

IEEE Journal on Selected Areas in Communications 22 (6), 999–1006.

Ortelli, F. and S. van de Geer (2018). On the total variation regularized estimator over a class of tree graphs.

Electron. J. Statist. 12 (2), 4517–4570.

Oymak, S. and B. Hassibi (2013). Sharp MSE bounds for proximal denoising. Foundations of Computational

Mathematics, 1–65.

Rigollet, P. and J.-C. H¨utter (2015). High dimensional statistics. Lecture notes for course 18S997 .
Rudin, L. I., S. Osher, and E. Fatemi (1992). Nonlinear total variation based noise removal algorithms.

Physica D: Nonlinear Phenomena 60 (1), 259–268.

Sadhanala, V., Y.-X. Wang, and R. J. Tibshirani (2016). Total variation classes beyond 1d: Minimax rates,
In Advances in Neural Information Processing Systems, pp.

and the limitations of linear smoothers.
3513–3521.

Scornet, E., G. Biau, and J.-P. Vert (2015). Consistency of random forests. The Annals of Statistics 43 (4),

1716–1741.

Scott, C. and R. D. Nowak (2006). Minimax-optimal classiﬁcation with dyadic decision trees. IEEE trans-

actions on information theory 52 (4), 1335–1353.

Steidl, G., S. Didas, and J. Neumann (2006). Splines in higher order tv regularization. International journal

of computer vision 70 (3), 241–255.

Tibshirani, R. (2015). Nonparametric regression (and classiﬁcation).
Tibshirani, R. J. (2014). Adaptive piecewise polynomial estimation via trend ﬁltering. The Annals of

Statistics 42 (1), 285–323.

T´oth, C. D. (2005). Binary space partitions: recent developments. Combinatorial and Computational Ge-

ometry. MSRI Publications 52, 529–556.

van de Geer, S. and F. Ortelli (2019). Prediction bounds for (higher order) total variation regularized least

squares. arXiv preprint arXiv:1904.10871 .

Wager, S. and G. Walther (2015). Adaptive concentration of regression trees, with application to random

forests. arXiv preprint arXiv:1503.06388 .

Wang, Y.-X., A. J. Smola, and R. J. Tibshirani (2014). The falling factorial basis and its statistical appli-

cations. In ICML, pp. 730–738.

Willett, R. M. and R. D. Nowak (2007). Multiscale poisson intensity and density estimation. IEEE Trans-

actions on Information Theory 53 (9), 3171–3187.

Winkler, G. and V. Liebscher (2002). Smoothers for discontinuous signals. Journal of Nonparametric

Statistics 14 (1-2), 203–222.

Wu, F.-Y. (1982). The potts model. Reviews of modern physics 54 (1), 235.

imsart-aos ver. 2014/02/20 file: CART_Arxiv.tex date: January 18, 2021

