2
2
0
2

y
a
M
6
2

]

R
A
.
s
c
[

1
v
5
7
6
3
1
.
5
0
2
2
:
v
i
X
r
a

Reinforcement Learning Approach for Mapping
Applications to DataÔ¨Çow-Based Coarse-Grained
ReconÔ¨Ågurable Array

Andre Xian Ming Chang‚àó, Parth Khopkar‚àó, Bashar Romanous, Abhishek Chaurasia, Patrick Estep,
Skyler Windh, Doug Vanesko, Sheik Dawood Beer Mohideen, Eugenio Culurciello
{andrexianmin, pkhopkar, basharromano, achaurasia, pestep, swindh, dvanesko}@micron.com
Micron Technology, Inc.

Abstract‚ÄîThe Streaming Engine (SE) is a Coarse-Grained
ReconÔ¨Ågurable Array which provides programming Ô¨Çexibility
and high-performance with energy efÔ¨Åciency. An application
program to be executed on the SE is represented as a combination
of Synchronous Data Flow (SDF) graphs, where every instruction
is represented as a node. Each node needs to be mapped to the
right slot and array in the SE to ensure the correct execution of
the program. This creates an optimization problem with a vast
and sparse search space for which Ô¨Ånding a mapping manually
is impractical because it requires expertise and knowledge of the
SE micro-architecture. In this work we propose a Reinforcement
Learning framework with Global Graph Attention (GGA) mod-
ule and output masking of invalid placements to Ô¨Ånd and optimize
instruction schedules. We use Proximal Policy Optimization in
order to train a model which places operations into the SE
tiles based on a reward function that models the SE device
and its constraints. The GGA module consists of a graph neural
network and an attention module. The graph neural network
creates embeddings of the SDFs and the attention block is used
to model sequential operation placement. We show results on
how certain workloads are mapped to the SE and the factors
affecting mapping quality. We Ô¨Ånd that the addition of GGA,
on average, Ô¨Ånds 10% better instruction schedules in terms of
total clock cycles taken and masking improves reward obtained
by 20%.

Index Terms‚Äîreinforcement
coarse-grained reconÔ¨Ågurable array

learning, data-Ô¨Çow mapping,

I. INTRODUCTION

As Dennard scaling ends, big-data applications such as real-
time image processing, graph analytics, and deep learning
continue to push the boundaries of performance and energy
efÔ¨Åciency requirements for computing system. One solution
to this challenge is to move compute closer to memory or
storage for substantial energy savings of data movement. We
have developed an innovative Near-Data Computing (NDC)
architecture that leverages the dramatic opportunities provided
by the new CXL protocol [1]. NDC incorporates heterogenous
compute elements in the memory/storage subsystem to accel-
erate various computing tasks near data. One of these compute
elements is the Streaming Engine (SE).

‚àó

equal contribution

Code available at https://github.com/micronDLA/RL streaming engine

The SE is a Coarse-Grained ReconÔ¨Ågurable Array (CGRA)
that is composed of interconnected compute tiles. The compute
tiles are interconnected with both a Synchronous Fabric (SF)
and an Asynchronous Fabric (AF). The SF enables neighbor-
ing tiles to be pipelined, forming an Synchronous Data Flow
(SDF). The AF connects each tile with all the other tiles, as
well as, the dispatch interface (DI), and memory interfaces
(MI). Together, the SF and AF allow the tiles to efÔ¨Åciently
execute high-level programming language constructs. Simu-
lation results of hand-crafted SE kernels have shown orders-
of-magnitude better performance per watt on data-intensive
applications than existing computing platforms.

A simple example that illustrates a program mapped to the
SE is shown in Fig. 1. The program in this example is a
distance calculation function shown in Eq. 1.

D = (cid:112)(x ‚àí x0)2 + (y ‚àí y0)2 + (z ‚àí z0)2

(1)

To keep this example simple, we ignore the square root part
of the equation. The program is represented as an SDF graph
as shown in Fig. 1. Each operation in Eq. 1 is an instruction
that is mapped to a slot at a tile on the SE.

Since the output of the MS unit is connected to the input
of the AL unit, instruction #6 produces ‚àÜz2 on the output on
the MS unit and adds it to (‚àÜx2 + ‚àÜy2) at the AL unit.

Mapping the instructions from a program‚Äôs SDFs onto
the compute elements of the SE while adhering to archi-
tectural constraints is an NP-hard problem with a vast and
sparse search space [2]. Constraints related to tile memory,
synchronous dataÔ¨Çow,
the use of delays to match timing
requirements and more are necessary to ensure correct exe-
cution. Creating the mappings manually or using brute force
algorithms takes time and lots of effort even for the simplest
of programs. This process also adds assumptions that reduce
the search space, trading off optimization possibilities.

In this work we propose a deep Reinforcement Learning
(RL) method to explore and Ô¨Ånd optimal mappings in an
unsupervised manner. Using the Proximal Policy Optimization
(PPO) method we train a neural network model
to place
instructions onto the SE tiles guided by a reward function in an

 
 
 
 
 
 
masking and iteration order.

This work is motivated by the need to provide an improved
SE toolset that lowers the SE usability barrier for a program-
mer. In addition, the proposed mapper can be used along with
other SE tools or assist in the manual mapping of applications
to the SE by providing partial placement suggestions or tile
conÔ¨Åguration labels. The RL mapper performs unsupervised
learning and optimization allowing it to search a wide and
sparse search space. Each node in the SDF is an instruction
that is mapped to a speciÔ¨Åc slot on a tile.

This line of research is inspired by recent work that used RL
for chip placement [3]. The problem requirement of mapping
nodes of a SDF to available compute elements is similar to the
problem of placing nodes of a chip netlist on a chip canvas.
The rest of this paper is organized as follows: Section II
discusses related work. Section III provides the necessary
background on the SE device architecture needed to under-
stand the details of the RL approach. Section IV presents
the proposed RL approach for mapping applications to the
SE. Section V discusses results, and Section VI concludes the
paper along with a discussion on future work.

II. RELATED WORK

A. CGRA

. CGRAs are a heavily researched architectural paradigm
with a long history and can provide an excellent balance of
high-performance compute, memory bandwidth, and area and
energy efÔ¨Åciency [4]. Due to such advantages, CGRAs are
currently enjoying a resurgence in interest, not just in the
research realm [5], but also commercially [6]‚Äì[8]. Research
compilers targeting CGRAs are available (for example, [5],
[9]‚Äì[11]) but they are limited in quality and code coverage.
Industry-strength compilers such as Clang/LLVM do not pro-
vide ofÔ¨Åcial support for CGRA-like architectures. For further
information on the various taxonomy of CGRAs architectures
and design, we refer the reader to [12], [13].

Mapping various programming constructs onto CGRAs is an
extensive research topic. The mapping of large and irregular
loops onto CGRAs is analyzed in [14] which proposed paying
more attention to temporal mapping than spatial mapping.
The proposed temporal mapping utilizes a buffer allocating
heuristic with constraint on computations and interconnection
resources. The proposed spatial mapping uses a backtracking
and reordering mechanism with greedy algorithm with the
goal of minimizing the Initiation Interval (II). A method
for mapping SDFs to a CGRA is introduced in [15]. The
proposed method relies on modulo-scheduling to provide
a spatio-temporal mapping. ChordMap creates a schedule
and partitions the SDF and CGRA, then performs spatio-
temporal mapping of every kernel instance in each partition.
ChordMap operates on three levels of parallelism: application-
level, kernel instances-level, and instruction-level parallelism.
In CGRA-ME [10], a CGRA device model is built using
Module Routing Resource Graph (MRRG) [16]. The parser
converts an optimized C-language benchmark into a Data

Fig. 1: Example showing a mapping of a distance calculation
function onto the SE device. The instruction execution latency
is three clock cycles. The solid and dotted lines connecting
the tiles, have one and two clock cycles transfer latency
respectively. E.g., instruction #0 is scheduled at (tile #3, slot
#0) and its output is sent to instruction #3. Instruction #0
output is ready after three clock cycles plus two clock cycles
to move data to tile #1. Thus, instruction #3 gets scheduled
on (tile #1, slot #1).

environment that models the SE device and its constraints. We
also present Global Graph Attention (GGA) module that im-
proves the baseline feedforward models by providing attention
mechanism to the RL model. Our RL method is combined with
output masking, Ô¨Ånetuning and sorted iteration order, which
will be presented in the following sections.

The trained model is able to create valid mappings for the
SE by learning about the structure of the problem domain. On
average addition of GGA Ô¨Ånds 10% better instruction sched-
ules in terms of clock cycles for varying graphs complexities
and different SE device conÔ¨Ågurations.

The key contributions of this paper are as follows:

1) A Reinforcement Learning methodology that is able to
map the instructions from a given application to the
processing elements in the SE and has the potential to
be reused across different applications.

2) An analysis of different factors that impacts the quality
of mappings obtained, such as attention module, output

ApplicationGenerate SDF IRRL MapperSE MappingRL MapperTile 0Tile 1Tile 2Tile 3Flow Graph (DFG) using LLVM compiler framework. Each
operation in the DFG is mapped to a functional unit in the
MMRG using simulated annealing. The routing between inputs
and outputs of each operation is selected using PathFinder-like
algorithm [17].

SE differentiates from others as the Ô¨Årst CGRA in a near-
data computing architecture. SE also provides asynchronous
messaging as a Ô¨Årst-class programming construct along with
an SDF.

B. Reinforcement Learning

RL is widely used to tackle unsupervised optimization prob-
lems. Typically in RL, a policy is a function that outputs the
action that an agent should take in a system given the current
state of the system. It has been applied in chip placement
[3], workload distribution [18]‚Äì[20], compiler optimizations
[21] and other decision based tasks [22], [23]. Alternative
optimization algorithms include evolutionary strategies [24]
and bayesian optimization [25]. An advantage of an RL
approach is that it is able to learn from a collection of programs
and reuse previous data for new programs by training a Deep
Learning model [20]. When RL is used in a deep learning
paradigm, the policy is represented by neural networks.

Recurrent Neural Networks (RNN) [26] were previously a
popular approach to process a sequence of nodes from a graph
representation. Recently, Graph Neural Networks (GNN) [27]
have shown success in processing structured data without
needing the preprocessing required for RNNs. GNNs are
widely used for tasks involving graph processing [20], [21].
Attention modules have sometimes been used in literature
to supplement the embeddings created by GNNs to further
improve results [19].

In [3], an RL based method for chip placement is presented
where a graph neural network is used to create embeddings
from a netlist graph and then passed through an actor model to
get placements on a chip canvas. The training is done using the
PPO approach and one component from the netlist is placed at
each step until all required components have been placed. The
actor network in this method is composed of deconvolution
layers which are more computationally intensive than our
approach. An RL based method, proposed in [28], that uses
a Deep Deterministic Policy Gradient (DDPG) method for
component placement in multi-chip many-core systems. A
review of various machine learning approaches is presented in
[29] which including reinforcement learning for chip design.
The work proposed in [20] presents a combination of graph
neural network and transformer-XL model to place operations
in dataÔ¨Çow graphs on suitable devices. In each of these works,
the aim is similar to ours i.e., to reduce the manual labor and
domain expertise required to produce mappings.

A key difference between our work and [20] is that our
strategy is to place one node at a time, instead of generating
a complete assignment per iteration. This allows our model
to break down the placement problem into sub-problems and
also allows the framework to start from a different initial
conÔ¨Åguration. For example, if some nodes are already placed

by some other algorithm such as brute force, the RL mapper
can place the remaining nodes. This approach also allows us to
obtain more data during the sampling phase which consists of
partial assignments, instead of only saving one training sample
for an entire sequence of nodes.

III. BACKGROUND ON THE SE

The SE is a coarse-grained fabric, shown in Fig. 2a,
composed of compute elements or tiles. These tiles are
interconnected with an SF, allowing data to traverse from
one tile to another without queuing. This SF allows many
tiles to be pipelined together to produce a continuous data
Ô¨Çow through SIMD arithmetic operations. Each tile, inter-
connects local memory, multiplexers, a Multiply/Shift (MS),
and an Arithmetic/Logic (AL) SIMD capable units as shown
in Fig. 2b. The tiles are also interconnected with an AF
that allows synchronous domains of compute to be bridged
by asynchronous operations. These asynchronous operations
include initiating SDF operations, transferring data from one
SDF to another, accessing system memory (read and write),
and performing branching and looping constructs.

As shown in the Fig. 2, tiles are interconnected in a 1 √ó 16
conÔ¨Åguration. The SF is used to connect a tile to another
tile one hop above it, a tile two hops above it, a tile one
hop below it and a tile two hops below it. Information is
transferred over the SF with a deterministic latency. Each tile
acts independently, streaming data through internal memory
and MS/AL units to other tiles over the SF and AF. The tiles
use the AF to communicate between synchronous domains,
send loads and stores to memory through the MI, and receive
commands from the host through the DI to initiate work on
the SE. Information is transferred over the AF with a non-
deterministic latency.

A. Synchronous Fabric Interface

All tiles that participate in a synchronous domain act as
a single pipelined data path. The SDF‚Äôs entry tile is deÔ¨Åned
as the tile that executes the Ô¨Årst instruction of the SDF. It
is responsible for initiating a thread of work through the
pipelined tiles at a predeÔ¨Åned cadence, referred to as the Spoke
Count or II. E.g., For II = 3, as in the example in Fig. 1,
then the entry tile can initiate work every third clock cycle.

B. Asynchronous Fabric interface

The Asynchronous Fabric is used to perform operations
that occur asynchronously to a synchronous domain. Each
tile contains an interface to the AF. AF messages can be
classiÔ¨Åed as either data messages or control. Data messages
contain a SIMD width data value that is written to one of
the two tile memories. Control messages are for controlling
thread creation, freeing resources, or issuing external memory
requests.

C. Tile Base

The tile base contains data structures that are used to initiate
an SDF. If the tile was an entry tile of an SDF, then the tile

D. Tile Memory

Each tile contains two memories. Each are the width of the
data path (512-bits), and the depth will be in the range of 512
to 1024 elements. The tile memories are used to store data
required to support data path operations. The stored data can
be constants loaded as part of the program‚Äôs arguments, or
variables calculated as part of the data Ô¨Çow. The tile memory
can be written from the AF as either a data transfer from
another SDF, or the result of a load operation initiated by
another SDF.

E. Instructions RAM

Each tile has an instruction RAM has multiple entries to
allow a tile to be time sliced, performing multiple, different
operations in a pipelined synchronous domain.

F. Spoke RAM

The Spoke RAM has multiple entries to reconÔ¨Ågure the tile
at each time slice (clock cycle). The number of active spoke
RAM slots in a tile is equal to the II of that tile. A couple of
relevant conÔ¨Ågurations are:

‚Ä¢ Which of the four SF inputs, feedback from the output
of the tile‚Äôs MS/AL unit, or the tile base is the master
input.

‚Ä¢ Which of the four SF outputs are used to send the output

of the MS/AL unit to another tile or tiles using SF.

The Spoke RAM iterates over its slots using a counter that
modulo counts from zero to II minus one and back to zero.
The proposed RL mapper provides the II and the conÔ¨Åguration
of each spoke RAM slot, including which tile instruction will
be active, on each utilized tile.

G. Programming the SE

The SE programmer breaks down the desired application
into a set of one or more SDFs. An SDF is marked by in-
structions sending and receiving data in deterministic latency.
All operations with nondeterministic latencies, e.g., load/store
requests to the MI, mark either the beginning and/or ending
of an SDF. We have written an in-house parser that enables
the programmer to express the program in terms of SDFs
using our deÔ¨Åned assembly language. The parser produces an
Intermediate Representation (IR) that is used as an input to
our proposed mapper.

The IR is a graph representation of the program which
consists of SDF subgraphs. Each SDF is a disconnected
component of the IR graph. Here every instruction is a node.
Each node represents an instruction that needs to be placed
onto a SE tile at the proper time-slice such that when all
nodes are placed, the program executes correctly. In Fig. 1,
the edges of the graph represent the data dependencies of
the instructions. The nodes also contain information about
the variables that need to be present in tile memories during
instruction execution. The instruction intended to execute at a
speciÔ¨Åc time-slice (clock cycle) will only execute when the
master input conÔ¨Ågured in the corresponding Spoke RAM
entry receives a valid control message.

(a) SE device

(b) SE tile

Fig. 2: Fig. 2a shows layout of the SE device and dataÔ¨Çow
paths between tiles. Fig. 2b is a detailed diagram of a tile in
the SE device. Note the two tile memory units, the MS and
AL units, and the four SF inputs and four SF outputs.

base launches a new thread every II. The II allows the base
logic to launch new threads or continue previously launched
threads when all resources needed for execution are available.

TILE 15 (3,3)TILE 14 (3,2)TILE 13 (3,1)TILE 12 (3,0)MITILE 11 (2,3)TILE 10 (2,2)TILE 9 (2,1)TILE 8 (2,0)MITILE 7 (1,3)TILE 6 (1,2)TILE 5 (1,1)TILE 4 (1,0)MITILE 3 (0,3)TILE 2 (0,2)TILE 1 (0,1)TILE 0 (0,0)MI/DINOCNOCNOCNOC Synchronous Fabric (next nearest)Synchronous Fabric  (nearest)Asynchronous FabricNOC FabricCoord -> Tile Index(V,H)  H. SE Mapping Constraints

The SE hardware imposes constraints that the RL mapper
must adhere to in order to for it to produce a valid mapping.
These constraints are:

1) Instructions that share one or more tile memory variables

must be placed on the same tile.

2) No two or more instructions that start an SDF can share

a tile.

3) No two or more instructions that are siblings in the SDF

can share a tile.

The SE device conÔ¨Åguration and its constraints are modeled
in a simulation environment and a reward function is used
to determine the quality of placements obtained. To enforce
these constraints, we explore two methods in Section V-C. The
Ô¨Årst method is to give a negative reward when a constraint
is violated and the second is creation of a mask on the
invalid actions so that the network only outputs valid actions.
In both cases, placing instructions sub-optimally leads to a
reduced reward whereas placements that optimally reduce total
execution time of the graph are rewarded.

Fig. 3: Diagram of the RL framework showing the role of
actor and critic networks during RL training.

‚Ä¢ Action: The action a consists of the node to be placed
along with the tile and spoke location it is to be placed
at.

‚Ä¢ Reward function: The reward obtained is based on the
number of clock cycles taken for a node to Ô¨Ånish execut-
ing after its predecessors Ô¨Ånished executing.

IV. METHOD

C. State Representation

A. Reinforcement learning

We present an RL based framework to explore and optimize
the mapping of instructions for a given application to the
SE, guided by a reward function that informs the mapping
algorithm about the quality of the produced mappings at each
step. In this section, we give an overview of the methods used
along with the formulation required for detailed description of
the various components of the RL approach.

B. Overview of RL Methodology

PPO [30] is an RL method that is widely used for continuous
and discrete action problems. It trains an actor and a critic
model (represented as neural networks). The actor model is
trained to produce actions (node placements in our case) from
sample states obtained during simulation and the critic model
is trained to match the sampled rewards from the actions
produced by the actor using a surrogate loss function. This
sample and train process is repeated over various iterations.
In this manner, the problem search space is explored using
the reward function as a heuristic. Refer to algorithm Alg. 1
for the complete RL training process. Our SE mapping task
is formulated as a discrete action problem where the goal is
to place one node at each time step. Samples of SE state,
actions performed, and rewards obtained at each time step
are collected after executing the actions provided by the actor
model in simulation. These samples are stored in a buffer and
are used as data to train the models. Fig. 3 shows the overall
RL framework.

The key components of our RL method are as follows:
‚Ä¢ States: The state is s represented by a concatenation of
current SE state, an embedding of the whole computation
graph and the selected node to be placed next.

The state s is a vector of size |T S| + f + 1 where T S is the
set of tile slices in the Streaming Engine with T S = T √ó S
and f is the length of graph features. Here T and S are sets of
all tiles and slots in the SE respectively. The number of slots
available in each tile is equal to number of Initiation Intervals.

D. Action Representation

The action a at each step is a tuple (n, t, s) where n ‚àà N
(the set of all nodes) is the node to be placed and t ‚àà T and
s ‚àà S are respectively the tile and slot at which the node n
is to be placed. We place nodes in a topological order which
ensures that all of a node‚Äôs predecessors have been placed
before the node. We do not make selecting the node to be
placed a part of the learning problem since it increases the
complexity of the learning process and makes training harder.
Masked Actions: In order to enure that the network only
outputs valid actions, we determine a binary mask over all
possible actions and set the value of logits (unnormalized
outputs of last layer of the neural network) corresponding to
invalid actions to ‚àí‚àû in the actor network. This in turn sets
the probability of sampling invalid actions to 0, ensuring that
we never take an invalid action. Finally, we only calculate
entropy on valid actions, making sure that our algorithm
maximizes exploration only for valid actions in a given state.

E. Reward Function

Our goal in this work is to get mappings that are optimal
in terms of total clock cycles taken, and for this purpose, the
reward that is obtained at each time step is the difference
between the clock cycle at which the current node to be placed
is ready (its ready time) and the ready time of its predecessor.
Rn is the reward obtained for placing node n and tn is its
ready time. The function p(n) gives the predecessor of n in

Micron ConfidentialMicron ConfidentialActor NetworkSimulationEnvironmentAction (Node placement)SE StateRewardBufferCritic NetworkCalculateLossPredicted Rewards Updatean SDF graph. If the current node can‚Äôt be placed because
of the constraints (all values in the mask vector m are zero),
then a high negative reward ‚àíŒª is given. If a node has more
than one predecessor, then the node with the later ready time
is chosen for the purpose of determining reward value.

(cid:40)

Rn =

‚àíŒª,
tn ‚àí tp(n), otherwise

mi = 0, ‚àÄ i ‚àà T √ó S

F. Model design

The architecture for actor and critic models is shown in
Fig. 4. The input is separated in two categories: static and
dynamic data. Static data is information that doesn‚Äôt change
as nodes are being placed and includes the graph features
from the SDFs that encapsulate node dependence based on SE
constraints. Device state (including placed nodes) and node to
be placed are dynamic data that change during placement.

G. Global Graph Attention Module

A GNN is used to process the IR graph where each node
contains a feature vector that is initialized with a combination
of tile memory constraints and node data dependencies. The
input IR for an application is the same during placement of
each node in the SDFs for that application. After placing
all nodes in an episode, a different IR graph for another
application can be fed into the RL model to train it on a
collection of application tasks.

The Global Graph Attention Module is the combination of
the GNN and attention modules. The GNN is composed of
two layers of graph convolution [31] and a graph average
pool layer which produce a vector embedding. An attention
module is applied to the embedding produced by the GNN
to highlight relevant components for the current node to be
placed. We evaluated two attention module implementations.
A transformer encoder layer [32] and a modiÔ¨Åed Position
Attention Module (PAM) from [33], in which the positions
represent different node embeddings. In our experiments,
both produced similar results. Having a higher number of
transformer encoder layers improves the best reward obtained
but it is achieved at a cost of more parameters, making training
more expensive.

The selected node and SE state change for each iteration
over all nodes in the SDF graph. This dynamic data is fed
into a Fully Connected (FC) layer model to create another
embedding to represent the current state. The embeddings from
dynamic data and static data are concatenated and fed to a
MultiLayer Perceptron (MLP) model with three FC layers to
obtain placements. Invalid actions are masked before being
sent to the SE simulation environment. Output masking to Ô¨Ålter
invalid placements has previously been shown to be effective
for RL in game environments [34]. We see the addition of
GGA module provides improved reward and sample efÔ¨Åciency
for various applications and different device conÔ¨Ågurations as
shown in Section V-A.

V. RESULTS

The aim of this section is to evaluate the RL framework
for mapping applications to the SE. We analyze the effects
that different components of the RL model such as GGA
module and training strategies such as node iteration order-
ing and output masking have on the quality of placements
generated. We also analyze the results of pre-training the RL
model on multiple graphs and then Ô¨Åne-tuning it on a desired
application.

The implemented RL approach was able to successfully map
different applications such as vector add, distance calculation
function and Fast Fourier Transform (FFT). FFT is widely
used in several areas such as digital signal and image process-
ing, pattern recognition, solving partial differential equations,
error-correcting codes, and many others [35]. FFT has time
complexity of O(N log N ) and two nested loops. The outer
loop and inner loop have time complexity of O(log N ) and
O(N ) respectively. The inner loop is targeted for acceleration.
The inner loop code is optimized and broken down into four
SDFs which are shown in Fig. 5. The SE device conÔ¨Åguration
for the following experiments used 16 tiles and a maximum
slot count of six. Other SE device conÔ¨Ågurations are possible.
We also benchmarked our RL framework against a collec-
tion of random directed graphs meant to simulate real life
applications. We compare a baseline PPO method composed
of three stacked MLPs against an actor model with the
proposed GGA module. We evaluate these models on varying
application IR graphs with different complexities by increasing
the total number of nodes. We also tested on a larger device
with 64 tiles. In Fig. 6, we observe that the RL approach Ô¨Ånds
mappings for a variety of IR graphs for randomly generated
applications, and GGA improves the best schedule found for
the same number of training epochs. The GGA also Ô¨Ånds an
improved schedule in terms of clock cycles taken for a larger
device conÔ¨Åguration. For the results in Fig. 6, the epochs
were limited to 50,000 for both models. The cycle count is
the number of SE execution steps taken to process all nodes
for a mapping, which is calculated from the simulated SE
environment.

In Fig. 7, we evaluate the RL method for the FFT application
and compare it against Simulated Annealing (SA) optimization
method [36]. We observe that our method Ô¨Ånds higher rewards
over time, while SA Ô¨Ånds it difÔ¨Åcult to improve reward in the
SE‚Äôs sparse search space.

A. GGA

In Fig. 8, we see the addition of GGA module provides
improved reward and sample efÔ¨Åciency for the FFT application
by 10%. The node to be placed and SE state changes for
each iteration over all nodes in the SDFs. The GGA module
provides a representation of the entire IR graph while placing
each node.

The attention module assists in highlighting the node de-
pendencies when the RL model is predicting the placement
of a node. In Fig. 9, we plot the attention matrix produced
by the GGA module when mapping nodes from the FFT

Fig. 4: Actor and critic model architecture. GNN is used to process the SDF graphs (static data). Attention module is used
to determine relative importance of nodes that are relevant to the one currently being placed. The embedding created from
dynamic data is combined with static data embedding. A Ô¨Ånal MLP model is used to generate actions. Actions are masked to
ensure only valid actions are produced.

Fig. 5: IR for FFT application consists of 4 SDFs. The
arrows show data dependence between nodes. Tile memory
dependencies have not been shown for the sake of simplicity.

application shown in Fig. 5. The x-axis is the node index,
the title indices which node is being placed and the color axis
indicates attention scale. We see that when placing node 1, the
model focuses on nodes 2 and 3. When placing node 3, the
attention is focused on node 6. These attention values match

Fig. 6: Cycle count for running all nodes in the best mapping
given by RL model over 50,000 epochs. PPO baseline MLP
model and GGA model were evaluated with computation
graphs with increasing number of nodes. A larger device
conÔ¨Åguration with 64 tiles was used for experiments with IR
graphs with 40 and 50 nodes.

our expectations since these nodes are related and have direct
data dependencies.

Micron ConfidentialMicron ConfidentialSE StateGraphFeaturesGraph convAvg poolingNode to be placedAttention modulefc‚äôMaskMaskedPolicy1|ùëáùëÜ|128128fcGlobal Graph Attention (GGA) Module|ùëáùëÜ||ùëáùëÜ||ùëáùëÜ|ValueNetworkActorNetworkMicron ConfidentialMicron Confidential10152040500102030405060variableMLPGGA+MLP16 tiles 64 tilesNumber of NodesBest schedule cycle countAlgorithm 1 RL Mapper Training Algorithm

for i = 0, 1, 2, ... do

1: Input: Œ∏0: Initial actor network parameters, œÜ0: Initial value network parameters, Graph G which is to be placed
2: for k = 0, 1, 2, ... do
3:
4:
5:
6:
7:

Get placement mask m for n
Get placement for n by running policy œÄ(Œ∏k, m)
Store node placements in buffer Dk.

for node n ‚àà G in topological order do

8:
9:
10:
11:
12:

end for

end for
Compute discounted rewards ÀÜRt.
Compute advantage estimates, ÀÜAt = ÀÜRt ‚àí ÀÜVt using current value function VœÜk .
Update the policy by maximizing the PPO-Clip objective:

Œ∏k+1 = arg max

Œ∏

1
|Dk|T

(cid:88)

T
(cid:88)

œÑ ‚ààDk

t=0

min

(cid:18) œÄŒ∏(at|st)
œÄŒ∏k (at|st)

AœÄŒ∏k (st, at), g((cid:15), AœÄŒ∏k (st, at))

(cid:19)

,

via stochastic gradient ascent with Adam where

g((cid:15), A) =

(cid:26) (1 + (cid:15))A A ‚â• 0
(1 ‚àí (cid:15))A A < 0.

13:

Fit value function by regression on mean-squared error:

œÜk+1 = arg min

œÜ

1
|Dk|T

(cid:88)

T
(cid:88)

(cid:16)

VœÜ(st) ‚àí ÀÜRt

(cid:17)2

,

œÑ ‚ààDk

t=0

via gradient descent algorithm.

14: end for

Fig. 7: Comparison between training RL model (blue) versus
Simulated Annealing (SA) method (red).

Fig. 8: Effect of using Global Graph Attention (GGA) module
for mapping the FFT application on device with 16 tiles. GGA
module provides better sample efÔ¨Åciency and higher reward
after training.

B. Node Iteration Order

The iteration order is the sequence in which nodes are fed to
the actor model in the RL framework and plays an important
role in deciding whether the nodes can be successfully placed
or not. In Fig. 10, we observe that iterating over nodes in
topological ordering results in higher reward and eases the
node placement task. On the other hand, when placing the
nodes randomly, sometimes nodes get placed before their

predecessors are placed and the model needs to predict which
tiles the predecessor nodes will be placed on, making training
and the learning problem harder.

C. Output Masking

The SE instruction scheduling task has several

invalid
actions for a given state that add noise to the samples and

20k40k60k80k100k67891011GGA+MLPSAEpochsRewards50k100k150k67891011GGA+MLPMLPEpochsRewardsFig. 9: Attention scores from transformer module when placing
certain nodes for the FFT application.

Fig. 11: Reward comparison between node placement with
output masking (blue) and without output masking (red).

Fig. 10: Iterating nodes in topological ordering results in
higher reward and eases the placement task. The blue line is
the reward curve for when nodes were randomly selected for
placement. The red line is the reward curve for when nodes
were iterated upon in topological order. The task involved is
of placing 15 nodes of an IR graph onto a device with 16 tiles.

Fig. 12: Comparison between training RL model from scratch
(red) versus Ô¨Åne-tuning a model pre-trained on random graphs
(blue).

decision of the RL model, making convergence harder. After
placement of each node, the succeeding nodes have fewer
placement options due to their data dependency with other
nodes and device constraints. Masking invalid placements
reduces the search space as each node is being placed. Fig. 11
demonstrates that masking improves the rewards obtained
when masking is used by 20% as compared to using a negative
reward for invalid actions. We can also see that making helps
improve sample efÔ¨Åciency of the RL model.

D. Pre-training and Ô¨Åne-tuning

After training the RL model on various applications, the
RL model can be used for Ô¨Åne-tunning on a speciÔ¨Åc task or
for inferencing. In Fig. 12, the RL model was pre-trained on
a collection of randomly generated IR graphs and then Ô¨Åne-
tuned for the FFT application. The initial and Ô¨Ånal reward
of the Ô¨Åne tuned RL model is higher than training the model
from scratch. This demonstrates that the model is able to reuse
some of the previous experience of placing nodes for random
input graphs.

VI. CONCLUSION

The proposed RL mapper is a key component of the SE
device toolchain. It can search for optimal mappings while
using the learning to map other previously unseen workloads
more efÔ¨Åciently. This improves upon the total time required
to get mappings as compared to the existing manual place-
ment approach and also reduces the amount of manual labor
required. It also allows for an automated search of mappings
with different optimizations and trade-offs. In this paper we
have given a brief overview of the SE device architecture
along with an analysis of how different design choices in
terms of simulating the SE and neural network design impact
the quality of mappings obtained. As future work we wish to
explore:

‚Ä¢ Increasing sample efÔ¨Åciency of learning methods and
improving the simulation environment for the SE by
adding more constraints.

‚Ä¢ Integration of RL mapper into the SE toolset.
‚Ä¢ Investigate the application of the proposed techniques for

broader problems such as chip placement.

05101505101505101505101500.20.40.60.8100.20.40.60.8100.20.40.60.8100.20.40.60.81node 0node 1node 3node 4Loading [MathJax]/extensions/MathMenu.js5k10k15k4681012OrderedNot OrderedEpochsRewards20k40k60k80k100k34567891011With MaskWithout MaskEpochsRewards20k40k60k7.588.599.51010.511fine-tunedscratchEpochsRewardsACKNOWLEDGMENT

The authors acknowledge Balint Fleischer, Glen Edwards,
Jon Carter, Jeff Quigley, Mark Hur, Steve Pawlowski, and
Tony Brewer for their continuous and extensive support for
this project.

REFERENCES

[1] D. D. Sharma and S. Tavallaei, ‚ÄúCompute express link 2.0 white paper,‚Äù

Tech. Rep., 2020.

[2] R. N. Uma and J. Wein, ‚ÄúOn the relationship between combinatorial
and lp-based approaches to np-hard scheduling problems,‚Äù in Integer
Programming and Combinatorial Optimization, R. E. Bixby, E. A. Boyd,
and R. Z. R¬¥ƒ±os-Mercado, Eds.
Berlin, Heidelberg: Springer Berlin
Heidelberg, 1998, pp. 394‚Äì408.

[3] A. Mirhoseini, A. Goldie, M. Yazgan, J. Jiang, E. M. Songhori,
S. Wang, Y. Lee, E. Johnson, O. Pathak, S. Bae, A. Nazi, J. Pak,
A. Tong, K. Srinivasa, W. Hang, E. Tuncer, A. Babu, Q. V. Le,
J. Laudon, R. C. Ho, R. Carpenter, and J. Dean, ‚ÄúChip placement
with deep reinforcement learning,‚Äù CoRR, vol. abs/2004.10746, 2020.
[Online]. Available: https://arxiv.org/abs/2004.10746

[4] G. Theodoridis, D. Soudris, and S. Vassiliadis, ‚ÄúA survey of coarse-grain
reconÔ¨Ågurable architectures and cad tools,‚Äù in Fine-and Coarse-Grain
ReconÔ¨Ågurable Computing. Springer, 2007, pp. 89‚Äì149.

[5] R. Prabhakar, Y. Zhang, D. Koeplinger, M. Feldman, T. Zhao, S. Hadjis,
A. Pedram, C. Kozyrakis, and K. Olukotun, ‚ÄúPlasticine: a reconÔ¨Ågurable
accelerator for parallel patterns,‚Äù IEEE Micro, vol. 38, no. 3, pp. 20‚Äì31,
2018.

[6] T. P. Morgan, ‚ÄúIntel‚Äôs exascale dataÔ¨Çow engine drops x86 and von

neumann,‚Äù 2018.

[7] C. Nicol, ‚ÄúA coarse grain reconÔ¨Ågurable array (cgra) for statically

scheduled data Ô¨Çow computing,‚Äù Wave computing white paper, 2017.

[8] K. Vissers, ‚ÄúVersal: The xilinx adaptive compute acceleration platform
(acap),‚Äù in Proceedings of the 2019 ACM/SIGDA International Sympo-
sium on Field-Programmable Gate Arrays, 2019, pp. 83‚Äì83.

[9] M. Adriaansen, M. Wijtvliet, R. Jordans, L. Waeijen, and H. Corporaal,
‚ÄúCode generation for reconÔ¨Ågurable explicit datapath architectures with
llvm,‚Äù in 2016 Euromicro Conference on Digital System Design (DSD).
IEEE, 2016, pp. 30‚Äì37.

[10] S. A. Chin, N. Sakamoto, A. Rui, J. Zhao, J. H. Kim, Y. Hara-
Azumi, and J. Anderson, ‚ÄúCGRA-ME: A uniÔ¨Åed framework for CGRA
modelling and exploration,‚Äù in 2017 IEEE 28th International Conference
on Application-speciÔ¨Åc Systems, Architectures and Processors (ASAP),
Jul. 2017, pp. 184‚Äì189, iSSN: 2160-052X.

[11] B. Mei, S. Vernalde, D. Verkest, H. De Man, and R. Lauwereins,
‚ÄúExploiting loop-level parallelism on coarse-grained reconÔ¨Ågurable ar-
chitectures using modulo scheduling,‚Äù IEE Proceedings-Computers and
Digital Techniques, vol. 150, no. 5, p. 255, 2003.

[12] L. Liu, J. Zhu, Z. Li, Y. Lu, Y. Deng, J. Han, S. Yin, and
S. Wei, ‚ÄúA Survey of Coarse-Grained ReconÔ¨Ågurable Architecture
and Design: Taxonomy, Challenges, and Applications,‚Äù ACM Comput.
Surv., vol. 52, no. 6, Oct. 2019, place: New York, NY, USA
Publisher: Association for Computing Machinery. [Online]. Available:
https://doi.org/10.1145/3357375

[13] V. Tehre and R. Kshirsagar, ‚ÄúSurvey on coarse grained reconÔ¨Ågurable
architectures,‚Äù International Journal of Computer Applications, vol. 48,
no. 16, pp. 1‚Äì7, 2012, publisher: Citeseer.

[14] Z. Zhao, W. Sheng, Q. Wang, W. Yin, P. Ye, J. Li, and Z. Mao,
‚ÄúTowards Higher Performance and Robust Compilation for CGRA
Modulo Scheduling,‚Äù IEEE Transactions on Parallel and Distributed
Systems, vol. 31, no. 9, pp. 2201‚Äì2219, Sep. 2020.

[15] Z. Li, D. Wijerathne, X. Chen, A. Pathania, and T. Mitra, ‚ÄúChordMap:
Automated Mapping of Streaming Applications Onto CGRA,‚Äù IEEE
Transactions on Computer-Aided Design of Integrated Circuits and
Systems, vol. 41, no. 2, pp. 306‚Äì319, Feb. 2022.

[16] B. Mei, S. Vernalde, D. Verkest, H. De Man, and R. Lauwere-
ins, ‚ÄúDRESC: a retargetable compiler for coarse-grained reconÔ¨Åg-
urable architectures,‚Äù in 2002 IEEE International Conference on Field-
Programmable Technology, 2002. (FPT). Proceedings., Dec. 2002, pp.
166‚Äì173.

[17] L. McMurchie and C. Ebeling, ‚ÄúChapter 17 - pathÔ¨Ånder: A negotiation-
based performance-driven router for fpgas,‚Äù in ReconÔ¨Ågurable Comput-
ing, ser. Systems on Silicon, S. Hauck and A. Dehon, Eds. Burlington:
Morgan Kaufmann, 2008, pp. 365‚Äì381.

[18] A. Mirhoseini, H. Pham, Q. V. Le, B. Steiner, R. Larsen, Y. Zhou,
N. Kumar, M. Norouzi, S. Bengio, and J. Dean, ‚ÄúDevice placement
optimization with reinforcement learning,‚Äù CoRR, vol. abs/1706.04972,
2017. [Online]. Available: http://arxiv.org/abs/1706.04972

[19] R. Addanki, S. B. Venkatakrishnan, S. Gupta, H. Mao, and M. Alizadeh,
‚ÄúPlaceto: Learning generalizable device placement algorithms for
distributed machine learning,‚Äù CoRR, vol. abs/1906.08879, 2019.
[Online]. Available: http://arxiv.org/abs/1906.08879

[20] Y. Zhou, S. Roy, A. Abdolrashidi, D. L. Wong, P. C. Ma,
Q. Xu, M. Zhong, H. Liu, A. Goldie, A. Mirhoseini,
and
for dataÔ¨Çow
J. Laudon,
graphs,‚Äù CoRR, vol. abs/1910.01578, 2019.
[Online]. Available:
http://arxiv.org/abs/1910.01578

‚ÄúGDP: generalized device placement

[21] Y. Zhou, S. Roy, A. Abdolrashidi, D. Wong, P. C. Ma, Q. Xu,
H. Liu, M. P. Phothilimtha, S. Wang, A. Goldie, A. Mirhoseini, and
J. Laudon, ‚ÄúTransferable graph optimizers for ML compilers,‚Äù CoRR,
vol. abs/2010.12438, 2020. [Online]. Available: https://arxiv.org/abs/
2010.12438

[22] P. Kormushev, S. Calinon, and D. G. Caldwell, ‚ÄúReinforcement learning
in robotics: Applications and real-world challenges,‚Äù Robotics, vol. 2,
no. 3, pp. 122‚Äì148, 2013.

[23] B. Zoph and Q. V. Le, ‚ÄúNeural architecture search with reinforcement
[Online]. Available:

learning,‚Äù CoRR, vol. abs/1611.01578, 2016.
http://arxiv.org/abs/1611.01578

[24] Z. Lu, I. Whalen, V. Boddeti, Y. D. Dhebar, K. Deb, E. D. Goodman,
and W. Banzhaf, ‚ÄúNSGA-NET: A multi-objective genetic algorithm for
neural architecture search,‚Äù CoRR, vol. abs/1810.03522, 2018. [Online].
Available: http://arxiv.org/abs/1810.03522

[25] Z. Shi, C. Sakhuja, M. Hashemi, K. Swersky, and C. Lin, ‚ÄúLearned
hardware/software co-design of neural accelerators,‚Äù CoRR, vol.
abs/2010.02075, 2020. [Online]. Available: https://arxiv.org/abs/2010.
02075

[26] S. Hochreiter and J. Schmidhuber, ‚ÄúLstm can solve hard long time lag
problems,‚Äù Advances in neural information processing systems, vol. 9,
1996.

[27] M. Gori, G. Monfardini, and F. Scarselli, ‚ÄúA new model for learning
in graph domains,‚Äù in Proceedings. 2005 IEEE international
joint
conference on neural networks, vol. 2, no. 2005, 2005, pp. 729‚Äì734.

[28] N. Wu, L. Deng, G. Li, and Y. Xie, ‚ÄúCore Placement Optimization for
Multi-Chip Many-Core Neural Network Systems with Reinforcement
Learning,‚Äù ACM Trans. Des. Autom. Electron. Syst., vol. 26, no. 2, Oct.
2020, place: New York, NY, USA Publisher: Association for Computing
Machinery. [Online]. Available: https://doi.org/10.1145/3418498
[29] B. Khailany, H. Ren, S. Dai, S. Godil, B. Keller, R. Kirby, A. Klinefelter,
R. Venkatesan, Y. Zhang, B. Catanzaro, and W. J. Dally, ‚ÄúAccelerating
Chip Design With Machine Learning,‚Äù IEEE Micro, vol. 40, no. 6, pp.
23‚Äì32, Nov. 2020.

[30] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, ‚ÄúProx-
imal policy optimization algorithms,‚Äù arXiv preprint arXiv:1707.06347,
2017.

[31] F. Wu, A. Souza, T. Zhang, C. Fifty, T. Yu, and K. Weinberger,
‚ÄúSimplifying graph convolutional networks,‚Äù in International conference
on machine learning. PMLR, 2019, pp. 6861‚Äì6871.

[32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù Advances in
neural information processing systems, vol. 30, 2017.

[33] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu, ‚ÄúDual attention
network for scene segmentation,‚Äù in Proceedings of
the IEEE/CVF
conference on computer vision and pattern recognition, 2019, pp. 3146‚Äì
3154.

[34] S. Huang and S. OntaÀún¬¥on, ‚ÄúA closer look at invalid action masking
in policy gradient algorithms,‚Äù CoRR, vol. abs/2006.14171, 2020.
[Online]. Available: https://arxiv.org/abs/2006.14171

[35] D. Rockmore, ‚ÄúThe fft: an algorithm the whole family can use,‚Äù
Computing in Science Engineering, vol. 2, no. 1, pp. 60‚Äì64, 2000.
[36] S. Kirkpatrick, C. D. Gelatt Jr, and M. P. Vecchi, ‚ÄúOptimization by
simulated annealing,‚Äù science, vol. 220, no. 4598, pp. 671‚Äì680, 1983.

