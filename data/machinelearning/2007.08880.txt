0
2
0
2

l
u
J

7
1

]

G
L
.
s
c
[

1
v
0
8
8
8
0
.
7
0
0
2
:
v
i
X
r
a

A Differential Game Theoretic Neural
Optimizer for Training Residual Networks

Guan-Horng Liu, Tianrong Chen, and Evangelos A. Theodorou
Autonomous Control and Decision Systems Laboratory
Georgia Institute of Technology,Atlanta, GA 30332
{ghliu,tianrong.chen,evangelos.theodorou}@gatech.edu

Abstract

Connections between Deep Neural Networks (DNNs) training and optimal control
theory has attracted considerable attention as a principled tool of algorithmic
design. Differential Dynamic Programming (DDP) neural optimizer [1] is a recently
proposed method along this line. Despite its empirical success, the applicability has
been limited to feedforward networks and whether such a trajectory-optimization
inspired framework can be extended to modern architectures remains unclear.
In this work, we derive a generalized DDP optimizer that accepts both residual
connections and convolution layers. The resulting optimal control representation
admits a game theoretic perspective, in which training residual networks can be
interpreted as cooperative trajectory optimization on state-augmented dynamical
systems. This Game Theoretic DDP (GT-DDP) optimizer enjoys the same theoretic
connection in previous work, yet generates a much complex update rule that better
leverages available information during network propagation. Evaluation on image
classiﬁcation datasets (e.g. MNIST and CIFAR100) shows an improvement in
training convergence and variance reduction over existing methods. Our approach
highlights the beneﬁt gained from architecture-aware optimization.

1

Introduction

Attempts from different disciplines to provide a fundamental understanding of deep learning have
advanced rapidly in recent years. Among those, interpretation of DNNs as discrete-time nonlinear
dynamical systems, by viewing each layer as a distinct time step, has received tremendous focus
as it enables rich analysis ranging from numerical equations [2], mean-ﬁeld theory [3], to physics
[4, 5, 6]. For instance, interpretation of residual networks as a discretization of ordinary differential
equations (ODEs) [7] provides theoretical reasoning on its optimization landscape [8]. It also inspires
new architecture that inherits numerical stability [9, 10, 11] and differential limit [12, 13].

Development of practical optimization methods, however, remains relatively limited. This is primarily
because classical approach to optimize dynamical systems relies on the optimal control theory, which
typically considers systems with neither the dimensionality nor parameterization as high as DNNs.
Such a difﬁculty limits its application, despite showing promising convergence and robustness in
trajectory optimization [14], to mostly theoretical interpretation of DNNs training [15, 16]. The
algorithmic progress has been restricted to either speciﬁc network class (e.g. discrete weight [17])
or training procedure (e.g. hyper-parameter adaptation [18] or computational acceleration [19, 20]),
until the recently proposed Differential Dynamic Programming (DDP) optimizer [1].

DDP is a second-order optimizer built upon a formal connection between trajectory optimization
and training feedforward networks, and from such it suggests existing training algorithms can be
lifted to embrace the dynamic programming principle, resulting in superior parameter updates with
layer-wise feedback policies. However, DDP is an architecture-dependent optimizer, in that the

Preprint.

 
 
 
 
 
 
feedback policies need to be derived on a per architecture basis. This raise questions of its ﬂexibility
and scalability to training modern architectures such as residual networks [21], since the existing
formulation scales exponentially with the batch size (see Fig. 1).

In this work, we present a game-theoretic extension to the DDP
optimizer (GT-DDP) which arises naturally from the optimal
control representation of residual networks. GT-DDP treats each
layer as a decision maker in a multi-stage coalition game con-
nected through network propagation. This leads to much com-
plex feedback policies as information is allowed to exchanged
between layers. Despite the increasing computation, we lever-
age efﬁcient approximations which enable GT-DDP to run on
a faster wall-clock yet with less memory (see Fig. 1). On the
theoretical side, we extend previous analysis for feedforward
networks to arbitrary architecture (Proposition 4), and derive game-theoretic integration for existing
second-order optimizers (Theorem 3 and Corollary 7). GT-DDP shows an overall improvement on
image classiﬁcation dataset.

Figure 1: Comparison on MNIST.

There has been a rising interest in game-theoretic analysis since the landmark Generative Adversarial
Network [22]. By framing networks into a two-player competing game, prevalent efforts have been
spent on studying its convergence dynamics [23] and effective optimizers to ﬁnd stable saddle points
[24, 25], Notably, our layer-as-player formulation has appeared in Balduzzi [26] to study the signal
communication implied in the Back-propagation, yet without any practical algorithm being made.
On the other hand, the cooperative game framework has been used to discover neuron contribution in
representation learning and network pruning [27, 28], which are of independent interest for this work.

The paper is organized as follows. We ﬁrst review the connection between optimal control and DNNs
training in Sec. 2. Extending such framework to residual networks is then given in Sec. 3, with
GT-DDP demonstrated in Sec. 4. We provide empirical results and discussion in Sec. 5 and 6.

2 Preliminaries

2.1 Optimal Control Formulation of Training DNNs

Classical optimal control problem (OCP) in discrete time considers the following programming:

(cid:34)

min
¯u

J( ¯u; x0) :=

φ(xT ) +

T −1
(cid:88)

t=0

(cid:35)

(cid:96)t(xt, ut)

s.t. xt+1 = ft(xt, ut) ,

(1)

where xt ∈ Rnt and ut ∈ Rmt represent the state and control at each time step t ∈ {0, · · · , T }. ft,
(cid:96)t and φ respectively denote the dynamics, intermediate cost and terminal cost. The control trajectory
is denoted as ¯u (cid:44) {ut}T −1
t=0 . Eq. (1) can be interpreted as the training objective of DNNs by treating
xt and ut as the vectorized activation map (x0 and xT being input image and prediction vector) and
weight at each layer t. ft stands as the compositional module propagating the activation vector, e.g.
an afﬁne transformation followed by an element-wise activation in a feedforward network. (cid:96)t and φ
denote the per-layer regularization (e.g. weight decay) and terminal loss (e.g. cross-entropy).

Following these notations, the gradient descent (GD) update at iteration k can be written as ¯u(k+1) =
¯u(k) + δ ¯u∗ = ¯u(k) − η∇ ¯uJ, where η is the learning rate. We can further break down the update for
the full network to each layer, i.e. δ ¯u (cid:44) {δut}T −1

t=0 , computed backward by

δu∗

t = arg min
δut∈Rmt

{Jt + ∇utJ T

t δut + 1

2 δuT

t ( 1

η It)δut} ,

(2)

where Jt(xt, ut) (cid:44) (cid:96)t(ut) + Jt+1(ft(xt, ut), ut+1) , JT (xT ) (cid:44) φ(xT )
(3)
is the per-stage objective1 at layer t. It can be readily veriﬁed that ∇xtJt gives the exact Back-
propagation dynamics. Eq. (2) follows the standard optimization interpretation in which GD mini-
mizes the second-order Taylor expansion of Jt with its Hessian ∇2
η It, i.e. spherical
ut
Notation: t will always be denoted the time step of dynamics, or equivalently the layer’s index. Given
a time-dependent function Ft(xt, ut) : X × U (cid:55)→ R, we will denote and sometimes abbreviate it Jacobian,
Hessian, and mixed partial derivative respectively as ∇xt Ft ≡ F t
xu.
1 We drop xt in all (cid:96)t(·) hereafter as the layer-wise regularization typically involves network weight alone.

xx, and ∇xt ∇ut Ft ≡ F t

Jt replaced by 1

xt Ft ≡ F t

x, ∇2

2

101102Batch Size101100101wall-clock time  per iteration (s)100101102Memory (GB)DDP [1]GT-DDP (ours)Algorithm 1 DDP Neural Optimizer (at iteration k)

t=0 with weights ¯u(k)

xx = ∇2

xφ

Compute derivatives of Qt with V t+1
Compute kt, Kt, V t

x and V t
xx

x

, V t+1
xx

x = ∇xφ and V T

1: Input: forward pass {xt}T
2: Set V T
3: for t = T − 1 to 0 do
4:
5:
6: end for
7: Set ˆx0 = x0
8: for t = 0 to T − 1 do
ˆut = u(k)
9:
ˆxt+1 = ft( ˆxt, ˆut)
10:
11: end for
12: ¯u(k+1) ← { ˆut}T −1
t=0

t + kt + Ktδxt,

(δxt = ˆxt − xt)

Figure 2: Comparison of computational graphs in
feedforward networks.

curvature. In a similar vein, adaptive ﬁrst-order methods, such as RMSprop and Adam, approximate
∇2
Jt by diagonal matrices with the leading entries adapting to the second-moment statistics in each
ut
coordinate. Second-order optimizers like KFAC [29, 30] and EKFAC [31] compute much complex
non-diagonal curvature matrices with Gauss-Newton approximation, i.e. ∇2
ut

Jt ≈ J t

uJ t
u

T.

2.2 Differential Dynamic Programming Neural Optimizer

Differential Dynamic Programming (DDP) is a second-order trajectory optimization algorithm
that solves the same programming in Eq. (1). Instead of searching updates from Rmt, at each
decision stage DDP aims at ﬁnding a locally-optimal feedback policy, i.e. δut(δxt) ∈ Γδxt, where
Γδxt = {bt + Atδxt : bt ∈ Rmt , At ∈ Rmt×nt} denotes all possible afﬁne mappings from the
state differential δxt. The resulting per-stage updates can also be computed backward:
(cid:35)



T

δu∗

t (δxt) = arg min
δut∈Γδxt

{Qt +

where Vt(xt) (cid:44) min
ut∈Γxt

(cid:96)t(ut) + Vt+1(ft(xt, ut))
(cid:124)
(cid:123)(cid:122)
(cid:125)
Qt(xt,ut)≡Qt

1
2

(cid:34) 1
δxt
δut

(cid:35)T 


T Qt
0 Qt
x
u
xx Qt
x Qt
Qt
ux Qt
u Qt
Qt

xu



(cid:34) 1
δxt
δut

uu
, VT (xT ) (cid:44) φ(xT )

} ,

(4)

(5)

is the value function that summarizes the objective value when all the afterward stages, i.e. Qs≥t, are
minimized. Hereafter we will denote the quadratic expansion in Eq. (4) as δQt(δxt, δut). Qt will be
referred to the Bellman objective, as Eq. (5) is well-known as the Bellman equation [32].
The analytic solution to Eq. (4) is given by δu∗
t (δxt) = kt + Ktδxt, where kt (cid:44) −(Qt
and Kt (cid:44) −(Qt
evaluating the derivatives of Qt in Eq. (4) requires one to compute V t+1
can be obtained by simply substituting δu∗
x = ∇xt{Qt + δQt(δxt, δu∗
V t
{Qt + δQt(δxt, δu∗
xx = ∇2
V t
xt

uu)−1Qt
u
ux are the locally optimal open and feedback gains. From the chain rule,
xx . These quantities

t (δxt) to Eq. (5) at each stage:
x + Qt
t (δxt))} = Qt
xx + Qt
t (δxt))} = Qt

xukt ,
xuKt .

uu)−1Qt

and V t+1

(6)

x

It is obvious that Eq. (4, 5) resemble Eq. (2, 3) in several ways. Both classes of optimizer perform
quadratic approximation of the stage-wise objective, except DDP also expands the objective wrt δxt,
which requires computing the mixed partial derivatives Qt
ux. The theoretical connection between
these two approaches for feedforward networks has been made formally in Liu et al. [1].
Proposition 1 ([1]). When Qt
collapses to the Back-propagation gradient in feedforward networks, i.e. V t
computes the same update in stage-wise Newton2: δu∗
Qt

ux = 0 at all stages, the ﬁrst-order derivative of the value function
x. In this case, DDP
u. If we further assume

η It, then DDP degenerates to the Back-propagation with gradient descent.

t (δxt) = −(J t

uu)−1J t

uu = 1

x = J t

Proposition 1 suggests that by setting Qt
uu, we can recover existing
optimizers from DDP. Meanwhile, existing methods can be extended to accept DDP framework by
computing Qt
ux. The resulting layer-wise feedback policies generate weight update with additional
forward pass (lines 7-11 in Alg. 1), in which the state differential is computed. We summarize the
backward pass and weight update procedure of the DDP optimizer in Alg. 1 and Fig. 2.

ux = 0 and choosing a proper Qt

2 Stage-wise Newton preconditions the gradient by the block-wise inverse Hessian at each layer.

3

3 Optimal Control Representation for Residual Networks

In this section, we extend the Bellman optimization framework to networks consist of residual paths.
Despite that in the Back-propagation this simply involves merging additional gradient ﬂow from the
shortcut, its optimal control representation is much complex when second-order information and
Bellman minimization are involved. We leave the complete derivation in the Appendix A.

3.1 Residual Connection as State-Augmented Dynamics

Consider the residual network in Fig. 3a. Let us denote xr as the residual state shortcutting from the
layer ts to tf , so that the output is merged by xtf +1 = xr + ftf (xtf , utf ). The Bellman equation
along the residual path is given by

Vts(xts) = minut∈[ts,tf ] (cid:96)ts(uts) + · · · + (cid:96)tf (utf ) + Vtf +1(xr + (ftf ◦ · · · ◦ fts)(xts )) ,

(7)

which can be decomposed into the following minimization and solve recursively from tf :

Vt(xr, xt) = minut Qt(xr, xt, ut) :=

(cid:26)(cid:96)t(ut) + Vt+1(xr + ft(xt, ut)) ,
(cid:96)t(ut) + Vt+1(xr, ft(xt, ut)) ,

t = tf
t ∈ (ts, tf )

Vts(xts) = minuts

Qts(xts, uts ) := (cid:96)ts(uts) + Vts+1(xts , fts(xts, uts ))

(8a)
(8b)
(8c)

Eq. (8) suggests the value functions of layers parallel to the shortcut depend not only on its own state
xt but also the residual xr. This is better explained from the game theoretic viewpoint. As xr affects
the payoff obtained during t ∈ [ts, tf ] through the addition at tf + 1, it shall contribute to decisions
made at these stages. Notice that we can rewrite the propagation rule as state-augmented dynamics
ˆft(xr, xt, ut). Dynamics of such forms resemble time-delayed systems [33], f (xt−i, · · · , xt, ut).
Instead of a constant moving window, here we consider a ﬁxed time stamp anchored at ts.

The new DDP update can be solved similar to Eq. (4), except the Bellman objective should be
expended additionally wrt to δxr. The optimal feedback law thus depends on the differential of both
states:

δu∗

t (δxt, δxr) = kt + Ktδxt + Gtδxr , where Gt (cid:44) −(Qt

uu)−1f t
u

T

V t+1
xxr

(9)

is the optimal residual feedback gain. kt and Kt are the same open and feedback gains computed in
the absence of shortcut. Thus, the new update rule has an additional feedback from the channel of
residual state (cf. Fig. 3b). The term V t+1
xxr denotes the mixed partial derivatives of Vt+1(xr, xt+1),
quantifying how these two states should be correlated mathematically. It can be computed, together
with the residual value Hessian V t+1

xrxr , through backward recursions similar to Eq. (6),

− GT

t Qt

uuGt ,

(10)

V t
xxr

= f t
x

T

V t+1
xxr

− KT

t Qt
uuGt , V t
xxr = V tf +1

xrxr
xrxr = V tf +1
xx .

= V t+1
xrxr

with the terminal conditions given by V tf +1
It is natural to ask how the optimal control representation differs between residual and feedforward
networks. This is summarized in the following proposition.
Proposition 2. When networks contain shortcut from ts to tf , the derivatives of the value function at
stage ts, denoted ˜V ts
xx, by

x and V ts

x and ˜V ts
˜V ts
x = V ts
˜V ts
xx = V ts

xx, relate to the ones in feedforward networks, denoted V ts
x + V tf +1
xx + V tf +1

uukt ,
uuGt + V ts
xxr

t∈[ts,tf ] GT
t∈[ts,tf ] GT

− (cid:80)
xx − (cid:80)

t Qt
t Qt

+ V tsT
xxr

x

(11)

(12)

There are several interesting implications from Proposition 2. First, recall that in the Back-propagation,
x + J tf +1
the gradient at ts is obtained by simply merging the one from the shortcut, i.e. ˜J ts
.
In the Bellman framework, ˜V ts
x is modiﬁed in a similar manner, yet with an additional summation
coming from the Bellman minimization along the shortcut. Interpretation for ˜V ts
xx follows the same
road map, except the mixed partial derivative V ts
xxr also contributes to the Hessian of the value function
at ts. We highlight these traits which distinguish our work from both standard Back-propagation and
previous work [1].

x = J ts

x

4

Figure 3: Terminology and weight update graph for (a) standard Back-propagation and (b)(c) GT-DDP
optimizer with identity and arbitrary shortcut mapping.

3.2 Cooperative Trajectory Optimization with Non-identity Shortcut Mapping

In some cases, the dimension of feature map between ts and tf may be mismatched; thus the residual
path will contain a non-identity shortcut mapping [34]. For CNNs this is typically achieved by
down-sampling xr with an 1×1 convolution. Hereafter we will denote this non-identity mapping as
x(cid:48)
r = ht(xr, vt), where vt is the vectorized weight. The new Bellman equation, consider for instance
when we add the mapping to the middle of residual path, i.e. t ∈ (ts, tf ) in Eq. (8b), becomes

Vt(xr, xt) = min
ut,vt

(cid:96)(ut) + (cid:96)(vt) + Vt+1(ht(xr, vt), ft(xt, ut))
(cid:123)(cid:122)
(cid:125)
(cid:124)
(cid:44)Qt(xr,xt,ut,vt)

.

(13)

Minimizing Qt(xr, xt, ut, vt) simultaneously wrt ut and vt resembles the formulation in a complete
Cooperative Game (CG) [35]. In its common setup, two players observe the same state and decide
their policies to maximize a cooperative payoff. The game is complete in that all information is
known and shared in prior; thus can be leveraged to make better decisions. Application of DDP to
solving CG has been studied previously in robotics for robust trajectory optimization [36].

Before solving Eq. (13), it will be useful to ﬁrst revisit cases when each policy can be solved
independently, i.e. when Qt(xr, xt, ut, vt) = Qt(xt, ut)+Qt(xr, vt). In this case, we know kt, Kt
is the solution to arg minut Qt(xt, ut). Let us further denote It + Ltδxr = arg minvt Qt(xr, vt),
where It (cid:44) −(Qt
vv)−1Qt
vxr . Now, solving Eq. (13) by quadratically
expanding Qt(xr, xt, ut, vt) wrt all variables will arrive at the following form3:
δu∗

v and Lt (cid:44) −(Qt

vv)−1Qt

uvIt + (Qt

ux − Qt

uvQ−1

vv Qt

vx)δxt + (Qt

uxr

+ Qt

uvLt)δxr

(cid:16)

t (δxt, δxr) =˜kt + ˜Ktδxt + ˜Gtδxr
= − ˜Q−1
uu
t (δxt, δxr) =˜It + ˜Ltδxr + ˜Htδxt
= − ˜Q−1
vv

u + Qt

v + Qt

Qt

Qt

(cid:16)

δv∗

vukt + (Qt

vxr

− Qt

vuQ−1

uuQt

uxr

)δxr + (Qt

vx + Qt

vuKt)δxt

vv

uu

(cid:44) Qt

vv Qt

uuQt

uuQt

uu(Qt

vuQ−1

uvQ−1

u + Qt

vv −Qt

uu −Qt

vu and ˜Qt

where ˜Qt
(cid:44) Qt
uv result from the block-matrices
inversion with the Schur complement. The update rules provided in Eq. (14, 15) are much complex
and do not admit forms of superposition as in Eq. (9). To make some intuitions, compare for instance
u with its cooperative variant ˜kt (cid:44) − ˜Q−1
the open gain kt (cid:44) −Q−1
uvIt). The latter
adjusts the policy by knowing the companion’s update rule It, and information between two players’
actions communicates through Quv and Qvu. Similar interpretation can be drawn for the feedback
gains Kt and ˜Kt, as Qvu allows information to ﬂow from Qux through Qvx, and etc.
Fig. 3c illustrates how these feedback policies generate the weight update. kt and Kt are applied
in the same manner as in feedforward networks (c.f. Fig. 2). Layers parallel to the skip connection
receive additional residual feedback from Gt. At the decision stage when the non-identity shortcut
mapping is involved, policies will be modiﬁed to their cooperative form, i.e. ˜kt, ˜Kt, ˜Gt, ˜It, ˜Lt, ˜Ht.
Notice that the residual policies Gs≤t and Gs>t now take different state differential (δxr and δx(cid:48)
r
resp.). This implies the GT-DDP solution to residual networks is not unique, as placing ht(xr, vt) at
different location along the shortcut will result in different value of weight update. Despite seemly
unintuitive, from the game theoretic perspective it implies one would prefer δx(cid:48)
r to δxr whenever the
former is available, since states closer to the decision stage reveal more information.

3 We omit the superscript t of Q−1

uu,Q−1

vv , ˜Q−1

uu, ˜Q−1

vv sometimes for notational simplicity but stress that Q is

always time (i.e. layer) dependent in this work.

5

(14)
(cid:17)

,

(15)
(cid:17)

,

Table 1: Relation between existing ﬁrst (e.g. RMSprop) and
second-order (e.g. EKFAC) algorithms under GT-DDP framework4

Quu,Qvv

Quv,Qvu

RMSprop
GT-DDP-RMSprop
EKFAC
GT-DDP-EKFAC

1
η diag(Ju (cid:12) Ju + (cid:15))
1
η diag(Qu (cid:12) Qu + (cid:15))
E[xxT] ⊗ E[JhJ T
h]
E[xxT] ⊗ E[VhV T
h ]

0
0
0
Theorem 3

nonzero Qux,
Qvxr ,Quxr , Qvx
(cid:55)
(cid:51)
(cid:55)
(cid:51)

4 Game Theoretic DDP Neural Optimizer

In this section we discuss efﬁcient computation of the update rules proposed in the previous section to
training residual networks. As the algorithm generalizes the DDP framework [1] to new architectures
under game-theoretic perspective, we name it the Game Theoretic DDP neural optimizer (GT-DDP).
Detailed derivation and proof in this section are left in the Appendix B.

4.1 Curvature Approximation

Computation of the GT-DDP solution involves extensive evaluation of the derivatives of Qt wrt
different variables. Since ft is highly over-parametrized in each network layer, second-order deriva-
tives wrt the weight parameter, e.g. Qt
vv, are particularly expansive to compute, let alone their
inversions. Thus, approximation must be made for these matrices.

uu Qt

Following the curvature interpretation in Sec 2.1, one can simply substitute these expansive Hessians
with the ones considered in existing methods. For instance, replacing Qt
uu with an identity (or diago-
nal) matrix resembles the (adaptive) ﬁrst-order update rule. Note that this ﬁrst-order approximation
implicitly implies both Qt
vu to vanish, since by construction ﬁrst-order methods omit the
covariances among different weight coordinate.

uv and Qt

t ] ⊗ E[gtgT

t ], where ⊗ is the Kronecker operator and gt := J t

As for second-order approximation, in this work we consider the popular Kronecker factorization
used in EKFAC [31]. Let ft ≡ σ(Wtxt+bt) be the generic dynamics where σ is the activation
function, and denote ht ≡ Wtxt + bt as the pre-activation vector. EKFAC factorizes Qt
uu ≈
E[xtxT
h is the ﬁrst-order derivative of
the per-stage objective wrt the pre-activation vector5. The expectation is taken wrt the batch sample.
Factorizing GT-DDP with Kronecker operation requires one to derive the Kronecker representations
for the cooperative matrices appeared in CG, which are given below.
Theorem 3 (Kronecker factorization in Cooperative Game). Suppose Quu and Qvv are factorized
respectively by Quu ≈ Auu ⊗ Buu and Qvv ≈ Avv ⊗ Bvv, where
u] , Avv (cid:44) E[xvxT

v ] , Bvv (cid:44) E[gvgT
v ]

u] , Buu (cid:44) E[gugT

Auu (cid:44) E[xuxT

are the Kronecker block matrices for layers f (xu, u) and h(xv, v). Further, let Auv (cid:44) E[xuxT
v ]
and Buv (cid:44) E[gugT
v ], then the unique Kronecker factorizations for the matrices in CG are given by
uu ⊗ ˜B−1
(16)
vv ⊗ ˜B−1

uv)−1 ⊗ (Buu − BuvB−1
uvB−1

vv AT
uuAuv)−1 ⊗ (Bvv − BT

uu = (Auu − AuvA−1
vv = (Avv − AT
uvA−1

vv BT
uv)−1
uuBuv)−1 ,

(17)

vu ≈ −Auv ⊗ Buv. The CG update, take ˜kt for example, can be computed by

˜kt = −vec( ˜B−1

uu(Qu + BuvB−1

vv QvA−T

vv AT

uv) ˜A−T

uu) .

(18)

Hereafter we will refer these approximations respectively to GT-DDP-RMSprop, GT-DDP-EKFAC,
and etc. The algorithmic relation between existing methods and their DDP integration is summarized
in Table 1, with the theoretical connection given by the following proposition.
Proposition 4. The update rules derived from stage-wise minimization of the Bellman equation
degenerate to the method it uses to approximate the weight Hessian, i.e. Quu Qvv, when the Bellman
objective Qt at all stages satisﬁes (i) all mixed partial derivatives between parameter and activation,
e.g. Qux,Quxr , vanish, and (ii) parameters between distinct layers are uncorrelated.

4(cid:12) denotes element-wise multiplication. h is the pre-activation vector deﬁned in Sec. 4.
5 For GT-DDP-EKFAC, we have gt := V t

h. We left further introduction and derivation in Appendix B.1.

6

uu ≈ ˜A−1
˜Q−1
˜Q−1
vv ≈ ˜A−1
and Quv = QT

Note that Proposition 4 extends Proposition 1 to arbitrary architectures beyond feedforward and
residual networks, so long as its layer-wise Bellman objective is properly deﬁned.

4.2 Practical Implementation

0 }B

Block-diagonal Value Hessian: Extending the Bellman optimization
framework to accept mini-batch samples {x(i)
i=0 has been made in pre-
t = [· · · , x(i)
vious work [1] by augmenting the state space to x(cid:48)
, · · · ]T.
However, such a formulation can cause memory explosion when xt is
lifted to 3D feature map in convolutional layers, let alone the augmented
value function considered in GT-DDP (cf Eq. (8)). In this work, we
propose to approximate the batch-augmented value Hessian V t
x(cid:48)x(cid:48) as
block-diagonal. The approximation is made from an empirical obser-
vation (see Fig. 4) that V t
x(cid:48)x(cid:48) contains only nontrivial values along the
diagonal blocks, even when networks contain Batch Normalization (BN)
layers. This suggests one can reduce the memory consumption by approx-
imating the batch-augmented value Hessian as block-diagonal and only
carry batch matrices, {V t

t

Figure 4: Example of
V t
x(cid:48)x(cid:48) for batch size B =
4 in DIGITS dataset.
Higher (whiter) values
concentrate along the di-
agonal blocks V t

x(i)x(i)}B

i=0, along the backward computation.
Gauss-Newton (GN) Approximation at the Terminal Hessian: Next,
we impose Gauss-Newton approximation to the Hessian at the prediction
layer. Surprisingly, this will lead to a nontrivial factorization in the Bellman optimization framework.
For dynamics represented by feedforward networks, we have the following proposition.
Proposition 5 (Outer-product factorization in DDP). Consider the following form of OCP:

x(i)x(i)

(cid:34)

min
¯u

φ(xT ) +

T −1
(cid:88)

t=0

(cid:35)

(cid:96)t(ut)

s.t. xt+1 = ft(xt, ut) .

(19)

If the Hessian of the terminal loss can be expressed by an outer product of vectors, i.e. ∇2φ(xT ) ≈
x ⊗ zT
zT
x = ∇φ for GN approximation), then we have the factorization:
xx = zt
(20)

x (e.g. zT
u ⊗ qt

x for some vector zT

ux = qt

xx = qt

x , V t

x , Qt

∀t , Qt

x ⊗ zt

x ⊗ qt

x ,

where qt

u, qt

x, and zt

x are outer-product vectors which can be computed backward:

qt
u = f t
u

T

zt+1
x

, qt

x = f t
x

T

zt+1
x

, zt

x =

1 + qt T

u (Qt

uu)−1qt

u qt

x .

(21)

(cid:113)

In other words, the outer-product factorization at the ﬁnal stage can be backward propagated to all
proceeding layers. Thus, state-dependent second-order matrices can be represented as outer products
of vectors. We note that the low-rank structure at the prediction layer has been observed when
classiﬁcation loss (e.g. cross-entropy) is used [37, 38]. Prop. 5 can be extended to residual networks:
Proposition 6 (Outer-product factorization in GT-DDP). The residual value Hessians considered in
Eq. (10), when the same outer-product factorization is imposed at the terminal stage, take the form

, where zt
xr

(cid:113)

=

1 + qt T

u (Qt

uu)−1qt

u zt+1
xr

(22)

= zt

x ⊗ zt
xr

V t
xxr
and (qt
Eq. (13), is presented, the cooperative forms of zt

and V t

= zt
xr

⊗ zt
xr

u, qt

x, zt

xrxr

(cid:113)

˜zt
x =
where qt
xr

1 + qt T

uuqt

u + qt T

u Q−1
zt+1
xr

= ht T
xr

v Q−1
v = ht T

v qt
vv qt
x ,
v zt+1
.
xr

, and qt

x) are given by Eq. (21). When the non-identity shortcut mapping, i.e. ht(xr, vt) in
x and zt
xr
(cid:113)

, are given by

, denoted ˜zt

˜zt
xr

=

1 + qt T

u Q−1

uuqt

vv qt

v qt
xr

, (23)

x and ˜zt
xr
v Q−1

u + qt T

The outer-product factorization, together with the block-diagonal approximation, reduces the compu-
tational dependency by dropping the memory by 2/3 and the runtime by 1/5 compared with previous
work [1], as shown in Fig. 1. As such, we adopt both approximation in all experiments.

Jacobian of Layers Dynamics: Finally, computing the derivatives of the Bellman objective involve
evaluating the Jacobian associated with each layer, e.g. Qt
. These
computations can be done efﬁciently for both fully-connected (FC) and convolution (Conv) layers:

TV t+1
x

TV t+1
x

u = f t
u

x = f t
x

and Qt

T

f t
x

V t+1
x =

(cid:110) W T
t V t
h
ωT
t ˆ∗ V t
h

,

T

f t
x

V t+1
x =

(cid:110) xt ⊗ V t
h
xt ˆ∗ V t
h

, where f t

x = σt(ht) , ht (cid:44)

(cid:110) Wtxt + bt
ωt ∗ xt

7

respectively denote the pre-activation of FC and Conv layers. ∗ and ˆ∗ denote the convolution and
deconvolution (transposed convolution) operator [39, 40].

5 Evaluation on Classiﬁcation Data Set

Table 2: Performance comparison on train loss and validation accuracy (over 6 random seeds).
(+) and (-) respectively denote improvement and degradation over non-GT-DDP baselines.

Data Set

SGD RMSProp Adam EKFAC

GT-DDP
-SGD

GT-DDP
-RMSProp

GT-DDP
-Adam

GT-DDP
-EKFAC

DIGITS 0.0053
MNIST 0.0250
SVHN 0.2755
CIFAR-10 0.0296
CIFAR-100 0.0075

DIGITS 96.09
MNIST 98.59
SVHN 88.58
74.69
71.78

CIFAR-10
CIFAR-100

g
n
i
n
i
a
r
T

)

%

(

n
o
i
t
a
d
i
l
a
V

0.0247
0.0284
0.2670
0.0107
0.0058

95.61
98.52
88.96

70.88
71.65

0.0182 0.0514 0.0050 (+) 0.0124 (+) 0.0081 (+) 0.0514 (+)
0.0330 0.0290 0.0240 (+) 0.0282 (+) 0.0312 (+) 0.0291 (-)
0.2544 0.2049 0.2692 (+) 0.2637 (+) 0.2517 (+) 0.2047 (+)
0.0127 0.0922 0.0284 (+) 0.0069 (+) 0.0096 (+) 0.0907 (+)
0.0055 0.0120 0.0075 (-) 0.0058 (+) 0.0054 (+) 0.0125 (-)

95.81
98.51
89.20

72.51
71.96

95.31
98.56
88.75

74.33
71.95

96.10 (+)
98.62 (+)
89.90 (+)
74.69 (+)
72.06 (+)

95.92 (+)
98.53 (+)
89.02 (+)
70.97 (+)
71.91 (+)

95.84 (+)
98.51 (+)
89.22 (+)
72.68 (+)
72.19 (+)

95.55 (+)
98.56 (-)
89.91 (+)
74.18 (-)
72.24 (+)

In this section we verify the performance of our GT-DDP optimizer and discuss
the beneﬁt of having layer-wise feedback policies during weight update. Detail
experiment setup and additional results are provided in the Appendix C.

We validate the performance of GT-DDP on digits recognition and image
classiﬁcation data set. The networks consist of 1-4 residual blocks followed
by fully-connected (FC) layers (see Fig. 5), except that we use ResNet18 [21]
for the CIFAR-100 dataset. Each block contains a skip connection between 3
convolution modules, possibly with a non-identity shortcut mapping if needed.
Following the discussion in the previous section, we select our baselines
as SGD, RMSprop [41], Adam [42], and EKFAC [31], as they cover most
widely-used curvature approximation in training deep nets, including (adaptive)
diagonal matrices and second-order Kronecker factorization.

Figure 5: Archi-
tecture and residual
block in Sec. 5.

Table 2 summarizes our main results. In each experiment we keep the shared
hyper-parameters (e.g. learning rate and weight decay) between baselines
and their GT-DDP variants the same, so that the performance difference only comes from GT-DDP
framework. On all data set, GT-DDP achieves better or comparable results on both training and
accuracy. Notably, when comparing original methods with their GT-DDP integrated variants, the latter
improve training convergence on almost all dataset. Empirically, it also leads to better generalization.

Since the feedback updates are typically order of magnitude smaller than the open gain due to the
sparse Hessian of standard classiﬁcation loss (i.e. cross-entropy), GT-DDP follows similar training
trend with the baseline it used to approximate the parameter curvature (see Fig. 6a). Nevertheless,
these additional updates have a non-trivial effect on not only improving the convergence but robusti-
fying the training. As shown in Fig. 6b, GT-DDP reduces the variation of the performance difference
over random seeds subjected to same hyper-parameters6. In fact, the Bellman framework has been
shown numerically stable than direct optimization such as Newton method [43], since it takes into
account the temporal, i.e. layer-wise, structure inherit in Eq. (1). As the concern for reproducibility
arises [44], GT-DDP provides a principled way to improve the robustness and consistency during
training. We highlight this perspective as the beneﬁt gained from architecture-aware optimization.

To understand the effect of feedback policies more perceptually, we conduct eigen-decomposition
on the feedback matrices of convolution layers and project the leading eigenvectors back to image
space, using techniques proposed in [45]. These feature maps, denoted δxmax in Fig. 6c, correspond
to the dominating differential image that GT-DDP policies shall respond with during weight update.

6 Additional experiments across different hyper-parameters are provided in the Appendix C.

8

Figure 6: (a) Training performance on CIFAR-10 for Adam, RMSprop and their GT-DDP variants.
(b) Variation reduction over 3-6 random seeds on CIFAR-10 and CIFAR-100. We report the value
(VARGT-DDP-Baseline − VARBaseline)/VARBaseline. (c) Visualization of the feedback policies on MNIST.

Fig. 6c demonstrates that the feedback policies indeed capture non-trivial visual feature related to
the pixel-wise difference between spatially similar classes, e.g. (8, 3) or (7, 1). We note that these
differential maps differ from adversarial perturbation [46] as the former directly link the parameter
update to the change in activation; thus being more interpretable.

6 Discussion on Game-Theoretic Second-order Optimizer

Figure 7: (a) Illustration of the cooperative-game module. (b) Training and testing performance on
MNIST using the architecture in 7a. GT-EKFAC denotes integration of EKFAC with Corollary 7.

Theorem 3 may be of independent interest for developing game-theoretic second-order optimizer, as
Eq.(16,17) provide efﬁcient second-order approximation to the cooperative Hessian regardless of
the presence of Bellman framework. To better show its effectiveness, let us consider the modules in
Fig. 7a that resemble the cooperative game, i.e. two (p)layers take the same input and affect each
payoff through output addition. Such an architecture has also appeared in recent work of progressive
training [47]. Interestingly, for this particular structure, we have the following corollary to Thm. 3:
Corollary 7. Let Quu ≈ Auu ⊗ Buu = U ΣuuU T be the eigen-decomposition of the Kronecker
factorization, where Σuu = diag(λuu) + γI and γ > 0 is the Tikhonov damping. Consider the
architecture in Fig. 7a, its cooperative matrix corresponds to rescaling in the eigenspace of Quu, i.e.

˜Quu = U ˜ΣuuU T ,

˜Σuu = diag(˜λuu) + γI ,

and

˜λi
uu =

γ
γ + λi

uu

λi
uu .

(24)

γ
γ+λi

≤ 1 for positive eigenvalues; thus the inverse Hessian ˜Q−1

uu shall take a larger step
Notice that
in eigenspace compared with Q−1
uu. As shown in Fig 7b, integrating this game theoretic perspective
with existing second-order methods, denoted GT-EKFAC, leads to better convergence. Having
additional layer-wise policies from the GT-DDP framework further improves the performance.

uu

7 Conclusion

In this work, we present the Game-Theoretic Differential Dynamic Programming (GT-DDP) optimizer
as a new class of second-order algorithm. Theoretically, we strengthen the optimal control connection
proposed in previous work by showing training residual networks can be linked to trajectory opti-
mization in a cooperative game. Algorithmically, we propose several effective approximation which
scales GT-DDP to training modern architectures and suggest how existing methods can be extended
to accept such a game-theoretic perspective. We validate GT-DDP on several image classiﬁcation
dataset, showing improvement on both convergence and robustness.

9

2.5k5k7.5k10k103102101100AdamGT-DDP-AdamRMSpropGT-DDP-RMSprop0.30.40.50.60.7AdamGT-DDP-Adam05k10k0.20.30.40.50.60.7RMSpropGT-DDP-RMSpropAccuracyTraining LossIterationsCIFAR-10 Training & Accuracy Curve(a)0255075100Variation Reduction  over Random Seeds (%)GT-DDP-SGDGT-DDP-RMSpropGT-DDP-AdamGT-DDP-EKFACcifar100-traincifar100-testcifar10-traincifar10-test(b)02.5k5k7.5k10k100EKFACGT-EKFACGT-DDP-EKFAC02.5k5k7.5k10k0.750.850.95AccuracyEKFACGT-EKFACGT-DDP-EKFACTraining LossIterations(b)Acknowledgments

The authors would like to thank Chen-Hsuan Lin, Yunpeng Pan, and Yen-Cheng Liu for many helpful
discussions on the paper. The work is supported under Amazon AWS Machine Learning Research
Award (MLRA).

References

[1] Guan-Horng Liu, Tianrong Chen, and Evangelos A Theodorou. Differential dynamic program-

ming neural optimizer. arXiv preprint arXiv:2002.08809, 2020.

[2] Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond ﬁnite layer neural net-
works: Bridging deep architectures and numerical differential equations. arXiv preprint
arXiv:1710.10121, 2017.

[3] Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep informa-

tion propagation. arXiv preprint arXiv:1611.01232, 2016.

[4] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via

information. arXiv preprint arXiv:1703.00810, 2017.

[5] Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. In

Advances in Neural Information Processing Systems, pages 15353–15363, 2019.

[6] Yaofeng Desmond Zhong, Biswadip Dey, and Amit Chakraborty. Symplectic ode-net: Learning

hamiltonian dynamics with control. arXiv preprint arXiv:1909.12077, 2019.

[7] E Weinan. A proposal on machine learning via dynamical systems. Communications in

Mathematics and Statistics, 5(1):1–11, 2017.

[8] Yiping Lu, Chao Ma, Yulong Lu, Jianfeng Lu, and Lexing Ying. A mean-ﬁeld analysis of deep
resnet and beyond: Towards provable optimization via overparameterization from depth. arXiv
preprint arXiv:2003.05508, 2020.

[9] Qi Sun, Yunzhe Tao, and Qiang Du. Stochastic training of residual networks: a differential

equation viewpoint. arXiv preprint arXiv:1812.00174, 2018.

[10] Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, and Elliot Holtham. Reversible
architectures for arbitrarily deep residual neural networks. In Thirty-Second AAAI Conference
on Artiﬁcial Intelligence, 2018.

[11] Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse Problems,

34(1):014004, 2017.

[12] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In Advances in Neural Information Processing Systems, pages 6572–6583,
2018.

[13] Xuanqing Liu, Tesi Xiao, Si Si, Qin Cao, Sanjiv Kumar, and Cho-Jui Hsieh. Neural sde:
Stabilizing neural ode networks with stochastic noise. arXiv preprint arXiv:1906.02355, 2019.
[14] Yuval Tassa, Tom Erez, and Emanuel Todorov. Synthesis and stabilization of complex behav-
iors through online trajectory optimization. In 2012 IEEE/RSJ International Conference on
Intelligent Robots and Systems, pages 4906–4913. IEEE, 2012.

[15] E Weinan, Jiequn Han, and Qianxiao Li. A mean-ﬁeld optimal control formulation of deep

learning. arXiv preprint arXiv:1807.01083, 2018.

[16] Guan-Horng Liu and Evangelos A Theodorou. Deep learning theory review: An optimal control

and dynamical systems perspective. arXiv preprint arXiv:1908.10920, 2019.

[17] Qianxiao Li and Shuji Hao. An optimal control approach to deep learning and applications to

discrete-weight neural networks. arXiv preprint arXiv:1803.01299, 2018.

[18] Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modiﬁed equations and adaptive stochastic
gradient algorithms. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pages 2101–2110. JMLR. org, 2017.

[19] Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate
once: Accelerating adversarial training via maximal principle. arXiv preprint arXiv:1905.00877,
2019.

10

[20] Stefanie Gunther, Lars Ruthotto, Jacob B Schroder, Eric C Cyr, and Nicolas R Gauger. Layer-
parallel training of deep residual neural networks. SIAM Journal on Mathematics of Data
Science, 2(1):1–23, 2020.

[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770–778, 2016.

[22] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural
information processing systems, pages 2672–2680, 2014.

[23] Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of

gans. arXiv preprint arXiv:1705.07215, 2017.

[24] Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances

in Neural Information Processing Systems, pages 1825–1835, 2017.

[25] David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore
Graepel. The mechanics of n-player differentiable games. arXiv preprint arXiv:1802.05642,
2018.

[26] David Balduzzi. Grammars for games: a gradient-based, game-theoretic framework for opti-

mization in deep learning. Frontiers in Robotics and AI, 2:39, 2016.

[27] Amirata Ghorbani and James Zou. Neuron shapley: Discovering the responsible neurons. arXiv

preprint arXiv:2002.09815, 2020.

[28] Julian Stier, Gabriele Gianini, Michael Granitzer, and Konstantin Ziegler. Analysing neural
network topologies: a game theoretic approach. Procedia Computer Science, 126:234–243,
2018.

[29] James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored ap-
proximate curvature. In International conference on machine learning, pages 2408–2417,
2015.

[30] Roger Grosse and James Martens. A kronecker-factored approximate ﬁsher matrix for convolu-

tion layers. In International Conference on Machine Learning, pages 573–582, 2016.

[31] Thomas George, César Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. Fast
approximate natural gradient descent in a kronecker factored eigenbasis. In Advances in Neural
Information Processing Systems, pages 9550–9560, 2018.

[32] Richard Bellman. The theory of dynamic programming. Technical report, Rand corp santa

monica ca, 1954.

[33] David D Fan and Evangelos A Theodorou. Differential dynamic programming for time-delayed
systems. In 2016 IEEE 55th Conference on Decision and Control (CDC), pages 573–579. IEEE,
2016.

[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pages 630–645. Springer, 2016.
[35] David WK Yeung and Leon A Petrosjan. Cooperative stochastic differential games. Springer

Science & Business Media, 2006.

[36] Yunpeng Pan, Evangelos Theodorou, and Kaivalya Bakshi. Robust trajectory optimization: A
cooperative stochastic game theoretic approach. In Robotics: Science and Systems, 2015.
[37] Kamil Nar, Orhan Ocal, S Shankar Sastry, and Kannan Ramchandran. Cross-entropy loss and
low-rank features have responsibility for adversarial examples. arXiv preprint arXiv:1901.08360,
2019.

[38] José Lezama, Qiang Qiu, Pablo Musé, and Guillermo Sapiro. Ole: Orthogonal low-rank
embedding-a plug and play geometric loss for deep learning. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 8109–8118, 2018.

[39] Vincent Dumoulin and Francesco Visin. A guide to convolution arithmetic for deep learning.

arXiv preprint arXiv:1603.07285, 2016.

[40] Matthew D Zeiler, Dilip Krishnan, Graham W Taylor, and Rob Fergus. Deconvolutional net-
works. In 2010 IEEE Computer Society Conference on computer vision and pattern recognition,
pages 2528–2535. IEEE, 2010.

11

[41] Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning

lecture 6a overview of mini-batch gradient descent. 2012.

[42] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

[43] Li-zhi Liao and Christine A Shoemaker. Advantages of differential dynamic programming
over newton’s method for discrete-time optimal control problems. Technical report, Cornell
University, 1992.

[44] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David
In Thirty-Second AAAI Conference on

Meger. Deep reinforcement learning that matters.
Artiﬁcial Intelligence, 2018.

[45] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In

European conference on computer vision, pages 818–833. Springer, 2014.

[46] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversar-

ial examples. arXiv preprint arXiv:1412.6572, 2014.

[47] Lemeng Wu, Dilin Wang, and Qiang Liu. Splitting steepest descent for growing neural
architectures. In Advances in Neural Information Processing Systems, pages 10655–10665,
2019.

[48] Gunter Stein. Respect the unstable. IEEE Control systems magazine, 23(4):12–25, 2003.
[49] Emanuel Todorov and Weiwei Li. A generalized iterative lqg method for locally-optimal
feedback control of constrained nonlinear stochastic systems. In Proceedings of the 2005,
American Control Conference, 2005., pages 300–306. IEEE, 2005.

[50] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.

12

Supplementary Material

A Derivation of Optimal Control Representation for Residual networks

A.1 Derivation in Section 3.1

First, recall the Bellman objective in feedforward networks, Qt(xt, ut) (cid:44) (cid:96)t(ut) + Vt+1(ft(xt, ut)).
Following standard chain rule, the second-order expansion of Qt in Eq. (4) takes the form

δQt =







1
2

1

δxt

δut



T 









T Qt
0 Qt
x
u
xx Qt
x Qt
Qt
ux Qt
u Qt
Qt

xu

uu





T















,

1

δxt

δut

Qt
Qt
Qt
Qt
Qt

x =
u =
uu=
ux=
xx=

f t
x
f t
u
f t
u
f t
u
f t
x

TV t+1
x
TV t+1
x + (cid:96)t
u
TV t+1
u + V t+1
xx f t
TV t+1
x + V t+1
xx f t
TV t+1
x + V t+1
xx f t

x

x

x

,

(25)

uu + (cid:96)t

uu

· f t
· f t
· f t

ux

xx

where the dot notation represents the product of a vector with a 3D tensor. Note that in practice, the
dynamics is often expanded up to the ﬁrst order, i.e. by omitting f t
ux above, while keeping
the full second-order expansion of the value function V t+1
xx . This can be seen as Gauss-Newton
(GN) approximation, and the stability obtained by keeping only the linearized dynamics is discussed
thoroughly in trajectory optimization [14, 49]. As such, both DDP [1] and our GT-DDP optimizer
adopt the same setup.

uu,f t

xx,f t

Now, let us consider the value minimization described in Eq. (8) for residual networks. We shall
interpret the propagation rules as ˆft( ˆxt, ut), where ˆxt is the residual-augmented state ˆxt (cid:44) [xt, xr]T.
The Jacobian of this state-augmented dynamics and its relation to the ones in the absence of residual
paths, i.e. f t

x, can be summarized below:

u f t

At t = tf , Eq. (8a) gives xt+1 = xr + ft(xt, ut)
(cid:124)
(cid:125)

⇒ ˆf t

ˆx = (cid:2)f t

x

I(cid:3) , ˆf t

u = f t

u ,

(26a)

(cid:123)(cid:122)
xt+1= ˆft( ˆxt,ut)
(cid:21)

(cid:20) ft(xt, ut)
xr

=

(cid:20) xt+1
xr

(cid:124)

(cid:123)(cid:122)
ˆxt+1= ˆft( ˆxt,ut)

(cid:21)

(cid:20) xt+1
xr

(cid:20) ft(xt, ut)
xt

=

(cid:124)

(cid:123)(cid:122)
ˆxt+1= ˆft(xt,ut)

⇒ ˆf t

ˆx =

(cid:21)

(cid:20) f t

x 0
0 I

, ˆf t

u =

(cid:21)

(cid:20) f t
u
0

,

(26b)

⇒ ˆf t

ˆx =

(cid:21)

(cid:20) f t
x
I

, ˆf t

u =

(cid:21)

(cid:20) f t
u
0

,

(26c)

(cid:21)

(cid:125)

(cid:21)

(cid:125)

At t ∈ (ts, tf ), Eq. (8b) gives

At t = ts, Eq. (8c) gives

where I is the identity matrix.

Once we have the explicit form of dynamics written, the optimal control representation can be derived
by substituting Eq. (26) into Eq. (25). After some algebra, one can verify that for t ∈ (ts, tf ] we will
have

ˆx

ˆx = ˆf t T
ˆQt
u = ˆf t T
ˆQt
uu = ˆf t T
ˆQt
u ˆx = ˆf t T
ˆQt

u

u

u

ˆQt

ˆx ˆx = ˆf t T

ˆx

xr

(cid:3)T

x V t+1

ˆx = (cid:2)Qt
ˆV t+1
ˆV t+1
u = Qt
ˆx + (cid:96)t
u
ˆf t
ˆV t+1
uu = Qt
u + (cid:96)t
uu
ˆx ˆx
(cid:104)
T
ˆf t
ˆV t+1
f t
Qt
ˆx =
u
ˆx ˆx
(cid:20)

ˆV t+1
ˆx ˆx

ˆf t
ˆx =

(cid:105)

ux

V t+1
xxr
TV t+1
xx f t
x
xxr
x V t+1
xrxr

Qt
xrx f t
V t+1

(27a)

(27b)

(27c)

(27d)

(27e)

(cid:21)

.

The optimal feedback policy is given by
u + ˆQt

t (δ ˆxt) = −( ˆQt

uu)−1( ˆQt

δu∗

u ˆxδ ˆxt) = kt + Ktδxt −(Qt

uu)−1f t
u
(cid:123)(cid:122)
(cid:44)Gt

(cid:124)

T

V t+1
xxr
(cid:125)

δxr ,

(28)

13

ˆx

ˆx − ˆQt

and ˆV t+1

ˆx = ˆQt
ˆV t

ˆx ˆx are the derivatives of the value function ˆVt+1( ˆxt+1) induced by the

Note that ˆV t+1
state-augmented dynamics. We can compute these matrices backward from tf similar to Eq. (6):
(cid:20) Qt
V t+1
xr
(cid:20) Qt
xrx f t
V t+1
(cid:20) V t
xx V t
xrx V t
V t

xukt
t Qt
xx + Qt
xuKt
x − GT
t Qt
(cid:21)

(cid:20) V t
x
V t
xr
TV t+1
xxr

f t
x
uuKt V t+1
xrxr

− KT
t Qt
t Qt

x + Qt
− GT

ˆx ˆx = ˆQt
ˆV t

uu)−1 ˆQt

uu)−1 ˆQt

ˆx ˆx − ˆQt

ˆxu( ˆQt

ˆxu( ˆQt

xxr
xrxr

u =

uuGt

uuGt

− GT

u ˆx =

uukt

(29)

(30)

(cid:44)

(cid:44)

(cid:21)

(cid:21)

(cid:21)

,

,

with the terminal conditions given by V tf +1
As for the stage t = ts where the residual state is split out, the derivatives of Qt follow (again one
can readily verify by substituting Eq. (26c) into Eq. (25)) by

xrx = V tf +1
xx .

xrxr = V tf +1

xxr = V tf +1

xr = V tf +1

and V tf +1

x

,

ˆQts
ˆx = Qts
ˆQts
u ˆx = Qts
ˆQts
ˆx ˆx = Qts

x + V ts+1
xr
T
ux + f ts
u
xx + f ts
x

T

V ts+1
xxr
V ts+1
xxr

,

+ V ts+1

xrx f t

x + V ts+1
xrxr ,

(31a)

(31b)

(31c)

and ˆQt
u and ˆQt
form as in Eq. (28).

uu remain the same with Qt

u and Qt

uu. The resulting optimal policy admits the same

Proof of Proposition 2: Finally, one can verify Eq. (11, 12) by noticing that the derivatives of the
value function at t = ts follow

− f ts
u

T

V ts+1
xxr

kts)

t Qt

uukt ,

x

u

xr

x = ˆQts
˜V ts
= (Qts
= V ts
= V ts
xx = ˆQts
˜V ts
= (Qts

ˆx − ˆQts
x + Qts
x + V ts+1
x + V tf +1
ˆx ˆx − ˆQts
xx + Qts
xrx f ts

uu)−1 ˆQts
ˆxu( ˆQts
xukts) + (V ts+1
xr
− GT
Qts
uukts
ts
− (cid:80)
t∈[ts,tf ] GT
uu)−1 ˆQts
ˆxu( ˆQts
u ˆx
T
xuKts ) + (f ts
x
x − GT
ts
+ V tsT
xxr
+ V tsT
xxr

+ (V ts+1
xx + V ts
xxr
xx + V ts
xxr

= V ts
= V ts

uuGts )
Qts

− KT
V ts+1
Qts
ts
xxr
Qts
uuKts) + (V ts+1
− GT
xrxr
ts
+ V ts+1
Qts
− GT
uuGts
ts
xrxr
xx − (cid:80)
+ V tf +1
t∈[ts,tf ] GT

t Qt

uuGt ,

uuGts )

where the last equalities in Eq. (32, 33) follow by applying the recursions

V t
xr

V t
xrxr

(cid:44) V t+1
xr
(cid:44) V t+1
xrxr

Thus we conclude the proof.

A.2 Derivation in Section 3.2

− GT
− GT

t Qt
t Qt

V tf +1
xr = V tf +1
uukt ,
xrxr = V tf +1
uuGt , V tf +1

xx

x

.

(32)

(33)

(34)

Here we provide the derivation of Eq. (14, 15). Recall in Eq. (13) the cooperative Bellman objective
Qt(xr, xt, ut, vt) and expand it wrt all variables to the second order.

δQt =

T 





















1
δxt
δxr
δut
δvt

1
2

T

T Qt
0 Qt
x
xr
xx Qt
x Qt
Qt
xxr
xrx Qt
Qt
Qt
xrxr
xr
u Qt
Qt
ux Qt
uxr
Qt
v Qt
vx Qt
vxr

T Qt
T Qt
u
v
xu Qt
Qt
xv
xru Qt
Qt
Qt
uu Qt
Qt
vu Qt
vv

xrv
uv























1
δxt
δxr
δut
δvt

.

(35)

14

Similar to section A.1 where we consider the augmented state ˆxt (cid:44) [xt, xr]T, here we can addi-
tionally interpret the joint control as ˆut (cid:44) [ut, vt]T. The derivatives of the state-control-augmented
Bellman objective ˆQt( ˆxr, ˆut) thus follow

ˆQt

ˆu =

(cid:21)

(cid:20) Qt
u
Qt
v

,

ˆQt

ˆu ˆx =

(cid:20) Qt
Qt

ux Qt
vx Qt

uxr
vxr

(cid:21)

,

ˆQt

ˆu ˆu =

(cid:20) Qt
Qt

uu Qt
vu Qt
vv

uv

(cid:21)

,

and the feedback policy in this case is given by

δ ˆu∗

t (δ ˆxt) = −( ˆQt
(cid:20) Qt
Qt

ˆu ˆu)−1( ˆQt
uu Qt
vu Qt
vv

= −

ˆu ˆxδ ˆxt)
(cid:21)

ˆu + ˆQt
(cid:21)−1 (cid:18)(cid:20) Qt
u
Qt
v

uv

+

(cid:20) Qt
Qt

ux Qt
vx Qt

uxr
vxr

(cid:21)

(cid:19)

.

δ ˆxt

Now, we apply the block-matrices inversion with the Schur complement by recalling

(cid:20)Qt
Qt

uu Qt
vu Qt
vv

uv

(cid:21)−1

=

(cid:122)
(cid:20)(
Qt

˜Qt
uu
(cid:125)(cid:124)
uv(Qt
vv)−1Qt

uu − Qt
−( ˜Qt

vv)−1Qt
vu(Qt

uu)−1

(cid:123)
vu)−1 −( ˜Qt
Qt
(
(cid:124)

vv − Qt

uu)−1Qt
vu(Qt
(cid:123)(cid:122)
˜Qt
vv

vv)−1

uv(Qt
uu)−1Qt

uv)−1
(cid:125)

(36)

(37)

(cid:21)

.

(38)

Substitute Eq. (38) into Eq. (37) and after some algebra, we will arrive at

δ ˆu∗

t (δ ˆxt) =

(cid:21)

(cid:20)˜kt
˜It

+

(cid:21)

(cid:20) ˜Kt ˜Gt
˜Ht ˜Lt

δ ˆxt =

(cid:20)˜kt + ˜Ktδxt + ˜Gtδxr
˜It + ˜Htδxt + ˜Ltδxr

(cid:21)

(cid:44)

(cid:20)δu∗
δv∗

(cid:21)
t (δxt, δxr)
t (δxt, δxr)

,

where

˜kt = −( ˜Qt
˜Kt = −( ˜Qt
˜Gt = −( ˜Qt
˜It = −( ˜Qt
˜Ht = −( ˜Qt
˜Lt = −( ˜Qt

uu)−1(Qt
uu)−1(Qt
uu)−1(Qt
vv)−1(Qt
vv)−1(Qt
vv)−1(Qt

uv(Qt
uv(Qt
uv(Qt

u − Qt
ux − Qt
− Qt
vu(Qt
vu(Qt
vu(Qt

uxr
v − Qt
vx − Qt
− Qt

vv)−1Qt
v) ,
vv)−1Qt
vx) ,
vv)−1Qt
uu)−1Qt
u) ,
uu)−1Qt
ux) ,
uu)−1Qt

vxr

uxr

vxr

) ,

) ,

(39)

(40a)

(40b)

(40c)

(40d)

(40e)

(40f)

which conclude Eq. (14, 15).

B Derivation in Section 4

B.1 Preliminary on Second-Order Kronecker Factorization

Popular curvature factorization methods, such as KFAC [29] and EKFAC [31], rely on the fact that
for feedforward networks:

xt+1 = σt(ht) , ht ≡ Wtxt + bt ,

(41)

where σt is the nonlinear activation function and ht denotes the pre-activation vector, we have
J t
u = xt ⊗ J t
h. ⊗ denotes the Kronecker product and Jt is the per-stage objective deﬁned in Eq. (3).
Thus, the Gauss-Newton (GN) approximation of J t

uu can be computed as

uu ≈ E[J t
J t

uJ t T

u ] = E[(xt ⊗ J t

h)(xt ⊗ J t

h)T] ≈ E[(xtxT

t )] ⊗ E[(J t

hJ t
h

T

)] ,

(42)

where the expectation is taken over the mini-batch.

The factorization in Eq. (42) is also applicable to DDP and GT-DDP, as Eq. (41) can be expressed by
xt+1 = ft(xt, ut), with ut (cid:44) [vec(Wt), bt]T; thus it is a valid dynamics. Further, we have

T

f t
u

x = xt ⊗ V t
V t+1

h, where V t

h = σt T

h V t+1
x

(43)

15

is the derivative of the value function wrt to the pre-activation. Following similar derivation, we will
arrive at the Kronecker approximation of Qt

uu:

Qt

uu ≈ E[Qt

uQt
u

T

] = E[(xt ⊗ V t

h)(xt ⊗ V t

h)T] ≈ E[xtxT

t ] ⊗ E[V t

hV t
h

T

] .

(44)

The Kronecker factorization allows us to compute the preconditioned update efﬁciently by noticing
that for matrices A ∈ Rn×n, B ∈ Rm×m, and X ∈ Rm×n, we have

(A ⊗ B)vec(X) = vec(BXAT) ,

(45)

where vec denotes the vectorization. Here, we shall interpret A and B respectively as E[xtxT
E[V t

T]. Additionally, the following properties will become handy for the later derivation.

t ] and

hV t
h

(A ⊗ B)−1 = A−1 ⊗ B−1
(A ⊗ B)T = AT ⊗ BT .

(46)

(47)

B.2 Derivation of Theorem 3

Let us consider two distinct layers, f (xu, u) and h(xv, v), and denote the propagation rules of their
pre-activation, along with the Kronecker factorization, respectively as

hu = uxu , Quu ≈ E[xuxT
hv = vxv , Qvv ≈ E[xvxT

u] ⊗ E[gugT
v ] ⊗ E[gvgT

u] (cid:44) Auu ⊗ Buu ,
v ] (cid:44) Avv ⊗ Bvv ,

(48)

where gu ≡ Vhu and gv ≡ Vhv for notational simplicity. We drop the bias in the propagation rules
but note that our derivation extends to the bias cases. Following Eq. (45, 46), the preconditioned
update, take kt for instance, can be computed by kt (cid:44) −Q−1
Now consider the CG formulation where the two layers are placed parallel in a residual network. A.2
suggests that one can derive the cooperative representation by considering the joint parametrization
[u, v]T and state augmentation ˆx = [xu, xv]T. To this end, we interpret Eq. (48) as an augmented
dynamics and rewrite it compactly as
(cid:20)hu
(cid:21)
hv

uuvec(Qu) ≈ −vec(B−1

⇔ ˆh = w ˆx .

(cid:21) (cid:20)xu
xv

uuQuA−T

(cid:20)u 0
0 v

uu).

(49)

=

(cid:21)

The approximated Hessian can thus be factorized as Qww ≈ Aww ⊗ Bww, where

Aww = E[ ˆx ˆxT] =

Bww = E[ˆg ˆgT] =

(cid:20)E[xuxu
E[xvxu
(cid:20)E[gugu
E[gvgu

T] E[xuxv
T] E[xvxv
T] E[gugv
T]
T] E[gvgv
T]

(cid:21)
T]
T]
(cid:21)

(cid:21)

(cid:21)

(cid:20)Auu Auv
Avu Avv
(cid:20)Buu Buv
Bvu Bvv

=

=

(50)

are the Kronecker blocks. Their inverse matrices are given by the Schur component (c.f. Eq. (38)):

A−1

ww =

B−1

ww =

(cid:20)
− ˜A−1
(cid:20)
− ˜B−1

˜A−1
uu
vv AvuA−1
uu

˜B−1
uu
vv BvuB−1
uu

− ˜A−1

− ˜B−1

(cid:21)

uuAuvA−1
vv
˜A−1
vv
uuBuvB−1
vv
˜B−1
vv

, where

(cid:21)

, where

(cid:26) ˜Auu (cid:44) Auu − AuvA−1
˜Avv (cid:44) Avv − AuvA−1
(cid:26) ˜Buu (cid:44) Buu − BuvB−1
˜Bvv (cid:44) Bvv − BuvB−1

vv Avu
vv Avu
vv Bvu
vv Bvu

(51)

Now, we are ready to derive Theorem 3. First notice that the preconditioned open gain can be
computed by

−Q−1

wwvec(

(cid:20)Qu 0
0 Qv

(cid:21)
) = −(A−1

ww ⊗ B−1

ww)vec(

(cid:20)Qu 0
0 Qv

(cid:21)
) = −vec(B−1
ww

(cid:21)

(cid:20)Qu 0
0 Qv

A−T

ww) (52)

Expanding Eq. (52) by substituting B−1

˜k ≈ −vec( ˜B−1
˜I ≈ −vec( ˜B−1

uuQu ˜A−T
vv Qv ˜A−T

ww and A−T
uu + ˜B−1
vv + ˜B−1

ww with Eq. (51), after some algebra we will arrive at
uuBuvB−1
vv BvuB−1

vv Qv( ˜A−1
uuQu( ˜A−1

uuAuvA−1
vv AvuA−1

vv )T) ,
uu)T) ,

(53)

16

which give the Kronecker approximation of the cooperative open gains. The Kronecker factorization
for each cooperative matrix can be obtained by decomposed Eq. (53) into the following

vv )T)

˜k = − vec( ˜B−1
= − vec( ˜B−1
= − ( ˜A−1
= − ( ˜A−1
= − ( ˜A−1
= − ( ˜A−1
(cid:124)

uuQu ˜A−T
uu + ˜B−1
uu(Qu + BuvB−1

vv Qv( ˜A−1
uuAuvA−1
uuBuvB−1
uv) ˜A−T
vv AT
vv QvA−T
uu)
vv AT
vv QvA−T
uu)vec(Qu + BuvB−1
uv)
vv AT
uu)(vec(Qu) + vec(BuvB−1
vv QvA−T
uv))
vv QvA−T
uu)(vec(Qu) + (Auv ⊗ Buv)vec(B−1

uu ⊗ ˜B−1
uu ⊗ ˜B−1
uu ⊗ ˜B−1
uu ⊗ ˜B−1
(A−1
)(vec(Qu) +(Auv ⊗ Buv)
uu
(cid:124)
(cid:125)
(cid:123)(cid:122)
(cid:125)
≈ ˜Q−1
uu

(cid:123)(cid:122)
≈−Quv

(cid:124)

vv ))

vv ⊗ B−1
)vec(Qv)) ,
vv
(cid:125)
(cid:123)(cid:122)
≈Q−1
vv

(54)

where we recall the deﬁnition ˜k (cid:44) − ˜Q−1
vv vec(Qv)). Similarly, it can be readily
veriﬁed that ˜Q−1
vv ≈ ˜A−1
vv . Note that when Quv vanishes, i.e. Auv = Buv = 0, Eq. (54) will
degenerate to original Kronecker factorization for the non-cooperative update. Thus, we conclude the
proof.

uu(vec(Qu) − QuvQ−1

vv ⊗ ˜B−1

B.3 Derivation of Corollary 7

Before deriving Corollary 7, we ﬁrst review the eigen-basis representation of the Kronecker ap-
proximation appeared in George et al. [31]. Recall the factorization Quu ≈ Auu ⊗ Buu and let
Auu = UAΣAU T
B be their eigen-decomposition. We can rewrite the Kronecker
factorization in its eigen-basis

A, Buu = UBΣBU T

Auu ⊗ Buu =(UAΣAU T

A) ⊗ (UBΣBU T
B)
=(UA ⊗ UB)(ΣA ⊗ ΣB)(UA ⊗ UB)T
(cid:44) U ΣuuU T ,

(55)

where U is the eigen-basis of the Kronecker factorization. Σuu (cid:44) diag(λuu) contains eigenvalues
along the diagonal entries. In practice, we will also add a positive Tikhonov coefﬁcient γ > 0 for
regularization purpose.

Now, observe that for the cooperative game module in Fig. 7a, we have

Auu = Auv = Avv , Buu = Buv = Bvv ,

(56)

since the two layers share the same input xu = xv and output derivative gu = gv. In other words,
Quv and Qvv are factorized by the same Kronecker blocks with Quu; thus they share the same
eigen-basis U . The cooperative matrix ˜Quu can thus be rewritten as

˜Quu = Quu − QuvQ−1

vv QT
uv

= Auu ⊗ Buu − (−Auv ⊗ Buv)(Avv ⊗ Bvv)−1(−Auv ⊗ Buv)T
= U (γI + Σuu)U T − (−U ΣuuU T)(U (γI + Σuu)−1U T)(−U ΣuuU T)T
= U ˜ΣuuU T ,
where ˜Σuu = γI + diag(˜λuu) and

˜λi
uu = λi

uu −

(λi
uu)2
γ + λi

=

γ
γ + λi

λi
uu .

uu
In short, the cooperative matrix ˜Quu admits a scaling in the eigen-basis of its non-cooperative variant.

uu

B.4 Proof for Proposition 4

Recall the connection we made in Sec. 2.1 and 2.2. It is sufﬁcient to show that when the two
conditions in Proposition 4 are met, we will have Eq. (4, 5) collapse exactly with Eq. (2, 3). First,
notice that at the ﬁnal layer, we have V T
xφ without any
condition. Further, Eq. (29, 30) suggest that when all mixed partial derivatives between parameter and

x = ∇xφ and V T

xx = ∇2

xx = J T

x = J T

17

(57)

(58)

activation vanish, the backward dynamics of (V t
Jt wrt xt in this case (c.f. Eq. (3)),

x,V t

xx) degenerates to (Qt

x, Qt

xx). The derivatives of

are the same as the backward dynamics for (V t

x,V t

xx),

x = f t
J t
x

T

J t+1
x

, J t

xx = f t
x

T

xx f t
J t+1
x ,

x = Qt
V t

x = f t
x

T

V t+1
x

, V t

xx = Qt

xx = f t
x

T

xx f t
V t+1
x .

Thus the two functionals Jt and Vt coincide with each other.
Next, when the parameters between distinct layers are uncorrelated, we will have Qt
vu = 0
at all stages. The cooperative precondition matrices, if exist along the network, degenerate to the
curvature approximation it uses to approximate the parameter Hessian. In fact, we will have
˜Qt

uv = Qt

Qt

uu = Qt

uu = J t

u = J t

uu .

u ,

Thus, the update rule Eq. (4) also collapses to Eq. (2).

C Experiment Detail

C.1 Experiment Setup in Section 5 and 6

Network architectures for classiﬁcation task are shown in Fig. 5. We use 1 residual block for DIGITS,
MNIST, SVHN dataset and 4 residual blocks for CIFAR-10. For CIFAR-100, we use ResNet18 [21]
architecture. All networks use ReLU activation for the intermediate layers and identity mapping at the
last prediction layer. The batch size is set to 128 for all data set except 8 for DIGITS. As for section
6, the network contains 3 convolution CGBs (c.f. Fig. 7a), 1 fully-connected CGB, and ﬁnally 1
standard fully-connected layer with identity mapping. We use Tanh activation for this experiment but
note that similar trend can be observed for ReLU. The batch size is set to 12. Regarding the machine
information, we conduct our experiments on GTX 1080 TI, RTX TITAN, four Tesla V100 SXM2
16GB on AWS, and eight GTX TITAN X. All experiments are implemented and conducted with
Pytorch [50]. We use the implementation in https://github.com/Thrandis/EKFAC-pytorch
for EKFAC baseline.

C.2 Additional Result and Discussion

Variation Reduction Over Different Learning Rate. Recall Fig. 6b reports the variation reduction
on the hyper-parameter used in Table 2. Here we provide additional results and show that the
robustness gained from GT-DDP integration remains consistent across different hyper-parameters.
Particularly, in Fig. 8 we report the variance difference on 3 different learning rates for each GT-DDP
variant. We use the same setup as in Fig. 5, i.e. we keep all hyper-parameters the same for each
experiment so that the performance difference only comes from the existence of feedback policies.
For all cases, having additional updates from GT-DDP stabilizes the training dynamics by reducing
its variation over random initialization.

Figure 8: Variation reduction over 3 different learning rates for each GT-DDP variant on CIFAR-10.
We report the value (VARGT-DDP-Baseline −VARBaseline)/VARBaseline, where each variance is computed
over 3 random seeds.

18

0020406080Variation Reduction  over Random LR (%)GT-DDP-SGDGT-DDP-RMSpropGT-DDP-AdamGT-DDP-EKFACLR=2e-1LR=1e-1LR=7e-2LR=2e-3LR=1e-3LR=7e-4LR=1e-3LR=9e-4LR=7e-4LR=6e-2LR=4e-2LR=2e-2