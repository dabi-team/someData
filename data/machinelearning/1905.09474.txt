9
1
0
2

n
u
J

9
1

]
P
C
.
n
i
f
-
q
[

3
v
4
7
4
9
0
.
5
0
9
1
:
v
i
X
r
a

Machine Learning for Pricing American Options in
High-Dimensional Markovian and non-Markovian models

Ludovic Gouden`ege ∗†

Andrea Molent‡

Antonino Zanette§

Abstract

In this paper we propose two eﬃcient techniques which allow one to compute the price of American
basket options. In particular, we consider a basket of assets that follow a multi-dimensional Black-Scholes
dynamics. The proposed techniques, called GPR Tree (GRP-Tree) and GPR Exact Integration (GPR-EI),
are both based on Machine Learning, exploited together with binomial trees or with a closed formula for
integration. Moreover, these two methods solve the backward dynamic programming problem considering
a Bermudan approximation of the American option. On the exercise dates, the value of the option is ﬁrst
computed as the maximum between the exercise value and the continuation value and then approximated
by means of Gaussian Process Regression. The two methods mainly diﬀer in the approach used to compute
the continuation value: a single step of binomial tree or integration according to the probability density of
the process. Numerical results show that these two methods are accurate and reliable in handling American
options on very large baskets of assets. Moreover we also consider the rough Bergomi model, which provides
stochastic volatility with memory. Despite this model is only bidimensional, the whole history of the process
impacts on the price, and handling all this information is not obvious at all. To this aim, we present how to
adapt the GPR-Tree and GPR-EI methods and we focus on pricing American options in this non-Markovian
framework.

Keywords: Machine Learning, American Options, Multi-dimensional Black-Scholes Model, Rough Bergomi
Model, Binomial Tree Method, Exact Integration.

∗This work was supported by a public grant as part of the Investissement d’avenir project, reference ANR-11-LABX-0056-

LMH, LabEx LMH.

†F´ederation de Math´ematiques de CentraleSup´elec - CNRS FR3487, France - ludovic.goudenege@math.cnrs.fr
‡Dipartimento di Scienze Economiche e Statistiche, Universit`a degli Studi di Udine, Italy - andrea.molent@uniud.it
§Dipartimento di Scienze Economiche e Statistiche, Universit`a degli Studi di Udine, Italy - antonino.zanette@uniud.it

1

 
 
 
 
 
 
1

Introduction

Pricing American options is clearly a crucial question of ﬁnance but also a challenging one since computing
the optimal exercise strategy is not an evident task. This issue is even more exacting when the underling
of the option is a multi-dimensional process, such as a baskets of d assets, since in this case the direct
application of standard numerical schemes, such as ﬁnite diﬀerence or tree methods, is not possible because
of the exponential growth of the calculation time and the required working memory.

Common approaches in this ﬁeld can be divided in four groups: techniques which rely on recombinant
trees to discretize the underlyings (see [4], [11] and [24]), techniques which employ regression on a truncated
basis of L2 in order to compute the conditional expectations (see [28] and [32]), techniques which exploit
Malliavin calculus to obtain representation formulas for the conditional expectation (see [1], [3], [9], and [27])
and techniques which make use of duality-based approaches for Bermudan option pricing (see [21], [26] and
[31]).

Recently, Machine Learning algorithms (Rasmussen and Williams [33]) and Deep Learning techniques

(Nielsen [30]) have found great application in this sector of option pricing.

Neural networks are used by Kohler et al. [25] to price American options based on several underlyings.
Deep Learning techniques are nowadays widely used in solving large diﬀerential equations, which is intimately
related to option pricing. In particular, Han et al. [20] introduce a Deep Learning-based approach that can
handle general high-dimensional parabolic PDEs. E et al.
[14] propose an algorithm for solving parabolic
partial diﬀerential equations and backward stochastic diﬀerential equations in high dimension. Beck et al.
[7] introduce a method for solving high-dimensional fully nonlinear second-order PDEs. As far as American
options in high dimension are concerned, Becker et al.
[8] develop a Deep Learning method for optimal
stopping problems which directly learns the optimal stopping rule from Monte Carlo samples.

Also Machine Learning techniques have made their contribution. For example, Dixon and Cr´epey present
a multi-Gaussian process regression for estimating portfolio risk, and in particular the associated CVA. De
Spiegeleer et al.
[13] propose to apply Gaussian Process Regression (GPR) to predict the price of the
derivatives from a training set made of observed prices for particular combinations of model parameters.
Ludkovski [29] proposes to use GPR meta-models for ﬁtting the continuation values of Bermudan options.
Similarly, Gouden ˜Aˇsge et al.
[18] propose the GPR-MC, which is a backward induction algorithm that
employs Monte Carlo simulations and GPR to compute the price of American options in very high dimension
(up to 100). In the insurance context, Gan [16] studies the pricing of a large portfolio of Variable Annuities in
the Black-Scholes model by using clustering and GPR. Moreover, Gan and Lin [17] propose a novel approach
that combines clustering technique and GPR to eﬃciently evaluate policies considering nested simulations.
In this paper we present two numerical techniques which upgrade the GPR-MC approach by replacing
the Monte Carlo based computation of the continuation value respectively with a tree step and with an exact
integration step. In particular, the algorithms we propose proceed backward over time and compute the price
function only on a set of predetermined points. At each time step, a binomial tree step or a closed formula for
integration are used together with GPR to approximate the continuation value at these points. The option
price is then obtained as the maximum between the continuation value and the intrinsic value of the option
and the algorithms proceed backward. For the sake of simplicity, we name these new approaches Gaussian
Process Regression - Tree (GPR-Tree) and Gaussian Process Regression - Exact Integration (GPR-EI). We
observe that the use of the GPR method to extrapolate the option value is particularly eﬃcient in terms of
computing time with respect to other techniques such as Neural Networks, especially because a small dataset
is considered here. Moreover, Le Gratiet et Garnier [19] developed recent convergence results about GPR,
extending the outcomes of Rasmussen and Williams [33], and founding the convergence rate when diﬀerent

2

kernels are employed.

In order to demonstrate the wide applicability of the GPR methods, we also consider the rough Bergomi
model, which is a non-Markovian model with stochastic volatility. Such a model, introduced by Bayer et
al.
[5] stood out for explaining implied volatility smiles and other phenomena in the pricing of European
options. The non-Markovian property of the model makes it diﬃcult to implement a methodologically
correct approach to address the valuation of American options. The literature in this framework is really
poor. Horvat et al.
[23] propose an approach based on Donsker’s approximation for fractional Brownian
motion and on a tree with exponential complexity. More recently, Bayer et al. [6] introduce a method based
on Monte Carlo simulation and exercise Rate Optimization.

Numerical results show that both the GPR-Tree and the GPR-EI methods are accurate and reliable in
the multi-dimensional Black-Scholes model. Moreover the computational times with respect to the GPR-MC
method are improved. The GPR-Tree and the GPR-EI methods prove its accuracy also when applied to the
rough Bergomi model.

The reminder of the paper is organized as follows. Section 2 presents American options in the multi-
dimensional Black-Scholes model. Section 3 and Section 4 introduce the GPR-Tree and the GPR-EI methods
for the multi-dimensional Black-Scholes model respectively. Section 5 presents the American options in the
rough Bergomi model. Section 6 and Section 7 introduce the GPR-Tree and the GPR-EI methods for the
rough Bergomi model. Section 8 reports some numerical results. Section 9 draws some conclusions.

2 American options in the multi-dimensional Black-Scholes model

An American option with maturity T is a derivative instrument whose holder can exercise the intrinsic
optionality at any moment before maturity. Let S = (St)t∈[0,T ] denote the d-dimensional underlying process,
which is supposed to randomly evolve according to the multi-dimensional Black-Scholes model: under the
risk neutral probability, such a model is given by the following equation

dSi

t = r Si

t dt + σi Si

t dW i
t ,

i = 1, . . . , d,

(2.1)

(cid:1) ∈ Rd

0, . . . , sd
0

with S0 = (cid:0)s1
+ the spot price, r the (constant) interest rate, σ = (σ1, . . . , σd) the vector of
volatilities, W a d-dimensional correlated Brownian motion and ρij the instantaneous correlation coeﬃcient
between W i
t . Moreover, let Ψ(ST ) denote the cash-ﬂow associated with the option at maturity T .
+ → R is then
Thus, the price at time t of an American option having maturity T and payoﬀ function Ψ : Rd

t and W j

v(t, x) = sup
τ ∈Tt,T

Et,x

(cid:105)
(cid:104)
e−r(τ −t)Ψ(Sτ )

,

(2.2)

where Tt,T stands for the set of all the stopping times taking values on [t, T ] and Et,x [·] represents the
expectation given all the information at time t and in particular assuming St = x.

For simulation purposes, the d−dimensional Black-Scholes model can be written alternatively using the

Cholesky decomposition. Speciﬁcally, for i = 1, . . . , d we can write

dSi

t = Si

t(rdt + σiΣidBt),

(2.3)

where B is a d-dimensional uncorrelated Brownian motion and Σi is the i-th row of the matrix Σ deﬁned as

3

a square root of the correlation matrix Γ, given by

Γ =



1








ρ21
...
ρd1

ρ12

1
. . .
. . .










. . . ρ1d
...
. . .
...
. . .
1
. . .

(2.4)

3 The GPR-Tree method in the multi-dimensional Black-Scholes

model

The GPR-Tree method is similar to the GPR-MC method but the diﬀusion of the underlyings is performed
through a step of a binomial tree. In particular, the algorithm proceeds backward over time, approximating
the price of the American option with the price of a Bermudan option on the same basket. At each time step,
the price function is evaluated only on a set of predetermined points, through a binomial tree step together
with GPR to approximate the continuation value. Finally, the optionality is exploited by computing the
option value as the maximum between the continuation value and the exercise value.

Let N denote the number of time steps, ∆t = T /N be the time increment and tn = n ∆t represent the
discrete exercise dates for n = 0, 1, . . . , N . At any exercise date tn, the value of the option is determined by
the vector of the underlying prices Stn as follows:

v (tn, Stn ) = max (Ψ (Stn ) , C (tn, Stn)) ,

where C denotes the continuation value of the option and it is given by the following relation:

C (tn, Stn ) = Etn,Stn

(cid:2)e−r∆tv (cid:0)tn+1, Stn+1

(cid:1)(cid:3) .

(3.1)

(3.2)

We observe that, if the function v (tn+1, ·) is known, then it is possible to compute v (tn, ·) by approxi-
mating the expectation in (3.2). In order to obtain such an approximation, we consider a set X of P points
whose elements represent certain possible values for the underlyings S:

X = {xp = (xp

1, . . . , xp

d) , p = 1, . . . , P } ⊂ Rd
+,

(3.3)

+ = ]0, +∞[d. Such a set is determined as done by Gouden ˜Aˇsge et al. [18], that is the elements of
where Rd
X are obtained through a quasi-random simulation of ST based on the Halton sequence (see [18] for more
details).

The GPR-Tree method assesses v (tn, xp) for each xp ∈ X through one step of the binomial tree proposed

by Ekval [15]. In particular, for each xp ∈ X, we consider a set ˜X p of 2d possible values for Stn+1

˜X p =

(cid:110)

˜xp,k =

(cid:16)

1 , . . . , ˜xp,k
˜xp,k

d

(cid:17)

, k = 1, . . . , 2d(cid:111)

⊂ Rd
+

which are computed as follows:

i = xp
˜xp,k

i exp

(cid:18)(cid:18)

r −

(cid:19)

σ2
i
2

√

∆t + σi

∆tΣiGk

(cid:19)

, k = 1, . . . , 2d

(3.4)

(3.5)

being Gk the k-th point of the space {−1, +1}d. In particular, if Yk ∈ {0, 1}d is the vector whose components
are the digits of the binary representation of 2d − 1, then Gk = 2Yk − 1. It is worth noticing that, as pointed

4

out in [15], the elements of ˜X p are equally likely and this simpliﬁes the evaluation of the expected value to
the computation of the arithmetic mean of the future values. Using the tree step, the price function may be
approximated by

vT ree
n

(xp) = max

v (cid:0)tn+1, ˜xp,k(cid:1)

 .

(3.6)


Ψ (xp) ,

e−r∆t
2d

2d
(cid:88)

k=1



The computation in (3.6) can be performed only if the quantities v (cid:0)tn+1, ˜xp,k(cid:1) are known for all the future
points ˜xp,k. If we proceed backward, the function v (t, ·) is known at maturity since it is given by the payoﬀ
function Ψ (·) and so (3.6) can be computed at tN −1 and for all the points of X.
In order to compute
v (tN −2, xp) for all xp ∈ X, and thus going on up to t = 0, we have to evaluate the function v (tN −1, ·) for
all the points in ˜X = (cid:83)P
N −1 (·) at X. To overcome this issue, we employ the
N −1 (·) at any point of Rd and in particular at the elements of
GPR method to approximate the function vT ree
˜X. Speciﬁcally, let vGP R
N −1 (·), obtained by considering the predictor
set X and the response y ∈ RP given by

N −2 (·) denote the GPR prediction of vT ree

˜X p, but we only know vT ree

p=1

yp = vT ree

N −1 (xp) , p ∈ {1, . . . , P } .

(3.7)

The GPR-Tree approximation vGP R−T ree
as follows:

N −2

(·) of the value function v (tN −2, ·) at time tN −2 can be computed

vGP R−T ree
N −2

(xp) = max


Ψ (xp) ,

e−r∆t
2d

2d
(cid:88)

k=1



vGP R
N −1

(cid:0)˜xp,k(cid:1)

 , p ∈ {1, . . . , P } .

(3.8)

Following the same steps, the dynamic programming problem can be solved. Speciﬁcally, let n ∈ {0, . . . , N − 3}
and let vGP R
y ∈ RP given by

n+1 (·) denote the GPR prediction of vGP R−T ree

(·) obtained from predictor set X and the response

n+1

Then, the function vGP R−T ree

n

can be obtained as

yp = vGP R−T ree
n+1

(xp) .

vGP R−T ree
n

(xp) = max


Ψ (xp) ,

e−r∆t
2d

2d
(cid:88)

k=1



vGP R
n+1

(cid:0)˜xp,k(cid:1)

 .

(3.9)

(3.10)

4 The GPR-EI method in the multi-dimensional Black-Scholes

model

The GPR-EI method diﬀers from both the GPR-MC and GPR-Tree methods for two reasons. First of all, the
predictors employed in the GPR step are related to the logarithms of the predictors used in the GPR-Tree
method. Secondly, the continuation value at these points is computed through a closed formula which comes
from an exact integration.

Let X = {xp, p = 1, . . . , P } ⊂ Rd

by applying the natural logarithm to all the components of xp, that is log (xp) = (log (xp
Moreover, let us deﬁne the set

+ be the same set as in (3.3) and deﬁne log (xp) as the vector obtained
d))(cid:62).

1) , . . . , log (xp

(cid:26)

Z =

zp = log (xp) −

(cid:18)

r −

(cid:19)

1
2

σ2

T, p = 1, . . . , P

.

(cid:27)

(4.1)

5

In this case, we do not work directly with the function v, but we rather consider the function u : [0, T ]×Z → R
deﬁned as

(cid:18)

(cid:18)

(cid:18)

(cid:19)

(cid:19)(cid:19)

u (t, z) := v

t, exp

z +

r −

t

.

(4.2)

1
2

σ2

In a nutshell, the main idea is to approximate the function u at tN , tN −1, . . . , t1 by using the GPR method
on the ﬁxed grid Z. In particular, we employ the Squared Exponential Kernel kSE : Rd × Rd → R, which is
given by

kSE (a, b) = σ2

f exp

−

(cid:32)

(a − b)(cid:62) Id (a − b)
2σ2
l

(cid:33)

, a, b ∈ Rd,

(4.3)

where Id the d × d identity matrix, σl ∈ R is the characteristic length scale and σf ∈ R is the signal standard
deviation. These two parameters are obtained by means of a maximum likelihood estimation. The GPR
approach allows one to approximate the function u (tn, ·) at time tn by

uGP R
n

(z) =

P
(cid:88)

q=1

kSE (zq, z) ωq,

(4.4)

where ω1, . . . , ωP are weights that are computed by solving a linear system. The continuation value can be
computed by integrating the function uGP R against a d-dimensional probability density. This calculation
can be done easily by means of a closed formula.

Speciﬁcally, the GPR-EI method relies on the following Proposition.

Proposition 1. Let n ∈ {0, . . . , N − 1} and suppose the function u (tn+1, ·) at time tn+1 to be known at Z.
The GPR-EI approximation of the option value u (tn, ·) at time tn at zp is given by

uGP R−EI
n

(zp) = max

(cid:32)

(cid:18)

Ψ

exp

(cid:18)

(cid:18)

r −

zp +

(cid:19)

(cid:19)(cid:19)

tn

1
2

σ2

, e−r∆t

P
(cid:88)

q=1

ωqσ2

f σd
l

2 (zq−zp)(cid:62)(Π+σ2

l Id)−1

e− 1

(zq−zp)

(cid:33)

(cid:112)det (Π + σ2

l Id)

(4.5)
σf , σl, and ω1, . . . , ωP are certain constants determined by the GPR approximation of the function z (cid:55)→
u (tn+1, z) for k = 1, . . . , P , considering Z as the predictor set, and Π = (Πi,j) is the d × d covariance matrix
of the log-increments deﬁned by Πi,j = ρi,jσiσj∆t.

The proof of Proposition 1 is reported in the Appendix A. Equation (4.5) allows one to compute the option
price at time t = 0 by proceeding backward. In fact, the function u (tN , ·) is known at time tN = T throught
(4.2) since the price function v (tN , ·) is equal to the payoﬀ function Ψ (·). Moreover, if an approximation of
u (tn+1, ·) is available, then one can approximate u (tn, ·) at Z by means of relation (4.5). Finally, the option
price at time t = 0 is approximated by uGP R−EI
(log (S0)).

0

5 American options in the rough Bergomi model

The rough Bergomi model, introduced by Bayer et al. [5], shapes the underlying process St and its volatility
Vt through the following relations:

dSt = rStdt +

Vt = ξ0 (t) exp

(cid:112)

VtStdW 1
t
(cid:18)

η(cid:102)W H

t −

η2t2H

(cid:19)

,

1
2

(5.1)

(5.2)

6

with r the (constant) interest rate, η a positive parameter and H ∈ (0, 1) the Hurst parameter. The
deterministic function ξ0 (t) represents the forward variance curve and following Bayer et al. [5] we consider
it as constant. The process W 1
is a Brownian motion, whereas (cid:102)W H
is a Riemann-Liouville fractional
t
t
Brownian motion that can be expressed as a stochastic integral:

(cid:102)W H

t =

(cid:90) t

√

2H

0

(t − s)H− 1

2 dW 2
t ,

(5.3)

with W 2

t a Brownian motion and ρ the instantaneous correlation coeﬃcient between W 1

t and W 2
t .

The rough Bergomi model stood out for its ability to explain implied volatility and other phenomena
related to European options. Moreover, it is particularly interesting from a computational point of view
as it is a non-Markovian model and therefore it is not possible to apply standard techniques for American
options.

In this framework, the price at time t of an American option having maturity T and payoﬀ function

Ψ : R+ → R is then

v(t, Ft) = sup
τ ∈Tt,T

(cid:104)

E

e−r(τ −t)Ψ(Sτ )|Ft

(cid:105)

,

(5.4)

for s ∈ [0, t]. We point out that, as
where Ft is the natural ﬁltration generated by the couple
opposed to the multi-dimensional Brownian motion, in this case, the stopping time τ does not only depend
from the actual values of S and V but, since these are non-Markovian processes, it depends on the whole
ﬁltration, that is from the whole observed history of the processes.

W 1

s , (cid:102)W H
s

(cid:16)

(cid:17)

6 The GPR-Tree method in the rough Bergomi model

The GPR-Tree method can be adapted to price American options in the rough Bergomi model. Despite
the dimension of the model is only two, it is a non-Markovian model which obliges one to take into account
the past history when evaluating the price of an option. So, the price of an option at a certain moment
depends on all the ﬁltration at that moment. Clearly, evaluating an option by considering the whole history
of the process (a continuous process) is not possible. To overcome such an issue, we simulate the process on
a ﬁnite number of dates and we consider the sub-ﬁltration induced by these observations. First of all, we
consider a ﬁnite number N of time steps that determines the time increment ∆t = T
N , and we employ the
scheme presented in Bayer et. al [5] to generate a set of P simulations of the couple (St, Vt) at tn = n ∆t
for n = 1, . . . , N . In particular, if we set ∆W 1
, then the 2N -dimensional random vector R,
given by

n = W 1
tn

− W 1

tn−1

(cid:16)

R =

∆W 1

1 , (cid:102)W H
t1

, . . . , ∆W 1

N , (cid:102)W H
tN

(cid:17)(cid:62)

,

(6.1)

follows a zero-mean Gaussian distribution. Moreover, using the relations stated in Appendix B, one can
calculate the covariance matrix Υ of R and its lower triangular square root Λ by using the Cholesky fac-
torization. The vector R can be simulated by computing ΛG, where G = (G1, . . . , G2N )(cid:62) is a vector of
independent standard Gaussian random variables. Finally, a simulation for (Stn , Vtn )n=0,...,N can be obtained
from R by considering the initial values

St0 = S0, Vt0 = ξ0,

(6.2)

7

and the Euler–Maruyama scheme given by

Stn+1 = Stn exp

r −

(cid:18)(cid:18)

(cid:19)

1
2

Vtn

∆t + (cid:112)Vtn∆W 1

n+1

(cid:19)

,

(cid:18)

Vtn+1 = ξ0 exp

−

1
2

η2 (tn+1)2H + η(cid:102)W H

tn+1

(cid:19)

.

(6.3)

(6.4)

First of all, the GPR-Tree method simulates P diﬀerent samples for the vector G, namely Gp for p =
(cid:1) according to (6.2), (6.3) and (6.4).
t1, V p

1, . . . , P , and it computes the corresponding paths (cid:0)Sp
tN , V p
tN
To summarize the values assumed by S and V , let us deﬁne the vector

t1 , . . . , Sp

(cid:16)

SVp

i:j =

ti, V p
Sp

ti , Sp

ti+1, V p

ti+1, . . . , Sp

tj , V p
tj

(cid:17)(cid:62)

(6.5)

for i, j ∈ {0, . . . , N } and i < j. Moreover, we also deﬁne

log (cid:0)SVp

i:j

(cid:1) =

(cid:16)

log (cid:0)Sp
ti

(cid:1) , log (cid:0)V p
ti

(cid:1) , log

(cid:17)

(cid:16)

Sp
ti+1

, log

(cid:17)

(cid:16)
V p
ti+1

, . . . , log

(cid:17)

(cid:16)

Sp
tj

, log

(cid:16)

V p
tj

(cid:17)(cid:17)(cid:62)

,

(6.6)

where log stands for the natural logarithm.

Then, the GPR-Tree method computes the option value for each of these P trajectories, proceeding
backward in time and considering the past history coded into the ﬁltration. Since we consider only a ﬁnite
number of steps, we approximate the ﬁltration Ftn with the natural ﬁltration ˆFtn generated by the 2n
. Moreover, ˆFtn is equal to the ﬁltration generated by St1, Vt1 , . . . , Stn , Vtn
variables W 1
t1
because there exists a deterministic bijective function that allows one to obtain W 1
from
t1
St1 , Vt1, . . . , Stn, Vtn and vice versa. Therefore, when we calculate the option value conditioned by ﬁltration
ˆFtn , it is enough to conditioning with respect to the knowledge of the variables St1, Vt1 , . . . , Stn , Vtn .

, . . . , W 1
tn

, . . . , W 1
tn

, (cid:102)W H
tn

, (cid:102)W H
t1

, (cid:102)W H
tn

, (cid:102)W H
t1

The GPR-Tree method proceeds backward in time, using a tree method and the GPR to calculate the
option price with respect to the initially simulated trajectories. As opposed to the multi-dimensional Black-
Scholes model, here we perform more than one single tree step, so as to reduce the number of GPR regressions
and thus increasing the computational eﬃciency. In particular, we consider N = N T ree · m with N T ree and
m natural numbers that represent how many times the tree method is used and the number of time steps
employed, respectively.

After simulating the P random paths {SVp

option value v

(cid:16)
tN −m, SVp

1:(N −m)

(cid:17)

at time tN −m for each path as follows:

1:N , p = 1, . . . , P }, we compute the tree approximation of the

(cid:16)

vT ree
N −m

SVp

1:(N −m)

(cid:17)

= max

(cid:16)

(cid:16)

Ψ

Sp
tN −m

(cid:17)

, C T ree
N −m

(cid:16)

SVp

1:(N −m)

(cid:17)(cid:17)

,

(6.7)

with C T ree
N −m stands for the the approximation of the continuation value function at time tN −m obtained by
means of a tree approach, which discretizes each component of the Gaussian vector G[2(N −m)+1]:2N that
generates the process. As opposed to the multi-dimensional Black-Scholes model, the approximation of the
independent Gaussian components of G through the equiprobable couple {−1, +1} is not suitable since the
convergence to the right price is too slow. So, we propose to use the same discrete approximation employed
by Alfonsi in [2], which is stated in the following Lemma.

Lemma 2. The discrete variable A deﬁned by P
√

(cid:16)

= P

(cid:112)

A = −

3 −

√

(cid:17)

6

√

(cid:16)

P

(cid:112)

A =

(cid:17)
6
random variable.

3 −

= 1

2 −

6−2
√
6
4

(cid:16)

A =

(cid:112)

3 +

√

(cid:17)

6

(cid:16)

= P

A = −

(cid:112)

3 +

√

(cid:17)
6

=

√

6−2
√
6
4

and

ﬁts the ﬁrst seven moments of a standard Gaussian

8

So, for each path p, we consider a quadrinomial tree with m time steps, and we use it to compute
deﬁned

the continuation value. In particular, we consider the discrete time process

(cid:16) ˆSp

(cid:17)

k, ˆV p

k

k∈{N −m,...,N }

through

ˆSp
N −m = Sp

tN −m , ˆV p

N −m = V p

tN −m

k+1 = ˆSp
ˆSp

j−1 exp
(cid:18)

(cid:18)(cid:18)

r −

(cid:19)

1
2

ˆV p
k

(cid:113)

∆t +

k Λ2k+1 ˆGp
ˆV p
(cid:19)

ˆV p
k+1 = ξ0 exp

−

η2 (tk+1)2H + ηΛ2k+2 ˆGp

,

1
2

(cid:19)

,

(6.8)

(6.9)

(6.10)

where Λ2k+1 is the 2k + 1-th rows of the matrix Λ and Λ2k+2 the 2k + 2-th row. Moreover, ˆGp
j = 1, . . . , 2 (N − m) and the other components, that is ˆGp
using the random variable A of Lemma 2.

j for
j for j = 2 (N − m) + 1, . . . , 2N , are sampled by

j = Gp

(cid:17)

(cid:16) ˆSp

An option value is assigned to each node of the tree: at maturity, that is for k = N, it is equal to the
payoﬀ Ψ
, and for k = N − m, . . . , N − 1 it can be obtained as the maximum between the exercise
value and the discounted mean value at the future nodes, weighted according to the transition probabilities
determined by the probability distribution of A.

N

This approach allows us to compute the function vGP R−T ree

for p = 1, . . . , P . We point
out that, since the quadrinomial tree is not recombinant, the number of nodes grows exponentially with
the number of time steps m. Therefore, m must be small. A similar problem arises with the tree approach
proposed by Horvat et al. [23]. In order to overcome such an issue, we apply the GPR method to approximate
the function uGP R−T ree
= vGP R−T ree
. Speciﬁcally, consider a natural
N −m
number J and deﬁne dn = 2 min (n, J + 1). We train the GPR method considering the predictor set given
by

(cid:16)
SVp

SVp

1:(N −m)

1:(N −m)

1:(N −m)

N −m

N −m

log

(cid:17)(cid:17)

(cid:17)

(cid:16)

(cid:16)

SVp

(cid:16)

(cid:17)

(cid:110)

X =

xp = log

(cid:16)

SVp

max{1,N −m−J}:N −m

, p = 1, . . . , P

(cid:17)

(cid:111)

and the response y ∈ RP given by

yp = vT ree
N −m

(cid:16)

SVp

1:(N −m)

(cid:17)

.

⊂ RdN −m

(6.11)

(6.12)

(cid:17)

max{1,N −m−J}:N −m

We term uGP R
N −m the function obtained by the aforementioned regression, which depends on
(cid:16)
SVp
log
. We stress out that if we consider J = N − m − 1 (or greater), then the
function uGP R
N −m would consider all the observed values of S and V as predictors. Anyway, numerical tests
show that it is enough to consider smaller values of J, which reduces the dimension dN −m of the regression
and thus improves the numerical eﬃciency. A similar approach is taken by Bayer et al. [6].
(cid:16)
tN −2m, SVp

at time
N −m, we can approximate the option value v
tN −2m by means of the tree approach again. The only diﬀerence in this case is that the value attributed
to the terminal nodes is not determined by the payoﬀ function, but through the function uGP R
N −m. We term
vGP R−T ree
the function obtained after this backward tree step. If we train the GPR method considering the
N −2m
predictor set given by

Once we have obtained uGP R

1:(N −2m)

(cid:17)

X =

(cid:110)

xp = log

(cid:16)
SVp

max{1,N −2m−J}:N −2m

(cid:17)

, p = 1, . . . , P

(cid:111)

⊂ RdN −2m

and the response y ∈ RP given by

yp = vT ree

N −2m

(cid:16)
SVp

1:(N −2m)

(cid:17)

,

(6.13)

(6.14)

then we obtain the function uGP R
proceeding backward up to obtaining the initial option price by backward induction.

N −2m, which can be employed to repeat the tree step and the GPR step,

9

7 The GPR-EI method in the rough Bergomi model

The GPR-EI method can be adapted to price American options in the rough Bergomi model. Just like the
GPR-Tree approach, the GPR-EI method starts by simulating P diﬀerent paths (cid:0)Sp
(cid:1) for
the processes S and V , and it goes on by solving a backward induction problem, through the use of the GPR
method and a closed formula for integration.

t1 , . . . , Sp

tN , V p
tN

t1, V p

As opposed to the multi-dimensional Black-Scholes model, in the rough Bergomi case the use of the
squared exponential kernel is not suitable because it is a isotropic kernel and the predictors employed
have diﬀerent nature (prices and volatilities at diﬀerent times) and thus changes in each predictor impact
diﬀerently on the price. So, we employ the Automatic Relevance Determination (ARD) Squared Exponential
Kernel, that has separate length scale for each predictor and it is given by

kASE (a, b) = σ2

f exp

−

(cid:32)

(cid:33)

d
(cid:88)

i=1

(ai − bi)2
2σ2
i

, a, b ∈ Rd.

(7.1)

with d the number of the considered predictors. Speciﬁcally, the GPR-EI method relies on the following
Propositions.

Proposition 3. The GPR-EI approximation of the option value at time tN −1 at SVp
given by:

max{1,N −1−J}:(N −1) is

(cid:16)

vGP R−EI
N −1

SVp

max{1,N −1−J}:(N −1)

(cid:17)



= max

Ψ

(cid:16)
Sp
tN −1

(cid:17)

,

P
(cid:88)

q=1

ωqe−r∆tσ2
(cid:113)

f σl
N,p + σ2
σ2
l

(cid:32)

exp

−

(cid:1) − µN,p

(cid:0)log (cid:0)Sq
tN
N,p + 2σ2
2σ2
l

(cid:1)2

(cid:33)


(7.2)
where σf , σj, and ω1, . . . , ωP are certain constants determined by the GPR approximation of the function
log (ST ) (cid:55)→ Ψ (ST ). Moreover,

and

µN,p = log

(cid:16)

Sp
tN −1

(cid:17)

(cid:18)

+

r −

(cid:113)

1
2

V p
tN −1

(cid:19)

∆t

N,p = V p
σ2

tN −1 ∆t.

(7.3)

(7.4)

The proof of Proposition 3 is reported in the Appendix C. Therefore, we can compute the value of the

option at time tN −1 for each simulated path by using (7.2).

Proposition 4. Let n ∈ {0, . . . , N − 2} and suppose the option price function v (tn+1, ·) at time tn+1 to be
known for all the simulated paths {SVp

1:N , p = 1, . . . , P }. Deﬁne

µn+1,p =

(cid:18)

log (cid:0)Sp
tn

(cid:18)

(cid:1) +

r −

(cid:19)

1
2

V p
tn

∆t, log (ξ0) + ηΛ2n+2Gp −

,

(7.5)

(cid:19)(cid:62)

η2t2H
n+1

1
2
2n, 0 . . . , 0)(cid:62), and

where Λ2n+2 is the 2n + 2-th row of the matrix Λ and Gp = (Gp

1, . . . , Gp

Σn+1,p =





(cid:113)

η

∆tV p
tn

∆tV p

tn Λ2n,2n+1

(cid:113)

η
η2 (cid:0)Λ2

∆tV p

tnΛ2n+2,2n+1

2n+2,2n+2 + Λ2

2n+2,2n+1



 ,

(cid:1)

(7.6)

where Λi,j stands for the element of Λ in position i, j. Moreover, consider a natural number J ∈ N
and set dn+1 = 2 min {n + 1, J + 1} . Then, the GPR-EI approximation of the option value at time tn at

10

SVp

max{1,n−J}:n is given by

(cid:16)

vGP R−EI
n

SVp

max{1,n−J}:n

(cid:17)

= max

(cid:32)

Ψ (cid:0)Sp
tn

(cid:1) , e−r∆tσ2

f σdn+1−1σdn+1

(cid:33)

ωqhp

qf p
q

P
(cid:88)

q=1

(7.7)

where σdn+1−1, σdn+1, σf and ω1, . . . , ωP are certain constants determined by the GPR approximation of the
function log (SV1:n+1) (cid:55)→ v (tn+1, SV1:n+1) considering
as the predic-
tor set. Moreover, hp

max{1,n+1−J}:n+1, p = 1, . . . , P

q are two factors given by

q and f p

SVp

(cid:111)

(cid:110)

(cid:18)

− (cid:80)dn+1−2
i=1

hp
q =




exp


1

i )2

(zp

i −zq
2σ2
i

(cid:19)

if n > 0

if n = 0

(7.8)

and

f p
q =


− 1
2

exp

(cid:32)(cid:32)

zq
dn+1−1
zq
dn+1

(cid:33)

(cid:33)(cid:62) (cid:32)

− µn+1,p

Σn+1,p +

(cid:118)
(cid:117)
(cid:117)
(cid:116)det

(cid:32)

Σn+1,p +

(cid:32)

(cid:32)

σ2

dn+1−1
0

σ2

dn+1−1
0

0
σ2

dn+1

0
σ2

dn+1

(cid:33)(cid:33)−1 (cid:32)(cid:32)

(cid:33)(cid:33)

(cid:33)

zq
dn+1−1
zq
dn+1

− µn+1,p

(cid:33)


,

where zp

i = log

(cid:16)

Sp
n+1−(i−1)/2

(cid:17)

if i is even and zp

i = log

(cid:16)

V p
n+1−i/2

(cid:17)

if i is odd, for i = 1, . . . , dn+1.

(7.9)

The proof of Proposition 4 is reported in the Appendix D. Relations (7.2) and (7.7) can be used to

compute the option price at time t = 0 by backward induction.

8 Numerical results

In this Section we present some numerical results about the eﬀectiveness of the proposed algorithms. The
ﬁrst section is devoted to the numerical tests about the multi-dimensional Black-Scholes model, while the
second is devoted to the rough Bergomi model. The algorithms have been implemented in MATLAB and
computations have been preformed on a server which employs a 2.40 GHz Intel R(cid:13) Xenon R(cid:13) processor (Gold
6148, Skylake) and 20 GB of RAM.

8.1 Multi-dimensional Black-Scholes model

Following Gouden ˜Aˇsge et al. [18], we consider an Arithmetic basket Put, a Geometric basket Put and a Call
on the Maximum of d-assets.

In particular, we use the following parameters T = 1, Si

0 = 100, K = 100, r = 0.05, constant volatilities
σi = 0.2, constant correlations ρij = 0.2 and N = 10 exercise dates. Moreover, we consider P = 250, 500
or 1000 points. As opposed to the other input parameters, we vary the dimension d, considering d =
2, 5, 10, 20, 40 and 100.

We present now the numerical results obtained with the GPR-Tree and the GPR-EI methods for the

three payoﬀ examples.

11

d

2

5

10

20

40

100

P

250

GPR-Tree
500

1000

GPR-EI
500

250

1000

GPR-MC

Ekvall Benchmark

4.61
(4)

3.44
(9)

3.00
(10)

4.61
(7)

3.43
(15)

2.96
(33)

4.61
(22)

3.44
(23)

2.93
(60)

2.80
(4220)

2.72
(14304)

2.72
(49609)

4.58
(4)

3.40
(4)

2.85
(4)

2.63
(4)

2.45
(4)

2.27
(5)

4.58
(9)

3.43
(14)

2.88
(9)

2.73
(9)

2.52
(10)

2.32
(15)

4.57
(26)

3.41
(27)

2.93
(30)

2.63
(29)

2.53
(38)

2.39
(45)

4.57

3.41

2.90

2.70

2.57

2.40

4.62

3.44

4.62

3.45

2.97

2.70

2.56

2.47

Table 1: Results for a Geometric basket Put option using the GPR-Tree method and the GPR-EI method. In the
last columns, the prices obtained by using the GPR-MC method, the Ekvall multi-dimensional tree and the exact

benchmark (d is the dimension and P is the number of points). Values in brackets are the computational times (in

seconds).

8.1.1 Geometric basket Put option

Geometric basket Put is a particularly interesting option since it is possible to reduce the problem of pricing
it in the d-dimensional model to a one dimensional American Put option in the Black-Scholes model which
can be priced straightforwardly, for example using the CRR algorithm with 1000 steps (see Cox et al. [12]).
Therefore, in this case, we have a reliable benchmark to test the proposed methods. Moreover, when d is
smaller than 10 we can also compute the price by means of a multi-dimensional binomial tree (see Ekvall
[15]). In particular, the number of steps employed for the multi-dimensional binomial tree is equal to 200
when d = 2 and to 50 when d = 5. For values of d larger than 5, prices cannot be approximated via such
a tree, because the memory required for the calculations would be too large. Furthermore, we also report
the prices obtained with the GPR-MC method, employing P = 1000 points and M = 105 Monte Carlo
simulations, for comparison purposes. As far as the GPR-Tree is concerned, we compute the prices only for
the values of d smaller than 40 since for higher values of d the tree step becomes over time demanding. In
fact, the computation of the continuation value with the tree step grows exponentially with the dimension
d and for d = 40 it requires the evaluation of the GPR approximation at 240 ≈ 1012 points for every times
step and for every point of X.

Results are reported in Table 1. We observe that the two proposed methods provide accurate and stable
results and the computational time is generally very small, except for the GPR-Tree method at d = 20.
Moreover, the computer processing time of the GRP-EI method increases little with the size of the problem
and this makes the method particularly eﬀective when the dimension of the problem is high. This is because
the computation of the expected value and the training of the GPR model are minimally aﬀected by the
dimension of the problem.

Figure 8.1 investigates the convergence of the GPR methods changing the dimension d. As we can see,
the relative error is small with all the considered methods, but the computational time required by the
GPR-Tree method and the GPR-EI method is generally smaller with respect to the GPR-MC method.

12

Figure 8.1: Comparison among the GPR methods changing the dimension d and doubling the number of points from
P = 250 to P = 8000. As far as the GPR-MC method is concerned M = 104 Monte Carlo simulations are employed.

13

8.1.2 Arithmetic basket Put option

As opposed to the Geometric basket Put option, in this case we have no method to obtain a fully reliable
benchmark. Therefore we only consider the prices obtained by means of the GPR-MC method, employed
with P = 1000 points and M = 105 Monte Carlo simulations. Moreover, for small values of d, a benchmark
can be obtained by means of a multi-dimensional tree method (see Boyle et al.
[10]), just as shown for
the Geometric case. Results are reported in Table 2. Similarly to the Geometric basket Put, the prices
obtained are reliable and they do not change much with respect to the number P of points. As opposed to
the GPR-Tree method, which can not be applied for high values of d, the GPR-EI method requires a small
computational time for all the values concerned of d.

d

2

5

10

20

40

100

P

250

GPR-Tree
500

1000

GPR-EI
500

250

1000

GPR-MC

Ekvall

4.42
(5)

3.15
(5)

2.71
(10)

4.42
(9)

3.12
(9)

2.64
(21)

4.42
(25)

3.13
(24)

2.62
(70)

2.37
(4259)

2.35
(16343)

2.40
(57399)

4.42

3.15

4.38
(4)

3.09
(6)

2.49
(5)

2.26
(6)

2.18
(4)

2.35
(7)

4.38
(9)

3.12
(9)

2.56
(9)

2.31
(14)

2.18
(10)

2.01
(13)

4.37
(28)

3.10
(44)

2.60
(38)

2.28
(42)

2.16
(31)

2.06
(42)

4.37

3.09

2.58

2.38

2.17

1.92

Table 2: Results for an Arithmetic basket Put option using the GPR-Tree method and the GPR-EI method. In
the last columns, the prices obtained by using the GPR-MC method and the Ekvall multi-dimensional tree (d is the

dimension and P is the number of points). Values in brackets are the computational times (in seconds).

8.1.3 Call on the Maximum

As for the Arithmetic basket Put, in this case we have no numerical methods to obtain a fully reliable
benchmark. However, for small values of d, we can approximate the price obtained by means of a multi-
dimensional tree method. Moreover, we also consider the price obtained with the GPR-MC method. Results,
which are shown in Table 3, have an accuracy comparable to the one obtained for the Arithmetic basket Put
option.

14

P

250

GPR-Tree
500

16.94
(5)

27.14
(5)

35.27
(11)

16.94
(8)

27.17
(10)

34.97
(21)

1000

16.93
(20)

27.19
(26)

35.08
(106)

43.26
(4126)

43.21
(15025)

43.00
(51090)

d

2

5

10

20

40

100

GPR-EI
500

16.81
(10)

27.15
(9)

34.98
(10)

42.74
(11)

50.36
(10)

60.72
(13)

250

16.75
(4)

26.92
(4)

35.66
(4)

45.05
(4)

51.79
(5)

59.03
(5)

1000

16.82
(28)

26.95
(27)

34.84
(29)

42.62
(35)

49.53
(41)

60.96
(42)

GPR-MC

Ekvall

16.86

27.20

16.86

27.20

35.17

42.76

50.70

59.69

Table 3: Results for a Call on Maximum Put option using the GPR-Tree method and the GPR-EI method.
the last columns, the prices obtained by using the GPR-MC method and the Ekvall multi-dimensional tree (d is the

In

dimension and P is the number of points). Values in brackets are the computational times (in seconds).

8.2 Rough Bergomi model

Following Bayer et al.
[6], we consider an American Put option and we use the same parameters: T = 1,
H = 0.07, ρ = −0.90, ξ0 = 0.09, η = 1.9, S0 = 100, r = 0.05 and strike K = 70, 80, . . . , 120, 130 or 140. As
far as the GPR-Tree is concerned, we employ N = 50 or N = 100 time steps with m = 2, P = 500, 1000, 2000
or 4000 random paths, and J = 0, 1, 3, 7 or 15 past values. As far as the GPR-EI is concerned, we employ
N = 50 or N = 100 time steps, P = 1000, 2000, 4000 or 8000 random paths, and J = 0, 1, 3, 7 or 15 past
values. Similar to what observed by Bayer et al. [6], the diﬀerence changing the value of J does not impact
signiﬁcantly on the price, which indicates that considering the non-Markovian nature of the processes in
the formulation of the exercise strategies is not particularly relevant. Conversely, using a large number of
predictors signiﬁcantly increases computational time. Numerical results are reported in Tables 4 and 5,
together with the results reported by Bayer et al. in [6]. Prices are very close to the benchmark, except for
the case K = 120: in this case with both the two GPR methods we obtain a price which is close to 20.20
while Bayer et al. obtain 20.00. Anyway, it is worth noticing that the relative gap between these two results
is less than 1% .

15

GPR-Tree

Bayer et al.

N

P

50

100

500

1000

2000

4000

500

1000

2000

4000

1.87
(28)
1.86
(44)
1.87
(71)
1.91
(168)
1.94
(248)

3.18
(31)
3.19
(47)
3.19
(93)
3.21
(136)
3.24
(322)

5.24
(28)
5.25
(44)
5.27
(94)
5.28
(150)
5.28
(269)

8.36
(29)
8.39
(47)
8.42
(89)
8.43
(173)
8.44
(340)

13.04
(32)
13.09
(67)
13.11
(78)
13.11
(157)
13.09
(254)

20.19
(37)
20.20
(49)
20.19
(98)
20.20
(152)
20.19
(278)

1.88
(97)
1.87
(183)
1.86
(296)
1.87
(563)
1.88
(986)

3.19
(117)
3.19
(152)
3.19
(287)
3.20
(624)
3.20
(1186)

5.24
(102)
5.25
(163)
5.26
(330)
5.30
(561)
5.27
(1000)

8.37
(103)
8.40
(177)
8.42
(302)
8.43
(552)
8.44
(1134)

13.06
(90)
13.09
(180)
13.14
(282)
13.14
(520)
13.15
(1007)

20.19
(121)
20.21
(180)
20.19
(268)
20.18
(511)
20.17
(1036)

1.88
(391)
1.88
(607)
1.87
(1084)
1.86
(1930)
1.87
(4841)

3.20
(376)
3.20
(569)
3.20
(1070)
3.20
(2653)
3.20
(7011)

5.25
(359)
5.26
(512)
5.28
(1058)
5.29
(2253)
5.28
(4348)

8.37
(329)
8.39
(510)
8.43
(986)
8.44
(2083)
8.45
(4637)

13.08
(334)
13.12
(544)
13.17
(1119)
13.19
(2083)
13.17
(4449)

20.20
(304)
20.21
(494)
20.25
(1120)
20.17
(1935)
20.22
(4161)

1.86
(876)
1.87
(1672)
1.87
(3742)
1.87
(3501)
1.87
(7806)

3.20
(823)
3.20
(1166)
3.20
(2095)
3.21
(3721)
3.21
(7392)

5.26
(702)
5.27
(1185)
5.28
(1756)
5.29
(3595)
5.29
(7411)

8.39
(748)
8.42
(1145)
8.45
(1844)
8.45
(3926)
8.46
(7013)

13.12
(695)
13.15
(1135)
13.18
(1896)
13.19
(3659)
13.20
(7668)

20.22
(692)
20.25
(1047)
20.26
(2077)
20.26
(3592)
20.24
(7844)

1.87
(71)
1.86
(95)
1.86
(163)
1.85
(275)
1.87
(541)

3.17
(85)
3.18
(119)
3.17
(167)
3.19
(301)
3.18
(633)

5.25
(82)
5.27
(109)
5.30
(177)
5.30
(315)
5.28
(533)

8.42
(70)
8.43
(89)
8.47
(167)
8.47
(322)
8.51
(684)

13.15
(77)
13.17
(95)
13.18
(158)
13.20
(318)
13.22
(625)

20.21
(79)
20.21
(95)
20.23
(156)
20.25
(363)
20.18
(624)

1.86
(236)
1.87
(310)
1.87
(594)
1.87
(1141)
1.88
(2171)

3.18
(216)
3.20
(322)
3.21
(617)
3.21
(1134)
3.20
(1940)

5.28
(223)
5.29
(283)
5.31
(555)
5.33
(1089)
5.33
(2127)

8.45
(190)
8.46
(325)
8.50
(551)
8.51
(1117)
8.53
(2229)

13.18
(237)
13.20
(296)
13.23
(575)
13.22
(1058)
13.26
(2582)

20.24
(206)
20.24
(283)
20.26
(588)
20.25
(1139)
20.22
(1951)

1.86
(646)
1.87
(1222)
1.88
(1962)
1.88
(4579)
1.87
(10169)

3.19
(603)
3.20
(1107)
3.21
(1966)
3.21
(4179)
3.20
(10317)

5.26
(707)
5.28
(1144)
5.31
(1833)
5.33
(4319)
5.34
(17098)

8.42
(584)
8.44
(969)
8.49
(2322)
8.49
(4120)
8.48
(8403)

13.16
(572)
13.19
(1192)
13.23
(1917)
13.24
(4508)
13.27
(9055)

20.22
(662)
20.24
(959)
20.26
(2075)
20.23
(4395)
20.19
(8057)

1.86
(1337)
1.87
(2265)
1.88
(4222)
1.88
(7997)
1.86
(16466)

3.19
(1368)
3.20
(2396)
3.22
(4043)
3.23
(8031)
3.23
(20584)

5.28
(1504)
5.30
(3954)
5.32
(4226)
5.33
(7073)
5.34
(16804)

8.46
(1313)
8.48
(2058)
8.51
(4439)
8.53
(8324)
8.53
(14183)

13.20
(1364)
13.22
(2207)
13.26
(4028)
13.29
(7440)
13.24
(13191)

20.23
(1484)
20.26
(2029)
20.24
(3705)
20.28
(6293)
20.28
(15643)

Always 30.00

Always 40.00

Always 30.00

Always 40.00

1.88

3.22

5.31

8.50

13.23

20.00

30.00

40.00

J

0

1

3

7

15

0

1

3

7

15

0

1

3

7

15

0

1

3

7

15

0

1

3

7

15

0

1

3

7

15

K

70

80

90

100

110

120

130

140

Table 4: Results for an American Put option in the rough Bergomi model using the GPR-Tree method. N represents
the number of time steps, P the number of the simulated paths and J the number of past values employed in the

regression. Values in brackets are the computational times (in seconds).

16

GPR-EI

Bayer et al.

N

P

50

100

1000

2000

4000

8000

1000

2000

4000

8000

1.82
(101)
1.82
(96)
1.83
(263)
1.81
(497)
1.78
(820)

3.14
(86)
3.14
(127)
3.14
(160)
3.15
(453)
3.12
(947)

5.19
(77)
5.19
(89)
5.22
(239)
5.19
(307)
5.23
(1189)

8.30
(81)
8.30
(93)
8.37
(250)
8.39
(476)
8.30
(573)

13.05
(84)
13.08
(190)
13.06
(180)
13.05
(360)
13.05
(812)

20.19
(86)
20.19
(93)
20.19
(180)
20.19
(323)
20.16
(1227)

1.84
(253)
1.85
(525)
1.85
(1305)
1.85
(2706)
1.84
(4939)

3.16
(271)
3.16
(409)
3.18
(1334)
3.18
(3263)
3.16
(7107)

5.22
(271)
5.22
(416)
5.26
(1036)
5.23
(2490)
5.25
(5729)

8.33
(260)
8.33
(402)
8.35
(851)
8.39
(2957)
8.34
(3808)

13.07
(229)
13.09
(444)
13.08
(728)
13.16
(4208)
13.09
(5221)

20.20
(281)
20.20
(372)
20.20
(675)
20.22
(2411)
20.20
(5759)

1.85
(351)
1.85
(636)
1.83
(1118)
1.85
(3014)
1.83
(5802)

3.18
(348)
3.19
(601)
3.19
(1190)
3.19
(3197)
3.19
(5650)

5.24
(353)
5.24
(455)
5.27
(1259)
5.26
(2348)
5.26
(6236)

8.36
(472)
8.36
(413)
8.43
(1412)
8.44
(3366)
8.42
(6466)

13.10
(325)
13.12
(476)
13.17
(1162)
13.13
(2252)
13.19
(6290)

20.21
(307)
20.21
(454)
20.20
(1002)
20.21
(2307)
20.19
(3662)

1.85
(533)
1.85
(884)
1.84
(1630)
1.85
(3447)
1.83
(6006)

3.17
(558)
3.18
(865)
3.19
(1476)
3.18
(3252)
3.16
(7575)

5.24
(517)
5.25
(748)
5.24
(1230)
5.25
(2534)
5.27
(6503)

8.38
(566)
8.38
(732)
8.38
(1028)
8.42
(3556)
8.45
(10222)

13.10
(519)
13.14
(796)
13.10
(1111)
13.13
(2111)
13.12
(5257)

20.21
(704)
20.21
(736)
20.19
(1286)
20.22
(3043)
20.21
(7173)

1.86
(162)
1.86
(184)
1.86
(369)
1.80
(657)
1.79
(1932)

3.22
(162)
3.23
(212)
3.22
(357)
3.22
(631)
3.17
(2103)

5.29
(166)
5.31
(223)
5.33
(493)
5.32
(1584)
5.28
(2120)

8.44
(189)
8.44
(191)
8.44
(362)
8.44
(670)
8.44
(1361)

13.20
(216)
13.20
(182)
13.24
(454)
13.20
(772)
13.27
(2118)

20.24
(174)
20.24
(200)
20.25
(468)
20.24
(696)
20.24
(1300)

1.88
(579)
1.88
(816)
1.88
(2389)
1.87
(4848)
1.83
(11876)

3.24
(549)
3.24
(984)
3.24
(1411)
3.24
(5813)
3.12
(17466)

5.30
(470)
5.32
(887)
5.34
(2624)
5.30
(3909)
5.28
(9220)

8.46
(466)
8.46
(742)
8.46
(1256)
8.47
(3808)
8.46
(9213)

13.18
(486)
13.18
(737)
13.20
(1635)
13.25
(4496)
13.21
(9895)

20.21
(535)
20.21
(776)
20.22
(1825)
20.20
(5008)
20.22
(9185)

1.87
(689)
1.87
(913)
1.88
(2831)
1.88
(5576)
1.85
(14703)

3.21
(602)
3.21
(847)
3.23
(2739)
3.23
(5327)
3.23
(15258)

5.28
(608)
5.30
(1146)
5.28
(1427)
5.30
(4560)
5.28
(9943)

8.45
(625)
8.48
(1189)
8.51
(2344)
8.53
(4867)
8.49
(12488)

13.17
(646)
13.17
(857)
13.21
(2035)
13.24
(4532)
13.28
(13941)

20.21
(620)
20.22
(1025)
20.21
(1418)
20.21
(2715)
20.20
(5834)

1.88
(1011)
1.88
(1551)
1.89
(2994)
1.86
(4132)
1.88
(5870)

3.22
(1065)
3.22
(1285)
3.21
(2387)
3.25
(5035)
3.22
(5974)

5.29
(993)
5.29
(1266)
5.33
(2387)
5.34
(5803)
5.29
(6216)

8.45
(1099)
8.46
(1362)
8.45
(1886)
8.52
(5165)
8.51
(11531)

13.17
(1048)
13.17
(1770)
13.27
(2751)
13.21
(4336)
13.22
(6948)

20.21
(1087)
20.21
(1311)
20.21
(1971)
20.19
(4496)
20.21
(7580)

Always 30.00

Always 40.00

Always 30.00

Always 40.00

1.88

3.22

5.31

8.50

13.23

20.00

30.00

40.00

J

0

1

3

7

15

0

1

3

7

15

0

1

3

7

15

0

1

3

7

15

0

1

3

7

15

0

1

3

7

15

K

70

80

90

100

110

120

130

140

Table 5: Results for an American Put option in the rough Bergomi model using the GPR-EI method. N represents
the number of time steps, P the number of the simulated paths and J the number of past values employed in the

regression. Values in brackets are the computational times (in seconds).

17

9 Conclusions

In this paper we have presented two numerical methods to compute the price of American options on a
basket of underlyings following the Black-Scholes dynamics. These two methods are based on the GPR-
Monte Carlo method and improve its results in terms of accuracy and computational time. The GPR-Tree
method can be applied for dimensions up to d = 20 and it proves to be very eﬃcient when d ≤ 10. The
GPR-Exact Integration method proves to be particularly ﬂexible and stands out for the small computational
cost which allows one to obtain excellent estimates in a very short time. The two methods also turns out to
be an eﬀective tool to address non-Markovian problems such as the pricing of American options in the rough
Bergomi model. These two methods are thus a step forward in overcoming the curse of dimensionality.

References

[1] L. Abbas-Turki and B. Lapeyre. American options by Malliavin calculus and nonparametric variance

and bias reduction methods. SIAM Journal on Financial Mathematics, 3(1):479–510, 2012.

[2] A. Alfonsi. High order discretization schemes for the CIR process: application to aﬃne term structure

and Heston models. Mathematics of Computation, 79(269):209–237, 2010.

[3] V. Bally, L. Caramellino, and A. Zanette. Pricing and hedging American options by Monte Carlo
methods using a Malliavin calculus approach. Monte Carlo Methods and Application, 11(2):97–133,
2005.

[4] V. Bally, G. Pag`es, and J. Printems. First-order schemes in the numerical quantization method. Math-

ematical Finance, 13(1):1–16, 2003.

[5] C. Bayer, P. Friz, and J. Gatheral. Pricing under rough volatility. Quantitative Finance, 16(6):887–904,

2016.

[6] C. Bayer, R. Tempone, and S. Wolfers. Pricing American options by exercise rate optimization. arXiv:

1809.07300, 2018.

[7] C. Beck, W. E, and A. Jentzen. Machine Learning approximation algorithms for high-dimensional
fully nonlinear partial diﬀerential equations and second-order backward stochastic diﬀerential equations.
Journal of Nonlinear Science, pages 1–57, 2017.

[8] S. Becker, P. Cheridito, and A. Jentzen. Deep optimal stopping. Journal of Machine Learning Research,

20(74):1–25, 2019.

[9] B. Bouchard and N. Touzi. Discrete-time approximation and Monte-Carlo simulation of backward
stochastic diﬀerential equations. Stochastic Processes and their applications, 111(2):175–206, 2004.

[10] P. P. Boyle, J. Evnine, and S. Gibbs. Numerical evaluation of multivariate contingent claims. The

Review of Financial Studies, 2(2):241–250, 1989.

[11] M. Broadie and P. Glasserman. Pricing American-style securities using simulation. Journal of Economic

Dynamics and Control, 21(8-9):1323–1352, 1997.

[12] J. C. Cox, S. A. Ross, and M. Rubinstein. Option pricing: A simpliﬁed approach. Journal of Financial

Economics, 7(3):229–263, 1979.

18

[13] J. De Spiegeleer, D. B. Madan, S. Reyners, and W. Schoutens. Machine Learning for quantitative
ﬁnance: fast derivative pricing, hedging and ﬁtting. Quantitative Finance, 18(10):1635–1643, 2018.

[14] W. E, J. Han, and A. Jentzen. Deep Learning-based numerical methods for high-dimensional parabolic
partial diﬀerential equations and backward stochastic diﬀerential equations. Communications in Math-
ematics and Statistics, 5(4):349–380, 2017.

[15] N. Ekvall. A lattice approach for pricing of multivariate contingent claims. European Journal of

Operational Research, 91(2):214–228, 1996.

[16] G. Gan. Application of Data Clustering and Machine Learning in Variable Annuity valuation. Insurance:

Mathematics and Economics, 53(3):795–801, 2013.

[17] G. Gan and X. S. Lin. Valuation of large variable annuity portfolios under nested simulation: A

functional data approach. Insurance: Mathematics and Economics, 62:138 – 150, 2015.

[18] L. Gouden`ege, A. Molent, and A. Zanette. Machine Learning for pricing American options in high

dimension. arXiv: 1903.11275, 2019.

[19] L. L. Gratiet and J. Garnier. Regularity dependence of the rate of convergence of the learning curve for

Gaussian process regression. arXiv: 1210.2879, 2012.

[20] J. Han, A. Jentzen, and W. E. Solving high-dimensional partial diﬀerential equations using Deep

Learning. Proceedings of the National Academy of Sciences, 115(34):8505–8510, 2018.

[21] M. B. Haugh and L. Kogan. Pricing American options: a duality approach. Operations Research,

52(2):258–270, 2004.

[22] R. V. Hogg, J. McKean, and A. T. Craig. Introduction to Mathematical Statistics. Pearson Education,

2005.

[23] B. Horvath, A. Jacquier, and A. Muguruza. Functional central limit theorems for rough volatility. arXiv:

1711.03078, 2017.

[24] S. Jain and C. W. Oosterlee. Pricing high-dimensional Bermudan options using the stochastic grid

method. International Journal of Computer Mathematics, 89(9):1186–1211, 2012.

[25] M. Kohler, A. Krzy˙zak, and N. Todorovic. Pricing of high-dimensional American options by neural

networks. Mathematical Finance, 20(3):383–410, 2010.

[26] J. Lelong. Dual pricing of American options by Wiener chaos expansion. SIAM Journal on Financial

Mathematics, 9(2):493–519, 2018.

[27] P. Lions and H. Regnier. Calcul du prix et des sensibilit´es d’une option am´ericaine par une m´ethode de

Monte Carlo. preprint, 2, 2001.

[28] F. A. Longstaﬀ and E. S. Schwartz. Valuing American options by simulation: a simple least-squares

approach. The Review of Financial Studies, 14(1):113–147, 2001.

[29] M. Ludkovski. Kriging metamodels and experimental design for Bermudan option pricing. Journal of

Computational Finance, 22(1), 2018.

19

[30] M. A. Nielsen. Neural Networks and Deep Learning, volume 25. Determination press San Francisco,

CA, USA, 2015.

[31] L. C. Rogers. Monte Carlo valuation of American options. Mathematical Finance, 12(3):271–286, 2002.

[32] J. N. Tsitsiklis and B. Van Roy. Optimal stopping of Markov processes: Hilbert space theory, ap-
IEEE

proximation algorithms, and an application to pricing high-dimensional ﬁnancial derivatives.
Transactions on Automatic Control, 44(10):1840–1851, 1999.

[33] C. K. Williams and C. E. Rasmussen. Gaussian Processes for Machine Learning, volume 2. MIT Press

Cambridge, MA, 2006.

A Proof of Proposition 1

Let n ∈ {0, . . . , N − 1} and suppose the function u (tn+1, ·) at time tn+1 to be known at Z. Let us deﬁne
the quantity

(cid:18)

(cid:18)

ˆxp = exp

zp +

r −

1
2

σ2

(cid:19)

(cid:19)

tn

for p = 1, . . . , P . The function u (tn, ·) at time tn at zp follows

u (tn, zp) = v (tn, ˆxp) .

= max (Ψ (ˆxp) , C (tn, ˆxp)) ,

C (tn, ˆxp) = Etn,ˆxp

(cid:2)e−r∆tv (cid:0)tn+1, Stn+1

(cid:1)(cid:3)

where

We can also write

C (tn, ˆxp) = Etn,ˆxp

(cid:20)
e−r∆tu

(cid:18)

tn+1, log (cid:0)Stn+1

(cid:1) −

(cid:18)

r −

(cid:19)

1
2

σ2

(cid:19)(cid:21)

tn+1

= Etn,ˆxp

(cid:2)e−r∆tu (cid:0)tn+1, Ztn+1

(cid:1)(cid:3)

where Ztn+1 is the random variable deﬁned as

Ztn+1 = log (cid:0)Stn+1

(cid:1) −

(cid:18)

r −

(cid:19)

1
2

σ2

tn+1.

(A.1)

(A.2)

(A.3)

(A.4)

(A.5)

(A.6)

(A.7)

Let us deﬁne Π = (Πi,j) as the d × d covariance matrix of the log-increments, that is Πi,j = ρi,jσiσj∆t .
Moreover, let Λ be a square root of Π and G as a vector that follows a standard Gaussian law. Then, we
observe that Ztn+1 has the following conditional law

In fact, simple Algebra leads to

Ztn+1 |Stn = ˆxp ∼ N (zp, Π) .

Ztn+1 = zp + ΛG.

Moreover, relation (A.8) can also be stated as

Ztn+1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18)

(cid:18)

log (Stn ) −

r −

(cid:19)

1
2

σ2

(cid:19)

tn = zp

∼ N (zp, Π) .

20

(A.8)

(A.9)

(A.10)

Let fzp (z) denote the density function of Ztn+1 given log (Stn ) − (cid:0)r − 1

2 σ2(cid:1) tn = zp . Speciﬁcally,

fzp (z) =

1

(2π)

d

2 (cid:112)det (Π)

(cid:18)

exp

−

1
2

(z − zp)(cid:62) Π−1 (z − zp)

(cid:19)

.

(A.11)

Then, according to (A.6),we can write

C (tn, ˆxp) = e−r∆t

(cid:90)

Rd

fzp (z) u (tn+1, z) dz.

(A.12)

Now, let us consider GPR approximation of the function u (tn+1, ·), obtained by assuming Z as the

predictor set and by employing the Squared Exponential Kernel kSE : Rd × Rd → R, which is given by

kSE (a, b) = σ2

f exp

−

(cid:32)

(a − b)(cid:62) Id (a − b)
2σ2
l

(cid:33)

, a, b ∈ Rd.

(A.13)

In particular, with reference to (A.13), the additional parameters σl and σf are called hyperparameters and
are obtained by means of a maximum likelihood estimation. So let

uGP R
n+1 (z) =

P
(cid:88)

q=1

ωqkSE (zq, z) ,

(A.14)

be the GPR approximation of the function u (tn+1, z), where ω = (ω1, . . . , ωq, . . . ωP )(cid:62) in (A.14) is a vector
of weights that can be computed by solving a linear system (see Rasmussen and Williams [33]). The GPR-EI
approximation C GP R−EI

of the continuation value is then given by

n

C GP R−EI

n

(ˆxp) = e−r∆t

= e−r∆t

(cid:90)

Rd

P
(cid:88)

q=1

fzp (z) uGP R

n+1 (z) dz

ωq

(cid:90)

Rd

fzp (z) kSE (zq, z) dz.

(A.15)

(A.16)

To compute each integral in (A.16), we observe that

(cid:90)

Rd

fzp (z) kSE (zq, z) dz =

= (2π)

d

2 σ2

f σd
l

(cid:90)

1

Rd

(2π)

d

2 (cid:112)det (Π)

e− 1

2 (z−zp)(cid:62)Π−1(z−zp)

1
(cid:113)
d
2

σ2d
l

(2π)

2 (z−zq)(cid:62)(σ2

l Id)−1

e− 1

= (2π)

d

2 σ2

f σd
l

(cid:90)

1

Rd

(2π)

d

2 (cid:112)det (Π)

e− 1

2 (z−zp)(cid:62)Π−1(z−zp)

1
(cid:113)
d
2

σ2d
l

(2π)

2 ((0−z)−(−zq))(cid:62)(σ2

l Id)−1

e− 1

(z−zq)dz

((0−z)−(−zq))dz

= (2π)

d

2 σ2

f σd

l fzp ∗ g−zq (0)

(A.17)

where ∗ is the convolution product and g−zq is the density function of a Gaussian random vector which has
(cid:1). Moreover, the convolution product of the densities of two independent random
law given by N (cid:0)−zq, σ2
variables is equal to the density of their sum (see Hogg et al. [22]) and we can obtain the following relation
which allows one to exactly compute the integrals in (A.16):

l Id

fxp ∗ g−xq (0) =

(2π)

1
2 (cid:112)det (Π + σ2

d

l Id)

21

2 (zq−zp)(cid:62)(Π+σ2

l Id)−1

e− 1

(zq−zp).

(A.18)

Therefore, the GPR-EI approximation C GP R−EI

n

at ˆxp reads

C GP R−EI

n

(ˆxp) = e−r∆t

P
(cid:88)

q=1

ωqσ2

f σd
l

2 (zq−zp)(cid:62)(Π+σ2

l Id)−1

e− 1

(zq−zp)

(cid:112)det (Π + σ2

l Id)

,

(A.19)

and the GPR-EI approximation uGP R−EI

n

of the option value u (tn, ·) at time tn and at zp is given by

uGP R−EI
n

(zp) = max

Ψ (ˆxp) , e−r∆t

(cid:32)

P
(cid:88)

q=1

ωqσ2

f σd
l

2 (zq−zp)(cid:62)(Π+σ2

l Id)−1

e− 1

(zq−zp)

(cid:33)

(cid:112)det (Π + σ2

l Id)

.

(A.20)

B Covariance of the vector R in (6.1)

Let us report the formulas for the covariance of the components of the vector R in (6.1). For all n = 1, . . . , N ,
and m = 1, . . . , n − 1, the following relations hold:

Cov (cid:0)∆W 1

n, ∆W 1
n

(cid:1) = ∆t,

(cid:16)

Cov

∆W 1

n, (cid:102)W H
tn

(cid:17)

=

(∆t)H+ 1
2 ,

Cov

(cid:16)

(cid:102)W H
tn

, (cid:102)W H
tn

= (tn)2H

√

2H
2ρ
2H + 1
(cid:17)

Cov (cid:0)∆W 1

m, ∆W 1
n

(cid:1) = 0,

(cid:16)

Cov

∆W 1

n, (cid:102)W H
tm

(cid:17)

= 0,

(cid:16)

Cov

∆W 1

m, (cid:102)W H
tn

(cid:17)

=

√

2ρ
2H
2H + 1

(cid:16)

(tn − tm−1)H+ 1

2 − (tn − tm)H+ 1

2

(cid:16)

Cov

(cid:102)W H
tm

, (cid:102)W H
tn

(cid:17)

= 2H (tm)2H ·

(cid:90) 1

0

(1 − s)

ds
2 −H (cid:16) tm
tn

1

C Proof of Proposition 3

(cid:17) 1

2 −H

− s

(B.1)

(B.2)

(B.3)

(B.4)

(B.5)

(B.6)

(B.7)

(cid:17)

,

.

Let us denote the random vector (cid:0)Sti, Vti, Sti+1, Vti+1, . . . , Stj , Vtj
for i, j ∈ {0, . . . , N } and i < j with
SVi:j . We observe that the option value v (tN , ·) at time tN is given by the payoﬀ function Ψ, which only
depends by the ﬁnal value of the underlying. The option value v (tN −1, ·) at time tN −1 about the p-th path
is given by

(cid:1)(cid:62)

(cid:16)

v

tN −1, SVp

1:(N −1)

(cid:17)

= max

Ψ

(cid:16)

(cid:16)

(cid:17)

Sp
tN −1

(cid:16)

, e−r∆tC

tN −1, SVp

1:(N −1)

(cid:17)(cid:17)

where C stands for the continuation value and it is equal to

(cid:16)

C

tN −1, SVp

1:(N −1)

(cid:17)

= E

(cid:104)
e−r∆tΨ (StN )

(cid:12)
(cid:16)
(cid:12)
(cid:12)

SV1:(N −1) = SVp

1:(N −1)

(cid:17) (cid:105)

.

(C.1)

(C.2)

We approximate the continuation value in (C.2) by means of the GPR approximation of Ψ. In particular,
let ΨGP R (z) be the approximation of the function z (cid:55)→ Ψ (exp (z)) by using the GPR method employing the

22

(C.3)

(C.4)

(C.5)

(C.6)

(C.7)

(C.8)

(C.9)

Squared Exponential Kernel and considering the log-underlying values at maturity as predictors. Speciﬁcally,
the predictor set is

Z = (cid:8)zp = log (cid:0)Sp
tN

(cid:1) , p = 1, . . . , P (cid:9) ⊂ R

and the response y ∈ RP is given by

In particular, we can write

yp = Ψ (cid:0)Sp
tN

(cid:1) .

ΨGP R (z) =

P
(cid:88)

q=1

kSE

(cid:0)log (cid:0)Sq
tN

(cid:1) , z(cid:1) ωq = σ2

f

P
(cid:88)

q=1

(cid:32)

exp

−

(cid:1) − z(cid:1)2

(cid:0)log (cid:0)Sq
tN
2σ2
l

(cid:33)

ωq

where kSE is the Squared Exponential kernel, σl is the characteristic length scale, σf is the signal standard
deviation and ω1, . . . , ωP are weights.

So we approximate the continuation value C

with the expression:

(cid:16)

tN −1, SVp

1:(N −1)

(cid:17)

(cid:104)

E

e−r∆tΨGP R (ln (StN ))

SV1:(N −1) = SVp

1:(N −1)

(cid:17) (cid:105)

.

(cid:16)

(cid:12)
(cid:12)
(cid:12)
t1, . . . , Sp

We observe that the law of log (StN ) given Sp

t1, V p

log (StN )

(cid:16)

(cid:12)
(cid:12)
(cid:12)

SV1:(N −1) = SVp

1:(N −1)

∼ N (cid:0)µN,p, σ2

N,p

(cid:1) ,

tN −1 is normal

tN −1, V p
(cid:17)

where

and

µN,p = log

(cid:16)

Sp
tN −1

(cid:17)

(cid:18)

+

r −

1
2

V p
tN −1

(cid:19)

∆t

N,p = V p
σ2

tN −1∆t.

Therefore, the GPR-EI approximation for the continuation value at time tN −1 is as follows:

C GP R−EI
N −1

(cid:16)

SVp

1:(N −1)

(cid:17)

= e−r∆t

(cid:90)

exp

R

(cid:16)

− (z−µN,p)2
2σ2
(cid:113)

N,p

2πσ2

N,p

(cid:17)

ΨGP R (z) dz

= e−r∆tσ2
f

(cid:113)

2πσ2
l

exp

P
(cid:88)

(cid:90)

R

q=1

(cid:16)
− (z−µN,p)2
2σ2
(cid:113)

N,p

2πσ2

N,p

(cid:17)

(cid:18)

exp

−

(log(Sq

tN )−z)2
2σ2
l
(cid:112)2πσ2

l

(cid:19)

ωqdz.

(C.10)

Taking advantage of the properties of the convolution between density functions, we obtain

C GP R−EI
N −1

(cid:16)

SVp

1:(N −1)

(cid:17)

= e−r∆t

P
(cid:88)

q=1

D Proof of Proposition 4

ωqσ2

f σl
N,p + σ2
σ2
l

(cid:113)

(cid:32)

exp

−

(cid:1) − µN,p

(cid:0)log (cid:0)Sq
tN
2σ2
N,p + 2σ2
l

(cid:1)2

(cid:33)

.

(C.11)

In order to proceed backward, from tN −2 up to t1 we consider an integer positive value J and train the GPR
method considering the last J + 1 observed values of the couple (cid:0)log (cid:0)Sp
(cid:1)(cid:1) as predictors, and the
tn
option price as response. Speciﬁcally, the predictor set is

(cid:1) , log (cid:0)V p
tn

(cid:110)

zp = log

(cid:16)

Z =

SVp

max{1,N −1−J}:(N −1)

(cid:17)

, p = 1, . . . , P

(cid:111)

⊂ RdN −1

(D.1)

23

where dN −1 = 2 min {N − 1, J + 1} and the response y ∈ RP is given by

(cid:16)

yp = v

tN −1, SVp

1:(N −1)

(cid:17)

.

We term uGP R

N −1 the obtained function. In particular, uGP R

N −1 : RdN −1 → R and

uGP R
N −1

(cid:16)

(cid:16)

log

SVp

max{1,N −1−J}:(N −1)

(cid:17)(cid:17)

(D.2)

(D.3)

approximates v

(cid:16)

tN −1, SVp

1:(N −1)

(cid:17)

.

Since the predictors have diﬀerent nature (log-prices and log-volatilities at diﬀerent times), we use the Au-
tomatic Relevance Determination (ARD) Squared Exponential Kernel kASE to perform the GPR regression.
In particular, if d is the dimension of the space containing the predictors, it holds

kASE (a, b) = σ2

f exp

−

(cid:32)

(cid:33)

d
(cid:88)

i=1

(ai − bi)2
2σ2
i

, a, b ∈ Rd,

(D.4)

As opposed to the Squared Exponential kernel, the ARD Squared Exponential kernel considers a diﬀerent
length scale σi for each predictor that allows the regression to better learn the impact of each predictor on
the response.

We present now how to perform the backward induction. So, let us consider n ∈ {0, . . . , N − 2} and
n+1 : Rdn+1 → R to be known. In particular, dn+1 = 2 min {n + 1, J + 1}

suppose the GPR approximation uGP R
and for each z = (cid:0)z1, . . . , zdn+1

(cid:1) ∈ Rdn+1 , it holds

n+1 (z) = σ2
uGP R
f

P
(cid:88)

q=1

(cid:32)

ωq exp

−

dn(cid:88)

i=1

(zq

i − zi)2
2σ2
i

(cid:33)

,

(D.5)

(cid:16)

(cid:17)

(cid:16)

(cid:17)

i = log

Sq
n+1−(i−1)/2

where zq
means that zq
log-volatility at time tn+1−(i−1)/2 of the q-th path if i is odd.

V q
if i is odd, for i = 1, . . . , dn+1. This
n+1−i/2
i is the observed log-price at time tn+1−(i−1)/2 of the q-th path if i is even, and it is the observed

if i is even and zq

i = log

We explain now how to compute the GPR approximation vGP R−EI

time tn. First of all, we observe that the vector
(cid:17)

, log
is ˆFtn -measurable. The law of (cid:0)log (cid:0)Stn+1

log

max{1,n+1−J}:n

(cid:16)

(cid:16)

(cid:16)

(cid:17)

Sp
tn+1

n

(cid:16)

: Rdn → R of the price function at
(cid:17)(cid:17)(cid:62)
is not ˆFtn -measurable whereas
(cid:1)(cid:1)(cid:62)
t1 , V p
tn , V p
t1

tn , . . . , Sp

given Sp

V p
tn+1
(cid:1) , log (cid:0)Vtn+1

SVp
log
is normal:

(cid:0)log (cid:0)Stn+1

(cid:1) , log (cid:0)Vtn+1

(cid:1)(cid:1)(cid:62)

|(SV1:n = SVp

1:n) ∼ N (µn+1,p, Σn+1,p) ,

In particular

µn+1,p =

(cid:18)

log (cid:0)Sp
tn

(cid:18)

r −

(cid:1) +

(cid:19)

1
2

V p
tn

∆t, log (ξ0) + ηΛ2n+2Gp −

(cid:19)(cid:62)

,

η2t2H
n+1

1
2

(D.6)

(D.7)

where Λ2n+2 is the 2n+2-th row of the matrix Λ and Gp = (Gp
matrix is given by

1, . . . , Gp

2n, 0 . . . , 0)(cid:62). Moreover, the covariance

Σn+1,p =





η

(cid:113)

∆tV p
tn

(cid:113)

η
η2 (cid:0)Λ2

∆tV p

tn Λ2n+2,2n+1

2n+2,2n+2 + Λ2

2n+2,2n+1

∆tV p

tn Λ2n+2,2n+1



 ,

(cid:1)

(D.8)

24

where Λi,j stands for the element of Λ in position i, j. Using a similar reasoning as done for the continuation
value at time tN −1, one can obtain the following GPR-EI approximation for the continuation value at time
tn−1:

C GP R−EI

n

(cid:16)

SVp

max{1,n−J}:n

(cid:17)

= e−r∆tσ2

f σdn+1−1σdn+1

P
(cid:88)

q=1

ωqhp

qf p
q ,

where hp

q and f p

q are two factors given by



hp
q = exp

−

dn+1−2
(cid:88)

i=1

(zp

i )2

i − zq
2σ2
i





(D.9)

(D.10)

and

f p
q =


− 1
2

exp

(cid:32)(cid:32)

zq
dn+1−1
zq
dn+1

(cid:33)

(cid:33)(cid:62) (cid:32)

− µn+1,p

Σn+1,p +

(cid:118)
(cid:117)
(cid:117)
(cid:116)det

(cid:32)

Σn+1,p +

(cid:32)

(cid:32)

σ2

dn+1−1
0

σ2

dn+1−1
0

0
σ2

dn+1

0
σ2

dn+1

(cid:33)(cid:33)−1 (cid:32)(cid:32)

(cid:33)(cid:33)

(cid:33)

zq
dn+1−1
zq
dn+1

− µn+1,p

(cid:33)


.

In particular, hp
changes due to the diﬀusion of the underlying and its volatility.

q measures the impact of the past observed values on the price, whereas f p

(D.11)
q integrates the

Therefore, we obtain

(cid:16)

vGP R−EI
n

SVp

max{1,n−J}:n

(cid:17)

= max

(cid:32)

Ψ (cid:0)Sp
tn

(cid:1) , e−r∆tσ2

f σdn+1−1σdn+1

(cid:33)

ωqhp

qf p
q

.

P
(cid:88)

q=1

Finally, we observe that, in order to compute uGP R
given by

n

, we train the GPR method considering the predictor set

(cid:110)

Z =

zp = log

(cid:16)

SVp

max{1,n−J}:n

(cid:17)

, p = 1, . . . , P

(cid:111)

⊂ Rdn

and the response y ∈ RP is given by

yp = vGP R−EI
n

(cid:16)

SVp

max{1,n−J}:n

(cid:17)

.

(D.12)

(D.13)

By induction we can compute the option price value for n = N − 2, . . . , 0 .
To conclude, we observe that the continuation value at time t = 0 can be computed by using (D.9) and

considering hp

q = 1 for q = 1, . . . , P since in this case, there are no past values to consider.

25

