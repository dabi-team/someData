BF++: a language for general-purpose program synthesis
Preprint, compiled July 11, 2022

Vadim Liventsev1∗, Aki Härmä2, and Milan Petkovi´c3

1,3Eindhoven University of Technology
1,2,3Philips Research Eindhoven

2
2
0
2

l
u
J

8

]
I

A
.
s
c
[

6
v
1
7
5
9
0
.
1
0
1
2
:
v
i
X
r
a

Abstract
Most state of the art decision systems based on Reinforcement Learning (RL) are data-driven black-box neural
models, where it is often diﬃcult to incorporate expert knowledge into the models or let experts review and
validate the learned decision mechanisms. Knowledge-insertion and model review are important requirements in
many applications involving human health and safety. One way to bridge the gap between data and knowledge
driven systems is program synthesis: replacing a neural network that outputs decisions with a symbolic program
generated by a neural network or by means of genetic programming. We propose a new programming language,
BF++, designed speciﬁcally for automatic programming of agents in a Partially Observable Markov Decision
Process (POMDP) setting and apply neural program synthesis to solve standard OpenAI Gym benchmarks.
Source code is available at https://github.com/vadim0x60/cibi

Keywords Reinforcement Learning · Program Synthesis · Programming Languages

1 Introduction

Reinforcement Learning (RL) has been applied successfully
in ﬁelds like Energy, Finance and Robotics [1]. However, tra-
ditional approaches to Reinforcement Learning involve black
box models that preclude any exchange of knowledge between
experts and ML algorithms. In safety-critical ﬁelds2 like Health-
care [2] the ability to understand the decision algorithms induced
by artiﬁcial intelligence, as well as to initialize the system using
expert knowledge for an acceptable baseline performance, is
required for acceptability.

In this work we focus on an alternative approach for RL based
on program induction, known as Programmatically Interpretable
Reinforcement Learning [3]. We introduce BF++, a new pro-
gramming language tailor-made for this approach (section 4.1).
We then demonstrate that neural program synthesis with BF++
can solve arbitrary reinforcement learning challenges and gives
us an avenue for knowledge sharing between domain experts
and data-driven models via the mechanism of expert inspiration
(section 5.5).

2 Background

In this paper we deﬁne a Reinforcement Learning environment
as Partially Observable Markov Decision Process [4, 5]: when
at step i the agent takes action ai ∈ A it has an impact on the
state of the environment si ∈ S via distribution ps(si+1|si, ai) of
conditional probabilities of possible subsequent states. State
is a latent variable that the agent cannot observe. Instead, the
agent can see an observation oi ∈ O which is a random variable
that depends on the latent state via distribution po(oi|si, ai). A,
S and O are sets of all possible actions, states and observations
respectively. Finally, at every step the agent observes a reward
ri = R(si, ai)

Given this limited toolset, without full (or any) prior knowledge
of how the agent’s actions inﬂuence the the environment (dis-
tributions ps(si+1|si, ai) and po(oi|si, ai)), the agent has to come
up with a strategy that will maximize n-step return Rn = (cid:80)n
t=i rt
where n is the agent’s planning horizon. It is, in the general
sense, a hyperparameter, however if an environment has a limit
on how many steps an episode can last, it is reasonable to set n
equal to the step limit.

Conventional solutions [6] introduce a parametrized policy func-
tion πφ(a|s) that deﬁnes agent’s behavior as a probability distri-
bution over actions and/or function Qφ(a|s) that deﬁnes what Rn
the agent is expecting to receive if they take action a. Parameters
φ are learned empirically, using gradient descent or evolutionary
methods [7, 8].

This approach has been applied extensively and with great
success [9] in Partially Observable Markov Decision Process
(POMDP) settings, however it does have major limitations:

1. The agent is deﬁned as stateless. As such, when making
a decision ai the agent is unable to take into account
any observations it made prior to step i. Long-term
dependencies like "this patient should not receive this
drug since she has shown signs of allergy when this
drug was administered to her 17 iterations ago" cannot
be captured by a memoryless model.

2. The agent is represented as a set of model weights φ,
often with millions of parameters. Such a program can
be used as a black box decision system, but domain
experts are unable to understand and/or make their
contributions to the agent’s programming.

2Safety requirements in healthcare are the main motivation for our
research. However, in this paper we use conventional OpenAI Gym
benchmarks to enable comparison between methods

In this paper, we address these limitations by representing an RL
agent with a program in a specialized language, to be introduced
in section 4.1, as opposed to πφ and Qφ

*correspondence: v.liventsev@tue.nl

 
 
 
 
 
 
Preprint – BF++: a language for general-purpose program synthesis

2

3 Related work

Despite Program Synthesis being one of the most challenging
tasks in Computer Science, many solutions exist, see [10]. They
can be roughly classiﬁed by speciﬁcation modality: how are the
requirements for a program to be synthesized communicated to
the generative model?

The most advanced program synthesis technology to date is deep
neural network-based language modeling [11]. This models are
autocomplete engines that given a fragment of a program, also
known as a prompt, predict the fragment to follow afterwards.
Such models can be very powerful, however, the prompt is
a suboptimal form of speciﬁcation, creating an open problem
of prompt engineering - generating the ﬁrst fragment of the
program in such a way that it encourages the language model to
write a particular prompt [12].

If the requirements are speciﬁed as a natural language descrip-
tion of what a program should do, program synthesis becomes a
machine translation task [13]. A neural model can be pre-trained
as a language model and ﬁne-tuned on a translation dataset like
CoNaLa [14] or, in the case of AlphaCode [15], a dataset of
competitive programming tasks and submitted solutions.

If the requirements are speciﬁed as a set of inputs to the program
along with expected outputs, the task is known as programming
by example [16, 17] using techniques like neural-guided pro-
gram search [18]. One can also generate input-output pairs artiﬁ-
cially [19]. Models like Neural Turing Machines [20], Memory
Networks [21] and Neural Random Access Machines [22] are
also trained with input-output pairs and even though they don’t
explicitly generate code, they ﬁt the deﬁnition of program.

In this work our goal is to synthesize programs with no explicit
speciﬁcation - only an environment where the program can
be tested. This task is typically tackled with neural program
synthesis [3, 23] or genetic programming [24] in a domain-
speciﬁc language, where the reward function of the POMDP
is known as the ﬁtness function. However, to the best of our
knowledge, there is no programming language for POMDP
settings speciﬁcally. Because of this, applications of genetic
programming for Reinforcement Learning challenges have been
limited and [23], for example, only supports non-interactive
programs, i.e. a program is a mapping from an input string to an
output string.

4 BF++

4.1 BF syntax

Abolaﬁa et al [23] picked BF3 [25] as their language for program
synthesis for the following reasons:

• In industry-grade programming languages like Python
or Java program code can contain a very large variety
of characters since any of the 143859 Unicode [26]
characters can be used in string literals. In BF, however,
only 8 characters can be used: they can be one-hot-
encoded with vectors of size 8.

• BF’s simple syntax means that an arbitrary string of
valid characters is likely to be a valid program. In

3Brainfuck

more complex languages, most possible strings result
in a syntax error. A generative model being trained to
write programs in such a language risks being stuck
in a long exploration phase when all the programs it
generates are invalid and it has no positive examples in
the dataset.

• Despite all of the above, it is a Turing-complete lan-

guage.

The simplicity of the language also means that it is relatively
easy to develop a compiler that translates programs from an
industry-standard programming languages like Java and Python
to BF thus making use of the expert knowledge existing in those
languages.

In the current paper, we introduce an extended version of the
original BF language, BF++. As explained below, the extensions
to the original BF syntax are particularly useful in the RL use
cases.

BF’s runtime model is inspired by the classic Turing Machine
[27]: at any point during the program’s execution, the state of
the program consists of:

• An inﬁnite4 tape of cells T where each cell holds an

integer number.

• A memory pointer pT that points to a certain cell in the

tape (active cell T pT ).

• A string of characters C that represents program code.
• A code pointer pC pointing to a character about to be

executed.

The code pointer starts at the ﬁrst character, then this character
gets executed and the pointer is incremented (moved to the next
character). There are 8 possible characters:

> Move the memory pointer one cell right. pT := pT + 1
< Move the memory pointer one cell left. pT := pT − 1
+ Increment the active cell. T pT := T pT + 1
- Decrement the active cell. T pT := T pT − 1

. Write T pT from the active cell to the output stream5
, Read x from the input stream to the active cell. T pT := x
[ If the active cell T pT = 0, jump (move pC) to the matching

].

] If the active cell T pT (cid:44) 0, jump (move pC) to the matching [

[ and ] commands constitute a loop that will be executed re-
peatedly until the active cell becomes zero. They are also the
only way to write a BF program with a syntax error: a valid BF
program is one that doesn’t contain non-matching [ or ]

4If you happen to be executing a BF program on a computer with
ﬁnite memory, the tape will be ﬁnite due to your hardware limitations.
5The deﬁnition of input and output streams is purposefully under-

speciﬁed, it may depend on the particular implementation.

Preprint – BF++: a language for general-purpose program synthesis

3

4.2 Negative values

4.4 Virtual comma

In BF memory cells T i hold non-negative values only. In BF++
T i ∈ Z, a negation operator ˜ is introduced and operators []are
redeﬁned to loop while the active cell is non-positive, i.e.

˜ If the active cell T pT := −T pT .

[ If the active cell T pT ≥ 0, jump (move pC) to the matching

].

] If the active cell T pT < 0, jump (move pC) to the matching [

This decision was taken because negative observations are com-
mon in control problems (see section 5) as is branching on
whether the observed value is positive or negative.

4.3 Non-blocking action operators

The main issue of BF as a language for Reinforcement Learning
is its input-output system.
It assumes that the program can
freely decide on the relative frequency of inputs to outputs. For
example, the following program

+[.....,]

inputs 5 integers, outputs the 5th character it read, then goes
back to the beginning and proceeds indeﬁnitely outputting every
5th character it inputs. Thus it assumes a 5:1 frequency of inputs
to outputs. If we simply assume that inputs are observations and
outputs are actions, such program will not be able to operate in
a POMDP environment where I/O frequency is ﬁxed at 1:1 and
the agent that has made an observation has to act before it can
make the next observation. In other words, operators . and , are
blocking: . stops program execution and waits until new input
is received to resume execution, , stops program execution and
waits until there is an opportunity to act in the environment.
To address this, in BF++ . operator is non-blocking. It outputs
the current value of the active cell by placing it at the bottom of
the action queue S - a sequence of integer numbers that represent
actions the program is planning to take in the environment. We
also introduce a non-blocking operator ! that places T pT on top
of the action queue.

S := S (cid:95)(T pT )
.
! S := (T pT )(cid:95)S

(1)

where (cid:95) denotes concatenation of tuples

The program can thus decide by using . or ! whether the
newly added action takes precedence over ones already in the
queue. As soon as an opportunity to act arises, the top of the
action queue (item S 1 or several items S 1, S 2, . . . , see section
5.2) deﬁnes which action the program takes and is then removed
from the queue. If S k does not exist (the queue is empty or
shorter than k) default value of S k = 0 is assumed.

, operator, on the other hand, is blocking. Thus its function is
more important than just reading an observation into memory.
Executing , is when the program moves to the next step of
POMDP.

The system where the only way to proceed to the following
iteration is the , operator, naively implemented, means that to
be successful in any POMDP environment, a program has to
contain an inﬁnite loop with a , operator. Any program that
has a ﬁnite number of , steps will terminate prematurely in an
environment that supports arbitrarily long number of iterations.
Since we originally set out to develop a language where most
random programs would be valid, this had to be addressed.
We decided to turn any BF++ program into an inﬁnite loop with
a , operator by default:

1. Every BF++ program starts with a virtual , operator
at address pC = −1: it is executed before all operators
in the code of the program, they are indexed starting
from pC = 0

2. When the code pointer pC reaches the end of the pro-
gram it loops back to the virtual comma pC := −1

Due to the virtual comma, every program starts executing with
the initial observation already stored in memory and available
for branching/decision-making.

4.5 Observation discretization

Another issue complicating applications of BF to Reinforce-
ment Learning is that since its memory tape holds only integer
numbers its inputs and outputs have to be integer as well. And
this issue cannot be ﬁxed simply by replacing an integer tape
with a tape of ﬂoating point numbers as BF’s only operations for
manipulating numbers are + and - - increment and decrement.
Non-integer action and observation spaces are fairly common in
reinforcement learning tasks hence BF++ implements coercion
mechanisms for reading and writing continuous vectors into
discrete memory.

We assume that the vector observation space O is a hypercube
deﬁned as an intersection of n separate scalar observation spaces
Ok such that

o1 ∈ Ok

1, o2 ∈ Ok

2, . . . , on ∈ Ok

n ⇔ (o1, o2, . . . , on) ∈ O

(2)

This assumption theoretically excludes some possible obser-
vation spaces, but almost all POMDP tasks discussed in the
research literature and all OpenAI Gym tasks conform to this
assumption.

To write an observation onto the memory tape we the ob-
servation vector of size n is aligned with memory cells
T pT , T pT +1, . . . , T pT +n−1 and turned into an integer with the use
of d discretization bins.

T pT +k−1 :=

min
ω∈1,...d|ok<τk
ω

ω

(3)

If Ok is an interval Ok = [olow, ohigh], it is split into discretization
bins evenly, as in eq. 4:

τω =

(cid:40)

olow + ohigh−olow
d
+∞, ω = d

ω, ω = 1, 2, . . . , d − 1

(4)

Preprint – BF++: a language for general-purpose program synthesis

4

be pushed onto the action stack. However, the action that’s
output to the environment has to belong to a N-dimensional
action space A, an intersection of unidimensional action spaces
Ak. The "act" operation thus includes a coercion system and is
deﬁned as:

Figure 1: Fluid discretization example in Mountain Car

Some environments, however, have unbounded observation
spaces Ok = (−∞; +∞), Ok = (−∞; ohigh], Ok = [olow; +∞).
This spaces are challenging because the formal description
Ok does not in any way reﬂect the actual underlying distribu-
tions of observations.
It can be the case, for example, that
Ok = (−∞; +∞) but most observations found in the environ-
ment fall in the interval Ok = [42; 43]. For such observation
spaces, BF++ uses a ﬂuid discretization system that learns the
true distribution of observations online. The idea was inspired
by a work of Touati et al [28], although, they assumed that Ok
has a ﬁnite diameter and didn’t support unbounded observation
spaces. Initial thresholds τω can be arbitrary. With each new
observation, thresholds τω are readjusted so that among h prior
observations, roughly ω out of d observations are lower values
that τω:

minimize
τ

(cid:88)

ω∈0,1,...d

(cid:80)

|

ω
d

−

i(cid:48)∈i−h,i−h+1,...,i−1

I(ok

i(cid:48) < τω)

h

|

(5)

To solve this optimization problem, one has to sort previous d
observations in ascending order so that



ak :=




S k
d−1 , Ak = (−∞; +∞)
amin + | S k
amax − |amax − S k
amin + (S k mod d)
S k, Ak ⊂ Z
S := (S N+1, S N+2, . . . )

d−1

d−1 − amin|, Ak = [amin; +∞)
d−1 |, Ak = (−∞; amax]
∗ (amax − amin), Ak = [amin; amax]

(8)

4.7 Goto

It is notoriously hard to introduce any kind of branching behavior
in BF [29]. To facilitate if-then style programs we introduce a
goto operator ^ deﬁned as

pT := T pT

(9)

Note that it is not a goto; in the traditional C sense, since the
memory pointer is being moved, not the code pointer. Still, it lets
the agent preemptively store potential actions in memory cells
and than branch between this actions based on the observation.

4.8 Random number generator

Operator @ writes a random number into the active cell. A
random agent is often used as a starting point for exploration
and in BF++ a random agent can be implemented as @!

sort : {oi|i ∈ i − h, i − h + 1, . . . , i − 1} −→ {si|i ∈ 1, 2, . . . , h} (6)

is such a bijection that s1 < s2 < · · · < sh holds and set

4.9

Shorthands

τω = s(cid:100) ω

d h(cid:101)

See ﬁgure 1 for a visual example.

(7)

With all the commands we introduced in sections 4.1 - 4.7 it is
still surprisingly hard to encode relatively simple decisions like
"add action 5 to the top of the action queue":

This system has 2 hyperparameters: d and h. With a low d a lot
of the information observed form the environment is lost, while
when d is in the hunderds the generated programs can become
very complex. h switches between relative and absolute observa-
tions. With a very high h, ω = 0 means that this observation is
one of the lowest that can be observed in this environment, with
h = 1 it means that the observation is lower than the previous
one.

High values of h present an additional challenge: how to cor-
rectly discretize observation in the ﬁrst h iterations? We imple-
mented burn-in: before training or evaluation we run h iterations
of a random agent (see section 4.8) to collect a history of h
observations and pick correct thresholds.

4.6 Action coercion

A symmetrical problem arises with actions taken by the agent.
Memory tape holds integer numbers T k ∈ Z and any value can

[>]+++++!

This program moves the memory pointer right until it hits a
cell that contains zero, increments it ﬁve times, and then pushes
T pT to the top of the action queue. It also loses the current
value of the memory pointer which might be meaningful. Our
experiments have shown that it takes a very long time for the
neural model to learn to write this kind of combinations.

To mitigate this issue we introduce shorthands: commands
01234 mean "write the respective number (0,1,2,3 or 4)" into
the cell and commands abcde mean "move the memory pointer
to cell a,b,c,d or e" where cells a,b,c,d and e are the ﬁrst 5
cells in the memory tape. We intentionally made the number
of shorthands equal to discretization constant d = 5. Due to
our method of discretization of continious action spaces (see
sections 4.5, 4.6) the program will often encounter situations
when it can choose between d diﬀerent actions and thanks to
shorthands taking them can be encoded as 1!, 2!, . . .

Preprint – BF++: a language for general-purpose program synthesis

5

Summary

4.10
In total (assuming 5 shorthands) BF++ has 22 commands:

><^@+~-[].,!01234abcde

Commands @^~01234abcde are considered optional and can be
disabled if the task at hand calls for it. The number of shorthand
commands can be increased or decreased.

Observation discretization and action coercion techniques built
into the language mean that BF++ is compatible with any
POMDP environment. However, in practice, there is one im-
portant limitation: the complexity of the program required to
operate in an environment is directly proportional to dimen-
sionality of it’s action and observation spaces A and O. If, for
example the observation space is 10000-dimensional, once an
observation is read onto tape T it takes 9999 > operators to reach
second to last observation. Thus, in practice, BF++ should be
used with low-dimensional POMDPs.

An extension of our methodology to high-dimensional POMDPs
(such as Atari games [30], where the observation is a matrix of
pixels on simulated game screen) can be achieved by adding a
scene encoder neural network that maps the observed image to
a low-dimensional vector as proposed in [31].

5 Experimental setup

5.1 Hypotheses and goals

Our experiments were designed to test the following hypotheses:

H1 BF++ can be used in conjunction with a program synthe-
sis algorithm to solve arbitrary reinforcement learning
challenges (POMDPs)

H2 BF++ can be used to take a program written by an expert
and use program synthesis to automatically improve it
H3 BF++ can be used to generate an interpretable solutions to
Reinforcement Learning Challenges that experts can
learn from

H4 Optional commands @^~01234abcde introduced for conve-
nience make it easier for experts to write programs in
BF++

H5 Optional commands @^~01234abcde improve the quality

of programs synthesised by neural models

Hence we

1. Pick several commonly studied reinforcement learning

environments

2. Employ an expert6 to write BF++ programs to solve

them

3. Develop a program synthesis model following from

[23]

4. Compare the best programs generated by the model
with expert programs in terms of program quality

5. Perform ablation studies: remove some of the optional
commands from the language (resulting language is
called BF+), remove the expert program from the
model’s program pool, compare program quality

6. Perform case studies: analyze programs generated
by the model to gain insight into how the model ap-
proached the problem

5.2 Environments

We evaluate our framework on 4 low-dimensional (see section
4.10) POMDPs sampled from OpenAI Gym [32] leaderboard7:

1. CartPole-v1 [33]. A pole is attached to a cart. which
moves along a frictionless track. The agent observes
cart position, cart velocity, pole angle and pole velocity
at tip. The goal is to keep the pole upright by applying
force between -1 and 1 to the cart. At every step the
agent receives a +1 reward for survival. The episode
terminates when the pole inclines too far.

2. MountainCarContinuous-v0 [34]. A car is on a one-
dimensional track, positioned between two "moun-
tains". The goal is to drive up the mountain consum-
ing a minimal amount of fuel by controlling the en-
gine, setting it’s torque in the range [−1; 1]; however,
the engine is not strong enough to scale the moun-
tain in a single pass. Therefore, the only way to suc-
ceed is to drive back and forth to build up momentum.
We picked MountainCarContinuous-v0 as opposed to
MountainCar-v0 to demonstrate the performance of
our discretization system.

3. Taxi-v3 [35]. There are 4 locations (labeled by dif-
ferent letters) and the goal is to pick up the passenger
at one location and drop him oﬀ in another in as few
timesteps as possible spending as little fuel as possible.

4. BipedalWalker-v2. A simulated 2D robot with legs
has to learn how to walk. Moving rightwards is re-
warded, falling is penalized. Observation vector con-
sists of speeds, angular speeds and joint positions col-
lected by the robot’s sensors. These observations do
not, however, include any global coordinates - they can
only be inferred from sensor inputs. With action vector
of size 4 the agent controls speeds of the robots hip and
knee motors.

5.3 Hyperparameters
For observation discretization (section 4.5) we picked d = 5 (so
that it’s equal to the number of shorthands) and h = 500 for our
experiments, hence when the observation is among the highest
20% of the last 500 observations it is written into memory as 4
while if it falls between 40-th and 60-th percentiles it is 2.

5.4 Expert programs

For CartPole we wrote 2 programs. One completely ignores
all observations and just alternates between "move right" and
"move left":

6ﬁrst author of this paper

7https://github.com/openai/gym/wiki/Leaderboard

Preprint – BF++: a language for general-purpose program synthesis

6

(a) CartPole-v1

(b) MountainCarContinuous-v0

(c) Taxi-v3

(d) BipedalWalker-v2

Figure 2: Selected environments, visualized

0!,1!

Another calculates the diﬀerence between velocity of the cart
and angular velocity of the pole. If it’s positive, the cart is
pushed to the right (the cart has to catch up with the pole), if
it’s negative the cart is pushed to the left, if zero it is pushed
randomly:

[a0>0>0>0>0>@>1>1>1>1>1>,>[->>-<<]>>+++++^!1]

The ﬁrst part of this program sets up an action map on the tape
where every possible value of the velocity diﬀerential has a
respective cell with 0, 1 or (in the center) random number. Then
[->>-<<] block does subtraction, +++++ adds 5 to the result,
so that it belongs to in 0..10 and not −5..5, ^ moves the memory
pointer to the correct cell in the action map and ! puts the action
onto the action stack.

For Mountain Car we wrote an elegant algorithm that reads the
observation vector into the tape, goes to the second observation
(car velocity) and outputs it as action:

>!a

In other words, we apply motor torque in the same direction
where we’re currently headed, thus always accelerating our car.
If we’re headed right, that helps us get to the destination and if
we’re headed left that helps us get as high as possible onto the
hill so that when direction reverses, the car has more energy to
push through the right hill.

Developing programs for Bipedal Walker is, unfortunately,
above our expert’s paygrade.

5.5 Program synthesis model
In order to train a generative model g to write BF++ programs
we treat the writing process as a reinforcement learning episode
in its own right [23] . Every character of a program is an action
taken by the writer agent, the programs are terminated by a
NULL character. When the NULL character is written, a BF++
agent is created in the target POMDP environment (e.g. Cart-
Pole) and sum total of rewards Q collected in that episode is
assigned as a reward to the writer agent for the NULL character.
All other characters are rewarded with zero.

The writer agent’s policy is modeled with an LSTM [36] neural
network and is trained with a modiﬁed version of REINFORCE
[37] algorithm. While standard REINFORCE optimizes Policy
Gradient:

OPG(φ) = Eπ(C;φ)(Q)

(10)

where φ are LSTM parameters, C - program, Q - reward obtained
by the program in target environment,

we optimize

O(φ) = OPG(φ) + OPQT (φ)

(11)

For Taxi we introduce 2 programs. The ﬁrst program:

where

1. Finds the coordinates of the current destination (pas-
senger to pick up or current passenger’s destination)

2. Subtracts the current destination
3. Moves in the resulting direction

The problem with this approach is that it always gets stuck when
it hits a wall. To compensate for that, the second program alter-
nates between the strategy above (for 5 iterations) and random
movements (for 5 iterations) so that it eventually gets unstuck.
See source code repository for the programs.

Optional commands @^~01234abcde have all been invaluable
in developing these programs - a fact in support of H4. A more
rigorous way to conﬁrm it would be employing several human
experts to develop programs with and without optional operators,
but ﬁnding volunteer BF++ developers has proven diﬃcult.

OPQT = 1
K

K(cid:88)

k=0

log π(Ck; φ)

(12)

where C1 is the best (highest Q) known program, C2 - second
best, . . .

Intuitively, both OPG(φ) and OPQT(φ) when optimized update
the weights of the LSTM so that programs that we have found
to be successful are more likely. But Policy Gradient weighs
programs proportionately to their respective rewards while PQT
creates a priority queue of the best known programs and assigns
a high importance to them and zero to the rest.
OPQT component has been shown to have "a stabilizing aﬀect
and helps reduce catastrophic forgetting in the policy" [23]. In
addition to this, we use OPQT to implement expert inspiration.

Preprint – BF++: a language for general-purpose program synthesis

7

Figure 3: Neural-symbolic learning cycle [38]

By default, the priority queue of the best known programs is
initialized as an empty set. But if expert-written programs are
available, it can be prepopulated with these programs that act
as useful positive examples for teaching the writer agent. This
approach is used to incorporate programs from section 5.4 and
transfer knowledge from experts to the neural developer.

This approach to expert inspiration follows what’s known as
neural-symbolic learning cycle, displayed in ﬁgure 3 - expert
knowledge is represented symbolically, in terms of a BF++ pro-
gram, then a neural network is trained to generate this program,
eﬀectively translating the expert knowledge from symbolic into
connectionist format (representation), the neural network learns
from reinforcement how to solve the task better than the expert
(training). Unlike in most neural-symbolic systems [39] that
extract knowledge from connectionist systems with algortihms
like TREPAN [40] or JRip extraction [41], the extraction step
is trivial since the neural network outputs a symbolic program
directly.

In all experiments below, the writer agent’s LSTM has hidden
size of 50, batch size of 4 and is trained with RMSProp [42]
optimizer.

5.6 Stopping and Scoring

All experiments were run with an upper limit of 100000 training
episodes. Environments other than Taxi also used Exponential
Variance Elimination [43] early stopping technique - training
was stopped when the postive trend in the quality of the best
found program stopped, i.e. when the exponential moving av-
erage of program quality is lower that it was 1000 episodes
ago. Agents for Taxi are trained for a ﬁxed number of episodes,
because we noticed that in this environment the longest part of
the training process is learning to pick up your ﬁrst passenger
and until that happens Q = −200 holds.

Once the training process is ﬁnished, we take the best known
programs and since each of them was only tested once (leading
to high variance) we test them again, averaging total rewards
over 100 episodes. We use this averaged reward to pick the best
program.

Figure 4: Visual summary of the strategy enacted by -.. on
Mountain Car

6 Results

6.1 Quantitative results

Table 1 presents the quality metric (average 100-episode reward)
of the best program in every category, compared to that of a fully
random agent and the result required to join the OpenAI gym
leaderboard for context. Note that the expert programs used a lot
of optional operators (shorthands and @^!), so it wasn’t possible
to implement expert inspiration with limited command sets.

These results support (see section 5.1) hypothesis H1 - we have
obtained functional programs for all environments, H2 - when
expert inspiration was used the resulting programs were bet-
ter than expert programs and better than programs generated
without expert inspiration and H4 - ablation studies for optional
operators do indeed show that those operators are useful.

6.2 Case studies

We have established that the program synthesis model is able
to learn from human experts. But can experts learn from the
model? (H3) To conﬁrm this, we oﬀer a detailed explanation of
the most successful program of all experiments listed in section
5.

This program scored 91.39 on Mountain Car:

-..~+

The trailing ~ and + do not aﬀect the behavior of the agent: they
modify the value of the active cell only for it to be immediately
rewritten by the virtual comma (section 4.4) before it has any
chance to inﬂuence actions. One can think about these com-
mands as inactive genes in the DNA - we have found many
resulting programs to contain such commands. If necessary this
eﬀect can be accounted for by incorporating program length into
the loss function. So this program is equivalent to:

-..

5.7

Implementation

BF++ interpreter and the training system were written in Python
with TensorFlow for neural models. GPU resources weren’t
used, because the performance bottleneck of the system is not
backpropagation but rather testing a BF++ program in the envi-
ronment, single experiment runtime was between 1 hour (Cart-
Pole) and 10 (Taxi).

When the virtual comma is executed, car position and car veloc-
ity are read into memory, discretized into integers 0 . . . 4. The
position is read into the active memory cell pT , while the veloc-
ity is in cell pT + 1. Then the active cell is decremented and the
resulting number is put onto the action stack twice. There is 1
read operation and 2 write operations to the end of the action
stack, which introduces a delay before the actions get executed.

Preprint – BF++: a language for general-purpose program synthesis

8

Table 1: Total episode reward Q achieved by best programs found, averaged over 100 episodes

Environment

CartPole-v1 MountainCarContinuous-v0 Taxi-v3 BipedalWalker-v2

Random agent
BF++ expert program 1
BF++ expert program 2
BF+ (without shorthands) LSTM
BF+ (without @^~) LSTM
BF++ LSTM
BF++ LSTM with expert inspiration

Leaderboard threshold

9.3

20.48
18.23
44.55
48.14
71.38
96.64

195

0

-6.55
-
91.57
81.16
88.41
91.39

90

-200

-179.49
-150.44
-57.93
-42.21
-199.82
-60.65

0

-91.92

-
-
-91.9
-31.79
-26.97
-

300

When it’s time to act, the number on the action stack is coerced
to one of the actions possible in this environment (0 for going
left, 1 for doing nothing, 2 for right).

A strategy emerges, illustrated on ﬁgure 4, in which the car puts
"going right" onto the agenda if it’s on the far left or the center
right of the landscape, puts "going left" onto the agenda when
it’s on the far right or center left and schedules doing nothing if
it’s in the center. This strategy helps the car successfully reach
the right fringe every time it is applied.

7 Conclusions

In this paper, we have introduced a new programming language
tailored to the task of programmatically interpretable reinforce-
ment learning. We have shown experimentally that this language
can facilitate program synthesis as well as knowledge transfer
between expert-based systems and data-driven systems.

The results in the OpenAI gym test examples show that the
proposed system is able to ﬁnd a functional solution to the
problem. In some cases the performance is similar to the best
deep learning solution but the obtained program remains still
explainable. This is a very encouraging result and suggest that
the use of program induction methods may indeed be a viable
way towards explainable solutions in RL applications.

We propose the following directions for future work:

1. Develop translation mechanisms between BF++ and
other languages. Potentially, BF++ can be used as
bytecode [44] for reinforcement learning. The expert
would write a program in a higher-level language and
transpile it into BF++ so that the program then can be
improved with reinforcement learning.

2. Use other neural network architectures as well as non-
neural evolution methods like genetic programming
[45] in conjunction with BF++

3. Apply the framework to problems in Healthcare where
expert inspiration is important for crossing the AI
chasm [46].

4. Use Natural Language Generation techniques to trans-
late the BF++ code automatically to a friendly human-
readable text description as in [47, 48].

Acknowledgements

This work was funded by the European Union’s Horizon 2020 re-
search and innovation programme under grant agreement n° 812882.
This work is part of "Personal Health Interfaces Leveraging HUman-
MAchine Natural interactionS" (PhilHumans) project: https://www.
philhumans.eu

References

[1] Yuxi Li. Reinforcement learning applications. CoRR,
abs/1908.06973, 2019. URL http://arxiv.org/abs/1908.
06973.

[2] Chao Yu, Jiming Liu, and Shamim Nemati. Reinforce-
ment learning in healthcare: a survey. arXiv preprint
arXiv:1908.08796, 2019.

[3] Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh,
Pushmeet Kohli, and Swarat Chaudhuri. Programmatically
interpretable reinforcement learning. In Jennifer Dy and
Andreas Krause, editors, Proceedings of the 35th Interna-
tional Conference on Machine Learning, volume 80 of Pro-
ceedings of Machine Learning Research, pages 5045–5054,
Stockholmsmässan, Stockholm Sweden, 10–15 Jul 2018.
PMLR. URL http://proceedings.mlr.press/v80/verma18a.
html.

[4] K J Åström. Optimal control of Markov processes with
incomplete state information. Journal of Mathematical
Analysis and Applications, 10(1):174–205, 1965. ISSN
0022-247X. doi: https://doi.org/10.1016/0022-247X(65)
90154-X. URL http://www.sciencedirect.com/science/
article/pii/0022247X6590154X.

[5] Jr Kramer, J David R. Partially Observable Markov Pro-

cesses., 1964.

[6] Richard S Sutton and Andrew G Barto. Reinforcement
Learning: An Introduction, Second edition in progress,
volume 3. 2017. doi: 10.1016/S1364-6613(99)01331-5.
[7] Seyed Sajad Mousavi, Michael Schukat, and Enda Howley.
Deep Reinforcement Learning: An Overview. In Lecture
Notes in Networks and Systems, volume 16, pages 426–
440. 2018. doi: 10.1007/978-3-319-56991-8_32. URL
https://arxiv.org/abs/.

[8] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and
A. A. Bharath. Deep reinforcement learning: A brief
survey. IEEE Signal Processing Magazine, 34(6):26–38,
2017. doi: 10.1109/MSP.2017.2743240.

Preprint – BF++: a language for general-purpose program synthesis

9

[9] Leslie Pack Kaelbling, Michael L Littman, and Andrew W
Moore. Reinforcement learning: A survey. Journal of
artiﬁcial intelligence research, 4:237–285, 1996.

[10] Sumit Gulwani, Oleksandr Polozov, and Rishabh Singh.
Program synthesis. Foundations and Trends in Program-
ming Languages, 4(1-2):1–119, 2017. ISSN 23251131.
doi: 10.1561/2500000010. URL www.nowpublishers.
com;.

[11] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri-
son Edwards, Yuri Burda, Nicholas Joseph, Greg Brock-
man, Alex Ray, Raul Puri, Gretchen Krueger, Michael
Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,
Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
Alethea Power, Lukasz Kaiser, Mohammad Bavarian,
Clemens Winter, Philippe Tillet, Felipe Petroski Such,
Dave Cummings, Matthias Plappert, Fotios Chantzis, Eliz-
abeth Barnes, Ariel Herbert-Voss, William Hebgen Guss,
Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor
Babuschkin, Suchir Balaji, Shantanu Jain, William Saun-
ders, Christopher Hesse, Andrew N. Carr, Jan Leike,
Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Rad-
ford, Matthew Knight, Miles Brundage, Mira Murati, Katie
Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam
McCandlish, Ilya Sutskever, and Wojciech Zaremba. Eval-
uating large language models trained on code. CoRR,
abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.
03374.

[12] The unreasonable eﬀectiveness of language models for
source code · vadim liventsev. https://vadim.me/posts/
unreasonable/. (Accessed on 07/08/2022).

[13] Xiaojun Xu, Chang Liu, and Dawn Song. Sqlnet: Generat-
ing structured queries from natural language without rein-
forcement learning. CoRR, abs/1711.04436, 2017. URL
http://arxiv.org/abs/1711.04436.

[14] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan
Vasilescu, and Graham Neubig. Learning to mine aligned
code and natural language pairs from stack overﬂow. In
International Conference on Mining Software Reposito-
ries, MSR, pages 476–486. ACM, 2018.
doi: https:
//doi.org/10.1145/3196398.3196408.

[15] Yujia Li, David Choi, Junyoung Chung, Nate Kushman,
Julian Schrittwieser, Rémi Leblond, Tom Eccles, James
Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hu-
bert, Peter Choy, Cyprien de Masson d’Autume, Igor
Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl,
Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J.
Mankowitz, Esme Sutherland Robson, Pushmeet Kohli,
Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals.
Competition-level code generation with alphacode, 2022.
URL https://arxiv.org/abs/2203.07814.

[16] Neel Kant. Recent Advances in Neural Program Synthesis.

2018. URL http://arxiv.org/abs/1802.02353.

[17] Oleksandr Polozov and Sumit Gulwani. Flashmeta: A
framework for inductive program synthesis. In Proceed-
ings of the 2015 ACM SIGPLAN International Conference
on Object-Oriented Programming, Systems, Languages,
and Applications, OOPSLA 2015, page 107–126, New

York, NY, USA, 2015. Association for Computing Ma-
chinery. ISBN 9781450336895. doi: 10.1145/2814270.
2814310. URL https://doi.org/10.1145/2814270.2814310.
[18] Ashwin K Vijayakumar, Dhruv Batra, Abhishek Mohta,
Prateek Jain, Oleksandr Polozov, and Sumit Gulwani.
Neural-guided deductive search for real-time program syn-
thesis from examples. In 6th International Conference on
Learning Representations, ICLR 2018 - Conference Track
Proceedings, 2018. URL https://microsoft.github.io/prose/
impact/.

[19] Richard Shin, Neel Kant, Kavi Gupta, Christopher Ben-
der, Brandon Trabucco, Rishabh Singh, and Dawn Song.
Synthetic datasets for neural program synthesis. Technical
report, 2019.

[20] Wojciech Zaremba and Ilya Sutskever. Reinforcement
learning neural turing machines. CoRR, abs/1505.00521,
2015. URL http://arxiv.org/abs/1505.00521.

[21] Jason Weston, Sumit Chopra, and Antoine Bordes. Mem-
ory networks. In 3rd International Conference on Learning
Representations, ICLR 2015 - Conference Track Proceed-
ings, oct 2015. URL http://arxiv.org/abs/1410.3916.
[22] Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever.
Neural random-access machines. In 4th International Con-
ference on Learning Representations, ICLR 2016 - Confer-
ence Track Proceedings, 2016.

[23] Daniel A. Abolaﬁa, Mohammad Norouzi, Jonathan Shen,
Rui Zhao, and Quoc V. Le. Neural Program Synthesis with
Priority Queue Training. 2018. URL http://arxiv.org/abs/
1801.03526.

[24] Milad Taleby Ahvanooey, Qianmu Li, Ming Wu, and Shuo
Wang. A survey of genetic programming and its appli-
cations. KSII Transactions on Internet and Information
Systems (TIIS), 13(4):1765–1794, 2019.

[25] U. Muller. Brainfuck – an eight-instruction turing-
complete programming language. Available at the inter-
net address http://en.wikipedia.org/wiki/Brainfuck, 1993.
URL http://en.wikipedia.org/wiki/Brainfuck.

[26] Julie D Allen, Deborah Anderson, Joe Becker, Richard
Cook, Mark Davis, Peter Edberg, Michael Everson, As-
mus Freytag, Laurentiu Iancu, Richard Ishida, et al. The
unicode standard. Mountain view, CA, 2012.

[27] A M Turing. On computable numbers, with an application
to the entscheidungsproblem. a correction. Proceedings
of the London Mathematical Society, s2-43(1):544–546,
1938. ISSN 1460244X. doi: 10.1112/plms/s2-43.6.544.

[28] Ahmed Touati, Adrien Ali Taiga, and Marc G Bellemare.
Zooming for eﬃcient model-free reinforcement learning
in metric spaces. arXiv preprint arXiv:2003.04069, 2020.
control ﬂow in brainfuck | matslina,
URL http://calmerthanyouare.org/2016/01/14/

[29] Mats Linander.

2016.
control-ﬂow-in-brainfuck.html.

[30] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling.
The arcade learning environment: An evaluation platform
for general agents. Journal of Artiﬁcial Intelligence Re-
search, 47:253–279, jun 2013.

[31] Daiki Kimura. Daqn: Deep auto-encoder and q-network.

arXiv preprint arXiv:1806.00630, 2018.

Preprint – BF++: a language for general-purpose program synthesis

10

[32] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas
Schneider, John Schulman, Jie Tang, and Wojciech
Zaremba. Openai gym. CoRR, abs/1606.01540, 2016.
URL http://arxiv.org/abs/1606.01540.

[47] Kyle Richardson, Sina Zarrieß, and Jonas Kuhn. The
code2text challenge: Text generation in source code li-
braries. CoRR, abs/1708.00098, 2017. URL http://arxiv.
org/abs/1708.00098.

[48] A. LeClair, S. Jiang, and C. McMillan. A neural model for
generating natural language summaries of program subrou-
tines. In 2019 IEEE/ACM 41st International Conference
on Software Engineering (ICSE), pages 795–806, 2019.
doi: 10.1109/ICSE.2019.00087.

[33] A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike
adaptive elements that can solve diﬃcult learning control
problems. IEEE Transactions on Systems, Man, and Cyber-
netics, SMC-13(5):834–846, Sep. 1983. ISSN 2168-2909.
doi: 10.1109/TSMC.1983.6313077.

[34] Andrew William Moore. Eﬃcient memory-based learning

for robot control. Technical report, 1990.

[35] Thomas G. Dietterich. Hierarchical reinforcement learning
with the maxq value function decomposition. Journal of
Artiﬁcial Intelligence Research, 13:227–303, 2000.

[36] Felix A Gers, Jürgen Schmidhuber, and Fred Cummins.
Learning to forget: Continual prediction with lstm. 1999.

[37] Ronald J Williams. Simple statistical gradient-following
algorithms for connectionist reinforcement learning. Ma-
chine learning, 8(3-4):229–256, 1992.

[38] Sebastian Bader and Pascal Hitzler. Dimensions of neural-
symbolic integration-a structured survey. arXiv preprint
cs/0511042, 2005.

[39] Tarek R Besold, Artur d’Avila Garcez, Sebastian Bader,
Howard Bowman, Pedro Domingos, Pascal Hitzler, Kai-
Uwe Kühnberger, Luis C Lamb, Daniel Lowd, Priscila
Machado Vieira Lima, et al. Neural-symbolic learning
and reasoning: A survey and interpretation. arXiv preprint
arXiv:1711.03902, 2017.

[40] Manoel Vitor Macedo França, Artur S d’Avila Garcez, and
Gerson Zaverucha. Relational knowledge extraction from
neural networks. In CoCo@ NIPS, 2015.

[41] Martin Svatoš, Gustav Šourek, and Filip Železný. Revis-
iting neural-symbolic learning cycle. In 14TH INTERNA-
TIONAL WORKSHOP ON NEURAL-SYMBOLIC LEARN-
ING AND REASONING, 2019. URL https://sites.google.
com/view/nesy2019/home.

[42] T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Di-
vide the gradient by a running average of its recent magni-
tude. COURSERA: Neural Networks for Machine Learn-
ing, 2012.

[43] Vadim Liventsev. vadim0x60/evestop: Early stopping
with exponential variance elmination. https://github.com/
vadim0x60/evestop, 2021. (Accessed on 01/20/2021).

[44] Wikipedia contributors. Bytecode — Wikipedia, the free
encyclopedia, 2020. URL https://en.wikipedia.org/w/index.
php?title=Bytecode&oldid=995026385. [Online; accessed
21-January-2021].

[45] Riccardo Poli, William B Langdon, Nicholas F McPhee,
and John R Koza. A ﬁeld guide to genetic programming.
Lulu. com, 2008.

[46] P. A. Keane and E. J. Topol. With an eye to AI and au-
tonomous diagnosis. NPJ Digit Med, 1:40, 2018. [PubMed
Central:PMC6550235] [DOI:10.1038/s41746-018-0048-
y] [PubMed:29618526].

