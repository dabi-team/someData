1
2
0
2
c
e
D
2

]

C
D
.
s
c
[

1
v
5
7
0
1
0
.
2
1
1
2
:
v
i
X
r
a

Memory-efficient array redistribution through portable
collective communication
NORMAN A. RINK, DeepMind, United Kingdom
ADAM PASZKE, Google Research, Poland
DIMITRIOS VYTINIOTIS, DeepMind, United Kingdom
GEORG STEFAN SCHMIDâˆ—, EPFL, Switzerland

Modern large-scale deep learning workloads highlight the need for parallel execution across many devices
in order to fit model data into hardware accelerator memories. In these settings, array redistribution may
be required during a computation, but can also become a bottleneck if not done efficiently. In this paper we
address the problem of redistributing multi-dimensional array data in SPMD computations, the most prevalent
form of parallelism in deep learning. We present a type-directed approach to synthesizing array redistributions
as sequences of MPI-style collective operations. We prove formally that our synthesized redistributions are
memory-efficient and perform no excessive data transfers. Array redistribution for SPMD computations using
collective operations has also been implemented in the context of the XLA SPMD partitioner, a production-
grade tool for partitioning programs across accelerator systems. We evaluate our approach against the XLA
implementation and find that our approach delivers a geometric mean speedup of 1.22Ã—, with maximum
speedups as a high as 5.7Ã—, while offering provable memory guarantees, making our system particularly
appealing for large-scale models.

1 INTRODUCTION
The growth in model complexity and size has driven high-performance computing, and especially
deep learning (DL), towards distributed computing, which offers two key benefits. First, it increases
the available compute power and can thus speed up workloads. Second, it makes it possible to
scale the computation beyond the memory capacity of a single device. These benefits come at the
cost of a change in programming model, which must reflect the distributed memory structure. A
common programming model for training DL models, e.g. [32], is the single-program-multiple-data
(SPMD) model, where all devices run the same executable and can also perform MPI-style collective
operations to synchronize and exchange data. While very simple, the SPMD model is expressive
enough to implement techniques such as data parallelism, parameter sharding, and even pipeline
parallelism [34].

The SPMD model is available in many DL frameworks, but it is exposed in different ways.
The simplest one surfaces the SPMD computation to the user and forces them to insert cross-
device collectives manually (e.g. xmap() in JAX, Mesh Tensorflow [24]). A slightly more automated
approach is to transform a single-device program with user-specified partitioning annotations
into an explicit SPMD computation [34]. Finally, fully-automated approaches can rewrite almost
arbitrary single-device programs as SPMD programs with minimal supervision [13]. Many of these
systems rely on similar concepts: the dimensions of arrays can be partitioned over the axes of
a device mesh, while operators or even whole sub-computations are executed on every device
independently, with brief synchronization points in between to ensure the correct assignment of
data chunks to devices.

While these synchronization points may seem minor, they correspond to distributed communi-
cation, commonly done through widely available MPI-style communication operations (cf. NVIDIA

âˆ—Work done while at DeepMind, United Kingdom.

Authorsâ€™ addresses: Norman A. Rink, nrink@google.com, DeepMind, United Kingdom; Adam Paszke, apaszke@google.com,
Google Research, Poland; Dimitrios Vytiniotis, dvytin@google.com, DeepMind, United Kingdom; Georg Stefan Schmid,
georg.schmid@epfl.ch, EPFL, Switzerland.

 
 
 
 
 
 
2

Norman A. Rink, Adam Paszke, Dimitrios Vytiniotis, and Georg Stefan Schmid

NCCL, XLA, commercial MPI implementations). We focus specifically on the communication
required to redistribute multi-dimensional arrays across the devices in a distributed system ex-
ecuting SPMD programs. Redistribution can easily become a bottleneck due to the bandwidth
of cross-device links usually being magnitudes smaller than that of the on-device memory bus.
Therefore, a lot of effort is typically spent on carefully hand-crafting partitioning strategies to
minimize expensive data transfers. Automatic partitioning tools can take on this effort, but then
the tools become responsible for inserting redistributions. Hence there is a clear need for efficient
array redistribution.

We propose a novel approach to synthesizing array redistributions as sequences of MPI-style
collective operations. While partitioning of multi-dimensional arrays has a long history, by intro-
ducing types and formal semantics for distributed arrays that extend to MPI-style collectives we
are able to make the following contributions:

â€¢ We show that redistributions implemented as sequences of collectives enjoy normal forms
that guarantee that per-device memory never exceeds the maximum of the per-device input
and output tile sizes. (Section 4)

â€¢ We prove that any redistribution problem can be solved by a sequence of collectives that is
both in normal form and optimal with respect to data transfer, up to a final permutation of
data across devices. (Section 6)

â€¢ We devise a search procedure that synthesizes a nearly-optimal sequence of collectives for
any given redistribution problem. (Section 7) The search space is made manageable by passing
to a weak interpretation of types, which we originally introduce to facilitate formal work.
(Section 5)

â€¢ We experimentally evaluate our search-based synthesis and compare with redistributions
generated by the XLA SPMD partitioner. Our approach delivers a geometric mean speedup
of 1.22Ã—, while also guaranteeing memory efficiency. (Section 8)

Since we synthesize sequences of portable MPI-style collectives, our work can be directly trans-

ferred to any framework or system that has access to such collectives.

2 BACKGROUND AND MOTIVATION
Our work applies to any array language that supports distributed arrays. To be specific in our
presentation we will introduce distributed array types and motivate redistribution in the context
of PartIR:SPMD, an intermediate representation used in generating code from the higher-level
array language PartIR (â€œPartitioning Intermediate Representationâ€). PartIR is designed to support
automated exploration of partitioning strategies; it has array types and operations, as well as
tiling loop constructs, but neither distributed arrays nor redistribution instructions. At the level
of PartIR, partitioning of arrays is an implicit consequence of how arrays are used by specific
instructions. Lowering to PartIR:SPMD happens after all exploration decisions that affect the
partitioning of arrays have been taken and, in lowering to PartIR:SPMD, all redistributions become
explicit instructions. A detailed presentation of PartIR and PartIR:SPMD is beyond the scope of
this paper and is not required to discuss and solve the problem of efficiently redistributing arrays.
Note though that the exploration of partitioning strategies in PartIR can generate many array
redistributions in PartIR:SPMD, which have to run efficiently, further motivating our work. We
introduce PartIR:SPMD by example, starting with Listing 1.

1 mesh {" devices ": 32}
2

3 fn main (x : [8{ " devices " }256 , 1024] , p : [1024 , 10]) -> [8{ " devices " }256 , 10] {
4

y = spmd_op x p ( xtile : [8 , 1024] , ptile : [1024 , 10]) {

Memory-efficient array redistribution through portable collective communication

3

w = matmul xtile ptile : [8 , 10]
return w

} as [8{ " devices " }256 , 10]

return y

5

6

7

8

9 }

Listing 1. Distributed types in SPMD computations.

2.1 Distributed array types
The first line of Listing 1 declares the mesh, a set of named axes which describes the available
hardware resources. Here, we use a mesh with a single axis of size 32. Although the mesh describes
the available devices, it does not denote concrete hardware devices but only a logical space of device
coordinates. To execute a PartIR:SPMD program, the runtime system must choose a mapping from
logical device coordinates in a mesh to real hardware devices.

In line 3, we declare a function main() that executes over the mesh. The functionâ€™s signature is
specified with distributed array types. The syntax of distributed types is similar to that of regular
array types, but the entries for dimensions are no longer restricted to integer literals. Instead we
additionally allow partitioning specifications of the form:

tileSize{ğ‘ğ‘¥ğ‘–ğ‘ 1, . . . , ğ‘ğ‘¥ğ‘–ğ‘ ğ‘› }globalSize
where tileSize and globalSize are integer literals, while ğ‘ğ‘¥ğ‘–ğ‘ 1, . . . , ğ‘ğ‘¥ğ‘–ğ‘ ğ‘› are a subset of axes in the
mesh. The meaning of this annotation is that the data represents a global array with size globalSize
in that dimension, but it has been partitioned over the specified mesh axes. After partitioning, every
device only stores a tile of the global array with size tileSize in that dimension.1

When working over a mesh with multiple axes, e.g. {"x": 2, "y": 2}, the order of axes in a
single partitioning specification matters. It does not affect the way the data is partitioned â€” we
always split the global array into as many tiles as the product of axis sizes dictates â€” but it does
affect how those tiles are assigned to devices. E.g., an array [8{"x", "y"}32] is first partitioned
into 2 tiles over "y", and then each of these tiles is further partitioned into 2 tiles along "x". This
yields 4 final tiles of shape [8] each. Reversing the order of axes will result in the same tile shape,
but in a different device assignment. We use the convention of interpreting the mesh axes as being
listed in minor-to-major order (fastest-changing first).

Finally, we remark that if an array type has no partitioned dimensions, then it is replicated on all

devices, i.e. each device holds an identical copy of the data in its entirety.

2.2 SPMD computations
The spmd_op operator from line 4 of Listing 1 is the key computational construct of PartIR:SPMD. It
accepts a number of distributed arrays as arguments and returns distributed arrays. In addition, it is
parameterized by a lambda expression that takes the local tiles of the spmd_op arguments, performs
some (non-distributed) computation on these tiles independently of other devices, and returns the
resulting local tiles of the output distributed arrays.

In Listing 1 the spmd_op takes two arguments: x of type [8{"devices"}256, 1024] which translates
into an argument xtile of type [8, 1024], and p of type [1024, 10] which matches the type of
ptile because that value is replicated. The spmd_op returns local tiles of shape [8, 10] that are
interpreted as pieces of a global array of shape 256 Ã— 10 that has been partitioned along its first
dimension, as indicated by type [8{"devices"}256, 10] following the as keyword.

1Our notation can simplify reasoning but is generally a bit verbose: globalSize must be equal to tileSize times the
product of axis sizes.

4

Norman A. Rink, Adam Paszke, Dimitrios Vytiniotis, and Georg Stefan Schmid

Fig. 1. Distributed types over a two-dimensional mesh {"xdev": 4, "ydev": 8}. Numbers and colors
identify distinct tiles of data. Left: [64{"xdev"}256, 1024]. Right: [1024, 128{"ydev"}1024]. Cf. argument
types of function main() in Listing 2.

2.3 Example: distributed matrix multiplication
It is often convenient to introduce multiple axes of parallelism in a program, viewing the available
hardware resources as a multi-dimensional mesh. A popular example is Megatron-style partitioning
of transformers [25], combining data parallelism and parameter sharding. Here we consider the
simpler example of matrix multiplication in Listing 2.

1 mesh {" xdev ": 4, " ydev ": 8}
2

3 fn main (x : [64{ " xdev " }256 , 1024] ,
4

p : [1024 , 128{ " ydev " }1024]) -> [64{ " xdev " }256 , 128{ " ydev " }1024] {

y = spmd_op x p ( xtile : [64 , 1024] , ptile : [1024 , 128]) {

w = matmul xtile ptile : [64 , 128]
return w

} as [64{ " xdev " }256 , 128{ " ydev " }1024]

return y

5

6

7

8

9

10 }

Listing 2. Distributed types over a multi-dimensional mesh.

Both arguments to main() are partitioned. However, each is split along only one of the two
available mesh axes, leading to a situation often called partial replication. Figure 1 shows how the
tiles of the arguments are laid out across the mesh.

Now, recall that the first dimension of xtile (of size 64) comes from partitioning a dimension of
x, and similarly for the second dimension of ptile (of size 128). Hence, the matrix multiplication
on line 6 contracts away the unpartitioned dimension of size 1024, and produces an output that has
its first dimension derived from the partitioning of xtile, and the second dimension derived from
the partitioning of ptile. This means that the outputs of the spmd_op actually vary along both mesh
axes, which is why we ascribe type [64{"xdev"}256, 128{"ydev"}1024] to the result.2

2.4 Redistribution
When multiple spmd_op appear in a PartIR:SPMD program, it is not uncommon that produced and
consumed arrays are incompatibly partitioned. This is the case in the program in Listing 3, which
specifies a chain matrix multiplication where x is first multiplied by w1 and then by w2.

1 mesh {" devs ": 32}
2

3 fn main (x : [32 , 1024] ,
4

5

w1 : [1024 , 64{ " devs " }2048] ,
w2 : [2048 , 256]) -> [1{ " devs " }32 , 256] {

2The semantics of PartIR:SPMD allows other types to be ascribed to the output of the spmd_op, but then Listing 2 would
not denote matrix multiplication.

32103210321032103210321032103210xdevydev01234567012345670123456701234567ydevxdevMemory-efficient array redistribution through portable collective communication

5

Fig. 2. The redistribution from Listing 3. Colors identify tiles of data, inscribed numbers identify devices.

y = spmd_op x w1 ( xtile : [32 , 1024] , wtile : [1024 , 64]) {

return ( matmul xtile wtile )

} as [32 , 64{ " devs " }2048]

z = redistribute y as [1{ " devs " }32 , 2048]
w = spmd_op z w2 ( ztile : [1 , 2048] , wtile : [2048 , 256]) {

return ( matmul ztile wtile )

} as [1{ " devs " }32 , 256]

return w

6

7

8

9

10

11

12

13

14 }

Listing 3. Partitioned chain matrix multiplication.

The first spmd_op implements the first of the matrix multiplications. The 1024-sized dimension is
not partitioned in either x or w1, so this can be carried out on every device independently, without
any communication. The result is again a matrix, but with its second dimension partitioned in the
same way as that of w1. Now we would like to perform the second multiplication, but there is an
issue: the 2048-sized dimension we want to contract over is partitioned in the intermediate value y,
meaning that no device can compute the output independently. To resolve this issue, we insert a
redistribute operation, which intuitively acts as a type cast between two distributed array types:
[32, 64{"devs"}2048] â‡ [1{"devs"}32, 2048]
Operationally, it has to perform data exchange to obtain tiles of the global array that fit the new
partitioning. Figure 2 depicts this redistribute operation: the input matrix is split column-wise
into tiles of shape [32, 64], while the output matrix is partitioned row-wise into tiles of shape
[1, 2048]. After this redistribution, the 2048-sized dimension has become local and the second
matrix multiplication can be done locally, as shown in the second spmd_op in Listing 3.

2.5 Redistribution semantics
The following must hold for redistribute operations:

A redistribution from ğœ1 to ğœ2 is valid iff the global array types for ğœ1 and ğœ2 are the same.
For instance, it is not meaningful to redistribute [512, 32{"devs"}1024] to [1024, 32{"devs"
}1024] since the target type semantically represents a larger global array, and we have no way
of producing that extra data. The reverse redistribution is also invalid since we would not know
which part of the data to retain and which to throw out.

More interestingly, our validity criterion also excludes a redistribution from [32{"xdevs"}128,
32{"ydevs"}256] to [32{"xdevs", "ydevs"}1024, 32] on the mesh {"xdevs": 4, "ydevs": 8}. The

(cid:62)(cid:19)(cid:15)(cid:19)(cid:64)(cid:62)(cid:19)(cid:15)(cid:20)(cid:64)(cid:62)(cid:19)(cid:15)(cid:21)(cid:64)(cid:62)(cid:19)(cid:15)(cid:22)(cid:64)(cid:62)(cid:19)(cid:15)(cid:23)(cid:64)(cid:62)(cid:19)(cid:15)(cid:24)(cid:64)(cid:62)(cid:19)(cid:15)(cid:25)(cid:64)(cid:62)(cid:19)(cid:15)(cid:26)(cid:64)(cid:62)(cid:22)(cid:15)(cid:19)(cid:64)(cid:62)(cid:22)(cid:15)(cid:20)(cid:64)(cid:62)(cid:22)(cid:15)(cid:21)(cid:64)(cid:62)(cid:22)(cid:15)(cid:22)(cid:64)(cid:62)(cid:22)(cid:15)(cid:23)(cid:64)(cid:62)(cid:22)(cid:15)(cid:24)(cid:64)(cid:62)(cid:22)(cid:15)(cid:25)(cid:64)(cid:62)(cid:22)(cid:15)(cid:26)(cid:64)(cid:17)(cid:17)(cid:17)(cid:62)(cid:19)(cid:15)(cid:19)(cid:64)(cid:62)(cid:20)(cid:15)(cid:19)(cid:64)(cid:62)(cid:21)(cid:15)(cid:19)(cid:64)(cid:62)(cid:22)(cid:15)(cid:19)(cid:64)(cid:62)(cid:19)(cid:15)(cid:20)(cid:64)(cid:62)(cid:20)(cid:15)(cid:20)(cid:64)(cid:62)(cid:21)(cid:15)(cid:20)(cid:64)(cid:62)(cid:22)(cid:15)(cid:20)(cid:64)(cid:62)(cid:19)(cid:15)(cid:25)(cid:64)(cid:62)(cid:20)(cid:15)(cid:25)(cid:64)(cid:62)(cid:21)(cid:15)(cid:25)(cid:64)(cid:62)(cid:22)(cid:15)(cid:25)(cid:64)(cid:62)(cid:19)(cid:15)(cid:26)(cid:64)(cid:62)(cid:20)(cid:15)(cid:26)(cid:64)(cid:62)(cid:21)(cid:15)(cid:26)(cid:64)(cid:62)(cid:22)(cid:15)(cid:26)(cid:64)(cid:265)(cid:265)(cid:265)(cid:303)(cid:253)(cid:251)(cid:247)(cid:248)(cid:250)(cid:249)(cid:250)(cid:247)(cid:250)(cid:248)(cid:247)(cid:248)(cid:250)(cid:247)(cid:250)(cid:248)(cid:20)(cid:21)(cid:27)(cid:25)(cid:23)(cid:284)(cid:250)(cid:249)(cid:266)(cid:303)(cid:253)(cid:251)(cid:282)(cid:301)(cid:139)(cid:143)(cid:225)(cid:204)(cid:301)(cid:283)(cid:249)(cid:247)(cid:251)(cid:255)(cid:285)(cid:303)(cid:284)(cid:248)(cid:282)(cid:301)(cid:139)(cid:143)(cid:225)(cid:204)(cid:301)(cid:283)(cid:250)(cid:249)(cid:266)(cid:303)(cid:249)(cid:247)(cid:251)(cid:255)(cid:285)(cid:284)(cid:253)(cid:251)(cid:282)(cid:301)(cid:232)(cid:139)(cid:143)(cid:225)(cid:301)(cid:266)(cid:303)(cid:301)(cid:231)(cid:139)(cid:143)(cid:225)(cid:301)(cid:283)(cid:249)(cid:247)(cid:251)(cid:255)(cid:266)(cid:248)(cid:249)(cid:255)(cid:285)(cid:284)(cid:253)(cid:251)(cid:282)(cid:301)(cid:231)(cid:139)(cid:143)(cid:225)(cid:301)(cid:266)(cid:303)(cid:301)(cid:232)(cid:139)(cid:143)(cid:225)(cid:301)(cid:283)(cid:249)(cid:247)(cid:251)(cid:255)(cid:266)(cid:248)(cid:249)(cid:255)(cid:285)(cid:265)(cid:265)(cid:265)(cid:303)(cid:248)(cid:249)(cid:247)(cid:251)(cid:255)(cid:17)(cid:17)(cid:17)6

Norman A. Rink, Adam Paszke, Dimitrios Vytiniotis, and Georg Stefan Schmid

[64{"ydev", "xdev"}2048, 128] â‡ [64{"xdev", "ydev"}2048, 128]
[32{"xdev"}128, 16{"ydev"}64] â‡ [32{"ydev"}128, 16{"xdev"}64]
[32{"xdev"}128] â‡ [32{"ydev"}128]

swap within dimension
swap across dimensions
swap for replicated axis

Fig. 3. Redistributions via permutation over the mesh {"xdev": 4, "ydev": 4}.

per-device tile shape is equal for both types, and there are equal numbers of tiles, but the semantics
of the two types have to be different as each represents a global array of a different shape.

2.6 Collective operations
We do not want to focus on the problem of fine-grained data transfers, but instead defer data
exchange to well-established, portable and highly optimized routines for collective communication.
We have isolated a set of common collectives that are sufficient to implement array redistribution:3
â€¢ allgather(ğ‘–) removes the minor-most (left-most) axis from the partitioning specification of

dimension ğ‘–, as illustrated on the right of Figure 4. (T-AllGather)

â€¢ dynslice(ğ‘–, ğ‘¥) introduces a new minor-most axis into the partitioning specification of dimen-
sion ğ‘–. The local size of dimension ğ‘– has to be divisible by the size of the introduced axis, and
that axis cannot already partition any array dimension. (T-DynSlice)

â€¢ alltoall(ğ‘–, ğ‘—) transfers the minor-most axis from the partitioning specification of dimension
ğ‘– to dimension ğ‘—. The local size of dimension ğ‘— must be divisible by the size of the moved
mesh axis. (T-AllToAll)

â€¢ allpermute can transform any ğœ1 to ğœ2 if their local and global shapes match. (T-Permute)
While the kinds of redistribution problems that can be handled by the first three collectives are
relatively easy to understand, let us give a few concrete examples showing the utility of allpermute.

Example 2.1 (Redistributions via permutation). Any bijective reassignment of local tiles to devices
can be performed by a single allpermute collective. Consider the mesh {"xdev": 4, "ydev": 4}.
Three prominent cases handled by permutation are listed in Figure 3. In all these cases the local
per-device type does not change between the source and target type, and this is precisely what
justifies that they are implementable directly with a permutation.

Figure 4 (left) illustrates tile reassignment for the last permutation in Figure 3 (i.e. swap for a

replicated axis).

Finally, note that while each of the three redistributions in Figure 3 can be handled by allpermute,
it can additionally perform multiple of them at the same time. For example, it is possible to swap
some axes within one dimension, move axes between other dimensions, and swap out certain
axes for ones that replicate the data, all at the same time. Conversely, any permutation of tiles
facilitated by allpermute is a composition of the three redistributions in Figure 3 (i.e. swapping
within a dimension, swapping across dimensions and swapping for a replicated axis).

3 CHALLENGE: SYNTHESIZING REDISTRIBUTIONS
So far we have seen simple examples of redistribution problems that can be solved using a single
collective operation. In general, however, we need to synthesize whole sequences of collectives
to implement a specific redistribution, with the space of potential candidate sequences growing
quickly with the number of mesh axes and complexity of the partitioning specifications in the

3The names T-AllGather, T-DynSlice, T-AllToAll and T-Permute that appear in parentheses refer to Figure 8, which is
discussed in detail starting in Section 4.

Memory-efficient array redistribution through portable collective communication

7

Fig. 4. Redistributions over {"xdev": 4, "ydev": 4}. Left: Permutation by exchanging for a replicated axis,
from type [32{"xdev"}128] to [32{"ydev"}128]. Right: allgather from type [32{"xdev","ydev"}512, 512] to
[128{"ydev"}512, 512].

[32{ğ‘¥, ğ‘¦ }512, 128]
[32{ğ‘¦, ğ‘¥ }512, 128]
[128{ğ‘¥ }512, 32{ğ‘¦ }128]
[32{ğ‘¥ }128, 512, 32{ğ‘¦ }128]
[128, 128{ğ‘¥ }512, 32{ğ‘¦ }128]
[128, 32{ğ‘¦, ğ‘¥ }512, 128]

allpermute
alltoall(0, 1)

alltoall(0, 1)
alltoall(2, 1)

[32{ğ‘¥ }128, 512, 32{ğ‘¦ }128]
[128, 512, 32{ğ‘¦ }128]
[128, 128{ğ‘¦ }512, 128]

allgather(0)
alltoall(2, 1)

[32{ğ‘¥, ğ‘¦ }512, 128]
[128{ğ‘¦ }512, 32{ğ‘¥ }128]
[128{ğ‘¥ }512, 32{ğ‘¦ }128]
[32{ğ‘¥ }128, 512, 32{ğ‘¦ }128]
[32{ğ‘¥ }128, 128{ğ‘¦ }512, 128]
[128, 32{ğ‘¥, ğ‘¦ }512, 128]
[128, 32{ğ‘¦, ğ‘¥ }512, 128]
[32{ğ‘¥ }128, 512, 32{ğ‘¦ }128]
[32{ğ‘¥ }128, 128{ğ‘¦ }512, 128]
[128, 128{ğ‘¦ }512, 128]

alltoall(0, 1)

allpermute

alltoall(2, 1)
alltoall(0, 1)
allpermute

alltoall(2, 1)
allgather(0)

Fig. 5. Examples of redistributions implemented by sequences of collective operations on the mesh {ğ‘¥ : 4, ğ‘¦ : 4}.
Every row shows two different implementations of the same redistribution (and more may be possible).

source and target distributed types. Some examples of redistribution problems that require multiple
steps and allow multiple programs can be found in Figure 5.

The challenge we focus on for the rest of this paper is how to synthesize efficient redistribution
programs (sequences of collectives), taking into account both the peak per-device memory usage
(to avoid out-of-memory errors) as well as the overall amount of data transferred. The following
challenging example motivates much of the subsequent work.

Example 3.1 (Factor decomposition). Consider the problem of redistributing the distributed array
of type [3{"x"}12, 2{"y"}12] to type [2{"y"}12, 3{"x"}12], over the mesh {"x": 4, "y": 6} of
24 devices. Our collective operations do not leave much space for an efficient execution of this
redistribution other than allgather, to first create the fully replicated type [12, 12], followed by
dynslice to partition along the axes again. This is inefficient both in terms of peak memory usage,
since all data has to be temporarily replicated in all device memories, and communication, because
the dynslice operations discard a large portion of the data received.

The root of the issue is that "x" and "y" cannot partition a single dimension at the same time,
and mesh axes are atomic so that we cannot just move an axis partially to avoid over-partitioning a
dimension. But, consider temporarily viewing our 2D mesh as a 4D mesh given by {"x1": 2, "x2":
2, "y1": 3, "y2": 2}. In that case our redistribution can be restated as:

[3{"x1","x2"}12, 2{"y1","y2"}12] â‡ [2{"y1","y2"}12, 3{"x1","x2"}12]
And this equivalent redistribution problem can be solved without ever gathering all of the data on
a single device:

32103210321032100000111122223333xdevydevhgfelkjiponmxdevydevabcdabcdabcdabcdefghefghefghefghijklijklijklijklmnopmnopmnopmnopdcba8

Norman A. Rink, Adam Paszke, Dimitrios Vytiniotis, and Georg Stefan Schmid

, 2{"y1","y2"}12 ] â‡alltoall(1,0)

[3{"x1","x2"}12
[1{"y1","x1","x2"}12, 6{"y2"}12
[1{"x1","y1","x2"}12, 6{"y2"}12
[2{"y1","x2"}12
[2{"y1","y2"}12

, 3{"x1","y2"}12 ] â‡allpermute
, 3{"x1","x2"}12 ]

] â‡allpermute
] â‡alltoall(1,0)

Example 3.1 lets us hope that we can generate sequences of collectives whose per-device memory

usage never exceeds input or output tile sizes, provided we observe the following:

Principle 1 Memory-efficient redistribution (with collective operations) requires working
with multi-dimensional meshes that have all axes of prime sizes.

From now on we assume that meshes have been decomposed into axes of prime sizes. This happens
without loss of generality because every redistribution problem can be converted into an equivalent
one that satisfies this assumption.

Example 3.1 reveals another challenge: the allpermute operation between the alltoall operations
had been inserted there purely for the sake of type bookkeeping because alltoall cannot transfer
the axis "x1" unless it appears at the front of the partitioning specification for a dimension. Similar
problems arise countless times in more involved examples, and we would hope to elide intermediate
permutations for the sake of efficiency. Intuitively this should be possible: allpermute only changes
the assignment of tiles to devices, and this is necessary only because alltoall is parameterized by
a mesh axis. If we could specify explicit device coordinates instead, then it would be possible to
perform the second alltoall without the intermediate allpermute operation.

Later (Section 6) we will give a more general formulation of the collective operations that allows
us to use collectives more flexibly, i.e. addressing individual devices rather than axes in a mesh. For
now, we only state a principle that lets us elide intermediate permutations:

Principle 2 To optimize the cost of redistribution, we should use collective operations
that work up to permutation of the tiles on the mesh devices.

Our two principles will guide the reasoning about array redistribution in the next sections.

4 FORMAL SEMANTICS FOR REDISTRIBUTION
Starting in this section, we develop formal results about array redistribution based on collective
operations. While our results are cast into lemmas and theorems, we only outline proofs in prose,
and only for interesting results. More detailed proofs of all our lemmas and theorems are given in
Appendix A.

Figure 6 formally defines the syntax of distributed array types and (device) meshes. As in a
PartIR:SPMD program, we usually assume a fixed global mesh ğ» , consisting of prime-factor axes,
as discussed in Section 3. Figure 6 also gives precise definitions of auxiliary meta-functions that we
have informally alluded to before, e.g. ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’. As stated in Section 2.5, we consider redistribution
between types ğœ1, ğœ2 to be valid only if ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ1) = ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ2).

4.1 Semantics for distributed array types
A distributed type ğœ specifies how global data is distributed across ğ» . The meaning of ğœ is a map
that assigns exactly one tile of the global array data to each index tuple ğ‘– âˆˆ ğ» . The ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ)
gives the dimensions of each local tile, which must be the same for all tiles. Each tile is identified
by its base offset, which is the (lexicographically) lowest index tuple for the data in the tile when
viewed as data in the global array. We use the term base offset map to refer to a map that assigns
a base offset to each point ğ‘– âˆˆ ğ» . In Figure 7a, T âŸ¦ğœâŸ§ğ» gives a precise definition of the base offset
map for a type ğœ.

Memory-efficient array redistribution through portable collective communication

9

integer literals
string literals

::= {ğ›¼1, . . . , ğ›¼ğ‘› }
::= ğ‘¥ : ğ‘˜

ğ‘, ğ‘ , ğ‘›
âˆˆ
ğ‘¥, ğ‘¦, ğ‘§
âˆˆ
Meshes and axes
ğ»
ğ›¼
Distributed dimensions and types
ğ‘’, ğ‘‘, ğ‘”
ğœ, ğœ
Notation for (possibly empty) sequences
ğ‘’

::= ğ‘˜ {ğ‘¥ }ğ‘›
::= [ğ‘‘1, . . . , ğ‘‘ğ‘›]

::= Â·

| ğ‘’, ğ‘’

ğ‘ğ‘¥ğ‘’ğ‘  (ğ‘{ğ‘¥ }ğ‘›)
ğ‘ğ‘¥ğ‘’ğ‘  ([ğ‘0{ğ‘¥ 0}ğ‘ 0, . . . , ğ‘ğ‘› {ğ‘¥ğ‘› }ğ‘ ğ‘›])
ğ‘Ÿğ‘ğ‘›ğ‘˜ ([ğ‘0{ğ‘¥ 0}ğ‘ 0, . . . , ğ‘ğ‘› {ğ‘¥ğ‘› }ğ‘ ğ‘›])
ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ ([ğ‘0{ğ‘¥ 0}ğ‘ 0, . . . , ğ‘ğ‘› {ğ‘¥ğ‘› }ğ‘ ğ‘›]) = [ğ‘ 0, . . . , ğ‘ ğ‘›]
ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ ([ğ‘0{ğ‘¥ 0}ğ‘ 0, . . . , ğ‘ğ‘› {ğ‘¥ğ‘› }ğ‘ ğ‘›])
ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ ([ğ‘0{ğ‘¥ 0}ğ‘ 0, . . . , ğ‘ğ‘› {ğ‘¥ğ‘› }ğ‘ ğ‘›])
ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ ([ğ‘0{ğ‘¥ 0}ğ‘ 0, . . . , ğ‘ğ‘› {ğ‘¥ğ‘› }ğ‘ ğ‘›])

= (cid:206)ğ‘›
ğ‘–=0 ğ‘ ğ‘–
= [ğ‘0, . . . , ğ‘ğ‘›]
= (cid:206)ğ‘›
ğ‘–=0 ğ‘ğ‘–

= ğ‘¥
= âˆªğ‘›
ğ‘–=0ğ‘¥ğ‘–
= ğ‘› + 1

Fig. 6. Meshes, distributed types, and meta-functions.

DâŸ¦ğ‘‘âŸ§ğ» : ğ» â†’ N
DâŸ¦ğ‘ { }ğ‘âŸ§ğ» (ğ‘–)
DâŸ¦ğ‘ {ğ‘¥, ğ‘¥ }ğ‘›âŸ§ğ» (ğ‘–)

TâŸ¦ğœâŸ§ğ» : ğ» â†’ Nğ‘Ÿğ‘ğ‘›ğ‘˜ (ğœ )
TâŸ¦[ğ‘‘1, . . . , ğ‘‘ğ‘› ]âŸ§ğ» (ğ‘–)

=
=

=

0
ğ‘ Â· ğ‘–ğ‘¥ + DâŸ¦(ğ‘ Â· ğ‘˜) {ğ‘¥ }ğ‘›âŸ§ğ» (ğ‘–)

( DâŸ¦ğ‘‘1âŸ§ğ» (ğ‘–), . . . , DâŸ¦ğ‘‘ğ‘›âŸ§ğ» (ğ‘–))

ğ‘¥ğ‘– : ğ‘˜ğ‘– âˆˆ ğ» ğ‘ Â· (cid:206)ğ‘– ğ‘˜ğ‘– = ğ‘›
ğ‘¥ğ‘– â‰  ğ‘¥ ğ‘— for every ğ‘–, ğ‘—
ğ» âŠ¢ ğ‘ {ğ‘¥ }ğ‘›

wf-dim

ğ» âŠ¢ ğ‘ğ‘– {ğ‘¥ğ‘– }ğ‘ ğ‘–
ğ‘¥ğ‘– #ğ‘¥ ğ‘— for every ğ‘–, ğ‘—
ğ» âŠ¢ [ğ‘0 {ğ‘¥ 0 }ğ‘ 0, . . . , ğ‘ğ‘› {ğ‘¥ğ‘› }ğ‘ ğ‘› ]

wf-type

(a) Base offset maps

(b) Well-formedness rules

Fig. 7. Semantics for distributed dimensions and types.

ğ‘¥ : ğ‘› âˆˆ ğ»
ğ» âŠ¢ allgather(ğ‘–) : [. . . , ğ‘ğ‘– {ğ‘¥, ğ‘¥ğ‘– }ğ‘ ğ‘–, . . .] â†’ [. . . , (ğ‘ğ‘– Â· ğ‘›) {ğ‘¥ğ‘– }ğ‘ ğ‘–, . . .]

t-allgather

ğ‘¥ : ğ‘› âˆˆ ğ» ğ‘¥ âˆ‰ ğ‘ğ‘¥ğ‘’ğ‘  ( [. . . , (ğ‘ğ‘– Â· ğ‘›) {ğ‘¥ğ‘– }ğ‘ ğ‘–, . . .])
ğ» âŠ¢ dynslice(ğ‘–, ğ‘¥) : [. . . , (ğ‘ğ‘– Â· ğ‘›) {ğ‘¥ğ‘– }ğ‘ ğ‘–, . . .] â†’ [. . . , ğ‘ğ‘– {ğ‘¥, ğ‘¥ğ‘– }ğ‘ ğ‘–, . . .]

t-dynslice

ğ‘’ğ‘– = ğ‘ğ‘– {ğ‘¥, ğ‘¥ğ‘– }ğ‘ ğ‘–

ğ‘’ ğ‘— = ğ‘ ğ‘— {ğ‘¥ ğ‘— }ğ‘  ğ‘—

ğ‘’â€²
ğ‘– = (ğ‘ğ‘– Â· ğ‘›) {ğ‘¥ğ‘– }ğ‘ ğ‘–

ğ‘¥ : ğ‘› âˆˆ ğ» ğ‘ ğ‘— ğ‘šğ‘œğ‘‘ ğ‘› = 0
ğ‘’â€²
ğ‘— = (ğ‘ ğ‘— ğ‘‘ğ‘–ğ‘£ ğ‘›) {ğ‘¥, ğ‘¥ ğ‘— }ğ‘  ğ‘—
ğ‘–, ğ‘— â†¦â†’ ğ‘’â€²
ğ‘— ]

ğ‘’â€² = ğ‘’ [ğ‘– â†¦â†’ ğ‘’â€²

ğ» âŠ¢ alltoall(ğ‘–, ğ‘—) : [ğ‘’ ] â†’ [ğ‘’â€²]

t-alltoall

localtype(ğœ1) = localtype(ğœ2)
ğ» âŠ¢ allpermute : ğœ1 â†’ ğœ2

t-permute

Fig. 8. Typing collective operations.

The rules in Figure 7b formalize that axes must appear in a well-formed distributed type in an
affine way. If this is the case for type ğœ, then the image of T âŸ¦ğœâŸ§ğ» contains base offsets for all tiles
that make up the global array data, as the following lemmas establish.

Lemma 4.1. If ğ» âŠ¢ ğ‘{ğ‘¥ }ğ‘›, then the image of DâŸ¦ğ‘{ğ‘¥ }ğ‘›âŸ§ğ» consists of all multiples of ğ‘ below ğ‘›.

Lemma 4.2. If ğ» âŠ¢ ğœ, then the image of T âŸ¦ğœâŸ§ğ» consists of all base offsets that are (lexicographically)

below ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ).

10

Norman A. Rink, Adam Paszke, Dimitrios Vytiniotis, and Georg Stefan Schmid

ğ‘¥ : ğœ âˆˆ Î“
ğ‘ ğ‘– = localtype(ğœğ‘– )
ğ‘¥ğ‘  : ğ‘  âŠ¢xla ğ‘’ : [ğ‘1, . . . , ğ‘ğ‘› ]
localtype(ğœ) = [ğ‘1, . . . , ğ‘ğ‘› ]
ğ» ; Î“ âŠ¢ spmd_op ğ‘¥ (ğœ†ğ‘¥ğ‘  â†’ ğ‘’) as ğœ : ğœ

t-spmd

âŸ¦ğ‘¥ ğ‘— âŸ§Î”

ğ» = (ğ‘£ğ‘— , ğ›½ ğ‘— )

ğ›½ = TâŸ¦ğœâŸ§ğ»

for all ğ‘– âˆˆ ğ», âŸ¦ğ‘’âŸ§xla

ğ‘¥ ğ‘— â†¦â†’ğ‘£ğ‘— [ğ›½ ğ‘— (ğ‘– ) ]

= ğ‘£ [ğ›½ (ğ‘–) ]

âŸ¦spmd_op ğ‘¥ (ğœ†ğ‘¥ğ‘  â†’ ğ‘’) as ğœâŸ§Î”

ğ» = (ğ‘£, ğ›½)

d-spmd

Fig. 9. Typing (left) and semantics (right) for SPMD regions. Left: a typing relation for the underlying array
language (XLA) is assumed. Right: the environment Î” maps variables to their denotations, i.e. to pairs of
global values and base offset maps; ğ‘£ [ğ›½ (ğ‘–)] denotes the tile of ğ‘£ that starts at the base offset given by ğ›½ (ğ‘–)
and has the appropriate size.

We only consider well-formed distributed types ğœ. By the previous lemma, the tiles with base
offsets in the image of T âŸ¦ğœâŸ§ğ» form a partitioning4 of the global data with shape ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ), as
we would expect of a meaningful definition of distributed array type.

4.2 Semantics of collective operations
Having given semantics to distributed array types, it is now straightforward to assign meanings
also to collective operations. In fact, the typing rules for collective operations in Figure 8 fully
determine the semantics that must be assigned to these operations. To understand why, note that
the type signatures from Figure 8 give much stronger guarantees than common function types in
programming languages: this is because our collective operations do not manipulate data, they
merely modify how tiles of data are placed on devices, and that is precisely what is prescribed by
our distributed types. Consequently, the meaning of a collective operation is how it transforms
base offset maps, which we can directly â€œread offâ€ its typing rule.

Definition 4.3. Let ğ‘ be an allgather, alltoall, dynslice or allpermute operation. Define the

semantics of ğ‘ as a relation

ğ‘
âˆ’â†’ on base offset maps:
ğ‘
âˆ’â†’ T âŸ¦ğœ2âŸ§ğ» iff ğ» âŠ¢ ğ‘ : ğœ1 â†’ ğœ2 .

T âŸ¦ğœ1âŸ§ğ»

(1)

We refer to the base offset map T âŸ¦ğœâŸ§ğ» as a semantic type.

The term semantic type may seem unusual for a definition that does not include the actual values
in hand, i.e. the data in the global array. In principle, the denotation of a type should also include
values: âŸ¦ğœâŸ§ğ» = {(ğ‘£, T âŸ¦ğœâŸ§ğ» ) for ğ‘£ : Rğ‘Ÿğ‘ğ‘›ğ‘˜ (ğœ) }. However, since the data in the global array never
changes during redistribution, we can safely ignore the actual values. For the sake of completeness,
Figure 9 gives the typing and denotation of spmd_op,

4.3 Sequences of collectives in normal form
To implement redistribution between pairs of distributed types, we must generally form sequences
of collective operations. The abstract formulation of this problem is this:5

Easy redistribution: For ğœ1, ğœ2 with ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ1) = ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ2), find a sequence of
ğ‘ğ‘›
collective operations ğ‘ğ‘– such that ğœ1
âˆ’âˆ’â†’ ğœ2.

ğ‘1
âˆ’â†’ Â· Â· Â·

This redistribution problem is an easy one because it has the following simple solution: fully
undistribute the initially distributed array of type ğœ1, so that every device has a full copy of the

4Mathematically speaking, the image of TâŸ¦ğœâŸ§ğ» defines a partitioning of the global data only if TâŸ¦ğœâŸ§ğ» is 1-1; otherwise,
merely a cover is defined.
5To ease notation, we will henceforth write a distributed type ğœ to mean TâŸ¦ğœâŸ§ğ» in places where a semantic type (i.e. the
base offset map that arises from ğœ) is expected. This notational shortcut should not cause any confusion since we are only
interested in base offset maps that are semantic types.

Memory-efficient array redistribution through portable collective communication

11

global data. Then, let every device slice out the local tile of data it needs to retain according to the
desired final type ğœ2. Formally we might write this solution as

allgather(0)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ Â· Â· Â·

allgather(ğ‘›)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ [ğ‘ 0{}ğ‘ 0, . . . , ğ‘ ğ‘› {}ğ‘ ğ‘›]

dynslice(0,...)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ Â· Â· Â·

dynslice(ğ‘›,...)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ2 .

ğœ1

(2)

This sequence suffers from the problem that it includes types, esp. [ğ‘ 0{}ğ‘ 0, . . . , ğ‘ ğ‘› {}ğ‘ ğ‘›], that consume
more memory per device than ğœ1 and ğœ2. The per-device memory footprint of a distributed array is
identical to the ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ of its type. Because of the allgather and dynslice in (2), we have

ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ ( [ğ‘ 0{}ğ‘ 0, . . . , ğ‘ ğ‘› {}ğ‘ ğ‘›]) â‰¥ max(ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1), ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ2) .

(3)

We would in fact expect the opposite of a good solution to the redistribution problem, namely
that all types that appear in the sequence ğœ1 âˆ’â†’âˆ— ğœ2 should have a ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ no greater than the
right-hand side of (3). To formalize this expectation, we define the â„ğ‘’ğ‘–ğ‘”â„ğ‘¡ of a sequence.

Definition 4.4. Let ğœ1 âˆ’â†’âˆ— ğœ2 be a sequence of collective operations, i.e. ğœ1 = ğœ0 âˆ’â†’ ğœ1 âˆ’â†’ Â· Â· Â· âˆ’â†’
ğœğ‘›âˆ’1 âˆ’â†’ ğœğ‘› = ğœ2, with intermediate types ğœ1, . . . , ğœğ‘›âˆ’1. The height ğ”¥ of this sequence is defined as
ğ”¥(ğœ1 âˆ’â†’âˆ— ğœ2) = maxğ‘–=0,...,ğ‘› ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœğ‘– ).

We now formulate the memory-constrained redistribution problem, solutions to which never

consume more per-device memory than the maximum of ğœ1 and ğœ2.

Memory-constrained redistribution: For ğœ1, ğœ2 with ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ1) = ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ2), find
a sequence ğœ1 âˆ’â†’âˆ— ğœ2 such that

ğ”¥(ğœ1 âˆ’â†’âˆ— ğœ2) â‰¤ max(ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1), ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ2)) .
We will solve this memory-constrained problem by successively transforming an arbitrary
sequence ğœ1 âˆ’â†’âˆ— ğœ2 into one that satisfies (4). In fact, it suffices to restrict attention to sequences in
normal form, as defined next.

(4)

Definition 4.5. A sequence ğœ0 â†’âˆ— ğœğ‘› is in normal form if the string formed by its labels ğ‘ğ‘– is

matched by the regular expression dynsliceâˆ—{alltoall | allpermute}âˆ—allgatherâˆ—.

Normal forms are relevant to the memory-constrained redistribution problem because (a) the
initial sub-sequence of dynslice reduces ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’, (b) the intermediate sub-sequence of alltoall
and allpermute does not change ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’, and (c) the final sub-sequence of allgather increases
ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’. It is instructive to draw normal forms in two dimensions, where the vertical direction
indicates the ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ of each distributed type ğœ0, . . . , ğœğ‘› in the sequence:

ğœ0

ğœğ‘›

dynsliceâˆ—

ğœğ‘–

ğœ ğ‘—

allgatherâˆ—

{alltoall | allpermute}âˆ—

From this it is immediately clear that a sequence in normal form satisfies

ğ”¥(ğœ0 âˆ’â†’âˆ— ğœğ‘›) â‰¤ max(ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ0), ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœğ‘›)) ,

i.e. normal forms solve the memory-constrained redistribution problem between types ğœ0, ğœğ‘›.

Moreover, any given sequence ğœ1 âˆ’â†’âˆ— ğœ2 can be brought into normal form by successively

removing peaks. The following lemma formalizes the removal of a single peak.

Lemma 4.6 (Peak Lemma). Given the following sequence:

12

Norman A. Rink, Adam Paszke, Dimitrios Vytiniotis, and Georg Stefan Schmid

allgather(ğ‘–)

ğœ1

dynslice( ğ‘—, ğ‘¥)

ğœ0

ğœ2

There exists a sequence ğœ0 âˆ’â†’3 ğœ2 (where âˆ’â†’ğ‘˜ is the ğ‘˜-step closure of the relation from (1)) such that

ğ”¥(ğœ0 âˆ’â†’3 ğœ2) â‰¤ max(ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ0), ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ2)) .

The proof of Lemma 4.6 is a straightforward case analysis on ğ‘–, ğ‘—, ğ‘¥ and the axis that allgather
operates on. The prove crucially relies on the fact that all axis sizes are prime. Note that the resulting
sequence is only guaranteed to be in âˆ’â†’3, i.e. it may be one step longer than the original peak. This
is a result of the potential need to introduce an allpermute operation for correct bookkeeping of
axes in distributed types.

The next lemma is very similar to Lemma 4.6. It formalizes the moving of rising and falling
edges, which are the initial and final segments, respectively, of broader peaks, or plateaus. The
proof of Lemma 4.7 also relies on axis sizes being prime, it may also need to introduce an additional
allpermute into the constructed sequence, and overall it is analogous to the proof of Lemma 4.6.

Lemma 4.7 (Rising and Falling Edges Lemma). Given one of the sequences on the left, with
ğ‘ âˆˆ {alltoall, allpermute}, we can construct the corresponding sequence on the right, where
ğ‘1, . . . , ğ‘ğ‘Ÿ âˆˆ {alltoall, allpermute}, ğ‘Ÿ â‰¤ 2, and at most one of the ğ‘ğ‘– an alltoall operation:

ğ‘

ğœ1

ğœ2

allgather(ğ‘–)

ğœ0

ğ‘

ğœ0

ğœ1

dynslice

ğœ2

ğ‘1

. . .

ğ‘ğ‘Ÿ

ğœâ€²
1

allgather( ğ‘—)

ğœ2

ğœ0

ğœ0

dynslice

ğœâ€²
1

ğ‘1

. . .

ğ‘ğ‘Ÿ

ğœ2

Lemmas 4.6 and 4.7 can be summarized intuitively as follows: Lemma 4.6 turns a peak into a
flat line or a valley; and Lemma 4.7 either moves a rising edge further to the right or moves a
falling edge further to the left. Repeated application of Lemmas 4.6 and 4.7 to an arbitrary sequence
reaches a fixed point precisely when the sequence is in normal form. Hence the next theorem.

Theorem 4.8 (Normal Form Theorem). For any sequence ğœ0 âˆ’â†’âˆ— ğœğ‘›, there exists a sequence

ğœ0 âˆ’â†’âˆ— ğœğ‘› in normal form.

Normal forms solve the memory-constrained redistribution problem, and the proof of Theorem 4.8
is fully constructive. However, this construction may not yield good redistributions because it does
not control the number of inserted allpermute operations, as the next example illustrates.

Example 4.9. Consider the redistribution problem ğœ1 = [1{ğ‘}8, 8{}8], ğœ2 = [8{}8, 1{ğ‘}8] in
the context of the mesh {ğ‘ : 8}. Clearly alltoall(0, 1) is a solution of the memory-constrained
problem. To solve this problem using Theorem 4.8, we first need to decompose axis ğ‘ into prime
factors, giving the equivalent mesh {ğ‘0 : 2, ğ‘1 : 2, ğ‘2 : 2} and, correspondingly, the new types
ğœ1 = [1{ğ‘0, ğ‘1, ğ‘2}8, 8{}8], ğœ2 = [8{}8, 1{ğ‘0, ğ‘1, ğ‘2}8]. One can then apply Theorem 4.8 to, say, a

Memory-efficient array redistribution through portable collective communication

13

naive sequence ğœ1 âˆ’â†’âˆ— ğœ2 analogous to the one in (2). As a result, one might obtain the sequence

alltoall(0,1)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ [2{ğ‘1, ğ‘2}8, 4{ğ‘0}8]

allpermute
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ [2{ğ‘2, ğ‘1}8, 4{ğ‘0}8]

[1{ğ‘0, ğ‘1, ğ‘2}8, 8{}8]
alltoall(0,1)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ [4{ğ‘1}8, 2{ğ‘2, ğ‘0}8]
alltoall(0,1)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ [8{}8, 1{ğ‘1, ğ‘0, ğ‘2}8]

allpermute
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ [4{ğ‘1}8, 2{ğ‘0, ğ‘2}8]

allpermute
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ [8{}8, 1{ğ‘0, ğ‘1, ğ‘2}8] ,

(5)

which is a lot less efficient than the solution we first guessed, i.e. alltoall(0, 1). To recover this
simpler normal form from the one in (5), we somehow have to remove the allpermute operations
and merge the alltoall operations into a single one. The next section shows how to deal with the
intermediate permutations, and Section 7 shows how to merge collective operations that manipulate
more than one axis in the same dimension.

5 THE WEAK INTERPRETATION OF COLLECTIVES
Whenever ğœ1
bijective map ğœ‹ : ğ» â†’ ğ» .

allpermute
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ2 holds, the maps T âŸ¦ğœ1âŸ§ğ» , T âŸ¦ğœ2âŸ§ğ» are related by a permutation, i.e. a

Lemma 5.1. If ğ» âŠ¢ ğœ1, ğœ2, ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ1) = ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ2) and ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ1) = ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ2),

then there exists a permutation ğœ‹ : ğ» â†’ ğ» such that T âŸ¦ğœ2âŸ§ğ» = T âŸ¦ğœ1âŸ§ğ» â—¦ ğœ‹.

We now decree two base offset maps equivalent if they are related by a permutation.

Definition 5.2 (Equivalence of Base Offset Maps, Weak Semantic Types). Two base offset maps
ğ›½1, ğ›½2 : ğ» â†’ Nğ‘Ÿ are equivalent, in symbols ğ›½1 âˆ¼ ğ›½2, if there exists a permutation ğœ‹ : ğ» â†’ ğ» such
that ğ›½2 = ğ›½1 â—¦ ğœ‹. The relation âˆ¼ is an equivalence relation and we denote the equivalence class of
ğ›½1 as [ğ›½1]âˆ¼. We define EâŸ¦ğœâŸ§ğ» := [T âŸ¦ğœâŸ§ğ» ]âˆ¼, and we refer to EâŸ¦ğœâŸ§ğ» as the weak (semantic) type of
an array of (syntactic) type ğœ.

The relation âˆ¼ makes it easy to remove allpermute operations from sequences of collective
ğ‘
âˆ’â†’ (defined in (1)) by interpreting types not with T âŸ¦Â·âŸ§ğ» but

operations: we modify the relation
with EâŸ¦Â·âŸ§ğ» instead.

Definition 5.3 (Weak Collective Operations). The relation â–¶ is defined on equivalence classes of

base offset maps as follows: For ğ‘ âˆˆ {allgather, alltoall, dynslice},

(6)
We refer to EâŸ¦ğœ1âŸ§ğ» â–¶ğ‘ EâŸ¦ğœ2âŸ§ğ» as a weak collective operation between the distributed types ğœ1
and ğœ2.6

EâŸ¦ğœ1âŸ§ğ» â–¶ğ‘ EâŸ¦ğœ2âŸ§ğ» iff T âŸ¦ğœ1âŸ§ğ»

ğ‘
âˆ’â†’ T âŸ¦ğœ2âŸ§ğ» .

In the previous definition, ğ‘ âˆˆ {allpermute} has been omitted. This is because (6) would define
ğœ‰ â–¶allpermute ğœ’ only for equivalence classes ğœ‰ = ğœ’. Since we are typically interested in the reflexive,
transitive closure â–¶âˆ—, we need not take care to make â–¶ reflexive already. Note, however, that since
there are no â–¶allpermute transitions, no allpermute operations occur in sequences in â–¶âˆ—.

Consider the redistribution problem for weak collectives:

Weak memory-constrained redistribution: For ğœ1, ğœ2 with ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ1) = ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ2),
find a sequence ğœ1 â–¶âˆ— ğœ2 such that

ğ”¥(ğœ1 â–¶âˆ— ğœ2) â‰¤ max(ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1), ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ2))
(where the height of a sequence ğœ1 â–¶âˆ— ğœ2 is defined analogously to the height of ğœ1 âˆ’â†’âˆ— ğœ2).

6As before, we typically write ğœ1 â–¶ ğœ2 instead of EâŸ¦ğœ1âŸ§ğ» â–¶ EâŸ¦ğœ2âŸ§ğ» .

14

Norman A. Rink, Adam Paszke, Dimitrios Vytiniotis, and Georg Stefan Schmid

Lemmas 4.6, 4.7 and Theorem 4.8 also hold for â–¶âˆ—. In fact, their proofs for â–¶âˆ— are noticeably
simpler because fewer cases need be analyzed in the absence of allpermute operations. As before,
Theorem 4.8 for â–¶âˆ— lets us construct solutions of the weak memory-constrained redistribution
problem. However, from the equivalence class EâŸ¦ğœ2âŸ§ğ» at the end of a normal form sequence ğœ1 â–¶âˆ— ğœ2,
we cannot immediately recover the desired base offset map T âŸ¦ğœ2âŸ§ğ» . We only know that T âŸ¦ğœ2âŸ§ğ»
is an element of the equivalence class EâŸ¦ğœ2âŸ§ğ» . Nonetheless, for any representative ğ›½ âˆˆ EâŸ¦ğœ2âŸ§ğ» ,
we can obtain the desired T âŸ¦ğœ2âŸ§ğ» by composing with a suitable permutation ğœ‹ : ğ» â†’ ğ» . We will
use this observation in the next section to find solutions to the (non-weak) redistribution problem
that include at most one allpermute operation.

6 LOW-LEVEL MPI-STYLE COLLECTIVES
ğ‘
Our collective operations ğœ1
âˆ’â†’ ğœ2 are quite abstract. Specifically, they only operate in the context
of a fixed (logical) mesh of devices ğ» . Real-world MPI-style primitives typically allow more fine-
grained control over how individual devices (â€œranksâ€ in MPI parlance) participate in communication.
We now make more control available in our formalism by introducing a set ğ· of physical devices
and a map that mediates between the logical mesh ğ» and this physical ğ·.

Definition 6.1. A device map ğœ™ is a bijection ğœ™ : ğ» â†’ ğ·. A device assignment is a pair âŸ¨ğœ™, ğ›½âŸ©ğ» ,
where ğœ™ : ğ» â†’ ğ· is a device map and ğ›½ is a base offset map. The device assignment âŸ¨ğœ™, ğ›½âŸ©ğ» assigns
a tile of an array to each device ğ‘‘ âˆˆ ğ· by mapping ğ‘‘ to the base offset ğ›½ â—¦ ğœ™ âˆ’1(ğ‘‘).

1 = ğ›½2 â—¦ ğœ™ âˆ’1
2 .

Definition 6.2. Device assignments âŸ¨ğœ™1, ğ›½1âŸ©ğ»1 , âŸ¨ğœ™2, ğ›½2âŸ©ğ»2 are equivalent if ğ›½1 â—¦ ğœ™ âˆ’1
Figure 10 specifies low-level MPI-style collectives that operate on device assignments. The key
difference between â‡’ and âˆ’â†’ is that device maps ğœ™, ğœ™ â€² now appear in the rules in Figure 10.
These device maps give additional flexibility to the rules allgather and alltoall, which can now
operate on an axis ğ‘¥ that need not be in the left-most position within its distributed dimension. The
precondition ğ›½ â€² â—¦ ğœ™ â€²âˆ’1 = ğ›½ â—¦ ğœ™ âˆ’1 should be read as a definition of a new device map ğœ™ â€² : ğ» â†’ ğ·.
With respect to this device map, the distributed array whose original type was given by ğ›½ now
has the type ğ›½ â€². Note how ğ›½ â€² has the right form for our old collective operations allgather and
alltoall (in âˆ’â†’) to operate on. The corresponding operations in Figure 10 then produce the new
type ğ›½ â€²â€², but now with respect to ğœ™ â€².

The device map ğœ™ â€² in the rules allgather and alltoall is precisely what enables us to apply
these rules also to types where we would have to perform a permutation first if we wanted to apply
the corresponding T-AllGather and T-AllToAll rules from Figure 8. By changing the original
device map ğœ™ to ğœ™ â€² in Figure 10, the permutation operation can be skipped. Note that no data needs
to move in going from âŸ¨ğœ™, ğ›½âŸ©ğ» to âŸ¨ğœ™ â€², ğ›½ â€²âŸ©ğ» because of the requirement ğ›½ â€² â—¦ ğœ™ â€²âˆ’1 = ğ›½ â—¦ ğœ™ âˆ’1 in the
preconditions of allgather and alltoall.

6.1 Lowering of weak collective operations

ğ‘
âˆ’â†’ ğœ2 can be implemented by the collectives in Figure 10
Our original collective operations ğœ1
simply by setting ğ›½ â€² = ğ›½, which will entail ğœ™ â€² = ğœ™. It is more interesting to look at how weak
collectives in â–¶ are lowered to the operations from Figure 10. Since â–¶ relates equivalence classes of
base offset maps, lowering requires us to specify an explicit initial ğ›½. Moreover, since a device map
ğœ™ is also required for the rules in Figure 10, we will in fact lower â–¶ in the presence of a given initial
device assignment âŸ¨ğœ™, ğ›½âŸ©ğ» .

The following lemma is established by mapping every transition â–¶ğ‘ to the corresponding â‡’ğ‘

from Figure 10.

Memory-efficient array redistribution through portable collective communication

15

ğ‘¥ : ğ‘› âˆˆ ğ» ğ›½ = TâŸ¦[. . . , ğ‘ğ‘– {ğ‘¦ğ‘–, ğ‘¥, ğ‘¥ğ‘– }ğ‘ ğ‘–, . . .]âŸ§ğ» ğ›½â€² = TâŸ¦[. . . , ğ‘ğ‘– {ğ‘¥, ğ‘¦ğ‘–, ğ‘¥ğ‘– }ğ‘ ğ‘–, . . .]âŸ§ğ»

ğ›½â€² â—¦ ğœ™â€²âˆ’1 = ğ›½ â—¦ ğœ™ âˆ’1

ğ›½â€²â€² = TâŸ¦[. . . , (ğ‘ğ‘– Â· ğ‘›) {ğ‘¦ğ‘–, ğ‘¥ğ‘– }ğ‘ ğ‘–, . . .]âŸ§ğ»

âŸ¨ğœ™, ğ›½ âŸ©ğ» â‡’allgather(ğ‘¥ ) âŸ¨ğœ™â€², ğ›½â€²â€²âŸ©ğ»

allgather

ğ‘’ğ‘– = ğ‘ğ‘– {ğ‘¦ğ‘–, ğ‘¥, ğ‘¥ğ‘– }ğ‘ ğ‘–

ğ‘’â€² = ğ‘’ [ğ‘– â†¦â†’ ğ‘’â€²
ğ‘’â€²
ğ‘– = ğ‘ğ‘– {ğ‘¥, ğ‘¦ğ‘–, ğ‘¥ğ‘– }ğ‘ ğ‘–
ğ‘– ]
ğ‘’â€²â€²
ğ‘— = (ğ‘ ğ‘— ğ‘‘ğ‘–ğ‘£ ğ‘›) {ğ‘¥, ğ‘¥ ğ‘— }ğ‘  ğ‘—

ğ‘’â€²â€²
ğ‘– = (ğ‘ğ‘– Â· ğ‘›) {ğ‘¦ğ‘–, ğ‘¥ğ‘– }ğ‘ ğ‘–

ğ‘’ ğ‘— = ğ‘ ğ‘— {ğ‘¥ ğ‘— }ğ‘  ğ‘—

ğ‘’â€²â€² = ğ‘’ [ğ‘– â†¦â†’ ğ‘’â€²â€²

ğ‘¥ : ğ‘› âˆˆ ğ» ğ‘ ğ‘— ğ‘šğ‘œğ‘‘ ğ‘› = 0
ğ‘– , ğ‘— â†¦â†’ ğ‘’â€²â€²
ğ‘— ]

ğ›½ = TâŸ¦ğ‘’âŸ§ğ» ğ›½â€² = TâŸ¦ğ‘’â€²âŸ§ğ» ğ›½â€² â—¦ ğœ™â€²âˆ’1 = ğ›½ â—¦ ğœ™ âˆ’1

ğ›½â€²â€² = TâŸ¦ğ‘’â€²â€²âŸ§ğ»

âŸ¨ğœ™, ğ›½ âŸ©ğ» â‡’alltoall(ğ‘¥,ğ‘— ) âŸ¨ğœ™â€², ğ›½â€²â€²âŸ©ğ»

alltoall

ğ‘¥ : ğ‘› âˆˆ ğ» ğ‘¥ âˆ‰ ğ‘ğ‘¥ğ‘’ğ‘  ( [. . . , (ğ‘ğ‘– Â· ğ‘›) {ğ‘¥ğ‘– }ğ‘ ğ‘–, . . .])
ğ›½ = TâŸ¦[. . . , (ğ‘ğ‘– Â· ğ‘›) {ğ‘¥ğ‘– }ğ‘ ğ‘–, . . .]âŸ§ğ»
ğ›½â€² = TâŸ¦[. . . , ğ‘ğ‘– {ğ‘¥, ğ‘¥ğ‘– }ğ‘ ğ‘–, . . .]âŸ§ğ»
âŸ¨ğœ™, ğ›½ âŸ©ğ» â‡’dynslice(ğ‘–,ğ‘¥ ) âŸ¨ğœ™, ğ›½â€²âŸ©ğ»

dynslice

ğ›½â€² â—¦ ğœ™â€²âˆ’1 = ğ›½ â—¦ ğœ™ âˆ’1 â—¦ ğœ‹
ğœ‹ : ğ· â†’ ğ· a bijection

âŸ¨ğœ™, ğ›½ âŸ©ğ» â‡’allpermute âŸ¨ğœ™â€², ğ›½â€²âŸ©ğ» â€²

permute

Fig. 10. Semantics of low-level collective operations as a relation on device assignments.

Lemma 6.3 (Lowering Lemma). Let ğœ1 â–¶ğ‘ ğœ2, ğ‘ âˆˆ {allgather, alltoall, dynslice}. Further let

ğ›½ âˆˆ EâŸ¦ğœ1âŸ§ğ» , and let ğœ™ : ğ» â†’ ğ· be a device map. Then,

âŸ¨ğœ™, ğ›½âŸ©ğ» â‡’ğ‘ âŸ¨ğœ™ â€², ğ›½ â€²âŸ©ğ»

with ğ›½ â€² âˆˆ EâŸ¦ğœ2âŸ§ğ» .

Repeatedly applying Lemma 6.3 lets us lower a sequence in â–¶âˆ— to â‡’âˆ—. The fact that the resulting
sequence in â‡’âˆ— contains no allpermute operations is then a direct consequence of there not being
any allpermute operations in â–¶.

Theorem 6.4 (Lowering Theorem). Given ğœ1 â–¶âˆ— ğœ2, ğ›½ âˆˆ EâŸ¦ğœ1âŸ§ğ» and a device map ğœ™ : ğ» â†’ ğ·.
There exists a sequence âŸ¨ğœ™, ğ›½âŸ©ğ» â‡’âˆ— âŸ¨ğœ™ â€², ğ›½ â€²âŸ©ğ» with ğ›½ â€² âˆˆ EâŸ¦ğœ2âŸ§ğ» , and the sequence includes no
allpermute transitions.

With Theorem 6.4 we can now turn any sequence ğœ1 âˆ’â†’âˆ— ğœ2 into a sequence âŸ¨ğœ™, T âŸ¦ğœ1âŸ§ğ» âŸ©ğ» â‡’âˆ—
âŸ¨ğœ™, T âŸ¦ğœ2âŸ§ğ» âŸ©ğ» that includes at most one permutation (at the end). To do this, first pass from
ğœ1 âˆ’â†’âˆ— ğœ2 to ğœ1 â–¶âˆ— ğœ2, and then apply Theorem 6.4 with ğ›½ = T âŸ¦ğœ1âŸ§ğ» . This yields a sequence
âŸ¨ğœ™, ğ›½âŸ©ğ» â‡’âˆ— âŸ¨ğœ™ â€², ğ›½ â€²âŸ©ğ» with ğ›½ â€² âˆˆ EâŸ¦ğœ2âŸ§ğ» . There then exists a permutation ğœŒ : ğ» â†’ ğ» such that
ğ›½ â€² = T âŸ¦ğœ2âŸ§ğ» â—¦ ğœŒ. Now, define a permutation ğœ‹ = ğœ™ â€² â—¦ ğœŒ âˆ’1 â—¦ ğœ™ âˆ’1 : ğ· â†’ ğ·. Since

we can apply rule allpermute from Figure 10 to get

T âŸ¦ğœ2âŸ§ğ» â—¦ ğœ™ âˆ’1 = ğ›½ â€² â—¦ ğœ™ â€²âˆ’1 â—¦ ğœ‹ ,

âŸ¨ğœ™, ğ›½âŸ©ğ» â‡’âˆ— âŸ¨ğœ™ â€², ğ›½ â€²âŸ©ğ» â‡’allpermute âŸ¨ğœ™, T âŸ¦ğœ2âŸ§ğ» âŸ©ğ» ,

(7)

(8)

where the sub-sequence âŸ¨ğœ™, ğ›½âŸ©ğ» â‡’âˆ— âŸ¨ğœ™ â€², ğ›½ â€²âŸ©ğ» includes no allpermute transitions (by Theorem 6.4).
The observation that at most one permutation is required at the end of a sequence allows us to
give a low upper bound for the cost of normal form sequences. To discuss cost, however, we first
need to introduce a cost model.

6.2 Cost model based on data transfers
We model the cost of collective operations in terms of the amount of data that is transferred. This
amount is proportional to the number of devices that communicate, which we have therefore

16

Norman A. Rink, Adam Paszke, Dimitrios Vytiniotis, and Georg Stefan Schmid

ğ‘ğ‘–
âˆ’âˆ’â†’ ğœğ‘–+1)

ğ‘ğ‘œğ‘ ğ‘¡

(cid:16)
ğœ0

ğ‘ğ‘œğ‘ ğ‘¡ (ğœ1
ğ‘ğ‘œğ‘ ğ‘¡ (ğœ1
ğ‘ğ‘œğ‘ ğ‘¡ (ğœ1
ğ‘ğ‘œğ‘ ğ‘¡ (ğœ1

(cid:17)

ğ‘ğ‘›âˆ’1
âˆ’âˆ’âˆ’âˆ’â†’ ğœğ‘›

ğ‘0
âˆ’âˆ’â†’ Â· Â· Â·
allpermute
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ2)
dynslice
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ2)
allgather
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ2)
alltoall
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ2)

= (cid:205)ğ‘›âˆ’1

ğ‘–=0 ğ‘ğ‘œğ‘ ğ‘¡ (ğœğ‘–
= ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1)
= 0
= ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ2)
= ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1)

Fig. 11. Costs derived from the total number of bytes communicated, normalized to the number of devices.
Derivations appear in Appendix B.

factored out of the ğ‘ğ‘œğ‘ ğ‘¡ defined in Figure 11. Although the definitions in Figure 11 are written out
for ğœ1 âˆ’â†’âˆ— ğœ2, they equally apply also to ğœ1 â–¶âˆ— ğœ2 and âŸ¨ğœ™, ğ›½âŸ©ğ» â‡’âˆ— âŸ¨ğœ™ â€², ğ›½ â€²âŸ©ğ» .

Given the cost model, we can state the most specific version of our redistribution problem:

Memory-constrained optimal-cost redistribution: For ğœ1, ğœ2 with ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ1) = ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ2),
find ğ‘  : âŸ¨ğœ™, T âŸ¦ğœ1âŸ§ğ» âŸ©ğ» â‡’âˆ— âŸ¨ğœ™, T âŸ¦ğœ2âŸ§ğ» âŸ©ğ» with

ğ”¥(ğ‘ ) â‰¤ max(ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1), ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ2))

and such that ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘ ) is minimal.

The number of sequences that must be considered as candidate solutions to this problem is finite:
there are only finitely many distributed types ğœ with ğ» âŠ¢ ğœ and given ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ). Sequences
which include loops need not be considered since they cannot have minimum cost. Note that no
loop can be formed only with the zero-cost dynslice operations. Finiteness implies that there exists
a solution that is optimal with respect to ğ‘ğ‘œğ‘ ğ‘¡. We will now show that the theory established in the
previous sections finds nearly optimal solutions.

We first note that Lemmas 4.6 and 4.7 interact well with our cost model. Specifically, the trans-
formation of a peak (or plateau) never yields a sequence of greater cost than the original peak (or
plateau). Hence, we arrive at the following statement about normal forms.

Lemma 6.5. For any sequence ğ‘  : ğœ1 â–¶âˆ— ğœ2 between ğœ1 and ğœ2, there exists a sequence ğ‘ nf : ğœ1 â–¶âˆ— ğœ2 in

normal form such that ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘ nf) â‰¤ ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘ ).

Lemma 6.5 does not say that normal forms are optimal with respect to our cost model, but it
implies that there exists a normal form sequence (in â–¶âˆ—) of optimal cost. To use Lemma 6.5 in the
context of a sequence âŸ¨ğœ™, T âŸ¦ğœ1âŸ§ğ» âŸ©ğ» â‡’âˆ— âŸ¨ğœ™ â€², T âŸ¦ğœ2âŸ§ğ» âŸ©ğ» , we first need to lift such a sequence to
â–¶âˆ—:7

Lemma 6.6 (Lifting Lemma). Given a sequence

ğ‘  : âŸ¨ğœ™, T âŸ¦ğœ1âŸ§ğ» âŸ©ğ» â‡’ğ‘1 Â· Â· Â· â‡’ğ‘ğ‘š âŸ¨ğœ™ â€², T âŸ¦ğœ2âŸ§ğ» âŸ©ğ» .
We can construct a sequence ğ‘ ğ‘¤ : ğœ1 â–¶ ğ‘1 Â· Â· Â· â–¶ğ‘ğ‘› ğœ2 where the ğ‘ğ‘– are precisely those ğ‘ ğ‘— that are not
allpermute operations.

Lemma 6.6 is proven by mapping every transition â‡’ğ‘ ğ‘— where ğ‘ ğ‘— is not an allpermute operation
to the corresponding transition â–¶ğ‘ . This works because of the preconditions ğ›½ â€² â—¦ ğœ™ â€²âˆ’1 = ğ›½ â—¦ ğœ™ âˆ’1
in Figure 10, which imply ğ›½ â€² âˆ¼ ğ›½.

We can now establish the main result of our formal work.

7The terminology of lifting to â–¶âˆ— may seem counter-intuitive given that the relation â–¶ is a quotient: one might expect lifting
to proceed from the quotient. We use the the term lifting here to mean the inverse process of the lowering from Theorem 6.4,
where lowering carries its usual meaning in the context of code generation as â‡’ describes lower-level operations than â–¶.

Memory-efficient array redistribution through portable collective communication

17

Theorem 6.7 (Near Optimality). Let

ğ‘  : âŸ¨ğœ™, T âŸ¦ğœ1âŸ§ğ» âŸ©ğ» â‡’âˆ— âŸ¨ğœ™, T âŸ¦ğœ2âŸ§ğ» âŸ©ğ»
be a sequence in â‡’âˆ— of minimal ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘ ). There exists a sequence ğ‘  â€² : âŸ¨ğœ™, T âŸ¦ğœ1âŸ§ğ» âŸ©ğ» â‡’âˆ— âŸ¨ğœ™, T âŸ¦ğœ2âŸ§ğ» âŸ©ğ»
such that ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘  â€²) â‰¤ ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘ ) + ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ2) and ğ‘  â€² is in normal form except for possibly a single
allpermute as its last step.

To prove Theorem 6.7, first lift ğ‘  to a sequence in â–¶âˆ—. Then, obtain a normal form sequence
using Lemma 6.5. Finally, apply Theorem 6.4 and the reasoning that led to (8) to arrive at the desired
sequence ğ‘  â€².

We illustrate this result by taking another look at Example 4.9, which highlighted the problem

that sequences of collective operations may include extraneous permutations.

Example 6.8. Consider the redistribution problem from Example 4.9 over {ğ‘0 : 2, ğ‘1 : 2, ğ‘2 : 2}.

Theorem 6.7 may yield the solution

âŸ¨ğœ™, [1{ğ‘0, ğ‘1, ğ‘2}8, 8{}8]âŸ©ğ» â‡’alltoall âŸ¨ğœ™1, [2{ğ‘1, ğ‘2}8, 4{ğ‘0}8]âŸ©ğ»

â‡’alltoall âŸ¨ğœ™2, [4{ğ‘2}8, 2{ğ‘1, ğ‘0}8]âŸ©ğ» â‡’alltoall âŸ¨ğœ™3, [8{}8, 1{ğ‘2, ğ‘1, ğ‘0}8]âŸ©ğ»
â‡’allpermute âŸ¨ğœ™, [8{}8, 1{ğ‘0, ğ‘1, ğ‘2}8]âŸ©ğ» .

(9)

Of the three allpermute operations in (5) only one remains in (9), and it appears at the end. Note
that Theorem 6.4 does not control which base offset map in EâŸ¦ğœ2âŸ§ğ» is obtained after lowering
ğœ1 â–¶âˆ— ğœ2 to â‡’âˆ—. This means that moving permutations to the end may alternatively lead to

âŸ¨ğœ™, [1{ğ‘0, ğ‘1, ğ‘2}8, 8{}8]âŸ©ğ» â‡’alltoall âŸ¨ğœ™1, [2{ğ‘0, ğ‘1}8, 4{ğ‘2}8]âŸ©ğ»

â‡’alltoall âŸ¨ğœ™2, [4{ğ‘0}8, 2{ğ‘1, ğ‘2}8]âŸ©ğ» â‡’alltoall âŸ¨ğœ™3, [8{}8, 1{ğ‘0, ğ‘1, ğ‘2}8]âŸ©ğ»
â‡’allpermute âŸ¨ğœ™, [8{}8, 1{ğ‘0, ğ‘1, ğ‘2}8]âŸ©ğ» .

(10)

If it then happens to be the case that ğœ™3 = ğœ™, the final allpermute operation is in fact not needed.

It is worth stressing that, apart from Lemma 6.5, the results in the present section do not depend
on the details of our cost model. Analogous results can be derived for any other choice of cost
model so long as one takes care to separately track the cost of allpermute operations.

7 FINDING NEARLY-OPTIMAL REDISTRIBUTIONS
We know now that (i) sequences in normal form solve the memory-constrained redistribution
problem; (ii) we can eliminate all intermediate allpermute collective operations in favour of at
most a single allpermute operation at the end of a sequence; and (iii) there exist sequences that are
within a bound of the minimal cost and are in normal form except for a potential final allpermute
(which does not increase per-device memory consumption). Looking back at Examples 4.9 and 6.8,
our central result Theorem 6.7 may construct needlessly long sequences for a redistribution as
simple as [1{ğ‘}8, 8{}8] â‡ [8{}8, 1{ğ‘}8]. This is a consequence of our requirement that axes be
decomposed into axes of prime sizes. We now show how to find near-optimal sequences and how
to merge (sub-)sequences of the same kind of operation into a single one. The decomposition of
axes into prime factors in fact allows us to resolve both issues simultaneously.

7.1 Merging collective operations
The rules in Figure 10 only allow for transfers of a single axis ğ‘¥ at a time, with the exception of
the allpermute rule. To enable merging of collective operations, e.g. the alltoalls in (9) or (10), we
need more general allgather, alltoall and dynslice rules that can act on multiple axes at once.

18

Norman A. Rink, Adam Paszke, Dimitrios Vytiniotis, and Georg Stefan Schmid

This is straightforward: the generalized multi-axes rules are obtained by replacing the single axis ğ‘¥
in allgather, alltoall and dynslice with a sequence of axes ğ‘¥.8

The multi-axes rules are well within the capabilities of distributed systems that provide MPI-
style collectives; and the cost model from Figure 11 applies verbatim to the multi-axes collectives.
What is more remarkable, but largely trivial to show, is that our results from Sections 4â€“6 carry
over to collectives that act on multiple axes simultaneously. In particular, weak collectives are
introduced completely analogously, and lowering from weak to low-level MPI-style collectives
can be used as before to move allpermute operations to the end of a sequence. Hence, our central
result, Theorem 6.7, holds also for sequences of multi-axes collectives.

The multi-axes rules clearly enable merging of operations: whenever a collective operation that
acts on axis ğ‘¥ appears next to an operation of the same kind that acts on axis ğ‘¦, a single instance of
the same operation can be used to operate on both axes ğ‘¥, ğ‘¦ simultaneously. While semantically
equivalent, using different numbers of collectives of the same kind may incur different costs.

Example 7.1. Consider sequence (10). Its cost is 3 Â· 8 for the three allgather operations (plus
another 8 if the final allpermute is required). In the presence of multi-axes collectives, sequence (10)
is not the nearly-optimal solution that is guaranteed to exist by Theorem 6.7: the cost of

âŸ¨ğœ™, [1{ğ‘0, ğ‘1, ğ‘2}8, 8{}8]âŸ©ğ» â‡’alltoall âŸ¨ğœ™3, [8{}8, 1{ğ‘0, ğ‘1, ğ‘2}8]âŸ©ğ»

â‡’allpermute âŸ¨ğœ™, [8{}8, 1{ğ‘0, ğ‘1, ğ‘2}8]âŸ©ğ»

is only 8 for the single allgather operation (plus another 8 if the final allpermute is required).

7.2 Redistribution as a shortest path problem
We obtain near-optimal solutions of the memory-constrained redistribution problem by phrasing
this problem as a shortest path search in a graph with weighted edges. From Section 6 we know that
near-optimal sequences can be obtained by passing to weak semantic types and weak collectives.
Based on this knowledge, we perform a shortest path search in the graph whose nodes are weak
types EâŸ¦ğœâŸ§ğ» , with fixed ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ). The edges in the graph are the weak collectives that define
the relation â–¶, and their weights are assigned according to our cost model from Figure 11.

Having graph nodes represent weak types facilitates a simple encoding of nodes: it suffices to
store only ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ) since, with fixed ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ), only ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ (ğœ) is needed to identify the
equivalence class EâŸ¦ğœâŸ§ğ» . This has the convenient side effect of avoiding a combinatorial explosion
that would result from tracking all possible permutations of axes in distributed dimensions. We
further limit the nodes in our graph to only those types ğœ for which ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ) is bounded
according to (4) because we are interested in solving memory-constrained redistribution problems.
Graph edges correspond to transitions in the weak relation â–¶ for multi-axes collectives, i.e. mul-
tiple axes can be transferred along an edge. We run the shortest-path search with axes that have
been fully decomposed into prime factors: if each axis corresponds to a prime factor, the multi-axes
collectives will give rise to graph edges for all possible transfers of (non-prime) factors between
distributed dimensions. In other words, our search space is not artificially constrained.

Since our search procedure has access to graph edges that can transfer multiple axes at once, we
know that a shortest path will necessarily contain only maximally merged allgather and alltoall

8There is an alternative route to the multi-axes rules via another mild generalization of the low-level collectives from
Figure 10: simply allow ğ» to vary across â‡’. Varying ğ» as in âŸ¨ğœ™, ğ›½ âŸ©ğ» â‡’ âŸ¨ğœ™â€², ğ›½â€²âŸ©ğ» â€² is no challenge for real systems since
ğ» , ğ» â€² have no physical meaning: only the combined maps ğ›½ â—¦ ğœ™ âˆ’1, ğ›½â€² â—¦ ğœ™â€²âˆ’1 : ğ· â†’ Nğ‘Ÿ are physically significant. If ğ» is
allowed to vary, then the multi-axes rules can be implemented on top of the ones from Figure 10 by passing to a new mesh
ğ» â€² with a single axis ğ‘¥ that has been formed by merging the ğ‘¥ from ğ» .

Memory-efficient array redistribution through portable collective communication

19

[1{ğ‘¦, ğ‘¥ }8, 8, 8, 4]
[2{ğ‘¥ }8, 4{ğ‘¦ }8, 8, 4]
[8, 4{ğ‘¦ }8, 2{ğ‘¥ }8, 4]

alltoall(0, 1), cost 256
alltoall(0, 2), cost 256

[1{ğ‘¦, ğ‘¥ }8, 8, 8, 4]
[1{ğ‘¦, ğ‘¥ }8, 8, 8, 1{ğ‘§ }4]
[2{ğ‘¥ }8, 4{ğ‘¦ }8, 8, 1{ğ‘§ }4]
[8, 4{ğ‘¦ }8, 2{ğ‘¥ }8, 1{ğ‘§ }4]
[8, 4{ğ‘¦ }8, 2{ğ‘¥ }8, 4]

dynslice(3, ğ‘§), cost 0
alltoall(0, 1), cost 64
alltoall(0, 2), cost 64
allgather(3), cost 256

combined cost 512

combined cost 384

Fig. 12. Redistribution over the mesh {ğ‘¥ : 4, ğ‘¦ : 2, ğ‘§ : 4} without over-partitioning (left) and with over-
partitioning (right).

operations. This may not be the case for dynslice operations since they have zero cost, which
means that having non-merged dynslice operations in a sequence is no disadvantage.

Over-partitioning due to dynslice operations having cost zero. The fact that dynslice operations
are assigned zero cost has another interesting consequence. If replicated axes are available in the
mesh during the redistribution process, it may be beneficial to additionally partition the data that is
being redistributed along these axes. This does not affect the semantics of the overall redistribution,
but can have the effect of lowering the cost of intermediate alltoall operations. An example
appears in Figure 12, where ğ‘§ plays the role of the additionally available replicated axes. We refer
to this effect as over-partitioning, and we have confirmed that over-partitioning does occur and can
indeed outperform sequences without over-partitioning, but we leave a more detailed analysis to
future work.

7.3 Optimizations for (single) permutations
Theorem 6.7 guarantees optimality up to at most a single permutation. In practice, whether this
permutation occurs and where it occurs in a sequence of collectives affects performance. We address
this with two optimizations:

â€¢ If an allpermute is required at the end of our near-optimal sequence, we move this allpermute
forward past any final allgather operations in the shortest path (if there are any). This makes
the permutation cheaper due to smaller tile sizes.

â€¢ When lowering the shortest path in â–¶âˆ— to â‡’âˆ— (cf. Theorem 6.4), we need to concretize
intermediate (semantic) types ğœ â€² (equivalently: base offset maps ğ›½ â€² âˆˆ EâŸ¦ğœ â€²âŸ§ğ» ). We do this by
picking types ğœ â€² based on the axes that appear in the final type ğœ2.

To give an intuition for the latter optimization, consider the mesh {"x": 4, "y": 4} and the problem
of redistributing [16] to [4{"x"}16]. Our search algorithm will (correctly) pick a dynslice but could
accidentally lower by picking axis "y", resulting in the need to fix up with a permutation. (Note that
the permutation is not a graph edge and hence it is hard to attribute a cost to it in our shortest-path
search.) However, if we bias the algorithm to pick axis "x" from the final type, then this permutation
can be elided.

8 EVALUATION
Based on the shortest path search, we have implemented a redistribution program synthesizer to
answer the following research questions:

RQ1. Can the shortest path search synthesize redistributions within an acceptable time budget?

RQ2. How do our synthesized programs perform in comparison to redistributions generated by

state-of-the-art tools?

20

Norman A. Rink, Adam Paszke, Dimitrios Vytiniotis, and Georg Stefan Schmid

RQ3. How effective is our simple cost model at driving the shortest path search towards good

solutions of redistribution problems?

To answer these questions, we have randomly sampled 1000 redistribution problems with global
array sizes of 64MBâ€“800MB. The sampled problems use 3 mesh axes and up to 6D arrays, which
are common parameters for arrays in deep learning applications. Each axis is randomly chosen
either to be replicated or to partition one of the dimensions.

With the sampled problems we were able to answer RQ1 in the affirmative: a non-optimized
Python-based implementation of the shortest path search synthesizes a redistribution program for
each of the sampled problems in under a second. This is certainly acceptable in the context of PartIR
where the bottleneck of code generation is the search for partitioning strategies (cf. Section 2),
typically taking minutes. We attribute the good performance of our search-based synthesizer to the
elision of intermediate permutations, avoiding a combinatorial explosion, and the fact that typical
redistribution problems use only a few axes and dimensions.

To address RQ2, we compared with the SPMD partitioner built into XLA [34], which is a widely
used compiler for ML workloads that features automatic partitioning utilities and hence is capable
of generating redistribution programs. According to the XLA source code9, the redistribution
algorithm is based on a set of carefully hand-crafted heuristics (that attempt, e.g., to synthesize
alltoall sequences or to detect cases directly implementable via allpermute) with a fallback to
allgather and dynslice (analogous to (2)).

For the sampled redistribution problems, we have benchmarked programs generated both with
our shortest path search and with XLA on a system of 8 Google TPU devices [14]. Figure 14
displays the results, which suggest that for small redistribution problems our method yields largely
equivalent performance to the XLA SPMD partitioner, but we can also demonstrate fairly significant
improvements. Across all sampled problems our method attains a geometric mean speedup of 1.22Ã—.
Big speedups of up to 5.7Ã—, cf. Figure 14b, occur when XLA falls back to an expensive allgather
followed by dynslice. Our method never issues these expensive allgather operations as they are
also memory inefficient.

For a few small redistribution problems the XLA heuristics outperform our method by up to
1.6Ã—. The problems for which our method exhibits the biggest slowdowns compared to XLA are
listed in Figure 13. A detailed analysis reveals that for these small transfers, dominated by latency,
allgather followed by dynslice is faster (P1â€“P3) than the redistributions found by our search,
which generates dynslice, alltoall and allpermute collectives to guarantee memory efficiency.
Problem P4 is more interesting: XLA solves this by performing an allpermute and single allgather
on a leading dimension to create an array of ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘¡ğ‘¦ğ‘ğ‘’ [4,8,16,16,8,16,8]. Through a sequence
of local transpose and reshape operations this is transformed into the desired target type. Exploiting
local reshapes like this, particularly for small transfers, would allow us to fuse allgather operations
and is an interesting optimization that we could additionally apply. The complication is that
reasoning about the cost of local reshapes is not straightforward as it may involve data re-layout
and is backend-dependent.

Figure 14a and the previous discussion around Figure 13 also let us answer RQ3: for larger
redistributions, where the cost of data transfers dominates latency, our cost model clearly drives
the shortest path search to efficient solutions. In the future, we may consider enhancing our cost
model with latency characteristics and exploring the possibility of trading memory efficiency for
lower run-time.

9https://github.com/tensorflow/tensorflow/blob/8c6d9ae2b497ac99ec0b5a4a9a537d4f66e5e678/tensorflow/compiler/xla/
service/spmd/spmd_partitioner.cc

Memory-efficient array redistribution through portable collective communication

21

P1
P2
P3
P4

Source type
[360, 184{c:2}368, 320]
[80, 40{c:2}80, 72, 64]
[296, 360, 156{c:2}312]
[8{c:2}16, 16, 16, 8{a:2}16, \
16, 8{b:2}16]

Target type
[90{c:2,a:2}360, 368, 160{b:2}320]
[40{b:2}80, 80, 36{c:2}72, 64]
[74{b:2,c:2}296, 180{a:2}360, 312]
[16, 16, 16, 16, 16, 8{a:2}16]

Global Size
162MB
113MB
127MB
64MB

Slowdown
1.6Ã—
1.6Ã—
1.5Ã—
1.5Ã—

Fig. 13. Redistribution problems where our approach leads to the biggest slowdowns when compared with
the XLA SPMD partitioner.

(a) Run-times: Points correspond to individual redis-
tribution problems. Along the diagonal both meth-
ods are equally fast. Points below the diagonal: our
method outperforms the XLA SPMD partitioner.

(b) Distribution of ratios of run-times. Values larger
than 1.0 (to the right of the vertical dashed line) cor-
respond to our search procedure yielding faster pro-
grams than the XLA SPMD partitioner.

Fig. 14. Comparison of redistribution programs synthesized by our search procedure and by the XLA SPMD
partitioner.

9 DISCUSSION
We have established strong theoretical efficiency results for array redistribution when implemented
with portable MPI-style collective operations. We have also confirmed experimentally that our theo-
retical results translate into implementations of redistribution that yield good run-time performance,
while guaranteeing memory-efficiency too.

As we discuss in Section 10, previous research on redistributing arrays typically employs individ-
ual device-to-device transfers, and our distributed type semantics in terms of base offset maps could
readily be used to calculate such transfers. Instead, our work builds on collective operations for
several reasons: (1) collective operations provide a simple set of primitives to use in the context of
SPMD computations; (2) synthesizing sequences of collectives makes our work portable: instead of
optimizing schedules of data transfers for different target platforms, we rely on already optimized
collectives; (3) using collectives is the path taken by the production XLA SPMD implementation;
(4) as demonstrated in this paper, collectives can be typed using distributed types, which provide
an intuitive and firm framework for reasoning about correctness. However, collective communi-
cation comes with the downside of global barriers where all devices have to synchronize, even
if not all of them need to exchange data (e.g. in a permutation spanning only a few devices). A
direct quantitative comparison between our approach based on collectives and more conventional
device-to-device transfers is not meaningful in isolation since it depends strongly on the amount

051015202530354045XLA SPMD [ms]051015202530354045Ours [ms]0.61.01.41.72.12.52.83.23.63.94.34.65.05.45.7XLA SPMD / Ours (ratio)050100150200250300350400Count22

Norman A. Rink, Adam Paszke, Dimitrios Vytiniotis, and Georg Stefan Schmid

of exploitable asynchrony in a system, a topic that we will likely revisit as our PartIR:SPMD and
PartIR ecosystem matures.

10 RELATED WORK

Partitioning for machine learning (ML). The increasing scale of ML models has led to the creation
of automated partitioners aimed at exposing model parallelism. Systems developed in the context
of TensorFlow and XLA [16, 24, 34] emit SPMD programs, and could directly benefit from the
redistribution techniques described in this paper. Popular ML frameworks such as PyTorch [21]
and JAX [3] expose MPI collectives to their users, allowing them to explicitly program in the SPMD
model, though JAX goes one step further and provides higher-level programming abstractions such
as xmap(). The latest instantiation of XLA-based partitioning, the GSPMD partitioner [34], optimizes
certain patterns of redistributions automatically, but leaves significant room for improvement, in
particular for larger redistributions (as shown in Section 8).

DistIR [23] is an automated partitioner for the MPMD model. FlexFlow [13] automatically
partitions DNNs and executes the resulting graphs using the Legion runtime [2]. Legate NumPy [1]
also builds on the Legion runtime and targets NumPy programming. When redistribution is required,
systems like Legate or FlexFlow pass information to the Legion runtime about which tiles are
needed on each processor for the next task, and Legion works out the minimum amount of data that
needs to be sent from the source processors and schedules individual memory transfers for each
tile. The HeAT system [11] also includes array partitioning specifications similar to our distributed
types. Interestingly, HeAT does use an MPI-syle collectives interface like allgather to implement
resplit() (redistribution) operations, and our work could be directly applicable.

Data distributions for high-performance computing. Our distributed types essentially represent
block decompositions of global arrays: each node holds a contiguous, local block, i.e. a tile, of the
global array. This distribution strategy is particularly well-suited to linear algebra operations that
expose â€œembarrassing parallelismâ€, such as element-wise operators and matrix multiplications.
Earlier work on High Performance Fortran (HPF) [15] and distributed linear algebra libraries such
as ScaLAPACK [10] considered the more general block-cyclic distribution and produced a wealth of
literature on redistribution [8, 9, 20, 22, 28, 33]. Unlike our approach, these redistribution algorithms
do not use MPI collectives, but typically rely on send/receive primitives to exchange data. We
assume this is, at least in part, due to the MPI standard only stabilizing around the same time.
Some works, e.g. [33], anticipate the utility of alltoall for reducing the number of individual
communication operations, especially as redistributing multi-dimensional arrays has traditionally
been done one dimension at a time.

Languages and types for distributed data. Languages such as X10 [6], Chapel [4] and Regent [26]
aim to make programming with distributed memory safe and efficient. To this end, type systems
and static analyses have been proposed that track the locality of data [5, 7, 12, 17â€“19, 27] to ensure
safety, to avoid inefficient access patterns, and to elide runtime checks [29â€“31]. PartIR:SPMD is
a much simpler domain-specific language and its spmd_op can, by construction, only access local
data. Hence a lot of this work is not directly applicable. In essence, we use the distributed types
mainly as a formal language for the redistribution problem and its optimization.

A PROOFS OF LEMMAS AND THEOREMS

Proof of Lemma 4.1. By induction on ğ‘¥. If ğ‘¥ is empty, then ğ‘ = ğ‘› and 0 is the only multiple of ğ‘›
below ğ‘›. The induction step makes use of the fact that, by ğ» âŠ¢ ğ‘{ğ‘¥ }ğ‘›, no axis appears in ğ‘¥ more
â–¡
than once.

Memory-efficient array redistribution through portable collective communication

23

Proof of Lemma 4.2. Let ğœ = [ğ‘‘1, . . . , ğ‘‘ğ‘›] and apply Lemma 4.1 to each of the ğ‘‘ğ‘– . The tuples in
the image of T âŸ¦ğœâŸ§ğ» then contain all combinations of values from the images of the DâŸ¦ğ‘‘ğ‘– âŸ§ğ» since,
â–¡
by ğ» âŠ¢ ğœ, no axis appears in more than one of the ğ‘‘ğ‘– .

Proof of Lemma 4.6. By T-AllGather, the allgather(ğ‘–) operates on an axis ğ‘¦ : ğ‘š âˆˆ ğ» at

dimension ğ‘–. By T-DynSlice, we have ğ‘¥ : ğ‘› âˆˆ ğ» . Now the proof proceeds by case analysis.

(1) ğ‘– = ğ‘—, ğ‘¥ = ğ‘¦: In this case, ğœ0 = ğœ2 and the desired sequence is the empty one.
(2) ğ‘– = ğ‘—, ğ‘¥ â‰  ğ‘¦: If ğ‘š = ğ‘›, then ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ0) = ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ2) and hence ğœ0

allpermute
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ2. If

ğ‘š â‰  ğ‘›:

ğœ0 = [. . . , ğ‘ğ‘– {ğ‘¦, ğ‘¥ğ‘– }ğ‘ ğ‘–, . . . , . . .]

dynslice( ğ‘—,ğ‘¥)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ [. . . , ğ‘ğ‘– /ğ‘›{ğ‘¥, ğ‘¦, ğ‘¥ğ‘– }ğ‘ ğ‘–, . . .]

allpermute
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ [. . . , ğ‘ğ‘– /ğ‘›{ğ‘¦, ğ‘¥, ğ‘¥ğ‘– }ğ‘ ğ‘–, . . .]

allgather(ğ‘–)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ [. . . , (ğ‘ğ‘– /ğ‘› Â· ğ‘š){ğ‘¥, ğ‘¥ğ‘– }ğ‘ ğ‘–, . . .] = ğœ2 .

Here we used the fact that ğ‘š, ğ‘› are assumed prime. From rule T-DynSlice in the original
Â· ğ‘›, which
sequence we know that there exists ğ‘ â€²
allows us to apply T-DynSlice to ğœ0.

ğ‘– Â· ğ‘›. Therefore, ğ‘ğ‘– = ğ‘ â€²â€²
ğ‘–

ğ‘– such that ğ‘ğ‘– Â· ğ‘š = ğ‘ â€²

(3) ğ‘– â‰  ğ‘—, ğ‘¥ = ğ‘¦: In this case, ğœ0
(4) ğ‘– â‰  ğ‘—, ğ‘¥ â‰  ğ‘¦: In this case, the operations allgather(ğ‘–) and dynslice( ğ‘—, ğ‘¥) commute.

alltoall(ğ‘–,ğ‘—)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ2 is the desired sequence.

Note that in cases (2) and (4) the peak of the original sequence has been transformed into a valley.
In cases (1) and (3), it has been replaced with a flat line, where the trivial sequence from case (1) is
â–¡
considered flat.

Proof of Lemma 4.7. Consider the case of a rising edge, i.e. the situation in the top left corner of
the (graphical) statement of the lemma. The proof proceeds by analyzing all possible cases for ğ‘ and
ğ‘–. Here, we restrict ourselves to the cases for ğ‘ = alltoall(ğ‘˜, ğ‘™). Note first that by T-AllGather,
the allgather(ğ‘–) operates on an axis ğ‘¥ : ğ‘š âˆˆ ğ» ; and by T-AllToAll, ğ‘ operates on axis ğ‘¦ : ğ‘› âˆˆ ğ» .
(1) ğ‘– = ğ‘˜ â‰  ğ‘™: In this case, ğœ0 = [. . . , ğ‘ğ‘– {ğ‘¥, ğ‘¦, ğ‘¥ }ğ‘ ğ‘–, . . .] and hence there exists a permutation such
allgather(ğ‘–)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ2.

alltoall(ğ‘–,ğ‘™)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ â€²
1

that ğœ0

allpermute
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ â€²
0
(2) ğ‘– = ğ‘™ â‰  ğ‘˜: In this case,

ğœ0 = [. . . , ğ‘ğ‘– {ğ‘¥, ğ‘¥ğ‘– }ğ‘ ğ‘–, . . . , ğ‘ğ‘˜ {ğ‘¦, ğ‘¥ğ‘˜ }ğ‘ ğ‘˜, . . .] ,
ğœ2 = [. . . , (ğ‘ğ‘– /ğ‘› Â· ğ‘š){ğ‘¦, ğ‘¥ğ‘– }ğ‘ ğ‘–, . . . , (ğ‘ğ‘˜ Â· ğ‘›){ğ‘¥ğ‘˜ }ğ‘ ğ‘˜, . . .] .

If ğ‘š = ğ‘›, then ğœ0
are prime, the following sequence exists:

allpermute
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ â€²
1

allgather(ğ‘˜)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ2. If ğ‘š â‰  ğ‘›, using again the fact that ğ‘š and ğ‘›

ğœ0 = [. . . , ğ‘ğ‘– {ğ‘¥, ğ‘¥ğ‘– }ğ‘ ğ‘–, . . . , ğ‘ğ‘˜ {ğ‘¦, ğ‘¥ğ‘˜ }ğ‘ ğ‘˜, . . .]

alltoall(ğ‘˜,ğ‘–)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ [. . . , ğ‘ğ‘– /ğ‘›{ğ‘¦, ğ‘¥, ğ‘¥ğ‘– }ğ‘ ğ‘–, . . . , (ğ‘ğ‘˜ Â· ğ‘›){ğ‘¥ğ‘˜ }ğ‘ ğ‘˜, . . .]

allpermute
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ [. . . , ğ‘ğ‘– /ğ‘›{ğ‘¥, ğ‘¦, ğ‘¥ğ‘– }ğ‘ ğ‘–, . . . , (ğ‘ğ‘˜ Â· ğ‘›){ğ‘¥ğ‘˜ }ğ‘ ğ‘˜, . . .]
allgather(ğ‘–)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ [. . . , (ğ‘ğ‘– /ğ‘› Â· ğ‘š){ğ‘¦, ğ‘¥ğ‘– }ğ‘ ğ‘–, . . . , (ğ‘ğ‘˜ Â· ğ‘›){ğ‘¥ğ‘˜ }ğ‘ ğ‘˜, . . .] = ğœ2 .

(3) ğ‘– â‰  ğ‘˜, ğ‘– â‰  ğ‘™: In this case, the operations allgather(ğ‘–) and alltoall(ğ‘˜, ğ‘™) commute.
The remaining cases, i.e. where ğ‘ âˆˆ {allpermute}, are treated analogously. Note that to make
the case analysis manageable, one should split allpermute into three more low-level permutations:
(i) swapping two axes within a dimension, (ii) swapping two axes across different dimensions, (iii)

24

Norman A. Rink, Adam Paszke, Dimitrios Vytiniotis, and Georg Stefan Schmid

swapping an axis for a replicated one. This presents no loss of generality since any permutation
can be decomposed into a sequence of these special permuations (i)â€“(iii).

Finally, the case of a falling edge, i.e. the situation in the bottom left corner of the (graphical)
â–¡

lemma statement, is handled in an entirely dual fashion.

Proof of Theorem 4.8. Repeated application of Lemmas 4.6, 4.7. This process reaches a fixed
point when there are no more peaks or rising or falling edges. Hence, the resulting sequence is in
â–¡
normal form.

Proof of Lemma 5.1. The preconditions in the lemma statement guarantee that the images of
T âŸ¦ğœ1âŸ§ğ» , T âŸ¦ğœ2âŸ§ğ» contain the same base offsets, for identical numbers of tiles of identical sizes. â–¡

Proof of Lemma 6.3. Consider ğ‘ âˆˆ {allgather}. We obtain ğ›½ âˆ¼ T âŸ¦[. . . , ğ‘ğ‘– {ğ‘¥, ğ‘¥ğ‘– }ğ‘ ğ‘–, . . .]âŸ§ğ» by
applying rule inversion to ğœ1 â–¶ğ‘ ğœ2. Hence, ğ›½ = T âŸ¦[. . . , ğ‘ğ‘– {ğ‘¦, ğ‘§, ğ‘¥ }ğ‘ ğ‘–, . . .]âŸ§ğ» with suitable ğ‘¥, ğ‘¦, ğ‘§,
where axis ğ‘§ has the same size as ğ‘¥. (This uses the fact that axis sizes are prime.) From rule allgather
in Figure 10 we then get âŸ¨ğœ™, ğ›½âŸ©ğ»,ğ· â‡’ğ‘ âŸ¨ğœ™ â€², ğ›½ â€²â€²âŸ©ğ»,ğ· with ğ›½ â€²â€² = T âŸ¦[. . . , (ğ‘ğ‘– Â·ğ‘›){ğ‘¦, ğ‘¥ }ğ‘ ğ‘–, . . .]âŸ§ğ» . Because
of ğœ1 â–¶ğ‘ ğœ2 we also have EâŸ¦ğœ2âŸ§ğ» = EâŸ¦[. . . , (ğ‘ğ‘– Â· ğ‘›){ğ‘¥ğ‘– }ğ‘ ğ‘–, . . .]âŸ§ğ» = EâŸ¦[. . . , (ğ‘ğ‘– Â· ğ‘›){ğ‘¦, ğ‘¥ }ğ‘ ğ‘–, . . .]âŸ§ğ» .
Hence, ğ›½ â€²â€² âˆˆ EâŸ¦ğœ2âŸ§ğ» .

Consider now ğ‘ âˆˆ {dynslice}. As before, we get ğ›½ âˆ¼ T âŸ¦[. . . , (ğ‘ğ‘– Â·ğ‘›){ğ‘¥ğ‘– }ğ‘ ğ‘–, . . .]âŸ§ğ» and EâŸ¦ğœ2âŸ§ğ» =
EâŸ¦[. . . , ğ‘ğ‘– {ğ‘¥, ğ‘¥ğ‘– }ğ‘ ğ‘–, . . .]âŸ§ğ» by inverting ğœ1 â–¶ğ‘ ğœ2. Therefore ğ›½ = T âŸ¦[. . . , (ğ‘ğ‘– Â· ğ‘›){ğ‘¥ }ğ‘ ğ‘–, . . .]âŸ§ğ» with
suitable ğ‘¥. Hence, from dynslice in Figure 10 we conclude âŸ¨ğœ™, ğ›½âŸ©ğ»,ğ· â‡’ğ‘ âŸ¨ğœ™ â€², ğ›½ â€²âŸ©ğ»,ğ· with ğ›½ â€² =
T âŸ¦[. . . , ğ‘ğ‘– {ğ‘¥, ğ‘¥ }ğ‘ ğ‘–, . . .]âŸ§ğ» âˆˆ EâŸ¦ğœ2âŸ§ğ» .

The case ğ‘ âˆˆ {alltoall} is treated as an amalgamation of the previously considered cases. â–¡

Proof of Theorem 6.4. By induction on the length of the sequence ğœ1 â–¶âˆ— ğœ2 and application of
â–¡

Lemma 6.3.

Proof of Lemma 6.5. Lemmas 4.6 and 4.7 carry over to â–¶. In these weak version of the lemmas,
none of the transformations applied to a sequence in â–¶âˆ— increase ğ‘ğ‘œğ‘ ğ‘¡. In particular, Lemma 4.7 does
not introduce more alltoall operations than are already in the sequence. Therefore, repeatedly
â–¡
applying the weak versions of Lemmas 4.6 and 4.7 to ğ‘  yields the desired ğ‘ nf in normal form.

Proof of Lemma 6.6. The proof is straightforward by turning rules from Figure 10 with labels
ğ‘ ğ‘— âˆ‰ {allpermute} into the corresponding transitions for â–¶. Note that this works for ğ‘ ğ‘— âˆˆ
{allgather, alltoall} because the precondition ğ›½ â€² â—¦ ğœ™ â€²âˆ’1 = ğ›½ â—¦ ğœ™ âˆ’1 can be written as ğ›½ â€² =
ğ›½ â—¦ ğœ™ âˆ’1 â—¦ ğœ™ â€², which means that ğ›½ â€² âˆ¼ ğ›½. Similarly, the ğ‘ ğ‘— âˆˆ {allpermute} disappear when passing
â–¡
to â–¶ because the preconditions of allpermute in Figure 10 imply ğ›½ â€² âˆ¼ ğ›½.

Proof of Theorem 6.7. First use Lemma 6.6 to pass to a sequence ğ‘ ğ‘¤ : ğœ1 â–¶âˆ— ğœ2 with ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘ ğ‘¤) â‰¤
ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘ ) (because there are no allpermute transitions in ğ‘ ğ‘¤). Then, by Lemma 6.5, there exists
a normal form sequence ğ‘ nf : ğœ1 â–¶âˆ— ğœ2 with ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘ nf) â‰¤ ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘ ğ‘¤). By Theorem 6.4 we obtain a
sequence ğ‘  â€²â€² : âŸ¨ğœ™, T âŸ¦ğœ1âŸ§ğ» âŸ©ğ» â‡’âˆ— âŸ¨ğœ™ â€², ğ›½ â€²âŸ©ğ» with ğ›½ â€² âˆˆ EâŸ¦ğœ2âŸ§ğ» and ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘  â€²â€²) = ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘ nf). Then apply
the reasoning that led to (8) to obtain ğ‘  â€² : âŸ¨ğœ™, T âŸ¦ğœ1âŸ§ğ» âŸ©ğ» â‡’âˆ— âŸ¨ğœ™, T âŸ¦ğœ2âŸ§ğ» âŸ©ğ» , which may include a
single, final allpermute. Hence ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘  â€²) â‰¤ ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘  â€²â€²) + ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ2). Putting all previous estimates
for ğ‘ğ‘œğ‘ ğ‘¡ together, we find

ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘  â€²) â‰¤ ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘  â€²â€²) + ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ2)
= ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘ nf) + ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ2)
â‰¤ ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘ ğ‘¤) + ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ2)
â‰¤ ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘ ) + ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ2) ,

as required.

â–¡

Memory-efficient array redistribution through portable collective communication

25

B DERIVATION OF DATA TRANSFER COST
To derive ğ‘ğ‘œğ‘ ğ‘¡ from Figure 11, we count the number of array elements that are transferred between
devices by each of the collective operations. We let ğ›¿ be the number of devices in the fixed device
mesh ğ» . Note that ğ›¿ is equal to the product of the sizes of the axes in ğ» .

(1) ğœ1

allpermute
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ2

Each device in the mesh ğ» holds a local tile of ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1) array elements. In a general
permutation, each tile must move across the device network to a new device in order to
produce the final distributed type ğœ2. Hence, the total number of array elements transferred
is ğ›¿ Â· ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1). To obtain ğ‘ğ‘œğ‘ ğ‘¡, we normalize this number by ğ›¿, which gives

(cid:18)
ğœ1

ğ‘ğ‘œğ‘ ğ‘¡

allpermute
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ2

(cid:19)

= ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1) .

Note that since ğœ1 and ğœ2 are related by an allpermute operation, we have ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1) =
ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ2).

(2) ğœ1

dynslice
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ2

Since dynslice is a purely local operation, no data is transferred. Therefore,

(cid:18)
ğœ1

ğ‘ğ‘œğ‘ ğ‘¡

dynslice(_,_)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ2

(cid:19)

= 0 .

(3) ğœ1

allgather
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ2

Let ğ‘› be the size of the axis that the allgather operates on. When considering a multi-axes
allgather that operates on axes ğ‘¥, then ğ‘› is the product of the sizes of the axes in ğ‘¥. (Note
that this product is no longer prime if there is more than one axis in ğ‘¥.)
An allgather operation gathers tiles among groups of devices along axis ğ‘¥ (along multiple
axes ğ‘¥, respectively). Each group contains ğ‘› devices. Hence there are ğ›¿/ğ‘› groups in total. Each
device holds ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1) array elements and within each group, tiles must be exchanged
between each pair of devices. Hence, the communication within a group amounts to ğ‘›2 Â·
ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1). This makes for a total transfer of

ğ›¿/ğ‘› Â· ğ‘›2 Â· ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1) = ğ›¿ Â· ğ‘› Â· ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1)

array elements. Normalizing by ğ›¿, and noting that ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ2) = ğ‘› Â· ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1), we find
(cid:19)

(cid:18)
ğœ1

ğ‘ğ‘œğ‘ ğ‘¡

allgather(_)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ2

= ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ2) .

(4) ğœ1

alltoall
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ2

Let ğ‘› be the size of the axis that the alltoall operates on, i.e. the axis that is transferred
between dimensions by the alltoall. When considering a multi-axes alltoall that operates
on axes ğ‘¥, then ğ‘› is, again, the product of the sizes of the axes in ğ‘¥.
Now, alltoall operates as follows:
â€¢ Split the local tile on each device into ğ‘› tiles of size ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1)/ğ‘›.
â€¢ Each of these ğ‘› tiles either remains local or is sent to exactly one other device.

26

Norman A. Rink, Adam Paszke, Dimitrios Vytiniotis, and Georg Stefan Schmid

â€¢ After all communication is done, each device locally concatenates all smaller tiles of size
ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1)/ğ‘› it has received, which produces local tiles of ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1) = ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ2).
Since each device sends ğ‘› tiles of size ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1)/ğ‘›, the total amount of transferred data is

ğ›¿ Â· ğ‘› Â· ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1)/ğ‘› = ğ›¿ Â· ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1) .

(Note that the number of tiles that remain local is of lower order in ğ‘›. To a first approximation,
we neglect this number.) After normalizing by ğ›¿, we find

(cid:18)
ğœ1

ğ‘ğ‘œğ‘ ğ‘¡

alltoall(_,_)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğœ2

(cid:19)

= ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘ ğ‘–ğ‘§ğ‘’ (ğœ1) .

REFERENCES
[1] Michael Bauer and Michael Garland. 2019. Legate NumPy: Accelerated and Distributed Array Computing. In Proceedings
of the International Conference for High Performance Computing, Networking, Storage and Analysis (Denver, Colorado)
(SC â€™19). Association for Computing Machinery, New York, NY, USA, Article 23, 23 pages. https://doi.org/10.1145/
3295500.3356175

[2] Michael Bauer, Sean Treichler, Elliott Slaughter, and Alex Aiken. 2012. Legion: Expressing Locality and Independence
with Logical Regions. In Proceedings of the International Conference on High Performance Computing, Networking,
Storage and Analysis (Salt Lake City, Utah) (SC â€™12). IEEE Computer Society Press, Washington, DC, USA, Article 66,
11 pages.

[3] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula,
Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of
Python+NumPy programs. http://github.com/google/jax

[4] B.L. Chamberlain, D. Callahan, and H.P. Zima. 2007. Parallel Programmability and the Chapel Language. The
International Journal of High Performance Computing Applications 21, 3 (2007), 291â€“312. https://doi.org/10.1177/
1094342007078442 arXiv:https://doi.org/10.1177/1094342007078442

[5] Satish Chandra, Vijay Saraswat, Vivek Sarkar, and Rastislav Bodik. 2008. Type Inference for Locality Analysis of
Distributed Data Structures. In Proceedings of the 13th ACM SIGPLAN Symposium on Principles and Practice of Parallel
Programming (Salt Lake City, UT, USA) (PPoPP â€™08). Association for Computing Machinery, New York, NY, USA, 11â€“22.
https://doi.org/10.1145/1345206.1345211

[6] Philippe Charles, Christian Grothoff, Vijay Saraswat, Christopher Donawa, Allan Kielstra, Kemal Ebcioglu, Christoph
von Praun, and Vivek Sarkar. 2005. X10: An Object-Oriented Approach to Non-Uniform Cluster Computing. In
Proceedings of the 20th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and
Applications (San Diego, CA, USA) (OOPSLA â€™05). Association for Computing Machinery, New York, NY, USA, 519â€“538.
https://doi.org/10.1145/1094811.1094852

[7] GwenaÃ«l Delaval, Alain Girault, and Marc Pouzet. 2008. A type system for the automatic distribution of higher-order
synchronous dataflow programs. In Proceedings of the 2008 ACM SIGPLAN-SIGBED conference on Languages, compilers,
and tools for embedded systems. 101â€“110.

[8] F. Desprez, J. Dongarra, A. Petitet, C. Randriamaro, and Y. Robert. 1998. Scheduling block-cyclic array redistribution.

IEEE Transactions on Parallel and Distributed Systems 9, 2 (1998), 192â€“205. https://doi.org/10.1109/71.663945

[9] Jack J Dongarra, Robert van de Geijn, and David W Walker. 1992. A look at scalable dense linear algebra libraries.

Technical Report. Oak Ridge National Lab., TN (United States).

[10] Jack J Dongarra and David W Walker. 1995. Software libraries for linear algebra computations on high performance

computers. SIAM review 37, 2 (1995), 151â€“180.

[11] Markus GÃ¶tz, Charlotte Debus, Daniel Coquelin, Kai Krajsek, Claudia Comito, Philipp Knechtges, BjÃ¶rn Hagemeier,
Michael Tarnawa, Simon Hanselmann, Martin Siggel, Achim Basermann, and Achim Streit. 2020. HeAT - a Distributed
and GPU-accelerated Tensor Framework for Data Analytics. In IEEE International Conference on Big Data, Big Data
2020, Atlanta, GA, USA, December 10-13, 2020, Xintao Wu, Chris Jermaine, Li Xiong, Xiaohua Hu, Olivera Kotevska,
Siyuan Lu, Weija Xu, Srinivas Aluru, Chengxiang Zhai, Eyhab Al-Masri, Zhiyuan Chen, and Jeff Saltz (Eds.). IEEE,
276â€“287. https://doi.org/10.1109/BigData50022.2020.9378050

[12] Christian Grothoff, Jens Palsberg, and Vijay Saraswat. 2006. A type system for distributed arrays. Unpublished draft

(2006).

[13] Zhihao Jia, Matei Zaharia, and Alex Aiken. 2019. Beyond Data and Model Parallelism for Deep Neural Networks. In
Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April 2, 2019, Ameet
Talwalkar, Virginia Smith, and Matei Zaharia (Eds.). mlsys.org. https://proceedings.mlsys.org/book/265.pdf

Memory-efficient array redistribution through portable collective communication

27

[14] Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh
Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike
Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert
Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski,
Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James
Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire
Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick,
Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory
Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia
Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. 2017.
In-
Datacenter Performance Analysis of a Tensor Processing Unit. SIGARCH Comput. Archit. News 45, 2 (June 2017), 1â€“12.
https://doi.org/10.1145/3140659.3080246

[15] Charles H Koelbel, David B Loveman, Robert S Schreiber, Guy Lewis Steele Jr, and Mary Zosel. 1994. The high

performance Fortran handbook. MIT press.

[16] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam
Shazeer, and Zhifeng Chen. 2020. GShard: Scaling Giant Models with Conditional Computation and Automatic
Sharding. CoRR abs/2006.16668 (2020). arXiv:2006.16668 https://arxiv.org/abs/2006.16668

[17] Ben Liblit and Alexander Aiken. 2000. Type Systems for Distributed Data Structures. In Proceedings of the 27th ACM
SIGPLAN-SIGACT Symposium on Principles of Programming Languages (Boston, MA, USA) (POPL â€™00). Association for
Computing Machinery, New York, NY, USA, 199â€“213. https://doi.org/10.1145/325694.325717

[18] Ben Liblit, Alex Aiken, and Katherine Yelick. 2003. Type systems for distributed data sharing. In International Static

Analysis Symposium. Springer, 273â€“294.

[19] Tom Murphy VII, Karl Crary, and Robert Harper. 2007. Type-Safe Distributed Programming with ML5. In Trustworthy
Global Computing, Third Symposium, TGC 2007, Sophia-Antipolis, France, November 5-6, 2007, Revised Selected Papers
(Lecture Notes in Computer Science, Vol. 4912), Gilles Barthe and CÃ©dric Fournet (Eds.). Springer, 108â€“123. https:
//doi.org/10.1007/978-3-540-78663-4_9

[20] Neungsoo Park, Viktor K. Prasanna, and Cauligi Raghavendra. 1998. Efficient Algorithms for Block-Cyclic Array
Redistribution between Processor Sets. In Proceedings of the 1998 ACM/IEEE Conference on Supercomputing (San Jose,
CA) (SC â€™98). IEEE Computer Society, USA, 1â€“13.

[21] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,
Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An
Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems 32,
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and R. Garnett (Eds.). Curran Associates, Inc., 8024â€“
8035. http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
[22] S. Ramasulamy and P. Banerjee. 1995. Automatic generation of efficient array redistribution routines for distributed
memory multicomputers. In Proceedings Frontiers â€™95. The Fifth Symposium on the Frontiers of Massively Parallel
Computation. 342â€“349. https://doi.org/10.1109/FMPC.1995.380436

[23] Keshav Santhanam, Siddharth Krishna, Ryota Tomioka, Andrew Fitzgibbon, and Tim Harris. 2021. DistIR: An
Intermediate Representation for Optimizing Distributed Neural Networks. In Proceedings of the 1st Workshop on
Machine Learning and Systems (Online, United Kingdom) (EuroMLSys â€™21). Association for Computing Machinery, New
York, NY, USA, 15â€“23. https://doi.org/10.1145/3437984.3458829

[24] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins,
HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake Hechtman. 2018. Mesh-TensorFlow: Deep
Learning for Supercomputers. In Neural Information Processing Systems.

[25] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-
LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. CoRR abs/1909.08053 (2019).
arXiv:1909.08053 http://arxiv.org/abs/1909.08053

[26] Elliott Slaughter, Wonchan Lee, Sean Treichler, Michael Bauer, and Alex Aiken. 2015. Regent: A High-Productivity
Programming Language for HPC with Logical Regions. In Proceedings of the International Conference for High Perfor-
mance Computing, Networking, Storage and Analysis (Austin, Texas) (SC â€™15). Association for Computing Machinery,
New York, NY, USA, Article 81, 12 pages. https://doi.org/10.1145/2807591.2807629

[27] Wouter Swierstra and Thorsten Altenkirch. 2008. Dependent Types for Distributed Arrays. Trends in Functional

Programming 9 (2008).

[28] R. Thakur, A. Choudhary, and J. Ramanujam. 1996. Efficient algorithms for array redistribution. IEEE Transactions on

Parallel and Distributed Systems 7, 6 (1996), 587â€“594. https://doi.org/10.1109/71.506697

28

Norman A. Rink, Adam Paszke, Dimitrios Vytiniotis, and Georg Stefan Schmid

[29] Arun Thangamani and V Krishna Nandivada. 2018. Optimizing Remote Data Transfers in X10. In Proceedings of the 27th
International Conference on Parallel Architectures and Compilation Techniques (Limassol, Cyprus) (PACT â€™18). Association
for Computing Machinery, New York, NY, USA, Article 27, 15 pages. https://doi.org/10.1145/3243176.3243209
[30] Sean Treichler, Michael Bauer, and Alex Aiken. 2013. Language Support for Dynamic, Hierarchical Data Partitioning.

SIGPLAN Not. 48, 10 (Oct. 2013), 495â€“514. https://doi.org/10.1145/2544173.2509545

[31] Sean Treichler, Michael Bauer, Rahul Sharma, Elliott Slaughter, and Alex Aiken. 2016. Dependent Partitioning. In
Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages,
and Applications (Amsterdam, Netherlands) (OOPSLA 2016). Association for Computing Machinery, New York, NY,
USA, 344â€“358. https://doi.org/10.1145/2983990.2984016

[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, undefinedukasz Kaiser,
and Illia Polosukhin. 2017. Attention is All You Need. In Proceedings of the 31st International Conference on Neural
Information Processing Systems (Long Beach, California, USA) (NIPSâ€™17). Curran Associates Inc., Red Hook, NY, USA,
6000â€“6010.

[33] D.W. Walker and S.W. Otto. 1996. Redistribution of block-cyclic data distributions using MPI. Concurrency: Practice
and Experience 8, 9 (1996), 707â€“728. https://doi.org/10.1002/(SICI)1096-9128(199611)8:9<707::AID-CPE269>3.0.CO;2-V
[34] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake A. Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun,
Dmitry Lepikhin, Andy Ly, Marcello Maggioni, Ruoming Pang, Noam Shazeer, Shibo Wang, Tao Wang, Yonghui
Wu, and Zhifeng Chen. 2021. GSPMD: General and Scalable Parallelization for ML Computation Graphs. CoRR
abs/2105.04663 (2021). arXiv:2105.04663 https://arxiv.org/abs/2105.04663

