Discovering Imperfectly Observable Adversarial Actions
using Anomaly Detection

Olga Petrova
Avast Software
olga.petrova@avast.com

Karel Horak
Dept. of Computer Science, FEE,
Czech Technical University in Prague
karel.horak@fel.cvut.cz

Karel Durkota
Dept. of Computer Science, FEE,
Czech Technical University in Prague
karel.durkota@fel.cvut.cz

Michal Najman
Avast Software
michal.najman@avast.com

Galina Alperovich
Avast Software
galina.alperovich@avast.com

Branislav Bosansky
Dept. of Computer Science, FEE,
Czech Technical University in Prague
Avast Software
bosansky@fel.cvut.cz

Viliam Lisy
Dept. of Computer Science, FEE,
Czech Technical University in Prague
Avast Software
viliam.lisy@fel.cvut.cz

ABSTRACT
Anomaly detection is a method for discovering unusual and suspi-
cious behavior. In many real-world scenarios, the examined events
can be directly linked to the actions of an adversary, such as attacks
on computer networks or frauds in ﬁnancial operations. While the
defender wants to discover such malicious behavior, the attacker
seeks to accomplish their goal (e.g., exﬁltrating data) while avoiding
the detection. To this end, anomaly detectors have been used in a
game-theoretic framework that captures these goals of a two-player
competition. We extend the existing models to more realistic settings
by (1) allowing both players to have continuous action spaces and by
assuming that (2) the defender cannot perfectly observe the action
of the attacker. We propose two algorithms for solving such games –
a direct extension of existing algorithms based on discretizing the
feature space and linear programming and the second algorithm
based on constrained learning. Experiments show that both algo-
rithms are applicable for cases with low feature space dimensions
but the learning-based method produces less exploitable strategies
and it is scalable to higher dimensions. Moreover, we use real-world
data to compare our approaches with existing classiﬁers in a data-
exﬁltration scenario via the DNS channel. The results show that our
models are signiﬁcantly less exploitable by an informed attacker.

INTRODUCTION

1
Anomaly detection is a general machine learning technique often
used to ﬁnd anomalous data, defects in products, or breakdowns of
machinery. However, anomaly detection is also commonly used to
detect malicious behavior, such as intrusions in computer networks
[12], fraud in ﬁnancial transactions [1], or malicious behavior of
software [8]. In these domains, the effects of adversarial actions are
being examined. The attackers execute their actions to optimize two
aspects: (1) to not be detected by the anomaly detector and (2) to
achieve their malicious goal (e.g., exﬁltrate data from a computer

network). From this perspective, using standard anomaly detection
methods ignores the second aspect optimized by the attackers.

To explicitly reason about the goals of other agents, the game-
theoretic framework is often used. There are several existing works
where anomaly detection is integrated into a game-theoretic frame-
work [6, 9, 10]. However, to the best of our knowledge, the existing
game-theoretic models have two or more of the following limitations:
They (i) assume discrete feature space over which the classiﬁcation
occurs; (ii) have very limited scalability with increasing dimension-
ality of this feature space; (iii) do not allow direct control of the
false positive rate; (iv) and assume perfect observation of actions
performed by the attacker. As a result, they are not readily applicable
to many real-world problems, such as service misuse detection and
data exﬁltration. In these practical cases, features used for anomaly
detection are often continuous (e.g., amount of data, entropy, etc.)
and the actions of the attacker are interleaving with actions of reg-
ular benign users (e.g., when the attacker uses regular service for
uploading data such as Dropbox or Google Drive that are also being
used by the regular user). For this latter reason, it is not possible for
the defender to exactly determine the action of the attacker nor the
reward of the attacker. The defender only observes a signal that can
be a combination of a benign and a malicious action (e.g., the total
amount of uploaded data within a time window).

In this paper, we address all mentioned limitations and formulate
the problem of adversarial anomaly detection where: (i) we allow
features for anomaly detection to be continuous; (ii) there are hard
constraints on false-positive rates for the anomaly detector; (iii) the
actions of the attacker are not directly observable by the defender.
We show that satisfying the ﬁrst two extensions is possible by ex-
tending the existing results [9]. We derive a closed-form solution for
the case with a known value of the game and combine this solution
with a binary-search algorithm to optimize the value of the game
with respect to the desired false-positive rate. For the case with un-
observable actions of the attacker, we give two different algorithms.

As a baseline solution based on the existing works in this domain
(e.g., [10]), we provide a linear programming (LP) formulation to
compute the optimal solution of a discretized version of the problem.
However, as we show in the experimental evaluation, the LP is not
able to solve problems where the feature space for anomaly detection
has many dimensions. To this end, we extend a recent learning-based
algorithm exploitability descent [17] and we show that such an algo-
rithm can be complemented with false-positive constraints and thus
used in the domain of adversarial anomaly detection (or adversarial
machine learning). We call this algorithm Exploitability Descent
for Adversarial Anomaly Detection (EDA). This approach avoids
the discretization step and approximates the optimal strategy of the
defender by a neural network (NN). EDA ensures that the neural
network is trained to make the attacker indifferent among his best
options and that it meets the false-positive rate constraint.

The experimental evaluation on synthetic data shows that LP
and NN produce very similar results in one or two-dimensional
problems. However, any discretization-based approach cannot scale
to larger dimensions. In order to compute some approximation
within computation-time and memory limitations, we use increas-
ingly sparse discretization points for increasing dimensions. In this
comparison, the NN-based approach quickly outperforms the LP
by producing more robust strategies for the defender. Second, we
perform the experiment with real-world data of DNS queries that
can be used for data exﬁltration. We used a simple setting with only
3 features over the queries and show that our proposed method is
signiﬁcantly more robust compared to classical anomaly detectors
that do not take the goals of the attackers into consideration. Com-
pared to the detector computed by the proposed method, the attacker
could gain 50% higher throughput of exﬁltrated data against the best
classical anomaly detector.

2 RELATED WORK
Widespread use of machine learning in security domains inspires
analyzing their robustness against attackers. Empirical studies have
shown that standard machine learning algorithms are vulnerable
to well-tailored attacks [21, 24–26, 28]. It led a new ﬁeld study –
adversarial machine learning (AML). Surveys of the earlier works
in AML for security domains are in [3, 14] and the more recent
works are summarized in [27].

Research in AML consists of efﬁcient generation of adversarial
examples [11, 19] and designing machine learning models robust
against manipulation. In this paper, we focus on the latter. One class
of approaches for creating robust classiﬁers obfuscates model gra-
dients in order to make searching for adversarial examples difﬁcult.
However, these approaches have been shown ineffective and easy to
circumvent [2]. Another class, called adversarial training, modiﬁes
the training process to use the standard training data in combination
with repetitively generated adversarial examples [13, 18, 25]. These
approaches are similar to the method presented in this paper, but
they do not have clear theoretic foundations and do not allow explicit
consideration of players’ incentives.

Recent years brought several principled approaches to AML. If
the attacker tries to avoid detection without other preferences over
the produced data samples, the problem may be formulated using
robust optimization techniques [29]. In the case of a more complex

structure of incentives, the framework of game theory is more suit-
able. Game theory provides a range of tools for computing optimal
solutions in discrete game models. Hence, existing methods often
relay on discretization [9, 10]. On the other hand, feature space
discretization leads to computational complexity exponential in its
dimensionality, which severely limits scalability. It can be partially
resolved by training the models disregarding the attackers and mak-
ing them robust by strategic selection of low-dimensional operating
points [16]. This is sub-optimal since the models are not trained
directly for the solved task.

Game-theoretic methods working directly with continuous fea-
ture spaces either introduce very limiting assumptions on the game
structure [5, 6], or approximate behavior of discrete algorithms by
neural networks and lose theoretic guarantees [22]. We follow the
second approach. Based on the structure solutions introduced in [9],
we propose an optimal game-theoretic solution for the simpliﬁed
problem with false positive constraints, but perfect observation of
the attacker’s actions. We then extend this solution to the novel set-
ting with uncertain observations using neural networks. To evaluate
the performance of this extension, we propose an LP formulation
based on [10] extended by kernel-based estimation of the benign
data distribution.

3 PROBLEM SETTING
The interaction between the defender and the attacker can be mod-
eled as a two-player game. The defender observes events that corre-
spond to points in =-dimensional feature space and needs to decide
whether to classify them as malicious. This decision can be random-
ized, and hence we consider that the classiﬁer outputs a probability
of a positive classiﬁcation. The attacker observes the classiﬁer and
chooses an action which (1) generates an observation for the de-
fender, and (2) provides the attacker with a reward in case the action
goes undetected. For simplicity of the presentation, we assume that
every action of the attacker can be associated with a point in the
feature space (e.g., observation the defender would have received if
he was able to separate effects of attacker’s action from the effects
caused by benign users), and that for every point in the feature space,
there exists an action that maps to that point. This allows us to as-
sume that the attacker directly chooses a point 50 in the feature space
(however, this point is not directly observed by the defender). To
emphasize this distinction, we use 50 for the actions of the attacker,
and 5 for the feature vectors observed by the defender.

F

(F

T )

where

,⇠,',% ⇡,q,

Formally, we deﬁne a Generalized Classiﬁcation Game as a tuple
corresponds to =-dimensional real-
⌧ =
valued feature space corresponding to action space of the attacker
and ⇠ corresponds to a set of all classiﬁers that can be selected by
⇠ corresponds to a classiﬁcation
the defender. Each classiﬁer 2
2
function 2 :
, and 2
5
is the probability that an event
(
)
represented by a feature vector 5 =
gets inspected
by the defender. We assume that each feature 5 8 is bounded, and
!8,* 8
we denote these bounds by
. In case the attacker executes
action 50
and does not get detected, he receives a non-negative
reward '
0.The inspection capacity of the defender is limited,
and a maximum false-positive rate q
0, 1
of classifying benign
events as malicious is allowed. We assume that the benign events are

5 1, . . . ,5 =

2F
50
(

0, 1
]

F![

) 

2[

[

]

]

(

)

|

5

)

)

]

(

T

T

50

50

)2

T [

2F

T (

(F )

= E5

5 = 50

Finally,

%⇡ 2
⇠

T
5 = 50

%⇡ , i.e., Pr

samples from a distribution %⇡ , hence the expected false-positive
2
rate of a classiﬁer 2, denoted  ⇡ (
2
, satisﬁes  ⇡ (
.
)
corresponds to an observation transformation function.
For a given action of the attacker, associated with a feature vector
50
, the defender may observe feature vectors sampled from
.1 In general, we do not
 
a probability distribution
pose any restrictions on the transformation function
, but we focus
on two speciﬁc cases in the study. First, we describe a simple case
reveals the action of
that corresponds to existing models where
= 1. Second,
the attacker to the defender, i.e., Pr
we describe a case where the action of the attacker is additively
combined with an action 51 ⇠
=
50
51 |
50
T [

%⇡ of a legitimate user,
51 ]

+
Example 3.1. Consider a case where the attacker wants to upload
a 100 MB ﬁle from an inﬁltrated computer. To hide the activity,
the attacker is using a Dropbox folder for the upload. The goal of
the attacker is to upload the ﬁle as quickly as possible, however,
he observes that the standard user of the inﬁltrated computer uses
Dropbox very rarely and the user never uploads large ﬁles. The
attacker knows that an anomaly detection system is running on a
network and data over every 1 hour are aggregated and evaluated for
anomalies. If during this 1 hour time window both the user and the
attacker upload data to Dropbox, the aggregated statistic shows the
sum of these two uploads (function
). Therefore, the attacker has
to consider the behavior of the user (in a form of approximating a
probability distribution %⇡ ) and choose the amount of the uploaded
data (50) such that if the user uploads data as well, the overall amount
of data is unlikely to be detected as an anomaly.

= Pr%⇡ [

T (

50

T

+

)

]

.

Expected Utility. The attacker only gets a reward for executing
action 50 if he does not get detected by the defender. His expected
utility when executing action 50 against classiﬁer 2, denoted D0
,
hence satisﬁes

2, 50
(

)

(
5

D0

2, 50
(

)

=

1

d2

50

(

 

)) ·

50

'

(

)

,

(1)

)

(

50

= E5

is the expected probability that 2
where d2
⇠T (
classiﬁes action 50 (based on observations 5
50
) as anomalous.
We assume a zero-sum game setting where the goal of the defender
is to minimize the expected reward of the attacker.

2
50) [

⇠T (

)]

)

(

The solution of the Generalized Classiﬁcation Game corresponds
to a maximin (or a Stackelberg equilibrium) where the defender
chooses a classiﬁer 2 ﬁrst and the attacker follows by playing a
best response against the classiﬁer, i.e., action 50 that maximizes
attacker’s expected utility D0
. Formally, the defender seeks a
)
classiﬁer 2⇤ that minimizes the attacker’s utility under a false-positive
rate constraint assuming that the attacker plays a best response:

2, 50
(

2⇤ = arg min

2

max
⇠ {
50 2F
2
2
s.t.  %⇡ (

D0

2, 50
(

)}

q

)

(2a)

(2b)

We use ⌫'0
classiﬁer 2, where ⌫'0

2
(

)

2
(

)

= arg max50 D0

2, 50
(

)

.

to denote the best response of the attacker to the

1Recall that the feature vector 50 corresponding to attacker’s action cannot be observed
and used for the classiﬁcation.

4

IDENTITY OBSERVATION
TRANSFORMATION FUNCTION

Before discussing the more general setting, we ﬁrst describe the
is an identity and that corresponds to previous
baseline case where
work [9] that operated over a discrete feature space:

T

Pr

5

50

|

]

T [

=

1
0

(

5 = 50
5 < 50.

(3)

(

)

=

50

F![

0, 1
]

2, 50
(

In this case, the defender can directly observe the attacker’s action
50 and thus also the attacker’s reward '
. In order to obtain a
robust anomaly detector, we optimize the classiﬁcation strategy
2 :
against the worst-case attacker. Such an attacker aims
to choose the action with the highest expected reward for the attacker
D0
. Since the action of the attacker is
observable, we deﬁne the classiﬁer directly on the attacker’s actions,
and we write 2
.
)
Following Dritsoula et al. [9], we formulate a simple algorithm
that solves this game. First, assume that the value + ⇤  
0 of the
game is known. Then, the utility of any action 50 of the attacker
must be less than or equal to + ⇤,

instead of 2

50

50

50

))

 

'

1

2

)

(

)

(

)

(

(

(

5

1

2

50

'

50

+ ⇤ .

(4)

(

(

{

1

 

))

50

50

, 0

)

) 

max

'
+ ⇤/

(
This yields a lower bound on the probability an action 50 of the
attacker has to be inspected by an optimal classiﬁer 2⇤ to achieve (at
least) the value + ⇤,
d2⇤ (

 
Since the defender is able to observe attacker’s action 50, the proba-
coincide. Hence, an optimal classiﬁer 2⇤
bilities d2⇤ (
50
50
)
)
50
50
can be constructed as 2⇤ (
{
We have shown that in the case + ⇤ is known, we can represent
the optimal classiﬁer using a closed-form solution. In practice, the
value + ⇤ of the game is, however, unknown. To ﬁnd it we propose an
approach termed Closed-Form based Search (CFS) algorithm shown
a classiﬁer that achieves value +
in Algorithm 1. Denote CFS
using Eq. (5), i.e.,

and 2⇤ (

'
+ ⇤/

= max

+
(

(5)

, 0

 

1

}

}

(

)

)

)

)

(

.

.

CFS

5

+
(

)(

)

= max

1

{

 

', 0

+ ⇤/

.

}

(6)

5

)

(

]

)

'

))

50

CFS

+ , +
[

. Initially, we set + = 0 and + = max5

The Algorithm 1 uses bisection method to locate + ⇤ within an in-
terval
. Value
2F
that classiﬁes each 50 with
+ = 0 is achieved by a classiﬁer CFS
+
(
)
> 0 with probability CFS
+
'
5
= 1 and thus possibly has
(
(
)(
)
> q. In contrary, value
excessive false-positive rate  ⇡ (
+
CFS
(
+ is achieved by a classiﬁer CFS
+
which classiﬁes all events as
(
benign. To minimize the attacker’s utility, we search for + ⇤ such that
false-positive rate of classiﬁer CFS
is maximized and satisﬁes
q.
 ⇡ (

)
+ ⇤)
(
In each iteration, the algorithm considers the middle point ˆ+ of the
and inspects the false positive

+ , +
[
. If the false-positive rate  ⇡ (

currently considered interval
rate of the classiﬁer CFS
))
of the classiﬁer higher than the desired rate q, the value ˆ+ cannot
be achieved within this false positive constraint. Hence the value
of the game has to be higher (i.e., better for the attacker) and the
on line 3. On the contrary, when
algorithm searches interval
]
the false-positive constraint is less than q, the algorithm searches the
trying to minimize the value of the game. When
other interval

+ ⇤)) 
(

ˆ+, +
[

ˆ+
(

ˆ+
(

CFS

)

]

+ , ˆ+
[

]

Algorithm 1: Closed-Form based Search (CFS)

sets of actions (e.g., from [23]):

1 while +
2

 
ˆ+
 (
if  ⇡ (
else +
4
5 return CFS

3

+
 
+
+
CFS

n do
+
2
)/
+ ⇤))
(
ˆ+

+
(

))

> q then +

ˆ+

s.t.:

¯50

(8

2F 3 )

the desired n-approximation of + ⇤ is found, the algorithm returns
classiﬁer CFS

.

+
(

)

5 GENERAL OBSERVATION

TRANSFORMATION FUNCTION

The key assumption of the previous case is that the defender perfectly
observes the action of the attacker 50, the defender thus exactly
knows the reward of the attacker '
, and can apply Eq. (5) to
calculate the probability with which the event has to be inspected.
We now describe the case with a general observation transformation
function
where the action of the attacker (and thus neither the
reward) is observable by the defender and the approach described in
the previous section cannot be applied.

50

T

)

(

Following our example, we primarily focus on the observation
transformation function
that corresponds to a sum of effects of
actions of the attacker (50) and the user (51 ). In this case, the actions
%⇡
of the attacker 50 are additively combined with the actions 51 ⇠
of the legitimate users, i.e.,

T

5 = 50

51 |

50

]

= Pr
%⇡ [

51 ]

.

+

Pr
T

[

(7)

Note that the behavior of the legitimate user is stochastic and unpre-
=
dictable. The defender observes only a single data point
50
51 and cannot easily infer the action of the attacker 50 since the
action of the user 51 is also not known and only the distribution %⇡
is known.

T (

50

+

)

In this section, we propose two algorithms to solve classiﬁcation
games with general observation transformation functions
– one
based on the discretization of action and feature spaces and linear
programming, and the second one that approximates the optimal
strategy using a neural network.

T

min
¯5> 2F3 :2
¯50

+

¯5> )

(

Pr

8
: ˆ'

(

)

(8)

¯5>

¯50

ˆ
T (

|

[

1

2

(

 

)] (

¯5>

)) 

+

(9)

’¯5> 2F3
¯5>
Pr

[

2

%⇡ ]

|

(

¯5>

)

q

(10)

(11)

¯5>

(8

2F 3 )

’¯5> 2F3
2
: 0


¯5>

(

)

1,

(

There are two types of variables – + , representing the value of the
¯5>
discretized game that as expected utility of the attacker, and 2
)
for each ¯5>
2F 3 representing a probability with which an even
from the grid cell ¯5> will be inspected by the defender. Constraints
(9) correspond to best responses of the attacker, thus an expected
value for each action of the attacker ¯50 is less or equal + . Since
the feature space is discretized, we assume the attacker is choosing
such an action from a grid cell ¯50 that maximizes his reward 5 <
0 =
= '
arg max5 00 2
. The
strategy of the defender is a probability distribution over each grid
cell to inspect, hence the values are bounded (11) and constraint (10)
restricts the defense strategy to allow the maximal false-positive rate
of q. While LP can be solved in polynomial time w.t.r. to the size of
the input, in our case the input matrix size grows exponentially with
dimensions =.

, thus ˆ'

5 <
0 )

5 <
0 )

and ˆ

5 00 )

T (

T (

¯50

¯50

¯50

=

'

)

(

)

(

(

There are two main disadvantages of the LP-based approach.
First, the scalability of the method is limited due to the size of the
program. While this can be to some extent tackled by the incremental
generation of constraints and/or variables, the exponential size of
the LP is unavoidable in general. Second, the LP in the present form
overﬁts the benign data. If the input for the program is a sample of
data (in case we do not know the exact distribution), the strategy
computed using the linear program and data samples ⇡ can violate
the false-positive constraints on a different data sample ⇡ 0 from the
same distribution.

To overcome this issue, we approximate the distribution %⇡ from
a data sample ⇡ using Kernel Density Estimation (KDE) with ker-
nel bandwidth parameter ⌘. We ﬁnd the best value for the hyper-
parameter ⌘ using a binary search. Consequently, the approximated
distribution is used for a false-positive constraint (10).

5.1 Solving the Generalized Classiﬁcation Game -

5.2 Exploitability Descent for Adversarial

Linear Programming Approach (LP)

Anomaly Detection (EDA)

We ﬁrst present the algorithm that is a natural extension of previous
algorithms that solve the discrete cases [10]. The algorithm assumes
a discretized feature space such that each dimension (feature) is
divided into 3 uniform steps (subintervals). We denote
F3 the set
of all the subintervals of the feature space (or grid cells). We use
bar notation ¯5
to denote one element of the discretized set
⇢F
¯5
such that 5
2F 3 . Discretization of the feature space modiﬁes
the game such that both players have a ﬁnite number of actions –
the action of the attacker corresponds to choosing a grid cell, and
the defender chooses a probability of inspection of each grid cell.
denote the probability density in a cell ¯5 given the
Let Pr
probability distribution  . Now, we can formalize a linear program
based on standard LP for solving one-shot (matrix) games with ﬁnite

¯5
|

 

2

[

]

We now turn to the main algorithm inspired by exploitability de-
scent [17]. We solve the min-max problem in Eq. (2a) iteratively by
conducting two phases at each iteration: ﬁrst, we estimate the best
response of the attacker 50 to the current classiﬁer 2 and, second,
we update the classiﬁer 2 with stochastic gradient descent while
taking into account the false-positive rate constraint. We model the
classiﬁer with a neural net 2\ which is parametrized by the weights
\ .

The adversarial nature of the problem makes the classiﬁer tend
to output inspection probabilities of the attack close to 1 and the
inspection probabilities of the benign samples close to 0. Such values
may cause the vanishing gradients problem due to the sigmoid in the
last layer of the neural net [15, 20]. To partially overcome this issue,

 
 
Algorithm 2: Exploitability Descent for Adversarial Anom-
aly Detection (EDA).
Input: ˆ+,q

initial random NN
1.0, 8
0

1 \ 0
2 _0
3 while termination condition is not met do
4

50  
ˆq8
8

L
\ 8
_8
8

+
1

+

)
)

28
⌫'0 (
28
 ⇡ (
E5
50
) [
⇠T (
\ 8
U
r\ L
 
0,_ 8
max
+
1

{

   
1

8

+

$DC?DC⇠;0BB8 5 84A

5

6

7

8

9
10 \

Output: \

28

(

5

(

)) ] +

_8

ˆq8

(

q

)

 

log
8

V

r_ L

8

}

\ 0,\ 1, . . .

( {

,q
}

)

we conveniently construct an upper bound of the outer minimization
problem of (2a) by taking the logarithm of the criterion and minimize
the upper bound instead.

Objective. Given a sample of data ⇡ and a best response 50

2
, the optimal classiﬁer 2\ ⇤ is obtained by solving the follow-

⌫'0
2\ )
(
ing task:

,

5

 

5

)

1

q

'

E

 

50

⇠T (

50)

(13)

(12)

min
⇥
\
2

·
(
2\ )

2\ (
[
)]
 
subject to:  ⇡ (
Once we compute the best response 50 in the ﬁrst phase of each
iteration of EDA, we consider it is ﬁxed. Therefore, in the second
phase, the reward '
is not a function of the weights \ and it
can be omitted from the criterion. This gives a simpliﬁed objective
. Note that the simpliﬁed objective is minimized
 
by the same optimal classiﬁer classiﬁer 2\ ⇤ as the following crite-
rion
due to monotonicity of logarithm. Per
Jensen’s inequality, we construct the upper bound on the simpliﬁed
objective.

2
50) [

log E5

2\ (

50) [

⇠T (

⇠T (

)])

E5

50

)]

 

(

)

(

5

5

log

5

 

E

⇠T (

50)

5

2\ (
[

)])   

5

E

⇠T (

[

50)

log

5

2\ (
(

))]

(14)

We further use Lagrangian relaxation procedure (described in
Chapter 3 [4, 7]) to move the false-positive rate constraint into the
objective to obtain unconstrained problem as follows:

max
_
0
 

min
\
2

⇥  

5

E

⇠T (

[

50)

log

5

2\ (
(

))] +

_

 ⇡ (

2\ ) 

· (

q

)

(15)

Pseudocode. The pseudocode of EDA is presented in Algorithm 2.
The algorithm takes false-positive rate threshold q and a set of benign
data ⇡ as an input. The neural network 2\ is parameterized by \ . The
output of the algorithm is an optimal classiﬁer 2\ ⇤ The parameters \
and _ are updated in each iteration.

At each iteration, the algorithm estimates the best response of
the attacker (line 4) against the current classiﬁer 2\ 8 and the current
false-positive rate ˆq8 given the benign data ⇡ (line 4). Then, as in
8 is constructed by combining the upper
Eq. (15), the Lagrangian
bound and the constraint (line 6). Finally, we update the weights

L

\8 with a gradient descent step (line 7) and the multiplier _8 with a
gradient ascent step (line 8).

5.3 Selecting the Final Classiﬁer
At each iteration of the algorithm, a new classiﬁer 28 is constructed.
Since the false-positive constraint is not strictly enforced during the
learning, the false positive rate ˆq8 of some of the classiﬁers 28 may ex-
ceed the desired false-positive rate q, and thus become unacceptable
as a solution of the problem. Although such classiﬁers exceed the
threshold on false-positives, they may achieve a good value against
the best response of the attacker D0
—and disregarding
them completely can be wasteful. We show, however, that multiple
classiﬁers can be linearly combined to obtain a new classiﬁer that
matches the desired false-positive rate, and that provides a superior
classiﬁcation performance.

28,⌫' 0
(

28
(

))

2 9
(

PROPOSITION 5.1. Let 28 and 2 9 be two classiﬁers such that
28,⌫' 0
the best-responding attacker achieves utilities D0
and
(
2 9 ,⌫' 0
, respectively. Let _
D0
and consider a classiﬁer
))
(
28 9
, i.e., 28 9
_ such that 28 9
= _28
5
1
_ is a convex
_ (
(
combination of classiﬁers 28 and 2 9 . Then 28 9
_ satisﬁes
28 9
28
= _ ⇡ (
(1)  ⇡ (
, and
_ )
28 9
28 9
_ ,⌫' 0
(2) D0
_ )) 
(
(

2 9
 ⇡ (
_
)
 
28
28,⌫' 0
(
(

2 9 ,⌫' 0
(

) + (
_D0

0, 1
]
2 9
_

D0
)

)) + (

2 9
(

2[
 

28
(

) + (

))

))

 

_

1

1

)

(

)

)

)

5

5

.

PROOF. The classiﬁer 28 9

_ corresponds to rolling dice before clas-
sifying and using the classiﬁer 28 with probability _ (and 2 9 other-
wise). Hence the false-positive rate of 28 9

_ satisﬁes

 ⇡ (

28 9
_ )

= E
5

2
%⇡ [
⇠
2
%⇡ [
⇠

) |

(

5

5

) |
(
2 = 28

= E
5

2 = 28 9
_ ]
2 = 28
[

%

]

]
2 = 2 9

+

5

5

2
E
%⇡ [
⇠
5

(
2 = 28

) |

(

) |

1

_

)

 

) + (

1

] + (
2 9
 ⇡ (

)

= _ E
5

2
%⇡ [
⇠
28
= _ ⇡ (

%

2 = 2 9
[

]

]

_

)

5

2
E
%⇡ [
⇠

5

(

) |

2 = 2 9

]

 

.

Similarly, if the attacker knew which one of the classiﬁers is used
(i.e., the result of the dice roll), he would have been able to play the
2 9
best responses ⌫'0
, respectively, to achieve utility
(
)
2 9 ,⌫' 0
28,⌫' 0
_D0
. Since he does not have
(
(
access to this information, his utility can only worsen, which proves
⇤
(2).

and ⌫'0
D0
_
)

28
(
1
)) + (

2 9
(

28
(

)
 

))

Based on Proposition 5.1, we compute the resulting classiﬁer by
ﬁnding the optimal pair of classiﬁers 28 and 2 9 , and the optimal coefﬁ-
28
cient _ of convex combination, such that _ ⇡ (
28,⌫' 0
_
q and the upper bound _D0
1
 
(
the attacker’s utility is minimized.

_
1
) + (
 
)
2 9 ,⌫' 0
D0
(
)

2 9
 ⇡ (
2 9
(

)
on

)) + (

28
(

))

We represent the performance of the classiﬁers in the form of a
2-dimensional Pareto frontier, where each classiﬁer 28 is assigned a
point
. We also use this Pareto frontier to
establish the termination condition of the Algorithm 2. Speciﬁcally,
we terminate the algorithm when the Pareto frontier does not change
for   iterations, where we use   = 50 in the experiments.

28,⌫' 0
(

28
,  ⇡ (

D0
(

28
(

))

))

 
 
 
 
 
 
 
 
Learning Rates. Finally, we discuss the learning-rate steps U and
V for updating weights of the neural network and lambda, respec-
tively. We chose the values according to condition (12) from [7] such
that the step decreases faster for the neural network and for _. Since
we use the same learning rates for neural networks of various sizes,
we use a simple restarting mechanism whenever the false-positive
value remains constant and within the distance of 0.01 from either
value 0 or 1 for ! = 500 iterations. In this case, the whole algorithm
is restarted and the constants of learning rates are decreased by 10%
while still satisfying the condition (12).

6 EXPERIMENTS
We now turn to the experimental evaluation of the algorithms. We
ﬁrst focus on synthetically generated data to examine the quality of
computed strategies with varying parameters of the problem. We
follow by a real-world scenario where data correspond to DNS
requests and the goal is to identify anomalies caused by an attacker
exﬁltrating data using DNS. All algorithms were run on Intel Xeon
Scalable Gold 6150 (2.7GHz) with 20 GB memory limit and 24
hours time-limit per instance. We use PyTorch to train the neural
networks and Gurobi solver to solve LP problems.

In the experiments, we always consider three data sets: training
⇡CA08= (50% of all data points), validation ⇡E0; (25% data points)
and testing ⇡C4BC (25% data points). The training set is used to train
the model, the validation set to ﬁnd the model hyperparameters
and thresholds (e.g., KDE bandwidth parameter ⌘, thresholds for
classical anomaly detectors, etc.). Finally, the testing data set is used
for the ﬁnal performance evaluation (and constitutes the reported
results). We consider false-positive rate threshold q = 0.05.

Parameters for EDA. Based on the experimental evaluation, we
use the following architecture of neural networks for the EDA al-
gorithm. The neural networks consist of 3 fully-connected layers
2=, and 1 (output) neurons, where = is the
with 32
dimension of the feature space. We use the ReLU activation function
for all neurons except the last one, where the sigmoid function is
used.

2=, 32

2=, 16

+

+

+

.

)

)

)

2

2
(

2
(

We estimate the best response ⌫'0

of the attackers (line 4 of
the Algorithm 2) by sampling a set ( of 256= attacker’s actions from
( is then improved using hill-climbing
2
( with the highest reward of the attacker

. Each sampled action 50
F
method and the sample 50
D0

is used as an estimate of ⌫'0

2, 50
(
Method for Comparing Different Classiﬁers. We are interested
in the quality of classiﬁers that is determined by the exploitability
– what is a value of the best response of the attacker against the
computed model. Due to the size of the problem, the best response
cannot be solved optimally and we can only estimate the value of a
best response. To do so, we sample the feature space with a given
= samples for the evaluation on synthetic
number of samples (2, 000
data, 30, 000 samples for the real-world data) and select the best
value for the attacker (subsequent stochastic gradient descent is
not used for this evaluation due to the discretization used for LP-
based methods). Such a heuristic estimation of a best response does
not have any theoretical guarantees. Therefore we experimentally
evaluate the variance of the sampled best response. In Figure 1
we present a histogram of values of 10, 000 estimations based on

·

Figure 1: Evaluation of variance of sampled best response with
30, 000 samples used for data exﬁltration experiments.

alg

1
1,000
0.1
3,241

2
80
5.6
3,719

=
\
LP 3
LP [s]
EDA [s]

3
12
247.8
4,782
Table 1: Algorithms run time (in seconds) for different number
of dimensions =. For LP, we used discretization 3 according to
the table.

5
4
1,885.7
5,935

10
-
N/A
12,571

4
5
424.3
5,967

sampled best response with 30, 000 samples used for experiments
in Section 6.2. The results show that the variance of the sampled
best response is rather small as the lowest value within these 10, 000
estimations was 58.37 while the highest estimation was 58.75.

6.1 Evaluation on Synthetic Benign Data
We use synthetic benign data to analyze our game-theoretic models
and determine and compare the performance of described algorithms
for varying parameters, focusing primarily on the dimension of the
feature space. We create the datasets of benign data by sampling
each coordinate 5 8
1 , . . . ,5 =
5 1
from normal
1 )
. Means `8 are sampled from uniform distri-
distribution
and the standard deviation f8 is sampled from a
bution *
uniform distribution *
. By repeating the process (for ﬁxed
`8 and f8 ), we generate a dataset of 1000= samples. We consider that
the reward function ' of the attacker is a sum of individual features
from the feature vector 50, '
=

1 of a feature vector 51 =
`8,f 8

N (
1.6, 8.4

1, 1.25

50

)

(

(

(

)

)

=
8=0 5 8
0 .

(

)

Õ

6.1.1 Algorithm Scalability. We ﬁrst examine the scalability
of the methods with the increasing dimension of the feature space.
Table 1 presents the mean running times (5 runs). The number of
discrete steps in each dimension is decreasing for the LP-based
method in order to compensate for the exponential number of vari-
ables/constraints. The results show that while LP-based method
is faster for smaller dimensions, the computation time required to
construct the LP is increasing with the dimensions due to the com-
putation of the density w.r.t. to KDE that is necessary for keeping
the false-positive rate close to the desired value. Note that we were
unable to run the LP-based method for = = 10 within 24 hours dead-
line. On the other hand, EDA was able to converge in all instances
within the time limit, including the case with dimension = = 20
that took slightly more than 14 hours to converge on average. Note
however that the majority of the computation time of EDA was
spent in the best-response computation. Already for 3 dimensions,

Identity

General

LP[%]

LP[%]

=
1
2
3
4
5
10

4.98
4.95
5.03
4.74
4.95

T
EDA[%]
0.45
4.96
0.30
5.02
0.26
5.11
0.18
4.84
0.27
4.99
0.07
5.06
Table 2: Mean false-positive rates for different observation
transformation functions

T
EDA[%]
0.68
5.19
0.91
4.76
0.61
5.54
0.49
5.06
0.41
4.91
0.28
5.09

±
±
±
±
±
N/A

±
±
±
±
±
N/A

5.27
4.51
6.69
4.24
4.72

1.12
1.22
2.31
1.71
5.62

0.46
0.19
0.24
0.11
0.21

±
±
±
±
±
±

±
±
±
±
±
±

.

T

computation of best responses takes more than 93% of time and
the fraction is further increasing for higher dimensions. By imple-
menting a problem-speciﬁc best-response algorithm or using parallel
(possibly GPU-based) best-response computation, the computation
time of EDA can be signiﬁcantly reduced.

6.1.2 Solution Quality. Next, we compare the quality of com-
puted classiﬁers. Since each randomly-generated instance may have
a different game value, we use the concept of relative regret to com-
pare the performance of the classiﬁers across different instances. We
deﬁne the relative regret of two classiﬁers 2 and 210B4 as
⌫'0
210B4 )
(
210B4 )
(
. The value RelReg(2,2 10B4 ) states

2,2 10B4 )
(
D0

2
(
) 
⌫'0

where ⌫'0

RelReg

= 100

⌫'0

(16)

,

2
(

)

= max
50 2F

2, 50
(

)

how much the defender regrets choosing classiﬁer 2 compared to
210B4 . In case the value is negative, the quality of classiﬁer 2 is better
compared to the baseline.

Identity Transformation Function. First, we compare EDA and
LP in a simple scenario where the observation transformation func-
tion is an identity. In this case, we can closely approximate the
optimal value using CFS. Figure 2(left) depicts relative regrets of
LP and EDA compared to CFS. With the increasing dimension, the
problem gets more complicated, thus the relative regret for each of
the algorithm increases. However, while for the EDA the increase is
not that dramatic and the mean value even for the feature space of
dimension 20 is 6.46%, the relative regret for the LP-based method
increases more dramatically, and even for dimension 5 exceeds 15%.
Next, we compare what is the false-positive rate of the algorithms
on testing data. Table 2 shows the results, and both algorithms are
able to ﬁnd a classiﬁer that keeps the desired false-positive rate close
to 0.05.

General Transformation Function. For the more general case,
we are unable to approximate the value of the game closely. Thus
we compare only two algorithms – EDA and the LP-based method.
Figure 2(right) depicts the relative regret showing that EDA produces
signiﬁcantly better classiﬁers compared to the LP-based method (the
best response value of the attacker against EDA is signiﬁcantly
lower compared to the value against LP-based strategy). The main
reason is that the discretization is much more coarse for higher
dimensions. Moreover, since the LP has a hard constraint on the
false-positive rate, the probability of detection for large cells created
by the discretization has to be low and the attacker can exploit it
more and achieve a better value. The second reason is that using
KDE smoothing of input data can increase the density of benign

EDA
PCA
IF
KNN
CBLOF

D0⇤
58.75
85.83
86.74
87.38
131.00

(

'
50⇤ )
146.69
167.17
217.94
188.22
231.61

Table 3: The performance of different anomaly detectors for
the exﬁltration experiment. The models are sorted by attacker’s
best responce utility D0⇤ .

data in some cells compared to original data. Comparison of the
false-positive rates in Table 2 shows that due to coarse discretization
in LP and transformation function, the EDA method is signiﬁcantly
better in maintaining the desired false-positive rate on testing data.

6.1.3 EDA Convergence. Figure 3 presents a typical learning
of EDA during the training. There are two clear trends – ﬁrst, the
algorithm converges fairly quickly and already in iteration 250 the
best response of the attacker decreases very close to the ﬁnal value.
Note that afterward, both the best response value of the attacker
and the false-positive rate oscillate in opposite directions. This is
expected since with increased false-positive rate the classiﬁer is less
exploitable and vice versa. Such an oscillation generates many points
for the Pareto frontier from which the ﬁnal classiﬁer is selected (see
Section 5.3).

6.2 Case Study: Comparison with Classical

Anomaly Detectors over Data Exﬁltration
Problem

As the ﬁnal experiment, we compare the performance of the EDA
method in a real-world setting against standard anomaly detectors.
We focus on the data exﬁltration case via DNS protocol and use
anonymized real-world 20,000 DNS queries. When the attacker
is exﬁltrating data via DNS queries, he splits data into parts and
repeatedly encodes part of data as a part of the query (e.g., a third-
level in a domain). We aggregate queries in a 20s time interval based
on a common second-level domain. For simplicity, we consider only
three features for this problem – the sum of lengths of all queries in
a group within the time interval, the sum of entropy, and the number
of special symbols. Reward function of the attacker should correlate
with the amount of exﬁltrated data and thus we approximate it as a
multiplication of the ﬁrst two feature values while the third feature
does not affect the reward function.

We compare EDA with traditional anomaly detectors. We have
chosen four well-known models: Principal Component Analysis
(PCA), Isolation Forest (IF), K-nearest neighbors with largest (KNN)
distances to the k-th neighbor as the outlier score and cluster-based
local outlier factor (CBLOF) algorithm. We have used PyOD toolkit
[30] with the implementations of these models.

Table 3 shows the results of all the classiﬁers sorted based on the
performance against the best responding opponent (sample-based
best response with 30, 000 samples). The results show that even in
such a simple scenario, our EDA method outperforms all standard
non-adversarial methods. The best non-adversarial anomaly detector,
PCA, is much more exploitable compared to EDA and the expected

Figure 2: (left) Relative regret of LP and EDA compared to the optimal game-values computed with CFS identity transformation
function

; (right) Relative regret of EDA compared to LP for general transformation function

.

T

T

exploitability descent, can be used for adversarial anomaly detec-
tion but also adversarial machine learning in general and thus opens
possibilities of adapting this approach to other domains.

REFERENCES
[1] Ahmed, M.; Mahmood, A. N.; and Islam, M. R. 2016. A survey of anomaly
detection techniques in ﬁnancial domain. Future Generation Computer Systems
55:278–288.

[2] Athalye, A.; Carlini, N.; and Wagner, D. 2018. Obfuscated gradients give a false
sense of security: Circumventing defenses to adversarial examples. In International
Conference on Machine Learning, 274–283.

[3] Barreno, M.; Nelson, B.; Joseph, A. D.; and Tygar, J. D. 2010. The security of

machine learning. Machine Learning 81(2):121–148.

[4] Bertsekas, D. 1999. Nonlinear programming. Athena Scientiﬁc optimization and

computation series. Athena Scientiﬁc.

[5] Brückner, M., and Scheffer, T. 2011. Stackelberg games for adversarial prediction
problems. In Proceedings of the 17th ACM SIGKDD international conference on
Knowledge discovery and data mining, 547–555. ACM.

[6] Brückner, M.; Kanzow, C.; and Scheffer, T. 2012. Static prediction games for
adversarial learning problems. Journal of Machine Learning Research 13(Sep):2617–
2654.

[7] Chow, Y.; Ghavamzadeh, M.; Janson, L.; and Pavone, M. 2017. Risk-constrained
reinforcement learning with percentile risk criteria. The Journal of Machine Learning
Research 18(1):6070–6120.

[8] Dini, G.; Martinelli, F.; Saracino, A.; and Sgandurra, D. 2012. Madam: a multi-level
anomaly detector for android malware. In International Conference on Mathemat-
ical Methods, Models, and Architectures for Computer Network Security, 240–253.
Springer.

[9] Dritsoula, L.; Loiseau, P.; and Musacchio, J. 2017. A game-theoretic analysis of
adversarial classiﬁcation. IEEE Transactions on Information Forensics and Security
12(12):3094–3109.
[10] Durkota, K.; Lisý, V.; Kiekintveld, C.; Horák, K.; Bošanský, B.; and Pevný,
T. 2017. Optimal strategies for detecting data exﬁltration by internal and external
attackers. In International Conference on Decision and Game Theory for Security,
171–192. Springer.
[11] Eykholt, K.; Evtimov, I.; Fernandes, E.; Li, B.; Rahmati, A.; Xiao, C.; Prakash,
A.; Kohno, T.; and Song, D. 2017. Robust physical-world attacks on deep learning
models. arXiv preprint arXiv:1707.08945.

[12] Garcia-Teodoro, P.; Diaz-Verdejo, J.; Maciá-Fernández, G.; and Vázquez, E. 2009.
Anomaly-based network intrusion detection: Techniques, systems and challenges.
computers & security 28(1-2):18–28.

[13] Goodfellow, I. J.; Shlens, J.; and Szegedy, C. 2014. Explaining and harnessing

adversarial examples. arXiv preprint arXiv:1412.6572.

[14] Huang, L.; Joseph, A. D.; Nelson, B.; Rubinstein, B. I.; and Tygar, J. 2011.
Adversarial machine learning. In Proceedings of the 4th ACM workshop on Security
and artiﬁcial intelligence, 43–58. ACM.

[15] Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet classiﬁcation
with deep convolutional neural networks. In Pereira, F.; Burges, C. J. C.; Bottou, L.;
and Weinberger, K. Q., eds., Advances in Neural Information Processing Systems 25.
Curran Associates, Inc. 1097–1105.

[16] Lisý, V.; Kessl, R.; and Pevný, T. 2014. Randomized operating point selection in
adversarial classiﬁcation. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, 240–255. Springer.

[17] Lockhart, E.; Lanctot, M.; Pérolat, J.; Lespiau, J.-B.; Morrill, D.; Timbers, F.; and
Tuyls, K. 2019. Computing approximate equilibria in sequential adversarial games by
exploitability descent. arXiv preprint arXiv:1903.05614.

[18] Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and Vladu, A. 2017. Towards
deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083.
[19] Moosavi-Dezfooli, S.-M.; Fawzi, A.; and Frossard, P. 2016. Deepfool: a simple
In Proceedings of the IEEE

and accurate method to fool deep neural networks.

Figure 3: Visualisation of the convergence of variable _, current
false positive rate ˆq8 , and current best response value estima-
tion D0 for EDA during learning (the convergence is for the case
study scenario).

(

50⇤ )

best response utility of the attacker is almost 150% of the value
against EDA. Moreover, the second column shows the reward values
'
for the best-response action of the attacker – the higher the
value, the better is the action for the attacker and this is the reward
if this particular action is not investigated by the anomaly detector.
Since EDA takes goals of the attacker into consideration, the classi-
ﬁer forces the attacker to choose best response actions with lower
reward.

7 CONCLUSION
We present a novel game-theoretic model for detecting anomalous
events caused by the attacker. There are four key characteristics of
our approach: the features considered by the anomaly detector can be
continuous, there is a hard constraint for the expected false-positive
rate of the detector, the defender cannot fully observe the action of
the attacker, and the goal is to solve problems with multidimensional
feature space. To address the computational challenges, we adapt
a recent algorithm that trains a randomized classiﬁer based on a
neural network in order to minimize the exploitability. Experimental
results show that our algorithm scales to larger dimensions com-
pared to existing LP-based methods and also outperforms standard
anomaly detectors on real-world DNS data exﬁltration problem. Our
work demonstrates that using game-theoretic algorithms, such as

conference on computer vision and pattern recognition, 2574–2582.

arXiv:1312.6199.

[20] Nair, V., and Hinton, G. E. 2010. Rectiﬁed linear units improve restricted boltz-
mann machines. In Proceedings of the 27th International Conference on International
Conference on Machine Learning, ICML’10, 807–814. USA: Omnipress.

[21] Nelson, B.; Barreno, M.; Chi, F. J.; Joseph, A. D.; Rubinstein, B. I.; Saini, U.;
Sutton, C.; Tygar, J.; and Xia, K. 2009. Misleading learners: Co-opting your spam
ﬁlter. In Machine learning in cyber trust. Springer. 17–51.

[22] Perolat, J.; Malinowski, M.; Piot, B.; and Pietquin, O. 2018. Playing the game of

universal adversarial perturbations. arXiv preprint arXiv:1809.07802.

[23] Shoham, Y., and Leyton-Brown, K. 2008. Multiagent systems: Algorithmic,

game-theoretic, and logical foundations. Cambridge University Press.

[24] Sommer, R., and Paxson, V. 2010. Outside the closed world: On using machine
learning for network intrusion detection. In 2010 IEEE symposium on security and
privacy, 305–316. IEEE.

[25] Szegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan, D.; Goodfellow, I.;
Intriguing properties of neural networks. arXiv preprint

and Fergus, R. 2013.

[26] Thomas, K.; McCoy, D.; Grier, C.; Kolcz, A.; and Paxson, V. 2013. Trafﬁcking
fraudulent accounts: The role of the underground market in twitter spam and abuse. In
Presented as part of the 22nd USENIX Security Symposium (USENIX Security 13),
195–210.
[27] Vorobeychik, Y., and Kantarcioglu, M. 2018. Adversarial machine learning.

Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning 12(3):1–169.

[28] Wang, G.; Wang, T.; Zheng, H.; and Zhao, B. Y. 2014. Man vs. machine: Practical
adversarial detection of malicious crowdsourcing workers. In 23rd USENIX Security
Symposium (USENIX Security 14), 239–254.

[29] Wong, E., and Kolter, Z. 2018. Provable defenses against adversarial examples
via the convex outer adversarial polytope. In International Conference on Machine
Learning, 5283–5292.

[30] Zhao, Y.; Nasrullah, Z.; and Li, Z. 2019. Pyod: A python toolbox for scalable

outlier detection. Journal of Machine Learning Research 20(96):1–7.

