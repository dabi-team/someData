9
1
0
2

t
c
O
2

]
E
N
.
s
c
[

1
v
5
4
9
0
0
.
0
1
9
1
:
v
i
X
r
a

Optimising Optimisers with Push GP

Michael A. Lones∗

Abstract

This work uses Push GP to automatically design both local and population-based optimisers
for continuous-valued problems. The optimisers are trained on a single function optimisation
landscape, using random transformations to discourage overﬁtting. They are then tested for
generality on larger versions of the same problem, and on other continuous-valued problems. In
most cases, the optimisers generalise well to the larger problems. Surprisingly, some of them also
generalise very well to previously unseen problems, outperforming existing general purpose opti-
misers such as CMA-ES. Analysis of the behaviour of the evolved optimisers indicates a range of
interesting optimisation strategies that are not found within conventional optimisers, suggesting
that this approach could be useful for discovering novel and eﬀective forms of optimisation in an
automated manner.

1 Introduction

This work is motivated by two issues. First, due to the innate constraints and biases of human
thought, it is likely that manual design of optimisers only explores a subspace of optimiser design
space. It is unlikely that this subspace contains optimal optimisers for all optimisation problems.
Second, recent attempts to create novel optimisers from models of natural systems have been largely
unsuccessful in broadening the scope of optimiser designs, instead tending only to generate variants
of existing metaheuristic frameworks [Sörensen, 2015, Lones, 2019b].

This work attempts to address both of these issues by using Genetic Programming (GP) to
explore the broader space of optimisation algorithms, with the aim of discovering novel optimisation
behaviours that diﬀer from those used by existing algorithms. In order to make the optimiser search
space as broad as possible, a Turing-complete language, Push, is used to represent the optimisers,
and the Push GP system is used to optimise them [Spector, 2001]. In [Lones, 2019a], this approach
was used to evolve local optimisers that can solve continuous-valued problems. In this work, this
approach is extended to the population-based case, using Push GP to automatically design both
local and population-based optimisers from primitive instructions.

The paper is organised as follows. Section 2 reviews existing work on the automated design
of optimisers. Section 3.1 gives a brief overview of the Push language and the Push GP system,
section 3.2 describes how Push GP has been modiﬁed to support the evolution of population-based
optimisers, and section 3.3 outlines how the optimisers are evaluated. Section 4 presents results and
analysis. Section 5 concludes.

∗School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh, UK, m.lones@hw.ac.uk.

1

 
 
 
 
 
 
2 Related Work

There is a signiﬁcant history of using GP to optimise optimisers. This can be divided into two
areas: using GP to optimise GP, and using GP to optimise other kinds of optimiser. The former
approaches use a GP system to optimise the solution generation operators of a GP framework
[Edmonds, 1998, Kantschik et al., 1999, Spector, 2001]. Autoconstructive evolution [Spector, 2001]
is a particularly open-ended approach to doing this in which programs contains code that generates
their own oﬀspring; also notable is that, like our work, it uses the Push language.

However, more relevant is previous work on using GP to optimise non-GP optimisers. Much of
this work has taken place within the context of hyperheuristics, which involves specialising existing
optimisation frameworks so that they are better suited to solving particular problem classes.
In
this context, GP has been used to re-design components of evolutionary algorithms, such as their
mutation [Woodward and Swan, 2012], recombination [Goldman and Tauritz, 2011] and selection
operators [Richter and Tauritz, 2018], with the aim of making them better adapted to particular
solution landscapes. Other hyperheuristic approaches have used GP to generate new optimisa-
tion algorithms by recombining the high-level building blocks of existing metaheuristic frameworks
[Oltean, 2005, Martin and Tauritz, 2013, Ryser-Welch et al., 2016, Bogdanova et al., 2019]. Recently,
this kind of approach has also been used to explore the design space of swarm algorithms, using
grammatical evolution to combine high-level building blocks derived from existing metaheuristics
[Bogdanova et al., 2019]. Our approach diﬀers from this, and previous work in hyperheuristics,
in that it focuses on designing optimisers largely from scratch. By not reusing or building upon
components of existing optimisers, the intention is to reduce the amount of bias in the exploration
of optimiser design space, potentially allowing the exploration of previously unexplored areas.

Another recent development, which has some similarities to our work, is the use of deep learning
to optimise optimisers [Andrychowicz et al., 2016, Wichrowska et al., 2017, Metz et al., 2019]. So
far these approaches have focused on improving the training algorithms used by deep learners, i.e.
they are somewhat akin to using GP to optimise GP, though it is plausible that deep learning could
be applied to the task of designing optimisers for non-neural domains. However, this is arguably
an area in which GP is better suited than deep learning, since the optimisers produced by GP are
likely to be far more eﬃcient than those produced by deep learning. Eﬃciency is an important
consideration for optimisers, since the same code is typically called over and over again during the
course of an optimisation trajectory. Another advantage of GP is the relative interpretability of its
solutions when compared to deep learning, and the potential that more general insights could be
made into the design of optimisers by studying the code of evolved solutions.

3 Methods

3.1 Push and Push GP

In this work, optimisation behaviours are expressed using the Push language. Push was designed
for use within a GP context, and has a number of features that promote evolvability [Spector, 2001,
Spector and Robinson, 2002, Spector et al., 2004]. This includes the use of stacks, a mechanism
that enables evolving programs to maintain state with less fragility than using conventional indexed
memory instructions [Langdon, 2012]. However, it is also Turing-complete, meaning that it is more
expressive that many languages used within GP systems. Another notable strength is its type system,
which is designed so that all randomly generated programs are syntactically valid, meaning that
(unlike type systems introduced to more conventional forms of GP) there is no need to complicate
the variation operators or penalise/repair invalid solutions. This is implemented by means of multiple

2

Table 1: Psh parameter settings

Population size = 200
Maximum generations = 50
Tournament size = 5
Program size limit = maximum of 100 instructions
Execution limit = maximum of 100 instruction executions per move
Instructions = boolean/float/integer/vector.{dup flush pop rand rot shove stackdepth
swap yank yankdup}; boolean.{= and fromfloat frominteger not or xor}; exec.{=
do*count do*range do*times if iflt noop}; float.{% * + - / < = > abs cos erc exp
fromboolean frominteger ln log max min neg pow sin tan}; input.{inall inallrev
index}; integer.{% * + - / < = > abs erc fromboolean fromfloat ln log max min
neg pow}; vector.{* / + - apply between dim+ dim* dprod mag pop scale urand wrand
zip}; false; true

stacks; each stack contains values of a particular type, and all instructions are typed, and will only
execute when values are present on their corresponding type stacks. There are stacks for primitive
data types (booleans, ﬂoats, integers) and each of these have both special-purpose instructions (e.g.
arithmetic instructions for the integer and ﬂoat stacks, logic operators for the boolean stack) and
general-purpose stack instructions (push, pop, swap, duplicate, rot etc.) associated with them.
Another important stack is the execution stack. At the start of execution, the instructions in a
Push program are placed onto this stack and can be manipulated by special instructions; this allows
behaviours like looping and conditional execution to be carried out. Finally, there is an input
stack, which remains ﬁxed during execution. This provides a way of passing non-volatile constants
to a Push program; when popped from the input stack, corresponding values get pushed to the
appropriate type stack. Push programs are evolved using the Push GP system. Since Push programs
are basically a list of instructions, they can be represented as a linear array and manipulated using
genetic algorithm-like mutation and crossover operators.

3.2 Evolving Population-Based Optimisers

In order to evolve population-based optimisers, this work uses a modiﬁed version of Psh (http:
//spiderland.org/Psh/), a Java implementation of Push GP. To allow programs to store and
manipulate search points, an extra vector type has been added to the Push language. This repre-
sents search points as ﬁxed-length ﬂoating point vectors, and these can be manipulated using the
special-purpose vector instructions shown in Table 2; see [Lones, 2019a] for more details about these
instructions. Evolutionary parameters are shown in Table 1.

Algorithm 1 outlines the procedure for evaluating evolved Push optimisers. To reduce evolutionary
eﬀort, a Push program is only required to carry out a single move, or optimisation step, each time
it is called. In order to generate an optimisation trajectory within a given search space, the Push
program is then called multiple times by an outer loop until a speciﬁed evaluation budget has been
reached. After each call, the value at the top of the Push program’s vector stack is popped and
the corresponding search point is evaluated. The objective value of this search point, as well as
information about whether it was an improving move and whether it moved outside the problem’s
search bounds, are then passed back to the Push program via the relevant type stacks. Since the
contents of a program’s stacks are preserved between calls, Push programs have the capacity to build
up their own internal state during the course of an optimisation run, and consequently the potential

3

Instruction

Pop from

Push to Description

Table 2: Vector stack instructions

vector.+
vector.-
vector.*
vector./
vector.scale
vector.dprod
vector.mag
vector.dim+
vector.dim*
vector.apply
vector.zip
vector.between

vector.rand
vector.urand
vector.wrand

vector, vector
vector, vector
vector, vector
vector, vector
vector, ﬂoat
vector, vector
vector
vector, ﬂoat, int
vector, ﬂoat, int
vector, code
vector, vector, code
vector, vector, ﬂoat

ﬂoat

vector.current
vector.best

integer
integer

vector
vector
vector
vector
vector
ﬂoat
ﬂoat
vector
vector
vector
vector
vector

vector
vector
vector

vector
vector

Add two vectors
Subtract two vectors
Multiply two vectors
Divide two vectors
Scale vector by scalar
Dot product of two vectors
Magnitude of vector
Add ﬂoat to speciﬁed component
Multiple speciﬁed component by ﬂoat
Apply code to each component
Apply code to each pair of components
Generate point between two vectors

Generate random vector of ﬂoats
Generate random unit vector
Generate random vector within bounds

Get current point of given pop member
Get best point of given pop member

to carry out diﬀerent types of moves as search progresses.

To handle population-based optimisation, multiple copies of the Push program are run in paral-
lel, one for each population member. Each copy of the program has its own stacks, so population
members are able to build up their internal state independently. Population members are persistent,
meaning there is no explicit mechanism to create or destroy them during the course of an optimisa-
tion run. To allow coordination between population members, two extra instructions are provided,
vector.current and vector.best. These both look up information about another population mem-
ber’s search state, returning either its current or best seen point of search. The target population
member is determined by the value at the top of the integer stack (modulus the population size
to ensure a valid number); if this stack is empty, or contains a negative value, the current or best
search point of the current population member is returned. This sharing mechanism, combined with
the use of persistent search processes, means that the evolved optimisers resemble swarm algorithms
in their general mechanics. However, there is no selective pressure to use these mechanisms in any
particular way, so evolved optimisers are not constrained by the design space of existing swarm
optimisers.

3.3 Evaluation

Evolved optimisers are evaluated using a selection of functions taken from the widely used CEC 2005
real-valued parameter optimisation benchmarks. These are all minimisation problems, meaning that
the aim is to ﬁnd the input vector (i.e. the search point) that generates the lowest value when passed
as an argument to the function. The functions used during ﬁtness evaluation, which were selected
to provide a diverse range of optimisation landscapes, are:

• F1, the sphere function, a separable unimodal bowl-shaped function. It is the simplest of the

4

Algorithm 1 Evaluating an evolved Push GP optimiser

1: ﬁtness ← 0
2: for repeats do
pbest ← ∞
3:
for p ← 1, popsize do

4:

(cid:46) Measure ﬁtness over multiple optimisation runs

(cid:46) Initialise population state

prog p ← copy of evolved Push program
clearstacks(prog p)
point p ← random initial point within search bounds
value p ← evaluate(point p)
push(point p, prog p.vector)
push(value p, prog p.float)
push(true, prog p.boolean)
push(bounds, prog p.input)
bestval p ← value p
if bestval p < pbest then

(cid:46) Pass initial search point to program
(cid:46) Pass initial objective value to program

(cid:46) Put search space bounds on input stack

pbest ← bestval p, pbestindex ← p

end if

end for
for m ← 1, moves do

for p ← 1, popsize do

push(m, prog p.integer)
push(p, prog p.integer)
push(pbestindex, prog p.integer)
previous ← value p
execute(prog p)
point p ← peek(prog p.vector)
if point p is within search bounds then

value p ← evaluate(point p)
if value p < bestval p then

bestval p ← value p, best p ← point p

end if
if value p < previous then

push(true, prog p.boolean)

else

push(false, prog p.boolean)
push(best p, prog p.vector)

end if
push(value p, prog p.float)

else

push(false, prog p.boolean)
push(∞, prog p.float)

end if
if best p < pbest then pbest ← best p

end for

(cid:46) Pass move number to program
(cid:46) Pass population index to program
(cid:46) Pass index of pbest to program

(cid:46) Get next search point from program

(cid:46) Tell program it improved

(cid:46) Tell program it didn’t improve
(cid:46) and remind it of its best point

(cid:46) Pass new objective value

(cid:46) Or indicate move was out of bounds

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

23:

24:

25:

26:

27:

28:

29:

30:

31:

32:

33:

34:

35:

36:

37:

38:

39:

40:

41:

42:

43:

44:

end for
ﬁtness ← ﬁtness + pbest

45:
46: end for
47: ﬁtness ← ﬁtness/repeats

(cid:46) Mean of best objective values found in each repeat

5

benchmarks, and can be solved by gradient descent.

• F9, Rastrigin’s function, has a large number of regularly spaced local optima whose magnitudes
curve towards a bowl where the global minimum is found. The diﬃculty of this function lies in
avoiding the many local optima on the path to the global optimum, though it is made easier by
the regular spacing, since the distance between local optima basins can in principle be learnt.

• F12, Schwefel’s problem number 2.13, is multimodal and has a small number of peaks that can
be followed down to a shared valley region. Gradient descent can be used to ﬁnd the valley, but
the diﬃculty lies in ﬁnding the global mimimum, since it contains multiple irregularly-spaced
local optima.

• F13 is a composition of Griewank’s and Rosenbrock’s functions. This composition leads to a
complex surface that is highly multimodal and irregular, and hence challenging for optimisers
to navigate.

• F14, a version of Schaﬀer’s F6 Function, comprises concentric elliptical ridges. In the centre is
a region of greater complexity where the global optimum lies. It is challenging due to the lack
of useful gradient information in most places, and the large number of local optima.

To discourage overﬁtting to a particular problem instance, random transformations are applied to
each dimension of these functions when they are used to measure ﬁtness during the course of an
evolutionary run. Random translations (of up to ±50% for each axis) prevent the evolving optimisers
from learning the location of the optimum, random scalings (50-200% for each axis) prevent them
from learning the distance between features of the landscape, and random axis ﬂips (with 50%
probability per axis) prevent directional biases, e.g. learning which corner of the landscape contains
the global optimum. Fitness is the mean of 10 optimisation runs, each with random initial locations
and random transformations. The 10-dimensional versions of the problems are used for training,
with an evaluation budget of 1E+3 ﬁtness evaluations (FEs). For the results tables and ﬁgures shown
in the following section, the best-of-run optimisers are reevaluated over the CEC 2005 benchmark
standard of 25 optimisation runs, and random transformations are not applied.

4 Results

For a population-based optimiser, the 1E+3 evaluation budget can be split between the population
size and the number of iterations/generations in diﬀerent ways.
In these experiments, splits of
(population size × iterations) 50×20, 25×40, 5×200 and 1×1000 are used. The latter is included to
give a comparison against local search, i.e. optimisers which only use a single point of search. Fig.
1 shows the ﬁtness distributions over 50 evolutionary runs for each of these conﬁgurations, where
ﬁtness is the mean error when the best-of-run optimisers are reevaluated over 25 optimisation runs.
To give an idea of how these error rates compare to established general purpose optimisers, Fig. 1
also reproduces the mean errors achieved by two algorithms from the original CEC 2005 competition.
G-CMA-ES [Auger and Hansen, 2005] is a variant of the Covariance Matrix Adaptation Evolution
Strategy (CMA-ES) with the addition of restarts and an increasing population size at each restart;
it is a relatively complex algorithm and is generally regarded as the overall winner of the CEC 2005
competition. Diﬀerential Evolution (DE) [Ronkkonen et al., 2005], although less successful than
G-CMA-ES in the competition, is another example of a well-regarded population-based optimiser.
Fig. 1 compares the ability of Push GP to ﬁnd optimisers with diﬀerent trade-oﬀs between pop-
ulation size and number of iterations. The distributions show that this trade-oﬀ is more important

6

Figure 1: Fitness distributions across 50 runs for each problem. The value shown for each each run
is the mean ﬁtness of the best solution over 25 reevaluations. Published results for CMA-ES (blue)
and DE (green) are also shown.

7

1e−041e−021e+001e+021e+04F1_1x1000F1_5x200F1_25x40F1_50x20F1_1x1000F1_5x200F1_25x40F1_50x200.10.51.05.010.050.0100.0500.0F9_1x1000F9_5x200F9_25x40F9_50x20F9_1x1000F9_5x200F9_25x40F9_50x201e+035e+031e+045e+041e+055e+051e+06F12_1x1000F12_5x200F12_25x40F12_50x20F12_1x1000F12_5x200F12_25x40F12_50x201e+001e+021e+041e+06F13_1x1000F13_5x200F13_25x40F13_50x20F13_1x1000F13_5x200F13_25x40F13_50x203.84.04.24.44.64.85.05.2F14_1x1000_25082019F14_5x200_26082019F14_25x40_25082019F14_50x20_26082019F14_1x1000_25082019F14_5x200_26082019F14_25x40_25082019F14_50x20_26082019for some problems than others. For F1, better optimisers are generally found for smaller population
sizes, with the 1×1000 distribution having the lowest mean error. This makes sense, because the
unimodal F1 landscape favours intensiﬁcation over diversiﬁcation. For F12, the sweet spot appears
to be for 5×200, possibly reﬂecting the number of peaks in the landscape, i.e. 5. For the other
problems, the diﬀerences appear relatively minor, and eﬀective optimisers could be evolved for all
conﬁgurations. In most cases, the best optimiser for a particular problem is an outlier within the
distributions, so may not reﬂect any intrinsic beneﬁt of one conﬁguration over another. That said,
four of these best-in-problem classiﬁers used small populations (2 with 1×1000 and 2 with 5×200),
so maybe it is easier to ﬁnd eﬀective optimisers that use small populations than larger ones.

Perhaps more importantly, Fig. 1 shows that the Push GP runs found at least one optimiser
that performed better, on average, than CMA-ES and DE. For the simplest problem F1, there
was only one evolved optimiser that beat the general purpose optimisers. For the other problems,
many optimisers were found that performed better. This reﬂects the results in [Lones, 2019a],
and is perhaps unsurprising given that the capacity to overﬁt problems is a central motivation for
existing work in hyperheuristics. However, an important diﬀerence in this paper is the use of random
problem transformations during training, since this causes the problems to exhibit greater generality,
preventing optimisers from over-learning speciﬁc features of the landscape. The results suggest that
this does not eﬀect the ability of evolved optimisers to out-perform general purpose optimisers.

The ability to out-perform general purpose optimisers on the problem on which they were trained
is arguably not that important. Of more interest is how they generalise to larger and diﬀerent
problems. Table 3 gives an insight into this, showing how well the best evolved optimiser for each
training problem generalises to larger instances of the same problem and to the other four problems.
Mean error rates are shown both for the 10-dimensional problems with the 1E+3 evaluation budget
used in training, and for 30-dimensional versions of the same problems and 1E+4 evaluation budgets.
First of all, these ﬁgures show that the evolved optimisers do not stop progressing after the 1E+3
solution evaluations on which they were trained, since they make signiﬁcantly more progress on the
same problem when given a budget of 1E+4 solution evaluations. Also, it is evident that most of
the optimisers generalise well to 30-dimensional versions of the same problem. The best optimisers
evolved on the 10D F12, F13 and F14 problems do particularly well in this regard, outperforming
CMA-ES and DE on both the 10D and 30D versions of the problems. The F1 optimiser is the
only one which generalises relatively poorly, being beaten by CMA-ES, DE and several of the other
optimisers on the 30D version.

The most interesting insight from Table 3 is that many of the optimisers also generalise to other
problems. For the 10D, 1E+3 evaluations case, all of the optimisers do better than DE when their
average rank is taken across all ﬁve problems. More surprisingly, the F12 optimiser does as well as
CMA-ES across all problems, despite only having been trained on one of them. Its average rank
does drop slightly when its F12 rank is removed from the calculation of its average rank, suggesting
it doesn’t generalise quite as well as CMA-ES on the 10D problems. However, the ﬁgures for the 30D
case are even more surprising, with the F12 optimiser doing better across the ﬁve problems (even
with F12 discounted) than CMA-ES. Also notable is that the F13 optimiser comes ﬁrst in three out
of the ﬁve 30D problems, though this is balanced by coming last in the other two. CMA-ES does
do slightly better than the F12 optimiser when given a budget of 1E+4 solution evaluations, but the
diﬀerence is slight, and the best mean error rates for the four most diﬃcult problems are found by
the evolved optimisers.

Table 4 shows the evolved Push expression used by each best-in-problem optimiser, in each case
slightly simpliﬁed by removing instructions that have no eﬀect on their ﬁtness. Whilst it is diﬃcult to
understand their behaviour by looking at these expressions alone, it is usually possible to gain more
insight by observing the interpreter’s stack states as they run, and by observing their trajectories

8

Table 3: Generality of evolved optimisers. For each optimiser, mean errors are shown for 25
optimisation runs on 10D and 30D problems. The mean rank including (and excluding) the problem
the optimiser was trained on is also shown, and the best result for each combination of problem
dimensionality (D) and ﬁtness evaluation budget (FEs) is underlined for each problem number and
ranking.

D FEs

Optimiser F1

F9

F12

F13

F14

Rank

10

1E+3 CMA-ES

DE
F1 best
F19 best
F12 best
F13 best
F14 best

1E+4 CMA-ES

DE
F1 best
F9 best
F12 best
F13 best
F14 best

30

1E+3 CMA-ES

DE
F1 best
F9 best
F12 best
F13 best
F14 best

1E+4 CMA-ES

DE
F1 best
F9 best
F12 best
F13 best
F14 best

1.70E−2
4.21E+2
2.48E−3
1.32E+4
3.10E+3
3.56E+4
4.11E+2

5.20E−9
2.00E+1
2.44E−6
1.45E−3
5.96E−4
1.51E−4
1.37E+1

8.16E+2
2.06E+4
7.75E+4
7.63E+4
5.74E+4
1.63E+5
2.14E+4

5.42E−9
4.71E+0
1.36E+2
6.40E+4
5.97E−2
2.44E+4
1.64E+2

3.07E+1
3.11E+1
7.28E+1
3.27E−1
7.28E+0
2.44E+0
7.76E+1

6.21E+0
5.49E−9
8.05E+1
2.06E−1
7.47E−2
3.66E−6
5.16E+1

2.53E+2
3.77E+2
4.36E+2
3.24E+2
1.18E+2
1.00E+2
4.15E+2

4.78E+1
9.85E+1
3.68E+2
3.27E+2
5.76E+0
5.04E−2
3.33E+2

3.59E+4
7.48E+4
3.29E+4
9.32E+3
2.79E+3
4.63E+4
9.97E+4

2.98E+3
1.64E+4
2.36E+4
7.72E+3
3.93E+2
3.07E+4
3.77E+4

1.67E+6
1.53E+6
1.07E+6
1.07E+6
3.46E+5
1.73E+5
2.19E+6

2.51E+5
9.29E+5
4.08E+5
1.09E+6
3.43E+4
1.26E+5
1.17E+6

3.84E+0
1.62E+3
5.26E+0
1.18E+0
2.43E+0
1.05E+0
2.69E+2

9.71E−1
9.05E+0
3.60E+0
7.04E−1
4.98E−1
3.45E−1
1.62E+1

1.14E+2
1.62E+5
3.47E+4
4.00E+3
3.62E+1
1.84E+1
3.52E+4

3.80E+0
1.02E+2
4.18E+1
3.52E+3
5.00E+0
1.42E+0
3.97E+3

4.28E+0
4.34E+0
4.47E+0
4.86E+0
4.52E+0
4.82E+0
4.04E+0

3.91E+0
4.02E+0
4.50E+0
4.85E+0
4.21E+0
4.90E+0
3.57E+0

1.42E+1
1.41E+1
1.45E+1
1.45E+1
1.44E+1
1.47E+1
1.38E+1

1.38E+1
1.39E+1
1.44E+1
1.46E+1
1.41E+1
1.47E+1
1.33E+1

3.4
5.0
4.0 (4.8)
3.6 (4.3)
3.4 (4.0)
4.2 (5.0)
4.4 (5.3)

2.8
4.2
4.8 (5.5)
3.8 (3.8)
3.0 (3.5)
4.0 (4.8)
5.4 (6.5)

3.2
4.2
5.2 (5.0)
4.2 (4.3)
2.8 (3.0)
3.4 (4.0)
4.6 (5.5)

2.2
4.0
4.8 (5.0)
6.0 (6.3)
2.4 (2.8)
3.4 (4.0)
5.2 (6.3)

on 2D versions of the problems on which they were trained. Fig. 2 shows examples of the latter; in
almost all cases, optimisers generalise well to these easier 2D problems, and it can be seen in each
case that the global optimum is found. It can also be seen from the trajectories that the behaviours
of the ﬁve optimisers are quite diverse, and this is reﬂected in their program-level behaviours:

• Each particle in the F1 optimiser looks up the population best and then adds a random vector
to this to generate a new search point. Notably, the size of this random vector is determined
using a trigonometric expression based on the components of the particle’s current and best
search points, meaning that the move size carried out by each particle in the population is
diﬀerent.

9

Table 4: Evolved Push expressions of best-in-problem optimisers

F1

F9

F12

F13

F14

(exec.dup float.- vector.- float.pop vector.zip vector.zip integer.swap
float.cos float.- float.cos float.- float.yank vector.best vector.wrand
float.abs float.dup float.frominteger vector.- vector.dim*)
(input.stackdepth float.frominteger vector.yank vector.wrand boolean.dup
integer.fromboolean vector.swap integer.rot float.frominteger float.sin
vector.yank vector.shove vector.dim+ vector.yank 0.0 float.> input.inall
boolean.not 1 boolean.dup vector.pop boolean.stackdepth)
(vector.stackdepth vector.swap float.fromboolean integer.fromboolean
integer.rand vector.dim+ float.+ vector.swap integer.rand 0 vector.swap
integer.max integer.= vector.stackdepth integer.dup vector.- integer.dup
integer.rand vector.- vector.dim+ vector.mag float.frominteger float.tan
integer.rot vector.dim+)
(integer.- float.sin vector.wrand integer.yankdup vector.dim* vector.-
input.inall float.sin vector.-)
(float.< float./ vector.best vector.yankdup float.ln float.max
float.stackdepth 0.48999998 float.abs vector.between vector.wrand
vector.scale integer.yank input.index vector.- float.rand float.neg
0.97999996 float.- 0.97999996 vector.wrand vector.scale vector.-)

• The F9 optimiser (which uses only one point of search) continually switches between searching
around the best-seen search point and evaluating a random search point. When searching
around the best point, at each iteration it adds the sine of the move number to a single
dimension, moving along two dimensions each time; in essence, this causes it to systematically
explore the nearby search space, building up the space-ﬁlling pattern seen in Fig. 2.

• The F12 optimiser is the most complex, and its behaviour at the instruction level is hard to
understand. However, it does use the particle’s index and the index (but not the vector) of the
population best, and both the improvement and out-of-bounds Boolean signals to determine
each move. By observing its search trajectories, it is evident that it builds up a geometric
pattern that causes it to explore moves with a power series distribution—in essence, a novel
form of variable neighbourhood search.

• The F13 optimiser, by comparison, has the simplest program. Each iteration, it adds a random
value to one of the dimensions of the best-seen search point, cycling through the dimensions
on each subsequent move (hence why it generates a cross-shaped trajectory). The size of the
move (the upper bound of the random value) is determined by both the sine of the objective
value of the current point and the sine of the maximum dimension size, the former causing it
to vary cyclically as search progresses, and the latter allowing it to adapt the move size to the
search area.

• The F14 optimiser is the only one of these ﬁve which uses both a larger population and the
vector.between instruction. Each iteration, it uses this to generate a new population of search
points half-way between the population best and one of each particle’s previous positions.
Interestingly, which previous position is used for a particular particle is determined by its
index; the ﬁrst particle uses its current position, higher numbered particles go back further in
time. This may allow backtracking, which could be useful for landscapes that are deceptive

10

Figure 2: Example trajectories of the best-in-problem optimisers (F1 and F9 top, F12 and F13
middle, F14 bottom) on the 2D versions of the benchmark problems they were trained on. The
global minimum is shown as a black circle. The best point reached by the optimiser is shown as a
black cross. Each population member’s trajectory is shown as a separate colour, with each search
point shown as a point. Initial search points are surrounded by small coloured circles. The search
landscape is shown in the background as a contour plot.

11

Figure 3: Examples of the best evolved optimisers for each problem (top to bottom: F1,9,12,13,14)
applied to each of the other problems (left-right: F1,9,12,13,14). See caption of Figure 2 for more
information.

and have limited gradient information (such as F14). A small random vector is added to each
half-way point, presumably to inject further diversity.

Fig. 3 shows examples of trajectories when each of these optimisers are applied to 2D versions
of the other four problems. These suggest that optimisers may fail to generalise not because of
intrinsic assumptions about properties of landscapes, but because they make assumptions about the
dimensions of the search area. For example, the F9 and F13 optimisers appear to fail on the F14
landscape because they are making moves, or sampling regions, which are only appropriate for a

12

landscape with much smaller overall dimensions. Using a larger range of random scalings during
training might help with this.

However, these optimisers were not evolved for generality, so the fact that most of them generalise
to other problems is a fortunate bi-product. Furthermore, it is likely that the optimisers that do
best on one problem are not likely to be the best in terms of generality. Hence, in practice there is
likely to be a beneﬁt to looking at the best optimisers from the other 245 runs depicted in Fig. 1.
Fig. 4 gives a snapshot of these, showing one example for each combination of training problem and
optimiser population size. This illustrates some of the broad diversity seen amongst the solutions.
Many of these trajectories look nothing like conventional optimisers, so it is likely that interesting
ideas of how to do optimisation could be gained by looking more closely at them. Another interesting
direction for future work would be to consider ensembles of optimisers. There are many potential
ways of doing this. For example, early results suggest that it may be advantageous, in terms of
generality, to form a heterogenous population-based optimiser by combining the best programs from
multiple runs.

5 Conclusions

In recent years, there has been a lot of criticism of the ad hoc design of new optimisers through
mimicry of natural phenomena. Despite early success with evolutionary algorithms and particle
swarm optimisation, this trend has increasingly resulted in optimisers that are technically novel, but
which diﬀer in minor and often arbitrary ways from existing optimisers. If we are to create new
optimisation algorithms (and the no free lunch theorem [Wolpert et al., 1997] suggests a need for
diverse optimisers), then perhaps it is better to do this in a more systematic, objective and auto-
mated manner. This paper contributes towards this direction of research by investigating the utility
of Push GP for exploring the space of optimisers. The results show that Push GP can both discover
and express optimisation behaviours that are eﬀective, complex and diverse. Encouragingly, the
evolved optimisers scale to problems they did not see during training, and often out-perform general
purpose optimisers on these previously unseen problems. The behavioural analysis shows that the
evolved optimisers use a diverse range of metaheuristic strategies to explore optimisation landscapes,
using behaviours that diﬀer signiﬁcantly from existing local and population-based optimisers. Fur-
thermore, these are only the tip of the iceberg; the evolved optimiser populations appear to contain
broad behavioural diversity, and there are many potential ways of combining diverse optimisers to
create ensembles.

References

M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoﬀman, D. Pfau, T. Schaul, B. Shillingford, and
N. De Freitas. Learning to learn by gradient descent by gradient descent. In Advances in neural
information processing systems, 2016.

A. Auger and N. Hansen. A restart CMA evolution strategy with increasing population size. In
Proceedings of the IEEE Congress on Evolutionary Computation, volume 2 of IEEE CEC ’05,
pages 1769–1776. IEEE, 2005.

A. Bogdanova, J. P. Junior, and C. Aranha. Franken-swarm: grammatical evolution for the auto-
matic generation of swarm-like meta-heuristics. In Proceedings of the Genetic and Evolutionary
Computation Conference Companion, GECCO ’19, pages 411–412. ACM, 2019.

13

Figure 4: Example trajectories of other evolved optimisers. One example is shown for each com-
bination of problem (top to bottom: F1,9,12,13,14) and population size (left to right: 1, 5, 25, 50).
See caption of Figure 2 for more information.

14

B. Edmonds. Meta-genetic programming: Co-evolving the operators of variation. Technical Report

CPM Report 98-32, Manchester Metropolitan University, 1998.

B. W. Goldman and D. R. Tauritz. Self-conﬁguring crossover. In Proceedings of the Genetic and
Evolutionary Computation Conference Companion, GECCO ’11, pages 575–582. ACM, 2011.

W. Kantschik, P. Dittrich, M. Brameier, and W. Banzhaf. Meta-evolution in graph GP. In European

Conference on Genetic Programming, EuroGP 1999, pages 15–28. Springer, 1999.

W. B. Langdon. Genetic programming and data structures: Genetic Programming + Data Structures

= Automatic Programming!, volume 1. Springer Science & Business Media, 2012.

M. A. Lones.

Instruction-level design of local optimisers using push GP.

In Proceedings of the
Genetic and Evolutionary Computation Conference Companion, GECCO ’19, pages 1487–1494.
ACM, 2019a.

M. A. Lones. Mitigating metaphors: A comprehensible guide to recent nature-inspired algorithms.

arXiv preprint arXiv:1902.08001, 2019b.

M. A. Martin and D. R. Tauritz. Evolving black-box search algorithms employing genetic pro-
gramming. In Proceedings of the Genetic and Evolutionary Computation Conference Companion,
GECCO ’13, pages 1497–1504. ACM, 2013.

L. Metz, N. Maheswaranathan, J. Nixon, D. Freeman, and J. Sohl-dickstein. Learned optimizers
that outperform SGD on wall-clock and test loss. In Proceedings of the 2nd Workshop on Meta-
Learning, MetaLearn 2018, 2019.

M. Oltean. Evolving evolutionary algorithms using linear genetic programming. Evolutionary Com-

putation, 13(3):387–410, 2005.

S. N. Richter and D. R. Tauritz. The automated design of probabilistic selection methods for
evolutionary algorithms. In Proceedings of the Genetic and Evolutionary Computation Conference
Companion, GECCO ’18, pages 1545–1552. ACM, 2018.

J. Ronkkonen, S. Kukkonen, and K. V. Price. Real-parameter optimization with diﬀerential evolu-
tion. In Proceedings of the IEEE Congress on Evolutionary Computation, volume 1 of IEEE CEC
’05, pages 506–513. IEEE, 2005.

P. Ryser-Welch, J. F. Miller, J. Swan, and M. A. Trefzer. Iterative cartesian genetic programming:
Creating general algorithms for solving travelling salesman problems. In European Conference on
Genetic Programming, EuroGP ’16, pages 294–310. Springer, 2016.

K. Sörensen. Metaheuristics—the metaphor exposed.

International Transactions in Operational

Research, 22(1):3–18, 2015.

L. Spector. Autoconstructive evolution: Push, pushGP, and pushpop. In Proceedings of the Genetic

and Evolutionary Computation Conference, volume 137 of GECCO ’19, 2001.

L. Spector and A. Robinson. Genetic programming and autoconstructive evolution with the push

programming language. Genetic Programming and Evolvable Machines, 3(1):7–40, 2002.

L. Spector, C. Perry, J. Klein, and M. Keijzer. Push 3.0 programming language description. Technical

report, HC-CSTR-2004-02, School of Cognitive Science, Hampshire College, 2004.

15

O. Wichrowska, N. Maheswaranathan, M. W. Hoﬀman, S. G. Colmenarejo, M. Denil, N. de Freitas,
and J. Sohl-Dickstein. Learned optimizers that scale and generalize. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, ICML ’17, pages 3751–3760, 2017.

D. H. Wolpert, W. G. Macready, et al. No free lunch theorems for optimization. IEEE Transactions

on Evolutionary Computation, 1(1):67–82, 1997.

J. R. Woodward and J. Swan. The automatic generation of mutation operators for genetic algorithms.
In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’12, pages 67–
74. ACM, 2012.

16

