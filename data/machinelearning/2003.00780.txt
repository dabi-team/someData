0
2
0
2

r
a

M
2

]

C
O
.
h
t
a
m

[

1
v
0
8
7
0
0
.
3
0
0
2
:
v
i
X
r
a

Risk-Averse Learning by Temporal Difference Methods

¨Umit K¨ose and Andrzej Ruszczy´nski∗

March 1,2020

Abstract

We consider reinforcement learning with performance evaluated by a dynamic risk measure. We
construct a projected risk-averse dynamic programming equation and study its properties. Then we
propose risk-averse counterparts of the methods of temporal differences and we prove their convergence
with probability one. We also perform an empirical study on a complex transportation problem.
Keywords: Reinforcement Learning, Risk, Stochastic Approximation
AMS: 49L20, 62L20, 90C39

1 Introduction

The objective of this paper is to propose and analyze new risk-averse reinforcement learning methods for
Markov Decision Processes (MDPs). Our goal is to combine the efﬁcacy of the methods of temporal differ-
ences with the robustness of dynamic risk measures, and to provide a rigorous mathematical analysis of the
methods.

MDPs are well-known models of stochastic sequential decision problems, covered in multiple mono-
graphs [5, 32, 55, 7], and having countless applications. In the classical setting, the goal of an MDP is
to ﬁnd a policy minimizing the expected cost over a ﬁnite or inﬁnite horizon. Traditional MDP models,
although effective for small to medium size problems, suffer from the curse of dimensionality in problems
with large state space. Approximate dynamic programming approaches try to tackle the curse of dimen-
sionality and provide an approximate solution of an MDP (see [52] for an overview). Such methods usually
involve value function approximations, where the value of a state of the Markov process is approximated by
a simple, usually linear, function of some selected features of the state [6].

Reinforcement learning methods [67, 52] involve simulation or observation of a Markov process to
approximate the value function and learn the corresponding policies. The ﬁrst studies attempted to emulate
neural networks and biological learning processes, learning by trial and error [46, 27]. Some learning
algorithms, such as Q-Learning [73, 74] and SARSA [59], follow this idea. One of core approaches in
reinforcement learning is the method of temporal differences [66], known as TD(λ ). It uses differences
between the values of the approximate value function at successive states to improve the approximation,
concurrently with the evolution of the system. TD(λ ) is a continuum of algorithms depending on a parameter
λ ∈ [0, 1] which is used to exponentially weight past observations. Consequently, related methods such as
Q(λ ) [73, 49, 50, 58] and SARSA(λ ) were developed [59, 58]. The methods of temporal differences have

∗Department of Management Science and Information Systems, Rutgers University, Piscataway, NJ 08854, USA; email:

{uek1,rusz}@rutgers.edu; the order of authors is alphabetical

1

 
 
 
 
 
 
been proven to converge in the mean in [19] and almost surely by several studies, with different degrees of
generality and precision [49, 20, 71, 34, 72].

We introduce risk models into temporal difference learning.

In the extant literature, three basic ap-
proaches to risk aversion in MDPs have been employed: utility functions (see, e.g., [35, 36, 17, 21, 29, 4,
37]), mean–variance models (see. e.g., [76, 28, 44, 1, 13]), and entropic (exponential) models (see, e.g.,
[33, 45, 8, 18, 24, 41, 4]). Our research is rooted in the theory of dynamic measures of risk, which has been
intensively developed in the last 15 years (see [64, 56, 57, 30, 14, 61, 3, 51, 39, 38, 15] and the references
therein).

In [60], we introduced the class of Markov dynamic risk measures, specially tailored for the MDPs.
It allowed for the development of dynamic programming equations and corresponding solution methods,
generalizing the well-known results for the expected value problems. Our ideas were successfully extended
to undiscounted problems in [11, 12], partially observable and history-dependent systems in [26, 25], and
further generalized in [42, 65].

A number of works introduce models of risk into reinforcement learning: exponential utility functions
[10, 9] and mean-variance models [69, 54]. Few later studies propose heuristic approaches involving coher-
ent risk measures and their mean-risk counterparts [16, 68]; these studies employ policy gradients and use
them in actor-critic type algorithms. Distributed policy gradient methods with risk measures were proposed
in [43]. Model-related uncertainties are discussed in [70].

In this paper, we use Markov risk measures of [60] in conjunction with linear approximations of the

value function. Our contributions can be summarized as follows:

• A projected risk-averse dynamic programming equation and analysis of its properties (§2);
• A risk-averse method of temporal differences (§3) and proof of its convergence (§4);
• A multistep risk-averse method of temporal differences (§5) and its convergence proof (§6);
• An empirical study comparing the efﬁcacy of the methods (§7).

2 The Projected Risk-Averse Dynamic Programming Equation

We consider a Markov decision process (MDP) with a ﬁnite state space X = {1, . . . , n}, ﬁnite action sets
U (i) for all i ∈ X , controlled transition probabilities Pi j(u) where i, j ∈ X and u ∈ U (i), and one-step
cost function c(i, u), where i ∈ X and u ∈ U (i). For a discount factor α ∈ (0, 1) and any non-anticipative
policy π for determining controls ut ∈ U (it), t = 0, 1, 2, . . . , the expected discounted cost

vπ (i) = E

(cid:104) ∞
∑
t=0

αtc(it, ut)

(cid:12)
(cid:105)
(cid:12)
(cid:12) i0 = i
,

is ﬁnite. For every Markovian policy π, the value function associated with this policy satisﬁes the linear
equation

vπ (i) = c(i, π(i)) + α ∑
j∈X

Pi j(π(i)) vπ ( j),

i ∈ X .

Viewing vπ as a vector, and deﬁning the vector cπ with elements c(i, π(i)), i ∈ X , and the matrix Pπ with
entries Pi j(π(i)), i, j ∈ X , we can compactly write the policy evaluation equation as

vπ = cπ + αPπ vπ .

(1)

In [60], in a more general setting in a Polish space X , Markov risk measures for cost evaluation in an MDP
were introduced. In a ﬁnite-horizon setting, a Markov risk measure evaluates the sequence of discounted

2

costs αtc(xt, ut), t = 0, 1, 2, . . . , T , under a Markov policy π, in a recursive way. Denoting by ρ π
of the system starting from state i at time t, we have

t,T (i) the risk

t,T (i) = cπ
ρ π

i + ασi

(cid:0)Pπ

i , ρ π

t+1,T (·)(cid:1),

i ∈ X ,

t = 0, 1, . . . , T − 1,

(2)

T,T (i) = cπ

i , i ∈ X . In equation (2), the operator σ : X × P(X ) × V → R, where P(X ) is the
with ρ π
space of probability measures on X and V is the space of bounded functions on X , is a transition risk
mapping. It can be interpreted as risk-averse analog of the conditional expectation. Its ﬁrst argument is the
state i (which we write as a subscript). The second argument, the vector Pπ
i , is the ith row of the matrix
Pπ : the probability distribution of the state following i under the policy π. The last argument, the function
ρ π
t+1,T (·), is the risk of running the system from the next state in the time interval from t + 1 to T . The
transition risk mapping is a special case of a risk form: a generalization of a risk measure introduced in [23]
to accommodate the dependence of measures of risk on the underlying probability distribution. In the case
of controlled Markov systems, this dependence is germane for the analysis.

As in [60], we assume that for each i ∈ X and each Pπ

i ∈ P(X ), the transition risk mapping σi(p, ·),
understood as a function its last argument, satisﬁes the axioms of a coherent measure of risk [2]. In the
axioms below we suppress the argument Pπ
i , focusing on the dependence on the third argument, a function
of a state:
Convexity: σi(αv + (1 − α)w) ≤ ασi(v) + (1 − α)σi(w), ∀ α ∈ [0, 1], ∀ v, w ∈ V ;
Monotonicity: If v ≤ w (componentwise) then σi(v) ≤ σi(w);
Translation equivariance: σi(v + β 1) = σi(v) + β , for all β ∈ R;
Positive homogeneity: σi(β v) = β σi(v), for all β ≥ 0.
Under these conditions, one can pass to the limit with T → ∞ in (2) and prove the existence of an inﬁnite-
horizon discounted risk measure [60]

ρ π
0,∞(i) = lim
T →∞

ρ π
0,T (i),

i ∈ X .

We still denote its value at state i by vπ (i); it will never lead to misunderstanding. The policy value vπ (·)
satisﬁes the risk-averse policy evaluation equation:

vπ (i) = cπ (i) + ασi

(cid:0)Pπ

i , vπ (·)(cid:1),

i ∈ X .

We introduce the space Q of transition kernels on X , deﬁne a vector-valued transition risk operator σ :
Q × V → V , with components σi(Pπ

i , ·), i ∈ X , and rewrite the last equation in a way similar to (1):

vπ = cπ + ασ (Pπ , vπ ).

(3)

The only difference between (1) and (3) is that the matrix Pπ has been replaced by a convex operator σ
(which still depends on Pπ ). The risk-neutral case is a special case of (3) with σ (P, v) = Pv. References
[60, 11, 12, 25] outline the theory, provide examples and applications.

Coherent risk measures admit a dual representation [62], which in our case can be stated as follows. For

every i ∈ X a convex, closed and bounded set Ai(Pπ

i ) of probability measures on X exists, such that

σi(Pπ

i , v) = max
µ∈Ai(Pπ
i )

(cid:104)µ, v(cid:105),

v ∈ V .

(4)

In a risk-neutral case, the set Ai(Pπ
and has Pπ

i as one of its elements, provided we always have σi(Pπ

i , v) ≥ Pπ

i ) = ∂ σi(Pπ

i , 0) contains only one element, Pπ

i , but in general it is larger
i v. The multifunction A : X →

3

P(X ) ⇒ P(X ) is called the risk multikernel. Every µ ∈ Ai(Pπ
Pπ
i .

i ) is absolutely continuous with respect to

While equation (3) can be solved by a nonsmooth Newton’s method and the resulting evaluation used in
a policy iteration method [60], all these techniques become impractical, when the size of the state space is
very large.

An established approach to such a situation in expected value models is to assume that each state i ∈ X
has a number of relevant features ϕ j(i) ∈ R, j = 1, . . . , m, where m (cid:28) n, and that the value vπ (i) of a state
can be approximated by a linear combination of its features:

vπ (i) ≈ (cid:101)vπ (i) =

m
∑
j=1

r jϕ j(i),

i ∈ X .

(5)

From now on, we suppress the superscript π, because most of our considerations focus on evaluating a ﬁxed
policy. We deﬁne the matrix of the features of all states, namely

Φ =















ϕ (cid:62)(1)
ϕ (cid:62)(2)
...
ϕ (cid:62)(n)

.

Now we can write our approximation as v ≈ ˜v = Φr. Similar to the expected value case, if we attempt to
emulate (3) with the approximate value function, we may observe that the right hand side of the equation,
c + ασ (P, Φr), may not be represented as a linear combination of the features. Therefore, we need to project
this vector on the subspace spanned by the features, range(Φ). Accordingly, we deﬁne a projection operator,
L : V → range(Φ), and formulate the projected risk-averse approximate dynamic programming equation:
Φr = L(cid:0)c + ασ (P, Φr)(cid:1).

(6)

Still following the expected value case, we assume that the Markov system under policy π is ergodic, and
we denote its vector of stationary probabilities by q. We deﬁne the projection operator using the following
scalar product and the associated norm: (cid:104)v, w(cid:105)q = ∑n

q = (cid:104)w, w(cid:105)q. Then

i=1 qiviwi, (cid:107)w(cid:107)2

L(w) = argmin
z∈range(Φ)

||z − w||q, w ∈ V .

(7)

The fundamental question is the existence and uniqueness of a solution of equation (6). This can be answered
by establishing the contraction mapping property of the right hand side of (6):

which would imply the existence and uniqueness of a solution of the equation

D(v) = L(cid:0)c + ασ (P, v)(cid:1),

v ∈ V ,

v = Dv.

Crucial in this context is the distortion coefﬁcient of the risk multikernel A:

κ = max

(cid:26) |µi j − pi j|
pi j

: µi ∈ Ai(Pπ

i ), pi j > 0, i, j ∈ X

(cid:27)

.

(8)

(9)

By deﬁnition, κ ≥ 0, with the value 0 corresponding to the risk-neutral model. We also recall that for pi j = 0
we always have mi j = 0, for all mi ∈ Ai(Pπ

i ).

4

Lemma 1. The transition risk operator satisﬁes for all w, v ∈ V the inequalities:

(cid:107)σ (P, w) − σ (P, v)(cid:107)q ≤

√

1 + κ (cid:107)w − v(cid:107)q,

and

(cid:107)σ (P, w) − σ (P, v) − P(w − v)(cid:107)q ≤ κ (cid:107)w − v(cid:107)q.

(10)

(11)

Proof. For brevity, we omit the argument P of σ (P, ·), because it is ﬁxed. For every i = 1, . . . , n, by the
mean value theorem for convex functions [75, 31], a point ¯v(i) = (1 − θi)v + θiw exists, with θi ∈ [0, 1], and
a subgradient mi ∈ ∂ ρi( ¯v(i)) exists, such that

σi(w) − σi(v) = (cid:104)mi, w − v(cid:105).

Since the subdifferential ∂ ρi(·) ⊆ Ai, we have mi ∈ Ai. Therefore, for a matrix M having mi, i = 1, . . . , n, as
its rows,

σ (w) − σ (v) = M(w − v).

(12)

As each mi is a probability vector, Jensen’s inequality with h = w − v, and the equation q(cid:62)P = q(cid:62) yield

(cid:107)Mh(cid:107)2

q = ∑
i∈X

(cid:16)

qi

∑
j∈X

(cid:17)2

mi jh j

≤ ∑
i∈X

mi jh2
j

qi ∑
j∈X
≤ (1 + κ) ∑
i∈X

qi ∑
j∈X

pi jh2

j = (1 + κ) ∑
j∈X

q jh2

j = (1 + κ)(cid:107)h(cid:107)2
q.

(13)

The last two relations imply (10). In a similar way, it follows from (12) that

(cid:13)σ (P, w) − σ (P, v) − P(w − v)(cid:13)
(cid:13)
2
q = (cid:107)(M − P)h(cid:107)2
(cid:13)

q ≤ ∑
i∈X

(cid:16)

qi

|mi j − pi j||h j|

(cid:17)2

∑
j∈X
(cid:17)2

≤ κ2 ∑
i∈X

(cid:16)

qi

∑
j∈X

which is (11).

pi j|h j|

≤ κ2 ∑
i∈X

qi ∑
j∈X

pi j|h j|2 = κ2 (cid:107)w − v(cid:107)2
q,

We can now prove the existence and uniqueness of the solution of the risk-averse equation (9).

Theorem 2. If α

√

1 + κ < 1 then the equation (9) has a unique solution v∗.

Proof. We verify that the operator (8) is a contraction mapping in the norm (cid:107) · (cid:107)q. The orthogonal projection
L is nonexpansive. The operator P is nonexpansive in the norm (cid:107) · (cid:107)q as well (this is a special case of (13)
with M = P and κ = 0). The transition risk operator σ (·) multiplied by α is a contraction by Lemma 1. The
assertion follows now from the Banach contraction mapping theorem.

If Φ has full column rank, equation (6) has a unique ﬁxed point as well.

5

3 The Risk-Averse Method of Temporal Differences

We propose to solve (6) by a risk-averse analog of the classical method of temporal differences [66]. We
deﬁne v∗ to be the solution of equation (9) (which exists and is unique, if α

1 + κ < 1).

√

Consider the evolution of the system under policy π, resulting in a random trajectory of states it, t =
0, 1, 2 . . . . At each time t, we have an approximation rt of a solution of the equation (6). Let Ft be the
σ -algebra deﬁned by all observations gathered up to time t.

The difference between the left and the right hand sides of equation (6) with coefﬁcient values rt and

state it is the risk-averse temporal difference:

dt = ϕ (cid:62)(it)rt − c(it) − ασit (Pit , Φrt),

t = 0, 1, 2, . . . .

(14)

Evidently, it cannot be easily computed or observed; this would require the evaluation of the risk σit (Pit , v)
and thus consideration of all possible transitions from state it. Instead, we assume that we can observe a
random estimate (cid:101)σit (Pit , ·), such that

(cid:101)σit (Pit , Φrt) = σit (Pit , Φrt) + ξt,

t = 0, 1, 2, . . . ,

(15)

with some random errors ξt. The conditions on {ξt} will be speciﬁed later. This allows us to deﬁne the
observed risk-averse temporal differences,

(cid:101)dt = ϕ (cid:62)(it)rt − c(it) − α (cid:101)σit (Pit , Φrt),

t = 0, 1, 2, . . . ,

and to construct the risk-averse temporal difference method as follows:

rt+1 = rt − γtϕ(it) (cid:101)dt,

t = 0, 1, 2, . . . .

(16)

(17)

Before proceeding to the detailed convergence proof in the stochastic case, we analyze a deterministic
model of the method, in which the errors ξt are ignored and the updates of the sequence {rt} are averaged
over all states (with the distribution q). We deﬁne the operator:

U(r) = Ei∼q

(cid:2)ϕ(i)(cid:0)ϕ (cid:62)(i)r − c(i) − ασi(Pi, Φr)(cid:1)(cid:3) = Φ (cid:62)Q(cid:2)Φr − c − α σ (P, Φr)(cid:3).

The deterministic analog of (16)–(17) reads:

¯rt+1 = ¯rt − γ U(rt),

t = 0, 1, 2, . . . ,

γ > 0.

By the deﬁnition of the projection operator L, a point r∗ is a solution of (6) if and only if

(18)

(19)

r∗ = argmin

r

1
2

(cid:13)Φr − (cid:0)c + ασ (P, Φr∗)(cid:1)(cid:13)
(cid:13)
2
q.
(cid:13)

This occurs if and only if r∗ is a zero of U(·) and thus supports our idea of using the method (16)–(17).

√

Theorem 3. If α
sequence {¯rt} convergent to a point r∗ such that U(r∗) = 0.

1 + κ < 1, then γ0 > 0 exists, such that for all γ ∈ (0, γ0) the algorithm (19) generates a

6

Proof. We shall show that for sufﬁciently small γ > 0 the operator I − γU is a contraction. For arbitrary r(cid:48)
and r(cid:48)(cid:48), we have

(cid:13)(r(cid:48) − γU(r(cid:48))) − (r(cid:48)(cid:48) − γU(r(cid:48)(cid:48)))(cid:13)
(cid:13)
2 = (cid:107)r(cid:48) − r(cid:48)(cid:48)(cid:107)2
(cid:13)

− 2γ(cid:104)r(cid:48) − r(cid:48)(cid:48), Φ (cid:62)QΦ(r(cid:48) − r(cid:48)(cid:48))(cid:105) + 2γα(cid:10)r(cid:48) − r(cid:48)(cid:48), Φ (cid:62)Q(cid:2)σ (P, Φr(cid:48)) − σ (P, Φr(cid:48)(cid:48))(cid:3)(cid:11)

+ γ 2(cid:13)
(cid:13)

(cid:13)Φ (cid:62)QΦ(r(cid:48) − r(cid:48)(cid:48)) − αΦ (cid:62)Q(cid:2)σ (P, Φr(cid:48)) − σ (P, Φr(cid:48)(cid:48))(cid:3)(cid:13)

2
(cid:13)
(cid:13)

.

The last term (with γ 2) can be bounded by γ 2C(cid:107)Φ(r(cid:48) − r(cid:48)(cid:48))(cid:107)2

q where C is some constant. Then

(cid:13)(r(cid:48) − γU(r(cid:48))) − (r(cid:48)(cid:48) − γU(r(cid:48)(cid:48)))(cid:13)
(cid:13)
2 ≤ (cid:107)r(cid:48) − r(cid:48)(cid:48)(cid:107)2 − 2γ(cid:107)Φ(r(cid:48) − r(cid:48)(cid:48))(cid:107)2
(cid:13)
q

+ 2γα(cid:10)Φ(r(cid:48) − r(cid:48)(cid:48)), σ (Φr(cid:48)) − σ (Φr(cid:48)(cid:48))(cid:11)

q + γ 2C(cid:107)Φ(r(cid:48) − r(cid:48)(cid:48))(cid:107)2
q.

The scalar product can be bounded by (10), and thus

(cid:13)(r(cid:48) − γU(r(cid:48))) − (r(cid:48)(cid:48) − γU(r(cid:48)(cid:48)))(cid:13)
(cid:13)
2
(cid:13)

≤ (cid:107)r(cid:48) − r(cid:48)(cid:48)(cid:107)2 − 2γ(cid:107)Φ(r(cid:48) − r(cid:48)(cid:48))(cid:107)2

= (cid:107)r(cid:48) − r(cid:48)(cid:48)(cid:107)2 − 2γ

(cid:18)

1 − α

√

q + 2γα
γC
2

1 + κ +

(cid:19)

(cid:107)Φ(r(cid:48) − r(cid:48)(cid:48))(cid:107)2
q.

√

1 + κ(cid:107)Φ(r(cid:48) − r(cid:48)(cid:48))(cid:107)2

q + γ 2C(cid:107)Φ(r(cid:48) − r(cid:48)(cid:48))(cid:107)2
q

Since α

√

1 + κ < 1, then using 0 < γ < 2(1 − α

√

1 + κ)/C, we have

(cid:13)(r(cid:48) − γU(r(cid:48))) − (r(cid:48)(cid:48) − γU(r(cid:48)(cid:48)))(cid:13)
(cid:13)
2 ≤ (cid:107)r(cid:48) − r(cid:48)(cid:48)(cid:107)2 − γβ (cid:107)Φ(r(cid:48) − r(cid:48)(cid:48))(cid:107)2
q,
(cid:13)

(20)

with some β > 0. In particular, setting r(cid:48) = ¯rt and r(cid:48)(cid:48) = r∗ for a solution r∗ of (6), we obtain the following
relation between the successive iterates of the method (19):

(cid:107)¯rt+1 − r∗(cid:107)2 ≤ (cid:107)¯rt − r∗(cid:107)2 − γβ (cid:107)Φ(¯rt − r∗)(cid:107)2
q.

(21)

This immediately proves that the sequence {¯rt} is bounded and Φ ¯rt → Φr∗. Every accumulation point ˆr of
{¯rt} must be then a solution of equation (6). Substituting this accumulation point for r∗ in the last inequality,
we conclude that ¯rt → ˆr.

If Φ has full column rank, the solution r∗ is unique, because substituting another solution for ¯rt in (19)

we obtain rt+1 = rt, which leads to a contradiction in (21).

4 Convergence of the Risk-Averse Method of Temporal Differences

We shall use the following result on convergence of deterministic nonmonotonic algorithms [47].

Theorem 4. Let Y ∗ ⊂ Rm. Suppose {rt} ⊂ Rm is a bounded sequence which satisﬁes the following assump-
tions:

A) If a subsequence {rt}t∈K converges to r(cid:48) ∈ Y ∗, then (cid:107)rt+1 − rt(cid:107) → 0, as t → ∞ , t ∈ K ;
B) If a subsequence {rt}t∈K converged to r(cid:48) /∈ Y ∗, then ε0 > 0 would exist such that for all ε ∈ (0, ε0]

and for all k ∈ K , the index s(t, ε) = min (cid:8)(cid:96) ≥ k : (cid:107)r(cid:96) − rt(cid:107) > ε(cid:9) would be ﬁnite;

7

C) A continuous function W : Rm → R exists such that if {rt}t∈K converged to r(cid:48) /∈ Y ∗ then ε1 > 0 would

exist such that for all ε ∈ (0, ε1] we would have

lim sup
t∈K

W (rs(t,ε)) < W (r(cid:48)),

where s(t, ε) is deﬁned in B);

D) The set {W (r) : r ∈ Y ∗} does not contain any segment of nonzero length.

Then the sequence {W (rt)} is convergent and all limit points of the sequence {rt} belong to Y ∗.

We deﬁne the set of solutions of equation (6):

Y ∗ = {r ∈ Rm : Φr = v∗},

1 + κ < 1 We shall show that the method (17) converges
where v∗ is the unique solution of (9), provided α
to Y ∗, under the above-mentioned condition and some additional conditions on the stepsizes {γt} and errors
{ξt}.

We deﬁne Ft to be the σ -algebra generated by {i0, r0, . . . , it, rt}, t = 0, 1, . . . , and make the following

√

assumptions about the stepsize and error sequences. We allow the stepsizes to be random.

Assumption 1. The sequence {γt} is adapted to the ﬁltration {Ft} and such that
(i) γt > 0, t = 0, 1, . . . , and limt→∞ γt = 0 a.s.;
(ii) ∑∞
(iii) E ∑∞

t=0 γt = ∞ a.s.;

t < ∞;

t=0 γ 2

(iv) For any ε > 0,

lim
t0→∞

sup

{T :∑T

t=t0

γt ≤ε}

T
∑
t=t0

|γt − γt+1| = 0

a.s.

Assumption 2. The sequence of errors {ξt}t≥1 satisﬁes for t = 0, 1, 2 . . . the conditions
(i) E[ξt | Ft] = 0
(ii) E[(cid:107)ξt(cid:107)2 | Ft] ≤ const a.s..

a.s.;

First, we establish an important implication of the ergodicity of the chain. We write ei for the ith unit

vector in Rn.

Lemma 5. If the chain {it} is ergodic with stationary distribution q and Assumption 1 is satisﬁed, then

lim
T →∞

∑T

t=0 γt(eit − q)
t=0 γt

∑T

= 0,

a.s.,

and for any ε > 0,

lim
t0→∞

sup
T ≥t0

∑T

t=t0 γt(eit − q)
(cid:16)
ε, ∑T
t=t0 γt

max

(cid:17) = 0,

a.s..

Proof. Due to the ergodicity of the chain, the vectors

ν(i) = E

(cid:34) ∞
∑
t=0

(eit − q)

(cid:35)
(cid:12)
(cid:12)
(cid:12) i0 = i

,

i ∈ X ,

8

(22)

(23)

are ﬁnite and satisfy the Poisson equation

ν(i) = ei − q + ∑
j∈X

Pi jν( j),

i ∈ X .

Consider the sums ∑T

t=0 γt(eit − q). By the Poisson equation,

eit − q = ν(it) − ∑
j∈X

Pit jν( j) = (cid:2)ν(it) − ν(it+1)(cid:3) +

(cid:104)
ν(it+1) − ∑
j∈X

(cid:105)
Pit jν( j)
.

(24)

(25)

We consider the two components of the right hand side of (25), marked with brackets, separately. Due to
Assumption 1, (i)—(iii), the series

∞
∑
t=1

γt

(cid:104)
ν(it+1) − ∑
j∈X

(cid:105)
Pit jν( j)

=

∞
∑
t=1

(cid:0)ν(it+1) − E[ν(it+1) | Ft](cid:1)

γt

is a convergent martingale. Therefore,

∑T

t=0 γt

(cid:0)ν(it+1) − Et[ν(it+1)](cid:1)

∑T

t=0 γt

lim
T →∞

= 0,

a.s.

We now focus on the sums

T
∑
t=0

(cid:2)ν(it) − ν(it+1)(cid:3) = γ0ν(i0) +

γt

T
∑
t=1

(γt − γt−1)ν(it) − γT ν(iT +1).

Using Assumption 1(iv) and [63, Lem. A.3], we obtain (22)–(23).

We can now prove the convergence of the method.

√

Theorem 6. Suppose the random estimates (cid:101)σit (Pit , Φrt) satisfy (15), Assumptions 1 and 2 are satisﬁed, and
1 + κ < 1. If the sequence {rt} is bounded with probability 1, then every accumulation point of the
α
sequence {rt} is an element of Y ∗, with probability 1.

Proof. We use the global Lyapunov function:

W (r) = min
r∗∈Y ∗

(cid:107)r − r∗(cid:107)2.

The direction used in (17) at step t can be represented as

ϕ(it) (cid:101)dt = U(rt) + ∆t,

with the operator U(·) deﬁned in (18), and

∆t = −αξtϕ(it) + Φ (cid:62) diag (cid:0)eit − q(cid:1)(cid:2)Φrt − c − ασ (P, Φrt)(cid:3).

(26)

(27)

(28)

Our intention is to verify the conditions of Theorem 4 for almost all paths of the sequence {rt}. For this
purpose, we estimate the decrease of the function (26) in iteration t. For any r∗ ∈ Y ∗ we have:

(cid:107)rt+1 − r∗(cid:107)2 = (cid:107)rt − γtU(rt) − r∗(cid:107)2 − 2γt(cid:104)∆t, rt − γtU(rt) − r∗(cid:105) + γ 2

t (cid:107)∆t(cid:107)2.

9

The term involving U(rt) was estimated in the derivation of (21). We obtain the inequality

(cid:107)rt+1 − r∗(cid:107)2 ≤ (cid:107)rt − r∗(cid:107)2 − 2γt(1 − α

√

1 + κ)(cid:107)Φ(rt − r∗)(cid:107)2

q − 2γt(cid:104)∆t, rt − γtU(rt) − r∗(cid:105) +Cγ 2
t .

(29)

Now we can verify the conditions of Theorem 4 for almost all paths of the sequence {rt}.

Condition A. Due to the boundedness of {rt} the sequence {U(rt)} is bounded as well. In view of (27), it is
sufﬁcient to verify that γtξt → 0. By Assumption 2(i), the sequence

ST =

T
∑
t=0

γtξt, T = 0, 1, 2, . . . ,

(30)

T ] ≤ const · E[∑T

is a martingale. Due to Assumption 2(ii), E[S2
of the martingale convergence theorem, {ST } is convergent a.s., which yields limt→∞ γtξt = 0.
Condition B. Suppose rk → r(cid:48) /∈ Y ∗ for k ∈ K (on a certain path ω). If B were false, then for all ε0 > 0 we
could ﬁnd ε ∈ (0, ε0] and k ∈ K such that (cid:107)rt − rk(cid:107) ≤ ε for all t ≥ k. Then for all k0 ∈ K , k0 ≥ k, we have
(cid:107)rt − rk0(cid:107) ≤ 2ε for all t ≥ k0. Since r(cid:48) is not optimal, we can choose ε0 > 0 small enough, k0 ∈ K large
enough, and δ > 0 small enough, so that (cid:107)Φ(rt − r∗)(cid:107)2

t ]. In view of Assumption 1(ii), by virtue

q > δ for all t ≥ k0. Then (29) yields

t=0 γ 2

(cid:107)rT − r∗(cid:107)2 ≤ (cid:107)rk0 − r∗(cid:107)2

(cid:32)

+

−δ (1 − α

√

1 + κ) +

∑T −1
t=k0

γt(cid:104)∆t, rt − γtU(rt) − r∗(cid:105)
γt

∑T −1
t=k0

+C

∑T −1
t=k0
∑T −1
t=k0

γ 2
t
γt

(cid:33) T −1
∑
t=k0

γt.

(31)

We ﬁx r∗ = ProjY ∗(rk0) and estimate the growth of the sums involving ∆t. We write ∆t = ∆(1)
in view of (28),

t + ∆(2)

t

, where,

t = −αξtϕ(it), ∆(2)
∆(1)

t = Φ (cid:62) diag (cid:0)eit − q(cid:1)(cid:2)Φrt − c − ασ (P, Φrt)(cid:3).

Since (30) is a convergent martingale and the terms (cid:104)ϕ(it), rt − γtU(rt) − r∗(cid:105) are bounded and Ft-

measurable, we have

(cid:12)
(cid:12)∑T −1
(cid:12)
t=k0

lim
T →∞

γt(cid:104)∆(1)
t

(cid:12)
, rt − γtU(rt) − r∗(cid:105)
(cid:12)
(cid:12)
∑T −1
t=k0

γt

= 0.

To deal with the sum involving ∆(2)

t

, observe that (cid:107)rt − rk0(cid:107) ≤ 2ε0 and thus

(cid:104)∆(2)
t

, rt − γtU(rt) − r∗(cid:105) =

(cid:68)

(cid:69)
diag (cid:0)eit − q(cid:1)(cid:2)Φrk0 − c − ασ (P, Φrk0)(cid:3), Φ(rk0 − r∗)

+ ht = (cid:104)eit − q, w(cid:105) + ht,

(32)

where |ht| ≤ Cε0 and w is a ﬁxed vector (depending on k0 only). It follows that

(cid:12)
T −1
(cid:12)
(cid:12)
∑
(cid:12)
(cid:12)
t=k0

γt(cid:104)∆(2)
t

(cid:12)
(cid:12)
, rt − γtU(rt) − r∗(cid:105)
(cid:12)
(cid:12)
(cid:12)

≤ C

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T −1
∑
t=k0

(cid:13)
(cid:13)
(cid:13)
γt(eit − q)
(cid:13)
(cid:13)

+Cε0

T −1
∑
t=k0

γt.

(33)

Dividing both sides of (33) by ∑T −1
t=k0
K large enough, so that the entire expression in parentheses in (31) is smaller than −δ (1 − α

γt and using (22), we see that we can choose ε > 0 small enough and k0 ∈
1 + κ)/2,

√

10

if T is large enough. But this yields (cid:107)rT − r∗(cid:107) → −∞, as T → ∞, a contradiction. Therefore, Condition B is
satisﬁed.
Condition C. The inequality (31) remains valid for T = s(k0, ε). By the deﬁnition of s(k0, ε),

(cid:13)
(cid:13)
(cid:13)

T −1
∑
t=k0

γt(dt + ξt)

(cid:13)
(cid:13)
(cid:13) ≥ ε.

By the convergence of (30), and the boundedness of {dt}, a constant C > 0 exists such that for all sufﬁciently
large k0 and sufﬁciently small ε, we have

T −1
∑
t=k0

γt ≥ ε/C.

Using (23), by a similar argument as in the analysis of Condition B, we can choose ε1 > 0 small enough
that for all k0 ∈ K large enough so that the entire expression in parentheses in (31) is smaller than −δ (1 −
α

1 + κ)/2. Therefore, for all ε ∈ (0, ε1] and all sufﬁciently large k0 ∈ K

√

(cid:107)rs(k0,ε) − r∗(cid:107)2 ≤ (cid:107)rk0 − r∗(cid:107)2 −

δ (1 − α

√

1 + κ)ε

2C

.

We ﬁx r∗ = ProjY ∗(rk0) on the right hand side, and obtain

W (rs(k0,ε)) ≤ (cid:107)rs(k0,ε) − r∗(cid:107)2 ≤ W (rk0) −

δ (1 − α

√

1 + κ)ε

2C

.

Now, the limit with respect to k0 → ∞, k0 ∈ K , proves Condition C.
Condition D is satisﬁed trivially, because W (r∗) ≡ 0 for r∗ ∈ Y ∗.

The only question remaining is the boundedness of the sequence {rt}.

It is a common issue in the
analysis of stochastic approximation algorithms [40, §5.1]. In our case, no additional conditions and analysis
are needed, because our Lyapunov function (26) is the squared distance to the optimal set. Therefore, a
simple algorithmic modiﬁcation: the projection on a bounded set Y intersecting with {r ∈ Rm : Φr = v∗}, is
sufﬁcient to guarantee boundedness. The modiﬁed method (17) reads:

rt+1 = ProjY

(cid:0)rt − γtϕ(it) (cid:101)dt

(cid:1),

t = 0, 1, 2, . . . .

(34)

Now, Y ∗ = {r ∈ Y : Φr = v∗} and we require that this set is nonempty. This modiﬁcation does not affect our
analysis in any meaningful way, because the projection is nonexpansive. In the proof of Theorem 3, we use
the inequality

2 ≤ (cid:13)
(cid:13)ProjY (r(cid:48) − γU(r(cid:48))) − ProjY (r(cid:48)(cid:48) − γU(r(cid:48)(cid:48)))(cid:13)
(cid:13)
(cid:13)

(cid:13)(r(cid:48) − γU(r(cid:48))) − (r(cid:48)(cid:48) − γU(r(cid:48)(cid:48)))(cid:13)
2
(cid:13)

and proceed as before. In the proof of Theorem 6, we start from

(cid:107)rt+1 − r∗(cid:107)2 = (cid:13)

(cid:13)ProjY

2 ≤ (cid:13)
(cid:0)rt − γt(U(rt) + ∆t)(cid:1) − r∗(cid:13)
(cid:13)

(cid:13)rt − γt(U(rt) + ∆t) − r∗(cid:13)
2,
(cid:13)

and then continue in the same way as before. We did not include projection into the method originally,
because it obscures the presentation. In practice, we have not yet encountered any need for it.

11

5 The Multistep Risk-Averse Method of Temporal Differences

In the method discussed so far, the residuals are corrected by moving in the direction of the last feature
vector ϕ(it). Alternatively, we may use the weighted averages of all previous observations, where the
highest weight is given to the most recent observation and the weights decrease exponentially as we look
into the past observations. This idea is the core of the well-known TD(λ ) algorithm [66]. We generalize it
to the risk-averse case.

For a ﬁxed policy π, we refer to vπ as v, and to Pπ as P, for simplicity. The multistep risk-averse method

of temporal differences carries out the following iterations:

zt = λ αzt−1 + ϕ(it),

t = 0, 1, 2, . . . ,

rt+1 = rt − γtzt (cid:101)dt,

t = 0, 1, 2, . . .

(35)

(36)

where λ ∈ [0, 1], and (cid:101)dt is given by (16). For simplicity, z−1 is assumed to be the zero vector.
risk-neutral case, when σit (Pit , Φrt) = Pit Φrt, the method reduces to the classical TD(λ ).

In the

Our convergence analysis will use some ideas from the analysis in the previous two sections, albeit in a
form adapted to the version with exponentially averaged features. However, contrary to the expected value
setting, the method (35)–(36) will converge to a solution of an equation different from (9), but still relevant
for our problem.

We start from a heuristic analysis of a deterministic counterpart of the method, to extract its drift. In the
next section, we make all approximations precise, but we believe that this introduction is useful to decipher
our detailed approach to follow. By direct calculation,

zt =

t
∑
k=0

(λ α)t−kϕ(ik),

(37)

and thus

ztdt = Φ (cid:62)

t
∑
k=0

(λ α)t−keik e(cid:62)
it

(cid:0)Φrt − c − ασ (P, Φrt)(cid:1).

Heuristically assuming that rt ≈ r(cid:48), we focus on the operator acting on the expected temporal differences.
As each of the observed feature vectors ϕ(ik) affects all succeeding steps of the method, via the ﬁlter (35),
we need to study the cumulative effect of many steps. We look, therefore, at the sums

GT = E

(cid:20) T
∑
t=0

γt

t
∑
k=0

(λ α)t−keik e(cid:62)
it

(cid:21)
.

Changing the order of summation and using the fact that {(λ α)t−k}t≥k diminishes very fast, as compared to
{γt}t≥k, we get

GT = E

(cid:20) T
∑
k=0

T
∑
t=k

γt(λ α)t−keik e(cid:62)
it

(cid:21)

≈ E

(cid:20) T
∑
k=0

γk

T
∑
t=k

(λ α)t−keik e(cid:62)
it

(cid:21)
.

12

Therefore

GT ≈ E

(cid:20) T
∑
k=0

γk

T
∑
t=k

(λ α)t−keik

E(cid:2)e(cid:62)
it

(cid:12)
(cid:12) Fk

(cid:21)
(cid:3)

= E

(cid:20) T
∑
k=0

γk

T
∑
t=k

(λ α)t−keik e(cid:62)
ik

Pt−k

(cid:21)

=

γkE[diag(eik )(cid:3)

T
∑
t=k

(λ α)t−kPt−k ≈

T
∑
k=0

γkE[diag(eik )(cid:3)

∞
∑
t=k

(λ α)t−kPt−k

T
∑
k=0
T
∑
k=0

≈ Q

γk

∞
∑
t=k

(λ α)t−kPt−k.

The last approximations are possible because λ α ∈ [0, 1) and E[diag(eik )(cid:3) → q at an exponential rate. We
now deﬁne the multistep transition matrix,

P = (1 − λ α)

∞
∑
(cid:96)=0

(λ α)(cid:96)P(cid:96).

By construction, P ∈ conv{I, P, P2, . . . }. With these approximations, we can simply write

GT ≈

1
1 − λ α

QP

T
∑
k=0

γk.

Deﬁne the operators

U(r) = Φ (cid:62)QP(cid:2)Φr − c − α σ (P, Φr)(cid:3),

t = 0, 1, 2, . . .

and consider the following deterministic counterpart of (35)–(36), with γ ∼ γt/(1 − λ α):

rr+1 = rt − γ U(rt),

t = 0, 1, 2, . . . ,

γ > 0.

(38)

(39)

(40)

Our intention is to show that for sufﬁciently small γ the method (40) converges to a point r∗ such that U(r∗) =
0. Such a point is also a solution of the following projected multistep risk-averse dynamic programming
equation:

LPΦr = LP(cid:0)c + ασ (P, Φr)(cid:1),

(41)

where L is the projection operator deﬁned in (7). The solutions of (41) differ from the solutions of (6), unlike
in the risk-neutral case (κ = 0). If we replace P with I, (41) reduces to (6).

Theorem 7. If α(1 + κ) < 1, then γ 0 > 0 exists, such that for all γ ∈ (0, γ 0) the algorithm (40) generates a
sequence {rt} convergent to a point r∗ such that U(r∗) = 0.

Proof. For two arbitrary points r(cid:48) and r(cid:48)(cid:48) we have

(cid:0)r(cid:48) − γ U(r(cid:48))(cid:1) − (cid:0)r(cid:48)(cid:48) − γ U(r(cid:48)(cid:48))(cid:1)(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
r(cid:48) − r(cid:48)(cid:48), Φ (cid:62)QP(cid:2) − Φ(r(cid:48) − r(cid:48)(cid:48)) + ασ (P, Φr(cid:48)) − ασ (P, Φr(cid:48)(cid:48))(cid:3)(cid:69)
(cid:68)

= (cid:107)r(cid:48) − r(cid:48)(cid:48)(cid:107)2 + 2γ

+ γ 2(cid:13)
(cid:13)

(cid:13)Φ (cid:62)QPΦ(r(cid:48) − r(cid:48)(cid:48)) − αΦ (cid:62)QP(cid:2)σ (P, Φr(cid:48)) − σ (P, Φr(cid:48)(cid:48))(cid:3)(cid:13)

2
(cid:13)
(cid:13)

.

(42)

13

We focus on the scalar product in the middle of the right hand side of (42):

Φ(r(cid:48) − r(cid:48)(cid:48)), P(cid:2) − Φ(r(cid:48) − r(cid:48)(cid:48)) + ασ (P, Φr(cid:48)) − ασ (P, Φr(cid:48)(cid:48))(cid:3)(cid:69)
(cid:68)
Φ(r(cid:48) − r(cid:48)(cid:48)), P(cid:2) − Φ(r(cid:48) − r(cid:48)(cid:48)) + αPΦ(r(cid:48) − r(cid:48)(cid:48))(cid:3)(cid:69)
(cid:68)

=

q

q

+ α

Φ(r(cid:48) − r(cid:48)(cid:48)), P(cid:2)σ (P, Φr(cid:48)) − σ (P, Φr(cid:48)(cid:48)) − PΦ(r(cid:48) − r(cid:48)(cid:48))(cid:3)(cid:69)
(cid:68)

q

.

(43)

Setting h = Φ(r(cid:48) − r(cid:48)(cid:48)), we can estimate the ﬁrst (quadratic) term on the right hand side of (43) by a calcu-
lation borrowed from [72, Lem. 8], with h = Φ(r(cid:48) − r(cid:48)(cid:48)):

h, P(cid:2) − h + αPh(cid:3)(cid:69)
(cid:68)

q

= (1 − αλ )

h,

(cid:68)

∞
∑
(cid:96)=0

(αλ )(cid:96)P(cid:96)(cid:2) − h + αPh(cid:3)(cid:69)

q

= (1 − αλ )(1 − λ )

h,

(cid:68)

= (1 − αλ )(1 − λ )

h,

(cid:68)

λ k

k
∑
(cid:96)=0

α (cid:96)P(cid:96)(cid:2) − h + αPh(cid:3)(cid:69)

q

λ k(cid:2)α k+1Pk+1h − h(cid:3)(cid:69)

q

= (1 − αλ )

h, (1 − λ )

(cid:68)

(cid:69)
λ kα k+1Pk+1h − h

q

∞
∑
k=0
∞
∑
k=0
∞
∑
k=0

= (1 − αλ )

h,

(cid:68)

α(1 − λ )
1 − αλ

PPh − h

(cid:69)

q

≤ (α − 1)(cid:107)h(cid:107)2
q.

The last inequality is due to the fact that both P and P are nonexpansive in (cid:107) · (cid:107)q.

The second (nonsmooth) term on the right hand side of (43) can be estimated by (11), again with the use

of the nonexpansiveness of P:

Φ(r(cid:48) − r(cid:48)(cid:48)), P(cid:2)σ (P, Φr(cid:48)) − σ (P, Φr(cid:48)(cid:48)) − PΦ(r(cid:48) − r(cid:48)(cid:48))(cid:3)(cid:69)
(cid:68)

q

≤ κ (cid:13)

(cid:13)Φ(r(cid:48) − r(cid:48)(cid:48))(cid:13)
2
q.
(cid:13)

The last term on the right hand side of (42) (with γ 2) can be bounded by γ 2C(cid:107)Φ(r(cid:48) − r(cid:48)(cid:48))(cid:107)2
constant. Integrating all these estimates into (42), we obtain the inequality

q, where C is some

(cid:13)(I − γ U)(r(cid:48)) − (I − γ U)(r(cid:48)(cid:48))(cid:13)
(cid:13)
2 ≤ (cid:107)r(cid:48) − r(cid:48)(cid:48)(cid:107)2 − 2γ
(cid:13)

(cid:16)
1 − α(1 + κ) −

γ C
2

(cid:17)(cid:13)
(cid:13)Φ(r(cid:48) − r(cid:48)(cid:48))(cid:13)
2
q.
(cid:13)

If α(1 + κ) < 1, then using 0 < γ < 2(1 − α(1 + κ))/C, we obtain:

(cid:13)(I − γ U)(r(cid:48)) − (I − γ U)(r(cid:48)(cid:48))(cid:13)
(cid:13)
2 ≤ (cid:107)r(cid:48) − r(cid:48)(cid:48)(cid:107)2 − γβ (cid:107)Φ(r(cid:48) − r(cid:48)(cid:48))(cid:107)2
q,
(cid:13)

(44)

with some β > 0. In particular, setting r(cid:48) = ¯rt and r(cid:48)(cid:48) = r∗ for a solution r∗ of (6), we obtain the following
relation between successive iterates of the method (40):

(cid:107)rt+1 − r∗(cid:107)2 ≤ (cid:107)rt − r∗(cid:107)2 − γβ (cid:107)Φ(rt − r∗)(cid:107)2
q.
This immediately proves that the sequence {rt} is bounded and Φrt → Φr∗. Every accumulation point ˆr
of {rt} must be then a solution of equation (41). Substituting this accumulation point for r∗ in the last
inequality, we conclude that the entire sequence {rt} is convergent to ˆr.

(45)

If Φ has full column rank, the solution r∗ is unique, because substituting another solution for rt in (40)

we obtain rt+1 = rt, which leads to a contradiction in (45).

14

6 Convergence of the Risk-Averse Multistep Method

We now carry out a detailed analysis of the stochastic method (35)–(36).
Lemma 8. For any array of uniformly bounded random variables (cid:8)Ak,t

(cid:9)

k≥0,t≥0

lim
T →∞

∑T

k=0 ∑T

t=k γt(λ α)t−kAk,t − ∑T
k=0 γk

∑T

k=0 γk ∑∞

t=k(λ α)t−kAk,t

= 0,

a.s.

Proof. Changing the order of summation twice, we obtain

T
∑
k=0

T
∑
t=k+1

|γt − γk|(λ α)t−k ≤

T
∑
k=0

T
∑
t=k+1

t
∑
(cid:96)=k+1

|γ(cid:96) − γ(cid:96)−1|(λ α)t−k

|γ(cid:96) − γ(cid:96)−1|(λ α)(cid:96)−k =

1
1 − λ α

T
∑
(cid:96)=1

|γ(cid:96) − γ(cid:96)−1|

(cid:96)−1
∑
k=0

(λ α)(cid:96)−k

≤

≤

1
1 − λ α

T
∑
k=0

λ α
(1 − λ α)2

T
∑
(cid:96)=k+1
T
∑
(cid:96)=1

|γ(cid:96) − γ(cid:96)−1|.

Therefore, with C being the uniform bound on (cid:107)Ak,t(cid:107) and γ max

k = maxt≥k γt, we obtain

(cid:13)
(cid:13)
(cid:13)
(cid:13)

T
∑
k=0

≤ C

T
∑
t=k
T
∑
k=0

T
∑
t=k+1

γt(λ α)t−kAk,t −

T
∑
k=0

γk

∞
∑
t=k

(λ α)t−kAk,t

(cid:13)
(cid:13)
(cid:13)
(cid:13)

|γt − γk|(λ α)t−k +C

γt(λ α)t−k

∞
∑
t=T +1

T
∑
k=0
Cγ max
T +1λ α
(1 − λ α)2 .

≤

Cλ α
(1 − λ α)2

T
∑
(cid:96)=1

|γ(cid:96) − γ(cid:96)−1| +

Assumption 1(iv) and [63, Lem. A.3] imply the assertion.

We need another auxiliary result, extending Lemma 5 to our case.

Lemma 9.

and for any ε > 0,

(cid:18)

∑T

t=0 γt

lim
T →∞

∑t

k=0(λ α)t−keik e(cid:62)
it
∑T

t=0 γt

(cid:19)

− 1

1−λ α QP

= 0 a.s.,

(46)

(cid:18)

∑T

t=t0 γt

lim
t0→∞

sup
T ≥t0

(cid:19)

− 1

1−λ α QP

∑t

k=0(λ α)t−keik e(cid:62)
it
(cid:17)

(cid:16)
ε, ∑T

max

t=t0 γt

= 0 a.s..

(47)

Proof. Consider the sums appearing in the numerator of (9):

T
∑
t=0

γt

t
∑
k=0

(λ α)t−keik e(cid:62)

it =

(λ α)t−kγte(cid:62)

it , T = 1, 2, . . . .

T
∑
k=0

eik

T
∑
t=k

15

In view of Lemma 8, it is sufﬁcient to consider the sums

ST =

T
∑
k=0

γk eik

∞
∑
t=k

(λ α)t−ke(cid:62)

it , T = 1, 2, . . . .

We transform the inner sum:

∞
∑
t=k

((λ α))t−ke(cid:62)

it =

∞
∑
t=k

(λ α)t−k(cid:110) t
∑
(cid:96)=k+1

(cid:2)e(cid:62)
i(cid:96)

Pt−(cid:96) − e(cid:62)
i(cid:96)−1

Pt−(cid:96)+1(cid:3) + e(cid:62)
ik

Pt−k(cid:111)

=

∞
∑
t=k

(λ α)t−ke(cid:62)
ik

Pt−k +

∞
∑
(cid:96)=k+1

∞
∑
t=(cid:96)

(λ α)t−k(cid:2)e(cid:62)
i(cid:96)

Pt−(cid:96) − e(cid:62)
i(cid:96)−1

Pt−(cid:96)+1(cid:3).

We can thus write ST = S(1)

T + S(2)

T , with

S(1)
T =

T
∑
k=0

γk eik e(cid:62)
ik

∞
∑
t=k

(λ α)t−kPt−k =

1
1 − λ α

T
∑
k=0

γk diag(eik )P

and

S(2)
T =

=

=

T
∑
k=0

γk eik

1
1 − λ α

∞
∑
(cid:96)=k+1
T
∑
k=0

γk eik

∞
∑
t=(cid:96)

(λ α)t−k(cid:2)e(cid:62)
i(cid:96)

Pt−(cid:96) − e(cid:62)
i(cid:96)−1

Pt−(cid:96)+1(cid:3)

∞
∑
(cid:96)=k+1

(λ α)(cid:96)−k(cid:2)e(cid:62)
i(cid:96)

− e(cid:62)
i(cid:96)−1

P(cid:3)P

1
1 − λ α

∞
∑
(cid:96)=1

min(T,(cid:96)−1)
∑
k=0

γk eik (λ α)(cid:96)−k(cid:2)e(cid:62)
i(cid:96)

− e(cid:62)
i(cid:96)−1

P(cid:3)P.

The second sum is a convergent martingale, because E(cid:2)e(cid:62)
i(cid:96)

(cid:12)
(cid:12) F(cid:96)−1
k=0 γk diag(q)P, we obtain both assertions.

(cid:3) = e(cid:62)
i(cid:96)−1

P. Therefore, it satisﬁes (46).

Applying Lemma 5 to S(1)

T − 1

1−λ α ∑T

Now we can follow the arguments of §4 and establish the convergence of the multistep method.

Theorem 10. Assume that α(1 + κ) < 1, the sequence {rt} is bounded with probability 1, and the random
estimates (cid:101)σit (Pit , Φrt) satisfy (15). Then, with probability 1, every accumulation point of the sequence {rt}
generated by (35)–(36) is a solution of (41).

Proof. We represent the direction used in (36) at step t as

zt (cid:101)dt =

1
1 − λ α

with the operator U t(·) deﬁned in (39), and

U t(rt) + ∆(1)

t + ∆(2)

t

,

∆(1)
t = −αztξt,
∆(2)
t = ztdt −

1
1 − λ α

U t(rt).

16

For any r∗ solving (41), with γt = γt/(1 − λ α), we have

2 = (cid:13)
(cid:13)rt+1 − r∗(cid:13)
(cid:13)
(cid:13)

(cid:13)rt − γtU t(rt) − r∗(cid:13)
2 − 2γt(cid:104)∆(1)
(cid:13)

t + ∆(2)

t

, rt − γtU t(rt) − r∗(cid:105) + γ 2
t

(cid:13)
(cid:13)∆(1)

t + ∆(2)

t

(cid:13)
2.
(cid:13)

Our intention is to verify the conditions of Theorem 4 for almost all paths of the sequence {rt}.

Condition A. The sequence {zt} is bounded by construction. Since the series (30) is a convergent mar-

tingale, we conclude that limt→∞ γtzt (cid:101)dt = 0.

Conditions B and C: We follow the proof of Theorem 6. The deterministic term involving U t(rt) can be

estimated as in (45):

2 ≤ (cid:13)
(cid:13)rt − γtU t(rt) − r∗(cid:13)
(cid:13)
(cid:13)

(cid:13)rt − r∗(cid:13)
2 − 2γt
(cid:13)

(cid:0)1 − α(1 + κ)(cid:1)(cid:13)

(cid:13)Φ(rt − r∗)(cid:13)
2
q +Cγ 2
t .
(cid:13)

Since {zt} and {rt} are bounded, Assumptions 1 and 2 imply that ∑∞
vergent martingale.

t=0 γt(cid:104)∆(1)

t

, rt − γtU t(rt) − r∗(cid:105) is a con-

To analyze the second error term, ∆(2)

t

otherwise, the formula (37) yields

, we observe that for a vector eik having 1 at position ik and zero

ztdt =

(λ α)t−kϕ(ik)(cid:0)ϕ (cid:62)(it)rt − c(it) − ασit (Pit , Φrt)(cid:1)

(λ α)t−keik e(cid:62)
it

(cid:17)(cid:0)Φrt − c − ασ (P, Φrt)(cid:1).

t
∑
k=0
= Φ (cid:62)(cid:16) t
∑
k=0

Subtracting (39), we obtain

t = Φ (cid:62)(cid:16) t
∆(2)
∑
k=0

(λ α)t−keik e(cid:62)

it −

1
1 − λ α

(cid:17)(cid:2)Φrt − c − ασ (P, Φrt)(cid:3).

QPt

By virtue of Lemma 9, for any ε > 0,

lim
T →∞

t=0 γt∆(2)
∑T
t
∑T
t=0 γt

= 0,

lim
t0→∞

sup
T ≥t0

t

t=t0 γt∆(2)
∑T
(cid:16)
ε, ∑T
t=t0 γt

max

(cid:17) = 0

a.s..

The remaining analysis is the same as in the proof of Theorem 6. We obtain an inequality corresponding

to (31):

(cid:107)rT − r∗(cid:107)2 ≤ (cid:107)rk0 − r∗(cid:107)2

(cid:32)

+

−δ (1 − α(1 + κ)) +

∑T −1
t=k0

γt(cid:104)∆(1)

t + ∆(2)
t
∑T −1
t=k0

γt

, rt − γtU(rt) − r∗(cid:105)

+C

∑T −1
t=k0
∑T −1
t=k0

γ 2
t
γt

(cid:33) T −1
∑
t=k0

γt,

with δ > 0. This allows us to verify the conditions of Theorem 4 and prove our assertion following the last
steps of the proof of Theorem 6 verbatim.

It is worth mentioning that the convergence condition for the multistep method: α(1 + κ) < 1, is slightly

stronger that the condition for the basic method: α

√

1 + κ < 1.

17

Again, as in the case of the basic method, discussed in §4, the boundedness of the sequence {rk} is not
an issue of concern, because it can be guaranteed by projection on a bounded set Y . The modiﬁed method
has the following form:

rt+1 = ProjY

(cid:0)rt − γtzt (cid:101)dt

(cid:1),

t = 0, 1, 2, . . . .

(48)

We just need Y to have a nonempty intersection Y ∗ with the set of solutions of (41). Due to the nonexpan-
siveness of the projection operator, all our proofs remain unchanged with this modiﬁcation, as discussed at
the end of §4.

7 Empirical Study

7.1 Risk estimation

We ﬁrst discuss the issue of obtaining stochastic estimates (cid:101)σit (Pit , ·) satisfying (15) and Assumption 2:

E(cid:2)

(cid:101)σit (Pit , Φrt)(cid:12)
In the expected value case, where σit (Pit , Φrt) = Pit Φrt = E(cid:2)ϕ (cid:62)(it+1)rt
(cid:3), we could just use the ap-
proximation value at the next state observed, ϕ (cid:62)(it+1)rt, as the stochastic estimate of the expected value
function. However, due to the nonlinearity of a risk measure with respect to the probability measure Pit ,
such a straightforward approach is no longer possible.

(cid:3) = σit (Pit , Φrt),

t = 0, 1, 2, . . . ,

(cid:12)
(cid:12) Ft

(cid:12) Ft

(49)

Statistical estimation of measures of risk is a challenging problem, for which, so far, only solutions in
special cases have been found [22]. To mitigate this problem, we propose to use a special class of transition
risk mappings which are very convenient for statistical estimation. For a given transition risk mapping
σ i(Pi, v), we sample N conditionally independent transitions from the state i, resulting in states j1, . . . , jN.
k=1 ejk , where ej is the jth unit vector in Rn.
This sample deﬁnes a random empirical distribution, PN
Since the sample is ﬁnite, we can calculate the plug-in risk measure estimate,

i = 1

N ∑N

i (Pi, v) = σ i(PN
(cid:101)σ N

i , v),

(50)

by a closed-form expression. One can verify directly from the deﬁnition that the resulting sample-based
transition risk mapping

i (Pi, v) = E(cid:2)σ i(PN
σ N
satisﬁes all conditions of a transition risk mapping of §2, if σ i(·, ·) does. The expectation above is over all
possible N-samples. Therefore, if we treat σ N
i (·, ·) as the “true” risk measure that we want to estimate, the
plug-in formula (50) satisﬁes (15) and Assumption 2. In fact, for a broad class of measures of risk σ i(Pi, v),
we have a central limit result: σ i(PN
N, and the error has an
approximately normal distribution [22]. However, we do not rely on this result here, because we work with
ﬁxed N. In our experiments, the sample size N = 4 turned out to be sufﬁcient, and even N = 2 would work
well.

i , v) is convergent to σ i(Pi, v) at the rate 1/

i , v)(cid:3),

√

7.2 Example

We apply the risk-averse methods of temporal differences to a version of a transportation problem discussed
in [53]. We have vehicles at M = 50 locations. At each time period t, a stochastic demand Dijt for transporta-
tion from location i to location j occurs, i, j = 1, . . . , M, t = 1, 2, . . .. The demand arrays Dt in different time

18

periods are independent. The vehicles available at location i may be used to satisfy this demand. They may
also be moved empty. The state xt of the system at time t is the M-dimensional integer vector containing the
numbers of vehicles at each location.

For simplicity, we assume that a vehicle can carry a unit demand, and the total demand at the location i
at time t can be satisﬁed only if xit ≥ ∑M
j=1 Dijt; otherwise, the demand may be only partially satisﬁed and
the excess demand is lost. One can relocate the vehicles empty or loaded, and we denote the cost of moving
a vehicle empty from location i to location j as ce
i j. Since we stay in a cost minimization setting, we also
denote the net negative proﬁt of moving a vehicle loaded from location i to location j as c(cid:96)
ijt be the
number of vehicles moved empty from location i to location j at time t and u(cid:96)
ijt be the number of vehicles
that are moved loaded. For simplicity, let us refer to the combination of ue

i j. Let ue

t as ut and denote:

t and u(cid:96)

c(cid:62)ut =

M
∑
i, j=1

(cid:0)ce

i jue

ijt + c(cid:96)

i ju(cid:96)
ijt

(cid:1).

In this problem, the control ut is decided after the state xt and the demand Dt are observed. The next state is
a linear function of xt and ut:

xt+1 = xt − Aut,

where A can be written in an explicit way by counting the outgoing and incoming vehicles.

We denote by U (xt, Dt) the set of decisions that can be taken at state xt under demand Dt. Our approach

allows us to evaluate a look-ahead policy deﬁned by a simple linear programming problem:

uπ
t (xt, Dt) = argmin
u∈U (xt ,Dt )

(cid:110)

c(cid:62)u + απ (cid:62)(xt − Au)

(cid:111)
.

(51)

Here, π is the vector of approximate next-state values fully deﬁning the policy. In our case, the immediate
cost c(cid:62)ut depends on Dt, and thus the risk-averse policy evaluation equation (3) has the following form:

vπ (x) = σ

P, c(cid:62)uπ (x, D) + αvπ (cid:0)x − Auπ (x, D)(cid:1)(cid:17)
(cid:16)
,

with P denoting the distribution of the demand. Our objective is to evaluate the policy π and to improve it.
As the size of the state space is enormous, we resort to linear approximations of form (5), using the state x
as the feature vector: (cid:101)v(xt) = x(cid:62)
t r. The approximate risk-averse dynamic programming equation (6) takes
on the form:

r(cid:62)x = σ

P, c(cid:62)uπ (x, D) + αr(cid:62)(cid:0)x − Auπ (x, D)(cid:1)(cid:17)
(cid:16)
.

(52)

We omit the projection operator, because the feature space has full dimension. Thanks to that, the multi-
step approximate risk-averse dynamic programming equation (41) coincides with (52), and all risk-averse
methods with λ ∈ [0, 1] solve the same equation.

In fact, we can combine the learning and policy improvement in one process, known as the optimistic

approach, in which we always use the current rt as the vector π deﬁning the policy.

7.3 Results

We tested the risk-averse and the risk-neutral TD(λ ) methods under the same long simulated sequence of
demand vectors. At every time t, we sampled N = 4 instances of the demand vectors, and for each instance,

19

we computed the best decisions by (51), and the resulting states. Then we computed the empirical risk
measure (50) of the approximate value of the next state, and we used it in the observed temporal difference
calculation (16):

(cid:101)dt = r(cid:62)

t xt − ασ

(cid:16)
PN, c(cid:62)urt (xt, D) + αr(cid:62)
t

(cid:0)xt − Aurt (xt, D)(cid:1)(cid:17)
.

We used the mean–semideviation risk measure [48] as σ (·, ·), which can be calculated in closed form for an
empirical distribution PN with observed transition costs w(1), . . . , v(N):

σ (PN, v) = µ + β

1
N

N
∑
j=1

max(0, w( j) − µ), µ =

1
N

N
∑
j=1

w( j),

β ∈ [0, 1].

We used β = 1, N = 4, and α = 0.95. In the expected value model (β = 0), we also used N = 4 observations
per stage, and we averaged them, to make the comparison fair. The choice of N = 4 was due to the use of a
four-core computer, on which the N transitions can be simulated and analyzed in parallel.

We compared the performance of the risk-averse and risk-neutral TD(λ ) algorithms for λ = 0, 0.5, and
0.9, in terms of average proﬁt per stage, on a trajectory with 20,000 decision stages. The results are depicted
in Figure 1. We observe that the risk-averse algorithms outperform their risk-neutral counterparts in terms
of the average proﬁt in the long run. We also observe that the difference in performance is more signiﬁcant
when λ is closer to zero. It would appear that with risk-averse learning no additional advantage is gained by
using λ > 0.

In addition to these results, we used 207 distinct trajectories, each with 200 decision stages, to compare
the performance of the risk-averse and risk-neutral algorithms at the early training stages in terms of proﬁt
per stage. Figure 2 shows the empirical distribution function of the proﬁt per stage of the risk-averse and
risk-neutral algorithms at t = 200, for λ = 0, 0.5, and 0.9. The results demonstrate that in the early stages
of learning (t = 200), the average proﬁt of the risk-averse algorithm is more likely to be higher than that
of the risk-neutral algorithm, and the difference is very pronounced for lower values of λ . The ﬁrst order
stochastic dominance relation between empirical distributions appears to exist.

Although the risk-averse methods aim at optimizing the dynamic risk measure, rather than the expected
value, they outperform the expected value model also in expectation. This may be due to the fact that the use
of risk measures makes the method less sensitive to the imperfections of the value function approximation.

Acknowledgments

The authors acknowledge the Ofﬁce of Advanced Research Computing (http://oarc.rutgers.edu) at Rutgers,
The State University of New Jersey, for providing access to the Amarel cluster and associated research
computing resources that have contributed to the results reported here.

20

(a) λ = 0

(b) λ = 0.5

(c) λ = 0.9

Figure 1: Evolution of the average proﬁt per stage.

References

[1] A. Arlotto, N. Gans, and J. M. Steele. Markov decision problems where means bound variances.

Operations Research, 62(4):864–875, 2014.

[2] P. Artzner, F. Delbaen, J.-M. Eber, and D Heath. Coherent measures of risk. Mathematical Finance,

9(3):203–228, 1999.

[3] P. Artzner, F. Delbaen, J.-M. Eber, D. Heath, and H. Ku. Coherent multiperiod risk adjusted values

and Bellman’s principle. Annals of Operations Research, 152:5–22, 2007.

[4] N. B¨auerle and U. Rieder. More risk-sensitive Markov decision processes. Mathematics of Operations

Research, 39(1):105–120, 2013.

[5] R. E. Bellman. A Markovian decision process. Journal of Mathematics and Mechanics, 6(5):679–684,

1957.

21

(a) λ = 0

(b) λ = 0.5

(c) λ = 0.9

Figure 2: Empirical distribution of the average proﬁt at t = 200.

[6] Richard Bellman, Robert Kalaba, and Bella Kotkin. Polynomial approximation – a new computational

technique in dynamic programming. Math. Comp., 17(8):155–161, 1963.

[7] D. P. Bersekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc, 4 edition, 2017.

[8] T. Bielecki, D. Hern´andez-Hern´andez, and S. R. Pliska. Risk sensitive control of ﬁnite state Markov
chains in discrete time, with applications to portfolio management. Mathematical Methods of Opera-
tions Research, 50(2):167–188, 1999.

[9] V. S. Borkar. Q-learning for risk-sensitive control. Mathematics of Operations Research, 27(2):294–

311, 2002.

[10] V.S. Borkar. A sensitivity formula for risk-sensitive cost and the actorcritic algorithm. Systems &

Control Letters, 44(5):339 – 346, 2001.

22

[11]

¨O. C¸ avus and A. Ruszczy´nski. Computational methods for risk-averse undiscounted transient Markov
models. Operations Research, 62(2):401–417, 2014.

[12]

¨O. C¸ avus and A. Ruszczy´nski. Risk-averse control of undiscounted transient Markov models. SIAM
Journal on Control and Optimization, 52(6):3935–3966, 2014.

[13] Z. Chen, G. Li, and Y. Zhao. Time-consistent investment policies in Markovian markets: a case of

mean-variance analysis. J. Econom. Dynam. Control, 40:293–316, 2014.

[14] P. Cheridito, F. Delbaen, and M. Kupper. Dynamic monetary risk measures for bounded discrete-time

processes. Electronic Journal of Probability, 11:57–106, 2006.

[15] P. Cheridito and M. Kupper. Composition of time-consistent dynamic monetary risk measures in
discrete time. International Journal of Theoretical and Applied Finance, 14(01):137–162, 2011.

[16] Y. Chow and M. Ghavamzadeh. Algorithms for CVaR optimization in MDPs. In Advances in neural

information processing systems, pages 3509–3517, 2014.

[17] K. J. Chung and M. J. Sobel. Discounted MDPs: distribution functions and exponential utility maxi-

mization. SIAM, 25:49–62, 1987.

[18] S. P. Coraluppi and S. I. Marcus. Risk-sensitive and minimax control of discrete-time, ﬁnite-state

Markov decision processes. Automatica, 35(2):301–309, 1999.

[19] P. Dayan. The convergence of TD(λ ) for general λ . Machine Learning, 8:341–362, 1992.

[20] P. Dayan and T. Sejnowski. TD(λ ) converges with probability 1. Machine Learning, 14:295–301,

1994.

[21] E. V. Denardo and U. G. Rothblum. Optimal stopping, exponential utility, and linear programming.

Math. Programming, 16(2):228–244, 1979.

[22] D. Dentcheva, S. Penev, and A. Ruszczy´nski. Statistical estimation of composite risk functionals and

risk optimization problems. Annals of the Institute of Statistical Mathematics, 69(4):737–760, 2017.

[23] D. Dentcheva and A. Ruszczy´nski. Risk forms: representation, disintegration, and application to

partially observable two-stage systems. Mathematical Programming, pages 1–21, 2019.

[24] G. B. Di Masi and Ł. Stettner. Risk-sensitive control of discrete-time Markov processes with inﬁnite

horizon. SIAM J. Control Optim., 38(1):61–78, 1999.

[25] J. Fan and A. Ruszczy´nski. Process-based risk measures and risk-averse control of discrete-time sys-

tems. Mathematical Programming, pages 1–28, 2018.

[26] J. Fan and A. Ruszczy´nski. Risk measurement and risk-averse control of partially observable discrete-

time markov systems. Mathematical Methods of Operations Research, pages 1–24, 2018.

[27] B. G. Farley and W. A. Clark. Simulation of self-organizing systems by digital computer. IRE Trans-

actions on Information Theory, 4:76–84, 1954.

[28] J. A. Filar, L. C. M. Kallenberg, and H.-M. Lee. Variance-penalized Markov decision processes. Math.

Oper. Res., 14(1):147–161, 1989.

23

[29] W. H. Fleming and S. J. Sheu. Optimal long term growth rate of expected utility of wealth. The Annals

of Applied Probability, 9:871–903, 1999.

[30] H. F¨ollmer and I. Penner. Convex risk measures and the dynamics of their penalty functions. Statistics

& Decisions, 24(1/2006):61–96, 2006.

[31] JB Hiriart-Urruty. Mean value theorems in nonsmooth analysis. Numerical Functional Analysis and

Optimization, 2(1):1–30, 1980.

[32] R. A. Howard. Dynamic Programming and Markov Processes. John Wiley & Sons, 1960.

[33] R. A. Howard and J. E. Matheson. Risk-sensitive Markov decision processes. Management Sci.,

18:356–369, 1971/72.

[34] T. Jaakkola, M. I. Jordan, and S. P. Singh. On the convergence of stochastic iterative dynamic pro-

gramming algorithms. Neural Computation, 6:1185–1201, 1994.

[35] S. C. Jaquette. Markov decision processes with a new optimality criterion: discrete time. Ann. Statist.,

1:496–505, 1973.

[36] S. C. Jaquette. A utility criterion for Markov decision processes. Management Sci., 23(1):43–49,

1975/76.

[37] A. Ja´skiewicz, J. Matkowski, and A. S. Nowak. Persistently optimal policies in stochastic dynamic
programming with generalized discounting. Mathematics of Operations Research, 38(1):108–121,
2013.

[38] A. Jobert and L. C. G. Rogers. Valuations and dynamic convex risk measures. Mathematical Finance,

18(1):1–22, 2008.

[39] S. Kl¨oppel and M. Schweizer. Dynamic indifference valuation via convex risk measures. Math. Fi-

nance, 17(4):599–627, 2007.

[40] H. Kushner and G. G. Yin. Stochastic Approximation Algorithms and Applications. Springer, New

York, 2003.

[41] S. Levitt and A. Ben-Israel. On modeling risk in Markov decision processes. In Optimization and
Related Topics (Ballarat/Melbourne, 1999), volume 47 of Appl. Optim., pages 27–40. Kluwer Acad.
Publ., Dordrecht, 2001.

[42] K. Lin and S. I. Marcus. Dynamic programming with non-convex risk-sensitive measures. In American

Control Conference (ACC), 2013, pages 6778–6783. IEEE, 2013.

[43] W.-J. Ma, D. Dentcheva, and M. M. Zavlanos. Risk-averse sensor planning using distributed policy

gradient. In 2017 American Control Conference (ACC), pages 4839–4844. IEEE, 2017.

[44] S. Mannor and J. N. Tsitsiklis. Algorithmic aspects of mean-variance optimization in Markov decision

processes. European J. Oper. Res., 231(3):645–653, 2013.

24

[45] S. I. Marcus, E. Fern´andez-Gaucherand, D. Hern´andez-Hern´andez, S. Coraluppi, and P. Fard. Risk
sensitive Markov decision processes. In Systems and Control in the Twenty-First Century (St. Louis,
MO, 1996), volume 22 of Progr. Systems Control Theory, pages 263–279. Birkh¨auser, Boston, MA,
1997.

[46] M. L. Minsky. Theory of Neural-Analog Reinforcement Systems and Its Application to the Brain-Model

Problem. PhD thesis, Princeton University, 1954.

[47] E. A. Nurminski. Convergence conditions for nonlinear programming methods. Kibernetika (Kiev),

(6):79–81, 1972.

[48] W. Ogryczak and A. Ruszczy´nski. From stochastic dominance to mean–risk models: semideviations

as risk measures. European Journal of Operational Research, 116:33–50, 1999.

[49] J. Peng. Efﬁcient Dynamic Programming-Based Learning for Control. PhD thesis, Northeastern Uni-

versity, 1993.

[50] J. Peng and R. J. Williams. Incremental multi-step Q-learning. Proceedings of the Eleventh Interna-

tional Conference on Machine Learning, pages 226–232, 1994.

[51] G.Ch. Pﬂug and W. R¨omisch. Modeling, Measuring and Managing Risk. World Scientiﬁc, Singapore,

2007.

[52] W. B. Powell. Approximate Dynamic Programming - Solving the Curses of Dimensionality. Wiley,

2011.

[53] W. B. Powell and H. Topaloglu. Approximate dynamic programming for large-scale resource alloca-
tion problems. In Models, Methods, and Applications for Innovative Decision Making, pages 123–147.
INFORMS, 2006.

[54] L. A. Prashanth and Mohammad Ghavamzadeh. Actor-critic algorithms for risk-sensitive reinforce-

ment learning. CoRR, abs/1403.6530, 2014.

[55] M. L. Puterman. Markov Decision Processes. Wiley, 1994.

[56] F. Riedel. Dynamic coherent risk measures. Stochastic Processes and Their Applications, 112:185–

200, 2004.

[57] B. Roorda, J. M. Schumacher, and J. Engwerda. Coherent acceptability measures in multiperiod mod-

els. Mathematical Finance, 15(4):589–612, 2005.

[58] G. A. Rummery. Problem Solving with Reinforcement Learning. PhD thesis, Cambridge University,

1995.

[59] G. A. Rummery and M. Niranjan. On-line Q-learning using connectionist systems. Technical report,

Engineering Department, Cambridge University, 1994.

[60] A. Ruszczy´nski. Risk-averse dynamic programming for Markov decision processes. Math. Program.,

125(2, Ser. B):235–261, 2010.

25

[61] A. Ruszczy´nski and A. Shapiro. Conditional risk mappings. Mathematics of Operations Research,

31(3):544–561, 2006.

[62] A. Ruszczy´nski and A. Shapiro. Optimization of convex risk functions. Mathematics of Operations

Research, 31(3):433–452, 2006.

[63] A. Ruszczy´nski and W. Syski. Stochastic approximation method with gradient averaging for uncon-

strained problems. IEEE Transactions on Automatic Control, 28(12):1097–1105, 1983.

[64] G. Scandolo. Risk Measures in a Dynamic Setting. PhD thesis, Universit`a degli Studi di Milano, Milan,

Italy, 2003.

[65] Y. Shen, W. Stannat, and K. Obermayer. Risk-sensitive Markov control processes. SIAM Journal on

Control and Optimization, 51(5):3652–3672, 2013.

[66] R. S. Sutton. Learning to predict by the method of temporal differences. Machine Learning, 3:9–44,

1988.

[67] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. Cambridge: MIT press, 1998.

[68] A. Tamar, Y. Chow, M. Ghavamzadeh, and S. Mannor. Sequential decision making with coherent risk.

IEEE Transactions on Automatic Control, 62(7):3323–3338, 2017.

[69] A. Tamar, D. Di Castro, and S. Mannor. Policy gradients with variance related risk criteria. In Pro-
ceedings of the twenty-ninth international conference on machine learning, pages 387–396, 2012.

[70] A. Tamar, S. Mannor, and H. Xu. Scaling up robust mdps using function approximation. In Interna-

tional Conference on Machine Learning, pages 181–189, 2014.

[71] J. N. Tsitsiklis. Asynchronous stochastic approximation and Q-learning. Machine Learning, 16:185–

202, 1994.

[72] J. N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approxima-

tion. IEEE Fransactions on Automatic Control, 42(5):674–690, 1997.

[73] C. J. C. H. Watkins. Learning from Delayed Rewards. PhD thesis, Cambridge University, 1989.

[74] C. J. C. H. Watkins and P. Dayan. Q - learning. Machine Learning, 8(3-4):279–292, 1992.

[75] L. L. Wegge. Mean value theorem for convex functions.

Journal of Mathematical Economics,

1(2):207–208, 1974.

[76] D. J. White. Mean, variance, and probabilistic criteria in ﬁnite Markov decision processes: a review.

J. Optim. Theory Appl., 56(1):1–29, 1988.

26

