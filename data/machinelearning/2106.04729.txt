1
2
0
2

v
o
N
9
2

]

C
O
.
h
t
a
m

[

2
v
9
2
7
4
0
.
6
0
1
2
:
v
i
X
r
a

A MARKOV DECISION PROCESS APPROACH FOR MANAGING
MEDICAL DRONE DELIVERIES

Amin Asadi ∗
Department of Industrial Engineering
and Business Information Systems
University of Twente, Enschede, Netherlands

Sarah Nurre Pinkley
Department of Industrial Engineering
University of Arkansas
4207 Bell Engineering, Fayetteville

Department of Industrial Engineering
University of Arkansas
4207 Bell Engineering, Fayetteville
amin.asadi@utwente.nl

Martijn Mes
Department of Industrial Engineering
and Business Information Systems
University of Twente, Enschede, Netherlands

ABSTRACT

We consider the problem of optimizing the distribution operations at a drone hub that dispatches
drones to different geographic locations generating stochastic demands for medical supplies. Drone
delivery is an innovative method that introduces many beneﬁts, such as low-contact delivery, thereby
reducing the spread of pandemic and vaccine-preventable diseases. While we focus on medical
supply delivery for this work, drone delivery is suitable for many other items, including food, postal
parcels, and e-commerce. In this paper, our goal is to address drone delivery challenges related to the
stochastic demands of different geographic locations. We consider different classes of demand related
to geographic locations that require different ﬂight ranges, which is directly related to the amount of
charge held in a drone battery. We classify the stochastic demands based on their distance from the
drone hub, use a Markov decision process to model the problem, and perform computational tests
using realistic data representing a prominent drone delivery company. We solve the problem using
a reinforcement learning method and show its high performance compared with the exact solution
found using dynamic programming. Finally, we analyze the results and provide insights for managing
the drone hub operations.

Keywords Markov Decision Processes · Drones · Healthcare · Routing · Dynamic Scheduling Allocation ·
Reinforcement Learning

1

Introduction

During the last decade, there has been substantial growth in the use of drones for various applications, including but not
limited to transportation, agriculture, and delivery [1, 2, 3, 4]. Speciﬁcally, delivery using drones has received extensive
attention as it can reduce air pollution and trafﬁc in congested areas [5]. Moreover, drones are a viable option to reach
remote locations with inadequate road infrastructure [6]. During pandemics, drones provide a safe and low contact
delivery method, which can effectively slow down the spread of the diseases [7, 8]. Many companies and organizations,
including Vanuatu’s Ministry of Health and Civil Aviation [9], Zipline [10], Matternet [11], and Manna Aero [12] use
drones to deliver and distribute medical supplies such as vaccines, medicine, and blood units. Effective operations of

∗Corresponding author.

 
 
 
 
 
 
a ﬂeet of drones require addressing battery-oriented issues, including limited ﬂight range, time-consuming charging
operations, and the high price and short lifetime of batteries. In this research, we provide a model that effectively uses
the charge inside the batteries to maximize the amount of stochastic demand met. Speciﬁcally, we classify the stochastic
demands according to drones’ ﬂight range, which depends on the charge inside drone batteries. We ultimately maximize
the expected total met demand for delivering medical items using drones dispatched from a battery swap station located
in a drone hub.

Battery swap stations are a solution to alleviate the aforementioned issues. In battery swap stations, charged batteries
are swapped with empty batteries in short minutes. For instance, Matternet provides a station to automatically swap
drone batteries used for delivering blood units and medicine between the supplier and hospitals [13]. Besides the
quick swapping operation, recharging batteries in anticipation of demand can reduce overcharging and fast charging
of batteries, which are shown to accelerate the battery degradation process [14, 15]. The faster batteries degrade, the
quicker the need for battery replacement, leading to a higher cost and environmental waste. The application of a battery
swap station is not limited to drones and can be extended to electric vehicles [16, 17], electric scooters [18] and cell
phone battery swaps in airports, hotels, and amusement parks [19]. Notably, the number of electric vehicle swap stations
is growing in different regions of the world [20, 21, 22, 23]. In this research, we consider a swap station located at
a drone hub that dispatches drones to satisfy multiple classes of stochastic demand for medical supplies, which are
classiﬁed based on their distance from the station.

Given the growth in the number and applications of battery swap stations, it is crucial to optimally manage the
stations’ operations to reach the highest performance of the station. Thus, we design a decision-making framework to
provide optimal recharging and distribution policies when considering the stochastic demand originating from different
geographical locations. The drone hub can send drones to locations within their ﬂight range, which differ based on the
level of charge inside their batteries. It is a complicated setting, given that the level of charge inside batteries can be any
number between 0 and 1, and different combinations of charge levels can be used to satisfy the stochastic demands
generated from places located at different distances from the drone hub. Hence, using aggregation and discretization,
we classify the demand based on the distance between the hub and demand locations. To the best of our knowledge, we
are the ﬁrst to use such classiﬁcation for this problem and link a level of charge inside batteries with a class of demand
such that the demand of each class can be satisﬁed with batteries having the same or higher level of charge. That is,
each class of demand can be satisﬁed with one or multiple levels of charge inside batteries. We formulate this problem
as a stochastic scheduling and allocation problem with multiple classes of demand (SA-MCD).

We use the Markov decision process (MDP) to model the stochastic SA-MCD. It is an appropriate modeling approach
for problems like ours that are in the class of sequential decision-making under uncertainty problems [24]. The decisions
are made in discrete points in time or decision epochs. We represent the state of the system in the MDP as the number of
batteries within each charge level class. The actions of the MDP are the number of batteries recharging from one level
to an upper level of battery charge. The transition probability is a complex function governed by multiple classes of
stochastic demand. In our MDP, the optimal policy determines the maximum expected total reward, which is a function
of total weighted met demand of different classes.

We use backward induction (BI) and a reinforcement learning method with a descending (cid:15)-greedy exploration feature
(D(cid:15)RL) to solve SA-MCDs. The exploration feature allows the algorithm to take random actions to assist in escaping
short-term or local optima by visiting apparently non-attractive states. The descending exploration means that as the
algorithm proceeds in iterations, the probability of taking an arbitrary action decreases, and the algorithm perform
more exploitation (taking favorable actions) by visiting attractive states. BI can provide exact solutions for problems
like SA-MCDs that have ﬁnite state and action spaces [24]. However, BI runs into the curses of dimensionality and
faces computational time and memory issues as our problem size increases. Thus, we apply an RL method with an
exploration feature that is able to ﬁnd high-quality approximate solutions for large-scale SA-MCDs, which are not
solvable using BI [25]. We show the convergence of our RL method and demonstrate its capacity to save computational
time and memory.

We computationally test the SA-MCD model and solution methods on a case study related to the Zipline drone delivery
company, which delivers blood units, vaccines, and medical supplies in Rwanda. We consider the drone delivery of
these supplies from its station located in the Muhanga district, Rwanda, to the reachable hospitals throughout the
country. We consider the population of districts, number of hospitals in each district, number of people using a hospital,
and rate of arrivals to each hospital to ﬁnd the stochastic orders for medical supplies. Then, we convert the orders
to the demand for drone missions, given that each drone can carry 2kg of medical products at a time [26]. For our
computational experiments, we classify this demand into two classes based on the distance between the station and each
hospital (i.e., closer hospitals are classiﬁed in level 1 demand, and farther hospitals are classiﬁed in level 2 demand).
We import the real data associated with the distance between locations, the population of districts, ﬂight regulations in
Rwanda, and the Zipline drone conﬁguration, including the speed, ﬂight range, and recharging time.

2

We derive insights from solving SA-MCDs to manage the distribution operations of the swap station using different sets
of computational experiments. We provide the optimality gap and average percentage of the met demand using the
D(cid:15)RL method for a modest problem (15-21 drones). Solving the modest size problem shows that the Zipline company
needs more drones to satisfy 100% of the stochastic demand. Hence, we draw the relationship between the number
of drones in the station and the amount of met demand using our RL solution for larger instances of SA-MCD. We
also analyze the interplay between the different demand classes and the use of higher-level charged batteries to satisfy
lower-class demand.

Main Contributions. We summarize the main contribution of this paper as follows.

• We propose stochastic scheduling and allocation problems with multiple classes of demand (SA-MCD) for
managing operations of a drone swap station located at drone a hub. We classify the demand based on the
distance between the station and hospitals generating the stochastic demands.

• We develop an MDP model for the SA-MCD and solve small instances of SA-MCD using backward induction

(BI) and show inability of BI to solve large-scale SA-MCDs required for the real application.

• We propose applying a descending (cid:15)-greedy reinforcement learning method (D(cid:15)RL) to ﬁnd optimal and
near-optimal policies for the station that faces stochastic demand for sending drones to deliver medical supplies.
We show high performance of D(cid:15)RL for solving SA-MCDs.

• We leverage our model and our approximate solution method to be used for a real application, which is the

case study related to Zipline medical supply delivery using drones in Rwanda.

• We conduct different sets of experiments to derive insights for managing the operations in a swap station to
maximize demand satisfaction. We show that demand classiﬁcation approach in modeling improves demand
satisfaction, which is the primary purpose of delivering medical supplies using drones.

The remainder of this paper is organized as follows. In Section 2, we discuss relevant literature related to the modeling,
application, and solution methods for the stochastic SA-MCD. In Section 3, we present our Markov Decision Process to
model the stochastic SA-MCD. In Section 4, we discuss the exact and approximate solution methods. In Section 5, we
outline the computational experiments conducted and provide insights for managing swap station operations. We end
with concluding remarks and propose directions for future work in Section 6.

2 Related work

There is a growing interest in the use of drones for various applications. We provide an overview of scientiﬁc works,
which are more relevant to the model, application, and solution methods presented in this paper. Therefore, we focus
on providing an overview of research related to managing operations in swap stations, delivering medical items using
drones, Markov Decision Process (MDP) modeling for dynamic problems, demand classiﬁcation, and reinforcement
learning (RL) methods.

Many researchers have studied managing swap station operations. Asadi and Nurre Pinkley [27] present an MDP model
to ﬁnd the optimal/near-optimal policies (number of recharging/discharging and replacement actions) to maximize
the expected total proﬁt for the station facing stochastic demands and battery degradation. They solve this problem
using a heuristic, RL methods, and a monotone approximate dynamic programming algorithm [28] to provide insights
for managing the internal operations in the swap stations. Widrick et al. [29] propose an MDP model for the same
problem when no battery degradation is considered. Nurre et al. [30] do not consider stochasticity and provide a
deterministic model to ﬁnd the optimal policies for managing swap stations. Our work is fundamentally different from
the discussed papers as they do not consider different demand classes and multiple states of charge of batteries. Besides,
our objective is satisfying the amount of met demand, which suits the healthcare delivery application, that differs from
the aforementioned papers. Kwizera and Nurre [31] propose a two-level integrated inventory model to manage internal
operations in a drone swap station delivering to multiple customers (or classes, equivalently) but exclude the uncertainty
in the system. To the best of our knowledge, we are the ﬁrst to introduce the stochastic SA-MCDs for managing internal
operations in a swap station facing stochastic demands from different geographical locations.

In recent years, there has been a rapid growth in using drones for many innovative applications [32]. Several papers
[33, 34, 35, 36] review the applications of drones in different contexts, and we refer the reader to Otto et al. [35] for an
extensive review on the optimization approaches for civil applications of drones. Delivering portable medical items
such as blood units and vaccines using drones can positively impact the levels of medical service in remote or congested
places where roads are not a viable option for transportation and delivery [35, 5]. Several companies are using drones to
deliver medical supplies in different parts of the world [9, 10, 11, 12]. Notably, we focus on a case study related to
Zipline, a drone delivery company that started with 15 drones delivering medical items to remote locations in Rwanda

3

in 2016 [37]. After successful operations in Rwanda, Zipline expanded its medical delivery service in the south of
Ghana using 30 drones in 2019 [38]. During the COVID-19 pandemic, drone delivery provides a fast, cheap, and
reliable method to distribute COVID-19 vaccines. For instance, in Ghana, Zipline already delivered 11000 doses and
will deliver more than 2.5 million doses in 2021 [39]. Draganﬂy, a Canada-based company, will use drones to distribute
COVID-19 vaccines to remote areas of Texas starting in Summer 2021 [40].

A drone can store a limited amount of energy that restricts its ﬂight range. This limitation needs to be considered
when modeling real-world problems. Common modeling approaches are to use the maximal operation time [41, 42]
and maximal ﬂying distance [43, 44]. In this paper, we use the maximal coverage of 80km radius (160km round-trip)
[45] from our swap station, which is located at the Muhanga Zipline drone hub, for geographically-based demand
classiﬁcation in our case study.

Our SA-MCD problem is in the class of sequential decision-making under uncertainty, and we use a Markov Decision
Process (MDP) model, which is appropriate for this class of problems [24]. There is extensive research on the use of
MDPs for stochastic problems in the operations research community. A sample of problems and applications that are
close to our research include drone applications [46, 47, 48], dynamic inventory and allocation [49, 50], and optimal
timing of decisions [51, 52, 53, 54].

In SA-MCD, we use demand classiﬁcation that researchers broadly utilize to study scheduling, allocation, supply chain
management, and inventory control problems. We discuss a sample of scientiﬁc works that used such a classiﬁcation
in combination with a MDP modeling approach. Gayon et al. [55] provide optimal production policies for a supplier
facing multiple classes of demands that are different in the demand rates, expected due dates, cancellation probabilities,
and shortage costs. Benjaafar et al. [56] formulate an MDP model to derive optimal production policies for an assembly
system wherein the demands are classiﬁed based on the difference between shortage penalties incurred due to the lack
of inventory to satisfy orders. Thompson et al. [57] categorize patients served by a hospital according to the ﬂoors
treating patients and the lengths of stay in hospitals. Milnar and Chevalier [58] use an inﬁnite horizon MDP to model
an admission control problem to maximize the expected total proﬁt of a ﬁrm serving two classes of customers. The
customers are classiﬁed based on proﬁt margins, order sizes, and lead time. We use a geographically-based demand
classiﬁcation inﬂuenced by different ﬂight ranges of a drone based on the level of charge inside its batteries. An instance
of considering travel range for demand classiﬁcation can be found in [59] wherein it is assumed that different types of
EVs have different drive ranges. They incorporate this information to provide a framework that estimates the charging
demands for charging stations and determine the service capacity of the stations without optimizing the system. As they
do not consider optimization, this work is signiﬁcantly different from our problem that considers optimization under
uncertainty.

When an MDP model is large and complex it suffers from the curses of dimensionality [25, 60], and thus, is traditionally
solved using approximate solution methods. Researchers apply Reinforcement Learning (RL) (or approximate dynamic
programming (ADP), the term more used in the operations research community [25]) to ﬁnd approximate solutions
that are not solvable using standard exact solution methods, (e.g., dynamic programming [24]). Examples of various
ADP/RL methods in dynamic allocation problems are temporal difference learning [61, 62, 63], case-based myopic
RL [64], Q-Learning [65], value function approximation [66, 67, 68], linear function approximations [69], and policy
iteration [70]. In this paper, we apply a value function approximation using a look-up table (e.g., [64, 71]) with an
ε-greedy exploration feature [25, 72] to make our RL method visit and update the value of more (both attractive and
unattractive) states in the state space. We reduce the exploration rate (increase the exploitation rate) to make the
algorithm converge as it proceeds toward iterations. In Section 4.2, we present a comprehensive explanation of our RL
method.

3 Problem description and formulation

In this section, we present our Markov Decision Process (MDP) approach to model the stochastic scheduling and
allocation problem with multiple classes of demand (SA-MCD). We proceed by formally describing the classes of
demand and the components of our MDP model.

For our problem, we consider a set of medical facilities, each with an unknown number of requests (i.e., demand) for
drone delivery over time. We know how long a drone needs to ﬂy from the drone hub (located in the swap station) and
back to satisfy a request for each medical facility. We cluster medical facilities with similar ﬂight times into demand
classes. The demand for each medical facility is then aggregated by demand class. The uncertainty in our MDP is
the number of requests (i.e., demand) for each demand class by time. We assume that there is a known probability
distribution that governs the uncertainty for each demand class over time. We depict an example of geographically-based
demand classiﬁcation in Figure 1.

4

Drone hub

Demand Class 3

Demand Class 2

Demand Class 1

Figure 1: An example of demand classiﬁcation based on the distance between the location of demand and the drone hub.

We link each demand class with the required amount of battery charge that is necessary to make the round-trip ﬂight
from the drone hub to the medical facility and back. In other words, higher demand classes that are farther from the
drone hub require more charge than those closer to the hub. Charging all batteries to full charge ensures that each
drone+battery pair can satisfy a request from any demand class. However, in reality, this strategy results in a higher
total cost, longer recharge times, and faster battery degradation. Thus, we make decisions about how many batteries are
recharged to different charge levels over time. Our system incorporates time-varying elements, including the mean
demand per class over time. We model the system using a ﬁnite horizon MDP. In addition to the relevance of ﬁnite
horizon MDP for highly variable and dynamic systems, like ours, the ﬁnite horizon model is applicable for the station
to check the quality (capacity) of batteries at the end of the time horizon and decide to replace batteries if needed. We
seek to maximize the expected total reward, which equals the summation of the weighted met demand over all demand
classes and time periods. We note that the main purpose of our swap station is demand satisfaction; thus, we do not
directly incorporate different charging costs and times into our model. However, in the objective function, we use
multipliers for the amount of demand of each class that is satisﬁed using the batteries of higher charge levels. In this
design, higher charge level batteries are capable of satisfying demand for lower classes, but this can be penalized as it
caused unnecessary charging costs. Further, because the mission of our drones is delivering medical items, which are
often vital at the moment of orders, we do not consider backlogging or rolling over unmet demands. To maximize the
expected total weighted demand, we seek optimal policies indicating how many batteries are charged to each charge
level by state and time. We proceed by detailing the speciﬁc components of our MDP.

Decision Epochs: Discrete times within the ﬁnite time horizon N < ∞ in which we make decisions. The set of decision
epochs is T = {1, 2, . . . , N − 1}.

t , s2

t , . . . , sC

States: The state of the system, st, is dynamic and deﬁned in the C-dimensional state space S. Thus, st =
(s1
t ) ∈ S = (S1 × S2 × . . . SC), where Si is the state space for battery charge level i for i = 1, . . . , C. Each
t ∈ Si equals the number of
Si = {0, . . . , M } where M represents the total number of batteries. For each dimension, si
batteries with i level charge. The total number of batteries over all charge levels must not exceed M in accordance with
Equation (1). All batteries with a charge level lower than the lowest charge level (i.e., level 1) are implicitly denoted as
being level 0 which does not need to be stored but instead, can be calculated as s0

t = M − (cid:80)C

i=1 si
t.

(cid:26)

S =

(s1

t , s2

t , . . . sC

t ) :

(cid:32) C
(cid:88)

i=1

si
t ≤ M,

(cid:33) (cid:27)

∀t ∈ T

.

(1)

The C-dimensional state space corresponds to the C demand classes. As previously mentioned, each demand class
represents a set of medical facilities with similar round-trip delivery ﬂight times. We link the state space with these
demand classes by stating that a drone powered with charge level i is able to satisfy requests from demand class i and
lower. In other words, a request from demand class i is able to be satisﬁed by a drone powered with charge level i or
higher. We make the assumption that a demand request from class i is satisﬁed using a battery from the lowest, capable,
available charge level. For example, imagine we have a request from demand class 2. Any battery with charge level
2, . . . , C is capable of satisfying this request. If a battery is available with charge level 2, this battery is used. However,
if no batteries are available with charge level 2, then we look to assign a battery with charge level 3, and continue

5

increasing the charge level until a battery is available. If none are available, we designate this demand as unmet. With
this assumption, we maintain higher charge level battery in inventory.
Actions: We use at to denote the recharging action at time t using a vector of size (C(C − 1)/2) such that aik
t
the number of batteries starting at charge level i which are recharged to level k for k > i at time t. As follows,

represents

where

at = {aik

t ∈ Aik
st

: ∀i = 0, 1, . . . , C − 1, k = i + 1, . . . , C}

Aik
st

= {0, 1, . . . , si
t}

∀i = 1, . . . , C − 1, k = i + 1, . . . , C, and

A0k
st

= {0, 1, . . . , M −

C
(cid:88)

i=1

si
t} ∀k = 1, . . . , C.

(2)

(3)

(4)

To ensure that the number of batteries selected to be recharged from each charge level does not exceed the number of
batteries within that class, we force the actions to satisfy Equations (5) and (6).

C
(cid:88)

k=1

a0k
t ≤ M −

C
(cid:88)

i=1

si
t.

C
(cid:88)

k=i+1

t ≤ si
aik
t

∀i = 1, . . . , C − 1, ∀t ∈ T.

(5)

(6)

In Figure 2, we display the state transitions between different states due to different recharging actions or demand
satisfaction for a single battery. Recharging/demand satisfaction increases/decreases the level of charge of a battery
depending on the level of recharging/classes of met demand.

C lvl. charg. from 0 to C

C − 1 lvl. charg. from 1 to C

2 lvl. charg. from 0 to 2

C − 2 lvl. charg. from 2 to C

1 lvl. charg. from 0 to 1

1 lvl. charg. from 1 to 2

Not used

0

Not used

1

Not used

2

· · ·

Not used

C

Satisfy Dem. from cls. 1

Satisfy Dem. from cls. 1

Satisfy Dem. from cls. 2

Satisfy Dem. from cls. C − 2

Satisfy Dem. from cls. C − 1

Satisfy Dem. from cls. C

Figure 2: An instance of state transition for a single battery.

Transition Probabilities: The system transitions from state st to a future state st+1 according to the selected action and
the realized demand within each demand class. In our system, the demand at time t, denoted Dt, is a vector of size C,
i.e., Dt = (D1
t, for i = 1, . . . , C, is a random variable representing the number of requests
for demand class i. As we explain later, the state transitions and the probability transition function are complex, but we
illustrate these functions of our MDP where C = 2. However, we note that the model could be applied to a problem
with C > 2.

t ) where each Di

t , . . . , DC

As our state transition is complex, we ﬁrst deﬁne an intermediate state of the system. We deﬁne the intermediate state
of the system as L = (L1, L2) and allow this to represent the number of batteries currently charged at levels 1 and 2
after all actions are taken and batteries are allocated to satisfy demand within their class (i.e., batteries with charge

6

levels 1 and 2 are used to satisfy the demand of class 1 and class 2, respectively). Therefore, the intermediate states
should not be mistaken for the post-decision states that show the system’s state immediately after making decisions
before realizing the uncertainty. We note, this intermediate transition does not incorporate the batteries from charge
level 2 used to satisfy remaining demand from class 1. The transitions to intermediate states are governed by Equations
(7) and (8). In Equation (7), min{s1
t } equals the satisﬁed demand of class 1 using the available batteries with
charge level 1. Similarly, min{s2
t } denotes the satisﬁed demands of class 2 using batteries with charge level 2 in
Equation (8).

t − a12

t , D1

t , D2

L1 = s1

t + a01

t − a12

t − min{s1

L2 = s2

t + a02

t + a12

t − min{s2

t }.

t − a12
t , D2

t , D1
t }.

(7)

(8)

Using this intermediate state, we now present the entire state transition equations. Given L = (L1, L2), we can now
use the remaining batteries with level 2 charge to satisfy any remaining demand for class 1. We present the full future
state of the system with Equations (9) and (10).

t+1 = L1 + min(cid:8)max{0, D1
s1
t+1 = L2 − min(cid:8)max{0, D1
s2

t − (s1
t − (s1

t − a12
t − a12

t )}, max{0, s2
t )}, max{0, s2

t − D2
t − D2

t )}(cid:9).
t )}(cid:9).

(9)

(10)

To illustrate the state transitions, we use Figure 3 to display the timing of events between two consecutive decision
t , a12
epochs. At epoch t, we observe the state of system (s1
t ).
Then, we realize the stochastic demand at epoch t. It means the information for the number of demands of each class
becomes available after we make the decisions for charging actions. Between two decision epochs, we ﬁrst allocate
drones with level 1 and 2 charges to satisfy the demand class 1 and 2, respectively. Then, we can determine the
intermediate state of the system and allocate the leftover batteries of level 2 charge to meet the unmet demand of class 1.
Ultimately, at epoch t + 1, we update the state of system using Equations (7), (8), (9), and (10).

t ) and make a decision for charging actions (a01

t , a02

t , s2

epoch t

epoch t + 1

Observed
state at
epoch t

Decisions for
recharging
t , a12
t , a02
t )

(a01

Updated the
Intermediate
state (L1, L2)

Updated state using
info. of charged batteries
and satisﬁed demand

Observed state
at epoch t + 1

# charged
at level 1 and 2
t , s2
t )

(s1

D1

Allocate drones to meet
t and D2
t
using available
batteries with
the same level of charge

Allocate drones to meet
unmet demand of D1
t
using available
batteries with the
higher level of charge

# charged
at level 1 and 2
t+1, s2
(s1
t+1)

Figure 3: Diagram outlining the timing of events for the SA-MCD model.

In Equations (9) and (10), U 1 = max{0, D1
t )} is the amount of unsatisﬁed class 1 demand after using level
1 charged batteries and U 2 = max{0, s2
t )} is the number of leftover level 2 charged batteries after satisfying
class 2 demands. Hence, the amount of remaining class 1 demand that can be satisﬁed using remaining level 2 charged
batteries is the minimum of (U 1, U 2). We note that our system holds the Markov property that means the system’s
future state does not depend on the state of the system in the past and can be derived using solely the present state, taken
action, and realized uncertainty [24].

t − D2

t − a12

t − (s1

t , s2

We provide an example to clarify the state transitions. Suppose we have 10 batteries and the system’s state is
(s1
t ) = (3, 6), which means 3 and 6 batteries have level 1 and 2 charges, respectively, and one battery is depleted.
Lets assume, we take the action (a01
t ) = (5, 2). Incorporating
this information, the number of met demand of class 1 and 2 using batteries with the same level of charge equals
min{s1
t } = min{6, 2} = 2, respectively. The amount of unsatisﬁed

t ) = (0, 1, 2) and the realized demand (D1

t } = min{3 − 2, 5} = 1 and min{s2

t − a12

t , a02

t , a12

t , D1

t , D2

t , D2

7

class 1 demand is U 1 = max{0, D1
t )} = 4. The number of leftover level 2 charged batteries that can be
used to satisfy unmet demand of class 1 is U 2 = max{0, s2
t } = max{0, 6 − 2} = 4. Using Equations (7) and (8),
the intermediate state of the system is (L1, L2) = (0, 7). Then, we use 4 out of 4 level 2 charged batteries to satisfy
the demand of class 1 as min(U 1, U 2) = 4. These 4 batteries will return to the station with level 1 charge. Hence, the
future state s1

t+1 = L1 + 4 = 4 and s2

t+1 = L2 − 4 = 3.

t − a12

t − (s1

t − D2

u=x pi

x = P (Di

u = P (Di

x = (cid:80)∞

t = x) and qi

Now, we present the transition probability function from state st to state st+1 = j = (j1, j2) using Equation (11).In
this equation, pi
t ≥ x) ∀i = 1, 2. In all of the cases in Equation (11), the
intermediate state transitions are calculated using Equations (7) and (8). The ﬁrst case in Equation (11) calculates the
transition probabilities when the stochastic demand of class 1 and 2 is less than the number of charged level 1 and 2
batteries, respectively. The future state equates the intermediate state for each charge level. In the second case, the
demand of level 2 is greater than or equal to the number of batteries with level 2 charge; hence the number of batteries
with level 2 charge at time t + 1 equals the number of recently charged batteries from empty or level 1 charge. The
future state of the system equates to the intermediate state of the system. The third case is similar to the second case
except that in the second case, the stochastic demand of class 1 is less than the number of level 1 charged batteries but in
the third case, the demand is greater than or equal to the number of level 1 charged batteries. The fourth case describes
the condition that all of the demand for the class 1 charge can be satisﬁed using all the available level 1 charged batteries
plus the leftover batteries of level 2 after satisfying the demand of class 2. The future state of level 2 batteries will be no
more than the intermediate state for level 2. The amount of satisﬁed demand in stage 2 (satisfying demand of class 1
using remaining batteries of class 2) equals the difference between the intermediate and future state of level 2 charged
batteries. The ﬁfth case is similar to the fourth case, except that all of the demands of class 1 can not be satisﬁed in the
ﬁrst and second stage of the demand satisfaction process.

p(j|st, at) =






(p1
t +a01
s1

t −a12

t −j1)(p2

t +a02
s2

t +a12

t −j2)

t < L1 ≤ s1

if a01
t < L2 ≤ s2
t + a12
a01
j2 = L2, and j1 = L1

t + a01

t − a12
t ,
t + a12
t ,

t + a02

(p1
t +a01
s1

t −a12

t −j1)(q2

s2
t

)

if a01
t + a12
a01

t < L1 ≤ s1

t + a01

t − a12
t ,

t = L2, j2 = L2, and j1 = L1

(q1
t −a12
s1
t

)(q2
s2
t

)

t + a12
t = L1, a01
if a01
j2 = L2, and j1 = L1

t = L2,

(p1
t −a12
s1

t +L2−j2)(p2

t +a02
s2

t +a12

t −L2)

(q1
t −a12
s1

t +L2−j2)(p2

t +a02
s2

t +a12

t −L2)

if a01
t + a12
a02

t = L1, a01

t + a12

t < L2 ≤ s2
t < j2 ≤ L2, and j1 = a01

t + a02
t + L2

t + a12
t ,
t − j2

if a01
t + a12
a02

t = L1, a01

t + a12
t = j2, and j1 = a01

t < L2 ≤ s2
t + L2

t + a02
t − j2

t + a12
t ,

0

otherwise.

(11)

We note that if the number of classes increases to C = 3, then there are 13 different cases to be considered. Further,
we need to consider two intermediate states to capture demand satisfaction with the same level of charge and one and
two-level of charge higher. For the application of our problem, two demand classes are adequate to observe a signiﬁcant
improvement in the quality of solutions (see Section 5) without making the model excessively complicated. If the
application requires having more than three classes, we suggest using alternative strategies such as non-Markovian
modeling and/or only applying ADP/RL/Q-learning methods, which does not require an explicit transition probability
function.

Despite the apparent complexity of our transition probability function, we would like to point out the beneﬁt of our
MDP model. Incorporating the assumptions for the order of demand satisfaction and timing of events in our model
enables us to capture the system’s dynamic without including the past state(s) information. As a result, our model holds
the Markov property, allowing us to beneﬁt from the MDP properties and guaranteed solution methods.

Reward: We calculate the immediate reward of taking action at when the transition from state st to state st+1 = j
occurs using Equation (12)

rt(st, at, j) = ρ11(s1

t + a01

t − a12

t − L1) + ρ21(L2 − j2) + ρ22(s2

t + a02

t + a12

t − L2)

(12)

8

t + a01

t + a02

t − a12

t + a12

for t = 1, . . . , N − 1. We use ρij to put weights on the amount of met demand of class j using level i charged batteries.
In the ﬁrst term, (s1
t − L1) denotes the number of level 1 charged drones used to satisfy class 1 demand. In
the second term, (L2 − j2) equals to the number of level 2 charged batteries used to satisfy class 1 demand. In the
third term, (s2
t − L2) determines the number of level 2 charged batteries used to satisfy class 2 demand.
We note that our objective is to maximize the expected total satisﬁed demand which does not directly incorporate
cost. However, with adjusting the weights of ρij, we implicitly include a cost factor via assigning a penalty/reward to
demand satisfaction with an excessive level of charge. For instance, when ρ21 = ρ22 = ρ11 = 1, there is no beneﬁt in
recharging batteries to level 1 to satisfy class 1 demand because level 2 charged batteries can satisfy class 1 demand
by generating the same reward while these batteries can satisfy class 2 demands, too. However, if ρ21 = 0.5, then we
expect to recharge to/use more level 1 charged batteries to satisfy class 1 demand given that ρ11 = 2ρ21, which means
we penalize the reward of satisfying class 1 demand with level 2 charged batteries. We vary this parameter and analyze
the results in Section 5. In practice, the drone delivery companies can select the value of this parameter based on the
insights or their preferences.

At the end of the time horizon we calculate the terminal reward. We assume that no action is taken at the end of the
time horizon and that all remaining batteries can be used to satisfy future demand, and there is sufﬁcient demand for
each level. Thus, we deﬁne the terminal reward using Equation (13).

rN (sN ) = ρ11s1

N + ρ22s2
N

(13)

We calculate the immediate expected reward rt(st, at), using the immediate reward and transition probability functions
given by Equation (14),

rt(st, at) =

(cid:88)

j,L∈S

(cid:20)
pt(j|st, at)(cid:0)ρ11(s1

t + a01

t − a12

t − L1) + ρ21(L2 − j2)+

ρ22(s2

t + a02

t + a12

t − L2)(cid:1)

(cid:21)
.

(14)

We derive the decision rules, dt(st) : st → Ast , from the action set to maximize the total expected reward. Because we
select a single action based on the present state, which does not depend on the past states and actions, our decision rules
belong to the Markovian decision rules [24]. A policy π is a sequence of decision rules for all decision epochs, that is
dπ
t (st) ∀ t ∈ T . We can calculate the expected total reward of policy π for the problems starting from an arbitrary
initial state s1 using Equation (15). The optimal policy, π∗, maximizes the expected total reward.

N (s1) = Eπ
V π
s1

rt(st, at) + rN (sN )

(cid:21)
.

(cid:20) N −1
(cid:88)

t=1

(15)

4 Solution methodology

In this section, we ﬁrst present the exact solution method, backward induction (BI), and continue with our approximate
solution method, the reinforcement learning method with a descending (cid:15)-greedy exploration feature (D(cid:15)RL) to solve the
stochastic SA-MCDs.

4.1 Backward induction

As our Markov Decision Process (MDP) model has ﬁnite state and action spaces, there is at least one deterministic
optimal policy [24]; thus, backward induction (BI) can determine such policy (number of recharging actions) that
maximizes the expected total reward or weighted met demand over time. Let V ∗
t (st) be the optimal value function
equivalent to the maximum expected total reward from decision epoch t onward when the system is in state st. Then,
we can use the optimality (Bellman) equations, given by Equation (16), to ﬁnd the optimal policies for all the decision
epochs when moving backward in time. That is, BI sets the value of being in state sN at the end of the time horizon
N to be equal to the terminal reward value given by Equation (13). Then, the algorithm starts from the last decision
epoch and ﬁnds the optimal actions (a∗
st,t) and corresponding values (V ∗
t (st)) using Equations (16) and (17) stepping
backward in time. The algorithm aims to ﬁnd the optimal expected total reward over the time horizon, V ∗
1 (s1), for
state s1, which is the system’s initial state at time t = 1. In other words, solving the optimality equations for t = 1 is
equivalent to the expected total reward over the time horizon.

V ∗
t (st) = max
at∈Ast






rt(st, at) +

(cid:88)

j∈S

pt(j | st, at)Vt+1(j)






.

(16)

9

a∗
st,t = arg maxat∈Ast






rt(st, at) +

(cid:88)

j∈S

pt(j|st, at)Vt+1(j)






.

(17)

The size of state space, action space, transition probability, and optimal policies (the best actions for all the states over
time) are functions of O(M 2) and O(M 3), O(M 7N ), O(M 2N ), respectively. As the size of the problem increases, it
becomes challenging for BI to ﬁnd the optimal solution due to the curses of dimensionality, which causes a drastic
increase in computational time and memory. Hence, we proceed with presenting our reinforcement learning (RL)
method, which is capable of circumventing such problems [25, 60].

4.2 Reinforcement learning

In this section, we explain the reinforcement learning method to ﬁnd near-optimal policies and to overcome the curses
of dimensionality [25] of the stochastic scheduling and allocation problem with multiple classes of demand (SA-MCD).
We use a value iteration approach, reinforcement learning with a descending (cid:15)-greedy exploration feature (D(cid:15)RL),
which provides high-quality approximate solutions for SA-MCD ( see Section 5). We proceed by introducing the
notation and continue with our RL features and procedure.

Notation Description
τ1
The number of core RL iterations
τ2
The number of sample paths of demands (realized uncertainty)
V n
t (st)
The optimal value of being in state st at time t for iteration n
n
t (st)
V
The approximate value of being in state st at time t for iteration n
ˆυn
t (st)
The observed value of state st at time t for iteration n
αn
The step-size value at iteration n
εn
The exploration rate at iteration n
u
A random number that is used to select exploration or exploitation
t (sn
zn
t )
The smoothed value of being in state st at time t for iteration n
Table 1: Notation used in the reinforcement learning algorithm.

We ﬁrst determine the number of drones (M ), decision epochs (N − 1), (τ1) iterations, and (τ2) sample paths in our RL
method. Then, we initialize the approximate value at the end of the time horizon, t = N , for all iterations using the
terminal reward function given by Equation (13). For every iteration, we select an initial state sn
1 . To select the action,
we use the ε-greedy method [25] that allows exploring the action space works as follows. We generate a random number,
u. Then, we compare u with the exploration rate, εn, at iteration n. We use a descending function for the exploration
rate over the iterations to ensure more states (both attractive and unattractive) are visited and facilitate the algorithm
convergence. If Rand < εn, we select a feasible action arbitrarily. Otherwise, we generate τ2 sample paths of demands
(realized uncertainty) and select the action that maximizes the observed value ˆυn
t (st) according to Equations (18) and
n−1
t+1 (st+1) is used to approximate the value of E(Vt+1(st+1) | st, at) for each sample path. If an action,
(19), wherein V
n−1
is selected over multiple sample paths, we use the average of V
t+1 (st+1) as the approximation. The observed value
and the approximated value at the previous iteration are smoothed using a stepsize function. This value is now used
as the present approximation value of the observed state. When an action is selected, we sample an observation of
uncertainty (generate a realized value for stochastic demand) to ﬁnd the future state. The algorithm steps forward in
time and moves to the future observed state until it reaches the last decision epoch and new iteration starts. The same
process is repeated until τ1 iterations are completed.

an
st,t = arg max
at∈Ast

(cid:110)

rt(st, at) + V

n−1
t+1 (st+1)

(cid:111)

.

t (sn
ˆυn

t ) = max
at∈Ast

{rt(st, at) + E(Vt+1(st+1) | st, at)} .

(18)

(19)

5 Computational results

In this section, we explain the results of solving the stochastic scheduling and allocation problems with multiple classes
of demand (SA-MCD) using realistic data related to the drone delivery company Zipline. We created this dataset to

10

Algorithm 1 Reinforcement Learning Method with a Descending (cid:15)-greedy Exploration Feature (D(cid:15)RL)

Select initial state sn
1
for t = 1, . . . , N − 1 do

Generate a random number u
if Rand < εn then

n
N (s) = rN (s) for s ∈ S and n = 1, . . . , τ1

1: Initialize M drones, N − 1 decision epochs, τ1 iterations, and τ2 sample paths
2: Set V
3: Set n = 1
4: while n ≤ τ1 do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

Generate τ2 sample paths of the uncertainty
Select an action that maximizes ˆυn
Sample an observation of the uncertainty, Dt
Calculate the observed value, ˆυn

Sample an observation of the uncertainty, Dt
Determine a random feasible action, at
n
Calculate the observed value, ˆυn

end if
Smooth the new observation with the previous approximated value,

t (st) over τ2 sample paths

t (sn
t )

t (sn
t )

else

t (sn
zn

t ) = (1 − αn)V

n−1
t

(sn

t ) + αn ˆυn

t (sn
t )

(20)

Update the present approximation using the smoothed value, V
Determine next state, sn

t+1

19:
20:
21:
22:
23: end while

end for
Increment n = n + 1

n
t (sn

t ) ← zn

t (sn
t )

mimic the geographical locations of the Zipline drone hub and hospitals in Rwanda, Africa, the population of districts,
ﬂight regulations in the country, Zipline drone conﬁguration, including the speed, ﬂight range, and recharging time. We
solve modest SA-MCDs (15-21 drones) using exact solution methods. We note that we use the number of batteries and
the number of drones interchangeably as each drone has a battery pack, which might be consist of multiple batteries that
can be charged in parallel. As we run into the curses of dimensionality for larger instances of SA-MCD, we present the
results of our reinforcement learning (RL) method that can provide near-optimal solutions for the modest instances and
solve larger problem instances. We deduce managerial insights for managing the swap station’s distribution operations
that maximize the expected total weighted met demand of multiple classes. We proceed by ﬁrst explaining the data.

5.1 Data

In this paper, we present an actual case study related to the vital operations of Zipline drones, which are delivering
medical items in Rwanda, Africa. The Zipline station is located at the Muhanga district, west of Rwanda’s capital
city, Kigali. We focus on drone delivery to satisfy the stochastic demand for blood units originating from hospitals in
Rwanda. In Table 2, we summarize the input data, including, the name of each hospital, their location, the distance
between the station and each hospital, the approximated population of people using each hospital, the number of blood
units and ﬂights needed per day for each hospital, and the demand class associated with each hospital. We categorize
the demand into two classes based on the distance between the Zipline station and hospitals. As the ﬂight range of the
drone is estimated to be 80km [73], we assume demands of hospitals located within [0km, 40km) and [40km, 80km]
fall into class 1 and class 2, respectively. The demand class NA means that the hospital is not reachable from the Zipline
station and excluded from further analysis. We exclude six hospitals (denoted by NA in the last column of Table 2) from
the 33 identiﬁed hospitals as they are located at a distance out of the drone’s ﬂight range from the origin, Zipline station.
We consider ten hospitals in class 1 and 17 hospitals in class 2 which are located throughout 15 distinct districts in
Rwanda.

We approximate the air travel distance between the station and hospitals using the Haversine formula [74] that is broadly
used to ﬁnd the distance between two points on the earth. When calculating the distance, we consider the rules for
ﬂying drones in Rwanda, which does not allow drones to ﬂy within a 10km radius from airports [75]. Therefore, we
need to adjust the travel distance between the station and two hospitals, Kiziguro and Rwamagana. Hence, we calculate
the closest travel distances such that the ﬂights to these destinations do not violate the rules for ﬂying drones in Rwanda.
In Figure 4, we display the geographical locations of airports, hospitals, and the Zipline station.

11

District

Hospital
name
Bugesera
Nyamata
Burera
Butaro
Gakenke
Nemba
Gakenke
Ruli
Gatsibo
Kiziguro
Gatsibo
Ngarama
Gicumbi
Byumba
Gisagara
Gakoma
Remera Rukoma Kamonyi
Karongi
Kibuye Referral
Karongi
Kirinda
Karongi
Mugonero
Kayonza
Gahini
Kayonza
Rwinkwavu
Kirehe
Kirehe
Muhanga
Kabgayi
Ngoma
Kibungo
Ngororero
Muhororo
Nyabihu
Shyira
Nyagatare
Nyagatare
Nyamagabe
Kaduha
Nyamagabe
Kigeme
Nyamasheke
Kibogora
Nyanza
Nyanza
Nyaruguru
Munini
Rubavu
Kabaya
Ruhango
Gitwe
Ruhango
Ruhango
Rulindo
Kinihira
Rulindo
Rutongo
Rusizi
Mibilizi
Rutsiro
Murunda
Rwamagana
Rwamagana

Dist. to Zipline
station (km)
34.3
73.5
47.8
27.6
75.8
77.5
61.3
34.9
20.3
48.2
22.6
55.6
85.4
94.0
99.6
5.2
85.1
22.6
46.0
104.9
40.8
53.8
77.5
31.9
76.8
44.7
22.4
19.8
50.7
41.9
107.3
48.3
73.7

Pop. reach
the hospital
361914
336582
169117
169117
216510
216510
395606
161253
340501
110603
110603
110603
172079
172079
340368
319141
336928
333713
294740
465855
170746
170746
190902
323719
294334
201831
159943
159943
143841
143841
200429
324654
313461

Pop. need
blood unit/year
7238.3
6731.6
3382.3
3382.3
4330.2
4330.2
7912.1
3225.1
6810.0
2212.1
2212.1
2212.1
3441.6
3441.6
6807.4
6382.8
6738.6
6674.3
5894.8
9317.1
3414.9
3414.9
3818.0
6474.4
5886.7
4036.6
3198.9
3198.9
2876.8
2876.8
4008.6
6493.1
6269.2

Pop. need Rounded # of ﬂights Class of
demand
needed/day
1
10
2
10
2
5
1
5
2
6
2
6
2
11
1
5
1
10
2
4
1
4
2
4
NA
5
NA
5
NA
10
1
9
NA
10
1
10
2
9
NA
13
2
5
2
5
2
6
1
9
2
9
2
6
1
5
1
5
2
4
2
4
NA
6
2
9
2
9

blood unit /day
19.8
18.4
9.3
9.3
11.9
11.9
21.7
8.8
18.7
6.1
6.1
6.1
9.4
9.4
18.7
17.5
18.5
18.3
16.2
25.5
9.4
9.4
10.5
17.7
16.1
11.1
8.8
8.8
7.9
7.9
11.0
17.8
17.2

Table 2: The data associated with blood unit delivery using Zipline drones in Rwanda, Africa.

Figure 4: Locations of hospitals (demand nodes), the swap station located in the Zipline drone hub, and airports in
Rwanda.

Consistent with [76, 77], we use a non-homogenous Poisson process to determine the patients’ arrival to hospitals. We
examine the daily operations of the drone swap station wherein the time between two consecutive decision epoch is
90 minutes (1.5hr). Thus, N = 24/1.5 + 1 = 17. The 90-minute intervals provide adequate time for drones to receive
charge [78] and complete a round-trip from the furthest delivery mission to the station given that the maximum speed of
the drone is 127km/hr [79].

To derive the mean demand of blood units per time t, we apply the following process. First, we determine the number
of people using a particular hospital based on the population of each district. If more than one hospital is located in a

12

−2.5−2.0−1.529.029.530.030.5LongitudeLatitudeAirportsHospitalsStationLocations of Hospitals, Airports, and Zipline Station in Rwanda, Africadistrict, we evenly distribute the district’s total population over the number of hospitals located in that district. Second,
we calculate the number of blood units needed per year for each hospital by multiplying an estimated portion of the
population that needs blood units per year (2% recommended by the World Health Organization (WHO) [80]) by the
number of people using that hospital. The yielded number is an overestimate of the number reported by [81]. However,
we use this number to account for pessimistic cases wherein the station faces more demand. Third, we divide the
number of blood units required per year by 365 to ﬁnd the number of blood units needed per day for each hospital.
Next, we use the pattern of patient arrivals to hospitals, consistent with [82, 83, 84], to derive the mean demand for
blood units of time t over a day. The pattern in the literature indicates an ascending trend of arrivals from 6:00am to the
peak at noon, followed by a descending trend from noon to 6:00am of the following day. Speciﬁcally, we use the data
from [82] and ﬁt a polynomial function for generating the mean arrival rate of time t. In Figure 5, we display the mean
demand of [82] and our ﬁtted function. We scale the mean demand of blood units of time t such that the summation of
the scaled demand over a day equals the calculated number of blood units required per day for each hospital. Then, as
each drone can carry two units of blood [26], we divide the mean demand of blood units of time t by two to ﬁnd the
mean demand for ﬂights for each hospital. Finally, the mean demand for either demand class, λ1
t , is the aggregation
of mean demands for ﬂights from the hospitals within each demand class.

t , λ2

Figure 5: Pattern of patients arrivals to a hospital over a day.

In the ﬁrst experiment, we consider Zipline has a ﬂeet of 15 drones [37]. We set ρ11 = ρ22 = 1 and ρ21 = 0.5 indicating
that satisfying a demand of class 1 using a level 1 charged battery (partially-charged) generates more immediate reward
than satisfying that demand using a level 2 charged battery (fully-charged). This setting implies the company provides
less reward when drones with excessive level of charge are used to satisfy demands, which can be interpreted as a
penalty to account for unnecessary higher recharging costs incurred.

The setting of our D(cid:15)RL parameters is as follows. The number of core iterations is τ1 = 200000. As we will see later,
the algorithm will converge after 50000 iterations; however, as the computational time is in a matter of minutes, we
keep 200000 iterations for our experiments. We test τ2 = 1, 5, 10, . . . , 50 and observe that increasing the parameter
to the value of 30 reduces the optimality gap and increases the robustness of the results and computational time, but
excessive increase in the value of τ2 only magniﬁes the computational time with little improvement in the quality of
the result; thus, we set τ2 = 30. We use the adaptive stepsize function provided by George and Powell [85]. We use
εn = 1/n to adjust the value of the exploration rate at iteration n used in the ε-greedy approach to select policies within
our RL method. With this function, we ensure a higher rate of exploration/exploitation in early/late iterations, which is
desirable for visiting more states and enabling the algorithm to converge as it proceeds with each iteration.

5.2 Discussion and analysis

In this section, we ﬁrst feed the data explained in Section 5.1 to solve the problem using exact and approximate solution
methods, Backward Induction (BI) and the reinforcement learning method with a descending (cid:15)-greedy exploration
feature (D(cid:15)RL), respectively. Then, we analyze the optimal policies (BI solutions) and assess the quality of near-optimal
solutions derived from D(cid:15)RL. Moreover, as the drone delivery company can control and adjust ρ21 (the weight of
satisfying class 1 demand using level 2 charged batteries), we analyze the impact of changing the parameter’s value on
the station’s operations and amount of met demand. We also conduct different sets of experiments to solve instances of

13

the problem and answer the following questions. How many batteries are needed in the station to satisfy a certain level
of the stochastic demand? What is the contribution of classifying the demand on the demand satisfaction? We use a
high-performance computer with four shared memory quad Xeon octa-core 2.4 GHz E5-4640 processors and 768GB of
memory for running all of our computational tests.

5.2.1 Comparing results of BI and RL

In this section, we present the results from solving stochastic SA-MCD with BI and D(cid:15)RL and using the data presented in
Section 5.1. The system’s initial state is s1 = (0, 15), which means all 15 batteries are charged to level 2 (fully-charged).
The time horizon is one day and N = 17 wherein the ﬁrst decision epoch is at midnight and the time between any
two consecutive decision epochs is 90 minutes. That is, the decisions are made at 16 decision epochs, t, where t =
00:00, 1:30, 3:00, . . . , 10:30, 12:00, 13:30, . . . , 22:30. The results of D(cid:15)RL in the last iteration can differ in terms of the
converged value and met demand. Hence, we generate independent sample paths to report robust results and evaluate
our RL method’s performance. Our preliminary experiments reveal that our result is robust when >100 sample paths of
demand are incorporated for evaluation (the percentage change in the converged values is satisfactory and less than 0.1).
Hence, to yield even a higher robustness level, we calculate the average percentage of met demand using Equations (21)
and (22) wherein we generate 500 sample paths of realized demand using the Poisson distribution at time t for each
class of demand.

(%) of Demand Met over Time for a Sample Path =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Tot. # Met Dem. over Time - Tot. # Realized Dem. over Time
Tot. # Realized Dem. over Time

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∗ 100%.

(21)

Average (%) of Demand Met =

# of Sample Paths
(cid:80)
i=1

(%) of Met Demand over Time for Sample Path i

# of Sample Paths

.

(22)

We summarize the results in Table 3. The percentage of met demand over a sample path equals the total number of
demand met over the total realized demand of both classes. We report the average of 500 sample paths in Table 3. We
provide more detailed results about demand satisfaction by class later in Table 5. As shown, D(cid:15)RL is faster and can
generate a high-quality solution with 5.3% of optimality gap (derived from Equation (23)) in 8 minutes. In Figure 6, we
show the convergence of our proposed D(cid:15)RL method.

Solution
Method
BI
D(cid:15)RL
Benchmark

Expected

Average Met Computational

Total Reward Demand (%)

115.1
109.0
105.6

63.7
60.9
58.5

Time (s)
6740.7
483.7
6442.1

Table 3: The expected total reward and computational time of solving SA-MCD with ρ21 = 0.5 and M = 15 using BI
and RL.

Optimality Gap =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Exp. Tot. Reward BI - Exp.Tot. Reward D(cid:15)RL Method
Exp. Tot. Reward BI

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∗ 100%.

(23)

In Table 3, we also provide the results of an easy-to-implement intuitive benchmark. As the model’s objective is to
maximize the amount of met demand, our proposed benchmark policy makes all the empty batteries fully-charged (usable
for demand satisfaction of all classes) at every decision epoch. We note the benchmark can be quickly implemented,
but it needs a considerable computational time to calculate the exact expected total reward. The computational time
for evaluating the benchmark includes all of the operations of backward induction, except the for loop for ﬁnding the
best actions as the actions are derived from the (ﬁxed) benchmark policy. Our results show that D(cid:15)RL outperforms the
benchmark policy regarding the expected total reward, average demand met, and computational time. Thus, we only
continue with D(cid:15)RL as the superior approximate solution method.

In Table 4, we provide the summary of the results from solving the problem with 15 to 21 drones using BI and D(cid:15)RL.
We note, BI cannot ﬁnd the optimal solution when the number of drones is greater than 21. For 15-21 drones, D(cid:15)RL

14

Figure 6: Expected total reward convergence of D(cid:15)RL.

provides an optimality gap of less than 6% for all instances and signiﬁcantly reduces computational time. The maximum
difference between the average percentage of met demand of D(cid:15)RL and BI is less than 5%. The results indicate the
high performance of our D(cid:15)RL method in providing approximate solutions.

Backward Induction (BI)

Reinforcement Learning (D(cid:15)RL)

M

15
16
17
18
19
20
21

Average Met Computational
Demand (%)
63.7
66.9
69.9
72.6
75.4
77.9
80.2

Memory
Time (s) Used (GB)
34.2
68.1
125.8
192.2
287.5
411.9
621.1

6740.7
11630.4
19356.4
31405.8
49812.5
77170.6
117154.0

Average Met Computational Memory
Used (GB)
Demand (%)
13.4
60.9
13.4
64.3
13.4
65.2
13.4
70.6
13.4
72.6
13.4
73.6
13.4
78.3

Time (s)
483.7
494.1
517.0
534.5
575.7
598.1
600.1

Optimality
Gap (%)
5.3
3.3
5.0
3.4
3.5
4.8
2.7

Table 4: Computational time, memory used, and average percentage of met demand over time for 500 sample paths
when ρ21 = 0.5 using BI and D(cid:15)RL methods.

We can ﬁnd the visited states and policies over time (sample paths of visited states and policies) using the sample paths
of realized demands. Given the initial state, taken actions, realized demand, and the state transition functions given by
Equations (9) and (10), we can ﬁnd the future visited state. The consecutive visited states form the sample path of states.
The sample paths of policies are the consecutive selected actions derived from the BI and D(cid:15)RL solution methods.

In Figure 7, the ﬁrst, second, and third row depict sample paths of states, optimal policy, and met demands when the
stochastic demand equals mean demand at time t and ρ21 = 0.5, 1, and 2, respectively, for M = 15. Intuitively, when
ρ21 = 0.5, the level 1 charged batteries are used to satisfy class 1 demand and a12
t = 0. When ρ21 increases more
batteries of class 1 are recharged to class 2 to be used for satisfying the demand of class 1. When ρ21 = 1, no batteries
are recharged to level 1 because there is no difference between the value of satisfying the class 1 demand using level 1
and level 2 charged batteries. Hence, the optimal policy includes recharging batteries to level 2 to be used for either
classes of demand. For all values of ρ, we observe more recharging actions when demands are in peak periods.
We also compare the average amount of met demand of either class and the optimal policies over time when ρ21
(controlled parameter by the station) varies between 0.5 to 2 with 0.1 increments. We calculate the percentage of
demand satisfaction of either class by inputting the associated value of each class into Equations (21) and (22) for
different values of ρ21. We use Equation (24) to ﬁnd the average number of actions over time over all sample paths. We
summarized the result based on 500 sample paths of realized demand in Table 5. We note that we display the number of
actions in the last three columns of Table 5, but we use percentage for the other columns related to demand satisfaction.
Intuitively, as ρ21 increases, more/less class 1 demand is satisﬁed using drones with level 2/level 1 charged batteries.
On average, more recharging occurs from level 1 to level 2 to satisfy the demand of either class. For smaller values of
ρ21, increasing the parameter value provides more incentive for recharging more drones up to level 2 and satisfying
both demand levels. However, for larger values of ρ21, as level 2 charged batteries are used more to satisfy class 1
demand, fewer level 2 charged drones are available to satisfy class 2 demand.

Average Number of Action a01, a02, and a12 =

Total Number of the Action over Time over Sample Paths
Total Number of Sample Paths

∗ 100%.

(24)

15

0.250.500.751.00050000100000150000200000Iterations Optimality GapFigure 7: Sample paths of states (I, IV, VII), optimal policies (II, V, VIII), and demands (III, VI, IX) for ρ21 = 0.5, 1, 2
when the realized demand of either class equal mean demand.

ρ21

0.5
0.6
0.7
0.8
0.9
1.0
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2.0

Avg Met

Avg Met

Avg Met

Avg Met

Avg Met

Demand C1 with Demand C1 with Demand C1(%) Demand C2(%) Both Class (%)
L1 Charge (%)

L2 Charge (%)

30.0
25.7
23.8
22.3
21.0
15.9
13.9
13.1
12.6
12.2
11.5
12.4
12.5
11.1
10.3
9.1

13.8
16.2
17.6
18.9
20.0
24.9
27.0
27.6
28.0
28.5
29.7
33.4
36.4
38.1
39.2
40.4

43.6
41.9
41.4
41.3
41.1
40.9
41.9
40.7
40.7
40.7
41.3
45.8
48.9
49.2
49.5
49.5

74.5
76.2
76.8
77.1
77.1
77.5
77.5
77.5
77.5
77.3
76.6
73.4
70.4
69.7
69.0
68.6

63.6
64.2
64.4
64.5
64.5
64.6
64.6
64.5
64.5
64.4
64.1
63.7
62.9
62.5
62.1
61.9

Avg Avg Avg
a12
a02
a01

0.67
0.45
0.35
0.27
0.20
0.00
0.00
0.00
0.00
0.00
0.02
0.15
0.25
0.28
0.31
0.32

4.97
5.14
5.21
5.25
5.29
5.32
5.24
5.22
5.20
5.17
5.08
4.76
4.48
4.34
4.24
4.15

0.05
0.06
0.08
0.09
0.11
0.27
0.41
0.46
0.49
0.53
0.62
0.84
1.04
1.17
1.26
1.35

Table 5: Average met demand and policies over time for 500 sample paths for different values of ρ21.

A signiﬁcant ﬁnding is that 15 drones are not sufﬁcient to satisfy the demand of either class. We proceed with analyzing
the impact of increasing the number of drones in the station on the amount of met demand.

5.2.2 Analysis on the number of required drones

In this section, we solve the problem for a larger number of drones in the station to ﬁnd the relationship between
this number and the amount of met demand. The analysis provides signiﬁcant insights for drone delivery companies
given the high price to purchase and maintain drones in swap stations. First, we note that backward induction (BI)
can solve the problem with at most 21 drones using our computational resources. We summarized the amount of met
demand, computational time, and memory used to solve the problem for 15 to 21 drones in Table 4 using BI and RL.
For M > 21, we report the results of our D(cid:15)RL method in Table 6.

16

M

21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40

Average Met Computational Memory
Time (s) Used (GB)
Demand (%)

M Average Met Computational Memory
Time (s) Used (GB)

Demand (%)

78.3
78.9
81.0
82.0
84.5
86.7
87.3
88.5
91.2
91.7
91.8
94.2
94.2
94.9
94.9
95.6
95.7
95.8
95.9
96.1

599.2
654.1
690.1
705.2
765.1
781.4
935.2
1036.8
1037.8
1221.1
1298.3
1330.8
1337.3
1374.1
1384.5
1406.9
1426.6
1453.5
1633.4
1651.4

13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4

41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60

96.6
97.4
97.8
98.3
98.6
98.7
99.6
99.6
99.7
99.7
99.9
99.9
99.9
100.0
100.0
100.0
100.0
100.0
100.0
100.0

1703.0
1761.6
1819.7
1835.9
1969.3
2106.8
2227.0
2582.8
2717.7
2976.0
2984.6
3166.9
3169.9
3289.2
3314.5
3379.8
3431.3
3489.0
3593.0
3597.5

13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4
13.4

Table 6: Computational time, memory used, and average percentage of met demand over time for 500 sample paths
when ρ21 = 0.5 using the D(cid:15)RL method.

As shown, when M ≥ 54, the average percentage of met demand over time for 500 sample paths is 100%. We depict
a sample path of policies for M = 54 when demand equals mean demand in Figure 8. As all batteries are initially
available with a level 2 charge, we recharge fewer drones in the early morning. Then, we recharge more batteries from
6:00 to 18:00 as the demand of either class increases. Overall, more batteries are recharged to level 2 to satisfy the
demand of either class.

Figure 8: Sample paths of states, optimal policies, and demands for 54 drones, ρ21 = 0.5 when the realized demand of
either class equal mean demand.

5.2.3 Demand Classiﬁcation Contribution

In this section, we compare the outputs of the models with and without demand classiﬁcation to illustrate the contribution
of classifying the stochastic demand. We focus on demand satisfaction as the crucial metric to assess the station’s
success in delivering medical supplies. We provide this metric for a different number of drones, which is important for
decision-makers given the high price of purchasing and maintaining drones.

In Figure 9, we show the average percentage of met demand for a different number of drones when we use/do not use
demand classiﬁcation. The red color shows the percentage for the different number of drones when demand is not
classiﬁed. In this model, the state of charge of batteries is either full or empty, and full batteries are used to satisfy the
demand without considering the classiﬁed distance between the station and hospitals. The system’s state is the number
of fully-charged batteries. The action is the number of recharging actions that make an empty battery fully-charged.
The uncertainty is the stochastic demand disregarding the distance between the station and hospitals (demand nodes).
Hence, when demand is not classiﬁed, drones that return from (even a close) delivery mission are not available before
recharging for the next delivery task. For this model, optimal policies indicate that more than 150 drones are needed
to satisfy 100% of demand over 500 sample paths. In the model with demand classiﬁcation, we note that ﬁnding the

17

optimal policy and, in turn, the average percentage of demand for M > 21 is beyond our computational resources.
Therefore, we only show the percentage derived from BI for M ≤ 21 using the color blue. However, using D(cid:15)RL
enables us to offer near-optimal policies for this model. As shown, D(cid:15)RL (black line in Figure 9) provides an upper
bound for the optimal number of drones needed to satisfy a particular level of demand for the model with demand
classiﬁcation. As shown, D(cid:15)RL’s policies constantly outperform the optimal policy of the model with no demand
classiﬁcation in terms of the average percentage of met demand. For instance, D(cid:15)RL’s policies can satisfy 100% of the
met demand with only 54 drones, which is signiﬁcantly lower than the required number of batteries to hit this target
when the demand is not classiﬁed.

Figure 9: Average percentage of met demand for 500 sample paths when ρ21 = 0.5 using different models and solution
methods.

6 Conclusion

In this research, we addressed managing distribution operations of a drone swap station located at a drone hub to
maximize the amount of stochastic met demand for ﬂights delivering medical supplies in Rwanda, Africa. We denoted
this problem as stochastic scheduling and allocation problem with multiple classes of demand (SA-MCD) where the
stochastic demand is classiﬁed based on the distances a drone can ﬂy, which is linked to the level of charge inside the
drone’s battery. We formulated the problem as a Markov Decision Process (MDP) model wherein the optimal policies
determine the number of recharging batteries from one level to a higher level of charge over time when encountering
stochastic demand from different demand classes. We solved the problem using backward induction (BI) and observed
that we run into time/memory issues when the number of drones is greater than 21. Hence, we applied a reinforcement
learning method with a descending (cid:15)-greedy exploration feature (D(cid:15)RL) to ﬁnd high-quality approximate solutions
quickly and overcome the time/memory issue. We designed a set of experiments to show the high performance of our
D(cid:15)RL method and obtain insights about how to manage the operations in the station to maximize the expected total
weighted met demand when the model parameters vary.

We found plenty of directions and opportunities related to this work for future research. For instance, we did not
consider the difference between the length of time for different charging levels (still, our model outperformed the
model with no demand classiﬁcation). Future research should consider the time difference between different recharging
actions to capture the system’s behavior more realistically. As our transition probability function is large-scale and
complex, it is worth investigating various model-free approaches, like RL/ADP/Q-learning, that can circumvent the
complexities of the burdensome function, particularly when more than three classes are needed. Hence, this research,
which introduces a new set of problems that can be used in many applications, opens the avenue for future studies from
many perspectives. In terms of modeling and application, we can have multiple demand classiﬁcation criteria, such as
level of emergency (plus distance). Future research can add backlogging unsatisﬁed demands if it suits the application.
Additionally, future research should consider how the operational charging and use actions for different demand classes
impacts battery degradation wherein excessive charging should be avoided to lead to longer battery lifecycles.

18

60708090100102030405060708090100110120130140150 Number of DronesAverage (%) of Met Demand BI two demand class BI without class RL two demand classAcknowledgement

This research is supported by the Arkansas High Performance Computing Center which is funded through multiple
National Science Foundation grants and the Arkansas Economic Development Commission.

19

References

[1] Ben Mutzabaugh. Drone taxis? Dubai plans roll out of self-ﬂying pods, February 2017. Last accessed on September
23, 2021 at https://www.usatoday.com/story/travel/flights/todayinthesky/2017/02/13/dubai-
passenger-carrying-drones-could-flying-july/97850596/.

[2] Jeremy Jensen. Agricultural drones: How drones are revolutionizing agriculture and how to break into this
booming market. UAV Coach, April 2019. Last accessed on September 21, 2021 at: https://uavcoach.com/
agricultural-drones/.

[3] Elizabeth Weise. UPS tested launching a drone from a truck for deliveries. USA Today, February 2017.
Last accessed on September 21, 2021 at https://www.usatoday.com/story/tech/news/2017/02/21/ups-
delivery-top-of-van-drone-workhorse/98057076/.

[4] DHL. Successful trial integration of DHL parcelcopter into logistics chain, May 2016. Last accessed on Septem-
ber 23, 2021 at http://www.dhl.com/en/press/releases/releases_2016/all/parcel_ecommerce/
successful_trial_integration_dhl_parcelcopter_logistics_chain.html.

[5] Jeremy Dhote and Sabine Limbourg. Designing unmanned aerial vehicle networks for biological material

transportation – The case of Brussels. Computers and Industrial Engineering, 148:106652, oct 2020.

[6] Lauren Davitt. Long-range drones deliver medical supplies to remote areas of Malawi. Forbes, June 2019.
Last accessed on September 23, 2021 at https://www.forbes.com/sites/unicefusa/2019/06/19/long-
range-drones-deliver-medical-supplies-to-remote-areas-of-malawi/?sh=259e1fd36add.
[7] UNICEF Supply Division. How drones can be used to combat COVID-19. Technical report, UNICEF, 2020.

[8] Miriam McNabb. Drone delivery in the era of the pandemic: From the ﬂoor at commercial UAV expo. drone
life, September 2020. Last accessed on September 23, 2021 at https://dronelife.com/2020/09/16/drone-
delivery-in-the-era-of-the-pandemic-from-the-floor-at-commercial-uav-expo/.

[9] Chloe Kent. Moving medical supplies: enter the drone. Medical Service Network, December 2019. Last ac-
cessed on September 23, 2021 at https://www.medicaldevice-network.com/features/medical-supply-
drones/.

[10] Kim Lyons. Zipline and Walmart to launch drone deliveries of health and wellness products, September 2020. Last
accessed on September 23, 2021 at https://www.theverge.com/2020/9/14/21435019/zipline-walmart-
drone-deliveries-healthcare-amazon.

[11] Matternet. Matternet’s M2 drone system enabling new U.S. hospital delivery network at Wake Forest Baptist Health,
July 2020. Last accessed on September 23, 2021 at https://www.suasnews.com/2020/07/matternets-
m2-drone-system-enabling-new-u-s-hospital-delivery-network-at-wake-forest-baptist-
health/.

[12] Simon Chandler. Coronavirus delivers ‘world’s ﬁrst’ drone delivery service. Forbes, April 2020. Last accessed
on September 23, 2021 at https://www.forbes.com/sites/simonchandler/2020/04/03/coronavirus-
delivers-worlds-first-drone-delivery-service/?sh=407e72774957.

[13] Mike Ball. Matternet unveils new medical drone payload exchange station. Unmanned Systems News, March
2020. Last accessed on September 23, 2021 at https://www.unmannedsystemstechnology.com/2020/03/
matternet-unveils-new-medical-drone-payload-exchange-station/.

[14] G. Lacey, T. Jiang, G. Putrus, and R. Kotter. The effect of cycling on the state of health of the electric vehicle
battery. In 48th International Universities’ Power Engineering Conference, pages 1–7, Dublin, Ireland, September
2013.

[15] Matthew Shirk and Jeffrey Wishart. Effects of electric vehicle fast charging on battery life and vehicle performance.
In SAE Technical Paper, pages 1–13, Detroit, MI, April 2015. SAE 2015 World Congress & Exhibition, SAE
International.

[16] Fred Lambert. NIO deploys 18 battery swap stations covering 2,000+ km expressway. electrek, November
2018. Last accessed on September 23, 2021 at https://electrek.co/2018/11/15/nio-battery-swap-
stations-network/.

[17] Li Fusheng.

China Daily, Septem-
ber 2019. Last accessed on September 23, 2021 at https://www.chinadaily.com.cn/a/201909/09/
WS5d75e342a310cf3e3556a816.html.

BJEV advocates battery swap service for e-vehicle owners.

[18] BBC. Electric scooter with swappable batteries hits market, June 2015. Last accessed on September 23, 2021 at

http://www.bbc.com/news/technology-33183031.

20

[19] FuelRod. Power ready to go, June 2017. Last accessed on September 23, 2021 at http://www.fuel-rod.com.

[20] Li Dongmei. China’s BAIC group launches EV battery-swap station network in Beijing. China Money Network,
November 2016. Last accessed on September 23, 2021 at https://www.chinamoneynetwork.com/2016/11/
03/chinas-baic-group-launches-ev-battery-swap-station-network-in-beijing.

[21] Audrey Chauvet. Ségolène royal: An agreement at COP21 would accelerate the energy transition, March
2015. Last accessed on September 23, 2021 at http://www.20minutes.fr/planete/1742767-20151203-
segolene-royal-accord-cop21-permettrait-accelerer-transition-energetique.

[22] Alnoor Peermohamed. Reva founder looks to make electric cars affordable in India. Business Standard, April 2017.
Last accessed on September 23, 2021 at http://www.business-standard.com/article/companies/reva-
founder-looks-to-make-electric-cars-affordable-in-india-117041201118_1.html.

[23] Nikki Gordon-Bloomﬁeld. Forget better place: Simple battery swap technology from Slovakia proves its worth,
February 2014. Last accessed on September 23, 2021 at https://transportevolved.com/2014/02/12/
forget-better-place-simple-battery-swap-technology-from-slovakia-proves-its-worth/.

[24] Martin L. Puterman. Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons,

Hoboken, New Jersey, 1st edition, 2005.

[25] Warren B. Powell. Approximate Dynamic Programming: Solving the Curses of Dimensionality. John Wiley &

Sons, New York, NY, USA, 2 edition, 2011.

[26] Aryn Baker. The American drones saving lives in Rwanda, July 2017. Last accessed on September 23, 2021 at

https://time.com/rwanda-drones-zipline/.

[27] Amin Asadi and Sarah Nurre Pinkley. A stochastic scheduling, allocation, and inventory replenishment problem
for battery swap stations. Transportation Research Part E: Logistics and Transportation Review, 146:102212,
2021.

[28] Amin Asadi and Sarah Nurre Pinkley. A monotone approximate dynamic programming approach for the stochastic
scheduling, allocation, and inventory replenishment problem: Applications to drone and electric vehicle battery
swap stations. CoRR, abs/2105.07026, 2021.

[29] Rebecca S. Widrick, Sarah G. Nurre, and Matthew J. Robbins. Optimal policies for the management of an electric

vehicle battery swap station. Transportation Science, 52(1):59–79, 2018.

[30] Sarah G. Nurre, Russell Bent, Feng Pan, and Thomas C. Sharkey. Managing operations of plug-in hybrid electric

vehicle (PHEV) exchange stations for use with a smart grid. Energy Policy, 67:364–377, 2014.

[31] Olivier Kwizera and Sarah G. Nurre. Using drones for delivery: A two-level integrated inventory problem with
battery degradation and swap stations. In Proceedings of the Industrial and Systems Engineering Research
Conferences, pages 1–6, Orlando, FL, 2018.

[32] Giusy Macrina, Luigi Di Puglia Pugliese, Francesca Guerriero, and Gilbert Laporte. Drone-aided routing: A

literature review. Transportation Research Part C: Emerging Technologies, 120:102762, nov 2020.

[33] Emmanouil N. Barmpounakis, Eleni I. Vlahogianni, and John C. Golias. Unmanned Aerial Aircraft Systems
for transportation engineering: Current practice and future challenges. International Journal of Transportation
Science and Technology, 5(3):111–122, oct 2016.

[34] Yong Sik Chang and Hyun Jung Lee. Optimal delivery routing with wider drone-delivery areas along a shorter

truck-route. Expert Systems with Applications, 104:307–317, aug 2018.

[35] Alena Otto, Niels Agatz, James Campbell, Bruce Golden, and Erwin Pesch. Optimization approaches for civil

applications of unmanned aerial vehicles (UAVs) or aerial drones: A survey. Networks, 72(4):411–458, 2018.

[36] Ines Khouﬁ, Anis Laouiti, and Cedric Adjih. A survey of recent extended variants of the traveling salesman and

vehicle routing problems for unmanned aerial vehicles. Drones, 3(3):1–30, 2019.

[37] Tracy Staedter. Drones now delivering life-saving blood in Rwanda, October 2016. Last accessed on September

21, 2021 at https://www.seeker.com/drones-deliver-blood-rwanda-2045341414.html.

[38] Robyn Bainbridge. Special report: Zipline international blood drone delivery service, August 2019. Last
accessed on September 21, 2021 at https://www.airmedandrescue.com/latest/long-read/special-
report-zipline-international-blood-drone-delivery-service.

[39] James Vincent. Self-ﬂying drones are helping speed deliveries of COVID-19 vaccines in Ghana, May 2021. Last
accessed on September 21, 2021 at https://www.theverge.com/2021/3/9/22320965/drone-delivery-
vaccine-ghana-zipline-cold-chain-storage.

21

[40] Ishveena Singh. Draganﬂy to start drone delivery of COVID-19 vaccine to rural Texas, May 2021. Last accessed
on September 21, 2021 at https://dronedj.com/2021/05/20/draganfly-drone-delivery-covid-19/.

[41] Pratap Tokekar, Joshua Vander Hook, David Mulla, and Volkan Isler. Sensor planning for a symbiotic UAV and
UGV system for precision agriculture. In 2013 IEEE/RSJ International Conference on Intelligent Robots and
Systems, pages 5321–5326, 2013.

[42] Xingyin Wang, Stefan Poikonen, and Bruce Golden. The vehicle routing problem with drones: several worst-case

results. Optimization Letters, 11(4):679–697, 2017.

[43] Halil Savuran and Murat Karakaya. Efﬁcient route planning for an unmanned air vehicle deployed on a moving

carrier. Soft Computing, 20(7):2905–2920, 2016.

[44] F. Guerriero, R. Surace, V. Loscrí, and E. Natalizio. A multi-objective approach for unmanned aerial vehicle
routing problem with soft time windows constraints. Applied Mathematical Modelling, 38:839–852, 2 2014.

[45] Engineering for Change. Special report: Zipline international blood drone delivery service, May 2021.
Last accessed on September 21, 2021 at https://www.engineeringforchange.org/solutions/product/
zipline/.

[46] W. H. Al-Sabban, L. F. Gonzalez, and R. N. Smith. Wind-energy based path planning for Unmanned Aerial
Vehicles using Markov Decision Processes. In 2013 IEEE International Conference on Robotics and Automation,
pages 784–789, May 2013.

[47] S. S. Baek, H. Kwon, J. A. Yoder, and D. Pack. Optimal path planning of a target-following ﬁxed-wing UAV using
sequential decision processes. In 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems,
pages 2955–2962, November 2013.

[48] Y. Fu, X. Yu, and Y. Zhang. Sense and collision avoidance of unmanned aerial vehicles using markov decision
process and ﬂatness approach. In Proceeding of the 2015 IEEE International Conference on Information and
Automation, pages 714–719, August 2015.

[49] Awi Federgruen and Paul Zipkin. Approximations of dynamic, multilocation production and inventory problems.

Management Science, 30(1):69–84, 1984.

[50] Aghil Rezaei Somarin, Songlin Chen, Sobhan Asian, and David Z.W. Wang. A heuristic stock allocation rule for

repairable service parts. International Journal of Production Economics, 184:131–140, 2017.

[51] Oguzhan Alagoz, Lisa M. Maillart, Andrew J. Schaefer, and Mark S. Roberts. The optimal timing of living-donor

liver transplantation. Management Science, 50(10):1420–1430, 2004.

[52] Jagpreet Chhatwal, Oguzhan Alagoz, and Elizabeth S. Burnside. Optimal breast biopsy decision-making based on

mammographic features and demographic factors. Operations Research, 58(6):1577–1591, 2010.

[53] Jingyu Zhang, Brian T. Denton, Hari Balasubramanian, Nilay D. Shah, and Brant A. Inman. Optimization of
prostate biopsy referral decisions. Manufacturing & Service Operations Management, 14(4):529–547, 2012.

[54] Anahita Khojandi, Lisa M. Maillart, Oleg A. Prokopyev, Mark S. Roberts, Timothy Brown, and William W.
Barrington. Optimal implantable cardioverter deﬁbrillator (ICD) generator replacement. INFORMS Journal on
Computing, 26(3):599–615, 2014.

[55] Jean-Philippe Gayon, Saif Benjaafar, and Francis de Véricourt. Using imperfect advance demand information in
production-inventory systems with multiple customer classes. Manufacturing & Service Operations Management,
11(1):128–143, 2009.

[56] Saif Benjaafar, Mohsen ElHafsi, Chung Yee Lee, and Weihua Zhou. Optimal control of an assembly system with

multiple stages and multiple demand classes. Operations Research, 59(2):522–529, 2011.

[57] Steven Thompson, Manuel Nunez, Robert Garﬁnkel, and Matthew D. Dean. Efﬁcient short-term allocation and
reallocation of patients to ﬂoors of a hospital during demand surges. Operations Research, 57(2):261–273, 2009.

[58] Tanja Mlinar and Philippe Chevalier. Dynamic admission control for two customer classes with stochastic demands

and strict due dates. International Journal of Production Research, 54(20):6156–6173, 2016.

[59] Hua Wang, De Zhao, Qiang Meng, Ghim Ping Ong, and Der-Horng Lee. A four-step method for electric-vehicle
charging facility deployment in a dense city: An empirical study in Singapore. Transportation Research Part A:
Policy and Practice, 119:224–237, 2019.

[60] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, Cambridge,

MA, 2 edition, 2018.

22

[61] B. Van Roy, D.P. Bertsekas, Y. Lee, and J.N. Tsitsiklis. A neuro-dynamic programming approach to retailer
inventory management. In Proceedings of the IEEE Conference on Decision and Control, volume 4, pages
4052–4057, 1997.

[62] Mustafa Çimen and Chris Kirkbride. Approximate dynamic programming algorithms for multidimensional
ﬂexible production-inventory problems. International Journal of Production Research, 55(7):2034–2050, 2017.
[63] Mustafa Çimen and Christopher Kirkbride. Approximate dynamic programming algorithms for multidimensional

inventory optimization problems. IFAC Proceedings Volumes, 46(9):2015–2020, 2013.

[64] Chengzhi Jiang and Zhaohan Sheng. Case-based reinforcement learning for dynamic inventory control in a

multi-agent supply-chain system. Expert Systems with Applications, 36:6520–6526, 2009.

[65] S. Kamal Chaharsooghi, Jafar Heydari, and S. Hessameddin Zegordi. A reinforcement learning model for supply
chain ordering management: An application to the beer game. Decision Support Systems, 45(4):949–959, 2008.
[66] Dimitris Bertsimas and Ramazan Demir. An approximate dynamic programming approach to multidimensional

knapsack problems. Management Science, 48(4):550–565, 2002.

[67] Alexander Erdelyi and Huseyin Topaloglu. Approximate dynamic programming for dynamic capacity allocation

with multiple priority levels. IIE Transactions, 43(2):129–142, 2010.

[68] Matthew S. Maxwell, Mateo Restrepo, Shane G. Henderson, and Huseyin Topaloglu. Approximate dynamic

programming for ambulance redeployment. INFORMS Journal on Computing, 22(2):266–281, 2010.

[69] Warren B. Powell and Huseyin Topaloglu. Approximate dynamic programming for large-scale resource allocation

problems. INFORMS TutORials in Operations Research, pages 123–147, 2005.

[70] Amirali Nasrollahzadeh, Amin Khademi, and Maria E. Mayorga. Real-time ambulance dispatching and relocation.

Manufacturing and Service Operations Management, 20(3):467–480, 2018.

[71] Ick-Hyun Kwon, Chang Ouk Kim, Jin Jun, and Jung Hoon Lee. Case-based myopic reinforcement learning for

satisfying target service level in supply chain. Expert Systems with Applications, 35(1):389–397, 2008.

[72] Ilya O. Ryzhov, Martijn R. K. Mes, Warren B. Powell, and Gerald van den Berg. Bayesian exploration strategies

for approximate dynamic programming. Operations Research, 67(1):198–214, 2019.

[73] Evan Ackerman. Zipline launches long-distance drone delivery of COVID-19 supplies in the U.S., May 2020. Last
accessed on September 21, 2021 at https://spectrum.ieee.org/automaton/robotics/drones/zipline-
long-distance-delivery-covid19-supplies.

[74] R.W. Sinnott. Virtues of the Haversine. Sky and Telescope, 68(2):158, 1984.
[75] Rwanda Civil Aviation Authority. Unmanned aircraft operations in Rwanda unmanned aircraft operations in
Rwanda, May 2021. Last accessed on September 21, 2021 at https://caa.gov.rw/index.php?id=110.
[76] Gordon Swartzman. The patient arrival process in hospitals: statistical analysis. Health services research,

5(4):320–329, Winter 1970.

[77] Mor Armony, Shlomo Israelit, Avishai Mandelbaum, Yariv N. Marmor, Yulia Tseytlin, and Galit B. Yom-Tov. On

patient ﬂow in hospitals: A data-based queueing-science perspective. Stochastic Systems, 5(1):146–194, 2015.

[78] Harry McNabb. A drone battery that charges in 5 minutes, August 2020. Last accessed on September 21, 2021 at

https://dronelife.com/2020/08/01/a-drone-battery-that-charges-in-5-minutes/.

[79] Magdalena Petrova and Lora Kolodny. Zipline’s new drone can deliver medical supplies at 79 miles per hour, April
2018. Last accessed on September 21, 2021 at https://www.cnbc.com/2018/04/02/zipline-new-zip-2-
drone-delivers-supplies-at-79-mph.html.

[80] Neelam Dhingra. Estimate blood requirements - search for a global standard, April 2010. Last accessed on Septem-
ber 21, 2021 at https://www.who.int/bloodsafety/transfusion_services/estimation_present.
[81] Ange Iliza. How can Rwanda tackle its blood donation shortfall?, June 2020. Last accessed on September 21,
2021 at https://www.newtimes.co.rw/news/rwanda-tackle-its-blood-donation-shortfall.
[82] Linda Green, Peter Kolesar, and Ward Whitt. Coping with time-varying demand when setting stafﬁng requirements

for a service system. Production and Operations Management, 16:13–39, 01 2007.

[83] Yogesh Tiwari, Sonu Goel, and Amarjeet Singh. Arrival time pattern and waiting time distribution of patients
in the emergency outpatient department of a tertiary level health care institution of North India. Journal of
Emergencies, Trauma and Shock, 7(3):160–165, 2014.

[84] Spencer S Jones, M Stat, and Todd Allen. Department Cycle to Improve Patient Flow. Radiology, 33(5):247–255,

2007.

23

[85] Abraham P. George and Warren B. Powell. Adaptive stepsizes for recursive estimation with applications in

approximate dynamic programming. Machine Learning, 65(1):167–198, 2006.

24

