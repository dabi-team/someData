Compound Probabilistic Context-Free Grammars
for Grammar Induction

Yoon Kim
Harvard University
Cambridge, MA, USA
yoonkim@seas.harvard.edu

Chris Dyer
DeepMind
London, UK
cdyer@google.com

Alexander M. Rush
Harvard University
Cambridge, MA, USA
srush@seas.harvard.edu

Abstract

In contrast

We study a formalization of the grammar
induction problem that models sentences as
being generated by a compound probabilis-
tic context-free grammar.
to
traditional formulations which learn a sin-
gle stochastic grammar, our grammar’s rule
probabilities are modulated by a per-sentence
continuous latent variable, which induces
marginal dependencies beyond the traditional
context-free assumptions.
Inference in this
grammar is performed by collapsed variational
inference, in which an amortized variational
posterior is placed on the continuous variable,
and the latent trees are marginalized out with
dynamic programming. Experiments on En-
glish and Chinese show the effectiveness of
our approach compared to recent state-of-the-
art methods when evaluated on unsupervised
parsing.

1

Introduction

Grammar induction is the task of inducing hier-
archical syntactic structure from data. Statistical
approaches to grammar induction require specify-
ing a probabilistic grammar (e.g. formalism, num-
ber and shape of rules), and ﬁtting its parameters
through optimization. Early work found that it was
difﬁcult to induce probabilistic context-free gram-
mars (PCFG) from natural language data through
direct methods, such as optimizing the log like-
lihood with the EM algorithm (Lari and Young,
1990; Carroll and Charniak, 1992). While the rea-
sons for the failure are manifold and not com-
two major potential causes
pletely understood,
are the ill-behaved optimization landscape and the
overly strict independence assumptions of PCFGs.
More successful approaches to grammar induction
have thus resorted to carefully-crafted auxiliary
objectives (Klein and Manning, 2002), priors or

Code: https://github.com/harvardnlp/compound-pcfg

non-parametric models (Kurihara and Sato, 2006;
Johnson et al., 2007; Liang et al., 2007; Wang and
Blunsom, 2013), and manually-engineered fea-
tures (Huang et al., 2012; Golland et al., 2012) to
encourage the desired structures to emerge.

We revisit these aforementioned issues in light
of advances in model parameterization and infer-
ence. First, contrary to common wisdom, we
ﬁnd that parameterizing a PCFG’s rule probabil-
ities with neural networks over distributed rep-
resentations makes it possible to induce linguis-
tically meaningful grammars by simply optimiz-
ing log likelihood. While the optimization prob-
lem remains non-convex, recent work suggests
that there are optimization beneﬁts afforded by
over-parameterized models (Arora et al., 2018;
Xu et al., 2018; Du et al., 2019), and we in-
deed ﬁnd that this neural PCFG is signiﬁcantly
easier to optimize than the traditional PCFG.
Second, this factored parameterization makes it
straightforward to incorporate side information
into rule probabilities through a sentence-level
continuous latent vector, which effectively allows
different contexts in a derivation to coordinate.
In this compound PCFG—continuous mixture of
PCFGs—the context-free assumptions hold con-
ditioned on the latent vector but not uncondition-
ally, thereby obtaining longer-range dependencies
within a tree-based generative process.

To utilize this approach, we need to efﬁciently
optimize the log marginal likelihood of observed
sentences. While compound PCFGs break efﬁ-
cient inference, if the latent vector is known the
distribution over trees reduces to a standard PCFG.
This property allows us to perform grammar in-
duction using a collapsed approach where the la-
tent trees are marginalized out exactly with dy-
namic programming. To handle the latent vec-
tor, we employ standard amortized inference us-
ing reparameterized samples from a variational

0
2
0
2

r
a

M
9
2

]
L
C
.
s
c
[

9
v
5
2
2
0
1
.
6
0
9
1
:
v
i
X
r
a

 
 
 
 
 
 
posterior approximated from an inference network
(Kingma and Welling, 2014; Rezende et al., 2014).
On standard benchmarks for English and Chi-
nese, the proposed approach is found to perform
favorably against recent neural approaches to un-
supervised parsing (Shen et al., 2018, 2019; Droz-
dov et al., 2019; Kim et al., 2019).

2 Probabilistic Context-Free Grammars

We consider context-free grammars (CFG) con-
sisting of a 5-tuple G = (S, N , P, Σ, R) where
S is the distinguished start symbol, N is a ﬁnite
set of nonterminals, P is a ﬁnite set of pretermi-
nals,1 Σ is a ﬁnite set of terminal symbols, and R
is a ﬁnite set of rules of the form,

S → A,

A ∈ N

A → B C,

A ∈ N , B, C ∈ N ∪ P

T → w,

T ∈ P, w ∈ Σ.

A probabilistic context-free grammar
(PCFG)
consists of a grammar G and rule probabilities
π = {πr}r∈R such that πr is the probability of
the rule r. Letting TG be the set of all parse trees of
G, a PCFG deﬁnes a probability distribution over
t ∈ TG via pπ(t) = (cid:81)
πr where tR is the set
r∈tR
of rules used in the derivation of t. It also deﬁnes
a distribution over string of terminals x ∈ Σ∗ via

pπ(x) =

(cid:88)

pπ(t),

t∈TG (x)

where TG(x) = {t | yield(t) = x}, i.e. the set of
trees t such that t’s leaves are x. We will slightly
abuse notation and use

pπ(t | x) (cid:44)

1[yield(t) = x]pπ(t)
pπ(x)

to denote the posterior distribution over the unob-
served latent trees given the observed sentence x,
where 1[·] is the indicator function.2

2.1 Parameterization

The standard way to parameterize a PCFG is to
simply associate a scalar to each rule πr with the

1Since we will be inducing a grammar directly from
words, P is roughly the set of part-of-speech tags and N is
the set of constituent labels. However, to avoid issues of label
alignment, evaluation is only on the tree topology.

2Therefore when used in the context of a posterior distri-
bution conditioned on a sentence x, the variable t does not
include the leaves x and only refers to the unobserved non-
terminal/preterminal symbols.

constraint that they form valid probability distri-
butions, i.e. each nonterminal is associated with
a fully-parameterized categorical distribution over
its rules. This direct parameterization is algorith-
mically convenient since the M-step in the EM al-
gorithm (Dempster et al., 1977) has a closed form.
However, there is a long history of work showing
that it is difﬁcult to learn meaningful grammars
from natural language data with this parameteri-
zation (Carroll and Charniak, 1992).3 Successful
approaches to unsupervised parsing have therefore
modiﬁed the model/learning objective by guiding
potentially unrelated rules to behave similarly.

Recognizing that sharing among rule types is
beneﬁcial, we propose a neural parameterization
where rule probabilities are based on distributed
representations. We associate embeddings with
each symbol, introducing input embeddings wN
for each symbol N on the left side of a rule (i.e.
N ∈ {S} ∪ N ∪ P). For each rule type r, πr is
parameterized as follows,

πS→A =

πA→BC =

πT →w =

A f1(wS) + bA)

A(cid:48) f1(wS) + bA(cid:48))

BC wA + bBC)

B(cid:48)C(cid:48) wA + bB(cid:48)C(cid:48))

,

exp(u(cid:62)
A(cid:48)∈N exp(u(cid:62)
exp(u(cid:62)
B(cid:48)C(cid:48)∈M exp(u(cid:62)

(cid:80)

(cid:80)

exp(u(cid:62)
w(cid:48)∈Σ exp(u(cid:62)

(cid:80)

w f2(wT ) + bw)

w(cid:48) f2(wT ) + bw(cid:48))

,

,

where M is the product space (N ∪P)×(N ∪P),
and f1, f2 are MLPs with two residual layers.
Note that we do not use an MLP for rules of the
type πA→BC, as it did not empirically improve re-
sults. See appendix A.1 for the full parameteriza-
tion. Going forward, we will use EG = {wU | U ∈
{S} ∪ N ∪ P} ∪ {uV | V ∈ N ∪ M ∪ Σ} to
denote the set of input/output symbol embeddings
for grammar G, and λ to refer to the parameters of
the neural network f1, f2 used to obtain the rule
probabilities. A graphical model-like illustration
of the neural PCFG is shown in Figure 1 (left).

It is clear that the neural parameterization does
not change the underlying probabilistic assump-
tions. The difference between the two is anal-
ogous to the difference between count-based vs.
feed-forward neural language models, where feed-
forward neural language models make the same
Markov assumptions as the count-based models
but are able to take advantage of shared, dis-
tributed representations.

3In preliminary experiments we were indeed unable to

learn linguistically meaningful grammars with this PCFG.

N

A1

A2

T3

T1

w1

T2

w2

w3

πS

πN

πP

EG

N

T1

w1

z

c

γ

A1

A2

T3

T2

w2

w3

πz,S

πz,N

πz,P

EG

Figure 1: A graphical model-like diagram for the neural PCFG (left) and the compound PCFG (right) for an example tree
structure. In the above, A1, A2 ∈ N are nonterminals, T1, T2, T3 ∈ P are preterminals, w1, w2, w3 ∈ Σ are terminals. In the
neural PCFG, the global rule probabilities π = πS ∪ πN ∪ πP are the output from a neural net run over the symbol embeddings
EG, where πN are the set of rules with a nonterminal on the left hand side (πS and πP are similarly deﬁned). In the compound
PCFG, we have per-sentence rule probabilities πz = πz,S ∪ πz,N ∪ πz,P obtained from running a neural net over a random
vector z (which varies across sentences) and global symbol embeddings EG. In this case, the context-free assumptions hold
conditioned on z, but they do not hold unconditionally: e.g. when conditioned on z and A2, the variables A1 and T1 are
independent; however when conditioned on just A2, they are not independent due to the dependence path through z. Note that
the rule probabilities are random variables in the compound PCFG but deterministic variables in the neural PCFG.

3 Compound PCFGs

probabilities given by πz,

A compound probability distribution (Robbins,
1951) is a distribution whose parameters are them-
selves random variables. These distributions gen-
eralize mixture models to the continuous case, for
example in factor analysis which assumes the fol-
lowing generative process,

z ∼ N(0, I),

x ∼ N(Wz, Σ).

Compound distributions provide the ability to
model rich generative processes, but marginaliz-
ing over the latent parameter can be computation-
ally intractable unless conjugacy can be exploited.
In this work, we study compound probabilis-
tic context-free grammars whose distribution over
trees arises from the following generative process:
we ﬁrst obtain rule probabilities via

z ∼ pγ(z),

πz = fλ(z, EG),

where pγ(z) is a prior with parameters γ (spheri-
cal Gaussian in this paper), and fλ is a neural net-
work that concatenates the input symbol embed-
dings with z and outputs the sentence-level rule
probabilities πz,

πz,S→A ∝ exp(u(cid:62)
πz,A→BC ∝ exp(u(cid:62)
πz,T →w ∝ exp(u(cid:62)

A f1([wS; z]) + bA),
BC [wA; z] + bBC),
w f2([wT ; z]) + bw),

t ∼ PCFG(πz),

x = yield(t).

This can be viewed as a continuous mixture of
PCFGs, or alternatively, a Bayesian PCFG with a
prior on sentence-level rule probabilities parame-
terized by z, λ, EG.4 Importantly, under this gen-
erative model the context-free assumptions hold
conditioned on z, but they do not hold uncondi-
tionally. This is shown in Figure 1 (right) where
there is a dependence path through z if it is not
conditioned upon. Compound PCFGs give rise to
a marginal distribution over parse trees t via

(cid:90)

pθ(t) =

p(t | z)pγ(z) dz,

where pθ(t | z) = (cid:81)
πz,r. The subscript in
r∈tR
πz,r denotes the fact that the rule probabilities de-
pend on z. Compound PCFGs are clearly more ex-
pressive than PCFGs as each sentence has its own
set of rule probabilities. However, it still assumes
a tree-based generative process, making it possible
to learn latent tree structures.

One motivation for the compound PCFG is
that simple, unlexicalized grammars (such as the
PCFG we have been working with) are unlikely
to represent an adequate model of natural lan-
guage, although they do facilitate efﬁcient learn-

where [w; z] denotes vector concatenation. Then
a tree/sentence is sampled from a PCFG with rule

4Under the Bayesian PCFG view, pγ(z) is a distribution

over z (a subset of the prior), and is thus a hyperprior.

ing and inference.5 We can in principle model
richer dependencies through vertical/horizontal
Markovization (Johnson, 1998; Klein and Man-
ning, 2003) and lexicalization (Collins, 1997).
However such dependencies complicate training
due to the rapid increase in the number of rules.
Under this view, we can interpret the compound
PCFG as a restricted version of some lexicalized,
higher-order PCFG where a child can depend on
structural and lexical context through a shared la-
tent vector.6 We hypothesize that this dependence
among siblings is especially useful in grammar in-
duction from words, where (for example) if we
know that watched is used as a verb then the noun
phrase is likely to be a movie.

In contrast to the usual Bayesian treatment of
PCFGs which places priors on global rule proba-
bilities (Kurihara and Sato, 2006; Johnson et al.,
2007; Wang and Blunsom, 2013), the compound
PCFG assumes a prior on local, sentence-level
rule probabilities.
It is therefore closely related
to the Bayesian grammars studied by Cohen et al.
(2009) and Cohen and Smith (2009), who also
sample local rule probabilities from a logistic nor-
mal prior for training dependency models with va-
lence (DMV) (Klein and Manning, 2004).

3.1

Inference in Compound PCFGs

The expressivity of compound PCFGs comes at
a signiﬁcant challenge in learning and inference.
Letting θ = {EG, λ} be the parameters of the
generative model, we would like to maximize the
log marginal likelihood of the observed sentence
log pθ(x). In the neural PCFG the log marginal
likelihood

log pθ(x) = log

(cid:88)

pθ(t),

t∈TG (x)

5A piece of evidence for the misspeciﬁcation of unlexi-
calized ﬁrst-order PCFGs as a statistical model of natural lan-
guage is that if one pretrains such a PCFG on supervised data
and continues training with the unsupervised objective (i.e.
log marginal likelihood), the resulting grammar deviates sig-
niﬁcantly from the supervised initial grammar while the log
marginal likelihood improves (Johnson et al., 2007). Simi-
lar observations have been made for part-of-speech induction
with Hidden Markov Models (Merialdo, 1994).

6Note that the compound “PCFG” is a slight misnomer
because the model is no longer context-free in the usual
sense. Another interpretation of the model is to view it as a
vectorized version of indexed grammars (Aho, 1968), which
extend CFGs by augmenting nonterminals with additional in-
dex strings that may be inherited or modiﬁed during deriva-
tion. Compound PCFGs instead equip nonterminals with a
continuous vector that is always inherited.

can be obtained by summing out the latent tree
structure using the inside algorithm (Baker, 1979),
which is differentiable and thus amenable to
gradient-based optimization.7
In the compound
PCFG, the log marginal likelihood is given by,

log pθ(x) = log

(cid:16) (cid:90)

pθ(x | z)pγ(z) dz

(cid:17)

(cid:16) (cid:90) (cid:88)

= log

pθ(t | z)pγ(z) dz

(cid:17)

.

t∈TG (x)

Notice that while the integral over z makes this
quantity intractable, when we condition on z, we
can tractably perform the inner summation to ob-
tain pθ(x | z) using the inside algorithm. We there-
fore resort to collapsed amortized variational in-
ference. We ﬁrst obtain a sample z from a vari-
ational posterior distribution (given by an amor-
tized inference network), then perform the inner
marginalization conditioned on this sample. The
evidence lower bound ELBO(θ, φ; x) is then,

Eqφ(z | x)[log pθ(x | z)] − KL[qφ(z | x) (cid:107) pγ(z)],

and we can calculate pθ(x | z) given a sample z
from a variational posterior qφ(z | x). For the vari-
ational family we use a diagonal Gaussian where
the mean/log-variance vectors are given by an
afﬁne layer over max-pooled hidden states from an
LSTM over x. We can obtain low-variance esti-
mators for ∇θ,φ ELBO(θ, φ; x) by using the repa-
rameterization trick for the expected reconstruc-
tion likelihood and the analytical expression for
the KL term (Kingma and Welling, 2014).

We remark that under the Bayesian PCFG view,
since the parameters of the prior (i.e. θ) are esti-
mated from the data, our approach can be seen as
an instance of empirical Bayes (Robbins, 1956).8

3.2 MAP Inference

After training, we are interested in comparing the
learned trees against an annotated treebank. This
requires inferring the most likely tree given a sen-
argmaxt pθ(t | x). For the neural
tence, i.e.
PCFG we can obtain the most likely tree by using

7In the context of the EM algorithm, directly per-
forming gradient ascent on the log marginal likelihood is
equivalent to performing an exact E-step (with the inside-
outside algorithm) followed by a gradient-based M-step,
i.e. ∇θ log pθ(x) = Epθ (t | x)[∇θ log pθ(t)] (Salakhutdinov
et al., 2003; Berg-Kirkpatrick et al., 2010; Eisner, 2016).

8See Berger (1985) (chapter 4), Zhang (2003), and Co-
hen (2016) (chapter 3) for further discussion on compound
models and empirical Bayes.

the Viterbi version of the inside algorithm (CKY
algorithm). For the compound PCFG, the argmax
is intractable to obtain exactly, and hence we esti-
mate it with the following approximation,

(cid:90)

argmax
t

= argmax

t

pθ(t | x, z)pθ(z | x) dz
(cid:0)t | x, µφ(x)(cid:1),

pθ

where µφ(x) is the mean vector from the infer-
ence network. The above approximates the true
posterior pθ(z | x) with δ(z − µφ(x)), the Dirac
delta function at the mode of the variational pos-
terior.9 This quantity is tractable as in the PCFG
case. Other approximations are possible: for ex-
ample we could use qφ(z | x) as an importance
sampling distribution to estimate the ﬁrst integral.
However we found the above approximation to be
efﬁcient and effective in practice.

4 Experimental Setup

4.1 Data

We test our approach on the Penn Treebank (PTB)
(Marcus et al., 1993) with the standard splits (2-21
for training, 22 for validation, 23 for test) and the
same preprocessing as in recent works (Shen et al.,
2018, 2019), where we discard punctuation, low-
ercase all tokens, and take the top 10K most fre-
quent words as the vocabulary. This setup is more
challenging than traditional setups, which usually
experiment on shorter sentences and use gold part-
of-speech tags.

We further experiment on Chinese with version
5.1 of the Chinese Penn Treebank (CTB) (Xue
et al., 2005), with the same splits as in Chen and
Manning (2014). On CTB we also remove punc-
tuation and keep the top 10K word types.

4.2 Hyperparameters

Our PCFG uses 30 nonterminals and 60 pretermi-
nals, with 256-dimensional symbol embeddings.
The compound PCFG uses 64-dimensional latent
vectors. The bidirectional LSTM inference net-
work has a single layer with 512 dimensions, and
the mean and the log variance vector for qφ(z | x)
are given by max-pooling the hidden states of
the LSTM and passing it through an afﬁne layer.
Model parameters are initialized with Xavier uni-
form initialization. For training we use Adam

9Since pθ(t | x, z) is continuous with respect to z, we

have (cid:82) pθ(t | x, z)δ(z − µφ(x)) dz = pθ

(cid:0)t | x, µφ(x)(cid:1).

(Kingma and Ba, 2015) with β1 = 0.75, β2 =
0.999 and learning rate of 0.001, with a maxi-
mum gradient norm limit of 3. We train for 10
epochs with batch size equal to 4. We employ a
curriculum learning strategy (Bengio et al., 2009)
where we train only on sentences of length up to
30 in the ﬁrst epoch, and increase this length limit
by 1 each epoch. Similar curriculum-based strate-
gies have used in the past for grammar induction
(Spitkovsky et al., 2012). During training we per-
form early stopping based on validation perplex-
ity.10 Finally, to mitigate against overﬁtting to
PTB, experiments on CTB utilize the same hyper-
parameters from PTB.

4.3 Baselines and Evaluation

While we induce a full stochastic grammar (i.e.
a distribution over symbolic rewrite rules) in this
work, directly assessing the learned grammar is it-
self nontrivial. As a proxy, we adopt the usual ap-
proach and instead evaluate the induced grammar
as an unsupervised parsing system. However, even
in this setting we observe that there is enough vari-
ation across prior work on to render a meaningful
comparison difﬁcult.

In particular, some important dimensions along
which prior works vary include, (1) input data:
earlier work on generally assumed gold (or in-
duced) part-of-speech tags (Klein and Manning,
2004; Smith and Eisner, 2004; Bod, 2006; Sny-
der et al., 2009), while more recent works induce
grammar directly from words (Spitkovsky et al.,
2013; Shen et al., 2018); (2) use of punctuation:
even within papers that induce parse trees directly
from words, some papers employ heuristics based
on punctuation as punctuation is usually a strong
signal for start/end of constituents (Seginer, 2007;
Ponvert et al., 2011; Spitkovsky et al., 2013), some
train with punctuation (Jin et al., 2018; Drozdov
et al., 2019; Kim et al., 2019), while others discard
punctuation altogether for training (Shen et al.,
2018, 2019); (3) train/test data: some works do
not explicitly separate out train/test sets (Reichart
and Rappoport, 2010; Golland et al., 2012) while
some do (Huang et al., 2012; Parikh et al., 2014;
Htut et al., 2018). Maintaining train/test splits is

10However, we used F1 against validation trees on PTB to
select some hyperparameters (e.g. grammar size), as is some-
times done in grammar induction. Hence our PTB results are
arguably not fully unsupervised in the strictest sense of the
term. The hyperparameters of the PRPN/ON baselines are
also tuned using validation F1 for fair comparison.

Model

PTB

CTB

Mean Max Mean Max

Neural Compound
PCFG

PCFG

38.1
PRPN (Shen et al., 2018)
49.4
ON (Shen et al., 2019)
URNNG† (Kim et al., 2019)
− 45.4
DIORA† (Drozdov et al., 2019) − 58.9

37.4
47.7

Left Branching
Right Branching
Random Trees
PRPN (tuned)
ON (tuned)
Scalar PCFG
Neural PCFG
Compound PCFG

Oracle Trees

−
−
−
−

−
−
−
−

9.7
20.0

15.7
30.4
25.4

16.0
31.5
25.7

< 15.0

25.7
36.0

29.5
39.8

8.7
39.5

19.2
47.3
48.1

19.5
47.9
50.0

< 35.0

50.8
55.2

52.6
60.1

84.3

81.1

Table 1: Unlabeled sentence-level F1 scores on PTB and
CTB test sets. Top shows results from previous work while
the rest of the results are from this paper. Mean/Max scores
are obtained from 4 runs of each model with different random
seeds. Oracle is the maximum score obtainable with bina-
rized trees, since we compare against the non-binarized gold
trees per convention. Results with † are trained on a version
of PTB with punctuation, and hence not strictly comparable
to the present work. For URNNG/DIORA, we take the parsed
test set provided by the authors from their best runs and eval-
uate F1 with our evaluation setup, which ignores punctuation.

less of an issue for unsupervised structure learn-
ing, however in this work we follow the latter
and separate train/test data.
(4) evaluation: for
unlabeled F1, almost all works ignore punctua-
tion (even approaches that use punctuation dur-
ing training typically ignore them during evalu-
ation), but there is some variance in discarding
trivial spans (width-one and sentence-level spans)
and using corpus-level versus sentence-level F1.11
In this paper we discard trivial spans and evalu-
ate on sentence-level F1 per recent work (Shen
et al., 2018, 2019). Given the above, we mainly
compare our approach against two recent, strong
baselines with open source code: Parsing Predict
Reading Network (PRPN)12 (Shen et al., 2018)
and Ordered Neurons (ON)13 (Shen et al., 2019).
These approaches train a neural language model
with gated attention-like mechanisms to induce bi-
nary trees, and achieve strong unsupervised pars-
ing performance even when trained on corpora
where punctuation is removed. Since the origi-
nal results were on both language modeling and
unsupervised parsing, their hyperparameters were
presumably tuned to do well on both and thus may
not be optimal for just unsupervised parsing. We

11Corpus-level F1 calculates precision/recall at the corpus
level to obtain F1, while sentence-level F1 calculates F1 for
each sentence and averages across the corpus.
12https://github.com/yikangshen/PRPN
13https://github.com/yikangshen/Ordered-Neurons

PRPN

ON

47.3
1.5
39.9
82.3

48.1
14.1
31.0
71.3

50.8
11.8
27.7
65.2

50.0% 51.2% 52.5%
59.2% 64.5% 71.2%
46.7% 41.0% 33.8%
57.2% 54.4% 58.8%
44.3% 38.1% 32.5%
32.8% 31.6% 45.5%

Gold
Left
Right
Self

SBAR
NP
VP
PP
ADJP
ADVP

55.2
13.0
28.4
66.8

56.1%
74.7%
41.7%
68.8%
40.4%
52.5%

Table 2: (Top) Mean F1 similarity against Gold, Left, Right,
and Self trees. Self F1 score is calculated by averaging over
(Bottom) Frac-
all 6 pairs obtained from 4 different runs.
tion of ground truth constituents that were predicted as a con-
stituent by the models broken down by label (i.e. label recall).

therefore tune the hyperparameters of these base-
lines for unsupervised parsing only (i.e. on valida-
tion F1).

5 Results and Discussion

Table 1 shows the unlabeled F1 scores for our
models and various baselines. All models soundly
outperform right branching baselines, and we ﬁnd
that the neural PCFG/compound PCFG are strong
models for grammar induction. In particular the
compound PCFG outperforms other models by
an appreciable margin on both English and Chi-
nese. We again note that we were unable to induce
meaningful grammars through a traditional PCFG
with the scalar parameterization despite a thor-
ough hyperparameter search.14 See appendix A.2
for the full results broken down by sentence length
for sentence- and corpus-level F1.

Table 2 analyzes the learned tree structures.
We compare similarity as measured by F1 against
gold, left, right, and “self” trees (top), where self
F1 score is calculated by averaging over all 6 pairs
obtained from 4 different runs. We ﬁnd that PRPN
is particularly consistent across multiple runs. We
also observe that different models are better at
identifying different constituent labels, as mea-
sured by label recall (Table 2, bottom). While left
as future work, this naturally suggests an ensemble
approach wherein the empirical probabilities of
constituents (obtained by averaging the predicted
binary constituent labels from the different mod-
els) are used either to supervise another model or
directly as potentials in a CRF constituency parser.

14Training perplexity was much higher than in the neural
case, indicating signiﬁcant optimization issues. However we
did not experiment with online EM (Liang and Klein, 2009),
and it is possible that such methods would yield better results.

PPL

Syntactic Eval.

LSTM LM
PRPN

Induced RNNG
Induced URNNG

ON

Induced RNNG
Induced URNNG

Neural PCFG

Induced RNNG
Induced URNNG

Compound PCFG
Induced RNNG
Induced URNNG

RNNG on Oracle Trees
+ URNNG Fine-tuning

86.2
87.1
95.3
90.1
87.2
95.2
89.9
252.6
95.8
86.0
196.3
89.8
83.7

80.6
78.3

60.9%
62.2%
60.1%
61.8%
61.6%
61.7%
61.9%
49.2%
68.1%
69.1%
50.7%
70.0%
76.1%

70.4%
76.1%

F1

−
47.9
47.8
51.6
50.0
50.6
55.1
52.6
51.4
58.7
60.1
58.1
66.9

71.9
72.8

Table 3: Results from training RNNGs on induced trees
from various models (Induced RNNG) on the PTB. Induced
URNNG indicates ﬁne-tuning with the URNNG objective.
We show perplexity (PPL), grammaticality judgment perfor-
mance (Syntactic Eval.), and unlabeled F1. PPL/F1 are cal-
culated on the PTB test set and Syntactic Eval.
is from
Marvin and Linzen (2018)’s dataset. Results on top do not
make any use of annotated trees, while the bottom two re-
sults are trained on binarized gold trees. The perplexity
numbers here are not comparable to standard results on the
PTB since our models are generative model of sentences and
hence we do not carry information across sentence bound-
aries. Also note that all the RNN-based models above (i.e.
LSTM/PRPN/ON/RNNG/URNNG) have roughly the same
model capacity (see appendix A.3).

Finally, all models seemed to have some difﬁculty
in identifying SBAR/VP constituents which typi-
cally span more words than NP constituents, in-
dicating further opportunities for improvement on
unsupervised parsing.

5.1

Induced Trees for Downstream Tasks

While the compound PCFG has fewer indepen-
dence assumptions than the neural PCFG, it is
still a more constrained model of language than
standard neural language models (NLM) and thus
not competitive in terms of perplexity: the com-
pound PCFG obtains a perplexity of 196.3 while
an LSTM language model (LM) obtains 86.2 (Ta-
ble 3).15 In contrast, both PRPN and ON perform
as well as an LSTM LM while maintaining good
unsupervised parsing performance.

We thus experiment to see if it is possible
to use the induced trees to supervise a more
ﬂexible generative model that can make use of

15We did manage to almost match the perplexity of an
NLM by additionally conditioning the terminal probabilities
on previous history, i.e.

πz,T →wt ∝ exp(u(cid:62)

w f2([wT ; z; ht]) + bw),

where ht is the hidden state from an LSTM over x<t. How-
ever the unsupervised parsing performance was far worse (≈
25 F1 on the PTB).

Figure 2: Alignment of induced nonterminals ordered from
top based on predicted frequency (therefore NT-04 is the most
frequently-predicted nonterminal). For each nonterminal we
visualize the proportion of correctly-predicted constituents
that correspond to particular gold labels. For reference we
also show the precision (i.e. probability of correctly predict-
ing unlabeled constituents) in the rightmost column.

tree structures—namely, recurrent neural network
grammars (RNNG) (Dyer et al., 2016). RNNGs
are generative models of language that jointly
model syntax and surface structure by incremen-
tally generating a syntax tree and sentence. As
with NLMs, RNNGs make no independence as-
sumptions, and have been shown to outperform
NLMs in terms of perplexity and grammatical-
ity judgment when trained on gold trees (Kuncoro
et al., 2018; Wilcox et al., 2019).

We take the best run from each model and parse
the training set,16 and use the induced trees to su-
pervise an RNNG for each model using the param-
eterization from Kim et al. (2019).17 We are also
interested in syntactic evaluation of our models,
and for this we utilize the framework and dataset
from Marvin and Linzen (2018), where a model is
presented two minimally different sentences such
as:

the senators near the assistant are old

*the senators near the assistant is old

and must assign higher probability to grammatical
sentence.

16The train/test F1 was similar for all models.
17https://github.com/harvardnlp/urnng

he retired as senior vice president ﬁnance and administration and chief ﬁnancial ofﬁcer of the company oct. N
kenneth j. (cid:104)unk(cid:105) who was named president of this thrift holding company in august resigned citing personal reasons
the former president and chief executive eric w. (cid:104)unk(cid:105) resigned in june
(cid:104)unk(cid:105) ’s president and chief executive ofﬁcer john (cid:104)unk(cid:105) said the loss stems from several factors
mr. (cid:104)unk(cid:105) is executive vice president and chief ﬁnancial ofﬁcer of (cid:104)unk(cid:105) and will continue in those roles
charles j. lawson jr. N who had been acting chief executive since june N will continue as chairman

(cid:104)unk(cid:105) corp. received an N million army contract for helicopter engines
boeing co. received a N million air force contract for developing cable systems for the (cid:104)unk(cid:105) missile
general dynamics corp. received a N million air force contract for (cid:104)unk(cid:105) training sets
grumman corp. received an N million navy contract to upgrade aircraft electronics
thomson missile products with about half british aerospace ’s annual revenue include the (cid:104)unk(cid:105) (cid:104)unk(cid:105) missile family
already british aerospace and french (cid:104)unk(cid:105) (cid:104)unk(cid:105) (cid:104)unk(cid:105) on a british missile contract and on an air-trafﬁc control radar system

meanwhile during the the s&p trading halt s&p futures sell orders began (cid:104)unk(cid:105) up while stocks in new york kept falling sharply
but the (cid:104)unk(cid:105) of s&p futures sell orders weighed on the market and the link with stocks began to fray again
on friday some market makers were selling again traders said
futures traders say the s&p was (cid:104)unk(cid:105) that the dow could fall as much as N points
meanwhile two initial public offerings (cid:104)unk(cid:105) the (cid:104)unk(cid:105) market in their (cid:104)unk(cid:105) day of national over-the-counter trading friday
traders said most of their major institutional investors on the other hand sat tight

Table 4: For each query sentence (bold), we show the 5 nearest neighbors based on cosine similarity, where we take the
representation for each sentence to be the mean of the variational posterior.

Additionally, Kim et al. (2019) report per-
plexity improvements by ﬁne-tuning an RNNG
trained on gold trees with the unsupervised RNNG
(URNNG)—whereas the RNNG is is trained
log likelihood log p(t),
to maximize the joint
the URNNG maximizes a lower bound on the
log marginal likelihood log (cid:80)
t∈TG (x) p(t) with
a structured inference network that approximates
the true posterior. We experiment with a similar
approach where we ﬁne-tune RNNGs trained on
induced trees with URNNGs. We perform early
stopping for both RNNG and URNNG based on
validation perplexity. See appendix A.3 for the full
experimental setup.

The results are shown in Table 3. For perplexity,
RNNGs trained on induced trees (Induced RNNG
in Table 3) are unable to improve upon an LSTM
LM, in contrast to the supervised RNNG which
does outperform the LSTM language model (Ta-
ble 3, bottom). For grammaticality judgment how-
ever, the RNNG trained with compound PCFG
trees outperforms the LSTM LM despite obtain-
ing worse perplexity,18 and performs on par with
the RNNG trained on binarized gold trees. Fine-
tuning with the URNNG results in improvements
in perplexity and grammaticality judgment across
the board (Induced URNNG in Table 3). We also
obtain large improvements on unsupervised pars-
ing as measured by F1, with the ﬁne-tuned URN-
NGs outperforming the respective original mod-
els.19 This is potentially due to an ensembling ef-
fect between the original model and the URNNG’s
structured inference network, which is parameter-

18Kuncoro et al. (2018, 2019) also observe that models that
achieve lower perplexity do not necessarily perform better on
syntactic evaluation tasks.

19Li et al. (2019) similarly obtain improvements by reﬁn-
ing a model trained on induced trees on classiﬁcation tasks.

ized as a neural CRF constituency parser (Durrett
and Klein, 2015; Liu et al., 2018).20

5.2 Model Analysis

We analyze our best compound PCFG model in
more detail. Since we induce a full set of nonter-
minals in our grammar, we can analyze the learned
nonterminals to see if they can be aligned with lin-
guistic constituent labels. Figure 2 visualizes the
alignment between induced and gold labels, where
for each nonterminal we show the empirical prob-
ability that a predicted constituent of this type will
correspond to a particular linguistic constituent in
the test set, conditioned on its being a correct con-
stituent (for reference we also show the precision).
We observe that some of the induced nonterminals
clearly align to linguistic nonterminals. Further re-
sults, including preterminal alignments to part-of-
speech tags,21 are shown in appendix A.4.

We next analyze the continuous latent space.
Table 4 shows nearest neighbors of some sen-
tences using the mean of the variational poste-
rior as the continuous representation of each sen-
tence. We qualitatively observe that the latent
space seems to capture topical information.

20While left as future work, it is possible to use the com-
pound PCFG itself as an inference network. Also note that
the F1 scores for the URNNGs in Table 3 are optimistic since
we selected the best-performing runs of the original models
based on validation F1 to parse the training set. Finally, as
noted by Kim et al. (2019), a URNNG trained from scratch
fails to outperform a right-branching baseline on this version
of PTB where punctuation is removed.

21As a POS induction system, the many-to-one perfor-
mance of the compound PCFG using the preterminals is 68.0.
A similarly-parameterized compound HMM with 60 hidden
states (an HMM is a particularly type of PCFG) obtains 63.2.
This is still quite a bit lower than the state-of-the-art (Tran
et al., 2016; He et al., 2018; Stratos, 2019), though compar-
ison is confounded by various factors such as preprocessing.
A neural PCFG/HMM obtains 68.2 and 63.4 respectively.

T-13

w1

NT-04

NT-12

NT-20

T-22

NT-20

T-40

w6

NT-07

T-35

w5

T-05

T-45

w4

w2

w3

NT-10

T-55

NT-05

w1

T-02

NT-19

PC -
of the company ’s capital structure
in the company ’s divestiture program
by the company ’s new board
in the company ’s core businesses
on the company ’s strategic plan

PC +
above the treasury ’s N-year note
above the treasury ’s seven-year note
above the treasury ’s comparable note
above the treasury ’s ﬁve-year note
measured the earth ’s ozone layer

PC -
to terminate their contract with warner
to support a coup in panama
to suit the bureaucrats in brussels
to thwart his bid for amr
to prevent the pound from rising

T-02

w1

w2

NT-06

NT-04

T-05

T-41

T-13

T-43

w3

w4

w5

w6

PC +
to change our strategy of investing
to offset the growth of minimills
to be a lot of art
to change our way of life
to increase the impact of advertising

NT-23

T-58

NT-04

w1

T-13

NT-12

w2

NT-06

NT-04

T-05

T-41

T-13

NT-12

w3

w4

w5

T-60

T-21

w6

w7

NT-05

NT-19

NT-06

NT-04

NT-20

T-22

T-13

NT-12

T-05

T-40

w4

w5

T-60

T-21

w2

w3

w6

w7

PC -
purchased through the exercise of stock options
circulated by a handful of major brokers
higher as a percentage of total loans
common with a lot of large companies
surprised by the storm of sell orders

PC +
brought to the u.s. against her will
laid for the arrest of opposition activists
uncertain about the magnitude of structural damage
held after the assassination of his mother
hurt as a result of the violations

PC -
raise the minimum grant for smaller states
veto a defense bill with inadequate funding
avoid an imminent public or private injury
ﬁeld a competitive slate of congressional candidates
alter a longstanding ban on such involvement

PC +
generate an offsetting proﬁt by selling waves
change an export loss to domestic plus
expect any immediate problems with margin calls
make a positive contribution to our earnings
ﬁnd a trading focus discouraging much participation

Table 5: For each subtree, we perform PCA on the variational posterior mean vectors that are associated with that particular
subtree and take the top principal component. We then list the top 5 constituents that had the lowest (PC -) and highest (PC +)
principal component values.

j

), where t(n)

We are also interested in the variation in the
leaves due to z when the variation due to the tree
structure is held constant. To investigate this, we
use the parsed dataset to obtain pairs of the form
(µφ(x(n)), t(n)
is the j-th subtree of
j
the (approximate) MAP tree t(n) for the n-th sen-
tence. Therefore each mean vector µφ(x(n)) is
associated with |x(n)| − 1 subtrees, where |x(n)|
is the sentence length. Our deﬁnition of subtree
here ignores terminals, and thus each subtree is
associated with many mean vectors. For a fre-
quently occurring subtree, we perform PCA on
the set of mean vectors that are associated with
the subtree to obtain the top principal compo-
nent. We then show the constituents that had the
5 most positive/negative values for this top prin-
cipal component in Table 5. For example, a par-
ticularly common subtree—associated with 180
unique constituents—is given by

(NT-04 (T-13 w1) (NT-12 (NT-20 (NT-20 (NT-07 (T-05 w2)

(T-45 w3)) (T-35 w4)) (T-40 w5)) (T-22 w6))).

The top 5 constituents with the most nega-
tive/positive values are shown in the top left part
of Table 5. We ﬁnd that the leaves [w1, . . . , w6],
which form a 6-word constituent, vary in a regu-
lar manner as z is varied. We also observe that
root of this subtree (NT-04) aligns to prepositional
phrases (PP) in Figure 2, and the leaves in Ta-
ble 5 (top left) are indeed mostly PP. However, the
model fails to identify ((T-40 w5) (T-22 w6)) as a con-
stituent in this case (as well as well in the bottom
right example). See appendix A.5 for more exam-

ples. It is possible that the model is utilizing the
subtrees to capture broad template-like structures
and then using z to ﬁll them in, similar to recent
works that also train models to separate “what to
say” from “how to say it” (Wiseman et al., 2018;
Peng et al., 2019; Chen et al., 2019a,b).

5.3 Limitations

We report on some negative results as well as im-
portant limitations of our work. While distributed
representations promote parameter sharing, we
were unable to obtain improvements through more
factorized parameterizations that promote even
greater parameter sharing. In particular, for rules
of the type A → BC, we tried having the out-
put embeddings be a function of the input embed-
dings (e.g. uBC = g([wB; wC]) where g is an
MLP), but obtained worse results. For rules of
the type T → w, we tried using a character-level
CNN (dos Santos and Zadrozny, 2014; Kim et al.,
2016) to obtain the output word embeddings uw
(Jozefowicz et al., 2016; Tran et al., 2016), but
found the performance to be similar to the word-
level case.22 We were also unable to obtain im-
provements by making the variational family more
ﬂexible through normalizing ﬂows (Rezende and
Mohamed, 2015; Kingma et al., 2016). How-
ever, given that we did not exhaustively explore
the full space of possible parameterizations, the
above modiﬁcations could eventually lead to im-

22It is also possible to take advantage of pretrained word
embeddings by using them to initialize output word embed-
dings or directly working with continuous emission distribu-
tions (Lin et al., 2015; He et al., 2018)

provements with the right setup.

Relatedly, the models were quite sensitive to pa-
rameterization (e.g. it was important to use resid-
ual layers for f1, f2), grammar size, and optimiza-
tion method. We also noticed some variance in
results across random seeds, as shown in Table 2.
Finally, despite vectorized GPU implementations,
training was signiﬁcantly more expensive (both in
terms of time and memory) than NLM-based un-
supervised parsing systems due to the O(|R||x|3)
dynamic program, which makes our approach po-
tentially difﬁcult to scale.

6 Related Work

Grammar induction and unsupervised parsing has
a long and rich history in natural language pro-
cessing.
Early work on with pure unsuper-
vised learning was mostly negative (Lari and
Young, 1990; Carroll and Charniak, 1992; Char-
niak, 1993), though Pereira and Schabes (1992)
reported some success on partially bracketed data.
Clark (2001) and Klein and Manning (2002)
were some of the ﬁrst successful statistical ap-
proaches.
In particular, the constituent-context
model (CCM) of Klein and Manning (2002),
which explicitly models both constituents and dis-
tituents, was the basis for much subsequent work
(Klein and Manning, 2004; Huang et al., 2012;
Golland et al., 2012). Other works have explored
imposing inductive biases through Bayesian pri-
ors (Johnson et al., 2007; Liang et al., 2007; Wang
and Blunsom, 2013), modiﬁed objectives (Smith
and Eisner, 2004), and additional constraints on
recursion depth (Noji et al., 2016; Jin et al., 2018).
While the framework of specifying the struc-
ture of a grammar and learning the parameters is
common, other methods exist. Bod (2006) con-
sider a nonparametric-style approach to unsuper-
vised parsing by using random subsets of training
subtrees to parse new sentences. Seginer (2007)
utilize an incremental algorithm to unsupervised
parsing which makes local decisions to create con-
stituents based on a complex set of heuristics.
Ponvert et al. (2011) induce parse trees through
cascaded applications of ﬁnite state models.

More

recently,

neural network-based ap-
proaches have shown promising results on
inducing parse trees directly from words. Shen
et al. (2018, 2019) learn tree structures through
soft gating layers within neural language models,
while Drozdov et al. (2019) combine recursive

autoencoders with the inside-outside algorithm.
Kim et al. (2019) train unsupervised recurrent
neural network grammars with a structured
inference network to induce latent trees, and Shi
et al. (2019) utilize image captions to identify and
ground constituents.

Our work is also related to latent variable
PCFGs (Matsuzaki et al., 2005; Petrov et al., 2006;
Cohen et al., 2012), which extend PCFGs to the la-
tent variable setting by splitting nonterminal sym-
bols into latent subsymbols. In particular, latent
vector grammars (Zhao et al., 2018) and composi-
tional vector grammars (Socher et al., 2013) also
employ continuous vectors within their grammars.
However these approaches have been employed
for learning supervised parsers on annotated tree-
banks, in contrast to the unsupervised setting of
the current work.

7 Conclusion

This work studies a neural network-based ap-
proach grammar induction with PCFGs. We ﬁrst
propose to parameterize a PCFG’s rule probabil-
ities with neural networks over distributed rep-
resentations of latent symbols, and ﬁnd that this
neural PCFG makes it possible to induce linguis-
tically meaningful grammars with simple maxi-
mum likelihood learning. We then extend the
neural PCFG through a sentence-level continu-
ous latent vector, which induces marginal depen-
dencies beyond the traditional ﬁrst-order context-
free assumptions. We show that this compound
PCFG learns richer grammars and leads to im-
proved performance when evaluated as an unsu-
pervised parser. The collapsed amortized varia-
tional inference approach is general and can be
used for generative models which admit tractable
inference through partial conditioning. Learning
deep generative models which exhibit such condi-
tional Markov properties is an interesting direction
for future work.

Acknowledgments

We thank Phil Blunsom for initial discussions
which seeded many of the core ideas in the present
work. We also thank Yonatan Belinkov and Shay
Cohen for helpful feedback, and Andrew Drozdov
for providing the parsed dataset from their DIORA
model. YK is supported by a Google Fellowship.
AMR acknowledges the support of NSF 1704834,
1845664, AWS, and Oracle.

References

Alfred Aho. 1968.

Indexed Grammars—An Extension of
Context-Free Grammars. Journal of the ACM, 15(4):647–
671.

Sanjeev Arora, Nadav Cohen, and Elad Hazan. 2018. On the
Optimization of Deep Networks: Implicit Acceleration by
Overparameterization. In Proceedings of ICML.

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.
2016. Layer Normalization. In Proceedings of NIPS.

James K. Baker. 1979. Trainable Grammars for Speech
Recognition. In Proceedings of the Spring Conference of
the Acoustical Society of America.

Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin.
1977. Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical Society,
Series B, 39(1):1–38.

Andrew Drozdov, Patrick Verga, Mohit Yadev, Mohit Iyyer,
and Andrew McCallum. 2019. Unsupervised Latent
Tree Induction with Deep Inside-Outside Recursive Auto-
Encoders. In Proceedings of NAACL.

Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti
Singh. 2019. Gradient Descent Provably Optimizes Over-
parameterized Neural Networks. In Proceedings of ICLR.

Greg Durrett and Dan Klein. 2015. Neural CRF Parsing. In

Proceedings of ACL.

Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Ja-
son Weston. 2009. Curriculum Learning. In Proceedings
of ICML.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and
Noah A. Smith. 2016. Recurrent Neural Network Gram-
mars. In Proceedings of NAACL.

Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cote, John
DeNero, and Dan Klein. 2010. Painless Unsupervised
Learning with Features. In Proceedings of NAACL.

James O. Berger. 1985.

Statistical Decision Theory and

Bayesian Analysis. Springer.

Rens Bod. 2006. An All-Subtrees Approach to Unsupervised

Parsing. In Proceedings of ACL.

Glenn Carroll and Eugene Charniak. 1992. Two Experi-
ments on Learning Probabilistic Dependency Grammars
from Corpora. In AAAI Workshop on Statistically-Based
NLP Techniques.

Eugene Charniak. 1993.

Statistical Language Learning.

MIT Press.

Danqi Chen and Christopher D. Manning. 2014. A Fast and
Accurate Dependency Parser using Neural Networks. In
Proceedings of EMNLP.

Mingda Chen, Qingming Tang, Sam Wiseman, and Kevin
Gimpel. 2019a. Controllable Paraphrase Generation with
a Syntactic Exemplar. In Proceedings of ACL.

Mingda Chen, Qingming Tang, Sam Wiseman, and Kevin
Gimpel. 2019b. A Multi-task Approach for Disentangling
Syntax and Semantics in Sentence Sepresentations.
In
Proceedings of NAACL.

Alexander Clark. 2001. Unsupervised Induction of Stochas-
tic Context Free Grammars Using Distributional Cluster-
ing. In Proceedings of CoNLL.

Shay B. Cohen. 2016. Bayesian Analysis in Natural Lan-

guage Processing. Morgan and Claypool.

Shay B. Cohen, Kevin Gimpel, and Noah A Smith. 2009. Lo-
gistic Normal Priors for Unsupervised Probabilistic Gram-
mar Induction. In Proceedings of NIPS.

Shay B. Cohen and Noah A Smith. 2009. Shared Logistic
Normal Distributions for Soft Parameter Tying in Unsu-
pervised Grammar Induction. In Proceedings of NAACL.

Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Fos-
ter, and Lyle Ungar. 2012. Spectral Learning of Latent-
Variable PCFGs. In Proceedings of ACL.

Michael Collins. 1997. Three Generative, Lexicalised Mod-

els for Statistical Parsing. In Proceedings of ACL.

Jason Eisner. 2016. Inside-Outside and Forward-Backward
Algorithms Are Just Backprop (Tutorial Paper). In Pro-
ceedings of the Workshop on Structured Prediction for
NLP.

Dave Golland, John DeNero, and Jakob Uszkoreit. 2012. A
Feature-Rich Constituent Context Model for Grammar In-
duction. In Proceedings of ACL.

Junxian He, Graham Neubig, and Taylor Berg-Kirkpatrick.
2018. Unsupervised Learning of Syntactic Structure with
Invertible Neural Projections. In Proceedings of EMNLP.

Phu Mon Htut, Kyunghyun Cho, and Samuel R. Bowman.
2018. Grammar Induction with Neural Language Models:
An Unusual Replication. In Proceedings of EMNLP.

Yun Huang, Min Zhang, and Chew Lim Tan. 2012. Improved
Constituent Context Model with Features. In Proceedings
of PACLIC.

Lifeng Jin, Finale Doshi-Velez, Timothy Miller, William
Schuler, and Lane Schwartz. 2018. Unsupervised Gram-
In Proceed-
mar Induction with Depth-bounded PCFG.
ings of TACL.

Mark Johnson. 1998. PCFG Models of Linguistic Tree Rep-
resentations. Computational Linguistics, 24:613–632.

Mark Johnson, Thomas L. Grifﬁths, and Sharon Goldwater.
2007. Bayesian Inference for PCFGs via Markov chain
Monte Carlo. In Proceedings of NAACL.

Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring the Limits
of Language Modeling. arXiv:1602.02410.

Yoon Kim, Yacine Jernite, David Sontag, and Alexander M.
Rush. 2016. Character-Aware Neural Language Models.
In Proceedings of AAAI.

Yoon Kim, Alexander M. Rush, Lei Yu, Adhiguna Kuncoro,
Chris Dyer, and G´abor Melis. 2019. Unsupervised Re-
In Proceedings of
current Neural Network Grammars.
NAACL.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method
for Stochastic Optimization. In Proceedings of ICLR.

Diederik P. Kingma, Tim Salimans, and Max Welling.
2016. Improving Variational Inference with Autoregres-
sive Flow. arXiv:1606.04934.

Diederik P. Kingma and Max Welling. 2014. Auto-Encoding

Variational Bayes. In Proceedings of ICLR.

Nikita Kitaev and Dan Klein. 2018. Constituency Parsing
with a Self-Attentive Encoder. In Proceedings of ACL.

Dan Klein and Christopher Manning. 2002. A Generative
Constituent-Context Model for Improved Grammar Induc-
tion. In Proceedings of ACL.

Dan Klein and Christopher Manning. 2004. Corpus-based
Induction of Syntactic Structure: Models of Dependency
and Constituency. In Proceedings of ACL.

Dan Klein and Christopher D. Manning. 2003. Accurate Un-

lexicalized Parsing. In Proceedings of ACL.

Adhiguna Kuncoro, Chris Dyer, John Hale, Dani Yogatama,
Stephen Clark, and Phil Blunsom. 2018. LSTMs Can
Learn Syntax-Sensitive Dependencies Well, But Model-
ing Structure Makes Them Better. In Proceedings of ACL.

Hiroshi Noji, Yusuke Miyao, and Mark Johnson. 2016. Us-
ing Left-corner Parsing to Encode Universal Structural
In Proceedings of
Constraints in Grammar Induction.
EMNLP.

Ankur P. Parikh, Shay B. Cohen, and Eric P. Xing. 2014.
Spectral Unsupervised Parsing with Additive Tree Met-
rics. In Proceedings of ACL.

Hao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan Dhin-
gra, and Dipanjan Das. 2019. Text Generation with
In Proceedings of
Exemplar-based Adaptive Decoding.
NAACL.

Fernando Pereira and Yves Schabes. 1992.

Inside-Outside
Reestimation from Partially Bracketed Corpora. In Pro-
ceedings of ACL.

Slav Petrov, Leon Barret, Romain Thibaux, and Dan Klein.
2006. Learning Accurate, Compact, and Interpretable
Tree Annotation. In Proceedings of ACL.

Adhiguna Kuncoro, Chris Dyer, Laura Rimell, Stephen
Clark, and Phil Blunsom. 2019. Scalable Syntax-Aware
Language Models Using Knowledge Distillation. In Pro-
ceedings of ACL.

Elis Ponvert, Jason Baldridge, and Katrin Erk. 2011. Sim-
pled Unsupervised Grammar Induction from Raw Text
In Proceedings of
with Cascaded Finite State Methods.
ACL.

Kenichi Kurihara and Taisuke Sato. 2006.

Variational
Bayesian Grammar Induction for Natural Language.
In
Proceedings of International Colloquium on Grammatical
Inference.

Karim Lari and Steve Young. 1990. The Estimation of
Stochastic Context-Free Grammars Using the Inside-
Outside Algorithm. Computer Speech and Language,
4:35–56.

Bowen Li, Lili Mou, and Frank Keller. 2019. An Imitation
Learning Approach to Unsupervised Parsing. In Proceed-
ings of ACL.

Percy Liang and Dan Klein. 2009. Online EM for Unsuper-

vised models. In Proceedings of NAACL.

Percy Liang, Slav Petrov, Michael I. Jordan, and Dan Klein.
2007. The Inﬁnite PCFG using Hierarchical Dirichlet Pro-
cesses. In Proceedings of EMNLP.

Chu-Cheng Lin, Waleed Ammar, Chris Dyer,

, and Lori
Levin. 2015. Unsupervised POS Induction with Word
Embeddings. In Proceedings of NAACL.

Yang Liu, Matt Gardner, and Mirella Lapata. 2018. Struc-
In
tured Alignment Networks for Matching Sentences.
Proceedings of EMNLP.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice
Santorini. 1993. Building a Large Annotated Corpus of
English: The Penn Treebank. Computational Linguistics,
19:313–330.

Rebecca Marvin and Tal Linzen. 2018. Targeted Syntac-
In Proceedings of

tic Evaluation of Language Models.
EMNLP.

Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii. 2005.
Probabilistic CFG with Latent Annotations. In Proceed-
ings of ACL.

Bernard Merialdo. 1994. Tagging English Text with a Prob-
abilistic Model. Computational Linguistics, 20(2):155–
171.

Oﬁr Press and Lior Wolf. 2016. Using the Output Embedding
to Improve Language Models. In Proceedings of EACL.

Roi Reichart and Ari Rappoport. 2010. Improved Fully Un-
In Proceed-

supervised Parsing with Zoomed Learning.
ings of EMNLP.

Danilo J. Rezende and Shakir Mohamed. 2015. Variational
In Proceedings of

Inference with Normalizing Flows.
ICML.

Danilo J. Rezende, Shakir Mohamed, and Daan Wierstra.
2014. Stochastic Backpropagation and Approximate In-
In Proceedings of
ference in Deep Generative Models.
ICML.

Herbert Robbins. 1951. Asymptotically Subminimax Solu-
tions of Compound Statistical Decision Problems. In Pro-
ceedings of the Second Berkeley Symposium on Mathemat-
ical Statistics and Probability, pages 131–149. Berkeley:
University of California Press.

Herbert Robbins. 1956. An Empirical Bayes Approach to
In Proceedings of the Third Berkeley Sympo-
Statistics.
sium on Mathematical Statistics and Probability, pages
157–163. Berkeley: University of California Press.

Ruslan Salakhutdinov, Sam Roweis, and Zoubin Ghahra-
mani. 2003. Optimization with EM and Expectation-
Conjugate-Gradient. In Proceedings of ICML.

C´ıcero Nogueira dos Santos and Bianca Zadrozny. 2014.
Learning Character-level Representations for Part-of-
Speech Tagging. In Proceedings of ICML.

Yoav Seginer. 2007. Fast Unsupervised Incremental Parsing.

In Proceedings of ACL.

Yikang Shen, Zhouhan Lin, Chin-Wei Huang, and Aaron
Courville. 2018. Neural Language Modeling by Jointly
Learning Syntax and Lexicon. In Proceedings of ICLR.

Yikang Shen, Shawn Tan, Alessandro Sordoni, and Aaron
Courville. 2019. Ordered Neurons:
Integrating Tree
Structures into Recurrent Neural Networks. In Proceed-
ings of ICLR.

Yanpeng Zhao, Liwen Zhang, and Kewei Tu. 2018. Gaussian
Mixture Latent Vector Grammars. In Proceedings of ACL.

Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. 2015.
Long Short-Term Memory Over Tree Structures. In Pro-
ceedings of ICML.

Haoyue Shi, Jiayuan Mao, Kevin Gimpel, and Karen Livescu.
2019. Visually Grounded Neural Syntax Acquisition. In
Proceedings of ACL.

Noah A. Smith and Jason Eisner. 2004. Annealing Tech-
niques for Unsupervised Statistical Language Learning.
In Proceedings of ACL.

Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised Multilingual Grammar Induction. In
Proceedings of ACL.

Richard Socher, John Bauer, Christopher D. Manning, and
Andrew Y. Ng. 2013. Parsing with Compositional Vector
Grammars. In Proceedings of ACL.

Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2012. Three Dependency-and-Boundary Models for
Grammar Induction. In Proceedings of EMNLP-CoNLL.

Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky.
2013. Breaking Out of Local Optima with Count Trans-
forms and Model Recombination: A Study in Grammar
Induction. In Proceedings of EMNLP.

Mitchell Stern, Jacob Andreas, and Dan Klein. 2017. A Min-
imal Span-Based Neural Constituency Parser. In Proceed-
ings of ACL.

Karl Stratos. 2019. Mutual Information Maximization for
In Pro-

Simple and Accurate Part-of-Speech Induction.
ceedings of NAACL.

Kai Sheng Tai, Richard Socher, and Christopher D. Manning.
2015.
Improved Semantic Representations From Tree-
Structured Long Short-Term Memory Networks. In Pro-
ceedings of ACL.

Ke Tran, Yonatan Bisk, Ashish Vaswani, Daniel Marcu,
and Kevin Knight. 2016. Unsupervised Neural Hidden
In Proceedings of the Workshop on
Markov Models.
Structured Prediction for NLP.

Pengyu Wang and Phil Blunsom. 2013. Collapsed Varia-
tional Bayesian Inference for PCFGs. In Proceedings of
CoNLL.

Wenhui Wang and Baobao Chang. 2016. Graph-based De-
pendency Parsing with Bidirectional LSTM. In Proceed-
ings of ACL.

Ethan Wilcox, Peng Qian, Richard Futrell, Miguel Balles-
teros, and Roger Levy. 2019. Structural Supervision Im-
proves Learning of Non-Local Grammatical Dependen-
cies. In Proceedings of NAACL.

Sam Wiseman, Stuart M. Shieber, and Alexander M. Rush.
2018. Learning Neural Templates for Text Generation. In
Proceedings of EMNLP.

Ji Xu, Daniel Hsu, and Arian Maleki. 2018. Beneﬁts of Over-
Parameterization with EM. In Proceedings of NeurIPS.

Naiwen Xue, Fei Xia, Fu dong Chiou, and Marta Palmer.
2005. The Penn Chinese Treebank: Phrase Structure An-
notation of a Large Corpus. Natural Language Engineer-
ing, 11:207–238.

Cun-Hui Zhang. 2003. Compound Decision Theory and Em-
pirical Bayes Methods. The Annals of Statistics, 31:379–
390.

A Appendix

A.1 Model Parameterization

We associate an input embedding wN for each
symbol N on the left side of a rule (i.e. N ∈
{S} ∪ N ∪ P) and run a neural network over wN
to obtain the rule probabilities. Concretely, each
rule type πr is parameterized as follows,

πS→A =

(cid:80)

πA→BC =

(cid:80)

πT →w =

(cid:80)

exp(u(cid:62)
A(cid:48)∈N exp(u(cid:62)
exp(u(cid:62)
B(cid:48)C(cid:48)∈M exp(u(cid:62)

exp(u(cid:62)
w(cid:48)∈Σ exp(u(cid:62)

A f1(wS) + bA)

A(cid:48) f1(wS) + bA(cid:48))

BC wA + bBC)

B(cid:48)C(cid:48) wA + bB(cid:48)C(cid:48))

,

w f2(wT ) + bw)

w(cid:48) f2(wT ) + bw(cid:48))

,

,

where M is the product space (N ∪P)×(N ∪P),
and f1, f2 are MLPs with two residual layers,

fi(x) =gi,1(gi,2(Wix + bi)),

gi,j(y) = ReLU(Vi,j ReLU(Ui,jy + pi,j)+

qi,j) + y.

In the compound PCFG the rule probabilities

πz given a latent vector z,

πz,S→A =

(cid:80)

πz,A→BC =

(cid:80)

πz,T →w =

(cid:80)

exp(u(cid:62)
A(cid:48)∈N exp(u(cid:62)
exp(u(cid:62)
B(cid:48)C(cid:48)∈M exp(u(cid:62)

exp(u(cid:62)
w(cid:48)∈Σ exp(u(cid:62)

A f1([wS; z]) + bA)

A(cid:48) f1([wS; z]) + bA(cid:48))

BC [wA; z] + bBC)

B(cid:48)C(cid:48) [wA; z] + bB(cid:48)C(cid:48))

w f2([wT ; z]) + bw)

w(cid:48) f2([wT ; z]) + bw(cid:48))

,

.

Again f1, f2 are as before where the ﬁrst layer’s
input dimensions are appropriately changed to ac-
count for concatenation with z.

A.2 Corpus/Sentence F1 by Sentence Length
For completeness we show the corpus-level and
sentence-level F1 broken down by sentence length
in Table 6, averaged across 4 different runs of each
model. In Figure 1 we use the following to refer
to rule probabilities of different rule types for the
neural PCFG (left),

πS = {πr | r ∈ L(S)},
πN = {πr | r ∈ L(A), A ∈ N },
πP = {πr | r ∈ L(T ), T ∈ P},
π = πS ∪ πN ∪ πP ,

where L(A) denotes the set of rules with A on the
left hand side. The set of rule probabilities for the
compound PCFG (right) is similarly deﬁned,

πz,S = {πz,r | r ∈ L(S)},
πz,N = {πz,r | r ∈ L(A), A ∈ N },
πz,P = {πz,r | r ∈ L(T ), T ∈ P},

πz = πz,S ∪ πz,N ∪ πz,P .

,

A.3 Experiments with RNNGs

For experiments on supervising RNNGs with in-
duced trees, we use the parameterization and hy-
perparameters from Kim et al. (2019), which
uses a 2-layer 650-dimensional stack LSTM (with
dropout of 0.5) and a 650-dimensional tree LSTM
(Tai et al., 2015; Zhu et al., 2015) as the composi-
tion function.

Concretely, the generative story is as follows:
ﬁrst, the stack representation is used to predict the
next action (SHIFT or REDUCE) via an afﬁne trans-
formation followed by a sigmoid. If SHIFT is cho-
sen, we obtain a distribution over the vocabulary
via another afﬁne transformation over the stack
representation followed by a softmax. Then we
sample the next word from this distribution and
shift the generated word onto the stack using the
stack LSTM. If REDUCE is chosen, we pop the last
two elements off the stack and use the tree LSTM
to obtain a new representation. This new repre-
sentation is shifted onto the stack via the stack
LSTM. Note that this RNNG parameterization is
slightly different than the original from Dyer et al.
(2016), which does not ignore constituent labels
and utilizes a bidirectional LSTM as the compo-
sition function instead of a tree LSTM. As our
RNNG parameterization only works with binary
trees, we binarize the gold trees with right bina-
rization for the RNNG trained on gold trees (trees
from the unsupervised methods explored in this
paper are already binary). The RNNG also trains
a discriminative parser alongside the generative
model for evaluation with importance sampling.
We use a CRF parser whose span score parame-
terization is similar similar to recent works (Wang
and Chang, 2016; Stern et al., 2017; Kitaev and
Klein, 2018): position embeddings are added to
word embeddings, and a bidirectional LSTM with
256 hidden dimensions is run over the input rep-
resentations to obtain the forward and backward
hidden states. The score sij ∈ R for a constituent

Sentence-level F1
WSJ-10 WSJ-20 WSJ-30 WSJ-40 WSJ-Full

Left Branching
Right Branching
Random Trees
PRPN (tuned)
ON (tuned)
Neural PCFG
Compound PCFG

Oracle

17.4
58.5
31.8
58.4
63.9
64.6
70.5

82.1

12.9
49.8
25.2
54.3
57.5
58.1
63.4

84.1

9.9
44.4
21.5
50.9
53.2
54.6
58.9

84.2

8.6
41.6
19.7
48.5
50.5
52.6
56.6

84.3

8.7
39.5
19.2
47.3
48.1
50.8
55.2

84.3

Corpus-level F1
WSJ-10 WSJ-20 WSJ-30 WSJ-40 WSJ-Full

Left Branching
Right Branching
Random Trees
PRPN (tuned)
ON (tuned)
Neural PCFG
Compound PCFG

Oracle

16.5
58.9
31.9
59.3
64.7
63.5
70.6

83.5

11.7
48.3
23.9
53.6
56.3
56.8
62.0

85.2

8.5
42.5
20.0
49.7
51.5
53.1
57.1

84.9

7.2
39.4
18.1
46.9
48.3
51.0
54.6

84.9

6.0
36.1
16.4
44.5
45.6
48.7
52.4

84.7

Table 6: Average unlabeled F1 for the various models broken down by sentence length on the PTB test set. For example
WSJ-10 refers to F1 calculated on the subset of the test set where the maximum sentence length is at most 10. Scores are
averaged across 4 runs of the model with different random seeds. Oracle is the performance of binarized gold trees (with right
branching binarization). Top shows sentence-level F1 and bottom shows corpus-level F1.

Wolf, 2016). Perplexity estimation for the RNNGs
and the compound PCFG uses 1000 importance-
weighted samples.

For grammaticality judgment, we modify the
publicly available dataset from Marvin and Linzen
(2018)24 to only keep sentence pairs that did not
have any unknown words with respect to our PTB
vocabulary of 10K words. This results in 33K sen-
tence pairs for evaluation.

A.4 Nonterminal/Preterminal Alignments

Figure 3 shows the part-of-speech alignments and
Table 7 shows the nonterminal label alignments
for the compound PCFG/neural PCFG.

A.5 Subtree Analysis

Table 8 lists more examples of constituents within
each subtree as the top principical component is
varied. Due to data sparsity, the subtree analysis is
performed on the full dataset. See section 5.2 for
more details.

spanning the i-th and j-th word is given by,
←−
h j]),

−→
h j+1 −

←−
h i−1 −

sij = MLP([

−→
h i;

where the MLP has a single hidden layer with
ReLU nonlinearity followed by layer normaliza-
tion (Ba et al., 2016).

For experiments on ﬁne-tuning the RNNG with
the unsupervised RNNG, we take the discrimina-
tive parser (which is also pretrained alongside the
RNNG on induced trees) to be the structured in-
ference network for optimizing the evidence lower
bound. We refer the reader to Kim et al. (2019)
and their open source implementation23 for addi-
tional details. We also observe that as noted by
Kim et al. (2019), a URNNG trained from scratch
on this version of PTB without punctuation failed
to outperform a right-branching baseline.

The LSTM language model baseline is the same
size as the stack LSTM (i.e. 2 layers, 650 hid-
den units, dropout of 0.5), and is therefore equiv-
alent to an RNNG with completely right branch-
ing trees. The PRPN/ON baselines for perplex-
ity/syntactic evaluation in Table 3 also have 2
layers with 650 hidden units and 0.5 dropout.
Therefore all models considered in Table 3 have
roughly the same capacity. For all models we
share input/output word embeddings (Press and

23https://github.com/harvardnlp/urnng

24https://github.com/BeckyMarvin/LM syneval

Figure 3: Preterminal alignment to part-of-speech tags for the compound PCFG (top) and the neural PCFG (bottom).

Label

NT-01
NT-02
NT-03
NT-04
NT-05
NT-06
NT-07
NT-08
NT-09
NT-10
NT-11
NT-12
NT-13
NT-14
NT-15
NT-16
NT-17
NT-18
NT-19
NT-20
NT-21
NT-22
NT-23
NT-24
NT-25
NT-26
NT-27
NT-28
NT-29
NT-30

NT-01
NT-02
NT-03
NT-04
NT-05
NT-06
NT-07
NT-08
NT-09
NT-10
NT-11
NT-12
NT-13
NT-14
NT-15
NT-16
NT-17
NT-18
NT-19
NT-20
NT-21
NT-22
NT-23
NT-24
NT-25
NT-26
NT-27
NT-28
NT-29
NT-30

S

SBAR

NP

VP

PP

ADJP ADVP

Other

Freq.

Acc.

0.0%
0.0%
0.9%
2.2%
0.0%
1.0%
2.2%
0.3%
0.0%
0.2%
0.0%
0.0%
0.0%
0.0%
2.2%
0.5%
5.6%
6.3%
0.1%
0.1%
0.0%
0.9%
0.2%
0.5%
0.1%
1.6%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
2.9%
96.4%
0.0%
0.3%
1.0%
3.9%
0.0%
0.0%
1.7%
94.4%
0.0%
0.1%
0.9%
0.4%
0.2%
0.0%
0.3%
0.0%
0.4% 60.7%
0.0%
0.0%
0.3%
88.2%
1.7%
0.0%
1.6% 94.5%

81.8%
90.8%
2.3%
0.5%
36.4%
99.1%
99.7%
23.3%
40.2%
1.4%
96.5%
94.4%
0.2%
0.0%
99.7%

5.9%
0.0%
1.1%
0.0%
0.9%
1.7%
0.0%
96.8%
0.0%
0.2%
2.0% 93.9%
0.0%
0.0%
56.9%
0.0%
0.1%
0.0%
0.0%
0.0%
0.3%
35.6% 11.3% 23.6%
1.2%
4.3% 32.6%
0.0%
58.8% 38.6%
0.0%
0.9%
0.9%
0.1%
0.2%
2.4%
0.1%
0.2%
97.7%
0.0%
0.0%
98.6%
0.0%
0.3%
0.0%
0.0%
0.0%
0.0% 100.0%
0.0%
0.0%
0.7%
0.0%
0.0%
0.3%
2.8%
88.7%
0.4%
2.6%
2.4%
86.6%
0.3%
0.0%
0.0%
99.0%
0.1%
0.3%
1.4%
2.0%
0.1%
1.1%
98.4%
0.0%
8.2% 18.5%
53.1%
14.0%
0.0%
0.0%
98.3%
1.5%
0.0%
98.3%
1.4%
0.0%
3.0% 15.4%
18.4%
0.4%
0.7% 13.1%
0.5%
48.7%
0.0%
0.1%
0.9%
3.8%
0.0%
0.7%
1.0%
95.8%
0.0%
1.2%
1.2%
0.6%

0.0% 11.2%
2.2%
1.3%
0.0%
0.0%
0.3%
0.6%
6.2%
0.2%
0.6%
0.2%
0.0%
0.0%
1.7%
1.7%
2.8%
7.0%
0.1%
0.8%
0.9%
0.0%
2.0%
0.2%
0.1%
0.1%
1.4%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
7.9%
0.0%
1.8%
1.3%
0.5%
0.2%
0.1%
0.0%
0.2%
0.2%
0.7%
4.3%
0.0%
0.0%
0.0%
0.0%
0.4%
1.3%
3.2% 33.8%
6.9%
0.0%
0.7%
0.0%
0.4%
0.4%

0.0%
0.0%
0.0%
0.0%
0.3%
0.3%
88.2%
3.6%
0.3%
0.0%
0.0% 100.0%
0.0%
0.0%
0.0%
0.0%
0.4%
0.4%
0.2%
95.3%
0.0%
95.3%
1.0%
0.4%
87.4%
0.0%
0.6%
3.0%
78.3% 17.9%
99.0%
0.0%
0.3%
76.5%
8.8%
0.0%
1.0%
2.0%
0.5%
99.1%
0.0%
0.0%
0.4%
0.5%
2.9%
17.9%
0.4%
0.4%
98.2%
0.0%
0.1%
95.7%
0.1%
0.0%
98.9%
0.0%
0.0%
3.0%
2.0% 22.7%
14.3%
0.0%
0.0%
11.0%
0.0%
1.4%
0.0%
0.1%
58.3%
0.0% 100.0%
0.0%
76.1%
0.0%
2.2%
2.3%
0.0%
0.0%
1.5%
0.2%
96.6%
1.5%
1.2%
3.7%
1.5%
3.0% 82.0%
1.0%
0.0%
0.0%

0.0%
99.2%
0.0%
99.2%
0.1%
1.0%
0.0%
0.0%
0.0%
96.6%
0.0%
98.8%
0.0%
0.9%
0.4%
2.3%
0.0%
1.9%
0.0%
0.5%
0.0%
0.3%
5.9%
2.9%
0.0%
96.6%
0.0%
0.0%
95.5%
0.4%
5.6% 64.1%
0.1%
0.5%
1.6%
0.0%
0.4%
0.0%
4.8% 63.9%
0.0%
42.9%
0.0%
86.3%
0.4%
0.8%
0.0%
0.0%
0.0%
4.3%
3.5%
94.2%
1.1%
0.3%
5.8% 85.7%
0.0%
13.5%
60.2% 19.4%

0.8%
0.0%
0.0%
0.3%
0.0%
0.0%
6.9%
0.0%
0.0%
0.0%
0.0%
0.0%
3.4%
0.0%
0.0%
0.4%
0.0%
0.0%
1.9%
0.1%
1.6%
0.3%
0.2%
0.2%
0.0% 10.1%
0.0%
0.3%
0.0%
0.0%
0.0%
0.0%
0.3%
5.9%
0.0%
0.0%
0.0%
0.0%
0.0%
0.4%
0.0%
0.6%
0.2%
0.0%
0.0%
4.4%
6.8%
0.4%
0.9%
0.1%
0.1%
2.3%
0.2%
0.1%
0.7%
0.0%
0.0%
0.6%
0.6%
2.3%
0.0%
0.0% 42.9%
0.0%
0.0%
1.4%
1.7% 33.7%
5.0%
0.0%
0.0%
0.0%
0.0% 15.2%
2.2%
0.0%
0.0%
0.0%
0.2%
0.0%
0.2%
0.9%
0.9%
0.3%
0.0%
0.0%
0.0%
4.9% 12.6%
1.9%

2.9% 13.8%
1.1% 44.0%
1.8% 37.1%
11.0% 64.9%
3.1% 57.1%
5.2% 89.0%
1.3% 59.3%
2.0% 44.3%
2.6% 52.1%
3.0% 50.5%
1.1% 42.9%
8.9% 74.9%
6.2% 46.0%
0.9% 54.1%
2.0% 76.9%
0.3% 29.9%
1.2% 24.4%
3.0% 28.3%
4.5% 53.4%
7.4% 17.5%
6.2% 34.7%
3.5% 77.6%
2.4% 49.1%
2.3% 47.3%
2.2% 34.6%
2.1% 23.4%
2.0% 59.7%
6.7% 76.5%
1.0% 62.8%
2.1% 49.4%

2.6% 41.1%
5.3% 15.4%
7.2% 71.4%
2.4%
0.5%
5.0%
1.2%
1.2% 43.7%
2.8% 60.6%
9.4% 63.0%
1.0% 33.8%
1.9% 42.0%
0.9% 70.3%
2.0%
3.6%
1.7% 50.7%
7.7% 14.8%
4.4% 45.2%
1.4% 38.1%
9.6% 85.4%
4.7% 56.2%
1.3% 72.6%
6.8% 59.0%
2.2%
0.7%
1.0% 15.2%
2.8% 62.7%
0.6% 70.2%
0.4% 23.5%
0.8% 24.0%
4.3% 32.2%
7.6% 64.9%
0.6% 45.4%
2.1% 10.4%

Gold

15.0%

4.8%

38.5%

21.7% 14.6%

1.7%

0.8%

2.9%

Table 7: Analysis of label alignment for nonterminals in the compound PCFG (top) and the neural PCFG (bottom). Label
alignment is the proportion of correctly-predicted constistuents that correspond to a particular gold label. We also show the
predicted constituent frequency and accuracy (i.e. precision) on the right. Bottom line shows the frequency in the gold trees.

would be irresponsible
could be delayed
can be held
can be proven
could be used

of federally subsidized loans
of criminal racketeering charges
for individual retirement accounts
without prior congressional approval
between the two concerns

by the supreme court
of the bankruptcy code
to the bankruptcy court
in a foreign court
for the supreme court

a syrian troop pullout
a conventional soviet attack
the house-passed capital-gains provision
the ofﬁcial creditors committee
a syrian troop withdrawal

(NT-13 (T-12 w1) (NT-25 (T-39 w2) (T-58 w3)))

has been growing
’ve been neglected
had been made
had been canceled
have been wary

(NT-04 (T-13 w1) (NT-12 (T-60 w2) (NT-18 (T-60 w3) (T-21 w4))))

in fairly thin trading
in quiet expiration trading
in big technology stocks
from small price discrepancies
by futures-related program buying

(NT-04 (T-13 w1) (NT-12 (T-05 w2) (NT-01 (T-18 w3) (T-25 w4))))

in a stock-index arbitrage
as a hedging tool
of the bond market
leaving the stock market
after the new york

(NT-12 (NT-20 (NT-20 (T-05 w1) (T-40 w2)) (T-40 w3)) (T-22 w4))

the frankfurt stock exchange
the late sell programs
a great buying opportunity
the most active stocks
a major brokerage ﬁrm

the frankfurt market was mixed
the u.s. unit edged lower
a news release was prepared
the stock market closed wednesday
the stock market remains fragile

have a high default risk
have a lower default risk
has a strong practical aspect
have a good strong credit
have one big marketing edge

has been operating in paris
has been taken in colombia
has been vacant since july
have been dismal for years
has been improving since then

(NT-21 (NT-22 (NT-20 (T-05 w1) (T-40 w2)) (T-22 w3)) (NT-13 (T-30 w4) (T-58 w5)))

the gramm-rudman targets are met
a private meeting is scheduled
the key assumption is valid
the budget scorekeeping is completed
the tax bill is enacted

(NT-03 (T-07 w1) (NT-19 (NT-20 (NT-20 (T-05 w2) (T-40 w3)) (T-40 w4)) (T-22 w5)))

rejected a reagan administration plan
approved a short-term spending bill
has an emergency relief program
writes the hud spending bill
adopted the underlying transportation measure

(NT-13 (T-12 w1) (NT-25 (T-39 w2) (NT-23 (T-58 w3) (NT-04 (T-13 w4) (T-43 w5)))))

will be used for expansion
might be room for ﬂexibility
may be built in britain
will be supported by advertising
could be used as weapons

(NT-04 (T-13 w1) (NT-12 (NT-06 (NT-20 (T-05 w2) (T-40 w3)) (T-22 w4)) (NT-04 (T-13 w5) (NT-12 (T-18 w6) (T-53 w7)))))

for a health center in south carolina
by a federal jury in new york
of the appeals court in new york
of the further thaw in u.s.-soviet relations
of the service corps of retired executives

with an opposite trade in stock-index futures
from the recent volatility in ﬁnancial markets
of another steep plunge in stock prices
over the past decade as pension funds
by a modest recovery in share prices

(NT-10 (T-55 w1) (NT-05 (T-02 w2) (NT-19 (NT-06 (T-05 w3) (T-41 w4)) (NT-04 (T-13 w5) (NT-12 (T-60 w6) (T-21 w7))))))

to integrate the products into their operations
to offset the problems at radio shack
to purchase one share of common stock
to tighten their hold on their business
to use the microprocessor in future products

to defend the company in such proceedings
to dismiss an indictment against her claiming
to death some N of his troops
to drop their inquiry into his activities
to block the maneuver on procedural grounds

(NT-13 (T-12 w1) (NT-25 (T-39 w2) (NT-23 (T-58 w3) (NT-04 (T-13 w4) (NT-12 (NT-20 (T-05 w5) (T-40 w6)) (T-22 w7))))))

has been mentioned as a takeover candidate
has been stuck in a trading range
had left announced to the trading mob
only become active during the closing minutes
will get settled in the short term

would be run by the joint chiefs
would be made into a separate bill
would be included in the ﬁnal bill
would be costly given the ﬁnancial arrangement
would be restricted by a new bill

(NT-10 (T-55 w) (NT-05 (T-02 w1) (NT-19 (NT-06 (T-05 w2) (T-41 w3)) (NT-04 (T-13 w4) (NT-12 (T-60 w5) (NT-18 (T-18 w6) (T-53 w7)))))))
to enjoy a loyalty among junk bond investors
to transfer their business to other clearing ﬁrms
to soften the blow of declining stock prices
to keep a lid on short-term interest rates
to urge the fed toward lower interest rates

to supply that country with other defense systems
to transfer its skill at designing military equipment
to improve the availability of quality legal service
to unveil a family of high-end personal computers
to arrange an acceleration of planned tariff cuts

(NT-21 (NT-22 (T-60 w1) (NT-18 (T-60 w2) (T-21 w3))) (NT-13 (T-07 w4) (NT-02 (NT-27 (T-47 w5) (T-50 w6)) (NT-10 (T-55 w7) (NT-05 (T-47 w8) (T-50 w9))))))
unconsolidated pretax proﬁt increased N % to N billion
its total revenue rose N % to N billion
total operating revenue grew N % to N billion
its group sales rose N % to N billion
total operating expenses increased N % to N billion

amex short interest climbed N % to N shares
its pretax proﬁt rose N % to N million
its pretax proﬁt rose N % to N billion
ﬁscal ﬁrst-half sales slipped N % to N million
total operating expenses increased N % to N billion

Table 8: For each subtree (shown at the top of each set of examples), we perform PCA on the variational posterior mean
vectors that are associated with that particular subtree and take the top principal component. We then list the top 5 constituents
that had the lowest (left) and highest (right) principal component values.

