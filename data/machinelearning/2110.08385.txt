1
2
0
2

t
c
O
5
1

]
I
S
.
s
c
[

1
v
5
8
3
8
0
.
0
1
1
2
:
v
i
X
r
a

Robust Correlation Clustering with Asymmetric
Noise

Jimit Majmudar
University of Waterloo
jmajmuda@uwaterloo.ca

Stephen Vavasis
University of Waterloo
vavasis@uwaterloo.ca

Abstract

Graph clustering problems typically aim to partition the graph nodes such that
two nodes belong to the same partition set if and only if they are similar. Cor-
relation Clustering is a graph clustering formulation which: (1) takes as input a
signed graph with edge weights representing a similarity/dissimilarity measure be-
tween the nodes, and (2) requires no prior estimate of the number of clusters in the
input graph. However, the combinatorial optimization problem underlying Corre-
lation Clustering is NP-hard. In this work, we propose a novel graph generative
model, called the Node Factors Model (NFM), which is based on generating fea-
ture vectors/embeddings for the graph nodes. The graphs generated by the NFM
contain asymmetric noise in the sense that there may exist pairs of nodes in the
same cluster which are negatively correlated. We propose a novel Correlation
Clustering algorithm, called ℓ2-norm-diag, using techniques from semideﬁnite
programming. Using a combination of theoretical and computational results, we
demonstrate that ℓ2-norm-diag recovers nodes with sufﬁciently strong cluster
membership in graph instances generated by the NFM, thereby making progress
towards establishing the provable robustness of our proposed algorithm.

1 Introduction

−

[0, 1], and we
Suppose we have n objects and for any two objects i, j, a similarity score pij ∈
wish to determine a clustering of the objects such that objects in the same cluster are similar and
objects in different clusters are dissimilar. Correlation Clustering, ﬁrst introduced by Bansal et al.
[2004], formulates this problem as an optimization problem which does not require a priori knowl-
edge about the number of clusters in the graph. The idea in Correlation Clustering is to ﬁrst form a
weighted graph on n nodes where the weight of edge ij is obtained from the similarity score using
the transformation log(pij /(1
pij)), and then to ﬁnd a partition of the nodes which maximizes
agreements, i.e. the sum of positive weights whose endpoints are put in the same cluster and the
absolute values of negative weights whose endpoints are put in different clusters, (or, equivalently,
minimizes disagreements, i.e. the sum of positive weights whose endpoints are put in different clus-
ters and the absolute values of negative weights whose endpoints are put in the same cluster). In
general, the aforementioned optimization problem is NP-hard. Interestingly, the objective functions
for disagreement minimization and agreement maximization differ by a constant, and as a result, an
approximation algorithm provides different approximation ratio guarantees for the two problems;
however, for the work presented in this work, the two problems are equivalent. While there has been
considerable interest in designing approximation algorithms (Bansal et al. [2004], Demaine et al.
[2006], Makarychev et al. [2015], Mathieu and Schudy [2010], Swamy [2004]), there have been
very few works focusing on average case analysis or recovery of a ground truth clustering, as dis-
cussed in the following literature review. This style of analysis has recently gained popularity in
tackling hard machine learning problems such as low-rank matrix completion (Candès and Recht
[2009], Recht [2011]), dictionary learning (Arora et al. [2014], Spielman et al. [2012]), and over-
lapping community detection (Anandkumar et al. [2013], Majmudar and Vavasis [2020], Mao et al.

 
 
 
 
 
 
[2017]), to name a few. In this work, we introduce a new graph generative model based on gener-
ating feature vectors/embeddings for the nodes in the graph, and propose a tuning-parameter-free
semideﬁnite-programming (SDP)-based algorithm to recover nodes with sufﬁciently strong cluster
membership.

Among existing provable methods, Joachims and Hopcroft [2005] propose a fully-random model
which generates signed graphs, and show that the model ground truth clustering is close to the
optimal solution of the combinatorial optimization problem of maximizing agreements. However, it
is not shown how to provably recover the model ground truth efﬁciently.

Mathieu and Schudy [2010] propose fully- and semi-random models, i.e. in which there is a proba-
bilistic component and a deterministic adversarial component, which generate signed graphs. Their
fully-random model can be interpreted as a special case of the planted partition model in which the
p and q are equal, and the lack of an edge is treated as an edge with weight
noise probabilities 1
1. For graph instances generated by the fully-random model, they propose a recovery algorithm
−
which uses a modiﬁcation of the SDP formulation proposed by Swamy [2004] followed by a novel
randomized rounding procedure. However, their proposed SDP formulation suffers from the limita-
tion that it has Θ(n3) (where n is the number of nodes in the graph) constraints corresponding to
triangle inequalities, making it almost unusable in practice for graphs with as low as 5000 nodes.

−

Chen et al. [2014] consider the planted partition model with the added difﬁculty that some entries
of the adjacency matrix are not known. The input graphs are signed as the lack of an edge is treated
as an edge with weight
1, and their algorithm uses a matrix-splitting SDP, originally introduced
to express a given matrix as the sum of a sparse and a low-rank matrix. They provide conditions
under which the SDP solution is integral and therefore their algorithm requires no rounding. They
also argue that the recovered model ground truth clustering coincides with the optimal solution of
the combinatorial optimization problem of minimizing disagreements.

−

Makarychev et al. [2015] propose a semi-random model which generates signed graphs. Their al-
gorithm also uses the SDP proposed by Swamy [2004] followed by a novel rounding procedure.
Their recovery result states that if the input graph is generated from their semi-random model and
additionally also satisﬁes some deterministic structural properties, then with constant probability at
most a fraction of the nodes are mis-clustered.

{

}

∈

×

R

R

N, M (

1, 2, . . . , n

. For any two sets

N, [n] denotes the set

, Rn denotes the vector
Notation: For any natural number n
space of n-dimensional real-valued vectors, and Sn denotes the vector space of n
n symmetric,
real-valued matrices. Let M be any matrix. We use Mij or M (i, j) to denote its entry ij; for any set
)) denotes the submatrix of M containing all columns (resp. rows)
, :) (resp. M (:,
R ⊆
) denotes the
but only the rows (resp. columns) indexed by
,
R
R
. We use max(M )
submatrix of M containing the rows indexed by
R
to denote its largest value, and M+ and M− to denote the projections onto the cone of non-negative
and non-positive matrices respectively. To show that M is a symmetric positive semideﬁnite (resp.
positive deﬁnite) matrix, we use the notation M
n square matrix,
≻
diag(M ) denotes a vector in Rn whose entry i is Mii for each i
[n], and Diag(M ) denotes an
n matrix whose diagonal is equal diag(M ) and whose each off-diagonal entry is zero. Let v be
n
any vector. We use vi or v(i) to denote its entry i; for any set
) denotes the subvector
of v containing entries indexed by
. We use max(v) to denote its largest value, and v+ and
v− to denote the projections onto the cone of non-negative and non-positive vectors respectively. If
n matrix whose diagonal is equal to v and whose each off-diagonal
v
R, we denote by v◦p the
entry is zero. If v is an entry-wise non-negative vector, then for any p
vector obtained by exponentiating each entry of v with p.

Rn, Diag(v) denotes an n

and the columns indexed by

0). If M is an n

0 (resp. M

N, M (

N, v(

R ⊆

,
R

S ⊆

R

R

(cid:23)

×

×

×

∈

∈

∈

S

S

For any two matrices X, Y of identical dimensions,
trace(X T Y ). For any two vectors u, v
Rn : ui ≤
[u, v] to denote the set

X, Y
h
Rn such that for each i
.
i
vi ∀

∈
xi ≤

[n]

∈

x

{

}

i

k · k

∈
to denote the ℓ2-norm for vectors. The Frobenius norm of a matrix is denoted by

We use
k · kF .
I and E denote the identity matrix and the matrix with each entry set to one respectively whose
dimensions will be clear from the context. For any positive integer i, ei denotes column i of the
identity matrix and e denotes the vector with each entry set to one; the dimension of these vectors
will be clear from context.

denotes the trace inner product
vi, then we use

[n], ui ≤

∈

2

For any graph G = (V, W ), i.e. graph with node set V and weighted adjacency matrix W , L(G)
denotes the graph Laplacian matrix deﬁned to be Diag(W e)
V of nodes,
G[V ′] denotes the subgraph of G induced by nodes in V ′.

W . For any subset V ′

⊆

−

2 Problem Formulation

2.1 Node Features Model (NFM)

We begin by deﬁning the generative model, called the Node Features Model (NFM), for which we
formulate the Correlation Clustering recovery problem.
Deﬁnition 2.1 (Node Features Model (NFM)). Let n and k be positive integers denoting the number
of nodes and the number of clusters respectively. Let the nodes and the clusters be labelled using the
Rk
sets [n] and [k] respectively. For each node i
from a probability distribution on the unit simplex. Generate a weighted random graph G on the n
nodes with weight matrix W deﬁned as

[n], draw independently a feature vector θi

∈

∈

Wii′ = 


log

0

θiT θi

′

θiT θi′

!

1

−

if i

= i′

otherwise.

For each j

∈

[k], deﬁne cluster Vj as

[n] : max(θi) > 0.5, arg max(θi) = j

Vj :=

i

and deﬁne the set of stray nodes Vstray as

{

∈

}

Vstray :=

i

{

∈

[n] : max(θi)

0.5

.

}

≤

∈

[n] of distinct nodes, we interpret θiT θi

The intuition behind NFM is that ﬁrst we generate a feature vector (or embedding) for each node
in the graph, then for any pair i, i′
as a similarity
score, i.e. the probability with which the two nodes belong to the same cluster, lastly we apply a
logarithmic transformation on the similarity score which produces a positive weight if the score is
greater than 0.5 and a negative weight if the score is less than 0.5. The transformation h(x) =
x)) which maps the set (0, 1) to arbitrary real values is called the logit or log-odds
log(x/(1
function in literature, and its inverse h−1(x) = 1/(1 + e−x) is the so-called logistic function. These
functions are commonly used in regression problems in which the output variable is interpreted
as a probability and is therefore expected to belong to the set (0, 1). For instance, a multivariate,
vector-valued generalization of the logistic function, called the softmax function, is widely used in
classiﬁcation problems to transform arbitrary real-valued vectors into probabilities corresponding to
class memberships.

−

′

We begin by asking the following question for the NFM described by (2.1):

Given W , how can we efﬁciently recover the sets V1, . . . , Vk using no prior knowledge of k?

Using a combination of theoretical analyses and computational experiments, we make progress to-
wards answering the question posed above by proposing two SDP-based recovery algorithms, called
1-diag and ℓ2-norm-diag. The ﬁrst recovery algorithm, 1-diag, is studied in Section 3 and is
based on the SDP formulation of Swamy Swamy [2004] whose variants have also been used in
Makarychev et al. [2015], Mathieu and Schudy [2010]. Then we demonstrate a limitation of the
aforementioned algorithm to handle certain noisy instances. Consequently, we propose and analyze
the novel ℓ2-norm-diag recovery algorithm in Section 4. Our theoretical analysis is not comprehen-
sive and the deﬁciencies are taken care of using evidence from computational experiments. Before
proceeding to the material on the two recovery algorithms, in the subsequent sections, we discuss
structural properties of the NFM relevant to the recovery problem we are interested in solving.

2.2 Nature of Noise in the NFM

We discuss the nature of noise in our model. Deﬁne the cluster set

Cj :=

x

{

∈

∆k−1 : xj > 0.5

}

3

 
6
(0, 0, 1)

C3

C

C1

C2

(1, 0, 0)

(0, 1, 0)

Figure 1: Central set C and cluster sets C1, C2, C3 for the unit simplex in R3.

for each j

∈

[k], and the central set

C :=

Figure 1 shows these sets for k = 3.

∆k−1 : max(x)

x

{

∈

0.5

.

}

≤

Note that in the light of the above deﬁnitions, we may equivalently redeﬁne the sets Vj, for each
j

[k], and Vstray in Deﬁnition 2.1 as

∈

Vj :=
Vstray :=

i

i

{

{

∈

∈

[n] : θi
[n] : θi

Cj }
.
C
}

∈

∈

∈

∈

∈

∈

∈

∈

≤

∆k−1, xT y

C and y

Cj, for some j

Cj′ , for some distinct j, j′

Observe that for any x
0.5. This suggests that in the weighted graphs
generated by the NFM, the stray nodes form negative edges with all other nodes in the graph, hence
justifying their name. Due to this property, such nodes are quite benign with regards to mathematical
analysis as any reasonable clustering algorithm, including the ones proposed in this work, ought to
be able to detect them exactly. For any x
Cj, y
[k], we have
that xT y < 0.5. This suggests that in the weighted graphs generated by the NFM, the clusters are
well-separated in the sense that each pair of nodes lying in distinct clusters shares a negative weight
[k], then xT y may or may not be larger than 0.5
edge. However, if both x, y
∈
and this is what introduces noise in our model. In other words, in the graphs generated by the NFM,
it is possible for two nodes lying in the same cluster to share a negative weight edge. Therefore
NFM models only one-sided noise. This behavior is well-motivated as real-world graphs do not
always have a symmetric two-sided noise. For instance, consider a social network of researchers
from the academic communities of mathematics, physics, history, and biology. Suppose the edge
weights represent pair-wise similarities between any two researchers determined using the number
of co-authored research articles. In this setting, we might have occasional collaborations amongst
researchers of different communities; however, we almost certainly cannot expect all researchers
in the same community to have collaborated with each other. In the language of weighted graphs,
if the different academic communities represent the clusters in the graph, then we should expect
a signiﬁcantly high number of within-cluster negative edges compared to between-cluster positive
edges. Due to such practical motivation, Correlation Clustering with asymmetric noise has also been
studied in Jafarov et al. [2020, 2021].

2.3 Feature Space for a Cluster in the NFM

As brieﬂy mentioned in Section 2.2, it is possible for two nodes belonging in the same cluster to
share a negative edge. It is instructive to understand further the nature of such negative edges. For
each j

[k], deﬁne a partition of the set Cj into strong and fringe sets as

∈

j

Cstrong
Cf ringe

j

:=

:=

x

x

{

{

∈

∈

∆k−1 : xj ≥
∆k−1 : 0.5
≤

1/√2
}
xj < 1/√2

.

}

Figure 2 shows these sets for k = 3.

4

(0, 0, 1)

C3

C

C1

C2

(1, 0, 0)

(0, 1, 0)

Figure 2: Central set C and the partition of corner sets C1, C2, C3 into strong and fringe sets, shown
using dotted lines, for the unit simplex in R3; for each corner set, the partition set containing a
simplex vertex denotes the strong set.

Consequently, for each j

∈

[k], we partition the cluster nodes Vj into strong and fringe nodes as

V strong
j
V f ringe
j

:=

:=

[n] : θi
[n] : θi

i

i

{

{

∈

∈

∈

∈

j

Cstrong
Cf ringe

j

}

}

.

These deﬁnitions are motivated by the intuition that the magnitude of the largest entry in the feature
vector of a node quantiﬁes the strength of cluster membership for that node. Moreover, the cut-off
of 1/√2 is chosen by noticing that any two points in the strong set of the same cluster have an
inner product of at least 0.5. In other words, any two nodes which are strong for the same cluster
share a non-negative edge. Therefore if the graph contains only strong nodes for each cluster, then it
has no noise in the form of a negative within-cluster edge. Fringe nodes, however, may potentially
share some negative edges among themselves and with other nodes in the same cluster because the
memberships of such nodes in their respective clusters are not sufﬁciently strong. Therefore we may
think that it is difﬁcult to cluster all the fringe nodes correctly.

2.4 Relation to the MMSB

The problem setup developed using the NFM bears some resemblance with the weighted version
of MMSB considered in Majmudar and Vavasis [2020]. In particular, the graphs obtained by the
NFM can be obtained by setting the community interaction matrix B to be the identity matrix in the
weighted MMSB. In terms of recovery, because we are modeling Correlation Clustering using the
NFM, our goal is to recover only the cluster labels without using an a priori estimate of the number
of clusters k. The weighted MMSB models the overlapping community detection problem in which
the goal was to recover the fractional memberships of each node in the different communities and
we were allowed to use a parameter corresponding to k in the recovery algorithm.

3 1-diag Recovery Algorithm

We ﬁrst present and analyze the 1-diag recovery algorithm which uses the SDP relaxation (P-1D)
ﬁrst introduced in Swamy [2004] to perform Correlation Clustering. For any node set V , we deﬁne
, 0/1 matrix whose entry ii′ is 1 if and
the cluster matrix for some partition of V to be a
V
only if nodes i and i′ belong to the same partition set.

| × |

V

|

|

5

Algorithm 1 1-diag
Input: Graph G = (V, W ) generated according to NFM
Output: Symmetric matrix X c of the same dimension as W whose each entry is in
1: X ∗ = arg max
W, X
i
h
2: X c = Round(X ∗, 0.5)
3: if X c is not the cluster matrix for some partition of V then
4: X c = 0
5: end if

0, Xii = 1

s.t. X

0, X

i
∀

[n]

(cid:23)

≥

∈

0, 1

}

{

Algorithm 2 Round
Input: Matrix X, scalar t
Output: 0/1 matrix X r of the same dimension as X
1: for i, j

2: X r

∈
ij =

3: end for

[n] do
1
0
(cid:26)

if Xij > t
if Xij ≤
t

Note that the output of 1-diag can possibly be the zero matrix and therefore does not deﬁne a
clustering for the input graph. However, the theory developed in Sections 3.1 and 3.2 provides
conditions on the input graph sufﬁcient for the output of 1-diag to induce a clustering.

max
X

W, X
h
i
s.t. X
0
≥
X
0
(cid:23)
Xii = 1

[n].

i
∀

∈

(P-1D)

3.1 Warmup

v

{

∈

(cid:23)

V1, . . . , Vk,

}v∈Vstray }

[k], L(G[Vj ])

We begin by analyzing the scenarios in which the the SDP (P-1D) has a 0/1 solution. The following
theorem provides a deterministic sufﬁcient condition on the graph instances for (P-1D) to have a 0/1
solution. Subsequently, we discuss the deterministic sufﬁcient condition in the context of the NFM.
Theorem 3.1. Let G = (V, W ) be a graph generated using the NFM. Suppose that for each
0. Let X ∗ denote the cluster matrix corresponding to the partition
j
. Then X ∗ is an optimal solution of (P-1D).

{
Recall that G[Vj] denotes the subgraph of G induced by the node set Vj and L(G[Vj ]) denotes the
Laplacian matrix of graph G[Vj ]. The above theorem states that exact recovery of true clusters
is achievable using (P-1D) provided each cluster Laplacian is positive semideﬁnite. To connect
this result with the NFM, we may quantify the probability such that each cluster Laplacian, in a
graph instances generated by the NFM, is positive semideﬁnite. Table 1 shows some computational
experiments in this regard. Each row in the table corresponds to 10 cluster instances generated using
the NFM in which the simplex distribution is chosen to be the Dirichlet distribution. We ﬁx k = 3
and the Dirichlet parameter α = 0.3e. The ﬁrst column denotes the range in which the cluster size
belongs, and the second column counts the PSD success, i.e. number of cluster instances, out of
10, which have a positive semideﬁnite Laplacian. Moreover, the third column contains the mean
smallest eigenvalue of the Laplacian.

These computational results suggests a weakness of Theorem 3.1 in the sense that the determinstic
condition required for exact recovery seems to hold with a probability converging to 0 for the NFM
with the Dirichlet distribution as the size of input graph grows. Morever, the decreasing smallest
eigenvalue of the cluster Laplacians may also be interpreted as an increasing amount of noise in the
clusters which motivates the following conjecture.
Conjecture 3.2. Let G be a graph generated according to NFM in which the simplex distribution is
chosen to be the Dirichlet distribution with constant parameter. No algorithm can exactly recover
the true clusters in G with probability not converging to 0 as n

.
→ ∞

6

Table 1: Veriﬁcation of positive semideﬁniteness of cluster Laplacians.

Cluster size range

PSD success (/10) Mean smallest Laplacian eigenvalue

10
15
20
25
30
35
40

6
11
16
21
26
31
36

−
−
−
−
−
−
−

4
4
0
1
1
0
0

1.31
1.31
3.60
2.82
4.26
5.96
6.35

−
−
−
−
−
−
−

Conjecture 3.2 highlights information-theoretic limitations for exactly recovering the ground truth
clusters, see Banks et al. [2016, 2018], Berthet and Baldin [2020], Berthet et al. [2019], Wang et al.
[2016] for instance, for results on information-theoretic limits for similar or related problems. The
above observations also lead us to reformulate the central question posed in Section 2.1 as follows.

Given W , how can we efﬁciently recover exactly k disjoint node sets, such that each node set
, . . . , V strong
k

contains exactly one of V strong

, using no prior knowledge of k?

1

We may interpret this reformulation as: instead of attempting to exactly recover the true clusters,
we focus on exactly recovering the strong nodes, possibly in the presence of fringe nodes, which
introduce noise in the form of negative within-cluster edges. The usage of the word “contains” in the
above question indicates that recovery of any fringe node for a cluster is not necessarily intended but
may happen. This perspective on robust Correlation Clustering, which involves clustering essentially
only a subgraph of the input graph, is similar to that in Krishnaswamy et al. [2019], which provides
an approximation algorithm for a generalized Correlation Clustering problem wherein the input
graph is corrupted with a given number of noisy nodes which must be discarded before performing
clustering.

To answer the reformulated question above, we adopt a two-step algorithm analysis approach de-
scribed as follows. Let G be a graph generated by the NFM and let
be a cluster recovery algorithm
A
j be the union of strong nodes and possibly some fringe nodes for
of interest. For each j
to successfully recover node sets V ′
1 , . . . , V ′
k with some non-trivial
cluster j, such that we expect
V ′
probability as the number of nodes n
j for
. In other words,
each j

is likely to fail on the sets Vj \

[k]. We may formalize the behavior of

using the following two steps.

[k], let V ′

→ ∞

A

A

∈

∈
1. For each j

C

j , for each j

∈
stray node set V stray
j
′ on node sets V ′

[k], perturb the features of the node set Vj \

V ′
j to the central set to obtain the
, and call the resulting graph G′. Prescribe deterministic conditions
.
A
2. For each j
so as to obtain the node
∈
j , which we may interpret as noisy nodes, i.e. we re-obtain graph G from G′.
V ′
set Vj \
is robust to the presence of node sets
Prescribe deterministic conditions
A
V ′
[k]. The desired robustness properties are established by applying
j , for each j
Vj \
∈
perturbation arguments to the analysis of

[k], re-perturb the features of the node set V stray

[k], which ensure their exact recoverability from G′ by

on G′ achieved in the previous step.

under which

∈

C

j

A

A

In terms of probability quantiﬁcation, we must also argue that for a graph G generated by the NFM,
the deterministic conditions required for provably robust recovery hold with probability not converg-
ing to 0 as n

.
→ ∞

3.2 Theoretical Guarantees

Using Theorem 3.1, we conclude that if each cluster Laplacian is positive semideﬁnite, then 1-diag
achieves exact recovery. Adopting the two-step approach outlined in the previous section, we are
now interested in the following two questions:

• What is the probability that, for each cluster, the subgraph induced by the union of strong

nodes and possibly some fringe nodes has a positive semideﬁnite Laplacian?

7

• Is the 1-diag recovery algorithm robust to the presence of noisy nodes, i.e. fringe nodes

that are close to being stray nodes?

In this section, we address the ﬁrst question above, and in Section 3.4, we address the second ques-
tion. Observe that if we restrict our attention to the cluster subgraph induced by merely the strong
nodes, then with probability 1, the Laplacian is positive semideﬁnite because each edge has a non-
negative weight. However, we are interested in extending this observation to a cluster subgraph in-
duced by strong nodes and some fringe nodes which also possibly contains negative edges. (Based
on the results in Table 1, we cannot expect to include all fringe nodes.) For the NFM, directly
quantifying the probability of Laplacian positive semideﬁniteness for a cluster subgraph comprised
of strong nodes and some fringe nodes appears a difﬁcult task. Therefore in the following, Theo-
rems 3.3 and 3.4 provide combinatorial sufﬁcient conditions for a graph Laplacian to be positive
semideﬁnite.
Theorem 3.3. Let G = (V, W ) be a signed graph. Suppose for each negative edge ii′ where
i, i′
P ii
l }l∈[m] of positive weights such
that

[n], there exists a set of m disjoint two-edge ii′-paths

∈

{

′

Wii′

−

≤

1
2 ×

Xl∈[m]

harmonic mean of the two weights on P ii
l

′

and the two-edge paths are disjoint across all negative edges, then L(G)

0.

(cid:23)

The intuition behind the proof of Theorem 3.3 is to express the graph Laplacian as the sum of mul-
tiple graph Laplacians (corresponding to subgraphs of G), and then argue for the positive semideﬁ-
niteness of each summand Laplacian. Considering subgraphs in this way makes it easier to analyze
negative edges; in particular a negative edge ii′ is included in a subgraph which also contains an
adequate number of positive ii′-paths so as to compensate the contribution of the edge ii′ to the
Laplacian. This idea is inspired by the support-graph technique used to design preconditioners for
conjugate gradient Bern et al. [2006].

Theorem 3.3 provides a sufﬁcient condition to ensure Laplacian positive semideﬁniteness, however,
it is seemingly weak as described by the following example.
Example 3.1. Generate a graph on n nodes using the NFM with the probability distribution over
the unit simplex ﬁxed as the Dirichlet distribution. Suppose cluster j of the graph contains fj
fringe nodes. Consider a case in which a constant fraction of all pairs of the fj fringe nodes
share a negative edge each. Then to use the sufﬁcient condition in Theorem 3.3 to ensure positive
semideﬁniteness of the Laplacian of cluster j, we require Ω(f 2
j ) strong nodes for that cluster. In
(√nj) fringe
other words, if the cluster contains nj nodes, then Theorem 3.3 allows for only
O
nodes. However letting p be the probability of a feature vector lying in the fringe set for cluster j,
we note that E[fj] = np. Moreover, using Hoeffding’s inequality, we have that fj ∈
[np/2, 3np/2]
np2/2). That is, fj = Θ(n), and consequently fj = Ω(nj),
with probability at least 1
−
with probability converging to 1 as n
. This suggests a potential weakness of the sufﬁcient
condition presented in Theorem 3.3 for establishing positive semideﬁniteness of cluster Laplacians.

2 exp(

→ ∞

−

The above shortcoming is addressed in the following theorem which provides a different combina-
torial condition to ensure Laplacian positive semideﬁniteness.
Theorem 3.4. Let G = (V, W ) be a signed graph. Let U
negative edge. That is, U :=
that for each u

V contain all nodes of G adjacent to a
U such

⊆
V : Wvw < 0 for some w

. If there exists S

{
∈
S, we have

U and s

⊆

∈

V

V

\

}

v

∈

∈

S

Wus ≥ −

|

|

then L(G)

0.

(cid:23)

2 

Wuu′ 

(1)

′

∈U:
Xu
Wuu′ <0









We revisit Example 3.1 in the light of Theorem 3.4. If we assume that all edges in cluster j other
than the ones among the fj fringe nodes have a non-negative weight, and that the positive and
negative weight magnitudes are of the same order, then to ensure positive semideﬁniteness of the
Laplacian of cluster j using the sufﬁcient condition obtained in Theorem 3.4, it sufﬁces to have

8

Table 2: Veriﬁcation of sufﬁcient condition (1) for Laplacian positive semideﬁniteness.

Cluster size range Combinatorial condition success (/10)

10
15
20
25
30
35
40

6
11
16
21
26
31
36

−
−
−
−
−
−
−

9
9
7
9
9
8
9

fj = Θ(nj). However, this example should not be interpreted to imply that Theorem 3.4 is a
strengthening of Theorem 3.3. For example, if we have a cluster in which each node is adjacent
to a negative edge, Theorem 3.3 may still be used to ensure positive semideﬁniteness of the cluster
Laplacian, but Theorem 3.4 does not apply due to the absence of a set S. But for the purpose of
analyzing a generative model such as the NFM, Theorem 3.4 appears to be a better tool because of
its tolerance to a number of fringe nodes that is linear in the size of the cluster, and because of the
presence of strong nodes in the NFM. This is further corroborated by computational results shown in
Table 2. Each row in the table corresponds to 10 cluster instances generated using the NFM in which
the simplex distribution is chosen to be the Dirichlet distribution. We ﬁx k = 3 and the Dirichlet
parameter α = 0.3e. The ﬁrst column denotes the range corresponding to the size of the subgraph
induced by strong nodes and fringe nodes whose feature vectors have largest entry at least 0.6; the
cut-off of 0.6 is based on manual parameter search for the given setting of k and α. The second
column counts the combinatorial condition success, i.e. number of instances, out of 10, for which
the subgraph satisﬁes (1).

These computational results suggest that the probability with which the cluster subgraphs consisting
of nodes whose feature vectors have largest entry at least 0.6 have a positive semideﬁnite Laplacian
does not apparently converge to 0 as n
Conjecture 3.5. Let G = (V, W ) be a graph generated using the NFM in which the simplex dis-
tribution is chosen to be the Dirichlet distribution with constant parameter α. Then there exists a
scalar t(k, α)
[k], with probability not converging to 0 as
, G[V ′
n

, and also motivate the following conjecture.

j ] satisﬁes the hypothesis of Theorem 3.4 where

(0.5, 1/√2) such that for each j

→ ∞

∈

∈

→ ∞

V ′
j :=

i

{

∈

[n] : θi

j ≥

t(k, α)
}

.

3.3 Proofs

In this section, we include proofs of Theorems 3.1, 3.3, and 3.4 stated in Section 3.2.

Proof of Theorem 3.1. Our analysis uses SDP duality and therefore note that the dual of (P-1D) is

min
(Y,Z,y)

s.t.

eT y

0
0

Y
Z
W + Y + Z = Diag(y).

≥
(cid:23)

(D-1D)

.

v

{

}v∈Vstray }

As mentioned in the theorem statement, X ∗ is the cluster matrix corresponding to the partition
V1, . . . , Vk,
{
Both optimization problems (P-1D) and (D-1D) have strictly feasible solutions. For instance, X ′ :=
0.5I + 0.5E is a positive, positive deﬁnite matrix which is feasible for (P-1D). Similarly, Y ′ := E,
Z ′ := (
(W + E) and y′ := (
+ ǫ)e gives a strictly feasible solution
k
k
(Y ′, Z ′, y′) for (D-1D) for any ǫ > 0. Therefore using the Karush-Kuhn-Tucker (KKT) optimality
Sn is an optimal solution for (P-1D) if and only if X ∗ is feasible
conditions, we observe that X ∗
for (P-1D) and there exists a feasible solution of (D-1D), (Y ∗, Z ∗, y∗) such that:

W + E

W + E

+ ǫ)I

−

∈

k

k

• X ∗

ijY ∗

ij = 0,

i, j

∀

∈

[n]

9

•

X ∗, Z ∗
h

i

= 0.

X ∗ has non-negative entries with each diagonal entry being equal to one. Additionally, up to a
permutation of its rows and columns, it is a block diagonal matrix in which each non-zero diagonal
block is the matrix of all ones. Therefore X ∗ is feasible for (P-1D), and in the rest of the proof, we
explicitly construct (Y ∗, Z ∗, y∗).

For each j

[k], we set

∈

For each distinct j, j′

[k], we set

∈

Y ∗(Vj , Vj) = 0
Z ∗(Vj , Vj) = L(G[Vj ])

y∗(Vj) = W (Vj , Vj)e.

Y ∗(Vj , Vj′ ) =
−
Z ∗(Vj , Vj′ ) = 0.

W (Vj , Vj′ )

For each stray node v

Vstray, we set

∈
Y ∗(v, :) =
−
Z ∗(v, :) = 0
y∗(v) = 0.

W (v, :)

(and Y ∗(:, v) =
−
(and Z ∗(:, v) = 0)

W (:, v))

Because each pair of nodes lying in distinct clusters shares a negative edge and because each stray
node shares a negative edge with every other node in the graph, we have that Y ∗
0. Similarly,
because L(G[Vj])
Matrices X ∗ and Y ∗ have disjoint supports by construction, and therefore X ∗
i, j

[k], we have that Z ∗

ij = 0 for each

[n]. Moreover

0 for each j

ijY ∗

0.

≥

(cid:23)

(cid:23)

∈

∈

X ∗, Z ∗
h

i

=

Xj∈[k]

X ∗(Vj, Vj ), Z ∗(Vj, Vj )
i
h

L(G[Vj ]), E
h

i

=

Xj∈[k]

= 0

where the last line uses the fact that each row of a Laplacian matrix sums to zero.
Lastly, we show that the equation W + Y ∗ + Z ∗ = Diag(y∗) is satisﬁed. For each j

W (Vj , Vj) + Y ∗(Vj , Vj) + Z ∗(Vj, Vj ) = W (Vj, Vj ) + L(G[Vj ])

[k], we have

∈

(using the deﬁnitions of Y ∗, Z ∗)

= Diag(y∗(Vj )).

(using the deﬁnition of y∗)

For each distinct j, j′

∈

[k], we have

W (Vj, Vj′ ) + Y ∗(Vj, Vj′ ) + Z ∗(Vj, Vj′ ) = 0

using the deﬁnitions of Y ∗, Z ∗. Similarly, for each stray node v, we have

W (v, :) + Y ∗(v, :) + Z ∗(v, :) = 0
W (:, v) + Y ∗(:, v) + Z ∗(:, v) = 0

using the deﬁnitions of Y ∗, Z ∗.

Now we provide proofs of Theorems 3.3 and 3.4 which provide combinatorial sufﬁcient conditions
for Laplacian positive semideﬁniteness.

10

i1

i2

i

im

i′

Figure 3: Subgraph of G containing negative edge ii′ and m dijsoint two-edge ii′-paths of positive
weights.

i′ denote m
Proof of Theorem 3.3. Pick any negative edge ii′ in G, and let i
disjoint two-edge ii′-paths of positive weights. Consider the subgraph of G containing edge ii′ and
these m disjoint paths as shown in Figure 3.

i′, . . . , i

im −

i1 −

−

−

The contribution of this subgraph to the Laplacian of G is the matrix, padded appropriately with
zeros,

Wii′ +

Wiil



l∈[m]
P
Wii′

−

Wii′

−
Wii′ +

Wi′il

Wii1

−

Wi′i1

−

Wii2

−

Wi′i2

−

. . .

. . .

Wiim

−

Wi′im



−

−
−

−
−

Wii1
Wii2
...
Wiim


Wii1 + Wi′i1


0

...




0


Now since each of Wii1 + Wi′i1 , . . . , Wiim + Wi′im is positive, using the Schur complement con-
dition for positive semideﬁniteness, the above matrix is positive semideﬁnite if and only if the 2
2
matrix

. . .
. . .
. . .
. . . Wiim + Wi′im

0
Wii2 + Wi′i2
...
0












0
0
...

×

−

−

.

l∈[m]
P
Wi′i1
Wi′i2
...
Wi′im

Wii′ +

Wiil



l∈[m]
P
Wii′

−



Wii′

−
Wii′ +

l∈[m]
P

Wiil
Wi′il(cid:21)



(cid:20)

[Wiil Wi′il ]

Wi′il

−

Wiil + Wi′il



Xl∈[m]





(2)







is positive semideﬁnite. However, the matrix in (2) can be rewritten as

Xl∈[m]
which is positive semideﬁnite if and only if



Wii′ +



Wii′

−

≤

which proves the desired statement.

Wiil Wi′il
Wiil + Wi′il 

1
1

(cid:20)

−

1
−
1

(cid:21)



Wiil Wi′il
Wiil + Wi′il

Xl∈[m]

Proof of Theorem 3.4. For notational ease, deﬁne L := L(G). Label the nodes of G using the set
[n] and assume, without loss of generality, that U = [m] for some m < n, and S =
m+ 1, . . . , m+
S

Sn as follows. For each u, u′

. We deﬁne matrix C

U ,

{

|

|}

∈

Moreover C(U, S) := −


and C(S, U ) := C(U, S)T . Lastly, we set C(S, S) :=

eT C(U, U )e
S

|

|

∈
= u′
if u
if u = u′

Luu′

Lul|




l∈[m]\{u}|
P

Cuu′ :=

C(U, U )eeT
S

|

|

I, and we set all other entries of C to be zeros. That is,

C(U, U ) C(U, S) 0
0
C(S, U ) C(S, S)
0#

0

0

.

C =

"

11

6
In the rest of the proof, we argue that each of C and L
the positive semideﬁniteness of L.

−

C is positive semideﬁnite, thereby proving

∪

S, U

To show the positive semideﬁniteness of C, it sufﬁces to show the positive semideﬁniteness of
S). First note that using the diagonal dominance property in C(U, U ), we conclude
C(U
that C(U, U ) is positive semideﬁnite. Morever, since each node in U is adjacent to at least one edge
with a negative weight each entry of C(U, U )e is positive. This implies that eT C(U, U )e is positive
which in turn implies that C(S, S) is invertible. Using the Schur complement condition for positive

∪

semideﬁniteness, C is positive semideﬁnite if and only if C(U, U )

semideﬁnite. Substituting for C(U, S) and C(S, U ), we get

C(U, S)C(S, U )
|
eT C(U, U )e

S

−

|

is positive

C(U, U )

C(U, S)C(S, U )
|
eT C(U, U )e

S

−

|

= C(U, U )

= C(U, U )

−

−

C(U, U )eeT eeT C(U, U )

|

|

S

eT C(U, U )e
C(U, U )eeT C(U, U )
eT C(U, U )e

= C(U, U )1/2

I
(cid:18)

−

C(U, U )1/2eeT C(U, U )1/2
eT C(U, U )e

C(U, U )1/2

(cid:19)

where the last line from bottom uses eT e =
and the last line uses the positive semideﬁniteness
of C(U, U ). Therefore to argue for the positive semideﬁniteness of the last term in the above chain,

S

|

|

it sufﬁces to show that I

is positive semideﬁnite. This follows from

−

C(U, U )1/2eeT C(U, U )1/2
eT C(U, U )e
C(U, U )1/2eeT C(U, U )1/2
eT C(U, U )e

simply noticing that

positive semideﬁnite.

is a rank-one matrix with eigenvalue 1. Thus C is

To show that L
non-negative weights. First notice that Ce = 0. Indeed, we have

C is also positive semideﬁnite, we show that it is the Laplacian of a graph with

−

Ce =

where

C(U, U )e + C(U, S)e
C(S, U )e + C(S, S)e
0

#

"

C(U, U )e + C(U, S)e = C(U, U )e

C(U, U )e = 0

−

using the construction of C(U, S), and

C(S, U )e + C(S, S)e = −

eeT C(U, U )e
S

|

|

+

eT C(U, U )ee
S

|

|

= 0

using the constructions of C(S, U ) and C(S, S). Subsequently, using the fact that Le = 0, we
conclude that (L

C)e = 0.

−
Deﬁning set R := V

(U

\

∪

S), we now show that each off-diagonal entry of

L(U, U )
L(S, U )

C(U, U ) L(U, S)
C(S, U ) L(S, S)

C(U, S) L(U, R)
C(S, S) L(S, R)

C =

L

−

"

−
−
L(R, U )

−
−
L(R, S)

L(R, R)#

12

is non-positive. For each u, u′
S, we have

∈

U , we have Luu′

−

Cuu′ = 0 by construction. For each u

U, s

∈

∈

Lus −

Cus = Lus +

= Lus +

1
S

| Xu′∈U
+

|
Cuu
S

|

|

|

Cuu′

(by construction of C(U, S))

1
S

Cuu′

| Xu′∈U\{u}

= Lus +

= Lus +

1
S

1
S

|

|

=

Wus −

−

0.

≤

Luu′

|

|

+ Luu′

|





Xu′∈U\{u}

2Luu′

(by construction of C(U, U ))





′

| Xu
∈U\{u}:
Luu′ >0
1
S

′

|

| Xu

∈U\{u}:
Wuu′ <0

2Wuu′

(∵ L = L(G))

(using (1))

−

Now it remains to consider the signs of the entries of L(V, R) and the off-diagonal entries of
C(S, S). Observe that each entry of L(V, R) is non-negative since each entry of W (V, R)
L(S, S)
is non-positive. Indeed any entry in W (V, R) corresponds to an edge whose one endpoint lies in
R; note that for a negative edge, both endpoints lie in U by deﬁnition. Lastly, every off-diagonal
entry of L(S, S)
C(S, S) is non-negative since such entries are 0 in C(S, S), by construction, and
non-negative in L(S, S) since they correspond to edges whose both endpoints lie in S.

−

C is a Laplacian matrix for a graph with non-negative weights,
Therefore we have shown that L
and is consequently positive semideﬁnite. Since we have shown the positive semideﬁniteness of
both C and L

C, we conclude that L is positive semideﬁnite.

−

−

3.4 Lack of Robustness

As discussed in Section 3.2, we are interested in understanding the robustness of 1-diag recovery
algorithm in the presence of noisy nodes, i.e.
fringe nodes that are close to being stray nodes.
However, through computational experiments, it is observed that 1-diag seems to have undesirable
In particular, there exist pathological instances in which the output of
behavior in this setting.
1-diag contains groups of noisy nodes as spurious cluster. The following example further illustrates
this phenomenon.
Example 3.2. Consider a graph G on n = 25 nodes containing k = 3 clusters. Suppose G has a
cluster j containing 6 nodes and the three-dimensional features of these nodes are as shown in the
rows of the 6

3 matrix below.

×



1.00 0.00 0.00
0.79 0.00 0.21
1.00 0.00 0.00
0.53 0.47 0.00
0.53 0.47 0.00
0.51 0.49 0.00















The submatrix of the output of 1-diag corresponding to the nodes in cluster j is

1
1

1
0


0

0




1 0
1 0
1 0
0 1
0 1
0 1
The above matrix is not the matrix of all ones. In fact, it breaks down the true cluster into two
clusters by creating one cluster each for strong and fringe nodes thereby creating a spurious cluster
made up of the fringe nodes.

0 0
0 0
0 0
1 1
1 1
1 1

1
1
1
0
0
0










.

13

This apparent limitation of 1-diag demotivates a theoretical analysis of cluster recovery using frac-
tional optimal solutions of (P-1D).

4 ℓ2-norm-diag Recovery Algorithm

We propose SDP formulation (P-ND) obtained by replacing the n diagonal constraints of (P-1D)
with a single ℓ2-norm constraint. Based on this formulation, we propose a novel recovery algorithm,
called ℓ2-norm-diag, which involves no tuning parameter.

Algorithm 3 ℓ2-norm-diag
Input: Graph G = (V, W ) generated according to NFM
Output: Symmetric matrix X c of the same dimension as W whose each entry is in
1: X ∗ = arg max
s.t. X
diag(X)
2: for t in entries of X ∗ sorted in non-increasing order do
3: X c
4:

i
ij = Round(X ∗, t)

if there exists a non-empty V ′

W, X
h

0, X

k ≤

0,

≥

(cid:23)

1

k

partition of V ′ then

⊆

0, 1

}

{

V such that X c(V ′, V ′) is the cluster matrix for some

break

else

5:
6:
7:
end if
8:
9: end for

X c = 0

Note that the output of ℓ2-norm-diag can possibly be the zero matrix and therefore does not deﬁne
a clustering for the input graph or any of its subgraphs. However, the theory developed in Section
4.1 provides conditions on the input graph sufﬁcient for the output of ℓ2-norm-diag to induce a
clustering for some subgraph of the input graph.

max
X

W, X
h
s.t. X
X

i
0
0
diag(X)

≥
(cid:23)

k

k ≤

(P-ND)

1.

SDPs (P-1D) and (P-ND) both have the non-negativity and positive semideﬁniteness constraints on
,
the variable matrix. The combination of these constraints, i.e.
}
forms the so-called doubly non-negative (DNN) cone. This cone has been studied in the context of
SDP relaxations for other graph problems such as the minimum cut problem Li et al. [2021] and the
quadratic assignment problem Hu et al. [2019], Oliveira et al. [2018].

X : X

the set

0, X

≥

(cid:23)

{

0

The rounding procedure in ℓ2-norm-diag is based on the observation from computational exper-
iments that the entries of an optimal solution of (P-ND) corresponding to the recovered clusters
are larger compared to, and therefore well-separated from, the rest of the entries. This implies the
existence of a ﬁxed threshold for rounding; however, computational experiments also suggest the de-
pendence of this rounding threshold on problem parameters n, k and α. An algorithmic dependence
on k and α is undesirable, especially in Correlation Clustering, since these parameters are latent and
encode information about the number and size of clusters in the graph. It is not clear to us whether
a ﬁxed-threshold-based rounding procedure exists which does not require prior estimate of k and α,
and this motivates the rounding procedure in ℓ2-norm-diag which adapts to matrix being rounded.

4.1 Theoretical Guarantees

Similar to our approach for the 1-diag recovery algorithm, we ﬁrst prove exact cluster recovery
under deterministic conditions on the input graph, followed by understanding the validity of these
deterministic conditions for the NFM, and the robustness properties of (P-ND).
Assumption 4.1. Suppose G = (V, W ) be a graph on n nodes where W = qqT
V contain all nodes of G adjacent to a negative edge. That is, U :=
U
v
0 for some w

. Suppose the following hold.

D + N . Let
V : Wvw <

−
∈

⊆

V

{

∈

}

14

1. q is a positive n-dimensional vector satisfying

max(q◦4/3)

q◦2/3
45

k

2

k

.

≤

Intuitively, this condition is likely to hold if the smallest entry in q is not too small.

2. There exists S

V

⊆

\

U such that for each u

U and s

∈

∈

S, we have

S

|

q1/3
s Wus ≥ −

|

6 

q1/3
u′ Wuu′ 

.

′

∈U:
Xu
Wuu′ <0









3. N is a n
satisﬁes

×

n symmetric matrix whose each diagonal entry is zero and, for each i

[n],

∈

(Recall the notation that for each i

∈

ni

k

2

k ≤

qik
45
k

q◦2/3
k
q◦1/3
k
[n], ni denotes row i of matrix N .)

.

4. D = Diag(q

q). This is a diagonal correction matrix chosen to ensure that the diagonal
of W is indeed zero, as deﬁned. Unlike (P-1D), the analysis of (P-ND) depends on the
diagonal entries of W , and therefore it is reasonable to assume each of them to be 0.

◦

Theorem 4.1. Let G = (V, W ) be a graph generated using the NFM, and suppose that for each
[k], W (Vj, Vj ) satisﬁes Assumption 4.1. Then (P-ND) has an optimal solution X ∗ satisfying
j
∈
X ∗
ii′ > 0 if and only if nodes i and i′ belong to the same cluster.

In terms of proof techniques, unlike the SDP (P-1D), (P-ND) does not lend itself to an explicit
construction of primal-dual optimal solutions and requires a more elaborate argument using Brouwer
ﬁxed-point theory.

Adopting the two-step approach outlined in Section 3.1, we are now interested in the following two
questions:

• What is the probability that, for each cluster, the subgraph induced by the union of strong

and some fringe nodes satisﬁes Assumption 4.1?

• Is the ℓ2-norm-diag recovery algorithm robust to the presence of noisy nodes, i.e. fringe

nodes that are close to being stray?

While we do not provide a precise answer to the ﬁrst question above, we demonstrate, computation-
ally, the connection between Assumption 4.1 and the NFM in which the distribution over the unit
simplex is chosen to be the Dirichlet distribution. For a graph G = (V, W ) generated according to
NFM, for each j
j to be the union of strong nodes and some fringe nodes in cluster j
such that the cut-off for selecting fringe nodes depends on problem parameters k and α. Let nj be
the cardinality of V ′
k matrix whose rows contain the feature vectors
R deﬁned as
corresponding to the nodes in V ′
R deﬁned as
g(x) := log(x/(1
l(x) := c

j . Observe that the univariate function g : (0, 1)
x)) can be approximated using a linear function l : (0, 1)

1) for a suitably chosen positive constant c. Then for each j

j , and let Θj denote the nj ×

[k], deﬁne V ′

[k], we have

→
→

(2x

−

∈

·

−

∈

W (V ′

j , V ′
j )

c

·

≈

(2ΘjΘT

j −

E).

(3)

Now through various computational experiments, we notice that the matrix c
E) is
almost a rank-one matrix such that eigenvector corresponding to the largest eigenvalue is a positive
vector. The following example concretely illustrates these observations.

j −

·

(2ΘjΘT

Example 4.1. Consider a graph generated using the NFM with n = 30 and k = 3. The distribution
over the unit simplex is chosen to be the Dirichlet distribution with parameter α = 0.3e. For some
cluster j, let V ′
j be the union of strong nodes and fringe nodes whose feature vectors have largest

15

entry at least 0.6. The three-dimensional features of the nodes in V ′
matrix Θj below.

j are shown in the rows of the

Θj =

0.05 0.83 0.11
0.04 0.69 0.27
0.03 0.92 0.05
0.02 0.73 0.25
0.11 0.88 0.01
0.25 0.60 0.15
0.00 0.99 0.01
0.12 0.67 0.21
0.01 0.95 0.04





























We notice that the matrix c
by 8.57, 0.25 and
is

−

(2ΘjΘT

E) with c = 2.2 has exactly three non-zero eigenvalues given
0.75. Moreover the unit eigenvector, vj, corresponding to the eigenvalue 8.57

j −

·

vj =

which has all positive entries.

0.33
0.17
0.43
0.22
0.38
0.06
0.51
0.15
0.45





























The observations made above regarding the spectral properties of the matrix 2.2
E)
are further shown to be consistent using the results in Table 3, 4, and 5. Each row in these tables
corresponds to 10 cluster instances generated using the NFM in which the simplex distribution is
chosen to be the Dirichlet distribution. We ﬁx k = 3 and the Dirichlet parameter α = 0.3e. The
ﬁrst column denotes the range corresponding to the size of the subgraph induced by strong nodes
and fringe nodes whose feature vectors have largest entry at least 0.6; the cut-off of 0.6 is based on
manual parameter search for the given setting of k and α. The second column counts eigenvector
success, i.e. the number of instances, out of 10, for which the eigenvector of 2.2
E)
corresponding to its largest eigenvalue is positive. For such instances, the third column contains the
non-zero eigenvalues of 2.2

(2ΘjΘT

j −

j −

E).

(2ΘjΘT

·

·

(2ΘjΘT

·

j −

These computational results motivate the following conjecture.
Conjecture 4.2. Let G = (V, W ) be a graph generated using the NFM in which the simplex dis-
tribution is chosen to be the Dirichlet distribution with constant parameter α. Then there exists a
scalar t(k, α)
[k], with probability not converging to 0 as
∈
, the largest eigenvalue of the matrix 4.4ΘjΘT
2.2E is well-separated from the remaining
n
j −
eigenvalues and the corresponding eigenvector is positive where
t(k, α)
}

(0.5, 1/√2) such that for each j

[n] : θi

V ′
j :=

→ ∞

j ≥

∈

∈

{

i

and

Θj := Θ(V ′

j , :).

Now let qj :=
eigenvector respectively of c

p

λjvj where λj and vj denote the largest eigenvalue and the corresponding unit

·

(2ΘjΘT

j −
j , V ′

W (V ′

E). We rewrite (3) as

j ) = qjqT

Dj + Nj

j −
nj diagonal matrix Diag(qjqT
j ) and Nj is the nj ×

nj symmetric matrix
where Dj is the nj ×
whose each diagonal entry is equal to 0 and the off-diagonal entries are chosen to make so as to
make (3) hold. That is, matrix Dj applies diagonal correction to ensure diag(W (Vj , Vj)) = 0 and
matrix Nj captures the error in approximating the logarithmic function g by the linear function l and
E) by qjqT
the error in approximating c
j . We show, using computational results, the
validity of Assumption 4.1 for quantities qj, Nj, W (Vj , Vj) described using (4) in Table 6. Each

(2ΘjΘT

j −

·

(4)

16

Table 3: Structure of subgraph induced by strong nodes and some fringe nodes for each cluster (part
1/3).

Cluster size range Eigenvector success (/10) Non-zero eigenvalues

10

6

−

11

−

15

16

−

20

10

10

10

7.8, 0.5,
9.8, 0.4,
9.2, 0.2,
10.1, 0.4,
10.1, 0.9,
8.5, 0.1,
12.7, 0.2,
6.6, 0.5,
9.8, 0.4,
7.4, 0.3,

1.1
0.7
0.4
0.3
0.6
0.3
0.6
0.4
0.5
0.4

−
−
−
−
−
−
−
−
−
−

16.8, 0.2,
14.2, 0.4,
12.9, 0.6,
16.7, 0.6,
15.8, 0.3,
15.9, 0.8,
16.1, 0.2,
14.0, 0.9,
12.8, 0.5,
14.0, 0.3,

16.3, 1.0,
18.1, 0.7,
17.0, 0.7,
21.5, 1.2,
14.5, 1.2,
21.2, 0.6,
19.4, 1.2,
21.6, 0.3,
23.5, 1.0,
20.1, 0.6,

0.5
0.4
0.5
0.7
0.5
0.8
0.4
0.8
0.4
0.5

1.3
0.9
0.7
1.9
1.3
0.7
1.0
0.6
1.0
1.0

−
−
−
−
−
−
−
−
−
−

−
−
−
−
−
−
−
−
−
−

row in the table corresponds to 10 cluster instances generated using the NFM in which the simplex
distribution is chosen to be the Dirichlet distribution. We ﬁx k = 3 and the Dirichlet parameter
α = 0.3e. The ﬁrst column denotes the range corresponding to the size of the subgraph induced by
strong nodes and fringe nodes whose feature vectors have largest entry at least 0.6; the cut-off of 0.6
is based on manual parameter search for the given setting of k and α. The second column counts
j , V ′
C1, C2 success, i.e. the number of instances, out of 10, for which vector qj and matrix W (V ′
j )
as highlighted in (4) satisfy conditions 1 and 2 in Assumption 4.1. We notice that condition 3 is not
satisﬁed for most instances; however, the violation is by a constant factor in the sense that the ratio

45

q◦1/3
2

kk
q◦2/3

ni
k
qik

k

k

(5)

∈

for each i
[n], is bounded above by a constant, albeit much larger than 1 as desired. Therefore in
the fourth column of Table 6, we present average C3 upper bound, i.e. the quantity (5) ﬁrst averaged
over all nodes in the graph, then averaged over the instances out 10 runs in which both conditions 1
and 2 are satisﬁed.

These computational results partly justify Assumption 4.1. Now we turn to the robustness aspect,
i.e. understanding the robustness of ℓ2-norm-diag to the presence of noisy nodes. We begin
by revisiting Example 3.2 mentioned in Section 3.4. In particular, the submatrix of the output of

17

Table 4: Structure of subgraph induced by strong nodes and some fringe nodes for each cluster (part
2/3).

Cluster size range Eigenvector success (/10) Non-zero eigenvalues

21

−

25

26

−

30

31

−

35

10

10

10

26.2, 1.7,
21.6, 1.5,
23.9, 1.1,
18.3, 1.6,
26.5, 0.9,
23.0, 2.5,
23.8, 1.6,
25.6, 1.0,
20.0, 2.0,
23.2, 1.4,

28.6, 1.2,
34.0, 2.0,
32.2, 1.3,
29.0, 2.4,
30.0, 1.6,
30.6, 1.7,
29.9, 1.4,
35.9, 1.3,
23.0, 1.7,
29.1, 1.5,

38.8, 1.5,
33.7, 1.8,
38.4, 2.3,
31.9, 1.9,
38.3, 1.7,
33.6, 1.9,
36.3, 2.2,
43.7, 1.1,
29.0, 2.8,
46.7, 0.9,

1.8
1.4
1.2
1.5
1.2
3.0
1.7
1.0
2.2
1.5

1.9
2.0
1.6
2.2
1.9
1.8
1.4
1.3
1.4
1.5

1.5
1.8
2.2
1.7
1.9
2.2
1.9
1.2
3.2
1.5

−
−
−
−
−
−
−
−
−
−

−
−
−
−
−
−
−
−
−
−

−
−
−
−
−
−
−
−
−
−

Table 5: Structure of subgraph induced by strong nodes and some fringe nodes for each cluster (part
3/3).

Cluster size range Eigenvector success (/10) Non-zero eigenvalues

36

−

40

10

18

35.6, 1.8,
44.1, 2.0,
40.2, 2.9,
40.2, 2.5,
44.5, 1.2,
49.8, 1.6,
33.5, 2.2,
35.1, 2.1,
46.3, 2.0,
40.1, 1.8,

2.3
2.5
2.8
2.6
2.0
1.5
2.6
2.1
2.1
1.9

−
−
−
−
−
−
−
−
−
−

Table 6: Veriﬁcation of Assumption 4.1

Cluster size range C1, C2 success (/10) Average C3 upper bound

1201
1301
1401
1501
1601
1701
1801

−
−
−
−
−
−
−

1300
1400
1500
1600
1700
1800
1900

4
4
3
4
5
8
3

13.3
14.1
13.2
13.3
13.4
13.5
13.6

ℓ2-norm-diag corresponding to the nodes in cluster j is

.



1
1
1
0
0
0

1
1

1
0


0

0




0 0
0 0
0 0
0 0
0 0
0 0

1 0
1 0
1 0
0 0
0 0
0 0
This shows that ℓ2-norm-diag is correctly able to cluster the strong nodes, for this example, de-
spite the presence of fringe nodes without creating spurious clusters using the fringe nodes. This
observation motivates the following results which show the robustness of the diagonal of an optimal
solution of the SDP (P-ND) to perturbations of the weighted adjacency matrix W .
Theorem 4.3. Let X ∗ be an optimal solution of the SDP (P-ND). If W contains at least one positive
entry, then diag(X ∗) is uniquely determined by W .
Theorem 4.4. Let X ∗ and X ′ be optimal solutions to the SDP (P-ND) for weighted adjacency
matrices W and W + ∆ respectively. If each of W and W + ∆ contains at least one positive entry,
then








diag(X ∗)

k

−

diag(X ′)

k ≤

2 (2n)1/4
∆
k
k
1/2
W, X ∗
i
h

1/2
F

.

While Theorem 4.4 shows the robustness of the diagonal of an optimal solution of (P-ND) to only
perturbations of the weighted adjacency matrix, it is, in fact, observed using computational experi-
ments that all entries of an optimal solution are robust to the presence of fringe nodes whose feature
vectors have a relatively smaller largest entry, i.e. fringe nodes that are close to being stray nodes.
In particular, the entries of an optimal solution corresponding to the cluster subgraphs comprised
of strong nodes and fringe nodes close to the strong set are larger compared to, and therefore well-
separated from, the rest of the entries. This is supported by computational results shown in Table
7 which show the performance of ℓ2-norm-diag. Each row in the table corresponds to 10 graph
instances generated using the NFM in which the simplex distribution is chosen to be the Dirichlet
distribution. We ﬁx k = 3 and the Dirichlet parameter α = 0.3e. The ﬁrst column denotes the
size of graph, i.e. number of nodes n. The second column counts the ℓ2-norm-diag success, i.e.
the number of instances, out of 10, for which the number of recovered clusters is equal to the true
number of clusters k such that the recovered clusters are disjoint and each recovered cluster contains
all strong nodes (and possibly some fringe nodes) from exactly one ground-truth cluster.

The theoretical and computational results regarding the performance of ℓ2-norm-diag presented in
this section lead us to make the following conjecture.
Conjecture 4.5. Let G be a graph generated according to NFM in which the simplex distribution is
chosen to be the Dirichlet distribution with constant parameter. Then with probability not converging
to 0 as n
k such that, for each
[k]
j

, ℓ2-norm-diag returns exactly k disjoint clusters V ′

1 , . . . , V ′

→ ∞

∈

V strong
j

V ′
j .

⊆

4.2 Proofs

In this section we build a proof of Theorem 4.1. Our strategy is to demonstrate the desired structure
in each of the submatrices of an optimal solution corresponding to a cluster. One key ingredient for

19

Table 7: Performance of ℓ2-norm-diag.

Graph size (number of nodes)

60
70
80
90
100
110
120
130
140

ℓ2-norm-diag success (/10)
9
8
10
9
9
10
9
10
10

this approach is to determine a point x such that

However, note that the exact solution to the system

(W x)◦1/3 = x.

(qqT x)◦1/3 = q
can be shown to be x = (βq)◦1/3 where β = [qT (q◦1/3)]3/2. Moreover, due to Assumption 4.1,
W can be interpreted as a perturbation of the matrix qqT by matrices D and N thereby motivating
the following lemma.
Lemma 4.6. Let G = (V, W ) be a graph on n nodes satisfying conditions 1, 3, 4 in Assumption 4.1.
Then the continuous function f : Rn

Rn deﬁned as f (x) := (W x)◦1/3 maps the set

S :=

1
2

(cid:20)

to itself.

(βq)◦1/3,

→
3
2

(βq)◦1/3

, where β = [qT (q◦1/3)]3/2,

(cid:21)

Proof. We will show that for any x
holds separately. Pick any x
(W x)◦1/3 = (qqT x

∈
S. We have
Dx + N x)◦1/3

∈

S, each of f (x)

(βq)◦1/3/2 and f (x)

3(βq)◦1/3/2

≤

≥

−

β1/3qqT (q◦1/3)
2

βq
2 −

Dx + N x
(cid:21)

βq
2 −

3β1/3D(q◦1/3)
2

βq
2 −

3β1/3q◦7/3
2

≥

=

≥

≥

(cid:20)

(cid:20)

(cid:20)

(cid:20)

◦1/3

Dx + N x
(cid:21)

−
◦1/3

◦1/3

+ N x
(cid:21)
◦1/3

(using the lower bound on x)

(using the deﬁnition of β)

(6)

(using the upper bound on x)

+ N x
(cid:21)

.

(using the deﬁnition of D)

Now we bound each of the second and third terms above separately. Condition 1 in Assumption 4.1
implies for each i

[n],

∈
q7/3
i ≤

2qi

q◦2/3
45

k

k

q4/3
i

qi

!

i∈[n]
P

45
[qT (q◦1/3)]qi
45

β2/3qi
45

.

(using the deﬁnition of β)

20

=

=

=

 
The above chain implies that

q◦7/3

3β1/3q◦7/3
2

⇐⇒

β2/3q
45
βq
30

.

≤

≤

Moreover, for each i

[n], we have

∈
(N x)i|

|

=

|

2

nix
|
ni
x
kk
k
q◦2/3
qik
x
k
q◦1/3
45
k
k
k
k
q◦2/3
2β1/3
qik
k
30
βqi
30

.

≤ k

≤

≤

=

(multiplying both sides by 3β1/3/2)

(using Cauchy-Schwarz inequality)

(using condition 3 in Assumption 4.1)

(using the upper bound on x)

(using the deﬁnition of β)

Then using (7) and (8) in (6), we get

(W x)◦1/3

≥

=

>

For any x

∈

S, we also have

(W x)◦1/3 = (qqT x

Dx + N x)◦1/3

−

βq
2 −

βq
15

1/3

◦1/3

(cid:19)

(βq)◦1/3

13
30

(cid:18)

(cid:18)
1
2

(cid:19)
(βq)◦1/3.

(7)

(8)

(9)

3β1/3qqT (q◦1/3)
2

Dx + N x
(cid:21)

−
1/3

◦1/3

(using the upper bound on x)

Dx + N x
(cid:21)

◦1/3

(using the deﬁnition of β)

(10)

+ N x
(cid:21)
(βq)◦1/3

1/3

(∵ Dx > 0)

(using (8))

(cid:20)

(cid:20)

(cid:20)

≤

=

≤

≤

<

3βq

2 −

3βq
2

23
15

(cid:19)
(βq)◦1/3.

(cid:18)
3
2

Combining (9) and (10), we conclude that the function f maps S to itself.

To proceed with the proof of Theorem 4.1, in addition to Lemma 4.6, we also make use of the
following Brouwer ﬁxed-point theorem.

Theorem 4.7 (Brouwer Fixed-Point Theorem Brouwer [1912]). Let C
compact set and let f : C
f (x) = x.

C be a continuous function. Then there exists a point x

→

⊆

Rn be a non-empty convex
C such that

∈

[k], let qj, Dj, Nj, Uj and Sj denote the quantities mentioned
Proof of Theorem 4.1. For each j
in Assumption 4.1, and deﬁne nj as the cardinality of Vj (i.e. the size of cluster j). Our analysis

∈

21

uses SDP duality and therefore note that the dual of (P-ND) is

min
(X,Y,Z,λ)

s.t.

2 + λ

diag(X)
k

λ
k
Y
Z
λ
W + Y + Z = λ

≥
(cid:23)
≥

0
0
0

(D-ND)

Diag(X).

·

Both optimization problems (P-ND) and (D-ND) have strictly feasible solutions. For instance, X ′ :=
(0.5I + 0.5E)/n is a positive, positive deﬁnite matrix which is feasible for (P-ND). Similarly, X ′ :=
I, Y ′ := E, Z ′ := (
(W + E) and λ′ := (
+ ǫ) gives a strictly feasible
k
k
solution (X ′, Y ′, Z ′, λ′) for (D-ND) for any ǫ > 0. Therefore using the Karush-Kuhn-Tucker (KKT)
conditions for optimality, X ∗ is an optimal solution for (P-ND) if and only if X ∗ is feasible for
Sn, a positive semideﬁnite matrix Z ∗, and a
(P-ND) and there exist a non-negative matrix Y ∗
non-negative scalar λ∗ such that:

W + E

W + E

+ ǫ)I

−

∈

k

k

∀
= 0

• X ∗

ijY ∗

ij = 0,

i, j

[n]

∈

•

X ∗, Z ∗
h
• λ∗

i

(
k

diag(X ∗)

·

k −
• W + Y ∗ + Z ∗ = λ∗

1) = 0

Diag(X ∗)

·

In the remainder of the proof, we will explicitly construct all the above mentioned quantities. Now
for each j
[k], since W (Vj , Vj) satisﬁes Assumption 4.1, using Lemma 4.6 and Theorem 4.7,
we conclude that there exists an nj-dimensional vector rj ∈
βj = [qT

)]3/2, such that

(βjqj )◦1/3,

(βjqj)◦1/3

, where

1
2

3
2

∈

(cid:21)

(cid:20)

j (q◦1/3

j

(11)

We set

For each j

[k], we set

∈

W (Vj, Vj )

rj = r◦3
j .

·

λ∗ =

s Xj∈[k]

rj ◦

k

rjk

2.

j /λ∗

X ∗(Vj , Vj) = rjrT
Y ∗(Vj , Vj) = 0
Z ∗(Vj , Vj) = Diag(rj ◦

rj)

−

W (Vj , Vj).

For each distinct j, j′

[k], we set

∈

X ∗(Vj , Vj′ ) = 0
Y ∗(Vj , Vj′ ) =
−
Z ∗(Vj , Vj) = 0.

W (Vj, Vj′ )

For each stray node v, we set

X ∗(v, :) = 0
Y ∗(v, :) =
−
Z ∗(v, :) = 0.

W (v, :)

(and X ∗(:, v) = 0)
(and Y ∗(:, v) =
−
(and Z ∗(:, v) = 0)

W (:, v))

First we show that the constructed X ∗ is feasible for (P-ND). Note that there exists a permutation of
the rows (and columns) of X ∗ which yields a block diagonal matrix in which the non-zero blocks
k /λ∗. This shows that
are given by the rank-one positive semideﬁnite matrices r1rT

1 /λ∗, . . . , rkrT

22

X ∗ is positive semideﬁnite. Morever, since vectors r1, . . . , rk are positive, we conclude that X ∗ is
non-negative. We also have

diag(X ∗)
k

k

j∈[k] k

2 = P
= 1.

rj ◦
λ∗2

2

rjk

(using the deﬁnition of λ∗)

Therefore X ∗ is feasible for (P-ND). Note that this also implies that λ∗(
k
Now we show that the constructed Y ∗, Z ∗, λ∗ satisfy the remaining desired properties. Because
each pair of nodes lying in distinct clusters shares a negative edge and because each stray node
0. Also note that λ∗
shares a negative edge with every other node in the graph, we have that Y ∗
is a positive scalar by construction.
Matrices X ∗ and Y ∗ have disjoint supports by construction, and therefore X ∗
i, j

[n]. Moreover, for each j

[k], using (11), we have

ij = 0 for each

diag(X ∗)

1) = 0.

ijY ∗

k −

≥

∈

∈

rj)

·

·
·

W (Vj, Vj )
W (Vj, Vj )
Z ∗(Vj , Vj)
Z ∗(Vj , Vj)
Z ∗(Vj , Vj)
·
Z ∗(Vj, Vj ), X ∗(Vj , Vj)
i

rj = r◦3
j
rj = Diag(rj ◦
rj = 0
rjrT
j /λ∗ = 0
X ∗(Vj , Vj) = 0
= 0

·

⇐⇒
⇐⇒

⇐⇒

⇐⇒
⇐⇒ h

rj

·

(using the deﬁnition of Z ∗)
(∵ rj > 0, λ∗ > 0)
(using the deﬁnition of X ∗)
(∵ X ∗, Z ∗
0)

(cid:23)

(12)

Therefore we have

X ∗, Z ∗
h

i

=

Xj∈[k]

= 0.

X ∗(Vj , Vj), Z ∗(Vj , Vj)
i
h

(using the deﬁnitions of X ∗, Z ∗)

(using (12))

Note that there exists a permutation of the rows (and columns) of Z ∗ which yields a block
diagonal matrix in which the non-zero blocks are given by the matrices Diag(r1 ◦
W (V1, V1), . . . , Diag(rk ◦
Z ∗, it sufﬁces to show that for each j
positive semideﬁnite. From (12), we know that Z ∗(Vj , Vj)
the null space of Diag(rj)

−
W (Vk, Vk). Therefore to show the positive semideﬁniteness of
W (Vj, Vj) is
rj = 0. This implies that e belongs to

[k], the matrix Z ∗(Vj , Vj ) = Diag(rj ◦
·
Diag(rj). Consequently, we observe that

∈
Z ∗(Vj , Vj)

rk)

r1)

rj)

−

−

·

·

¯Lj := Diag(rj)

Z ∗(Vj , Vj)

·

Diag(rj)

·

is the Laplacian matrix of a graph, called ¯Gj, on nj nodes whose weighted adjacency matrix is

¯Wj := Diag(rj)

W (Vj , Vj)

·

Diag(rj).

·

Moreover, Z ∗ is positive semideﬁnite if and only if the Laplacian ¯Lj is positive semideﬁnite since
each entry of rj is positive. Note that the sign of each edge in ¯Gj is identical to that of the corre-
sponding edge in G[Vj] which implies that the set of all nodes in ¯Gj adjacent to a negative edge is

23

Uj. Now for any u
Uj and s
∈
∈
¯Wj(u, s) =
Sj|

|

Sj, we have
Sj|
Sj|

rj(u)rj (s)W (u, s)
[βjqj(s)]1/3
2

rj(u)

|

≥ |

W (u, s)

(using the lower bound on rj )

3β1/3
j

≥ −








′

rj(u)qj(u′)1/3W (u, u′)





∈Uj :
Xu
′
W (u,u
(using condition 2 in Assumption 4.1)

)<0

≥

2 





′

∈Uj :
Xu
′
W (u,u

rj (u)rj(u′)W (u, u′)





)<0
(using the upper bound on rj )

= 2 





′

Xu
∈Uj :
′
¯Wj (u,u

.

¯Wj (u, u′)


)<0


(using the deﬁnition of ¯W )

Thus we have shown that graph ¯Gj satisﬁes (1) stated in Theorem 3.4 using which we conclude that
¯Lj is positive semideﬁnite.
Lastly, we show that the equation W + Y ∗ + Z ∗ = λ∗
have

Diag(X ∗) is satisﬁed. For each j

[k], we

∈

·

W (Vj , Vj) + Y ∗(Vj , Vj) + Z ∗(Vj, Vj ) = Diag(rj ◦

rj)
(using the deﬁnitions of Y ∗, Z ∗)

= λ∗

·

Diag(X ∗(Vj , Vj)).

(using the deﬁnition of X ∗)

For each distinct j, j′

[k], we have

∈

W (Vj, Vj′ ) + Y ∗(Vj, Vj′ ) + Z ∗(Vj, Vj′ ) = 0
using the deﬁnitions of Y ∗, Z ∗. Similarly, for each stray node v, we have

W (v, :) + Y ∗(v, :) + Z ∗(v, :) = 0
W (:, v) + Y ∗(:, v) + Z ∗(:, v) = 0

using the deﬁnitions of Y ∗, Z ∗.

Proof of Theorem 4.3. Observe that X = 0 is a feasible solution for (P-ND) which implies that the
optimal value of (P-ND) is non-negative. This implies that for any optimal solution, without loss
1 is tight. Since X ∗ is optimal for
of generality, we may assume that the constraint
k
(P-ND), and since both (P-ND) and its dual have strictly feasible solutions, using the Karush-Kuhn-
Sn, a positive
Tucker (KKT) conditions for optimality, there exist a non-negative matrix Y ∗
semideﬁnite matrix Z ∗, and a non-negative scalar λ∗ such that:

diag(X)

k ≤

∈

∀
= 0

• X ∗

ijY ∗

ij = 0,

i, j

[n]

∈

•

X ∗, Z ∗
h
• λ∗

(
k

·

i

diag(X ∗)

k −

1) = 0

24

• W + Y ∗ + Z ∗ = λ∗

Diag(X ∗)

·

We also note that λ∗ is a positive scalar. Indeed if λ∗ is zero, then the last condition above implies
diag(Z ∗) is zero and consequently Z ∗ = 0 since Z ∗ is positive semideﬁnite. This implies that
W which contradicts the non-negativity of Y ∗ since W contains a positive entry.
Y ∗ =
Let X ∗∗ be another optimal solution of (P-ND). Then we have

−

−

W, X ∗
0 =
h
λ∗
=
h
= λ∗
= λ∗
λ∗

X ∗∗
i
Diag(X ∗)
λ∗
λ∗
λ∗

−

−
Diag(X ∗), X ∗∗
Diag(X ∗), X ∗∗
Diag(X ∗), X ∗∗

Y ∗

·
−
−
−

· h
· h
· h

Z ∗, X ∗

i

X ∗∗
−
Y ∗ + Z ∗, X ∗
i − h
Y ∗ + Z ∗, X ∗∗
+
i
h
.
i

−
i

≥

= 0)
i
0)
i ≥
Using the fact that λ∗ is positive, the above implies that
1. How-
ever, since both diag(X ∗) and diag(X ∗∗) lie on the unit sphere, we conclude that diag(X ∗) =
diag(X ∗∗).

X ∗∗

i

(substituting for W )
diag(X ∗)
(∵
= 1)
k
k
Z ∗, X ∗
Y ∗, X ∗
(∵
=
h
h
i
Y ∗, X ∗∗
Z ∗, X ∗∗
(∵
,
h
i
h
diag(X ∗), diag(X ∗∗)
h

i ≥

Proof of Theorem 4.4. Note that since W has at least one positive entry, max(W+) is a positive
scalar. If Wii′ > 0 for some i, i′
i )/√2 is feasible for (P-ND) and we have
(13)

i′ + ei′eT
√2Wii′ > 0.

∈

[n], then (eieT
W, X ∗
h

i ≥
Using a similar argument, we also conclude that

W + ∆, X ′
h
Moreover since the optimal values of the two programs are positive, we have that
diag(X ∗)
k

diag(X ′)
k

= 1.

> 0.

=

k

i

kF
diag(X ∗)
k

(using Cauchy-Schwarz inequality)
(∵
(∵

diag(X ∗)
k

X ∗
kF ≤
diag(X ∗)
k

k
= 1)

√n

k

k

)

Observe that

∆, X ∗

|h

i| ≤ k

∆
√n
≤
= √n

k
X ∗

kF k
∆
kF k
k
kF .
∆

k

Similarly, we also have that

Now deﬁne

∆, X ′

|h

√n

kF .
∆

k

i| ≤

X ′′ :=

X ∗ + X ′
diag(X ∗) + diag(X ′)
k

.

k

Noting that X ′′ is feasible for (P-ND), and therefore using the fact that

W, X ∗
h

ik

diag(X ∗) + diag(X ′)

(14)

(15)

(16)

, we get
i

W, X ∗
W, X ∗
h
W, X ∗

+

+

+

i

i

i

k ≥ h
=

≥ h

W, X ∗
h

i ≥ h

W, X ′′

W, X ′
h
i
W + ∆, X ′
h
W + ∆, X ∗
h
(using the optimality of X ′)
+

∆, X ′
i
∆, X ′

i − h

i − h

∆, X ′

∆, X ∗
i − h
h
2√n
∆
kF
(using (15) and (16))

k

i

i

i

i −

= 2

2

≥

W, X ∗
h
W, X ∗
h

which is equivalent to

since

W, X ∗
h

i
diag(X ∗)

k

diag(X ∗) + diag(X ′)

k
is positive. Now we have

2

−

2√n

∆
k
W, X ∗
h

kF
i

k ≥

diag(X ′)
k

−

=

≤

2

4

(∵

− k

diag(X ∗) + diag(X ′)
k
diag(X ′)
diag(X ∗)
p
k
k
k
1/2
2√2n1/4
∆
F
k
k
1/2
W, X ∗
i
h
(using (17))

=

k

.

(17)

= 1)

This concludes the proof.

25

5 Conclusions

In this work, we propose a novel generative model, NFM, for graphs which, unlike the SBM, also
generates feature vectors for each node in the graph. We analyze, theoretically and computationally,
the performance of two different SDP formulations in recovering the true clusters in graph instances
generated according to the NFM. In particular, we begin with an algorithm based on the SDP (P-1D),
but then demonstrate its lack of robustness to certain noisy instances generated by the NFM. To
overcome this shortcoming, we propose a new algorithm based on a different SDP (P-ND). We
build theory towards showing that SDP (P-ND) can be used to provably recover, for each true cluster,
nodes with sufﬁciently strong membership signal in their feature vectors, in the presence of noisy
nodes, without involving any tuning parameters.

References

Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. Machine Learning, 56

(1-3):89–113, 2004.

Erik D Demaine, Dotan Emanuel, Amos Fiat, and Nicole Immorlica. Correlation clustering in

general weighted graphs. Theoretical Computer Science, 361(2-3):172–187, 2006.

Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Correlation clustering

with noisy partial information. In Conference on Learning Theory, pages 1321–1342, 2015.

Claire Mathieu and Warren Schudy. Correlation clustering with noisy input.

In Proceedings of
the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, pages 712–728. SIAM,
2010.

Chaitanya Swamy. Correlation clustering: Maximizing agreements via semideﬁnite programming.

In SODA, volume 4, pages 526–527. Citeseer, 2004.

Emmanuel J Candès and Benjamin Recht. Exact matrix completion via convex optimization. Foun-

dations of Computational mathematics, 9(6):717, 2009.

Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research,

12(12), 2011.

Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. More algorithms for provable dictionary

learning. arXiv preprint arXiv:1401.0579, 2014.

Daniel A Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries. In

Conference on Learning Theory, 2012.

Animashree Anandkumar, Rong Ge, Daniel Hsu, and Sham Kakade. A tensor spectral approach
to learning mixed membership community models. In Conference on Learning Theory, pages
867–881. PMLR, 2013.

Jimit Majmudar

and Stephen Vavasis.

tion in weighted graphs.
Balcan,
tems,
https://proceedings.neurips.cc/paper/2020/file/db957c626a8cd7a27231adfbf51e20eb-Paper.pdf.

detec-
In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Information Processing Sys-
URL

pages 19028–19038. Curran Associates,

editors, Advances

and H. Lin,

volume 33,

overlapping

community

in Neural

Provable

2020.

Inc.,

Xueyu Mao, Purnamrita Sarkar, and Deepayan Chakrabarti. On mixed memberships and symmet-
ric nonnegative matrix factorizations. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pages 2324–2333. JMLR. org, 2017.

Thorsten Joachims and John Hopcroft. Error bounds for correlation clustering. In Proceedings of

the 22nd International Conference on Machine Learning, pages 385–392, 2005.

Yudong Chen, Ali Jalali, Sujay Sanghavi, and Huan Xu. Clustering partially observed graphs via

convex optimization. Journal of Machine Learning Research, 15(1):2213–2238, 2014.

26

Jafar Jafarov, Sanchit Kalhan, Konstantin Makarychev, and Yury Makarychev. Correlation clustering
with asymmetric classiﬁcation errors. In International Conference on Machine Learning, pages
4641–4650. PMLR, 2020.

Jafar Jafarov, Sanchit Kalhan, Konstantin Makarychev, and Yury Makarychev. Local correlation
clustering with asymmetric classiﬁcation errors. In International Conference on Machine Learn-
ing, pages 4677–4686. PMLR, 2021.

Jess Banks, Cristopher Moore, Joe Neeman, and Praneeth Netrapalli. Information-theoretic thresh-
In Conference on Learning Theory, pages

olds for community detection in sparse networks.
383–416. PMLR, 2016.

Jess Banks, Cristopher Moore, Roman Vershynin, Nicolas Verzelen, and Jiaming Xu. Information-
theoretic bounds and phase transitions in clustering, sparse PCA, and submatrix localization.
IEEE Transactions on Information Theory, 64(7):4872–4894, 2018.

Quentin Berthet and Nicolai Baldin. Statistical and computational rates in graph logistic regression.
In International Conference on Artiﬁcial Intelligence and Statistics, pages 2719–2730. PMLR,
2020.

Quentin Berthet, Philippe Rigollet, and Piyush Srivastava. Exact recovery in the Ising blockmodel.

The Annals of Statistics, 47(4):1805–1834, 2019.

Tengyao Wang, Quentin Berthet, and Yaniv Plan. Average-case hardness of RIP certiﬁcation. Ad-

vances in Neural Information Processing Systems, 29:3819–3827, 2016.

Ravishankar Krishnaswamy, Nived Rajaraman, et al. Robust correlation clustering.

In Ap-
proximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques (AP-
PROX/RANDOM 2019). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2019.

Marshall Bern, John R Gilbert, Bruce Hendrickson, Nhat Nguyen, and Sivan Toledo. Support-graph

preconditioners. SIAM Journal on Matrix Analysis and Applications, 27(4):930–951, 2006.

Xinxin Li, Ting Kei Pong, Hao Sun, and Henry Wolkowicz. A strictly contractive peaceman-
rachford splitting method for the doubly nonnegative relaxation of the minimum cut problem.
Computational Optimization and Applications, 78(3):853–891, 2021.

Hao Hu, Renata Sotirov, and Henry Wolkowicz. Facial reduction for symmetry reduced semideﬁnite

doubly nonnegative programs. arXiv preprint arXiv:1912.10245, 2019.

Danilo Elias Oliveira, Henry Wolkowicz, and Yangyang Xu. ADMM for the SDP relaxation of the

QAP. Mathematical Programming Computation, 10(4):631–658, 2018.

L.E.J. Brouwer. Über abbildung von mannigfaltigkeiten. Mathematische Annalen, 71:97–115, 1912.

URL http://eudml.org/doc/158520.

27

