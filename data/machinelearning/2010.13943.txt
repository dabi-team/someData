0
2
0
2

t
c
O
6
2

]
E
N
.
s
c
[

1
v
3
4
9
3
1
.
0
1
0
2
:
v
i
X
r
a

Interior Point Solving for LP-based
prediction+optimisation

Jayanta Mandi
Data Analytics Laboratory
Vrije Universiteit Brussel
jayanta.mandi@vub.be

Tias Guns
Data Analytics Laboratory
Vrije Universiteit Brussel
tias.guns@vub.be

Abstract

Solving optimization problems is the key to decision making in many real-life
analytics applications. However, the coefﬁcients of the optimization problems
are often uncertain and dependent on external factors, such as future demand
or energy or stock prices. Machine learning (ML) models, especially neural
networks, are increasingly being used to estimate these coefﬁcients in a data-
driven way. Hence, end-to-end predict-and-optimize approaches, which consider
how effective the predicted values are to solve the optimization problem, have
received increasing attention. In case of integer linear programming problems,
a popular approach to overcome their non-differentiabilty is to add a quadratic
penalty term to the continuous relaxation, such that results from differentiating
over quadratic programs can be used. Instead we investigate the use of the more
principled logarithmic barrier term, as widely used in interior point solvers for
linear programming. Speciﬁcally, instead of differentiating the KKT conditions, we
consider the homogeneous self-dual formulation of the LP and we show the relation
between the interior point step direction and corresponding gradients needed for
learning. Finally our empirical experiments demonstrate our approach performs
as good as if not better than the state-of-the-art QPTL (Quadratic Programming
task loss) formulation of Wilder et al. [29] and SPO approach of Elmachtoub and
Grigas [12].

1

Introduction

There is recently a growing interest in data-driven decision making. In many analytics applications,
a combinatorial optimization is used for decision making with the aim of maximizing a predeﬁned
objective. However, in many real-world problems, there are uncertainties over the coefﬁcients of the
objective function and they must be predicted from historical data by using a Machine Learning (ML)
model, such as stock price prediction for portfolio optimization [5]. In this work, we propose a novel
approach to integrate ML and optimization in a deep learning architecture for such applications.

We consider combinatorial optimization problems that can be formulated as a mixed integer linear
program (MILP). MILP has been used, to tackle a number of combinatorial optimization problems,
for instance, efﬁcient micro-grid scheduling [20], sales promotion planning [8] and more. Speciﬁcally,
we want to train a neural network model to predict the coefﬁcients of the MILP in a way such that the
parameters of the neural network are determined by minimizing a task loss, which takes the effect of
the predicted coefﬁcients on the MILP output into consideration. The central challenge is how to
compute the gradients from the MILP-based task loss, given that MILP is discrete and the Linear
Programming (LP) relaxation is not twice differentiable.

To do so, Wilder et al. [29] proposed to compute the gradients by differentiating the KKT conditions
of the continuous relaxation of the MILP. However, to execute this, they have to add a quadratic

34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

 
 
 
 
 
 
regularizer term to the objective. We overcome this by differentiating the homogeneous self-dual
embedding of the relaxed LP. In summary, we present Linear Programs (LP) as the ﬁnal layer on top
of a standard neural network architecture and this enables us to perform end-to-end training of an
MILP optimization problem.

Related work Several approaches focus on differentiating an argmin optimization problem to ﬁt
it within neural network layers. For convex optimization problems, the KKT conidtions maps the
coefﬁcients to the set of solutions. So the KKT conditions can be differentiated, using implicit
function theorem, for argmin differentiation. Following this idea, Amos and Kolter [3] developed a
PyTorch compatible differentiable layer for Quadratic Programming (QP) optimization. In a similar
way Agrawal et al. [2] introduce a differntiable layer for convex cone program using LSQR 21.
Agrawal et al. [1] proposed to use this framework for convex optimization problem after transforming
it to convex cone programs.

Donti et al. [11] looked into end-to-end training for convex QP optimization. End-to-end training
of submodular optimization problems, zero-sum games and SAT problems have been proposed by
Wilder et al. [29], Ling et al. [17], Wang et al. [28] respectively. While Wilder et al. [29] studied
gradients for LP problems before, they proposed to add a quadratic regularization term to the LP such
that the QP framework of Amos and Kolter [3] can be used. For the case of MILP, Ferber et al. [13]
realized that before converting the MILP to an LP, the LP formulation can be strengthened by adding
MILP cutting planes to it.

A different yet effective approach was proposed by Elmachtoub and Grigas [12], who derive a
convex surrogate-loss based subgradient method for MILP which does not require differentiating
the optimization problem. Mandi et al. [18] showed that even in case of MILP, it is sufﬁcient and
more efﬁcient to use the subgradient of the LP relaxation. Recently, Poganˇci´c et al. [25] propose to
compute the gradient considering “implicit interpolation” of the argmin operator. Conceptually, their
approach is different from [12], but the computed gradient is very closely related to it.

Contributions
In contrast to the above, we propose to add a log-barrier term to the LP relaxation,
which also makes it twice differentiable and is standard practice in LP solving when using interior
point methods. Furthermore, taking inspiration from how such methods compute their search
direction, we propose to differentiate the homogenous self-dual embedding of the LP instead of
differentiating the KKT conditions. We also address practical challenges such as early stopping of
the interior point solving to avoid numerical issues and the use of damping to avoid ill-conditioned
matrices. Our comparative evaluations against the state of the art reveal that this approach yields
equal to better results while being a methodologically closer integration between LP solving and LP
task loss gradients.

2 Problem Description

Let us consider Mixed Integer Linear Programming (MILP) problems in the standard form [22]:

min c(cid:62)x
subject to Ax = b;

x ≥ 0;

some or all xi integer

(1)

with variables x and coefﬁcients c ∈ Rk, A ∈ Rp×k, b ∈ Rp. Note that any inequality constraints
Cx ≤ d can be straightforwardly transformed into this normal form by adding auxiliary slack
variables [22]. We denote the optimal solution of Eq (1) by x∗(c; A, b).

As a motivating example, consider the 0-1 knapsack problem consisting of a set of items with their
values and weights known. The objective is to ﬁll a knapsack obtaining as much value from the
items (e.g. c) without exceeding the capacity of the knapsack (expressed through A and b after
transformation to standard form).

Two-stage approach

We consider a predict-and-optimize setting, where the coefﬁcient c papapmeter of the optimization
problem is unknown. Furthermore, we assume training instances {zi, ci}n
i=1 are at our disposal and

2

Algorithm 1: End-to-end training of an LP (relaxed MILP) problem
Input :A, b, training data D ≡ {z, c}n

i=1

1 initialize θ
2 for epochs do
3

for batches do

4

5

6

7

8

sample batch (z, c) ∼ D
ˆc ← g(z, θ)
x∗ ← Neural LP layer Forward Pass to compute x∗(ˆc; A, b)
∂x∗
∂ˆc ← Neural LP layer Backward Pass
Compute ∂L
∂ˆc
∂θ and update θ

∂θ = ∂L
∂x∗

∂x∗
∂ˆc

end

9
10 end

hence can be utilized to train an ML model g(z; θ), consisting of model parameters θ, to predict
ˆc(θ) (for notational conciseness we will write ˆc in stead of ˆc(θ)). A straightforward approach in this
case would be to train the ML model g(., θ) to minimize a ML loss L(ˆc, c) like mean squared error,
without considering the downstream optimization task. Then on the test instances, ﬁrst the ML model
is used to predict ˆc and thereafter the predicted ˆc values are used to solve the MILP. As prediction
and optimization are executed separately here, this is a two-stage approach [9].

Regret

The shortcoming of the two-stage approach is that it does not take the effect on the optimization task
into account. Training to minimize an ML loss is not guaranteed to deliver better performance in
terms of the decision problem [15, 18]. Training the model parameters w.r.t a task-loss L(ˆc, c; A, b),
which considers the downstream optimization task, would allow for an end-to-end alternative.

x∗(ˆc; A, b) − x∗(c; A, b)

For an (MI)LP problem, the cost of using predicted ˆc instead of ground-truth c is measured by
(cid:17)
regret(ˆc, c; A, b) = c(cid:62)(cid:16)
; that is, the difference in value when optimizing
over the predictions ˆc instead of the actual values c. Note that one can equivalently use c(cid:62)x∗(ˆc; A, b),
the actual value when optimizing over the predictions, as c(cid:62)x∗(c; A, b) is constant for the given c.
As these losses consider how the prediction would do with respect to the optimization task; they are
valid task-losses.

The challenge in directly minimizing the regret lies in backpropagating it through the network in the
backward pass. For this purpose, the gradients of the regret with respect to the model parameters must
be computed which involves differentiating over the argmin of the optimisation problem. Speciﬁcally,
in order to update the model parameters (θ) with respect to the task loss L(θ), we have to compute
∂L
∂θ to execute gradient descent. By using the chain rule, this can be decomposed as follows [29]:

∂L
∂θ

=

∂L
∂x∗(ˆc; A, b)

∂x∗(ˆc; A, b)
∂ˆc

∂ˆc
∂θ

(2)

∂L

The ﬁrst term is the gradient of the task-loss with respect to the variable, in case of regret for an
∂x∗(ˆc;A,b) = c. The third term is the gradient of the predictions with respect to
(MI)LP this is simply
the model parameters, which is automatically handled by modern deep learning frameworks. The
challenge lies in computing the middle term, which we will expand on in the next section. Algorithm 1
depicts the high-level end-to-end learning approach with the objective of minimizing the task-loss for
a relaxed MILP problem.

3 End-to-end Predict-and-Optimize by Interior Point Method

For the discrete MILP problem, argmin is a piecewise constant function of ˆc as changing ˆc will change
the solution x∗(ˆc; A, b) only at some transition points [10]. Hence for every point in the solution space
the gradient is either 0 or undeﬁned. To resolve this, ﬁrst, we consider the continuous relaxation of the
MILP in Eq 1. The result is a Linear Program (LP) with the following primal and dual respectively [6]:

3

min c(cid:62)x
subject to Ax = b;

x ≥ 0

max b(cid:62)y

(3)

subject to A(cid:62)y + t = c;

(4)

t ≥ 0

with x and c, A, b as before; and dual variable y with slack variable t. The goal is to compute the
gradient ∂x∗(ˆc;A,b)

by differentiating the solution of this LP with respect to the predicted ˆc.

∂ˆc

3.1 Differentiating the KKT conditions

For reasons that will become clear, we write the objective function of the LP in Eq 3 as f (c, x). Using
the Lagrangian multiplier y, which correspond to the dual variable, the Lagrangian relaxation of Eq 3
can be written as

L(x, y; c) = f (c, x) + y(cid:62)(b − Ax)

(5)

An optimal primal-dual pair (x, y) obtained by solving x∗(c; A, b) must obey the Karush-Kuhn-
Tucker (KKT) conditions, obtained by setting the partial derivative of Eq 5 with respect to x and y to
0. Let fx

.
= ∂2f

.
= ∂2f

.
= ∂f

∂c∂x , we obtain:

∂x2 , fcx

∂x , fxx

fx(c, x) − A(cid:62)y = 0
Ax − b = 0

(6)

The implicit differentiation of these KKT conditions w.r.t. c allows us to ﬁnd the following system of
equalities:

(cid:21)

(cid:20)fcx(c, x)
0

+

(cid:20)fxx(c, x) −A(cid:62)

A

0

(cid:21)

(cid:21) (cid:20) ∂
∂c x
∂
∂c y

= 0

(7)

To obtain ∂x
c(cid:62)x and hence fx(c, x) = c and fxx(c, x) = 0 hence the second derivative is always 0.

∂c we could solve Eq 7 if f (c, x) is twice-differentiable; but for an LP problem f (c, x) =

Squared regularizer term One approach to obtain a non-zero second derivative is to add a
quadratic regularizer term λ (cid:107)x(cid:107)2 where λ is a user-deﬁned weighing hyperparameter. This is
proposed by [29] where they then use techniques for differentiating quadratic programs. An alterna-
tive is to add this squared term f (c, x) := c(cid:62)x + λ (cid:107)x(cid:107)2 which changes the Lagrangian relaxation
and makes Eq (7) twice differentiable.

(cid:17)
Logarithmic barrier term Instead, we propose to add the log barrier function[7] λ
i=1 ln(xi)
which is widely used by primal–dual interior point solvers[30]. Furthermore, it entails the constraint
x ≥ 0, for λ → 0, and this constraint is part of the LP constraints (Eq. (3)) yet it is not present when
using the previous term in the Lagrangian relaxation (Eq (5)).

(cid:16) (cid:80)k

(cid:17)
The objective transforms to f (c, x) := c(cid:62)x − λ
i=1 ln(xi)
fxx(c, x) = λX −2e (where X represents diag(x)). Denoting t
system of equations by substitution in Eq. (6):

(cid:16) (cid:80)k

hence fx(c, x) = c − λX −1e and
.
= λX −1e we obtain the following

c − t − A(cid:62)y = 0
Ax − b = 0

t = λX −1e

(8)

Notice, the ﬁrst equations of Eq 8 are same as the constraints of the primal dual constraints in
Eq (3) and Eq (4). This shows why log-barrier term is the most natural tool in LP problem, the
KKT conditions of the log-barrier function deﬁnes the primal and the dual constraints. Also notice,
t

.
= λX −1e implies x(cid:62)t = kλ (where k = dim(x)).

Differentiating Eq 8 w.r.t. c, we obtain a system of equations similar to Eq 7 with nonzero fxx
and solving this gives us ∂x∗(c;A,b)
. Note, both these approaches involve solving the LP to ﬁnd the
optimal solution (x, y) and then computing ∂x

∂c

∂c around it.

4

3.2 Choice of λ and early stopping

The sequence of points (xλ, yλ, tλ), parametrized by λ (>0), deﬁne the central path. When λ → 0, it
converges to the primal-dual solution of the LP. Basically, the central path provides a path that can
be followed to reach the optimal point. This is why in an interior point solver, Eq 8 is solved for a
decreasing sequence of λ. Remark that x(cid:62)t ( =kλ) is the duality gap [30] and λ → 0 means at the
optimal (x, y) the duality gap is zero.
Now, the approach proposed before, runs into a caveat. As we have shown fxx(c, x) = λX −2e,
hence λ → 0 implies fxx → 0.

In order to circumvent this, we propose to stop the interior point solver as soon as λ becomes smaller
than a given λ-cut-off; that is before λ becomes too small and the correspondingly small fxx leads to
numeric instability. There are two merits to this approach: 1) as λ is not too close to 0, we have better
numerical stability; and 2) by stopping early, the interior point method needs to do fewer iterations
and hence fewer computation time.

We remark that in case of a squared regularizer term one has to specify a ﬁxed λ; while in our case,
the interior point solver will systematically decrease λ and one has to specify a cut-off that prevents
λ from becoming too small.

3.3 Homogeneous self-dual Formulation

Primal–dual interior point solvers based on solving Eq (8) are known to not handle infeasible solutions
well, which can make it difﬁcult to ﬁnd an initial feasible point [30]. Instead, interior point solvers
often use the homogeneous self-dual formulation [31]. This formulation always has a feasible solution
and is known to generate well-centered points [24]. Because of this advantage, the following larger
formulation (the HSD formulation) is solved instead of (8) when iteratively updating the interior
point. This formulation always has a solution, where (x, τ ) and (t, κ) are strictly complementary:

Ax − bτ = 0
A(cid:62)y + t − cτ = 0
−c(cid:62)x + b(cid:62)y − κ = 0

t = λX −1e
λ
τ

κ =

x, t, τ, κ ≥ 0

(9)

Algorithm 2 shows the forward and backward pass of the proposed neural LP layer in pseudo code.

3.3.1 Forward pass

In the forward pass we use the existing homogeneous interior point algorithm [4] to search for the
LP solution. It starts from an interior point (x, y, t, τ, κ) with (x, t, τ, κ) > 0 and updates this point
iteratively for a decreasing sequence of λ through the Newton method. Based on Eq. (9), in each step
the following Newton equation system is solved to ﬁnd the directions dx, dy, dt, dτ , dκ in which to
update the interior point:


0
0 −b
A(cid:62) I −c
0
0 X 0
κ
0
0

η(Ax − bτ )
η(A(cid:62)y + t − cτ )
η(−c(cid:62)x + b(cid:62)y − κ)
Xt − γλe
τ κ − γλ
where T = diag(t) and γ and η are two nonnegative algorithmic parameters [4].
The last two equalities can be rewritten as follows: dt = X −1(ˆrxt − T dx) and dκ = (ˆrτ κ − κdτ )/τ .
Through substitution and algebraic manipulation we obtain the following set of equations:

0
0
0 −1
0
τ

ˆrp
ˆrd
ˆrg
ˆrxt
ˆrτ κ

dx
dy
dt
dτ
dκ

−c(cid:62) b(cid:62)









































= −

(10)

A
0

T
0

=



















−X −1T A(cid:62) −c
0
−b
b(cid:62) κ/τ

A
−c(cid:62)





(cid:35)

(cid:34)dx
dy
dτ

=

5





ˆrd − X −1ˆrxt
ˆrp
ˆrg + (ˆrτ κ/τ )





(11)

Algorithm 2: Neural LP layer

1 Hyperparameters: λ-cut-off, α damping factor
2 Forward Pass (c, A, b)
3

c, A, b ←PreSolve(c, A, b)
x, y, t, τ, κ ← Initialize()
repeat

4

5

/* such that x, t, τ, κ ≥ 0 */

7

8

6

9

(cid:17)

(cid:16)

Compute search-directions dx, dy, dτ from Eq 11 as in Appendix A.1
ω ← ﬁnd step size
x, y, t, τ, κ, dx, dy, dt, dτ , dκ
update x, y, t, τ, κ
λ ← x(cid:62)t+τ ×κ
dim(x)+1
until λ < λ-cut-off
(x, y, t) ← (x, y, t)/τ
return x
12
13 Backward Pass ()
14

11

10

retrieve (x, y, t; c, A, b)
compute M = AT −1XA(cid:62)
¯M = M + αI
return ∂x

15

16

17

(optional Tikhonov damping)
∂c by solving Eq 12 as in Appendix A.1 but with ¯M

(see Appendix A.1)

This can be solved by decomposing the LHS and solving the subparts through Cholesky decomposition
and substitution as explained in Appendix A.1 in the supplementary ﬁle.

3.3.2 Computing Gradients for the Backward pass

The larger HSD formulation contains more information than the KKT conditions in Eq 8 about
the LP solution, because of the added τ and κ where κ/τ represents the duality gap. Hence for
computing the gradients for the backward pass, we propose not to differentiate Eq 8 but rather the
HSD formulation in Eq 9 w.r.t. c. We do it following the procedure explained in Appendix A.2 in the
supplementary ﬁle. This enables us to write the following system of equations (T = diag(t)):





−X −1T A(cid:62) −c
0
−b
b(cid:62) κ/τ

A
−c(cid:62)



 =









∂x
∂c
∂y
∂c
∂τ
∂c





τ I
0
x(cid:62)





(12)

Note the similarity between Eq 11 solved in the forward pass to obtain the search direction dx for
improving x given ﬁxed coefﬁcients c, and Eq 12 in our backward pass to obtain the gradient ∂x
∂c for
how to improve c given a ﬁxed x!

Furthermore, because they only differ in their right-hand side, we can use the same procedure as
explained in Appendix A.1 to compute the desired ∂x

∂c in this set of equations.

Implementation consideration During the procedure of Appendix A.1, we have to solve a system
of the form M v = r, where M = AT −1XA(cid:62). Although M should be a positive deﬁnite (PD)
matrix for a full-row rank A, in practical implementation we often observed M is not a PD matrix.
To circumvent this, we replace M by its Tikhonov damped[19] form ¯M := M + αI, where α > 0 is
the damping factor.

4 Experiments

We evaluate our Interior point based approach (IntOpt) on three predict-and-optimize problems. We
compare it with a two-stage approach, the QPTL (quadratic programming task loss) [29] approach
and the SPO approach [12, 18]. Training is carried out over the relaxed problem, but we evaluate
the objective and the regret on the test data by solving the discrete MILP to optimality. We treat
the learning rate, epochs and weight decay as hyperparameters, selected by an initial random search
followed by grid search on the validation set. For the proposed IntOpt approach, the values of the
damping factor and the λ cut-off are chosen by grid search.

6

The neural network and the MILP model have been implemented using PyTorch 1.5.0 [23] and
Gurobipy 9.0 [14], respectively. The homogeneous algorithm implementation is based on the one of
the SciPy 1.4.1 Optimize module. All experiments were executed on a laptop with 8 × Intel® Core™
i7-8550U CPU @ 1.80GHz and 16 Gb of RAM. 1

MSE-loss

Regret (×104)

Two-stage QPTL
1550
(84)

11
(2)

SPO IntOpt
29
(8)

76
(31)

485
(0)

563
(300)

295
(177)

457
(295)

Table 1: Comparison among approaches for the Knapsack Problem. Maximisation problem, number
between brackets is standard deviation across 10 runs.

Knapsack formulation of real estate investments. In our ﬁrst experiment we formulate the decision
making of a real estate investor as a 0-1 knapsack problem. The prediction problem is to estimate the
sales price of housings before their constructions begin. The prediction is to be used by a real estate
agency to decide which projects to be undertaken with a budget constraint limitation. We assume the
cost of construction for each project is known to the agency. We use the dataset of Raﬁei and Adeli
[26], which comprises of two sets of features: a) the physical and ﬁnancial properties of the housings;
and b) the economic variables to gauge the state of the real estate market. The economic variables are
available upto ﬁve periods prior to the start of the construction.

We use an LSTM model across the 5 periods of economic variables, and then concatenate them
with the physical and ﬁnancial features before passing them to a fully connected layer for prediction.
Out of 372 instances 310 are used for training and cross validation and 62 for testing the model
performance.

In Table 1 we present both the MSE-loss of the predictions and the regret attained by using the
predictions. Clearly the MSE-loss is lowest for the two-stage approach, which optimizes MSE on the
training set. Note that with a linear objective, the predicted values are scale-invariant with respect to
the objective, and hence a higher MSE is not indicative of worse optimisation results.

SPO is able to surpass the performance of the two-stage approach in terms of the ﬁnal objective using
its subgradient approach. Intuitively, the subgradient indicates which items should be kept and which
to leave out, which may provide a stronger signal to learn from for this optimisation problem. Both
our approach and QPTL are found to perform worse than the two-stage approach, with IntOpt slightly
better than QPTL. Note also that the two-stage model achieves very low MSE meaning that there are
relatively few errors that can propagate into the optimisation problem.

Energy-cost aware scheduling. This is a resource-constrained day-ahead job scheduling problem to
minimize total energy cost [see 27]. Tasks must be assigned to a given number of machines, where
each task has a duration, an earliest start, a latest end, a resource requirement and a power usage,
and each machine has a resource capacity constraint. Also, tasks cannot be interrupted once started,
nor migrated to another machine and must be completed before midnight. Time is discretized in 48
timeslots. As day-ahead energy prices are not known, they must be predicted for each timeslot ﬁrst.

The energy price data is taken from the Irish Single Electricity Market Operator (SEMO) [15]. It
consists of historical energy price data at 30-minute intervals from 2011-2013. Out of the available
789 days, 552 are used for training, 60 for validation and 177 for testing. Each timeslot instance
has calendar attributes; day-ahead estimates of weather characteristics; SEMO day-ahead forecasted
energy-load, wind-energy production and prices; and actual wind-speed, temperature, CO2 intensity
and price. Of the actual attributes, we keep only the actual price, and use it as a target for prediction.
We use two kinds of neural networks, one without a hidden layer (0-layer) and one with a single
hidden layer (1-layer). Comparisons of the different approaches are presented in Table 2. We can see,
for all methods that the multilayer network (1-layer) slightly overﬁts and performs worse than 0-layer.
The 0-layer model performs the best both in terms of regret and prediction accuracy. Our IntOpt
approach is able to produce the lowest regret, followed by SPO with QPTL having worse results.

1implementation is available at https://github.com/JayMan91/NeurIPSIntopt

7

Two-stage

QPTL

SPO

IntOpt

0-layer

1-layer

0-layer

745
(7)

796
(5)

3516
(56)

1-layer
2 × 109
(4 × 107)

0-layer

1-layer

0-layer

3327
(485)

3955
(300)

2975
(620)

1-layer
1.6 × 107
(1 × 107)

13322
(1458)

13590
(288)
Table 2: Comparison among approaches for the Energy Scheduling problems

12342
(1335)

13590
(2021)

10774
(1715)

11073
(895)

13652
(325)

11406
(1238)

MSE-loss

Regret

λ / λ-cut-off

KKT, squared norm
10−3

10−10

10−1

KKT, log barrier
10−3

10−10

10−1

HSD, log barrier
10−3

10−1

10−10

Regret

15744

14365
14620
Table 3: Comparison among IntOpt variants for the Energy Scheduling problem

174717

209595

14958

21258

10774

21594

Choice of formulation and λ cut-off. We take a deeper look at the difference in choice of formu-
lation to differentiate over, as well as the choice of λ or λ cut-off. The results are in Table 3 and
shows that the HSD formulation performs best. This experiment also reveals stopping the forward
pass with a λ-threshold is effective, with worse results (we observed numerical issues) with a low
threshold value. Although a λ-cut-off of 0.1 seems to be the best choice, we recommend treating it as
a hyperparameter.

Shortest path problem. This experiment involves solving a shortest path problem on a grid, similar
to [12]. Instead of fully synthetic small-scale data, we use, as network structure, a twitter ego
networks comprising of 231 nodes and 2861 edges [16]. To solve the shortest path problem in this
subgraph, we created 115 problem instances each having different source and destination pair. This is
a predict-and-optimize problem, where ﬁrst the edge intensity must be predicted from node and edge
features, before solving the shortest path problem.

The set of hashtags and mentions used by a user during the observation period are the feature variables
of the corresponding node. To generate feature-correlated edge weights, we use a random network
approach inspired by [29]: we construct a random 3-layer network that receives a concatenation of
the node features of its edges and a random number as input. We add some additional Gaussian noise
to the resulting output and set that as edge weight. The same network is used for all edges.

neural
the 115 instances, we use 80 for

networks,

of

namely with

training,

selection

and

20

for model

1
15
testing.

and

two

kinds

hyperparameter

use
Out of

predictive model we

As
and 2 hidden layers.
for model
validation
Table 4 presents average MSE-loss and regrets ob-
tained on the test instances. IntOpt performs best
with respect to the regret, though marginally. The
MSE values of QPTL and IntOpt are noticeably high,
but we repeat that given the linear objective the pre-
dictions are scale-invariant from the optimisation per-
spective.

We can see this effect clearly in Figure 1, where
the predictions of IntOpt and the two-stage model are
plotted against the ground truth. Here we explain why
the MSE of IntOpt is high yet the regret is low. We
observe, because of the scale invariance property of
the linear objective, IntOpt predictions are typically
shifted by several magnitudes from the groundtruth.
But apart form that, it is easy to spot the similarity in
the relation between the two sets of predictions. If looked at carefully, IntOpt can indeed be seen to
predict extreme cases wrongly, which we can only assume will have little effect on the optimization
problem, as IntOpt attains lower regret. This validates that the IntOpt approach is able to learn the
relationship between the predictor and the feature variables from the indirect supervision of the
optimization problem alone.

Figure 1: Groundtruth vs Predictions

8

Two-stage

1-layer

2-layer

51
(3)

94
(1)

QPTL

1-layer
1 × 103
(1 × 102)

2-layer
1 × 103
(1 × 102 )

SPO

1-layer

2-layer

534
(12)

504
(120)

IntOpt

1-layer
1.7 × 108
(3.5 × 105)

2-layer
2.4 × 109
(4.3 × 108)

MSE-loss

Regret

143
(19)

197
(20)
Table 4: Comparison among approaches for the shortest path problem (minimization).

119
(47)

223
(14)

322
(17)

152
(2)

138
(8)

92
(46)

Discussion. Out of the three experimental setups considered, for the ﬁrst experiment, both the
prediction and the optimization task are fairly simple, whereas the optimization tasks are challenging
for the other two experiments. It seems, our approach does not perform well compared to SPO for
the easier problem, but yields better result for the challenging problems.

We would like to point out, the SPO approach can take advantage of any blackbox solver as an
optimization oracle, whereas both QPTL and IntOpt use interior point algorithms to solve the
optimization problem. So, another reason for the superior performance of SPO on the ﬁrst problem,
could be these problem instances are better suited to algorithms other than interior point, and SPO
might beneﬁt from the fact that it uses Gurobi as a blackbox solver. In case of other knapsack
instances (see Appendix A.4) IntOpt and QPTL performs better than SPO.

5 Conclusion

We consider the problem of differentiating an LP optimization problem for end-to-end predict-and-
optimize inside neural networks. We develop IntOpt, a well-founded interior point based approach,
which computes the gradient by differentiating the homogeneous self dual formulation LP using a
log barrier. The model is evaluated with MILP optimization problems and continuous relaxation is
performed before solving and differentiating the optimization problems. In three sets of experiments
our framework performs on par or better than the state of the art.

By proposing an interior point formulation for LP instead of adding a quadratic term and using
results from QP, we open the way to further improvements and tighter integrations using insights
from state-of-the-art interior point methods for LP and MILP. Techniques to tighten the LP relaxation
(such as [13]) can potentially further beneﬁt our approach.

Runtime is a potential bottleneck, as one epoch can take up to 15 minutes in the complex energy-cost
scheduling problem. We do note that we use a vanilla implementation of the homogenous algorithm,
instead of industry-grade optimisation solvers like Gurobi and CPlex. Furthermore, thanks to the
similarity of the direction computation in the forward pass and the gradient computation in the
backward pass, there is more potential for improving the equation system solving, as well as for
caching and reusing results across the epochs.

Broader Impact

Decision-focussed learning means that models might have lower accuracy but perform better at the
task they will be used for. This has the potential to change industrial optimisation signiﬁcantly, for
example in manufacturing, warehousing and logistics; where increasingly machine learning models
are used to capture the uncertainty, after which optimization is used.

In general, fusing barrier solving of integer linear programming with backpropagation enlarges the
scope of problems for which we can use deep learning, as well as further aligning the two domains.

Acknowledgments and Disclosure of Funding

We would like to thank the anonymous reviewers for the valuable comments and suggestions. We
thank Peter Stuckey and Emir Demirovic for the initial discussions. This research received funding
from the Flemish Government (AI Research Program) and FWO Flanders project G0G3220N.

9

References

[1] Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J Zico
Kolter. Differentiable convex optimization layers. In Advances in neural information processing
systems, pages 9562–9574, 2019.

[2] Akshay Agrawal, Shane Barratt, Stephen Boyd, Enzo Busseti, and Walaa M Moursi. Differenti-

ating through a cone program. arXiv preprint arXiv:1904.09043, 2019.

[3] Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 136–145. JMLR.org, 2017.

[4] Erling D. Andersen and Knud D. Andersen. The mosek interior point optimizer for linear pro-
gramming: An implementation of the homogeneous algorithm. High Performance Optimization,
pages 197–232, 2000. doi: 10.1007/978-1-4757-3216-0_8.

[5] Yoshua Bengio. Using a ﬁnancial training criterion rather than a prediction criterion. Interna-

tional Journal of Neural Systems, 8(04):433–443, 1997.

[6] Dimitris Bertsimas and John Tsitsiklis. Introduction to linear programming. Athena Scientiﬁc,

1, 1997.

[7] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge

university press, 2004.

[8] Maxime C Cohen, Ngai-Hang Zachary Leung, Kiran Panchamgam, Georgia Perakis, and
Anthony Smith. The impact of linear optimization on promotion planning. Operations Research,
65(2):446–468, 2017.

[9] Emir Demirovi´c, Peter J Stuckey, James Bailey, Jeffrey Chan, Chris Leckie, Kotagiri Ramamoha-
narao, and Tias Guns. An investigation into prediction+ optimisation for the knapsack problem.
In International Conference on Integration of Constraint Programming, Artiﬁcial Intelligence,
and Operations Research, pages 241–257. Springer, 2019.

[10] Emir Demirovi´c, Peter J Stuckey, James Bailey, Jeffrey Chan, Christopher Leckie, Kotagiri
Ramamohanarao, and Tias Guns. Dynamic programming for predict+ optimise. 2020.

[11] Priya L. Donti, Brandon Amos, and J. Zico Kolter. Task-based end-to-end model learning
in stochastic optimization. In Advances in Neural Information Processing Systems, pages
5484–5494, 2017.

[12] Adam N Elmachtoub and Paul Grigas. Smart "predict, then optimize". arXiv preprint

arXiv:1710.08005, 2017.

[13] Aaron Ferber, Bryan Wilder, Bistra Dilkina, and Milind Tambe. Mipaal: Mixed integer program

as a layer. In AAAI, pages 1504–1511, 2020.

[14] LLC Gurobi Optimization. Gurobi optimizer reference manual, 2020. URL http://www.

gurobi.com.

[15] Georgiana Ifrim, Barry O’Sullivan, and Helmut Simonis. Properties of energy-price forecasts for
scheduling. In International Conference on Principles and Practice of Constraint Programming,
pages 957–972. Springer, 2012.

[16] Jure Leskovec and Julian J Mcauley. Learning to discover social circles in ego networks. In

Advances in neural information processing systems, pages 539–547, 2012.

[17] Chun Kai Ling, Fei Fang, and J. Zico Kolter. What game are we playing? end-to-end learning
in normal and extensive form games. In IJCAI 2018: 27th International Joint Conference on
Artiﬁcial Intelligence, pages 396–402, 2018.

[18] Jayanta Mandi, Tias Guns, Emir Demirovi´c, and Peter. J Stuckey. Smart predict-and-optimize for
hard combinatorial optimization problems. In AAAI 2020 : The Thirty-Fourth AAAI Conference
on Artiﬁcial Intelligence, volume 34, pages 1603–1610, 2020.

10

[19] James Martens and Ilya Sutskever. Training deep and recurrent networks with hessian-free
optimization. In Neural networks: Tricks of the trade, pages 479–535. Springer, 2012.

[20] Hugo Morais, Péter Kádár, Pedro Faria, Zita A Vale, and HM Khodr. Optimal scheduling
of a renewable micro-grid in an isolated load area using mixed-integer linear programming.
Renewable Energy, 35(1):151–156, 2010.

[21] Christopher C Paige and Michael A Saunders. Lsqr: An algorithm for sparse linear equations
and sparse least squares. ACM Transactions on Mathematical Software (TOMS), 8(1):43–71,
1982.

[22] Christos H Papadimitriou and Kenneth Steiglitz. Combinatorial optimization: algorithms and

complexity. Courier Corporation, 1998.

[23] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. In Advances in Neural Information Processing
Systems, pages 8024–8035, 2019.

[24] PAN Ping-Qi. Linear programming computation. Springer, 2014.

[25] Marin Vlastelica Poganˇci´c, Anselm Paulus, Vit Musil, Georg Martius, and Michal Rolinek.
In ICLR 2020 : Eighth International

Differentiation of blackbox combinatorial solvers.
Conference on Learning Representations, 2020.

[26] Mohammad Hossein Raﬁei and Hojjat Adeli. A novel machine learning model for estimation
of sale prices of real estate units. Journal of Construction Engineering and Management, 142
(2):04015066, 2016.

[27] Helmut Simonis, Barry O’Sullivan, Deepak Mehta, Barry Hurley, and Milan De Cauwer.
CSPLib problem 059: Energy-cost aware scheduling. http://www.csplib.org/Problems/
prob059.

[28] Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter. Satnet: Bridging deep learning
and logical reasoning using a differentiable satisﬁability solver. In ICML 2019 : Thirty-sixth
International Conference on Machine Learning, pages 6545–6554, 2019.

[29] Bryan Wilder, Bistra Dilkina, and Milind Tambe. Melding the data-decisions pipeline: Decision-
focused learning for combinatorial optimization. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, volume 33, pages 1658–1665, 2019.

[30] Stephen J Wright. Primal-dual interior-point methods, volume 54. Siam, 1997.

[31] Xiaojie Xu, Pi-Fang Hung, and Yinyu Ye. A simpliﬁed homogeneous and self-dual linear
programming algorithm and its implementation. Annals of Operations Research, 62(1):151–
171, 1996.

11

A Supplementary Material for Interior Point Solving for LP-based

prediction+optimisation

A.1 Solution of Newton Equation System of Eq. (11)

Here we discuss how we solve an equation system of Eq (11), for more detail you can refer to[4].
Consider the following system with a generic R.H.S-





−X −1T A(cid:62) −c
−b
0
b(cid:62) κ/τ

A
−c(cid:62)





(cid:35)

(cid:34)x1
x2
x3

=

(cid:35)

(cid:34)r1
r2
r3

If we write:

.
=

W

(cid:20)−X −1T A(cid:62)
0

A

(cid:21)

(13)

(14)

then, observe W is nonsingular provided A is full row rank. So it is possible to solve the following
system of equations-

W

W

(cid:21)
(cid:20)p
q
(cid:21)
(cid:20)u
v

=

=

(cid:21)
(cid:20)c
b
(cid:20)r1
r2

(cid:21)

Once we ﬁnd p, q, u, v ﬁnally we compute x3 as:

x3 =

r3 + u(cid:62)c − v(cid:62)b
−c(cid:62)p + b(cid:62)q + κ
τ

;

x1 = u + px3
x2 = v + qx3

And ﬁnally

To solve equation of the form

W

(cid:21)
(cid:20)u
v

=

(cid:20)−X −1T A(cid:62)
0

A

(cid:21)
(cid:21) (cid:20)u
v

=

(cid:21)

(cid:20)r1
r2

(15)

(16)

(17)
(18)

Notice we can reduce it to M v = AT −1Xr1 + r2 (where M = AT −1XA(cid:62)). As M is positive
deﬁnite for a full row-rank A, we obtain v by Cholesky decomposition and ﬁnally u = T −1X(A(cid:62)v −
r1).

A.2 Differentiation of HSD formulation in Eq. (9)

We differentiate Eq. (9) with respect to c:

∂(Ax)
∂c

−

∂(A(cid:62)y)
∂c
∂(c(cid:62)x)
∂c

+

−

+

−

∂t
∂c
∂(b(cid:62)y)
∂c

∂(bτ )
∂c
∂(cτ )
∂c

−

∂κ
∂c
∂t
∂c
∂κ
∂c

= 0

= 0

= 0

=

=

∂(λX −1e)
∂c
∂( λ
τ )
∂c

(19)

12

Applying the product rule we can further rewrite this into:

= 0

+ τ I) = 0

A

− b

− (c

∂t
∂c

∂τ
∂c

∂x
∂c
A(cid:62) ∂y
∂τ
+
∂c
∂c
+ x(cid:62)) + b(cid:62) ∂y
−(c(cid:62) ∂x
∂κ
∂c
∂c
∂c
= −λX −2 ∂x
∂c
∂τ
∂κ
∂c
∂c

∂t
∂c

λ
τ 2

= −

−

= 0

(20)

∂c = −X −1T ∂x
Using t = λX −1e ↔ λe = XT e we can rewrite the fourth equation to ∂t
∂c . Similarly
∂c = − κ
we use κ = λ
∂τ
∂c . Substituting these into
the ﬁrst three we obtain:

τ ↔ λ = κ × τ and rewrite the ﬁfth equation to ∂κ

τ

A

∂x
∂c

− b

∂τ
∂c

= 0

∂x
− c
∂c
− x(cid:62) + b(cid:62) ∂y
∂c
This formulation is written in matrix form in Eq. (12).

A(cid:62) ∂y
∂c
−c(cid:62) ∂x
∂c

− X −1T

∂τ
∂c

+

− τ I = 0

κ
τ

∂τ
∂c

= 0

(21)

A.3 LP formulation of the Experiments

A.3.1 Details on Knapsack formulation of real estate investments

In this problem, H is the set of housings under consideration. For each housing h, ch is the known
construction cost of the housing and ph is the (predicted) sales price. With the limited budget B, the
constraint is

(cid:88)

h∈H

chxh = B, xh ∈ 0, 1

where xh is 1 only if the investor invests in housing h. The objective function is to maximize the
following proﬁt function

max
xh

(cid:88)

h∈H

phxh

A.3.2 Details on Energy-cost aware scheduling

In this problem J is the set of tasks to be scheduled on M number of machines maintaining resource
requirement of R resources. The tasks must be scheduled over T set of equal length time periods.
Each task j is speciﬁed by its duration dj, earliest start time ej, latest end time lj, power usage pj.ujr
is the resource usage of task j for resource r and cmr is the capacity of machine m for resource r.
Let xjmt be a binary variable which possesses 1 only if task j starts at time t on machine m. The
ﬁrst constraint ensures each task is scheduled and only once.

(cid:88)

(cid:88)

m∈M

t∈T

xjmt = 1 , ∀j∈J

The next constraints ensure the task scheduling abides by earliest start time and latest end time
constraints.

Finally the resource requirement constraint:

xjmt = 0 ∀j∈J ∀m∈M ∀t<ej
xjmt = 0 ∀j∈J ∀m∈M ∀t+dj >lj

(cid:88)

(cid:88)

j∈J

t−dj <t(cid:48)≤t

xjmt(cid:48)ujr ≤ cmr, ∀m∈M ∀r∈R∀t∈T

13

If ct is the (predicted) energy price at time t, the objective is to minimize the energy cost of running
all tasks, given by:

min
xjmt

(cid:88)

(cid:88)

(cid:88)

(cid:16) (cid:88)

xjmt

(cid:17)

pjct(cid:48)

j∈J

m∈M

t∈T

t≤t(cid:48)<t+dj

A.3.3 Details on Shortest path problem

In this problem, we consider a directed graph speciﬁed by node-set N and edge-set E. Let A be the
|N | × |E| incidence matrix, where for an edge e that goes from n1 to n2, the (n1, e)th entry is 1 and
(n2, e)th entry is -1 and the rest of entries in column e are 0. In order to, traverse from source node s
to destination node d, the following constraint must be satisﬁed:

Ax = b

where x is |E| dimensional binary vector whose entries would be 1 only if corresponding edge is
selected for traversal and b is |N | dimensional vector whose sth entry is 1 and dth entry is -1; and rest
are 0. With respect to the (predicted) cost vector c ∈ R|E|, the objective is to minimize the cost

c(cid:62)x

min
x

A.4 Additional Knapsack Experiments

This knapsack experiment is taken from [18], where the knapsack instances are created from the
energy price dataset 15. The 48 half-hour slots are considered as 48 knapsack items and a random
cost is assigned to each slot. The energy price of a slot is considered as the proﬁt-value and the
objective is to select a set of slots which maximizes the proﬁt ensuring the total cost of the selected
slots remains below a ﬁxed budget. We also added the approach of Blackbox [25], which also deals
with a combinatorial optimization problem with a linear objective.

Budget

60
120

Two-
stage
1042 (3)
1098 (5)

QPTL

SPO

Blackbox

IntOpt

579 (3)
380 (2)

624 (3)
425 (4)

533 (40)
383 (14)

570 (58)
406 (71)

A.5 Hyperparameters of the experiments 2

A.5.1 Knapsack formulation of real estate investments

Model
Two-stage
SPO
QPTL
IntOpt
* for all experiments embedding size: 7 number of layers:1,hidden layer size: 2

Hyperaprameters*
• optimizer: optim.Adam; learning rate: 10−3
• optimizer: optim.Adam; learning rate: 10−3
• optimizer: optim.Adam; learning rate: 10−3; τ (quadratic regularizer): 10−5
• optimizer: optim.Adam; learning rate: 10−2; λ-cut-off: 10−4; damping factor α: 10−3

A.5.2 Energy-cost aware scheduling

Model
Two-stage
SPO
QPTL
IntOpt

Hyperaprameters
• optimizer: optim.SGD; learning rate: 0.1
• optimizer: optim.Adam; learning rate: 0.7
• optimizer: optim.Adam; learning rate: 0.1; τ (quadratic regularizer): 10−5
• optimizer: optim.Adam; learning rate: 0.7; λ-cut-off: 0.1; damping factor α: 10−6

2For more details refer to https://github.com/JayMan91/NeurIPSIntopt

14

A.5.3 Shortest path problem

Model

Two-stage

SPO

QPTL

IntOpt

1-layer
2-layer
1-layer
2-layer
1-layer
2-layer
1-layer
2-layer

Hyperaprameters*
• optimizer: optim.Adam; learning rate: 0.01
• optimizer: optim.Adam; learning rate: 10−4
• optimizer: optim.Adam; learning rate: 10−3
• optimizer: optim.Adam; learning rate: 10−3
• optimizer: optim.Adam; learning rate: 0.7; τ (quadratic regularizer): 10−1
• optimizer: optim.Adam; learning rate: 0.7; τ (quadratic regularizer): 10−1
• optimizer: optim.Adam; learning rate: 0.7; λ-cut-off: 0.1; damping factor α: 10−2
• optimizer: optim.Adam; learning rate: 0.7; λ-cut-off: 0.1; damping factor α: 10−2

* for all experiments hidden layer size: 100

A.6 Learning Curves

(a) Energy-cost aware scheduling

(b) Shortest path problem

Figure 2: IntOpt Learning Curve

15

